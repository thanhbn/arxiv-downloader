# 2309.11210.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2309.11210.pdf
# File size: 378327 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SPEAK WHILE YOU THINK:
STREAMING SPEECH SYNTHESIS DURING TEXT GENERATION
Avihu Dekel, Slava Shechtman, Raul Fernandez, David Haws, Zvi Kons, Ron Hoory
IBM Research
ABSTRACT
Large Language Models (LLMs) demonstrate impressive
capabilities, yet interaction with these models is mostly facil-
itated through text. Using Text-To-Speech to synthesize LLM
outputs typically results in notable latency, which is impracti-
cal for fluent voice conversations. We propose LLM2Speech ,
an architecture to synthesize speech while text is being gen-
erated by an LLM which yields significant latency reduc-
tion. LLM2Speech mimics the predictions of a non-streaming
teacher model while limiting the exposure to future context in
order to enable streaming. It exploits the hidden embeddings
of the LLM, a by-product of the text generation that contains
informative semantic context. Experimental results show that
LLM2Speech maintains the teacher’s quality while reducing
the latency to enable natural conversations.
Index Terms —Incremental TTS, Speech Generation,
Large Language Models (LLMs)
1. INTRODUCTION
The appearance of conversational Large Language Models
(LLMs) [1, 2] has revolutionized the scope of interactions
with computers. By leveraging the principles of self-learning
and vast amounts of unlabeled training data, LLMs have es-
tablished a new state-of-the-art across a variety of tasks, and
showed great promise as a tool to augment human intelli-
gence. Currently, the interaction with LLMs is usually facili-
tated through text even though in many applications, such as
driving assistance, the spoken modality is far more preferable,
intuitive, and safe. As a result, pure audio-based LLMs [3, 4]
are gaining interest in the community, though their seman-
tic language understanding capabilities still lag behind tex-
tual LLMs. A simple alternative approach for spoken LLMs
that would address this issue would be to couple a text-based
language model with a neural Text-To-Speech (TTS) system
capable of producing high-fidelity speech samples [5, 6]. TTS
models, however, often require an entire sentence to generate
natural speech, resulting in notable latency when combined
with an LLM that typically generates text in a slow autore-
gressive fashion. This work tackles several challenges that
arise when trying to read aloud text generated by an LLM,
incrementally and with minimal delay.TTS systems typically adopt a two-step process, first con-
verting graphemes to phonemes (G2P) and then converting
phones to speech, as this process often achieves improved
quality and stability over character-to-speech approaches
for languages with irregular orthography (e.g. English,
French) [7, 8]. Context-dependent G2P methods look be-
yond the unigram level to improve the quality of the phonetic
prediction [9, 10, 11]. Such models can, e.g., better account
for cross-word-boundary flapping, vocalic reduction, and het-
eronym disambiguation. In these models, however, the con-
text required for disambiguation may be long, rendering them
unsuitable for streaming. Incremental TTS works create low-
latency TTS systems with limited lookahead (i.e. exposure
to future context) and minimal degradation [12, 13, 14, 15].
In most scenarios, the entire text is available before synthe-
sis, and the focus is on reducing algorithmic delay. These
methods may, e.g., run a full-sentence lightweight G2P mod-
ule before synthesis without significantly contributing to the
overall delay. When considering a slow incoming stream of
generated text, though, this assumption no longer holds.
This paper introduces LLM2Speech , a system that in-
tegrates a generative LLM with a streamable TTS system.
LLM2Speech can speak the text aloud while it is being gener-
ated by the LLM, without compromising correctness or natu-
ralness. LLM2Speech utilizes LLM embeddings, a by-product
of the text generation that contains semantic information and
might compensate for the lack of future context in streaming.
LLM2Speech consists of three parts (Fig. 1):
Fig. 1 : Streaming speech synthesis during text generation.
Word tokens and embeddings are incrementally generated by
the LLM, sent to LLM2PnP to produce phones and prosody
(PnP), which are sent to PnP2Speech to produce audio.
1. A pretrained LLM, which we deliberately freeze due to
the vast computational and human effort invested in bringing
it to its final state [16].arXiv:2309.11210v1  [eess.AS]  20 Sep 2023

--- PAGE 2 ---
2.LLM2PnP : an adaptor converting the LLM outputs to
Phones and Prosody ( PnP), which is described in Sec. 2.2.
3.PnP2Speech : a streamable version of the TTS system
in [17], which operates on chunks of PnP (see Sec. 2.3).
LLM2PnP is trained on a large textual dataset via offline-to-
streaming knowledge distillation [18, 19] during which it at-
tempts to mimic the predictions of a teacher model that has
access to the full text. We evaluate LLM2Speech against the
offline teacher TTS for phonetic accuracy as well as the qual-
ity of the synthesized speech. We demonstrate that the over-
all quality is maintained using both objective and qualita-
tive measures. LLM2Speech shows impressive prosodic pre-
dictions even for expressive inputs (e.g. happy, empathetic,
uncertain), and can also synthesize interjections and filled
pauses (e.g., hmm ,uh-huh ,oh, etc.).1
The work proposed here makes the following novel con-
tributions to the field of conversational speech synthesis:
1. It introduces a pipeline that converts an LLM output
text into expressive speech incrementally and with low delay.
2. It proposes a streaming knowledge-distillation method
for training PnP models based on large textual datasets.
3. It quantifies the contributions of LLM hidden embed-
dings to the task of PnP prediction.
2. METHOD
2.1. Dataset creation
AsLLM2Speech uses the tokens and embeddings of an LLM,
it is trained for a specific LLM. We experiment with the
T5language model [20], due to its capabilities to perform
diverse conditional generation tasks. Specifically, we use
T5-lm-adapt , which was finetuned for text completion.
We construct our training dataset based on the C4 (Common
Crawl Cleaned Corpus) dataset [20], which was also used
to train T5. As C4 contains 365M samples, we consider
only a random fraction of the dataset containing 3M training
and 130K validation samples. Each sample in C4 contains a
paragraph, which we split randomly into two parts: context
andtext-to-predict (t2pred) , such that t2pred has 1-5 sen-
tences. We simulate conditional text generation where the
LLM is prompted with a context and generates t2pred by
inputting context to the T5encoder and t2pred to the T5
decoder.2The inputs to the LLM2PnP training task are the
word tokens for t2pred and their contextual embeddings (see
Fig. 2). We also obtain the PnP annotations for t2pred using
a teacher model with no lookahead restrictions. The PnP
teacher model is a rules-based G2P model predicting the pho-
netic sequence and phrase type, followed by a neural model
predicting Hierarchical Prosodic Controls (HPCs) [21] for
an expressive conversational speaking style [22] and phone
1Audio samples can be found here: https://ibm.biz/BdMe5X
2When using decoder only LLMs, the text split is not needed.
Fig. 2 : Dataset creation. Above: extracting tokens and con-
textual embeddings from the LLM, which are conditioned on
thecontext . Below: pseudo-labeling for phones and prosody,
generated by the teacher model.
durations. HPCs are speaker-agnostic prosodic statistics that
can be calculated from recordings at various resolutions and
have been used for various tasks [22, 17]. We use duration
and pitch HPCs [17] augmented with maximal log-energy,
evaluated at the sentence, word, and phone hierarchies. To
account for text-normalization expansions (e.g. converting 23
totwenty-three ), we differentiate between regular andinner
word separators, where inner word separators are placed only
when expansions occur. During inference, LLM2PnP synthe-
sizes a word until reaching a regular word separator, and then
waits for the LLM to generate the next word.
2.2. LLM2PnP
TheLLM2PnP is a transformer encoder-decoder model, aug-
mented with attention restrictions. The encoder input is a se-
quence of tokens (word pieces) and their contextual embed-
dings, obtained from the hidden layers of the LLM, which
are projected to the encoder using a linear layer. The decoder
outputs are used by three prediction modules that predict the
identity, prosodic features, and phrase type of the next phone.
2.2.1. Restricted attention
To restrict dependence on future context, we formalize re-
stricted attention with a fixed word lookahead L. We first de-
fine the sequence of words w1, ..., w n, word tokens t1, ..., t m
andPnP tokens p1, ..., p k. Each word token tjis a part of
some word wi, which we denote by word( tj) =i. Similarly,
for every phoneme pjand its word wi, we denote word( pj) =
i. We now denote that an output token ycan attend to an input
token xbyy→x. In regular encoder attention and encoder-
decoder attention [23], ∀i, jwe have ti→tjandpi→tj(see
Fig 3a). We define restricted encoder and encoder-decoder at-
tention as follows:
ti→tj⇐⇒ word( tj)≤word( ti) (1)
pi→tj⇐⇒ word( tj)≤word( pi) +L (2)
We chose to use Lin the encoder-decoder attention since it
would not grow in consecutive decoder layers, unlike encoder
attention, where the lookahead would grow linearly with the

--- PAGE 3 ---
number of layers. Fig. 3b visualizes restricted attention, high-
lighting that t1̸→t3andp1̸→t3, which ensures the predic-
tion of p1would not depend on t3.
(a) Regular attention
 (b) Restricted attention
Fig. 3 : Regular/restricted attention (colored by word). In (a),
every encoder/decoder token can attend to every encoder to-
ken. In (b), dependence on future context is limited to the
current word ( L= 0) as in Eqs. 1–2, allowing streaming.
2.3. PnP2Speech
PnP2Speech is a streamable version of the HPC-based Par-
allel Prosody Transfer (PPT) model [17], composed of an
acoustic model, based on the non-attentive Tacotron (NAT)
backbone [24], followed by a lightweight and streamable
LPCNet vocoder [25]. PnP2Speech operates on small in-
put chunks instead of the entire sequence, and requires the
changes to [17] described below. First, BLSTMs [26] were
chunked, thus becoming LC-BLSTM [27] layers with zero
lookahead ( la= 0). Next, the growing right-receptive field
of convolution neural network (CNN) layers was addressed
using lookahead-constrained CNNs (LC-CNNs). Symmetric-
kernel convolutions are used as long as the lookahead con-
straint is met; otherwise, skewed-kernel convolutions (a gen-
eralization of causal convolutions [28]) are applied, resulting
in a constrained lookahead (See Fig. 4). Finally, on inference,
we include guardbands when chunking the PnP-to-frame
Gaussian upsampling matrix [24], as the upsampling depends
on the adjacent future PnP.
(a) CNN
 (b) LC-CNN ( la= 1)
Fig. 4 : Visualising the receptive field of p5(in orange) at the
output of two stacked convolution layers with a k= 3 ker-
nel. In Fig 4a, the right receptive field (i.e. lookahead) grows
with the number of layers. In Fig. 4b, the 2nd convolution is
skewed so that the final lookahead would be exactly la= 1.We train PnP2Speech on a 6.5-hr, proprietary, conversa-
tional speech corpus recorded by a professional US-English
female speaker. This set contains a variety of expressive dia-
log acts and interjections and has been described in [22]. To
further improve the performance of interjections and expres-
sive styles, we finetuned LLM2PnP on the same corpus de-
scribed above, training it to predict the conversational set of
PnPs from the conversational text.
3. EXPERIMENTS
In the following experiments, LLM2PnP has 4 encoder and 6
decoder layers, each one with token/feedforward dimensions
of 512/768 and 4 attention heads. LLM2PnP has a single word
lookahead and uses the T5-Base embeddings from layers 2,
6, and 10 (out of 12 layers, numbered from input to output).
PnP2Speech has a frame size of 256samples for 22kHz -
sampled speech. Its phonetic encoder has 3 LC-CNN layers
and a single chunked BLSTM layer, followed by Gaussian
upsampling, then the autoregressive LSTM decoder, and the
5-layered LC-CNN PostNet. LC-CNN layers have a kernel
size of 5and a lookahead of 2. Chunked BLSTM layers have
a chunk size of 4, and the Gaussian upsampler uses a 2-phone
guardband. The proposed PnP2Speech system results in an
algorithmic delay of 6PnP tokens plus 2 frames, which is
approximately equivalent to one word (where word separators
and pauses are also considered as PnP tokens). Consequently,
the total LLM2Speech lookahead sums up to 2 words.
3.1. Quality and naturalness assessment
We crowd-sourced a Mean Opinion Score (MOS) listening
test to evaluate LLM2Speech quality and naturalness, com-
paring three systems:
1. Teacher: The non-streamable, teacher PnP model, fol-
lowed by a non streaming TTS [17].
2.LLM2Speech (Ours): using LLM2PnP to extract PnP,
followed by PnP2Speech .
3. Stream-Teacher: forcing the teacher into the same
lookahead as LLM2Speech , by using the teacher G2P on text
prefixes with 1-word lookahead, the prosody model with
lookahead restrictions, and PnP2Speech for synthesis.
Model Lookahead MOS
Teacher ∞ 4.10±0.04
LLM2Speech 2 4.12±0.04
Stream-Teacher 2 3.46±0.06
Table 1 : Listening test results, reporting the MOS and the
95% confidence interval.
The synthesized audio was evaluated on 45 conversational
texts, and rated for overall quality and naturalness by 25 na-
tive listeners on a standard MOS 5-point scale. Results in

--- PAGE 4 ---
Table 1 show no statistically significant difference between
the teacher and the streamable LLM2Speech . Note that the
teacher makes predictions based on the entire text (which is
unrealistic for streaming) and utilizes the sub-style labeling
of the text (e.g., empathetic, happy, etc.) that LLM2Speech is
not exposed to.
3.2. G2P ablation study
We evaluate the G2P performance of LLM2PnP by measuring
the word error rate (WER) on the C4 validation set. We ac-
centuate the differences by presenting results on the following
challenging subsets: (i) Rare : least common words covering
20% of the text, (ii) Norm : words expanded by normalization,
e.g. 23, and (iii) OOV : words unseen during training.
Incremental TTS methods trade off latency and perfor-
mance, which are determined by the lookahead. In Table 2,
we show the effect on G2P performance by modifying the
lookahead of LLM2PnP . Results suggest the first lookahead
word is quite significant while the second yields smaller bene-
fits. This observation may be partly explained by post-lexical
processes in US English which influence the pronunciation of
a word depending on the word that follows.
Lookahead All Rare Norm OOV
0 6.40 14.99 21.71 37.33
1 1.95 2.71 6.28 18.90
2 1.69 2.55 6.02 18.62
∞ 1.31 2.17 5.28 17.28
Table 2 : Lookahead influence on G2P performance, mea-
sured by Word Error Rate (%), on all words and challenging
subsets as defined in Sec 3.2.
In the experiments described above we made use of the
T5-Base model. However, larger language models are more
commonly used due to their improved performance, and the
issue of using more or fewer LLM hidden layers could also be
considered. We investigate the influence of the LLM embed-
dings used by LLM2PnP on the G2P performance by adding
and removing embedding layers from T5-Base , and by also
making use of the 24-layer T5-Large andT5-XL models.
Table 3 suggests that both adding more layers and increasing
the LLM size improve the G2P performance. However, the
benefits gained by the choice of LLM embeddings are smaller
than those gained by an additional word lookahead.
3.3. Prosody ablation study
To estimate prosodic quality, we conducted ABX preference
tests, where we compared LLM2Speech synthesized audio
(A) with another system’s audio (B) on the same texts as in
Sec. 3.1. Each pair of audio samples was rated by 25 dis-
tinct listeners, who were asked to rate their preference on
a scale of [−2,−1,0,1,2], where -2 is “strongly prefer A”LLM Emb Layers All Rare Norm OOV
- - 2.10 2.93 6.61 20.05
Base 6 1.98 2.78 6.51 19.84
2,6,10 1.95 2.71 6.28 18.90
2,4,6,8,10 1.93 2.69 6.21 18.66
Large 6,12,18 1.94 2.71 6.31 18.91
XL 6,12,18 1.89 2.62 6.02 18.31
Table 3 : Embedding influence on G2P as in Table 2.
2 is “strongly prefer B” and 0 is “no preference” We com-
pared to a variant of LLM2Speech (i) without finetuning on
the conversational corpus (NoFT), (ii) without LLM embed-
dings (NoEmb), (iii) with T5-XL embeddings (T5XL), and
(iv) with a lookahead of L= 2forLLM2PnP (LA2). For fair-
ness, we removed interjections from the texts in (i), as NoFT
was not exposed to them during training. Results in Table 4
suggest that finetuning improved the overall naturalness, yet
other changes did not yield a significant difference.
Method BV ote Distribution (%)Avg Score
-2 -1 0 1 2
NoFT 8.9 30.1 30.2 24.6 6.2 -0.110
NoEmb 5.8 26.8 35.8 25.3 6.3 -0.005
T5XL 5.0 25.9 36.7 28.0 4.3 0.007
LA2 8.0 30.8 25.9 27.8 7.4 -0.042
Table 4 : ABX preference test results comparing LLM2Speech
(A) to another system (B). Negative scores mean A is pre-
ferred, and results indicating significant differences ( p <
0.01) are bolded.
4. DISCUSSION
Motivated by spoken conversational AI, we investigated read-
ing aloud LLM-generated text with low latency, paving the
way for natural AI conversations. We described simple mech-
anisms to limit the lookahead in attention and convolution
layers, with which we build a low-latency conversational
TTS system. We found that streaming TTS benefits from
offline-to-streaming distillation using large textual datasets,
even when the texts lack a conversational style. Moreover, the
LLM embeddings improved the phonetic prediction, yet did
not yield a significant improvement in the prosodic quality.
In future work, we aim to improve the audio quality by
utilizing natural speech and by extracting additional cues
from the LLM such as emotions. More broadly, we intend
to create a low-latency spoken dialogue system, powered by
an LLM semantic backbone. The system would consist of
a speech recognition model, followed by a conversational
LLM, which is coupled with LLM2Speech to produce speech
incrementally, with low latency.

--- PAGE 5 ---
5. REFERENCES
[1] OpenAI, “ChatGPT,” https://chat.openai.
com, 2021.
[2] Google, “Bard,” https://bard.google.com ,
2022.
[3] Z. Borsos et al., “AudioLM: A Language Modeling Ap-
proach to Audio Generation,” IEEE Trans. on ASLP ,
vol. 31, pp. 2523–2533, 2023.
[4] F. Kreuk et al., “AudioGen: Textually Guided Audio
Generation,” in ICLR , 2023.
[5] J. Shen et al., “Natural TTS Synthesis by Conditioning
Wavenet on MEL Spectrogram Predictions,” in Proc.
ICASSP . IEEE, 2018, pp. 4779–4783.
[6] C. Wang et al., “Neural Codec Language Models are
Zero-Shot Text to Speech Synthesizers,” arXiv preprint
arXiv:2301.02111 , 2023.
[7] J. Fong, J. Taylor, K. Richmond, and S. King, “A Com-
parison of Letters and Phones as Input to Sequence-to-
Sequence Models for Speech Synthesis,” in Proc. SSW
10, 2019, pp. 223–227.
[8] J. Taylor and K. Richmond, “Analysis of Pronunciation
Learning in End-to-End Speech Synthesis,” in Proc. In-
terspeech , 2019, pp. 2070–2074.
[9] A. Ploujnikov and M. Ravanelli, “SoundChoice:
Grapheme-to-Phoneme models with semantic disam-
biguation,” in Proc. Interspeech , 2022, pp. 486–490.
[10] M. ˇRez´aˇckov ´a, J. ˇSvec, and D. Tihelka, “T5G2P: Us-
ing Text-to-Text Transfer Transformer for Grapheme-to-
Phoneme Conversion,” in Proc. Interspeech , 2021, pp.
6–10.
[11] J. Zhu, C. Zhang, and D. Jurgens, “ByT5 model for mas-
sively multilingual grapheme-to-phoneme conversion,”
inProc. Interspeech , 2022, pp. 446–450.
[12] M. Ma et al., “Incremental Text-to-Speech Synthesis
with Prefix-to-Prefix Framework,” in Proc. EMNLP
2020 , Nov 2020, pp. 3886–3896.
[13] J. Chen et al., “Speech-T: Transducer for Text to Speech
and Beyond,” in Proc. NeurIPS , 2021, vol. 34, pp.
6621–6633.
[14] C. Wu et al., “Transformer-Based Acoustic Modeling
for Streaming Speech Synthesis,” in Proc. Interspeech
2021 , 2021, pp. 146–150.
[15] N. Ellinas et al., “High Quality Streaming Speech
Synthesis with Low, Sentence-Length-Independent La-
tency,” in Proc. Interspeech , 2020, pp. 2022–2026.[16] L. Ouyang et al., “Training language models to follow
instructions with human feedback,” in NeurIPS , 2022,
vol. 35, pp. 27730–27744.
[17] S. Shechtman and R. Fernandez, “A Neural TTS System
with Parallel Prosody Transfer from Unseen Speakers,”
inProc. Interspeech , 2023, pp. 4853–4857.
[18] D. Povey, H. Hadian, P. Ghahremani, K. Li, and S. Khu-
danpur, “A Time-Restricted Self-Attention Layer for
ASR,” in Proc. ICASSP , 2018, pp. 5874–5878.
[19] G. Kurata and G. Saon, “Knowledge Distillation from
Offline to Streaming RNN Transducer for End-to-End
Speech Recognition,” in Interspeech , 2020, pp. 2117–
2121.
[20] C. Raffel et al., “Exploring the Limits of Transfer Learn-
ing with a Unified Text-to-Text Transformer,” Journal of
Machine Learning Research , vol. 21, no. 140, pp. 1–67,
2020.
[21] S. Shechtman, R. Fernandez, and D. Haws, “Supervised
and unsupervised approaches for controlling narrow lex-
ical focus in sequence-to-sequence speech synthesis,” in
Proc. SLT , 2021, pp. 431–437.
[22] R. Fernandez, D. Haws, G. Lorberbom, S. Shecht-
man, and A. Sorin, “Transplantation of Conversa-
tional Speaking Style with Interjections in Sequence-
to-Sequence Speech Synthesis,” in Proc. Interspeech ,
2022, pp. 5488–5492.
[23] A. Vaswani et al., “Attention is all you need,” in Ad-
vances NIPS , 2017, vol. 30.
[24] J. Shen et al., “Non-Attentive Tacotron: Robust and
Controllable Neural TTS Synthesis Including Unsuper-
vised Duration Modeling,” CoRR , vol. abs/2010.04301,
2020.
[25] J-M Valin and J. Skoglund, “LPCNET: Improving Neu-
ral Speech Synthesis through Linear Prediction,” in
Proc. ICASSP , 2019, pp. 5891–5895.
[26] A. Graves and J. Schmidhuber, “Framewise phoneme
classification with bidirectional LSTM and other neural
network architectures,” Neural networks , vol. 18, no.
5-6, pp. 602–610, 2005.
[27] Y . Zhang et al., “Highway Long Short-Term Mem-
ory RNNs for Distant Speech Recognition,” in Proc.
ICASSP , 2016, pp. 5755–5759.
[28] A. Oord et al., “Wavenet: A generative model for raw
audio,” arXiv preprint arXiv:1609.03499 , 2016.
