# 2311.14957.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2311.14957.pdf
# Kích thước file: 4129851 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
BỘ PHÂN BIỆT BIẾN ĐỔI CONSTANT-Q ĐA THANG ĐỘ TIỂU DẢI CHO VOCODER ĐỘ TRUNG THỰC CAO
Yicheng Gu Xueyao Zhang Liumeng Xue Zhizheng Wu
Trường Khoa học Dữ liệu, Đại học Trung văn Hồng Kông, Thâm Quyến (CUHK-Shenzhen), Trung Quốc
TÓM TẮT
Các vocoder dựa trên Mạng Đối Kháng Sinh Tạo (GAN) vượt trội về tốc độ suy luận và chất lượng tổng hợp khi tái tạo sóng âm có thể nghe được từ biểu diễn âm thanh. Nghiên cứu này tập trung vào việc cải thiện bộ phân biệt để thúc đẩy các vocoder dựa trên GAN. Hầu hết các bộ phân biệt dựa trên biểu diễn thời gian-tần số hiện tại đều có nguồn gốc từ Biến đổi Fourier Thời gian Ngắn (STFT), có độ phân giải thời gian-tần số cố định trong phổ đồ, khiến nó không tương thích với các tín hiệu như giọng hát yêu cầu sự chú ý linh hoạt cho các dải tần số khác nhau. Được thúc đẩy bởi điều đó, nghiên cứu của chúng tôi sử dụng Biến đổi Constant-Q (CQT), có độ phân giải động giữa các tần số, góp phần vào khả năng mô hình hóa tốt hơn trong độ chính xác cao độ và theo dõi hài âm. Cụ thể, chúng tôi đề xuất Bộ Phân Biệt CQT Tiểu Dải Đa Thang Độ (MS-SB-CQT), hoạt động trên phổ đồ CQT ở nhiều thang độ và thực hiện xử lý tiểu dải theo các quãng tám khác nhau. Các thí nghiệm được thực hiện trên cả giọng nói và giọng hát xác nhận tính hiệu quả của phương pháp được đề xuất. Hơn nữa, chúng tôi cũng xác minh rằng các bộ phân biệt dựa trên CQT và dựa trên STFT có thể bổ sung cho nhau trong quá trình huấn luyện chung. Cụ thể, được tăng cường bởi MS-SB-CQT được đề xuất và các Bộ Phân Biệt MS-STFT hiện có, MOS của HiFi-GAN có thể được tăng từ 3.27 lên 3.87 cho các ca sĩ đã thấy và từ 3.40 lên 3.78 cho các ca sĩ chưa thấy.
Từ khóa chỉ mục —Neural vocoder, biến đổi constant-q, mạng đối kháng sinh tạo (GAN), bộ phân biệt

1. GIỚI THIỆU
Một neural vocoder tái tạo sóng âm có thể nghe được từ biểu diễn âm thanh. Các mô hình sinh tạo sâu bao gồm các mô hình dựa trên tự hồi quy [1, 2], dựa trên luồng [3, 4], dựa trên GAN [5–11], và dựa trên khuếch tán [12, 13] đã thành công cho nhiệm vụ này. Vì tốc độ suy luận và chất lượng tổng hợp vượt trội, các vocoder dựa trên GAN luôn hấp dẫn các nhà nghiên cứu. Tuy nhiên, để tổng hợp giọng nói hoặc giọng hát biểu cảm, các vocoder dựa trên GAN hiện tại vẫn còn các vấn đề như tạo tác phổ chẳng hạn như tiếng xì [9] và mất chi tiết ở các phần tần số trung và thấp [10].

Để theo đuổi các vocoder dựa trên GAN chất lượng cao, các nghiên cứu hiện tại nhằm cải thiện cả bộ sinh và bộ phân biệt. Đối với bộ sinh, SingGAN [10] áp dụng mô-đun bộ lọc nguồn thần kinh [14] để sử dụng kích thích sin. BigVGAN [11] giới thiệu hàm kích hoạt mới với các mô-đun chống răng cưa. Đối với bộ phân biệt, MelGAN [6] sử dụng bộ phân biệt dựa trên miền thời gian mô hình hóa thành công các cấu trúc dạng sóng ở các thang độ khác nhau lần đầu tiên. HiFi-GAN [8] mở rộng nó với Bộ Phân Biệt Đa Thang Độ và Bộ Phân Biệt Đa Chu Kỳ, và Fre-GAN [9] cải thiện thêm bằng cách thay thế pooling trung bình bằng các bộ lọc dựa trên biến đổi wavelet rời rạc để bảo toàn thông tin tần số. UniversalMelGAN [7] giới thiệu Bộ Phân Biệt Đa Độ Phân Giải, tiếp theo là [15] nhấn mạnh tầm quan trọng của nó. Encodec [16] mở rộng nó thành Bộ Phân Biệt STFT Đa Thang Độ (MS-STFT).

Nghiên cứu này tập trung vào việc cải thiện bộ phân biệt. Trong số các công trình hiện tại, hầu hết các bộ phân biệt dựa trên biểu diễn thời gian-tần số đều có nguồn gốc từ Biến đổi Fourier Thời gian Ngắn (STFT) [7, 15, 16], có thể nhanh chóng trích xuất các phổ đồ STFT dễ xử lý cho mạng thần kinh. Tuy nhiên, nó cũng có những hạn chế. Cụ thể, một phổ đồ STFT có độ phân giải thời gian-tần số cố định trên tất cả các bin tần số (Mục 2.1). Khi gặp các tín hiệu như giọng hát, yêu cầu sự chú ý khác nhau cho các dải tần số khác nhau [17], chỉ một phổ đồ STFT sẽ không đủ.

Được thúc đẩy bởi điều đó, bài báo này đề xuất bộ phân biệt dựa trên Biến đổi Constant-Q (CQT) [18]. Lý do là CQT có độ phân giải linh hoạt hơn cho các dải tần số khác nhau so với STFT. Trong dải tần số thấp, CQT có độ phân giải tần số cao hơn, có thể mô hình hóa thông tin cao độ chính xác. Trong dải tần số cao, CQT có độ phân giải thời gian cao hơn, có thể theo dõi các biến đổi hài âm thay đổi nhanh. Ngoài ra, CQT có tần số trung tâm phân bố theo thang log, có thể mang lại thông tin mức cao độ tốt hơn [19]. Cụ thể, chúng tôi thiết kế Bộ Phân Biệt CQT Tiểu Dải Đa Thang Độ (MS-SB-CQT). Bộ phân biệt hoạt động trên các phổ đồ CQT ở các thang độ khác nhau và thực hiện xử lý tiểu dải theo thông tin quãng tám của phổ đồ CQT. Hơn nữa, trong các thí nghiệm, chúng tôi thấy rằng các Bộ Phân Biệt MS-SB-CQT và MS-STFT [16] được đề xuất có thể được sử dụng chung để thúc đẩy bộ sinh hơn nữa, điều này tiết lộ vai trò bổ sung giữa các bộ phân biệt dựa trên CQT và dựa trên STFT.

2. BỘ PHÂN BIỆT BIẾN ĐỔI CONSTANT-Q TIỂU DẢI ĐA THANG ĐỘ

Kiến trúc của Bộ Phân Biệt MS-SB-CQT được đề xuất, có thể tích hợp vào bất kỳ vocoder dựa trên GAN nào, được minh họa trong Hình 1. Nó bao gồm các bộ phân biệt phụ có cấu trúc giống hệt nhau hoạt động trên các phổ đồ CQT ở các thang độ khác nhau. Mỗi bộ phân biệt phụ sẽ đầu tiên gửi các phần thực và ảo của CQT đến mô-đun Xử lý Tiểu Dải (SBP) được đề xuất của chúng tôi một cách riêng biệt để có được các biểu diễn ẩn của chúng. Hai biểu diễn này sau đó sẽ được nối và gửi đến các lớp tích chập để có được các đầu ra cho việc tính toán mất mát. Chi tiết của mỗi mô-đun sẽ được giới thiệu như sau.

2.1. Điểm Mạnh của Biến đổi Constant-Q

Trong mục này, điểm mạnh của CQT sẽ được thể hiện bằng cách giới thiệu ý tưởng thiết kế của nó. Chúng ta sẽ thấy CQT có độ phân giải thời gian-tần số linh hoạt như thế nào và tại sao nó có thể mô hình hóa thông tin mức cao độ tốt hơn.

Theo [18], CQT Xcq(k, n) có thể được định nghĩa là:
Xcq(k, n) =n+⌊Nk/2⌋X
j=n−⌊Nk/2⌋x(j)a∗
k(j−n+Nk/2), (1)

--- TRANG 2 ---
Hình 1: Kiến trúc của Bộ Phân Biệt Biến đổi Constant-Q Tiểu Dải Đa Thang Độ (MS-SB-CQT) được đề xuất, có thể tích hợp với bất kỳ vocoder dựa trên GAN nào. Toán tử "C" biểu thị sự nối. SBP có nghĩa là mô-đun Xử lý Tiểu Dải được đề xuất của chúng tôi. Có thể quan sát thấy rằng Phổ đồ CQT không đồng bộ (dưới-phải) đã được đồng bộ hóa (trên-phải) sau SBP.

trong đó k là chỉ số của bin tần số, x(j) là điểm mẫu thứ j của tín hiệu được phân tích, Nk là độ dài cửa sổ, ak(n) là hạt nhân có giá trị phức, và a∗k(n) là liên hợp phức của ak(n).

Các hạt nhân ak(n) có thể được thu được là:
ak(n) =1/Nk*wn/Nk*e^(-i2πnQk/Nk), (2)

trong đó w(t) là hàm cửa sổ, và Qk là yếu tố Q không đổi:
Qk ref.=fk/∆fk= (2^(1/B)−1)^(-1), (3)

trong đó fk là tần số trung tâm, ∆fk là băng thông xác định sự đánh đổi độ phân giải, và B là số bin trên một quãng tám.

Đáng chú ý, đối với STFT, ∆fk là hằng số, nghĩa là độ phân giải thời gian-tần số được cố định cho tất cả các tần số. Tuy nhiên, đối với CQT, ý tưởng chính của nó là giữ Qk không đổi. Kết quả là, các dải tần số thấp sẽ có ∆fk nhỏ hơn, mang lại độ phân giải tần số cao hơn, có thể mô hình hóa thông tin cao độ tốt hơn. Bên cạnh đó, các dải tần số cao sẽ có ∆fk lớn hơn, mang lại độ phân giải thời gian cao hơn, có thể theo dõi các biến đổi hài âm thay đổi nhanh tốt hơn.

Trong Phương trình (2), độ dài cửa sổ của bin tần số thứ k, Nk, có thể được thu được là:
Nk=fs/∆fk=fs/(fk·(2^(1/B)−1)^(-1)) (4)

trong đó fs là tần số lấy mẫu, và fk được định nghĩa là:
fk=f1·2^((k−1)/B), (5)

trong đó f1 là tần số trung tâm thấp nhất, được đặt là 32.7 Hz (C1) trong nghiên cứu của chúng tôi.

2.2. Các Bộ Phân Biệt Phụ Đa Thang Độ

Để nắm bắt thông tin dưới các độ phân giải thời gian-tần số đa dạng hơn, chúng tôi tận dụng ý tưởng đa thang độ [8, 15] và áp dụng các bộ phân biệt phụ trên CQT với các sự đánh đổi độ phân giải tổng thể khác nhau. Cho Phương trình (3) và (5), chúng ta có thể quan sát thấy rằng băng thông ∆fk, xác định sự đánh đổi độ phân giải, phụ thuộc vào số bin trên một quãng tám B. Nói cách khác, chúng ta có thể đặt B khác nhau để có được các phân phối độ phân giải khác nhau. Dựa trên điều đó, chúng tôi theo [15, 16] để áp dụng ba bộ phân biệt phụ với B bằng 24, 36, và 48, tương ứng.

2.3. Mô-đun Xử lý Tiểu Dải

Như hai mặt của một đồng xu, mặc dù băng thông động ∆fk mang lại độ phân giải thời gian-tần số linh hoạt, nó cũng mang lại độ dài cửa sổ không cố định Nk. Kết quả là, các hạt nhân ak(n) trong các bin tần số khác nhau không được đồng bộ hóa về mặt thời gian [20]. Phổ đồ CQT với các tạo tác như vậy đã được hình dung ở dưới phải của Hình 1.

Để giảm thiểu vấn đề này, [20] thiết kế một loạt hạt nhân được đồng bộ hóa về mặt thời gian trong một quãng tám. Thuật toán này cũng đã được sử dụng trong các bộ công cụ như librosa [21] và nnAudio [22]. Tuy nhiên, thuật toán như vậy chỉ làm cho ak(n) của nội quãng tám được đồng bộ hóa về mặt thời gian nhưng để lại những vấn đề của liên quãng tám chưa được giải quyết. Trong các thí nghiệm, chúng tôi thấy rằng chỉ sử dụng phổ đồ CQT với sự thiên vị như vậy thậm chí có thể làm hại chất lượng của các vocoder (Mục 3.4).

Dựa trên điều đó, chúng tôi sử dụng triết lý của học biểu diễn và thiết kế mô-đun Xử lý Tiểu Dải (SBP) để giải quyết vấn đề này hơn nữa. Cụ thể, phần thực hoặc ảo của phổ đồ CQT sẽ đầu tiên được chia thành các tiểu dải theo các quãng tám. Sau đó, mỗi dải sẽ được gửi đến lớp tích chập tương ứng của nó để có được biểu diễn của nó. Cuối cùng, chúng tôi nối các biểu diễn từ tất cả các dải để có được biểu diễn ẩn của phổ đồ CQT. Ở trên phải của Hình 1, có thể quan sát thấy rằng SBP được đề xuất của chúng tôi đã học thành công các biểu diễn được đồng bộ hóa về mặt thời gian giữa tất cả các bin tần số.

2.4. Tích hợp với Vocoder dựa trên GAN

Bộ phân biệt được đề xuất của chúng tôi có thể dễ dàng tích hợp với các vocoder dựa trên GAN hiện tại mà không can thiệp vào giai đoạn suy luận. Chúng tôi lấy HiFi-GAN [8] làm ví dụ. HiFi-GAN có một bộ sinh G và nhiều bộ phân biệt Dm. Mất mát sinh LG và mất mát phân biệt LD được định nghĩa là, LG=ΣM m=1[Ladv(G;Dm) +

--- TRANG 3 ---
Bảng 1: Kết quả phân tích-tổng hợp của các bộ phân biệt khác nhau khi được tích hợp vào HiFi-GAN [8]. Kết quả tốt nhất và tốt thứ hai của mỗi cột (trừ những kết quả từ Ground Truth) trong mỗi miền (giọng nói và giọng hát) được in đậm và in nghiêng. "S" và "C" đại diện cho Bộ Phân Biệt MS-STFT và MS-SB-CQT tương ứng. Các điểm MOS có Khoảng Tin Cậy 95% (CI).

Miền | Hệ thống | MCD (↓) | PESQ (↑) | FPC (↑) | F0RMSE (↓) | MOS (↑)
--- | --- | --- | --- | --- | --- | ---
| | Đã thấy | Chưa thấy | Đã thấy | Chưa thấy | Đã thấy | Chưa thấy | Đã thấy | Chưa thấy | Đã thấy | Chưa thấy
Giọng hát | Ground Truth | 0.00 | 0.00 | 4.50 | 4.50 | 1.000 | 1.000 | 0.00 | 0.00 | 4.85 ±0.06 | 4.73 ±0.09
| HiFi-GAN | 2.82 | 3.17 | 2.94 | 2.86 | 0.954 | 0.961 | 56.96 | 59.28 | 3.27 ±0.16 | 3.40 ±0.15
| HiFi-GAN (+S) | 2.97 | 3.37 | 2.95 | 2.87 | 0.967 | 0.968 | 39.06 | 46.49 | 3.42 ±0.16 | 3.56 ±0.17
| HiFi-GAN (+C) | 2.90 | 3.35 | 3.03 | 2.95 | 0.970 | 0.971 | 35.57 | 41.09 | 3.66 ±0.14 | 3.63 ±0.16
| HiFi-GAN (+S+C) | 2.54 | 3.08 | 3.09 | 2.98 | 0.971 | 0.973 | 35.45 | 39.90 | 3.87 ±0.14 | 3.78 ±0.12
Giọng nói | Ground Truth | 0.00 | 0.00 | 4.50 | 4.50 | 1.000 | 1.000 | 0.00 | 0.00 | 4.62 ±0.11 | 4.59 ±0.11
| HiFi-GAN | 3.21 | 2.10 | 3.01 | 3.14 | 0.883 | 0.781 | 186.19 | 293.34 | 3.91±0.17 | 3.96 ±0.16
| HiFi-GAN (+S) | 3.47 | 2.10 | 2.97 | 3.09 | 0.869 | 0.772 | 195.05 | 298.53 | 4.02±0.15 | 4.00±0.17
| HiFi-GAN (+C) | 3.26 | 2.07 | 3.04 | 3.16 | 0.884 | 0.768 | 180.29 | 301.83 | 4.01 ±0.15 | 4.13±0.14
| HiFi-GAN (+S+C) | 3.13 | 2.05 | 3.05 | 3.15 | 0.883 | 0.792 | 182.04 | 281.90 | 4.02 ±0.17 | 4.14 ±0.15

2Lfm(G;Dm)] + 45 Lmel, LD=ΣM m=1[Ladv(Dm;G), trong đó M là số bộ phân biệt, Dm biểu thị bộ phân biệt thứ m, Ladv là mất mát GAN đối kháng, Lfm là mất mát khớp đặc trưng, và Lmel là mất mát tái tạo phổ đồ mel. Trong số những mất mát này, chỉ Lfm và Ladv liên quan đến bộ phân biệt của chúng tôi. Do đó, chỉ cần thêm Ladv(G;DMS-SB-CQT) + 2Lfm(G;DMS-SB-CQT) vào LG và Ladv(DMS-SB-CQT;G) vào LD có thể tích hợp bộ phân biệt được đề xuất trong quá trình huấn luyện.

3. THÍ NGHIỆM

Chúng tôi thực hiện các thí nghiệm để điều tra bốn câu hỏi sau. EQ1: Bộ Phân Biệt MS-SB-CQT được đề xuất hiệu quả như thế nào? EQ2: Việc sử dụng chung các Bộ Phân Biệt MS-SB-CQT và MS-STFT có thể cải thiện vocoder hơn nữa không? EQ3: Bộ Phân Biệt MS-SB-CQT tổng quát như thế nào dưới các vocoder dựa trên GAN khác nhau? EQ4: Có cần thiết phải áp dụng mô-đun SBP được đề xuất không? Các mẫu âm thanh có sẵn trên trang demo của chúng tôi¹.

3.1. Thiết lập thí nghiệm

Tập dữ liệu Các tập dữ liệu thí nghiệm bao gồm cả giọng nói và giọng hát. Đối với giọng hát, chúng tôi áp dụng M4Singer [23], PJS [24], và một tập dữ liệu nội bộ. Chúng tôi ngẫu nhiên lấy mẫu 352 phát âm từ ba tập dữ liệu để đánh giá các ca sĩ đã thấy và để lại phần còn lại cho huấn luyện (39 giờ). 445 mẫu từ Opencpop [25], PopCS [26], OpenSinger [27], và CSD [28] được chọn để đánh giá các ca sĩ chưa thấy. Đối với giọng nói, chúng tôi sử dụng train-clean-100 từ LibriTTS [29] và LJSpeech [30]. Chúng tôi ngẫu nhiên lấy mẫu 2316 phát âm từ hai tập dữ liệu để đánh giá các người nói đã thấy và để lại phần còn lại cho huấn luyện (khoảng 75 giờ). 3054 mẫu từ VCTK [31] được chọn để đánh giá các người nói chưa thấy.

Chi tiết thực hiện CNN trong SBP sử dụng Conv2D với kích thước kernel (3, 9). Các CNN trong mỗi Bộ Phân Biệt Phụ bao gồm một Conv2D với kích thước kernel (3, 8) và 32 kênh, ba Conv2D với tỷ lệ dãn [1, 2, 4] trong chiều thời gian và bước nhảy 2 trên chiều tần số, và một Conv2D với kích thước kernel (3, 3) và bước nhảy (1, 1). Đối với CQT, độ dài hop toàn cục được đặt thực nghiệm là 256, và dạng sóng sẽ được lấy mẫu tăng từ fs lên 2fs trước khi tính toán để tránh fmax của quãng tám cao nhất chạm vào Tần số Nyquist.

¹https://vocodexelysium.github.io/MS-SB-CQTD/

Các thước đo đánh giá Để đánh giá khách quan, chúng tôi sử dụng Đánh giá Cảm nhận Chất lượng Giọng nói (PESQ) [32] và Biến dạng Cepstral Mel (MCD) [33] để đánh giá việc tái tạo phổ đồ. Chúng tôi sử dụng Sai số Căn bậc hai Trung bình F0 (F0RMSE) và Hệ số Tương quan Pearson F0 (FPC) để đánh giá tính ổn định cao độ. Điểm Ý kiến Trung bình (MOS) và Kiểm tra Sở thích được sử dụng để đánh giá chủ quan. Chúng tôi mời 20 tình nguyện viên có kinh nghiệm trong lĩnh vực tạo âm thanh tham gia đánh giá chủ quan. Mỗi thiết lập trong kiểm tra MOS bên dưới đã được chấm điểm 200 lần, và mỗi cặp trong kiểm tra sở thích đã được chấm điểm 120 lần.

3.2. Hiệu quả của Bộ Phân Biệt MS-SB-CQT (EQ1 & EQ2)

Để xác minh hiệu quả của bộ phân biệt được đề xuất, chúng tôi lấy HiFi-GAN làm ví dụ và tăng cường nó với các bộ phân biệt khác nhau. Kết quả của phân tích-tổng hợp được minh họa trong Bảng 1. Về giọng hát, chúng ta có thể quan sát thấy rằng: (1) cả HiFi-GAN (+C) và HiFi-GAN (+S) đều hoạt động tốt hơn HiFi-GAN, cho thấy tầm quan trọng của các bộ phân biệt dựa trên biểu diễn thời gian-tần số [15]; (2) HiFi-GAN (+C) hoạt động tốt hơn HiFi-GAN (+S) với sự tăng đáng kể trong MOS, cho thấy tính ưu việt của Bộ Phân Biệt MS-SB-CQT được đề xuất của chúng tôi; (3) HiFi-GAN (+S+C) hoạt động tốt nhất cả về mặt khách quan và chủ quan, cho thấy các bộ phân biệt khác nhau sẽ có thông tin bổ sung cho nhau, xác nhận hiệu quả của huấn luyện chung. Một kết luận tương tự có thể được rút ra cho đánh giá người nói chưa thấy của dữ liệu giọng nói.

Để khám phá thêm những lợi ích cụ thể của việc sử dụng chung các bộ phân biệt dựa trên CQT và dựa trên STFT, chúng tôi đã tiến hành một nghiên cứu trường hợp (Hình 2). Đáng chú ý, trong các phần tần số cao được hiển thị, STFT có độ phân giải tần số tốt hơn, và CQT có độ phân giải thời gian tốt hơn. Về các phần tần số trong hình chữ nhật, có thể quan sát thấy rằng: (1) HiFi-GAN với Bộ Phân Biệt MS-STFT (Hình 2b) có thể tái tạo tần số của nó một cách chính xác nhưng không thể theo dõi các thay đổi do độ phân giải thời gian không đủ; (2) HiFi-GAN với Bộ Phân Biệt MS-SB-CQT (Hình 2c) có thể theo dõi các hài âm, nhưng việc tái tạo tần số không chính xác do độ phân giải tần số thấp; (3) Việc tích hợp hai bộ phân biệt này kết hợp điểm mạnh của chúng và do đó đạt được chất lượng tái tạo tốt hơn (Hình 2d).

3.3. Khả năng Tổng quát của Bộ Phân Biệt MS-SB-CQT (EQ3)

Để xác minh khả năng tổng quát của Bộ Phân Biệt MS-SB-CQT được đề xuất, ngoài HiFi-GAN, chúng tôi cũng tiến hành thí nghiệm dưới

--- TRANG 4 ---
Bảng 2: Kết quả phân tích-tổng hợp của Bộ Phân Biệt MS-SB-CQT được đề xuất của chúng tôi khi tích hợp trong MelGAN [6] và NSF-HiFiGAN trong các tập dữ liệu giọng hát. Các cải thiện được hiển thị in đậm. "S" và "C" đại diện cho Bộ Phân Biệt MS-STFT và MS-SB-CQT tương ứng. Tất cả các cải thiện trong MCD, PESQ, và Sở thích đều có ý nghĩa thống kê (p-value <0.01).

Hệ thống | MCD (↓) | PESQ (↑) | FPC (↑) | F0RMSE (↓) | Sở thích (↑)
--- | --- | --- | --- | --- | ---
| Đã thấy | Chưa thấy | Đã thấy | Chưa thấy | Đã thấy | Chưa thấy | Đã thấy | Chưa thấy | Đã thấy | Chưa thấy
Ground Truth | 0.00 | 0.00 | 4.50 | 4.50 | 1.000 | 1.000 | 0.00 | 0.00 | / | /
MelGAN | 4.44 | 5.21 | 2.23 | 2.15 | 0.968 | 0.964 | 46.80 | 51.73 | 8.47% | 27.45%
MelGAN (+S+C) | 4.08 | 4.87 | 2.35 | 2.23 | 0.960 | 0.962 | 51.78 | 50.99 | 91.53% | 72.55%
NSF-HiFiGAN | 1.73 | 2.04 | 3.95 | 3.88 | 0.985 | 0.980 | 25.62 | 31.17 | 41.67% | 29.41%
NSF-HiFiGAN (+S+C) | 1.48 | 1.72 | 3.98 | 3.91 | 0.979 | 0.983 | 24.01 | 31.19 | 58.33% | 70.59%

(a) Ground Truth
(b) HiFi-GAN (+S)
(c) HiFi-GAN (+C)
(d) HiFi-GAN (+S+C)

Hình 2: So sánh các phổ đồ mel của HiFi-GAN được tăng cường bởi các bộ phân biệt khác nhau. "S" và "C" đại diện cho Bộ Phân Biệt MS-STFT và MS-SB-CQT tương ứng. Tích hợp với cả bộ phân biệt dựa trên CQT và STFT, HiFi-GAN có thể đạt được chất lượng tổng hợp cao hơn với việc theo dõi hài âm và tái tạo tần số chính xác hơn.

MelGAN² [6] và NSF-HiFiGAN³. Lưu ý rằng NSF-HiFiGAN là một trong những vocoder tiên tiến nhất cho giọng hát [34]. Nó kết hợp bộ lọc nguồn thần kinh (NSF) [14] để tăng cường bộ sinh của HiFi-GAN. Kết quả thí nghiệm được trình bày trong Bảng 2.

Kết quả cho thấy rằng: (1) Nói chung, hiệu suất của MelGAN và NSF-HiFiGAN có thể được cải thiện đáng kể bằng cách huấn luyện chung với các Bộ Phân Biệt MS-SB-CQT và MS-STFT, với cả các kiểm tra khách quan và sở thích chủ quan đều xác nhận hiệu quả; (2) Cụ thể, MelGAN có xu hướng học quá khớp phần tần số thấp và bỏ qua các thành phần tần số trung và cao, dẫn đến tiếng ồn kim loại có thể nghe thấy. Sau khi thêm các Bộ Phân Biệt MS-STFT và MS-SB-CQT, nó có thể mô hình hóa thông tin toàn cục của phổ đồ tốt hơn⁴, mang lại MCD và PESQ tốt hơn đáng kể. Mặc dù các thước đo liên quan đến tần số thấp xấu đi, kiểm tra sở thích cho thấy

²https://github.com/descriptinc/melgan-neurips/
³https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts
⁴Chúng tôi hiển thị các trường hợp đại diện trên trang demo.

chất lượng tổng thể đã tăng đáng kể; NSF-HiFiGAN có thể tổng hợp giọng hát độ trung thực cao. Tuy nhiên, nó vẫn thiếu chi tiết tần số. Việc thêm các Bộ Phân Biệt MS-STFT và MS-SB-CQT giải quyết vấn đề đó⁴, làm cho các mẫu tổng hợp gần hơn với sự thật cơ sở. Kết quả chủ quan với tỷ lệ sở thích cao hơn cũng chứng minh hiệu quả.

3.4. Tính Cần thiết của Xử lý Tiểu Dải (EQ4)

Như được giới thiệu trong Mục 2.3, chúng tôi đề xuất mô-đun Xử lý Tiểu Dải để có được các biểu diễn ẩn CQT được đồng bộ hóa về mặt thời gian. Để xác minh tính cần thiết của nó, chúng tôi tiến hành một nghiên cứu loại bỏ để loại bỏ mô-đun SBP khỏi Bộ Phân Biệt MS-SB-CQT được đề xuất. Chúng tôi áp dụng Opencpop [25] làm tập dữ liệu thí nghiệm. Chúng tôi ngẫu nhiên chọn 221 phát âm để đánh giá và phần còn lại để huấn luyện (khoảng 5 giờ).

Bảng 3: Kết quả phân tích-tổng hợp của HiFi-GAN được tăng cường bởi các bộ phân biệt dựa trên CQT khác nhau. Bộ Phân Biệt MS-CQT đại diện cho một bộ phân biệt chỉ loại bỏ mô-đun Xử lý Tiểu Dải khỏi Bộ Phân Biệt MS-SB-CQT được đề xuất của chúng tôi.

Hệ thống | MCD (↓) | PESQ (↑) | FPC (↑) | F0RMSE (↓)
--- | --- | --- | --- | ---
HiFi-GAN | 3.443 | 2.960 | 0.972 | 40.409
+ MS-CQT | 3.502 | 2.932 | 0.964 | 50.918
+ MS-SB-CQT | 3.263 | 2.985 | 0.986 | 28.313

Trong Bảng 3, chúng ta có thể thấy rằng HiFi-GAN có thể được tăng cường thành công bởi Bộ Phân Biệt MS-SB-CQT được đề xuất của chúng tôi. Tuy nhiên, chỉ áp dụng CQT thô cho bộ phân biệt (MS-CQT) thậm chí còn có thể làm hại chất lượng của HiFi-GAN. Chúng tôi suy đoán điều này là do sự không đồng bộ về mặt thời gian trong các liên quãng tám của CQT thô sẽ làm gánh nặng cho việc học mô hình. Do đó, cần thiết phải áp dụng mô-đun SBP được đề xuất để thiết kế bộ phân biệt dựa trên CQT.

4. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Nghiên cứu này đề xuất Bộ Phân Biệt Biến đổi Constant-Q Tiểu Dải Đa Thang Độ (MS-SB-CQT) cho vocoder dựa trên GAN. Bộ phân biệt được đề xuất vượt trội hơn Bộ Phân Biệt Biến đổi Fourier Thời gian Ngắn Đa Thang Độ (MS-STFT) hiện có trên cả giọng nói và giọng hát. Bên cạnh đó, bộ phân biệt dựa trên CQT được đề xuất có thể bổ sung cho bộ phân biệt dựa trên STFT hiện có để cải thiện vocoder hơn nữa. Trong công việc tương lai, chúng tôi sẽ khám phá thêm các biểu diễn thời gian-tần số và các phương pháp xử lý tín hiệu khác cho các bộ phân biệt hoặc bộ sinh tốt hơn.

--- TRANG 5 ---
5. TÀI LIỆU THAM KHẢO

[1] A ¨aron van den Oord, et al., "Wavenet: A generative model for raw audio," in SSW. 2016, p. 125, ISCA.

[2] Nal Kalchbrenner, et al., "Efficient neural audio synthesis," in ICML. 2018, vol. 80, pp. 2415–2424, PMLR.

[3] Ryan Prenger, et al., "Waveglow: A flow-based generative network for speech synthesis," in ICASSP. 2019, pp. 3617–3621, IEEE.

[4] Wei Ping, et al., "Waveflow: A compact flow-based model for raw audio," in ICML. 2020, vol. 119, pp. 7706–7716, PMLR.

[5] Ryuichi Yamamoto, et al., "Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram," in ICASSP. 2020, pp. 6199–6203, IEEE.

[6] Kundan Kumar, et al., "Melgan: Generative adversarial networks for conditional waveform synthesis," in NeurIPS, 2019, pp. 14881–14892.

[7] Won Jang, et al., "Universal melgan: A robust neural vocoder for high-fidelity waveform generation in multiple domains," arXiv, vol. abs/2011.09631, 2020.

[8] Jiaqi Su, et al., "Hifi-gan: High-fidelity denoising and dereverberation based on speech deep features in adversarial networks," in INTERSPEECH. 2020, pp. 4506–4510, ISCA.

[9] Ji-Hoon Kim, et al., "Fre-gan: Adversarial frequency-consistent audio synthesis," in INTERSPEECH. 2021, pp. 2197–2201, ISCA.

[10] Rongjie Huang, et al., "Singgan: Generative adversarial network for high-fidelity singing voice generation," in ACM Multimedia. 2022, pp. 2525–2535, ACM.

[11] Sang-gil Lee, et al., "Bigvgan: A universal neural vocoder with large-scale training," in ICLR. 2023, OpenReview.net.

[12] Nanxin Chen, et al., "Wavegrad: Estimating gradients for waveform generation," in ICLR. 2021, OpenReview.net.

[13] Zhifeng Kong, et al., "Diffwave: A versatile diffusion model for audio synthesis," in ICLR. 2021, OpenReview.net.

[14] Xin Wang, et al., "Neural source-filter-based waveform model for statistical parametric speech synthesis," in ICASSP. 2019, pp. 5916–5920, IEEE.

[15] Jaeseong You, et al., "GAN vocoder: Multi-resolution discriminator is all you need," in INTERSPEECH. 2021, pp. 2177–2181, ISCA.

[16] Alexandre D ´efossez, et al., "High fidelity neural audio compression," arXiv, vol. abs/2210.13438, 2022.

[17] Rongjie Huang, et al., "Multi-singer: Fast multi-singer singing voice vocoder with A large-scale corpus," in ACM Multimedia. 2021, pp. 3945–3954, ACM.

[18] Judith C. Brown and Miller Puckette, "An efficient algorithm for the calculation of a constant q transform," Journal of the Acoustical Society of America, vol. 92, pp. 2698–2701, 1992.

[19] Yizhi Li, et al., "MERT: acoustic music understanding model with large-scale self-supervised training," arXiv, vol. abs/2306.00107, 2023.

[20] Christian Sch ¨orkhuber and Anssi Klapuri, "Constant-q transform toolbox for music processing," in Sound and Music Computing Conference, 2010, pp. 3–64.

[21] Brian McFee, et al., "librosa: Audio and music signal analysis in python," in SciPy. 2015, pp. 18–24, scipy.org.

[22] Kin Wai Cheuk, et al., "nnaudio: An on-the-fly GPU audio to spectrogram conversion toolbox using 1d convolutional neural networks," IEEE Access, vol. 8, pp. 161981–162003, 2020.

[23] Lichao Zhang, et al., "M4singer: A multi-style, multi-singer and musical score provided mandarin singing corpus," in NeurIPS, 2022.

[24] Junya Koguchi, et al., "PJS: phoneme-balanced japanese singing-voice corpus," in APSIPA. 2020, pp. 487–491, IEEE.

[25] Yu Wang, et al., "Opencpop: A high-quality open source chinese popular song corpus for singing voice synthesis," in INTERSPEECH. 2022, pp. 4242–4246, ISCA.

[26] Jinglin Liu, et al., "Diffsinger: Singing voice synthesis via shallow diffusion mechanism," in AAAI. 2022, pp. 11020–11028, AAAI Press.

[27] Rongjie Huang, et al., "Multi-singer: Fast multi-singer singing voice vocoder with A large-scale corpus," in ACM Multimedia. 2021, pp. 3945–3954, ACM.

[28] Soonbeom Choi, et al., "Children's song dataset for singing voice research," in ISMIR, 2020.

[29] Heiga Zen, et al., "Libritts: A corpus derived from librispeech for text-to-speech," in INTERSPEECH. 2019, pp. 1526–1530, ISCA.

[30] Keith Ito and Linda Johnson, "The lj speech dataset," https://keithito.com/LJ-Speech-Dataset/, 2017.

[31] Junichi Yamagishi, et al., "Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92)," University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2019.

[32] Antony W. Rix, et al., "Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs," in ICASSP. 2001, pp. 749–752, IEEE.

[33] Robert Kubichek, "Mel-cepstral distance measure for objective speech quality assessment," in Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing. IEEE, 1993, vol. 1, pp. 125–128.

[34] Wen-Chin Huang, et al., "The singing voice conversion challenge 2023," arXiv, vol. abs/2306.14422, 2023.
