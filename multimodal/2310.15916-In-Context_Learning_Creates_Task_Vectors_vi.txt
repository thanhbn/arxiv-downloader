# 2310.15916.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.15916.pdf
# Kích thước tệp: 1643396 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Học trong ngữ cảnh tạo ra các vector nhiệm vụ
Roee Hendel
Đại học Tel Aviv
roee.hendel@mail.tau.ac.il Mor Geva
Google DeepMind
pipek@google.com Amir Globerson
Đại học Tel Aviv, Google
gamir@tauex.tau.ac.il

Tóm tắt
Học trong ngữ cảnh (ICL) trong các mô hình ngôn ngữ lớn (LLM) đã nổi lên như một mô hình học tập mạnh mẽ mới. Tuy nhiên, cơ chế cơ bản của nó vẫn chưa được hiểu rõ. Đặc biệt, việc ánh xạ nó vào khung "tiêu chuẩn" của học máy là một thách thức, nơi người ta sử dụng tập huấn luyện S để tìm hàm phù hợp nhất f(x) trong một lớp giả thuyết nào đó. Ở đây chúng tôi đạt được tiến bộ trong vấn đề này bằng cách cho thấy rằng các hàm được học bởi ICL thường có cấu trúc rất đơn giản: chúng tương ứng với transformer LLM có đầu vào duy nhất là truy vấn x và một "vector nhiệm vụ" đơn lẻ được tính toán từ tập huấn luyện. Do đó, ICL có thể được xem như việc nén S thành một vector nhiệm vụ đơn lẻ θ(S) và sau đó sử dụng vector nhiệm vụ này để điều chỉnh transformer để tạo ra đầu ra. Chúng tôi hỗ trợ khẳng định trên thông qua các thí nghiệm toàn diện trên nhiều mô hình và nhiệm vụ khác nhau.

1 Giới thiệu
Các mô hình ngôn ngữ lớn đã cải thiện đáng kể trong vài năm qua. Một tính chất nổi bật của các mô hình này là chúng có thể học các quy tắc mới từ rất ít minh họa. Ví dụ, một mô hình có thể được gợi ý với đầu vào "Apple → Red, Lime → Green, Corn →" và tạo ra đầu ra "Yellow". Mô hình đã học được một ánh xạ dựa trên chỉ hai ví dụ, và có thể áp dụng chính xác cho các ví dụ mới. Khả năng này, được gọi là Học trong ngữ cảnh (ICL), đã được sử dụng rộng rãi, mang lại kết quả thực nghiệm ấn tượng (Brown et al., 2020; Liu et al., 2023; Dong et al., 2022).

Với thành công này, việc hỏi cơ chế cơ bản đằng sau ICL là gì là điều tự nhiên. Cụ thể, mô hình sử dụng các minh họa S và truy vấn x như thế nào để tạo ra đầu ra yêu cầu? Ở đây chúng tôi tiếp cận câu hỏi này bằng cách sử dụng khái niệm lớp giả thuyết từ lý thuyết học thống kê (Shalev-Shwartz và Ben-David, 2014).

Trong công thức hóa lý thuyết học, người ta thường xem xét một lớp giả thuyết H, trong đó mỗi phần tử của H là một hàm h(x;θ), hoạt động trên đầu vào x, và được chỉ định bởi một vector tham số θ. Ví dụ, nếu x∈Rᵈ thì lớp H có thể là tập hợp các bộ phân loại tuyến tính, được định nghĩa bởi một vector hệ số θ như h(x;θ) = θ·x. Các thuật toán học tìm kiếm một phần tử h∈H phù hợp tốt với tập huấn luyện. Điều này được biết đến như Tối thiểu hóa rủi ro thực nghiệm.

Không rõ liệu ICL có hoạt động theo cách như vậy hay không vì việc dự đoán được thực hiện thông qua T([S, x]), trong đó T thường là một transformer tự hồi quy

--- TRANG 2 ---
và [S, x] là một nối của các token trong S và x. Do đó, trong trường hợp tổng quát, nó có thể là một hàm tùy ý hoạt động trên S và x để tạo ra đầu ra. Điều này có thể bao gồm các phương pháp "phi tham số" như láng giềng gần nhất. Nghiên cứu gần đây đã bắt đầu khám phá câu hỏi này. Ví dụ, đã được chỉ ra rằng khi huấn luyện transformer từ đầu để thực hiện hồi quy tuyến tính trong ngữ cảnh, thuật toán học nổi lên tương tự như Gradient Descent ngẫu nhiên (Akyürek et al., 2022; von Oswald et al., 2022). Tuy nhiên, đối với LLM thực hiện các nhiệm vụ ngôn ngữ tự nhiên phức tạp hơn, không rõ không gian giả thuyết có thể là gì.

Trong công trình này, chúng tôi cho thấy rằng trên một loạt các nhiệm vụ rộng, ICL trong LLM có thể được xem là làm việc trên một không gian giả thuyết rất tự nhiên. Chúng tôi lập luận rằng, cho một tập huấn luyện S, transformer ánh xạ nó thành một "vector nhiệm vụ" θ(S) về cơ bản đại diện cho ánh xạ/quy tắc được mô tả trong S. Cụ thể, cho transformer T và một vector θ, chúng ta có thể xây dựng một hàm mới f(x;θ) triển khai nhiệm vụ. Hàm f rất tương tự như transformer gốc được áp dụng cho x mà không có minh họa nhưng thay vào đó được điều chỉnh bởi θ (xem Hình 2).

Quan điểm của chúng tôi cũng liên quan đến soft prompts (Lester et al., 2021), vì cả hai phương pháp đều điều chỉnh chức năng của transformer hướng tới một nhiệm vụ cụ thể. Tuy nhiên, trong ICL, các vector nhiệm vụ được tính toán trong lượt truyền xuôi thay vì được tinh chỉnh.

Các đóng góp của chúng tôi bao gồm đề xuất một góc nhìn cơ học dựa trên lớp giả thuyết về ICL, và tiến hành thí nghiệm để xác thực quan điểm của chúng tôi trên một loạt LLM có sẵn công khai và một tập hợp đa dạng các nhiệm vụ. Kết quả của chúng tôi tiến xa hơn trong việc hiểu ICL và có thể có ý nghĩa thực tiễn cho việc thích ứng hiệu quả của LLM để thực hiện các nhiệm vụ cụ thể.

2 Một quan điểm lớp giả thuyết về ICL
Được thúc đẩy bởi quan điểm lớp giả thuyết của lý thuyết học, mục tiêu của chúng tôi là hiểu xem ICL có ánh xạ tập hợp các minh họa S thành một hàm trên truy vấn x hay không và ánh xạ này xảy ra như thế nào. Cụ thể, chúng tôi tìm cách xem ICL có chuyển đổi S thành θ - "tham số" của một hàm trong một không gian giả thuyết nhất định hay không. Các phát hiện thực nghiệm của chúng tôi cho thấy quan điểm này có thể áp dụng được, làm sáng tỏ cấu trúc của không gian giả thuyết mà ICL có thể được xem là hoạt động trên.

2.1 Khung lý thuyết
Chúng tôi sử dụng T để biểu thị một transformer LLM chỉ giải mã, S để biểu thị tập hợp các minh họa (tức là các ví dụ huấn luyện) được sử dụng làm đầu vào cho ICL, và x để biểu thị truy vấn mà ICL được yêu cầu cung cấp đầu ra. Chúng tôi sử dụng T([S, x]) để biểu thị đầu ra của ICL trên việc nối S và x.

Để chứng minh rằng ICL hoạt động trong một không gian giả thuyết, chúng tôi nhằm mục đích chỉ ra rằng cơ chế cơ bản của nó có thể được chia thành hai phần:
• Một "Thuật toán học" (được ký hiệu bởi A) ánh xạ S thành một "vector nhiệm vụ" θ, độc lập với truy vấn x. Cho rằng các lớp attention có thể truy cập cả S và x, sự độc lập này không tầm thường.
• Một "Áp dụng quy tắc" (được ký hiệu bởi f) ánh xạ truy vấn x thành đầu ra, dựa trên θ ≡ A(S), mà không phụ thuộc trực tiếp vào S. Một lần nữa, sự độc lập này không tầm thường.

Do đó, chúng tôi xem xét ánh xạ sau từ một tập hợp các minh họa và một truy vấn đến đầu ra dự đoán: T([S, x]) = f(x;A(S)).

Nếu chúng ta có thể chia lượt truyền xuôi của LLM thành hai thành phần trên, chúng ta có thể xem ICL là hoạt động trên lớp giả thuyết sau: H = {f(·;θ)|θ}. Trong phần tiếp theo, chúng tôi đề xuất một triển khai của lớp như vậy.

2.2 Một lớp giả thuyết đề xuất
Có nhiều cách hiện thực có thể của khung trên, tương ứng với các lựa chọn khác nhau của A và f. Tiếp theo chúng tôi mô tả cách hiện thực mà chúng tôi tập trung vào, tự nhiên theo từ kiến trúc transformer. Chúng tôi xem xét một thiết lập ICL như trong Hình 1, nơi đầu vào kết thúc bằng một truy vấn x (tức là, Corn) theo sau bởi một ký hiệu "→". Như đã đề cập ở trên, chúng tôi xem việc học được cấu thành từ hai bước: tính toán một vector tham số θ dựa trên mẫu huấn luyện S, và áp dụng quy tắc được định nghĩa bởi vector tham số này cho truy vấn x. Một cách có lẽ đơn giản để transformer làm điều này là để L lớp đầu tiên của biểu diễn → tính toán θ và sau đó để các lớp còn lại lấy θ và x làm đầu vào và tạo ra đầu ra. Xem Hình 1. Nhớ rằng S và x có thể truy cập được bởi transformer ở bất kỳ lớp nào, tạo ra thách thức với quan điểm của chúng tôi.

Trong các phần tiếp theo, chúng tôi giải quyết thách thức này và trình bày các thí nghiệm xác thực quan điểm của chúng tôi. Cụ thể, chúng tôi chỉ ra rằng chúng ta có thể tách biệt A và f đề xuất trong lượt truyền xuôi của LLM thực hiện ICL. Chúng tôi cũng chỉ ra rằng các vector θ có thể diễn giải và tương ứng với các nhiệm vụ đã học.

--- TRANG 3 ---
(a) (b)
Apple → Red Lime → Green Plum → Corn →
Minh họa (S) Mô hình ngôn ngữ lớn (T) Yellow
Truy vấn (x) x' A f
"Color" θ θ(T)

Hình 2: Tách biệt A và f. Để làm cho θ độc lập với truy vấn x, chúng tôi sử dụng một truy vấn giả (x' = Plum) và sử dụng biểu diễn của → ở lớp thứ L làm θ. Vector θ sau đó được vá vào cùng lớp trong một lượt truyền xuôi của transformer chỉ nhận x và → làm đầu vào, để ngăn chặn sự phụ thuộc trực tiếp của f vào S.

3 Tính hợp lệ của quan điểm lớp giả thuyết
Đầu tiên chúng tôi chỉ ra rằng việc tách lượt truyền xuôi thành hai thành phần riêng biệt A và f, được định nghĩa trong §2.2, duy trì độ chính xác cao của ICL.

3.1 Tách biệt A và f
Chúng tôi đối mặt với một số thách thức trong một lượt truyền xuôi thông thường: thứ nhất, L lớp đầu tiên tương ứng với A, cập nhật biểu diễn của → để tạo θ, có thể chú ý đến truy vấn x. Do đó, chúng có thể phụ thuộc vào x, tạo ra sự phụ thuộc không mong muốn của θ vào x. Thứ hai, các lớp còn lại tương ứng với f, có thể truy cập trực tiếp S, thay vì chỉ sử dụng x và θ.

Chúng tôi đề xuất quy trình sau để giải quyết những thách thức này: để giải quyết vấn đề đầu tiên, chúng tôi giới thiệu một "truy vấn giả" x' và tính toán biểu diễn của → sử dụng truy vấn đó. Chúng tôi sử dụng biểu diễn của → sau L lớp đầu tiên, được tính toán bằng x', làm vector θ (như được minh họa ở phía bên trái của Hình 2). Một phương án khác là chặn attention đến x, nhưng nó dẫn đến hiệu suất kém. Để giải quyết vấn đề thứ hai của việc tính toán f(x,θ) mà không cho phép sự phụ thuộc trực tiếp vào S, chúng tôi thực hiện một lượt truyền xuôi của transformer chỉ trên x và →, và "vá" θ mà chúng tôi đã trích xuất trước đó ở lớp thứ L của → (phía bên phải của Hình 2).

3.2 Nhiệm vụ và mô hình
Nhiệm vụ Chúng tôi xem xét một tập hợp đa dạng 18 nhiệm vụ trên 4 loại: thuật toán, dịch thuật, ngôn ngữ học, và kiến thức thực tế. Để đơn giản, chúng tôi giới hạn mình trong các đầu ra token đơn. Một tập hợp con đại diện của các nhiệm vụ được mô tả trong Bảng 1. Bảng chi tiết đầy đủ, cũng như thêm thông tin về dữ liệu, được cung cấp trong § A.1.

Mô hình Chúng tôi sử dụng nhiều LLM mở: LLaMA 7B, 13B, và 30B (Touvron et al., 2023), GPT-J 6B (Wang và Komatsuzaki, 2021), và Pythia 2.8B, 6.9B, và 12B (Biderman et al., 2023).

3.3 Tìm L
Cơ chế chúng tôi mô tả trong §2.2 có một tham số tự do - lớp L nơi A kết thúc và f bắt đầu. Chúng tôi sử dụng việc triển khai (A, f) đề xuất cho các lựa chọn khác nhau của L và đánh giá độ chính xác trên một tập phát triển để tìm lớp tốt nhất.

Hình 3 cho thấy độ chính xác trên tập phát triển, cho các lựa chọn khác nhau của L. Chúng tôi tập trung ở đây vào các mô hình LLaMA và bao gồm phần còn lại trong § A.2. Thú vị là, tất cả các mô hình đều thể hiện đỉnh hiệu suất tại một lớp trung gian tương tự, bất kể sự khác biệt về tham số và số lượng lớp.

--- TRANG 4 ---
Hình 4: Độ chính xác trung bình trên tất cả các nhiệm vụ cho mỗi mô hình, sử dụng mỗi trong ba quy trình: Baseline, Regular và Hypothesis.

3.4 Độ chính xác của dự đoán dựa trên giả thuyết
Tiếp theo chúng tôi so sánh độ chính xác của cơ chế (A, f) với cơ chế của một lượt truyền xuôi thông thường thực hiện ICL. Đối với mỗi mô hình và nhiệm vụ, chúng tôi đánh giá ba quy trình sau:
• Regular Một ứng dụng của LLM cho các minh họa S và truy vấn x. Cụ thể là T([S, x]), như trong ICL thông thường.
• Hypothesis Quy trình đề xuất của chúng tôi từ § 3.1 trong đó A tạo ra θ sử dụng một x' giả, và f(·;θ) được áp dụng cho x bằng cách chạy transformer trên [x,→] với θ được vá ở lớp L của →.
• Baseline Một lượt truyền xuôi của LLM chỉ trên x, không có minh họa S. Tức là, T([x,→]). Điều này giống như việc áp dụng f từ quy trình tách biệt của chúng tôi, nhưng không vá θ.

Hình 4 cho thấy độ chính xác trung bình trên tất cả các nhiệm vụ của 3 quy trình này, cho mỗi mô hình. Kết quả đầy đủ được báo cáo trong Bảng 6 trong § A.2. Trên tất cả các mô hình, quy trình của chúng tôi duy trì khoảng 80-90% độ chính xác của ICL thông thường, trong khi baseline chỉ đạt 10-20%. Điều này cho thấy rằng việc tách biệt đề xuất của chúng tôi thành A và f cung cấp một xấp xỉ thực nghiệm tốt của quá trình cơ bản của ICL.

4 Tính mạnh mẽ của các vector nhiệm vụ
Trong thiết lập của chúng tôi, θ được dẫn xuất từ S và một truy vấn giả x'. Việc kiểm tra tính mạnh mẽ của θ đối với các biến thể trong những đầu vào này là điều tự nhiên. Trực giác, nếu nó đại diện cho nhiệm vụ, nó nên ổn định trên các giá trị S và x' khác nhau.

Hình 5: Một biểu đồ t-SNE của các vector nhiệm vụ. Một biểu đồ t-SNE 2D trực quan hóa 50 vector nhiệm vụ cho mỗi nhiệm vụ, mỗi vector được tạo ra từ một lựa chọn khác nhau của S và x' sử dụng LLaMA 7B. Các điểm được mã hóa màu theo nhiệm vụ. Mỗi nhiệm vụ có thể được thấy tạo thành cụm riêng biệt.

Để kiểm tra điều này, chúng tôi sử dụng LLaMA 7B để tạo ra 50 vector nhiệm vụ cho mỗi nhiệm vụ với S và x' khác nhau và tiến hành hai phân tích.

Hình học của θ Một phép giảm chiều t-SNE (Hình 5) cho thấy các vector nhiệm vụ tạo thành các cụm riêng biệt, mỗi cụm chứa các vector nhiệm vụ của một nhiệm vụ duy nhất. Hình 9 tiếp tục cho thấy sự gần gũi giữa các nhiệm vụ của cùng một loại, củng cố ý tưởng rằng chúng đóng gói sự hiểu biết về nhiệm vụ.

Tính biến đổi của θ Hình 8 cho thấy biểu đồ của khoảng cách trong và giữa các nhiệm vụ. Có thể thấy rằng các vector trong cùng một nhiệm vụ gần nhau hơn so với những vector giữa các nhiệm vụ khác nhau, chỉ ra rằng θ ổn định trong các nhiệm vụ và không bị ảnh hưởng nhiều bởi x' hoặc S.

5 Sự chi phối của việc vá θ
Trong §3 chúng tôi ngăn chặn f truy cập trực tiếp S. Tuy nhiên, trong một lượt truyền xuôi thông thường trong ICL, token cuối cùng có thể chú ý đến S. Ở đây chúng tôi xác minh rằng ngay cả trong trường hợp này, f chủ yếu sử dụng vector nhiệm vụ θ, mà không truy cập trực tiếp các minh họa S. Để làm điều này, chúng tôi sử dụng một cặp nhiệm vụ, A và B, chia sẻ không gian đầu vào nhưng khác nhau về đầu ra. Đầu tiên chúng tôi sử dụng một lượt truyền xuôi "Regular", nơi chúng tôi cung cấp cho mô hình các minh họa S cho nhiệm vụ A (ký hiệu SA), để xác minh mô hình có thể thực hiện nhiệm vụ này bằng ICL. Sau đó, chúng tôi thực hiện một lượt truyền xuôi "Conflicting", vẫn cung cấp SA, trong khi tiêm θB. Để biết thêm chi tiết, tham khảo Hình 6 trong §A.1.

--- TRANG 5 ---
Bảng 2: Kết quả thí nghiệm nhiệm vụ xung đột. Độ chính xác của mô hình trên nhiệm vụ liên quan (A trong "Regular" và B trong "Conflicting") được hiển thị cho cả hai tình huống.

Nhiệm vụ A (S) | Nhiệm vụ B (θ) | Regular | Conflicting
Nhiệm vụ A | Nhiệm vụ B
Next Letter | To Upper | 0.92 | 0.77
List Last | List First | 0.95 | 0.78
Present to Past | to Gerund | 0.96 | 0.95

Trong Bảng 2, lượt truyền xuôi "Regular" cho thấy độ chính xác cao trên nhiệm vụ A (90%+), như dự đoán. Tuy nhiên, lượt truyền xuôi "Conflicting" mang lại độ chính xác cao trên nhiệm vụ B, tương ứng với vector nhiệm vụ θ được tiêm. Điều này hàm ý rằng mô hình chủ yếu dựa vào θ, phần lớn bỏ qua các minh họa S cho nhiệm vụ A. Chúng tôi lưu ý rằng độ chính xác trên nhiệm vụ B hơi thấp, có thể phù hợp với sự sụt giảm hiệu suất thấy trong Hình 6, và có thể bị ảnh hưởng thêm bởi sự hiện diện của S.

6 Diễn giải θ
Vector đã học θ trực giác nắm bắt thông tin về nhiệm vụ được minh họa bởi S. Ở đây chúng tôi cung cấp bằng chứng hỗ trợ cách diễn giải này. Vì θ là một trạng thái ẩn trung gian của transformer, chúng tôi có thể sử dụng phương pháp chiếu từ vựng (nostalgebraist, 2020; Dar et al., 2022). Cụ thể, chúng tôi kiểm tra các token hàng đầu trong phân phối trên từ vựng được tạo ra bởi trạng thái ẩn.

Bảng 3 cho thấy các token hàng đầu cho ba nhiệm vụ cho LLaMA 13B (thêm mô hình và nhiệm vụ được cung cấp trong Bảng 7 trong §A). Trong nhiều trường hợp, chúng tôi quan sát các token mô tả trực tiếp nhiệm vụ. Quan trọng là, những thuật ngữ này không bao giờ xuất hiện một cách rõ ràng trong ngữ cảnh. Ví dụ trong nhiệm vụ dịch từ tiếng Pháp sang tiếng Anh, chúng tôi quan sát các token như "English" và "translate". Điều này hỗ trợ quan điểm của chúng tôi rằng θ mang thông tin ngữ nghĩa quan trọng, không tầm thường về nhiệm vụ.

7 Nghiên cứu liên quan
Sự nổi lên của ICL Một câu hỏi quan trọng với ICL là nó nổi lên như một khả năng từ việc tiền huấn luyện LLM như thế nào. Levine et al. (2022) cung cấp kết quả theo hướng này nhấn mạnh tầm quan trọng của cấu trúc dữ liệu huấn luyện. Xie et al. sử dụng phân tích xác suất và mô hình hóa dữ liệu tiền huấn luyện sử dụng Mô hình Markov ẩn để giải thích lý thuyết sự nổi lên của ICL, trong khi Chan et al. (2022) khám phá thực nghiệm tác động của một số thuộc tính phân phối của dữ liệu tiền huấn luyện.

Meta-Learning trong Transformer Các nghiên cứu của Akyürek et al. (2022); von Oswald et al. (2022); Garg et al. tập trung vào khả năng meta-learning của transformer. Họ thường huấn luyện các mô hình từ đầu trên các nhiệm vụ cơ bản như hồi quy tuyến tính, rút ra những tương đồng lý thuyết với các thuật toán như Gradient Descent và chứng minh cách transformer có thể triển khai chúng. Một giả định quan trọng của những công trình này là một không gian tham số đã biết mà trong đó gradient descent hoạt động. Công trình của chúng tôi tập trung vào việc xác định một không gian tham số như vậy cho LLM.

ICL trong LLM Olsson et al. (2022) xác định "induction heads" trong transformer như một cơ chế chính có thể của ICL. Dai et al. (2022) cung cấp bằng chứng thực nghiệm cho mối liên hệ của ICL với Gradient Descent trong LLM, tập trung vào các nhiệm vụ phân loại. Công trình đồng thời của Merullo et al. (2023) cũng khám phá một hiện tượng tương tự với các vector nhiệm vụ mà chúng tôi nghiên cứu ở đây, nơi một vector đơn có thể mã hóa các hàm đã học. Các phát hiện của chúng tôi bổ sung cho của họ, và công trình tương lai có thể khám phá mối quan hệ giữa hai cái một cách chặt chẽ hơn.

8 Kết luận
Thông qua việc khám phá ICL trong LLM này, chúng tôi đã làm sáng tỏ một góc nhìn mới về cơ chế học ICL. Chúng tôi đã tiết lộ một cấu trúc đơn giản và tinh tế: ICL hoạt động bằng cách nén một tập huấn luyện đã cho thành một vector nhiệm vụ đơn, sau đó hướng dẫn transformer tạo ra đầu ra thích hợp cho các truy vấn. Công trình của chúng tôi cung cấp một bước đệm hướng tới việc hiểu cách LLM thực hiện ICL. Trong ánh sáng của các phát hiện của chúng tôi, công trình tương lai có thể tập trung vào việc hiểu cách vector nhiệm vụ được xây dựng cũng như cách nó được sử dụng để tính toán đầu ra.

--- TRANG 6 ---
Hạn chế
Chúng tôi nghiên cứu các nhiệm vụ tương đối đơn giản, trong khi ICL có thể học thực hiện các nhiệm vụ phức tạp hơn, chẳng hạn như giải quyết các vấn đề lý luận số học. Vẫn cần xem liệu và làm thế nào các cơ chế chúng tôi quan sát ở đây sẽ chuyển đổi sang những trường hợp này. Ví dụ, phương pháp của chúng tôi tập trung vào các trường hợp mà một vector nhiệm vụ đơn là đủ, trong khi các trường hợp ICL phức tạp hơn có thể yêu cầu tham số hóa phức tạp hơn. Chúng tôi cũng tập trung vào các nhiệm vụ có đầu ra là một token đơn, trong khi một số nhiệm vụ khác yêu cầu đầu ra nhiều token.

Cuối cùng, như đã lưu ý ở trên, chúng tôi không cung cấp giải thích cơ học về cách vector nhiệm vụ được hình thành hoặc cách nó được sử dụng. Cụ thể, chúng tôi không giải thích cách transformer thực hiện các tính toán này sử dụng các tham số của nó.

Lời cảm ơn
Dự án này được tài trợ bởi Hội đồng Nghiên cứu Châu Âu (ERC) trong khuôn khổ chương trình nghiên cứu và đổi mới Horizon 2020 của Liên minh Châu Âu (grant ERC HOLI 819080).

Tài liệu tham khảo
[Các tài liệu tham khảo được liệt kê theo định dạng ban đầu]

--- TRANG 7 ---
Ben Wang và Aran Komatsuzaki. 2021. GPT-J-6B: Một mô hình ngôn ngữ tự hồi quy 6 tỷ tham số. https://github.com/kingoflolz/mesh-transformer-jax.

Sang Michael Xie, Aditi Raghunathan, Percy Liang, và Tengyu Ma. Một giải thích về học trong ngữ cảnh như suy luận Bayesian ngầm. Trong Hội nghị quốc tế về biểu diễn học.

--- TRANG 8 ---
A Phụ lục
Ở đây chúng tôi cung cấp các chi tiết và kết quả bổ sung.

A.1 Chi tiết bổ sung
Mô tả nhiệm vụ đầy đủ Nghiên cứu của chúng tôi bao gồm 18 nhiệm vụ trong 4 loại: Thuật toán, Dịch thuật, Ngôn ngữ học và Kiến thức. Mô tả chi tiết của tất cả các nhiệm vụ được cung cấp trong Bảng 5.

Chi tiết mô hình Thêm chi tiết về các mô hình được sử dụng trong nghiên cứu được cung cấp trong Bảng 4.

Dữ liệu nhiệm vụ Ở đây chúng tôi chi tiết các nguồn dữ liệu cho mỗi nhiệm vụ. Kho GitHub đi kèm chứa bản thân dữ liệu cũng như mã được sử dụng để tạo ra nó.

• Thuật toán: Được tạo ra bằng chương trình.
• Dịch thuật: Đối với mỗi cặp ngôn ngữ, các từ thường gặp nhất trong ngôn ngữ nguồn được truy xuất đầu tiên từ https://github.com/frekwencja/most-common-words-multilingual và sau đó được dịch sang ngôn ngữ đích sử dụng gói mã nguồn mở nltk.
• Ngôn ngữ học: Dữ liệu cho các nhiệm vụ thì được phân tích từ https://github.com/Drulac/English-Verbs-Conjugates. Dữ liệu cho nhiệm vụ số nhiều-số ít được lấy từ https://github.com/sindresorhus/irregular-plurals. Cuối cùng, dữ liệu cho nhiệm vụ từ trái nghĩa được lấy từ https://github.com/SuzanaK/english_synonyms_antonyms_list.
• Kiến thức: Dữ liệu cho các nhiệm vụ kiến thức được lấy từ tập dữ liệu phản thực tế được giới thiệu trong (Meng et al., 2022).

Thí nghiệm nhiệm vụ xung đột Trong Hình 6, chúng tôi cung cấp thêm chi tiết và hình ảnh của thí nghiệm được mô tả trong §5.

A.2 Kết quả bổ sung
Tìm A và f Hình 7 cho thấy kết quả tương tự như Hình 3, nhưng cho các mô hình khác. Thú vị là quan sát rằng các đường cong tương tự nhau trên các mô hình có kích thước khác nhau.

Kết quả chi tiết cho Hình 4. Hình 4 trình bày kết quả cho phương pháp dựa trên giả thuyết (A, f) của chúng tôi, được lấy trung bình trên các nhiệm vụ. Bảng 6 cung cấp những kết quả này cho tất cả các nhiệm vụ cụ thể được xem xét.

Sự phụ thuộc của A vào x Hình 9 và Hình 8 cung cấp thêm kết quả về hình học của các vector θ (xem văn bản chính để thảo luận).

Kiểm tra các vector nhiệm vụ Bảng 7 là phiên bản mở rộng của Bảng 3, cung cấp thêm các phép chiếu từ vựng của θ cho các nhiệm vụ bổ sung và trên nhiều LLM.

Bảng 4: Các mô hình được sử dụng trong nghiên cứu, với thông tin kiến trúc.

Mô hình | Tham số | Chiều | Lớp | Đầu
LLaMA | 7B | 4096 | 32 | 32
       | 13B | 5120 | 40 | 40
       | 30B | 6656 | 60 | 52
GPT-J | 6B | 4096 | 28 | 16
Pythia | 2.8B | 2560 | 32 | 32
       | 6.9B | 4096 | 32 | 32
       | 12B | 5120 | 36 | 40

--- TRANG 9 ---
Bảng 5: Các nhiệm vụ được sử dụng trong nghiên cứu với các ví dụ đầu vào → đầu ra.

Loại | Nhiệm vụ | Mô tả | Ví dụ
Thuật toán | List first | Cho một danh sách các chữ cái, xuất ra chữ cái đầu tiên | a,b,c → a
| List last | Cho một danh sách các chữ cái, xuất ra chữ cái cuối cùng | a,b,c → c
| Next letter | Cho một chữ cái trong bảng chữ cái tiếng Anh, xuất ra chữ cái tiếp theo | a → b
| Previous letter | Cho một chữ cái trong bảng chữ cái tiếng Anh, xuất ra chữ cái trước đó | b → a
| To lowercase | Cho một chữ cái viết hoa, xuất ra chữ cái viết thường tương ứng | A → a
| To uppercase | Cho một chữ cái viết thường, xuất ra chữ cái viết hoa tương ứng | a → A
Dịch thuật | French to English | Cho một từ tiếng Pháp, dịch sang tiếng Anh | bonjour → hello
| Spanish to English | Cho một từ tiếng Tây Ban Nha, dịch sang tiếng Anh | hola → hello
| English to Spanish | Cho một từ tiếng Anh, dịch sang tiếng Tây Ban Nha | hello → hola
| English to French | Cho một từ tiếng Anh, dịch sang tiếng Pháp | hello → bonjour
Ngôn ngữ học | Present to gerund | Cho một động từ tiếng Anh ở thì hiện tại đơn, xuất ra dạng danh động từ tương ứng | go → going
| Present to past | Cho một động từ tiếng Anh ở thì hiện tại đơn, xuất ra động từ tương ứng ở thì quá khứ đơn | go → went
| Singular to plural | Cho một danh từ tiếng Anh ở dạng số ít, xuất ra dạng số nhiều | cat → cats
| Antonyms | Cho một tính từ tiếng Anh, xuất ra một từ trái nghĩa | happy → sad
Kiến thức | Country to Capital | Cho tên một quốc gia, xuất ra tên thủ đô | France → Paris
| Person to Language | Cho tên một người, xuất ra ngôn ngữ mẹ đẻ của họ | Macron → French
| Location to Continent | Cho tên một địa điểm, xuất ra châu lục của nó | Paris → Europe
| Religion | Cho tên một địa điểm hoặc một người, xuất ra tôn giáo liên quan | Muhammad → Islam

--- TRANG 10 ---
Hình 6: Thí nghiệm nhiệm vụ xung đột. Trong tình huống "Regular" (trên), mô hình đơn giản được cung cấp các minh họa SA cho Nhiệm vụ A (ví dụ: xuất ra chữ cái trước đó trong bảng chữ cái). Trong tình huống "Conflicting" (dưới), mô hình vẫn được cung cấp các minh họa cho Nhiệm vụ A, nhưng chúng tôi tiêm một vector nhiệm vụ θ(SB) từ Nhiệm vụ B xung đột (ví dụ: xuất ra chữ cái tiếp theo trong bảng chữ cái).

--- TRANG 11 ---
Hình 7: Độ chính xác cho mỗi lựa chọn của L (lớp trung gian nơi vector nhiệm vụ được tiêm), được lấy trung bình trên tất cả các nhiệm vụ. Đường liền nét đại diện cho giá trị trung bình, và vùng tô bóng mô tả độ lệch chuẩn.

--- TRANG 12 ---
Bảng 6: Kết quả hoàn chỉnh cho Hình 4, được báo cáo cho tất cả các nhiệm vụ và mô hình.

[Bảng chi tiết với kết quả cho các mô hình GPT-J 6B, LLaMA 13B, LLaMA 30B, LLaMA 7B, Pythia 12B với các phương pháp Baseline, Hypothesis, Regular cho các nhiệm vụ khác nhau]

--- TRANG 13 ---
[Tiếp tục Bảng 6 cho các mô hình Pythia 2.8B và Pythia 6.9B]

--- TRANG 14 ---
Hình 8: Tính biến đổi của vector nhiệm vụ. Đối với mỗi nhiệm vụ, hai biểu đồ được hiển thị: (xanh dương) phân phối khoảng cách giữa các vector nhiệm vụ khác nhau của nhiệm vụ này, được tạo ra từ S và x' khác nhau; (cam) phân phối khoảng cách giữa các vector nhiệm vụ của nhiệm vụ này và của các nhiệm vụ khác.

--- TRANG 15 ---
Hình 9: Một biểu đồ t-SNE 2D, trực quan hóa 50 vector nhiệm vụ cho mỗi nhiệm vụ, mỗi vector được tạo ra từ một lựa chọn khác nhau của S và x' sử dụng LLaMA 7B. Các điểm được mã hóa màu theo loại nhiệm vụ, chẳng hạn như thuật toán hoặc dịch thuật. Mỗi nhiệm vụ có thể được thấy tạo thành cụm riêng biệt. Các nhãn cung cấp tên đầy đủ của nhiệm vụ trong cụm.

--- TRANG 16 ---
Bảng 7: Top 20 token trong phân phối được tạo ra bởi vector nhiệm vụ, cho một nhiệm vụ mỗi loại.

Mô hình | Nhiệm vụ | Token
LLaMA 13B | Prev Letter | e, y, unknown, alphabet, preceding, c, Cad, zA, dit, bill, closer, etc, Stuart, aa, null, cin, ads, g, ulo, Ku
| FR-EN | Mason, gram, immer, Santi, latin, utter, Span, Conc, English, equivalent, engl, Usage, none, pron, ulo, translate, adu, Wiel, grammar, ML
| Present Simple to Gerund | cin, thats, gram, Lorenzo, cian, Isabel, uld, berto, partici, Sah, reporting, eing, tc, Roberto, habit, Writing, etc, ientos, ores, Dutch
| Country Capital | Paris, its, capital, central, Conc, cities, administrative, Los, Madrid, London, San, Isabel, exec, Ar, Bel, Wars, name, capit, Battle, History
Pythia 12B | Prev Letter | r, b, a, d, m, e, p, n, t, u, h, f, c, in, g, s, the, ar, l, x
| FR-EN | in, and, m, d, a, or, out, the, t, o, so, c, con, have, act, e, s, is, all, to
| Present Simple to Gerund | in, t, m, r, a, and, the, ing, action, d, o, e, current, simple, te, w, not, have, out, what
| Country Capital | the, in, a, C, N, B, L, M, T, P, S, R, G, and, F, I, K, U, D, H
GPT-J 6B | Prev Letter | b, c, v, g, s, name, i, ro, n, j, d, t, A, ai, com, m, ust, test, active, k
| FR-EN | other, name, the, true, is, social, s, active, time, car, type, money, F, force, a, public, heart, one, ms, life
| Present Simple to Gerund | getting, storing, working, moving, playing, doing, making, driving, shooting, picking, being, sending, putting, selling, watching, changing, taking, collecting, feeding, reading
| Country Capital | London, Paris, New, West, Berlin, South, Tokyo, San, Chicago, City, Moscow, Jerusalem, Amsterdam, Philadelphia, East, Madrid, Vienna, Beijing, Mexico, Germany
