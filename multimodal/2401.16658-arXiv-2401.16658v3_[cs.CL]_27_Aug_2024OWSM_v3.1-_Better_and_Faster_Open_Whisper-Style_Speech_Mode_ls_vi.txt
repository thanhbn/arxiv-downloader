# 2401.16658.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2401.16658.pdf
# Kích thước file: 112707 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
arXiv:2401.16658v3  [cs.CL]  27 Aug 2024OWSM v3.1: Mô hình Giọng nói Mở Kiểu Whisper Tốt hơn và Nhanh hơn dựa trên
E-Branchformer
Yifan Peng1, Jinchuan Tian1, William Chen1, Siddhant Arora1, Brian Yan1, Yui Sudo2, Muhammad
Shakeel2, Kwanghee Choi1, Jiatong Shi1, Xuankai Chang1, Jee-weon Jung1, Shinji Watanabe1
1Carnegie Mellon University, USA2Honda Research Institute Japan, Japan
yifanpen@andrew.cmu.edu, swatanab@andrew.cmu.edu

Tóm tắt
Các nghiên cứu gần đây đã nhấn mạnh tầm quan trọng của các mô hình nền tảng hoàn toàn mở. Mô hình Giọng nói Mở Kiểu Whisper (OWSM) là một bước đầu tiên hướng tới việc tái tạo OpenAI Whisper bằng cách sử dụng dữ liệu công cộng và các bộ công cụ mã nguồn mở. Tuy nhiên, các phiên bản trước đây của OWSM (v1 đến v3) vẫn dựa trên Transformer tiêu chuẩn, có thể dẫn đến hiệu suất kém hơn so với các kiến trúc mã hóa giọng nói tiên tiến. Công trình này nhằm cải thiện hiệu suất và hiệu quả của OWSM mà không cần thêm dữ liệu. Chúng tôi trình bày một loạt các mô hình dựa trên E-Branchformer có tên OWSM v3.1, từ 100M đến 1B tham số. OWSM v3.1 vượt trội so với phiên bản tiền nhiệm OWSM v3 trong hầu hết các benchmark đánh giá, đồng thời cho thấy tốc độ suy luận cải thiện lên đến 25%. Chúng tôi cũng tiết lộ khả năng nổi bật của OWSM v3.1 trong nhận dạng giọng nói thiên lệch ngữ cảnh zero-shot. Chúng tôi cũng cung cấp một mô hình được huấn luyện trên một tập con dữ liệu với hạn chế giấy phép thấp. Chúng tôi sẽ công bố mã nguồn, mô hình đã huấn luyện và nhật ký huấn luyện.1

Từ khóa: mô hình nền tảng giọng nói, nhận dạng giọng nói, dịch giọng nói, branchformer

1. Giới thiệu
Các mô hình nền tảng giọng nói lớn đã trở nên phổ biến gần đây. Nhờ việc mở rộng quy mô mô hình và dữ liệu cũng như chia sẻ kiến thức qua các ngôn ngữ và tác vụ, những mô hình đa ngôn ngữ và đa tác vụ quy mô lớn này đạt được hiệu suất tốt nhất (SOTA) trong các tác vụ xử lý giọng nói khác nhau [1–3]. OpenAI Whisper [1] là một trong những mô hình nền tảng giọng nói được sử dụng rộng rãi nhất, phát hành trọng số mô hình đã huấn luyện ở năm quy mô từ 39M đến 1.5B tham số. Tuy nhiên, quy trình phát triển đầy đủ, bao gồm chi tiết dữ liệu huấn luyện và động lực học mô hình, không có sẵn cho công chúng, điều này có thể dẫn đến rò rỉ dữ liệu và mối lo ngại về tính công bằng và thiên lệch. Các nghiên cứu gần đây đã ủng hộ việc tái tạo mã nguồn mở các mô hình nền tảng, bao gồm các mô hình ngôn ngữ lớn (LLM) [4–6], mô hình giọng nói tự giám sát [7, 8], và mô hình giọng nói kiểu Whisper [9].

Mô hình Giọng nói Mở Kiểu Whisper (OWSM) [9] là một bước đầu tiên hướng tới việc tái tạo huấn luyện kiểu Whisper sử dụng các bộ dữ liệu công cộng và bộ công cụ mã nguồn mở ESPnet [10]. Nó hỗ trợ nhận dạng giọng nói tự động đa ngôn ngữ (ASR), dịch giọng nói từ-bất-kỳ-đến-bất-kỳ (ST), nhận dạng ngôn ngữ (LID), và căn chỉnh cấp độ phát âm. Nó cũng công bố tất cả các script, trọng số mô hình đã huấn luyện và nhật ký huấn luyện. Để phù hợp với thiết kế của OpenAI Whisper, ba phiên bản trong [9], OWSM v1, v2, và v3, áp dụng kiến trúc Transformer tiêu chuẩn [11]. Tuy nhiên, điều này có thể dẫn đến hiệu suất không tối ưu so với các bộ mã hóa tiên tiến hơn như Conformer [12], Branchformer [13], và E-Branchformer [14].

Trong công trình này, mục tiêu của chúng tôi là cải thiện hiệu suất và hiệu quả của OWSM v3 trước đây bằng cách sử dụng cùng một lượng dữ liệu huấn luyện (xem Hình 1 cho kết quả ASR tiếng Anh). Chúng tôi tiến hành các thí nghiệm sơ bộ để so sánh các bộ mã hóa Transformer, Conformer, và E-Branchformer và chọn E-Branchformer do sự hội tụ nhanh hơn. Sau đó chúng tôi trình bày các mô hình OWSM v3.1 mới ở ba quy mô: base (101M), small (367M), và medium (1.02B). Để ổn định việc huấn luyện các mô hình E-Branchformer lớn, chúng tôi đề xuất một lịch trình tốc độ học tuyến tính từng đoạn. Kết quả trên các benchmark mở rộng cho thấy OWSM v3.1 vượt trội so với OWSM v3 trước đây trong 8 trong 9 bộ thử nghiệm ASR tiếng Anh, 10 trong 11 bộ thử nghiệm ASR đa ngôn ngữ, 13 trong 19 bộ thử nghiệm ST, và 3 trong 4 bộ thử nghiệm SLUE-PERB [15]. Ngoài ra, OWSM v3.1 nhanh hơn 24% cho ASR tiếng Anh và 16% đến 25% nhanh hơn cho ST trong quá trình suy luận, nhờ bộ giải mã nhỏ hơn. Hình 1 cho thấy OWSM v3.1 thậm chí đạt được sự cân bằng tốt hơn giữa hiệu suất và hiệu quả so với Whisper. Hơn nữa, chúng tôi tiết lộ rằng OWSM v3.1 có khả năng nổi bật trong ASR thiên lệch ngữ cảnh zero-shot. Để mở rộng khả năng tiếp cận mô hình của chúng tôi, chúng tôi cung cấp một mô hình kích thước nhỏ được huấn luyện trên một tập con dữ liệu với hạn chế thấp. Chúng tôi sẽ công bố mã nguồn, mô hình đã huấn luyện và nhật ký huấn luyện để thúc đẩy tính minh bạch và khoa học mở.

2. OWSM v3.1

2.1. Kiến trúc mô hình
Whisper [1] và OWSM v3 [9] áp dụng kiến trúc mã hóa-giải mã Transformer [11]. Các bộ mã hóa giọng nói tiên tiến hơn như Conformer [12] và Branchformer [13, 14] đã đạt được kết quả vượt trội trong các tác vụ xử lý giọng nói khác nhau [16, 17]. Do đó, việc khám phá chúng trong các mô hình nền tảng giọng nói lớn là tự nhiên và đầy hứa hẹn. Trong công trình này, chúng tôi chứng minh tính hiệu quả và khả năng mở rộng của E-Branchformer [14] lên đến quy mô 1B tham số. E-Branchformer là một Branchformer cải tiến [13], sử dụng các nhánh song song để nắm bắt thông tin cục bộ và toàn cầu và kết hợp chúng với tích chập. Trong huấn luyện kiểu Whisper, âm thanh đầu vào có độ dài cố định 30s, vì vậy chúng tôi đơn giản sử dụng mã hóa vị trí tuyệt đối hình sin. Bảng 1 tóm tắt các cấu hình mô hình. OWSM v3.1 được đề xuất chủ yếu tuân theo thiết kế của OWSM v3, ngoại trừ bộ mã hóa.

--- TRANG 2 ---
Bảng 1: Kiến trúc mô hình và thiết lập huấn luyện. LR (hạn chế thấp) là một mô hình kích thước nhỏ được huấn luyện trên một tập con dữ liệu với hạn chế giấy phép thấp.

Whisper [1] OWSM v3 [9] OWSM v3.1 (của chúng tôi)
base small medium medium base small medium LR
Kiến trúc mô hình
Tham số 74M 244M 769M 889M 101M 367M 1.02B 367M
Bộ mã hóa Transformer Transformer E-Branchformer
Bộ giải mã Transformer Transformer Transformer
Lớp 6 12 24 24 6 9 18 9
Ẩn 512 768 1024 1024 384 768 1024 768
Đầu 8 12 16 16 6 12 16 12
Thiết lập huấn luyện
Dữ liệu (h) 680K 180K 180K 70K
Ngôn ngữ 99 151 151 143
Giờ GPU không rõ 30.7K 2.3K 3.2K 24.6K 3.2K
LR tối đa 1e-3 5e-4 2.5e-4 2.5e-4 1e-3 5e-4 2e-4 5e-4

Chúng tôi sửa đổi kích thước ẩn và số lượng lớp để điều chỉnh kích thước mô hình. Chúng tôi cung cấp ba biến thể để khảo sát hành vi mở rộng quy mô, bao gồm base (101M), small (367M), và medium (1.02B). Mặc dù hơi lớn hơn OWSM v3 và Whisper ở cùng quy mô, các mô hình OWSM v3.1 thể hiện tốc độ suy luận nhanh hơn (xem Hình 1, Bảng 4, và Bảng 5), chủ yếu do bộ giải mã nhỏ hơn.

2.2. Chuẩn bị dữ liệu
Chúng tôi chuẩn bị dữ liệu huấn luyện bằng cách sử dụng các script được công bố bởi [9]. Bảng 1 cho thấy lượng dữ liệu và số lượng ngôn ngữ. Vui lòng tham khảo [9] để biết thêm chi tiết. Chúng tôi thực hiện tiền xử lý sau đây để làm cho các bản phiên âm văn bản nhất quán hơn, chỉ ảnh hưởng đến một lượng rất nhỏ dữ liệu.

• Chúng tôi loại trừ WSJ khỏi dữ liệu huấn luyện do phong cách nói và chú thích khác biệt, trong đó dấu câu được phát âm rõ ràng và chú thích như một từ.
• AMI [18] và VoxForge [19] cung cấp bản phiên âm chữ hoa. Chúng tôi chuyển đổi chúng thành chữ thường. Dữ liệu khác giữ nguyên.
• Chúng tôi gộp hai mã ngôn ngữ "cmn" và "zho" thành "zho".

Các mô hình base, small, và medium của chúng tôi được huấn luyện trên tất cả 180K giờ dữ liệu. Để mở rộng khả năng tiếp cận mô hình của chúng tôi, chúng tôi cũng huấn luyện một mô hình kích thước nhỏ sử dụng một tập con dữ liệu với hạn chế thấp (LR): AMI (CC-BY-4.0) [18], CommonVoice (CC0-1.0) [20], FLEURS (CC-BY-4.0) [21], KsponSpeech (MIT) [22], LibriSpeech (CC-BY-4.0) [23], Multilingual LibriSpeech (CC-BY-4.0) [24], và VCTK (CC-BY-4.0) [25]. Tập con này chứa 70K giờ dữ liệu ASR nhưng không có dữ liệu ST.

2.3. Thiết lập huấn luyện
Các mô hình của chúng tôi được triển khai trong ESPnet [10] với PyTorch [26]. Chúng tôi sử dụng FlashAttention [27] để cải thiện hiệu quả huấn luyện. Kích thước batch là 256. Các mô hình base, small, và medium của chúng tôi được huấn luyện trong khoảng 3 lần trọn vẹn qua 180K giờ dữ liệu sử dụng tương ứng 16, 16, và 64 GPU NVIDIA A100 (40GB). Mô hình hạn chế thấp tuân theo thiết lập của OWSM v3.1 small, nhưng chỉ sử dụng 70K giờ dữ liệu. Bảng 1 cho thấy số giờ GPU ước tính, giả sử một cluster GPU ổn định.

Chúng tôi thấy khó khăn trong việc huấn luyện các mô hình trên dữ liệu giọng nói đa ngôn ngữ, đa tác vụ và dạng dài quy mô lớn.2 Một chiến lược điển hình để cải thiện sự hội tụ là sử dụng tốc độ học rất nhỏ ở đầu quá trình huấn luyện. Tuy nhiên, với lịch trình khởi động tuyến tính, chúng tôi phải giảm đáng kể tốc độ học đỉnh hoặc tăng số bước khởi động, cả hai đều dẫn đến hiệu suất kém hơn theo các khám phá sơ bộ của chúng tôi. Để giảm thiểu vấn đề này, chúng tôi đề xuất một lịch trình khởi động tuyến tính từng đoạn tăng tốc độ học từ từ ở đầu và nhanh hơn sau đó. Cụ thể, tốc độ học được tăng tuyến tính đến một giá trị rất nhỏ (ví dụ 5e-5) trong 30K bước đầu tiên và sau đó tăng tuyến tính đến tốc độ học đỉnh trong 30K bước tiếp theo. Sau khởi động, nó được giảm theo cấp số nhân giống như phiên bản vanilla. Lịch trình tuyến tính từng đoạn được đề xuất cho phép huấn luyện thành công OWSM v3.1.

2Dựa trên kinh nghiệm của chúng tôi, điều này chủ yếu do định dạng dữ liệu dạng dài 30s. Ngay cả các mô hình nhỏ cũng khó hội tụ.

--- TRANG 3 ---
Bảng 2: WER (↓) của ASR tiếng Anh. Đậm: kết quả tốt nhất. Gạch dưới: OWSM v3.1 vượt trội so với OWSM v3. CV: CommonVoice. LS: LibriSpeech. MLS: Multilingual LibriSpeech.

Bộ thử nghiệm | Whisper | OWSM v3 | OWSM v3.1 (của chúng tôi)
base small medium medium base small medium LR
CV [20] 25.2 15.7 11.9 14.5 21.5 14.3 12.6 12.3
FLEURS [21] 12.4 9.6 6.4 10.9 14.8 10.3 9.0 10.8
LS clean [23] 5.1 3.3 2.8 2.7 3.6 2.5 2.4 2.1
LS other [23] 12.0 7.7 6.5 6.0 9.1 5.8 5.0 5.2
MLS [24] 13.4 9.1 10.2 7.4 12.0 8.1 7.1 7.0
SWBD [28] 25.7 22.2 19.4 17.2 22.9 17.4 16.3 31.5
TEDLIUM [29] 6.3 4.6 5.1 4.8 7.8 5.0 5.1 9.2
VoxPopuli [30] 10.2 8.5 7.6 9.2 12.0 9.1 8.4 13.8
WSJ [31] 5.0 4.3 2.9 13.4 5.3 3.8 3.5 4.9
WER trung bình (↓) 12.8 9.4 8.1 9.6 12.1 8.5 7.7 10.8
Tăng tốc (↑) 2.97x 1.81x 0.94x 1.00x 3.67x 2.21x 1.24x 2.50x

Hình 2 cho thấy các đường cong mất mát xác thực trong vòng 105K bước đầu tiên.3 E-Branchformer hội tụ nhanh hơn các mô hình khác, điều này phù hợp với công trình trước đây [17]. Do đó, chúng tôi áp dụng E-Branchformer trong các thí nghiệm chính.

3.2. Nhận dạng giọng nói tiếng Anh
Bảng 2 cho thấy kết quả ASR tiếng Anh. Hình 1 trực quan hóa tỷ lệ lỗi từ trung bình (WER) so với tăng tốc được đo trên GPU NVIDIA A40. Chúng tôi tuân theo [9] để thực hiện tìm kiếm tham lam và áp dụng bộ chuẩn hóa văn bản Whisper trước khi chấm điểm. Chúng tôi có những quan sát sau: (1) So với OWSM v3 trước đây, mô hình OWSM v3.1 medium được đề xuất hoạt động tốt hơn trong 8 trong 9 bộ thử nghiệm. Sự cải thiện đặc biệt lớn trong CommonVoice, FLEURS, LibriSpeech, Switchboard, VoxPopuli, và WSJ.4 Điều này xác minh tính hiệu quả của bộ mã hóa E-Branchformer của chúng tôi. (2) OWSM v3.1 thậm chí đạt được WER trung bình thấp hơn so với Whisper ở mỗi quy mô, chứng minh hiệu suất cạnh tranh của nó, mặc dù được huấn luyện trên ít dữ liệu ASR tiếng Anh hơn nhiều (73K so với 438K giờ). (3) OWSM v3.1 nhanh hơn trong quá trình suy luận so với các mô hình khác ở cùng quy mô, chủ yếu do bộ giải mã nhỏ hơn. (4) Mô hình hạn chế thấp (LR) kích thước nhỏ của chúng tôi đạt được hiệu suất hợp lý xem xét rằng nó được huấn luyện trên một tập con dữ liệu (xem Phần 2.2).

3.3. Nhận dạng giọng nói đa ngôn ngữ
Bảng 3 trình bày kết quả ASR đa ngôn ngữ. Chúng tôi thực hiện giải mã tham lam và áp dụng bộ chuẩn hóa văn bản Whisper trước khi tính tỷ lệ lỗi từ hoặc ký tự (WER/CER). Chúng tôi quan sát thấy OWSM v3.1 medium vượt trội so với OWSM v3 trong 10 trong 11 bộ thử nghiệm trong các ngôn ngữ khác nhau, thường với biên độ lớn. Cụ thể, tỷ lệ lỗi trung bình giảm từ 18.8% xuống 15.2%. So với Whisper, OWSM v3.1 vẫn thua trong nhiều ngôn ngữ châu Âu do dữ liệu huấn luyện hạn chế. Ngược lại, khi dữ liệu đủ (ví dụ tiếng Trung và tiếng Nhật), OWSM v3.1 đạt hiệu suất mạnh và vượt trội so với Whisper. Điều này tiết lộ tầm quan trọng của số lượng dữ liệu huấn luyện. Trong tương lai, chúng tôi sẽ bao gồm thêm dữ liệu từ các nguồn công cộng như YODAS [35] để cải thiện OWSM hơn nữa.

3.4. Dịch giọng nói
Chúng tôi đánh giá ST trên các bộ thử nghiệm CoVoST-2 [34]. Đối với tiếng Anh-sang-X, chúng tôi sử dụng tất cả 15 hướng. Đối với X-sang-tiếng Anh, chúng tôi báo cáo kết quả của các hướng mà OWSM có hơn 100 giờ dữ liệu huấn luyện. Đối với các hướng khác với dữ liệu huấn luyện rất hạn chế như tiếng Nhật- hoặc tiếng Trung-sang-tiếng Anh, OWSM thường không hoạt động [9]. Chúng tôi cũng ghi lại thời gian giải mã trung bình của mỗi ngôn ngữ ít tài nguyên, dẫn đến kết quả kém. Trong v3.1, chúng tôi loại trừ WSJ trong quá trình huấn luyện và đạt được WER thấp hơn đáng kể.

Bảng 4: BLEU (↑) của ST X-sang-En trên CoVoST-2 [34]. Kích thước dữ liệu huấn luyện (theo giờ) cũng được hiển thị. OWSM v3.1 sử dụng cùng lượng dữ liệu huấn luyện như OWSM v3. Đậm: kết quả tốt nhất. Gạch dưới: OWSM v3.1 vượt trội so với OWSM v3.

Nguồn | Whisper | OWSM v3 | OWSM v3.1 (của chúng tôi)
dữ liệu base small medium dữ liệu medium base small medium
Tiếng Đức 4.3K 11.4 25.0 33.6 0.2K 16.2 7.3 15.1 17.1
Tiếng Tây Ban Nha 6.7K 19.2 32.8 39.7 0.1K 20.5 10.0 19.3 22.3
Tiếng Pháp 4.5K 13.1 26.4 34.4 0.3K 21.7 11.1 20.3 22.7
Tiếng Catalan 0.2K 9.7 21.7 29.2 0.1K 16.8 9.0 16.2 18.4
BLEU trung bình (↑) 13.4 26.5 34.2 - 18.8 9.4 17.7 20.1
Tăng tốc (↑) 2.14x 1.80x 0.98x - 1.00x 3.23x 2.26x 1.16x

3Mất hơn một tuần để mô hình hội tụ hoàn toàn với 16 GPU. Do giới hạn ngân sách và thời gian, chúng tôi chỉ so sánh tốc độ hội tụ của chúng dựa trên 105K bước đầu tiên.

4Như đã thảo luận trong [9], dữ liệu huấn luyện WSJ được sử dụng bởi OWSM v3, nhưng bản phiên âm của nó hoàn toàn viết hoa. Mô hình có thể coi nó như một

--- TRANG 4 ---
Bảng 5: BLEU (↑) của ST En-sang-X trên CoVoST-2 [34]. Đậm: kết quả tốt nhất. Gạch dưới: OWSM v3.1 vượt trội so với OWSM v3.

Đích | Dữ liệu huấn luyện (h) | OWSM v3 | OWSM v3.1 (của chúng tôi)
medium base small medium
Tiếng Đức 14.0K 25.4 14.6 22.8 25.4
Tiếng Catalan 0.4K 20.0 7.7 15.9 19.6
Tiếng Trung 13.7K 33.4 14.5 26.7 32.1
Tiếng Ba Tư 0.8K 9.5 3.0 7.7 10.1
Tiếng Estonia 0.4K 7.8 1.8 5.8 7.7
Tiếng Mông Cổ 0.4K 3.1 1.0 3.3 4.6
Tiếng Thổ Nhĩ Kỳ 0.9K 6.1 1.2 4.8 6.5
Tiếng Ả Rập 0.9K 6.6 1.6 5.1 7.2
Tiếng Thụy Điển 0.4K 19.9 8.1 16.6 20.3
Tiếng Latvia 0.4K 6.3 1.3 4.4 6.4
Tiếng Slovenia 0.4K 8.6 0.7 5.7 9.0
Tiếng Tamil 0.4K 0.0 0.0 0.0 0.0
Tiếng Nhật 1.0K 17.3 8.7 16.4 19.6
Tiếng Indonesia 0.4K 14.5 5.1 12.4 16.1
Tiếng Welsh 0.4K 15.9 4.5 11.6 15.3
BLEU trung bình (↑) 13.0 4.9 10.6 13.3
Tăng tốc (↑) 1.00x 3.00x 2.43x 1.25x

Bảng 6: WER (↓) của ASR dạng dài trên TEDLIUM. Đậm: kết quả tốt nhất. Gạch dưới: OWSM v3.1 vượt trội so với OWSM v3.

Whisper | OWSM v3 | OWSM v3.1 (của chúng tôi)
base small medium medium base small medium
5.3 4.4 3.8 9.2 9.6 6.7 5.7

bộ thử nghiệm trên GPU NVIDIA A40 và tính toán tốc độ giải mã tương đối so với OWSM v3.

Đối với X-sang-tiếng Anh (được hiển thị trong Bảng 4), OWSM v3.1 medium được đề xuất đạt điểm BLEU cao hơn một cách nhất quán so với OWSM v3. BLEU trung bình được cải thiện từ 18.8 lên 20.1. OWSM v3.1 cũng nhanh hơn 16% so với OWSM v3 trong quá trình suy luận. So với Whisper, OWSM v3.1 vẫn hoạt động kém hơn do dữ liệu huấn luyện hạn chế. Nhưng OWSM v3.1 có tốc độ suy luận nhanh hơn so với Whisper ở mỗi quy mô, nhờ vào sự dịch chuyển thời gian lớn hơn trong bộ mã hóa (40 ms so với 20 ms) và bộ giải mã nhỏ hơn.

Đối với tiếng Anh-sang-X (được hiển thị trong Bảng 5), OWSM v3.1 vượt trội so với OWSM v3 trong 9 trong 15 hướng. BLEU trung bình được cải thiện nhẹ từ 13.0 lên 13.3 và tốc độ suy luận nhanh hơn 25%. Lưu ý rằng Whisper không thể thực hiện dịch thuật theo những hướng này.

3.5. Nhận dạng giọng nói dạng dài
Bảng 6 trình bày kết quả ASR tiếng Anh dạng dài trên bộ thử nghiệm TEDLIUM [29]. Tương tự như [1, 9], OWSM nhận toàn bộ bản ghi âm thanh làm đầu vào và tạo ra bản phiên âm theo từng đoạn. Mỗi đoạn có độ dài cố định 30s và được dịch chuyển dần dựa trên dấu thời gian dự đoán. OWSM v3.1 medium được đề xuất đạt WER 5.7%, so với 9.2% của OWSM v3. Điều này chứng minh độ bền vững của OWSM v3.1 đối với âm thanh dạng dài; các dấu thời gian dự đoán cũng có thể chính xác hơn. OWSM v3.1 vẫn thua Whisper, có thể do (1) dữ liệu huấn luyện của chúng tôi chỉ khoảng một phần tư so với dữ liệu huấn luyện của Whisper, và (2) nhiều bộ dữ liệu công cộng được sử dụng bởi OWSM không cung cấp dữ liệu dạng dài không phân đoạn và chúng tôi phải sử dụng âm thanh ngắn đã phân đoạn để huấn luyện, điều này dẫn đến sự không khớp giữa huấn luyện và suy luận. Trong tương lai, chúng tôi sẽ thêm nhiều dữ liệu dạng dài hơn để giảm thiểu vấn đề này.

3.6. Nhận dạng ngôn ngữ
Bảng 7 cho thấy độ chính xác của nhận dạng ngôn ngữ trên bộ thử nghiệm FLEURS. Chúng tôi nhận thấy sự suy giảm của OWSM v3.1 so với OWSM v3 trước đây, nhưng OWSM v3.1 medium vẫn tốt hơn nhiều so với Whisper medium vì mô hình của chúng tôi sử dụng dữ liệu FLEURS và CommonVoice đa ngôn ngữ quy mô lớn để huấn luyện. Chúng tôi cũng thấy rằng OWSM v3.1 được hưởng lợi nhiều hơn từ việc mở rộng quy mô so với Whisper. Từ base đến medium, độ chính xác của OWSM v3.1 gần như tăng gấp đôi (41.9% lên 75.6%), trong khi độ chính xác của Whisper chỉ tăng nhẹ (47.6% lên 54.8%). Một lý do có thể là OWSM hỗ trợ nhiều ngôn ngữ hơn cho ASR và cặp ngôn ngữ cho ST, điều này đầy thách thức hơn cho các mô hình nhỏ hơn để học.

Bảng 7: Độ chính xác % (↑) của LID trên FLEURS [21].

Whisper | OWSM v3 | OWSM v3.1 (của chúng tôi)
base small medium medium base small medium
47.6 53.1 54.8 81.4 41.9 67.1 75.6

Bảng 8: Điểm F1 (↑) của các tác vụ SLU trên SLUE-PERB [15].

Tác vụ | Chỉ số | OWSM v3 | OWSM v3.1 (của chúng tôi)
Phân tích tình cảm | Điểm F1 | 60.1 | 56.2
Nhận dạng thực thể có tên | Điểm F1 | 54.8 | 65.8
Định vị thực thể có tên | frame-F1 | 40.5 | 50.4
Phân loại hành động đối thoại | Điểm F1 | 56.5 | 64.8

3.7. Hiểu ngôn ngữ nói thông qua tinh chỉnh
Các mô hình giọng nói đã huấn luyện trước có thể được áp dụng cho các tác vụ downstream thông qua tinh chỉnh, thường cải thiện hiệu suất [36]. Chúng tôi lấy hiểu ngôn ngữ nói (SLU) làm ví dụ và đánh giá OWSM trên benchmark SLUE-PERB được đề xuất gần đây [15]. Cụ thể, bộ mã hóa giọng nói đã huấn luyện trước được đóng băng và một bộ giải mã nông được khởi tạo ngẫu nhiên được huấn luyện trên dữ liệu SLU cụ thể theo tác vụ. Mô hình sau đó được đánh giá trên dữ liệu thử nghiệm SLU tương ứng. Quy trình đánh giá này tương tự như benchmark SUPERB được sử dụng rộng rãi [37]. Chúng tôi xem xét bốn tác vụ SLU, tức là phân tích tình cảm (SA), nhận dạng thực thể có tên (NER), định vị thực thể có tên (NEL), và phân loại hành động đối thoại (DAC). Như được hiển thị trong Bảng 8, OWSM v3.1 medium được đề xuất vượt trội so với mô hình v3 trước đây với biên độ lớn trong NER, NEL, và DAC, xác nhận khả năng mạnh mẽ của bộ mã hóa E-Branchformer của chúng tôi.

3.8. Khả năng nổi bật cho thiên lệch ngữ cảnh zero-shot
OWSM tạo ra các giả thuyết ASR hoặc ST có điều kiện trên một lời nhắc văn bản tùy chọn. Trong quá trình huấn luyện, câu trước đó trong cùng một bản ghi được sử dụng làm lời nhắc theo xác suất 0.5. Trong quá trình suy luận, người dùng có thể cung cấp lời nhắc để có thể điều chỉnh đầu ra. Một ứng dụng của tính năng này là

Bảng 9: WER (↓) của thiên lệch ngữ cảnh zero-shot.

OWSM v3.1 | LibriSpeech test-clean | LibriSpeech test-other
WER U-WER B-WER WER U-WER B-WER
base 3.88 2.45 15.47 9.48 6.89 32.17
+ thiên lệch 4.37 3.09 14.79 12.49 10.45 30.36
small 2.68 1.63 11.27 6.16 4.21 23.27
+ thiên lệch 2.58 1.75 9.32 5.89 4.48 18.34
medium 2.59 1.61 10.61 5.31 3.52 21.12
+ thiên lệch 2.24 1.62 7.31 5.03 3.86 15.35

thiên lệch ngữ cảnh zero-shot, nhằm cải thiện hiệu suất ASR của các từ hiếm bằng cách cung cấp một danh sách các từ thiên lệch chứa mục tiêu thực và nhiều yếu tố gây nhiễu [38]. Chúng tôi đánh giá các mô hình OWSM v3.1 trên các bộ thử nghiệm thiên lệch LibriSpeech được tạo bởi [38]. Cụ thể, chúng tôi sử dụng 100 từ thiên lệch được phân tách bằng dấu cách làm lời nhắc và thực hiện giải mã tham lam. Không giống như Phần 3.2, chúng tôi không sử dụng bất kỳ bộ chuẩn hóa văn bản nào để phù hợp với điều kiện trong [38]. Thiên lệch ngữ cảnh nhằm giảm WER thiên lệch (B-WER) trong khi duy trì WER không thiên lệch (U-WER). Bảng 9 cho thấy WER của ba mô hình của chúng tôi. So với ASR không có thiên lệch, mô hình base cho thấy cải thiện nhỏ về B-WER nhưng suy giảm lớn hơn nhiều về U-WER, cho thấy rằng nó không thể phân biệt giữa thông tin ngữ cảnh hữu ích và các yếu tố gây nhiễu. Ngược lại, các mô hình small và medium giảm đáng kể B-WER và chủ yếu duy trì U-WER, chứng minh rằng các mô hình này có thể trích xuất và sử dụng thông tin ngữ cảnh hữu ích một cách zero-shot. Hiện tượng OWSM nhỏ hơn hoạt động rất kém trong ASR thiên lệch zero-shot trong khi các mô hình lớn hơn hoạt động tốt tiết lộ rằng các mô hình nền tảng giọng nói cũng có khả năng nổi bật, điều này đã được quan sát rộng rãi trong các LLM [39].

4. Kết luận và nghiên cứu tương lai
Chúng tôi trình bày OWSM v3.1, một họ các Mô hình Giọng nói Mở Kiểu Whisper dựa trên E-Branchformer, từ 100M đến 1B tham số. Mặc dù được huấn luyện trên cùng lượng dữ liệu, OWSM v3.1 đạt kết quả tốt hơn so với OWSM v3 trước đây trong phần lớn các bộ đánh giá, đồng thời cho thấy tốc độ suy luận nhanh hơn lên đến 25%. Chúng tôi tiếp tục khảo sát khả năng nổi bật của các mô hình nền tảng giọng nói sử dụng ASR thiên lệch ngữ cảnh zero-shot, xác minh lợi ích của việc mở rộng quy mô. Để mở rộng khả năng tiếp cận mô hình của chúng tôi, chúng tôi cung cấp một mô hình được huấn luyện trên một tập con dữ liệu với hạn chế giấy phép thấp. Chúng tôi sẽ công bố mã nguồn, trọng số mô hình đã huấn luyện và nhật ký huấn luyện để thúc đẩy tính minh bạch và tạo điều kiện phát triển các mô hình nền tảng trong lĩnh vực giọng nói.

Một hạn chế là công trình này không nâng cao số lượng hoặc chất lượng dữ liệu huấn luyện, có thể dẫn đến hiệu suất không tối ưu trong các ngôn ngữ ít tài nguyên. Các hướng nghiên cứu tương lai bao gồm khám phá tác động của tính đa dạng dữ liệu đến hiệu suất mô hình, thêm nhiều dữ liệu công cộng như YODAS [35] để có hiệu suất tốt hơn, nén mô hình đã huấn luyện trước để có hiệu quả tốt hơn [40–45], và khám phá các ứng dụng downstream khác nhau như SLU [36, 46] và mô hình ngôn ngữ giọng nói [47, 48].

5. Lời cảm ơn
Chúng tôi muốn cảm ơn Amazon AGI đã tài trợ. Chúng tôi sử dụng PSC Bridges2 và NCSA Delta thông qua ACCESS CIS210014, được tài trợ bởi các khoản tài trợ của Quỹ Khoa học Quốc gia #2138259, #2138286, #2138307, #2137603, và #2138296.

--- TRANG 5 ---
6. Tài liệu tham khảo
[1] A. Radford, J. W. Kim, T. Xu, et al., "Robust speech recognition via large-scale weak supervision," trong Proc. ICML, 2023.
[2] Y. Zhang, W. Han, J. Qin, et al., "Google usm: Scaling automatic speech recognition beyond 100 languages," arXiv preprint arXiv:2303.01037, 2023.
[3] L. Barrault, Y.-A. Chung, M. C. Meglioli, et al., "Seamless: Multilingual expressive and streaming speech translation," arXiv preprint arXiv:2312.05187, 2023.
[4] H. Touvron, T. Lavril, G. Izacard, et al., "Llama: Open and efficient foundation language models," arXiv:2302.13971, 2023.
[5] Z. Liu, A. Qiao, W. Neiswanger, et al., "Llm360: Towards fully transparent open-source llms," arXiv preprint arXiv:2312.06550, 2023.
[6] D. Groeneveld, I. Beltagy, P. Walsh, et al., "Olmo: Accelerating the science of language models," arXiv preprint arXiv:2402.00838, 2024.
[7] W. Chen, X. Chang, Y. Peng, et al., "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute," trong Proc. Interspeech, 2023.
[8] W. Chen, J. Shi, B. Yan, et al., "Joint prediction and denoising for large-scale multilingual self-supervised learning," trong Proc. ASRU, 2023.
[9] Y. Peng, J. Tian, B. Yan, et al., "Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data," trong Proc. ASRU, 2023.
[10] S. Watanabe, T. Hori, S. Karita, et al., "ESPnet: End-to-End Speech Processing Toolkit," trong Proc. Interspeech, 2018.
[11] A. Vaswani, N. Shazeer, N. Parmar, et al., "Attention is all you need," trong Proc. NeurIPS, 2017.
[12] A. Gulati, J. Qin, C.-C. Chiu, et al., "Conformer: Convolution-augmented Transformer for Speech Recognition," trong Proc. Interspeech, 2020.
[13] Y. Peng, S. Dalmia, I. Lane, và S. Watanabe, "Branchformer: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding," trong Proc. ICML, 2022.
[14] K. Kim, F. Wu, Y. Peng, et al., "E-branchformer: Branchformer with enhanced merging for speech recognition," trong Proc. SLT, 2023.
[15] S. Arora, R. Sharma, A. Pasad, et al., "SLUE-PERB: A Spoken Language Understanding Performance Benchmark and Toolkit," trong ASRU SPARKS Workshop, 2023.
[16] P. Guo, F. Boyer, X. Chang, et al., "Recent developments on espnet toolkit boosted by conformer," trong Proc. ICASSP, 2021.
[17] Y. Peng, K. Kim, F. Wu, et al., "A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks," trong Proc. Interspeech, 2023.
[18] J. Carletta, "Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus," Lang. Res. Eval., vol. 41, pp. 181–190, 2007.
[19] VoxForge: http://www.voxforge.org/.
[20] R. Ardila et al., "Common voice: A massively-multilingual speech corpus," arXiv:1912.06670, 2019.
[21] A. Conneau et al., "FLEURS: Few-Shot Learning Evaluation of Universal Representations of Speech," trong Proc. SLT, 2022.
[22] J.-U. Bang et al., "Ksponspeech: Korean spontaneous speech corpus for automatic speech recognition," Applied Sciences, vol. 10, no. 19, p. 6936, 2020.
[23] V. Panayotov et al., "Librispeech: An ASR corpus based on public domain audio books," trong ICASSP, 2015.
[24] V. Pratap et al., "MLS: A large-scale multilingual dataset for speech research," arXiv:2012.03411, 2020.
[25] J. Yamagishi et al., CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit, 2019.
[26] A. Paszke et al., "Pytorch: An imperative style, high-performance deep learning library," trong Proc. NeurIPS, 2019.
[27] T. Dao, D. Y. Fu, S. Ermon, et al., "Flashattention: Fast and memory-efficient exact attention with io-awareness," trong Proc. NeurIPS, 2022.
[28] J. Godfrey et al., "SWITCHBOARD: telephone speech corpus for research and development," trong Proc. ICASSP, 1992.
[29] F. Hernandez et al., "Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation," trong Speech & Computer, 2018, pp. 198–208.
[30] C. Wang et al., "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation," trong Proc. ACL, 2021.
[31] D. B. Paul và J. Baker, "The design for the Wall Street Journal-based CSR corpus," trong Proc. Workshop on Speech and Natural Language, 1992.
[32] H. Bu et al., "AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline," trong Proc. O-COCOSDA, 2017.
[33] Y. Yin, D. Mori, et al., ReazonSpeech: A Free and Massive Corpus for Japanese ASR, 2023.
[34] C. Wang et al., "CoVoST 2 and Massively Multilingual Speech Translation," trong Interspeech, 2021.
[35] X. Li, S. Takamichi, T. Saeki, et al., "Yodas: Youtube-oriented dataset for audio and speech," trong Proc. ASRU, 2023.
[36] Y. Peng, S. Arora, Y. Higuchi, et al., "A Study on the Integration of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding," trong Proc. SLT, 2022.
[37] S.-w. Yang, P.-H. Chi, Y.-S. Chuang, et al., "SUPERB: Speech Processing Universal PERformance Benchmark," trong Proc. Interspeech, 2021.
[38] D. Le, M. Jain, G. Keren, et al., "Contextualized streaming end-to-end speech recognition with trie-based deep biasing and shallow fusion," trong Proc. Interspeech, 2021.
[39] J. Wei, Y. Tay, R. Bommasani, et al., "Emergent abilities of large language models," Trans. Mach. Learn. Res., vol. 2022, 2022.
[40] H.-J. Chang, S.-w. Yang, và H.-y. Lee, "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert," trong Proc. ICASSP, 2022.
[41] C.-I. J. Lai, Y. Zhang, A. H. Liu, et al., "PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition," trong Proc. NeurIPS, 2021.
[42] Y. Peng, K. Kim, F. Wu, et al., "Structured pruning of self-supervised pre-trained models for speech recognition and understanding," trong Proc. ICASSP, 2023.
[43] Y. Peng, Y. Sudo, S. Muhammad, và S. Watanabe, "DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models," trong Proc. Interspeech, 2023.
[44] Y. Peng, J. Lee, và S. Watanabe, "I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition," trong Proc. ICASSP, 2023.
[45] S. Gandhi, P. von Platen, và A. M. Rush, "Distil-whisper: Robust knowledge distillation via large-scale pseudo labelling," arXiv preprint arXiv:2311.00430, 2023.
[46] S. Arora, H. Futami, J.-w. Jung, et al., "UniverSLU: Universal spoken language understanding for diverse classification and sequence generation tasks with a single network," arXiv preprint arXiv:2310.02973, 2023.
[47] M. Wang, W. Han, I. Shafran, et al., "SLM: Bridge the thin gap between speech and text foundation models," trong Proc. ASRU, 2023.
[48] C. Tang, W. Yu, G. Sun, et al., "Salmonn: Towards generic hearing abilities for large language models," arXiv preprint arXiv:2310.13289, 2023.
