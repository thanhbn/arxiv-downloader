# 2310.07478.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.07478.pdf
# Kích thước tệp: 1268251 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Học Đồ Thị Đa Phương Thức cho Các Nhiệm Vụ Sinh Tạo
Minji Yoon
Đại học Carnegie MellonJing Yu Koh
Đại học Carnegie MellonBryan Hooi
Đại học Quốc gia Singapore
Ruslan Salakhutdinov
Đại học Carnegie Mellon
Tóm tắt
Học đa phương thức kết hợp nhiều phương thức dữ liệu, mở rộng các loại và
độ phức tạp của dữ liệu mà các mô hình của chúng ta có thể sử dụng: ví dụ, từ văn bản thuần túy đến các cặp hình ảnh-
chú thích. Hầu hết các thuật toán học đa phương thức tập trung vào việc mô hình hóa các cặp dữ liệu một-một đơn giản từ hai phương thức, chẳng hạn như cặp hình ảnh-chú thích, hoặc cặp âm thanh-
văn bản. Tuy nhiên, trong hầu hết các tình huống thực tế, các thực thể của các phương thức khác nhau tương tác với nhau theo những cách phức tạp và đa mặt hơn, vượt ra ngoài ánh xạ một-một. Chúng tôi đề xuất biểu diễn những mối quan hệ phức tạp này dưới dạng đồ thị, cho phép chúng ta nắm bắt dữ liệu với bất kỳ số lượng phương thức nào, và với các mối quan hệ phức tạp giữa các phương thức có thể linh hoạt thay đổi từ mẫu này sang mẫu khác.
Hướng tới mục tiêu này, chúng tôi đề xuất Học Đồ Thị Đa Phương Thức (MMGL), một khung làm việc tổng quát và có hệ thống để nắm bắt thông tin từ nhiều hàng xóm đa phương thức với cấu trúc quan hệ giữa chúng. Cụ thể, chúng tôi tập trung vào MMGL cho các nhiệm vụ sinh tạo, xây dựng dựa trên các Mô hình Ngôn ngữ được đào tạo trước (LM), nhằm tăng cường việc sinh văn bản của chúng với bối cảnh hàng xóm đa phương thức. Chúng tôi nghiên cứu ba câu hỏi nghiên cứu được đặt ra bởi MMGL: (1) làm thế nào chúng ta có thể truyền nhiều thông tin hàng xóm vào các LM được đào tạo trước, đồng thời tránh các vấn đề về khả năng mở rộng? (2) làm thế nào chúng ta có thể truyền thông tin cấu trúc đồ thị giữa các hàng xóm đa phương thức vào LM? và (3) làm thế nào chúng ta có thể tinh chỉnh các LM được đào tạo trước để học từ bối cảnh hàng xóm theo cách hiệu quả về tham số? Chúng tôi tiến hành các thí nghiệm mở rộng để trả lời ba câu hỏi này về MMGL và phân tích các kết quả thực nghiệm để mở đường cho nghiên cứu MMGL trong tương lai.
1 Giới thiệu
Có các phương thức dữ liệu đa dạng trong các ứng dụng thực tế, từ văn bản, hình ảnh và video thường được quan sát đến dữ liệu chuỗi thời gian hoặc các phương thức đặc thù về lĩnh vực như chuỗi protein. Những phương thức khác nhau này không được thu thập riêng lẻ mà cùng với các mối quan hệ đa mặt giữa chúng.
Wikipedia [2] là một trong những nguồn nội dung web đa phương thức phổ biến nhất, cung cấp dữ liệu đa phương thức như văn bản, hình ảnh và chú thích. TimeBuilder [29], vừa được phát hành bởi Meta, xây dựng dòng thời gian cá nhân sử dụng dữ liệu đa phương thức của mỗi người dùng, bao gồm ảnh, bản đồ, mua sắm và lịch sử âm nhạc của họ. Ngoài những ví dụ này, các quyết định quan trọng trong công nghiệp và y tế cũng được đưa ra bằng cách xem xét dữ liệu đa phương thức đa dạng như hình ảnh, bảng hoặc âm thanh [13,26]. Những dữ liệu đa phương thức này có các mối quan hệ nhiều-nhiều phức tạp giữa các thực thể đa phương thức của chúng - có thể được biểu diễn dưới dạng đồ thị - cung cấp không gian nghiên cứu mở về cách hiểu chúng một cách toàn diện.
Với sự gia tăng của các tập dữ liệu đa phương thức, nhiều nghiên cứu đột phá đã được thực hiện trong học đa phương thức. Trước đây, học đa phương thức tập trung vào các kiến trúc mới, mở rộng transformer [9,19,30] hoặc mạng nơ-ron đồ thị [12,25], và đào tạo chúng từ đầu sử dụng các tập dữ liệu đa phương thức quy mô lớn. Được thúc đẩy bởi khả năng sinh mạnh mẽ của các Mô hình Ngôn ngữ được đào tạo trước (LM), các phương pháp đa phương thức gần đây [1,17,16] được xây dựng dựa trên LM được đào tạo trước và tập trung vào việc sinh tạo

--- TRANG 2 ---
(a) Học Đa Phương Thức 1-1
(b) Học Đồ Thị Đa Phương Thức nhiều-nhiều
Hình 1: Tập dữ liệu đa phương thức được trích xuất từ Wikipedia: (a) Hầu hết các mô hình đa phương thức nhắm vào tập dữ liệu đa phương thức với ánh xạ 1-1 rõ ràng giữa các phương thức. (b) Học Đồ Thị Đa Phương Thức (MMGL) xử lý tập dữ liệu đa phương thức với các mối quan hệ phức tạp giữa nhiều hàng xóm đa phương thức.
nội dung đa phương thức. Ví dụ, [16] tạo ra hình ảnh/văn bản dựa trên văn bản/hình ảnh đã cho bằng cách sử dụng bộ mã hóa hình ảnh được đào tạo trước và LM. Tuy nhiên, tất cả các mô hình hiện có đều giả định rằng một cặp phương thức với ánh xạ 1-1 rõ ràng được cung cấp làm đầu vào (ví dụ, cặp hình ảnh-chú thích trong Hình 1(a)). Do đó, chúng không thể được áp dụng trực tiếp trên tập dữ liệu đa phương thức với ánh xạ nhiều-nhiều tổng quát hơn giữa các phương thức (ví dụ, trang web Wikipedia đa phương thức trong Hình 1(b)).
Ở đây, chúng tôi mở rộng phạm vi của học đa phương thức vượt ra ngoài ánh xạ 1-1 vào học đồ thị đa phương thức (MMGL) đồng thời bảo tồn khả năng sinh tạo bằng cách tích hợp chúng vào LM được đào tạo trước. Chúng tôi giới thiệu một khung làm việc có hệ thống về cách MMGL xử lý thông tin hàng xóm đa phương thức với cấu trúc đồ thị giữa chúng và sinh văn bản tự do bằng cách sử dụng LM được đào tạo trước (Hình 2). Khung làm việc MMGL của chúng tôi trích xuất mã hóa hàng xóm, kết hợp chúng với thông tin cấu trúc đồ thị, và tối ưu hóa mô hình bằng cách sử dụng tinh chỉnh hiệu quả về tham số. Theo đó, chúng tôi định nghĩa ba không gian thiết kế để nghiên cứu ba câu hỏi nghiên cứu cho MMGL như sau:
•Câu hỏi Nghiên cứu 1. Làm thế nào chúng ta có thể cung cấp thông tin hàng xóm đa phương thức cho LM đồng thời tránh các vấn đề về khả năng mở rộng?
•Câu hỏi Nghiên cứu 2. Làm thế nào chúng ta có thể truyền thông tin cấu trúc đồ thị giữa các hàng xóm đa phương thức vào LM?
•Câu hỏi Nghiên cứu 3. Làm thế nào chúng ta có thể tinh chỉnh LM được đào tạo trước để học thông qua thông tin hàng xóm đa phương thức theo cách hiệu quả về tham số?
Trong học đa phương thức thông thường với giả định ánh xạ 1-1, thông thường chỉ có một hàng xóm được cung cấp (ví dụ, một hình ảnh cho một chú thích văn bản) [1,16,17]. Ngược lại, MMGL yêu cầu xử lý nhiều hàng xóm với các kích thước dữ liệu khác nhau (ví dụ, độ phân giải hình ảnh và chuỗi văn bản có độ dài khác nhau), điều này dẫn đến vấn đề về khả năng mở rộng. Đối với Câu hỏi Nghiên cứu 1, chúng tôi nghiên cứu ba mô hình mã hóa hàng xóm: (1) Self-Attention với Text + Embeddings (SA-Text+Embeddings) tính toán trước embedding hình ảnh bằng cách sử dụng bộ mã hóa đông cứng, sau đó nối chúng với chuỗi văn bản đầu vào với bất kỳ văn bản thô nào từ hàng xóm (ban đầu được đề xuất từ [31]), (2) Self-Attention với Embeddings (SA-Embeddings) tính toán trước embedding cho cả phương thức văn bản và hình ảnh bằng cách sử dụng bộ mã hóa đông cứng và nối với văn bản đầu vào, và (3) Cross-Attention với Embeddings (CA-Embeddings) đưa embedding văn bản hoặc hình ảnh được tính toán trước vào các lớp cross-attention của LM.
Trong Câu hỏi Nghiên cứu 2, chúng tôi nghiên cứu cách truyền thông tin cấu trúc đồ thị giữa các hàng xóm đa phương thức vào LM (ví dụ, phân cấp mục và thứ tự hình ảnh trong Hình 1(b)). Chúng tôi so sánh mã hóa vị trí tuần tự với hai mã hóa vị trí đồ thị được sử dụng rộng rãi trong transformer đồ thị [24,34]: mã hóa vị trí eigenvector Laplacian (LPE) [6] và mã hóa mạng nơ-ron đồ thị (GNN) [15] chạy GNN trên embedding hàng xóm được tính toán trước sử dụng cấu trúc đồ thị trước khi đưa chúng vào LM.
Câu hỏi Nghiên cứu 3 tìm cách cải thiện hiệu quả chi phí và bộ nhớ so với tinh chỉnh toàn bộ LM. Trong công việc này, chúng tôi khám phá ba phương pháp tinh chỉnh hiệu quả về tham số (PEFT) [10]: Prefix tuning [18], LoRA [11], và Flamingo tuning [1]. Phương pháp PEFT nào sử dụng phụ thuộc vào
2

--- TRANG 3 ---
mô hình mã hóa hàng xóm: khi thông tin hàng xóm được nối vào chuỗi đầu vào (SA-Text+Embeddings hoặc SA-Embeddings neighbor encodings), chúng ta có thể áp dụng Prefix tuning hoặc LoRA cho tinh chỉnh. Khi thông tin hàng xóm được đưa vào các lớp cross-attention (CA-Embeddings neighbor encoding), chúng ta áp dụng Flamingo tuning chỉ tinh chỉnh các lớp cross-attention với các module gating để tinh chỉnh ổn định [1].
Dựa trên khung làm việc MMGL của chúng tôi, chúng tôi chạy các thí nghiệm mở rộng trên tập dữ liệu đa phương thức được phát hành gần đây, WikiWeb2M [2]. WikiWeb2M hợp nhất nội dung mỗi trang web Wikipedia để bao gồm tất cả văn bản, hình ảnh và cấu trúc của chúng trong một ví dụ duy nhất. Điều này làm cho nó hữu ích cho việc nghiên cứu hiểu nội dung đa phương thức với mối quan hệ văn bản và hình ảnh nhiều-nhiều, trong bối cảnh các nhiệm vụ sinh tạo.
Ở đây, chúng tôi tập trung vào nhiệm vụ tóm tắt phần nhằm tạo ra một câu nắm bắt thông tin về nội dung của một phần bằng cách hiểu nội dung đa phương thức trên mỗi trang Wikipedia. Thông qua kiểm tra nghiêm ngặt trên WikiWeb2M, chúng tôi cung cấp câu trả lời thực nghiệm trực quan cho các câu hỏi nghiên cứu được đặt ra trong MMGL.
Tóm lại, đóng góp của chúng tôi bao gồm:
•Học Đồ Thị Đa Phương Thức (MMGL): Chúng tôi giới thiệu một khung làm việc MMGL có hệ thống để xử lý thông tin hàng xóm đa phương thức với cấu trúc đồ thị giữa chúng, và sinh văn bản tự do bằng cách sử dụng LM được đào tạo trước.
•Các Câu hỏi Nghiên cứu Có Nguyên tắc: Chúng tôi giới thiệu ba vấn đề nghiên cứu mà MMGL cần trả lời: (1) làm thế nào để cung cấp thông tin hàng xóm đa phương thức cho LM được đào tạo trước, (2) làm thế nào để truyền thông tin cấu trúc đồ thị vào LM, và (3) làm thế nào để tinh chỉnh LM một cách hiệu quả về tham số. Điều này mở đường cho các hướng nghiên cứu cho nghiên cứu MMGL trong tương lai.
•Kết quả Thực nghiệm Mở rộng: Chúng tôi cho thấy thực nghiệm rằng (1) bối cảnh hàng xóm cải thiện hiệu suất sinh, (2) SA-Text+Embeddings neighbor encoding cho thấy hiệu suất cao nhất trong khi hy sinh khả năng mở rộng, (3) GNN embeddings là mã hóa vị trí đồ thị hiệu quả nhất, và (4) SA-Text+Embeddings neighbor encoding với LoRA và CA-Embeddings neighbor encoding với Flamingo tuning cho thấy hiệu suất cao nhất trong các mô hình PEFT khác nhau.
Mã nguồn của chúng tôi được công bố tại1.
2 Công trình Liên quan
Học Đa Phương Thức End-to-End: Mặc dù nhiều mô hình đa phương thức phân biệt [14,22] cũng đã được phát triển, chúng tôi chủ yếu xem xét công trình liên quan về các mô hình đa phương thức sinh tạo, vì điều này liên quan chặt chẽ nhất với phương pháp của chúng tôi. Một số phương pháp gần đây giải quyết học đa phương thức bằng cách xây dựng dựa trên kiến trúc Transformer [32]. Các mở rộng đa phương thức thường sử dụng hoặc full self-attention trên các phương thức được nối theo chiều chuỗi [3,28] hoặc một lớp cross-modal attention [30]. Các phương pháp pretraining đa phương thức tự giám sát đào tạo những kiến trúc này từ dữ liệu đa phương thức quy mô lớn không có nhãn trước khi chuyển chúng đến các nhiệm vụ đa phương thức downstream thông qua tinh chỉnh [9,19]. Những phương pháp này thực hiện pre-training end-to-end, tốn chi phí tính toán cực kỳ cao, đặc biệt khi tham số mô hình tăng [17]. Hơn nữa, khung làm việc này tương đối không linh hoạt đối với các mô hình được pre-trained end-to-end để tận dụng các mô hình pre-trained đơn phương thức sẵn có, như LM chỉ văn bản hoặc các mô hình vision được đào tạo trước.
Học Đa Phương Thức với Bộ Mã hóa Hình ảnh Đông cứng và Mô hình Ngôn ngữ Lớn: Gần đây, nhiều mô hình vision-language khác nhau đã được đề xuất để tận dụng các mô hình pre-trained có sẵn và giữ chúng đông cứng trong quá trình pretraining [1,17,16]. Để đưa thông tin thị giác trực tiếp vào LLM chỉ văn bản đông cứng, một thách thức chính là căn chỉnh các đặc trưng thị giác với không gian văn bản. Được thúc đẩy bởi Frozen [31], tinh chỉnh một bộ mã hóa thị giác để ánh xạ hình ảnh vào không gian ẩn của LLM chỉ văn bản, Blip-2 [17] và GILL [16] tinh chỉnh các mạng ánh xạ hình ảnh riêng biệt có đầu vào được tính toán trước bởi bộ mã hóa hình ảnh đông cứng và đầu ra được sử dụng trực tiếp như soft prompts cho LLM. Mặt khác, Flamingo [1] chèn các lớp cross-attention mới vào LLM để tiêm các đặc trưng thị giác và pre-train các lớp mới trên cặp image-text. Lưu ý rằng tất cả các phương pháp này chủ yếu tập trung vào việc xử lý đầu vào hình ảnh và văn bản xen kẽ để tạo ra đầu ra văn bản.
Mạng Nơ-ron Đồ thị trên Đồ thị Đa Phương Thức Mạng Nơ-ron Đồ thị Dị thể (HGNN) mở rộng Mạng Nơ-ron Đồ thị (GNN) [36] để học từ đồ thị dị thể đa phương thức
3

--- TRANG 4 ---
(a) Hàng xóm đa phương thức với cấu trúc đồ thị
(b) Mã hóa hàng xóm đa phương thức bằng bộ mã hóa vision/text đông cứng
(c) Mã hóa vị trí đồ thị
(d) Sinh văn bản bằng LM đông cứng/tinh chỉnh và thông tin hàng xóm được mã hóa
Hình 2: Khung làm việc Học Đồ Thị Đa Phương Thức (MMGL): (a) Nhiều hàng xóm đa phương thức được cung cấp với văn bản đầu vào. (b) Hàng xóm đa phương thức được mã hóa trước bằng bộ mã hóa vision/text đông cứng và sau đó được căn chỉnh với không gian LM chỉ văn bản bằng cách sử dụng mapper MLP 1 lớp. Các mapper được đào tạo trong quá trình tinh chỉnh LM. Dựa trên lược đồ mã hóa hàng xóm, văn bản có thể được sử dụng mà không cần bất kỳ tiền xử lý nào (Self-Attention với Text+Embeddings) hoặc được mã hóa thành embedding (Self-Attention với Embeddings hoặc Cross-Attention với Embeddings). Hình ảnh luôn được mã hóa thành embedding để căn chỉnh với không gian LM chỉ văn bản. (c) Cấu trúc đồ thị giữa các hàng xóm được mã hóa như mã hóa vị trí đồ thị. (d) Thông tin hàng xóm được mã hóa có thể được truyền hoặc bằng cách nối với chuỗi đầu vào (Self-Attention với Text+Embeddings hoặc Self-Attention với Embeddings) hoặc đưa vào các lớp cross-attention (Cross-Attention với Embeddings). Mã hóa vị trí đồ thị được thêm vào token đầu vào/text/image embeddings.

đồ thị. Điều này được thực hiện thông qua việc tính toán trước embedding nút đầu vào bằng cách sử dụng bộ mã hóa đông cứng, và đào tạo GNN để ánh xạ embedding phương thức khác nhau ở lớp đầu vào [25], trung gian [12], hoặc lớp cuối [35]. Tuy nhiên, hầu hết các mô hình HGNN tập trung vào phân loại nút, và khó thích ứng cho các nhiệm vụ sinh tạo. Gần đây, nhiều phương pháp khác nhau đã được đề xuất để tinh chỉnh LLM với GNN trên đồ thị có thuộc tính văn bản [4,8,38]. Những phương pháp này chuyên về các nhiệm vụ phân loại nút/cạnh bằng cách đặt các mô hình GNN sau LLM, làm cho chúng khó thích ứng để sử dụng trong các nhiệm vụ sinh tạo.
3 Học Đồ Thị Đa Phương Thức cho Các Nhiệm Vụ Sinh Tạo
Cho đồ thị đa phương thức với văn bản hoặc hình ảnh trên mỗi nút, chúng ta nhằm sinh văn bản có điều kiện trên mỗi nút và các nút hàng xóm của nó. Cụ thể hơn, cho đầu vào văn bản trên nút đích, LM được đào tạo trước sinh văn bản tự do có điều kiện trên văn bản đầu vào và bối cảnh đa phương thức xung quanh nút đích. Trong khung làm việc học đồ thị đa phương thức (MMGL) của chúng tôi, chúng tôi đầu tiên mã hóa thông tin của mỗi hàng xóm riêng lẻ bằng cách sử dụng bộ mã hóa đông cứng (Hình 2(b)). Các bộ mã hóa đông cứng có thể là ViT được đào tạo trước [5] hoặc ResNeT [7] cho hình ảnh ánh xạ pixel thành embedding, và LM được đào tạo trước [22] cho văn bản ánh xạ văn bản thành embedding (tương tự cho các phương thức khác). Sau đó, chúng tôi mã hóa cấu trúc đồ thị xung quanh nút đích bằng cách sử dụng mã hóa vị trí đồ thị (Hình 2(c)). Cuối cùng, thông tin hàng xóm được mã hóa với mã hóa vị trí đồ thị được đưa vào LM được đào tạo trước với văn bản đầu vào để sinh văn bản có điều kiện trên nội dung đầu vào đa phương thức (Hình 2(d)).
Khung làm việc để lại cho chúng ta ba không gian thiết kế: (1) làm thế nào chúng ta có thể đưa thông tin hàng xóm vào LM? (2) làm thế nào chúng ta có thể truyền thông tin cấu trúc đồ thị giữa các hàng xóm đa phương thức vào LM? (3) làm thế nào chúng ta có thể tinh chỉnh LM được đào tạo trước để học từ bối cảnh hàng xóm một cách hiệu quả về tham số? Trong phần này, chúng tôi điều tra từng vấn đề và thảo luận về các phương pháp có thể áp dụng.
4

--- TRANG 5 ---
3.1 Câu hỏi Nghiên cứu 1: Mã hóa Hàng xóm
Không giống như học đa phương thức hiện tại, giả định một hình ảnh duy nhất (tương ứng với văn bản đầu vào) làm đầu vào, học đồ thị đa phương thức xem xét số lượng tùy ý các hình ảnh/văn bản hàng xóm làm đầu vào; do đó, khả năng mở rộng là vấn đề đầu tiên cần giải quyết để học từ nhiều hàng xóm đa phương thức.
Trong các mô hình vision-text, một công thức tiêu chuẩn là trước tiên xử lý hình ảnh bằng một bộ mã hóa hình ảnh (ví dụ, ViT, ResNet) thành embedding hình ảnh, sau đó ánh xạ các embedding vào không gian LM chỉ văn bản, và cuối cùng đưa chúng vào LM. Hai cách phổ biến để đưa embedding hình ảnh vào LM là với full self-attention trên các phương thức được nối theo chiều chuỗi [31] hoặc với các lớp cross-modal attention [30].
Được thúc đẩy bởi hai phương pháp này, chúng tôi đề xuất ba phương pháp mã hóa hàng xóm như sau:
•Self-Attention với Text + Embeddings (SA-Text+Embeddings): Hàng xóm văn bản được nối như văn bản thô, trong khi các phương thức khác được xử lý trước bởi bộ mã hóa đông cứng (ví dụ, ViT cho hình ảnh), và sau đó embedding của chúng được nối với chuỗi đầu vào. Chúng tôi thêm một mapper tuyến tính căn chỉnh embedding được tính toán trước vào không gian văn bản của LLM.
•Self-Attention với Embeddings (SA-Embeddings): Giống như SA-Text+Embeddings ngoại trừ hàng xóm văn bản cũng được xử lý bởi bộ mã hóa đông cứng riêng biệt, và embedding của chúng được nối với chuỗi đầu vào. Bộ mã hóa văn bản có thể giống hoặc khác với mô hình LLM cơ sở.
•Cross-Attention với Embeddings (CA-Embeddings): Tất cả hàng xóm được xử lý bởi bộ mã hóa đông cứng riêng biệt, được ánh xạ vào không gian văn bản bởi mapper tuyến tính, và sau đó được đưa vào các lớp cross-attention.
Nói chung, khi chúng ta cung cấp embedding văn bản thay vì văn bản thô, lượng thông tin mà LLM có thể khai thác bị hạn chế bởi embedding được tính toán trước. Tuy nhiên, văn bản thô gây ra các vấn đề về khả năng mở rộng vì cơ chế attention của LM sử dụng tính toán O(T²) với độ dài chuỗi T. Do đó, có sự đánh đổi giữa chi phí tính toán và khả năng mở rộng. Đối với SA-Text+Embeddings và SA-Embeddings, chúng ta chỉ có tham số bổ sung cho mapper nằm bên ngoài LM, trong khi CA-Embeddings chèn các lớp cross-attention bổ sung vào LM được đào tạo trước và đào tạo chúng từ đầu. Điều này có nghĩa là CA-Embeddings có thể dẫn đến trạng thái ban đầu không ổn định vì các lớp LLM được đào tạo trước bị ảnh hưởng bởi các lớp cross-attention được khởi tạo ngẫu nhiên.
Trong Phần 4.4, chúng tôi khám phá ba phương pháp này và thảo luận về kết quả thực nghiệm của chúng.
3.2 Câu hỏi Nghiên cứu 2: Mã hóa Cấu trúc Đồ thị
Cho thông tin hàng xóm, chúng ta có thể đơn giản nối thông tin hàng xóm hoặc như văn bản thô hoặc embedding và coi chúng như một chuỗi. Nhưng các hàng xóm có cấu trúc giữa chúng. Ví dụ, các phần có cấu trúc phân cấp, và hình ảnh được bao gồm trong các phần nhất định trong WikiWeb2M (Hình 1(b)). Để mã hóa cấu trúc đồ thị này giữa thông tin hàng xóm, chúng tôi mượn hai mã hóa vị trí đồ thị phổ biến từ transformer đồ thị và so sánh chúng với mã hóa vị trí tuần tự.
•Mã hóa Vị trí Laplacian (LPE): Chúng tôi khai thác eigenvector Laplacian của hàng xóm được tính toán từ cấu trúc đồ thị của chúng như mã hóa vị trí của chúng.
•Mạng Nơ-ron Đồ thị (GNN): Chúng tôi trước tiên tính toán embedding hàng xóm từ bộ mã hóa đông cứng và chạy GNN trên embedding bằng cách sử dụng cấu trúc đồ thị. Sau đó, chúng tôi sử dụng embedding GNN đầu ra, mã hóa thông tin cấu trúc đồ thị như mã hóa vị trí.
LPE có mapper MLP 1 lớp bổ sung để ánh xạ eigenvector Laplacian vào không gian văn bản của LM. Các tham số được sử dụng cho mã hóa cấu trúc đồ thị (ví dụ, mapper cho LPE hoặc tham số GNN) được đào tạo với LM theo cách end-to-end trong quá trình tinh chỉnh LM. Trong Phần 4.5, chúng tôi khám phá mức độ hiệu quả của các mã hóa vị trí khác nhau này mang thông tin cấu trúc đồ thị bổ sung giữa các hàng xóm vào LM và cải thiện hiệu suất.
5

--- TRANG 6 ---
3.3 Câu hỏi Nghiên cứu 3: Hiệu quả Tham số
Trong khi chúng ta cần tinh chỉnh mô hình LM được đào tạo trước cho nhiệm vụ cụ thể và thông tin hàng xóm mới được thêm vào, tinh chỉnh toàn bộ đòi hỏi chi phí tính toán cao và cũng mang lại bất tiện trong việc chia sẻ các module MMGL khi người dùng quyết định sử dụng thông tin hàng xóm. Gần đây, nhiều phương pháp tinh chỉnh hiệu quả về tham số (PEFT) khác nhau đã được đề xuất để tinh chỉnh chỉ một lượng nhỏ tham số trong khi bảo tồn hiệu suất tinh chỉnh toàn bộ. Chúng tôi chọn ba mô hình PEFT khác nhau phù hợp cho ba phương pháp mã hóa hàng xóm chúng tôi đã mô tả ở trên.
•Prefix tuning: Khi chúng ta chọn SA-Text+Embeddings hoặc SA-Embeddings cho mã hóa hàng xóm, chúng ta không có bất kỳ tham số mới được thêm vào nào ngoài các lớp self-attention; do đó, chúng ta có thể dễ dàng áp dụng Prefix tuning [18], giữ tham số mô hình ngôn ngữ đông cứng và thay vào đó tối ưu hóa một chuỗi vector liên tục cụ thể cho nhiệm vụ được thêm vào trước các vector kích hoạt ban đầu qua tất cả các lớp.
•LoRA: Giống như Prefix tuning, low-rank adaptation (LoRA) [11] phù hợp cho SA-Text+Embeddings hoặc SA-Embeddings neighbor encodings. LoRA tiêm các ma trận phân rã thứ hạng có thể đào tạo vào mỗi lớp trong khi đông cứng các tham số ban đầu.
•Flamingo: Đối với CA-Embeddings neighbor encoding, chúng ta có thể áp dụng trực tiếp Flamingo [1], chỉ tinh chỉnh các lớp cross-attention mới được thêm vào với tanh gating để giữ LM được đào tạo trước nguyên vẹn tại khởi tạo để cải thiện độ ổn định và hiệu suất.
Trong Phần 4.6, chúng tôi khám phá mức độ hiệu quả của các mô hình PEFT bảo tồn hiệu suất tinh chỉnh toàn bộ bằng cách điều chỉnh một số lượng nhỏ tham số.
4 Thí nghiệm
4.1 Tập dữ liệu WikiWeb2M
Tập dữ liệu WikiWeb2M [2] được xây dựng cho nghiên cứu tổng quát về hiểu nội dung đa phương thức với mối quan hệ văn bản và hình ảnh nhiều-nhiều. Được xây dựng dựa trên tập dữ liệu WIT [27] chỉ chứa các cặp image-caption, WikiWeb2M bao gồm tiêu đề trang, tiêu đề phần, văn bản phần, hình ảnh và chú thích của chúng, và chỉ số cho mỗi phần, phần cha của chúng, phần con của chúng, và nhiều hơn nữa.
Trong công việc này, chúng tôi tập trung vào nhiệm vụ tóm tắt phần để tạo ra một câu đơn làm nổi bật nội dung của một phần cụ thể. Bản tóm tắt được tạo ra cho tất cả hình ảnh và văn bản (không phải tóm tắt) có mặt trong phần đích và phần bối cảnh. Chúng tôi lấy mẫu 600k trang Wikipedia ngẫu nhiên từ WikiWeb2M cho nhiệm vụ tóm tắt phần. Tổng cộng, kích thước tập train/validation/test cho nhiệm vụ tóm tắt phần là 680k/170k/170k, tương ứng.
4.2 Cài đặt Thí nghiệm
Từ WikiWeb2M, chúng ta có thể nhận được bốn loại thông tin cho tóm tắt phần: (1) văn bản phần, (2) hình ảnh phần, (3) văn bản từ mô tả trang và các phần khác, và (4) hình ảnh từ mô tả trang và các phần khác. Chúng tôi cung cấp thông tin một cách tăng dần cho LM để nghiên cứu tính hiệu quả của thông tin hàng xóm đa phương thức: (1) văn bản phần, 2) tất cả phần (văn bản + hình ảnh), 3) văn bản trang (tất cả văn bản từ một trang Wikipedia mà phần đầu vào thuộc về), và 4) tất cả trang (tất cả văn bản và hình ảnh từ trang Wikipedia).
Chúng tôi sử dụng Open Pre-trained Transformer (OPT-125m) [37] cho LM cơ sở để đọc văn bản phần đầu vào và tạo ra bản tóm tắt. Đối với bộ mã hóa văn bản và hình ảnh cho thông tin hàng xóm, chúng tôi sử dụng bộ mã hóa văn bản/hình ảnh từ CLIP [22]. Theo [23], chúng tôi tinh chỉnh OPT trong 10000 bước với kích thước batch 125 và tỷ lệ học 10⁻⁴. Bộ mã hóa văn bản/hình ảnh được đông cứng trong tất cả các thí nghiệm. Chúng tôi đo điểm BLEU-4 [21], ROUGE-L [20], và CIDEr [33] trên tập validation. Tất cả các thí nghiệm được chạy trên 4 GPU Nvidia-RTX 3090 với bộ nhớ 24GB.
4.3 Hiệu quả của Thông tin Hàng xóm
Chúng tôi đầu tiên kiểm tra tính hiệu quả của thông tin hàng xóm đa phương thức. Như mô tả trong Phần 4.2, chúng tôi cung cấp thêm thông tin một cách tăng dần cho LM cơ sở: (1) văn bản phần, (2) tất cả phần (văn bản + hình ảnh), 3) văn bản trang, và 4) tất cả trang (tất cả văn bản và hình ảnh). Ở đây, chúng tôi sử dụng Self-Attention với Text+Embeddings (SA-Text+Embeddings) neighbor encoding qua các loại đầu vào khác nhau. Đối với hình ảnh, chúng tôi đầu tiên tính toán embedding hình ảnh từ bộ mã hóa hình ảnh CLIP đông cứng và nối chúng
6

--- TRANG 7 ---
Bảng 1: Hiệu quả của thông tin hàng xóm: Khi thêm thông tin hàng xóm vào LM cùng với văn bản đầu vào (văn bản phần, tất cả phần => văn bản trang, tất cả trang), hiệu suất sinh được cải thiện. Chúng tôi tăng độ dài chuỗi đầu vào lên 1024 để mã hóa văn bản trang và tất cả trang vì cần mã hóa nhiều thông tin hơn.
Kết quả tốt nhất được tô màu đỏ, trong khi kết quả tốt thứ hai được tô màu xanh.

Loại đầu vào | Độ dài đầu vào | BLEU-4 | ROUGE-L | CIDEr
Văn bản phần | 512 | 8.31 | 40.85 | 79.68
Tất cả phần | 512 | 8.03 | 40.41 | 77.45
Văn bản trang | 1024 | 9.81 | 42.94 | 92.71
Tất cả trang | 1024 | 9.96 | 43.32 | 96.62

Bảng 2: Mã hóa hàng xóm trong MMGL: Chúng tôi mã hóa thông tin hàng xóm đa phương thức bằng ba mã hóa hàng xóm khác nhau, Self-Attention với Text+Embeddings (SA-TE), Self-Attention với Embeddings (SA-E), và Cross-Attention với Embeddings (CA-E). Trong khi SA-TE cho thấy hiệu suất tốt nhất, SA-TE yêu cầu độ dài đầu vào dài hơn (1024) để mã hóa văn bản từ hàng xóm ngoài văn bản đầu vào ban đầu, dẫn đến vấn đề khả năng mở rộng. Kết quả tốt nhất được tô màu đỏ.

[Bảng với kết quả BLEU-4, ROUGE-L, CIDEr cho SA-TE, SA-E, CA-E với các loại đầu vào khác nhau]

ngay sau văn bản của phần mà mỗi hình ảnh thuộc về để bảo tồn cấu trúc. Kết quả trong Bảng 1 chỉ ra rằng thông tin hàng xóm đa phương thức nhiều hơn là hữu ích: hiệu suất được cải thiện đáng kể khi chuyển từ nội dung phần sang trang, và hơn nữa khi thêm tất cả nội dung trang, dựa trên điểm BLEU-4, ROUGE-L, và CIDEr của chúng.
Thảo luận: Phương thức Bị thiếu. Hiệu suất của tất cả phần giảm nhẹ so với văn bản phần, mặc dù có thêm hình ảnh phần. Trong Wikipedia, không phải mọi phần đều có hình ảnh tương ứng. Do đó, trong trường hợp tất cả phần, đầu vào cho LM không nhất quán với một số mẫu có văn bản và hình ảnh, trong khi các mẫu khác chỉ có văn bản. Điều này chỉ ra một vấn đề phương thức bị thiếu quan trọng chưa được giải quyết phổ biến trong thế giới thực, thường không gặp trong cài đặt đa phương thức 1-1 thông thường, nhấn mạnh tầm quan trọng của việc phát triển các phương pháp MMGL mạnh mẽ trước sự hiện diện của các phương thức bị thiếu.

4.4 Mã hóa Hàng xóm
Chúng tôi mã hóa thông tin hàng xóm đa phương thức bằng ba mã hóa hàng xóm khác nhau, Self-Attention với Text+Embeddings (SA-TE), Self-Attention với Embeddings (SA-E), và Cross-Attention với Embeddings (CA-E). Trong khi SA-E và CA-E mã hóa tất cả các phương thức, bao gồm văn bản, thành embedding bằng cách sử dụng bộ mã hóa đông cứng, SA-TE mã hóa hàng xóm văn bản như chúng bằng cách nối chúng với chuỗi văn bản đầu vào. Do đó SA-TE yêu cầu độ dài chuỗi đầu vào dài hơn (1024) để mã hóa văn bản bổ sung, dẫn đến vấn đề khả năng mở rộng tiềm năng. Mặt khác, SA-E và CA-E yêu cầu một độ dài token để mã hóa một hàng xóm văn bản, cải thiện khả năng mở rộng với độ dài đầu vào ngắn hơn (512).
Kết quả trong Bảng 2 chỉ ra rằng khả năng mở rộng được đánh đổi với hiệu suất: SA-TE nhất quán hoạt động tốt hơn SA-E và CA-E trên các loại đầu vào khác nhau với chi phí độ dài đầu vào dài hơn.
Thảo luận: Mất thông tin. Trong học đa phương thức thông thường với ánh xạ 1-1, SA-TE thường được sử dụng để truyền đầu vào văn bản như nó là, và đầu vào hình ảnh như embedding được tính toán trước bởi bộ mã hóa đông cứng [1,16,17]. Những phương pháp này thành công sinh văn bản dựa trên hình ảnh đầu vào, cho thấy hiệu quả của embedding hình ảnh như đầu vào cho LM được đào tạo trước. Tuy nhiên, khoảng cách hiệu suất giữa SA-TE và SA-E trong Bảng 2 chỉ ra rằng embedding văn bản có thể dẫn đến mất thông tin trong LM. Điều này có thể là do mapper MLP 1 lớp căn chỉnh embedding văn bản được tính toán trước vào không gian văn bản của LM không đủ biểu cảm, hoặc do văn bản đầu vào dài hơn
7

--- TRANG 8 ---
Bảng 3: Mã hóa cấu trúc đồ thị trong MMGL: Chúng tôi mã hóa cấu trúc đồ thị giữa các hàng xóm đa phương thức bằng cách sử dụng mã hóa vị trí tuần tự (Sequence), embedding Mạng Nơ-ron Đồ thị (GNN), và mã hóa vị trí Laplacian (LPE). Mã hóa vị trí được tính toán được thêm vào token đầu vào/văn bản/hình ảnh embedding và đưa vào LM. Chúng tôi sử dụng Self-Attention với Embeddings (SA-E) neighbor encoding và Prefix tuning trong thí nghiệm này. Kết quả tốt nhất được tô màu đỏ.

Metric | PEFT | Sequence | GNN | LPE
BLEU-4 | Prefix tuning | 6.91 | 6.98 | 6.80
       | LoRA | 7.12 | 7.30 | 7.13
ROUGE-L | Prefix tuning | 38.98 | 39.13 | 39.10
        | LoRA | 39.05 | 39.48 | 39.35
CIDEr | Prefix tuning | 68.20 | 69.29 | 68.15
      | LoRA | 68.86 | 70.86 | 69.34

Bảng 4: Tinh chỉnh hiệu quả tham số trong MMGL: Chúng tôi áp dụng Prefix tuning và LoRA cho Self-Attention với Text+Embeddings (SA-TE) và Self-Attention với Embeddings (SA-E) neighbor encodings. Đối với Cross-Attention với Embeddings (CA-E) neighbor encoding, chúng tôi áp dụng tinh chỉnh kiểu Flamingo chỉ tinh chỉnh các lớp cross-attention mới được thêm vào với các module gating. Lưu ý rằng SA-E và CA-E neighbor encodings có nhiều tham số hơn SA-TE vì các bộ mã hóa văn bản (đông cứng) được thêm vào để mã hóa hàng xóm văn bản. Kết quả tốt nhất được tô màu đỏ, trong khi kết quả tốt thứ hai được tô màu xanh.

[Bảng chi tiết với kết quả cho các neighbor encoding khác nhau và các metric tương ứng]

so với văn bản ngắn được sử dụng trong học đa phương thức thông thường (ví dụ, chú thích một câu) làm cho LM khó học từ embedding văn bản được tính toán trước. Từ góc độ thực tế, kết quả của chúng tôi làm sáng tỏ sự đánh đổi giữa khả năng mở rộng và hiệu suất. Trong khi đó, kết quả của chúng tôi nhấn mạnh nhu cầu nghiên cứu MMGL nhiều hơn để giải quyết vấn đề thách thức về mất thông tin khi sử dụng embedding để nắm bắt thông tin văn bản.

4.5 Mã hóa Cấu trúc Đồ thị
Ngoài mỗi phương thức trên hàng xóm, đồ thị đa phương thức chứa thông tin cấu trúc đồ thị giữa các hàng xóm. Chúng tôi mã hóa cấu trúc đồ thị giữa các hàng xóm đa phương thức bằng cách sử dụng mã hóa vị trí tuần tự (Sequence), embedding Mạng Nơ-ron Đồ thị (GNN), và mã hóa vị trí Laplacian (LPE). Mã hóa vị trí được tính toán trước tiên được ánh xạ vào không gian văn bản của LM bằng MLP 1 lớp, được thêm vào token đầu vào/văn bản/hình ảnh embedding, và được đưa vào LM. Trong Bảng 3, embedding GNN cho thấy hiệu suất tốt nhất. Đặc biệt, sự cải thiện so với mã hóa vị trí Sequence cho thấy tầm quan trọng của các phương pháp mã hóa cấu trúc nhận biết đồ thị trong MMGL.
8

--- TRANG 9 ---
4.6 Tinh chỉnh Hiệu quả Tham số
Tinh chỉnh toàn bộ LM được đào tạo trước yêu cầu chi phí tính toán cao. Đối với tinh chỉnh hiệu quả tham số cho MMGL, chúng tôi nghiên cứu Prefix tuning và LoRA cho Self-Attention với Text+Embeddings (SA-TE) và Self-Attention với Embeddings (SA-E) neighbor encodings. Đối với Cross-Attention với Embeddings (CA-E) neighbor encoding, chúng tôi áp dụng tinh chỉnh kiểu Flamingo chỉ tinh chỉnh các lớp cross-attention mới được thêm vào với các module gating.
Kết quả trong Bảng 4 cho thấy LoRA hoạt động tốt hơn Prefix tuning cho SA-TE và SA-E neighbor encodings với nhiều tham số được tinh chỉnh hơn (7-9% cho Prefix tuning và 26-33% cho LoRA). Tuy nhiên, Prefix tuning vẫn cho thấy hiệu suất có thể so sánh với LoRA sử dụng gần 4 lần ít tham số hơn với SA-TE neighbor encoding. Flamingo với CA-E neighbor encoding cho thấy hiệu suất có thể so sánh với LoRA với SA-TE neighbor encoding sử dụng số lượng tham số được tinh chỉnh tương tự (82M cho LoRA và 90M cho Flamingo). Lưu ý rằng SA-E và CA-E neighbor encodings có nhiều tham số hơn SA-TE, do việc bao gồm bộ mã hóa văn bản (đông cứng) để xử lý hàng xóm văn bản.
Trong Bảng 2 (không có PEFT), rõ ràng là CA-E neighbor encoding kém hiệu suất so với SA-TE neighbor encoding. Tuy nhiên, khi được kết hợp với Flamingo, các module gating trong Flamingo hiệu quả đảm bảo rằng LM được đào tạo trước không bị ảnh hưởng bởi các lớp cross-attention được đặt ngẫu nhiên tại khởi tạo, do đó tăng cường hiệu suất của CA-E, như được thể hiện trong Bảng 4 (với PEFT). Điều này nhấn mạnh vai trò then chốt của khởi tạo chiến lược khi giới thiệu các module bổ sung cho neighbor encoding trong MMGL và khi tích hợp chúng vào LM được đào tạo trước.

5 Kết luận
Trong công việc này, chúng tôi mở rộng học đa phương thức thông thường với ánh xạ 1-1 giữa một cặp phương thức vào học đồ thị đa phương thức (MMGL) với mối quan hệ nhiều-nhiều giữa nhiều phương thức. Khung làm việc MMGL của chúng tôi được cấu trúc có hệ thống xung quanh ba thành phần quan trọng: (1) mã hóa hàng xóm, (2) mã hóa cấu trúc đồ thị, và (3) tinh chỉnh hiệu quả tham số. Thông qua kiểm tra nghiêm ngặt trên tập dữ liệu WikiWeb2M, chúng tôi khám phá các tùy chọn khác nhau cho mỗi thành phần: (1) ba biến thể của mã hóa hàng xóm, Self-Attention với Text+Embeddings, Self-Attention với Embeddings, và Cross-Attention với Embeddings, làm nổi bật sự cân bằng giữa khả năng mở rộng và hiệu suất, (2) ba mã hóa vị trí đồ thị khác nhau, sequence, LPE, và GNN, và (3) ba mô hình PEFT, prefix tuning, LoRA, và Flamingo, và sự đánh đổi giữa hiệu quả tham số và hiệu suất. Phân tích sâu và phát hiện của chúng tôi nhằm đặt nền tảng cho nghiên cứu MMGL trong tương lai, khơi dậy thêm khám phá trong lĩnh vực này.

Tài liệu tham khảo
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: một mô hình ngôn ngữ thị giác cho học few-shot. Advances in Neural Information Processing Systems, 35:23716–23736, 2022.

[2] Andrea Burns, Krishna Srinivasan, Joshua Ainslie, Geoff Brown, Bryan A. Plummer, Kate Saenko, Jianmo Ni, và Mandy Guo. Một bộ các nhiệm vụ sinh tạo cho hiểu trang web đa phương thức đa cấp, 2023.

[3] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, và Jingjing Liu. Uniter: Học biểu diễn hình ảnh-văn bản toàn cầu. Trong European conference on computer vision, trang 104–120. Springer, 2020.

[4] Eli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Jiong Zhang, Olgica Milenkovic, và Inderjit S Dhillon. Trích xuất đặc trưng nút bằng dự đoán vùng lân cận đa quy mô tự giám sát. arXiv preprint arXiv:2111.00064, 2021.

[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. Một hình ảnh đáng giá 16x16 từ: Transformer cho nhận dạng hình ảnh ở quy mô. arXiv preprint arXiv:2010.11929, 2020.

[6] Vijay Prakash Dwivedi và Xavier Bresson. Một tổng quát hóa của mạng transformer thành đồ thị. arXiv preprint arXiv:2012.09699, 2020.

[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Học tàn dư sâu cho nhận dạng hình ảnh. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 770–778, 2016.

[8] Xiaoxin He, Xavier Bresson, Thomas Laurent, và Bryan Hooi. Giải thích như đặc trưng: Đặc trưng dựa trên LLM cho đồ thị có thuộc tính văn bản. arXiv preprint arXiv:2305.19523, 2023.

[9] Lisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste Alayrac, và Aida Nematzadeh. Tách rời vai trò của dữ liệu, attention, và loss trong transformer đa phương thức. Transactions of the Association for Computational Linguistics, 9:570–585, 2021.

[10] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. Học chuyển giao hiệu quả tham số cho nlp. Trong International Conference on Machine Learning, trang 2790–2799. PMLR, 2019.

[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. Lora: Thích ứng thứ hạng thấp của các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2106.09685, 2021.

[12] Ziniu Hu, Yuxiao Dong, Kuansan Wang, và Yizhou Sun. Transformer đồ thị dị thể. Trong Proceedings of the web conference 2020, trang 2704–2710, 2020.

[13] Bing Huang, Feng Yang, Mengxiao Yin, Xiaoying Mo, Cheng Zhong, et al. Một đánh giá về các kỹ thuật fusion hình ảnh y tế đa phương thức. Computational and mathematical methods in medicine, 2020, 2020.

[14] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, và Tom Duerig. Mở rộng học biểu diễn thị giác và vision-language với giám sát văn bản nhiễu. Trong International conference on machine learning, trang 4904–4916. PMLR, 2021.

[15] Thomas N Kipf và Max Welling. Phân loại bán giám sát với mạng tích chập đồ thị. arXiv preprint arXiv:1609.02907, 2016.

[16] Jing Yu Koh, Daniel Fried, và Ruslan Salakhutdinov. Sinh hình ảnh với mô hình ngôn ngữ đa phương thức. arXiv preprint arXiv:2305.17216, 2023.

[17] Junnan Li, Dongxu Li, Silvio Savarese, và Steven Hoi. Blip-2: Bootstrapping ngôn ngữ-hình ảnh pre-training với bộ mã hóa hình ảnh đông cứng và mô hình ngôn ngữ lớn. arXiv preprint arXiv:2301.12597, 2023.

[18] Xiang Lisa Li và Percy Liang. Prefix-tuning: Tối ưu hóa prompt liên tục cho sinh tạo. arXiv preprint arXiv:2101.00190, 2021.

[19] Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw, Yudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe Morency, và Russ Salakhutdinov. Transformer đa phương thức có tính đa dạng cao: Định lượng tính dị thể của phương thức & tương tác cho học biểu diễn có tính đa dạng cao. Transactions on Machine Learning Research, 2022.

[20] Chin-Yew Lin. ROUGE: Một gói để đánh giá tự động các bản tóm tắt. Trong Text Summarization Branches Out, trang 74–81, Barcelona, Tây Ban Nha, tháng 7 năm 2004. Association for Computational Linguistics.

[21] Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. Bleu: một phương pháp đánh giá tự động dịch máy. Trong Proceedings of the 40th annual meeting of the Association for Computational Linguistics, trang 311–318, 2002.

[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Học các mô hình thị giác có thể chuyển giao từ giám sát ngôn ngữ tự nhiên. Trong International conference on machine learning, trang 8748–8763. PMLR, 2021.

[23] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Khám phá giới hạn của học chuyển giao với một transformer văn bản-văn bản thống nhất. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.

[24] Ladislav Rampášek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, và Dominique Beaini. Công thức cho một transformer đồ thị tổng quát, mạnh mẽ, có thể mở rộng. Advances in Neural Information Processing Systems, 35:14501–14515, 2022.

[25] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, và Max Welling. Mô hình hóa dữ liệu quan hệ với mạng tích chập đồ thị. Trong The Semantic Web: 15th International Conference, ESWC 2018, Heraklion, Crete, Hy Lạp, 3-7 tháng 6 năm 2018, Proceedings 15, trang 593–607. Springer, 2018.

[26] Prabhjot Singh, Yanyan Wu, Robert Kaucic, Jiaqin Chen, và Francis Little. Kiểm tra và phân tích công nghiệp đa phương thức. 2007.

[27] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, và Marc Najork. Wit: Tập dữ liệu hình ảnh văn bản dựa trên Wikipedia cho học máy đa phương thức đa ngôn ngữ. Trong Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, trang 2443–2449, 2021.

[28] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, và Jifeng Dai. Vl-bert: Pre-training biểu diễn thị giác-ngôn ngữ tổng quát. arXiv preprint arXiv:1908.08530, 2019.

[29] Wang-Chiew Tan, Jane Dwivedi-Yu, Yuliang Li, Lambert Mathias, Marzieh Saeidi, Jing Nathan Yan, và Alon Y Halevy. Timelineqa: Một benchmark cho hỏi đáp trên dòng thời gian. arXiv preprint arXiv:2306.01069, 2023.

[30] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, và Ruslan Salakhutdinov. Transformer đa phương thức cho chuỗi ngôn ngữ đa phương thức không căn chỉnh. Trong Proceedings of the conference. Association for Computational Linguistics. Meeting, tập 2019, trang 6558. NIH Public Access, 2019.

[31] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, và Felix Hill. Học few-shot đa phương thức với mô hình ngôn ngữ đông cứng. Advances in Neural Information Processing Systems, 34:200–212, 2021.

[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[33] Ramakrishna Vedantam, C Lawrence Zitnick, và Devi Parikh. Cider: Đánh giá mô tả hình ảnh dựa trên đồng thuận. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 4566–4575, 2015.

[34] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, và Tie-Yan Liu. Transformer thực sự hoạt động kém cho biểu diễn đồ thị? Advances in Neural Information Processing Systems, 34:28877–28888, 2021.

[35] Minji Yoon, John Palowitch, Dustin Zelle, Ziniu Hu, Ruslan Salakhutdinov, và Bryan Perozzi. Học chuyển giao zero-shot trong một đồ thị dị thể thông qua mạng chuyển giao tri thức. Advances in Neural Information Processing Systems, 35:27347–27359, 2022.

[36] Si Zhang, Hanghang Tong, Jiejun Xu, và Ross Maciejewski. Mạng tích chập đồ thị: một đánh giá toàn diện. Computational Social Networks, 6(1):1–23, 2019.

[37] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Mô hình ngôn ngữ transformer pre-trained mở. arXiv preprint arXiv:2205.01068, 2022.

[38] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, và Jian Tang. Học trên đồ thị có thuộc tính văn bản quy mô lớn thông qua suy luận biến phân. arXiv preprint arXiv:2210.14709, 2022.

11
