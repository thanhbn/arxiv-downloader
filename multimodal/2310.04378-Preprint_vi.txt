# 2310.04378.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.04378.pdf
# Kích thước tệp: 15939992 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Preprint
MÔ HÌNH NHẤT QUÁN TIỀM ẨN:
TỔNG HỢP HÌNH ẢNH ĐỘ PHÂN GIẢI CAO
VỚI SUY LUẬN VÀI BƯỚC
Simian Luo∗Yiqin Tan∗Longbo Huang†Jian Li†Hang Zhao†
Viện Khoa học Thông tin Liên ngành, Đại học Thanh Hoa
{luosm22, tyq22 }@mails.tsinghua.edu.cn
{longbohuang, lijian83, hangzhao }@tsinghua.edu.cn
TÓM TẮT
Các mô hình Khuếch tán Tiềm ẩn (LDM) đã đạt được kết quả đáng kể trong việc tổng hợp hình ảnh độ phân giải cao. Tuy nhiên, quá trình lấy mẫu lặp lại tốn nhiều tài nguyên tính toán và dẫn đến việc tạo ra chậm. Lấy cảm hứng từ Mô hình Nhất quán (Song et al., 2023), chúng tôi đề xuất Mô hình Nhất quán Tiềm ẩn (LCM), cho phép suy luận nhanh chóng với số bước tối thiểu trên bất kỳ LDM được đào tạo trước nào, bao gồm Stable Diffusion (Rombach et al., 2022). Xem quá trình khuếch tán ngược có hướng dẫn như việc giải phương trình vi phân thường (ODE) dòng xác suất mở rộng (PF-ODE), LCM được thiết kế để dự đoán trực tiếp nghiệm của ODE như vậy trong không gian tiềm ẩn, giảm thiểu nhu cầu về nhiều lần lặp và cho phép lấy mẫu nhanh chóng, độ trung thực cao. Được chưng cất hiệu quả từ các mô hình khuếch tán có hướng dẫn không phân loại được đào tạo trước, một LCM chất lượng cao 768×768 với 2∼4 bước chỉ mất 32 giờ GPU A100 để đào tạo. Hơn nữa, chúng tôi giới thiệu Tinh chỉnh Nhất quán Tiềm ẩn (LCF), một phương pháp mới được thiết kế riêng để tinh chỉnh LCM trên các tập dữ liệu hình ảnh tùy chỉnh. Đánh giá trên tập dữ liệu LAION-5B-Aesthetics cho thấy LCM đạt được hiệu suất tạo văn bản-thành-hình ảnh tiên tiến với suy luận ít bước. Trang dự án: https://latent-consistency-models.github.io/

1 GIỚI THIỆU
Các mô hình khuếch tán đã xuất hiện như các mô hình tạo sinh mạnh mẽ đã thu hút sự chú ý đáng kể và đạt được kết quả đáng kể trong nhiều lĩnh vực khác nhau (Ho et al., 2020; Song et al., 2020a; Nichol & Dhariwal, 2021; Ramesh et al., 2022; Song & Ermon, 2019; Song et al., 2021). Đặc biệt, các mô hình khuếch tán tiềm ẩn (LDM) (ví dụ: Stable Diffusion (Rombach et al., 2022)) đã thể hiện hiệu suất đặc biệt, đặc biệt là trong các tác vụ tổng hợp văn bản-thành-hình ảnh độ phân giải cao. LDM có thể tạo ra hình ảnh chất lượng cao được điều kiện bởi các mô tả văn bản bằng cách sử dụng quá trình lấy mẫu ngược lặp lại thực hiện việc khử nhiễu mẫu từ từ. Tuy nhiên, các mô hình khuếch tán gặp phải một nhược điểm đáng chú ý: quá trình lấy mẫu ngược lặp lại dẫn đến tốc độ tạo ra chậm, hạn chế khả năng ứng dụng thời gian thực của chúng. Để khắc phục nhược điểm này, các nhà nghiên cứu đã đề xuất một số phương pháp để cải thiện tốc độ lấy mẫu, bao gồm việc tăng tốc quá trình khử nhiễu bằng cách cải thiện các bộ giải ODE (Ho et al., 2020; Lu et al., 2022a;b), có thể tạo ra hình ảnh trong vòng 10 ∼20 bước lấy mẫu. Một cách tiếp cận khác là chưng cất một mô hình khuếch tán được đào tạo trước thành các mô hình cho phép suy luận ít bước Salimans & Ho (2022); Meng et al. (2023). Đặc biệt, Meng et al. (2023) đã đề xuất một cách tiếp cận chưng cất hai giai đoạn để cải thiện hiệu quả lấy mẫu của các mô hình có hướng dẫn không phân loại. Gần đây, Song et al. (2023) đã đề xuất các mô hình nhất quán như một lựa chọn thay thế đầy hứa hẹn nhằm tăng tốc quá trình tạo ra. Bằng cách học các ánh xạ nhất quán duy trì tính nhất quán điểm trên quỹ đạo ODE, các mô hình này cho phép tạo ra một bước duy nhất, loại bỏ nhu cầu về các lần lặp tính toán chuyên sâu. Tuy nhiên, Song et al. (2023) bị giới hạn trong các tác vụ tạo hình ảnh không gian pixel, làm cho nó không phù hợp để tổng hợp hình ảnh độ phân giải cao. Hơn nữa, các ứng dụng cho mô hình khuếch tán có điều kiện và việc kết hợp hướng dẫn không phân loại chưa được khám phá, khiến các phương pháp của họ không phù hợp để tổng hợp tạo văn bản-thành-hình ảnh.
∗Đóng góp ngang bằng†Tác giả tương ứng
1arXiv:2310.04378v1 [cs.CV] 6 Oct 2023

--- TRANG 2 ---
Preprint
Suy luận 4 bước
Suy luận 1 bước
Suy luận 2 bước
Hình 1: Hình ảnh được tạo ra bởi Mô hình Nhất quán Tiềm ẩn (LCM) với tỷ lệ CFG ω= 8.0. LCM có thể được chưng cất từ bất kỳ Stable Diffusion (SD) được đào tạo trước nào chỉ trong 4,000 bước đào tạo (∼32 Giờ GPU A100) để tạo ra hình ảnh chất lượng cao độ phân giải 768×768 trong 2∼4 bước hoặc thậm chí một bước, tăng tốc đáng kể việc tạo văn bản-thành-hình ảnh. Chúng tôi sử dụng LCM để chưng cất phiên bản Dreamer-V7 của SD chỉ trong 4,000 lần lặp đào tạo.

Trong bài báo này, chúng tôi giới thiệu Mô hình Nhất quán Tiềm ẩn (LCM) để tạo ra hình ảnh độ phân giải cao nhanh chóng. Phản ánh LDM, chúng tôi sử dụng các mô hình nhất quán trong không gian tiềm ẩn hình ảnh của một bộ tự động mã hóa được đào tạo trước từ Stable Diffusion (Rombach et al., 2022). Chúng tôi đề xuất một phương pháp chưng cất có hướng dẫn một giai đoạn để chuyển đổi hiệu quả một mô hình khuếch tán có hướng dẫn được đào tạo trước thành một mô hình nhất quán tiềm ẩn bằng cách giải một PF-ODE mở rộng. Ngoài ra, chúng tôi đề xuất Tinh chỉnh Nhất quán Tiềm ẩn, cho phép tinh chỉnh một LCM được đào tạo trước để hỗ trợ suy luận ít bước trên các tập dữ liệu hình ảnh tùy chỉnh. Các đóng góp chính của chúng tôi được tóm tắt như sau:
• Chúng tôi đề xuất Mô hình Nhất quán Tiềm ẩn (LCM) để tạo ra hình ảnh độ phân giải cao nhanh chóng. LCM sử dụng các mô hình nhất quán trong không gian tiềm ẩn hình ảnh, cho phép lấy mẫu nhanh chóng ít bước hoặc thậm chí một bước với độ trung thực cao trên các mô hình khuếch tán tiềm ẩn được đào tạo trước (ví dụ: Stable Diffusion (SD)).
• Chúng tôi cung cấp một phương pháp chưng cất nhất quán có hướng dẫn một giai đoạn đơn giản và hiệu quả để chưng cất SD cho lấy mẫu ít bước (2∼4) hoặc thậm chí 1 bước. Chúng tôi đề xuất kỹ thuật BỎ QUA BƯỚC để tiếp tục

--- TRANG 3 ---
Preprint
tăng tốc sự hội tụ. Đối với suy luận 2 và 4 bước, phương pháp của chúng tôi chỉ tốn 32 giờ GPU A100 để đào tạo và đạt được hiệu suất tiên tiến trên tập dữ liệu LAION-5B-Aesthetics.
• Chúng tôi giới thiệu một phương pháp tinh chỉnh mới cho LCM, được gọi là Tinh chỉnh Nhất quán Tiềm ẩn, cho phép thích ứng hiệu quả một LCM được đào tạo trước với các tập dữ liệu tùy chỉnh trong khi bảo toàn khả năng suy luận nhanh.

2 CÔNG TRÌNH LIÊN QUAN
Các Mô hình Khuếch tán đã đạt được thành công lớn trong việc tạo ra hình ảnh (Ho et al., 2020; Song et al., 2020a; Nichol & Dhariwal, 2021; Ramesh et al., 2022; Rombach et al., 2022; Song & Ermon, 2019). Chúng được đào tạo để khử nhiễu dữ liệu bị hỏng bởi nhiễu để ước tính điểm của phân phối dữ liệu. Trong quá trình suy luận, các mẫu được rút ra bằng cách chạy quá trình khuếch tán ngược để khử nhiễu điểm dữ liệu từ từ. So với VAE (Kingma & Welling, 2013; Sohn et al., 2015) và GAN (Goodfellow et al., 2020), các mô hình khuếch tán có lợi ích về tính ổn định đào tạo và ước tính likelihood tốt hơn.

Tăng tốc DM. Tuy nhiên, các mô hình khuếch tán bị cản trở bởi tốc độ tạo ra chậm. Nhiều cách tiếp cận khác nhau đã được đề xuất, bao gồm các phương pháp không cần đào tạo như các bộ giải ODE (Song et al., 2020a; Lu et al., 2022a;b), các bộ giải kích thước bước thích ứng (Jolicoeur-Martineau et al., 2021), các phương pháp dự đoán-hiệu chỉnh (Song et al., 2020b). Các cách tiếp cận dựa trên đào tạo bao gồm rời rạc hóa tối ưu (Watson et al., 2021), khuếch tán cắt ngắn (Lyu et al., 2022; Zheng et al., 2022), toán tử thần kinh (Zheng et al., 2023) và chưng cất (Salimans & Ho, 2022; Meng et al., 2023). Gần đây hơn, các mô hình tạo sinh mới cho lấy mẫu nhanh hơn cũng đã được đề xuất (Liu et al., 2022; 2023).

Các Mô hình Khuếch tán Tiềm ẩn (LDM) (Rombach et al., 2022) xuất sắc trong việc tổng hợp văn bản-thành-hình ảnh độ phân giải cao. Ví dụ, Stable Diffusion (SD) thực hiện các quá trình khuếch tán thuận và ngược trong không gian tiềm ẩn dữ liệu, dẫn đến tính toán hiệu quả hơn.

Các Mô hình Nhất quán (CM) (Song et al., 2023) đã cho thấy tiềm năng lớn như một loại mô hình tạo sinh mới cho lấy mẫu nhanh hơn trong khi bảo toàn chất lượng tạo ra. CM áp dụng ánh xạ nhất quán để ánh xạ trực tiếp bất kỳ điểm nào trong quỹ đạo ODE tới gốc của nó, cho phép tạo ra nhanh chóng một bước. CM có thể được đào tạo bằng cách chưng cất các mô hình khuếch tán được đào tạo trước hoặc như các mô hình tạo sinh độc lập. Chi tiết của CM được mô tả chi tiết trong phần tiếp theo.

3 KIẾN THỨC CHUẨN BỊ
Trong phần này, chúng tôi xem xét ngắn gọn các mô hình khuếch tán và nhất quán và định nghĩa các ký hiệu liên quan.

Các Mô hình Khuếch tán: Các mô hình khuếch tán, hay các mô hình tạo sinh dựa trên điểm Ho et al. (2020); Song et al. (2020a) là một họ các mô hình tạo sinh tiến bộ tiêm nhiễu Gaussian vào dữ liệu, và sau đó tạo ra mẫu từ nhiễu thông qua quá trình khử nhiễu ngược. Đặc biệt, các mô hình khuếch tán định nghĩa một quá trình thuận chuyển tiếp từ phân phối dữ liệu gốc pdata(x) sang phân phối biên qt(xt), thông qua nhân chuyển tiếp: q0t(xt|x0) = N(xt|α(t)x0, σ2(t)I), trong đó α(t), σ(t) xác định lịch trình nhiễu. Trong góc nhìn thời gian liên tục, quá trình thuận có thể được mô tả bởi phương trình vi phân ngẫu nhiên (SDE) Song et al. (2020b); Lu et al. (2022a); Karras et al. (2022) cho t∈[0, T]:
dxt=f(t)xtdt+g(t)dwt,x0∼pdata(x0), trong đó wt là chuyển động Brown chuẩn, và
f(t) = d logα(t)/dt, g2(t) = dσ2(t)/dt - 2(d logα(t)/dt)σ2(t). (1)

Bằng cách xem xét SDE thời gian ngược (xem Phụ lục A để biết thêm chi tiết), ta có thể chỉ ra rằng phân phối biên qt(x) thỏa mãn phương trình vi phân thường sau, được gọi là ODE Dòng Xác suất (PF-ODE) (Song et al., 2020b; Lu et al., 2022a):
dxt/dt = f(t)xt - (1/2)g2(t)∇x log qt(xt), xT∼qT(xT). (2)

Trong các mô hình khuếch tán, chúng ta đào tạo mô hình dự đoán nhiễu ϵθ(xt, t) để phù hợp với -∇ log qt(xt) (được gọi là hàm điểm). Xấp xỉ hàm điểm bằng mô hình dự đoán nhiễu trong 21, ta có thể thu được PF-ODE thực nghiệm sau để lấy mẫu:
dxt/dt = f(t)xt + (g2(t)/(2σt))ϵθ(xt, t), xT∼N(0,˜σ2I). (3)

Đối với các mô hình khuếch tán có điều kiện lớp, Hướng dẫn Không Phân loại (CFG) (Ho & Salimans, 2022) là một kỹ thuật hiệu quả để cải thiện đáng kể chất lượng của các mẫu được tạo ra và đã được sử dụng rộng rãi trong một số mô hình khuếch tán quy mô lớn bao gồm GLIDE Nichol et al. (2021), Stable Diffusion (Rombach et al., 2022), DALL·E 2 (Ramesh et al., 2022) và Imagen (Saharia et al., 2022). Cho một tỷ lệ CFG ω, dự đoán nhiễu gốc được thay thế bằng tổ hợp tuyến tính của dự đoán nhiễu có điều kiện và không điều kiện, tức là ˜ϵθ(zt, ω,c, t) = (1 + ω)ϵθ(zt,c, t) - ωϵθ(z,∅, t).

Các Mô hình Nhất quán: Mô hình Nhất quán (CM) (Song et al., 2023) là một họ mô hình tạo sinh mới cho phép tạo ra một bước hoặc ít bước. Ý tưởng cốt lõi của CM là học hàm ánh xạ bất kỳ điểm nào trên quỹ đạo của PF-ODE tới gốc của quỹ đạo đó (tức là, nghiệm của PF-ODE). Chính thức hơn, hàm nhất quán được định nghĩa là f: (xt, t) → xϵ, trong đó ϵ là một số dương nhỏ cố định. Một quan sát quan trọng là hàm nhất quán phải thỏa mãn tính chất tự nhất quán:
f(xt, t) = f(xt', t'), ∀t, t' ∈ [ϵ, T]. (4)

Ý tưởng chính trong (Song et al., 2023) để học một mô hình nhất quán fθ là học một hàm nhất quán từ dữ liệu bằng cách thực thi hiệu quả tính chất tự nhất quán trong Phương trình 4. Để đảm bảo rằng fθ(x, ϵ) = x, mô hình nhất quán fθ được tham số hóa như:
fθ(x, t) = cskip(t)x + cout(t)Fθ(x, t), (5)
trong đó cskip(t) và cout(t) là các hàm có thể vi phân với cskip(ϵ) = 1 và cout(ϵ) = 0, và Fθ(x, t) là một mạng thần kinh sâu. Một CM có thể được chưng cất từ một mô hình khuếch tán được đào tạo trước hoặc được đào tạo từ đầu. Cái trước được gọi là Chưng cất Nhất quán. Để thực thi tính chất tự nhất quán, chúng ta duy trì một mô hình mục tiêu θ-, được cập nhật với trung bình trượt theo cấp số nhân (EMA) của tham số θ mà chúng ta dự định học, tức là θ- ← μθ- + (1-μ)θ, và định nghĩa tổn thất nhất quán như sau:
L(θ,θ-; Φ) = Ex,t[d(fθ(xtn+1, tn+1), fθ-(x̂ϕtn, tn))], (6)
trong đó d(·,·) là hàm metric được chọn để đo khoảng cách giữa hai mẫu, ví dụ, khoảng cách ℓ2 bình phương d(x,y) = ||x-y||2². x̂ϕtn là ước tính một bước của xtn từ xtn+1 như:
x̂ϕtn ← xtn+1 + (tn-tn+1)Φ(xtn+1, tn+1;ϕ). (7)
trong đó Φ biểu thị bộ giải ODE một bước được áp dụng cho PF-ODE trong Phương trình 24. (Song et al., 2023) đã sử dụng Euler (Song et al., 2020b) hoặc bộ giải Heun (Karras et al., 2022) làm bộ giải ODE số. Thêm chi tiết và mã giả cho chưng cất nhất quán (Thuật toán 2) được cung cấp trong Phụ lục A.

4 MÔ HÌNH NHẤT QUÁN TIỀM ẨN
Các Mô hình Nhất quán (CM) (Song et al., 2023) chỉ tập trung vào các tác vụ tạo hình ảnh trên ImageNet 64×64 (Deng et al., 2009) và LSUN 256×256 (Yu et al., 2015). Tiềm năng của CM để tạo ra các tác vụ văn bản-thành-hình ảnh độ phân giải cao hơn vẫn chưa được khám phá. Trong bài báo này, chúng tôi giới thiệu Mô hình Nhất quán Tiềm ẩn (LCM) trong Mục 4.1 để giải quyết những tác vụ thách thức hơn này, mở ra tiềm năng của CM. Tương tự như LDM, LCM của chúng tôi áp dụng một mô hình nhất quán trong không gian tiềm ẩn hình ảnh. Chúng tôi chọn Stable Diffusion (SD) mạnh mẽ làm mô hình khuếch tán cơ sở để chưng cất. Chúng tôi nhằm mục đích đạt được suy luận ít bước (2∼4) và thậm chí một bước trên SD mà không làm giảm chất lượng hình ảnh. Hướng dẫn không phân loại (CFG) (Ho & Salimans, 2022) là một kỹ thuật hiệu quả để cải thiện thêm chất lượng mẫu và được sử dụng rộng rãi trong SD. Tuy nhiên, việc áp dụng nó trong CM vẫn chưa được khám phá. Chúng tôi đề xuất một phương pháp chưng cất có hướng dẫn một giai đoạn đơn giản trong Mục 4.2 giải quyết một PF-ODE mở rộng, tích hợp CFG vào LCM một cách hiệu quả. Chúng tôi đề xuất kỹ thuật BỎ QUA BƯỚC để tăng tốc sự hội tụ của LCM trong Mục 4.3. Cuối cùng, chúng tôi đề xuất Tinh chỉnh Nhất quán Tiềm ẩn để tinh chỉnh một LCM được đào tạo trước cho suy luận ít bước trên một tập dữ liệu tùy chỉnh trong Mục 4.4.

4.1 CHƯNG CẤT NHẤT QUÁN TRONG KHÔNG GIAN TIỀM ẨN
Sử dụng không gian tiềm ẩn hình ảnh trong các mô hình khuếch tán quy mô lớn như Stable Diffusion (SD) (Rombach et al., 2022) đã cải thiện hiệu quả chất lượng tạo hình ảnh và giảm tải tính toán. Trong SD, một bộ tự động mã hóa (E,D) trước tiên được đào tạo để nén dữ liệu hình ảnh chiều cao thành vector tiềm ẩn chiều thấp z=E(x), sau đó được giải mã để tái tạo hình ảnh như x̂=D(z). Đào tạo các mô hình khuếch tán trong không gian tiềm ẩn giảm đáng kể chi phí tính toán so với các mô hình dựa trên pixel và tăng tốc quá trình suy luận; LDM làm cho việc tạo ra hình ảnh độ phân giải cao trên GPU laptop trở nên khả thi.

Đối với LCM, chúng tôi tận dụng lợi thế của không gian tiềm ẩn cho chưng cất nhất quán, trái ngược với không gian pixel được sử dụng trong CM (Song et al., 2023). Cách tiếp cận này, được gọi là Chưng cất Nhất quán Tiềm ẩn (LCD) được áp dụng cho SD được đào tạo trước, cho phép tổng hợp hình ảnh độ phân giải cao (ví dụ:

--- TRANG 4 ---
Preprint
768×768) trong 1∼4 bước. Chúng tôi tập trung vào việc tạo ra có điều kiện. Nhớ lại rằng PF-ODE của quá trình khuếch tán ngược (Song et al., 2020b; Lu et al., 2022a) là
dzt/dt = f(t)zt + (g2(t)/(2σt))ϵθ(zt,c, t), zT∼N(0,˜σ2I), (8)
trong đó zt là các tiềm ẩn hình ảnh, ϵθ(zt,c, t) là mô hình dự đoán nhiễu, và c là điều kiện đã cho (ví dụ văn bản). Các mẫu có thể được rút ra bằng cách giải PF-ODE từ T về 0. Để thực hiện LCD, chúng tôi giới thiệu hàm nhất quán fθ: (zt,c, t) → z0 để dự đoán trực tiếp nghiệm của PF-ODE (Phương trình 8) cho t= 0. Chúng tôi tham số hóa fθ bằng mô hình dự đoán nhiễu ϵ̂θ, như sau:
fθ(z,c, t) = cskip(t)z + cout(t)(z - σtϵ̂θ(z,c, t))/αt, (ε-Prediction) (9)
trong đó cskip(0) = 1, cout(0) = 0 và ϵ̂θ(z,c, t) là mô hình dự đoán nhiễu được khởi tạo với các tham số giống như mô hình khuếch tán giáo viên. Đáng chú ý, fθ có thể được tham số hóa theo nhiều cách khác nhau, tùy thuộc vào việc tham số hóa dự đoán của mô hình khuếch tán giáo viên (ví dụ, x,ε(Ho et al., 2020), v(Salimans & Ho, 2022)). Chúng tôi thảo luận về các cách tham số hóa khác có thể trong Phụ lục D.

Chúng tôi giả định rằng một bộ giải ODE hiệu quả Ψ(zt, t, s,c) có sẵn để xấp xỉ tích phân của vế phải của Phương trình 8 từ thời gian t đến s. Trong thực tế, chúng ta có thể sử dụng DDIM (Song et al., 2020a), DPM-Solver (Lu et al., 2022a) hoặc DPM-Solver++ (Lu et al., 2022b) như Ψ(·,·,·,·). Lưu ý rằng chúng tôi chỉ sử dụng các bộ giải này trong đào tạo/chưng cất, không phải trong suy luận. Chúng tôi sẽ thảo luận về các bộ giải này thêm khi chúng tôi giới thiệu kỹ thuật BỎ QUA BƯỚC trong Mục 4.3. LCM nhằm mục đích dự đoán nghiệm của PF-ODE bằng cách tối thiểu hóa tổn thất chưng cất nhất quán (Song et al., 2023):
LCD(θ,θ-; Ψ) = Ez,c,n[d(fθ(ztn+1,c, tn+1), fθ-(ẑΨtn,c, tn))]. (10)

Ở đây, ẑΨtn là một ước tính của sự tiến hóa của PF-ODE từ tn+1→tn bằng cách sử dụng bộ giải ODE Ψ:
ẑΨtn - ztn+1 = ∫[tn+1 to tn] [f(t)zt + (g2(t)/(2σt))ϵθ(zt,c, t)]dt ≈ Ψ(ztn+1, tn+1, tn,c), (11)
trong đó bộ giải Ψ(·,·,·,·) được sử dụng để xấp xỉ tích phân từ tn+1→tn.

4.2 CHƯNG CẤT CÓ HƯỚNG DẪN MỘT GIAI ĐOẠN BẰNG CÁCH GIẢI PF-ODE MỞ RỘNG
Hướng dẫn không phân loại (CFG) (Ho & Salimans, 2022) là rất quan trọng để tổng hợp hình ảnh chất lượng cao được căn chỉnh văn bản trong SD, thường cần tỷ lệ CFG ω trên 6. Do đó, việc tích hợp CFG vào một phương pháp chưng cất trở nên không thể thiếu. Phương pháp trước đó Guided-Distill (Meng et al., 2023) giới thiệu chưng cất hai giai đoạn để hỗ trợ lấy mẫu ít bước từ mô hình khuếch tán có hướng dẫn. Tuy nhiên, nó tốn nhiều tài nguyên tính toán (ví dụ ít nhất 45 Ngày GPU A100 cho suy luận 2 bước, ước tính trong (Liu et al., 2023)). Một LCM chỉ cần 32 Giờ GPU A100 đào tạo cho suy luận 2 bước, như được mô tả trong Hình 1. Hơn nữa, chưng cất có hướng dẫn hai giai đoạn có thể dẫn đến lỗi tích lũy, dẫn đến hiệu suất không tối ưu. Ngược lại, LCM áp dụng chưng cất có hướng dẫn một giai đoạn hiệu quả bằng cách giải một PF-ODE mở rộng. Nhớ lại CFG được sử dụng trong quá trình khuếch tán ngược:
˜ϵθ(zt, ω,c, t) := (1 + ω)ϵθ(zt,c, t) - ωϵθ(zt,∅, t), (12)
trong đó dự đoán nhiễu gốc được thay thế bằng tổ hợp tuyến tính của nhiễu có điều kiện và không điều kiện và ω được gọi là tỷ lệ hướng dẫn. Để lấy mẫu từ quá trình ngược có hướng dẫn, chúng ta cần giải PF-ODE mở rộng sau: (tức là, được mở rộng với các thuật ngữ liên quan đến ω)
dzt/dt = f(t)zt + (g2(t)/(2σt))˜ϵθ(zt, ω,c, t), zT∼N(0,˜σ2I). (13)

Để thực hiện chưng cất có hướng dẫn một giai đoạn hiệu quả, chúng tôi giới thiệu một hàm nhất quán mở rộng fθ: (zt, ω,c, t) → z0 để dự đoán trực tiếp nghiệm của PF-ODE mở rộng (Phương trình 13) cho t= 0. Chúng tôi tham số hóa fθ theo cùng cách như trong Phương trình 9, ngoại trừ việc ϵ̂θ(z,c, t) được thay thế bằng ϵ̂θ(z, ω,c, t), là mô hình dự đoán nhiễu khởi tạo với các tham số giống như mô hình khuếch tán giáo viên, nhưng cũng chứa các tham số có thể đào tạo bổ sung để điều kiện hóa trên ω. Tổn thất nhất quán giống như Phương trình 10 ngoại trừ việc chúng ta sử dụng hàm nhất quán mở rộng fθ(zt, ω,c, t).
LCD(θ,θ-; Ψ) = Ez,c,ω,n[d(fθ(ztn+1, ω,c, tn+1), fθ-(ẑΨ,ωtn, ω,c, tn))] (14)

--- TRANG 5 ---
Preprint
Trong Phương trình 14, ω và n được lấy mẫu đồng đều từ khoảng [ωmin, ωmax] và {1, . . . , N-1} tương ứng. ẑΨ,ωtn được ước tính bằng cách sử dụng mô hình nhiễu mới ˜ϵθ(zt, ω,c, t), như sau:
ẑΨ,ωtn - ztn+1 = ∫[tn+1 to tn] [f(t)zt + (g2(t)/(2σt))˜ϵθ(zt, ω,c, t)]dt
= (1 + ω)∫[tn+1 to tn] [f(t)zt + (g2(t)/(2σt))ϵθ(zt,c, t)]dt - ω∫[tn+1 to tn] [f(t)zt + (g2(t)/(2σt))ϵθ(zt,∅, t)]dt
≈ (1 + ω)Ψ(ztn+1, tn+1, tn,c) - ωΨ(ztn+1, tn+1, tn,∅). (15)

Một lần nữa, chúng ta có thể sử dụng DDIM (Song et al., 2020a), DPM-Solver (Lu et al., 2022a) hoặc DPM-Solver++ (Lu et al., 2022b) làm bộ giải PF-ODE Ψ(·,·,·,·).

4.3 TĂNG TỐC CHƯNG CẤT VỚI BỎ QUA BƯỚC THỜI GIAN
Các mô hình khuếch tán rời rạc (Ho et al., 2020; Song & Ermon, 2019) thường đào tạo các mô hình dự đoán nhiễu với lịch trình bước thời gian dài {ti}i (cũng được gọi là lịch trình rời rạc hóa hoặc lịch trình thời gian) để đạt được kết quả tạo ra chất lượng cao. Ví dụ, Stable Diffusion (SD) có lịch trình thời gian dài 1,000. Tuy nhiên, việc áp dụng trực tiếp Chưng cất Nhất quán Tiềm ẩn (LCD) cho SD với lịch trình mở rộng như vậy có thể có vấn đề. Mô hình cần lấy mẫu trên tất cả 1,000 bước thời gian, và tổn thất nhất quán cố gắng căn chỉnh dự đoán của mô hình LCM fθ(ztn+1,c, tn+1) với dự đoán fθ(ztn,c, tn) ở bước tiếp theo dọc theo cùng một quỹ đạo. Vì tn-tn+1 là rất nhỏ, ztn và ztn+1 (và do đó fθ(ztn+1,c, tn+1) và fθ(ztn,c, tn)) đã gần với nhau, gây ra tổn thất nhất quán nhỏ và do đó dẫn đến sự hội tụ chậm. Để giải quyết vấn đề này, chúng tôi giới thiệu phương pháp BỎ QUA BƯỚC để rút ngắn đáng kể độ dài của lịch trình thời gian (từ hàng nghìn xuống hàng chục) để đạt được sự hội tụ nhanh trong khi bảo toàn chất lượng tạo ra.

Các Mô hình Nhất quán (CM) (Song et al., 2023) sử dụng lịch trình thời gian liên tục EDM (Karras et al., 2022), và Euler, hoặc Heun Solver làm bộ giải PF-ODE liên tục số. Đối với LCM, để thích ứng với lịch trình thời gian rời rạc trong Stable Diffusion, chúng tôi sử dụng DDIM (Song et al., 2020a), DPM-Solver (Lu et al., 2022a), hoặc DPM-Solver++ (Lu et al., 2022b) làm bộ giải ODE. (Lu et al., 2022a) cho thấy rằng những bộ giải tiến bộ này có thể giải PF-ODE một cách hiệu quả trong Phương trình 8. Bây giờ, chúng tôi giới thiệu phương pháp BỎ QUA BƯỚC trong Chưng cất Nhất quán Tiềm ẩn (LCD). Thay vì đảm bảo tính nhất quán giữa các bước thời gian liền kề tn+1→tn, LCM nhằm mục đích đảm bảo tính nhất quán giữa bước thời gian hiện tại và k bước cách xa, tn+k→tn. Lưu ý rằng thiết lập k=1 giảm về lịch trình gốc trong (Song et al., 2023), dẫn đến sự hội tụ chậm, và k rất lớn có thể gây ra lỗi xấp xỉ lớn của các bộ giải ODE. Trong các thí nghiệm chính của chúng tôi, chúng tôi thiết lập k=20, giảm đáng kể độ dài của lịch trình thời gian từ hàng nghìn xuống hàng chục. Kết quả trong Mục 5.2 cho thấy hiệu ứng của các giá trị k khác nhau và tiết lộ rằng phương pháp BỎ QUA BƯỚC là rất quan trọng trong việc tăng tốc quá trình LCD. Cụ thể, tổn thất chưng cất nhất quán trong Phương trình 14 được sửa đổi để đảm bảo tính nhất quán từ tn+k đến tn:
LCD(θ,θ-; Ψ) = Ez,c,ω,n[d(fθ(ztn+k, ω,c, tn+k), fθ-(ẑΨ,ωtn, ω,c, tn))], (16)
với ẑΨ,ωtn là ước tính của ztn bằng cách sử dụng bộ giải PF-ODE mở rộng số Ψ:
ẑΨ,ωtn ← ztn+k + (1 + ω)Ψ(ztn+k, tn+k, tn,c) - ωΨ(ztn+k, tn+k, tn,∅). (17)

Sự suy diễn trên tương tự như Phương trình 15. Đối với LCM, chúng tôi sử dụng ba bộ giải ODE có thể ở đây: DDIM (Song et al., 2020a), DPM-Solver (Lu et al., 2022a), DPM-Solver++ (Lu et al., 2022b), và chúng tôi so sánh hiệu suất của chúng trong Mục 5.2. Trên thực tế, DDIM (Song et al., 2020a) là xấp xỉ rời rạc hóa bậc nhất của DPM-Solver (Được chứng minh trong (Lu et al., 2022a)). Ở đây chúng tôi cung cấp công thức chi tiết của bộ giải PF-ODE DDIM ΨDDIM từ tn+k đến tn. Các công thức của hai bộ giải khác ΨDPM-Solver, ΨDPM-Solver++ được cung cấp trong Phụ lục E.
ΨDDIM(ztn+k, tn+k, tn,c) = (αtn/αtn+k)ztn+k - (σtn/σtn+k)·(αtn/αtn+k)·(σtn)^(-1)ϵ̂θ(ztn+k,c, tn+k) | {z } DDIM Ước tính ztn-ztn+k (18)

Chúng tôi trình bày mã giả cho LCD với các kỹ thuật CFG và BỎ QUA BƯỚC trong Thuật toán 1. Các sửa đổi từ thuật toán Chưng cất Nhất quán (CD) gốc trong Song et al. (2023) được làm nổi bật bằng màu xanh lam. Ngoài ra, thuật toán lấy mẫu LCM 3 được cung cấp trong Phụ lục B.

--- TRANG 6 ---
Preprint
MÔ HÌNH (512×512) RESOFID↓ CLIP SCORE↑
1 BƯỚC 2 BƯỚC 4 BƯỚC 8 BƯỚC 1 BƯỚC 2 BƯỚC 4 BƯỚC 8 BƯỚC
DDIM (Song et al., 2020a) 183.29 81.05 22.38 13.83 6.03 14.13 25.89 29.29
DPM (Lu et al., 2022a) 185.78 72.81 18.53 12.24 6.35 15.10 26.64 29.54
DPM++ (Lu et al., 2022b) 185.78 72.81 18.43 12.20 6.35 15.10 26.64 29.55
Guided-Distill (Meng et al., 2023) 108.21 33.25 15.12 13.89 12.08 22.71 27.25 28.17
LCM (Của chúng tôi) 35.36 13.31 11.10 11.84 24.14 27.83 28.69 28.84

Bảng 1: Kết quả định lượng với ω= 8 ở độ phân giải 512×512. LCM vượt trội đáng kể so với các đường cơ sở trong vùng 1-4 bước trên tập dữ liệu LAION-Aesthetic-6+. Đối với LCM, DDIM-Solver được sử dụng với bước bỏ qua k= 20.

MÔ HÌNH (768×768) RESOFID↓ CLIP SCORE↑
1 BƯỚC 2 BƯỚC 4 BƯỚC 8 BƯỚC 1 BƯỚC 2 BƯỚC 4 BƯỚC 8 BƯỚC
DDIM (Song et al., 2020a) 186.83 77.26 24.28 15.66 6.93 16.32 26.48 29.49
DPM (Lu et al., 2022a) 188.92 67.14 20.11 14.08 7.40 17.11 27.25 29.80
DPM++ (Lu et al., 2022b) 188.91 67.14 20.08 14.11 7.41 17.11 27.26 29.84
Guided-Distill (Meng et al., 2023) 120.28 30.70 16.70 14.12 12.88 24.88 28.45 29.16
LCM (Của chúng tôi) 34.22 16.32 13.53 14.97 25.32 27.92 28.60 28.49

Bảng 2: Kết quả định lượng với ω= 8 ở độ phân giải 768×768. LCM vượt trội đáng kể so với các đường cơ sở trong vùng 1-4 bước trên tập dữ liệu LAION-Aesthetic-6.5+. Đối với LCM, DDIM-Solver được sử dụng với bước bỏ qua k= 20.

Thuật toán 1 Chưng cất Nhất quán Tiềm ẩn (LCD)
Đầu vào: tập dữ liệu D, tham số mô hình ban đầu θ, tỷ lệ học η, bộ giải ODE Ψ(·,·,·,·), metric khoảng cách d(·,·), tỷ lệ EMA μ, lịch trình nhiễu α(t), σ(t), tỷ lệ hướng dẫn [wmin, wmax], khoảng bỏ qua k, và bộ mã hóa E(·)
Mã hóa dữ liệu đào tạo vào không gian tiềm ẩn: Dz={(z,c)|z=E(x),(x,c)∈D}
θ-←θ
lặp lại
Lấy mẫu (z,c)∼Dz,n∼U[1, N-k] và ω∼[ωmin, ωmax]
Lấy mẫu ztn+k∼N(α(tn+k)z;σ2(tn+k)I)
ẑΨ,ωtn←ztn+k+(1+ω)Ψ(ztn+k, tn+k, tn,c)-ωΨ(ztn+k, tn+k, tn,∅)
L(θ,θ-; Ψ)←d(fθ(ztn+k, ω,c, tn+k),fθ-(ẑΨ,ωtn, ω,c, tn))
θ←θ-η∇θL(θ,θ-)
θ-←stopgrad(μθ-+(1-μ)θ)
cho đến khi hội tụ

4.4 TINH CHỈNH NHẤT QUÁN TIỀM ẨN CHO TẬP DỮ LIỆU TÙY CHỈNH
Các mô hình tạo sinh nền tảng như Stable Diffusion xuất sắc trong các tác vụ tạo văn bản-thành-hình ảnh đa dạng nhưng thường yêu cầu tinh chỉnh trên các tập dữ liệu tùy chỉnh để đáp ứng yêu cầu của các tác vụ hạ nguồn. Chúng tôi đề xuất Tinh chỉnh Nhất quán Tiềm ẩn (LCF), một phương pháp tinh chỉnh cho LCM được đào tạo trước. Lấy cảm hứng từ Đào tạo Nhất quán (CT) (Song et al., 2023), LCF cho phép suy luận ít bước hiệu quả trên các tập dữ liệu tùy chỉnh mà không cần dựa vào mô hình khuếch tán giáo viên được đào tạo trên dữ liệu như vậy. Cách tiếp cận này trình bày một lựa chọn thay thế khả thi cho các phương pháp tinh chỉnh truyền thống cho các mô hình khuếch tán. Mã giả cho LCF được cung cấp trong Thuật toán 4, với minh họa chi tiết hơn trong Phụ lục C.

5 THÍ NGHIỆM
Trong phần này, chúng tôi sử dụng chưng cất nhất quán độ trễ để đào tạo LCM trên hai tập con của LAION-5B. Trong Mục 5.1, chúng tôi trước tiên đánh giá hiệu suất của LCM trong các tác vụ tạo văn bản-thành-hình ảnh. Trong Mục 5.2, chúng tôi cung cấp một nghiên cứu tách biệt chi tiết để kiểm tra hiệu quả của việc sử dụng các bộ giải khác nhau, lịch trình bước bỏ qua và tỷ lệ hướng dẫn. Cuối cùng, trong Mục 5.3, chúng tôi trình bày kết quả thực nghiệm của tinh chỉnh nhất quán tiềm ẩn trên LCM được đào tạo trước trên các tập dữ liệu hình ảnh tùy chỉnh.

5.1 TẠO VĂN BẢN-THÀNH-HÌNH ẢNH
Tập dữ liệu Chúng tôi sử dụng hai tập con của LAION-5B (Schuhmann et al., 2022): LAION-Aesthetics-6+ (12M) và LAION-Aesthetics-6.5+ (650K) cho việc tạo văn bản-thành-hình ảnh. Các thí nghiệm của chúng tôi xem xét độ phân giải 512×512 và 768×768. Đối với độ phân giải 512, chúng tôi sử dụng LAION-Aesthetics-6+, bao gồm 12M cặp văn bản-hình ảnh với điểm thẩm mỹ được dự đoán cao hơn 6. Đối với độ phân giải 768, chúng tôi sử dụng LAION-Aesthetics-6.5+, với 650K cặp văn bản-hình ảnh với điểm thẩm mỹ cao hơn 6.5.

Cấu hình Mô hình Đối với độ phân giải 512, chúng tôi sử dụng Stable Diffusion-V2.1-Base được đào tạo trước (Rombach et al., 2022) làm mô hình giáo viên, được đào tạo ban đầu trên độ phân giải 512×512 với ϵ-Prediction (Ho et al., 2020). Đối với độ phân giải 768, chúng tôi sử dụng Stable Diffusion-V2.1 được đào tạo trước được sử dụng rộng rãi (Rombach et al., 2022), được đào tạo ban đầu trên độ phân giải 768×768 với v-Prediction (Salimans & Ho, 2022). Chúng tôi đào tạo LCM với 100K lần lặp và chúng tôi sử dụng kích thước batch 72 cho thiết lập (512×512), và 16 cho thiết lập (768×768), cùng tỷ lệ học 8e-6 và tỷ lệ EMA μ= 0.999943 như được sử dụng trong (Song et al., 2023). Đối với bộ giải PF-ODE mở rộng Ψ và bước bỏ qua k trong Phương trình 17, chúng tôi sử dụng DDIM-Solver (Song et al., 2020a) với bước bỏ qua k= 20. Chúng tôi thiết lập dải tỷ lệ hướng dẫn [wmin, wmax] = [2, 14], nhất quán với (Meng et al., 2023). Thêm chi tiết đào tạo được cung cấp trong Phụ lục F.

Đường cơ sở & Đánh giá Chúng tôi sử dụng DDIM (Song et al., 2020a), DPM (Lu et al., 2022a), DPM++ (Lu et al., 2022b) và Guided-Distill (Meng et al., 2023) làm đường cơ sở. Ba cái đầu tiên là các bộ lấy mẫu không cần đào tạo yêu cầu thêm bộ nhớ đỉnh trên mỗi bước với hướng dẫn không phân loại. Guided-Distill yêu cầu hai giai đoạn chưng cất có hướng dẫn. Vì Guided-Distill không được mở nguồn, chúng tôi tuân thủ nghiêm ngặt quy trình đào tạo được nêu trong bài báo để tái tạo kết quả. Do tài nguyên hạn chế (Meng et al. (2023) đã sử dụng kích thước batch lớn 512, yêu cầu ít nhất 32 GPU A100), chúng tôi giảm kích thước batch xuống 72, giống như của chúng tôi, và đào tạo trong cùng 100K lần lặp. Chi tiết tái tạo được cung cấp trong Phụ lục G. Chúng tôi thừa nhận rằng đào tạo lâu hơn và nhiều tài nguyên tính toán hơn có thể dẫn đến kết quả tốt hơn như được báo cáo trong (Meng et al., 2023). Tuy nhiên, LCM đạt được sự hội tụ nhanh hơn và kết quả vượt trội dưới cùng chi phí tính toán. Để đánh giá, chúng tôi tạo ra 30K hình ảnh từ 10K lời nhắc văn bản trong tập kiểm tra (3 hình ảnh mỗi lời nhắc), và áp dụng điểm FID và CLIP để đánh giá sự đa dạng và chất lượng của các hình ảnh được tạo ra. Chúng tôi sử dụng ViT-g/14 để đánh giá điểm CLIP.

Kết quả. Các kết quả định lượng trong Bảng 1 và 2 cho thấy LCM vượt trội đáng kể so với các phương pháp đường cơ sở ở độ phân giải 512 và 768, đặc biệt là trong chế độ bước thấp (1∼4), làm nổi bật hiệu quả và hiệu suất vượt trội của nó. Không giống như DDIM, DPM, DPM++, yêu cầu nhiều bộ nhớ đỉnh hơn trên mỗi bước lấy mẫu với CFG, LCM chỉ yêu cầu một lần đi qua thuận trên mỗi bước lấy mẫu, tiết kiệm cả thời gian và bộ nhớ. Hơn nữa, trái ngược với quy trình chưng cất hai giai đoạn được sử dụng trong Guided-Distill, LCM chỉ cần chưng cất có hướng dẫn một giai đoạn, đơn giản và thực tế hơn nhiều. Các kết quả định tính trong Hình 2 tiếp tục cho thấy ưu thế của LCM với suy luận 2 và 4 bước.

5.2 NGHIÊN CỨU TÁCH BIỆT
Các Bộ giải ODE & Lịch trình Bỏ qua Bước. Chúng tôi so sánh các bộ giải khác nhau Ψ (DDIM (Song et al., 2020a), DPM (Lu et al., 2022a), DPM++ (Lu et al., 2022b)) để giải PF-ODE mở rộng được chỉ định trong Phương trình 17, và khám phá các lịch trình bước bỏ qua khác nhau với k khác nhau. Kết quả được mô tả trong Hình 3. Chúng tôi quan sát thấy: 1) Sử dụng kỹ thuật BỎ QUA BƯỚC (xem Mục 4.3), LCM đạt được sự hội tụ nhanh trong vòng 2,000 lần lặp trong thiết lập suy luận 4 bước. Cụ thể, bộ giải DDIM hội tụ chậm ở bước bỏ qua k= 1, trong khi thiết lập k= 5,10,20 dẫn đến

--- TRANG 7 ---
Preprint
LCM 4 bước (Của chúng tôi)
Guided Distill 4 bước
LCM 2 bước (Của chúng tôi)
Guided Distill 2 bước
DPM-Solver++ 2 bước
DPM-Solver++ 4 bước
Hình 2: Kết quả tạo văn bản-thành-hình ảnh trên LAION-Aesthetic-6.5+ với suy luận 2-, 4-bước. Hình ảnh được tạo ra bởi LCM thể hiện chi tiết và chất lượng vượt trội, vượt trội hơn các đường cơ sở khác với biên độ lớn.

Hình 3: Nghiên cứu tách biệt về các bộ giải ODE khác nhau và bước bỏ qua k. Bước bỏ qua k phù hợp có thể tăng tốc đáng kể sự hội tụ và dẫn đến FID tốt hơn trong cùng số bước đào tạo.

sự hội tụ nhanh hơn nhiều, nhấn mạnh hiệu quả của phương pháp Bỏ qua Bước. 2) Các bộ giải DPM và DPM++ hoạt động tốt hơn ở bước bỏ qua lớn hơn (k= 50) so với bộ giải DDIM bị lỗi xấp xỉ ODE tăng lên với k lớn hơn. Hiện tượng này cũng được thảo luận trong (Lu et al., 2022a). 3) Các giá trị k rất nhỏ (1 hoặc 5) dẫn đến sự hội tụ chậm và những giá trị rất lớn (ví dụ, 50 cho DDIM) có thể dẫn đến kết quả kém hơn. Do đó, chúng tôi chọn k= 20, cung cấp hiệu suất cạnh tranh cho cả ba bộ giải, cho thí nghiệm chính của chúng tôi trong Mục 5.1.

Hiệu ứng của Tỷ lệ Hướng dẫn ω. Chúng tôi kiểm tra hiệu ứng của việc sử dụng các tỷ lệ CFG khác nhau ω trong LCM. Thông thường, ω cân bằng chất lượng và tính đa dạng của mẫu. Một ω lớn hơn thường có xu hướng cải thiện chất lượng mẫu (được chỉ ra bởi CLIP), nhưng có thể ảnh hưởng đến tính đa dạng (được đo bằng FID). Vượt quá một ngưỡng nhất định, một ω tăng lên mang lại điểm CLIP tốt hơn với chi phí của FID. Hình 4 trình bày kết quả cho các ω khác nhau trên các bước suy luận khác nhau. Những phát hiện của chúng tôi bao gồm: 1) Sử dụng ω lớn nâng cao chất lượng mẫu (Điểm CLIP) nhưng dẫn đến FID tương đối kém hơn. 2) Khoảng cách hiệu suất giữa các bước suy luận 2, 4 và 8 là không đáng kể, làm nổi bật hiệu quả của LCM trong các vùng 2∼8 bước. Tuy nhiên, một khoảng cách đáng chú ý tồn tại trong suy luận một bước, cho thấy có chỗ để cải thiện thêm. Chúng tôi trình bày các hình ảnh hóa cho các ω khác nhau trong Hình 5. Có thể thấy rõ ràng rằng một ω lớn hơn nâng cao chất lượng mẫu, xác minh hiệu quả của phương pháp chưng cất có hướng dẫn một giai đoạn của chúng tôi.

5.3 KẾT QUẢ TINH CHỈNH NHẤT QUÁN HẠ NGUỒN
Chúng tôi thực hiện Tinh chỉnh Nhất quán Tiềm ẩn (LCF) trên hai tập dữ liệu hình ảnh tùy chỉnh, tập dữ liệu Pokemon (Pinkney, 2022) và tập dữ liệu Simpsons (Norod78, 2022), để chứng minh hiệu quả của LCF. Mỗi tập dữ liệu, bao gồm hàng trăm cặp văn bản-hình ảnh tùy chỉnh, được chia sao cho 90% được sử dụng để tinh chỉnh và 10% còn lại để kiểm tra. Đối với LCF, chúng tôi sử dụng LCM được đào tạo trước ban đầu được đào tạo ở độ phân giải 768×768 được sử dụng trong Bảng 2. Đối với hai tập dữ liệu này, chúng tôi tinh chỉnh LCM được đào tạo trước trong 30K lần lặp với tỷ lệ học 8e-6. Chúng tôi trình bày kết quả định tính của việc áp dụng LCF trên hai tập dữ liệu hình ảnh tùy chỉnh trong Hình 6. LCM được tinh chỉnh có khả năng tạo ra hình ảnh với phong cách tùy chỉnh trong ít bước, cho thấy hiệu quả của phương pháp của chúng tôi.

6 KẾT LUẬN
Chúng tôi trình bày Mô hình Nhất quán Tiềm ẩn (LCM), và một phương pháp chưng cất có hướng dẫn một giai đoạn có hiệu quả cao cho phép suy luận ít bước hoặc thậm chí một bước trên các LDM được đào tạo trước. Hơn nữa, chúng tôi trình bày tinh chỉnh nhất quán tiềm ẩn (LCF), để cho phép suy luận ít bước của LCM trên các tập dữ liệu hình ảnh tùy chỉnh. Các thí nghiệm mở rộng trên tập dữ liệu LAION-5B-Aesthetics chứng minh sự vượt trội

--- TRANG 8 ---
Preprint
Hình 4: Nghiên cứu tách biệt về các tỷ lệ hướng dẫn không phân loại khác nhau ω. ω lớn hơn dẫn đến chất lượng mẫu tốt hơn (Điểm CLIP). Khoảng cách hiệu suất giữa 2, 4 và 8 bước là tối thiểu, cho thấy hiệu quả của LCM.

ω= 2.0 ω= 4.0 ω= 8.0 ω= 12.0 ω= 2.0 ω= 4.0 ω= 8.0 ω= 12.0
Hình 5: LCM 4 bước sử dụng các tỷ lệ CFG khác nhau ω. LCM sử dụng chưng cất có hướng dẫn một giai đoạn để kết hợp trực tiếp các tỷ lệ CFG ω. ω lớn hơn nâng cao chất lượng hình ảnh.

hiệu suất và hiệu quả của LCM. Công việc tương lai bao gồm mở rộng phương pháp của chúng tôi cho nhiều tác vụ tạo hình ảnh hơn như chỉnh sửa hình ảnh có hướng dẫn văn bản, sơn và siêu độ phân giải.

TÀI LIỆU THAM KHẢO
[Danh sách tài liệu tham khảo tiếng Anh được giữ nguyên do tính chất kỹ thuật]

--- TRANG 9 ---
Preprint
LCM Gốc Tinh chỉnh 1K Tinh chỉnh 10K Tinh chỉnh 30K
LCM Gốc Tinh chỉnh 1K Tinh chỉnh 10K Tinh chỉnh 30K
Hình 6: LCM 4 bước sử dụng Tinh chỉnh Nhất quán Tiềm ẩn (LCF) trên hai tập dữ liệu tùy chỉnh: Tập dữ liệu Pokemon (trái), Tập dữ liệu Simpsons (phải). Thông qua LCF, LCM tạo ra hình ảnh với phong cách tùy chỉnh.

[Phần còn lại của tài liệu bao gồm các phụ lục và tài liệu tham khảo được dịch tương tự]
