# 2310.02992.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.02992.pdf
# Kích thước tệp: 12282153 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
KOSMOS-G: Tạo Hình Ảnh Trong Ngữ Cảnh
với Mô Hình Ngôn Ngữ Lớn Đa Phương Thức
Xichen Pan1,2∗Li Dong1Shaohan Huang1Zhiliang Peng1Wenhu Chen3Furu Wei1
1Microsoft Research2New York University3University of Waterloo
https://aka.ms/GeneralAI
       mặctrongáo khoácchụp selfie tại
bơi dưới nước trongtại Acropolis trong trang phục batman
++
Thực thể
Thực thểLời nhắc Đa phương thức
Lời nhắc Đa phương thức
Hình ảnh Được TạoHình ảnh Được Tạo
Hình 1: Các ví dụ tạo hình ảnh theo chủ đề không cần tinh chỉnh với lời nhắc đa phương thức. Nhờ vào khả năng nhận thức đa phương thức tiên tiến của MLLM, KOSMOS-G có thể tạo ra hình ảnh theo chủ đề độ chính xác cao bằng cách tiếp cận tất cả đầu vào hình ảnh như một "ngôn ngữ ngoại lai".
∗Đóng góp trong thời gian thực tập tại Microsoft Research.arXiv:2310.02992v3 [cs.CV] 26 Apr 2024

--- TRANG 2 ---
Tóm tắt
Những tiến bộ gần đây trong việc tạo hình ảnh theo chủ đề đã đạt được những bước tiến đáng kể.
Tuy nhiên, các phương pháp hiện tại vẫn còn thiếu sót trong các tình huống ứng dụng đa dạng,
vì chúng yêu cầu tinh chỉnh thời gian thử nghiệm và không thể chấp nhận đầu vào hình ảnh và văn bản
đan xen nhiều ảnh. Những hạn chế này khiến chúng còn xa mục tiêu cuối cùng là "hình ảnh như một ngôn ngữ
ngoại lai trong việc tạo hình ảnh." Bài báo này trình bày KOSMOS-G, một mô hình
tận dụng khả năng nhận thức đa phương thức tiên tiến của Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs)
để giải quyết thách thức nêu trên. Cách tiếp cận của chúng tôi
căn chỉnh không gian đầu ra của MLLM với CLIP bằng cách sử dụng phương thức văn bản làm điểm neo
và thực hiện tinh chỉnh hướng dẫn thành phần trên dữ liệu được tuyển chọn. KOSMOS-G
thể hiện khả năng ấn tượng trong việc tạo hình ảnh theo chủ đề không cần tinh chỉnh với
đầu vào hình ảnh và văn bản đan xen nhiều ảnh. Đáng chú ý, việc tinh chỉnh hướng dẫn chưng cất điểm số
không yêu cầu sửa đổi bộ giải mã hình ảnh. Điều này cho phép thay thế liền mạch
CLIP và tích hợp dễ dàng với vô số kỹ thuật U-Net
từ điều khiển tinh vi đến các biến thể bộ giải mã hình ảnh cá nhân hóa. Chúng tôi coi
KOSMOS-G như một nỗ lực ban đầu hướng tới mục tiêu "hình ảnh như một ngôn ngữ ngoại lai
trong việc tạo hình ảnh." Mã nguồn có thể được tìm thấy tại https://aka.ms/Kosmos-G

1 Giới thiệu
Trong các nghiên cứu gần đây, những tiến bộ trong việc tạo hình ảnh từ văn bản (T2I), đặc biệt với các mô hình khuếch tán,
đã cho thấy sự tiến bộ đáng chú ý trong việc tạo ra những hình ảnh có tính chân thực cao, chính xác và đa dạng
từ mô tả bằng văn bản. Dựa trên thành công chưa từng có trong việc tạo ra những hình ảnh có độ chính xác cao
từ mô tả văn bản, nhiều nghiên cứu đã đi sâu vào việc tạo hình ảnh theo chủ đề phức tạp hơn
để tích hợp hình ảnh vào lời nhắc văn bản để tạo ra hình ảnh tùy chỉnh mới.

Một nhóm phương pháp [GAA+22,RLJ+22,KZZ+23,TGCA23,AAF+23,HHZW23,SHZ+23]
đề xuất tinh chỉnh các mô hình trên từng tập hình ảnh tham chiếu, và không đạt được việc tạo hình ảnh theo chủ đề
thông qua một mô hình được tiền huấn luyện tổng quát. [XYF+23,WZJ+23,CHL+23,CHSC22] tiêm
các đặc trưng hình ảnh vào U-Net của các mô hình khuếch tán. Tuy nhiên, những phương pháp tiêm như vậy tách biệt
hướng dẫn cho văn bản và hình ảnh, do đó hạn chế hiệu quả của việc mô hình hóa kết hợp giữa hai
phương thức. Ngoài ra, cách tiếp cận này khó mở rộng cho các tình huống có nhiều
thực thể. Công trình gần đây BLIP-Diffusion [LLH23] học biểu diễn đối tượng bằng cách tổng hợp hình ảnh
thông qua việc kết hợp các chủ đề với nền ngẫu nhiên. Cách tiếp cận này hiệu quả trong việc trang bị cho nó
khả năng tạo hình ảnh từ văn bản theo chủ đề không cần tinh chỉnh. Tuy nhiên, thiết kế cụ thể của
mẫu đầu vào và dữ liệu huấn luyện hạn chế khả năng mở rộng đến nhiều thực thể.

Trái ngược với các phương pháp trước đây hoạt động với bộ mã hóa văn bản CLIP gốc [RKH+21], chúng tôi đề xuất rằng
thông qua việc tận dụng Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs) [ADL+22,HSD+22,AHR+22,
HDW+23,LLSH23], hầu hết các thách thức trong việc tạo hình ảnh theo chủ đề có thể được giải quyết dễ dàng.
MLLMs đã mở rộng khả năng nhận thức của các mô hình ngôn ngữ đến đa phương thức, cho phép
chúng nhận thức các phương thức đa dạng như hình ảnh. Ý tưởng tận dụng MLLMs cho việc tạo hình ảnh theo chủ đề
mang lại một số lợi thế: 1) Nó tận dụng việc căn chỉnh thị giác-ngôn ngữ vốn có
trong MLLM. 2) Kiến trúc MLLM tự nhiên hỗ trợ đầu vào hình ảnh và văn bản đan xen nhiều ảnh. 3) MLLM được tiền huấn luyện có thể mô hình hóa hiệu quả đầu vào đa phương thức trong ngữ cảnh.

Để hỗ trợ việc tạo hình ảnh theo chủ đề không cần tinh chỉnh với đầu vào hình ảnh và văn bản đan xen nhiều ảnh, chúng tôi trình bày
KOSMOS-G, tận dụng khả năng nhận thức đa phương thức tiên tiến của MLLM theo cách "căn chỉnh
trước khi hướng dẫn". Cụ thể, chúng tôi bắt đầu từ giai đoạn mô hình hóa ngôn ngữ đa phương thức, dẫn đến
MLLM KOSMOS-1[HDW+23]. Nó hình dung các mô hình ngôn ngữ như một lớp tác vụ phổ quát,
nhận thức đầu vào thị giác-ngôn ngữ đan xen tự do và hợp nhất các dự đoán tác vụ khác nhau
thành các định dạng văn bản. Với biểu diễn thị giác-ngôn ngữ được căn chỉnh, sau đó chúng tôi sử dụng
phương thức ngôn ngữ làm điểm neo và căn chỉnh không gian đầu ra của MLLM với bộ mã hóa văn bản CLIP. Cuối cùng,
chúng tôi thực hiện tinh chỉnh hướng dẫn trên dữ liệu được tuyển chọn. KOSMOS-G chấp nhận chú thích làm đầu vào, trong đó
mỗi thực thể được theo sau bởi hình ảnh được phân đoạn của nó. Mô hình được huấn luyện để tái tạo chung thực
tất cả các thực thể, hiển thị nội dung văn bản và tuân theo hướng dẫn. Trong quá trình này, bộ giải mã hình ảnh khuếch tán
được tiền huấn luyện đông lạnh đóng vai trò như một chỉ số điểm số. Chúng tôi chưng cất phân phối dữ liệu đã học để truyền
gradient có thể vi phân đến MLLM. Điều này cho phép KOSMOS-G khai thác các đặc trưng phong phú từ

--- TRANG 3 ---
ℰ𝒟Cross AttnCross Attnlàm tranh sơn dầu theo phong cách
🔥 Mô hình Ngôn ngữ Lớn Đa phương thức
🔥AlignerNet
Lời nhắc Thị giác-Ngôn ngữ Đan xen
❄
❄Hình 2: KOSMOS-G bao gồm một MLLM cho nhận thức đa phương thức, kết hợp với một AlignerNet
kết nối MLLM với bộ giải mã hình ảnh U-Net khuếch tán. KOSMOS-G có thể truyền hướng dẫn
cấp độ khái niệm tinh vi từ đầu vào đan xen đến bộ giải mã hình ảnh, và cung cấp một giải pháp thay thế liền mạch cho CLIP.
Màu cam biểu thị các mô-đun có thể huấn luyện; Màu xanh biểu thị các mô-đun bị đông lạnh.

bộ mã hóa hình ảnh để tạo ra hình ảnh tái tạo chung thực nội dung qua các ngữ cảnh khác nhau (xem
Hình 1, chúng tôi cũng trình bày các ví dụ với sự đan xen đa dạng hơn trong Hình 9).

Được hưởng lợi từ việc tiền huấn luyện đa mục đích, KOSMOS-G tiếp cận mục tiêu "hình ảnh như một
ngôn ngữ ngoại lai trong việc tạo hình ảnh." Điều này có nghĩa là KOSMOS-G có thể nắm bắt các khái niệm mới từ
hình ảnh đầu vào và hướng dẫn việc tạo ra cá nhân hóa trong cài đặt không cần tinh chỉnh. Đáng chú ý, KOSMOS-G cũng là
mô hình đầu tiên thành thạo việc tạo hình ảnh theo chủ đề không cần tinh chỉnh với đầu vào hình ảnh và văn bản đan xen nhiều ảnh. Nhờ vào việc tinh chỉnh hướng dẫn chưng cất điểm số, KOSMOS-G không cần sửa đổi bất kỳ
tham số nào của bộ giải mã hình ảnh, tức là U-Net khuếch tán và VAE. Điều này khiến chúng tôi có thể
thay thế liền mạch CLIP bằng KOSMOS-G trong bất kỳ hệ thống tạo hình ảnh nào. Kết quả là, vô số
ứng dụng có thể được mở khóa kết hợp với các kỹ thuật U-Net, từ điều khiển tinh vi
như ControlNet [ZA23] đến các biến thể bộ giải mã hình ảnh cá nhân hóa hoặc phong cách hóa như các
checkpoint LoRA [HSW+22] đóng góp bởi cộng đồng tuyệt vời.

Tổng thể, chúng tôi đề xuất KOSMOS-G như một nỗ lực ban đầu hướng tới mục tiêu "hình ảnh như một ngôn ngữ
ngoại lai trong việc tạo hình ảnh." Chúng tôi tóm tắt những đóng góp chính như sau:

1. Chúng tôi đề xuất tận dụng khả năng nhận thức đa phương thức tiên tiến của MLLMs cho việc tạo hình ảnh theo chủ đề
với đầu vào hình ảnh và văn bản đan xen nhiều ảnh.

2. Chúng tôi đề xuất một tác vụ tinh chỉnh hướng dẫn thành phần, dẫn đến khả năng tạo hình ảnh theo chủ đề
nhiều thực thể không cần tinh chỉnh tuyệt vời.

3. Tinh chỉnh hướng dẫn chưng cất điểm số cho phép KOSMOS-G giao tiếp liền mạch với một
loạt các kỹ thuật U-Net, cho thấy khả năng ứng dụng rộng rãi và tiềm năng tích hợp
vào các khung khác nhau.

2 KOSMOS-G: Hình ảnh như một Ngôn ngữ Ngoại lai trong Việc Tạo Hình ảnh

Như được hiển thị trong Hình 2, KOSMOS-G là một mô hình có thể nhận thức đầu vào hình ảnh và văn bản đan xen nhiều ảnh
và tạo ra các điều kiện theo chủ đề. Cụ thể, xương sống của MLLM KOSMOS-G là
một mô hình ngôn ngữ nhân quả dựa trên Transformer, đóng vai trò như một giao diện đa mục đích cho đầu vào đa phương thức.
Chúng tôi huấn luyện KOSMOS-G theo cách "căn chỉnh trước khi hướng dẫn", toàn bộ quy trình huấn luyện
có thể được chia thành 3 giai đoạn:

1. Mô hình hóa Ngôn ngữ Đa phương thức: Chúng tôi tiền huấn luyện MLLM từ đầu trên tập ngữ liệu đa phương thức,
bao gồm dữ liệu đơn phương thức, dữ liệu ghép nối chéo-phương thức, và dữ liệu đa phương thức đan xen
với tổn thất mô hình hóa ngôn ngữ theo KOSMOS-1.

--- TRANG 4 ---
2. Căn chỉnh Bộ giải mã Hình ảnh: Chúng tôi sử dụng U-Net [RFB15] của Stable Diffusion v1.5 [RBL+22]
làm bộ giải mã hình ảnh của chúng tôi. Chúng tôi đã huấn luyện một AlignerNet chỉ trên dữ liệu văn bản để căn chỉnh không gian đầu ra
của KOSMOS-G với không gian đầu vào của U-Net thông qua giám sát CLIP. Ở đây, ngôn ngữ
đóng vai trò như phương thức neo, đảm bảo đầu vào hình ảnh cũng tương thích với bộ giải mã hình ảnh.

3. Tinh chỉnh Hướng dẫn: Chúng tôi tiếp tục tinh chỉnh KOSMOS-G thông qua một tác vụ tạo hình ảnh thành phần
trên dữ liệu được tuyển chọn, với gradient có thể vi phân được truyền từ U-Net bị đông lạnh.

Trong Giai đoạn 1, chỉ có MLLM được huấn luyện. Trong Giai đoạn 2, AlignerNet được huấn luyện với MLLM bị đông lạnh.
Trong Giai đoạn 3, cả AlignerNet và MLLM đều được huấn luyện cùng nhau. Bộ giải mã hình ảnh vẫn bị đông lạnh
trong suốt tất cả các giai đoạn.

2.1 Mô hình hóa Ngôn ngữ Đa phương thức

Theo KOSMOS-1, KOSMOS-G nhận thức các phương thức tổng quát theo cách thống nhất. Để đạt được điều này, chúng tôi
biểu diễn định dạng đầu vào như một chuỗi duy nhất sử dụng các token đặc biệt. Cụ thể, chúng tôi sử dụng các token
<s> và </s> để biểu thị bắt đầu và kết thúc chuỗi. Chúng tôi cũng kết hợp các token <image> và </image>
để chỉ ra sự bắt đầu và kết thúc của bất kỳ biểu diễn hình ảnh nhúng nào trong chuỗi.

Phương pháp của chúng tôi liên quan đến việc mã hóa cả token văn bản và hình ảnh thành vector, sau đó được đưa
vào bộ giải mã. Đối với token văn bản, chúng tôi sử dụng bảng tra cứu để ánh xạ chúng thành các embedding. Để xử lý
các hình ảnh đầu vào, chúng tôi sử dụng một Transformer thị giác [DBK+21] làm mô-đun embedding. Hơn nữa,
Resampler [ADL+22] được sử dụng như một cơ chế pooling chú ý để giảm số lượng embedding hình ảnh. Sau khi thu được các embedding của một chuỗi đầu vào, chúng tôi đưa chúng vào bộ giải mã dựa trên Transformer.
Bộ giải mã nhân quả từ trái sang phải xử lý chuỗi theo cách tự hồi quy.
Một bộ phân loại softmax trên Transformer được sử dụng để gán xác suất cho từng token trong từ vựng.

KOSMOS-G được huấn luyện đầu tiên bằng cách sử dụng tác vụ dự đoán token tiếp theo. Mục tiêu huấn luyện là tối đa hóa
log-likelihood của các token trong các ví dụ. Điều quan trọng cần lưu ý là tổn thất huấn luyện chỉ xem xét
các token rời rạc, cụ thể là token văn bản. Thành phần MLLM có 24 lớp với 2,048 chiều ẩn, kích thước trung gian FFN 8,192, và 32 đầu chú ý. Để hội tụ nhanh hơn,
biểu diễn hình ảnh được thu được từ mô hình CLIP ViT-L/14 được tiền huấn luyện với 1,024 chiều đặc trưng. Các hình ảnh được tiền xử lý thành độ phân giải 224×224 trong quá trình huấn luyện. Chúng tôi đông lạnh
các tham số của mô hình CLIP ngoại trừ lớp cuối cùng trong quá trình huấn luyện. Tổng số tham số
của MLLM là khoảng 1.6B.

2.2 Căn chỉnh Bộ giải mã Hình ảnh

Sau khi thực hiện mô hình hóa ngôn ngữ đa phương thức, chúng tôi đã căn chỉnh thành công nhận thức thị giác và ngôn ngữ
trong MLLM. Để làm cho KOSMOS-G có khả năng tạo hình ảnh, chúng tôi kết hợp
các mô hình khuếch tán [SWMG15] làm bộ giải mã hình ảnh của chúng tôi. Cụ thể, chúng tôi áp dụng
Stable Diffusion v1.5 [RBL+22] được chấp nhận rộng rãi. Điều quan trọng cần lưu ý là chúng tôi chỉ thay thế bộ mã hóa văn bản CLIP [RKH+21] bằng KOSMOS-G đa phương thức, mà không thực hiện bất kỳ sửa đổi nào đối với kiến trúc U-Net
hoặc trọng số. Thiết lập này cho phép KOSMOS-G hợp tác hiệu quả với các kỹ thuật
được áp dụng cho U-Net, như ControlNet [ZA23] và các biến thể LoRA [HSW+22] khác nhau của cộng đồng. Trong
phần này, chúng tôi sẽ cung cấp những điều kiện tiên quyết ngắn gọn về các mô hình khuếch tán tiềm ẩn, và sau đó đi sâu vào
quá trình căn chỉnh không gian đầu ra của KOSMOS-G với bộ giải mã hình ảnh sau khi thay thế nêu trên.

Điều kiện tiên quyết về Mô hình Khuếch tán Tiềm ẩn Các mô hình khuếch tán định nghĩa một chuỗi Markov của quá trình khuếch tán thuận q, thêm các mẫu nhiễu Gaussian vào dữ liệu thực ban đầu z0∼q(z) qua T bước.
Ở đây, z biểu thị các biểu diễn tiềm ẩn thay vì giá trị pixel. Không gian tiềm ẩn hiệu quả, có chiều thấp
gần như tương đương về mặt nhận thức với không gian RGB có chiều cao, trong khi
thông tin dư thừa không có ý nghĩa về mặt ngữ nghĩa có trong miền pixel bị loại bỏ. Các mô hình nén nhận thức (tức là VQ-VAE) bao gồm E và D mã hóa dữ liệu thực vào không gian tiềm ẩn
và ngược lại, sao cho D(E(x))≈x. Các mô hình khuếch tán tiềm ẩn sử dụng các biểu diễn tiềm ẩn z=E(x)
thay vì làm việc trực tiếp với giá trị pixel trong quá trình khuếch tán. Đầu ra cuối cùng có thể
được giải mã trở lại không gian pixel thông qua D(z). Giai đoạn nén nhận thức nhẹ riêng biệt chỉ
loại bỏ các chi tiết không thể nhận thức, dẫn đến kết quả tạo hình cạnh tranh với chi phí thấp hơn nhiều.

--- TRANG 5 ---
Không gian CLIP-TVăn bảnKéo Gầnℳ𝒩Tái cấu trúcKhông gian KOSMOS-GVăn bảnHình ảnh(a) Quá trình căn chỉnh. Văn bản đóng vai trò như một
điểm neo, embedding hình ảnh được căn chỉnh tự nhiên trong suốt quá trình.
ℳ-encoderℳ-decoder
Embedding nguồnℳ-Linear
Chú thích Đầu vàoKOSMOS-GCLIP-TEmbedding đíchmà cấu trúc nguồn embeddingTổn thất MSETổn thất MSEEmbedding được căn chỉnhTổn thất MSETái cấu trúc embedding nguồn𝒩-Linear𝒩-decoder𝒩-encoder(b) Kiến trúc AlignerNet. Các lớp Linear được sử dụng để chiếu
chiều đầu ra của MLLM thành d=768, các phần tử màu tím
biểu thị các truy vấn tiềm ẩn đã học QM và QN.

Hình 3: Tổng quan về căn chỉnh.

Quá trình thuận q(zt|zt−1) tại mỗi bước thời gian t có thể được biểu diễn như sau:
q(zt|zt−1) = N(zt;√(1−βt)zt−1, βtI)
q(z1:T|z0) = ∏(t=1 to T)q(zt|zt−1)   (1)

trong đó βt∈(0,1) biểu thị kích thước bước. Lưu ý βt−1 < βt.

Các mô hình khuếch tán học một U-Net [RFB15] được ký hiệu là ϵθ để đảo ngược quá trình khuếch tán thuận,
xây dựng các mẫu dữ liệu mong muốn từ nhiễu. Đặt αt = 1−βt và ᾱt = ∏(i=1 to t)αi. Chúng ta có thể
tham số hóa lại quá trình khử nhiễu p(zt−1|zt) cũng như một phân phối Gaussian. Phân phối này
có thể được ước tính bởi ϵθ và có dạng sau:
pθ(zt−1|zt) = N(zt−1;μθ(zt,t),Σθ(zt,t))
với μθ(zt,t) = (1/√αt)(zt − (βt/√(1−ᾱt))ϵθ(zt,t))   (2)

Mục tiêu học tập của các mô hình khuếch tán là xấp xỉ trung bình μθ(zt,t) trong quá trình khuếch tán ngược. Để đạt được điều này, chúng ta có thể sử dụng cận dưới biến phân (ELBO) [KW14] để
tối thiểu hóa negative log-likelihood của pθ(z0)[HJA20]. Mục tiêu đơn giản hóa có thể được biểu diễn
như một mục tiêu khử nhiễu:
Ldiff = Ez0,ϵ∼N(0,1),t[‖ϵ−ϵθ(zt,t)‖²]   (3)

Trong quá trình suy luận, [HS22] đề xuất sử dụng hướng dẫn không có bộ phân loại để thu được kết quả tạo hình
liên quan hơn.
ε̂ = w·ϵθ(zt,φ,t)−(w−1)·ϵθ(zt,t)   (4)
trong đó w là tỷ lệ hướng dẫn, φ biểu thị điều kiện.

Căn chỉnh Không gian Đầu ra với Các mô hình Khuếch tán Khi thay thế bộ mã hóa văn bản CLIP trước đó bằng
KOSMOS-G, trọng tâm chính là giải quyết vấn đề không căn chỉnh giữa KOSMOS-G và bộ giải mã hình ảnh. Chúng tôi phát hiện ra rằng việc chỉ tinh chỉnh KOSMOS-G bằng cách sử dụng gradient được truyền từ
bộ giải mã hình ảnh dẫn đến cả căn chỉnh tầm thường và chất lượng hình ảnh bị ảnh hưởng.

Được truyền cảm hứng bởi [QYX+23], chúng tôi đề xuất AlignerNet bao gồm một bộ mã hóa M và một bộ giải mã N
để học căn chỉnh giữa không gian nguồn S của KOSMOS-G và không gian đích T của bộ mã hóa văn bản CLIP. Cho một chú thích chỉ văn bản duy nhất C, bộ mã hóa nguồn KOSMOS-G và bộ mã hóa đích văn bản CLIP
mã hóa chú thích thành các embedding được ký hiệu là s∈R^(ls×ds) và t∈R^(lt×dt), tương ứng. Ở đây, l và
d chỉ ra độ dài của các đặc trưng và chiều embedding.

--- TRANG 6 ---
Như được hiển thị trong Hình 3a, chúng tôi sử dụng bộ mã hóa M để tối thiểu hóa khoảng cách giữa embedding nguồn văn bản và embedding đích, nhằm mục đích xấp xỉ gần M(s)≈t thông qua:
Lmse = Es∼S,t∼T[‖t−M(s)‖²₂]   (5)

Để giảm thiểu việc giảm phân biệt đặc trưng, chúng tôi cũng sử dụng một bộ giải mã N để tái cấu trúc embedding nguồn N(M(s))≈s thông qua:
Lrec = Es∼S[‖s−N(M(s))‖²₂]   (6)

Khác với [QYX+23], KOSMOS-G là một bộ mã hóa đa phương thức thị giác-ngôn ngữ. Phương thức ngôn ngữ đóng vai trò như một điểm neo trong suốt quá trình, căn chỉnh toàn bộ không gian KOSMOS-G với
không gian đầu vào của bộ giải mã hình ảnh, do đó cũng đạt được căn chỉnh ngữ nghĩa cho các embedding hình ảnh.

Để xử lý hiệu quả các chuỗi dài bao gồm nhiều hình ảnh và tối thiểu hóa việc sử dụng bộ nhớ,
KOSMOS-G mã hóa chuỗi đầu vào thị giác-ngôn ngữ đan xen thành các embedding có độ dài biến đổi. Tuy nhiên, việc sử dụng các embedding có độ dài biến đổi làm cho GlueNet dựa trên MLP [QYX+23]
không phù hợp để học căn chỉnh. Để giải quyết điều này, chúng tôi sử dụng một kiến trúc dựa trên Transformer trong
AlignerNet, cho phép nó căn chỉnh hiệu quả các không gian nguồn và đích với độ dài chuỗi không khớp và chiều embedding.

Như được hiển thị trong Hình 3b, cả M và N đều có cùng một thiết kế kiến trúc tương tự, bao gồm một bộ mã hóa Transformer và một bộ giải mã Transformer. Bộ mã hóa Transformer và bộ giải mã trong cả hai mô hình đều bao gồm 12 lớp, với chiều đầu vào d=768 và chiều ẩn là 3072. Cấu hình này
dẫn đến khoảng 225M tham số tổng cộng. Trong mô-đun cross attention của bộ giải mã Transformer, chúng tôi sử dụng các truy vấn tiềm ẩn đã học có độ dài biến đổi QM∈R^(lt×d) trong M và QN∈R^(ls×d) trong N
để khớp độ dài chuỗi. Lưu ý rằng như đã thảo luận trong Phần 4.3, chúng tôi vẫn có thể căn chỉnh MLLM với
Kosmos-G thông qua việc sử dụng trực tiếp tổn thất khuếch tán trong Phương trình 3 với sự giúp đỡ của AlignerNet. Trong khi
nó tốn kém hơn và dẫn đến hiệu suất tệ hơn dưới cùng số ngày GPU.

2.3 Tinh chỉnh Hướng dẫn

Sau khi đạt được căn chỉnh ngữ nghĩa giữa KOSMOS-G và bộ giải mã hình ảnh, mô hình của chúng tôi
có thể tạo hình ảnh thành công theo hướng dẫn thị giác-ngôn ngữ đan xen. Tuy nhiên, việc mô hình hóa ngôn ngữ đa phương thức và giai đoạn căn chỉnh chỉ văn bản chỉ bảo tồn sự nhất quán ngữ nghĩa
giữa đầu vào và đầu ra, KOSMOS-G vẫn không thể tận dụng các đặc trưng phong phú được trích xuất từ
bộ mã hóa hình ảnh để tạo ra hình ảnh tái tạo chung thực nội dung trong các ngữ cảnh khác nhau.

Để theo đuổi mục tiêu "hình ảnh như một ngôn ngữ ngoại lai trong việc tạo hình ảnh", chúng tôi tuyển chọn dữ liệu thị giác-ngôn ngữ đan xen và sử dụng tổn thất khuếch tán trong Phương trình 3 để tinh chỉnh thêm KOSMOS-G.
Cụ thể, chúng tôi đề xuất một tác vụ tạo hình ảnh thành phần trong đó chúng tôi đầu vào các chú thích chứa
các thực thể, với mỗi thực thể được theo sau bởi hình ảnh tương ứng của chúng, như "<s>Một con mèo <image> embedding hình ảnh của con mèo </image> và một con chó <image> embedding hình ảnh của con chó </image> đang ngủ
trong vườn <image> embedding hình ảnh của vườn </image> </s>". Mô hình của chúng tôi được huấn luyện để
tạo hình ảnh theo hướng dẫn đầu vào.

Để xây dựng dữ liệu cần thiết, trước tiên chúng tôi tạo chú thích cho hình ảnh, sau đó trích xuất các thực thể từ chú thích,
và thu được kết quả phân đoạn từ chính hình ảnh đó. Một giới thiệu chi tiết về toàn bộ
quy trình có thể được tìm thấy trong Phần 3.1. Ngoài ra, chúng tôi tận dụng dữ liệu được xây dựng bởi [BHE23] cho
InstructPix2Pix để cải thiện khả năng chỉnh sửa hình ảnh của KOSMOS-G. Dữ liệu này có cấu trúc như: "<s>
chú thích <image> embedding của hình ảnh gốc </image> hướng dẫn chỉnh sửa </s>". Chúng tôi cũng trộn
một số dữ liệu văn bản-thành-hình ảnh để bảo tồn căn chỉnh ngôn ngữ đã đạt được.

Mục tiêu của chúng tôi là tận dụng MLLMs để mô hình hóa phân phối hình ảnh thông qua việc lấy mẫu không gian tiềm ẩn trực tiếp.
Trong thiết lập này, U-Net Stable Diffusion được tiền huấn luyện đông lạnh đóng vai trò như một chỉ số điểm số, chưng cất
phân phối dữ liệu đã học. Chiến lược này tương tự như Score Distillation Sampling (SDS) [PJBM22].
Từ góc độ chưng cất điểm số, KL divergence giữa KOSMOS-G (được ký hiệu là ϕ,
mã hóa đầu vào thành điều kiện C) và hàm điểm số được tối thiểu hóa tương đương để chưng cất
mật độ xác suất đã học trong bộ giải mã hình ảnh:
minφ LDiff = Ez0,t,C[DKL(q(zt−1|zt,z0)‖pθ(zt−1|zt;C))]   (7)

--- TRANG 7 ---
Thẻ: giường, ghế, tivi, bànHình ảnhChú thích Hình ảnhChú thích: Một phòng ngủ với một chiếc giường, một chiếc ghế, một chiếc tivi, và một chiếc bànMô hình Ngôn ngữ LớnMô hình Phân đoạn Hình ảnh

Một phòng ngủ với một chiếc giường        , một chiếc ghế        , một chiếc tivi       , và một chiếc bàn

Hình 4: Tổng quan về quy trình xây dựng dữ liệu của chúng tôi cho tinh chỉnh hướng dẫn tạo hình thành phần.

Điều này cho phép KOSMOS-G tận dụng các đặc trưng phong phú từ bộ mã hóa hình ảnh để tạo ra một hình ảnh
tái tạo chung thực nội dung qua các ngữ cảnh khác nhau. Thêm chi tiết về tinh chỉnh hướng dẫn chưng cất điểm số
có thể được tìm thấy trong Phụ lục C.

3 Huấn luyện Mô hình

3.1 Dữ liệu Huấn luyện Đa phương thức

Giai đoạn mô hình hóa ngôn ngữ đa phương thức trong Phần 2.1 sử dụng cùng thiết lập của KOSMOS-
1[HDW+23], nơi các mô hình được huấn luyện trên tập ngữ liệu đa phương thức quy mô web, bao gồm tập ngữ liệu văn bản,
các cặp hình ảnh-chú thích, và dữ liệu đan xen của hình ảnh và văn bản. Cho giai đoạn căn chỉnh bộ giải mã hình ảnh trong Phần 2.2, chúng tôi chỉ sử dụng chú thích từ các cặp hình ảnh-chú thích. Cho giai đoạn tinh chỉnh hướng dẫn trong Phần 2.3, chúng tôi sử dụng dữ liệu được xây dựng từ bộ dữ liệu Open Images V7 [KRA+20], các cặp hình ảnh-chú thích, cũng như dữ liệu chỉnh sửa hình ảnh từ InstructPix2Pix [BHE23].

Chú thích Các cặp hình ảnh-chú thích được lấy từ nhiều bộ dữ liệu, bao gồm English
LAION-2B [SBV+22], LAION-400M [SVB+21], COYO-700M [BPK+22], và Conceptual Captions [SDGS18,CSDS21]. English LAION-2B, LAION-400M, và COYO-700M được thu thập từ
dữ liệu web Common Crawl bằng cách trích xuất hình ảnh và văn bản thay thế tương ứng. Conceptual captions
cũng được lấy từ các trang web.

Dữ liệu Được xây dựng Chúng tôi sử dụng khoảng 9M hình ảnh từ bộ dữ liệu Open Images V7 [KRA+20]
để xây dựng dữ liệu tinh chỉnh hướng dẫn tạo hình thành phần của chúng tôi. Như được minh họa trong Hình 4, chúng tôi
bắt đầu bằng cách tạo chú thích với BLIP-2-OPT-6.7b [LLSH23]. Tiếp theo, chúng tôi sử dụng một LLM
MPT-7B-Instruct [T+23] để trích xuất các thực thể từ chú thích. Hình ảnh gốc, cùng với văn bản
của mỗi thực thể, sau đó được đưa vào mô hình phân đoạn được nhắc bằng văn bản CLIPSeg [LE22] để thu được
hình ảnh tương ứng của mỗi thực thể.

3.2 Thiết lập Huấn luyện

Việc triển khai của chúng tôi dựa trên thư viện TorchScale [MWH+22], được thiết kế cho
việc huấn luyện mô hình quy mô lớn. Theo KOSMOS-1[HDW+23], chúng tôi cũng sử dụng MAGNETO [WMH+22], một
biến thể Transformer, làm kiến trúc xương sống của MLLM và AlignerNet của chúng tôi. Toàn bộ quá trình huấn luyện
mất khoảng bốn ngày với 256 GPU NVIDIA V100, tức là một ngày cho căn chỉnh bộ giải mã hình ảnh,
và ba ngày cho tinh chỉnh hướng dẫn. Trong giai đoạn tinh chỉnh hướng dẫn, chúng tôi sử dụng một hỗn hợp của dữ liệu được xây dựng,
dữ liệu InstructPix2Pix, và dữ liệu chú thích theo tỷ lệ 2:2:1. Đối với dữ liệu được xây dựng, để tăng cường
tính mạnh mẽ của đầu vào, chúng tôi ngẫu nhiên bỏ qua các văn bản của các thực thể với xác suất 0.5 và cũng duy trì
nền của các thực thể được phân đoạn với xác suất 0.5. Các cấu hình huấn luyện khác có thể
được tìm thấy trong Phụ lục A

4 Đánh giá

4.1 Kết quả Định tính Chính

Như được hiển thị trong Hình 5, KOSMOS-G mang lại kết quả tạo hình không cần tinh chỉnh ấn tượng qua các
thiết lập đa dạng, tạo ra đầu ra có ý nghĩa và mạch lạc ngay cả đối với các chủ đề được tùy chỉnh cao. Các

--- TRANG 8 ---
làm tranh sơn dầu bởi van Goghba lô có chủ đềtrong Minecrafttrong Unity3D
mặcăntrongbộ đồ củatrong phong cách củaHình 5: Các ví dụ tạo hình ảnh không cần tinh chỉnh với lời nhắc đa phương thức.

Phương pháp DINO ↑CLIP-I ↑CLIP-T ↑
Hình ảnh Thực (Oracle) 0.774 0.885 -
Tinh chỉnh
Textual Inversion [GAA+22] 0.569 0.780 0.255
DreamBooth [RLJ+22] 0.668 0.803 0.305
BLIP-Diffusion [LLH23] 0.670 0.805 0.302
Không cần Tinh chỉnh Thời gian Thử nghiệm
Re-Imagen∗[CHSC22] 0.600 0.740 0.270
SuTI [CHL+23] 0.741 0.819 0.304
BLIP-Diffusion∗[LLH23] 0.594 0.779 0.300
KOSMOS-G∗(đầu vào hình ảnh đơn) 0.694 0.847 0.287Phương pháp FID ↓
Mô hình T2I
GLIDE [NDR+22] 12.24
Make-A-Scene [GPA+22] 11.84
DALL-E 2 [RDN+22] 10.39
SD v1.5†[RBL+22] 9.34
Imagen-3.4B [SCS+22] 7.27
Mô hình VL2I Căn chỉnh CLIP
GILL-8B [KFS23] 12.20
Emu-14B [SYC+23] 11.66
KOSMOS-G-1.9B 10.99

Bảng 1: Trái: So sánh định lượng trên DreamBench. ∗biểu thị phương pháp không cần tinh chỉnh. Phải: So sánh FID không cần tinh chỉnh trên MS-COCO. †chỉ ra kết quả được chúng tôi đánh giá dưới cùng thiết lập và seed với KOSMOS-G.

mẫu hình ảnh thể hiện khả năng tạo hình trong việc tái ngữ cảnh hóa, phong cách hóa, sửa đổi, và
kết hợp phụ kiện. Đáng chú ý, việc tạo hình ảnh theo chủ đề nhiều thực thể rất thách thức ngay cả đối với
các phương pháp tinh chỉnh như DreamBooth [RLJ+22,AAF+23]. Trong khi nhờ vào việc tinh chỉnh hướng dẫn tạo hình thành phần mới, KOSMOS-G là mô hình đầu tiên có khả năng đạt được điều này trong
thiết lập không cần tinh chỉnh.

4.2 Kết quả Định lượng

Chúng tôi thực hiện đánh giá định lượng KOSMOS-G trên DreamBench [RLJ+22] cho việc tạo hình ảnh theo chủ đề
thực thể đơn và MS-COCO [LMB+14] cho việc tạo hình ảnh từ văn bản.

Bộ dữ liệu DreamBench chứa 30 chủ đề và có 25 mẫu lời nhắc, tạo ra 750
lời nhắc duy nhất bao gồm các kỹ năng như tái ngữ cảnh hóa, sửa đổi, phụ kiện hóa, v.v. Chúng tôi
theo các công trình trước đây để tạo ra 4 hình ảnh cho mỗi lời nhắc để tạo thành 3000 hình ảnh cho một
đánh giá toàn diện. Chúng tôi theo DreamBooth để áp dụng DINO, CLIP-I để đánh giá độ trung thực chủ đề,

--- TRANG 9 ---
KOSMOS-G trước tinh chỉnh hướng dẫn Stable Diffusion v1.5 [RBL+22]

Hình 6: So sánh với các trường hợp được trình bày trong hàng thứ hai của Hình 1.

làm tranh sơn dầu trong phong cách

(a) KOSMOS-G với điều khiển canny sử dụng ControlNet.

chơi

(b) KOSMOS-G với biến thể LoRA.

Hình 7: Các Ứng dụng Khác nhau của KOSMOS-G Kết hợp với Các Kỹ thuật U-Net. Trong Hình 7b,
hình ảnh bên trái được tạo ra bằng U-Net tiêu chuẩn, hình ảnh bên phải được tạo ra với U-Net được tinh chỉnh LoRA.

và CLIP-T để đánh giá độ trung thực văn bản. Chúng tôi sử dụng tỷ lệ hướng dẫn không có bộ phân loại là 7.5 và 100
bước suy luận DPM-Solver [LZB+22] cho việc lấy mẫu. Như được hiển thị trong Bảng 1, KOSMOS-G không cần tinh chỉnh vượt trội hơn Textual Inversion và Re-Imagen và thể hiện hiệu suất tốt hơn một chút so với
DreamBooth và BLIP-Diffusion chỉ với đầu vào hình ảnh đơn. Hơn nữa, kết quả của chúng tôi cũng
có thể so sánh với SuTI, mà không cần giám sát học việc đắt đỏ. KOSMOS-G
chấp nhận chỉ một hình ảnh đơn làm đầu vào, chúng tôi chọn một hình ảnh rõ ràng từ 4-7 hình ảnh được cung cấp cho mỗi
chủ đề để tránh che khuất. Chúng tôi sửa đổi nhẹ mẫu lời nhắc để đảm bảo căn chỉnh tốt hơn với
dữ liệu tinh chỉnh hướng dẫn. Các hình ảnh và lời nhắc được sử dụng có thể được tìm thấy trong Phụ lục B.

Đối với việc tạo hình ảnh từ văn bản, chúng tôi tạo hình ảnh bằng cách sử dụng 30,000 chú thích được lấy mẫu ngẫu nhiên
từ tập xác thực MS-COCO (2014). Chúng tôi sử dụng tỷ lệ hướng dẫn không có bộ phân loại là 3.0 và 250
bước suy luận DDIM [SME21] cho việc lấy mẫu. Như được hiển thị trong Bảng 1, KOSMOS-G vượt trội hơn các
mô hình VL2I căn chỉnh CLIP khác, mang lại kết quả căn chỉnh tối ưu.

4.3 Nghiên cứu Loại bỏ

Phương pháp FID ↓
SD v1.5 [RBL+22] 9.34
E2E w/o AlignerNet Thất bại
E2E w/ AlignerNet 11.30
12-Layers Decoder Thất bại
12-Layers AlignerNet 9.89
24-Layers AlignerNet 9.55

Bảng 2: Kết quả nghiên cứu loại bỏ
cho căn chỉnh bộ giải mã hình ảnh trên
MS-COCO.Chúng tôi thực hiện các nghiên cứu loại bỏ để tìm ra tầm quan trọng của việc căn chỉnh bộ giải mã hình ảnh
và tinh chỉnh hướng dẫn. Bảng 2 chứng minh rằng
việc tinh chỉnh end-to-end trực tiếp hoặc sử dụng kiến trúc chỉ bộ giải mã
không tạo ra hình ảnh có ý nghĩa. Tuy nhiên, việc kết hợp AlignerNet và
giám sát CLIP dẫn đến kết quả gần với SD v1.5 gốc. Việc huấn luyện end-to-end cũng khả thi với AlignerNet,
nhưng nó tốn kém hơn do tính toán bổ sung của
U-Net. Điều này dẫn đến hiệu suất tệ hơn trong cùng số ngày GPU. Chúng tôi cũng so sánh kết quả tạo hình từ KOSMOS-G
trước tinh chỉnh hướng dẫn và SD v1.5 tiêu chuẩn với mô hình cuối cùng của chúng tôi. Như được minh họa trong Hình 6, không có tinh chỉnh hướng dẫn,
KOSMOS-G chỉ có thể tạo ra nội dung căn chỉnh ngữ nghĩa với
đầu vào thị giác-ngôn ngữ. Baseline SD cũng vẫn ở mức ngữ nghĩa và không tái tạo chung thực các thực thể trong hình ảnh được tạo ra.

4.4 Ứng dụng

Như được nhấn mạnh trong Phần 2.3, KOSMOS-G có thể thay thế liền mạch CLIP trong bất kỳ hệ thống tạo hình ảnh nào. Thuộc tính đáng chú ý này mở khóa vô số ứng dụng hoàn toàn mới mà chưa từng

--- TRANG 10 ---
có thể thực hiện được trước đây. Chúng tôi chứng minh việc tích hợp với ControlNet [ZA23] và các biến thể LoRA [HSW+22] trong Hình 7. KOSMOS-G hoạt động hoàn hảo với các kỹ thuật này. Dựa trên
không gian CLIP, chúng tôi tin rằng mô hình của chúng tôi sẽ thúc đẩy quá trình chuyển đổi từ việc tạo hình có điều kiện văn bản
sang việc tạo hình thị giác-ngôn ngữ, mở đường cho nhiều ứng dụng mới lạ.

5 Kết luận

Chúng tôi đề xuất KOSMOS-G, một mô hình có khả năng tạo hình ảnh theo chủ đề không cần tinh chỉnh có độ trung thực cao từ
đầu vào hình ảnh và văn bản đan xen nhiều ảnh. Cách tiếp cận của chúng tôi dựa trên chiến lược tiền huấn luyện "căn chỉnh trước khi hướng dẫn" độc đáo. KOSMOS-G thể hiện khả năng tạo hình ảnh theo chủ đề thực thể đơn cạnh tranh
và khả năng tạo hình ảnh từ văn bản, nó cũng là mô hình đầu tiên mở rộng việc tạo hình ảnh theo chủ đề không cần tinh chỉnh
sang các tình huống nhiều thực thể. Hơn nữa, KOSMOS-G cho phép thay thế liền mạch CLIP,
mở khóa các ứng dụng mới khác nhau kết hợp với các kỹ thuật U-Net khác như ControlNet
và LoRA. Nói chung, chúng tôi trình bày KOSMOS-G như một nỗ lực sơ bộ nhằm đạt được mục tiêu
"hình ảnh như một ngôn ngữ ngoại lai trong việc tạo hình ảnh."

Tuyên bố Đạo đức

KOSMOS-G hoàn toàn là một dự án nghiên cứu. Hiện tại, chúng tôi không có kế hoạch kết hợp KOSMOS-G vào
một sản phẩm hoặc mở rộng quyền truy cập cho công chúng. Chúng tôi cũng sẽ đưa các nguyên tắc AI của Microsoft vào thực hành khi
tiếp tục phát triển các mô hình.

Trong bài báo nghiên cứu của chúng tôi, chúng tôi tính đến các mối quan tâm đạo đức liên quan đến nghiên cứu văn bản-thành-hình ảnh. Để
giảm thiểu các vấn đề liên quan đến dữ liệu huấn luyện, chúng tôi đã triển khai một quy trình lọc nghiêm ngặt để
loại bỏ khỏi dữ liệu huấn luyện của chúng tôi nội dung không phù hợp, chẳng hạn như hình ảnh khiêu dâm và ngôn ngữ xúc phạm, để
tối thiểu hóa khả năng tạo ra nội dung không phù hợp.

Tài liệu tham khảo

[AAF+23]Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, và Dani Lischinski.
Break-a-scene: Extracting multiple concepts from a single image. arXiv preprint
arXiv:2305.16311, 2023.

[ADL+22]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana
Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman
Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Mar-
ianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh,
Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew
Zisserman, và Karen Simonyan. Flamingo: a visual language model for few-shot
learning. In Advances in Neural Information Processing Systems, 2022.

[AHR+22]Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Na-
man Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, và Luke
Zettlemoyer. CM3: A causal masked multimodal model of the Internet. ArXiv preprint,
abs/2201.07520, 2022.

[BHE23] Tim Brooks, Aleksander Holynski, và Alexei A Efros. Instructpix2pix: Learning to
follow image editing instructions. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 18392–18402, 2023.

[BPK+22]Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, và
Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022.

[CHL+23]Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, và
William W Cohen. Subject-driven text-to-image generation via apprenticeship learning.
ArXiv preprint, abs/2304.00186, 2023.

[CHSC22] Wenhu Chen, Hexiang Hu, Chitwan Saharia, và William W Cohen. Re-imagen:
Retrieval-augmented text-to-image generator. ArXiv preprint, abs/2209.14491, 2022.

--- TRANG 11 ---
[CSDS21] Soravit Changpinyo, Piyush Sharma, Nan Ding, và Radu Soricut. Conceptual 12m:
Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual,
June 19-25, 2021, pages 3558–3568. Computer Vision Foundation / IEEE, 2021.

[DBK+21]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua
Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net,
2021.

[DHP+23]Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang
Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal
comprehension and creation. ArXiv preprint, abs/2309.11499, 2023.

[GAA+22]Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik,
và Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image
generation using textual inversion. ArXiv preprint, abs/2208.01618, 2022.

[GPA+22]Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, và Yaniv
Taigman. Make-a-scene: Scene-based text-to-image generation with human priors.
ArXiv preprint, abs/2203.13131, 2022.

[HDW+23]Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma,
Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all
you need: Aligning perception with language models. ArXiv preprint, abs/2302.14045,
2023.

[HHZW23] Shaozhe Hao, Kai Han, Shihao Zhao, và Kwan-Yee K Wong. Vico: Detail-
preserving visual condition for personalized text-to-image generation. arXiv preprint
arXiv:2306.00971, 2023.

[HJA20] Jonathan Ho, Ajay Jain, và Pieter Abbeel. Denoising diffusion probabilistic models.
In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, và
Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020.

[HS22] Jonathan Ho và Tim Salimans. Classifier-free diffusion guidance. ArXiv preprint,
abs/2207.12598, 2022.

[HSD+22]Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming
Ma, và Furu Wei. Language models are general-purpose interfaces. ArXiv preprint,
abs/2206.06336, 2022.

[HSW+22]Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, và Weizhu Chen. Lora: Low-rank adaptation of large language
models. In The Tenth International Conference on Learning Representations, ICLR
2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.

[KFS23] Jing Yu Koh, Daniel Fried, và Ruslan Salakhutdinov. Generating images with multi-
modal language models. ArXiv preprint, abs/2305.17216, 2023.

[KR18] Taku Kudo và John Richardson. SentencePiece: A simple and language independent
subword tokenizer and detokenizer for neural text processing. In Proceedings of the
2018 Conference on Empirical Methods in Natural Language Processing: System
Demonstrations, pages 66–71, Brussels, Belgium, 2018. Association for Computational
Linguistics.

[KRA+20]Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-
Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom
Duerig, và Vittorio Ferrari. The open images dataset v4: Unified image classification,
object detection, and visual relationship detection at scale. IJCV, 2020.

--- TRANG 12 ---
[KW14] Diederik P. Kingma và Max Welling. Auto-encoding variational bayes. In Yoshua
Bengio và Yann LeCun, editors, 2nd International Conference on Learning Rep-
resentations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
Proceedings, 2014.

[KZZ+23]Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, và Jun-Yan
Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1931–1941,
2023.

[LE22] Timo Lüddecke và Alexander Ecker. Image segmentation using text and image
prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7086–7096, 2022.

[LLH23] Dongxu Li, Junnan Li, và Steven CH Hoi. Blip-diffusion: Pre-trained subject rep-
resentation for controllable text-to-image generation and editing. ArXiv preprint,
abs/2305.14720, 2023.

[LLSH23] Junnan Li, Dongxu Li, Silvio Savarese, và Steven Hoi. BLIP-2: Bootstrapping
language-image pre-training with frozen image encoders and large language models.
ArXiv preprint, abs/2301.12597, 2023.

[LMB+14]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, và C Lawrence Zitnick. Microsoft coco: Common objects
in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer,
2014.

[LOG+19]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. RoBERTa: A robustly
optimized bert pretraining approach. ArXiv preprint, abs/1907.11692, 2019.

[Luo22] Calvin Luo. Understanding diffusion models: A unified perspective. arXiv preprint
arXiv:2208.11970, 2022.

[LZB+22]Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, và Jun Zhu. Dpm-
solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
Advances in Neural Information Processing Systems, 35:5775–5787, 2022.

[MWH+22]Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong,
Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, và Furu Wei. TorchScale:
Transformers at scale. ArXiv preprint, abs/2211.13184, 2022.

[NDR+22]Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela
Mishkin, Bob McGrew, Ilya Sutskever, và Mark Chen. GLIDE: towards photoreal-
istic image generation and editing with text-guided diffusion models. In Kamalika
Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, và Sivan Sabato,
editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022,
Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research,
pages 16784–16804. PMLR, 2022.

[PJBM22] Ben Poole, Ajay Jain, Jonathan T Barron, và Ben Mildenhall. Dreamfusion: Text-to-3d
using 2d diffusion. ArXiv preprint, abs/2209.14988, 2022.

[QYX+23]Can Qin, Ning Yu, Chen Xing, Shu Zhang, Zeyuan Chen, Stefano Ermon, Yun Fu,
Caiming Xiong, và Ran Xu. Gluegen: Plug and play multi-modal encoders for
x-to-image generation. ArXiv preprint, abs/2303.10056, 2023.

[RBL+22]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, và Björn Ommer.
High-resolution image synthesis with latent diffusion models. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684–
10695, 2022.

--- TRANG 13 ---
[RDN+22]Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, và Mark Chen. Hierarchi-
cal text-conditional image generation with clip latents. ArXiv preprint, abs/2204.06125,
2022.

[RFB15] Olaf Ronneberger, Philipp Fischer, và Thomas Brox. U-net: Convolutional networks
for biomedical image segmentation. In International Conference on Medical image
computing and computer-assisted intervention, pages 234–241. Springer, 2015.

[RKH+21]Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger,
và Ilya Sutskever. Learning transferable visual models from natural language supervi-
sion. In Marina Meila và Tong Zhang, editors, Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume
139 of Proceedings of Machine Learning Research, pages 8748–8763. PMLR, 2021.

[RLJ+22]Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, và Kfir
Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven
generation. ArXiv preprint, abs/2208.12242, 2022.

[SBV+22]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight-
man, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman,
et al. Laion-5b: An open large-scale dataset for training next generation image-text
models. ArXiv preprint, abs/2210.08402, 2022.

[SCS+22]Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton,
Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gon-
tijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language
understanding. ArXiv preprint, abs/2205.11487, 2022.

[SDGS18] Piyush Sharma, Nan Ding, Sebastian Goodman, và Radu Soricut. Conceptual captions:
A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 2556–2565, Melbourne, Australia, 2018. Association
for Computational Linguistics.

[SHZ+23]James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen,
và Hongxia Jin. Continual diffusion: Continual customization of text-to-image diffu-
sion with c-lora. arXiv preprint arXiv:2304.06027, 2023.

[SME21] Jiaming Song, Chenlin Meng, và Stefano Ermon. Denoising diffusion implicit models.
In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event,
Austria, May 3-7, 2021. OpenReview.net, 2021.

[SVB+21]Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clay-
ton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, và Aran Komatsuzaki. Laion-
400m: Open dataset of clip-filtered 400 million image-text pairs. ArXiv preprint,
abs/2111.02114, 2021.

[SWMG15] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, và Surya Ganguli. Deep
unsupervised learning using nonequilibrium thermodynamics. In Francis R. Bach và
David M. Blei, editors, Proceedings of the 32nd International Conference on Machine
Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop
and Conference Proceedings, pages 2256–2265. JMLR.org, 2015.

[SYC+23]Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang,
Hongcheng Gao, Jingjing Liu, Tiejun Huang, và Xinlong Wang. Generative pre-
training in multimodality. ArXiv preprint, abs/2307.05222, 2023.

[T+23]MN Team et al. Introducing MPT-7B: A new standard for open-source, commercially
usable llms, 2023.

[TGCA23] Yoad Tewel, Rinon Gal, Gal Chechik, và Yuval Atzmon. Key-locked rank one editing
for text-to-image personalization. arXiv preprint arXiv:2305.01644, 2023.

--- TRANG 14 ---
[WMH+22]Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang
Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu,
Vishrav Chaudhary, Xia Song, và Furu Wei. Foundation transformers. ArXiv preprint,
abs/2210.06423, 2022.

[WZJ+23]Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, và Wangmeng Zuo.
Elite: Encoding visual concepts into textual embeddings for customized text-to-image
generation. arXiv preprint arXiv:2302.13848, 2023.

[XYF+23]Guangxuan Xiao, Tianwei Yin, William T Freeman, Frédo Durand, và Song Han.
Fastcomposer: Tuning-free multi-subject image generation with localized attention.
arXiv preprint arXiv:2305.10431, 2023.

[ZA23] Lvmin Zhang và Maneesh Agrawala. Adding conditional control to text-to-image
diffusion models, 2023.

--- TRANG 15 ---
Hình 8: Các hình ảnh được chọn từ DreamBench.

A Thiết lập Huấn luyện Chi tiết

Mô hình hóa Ngôn ngữ Đa phương thức Chúng tôi sử dụng kích thước batch 1.2 triệu token được phân tích
như sau: 0.5 triệu token được lấy từ tập ngữ liệu văn bản, 0.5 triệu token được lấy từ
các cặp hình ảnh-chú thích, và 0.2 triệu token từ các tập dữ liệu đan xen. MLLM được huấn luyện trong
300,000 bước, tương ứng với khoảng 360 tỷ token tổng cộng. Chúng tôi áp dụng tối ưu hóa AdamW
với β=(0.9,0.98). Hơn nữa, chúng tôi cấu hình weight decay ở 0.01 và tỷ lệ dropout
ở 0.1. Tỷ lệ học được đặt để tăng lên 2e-4 trong 375 bước khởi động ban đầu và giảm
tuyến tính về 0 cho phần còn lại của các bước huấn luyện. Để ổn định tối ưu hóa, chúng tôi khởi tạo bằng Magneto.
Chúng tôi sử dụng SentencePiece [KR18] để token hóa văn bản. Chúng tôi tiền xử lý dữ liệu trong định dạng "full-sentence" [LOG+19], nơi mỗi chuỗi đầu vào được điền với các câu hoàn chỉnh được lấy mẫu liên tiếp
từ một hoặc nhiều tài liệu.

Căn chỉnh Bộ giải mã Hình ảnh AlignerNet trải qua huấn luyện sử dụng kích thước batch 3,584 câu
trong 300,000 bước, với tỷ lệ học tối đa 1e-3. Điều này tương đương với khoảng 1 tỷ
câu tổng thể. Các cấu hình còn lại giữ nguyên như giai đoạn trước.

Tinh chỉnh Hướng dẫn MLLM và AlignerNet được huấn luyện cùng nhau với kích thước batch 1,024
hình ảnh, tổng cộng khoảng 200 triệu hình ảnh qua 200,000 bước. Tỷ lệ học đạt đỉnh ở
1e-3. Các thiết lập còn lại giống như giai đoạn trước.

B Hình ảnh và Lời nhắc cho Đánh giá DreamBench

Các hình ảnh đầu vào được chọn cho mỗi thực thể để đánh giá DreamBench được hiển thị trong Hình 8. Như
được mô tả trong Bảng 3, chúng tôi cũng sửa đổi nhẹ lời nhắc gốc để làm cho nó căn chỉnh tốt hơn với
dữ liệu huấn luyện. Đối với lời nhắc còn lại, chúng tôi chỉ đơn giản loại bỏ tiền tố "a". Chúng tôi quan sát thấy tiền tố sẽ
ảnh hưởng nhẹ đến hiệu suất của việc chỉnh sửa hình ảnh hoặc tạo hình tùy chỉnh. Điều này có thể được quy cho
tần suất cao của các chú thích bắt đầu với "a photo of" được tạo ra bởi BLIP-2 trong dữ liệu huấn luyện được xây dựng của chúng tôi. Cho rằng dữ liệu tinh chỉnh hướng dẫn thành phần không chứa quá nhiều dữ liệu chỉnh sửa khi một lời nhắc bắt đầu với tiền tố như "a", mô hình thường tránh thay đổi vẻ ngoài
của hình ảnh đầu vào. Điều này có thể được tinh chỉnh thêm bằng cách thay đổi mô hình dữ liệu hoặc bằng cách tinh chỉnh
căn chỉnh.

Lời nhắc Gốc Lời nhắc Đã sửa đổi
a red {} {}, red
a purple {} {}, purple
a shiny {} {}, shiny
a wet {} {}, wet
a cube shaped {} {}, cube shaped

Bảng 3: Sửa đổi Lời nhắc

C Chi tiết Bổ sung về Tinh chỉnh Hướng dẫn Chưng cất Điểm số

Trong giai đoạn tinh chỉnh hướng dẫn, chúng tôi vẫn sử dụng tổn thất khuếch tán cho việc huấn luyện mô hình. Tuy nhiên,
tổn thất này cũng có thể được coi là chưng cất điểm số, và nó thực sự tương đương với việc tối ưu hóa
tổn thất khuếch tán như tổn thất Score Distillation Sampling (SDS) [PJBM22].

Mục tiêu của chúng tôi là chưng cất hàm điểm số đã học từ U-Net Stable Diffusion vào KOSMOS-G.
Quá trình này cho phép KOSMOS-G mã hóa các đặc trưng hình ảnh thành các embedding mà mô hình Stable Diffusion
có thể hiểu để tạo hình ảnh theo chủ đề. Về bản chất, cách tiếp cận này giống như việc tiền huấn luyện
một mô hình textual inversion [GAA+22] tổng quát, với tất cả các điều kiện có thể học được bằng cách tìm kiếm mô hình.

Xem xét mô hình KOSMOS-G, được ký hiệu là ϕ, nhận một đầu vào x và tạo ra một đầu ra
C=ϕ(x). Cùng với đó, chúng tôi có Diffusion U-Net, được biểu diễn là θ. Trong quá trình của chúng tôi, chúng tôi
tối ưu hóa mô hình KOSMOS-G ϕ sử dụng tổn thất khuếch tán, trong khi giữ các tham số của
Diffusion U-Net θ đông lạnh. Tổn thất khuếch tán được biểu diễn như sau:

Ldiff(ϕ) = Ez0,ϵ∼N(0,1),t[w(t)‖ϵθ(zt;C,t)−ϵ‖²]   (8)

Xem xét gradient của Ldiff:
∇φLdiff(ϕ) = Ez0,ϵ∼N(0,1),t[
w(t)(ϵθ(zt;C,t)−ϵ)
|{z}
Nhiễu Dư
∂ϵφ(zt;C,t)/∂C
|{z}
Jacobian U-Net
∂C/∂ϕ
|{z}
Jacobian KOSMOS-G]   (9)

Theo cách tiếp cận trong Dreamfusion [PJBM22], chúng tôi có thể đơn giản hóa phương trình này bằng cách bỏ qua một số
thuật ngữ, dẫn đến tổn thất SDS cho KOSMOS-G:
∇φLSDS(ϕ) = Ez0,ϵ∼N(0,1),t[w(t)(ϵθ(zt;C,t)−ϵ)∂C/∂ϕ]   (10)

Như đã được thiết lập trong Dreamfusion [PJBM22], việc tối ưu hóa tổn thất SDS thực sự tương đương với
việc tối ưu hóa tổn thất khuếch tán khi U-Net θ bị đông lạnh. Từ góc độ chưng cất điểm số,
khi sử dụng tổn thất khuếch tán, KL divergence được định nghĩa bởi các điều kiện và hàm điểm số đã học trước
được tối thiểu hóa tương đương để chưng cất mật độ xác suất đã học trong tổng hợp hình ảnh có điều kiện [DHP+23, Luo22]:
minφ LDiff(ϕ) = Ez0,t,C[DKL(q(zt−1|zt,z0)‖pθ(zt−1|zt;C))]   (11)

--- TRANG 16 ---
Một bức ảnh của một con chótrên bãi biển, đeo kính mát

Một bức tranh sơn dầu chân dung của một con chó                    trong bộ đồ của                    , theo phong cách của                     ,chi tiết cao, kiệt tác

Một bức selfie,                        đeo kính mát                    , đứng trước                    , chi tiết cao, chất lượng tốt nhất

Một bức hoạt hình của theirondman đứng trước một chiếc xe hơi

trên bãi biển, theo phong cách của, kiệt tác

Hình 9: Các ví dụ bổ sung dưới tình huống đan xen hình ảnh và văn bản đa ảnh thách thức. Các
trường hợp này cho thấy rằng với KOSMOS-G, người dùng có thể nhắc Stable Diffusion [RBL+22] bằng cách tiếp cận tất cả
đầu vào hình ảnh như một "ngôn ngữ ngoại lai".

D Các Ví dụ Bổ sung

Trong Hình 9, chúng tôi trình bày các trường hợp với các tình huống đan xen hình ảnh (3 đến 4) và văn bản đa dạng và phức tạp hơn. Các ví dụ này chứng minh hiệu suất mạnh mẽ của KOSMOS-G trong việc xử lý những
trường hợp thách thức này.
