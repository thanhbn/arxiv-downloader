# 2307.03972.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2307.03972.pdf
# File size: 1262263 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Evaluating the Capability of Large-scale Language Models
on Chinese Grammatical Error Correction Task
Fanyi Qu Chenming Tang Yunfang Wu∗
National Key Laboratory for Multimedia Information Processing, Peking University
MOE Key Laboratory of Computational Linguistics, Peking University
School of Computer Science, Peking University
{fanyiqu@, tangchenming@stu, wuyf@}pku.edu.cn
Abstract
Large-scale language models (LLMs) have
shown remarkable capability in various of Nat-
ural Language Processing (NLP) tasks and at-
tracted lots of attention recently. However,
some studies indicated that large language mod-
els fail to achieve promising result beyond the
state-of-the-art models in English grammati-
cal error correction (GEC) tasks. In this re-
port, we aim to explore how LLMs perform
on Chinese GEC (CGEC) tasks and provide
guidance for future work. We conduct experi-
ments with 12 LLMs of different model scales
on 4 Chinese GEC datasets. Our experimen-
tal results indicate that the performances of
LLMs on automatic evaluation metrics (e.g.
F0.5score) falls short of the previous state-of-
the-art (SOTA) models because of the problem
of over-correction. Furthermore, we also dis-
cover notable variations in the performance of
LLMs when evaluated on different data dis-
tributions and the priority of general-purpose
models over their reasoning counterparts. Our
findings demonstrates that further investiga-
tion is required for the application of LLMs
on CGEC task.
1 Introduction
Building on InstructGPT (Ouyang et al., 2022),
ChatGPT has demonstrated its powerful ability to
understand complex instructions and generate rea-
sonable responses on various of NLP tasks. Follow-
ing the technical trajectory of ChatGPT, a signifi-
cant number of high-quality LLMs have emerged
in recent times in both academia and industry, such
as LLaMA (Touvron et al., 2023), ChatGLM (Du
et al., 2022) and PaLM (Anil et al., 2023). Previous
studies found that these LLMs have achieved great
performance on a wide range of NLP tasks, includ-
ing machine translation (Jiao et al., 2023), named
entity recognition (Li et al., 2023), text summariza-
tion (Yang et al., 2023), etc.
∗Corresponding authorCertain studies have taken comprehensive in-
vestigations into the performance of LLMs in the
domain of English GEC, yielding some interesting
findings (Fang et al., 2023; Wu et al., 2023). LLMs
are not able to outperform SOTA models in terms
of automatic evaluation metrics. This is primarily
because LLMs tend to make unnecessary modifi-
cations to make the input sentences more fluent,
which may result in over-correction problem, and
in some cases, even alter the original semantics of
the input sentences.
In this report, we aim to explore the performance
of LLMs in Chinese GEC (CGEC) task. We con-
ducted experiments on various LLMs to investigate
the influence of model size on the GEC results.
Additionally, we attempted different test dataset
from various data sources to explore the impact
of data distribution on the outcomes. Experimen-
tal results indicate that up-to-date LLMs still lag
behind SOTA CGEC models. We also find that
model performance is subject to data distribution
and general-purpose language models outperform
powerful reasoning models even at lower cost.
2 Experimental Setup
2.1 Dataset
We conduct experiments on four CGEC datasets to
provide a comprehension demonstration of LLMs’
capability. The detailed statistics of these datasets
are shown in Table 1.
2.1.1 GEC data from Chinese learners
We apply the test set of NLPCC-2018 (Zhao et al.,
2018) and the validation set of MuCGEC (Zhang
et al., 2022) for evaluation. These two datasets
collect the grammatical errors made by foreigners
during their process of learning Chinese.arXiv:2307.03972v2  [cs.CL]  15 Mar 2025

--- PAGE 2 ---
Dataset Data Source #Sents #Err. Sent. Avg. Length Avg. Edits Avg. Refs
NLPCC Chinese learner 2000 1983 29.7 2.1 1.1
MuCGEC Chinese learner 1136 1136 44.0 4.0 2.2
FCGEC Native speaker 2000 1101 55.8 1.0 1.3
NaCGEC Native speaker 500 482 56.2 1.2 1.2
Table 1: Statistics of datasets.
Figure 1: The prompt for LLMs. ChatGLM-6B and LLaMA-7B are prompted with specially designed prompts.
Other models use the default one.
2.1.2 GEC data from Chinese native speaker
examination
We apply the validation set of FCGEC (Xu et al.,
2022) and the validation set of NaCGEC (Ma et al.,
2022) for evaluation. These two datasets are col-
lected from Chinese native speakers’ language ex-
aminations.
2.2 Model
We conduct experiments on 12 LLMs with different
model scales:
•OpenAI’s ChatGPT Herd of Mod-
els1: we evaluate the performance of
gpt-3.5-turbo ,gpt-4o-mini and
o3-mini with OpenAI’s API. The first two
are general-purpose models while the third is
a powerful reasoning model.
•ByteDance’s Doubao2: we evaluate
doubao-1.5-pro with V olcengine’s
API3.
•DeepSeek Herd of Models (DeepSeek-AI,
2024, 2025): we evaluate deepseek-v3
anddeepseek-r1 with V olcengine’s API.
The former is a general-purpose model while
the latter is a powerful reasoning model.
1https://platform.openai.com/docs/api-reference
2https://www.doubao.com
3https://www.volcengine.com/docs/82379•ChatGLM Herd of Models (Du et al.,
2022; Zeng et al., 2024): we evaluate
chatglm-6b on 4 NVIDIA 3080Ti GPUs
andglm3-130b with V olcengine’s API.
•LLaMA Herd of Models (Touvron
et al., 2023): we evaluate llama-7B
on 4 NVIDIA 3080Ti GPUs and
llama-3.1-8b-instruct on one
NVIDIA A40 GPU.
•Qwen Herd of Models (Yang et al., 2024):
we evaluate qwen-2.5-7b-instruct on
one NVIDIA A40 GPU and qwq-32b on one
NVIDIA A100 GPU. The former is a general-
purpose language model and the latter is a
reasoning model.
2.3 Evaluation Metric
We evaluate models’ performance with Precision
(P), Recall (R) and F0.5from word level and char
level respectively.
We adopt the official implementation of Max-
Match ( M2) (Dahlmeier and Ng, 2012) scorer
to calculate word-level F0.5score and choose
PKUNLP as our word segment tool. We apply
ChERRANT4for char-level metric calculation.
4https://github.com/HillZhang1999/MuCGEC/tree/main/
scorers/ChERRANT

--- PAGE 3 ---
NLPCC MuCGEC FCGEC NaCGEC
P R F0.5 P R F0.5 P R F0.5 P R F0.5
GPT-3.5-turbo 18.07 26.80 19.33 22.79 30.03 23.94 3.38 12.88 3.96 5.38 12.30 6.06
GPT-4o-mini 22.55 30.93 23.85 21.78 23.93 22.18 9.50 16.11 10.35 9.51 16.97 10.43
o3-mini 21.29 31.93 22.81 20.56 25.19 21.35 8.24 19.44 9.31 8.59 21.66 9.77
Doubao-1.5-pro 30.19 39.96 31.74 27.46 29.58 27.86 32.18 45.08 34.14 35.93 51.53 38.25
DeepSeek-V3 31.05 37.12 32.10 25.66 25.62 25.65 22.63 29.80 23.78 23.77 32.31 25.10
DeepSeek-R1 20.27 35.39 22.16 15.88 25.23 17.15 18.78 36.06 20.77 19.31 37.30 21.37
ChatGLM-6B 16.76 7.54 13.47 22.93 7.45 16.20 4.36 4.96 4.47 8.41 5.07 7.43
GLM3-130B 16.56 31.27 18.28 16.16 23.63 17.25 11.59 26.84 13.07 12.07 28.42 13.64
LLaMA-7B 8.90 5.43 7.89 13.37 7.04 11.34 1.43 1.78 1.49 2.83 2.17 2.67
LLaMA-3.1-8B-Instruct 16.30 20.06 16.94 19.58 16.54 18.89 6.35 9.25 6.78 5.37 8.86 5.83
Qwen-2.5-7B-Instruct 19.51 27.43 20.70 20.84 21.53 20.98 11.98 19.56 12.98 11.75 19.86 12.80
QwQ-32B 20.82 31.58 22.34 18.08 21.67 18.70 17.33 32.15 19.09 17.55 33.21 19.38
SOTA - - - 72.27 21.69 49.62 59.21 41.57 54.58 52.83 40.43 49.78
Table 2: Experimental results (char-level). The first and second highest F0.5scores are in bold andunderlined
respectively.
2.4 Prompt
Considering the differences in performance of large
language models, we designed different prompts
for them. These prompts are roughly the same in
semantics, but there are some differences in details.
The prompts are shown in Figure 1.
3 Experimental results
Char-level and word-level experimental results are
shown in Table 2 and 3 respectively.
First, different data sources result in distinct
evaluation results. Most LLMs exhibit signifi-
cantly superior performance on Chinese learner
data (NLPCC and MuCGEC), as opposed to Chi-
nese native speaker examination data (FCGEC
and NaCGEC). According to our observations,
the grammatical errors made by Chinese learners
primarily involve the misuse of similar words or
phrases, rather than incorrect sentence structures.
In contrast, GEC data from Chinese native speaker
examination maintains a higher level of regularity
and is consisted of more complex structural errors.
It is noteworthy that there exist gaps between GEC
data from Chinese examination and Chinese native
speakers’ daily spoken habits. Specially, Doubao,
QwQ and the DeepSeek models demonstrate com-
petitive performance on Chinese native speaker ex-
amination data. This might be due to data leakage
or specialized GEC content in training data.
Second, newer models perform better than prior
ones and larger models are better than smaller
ones. For example, the newer LLaMA-3.1 sig-
nificantly outperforms the older LLaMA on all
datasets, while larger Doubao and DeepSeek out-
performs all other relatively smaller models.Third, general-purpose models are better than
reasoning models at CGEC. The up-to-date rea-
soning models o3-mini anddeepseek-r1 ,
while costing more money and time, fails to
outperform their relatively older counterparts
gpt-4o-mini anddeepseek-v3 . The newer
and larger qwq-32b , albeit performing better than
qwen-2.5-7b-instruct , is at the cost of 4
times larger model size and much longer inference
time (64 hours on a single NVIDIA A100 GPU
while qwen-2.5-7b-instruct only takes 1
hour on a single NVIDIA A40). This indicate that
the long deep thinking process of reasoning models
cannot contribute to their CGEC performance. In
practical applications, it is recommended to apply
general-purpose language models for CGEC task.
Fourth, there still exist great gaps between SOTA
models and LLMs on automated evaluation metrics.
Previous work (Jiao et al., 2023) has found the
problem of over-correction for LLMs, which has
also been noticed in our experiment.
What’s more, it is hard to explain why the char-
level evaluation metrics is significantly lower than
word-level evaluation metrics, which is not noticed
in previous work.
4 Conclusion
In this report, we explore the performance of var-
ious LLMs on CGEC task. Experimental results
indicate that there still remains gap between LLMs’
performance and current SOTA models. Further-
more, the performance of different LLMs’ perfor-
mance is subject to the distribution of test data and
general-purpose models are better at CGEC than
reasoning models at lower cost. Future work can
focus on addressing the over-correction problem of

--- PAGE 4 ---
NLPCC MuCGEC FCGEC NaCGEC
P R F0.5 P R F0.5 P R F0.5 P R F0.5
GPT-3.5-turbo 28.69 33.92 29.60 36.69 36.26 36.61 5.47 14.94 6.26 10.25 17.83 11.20
GPT-4o-mini 32.12 37.78 33.11 32.84 29.32 32.07 12.24 17.33 13.00 13.01 20.84 14.06
o3-mini 32.15 40.27 33.50 33.30 32.19 33.07 12.78 23.42 14.05 13.58 26.97 15.08
Doubao-1.5-pro 39.07 46.86 40.41 37.42 34.31 36.75 35.37 46.73 37.18 39.44 56.27 41.95
DeepSeek-V3 40.35 43.21 40.89 37.82 31.20 36.28 25.82 30.76 26.67 26.85 35.61 28.24
DeepSeek-R1 33.49 45.79 35.38 32.14 35.13 32.70 24.36 38.51 26.29 24.77 42.11 26.99
ChatGLM-6B 24.60 9.82 18.91 34.31 9.38 22.41 7.09 5.65 6.74 16.98 7.89 13.80
GLM3-130B 29.44 41.81 31.29 30.35 32.63 30.78 17.13 30.51 18.78 18.14 34.50 20.04
LLaMA-7B 17.63 7.98 14.20 24.54 10.02 19.03 3.42 3.00 3.33 8.44 4.75 7.30
LLaMA-3.1-8B-Instruct 26.10 26.82 26.24 29.74 20.76 27.38 9.85 11.63 10.17 10.53 13.86 11.06
Qwen-2.5-7B-Instruct 29.26 34.59 30.19 31.53 26.10 30.27 16.35 22.04 17.24 16.08 23.73 17.18
QwQ-32B 32.39 40.30 33.71 32.14 29.94 31.68 22.66 34.50 24.33 23.19 38.11 25.16
SOTA 57.10 28.90 47.80 - - - - - - - - -
Table 3: Experimental results (word-level). The first and second highest F0.5scores are in bold andunderlined
respectively.
LLMs and explore the untapped potential of LLMs
in the field of GEC tasks.
References
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, Eric Chu, Jonathan H. Clark, Laurent El
Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau-
rav Mishra, Erica Moreira, Mark Omernick, Kevin
Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,
Yuanzhong Xu, Yujing Zhang, Gustavo Hernández
Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham,
Jan A. Botha, James Bradbury, Siddhartha Brahma,
Kevin Brooks, Michele Catasta, Yong Cheng, Colin
Cherry, Christopher A. Choquette-Choo, Aakanksha
Chowdhery, Clément Crepy, Shachi Dave, Mostafa
Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,
Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxi-
aoyu Feng, Vlad Fienber, Markus Freitag, Xavier
Garcia, Sebastian Gehrmann, Lucas Gonzalez, and
et al. 2023. Palm 2 technical report. CoRR ,
abs/2305.10403.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better eval-
uation for grammatical error correction. In Human
Language Technologies: Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, Proceedings, June 3-8, 2012, Mon-
tréal, Canada , pages 568–572. The Association for
Computational Linguistics.
DeepSeek-AI. 2024. Deepseek-v3 technical report.
ArXiv , abs/2412.19437.
DeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-
soning capability in llms via reinforcement learning.
ArXiv , abs/2501.12948.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 320–335.
Tao Fang, Shu Yang, Kaixin Lan, Derek F. Wong, Jin-
peng Hu, Lidia S. Chao, and Yue Zhang. 2023. Is
chatgpt a highly fluent grammatical error correc-
tion system? A comprehensive evaluation. CoRR ,
abs/2304.01746.
Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing
Wang, and Zhaopeng Tu. 2023. Is chatgpt A
good translator? A preliminary study. CoRR ,
abs/2301.08745.
Xianzhi Li, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu,
and Sameena Shah. 2023. Are chatgpt and GPT-
4 general-purpose solvers for financial text analyt-
ics? an examination on several typical tasks. CoRR ,
abs/2305.05862.
Shirong Ma, Yinghui Li, Rongyi Sun, Qingyu Zhou,
Shulin Huang, Ding Zhang, Li Yangning, Ruiyang
Liu, Zhongli Li, Yunbo Cao, et al. 2022. Linguis-
tic rules-based corpus generation for native chinese
grammatical error correction. In Findings of the As-
sociation for Computational Linguistics: EMNLP
2022 .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,
John Schulman, Jacob Hilton, Fraser Kelton, Luke
Miller, Maddie Simens, Amanda Askell, Peter Welin-
der, Paul F. Christiano, Jan Leike, and Ryan Lowe.
2022. Training language models to follow instruc-
tions with human feedback. In NeurIPS .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. CoRR ,
abs/2302.13971.

--- PAGE 5 ---
Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang
Jiao, and Michael R. Lyu. 2023. Chatgpt or gram-
marly? evaluating chatgpt on grammatical error cor-
rection benchmark. CoRR , abs/2303.13648.
Lvxiaowei Xu, Jianwang Wu, Jiawei Peng, Jiayu Fu,
and Ming Cai. 2022. FCGEC: fine-grained corpus
for chinese grammatical error correction. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2022, Abu Dhabi, United Arab Emirates, De-
cember 7-11, 2022 , pages 1900–1918. Association
for Computational Linguistics.
Qwen An Yang, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan
Li, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-
ran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei
Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Jun-
yang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin
Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin
Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia,
Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang
Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu
Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan,
and Zekun Wang. 2024. Qwen2.5 technical report.
ArXiv , abs/2412.15115.
Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and
Wei Cheng. 2023. Exploring the limits of chatgpt
for query or aspect-based text summarization. CoRR ,
abs/2302.08081.
Team Glm Aohan Zeng, Bin Xu, Bowen Wang, Chen-
hui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Han-
lin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Ji-
adai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie
Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu,
Lucen Zhong, Ming yue Liu, Minlie Huang, Peng
Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shu-
dan Zhang, Shulin Cao, Shuxun Yang, Weng Lam
Tam, Wenyi Zhao, Xiao Liu, Xiaoyu Xia, Xiaohan
Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu,
Xinyue Yang, Xixuan Song, Xunkai Zhang, Yi An,
Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi
Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhenyi
Yang, Zhengxiao Du, Zhen-Ping Hou, and Zihan
Wang. 2024. Chatglm: A family of large language
models from glm-130b to glm-4 all tools. ArXiv ,
abs/2406.12793.
Yue Zhang, Zhenghua Li, Zuyi Bao, Jiacheng Li,
Bo Zhang, Chen Li, Fei Huang, and Min Zhang.
2022. Mucgec: a multi-reference multi-source evalu-
ation dataset for chinese grammatical error correction.
InProceedings of the 2022 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL 2022, Seattle, WA, United States, July 10-15,
2022 , pages 3118–3130. Association for Computa-
tional Linguistics.
Yuanyuan Zhao, Nan Jiang, Weiwei Sun, and Xiaojun
Wan. 2018. Overview of the NLPCC 2018 shared
task: Grammatical error correction. In Natural Lan-
guage Processing and Chinese Computing - 7th CCFInternational Conference, NLPCC 2018, Hohhot,
China, August 26-30, 2018, Proceedings, Part II ,
volume 11109 of Lecture Notes in Computer Science ,
pages 439–445. Springer.
