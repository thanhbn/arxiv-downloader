# 2111.11418.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2111.11418.pdf
# Kích thước tệp: 1062564 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
MetaFormer Thực Sự Là Những Gì Bạn Cần Cho Thị Giác
Weihao Yu1,2*Mi Luo1Pan Zhou1Chenyang Si1Yichen Zhou1,2
Xinchao Wang2Jiashi Feng1Shuicheng Yan1
1Sea AI Lab2Đại học Quốc gia Singapore
weihaoyu6@gmail.com {luomi,zhoupan,sicy,zhouyc,fengjs,yansc }@sea.com xinchao@nus.edu.sg
Mã nguồn: https://github.com/sail-sg/poolformer

Tóm tắt
Transformers đã thể hiện tiềm năng lớn trong các tác vụ thị giác máy tính. Một niềm tin phổ biến là mô-đun trộn token dựa trên attention đóng góp nhiều nhất vào năng lực của chúng. Tuy nhiên, các nghiên cứu gần đây cho thấy mô-đun dựa trên attention trong Transformers có thể được thay thế bằng MLPs không gian và các mô hình kết quả vẫn hoạt động khá tốt. Dựa trên quan sát này, chúng tôi đưa ra giả thuyết rằng kiến trúc tổng quát của Transformers, thay vì mô-đun trộn token cụ thể, quan trọng hơn đối với hiệu suất của mô hình. Để xác minh điều này, chúng tôi cố ý thay thế mô-đun attention trong Transformers bằng toán tử pooling không gian cực kỳ đơn giản để chỉ thực hiện trộn token cơ bản. Đáng ngạc nhiên, chúng tôi quan sát thấy rằng mô hình dẫn xuất, được gọi là PoolFormer, đạt được hiệu suất cạnh tranh trên nhiều tác vụ thị giác máy tính. Ví dụ, trên ImageNet-1K, PoolFormer đạt độ chính xác top-1 82.1%, vượt qua các baseline Vision Transformer/MLP-like được điều chỉnh tốt DeiT-B/ResMLP-B24 với 0.3%/1.1% độ chính xác với ít hơn 35%/52% tham số và ít hơn 50%/62% MACs. Hiệu quả của PoolFormer xác minh giả thuyết của chúng tôi và thúc đẩy chúng tôi khởi xướng khái niệm "MetaFormer", một kiến trúc tổng quát được trừu tượng hóa từ Transformers mà không chỉ định trộn token. Dựa trên các thí nghiệm mở rộng, chúng tôi lập luận rằng MetaFormer là yếu tố then chốt trong việc đạt được kết quả vượt trội cho các mô hình Transformer và MLP-like gần đây trên các tác vụ thị giác. Công trình này kêu gọi nhiều nghiên cứu tương lai tập trung vào cải thiện MetaFormer thay vì tập trung vào các mô-đun trộn token. Ngoài ra, PoolFormer được đề xuất của chúng tôi có thể phục vụ như một baseline khởi đầu cho thiết kế kiến trúc MetaFormer trong tương lai.

--- TRANG 2 ---
1. Giới thiệu
Transformers đã thu hút nhiều sự quan tâm và thành công trong lĩnh vực thị giác máy tính [3, 8, 44, 55]. Kể từ công trình tiên phong của Vision Transformer (ViT) [17] mà adapts Transformers thuần túy cho các tác vụ phân loại hình ảnh, nhiều mô hình tiếp theo được phát triển để cải tiến hơn nữa và đạt được hiệu suất đầy hứa hẹn trong các tác vụ thị giác máy tính khác nhau [36, 53, 63].

Encoder Transformer, như được thể hiện trong Hình 1(a), bao gồm hai thành phần. Một là mô-đun attention để trộn thông tin giữa các token và chúng tôi gọi nó là trộn token. Thành phần khác chứa các mô-đun còn lại, chẳng hạn như MLPs kênh và kết nối dư. Bằng cách coi mô-đun attention như một trộn token cụ thể, chúng tôi tiếp tục trừu tượng hóa toàn bộ Transformer thành một kiến trúc tổng quát MetaFormer nơi trộn token không được chỉ định, như được thể hiện trong Hình 1(a).

Thành công của Transformers từ lâu được quy cho trộn token dựa trên attention [56]. Dựa trên niềm tin phổ biến này, nhiều biến thể của các mô-đun attention [13, 22, 57, 68] đã được phát triển để cải thiện Vision Transformer. Tuy nhiên, một công trình rất gần đây [51] thay thế hoàn toàn mô-đun attention bằng MLPs không gian như trộn token, và thấy rằng mô hình MLP-like dẫn xuất có thể dễ dàng đạt được hiệu suất cạnh tranh trên các benchmark phân loại hình ảnh. Các công trình tiếp theo [26, 35, 52] tiếp tục cải thiện các mô hình MLP-like bằng cách đào tạo hiệu quả dữ liệu và thiết kế mô-đun MLP cụ thể, dần dần thu hẹp khoảng cách hiệu suất với ViT và thách thức sự thống trị của attention như trộn token.

Một số phương pháp gần đây [32, 39, 40, 45] khám phá các loại trộn token khác trong kiến trúc MetaFormer, và đã chứng minh hiệu suất khuyến khích. Ví dụ, [32] thay thế attention bằng Fourier Transform và vẫn đạt được khoảng 97% độ chính xác của Transformers vanilla. Tổng hợp tất cả những kết quả này, có vẻ như miễn là một mô hình áp dụng MetaFormer làm kiến trúc tổng quát, các kết quả đầy hứa hẹn có thể đạt được. Do đó chúng tôi đưa ra giả thuyết so với các trộn token cụ thể, MetaFormer quan trọng hơn để mô hình đạt được hiệu suất cạnh tranh.

Để xác minh giả thuyết này, chúng tôi áp dụng một toán tử phi tham số cực kỳ đơn giản, pooling, làm trộn token để chỉ thực hiện trộn token cơ bản. Đáng kinh ngạc, mô hình dẫn xuất này, được gọi là PoolFormer, đạt được hiệu suất cạnh tranh, và thậm chí vượt trội liên tục so với các mô hình Transformer và MLP-like được điều chỉnh tốt, bao gồm DeiT [53] và ResMLP [52], như được thể hiện trong Hình 1(b). Cụ thể hơn, PoolFormer-M36 đạt được độ chính xác top-1 82.1% trên benchmark phân loại ImageNet-1K, vượt qua các baseline vision Transformer/MLP-like được điều chỉnh tốt DeiT-B/ResMLP-B24 với 0.3%/1.1% độ chính xác với ít hơn 35%/52% tham số và ít hơn 50%/62% MACs. Những kết quả này chứng minh rằng MetaFormer, ngay cả với một trộn token naive, vẫn có thể mang lại hiệu suất đầy hứa hẹn. Do đó chúng tôi lập luận rằng MetaFormer là nhu cầu thực sự của chúng ta cho các mô hình thị giác mà quan trọng hơn để đạt được hiệu suất cạnh tranh thay vì các trộn token cụ thể. Lưu ý rằng điều này không có nghĩa là trộn token không quan trọng. MetaFormer vẫn có thành phần trừu tượng này. Nó có nghĩa là trộn token không bị giới hạn ở một loại cụ thể, ví dụ attention.

Những đóng góp của bài báo của chúng tôi có hai khía cạnh. Thứ nhất, chúng tôi trừu tượng hóa Transformers thành một kiến trúc tổng quát MetaFormer, và chứng minh thực nghiệm rằng thành công của các mô hình Transformer/MLP-like phần lớn được quy cho kiến trúc MetaFormer. Cụ thể, bằng cách chỉ sử dụng một toán tử phi tham số đơn giản, pooling, như một trộn token cực kỳ yếu cho MetaFormer, chúng tôi xây dựng một mô hình đơn giản có tên PoolFormer và thấy rằng nó vẫn có thể đạt được hiệu suất rất cạnh tranh. Chúng tôi hy vọng những phát hiện của chúng tôi truyền cảm hứng cho nhiều nghiên cứu tương lai tập trung vào cải thiện MetaFormer thay vì tập trung vào các mô-đun trộn token. Thứ hai, chúng tôi đánh giá PoolFormer được đề xuất trên nhiều tác vụ thị giác bao gồm phân loại hình ảnh [14], phát hiện đối tượng [34], phân đoạn instance [34], và phân đoạn ngữ nghĩa [67], và thấy rằng nó đạt được hiệu suất cạnh tranh so với các mô hình SOTA sử dụng thiết kế phức tạp của trộn token. PoolFormer có thể dễ dàng phục vụ như một baseline tốt khởi đầu cho thiết kế kiến trúc MetaFormer trong tương lai.

2. Công trình liên quan
Transformers đầu tiên được đề xuất bởi [56] cho các tác vụ dịch thuật và sau đó nhanh chóng trở nên phổ biến trong các tác vụ NLP khác nhau. Trong các tác vụ tiền huấn luyện ngôn ngữ, Transformers được huấn luyện trên corpus văn bản không nhãn quy mô lớn và đạt được hiệu suất đáng kinh ngạc [2, 15]. Được truyền cảm hứng bởi thành công của Transformers trong NLP, nhiều nhà nghiên cứu áp dụng cơ chế attention và Transformers cho các tác vụ thị giác [3, 8, 44, 55]. Đáng chú ý, Chen et al. giới thiệu iGPT [6] nơi Transformer được huấn luyện để dự đoán pixel tự hồi quy trên hình ảnh cho học tự giám sát. Dosovitskiy et al. đề xuất Vision Transformer (ViT) với hard patch embedding như đầu vào [17]. Họ cho thấy rằng trên các tác vụ phân loại hình ảnh có giám sát, một ViT được tiền huấn luyện trên một tập dữ liệu propriety lớn (tập dữ liệu JFT với 300 triệu hình ảnh) có thể đạt được hiệu suất xuất sắc. DeiT [53] và T2T-ViT [63] tiếp tục chứng minh rằng ViT được tiền huấn luyện chỉ trên ImageNet-1K (~1.3 triệu hình ảnh) từ đầu có thể đạt được hiệu suất đầy hứa hẹn. Nhiều công trình đã tập trung vào cải thiện phương pháp trộn token của Transformers bằng cách shifted windows [36], relative position encoding [61], refining attention map [68], hoặc kết hợp convolution [12, 21, 60], v.v. Ngoài các trộn token giống attention, [51, 52] đáng ngạc nhiên thấy rằng chỉ việc áp dụng MLPs như trộn token vẫn có thể đạt được hiệu suất cạnh tranh. Khám phá này thách thức sự thống trị của trộn token dựa trên attention và kích hoạt một cuộc thảo luận sôi nổi trong cộng đồng nghiên cứu

--- TRANG 3 ---
Thuật toán 1 Pooling cho PoolFormer, Mã giống PyTorch
import torch.nn as nn
class Pooling(nn.Module):
def __init__(self, pool_size=3):
super().__init__()
self.pool = nn.AvgPool2d(
pool_size, stride=1,
padding=pool_size//2,
count_include_pad=False,
)
def forward(self, x):
"""
[B, C, H, W] = x.shape
Phép trừ của chính đầu vào được thêm vào
vì block đã có
kết nối dư.
"""
return self.pool(x) - x

về trộn token nào tốt hơn [7, 26]. Tuy nhiên, mục tiêu của công trình này không phải là tham gia vào cuộc tranh luận này cũng không phải là thiết kế các trộn token phức tạp mới để đạt được state of the art mới. Thay vào đó, chúng tôi xem xét một câu hỏi cơ bản: Điều gì thực sự chịu trách nhiệm cho thành công của Transformers và các biến thể của chúng? Câu trả lời của chúng tôi là kiến trúc tổng quát tức là MetaFormer. Chúng tôi chỉ đơn giản sử dụng pooling như trộn token cơ bản để thăm dò sức mạnh của MetaFormer.

Cùng thời gian, một số công trình đóng góp vào việc trả lời cùng một câu hỏi. Dong et al. chứng minh rằng không có kết nối dư hoặc MLPs, đầu ra hội tụ doubly exponentially về một ma trận rank one [16]. Raghu et al. [43] so sánh sự khác biệt đặc trưng giữa ViT và CNNs, thấy rằng self-attention cho phép thu thập thông tin toàn cục sớm trong khi kết nối dư truyền tuyệt vời các đặc trưng từ các lớp thấp hơn lên các lớp cao hơn. Park et al. [42] cho thấy rằng multi-head self-attentions cải thiện độ chính xác và khái quát hóa bằng cách làm phẳng các cảnh quan mất mát. Thật không may, họ không trừu tượng hóa Transformers thành một kiến trúc tổng quát và nghiên cứu chúng từ khía cạnh framework tổng quát.

3. Phương pháp
3.1. MetaFormer
Chúng tôi trình bày khái niệm cốt lõi "MetaFormer" cho công trình này trước tiên. Như được thể hiện trong Hình 1, được trừu tượng hóa từ Transformers [56], MetaFormer là một kiến trúc tổng quát nơi trộn token không được chỉ định trong khi các thành phần khác được giữ giống như Transformers. Đầu vào I đầu tiên được xử lý bởi embedding đầu vào, chẳng hạn như patch embedding cho ViTs [17],
X= InputEmb(I), (1)
nơi X∈RN×C biểu thị các token embedding với độ dài chuỗi N và chiều embedding C.

Sau đó, các token embedding được đưa vào các block MetaFormer lặp lại, mỗi block bao gồm hai sub-block dư. Cụ thể, sub-block đầu tiên chủ yếu chứa một trộn token để giao tiếp thông tin giữa các token và sub-block này có thể được biểu diễn là
Y= TokenMixer(Norm(X)) +X, (2)
nơi Norm(·) biểu thị chuẩn hóa như Layer Normalization [1] hoặc Batch Normalization [28]; TokenMixer(·) có nghĩa là một mô-đun chủ yếu làm việc để trộn thông tin token. Nó được triển khai bởi các cơ chế attention khác nhau trong các mô hình vision Transformer gần đây [17,63,68] hoặc MLP không gian trong các mô hình MLP-like [51, 52]. Lưu ý rằng chức năng chính của trộn token là truyền thông tin token mặc dù một số trộn token cũng có thể trộn kênh, như attention.

Sub-block thứ hai chủ yếu bao gồm một MLP hai lớp với kích hoạt phi tuyến,
Z=σ(Norm(Y)W1)W2+Y, (3)
nơi W1∈RC×rC và W2∈RrC×C là các tham số có thể học với tỷ lệ mở rộng MLP r; σ(·) là một hàm kích hoạt phi tuyến, chẳng hạn như GELU [25] hoặc ReLU [41].

Khởi tạo của MetaFormer. MetaFormer mô tả một kiến trúc tổng quát mà với đó các mô hình khác nhau có thể được thu được ngay lập tức bằng cách chỉ định thiết kế cụ thể của các trộn token. Như được thể hiện trong Hình 1(a), nếu trộn token được chỉ định là attention hoặc MLP không gian, MetaFormer sau đó trở thành một mô hình Transformer hoặc MLP-like tương ứng.

3.2. PoolFormer
Từ khi giới thiệu Transformers [56], nhiều công trình gán tầm quan trọng lớn cho attention và tập trung vào thiết kế các thành phần trộn token dựa trên attention khác nhau. Ngược lại, những công trình này ít chú ý đến kiến trúc tổng quát, tức là MetaFormer.

Trong công trình này, chúng tôi lập luận rằng kiến trúc tổng quát MetaFormer này đóng góp nhiều nhất vào thành công của các mô hình Transformer và MLP-like gần đây. Để chứng minh điều này, chúng tôi cố ý sử dụng một toán tử đơn giản đáng xấu hổ, pooling, như trộn token. Toán tử này không có tham số có thể học và nó chỉ làm cho mỗi token tổng hợp đều các đặc trưng token gần đó.

Vì công trình này nhắm đến các tác vụ thị giác, chúng tôi giả định đầu vào ở định dạng dữ liệu channel-first, tức là T∈RC×H×W. Toán tử pooling có thể được biểu diễn là
T′:,i,j=1/(K×K)∑K p,q=1T:,i+p−(K+1)/2,i+q−(K+1)/2−T:,i,j,(4)
nơi K là kích thước pooling. Vì block MetaFormer đã có kết nối dư, phép trừ của chính đầu vào được thêm vào trong Phương trình (4). Mã giống PyTorch của pooling được thể hiện trong Thuật toán 1.

--- TRANG 4 ---
[Hình 2 với các stage và kiến trúc được mô tả]

Như đã biết, self-attention và MLP không gian có độ phức tạp tính toán bậc hai so với số lượng token để trộn. Thậm chí tệ hơn, MLPs không gian mang lại nhiều tham số hơn khi xử lý các chuỗi dài hơn. Kết quả là, self-attention và MLPs không gian thường chỉ có thể xử lý hàng trăm token. Ngược lại, pooling cần độ phức tạp tính toán tuyến tính với độ dài chuỗi mà không có bất kỳ tham số có thể học nào. Do đó, chúng tôi tận dụng pooling bằng cách áp dụng cấu trúc phân cấp tương tự như các CNN truyền thống [24,31,49] và các biến thể Transformer phân cấp gần đây [36,57]. Hình 2 cho thấy framework tổng thể của PoolFormer. Cụ thể, PoolFormer có 4 stage với H/4×W/4, H/8×W/8, H/16×W/16, và H/32×W/32 token tương ứng, nơi H và W đại diện cho chiều rộng và chiều cao của hình ảnh đầu vào.

Có hai nhóm kích thước embedding: 1) các mô hình kích thước nhỏ với chiều embedding 64, 128, 320, và 512 phản hồi cho bốn stage; 2) các mô hình kích thước trung bình với chiều embedding 96, 192, 384, và 768. Giả định có L block PoolFormer tổng cộng, stage 1, 2, 3, và 4 sẽ chứa L/6, L/6, L/2, và L/6 block PoolFormer tương ứng. Tỷ lệ mở rộng MLP được đặt là 4. Theo quy tắc scaling mô hình đơn giản ở trên, chúng tôi thu được 5 kích thước mô hình khác nhau của PoolFormer và các siêu tham số của chúng được thể hiện trong Bảng 1.

4. Thí nghiệm
4.1. Phân loại hình ảnh
Thiết lập. ImageNet-1K [14] là một trong những tập dữ liệu được sử dụng rộng rãi nhất trong thị giác máy tính. Nó chứa khoảng 1.3M hình ảnh huấn luyện và 50K hình ảnh validation, bao gồm 1K lớp phổ biến. Sơ đồ huấn luyện của chúng tôi chủ yếu theo [53] và [54]. Cụ thể, MixUp [65], CutMix [64], CutOut [66] và RandAugment [11] được sử dụng cho data augmentation. Các mô hình được huấn luyện trong 300 epoch sử dụng optimizer AdamW với weight decay 0.05 và peak learning rate lr = 1e−3·batch size/1024 (batch size 4096 và learning rate 4e−3 được sử dụng trong bài báo này). Số epoch warmup là 5 và lịch cosine được sử dụng để giảm learning rate. Label Smoothing [50] được đặt là 0.1. Dropout bị vô hiệu hóa nhưng stochastic depth [27] và LayerScale [54] được

--- TRANG 5 ---
[Bảng so sánh hiệu suất của các kiểu mô hình khác nhau trên phân loại ImageNet-1K]

sử dụng để giúp huấn luyện các mô hình sâu. Chúng tôi đã sửa đổi Layer Normalization [1] để tính mean và variance dọc theo chiều token và channel so với chỉ chiều channel trong Layer Normalization vanilla. Modified Layer Normalization (MLN) có thể được triển khai cho định dạng dữ liệu channel-first với GroupNorm API trong PyTorch bằng cách chỉ định số nhóm là 1. MLN được PoolFormer ưa thích như được thể hiện trong Phần 4.4. Xem phụ lục để biết thêm chi tiết về các siêu tham số. Triển khai của chúng tôi dựa trên codebase Timm [58] và các thí nghiệm được chạy trên TPUs.

Kết quả. Bảng 2 cho thấy hiệu suất của PoolFormers trên phân loại ImageNet. Kết quả định tính được hiển thị trong phụ lục. Đáng ngạc nhiên, bất chấp trộn token pooling đơn giản, PoolFormers vẫn có thể đạt được hiệu suất rất cạnh tranh so với CNNs và các mô hình MetaFormer-like khác. Ví dụ, PoolFormer-S24 đạt độ chính xác top-1 hơn 80 trong khi chỉ yêu cầu 21M tham số và 3.4G MACs. Tương đối, baseline ViT được thiết lập tốt DeiT-S [53], đạt được độ chính xác hơi tệ hơn 79.8 và yêu cầu nhiều hơn 35% MACs (4.6G). Để có được độ chính xác tương tự, mô hình MLP-like ResMLP-S24 [52] cần nhiều hơn 43% tham số (30M) cũng như nhiều hơn 76% tính toán (6.0G) trong khi chỉ đạt được 79.4 độ chính xác. Thậm chí so với các biến thể ViT và MLP-like được cải thiện hơn [35, 57], PoolFormer vẫn cho thấy hiệu suất tốt hơn. Cụ thể, pyramid Transformer PVT-Medium đạt được độ chính xác top-1 81.2 với 44M tham số và 6.7G MACs trong khi PoolFormer-S36 đạt 81.4 với ít hơn 30% tham số (31M) và ít hơn 25% MACs (5.0G) so với PVT-Medium.

Ngoài ra, so với RSB-ResNet ("ResNet Strikes Back") [59] nơi ResNet [24] được huấn luyện với quy trình huấn luyện được cải thiện cho cùng 300 epoch, PoolFormer vẫn hoạt động tốt hơn. Với ~22M tham số/3.7G MACs, RSB-ResNet-34 [59] đạt được 75.5 độ chính xác trong khi PoolFormer-S24 có thể đạt được 80.3. Vì khả năng mô hình hóa không gian cục bộ của lớp pooling kém hơn nhiều so với lớp neural convolution, hiệu suất cạnh tranh của PoolFormer chỉ có thể được quy cho kiến trúc tổng quát MetaFormer của nó.

Với toán tử pooling, mỗi token tổng hợp đều các đặc trưng từ các token gần đó. Do đó nó là một hoạt động trộn token cực kỳ cơ bản. Tuy nhiên, kết quả thí nghiệm cho thấy rằng ngay cả với trộn token đơn giản đáng xấu hổ này, MetaFormer vẫn đạt được hiệu suất rất cạnh tranh. Hình 3 rõ ràng cho thấy rằng PoolFormer vượt qua các mô hình khác với ít MACs và tham số hơn. Phát hiện này truyền đạt rằng kiến trúc tổng quát MetaFormer thực sự là những gì chúng ta cần khi thiết kế các mô hình thị giác. Bằng cách áp dụng MetaFormer, được đảm bảo rằng các mô hình dẫn xuất sẽ có tiềm năng đạt được hiệu suất hợp lý.

4.2. Phát hiện đối tượng và phân đoạn instance
Thiết lập. Chúng tôi đánh giá PoolFormer trên benchmark COCO thách thức [34] bao gồm 118K hình ảnh huấn luyện (train2017) và 5K hình ảnh validation (val2017). Các mô hình được huấn luyện trên tập huấn luyện và hiệu suất trên tập validation được báo cáo. PoolFormer được sử dụng làm backbone cho hai detector tiêu chuẩn, tức là RetinaNet [33] và Mask R-CNN [23]. Trọng số được tiền huấn luyện ImageNet được sử dụng để khởi tạo các backbone và Xavier [20] để khởi tạo các lớp được thêm vào. AdamW [29,37] được áp dụng cho huấn luyện với learning rate ban đầu 1×10−4 và batch size 16. Theo [23, 33], chúng tôi sử dụng lịch huấn luyện 1×, tức là huấn luyện các mô hình detection trong 12 epoch. Các hình ảnh huấn luyện được thay đổi kích thước thành cạnh ngắn hơn 800 pixel và cạnh dài hơn không quá 1,333 pixel. Để kiểm tra, cạnh ngắn hơn của hình ảnh cũng được thay đổi kích thước thành 800 pixel. Việc triển khai dựa trên codebase mmdetection [4] và các thí nghiệm được chạy trên 8 GPU NVIDIA A100.

Kết quả. Được trang bị RetinaNet cho phát hiện đối tượng, các mô hình dựa trên PoolFormer liên tục vượt trội so với các đối tác ResNet tương tự như được thể hiện trong Bảng 3. Ví dụ, PoolFormer-S12 đạt được 36.2 AP, vượt xa so với ResNet-18 (31.8 AP). Kết quả tương tự được quan sát cho những mô hình dựa trên Mask R-CNN trên phát hiện đối tượng và phân đoạn instance. Ví dụ, PoolFormer-S12 vượt xa ResNet-18 (bounding box AP 37.3 vs. 34.0, và mask AP 34.6 vs. 31.2). Nhìn chung, cho phát hiện đối tượng và phân đoạn instance COCO, PoolFormers đạt được hiệu suất cạnh tranh, liên tục vượt trội so với những đối tác của ResNet.

4.3. Phân đoạn ngữ nghĩa
Thiết lập. ADE20K [67], một benchmark scene parsing thách thức, được chọn để đánh giá các mô hình cho phân đoạn ngữ nghĩa. Tập dữ liệu bao gồm 20K và 2K hình ảnh trong tập huấn luyện và validation tương ứng, bao gồm 150 danh mục ngữ nghĩa tinh tế. PoolFormers được đánh giá như các backbone được trang bị Semantic FPN [30]. Các checkpoint được huấn luyện ImageNet-1K được sử dụng để khởi tạo các backbone trong khi Xavier [20] được sử dụng để khởi tạo các lớp mới được thêm vào khác. Các thực hành phổ biến [5, 30] huấn luyện các mô hình trong 80K iteration với batch size 16. Để tăng tốc huấn luyện, chúng tôi tăng gấp đôi batch size thành 32 và giảm số iteration xuống 40K. AdamW [29,37] được sử dụng với learning rate ban đầu 2×10−4 sẽ giảm theo lịch polynomial decay với power 0.9. Hình ảnh được thay đổi kích thước và cắt thành 512×512 cho huấn luyện và được thay đổi kích thước thành cạnh ngắn hơn 512 pixel cho kiểm tra. Triển khai của chúng tôi dựa trên codebase mmsegmentation [10] và các thí nghiệm được thực hiện trên 8 GPU NVIDIA A100.

Kết quả. Bảng 4 cho thấy hiệu suất phân đoạn ngữ nghĩa ADE20K của các backbone khác nhau sử dụng FPN [30]. Các mô hình dựa trên PoolFormer liên tục vượt trội so với các mô hình với backbone dựa trên CNN ResNet [24] và ResNeXt [62] cũng như dựa trên Transformer PVT. Ví dụ, PoolFormer-12 đạt được mIoU 37.1, tốt hơn 4.3 và 1.5 so với ResNet-18 và PVT-Tiny tương ứng.

Những kết quả này chứng minh rằng PoorFormer của chúng tôi phục vụ như backbone có thể đạt được hiệu suất cạnh tranh trên phân đoạn ngữ nghĩa mặc dù nó chỉ sử dụng pooling để giao tiếp thông tin cơ bản giữa các token. Điều này tiếp tục chỉ ra tiềm năng lớn của MetaFormer và hỗ trợ tuyên bố của chúng tôi rằng MetaFormer thực sự là những gì chúng ta cần.

4.4. Nghiên cứu ablation
Các thí nghiệm của nghiên cứu ablation được thực hiện trên ImageNet-1K [14]. Bảng 5 báo cáo nghiên cứu ablation của PoolFormer. Chúng tôi thảo luận về ablation dưới đây theo các khía cạnh sau.

--- TRANG 6 ---
[Tiếp tục với nội dung còn lại...]

--- TRANG 7 ---
[Bảng ablation và các kết quả thí nghiệm]

Trộn token. So với Transformers, thay đổi chính được thực hiện bởi PoolFormer là sử dụng pooling đơn giản như một trộn token. Chúng tôi đầu tiên thực hiện ablation cho toán tử này bằng cách thay thế pooling trực tiếp bằng identity mapping. Đáng ngạc nhiên, MetaFormer với identity mapping vẫn có thể đạt được độ chính xác top-1 74.3%, hỗ trợ tuyên bố rằng MetaFormer thực sự là những gì chúng ta cần để đảm bảo hiệu suất hợp lý.

Sau đó pooling được thay thế bằng ma trận ngẫu nhiên toàn cục WR∈RN×N cho mỗi block. Ma trận được khởi tạo với các giá trị ngẫu nhiên từ phân phối đều trên khoảng [0, 1), và sau đó Softmax được sử dụng để chuẩn hóa mỗi hàng. Sau khởi tạo ngẫu nhiên, các tham số ma trận được đóng băng và nó thực hiện trộn token bằng X′=WRX nơi X∈RN×C là các đặc trưng token đầu vào với độ dài token N và chiều channel C. Trộn token của ma trận ngẫu nhiên giới thiệu thêm 21M tham số đóng băng cho mô hình S12 vì độ dài token cực kỳ lớn ở stage đầu tiên. Ngay cả với phương pháp trộn token ngẫu nhiên như vậy, mô hình vẫn có thể đạt được hiệu suất hợp lý 75.8% độ chính xác, cao hơn 1.5% so với identity mapping. Nó cho thấy rằng MetaFormer vẫn có thể hoạt động tốt ngay cả với trộn token ngẫu nhiên, chưa nói đến với các trộn token được thiết kế tốt khác.

Hơn nữa, pooling được thay thế bằng Depthwise Convolution [9, 38] có các tham số có thể học cho mô hình hóa không gian. Không đáng ngạc nhiên, mô hình dẫn xuất vẫn đạt được hiệu suất rất cạnh tranh với độ chính xác top-1 78.1%, cao hơn 0.9% so với PoolFormer-S12 do khả năng mô hình hóa không gian cục bộ tốt hơn. Cho đến nay, chúng tôi đã chỉ định nhiều trộn token trong Metaformer, và tất cả các mô hình kết quả đều giữ kết quả đầy hứa hẹn, hỗ trợ tốt tuyên bố rằng MetaFormer là chìa khóa để đảm bảo tính cạnh tranh của các mô hình. Do tính đơn giản của pooling, nó chủ yếu được sử dụng như một công cụ để chứng minh MetaFormer.

Chúng tôi kiểm tra tác động của kích thước pooling trên PoolFormer. Chúng tôi quan sát hiệu suất tương tự khi kích thước pooling là 3, 5, và 7. Tuy nhiên, khi kích thước pooling tăng lên 9, có một sự giảm hiệu suất rõ ràng 0.5%. Do đó, chúng tôi áp dụng kích thước pooling mặc định là 3 cho PoolFormer.

Chuẩn hóa. Chúng tôi sửa đổi Layer Normalization [1] thành Modified Layer Normalization (MLN) tính mean và variance dọc theo chiều token và channel so với chỉ chiều channel trong Layer Normalization vanilla. Hình dạng của các tham số affine có thể học của MLN giữ giống như Layer Normalization, tức là RC. MLN có thể được triển khai với GroupNorm API trong PyTorch bằng cách đặt số nhóm là 1. Xem phụ lục để biết chi tiết. Chúng tôi thấy PoolFormer ưa thích MLN với 0.7% hoặc 0.8% cao hơn Layer Normalization hoặc Batch Normalization. Do đó, MLN được đặt làm mặc định cho PoolFormer. Khi loại bỏ chuẩn hóa, mô hình không thể được huấn luyện để hội tụ tốt, và hiệu suất của nó giảm mạnh chỉ còn 46.1%.

Kích hoạt. Chúng tôi thay đổi GELU [25] thành ReLU [41] hoặc SiLU [18]. Khi ReLU được áp dụng cho kích hoạt, một sự giảm hiệu suất rõ ràng 0.8% được quan sát. Đối với SiLU, hiệu suất của nó gần như giống với GELU. Do đó, chúng tôi vẫn áp dụng GELU làm kích hoạt mặc định.

Các thành phần khác. Ngoài trộn token và chuẩn hóa được thảo luận ở trên, kết nối dư [24] và channel MLP [46, 47] là hai thành phần quan trọng khác trong MetaFormer. Không có kết nối dư hoặc channel MLP, mô hình không thể hội tụ và chỉ đạt được độ chính xác 0.1%/5.7%, chứng minh tính không thể thiếu của những phần này.

Stage lai. Trong số các trộn token dựa trên pooling, attention, và MLP không gian, cái dựa trên pooling có thể xử lý các chuỗi đầu vào dài hơn nhiều trong khi attention và MLP không gian tốt trong việc nắm bắt thông tin toàn cục. Do đó, trực quan là xếp chồng MetaFormers với pooling ở các stage dưới để xử lý các chuỗi dài và sử dụng mixer dựa trên attention hoặc MLP không gian ở các stage trên, xem xét các chuỗi đã được rút ngắn đáng kể. Do đó, chúng tôi thay thế trộn token pooling bằng attention hoặc spatial FC trong một hoặc hai stage trên trong PoolFormer. Từ Bảng 5, các mô hình lai hoạt động khá tốt. Biến thể với pooling ở hai stage dưới và attention ở hai stage trên mang lại hiệu suất rất cạnh tranh. Nó đạt được độ chính xác 81.0% chỉ với 16.5M tham số và 2.5G MACs. Để so sánh, ResMLP-B24 cần 7.0× tham số (116M) và 9.2× MACs (23.0G) để đạt được cùng độ chính xác. Những kết quả này chỉ ra rằng việc kết hợp pooling với các trộn token khác cho MetaFormer có thể là một hướng đầy hứa hẹn để cải thiện hơn nữa hiệu suất.

5. Kết luận và công việc tương lai
Trong công trình này, chúng tôi đã trừu tượng hóa attention trong Transformers như một trộn token, và toàn bộ Transformer như một kiến trúc tổng quát được gọi là MetaFormer nơi trộn token không được chỉ định. Thay vì tập trung vào các trộn token cụ thể, chúng tôi chỉ ra rằng MetaFormer thực sự là những gì chúng ta cần để đảm bảo đạt được hiệu suất hợp lý. Để xác minh điều này, chúng tôi cố ý chỉ định trộn token như pooling cực kỳ đơn giản cho MetaFormer. Được thấy rằng mô hình PoolFormer dẫn xuất có thể đạt được hiệu suất cạnh tranh trên các tác vụ thị giác khác nhau, điều này hỗ trợ tốt rằng "MetaFormer thực sự là những gì bạn cần cho thị giác".

Trong tương lai, chúng tôi sẽ tiếp tục đánh giá PoolFormer dưới nhiều thiết lập học tập khác nhau, chẳng hạn như học tự giám sát và transfer learning. Hơn nữa, thật thú vị khi xem liệu PoolFormer vẫn hoạt động trên các tác vụ NLP để tiếp tục hỗ trợ tuyên bố "MetaFormer thực sự là những gì bạn cần" trong domain NLP. Chúng tôi hy vọng rằng công trình này có thể truyền cảm hứng cho nhiều nghiên cứu tương lai tập trung vào cải thiện kiến trúc cơ bản MetaFormer thay vì quan tâm quá nhiều đến các mô-đun trộn token.

Lời cảm ơn
Các tác giả muốn cảm ơn Quanhong Fu tại Sea AI Lab vì sự giúp đỡ cải thiện khía cạnh viết kỹ thuật của bài báo này. Weihao Yu muốn cảm ơn chương trình TPU Research Cloud (TRC) và Google Cloud research credits vì sự hỗ trợ một phần tài nguyên tính toán. Dự án này được hỗ trợ một phần bởi NUS Faculty Research Committee Grant (WBS: A-0009440-00-00). Shuicheng Yan và Xinchao Wang là các tác giả liên lạc.

--- TRANG 8-17 ---
[Tiếp tục với tài liệu tham khảo, bảng siêu tham số, kết quả định tính, so sánh chuẩn hóa, và mã PyTorch...]
