# 2303.11126.pdf
# ÄÃ£ chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/multimodal/2303.11126.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 7789688 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
TÄƒng cÆ°á»ng Äá»™ bá»n cho CÆ¡ cháº¿ ChÃº Ã½ Token trong Vision Transformers
Yong Guo, David Stutz, Bernt Schiele
Max Planck Institute for Informatics, Saarland Informatics Campus
guoyongcs@gmail.com, {david.stutz,schiele }@mpi-inf.mpg.de
TÃ³m táº¯t
Máº·c dÃ¹ thÃ nh cÃ´ng cá»§a vision transformers (ViTs), chÃºng
váº«n gáº·p pháº£i sá»± sá»¥t giáº£m Ä‘Ã¡ng ká»ƒ vá» Ä‘á»™ chÃ­nh xÃ¡c khi cÃ³ máº·t
cÃ¡c lá»—i thÆ°á»ng gáº·p, nhÆ° nhiá»…u hoáº·c má». ThÃº vá»‹ lÃ ,
chÃºng tÃ´i quan sÃ¡t tháº¥y cÆ¡ cháº¿ chÃº Ã½ cá»§a ViTs cÃ³ xu hÆ°á»›ng
dá»±a vÃ o má»™t sá»‘ Ã­t token quan trá»ng, má»™t hiá»‡n tÆ°á»£ng chÃºng tÃ´i gá»i lÃ 
táº­p trung quÃ¡ má»©c vÃ o token. Quan trá»ng hÆ¡n, nhá»¯ng token nÃ y khÃ´ng
bá»n vá»¯ng trÆ°á»›c cÃ¡c lá»—i, thÆ°á»ng dáº«n Ä‘áº¿n cÃ¡c máº«u chÃº Ã½ khÃ¡c biá»‡t ráº¥t lá»›n.
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i dá»± Ä‘á»‹nh giáº£m thiá»ƒu váº¥n Ä‘á» táº­p trung quÃ¡ má»©c nÃ y
vÃ  lÃ m cho cÆ¡ cháº¿ chÃº Ã½ á»•n Ä‘á»‹nh hÆ¡n thÃ´ng qua hai
ká»¹ thuáº­t tá»•ng quÃ¡t: Äáº§u tiÃªn, mÃ´-Ä‘un Token-aware Average Pooling (TAP)
cá»§a chÃºng tÃ´i khuyáº¿n khÃ­ch vÃ¹ng lÃ¢n cáº­n Ä‘á»‹a phÆ°Æ¡ng cá»§a
má»—i token tham gia vÃ o cÆ¡ cháº¿ chÃº Ã½. Cá»¥ thá»ƒ,
TAP há»c cÃ¡c sÆ¡ Ä‘á»“ pooling trung bÃ¬nh cho má»—i token
sao cho thÃ´ng tin cá»§a cÃ¡c token cÃ³ kháº£ nÄƒng quan trá»ng
trong vÃ¹ng lÃ¢n cáº­n cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh Ä‘áº¿n má»™t cÃ¡ch thÃ­ch á»©ng.
Thá»© hai, chÃºng tÃ´i buá»™c cÃ¡c token Ä‘áº§u ra tá»•ng há»£p thÃ´ng tin
tá»« má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c token Ä‘áº§u vÃ o thay vÃ¬ chá»‰ táº­p trung
vÃ o má»™t sá»‘ Ã­t báº±ng cÃ¡ch sá»­ dá»¥ng Attention Diversification Loss (ADL)
cá»§a chÃºng tÃ´i. ChÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y báº±ng cÃ¡ch pháº¡t Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine
cao giá»¯a cÃ¡c vector chÃº Ã½ cá»§a cÃ¡c token khÃ¡c nhau. Trong cÃ¡c thÃ­ nghiá»‡m,
chÃºng tÃ´i Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p cá»§a mÃ¬nh cho má»™t loáº¡t rá»™ng cÃ¡c kiáº¿n trÃºc transformer
vÃ  cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘á»™ bá»n vá»¯ng. VÃ­ dá»¥,
chÃºng tÃ´i cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng vá»›i lá»—i trÃªn ImageNet-C
2,4% trong khi cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c 0,4% dá»±a trÃªn
kiáº¿n trÃºc bá»n vá»¯ng tiÃªn tiáº¿n FAN. NgoÃ i ra, khi fine-tuning
cÃ¡c tÃ¡c vá»¥ phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a, chÃºng tÃ´i cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng trÃªn
CityScapes-C 2,4% vÃ  ACDC 3,0%. MÃ£ nguá»“n cá»§a chÃºng tÃ´i cÃ³ sáºµn
táº¡i https://github.com/guoyongcs/TAPADL.

1. Giá»›i thiá»‡u
Máº·c dÃ¹ thÃ nh cÃ´ng cá»§a vision transformers (ViTs), hiá»‡u suáº¥t
cá»§a chÃºng váº«n giáº£m Ä‘Ã¡ng ká»ƒ trÃªn cÃ¡c lá»—i hÃ¬nh áº£nh thÆ°á»ng gáº·p
nhÆ° ImageNet-C [24, 52, 19], vÃ­ dá»¥ Ä‘á»‘i khÃ¡ng [18, 16, 40, 56],
vÃ  cÃ¡c vÃ­ dá»¥ ngoÃ i phÃ¢n phá»‘i nhÆ° Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trong ImageNet-A/R/P [64, 24].
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i kiá»ƒm tra má»™t thÃ nh pháº§n chÃ­nh cá»§a ViTs,
tá»©c lÃ  cÆ¡ cháº¿ self-attention, Ä‘á»ƒ hiá»ƒu nhá»¯ng sá»± sá»¥t giáº£m hiá»‡u suáº¥t nÃ y.
ThÃº vá»‹ lÃ , chÃºng tÃ´i phÃ¡t hiá»‡n ra má»™t hiá»‡n tÆ°á»£ng chÃºng tÃ´i gá»i lÃ  táº­p trung quÃ¡ má»©c vÃ o token

Äáº§u vÃ o FAN-B-Hybrid Cá»§a chÃºng tÃ´i Sáº¡ch Nhiá»…u Gaussian
HÃ¬nh 1. TÃ­nh á»•n Ä‘á»‹nh chá»‘ng láº¡i lá»—i hÃ¬nh áº£nh vá» máº·t trá»±c quan hÃ³a chÃº Ã½
(trÃ¡i, má»™t ma tráº­n 196Ã—196) vÃ  Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine cá»§a
chÃº Ã½ giá»¯a cÃ¡c vÃ­ dá»¥ sáº¡ch vÃ  bá»‹ lá»—i (pháº£i). TrÃ¡i: ChÃºng tÃ´i
tÃ­nh trung bÃ¬nh cÃ¡c báº£n Ä‘á»“ chÃº Ã½ qua cÃ¡c head khÃ¡c nhau Ä‘á»ƒ trá»±c quan hÃ³a
vÃ  hiá»ƒn thá»‹ káº¿t quáº£ cá»§a lá»›p cuá»‘i cÃ¹ng. ChÃºng tÃ´i quan sÃ¡t tháº¥y ViTs Ä‘áº·t
quÃ¡ nhiá»u sá»± táº­p trung vÃ o ráº¥t Ã­t token, má»™t hiá»‡n tÆ°á»£ng chÃºng tÃ´i gá»i lÃ 
táº­p trung quÃ¡ má»©c vÃ o token. Quan trá»ng hÆ¡n, sá»± chÃº Ã½ cá»§a mÃ´ hÃ¬nh
FAN cÆ¡ sá»Ÿ [16] dá»… vá»¡ vá»›i cÃ¡c lá»—i hÃ¬nh áº£nh, vÃ­ dá»¥ vá»›i nhiá»…u
Gaussian. NgÆ°á»£c láº¡i, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i giáº£m thiá»ƒu sá»± táº­p trung quÃ¡ má»©c vÃ o token
vÃ  do Ä‘Ã³ cáº£i thiá»‡n tÃ­nh á»•n Ä‘á»‹nh cá»§a sá»± chÃº Ã½ chá»‘ng láº¡i lá»—i.
Pháº£i: TrÃªn ImageNet, chÃºng tÃ´i váº½ phÃ¢n phá»‘i Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine
qua táº¥t cáº£ cÃ¡c lá»›p (khÃ´ng tÃ­nh trung bÃ¬nh head) giá»¯a cÃ¡c vÃ­ dá»¥ sáº¡ch vÃ  bá»‹ lá»—i.
ChÃºng tÃ´i cho tháº¥y mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng cao hÆ¡n Ä‘Ã¡ng ká»ƒ
so vá»›i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ.

cusing, trong Ä‘Ã³ chá»‰ cÃ³ má»™t sá»‘ Ã­t token quan trá»ng Ä‘Æ°á»£c cÆ¡ cháº¿ chÃº Ã½
dá»±a vÃ o qua táº¥t cáº£ cÃ¡c head vÃ  lá»›p. ChÃºng tÃ´i Ä‘Æ°a ra giáº£ thuyáº¿t ráº±ng sá»± táº­p trung quÃ¡ má»©c nÃ y
Ä‘áº·c biá»‡t dá»… vá»¡ trÆ°á»›c cÃ¡c lá»—i trÃªn hÃ¬nh áº£nh Ä‘áº§u vÃ o vÃ  cáº£n trá»Ÿ nghiÃªm trá»ng
Ä‘á»™ bá»n vá»¯ng cá»§a cÆ¡ cháº¿ chÃº Ã½.

Báº¯t Ä‘áº§u tá»« kiáº¿n trÃºc bá»n vá»¯ng tiÃªn tiáº¿n
FAN [67], chÃºng tÃ´i Ä‘iá»u tra vÃ­ dá»¥ lá»›p chÃº Ã½ cuá»‘i cÃ¹ng
(xem chÃº Ã½ cá»§a cÃ¡c lá»›p khÃ¡c trong phá»¥ lá»¥c). Cá»¥ thá»ƒ,
HÃ¬nh 1 cho tháº¥y má»™t hÃ¬nh áº£nh Ä‘áº§u vÃ o sáº¡ch vÃ  má»™t hÃ¬nh áº£nh bá»‹ lá»—i
cÅ©ng nhÆ° cÃ¡c báº£n Ä‘á»“ chÃº Ã½ tÆ°Æ¡ng á»©ng. ÄÃ¢y lÃ  cÃ¡c ma tráº­n
NÃ—N, vá»›i N lÃ  sá»‘ token Ä‘áº§u vÃ o/Ä‘áº§u ra. á» Ä‘Ã¢y, hÃ ng thá»© i
chá»‰ ra nhá»¯ng token Ä‘áº§u vÃ o (cá»™t) mÃ  token Ä‘áº§u ra thá»© i "chÃº Ã½" Ä‘áº¿n
- mÃ u Ä‘á» Ä‘áº­m hÆ¡n cho biáº¿t Ä‘iá»ƒm chÃº Ã½ cao hÆ¡n. Sá»± táº­p trung quÃ¡ má»©c vÃ o token
cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a má»™t cÃ¡ch khÃ´ng chÃ­nh thá»©c báº±ng cÃ¡ch quan sÃ¡t cÃ¡c Ä‘Æ°á»ng tháº³ng Ä‘á»©ng
rÃµ rÃ ng trong báº£n Ä‘á»“ chÃº Ã½: Äáº§u tiÃªn, má»—i token Ä‘áº§u ra (hÃ ng)
chá»‰ táº­p trung vÃ o má»™t sá»‘ Ã­t token Ä‘áº§u vÃ o quan trá»ng, bá» qua háº§u háº¿t arXiv:2303.11126v3 [cs.CV] 6 Sep 2023

--- TRANG 2 ---
cá»§a thÃ´ng tin khÃ¡c. Thá»© hai, táº¥t cáº£ token Ä‘áº§u ra dÆ°á»ng nhÆ°
táº­p trung vÃ o cÃ¹ng cÃ¡c token Ä‘áº§u vÃ o, dáº«n Ä‘áº¿n sá»± Ä‘a dáº¡ng ráº¥t tháº¥p
giá»¯a cÃ¡c vector chÃº Ã½ trong cÃ¡c hÃ ng khÃ¡c nhau. ChÃºng tÃ´i
nháº¥n máº¡nh ráº±ng váº¥n Ä‘á» táº­p trung quÃ¡ má»©c cÃ³ máº·t trong toÃ n bá»™
táº­p dá»¯ liá»‡u ImageNet, vÃ  cÅ©ng qua cÃ¡c kiáº¿n trÃºc Ä‘a dáº¡ng (xem thÃªm
vÃ­ dá»¥ trong HÃ¬nh 3 vÃ  phá»¥ lá»¥c).

HÆ¡n ná»¯a, váº¥n Ä‘á» táº­p trung quÃ¡ má»©c nÃ y cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c quan sÃ¡t
trong cÃ¡c cÃ´ng trÃ¬nh hiá»‡n cÃ³. VÃ­ dá»¥, HÃ¬nh 5 cá»§a [10] quan sÃ¡t
ráº¥t Ã­t token quan trá»ng (mÃ u Ä‘á» Ä‘áº­m) trong cÃ¡c lá»›p 3 âˆ¼6;
HÃ¬nh 1 cá»§a [16] cÅ©ng bÃ¡o cÃ¡o ráº¥t Ã­t token quan trá»ng (mÃ u vÃ ng).
Quan trá»ng hÆ¡n, chÃºng tÃ´i tháº¥y ráº±ng nhá»¯ng token quan trá»ng nÃ y
cá»±c ká»³ dá»… vá»¡ khi cÃ³ máº·t cÃ¡c lá»—i thÆ°á»ng gáº·p. Cá»¥ thá»ƒ,
khi Ã¡p dá»¥ng nhiá»…u Gaussian trÃªn hÃ¬nh áº£nh Ä‘áº§u vÃ o, cÃ¡c token Ä‘Æ°á»£c
nháº­n dáº¡ng lÃ  quan trá»ng thay Ä‘á»•i hoÃ n toÃ n, xem HÃ¬nh 1 (trÃ¡i, cá»™t thá»© hai).
Vá» máº·t Ä‘á»‹nh lÆ°á»£ng, Ä‘iá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c náº¯m báº¯t báº±ng cÃ¡ch tÃ­nh
Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine giá»¯a cÃ¡c báº£n Ä‘á»“ chÃº Ã½ sáº¡ch vÃ  bá»‹ lá»—i.
KhÃ´ng ngáº¡c nhiÃªn, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ bá»Ÿi há»™p mÃ u xanh trong HÃ¬nh 1 (pháº£i),
Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine thá»±c sá»± cá»±c ká»³ tháº¥p, xÃ¡c nháº­n giáº£ thuyáº¿t
ban Ä‘áº§u cá»§a chÃºng tÃ´i. Äiá»u nÃ y thÃºc Ä‘áº©y chÃºng tÃ´i tÄƒng cÆ°á»ng Ä‘á»™ bá»n cá»§a
sá»± chÃº Ã½ báº±ng cÃ¡ch giáº£m thiá»ƒu váº¥n Ä‘á» táº­p trung quÃ¡ má»©c vÃ o token.

Äá»ƒ lÃ m Ä‘iá»u nÃ y, chÃºng tÃ´i khuyáº¿n khÃ­ch mÃ´-Ä‘un chÃº Ã½ táº­p trung
vÃ o cÃ¡c token Ä‘áº§u vÃ o Ä‘a dáº¡ng. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c báº±ng cÃ¡ch
thay Ä‘á»•i cÃ¡c máº«u trong cáº£ cá»™t vÃ  hÃ ng cá»§a báº£n Ä‘á»“ chÃº Ã½,
Ä‘iá»u nÃ y thÃºc Ä‘áº©y hai thÃ nh pháº§n chÃ­nh cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i.
Äáº§u tiÃªn, khi so sÃ¡nh cÃ¡c cá»™t chÃº Ã½ trong HÃ¬nh 1, ráº¥t Ã­t
token quan trá»ng, dáº«n Ä‘áº¿n sá»± chÃº Ã½ dá»… vá»¡. Äá»ƒ giáº£i quyáº¿t Ä‘iá»u nÃ y,
chÃºng tÃ´i khuyáº¿n khÃ­ch cÃ¡c token Ä‘áº§u ra khÃ´ng chá»‰ táº­p trung vÃ o
cÃ¡c token Ä‘áº§u vÃ o riÃªng láº» mÃ  cÃ²n tÃ­nh Ä‘áº¿n vÃ¹ng lÃ¢n cáº­n Ä‘á»‹a phÆ°Æ¡ng
xung quanh nhá»¯ng token nÃ y, Ä‘á»ƒ lÃ m cho nhiá»u token (cá»™t) hÆ¡n
Ä‘Ã³ng gÃ³p thÃ´ng tin cÃ³ Ã½ nghÄ©a. Má»™t cÃ¡ch trá»±c quan, má»™t token riÃªng láº»
báº£n thÃ¢n nÃ³ cÃ³ thá»ƒ khÃ´ng quan trá»ng nhÆ°ng cÃ³ thá»ƒ Ä‘Æ°á»£c tÄƒng cÆ°á»ng
báº±ng cÃ¡ch tá»•ng há»£p thÃ´ng tin tá»« cÃ¡c token cÃ³ kháº£ nÄƒng quan trá»ng
náº±m trong vÃ¹ng lÃ¢n cáº­n cá»§a nÃ³. ChÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y báº±ng cÃ¡ch
sá»­ dá»¥ng má»™t cÆ¡ cháº¿ pooling trung bÃ¬nh cÃ³ thá»ƒ há»c Ä‘Æ°á»£c Ã¡p dá»¥ng cho
má»—i token Ä‘áº§u vÃ o Ä‘á»ƒ tá»•ng há»£p thÃ´ng tin trÆ°á»›c khi tÃ­nh self-attention.
Thá»© hai, khi so sÃ¡nh cÃ¡c hÃ ng chÃº Ã½ trong HÃ¬nh 1, háº§u háº¿t
token Ä‘áº§u ra táº­p trung vÃ o cÃ¹ng cÃ¡c token Ä‘áº§u vÃ o. Má»™t khi
nhá»¯ng token nÃ y bá»‹ bÃ³p mÃ©o, toÃ n bá»™ sá»± chÃº Ã½ cÃ³ thá»ƒ tháº¥t báº¡i.
Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, chÃºng tÃ´i tÃ¬m cÃ¡ch Ä‘a dáº¡ng hÃ³a táº­p há»£p
cÃ¡c token Ä‘áº§u vÃ o (cá»™t) mÃ  cÃ¡c token Ä‘áº§u ra (hÃ ng) dá»±a vÃ o.
ChÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng má»™t loss pháº¡t má»™t cÃ¡ch
rÃµ rÃ ng Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine cao qua cÃ¡c hÃ ng. Trong HÃ¬nh 1,
sá»± káº¿t há»£p cá»§a nhá»¯ng ká»¹ thuáº­t nÃ y dáº«n Ä‘áº¿n sá»± chÃº Ã½ cÃ¢n báº±ng hÆ¡n
qua cÃ¡c cá»™t vÃ  sá»± chÃº Ã½ Ä‘a dáº¡ng hÆ¡n qua cÃ¡c hÃ ng. Quan trá»ng hÆ¡n,
nhá»¯ng báº£n Ä‘á»“ chÃº Ã½ nÃ y á»•n Ä‘á»‹nh hÆ¡n trÆ°á»›c cÃ¡c lá»—i hÃ¬nh áº£nh.
Má»™t láº§n ná»¯a, chÃºng tÃ´i xÃ¡c nháº­n Ä‘á»‹nh lÆ°á»£ng Ä‘iá»u nÃ y trÃªn ImageNet
báº±ng cÃ¡ch sá»­ dá»¥ng Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine cao hÆ¡n Ä‘Ã¡ng ká»ƒ.

NhÃ¬n chung, chÃºng tÃ´i Ä‘Æ°a ra ba Ä‘Ã³ng gÃ³p chÃ­nh: 1) chÃºng tÃ´i Ä‘á» xuáº¥t
má»™t mÃ´-Ä‘un Token-aware Average Pooling (TAP) khuyáº¿n khÃ­ch
vÃ¹ng lÃ¢n cáº­n Ä‘á»‹a phÆ°Æ¡ng cá»§a cÃ¡c token tham gia vÃ o cÆ¡ cháº¿ self-attention.
Cá»¥ thá»ƒ, chÃºng tÃ´i thá»±c hiá»‡n pooling trung bÃ¬nh Ä‘á»ƒ tá»•ng há»£p thÃ´ng tin vÃ  kiá»ƒm soÃ¡t nÃ³ báº±ng
cÃ¡ch Ä‘iá»u chá»‰nh vÃ¹ng pooling cho má»—i token. 2) ChÃºng tÃ´i tiáº¿p tá»¥c phÃ¡t triá»ƒn
má»™t Attention Diversification Loss (ADL) Ä‘á»ƒ cáº£i thiá»‡n
sá»± Ä‘a dáº¡ng cá»§a sá»± chÃº Ã½ nháº­n Ä‘Æ°á»£c bá»Ÿi cÃ¡c token khÃ¡c nhau, tá»©c lÃ 
cÃ¡c hÃ ng trong báº£n Ä‘á»“ chÃº Ã½. Äá»ƒ lÃ m Ä‘iá»u nÃ y, chÃºng tÃ´i tÃ­nh
Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng chÃº Ã½ giá»¯a cÃ¡c hÃ ng trong má»—i lá»›p vÃ  giáº£m thiá»ƒu
nÃ³ trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. 3) ChÃºng tÃ´i nháº¥n máº¡nh ráº±ng cáº£ TAP vÃ 
ADL Ä‘á»u cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng trÃªn Ä‘á»‰nh cÃ¡c kiáº¿n trÃºc transformer Ä‘a dáº¡ng.
Trong cÃ¡c thÃ­ nghiá»‡m, cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘á» xuáº¥t luÃ´n
cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng cá»§a mÃ´ hÃ¬nh trÃªn cÃ¡c benchmark ngoÃ i phÃ¢n phá»‘i
má»™t cÃ¡ch Ä‘Ã¡ng ká»ƒ trong khi duy trÃ¬ sá»± cáº£i thiá»‡n cáº¡nh tranh
vá» Ä‘á»™ chÃ­nh xÃ¡c sáº¡ch cÃ¹ng lÃºc. VÃ­ dá»¥, chÃºng tÃ´i cáº£i thiá»‡n
Ä‘á»™ bá»n vá»¯ng chá»‘ng láº¡i lá»—i vÃ  sá»± dá»‹ch chuyá»ƒn phÃ¢n phá»‘i trÃªn
ImageNet-A/C/P/R Ã­t nháº¥t 1,5%, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 2.
NgoÃ i ra, sá»± cáº£i thiá»‡n cÅ©ng tá»•ng quÃ¡t hÃ³a tá»‘t cho cÃ¡c tÃ¡c vá»¥
downstream khÃ¡c, vÃ­ dá»¥ phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a, vá»›i sá»± cáº£i thiá»‡n
2,4% trong Báº£ng 4.

2. CÃ´ng trÃ¬nh liÃªn quan
Hiá»‡u suáº¥t Ä‘Ã¡ng chÃº Ã½ cá»§a ViTs trÃªn cÃ¡c tÃ¡c vá»¥ há»c táº­p khÃ¡c nhau
chá»§ yáº¿u Ä‘Æ°á»£c gÃ¡n cho viá»‡c sá»­ dá»¥ng self-attention [26].
Tá»± nhiÃªn, cÆ¡ cháº¿ self-attention Ä‘Ã£ Ä‘Æ°á»£c má»Ÿ rá»™ng
trong nhiá»u khÃ­a cáº¡nh khÃ¡c nhau, giáº£i quyáº¿t cÃ¡c thiáº¿u sÃ³t nhÆ°
sá»± phá»¥ thuá»™c náº·ng vÃ o pre-training [42, 62, 35] hoáº·c Ä‘á»™ phá»©c táº¡p
tÃ­nh toÃ¡n cao [29, 28, 13, 1, 8, 3, 47] vÃ  thÃ­ch á»©ng
kiáº¿n trÃºc vá»›i cÃ¡c tÃ¡c vá»¥ thá»‹ giÃ¡c báº±ng cÃ¡ch xem xÃ©t cÃ¡c
transformer Ä‘a tá»· lá»‡ [25, 15, 46, 48, 49, 9, 57, 53, 25, 50, 6, 58].
TÆ°Æ¡ng tá»± nhÆ° chÃºng tÃ´i, má»™t sá»‘ cÃ´ng trÃ¬nh liÃªn quan [66, 10, 44, 34, 45]
cÅ©ng Ä‘iá»u tra vÃ  trá»±c quan hÃ³a self-attention Ä‘á»ƒ cáº£i thiá»‡n
kiáº¿n trÃºc transformer, vÃ­ dá»¥ Ä‘á»ƒ há»c cÃ¡c transformer sÃ¢u hÆ¡n [66]
hoáº·c cáº¯t tá»‰a cÃ¡c head chÃº Ã½ [45]. Tuy nhiÃªn, theo hiá»ƒu biáº¿t
tá»‘t nháº¥t cá»§a chÃºng tÃ´i, chÃºng tÃ´i lÃ  nhá»¯ng ngÆ°á»i Ä‘áº§u tiÃªn bÃ¡o cÃ¡o
váº¥n Ä‘á» táº­p trung quÃ¡ má»©c vÃ o token vÃ  liÃªn káº¿t nÃ³ vá»›i
Ä‘á»™ bá»n vá»¯ng kÃ©m cá»§a ViTs.

CÅ©ng cÃ³ ráº¥t nhiá»u quan tÃ¢m trong viá»‡c hiá»ƒu vÃ  cáº£i thiá»‡n
Ä‘á»™ bá»n vá»¯ng cá»§a ViTs [5, 2, 41, 37, 4, 21, 31]. VÃ­ dá»¥,
[32] phÃ¡t triá»ƒn má»™t robust vision transformer (RVT) báº±ng cÃ¡ch
káº¿t há»£p nhiá»u thÃ nh pháº§n khÃ¡c nhau Ä‘á»ƒ tÄƒng cÆ°á»ng Ä‘á»™ bá»n vá»¯ng,
bao gá»“m cáº£i thiá»‡n scaling chÃº Ã½. FAN [67] káº¿t há»£p
token attention vÃ  channel attention [1] vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c xem lÃ 
tiÃªn tiáº¿n. Cáº£ hai phÆ°Æ¡ng phÃ¡p Ä‘á»u dá»±a vÃ o cÃ¡c cÆ¡ cháº¿
self-attention Ä‘Ã£ Ä‘Æ°á»£c sá»­a Ä‘á»•i vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c chá»©ng minh lÃ 
gáº·p pháº£i táº­p trung quÃ¡ má»©c vÃ o token. Do Ä‘Ã³, cÃ¡c ká»¹ thuáº­t cá»§a chÃºng tÃ´i
cÃ³ thá»ƒ Ä‘Æ°á»£c chá»©ng minh lÃ  cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng trÃªn Ä‘á»‰nh RVT vÃ  FAN,
tÆ°Æ¡ng á»©ng.

PhÆ°Æ¡ng phÃ¡p TAP cá»§a chÃºng tÃ´i cÅ©ng chia sáº» sá»± tÆ°Æ¡ng Ä‘á»“ng vá»›i
cÃ¡c Ã½ tÆ°á»Ÿng gáº§n Ä‘Ã¢y vá» viá»‡c cá»‘ gáº¯ng Ä‘Æ°a tÃ­nh Ä‘á»‹a phÆ°Æ¡ng vÃ o
cÆ¡ cháº¿ self-attention [54, 53, 17, 38, 61, 54, 51]. VÃ­ dá»¥,
CvT [53] Ä‘Æ°a convolution vÃ o kiáº¿n trÃºc self-attention Ä‘á»ƒ
tÄƒng cÆ°á»ng thÃ´ng tin Ä‘á»‹a phÆ°Æ¡ng bÃªn trong cÃ¡c token. MViTv2 [27]
khai thÃ¡c average pooling Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng Ä‘á»‹a phÆ°Æ¡ng vÃ 
cáº£i thiá»‡n hiá»‡u suáº¥t sáº¡ch. Tuy nhiÃªn, táº¥t cáº£ nhá»¯ng chiáº¿n lÆ°á»£c nÃ y
sá»­ dá»¥ng cÃ¹ng má»™t tá»•ng há»£p qua táº¥t cáº£ cÃ¡c token, trong khi
phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i thÃ­ch á»©ng chá»n má»™t vÃ¹ng lÃ¢n cáº­n tá»•ng há»£p
cho má»—i token Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng.

--- TRANG 3 ---
NhÃ¡nh 1 â„!Ã—#Ã—$ 
Äá»“ng nháº¥t (ğ‘‘=0) AvgPool3x3 (ğ‘‘=1) AvgPool3x3 (ğ‘‘=2) Tá»•ng cÃ³ trá»ng sá»‘
â‹¯
Token Ä‘áº§u vÃ o Conv3x3 Conv3x3 Softmax
Bá»™ dá»± Ä‘oÃ¡n Ä‘á»™ giÃ£n â„!Ã—#Ã—% â„!Ã—#Ã—% Token-aware Average Pooling (TAP) ğ‘ğ‘ ğ´=Softmax ğ‘„ğ¾!ğ¶ Value Ã—=ğ‘ğ¶ğ‘ğ¶ Patch Embedding Linear Linear Conv3x3 Äáº§u ra ğ¿Ã— ViT Blocks
Feed-Forward Network Self-Attention
FC + Softmax Äáº§u vÃ o Token-aware Average Pooling (TAP) Kiáº¿n trÃºc tá»•ng thá»ƒ
NhÃ¡nh 2 NhÃ¡nh 3 NhÃ¡nh ğ¾ AvgPool3x3 (ğ‘‘=ğ¾-1)
ğ‘Šğ»ğ‘=ğ»ğ‘Š: tá»•ng sá»‘ token ğ¶: chiá»u cá»§a Ä‘áº·c trÆ°ng â„!Ã—#Ã—$ ğ‘‘: Ä‘á»™ giÃ£n ğ» & ğ‘Š: kÃ­ch thÆ°á»›c khÃ´ng gian cá»§a token

HÃ¬nh 2. MÃ´-Ä‘un Token-aware Average Pooling (TAP) Ä‘Æ°á»£c Ä‘á» xuáº¥t (trÃ¡i) vÃ  kiáº¿n trÃºc tá»•ng thá»ƒ (pháº£i). TrÃ¡i: Trong TAP, chÃºng tÃ´i giá»›i thiá»‡u K nhÃ¡nh Ä‘á»ƒ cho phÃ©p cÃ¡c token xem xÃ©t cÃ¡c vÃ¹ng pooling khÃ¡c nhau vÃ  tÃ­nh tá»•ng cÃ³ trá»ng sá»‘ trÃªn chÃºng. BÃªn cáº¡nh Ä‘Ã³, chÃºng tÃ´i xÃ¢y dá»±ng má»™t bá»™ dá»± Ä‘oÃ¡n Ä‘á»™ giÃ£n nháº¹ Ä‘á»ƒ há»c cÃ¡c trá»ng sá»‘ cho cÃ¡c nhÃ¡nh khÃ¡c nhau. Pháº£i: ChÃºng tÃ´i giá»›i thiá»‡u má»™t lá»›p TAP vÃ o má»—i block cÆ¡ báº£n Ä‘á»ƒ khuyáº¿n khÃ­ch nhiá»u token tham gia tÃ­ch cá»±c vÃ o cÆ¡ cháº¿ self-attention tiáº¿p theo.

3. Robust Token Self-Attention
Trong pháº§n sau, chÃºng tÃ´i táº­p trung vÃ o cÆ¡ cháº¿ chÃº Ã½
trong ViTs, nháº±m cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng tá»•ng thá»ƒ cá»§a chÃºng. Äá»ƒ báº¯t Ä‘áº§u,
chÃºng tÃ´i Ä‘áº§u tiÃªn mÃ´ táº£ váº¥n Ä‘á» táº­p trung quÃ¡ má»©c vÃ o token Ä‘Ã£ quan sÃ¡t
trong Ä‘Ã³ ViTs táº­p trung vÃ o ráº¥t Ã­t nhÆ°ng khÃ´ng á»•n Ä‘á»‹nh token
chi tiáº¿t trong Pháº§n 3.1. Sau Ä‘Ã³, chÃºng tÃ´i Ä‘á» xuáº¥t hai ká»¹ thuáº­t
tá»•ng quÃ¡t Ä‘á»ƒ giáº£m thiá»ƒu váº¥n Ä‘á» nÃ y: Äáº§u tiÃªn, trong Pháº§n 3.2,
chÃºng tÃ´i Ä‘á» xuáº¥t má»™t mÃ´-Ä‘un Token-aware Average Pooling (TAP)
khuyáº¿n khÃ­ch vÃ¹ng lÃ¢n cáº­n cá»§a cÃ¡c token tham gia vÃ o
cÆ¡ cháº¿ chÃº Ã½. Äiá»u nÃ y Ä‘áº¡t Ä‘Æ°á»£c báº±ng cÃ¡ch sá»­ dá»¥ng má»™t vÃ¹ng
pooling cÃ³ thá»ƒ há»c nhÆ° Ä‘Æ°á»£c minh há»a trong HÃ¬nh 2. Thá»© hai,
trong Pháº§n 3.3, chÃºng tÃ´i phÃ¡t triá»ƒn má»™t Attention Diversification Loss (ADL)
má»›i Ä‘á»ƒ cáº£i thiá»‡n sá»± Ä‘a dáº¡ng cá»§a cÃ¡c máº«u chÃº Ã½ qua cÃ¡c token.
Cáº£ hai phÆ°Æ¡ng phÃ¡p Ä‘á»u cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho háº§u háº¿t cÃ¡c kiáº¿n trÃºc
transformer vÃ  chÃºng tÃ´i sáº½ cho tháº¥y chÃºng cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ
Ä‘á»™ bá»n vá»¯ng vá»›i chi phÃ­ huáº¥n luyá»‡n khÃ´ng Ä‘Ã¡ng ká»ƒ.

3.1. Táº­p trung quÃ¡ má»©c vÃ o Token
NhÆ° Ä‘Æ°á»£c minh há»a trong HÃ¬nh 1, chÃºng tÃ´i cÃ³ thá»ƒ trá»±c quan hÃ³a
cÆ¡ cháº¿ self-attention trong má»—i lá»›p dÆ°á»›i dáº¡ng ma tráº­n chÃº Ã½ NÃ—N.
á» Ä‘Ã¢y, N lÃ  sá»‘ token Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra vÃ 
má»—i má»¥c (i, j) biá»ƒu thá»‹ sá»± chÃº Ã½ mÃ  token Ä‘áº§u ra thá»© i
(hÃ ng thá»© i) Ä‘áº·t lÃªn token Ä‘áº§u vÃ o thá»© j (cá»™t thá»© j)
â€“ mÃ u Ä‘á» Ä‘áº­m hÆ¡n biá»ƒu thá»‹ Ä‘iá»ƒm chÃº Ã½ cao hÆ¡n. Äá»ƒ xá»­ lÃ½
multi-head self-attention, chÃºng tÃ´i trá»±c quan hÃ³a ma tráº­n nÃ y báº±ng cÃ¡ch
tÃ­nh trung bÃ¬nh qua cÃ¡c head chÃº Ã½.

NhÆ° baseline, HÃ¬nh 1 nháº¥n máº¡nh kiáº¿n trÃºc FAN [67] gáº§n Ä‘Ã¢y,
hiá»ƒn thá»‹ báº£n Ä‘á»“ chÃº Ã½ cá»§a lá»›p cuá»‘i cÃ¹ng lÃ m vÃ­ dá»¥. ChÃºng tÃ´i quan sÃ¡t tháº¥y sá»± chÃº Ã½ thÆ°á»ng ráº¥t
thÆ°a thá»›t trong cÃ¡c cá»™t, cÃ³ nghÄ©a lÃ  háº§u háº¿t cÃ¡c token Ä‘áº§u vÃ o khÃ´ng Ä‘Æ°á»£c
chÃº Ã½ Ä‘áº¿n vÃ  ráº¥t Ã­t token Ä‘Æ°á»£c táº­p trung quÃ¡ má»©c. Quan trá»ng hÆ¡n,
nhá»¯ng token "quan trá»ng" nÃ y thÆ°á»ng tÆ°Æ¡ng tá»± qua cÃ¡c
token Ä‘áº§u ra (hÃ ng). ChÃºng tÃ´i gá»i hiá»‡n tÆ°á»£ng nÃ y lÃ  táº­p trung quÃ¡ má»©c vÃ o token.
Äiá»u nÃ y dáº«n Ä‘áº¿n váº¥n Ä‘á» khi Ä‘á»‘i máº·t vá»›i lá»—i nhÆ° nhiá»…u Gaussian:
táº­p há»£p cÃ¡c token quan trá»ng cÃ³ thá»ƒ thay Ä‘á»•i hoÃ n toÃ n (HÃ¬nh 1, cá»™t thá»© hai).
Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c hiá»ƒu lÃ  cÃ¡c token ban Ä‘áº§u khÃ´ng náº¯m báº¯t
thÃ´ng tin bá»n vá»¯ng. ChÃºng tÃ´i cÅ©ng cÃ³ thá»ƒ náº¯m báº¯t Ä‘á»‹nh lÆ°á»£ng
sá»± khÃ´ng á»•n Ä‘á»‹nh nÃ y báº±ng cÃ¡ch tÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine giá»¯a
cÃ¡c báº£n Ä‘á»“ chÃº Ã½ sáº¡ch vÃ  bá»‹ lá»—i qua táº¥t cáº£ cÃ¡c hÃ¬nh áº£nh ImageNet.
NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ bá»Ÿi há»™p mÃ u xanh trong HÃ¬nh 1 (pháº£i), mÃ´ hÃ¬nh
baseline Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine ráº¥t tháº¥p, cho tháº¥y
Ä‘á»™ bá»n vá»¯ng kÃ©m cá»§a self-attention tiÃªu chuáº©n.

ChÃºng tÃ´i tháº¥y ráº±ng hiá»‡n tÆ°á»£ng nÃ y tá»“n táº¡i qua cÃ¡c kiáº¿n trÃºc
Ä‘a dáº¡ng, vÃ­ dá»¥ DeiT [43] vÃ  RVT [32], vÃ  cÃ¡c tÃ¡c vá»¥ Ä‘a dáº¡ng,
bao gá»“m cáº£ phÃ¢n loáº¡i hÃ¬nh áº£nh vÃ  phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a (xem
vÃ­ dá»¥ trá»±c quan trong phá»¥ lá»¥c). HÆ¡n ná»¯a, chÃºng tÃ´i nháº¥n máº¡nh
ráº±ng quan sÃ¡t cá»§a chÃºng tÃ´i phÃ¹ há»£p vá»›i cÃ¡c cÃ´ng trÃ¬nh hiá»‡n cÃ³ [10, 36].
Cá»¥ thá»ƒ, nhá»¯ng cÃ´ng trÃ¬nh nÃ y quan sÃ¡t tháº¥y cÃ¡c lá»›p giá»¯a vÃ  sÃ¢u
(thÆ°á»ng cÃ³ váº¥n Ä‘á» táº­p trung quÃ¡ má»©c) cÃ³ xu hÆ°á»›ng náº¯m báº¯t
thÃ´ng tin toÃ n cá»¥c nhÆ°ng Ä‘áº·t sá»± táº­p trung vÃ o ráº¥t Ã­t token,
vÃ­ dá»¥ ráº¥t Ã­t token mÃ u Ä‘á» Ä‘áº­m trong HÃ¬nh 5 cá»§a [10] vÃ 
HÃ¬nh 3 (Giai Ä‘oáº¡n 4) trong [36]. Äiá»u nÃ y tiáº¿p tá»¥c chá»©ng minh
sá»± tá»“n táº¡i vÃ  táº§m quan trá»ng cá»§a váº¥n Ä‘á» táº­p trung quÃ¡ má»©c vÃ o token.
Äá»ƒ giáº£m thiá»ƒu váº¥n Ä‘á» nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t hai ká»¹ thuáº­t tá»•ng quÃ¡t
Ä‘á»ƒ tÄƒng cÆ°á»ng Ä‘á»™ bá»n cá»§a self-attention trong pháº§n cÃ²n láº¡i cá»§a pháº§n nÃ y.

--- TRANG 4 ---
3.2. Token-aware Average Pooling (TAP)
Trong pháº§n Ä‘áº§u tiÃªn cá»§a phÆ°Æ¡ng phÃ¡p, chÃºng tÃ´i tÃ¬m cÃ¡ch khuyáº¿n khÃ­ch
nhiá»u token Ä‘áº§u vÃ o tham gia vÃ o cÆ¡ cháº¿ self-attention,
tá»©c lÃ  Ä‘áº¡t Ä‘Æ°á»£c nhiá»u cá»™t cÃ³ Ä‘iá»ƒm cao hÆ¡n trong
báº£n Ä‘á»“ chÃº Ã½. Äá»ƒ lÃ m Ä‘iá»u nÃ y, chÃºng tÃ´i khuyáº¿n khÃ­ch má»—i token Ä‘áº§u vÃ o
tá»•ng há»£p má»™t cÃ¡ch rÃµ rÃ ng thÃ´ng tin há»¯u Ã­ch tá»« vÃ¹ng lÃ¢n cáº­n
Ä‘á»‹a phÆ°Æ¡ng cá»§a nÃ³, trong trÆ°á»ng há»£p token Ä‘Ã³ khÃ´ng chá»©a
thÃ´ng tin quan trá»ng. PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c chá»©ng minh bá»Ÿi
cÃ¡c cÃ´ng trÃ¬nh hiá»‡n cÃ³ [53, 38, 27] vÃ  quan sÃ¡t ráº±ng viá»‡c Ä‘Æ°a
báº¥t ká»³ tá»•ng há»£p Ä‘á»‹a phÆ°Æ¡ng nÃ o trÆ°á»›c self-attention luÃ´n
cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng, xem Báº£ng 1 (cá»™t cuá»‘i). Thá»±c táº¿,
nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y Ã¡p dá»¥ng má»™t kernel convolution cá»‘ Ä‘á»‹nh hoáº·c
vÃ¹ng pooling cho táº¥t cáº£ cÃ¡c token. Tuy nhiÃªn, cÃ¡c token thÆ°á»ng
khÃ¡c nhau vÃ  má»—i token nÃªn yÃªu cáº§u má»™t chiáº¿n lÆ°á»£c
tá»•ng há»£p Ä‘á»‹a phÆ°Æ¡ng cá»¥ thá»ƒ. Äiá»u nÃ y thÃºc Ä‘áº©y chÃºng tÃ´i thÃ­ch á»©ng
chá»n kÃ­ch thÆ°á»›c vÃ¹ng lÃ¢n cáº­n vÃ  chiáº¿n lÆ°á»£c tá»•ng há»£p phÃ¹ há»£p.

ÄÆ°á»£c truyá»n cáº£m há»©ng tá»« Ä‘iá»u nÃ y, chÃºng tÃ´i cho phÃ©p má»—i token
chá»n má»™t vÃ¹ng/chiáº¿n lÆ°á»£c thÃ­ch há»£p Ä‘á»ƒ thá»±c hiá»‡n tá»•ng há»£p Ä‘á»‹a phÆ°Æ¡ng.
Cá»¥ thá»ƒ, chÃºng tÃ´i phÃ¡t triá»ƒn má»™t Token-aware Average Pooling (TAP)
thá»±c hiá»‡n average pooling vÃ  thÃ­ch á»©ng Ä‘iá»u chá»‰nh
vÃ¹ng pooling cho má»—i token. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 2,
chÃºng tÃ´i khai thÃ¡c má»™t cáº¥u trÃºc Ä‘a nhÃ¡nh tÃ­nh tá»•ng cÃ³ trá»ng sá»‘
trÃªn nhiá»u nhÃ¡nh, má»—i nhÃ¡nh cÃ³ má»™t vÃ¹ng pooling cá»¥ thá»ƒ.
Thay vÃ¬ Ä‘Æ¡n giáº£n thay Ä‘á»•i kÃ­ch thÆ°á»›c kernel tÆ°Æ¡ng tá»± nhÆ° [59],
TAP thay Ä‘á»•i Ä‘á»™ giÃ£n Ä‘á»ƒ Ä‘iá»u chá»‰nh vÃ¹ng pooling.
Quan sÃ¡t chÃ­nh Ä‘áº±ng sau Ä‘iá»u nÃ y lÃ  average pooling vá»›i
kernel lá»›n, khÃ´ng cÃ³ Ä‘á»™ giÃ£n, dáº«n Ä‘áº¿n sá»± chá»“ng chÃ©o cá»±c ká»³
lá»›n giá»¯a cÃ¡c vÃ¹ng pooling lÃ¢n cáº­n vÃ  do Ä‘Ã³ dÆ° thá»«a nghiÃªm trá»ng
trong cÃ¡c token Ä‘áº§u ra. VÃ­ dá»¥, Ä‘iá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c tháº¥y trong
Báº£ng 1 nÆ¡i AvgPool5x5 gÃ¢y ra sá»± giáº£m lá»›n vá» Ä‘á»™ chÃ­nh xÃ¡c
sáº¡ch khoáº£ng 1,2%. TÆ°Æ¡ng tá»± nhÆ° [54, 53, 17, 38], chÃºng tÃ´i cÅ©ng
Ä‘iá»u tra viá»‡c sá»­ dá»¥ng cÃ¡c convolution cÃ³ thá»ƒ há»c, nhÆ°ng chá»‰ quan sÃ¡t
Ä‘Æ°á»£c cáº£i thiá»‡n nhá» cÃ¹ng vá»›i sá»± gia tÄƒng Ä‘Ã¡ng ká»ƒ chi phÃ­ tÃ­nh toÃ¡n.

Dá»±a trÃªn nhá»¯ng quan sÃ¡t nÃ y, chÃºng tÃ´i xÃ¢y dá»±ng TAP dá»±a trÃªn
average pooling vá»›i cÃ¡c Ä‘á»™ giÃ£n Ä‘a dáº¡ng: KhÃ´ng máº¥t tÃ­nh tá»•ng quÃ¡t,
vá»›i K nhÃ¡nh, chÃºng tÃ´i xem xÃ©t Ä‘á»™ giÃ£n trong pháº¡m vi dâˆˆ[0, Kâˆ’1].
á» Ä‘Ã¢y, d= 0 ngá»¥ Ã½ identity mapping khÃ´ng cÃ³ báº¥t ká»³ tÃ­nh toÃ¡n nÃ o,
tá»©c lÃ  khÃ´ng cÃ³ tá»•ng há»£p Ä‘á»‹a phÆ°Æ¡ng. Äá»™ giÃ£n tá»‘i Ä‘a Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh
bá»Ÿi K lÃ  má»™t siÃªu tham sá»‘ vÃ  chÃºng tÃ´i tháº¥y ráº±ng hiá»‡u suáº¥t vÃ 
cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng bÃ£o hÃ²a á»Ÿ khoáº£ng K= 5 (xem HÃ¬nh 7 trÃªn).
Trong pháº¡m vi Ä‘á»™ giÃ£n Ä‘Æ°á»£c phÃ©p, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i bao gá»“m
má»™t bá»™ dá»± Ä‘oÃ¡n Ä‘á»™ giÃ£n nháº¹ Ä‘á»ƒ dá»± Ä‘oÃ¡n Ä‘á»™ giÃ£n nÃ o
(tá»©c lÃ  nhÃ¡nh nÃ o trong HÃ¬nh 2) Ä‘á»ƒ sá»­ dá»¥ng. LÆ°u Ã½ ráº±ng
Ä‘iá»u nÃ y cÅ©ng cÃ³ thá»ƒ lÃ  má»™t káº¿t há»£p cÃ³ trá»ng sá»‘ cá»§a nhiá»u d.
ChÃºng tÃ´i nháº¥n máº¡nh ráº±ng bá»™ dá»± Ä‘oÃ¡n nÃ y ráº¥t hiá»‡u quáº£ vÃ¬ nÃ³
giáº£m chiá»u Ä‘áº·c trÆ°ng (tá»« C Ä‘áº¿n K trong HÃ¬nh 2) sao cho
nÃ³ thÃªm chi phÃ­ tÃ­nh toÃ¡n vÃ  tham sá»‘ mÃ´ hÃ¬nh tá»‘i thiá»ƒu.
CÃ¹ng má»™t phÆ°Æ¡ng phÃ¡p cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho average-pooling
khÃ´ng giÃ£n nÆ¡i d kiá»ƒm soÃ¡t kÃ­ch thÆ°á»›c kernel, Ä‘Æ°á»£c gá»i lÃ 
TAP (multi-kernel). Tá»« Báº£ng 1, TAP cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i
Ä‘Ã¡ng ká»ƒ so vá»›i biáº¿n thá»ƒ nÃ y, cho tháº¥y hiá»‡u quáº£ cá»§a viá»‡c
sá»­ dá»¥ng Ä‘á»™ giÃ£n cho pooling.

MÃ´ hÃ¬nh #FLOPs (G) #Params (M) ImageNet ImageNet-C â†“
Baseline (FAN-B-Hybrid) 11.7 50.4 83.9 46.1
+ AvgPool3x3 11.7 50.4 83.6 45.6 (-0.5)
+ AvgPool5x5 11.7 50.4 82.7 45.5 (-0.6)
+ Conv3x3 17.3 79.4 84.0 45.9 (-0.2)
+ Conv5x5 27.4 130.7 84.4 45.8 (-0.3)
+ TAP (multi-kernel) 11.8 50.7 84.1 45.5 (-0.6)
+ TAP (Cá»§a chÃºng tÃ´i) 11.8 50.7 84.3 44.9 (-1.2)

Báº£ng 1. So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p tá»•ng há»£p Ä‘á»‹a phÆ°Æ¡ng dá»±a trÃªn
FAN-B-Hybrid. ChÃºng tÃ´i cho tháº¥y viá»‡c thá»±c hiá»‡n average pooling cho táº¥t cáº£
cÃ¡c token cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng nhÆ°ng cáº£n trá»Ÿ Ä‘á»™ chÃ­nh xÃ¡c sáº¡ch.
Viá»‡c Ä‘Æ°a convolution vÃ o má»—i block tÄƒng Ä‘Ã¡ng ká»ƒ Ä‘á»™ phá»©c táº¡p mÃ´ hÃ¬nh.
NgoÃ i ra, chÃºng tÃ´i cÅ©ng so sÃ¡nh má»™t biáº¿n thá»ƒ cá»§a TAP,
tá»©c lÃ  TAP (multi-kernel), xem xÃ©t nhiá»u kÃ­ch thÆ°á»›c kernel
cho pooling vÃ  há»c trá»ng sá»‘ cho má»—i nhÃ¡nh. TAP cá»§a chÃºng tÃ´i
vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i biáº¿n thá»ƒ nÃ y, cho tháº¥y hiá»‡u quáº£
cá»§a viá»‡c sá»­ dá»¥ng Ä‘á»™ giÃ£n. HÆ¡n ná»¯a, TAP mang láº¡i sá»± cÃ¢n báº±ng
tá»‘t nháº¥t giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ bá»n vá»¯ng cÃ¹ng vá»›i chi phÃ­
tÃ­nh toÃ¡n khÃ´ng Ä‘Ã¡ng ká»ƒ.

3.3. Attention Diversification Loss (ADL)
Trong pháº§n thá»© hai cá»§a phÆ°Æ¡ng phÃ¡p, chÃºng tÃ´i tÃ¬m cÃ¡ch cáº£i thiá»‡n
sá»± Ä‘a dáº¡ng cá»§a sá»± chÃº Ã½ qua cÃ¡c token Ä‘áº§u ra, tá»©c lÃ  khuyáº¿n khÃ­ch
cÃ¡c hÃ ng khÃ¡c nhau trong HÃ¬nh 1 chÃº Ã½ Ä‘áº¿n cÃ¡c token Ä‘áº§u vÃ o khÃ¡c nhau.
Dá»±a trÃªn má»¥c tiÃªu nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t Attention Diversification Loss (ADL)
giáº£m má»™t cÃ¡ch rÃµ rÃ ng Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine cá»§a sá»± chÃº Ã½
giá»¯a cÃ¡c token Ä‘áº§u ra khÃ¡c nhau (hÃ ng). Tuy nhiÃªn, Ä‘á»ƒ phÆ°Æ¡ng phÃ¡p nÃ y
hoáº¡t Ä‘á»™ng, cÃ³ má»™t sá»‘ thÃ¡ch thá»©c cáº§n vÆ°á»£t qua. Äáº§u tiÃªn,
viá»‡c tÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine giá»¯a cÃ¡c sá»± chÃº Ã½ khÃ³ khÄƒn vá» máº·t sá»‘ há»c.
VÃ­ dá»¥, náº¿u hai hÃ ng (tá»©c lÃ  token Ä‘áº§u ra) cÃ³ cÃ¡c máº«u chÃº Ã½
ráº¥t riÃªng biá»‡t, chÃºng ta mong Ä‘á»£i Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine tháº¥p gáº§n 0.
Tuy nhiÃªn, ngay cáº£ Ä‘á»‘i vá»›i cÃ¡c token khÃ´ng Ä‘Æ°á»£c chÃº Ã½ Ä‘áº¿n,
Ä‘iá»ƒm chÃº Ã½ sáº½ khÃ´ng báº±ng khÃ´ng. Äá»‘i vá»›i N lá»›n, viá»‡c tÃ­nh
tÃ­ch vÃ´ hÆ°á»›ng vÃ  cá»™ng nhá»¯ng giÃ¡ trá»‹ nÃ y cÃ³ xu hÆ°á»›ng dáº«n Ä‘áº¿n
Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine cao hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i khÃ´ng. Äá»ƒ giáº£m thiá»ƒu
váº¥n Ä‘á» nÃ y, chÃºng tÃ´i khai thÃ¡c má»™t thá»§ thuáº­t ngÆ°á»¡ng Ä‘á»ƒ lá»c ra
nhá»¯ng giÃ¡ trá»‹ ráº¥t nhá» vÃ  chá»‰ táº­p trung vÃ o nhá»¯ng giÃ¡ trá»‹ quan trá»ng nháº¥t.
Gá»i 1(Â·) lÃ  hÃ m chá»‰ thá»‹, vÃ  A(l)i lÃ  vector chÃº Ã½
cá»§a token thá»© i (hÃ ng) trong lá»›p thá»© l. ChÃºng tÃ´i Ä‘Æ°a ra
má»™t ngÆ°á»¡ng Ï„ (xem ablation trong Báº£ng 7) phá»¥ thuá»™c vÃ o
sá»‘ token N, tá»©c lÃ  Ï„/N. Do Ä‘Ã³, sá»± chÃº Ã½ sau
ngÆ°á»¡ng trá»Ÿ thÃ nh

Ë†A(l)i = 1(A(l)i â‰¥ Ï„/N) Â· A(l)i. (1)

Thá»© hai, Ä‘á»ƒ trÃ¡nh Ä‘á»™ phá»©c táº¡p báº­c hai cá»§a viá»‡c tÃ­nh
Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c cáº·p N hÃ ng, chÃºng tÃ´i xáº¥p xá»‰
nÃ³ báº±ng cÃ¡ch tÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine giá»¯a má»—i
vector chÃº Ã½ cÃ¡ nhÃ¢n Ë†A(l)i vá»›i chÃº Ã½ trung bÃ¬nh Â¯A(l) := 
1/N âˆ‘Ni=1 Ë†A(l)i. Khi xem xÃ©t má»™t mÃ´ hÃ¬nh vá»›i L lá»›p, chÃºng tÃ´i
tÃ­nh trung bÃ¬nh ADL loss qua táº¥t cáº£ cÃ¡c lá»›p báº±ng:

LADL = 1/L âˆ‘Ll=1 L(l)ADL, L(l)ADL = 1/N âˆ‘Ni=1 (Ë†A(l)i Â· Â¯A(l))/(âˆ¥Ë†A(l)iâˆ¥ âˆ¥Â¯A(l)âˆ¥). (2)

--- TRANG 5 ---
PhÆ°Æ¡ng phÃ¡p #Params (M) #FLOPs (G) ImageNet â†‘ImageNet-C â†“ImageNet-P â†“ImageNet-A â†‘ImageNet-R â†‘
ConvNeXt-B [30] 88.6 15.4 83.8 46.8 - 36.7 51.3
ConViT-B [14] 86.5 17.7 82.4 46.9 32.2 29.0 48.4
Swin-B [29] 87.8 15.4 83.4 54.4 32.7 35.8 46.6
T2T-ViT t-24 [62] 64.1 15.0 82.6 48.0 31.8 28.9 47.9
RSPC (FAN-B-Hybrid) [20] 50.5 11.7 84.2 44.5 30.0 41.1 -
RVT-B [32] 91.8 17.7 82.6 46.8 31.9 28.5 48.7
+ TAP 92.1 17.9 83.0 (+0.4) 45.5 (-1.3) 30.6 (-1.3) 30.0 (+1.5) 49.4 (+0.7)
+ ADL 91.8 17.7 82.6 (+0.0) 45.2 (-1.6) 30.2 (-1.7) 30.8 (+2.3) 49.8 (+1.1)
+ TAP & ADL 92.1 17.9 83.1 (+0.5) 44.7 (-2.1) 29.6 (-2.3) 32.7 (+4.2) 50.2 (+1.5)
FAN-B-Hybrid [67] 50.4 11.7 83.9 46.1 31.3 39.6 52.7
+ TAP 50.7 11.8 84.3 (+0.4) 44.9 (-1.2) 30.3 (-1.0) 41.0 (+1.4) 53.9 (+1.2)
+ ADL 50.4 11.7 84.0 (+0.1) 44.4 (-1.7) 29.8 (-1.5) 41.4 (+1.8) 54.2 (+1.5)
+ TAP & ADL 50.7 11.8 84.3 (+0.4) 43.7 (-2.4) 29.2 (-2.1) 42.3 (+2.7) 54.6 (+1.9)

Báº£ng 2. So sÃ¡nh trÃªn ImageNet vÃ  cÃ¡c benchmark Ä‘á»™ bá»n vá»¯ng Ä‘a dáº¡ng. ChÃºng tÃ´i bÃ¡o cÃ¡o mean corruption error (mCE) trÃªn ImageNet-C vÃ  mean flip rate (mFR) trÃªn ImageNet-P. Äá»‘i vá»›i nhá»¯ng chá»‰ sá»‘ nÃ y, tháº¥p hÆ¡n lÃ  tá»‘t hÆ¡n. HÆ¡n ná»¯a, chÃºng tÃ´i bÃ¡o cÃ¡o trá»±c tiáº¿p Ä‘á»™ chÃ­nh xÃ¡c trÃªn ImageNet-A vÃ  ImageNet-R. Dá»±a trÃªn hai baseline Ä‘Æ°á»£c xem xÃ©t, cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i luÃ´n cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ bá»n vá»¯ng trÃªn cÃ¡c benchmark Ä‘a dáº¡ng.

Trong thá»±c táº¿, chÃºng tÃ´i káº¿t há»£p ADL vá»›i loss cross-entropy (CE) tiÃªu chuáº©n vÃ  Ä‘Æ°a ra má»™t siÃªu tham sá»‘ Î» (xem ablation trong HÃ¬nh 7) Ä‘á»ƒ kiá»ƒm soÃ¡t táº§m quan trá»ng cá»§a ADL:

L = LCE + Î»LADL. (3)

ChÃºng tÃ´i nháº¥n máº¡nh ráº±ng ADL cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ tÄƒng cÆ°á»ng Ä‘á»™ bá»n vá»¯ng trÃªn cÃ¡c tÃ¡c vá»¥ Ä‘a dáº¡ng, bao gá»“m phÃ¢n loáº¡i hÃ¬nh áº£nh vÃ  phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a (xem Báº£ng 2 vÃ  Báº£ng 4).

4. ThÃ­ nghiá»‡m
ChÃºng tÃ´i thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m má»Ÿ rá»™ng Ä‘á»ƒ xÃ¡c minh phÆ°Æ¡ng phÃ¡p trÃªn cáº£ tÃ¡c vá»¥ phÃ¢n loáº¡i hÃ¬nh áº£nh vÃ  phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a. Trong Pháº§n 4.1, chÃºng tÃ´i Ä‘áº§u tiÃªn huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i trÃªn ImageNet [12] vÃ  chá»©ng minh ráº±ng cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ trÃªn cÃ¡c benchmark Ä‘á»™ bá»n vá»¯ng khÃ¡c nhau, bao gá»“m ImageNet-A [64], ImageNet-C [24], ImageNet-R [23], vÃ  ImageNet-P [24]. Sau Ä‘Ã³, trong Pháº§n 4.2, chÃºng tÃ´i láº¥y mÃ´ hÃ¬nh pre-trained tá»‘t nháº¥t vÃ  tiáº¿p tá»¥c fine-tune nÃ³ trÃªn Cityscapes [11] cho phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a. Trong thá»±c táº¿, cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ mIoU trÃªn hai benchmark Ä‘á»™ bá»n vá»¯ng phá»• biáº¿n, bao gá»“m Cityscapes-C [33] vÃ  ACDC [39], cÃ¹ng vá»›i hiá»‡u suáº¥t cáº¡nh tranh trÃªn dá»¯ liá»‡u sáº¡ch. Cáº£ mÃ£ nguá»“n vÃ  cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n sáº½ sá»›m cÃ³ sáºµn.

4.1. Káº¿t quáº£ trÃªn PhÃ¢n loáº¡i HÃ¬nh áº£nh
Trong thÃ­ nghiá»‡m nÃ y, chÃºng tÃ´i xÃ¢y dá»±ng phÆ°Æ¡ng phÃ¡p trÃªn Ä‘á»‰nh hai kiáº¿n trÃºc bá»n vá»¯ng tiÃªn tiáº¿n: RVT [32] vÃ  FAN [67] vá»›i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh "Base", tá»©c lÃ  RVT-B vÃ  FAN-B-Hybrid. ChÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh trÃªn ImageNet vÃ  Ä‘Ã¡nh giÃ¡ chÃºng trÃªn má»™t sá»‘ benchmark Ä‘á»™ bá»n vá»¯ng. ChÃºng tÃ´i tuÃ¢n thá»§ cháº·t cháº½ cÃ¡c cÃ i Ä‘áº·t cá»§a RVT vÃ  FAN Ä‘á»ƒ huáº¥n luyá»‡n. Cá»¥ thá»ƒ, chÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng cÃ¹ng sÆ¡ Ä‘á»“ augmentation vÃ  Ã¡p dá»¥ng batch size 2048. ChÃºng tÃ´i Ä‘áº·t learning rate thÃ nh 2Ã—10âˆ’3 vÃ  huáº¥n luyá»‡n táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh trong 300 epoch. Trong táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m, theo máº·c Ä‘á»‹nh, chÃºng tÃ´i Ä‘áº·t K= 4 vÃ  Î»= 1 Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i. Äá»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ bá»n vá»¯ng, chÃºng tÃ´i xem xÃ©t má»™t sá»‘ benchmark Ä‘á»™ bá»n vá»¯ng, bao gá»“m ImageNet-A [64], ImageNet-C [24], ImageNet-R [23], vÃ  ImageNet-P [24]. LÆ°u Ã½ ráº±ng, chÃºng tÃ´i bÃ¡o cÃ¡o mean corruption error (mCE) trÃªn ImageNet-C vÃ  mean flip rate (mFR) trÃªn ImageNet-P. Äá»‘i vá»›i cáº£ hai chá»‰ sá»‘, tháº¥p hÆ¡n lÃ  tá»‘t hÆ¡n. Vá» máº·t thá»±c nghiá»‡m, chÃºng tÃ´i chá»©ng minh ráº±ng viá»‡c sá»­ dá»¥ng TAP hoáº·c ADL riÃªng láº» Ä‘á»u cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng. Khi káº¿t há»£p chÃºng láº¡i, cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i vÆ°á»£t trá»™i hÆ¡n baseline má»™t cÃ¡ch lá»›n hÆ¡n vÃ  sá»± cáº£i thiá»‡n hiá»‡u suáº¥t tá»•ng quÃ¡t hÃ³a tá»‘t cho cÃ¡c kiáº¿n trÃºc Ä‘a dáº¡ng (xem Báº£ng 5).

4.1.1 So sÃ¡nh trÃªn ImageNet
NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 2, so vá»›i cÃ¡c baseline máº¡nh RVT vÃ  FAN, cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i luÃ´n cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng trÃªn ImageNet-C >2.1% vÃ  cÅ©ng mang láº¡i cáº£i thiá»‡n tÆ°Æ¡ng Ä‘Æ°Æ¡ng trÃªn cÃ¡c benchmark Ä‘á»™ bá»n vá»¯ng khÃ¡c, bao gá»“m ImageNet-A/R/SK. HÆ¡n ná»¯a, cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i cÅ©ng Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n cáº¡nh tranh vá» Ä‘á»™ chÃ­nh xÃ¡c sáº¡ch trÃªn ImageNet. VÃ­ dá»¥, chÃºng tÃ´i cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c >0.4% trÃªn cáº£ hai kiáº¿n trÃºc baseline Ä‘Æ°á»£c xem xÃ©t. Quan trá»ng hÆ¡n, chÃºng tÃ´i nháº¥n máº¡nh ráº±ng nhá»¯ng cáº£i thiá»‡n nÃ y chá»‰ Ä‘i kÃ¨m vá»›i chi phÃ­ tÃ­nh toÃ¡n khÃ´ng Ä‘Ã¡ng ká»ƒ vá» cáº£ sá»‘ tham sá»‘ vÃ  sá»‘ phÃ©p toÃ¡n dáº¥u pháº©y Ä‘á»™ng (FLOP). NgoÃ i ra, chÃºng tÃ´i cÅ©ng bÃ¡o cÃ¡o chi tiáº¿t corruption error trÃªn tá»«ng loáº¡i lá»—i riÃªng láº» cá»§a ImageNet-C dá»±a trÃªn FAN-B-Hybrid. Tá»« Báº£ng 3, mÃ´ hÃ¬nh tá»‘t nháº¥t cá»§a chÃºng tÃ´i (káº¿t há»£p TAP vÃ  ADL láº¡i) Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t nháº¥t trÃªn háº§u háº¿t cÃ¡c loáº¡i lá»—i. ÄÃ¡ng chÃº Ã½ lÃ  mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i Ä‘áº·c biá»‡t hiá»‡u quáº£ chá»‘ng láº¡i lá»—i nhiá»…u, vÃ­ dá»¥ mang láº¡i cáº£i thiá»‡n lá»›n 6,25% trÃªn lá»—i nhiá»…u Gaussian. NhÃ¬n chung, nhá»¯ng thÃ­ nghiá»‡m nÃ y cho tháº¥y viá»‡c tÄƒng cÆ°á»ng Ä‘á»™ bá»n cho sá»± chÃº Ã½ luÃ´n cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng qua cÃ¡c kiáº¿n trÃºc vÃ  benchmark khÃ¡c nhau.

--- TRANG 6 ---
PhÆ°Æ¡ng phÃ¡p mCE Noise Blur Weather Digital
Gaussian Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Brightness Contrast Elastic Pixelate JPEG
FAN-B-Hybrid 46.2 40.12 39.27 36.80 51.58 63.96 47.53 54.98 40.24 43.96 36.98 36.68 34.17 61.59 53.25 51.86
+ TAP 44.9 36.02 36.26 34.16 52.72 65.07 45.73 54.90 39.75 42.39 35.68 37.38 33.21 62.75 48.85 49.46
+ ADL 44.3 35.61 35.55 33.51 50.80 64.27 45.47 54.47 38.06 40.46 37.92 36.99 32.70 61.45 47.78 49.00
+ TAP & ADL 43.7 33.87 34.24 32.04 51.29 61.51 44.76 54.39 38.14 40.12 35.27 36.43 32.25 62.12 46.78 49.55

Báº£ng 3. So sÃ¡nh corruption error (tháº¥p hÆ¡n lÃ  tá»‘t hÆ¡n) trÃªn tá»«ng loáº¡i lá»—i riÃªng láº» cá»§a ImageNet-C dá»±a trÃªn FAN-B-Hybrid. Káº¿t há»£p TAP vÃ  ADL láº¡i mang láº¡i káº¿t quáº£ tá»‘t nháº¥t trÃªn háº§u háº¿t cÃ¡c loáº¡i lá»—i.

4.1.2 TÃ­nh á»•n Ä‘á»‹nh ChÃº Ã½ vÃ  Káº¿t quáº£ Trá»±c quan hÃ³a
ChÃºng tÃ´i chá»©ng minh ráº±ng cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ tÃ­nh á»•n Ä‘á»‹nh chÃº Ã½ chá»‘ng láº¡i lá»—i cáº£ vá» máº·t Ä‘á»‹nh tÃ­nh vÃ  Ä‘á»‹nh lÆ°á»£ng. ThÃº vá»‹ lÃ , trong má»—i lá»›p, cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c sá»± Ä‘a dáº¡ng chÃº Ã½ cao hÆ¡n giá»¯a cÃ¡c head chÃº Ã½ khÃ¡c nhau.

TÃ­nh á»•n Ä‘á»‹nh chÃº Ã½. Trong HÃ¬nh 3, chÃºng tÃ´i Ä‘áº§u tiÃªn trá»±c quan hÃ³a má»©c Ä‘á»™ thay Ä‘á»•i cá»§a sá»± chÃº Ã½ khi Ä‘á»‘i máº·t vá»›i lá»—i hÃ¬nh áº£nh, vÃ­ dá»¥ nhiá»…u Gaussian. ChÃºng tÃ´i láº¥y FAN-B-Hybrid lÃ m baseline vÃ  so sÃ¡nh mÃ´ hÃ¬nh tá»‘t nháº¥t cá»§a chÃºng tÃ´i vá»›i hai biáº¿n thá»ƒ chá»‰ chá»©a TAP vÃ  ADL riÃªng láº». Qua cÃ¡c vÃ­ dá»¥ Ä‘a dáº¡ng, mÃ´ hÃ¬nh baseline gÃ¢y ra váº¥n Ä‘á» táº­p trung quÃ¡ má»©c vÃ o token nghiÃªm trá»ng ráº±ng nÃ³ Ä‘áº·t quÃ¡ nhiá»u sá»± táº­p trung vÃ o ráº¥t Ã­t token vÃ  Ä‘i kÃ¨m vá»›i sá»± dá»‹ch chuyá»ƒn chÃº Ã½ Ä‘Ã¡ng ká»ƒ khi Ä‘á»‘i máº·t vá»›i lá»—i. Vá»›i sá»± giÃºp Ä‘á»¡ cá»§a mÃ´-Ä‘un tá»•ng há»£p Ä‘á»‹a phÆ°Æ¡ng TAP, mÃ´ hÃ¬nh TAP cá»§a chÃºng tÃ´i gÃ¡n sá»± chÃº Ã½ cho nhiá»u token xung quanh má»™t sá»‘ token quan trá»ng, giáº£m thiá»ƒu váº¥n Ä‘á» táº­p trung quÃ¡ má»©c vÃ o token á»Ÿ má»™t má»©c Ä‘á»™ nháº¥t Ä‘á»‹nh. Tuy nhiÃªn, chÃºng tÃ´i váº«n quan sÃ¡t tháº¥y sá»± dá»‹ch chuyá»ƒn chÃº Ã½ giá»¯a cÃ¡c vÃ­ dá»¥ sáº¡ch vÃ  bá»‹ lá»—i. Khi huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh vá»›i ADL, sá»± chÃº Ã½ tuÃ¢n theo má»™t máº«u Ä‘Æ°á»ng chÃ©o sao cho cÃ¡c token tá»•ng há»£p thÃ´ng tin tá»« nhá»¯ng token khÃ¡c trong khi giá»¯ láº¡i háº§u háº¿t thÃ´ng tin tá»« chÃ­nh nÃ³. ChÃºng tÃ´i nháº¥n máº¡nh ráº±ng máº«u Ä‘Æ°á»ng chÃ©o nÃ y pháº§n nÃ o tÆ°Æ¡ng tá»± vá»›i há»c táº­p dÆ° (residual learning) [22] mÃ  há»c thÃªm má»™t nhÃ¡nh dÆ° trong khi giá»¯ Ä‘áº·c trÆ°ng khÃ´ng Ä‘á»•i báº±ng cÃ¡ch sá»­ dá»¥ng má»™t shortcut Ä‘á»“ng nháº¥t. RÃµ rÃ ng, sá»± chÃº Ã½ trá»Ÿ nÃªn á»•n Ä‘á»‹nh hÆ¡n nhiá»u chá»‘ng láº¡i lá»—i. Khi káº¿t há»£p TAP vÃ  ADL láº¡i, chÃºng tÃ´i tiáº¿p tá»¥c khuyáº¿n khÃ­ch máº«u Ä‘Æ°á»ng chÃ©o má»Ÿ rá»™ng trong má»™t vÃ¹ng Ä‘á»‹a phÆ°Æ¡ng. Báº±ng cÃ¡ch nÃ y, má»—i token Ä‘áº·t nhiá»u sá»± táº­p trung hÆ¡n vÃ o vÃ¹ng lÃ¢n cáº­n cá»§a nÃ³ ngoÃ i chÃ­nh nÃ³ vÃ  do Ä‘Ã³ Ä‘áº¡t Ä‘Æ°á»£c Ä‘áº·c trÆ°ng máº¡nh hÆ¡n. Vá» máº·t Ä‘á»‹nh lÆ°á»£ng, chÃºng tÃ´i cÅ©ng Ä‘Ã¡nh giÃ¡ tÃ­nh á»•n Ä‘á»‹nh chÃº Ã½ báº±ng cÃ¡ch tÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine cá»§a sá»± chÃº Ã½ giá»¯a cÃ¡c vÃ­ dá»¥ sáº¡ch vÃ  bá»‹ lá»—i qua toÃ n bá»™ ImageNet. Tá»« HÃ¬nh 5, TAP mang láº¡i tÃ­nh á»•n Ä‘á»‹nh chÃº Ã½ cao hÆ¡n so vá»›i mÃ´ hÃ¬nh baseline. Nhá» máº«u Ä‘Æ°á»ng chÃ©o trong sá»± chÃº Ã½, mÃ´ hÃ¬nh vá»›i ADL cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng má»™t cÃ¡ch lá»›n, cho tháº¥y sá»± chÃº Ã½ ráº¥t á»•n Ä‘á»‹nh chá»‘ng láº¡i lá»—i. Khi káº¿t há»£p cáº£ TAP vÃ  ADL, chÃºng tÃ´i cÃ³ thá»ƒ tiáº¿p tá»¥c cáº£i thiá»‡n tÃ­nh á»•n Ä‘á»‹nh chÃº Ã½.

ChÃº Ã½ cá»§a má»—i head vÃ  sá»± Ä‘a dáº¡ng chÃº Ã½. Trong pháº§n nÃ y, chÃºng tÃ´i tiáº¿p tá»¥c Ä‘iá»u tra báº£n Ä‘á»“ chÃº Ã½ trong má»—i head riÃªng láº». NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 4, Ä‘á»‘i vá»›i mÃ´ hÃ¬nh baseline, cÃ¡c token quan trá»ng nháº¥t luÃ´n lÃ  nhá»¯ng token quan trá»ng nháº¥t trong háº§u háº¿t táº¥t cáº£ cÃ¡c head, dáº«n Ä‘áº¿n sá»± Ä‘a dáº¡ng ráº¥t tháº¥p

Äáº§u vÃ o Baseline (FAN-B-Hybrid) TAP TAP + ADL ADL
HÃ¬nh 3. So sÃ¡nh cÃ¡c báº£n Ä‘á»“ chÃº Ã½ giá»¯a cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau. So vá»›i mÃ´ hÃ¬nh baseline, TAP cá»§a chÃºng tÃ´i giáº£m thiá»ƒu váº¥n Ä‘á» táº­p trung quÃ¡ má»©c báº±ng cÃ¡ch khuyáº¿n khÃ­ch cÃ¡c token xung quanh nhá»¯ng token quan trá»ng nháº¥t cÃ³ Ä‘iá»ƒm chÃº Ã½ cao hÆ¡n. Khi chá»‰ sá»­ dá»¥ng ADL, chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c má»™t báº£n Ä‘á»“ chÃº Ã½ tuÃ¢n theo máº«u Ä‘Æ°á»ng chÃ©o, tá»©c lÃ  giá»¯ láº¡i chÃ­nh token Ä‘Ã³ trong khi tá»•ng há»£p thÃ´ng tin tá»« cÃ¡c token khÃ¡c. RÃµ rÃ ng, cÃ¡c hÃ ng chÃº Ã½ khÃ¡c nhau vá»›i nhau, hoÃ n thÃ nh má»¥c tiÃªu giáº£m Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c hÃ ng. Khi káº¿t há»£p TAP vÃ  ADL láº¡i, máº«u Ä‘Æ°á»ng chÃ©o Ä‘Æ°á»£c má»Ÿ rá»™ng thÃªm vÃ o cÃ¡c khu vá»±c lÃ¢n cáº­n nhá» mÃ´-Ä‘un TAP.

cá»§a sá»± chÃº Ã½ giá»¯a cÃ¡c head khÃ¡c nhau. NgÆ°á»£c láº¡i, trong mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i, chá»‰ cÃ³ hai head tuÃ¢n theo máº«u Ä‘Æ°á»ng chÃ©o vÃ  sÃ¡u head cÃ²n láº¡i cÃ³ cÃ¡c máº«u chÃº Ã½ khÃ¡c nhau. ChÃºng tÃ´i nháº¥n máº¡nh ráº±ng sá»± chÃº Ã½ cá»§a chÃºng tÃ´i lÃ  sá»± káº¿t há»£p cá»§a cáº£ bá»™ lá»c Ä‘á»‹a phÆ°Æ¡ng vÃ  toÃ n cá»¥c w.r.t cÃ¡c head khÃ¡c nhau. Cá»¥ thá»ƒ, hai head Ä‘Æ°á»ng chÃ©o nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  bá»™ lá»c Ä‘á»‹a phÆ°Æ¡ng Ä‘á»ƒ trÃ­ch xuáº¥t thÃ´ng tin Ä‘á»‹a phÆ°Æ¡ng vÃ¬ máº«u Ä‘Æ°á»ng chÃ©o khuyáº¿n khÃ­ch cÃ¡c token tá»•ng há»£p thÃ´ng tin trong vÃ¹ng lÃ¢n cáº­n Ä‘á»‹a phÆ°Æ¡ng cá»§a chÃºng. Äá»‘i vá»›i sÃ¡u head cÃ²n láº¡i, sá»± chÃº Ã½ Ä‘Æ°á»£c phÃ¢n phá»‘i qua toÃ n bá»™ báº£n Ä‘á»“ vÃ  do Ä‘Ã³ chÃºng cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  bá»™ lá»c toÃ n cá»¥c. Tá»« káº¿t quáº£ trá»±c quan hÃ³a trong HÃ¬nh 4, mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i cÃ³ sá»± Ä‘a dáº¡ng chÃº Ã½ cao hÆ¡n giá»¯a cÃ¡c head khÃ¡c nhau. Äá»ƒ Ä‘á»‹nh lÆ°á»£ng Ä‘iá»u nÃ y, theo phÆ°Æ¡ng phÃ¡p Ä‘Ã£ tháº£o luáº­n trÆ°á»›c Ä‘Ã³, chÃºng tÃ´i trá»±c tiáº¿p tÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine cá»§a cÃ¡c báº£n Ä‘á»“ chÃº Ã½ giá»¯a báº¥t ká»³ hai head nÃ o trong má»—i lá»›p (xem phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n chi tiáº¿t trong phá»¥

--- TRANG 7 ---
Head 1 Head 2 Head 3 Head 4 Head 5 Head 6 Head 7 Head 8 Äáº§u vÃ o FAN-B-Hybrid Cá»§a chÃºng tÃ´i Trung bÃ¬nh

HÃ¬nh 4. Báº£n Ä‘á»“ chÃº Ã½ cá»§a cÃ¡c head chÃº Ã½ khÃ¡c nhau trong lá»›p cuá»‘i cÃ¹ng. Äá»‘i vá»›i mÃ´ hÃ¬nh baseline, cÃ¡c token quan trá»ng nháº¥t thÆ°á»ng Ä‘Æ°á»£c chia sáº» qua cÃ¡c head khÃ¡c nhau. Trong mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i, hai head cÃ³ sá»± chÃº Ã½ vá»›i máº«u Ä‘Æ°á»ng chÃ©o vÃ  cÃ¡c head khÃ¡c cÃ³ cÃ¡c máº«u cá»¥ thá»ƒ Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng khÃ¡c nhau, mang láº¡i sá»± Ä‘a dáº¡ng chÃº Ã½ cao hÆ¡n giá»¯a cÃ¡c head (xem káº¿t quáº£ Ä‘á»‹nh lÆ°á»£ng trong Pháº§n 4.1.2).

HÃ¬nh 5. PhÃ¢n phá»‘i Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine cá»§a cÃ¡c báº£n Ä‘á»“ chÃº Ã½ trung gian giá»¯a cÃ¡c vÃ­ dá»¥ sáº¡ch vÃ  bá»‹ lá»—i (vÃ­ dá»¥ vá»›i nhiá»…u Gaussian) trÃªn ImageNet. ChÃºng tÃ´i chá»©ng minh ráº±ng TAP hoáº·c ADL Ä‘á»u cÃ³ thá»ƒ cáº£i thiá»‡n tÃ­nh á»•n Ä‘á»‹nh cá»§a sá»± chÃº Ã½ má»™t cÃ¡ch Ä‘á»™c láº­p. Khi káº¿t há»£p chÃºng láº¡i, chÃºng tÃ´i tiáº¿p tá»¥c cáº£i thiá»‡n tÃ­nh á»•n Ä‘á»‹nh/tÆ°Æ¡ng Ä‘á»“ng cá»§a sá»± chÃº Ã½ chá»‘ng láº¡i lá»—i.

lá»¥c). NÃ³i cÃ¡ch khÃ¡c, Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng cÃ ng tháº¥p, sá»± Ä‘a dáº¡ng chÃº Ã½ cÃ ng cao. Trong thá»±c táº¿, mÃ´ hÃ¬nh baseline Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng cao 0,63 khi tÃ­nh trung bÃ¬nh qua toÃ n bá»™ ImageNet, cho tháº¥y sá»± Ä‘a dáº¡ng chÃº Ã½ ráº¥t tháº¥p giá»¯a cÃ¡c head khÃ¡c nhau. NgÆ°á»£c láº¡i, mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i mang láº¡i Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng tháº¥p hÆ¡n lÃ  0,27, phÃ¹ há»£p vá»›i quan sÃ¡t trÆ°á»›c Ä‘Ã³ ráº±ng mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i táº¡o ra sá»± chÃº Ã½ Ä‘a dáº¡ng qua cÃ¡c head khÃ¡c nhau.

4.2. Káº¿t quáº£ trÃªn PhÃ¢n Ä‘oáº¡n Ngá»¯ nghÄ©a
Trong pháº§n nÃ y, chÃºng tÃ´i tiáº¿p tá»¥c Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p cho cÃ¡c tÃ¡c vá»¥ phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a. ChÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh trÃªn Cityscapes [11] vÃ  Ä‘Ã¡nh giÃ¡ Ä‘á»™ bá»n vá»¯ng trÃªn hai benchmark phá»• biáº¿n, bao gá»“m Cityscapes-C [33] vÃ  ACDC [39]. Cá»¥ thá»ƒ, Cityscapes-C chá»©a 16 loáº¡i lá»—i cÃ³ thá»ƒ Ä‘Æ°á»£c chia thÃ nh 4 danh má»¥c: noise, blur, weather, vÃ  digital. ACDC thu tháº­p cÃ¡c hÃ¬nh áº£nh vá»›i Ä‘iá»u kiá»‡n báº¥t lá»£i, bao gá»“m Ä‘Ãªm, sÆ°Æ¡ng mÃ¹, mÆ°a, vÃ  tuyáº¿t. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i bÃ¡o cÃ¡o mIoU qua cÃ¡c táº­p dá»¯ liá»‡u Ä‘a dáº¡ng. Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, chÃºng tÃ´i tuÃ¢n theo cÃ¹ng cÃ i Ä‘áº·t cá»§a SegFormer [55] Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i. ChÃºng tÃ´i chá»©ng minh ráº±ng TAP vÃ  ADL cá»§a chÃºng tÃ´i cÅ©ng tá»•ng quÃ¡t hÃ³a tá»‘t cho cÃ¡c tÃ¡c vá»¥ phÃ¢n Ä‘oáº¡n vÃ  cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘á»™ bá»n vá»¯ng.

MÃ´ hÃ¬nh Cityscapes â†‘ Cityscapes-C â†‘ ACDC â†‘
DeepLabv3+ (R101) [7] 77.1 39.4 41.6
ICNet [63] 65.9 28.0 -
DilatedNet [60] 68.6 30.3 -
Swin-T [29] 78.1 47.3 56.3
SETR [65] 79.5 63.1 60.2
Segformer-B5 [55] 82.4 65.8 62.0
FAN-B-Hybrid [67] 82.2 67.3 60.6
+ TAP 82.7 (+0.5) 69.2 (+1.9) 62.7 (+2.1)
+ ADL 82.4 (+0.2) 69.4 (+2.1) 63.1 (+2.5)
+ TAP & ADL 82.9 (+0.7) 69.7 (+2.4) 63.6 (+3.0)

Báº£ng 4. So sÃ¡nh cÃ¡c mÃ´ hÃ¬nh phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a trÃªn táº­p validation Cityscapes, Cityscapes-C, vÃ  táº­p test ACDC. Cáº£ TAP vÃ  ADL cá»§a chÃºng tÃ´i Ä‘á»u cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘á»™ bá»n vá»¯ng. ChÃºng tÃ´i cÃ³ thá»ƒ tiáº¿p tá»¥c cáº£i thiá»‡n káº¿t quáº£ khi káº¿t há»£p TAP vÃ  ADL láº¡i.

4.2.1 So sÃ¡nh Äá»‹nh lÆ°á»£ng
NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 4, so vá»›i mÃ´ hÃ¬nh baseline Ä‘Æ°á»£c xem xÃ©t, viá»‡c sá»­ dá»¥ng TAP hoáº·c ADL riÃªng láº» Ä‘á»u cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘á»™ bá»n vá»¯ng trÃªn Cityscapes-C vÃ  ACDC. Vá»›i sá»± giÃºp Ä‘á»¡ cá»§a mÃ´-Ä‘un tá»•ng há»£p Ä‘á»‹a phÆ°Æ¡ng hiá»‡u quáº£ TAP, chÃºng tÃ´i nháº¥n máº¡nh ráº±ng chÃºng tÃ´i cÅ©ng Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n Ä‘Ã¡ng há»©a háº¹n 0,5% mIoU trÃªn dá»¯ liá»‡u sáº¡ch. Khi káº¿t há»£p TAP vÃ  ADL láº¡i, chÃºng tÃ´i tiáº¿p tá»¥c cáº£i thiá»‡n hiá»‡u suáº¥t vÃ  Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n lá»›n hÆ¡n 2,4% vÃ  3,0% trÃªn Cityscapes-C vÃ  ACDC, tÆ°Æ¡ng á»©ng. HÆ¡n ná»¯a, mÃ´ hÃ¬nh tá»‘t nháº¥t cá»§a chÃºng tÃ´i cÅ©ng vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i má»™t sá»‘ mÃ´ hÃ¬nh phÃ¢n Ä‘oáº¡n phá»• biáº¿n vá»›i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh tÆ°Æ¡ng Ä‘Æ°Æ¡ng. NhÃ¬n chung, nhá»¯ng káº¿t quáº£ nÃ y cho tháº¥y hai ká»¹ thuáº­t Ä‘á» xuáº¥t khÃ´ng chá»‰ hoáº¡t Ä‘á»™ng cho phÃ¢n loáº¡i hÃ¬nh áº£nh mÃ  cÃ²n tá»•ng quÃ¡t hÃ³a tá»‘t cho cÃ¡c tÃ¡c vá»¥ phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a.

4.2.2 So sÃ¡nh Trá»±c quan
Trong pháº§n nÃ y, chÃºng tÃ´i so sÃ¡nh káº¿t quáº£ trá»±c quan hÃ³a cá»§a cÃ¡c máº·t náº¡ phÃ¢n Ä‘oáº¡n Ä‘Æ°á»£c dá»± Ä‘oÃ¡n dá»±a trÃªn cÃ¡c vÃ­ dá»¥ cá»§a cÃ¡c benchmark Ä‘á»™ bá»n vá»¯ng Ä‘a dáº¡ng. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 6, Ä‘á»‘i vá»›i vÃ­ dá»¥ Ä‘áº§u tiÃªn vá»›i lá»—i tuyáº¿t, mÃ´ hÃ¬nh baseline khÃ´ng thá»ƒ phÃ¡t hiá»‡n má»™t vÃ¹ng lá»›n Ä‘Æ°á»ng (Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u bá»Ÿi há»™p mÃ u Ä‘á»

--- TRANG 8 ---
HÃ¬nh áº£nh tuyáº¿t trong Cityscapes-C FAN-B-Hybrid FAN-B-Hybrid (TAP & ADL) Ground Truth HÃ¬nh áº£nh

HÃ¬nh áº£nh Ä‘Ãªm trong ACDC

HÃ¬nh 6. So sÃ¡nh trá»±c quan káº¿t quáº£ phÃ¢n Ä‘oáº¡n. Khi Ä‘á»‘i máº·t vá»›i lá»—i hÃ¬nh áº£nh hoáº·c Ä‘iá»u kiá»‡n báº¥t lá»£i, mÃ´ hÃ¬nh FAN-B-Hybrid baseline khÃ´ng thá»ƒ phÃ¡t hiá»‡n má»™t sá»‘ Ä‘á»‘i tÆ°á»£ng quan trá»ng (vÃ­ dá»¥ Ä‘Æ°á»ng trong vÃ­ dá»¥ Ä‘áº§u tiÃªn) hoáº·c nháº­n dáº¡ng nháº§m má»™t pháº§n xe thÃ nh ngÆ°á»i lÃ¡i xe (trong vÃ­ dá»¥ thá»© hai). NgÆ°á»£c láº¡i, mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i bá»n vá»¯ng hÆ¡n nhiá»u chá»‘ng láº¡i nhá»¯ng lá»—i vÃ  Ä‘iá»u kiá»‡n báº¥t lá»£i nÃ y.

), Ä‘iá»u nÃ y gÃ¢y ra nhá»¯ng rá»§i ro tiá»m áº©n khi Ã¡p dá»¥ng trong má»™t sá»‘ á»©ng dá»¥ng thá»±c táº¿, vÃ­ dá»¥ lÃ¡i xe tá»± Ä‘á»™ng. HÆ¡n ná»¯a, trong vÃ­ dá»¥ thá»© hai vá»›i Ä‘iá»u kiá»‡n Ä‘Ãªm, mÃ´ hÃ¬nh baseline nháº­n dáº¡ng má»™t pháº§n xe thÃ nh ngÆ°á»i lÃ¡i xe vÃ  Ä‘Æ°a ra nhiá»u artifact trong máº·t náº¡ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n. NgÆ°á»£c láº¡i, mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i bá»n vá»¯ng hÆ¡n nhiá»u vÃ  cÃ³ thá»ƒ phÃ¡t hiá»‡n chÃ­nh xÃ¡c háº§u háº¿t cÃ¡c pháº§n cá»§a Ä‘Æ°á»ng vÃ  xe trong cáº£ hai trÆ°á»ng há»£p. ChÃºng tÃ´i nháº¥n máº¡nh ráº±ng sá»± vÆ°á»£t trá»™i vá» Ä‘á»™ bá»n vá»¯ng cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c quan sÃ¡t trÃªn háº§u háº¿t cÃ¡c vÃ­ dá»¥ cá»§a cÃ¡c benchmark Ä‘Æ°á»£c xem xÃ©t. Vui lÃ²ng tham kháº£o thÃªm so sÃ¡nh trá»±c quan trong phá»¥ lá»¥c.

5. PhÃ¢n tÃ­ch vÃ  Tháº£o luáº­n
Trong pháº§n sau, chÃºng tÃ´i trÃ¬nh bÃ y thÃªm cÃ¡c thÃ­ nghiá»‡m ablation vÃ  tháº£o luáº­n. Trong Pháº§n 5.1, chÃºng tÃ´i chá»©ng minh ráº±ng hai phÆ°Æ¡ng phÃ¡p Ä‘á» xuáº¥t lÃ  nhá»¯ng ká»¹ thuáº­t tá»•ng quÃ¡t vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng trÃªn Ä‘á»‰nh cÃ¡c kiáº¿n trÃºc transformer Ä‘a dáº¡ng. Trong Pháº§n 5.2, chÃºng tÃ´i nghiÃªn cá»©u tÃ¡c Ä‘á»™ng cá»§a sá»‘ nhÃ¡nh K trong TAP vÃ  trá»ng sá»‘ cá»§a ADL trong loss huáº¥n luyá»‡n. Trong Pháº§n 5.3, chÃºng tÃ´i Ä‘iá»u tra chiáº¿n lÆ°á»£c phÃ¢n bá»• cÃ¡c lá»›p TAP. Trong thá»±c táº¿, viá»‡c phÃ¢n bá»• Ä‘á»“ng Ä‘á»u TAP vÃ o má»—i block mang láº¡i káº¿t quáº£ tá»‘t nháº¥t. Trong Pháº§n 5.4, chÃºng tÃ´i nghiÃªn cá»©u áº£nh hÆ°á»Ÿng cá»§a ngÆ°á»¡ng chÃº Ã½ Ï„ Ä‘Æ°á»£c sá»­ dá»¥ng trong PhÆ°Æ¡ng trÃ¬nh (1).

5.1. Hiá»‡u quáº£ trÃªn CÃ¡c Kiáº¿n trÃºc Äa dáº¡ng
BÃªn cáº¡nh RVT vÃ  FAN, chÃºng tÃ´i bá»• sung Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p cá»§a mÃ¬nh trÃªn Ä‘á»‰nh nhiá»u kiáº¿n trÃºc transformer hÆ¡n, bao gá»“m DeiT [43] vÃ  Swin [29]. Trong thÃ­ nghiá»‡m nÃ y, chÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c trÃªn ImageNet vÃ  Ä‘á»™ bá»n vá»¯ng vá» máº·t mCE (tháº¥p hÆ¡n lÃ  tá»‘t hÆ¡n) trÃªn ImageNet-C. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 5, dá»±a trÃªn DeiT-B, chÃºng tÃ´i cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘á»™ bá»n vá»¯ng vá»›i lá»—i báº±ng cÃ¡ch giáº£m mCE 1,9% vÃ  mang láº¡i cáº£i thiá»‡n Ä‘Ã¡ng há»©a háº¹n 0,4% trÃªn dá»¯ liá»‡u sáº¡ch. Äá»‘i vá»›i Swin-B, chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c quan sÃ¡t tÆ°Æ¡ng tá»± ráº±ng cÃ¡c phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i Ä‘áº·c biá»‡t hiá»‡u quáº£ trong viá»‡c cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng vá»›i lá»—i, giáº£m mCE tá»« 54,4% xuá»‘ng 51,9%. Nhá»¯ng káº¿t quáº£ nÃ y cho tháº¥y cÃ¡c phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³ thá»ƒ tá»•ng quÃ¡t hÃ³a tá»‘t qua cÃ¡c kiáº¿n trÃºc Ä‘a dáº¡ng.

PhÆ°Æ¡ng phÃ¡p ImageNet ImageNet-C (mCE) â†“
DeiT-B [43] 82.0 48.5
+ TAP & ADL 82.4 (+0.4) 46.6 (-1.9)
Swin-B [29] 83.4 54.4
+ TAP & ADL 84.0 (+0.6) 51.9 (-2.5)
RVT-B [32] 82.6 46.8
+ TAP & ADL 83.1 (+0.5) 44.7 (-2.1)
FAN-B-Hybrid [67] 83.9 46.1
+ TAP & ADL 84.3 (+0.4) 43.7 (-2.4)

Báº£ng 5. Káº¿t quáº£ trÃªn Ä‘á»‰nh cÃ¡c kiáº¿n trÃºc Ä‘a dáº¡ng. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c vÃ  mean corruption error (mCE) trÃªn ImageNet vÃ  ImageNet-C, tÆ°Æ¡ng á»©ng. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i luÃ´n cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng vÃ  Ä‘á»™ chÃ­nh xÃ¡c qua cÃ¡c kiáº¿n trÃºc khÃ¡c nhau.

5.2. TÃ¡c Ä‘á»™ng cá»§a SiÃªu tham sá»‘ K vÃ  Î»
ChÃºng tÃ´i thá»±c hiá»‡n ablation trÃªn ImageNet-C Ä‘á»ƒ nghiÃªn cá»©u tÃ¡c Ä‘á»™ng cá»§a hai siÃªu tham sá»‘ cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i, bao gá»“m sá»‘ nhÃ¡nh K trong TAP vÃ  trá»ng sá»‘ cá»§a ADL.

Sá»‘ nhÃ¡nh K. NhÆ° Ä‘Æ°á»£c chi tiáº¿t trong HÃ¬nh 2, chÃºng tÃ´i xÃ¢y dá»±ng TAP vá»›i K nhÃ¡nh Ä‘á»ƒ cho phÃ©p cÃ¡c token xem xÃ©t cÃ¡c vÃ¹ng pooling Ä‘a dáº¡ng. Thá»±c táº¿, giÃ¡ trá»‹ cá»§a K lÃ  má»™t yáº¿u tá»‘ quan trá»ng cho hiá»‡u suáº¥t cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i. ÄÃ¡ng chÃº Ã½ ráº±ng K= 1 vá» cÆ¡ báº£n tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i mÃ´ hÃ¬nh baseline khÃ´ng cÃ³ TAP. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 7 (trÃªn), chÃºng tÃ´i cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘á»™ bá»n vá»¯ng vá»›i Ä‘iá»ƒm mCE tháº¥p hÆ¡n trÃªn ImageNet-C khi tÄƒng dáº§n K tá»« 1 Ä‘áº¿n 4. Náº¿u chÃºng tÃ´i tiáº¿p tá»¥c tÄƒng K, viá»‡c há»c trá»ng sá»‘ cho quÃ¡ nhiá»u vÃ¹ng pooling á»©ng viÃªn (Ä‘á»™ giÃ£n) cho má»—i token trá»Ÿ nÃªn ngÃ y cÃ ng khÃ³ khÄƒn vÃ  chÃºng tÃ´i khÃ´ng thá»ƒ quan sÃ¡t Ä‘Æ°á»£c cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ. Máº·c dÃ¹ cÃ¡c nhÃ¡nh bá»• sung chá»‰ Ä‘Æ°a ra chi phÃ­ tá»‘i thiá»ƒu vá» kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n, K lá»›n cháº¯c cháº¯n sáº½ yÃªu cáº§u dung lÆ°á»£ng bá»™ nhá»› lá»›n hÆ¡n. Do Ä‘Ã³, chÃºng tÃ´i chá»n K= 4 Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t nháº¥t vá»›i chi phÃ­ bá»™ nhá»› bá»• sung tá»‘i thiá»ƒu.

--- TRANG 9 ---
123456
GiÃ¡ trá»‹ cá»§a K 44.0 44.5 45.0 45.5 46.0 46.5 mCE trÃªn ImageNet-C (Tháº¥p hÆ¡n lÃ  Tá»‘t hÆ¡n) FAN-B-Hybrid (Baseline)
Sá»­ dá»¥ng TAP (w/o ADL)

10-3 10-2 10-1 100 101
GiÃ¡ trá»‹ cá»§a Î» 44.0 44.5 45.0 45.5 46.0 46.5 mCE trÃªn ImageNet-C (Tháº¥p hÆ¡n lÃ  Tá»‘t hÆ¡n) FAN-B-Hybrid (Baseline)
Sá»­ dá»¥ng ADL (w/o TAP)

HÃ¬nh 7. Äá»™ bá»n vá»¯ng vá» máº·t mean corruption error (mCE, tháº¥p hÆ¡n lÃ  tá»‘t hÆ¡n) trÃªn ImageNet-C so vá»›i sá»‘ nhÃ¡nh K (trÃªn) vÃ  táº§m quan trá»ng cá»§a ADL loss Î» (dÆ°á»›i). TrÃªn: Khi chá»‰ giá»›i thiá»‡u TAP mÃ  khÃ´ng cÃ³ ADL, mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i luÃ´n vÆ°á»£t trá»™i hÆ¡n mÃ´ hÃ¬nh baseline khi tÄƒng giÃ¡ trá»‹ K vÃ  mang láº¡i káº¿t quáº£ tá»‘t nháº¥t vá»›i K= 4. DÆ°á»›i: Khi sá»­ dá»¥ng ADL Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh (khÃ´ng cÃ³ TAP), chÃºng tÃ´i quan sÃ¡t tháº¥y Î» quÃ¡ nhá» hoáº·c quÃ¡ lá»›n giáº£m lá»£i Ã­ch cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i. Trong thá»±c táº¿, Î»= 1 hoáº¡t Ä‘á»™ng tá»‘t nháº¥t trong háº§u háº¿t cÃ¡c trÆ°á»ng há»£p.

Trá»ng sá»‘ cá»§a ADL Î». Trong HÃ¬nh 7 (dÆ°á»›i), chÃºng tÃ´i thay Ä‘á»•i giÃ¡ trá»‹ Î» trong PhÆ°Æ¡ng trÃ¬nh (3). Trong thá»±c táº¿, Î» lá»›n hÆ¡n khuyáº¿n khÃ­ch cÃ¡c mÃ´ hÃ¬nh Ä‘a dáº¡ng hÃ³a sá»± chÃº Ã½ giá»¯a cÃ¡c hÃ ng khÃ¡c nhau trong báº£n Ä‘á»“ chÃº Ã½ má»™t cÃ¡ch tÃ­ch cá»±c hÆ¡n. Vá»›i má»™t táº­p há»£p giÃ¡ trá»‹ Î»âˆˆ {0.001, 0.01, 0.1, 1, 10}, chÃºng tÃ´i tÄƒng dáº§n Ä‘á»™ bá»n vá»¯ng (giáº£m Ä‘iá»ƒm mCE) cho Ä‘áº¿n Î»= 1. Khi xem xÃ©t Î»= 10 tháº­m chÃ­ lá»›n hÆ¡n, chÃºng tÃ´i quan sÃ¡t tháº¥y sá»± sá»¥t giáº£m hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ vÃ¬ Î» quÃ¡ lá»›n cho ADL cÃ³ thá»ƒ cáº£n trá»Ÿ loss cross-entropy tiÃªu chuáº©n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘áº·t Î»= 1 vÃ  nÃ³ tá»•ng quÃ¡t hÃ³a tá»‘t qua táº¥t cáº£ cÃ¡c kiáº¿n trÃºc vÃ  tÃ¡c vá»¥ há»c táº­p Ä‘Æ°á»£c xem xÃ©t.

5.3. Chiáº¿n lÆ°á»£c PhÃ¢n bá»• CÃ¡c lá»›p TAP
Trong pháº§n nÃ y, chÃºng tÃ´i Ä‘iá»u tra má»™t cÃ¡ch rÃµ rÃ ng cÃ¡ch phÃ¢n bá»• cÃ¡c lá»›p TAP vÃ o má»™t mÃ´ hÃ¬nh transformer. NhÆ° Ä‘Ã£ tháº£o luáº­n trong Pháº§n 3.2, TAP lÃ  má»™t mÃ´-Ä‘un cÃ³ thá»ƒ há»c Ä‘á»ƒ Ä‘iá»u chá»‰nh chÃ­nh nÃ³ cho phÃ¹ há»£p vá»›i cÃ¡c lá»›p khÃ¡c nhau. Cá»¥ thá»ƒ, khi táº­p trung quÃ¡ má»©c khÃ´ng nghiÃªm trá»ng, TAP cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£m thÃ nh identity mapping báº±ng cÃ¡ch Ä‘áº·t trá»ng sá»‘ cá»§a nhÃ¡nh Ä‘áº§u tiÃªn thÃ nh 1 trong HÃ¬nh 2. Vá» máº·t thá»±c nghiá»‡m, trong Báº£ng 6 (khÃ´ng cÃ³ ADL), khi giá»›i thiá»‡u TAP vÃ o cÃ¡c lá»›p nÃ´ng (30% lá»›p Ä‘áº§u tiÃªn) nÆ¡i táº­p trung quÃ¡ má»©c khÃ´ng nghiÃªm trá»ng, chÃºng tÃ´i quan sÃ¡t cáº£i thiá»‡n nhá» trÃªn ImageNet-C. Tuy nhiÃªn, Ä‘á»‘i vá»›i cÃ¡c lá»›p giá»¯a vÃ  sÃ¢u (70% lá»›p cÃ²n láº¡i vá»›i táº­p trung quÃ¡ má»©c), chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n lá»›n, tÆ°Æ¡ng tá»± nhÆ° viá»‡c phÃ¢n bá»• TAP Ä‘á»“ng Ä‘á»u. Äá»ƒ trÃ¡nh viá»‡c chá»n lá»›p thá»§ cÃ´ng, chÃºng tÃ´i Ä‘á» xuáº¥t phÃ¢n bá»• TAP vÃ o má»—i block.

PhÆ°Æ¡ng phÃ¡p Baseline TAP (Shallow) TAP (Middle+Deep) TAP (All)
ImageNet â†‘ 83.9 84.1 (+0.2) 84.3 (+0.4) 84.3 (+0.4)
ImageNet-C â†“ 46.1 45.7 (-0.4) 45.0 (-1.1) 44.9 (-1.2)

Báº£ng 6. So sÃ¡nh Ä‘á»™ chÃ­nh xÃ¡c trÃªn ImageNet vÃ  mCE (tháº¥p hÆ¡n lÃ  tá»‘t hÆ¡n) trÃªn ImageNet-C giá»¯a viá»‡c phÃ¢n bá»• TAP Ä‘á»“ng Ä‘á»u vÃ  khÃ´ng Ä‘á»“ng Ä‘á»u. Láº¥y FAN-B-Hybrid lÃ m baseline, viá»‡c phÃ¢n bá»• TAP vÃ o má»—i block mang láº¡i káº¿t quáº£ tá»‘t hÆ¡n so vá»›i cÃ¡c chiáº¿n lÆ°á»£c phÃ¢n bá»• khÃ´ng Ä‘á»“ng Ä‘á»u vá» cáº£ Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ bá»n vá»¯ng.

Ï„ 0 (Baseline) 1 2 3 5
ImageNet â†‘ 83.9 83.9 84.0 84.0 83.9
ImageNet-C (mCE) â†“ 46.1 45.1 (-1.0) 44.4 (-1.7) 45.5 (-0.6) 45.8 (-0.3)

Báº£ng 7. So sÃ¡nh Ä‘á»™ chÃ­nh xÃ¡c trÃªn ImageNet vÃ  mCE (tháº¥p hÆ¡n lÃ  tá»‘t hÆ¡n) trÃªn ImageNet-C qua cÃ¡c Ï„ Ä‘a dáº¡ng. ChÃºng tÃ´i láº¥y FAN-B-Hybrid lÃ m baseline vÃ  quan sÃ¡t tháº¥y Ï„ quÃ¡ nhá» hoáº·c quÃ¡ lá»›n giáº£m lá»£i Ã­ch cá»§a ADL. Trong thá»±c táº¿, Ï„= 2 hoáº¡t Ä‘á»™ng tá»‘t nháº¥t trong háº§u háº¿t cÃ¡c trÆ°á»ng há»£p.

5.4. áº¢nh hÆ°á»Ÿng cá»§a NgÆ°á»¡ng ChÃº Ã½ Ï„
Theo PhÆ°Æ¡ng trÃ¬nh (1), chÃºng tÃ´i sá»­ dá»¥ng ngÆ°á»¡ng Ï„/N vá»›i N lÃ  sá»‘ token Ä‘á»ƒ lá»c ra cÃ¡c giÃ¡ trá»‹ ráº¥t nhá» vÃ  chá»‰ táº­p trung vÃ o nhá»¯ng giÃ¡ trá»‹ quan trá»ng nháº¥t trong báº£n Ä‘á»“ chÃº Ã½ thÃ´ng qua Ë†A(l)i = 1(A(l)i â‰¥ Ï„/N) Â· A(l)i. á» Ä‘Ã¢y, chÃºng tÃ´i nghiÃªn cá»©u má»™t cÃ¡ch rÃµ rÃ ng áº£nh hÆ°á»Ÿng cá»§a ngÆ°á»¡ng chÃº Ã½ Ï„ Ä‘á»ƒ tÃ­nh ADL cá»§a chÃºng tÃ´i. NhÆ° Ä‘Æ°á»£c chi tiáº¿t trong Báº£ng 7, khi sá»­ dá»¥ng ADL Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh (khÃ´ng cÃ³ TAP), ADL cá»§a chÃºng tÃ´i chá»‰ mang láº¡i cáº£i thiá»‡n nhá» vá» hiá»‡u suáº¥t sáº¡ch trÃªn ImageNet. Tuy nhiÃªn, ADL cá»§a chÃºng tÃ´i trá»Ÿ nÃªn Ä‘áº·c biá»‡t hiá»‡u quáº£ trong viá»‡c cáº£i thiá»‡n Ä‘á»™ bá»n vá»¯ng, vÃ­ dá»¥ giáº£m Ä‘Ã¡ng ká»ƒ mCE trÃªn ImageNet-C. Trong thá»±c táº¿, Ï„ quÃ¡ nhá» hoáº·c quÃ¡ lá»›n giáº£m lá»£i Ã­ch cá»§a ADL. Trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, chÃºng tÃ´i Ä‘áº·t Ï„= 2 Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t nháº¥t.

6. Káº¿t luáº­n
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i giáº£i quyáº¿t váº¥n Ä‘á» táº­p trung quÃ¡ má»©c vÃ o token trong vision transformers (ViTs) sao cho ViTs cÃ³ xu hÆ°á»›ng dá»±a vÃ o ráº¥t Ã­t token quan trá»ng trong cÆ¡ cháº¿ chÃº Ã½. Thá»±c táº¿, sá»± chÃº Ã½ khÃ´ng bá»n vá»¯ng vÃ  thÆ°á»ng Ä‘áº¡t Ä‘Æ°á»£c cÃ¡c máº«u chÃº Ã½ khÃ¡c biá»‡t ráº¥t lá»›n khi cÃ³ máº·t lá»—i. Äá»ƒ giáº£m thiá»ƒu Ä‘iá»u nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t hai ká»¹ thuáº­t tá»•ng quÃ¡t. Äáº§u tiÃªn, mÃ´-Ä‘un Token-aware Average Pooling (TAP) cá»§a chÃºng tÃ´i khuyáº¿n khÃ­ch vÃ¹ng lÃ¢n cáº­n Ä‘á»‹a phÆ°Æ¡ng cá»§a cÃ¡c token tham gia vÃ o self-attention báº±ng cÃ¡ch há»c má»™t sÆ¡ Ä‘á»“ average pooling thÃ­ch á»©ng cho má»—i token. Thá»© hai, Attention Diversification Loss (ADL) cá»§a chÃºng tÃ´i giáº£m má»™t cÃ¡ch rÃµ rÃ ng Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine cá»§a sá»± chÃº Ã½ giá»¯a cÃ¡c token. Trong thá»±c táº¿, chÃºng tÃ´i Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p cá»§a mÃ¬nh cho cÃ¡c kiáº¿n trÃºc Ä‘a dáº¡ng vÃ  Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ vá» Ä‘á»™ bá»n vá»¯ng trÃªn cÃ¡c benchmark vÃ  tÃ¡c vá»¥ há»c táº­p khÃ¡c nhau.

--- TRANG 10 ---
TÃ i liá»‡u tham kháº£o
[1] Alaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Cross-covariance image transformers. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, 2021. 2

[2] Yutong Bai, Jieru Mei, Alan L Yuille, and Cihang Xie. Are transformers more robust than cnns? In Advances in Neural Information Processing Systems (NeurIPS), volume 34, 2021. 2

[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. 2

[4] Philipp Benz, Chaoning Zhang, Soomin Ham, Adil Karjauv, and I Kweon. Robustness comparison of vision transformer and mlp-mixer to cnns. In Proceedings of the CVPR 2021 Workshop on Adversarial Machine Learning in Real-World Computer Vision Systems and Online Challenges (AML-CV), pages 21â€“24, 2021. 2

[5] Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and Andreas Veit. Understanding robustness of transformers for image classification. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 10231â€“10241, 2021. 2

[6] Chun-Fu Chen, Rameswar Panda, and Quanfu Fan. Regionvit: Regional-to-local attention for vision transformers. In Proc. of the International Conference on Learning Representations (ICLR), 2021. 2

[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017. 7

[8] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv:1904.10509, 2019. 2

[9] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. Advances in Neural Information Processing Systems, 34:9355â€“9366, 2021. 2

[10] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-attention and convolutional layers. In Proc. of the International Conference on Learning Representations (ICLR), 2020. 2, 3

[11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 5, 7

[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 5

[13] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 12124â€“12134, 2022. 2

[14] StÃ©phane d'Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In Proc. of the International Conference on Machine Learning (ICML), pages 2286â€“2296. PMLR, 2021. 5

[15] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, and Qi Tian. Msg-transformer: Exchanging local spatial information by manipulating messenger tokens. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 12063â€“12072, 2022. 2

[16] Yonggan Fu, Shunyao Zhang, Shang Wu, Cheng Wan, and Yingyan Lin. Patch-fool: Are vision transformers always robust against adversarial perturbations? In Proc. of the International Conference on Learning Representations (ICLR), 2022. 1, 2

[17] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, HervÃ© JÃ©gou, and Matthijs Douze. Levit: a vision transformer in convnet's clothing for faster inference. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 12259â€“12269, 2021. 2, 4

[18] Jindong Gu, Volker Tresp, and Yao Qin. Are vision transformers robust to patch perturbations? In Proc. of the European Conference on Computer Vision (ECCV), pages 404â€“421. Springer, 2022. 1

[19] Yong Guo, David Stutz, and Bernt Schiele. Improving robustness by enhancing weak subnets. In European Conference on Computer Vision, pages 320â€“338. Springer, 2022. 1

[20] Yong Guo, David Stutz, and Bernt Schiele. Improving robustness of vision transformers by reducing sensitivity to patch corruptions. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4108â€“4118, 2023. 5

[21] Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, and Nhat Ho. Robustify transformers with robust kernel density estimation. arXiv.org, 2210.05794, 2022. 2

[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 6

[23] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340â€“8349, 2021. 5

[24] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In Proc. of the International Conference on Learning Representations (ICLR), 2019. 1, 5

[25] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu. Shuffle transformer: Rethinking

--- TRANG 11 ---
spatial shuffle for vision transformer. arXiv preprint arXiv:2106.03650, 2021. 2

[26] Salman H. Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. ACM Comput. Surv., 54(10s):200:1â€“200:41, 2022. 2

[27] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4804â€“4814, 2022. 2, 4

[28] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 12009â€“12019, 2022. 2

[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 10012â€“10022, 2021. 2, 5, 7, 8

[30] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11976â€“11986, 2022. 5

[31] Kaleel Mahmood, Rigel Mahmood, and Marten van Dijk. On the robustness of vision transformers to adversarial examples. In Proc. of the IEEE International Conference on Computer Vision (ICCV), 2021. 2

[32] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, and Hui Xue. Towards robust vision transformer. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3, 5, 8

[33] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking robustness in object detection: Autonomous driving when winter is coming. arXiv preprint arXiv:1907.07484, 2019. 5, 7

[34] Muhammad Muzammal Naseer, Kanchana Ranasinghe, Salman H Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. Advances in Neural Information Processing Systems (NeurIPS), 34:23296â€“23308, 2021. 2

[35] Haolin Pan, Yong Guo, Qinyi Deng, Haomin Yang, Jian Chen, and Yiqun Chen. Improving fine-tuning of self-supervised models with contrastive initialization. Neural Networks, 159:198â€“207, 2023. 2

[36] Zizheng Pan, Bohan Zhuang, Haoyu He, Jing Liu, and Jianfei Cai. Less is more: Pay less attention in vision transformers. In Proc. of the Conference on Artificial Intelligence (AAAI), volume 36, pages 2035â€“2043, 2022. 3

[37] Sayak Paul and Pin-Yu Chen. Vision transformers are robust learners. In Proc. of the Conference on Artificial Intelligence (AAAI), volume 36, pages 2071â€“2081, 2022. 2

[38] Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei Wang, Jianbin Jiao, and Qixiang Ye. Conformer: Local features coupling global representations for visual recognition. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 367â€“376, 2021. 2, 4

[39] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Acdc: The adverse conditions dataset with correspondences for semantic driving scene understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10765â€“10775, 2021. 5, 7

[40] Yucheng Shi, Yahong Han, Yu-an Tan, and Xiaohui Kuang. Decision-based black-box attack against vision transformers via patch-wise adversarial removal. Advances in Neural Information Processing Systems (NeurIPS), 35:12921â€“12933, 2022. 1

[41] Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, and Cho-Jui Hsieh. Robustness verification for transformers. In Proc. of the International Conference on Learning Representations (ICLR), 2020. 2

[42] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and HervÃ© JÃ©gou. Training data-efficient image transformers & distillation through attention. In Proc. of the International Conference on Machine Learning (ICML), pages 10347â€“10357. PMLR, 2021. 2

[43] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and HervÃ© JÃ©gou. Training data-efficient image transformers & distillation through attention. In Proc. of the International Conference on Machine Learning (ICML), 2021. 3, 8

[44] Jesse Vig. A multiscale visualization of attention in the transformer model. In Marta R. Costa-jussÃ  and Enrique Alfonseca, editors, Annual Meeting of the Association for Computational Linguistics (ACL), pages 37â€“42, 2019. 2

[45] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Annual Meeting of the Association for Computational Linguistics (ACL), 2019. 2

[46] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Standalone axial-attention for panoptic segmentation. In Proc. of the European Conference on Computer Vision (ECCV), pages 108â€“126. Springer, 2020. 2

[47] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 2

[48] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF international conference on computer vision, pages 568â€“578, 2021. 2

[49] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415â€“424, 2022. 2

[50] Wenxiao Wang, Lu Yao, Long Chen, Deng Cai, Xiaofei He, and Wei Liu. Crossformer: A versatile vision transformer

--- TRANG 12 ---
based on cross-scale attention. In Proc. of the International Conference on Learning Representations (ICLR), 2021. 2

[51] Yujing Wang, Yaming Yang, Jiangang Bai, Mingliang Zhang, Jing Bai, Jing Yu, Ce Zhang, Gao Huang, and Yunhai Tong. Evolving attention with residual convolutions. In Proc. of the International Conference on Machine Learning (ICML), pages 10971â€“10980. PMLR, 2021. 2

[52] Florian Wenzel, Andrea Dittadi, Peter Vincent Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow, David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, et al. Assaying out-of-distribution generalization in transfer learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 1

[53] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 22â€“31, 2021. 2, 4

[54] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr DollÃ¡r, and Ross Girshick. Early convolutions help transformers see better. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 30392â€“30400, 2021. 2, 4

[55] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems (NeurIPS), 34:12077â€“12090, 2021. 7

[56] Bingna Xu, Yong Guo, Luoqian Jiang, Mianjie Yu, and Jian Chen. Downscaled representation matters: Improving image rescaling with collaborative downscaled images. In Proc. of the IEEE International Conference on Computer Vision (ICCV), 2023. 1

[57] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 9981â€“9990, 2021. 2

[58] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021. 2

[59] Donggeun Yoo, Sunggyun Park, Joon-Young Lee, and In So Kweon. Multi-scale pyramid pooling for deep convolutional representation. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 71â€“80, 2015. 4

[60] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In Proc. of the International Conference on Learning Representations (ICLR), 2016. 7

[61] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 579â€“588, 2021. 2

[62] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 558â€“567, 2021. 2, 5

[63] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic segmentation on high-resolution images. In Proc. of the European Conference on Computer Vision (ECCV), pages 405â€“420, 2018. 7

[64] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. Proc. of the International Conference on Learning Representations (ICLR), 2018. 1, 5

[65] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6881â€“6890, 2021. 7

[66] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021. 2

[67] Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Anima Anandkumar, Jiashi Feng, and Jose M Alvarez. Understanding the robustness in vision transformers. In Proc. of the International Conference on Machine Learning (ICML), pages 27378â€“27394. PMLR, 2022. 1, 2, 3, 5, 7, 8
