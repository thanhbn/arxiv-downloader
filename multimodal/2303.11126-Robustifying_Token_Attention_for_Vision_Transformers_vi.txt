# 2303.11126.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2303.11126.pdf
# Kích thước tệp: 7789688 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Tăng cường Độ bền cho Cơ chế Chú ý Token trong Vision Transformers
Yong Guo, David Stutz, Bernt Schiele
Max Planck Institute for Informatics, Saarland Informatics Campus
guoyongcs@gmail.com, {david.stutz,schiele }@mpi-inf.mpg.de
Tóm tắt
Mặc dù thành công của vision transformers (ViTs), chúng
vẫn gặp phải sự sụt giảm đáng kể về độ chính xác khi có mặt
các lỗi thường gặp, như nhiễu hoặc mờ. Thú vị là,
chúng tôi quan sát thấy cơ chế chú ý của ViTs có xu hướng
dựa vào một số ít token quan trọng, một hiện tượng chúng tôi gọi là
tập trung quá mức vào token. Quan trọng hơn, những token này không
bền vững trước các lỗi, thường dẫn đến các mẫu chú ý khác biệt rất lớn.
Trong bài báo này, chúng tôi dự định giảm thiểu vấn đề tập trung quá mức này
và làm cho cơ chế chú ý ổn định hơn thông qua hai
kỹ thuật tổng quát: Đầu tiên, mô-đun Token-aware Average Pooling (TAP)
của chúng tôi khuyến khích vùng lân cận địa phương của
mỗi token tham gia vào cơ chế chú ý. Cụ thể,
TAP học các sơ đồ pooling trung bình cho mỗi token
sao cho thông tin của các token có khả năng quan trọng
trong vùng lân cận có thể được tính đến một cách thích ứng.
Thứ hai, chúng tôi buộc các token đầu ra tổng hợp thông tin
từ một tập hợp đa dạng các token đầu vào thay vì chỉ tập trung
vào một số ít bằng cách sử dụng Attention Diversification Loss (ADL)
của chúng tôi. Chúng tôi đạt được điều này bằng cách phạt độ tương đồng cosine
cao giữa các vector chú ý của các token khác nhau. Trong các thí nghiệm,
chúng tôi áp dụng các phương pháp của mình cho một loạt rộng các kiến trúc transformer
và cải thiện đáng kể độ bền vững. Ví dụ,
chúng tôi cải thiện độ bền vững với lỗi trên ImageNet-C
2,4% trong khi cải thiện độ chính xác 0,4% dựa trên
kiến trúc bền vững tiên tiến FAN. Ngoài ra, khi fine-tuning
các tác vụ phân đoạn ngữ nghĩa, chúng tôi cải thiện độ bền vững trên
CityScapes-C 2,4% và ACDC 3,0%. Mã nguồn của chúng tôi có sẵn
tại https://github.com/guoyongcs/TAPADL.

1. Giới thiệu
Mặc dù thành công của vision transformers (ViTs), hiệu suất
của chúng vẫn giảm đáng kể trên các lỗi hình ảnh thường gặp
như ImageNet-C [24, 52, 19], ví dụ đối kháng [18, 16, 40, 56],
và các ví dụ ngoài phân phối như được đánh giá trong ImageNet-A/R/P [64, 24].
Trong bài báo này, chúng tôi kiểm tra một thành phần chính của ViTs,
tức là cơ chế self-attention, để hiểu những sự sụt giảm hiệu suất này.
Thú vị là, chúng tôi phát hiện ra một hiện tượng chúng tôi gọi là tập trung quá mức vào token

Đầu vào FAN-B-Hybrid Của chúng tôi Sạch Nhiễu Gaussian
Hình 1. Tính ổn định chống lại lỗi hình ảnh về mặt trực quan hóa chú ý
(trái, một ma trận 196×196) và độ tương đồng cosine của
chú ý giữa các ví dụ sạch và bị lỗi (phải). Trái: Chúng tôi
tính trung bình các bản đồ chú ý qua các head khác nhau để trực quan hóa
và hiển thị kết quả của lớp cuối cùng. Chúng tôi quan sát thấy ViTs đặt
quá nhiều sự tập trung vào rất ít token, một hiện tượng chúng tôi gọi là
tập trung quá mức vào token. Quan trọng hơn, sự chú ý của mô hình
FAN cơ sở [16] dễ vỡ với các lỗi hình ảnh, ví dụ với nhiễu
Gaussian. Ngược lại, phương pháp của chúng tôi giảm thiểu sự tập trung quá mức vào token
và do đó cải thiện tính ổn định của sự chú ý chống lại lỗi.
Phải: Trên ImageNet, chúng tôi vẽ phân phối độ tương đồng cosine
qua tất cả các lớp (không tính trung bình head) giữa các ví dụ sạch và bị lỗi.
Chúng tôi cho thấy mô hình của chúng tôi đạt được điểm tương đồng cao hơn đáng kể
so với mô hình cơ sở.

cusing, trong đó chỉ có một số ít token quan trọng được cơ chế chú ý
dựa vào qua tất cả các head và lớp. Chúng tôi đưa ra giả thuyết rằng sự tập trung quá mức này
đặc biệt dễ vỡ trước các lỗi trên hình ảnh đầu vào và cản trở nghiêm trọng
độ bền vững của cơ chế chú ý.

Bắt đầu từ kiến trúc bền vững tiên tiến
FAN [67], chúng tôi điều tra ví dụ lớp chú ý cuối cùng
(xem chú ý của các lớp khác trong phụ lục). Cụ thể,
Hình 1 cho thấy một hình ảnh đầu vào sạch và một hình ảnh bị lỗi
cũng như các bản đồ chú ý tương ứng. Đây là các ma trận
N×N, với N là số token đầu vào/đầu ra. Ở đây, hàng thứ i
chỉ ra những token đầu vào (cột) mà token đầu ra thứ i "chú ý" đến
- màu đỏ đậm hơn cho biết điểm chú ý cao hơn. Sự tập trung quá mức vào token
có thể được định nghĩa một cách không chính thức bằng cách quan sát các đường thẳng đứng
rõ ràng trong bản đồ chú ý: Đầu tiên, mỗi token đầu ra (hàng)
chỉ tập trung vào một số ít token đầu vào quan trọng, bỏ qua hầu hết arXiv:2303.11126v3 [cs.CV] 6 Sep 2023

--- TRANG 2 ---
của thông tin khác. Thứ hai, tất cả token đầu ra dường như
tập trung vào cùng các token đầu vào, dẫn đến sự đa dạng rất thấp
giữa các vector chú ý trong các hàng khác nhau. Chúng tôi
nhấn mạnh rằng vấn đề tập trung quá mức có mặt trong toàn bộ
tập dữ liệu ImageNet, và cũng qua các kiến trúc đa dạng (xem thêm
ví dụ trong Hình 3 và phụ lục).

Hơn nữa, vấn đề tập trung quá mức này cũng đã được quan sát
trong các công trình hiện có. Ví dụ, Hình 5 của [10] quan sát
rất ít token quan trọng (màu đỏ đậm) trong các lớp 3 ∼6;
Hình 1 của [16] cũng báo cáo rất ít token quan trọng (màu vàng).
Quan trọng hơn, chúng tôi thấy rằng những token quan trọng này
cực kỳ dễ vỡ khi có mặt các lỗi thường gặp. Cụ thể,
khi áp dụng nhiễu Gaussian trên hình ảnh đầu vào, các token được
nhận dạng là quan trọng thay đổi hoàn toàn, xem Hình 1 (trái, cột thứ hai).
Về mặt định lượng, điều này có thể được nắm bắt bằng cách tính
độ tương đồng cosine giữa các bản đồ chú ý sạch và bị lỗi.
Không ngạc nhiên, như được hiển thị bởi hộp màu xanh trong Hình 1 (phải),
độ tương đồng cosine thực sự cực kỳ thấp, xác nhận giả thuyết
ban đầu của chúng tôi. Điều này thúc đẩy chúng tôi tăng cường độ bền của
sự chú ý bằng cách giảm thiểu vấn đề tập trung quá mức vào token.

Để làm điều này, chúng tôi khuyến khích mô-đun chú ý tập trung
vào các token đầu vào đa dạng. Điều này có thể đạt được bằng cách
thay đổi các mẫu trong cả cột và hàng của bản đồ chú ý,
điều này thúc đẩy hai thành phần chính của phương pháp chúng tôi.
Đầu tiên, khi so sánh các cột chú ý trong Hình 1, rất ít
token quan trọng, dẫn đến sự chú ý dễ vỡ. Để giải quyết điều này,
chúng tôi khuyến khích các token đầu ra không chỉ tập trung vào
các token đầu vào riêng lẻ mà còn tính đến vùng lân cận địa phương
xung quanh những token này, để làm cho nhiều token (cột) hơn
đóng góp thông tin có ý nghĩa. Một cách trực quan, một token riêng lẻ
bản thân nó có thể không quan trọng nhưng có thể được tăng cường
bằng cách tổng hợp thông tin từ các token có khả năng quan trọng
nằm trong vùng lân cận của nó. Chúng tôi đạt được điều này bằng cách
sử dụng một cơ chế pooling trung bình có thể học được áp dụng cho
mỗi token đầu vào để tổng hợp thông tin trước khi tính self-attention.
Thứ hai, khi so sánh các hàng chú ý trong Hình 1, hầu hết
token đầu ra tập trung vào cùng các token đầu vào. Một khi
những token này bị bóp méo, toàn bộ sự chú ý có thể thất bại.
Để giải quyết vấn đề này, chúng tôi tìm cách đa dạng hóa tập hợp
các token đầu vào (cột) mà các token đầu ra (hàng) dựa vào.
Chúng tôi đạt được điều này bằng cách sử dụng một loss phạt một cách
rõ ràng độ tương đồng cosine cao qua các hàng. Trong Hình 1,
sự kết hợp của những kỹ thuật này dẫn đến sự chú ý cân bằng hơn
qua các cột và sự chú ý đa dạng hơn qua các hàng. Quan trọng hơn,
những bản đồ chú ý này ổn định hơn trước các lỗi hình ảnh.
Một lần nữa, chúng tôi xác nhận định lượng điều này trên ImageNet
bằng cách sử dụng độ tương đồng cosine cao hơn đáng kể.

Nhìn chung, chúng tôi đưa ra ba đóng góp chính: 1) chúng tôi đề xuất
một mô-đun Token-aware Average Pooling (TAP) khuyến khích
vùng lân cận địa phương của các token tham gia vào cơ chế self-attention.
Cụ thể, chúng tôi thực hiện pooling trung bình để tổng hợp thông tin và kiểm soát nó bằng
cách điều chỉnh vùng pooling cho mỗi token. 2) Chúng tôi tiếp tục phát triển
một Attention Diversification Loss (ADL) để cải thiện
sự đa dạng của sự chú ý nhận được bởi các token khác nhau, tức là
các hàng trong bản đồ chú ý. Để làm điều này, chúng tôi tính
độ tương đồng chú ý giữa các hàng trong mỗi lớp và giảm thiểu
nó trong quá trình huấn luyện. 3) Chúng tôi nhấn mạnh rằng cả TAP và
ADL đều có thể được áp dụng trên đỉnh các kiến trúc transformer đa dạng.
Trong các thí nghiệm, các phương pháp đề xuất luôn
cải thiện độ bền vững của mô hình trên các benchmark ngoài phân phối
một cách đáng kể trong khi duy trì sự cải thiện cạnh tranh
về độ chính xác sạch cùng lúc. Ví dụ, chúng tôi cải thiện
độ bền vững chống lại lỗi và sự dịch chuyển phân phối trên
ImageNet-A/C/P/R ít nhất 1,5%, như được hiển thị trong Bảng 2.
Ngoài ra, sự cải thiện cũng tổng quát hóa tốt cho các tác vụ
downstream khác, ví dụ phân đoạn ngữ nghĩa, với sự cải thiện
2,4% trong Bảng 4.

2. Công trình liên quan
Hiệu suất đáng chú ý của ViTs trên các tác vụ học tập khác nhau
chủ yếu được gán cho việc sử dụng self-attention [26].
Tự nhiên, cơ chế self-attention đã được mở rộng
trong nhiều khía cạnh khác nhau, giải quyết các thiếu sót như
sự phụ thuộc nặng vào pre-training [42, 62, 35] hoặc độ phức tạp
tính toán cao [29, 28, 13, 1, 8, 3, 47] và thích ứng
kiến trúc với các tác vụ thị giác bằng cách xem xét các
transformer đa tỷ lệ [25, 15, 46, 48, 49, 9, 57, 53, 25, 50, 6, 58].
Tương tự như chúng tôi, một số công trình liên quan [66, 10, 44, 34, 45]
cũng điều tra và trực quan hóa self-attention để cải thiện
kiến trúc transformer, ví dụ để học các transformer sâu hơn [66]
hoặc cắt tỉa các head chú ý [45]. Tuy nhiên, theo hiểu biết
tốt nhất của chúng tôi, chúng tôi là những người đầu tiên báo cáo
vấn đề tập trung quá mức vào token và liên kết nó với
độ bền vững kém của ViTs.

Cũng có rất nhiều quan tâm trong việc hiểu và cải thiện
độ bền vững của ViTs [5, 2, 41, 37, 4, 21, 31]. Ví dụ,
[32] phát triển một robust vision transformer (RVT) bằng cách
kết hợp nhiều thành phần khác nhau để tăng cường độ bền vững,
bao gồm cải thiện scaling chú ý. FAN [67] kết hợp
token attention và channel attention [1] và có thể được xem là
tiên tiến. Cả hai phương pháp đều dựa vào các cơ chế
self-attention đã được sửa đổi và có thể được chứng minh là
gặp phải tập trung quá mức vào token. Do đó, các kỹ thuật của chúng tôi
có thể được chứng minh là cải thiện độ bền vững trên đỉnh RVT và FAN,
tương ứng.

Phương pháp TAP của chúng tôi cũng chia sẻ sự tương đồng với
các ý tưởng gần đây về việc cố gắng đưa tính địa phương vào
cơ chế self-attention [54, 53, 17, 38, 61, 54, 51]. Ví dụ,
CvT [53] đưa convolution vào kiến trúc self-attention để
tăng cường thông tin địa phương bên trong các token. MViTv2 [27]
khai thác average pooling để trích xuất đặc trưng địa phương và
cải thiện hiệu suất sạch. Tuy nhiên, tất cả những chiến lược này
sử dụng cùng một tổng hợp qua tất cả các token, trong khi
phương pháp của chúng tôi thích ứng chọn một vùng lân cận tổng hợp
cho mỗi token để cải thiện độ bền vững.

--- TRANG 3 ---
Nhánh 1 ℝ!×#×$ 
Đồng nhất (𝑑=0) AvgPool3x3 (𝑑=1) AvgPool3x3 (𝑑=2) Tổng có trọng số
⋯
Token đầu vào Conv3x3 Conv3x3 Softmax
Bộ dự đoán độ giãn ℝ!×#×% ℝ!×#×% Token-aware Average Pooling (TAP) 𝑁𝑁 𝐴=Softmax 𝑄𝐾!𝐶 Value ×=𝑁𝐶𝑁𝐶 Patch Embedding Linear Linear Conv3x3 Đầu ra 𝐿× ViT Blocks
Feed-Forward Network Self-Attention
FC + Softmax Đầu vào Token-aware Average Pooling (TAP) Kiến trúc tổng thể
Nhánh 2 Nhánh 3 Nhánh 𝐾 AvgPool3x3 (𝑑=𝐾-1)
𝑊𝐻𝑁=𝐻𝑊: tổng số token 𝐶: chiều của đặc trưng ℝ!×#×$ 𝑑: độ giãn 𝐻 & 𝑊: kích thước không gian của token

Hình 2. Mô-đun Token-aware Average Pooling (TAP) được đề xuất (trái) và kiến trúc tổng thể (phải). Trái: Trong TAP, chúng tôi giới thiệu K nhánh để cho phép các token xem xét các vùng pooling khác nhau và tính tổng có trọng số trên chúng. Bên cạnh đó, chúng tôi xây dựng một bộ dự đoán độ giãn nhẹ để học các trọng số cho các nhánh khác nhau. Phải: Chúng tôi giới thiệu một lớp TAP vào mỗi block cơ bản để khuyến khích nhiều token tham gia tích cực vào cơ chế self-attention tiếp theo.

3. Robust Token Self-Attention
Trong phần sau, chúng tôi tập trung vào cơ chế chú ý
trong ViTs, nhằm cải thiện độ bền vững tổng thể của chúng. Để bắt đầu,
chúng tôi đầu tiên mô tả vấn đề tập trung quá mức vào token đã quan sát
trong đó ViTs tập trung vào rất ít nhưng không ổn định token
chi tiết trong Phần 3.1. Sau đó, chúng tôi đề xuất hai kỹ thuật
tổng quát để giảm thiểu vấn đề này: Đầu tiên, trong Phần 3.2,
chúng tôi đề xuất một mô-đun Token-aware Average Pooling (TAP)
khuyến khích vùng lân cận của các token tham gia vào
cơ chế chú ý. Điều này đạt được bằng cách sử dụng một vùng
pooling có thể học như được minh họa trong Hình 2. Thứ hai,
trong Phần 3.3, chúng tôi phát triển một Attention Diversification Loss (ADL)
mới để cải thiện sự đa dạng của các mẫu chú ý qua các token.
Cả hai phương pháp đều có thể được áp dụng cho hầu hết các kiến trúc
transformer và chúng tôi sẽ cho thấy chúng cải thiện đáng kể
độ bền vững với chi phí huấn luyện không đáng kể.

3.1. Tập trung quá mức vào Token
Như được minh họa trong Hình 1, chúng tôi có thể trực quan hóa
cơ chế self-attention trong mỗi lớp dưới dạng ma trận chú ý N×N.
Ở đây, N là số token đầu vào và đầu ra và
mỗi mục (i, j) biểu thị sự chú ý mà token đầu ra thứ i
(hàng thứ i) đặt lên token đầu vào thứ j (cột thứ j)
– màu đỏ đậm hơn biểu thị điểm chú ý cao hơn. Để xử lý
multi-head self-attention, chúng tôi trực quan hóa ma trận này bằng cách
tính trung bình qua các head chú ý.

Như baseline, Hình 1 nhấn mạnh kiến trúc FAN [67] gần đây,
hiển thị bản đồ chú ý của lớp cuối cùng làm ví dụ. Chúng tôi quan sát thấy sự chú ý thường rất
thưa thớt trong các cột, có nghĩa là hầu hết các token đầu vào không được
chú ý đến và rất ít token được tập trung quá mức. Quan trọng hơn,
những token "quan trọng" này thường tương tự qua các
token đầu ra (hàng). Chúng tôi gọi hiện tượng này là tập trung quá mức vào token.
Điều này dẫn đến vấn đề khi đối mặt với lỗi như nhiễu Gaussian:
tập hợp các token quan trọng có thể thay đổi hoàn toàn (Hình 1, cột thứ hai).
Điều này có thể được hiểu là các token ban đầu không nắm bắt
thông tin bền vững. Chúng tôi cũng có thể nắm bắt định lượng
sự không ổn định này bằng cách tính độ tương đồng cosine giữa
các bản đồ chú ý sạch và bị lỗi qua tất cả các hình ảnh ImageNet.
Như được hiển thị bởi hộp màu xanh trong Hình 1 (phải), mô hình
baseline đạt được độ tương đồng cosine rất thấp, cho thấy
độ bền vững kém của self-attention tiêu chuẩn.

Chúng tôi thấy rằng hiện tượng này tồn tại qua các kiến trúc
đa dạng, ví dụ DeiT [43] và RVT [32], và các tác vụ đa dạng,
bao gồm cả phân loại hình ảnh và phân đoạn ngữ nghĩa (xem
ví dụ trực quan trong phụ lục). Hơn nữa, chúng tôi nhấn mạnh
rằng quan sát của chúng tôi phù hợp với các công trình hiện có [10, 36].
Cụ thể, những công trình này quan sát thấy các lớp giữa và sâu
(thường có vấn đề tập trung quá mức) có xu hướng nắm bắt
thông tin toàn cục nhưng đặt sự tập trung vào rất ít token,
ví dụ rất ít token màu đỏ đậm trong Hình 5 của [10] và
Hình 3 (Giai đoạn 4) trong [36]. Điều này tiếp tục chứng minh
sự tồn tại và tầm quan trọng của vấn đề tập trung quá mức vào token.
Để giảm thiểu vấn đề này, chúng tôi đề xuất hai kỹ thuật tổng quát
để tăng cường độ bền của self-attention trong phần còn lại của phần này.

--- TRANG 4 ---
3.2. Token-aware Average Pooling (TAP)
Trong phần đầu tiên của phương pháp, chúng tôi tìm cách khuyến khích
nhiều token đầu vào tham gia vào cơ chế self-attention,
tức là đạt được nhiều cột có điểm cao hơn trong
bản đồ chú ý. Để làm điều này, chúng tôi khuyến khích mỗi token đầu vào
tổng hợp một cách rõ ràng thông tin hữu ích từ vùng lân cận
địa phương của nó, trong trường hợp token đó không chứa
thông tin quan trọng. Phương pháp này được chứng minh bởi
các công trình hiện có [53, 38, 27] và quan sát rằng việc đưa
bất kỳ tổng hợp địa phương nào trước self-attention luôn
cải thiện độ bền vững, xem Bảng 1 (cột cuối). Thực tế,
những phương pháp này áp dụng một kernel convolution cố định hoặc
vùng pooling cho tất cả các token. Tuy nhiên, các token thường
khác nhau và mỗi token nên yêu cầu một chiến lược
tổng hợp địa phương cụ thể. Điều này thúc đẩy chúng tôi thích ứng
chọn kích thước vùng lân cận và chiến lược tổng hợp phù hợp.

Được truyền cảm hứng từ điều này, chúng tôi cho phép mỗi token
chọn một vùng/chiến lược thích hợp để thực hiện tổng hợp địa phương.
Cụ thể, chúng tôi phát triển một Token-aware Average Pooling (TAP)
thực hiện average pooling và thích ứng điều chỉnh
vùng pooling cho mỗi token. Như được hiển thị trong Hình 2,
chúng tôi khai thác một cấu trúc đa nhánh tính tổng có trọng số
trên nhiều nhánh, mỗi nhánh có một vùng pooling cụ thể.
Thay vì đơn giản thay đổi kích thước kernel tương tự như [59],
TAP thay đổi độ giãn để điều chỉnh vùng pooling.
Quan sát chính đằng sau điều này là average pooling với
kernel lớn, không có độ giãn, dẫn đến sự chồng chéo cực kỳ
lớn giữa các vùng pooling lân cận và do đó dư thừa nghiêm trọng
trong các token đầu ra. Ví dụ, điều này có thể được thấy trong
Bảng 1 nơi AvgPool5x5 gây ra sự giảm lớn về độ chính xác
sạch khoảng 1,2%. Tương tự như [54, 53, 17, 38], chúng tôi cũng
điều tra việc sử dụng các convolution có thể học, nhưng chỉ quan sát
được cải thiện nhỏ cùng với sự gia tăng đáng kể chi phí tính toán.

Dựa trên những quan sát này, chúng tôi xây dựng TAP dựa trên
average pooling với các độ giãn đa dạng: Không mất tính tổng quát,
với K nhánh, chúng tôi xem xét độ giãn trong phạm vi d∈[0, K−1].
Ở đây, d= 0 ngụ ý identity mapping không có bất kỳ tính toán nào,
tức là không có tổng hợp địa phương. Độ giãn tối đa được xác định
bởi K là một siêu tham số và chúng tôi thấy rằng hiệu suất và
cải thiện độ bền vững bão hòa ở khoảng K= 5 (xem Hình 7 trên).
Trong phạm vi độ giãn được phép, phương pháp của chúng tôi bao gồm
một bộ dự đoán độ giãn nhẹ để dự đoán độ giãn nào
(tức là nhánh nào trong Hình 2) để sử dụng. Lưu ý rằng
điều này cũng có thể là một kết hợp có trọng số của nhiều d.
Chúng tôi nhấn mạnh rằng bộ dự đoán này rất hiệu quả vì nó
giảm chiều đặc trưng (từ C đến K trong Hình 2) sao cho
nó thêm chi phí tính toán và tham số mô hình tối thiểu.
Cùng một phương pháp cũng có thể được áp dụng cho average-pooling
không giãn nơi d kiểm soát kích thước kernel, được gọi là
TAP (multi-kernel). Từ Bảng 1, TAP của chúng tôi vượt trội
đáng kể so với biến thể này, cho thấy hiệu quả của việc
sử dụng độ giãn cho pooling.

Mô hình #FLOPs (G) #Params (M) ImageNet ImageNet-C ↓
Baseline (FAN-B-Hybrid) 11.7 50.4 83.9 46.1
+ AvgPool3x3 11.7 50.4 83.6 45.6 (-0.5)
+ AvgPool5x5 11.7 50.4 82.7 45.5 (-0.6)
+ Conv3x3 17.3 79.4 84.0 45.9 (-0.2)
+ Conv5x5 27.4 130.7 84.4 45.8 (-0.3)
+ TAP (multi-kernel) 11.8 50.7 84.1 45.5 (-0.6)
+ TAP (Của chúng tôi) 11.8 50.7 84.3 44.9 (-1.2)

Bảng 1. So sánh các phương pháp tổng hợp địa phương dựa trên
FAN-B-Hybrid. Chúng tôi cho thấy việc thực hiện average pooling cho tất cả
các token cải thiện độ bền vững nhưng cản trở độ chính xác sạch.
Việc đưa convolution vào mỗi block tăng đáng kể độ phức tạp mô hình.
Ngoài ra, chúng tôi cũng so sánh một biến thể của TAP,
tức là TAP (multi-kernel), xem xét nhiều kích thước kernel
cho pooling và học trọng số cho mỗi nhánh. TAP của chúng tôi
vượt trội đáng kể so với biến thể này, cho thấy hiệu quả
của việc sử dụng độ giãn. Hơn nữa, TAP mang lại sự cân bằng
tốt nhất giữa độ chính xác và độ bền vững cùng với chi phí
tính toán không đáng kể.

3.3. Attention Diversification Loss (ADL)
Trong phần thứ hai của phương pháp, chúng tôi tìm cách cải thiện
sự đa dạng của sự chú ý qua các token đầu ra, tức là khuyến khích
các hàng khác nhau trong Hình 1 chú ý đến các token đầu vào khác nhau.
Dựa trên mục tiêu này, chúng tôi đề xuất một Attention Diversification Loss (ADL)
giảm một cách rõ ràng độ tương đồng cosine của sự chú ý
giữa các token đầu ra khác nhau (hàng). Tuy nhiên, để phương pháp này
hoạt động, có một số thách thức cần vượt qua. Đầu tiên,
việc tính độ tương đồng cosine giữa các sự chú ý khó khăn về mặt số học.
Ví dụ, nếu hai hàng (tức là token đầu ra) có các mẫu chú ý
rất riêng biệt, chúng ta mong đợi độ tương đồng cosine thấp gần 0.
Tuy nhiên, ngay cả đối với các token không được chú ý đến,
điểm chú ý sẽ không bằng không. Đối với N lớn, việc tính
tích vô hướng và cộng những giá trị này có xu hướng dẫn đến
độ tương đồng cosine cao hơn đáng kể so với không. Để giảm thiểu
vấn đề này, chúng tôi khai thác một thủ thuật ngưỡng để lọc ra
những giá trị rất nhỏ và chỉ tập trung vào những giá trị quan trọng nhất.
Gọi 1(·) là hàm chỉ thị, và A(l)i là vector chú ý
của token thứ i (hàng) trong lớp thứ l. Chúng tôi đưa ra
một ngưỡng τ (xem ablation trong Bảng 7) phụ thuộc vào
số token N, tức là τ/N. Do đó, sự chú ý sau
ngưỡng trở thành

ˆA(l)i = 1(A(l)i ≥ τ/N) · A(l)i. (1)

Thứ hai, để tránh độ phức tạp bậc hai của việc tính
độ tương đồng giữa các cặp N hàng, chúng tôi xấp xỉ
nó bằng cách tính độ tương đồng cosine giữa mỗi
vector chú ý cá nhân ˆA(l)i với chú ý trung bình ¯A(l) := 
1/N ∑Ni=1 ˆA(l)i. Khi xem xét một mô hình với L lớp, chúng tôi
tính trung bình ADL loss qua tất cả các lớp bằng:

LADL = 1/L ∑Ll=1 L(l)ADL, L(l)ADL = 1/N ∑Ni=1 (ˆA(l)i · ¯A(l))/(∥ˆA(l)i∥ ∥¯A(l)∥). (2)

--- TRANG 5 ---
Phương pháp #Params (M) #FLOPs (G) ImageNet ↑ImageNet-C ↓ImageNet-P ↓ImageNet-A ↑ImageNet-R ↑
ConvNeXt-B [30] 88.6 15.4 83.8 46.8 - 36.7 51.3
ConViT-B [14] 86.5 17.7 82.4 46.9 32.2 29.0 48.4
Swin-B [29] 87.8 15.4 83.4 54.4 32.7 35.8 46.6
T2T-ViT t-24 [62] 64.1 15.0 82.6 48.0 31.8 28.9 47.9
RSPC (FAN-B-Hybrid) [20] 50.5 11.7 84.2 44.5 30.0 41.1 -
RVT-B [32] 91.8 17.7 82.6 46.8 31.9 28.5 48.7
+ TAP 92.1 17.9 83.0 (+0.4) 45.5 (-1.3) 30.6 (-1.3) 30.0 (+1.5) 49.4 (+0.7)
+ ADL 91.8 17.7 82.6 (+0.0) 45.2 (-1.6) 30.2 (-1.7) 30.8 (+2.3) 49.8 (+1.1)
+ TAP & ADL 92.1 17.9 83.1 (+0.5) 44.7 (-2.1) 29.6 (-2.3) 32.7 (+4.2) 50.2 (+1.5)
FAN-B-Hybrid [67] 50.4 11.7 83.9 46.1 31.3 39.6 52.7
+ TAP 50.7 11.8 84.3 (+0.4) 44.9 (-1.2) 30.3 (-1.0) 41.0 (+1.4) 53.9 (+1.2)
+ ADL 50.4 11.7 84.0 (+0.1) 44.4 (-1.7) 29.8 (-1.5) 41.4 (+1.8) 54.2 (+1.5)
+ TAP & ADL 50.7 11.8 84.3 (+0.4) 43.7 (-2.4) 29.2 (-2.1) 42.3 (+2.7) 54.6 (+1.9)

Bảng 2. So sánh trên ImageNet và các benchmark độ bền vững đa dạng. Chúng tôi báo cáo mean corruption error (mCE) trên ImageNet-C và mean flip rate (mFR) trên ImageNet-P. Đối với những chỉ số này, thấp hơn là tốt hơn. Hơn nữa, chúng tôi báo cáo trực tiếp độ chính xác trên ImageNet-A và ImageNet-R. Dựa trên hai baseline được xem xét, các mô hình của chúng tôi luôn cải thiện độ chính xác và độ bền vững trên các benchmark đa dạng.

Trong thực tế, chúng tôi kết hợp ADL với loss cross-entropy (CE) tiêu chuẩn và đưa ra một siêu tham số λ (xem ablation trong Hình 7) để kiểm soát tầm quan trọng của ADL:

L = LCE + λLADL. (3)

Chúng tôi nhấn mạnh rằng ADL có thể được áp dụng để tăng cường độ bền vững trên các tác vụ đa dạng, bao gồm phân loại hình ảnh và phân đoạn ngữ nghĩa (xem Bảng 2 và Bảng 4).

4. Thí nghiệm
Chúng tôi thực hiện các thí nghiệm mở rộng để xác minh phương pháp trên cả tác vụ phân loại hình ảnh và phân đoạn ngữ nghĩa. Trong Phần 4.1, chúng tôi đầu tiên huấn luyện các mô hình phân loại trên ImageNet [12] và chứng minh rằng các mô hình của chúng tôi đạt được cải thiện đáng kể trên các benchmark độ bền vững khác nhau, bao gồm ImageNet-A [64], ImageNet-C [24], ImageNet-R [23], và ImageNet-P [24]. Sau đó, trong Phần 4.2, chúng tôi lấy mô hình pre-trained tốt nhất và tiếp tục fine-tune nó trên Cityscapes [11] cho phân đoạn ngữ nghĩa. Trong thực tế, các mô hình của chúng tôi cải thiện đáng kể mIoU trên hai benchmark độ bền vững phổ biến, bao gồm Cityscapes-C [33] và ACDC [39], cùng với hiệu suất cạnh tranh trên dữ liệu sạch. Cả mã nguồn và các mô hình đã huấn luyện sẽ sớm có sẵn.

4.1. Kết quả trên Phân loại Hình ảnh
Trong thí nghiệm này, chúng tôi xây dựng phương pháp trên đỉnh hai kiến trúc bền vững tiên tiến: RVT [32] và FAN [67] với kích thước mô hình "Base", tức là RVT-B và FAN-B-Hybrid. Chúng tôi huấn luyện các mô hình trên ImageNet và đánh giá chúng trên một số benchmark độ bền vững. Chúng tôi tuân thủ chặt chẽ các cài đặt của RVT và FAN để huấn luyện. Cụ thể, chúng tôi huấn luyện các mô hình sử dụng cùng sơ đồ augmentation và áp dụng batch size 2048. Chúng tôi đặt learning rate thành 2×10−3 và huấn luyện tất cả các mô hình trong 300 epoch. Trong tất cả các thí nghiệm, theo mặc định, chúng tôi đặt K= 4 và λ= 1 để huấn luyện các mô hình của chúng tôi. Để đánh giá độ bền vững, chúng tôi xem xét một số benchmark độ bền vững, bao gồm ImageNet-A [64], ImageNet-C [24], ImageNet-R [23], và ImageNet-P [24]. Lưu ý rằng, chúng tôi báo cáo mean corruption error (mCE) trên ImageNet-C và mean flip rate (mFR) trên ImageNet-P. Đối với cả hai chỉ số, thấp hơn là tốt hơn. Về mặt thực nghiệm, chúng tôi chứng minh rằng việc sử dụng TAP hoặc ADL riêng lẻ đều có thể cải thiện độ bền vững. Khi kết hợp chúng lại, các mô hình của chúng tôi vượt trội hơn baseline một cách lớn hơn và sự cải thiện hiệu suất tổng quát hóa tốt cho các kiến trúc đa dạng (xem Bảng 5).

4.1.1 So sánh trên ImageNet
Như được hiển thị trong Bảng 2, so với các baseline mạnh RVT và FAN, các mô hình của chúng tôi luôn cải thiện độ bền vững trên ImageNet-C >2.1% và cũng mang lại cải thiện tương đương trên các benchmark độ bền vững khác, bao gồm ImageNet-A/R/SK. Hơn nữa, các mô hình của chúng tôi cũng đạt được cải thiện cạnh tranh về độ chính xác sạch trên ImageNet. Ví dụ, chúng tôi cải thiện độ chính xác >0.4% trên cả hai kiến trúc baseline được xem xét. Quan trọng hơn, chúng tôi nhấn mạnh rằng những cải thiện này chỉ đi kèm với chi phí tính toán không đáng kể về cả số tham số và số phép toán dấu phẩy động (FLOP). Ngoài ra, chúng tôi cũng báo cáo chi tiết corruption error trên từng loại lỗi riêng lẻ của ImageNet-C dựa trên FAN-B-Hybrid. Từ Bảng 3, mô hình tốt nhất của chúng tôi (kết hợp TAP và ADL lại) đạt được kết quả tốt nhất trên hầu hết các loại lỗi. Đáng chú ý là mô hình của chúng tôi đặc biệt hiệu quả chống lại lỗi nhiễu, ví dụ mang lại cải thiện lớn 6,25% trên lỗi nhiễu Gaussian. Nhìn chung, những thí nghiệm này cho thấy việc tăng cường độ bền cho sự chú ý luôn cải thiện độ bền vững qua các kiến trúc và benchmark khác nhau.

--- TRANG 6 ---
Phương pháp mCE Noise Blur Weather Digital
Gaussian Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Brightness Contrast Elastic Pixelate JPEG
FAN-B-Hybrid 46.2 40.12 39.27 36.80 51.58 63.96 47.53 54.98 40.24 43.96 36.98 36.68 34.17 61.59 53.25 51.86
+ TAP 44.9 36.02 36.26 34.16 52.72 65.07 45.73 54.90 39.75 42.39 35.68 37.38 33.21 62.75 48.85 49.46
+ ADL 44.3 35.61 35.55 33.51 50.80 64.27 45.47 54.47 38.06 40.46 37.92 36.99 32.70 61.45 47.78 49.00
+ TAP & ADL 43.7 33.87 34.24 32.04 51.29 61.51 44.76 54.39 38.14 40.12 35.27 36.43 32.25 62.12 46.78 49.55

Bảng 3. So sánh corruption error (thấp hơn là tốt hơn) trên từng loại lỗi riêng lẻ của ImageNet-C dựa trên FAN-B-Hybrid. Kết hợp TAP và ADL lại mang lại kết quả tốt nhất trên hầu hết các loại lỗi.

4.1.2 Tính ổn định Chú ý và Kết quả Trực quan hóa
Chúng tôi chứng minh rằng các mô hình của chúng tôi cải thiện đáng kể tính ổn định chú ý chống lại lỗi cả về mặt định tính và định lượng. Thú vị là, trong mỗi lớp, các mô hình của chúng tôi đạt được sự đa dạng chú ý cao hơn giữa các head chú ý khác nhau.

Tính ổn định chú ý. Trong Hình 3, chúng tôi đầu tiên trực quan hóa mức độ thay đổi của sự chú ý khi đối mặt với lỗi hình ảnh, ví dụ nhiễu Gaussian. Chúng tôi lấy FAN-B-Hybrid làm baseline và so sánh mô hình tốt nhất của chúng tôi với hai biến thể chỉ chứa TAP và ADL riêng lẻ. Qua các ví dụ đa dạng, mô hình baseline gây ra vấn đề tập trung quá mức vào token nghiêm trọng rằng nó đặt quá nhiều sự tập trung vào rất ít token và đi kèm với sự dịch chuyển chú ý đáng kể khi đối mặt với lỗi. Với sự giúp đỡ của mô-đun tổng hợp địa phương TAP, mô hình TAP của chúng tôi gán sự chú ý cho nhiều token xung quanh một số token quan trọng, giảm thiểu vấn đề tập trung quá mức vào token ở một mức độ nhất định. Tuy nhiên, chúng tôi vẫn quan sát thấy sự dịch chuyển chú ý giữa các ví dụ sạch và bị lỗi. Khi huấn luyện các mô hình với ADL, sự chú ý tuân theo một mẫu đường chéo sao cho các token tổng hợp thông tin từ những token khác trong khi giữ lại hầu hết thông tin từ chính nó. Chúng tôi nhấn mạnh rằng mẫu đường chéo này phần nào tương tự với học tập dư (residual learning) [22] mà học thêm một nhánh dư trong khi giữ đặc trưng không đổi bằng cách sử dụng một shortcut đồng nhất. Rõ ràng, sự chú ý trở nên ổn định hơn nhiều chống lại lỗi. Khi kết hợp TAP và ADL lại, chúng tôi tiếp tục khuyến khích mẫu đường chéo mở rộng trong một vùng địa phương. Bằng cách này, mỗi token đặt nhiều sự tập trung hơn vào vùng lân cận của nó ngoài chính nó và do đó đạt được đặc trưng mạnh hơn. Về mặt định lượng, chúng tôi cũng đánh giá tính ổn định chú ý bằng cách tính độ tương đồng cosine của sự chú ý giữa các ví dụ sạch và bị lỗi qua toàn bộ ImageNet. Từ Hình 5, TAP mang lại tính ổn định chú ý cao hơn so với mô hình baseline. Nhờ mẫu đường chéo trong sự chú ý, mô hình với ADL cải thiện đáng kể điểm tương đồng một cách lớn, cho thấy sự chú ý rất ổn định chống lại lỗi. Khi kết hợp cả TAP và ADL, chúng tôi có thể tiếp tục cải thiện tính ổn định chú ý.

Chú ý của mỗi head và sự đa dạng chú ý. Trong phần này, chúng tôi tiếp tục điều tra bản đồ chú ý trong mỗi head riêng lẻ. Như được hiển thị trong Hình 4, đối với mô hình baseline, các token quan trọng nhất luôn là những token quan trọng nhất trong hầu hết tất cả các head, dẫn đến sự đa dạng rất thấp

Đầu vào Baseline (FAN-B-Hybrid) TAP TAP + ADL ADL
Hình 3. So sánh các bản đồ chú ý giữa các mô hình khác nhau. So với mô hình baseline, TAP của chúng tôi giảm thiểu vấn đề tập trung quá mức bằng cách khuyến khích các token xung quanh những token quan trọng nhất có điểm chú ý cao hơn. Khi chỉ sử dụng ADL, chúng tôi đạt được một bản đồ chú ý tuân theo mẫu đường chéo, tức là giữ lại chính token đó trong khi tổng hợp thông tin từ các token khác. Rõ ràng, các hàng chú ý khác nhau với nhau, hoàn thành mục tiêu giảm độ tương đồng giữa các hàng. Khi kết hợp TAP và ADL lại, mẫu đường chéo được mở rộng thêm vào các khu vực lân cận nhờ mô-đun TAP.

của sự chú ý giữa các head khác nhau. Ngược lại, trong mô hình của chúng tôi, chỉ có hai head tuân theo mẫu đường chéo và sáu head còn lại có các mẫu chú ý khác nhau. Chúng tôi nhấn mạnh rằng sự chú ý của chúng tôi là sự kết hợp của cả bộ lọc địa phương và toàn cục w.r.t các head khác nhau. Cụ thể, hai head đường chéo này có thể được coi là bộ lọc địa phương để trích xuất thông tin địa phương vì mẫu đường chéo khuyến khích các token tổng hợp thông tin trong vùng lân cận địa phương của chúng. Đối với sáu head còn lại, sự chú ý được phân phối qua toàn bộ bản đồ và do đó chúng có thể được coi là bộ lọc toàn cục. Từ kết quả trực quan hóa trong Hình 4, mô hình của chúng tôi có sự đa dạng chú ý cao hơn giữa các head khác nhau. Để định lượng điều này, theo phương pháp đã thảo luận trước đó, chúng tôi trực tiếp tính độ tương đồng cosine của các bản đồ chú ý giữa bất kỳ hai head nào trong mỗi lớp (xem phương pháp tính toán chi tiết trong phụ

--- TRANG 7 ---
Head 1 Head 2 Head 3 Head 4 Head 5 Head 6 Head 7 Head 8 Đầu vào FAN-B-Hybrid Của chúng tôi Trung bình

Hình 4. Bản đồ chú ý của các head chú ý khác nhau trong lớp cuối cùng. Đối với mô hình baseline, các token quan trọng nhất thường được chia sẻ qua các head khác nhau. Trong mô hình của chúng tôi, hai head có sự chú ý với mẫu đường chéo và các head khác có các mẫu cụ thể để trích xuất các đặc trưng khác nhau, mang lại sự đa dạng chú ý cao hơn giữa các head (xem kết quả định lượng trong Phần 4.1.2).

Hình 5. Phân phối độ tương đồng cosine của các bản đồ chú ý trung gian giữa các ví dụ sạch và bị lỗi (ví dụ với nhiễu Gaussian) trên ImageNet. Chúng tôi chứng minh rằng TAP hoặc ADL đều có thể cải thiện tính ổn định của sự chú ý một cách độc lập. Khi kết hợp chúng lại, chúng tôi tiếp tục cải thiện tính ổn định/tương đồng của sự chú ý chống lại lỗi.

lục). Nói cách khác, điểm tương đồng càng thấp, sự đa dạng chú ý càng cao. Trong thực tế, mô hình baseline đạt được điểm tương đồng cao 0,63 khi tính trung bình qua toàn bộ ImageNet, cho thấy sự đa dạng chú ý rất thấp giữa các head khác nhau. Ngược lại, mô hình của chúng tôi mang lại độ tương đồng thấp hơn là 0,27, phù hợp với quan sát trước đó rằng mô hình của chúng tôi tạo ra sự chú ý đa dạng qua các head khác nhau.

4.2. Kết quả trên Phân đoạn Ngữ nghĩa
Trong phần này, chúng tôi tiếp tục áp dụng phương pháp cho các tác vụ phân đoạn ngữ nghĩa. Chúng tôi huấn luyện các mô hình trên Cityscapes [11] và đánh giá độ bền vững trên hai benchmark phổ biến, bao gồm Cityscapes-C [33] và ACDC [39]. Cụ thể, Cityscapes-C chứa 16 loại lỗi có thể được chia thành 4 danh mục: noise, blur, weather, và digital. ACDC thu thập các hình ảnh với điều kiện bất lợi, bao gồm đêm, sương mù, mưa, và tuyết. Trong bài báo này, chúng tôi báo cáo mIoU qua các tập dữ liệu đa dạng. Trong quá trình huấn luyện, chúng tôi tuân theo cùng cài đặt của SegFormer [55] để huấn luyện các mô hình của chúng tôi. Chúng tôi chứng minh rằng TAP và ADL của chúng tôi cũng tổng quát hóa tốt cho các tác vụ phân đoạn và cải thiện đáng kể độ bền vững.

Mô hình Cityscapes ↑ Cityscapes-C ↑ ACDC ↑
DeepLabv3+ (R101) [7] 77.1 39.4 41.6
ICNet [63] 65.9 28.0 -
DilatedNet [60] 68.6 30.3 -
Swin-T [29] 78.1 47.3 56.3
SETR [65] 79.5 63.1 60.2
Segformer-B5 [55] 82.4 65.8 62.0
FAN-B-Hybrid [67] 82.2 67.3 60.6
+ TAP 82.7 (+0.5) 69.2 (+1.9) 62.7 (+2.1)
+ ADL 82.4 (+0.2) 69.4 (+2.1) 63.1 (+2.5)
+ TAP & ADL 82.9 (+0.7) 69.7 (+2.4) 63.6 (+3.0)

Bảng 4. So sánh các mô hình phân đoạn ngữ nghĩa trên tập validation Cityscapes, Cityscapes-C, và tập test ACDC. Cả TAP và ADL của chúng tôi đều cải thiện đáng kể độ bền vững. Chúng tôi có thể tiếp tục cải thiện kết quả khi kết hợp TAP và ADL lại.

4.2.1 So sánh Định lượng
Như được hiển thị trong Bảng 4, so với mô hình baseline được xem xét, việc sử dụng TAP hoặc ADL riêng lẻ đều có thể cải thiện đáng kể độ bền vững trên Cityscapes-C và ACDC. Với sự giúp đỡ của mô-đun tổng hợp địa phương hiệu quả TAP, chúng tôi nhấn mạnh rằng chúng tôi cũng đạt được cải thiện đáng hứa hẹn 0,5% mIoU trên dữ liệu sạch. Khi kết hợp TAP và ADL lại, chúng tôi tiếp tục cải thiện hiệu suất và đạt được cải thiện lớn hơn 2,4% và 3,0% trên Cityscapes-C và ACDC, tương ứng. Hơn nữa, mô hình tốt nhất của chúng tôi cũng vượt trội đáng kể so với một số mô hình phân đoạn phổ biến với kích thước mô hình tương đương. Nhìn chung, những kết quả này cho thấy hai kỹ thuật đề xuất không chỉ hoạt động cho phân loại hình ảnh mà còn tổng quát hóa tốt cho các tác vụ phân đoạn ngữ nghĩa.

4.2.2 So sánh Trực quan
Trong phần này, chúng tôi so sánh kết quả trực quan hóa của các mặt nạ phân đoạn được dự đoán dựa trên các ví dụ của các benchmark độ bền vững đa dạng. Như được hiển thị trong Hình 6, đối với ví dụ đầu tiên với lỗi tuyết, mô hình baseline không thể phát hiện một vùng lớn đường (được đánh dấu bởi hộp màu đỏ

--- TRANG 8 ---
Hình ảnh tuyết trong Cityscapes-C FAN-B-Hybrid FAN-B-Hybrid (TAP & ADL) Ground Truth Hình ảnh

Hình ảnh đêm trong ACDC

Hình 6. So sánh trực quan kết quả phân đoạn. Khi đối mặt với lỗi hình ảnh hoặc điều kiện bất lợi, mô hình FAN-B-Hybrid baseline không thể phát hiện một số đối tượng quan trọng (ví dụ đường trong ví dụ đầu tiên) hoặc nhận dạng nhầm một phần xe thành người lái xe (trong ví dụ thứ hai). Ngược lại, mô hình của chúng tôi bền vững hơn nhiều chống lại những lỗi và điều kiện bất lợi này.

), điều này gây ra những rủi ro tiềm ẩn khi áp dụng trong một số ứng dụng thực tế, ví dụ lái xe tự động. Hơn nữa, trong ví dụ thứ hai với điều kiện đêm, mô hình baseline nhận dạng một phần xe thành người lái xe và đưa ra nhiều artifact trong mặt nạ được dự đoán. Ngược lại, mô hình của chúng tôi bền vững hơn nhiều và có thể phát hiện chính xác hầu hết các phần của đường và xe trong cả hai trường hợp. Chúng tôi nhấn mạnh rằng sự vượt trội về độ bền vững của chúng tôi có thể được quan sát trên hầu hết các ví dụ của các benchmark được xem xét. Vui lòng tham khảo thêm so sánh trực quan trong phụ lục.

5. Phân tích và Thảo luận
Trong phần sau, chúng tôi trình bày thêm các thí nghiệm ablation và thảo luận. Trong Phần 5.1, chúng tôi chứng minh rằng hai phương pháp đề xuất là những kỹ thuật tổng quát và có thể được áp dụng trên đỉnh các kiến trúc transformer đa dạng. Trong Phần 5.2, chúng tôi nghiên cứu tác động của số nhánh K trong TAP và trọng số của ADL trong loss huấn luyện. Trong Phần 5.3, chúng tôi điều tra chiến lược phân bổ các lớp TAP. Trong thực tế, việc phân bổ đồng đều TAP vào mỗi block mang lại kết quả tốt nhất. Trong Phần 5.4, chúng tôi nghiên cứu ảnh hưởng của ngưỡng chú ý τ được sử dụng trong Phương trình (1).

5.1. Hiệu quả trên Các Kiến trúc Đa dạng
Bên cạnh RVT và FAN, chúng tôi bổ sung áp dụng các phương pháp của mình trên đỉnh nhiều kiến trúc transformer hơn, bao gồm DeiT [43] và Swin [29]. Trong thí nghiệm này, chúng tôi báo cáo độ chính xác trên ImageNet và độ bền vững về mặt mCE (thấp hơn là tốt hơn) trên ImageNet-C. Như được hiển thị trong Bảng 5, dựa trên DeiT-B, chúng tôi cải thiện đáng kể độ bền vững với lỗi bằng cách giảm mCE 1,9% và mang lại cải thiện đáng hứa hẹn 0,4% trên dữ liệu sạch. Đối với Swin-B, chúng tôi đạt được quan sát tương tự rằng các phương pháp của chúng tôi đặc biệt hiệu quả trong việc cải thiện độ bền vững với lỗi, giảm mCE từ 54,4% xuống 51,9%. Những kết quả này cho thấy các phương pháp của chúng tôi có thể tổng quát hóa tốt qua các kiến trúc đa dạng.

Phương pháp ImageNet ImageNet-C (mCE) ↓
DeiT-B [43] 82.0 48.5
+ TAP & ADL 82.4 (+0.4) 46.6 (-1.9)
Swin-B [29] 83.4 54.4
+ TAP & ADL 84.0 (+0.6) 51.9 (-2.5)
RVT-B [32] 82.6 46.8
+ TAP & ADL 83.1 (+0.5) 44.7 (-2.1)
FAN-B-Hybrid [67] 83.9 46.1
+ TAP & ADL 84.3 (+0.4) 43.7 (-2.4)

Bảng 5. Kết quả trên đỉnh các kiến trúc đa dạng. Chúng tôi báo cáo độ chính xác và mean corruption error (mCE) trên ImageNet và ImageNet-C, tương ứng. Phương pháp của chúng tôi luôn cải thiện độ bền vững và độ chính xác qua các kiến trúc khác nhau.

5.2. Tác động của Siêu tham số K và λ
Chúng tôi thực hiện ablation trên ImageNet-C để nghiên cứu tác động của hai siêu tham số của các phương pháp chúng tôi, bao gồm số nhánh K trong TAP và trọng số của ADL.

Số nhánh K. Như được chi tiết trong Hình 2, chúng tôi xây dựng TAP với K nhánh để cho phép các token xem xét các vùng pooling đa dạng. Thực tế, giá trị của K là một yếu tố quan trọng cho hiệu suất của phương pháp chúng tôi. Đáng chú ý rằng K= 1 về cơ bản tương đương với mô hình baseline không có TAP. Như được hiển thị trong Hình 7 (trên), chúng tôi cải thiện đáng kể độ bền vững với điểm mCE thấp hơn trên ImageNet-C khi tăng dần K từ 1 đến 4. Nếu chúng tôi tiếp tục tăng K, việc học trọng số cho quá nhiều vùng pooling ứng viên (độ giãn) cho mỗi token trở nên ngày càng khó khăn và chúng tôi không thể quan sát được cải thiện đáng kể. Mặc dù các nhánh bổ sung chỉ đưa ra chi phí tối thiểu về kích thước mô hình và độ phức tạp tính toán, K lớn chắc chắn sẽ yêu cầu dung lượng bộ nhớ lớn hơn. Do đó, chúng tôi chọn K= 4 để đạt được kết quả tốt nhất với chi phí bộ nhớ bổ sung tối thiểu.

--- TRANG 9 ---
123456
Giá trị của K 44.0 44.5 45.0 45.5 46.0 46.5 mCE trên ImageNet-C (Thấp hơn là Tốt hơn) FAN-B-Hybrid (Baseline)
Sử dụng TAP (w/o ADL)

10-3 10-2 10-1 100 101
Giá trị của λ 44.0 44.5 45.0 45.5 46.0 46.5 mCE trên ImageNet-C (Thấp hơn là Tốt hơn) FAN-B-Hybrid (Baseline)
Sử dụng ADL (w/o TAP)

Hình 7. Độ bền vững về mặt mean corruption error (mCE, thấp hơn là tốt hơn) trên ImageNet-C so với số nhánh K (trên) và tầm quan trọng của ADL loss λ (dưới). Trên: Khi chỉ giới thiệu TAP mà không có ADL, mô hình của chúng tôi luôn vượt trội hơn mô hình baseline khi tăng giá trị K và mang lại kết quả tốt nhất với K= 4. Dưới: Khi sử dụng ADL để huấn luyện mô hình (không có TAP), chúng tôi quan sát thấy λ quá nhỏ hoặc quá lớn giảm lợi ích của phương pháp chúng tôi. Trong thực tế, λ= 1 hoạt động tốt nhất trong hầu hết các trường hợp.

Trọng số của ADL λ. Trong Hình 7 (dưới), chúng tôi thay đổi giá trị λ trong Phương trình (3). Trong thực tế, λ lớn hơn khuyến khích các mô hình đa dạng hóa sự chú ý giữa các hàng khác nhau trong bản đồ chú ý một cách tích cực hơn. Với một tập hợp giá trị λ∈ {0.001, 0.01, 0.1, 1, 10}, chúng tôi tăng dần độ bền vững (giảm điểm mCE) cho đến λ= 1. Khi xem xét λ= 10 thậm chí lớn hơn, chúng tôi quan sát thấy sự sụt giảm hiệu suất đáng kể vì λ quá lớn cho ADL có thể cản trở loss cross-entropy tiêu chuẩn trong quá trình huấn luyện. Trong bài báo này, chúng tôi đặt λ= 1 và nó tổng quát hóa tốt qua tất cả các kiến trúc và tác vụ học tập được xem xét.

5.3. Chiến lược Phân bổ Các lớp TAP
Trong phần này, chúng tôi điều tra một cách rõ ràng cách phân bổ các lớp TAP vào một mô hình transformer. Như đã thảo luận trong Phần 3.2, TAP là một mô-đun có thể học để điều chỉnh chính nó cho phù hợp với các lớp khác nhau. Cụ thể, khi tập trung quá mức không nghiêm trọng, TAP có thể được giảm thành identity mapping bằng cách đặt trọng số của nhánh đầu tiên thành 1 trong Hình 2. Về mặt thực nghiệm, trong Bảng 6 (không có ADL), khi giới thiệu TAP vào các lớp nông (30% lớp đầu tiên) nơi tập trung quá mức không nghiêm trọng, chúng tôi quan sát cải thiện nhỏ trên ImageNet-C. Tuy nhiên, đối với các lớp giữa và sâu (70% lớp còn lại với tập trung quá mức), chúng tôi đạt được cải thiện lớn, tương tự như việc phân bổ TAP đồng đều. Để tránh việc chọn lớp thủ công, chúng tôi đề xuất phân bổ TAP vào mỗi block.

Phương pháp Baseline TAP (Shallow) TAP (Middle+Deep) TAP (All)
ImageNet ↑ 83.9 84.1 (+0.2) 84.3 (+0.4) 84.3 (+0.4)
ImageNet-C ↓ 46.1 45.7 (-0.4) 45.0 (-1.1) 44.9 (-1.2)

Bảng 6. So sánh độ chính xác trên ImageNet và mCE (thấp hơn là tốt hơn) trên ImageNet-C giữa việc phân bổ TAP đồng đều và không đồng đều. Lấy FAN-B-Hybrid làm baseline, việc phân bổ TAP vào mỗi block mang lại kết quả tốt hơn so với các chiến lược phân bổ không đồng đều về cả độ chính xác và độ bền vững.

τ 0 (Baseline) 1 2 3 5
ImageNet ↑ 83.9 83.9 84.0 84.0 83.9
ImageNet-C (mCE) ↓ 46.1 45.1 (-1.0) 44.4 (-1.7) 45.5 (-0.6) 45.8 (-0.3)

Bảng 7. So sánh độ chính xác trên ImageNet và mCE (thấp hơn là tốt hơn) trên ImageNet-C qua các τ đa dạng. Chúng tôi lấy FAN-B-Hybrid làm baseline và quan sát thấy τ quá nhỏ hoặc quá lớn giảm lợi ích của ADL. Trong thực tế, τ= 2 hoạt động tốt nhất trong hầu hết các trường hợp.

5.4. Ảnh hưởng của Ngưỡng Chú ý τ
Theo Phương trình (1), chúng tôi sử dụng ngưỡng τ/N với N là số token để lọc ra các giá trị rất nhỏ và chỉ tập trung vào những giá trị quan trọng nhất trong bản đồ chú ý thông qua ˆA(l)i = 1(A(l)i ≥ τ/N) · A(l)i. Ở đây, chúng tôi nghiên cứu một cách rõ ràng ảnh hưởng của ngưỡng chú ý τ để tính ADL của chúng tôi. Như được chi tiết trong Bảng 7, khi sử dụng ADL để huấn luyện mô hình (không có TAP), ADL của chúng tôi chỉ mang lại cải thiện nhỏ về hiệu suất sạch trên ImageNet. Tuy nhiên, ADL của chúng tôi trở nên đặc biệt hiệu quả trong việc cải thiện độ bền vững, ví dụ giảm đáng kể mCE trên ImageNet-C. Trong thực tế, τ quá nhỏ hoặc quá lớn giảm lợi ích của ADL. Trong các thí nghiệm của chúng tôi, chúng tôi đặt τ= 2 để đạt được kết quả tốt nhất.

6. Kết luận
Trong bài báo này, chúng tôi giải quyết vấn đề tập trung quá mức vào token trong vision transformers (ViTs) sao cho ViTs có xu hướng dựa vào rất ít token quan trọng trong cơ chế chú ý. Thực tế, sự chú ý không bền vững và thường đạt được các mẫu chú ý khác biệt rất lớn khi có mặt lỗi. Để giảm thiểu điều này, chúng tôi đề xuất hai kỹ thuật tổng quát. Đầu tiên, mô-đun Token-aware Average Pooling (TAP) của chúng tôi khuyến khích vùng lân cận địa phương của các token tham gia vào self-attention bằng cách học một sơ đồ average pooling thích ứng cho mỗi token. Thứ hai, Attention Diversification Loss (ADL) của chúng tôi giảm một cách rõ ràng độ tương đồng cosine của sự chú ý giữa các token. Trong thực tế, chúng tôi áp dụng các phương pháp của mình cho các kiến trúc đa dạng và đạt được cải thiện đáng kể về độ bền vững trên các benchmark và tác vụ học tập khác nhau.

--- TRANG 10 ---
Tài liệu tham khảo
[1] Alaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Cross-covariance image transformers. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, 2021. 2

[2] Yutong Bai, Jieru Mei, Alan L Yuille, and Cihang Xie. Are transformers more robust than cnns? In Advances in Neural Information Processing Systems (NeurIPS), volume 34, 2021. 2

[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. 2

[4] Philipp Benz, Chaoning Zhang, Soomin Ham, Adil Karjauv, and I Kweon. Robustness comparison of vision transformer and mlp-mixer to cnns. In Proceedings of the CVPR 2021 Workshop on Adversarial Machine Learning in Real-World Computer Vision Systems and Online Challenges (AML-CV), pages 21–24, 2021. 2

[5] Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and Andreas Veit. Understanding robustness of transformers for image classification. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 10231–10241, 2021. 2

[6] Chun-Fu Chen, Rameswar Panda, and Quanfu Fan. Regionvit: Regional-to-local attention for vision transformers. In Proc. of the International Conference on Learning Representations (ICLR), 2021. 2

[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017. 7

[8] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv:1904.10509, 2019. 2

[9] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. Advances in Neural Information Processing Systems, 34:9355–9366, 2021. 2

[10] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-attention and convolutional layers. In Proc. of the International Conference on Learning Representations (ICLR), 2020. 2, 3

[11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 5, 7

[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 5

[13] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 12124–12134, 2022. 2

[14] Stéphane d'Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In Proc. of the International Conference on Machine Learning (ICML), pages 2286–2296. PMLR, 2021. 5

[15] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, and Qi Tian. Msg-transformer: Exchanging local spatial information by manipulating messenger tokens. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 12063–12072, 2022. 2

[16] Yonggan Fu, Shunyao Zhang, Shang Wu, Cheng Wan, and Yingyan Lin. Patch-fool: Are vision transformers always robust against adversarial perturbations? In Proc. of the International Conference on Learning Representations (ICLR), 2022. 1, 2

[17] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, and Matthijs Douze. Levit: a vision transformer in convnet's clothing for faster inference. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 12259–12269, 2021. 2, 4

[18] Jindong Gu, Volker Tresp, and Yao Qin. Are vision transformers robust to patch perturbations? In Proc. of the European Conference on Computer Vision (ECCV), pages 404–421. Springer, 2022. 1

[19] Yong Guo, David Stutz, and Bernt Schiele. Improving robustness by enhancing weak subnets. In European Conference on Computer Vision, pages 320–338. Springer, 2022. 1

[20] Yong Guo, David Stutz, and Bernt Schiele. Improving robustness of vision transformers by reducing sensitivity to patch corruptions. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4108–4118, 2023. 5

[21] Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, and Nhat Ho. Robustify transformers with robust kernel density estimation. arXiv.org, 2210.05794, 2022. 2

[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 6

[23] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340–8349, 2021. 5

[24] Dan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In Proc. of the International Conference on Learning Representations (ICLR), 2019. 1, 5

[25] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu. Shuffle transformer: Rethinking

--- TRANG 11 ---
spatial shuffle for vision transformer. arXiv preprint arXiv:2106.03650, 2021. 2

[26] Salman H. Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. ACM Comput. Surv., 54(10s):200:1–200:41, 2022. 2

[27] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4804–4814, 2022. 2, 4

[28] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 12009–12019, 2022. 2

[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 10012–10022, 2021. 2, 5, 7, 8

[30] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11976–11986, 2022. 5

[31] Kaleel Mahmood, Rigel Mahmood, and Marten van Dijk. On the robustness of vision transformers to adversarial examples. In Proc. of the IEEE International Conference on Computer Vision (ICCV), 2021. 2

[32] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, and Hui Xue. Towards robust vision transformer. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3, 5, 8

[33] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking robustness in object detection: Autonomous driving when winter is coming. arXiv preprint arXiv:1907.07484, 2019. 5, 7

[34] Muhammad Muzammal Naseer, Kanchana Ranasinghe, Salman H Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. Advances in Neural Information Processing Systems (NeurIPS), 34:23296–23308, 2021. 2

[35] Haolin Pan, Yong Guo, Qinyi Deng, Haomin Yang, Jian Chen, and Yiqun Chen. Improving fine-tuning of self-supervised models with contrastive initialization. Neural Networks, 159:198–207, 2023. 2

[36] Zizheng Pan, Bohan Zhuang, Haoyu He, Jing Liu, and Jianfei Cai. Less is more: Pay less attention in vision transformers. In Proc. of the Conference on Artificial Intelligence (AAAI), volume 36, pages 2035–2043, 2022. 3

[37] Sayak Paul and Pin-Yu Chen. Vision transformers are robust learners. In Proc. of the Conference on Artificial Intelligence (AAAI), volume 36, pages 2071–2081, 2022. 2

[38] Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei Wang, Jianbin Jiao, and Qixiang Ye. Conformer: Local features coupling global representations for visual recognition. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 367–376, 2021. 2, 4

[39] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Acdc: The adverse conditions dataset with correspondences for semantic driving scene understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10765–10775, 2021. 5, 7

[40] Yucheng Shi, Yahong Han, Yu-an Tan, and Xiaohui Kuang. Decision-based black-box attack against vision transformers via patch-wise adversarial removal. Advances in Neural Information Processing Systems (NeurIPS), 35:12921–12933, 2022. 1

[41] Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, and Cho-Jui Hsieh. Robustness verification for transformers. In Proc. of the International Conference on Learning Representations (ICLR), 2020. 2

[42] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In Proc. of the International Conference on Machine Learning (ICML), pages 10347–10357. PMLR, 2021. 2

[43] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In Proc. of the International Conference on Machine Learning (ICML), 2021. 3, 8

[44] Jesse Vig. A multiscale visualization of attention in the transformer model. In Marta R. Costa-jussà and Enrique Alfonseca, editors, Annual Meeting of the Association for Computational Linguistics (ACL), pages 37–42, 2019. 2

[45] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Annual Meeting of the Association for Computational Linguistics (ACL), 2019. 2

[46] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Standalone axial-attention for panoptic segmentation. In Proc. of the European Conference on Computer Vision (ECCV), pages 108–126. Springer, 2020. 2

[47] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 2

[48] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF international conference on computer vision, pages 568–578, 2021. 2

[49] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415–424, 2022. 2

[50] Wenxiao Wang, Lu Yao, Long Chen, Deng Cai, Xiaofei He, and Wei Liu. Crossformer: A versatile vision transformer

--- TRANG 12 ---
based on cross-scale attention. In Proc. of the International Conference on Learning Representations (ICLR), 2021. 2

[51] Yujing Wang, Yaming Yang, Jiangang Bai, Mingliang Zhang, Jing Bai, Jing Yu, Ce Zhang, Gao Huang, and Yunhai Tong. Evolving attention with residual convolutions. In Proc. of the International Conference on Machine Learning (ICML), pages 10971–10980. PMLR, 2021. 2

[52] Florian Wenzel, Andrea Dittadi, Peter Vincent Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow, David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, et al. Assaying out-of-distribution generalization in transfer learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 1

[53] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 22–31, 2021. 2, 4

[54] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, and Ross Girshick. Early convolutions help transformers see better. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 30392–30400, 2021. 2, 4

[55] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems (NeurIPS), 34:12077–12090, 2021. 7

[56] Bingna Xu, Yong Guo, Luoqian Jiang, Mianjie Yu, and Jian Chen. Downscaled representation matters: Improving image rescaling with collaborative downscaled images. In Proc. of the IEEE International Conference on Computer Vision (ICCV), 2023. 1

[57] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 9981–9990, 2021. 2

[58] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021. 2

[59] Donggeun Yoo, Sunggyun Park, Joon-Young Lee, and In So Kweon. Multi-scale pyramid pooling for deep convolutional representation. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 71–80, 2015. 4

[60] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In Proc. of the International Conference on Learning Representations (ICLR), 2016. 7

[61] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 579–588, 2021. 2

[62] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proc. of the IEEE International Conference on Computer Vision (ICCV), pages 558–567, 2021. 2, 5

[63] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic segmentation on high-resolution images. In Proc. of the European Conference on Computer Vision (ECCV), pages 405–420, 2018. 7

[64] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples. Proc. of the International Conference on Learning Representations (ICLR), 2018. 1, 5

[65] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6881–6890, 2021. 7

[66] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021. 2

[67] Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Anima Anandkumar, Jiashi Feng, and Jose M Alvarez. Understanding the robustness in vision transformers. In Proc. of the International Conference on Machine Learning (ICML), pages 27378–27394. PMLR, 2022. 1, 2, 3, 5, 7, 8
