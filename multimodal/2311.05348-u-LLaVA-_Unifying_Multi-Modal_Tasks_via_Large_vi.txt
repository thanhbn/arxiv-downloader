# u-LLaVA: Thống nhất các Nhiệm vụ Đa phương thức thông qua Mô hình Ngôn ngữ Lớn

Jinjin Xua, Liwu Xua, Yuzhe Yanga, Xiang Lia, Fanyi Wanga, Yanchun Xiea, Yi-Jie Huangaand Yaqian Lia,*
aOPPO AI Center

Tóm tắt. Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn đa phương thức (MLLMs) đã dẫn đến những cải tiến đáng kể trong hiểu biết thị giác, chủ yếu được thúc đẩy bởi các chiến lược căn chỉnh phương thức tinh vi. Tuy nhiên, các phương pháp chủ đạo ưu tiên hiểu biết toàn cục hoặc khu vực, với ít tập trung vào các nhiệm vụ mịn, ở cấp độ pixel. Để giải quyết khoảng cách này, chúng tôi giới thiệu u-LLaVA, một khung công việc đa nhiệm vụ thống nhất sáng tạo tích hợp các tính năng pixel, khu vực và toàn cục để tinh chỉnh khả năng nhận thức của MLLMs. Chúng tôi bắt đầu bằng cách tận dụng một phương pháp căn chỉnh phương thức hiệu quả, khai thác cả bộ dữ liệu hình ảnh và video để củng cố hiểu biết cơ bản của mô hình qua các bối cảnh thị giác đa dạng. Sau đó, một phương pháp điều chỉnh hướng dẫn kết hợp với các bộ chiếu và bộ giải mã cụ thể theo nhiệm vụ cho việc huấn luyện downstream từ đầu đến cuối được trình bày. Hơn nữa, công việc này đóng góp một bộ dữ liệu đa nhiệm vụ dựa trên mask mới bao gồm 277K mẫu, được tạo ra để thách thức và đánh giá khả năng nhận thức mịn của MLLMs. Khung công việc tổng thể đơn giản, hiệu quả và đạt được hiệu suất tiên tiến trên nhiều benchmark. Chúng tôi công khai mô hình, dữ liệu và mã tại https://github.com/OPPOMKLab/u-LLaVA.

1 Giới thiệu

Do những khó khăn nội tại liên quan đến trích xuất đặc trưng trong các nhiệm vụ thị giác máy tính (CV), các nhà nghiên cứu chủ yếu đã tập trung vào nhận thức hơn là nhận thức trong một thời gian dài. Sự nhấn mạnh này có tác động đáng kể đến sự phát triển và hiểu biết về các phương pháp CV khác nhau [29]. Mặc dù sự phát triển của mạng nơ-ron sâu và các kỹ thuật tiền huấn luyện đã giảm đáng kể khó khăn của nhận thức, việc đạt được tính đồng nhất qua các nhiệm vụ downstream vẫn là thách thức do sự khác biệt đáng kể trong các mục tiêu tương ứng của chúng. Gần đây, các mô hình ngôn ngữ lớn nhân quả như GPT [35, 36, 5], Gemini [44] và LLaMA [46] đã đạt được hoặc gần đạt được hiệu suất ở mức con người trên nhiều nhiệm vụ khác nhau. Những tiến bộ này cũng đã thúc đẩy các nhà nghiên cứu kết hợp LLMs như các thành phần [25, 66] hoặc yếu tố cốt lõi [44, 50] trong các nhiệm vụ thị giác, dẫn đến sự phát triển của các mô hình ngôn ngữ thị giác (VLMs), hoặc mô hình ngôn ngữ lớn đa phương thức (MLLMs). Kết quả là, các phương pháp này đã thu hút sự chú ý ngày càng tăng trong thời gian gần đây.

Thông thường, một MLLM đa phương thức bao gồm một hoặc nhiều bộ mã hóa để trích xuất đặc trưng, được ghép nối với các thành phần ánh xạ phù hợp (như MLP [25], Q-Former[66], hoặc cross-attention [2]), để căn chỉnh các phương thức khác với miền văn bản. So với hiệu suất ấn tượng của MLLMs trên các nhiệm vụ hiểu biết đa mục đích, như trả lời câu hỏi thị giác (VQA), khả năng của chúng trong các nhiệm vụ cấp khu vực và pixel có phần kém đáng kể hơn [25, 66]. Để đạt được hiểu biết cấp khu vực, thông thường là chuyển đổi tọa độ mục tiêu thành token cho mô hình ngôn ngữ nhân quả, như Shikra [7] và KOSMOS-2 [34]. Để thực hiện thêm hiểu biết cấp pixel, các bộ giải mã hoặc trích xuất cấp mask được giới thiệu, như LISA [17], Osprey [60] và Next-Chat [61]. Tuy nhiên, hiểu biết khu vực như vậy đòi hỏi dữ liệu mở rộng cho huấn luyện, kéo theo chi phí cao. Các phương pháp hiểu biết cấp pixel cung cấp sự linh hoạt hơn, nhưng kéo theo việc giới thiệu hoặc thiết kế các module phân đoạn cụ thể.

Trong bài báo này, chúng tôi đề xuất u-LLaVA, một phương pháp mới để nâng cao khả năng hiểu biết tổng quát, khu vực và thậm chí cấp pixel của MLLMs. Để đạt được điều này, trước tiên chúng tôi thiết kế một chiến lược căn chỉnh thị giác hiệu quả với biểu diễn hình ảnh và không gian-thời gian, và các bộ chiếu và bộ giải mã cụ thể theo nhiệm vụ được tích hợp để điều chỉnh hướng dẫn kết hợp. Đường ống tổng thể được minh họa trong Hình 1.

Để cho phép hiểu biết cấp pixel, chúng tôi sử dụng một bộ chiếu để kết nối MLLMs và SAM [17], đạt được hai mục tiêu: a) truyền đạt khả năng hiểu biết ngữ nghĩa cho SAM bằng cách tận dụng kiến thức thế giới vốn có trong LLM; và b) nâng cao khả năng hiểu biết cấp pixel của LLM bằng cách khai thác SAM. Để nâng cao hiệu suất của hiểu biết cấp khu vực, chúng tôi đã giới thiệu một bộ giải mã vị trí độc lập để giải mã tọa độ mục tiêu từ trạng thái ẩn hoặc đầu ra của MLLMs, điều này giảm đáng kể lượng dữ liệu cần thiết cho huấn luyện. Để phù hợp với việc huấn luyện các mô hình nêu trên, chúng tôi đã cẩn thận thiết kế một loạt các nhóm prompt liên quan đến nhiệm vụ, và giới thiệu một bộ dữ liệu dựa trên mask, cụ thể theo khu vực, tức là ullava-277K. Hầu hết dữ liệu đã được thu thập từ các bộ dữ liệu có sẵn công khai, với các chú thích bị thiếu được bổ sung cẩn thận bởi GPT-3.5.

Những đóng góp có thể được tóm tắt thành ba phần:

• Chúng tôi đề xuất một phương pháp căn chỉnh thị giác hiệu quả cho tiền huấn luyện đa phương thức, tận dụng hình ảnh (tính năng không gian) và video (tính năng không gian-thời gian) để nâng cao khả năng nhận thức của MLLMs.

• Chúng tôi lần đầu tiên giới thiệu phương pháp điều chỉnh hướng dẫn kết hợp trong cùng một giai đoạn để cho phép hiểu biết đa cấp với các bộ chiếu và bộ giải mã cụ thể theo nhiệm vụ, xem Bảng 1 để biết chi tiết.

• Chúng tôi phát hành bộ dữ liệu điều chỉnh hướng dẫn kết hợp, ullava-277K, mô hình và mã có sẵn công khai. Ngoài ra, chúng tôi tiến hành các thí nghiệm toàn diện và chứng minh hiệu quả của phương pháp đề xuất.

2 Công trình liên quan

2.1 MLLMs

Bất ngờ bởi những khả năng đáng kể của các mô hình ngôn ngữ lớn, các nhà nghiên cứu đã thể hiện sự quan tâm lớn trong việc chuyển giao các khả năng của LLM sang các lĩnh vực khác [58, 53]. Trong những tháng gần đây, tiến bộ đáng kể đã được thực hiện trong lĩnh vực này, như LLaVA [25], MiniGPT-4 [66], Otter [19], KOSMOS-1/2 [12, 34], mPLUG-owl [57] và Flamingo [1], v.v. Trong khi đã chứng minh hiệu suất ấn tượng trong hiểu biết cấp hình ảnh, các phương pháp này thể hiện khả năng hạn chế trên các nhiệm vụ cấp pixel hoặc khu vực.

2.2 Hiểu biết Cấp Khu vực

Hiểu biết biểu thức tham chiếu (REC) là một trong những nhiệm vụ hiểu biết cấp khu vực điển hình nhất, và RefCOCO [59], RefCOCO+ [59] và RefCOCOg [32], RefCLEF [15] là các bộ dữ liệu phổ biến cho REC. Gần đây, một số phương pháp đã sử dụng phương pháp pix2seq để đạt được hiểu biết khu vực [7, 34]. Một số chiến lược tiếp tục kết hợp mã hóa-giải mã khu vực [61], trong khi những phương pháp khác sử dụng các module bên ngoài để hoàn thành nhiệm vụ [63].

2.3 Hiểu biết Cấp Pixel

Sự xuất hiện của MLLMs đã giảm khó khăn của các nhiệm vụ thị giác chủ quan, nhưng tiến bộ trên các nhiệm vụ nhận thức mask, như phân đoạn biểu thức tham chiếu (RES) và phân đoạn đối tượng nổi bật, đã tương đối chậm do khó khăn trong việc thiết kế các token cấp pixel. Các phương pháp phổ biến hiện tại liên quan đến việc sử dụng đầu ra của grounding làm đầu vào cho SAM [61], hoặc sử dụng một bộ giải mã cụ thể cho huấn luyện từ đầu đến cuối [17, 60].

3 Phương pháp

Khung công việc tổng thể của u-LLaVA được trình bày trong Hình 1. Như chúng ta có thể thấy, u-LLaVA là một chatbot đa phương thức đa nhiệm vụ nhận văn bản, hình ảnh và video làm đầu vào. Nó đạt được điều này bằng cách thống nhất không gian biểu diễn của các yếu tố thị giác và văn bản ở giai đoạn I, và hiểu biết biểu diễn khu vực và pixel một cách kết hợp ở giai đoạn II. Trong phần này, chúng tôi sẽ giới thiệu trước kiến trúc mô hình và chiến lược căn chỉnh phương thức trong Mục 3.1, theo sau là một thảo luận về quá trình điều chỉnh hướng dẫn kết hợp trong Mục 3.2. Cuối cùng, chúng tôi sẽ trình bày các phương pháp xây dựng bộ dữ liệu.

3.1 Căn chỉnh Thị giác Hiệu quả

Để căn chỉnh biểu diễn giữa các phương thức khác nhau, cấu trúc dựa trên bộ chiếu được áp dụng trong công việc này: CLIP ViT-L/14 [37] được tiền huấn luyện và một bộ chiếu thị giác được kết hợp để mã hóa đầu vào hình ảnh, trong khi Vicuna [8] được sử dụng làm module nhận thức. Ngoài ra, u-LLaVA hỗ trợ phương thức video bằng cách nối biểu diễn không gian và thời gian, chỉ yêu cầu thêm hai token video đặc biệt và một lượng tham số có thể huấn luyện tối thiểu.

Nói chung, tối đa hóa hàm likelihood dưới đây để căn chỉnh không gian biểu diễn của hình ảnh/video và văn bản là một phương pháp được sử dụng rộng rãi cho tiền huấn luyện [25]. Đối với một embedding hình ảnh hoặc video xe nhất định, và một danh sách cuộc trò chuyện gồm L token xt={x1t, x2t, ..., xLt}, chúng ta có mục tiêu huấn luyện sau, được gọi là mất mát hạt thô:

Lcgl=∑ilogP(xi|xe, xi−k, ..., xi−1;θ), (1)

trong đó, theo [35], k, P, và θ lần lượt là kích thước cửa sổ ngữ cảnh, xác suất có điều kiện, và các tham số mạng.

3.2 Điều chỉnh Hướng dẫn Kết hợp

Điều chỉnh hướng dẫn thị giác là một chiến lược phổ biến để tinh chỉnh MLLM, nhưng hầu hết các phương pháp chỉ bao gồm một hoặc hai trong số các khía cạnh tổng quát, khu vực và cấp pixel trong giai đoạn huấn luyện. Trong công việc này, chúng tôi lần đầu tiên kết hợp các tính năng tổng quát, khu vực và pixel trong cùng một giai đoạn điều chỉnh.

Điều chỉnh nhận thức tổng quát: Trong phần này, không có điều chỉnh nào được thực hiện đối với cấu trúc mô hình. Tuy nhiên, không giống như giai đoạn đầu tiên, chúng tôi nhấn mạnh việc sử dụng các cuộc đối thoại đa lượt và bộ dữ liệu lý luận phức tạp để nâng cao thêm khả năng hiểu biết của mô hình. Các token đặc biệt được sử dụng trong công việc này được liệt kê trong Bảng 2.

Điều chỉnh nhận thức Mask: Lấy cảm hứng từ LISA [17], chúng tôi sử dụng một bộ chiếu để ánh xạ các trạng thái ẩn của các token đặc biệt liên quan đến mask và sau đó kết hợp chúng vào bộ giải mã SAM như các embedding văn bản để tạo điều kiện cho hiểu biết cấp pixel. Chúng tôi sử dụng một bộ chiếu để kết nối SAM và MLLM, và ánh xạ các trạng thái ẩn liên quan đến mask như embedding văn bản cho SAM. Điều này trao cho SAM khả năng nhận thức ngữ nghĩa trong khi đạt được nhận thức cấp pixel cho MLLM.

Điều chỉnh nhận thức Khu vực: Tương tự như nhận thức pixel, chúng tôi sử dụng một bộ chiếu và một bộ giải mã vị trí, ánh xạ các trạng thái ẩn của các token đặc biệt liên quan đến vị trí trực tiếp đến tọa độ mục tiêu, trong đó bộ giải mã bao gồm một MLP được khởi tạo ngẫu nhiên. Để nâng cao khối lượng dữ liệu và cải thiện hiệu suất của bộ giải mã, chúng tôi chuyển đổi các chú thích phân đoạn thành hộp giới hạn cho các mẫu thiếu chú thích phát hiện.

Nói chung, chúng ta có mục tiêu huấn luyện sau, tức là mất mát hạt mịn:

Lfgl=Lcgl+{ Lpixel, nếu mask tồn tại; Lregion, nếu bbox tồn tại; 0, nếu không }(2)

trong đó số hạng Lpixel =α1Lbce+α2Ldice biểu thị mất mát dự đoán mask, và Lregion =β1L1+β2Lgiou là mất mát dự đoán cho hộp giới hạn mục tiêu. Các giá trị của α1, α2, β1, và β2 được đặt lần lượt là 2.0, 0.5, 1.0, và 1.0.

3.3 Xây dựng Bộ dữ liệu

Để phù hợp với việc huấn luyện các mô hình nêu trên, chúng tôi tổ chức lại hoặc xây dựng lại các loại bộ dữ liệu công khai khác nhau, chi tiết được tóm tắt trong Bảng 3.

Đối với các bộ dữ liệu phân đoạn tham chiếu và ngữ nghĩa, tất cả các tham chiếu hoặc nhãn ngữ nghĩa đều được trích xuất và sau đó được hình thành với các template nhất định. Tuy nhiên, các bộ dữ liệu phát hiện/phân đoạn đối tượng nổi bật thường thiếu mô tả về các đối tượng mục tiêu. Để giải quyết vấn đề này, chúng tôi sử dụng thông tin mask để trích xuất các đối tượng chính từ hình ảnh trong MSRA-10K [10] và MSRA-B [47]. Các đối tượng được trích xuất sau đó được đưa vào BLIP2 [20] để tạo ra mô tả chỉ cho các đối tượng. Cuối cùng, GPT-4o được sử dụng để phân tích các thẻ đối tượng từ mô tả được tạo ra, theo sau là việc tích hợp các template được xác định trước để hoàn thành quá trình tái cấu trúc. Chúng tôi gọi bộ dữ liệu hướng dẫn nổi bật được tái cấu trúc là Salient-15K cho ngắn gọn. Các ví dụ template và quá trình xây dựng của Salient-15K được tóm tắt trong Phụ lục.

4 Thí nghiệm

4.1 Chi tiết Thực hiện

Tất cả các thí nghiệm đều được tiến hành với 8 GPU NVIDIA Tesla A100 80G và framework Pytorch [33]. Vicuna v1.1 [8] và CLIP ViT-L/14 [37] được đặt làm mô hình ngôn ngữ nền tảng và bộ mã hóa hình ảnh. Đối với nhiệm vụ hiểu biết khu vực, bộ chiếu và bộ giải mã được thực hiện bằng hai MLP. Cụ thể, bộ chiếu được cấu hình với các lớp [4096->4096, 4096->256], trong khi bộ giải mã bao gồm các lớp [256->256, 256->128, 128->4]. Đối với hiểu biết pixel, một MLP hai lớp được sử dụng làm bộ chiếu với các lớp [4096->4096, 4096->256]. Bộ giải mã được thực hiện bằng SAM ViT-H [16] có sẵn. Đối với huấn luyện căn chỉnh và hướng dẫn, AdamW được sử dụng làm bộ tối ưu hóa với weight decay là 0. Tốc độ học được đặt thành 2e-3 và 2e-5 (2e-4 nếu LoRA [11] được sử dụng). Kích thước batch trên mỗi thiết bị được cấu hình thành 48 và 16 (32 nếu LoRA), với bước tích lũy gradient là 1. Ngoài ra, độ dài token được đặt thành 1024 và 512. Dưới các cài đặt trên, mỗi bước huấn luyện yêu cầu khoảng 7s và 5s (9.5s nếu LoRA), với BF16 và DeepSpeed ZeRO-2 được bật.

4.2 Chỉ số Đánh giá

Chúng tôi tuân theo các công trình trước [23, 17] để xác thực hiệu suất định lượng của thuật toán đề xuất, với chi tiết như sau:

Phân đoạn Pixel: Cumulative-IoU (cIoU) là một chỉ số hiệu suất được sử dụng rộng rãi trong các nhiệm vụ phân đoạn, tính tổng số pixel giao nhau trên tổng số pixel hợp. Trong một số công trình, nó cũng được gọi là overall-IoU (oIoU), như thấy trong [56, 54].

Định vị Khu vực: Tỷ lệ phần trăm mẫu có IoU cao hơn ngưỡng X là một chỉ số thường được sử dụng trong các nhiệm vụ định vị thị giác, ký hiệu là Precision@X (Prec@X). Trong công việc này, chúng tôi đặt ngưỡng thành 0.5 theo [7].

4.3 Hiệu suất Hiểu biết Cấp Pixel

Để chứng minh hiệu suất của phương pháp đề xuất trên hiểu biết cấp pixel, chúng tôi tiến hành thí nghiệm trên các benchmark RES được sử dụng rộng rãi, RefCOCO, RefCOCO+, và RefCOCOg. Việc so sánh được thực hiện giữa các mô hình chuyên gia tiên tiến (SOTA) hiện tại và MLLMs với chỉ số cIoU, như được trình bày trong Bảng 4.

Như có thể thấy từ bảng, ngay cả với LoRA, phương pháp của chúng tôi vẫn đạt được kết quả tốt nhất trong số các phương pháp MLLMs. Đáng chú ý hơn, u-LLaVA-7B vượt qua hiệu suất của phương pháp MLLM tiên tiến hiện tại, LISA-7B*(ft), đạt được cải thiện trung bình 9.28 trong chỉ số cIoU. Cũng đáng chú ý là u-LLaVA vượt qua hiệu suất của mô hình chuyên gia hàng đầu hiện tại, UNINEXT(H) [54], trên ba benchmark, tất cả trong khi chỉ sử dụng một phần mười dữ liệu huấn luyện. Những phát hiện này phục vụ như một minh chứng cho hiệu quả của LLM trong các nhiệm vụ đòi hỏi khả năng dựa trên hiểu biết.

4.4 Hiệu suất Hiểu biết Ý định Cấp Pixel

Chúng tôi tiếp tục kiểm tra hiệu suất zero-shot của u-LLaVA trong các bộ dữ liệu phân đoạn nổi bật được công nhận rộng rãi để làm rõ sự ưu việt của MLLMs trong việc hiểu ý định chủ quan của con người.

Ở đây, các bộ dữ liệu DUT-OMRON [55] (5.168 hình ảnh test), DUTS-TE [48] (5.019 hình ảnh test), và ECSSD [40] (1000 hình ảnh test) được chọn để xác thực. Để đảm bảo công bằng, chúng tôi so sánh phương pháp của mình với một loạt các thuật toán không giám sát khác được thực hiện trước đó. Như được tóm tắt trong Bảng 5, u-LLaVA vượt trội hơn những phương pháp còn lại, đạt được hiệu suất SOTA trên tất cả ba benchmark, tiếp tục củng cố hiệu quả và sự ưu việt của phương pháp chúng tôi.

4.5 Hiệu suất Hiểu biết Cấp Khu vực

Trong phần này, chúng tôi tiến hành phân tích so sánh để đánh giá hiệu suất của u-LLaVA so với các mô hình MLLM 7B khác trong bối cảnh các nhiệm vụ hiểu biết cấp khu vực, sử dụng nhiệm vụ REC làm benchmark.

Cần nhấn mạnh rằng chúng tôi kết hợp tất cả kết quả trung gian của mô hình, bao gồm đầu ra của bộ giải mã khu vực và mask được tạo ra, để tạo ra hộp hồi quy cuối cùng do đó tối ưu hóa hiệu suất. Người ta có thể sử dụng thêm mô hình grounding có sẵn và các thẻ được tạo ra cho sự dự phòng, như được đóng gói trong Hình 2, và kết quả thí nghiệm tương ứng được tóm tắt trong Bảng 6. Có thể quan sát thấy, u-LLaVA vượt trội hơn các MLLMs khác như Shikra [7], trong khi chỉ sử dụng một phần mười dữ liệu. Tuy nhiên, tồn tại một khoảng cách hiệu suất có thể nhận biết khi so sánh với các mô hình chuyên gia như UNINEXT(H), điều quan trọng là phải xem xét rằng điều này bị ảnh hưởng bởi nhiều yếu tố, bao gồm, nhưng không giới hạn, độ phân giải đầu vào và sự can thiệp nhiệm vụ.

4.6 Benchmark Tổng quát

Trong Bảng 7, chúng tôi trình bày so sánh mô hình của chúng tôi, u-LLaVA, với các MLLM 7B phổ biến trên một số benchmark đa phương thức, bao gồm MMBench-Dev/Test [28], TextVQA [43], GQA [13], ScienceQA-IMG [30], và RefCOCO val. Đáng chú ý, chúng tôi mở rộng độ phân giải đầu vào của u-LLaVA lên 336, tức là u-LLaVA-1.5, để nâng cao hiệu suất của mô hình trên những nhiệm vụ như vậy. Trong khi những nhiệm vụ này không phải là trọng tâm chính của nghiên cứu hiện tại, phương pháp của chúng tôi chứng minh hiệu suất cạnh tranh so với các mô hình 7B khác. Cụ thể, u-LLaVA-1.5 đạt được kết quả tốt nhất trên nhiệm vụ ScienceQA-IMG sử dụng Vicuna-7B-v1.1 và xếp thứ hai chỉ sau LLaVA-1.5 trên các benchmark MMBench-Test và GQA.

4.7 Ablation Bộ dữ liệu

Như được hiển thị trong Bảng 8, chúng tôi xác thực tác động của việc sử dụng các loại bộ dữ liệu khác nhau trong giai đoạn thứ hai của huấn luyện mô hình đối với hiệu suất tổng thể của nó. Kết quả cho thấy việc chấp nhận sự đa dạng trong các loại bộ dữ liệu thúc đẩy khả năng tổng quát hóa được cải thiện của thuật toán, do đó tránh được rủi ro tiềm ẩn của overfitting trên các nhiệm vụ cụ thể. Về bản chất, tính mạnh mẽ của thuật toán được nâng cao với sự đa dạng tăng lên của các loại bộ dữ liệu.

4.8 Ví dụ Định tính

So sánh định tính với các phương pháp MLLM đa nhiệm vụ hiện tại, LISA [17], Shikra [7] và CogVLM [50], trên các nhiệm vụ grounding và phân đoạn được đưa ra trong Hình 3. Thêm minh họa cuộc trò chuyện có thể được tìm thấy trong Hình 4 và Hình 5.

5 Kết luận

Trong công việc này, chúng tôi giới thiệu u-LLaVA, một mô hình ngôn ngữ lớn đa phương thức điều chỉnh hướng dẫn một cách kết hợp ở cấp độ toàn cục, khu vực và pixel. Thông qua thiết kế cấu trúc sáng tạo và cấu hình dữ liệu, chúng tôi đã đạt được hiệu suất tối ưu trong các nhiệm vụ dựa trên hiểu biết khác nhau.

Hiện tại, tiền huấn luyện và thích ứng nhiệm vụ của MLLMs vẫn là một lĩnh vực mở với nhiều hướng chưa được khám phá. Nghiên cứu này đại diện cho một nỗ lực khám phá và thí nghiệm xây dựng trên các công trình trước đây như LLaVA và LISA. Chúng tôi tin rằng việc mở mã nguồn công việc của chúng tôi có thể cung cấp hỗ trợ có giá trị cho sự phát triển của lĩnh vực này.

6 Lời cảm ơn

Công việc này được tài trợ bởi Chương trình Pujiang Thượng Hải (23PJ1421800).

7 Phụ lục

7.1 Template

Ở đây, chúng tôi trình bày các ví dụ về template nhiệm vụ được sử dụng bởi u-LLaVA trên các loại dữ liệu huấn luyện khác nhau.

Ví dụ template cho nhiệm vụ phân đoạn nổi bật
<image> Điều gì làm cho hình ảnh nổi bật?
<image> Điều gì nổi bật trong hình ảnh này?
<image> Nhìn vào hình ảnh, phân đoạn đối tượng chính trong hình và giải thích.

Ví dụ template cho nhiệm vụ tạo chú thích video
<video> Mô tả video một cách ngắn gọn.
<video> Chuyện gì đang xảy ra trong video này?
<video> Viết một bản tóm tắt súc tích nhưng thông tin của VCR.

Ví dụ template cho nhiệm vụ RES
<image> Phân đoạn <class>.
<image> Xuất ra mask của <class>.
<image> Tìm <class> trong hình ảnh.

7.2 Xây dựng Salient-15K

Như được hiển thị trong Hình 7, với việc các bộ dữ liệu MSRA-10K và MSRA-B không có thông tin nhãn và mô tả hình ảnh liên quan đến các chủ thể chính, chúng tôi ban đầu tiến hành bằng cách trích xuất các chủ thể từ hình ảnh và sau đó nhập chúng vào BLIP2 [20] để có mô tả cơ bản. Sau đó, chúng tôi sử dụng GPT3.5 để phân tích các nhãn mục tiêu phát sinh từ mô tả cơ bản, do đó cho phép mở rộng thông tin mô tả. Phương pháp này tạo điều kiện cho sự hiểu biết toàn diện hơn về các chủ thể trong bộ dữ liệu trong khi bù đắp cho việc thiếu dữ liệu mô tả ban đầu.

• BLIP2: Người trượt ván đang thực hiện một thủ thuật.
• GPT: người trượt ván
• NGƯỜI DÙNG: <image> Điều gì làm cho ảnh này nổi bật?
• TRỢ LÝ: Đó là người trượt ván đang thực hiện một thủ thuật.
Thông tin: [SEG]; [LOC]; [tag]người trượt ván [/tag]
