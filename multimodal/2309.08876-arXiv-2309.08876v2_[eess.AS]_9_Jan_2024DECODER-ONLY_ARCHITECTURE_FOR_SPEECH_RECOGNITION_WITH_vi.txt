# 2309.08876.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.08876.pdf
# Kích thước tệp: 186264 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2309.08876v2  [eess.AS]  9 Jan 2024KIẾN TRÚC CHỈ DECODER CHO NHẬN DẠNG GIỌNG NÓI VỚI
CÁC PROMPT CTC VÀ TĂNG CƯỜNG DỮ LIỆU VÀN BẢN
Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi
Sony Group Corporation, JapanSiddhant Arora, Shinji Watanabe
Carnegie Mellon University, USA
TÓM TẮT
Việc thu thập các cặp âm thanh-văn bản rất tốn kém; tuy nhiên, việc truy cập dữ liệu chỉ có văn bản dễ dàng hơn nhiều. Trừ khi sử dụng shallow fusion, các mô hình nhận dạng giọng nói tự động (ASR) end-to-end yêu cầu sửa đổi kiến trúc hoặc các sơ đồ huấn luyện bổ sung để sử dụng dữ liệu chỉ có văn bản. Lấy cảm hứng từ những tiến bộ gần đây trong các mô hình ngôn ngữ chỉ decoder (LM), như GPT-3 và PaLM được áp dụng cho các tác vụ xử lý giọng nói, chúng tôi đề xuất sử dụng kiến trúc chỉ decoder cho ASR với việc tăng cường văn bản đơn giản. Để cung cấp thông tin âm thanh, các đặc trưng encoder được nén bằng dự đoán CTC được sử dụng như prompt cho decoder, điều này có thể được coi là tinh chỉnh dự đoán CTC sử dụng mô hình chỉ decoder. Bởi vì kiến trúc decoder giống như một LM tự hồi quy, nó đơn giản để tăng cường mô hình bằng cách tận dụng dữ liệu văn bản bên ngoài với huấn luyện LM. Một so sánh thực nghiệm sử dụng LibriSpeech và Switchboard cho thấy các mô hình đề xuất của chúng tôi với huấn luyện tăng cường văn bản đã giảm tỷ lệ lỗi từ từ CTC thông thường 0.3% và 1.4% trên bộ test-clean và test-other của LibriSpeech, tương ứng, và 2.9% và 5.0% trên Switchboard và CallHome. Mô hình đề xuất có lợi thế về hiệu quả tính toán so với các mô hình ASR encoder-decoder thông thường với thiết lập tham số tương tự, và vượt trội hơn chúng trong các kịch bản huấn luyện LibriSpeech 100h và Switchboard.
Từ khóa chỉ mục —Nhận dạng giọng nói, ASR chỉ Decoder, CTC, Prompt

1. GIỚI THIỆU
Nhận dạng giọng nói tự động (ASR) end-to-end yêu cầu một lượng lớn dữ liệu cặp âm thanh-văn bản, điều này khó có được, trong khi có thể dễ dàng thu thập được một lượng lớn dữ liệu chỉ có văn bản. Cách khai thác dữ liệu văn bản không ghép cặp không phải là một vấn đề tầm thường cho các tác vụ ASR. Việc thực hiện nội suy điểm với một mô hình ngôn ngữ (LM) bên ngoài được huấn luyện với dữ liệu văn bản cùng với một mô hình ASR là phổ biến [1, 2]. Một số nghiên cứu đã tiết lộ rằng việc ước tính độ lệch ngôn ngữ trong các mô hình ASR, một LM nội bộ, phù hợp hơn cho việc fusion hiệu quả hơn [3–5]. Để khai thác dữ liệu chỉ có văn bản có thể truy cập trực tiếp vào các mô hình, một số nghiên cứu đã sửa đổi kiến trúc mô hình để chấp nhận cả cặp âm thanh-văn bản và dữ liệu chỉ có văn bản làm đầu vào cho huấn luyện theo cách học đa tác vụ [6–9].

Gần đây, các tài nguyên văn bản như vậy đã được sử dụng hiệu quả bằng cách giới thiệu các LM lớn đã được huấn luyện trước (LLM) bao gồm các LM chỉ decoder như GPT-3 [10] và PaLM [11] vào công thức ASR. Các nghiên cứu đã thành công trong việc điều chỉnh các LM chỉ decoder cho các tác vụ xử lý giọng nói [12–17]. Để kết nối các phương thức âm thanh-văn bản, thông tin âm thanh được tiêm vào các LLM như một prompt, đó là các đơn vị âm thanh rời rạc [12–14] hoặc các biểu diễn liên tục được tiêm trực tiếp vào không gian nhúng ngôn ngữ [15, 16]. Trong phương pháp sau, các đặc trưng âm thanh được mã hóa được nén bằng các lớp convolution [15, 16] hoặc bằng dự đoán CTC [16], cũng đã được giới thiệu trong dịch thuật giọng nói (ST) [18] và giải mã RNN transducer (RNN-T) [19,20]. Nhờ tiềm năng mạnh mẽ của các LLM, các phương pháp này hoạt động tốt trong các kịch bản đa tác vụ của xử lý giọng nói-văn bản, như ASR, tổng hợp giọng nói, và ST. Lấy cảm hứng từ điều này, chúng tôi áp dụng kiến trúc chỉ decoder cho các tác vụ ASR có thể được tăng cường hiệu quả với dữ liệu chỉ có văn bản bên ngoài.

Nghiên cứu này nhằm xây dựng một kiến trúc chỉ decoder cho các tác vụ ASR và tăng cường hiệu suất của nó với việc tăng cường văn bản sử dụng dữ liệu chỉ có văn bản bên ngoài. Chúng tôi cung cấp thông tin âm thanh như các prompt được nén bằng dự đoán CTC, điều này cũng có thể được coi là tinh chỉnh dự đoán CTC sử dụng mô hình chỉ decoder. Mặc dù Wu et al. đã cố gắng huấn luyện một decoder-only từ đầu, họ đã thu được một mô hình ST hơi suy giảm so với các mô hình encoder-decoder thông thường. Tuy nhiên, chúng tôi huấn luyện decoder cho các tác vụ ASR sử dụng không chỉ cặp âm thanh-văn bản mà còn cả dữ liệu chỉ có văn bản bên ngoài làm tăng cường. Do đó, mô hình được huấn luyện từ đầu cho các tác vụ ASR và LM đồng thời. Qua thực nghiệm, chúng tôi xác nhận rằng mô hình đề xuất đã thành công trong việc tinh chỉnh kết quả CTC trong khi đạt được suy luận nhanh hơn nhờ cơ chế nén. Sử dụng tập con LibriSpeech 100h và Switchboard với tăng cường văn bản, mô hình chỉ decoder đã vượt trội hơn các mô hình encoder-decoder thông thường với thiết lập tham số tương tự. Những đóng góp chính của nghiên cứu này như sau:

• Chúng tôi đề xuất một sơ đồ huấn luyện mới để xây dựng một mô hình chỉ decoder từ đầu cho các tác vụ ASR đồng thời được tăng cường bằng dữ liệu chỉ có văn bản.
• Theo hiểu biết của chúng tôi, đây là nghiên cứu đầu tiên thành công vượt trội hơn các mô hình encoder-decoder thông thường với mô hình chỉ decoder có kích thước tham số tương tự bằng cách khai thác dữ liệu chỉ có văn bản bên ngoài hiệu quả.
• Chúng tôi thực nghiệm chỉ ra rằng phương pháp đề xuất của chúng tôi đạt được tỷ lệ lỗi từ (WER) thấp hơn 0.3% và 1.4% cho LibriSpeech test-clean và test-other, tương ứng, so với mô hình encoder-decoder, với chi phí tính toán khoảng một nửa, khi chúng tôi sử dụng dữ liệu ghép cặp 100h với tăng cường văn bản 960h dữ liệu phiên âm.

2. PROMPT CTC CHO ASR CHỈ DECODER
2.1. Kiến trúc chỉ decoder
Chúng tôi tuân theo mô hình ASR conformer encoder-decoder [21], ngoại trừ việc không có các lớp attention nguồn-đích trong transformer decoder. Một transformer decoder không có các lớp attention nguồn-đích được coi là một LM chỉ decoder tự hồi quy như GPT-3 [10] hoặc PaLM [11].

ASR là một tác vụ dự đoán chuỗi token có độ dài I có xác suất cao nhất YI cho một âm thanh đầu vào có độ dài T là XT, tức là, YI với xác suất cao nhất p(YI|XT). Thay vì sử dụng trực tiếp đầu vào âm thanh XT

--- TRANG 2 ---
EncoderCTC
<aud> <sos><eos>
Dự đoán token tiếp theo
Miền 
giọng nóiMiền 
ngôn ngữ
Decoder
không gian
nhúng
token
tín hiệu âm thanhDự đoán CTC
Hình 1. Kiến trúc mô hình của mô hình chỉ decoder cho ASR với prompt CTC.

trong decoder, nó thường được xấp xỉ bằng biểu diễn âm thanh compact. Thông tin âm thanh được cung cấp như các prompt có độ dài τ là ˆHτ={ˆht|1≤t≤τ} trực tiếp trong không gian nhúng của decoder. Chúng tôi đề xuất sử dụng dự đoán CTC để tạo ra các prompt một cách hiệu quả, cụ thể là các prompt CTC, được trình bày trong Sec.2.2. Decoder dự đoán tự hồi quy token ngôn ngữ tiếp theo yi cho một token prompt âm thanh ⟨aud⟩, các prompt CTC ˆHτ, và các đầu ra trước đó Y<i={yj|0≤j < i}. Do đó,

p(Yi) =∏(j=1 to i) p(yj|ˆHτ,Y<j)
=∏(j=1 to i) Dec(ˆHτ,Y<j), (1)

trong đó y0 là token bắt đầu chuỗi, ⟨sos⟩. Một tổng quan về phương pháp đề xuất được trình bày trong Hình 1.

2.2. Prompt âm thanh sử dụng dự đoán CTC
Các prompt CTC được tạo ra bằng cách sử dụng một encoder. Nói chung, độ dài của âm thanh dài hơn độ dài của chuỗi văn bản, điều này đôi khi trở nên có vấn đề vì việc tính toán self-attention của decoder tăng theo bậc hai với độ dài đầu vào. Một phương pháp phổ biến là sử dụng các lớp convolution [15,16], giảm mẫu đầu vào âm thanh xuống tốc độ khung hình 80 ms hoặc cao hơn để phù hợp với độ chi tiết của các token ngôn ngữ. Tuy nhiên, Wu et al. đã chỉ ra rằng nén dựa trên CTC [18] hiệu quả hơn các lớp convolution [16]. Trong tài liệu, họ so sánh việc loại bỏ các khung hình mà CTC dự đoán là "blank" và trung bình hóa các khung hình của cùng một dự đoán CTC. Chúng tôi áp dụng phương pháp loại bỏ khung hình, và tất cả các phương pháp đều được so sánh thực nghiệm trong Sec. 3.2.

Encoder conformer xuất ra các đặc trưng âm thanh được mã hóa có độ dài T′ là HT′={ht|1≤t≤T′} từ đầu vào âm thanh XT với tốc độ giảm mẫu N như

HT′= Enc(XT), (2)

trong đó T′=T/N. Tiếp theo, module CTC ánh xạ HT′ đến phân phối xác suất của từ vựng V được bổ sung với token blank ⟨blank⟩, được huấn luyện kết hợp như trong [22].

p(at) = CTC(ht), (3)

trong đó at∈ {V ∪⟨blank⟩}.

Vì độ dài của các đặc trưng âm thanh T′ thường dài hơn độ dài của các chuỗi văn bản, các prompt âm thanh ˆHτ được giảm mẫu sao cho τ < T′. Chúng tôi loại bỏ các khung hình được dự đoán là blank bởi CTC và ánh xạ chúng vào không gian nhúng của decoder, như sau.

ˆHτ={MLP(ht)|t: ˆat≠⟨blank⟩}, (4)

trong đó ˆat là cái có xác suất cao nhất trong phân phối xác suất (3) và MLP(·) là một hàm ánh xạ từ đầu ra encoder đến không gian nhúng, mà chúng tôi áp dụng một lớp tuyến tính cho đơn giản. Do đó, prompt âm thanh được nén được tiêm trực tiếp vào không gian nhúng liên tục của decoder. Decoder cũng có thể được xem như một sự tinh chỉnh của dự đoán CTC.

2.3. Huấn luyện với tăng cường văn bản
Trong khi huấn luyện ASR yêu cầu dữ liệu cặp âm thanh-văn bản, một kho văn bản lớn dễ thu thập hơn nhiều. Do đó, việc tăng cường hiệu suất ASR với dữ liệu văn bản như vậy là hợp lý, thường với shallow fusion của các mô hình LM bên ngoài [1,2]; ví dụ, kho LibriSpeech [23] cung cấp đáng kể nhiều dữ liệu chỉ có văn bản để huấn luyện các LM bên ngoài ngoài dữ liệu cặp âm thanh-phiên âm 960h. Chúng tôi tập trung vào các kịch bản này và nhằm tăng cường kiến trúc chỉ decoder sử dụng một tập văn bản bổ sung. Đối với tác vụ ASR, encoder cho các prompt CTC và decoder tự hồi quy được huấn luyện kết hợp. Bởi vì kiến trúc của mô hình chỉ decoder đề xuất và các LM tự hồi quy tổng quát được biểu diễn bằng cùng một transformer tự hồi quy, chúng tôi sử dụng dữ liệu chỉ có văn bản để tăng cường văn bản để huấn luyện chỉ decoder cho một tác vụ LM.

Encoder và decoder có thể được huấn luyện trước sử dụng dữ liệu ghép cặp và dữ liệu văn bản không ghép cặp, tương ứng, theo sau bằng fine-tuning với dữ liệu ghép cặp như trong [15]. Tuy nhiên, chúng tôi nhằm huấn luyện chúng từ đầu vì nó được coi là hội tụ đến các mô hình tối ưu hơn, điều này đã được xác nhận trong Sec. 3.2. Để thực hiện huấn luyện kết hợp, chúng tôi chia mini-batch B cho cả hai tác vụ và tính toán mỗi loss đồng thời, tức là, B={BASR,BLM}.

2.3.1. Huấn luyện ASR
Tuân theo huấn luyện kết hợp của encoder-decoder và CTC trong [22], chúng tôi huấn luyện encoder, decoder, và CTC đồng thời. Trong mỗi bước huấn luyện, ngoài loss CTC Lctc, loss decoder Latt

--- TRANG 3 ---
Bảng 1. Kết quả ASR với dữ liệu cặp âm thanh-văn bản LibriSpeech 100h và kết quả dữ liệu văn bản 960h. LM bên ngoài được huấn luyện sử dụng dữ liệu văn bản bên ngoài.
IDs Mô hình Dữ liệu Huấn luyện Prompt Giải mã Fusion WER Param RTF
ext-text CTC ext-LM dev-clean dev-other test-clean test-other
B1 Baseline CTC [24] 9.2 22.8 9.6 23.5 34.8M 0.14
B2 Baseline CTC [24] ✓ ✓ 7.1 18.8 7.3 19.6 44.2M 0.29
B3 Baseline RNN-T [25] ✓ ✓ 7.0 18.6 7.5 19.4 54.4M 0.67
B4 Baseline EncDec [21] 8.6 20.9 8.9 21.5 46.8M 0.53
B5 ✓ ✓ ✓ 6.9 18.0 7.3 18.9 56.2M 0.58
P1 Dec-only downsample 14.1 26.9 14.8 27.7 45.3M 0.38
P2 CTC average 30.5 41.8 30.2 41.2 45.3M 0.55
P3 CTC remove 9.3 22.4 9.5 23.0 45.3M 0.23
D1 Dec-only w/ textAug ✓ CTC remove 8.7 19.6 9.3 19.9 45.3M 0.24
D2 ✓ CTC remove ✓ 7.0 18.1 7.2 18.5 45.3M 0.24
D3 ✓ CTC remove ✓ ✓ 6.7 16.8 7.0 17.5 54.7M 0.29
F1 Dec-only (fine-tuned) [15] ✓ CTC remove ✓ ✓ 7.0 18.0 7.3 18.7 54.7M 0.29

được xem xét chỉ cho phần phiên âm, không bao gồm các prompt, sử dụng teacher-forcing như:

LASR=λLctc+(1−λ)Latt
=λLctc+(1−λ)∑(i=0 to I) −logp(yi|ˆHτ,˜Y<i), (5)

trong đó ˜Y<i đến từ ground truth và ˆHτ được cung cấp bởi dự đoán CTC của mô hình hiện tại được giới thiệu trong (4). Các gradient được truyền qua ˆHτ sao cho các tham số encoder được cập nhật dựa trên (5). Ở đầu quá trình huấn luyện, các dự đoán CTC chưa đủ chín muồi và có xu hướng có ít token blank hơn. Điều này dẫn đến các prompt CTC dài hơn, có thể có vấn đề cho mục đích huấn luyện. Do đó, chúng tôi sử dụng ngưỡng có thể điều chỉnh θ để phát hiện các dự đoán chưa chín muồi. Khi τ > θI, chúng tôi xem xét các dự đoán CTC là chưa chín muồi và, đối với câu huấn luyện cụ thể đó, chúng tôi thay thế Latt bằng LLM, sẽ được giới thiệu trong Sec. 2.3.2.

2.3.2. Huấn luyện LM
Bởi vì kiến trúc của mô hình chỉ decoder và một LM được biểu diễn bằng cùng một transformer tự hồi quy, chúng tôi có thể chỉ huấn luyện decoder với một tác vụ LM sử dụng các câu văn bản được chọn ngẫu nhiên từ dữ liệu tăng cường. Một phương pháp đơn giản là tuân theo huấn luyện LM thông thường và tối thiểu hóa negative log-likelihood, tương đương với việc tối thiểu hóa perplexity.

LLM=∑(i=0 to I) −logp(yi|˜Y<i) (6)

Quá trình này được minh họa trong Hình 2(a). Tuy nhiên, trong các tác vụ ASR, decoder luôn mong đợi các prompt CTC. Do đó, việc tối thiểu hóa khoảng cách giữa huấn luyện LM và suy luận ASR bằng cách tạo ra các prompt âm thanh giả với chuỗi nhúng ground-truth như trong Hình 2(b) là hợp lý. Đối với một tập con của các mini-batch B*LM, các prompt CTC trong (1) được thay thế bằng ˜YI, như

L*LM=∑(i=0 to I) −logp(yi|˜YI,˜Y<i). (7)

Tác vụ sau này dễ dàng hơn nhiều vì nó sao chép chuỗi prompt và xuất ra nó trực tiếp. Do đó, chúng tôi chia batch BLM thành các batch có và không có prompt CTC giả để đảm bảo khả năng dự đoán token dựa trên (6), tức là, BLM→ {BLM,B*LM}.

<aud>(a)
(b)<sos><eos>
<sos><eos>Decoder
Decoder
Hình 2. Các phương pháp huấn luyện LM. (a) là huấn luyện LM thông thường. (b) là prompt giả với các vector nhúng ground truth.

3. THỰC NGHIỆM
3.1. Thiết lập thực nghiệm
Để đánh giá mô hình ASR chỉ decoder đề xuất với các prompt CTC, chúng tôi sử dụng LibriSpeech [23], có chứa một kho LM bên ngoài chỉ có văn bản. Chúng tôi cũng sử dụng bộ dữ liệu Switchboard, thường được kết hợp với kho Fisher, như dữ liệu chỉ có văn bản bên ngoài cho LM. Tuân theo [21], chúng tôi huấn luyện cả mô hình conformer cơ sở (Baseline EncDec) và mô hình chỉ decoder với các prompt CTC (Decoder-only); sự khác biệt là cái sau không có các lớp attention nguồn-đích như mô tả trong Sec. 2.1.

Các đặc trưng âm thanh đầu vào là các đặc trưng filter-bank 80 chiều. Decoder (Eq. (1)) là một transformer 6 khối, và encoder (Eq. (2)) là một conformer 12 khối, cả hai đều có các lớp attention 4 đầu 256 đơn vị và các lớp feed-forward 2048 đơn vị. Các mô hình được huấn luyện sử dụng học đa tác vụ với loss CTC, như mô tả trong Sec 2.3.1, với trọng số λ= 0.3 trong (5). Bởi vì chúng tôi thực nghiệm thấy rằng huấn luyện ASR (Eq. (5)) quan trọng hơn huấn luyện LM với tăng cường văn bản (Eq. (6)) cho các tác vụ ASR, chúng tôi phân bổ hầu hết các mini-batch cho các tác vụ ASR và sử dụng 10% trong số chúng cho các tác vụ LM. Hơn nữa, chúng tôi chia minibatch cho các tác vụ LM như |BLM|:|B*LM|= 1 : 1 như đã thảo luận trong Sec. 2.3.2. Ngưỡng θ trong Sec. 2.3.1 được đặt thành hai. Chúng tôi sử dụng bộ tối ưu hóa Adam với suy giảm tốc độ học Noam.

Các LM bên ngoài được huấn luyện sử dụng dữ liệu chỉ có văn bản, bao gồm một kho văn bản bổ sung từ LibriSpeech hoặc Fisher. Các LM là các LSTM hai lớp với 512 đơn vị. Chúng tôi áp dụng token hóa subword byte-pair encoding với 5,000 lớp token cho LibriSpeech và 2000 cho Switchboard.

Để so sánh, chúng tôi đánh giá WER của CTC [24] và RNN-T [25]. CTC cơ sở sử dụng cùng kiến trúc như encoder conformer đã đề cập ở trên. RNN-T sử dụng một encoder conformer 15 khối với một LSTM một lớp 256 đơn vị cho mạng dự đoán. Trong khi các mô hình Baseline EncDec và Decoder-only đề xuất được giải mã theo cách đồng bộ nhãn, các mô hình CTC và RNN-T sử dụng tìm kiếm beam đồng bộ thời gian với kích thước beam 10, trọng số fusion LM 0.4, và độ dài penalty 1.0.

3.2. Thực nghiệm LibriSpeech 100h
Đầu tiên, chúng tôi sử dụng tập con clean 100h của dữ liệu ghép cặp LibriSpeech và so sánh mô hình đề xuất của chúng tôi dưới các điều kiện khác nhau. Khi sử dụng dữ liệu chỉ có văn bản bên ngoài cho tăng cường văn bản hoặc huấn luyện LM, các phiên âm của tập huấn luyện đầy đủ 960h được sử dụng. Chúng tôi huấn luyện mô hình conformer Baseline EncDec, RNN-T, và mô hình chỉ decoder đề xuất sử dụng các prompt CTC trong 100 epoch. Đối với

--- TRANG 4 ---
Bảng 2. Một ví dụ về tinh chỉnh phiên âm bằng kiến trúc chỉ decoder.
Tham khảo the life of every man in the castle shall answer it if a hair of his head be signed show me his chamber
CTC the life of every man in the council shall answer it if a hair of his head be singinged show me his chamber
Decoder-only the life of every man in the castle shall answer it if a hair of his head be signed show me his chamber

Bảng 3. Kết quả ASR với dữ liệu ghép cặp LibriSpeech 960h và kết quả văn bản bên ngoài. LM bên ngoài được fusion với tất cả các mô hình.
Mô hình WER RTF
test-clean test-other
Baseline CTC [24] 3.1 7.0 0.30
Baseline EncDec [21] 2.6 6.2 0.54
Decoder-only w/ textAug 3.0 6.6 0.29

các mô hình Baseline EncDec và Decoder-only đề xuất, chúng tôi fusion CTC và LM với trọng số 0.4 và 0.6, tương ứng, và kích thước beam là 10. Để đánh giá hiệu suất thuần túy của các mô hình chỉ decoder, chúng tôi cũng huấn luyện các mô hình chỉ với cặp âm thanh-văn bản của tập 100h, tức là, B=BASR, không có bất kỳ fusion nào. Hơn nữa, chúng tôi đo hệ số thời gian thực (RTF) của suy luận trên tập test-clean sử dụng CPU Intel i9-9900K 8 nhân 3.60 GHz.

Kết quả được liệt kê trong Bảng 1. Đầu tiên, các phương pháp nén prompt được thảo luận trong Sec. 2.2 được so sánh (P1–3). Tuân theo [15], chúng tôi áp dụng các lớp convolution để giảm mẫu tốc độ khung hình của encoder xuống 80 ms (P1). Mặc dù họ báo cáo rằng tốc độ khung hình 80 ms hoạt động tốt nhất trong một đánh giá đa ngôn ngữ, nó không hoạt động tốt trong thiết lập của chúng tôi. [16] báo cáo rằng việc trung bình hóa các khung hình của cùng một dự đoán CTC hơi vượt trội hơn việc loại bỏ các khung hình blank cho fine-tuning trong các tác vụ ST; tuy nhiên, họ không sử dụng nó cho các mô hình được huấn luyện từ đầu. Chúng tôi cũng thấy rằng phương pháp trung bình hóa (P2) gặp khó khăn trong huấn luyện cho các tác vụ ASR. Do đó, chúng tôi xác nhận rằng việc loại bỏ các khung hình của dự đoán blank là lựa chọn tốt nhất để nén các prompt (P3), và chúng tôi sử dụng phương pháp này cho các thực nghiệm tiếp theo. Tuy nhiên, chỉ với dữ liệu cặp âm thanh-văn bản (P3), kiến trúc chỉ decoder không thể vượt trội hơn kết quả Baseline EncDec (B4), như [16] báo cáo rằng huấn luyện mô hình chỉ decoder từ đầu hơi tệ hơn mô hình encoder-decoder thông thường.

Tiếp theo, chúng tôi huấn luyện các mô hình với tăng cường văn bản đề xuất (D1–3). Trái ngược với huấn luyện chỉ dữ liệu ghép cặp, tăng cường văn bản sử dụng kho văn bản chỉ có văn bản bên ngoài (D1) đã giảm đáng kể WER từ 9.5% và 23.0% xuống 9.3% và 19.9% cho các tập test-clean và test-other, tương ứng, so với (P3). Như thảo luận trong Sec. 2.3, bởi vì cấu trúc của decoder giống hệt với các LM tự hồi quy, đây là một lợi thế của kiến trúc chỉ decoder để tăng cường mô hình với huấn luyện LM mà không cần sửa đổi nào. So với Baseline EncDec được fusion với CTC và LM (B5), phương pháp đề xuất chỉ với fusion CTC (D2) đạt được WER tương đương. Việc nén prompt đã đóng góp vào việc tăng tốc tính toán; nó ít hơn một nửa RTF của Baseline EncDec (B5)¹. LM bên ngoài cũng hiệu quả và bổ sung cho tăng cường văn bản, vì fusion cả CTC và LM (D3) đạt được hiệu suất tốt nhất; 7.0% và 17.5% cho test-clean và test-other. Nó cũng nhanh tương đương với Baseline CTC với fusion LM (B2), trong đó tính toán LM tốn kém trong giải mã đồng bộ thời gian. Như trong [15], chúng tôi cũng đánh giá việc huấn luyện trước của CTC và decoder với dữ liệu ghép cặp và dữ liệu văn bản không ghép cặp, tương ứng, theo sau bằng fine-tuning sử dụng dữ liệu ghép cặp (F1), như mô tả trong Sec. 2.3. Tuy nhiên, phương pháp fine-tuning hơi tệ hơn huấn luyện từ đầu (D3), có thể vì phương pháp fine-tuning ít tối ưu hơn. Do đó, sử dụng tăng cường văn bản, nghiên cứu này thành công vượt trội hơn mô hình encoder-decoder thông thường với kích thước tham số nhỏ hơn, RTF thấp hơn, và WER thấp hơn bằng cách huấn luyện mô hình chỉ decoder sử dụng cùng lượng dữ liệu từ đầu.

¹Số lượng khung hình đã giảm xuống trung bình 14.55%.

Bảng 4. Kết quả ASR với dữ liệu ghép cặp Switchboard và kết quả kho văn bản Fisher. LM bên ngoài được fusion với tất cả các mô hình.
Mô hình WER RTF
Switchboard CallHome
Baseline CTC [24] 8.9 15.5 0.28
Baseline EncDec [21] 8.1 14.8 0.42
Decoder-only w/ textAug 7.3 13.3 0.28

3.3. Thực nghiệm LibriSpeech 960h
Chúng tôi cũng đánh giá tập ghép cặp đầy đủ 960h của LibriSpeech và kho văn bản chỉ có văn bản bên ngoài của nó. Chúng tôi huấn luyện các mô hình trong 30 epoch và sử dụng cùng kích thước beam và trọng số fusion như trong Sec. 3.2 cho suy luận. Kết quả được liệt kê trong Bảng 3. Mô hình chỉ decoder đã thành công tinh chỉnh phiên âm CTC, đặc biệt trong tập test-other, từ 7.0% xuống 6.6%. Chúng tôi lấy mẫu một phát ngôn từ tập test-other để chỉ ra cách decoder tinh chỉnh dự đoán CTC trong Bảng 2. Tuy nhiên, so với Baseline EncDec, phương pháp của chúng tôi không đạt được hiệu suất của nó. Chúng tôi giả định rằng khả năng của decoder tương đối nhỏ vì nó là một transformer 6 khối trong khi một LM transformer thường có 12 khối hoặc hơn [26]. Tuy nhiên, phương pháp đề xuất của chúng tôi vẫn cho thấy lợi thế về tốc độ tính toán; 0.29 RTF so với 0.54.

3.4. Thực nghiệm Switchboard và Fisher
Chúng tôi đánh giá các mô hình Switchboard sử dụng Hub5'00 với các tập con Switchboard và CallHome. Chúng tôi fusion CTC và LM với trọng số 0.4 và 0.4, tương ứng, với kích thước beam 10. Bảng 4 liệt kê kết quả. Chúng tôi quan sát xu hướng tương tự như trong Phần 3.2; mô hình đề xuất thành công vượt trội hơn mô hình Baseline EncDec trong cả hai tập con test, nhờ tăng cường văn bản hiệu quả.

4. KẾT LUẬN
Chúng tôi đề xuất một kiến trúc chỉ decoder cho các tác vụ ASR áp dụng hiệu quả tăng cường văn bản sử dụng dữ liệu bổ sung chỉ có văn bản. Thông tin âm thanh được cung cấp như các prompt CTC, ánh xạ đầu ra của module encoder được nén bằng dự đoán CTC vào không gian nhúng của decoder. Mặc dù mô hình được huấn luyện sử dụng tác vụ ASR, decoder đồng thời được huấn luyện cho tác vụ LM sử dụng dữ liệu văn bản tăng cường. Chúng tôi thực nghiệm xác nhận rằng các phương pháp đề xuất đã tinh chỉnh CTC cơ sở sử dụng decoder và vượt trội hơn các mô hình RNN-T và encoder-decoder thông thường với khoảng một nửa chi phí tính toán trong các thiết lập LibriSpeech 100h và Switchboard.

Công việc tương lai sẽ bao gồm việc mở rộng quy mô decoder để có thêm khả năng cho LM trong khi duy trì lợi thế về tốc độ suy luận. Mở rộng phương pháp streaming cũng có thể được xem xét bằng cách khai thác tính compact và hiệu quả của phương pháp đề xuất.

--- TRANG 5 ---
5. TÀI LIỆU THAM KHẢO
[1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., "Deep speech: Scaling up end-to-end speech recognition," arXiv preprint arXiv:1412.5567, 2014.
[2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, và Yoshua Bengio, "Attention-based models for speech recognition," trong Proc. of NIPS, 2015, pp. 577–585.
[3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, và Yifan Gong, "Internal language model estimation for domain-adaptive end-to-end speech recognition," trong 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243–250.
[4] Albert Zeyer, André Merboldt, Wilfried Michel, Ralf Schlüter, và Hermann Ney, "Librispeech transducer model with internal language model prior correction," trong Proc. of Interspeech, 2021, pp. 2052–2056.
[5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, và Shinji Watanabe, "Residual language model for end-to-end speech recognition," trong Proc. of Interspeech 2022, 2022, pp. 3899–3903.
[6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, và Shinji Watanabe, "Multi-modal data augmentation for end-to-end asr," Proc. of Interspeech 2018, pp. 2394–2398, 2018.
[7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., "Independent language modeling architecture for end-to-end ASR," trong Proc. of ICASSP, 2020, pp. 7059–7063.
[8] Peidong Wang, Tara N. Sainath, và Ron J. Weiss, "Multitask training with text data for end-to-end speech recognition," trong Proc. of Interspeech, 2021, pp. 2566–2570.
[9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ramabhadran, Pedro J. Moreno, Ankur Bapna, và Heiga Zen, "MAESTRO: Matched speech text representations through modality matching," trong Proc. of Interspeech, 2022, pp. 4093–4097.
[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., "Language models are few-shot learners," Proc. of NurIPS, vol. 33, pp. 1877–1901, 2020.
[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al., "PaLM: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.
[12] Kai-Wei Chang, Yu-Kai Wang, Hua Shen, Iu-thing Kang, Wei-Cheng Tseng, Shang-Wen Li, và Hung-yi Lee, "Speechprompt v2: Prompt tuning for speech classification tasks," arXiv preprint arXiv:2303.00733, 2023.
[13] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, và Xipeng Qiu, "SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities," arXiv preprint arXiv:2305.11000, 2023.
[14] Paul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al., "AudioPaLM: A large language model that can speak and listen," arXiv preprint arXiv:2306.12925, 2023.
[15] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al., "Prompting large language models with speech recognition abilities," arXiv preprint arXiv:2307.11795, 2023.
[16] Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, et al., "On decoder-only architecture for speech-to-text and large language model integration," arXiv preprint arXiv:2307.03917, 2023.
[17] Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Brian Yan, và Shinji Watanabe, "Integrating pretrained ASR and LM to perform sequence generation for spoken language understanding," trong Proc. INTERSPEECH 2023, 2023, pp. 720–724.
[18] Marco Gaido, Mauro Cettolo, Matteo Negri, và Marco Turchi, "CTC-based compression for direct speech translation," trong Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, 2021, pp. 690–696.
[19] Zhengkun Tian, Jiangyan Yi, Ye Bai, Jianhua Tao, Shuai Zhang, và Zhengqi Wen, "FSR: Accelerating the inference process of transducer-based models by applying fast-skip regularization," trong Proc. of Interspeech, 2021, pp. 4034–4038.
[20] Yongqiang Wang, Zhehuai Chen, Chengjian Zheng, Yu Zhang, Wei Han, và Parisa Haghani, "Accelerating RNN-T training and inference using CTC guidance," trong Proc. of ICASSP, 2023, pp. 1–5.
[21] Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, et al., "Recent developments on espnet toolkit boosted by conformer," arXiv preprint arXiv:2010.13956, 2020.
[22] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hershey, và Tomoki Hayashi, "Hybrid CTC/attention architecture for end-to-end speech recognition," Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017.
[23] Vassil Panayotov, Guoguo Chen, Daniel Povey, và Sanjeev Khudanpur, "LibriSpeech: an ASR corpus based on public domain audio books," trong Proc. of ICASSP, 2015, pp. 5206–5210.
[24] Alex Graves, Santiago Fernández, Faustino Gomez, và Jürgen Schmidhuber, "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks," trong Proc. of 23rd International Conference on Machine Learning, 2006, pp. 369–376.
[25] Alex Graves, Abdel-Rahman Mohamed, và Geoffrey Hinton, "Speech recognition with deep recurrent neural networks," trong Proc. of ICASSP, 2013, pp. 6645–6649.
[26] Kazuki Irie, Albert Zeyer, Ralf Schlüter, và Hermann Ney, "Language modeling with deep transformers," trong Proc. of Interspeech, 2019, pp. 3905–3909.
