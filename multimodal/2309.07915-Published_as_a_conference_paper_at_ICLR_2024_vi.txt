# 2309.07915.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.07915.pdf
# Kích thước tệp: 9138737 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như bài báo hội nghị tại ICLR 2024
MMICL: TĂNG CƯỜNG MÔ HÌNH THỊ GIÁC-NGÔN NGỮ
VỚI HỌC TẬP TRONG BỐI CẢNH ĐA PHƯƠNG THỨC

Haozhe Zhao˚§1,2, Zefan Cai˚1, Shuzheng Si˚1, Xiaojian Ma3, Kaikai An1,
Liang Chen1, Zixuan Liu4, Sheng Wang4, Wenjuan Han:5, Baobao Chang:1
1Phòng thí nghiệm Trọng điểm Quốc gia về Xử lý Thông tin Đa phương tiện, Đại học Bắc Kinh
2Trường Phần mềm và Vi điện tử, Đại học Bắc Kinh, Trung Quốc
3Phòng thí nghiệm Trọng điểm Quốc gia về Trí tuệ Nhân tạo Tổng quát, BIGAI
4Trường Khoa học Máy tính và Kỹ thuật Paul G. Allen, Đại học Washington
5Đại học Giao thông Bắc Kinh
mimazhe55360@gmail.com ,zefncai@gmail.com ,sishuzheng@stu.pku.edu.cn
https://github.com/PKUnlp-icler/MIC

TÓM TẮT
Kể từ khi học sâu hồi sinh, các mô hình thị giác-ngôn ngữ (VLM) được tăng cường bởi các mô hình ngôn ngữ lớn (LLM) đã phát triển theo cấp số nhân về mức độ phổ biến. Tuy nhiên, trong khi LLM có thể sử dụng kiến thức nền rộng lớn và thông tin nhiệm vụ với học tập trong bối cảnh, hầu hết các VLM vẫn gặp khó khăn trong việc hiểu các lời nhắc đa phương thức phức tạp với nhiều hình ảnh, làm cho VLM kém hiệu quả hơn trong các nhiệm vụ thị giác-ngôn ngữ hạ nguồn. Trong bài báo này, chúng tôi giải quyết hạn chế trên bằng cách 1) giới thiệu Mô hình thị giác-ngôn ngữ với Học tập Trong bối cảnh Đa phương thức (MMICL), một phương pháp mới để cho phép VLM xử lý đầu vào đa phương thức một cách hiệu quả; 2) đề xuất một lược đồ bối cảnh mới để tăng cường khả năng học tập trong bối cảnh của VLM; 3) xây dựng bộ dữ liệu Học tập Trong bối cảnh Đa phương thức (MIC), được thiết kế để nâng cao khả năng của VLM trong việc hiểu các lời nhắc đa phương thức phức tạp. Các thí nghiệm của chúng tôi xác nhận rằng MMICL đạt được hiệu suất zero-shot tiên tiến mới trên một loạt các nhiệm vụ thị giác-ngôn ngữ tổng quát, đặc biệt là đối với các benchmark phức tạp, bao gồm MME và MMBench. Phân tích của chúng tôi chứng minh rằng MMICL giải quyết hiệu quả thách thức hiểu lời nhắc đa phương thức phức tạp và xuất hiện khả năng ICL ấn tượng. Hơn nữa, chúng tôi quan sát thấy rằng MMICL thành công trong việc giảm thiểu thiên lệch ngôn ngữ trong VLM, một vấn đề phổ biến đối với VLM thường dẫn đến ảo giác khi đối mặt với bối cảnh văn bản rộng lớn. Mã, bộ dữ liệu, công cụ bộ dữ liệu và mô hình của chúng tôi có sẵn tại https://github.com/PKUnlp-icler/MIC .

1 GIỚI THIỆU
Các mô hình được tiền huấn luyện thị giác-ngôn ngữ đa năng (VLM) đã có những tiến bộ đáng kể (Li et al., 2022; 2023d;g; Zhu et al., 2023; Li et al., 2023b). Các VLM gần đây chủ yếu tăng cường một mô hình ngôn ngữ lớn (LLM) với một bộ mã hóa thị giác và thể hiện khả năng zero-shot ấn tượng trong các nhiệm vụ thị giác khác nhau. Tuy nhiên, không giống như LLM có thể trích xuất kiến thức nền phong phú và thông tin nhiệm vụ từ lời nhắc với học tập trong bối cảnh (ICL), hầu hết các VLM vẫn gặp khó khăn trong việc hiểu các lời nhắc đa phương thức phức tạp bao gồm nhiều hình ảnh. Các nghiên cứu trước đây (Li et al., 2023d;b) chủ yếu tập trung vào xử lý các truy vấn người dùng với một hình ảnh duy nhất thay vì các lời nhắc đa phương thức với nhiều hình ảnh và văn bản xen kẽ. Mặc dù một số VLM như Flamingo (Alayrac et al., 2022) và Kosmos-1 (Huang et al., 2023b) có thể xử lý các truy vấn người dùng với nhiều hình ảnh, dữ liệu tiền huấn luyện của chúng không thể cung cấp các lời nhắc đa phương thức phức tạp hơn so với hình ảnh và văn bản xen kẽ được thu thập từ web (Awadalla et al., 2023). Do đó, có một khoảng cách giữa các lời nhắc được sử dụng trong tiền huấn luyện các VLM này và các truy vấn người dùng trong các tình huống thực tế, mà luôn chứa nhiều hình ảnh và văn bản phức tạp hơn. Cụ thể, các VLM này có thể gặp phải ba hạn chế sau, điều này làm cho VLM kém hiệu quả hơn trong các nhiệm vụ thị giác-ngôn ngữ hạ nguồn.

Khó hiểu Tham chiếu Văn bản-đến-Hình ảnh: Các nghiên cứu trước đây hiếm khi cố gắng giải quyết vấn đề tham chiếu văn bản-đến-hình ảnh trong các lời nhắc đa phương thức. Tuy nhiên, thường có các mối quan hệ tham chiếu phức tạp giữa văn bản và hình ảnh trong các truy vấn người dùng, với các từ khác nhau đề cập đến các hình ảnh khác nhau. Ví dụ, người dùng có thể đặt một câu hỏi cụ thể về nhiều hình ảnh (Hình 1.c và Hình 1.f) hoặc sử dụng nhiều hình ảnh như các ví dụ để đặt câu hỏi chỉ về một hình ảnh cụ thể (Hình 1.d). Tuy nhiên, dữ liệu huấn luyện được sử dụng trong các nghiên cứu trước đây (Li et al., 2023d; Alayrac et al., 2022; Huang et al., 2023a) được thu thập từ web và có thể thiếu các tham chiếu văn bản-đến-hình ảnh rõ ràng. Do đó, VLM có thể thất bại trong việc xử lý các truy vấn người dùng liên quan đến các tham chiếu văn bản-đến-hình ảnh phức tạp.

Khó hiểu Mối quan hệ giữa Nhiều Hình ảnh: Thường có các mối quan hệ không gian, thời gian và logic giữa nhiều hình ảnh, và việc hiểu chính xác chúng cho phép mô hình xử lý các truy vấn người dùng tốt hơn. Tuy nhiên, dữ liệu tiền huấn luyện được sử dụng bởi các VLM trước đây (Alayrac et al., 2022) được thu thập từ internet, thiếu kết nối chặt chẽ giữa các hình ảnh, đặc biệt là khi các hình ảnh này cách xa nhau trên cùng một trang web. Điều này cản trở khả năng của VLM trong việc hiểu các mối quan hệ phức tạp giữa các hình ảnh và hạn chế hơn nữa khả năng lý luận của chúng.

Khó Học từ Các Minh chứng Đa phương thức Trong bối cảnh: Các nghiên cứu trước đây đã chỉ ra rằng các LLM được tiền huấn luyện có thể hưởng lợi từ một số ít minh chứng trong bối cảnh (Brown et al., 2020; Dong et al., 2023). Tuy nhiên, khả năng ICL của các VLM hiện tại khá hạn chế, cụ thể: 1) Các VLM như BLIP-2 (Li et al., 2023d), LLaVA (Li et al., 2023b) chỉ hỗ trợ các lời nhắc đa phương thức với một hình ảnh duy nhất, cản trở khả năng của chúng trong việc sử dụng nhiều minh chứng đa phương thức để tăng cường hiệu suất của chúng trong quá trình suy luận; 2) Mặc dù các VLM như Flamingo (Alayrac et al., 2022) hỗ trợ đầu vào nhiều hình ảnh trong quá trình tiền huấn luyện và xuất hiện khả năng ICL, các lược đồ bối cảnh của chúng thất bại trong việc cung cấp tham chiếu văn bản-hình ảnh và các hình ảnh liên quan chặt chẽ. Điều này ngăn cản chúng cung cấp các lời nhắc đủ phức tạp cho VLM, từ đó hạn chế hiệu quả của khả năng ICL của chúng. Ngoài ra, việc thiếu điều chỉnh hướng dẫn có giám sát thêm cản trở hiệu quả của chúng trong các nhiệm vụ hạ nguồn.

Trong bài báo này, để giải quyết các hạn chế đã nêu trên 1) Chúng tôi trình bày MMICL, một phương pháp mới để cho phép VLM xử lý đầu vào đa phương thức một cách hiệu quả, bao gồm các mối quan hệ giữa nhiều hình ảnh và tham chiếu văn bản-đến-hình ảnh. 2) Chúng tôi đề xuất một lược đồ bối cảnh mới trong đó kết hợp một phần khai báo hình ảnh bổ sung, cùng với việc bao gồm các token đại diện hình ảnh, tăng cường khả năng ICL của VLM. 3) Chúng tôi xây dựng một bộ dữ liệu học tập trong bối cảnh đa phương thức phù hợp với lược đồ được đề xuất. Bộ dữ liệu được điều chỉnh từ một loạt các bộ dữ liệu hiện có và có thể được sử dụng để cung cấp hỗ trợ cho việc huấn luyện các VLM có khả năng hơn.

Các thí nghiệm của chúng tôi cho thấy rằng MMICL đạt được hiệu suất tiên tiến mới trên nhiều benchmark thị giác-ngôn ngữ bao gồm MME (Fu et al., 2023) và MMBench (Liu et al., 2023d)1. Các kiểm tra toàn diện về ba hạn chế mà chúng tôi nhắm đến giải quyết cho thấy rằng MMICL thể hiện khả năng đặc biệt trong việc hiểu tham chiếu văn bản-đến-hình ảnh (cải thiện 13 điểm trên benchmark tổng hợp thị giác-ngôn ngữ, Winoground (Thrush et al., 2022a)) và các mối quan hệ phức tạp giữa các hình ảnh (cải thiện 12 điểm trên benchmark lý luận nhiều hình ảnh, RAVEN (Huang et al., 2023a)). Hơn nữa, MMICL thể hiện hiệu suất ICL đa phương thức ấn tượng trên các nhiệm vụ khác nhau. Chúng tôi cũng quan sát thấy rằng MMICL giảm thiểu hiệu quả thiên lệch ngôn ngữ, thường gây ra VLM bỏ qua nội dung thị giác khi đối mặt với bối cảnh văn bản rộng lớn, dẫn đến ảo giác.

2 MMICL

2.1 KIẾN TRÚC MÔ HÌNH

Hầu hết các VLM sử dụng Bộ sinh Lời nhắc Thị giác (VPG) (ví dụ: Resampler (Alayrac et al., 2022), Q-former (Li et al., 2023d)) để trích xuất các embedding thị giác từ các đặc trưng hình ảnh được mã hóa bởi các backbone thị giác và sử dụng các embedding thị giác để giúp LLM hiểu các đầu vào thị giác. Kiến trúc mô hình được hiển thị trong Hình 2.a thuộc về VLM tập trung vào các lời nhắc với một hình ảnh duy nhất, chẳng hạn như Blip-2 (Li et al., 2023d), luôn đặt hình ảnh ở đầu toàn bộ đầu vào và không thể xử lý các đầu vào với nhiều hình ảnh. Trong Hình 2.b, VLM với khả năng few-shot, chẳng hạn như Flamingo (Alayrac et al., 2022), mã hóa hình ảnh thành các embedding hình ảnh với số lượng token thị giác cố định và chèn các lớp cross-attention được cổng hóa mới vào LLM để tiêm các đặc trưng thị giác. Khác với công trình trước đây, MMICL được hiển thị trong Hình 2.c xử lý biểu diễn hình ảnh và văn bản một cách bình đẳng và thiết lập tham chiếu giữa hình ảnh và văn bản thông qua khai báo hình ảnh. Nó cho phép người dùng linh hoạt nhập nhiều hình ảnh và văn bản theo bất kỳ thứ tự mong muốn nào, không có hạn chế về số lượng hoặc vị trí của hình ảnh trong bối cảnh. Như được hiển thị trong Hình 5, mỗi hình ảnh được cho sẽ được mã hóa bởi một bộ mã hóa thị giác (ví dụ: ViT (Radford et al., 2021)) để có được biểu diễn hình ảnh. Sau đó, chúng tôi sử dụng Q-former như VPG để mã hóa hình ảnh thành các embedding có thể hiểu được bởi mô hình ngôn ngữ. Chúng tôi sử dụng một lớp được kết nối đầy đủ như lớp chiếu để chuyển đổi mỗi embedding thị giác thành cùng một chiều với embedding văn bản của LLM. Cuối cùng, chúng tôi kết hợp các embedding thị giác và văn bản theo kiểu xen kẽ và đưa chúng vào LLM. Thiết kế này là một mở rộng tự nhiên của cơ chế attention ban đầu trong LLM. Chúng tôi đặt trọng số để ánh xạ các vector truy vấn và giá trị trong lớp attention của LLM như có thể học được để thích nghi tốt hơn với các lời nhắc đa phương thức với nhiều hình ảnh. Thêm chi tiết được trình bày trong Phụ lục E.

2.2 THIẾT KẾ LƯỢC ĐỒ BỐI CẢNH CỦA MMICL

Trong phần này, chúng tôi phác thảo thiết kế Lược đồ Bối cảnh cho MMICL. Lược đồ được đề xuất được thiết kế để chuyển đổi thành thạo dữ liệu hình ảnh-văn bản xen kẽ thành bối cảnh huấn luyện cho MMICL.

2.2.1 KHAI BÁO HÌNH ẢNH

Người dùng có thể sử dụng mô tả văn bản để tham chiếu đến các hình ảnh cụ thể trong các truy vấn của họ. Tham chiếu như vậy có thể cung cấp thông tin về nội dung thị giác được đề cập trong văn bản cho VLM, cho phép nó học được sự căn chỉnh giữa hai phương thức. Để liên kết chính xác văn bản và hình ảnh, chúng tôi tạo thành các mẫu khai báo hình ảnh cho mỗi hình ảnh trong đầu vào hỗn hợp, như được hiển thị trong Hình 3.a. Đầu tiên, chúng tôi phân bổ một đại diện hình ảnh duy nhất ([IMG j]) để tham chiếu đến embedding thị giác của hình ảnh j, cung cấp một định danh duy nhất cho VLM để lập chỉ mục và phân biệt giữa các embedding thị giác và văn bản. Sau đó, chúng tôi sử dụng các lời nhắc ngôn ngữ tự nhiên để thiết lập tham chiếu giữa văn bản và hình ảnh. Việc kết hợp tham chiếu văn bản-đến-hình ảnh rõ ràng trong khai báo hình ảnh hỗ trợ mô hình trong việc liên kết văn bản với hình ảnh thích hợp. Đồng thời, khai báo hình ảnh, được duy trì như nội dung văn bản, cũng có thể bảo tồn tính linh hoạt để xuất hiện tại bất kỳ vị trí nào trong lời nhắc. Mỗi trường hợp Ii tuân theo cấu trúc, nơi Xi tượng trưng cho tập hợp các trang trí hình ảnh có thể được đặt bất cứ đâu trong trường hợp Ii. qi và ai biểu thị câu hỏi với hướng dẫn và câu trả lời tương ứng, lần lượt.

Ii"pXi,qi,aiq (1)

2.2.2 DỮ LIỆU ĐA PHƯƠNG THỨC VỚI HÌNH ẢNH LIÊN KẾT

Để kết hợp thông tin nhiều hình ảnh phong phú trong lược đồ bối cảnh của MMICL, chúng tôi tạo ra dữ liệu nhiều hình ảnh liên kết bao gồm các mối quan hệ không gian, logic và thời gian. Nó hỗ trợ MMICL trong việc hiểu các mối quan hệ phức tạp giữa các hình ảnh trong các truy vấn người dùng. Cụ thể, chúng tôi rút ra các khung hình từ video để xây dựng dữ liệu nhiều hình ảnh. Các khung hình được trích xuất từ video vốn duy trì mối quan hệ thời gian và không gian chặt chẽ, điều này truyền thông tin tương quan không gian và thời gian giữa các hình ảnh vào lược đồ bối cảnh. Ngoài ra, chúng tôi xây dựng dữ liệu nhiều hình ảnh từ các hình ảnh mô tả tương tác nhiều đối tượng. Chúng tôi phát hiện các đối tượng trong hình ảnh và tạo ra các hộp giới hạn cho mỗi đối tượng. Chúng tôi có được nhiều hình ảnh con của các đối tượng khác nhau bằng cách cắt hình ảnh theo các hộp giới hạn. Sau đó, chúng tôi thay thế các tham chiếu văn bản đến các đối tượng này bằng các hình ảnh đã cắt tương ứng của chúng, do đó tạo thành dữ liệu đa phương thức xen kẽ với các hình ảnh liên kết logic và nhân quả, như được mô tả trong Hình 3.b và Hình 4. Mỗi trường hợp Ii bao gồm một cặp câu hỏi-trả lời văn bản cùng với K hình ảnh, nơi xi,kPXi đại diện cho khai báo hình ảnh cho hình ảnh thứ k.

Ii"ptx1,x2, . . . ,xku,qi,aiq (2)

2.2.3 ĐỊNH DẠNG TRONG BỐI CẢNH ĐA PHƯƠNG THỨC THỐNG NHẤT CHO CÁC NHIỆM VỤ KHÁC NHAU

Chúng tôi đề xuất một thiết kế để tạo ra dữ liệu học tập trong bối cảnh đa phương thức cho các nhiệm vụ khác nhau để làm phong phú lược đồ bối cảnh của MMICL. Nó nhằm cải thiện khả năng nhận biết hướng dẫn của VLM và mở rộng khả năng của nó cho việc học tập trong bối cảnh đa phương thức thành thạo. Cụ thể, chúng tôi bắt đầu bằng cách tạo ra các hướng dẫn đa dạng cho mỗi nhiệm vụ và tạo ra các mẫu khác nhau cho nhiệm vụ sử dụng các hướng dẫn này. Sau đó, chúng tôi điền vào mẫu được chọn ngẫu nhiên với nhiệm vụ ban đầu để tập hợp dữ liệu được trang bị hướng dẫn như Phụ lục G. Hơn nữa, chúng tôi chuyển đổi dữ liệu thành định dạng trong bối cảnh đa phương thức bằng cách xây dựng một số ít ví dụ được tạo ra bởi các trường hợp lấy mẫu từ dữ liệu. Các ví dụ này được kết hợp với trường hợp đầu vào để tạo ra dữ liệu trong bối cảnh đa phương thức. Bằng cách này, chúng tôi có thể chuyển đổi tất cả các nhiệm vụ thành định dạng trong bối cảnh đa phương thức thống nhất, như được minh họa trong Hình 3.c. Phương pháp này tạo điều kiện cho việc tích lũy một lượng lớn dữ liệu chất lượng cao từ các nhiệm vụ khác nhau, làm phong phú lược đồ bối cảnh của MMICL với sự đa dạng phong phú của dữ liệu trong bối cảnh đa phương thức tràn ngập với các hướng dẫn đa dạng. Cuối cùng, điều này cải thiện khả năng tuân theo hướng dẫn và khả năng học tập trong bối cảnh đa phương thức của mô hình. Mỗi trường hợp Ii bao gồm N ví dụ.

Ii"ptP1,¨¨¨,PNu,Xi,qi,aiq (3)

Mỗi ví dụ Pj"pXj,qj,ajq, Xj biểu thị khai báo hình ảnh của ví dụ thứ j. qj và aj biểu thị câu hỏi và câu trả lời cho ví dụ thứ j, lần lượt.

2.3 XÂY DỰNG BỘ DỮ LIỆU HỌC TẬP TRONG BỐI CẢNH ĐA PHƯƠNG THỨC (MIC)

Để giúp VLM hiểu các lời nhắc phức tạp, chúng tôi xây dựng bộ dữ liệu MIC bằng cách thu thập dữ liệu từ các nguồn dữ liệu công cộng và chuyển đổi chúng dựa trên lược đồ bối cảnh. Nó có ba khía cạnh chính: 1) khai báo hình ảnh, 2) dữ liệu đa phương thức với các hình ảnh liên quan chặt chẽ, và 3) dữ liệu trong bối cảnh đa phương thức cho các nhiệm vụ khác nhau. Tập huấn luyện của MIC đến từ 16 bộ dữ liệu trên 8 danh mục, trong khi tập kiểm tra đến từ 18 bộ dữ liệu trên 10 danh mục.

Bộ dữ liệu của chúng tôi được xây dựng tự động dựa trên các bộ dữ liệu hiện có. Đầu tiên, chúng tôi tạo ra một khai báo hình ảnh cho mỗi trường hợp trong tất cả các bộ dữ liệu để tạo ra các bộ dữ liệu với tham chiếu văn bản-đến-hình ảnh rõ ràng. Thứ hai, chúng tôi tạo ra một mẫu hướng dẫn cho mỗi bộ dữ liệu và yêu cầu Chatgpt viết lại các hướng dẫn, điền vào dữ liệu từ các bộ dữ liệu hiện có để có được một bộ dữ liệu với các định dạng hướng dẫn đa dạng. Cuối cùng, chúng tôi sử dụng các bộ dữ liệu đó với hướng dẫn để xây dựng bộ dữ liệu MIC theo lược đồ bối cảnh được đề xuất của chúng tôi. Đối với ví dụ được trình bày trong Hình 3 và Hình 4 (ví dụ: hai người cãi nhau với nhau), chúng tôi xây dựng dữ liệu dựa trên các chú thích hiện có (tức là các hộp giới hạn và mối quan hệ giữa các hộp giới hạn) được cung cấp bởi bộ dữ liệu VCR (Zellers et al., 2019). Ngoài ra, chúng tôi cũng xây dựng một bộ dữ liệu học tập trong bối cảnh bằng cách lấy mẫu các ví dụ từ bộ dữ liệu ban đầu. Chúng tôi cũng trích xuất tám khung hình trên mỗi video từ các bộ dữ liệu video để tạo ra dữ liệu đa phương thức với các hình ảnh liên kết. Chi tiết được trình bày tại Phụ lục D.

Chúng tôi chuyển đổi tất cả dữ liệu thành định dạng Q&A thị giác-ngôn ngữ để tạo ra dữ liệu huấn luyện đa phương thức chất lượng cao và tích lũy 5.8M mẫu trong bộ dữ liệu MIC. Do hạn chế về tài nguyên, chúng tôi sử dụng khoảng 10% dữ liệu với chiến lược lấy mẫu được mô tả trong Phụ lục F để tinh chỉnh MMICL. Dự kiến rằng một mô hình lớn hơn được huấn luyện trên tất cả dữ liệu của chúng tôi sẽ mang lại kết quả hứa hẹn hơn.

2.4 MÔ HÌNH HUẤN LUYỆN

Giai đoạn I: Tiền huấn luyện. Giai đoạn này nhằm hỗ trợ mô hình trong việc căn chỉnh các embedding hình ảnh và văn bản. Trong giai đoạn này, cả bộ mã hóa thị giác và LLM đều được đóng băng. VPG (tức là Q-Former) và lớp chiếu được huấn luyện để học các embedding thị giác có thể được diễn giải bởi LLM.

Giai đoạn II: Điều chỉnh Trong bối cảnh Đa phương thức. Trong giai đoạn này, chúng tôi nhằm giải quyết các hạn chế đã nêu trên và đưa mô hình của chúng tôi lên một bước tiến xa hơn bằng cách mở rộng nó sang học tập trong bối cảnh đa phương thức. Cụ thể, chúng tôi nhằm làm cho mô hình hiểu được các mối quan hệ tham chiếu phức tạp giữa văn bản và hình ảnh và các mối quan hệ phức tạp giữa nhiều hình ảnh và cuối cùng có được khả năng học tập trong bối cảnh đa phương thức thành thạo. Do đó, chúng tôi thực hiện Điều chỉnh Trong bối cảnh Đa phương thức trên bộ dữ liệu MIC. Trong giai đoạn II, chúng tôi đóng băng bộ mã hóa hình ảnh, Q-former và LLM trong khi huấn luyện chung lớp chiếu và các vector truy vấn và giá trị. Chi tiết có thể được tìm thấy trong Phụ lục H.

3 THÍ NGHIỆM

3.1 THIẾT LẬP THÍ NGHIỆM

Thiết lập Đánh giá. Chúng tôi nhằm phát triển các VLM đa năng có thể thích nghi tổng quát với các lời nhắc đa phương thức đa dạng, thách thức. Do đó, chúng tôi đánh giá các mô hình của chúng tôi trong một số benchmark thị giác-ngôn ngữ, bao gồm các nhiệm vụ liên quan đến hình ảnh và video. Các số liệu được sử dụng trong các benchmark này và chi tiết thêm được hiển thị trong Phụ lục M.

Mô hình và Đường chuẩn. Chúng tôi cung cấp hai phiên bản của MMICL: (1) MMICL (FLAN-T5) sử dụng BLIP-2 (Li et al., 2023d) như backbone và (2) MMICL (Instruct-FLAN-T5) sử dụng Instruct-BLIP (Dai et al., 2023) như backbone. Chúng tôi cũng áp dụng XL và XXL của FLANT5 (Chung et al., 2022) cho cả hai phiên bản. Chúng tôi so sánh MMICL với các đường chuẩn mạnh sau: Flamingo (Alayrac et al., 2022), KOSMOS-1 (Huang et al., 2023a), BLIP-2-FLAN-T5, InstructBLIP-FLAN-T5, Shikra (Chen et al., 2023a), Otter (Li et al., 2023a), Ying-VLM (Li et al., 2023e). Chi tiết về MMICL và các đường chuẩn được hiển thị trong Phụ lục H và Phụ lục O.

3.2 ĐÁNH GIÁ HIỆU SUẤT TỔNG QUÁT

Chúng tôi đánh giá hiệu suất tổng quát của MMICL trên cả benchmark MME (Fu et al., 2023) và MMBench (Liu et al., 2023d)2. MME đánh giá VLM với 14 nhiệm vụ phụ bao gồm khả năng nhận thức và tri giác. Kết quả trong Bảng 1 cho thấy rằng MMICL có thể đạt được điểm trung bình tốt nhất so với các VLM hiện tại trên các nhiệm vụ nhận thức và tri giác. MMICL cũng thể hiện hiệu suất xuất sắc và vượt trội đáng kể so với các VLM khác trên benchmark MMBench, cái mà đánh giá kỹ lưỡng các kỹ năng đa dạng của VLM. Kết quả chi tiết được trình bày trong Bảng 22. Xem Phụ lục I và J để biết chi tiết đánh giá của MMICL và so sánh với các VLM khác.

3.3 THĂM DÒ HIỆU SUẤT

3.3.1 HIỂU THAM CHIẾU VĂN BẢN-ĐẾN-HÌNH ẢNH

Winoground (Thrush et al., 2022b) đề xuất một nhiệm vụ khớp chính xác hai hình ảnh và chú thích được cho, như được mô tả ở bên trái của Hình 6. Thách thức nằm ở chỗ cả hai chú thích đều bao gồm chính xác cùng những từ, mặc dù theo thứ tự khác nhau. VLM phải so sánh cả hình ảnh và văn bản để phân biệt sự khác biệt tinh tế của chúng và nắm bắt tham chiếu ngầm giữa chúng. Do đó, chúng tôi chọn Winoground để đánh giá liệu VLM có hiểu tham chiếu văn bản-đến-hình ảnh hay không. MMICL được cho hai hình ảnh và hai chú thích trong mỗi lời nhắc trong quá trình đánh giá. Kết quả trong Bảng 2 chứng minh rằng MMICL nắm bắt được mối quan hệ tham chiếu giữa hình ảnh và văn bản, vượt trội hơn các đường chuẩn trước đây.

3.3.2 HIỂU MỐI QUAN HỆ HÌNH ẢNH-ĐẾN-HÌNH ẢNH PHỨC TẠP

Kiểm tra RAVEN (Zhang et al., 2019; Huang et al., 2023a) được sử dụng rộng rãi để đánh giá khả năng lý luận phi ngôn ngữ của VLM. Mỗi trường hợp có 3 hoặc 8 hình ảnh như đầu vào và 6 hình ảnh ứng viên với một câu trả lời duy nhất, và mục tiêu là dự đoán hình ảnh đúng như được hiển thị ở bên phải của Hình 6. Nó đòi hỏi kỹ năng thị giác và logic để hiểu các mối quan hệ giữa các hình ảnh. Chúng tôi tiến hành các thí nghiệm zero-shot trên kiểm tra Raven để đánh giá khả năng của VLM trong việc hiểu mối quan hệ hình ảnh-đến-hình ảnh. Kết quả trong Bảng 3 cho thấy rằng MMICL đạt được cải thiện 12 điểm so với KOSMOS-1. Nó chỉ ra rằng MMICL có thể nắm bắt các mối quan hệ hình ảnh-đến-hình ảnh phức tạp và tiến hành các nhiệm vụ lý luận thị giác phi ngôn ngữ.

3.4 HỌC TẬP TỪ CÁC MINH CHỨNG ĐA PHƯƠNG THỨC TRONG BỐI CẢNH

Như được hiển thị trong Bảng 4, chúng tôi đánh giá khả năng học tập trong bối cảnh đa phương thức của MMICL trên các nhiệm vụ thị giác-ngôn ngữ khác nhau. MMICL vượt trội hơn các VLM khác trên cả các bộ dữ liệu held-in và held-out và đạt được hiệu suất few-shot tiên tiến. Ví dụ, đánh giá few-shot (4-shot) của MMICL trên benchmark VizWiz vượt trội hơn đường chuẩn Flamingo-9B (Alayrac et al., 2022) và KOSMOS-1 (Huang et al., 2023b) lần lượt 15.38 và 14.98 điểm. Vì VizWiz chưa bao giờ được tiếp xúc trong dữ liệu huấn luyện, điều vượt trội này gợi ý khả năng của MMICL trong việc tổng quát hóa đến các nhiệm vụ mới với một vài ví dụ. Hiệu suất few-shot của Flickr30K giảm với các ví dụ được cho vì các ví dụ chú thích có thể cung cấp nhiễu cho VLM để hoàn thành nhiệm vụ (tức là các ví dụ trong bối cảnh thường không cung cấp gợi ý cho các mô hình thực hiện các nhiệm vụ chú thích hình ảnh).

3.5 ẢO GIÁC VÀ THIÊN LỆ NGÔN NGỮ CỦA VLM

Các VLM hiện tại có ảo giác thị giác đáng kể (Li et al., 2023f), ngăn cản VLM hưởng lợi từ ICL đa phương thức. Đặc biệt khi xử lý các lời nhắc phức tạp với nhiều hình ảnh (ví dụ: chuỗi suy nghĩ đa phương thức (Zhang et al., 2023b)), VLM thường bỏ qua nội dung thị giác khi đối mặt với văn bản rộng lớn. Thiên lệch ngôn ngữ này làm giảm hiệu quả của chúng trong việc trả lời các câu hỏi đòi hỏi cả hình ảnh và văn bản. ScienceQA-IMG (Lu et al., 2022) là một nhiệm vụ thách thức đòi hỏi một mô hình sử dụng cả hai phương thức để trả lời câu hỏi. Chúng tôi chia thủ công bộ dữ liệu thành hai nhóm: các câu hỏi cần hình ảnh để trả lời và những câu hỏi không cần. Chi tiết được trình bày trong Phụ lục Q. Các thí nghiệm rộng rãi trong Bảng 5 chứng minh rằng MMICL giảm thiểu hiệu quả thiên lệch ngôn ngữ vì nó hoạt động tốt như nhau trong cả hai nhóm. Chúng tôi cũng kiểm tra ảo giác đối tượng trong MMICL trong Phụ lục L, cái mà cho thấy hiệu suất ấn tượng.

3.6 NGHIÊN CỨU LOẠI BỎ

Nghiên cứu Loại bỏ về Mô hình Huấn luyện: Chúng tôi tiến hành một nghiên cứu loại bỏ trên các nhiệm vụ khác nhau để đánh giá hiệu ứng của điều chỉnh trong bối cảnh đa phương thức. Bảng 6 hiển thị một sự tăng cường đáng kể do điều chỉnh trong bối cảnh đa phương thức. Nó có thể được quan sát trên tất cả các loại và kích thước mô hình, đặc biệt là đối với các nhiệm vụ liên quan đến nhiều hình ảnh. Điều này chỉ ra rằng với sự giúp đỡ của Giai đoạn II, MMICL có thể xử lý các lời nhắc đa phương thức phức tạp và hoàn thành các nhiệm vụ thách thức với nhiều hình ảnh. Kết quả trong Phụ lục K cũng xác nhận điểm này với hiệu suất xuất sắc của MMICL trên các bộ dữ liệu video.

Nghiên cứu Loại bỏ về Lược đồ Bối cảnh: Chúng tôi tiến hành các thí nghiệm loại bỏ sử dụng InstructBLIP-FLANT5-XL như mô hình backbone trong các cài đặt khác nhau để xác minh hiệu quả của lược đồ bối cảnh, nhằm xác định chính xác nguồn gốc thúc đẩy cải thiện trong các khả năng nhất định của mô hình chúng tôi. Như được hiển thị trong Bảng 7, sự vượt trội của MMICL được thúc đẩy bởi tác động tập thể của các yếu tố thiết kế của chúng tôi—loại bỏ bất kỳ thành phần nào không thể đảm bảo hiệu suất vượt trội của mô hình chúng tôi. Mỗi thành phần của thiết kế chúng tôi đóng góp đáng kể vào các khía cạnh khác nhau của mô hình chúng tôi. Chi tiết và phân tích thêm có sẵn trong Phụ lục N.

4 KẾT LUẬN

Trong bài báo này, chúng tôi nêu bật các hạn chế của VLM trong việc xử lý các lời nhắc đa phương thức phức tạp với nhiều hình ảnh, điều này làm cho VLM kém hiệu quả hơn trong các nhiệm vụ thị giác-ngôn ngữ hạ nguồn. Chúng tôi giới thiệu MMICL để giải quyết các hạn chế đã nêu trên và đưa mô hình của chúng tôi lên một bước tiến xa hơn bằng cách mở rộng nó sang học tập trong bối cảnh đa phương thức. Đột phá này cho phép VLM hiểu tốt hơn các lời nhắc đa phương thức phức tạp. Hơn nữa, MMICL thiết lập hiệu suất tiên tiến mới trên các benchmark VLM tổng quát và các benchmark lý luận đa phương thức phức tạp.

5 LỜI CẢM ƠN

Chúng tôi bày tỏ lòng biết ơn sâu sắc đến các nhà đánh giá ẩn danh mà sự tận tâm và phản hồi sâu sắc của họ đã nâng cao đáng kể chất lượng của bài báo này. Những phê bình mang tính xây dựng và gợi ý quý giá của họ đã là công cụ trong việc hoàn thiện công trình của chúng tôi. Ngoài ra, chúng tôi đánh giá cao sâu sắc các Chủ tịch Chương trình và Chủ tịch Khu vực vì việc xử lý tỉ mỉ bài nộp của chúng tôi và phản hồi toàn diện và vô giá của họ. Sự hướng dẫn của họ đã là then chốt trong việc nâng cao chất lượng nghiên cứu của chúng tôi.

Công trình này được hỗ trợ bởi Quỹ Khoa học Quốc gia Trung Quốc theo Số tài trợ 61936012 và 61876004.

TÀI LIỆU THAM KHẢO

[Danh sách các tài liệu tham khảo được dịch tương tự với phần trước, bao gồm tất cả các tác giả, tiêu đề, tạp chí, và thông tin xuất bản được dịch sang tiếng Việt]
