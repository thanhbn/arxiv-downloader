# 2309.11210.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.11210.pdf
# Kích thước tệp: 378327 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
NÓI TRONG KHI BẠN NGHĨ:
TỔNG HỢP GIỌNG NÓI STREAMING TRONG QUA TRÌNH TẠO VĂN BẢN
Avihu Dekel, Slava Shechtman, Raul Fernandez, David Haws, Zvi Kons, Ron Hoory
IBM Research
TÓM TẮT
Các Mô hình Ngôn ngữ Lớn (LLMs) thể hiện khả năng ấn tượng, 
tuy nhiên việc tương tác với các mô hình này chủ yếu được 
thực hiện thông qua văn bản. Việc sử dụng Text-To-Speech để 
tổng hợp đầu ra của LLM thường dẫn đến độ trễ đáng chú ý, 
điều này không thực tế cho các cuộc hội thoại bằng giọng nói 
lưu loát. Chúng tôi đề xuất LLM2Speech, một kiến trúc để 
tổng hợp giọng nói trong khi văn bản đang được tạo ra bởi 
LLM, mang lại việc giảm độ trễ đáng kể. LLM2Speech bắt chước 
các dự đoán của mô hình giáo viên không streaming trong khi 
hạn chế việc tiếp xúc với bối cảnh tương lai để cho phép 
streaming. Nó khai thác các embedding ẩn của LLM, một sản 
phẩm phụ của việc tạo văn bản chứa ngữ cảnh ngữ nghĩa thông 
tin. Kết quả thực nghiệm cho thấy LLM2Speech duy trì chất 
lượng của giáo viên trong khi giảm độ trễ để cho phép các 
cuộc hội thoại tự nhiên.
Từ khóa chỉ mục —TTS Tăng dần, Tạo Giọng nói, 
Mô hình Ngôn ngữ Lớn (LLMs)

1. GIỚI THIỆU
Sự xuất hiện của các Mô hình Ngôn ngữ Lớn hội thoại 
(LLMs) [1, 2] đã cách mạng hóa phạm vi tương tác với máy 
tính. Bằng cách tận dụng các nguyên tắc tự học và lượng lớn 
dữ liệu huấn luyện không nhãn, LLMs đã thiết lập một trạng 
thái nghệ thuật mới trên nhiều nhiệm vụ khác nhau, và cho 
thấy tiềm năng lớn như một công cụ để tăng cường trí tuệ con 
người. Hiện tại, việc tương tác với LLMs thường được thực 
hiện thông qua văn bản mặc dù trong nhiều ứng dụng, chẳng 
hạn như hỗ trợ lái xe, phương thức nói là ưu tiên hơn nhiều, 
trực quan và an toàn hơn. Kết quả là, các LLMs dựa trên âm 
thanh thuần túy [3, 4] đang nhận được sự quan tâm trong 
cộng đồng, mặc dù khả năng hiểu ngôn ngữ ngữ nghĩa của 
chúng vẫn còn thua kém so với LLMs văn bản. Một phương pháp 
thay thế đơn giản cho LLMs nói mà sẽ giải quyết vấn đề này 
là kết hợp một mô hình ngôn ngữ dựa trên văn bản với hệ 
thống Text-To-Speech (TTS) thần kinh có khả năng tạo ra các 
mẫu giọng nói chất lượng cao [5, 6]. Tuy nhiên, các mô hình 
TTS thường yêu cầu toàn bộ câu để tạo ra giọng nói tự nhiên, 
dẫn đến độ trễ đáng chú ý khi kết hợp với LLM thường tạo 
văn bản theo cách tự hồi quy chậm. Công trình này giải quyết 
một số thách thức phát sinh khi cố gắng đọc to văn bản được 
tạo bởi LLM, một cách tăng dần và với độ trễ tối thiểu.

Các hệ thống TTS thường áp dụng quy trình hai bước, đầu tiên 
chuyển đổi graphemes thành phonemes (G2P) và sau đó chuyển 
đổi phones thành giọng nói, vì quy trình này thường đạt được 
chất lượng và ổn định được cải thiện so với các phương pháp 
character-to-speech cho các ngôn ngữ có chính tả bất thường 
(ví dụ: tiếng Anh, tiếng Pháp) [7, 8]. Các phương pháp G2P 
phụ thuộc ngữ cảnh nhìn ra ngoài mức unigram để cải thiện 
chất lượng dự đoán âm vị [9, 10, 11]. Chẳng hạn, các mô 
hình như vậy có thể tính toán tốt hơn cho flapping ranh giới 
giữa các từ, giảm nguyên âm và phân biệt heteronym. Tuy 
nhiên, trong các mô hình này, ngữ cảnh cần thiết cho việc 
phân biệt có thể dài, khiến chúng không phù hợp cho 
streaming. Các nghiên cứu TTS tăng dần tạo ra các hệ thống 
TTS độ trễ thấp với lookahead hạn chế (tức là tiếp xúc với 
ngữ cảnh tương lai) và suy giảm tối thiểu [12, 13, 14, 15]. 
Trong hầu hết các tình huống, toàn bộ văn bản có sẵn trước 
khi tổng hợp, và trọng tâm là giảm độ trễ thuật toán. Ví dụ, 
các phương pháp này có thể chạy một mô-đun G2P nhẹ toàn 
câu trước khi tổng hợp mà không đóng góp đáng kể vào độ trễ 
tổng thể. Tuy nhiên, khi xem xét luồng văn bản được tạo ra 
chậm, giả định này không còn đúng nữa.

Bài báo này giới thiệu LLM2Speech, một hệ thống tích hợp 
LLM tạo sinh với hệ thống TTS có thể streaming. LLM2Speech 
có thể nói to văn bản trong khi nó đang được tạo ra bởi LLM, 
mà không ảnh hưởng đến tính chính xác hoặc tự nhiên. 
LLM2Speech sử dụng các embedding của LLM, một sản phẩm phụ 
của việc tạo văn bản chứa thông tin ngữ nghĩa và có thể bù 
đắp cho việc thiếu ngữ cảnh tương lai trong streaming. 
LLM2Speech bao gồm ba phần (Hình 1):

Hình 1: Tổng hợp giọng nói streaming trong quá trình tạo văn 
bản. Các token từ và embedding được tạo ra tăng dần bởi LLM, 
gửi đến LLM2PnP để tạo ra phones và prosody (PnP), sau đó 
được gửi đến PnP2Speech để tạo ra âm thanh.

1. Một LLM được huấn luyện trước, mà chúng tôi cố ý đóng 
băng do nỗ lực tính toán và con người khổng lồ được đầu tư 
để đưa nó đến trạng thái cuối cùng [16].

--- TRANG 2 ---
2. LLM2PnP: một bộ chuyển đổi chuyển đổi đầu ra LLM thành 
Phones và Prosody (PnP), được mô tả trong Mục 2.2.
3. PnP2Speech: một phiên bản có thể streaming của hệ thống 
TTS trong [17], hoạt động trên các khối PnP (xem Mục 2.3).

LLM2PnP được huấn luyện trên một tập dữ liệu văn bản lớn 
thông qua chưng cất kiến thức offline-to-streaming [18, 19] 
trong đó nó cố gắng bắt chước các dự đoán của mô hình giáo 
viên có quyền truy cập vào toàn bộ văn bản. Chúng tôi đánh 
giá LLM2Speech so với TTS giáo viên offline về độ chính xác 
âm vị cũng như chất lượng của giọng nói tổng hợp. Chúng tôi 
chứng minh rằng chất lượng tổng thể được duy trì bằng cách 
sử dụng cả các biện pháp khách quan và định tính. LLM2Speech 
cho thấy các dự đoán prosodic ấn tượng ngay cả đối với các 
đầu vào biểu cảm (ví dụ: vui vẻ, đồng cảm, không chắc 
chắn), và cũng có thể tổng hợp các từ xen kẽ và tạm dừng 
(ví dụ: hmm, uh-huh, oh, v.v.).¹

Công trình được đề xuất ở đây đưa ra những đóng góp mới sau 
đây cho lĩnh vực tổng hợp giọng nói hội thoại:
1. Nó giới thiệu một pipeline chuyển đổi văn bản đầu ra LLM 
thành giọng nói biểu cảm một cách tăng dần và với độ trễ thấp.
2. Nó đề xuất một phương pháp chưng cất kiến thức streaming 
để huấn luyện các mô hình PnP dựa trên các tập dữ liệu văn 
bản lớn.
3. Nó định lượng các đóng góp của các embedding ẩn LLM cho 
nhiệm vụ dự đoán PnP.

2. PHƯƠNG PHÁP
2.1. Tạo tập dữ liệu
Vì LLM2Speech sử dụng các token và embedding của LLM, nó 
được huấn luyện cho một LLM cụ thể. Chúng tôi thực nghiệm 
với mô hình ngôn ngữ T5 [20], do khả năng thực hiện các 
nhiệm vụ tạo sinh có điều kiện đa dạng. Cụ thể, chúng tôi sử 
dụng T5-lm-adapt, được tinh chỉnh để hoàn thành văn bản.

Chúng tôi xây dựng tập dữ liệu huấn luyện dựa trên tập dữ 
liệu C4 (Common Crawl Cleaned Corpus) [20], cũng được sử 
dụng để huấn luyện T5. Vì C4 chứa 365M mẫu, chúng tôi chỉ 
xem xét một phần ngẫu nhiên của tập dữ liệu chứa 3M mẫu 
huấn luyện và 130K mẫu xác thực. Mỗi mẫu trong C4 chứa một 
đoạn văn, mà chúng tôi chia ngẫu nhiên thành hai phần: 
context và text-to-predict (t2pred), sao cho t2pred có 1-5 
câu. Chúng tôi mô phỏng tạo văn bản có điều kiện trong đó 
LLM được nhắc với một context và tạo ra t2pred bằng cách 
nhập context vào T5 encoder và t2pred vào T5 decoder.² Các 
đầu vào cho nhiệm vụ huấn luyện LLM2PnP là các token từ cho 
t2pred và các embedding ngữ cảnh của chúng (xem Hình 2). 
Chúng tôi cũng thu được các chú thích PnP cho t2pred bằng 
cách sử dụng mô hình giáo viên không có hạn chế lookahead. 
Mô hình giáo viên PnP là một mô hình G2P dựa trên quy tắc 
dự đoán chuỗi âm vị và loại cụm từ, theo sau là một mô hình 
thần kinh dự đoán Hierarchical Prosodic Controls (HPCs) [21] 
cho phong cách nói chuyện biểu cảm [22] và thời lượng phone.

¹ Các mẫu âm thanh có thể được tìm thấy tại: https://ibm.biz/BdMe5X
² Khi sử dụng các LLM chỉ decoder, việc chia văn bản không cần thiết.

Hình 2: Tạo tập dữ liệu. Trên: trích xuất token và embedding 
ngữ cảnh từ LLM, được điều kiện hóa trên context. Dưới: 
pseudo-labeling cho phones và prosody, được tạo ra bởi mô 
hình giáo viên.

HPCs là các thống kê prosodic độc lập với người nói có thể 
được tính toán từ các bản ghi ở các độ phân giải khác nhau 
và đã được sử dụng cho các nhiệm vụ khác nhau [22, 17]. 
Chúng tôi sử dụng các HPC thời lượng và cao độ [17] được bổ 
sung với log-energy tối đa, được đánh giá ở các cấp bậc câu, 
từ và phone. Để tính đến các mở rộng chuẩn hóa văn bản (ví 
dụ: chuyển đổi 23 thành twenty-three), chúng tôi phân biệt 
giữa các dấu phân cách từ thông thường và inner, trong đó 
các dấu phân cách từ inner chỉ được đặt khi có mở rộng xảy 
ra. Trong quá trình suy luận, LLM2PnP tổng hợp một từ cho 
đến khi đạt đến dấu phân cách từ thông thường, và sau đó chờ 
LLM tạo ra từ tiếp theo.

2.2. LLM2PnP
LLM2PnP là một mô hình transformer encoder-decoder, được bổ 
sung với các hạn chế attention. Đầu vào encoder là một chuỗi 
các token (các phần từ) và các embedding ngữ cảnh của 
chúng, thu được từ các lớp ẩn của LLM, được chiếu vào 
encoder bằng một lớp tuyến tính. Các đầu ra decoder được sử 
dụng bởi ba mô-đun dự đoán dự đoán danh tính, đặc trưng 
prosodic và loại cụm từ của phone tiếp theo.

2.2.1. Attention bị hạn chế
Để hạn chế sự phụ thuộc vào ngữ cảnh tương lai, chúng tôi 
chính thức hóa attention bị hạn chế với một lookahead từ cố 
định L. Trước tiên chúng tôi định nghĩa chuỗi các từ w₁, 
..., wₙ, các token từ t₁, ..., tₘ và các token PnP p₁, ..., 
pₖ. Mỗi token từ tⱼ là một phần của một từ wᵢ nào đó, mà 
chúng tôi ký hiệu là word(tⱼ) = i. Tương tự, đối với mỗi 
phoneme pⱼ và từ wᵢ của nó, chúng tôi ký hiệu word(pⱼ) = i. 
Bây giờ chúng tôi ký hiệu rằng một token đầu ra y có thể chú 
ý đến một token đầu vào x bằng y→x. Trong attention encoder 
thông thường và attention encoder-decoder [23], ∀i, j chúng 
ta có tᵢ→tⱼ và pᵢ→tⱼ (xem Hình 3a). Chúng tôi định nghĩa 
attention encoder và encoder-decoder bị hạn chế như sau:

tᵢ→tⱼ ⟺ word(tⱼ) ≤ word(tᵢ) (1)
pᵢ→tⱼ ⟺ word(tⱼ) ≤ word(pᵢ) + L (2)

Chúng tôi chọn sử dụng L trong attention encoder-decoder vì 
nó sẽ không tăng trong các lớp decoder liên tiếp, không giống 
như attention encoder, trong đó lookahead sẽ tăng tuyến tính 
với số lượng lớp.

--- TRANG 3 ---
Hình 3b trực quan hóa attention bị hạn chế, làm nổi bật rằng 
t₁̸→t₃ và p₁̸→t₃, điều này đảm bảo dự đoán của p₁ sẽ không 
phụ thuộc vào t₃.

(a) Attention thông thường
(b) Attention bị hạn chế

Hình 3: Attention thông thường/bị hạn chế (được tô màu theo 
từ). Trong (a), mọi token encoder/decoder có thể chú ý đến 
mọi token encoder. Trong (b), sự phụ thuộc vào ngữ cảnh 
tương lai bị giới hạn ở từ hiện tại (L = 0) như trong Phương 
trình 1-2, cho phép streaming.

2.3. PnP2Speech
PnP2Speech là một phiên bản có thể streaming của mô hình 
Parallel Prosody Transfer (PPT) dựa trên HPC [17], bao gồm 
một mô hình acoustic, dựa trên backbone non-attentive 
Tacotron (NAT) [24], theo sau là một vocoder LPCNet nhẹ và 
có thể streaming [25]. PnP2Speech hoạt động trên các khối 
đầu vào nhỏ thay vì toàn bộ chuỗi, và yêu cầu các thay đổi 
đối với [17] được mô tả dưới đây. Đầu tiên, các BLSTM [26] 
được chia thành khối, do đó trở thành các lớp LC-BLSTM [27] 
với lookahead bằng không (la = 0). Tiếp theo, trường tiếp 
nhận phải ngày càng tăng của các lớp mạng nơ-ron tích chập 
(CNN) được giải quyết bằng cách sử dụng CNN có hạn chế 
lookahead (LC-CNN). Các tích chập kernel đối xứng được sử 
dụng miễn là ràng buộc lookahead được đáp ứng; nếu không, 
các tích chập kernel xiên (một tổng quát của tích chập 
causal [28]) được áp dụng, dẫn đến lookahead bị hạn chế 
(Xem Hình 4). Cuối cùng, trong suy luận, chúng tôi bao gồm 
các guardbands khi chia ma trận upsampling Gaussian PnP-to-
frame [24], vì upsampling phụ thuộc vào PnP tương lai liền kề.

(a) CNN
(b) LC-CNN (la = 1)

Hình 4: Trực quan hóa trường tiếp nhận của p₅ (màu cam) ở 
đầu ra của hai lớp tích chập xếp chồng với kernel k = 3. 
Trong Hình 4a, trường tiếp nhận phải (tức là lookahead) tăng 
theo số lượng lớp. Trong Hình 4b, tích chập thứ 2 bị xiên 
để lookahead cuối cùng sẽ chính xác là la = 1.

Chúng tôi huấn luyện PnP2Speech trên một corpus giọng nói 
hội thoại độc quyền 6.5 giờ được ghi bởi một nữ diễn viên 
chuyên nghiệp người Mỹ nói tiếng Anh. Tập này chứa nhiều 
loại hành động đối thoại biểu cảm và từ xen kẽ và đã được 
mô tả trong [22]. Để cải thiện thêm hiệu suất của các từ xen 
kẽ và phong cách biểu cảm, chúng tôi đã tinh chỉnh LLM2PnP 
trên cùng corpus được mô tả ở trên, huấn luyện nó để dự đoán 
tập PnP hội thoại từ văn bản hội thoại.

3. THỰC NGHIỆM
Trong các thực nghiệm sau đây, LLM2PnP có 4 lớp encoder và 
6 lớp decoder, mỗi lớp có kích thước token/feedforward là 
512/768 và 4 attention heads. LLM2PnP có lookahead một từ 
duy nhất và sử dụng các embedding T5-Base từ các lớp 2, 6 
và 10 (trong số 12 lớp, được đánh số từ đầu vào đến đầu ra). 
PnP2Speech có kích thước frame là 256 samples cho giọng nói 
được lấy mẫu 22kHz. Encoder âm vị của nó có 3 lớp LC-CNN 
và một lớp BLSTM chia khối duy nhất, theo sau là upsampling 
Gaussian, sau đó decoder LSTM tự hồi quy, và PostNet LC-CNN 
5 lớp. Các lớp LC-CNN có kích thước kernel là 5 và lookahead 
là 2. Các lớp BLSTM chia khối có kích thước khối là 4, và 
Gaussian upsampler sử dụng guardband 2-phone. Hệ thống 
PnP2Speech được đề xuất dẫn đến độ trễ thuật toán là 6 token 
PnP cộng với 2 frame, tương đương khoảng một từ (trong đó 
dấu phân cách từ và tạm dừng cũng được coi là token PnP). 
Do đó, tổng lookahead LLM2Speech cộng lại bằng 2 từ.

3.1. Đánh giá chất lượng và tự nhiên
Chúng tôi đã crowd-source một bài kiểm tra nghe Mean 
Opinion Score (MOS) để đánh giá chất lượng và tự nhiên của 
LLM2Speech, so sánh ba hệ thống:
1. Teacher: Mô hình PnP giáo viên không thể streaming, theo 
sau là TTS không streaming [17].
2. LLM2Speech (Của chúng tôi): sử dụng LLM2PnP để trích 
xuất PnP, theo sau là PnP2Speech.
3. Stream-Teacher: buộc giáo viên vào cùng lookahead như 
LLM2Speech, bằng cách sử dụng G2P giáo viên trên các tiền 
tố văn bản với lookahead 1 từ, mô hình prosody với hạn chế 
lookahead, và PnP2Speech để tổng hợp.

Mô hình    Lookahead    MOS
Teacher    ∞            4.10±0.04
LLM2Speech    2         4.12±0.04
Stream-Teacher 2        3.46±0.06

Bảng 1: Kết quả kiểm tra nghe, báo cáo MOS và khoảng tin 
cậy 95%.

Âm thanh tổng hợp được đánh giá trên 45 văn bản hội thoại, 
và được đánh giá về chất lượng và tự nhiên tổng thể bởi 25 
người nghe bản ngữ trên thang điểm MOS 5 điểm tiêu chuẩn. 
Kết quả trong Bảng 1 cho thấy không có sự khác biệt có ý 
nghĩa thống kê giữa giáo viên và LLM2Speech có thể streaming.

--- TRANG 4 ---
Lưu ý rằng giáo viên đưa ra dự đoán dựa trên toàn bộ văn 
bản (điều này không thực tế cho streaming) và sử dụng nhãn 
phụ-phong cách của văn bản (ví dụ: đồng cảm, vui vẻ, v.v.) 
mà LLM2Speech không được tiếp xúc.

3.2. Nghiên cứu loại bỏ G2P
Chúng tôi đánh giá hiệu suất G2P của LLM2PnP bằng cách đo 
word error rate (WER) trên tập xác thực C4. Chúng tôi nhấn 
mạnh sự khác biệt bằng cách trình bày kết quả trên các tập 
con thách thức sau: (i) Rare: các từ ít phổ biến nhất bao 
phủ 20% văn bản, (ii) Norm: các từ được mở rộng bởi chuẩn 
hóa, ví dụ: 23, và (iii) OOV: các từ không được nhìn thấy 
trong quá trình huấn luyện.

Các phương pháp TTS tăng dần đánh đổi giữa độ trễ và hiệu 
suất, được xác định bởi lookahead. Trong Bảng 2, chúng tôi 
cho thấy tác động đến hiệu suất G2P bằng cách sửa đổi 
lookahead của LLM2PnP. Kết quả cho thấy từ lookahead đầu 
tiên khá quan trọng trong khi từ thứ hai mang lại lợi ích 
nhỏ hơn. Quan sát này có thể được giải thích một phần bởi 
các quá trình post-lexical trong tiếng Anh Mỹ ảnh hưởng đến 
cách phát âm của một từ tùy thuộc vào từ theo sau.

Lookahead    Tất cả    Rare    Norm    OOV
0            6.40      14.99   21.71   37.33
1            1.95      2.71    6.28    18.90
2            1.69      2.55    6.02    18.62
∞            1.31      2.17    5.28    17.28

Bảng 2: Ảnh hưởng của lookahead đến hiệu suất G2P, được đo 
bằng Word Error Rate (%), trên tất cả các từ và các tập con 
thách thức như định nghĩa trong Mục 3.2.

Trong các thực nghiệm được mô tả ở trên, chúng tôi đã sử 
dụng mô hình T5-Base. Tuy nhiên, các mô hình ngôn ngữ lớn 
hơn được sử dụng phổ biến hơn do hiệu suất được cải thiện, 
và vấn đề sử dụng nhiều hoặc ít lớp ẩn LLM hơn cũng có thể 
được xem xét. Chúng tôi điều tra ảnh hưởng của các embedding 
LLM được sử dụng bởi LLM2PnP đến hiệu suất G2P bằng cách 
thêm và loại bỏ các lớp embedding từ T5-Base, và cũng bằng 
cách sử dụng các mô hình T5-Large và T5-XL 24 lớp. Bảng 3 
cho thấy rằng cả việc thêm nhiều lớp hơn và tăng kích thước 
LLM đều cải thiện hiệu suất G2P. Tuy nhiên, lợi ích đạt được 
từ việc lựa chọn embedding LLM nhỏ hơn so với lợi ích đạt 
được từ lookahead từ bổ sung.

3.3. Nghiên cứu loại bỏ Prosody
Để ước tính chất lượng prosodic, chúng tôi đã tiến hành các 
bài kiểm tra ưu tiên ABX, trong đó chúng tôi so sánh âm 
thanh tổng hợp LLM2Speech (A) với âm thanh của hệ thống 
khác (B) trên cùng văn bản như trong Mục 3.1. Mỗi cặp mẫu 
âm thanh được đánh giá bởi 25 người nghe riêng biệt, những 
người được yêu cầu đánh giá sở thích của họ trên thang điểm 
[-2, -1, 0, 1, 2], trong đó -2 là "ưu tiên mạnh A"

LLM        Lớp Emb    Tất cả    Rare    Norm    OOV
-          -          2.10      2.93    6.61    20.05
Base       6          1.98      2.78    6.51    19.84
           2,6,10     1.95      2.71    6.28    18.90
           2,4,6,8,10 1.93      2.69    6.21    18.66
Large      6,12,18    1.94      2.71    6.31    18.91
XL         6,12,18    1.89      2.62    6.02    18.31

Bảng 3: Ảnh hưởng embedding đến G2P như trong Bảng 2.

2 là "ưu tiên mạnh B" và 0 là "không ưu tiên". Chúng tôi so 
sánh với một biến thể của LLM2Speech (i) không có tinh chỉnh 
trên corpus hội thoại (NoFT), (ii) không có embedding LLM 
(NoEmb), (iii) với embedding T5-XL (T5XL), và (iv) với 
lookahead L = 2 cho LLM2PnP (LA2). Để công bằng, chúng tôi 
đã loại bỏ các từ xen kẽ khỏi văn bản trong (i), vì NoFT 
không được tiếp xúc với chúng trong quá trình huấn luyện. 
Kết quả trong Bảng 4 cho thấy rằng tinh chỉnh đã cải thiện 
tính tự nhiên tổng thể, tuy nhiên các thay đổi khác không 
mang lại sự khác biệt đáng kể.

Phương pháp    Phân phối Bỏ phiếu B (%)    Điểm Trung bình
               -2    -1    0     1     2
NoFT           8.9   30.1  30.2  24.6  6.2    -0.110
NoEmb          5.8   26.8  35.8  25.3  6.3    -0.005
T5XL           5.0   25.9  36.7  28.0  4.3     0.007
LA2            8.0   30.8  25.9  27.8  7.4    -0.042

Bảng 4: Kết quả kiểm tra ưu tiên ABX so sánh LLM2Speech (A) 
với hệ thống khác (B). Điểm số âm có nghĩa là A được ưu 
tiên, và kết quả chỉ ra sự khác biệt đáng kể (p < 0.01) được 
in đậm.

4. THẢO LUẬN
Được thúc đẩy bởi AI hội thoại nói, chúng tôi đã điều tra 
việc đọc to văn bản được tạo bởi LLM với độ trễ thấp, mở 
đường cho các cuộc hội thoại AI tự nhiên. Chúng tôi đã mô tả 
các cơ chế đơn giản để hạn chế lookahead trong các lớp 
attention và convolution, với đó chúng tôi xây dựng một hệ 
thống TTS hội thoại độ trễ thấp. Chúng tôi phát hiện ra rằng 
TTS streaming có lợi từ chưng cất offline-to-streaming bằng 
cách sử dụng các tập dữ liệu văn bản lớn, ngay cả khi các 
văn bản thiếu phong cách hội thoại. Hơn nữa, các embedding 
LLM đã cải thiện dự đoán âm vị, tuy nhiên không mang lại 
cải thiện đáng kể trong chất lượng prosodic.

Trong công việc tương lai, chúng tôi nhằm mục đích cải thiện 
chất lượng âm thanh bằng cách sử dụng giọng nói tự nhiên và 
bằng cách trích xuất các gợi ý bổ sung từ LLM như cảm xúc. 
Rộng hơn, chúng tôi dự định tạo ra một hệ thống đối thoại 
nói độ trễ thấp, được hỗ trợ bởi backbone ngữ nghĩa LLM. Hệ 
thống sẽ bao gồm một mô hình nhận dạng giọng nói, theo sau 
là một LLM hội thoại, được kết hợp với LLM2Speech để tạo ra 
giọng nói một cách tăng dần, với độ trễ thấp.

--- TRANG 5 ---
5. TÀI LIỆU THAM KHẢO
[1] OpenAI, "ChatGPT," https://chat.openai.com, 2021.
[2] Google, "Bard," https://bard.google.com, 2022.
[3] Z. Borsos et al., "AudioLM: A Language Modeling Approach to Audio Generation," IEEE Trans. on ASLP, vol. 31, pp. 2523–2533, 2023.
[4] F. Kreuk et al., "AudioGen: Textually Guided Audio Generation," in ICLR, 2023.
[5] J. Shen et al., "Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions," in Proc. ICASSP. IEEE, 2018, pp. 4779–4783.
[6] C. Wang et al., "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers," arXiv preprint arXiv:2301.02111, 2023.
[7] J. Fong, J. Taylor, K. Richmond, and S. King, "A Comparison of Letters and Phones as Input to Sequence-to-Sequence Models for Speech Synthesis," in Proc. SSW 10, 2019, pp. 223–227.
[8] J. Taylor and K. Richmond, "Analysis of Pronunciation Learning in End-to-End Speech Synthesis," in Proc. Interspeech, 2019, pp. 2070–2074.
[9] A. Ploujnikov and M. Ravanelli, "SoundChoice: Grapheme-to-Phoneme models with semantic disambiguation," in Proc. Interspeech, 2022, pp. 486–490.
[10] M. Řezáčková, J. Švec, and D. Tihelka, "T5G2P: Using Text-to-Text Transfer Transformer for Grapheme-to-Phoneme Conversion," in Proc. Interspeech, 2021, pp. 6–10.
[11] J. Zhu, C. Zhang, and D. Jurgens, "ByT5 model for massively multilingual grapheme-to-phoneme conversion," in Proc. Interspeech, 2022, pp. 446–450.
[12] M. Ma et al., "Incremental Text-to-Speech Synthesis with Prefix-to-Prefix Framework," in Proc. EMNLP 2020, Nov 2020, pp. 3886–3896.
[13] J. Chen et al., "Speech-T: Transducer for Text to Speech and Beyond," in Proc. NeurIPS, 2021, vol. 34, pp. 6621–6633.
[14] C. Wu et al., "Transformer-Based Acoustic Modeling for Streaming Speech Synthesis," in Proc. Interspeech 2021, 2021, pp. 146–150.
[15] N. Ellinas et al., "High Quality Streaming Speech Synthesis with Low, Sentence-Length-Independent Latency," in Proc. Interspeech, 2020, pp. 2022–2026.
[16] L. Ouyang et al., "Training language models to follow instructions with human feedback," in NeurIPS, 2022, vol. 35, pp. 27730–27744.
[17] S. Shechtman and R. Fernandez, "A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers," in Proc. Interspeech, 2023, pp. 4853–4857.
[18] D. Povey, H. Hadian, P. Ghahremani, K. Li, and S. Khudanpur, "A Time-Restricted Self-Attention Layer for ASR," in Proc. ICASSP, 2018, pp. 5874–5878.
[19] G. Kurata and G. Saon, "Knowledge Distillation from Offline to Streaming RNN Transducer for End-to-End Speech Recognition," in Interspeech, 2020, pp. 2117–2121.
[20] C. Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer," Journal of Machine Learning Research, vol. 21, no. 140, pp. 1–67, 2020.
[21] S. Shechtman, R. Fernandez, and D. Haws, "Supervised and unsupervised approaches for controlling narrow lexical focus in sequence-to-sequence speech synthesis," in Proc. SLT, 2021, pp. 431–437.
[22] R. Fernandez, D. Haws, G. Lorberbom, S. Shechtman, and A. Sorin, "Transplantation of Conversational Speaking Style with Interjections in Sequence-to-Sequence Speech Synthesis," in Proc. Interspeech, 2022, pp. 5488–5492.
[23] A. Vaswani et al., "Attention is all you need," in Advances NIPS, 2017, vol. 30.
[24] J. Shen et al., "Non-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling," CoRR, vol. abs/2010.04301, 2020.
[25] J-M Valin and J. Skoglund, "LPCNET: Improving Neural Speech Synthesis through Linear Prediction," in Proc. ICASSP, 2019, pp. 5891–5895.
[26] A. Graves and J. Schmidhuber, "Framewise phoneme classification with bidirectional LSTM and other neural network architectures," Neural networks, vol. 18, no. 5-6, pp. 602–610, 2005.
[27] Y. Zhang et al., "Highway Long Short-Term Memory RNNs for Distant Speech Recognition," in Proc. ICASSP, 2016, pp. 5755–5759.
[28] A. Oord et al., "Wavenet: A generative model for raw audio," arXiv preprint arXiv:1609.03499, 2016.
