# 2210.04428.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2210.04428.pdf
# File size: 120835 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2210.04428v2  [cs.CV]  29 Mar 2023A Simple Baseline that Questions the Use of
Pretrained-Models in Continual Learning
Paul Janson1,3∗, Wenxuan Zhang1, Rahaf Aljundi2, and Mohamed Elhoseiny1
1King Abdullah University of Science and Technology, Saudi A rabia
2Toyota Motor Europe, Belgium
3University of Moratuwa, Sri Lanka
Abstract
With the success of pre-training techniques in representat ion learning, a number
of continual learning methods based on pre-trained models h ave been proposed.
Some of these methods design continual learning mechanisms on the pre-trained
representations and only allow minimum updates or even no up dates of the back-
bone models during the training of continual learning. In th is paper, we ques-
tion whether the complexity of these models is needed to achi eve good perfor-
mance by comparing them to a very simple baseline that we desi gn. We argue
that the pre-trained feature extractor itself can be strong enough to achieve a com-
petitive or even better continual learning performance on S plit-CIFAR100 and
CoRe 50 benchmarks. To validate this, we conduct the baselin e that 1) uses a
frozen pre-trained model to extract image features for ever y class and computes
their corresponding mean features during the training time , and 2) makes pre-
dictions based on the nearest neighbor distance between tes t samples and mean
features of the classes; i.e., Nearest Mean Classiﬁer (NMC) . This baseline is
single-headed, exemplar-free, and can be task-free by upda ting the mean fea-
tures continually. This baseline achieved 83.70% on 10-Split-CIFAR-100, sur-
passing most state-of-the-art continual learning methods , with all initialized by
the same pre-trained transformer model. We hope our baselin e may encourage
future progress in designing learning systems that can cont inually add quality to
the learning representations even if they start from pre-tr ained weights. Code is
availablehttps://github.com/Pauljanson002/pre-trained-cl.git
1 Introduction
Conventional machine learning models struggle to perform w ell when the i.i.d. assumption is vio-
lated in the real world, where data arrives sequentially fro m tasks with shifting distributions over
time. These models suffer from catastrophic forgetting of e arlier tasks[15, 10]. Continual learning,
in which the agent is expected to learn new tasks without forg etting the old ones, has been studied
extensively as a potential solution to this issue. Early app roaches start training with a model from
scratch, continually adapt the model for future tasks, and p revent forgetting by replaying the data,
designing the penalization of the updates of the model param eters, and/or dynamically increasing
model parameters to incorporate new knowledge.
The use of pre-training [24] on large-scale datasets, such a s ImageNet-1k[23] and ImageNet-21k
[22] has led to signiﬁcant advances. With sufﬁcient and dive rse data, the output features of a pre-
trained model generalize well to a vast array of tasks, great ly increasing the performance of challeng-
ing learning scenarios such as continual learning. As a resu lt, some existing works directly deploy
the pre-trained models as feature extractors, and apply con tinual learning techniques on the feature
∗Workdone during internship at KAUST
Workshop on Distribution Shifts, 36th Conference on Neural Information Processing Systems (NeurIPS 2022).

--- PAGE 2 ---
level. These methods imply that pre-trained models provide general and coarse features that need
to be task-speciﬁcally ﬁne-tuned. However, [20] stated lar ge-scale pre-trained models have excel-
lent classiﬁcation performance to the data outside of the pr e-trained distribution even without extra
training. We are questioning whether these pre-trained fea tures are competitive for downstream
tasks and whether sophisticated continual learning techni ques are indeed needed to achieve a good
performance on the studied continual learning benchmarks.
To answer these questions, we implement a simple baseline in continual learning. We use a frozen
pre-trained model to extract features for the training set a nd compute mean features to represent
each class. During the testing time, we make predictions bas ed on the distance between the test
samples and class mean features. Our results are surprising ly competitive with SOTA methods
using the same pre-trained backbone network. We achieve 83.70% Average Accuracy at the end
of the learning sequence of Split CIFAR100 compared to 83.83% of L2P[27], and 83.23% on the
evaluation task of CoRe50 compared to 78.33% of L2P. This implies that the pre-trained models
provide quality and robust learning representations under distribution shifts. We also present the
experimental results on two more diverse benchmarks, 5-dat aset and Split-ImageNet-R. The results
on these datasets also show competitive performance outper forming most methods.
Our proposed baseline is single-headed, task-free, and exe mplar-free as an additional bonus. We
argue that this is a simple yet powerful baseline that every c ontinual learning method should compare
against. The intentions of these properties are to make less use of task labels (which is unrealistic in
practice) and replay data (which raises privacy concerns) i n the techniques. We observe that some
prior works outperform this baseline, but at the sacriﬁce of one or more of these characteristics.
There are few comparable approaches with ours when all the co nditions align. We hope that this
work sheds some light on examining the practicality of pre-t raining in continual learning and whether
the new methods are improving the learned representation qu ality continually.
2 Related Work
Continual learning with pre-trained models Continual learning methods generally trained feature
extractors from scratch and constrained the drift in featur e representation [10, 16]. Recently, the us-
age of pre-trained models has attracted more attention in co ntinual learning. [2] viewed the ﬁrst task
as a pre-training stage and froze the feature representatio n after the ﬁrst task. [7] found that a larger
size of data in the ﬁrst task and self-supervised pre-traini ng helped to decrease catastrophic forget-
ting. [12] empirically analyzed the effect of training the l ast layer with a ﬁxed feature extractor. [18]
studied foundation models and replay of frozen latent featu res to overcome catastrophic forgetting.
Recently [27] proposed to use prompt based ﬁne-tuning with a pre-trained transformer [5, 25] and
compared it with other methods in the same initialization. W e adopt this strategy and point out that
classiﬁer learning actually reduced the power of represent ations learned by the pre-trained model.
Task-aware/free continual learning Task incremental method requires the task identiﬁers of the
samples during the inference, which reduces the practical a pplication of these methods, whereas
class-incremental methods do not require those. However, t ask-aware/free methods suffer from class
recency bias. To overcome such recency bias, BiC[29] propos ed to add a ﬁnal layer that reduces the
ﬁnal bias. LUCIR [9] proposed to use a balanced ﬁne-tuning to train the classiﬁer after freezing
feature representation. Our method also focuses on this pro blem and uses the simple nearest mean
classiﬁer on top of pre-trained transformer.
Exemplar-free continual learning Earlier methods were proposed to store raw data, learned fea -
tures, or generated features from previous tasks. Since the replay mechanism such as ER [4] is
orthogonal to architecture-based and regularization-bas ed methods, they are widely adopted in other
methods to improve performance. Recently there has been an i ncreased interest in exemplar-free
continual learning to account for privacy concerns in stori ng raw samples and storage concerns.
iCaRL [21] introduced the method of using exemplars for cont inual learning and selected exemplars
close to the class means. Our baseline follows a similar stra tegy but only stores the class-mean in
feature space which save the storage by reducing the number o f latent variables to keep per class.
3 Methodology
Problem Setup We adopt the standard continual learning scenario where a mo del learns from a
non-i.i.d. data stream, represented as D1,...,DT, whereDt={(xt
i,yt
i)}Nt
i=1is the task-speciﬁc
2

--- PAGE 3 ---
subset,xt
i∈Rw×h×cis an image input and yt
i∈Zis its corresponding label. The goal of continual
learning is to learn a function fθwhich maps the input xto the label yfrom an arbitrary task seen so
far. We focus on two scenarios. In the class-incremental set ting, each subset Dtcontains a disjoint
class label set. In the domain-incremental setting, the sub setsD1,...,DTshare the class labels, but
the input distributions varies over time.
Nearest Mean Classiﬁer (NMC) We decouple the goal of continual learning fθinto two steps. The
ﬁrst step is to learn the representation hand the next is to learn the classiﬁer g. We directly adopt
a pre-trained vision transformer as our feature representa tion without training. For the classiﬁer,
inspired by [21, 17] we use the nearest mean classiﬁcation st rategy. During the training stage of task
t, we calculate the mean features of a class in Dt
µk=1
|Ck|/summationdisplay
x∈Ckh(x), (1)
whereCkdenotes the set of training samples belonging to class k. Only class mean features are
saved in the memory and will be used during evaluation. At the test time of task t, the feature of a
test sample is extracted by the pre-trained model, and the pr edicted class label is taken as the class
whose mean features is the closest (over all the seen classes so far) to the feature of a test sample.
ˆy=argmin
k||h(x)−µk|| (2)
4 Experiments
We follow the experimental setup used in [27] to evaluate our method for a fair comparison. We test
our method in class incremental learning, where new sets of c lasses are introduced to the model, and
in domain incremental learning, where classes remain the sa me and the domain changes; see Sec 3.
Datasets: We evaluate our baseline on four common continual learning b enchmarks, Split- CI-
FAR100 [11] , 5-datasets [6] and Split-ImageNet-R [8] in the class incremental learning setting.
As proposed by [27] and [26]. Split-CIFAR-100 contains 10 ta sks with 10 classes for each task.
The 5-dataset benchmark concatenates 5 datasets, MNIST, SV HN, notMNIST , FashionMNIST and
CIFAR10, with each dataset forming one task. Split ImageNet -R is a newly proposed dataset for
continual learning by [26]. It consists of 200 classes which are randomly divided into 10 tasks. It
contains the same object types however presented in differe nt styles such as cartoon , grafﬁti and
origami. These variations make the continual learning more challenging. For the domain incremen-
tal learning setting, we use CoRe50 proposed by [14]. It cont ains 50 objects collected in 11 distinct
domains(tasks). 8 domains were faced and learned increment ally while the test is performed on
the remaining three domains. Since a single test task is used , we do not report forgetting and joint
training results in that scenario.
Evaluation Methods: For our approach ( Ours ), we employ the widely used ViT-B/16[5] model
pre-trained on ImageNet-21k [24] provided by timm library [28]. We mainly compare our baseline
with the recent L2P [27] which adopts the pretrained model as ours and learns a prompt pool with
a prompt selection mechanism to modify the pretrained repre sentations. We also consider popular
continual learning methods including regularization-bas ed methods (LwF[13] , EWC [10] ) and
rehearsal-based methods (ER[4], GDumb[19], BiC [29], DER+ + [1] and Co2L [3]). We present
joint training results where the training data is, i.i.d. di stributed among the whole benchmark with
no task split. FT-frozen adds a fully-connected layer on top of the frozen feature extractor as the
classiﬁcation head, and FT allows end-to-end training on th e feature extractor. Note that FT-frozen
is different from our baseline, as we use NMC classiﬁer and bu ild it incrementally.
Results: Table 1, 4, and 2 report the performance of continual learnin g in incremental setting in
Average Accuracy at the end of the learning sequence of Split -CIFAR100, Split-ImageNet-R, and
the 5-dataset respectively. Table 4 shows the results of the continual learning in domain incremental
setting on CoRe50[14]. The results are grouped based on the u se of replay sample. Our simple
baseline achieves the competitive performance on Split CIF AR-100 and CoRe50 , Split-ImageNet-
R and 5-dataset benchmarks. Our results on Split-CIFAR-100 are even better than methods that use
replay samples. Concretely, our baseline achieves 83.70% w ith zero buffer size. This suggests that
pre-trained transformer offers a strong representation th at achieves competitive performance. We
think that the main reason for a possible inferior performan ce could be the ineffective design of the
continual learning mechanisms compared to the robust featu res provided by the pretrained model.
Such a model, pretrained on a large and diverse dataset, may h ave already captured most of the
3

--- PAGE 4 ---
Table 1: Continual learning performance ex-
pressed in Average Accuracy and Forgetting
at the end of the learning sequence of CIFAR-
100[11]. All methods are initialized with pre-
trained weights for a fair comparison. Our base-
line shows competitive performance on this
benchmark.
Method Buffer size Average Acc Forgetting
FT - frozen 0 17.72 59.09
FT 0 33.61 86.87
EWC[10] 0 47.01 33.27
LwF [13] 0 60.69 27.77
L2P [27] 0 83.83 7.63
Ours 0 83.70 -
ER [4] 50/class 82.53 16.46
GDumb [19] 50/class 81.67 -
BiC [29] 50/class 81.42 17.31
DER++ [1] 50/class 83.94 14.55
Co2L [3] 50/class 82.49 17.48
L2P [27] 50/class 86.31 5.83
Joint - 90.85 -Table 2: Continual learning performance ex-
pressed in Average Accuracy and Forgetting at
the end of the learning sequence of the 5-dataset
benchmark [6]. All methods are initialized with
pretrained weights of the transformer for a fair
comparison. Our baseline performs competi-
tively with the exemplar-free methods in this
benchmark
Method Buffer size Average Acc Forgetting
FT - frozen 0 39.49 42.62
FT 0 20.12 94.63
EWC[10] 0 50.93 34.94
LwF [13] 0 47.91 38.01
L2P [27] 0 81.14 4.64
Ours 0 79.84 -
ER [4] 50/class 84.26 12.85
GDumb [19] 50/class 70.76 -
BiC [29] 50/class 85.53 10.27
DER++ [1] 50/class 84.88 10.46
Co2L [3] 50/class 86.05 12.28
L2P[27] 50/class 88.95 4.92
Joint 93.93
Table 3: Continual learning performance in Av-
erage Accuracy at the evaluation task of CoRe50
[14]. All methods are initialized with pretrained
weight for fair comparison
Method Buffer size Test Acc
EWC[10] 0 74.82
LwF[13] 0 75.45
L2P[27] 0 78.33
Ours 0 83.23
ER[4] 50/class 80.1
GDumb[19] 50/class 74.92
BiC[29] 50/class 79.28
DER++[1] 50/class 79.7
Co2L[3] 50/class 79.75
L2P[27] 50/class 81.07Table 4: Continual learning performance in Av-
erage Accuracy and Forgetting at the end of
the learning sequence of the Split-ImageNet-R[8]
benchmark. All methods are initialized with pre-
trained weights for fair comparison.
Method Buffer size Average Acc. Forgetting
FT - frozen 0 39.49 42.62
FT 0 28.87 63.80
EWC [10] 0 35.00 56.16
LwF [13] 0 38.54 52.37
L2P [27] 0 61.57 9.73
Ours 0 55.56 -
ER [4] 5000 65.18 23.31
GDumb [19] 5000 65.90 -
BiC [29] 5000 64.63 22.25
DER++ [1] 5000 66.73 20.67
Co2L [3] 5000 65.90 23.36
Joint - 79.13 -
distribution properties in the evaluation benchmarks. The n a desired continual learning mechanism
needs to further encourage the model to produce the task-inv ariant features speciﬁc to the deployed
benchmark. Our nearest neighbor baseline is also competent among exemplar-free methods on
Split-ImageNet-R and 5-datasets benchmarks, which is more diverse than CIFAR-100 and CoRe50.
However, methods with higher buffer sizes and ﬁnely designe d continual learning mechanisms do
improve the representations extracted from pretrained mod els.
5 Conclusion
In this work, we explore the representational capacity of la rge-scale pre-trained models in continual
learning settings. We provide simple nearest neighbor base line experiments on four benchmarks,
showing competitive performance to more sophisticated sta te-of-the-art continual learning methods
which also leverage the same pretrained models. We agree tha t using pretrained weights can be a
reasonable practice even in continual learning. However, t o show real progress in continual learning
systems, we need to focus more on building methods that can co ntinually add quality to the learning
representations. A desired continual learning algorithm s hall go signiﬁcantly beyond the knowledge
embedded in the pretrained model. Another important aspect is the considered benchmarks for
evaluating continual learning methods. Such benchmarks ne ed to be sufﬁciently challenging and
different from the data distributions employed for pretrai ned models.
4

--- PAGE 5 ---
References
[1] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davi de Abati, and SIMONE CALDER-
ARA. Dark Experience for General Continual Learning: a Stro ng, Simple Baseline. In Ad-
vances in Neural Information Processing Systems , volume 33, pages 15920–15930. Curran
Associates, Inc., 2020.
[2] Francisco M. Castro, Manuel J. Marín-Jiménez, Nicolás G uil, Cordelia Schmid, and Karteek
Alahari. End-to-End Incremental Learning. In Vittorio Fer rari, Martial Hebert, Cristian Smin-
chisescu, and Yair Weiss, editors, Computer Vision – ECCV 2018 , volume 11216, pages 241–
257. Springer International Publishing, Cham, 2018. Serie s Title: Lecture Notes in Computer
Science.
[3] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrasti ve continual learning. In Proceed-
ings of the IEEE/CVF International Conference on Computer V ision 2021 , pages 9516–9525,
2021.
[4] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, T halaiyasingam Ajanthan,
Puneet K. Dokania, Philip H. S. Torr, and Marc’Aurelio Ranza to. On Tiny Episodic Mem-
ories in Continual Learning, June 2019. arXiv:1902.10486 [ cs, stat].
[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov , Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
et al. An image is worth 16x16 words: Transformers for image r ecognition at scale. In Inter-
national Conference on Learning Representations , 2020.
[6] Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Tre vor Darrell, and Marcus Rohrbach.
Adversarial continual learning. In European Conference on Computer Vision , pages 386–402.
Springer, 2020.
[7] Jhair Gallardo. Self-Supervised Training Enhances Onl ine Continual Learning. page 15.
[8] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavat h, Frank Wang, Evan Dorundo,
Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A
critical analysis of out-of-distribution generalization . In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 8340–8349, 2021.
[9] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Da hua Lin. Learning a Uniﬁed
Classiﬁer Incrementally via Rebalancing. In 2019 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 831–839, Long Beach, CA, USA, June 2019. IEEE.
[10] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, J oel Veness, Guillaume Desjardins,
Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agni eszka Grabska-Barwinska,
Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Rai a Hadsell. Overcoming catas-
trophic forgetting in neural networks. Proceedings of the National Academy of Sciences ,
114(13):3521–3526, March 2017. Publisher: Proceedings of the National Academy of Sci-
ences.
[11] Alex Krizhevsky et al. Learning multiple layers of feat ures from tiny images. 2009.
[12] Timothée Lesort, Oleksiy Ostapenko, Diganta Misra, Md Rifat Areﬁn, Pau Rodríguez, Lau-
rent Charlin, and Irina Rish. Scaling the Number of Tasks in C ontinual Learning, July 2022.
arXiv:2207.04543 [cs].
[13] Zhizhong Li and Derek Hoiem. Learning without Forgetti ng.IEEE Transactions on Pattern
Analysis and Machine Intelligence , 40(12):2935–2947, December 2018. Conference Name:
IEEE Transactions on Pattern Analysis and Machine Intellig ence.
[14] Vincenzo Lomonaco and Davide Maltoni. Core50: a new dat aset and benchmark for continu-
ous object recognition. In Conference on Robot Learning , pages 17–26. PMLR, 2017.
[15] Michael McCloskey and Neal J. Cohen. Catastrophic Inte rference in Connectionist Networks:
The Sequential Learning Problem. In Gordon H. Bower, editor ,Psychology of Learning and
Motivation , volume 24, pages 109–165. Academic Press, January 1989.
[16] Michael McCloskey and Neal J. Cohen. Catastrophic Inte rference in Connectionist Networks:
The Sequential Learning Problem. In Gordon H. Bower, editor ,Psychology of Learning and
Motivation , volume 24, pages 109–165. Academic Press, January 1989.
5

--- PAGE 6 ---
[17] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Dis tance-Based Image Classiﬁcation:
Generalizing to New Classes at Near-Zero Cost. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 35(11):2624–2637, November 2013.
[18] Oleksiy Ostapenko, Timothee Lesort, Pau Rodríguez, Md Rifat Areﬁn, Arthur Douillard, Irina
Rish, and Laurent Charlin. Continual Learning with Foundat ion Models: An Empirical Study
of Latent Replay, July 2022. arXiv:2205.00329 [cs].
[19] Ameya Prabhu, Philip H. S. Torr, and Puneet K. Dokania. G Dumb: A Simple Approach that
Questions Our Progress in Continual Learning. In Andrea Ved aldi, Horst Bischof, Thomas
Brox, and Jan-Michael Frahm, editors, Computer Vision – ECCV 2020 , volume 12347, pages
524–540. Springer International Publishing, Cham, 2020. S eries Title: Lecture Notes in Com-
puter Science.
[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Rame sh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Cla rk, et al. Learning transferable
visual models from natural language supervision. In International Conference on Machine
Learning , pages 8748–8763. PMLR, 2021.
[21] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Geor g Sperl, and Christoph H. Lampert.
iCaRL: Incremental Classiﬁer and Representation Learning , April 2017. arXiv:1611.07725
[cs, stat].
[22] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zeln ik-Manor. ImageNet-21K Pretrain-
ing for the Masses. December 2021.
[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sa njeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-
Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer
Vision , 115(3):211–252, December 2015.
[24] Andreas Peter Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit,
and Lucas Beyer. How to train your vit? data, augmentation, a nd regularization in vision
transformers. Transactions on Machine Learning Research , 2022.
[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko reit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you ne ed.Advances in neural information
processing systems 2017 , 30, 2017.
[26] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, H an Zhang, Chen-Yu Lee, Xiaoqi
Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pﬁste r. DualPrompt: Complementary
Prompting for Rehearsal-free Continual Learning, August 2 022. arXiv:2204.04799 [cs].
[27] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruox i Sun, Xiaoqi Ren, Guolong Su,
Vincent Perot, Jennifer Dy, and Tomas Pﬁster. Learning to Pr ompt for Continual Learning.
In2022 IEEE/CVF Conference on Computer Vision and Pattern Rec ognition (CVPR) . IEEE,
March 2022.
[28] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models ,
2019.
[29] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zichen g Liu, Yandong Guo, and Yun
Fu. Large Scale Incremental Learning. In 2019 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 374–382, Long Beach, CA, USA, June 2019. IEEE.
6
