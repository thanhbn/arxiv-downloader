# 2309.07117.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2309.07117.pdf
# File size: 361557 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SCIENCE CHINA
Information Sciences
2025, Vol. 68, Iss. , 000000:1–000000:2
https://doi.org/10.1007/s11432-024-4276-4
©Science China Press 2025 info.scichina.com link.springer.com.NEWS & VIEWS .
PILOT: A Pre-Trained Model-Based
Continual Learning Toolbox
Hai-Long Sun1,2, Da-Wei Zhou1,2*, De-Chuan Zhan1,2& Han-Jia Ye1,2*
1School of Artificial Intelligence, Nanjing University, China
2National Key Laboratory for Novel Software Technology, Nanjing University, China
Received 9 April 2024/Revised 12 July 2024/Accepted 17 January 2025
Citation Sun H-L, Zhou D-W, Zhan D-C, et al. PILOT: A Pre-Trained Model-Based Continual Learning Toolbox. Sci China
Inf Sci, 2025, 68(): 000000, https://doi.org/10.1007/s11432-024-4276-4
The rapid advancements in deep learning have resulted
in significant achievements across various fields. However,
our ever-changing world often presents training data in a
streaming format from an open environment. For example,
while ChatGPT demonstrates exceptional inference capabil-
ities, it struggles to provide users with the most up-to-date
information. This challenge arises from the high costs as-
sociated with retraining a GPT model on new data daily.
Therefore, the ability to continually update the model is
critically important. Continual learning has been proposed
as a solution to this challenge, allowing models to learn
from streaming data. A major concern in continual learning
iscatastrophic forgetting , where models forget previously
learned information when acquiring new knowledge. Many
methods have been developed to address this issue and
enable models to learn from new data without forgetting
former knowledge. In this paper, we focus on the Class-
Incremental Learning (CIL) setting, which is a common
scenario in continual learning.
Traditional approaches assume that models are “trained
from scratch.” However, with the rapid evolution of pre-
training techniques, Pre-Trained Models (PTMs) have be-
come widely used for downstream tasks. These PTMs are
typically trained on extensive corpora or massive image
datasets, resulting in robust generalizability. Consequently,
research in CIL is shifting from training models from scratch
to leveraging the power of PTMs. According to a recent
survey [1], methods based on PTMs exhibit significantly su-
perior performance compared to traditional methods relying
on random initialization. This raises an important question:
Is there still a need to study traditional CIL? To ad-
dress this inquiry, we not only reproduce state-of-the-art
methods in PTM-based CIL but also modify several tradi-
tional methods to be compatible with PTMs. This enables
a fair comparison between PTM-based methods and tradi-
tional methods.
We open-source the Pre-tra Ined mode L-based c Ontinual
learning Toolbox ( PILOT ) for the machine learning com-
munity. It includes several traditional CIL approaches
modified by PTMs and offers state-of-the-art algorithmsto advance PTM-based CIL research. The source code
ofPILOT is available at https://github.com/sun-hailong/
LAMDA-PILOT.
Compared with Other Toolkits . Since the current machine
learning community lacks a toolbox that includes numerous
PTM-based methods, there is an urgent need to develop
a dedicated PTM-based toolbox. This toolbox will facil-
itate cutting-edge research and allow for fair comparisons
between traditional methods and PTM-based methods us-
ing the same backbone. We primarily compare PILOT and
other toolkits in the following three aspects:
Incorporation of PTMs. PILOT not only encompasses
traditional CIL methods but also extends support for the
latest PTM-based CIL approaches. In contrast, other toolk-
its have mainly focused on conventional CIL methods and
have not explored the integration of PTMs.
Network Architecture and Parameter Tuning. By
transitioning from the typical ResNet backbone to using
PTMs, we design a unique parameter setting and tuning
approach. While traditional toolkits can potentially be ex-
tended to accommodate PTMs, they are primarily designed
with Convolution Neural Network (CNN). Hence, the pa-
rameters and hyper-parameter suited for CNN might not be
optimal for PTMs.
Benchmarks and Datasets. We provide benchmarks and
datasets specifically curated for scenarios involving PTMs.
These dedicated resources can play a pivotal role in obtain-
ing accurate performance metrics and evaluations tailored
to PTM-based CIL.
Implemented Algorithms . InPILOT , we implement 15
typical algorithms for CIL, including traditional methods
modified by PTMs and PTM-based methods. Below, we
list the latest PTM-based methods: Finetune involves
continually training a pre-trained model on new tasks. It
updates all parameters and is vulnerable to severe catas-
trophic forgetting. SimpleCIL [2] constructs classifiers
continually by extracting prototype features using PTMs,
without the need for additional training on the downstream
task. L2P [3] incorporates visual prompt tuning into CIL
using a pre-trained vision transformer and establishes a
* Corresponding author (email: zhoudw@lamda.nju.edu.cn, yehj@lamda.nju.edu.cn)arXiv:2309.07117v3  [cs.LG]  8 Mar 2025

--- PAGE 2 ---
Sun H-L , et al. Sci China Inf Sci 2025, Vol. 68, Iss. , 000000:2
/uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000048/uni00000056/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000057/uni00000058/uni00000051/uni00000048
/uni0000004c/uni00000026/uni00000044/uni00000035/uni0000002f
/uni00000036/uni0000004c/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000026/uni0000002c/uni0000002f
/uni00000027/uni00000028/uni00000035/uni00000030/uni00000028/uni00000030/uni00000032
/uni00000026/uni00000052/uni0000004c/uni0000004f
/uni00000029/uni00000032/uni00000036/uni00000037/uni00000028/uni00000035
/uni0000002f/uni00000015/uni00000033/uni00000026/uni00000032/uni00000027/uni00000024/uni00000010/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057
/uni00000027/uni00000058/uni00000044/uni0000004f/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057
/uni00000024/uni00000027/uni00000024/uni00000030/uni00000003/uni0000005a/uni00000012/uni00000003/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000048/uni00000055
/uni00000024/uni00000027/uni00000024/uni00000030/uni00000003/uni0000005a/uni00000012/uni00000003/uni00000039/uni00000033/uni00000037/uni00000010/uni00000027/uni00000048/uni00000048/uni00000053
(a)CIFAR100, 10 Stages
/uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000048/uni00000056/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000057/uni00000058/uni00000051/uni00000048
/uni0000004c/uni00000026/uni00000044/uni00000035/uni0000002f
/uni00000036/uni0000004c/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000026/uni0000002c/uni0000002f
/uni00000027/uni00000028/uni00000035/uni00000030/uni00000028/uni00000030/uni00000032
/uni00000026/uni00000052/uni0000004c/uni0000004f
/uni00000029/uni00000032/uni00000036/uni00000037/uni00000028/uni00000035
/uni0000002f/uni00000015/uni00000033/uni00000026/uni00000032/uni00000027/uni00000024/uni00000010/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057
/uni00000027/uni00000058/uni00000044/uni0000004f/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057
/uni00000024/uni00000027/uni00000024/uni00000030/uni00000003/uni0000005a/uni00000012/uni00000003/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000048/uni00000055
/uni00000024/uni00000027/uni00000024/uni00000030/uni00000003/uni0000005a/uni00000012/uni00000003/uni00000039/uni00000033/uni00000037/uni00000010/uni00000027/uni00000048/uni00000048/uni00000053 (b)ImageNet-R, 10 Stages
/uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000018/uni00000013 /uni00000015/uni00000013/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000048/uni00000056/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000002f/uni00000015/uni00000033
/uni00000027/uni00000058/uni00000044/uni0000004f/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057
/uni00000036/uni0000004c/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000026/uni0000002c/uni0000002f
/uni00000024/uni00000027/uni00000024/uni00000030/uni0000000e/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000057/uni00000058/uni00000051/uni00000048
/uni00000024/uni00000027/uni00000024/uni00000030/uni0000000e/uni00000039/uni00000033/uni00000037/uni00000010/uni00000036
/uni00000024/uni00000027/uni00000024/uni00000030/uni0000000e/uni00000039/uni00000033/uni00000037/uni00000010/uni00000027/uni00000024/uni00000027/uni00000024/uni00000030/uni0000000e/uni00000036/uni00000036/uni00000029
/uni00000024/uni00000027/uni00000024/uni00000030/uni0000000e/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000048/uni00000055
/uni00000026/uni00000032/uni00000027/uni00000024/uni00000010/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057
/uni0000002f/uni00000024/uni00000028
/uni00000028/uni00000024/uni00000036/uni00000028
/uni00000036/uni0000002f/uni00000026/uni00000024 (c)ImageNet-R, 10 Stages
/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000048/uni00000056/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000002f/uni00000015/uni00000033
/uni00000027/uni00000058/uni00000044/uni0000004f/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057
/uni00000036/uni0000004c/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000026/uni0000002c/uni0000002f
/uni00000024/uni00000027/uni00000024/uni00000030/uni0000000e/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000057/uni00000058/uni00000051/uni00000048
/uni00000024/uni00000027/uni00000024/uni00000030/uni0000000e/uni00000039/uni00000033/uni00000037/uni00000010/uni00000036
/uni00000024/uni00000027/uni00000024/uni00000030/uni0000000e/uni00000039/uni00000033/uni00000037/uni00000010/uni00000027/uni00000024/uni00000027/uni00000024/uni00000030/uni0000000e/uni00000036/uni00000036/uni00000029
/uni00000024/uni00000027/uni00000024/uni00000030/uni0000000e/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000048/uni00000055
/uni00000026/uni00000032/uni00000027/uni00000024/uni00000010/uni00000033/uni00000055/uni00000052/uni00000050/uni00000053/uni00000057
/uni0000002f/uni00000024/uni00000028
/uni00000028/uni00000024/uni00000036/uni00000028
/uni00000036/uni0000002f/uni00000026/uni00000024 (d)VTAB, 5 Stages
Figure 1 Reproduced incremental accuracy on CIFAR100, ImageNet-R, and VTAB. Subfigures (a) and (b) utilize the ViT-B/16-
IN1K backbone, while subfigures (c) and (d) employ the ViT-B/16-IN21K backbone.
prompt pool for selecting the instance-specific prompts.
DualPrompt [4] proposes two kinds of prompts based on
L2P, i.e., general and expert prompts. CODA-Prompt [5]
improves the prompt selection process with an attention
mechanism. APER [2], based on SimpleCIL, employs
parameter-efficient fine-tuning to acquire an adapted model.
Subsequently, it concatenates the adapted model with the
original one to obtain augmented features for constructing
a prototype-based classifier. RanPAC [6] injects a frozen
random projection layer with nonlinear activation to capture
interactions between features with expanded dimensionality.
SLCA [7] improves the classification layer by modeling the
class-wise distributions and aligning the classification layers
in a post-hoc fashion. LAE [8] defines the online and offline
learning protocol, where the online model is updated with
cross-entropy loss, aiming to acquire new knowledge in new
tasks. EASE [9] designs an expandable subspace ensemble
method for PTM-based CIL.
Supported Datasets .: Due to the overlap in data between
ImageNet-based benchmarks and the pre-trained dataset,
ImageNet is not an appropriate choice for assessing PTM-
based CIL methods, we provide some novel benchmarks
for CIL which: 1) are entirely distinct from the ImageNet
dataset, 2) present a significant domain gap from ImageNet,
thereby challenging the PTM’s ability to generalize, and 3)
encompass large-scale datasets from various domains to es-
tablish a cross-domain class-incremental benchmark. On the
other hand, since pre-trained models may possess extensive
knowledge of upstream tasks, we evaluate performance on
CIFAR100, CUB200, ImageNet-R, ImageNet-A, ObjectNet,
OmniBenchmark, and VTAB. These datasets represent typi-
cal CIL benchmarks and include out-of-distribution datasets
that exhibit a significant domain gap with ImageNet ( i.e.,
the pre-trained dataset). Specifically, there are 50 classes
in VTAB, 100 classes in CIFAR100, 200 classes in CUB,
ImageNet-R, ImageNet-A, and ObjectNet, and 300 classes
in OmniBenchmark.
Evaluation Methodology . In CIL, a widely used perfor-
mance metric is the test accuracy at each incremental stage,
denoted as Ab, where brepresents the stage index. Another
important metric is the average accuracy across all stages,
given by ¯A=1
BPB
b=1Ab. In this work, we evaluate the in-
cremental performance (Top-1 accuracy) at each stage, with
results shown in Figure 1. We utilize datasets such as CI-
FAR100, ImageNet-R, ObjectNet, and VTAB, dividing all
classes into several incremental stages. Due to some missing
parameters in some papers ( e.g., L2P), we have optimized
a suitable parameter set for these methods. It is encourag-
ing to observe that most re-implemented algorithms either
match or exceed the performance benchmarks of the origi-
nal publication. Moreover, we find that although traditional
methods use PTM backbones and preserve some samples forreplay, their performance is generally lower than PTM-based
methods. This highlights the importance of leveraging pre-
training techniques to design efficient CIL methods.
Conclusion . We have introduced PILOT , a pre-trained
model-based continual learning toolbox. It includes a collec-
tion of reproduced PTM-based CIL methods and provides
state-of-the-art algorithms for advanced research. PILOT
aims to facilitate innovative research and development in the
field of continual learning. In the future, we will continue to
update our toolbox, expanding it to include more algorithms
and datasets, and applying it to a wider range of settings.
Acknowledgements This work is partially supported by
National Key R&D Program of China (2022ZD0114805),
NSFC (62476123, 62376118, 62006112, 62250069, 61921006),
Fundamental Research Funds for the Central Universities
(2024300373, 14380021), Key Program of Jiangsu Science Foun-
dation (BK20243012), CCF-Tencent Rhino-Bird Open Research
Fund RAGR20240101, the AI & AI for Science Project of Nan-
jing University, Collaborative Innovation Center of Novel Soft-
ware Technology and Industrialization.
Supporting information Appendix A. The supporting in-
formation is available online at info.scichina.com and link.
springer.com. The supporting materials are published as sub-
mitted, without typesetting or editing. The responsibility for
scientific accuracy and content remains entirely with the au-
thors.
References
1 Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, and
De-Chuan Zhan. Continual learning with pre-trained mod-
els: A survey. In IJCAI , pages 8363–8371, 2024.
2 Da-Wei Zhou, Zi-Wen Cai, Han-Jia Ye, De-Chuan Zhan, and
Ziwei Liu. Revisiting class-incremental learning with pre-
trained models: Generalizability and adaptivity are all you
need. International Journal of Computer Vision , 2024.
3 Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,
Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer
Dy, and Tomas Pfister. Learning to prompt for continual
learning. In CVPR , pages 139–149, 2022.
4 Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,
Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent
Perot, Jennifer Dy, et al. Dualprompt: Complementary
prompting for rehearsal-free continual learning. In ECCV ,
pages 631–648. Springer, 2022.
5 James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta,
Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle,
Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-
prompt: Continual decomposed attention-based prompting
for rehearsal-free continual learning. In CVPR , pages
11909–11919, 2023.
6 Mark D McDonnell, Dong Gong, Amin Parvaneh, Ehsan Ab-
basnejad, and Anton van den Hengel. Ranpac: Random
projections and pre-trained models for continual learning.
NeurIPS , 36, 2024.
7 Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen,
and Yunchao Wei. Slca: Slow learner with classifier align-
ment for continual learning on a pre-trained model. In
ICCV , pages 19148–19158, 2023.
8 Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang,
Bernard Ghanem, and Jian Zhang. A unified continual
learning framework with general parameter-efficient tuning.
InICCV , pages 11483–11493, 2023.
9 Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, and De-Chuan
Zhan. Expandable subspace ensemble for pre-trained model-
based class-incremental learning. In CVPR , pages 23554–
23564, 2024.
