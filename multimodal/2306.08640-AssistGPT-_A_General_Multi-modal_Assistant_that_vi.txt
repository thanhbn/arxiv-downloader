# AssistGPT: Trợ lý Đa phương tiện Tổng quát có thể Lập kế hoạch, Thực thi, Kiểm tra và Học hỏi

Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen,
Zihan Fan, Mike Zheng Shou∗
Show Lab, Đại học Quốc gia Singapore,
https://showlab.github.io/assistgpt/

## Tóm tắt

Nghiên cứu gần đây về Mô hình Ngôn ngữ Lớn (LLMs) đã dẫn đến những tiến bộ đáng kể trong các trợ lý AI NLP tổng quát. Một số nghiên cứu đã khám phá thêm việc sử dụng LLMs để lập kế hoạch và gọi các mô hình hoặc APIs để giải quyết các truy vấn đa phương tiện tổng quát hơn của người dùng. Bất chấp tiến bộ này, các tác vụ phức tạp dựa trên thị giác vẫn còn thách thức do tính chất đa dạng của các tác vụ thị giác. Sự đa dạng này được phản ánh trong hai khía cạnh: 1) Đường dẫn suy luận. Đối với nhiều ứng dụng thực tế, rất khó để phân tách chính xác một truy vấn chỉ bằng cách kiểm tra chính truy vấn đó. Thường cần lập kế hoạch dựa trên nội dung thị giác cụ thể và kết quả của mỗi bước. 2) Đầu vào linh hoạt và kết quả trung gian. Các hình thức đầu vào có thể linh hoạt cho các trường hợp thực tế, và không chỉ bao gồm một hình ảnh hoặc video đơn lẻ mà là hỗn hợp của video và hình ảnh, ví dụ: một hình ảnh góc nhìn người dùng với một số video tham khảo. Bên cạnh đó, một quá trình suy luận phức tạp cũng sẽ tạo ra các kết quả trung gian đa phương tiện đa dạng, ví dụ: lời tường thuật video, các đoạn video được phân đoạn, v.v. Để giải quyết các trường hợp tổng quát như vậy, chúng tôi đề xuất một trợ lý AI đa phương tiện, AssistGPT, với phương pháp suy luận mã và ngôn ngữ xen kẽ được gọi là Lập kế hoạch, Thực thi, Kiểm tra và Học hỏi (PEIL) để tích hợp LLMs với các công cụ khác nhau. Cụ thể, Planner có khả năng sử dụng ngôn ngữ tự nhiên để lập kế hoạch công cụ nào trong Executor nên thực hiện tiếp theo dựa trên tiến trình suy luận hiện tại. Inspector là một trình quản lý bộ nhớ hiệu quả để hỗ trợ Planner đưa thông tin thị giác phù hợp vào một công cụ cụ thể. Cuối cùng, vì toàn bộ quá trình suy luận phức tạp và linh hoạt, một Learner được thiết kế để cho phép mô hình khám phá và khám phá tự động giải pháp tối ưu. Chúng tôi đã tiến hành thí nghiệm trên các benchmark A-OKVQA và NExT-QA, đạt được kết quả tốt nhất hiện tại. Hơn nữa, các trình diễn cho thấy khả năng của hệ thống chúng tôi để xử lý các câu hỏi phức tạp hơn nhiều so với những câu hỏi được tìm thấy trong các benchmark.

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLMs), đặc biệt là ChatGPT, đã đạt được tiến bộ đáng kể trong những tháng gần đây, thúc đẩy đáng kể lĩnh vực phát triển trợ lý AI. Bất chấp những tiến bộ này, một LLM đơn lẻ phục vụ như một trợ lý AI vẫn thể hiện những hạn chế vốn có trong một số khả năng nhất định, chẳng hạn như hiểu môi trường thị giác và hiểu các tác vụ phức tạp, điều này hạn chế tính hữu ích của chúng trong các ứng dụng thế giới thực. Để giải quyết những thiếu sót này, một giải pháp đầy hứa hẹn là khám phá việc tích hợp và hợp tác của nhiều chuyên gia miền, ví dụ: các mô hình hoặc APIs được đào tạo trước, để giải quyết các tác vụ phức tạp. Nhiều nỗ lực đã được thực hiện theo hướng này. Một số công trình sử dụng ngôn ngữ như một cầu nối và chuyển đổi đầu vào thị giác thành văn bản thuần túy bằng cách sử dụng các mô hình thị giác nền tảng, chẳng hạn như captioner, bộ phát hiện đối tượng và mô hình OCR. Sau đó, các văn bản được trích xuất được đưa vào LLMs cho các tác vụ suy luận như trả lời câu hỏi. Tuy nhiên, đối với các

*Tác giả liên hệ.

--- TRANG 2 ---

Chạy công cụ được chọn
Trả lời: Nhấp chuột phải vào 
"Midjourney Bot"
visual-0
visual-1

Người dùng AssistGPT
Câu hỏi: Tôi đang cố gắng thêm Midjourney
vào máy chủ của riêng tôi. Tôi đã nhấp vào
'hiển thị biểu tượng thành viên', tôi nên làm gì
ở bước tiếp theo? Hiển thị nó trên ảnh chụp màn hình
mà tôi cung cấp.

visual-0: video 627 giây, ..., Hướng dẫn cho người mới bắt đầu Midjourney
visual-1: một hình ảnh, ..., cửa sổ trò chuyện với một bức tranh trên đó.

Suy nghĩ: Tìm đoạn video về việc thêm Midjourney vào máy chủ cá nhân của mình.
Hành động:
subtitle_ground ("Khi nào video thảo luận về việc thêm Midjourney vào máy chủ?", visual[0])
Quan sát: đoạn mục tiêu từ 03:58 –04:15

visual-2: video 17 giây, ..., đoạn mục tiêu cho truy vấn "Khi nào video ...?"

Phát hiện Đối tượng
Định vị Phụ đề
Tường thuật Video
Định vị Vùng
Phát hiện OCR
ASR...

Executor
visual-2

LLM lập kế hoạch công cụ nào
sẽ sử dụng

Tạo tóm tắt ngắn gọn về
đầu vào thị giác và
kết quả trung gian

Suy nghĩ: Suy luận bước tiếp theo sau khi
chọn biểu tượng 'hiển thị thành viên' là gì?
Hành động:
subtitle_reason ("Bước tiếp theo sau khi chọn biểu tượng 'hiển thị thành viên' là gì?", visual[2])
Quan sát: Nhấp chuột phải vào "mid-journey bot"

Suy nghĩ: Tìm "mid-journey bot" trên ảnh chụp màn hình
Hành động: text_ground ("mid-journey bot", visual[1])
Quan sát: vùng được tìm thấy

Planner
Inspector

visual-3: một hình ảnh, ..., vùng mục tiêu cho truy vấn "mid-journey bot"
visual-3

Đây là một lần thử thành công, lưu toàn bộ quá trình suy luận như một ví dụ trong ngữ cảnh

Kiểm tra đường dẫn suy luận để
thử lại hoặc lưu các lần thử thành công

Learner

Suy nghĩ: Tôi biết câu trả lời cuối cùng.
Câu trả lời cuối cùng: (Nhấp chuột phải vào "mid-journey bot", visual[3])

Hình 1: Ví dụ thực tế của AssistGPT. AssistGPT có thể suy luận theo định dạng ngôn ngữ và mã xen kẽ. Với một truy vấn đầu vào và đầu vào thị giác, AssistGPT lập kế hoạch đường dẫn giải quyết vấn đề bằng ngôn ngữ, sử dụng mã có cấu trúc để gọi các công cụ mạnh mẽ khác nhau. Inspector, một phần của hệ thống, có thể quản lý đầu vào thị giác và kết quả trung gian, hỗ trợ Planner gọi các công cụ. Trong khi đó, Learner có thể đánh giá quá trình suy luận và thu thập các ví dụ trong ngữ cảnh.

kịch bản thị giác phức tạp như video dài với chuyển cảnh phức tạp, như được hiển thị trong Hình 1, các văn bản được tạo có thể vượt xa các yêu cầu truy vấn. Điều này có thể dẫn đến thông tin dư thừa dồi dào trong khi các chi tiết quan trọng liên quan đến truy vấn có thể bị bỏ qua.

Một số công trình đồng thời khác đề xuất phân tách các truy vấn người dùng thành các tác vụ con và lập kế hoạch gọi tuần tự các mô hình hoặc APIs bên ngoài để trả lời chúng. Hiện tại, có hai nhánh phương pháp. Nhánh đầu tiên là lập kế hoạch dựa trên ngôn ngữ. Ví dụ, HuggingGPT và Chameleon đề xuất sử dụng LLM như một bộ điều khiển, quản lý và tổ chức sự hợp tác của các mô hình chuyên gia. Nhánh công trình khác là lập kế hoạch dựa trên mã. ViperGPT đề xuất sử dụng Codex để viết mã Python gọi các APIs liên quan đến thị giác để xử lý các tác vụ đa phương tiện. Những phương pháp này cho phép gọi các mô hình chỉ khi cần thiết, điều này cho phép các mô hình chỉ xuất thông tin hữu ích và tối ưu hóa việc sử dụng tài nguyên tính toán.

Bất chấp tiến bộ này, việc giải quyết các truy vấn cấp cao vẫn đầy thách thức. Cụ thể, các câu hỏi hiện tại trong các benchmark hiện có thường trực tiếp ngụ ý cách lập kế hoạch suy luận. Ví dụ, đối với câu hỏi như "Đối tượng màu đỏ được sử dụng để làm gì?", bất kể hình ảnh là gì, các bước suy luận tương đối cố định, tức là nhận diện đối tượng màu đỏ, sau đó tìm ra chức năng của nó. Tuy nhiên, đối với các câu hỏi phức tạp hơn, có thể có các đường dẫn suy luận đa dạng. Ví dụ, đối với câu hỏi "Tôi nên sử dụng bao nhiêu tiêu đen cho 700g thịt bò?" trong Hình 2, sự biến đổi trong cách trình bày thông tin liên quan, cho dù đó là dưới dạng phụ đề, hành động, văn bản trong video, hoặc sự kết hợp của những thứ này, có thể dẫn đến các đường dẫn suy luận khác biệt. Do đó, như được hiển thị trong Hình 2, một khi phương pháp chỉ suy luận mắc lỗi, trở nên khó khăn để tự sửa lỗi.

Các phương pháp tương tự đã được đề xuất trong lĩnh vực NLP, như ReAct và ToolFormer. Tuy nhiên, có một thách thức độc đáo trong các tác vụ đa phương tiện: Làm thế nào để xử lý kết quả trung gian không phải văn bản?

--- TRANG 3 ---

Suy nghĩ: Tôi cần xem liệu video có đưa ra
các phép đo cụ thể để sử dụng tiêu đen
trên thịt bò không.
Hành động:
Subtitle Ground
Đầu vào hành động: tiêu đen cho thịt bò

Suy nghĩ: Tôi cần suy luận lượng tiêu đen
cho 700g thịt bò dựa trên thông tin từ video.
Hành động:
Subtitle Reason
Đầu vào hành động: tôi nên sử dụng bao nhiêu tiêu đen
cho 700g thịt bò?

Câu hỏi: Tôi nên sử dụng bao nhiêu
tiêu đen cho 700g thịt bò?

Suy nghĩ: ... sử dụng Mô-đun Subtitle Ground để ...
visual-0: video 197 giây, do người dùng
cung cấp, phụ đề thưa thớt, Cách nấu
Xào thịt bò hoàn hảo mọi lúc

subtitle_ground("tiêu đen cho thịt bò",
visual[0])
Quan sát: Không tìm thấy đoạn.

Suy nghĩ: Tôi cần biết thêm thông tin..
video_narration("Mô tả nó", visual[0])
Quan sát: ...
3-6: một người đang cắt một miếng thịt bò trên thớt cắt.
một số văn bản trên khung hình (Thịt bò 350 g Taste) ...

narration_reason("bao nhiêu ...", visual[0])

Suy nghĩ: Tôi cần suy luận bao nhiêu tiêu đen
tôi nên sử dụng cho 700g thịt bò ...
Quan sát: 1 tsp

Chỉ suy luận với Ngôn ngữ
ReAct với PEIL

Đầu vào
video_segment = VideoSegment(video)
for frame in video_segment.frame_iterator():
    if frame.exists("thịt bò") and \
       frame.exists("tiêu đen"):
        return frame.simple_query("Tôi nên sử dụng bao nhiêu
        tiêu đen cho 700g thịt bò?")

code = generate_code(question)
Mã:

Thực thi mã:
answer = execute_code(code, video)

Phân tích câu hỏi:
Rủi ro: phụ đề có thể không cung cấp ngữ cảnh liên quan.
Rủi ro: Khi giới thiệu các phép đo, thịt bò và tiêu có thể không xuất hiện cùng nhau.

Chỉ suy luận với Mã

Hình 2: So sánh PEIL và hai phương pháp suy luận chính trong các tác vụ đa phương tiện.

kết quả? Đối với ReAct và ToolFormer, đầu ra của các mô hình bên ngoài có thể được đưa trực tiếp vào Planner và chuyển đến các mô hình tiếp theo. Trong khi các kết quả trung gian thu được trong các tác vụ đa phương tiện thường là các vùng được cắt cho mô-đun định vị hình ảnh, và các đoạn video được phân đoạn cho mô-đun định vị thời gian, như được hiển thị trong Hình 1. Trong các trường hợp phức tạp, khó để Planner quản lý thông tin nào nên được đưa vào mô-đun tiếp theo.

Trong bài báo này, chúng tôi đề xuất một hệ thống Trợ lý AI đa phương tiện, được đặt tên là AssistGPT (Thiết kế biểu tượng mô hình của chúng tôi được lấy cảm hứng từ HAL 9000 từ bộ phim "A Space Odyssey", một nhân vật trí tuệ nhân tạo hư cấu), với phương pháp suy luận ngôn ngữ và mã xen kẽ, kế thừa các ưu điểm của suy luận linh hoạt trong ReAct và gọi công cụ mạnh mẽ trong lập kế hoạch dựa trên chương trình. Cụ thể, hệ thống của chúng tôi bao gồm bốn phần, Planner, Executor, Inspector và Learner. Chúng tôi hiển thị cách hệ thống của chúng tôi hoạt động trong Hình 1. Tương tự như ReAct, Planner suy nghĩ về những gì cần được thực hiện tiếp theo dựa trên tiến trình suy luận hiện tại và gọi các mô hình bên ngoài. Điều làm cho phương pháp của chúng tôi khác biệt là việc sử dụng mã có định dạng để gọi các mô hình bên ngoài. Executor bao bọc các công cụ bên ngoài thành một định dạng đầu vào và đầu ra thống nhất, cho phép công cụ được gọi bằng các lệnh có cấu trúc. Đồng thời, chúng tôi cũng đã đề xuất một Inspector, quản lý đầu vào thị giác và kết quả trung gian trong quá trình suy luận. Nó cung cấp cho Planner các tóm tắt và siêu dữ liệu của tất cả các tài liệu thị giác hiện có sẵn. Sự kết hợp của Inspector và Executor cho phép mô hình thực hiện hiệu quả suy luận phức tạp. Hơn nữa, thật thách thức đối với mô hình để đảm bảo suy luận chính xác trong kịch bản zero-shot. Planner có thể xuất mã không hợp lệ hoặc các đường dẫn không hợp lý. Để cho phép hệ thống liên tục cải tiến, chúng tôi đề xuất Learner, kiểm tra xem quá trình dự đoán có hợp lý hay không hoặc đánh giá tính chính xác của kết quả dự đoán dựa trên chú thích. Nó cho phép hệ thống thử nhiều lần và ghi lại các ví dụ thành công như các ví dụ trong ngữ cảnh.

Phiên bản hiện tại của AssistGPT tích hợp hơn 10 công cụ cho các chức năng khác nhau, bao gồm phát hiện hình ảnh, captioning, định vị vùng, định vị thời gian, mô-đun OCR, liệt kê đối tượng, speech-to-text, v.v. Bằng cách kết hợp các chức năng này, AssistGPT có thể hoàn thành một loạt các tác vụ đa phương tiện vẫn còn khó khăn đối với các hệ thống hiện có.

Tóm lại, các đóng góp của chúng tôi như sau: 1) Chúng tôi đã xây dựng một trợ lý AI đa phương tiện tổng quát có thể hoàn thành các tác vụ liên quan đến thị giác đa dạng với sự hợp tác của nhiều mô hình. 2) Chúng tôi đề xuất một phương pháp suy luận tổ hợp mới suy luận theo cách ngôn ngữ và mã xen kẽ. Một cơ chế học tập đơn giản cũng được đề xuất để cải thiện khả năng lập kế hoạch của AssistGPT. 3) Chúng tôi trình bày khả năng của AssistGPT không chỉ kết quả benchmark mà còn một số ứng dụng thực tế để xử lý hình ảnh phức tạp và video dài, hiểu các truy vấn cấp cao và xử lý đầu vào linh hoạt.

## 2 Công trình liên quan

Hệ thống đa phương tiện. Trước sự ra đời của LLM, các công trình đáng kể đã được thực hiện để thiết kế các mô hình đa phương tiện cho một hoặc một số tác vụ cụ thể, chẳng hạn như tập trung vào diện mạo thị giác, kiến thức liên quan đến thị giác, hành động, video ego-centric, video hướng dẫn, văn bản cảnh, v.v. Họ đã đạt được kết quả đáng khen ngợi trong các tác vụ cụ thể, tuy nhiên, khả năng tổng quát hóa của chúng tương đối hạn chế, làm cho việc giải quyết các câu hỏi phức tạp và đa dạng hơn trong các kịch bản thế giới thực trở nên thách thức.

--- TRANG 4 ---

Gần đây, hai loại chiến lược được đề xuất để phát triển một hệ thống đa phương tiện tổng quát. Một là đào tạo trước LLM để hỗ trợ các đặc trưng thị giác như đầu vào có điều kiện. Các mô hình đại diện là GPT-4, PaLM-E, BLIP-2 và Mini-GPT4. Bất chấp những phương pháp này có khả năng xử lý trực tiếp đầu vào đa phương tiện, chúng vẫn thể hiện hạn chế trong việc giải quyết các nhu cầu chức năng nâng cao, chẳng hạn như định vị không gian hình ảnh, định vị video dài và hiểu âm thanh. Ngoài ra, chi phí tính toán để mở rộng các mô hình này có thể cực kỳ cao. Chiến lược thay thế nhằm kết hợp nhiều mô hình hoặc APIs để hoàn thành suy luận đa phương tiện phức tạp. Ví dụ, các mô hình như mô hình Socratic và Visual ChatGPT đạt được điều này bằng cách kết nối ChatGPT với các mô hình tạo hình ảnh. HuggingGPT kết hợp một loạt các mô hình Huggingface với LLMs. ViperGPT sử dụng Codex để gọi các APIs thị giác thông qua lập trình Python. AssistGPT của chúng tôi thuộc về danh mục thứ hai bằng cách kết hợp và gọi các mô-đun khác nhau cho suy luận đa phương tiện, nhưng chúng tôi đề xuất một framework mới PEIL để tích hợp các công cụ và mô hình bên ngoài.

Suy luận tổ hợp. Các phương pháp suy luận tổ hợp trong lĩnh vực trả lời câu hỏi thị giác thường phân tách câu hỏi thành một số tác vụ con, mỗi tác vụ được giải quyết bởi một mô-đun cụ thể. Loại phương pháp này cung cấp khả năng diễn giải mạnh mẽ do cấu trúc mô-đun của nó và sự phân chia trách nhiệm rõ ràng giữa các thành phần riêng lẻ. Ý tưởng này ban đầu được đưa ra bởi. Sau đó, đã giới thiệu một biến thể end-to-end dựa trên LSTM và CNN. Các phương pháp suy luận tổ hợp truyền thống bị hạn chế bởi khả năng phân tích của các mô hình ngôn ngữ, thường yêu cầu phân tách câu hỏi ground-truth hoặc học tăng cường để sử dụng mô-đun tối ưu.

Với sự ra đời của LLMs, phân tách câu hỏi có thể được thực hiện một cách đáng kể trong cách thức zero-shot. Chain-of-thought prompts, Toolformer và ReAct cho phép các mô hình lập kế hoạch cách giải quyết một vấn đề NLP. HuggingGPT và ViperGPT là các hệ thống đa phương tiện sử dụng LLM để phân tích một câu hỏi thành một loạt các bước suy luận. Tuy nhiên, đối với các truy vấn phức tạp, mô hình cần xác định các bước tiếp theo dựa trên không chỉ câu hỏi mà còn đầu vào thị giác hoặc phản hồi từ các mô-đun đã thực hiện trước đó. MMReAct đã giới thiệu ý tưởng của ReAct vào một hệ thống đa phương tiện để khắc phục điều này, trong khi nó vẫn đang phát triển và chưa chứng minh hiệu quả của nó trên benchmark. Các phương pháp trước đây suy luận qua suy luận ngôn ngữ hoặc mã, và như đã nêu trong phần giới thiệu, cả hai đều có những thiếu sót nhất định. Công trình của chúng tôi đầu tiên đề xuất một cách suy luận ngôn ngữ và mã xen kẽ có thể xử lý tốt hơn các truy vấn tổng quát và đầu vào thị giác phức tạp.

Sơ đồ học tập cho Hệ thống mô-đun. Các mô hình mô-đun sớm chủ yếu sử dụng Học tăng cường (RL) end-to-end để đào tạo lập kế hoạch và hành động của mỗi mô-đun từ đầu. Mặc dù phương pháp này thực tế đối với các mô hình nhẹ, RL có thể giới thiệu chi phí đáng kể cho các hệ thống mà mỗi mô-đun là một LLM. Toolformer đề xuất một kỹ thuật tự giám sát tối ưu hóa lập kế hoạch chỉ yêu cầu một số ít demonstration cho mỗi API. Cụ thể, Toolformer thử các APIs khác nhau để tìm các ví dụ thành công và sau đó fine-tune mô hình. Ngược lại, chúng tôi đề xuất một cơ chế đơn giản trong lĩnh vực đa phương tiện, có thể hướng dẫn hệ thống thử lại và bảo tồn các khám phá thành công như các ví dụ trong ngữ cảnh.

## 3 AssistGPT

Tổng quan. AssistGPT là một hệ thống trợ lý AI đa phương tiện tổng quát có thể tương tác động với các công cụ khác nhau theo cách ngôn ngữ và mã xen kẽ. Cụ thể, với một truy vấn ngôn ngữ tổng quát và hình ảnh hoặc video tham khảo làm đầu vào, mục tiêu của AssistGPT là tạo ra câu trả lời mong muốn. Như được hiển thị trong Hình 3, AssistGPT được thực hiện bằng sự hợp tác với bốn mô-đun cốt lõi: (a)Planner, (b) Executor, (c)Inspector và (d)Learner. Planner §3.1 nhằm mục đích kiểm soát toàn bộ quá trình suy luận, với Executor §3.2 cung cấp phản hồi có giá trị cho Planner bằng cách thực hiện các công cụ bên ngoài. Inspector §3.3 quản lý đầu vào và kết quả trung gian và hỗ trợ Planner đưa nội dung phù hợp vào Executor. Learner §3.4 có khả năng đánh giá hiệu suất hệ thống và ghi lại các khám phá thành công như các ví dụ trong ngữ cảnh. Trong các phần sau, chúng tôi sẽ đi qua từng mô-đun một cách chi tiết.

--- TRANG 5 ---

1. Kiểm tra xác thực
có

Người dùng
Suy nghĩ: Tôi cần tìm ...
Hành động:
[...lặp lại N lần cho đến khi có câu trả lời cuối cùng]
Suy nghĩ: Tôi biết câu trả lời cuối cùng
Câu trả lời cuối cùng: ...

Planner

Lời nhắc hướng dẫn:
Trả lời câu hỏi sau tốt nhất có thể.
[Minh họa bộ công cụ]
Mô-đun phát hiện văn bản: Được sử dụng để phát hiện văn bản [Ví dụ trong ngữ cảnh]
Câu hỏi: .... Suy nghĩ: ... Hành động: ... Quan sát:...

Executor Inspector

Tạo thông báo lỗi:
<mô-đun> chỉ
nhận video làm
đầu vào.
không

2. Thực thi mô-đun:
3. Xử lý hậu kỳ:
Cắt hình ảnh / Phân đoạn video

BLIP
Grounding DINO

Quan sát: ......

Thuộc tính Giá trị
loại video
nguồn do người dùng cung cấp
thời lượng 17
tóm tắt [caption]
... ...

Captioner hình ảnh/khung hình chính
và công cụ siêu dữ liệu

Tóm tắt siêu dữ liệu: visual-1: video 17 giây, video được phân đoạn
từ visual-0, phụ đề dày đặc, đoạn mục tiêu
cho truy vấn "Khi nào video ...

video_narration("Mô tả những gì xảy ra gần cuối video.",
visual[0])

Tóm tắt: visual-0, video 48.27 giây,
phụ đề thưa thớt, video do người dùng cung cấp, một
đoàn tàu đồ chơi trên sàn ...

narration_reasoning("tại sao đoàn tàu
dừng lại một lúc gần cuối video?", visual[0])

Câu trả lời cuối cùng: không có chỗ để đẩy xe

Lần thử thứ 1

1. Tự kiểm tra hoặc
so sánh GT

video_ground("Xác định cuối video.", visual[0])

video_narration("Mô tả nó.",
visual[1])

Câu trả lời cuối cùng: bị chặn bởi tay của cậu bé

narration_reasoning("tại sao đoàn tàu dừng lại một lúc gần cuối video?", visual[1])

visual-1: video 12 giây, phụ đề thưa thớt,
video được phân đoạn từ visual-0, đoạn mục tiêu
cho truy vấn "Xác định cuối ...

2. Ngân hàng bộ nhớ trong ngữ cảnh:
Lưu như ví dụ trong ngữ cảnh cho
lập kế hoạch và lời nhắc mô-đun tốt hơn

Learner

Quan sát: Đầu ra của một công cụ
Tóm tắt: visual-1: video 17 giây, ...

module_name(arg1, arg2)

Inspector
Inspector
Executor

Tóm tắt video và hình ảnh đầu vào: visual-0, video 48.27 giây, ...

Truy vấn đầu vào: Khi nào video ...?

Người dùng

✅module_name(arg1, arg2)

Truy vấn đầu vào: tại sao đoàn tàu dừng lại
một lúc gần cuối video?

Người dùng

Executor

Video và hình ảnh
đầu vào

Video và hình ảnh
đã xử lý

Lần thử thứ 2

nếu thất bại nếu thành công

đánh giá
đánh giá

❓
✅

Hình 3: Minh họa sơ đồ của hệ thống AssistGPT. Nó bao gồm bốn mô-đun cốt lõi:
●Planner: kiểm soát toàn bộ quá trình suy luận; ●Executor: thực hiện công cụ bên ngoài và trả về
phản hồi cho Planner; ●Inspector: quản lý đầu vào và kết quả trung gian; ●Learner: đánh giá
hiệu suất hệ thống và ghi lại các lần thử thành công như các ví dụ trong ngữ cảnh.

### 3.1 Planner

●Planner sử dụng một LLM rất thông minh tức là GPT-4 như bộ não trung tâm để kiểm soát lập kế hoạch suy luận toàn cục. Nó bắt đầu quá trình lập kế hoạch bằng cách nhận đầu vào từ ba loại thông tin: một Lời nhắc hướng dẫn bao gồm [Minh họa bộ công cụ] và [Ví dụ trong ngữ cảnh], Truy vấn đầu vào, và Tóm tắt đầu vào thị giác được tạo bởi ●Inspector.

Sau đó nó tạo ra đầu ra phù hợp cho bước tiếp theo, bao gồm hai phần: Suy nghĩ: một cụm từ ngôn ngữ chỉ ra những gì nên được thực hiện tiếp theo. Mặc dù nó không ảnh hưởng trực tiếp đến việc gọi mô-đun hoặc API, nó hỗ trợ quy trình lập kế hoạch LLM. Hành động: một chuỗi có cấu trúc tuân theo mẫu được xác định trước được cung cấp trong hướng dẫn. Nó chỉ định công cụ bên ngoài nào để gọi và đối số nào để nhập, ví dụ: caption("màu gì của chiếc xe?", visual[0]).

Sau mỗi lần ●Executor gọi đến một công cụ bên ngoài, công cụ trả về đầu ra dưới dạng ngôn ngữ tự nhiên, mà chúng tôi gọi là Quan sát. Nếu công cụ tạo ra một kết quả trung gian, ví dụ: một video được phân đoạn, ●Inspector của chúng tôi sẽ lưu trữ nó và tạo một Tóm tắt cho nó. Cả Quan sát và Tóm tắt sẽ được đưa vào ●Planner để hướng dẫn lập kế hoạch bước tiếp theo. Các phần sau sẽ giới thiệu thêm chi tiết về Hành động, Quan sát và Tóm tắt.

Hiện tại, chúng tôi tích hợp 13 công cụ chức năng trong AssistGPT để cung cấp hỗ trợ đa phương tiện, như được hiển thị trong Bảng 1. Các mô-đun này có thể được phân loại chủ yếu thành ba loại:

• Descriptor: Để hiểu và sử dụng hiệu quả dữ liệu thu được từ môi trường đa phương tiện phức tạp ví dụ: hình ảnh, video, âm thanh và văn bản, chúng tôi sử dụng nhiều mô hình cơ bản khác nhau như các bộ mô tả cơ bản để nhận thức. Các mô hình này, bao gồm (a) Image Caption, (b) Video Narration, (c) Object Detection, (d) Text Detection và (e) ASR Translation, cho phép chúng tôi trích xuất đủ thông tin từ nguồn đa dạng, do đó tăng cường hiểu biết của chúng tôi về kịch bản đa phương tiện.

--- TRANG 6 ---

Bảng 1: Mô-đun được sử dụng trong AssistGPT. Một mô-đun có thể có các mô hình khác nhau, được phân tách bằng dấu gạch chéo (/).

| Mô-đun | Sử dụng | Mô hình cốt lõi | Đầu vào | Đầu ra | Mô-đun | Sử dụng | Mô hình cốt lõi | Đầu vào | Đầu ra |
|---------|---------|----------------|---------|---------|---------|---------|----------------|---------|---------|
| (a) Image Caption | BLIP series | T, I | T | (h) Text Ground | Program + SSA | T, I | T, I |
| (b) Video Narration | BLIP series | T, V | T | (i) Subtitle Ground | GPT | T, Sub. | T, V |
| (c) Object Detection | G. Dino / GLIP | T, I | T | (j) Knowledge Reason | GPT | T | T |
| (d) Text Detection | Google OCR | I | T | (k) Narration Reason | GPT | T, Nar. | T |
| (e) ASR Translation | Whisper | A | T | (l) Subtitle Reason | GPT | T, Sub. | T |
| (f) Region Ground | OFA | T, I | T, I | (m) Temporal Reason | Rule-based | T, V | T, V |
| (g) Narration Ground | GPT / CLIP | T, Nar. | T, V | | | | | |

I: Hình ảnh, V: Video, T: Văn bản, A: Âm thanh, Nar.: Tường thuật, Sub.: Phụ đề

• Locator: Như câu nói, một hình ảnh đáng giá nghìn lời. Hình ảnh hoặc video thường chứa một kho tàng thông tin - đối tượng, thuộc tính, hành động, sự kiện, v.v. Tuy nhiên, sự dồi dào của thông tin đôi khi có thể cản trở quá trình giải quyết vấn đề của chúng ta. Một giải pháp quan trọng là xác định thông tin quan trọng và có giá trị nhất từ biển dữ liệu thị giác, văn bản và âm thanh phong phú. Phần này kết hợp một số mô-đun như (f) Region Ground, (g) Narration Ground, (h) Text Ground và (i) Subtitle Ground.

• Reasoner: Hai bộ công cụ ban đầu chủ yếu xử lý thu thập và xác định dữ liệu, trong khi bộ thứ ba tập trung vào suy luận, sử dụng thông tin được trích xuất và kiến thức bên ngoài. Phần này kết hợp các mô-đun như (j) Knowledge Reason, (k) Narration Reason, (l) Subtitle reason và (m) Temporal Reason. Các mô-đun này chủ yếu sử dụng LLM làm cốt lõi bằng cách nhận các loại thông tin và lời nhắc khác nhau làm đầu vào hoặc một chương trình đơn giản.

### 3.2 Executor

●Executor nhận mã được tạo bởi ●Planner làm đầu vào, sau đó gọi một mô-đun để tạo ra đầu ra bằng cách thực hiện ba bước để có được kết quả cuối cùng. Các bước này bao gồm kiểm tra xác thực, thực thi mô-đun và xử lý hậu kỳ, như được hiển thị trong Hình 3.

• Kiểm tra xác thực: Ngay cả LLM mạnh mẽ như GPT-4 đôi khi có thể tạo ra mã không hợp lệ. Ví dụ, một mô-đun caption hình ảnh chấp nhận một video dài làm đầu vào. Chúng tôi đã thiết kế một kiểm tra tính hợp pháp cho mỗi mô-đun để xác định xem mã có thể thực thi được hay không. Hơn nữa, nếu mã bao gồm lỗi, chúng tôi không ngắt toàn bộ quá trình suy luận. Thay vào đó, chúng tôi trả về một thông báo lỗi như đầu ra mã cho ●Planner, cho phép nó tối ưu hóa quá trình lập kế hoạch trong thời gian thực.

• Thực thi mô-đun: Chúng tôi tiêu chuẩn hóa các mô-đun hoặc APIs khác nhau thành một giao diện thống nhất sử dụng mẫu kiểu mã tức là [Module_Name](<text_query>, <visual_index>). Mỗi mô-đun được thiết kế để chấp nhận nhiều truy vấn văn bản và dữ liệu thị giác (hình ảnh hoặc video) làm đầu vào. Trong mỗi mô-đun tiêu chuẩn, chúng tôi cung cấp hướng dẫn về chức năng của nó và các yêu cầu của đối số, được sử dụng cho [Minh họa bộ công cụ] trong ●Planner. Ngoài ra, vì lý do đơn giản và chính xác trong Lập kế hoạch, mã được tạo được đơn giản hóa. Sau đó, một hàm dựa trên quy tắc đơn giản sẽ ánh xạ nó vào các mã có thể thực thi và sau đó thực hiện nó để có được kết quả cuối cùng.

• Xử lý hậu kỳ: Đối với tất cả các mô-đun, các kết quả được tạo sẽ được dịch thành định dạng ngôn ngữ để thông báo cho ●Planner về kết quả, như phần Quan sát được minh họa ở trên. Ví dụ, đối với mô-đun Narration Ground, mô hình sẽ trả về xem nó có tìm thấy đoạn liên quan hay không. Nếu có, xuất thời gian bắt đầu và kết thúc của đoạn. Ngoài ra, nhiều mô-đun liên quan đến Ground sẽ gửi video được phân đoạn hoặc vùng hình ảnh được cắt của chúng đến trình quản lý kết quả thị giác tiếp theo tức là ●Inspector.

### 3.3 Inspector

Mục tiêu của ●Inspector là quản lý các đầu vào thị giác được cung cấp bởi người dùng và các kết quả trung gian được tạo ra bởi hệ thống của chúng tôi để hỗ trợ ●Planner quyết định nguồn nào nên được chỉ đạo đến mô-đun nào. Cụ thể, ●Inspector ghi lại siêu dữ liệu của mỗi phần tử thị giác, bao gồm loại của nó (hình ảnh hoặc video), nguồn (được cung cấp bởi người dùng hoặc được tạo ra bởi hệ thống), và một mô tả ngắn gọn về nội dung (thu được từ mô hình caption, hoặc tiêu đề của một video trực tuyến). Đối với video, có một số siêu dữ liệu bổ sung, chẳng hạn như thời lượng của video, liệu nó có chứa âm thanh và phụ đề hay không. ●Inspector giám sát các đầu vào từ người dùng và các đầu ra từ ●Executor. Ngay khi một phần tử thị giác mới được nhận, nó thêm siêu dữ liệu, được ghi chú là Tóm tắt ở trên, vào lịch sử suy luận của ●Planner. Với sự hợp tác của ●Planner, ●Executor và ●Inspector, hệ thống của chúng tôi có thể tạo ra câu trả lời cho các truy vấn khó khăn với các đầu vào thị giác phức tạp.

### 3.4 Learner

Bất chấp khả năng tổng quát hóa mạnh mẽ của LLMs, chúng vẫn có thể dễ dàng gặp lỗi khi xử lý các truy vấn loại đa phương tiện. Do đó, việc có cơ chế tự đánh giá là cần thiết đối với một trợ lý AI. Để đạt được mục tiêu này, chúng tôi hy vọng rằng mô hình có thể tự kiểm tra tính hợp lý của đầu ra của nó. Mặt khác, khi ground truth có sẵn, chúng tôi dự định thu thập các trường hợp dự đoán thành công như các ví dụ trong ngữ cảnh. Cụ thể, AssistGPT sẽ liên tục cố gắng cung cấp câu trả lời khi phản hồi không thỏa mãn cho đến khi hoặc vượt qua tự kiểm tra, hoặc câu trả lời chính xác được đưa ra (khi ground truth có sẵn) hoặc đạt đến số lần thử tối đa được định trước. ●Learner bao gồm một trình đánh giá được thực hiện bởi LLM, hoạt động trong hai chế độ: tự đánh giá và so sánh ground-truth. Các chế độ này được kích hoạt tùy thuộc vào tính khả dụng của ground truth, và chúng tôi thảo luận về cả hai một cách riêng biệt.

• Chế độ tự đánh giá được kích hoạt khi không có phản hồi của người dùng hoặc ground truth có sẵn. Nó nhận vết suy luận và kết quả của mỗi bước làm đầu vào, cho phép GPT đánh giá xem suy luận có hoàn chỉnh, nhất quán và tuân thủ định dạng yêu cầu hay không.

• Chế độ so sánh ground-truth được kích hoạt khi các chú thích viên cung cấp ground truth. Trong chế độ này, GPT đánh giá xem dự đoán của AssistGPT có nhất quán về mặt ngữ nghĩa với ground truth được cung cấp hay không.

Hơn nữa, ●Learner khuyến khích tiếp tục cố gắng cho đến khi nhận được phản hồi tích cực hoặc đạt đến số lần thử tối đa. Sau khi tiến hành N lần khám phá, một số kết quả có thể phát sinh:

• Không cần điều chỉnh: Nếu mô hình đưa ra câu trả lời chính xác trong lần thử đầu tiên, điều này cho thấy AssistGPT có thể giải quyết câu hỏi hiện tại một cách hiệu quả. Do đó, không cần cải thiện.

• Sửa đổi kế hoạch: Nếu mô hình tạo ra câu trả lời chính xác sau n lần thử, trong đó 1 < n ≤ N, điều này ngụ ý rằng có chỗ để cải thiện khả năng lập kế hoạch của mô hình. Do đó, chúng tôi lưu vết suy luận thành công vào [Ngân hàng bộ nhớ trong ngữ cảnh]. Do đó, khi mô hình gặp một truy vấn tương tự trong tương lai, nó có thể sử dụng điều này như một ví dụ trong ngữ cảnh.

• Cập nhật chức năng: Nếu mô hình vẫn không thể cung cấp câu trả lời chính xác ngay cả sau N lần thử, rất có thể vấn đề nằm ở một mô-đun hoặc API cụ thể chứ không phải quá trình lập kế hoạch. Nó có thể cần cập nhật tăng dần cho mô-đun. Chúng tôi sẽ để dành điều này cho công việc tương lai.

## 4 Thí nghiệm

### 4.1 Thiết lập thí nghiệm

Bộ dữ liệu. Hệ thống của chúng tôi được đánh giá trên các benchmark A-OKVQA và NExT-QA được thiết kế để kiểm tra khả năng đa phương tiện toàn diện, bao gồm các sự kiện thị giác, thường thức, chuỗi thời gian, quan hệ nhân quả, v.v. A-OKVQA là một benchmark sáng tạo cho trả lời câu hỏi thị giác nhận biết kiến thức với 25K câu hỏi yêu cầu hiểu biết cấp cao về thường thức và kiến thức thế giới. Những câu hỏi trong A-OKVQA vượt ra ngoài thông tin chứa trong hình ảnh và không thể được trả lời chỉ bằng cách truy vấn một cơ sở kiến thức. Bên cạnh đó, câu hỏi đa dạng, bao trùm một loạt các miền như suy luận thường thức, dựa trên thị giác, dựa trên kiến thức và hiểu biết vật lý. Trong các thí nghiệm của chúng tôi, chúng tôi đánh giá hiệu suất mô hình trong thiết lập học trong ngữ cảnh trên tập validation, bao gồm 1.145 câu hỏi. NExT-QA là một benchmark để đánh giá khả năng suy luận nhân quả, suy luận hành động thời gian và tương tác đối tượng phong phú của hệ thống AI trong trả lời câu hỏi video. NExT-QA có tổng cộng 5.440 video, với độ dài trung bình 44 giây, và khoảng 52K cặp câu hỏi-trả lời được chú thích thủ công. Trong các thí nghiệm của chúng tôi, chúng tôi đánh giá hiệu suất mô hình trong thiết lập học trong ngữ cảnh trên tập validation, bao gồm 4.996 câu hỏi.

Chi tiết triển khai. Trong các thí nghiệm sau, chúng tôi sử dụng GPT-4 API được cung cấp bởi OpenAI như Planner. Trong các thí nghiệm A-OKVQA, chúng tôi thiết lập mô-đun Caption là BLIP2 hoặc InstructBLIP (viết tắt là Ins.BLIP), sử dụng Grounding Dino cho mô hình Object Detection và Google

--- TRANG 7 ---

OCR cho Text Detection. Đối với các thí nghiệm NExT-QA, mô-đun Video Narration của chúng tôi dựa trên InstructBLIP Vicuna-7B. Các thí nghiệm của chúng tôi được thực hiện trên 4 GPU A5000.

### 4.2 Kết quả định lượng

Bảng 2: So sánh SOTAs trên bộ dữ liệu A-OKVQA.
D.A. và M.C. chỉ câu trả lời trực tiếp và đa lựa chọn.
ICL: Học trong ngữ cảnh. ZS: Suy luận zero-shot.

| Mô hình | D. A. | M.C. | Sup. |
|---------|-------|------|------|
| LXMERT | 30.7 | 51.4 | |
| KRISP | 33.7 | 51.9 | |
| GPV-2 | 48.6 | 60.3 | |
| InstructBLIP Vicuna-7B | 64.0 | 75.7 | ICL |
| PromptCap | 56.3 | 73.2 | |
| AssistGPT (BLIP2 FlanT5 XL) | 42.6 | 73.7 | |
| AssistGPT (Ins.BLIP Vicuna-7B) | 44.3 | 74.7 | |

Bảng 3: Nghiên cứu phá bỏ của AssistGPT trên bộ dữ liệu A-OKVQA. Ins.BLIP được sử dụng ở đây là phiên bản được đào tạo trước.

| Mô hình LLM | D.A. | M.C. |
|-------------|------|------|
| - Ins.BLIP | 13.4 | 53.8 |
| Ins.BLIP + GPT-4 | 27.9 | 55.2 |
| AssistGPT Reason only | 28.8 | 65.9 |
| ReAct | 30.1 | 68.2 |
| PIE | 32.4 | 72.4 |
| PEIL w. Self-Check | 41.2 | 74.2 |
| PEIL w. GT-Check | 44.3 | 74.7 |

So sánh với State-of-the-arts. Từ kết quả trong Bảng 2, có thể thấy rằng trong track đa lựa chọn, hai phiên bản AssistGPT của chúng tôi (tức là với BLIP2 FlanT5 XL nhẹ và Ins.BLIP Vicuna-7B mạnh mẽ hơn) đạt được kết quả tốt nhất trong tất cả các phương pháp hiện tại trong thiết lập học trong ngữ cảnh. Đáng chú ý là chúng tôi sử dụng phiên bản được đào tạo trước của InstructBLIP, hoạt động ở mức 53.3%, như được hiển thị trong Bảng 3. Khi được tích hợp vào hệ thống của chúng tôi, nó có thể nâng cao hiệu suất của nó lên mức độ của mô hình fine-tuning. Đối với câu hỏi trả lời trực tiếp, mặc dù hiệu suất của chúng tôi có thể không phù hợp với các mô hình được đề xuất gần đây, nó vẫn có thể so sánh với SOTA được giám sát trước đó, như GPV-2.

Hiệu suất của chúng tôi trên câu trả lời trực tiếp không vượt qua các phương pháp trước đó. Lý do chính là đối với câu hỏi mở, các mô hình dựa vào LLM có xu hướng xuất các cụm từ hoàn chỉnh thay vì một từ đơn làm câu trả lời cuối cùng, ngay cả khi chúng tôi nhắc chúng cung cấp câu trả lời ngắn gọn nhất có thể. Ví dụ, đối với một câu hỏi đã cho "Lá cờ nào được đại diện trên tường?", AssistGPT xuất câu trả lời, "Lá cờ Hoa Kỳ", nhưng câu trả lời chính xác không bao gồm từ "lá cờ", do đó nó được coi là không chính xác. Loại lỗi này rất phổ biến trong AssistGPT. Trong phụ lục, chúng tôi hiển thị thêm các ví dụ để phân tích các trường hợp thất bại. Hơn nữa, so với phương pháp SOTA, PromptCap, nó đặc biệt đào tạo một mô hình caption hướng tới việc tạo caption cho A-OKVQA, đây cũng là lý do cho hiệu suất tốt của nó, trong khi hệ thống của chúng tôi tổng quát hơn.

Từ kết quả trong Bảng 4, AssistGPT đạt được hiệu suất cao hơn các phương pháp được giám sát được đề xuất gần đây, chứng minh hiệu quả của phương pháp của chúng tôi. Chúng ta có thể thấy rằng hiệu suất của mô hình chúng tôi chủ yếu cho thấy cải thiện đầy hứa hẹn hơn trong các câu hỏi Nhân quả và Mô tả, chủ yếu do mô hình của chúng tôi liên tục thu được thông tin chi tiết liên quan đến câu hỏi từ các video. Hơn nữa, phương pháp của chúng tôi không hoạt động tốt trên các câu hỏi thời gian. Lý do chính cho điều này là có tương đối ít mô hình định vị thời gian thế giới mở có sẵn, và công trình chính thống vẫn liên quan đến fine-tuning trên các bộ dữ liệu thế giới đóng. Do đó, chúng tôi phải sử dụng captioner hình ảnh InstructBLIP với GPT-4 để đạt được định vị thời gian. Hiệu quả không tốt bằng các mô hình được fine-tune nhưng có khả năng tổng quát hóa hơn. Hơn nữa, hiệu suất của nó cũng rất gần với công trình đồng thời gần đây, ViperGPT. ViperGPT vượt trội hơn một chút so với chúng tôi, có thể vì nó đã thiết kế một phương pháp dựa trên quy tắc tinh vi, kiểm tra lặp đi lặp lại xem các đối tượng có xuất hiện trong khung hình để thực hiện định vị thời gian hay không.

Nghiên cứu phá bỏ. Chúng tôi đã thiết kế một số biến thể của AssistGPT để kiểm tra hiệu quả của phương pháp được đề xuất của chúng tôi. Baseline cơ bản nhất là InstructBLIP (lưu ý rằng tất cả các mô hình sau đều sử dụng phiên bản Vicuna-7B), là nguồn thông tin thị giác chính trong AssistGPT. Vì InstructionBLIP không nhất thiết có thể xuất câu trả lời theo định dạng yêu cầu, chúng tôi thiết kế một biến thể, InstructionBLIP+GPT-4 cho phép GPT-4 tinh chỉnh thêm đầu ra của InstructionBLIP. Mô hình Reason-only lập kế hoạch trực tiếp tất cả các bước mà các mô hình cần chạy, tương tự như các công trình trước đó. Mô hình ReAct có khả năng thực hiện ReAct dựa trên ngôn ngữ. Tuy nhiên, không có Inspector và các dạng gọi giống như Code, một mô hình tiếp theo chỉ có thể chấp nhận đầu ra của mô hình trước đó, tương tự như. Chúng tôi cũng phá bỏ Learner, có ba phiên bản, PIE (tức là w/o. Learner), PEIL w. Self-Check và PEIL w. GT-Check.

Từ kết quả trong Bảng 3, chúng ta có thể thấy rằng mô hình Reason-only, lập kế hoạch tất cả các bước mà các mô hình cần thực hiện, cho thấy cải thiện đáng kể trong D.A. và M.C. Điều này chỉ ra rằng

--- TRANG 8 ---

Bảng 4: So sánh AssistGPT của chúng tôi với SOTAs trên bộ dữ liệu NExT-QA.

| Phương pháp | Nhân quả | Thời gian | Mô tả | Tất cả | Sup. |
|-------------|----------|-----------|-------|--------|------|
| HGA | 44.22 | 52.49 | 44.07 | 49.74 | |
| VQA-T | 49.60 | 51.49 | 63.19 | 52.32 | |
| ATP | 53.10 | 50.20 | 66.80 | 54.30 | |
| VGT | 52.28 | 55.09 | 64.09 | 55.02 | |
| MIST | 54.62 | 56.64 | 66.92 | 57.18 | |
| ViperGPT | - | - | - | 60.00 | ICL |
| AssistGPT | 60.02 | 51.38 | 67.26 | 58.36 | |

việc tích hợp nhiều mô hình có thể nâng cao hiệu suất mô hình. Mô hình ReAct, bất chấp không có Inspector và các dạng gọi giống như Code, cho thấy cải thiện thêm trong cả hai metric, vượt qua mô hình Reason-only. Điều này cho thấy hiệu quả của cách thức ReAct. Nhưng việc kết hợp ngôn ngữ và mã xen kẽ của chúng tôi, tức là PIE, mang lại cải thiện đáng kể hơn trên M.C. Cuối cùng, hai biến thể của PIE với các phá bỏ một phần, PEIL w. Self-Check và PEIL w. GT-Check, ghi được điểm cao nhất trên cả hai track, cho thấy hiệu quả của Learner. Learner cho thấy cải thiện đáng kể hơn trên track D.A. vì các mô hình trên D.A. thường không xuất được câu trả lời cực kỳ ngắn như yêu cầu bởi A-OKVQA. Learner có thể giảm thiểu điều này bằng cách thu thập các ví dụ trong ngữ cảnh.

### 4.3 Kết quả định tính

Câu hỏi 1: Loại phòng nào ở bên trái đoàn tàu màu vàng? Bạn phải chọn một câu trả lời từ: phòng trẻ em, phòng tắm, phòng thay đồ, phòng chờ

visual-0: một hình ảnh, do người dùng cung cấp, một đoàn tàu màu vàng và trắng đang tiến vào ga tàu

Suy nghĩ: Kiểm tra thông tin thị giác về phòng ở bên trái đoàn tàu màu vàng.
Hành động:
caption("Loại phòng nào ở bên trái đoàn tàu màu vàng?", visual[0])
Quan sát: Không có phòng nào ở bên trái đoàn tàu màu vàng.

Suy nghĩ: Quan sát mâu thuẫn với mô tả thị giác. Tôi sẽ cố gắng xác định vị trí phòng trong hình ảnh.
Hành động:
region_ground("phòng ở bên trái đoàn tàu màu vàng", visual[0])
Quan sát: tìm thấy vùng

visual-1: một hình ảnh, được cắt ...

Suy nghĩ: Bây giờ tôi có hình ảnh được cắt của phòng, tôi có thể kiểm tra loại của nó.
Hành động:
caption("Đây là loại phòng gì?", visual[1])
Quan sát: Ga tàu

Suy nghĩ: Bây giờ tôi có hình ảnh được cắt của phòng, tôi có thể kiểm tra loại của nó.
Hành động:
caption("Đây có phải là phòng trẻ em không? Đây có phải là phòng tắm không? ...", visual[1])
Quan sát: Câu trả lời cho mỗi câu hỏi là: không, không, không, không

Suy nghĩ: Không có lựa chọn nào phù hợp với quan sát.
Câu trả lời cuối cùng: Không có lựa chọn nào phù hợp với phòng.

AssistGPT: Lần thử đầu

caption("Vui lòng cung cấp thông tin chi tiết hơn về phòng này?", visual[1])
Quan sát: Hình ảnh mô tả một nhóm người tập hợp tại ga tàu, chờ đợi đoàn tàu đến.

AssistGPT: Lần thử thành công

knowledge_reason("Dựa trên thông tin chi tiết về phòng, lựa chọn nào sau đây có khả năng phù hợp nhất: phòng trẻ em, phòng tắm, phòng thay đồ, phòng chờ??", visual[1])
Quan sát: phòng chờ

region_ground("phòng ở bên trái đoàn tàu màu vàng", visual[0])

Suy nghĩ: Tôi biết câu trả lời cuối cùng.
Câu trả lời cuối cùng: phòng chờ

Baseline chỉ suy luận

Mô tả hình ảnh một cách chi tiết: Hình ảnh mô tả một ga tàu đông đúc, với một số người đứng trên sân ga chờ đợi đoàn tàu của họ. Có một đoàn tàu màu vàng và trắng đậu trên một trong những đường ray, với hành khách lên và xuống tàu. Ở phía sau, có một số đoàn tàu khác ....

InstructBLIP:
Trả lời câu hỏi dựa trên thông tin trên của hình ảnh:
Câu hỏi: ... Bạn phải chọn một câu trả lời từ: ...

Thông tin được cung cấp về hình ảnh không chỉ định loại phòng nào ở bên trái đoàn tàu màu vàng. Do đó, không thể trả lời một cách dứt khoát câu hỏi này dựa trên các chi tiết đã cho.

GPT-4:

Câu hỏi 2: tại sao em bé dừng lại một lúc gần cuối video? Bạn phải ...

Suy nghĩ: Xác định thời điểm cuối cùng trong video.
Hành động:
temporal_reason("cuối", visual[0])
Quan sát: Đoạn mục tiêu từ 02:05 -02:21.

visual-1: video 16 giây, được cắt ...

Suy nghĩ: Tôi cần biết thêm thông tin về video.
Hành động:
Video_narration("em bé có chơi với đồ chơi không? em bé có bị thu hút bởi chiếc ghế không? em bé có nhìn người đang quay phim không? em bé có vấp không? em bé có đọc từ giấy không?", visual[1])
Quan sát: Câu trả lời cho mỗi câu hỏi là: có, không, có, không, không

visual-0: video 81 giây, phụ đề dày đặc, video do người dùng cung cấp, một cô bé đang đi xe đạp ba bánh trên vỉa hè

Suy nghĩ: Tôi cần suy luận tại sao em bé dừng lại một lúc gần cuối video?
Hành động:
knowledge_reason("tại sao em bé dừng lại một lúc gần cuối video? ...", visual[1])
Quan sát: để nhìn người đang quay phim

Suy nghĩ: Tôi biết câu trả lời cuối cùng.
Câu trả lời cuối cùng: để nhìn người đang quay phim

Hình 4: Kết quả định tính trên bộ dữ liệu A-OKVQA (Câu hỏi 1) và NExT-QA (Câu hỏi 2).

Trong Hình 4, chúng tôi hình dung một số trường hợp dự đoán từ A-OKVQA (Câu hỏi 1) và NExT-QA (Câu hỏi 2). Từ cả hai ví dụ, có thể thấy rằng AssistGPT có thể phân tách câu hỏi thành các tác vụ con hợp lý và sau đó hoàn thành chúng từng bước một, cuối cùng thu được câu trả lời cuối cùng. Hơn nữa, do phương pháp suy luận mã và ngôn ngữ xen kẽ, mô hình có thể gọi hiệu quả nội dung cần thiết làm đầu vào. Từ quá trình suy luận của Câu hỏi 1, chúng ta cũng có thể thấy khả năng tự sửa lỗi của AssistGPT. Khi mô hình thị giác xuất kết quả không thỏa mãn, AssistGPT có thể gọi động các mô-đun khác, như mô-đun ground để suy luận qua một đường dẫn khác. Ngoài ra, đối với Câu hỏi

--- TRANG 9 ---

1, lần thử đầu tiên của mô hình không mang lại kết quả hiệu quả, và nó sẽ tự động tối ưu hóa kế hoạch vì nó không vượt qua tự kiểm tra. Ngoài ra, chúng tôi cũng trình bày kết quả của baseline chỉ suy luận. Nó đầu tiên gọi InstructBLIP để xuất một caption, sau đó sử dụng GPT-4 để suy luận. Vì thông tin từ caption không đáp ứng yêu cầu, dẫn đến kết quả không chính xác. Tuy nhiên, một khi dự đoán thất bại, mô hình không có cách để tự tối ưu. Đáng chú ý là tính năng quan trọng nhất của phương pháp chúng tôi là nó có thể giải quyết các vấn đề phức tạp hơn so với những vấn đề trong benchmark, như ví dụ trong Hình 1. Chúng tôi hiển thị thêm các ví dụ thực tế trong Phụ lục.

## 5 Kết luận và Hạn chế

Trong bài báo này, chúng tôi đề xuất một hệ thống trợ lý đa phương tiện mới có tên AssistGPT tận dụng phương pháp suy luận mã và ngôn ngữ xen kẽ, cụ thể là Lập kế hoạch, Thực thi, Kiểm tra và Học hỏi (PEIL). Hệ thống sáng tạo này tích hợp LLM với các công cụ khác nhau để giải quyết các thách thức do các tác vụ phức tạp dựa trên thị giác đặt ra. Kết quả thí nghiệm của chúng tôi trên các benchmark A-OKVQA và NExT-QA chứng minh hiệu quả của AssistGPT. Hơn nữa, chúng tôi trình bày khả năng của hệ thống trong việc xử lý các kịch bản thế giới thực đa dạng và phức tạp. Hệ thống của chúng tôi cũng có một số hạn chế. Phương pháp của chúng tôi không đề xuất một giải pháp cập nhật end-to-end, điều quan trọng khi các công cụ được sử dụng mắc lỗi. Một hạn chế khác là quá trình lập kế hoạch yêu cầu giải thích rộng rãi về các công cụ, dẫn đến chi phí tương đối lớn, có thể được cải thiện bằng cách chưng cất một planner kích thước nhỏ hơn.

## Phụ lục

Trong phụ lục, chúng tôi cung cấp các chi tiết bổ sung cho bài báo chính:
• Thêm thảo luận với các hệ thống mô-đun hiện có trong Phần A
• Thêm chi tiết về AssistGPT trong Phần B.
• Thêm kết quả định tính của A-OKVQA trong Phần C.
• Thêm ví dụ thực tế trong Phần D.

### A Thảo luận với các Hệ thống mô-đun điều khiển bởi LLM hiện có

Bảng 5: So sánh các hệ thống mô-đun điều khiển bởi LLM hiện có. Chúng tôi so sánh các phương pháp hiện có từ bốn chiều: Tập trung tác vụ, Phương pháp suy luận, Quản lý nguồn (cách họ quản lý đầu vào và kết quả trung gian), và liệu họ có khả năng học tập hay không. Thuật ngữ "ReAct" trong bảng không chỉ nghiêm ngặt đề cập đến việc sử dụng ReAct, mà là nó biểu thị lập kế hoạch và thực hiện đồng thời.

| Mô hình | Tập trung tác vụ | Suy luận | Quản lý nguồn | Học tập |
|---------|------------------|----------|---------------|---------|
| | NLP | Hình ảnh | Video | Định dạng | ReAct | Định dạng đầu vào | Phương pháp | |
| Toolformer | ✓ | ✗ | ✗ | lang. & prog. | ✓ | chỉ văn bản | - | ✓ |
| WebGPT | ✓ | ✗ | ✗ | chương trình | ✓ | chỉ kiểm tra | - | ✓ |
| Visual ChatGPT | ✗ | ✓ | ✗ | ngôn ngữ | ✗ | đa V. | Tên tệp | ✗ |
| ViperGPT | ✗ | ✓ | ✓ | chương trình | ✗ | đơn V. | Biến | ✗ |
| VisProg | ✗ | ✓ | ✗ | chương trình | ✗ | đơn V. | Biến | ✗ |
| MM-ReAct | ✗ | ✓ | ✓ | ngôn ngữ | ✓ | đa V. | Tên tệp | ✗ |
| Chameleon | ✓ | ✓ | ✗ | ngôn ngữ | ✗ | đơn V. | Cập nhật cache | ✗ |
| HuggingGPT | ✗ | ✓ | ✓ | ngôn ngữ | ✗ | đa V. | Tên tệp | ✗ |
| AssistGPT (của chúng tôi) | ✗ | ✓ | ✓ | lang. & prog. | ✓ | đa V. | Inspector | ✓ |

Trong Bảng 5, chúng tôi so sánh các hệ thống mô-đun điều khiển bởi LLM hiện có với AssistGPT của chúng tôi từ bốn góc độ:

Tập trung tác vụ. Từ góc độ Tập trung tác vụ, hiện tại có ba công trình có thể xử lý video: Hugging GPT, MM-ReAct và ViperGPT. Hugging GPT và MM-ReAct chỉ chứng minh khả năng xử lý video của họ thông qua một số ví dụ đơn giản (do đó chúng tôi đánh dấu chúng với dấu kiểm màu cam ✓). Ví dụ, Hugging GPT thể hiện tính năng tạo video của nó, trong khi MM-ReAct trình bày khả năng thực hiện các tác vụ như tóm tắt và định vị dựa trên phụ đề. Tuy nhiên, những phương pháp này chưa được xác thực trên bất kỳ benchmark nào. ViperGPT có thể xử lý câu hỏi dựa trên nội dung thị giác. So với những công trình này, AssistGPT có khả năng xử lý các tác vụ trả lời câu hỏi video phức tạp và tổng quát hơn, bao gồm hiểu phụ đề, nội dung thị giác và OCR, và chứng minh khả năng hiểu video dài.

--- TRANG 10 ---

Suy luận. Về mặt suy luận, các mô hình đa phương tiện hiện có chủ yếu áp dụng kiểu chỉ suy luận, tức là trực tiếp suy ra các bước giải pháp dựa trên câu hỏi. Phương pháp này gặp khó khăn trong việc xử lý đầu vào thị giác phức tạp, và khi kết quả trung gian không đáp ứng kỳ vọng, mô hình cũng khó tự sửa lỗi. MM-ReAct giới thiệu ReAct gốc cho suy luận trong các tác vụ đa phương tiện, nhưng do sự không đầy đủ của ReAct gốc trong việc xử lý kết quả trung gian phức tạp không phải văn bản, sơ đồ lập kế hoạch hiện tại của nó để giải quyết các vấn đề liên quan đến video về cơ bản là hai bước: trích xuất tất cả thông tin từ video và sau đó có một LLM trả lời câu hỏi. Ngược lại, bài báo này đề xuất một sơ đồ suy luận Lập kế hoạch, Thực thi, Kiểm tra và Học hỏi (PEIL) tổng quát hơn. Trong trường hợp video phức tạp, phương pháp suy luận ngôn ngữ và mã xen kẽ của chúng tôi cho phép lập kế hoạch ngôn ngữ linh hoạt cho bước tiếp theo, và mã có cấu trúc để gọi đầu vào và kết quả trung gian, do đó tạo thuận lợi cho việc xử lý câu hỏi và nội dung thị giác phức tạp.

Quản lý nguồn. Xử lý đầu vào phức tạp và một số lượng lớn kết quả trung gian thường rất quan trọng trong các quá trình suy luận phức tạp. Các phương pháp suy luận dựa trên ngôn ngữ hiện tại chủ yếu sử dụng tên tệp để gắn nhãn tài nguyên. Chameleon đề xuất một cơ chế cập nhật với cache liên tục cập nhật kết quả suy luận hiện tại. Suy luận dựa trên chương trình, mặt khác, sử dụng các biến để lưu trữ kết quả trung gian. Một thiếu sót của các phương pháp này là sự không thể của Planner dựa trên ngôn ngữ để nhanh chóng hiểu nội dung của các nguồn thị giác, điều này cản trở việc sử dụng hiệu quả các nguồn khác nhau để hoàn thành các tác vụ con khác nhau. Kết quả là, công trình hiện có gặp khó khăn trong việc xử lý đầu vào linh hoạt và kết quả trung gian. Mặc dù một số công trình hỗ trợ nhiều nguồn thị giác như đầu vào, chúng thường được xử lý theo lô cho các tác vụ tương tự, với mỗi nguồn yêu cầu các hoạt động tương tự. Ví dụ, trong HuggingGPT, tác vụ tính tổng số ngựa vằn trong một số hình ảnh liên quan đến việc đếm số lượng ngựa vằn trong mỗi hình ảnh. Ngược lại, công trình của chúng tôi giới thiệu Inspector, ghi lại siêu dữ liệu và tóm tắt của mỗi nguồn thị giác và cung cấp nó cho Planner để suy luận. Thiết kế này có thể hỗ trợ đầu vào phức tạp. Ví dụ, một hình ảnh góc nhìn người dùng mô tả vấn đề hiện tại của người dùng, và một video tham khảo như một nguồn kiến thức, AssistGPT sau đó có thể sử dụng hai loại nguồn khác nhau này để cùng nhau trả lời câu hỏi của người dùng.

Học tập. Hầu hết các hệ thống mô-đun đa phương tiện thiếu khả năng tối ưu hóa liên tục. Bài báo này đề xuất một cơ chế cập nhật đơn giản cho phép mô hình tự kiểm tra tính hợp lý của đầu ra của nó và cuối cùng tiếp tục thu thập các ví dụ học tập trong ngữ cảnh.

### B Thêm chi tiết về AssistGPT

Bảng 6: Lệnh gọi và Minh họa cho các Mô-đun trong AssistGPT.

| Mô-đun | Lệnh gọi | Minh họa |
|---------|----------|----------|
| (a) Image Caption | caption(query, visual[i]) | trích xuất thông tin thị giác trong một hình ảnh. |
| (b) Video Narration | video_narration(query, visual[i]) | xuất tường thuật dựa trên thông tin thị giác của video. |
| (c) Object Detection | object_detect(query, visual[i]) | phát hiện các đối tượng yêu cầu trong một hình ảnh. |
| (d) Text Detection | text_detect(None, visual[i]) | trích xuất OCR trong một hình ảnh. |
| (e) ASR Translation | asr(None, visual[i]) | chuyển âm thanh thành văn bản. |
| (f) Region Ground | region_ground(query, visual[i]) | xác định vùng được truy vấn trong một hình ảnh. |
| (g) Narration Ground | narration_ground(query, visual[i]) | tìm đoạn dựa trên tường thuật của video. |
| (h) Text Ground | text_ground(query, visual[i]) | tìm vị trí của một văn bản cụ thể trong hình ảnh. |
| (i) Subtitle Ground | subtitle_ground(query, visual[i]) | tìm đoạn dựa trên phụ đề của video. |
| (j) Knowledge Reason | knowledge_reason(query, []) | suy luận câu trả lời dựa trên thường thức. |
| (k) Narration Reason | narration_reason(query, visual[i]) | suy luận câu trả lời dựa trên tường thuật của video. |
| (l) Subtitle Reason | subtitle_reason(query, visual[i]) | suy luận câu trả lời dựa trên phụ đề của video. |
| (m) Temporal Reason | temporal_reason(query, visual[i]) | tìm đoạn dựa trên từ quan hệ thời gian. |

Trong Bảng 6, chúng tôi hiển thị các lệnh gọi và minh họa của mỗi mô-đun trong AssistGPT. Chúng tôi cung cấp thêm chi tiết về cách mỗi mô-đun được triển khai.

• Image Caption: Mô hình cốt lõi của mô-đun này là một mô hình captioning có điều kiện văn bản, ví dụ: BLIP2, InstructBLIP, tương tự như một mô hình Trả lời câu hỏi thị giác mở.

• Video Narration: Vì các mô hình tường thuật video tổng quát chưa chín muồi, chúng tôi hiện tại sử dụng mô hình captioning hình ảnh để thực hiện chức năng này. Cụ thể, chúng tôi lấy mẫu khung hình ảnh (1/3 FPS cho triển khai hiện tại) và thực hiện captioning có điều kiện văn bản trên mỗi khung hình. Chúng tôi sử dụng captioning có điều kiện văn bản vì, nếu chúng tôi sử dụng captioning dày đặc, văn bản đầu ra sẽ quá dồi dào, làm cho các mô hình tiếp theo khó sử dụng. Tính năng Video Narration cũng có thể tùy chọn đọc nội dung OCR trong các khung hình. OCR được trích xuất sẽ được thêm vào caption của mỗi khung hình.

• Object Detection: Chức năng chính của mô-đun này là xác định xem hình ảnh có chứa các đối tượng được đề cập trong truy vấn hay không và giải quyết các câu hỏi liên quan đến đếm. Do đó, nó chứa một mô hình phát hiện đối tượng tập mở, ví dụ: Grounding DINO, có thể xuất các hộp giới hạn của các đối tượng liên quan dựa trên truy vấn. Chúng tôi cũng để mô-đun tính số lượng các đối tượng liên quan.

• Text Detection: Mô hình này được sử dụng để trích xuất OCR từ hình ảnh, và văn bản được trích xuất được trả về cho Planner. Chúng tôi sử dụng Google OCR để đạt được mục đích này.

• ASR Translation: Mô hình này được sử dụng để chuyển đổi âm thanh từ video thành văn bản. Chúng tôi sử dụng mô hình ASR (Nhận dạng giọng nói tự động) mã nguồn mở của OpenAI, Whisper, để thực hiện điều này. ASR được phát hiện tổ chức dấu thời gian và văn bản theo cách tương tự như phụ đề. Trong triển khai, chúng tôi tự động chạy mô-đun này ngay khi chúng tôi nhận được một video có âm thanh.

• Region Ground: Mục đích của mô-đun này là tìm một khu vực cụ thể của hình ảnh dựa trên truy vấn. Chúng tôi sử dụng OFA-Large, được fine-tune trên RefCOCO, để đạt được nó.

• Narration Ground: Chức năng của mô hình này là tìm các đoạn thời gian liên quan đến truy vấn dựa trên tường thuật của video. Chúng tôi đề xuất hai triển khai: 1) Chúng tôi sử dụng GPT-4, nhận tường thuật của video và truy vấn làm lời nhắc, để xuất dấu thời gian của các đoạn thời gian. 2) Một giải pháp khác là sử dụng CLIP để thực hiện điều đó. Chúng tôi có thể chia video thành một số đoạn, và tính toán độ tương tự giữa khung hình trong mỗi đoạn và truy vấn. Dấu thời gian của đoạn có độ tương tự cao nhất sẽ được xuất. Trong các thí nghiệm sơ bộ của chúng tôi, giải pháp đầu tiên cho thấy khả năng diễn giải và tổng quát hóa tốt hơn, vì vậy nó được áp dụng trong đánh giá benchmark.

• Text Ground: Mục đích của mô hình này là xác định các khu vực cụ thể của hình ảnh tương ứng với một văn bản nhất định. Khả năng này có thể hướng dẫn người dùng xác định thông tin quan trọng trong hình ảnh phức tạp, giàu văn bản, chẳng hạn như giao diện người dùng. Định dạng truy vấn là text[:object_name], trong đó text biểu thị văn bản cần được xác định, và object_name (tùy chọn) được sử dụng để xác định văn bản trên một đối tượng cụ thể, ví dụ: "menu: button". Cụ thể, mô hình hoạt động trong hai giai đoạn: 1) Dựa trên kết quả phát hiện Nhận dạng ký tự quang học (OCR), mô hình xác định các khu vực của hình ảnh phù hợp với đoạn văn bản của truy vấn. Điều này được đạt được bằng cách tính toán khoảng cách giữa truy vấn và OCR được trích xuất, và khi khoảng cách chỉnh sửa dưới một ngưỡng cụ thể, nó được coi là một kết quả phù hợp. 2) Nếu nhiều hơn một khu vực văn bản được xác định, chúng tôi tinh chỉnh thêm kết quả dựa trên tên đối tượng. Chúng tôi sử dụng Semantic Segment Anything (SSA) để phân đoạn hình ảnh về mặt ngữ nghĩa, xác định các vùng phù hợp với tên đối tượng được đề cập trong truy vấn.

• Subtitle Ground: Mô hình này tương tự như mô hình narration grounding, nhưng nó sử dụng phụ đề của video làm đầu vào thay vì tường thuật. Do đó, chúng tôi cũng sử dụng GPT-4 để đạt được nó.

• Knowledge Reason: Mục đích của mô hình này là cho phép mô hình áp dụng kiến thức bên ngoài để trả lời câu hỏi. Chúng tôi hiện tại không kết nối internet để lấy kiến thức, nhưng sử dụng kiến thức mà GPT-4 đã học được. Cụ thể, mô hình này cho phép GPT-4 sử dụng kiến thức riêng của nó để suy luận câu trả lời dựa trên câu hỏi và kết quả của tất cả các bước suy luận trước đó.

• Narration Reason: Mục đích của mô-đun này là suy luận một số thông tin dựa trên nội dung thị giác của video. Mô-đun này cũng sử dụng GPT-4, nhận truy vấn và tường thuật của video đầu vào làm lời nhắc, để suy luận câu trả lời.

• Subtitle Reason: Mục đích của mô-đun này là suy luận một số thông tin dựa trên phụ đề của video. Nó tương tự như Narration Reason, nhưng nhận phụ đề của video đầu vào và truy vấn làm lời nhắc, để suy luận câu trả lời.

• Temporal Reason: Mô hình này có thể tìm một đoạn video dựa trên một số từ quan hệ thời gian. Đầu vào cho mô-đun này tuân theo định dạng sau: temporal_word: time stamps, ví dụ: after: 3 - 6. Từ quan hệ thời gian bao gồm hai loại, một là từ quan hệ thời gian tuyệt đối, chẳng hạn như ở giữa/đầu/cuối video. Loại thứ hai là từ quan hệ thời gian tương đối, chẳng hạn như trước và sau. Đối với loại từ đầu tiên, chúng tôi chia video thành 5 đoạn và sau đó xuất dấu thời gian của đoạn tương ứng theo temporal_word. Đối với loại thứ hai, chúng tôi chia video thành 8 đoạn, và sau đó, theo dấu thời gian đầu vào, chúng tôi xuất dấu thời gian của đoạn trước hoặc sau nó. Các siêu tham số hiện tại, việc chia các đoạn video, vẫn còn sơ bộ. Sẽ tốt hơn nhiều nếu sử dụng mô hình để chia chúng về mặt ngữ nghĩa, và sau đó thực hiện suy luận thời gian trong tương lai.

### C Kết quả định tính trong A-OKVQA

Trong Hình 5, chúng tôi trình bày một trường hợp thành công cùng với một số ví dụ thất bại, minh họa các mẫu lỗi thường gặp nhất trong A-OKVQA. Như có thể thấy, AssistGPT có thể tạo ra các quá trình trả lời có tính diễn giải cao. Hơn nữa, ngay cả trong các trường hợp câu hỏi được trả lời không chính xác, vẫn có những giải thích tương đối hợp lý được cung cấp. Trong phần sau, chúng tôi minh họa chi tiết các mẫu lỗi phổ biến:

• Định dạng đầu ra không mong muốn: Đối với câu hỏi Trả lời trực tiếp, như Q2, kết quả của AssistGPT giống với câu trả lời chính xác về mặt ý nghĩa, nhưng cách biểu đạt khác nhau, điều này sẽ được coi là không chính xác theo các metric hiện có.

• Nhận dạng chi tiết: Việc nhận dạng các danh mục chi tiết của một số đối tượng vẫn không được thực hiện tốt bởi các mô hình thị giác hiện có, dẫn đến câu trả lời cuối cùng không chính xác. Ví dụ, AssistGPT không thành công trong việc nhận dạng viên thuốc ho trong Q3.

• Tư thế thành văn bản: Hiện tại, có rất ít mô hình có thể ánh xạ tư thế hoặc hành động chi tiết của con người hoặc động vật thành ngôn ngữ tự nhiên. Ví dụ, việc nắm bắt hành động nhảy lên của con mèo trong Q4 là một thách thức. AssistGPT hiện tại không kết hợp một mô hình liên quan để nắm bắt thông tin như vậy. Thay vào đó, nó đưa ra dự đoán dựa trên các đối tượng xung quanh liên quan đến con mèo.

• Suy luận không nhất quán: Bất chấp AssistGPT có một số cơ chế tự sửa lỗi, đôi khi nó thể hiện sự không nhất quán trong quá trình suy luận, có thể dẫn đến sự không chính xác cuối cùng. Ví dụ, trong Q5, mô hình ban đầu xác định xe màu cam là một xe tải, nhưng trong các bước tiếp theo, nó được gọi là xe buýt đưa đón. Thật không may, AssistGPT không phát hiện được sự không nhất quán này và không tiến hành thực hiện các sửa chữa cần thiết.

### D Ví dụ dự đoán thực tế

Chúng tôi hiển thị một số ví dụ về AssistGPT xử lý các kịch bản thực tế trong Hình 6 và Hình 7. Từ các ví dụ thực tế khác nhau, rõ ràng là AssistGPT có thể xử lý khéo léo một loạt các loại video, dù đó là video hướng dẫn dày đặc, có phụ đề (Q2, Q3), hoặc những video có nội dung thị giác phong phú với văn bản trên khung hình thưa thớt (Q1, Q4, Q5). Thật ấn tượng, khi đối mặt với các truy vấn cấp cao (Q2 và Q3), mô hình thể hiện khả năng xác định chiến lược nội dung hữu ích, xác định chính xác các phản hồi đúng và đưa ra câu trả lời đa phương tiện toàn diện. Một khả năng tự sửa lỗi đáng chú ý cũng rõ ràng trong quá trình suy luận của nó, như được chứng minh trong Q2. Ở đây, mô hình tường thuật không thể tạo ra các tường thuật có ý nghĩa và, do đó, đã chọn sử dụng phụ đề để trả lời câu hỏi.

Hơn nữa, trong Q5, chúng tôi nhấn mạnh rằng mô hình của chúng tôi có thể xử lý hiệu quả nhiều đầu vào video phục vụ các chức năng khác nhau. Điều này bao gồm một hình ảnh góc nhìn người dùng và một cặp video tham khảo. Điều quan trọng cần lưu ý là mô hình của chúng tôi có thể chứa bất kỳ số lượng đầu vào nào. Do đó, với việc kết hợp chức năng tìm kiếm video YouTube, mô hình có thể tự động tìm kiếm một số video tham khảo và sau đó tham chiếu chéo chúng để phân biệt ý định của người dùng.

Tóm lại, chúng tôi muốn nhấn mạnh rằng AssistGPT là một hệ thống trợ lý đa phương tiện toàn diện, có khả năng quản lý một loạt các truy vấn ứng dụng thế giới thực phức tạp và toàn diện hơn nhiều so với các mẫu được cung cấp trong các benchmark.

--- TRANG 14-21 ---

[Phần còn lại của tài liệu chứa các hình ảnh minh họa chi tiết về quá trình suy luận của AssistGPT trong các tình huống thực tế khác nhau, bao gồm các câu hỏi về nhận dạng các thiết lập trong video, hướng dẫn nấu ăn, và các tài liệu tham khảo học thuật. Nội dung này bao gồm các ví dụ cụ thể về cách hệ thống xử lý các truy vấn phức tạp và danh sách tài liệu tham khảo đầy đủ.]
