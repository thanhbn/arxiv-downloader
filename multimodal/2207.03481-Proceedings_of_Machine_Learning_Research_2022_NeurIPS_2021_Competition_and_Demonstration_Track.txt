# 2207.03481.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2207.03481.pdf
# File size: 1963739 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Proceedings of Machine Learning Research 2022 NeurIPS 2021 Competition and Demonstration Track
Training Transformers Together
Alexander Borzunovâˆ—borzunov.alexander@gmail.com
Max Ryabininâˆ—mryabinin0@gmail.com
HSE University, Yandex
Tim Dettmersâˆ—dettmers@cs.washington.edu
University of Washington
Quentin Lhoestâˆ—quentin@huggingface.co
Lucile Saulnierâˆ—lucile@huggingface.co
Hugging Face
Michael Diskin michael.s.diskin@gmail.com
HSE University, Yandex
Yacine Jernite yacine@huggingface.co
Thomas Wolf thomas@huggingface.co
Hugging Face
Abstract
The infrastructure necessary for training state-of-the-art models is becoming overly expen-
sive, which makes training such models aordable only to large corporations and institu-
tions. Recent work proposes several methods for training such models collaboratively, i.e.,
by pooling together hardware from many independent parties and training a shared model
over the Internet. In this demonstration, we collaboratively trained a text-to-image trans-
former similar to OpenAI DALL-E. We invited the viewers to join the ongoing training
run, showing them instructions on how to contribute using the available hardware. We
explained how to address the engineering challenges associated with such a training run
(slow communication, limited memory, uneven performance between devices, and security
concerns) and discussed how the viewers can set up collaborative training runs themselves.
Finally, we show that the resulting model generates images of reasonable quality on a
number of prompts.
Keywords: distributed training, volunteer computing, transformers, text-to-image, mem-
ory eciency, communication eciency, heterogeneous hardware, security
1. Introduction
Training state-of-the-art deep learning models is becoming ever more computationally de-
manding. One infamous example of this trend is transformers (Vaswani et al., 2017), a
popular architecture widely used in NLP (Devlin et al., 2019; Liu et al., 2019; Brown et al.,
2020), speech processing (Gulati et al., 2020; Li et al., 2019), and computer vision (Doso-
vitskiy et al., 2020; Touvron et al., 2021; Caron et al., 2021). Transformers benet from
having billions of parameters (Brown et al., 2020; Kaplan et al., 2020; Ott et al., 2018) and
large-batch training (Popel and Bojar, 2018), which makes them dependent on large-scale
training infrastructure (Narayanan et al., 2021; Shoeybi et al., 2019; Lepikhin et al., 2020).
âˆ—Equal contribution.
Â©2022 A. Borzunov, M. Ryabinin, T. Dettmers, Q. Lhoest, L. Saulnier, M. Diskin, Y. Jernite & T. Wolf.arXiv:2207.03481v1  [cs.LG]  7 Jul 2022

--- PAGE 2 ---
Borzunov Ryabinin Dettmers Lhoest Saulnier Diskin Jernite Wolf
Unfortunately, this kind of infrastructure can be prohibitively expensive, whether one
buys the hardware or rents cloud resources (Turner; Li, 2020). As a result, most researchers
simply cannot aord to conduct the necessary experiments to develop their ideas, which
ultimately slows down scientic progress.
To make large-scale deep learning more accessible, recent work proposes to train these
models collaboratively, i.e., to pool together the hardware from many independent parties
and train a shared model over the Internet (Pascutto and Linscott, 2019; Ryabinin and
Gusev, 2020; Kijsipongse et al., 2018; Atre et al., 2021; Diskin et al., 2021). Such work
proposes general distributed algorithms for training on many devices with uneven compute
capability and reliability. However, to make them practical, one must overcome several
engineering challenges, such as slow communication, limited memory, and security concerns.
In this demonstration, we collaboratively trained a text-to-image transformer similar to
DALL-E (Ramesh et al., 2021). Our contributions are the following:
â€¢We modify the DALL-E model, making it suitable for training over the Internet using
the method from Diskin et al. (2021) and the hivemind library (hivemind, 2020). We
set up the infrastructure for such a training run and publish the training results.
â€¢We provide a webpage1explaining how to join the ongoing training run, address
challenges related to collaborative training runs (slow communication, low memory
budget, support of heterogeneous devices), and set up such a training run by yourself.
â€¢We provide an interactive \calculator" that shows the memory consumed by dierent
models in case of using various memory-eciency techniques. Also, we present a
tutorial on setting up dataset streaming and model compression using the datasets
andbitsandbytes libraries (Lhoest et al., 2021; Dettmers et al., 2021).
2. Demonstration Contents
2.1. Main webpage
The central part of our demonstration is a webpage where people can explore the demon-
stration materials. The webpage describes the motivation behind collaborative training
projects, the method for ecient training from Diskin et al. (2021), and the ongoing col-
laborative training of our adapted version of DALL-E (see Section 3). Here, we also show
a plot of the training objective and the number of active participants.
Next, we provide instructions on how to join the training run using free cloud providers or
their own GPU. This involves (1) joining a specic Hugging Face organization, where we can
authenticate the users and measure their contribution, and (2) running a Jupyter notebook
(Kluyver et al., 2016) with the training code. Our intention was that the user can explore
our collaborative training environment through active participation while at the same time
reading the detailed explanations of how it works. Here, we also provide the link to the
interactive dashboard which shows the statistics and the leaderboard of contributors and
provides further information about the training run, such as model checkpoints uploaded
to the Model Hub, notebooks for inference, and links to the source code.
Then, we proceed to discuss the engineering challenges of collaborative training runs:
1. See https://training-transformers-together.github.io
2

--- PAGE 3 ---
Training Transformers Together
â€¢Communication eciency. Most distributed training algorithms are designed for
the networks inside HPC clusters with a 10{100 Gbit/s bandwidth. However, typical
Internet connections are orders of magnitude slower (10{100 Mbit/s). To make train-
ing over the Internet practical, one can reduce the communication costs using large-
batch training (You et al., 2020), gradient compression (Dettmers, 2015; Lin et al.,
2018; Vogels et al., 2019; Tang et al., 2021), parameter sharing (Lan et al., 2020; Xue
et al., 2021), and overlapping computation with communication (Ren et al., 2021).
â€¢Uneven device performance. Traditional data-parallel training waits for the slow-
est device on every batch. Diskin et al. (2021) allow the devices to process dierent
numbers of samples for a batch, while keeping the guarantees of synchronous training.
â€¢Memory eciency. Distributed training requires either storing all parameters and
optimizer statistics on each participant, which is challenging in the case of low-end
hardware, or using model parallelism which introduces another level of complexity.
Fortunately, the rst option is often viable if we reduce the memory consumption
with 8-bit optimizers (Dettmers et al., 2021), by ooading the statistics to CPU,
with gradient checkpointing or parameter sharing (Lan et al., 2020; Xue et al., 2021).
â€¢Dataset streaming. Participants often cannot store or even download the whole
dataset, since datasets used for pretraining transformers may contain hundreds of
gigabytes of data. To address that, one can use dataset streaming tools, such as the
datasets library (Lhoest et al., 2021).
â€¢Security. Crucially, the participants only exchange tensors and never send code
to be executed on each other's computers. Since a malicious participant also could
inuence the training outcome by sending wrong tensors, we should either authenticate
participants, as described in Diskin et al. (2021), and/or use gradient aggregation
techniques robust to outliers (Karimireddy et al., 2020; Gorbunov et al., 2021).
Finally, we provide a recipe on how to combine all that and set up a new collaborative
training run using the hivemind library (hivemind, 2020).
2.2. Memory calculator
The demonstration webpage includes an interactive \calculator" showing the benets of var-
ious memory-eciency techniques and their combinations. It can compute the consumption
of RAM and GPU memory for BERT (Devlin et al., 2019), T5 (Rael et al., 2020), GPT-2
(Radford et al., 2019), GPT-3 (Brown et al., 2020), GPT-J (Wang and Komatsuzaki, 2021),
and DALL-E (Ramesh et al., 2021) in case of using 8-bit optimizers, ooading the optimizer
statistics to CPU, using gradient checkpointing and parameter sharing.
2.3. Tutorial on memory-eciency techniques
The demonstration webpage refers to a tutorial on setting up dataset streaming with the
datasets library (Lhoest et al., 2021) and model compression with the bitsandbytes
library (Dettmers et al., 2021). The goal of the tutorial is to ne-tune the GPT-2 Large
model (Radford et al., 2019) on the C4 dataset (Rael et al., 2020) using only a low-end
GPU, which is possible with the 8-bit Adam optimizer.
3

--- PAGE 4 ---
Borzunov Ryabinin Dettmers Lhoest Saulnier Diskin Jernite Wolf
3. Collaborative Training Run
3.1. Model
For the practical example of a collaborative training run, we chose to train a text-to-image
transformer similar to DALL-E (Ramesh et al., 2021), based on the code from Wang (2021).
Specically, we used a decoder-only transformer with 1024 hidden units and 64 layers, each
of which uses 16 attention heads with a per-head state size of 64 ( 1.1B parameters in
total). We alternated the attention masks as in the original paper, i.e., repeated \row,
column, row, row" masks until the last layer, which had the convolutional mask.
To improve communication and memory eciency, we tied weights of all \row, column,
row, row" layer groups (Lan et al., 2020) and tied the input and output embeddings (Press
and Wolf, 2016), so the model uses 8x fewer parameters (but the same amount of compute).
We also used reversible layers (Br ugger et al., 2019) to reduce memory usage and rotary
embeddings (Su et al., 2021) to improve training stability.
We replaced dVAE with VQ-GAN (Esser et al., 2021), since it has a smaller reconstruc-
tion error. We used the checkpoint with f=8 and the codebook size 8192. Finally, we used
CLIP ViT/B-32 (Radford et al., 2021) to choose the best 4 out of 128 generated images.
3.2. Dataset
We trained the model on the rst 100 million image-text pairs from LAION-400M (Schuh-
mann et al., 2021). We skipped 10% images due to short captions, extreme aspect ratios,
and NSFW labels.
Before training, we preprocessed all images with VQGAN and uploaded the VQGAN
codes and captions, both compressed with Brotli (Alakuijala et al., 2018), to the Hugging
Face Dataset Hub (Lhoest et al., 2021). During training, we streamed the compressed codes
instead of the original images, thus consuming 18x less bandwidth.
3.3. Training procedure
We followed the distributed training procedure from Diskin et al. (2021) and used the 8-
bit LAMB optimizer (You et al., 2020; Dettmers et al., 2021) ooaded to CPU. We used
the linear training schedule with 31250 steps (the rst 10% is the warm-up) and the peak
learning rate of 2 :510 3. While exchanging gradients and parameters, we used the 8-bit
quantization (Dettmers, 2015) for tensors with 216elements and the 16-bit precision for
other tensors. Unlike the original paper, we did not use PowerSGD (Vogels et al., 2019).
3.4. Results
The training run lasted for 2.5 months and passed 80% of the training schedule. Besides
the authors, 37 volunteers have contributed for at least 10 minutes (see Appendix A).
During inference, we note that limiting sampling to top 256 logits or top logits whose
probability sums up to p= 0:75 greatly improves the image quality. The nal model
generates realistic images for some prompts but fails to draw correct shapes for the others,
while using the appropriate image style, textures, and colors (see Appendix B). We attribute
that to the fact that our model is too small to remember the full diversity of images in
LAION-400M. Still, the model can generalize to the concepts not present in the dataset.
4

--- PAGE 5 ---
Training Transformers Together
References
Jyrki Alakuijala, Andrea Farruggia, Paolo Ferragina, Eugene Kliuchnikov, Robert Obryk,
Zoltan Szabadka, and Lode Vandevenne. Brotli: A general-purpose data compressor.
ACM Transactions on Information Systems (TOIS) , 37(1):1{30, 2018.
Medha Atre, Birendra Jha, and Ashwini Rao. Distributed deep learning using volunteer
computing-like paradigm, 2021.
Romain Beaumont. Easily compute CLIP embeddings and build a CLIP retrieval system
with them. https://github.com/rom1504/clip-retrieval , 2021.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020.
Robin Br ugger, Christian F. Baumgartner, and Ender Konukoglu. A partially reversible
u-net for memory-ecient volumetric image segmentation. arXiv:1906.06148 , 2019.
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv e J egou, Julien Mairal, Piotr Bojanowski,
and Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv
preprint arXiv:2104.14294 , 2021.
Boris Dayma. DALL Â·E Mega - Training Journal, Sample Pre-
dictions. https://wandb.ai/dalle-mini/dalle-mini/reports/
DALL-E-Mega-Training-Journal--VmlldzoxODMxMDI2#sample-predictions , 2022.
Tim Dettmers. 8-bit approximations for parallelism in deep learning. ICLR , 2015.
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-
wise quantization, 2021.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training
of deep bidirectional transformers for language understanding. In NAACL-HLT , 2019.
Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Anton Sinitsin, Dmitry
Popov, Dmitry V Pyrkin, Maxim Kashirin, Alexander Borzunov, Albert Villanova del
Moral, et al. Distributed deep learning in open collaborations. Advances in Neural
Information Processing Systems , 34:7879{7897, 2021.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain
Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929 , 2020.
Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution
image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 12873{12883, 2021.
Eduard Gorbunov, Alexander Borzunov, Michael Diskin, and Max Ryabinin. Secure dis-
tributed training at scale. arXiv preprint arXiv:2106.11257 , 2021.
5

--- PAGE 6 ---
Borzunov Ryabinin Dettmers Lhoest Saulnier Diskin Jernite Wolf
Anmol Gulati, Chung-Cheng Chiu, James Qin, Jiahui Yu, Niki Parmar, Ruoming Pang,
Shibo Wang, Wei Han, Yonghui Wu, Yu Zhang, and Zhengdong Zhang, editors. Con-
former: Convolution-augmented Transformer for Speech Recognition , 2020.
hivemind. Hivemind: a Library for Decentralized Deep Learning. https://github.com/
learning-at-home/hivemind , 2020.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jerey Wu, and Dario Amodei. Scaling laws for neural
language models, 2020.
Sai Praneeth Karimireddy, Lie He, and Martin Jaggi. Learning from history for byzantine
robust optimization. arXiv preprint arXiv:2012.10333v1 , 2020.
Ekasit Kijsipongse, Apivadee Piyatumrong, and Suriya U-ruekolan. A hybrid gpu cluster
and volunteer computing platform for scalable deep learning. The Journal of Supercom-
puting , 04 2018. doi: 10.1007/s11227-018-2375-9.
Thomas Kluyver, Benjamin Ragan-Kelley, Fernando P erez, Brian E Granger, Matthias Bus-
sonnier, Jonathan Frederic, Kyle Kelley, Jessica B Hamrick, Jason Grout, Sylvain Corlay,
et al. Jupyter Notebooks-a publishing format for reproducible computational workows. ,
volume 2016. 2016.
Zhen-Zhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
Radu Soricut. Albert: A lite bert for self-supervised learning of language representations.
InInternational Conference on Learning Representations , 2020.
Dmitry Lepikhin, H. Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Y. Huang, M. Krikun,
Noam Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation
and automatic sharding. ArXiv , abs/2006.16668, 2020.
Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick
von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall,
Joe Davison, Mario Sa sko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven
Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp
Schmid, Sylvain Gugger, Cl ement Delangue, Th eo Matussi ere, Lysandre Debut, Stas
Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran cois Lagunas, Alexan-
der Rush, and Thomas Wolf. Datasets: A community library for natural language
processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations , pages 175{184, Online and Punta Cana,
Dominican Republic, November 2021. Association for Computational Linguistics. URL
https://aclanthology.org/2021.emnlp-demo.21 .
Chuan Li. Demystifying gpt-3 language model: A technical overview, 2020. " https:
//lambdalabs.com/blog/demystifying-gpt-3 ".
N. Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with
transformer network. In AAAI , 2019.
6

--- PAGE 7 ---
Training Transformers Together
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep Gradient Compres-
sion: Reducing the communication bandwidth for distributed training. In The Interna-
tional Conference on Learning Representations , 2018.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized
bert pretraining approach. ArXiv , abs/1907.11692, 2019.
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Pat-
wary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan
Catanzaro, et al. Ecient large-scale language model training on gpu clusters. arXiv
preprint arXiv:2104.04473 , 2021.
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine trans-
lation. In Proceedings of the Third Conference on Machine Translation: Research Papers ,
pages 1{9, Brussels, Belgium, October 2018. Association for Computational Linguistics.
doi: 10.18653/v1/W18-6301. URL https://www.aclweb.org/anthology/W18-6301 .
Gian-Carlo Pascutto and Gary Linscott. Leela chess zero, 2019. URL http://lczero.org/ .
M. Popel and Ondrej Bojar. Training tips for the transformer model. The Prague Bulletin
of Mathematical Linguistics , 110:43 { 70, 2018.
Or Press and Lior Wolf. Using the output embedding to improve language models. arXiv
preprint arXiv:1608.05859 , 2016.
Alec Radford, Je Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
Language models are unsupervised multitask learners. 2019.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervision. In International Conference
on Machine Learning , pages 8748{8763. PMLR, 2021.
Colin Rael, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, W. Li, and Peter J. Liu. Exploring the limits of transfer learning
with a unied text-to-text transformer. ArXiv , abs/1910.10683, 2020.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford,
Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International
Conference on Machine Learning , pages 8821{8831. PMLR, 2021.
Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan
Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-ooad: Democratizing billion-scale
model training, 2021.
Max Ryabinin and Anton Gusev. Towards crowdsourced training of large neural networks
using decentralized mixture-of-experts. Advances in Neural Information Processing Sys-
tems, 33:3659{3672, 2020.
7

--- PAGE 8 ---
Borzunov Ryabinin Dettmers Lhoest Saulnier Diskin Jernite Wolf
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton
Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-
400m: Open dataset of clip-ltered 400 million image-text pairs. arXiv preprint
arXiv:2111.02114 , 2021.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and
Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using
gpu model parallelism. arXiv preprint arXiv:1909.08053 , 2019.
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced trans-
former with rotary position embedding. arXiv preprint arXiv:2104.09864 , 2021.
Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li,
Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 1-bit adam: Communication e-
cient large-scale training with adam's convergence speed. In International Conference on
Machine Learning , pages 10118{10129. PMLR, 2021.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles,
and Herv e J egou. Training data-ecient image transformers & distillation through at-
tention. In International Conference on Machine Learning , pages 10347{10357. PMLR,
2021.
Elliot Turner. Estimate of GPT-3 training cost based on public cloud GPU/TPU cost
models, from Elliot Turner's personal page (accessed on May 29, 2020).
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in
neural information processing systems , 30, 2017.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Powersgd: Practical low-rank
gradient compression for distributed optimization. Advances in Neural Information Pro-
cessing Systems , 32, 2019.
Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Lan-
guage Model. https://github.com/kingoflolz/mesh-transformer-jax , May 2021.
Phil Wang. DALLE-pytorch. Implementation / replication of DALL-E, OpenAI's Text
to Image Transformer, in Pytorch. https://github.com/lucidrains/DALLE-pytorch ,
2021.
Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, and Yang You. Go wider instead
of deeper. arXiv preprint arXiv:2107.11817 , 2021.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli,
Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimiza-
tion for deep learning: Training bert in 76 minutes, 2020.
8

--- PAGE 9 ---
Training Transformers Together
Appendix A. Top Volunteers by Contributed Compute Time
0 5 10 15 20 25
GPU-daysbrickfrogExpl0dingCatnaclbitNikiTrickygenerallyintelligent-ainicobossAnnasflyyufelixjkintree
Figure 1: Hugging Face usernames of volunteers who contributed the most compute time.
Appendix B. Model Inference Results
(a) aerial view of the beach during daytime
(b) a beautiful sunset at a beach with a shell on the shore
(c) flower dress, size M
(d) a photo of san francisco golden gate bridge
9

--- PAGE 10 ---
Borzunov Ryabinin Dettmers Lhoest Saulnier Diskin Jernite Wolf
(e) a graphite sketch of a gothic cathedral
(f) Pele and Maradona in a match
(g) a small red block sitting on a large green block
(h) half human half Eiffel tower
(i) the Communist Statue of Liberty
Figure 2: Inference results of the nal model (the prompts are taken from Dayma (2022)):
(a){(c) Prompts leading to realistic outputs.
(d){(f) Prompts where the model fails to draw the correct object shapes, but
uses the appropriate image style, textures, and colors.
(g){(i) Prompts where the model is able to generalize and draw the concepts
not present in the training set. This is checked by inspecting training set images
whose CLIP embeddings are close to the prompt embeddings (Beaumont, 2021).
10
