# Trung bình trọng số chuyên gia: Một kế hoạch đào tạo tổng quát mới cho Vision Transformers

Yongqi Huang1†, Peng Ye1†, Xiaoshui Huang2, Sheng Li3,
Tao Chen1∗, Tong He2, Wanli Ouyang2
1Trường Khoa học và Công nghệ Thông tin, Đại học Fudan, 2Phòng thí nghiệm AI Thượng Hải,
3Đại học Tài chính và Kinh tế Giang Tây
{19307130163, yepeng20}@fudan.edu.cn

Tóm tắt
Tái tham số hóa cấu trúc là một kế hoạch đào tạo tổng quát cho Mạng nơ-ron tích chập (CNN), đạt được cải thiện hiệu suất mà không tăng chi phí suy luận. Khi Vision Transformers (ViTs) đang dần vượt qua CNN trong các tác vụ thị giác khác nhau, người ta có thể đặt câu hỏi: có tồn tại một kế hoạch đào tạo dành riêng cho ViTs mà cũng có thể đạt được cải thiện hiệu suất mà không tăng chi phí suy luận không? Gần đây, Mixture-of-Experts (MoE) đã thu hút sự chú ý ngày càng tăng, vì nó có thể mở rộng hiệu quả dung lượng của Transformers với chi phí cố định thông qua các chuyên gia được kích hoạt thưa thớt. Xem xét rằng MoE cũng có thể được xem như một cấu trúc đa nhánh, chúng ta có thể sử dụng MoE để thực hiện một kế hoạch đào tạo ViT tương tự như tái tham số hóa cấu trúc không? Trong bài báo này, chúng tôi trả lời khẳng định những câu hỏi này, với một chiến lược đào tạo tổng quát mới cho ViTs. Cụ thể, chúng tôi tách riêng các giai đoạn đào tạo và suy luận của ViTs. Trong quá trình đào tạo, chúng tôi thay thế một số Mạng Feed-Forward (FFN) của ViT bằng các MoE được thiết kế đặc biệt, hiệu quả hơn, gán token cho các chuyên gia bằng phân vùng đồng nhất ngẫu nhiên, và thực hiện Trung bình trọng số chuyên gia (EWA) trên các MoE này ở cuối mỗi lần lặp. Sau khi đào tạo, chúng tôi chuyển đổi mỗi MoE thành một FFN bằng cách tính trung bình các chuyên gia, biến đổi mô hình trở lại thành ViT ban đầu để suy luận. Chúng tôi cũng cung cấp một phân tích lý thuyết để cho thấy tại sao và cách thức hoạt động. Các thí nghiệm toàn diện trên các tác vụ thị giác 2D và 3D khác nhau, kiến trúc ViT, và bộ dữ liệu xác nhận hiệu quả và khả năng tổng quát hóa của kế hoạch đào tạo đề xuất. Bên cạnh đó, kế hoạch đào tạo của chúng tôi cũng có thể được áp dụng để cải thiện hiệu suất khi tinh chỉnh ViTs. Cuối cùng, nhưng không kém phần quan trọng, kỹ thuật EWA đề xuất có thể cải thiện đáng kể hiệu quả của MoE ngây thơ trong các bộ dữ liệu nhỏ thị giác 2D khác nhau và các tác vụ thị giác 3D.

1 Giới thiệu

Trong những năm gần đây, việc phát triển các kế hoạch đào tạo tổng quát đã đóng vai trò quan trọng trong học sâu. Những phương pháp này tăng cường các mô hình hiện có mà không cần sửa đổi kiến trúc của chúng, và không yêu cầu bất kỳ giả định nào về các kiến trúc cụ thể. Kết quả là, chúng có thể được sử dụng cho các mô hình khác nhau và cung cấp lợi ích đồng nhất.

Đối với kiến trúc CNN, kế hoạch đào tạo tổng quát được sử dụng rộng rãi là tái tham số hóa cấu trúc [6; 9;10;8] (Hình 1(b)). Cách tiếp cận này tách riêng các giai đoạn đào tạo và suy luận của mô hình CNN bằng cách sử dụng cấu trúc đa nhánh trong quá trình đào tạo, sau đó được chuyển đổi tương đương trở lại cấu trúc đơn nhánh ban đầu trong quá trình suy luận. Tái tham số hóa cấu trúc đạt được hiệu suất cải thiện mà không tăng chi phí suy luận. Tuy nhiên, cấu trúc đa nhánh được sử dụng trong quá trình đào tạo có thể làm chậm tốc độ đào tạo (như thể hiện trong cột thứ hai của Bảng 1, độ trễ đào tạo của nó lên đến 2.18 × so với đào tạo thông thường). Ngoài ra, tái tham số hóa cấu trúc chỉ áp dụng cho CNN, không áp dụng cho các kiến trúc mạnh mẽ khác gần đây như ViTs (xem cột thứ sáu của Bảng 1).

Đối với kiến trúc transformer, phương pháp đào tạo tổng quát gần đây bao gồm việc thay thế lớp mạng feed-forward (FFN) bằng một hỗn hợp thưa thớt các chuyên gia (MoE) [23;12;11;34;16] (Hình 1(a)). MoE là một thành phần mạng mới bao gồm nhiều chuyên gia (FFN) với trọng số độc nhất và một mạng định tuyến có thể huấn luyện. Trong các giai đoạn đào tạo và suy luận, mạng định tuyến chọn một kết hợp thưa thớt các chuyên gia cho mỗi đầu vào, cho phép mở rộng hiệu quả dung lượng của các mô hình transformer thông qua tính toán thưa thớt. Trong lĩnh vực xử lý ngôn ngữ tự nhiên, nhiều mô hình MoE thưa thớt [23; 12; 11] đã chứng minh rằng chúng vượt trội hơn các đối tác dày đặc.

Tuy nhiên, MoE vẫn có một số hạn chế làm hạn chế việc sử dụng rộng rãi trong Vision Transformers (ViTs). Thứ nhất, V-MoE [34] và SwinV2-MoE [16] được tiền huấn luyện trên các bộ dữ liệu quy mô lớn (JFT-300M [38], ImageNet-22K [33]), và khi được tiền huấn luyện trên ImageNet-1K [5], hiệu suất của V-MoE thấp hơn ViT [44]. Hơn nữa, không có nghiên cứu nào tìm hiểu MoE Vision Transformer trên các bộ dữ liệu nhỏ hơn như CIFAR-100 [20] và Tiny-ImageNet [21]. Thứ hai, các MoE ViTs hiện tại [34;45;44;16;19] tập trung vào các tác vụ thị giác 2D, và không có MoE ViTs cho các tác vụ thị giác 3D (xem hàng thứ tư của Bảng 1). Thứ ba, cơ chế định tuyến top-k của MoE bản thân nó không hiệu quả và không hữu hiệu. Như thể hiện trong hàng thứ tư của Bảng 1, độ trễ đào tạo và suy luận đều tăng lên 1.12 × vì định tuyến top-k yêu cầu tính toán bổ sung để đưa ra quyết định định tuyến cho mỗi đầu vào, và các tham số đào tạo và suy luận đều tăng lên rất nhiều đến 3.25 × gây gánh nặng cho việc triển khai mô hình. Bên cạnh đó, định tuyến top-k dễ dẫn đến mất cân bằng tải trên các chuyên gia, do đó nhiều nghiên cứu đã được dành để giải quyết vấn đề này thông qua các thiết kế khác nhau, như thêm các hàm mất mát cân bằng tải phụ trợ [23;12], đưa ra quyết định định tuyến toàn cầu [24], hoặc thiết kế lại thuật toán định tuyến [35; 50].

Do đó, những câu hỏi sau đây nảy sinh: Có tồn tại một phương pháp đào tạo mới có thể huấn luyện hiệu quả các chuyên gia được kích hoạt thưa thớt trong giai đoạn đào tạo, và một mô hình thưa thớt có thể được chuyển đổi thành một mô hình dày đặc mà không có chi phí tính toán bổ sung trong giai đoạn suy luận không? Chúng ta có thể mở rộng thêm hiệu quả của MoE cho cả các bộ dữ liệu thị giác 2D và các tác vụ thị giác 3D không?

Trong bài báo này, chúng tôi thiết kế một chiến lược đào tạo tổng quát mới cho ViTs được gọi là đào tạo Trung bình trọng số chuyên gia (EWA). Cách tiếp cận của chúng tôi đạt được cải thiện hiệu suất mà không tăng độ trễ suy luận và tham số, như thể hiện trong hàng thứ hai và cuối cùng của Bảng 1. Cụ thể, chúng tôi tách riêng các giai đoạn đào tạo và suy luận của ViT, như thể hiện trong Hình 1(c). Trong quá trình đào tạo, chúng tôi thay thế một số FFN của ViT thông thường bằng các MoE được thiết kế đặc biệt, hiệu quả hơn, gán token cho các chuyên gia bằng Phân vùng đồng nhất ngẫu nhiên (RUP), không yêu cầu tham số bổ sung, thiết kế đặc biệt, hoặc hàm mất mát phụ trợ. Hơn nữa, ở cuối mỗi lần lặp, chúng tôi thực hiện Trung bình trọng số chuyên gia (EWA) trên mỗi MoE. Trong quá trình suy luận, bằng cách đơn giản tính trung bình các chuyên gia của mỗi MoE thành một FFN duy nhất, chúng tôi có thể chuyển đổi mô hình MoE thưa thớt thành một mô hình ViT thông thường mà không mất hiệu suất.

Chúng tôi tiến hành các thí nghiệm toàn diện trên các tác vụ thị giác 2D và 3D khác nhau, kiến trúc ViT, và bộ dữ liệu để xác nhận hiệu quả và khả năng tổng quát hóa của kế hoạch đào tạo đề xuất. Ví dụ, phương pháp đào tạo đề xuất của chúng tôi đạt được cải thiện 1.72% trong tác vụ phân loại hình ảnh và cải thiện 1.74% mIoU trong tác vụ phân đoạn ngữ nghĩa điểm mây. Hơn nữa, đào tạo EWA cũng có thể được sử dụng để tinh chỉnh các mô hình ViT được tiền huấn luyện. Phương pháp đào tạo của chúng tôi cải thiện việc tinh chỉnh ViT-B trên CIFAR100 từ 90.71% lên 91.42% (↑0.71%). Cuối cùng, chúng tôi phát hiện rằng công nghệ chính của Trung bình trọng số chuyên gia trong đào tạo EWA có thể cải thiện rất nhiều hiệu quả của MoE ngây thơ trong các bộ dữ liệu nhỏ thị giác 2D khác nhau và các tác vụ thị giác 3D.

Các đóng góp chính được tóm tắt như sau:
• Chúng tôi đề xuất một kế hoạch đào tạo tổng quát mới cho ViTs, đạt được cải thiện hiệu suất mà không tăng độ trễ suy luận và tham số. Trong quá trình đào tạo, chúng tôi thay thế một số FFN trong kiến trúc ViT bằng MoE sử dụng Phân vùng đồng nhất ngẫu nhiên và thực hiện Trung bình trọng số chuyên gia trên các lớp MoE này sau mỗi lần cập nhật trọng số mô hình. Sau khi đào tạo, chúng tôi chuyển đổi mỗi lớp MoE trong mô hình thành một lớp FFN bằng cách tính trung bình các chuyên gia, do đó biến đổi mô hình trở lại kiến trúc ViT ban đầu để suy luận.
• Chúng tôi xác nhận rằng đào tạo EWA có thể mang lại cải thiện hiệu suất thống nhất cho các kiến trúc Vision Transformer khác nhau trên các tác vụ thị giác 2D/3D và bộ dữ liệu khác nhau. Hơn nữa, chúng tôi phát hiện rằng đào tạo EWA cũng có thể được sử dụng để tinh chỉnh các mô hình ViT được tiền huấn luyện, đạt được cải thiện hiệu suất hơn nữa. Chúng tôi cung cấp phân tích lý thuyết để cho thấy tại sao và cách thức hoạt động.
• Chúng tôi khám phá hiệu quả của MoE ngây thơ trên các bộ dữ liệu thị giác 2D nhỏ và các tác vụ thị giác 3D. Chúng tôi phát hiện rằng việc sử dụng MoE ngây thơ dẫn đến hiệu suất thấp hơn so với các đối tác dày đặc, nhưng kỹ thuật Trung bình trọng số chuyên gia đề xuất có thể cải thiện rất nhiều hiệu quả của MoE ngây thơ.

2 Các nghiên cứu liên quan

2.1 MoE

Sparsely-Gated Mixture-of-Experts (MoE) được giới thiệu lần đầu trong [36] và đã cho thấy những cải thiện đáng kể về thời gian đào tạo, dung lượng mô hình, và hiệu suất. Hơn nữa, khi MoE được giới thiệu vào các mô hình ngôn ngữ dựa trên transformer, nó nhận được sự chú ý ngày càng tăng [23;12;11]. Đến nay, nhiều nghiên cứu đã cải thiện thành phần chính của MoE, đó là mạng định tuyến [12;24; 35;50;51]. Một mặt, đối với các mạng định tuyến có thể học truyền thống, Switch Transformer [12] đơn giản hóa định tuyến bằng cách chỉ chọn chuyên gia top-1 cho mỗi token, Base Layer [24] xử lý bài toán gán tuyến tính để thực hiện định tuyến, và Expert Choice [50] định tuyến token top-k cho mỗi chuyên gia. Mặt khác, Hash Layer [35] thiết kế hash xác định cho định tuyến token, và THOR [51] kích hoạt ngẫu nhiên các chuyên gia cho mỗi đầu vào. Ngoài các cải tiến mạng định tuyến, nhiều nghiên cứu đã khám phá việc triển khai MoE trên các phần cứng hiện đại [14;31;28;16;27], các tính chất mở rộng [12;1;11;4], và các ứng dụng [23;34;46;26;45;44;19;49;3]. Tuy nhiên, hầu hết các nghiên cứu MoE đều dựa trên các mô hình ngôn ngữ dựa trên transformer, và chỉ có một vài nghiên cứu MoE Vision Transformer tồn tại trong lĩnh vực thị giác máy tính [34;45;44;16;19]. Riquelme et al. [34] tạo ra V-MoE bằng cách thay thế một số lớp FFN của ViT bằng các lớp MoE, tiền huấn luyện V-MoE trên JFT-300M hoặc JFT-3B và đạt được thành công lớn trong phân loại hình ảnh. Hwang et al. [16] thêm các lớp MoE vào kiến trúc Swin-transformer V2 để tiền huấn luyện trên ImageNet-22K cho các tác vụ phân loại hình ảnh và phát hiện đối tượng. Xue et al. [45] đề xuất một framework hiệu quả hơn về tham số và hiệu quả hơn, WideNet, bằng cách thay thế FFN bằng MoE và chia sẻ lớp MoE giữa các khối transformer, đạt được cải thiện hiệu suất trên tiền huấn luyện ImageNet-1K. Komatsuzaki et al. [19] sử dụng một checkpoint transformer dày đặc được tiền huấn luyện để khởi động ấm việc đào tạo một mô hình MoE, và đạt được kết quả hứa hẹn trên tiền huấn luyện JFT-300M và tinh chỉnh ImageNet-1K. Xue et al. [44] đạt được chưng cất kiến thức từ một mô hình MoE được tiền huấn luyện để học một mô hình dày đặc, và xác minh nó trên ImageNet-1K.

Trong bài báo này, chúng tôi đề xuất một đào tạo tổng quát mới để cải thiện hiệu suất của ViTs mà không tăng chi phí suy luận, có các mục tiêu và phương pháp triển khai khác với MoE. Bên cạnh đó, các nghiên cứu MoE-ViT trước đây chủ yếu tập trung vào các bộ dữ liệu thị giác 2D lớn hơn, với ít khám phá được tiến hành trên các bộ dữ liệu thị giác 2D nhỏ hơn và thị giác 3D. Bài báo này cho thấy rằng việc thêm MoE ngây thơ vào ViT làm giảm hiệu suất của ViTs trên các bộ dữ liệu thị giác 2D nhỏ hơn và thị giác 3D, và giới thiệu một cách tiếp cận đào tạo Early EMA mới có thể cải thiện rất nhiều hiệu suất của chúng.

2.2 Tái tham số hóa cấu trúc

Bản chất của tái tham số hóa cấu trúc là sử dụng một cấu trúc đa nhánh thay vì một lớp tích chập hoặc kết nối đầy đủ để tăng cường mô hình trong quá trình đào tạo. Sau khi đào tạo, nhiều nhánh được hợp nhất để suy luận bằng cách sử dụng các phép biến đổi tham số tương đương hoặc các thuật toán cụ thể. Các ví dụ điển hình là ACNet [6], DBB [9] và RepVGG [10]. Cụ thể, ACNet [6] thay thế các tích chập K×K thông thường trong CNN bằng ACBlocks có cấu trúc song song của các tích chập K×1, 1×K, và K×K trong quá trình đào tạo. Sau khi đào tạo, ACBlocks được chuyển đổi tương đương thành một lớp tích chập K×K duy nhất. DBB [9] sử dụng một cấu trúc đa nhánh của tích chập 1×1, tích chập K×K, tích chập 1×1-K×K, và tích chập 1×1-average pooling để thay thế tích chập K×K ban đầu trong quá trình đào tạo, được chuyển đổi tương đương thành một tích chập K×K duy nhất trong quá trình suy luận. RepVGG [10] thêm nhánh tích chập 1×1 song song và nhánh đồng nhất vào mỗi lớp tích chập 3×3 trong quá trình đào tạo và được chuyển đổi tương đương thành một lớp tích chập 3×3 duy nhất sau khi đào tạo. Do cải thiện hiệu suất mà không tăng chi phí suy luận mà nó mang lại, tái tham số hóa cấu trúc đã nhận được sự chú ý và ứng dụng ngày càng tăng trong các tác vụ thị giác máy tính khác nhau. Ví dụ, ExpandNets [13] sử dụng nó để thiết kế các mô hình gọn nhẹ, RepNAS [48] sử dụng nó trong tìm kiếm kiến trúc mạng nơ-ron, và ResRep [7] kết hợp nó với pruning.

Tuy nhiên, các nghiên cứu trước đây về tái tham số hóa cấu trúc chủ yếu tập trung vào các kiến trúc CNN, khiến chúng không tương thích với các kiến trúc ViT. Bài báo này giới thiệu một cách tiếp cận mới tách riêng các giai đoạn đào tạo và suy luận của các kiến trúc ViT. Trong quá trình đào tạo, chúng tôi sử dụng tính toán thưa thớt MoE hiệu quả với các cơ chế Phân vùng đồng nhất ngẫu nhiên và Trung bình trọng số chuyên gia. Trong quá trình suy luận, chúng tôi chuyển đổi mô hình thành kiến trúc ViT ban đầu, dẫn đến cải thiện hiệu suất mà không tăng tham số suy luận và độ trễ. Cách tiếp cận này là đầu tiên áp dụng tính toán thưa thớt MoE để cải thiện hiệu suất của các kiến trúc ViT.

2.3 Trung bình trọng số

Trung bình trọng số là một kỹ thuật được sử dụng phổ biến trong học sâu và đã được sử dụng rộng rãi trong các tác vụ khác nhau. Ví dụ, trong học tự giám sát và bán giám sát, [39;2] sử dụng trọng số trung bình động mũ (EMA) của mô hình học sinh làm giáo viên để cung cấp mục tiêu mượt mà hơn cho học sinh. Tương tự, trong chưng cất kiến thức trực tuyến, [42] sử dụng trọng số EMA của mỗi nhánh làm giáo viên trung bình đồng cấp để chuyển giao kiến thức cộng tác giữa các nhánh. Ngoài ra, nhiều nghiên cứu sử dụng trung bình trọng số để tăng cường khả năng tổng quát hóa của mô hình [17;41;40;32]. Ví dụ, trung bình trọng số ngẫu nhiên (SWA) [17] tính trung bình trọng số của nhiều điểm trên quỹ đạo tối ưu hóa SGD và đạt được khả năng tổng quát hóa tốt hơn so với đào tạo thông thường. Wortsman et al. [41] tính trung bình mô hình zero-shot ban đầu và mô hình được tinh chỉnh, dẫn đến cải thiện trong cả tổng quát hóa trong phân phối và ngoài phân phối. Model Soup [40] tính trung bình các mô hình có cùng khởi tạo tiền huấn luyện nhưng các cấu hình siêu tham số khác nhau trong quá trình tinh chỉnh để có được một mô hình tốt hơn. DiWA [32] cung cấp phân tích lý thuyết cho thành công của trung bình trọng số trong OOD và đề xuất giảm hiệp phương sai giữa các dự đoán và cải thiện tổng quát hóa OOD bằng cách tính trung bình trọng số của các mô hình được đào tạo độc lập đa dạng.

Không giống như các nghiên cứu đề cập ở trên tiến hành trung bình trọng số ở cấp độ mô hình để có được một mô hình duy nhất, cách tiếp cận của chúng tôi thực hiện trung bình trọng số ở cấp độ chuyên gia trong MoE trong quá trình đào tạo. Bằng cách tính trung bình trọng số của các chuyên gia khác lên mỗi chuyên gia, chúng tôi có được nhiều chuyên gia.

2.4 Dropout và các biến thể khác cho ViT

Dropout [37] là một kỹ thuật phổ biến được sử dụng để cải thiện khả năng tổng quát hóa mạng bằng cách loại bỏ ngẫu nhiên các nơ-ron và các kết nối tương ứng của chúng trong quá trình đào tạo để ngăn chặn overfitting. Đối với ViTs, hầu hết các nghiên cứu sử dụng dropout với tỷ lệ drop cố định cho các lớp self-attention hoặc FFN. Trong khi đó, Stochastic Depth [15] thường được sử dụng trong ViTs để loại bỏ ngẫu nhiên một số khối và áp dụng tỷ lệ drop cao hơn cho các khối sâu hơn để cải thiện tổng quát hóa. Gần đây, một biến thể dropout mới cho ViT gọi là dropkey được giới thiệu. Dropkey [25] gán một toán tử thích ứng cho mỗi khối attention bằng cách loại bỏ ngẫu nhiên một số key trong quá trình tính toán attention. Chiến lược này hạn chế phân phối attention, làm cho nó mượt mà hơn và hiệu quả hơn trong việc giảm nhẹ overfitting. Hơn nữa, dropkey giảm dần tỷ lệ drop khi số lượng khối tăng lên, tránh sự bất ổn có thể xảy ra từ tỷ lệ drop không đổi trong các phương pháp dropout truyền thống trong quá trình đào tạo.

Trong phương pháp của chúng tôi, đối với mỗi lớp MoE, chúng tôi chia ngẫu nhiên và đều tất cả token giữa tất cả chuyên gia trong quá trình đào tạo. Điều này tương đương với việc loại bỏ ngẫu nhiên một số lượng lớn token tại mỗi chuyên gia. Một hoạt động như vậy có thể giúp ngăn chặn overfitting trong khi đảm bảo độ trễ đào tạo vẫn gần như tương đương với đào tạo thông thường.

3 Phương pháp

3.1 SƠ LƯỢC

MoE. Như thể hiện trong Hình 1(a), một lớp MoE tiêu chuẩn bao gồm một tập hợp N chuyên gia (tức là N FFN) {E1, E2, . . . , EN}, và một mạng định tuyến G với trọng số Wg. Cho một ví dụ đầu vào x, đầu ra của cả lớp MoE thời gian đào tạo và thời gian suy luận có thể được viết như:

y = ∑(i=1 to N) G(x)i·Ei(x)                     (1)

G(x) = TopK(softmax(x·Wg), k)                    (2)

trong đó G(x)i biểu thị điểm số định tuyến đến chuyên gia thứ i, Ei(x) đại diện cho đầu ra của chuyên gia thứ i, TopK(·, k) có nghĩa là chọn k chuyên gia hàng đầu và đặt G(x) của các chuyên gia khác là 0. Thông thường, k≪N, có nghĩa là G(x) là một vector N chiều thưa thớt. Khi G(x)i = 0, Ei(x) không cần được tính toán.

Tái tham số hóa cấu trúc. Như thể hiện trong Hình 1(b), tái tham số hóa cấu trúc sử dụng một cấu trúc CNN đa nhánh để thay thế lớp tích chập thông thường trong quá trình đào tạo. Hãy biểu thị {f1, f2, . . . , fM} là tổng số M nhánh (thường là M toán tử khác nhau với kích thước tương thích). Đối với một đầu vào x, đầu ra của cấu trúc thời gian đào tạo có thể được biểu thị như:

y = ∑(i=1 to M) fi(x)                           (3)

Sau khi đào tạo, những M nhánh này sẽ được chuyển đổi tương đương thành một lớp tích chập duy nhất F để suy luận. Đối với một đầu vào thời gian suy luận x, đầu ra trở thành y = F(x).

3.2 Một kế hoạch đào tạo tổng quát mới

3.2.1 Động lực

Bài báo này nhằm mục đích thiết kế một kế hoạch đào tạo tổng quát mới cho ViTs, đạt được cải thiện hiệu suất mà không tăng chi phí suy luận. Như thể hiện trong Phương trình 3, tái tham số hóa cấu trúc cổ điển chỉ áp dụng cho các CNN được cấu thành từ các tích chập và tăng đáng kể chi phí đào tạo do cấu trúc đa nhánh. Như thể hiện trong Phương trình 2, MoE gần đây bao gồm N chuyên gia và một mạng định tuyến. Thứ nhất, mạng định tuyến mang lại các tham số bổ sung cần học và dễ dẫn đến vấn đề mất cân bằng tải. Thứ hai, N chuyên gia tăng lên rất nhiều gánh nặng triển khai mô hình. Thứ ba, điểm số định tuyến G(x) là một nút thắt cổ chai trong việc chuyển đổi MoE thành FFN. Xem xét những điều này, chúng tôi đầu tiên đề xuất một MoE được thiết kế đặc biệt, hiệu quả hơn để thay thế một số FFN của ViT để đào tạo, gán token cho các chuyên gia bằng Phân vùng đồng nhất ngẫu nhiên. Sau đó, chúng tôi đề xuất một kỹ thuật đơn giản nhưng hiệu quả gọi là Trung bình trọng số chuyên gia, tính trung bình trọng số của các chuyên gia khác cho mỗi chuyên gia ở cuối mỗi lần lặp đào tạo. Như vậy, sau khi đào tạo, chúng tôi có thể chuyển đổi mỗi MoE thành một FFN duy nhất để suy luận. Chúng tôi cho thấy cách áp dụng nó trong quá trình đào tạo và tinh chỉnh dưới đây.

3.2.2 Đào tạo EWA

Tổng quan. Khi áp dụng đào tạo EWA để đào tạo một mô hình ViT từ đầu, trước tiên chúng tôi tạo một MoE ViT được khởi tạo ngẫu nhiên bằng cách thay thế một số FFN bằng các lớp MoE dựa trên Phân vùng đồng nhất ngẫu nhiên. Trong quá trình đào tạo, chúng tôi thực hiện Trung bình trọng số chuyên gia với tỷ lệ chia sẻ β cho mỗi lớp MoE sau mỗi lần cập nhật trọng số, và β tăng tuyến tính theo epoch đào tạo từ 0 đến siêu tham số tỷ lệ chia sẻ. Sau khi đào tạo, chúng tôi chuyển đổi mỗi MoE thành một FFN bằng cách tính trung bình các chuyên gia.

Lớp MoE với Phân vùng đồng nhất ngẫu nhiên. Cho một lớp MoE bao gồm N chuyên gia (tức là N FFN giống hệt nhau), {E1, E2, . . . , EN}, giả sử có T đầu vào {x1, x2, . . . , xT} và T có thể chia hết cho N. Đầu ra của lớp MoE với Phân vùng đồng nhất ngẫu nhiên (RUP) có thể được viết như:

{x1, x2, . . . , xT} → {X1, X2, . . . , XN}       (4)

y = Ei(x) nếu x ∈ Xi                           (5)

Như thể hiện trong Phương trình 4, T đầu vào được phân vùng ngẫu nhiên và đồng nhất thành N phần, trong đó Xi biểu thị T/N đầu vào được gán cho chuyên gia thứ i. Hơn nữa, như thể hiện trong Phương trình 5, đối với mỗi đầu vào trong Xi, đầu ra của nó là Ei(x). Chúng tôi trình bày mã giống pytorch của RUP trong Phụ lục.

Trung bình trọng số chuyên gia. Cho trọng số của N chuyên gia {W1, W2, . . . , WN} và tỷ lệ chia sẻ β, Trung bình trọng số chuyên gia (EWA) được thực hiện trên mỗi lớp MoE có thể được công thức hóa như:

Wi = (1-β)Wi + ∑(j≠i) β/(N-1) Wj              (6)

Wi biểu thị trọng số mới của chuyên gia thứ i. Nói ngắn gọn, chúng tôi tính trung bình trọng số của các chuyên gia khác cho mỗi chuyên gia và có được nhiều chuyên gia. Mã giống pytorch của EWA được cung cấp trong Phụ lục.

Chuyển đổi MoE thành FFN. Sau khi đào tạo, mỗi lớp MoE sẽ được chuyển đổi thành một lớp FFN bằng cách đơn giản tính trung bình trọng số chuyên gia. Cho một lớp MoE bao gồm N chuyên gia {E1, E2, . . . , EN}, FFN thời gian suy luận tương ứng có thể được biểu thị như:

FFN = (1/N) ∑(i=1 to N) Ei                    (7)

Bằng cách này, mô hình MoE thời gian đào tạo sẽ được chuyển đổi thành một mô hình ViT thông thường để suy luận. Chúng tôi tiến hành thí nghiệm để cho thấy tính hợp lý trong Phụ lục.

3.2.3 Tinh chỉnh EWA

Tổng quan. Khi áp dụng đào tạo EWA để tinh chỉnh một mô hình ViT được tiền huấn luyện, sự khác biệt duy nhất so với đào tạo ViT từ đầu là khởi tạo trọng số của mô hình MoE ViT được tạo. Thay vì khởi tạo ngẫu nhiên, chúng tôi khởi tạo MoE ViT với checkpoint mô hình ViT đã cho. Khi thay thế một số FFN bằng các lớp MoE, mỗi chuyên gia của một lớp MoE cụ thể được khởi tạo như một bản sao của lớp FFN tương ứng trong mô hình ViT được tiền huấn luyện. Đối với các lớp khác vẫn không thay đổi, trọng số của chúng được kế thừa trực tiếp từ checkpoint ViT ban đầu.

4 Phân tích lý thuyết

Trong phần này, chúng tôi phân tích lý thuyết tại sao và cách thức đào tạo EWA hoạt động. Để thuận tiện cho phân tích, chúng tôi tập trung vào một lớp MoE và m bước đào tạo. Giả sử rằng bước đào tạo hiện tại là t, theo Phương trình 6, trọng số mới của chuyên gia thứ i sau khi tính trung bình trọng số chuyên gia có thể được viết như:

Wti = (1-β)Wti + ∑(j≠i) β/(N-1) Wtj         (8)

Sau đó, bước đào tạo thứ (t+1) bắt đầu, và trọng số của mỗi chuyên gia của MoE được cập nhật như {Wt+11, Wt+12, . . . , Wt+1N}, trong đó Wt+1i = Wti + η∇Wti, η biểu thị tỷ lệ học. Hơn nữa, việc tính trung bình trọng số chuyên gia được thực hiện, như thể hiện trong Phương trình 9. Dựa trên Phương trình 8 và Wt+1i, Phương trình 9 có thể được công thức hóa lại như Phương trình 10.

Wt+1i = (1-β)Wt+1i + ∑(j≠i) β/(N-1) Wt+1j    (9)

= (1-β)²Wti + η(1-β)∇Wti + ∑(j≠i) β/(N-1) Wt+1j + β(1-β)/(N-1) Wtj  (10)

Tương tự, chúng tôi có thể nhận được Wt+2i, Wt+3i và Wt+mi. Thông qua quy nạp toán học, chúng tôi có Phương trình 12.

Wt+mi = (1-β)Wt+mi + ∑(j≠i) β/(N-1) Wt+mj    (11)

= (1-β)m+1Wti + η∑(k=1 to m)(1-β)k∇Wt+m-ki + β/(N-1) ∑(j≠i)∑(k=0 to m)(1-β)m-k·Wt+kj  (12)

Theo suy luận trên, có hai phát hiện: 1) Quan sát số hạng đầu tiên của Phương trình 9 và Phương trình 11, có một phân rã trọng số theo lớp khi thực hiện tính trung bình trọng số chuyên gia; 2) Quan sát số hạng cuối cùng của Phương trình 10 và Phương trình 12, có sự tích lũy liên tục trọng số trung bình mũ lịch sử đa chuyên gia dọc theo lần lặp đào tạo của đào tạo EWA. Các chi tiết khác được trình bày trong Phụ lục.

5 Thí nghiệm

Khi áp dụng đào tạo EWA để đào tạo hoặc tinh chỉnh ViTs, chúng tôi luôn sử dụng MoE dựa trên phân vùng đồng nhất ngẫu nhiên để thay thế một số FFN. Trừ khi được chỉ định khác, số lượng chuyên gia của mỗi MoE được đặt thành 4 theo mặc định. Siêu tham số tỷ lệ chia sẻ được điều chỉnh đơn giản trong {0.1, 0.2, 0.3, 0.4, 0.5} cho các kiến trúc ViT khác nhau để có được hiệu suất tốt nhất. Theo V-MoE [34], vị trí chèn các lớp MoE này được điều chỉnh đơn giản trong {mỗi-2, cuối-4}. Tất cả chi tiết được trình bày trong Phụ lục.

5.1 Đào tạo EWA

5.1.1 Phân loại 2D trên các kiến trúc và bộ dữ liệu khác nhau

Cài đặt. Chúng tôi tiến hành các thí nghiệm toàn diện trên các kiến trúc và bộ dữ liệu khác nhau để đánh giá kế hoạch đào tạo EWA. Đối với bộ dữ liệu, chúng tôi sử dụng CIFAR-100 và Tiny-ImageNet. Đối với kiến trúc, bên cạnh ViT-S tiêu chuẩn, chúng tôi cũng áp dụng các kiến trúc ViT khác nhau được thiết kế đặc biệt cho các bộ dữ liệu nhỏ như ViT-Tiny nhỏ hơn (chúng tôi gọi là ViT-XT), PiT-XS, T2T-12, Swin-T, và các đối tác SL tăng cường của chúng [22]. SL có nghĩa là áp dụng shifted patch tokenization và locality self-attention, cho phép ViTs học tốt hơn trên các bộ dữ liệu kích thước nhỏ [22]. Chi tiết của tất cả ViTs được sử dụng được trình bày trong Phụ lục. Đối với đào tạo thông thường và EWA, chúng tôi luôn áp dụng CutMix, Mixup, Auto Augment, random erasing, và label smoothing nhất quán cho tất cả mô hình. Trên CIFAR-100, kích thước đầu vào là 32×32, kích thước patch của ViT được đặt thành 4, trong khi 2 cho PiT và Swin. Trên Tiny-ImageNet, kích thước đầu vào là 64×64, kích thước patch được đặt thành 8 cho ViT, trong khi 4 cho PiT và Swin. Tất cả mô hình được đào tạo với bộ tối ưu hóa AdamW, lịch trình cosine và batch size 128. Đối với ViT-S, chúng tôi đào tạo nó trong 300 epoch với 30 epoch khởi động ấm, tỷ lệ học được đặt thành 0.0006, weight decay được đặt thành 0.06. Đối với các mô hình còn lại, chúng tôi đào tạo chúng trong 100 epoch với 10 epoch khởi động ấm, weight decay được đặt thành 0.05, tỷ lệ học được đặt thành 0.003 (0.001 cho Swin-T).

Kết quả. Như thể hiện trong Bảng 2, so với đào tạo thông thường, đào tạo EWA có thể mang lại cải thiện hiệu suất nhất quán cho các kiến trúc ViT khác nhau trên các bộ dữ liệu khác nhau. Đối với ViT-S tiêu chuẩn, đào tạo EWA có thể mang lại cải thiện lớn {1.72%, 2.09%} trên {CIFAR-100, Tiny-ImageNet} tương ứng. Ngay cả đối với các kiến trúc ViT được thiết kế đặc biệt và các đối tác tăng cường SL của chúng, việc thêm kế hoạch đào tạo EWA vẫn có thể cải thiện hiệu suất của chúng, điều này càng xác nhận tính hiệu quả và tổng quát hóa của nó. Ví dụ, Swin-T tăng cường SL với đào tạo thông thường đã đạt được độ chính xác top-1 tương đối cao, 80.31% trên CIFAR-100 và 64.34% trên Tiny-ImageNet. Nó vẫn có thể có được cải thiện hiệu suất {0.21%, 0.79%} từ đào tạo EWA cho mỗi bộ dữ liệu.

5.1.2 Tác vụ thị giác 3D

Cài đặt. Để chứng minh khả năng đào tạo tổng quát của phương pháp chúng tôi, chúng tôi cũng tiến hành các thí nghiệm trên các tác vụ phân loại điểm mây (thưa thớt) và phân đoạn ngữ nghĩa (dày đặc). Theo cài đặt thí nghiệm của PointBERT [47], chúng tôi tiến hành phân loại điểm mây trên ModelNet40 [30]. Theo cài đặt thí nghiệm của Pix4point [29], chúng tôi tiến hành tác vụ phân đoạn ngữ nghĩa điểm mây trên S3DIS [43] và để lại area 5 để kiểm tra, các area khác để đào tạo. Theo PointBERT và Pix4point, chúng tôi sử dụng encoder tiêu chuẩn trong ViT-S làm backbone transformer.

Kết quả. Như thể hiện trong Bảng 3, kế hoạch đào tạo EWA đề xuất có được cải thiện hiệu suất nhất quán trên cả tác vụ phân loại điểm mây và phân đoạn ngữ nghĩa. Đáng chú ý, phương pháp của chúng tôi đạt được cải thiện 0.52% về độ chính xác tổng thể (OA), 1.40% về độ chính xác trung bình (mAcc) và 1.74% về mean IoU (mIoU) trên tác vụ phân đoạn.

5.2 Tinh chỉnh EWA

Cài đặt. Để đánh giá tinh chỉnh EWA, chúng tôi lấy ViT-B và ViT-S được tiền huấn luyện từ thư viện timm và tinh chỉnh chúng trên CIFAR-100 và Food-101. Đối với cả CIFAR-100 và Food-101 [18], batch size được đặt thành 128 và tất cả hình ảnh được chia tỷ lệ thành 224×224 để tinh chỉnh. Đối với CIFAR-100, tổng số bước tinh chỉnh được đặt thành 5000 với 500 bước khởi động ấm. Đối với Food-101, trước tiên chúng tôi khởi động ấm 100 bước và số bước tổng cộng là 1000. Chúng tôi sử dụng bộ tối ưu hóa SGD với momentum 0.9. Tỷ lệ học là 0.01 cho CIFAR-100 và 0.03 cho Food-101. Chúng tôi chỉ đơn giản đặt MoE trên mỗi lớp khác và đặt số lượng chuyên gia của lớp MoE thành 4. β của Trung bình trọng số chuyên gia (Phương trình 6) được tăng tuyến tính với bước hiện tại từ 0 đến siêu tham số tỷ lệ chia sẻ đã cho.

Kết quả. Bảng 4 cho thấy so sánh hiệu suất tinh chỉnh giữa tinh chỉnh thông thường và tinh chỉnh EWA. Mặc dù hiệu suất của tinh chỉnh thông thường khá cao, tinh chỉnh EWA vẫn có thể mang lại cải thiện hơn nữa. Cụ thể, được trang bị tinh chỉnh EWA, ViT-S và ViT-B có được cải thiện hiệu suất {0.38%, 0.45%} và {0.54%, 0.71%} trên {Food-101, CIFAR-100} tương ứng.

5.3 Nghiên cứu loại bỏ

Lịch trình chia sẻ và Tỷ lệ chia sẻ. Trước tiên chúng tôi nghiên cứu cách đặt siêu tham số tỷ lệ chia sẻ và lịch trình chia sẻ cho Trung bình trọng số chuyên gia. Ở trên, lịch trình chia sẻ được đặt thành tăng tuyến tính theo mặc định, chúng tôi ở đây so sánh nó với lịch trình chia sẻ không đổi. Đối với tỷ lệ chia sẻ, chúng tôi điều chỉnh nó trong {0.1, 0.2, 0.3, 0.4, 0.5}. Chúng tôi đào tạo ViT-S trên CIFAR100 sử dụng đào tạo EWA với các lịch trình chia sẻ và tỷ lệ chia sẻ khác nhau. Như thể hiện trong Hình 2, đào tạo EWA với các lịch trình và tỷ lệ chia sẻ khác nhau luôn có thể vượt trội hơn đào tạo thông thường, và đào tạo EWA với lịch trình tăng tuyến tính hoạt động tốt hơn so với lịch trình không đổi trong hầu hết các trường hợp. Các nghiên cứu loại bỏ về số lượng chuyên gia được trình bày trong Phụ lục.

6 Thảo luận và phát hiện khác

Thú vị hơn, chúng tôi phát hiện rằng, trên các bộ dữ liệu thị giác 2D nhỏ và các tác vụ thị giác 3D, MoE ngây thơ dẫn đến hiệu suất thấp hơn so với các đối tác dày đặc ban đầu của nó, và công nghệ EWA của chúng tôi có thể được áp dụng một cách liền mạch cho MoE ngây thơ và giúp nó học tốt hơn.

MoE ngây thơ. Theo các cài đặt đào tạo ViT-S trên CIFAR-100, Tiny-ImageNet, ModelNet40 và S3DIS, chúng tôi đào tạo thêm V-MoE-S của chúng tôi, thay thế một số FFN của ViT-S bằng MoE định tuyến top-1 ngây thơ [12] mỗi khối khác. Chúng tôi đặt số lượng chuyên gia là 4, trọng số của hàm mất mát cân bằng tải λ là 0.01 và tỷ lệ dung lượng C là 1.05 cho bộ dữ liệu 2D và 1.2 cho bộ dữ liệu 3D. Như thể hiện trong Bảng 5, với đào tạo thông thường, V-MoE-S ngây thơ làm giảm hiệu suất so với ViT-S trên các bộ dữ liệu khác nhau.

MoE ngây thơ + Early EWA. Mọi thứ thay đổi khi chúng tôi giới thiệu Trung bình trọng số chuyên gia (EWA) vào MoE ngây thơ. Xem xét rằng việc sử dụng EWA trong suốt toàn bộ quá trình đào tạo cuối cùng sẽ dẫn đến trọng số gần như giống hệt nhau giữa các chuyên gia, chúng tôi chỉ thực hiện EWA trên MoE ngây thơ trong giai đoạn đầu của đào tạo. Theo mặc định, chúng tôi sử dụng lịch trình chia sẻ không đổi và chỉ thực hiện Early EWA trong nửa đầu của tổng số epoch đào tạo. Với đào tạo Early EWA, hiệu suất của V-MoE-S được cải thiện rất nhiều, dẫn đến cải thiện độ chính xác top-1 {2.14%, 3.63%} trên {CIFAR-100, Tiny-ImageNet} tương ứng. Ngoài ra, đào tạo Early EWA của chúng tôi có thể cải thiện hiệu suất của V-MoE-S trên các tác vụ thị giác 3D khác nhau, với cải thiện độ chính xác 0.36% trên phân loại ModelNet40, và cải thiện 0.30% OA, 1.40% mAcc, 1.31% mIoU trên phân đoạn S3DIS Area5. Các nghiên cứu sâu hơn được trình bày trong Phụ lục.

7 Kết luận

Tóm lại, kế hoạch đào tạo đề xuất của chúng tôi cho ViTs đạt được cải thiện hiệu suất mà không tăng độ trễ suy luận và tham số. Bằng cách thiết kế các MoE hiệu quả và EWA trong quá trình đào tạo, và chuyển đổi mỗi MoE trở lại thành một FFN bằng cách tính trung bình các chuyên gia trong quá trình suy luận, chúng tôi tách riêng các giai đoạn đào tạo và suy luận của ViTs. Các thí nghiệm toàn diện trên các tác vụ thị giác 2D và 3D khác nhau, kiến trúc ViT, và bộ dữ liệu chứng minh tính hiệu quả và khả năng tổng quát hóa. Phân tích lý thuyết hỗ trợ thêm cho cách tiếp cận của chúng tôi, và kế hoạch đào tạo của chúng tôi cũng có thể được áp dụng để tinh chỉnh ViTs và cải thiện hiệu quả của MoE ngây thơ trong các tác vụ thị giác khác nhau. Nhìn chung, kế hoạch đào tạo đề xuất của chúng tôi cung cấp một hướng đi hứa hẹn để tăng cường hiệu suất của ViTs trong các tác vụ thị giác khác nhau, và chúng tôi tin rằng nó có thể dẫn đến nghiên cứu và phát triển hơn nữa trong lĩnh vực này.
