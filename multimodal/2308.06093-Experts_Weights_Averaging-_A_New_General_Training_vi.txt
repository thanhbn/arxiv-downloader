<<<<<<< Updated upstream
# Trung bình trọng số chuyên gia: Một kế hoạch đào tạo tổng quát mới cho Vision Transformers

Yongqi Huang1†, Peng Ye1†, Xiaoshui Huang2, Sheng Li3,
Tao Chen1∗, Tong He2, Wanli Ouyang2
1Trường Khoa học và Công nghệ Thông tin, Đại học Fudan, 2Phòng thí nghiệm AI Thượng Hải,
=======
# 2308.06093.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2308.06093.pdf
# Kích thước tệp: 381789 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Trung Bình Trọng Số Chuyên Gia: Một Sơ Đồ Huấn Luyện Chung Mới cho Vision Transformers
Yongqi Huang1†, Peng Ye1†, Xiaoshui Huang2, Sheng Li3,
Tao Chen1∗, Tong He2, Wanli Ouyang2
1Trường Khoa học và Công nghệ Thông tin, Đại học Fudan,2Phòng thí nghiệm AI Thượng Hải,
>>>>>>> Stashed changes
3Đại học Tài chính và Kinh tế Giang Tây
{19307130163, yepeng20}@fudan.edu.cn

Tóm tắt
<<<<<<< Updated upstream
Tái tham số hóa cấu trúc là một kế hoạch đào tạo tổng quát cho Mạng nơ-ron tích chập (CNN), đạt được cải thiện hiệu suất mà không tăng chi phí suy luận. Khi Vision Transformers (ViTs) đang dần vượt qua CNN trong các tác vụ thị giác khác nhau, người ta có thể đặt câu hỏi: có tồn tại một kế hoạch đào tạo dành riêng cho ViTs mà cũng có thể đạt được cải thiện hiệu suất mà không tăng chi phí suy luận không? Gần đây, Mixture-of-Experts (MoE) đã thu hút sự chú ý ngày càng tăng, vì nó có thể mở rộng hiệu quả dung lượng của Transformers với chi phí cố định thông qua các chuyên gia được kích hoạt thưa thớt. Xem xét rằng MoE cũng có thể được xem như một cấu trúc đa nhánh, chúng ta có thể sử dụng MoE để thực hiện một kế hoạch đào tạo ViT tương tự như tái tham số hóa cấu trúc không? Trong bài báo này, chúng tôi trả lời khẳng định những câu hỏi này, với một chiến lược đào tạo tổng quát mới cho ViTs. Cụ thể, chúng tôi tách riêng các giai đoạn đào tạo và suy luận của ViTs. Trong quá trình đào tạo, chúng tôi thay thế một số Mạng Feed-Forward (FFN) của ViT bằng các MoE được thiết kế đặc biệt, hiệu quả hơn, gán token cho các chuyên gia bằng phân vùng đồng nhất ngẫu nhiên, và thực hiện Trung bình trọng số chuyên gia (EWA) trên các MoE này ở cuối mỗi lần lặp. Sau khi đào tạo, chúng tôi chuyển đổi mỗi MoE thành một FFN bằng cách tính trung bình các chuyên gia, biến đổi mô hình trở lại thành ViT ban đầu để suy luận. Chúng tôi cũng cung cấp một phân tích lý thuyết để cho thấy tại sao và cách thức hoạt động. Các thí nghiệm toàn diện trên các tác vụ thị giác 2D và 3D khác nhau, kiến trúc ViT, và bộ dữ liệu xác nhận hiệu quả và khả năng tổng quát hóa của kế hoạch đào tạo đề xuất. Bên cạnh đó, kế hoạch đào tạo của chúng tôi cũng có thể được áp dụng để cải thiện hiệu suất khi tinh chỉnh ViTs. Cuối cùng, nhưng không kém phần quan trọng, kỹ thuật EWA đề xuất có thể cải thiện đáng kể hiệu quả của MoE ngây thơ trong các bộ dữ liệu nhỏ thị giác 2D khác nhau và các tác vụ thị giác 3D.

1 Giới thiệu

Trong những năm gần đây, việc phát triển các kế hoạch đào tạo tổng quát đã đóng vai trò quan trọng trong học sâu. Những phương pháp này tăng cường các mô hình hiện có mà không cần sửa đổi kiến trúc của chúng, và không yêu cầu bất kỳ giả định nào về các kiến trúc cụ thể. Kết quả là, chúng có thể được sử dụng cho các mô hình khác nhau và cung cấp lợi ích đồng nhất.

Đối với kiến trúc CNN, kế hoạch đào tạo tổng quát được sử dụng rộng rãi là tái tham số hóa cấu trúc [6; 9;10;8] (Hình 1(b)). Cách tiếp cận này tách riêng các giai đoạn đào tạo và suy luận của mô hình CNN bằng cách sử dụng cấu trúc đa nhánh trong quá trình đào tạo, sau đó được chuyển đổi tương đương trở lại cấu trúc đơn nhánh ban đầu trong quá trình suy luận. Tái tham số hóa cấu trúc đạt được hiệu suất cải thiện mà không tăng chi phí suy luận. Tuy nhiên, cấu trúc đa nhánh được sử dụng trong quá trình đào tạo có thể làm chậm tốc độ đào tạo (như thể hiện trong cột thứ hai của Bảng 1, độ trễ đào tạo của nó lên đến 2.18 × so với đào tạo thông thường). Ngoài ra, tái tham số hóa cấu trúc chỉ áp dụng cho CNN, không áp dụng cho các kiến trúc mạnh mẽ khác gần đây như ViTs (xem cột thứ sáu của Bảng 1).

Đối với kiến trúc transformer, phương pháp đào tạo tổng quát gần đây bao gồm việc thay thế lớp mạng feed-forward (FFN) bằng một hỗn hợp thưa thớt các chuyên gia (MoE) [23;12;11;34;16] (Hình 1(a)). MoE là một thành phần mạng mới bao gồm nhiều chuyên gia (FFN) với trọng số độc nhất và một mạng định tuyến có thể huấn luyện. Trong các giai đoạn đào tạo và suy luận, mạng định tuyến chọn một kết hợp thưa thớt các chuyên gia cho mỗi đầu vào, cho phép mở rộng hiệu quả dung lượng của các mô hình transformer thông qua tính toán thưa thớt. Trong lĩnh vực xử lý ngôn ngữ tự nhiên, nhiều mô hình MoE thưa thớt [23; 12; 11] đã chứng minh rằng chúng vượt trội hơn các đối tác dày đặc.

Tuy nhiên, MoE vẫn có một số hạn chế làm hạn chế việc sử dụng rộng rãi trong Vision Transformers (ViTs). Thứ nhất, V-MoE [34] và SwinV2-MoE [16] được tiền huấn luyện trên các bộ dữ liệu quy mô lớn (JFT-300M [38], ImageNet-22K [33]), và khi được tiền huấn luyện trên ImageNet-1K [5], hiệu suất của V-MoE thấp hơn ViT [44]. Hơn nữa, không có nghiên cứu nào tìm hiểu MoE Vision Transformer trên các bộ dữ liệu nhỏ hơn như CIFAR-100 [20] và Tiny-ImageNet [21]. Thứ hai, các MoE ViTs hiện tại [34;45;44;16;19] tập trung vào các tác vụ thị giác 2D, và không có MoE ViTs cho các tác vụ thị giác 3D (xem hàng thứ tư của Bảng 1). Thứ ba, cơ chế định tuyến top-k của MoE bản thân nó không hiệu quả và không hữu hiệu. Như thể hiện trong hàng thứ tư của Bảng 1, độ trễ đào tạo và suy luận đều tăng lên 1.12 × vì định tuyến top-k yêu cầu tính toán bổ sung để đưa ra quyết định định tuyến cho mỗi đầu vào, và các tham số đào tạo và suy luận đều tăng lên rất nhiều đến 3.25 × gây gánh nặng cho việc triển khai mô hình. Bên cạnh đó, định tuyến top-k dễ dẫn đến mất cân bằng tải trên các chuyên gia, do đó nhiều nghiên cứu đã được dành để giải quyết vấn đề này thông qua các thiết kế khác nhau, như thêm các hàm mất mát cân bằng tải phụ trợ [23;12], đưa ra quyết định định tuyến toàn cầu [24], hoặc thiết kế lại thuật toán định tuyến [35; 50].

Do đó, những câu hỏi sau đây nảy sinh: Có tồn tại một phương pháp đào tạo mới có thể huấn luyện hiệu quả các chuyên gia được kích hoạt thưa thớt trong giai đoạn đào tạo, và một mô hình thưa thớt có thể được chuyển đổi thành một mô hình dày đặc mà không có chi phí tính toán bổ sung trong giai đoạn suy luận không? Chúng ta có thể mở rộng thêm hiệu quả của MoE cho cả các bộ dữ liệu thị giác 2D và các tác vụ thị giác 3D không?

Trong bài báo này, chúng tôi thiết kế một chiến lược đào tạo tổng quát mới cho ViTs được gọi là đào tạo Trung bình trọng số chuyên gia (EWA). Cách tiếp cận của chúng tôi đạt được cải thiện hiệu suất mà không tăng độ trễ suy luận và tham số, như thể hiện trong hàng thứ hai và cuối cùng của Bảng 1. Cụ thể, chúng tôi tách riêng các giai đoạn đào tạo và suy luận của ViT, như thể hiện trong Hình 1(c). Trong quá trình đào tạo, chúng tôi thay thế một số FFN của ViT thông thường bằng các MoE được thiết kế đặc biệt, hiệu quả hơn, gán token cho các chuyên gia bằng Phân vùng đồng nhất ngẫu nhiên (RUP), không yêu cầu tham số bổ sung, thiết kế đặc biệt, hoặc hàm mất mát phụ trợ. Hơn nữa, ở cuối mỗi lần lặp, chúng tôi thực hiện Trung bình trọng số chuyên gia (EWA) trên mỗi MoE. Trong quá trình suy luận, bằng cách đơn giản tính trung bình các chuyên gia của mỗi MoE thành một FFN duy nhất, chúng tôi có thể chuyển đổi mô hình MoE thưa thớt thành một mô hình ViT thông thường mà không mất hiệu suất.

Chúng tôi tiến hành các thí nghiệm toàn diện trên các tác vụ thị giác 2D và 3D khác nhau, kiến trúc ViT, và bộ dữ liệu để xác nhận hiệu quả và khả năng tổng quát hóa của kế hoạch đào tạo đề xuất. Ví dụ, phương pháp đào tạo đề xuất của chúng tôi đạt được cải thiện 1.72% trong tác vụ phân loại hình ảnh và cải thiện 1.74% mIoU trong tác vụ phân đoạn ngữ nghĩa điểm mây. Hơn nữa, đào tạo EWA cũng có thể được sử dụng để tinh chỉnh các mô hình ViT được tiền huấn luyện. Phương pháp đào tạo của chúng tôi cải thiện việc tinh chỉnh ViT-B trên CIFAR100 từ 90.71% lên 91.42% (↑0.71%). Cuối cùng, chúng tôi phát hiện rằng công nghệ chính của Trung bình trọng số chuyên gia trong đào tạo EWA có thể cải thiện rất nhiều hiệu quả của MoE ngây thơ trong các bộ dữ liệu nhỏ thị giác 2D khác nhau và các tác vụ thị giác 3D.

Các đóng góp chính được tóm tắt như sau:
• Chúng tôi đề xuất một kế hoạch đào tạo tổng quát mới cho ViTs, đạt được cải thiện hiệu suất mà không tăng độ trễ suy luận và tham số. Trong quá trình đào tạo, chúng tôi thay thế một số FFN trong kiến trúc ViT bằng MoE sử dụng Phân vùng đồng nhất ngẫu nhiên và thực hiện Trung bình trọng số chuyên gia trên các lớp MoE này sau mỗi lần cập nhật trọng số mô hình. Sau khi đào tạo, chúng tôi chuyển đổi mỗi lớp MoE trong mô hình thành một lớp FFN bằng cách tính trung bình các chuyên gia, do đó biến đổi mô hình trở lại kiến trúc ViT ban đầu để suy luận.
• Chúng tôi xác nhận rằng đào tạo EWA có thể mang lại cải thiện hiệu suất thống nhất cho các kiến trúc Vision Transformer khác nhau trên các tác vụ thị giác 2D/3D và bộ dữ liệu khác nhau. Hơn nữa, chúng tôi phát hiện rằng đào tạo EWA cũng có thể được sử dụng để tinh chỉnh các mô hình ViT được tiền huấn luyện, đạt được cải thiện hiệu suất hơn nữa. Chúng tôi cung cấp phân tích lý thuyết để cho thấy tại sao và cách thức hoạt động.
• Chúng tôi khám phá hiệu quả của MoE ngây thơ trên các bộ dữ liệu thị giác 2D nhỏ và các tác vụ thị giác 3D. Chúng tôi phát hiện rằng việc sử dụng MoE ngây thơ dẫn đến hiệu suất thấp hơn so với các đối tác dày đặc, nhưng kỹ thuật Trung bình trọng số chuyên gia đề xuất có thể cải thiện rất nhiều hiệu quả của MoE ngây thơ.

2 Các nghiên cứu liên quan

2.1 MoE

Sparsely-Gated Mixture-of-Experts (MoE) được giới thiệu lần đầu trong [36] và đã cho thấy những cải thiện đáng kể về thời gian đào tạo, dung lượng mô hình, và hiệu suất. Hơn nữa, khi MoE được giới thiệu vào các mô hình ngôn ngữ dựa trên transformer, nó nhận được sự chú ý ngày càng tăng [23;12;11]. Đến nay, nhiều nghiên cứu đã cải thiện thành phần chính của MoE, đó là mạng định tuyến [12;24; 35;50;51]. Một mặt, đối với các mạng định tuyến có thể học truyền thống, Switch Transformer [12] đơn giản hóa định tuyến bằng cách chỉ chọn chuyên gia top-1 cho mỗi token, Base Layer [24] xử lý bài toán gán tuyến tính để thực hiện định tuyến, và Expert Choice [50] định tuyến token top-k cho mỗi chuyên gia. Mặt khác, Hash Layer [35] thiết kế hash xác định cho định tuyến token, và THOR [51] kích hoạt ngẫu nhiên các chuyên gia cho mỗi đầu vào. Ngoài các cải tiến mạng định tuyến, nhiều nghiên cứu đã khám phá việc triển khai MoE trên các phần cứng hiện đại [14;31;28;16;27], các tính chất mở rộng [12;1;11;4], và các ứng dụng [23;34;46;26;45;44;19;49;3]. Tuy nhiên, hầu hết các nghiên cứu MoE đều dựa trên các mô hình ngôn ngữ dựa trên transformer, và chỉ có một vài nghiên cứu MoE Vision Transformer tồn tại trong lĩnh vực thị giác máy tính [34;45;44;16;19]. Riquelme et al. [34] tạo ra V-MoE bằng cách thay thế một số lớp FFN của ViT bằng các lớp MoE, tiền huấn luyện V-MoE trên JFT-300M hoặc JFT-3B và đạt được thành công lớn trong phân loại hình ảnh. Hwang et al. [16] thêm các lớp MoE vào kiến trúc Swin-transformer V2 để tiền huấn luyện trên ImageNet-22K cho các tác vụ phân loại hình ảnh và phát hiện đối tượng. Xue et al. [45] đề xuất một framework hiệu quả hơn về tham số và hiệu quả hơn, WideNet, bằng cách thay thế FFN bằng MoE và chia sẻ lớp MoE giữa các khối transformer, đạt được cải thiện hiệu suất trên tiền huấn luyện ImageNet-1K. Komatsuzaki et al. [19] sử dụng một checkpoint transformer dày đặc được tiền huấn luyện để khởi động ấm việc đào tạo một mô hình MoE, và đạt được kết quả hứa hẹn trên tiền huấn luyện JFT-300M và tinh chỉnh ImageNet-1K. Xue et al. [44] đạt được chưng cất kiến thức từ một mô hình MoE được tiền huấn luyện để học một mô hình dày đặc, và xác minh nó trên ImageNet-1K.

Trong bài báo này, chúng tôi đề xuất một đào tạo tổng quát mới để cải thiện hiệu suất của ViTs mà không tăng chi phí suy luận, có các mục tiêu và phương pháp triển khai khác với MoE. Bên cạnh đó, các nghiên cứu MoE-ViT trước đây chủ yếu tập trung vào các bộ dữ liệu thị giác 2D lớn hơn, với ít khám phá được tiến hành trên các bộ dữ liệu thị giác 2D nhỏ hơn và thị giác 3D. Bài báo này cho thấy rằng việc thêm MoE ngây thơ vào ViT làm giảm hiệu suất của ViTs trên các bộ dữ liệu thị giác 2D nhỏ hơn và thị giác 3D, và giới thiệu một cách tiếp cận đào tạo Early EMA mới có thể cải thiện rất nhiều hiệu suất của chúng.

2.2 Tái tham số hóa cấu trúc

Bản chất của tái tham số hóa cấu trúc là sử dụng một cấu trúc đa nhánh thay vì một lớp tích chập hoặc kết nối đầy đủ để tăng cường mô hình trong quá trình đào tạo. Sau khi đào tạo, nhiều nhánh được hợp nhất để suy luận bằng cách sử dụng các phép biến đổi tham số tương đương hoặc các thuật toán cụ thể. Các ví dụ điển hình là ACNet [6], DBB [9] và RepVGG [10]. Cụ thể, ACNet [6] thay thế các tích chập K×K thông thường trong CNN bằng ACBlocks có cấu trúc song song của các tích chập K×1, 1×K, và K×K trong quá trình đào tạo. Sau khi đào tạo, ACBlocks được chuyển đổi tương đương thành một lớp tích chập K×K duy nhất. DBB [9] sử dụng một cấu trúc đa nhánh của tích chập 1×1, tích chập K×K, tích chập 1×1-K×K, và tích chập 1×1-average pooling để thay thế tích chập K×K ban đầu trong quá trình đào tạo, được chuyển đổi tương đương thành một tích chập K×K duy nhất trong quá trình suy luận. RepVGG [10] thêm nhánh tích chập 1×1 song song và nhánh đồng nhất vào mỗi lớp tích chập 3×3 trong quá trình đào tạo và được chuyển đổi tương đương thành một lớp tích chập 3×3 duy nhất sau khi đào tạo. Do cải thiện hiệu suất mà không tăng chi phí suy luận mà nó mang lại, tái tham số hóa cấu trúc đã nhận được sự chú ý và ứng dụng ngày càng tăng trong các tác vụ thị giác máy tính khác nhau. Ví dụ, ExpandNets [13] sử dụng nó để thiết kế các mô hình gọn nhẹ, RepNAS [48] sử dụng nó trong tìm kiếm kiến trúc mạng nơ-ron, và ResRep [7] kết hợp nó với pruning.

Tuy nhiên, các nghiên cứu trước đây về tái tham số hóa cấu trúc chủ yếu tập trung vào các kiến trúc CNN, khiến chúng không tương thích với các kiến trúc ViT. Bài báo này giới thiệu một cách tiếp cận mới tách riêng các giai đoạn đào tạo và suy luận của các kiến trúc ViT. Trong quá trình đào tạo, chúng tôi sử dụng tính toán thưa thớt MoE hiệu quả với các cơ chế Phân vùng đồng nhất ngẫu nhiên và Trung bình trọng số chuyên gia. Trong quá trình suy luận, chúng tôi chuyển đổi mô hình thành kiến trúc ViT ban đầu, dẫn đến cải thiện hiệu suất mà không tăng tham số suy luận và độ trễ. Cách tiếp cận này là đầu tiên áp dụng tính toán thưa thớt MoE để cải thiện hiệu suất của các kiến trúc ViT.

2.3 Trung bình trọng số

Trung bình trọng số là một kỹ thuật được sử dụng phổ biến trong học sâu và đã được sử dụng rộng rãi trong các tác vụ khác nhau. Ví dụ, trong học tự giám sát và bán giám sát, [39;2] sử dụng trọng số trung bình động mũ (EMA) của mô hình học sinh làm giáo viên để cung cấp mục tiêu mượt mà hơn cho học sinh. Tương tự, trong chưng cất kiến thức trực tuyến, [42] sử dụng trọng số EMA của mỗi nhánh làm giáo viên trung bình đồng cấp để chuyển giao kiến thức cộng tác giữa các nhánh. Ngoài ra, nhiều nghiên cứu sử dụng trung bình trọng số để tăng cường khả năng tổng quát hóa của mô hình [17;41;40;32]. Ví dụ, trung bình trọng số ngẫu nhiên (SWA) [17] tính trung bình trọng số của nhiều điểm trên quỹ đạo tối ưu hóa SGD và đạt được khả năng tổng quát hóa tốt hơn so với đào tạo thông thường. Wortsman et al. [41] tính trung bình mô hình zero-shot ban đầu và mô hình được tinh chỉnh, dẫn đến cải thiện trong cả tổng quát hóa trong phân phối và ngoài phân phối. Model Soup [40] tính trung bình các mô hình có cùng khởi tạo tiền huấn luyện nhưng các cấu hình siêu tham số khác nhau trong quá trình tinh chỉnh để có được một mô hình tốt hơn. DiWA [32] cung cấp phân tích lý thuyết cho thành công của trung bình trọng số trong OOD và đề xuất giảm hiệp phương sai giữa các dự đoán và cải thiện tổng quát hóa OOD bằng cách tính trung bình trọng số của các mô hình được đào tạo độc lập đa dạng.

Không giống như các nghiên cứu đề cập ở trên tiến hành trung bình trọng số ở cấp độ mô hình để có được một mô hình duy nhất, cách tiếp cận của chúng tôi thực hiện trung bình trọng số ở cấp độ chuyên gia trong MoE trong quá trình đào tạo. Bằng cách tính trung bình trọng số của các chuyên gia khác lên mỗi chuyên gia, chúng tôi có được nhiều chuyên gia.

2.4 Dropout và các biến thể khác cho ViT

Dropout [37] là một kỹ thuật phổ biến được sử dụng để cải thiện khả năng tổng quát hóa mạng bằng cách loại bỏ ngẫu nhiên các nơ-ron và các kết nối tương ứng của chúng trong quá trình đào tạo để ngăn chặn overfitting. Đối với ViTs, hầu hết các nghiên cứu sử dụng dropout với tỷ lệ drop cố định cho các lớp self-attention hoặc FFN. Trong khi đó, Stochastic Depth [15] thường được sử dụng trong ViTs để loại bỏ ngẫu nhiên một số khối và áp dụng tỷ lệ drop cao hơn cho các khối sâu hơn để cải thiện tổng quát hóa. Gần đây, một biến thể dropout mới cho ViT gọi là dropkey được giới thiệu. Dropkey [25] gán một toán tử thích ứng cho mỗi khối attention bằng cách loại bỏ ngẫu nhiên một số key trong quá trình tính toán attention. Chiến lược này hạn chế phân phối attention, làm cho nó mượt mà hơn và hiệu quả hơn trong việc giảm nhẹ overfitting. Hơn nữa, dropkey giảm dần tỷ lệ drop khi số lượng khối tăng lên, tránh sự bất ổn có thể xảy ra từ tỷ lệ drop không đổi trong các phương pháp dropout truyền thống trong quá trình đào tạo.

Trong phương pháp của chúng tôi, đối với mỗi lớp MoE, chúng tôi chia ngẫu nhiên và đều tất cả token giữa tất cả chuyên gia trong quá trình đào tạo. Điều này tương đương với việc loại bỏ ngẫu nhiên một số lượng lớn token tại mỗi chuyên gia. Một hoạt động như vậy có thể giúp ngăn chặn overfitting trong khi đảm bảo độ trễ đào tạo vẫn gần như tương đương với đào tạo thông thường.

3 Phương pháp

3.1 SƠ LƯỢC

MoE. Như thể hiện trong Hình 1(a), một lớp MoE tiêu chuẩn bao gồm một tập hợp N chuyên gia (tức là N FFN) {E1, E2, . . . , EN}, và một mạng định tuyến G với trọng số Wg. Cho một ví dụ đầu vào x, đầu ra của cả lớp MoE thời gian đào tạo và thời gian suy luận có thể được viết như:

y = ∑(i=1 to N) G(x)i·Ei(x)                     (1)

G(x) = TopK(softmax(x·Wg), k)                    (2)

trong đó G(x)i biểu thị điểm số định tuyến đến chuyên gia thứ i, Ei(x) đại diện cho đầu ra của chuyên gia thứ i, TopK(·, k) có nghĩa là chọn k chuyên gia hàng đầu và đặt G(x) của các chuyên gia khác là 0. Thông thường, k≪N, có nghĩa là G(x) là một vector N chiều thưa thớt. Khi G(x)i = 0, Ei(x) không cần được tính toán.

Tái tham số hóa cấu trúc. Như thể hiện trong Hình 1(b), tái tham số hóa cấu trúc sử dụng một cấu trúc CNN đa nhánh để thay thế lớp tích chập thông thường trong quá trình đào tạo. Hãy biểu thị {f1, f2, . . . , fM} là tổng số M nhánh (thường là M toán tử khác nhau với kích thước tương thích). Đối với một đầu vào x, đầu ra của cấu trúc thời gian đào tạo có thể được biểu thị như:

y = ∑(i=1 to M) fi(x)                           (3)

Sau khi đào tạo, những M nhánh này sẽ được chuyển đổi tương đương thành một lớp tích chập duy nhất F để suy luận. Đối với một đầu vào thời gian suy luận x, đầu ra trở thành y = F(x).

3.2 Một kế hoạch đào tạo tổng quát mới

3.2.1 Động lực

Bài báo này nhằm mục đích thiết kế một kế hoạch đào tạo tổng quát mới cho ViTs, đạt được cải thiện hiệu suất mà không tăng chi phí suy luận. Như thể hiện trong Phương trình 3, tái tham số hóa cấu trúc cổ điển chỉ áp dụng cho các CNN được cấu thành từ các tích chập và tăng đáng kể chi phí đào tạo do cấu trúc đa nhánh. Như thể hiện trong Phương trình 2, MoE gần đây bao gồm N chuyên gia và một mạng định tuyến. Thứ nhất, mạng định tuyến mang lại các tham số bổ sung cần học và dễ dẫn đến vấn đề mất cân bằng tải. Thứ hai, N chuyên gia tăng lên rất nhiều gánh nặng triển khai mô hình. Thứ ba, điểm số định tuyến G(x) là một nút thắt cổ chai trong việc chuyển đổi MoE thành FFN. Xem xét những điều này, chúng tôi đầu tiên đề xuất một MoE được thiết kế đặc biệt, hiệu quả hơn để thay thế một số FFN của ViT để đào tạo, gán token cho các chuyên gia bằng Phân vùng đồng nhất ngẫu nhiên. Sau đó, chúng tôi đề xuất một kỹ thuật đơn giản nhưng hiệu quả gọi là Trung bình trọng số chuyên gia, tính trung bình trọng số của các chuyên gia khác cho mỗi chuyên gia ở cuối mỗi lần lặp đào tạo. Như vậy, sau khi đào tạo, chúng tôi có thể chuyển đổi mỗi MoE thành một FFN duy nhất để suy luận. Chúng tôi cho thấy cách áp dụng nó trong quá trình đào tạo và tinh chỉnh dưới đây.

3.2.2 Đào tạo EWA

Tổng quan. Khi áp dụng đào tạo EWA để đào tạo một mô hình ViT từ đầu, trước tiên chúng tôi tạo một MoE ViT được khởi tạo ngẫu nhiên bằng cách thay thế một số FFN bằng các lớp MoE dựa trên Phân vùng đồng nhất ngẫu nhiên. Trong quá trình đào tạo, chúng tôi thực hiện Trung bình trọng số chuyên gia với tỷ lệ chia sẻ β cho mỗi lớp MoE sau mỗi lần cập nhật trọng số, và β tăng tuyến tính theo epoch đào tạo từ 0 đến siêu tham số tỷ lệ chia sẻ. Sau khi đào tạo, chúng tôi chuyển đổi mỗi MoE thành một FFN bằng cách tính trung bình các chuyên gia.

Lớp MoE với Phân vùng đồng nhất ngẫu nhiên. Cho một lớp MoE bao gồm N chuyên gia (tức là N FFN giống hệt nhau), {E1, E2, . . . , EN}, giả sử có T đầu vào {x1, x2, . . . , xT} và T có thể chia hết cho N. Đầu ra của lớp MoE với Phân vùng đồng nhất ngẫu nhiên (RUP) có thể được viết như:

{x1, x2, . . . , xT} → {X1, X2, . . . , XN}       (4)

y = Ei(x) nếu x ∈ Xi                           (5)

Như thể hiện trong Phương trình 4, T đầu vào được phân vùng ngẫu nhiên và đồng nhất thành N phần, trong đó Xi biểu thị T/N đầu vào được gán cho chuyên gia thứ i. Hơn nữa, như thể hiện trong Phương trình 5, đối với mỗi đầu vào trong Xi, đầu ra của nó là Ei(x). Chúng tôi trình bày mã giống pytorch của RUP trong Phụ lục.

Trung bình trọng số chuyên gia. Cho trọng số của N chuyên gia {W1, W2, . . . , WN} và tỷ lệ chia sẻ β, Trung bình trọng số chuyên gia (EWA) được thực hiện trên mỗi lớp MoE có thể được công thức hóa như:

Wi = (1-β)Wi + ∑(j≠i) β/(N-1) Wj              (6)

Wi biểu thị trọng số mới của chuyên gia thứ i. Nói ngắn gọn, chúng tôi tính trung bình trọng số của các chuyên gia khác cho mỗi chuyên gia và có được nhiều chuyên gia. Mã giống pytorch của EWA được cung cấp trong Phụ lục.

Chuyển đổi MoE thành FFN. Sau khi đào tạo, mỗi lớp MoE sẽ được chuyển đổi thành một lớp FFN bằng cách đơn giản tính trung bình trọng số chuyên gia. Cho một lớp MoE bao gồm N chuyên gia {E1, E2, . . . , EN}, FFN thời gian suy luận tương ứng có thể được biểu thị như:

FFN = (1/N) ∑(i=1 to N) Ei                    (7)

Bằng cách này, mô hình MoE thời gian đào tạo sẽ được chuyển đổi thành một mô hình ViT thông thường để suy luận. Chúng tôi tiến hành thí nghiệm để cho thấy tính hợp lý trong Phụ lục.

3.2.3 Tinh chỉnh EWA

Tổng quan. Khi áp dụng đào tạo EWA để tinh chỉnh một mô hình ViT được tiền huấn luyện, sự khác biệt duy nhất so với đào tạo ViT từ đầu là khởi tạo trọng số của mô hình MoE ViT được tạo. Thay vì khởi tạo ngẫu nhiên, chúng tôi khởi tạo MoE ViT với checkpoint mô hình ViT đã cho. Khi thay thế một số FFN bằng các lớp MoE, mỗi chuyên gia của một lớp MoE cụ thể được khởi tạo như một bản sao của lớp FFN tương ứng trong mô hình ViT được tiền huấn luyện. Đối với các lớp khác vẫn không thay đổi, trọng số của chúng được kế thừa trực tiếp từ checkpoint ViT ban đầu.

4 Phân tích lý thuyết

Trong phần này, chúng tôi phân tích lý thuyết tại sao và cách thức đào tạo EWA hoạt động. Để thuận tiện cho phân tích, chúng tôi tập trung vào một lớp MoE và m bước đào tạo. Giả sử rằng bước đào tạo hiện tại là t, theo Phương trình 6, trọng số mới của chuyên gia thứ i sau khi tính trung bình trọng số chuyên gia có thể được viết như:

Wti = (1-β)Wti + ∑(j≠i) β/(N-1) Wtj         (8)

Sau đó, bước đào tạo thứ (t+1) bắt đầu, và trọng số của mỗi chuyên gia của MoE được cập nhật như {Wt+11, Wt+12, . . . , Wt+1N}, trong đó Wt+1i = Wti + η∇Wti, η biểu thị tỷ lệ học. Hơn nữa, việc tính trung bình trọng số chuyên gia được thực hiện, như thể hiện trong Phương trình 9. Dựa trên Phương trình 8 và Wt+1i, Phương trình 9 có thể được công thức hóa lại như Phương trình 10.

Wt+1i = (1-β)Wt+1i + ∑(j≠i) β/(N-1) Wt+1j    (9)

= (1-β)²Wti + η(1-β)∇Wti + ∑(j≠i) β/(N-1) Wt+1j + β(1-β)/(N-1) Wtj  (10)

Tương tự, chúng tôi có thể nhận được Wt+2i, Wt+3i và Wt+mi. Thông qua quy nạp toán học, chúng tôi có Phương trình 12.

Wt+mi = (1-β)Wt+mi + ∑(j≠i) β/(N-1) Wt+mj    (11)

= (1-β)m+1Wti + η∑(k=1 to m)(1-β)k∇Wt+m-ki + β/(N-1) ∑(j≠i)∑(k=0 to m)(1-β)m-k·Wt+kj  (12)

Theo suy luận trên, có hai phát hiện: 1) Quan sát số hạng đầu tiên của Phương trình 9 và Phương trình 11, có một phân rã trọng số theo lớp khi thực hiện tính trung bình trọng số chuyên gia; 2) Quan sát số hạng cuối cùng của Phương trình 10 và Phương trình 12, có sự tích lũy liên tục trọng số trung bình mũ lịch sử đa chuyên gia dọc theo lần lặp đào tạo của đào tạo EWA. Các chi tiết khác được trình bày trong Phụ lục.

5 Thí nghiệm

Khi áp dụng đào tạo EWA để đào tạo hoặc tinh chỉnh ViTs, chúng tôi luôn sử dụng MoE dựa trên phân vùng đồng nhất ngẫu nhiên để thay thế một số FFN. Trừ khi được chỉ định khác, số lượng chuyên gia của mỗi MoE được đặt thành 4 theo mặc định. Siêu tham số tỷ lệ chia sẻ được điều chỉnh đơn giản trong {0.1, 0.2, 0.3, 0.4, 0.5} cho các kiến trúc ViT khác nhau để có được hiệu suất tốt nhất. Theo V-MoE [34], vị trí chèn các lớp MoE này được điều chỉnh đơn giản trong {mỗi-2, cuối-4}. Tất cả chi tiết được trình bày trong Phụ lục.

5.1 Đào tạo EWA

5.1.1 Phân loại 2D trên các kiến trúc và bộ dữ liệu khác nhau

Cài đặt. Chúng tôi tiến hành các thí nghiệm toàn diện trên các kiến trúc và bộ dữ liệu khác nhau để đánh giá kế hoạch đào tạo EWA. Đối với bộ dữ liệu, chúng tôi sử dụng CIFAR-100 và Tiny-ImageNet. Đối với kiến trúc, bên cạnh ViT-S tiêu chuẩn, chúng tôi cũng áp dụng các kiến trúc ViT khác nhau được thiết kế đặc biệt cho các bộ dữ liệu nhỏ như ViT-Tiny nhỏ hơn (chúng tôi gọi là ViT-XT), PiT-XS, T2T-12, Swin-T, và các đối tác SL tăng cường của chúng [22]. SL có nghĩa là áp dụng shifted patch tokenization và locality self-attention, cho phép ViTs học tốt hơn trên các bộ dữ liệu kích thước nhỏ [22]. Chi tiết của tất cả ViTs được sử dụng được trình bày trong Phụ lục. Đối với đào tạo thông thường và EWA, chúng tôi luôn áp dụng CutMix, Mixup, Auto Augment, random erasing, và label smoothing nhất quán cho tất cả mô hình. Trên CIFAR-100, kích thước đầu vào là 32×32, kích thước patch của ViT được đặt thành 4, trong khi 2 cho PiT và Swin. Trên Tiny-ImageNet, kích thước đầu vào là 64×64, kích thước patch được đặt thành 8 cho ViT, trong khi 4 cho PiT và Swin. Tất cả mô hình được đào tạo với bộ tối ưu hóa AdamW, lịch trình cosine và batch size 128. Đối với ViT-S, chúng tôi đào tạo nó trong 300 epoch với 30 epoch khởi động ấm, tỷ lệ học được đặt thành 0.0006, weight decay được đặt thành 0.06. Đối với các mô hình còn lại, chúng tôi đào tạo chúng trong 100 epoch với 10 epoch khởi động ấm, weight decay được đặt thành 0.05, tỷ lệ học được đặt thành 0.003 (0.001 cho Swin-T).

Kết quả. Như thể hiện trong Bảng 2, so với đào tạo thông thường, đào tạo EWA có thể mang lại cải thiện hiệu suất nhất quán cho các kiến trúc ViT khác nhau trên các bộ dữ liệu khác nhau. Đối với ViT-S tiêu chuẩn, đào tạo EWA có thể mang lại cải thiện lớn {1.72%, 2.09%} trên {CIFAR-100, Tiny-ImageNet} tương ứng. Ngay cả đối với các kiến trúc ViT được thiết kế đặc biệt và các đối tác tăng cường SL của chúng, việc thêm kế hoạch đào tạo EWA vẫn có thể cải thiện hiệu suất của chúng, điều này càng xác nhận tính hiệu quả và tổng quát hóa của nó. Ví dụ, Swin-T tăng cường SL với đào tạo thông thường đã đạt được độ chính xác top-1 tương đối cao, 80.31% trên CIFAR-100 và 64.34% trên Tiny-ImageNet. Nó vẫn có thể có được cải thiện hiệu suất {0.21%, 0.79%} từ đào tạo EWA cho mỗi bộ dữ liệu.

5.1.2 Tác vụ thị giác 3D

Cài đặt. Để chứng minh khả năng đào tạo tổng quát của phương pháp chúng tôi, chúng tôi cũng tiến hành các thí nghiệm trên các tác vụ phân loại điểm mây (thưa thớt) và phân đoạn ngữ nghĩa (dày đặc). Theo cài đặt thí nghiệm của PointBERT [47], chúng tôi tiến hành phân loại điểm mây trên ModelNet40 [30]. Theo cài đặt thí nghiệm của Pix4point [29], chúng tôi tiến hành tác vụ phân đoạn ngữ nghĩa điểm mây trên S3DIS [43] và để lại area 5 để kiểm tra, các area khác để đào tạo. Theo PointBERT và Pix4point, chúng tôi sử dụng encoder tiêu chuẩn trong ViT-S làm backbone transformer.

Kết quả. Như thể hiện trong Bảng 3, kế hoạch đào tạo EWA đề xuất có được cải thiện hiệu suất nhất quán trên cả tác vụ phân loại điểm mây và phân đoạn ngữ nghĩa. Đáng chú ý, phương pháp của chúng tôi đạt được cải thiện 0.52% về độ chính xác tổng thể (OA), 1.40% về độ chính xác trung bình (mAcc) và 1.74% về mean IoU (mIoU) trên tác vụ phân đoạn.

5.2 Tinh chỉnh EWA

Cài đặt. Để đánh giá tinh chỉnh EWA, chúng tôi lấy ViT-B và ViT-S được tiền huấn luyện từ thư viện timm và tinh chỉnh chúng trên CIFAR-100 và Food-101. Đối với cả CIFAR-100 và Food-101 [18], batch size được đặt thành 128 và tất cả hình ảnh được chia tỷ lệ thành 224×224 để tinh chỉnh. Đối với CIFAR-100, tổng số bước tinh chỉnh được đặt thành 5000 với 500 bước khởi động ấm. Đối với Food-101, trước tiên chúng tôi khởi động ấm 100 bước và số bước tổng cộng là 1000. Chúng tôi sử dụng bộ tối ưu hóa SGD với momentum 0.9. Tỷ lệ học là 0.01 cho CIFAR-100 và 0.03 cho Food-101. Chúng tôi chỉ đơn giản đặt MoE trên mỗi lớp khác và đặt số lượng chuyên gia của lớp MoE thành 4. β của Trung bình trọng số chuyên gia (Phương trình 6) được tăng tuyến tính với bước hiện tại từ 0 đến siêu tham số tỷ lệ chia sẻ đã cho.

Kết quả. Bảng 4 cho thấy so sánh hiệu suất tinh chỉnh giữa tinh chỉnh thông thường và tinh chỉnh EWA. Mặc dù hiệu suất của tinh chỉnh thông thường khá cao, tinh chỉnh EWA vẫn có thể mang lại cải thiện hơn nữa. Cụ thể, được trang bị tinh chỉnh EWA, ViT-S và ViT-B có được cải thiện hiệu suất {0.38%, 0.45%} và {0.54%, 0.71%} trên {Food-101, CIFAR-100} tương ứng.

5.3 Nghiên cứu loại bỏ

Lịch trình chia sẻ và Tỷ lệ chia sẻ. Trước tiên chúng tôi nghiên cứu cách đặt siêu tham số tỷ lệ chia sẻ và lịch trình chia sẻ cho Trung bình trọng số chuyên gia. Ở trên, lịch trình chia sẻ được đặt thành tăng tuyến tính theo mặc định, chúng tôi ở đây so sánh nó với lịch trình chia sẻ không đổi. Đối với tỷ lệ chia sẻ, chúng tôi điều chỉnh nó trong {0.1, 0.2, 0.3, 0.4, 0.5}. Chúng tôi đào tạo ViT-S trên CIFAR100 sử dụng đào tạo EWA với các lịch trình chia sẻ và tỷ lệ chia sẻ khác nhau. Như thể hiện trong Hình 2, đào tạo EWA với các lịch trình và tỷ lệ chia sẻ khác nhau luôn có thể vượt trội hơn đào tạo thông thường, và đào tạo EWA với lịch trình tăng tuyến tính hoạt động tốt hơn so với lịch trình không đổi trong hầu hết các trường hợp. Các nghiên cứu loại bỏ về số lượng chuyên gia được trình bày trong Phụ lục.

6 Thảo luận và phát hiện khác

Thú vị hơn, chúng tôi phát hiện rằng, trên các bộ dữ liệu thị giác 2D nhỏ và các tác vụ thị giác 3D, MoE ngây thơ dẫn đến hiệu suất thấp hơn so với các đối tác dày đặc ban đầu của nó, và công nghệ EWA của chúng tôi có thể được áp dụng một cách liền mạch cho MoE ngây thơ và giúp nó học tốt hơn.

MoE ngây thơ. Theo các cài đặt đào tạo ViT-S trên CIFAR-100, Tiny-ImageNet, ModelNet40 và S3DIS, chúng tôi đào tạo thêm V-MoE-S của chúng tôi, thay thế một số FFN của ViT-S bằng MoE định tuyến top-1 ngây thơ [12] mỗi khối khác. Chúng tôi đặt số lượng chuyên gia là 4, trọng số của hàm mất mát cân bằng tải λ là 0.01 và tỷ lệ dung lượng C là 1.05 cho bộ dữ liệu 2D và 1.2 cho bộ dữ liệu 3D. Như thể hiện trong Bảng 5, với đào tạo thông thường, V-MoE-S ngây thơ làm giảm hiệu suất so với ViT-S trên các bộ dữ liệu khác nhau.

MoE ngây thơ + Early EWA. Mọi thứ thay đổi khi chúng tôi giới thiệu Trung bình trọng số chuyên gia (EWA) vào MoE ngây thơ. Xem xét rằng việc sử dụng EWA trong suốt toàn bộ quá trình đào tạo cuối cùng sẽ dẫn đến trọng số gần như giống hệt nhau giữa các chuyên gia, chúng tôi chỉ thực hiện EWA trên MoE ngây thơ trong giai đoạn đầu của đào tạo. Theo mặc định, chúng tôi sử dụng lịch trình chia sẻ không đổi và chỉ thực hiện Early EWA trong nửa đầu của tổng số epoch đào tạo. Với đào tạo Early EWA, hiệu suất của V-MoE-S được cải thiện rất nhiều, dẫn đến cải thiện độ chính xác top-1 {2.14%, 3.63%} trên {CIFAR-100, Tiny-ImageNet} tương ứng. Ngoài ra, đào tạo Early EWA của chúng tôi có thể cải thiện hiệu suất của V-MoE-S trên các tác vụ thị giác 3D khác nhau, với cải thiện độ chính xác 0.36% trên phân loại ModelNet40, và cải thiện 0.30% OA, 1.40% mAcc, 1.31% mIoU trên phân đoạn S3DIS Area5. Các nghiên cứu sâu hơn được trình bày trong Phụ lục.

7 Kết luận

Tóm lại, kế hoạch đào tạo đề xuất của chúng tôi cho ViTs đạt được cải thiện hiệu suất mà không tăng độ trễ suy luận và tham số. Bằng cách thiết kế các MoE hiệu quả và EWA trong quá trình đào tạo, và chuyển đổi mỗi MoE trở lại thành một FFN bằng cách tính trung bình các chuyên gia trong quá trình suy luận, chúng tôi tách riêng các giai đoạn đào tạo và suy luận của ViTs. Các thí nghiệm toàn diện trên các tác vụ thị giác 2D và 3D khác nhau, kiến trúc ViT, và bộ dữ liệu chứng minh tính hiệu quả và khả năng tổng quát hóa. Phân tích lý thuyết hỗ trợ thêm cho cách tiếp cận của chúng tôi, và kế hoạch đào tạo của chúng tôi cũng có thể được áp dụng để tinh chỉnh ViTs và cải thiện hiệu quả của MoE ngây thơ trong các tác vụ thị giác khác nhau. Nhìn chung, kế hoạch đào tạo đề xuất của chúng tôi cung cấp một hướng đi hứa hẹn để tăng cường hiệu suất của ViTs trong các tác vụ thị giác khác nhau, và chúng tôi tin rằng nó có thể dẫn đến nghiên cứu và phát triển hơn nữa trong lĩnh vực này.
=======
Tái tham số hóa cấu trúc là một sơ đồ huấn luyện chung cho Mạng Nơ-ron Tích chập (CNN), đạt được cải thiện hiệu suất mà không tăng chi phí suy luận. Khi Vision Transformers (ViTs) đang dần vượt trội hơn CNNs trong các nhiệm vụ thị giác khác nhau, người ta có thể đặt câu hỏi: liệu có tồn tại một sơ đồ huấn luyện dành riêng cho ViTs cũng có thể đạt được cải thiện hiệu suất mà không tăng chi phí suy luận không? Gần đây, Hỗn hợp Chuyên gia (MoE) đã thu hút sự chú ý ngày càng tăng, vì nó có thể mở rộng hiệu quả khả năng của Transformers với chi phí cố định thông qua các chuyên gia được kích hoạt thưa thớt. Xét rằng MoE cũng có thể được xem như một cấu trúc đa nhánh, liệu chúng ta có thể sử dụng MoE để thực hiện một sơ đồ huấn luyện ViT tương tự như tái tham số hóa cấu trúc không? Trong bài báo này, chúng tôi trả lời khẳng định những câu hỏi này, với một chiến lược huấn luyện chung mới cho ViTs. Cụ thể, chúng tôi tách biệt giai đoạn huấn luyện và suy luận của ViTs. Trong quá trình huấn luyện, chúng tôi thay thế một số Mạng Truyền Thẳng (FFNs) của ViT bằng các MoE được thiết kế đặc biệt, hiệu quả hơn, gán token cho các chuyên gia bằng phân vùng đồng nhất ngẫu nhiên, và thực hiện Trung Bình Trọng Số Chuyên Gia (EWA) trên các MoE này ở cuối mỗi lần lặp. Sau khi huấn luyện, chúng tôi chuyển đổi mỗi MoE thành một FFN bằng cách lấy trung bình các chuyên gia, biến đổi mô hình trở lại thành ViT gốc để suy luận. Chúng tôi cũng cung cấp phân tích lý thuyết để chỉ ra lý do và cách thức hoạt động. Các thí nghiệm toàn diện trên nhiều nhiệm vụ thị giác 2D và 3D, kiến trúc ViT, và bộ dữ liệu xác thực tính hiệu quả và khả năng tổng quát hóa của sơ đồ huấn luyện được đề xuất. Bên cạnh đó, sơ đồ huấn luyện của chúng tôi cũng có thể được áp dụng để cải thiện hiệu suất khi tinh chỉnh ViTs. Cuối cùng, nhưng không kém phần quan trọng, kỹ thuật EWA được đề xuất có thể cải thiện đáng kể hiệu quả của MoE thô sơ trong nhiều bộ dữ liệu thị giác 2D nhỏ và các nhiệm vụ thị giác 3D.

1 Giới thiệu
Trong những năm gần đây, việc phát triển các sơ đồ huấn luyện chung đã đóng vai trò quan trọng trong học sâu. Những phương pháp này nâng cao các mô hình hiện có mà không cần thay đổi kiến trúc của chúng, và không yêu cầu bất kỳ giả định nào về các kiến trúc cụ thể. Kết quả là, chúng có thể được sử dụng cho các mô hình khác nhau và mang lại lợi ích thống nhất.

Đối với kiến trúc CNN, sơ đồ huấn luyện chung được sử dụng rộng rãi là tái tham số hóa cấu trúc [6; 9; 10; 8] (Hình 1(b)). Phương pháp này tách biệt các giai đoạn huấn luyện và suy luận của mô hình CNN bằng cách sử dụng cấu trúc đa nhánh trong quá trình huấn luyện, sau đó được chuyển đổi tương đương trở lại cấu trúc đơn nhánh gốc trong quá trình suy luận. Tái tham số hóa cấu trúc đạt được hiệu suất cải thiện mà không tăng chi phí suy luận. Tuy nhiên, cấu trúc đa nhánh được sử dụng trong quá trình

∗Tác giả liên hệ (eetchen@fudan.edu.cn).†Đóng góp ngang nhau.
Bản thảo. Đang được xem xét.arXiv:2308.06093v2 [cs.CV] 25 Aug 2023

--- TRANG 2 ---
(a) Mô hình MoE thời gian huấn luyện & suy luận+MoE thưa thớt
Token 2 Token 1 Token xRouter Router Router
………p=0.8
p=0.6
p=0.9
NormĐa-Đầu 
Attention
+Norm
(b) ACNet thời gian huấn luyện & suy luận
(c) Mô hình EWA thời gian huấn luyện & suy luận
NormĐa-Đầu 
Attention
+ +Huấn luyện EWA
Token 2 Token 1 Token x……
Phân vùng đồng nhất 
ngẫu nhiênTrung bình trọng số chuyên gia
Norm
+
FFN
NormĐa-Đầu 
Attention
+NormChuyên giaNorm
3*3 
Conv
+
Norm
3*1 
ConvNorm
1*3 
ConvTái tham số hóa
Cấu trúcNorm
3*3 
Conv
+
Trung bình trọng sốThời gian huấn luyện & suy luận Thời gian huấn luyện
Thời gian huấn luyện & suy luậnThời gian suy luận
Thời gian huấn luyện Thời gian suy luận
FFN-1 FFN-n FFN-1 FFN-n

Hình 1: So sánh thời gian huấn luyện & suy luận của (a) khối MoE gần đây, (b) khối tái tham số hóa cấu trúc điển hình, và (c) khối huấn luyện EWA của chúng tôi. Mỗi FFN biểu thị một chuyên gia của MoE.

Bảng 1: So sánh các hiệu ứng đạt được giữa huấn luyện vanilla, tái tham số hóa cấu trúc, huấn luyện MoE, và huấn luyện EWA của chúng tôi. Môi trường thử nghiệm là GTX3090. Đối với tái tham số hóa cấu trúc, baseline là ResNet-18, và mô hình so sánh là đối tác ACNet của nó [6]. Đối với huấn luyện MoE, baseline là ViT-S tiêu chuẩn, và mô hình so sánh là V-MoE-S của chúng tôi thay thế mỗi FFN khác của ViT-S bằng MoE 8 chuyên gia sử dụng định tuyến top-1 [12]. Đối với huấn luyện EWA, chúng tôi thay thế định tuyến top-1 bằng phân vùng đồng nhất ngẫu nhiên cho V-MoE-S. Để kiểm tra độ trễ, chúng tôi sử dụng 128 ×3×224×224 làm đầu vào.

-Độ trễ
HuấnluyệnTham số
HuấnluyệnĐộ trễ
Suy luậnTham số
Suy luậnKiến trúc Ứng dụng
Huấn luyện
Vanilla1.0× 1.0× 1.0× 1.0× - -
Tái tham số
Cấu trúc2.18× 1.63× 1.0× 1.0× CNN 2D
Huấn luyện
MoE1.12× 3.25× 1.12× 3.25× ViT 2D
Huấn luyện
EWA1.07× 3.25× 1.0× 1.0× ViT 2D & 3D

huấn luyện có thể làm chậm tốc độ huấn luyện (như thể hiện trong cột thứ hai của Bảng 1, độ trễ huấn luyện của nó lên đến 2.18 × so với huấn luyện vanilla). Ngoài ra, tái tham số hóa cấu trúc chỉ áp dụng cho CNNs, không áp dụng cho các kiến trúc mạnh hơn gần đây như ViTs (xem cột thứ sáu của Bảng 1).

Đối với kiến trúc transformer, phương pháp huấn luyện chung gần đây bao gồm việc thay thế lớp mạng truyền thẳng (FFN) bằng hỗn hợp chuyên gia thưa thớt (MoE) [23; 12; 11; 34; 16] (Hình 1(a)). MoE là một thành phần mạng mới bao gồm nhiều chuyên gia (FFNs) với trọng số duy nhất và một mạng định tuyến có thể huấn luyện. Trong giai đoạn huấn luyện và suy luận, mạng định tuyến chọn một kết hợp thưa thớt của các chuyên gia cho mỗi đầu vào, cho phép mở rộng hiệu quả khả năng của các mô hình transformer thông qua tính toán thưa thớt. Trong lĩnh vực xử lý ngôn ngữ tự nhiên, nhiều mô hình MoE thưa thớt [23; 12; 11] đã chứng minh rằng chúng vượt trội hơn các đối tác dày đặc.

Tuy nhiên, MoE vẫn có một số hạn chế làm hạn chế việc sử dụng rộng rãi trong Vision Transformers (ViTs). Thứ nhất, V-MoE [34] và SwinV2-MoE [16] đã được huấn luyện trước trên các bộ dữ liệu quy mô lớn (JFT-300M [38], ImageNet-22K [33]), và khi được huấn luyện trước trên ImageNet-1K [5], hiệu suất của V-MoE thấp hơn ViT [44]. Hơn nữa, không có nghiên cứu nào về MoE Vision Transformer trên các bộ dữ liệu nhỏ hơn như CIFAR-100 [20] và Tiny-ImageNet [21]. Thứ hai, các MoE ViTs hiện tại [34; 45; 44; 16; 19] tập trung vào các nhiệm vụ thị giác 2D, và không có MoE ViTs cho các nhiệm vụ thị giác 3D (xem dòng thứ tư của Bảng 1). Thứ ba, cơ chế định tuyến top-k của MoE bản thân nó không hiệu quả và không có hiệu quả. Như thể hiện trong dòng thứ tư của Bảng 1, độ trễ huấn luyện và suy luận đều tăng lên 1.12 × vì định tuyến top-k yêu cầu tính toán bổ sung để đưa ra quyết định định tuyến cho mỗi đầu vào, và tham số huấn luyện và suy luận đều tăng lên đáng kể đến 3.25 × gây gánh nặng cho việc triển khai mô hình. Bên cạnh đó, định tuyến top-k dễ dẫn đến mất cân bằng tải giữa các chuyên gia, do đó nhiều nghiên cứu đã dành tâm huyết để xử lý vấn đề này thông qua các thiết kế khác nhau, chẳng hạn như thêm tổn thất cân bằng tải phụ trợ [23; 12], đưa ra quyết định định tuyến toàn cầu [24], hoặc thiết kế lại thuật toán định tuyến [35; 50].

Do đó, những câu hỏi sau đây xuất hiện: Liệu có phương pháp huấn luyện mới nào có thể huấn luyện hiệu quả các chuyên gia được kích hoạt thưa thớt trong giai đoạn huấn luyện, và liệu một mô hình thưa thớt có thể được chuyển đổi thành một mô hình dày đặc mà không có chi phí tính toán bổ sung trong giai đoạn suy luận không? Liệu chúng ta có thể mở rộng thêm hiệu quả của MoE cho cả bộ dữ liệu thị giác 2D và các nhiệm vụ thị giác 3D không?

Trong bài báo này, chúng tôi thiết kế một chiến lược huấn luyện chung mới cho ViTs gọi là huấn luyện Trung Bình Trọng Số Chuyên Gia (EWA). Phương pháp của chúng tôi đạt được cải thiện hiệu suất mà không tăng độ trễ và tham số suy luận, như thể hiện trong dòng thứ hai và cuối cùng của Bảng 1. Cụ thể, chúng tôi tách biệt giai đoạn huấn luyện và suy luận của ViT, như thể hiện trong Hình 1(c). Trong quá trình huấn luyện, chúng tôi thay thế một số FFNs của ViT vanilla bằng các MoE được thiết kế đặc biệt, hiệu quả hơn, gán token cho các chuyên gia bằng Phân vùng Đồng nhất Ngẫu nhiên (RUP), không yêu cầu tham số bổ sung, thiết kế đặc biệt, hoặc tổn thất phụ trợ. Hơn nữa, ở cuối mỗi lần lặp, chúng tôi thực hiện Trung Bình Trọng Số Chuyên Gia (EWA) trên mỗi MoE. Trong quá trình suy luận, bằng cách đơn giản lấy trung bình các chuyên gia của mỗi MoE thành một FFN duy nhất, chúng tôi có thể chuyển đổi mô hình MoE thưa thớt thành mô hình ViT vanilla mà không mất hiệu suất.

Chúng tôi tiến hành các thí nghiệm toàn diện trên các nhiệm vụ thị giác 2D và 3D khác nhau, kiến trúc ViT, và bộ dữ liệu để xác thực tính hiệu quả và khả năng tổng quát hóa của sơ đồ huấn luyện được đề xuất. Ví dụ, phương pháp huấn luyện được đề xuất của chúng tôi đạt được cải thiện 1.72% trong nhiệm vụ phân loại hình ảnh và cải thiện 1.74% mIoU trong nhiệm vụ phân đoạn ngữ nghĩa đám mây điểm. Hơn nữa, huấn luyện EWA cũng có thể được sử dụng để tinh chỉnh các mô hình ViT đã được huấn luyện trước. Phương pháp huấn luyện của chúng tôi cải thiện việc tinh chỉnh ViT-B trên CIFAR100 từ 90.71% lên 91.42% (↑0.71%). Cuối cùng, chúng tôi thấy rằng công nghệ chính của Trung Bình Trọng Số Chuyên Gia trong huấn luyện EWA có thể cải thiện đáng kể hiệu quả của MoE thô sơ trong nhiều bộ dữ liệu thị giác 2D nhỏ và các nhiệm vụ thị giác 3D.

Các đóng góp chính được tóm tắt như sau:
• Chúng tôi đề xuất một sơ đồ huấn luyện chung mới cho ViTs, đạt được cải thiện hiệu suất mà không tăng độ trễ và tham số suy luận. Trong quá trình huấn luyện, chúng tôi thay thế một số FFNs trong kiến trúc ViT bằng MoE sử dụng Phân vùng Đồng nhất Ngẫu nhiên và thực hiện Trung Bình Trọng Số Chuyên Gia trên các lớp MoE này sau mỗi lần cập nhật trọng số mô hình. Sau khi huấn luyện, chúng tôi chuyển đổi mỗi lớp MoE trong mô hình thành lớp FFN bằng cách lấy trung bình các chuyên gia, do đó biến đổi mô hình trở lại kiến trúc ViT gốc để suy luận.
• Chúng tôi xác thực rằng huấn luyện EWA có thể mang lại cải thiện hiệu suất thống nhất cho các kiến trúc Vision Transformer khác nhau trên các nhiệm vụ thị giác 2D/3D và bộ dữ liệu khác nhau. Hơn nữa, chúng tôi thấy rằng huấn luyện EWA cũng có thể được sử dụng để tinh chỉnh các mô hình ViT đã được huấn luyện trước, đạt được cải thiện hiệu suất hơn nữa. Chúng tôi cung cấp phân tích lý thuyết để chỉ ra lý do và cách thức hoạt động.
• Chúng tôi khám phá hiệu quả của MoE thô sơ trên các bộ dữ liệu thị giác 2D nhỏ và các nhiệm vụ thị giác 3D. Chúng tôi thấy rằng sử dụng MoE thô sơ dẫn đến hiệu suất thấp hơn so với các đối tác dày đặc, nhưng kỹ thuật Trung Bình Trọng Số Chuyên Gia được đề xuất có thể cải thiện đáng kể hiệu quả của MoE thô sơ.

2 Công trình liên quan

2.1 MoE
Hỗn hợp Chuyên gia Gated Thưa thớt (MoE) được giới thiệu lần đầu trong [36] và đã cho thấy những cải thiện đáng kể về thời gian huấn luyện, khả năng mô hình và hiệu suất. Hơn nữa, khi MoE được đưa vào các mô hình ngôn ngữ dựa trên transformer, nó nhận được sự chú ý ngày càng tăng [23; 12; 11]. Cho đến nay, nhiều công trình đã cải thiện thành phần chính của MoE, đó là mạng định tuyến [12; 24; 35; 50; 51]. Một mặt, đối với các mạng định tuyến có thể học truyền thống, Switch Transformer [12] đơn giản hóa định tuyến bằng cách chỉ chọn chuyên gia top-1 cho mỗi token, Base Layer [24] xử lý bài toán phân công tuyến tính để thực hiện định tuyến, và Expert Choice [50] định tuyến token top-k cho mỗi chuyên gia. Mặt khác, Hash Layer [35] thiết kế băm xác định cho định tuyến token, và THOR [51] kích hoạt ngẫu nhiên các chuyên gia cho mỗi đầu vào. Ngoài việc cải thiện mạng định tuyến, nhiều công trình đã khám phá việc triển khai MoE trên phần cứng hiện đại [14; 31; 28; 16; 27], đặc tính mở rộng [12; 1; 11; 4], và các ứng dụng [23; 34; 46; 26; 45; 44; 19; 49; 3]. Tuy nhiên, hầu hết các công trình MoE đều dựa trên các mô hình ngôn ngữ dựa trên transformer, và chỉ có một số ít công trình MoE Vision Transformer tồn tại trong lĩnh vực thị giác máy tính [34; 45; 44; 16; 19]. Riquelme et al. [34] tạo ra V-MoE bằng cách thay thế một số lớp FFN của ViT bằng các lớp MoE, huấn luyện trước V-MoE trên JFT-300M hoặc JFT-3B và đạt được thành công lớn trong phân loại hình ảnh. Hwang et al. [16] thêm các lớp MoE vào kiến trúc Swin-transformer V2 để huấn luyện trước trên ImageNet-22K cho các nhiệm vụ phân loại hình ảnh và phát hiện đối tượng. Xue et al. [45] đề xuất một framework hiệu quả hơn về tham số và hiệu quả hơn, WideNet, bằng cách thay thế FFN bằng MoE và chia sẻ lớp MoE giữa các khối transformer, đạt được cải thiện hiệu suất trên huấn luyện trước ImageNet-1K. Komatsuzaki et al. [19] sử dụng checkpoint transformer dày đặc đã được huấn luyện trước để khởi động huấn luyện của mô hình MoE, và đạt được kết quả đầy hứa hẹn trên huấn luyện trước JFT-300M và tinh chỉnh ImageNet-1K. Xue et al. [44] đạt được chưng cất kiến thức từ mô hình MoE đã được huấn luyện trước để học một mô hình dày đặc, và xác minh nó trên ImageNet-1K.

Trong bài báo này, chúng tôi đề xuất một huấn luyện chung mới để cải thiện hiệu suất của ViTs mà không tăng chi phí suy luận, có mục tiêu và phương pháp thực hiện khác với MoE. Bên cạnh đó, các nghiên cứu MoE-ViT trước đây chủ yếu tập trung vào các bộ dữ liệu thị giác 2D lớn hơn, với ít khám phá được thực hiện trên các bộ dữ liệu thị giác 2D nhỏ hơn và thị giác 3D. Bài báo này cho thấy rằng việc thêm MoE thô sơ vào ViT làm giảm hiệu suất của ViTs trên các bộ dữ liệu thị giác 2D nhỏ hơn và thị giác 3D, và giới thiệu một phương pháp huấn luyện Early EMA mới có thể cải thiện đáng kể hiệu suất của chúng.

2.2 Tái tham số hóa Cấu trúc
Bản chất của tái tham số hóa cấu trúc là sử dụng cấu trúc đa nhánh thay vì lớp tích chập hoặc kết nối đầy đủ để nâng cao mô hình trong quá trình huấn luyện. Sau khi huấn luyện, nhiều nhánh được hợp nhất để suy luận bằng cách sử dụng các phép biến đổi tham số tương đương hoặc thuật toán cụ thể. Các ví dụ điển hình là ACNet [6], DBB [9] và RepVGG [10]. Cụ thể, ACNet [6] thay thế các tích chập K×K thông thường trong CNN bằng ACBlocks có cấu trúc song song của các tích chập K×1, 1×K, và K×K trong quá trình huấn luyện. Sau khi huấn luyện, ACBlocks được chuyển đổi tương đương thành một lớp tích chập K×K duy nhất. DBB [9] sử dụng cấu trúc đa nhánh của tích chập 1×1, tích chập K×K, tích chập 1×1-K×K, và tích chập 1×1-pooling trung bình để thay thế tích chập K×K gốc trong quá trình huấn luyện, được chuyển đổi tương đương thành một tích chập K×K duy nhất trong quá trình suy luận. RepVGG [10] thêm nhánh tích chập 1×1 song song và nhánh đồng nhất vào mỗi lớp tích chập 3×3 trong quá trình huấn luyện và được chuyển đổi tương đương thành một lớp tích chập 3×3 duy nhất sau khi huấn luyện. Do cải thiện hiệu suất mà không tăng chi phí suy luận mà nó mang lại, tái tham số hóa cấu trúc đã nhận được sự chú ý và ứng dụng ngày càng tăng trong các nhiệm vụ thị giác máy tính khác nhau. Ví dụ, ExpandNets [13] sử dụng nó để thiết kế các mô hình nhỏ gọn, RepNAS [48] sử dụng nó trong tìm kiếm kiến trúc mạng nơ-ron, và ResRep [7] kết hợp nó với pruning.

Tuy nhiên, các nghiên cứu trước đây về tái tham số hóa cấu trúc chủ yếu tập trung vào kiến trúc CNN, làm cho chúng không tương thích với kiến trúc ViT. Bài báo này giới thiệu một phương pháp mới tách biệt giai đoạn huấn luyện và suy luận của kiến trúc ViT. Trong quá trình huấn luyện, chúng tôi sử dụng tính toán thưa thớt MoE hiệu quả với cơ chế Phân vùng Đồng nhất Ngẫu nhiên và Trung Bình Trọng Số Chuyên Gia. Trong quá trình suy luận, chúng tôi chuyển đổi mô hình về kiến trúc ViT gốc, dẫn đến cải thiện hiệu suất mà không tăng tham số và độ trễ suy luận. Phương pháp này là đầu tiên trong loại hình áp dụng tính toán thưa thớt MoE để cải thiện hiệu suất của kiến trúc ViT.

2.3 Trung bình Trọng số
Trung bình trọng số là một kỹ thuật được sử dụng phổ biến trong học sâu và đã được sử dụng rộng rãi trong các nhiệm vụ khác nhau. Ví dụ, trong học tự giám sát và bán giám sát, [39; 2] sử dụng trọng số trung bình động theo hàm mũ (EMA) của mô hình học sinh làm giáo viên để cung cấp mục tiêu mượt mà hơn cho học sinh. Tương tự, trong chưng cất kiến thức trực tuyến, [42] sử dụng trọng số EMA của mỗi nhánh làm giáo viên trung bình đồng cấp để chuyển giao kiến thức một cách hợp tác giữa các nhánh. Ngoài ra, nhiều nghiên cứu sử dụng trung bình trọng số để nâng cao khả năng tổng quát hóa của mô hình [17; 41; 40; 32]. Ví dụ, trung bình trọng số ngẫu nhiên (SWA) [17] lấy trung bình trọng số của nhiều điểm trên quỹ đạo tối ưu hóa SGD và đạt được khả năng tổng quát hóa tốt hơn so với huấn luyện thông thường. Wortsman et al. [41] lấy trung bình mô hình zero-shot gốc và mô hình đã được tinh chỉnh, dẫn đến cải thiện trong cả tổng quát hóa trong phân phối và ngoài phân phối. Model Soup [40] lấy trung bình các mô hình có cùng khởi tạo huấn luyện trước nhưng cấu hình siêu tham số khác nhau trong quá trình tinh chỉnh để có được một mô hình tốt hơn. DiWA [32] cung cấp phân tích lý thuyết cho sự thành công của trung bình trọng số trong OOD và đề xuất giảm hiệp phương sai giữa các dự đoán và cải thiện tổng quát hóa OOD bằng cách lấy trung bình trọng số của các mô hình được huấn luyện độc lập đa dạng.

Không giống như các nghiên cứu được đề cập ở trên thực hiện trung bình trọng số ở cấp mô hình để có được một mô hình duy nhất, phương pháp của chúng tôi thực hiện trung bình trọng số ở cấp chuyên gia trong MoE trong quá trình huấn luyện. Bằng cách lấy trung bình trọng số của các chuyên gia khác lên mỗi chuyên gia, chúng tôi có được nhiều chuyên gia.

2.4 Dropout và các biến thể khác cho ViT
Dropout [37] là một kỹ thuật phổ biến được sử dụng để cải thiện khả năng tổng quát hóa của mạng bằng cách loại bỏ ngẫu nhiên các nơ-ron và các kết nối tương ứng của chúng trong quá trình huấn luyện để ngăn ngừa overfitting. Đối với ViTs, hầu hết các công trình sử dụng dropout với tỷ lệ drop cố định cho các lớp self-attention hoặc FFN. Trong khi đó, Stochastic Depth [15] thường được sử dụng trong ViTs để loại bỏ ngẫu nhiên một số khối và áp dụng tỷ lệ drop cao hơn cho các khối sâu hơn để cải thiện tổng quát hóa. Gần đây, một biến thể dropout mới cho ViT được gọi là dropkey đã được giới thiệu. Dropkey [25] gán một toán tử thích ứng cho mỗi khối attention bằng cách loại bỏ ngẫu nhiên một số key trong quá trình tính toán attention. Chiến lược này hạn chế phân phối attention, làm cho nó mượt mà hơn và hiệu quả hơn trong việc giảm bớt overfitting. Hơn nữa, dropkey giảm dần tỷ lệ drop khi số lượng khối tăng lên, tránh sự bất ổn có thể xảy ra từ tỷ lệ drop không đổi trong các phương pháp dropout truyền thống trong quá trình huấn luyện.

Trong phương pháp của chúng tôi, đối với mỗi lớp MoE, chúng tôi chia ngẫu nhiên và đều tất cả các token giữa tất cả các chuyên gia trong quá trình huấn luyện. Điều này tương đương với việc loại bỏ ngẫu nhiên một số lượng lớn token tại mỗi chuyên gia. Một thao tác như vậy có thể giúp ngăn ngừa overfitting trong khi đảm bảo độ trễ huấn luyện vẫn gần như tương đương với huấn luyện vanilla.

3 Phương pháp

3.1 KIẾN THỨC CƠ BẢN
MoE. Như thể hiện trong Hình 1(a), một lớp MoE tiêu chuẩn bao gồm một tập hợp N chuyên gia (tức là N FFNs) {E1, E2, . . . , EN}, và một mạng định tuyến G với trọng số Wg. Cho một ví dụ đầu vào x, đầu ra của cả lớp MoE thời gian huấn luyện và suy luận có thể được viết là:

y=∑(i=1 to N) G(x)i·Ei(x) (1)

G(x) = TopK(softmax(x·Wg), k) (2)

trong đó G(x)i biểu thị điểm định tuyến cho chuyên gia thứ i, Ei(x) đại diện cho đầu ra của chuyên gia thứ i, TopK(·, k) có nghĩa là chọn k chuyên gia hàng đầu và đặt G(x) của các chuyên gia khác là 0. Thông thường, k≪N, có nghĩa là G(x) là một vector N chiều thưa thớt. Khi G(x)i = 0, Ei(x) không cần được tính toán.

Tái tham số hóa cấu trúc. Như thể hiện trong Hình 1(b), tái tham số hóa cấu trúc sử dụng cấu trúc CNN đa nhánh để thay thế lớp tích chập thông thường trong quá trình huấn luyện. Hãy ký hiệu {f1, f2, . . . , fM} là tổng M nhánh (thường là M toán tử khác nhau với kích thước tương thích). Đối với một đầu vào x đã cho, đầu ra của cấu trúc thời gian huấn luyện có thể được biểu diễn là:

y=∑(i=1 to M) fi(x) (3)

Sau khi huấn luyện, M nhánh này sẽ được chuyển đổi tương đương thành một lớp tích chập duy nhất F để suy luận. Đối với đầu vào x thời gian suy luận, đầu ra trở thành y=F(x).

3.2 Một Sơ đồ Huấn luyện Chung Mới

3.2.1 Động lực
Bài báo này nhằm mục đích thiết kế một sơ đồ huấn luyện chung mới cho ViTs, đạt được cải thiện hiệu suất mà không tăng chi phí suy luận. Như thể hiện trong Phương trình 3, tái tham số hóa cấu trúc cổ điển chỉ áp dụng cho CNNs bao gồm các tích chập và tăng đáng kể chi phí huấn luyện do cấu trúc đa nhánh. Như thể hiện trong Phương trình 2, MoE gần đây bao gồm N chuyên gia và một mạng định tuyến. Thứ nhất, mạng định tuyến mang lại thêm tham số cần học và dễ dẫn đến vấn đề mất cân bằng tải. Thứ hai, N chuyên gia làm tăng đáng kể gánh nặng triển khai mô hình. Thứ ba, điểm định tuyến G(x) là một nút cổ chai trong việc chuyển đổi MoE thành FFN. Xét đến những điều này, đầu tiên chúng tôi

--- TRANG 6 ---
đề xuất một MoE được thiết kế đặc biệt, hiệu quả hơn để thay thế một số FFNs của ViT để huấn luyện, gán token cho các chuyên gia bằng Phân vùng Đồng nhất Ngẫu nhiên. Sau đó, chúng tôi đề xuất một kỹ thuật đơn giản nhưng hiệu quả gọi là Trung Bình Trọng Số Chuyên Gia, lấy trung bình trọng số của các chuyên gia khác cho mỗi chuyên gia ở cuối mỗi lần lặp huấn luyện. Như vậy, sau khi huấn luyện, chúng tôi có thể chuyển đổi mỗi MoE thành một FFN duy nhất để suy luận. Chúng tôi chỉ ra cách áp dụng nó trong quá trình huấn luyện và tinh chỉnh dưới đây.

3.2.2 Huấn luyện EWA
Tổng quan. Khi áp dụng huấn luyện EWA để huấn luyện mô hình ViT từ đầu, đầu tiên chúng tôi tạo ra một MoE ViT được khởi tạo ngẫu nhiên bằng cách thay thế một số FFNs bằng các lớp MoE dựa trên Phân vùng Đồng nhất Ngẫu nhiên. Trong quá trình huấn luyện, chúng tôi thực hiện Trung Bình Trọng Số Chuyên Gia với tỷ lệ chia sẻ β cho mỗi lớp MoE sau mỗi lần cập nhật trọng số, và β tăng tuyến tính theo epoch huấn luyện từ 0 đến siêu tham số tỷ lệ chia sẻ. Sau khi huấn luyện, chúng tôi chuyển đổi mỗi MoE thành một FFN bằng cách lấy trung bình các chuyên gia.

Lớp MoE với Phân vùng Đồng nhất Ngẫu nhiên. Cho một lớp MoE bao gồm N chuyên gia (tức là N FFNs giống hệt nhau), {E1, E2, . . . , EN}, giả sử có T đầu vào {x1, x2, . . . , xT} và T có thể chia hết cho N. Các đầu ra của lớp MoE với Phân vùng Đồng nhất Ngẫu nhiên (RUP) có thể được viết là:

{x1, x2, . . . , xT} → {X1, X2, . . . , XN} (4)
y = Ei(x) if x ∈ Xi (5)

Như thể hiện trong Phương trình 4, T đầu vào được phân vùng ngẫu nhiên và đồng nhất thành N phần, trong đó Xi biểu thị T/N đầu vào được gán cho chuyên gia thứ i. Hơn nữa, như thể hiện trong Phương trình 5, đối với mỗi đầu vào trong Xi, đầu ra của nó là Ei(x). Chúng tôi hiển thị mã giống pytorch của RUP trong Phụ lục.

Trung Bình Trọng Số Chuyên Gia. Cho trọng số của N chuyên gia {W1, W2, . . . , WN} và tỷ lệ chia sẻ β, Trung Bình Trọng Số Chuyên Gia (EWA) được thực hiện trên mỗi lớp MoE có thể được công thức hóa là:

Wi = (1-β)Wi + ∑(j≠i) β/(N-1) Wj (6)

Wi biểu thị trọng số mới của chuyên gia thứ i. Tóm lại, chúng tôi lấy trung bình trọng số của các chuyên gia khác cho mỗi chuyên gia và có được nhiều chuyên gia. Mã giống pytorch của EWA được cung cấp trong Phụ lục.

Chuyển đổi MoE thành FFN. Sau khi huấn luyện, mỗi lớp MoE sẽ được chuyển đổi thành lớp FFN bằng cách đơn giản lấy trung bình trọng số của các chuyên gia. Cho một lớp MoE bao gồm N chuyên gia {E1, E2, . . . , EN}, FFN thời gian suy luận tương ứng có thể được biểu diễn là:

FFN = (1/N) ∑(i=1 to N) Ei (7)

Bằng cách này, mô hình MoE thời gian huấn luyện sẽ được chuyển đổi thành mô hình ViT vanilla để suy luận. Chúng tôi thực hiện các thí nghiệm để chỉ ra tính hợp lý trong Phụ lục.

3.2.3 Tinh chỉnh EWA
Tổng quan. Khi áp dụng huấn luyện EWA để tinh chỉnh mô hình ViT đã được huấn luyện trước, điểm khác biệt duy nhất so với huấn luyện ViT từ đầu là việc khởi tạo trọng số của mô hình MoE ViT được tạo ra. Thay vì khởi tạo ngẫu nhiên, chúng tôi khởi tạo MoE ViT với checkpoint mô hình ViT đã cho. Khi thay thế một số FFNs bằng các lớp MoE, mỗi chuyên gia của một lớp MoE cụ thể được khởi tạo như một bản sao của lớp FFN tương ứng trong mô hình ViT đã được huấn luyện trước. Đối với các lớp khác không thay đổi, trọng số của chúng được kế thừa trực tiếp từ checkpoint ViT gốc.

4 Phân tích Lý thuyết
Trong phần này, chúng tôi phân tích lý thuyết tại sao và cách thức hoạt động của huấn luyện EWA. Để thuận tiện cho phân tích, chúng tôi tập trung vào một lớp MoE và m bước huấn luyện. Giả sử rằng bước huấn luyện hiện tại là t, theo Phương trình 6, trọng số mới của chuyên gia thứ i sau khi trung bình trọng số chuyên gia có thể được viết là:

W̄t_i = (1-β)Wt_i + ∑(j≠i) β/(N-1) Wt_j (8)

--- TRANG 7 ---
Bảng 2: So sánh hiệu suất giữa huấn luyện vanilla và EWA trên các kiến trúc và bộ dữ liệu khác nhau. Đối với kiến trúc ViT (-XT, -XS, -l2, -T) và phương pháp huấn luyện (+SL) được thiết kế đặc biệt cho bộ dữ liệu nhỏ [22], việc thêm phương pháp EWA của chúng tôi vẫn có thể cải thiện hiệu suất của chúng.

(a) trên bộ dữ liệu CIFAR-100
Mô hình | Vanilla Top-1 | EWA Top-1 | Δ Top-1
ViT-S | 72.61 | 74.33 | +1.72
ViT-XT | 73.93 | 75.20 | +1.27
ViT-XT + SL | 77.04 | 77.94 | +0.90
PiT-XS | 74.99 | 75.65 | +0.66
PiT-XS + SL | 79.73 | 80.39 | +0.66
T2T-12 | 77.19 | 77.80 | +0.61
T2T-12 + SL | 78.35 | 78.81 | +0.46
Swin-T | 77.38 | 77.65 | +0.27
Swin-T + SL | 80.31 | 80.52 | +0.21

(b) trên bộ dữ liệu Tiny-ImageNet
Mô hình | Vanilla Top-1 | EWA Top-1 | Δ Top-1
ViT-S | 57.41 | 59.50 | +2.09
ViT-XT | 55.92 | 57.86 | +1.94
ViT-XT + SL | 59.72 | 60.75 | +1.03
PiT-XS | 59.03 | 61.61 | +2.58
PiT-XS + SL | 63.11 | 64.53 | +1.42
T2T-12 | 60.39 | 61.65 | +1.26
T2T-12 + SL | 62.57 | 63.36 | +0.79
Swin-T | 59.70 | 60.66 | +0.96
Swin-T + SL | 64.34 | 65.13 | +0.79

Sau đó, bước huấn luyện thứ (t+1) bắt đầu, và trọng số của mỗi chuyên gia của MoE được cập nhật thành {Wt+1_1, Wt+1_2, . . . , Wt+1_N}, trong đó Wt+1_i = W̄t_i + η∇W̄t_i, η biểu thị tỷ lệ học. Hơn nữa, quá trình trung bình trọng số chuyên gia được thực hiện, như thể hiện trong Phương trình 9. Dựa trên Phương trình 8 và Wt+1_i, Phương trình 9 có thể được công thức hóa lại thành Phương trình 10.

W̄t+1_i = (1-β)Wt+1_i + ∑(j≠i) β/(N-1) Wt+1_j (9)

= (1-β)²W̄t_i + η(1-β)∇W̄t_i + ∑(j≠i) β/(N-1) Wt+1_j + β(1-β)/(N-1) W̄t_j (10)

Tương tự, chúng ta có thể tính được W̄t+2_i, W̄t+3_i và W̄t+m_i. Thông qua quy nạp toán học, chúng ta có Phương trình 12.

W̄t+m_i = (1-β)Wt+m_i + ∑(j≠i) β/(N-1) Wt+m_j (11)

= (1-β)m+1W̄t_i + η∑(k=1 to m)(1-β)k∇W̄t+m-k_i + β/(N-1) ∑(j≠i)∑(k=0 to m)(1-β)m-k·W̄t+k_j (12)

Theo việc đạo hàm trên, có hai khám phá: 1) Quan sát số hạng đầu tiên của Phương trình 9 và Phương trình 11, có một suy giảm trọng số theo lớp khi thực hiện trung bình trọng số chuyên gia; 2) Quan sát số hạng cuối cùng của Phương trình 10 và Phương trình 12, có sự tích lũy liên tục các trọng số trung bình hàm mũ lịch sử đa chuyên gia dọc theo lần lặp huấn luyện của huấn luyện EWA. Thêm chi tiết được hiển thị trong Phụ lục.

5 Thí nghiệm
Khi áp dụng huấn luyện EWA để huấn luyện hoặc tinh chỉnh ViTs, chúng tôi luôn sử dụng MoE dựa trên phân vùng đồng nhất ngẫu nhiên để thay thế một số FFNs. Trừ khi có quy định khác, số lượng chuyên gia của mỗi MoE được đặt mặc định là 4. Siêu tham số tỷ lệ chia sẻ được điều chỉnh đơn giản trong {0.1, 0.2, 0.3, 0.4, 0.5} cho các kiến trúc ViT khác nhau để có được hiệu suất tốt nhất. Theo V-MoE [34], vị trí chèn các lớp MoE này được điều chỉnh đơn giản trong {every-2, last-4}. Tất cả chi tiết được hiển thị trong Phụ lục.

5.1 Huấn luyện EWA

5.1.1 Phân loại 2D trên các kiến trúc và bộ dữ liệu khác nhau
Thiết lập. Chúng tôi thực hiện các thí nghiệm toàn diện trên các kiến trúc và bộ dữ liệu khác nhau để đánh giá sơ đồ huấn luyện EWA. Đối với bộ dữ liệu, chúng tôi sử dụng CIFAR-100 và Tiny-ImageNet. Đối với kiến trúc, bên cạnh ViT-S tiêu chuẩn, chúng tôi cũng áp dụng các kiến trúc ViT khác nhau được thiết kế đặc biệt cho bộ dữ liệu nhỏ như ViT-Tiny nhỏ hơn (chúng tôi gọi là ViT-XT), PiT-XS, T2T-12, Swin-T, và các đối tác SL tăng cường của chúng [22]. SL có nghĩa là áp dụng tokenization patch dịch chuyển và self-attention tính địa phương, cho phép ViTs học tốt hơn trên bộ dữ liệu kích thước nhỏ [22]. Chi tiết của tất cả ViTs được sử dụng được hiển thị trong Phụ lục. Đối với huấn luyện vanilla và EWA, chúng tôi luôn áp dụng CutMix, Mixup, Auto Augment, random erasing, và label smoothing nhất quán cho tất cả các mô hình. Trên CIFAR-100, kích thước đầu vào là 32×32, kích thước patch của ViT được đặt là 4, trong khi 2 cho PiT và Swin. Trên Tiny-ImageNet, kích thước đầu vào là 64×64, kích thước patch được đặt là 8 cho ViT, trong khi 4 cho PiT và Swin. Tất cả các mô hình được huấn luyện với bộ tối ưu hóa AdamW, lịch trình cosine và kích thước batch là 128. Đối với ViT-S, chúng tôi huấn luyện nó trong 300 epoch với 30 epoch khởi động, tỷ lệ học được đặt là 0.0006, weight decay được đặt là 0.06. Đối với các mô hình còn lại, chúng tôi huấn luyện chúng trong 100 epoch với 10 epoch khởi động, weight decay được đặt là 0.05, tỷ lệ học được đặt là 0.003 (0.001 cho Swin-T).

Kết quả. Như thể hiện trong Bảng 2, so với huấn luyện vanilla, huấn luyện EWA có thể mang lại cải thiện hiệu suất nhất quán cho các kiến trúc ViT khác nhau trên các bộ dữ liệu khác nhau. Đối với ViT-S tiêu chuẩn, huấn luyện EWA có thể mang lại cải thiện lớn {1.72%, 2.09%} trên {CIFAR-100, Tiny-ImageNet} tương ứng. Ngay cả đối với các kiến trúc ViT được thiết kế đặc biệt và các đối tác tăng cường SL của chúng, việc thêm sơ đồ huấn luyện EWA vẫn có thể cải thiện hiệu suất của chúng, điều này xác nhận thêm tính hiệu quả và tổng quát hóa của nó. Ví dụ, Swin-T tăng cường SL với huấn luyện vanilla đã đạt được độ chính xác top-1 tương đối cao, 80.31% trên CIFAR-100 và 64.34% trên Tiny-ImageNet. Nó vẫn có thể đạt được cải thiện hiệu suất {0.21%, 0.79%} từ huấn luyện EWA cho mỗi bộ dữ liệu.

5.1.2 Nhiệm vụ thị giác 3D
Thiết lập. Để chứng minh khả năng huấn luyện chung của phương pháp của chúng tôi, chúng tôi cũng thực hiện các thí nghiệm về các nhiệm vụ phân loại đám mây điểm (thưa thớt) và phân đoạn ngữ nghĩa (dày đặc). Theo thiết lập thí nghiệm của PointBERT [47], chúng tôi thực hiện phân loại đám mây điểm trên ModelNet40 [30]. Theo thiết lập thí nghiệm của Pix4point [29], chúng tôi thực hiện nhiệm vụ phân đoạn ngữ nghĩa đám mây điểm trên S3DIS [43] và để lại khu vực 5 làm thử nghiệm, các khu vực khác làm huấn luyện. Theo PointBERT và Pix4point, chúng tôi sử dụng encoder tiêu chuẩn trong ViT-S làm backbone transformer.

Kết quả. Như thể hiện trong Bảng 3, sơ đồ huấn luyện EWA được đề xuất đạt được cải thiện hiệu suất nhất quán trên cả nhiệm vụ phân loại đám mây điểm và phân đoạn ngữ nghĩa. Đáng chú ý, phương pháp của chúng tôi đạt được cải thiện 0.52% về độ chính xác tổng thể (OA), 1.40% về độ chính xác trung bình (mAcc) và 1.74% về IoU trung bình (mIoU) trên nhiệm vụ phân đoạn.

5.2 Tinh chỉnh EWA
Thiết lập. Để đánh giá tinh chỉnh EWA, chúng tôi lấy ViT-B và ViT-S đã được huấn luyện trước từ thư viện timm và tinh chỉnh chúng trên CIFAR-100 và Food-101. Đối với cả CIFAR-100 và Food-101 [18], kích thước batch được đặt là 128 và tất cả hình ảnh được thu nhỏ thành 224×224 để tinh chỉnh. Đối với CIFAR-100, tổng số bước tinh chỉnh được đặt là 5000 với 500 bước khởi động. Đối với Food-101, chúng tôi khởi động 100 bước đầu tiên và số bước tổng cộng là 1000. Chúng tôi sử dụng bộ tối ưu hóa SGD với momentum 0.9. Tỷ lệ học là 0.01 cho CIFAR-100 và 0.03 cho Food-101. Chúng tôi chỉ đơn giản đặt MoE trên mỗi lớp khác và đặt số lượng chuyên gia của lớp MoE là 4. β của Trung Bình Trọng Số Chuyên Gia (Phương trình 6) tăng tuyến tính theo bước hiện tại từ 0 đến siêu tham số tỷ lệ chia sẻ đã cho.

--- TRANG 8 ---
Kết quả. Bảng 4 hiển thị so sánh hiệu suất tinh chỉnh giữa tinh chỉnh vanilla và EWA. Mặc dù hiệu suất của tinh chỉnh vanilla khá cao, tinh chỉnh EWA vẫn có thể mang lại cải thiện thêm. Cụ thể, được trang bị tinh chỉnh EWA, ViT-S và ViT-B đạt được cải thiện hiệu suất {0.38%, 0.45%} và {0.54%, 0.71%} trên {Food-101, CIFAR-100} tương ứng.

5.3 Nghiên cứu Khử bỏ
Lịch trình chia sẻ và Tỷ lệ chia sẻ. Chúng tôi đầu tiên nghiên cứu cách đặt siêu tham số tỷ lệ chia sẻ và lịch trình chia sẻ cho Trung Bình Trọng Số Chuyên Gia. Trong phần trên, lịch trình chia sẻ được đặt là tăng tuyến tính theo mặc định, chúng tôi ở đây so sánh nó với lịch trình chia sẻ không đổi. Đối với tỷ lệ chia sẻ, chúng tôi điều chỉnh nó trong {0.1, 0.2, 0.3, 0.4, 0.5}. Chúng tôi huấn luyện ViT-S trên CIFAR100 sử dụng huấn luyện EWA với các lịch trình chia sẻ và tỷ lệ chia sẻ khác nhau. Như thể hiện trong Hình 2, huấn luyện EWA với các lịch trình và tỷ lệ chia sẻ khác nhau luôn có thể vượt trội hơn huấn luyện vanilla, và huấn luyện EWA với lịch trình tăng tuyến tính hoạt động tốt hơn lịch trình không đổi trong hầu hết các trường hợp. Các nghiên cứu khử bỏ về số lượng chuyên gia được hiển thị trong Phụ lục.

Bảng 3: So sánh hiệu suất giữa huấn luyện vanilla và EWA về phân loại đám mây điểm và phân đoạn ngữ nghĩa đám mây điểm.

(a) Phân loại trên ModelNet40.
Sơ đồ | Acc.(không vote) | Acc.(có vote)
Vanilla | 92.42 | 92.67
EWA | 92.54 | 92.79

(b) Phân đoạn ngữ nghĩa trên S3DIS Area5.
Sơ đồ | OA | mAcc. | mIoU
Vanilla | 89.53 | 72.43 | 66.62
EWA | 90.05 | 73.83 | 68.36

Bảng 4: So sánh hiệu suất của tinh chỉnh vanilla và EWA trên các mô hình và bộ dữ liệu khác nhau.

(a) trên bộ dữ liệu Food-101
Mô hình | Vanilla Top-1 | EWA Top-1 | Δ Top-1
ViT-S | 88.04 | 88.42 | +0.38
ViT-B | 86.78 | 87.32 | +0.54

(b) trên bộ dữ liệu CIFAR-100
Mô hình | Vanilla Top-1 | EWA Top-1 | Δ Top-1
ViT-S | 90.22 | 90.67 | +0.45
ViT-B | 90.71 | 91.42 | +0.71

[Hình 2: Nghiên cứu khử bỏ về lịch trình chia sẻ và tỷ lệ chia sẻ của sơ đồ EWA được đề xuất.]

6 Thảo luận và Khám phá Thêm
Thú vị hơn, chúng tôi thấy rằng, trên các bộ dữ liệu thị giác 2D nhỏ và các nhiệm vụ thị giác 3D, MoE thô sơ dẫn đến hiệu suất thấp hơn so với các đối tác dày đặc gốc của nó, và công nghệ EWA của chúng tôi có thể được áp dụng liền mạch vào MoE thô sơ và giúp nó học tốt hơn.

MoE thô sơ. Theo thiết lập huấn luyện ViT-S trên CIFAR-100, Tiny-ImageNet, ModelNet40 và S3DIS, chúng tôi tiếp tục huấn luyện V-MoE-S của chúng tôi, thay thế một số FFN của ViT-S bằng MoE định tuyến top-1 thô sơ [12] mỗi khối khác. Chúng tôi đặt số lượng chuyên gia là 4, trọng số của tổn thất cân bằng tải λ là 0.01 và tỷ lệ dung lượng C là 1.05 cho bộ dữ liệu 2D và 1.2 cho bộ dữ liệu 3D. Như thể hiện trong Bảng 5, với huấn luyện vanilla, V-MoE-S thô sơ làm giảm hiệu suất so với ViT-S trên các bộ dữ liệu khác nhau.

Bảng 5: So sánh giữa các mô hình khác nhau với các sơ đồ huấn luyện khác nhau. So với ViT-S với huấn luyện vanilla, V-MoE-S với huấn luyện vanilla làm giảm hiệu suất, trong khi V-MoE-S với Early EWA cải thiện hiệu suất đáng kể, trên các bộ dữ liệu 2D/3D khác nhau.

Mô hình+sơ đồ | ViT-S+Vanilla | V-MoE-S+Vanilla | V-MoE-S+Early EWA
C100 Acc | 72.61 | 72.17 | 74.31
Tiny-Img Acc | 57.41 | 56.28 | 59.91
MN40 Acc | 92.42 | 92.27 | 92.63
S3DIS mIoU | 66.62 | 66.36 | 67.67

MoE thô sơ + Early EWA. Mọi thứ thay đổi khi chúng tôi đưa Trung Bình Trọng Số Chuyên Gia (EWA) vào MoE thô sơ. Xét rằng sử dụng EWA trong toàn bộ quá trình huấn luyện cuối cùng sẽ dẫn đến trọng số gần như giống hệt nhau giữa các chuyên gia, chúng tôi chỉ thực hiện EWA trên MoE thô sơ trong giai đoạn đầu của huấn luyện. Theo mặc định, chúng tôi sử dụng lịch trình chia sẻ không đổi và chỉ thực hiện Early EWA trong nửa đầu của tổng số epoch huấn luyện. Với huấn luyện Early EWA, hiệu suất của V-MoE-S được cải thiện đáng kể, dẫn đến tăng độ chính xác top-1 {2.14%, 3.63%} trên {CIFAR-100, Tiny-ImageNet} tương ứng. Ngoài ra, huấn luyện Early EWA của chúng tôi có thể cải thiện hiệu suất của V-MoE-S trên các nhiệm vụ thị giác 3D khác nhau, với tăng 0.36% độ chính xác trên phân loại ModelNet40, và tăng 0.30% OA, 1.40% mAcc, 1.31% mIoU trên phân đoạn S3DIS Area5. Các nghiên cứu thêm được hiển thị trong Phụ lục.

7 Kết luận
Kết luận, sơ đồ huấn luyện được đề xuất của chúng tôi cho ViTs đạt được cải thiện hiệu suất mà không tăng độ trễ và tham số suy luận. Bằng cách thiết kế các MoE hiệu quả và EWA trong quá trình huấn luyện, và chuyển đổi mỗi MoE trở lại thành FFN bằng cách lấy trung bình các chuyên gia trong quá trình suy luận, chúng tôi tách biệt

--- TRANG 9 ---
giai đoạn huấn luyện và suy luận của ViTs. Các thí nghiệm toàn diện trên nhiều nhiệm vụ thị giác 2D và 3D, kiến trúc ViT, và bộ dữ liệu chứng minh tính hiệu quả và khả năng tổng quát hóa. Phân tích lý thuyết hỗ trợ thêm cho phương pháp của chúng tôi, và sơ đồ huấn luyện của chúng tôi cũng có thể được áp dụng để tinh chỉnh ViTs và cải thiện hiệu quả của MoE thô sơ trong các nhiệm vụ thị giác khác nhau. Tổng thể, sơ đồ huấn luyện được đề xuất của chúng tôi cung cấp một hướng đầy hứa hẹn để nâng cao hiệu suất của ViTs trong các nhiệm vụ thị giác khác nhau, và chúng tôi tin rằng nó có thể dẫn đến nghiên cứu và phát triển thêm trong lĩnh vực này.

Tài liệu tham khảo

[1] Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684, 2021.

[2] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650–9660, 2021.

[3] Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, Hengshuang Zhao, Erik Learned-Miller, and Chuang Gan. Mod-squad: Designing mixture of experts as modular multi-task learners. arXiv preprint arXiv:2212.08066, 2022.

[4] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International Conference on Machine Learning, pages 4057–4086. PMLR, 2022.

[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.

[6] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong Han. Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1911–1920, 2019.

[7] Xiaohan Ding, Tianxiang Hao, Jianchao Tan, Ji Liu, Jungong Han, Yuchen Guo, and Guiguang Ding. Resrep: Lossless cnn pruning via decoupling remembering and forgetting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4510–4520, 2021.

[8] Xiaohan Ding, Chunlong Xia, Xiangyu Zhang, Xiaojie Chu, Jungong Han, and Guiguang Ding. Repmlp: Re-parameterizing convolutions into fully-connected layers for image recognition. arXiv preprint arXiv:2105.01883, 2021.

[9] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Diverse branch block: Building a convolution as an inception-like unit. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10886–10895, 2021.

[10] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13733–13742, 2021.

[11] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547–5569. PMLR, 2022.

[12] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232–5270, 2022.

[13] Shuxuan Guo, Jose M Alvarez, and Mathieu Salzmann. Expandnets: Linear over-parameterization to train compact convolutional networks. Advances in Neural Information Processing Systems, 33:1298–1310, 2020.

[14] Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, and Jie Tang. Fastmoe: A fast mixture-of-expert training system. arXiv preprint arXiv:2103.13262, 2021.

[15] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages 646–661. Springer, 2016.

--- TRANG 10 ---

[16] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, et al. Tutel: Adaptive mixture-of-experts at scale. arXiv preprint arXiv:2206.03382, 2022.

[17] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018.

[18] Parneet Kaur, Karan Sikka, and Ajay Divakaran. Combining weakly and webly supervised learning for classifying food images. arXiv preprint arXiv:1712.08730, 2017.

[19] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints. arXiv preprint arXiv:2212.05055, 2022.

[20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

[21] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.

[22] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision transformer for small-size datasets. arXiv preprint arXiv:2112.13492, 2021.

[23] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.

[24] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning, pages 6265–6274. PMLR, 2021.

[25] Bonan Li, Yinhan Hu, Xuecheng Nie, Congying Han, Xiangjian Jiang, Tiande Guo, and Luoqi Liu. Dropkey. arXiv preprint arXiv:2208.02646, 2022.

[26] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal contrastive learning with limoe: the language-image mixture of experts. arXiv preprint arXiv:2206.02770, 2022.

[27] Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue, Lingxiao Ma, Gang Cao, and Bin Cui. Flexmoe: Scaling large-scale sparse pre-trained model training via dynamic device placement. arXiv preprint arXiv:2304.03946, 2023.

[28] Xiaonan Nie, Pinxue Zhao, Xupeng Miao, and Bin Cui. Hetumoe: An efficient trillion-scale mixture-of-expert distributed training system. arXiv preprint arXiv:2203.14685, 2022.

[29] Guocheng Qian, Xingdi Zhang, Abdullah Hamdi, and Bernard Ghanem. Pix4point: Image pretrained transformers for 3d point cloud understanding. arXiv preprint arXiv:2208.12259, 2022.

[30] Shi Qiu, Saeed Anwar, and Nick Barnes. Geometric back-projection network for point cloud classification. IEEE Transactions on Multimedia, 24:1943–1955, 2021.

[31] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale. In International Conference on Machine Learning, pages 18332–18346. PMLR, 2022.

[32] Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. arXiv preprint arXiv:2205.09739, 2022.

[33] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021.

[34] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:8583–8595, 2021.

[35] Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. Advances in Neural Information Processing Systems, 34:17555–17566, 2021.

--- TRANG 11 ---

[36] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.

[37] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.

[38] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pages 843–852, 2017.

[39] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.

[40] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, pages 23965–23998. PMLR, 2022.

[41] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7959–7971, 2022.

[42] Guile Wu and Shaogang Gong. Peer collaborative learning for online knowledge distillation. In Proceedings of the AAAI Conference on artificial intelligence, volume 35, pages 10302–10310, 2021.

[43] Qiangeng Xu, Xudong Sun, Cho-Ying Wu, Panqu Wang, and Ulrich Neumann. Grid-gcn for fast and scalable point cloud learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5661–5670, 2020.

[44] Fuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou, and Yang You. One student knows all experts know: From sparse to dense. arXiv preprint arXiv:2201.10890, 2022.

[45] Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, and Yang You. Go wider instead of deeper. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 8779–8787, 2022.

[46] Zhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe: Scaling to large acoustic models with dynamic routing mixture of experts. arXiv preprint arXiv:2105.03036, 2021.

[47] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19313–19322, 2022.

[48] Mingyang Zhang, Xinyi Yu, Jingtao Rong, and Linlin Ou. Repnas: Searching for efficient re-parameterizing blocks. arXiv preprint arXiv:2109.03508, 2021.

[49] Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, and Zhang Xiong. Mixture of attention heads: Selecting attention heads per token. arXiv preprint arXiv:2210.05144, 2022.

[50] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:7103–7114, 2022.

[51] Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, and Jianfeng Gao. Taming sparsely activated transformer with stochastic experts. arXiv preprint arXiv:2110.04260, 2021.

12

--- TRANG 12 ---
>>>>>>> Stashed changes
