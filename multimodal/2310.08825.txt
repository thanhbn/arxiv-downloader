# 2310.08825.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.08825.pdf
# File size: 6754094 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
From CLIP to DINO: Visual Encoders Shout in
Multi-modal Large Language Models
Dongsheng Jiang1⋆, Yuchen Liu2∗, Songlin Liu1∗, Jin’e Zhao3, Hao Zhang3,
Zhen Gao3, Xiaopeng Zhang1, Jin Li2, Hongkai Xiong2.
1Huawei Cloud2Shanghai Jiao Tong University3Yunding Technology
dongsheng_jiang@outlook.com, wspolsl@gmail.com, zxphistory@gmail.com
{liuyuchen6666, deserve_lj, xionghongkai}@sjtu.edu.cn
Abstract. Multi-modal Large Language Models (MLLMs) have made
significant strides in expanding the capabilities of Large Language Mod-
els (LLMs) through the incorporation of visual perception interfaces.
Despite the emergence of exciting applications and the availability of
diverse instruction tuning data, existing approaches often rely on CLIP
or its variants as the visual branch, and merely extract features from
the deep layers. However, these methods lack a comprehensive analysis
of the visual encoders in MLLMs. In this paper, we conduct an exten-
sive investigation into the effectiveness of different vision encoders within
MLLMs. Our findings reveal that the shallow layer features of CLIP offer
particular advantages for fine-grained tasks such as grounding and region
understanding. Surprisingly, the vision-only model DINO, which is not
pretrained with text-image alignment, demonstrates promising perfor-
mance as a visual branch within MLLMs. By simply equipping it with
an MLP layer for alignment, DINO surpasses CLIP in fine-grained re-
lated perception tasks. Building upon these observations, we propose a
simple yet effective feature merging strategy, named COMM , that inte-
grates CLIP and DIN OwithMulti-level features Merging, to enhance
the visual capabilities of MLLMs. We evaluate COMM through com-
prehensive experiments on a wide range of benchmarks, including image
captioning, visual question answering, visual grounding, and object hal-
lucination. Experimental results demonstrate the superior performance
ofCOMM compared to existing methods, showcasing its enhanced vi-
sual capabilities within MLLMs.
1 Introduction
LargeLanguageModels(LLMs)[10,31,32,39,42,43]havemadesignificantstrides
in the domains of language understanding and generation, achieving remarkable
progress recently. Through instruction tuning [44,45], existing LLMs demon-
strate their versatility as general-purpose models capable of handling a wide
range of tasks. This capability unlocks their potential zero-shot learning abil-
ity, enabling seamless task switching guided by instructions. Building upon the
promising performance of LLMs, researchers are now motivated to enhance their
capabilities by incorporating visual signals as inputs. This extension allows the
⋆Equal contribution. This work was done when Yuchen Liu and Jin Li worked as
interns at Huawei Cloud.arXiv:2310.08825v3  [cs.CV]  8 Mar 2024

--- PAGE 2 ---
2 D. Jiang et al.
generation of textual outputs that are closely related to visual content, opening
up exciting possibilities in the realm of vision-language understanding.
To this end, Flamingo [1] and BLIP2 [19] align the powerful LLMs with a
frozen visual encoder to understand visual inputs and perform various vision-
language tasks. A series of following works, LLaVA [23], InstructBLIP [11],
MiniGPT-4 [53] and mPLUG-OWL [46] further improve the ability to follow
human instructions by constructing multi-modal instruction-following datasets
for training. However, these methods are built on image-level alignments, which
sufferfromthelimitedfine-grainedunderstanding(suchasregiondescription[25]
and reasoning [49]) and severe object hallucination problem [21]. To this end,
GPT4ROI [51] proposes instruction tuning on region-of-interest and unlocks the
region-levelmultimodalcapacities.Kosmos-2[34]andShikra[8]furtherintegrate
the grounding abilities into LLMs and unlock the referential ability in dialogue,
i.e., enable the user to point to the object or region as input and the model
responds with spatial coordinates of bounding boxes. Such grounding capacity
can fulfill numerous vision-language tasks, which is a great progress in MLLMs.
Despite a wide variety of exciting methods and applications, most of existing
multi-modal LLMs employ CLIP [36] or its variants [38] as the visual branch,
where the features output from the deep layers ( e.g., the penultimate layer)
are usually employed as inputs to the language decoders. However, it still lacks
analysis that: Whether using the Vanilla CLIP features as visual encoder is the
best way for MLLMs? Though the visual encoder of CLIP is apparently well
aligned with the word embedding space by image-text contrastive learning, it
fails to learn more detailed pixel-level information such as color and position-
ing due to the global supervision of image captions, which might hinder the
fine-grained perception ability in MLLMs. Besides, existing MLLMs have quite
unbalanced visual and language encoders ( e.g., ViT-Large-300M vs. Vicuna-
7B/13B). Since the language models have succeeded in scaling up the model size
with progressively powerful language abilities, the short plate of the Buckets
Effect for MLLMs lies in the visual models, which fails to demonstrate emerging
capabilities, and suffer from domain gap and limited zero-shot ability. Conse-
quently, it is critical to enhance the visual capabilities for boosting MLLMs.
This paper presents an extensive investigation into different visual encoders
for MLLMs. Four typical visual foundation models are considered, i.e., image-
text contrastive learning CLIP, image-only contrastive learning DINOv2 [33],
masked image modeling MAE [14] and supervised learning DeiT [40]. We eval-
uate the performance on commonly-used vision-language tasks including visual
grounding,objecthallucination,visualquestionanswering,imagecaptioningand
MME benchmark. Our analysis reveals that different layers of features exhibit
varying biases towards local and global patterns. Shallow layer features con-
taining low-level detailed information prove beneficial for fine-grained percep-
tion tasks such as grounding and positioning ability, while deep layer features
are superior at global understanding. To enhance representation, we propose a
multi-level feature merging strategy that incorporates both low-level and high-
level features. Surprisingly, when equipped with an MLP layer for alignment, the

--- PAGE 3 ---
From CLIP to DINO: Visual Encoders Shout in MLLMs 3
vision-only model DINOv2 shows promise as a visual branch for MLLMs. We
attribute this to the fine-grained localization information captured by DINOv2.
Conversely, MAE and DeiT perform inferiorly as visual branches for MLLMs.
MAE learns limited semantic information, while DeiT’s strong supervised train-
ing makes the alignment with the textual space challenging. Based on the above
observations, we propose a fusion strategy that integrates CLIP and DIN O
withMulti-level features Merging), dubbed as COMM , for boosting the visual
branches of MLLMs. Experimental results demonstrate clear advantages of the
proposed model over existing approaches and highlight the enhanced visual ca-
pabilities brought by COMM . In a nutshell, the contributions of this paper are
summarized as follows:
–We are the first to extensively investigate the effectiveness of different vi-
sual encoders for MLLMs. Based on the analysis that shallow layer features
contain low-level detailed information which is helpful for fine-grained tasks,
we propose a multi-level feature fusion strategy to incorporate low-level and
high-level features for improving representation.
–Our analysis indicates that vision-only DINOv2 achieves promising results
in MLLMs with only an MLP layer for alignment. Considering fine-grained
pixel information in DINOv2 and global semantic information in CLIP, we
proposeCOMM to fuse the visual embeddings of these two models to en-
hance visual capabilities for boosting MLLMs.
–Extensive experiments on a wide range of tasks including visual grounding,
referring expression generation, object hallucination, visual question answer-
ing and image captioning demonstrate the superiority of COMM over ex-
isting works.
2 Related Work
Multi-modal Large Language Model. LLMs [5,12] have garnered significant
attention in both academia and industry due to their remarkable understanding
and generative abilities. The success of LLMs has motivated researchers to ex-
plore the integration of vision into these models, leading to the development of
powerful multi-modal LLMs (MLLMs). Flamingo [1] employs a cross-attention
module to extract visual contexts, which are concatenated with text token as
input for LLMs. LLaVA [24] and FROMAGe [16] leverage the vision encoder of
CLIP to extract visual features, which is aligned to text features using a single
linear layer and then input to LLMs. Models such as BLIP-2 [20], mPLUG-
OWL [46], MiniGPT-4 [53] and InstructBLIP [11] employ Q-former to extract
text-aligned visual features for LLMs. Recently, some interesting works extend
LLMs to image retrieval [16], video understanding [50], audio [37], biomedical
analysis [18], control systems [13].
In recent studies, there has been a growing interest in extending MLLMs
to improve their fine-grained understanding abilities through region-level image-
textalignment.Kosmos-2[34]addressesthisbyconstructingalarge-scaledataset
of grounded region-text pairs, enabling the integration of grounding abilities

--- PAGE 4 ---
4 D. Jiang et al.
into LLMs. GPT4RoI [51] reformulates the bounding box as a spatial instruc-
tion format and extracts visual features based on region-of-interest, facilitat-
ing region-level multi-modal understanding. Shikra [8] proposes a unified model
that handles spatial coordinates to possess referential abilities in dialogue con-
texts. Ferret [47] and ViP-LLaVA [7] further extends with a broader range of
free-form shapes for referring, including points, boxes, sketches and scribbles.
Additionally, Qwen [3] presents a set of MLLMs that demonstrate remarkable
performance across various tasks. However, previous works have predominantly
focused on extracting visual features solely from the last few layers of the CLIP
model, resulting in an emphasis on global image properties. In this study, we
draw attention to the fact that features extracted from shallower layers exhibit
a stronger focus on localized properties, which we argue can be more potent
in comprehending object locations and image details. Additionally, while CLIP
primarily learns globally aligned features, advanced vision-alone models such as
DINOv2excelincapturingmorefine-grainedvisionfeatures.Wepositthatlever-
aging these fine-grained vision features can effectively enhance the capabilities
of MLLMs, as demonstrated in our analysis. To further advance this line of re-
search, we introduce a novel fusion module that expands and enhances the visual
branches, thereby aiming to significantly improve the performance of MLLMs.
Large Vision Foundation Model. Recent progresses in training vision
foundation models with large-scale image data focus on contrastive learning,
maskedimagemodelingandsupervisedtraining.Foronething,contrastivelearn-
ing can be conducted in an image-only or image-text manner. DINOv2 [33] pre-
trains the image encoder on large curated image data, which shows a superior
understanding of object parts and scene geometry across image domains. Image-
text contrastive learning as CLIP [36] and EVA-CLIP [38] employs the natural
language as weak supervision to guide the learning of visual features. For an-
other, BEiT [4] predicts discrete tokens based on a pre-trained image tokenizer
whileiBOT[52]proposesanonlineimagetokenizer.MAE[14]proposesamasked
autoencoder for reconstructing image pixels. Besides, DeiT III [41] proposes a
training recipe to achieve promising performance. Recent MLLMs employ the
vision encoder of CLIP/EVA-CLIP without considering the properties of spe-
cific visual models. In this paper, we are the first to re-examine the effectiveness
of existing visual models in MLLMs and propose a simple yet effective fusion
strategy for boosting visual capabilities.
3 Analysis of the Visual Branch in MLLMs
Previous MLLMs [3,8,11,22,23,34,46,47,53] usually utilize the vision encoder
of CLIP as their visual branch. Typically, these models extract features from
the last few layers, such as the penultimate layer, which are then fed into an
alignment network. Subsequently, the aligned features are concatenated with
text tokens to serve as input for the LLMs. While the image-text pretraining
of CLIP aligns well with the language model, it primarily learns image-level
features but overlooks the richer pixel-level features due to the constraint of
limited fine-grained information in the guided captions. Moreover, the deep-layer

--- PAGE 5 ---
From CLIP to DINO: Visual Encoders Shout in MLLMs 5
Fig. 1:Feature correspondence visualization by computing the cosine similarity of
differentvisualtokensextractedfromtheshallowanddeeplayersofCLIPandDINOv2.
features primarily focus on global image properties and inadequately explore the
intricate details of local object parts. As depicted in Fig. 1, the visual features
extracted from the shallow layers of CLIP and the deep visual features obtained
fromthevisual-onlymodelDINOv2 containmoredetailedinformation regarding
local objects, such as shape or texture. Leveraging these detailed features may
enhance the MLLMs’ fine-grained perception abilities.
Evaluation Settings. For further analysis, we conduct a series of quanti-
tative experiments using different kinds of visual models, i.e., image-text con-
trastive learning CLIP, image-only contrastive learning DINOv2, masked image
modeling MAE and supervised learning DeiT. In specific, the visual features ex-
tracted from different layers of visual models (based on ViT-Large) are aligned
using a linear projection layer and then concatenated with text tokens as the
input for LLMs (here we use Vicuna-7B [10]). The overall architecture and train-
ing process follow Shikra [8] but with fewer iterations (9400 iterations, batch size
16 on 4 A800) to save the computation cost. Then, we measure the capability
of the trained MLLMs on referring expression comprehension (REC) [8], re-
ferring expression generation (REG) [34] and object hallucination benchmark
(POPE) [21]. Detailed descriptions of these tasks can be referred to Sec. 5.
CLIP as the Visual Branch of MLLMs. As depicted in Fig. 2, we ob-
serve that different layers of features exhibit varying biases towards grounding
and understanding abilities. For instance, the shallow features demonstrate rel-
atively higher accuracy in terms of REC and reach their optimal value at layer
12. Conversely, the deep features achieve higher accuracy in terms of POPE,
indicating superior understanding ability. Notably, the relatively deep features
(layer 16) display the best REG CIDEr score, showcasing promising region un-
derstanding capabilities. Consequently, instead of solely relying on deep features
as done in previous works, we argue that integrating both shallow and deep
features is crucial for MLLMs with improved overall performance.
Wefurtherexplorevariousmergingmodesoflow-levelandhigh-levelfeatures.
Denoting the output features from each transformer layer of ViT with a depth of
Nasz= [z1, .., zi, ..., z N], we discuss several multi-level feature merging (MFM)
strategies for combining shallow and deep features, namely:
•Mean(half) : averaging output patch token features in the second half of
the backbone as z= (zN/2+···+zN)/(N/2).
•Mean(all) : averaging features output by all layers as z= (z1+···+zN)/N.
•Layerscale(all) : learning a scale parameter as the weight to sum features
output by all layers as z=w1z1+···+wNzN, where wirefers to the weight

--- PAGE 6 ---
6 D. Jiang et al.
(a)Average REC accuracy.
 (b)Average POPE accuracy.
 (c)Average REG CIDEr.
Fig. 2:Average REC, POPE accuracy and REG CIDEr for using different layers of
features extracted from various vision models (CLIP, DINOv2 and MAE), as input to
MLLMs. Shikra uses the 23rd layer features of CLIP and we reproduce its results with
fewer iterations (denoted as Shikra∗).
(a)REC acc for CLIP.
 (b)POPE acc for CLIP.
 (c)REC acc for DINO.
 (d)POPE acc for DINO.
Fig. 3:Average REC and POPE accuracy for merging different layers of features with
mutli-feature merging (MFM) strategies as input to MLLMs for visual backbones of
CLIP and DINOv2.
assigned to the i-th layer feature and all these weights are dynamically updated
and summed up to 1.
•LLN-Layerscale(all) : using a linear-layernorm module to align the feature
space between different layers’ features and then summed by Layerscale asz=
w1LLN( z1) +···+wNLLN( zN).
•Conv-Layerscale(all) : using a convolution and bn module to align the fea-
ture space between different layers’ features and then summed by Layerscale as
z=w1Conv( z1) +···+wNConv( zN).
Fig.3(a)and(b)showsthatsimplyaveragingallshallowanddeepfeaturesof
CLIP can de facto achieve a satisfactory accuracy and LLN-Layerscale strategy
further improves performance. With LLN-Layerscale as MFM module, the per-
formance of CLIP can be evidently improved on commonly-used vision-language
tasks as shown in Table 1.
DINOv2 as the Visual Branch of MLLMs. To leverage the rich fine-
grained visual information present in DINOv2, but not inherently aligned with
text, we employ a non-linear Multi-Layer Perceptron (MLP) module to align
the image features with the word embedding space. Fig. 2 demonstrates that
the deep-layer features of DINOv2 exhibit superior grounding abilities, as evi-
denced by higher REC accuracy, and display satisfactory understanding abili-
ties, as indicated by favorable POPE and REG results. Additionally, we explore
the efficacy of multi-level feature merging to enhance performance. In contrast

--- PAGE 7 ---
From CLIP to DINO: Visual Encoders Shout in MLLMs 7
Table 1: ComparisonofthevisualmodelusingCLIP,DINOv2withMulti-levelFeature
Merging (MFM) and COMM to incorporate visual embeddings of both models on VL
tasks. CLIP baseline use the 23rd layer features, which follows Shikra but with fewer
training iterations. DINOv2 baseline is w/o MLP module. MME CS and PS indicate
cognition and perception score, respectively.
Visual Model Avg REC Avg POPE COCO Flickr30k MME CS MME PS VQAv2 OK-VQA
CLIP 47.3 82.3 125.0 80.7 209.6 1107.8 68.8 44.2
DINOv2 54.8 78.3 118.0 68.9 261.8 930.5 63.1 41.9
CLIP w/ MFM 70.0 83.4 125.8 81.0 296.6 1164.4 69.5 44.7
DINOv2 w/ MFM 72.8 83.3 123.4 76.3 252.9 1086.8 68.0 42.1
COMM 72.8 83.6 127.3 81.9 360.4 1234.9 70.1 45.0
to CLIP, the merging of shallow features from DINOv2 leads to a significant
performance degradation. Specifically, in Fig. 3(c) and (d), it is evident that
Mean(all) performs notably worse than Mean(19-24) in terms of both REC
and POPE accuracy, indicating that the shallow representations lack sufficient
semantic information. Building upon the LLN-Layerscale approach, the incorpo-
ration of the MLP module for a more potent connection between the visual and
text spaces demonstrates a clear improvement in performance. Table 1 showcases
the substantial performance gains achieved by employing LLN-Layerscale-MLP
as Multi-Level Feature Merging (MFM) module across various vision language
tasks. Further detailed ablation studies on the MLP module are in Section 5.5.
MAE and DeiT as the Visual Branch of MLLMs. Fig. 2 shows that
MAE features achieve acceptable REC accuracy, but suffers large performance
droponPOPEandREGevaluation.ThisisbecauseMAEfeatureslacksufficient
semanticinformationforglobalorregionalunderstanding.Therefore,MAEisnot
suitable as the visual branch for MLLMs. DeiT performs even worse than MAE
(details in Section 5.5). We speculate that this is because supervised training is
too strong, which learns a specialized visual space that is difficult to align with
the word embedding space.
4 COMM: Combining CLIP and DINO With Multi-level
Feature Merging
Architecture Overview. In this section, we introduce the proposed COMM ,
that integrates CLIP and DINO with Multi-level features Merging to enhance
the visual capabilities of MLLMs. The overall framework is illustrated in Fig. 4,
COMM is incorporated into a vision-language instruction following model built
upon the recent advanced language and vision-language foundation models. Fol-
lowing the input instructions, our model takes vision and language as inputs to
generate text responses following the input instructions. Specifically, we adopt
thevisualencoderofCLIPandDINOv2(basedonViT-Large)withourproposed
fusion strategy as the visual branch, and Vicuna [10] (7B/13B) as language de-
coder. The visual encoder is downsampled with rate 14, meaning that an image

--- PAGE 8 ---
8 D. Jiang et al.
Input Image
What is the man in a white shirt doing?User Instruction…
…DINOv2CLIP
Text Embeddings……𝛼!𝛼"𝛼&
𝛽$Linear Projection𝛼&%!…Large Language Model
MLP𝛽!
Block 1Block 2Block M-1Block MLLNLLNLLN…LLNMulti-level Feature MergingBlock 1Block 2…Block 𝑗Block MLLNLLN…Multi-level Feature Merging…
Fig. 4:Overviewofour COMM .TheimageisinputtothevisionencoderofCLIPand
DINOv2, and the features from shallow and deep layers are incorporated by multi-level
feature merging. The features of DINOv2 are aligned with an MLP and concatenated
with features of CLIP, which are input to a linear layer. Then the fused features are
concatenated with text tokens as input to LLMs.
with resolution H×Wwill be represented by a sequence ofH
14×W
14tokens. The
fused token features are projected using a linear layer and then concatenated
with the instruction tokens as inputs to the language decoder, which is a generic
interface to unify various vision-language tasks as text generation task.
Specifically, denote the visual encoder of CLIP and DINOv2 (ViT Large
used) as f1andf2, respectively. Given an input image x, we extract the patch
token features output by all layers of CLIP as f1(x) = [v1
1, ..., vi
1, ..., v24
1], where
vi
1∈RN×D,Nisthenumberofpatchtokensand Distheembeddingdimension.
ThefeaturesoutputbythedeeplayersofDINOv2are f2(x) = [v19
2, ..., vi
2, ..., v24
2].
Thenweconcatenatethefeaturesoutputbythesetwomodelsas v= [v1
1, ..., v24
1, v19
2, ..., v24
2].
A linear-layernorm module is employed to align the feature space between dif-
ferent layers’ features and layerscale is used to merge multiple layer features as
v1=24X
i=1αi·Linear(LN( vi
1)), v2=24X
j=19βj·Linear(LN( vj
2))(1)
where αandβare the learnable scaling parameter. Then, we employ an MLP
layer to project the features of DINOv2 and concatenate the output features
with that of CLIP as v= [v1,MLP( v2)]. Then, a linear layer is employed to
match the dimension of visual features to that of text features as ˆ v= Linear( v).
Finally, fused visual features ˆ vare concatenated with text tokens as input to
LLMs.
5 Experiments
In this section, we conduct extensive evaluation on four kinds of vision-language
tasks to comprehensively evaluate the visual understanding ability of our model,

--- PAGE 9 ---
From CLIP to DINO: Visual Encoders Shout in MLLMs 9
Table 2: Results on standard referring expression comprehension (REC) task. Gen-
eralist VL models can perform various vision-language tasks. Specialist models are
designed specifically for localization tasks or generalist pretraining models that under-
gone finetuning. The results of Shikra, Qwen, Ferret and Griffon are from their papers.
Model type ModelRefCOCO RefCOCO+ RefCOCOg
val test-A test-B val test-A test-B val-u test-u
Generalist VL SOTAs
(w/o finetuning)OFA-L* 79.96 83.67 76.39 68.29 76.00 61.75 67.57 67.58
VisionLLM-H - 86.70 - - - - - -
Shikra-7B 87.01 90.61 80.24 81.60 87.36 72.12 82.27 82.19
Shikra-13B 87.83 91.11 81.81 82.89 87.79 74.41 82.64 83.16
Ferret-7B 87.49 91.35 82.45 80.78 87.38 73.14 83.93 84.76
Ferret-13B 89.48 92.41 84.36 82.81 88.14 75.17 85.83 86.34
Griffon-13B 88.00 92.10 81.90 81.50 88.20 73.30 82.90 84.30
Qwen-VL-7B 89.36 92.26 85.34 83.12 88.25 77.21 85.58 85.48
Qwen-VL-7B-Chat 88.55 92.27 84.51 82.82 88.59 76.79 85.96 86.32
COMM-7B (Ours) 91.73 94.06 88.85 87.21 91.74 81.39 87.32 88.33
Specialist SOTAs
(Specialist/Finetuned)G-DINO-L 90.56 93.19 88.24 82.75 88.95 75.92 86.13 87.02
UNINEXT-H 92.64 94.33 91.46 85.24 89.63 79.79 88.73 89.37
ONE-PEACE 92.58 94.18 89.26 88.77 92.21 83.23 89.22 89.27
namely, Referring Expression Comprehension, Referring Expression Generation,
Object Hallucination Benchmark, and Visual Question Answering and Image
Captioning.
Training Details. Similar to previous MLLM methods, COMM is trained
in two stages. In the first pretraining stage, we train the model on the reorga-
nized vision-language dataset as [8], including public VQA, Image Captioning
datset and several datasets containing positional annotation RefCOCO, visual
gemone [17] and Visual-7W [28]. The first pretraining stage is conducted for
100K steps. In the second instruction tuning stage, we set the sampling ratio
to 50% on LLaVA-Instruct-150K [23] and Shikra-RD [8]. Instead of 224 ×224
resolution currently used by existing MLLMs, we use 336 ×336 resolution to
reduce the information loss caused by image down-sampling and promote the
fine-grained perception ability. In both stages, we freeze the visual encoder and
tune all parameters inLLMs,alignment layer and multi-level feature fusion mod-
ule. We adopt AdamW [27] as the optimizer and cosine annealing scheduler [26]
as learning rate scheduler with an initial learning rate of 2e-5 and global batch
size of 64. All training runs on 8 NVIDIA A800 GPUs. It takes around 100h for
stage one training and 20h for stage two.
5.1 Referring Expression Comprehension
To evaluate the fine-grained understanding and positioning capability of our
model, we investigate the referring expression comprehension task on bench-
marksasRefCOCO[15],RefCOCO+[29]andRefCOCOg[29],wheremodelsare
asked to localize the object described with an expression. As shown in Table 2,
compared with generalist VL models and previous SOTA MLLMs, COMM

--- PAGE 10 ---
10 D. Jiang et al.
Table 3: Results on standard referring expression generation (REG) task in CIDEr
score. We reproduce the results of Shikra-7B using its officially released checkpoint.
SLR is a finetuned listener-speaker model with an added reward-based module (SLR).
ModelRefCOCO RefCOCO+ RefCOCOg
val test-A test-B val test-A test-B val-u test-u
SLR [48] - 69.7 132.3 - 49.4 70.9 59.2 -
SLR+Rerank [48] - 77.5 132.0 - 52.0 73.5 66.2 -
Shikra 75.61 44.26 104.83 56.42 40.98 68.25 62.71 65.58
Kosmos-2 - - - - - - 62.3 -
COMM (Ours) 93.35 54.95 131.13 70.00 52.27 79.05 79.22 77.96
Table 4: Object hallucination benchmark using POPE evaluation pipeline [21]. The
results of Shikra-7B are taken from its paper. Except for Shikra-7B, the other results
are obtained from [21].
Datasets COMM Shikra InstructBLIP MiniGPT-4 LLaVA MM-GPT mPLUG-Owl
Random 87.29 86.90 88.57 79.67 50.37 50.10 53.97
Popular 86.50 83.97 82.77 69.73 49.87 50.00 50.90
Adversarial 84.50 83.10 72.10 65.17 49.70 50.00 50.67
achieves significant performance gain on all benchmarks, i.e.,COMM -7B out-
performs Shikra-13B and Qwen-VL-7B-Chat by 4.87% and 3.10% accuracy on
average, respectively. With more powerful visual capabilities of our proposed
fusion model, we can evidently surpass recent SOTA MLLMs in a more efficient
way, e.g., using a smaller LLM than Shikra (7B vs. 13B) and less training data
than Qwen (3.6M vs. 1.4B). Besides, our generalist model even achieves com-
parable results with specialist SOTA methods, showing the superior grounding
ability of our MLLMs.
5.2 Referring Expression Generation
Moreover,weevaluatetheabilitytounderstandimageregionsorobjectsreferred
via inputting bounding boxes. Instead of referring image regions or objects via
detailed text descriptions, directly referring to image regions via its bounding
boxes is more effective and can reduce the ambiguity. The experiments are con-
ducted on the referring expression generation task with RefCOCO, RefCOCO+
and RefCOCOg, aiming to generate text descriptions of specific regions in the
bounding box. Table 3 shows that our model outperforms Shikra and Kosmos-2
by a considerable margin of 16.51 CIDEr and 16.92 CIDEr gain on RefCOCOg,
demonstrating the effectiveness of our model for fine-grained understanding. Be-
sides,COMM evenoutperformsfinetunedSLRonRefCOCO+andRefCOCOg.
5.3 Object Hallucination Benchmark
We compare our model against the baseline models on the hallucination evalu-
ation dataset recently introduced by POPE [21], which randomly selects 500

--- PAGE 11 ---
From CLIP to DINO: Visual Encoders Shout in MLLMs 11
Table 5: Results on visual question answering (VQA) and image captioning. For VQA,
we evaluate SOTA generalist models and our COMM onVQAv2 and OK-VQA follow-
ing the normalization rules. Shikra and LLaVA-1.5 [22] is based on the 13B variant.
For image captioning, we evaluate them on COCO and Flickr30k in CIDEr score. We
call Flamingo as FM for short.
Datasets COMM LLaVA-1.5 Qwen Shikra FM-80B BLIP-2 Unified-IO VPGTrans
VQAVQAv2val79.05 - - 75.33 - 65.2 - 65.2
VQAv2dev81.04 80.0 79.5 77.36 56.3 65.0 77.9 -
VQAv2std81.17 - - 77.51 - - - -
OK-VQA 59.18 - 58.6 47.16 50.6 45.9 54.0 45.0
CaptionFlickr30k 88.2 - 85.8 73.9 67.2 - - -
COCO 132.7 - - 117.5 84.3 - 122.3 -
images from COCO [6]. Table 4 shows that COMM surpasses recent pop-
ular MLLMs with 1.44% and 4.95% higher accuracy on average than Shikra
and InstrutBLIP, respectively. By enhancing the fine-grained visual capabilities,
COMM can effectively alleviate the object hallucination problem.
5.4 Visual Question Answering and Image Captioning
We evaluate COMM on conventional VL tasks of VQA and Image Captioning.
Specifically, image captioning requires the model to generate description for the
given image and VQA asks the model to generate answer for the given image-
question pair. For image captioning, we choose COCO [9] and Flickr30K [35]
as benchmarks and report the CIDEr score. For VQA task, we experiment on
VQAv2 [2] and OK-VQA [30]. As shown in Table 5, COMM achieves state-
of-the-art performance on image captioning task, i.e., 88.2 CIDEr score on
Flickr30K and 132.7 CIDEr score on COCO, even outperforms previous SOTA
models with much more parameters ( e.g., Shikra-13B with 13B parameters) or
much more training data ( e.g., Qwen with 1.4B data). For VQA task, our model
alsoshowssignificantadvantagescomparedtootherMLLMs.OnVQAv2val,dev
and std, our model achieves 79.05, 81.04 and 81.17 accuracy respectively, which
surpasses recent proposed Shikra with the same training data and procedure by
a large margin, demonstrating the effectiveness of merging visual embeddings
of DINOv2 and CLIP for enhancing visual capabilities. Besides, our COMM
model outperforms Qwen with 1.54 and 0.58 accuracy gain on VQAv2 dev and
OK-VQA respectively with less VQA training data, i.e., we use 0.6M and Qwen
with 3.6M. Training with more VQA data might further improve performance
and we leave it as future work.
5.5 Ablation Study
Ablation on the MLP of DINOv2. We conduct ablation study on the design
of the MLP module in DINOv2 for aligning visual and text embedding space. We
ablate on the number and the expanding ratio of MLP module. Table 6 shows
that increasing the number of MLP to 2 can evidently improve performance,

--- PAGE 12 ---
12 D. Jiang et al.
Table 6: Ablation study on the number and expanding ratio of MLP module. Exper-
iments are conducted on referring expression comprehension and object hallucination
benchmark on Random (R), Adversarial (A), and Popular (P).
Visual ModelRefCOCO+ RefCOCOg RefCOCO POPE
test-A test-B val test-u val-u test-A test-B val A/P/R
DINOv2 w/ MLP Ratio 4 75.3 59.3 67.0 73.0 71.8 84.4 74.1 79.6 80.3/84.2/85.5
DINOv2 w/ 2MLP Ratio 4 77.5 60.3 69.2 74.6 74.7 86.5 75.3 81.4 82.4/84.5/86.2
DINOv2 w/ 4MLP Ratio 4 53.7 34.4 45.3 49.0 48.8 65.4 48.0 57.9 79.2/82.9/84.6
DINOv2 w/ 8MLP Ratio 4 8.2 6.5 7.4 6.8 6.7 14.8 12.9 14.9 56.0/55.3/59.0
DINOv2 w/ MLP Ratio 8 77.4 59.9 69.773.7 73.3 85.7 74.1 80.9 81.5/ 85.8/86.7
DINOv2 w/ MLP Ratio 16 76.2 60.2 69.774.5 74.6 85.7 75.5 81.5 80.4/83.7/85.7
DINOv2 w/ Linear 61.8 48.8 55.1 64.1 62.9 76.5 67.0 71.9 75.6/79.3/83.7
Table 7: Comparison of the visual model using CLIP, DINOv2 with our multi-level
feature merging (MFM), MAE and DeiT. MAE-20 denotes using the features output
by the 20-th layer of MAE. DeiT-20 denotes using the features output by 20-th layer.
Visual ModelRefCOCO+ RefCOCOg RefCOCO POPE
test-A test-B val test-u val-u test-A test-B val A/P/R
CLIP w/ MFM 73.7 53.8 64.3 69.1 70.3 83.8 68.4 76.4 80.7/84.2/85.8
DINOv2 w/ MFM 75.3 59.3 67.0 73.0 71.8 84.4 74.1 79.6 80.3/84.2/85.5
MAE-20 64.7 49.4 56.8 63.7 62.8 77.9 68.6 73.6 66.8/71.1/76.7
MAE-22 65.9 50.0 58.5 64.2 63.2 79.3 69.8 74.9 68.0/71.2/77.5
DeiT-20 18.4 13.0 15.9 17.0 16.2 29.0 21.6 25.7 66.2/69.6/77.9
DeiT-22 25.3 15.4 19.4 22.6 21.8 36.9 25.3 32.0 67.9/71.6/78.7
demonstrate the effectiveness of using a more powerful network to align the
vision only model DINOv2 to the word embedding space. However, increasing
the number beyond 2 suffers the degraded performance. For the expanding ratio,
increasing to 8 can improve performance, while increasing to 16 does not achieve
significant performance gain. Moreover, we experiment with one linear layer,
whichsufferssevereperformancedegradation.Thus,non-linearMLPisnecessary
for aligning the features of vision-only DINOv2 to the word embedding space.
Ablation on the visual model of MAE and DeiT. As shown in Table 7,
MAE and DeiT suffers from evident performance degradation. For one thing,
the visual features of MAE lack sufficient semantic information for global or
regional understanding. For another, the supervised training of DeiT is so strong
that it learns specialized visual space, making it difficult to align with the word
embedding space.
5.6 Demonstrations
As shown in Fig. 7, our COMM model exhibits a multitude of promising capa-
bilitiesincludingvisualgrounding,fine-grainedregionunderstandingandrobust-
ness to object hallucination. The first example showcases our strong fine-grained

--- PAGE 13 ---
From CLIP to DINO: Visual Encoders Shout in MLLMs 13
perception ability, which identifies implicit strawberries in a blender. The second
example exhibits our strong visual grounding ability to successfully locates the
object of sugar. The third case demonstrates our robustness to object halluci-
nation. In contrast, Shikra fails on these challenging cases, showing the superior
capabilities of our model. We provide additional demonstrations of our COMM
modelinthissectiontodemonstrateamultitudeofpromisingcapabilitiesinclud-
ing visual grounding, fine-grained region understanding and robustness to object
hallucination. For instance, we showcase Referring Expression Comprehension in
Fig. 5 and Object Hallucination in Fig. 6.
Fig. 5:Referring Expression Comprehension (REC) using our COMM -7B. The task
intends to localize a target object in an image described by a referring expression.
Fig. 6:Object hallucination using our COMM -7B. This task aims to evaluate the
robustness to object hallucination, i.e., answer yes or no for the existence of questioned
object.
6 Conclusion
This paper presented an extensive investigation into the efficacy of different
visual models when employed as the visual branch in MLLMs. Through a sys-
tematic analysis, we highlight the significance of shallow layer features, which
capture low-level details that prove beneficial for grounding and positioning
tasks. Furthermore, we recognize the potential of the vision-only model DINOv2,
which leverages its inherent fine-grained pixel-level information for enhanced
fine-grained perception in MLLMs when combined with an MLP layer for align-
ment purposes. Motivated by our analysis, we introduce a fusion approach to

--- PAGE 14 ---
14 D. Jiang et al.
Fig. 7:Qualitative comparison between Shikra with its official checkpoint and our
COMM .
combine the visual features obtained from CLIP and DINOv2, thereby further
augmenting the visual capabilities and performance of MLLMs. Through quali-
tativeanalysisandextensivequantitativeexperiments,wedemonstratetheeffec-
tiveness of our proposed method, surpassing the performance of existing MLLM
models across diverse benchmark datasets. Looking ahead, we encourage future
research to explore the integration of more powerful vision models to enhance
the capabilities of visual branches in MLLMs. We believe that this avenue of
investigation holds the key to unlocking the potential of the next generation of
MLLMs.

--- PAGE 15 ---
From CLIP to DINO: Visual Encoders Shout in MLLMs 15
References
1. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,
Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model
for few-shot learning. NeurIPS (2022) 2, 3
2. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.:
Vqa: Visual question answering. In: Proceedings of the IEEE international confer-
ence on computer vision. pp. 2425–2433 (2015) 11
3. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou,
J.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv
preprint arXiv:2308.12966 (2023) 4
4. Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. arXiv:
Computer Vision and Pattern Recognition (2021) 4
5. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners. Advances in neural information processing systems 33, 1877–1901 (2020)
3
6. Caesar, H., Uijlings, J., Ferrari, V.: Coco-stuff: Thing and stuff classes in context.
In: CVPR (2018) 11
7. Cai, M., Liu, H., Mustikovela, S.K., Meyer, G.P., Chai, Y., Park, D., Lee, Y.J.:
Making large multimodal models understand arbitrary visual prompts. arXiv
preprint arXiv:2312.00784 (2023) 4
8. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleash-
ing multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195
(2023) 2, 4, 5, 9
9. Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L.:
Microsoft coco captions: Data collection and evaluation server. arXiv preprint
arXiv:1504.00325 (2015) 11
10. Chiang, W., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S.,
Zhuang, Y., Gonzalez, J., et al.: Vicuna: An open-source chatbot impressing gpt-4
with 90%* chatgpt quality, mar. 2023 1, 5, 7
11. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi,
S.: Instructblip: Towards general-purpose vision-language models with instruction
tuning. arXiv preprint arXiv:2305.06500 (2023) 2, 3, 4
12. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.V., Salakhutdinov, R.:
Transformer-xl: Attentive language models beyond a fixed-length context. arXiv
preprint arXiv:1901.02860 (2019) 3
13. Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter, B., Wahid,
A., Tompson, J., Vuong, Q., Yu, T., et al.: PaLM-E: An embodied multimodal
language model. arXiv preprint arXiv:2303.03378 (2023) 3
14. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders are
scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. pp. 16000–16009 (2022) 2, 4
15. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.: Referitgame: Referring to
objects in photographs of natural scenes. In: EMNLP (2014) 9
16. Koh, J.Y., Salakhutdinov, R., Fried, D.: Grounding language models to images for
multimodal inputs and outputs (2023) 3
17. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,
Kalantidis,Y.,Li,L.J.,Shamma,D.A.,etal.:VisualGenome:Connectinglanguage
and vision using crowdsourced dense image annotations 123, 32–73 (2017) 9

--- PAGE 16 ---
16 D. Jiang et al.
18. Li, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T.,
Poon, H., Gao, J.: Llava-med: Training a large language-and-vision assistant for
biomedicine in one day. arXiv preprint arXiv:2306.00890 (2023) 3
19. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 (2023) 2
20. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. arXiv:2301.12597
(2023) 3
21. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.R.: Evaluating object hal-
lucination in large vision-language models. arXiv preprint arXiv:2305.10355 (2023)
2, 5, 10
22. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning
(2023) 4, 11
23. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint
arXiv:2304.08485 (2023) 2, 4, 9
24. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv:2304.08485
(2023) 3
25. Liu, J., Wang, L., Yang, M.H.: Referring expression generation and comprehension
via attributes. In: Proceedings of the IEEE International Conference on Computer
Vision. pp. 4856–4864 (2017) 2
26. Loshchilov, I., Hutter, F.: SGDR: stochastic gradient descent with warm restarts.
In: 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net (2017),
https://openreview.net/forum?id=Skq89Scxx 9
27. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: 7th Inter-
national Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019. OpenReview.net (2019), https://openreview.net/forum?
id=Bkg6RiCqY7 9
28. Mani, A., Yoo, N., Hinthorn, W., Russakovsky, O.: Point and ask: Incorporating
pointing into visual question answering. arXiv preprint arXiv:2011.13681 (2020) 9
29. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A.L., Murphy, K.: Generation
and comprehension of unambiguous object descriptions. pp. 11–20 (2016) 9
30. Marino,K.,Rastegari,M.,Farhadi,A.,Mottaghi,R.:Ok-vqa:Avisualquestionan-
swering benchmark requiring external knowledge. In: Proceedings of the IEEE/cvf
conference on computer vision and pattern recognition. pp. 3195–3204 (2019) 11
31. OpenAI: Chatgpt: A language model for conversational ai. Tech. rep., OpenAI
(2023), https://www.openai.com/research/chatgpt 1
32. OpenAI: Gpt-4 technical report (2023) 1
33. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V.,
Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust
visual features without supervision. arXiv preprint arXiv:2304.07193 (2023) 2, 4
34. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-
2: Grounding multimodal large language models to the world. arXiv:2306.14824
(2023) 2, 3, 4, 5
35. Plummer,B.A.,Wang,L.,Cervantes,C.M.,Caicedo,J.C.,Hockenmaier,J.,Lazeb-
nik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer
image-to-sentence models. In: Proceedings of the IEEE international conference on
computer vision. pp. 2641–2649 (2015) 11

--- PAGE 17 ---
From CLIP to DINO: Visual Encoders Shout in MLLMs 17
36. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
natural language supervision. In: International conference on machine learning. pp.
8748–8763. PMLR (2021) 2, 4
37. Su, Y., Lan, T., Li, H., Xu, J., Wang, Y., Cai, D.: Pandagpt: One model to
instruction-follow them all. arXiv preprint arXiv:2305.16355 (2023) 3
38. Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: Eva-clip: Improved training tech-
niques for clip at scale. arXiv preprint arXiv:2303.15389 (2023) 2, 4
39. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P.,
Hashimoto, T.B.: Stanford alpaca: An instruction-following llama model (2023) 1
40. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H.: Training
data-efficient image transformers & distillation through attention. In: International
conference on machine learning. pp. 10347–10357. PMLR (2021) 2
41. Touvron, H., Cord, M., Jegou, H.: Deit iii: Revenge of the vit. arXiv preprint
arXiv:2204.07118 (2022) 4
42. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
foundation language models. arXiv:2302.13971 (2023) 1
43. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C.C.,
Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao,
C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kar-
das, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.S., Lachaux,
M.A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov,
T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Sal-
adi, K., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X.E., Tang,
B., Taylor, R., Williams, A., Kuan, J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan,
A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., Scialom, T.:
Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288 (2023) 1
44. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.A., Khashabi, D., Hajishirzi,
H.: Self-instruct: Aligning language model with self generated instructions. arXiv
preprint arXiv:2212.10560 (2022) 1
45. Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N., Dai,
A.M., Le, Q.V.: Finetuned language models are zero-shot learners. arXiv preprint
arXiv:2109.01652 (2021) 1
46. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P.,
Shi, Y., et al.: mplug-owl: Modularization empowers large language models with
multimodality. arXiv:2304.14178 (2023) 2, 3, 4
47. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F.,
Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. arXiv
preprint arXiv:2310.07704 (2023) 4
48. Yu,L.,Tan,H.,Bansal,M.,Berg,T.L.:Ajointspeaker-listener-reinforcermodelfor
referring expressions. In: Proceedings of the IEEE conference on computer vision
and pattern recognition. pp. 7282–7290 (2017) 10
49. Zellers, R., Bisk, Y., Farhadi, A., Choi, Y.: From recognition to cognition: Visual
commonsensereasoning.In:ProceedingsoftheIEEE/CVFconferenceoncomputer
vision and pattern recognition. pp. 6720–6731 (2019) 2
50. Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual lan-
guage model for video understanding. arXiv preprint arXiv:2306.02858 (2023) 3

--- PAGE 18 ---
18 D. Jiang et al.
51. Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K.,
Luo, P.: Gpt4roi: Instruction tuning large language model on region-of-interest.
arXiv:2307.03601 (2023) 2, 4
52. Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., Kong, T.: ibot: Image
bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832 (2021) 4
53. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-
language understanding with advanced large language models. arXiv:2304.10592
(2023) 2, 3, 4
