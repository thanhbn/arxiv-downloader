# 2207.03481.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2207.03481.pdf
# Kích thước tệp: 1963739 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Kỷ yếu Nghiên cứu Máy học 2022 NeurIPS 2021 Competition and Demonstration Track
Huấn luyện Transformers cùng nhau
Alexander Borzunov∗borzunov.alexander@gmail.com
Max Ryabinin∗mryabinin0@gmail.com
Đại học HSE, Yandex
Tim Dettmers∗dettmers@cs.washington.edu
Đại học Washington
Quentin Lhoest∗quentin@huggingface.co
Lucile Saulnier∗lucile@huggingface.co
Hugging Face
Michael Diskin michael.s.diskin@gmail.com
Đại học HSE, Yandex
Yacine Jernite yacine@huggingface.co
Thomas Wolf thomas@huggingface.co
Hugging Face
Tóm tắt
Cơ sở hạ tầng cần thiết để huấn luyện các mô hình tối tân đang trở nên quá đắt đỏ, khiến việc huấn luyện những mô hình như vậy chỉ có thể chi trả được bởi các tập đoàn và tổ chức lớn. Nghiên cứu gần đây đề xuất một số phương pháp để huấn luyện các mô hình này một cách hợp tác, tức là bằng cách tập hợp phần cứng từ nhiều bên độc lập và huấn luyện một mô hình chung qua Internet. Trong cuộc thể hiện này, chúng tôi đã huấn luyện hợp tác một transformer chuyển đổi văn bản thành hình ảnh tương tự như OpenAI DALL-E. Chúng tôi mời người xem tham gia vào quá trình huấn luyện đang diễn ra, hướng dẫn họ cách đóng góp sử dụng phần cứng hiện có. Chúng tôi giải thích cách giải quyết các thách thức kỹ thuật liên quan đến quá trình huấn luyện như vậy (giao tiếp chậm, bộ nhớ hạn chế, hiệu suất không đồng đều giữa các thiết bị và mối quan tâm về bảo mật) và thảo luận về cách người xem có thể thiết lập các cuộc huấn luyện hợp tác cho riêng mình. Cuối cùng, chúng tôi chỉ ra rằng mô hình thu được tạo ra hình ảnh có chất lượng hợp lý với một số lời nhắc.
Từ khóa: huấn luyện phân tán, tính toán tình nguyện, transformers, văn bản thành hình ảnh, hiệu quả bộ nhớ, hiệu quả giao tiếp, phần cứng không đồng nhất, bảo mật
1. Giới thiệu
Huấn luyện các mô hình học sâu tối tân ngày càng trở nên đòi hỏi tính toán nhiều hơn. Một ví dụ nổi tiếng của xu hướng này là transformers (Vaswani et al., 2017), một kiến trúc phổ biến được sử dụng rộng rãi trong NLP (Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020), xử lý giọng nói (Gulati et al., 2020; Li et al., 2019), và thị giác máy tính (Dosovitskiy et al., 2020; Touvron et al., 2021; Caron et al., 2021). Transformers có lợi từ việc có hàng tỷ tham số (Brown et al., 2020; Kaplan et al., 2020; Ott et al., 2018) và huấn luyện batch lớn (Popel and Bojar, 2018), điều này làm cho chúng phụ thuộc vào cơ sở hạ tầng huấn luyện quy mô lớn (Narayanan et al., 2021; Shoeybi et al., 2019; Lepikhin et al., 2020).
∗Đóng góp bằng nhau.
©2022 A. Borzunov, M. Ryabinin, T. Dettmers, Q. Lhoest, L. Saulnier, M. Diskin, Y. Jernite & T. Wolf.arXiv:2207.03481v1  [cs.LG]  7 Jul 2022

--- TRANG 2 ---
Borzunov Ryabinin Dettmers Lhoest Saulnier Diskin Jernite Wolf
Không may, loại cơ sở hạ tầng này có thể đắt đỏ một cách cấm đoán, dù người ta mua phần cứng hay thuê tài nguyên đám mây (Turner; Li, 2020). Kết quả là, hầu hết các nhà nghiên cứu đơn giản không thể đủ khả năng để tiến hành các thí nghiệm cần thiết để phát triển ý tưởng của họ, điều này cuối cùng làm chậm tiến bộ khoa học.
Để làm cho học sâu quy mô lớn trở nên dễ tiếp cận hơn, nghiên cứu gần đây đề xuất huấn luyện các mô hình này một cách hợp tác, tức là tập hợp phần cứng từ nhiều bên độc lập và huấn luyện một mô hình chung qua Internet (Pascutto and Linscott, 2019; Ryabinin and Gusev, 2020; Kijsipongse et al., 2018; Atre et al., 2021; Diskin et al., 2021). Nghiên cứu như vậy đề xuất các thuật toán phân tán tổng quát để huấn luyện trên nhiều thiết bị với khả năng tính toán và độ tin cậy không đồng đều. Tuy nhiên, để làm cho chúng thực tế, người ta phải vượt qua một số thách thức kỹ thuật, chẳng hạn như giao tiếp chậm, bộ nhớ hạn chế và mối quan tâm về bảo mật.
Trong cuộc thể hiện này, chúng tôi đã huấn luyện hợp tác một transformer chuyển đổi văn bản thành hình ảnh tương tự như DALL-E (Ramesh et al., 2021). Những đóng góp của chúng tôi như sau:
•Chúng tôi chỉnh sửa mô hình DALL-E, làm cho nó phù hợp để huấn luyện qua Internet bằng phương pháp từ Diskin et al. (2021) và thư viện hivemind (hivemind, 2020). Chúng tôi thiết lập cơ sở hạ tầng cho quá trình huấn luyện như vậy và công bố kết quả huấn luyện.
•Chúng tôi cung cấp một trang web giải thích cách tham gia vào quá trình huấn luyện đang diễn ra, giải quyết các thách thức liên quan đến các cuộc huấn luyện hợp tác (giao tiếp chậm, ngân sách bộ nhớ thấp, hỗ trợ thiết bị không đồng nhất), và thiết lập quá trình huấn luyện như vậy cho bản thân.
•Chúng tôi cung cấp một "máy tính" tương tác hiển thị bộ nhớ được tiêu thụ bởi các mô hình khác nhau trong trường hợp sử dụng các kỹ thuật hiệu quả bộ nhớ khác nhau. Ngoài ra, chúng tôi trình bày một hướng dẫn về thiết lập streaming dataset và nén mô hình sử dụng các thư viện datasets và bitsandbytes (Lhoest et al., 2021; Dettmers et al., 2021).
2. Nội dung Thể hiện
2.1. Trang web chính
Phần trung tâm của cuộc thể hiện của chúng tôi là một trang web nơi mọi người có thể khám phá các tài liệu thể hiện. Trang web mô tả động lực đằng sau các dự án huấn luyện hợp tác, phương pháp huấn luyện hiệu quả từ Diskin et al. (2021), và việc huấn luyện hợp tác đang diễn ra của phiên bản chỉnh sửa DALL-E của chúng tôi (xem Phần 3). Ở đây, chúng tôi cũng hiển thị một biểu đồ của mục tiêu huấn luyện và số lượng người tham gia hoạt động.
Tiếp theo, chúng tôi cung cấp hướng dẫn về cách tham gia vào quá trình huấn luyện bằng cách sử dụng các nhà cung cấp đám mây miễn phí hoặc GPU của riêng họ. Điều này bao gồm (1) tham gia một tổ chức Hugging Face cụ thể, nơi chúng tôi có thể xác thực người dùng và đo lường đóng góp của họ, và (2) chạy một notebook Jupyter (Kluyver et al., 2016) với mã huấn luyện. Ý định của chúng tôi là người dùng có thể khám phá môi trường huấn luyện hợp tác của chúng tôi thông qua sự tham gia tích cực trong khi cùng lúc đọc các giải thích chi tiết về cách nó hoạt động. Ở đây, chúng tôi cũng cung cấp liên kết đến bảng điều khiển tương tác hiển thị thống kê và bảng xếp hạng đóng góp và cung cấp thêm thông tin về quá trình huấn luyện, chẳng hạn như các checkpoint mô hình được tải lên Model Hub, notebooks để suy luận, và liên kết đến mã nguồn.
Sau đó, chúng tôi tiến hành thảo luận về các thách thức kỹ thuật của các cuộc huấn luyện hợp tác:
1. Xem https://training-transformers-together.github.io
2

--- TRANG 3 ---
Huấn luyện Transformers cùng nhau
•Hiệu quả giao tiếp. Hầu hết các thuật toán huấn luyện phân tán được thiết kế cho các mạng bên trong cụm HPC với băng thông 10-100 Gbit/s. Tuy nhiên, các kết nối Internet điển hình chậm hơn nhiều bậc (10-100 Mbit/s). Để làm cho việc huấn luyện qua Internet trở nên thực tế, người ta có thể giảm chi phí giao tiếp bằng cách sử dụng huấn luyện batch lớn (You et al., 2020), nén gradient (Dettmers, 2015; Lin et al., 2018; Vogels et al., 2019; Tang et al., 2021), chia sẻ tham số (Lan et al., 2020; Xue et al., 2021), và chồng chéo tính toán với giao tiếp (Ren et al., 2021).
•Hiệu suất thiết bị không đồng đều. Huấn luyện song song dữ liệu truyền thống chờ thiết bị chậm nhất ở mỗi batch. Diskin et al. (2021) cho phép các thiết bị xử lý số lượng mẫu khác nhau cho một batch, trong khi vẫn giữ các đảm bảo của huấn luyện đồng bộ.
•Hiệu quả bộ nhớ. Huấn luyện phân tán yêu cầu hoặc lưu trữ tất cả tham số và thống kê optimizer trên mỗi người tham gia, điều này là thách thức trong trường hợp phần cứng cấp thấp, hoặc sử dụng song song mô hình điều này giới thiệu một mức độ phức tạp khác. May mắn thay, tùy chọn đầu tiên thường khả thi nếu chúng ta giảm tiêu thụ bộ nhớ bằng optimizer 8-bit (Dettmers et al., 2021), bằng cách chuyển thống kê sang CPU, với gradient checkpointing hoặc chia sẻ tham số (Lan et al., 2020; Xue et al., 2021).
•Streaming dataset. Người tham gia thường không thể lưu trữ hoặc thậm chí tải xuống toàn bộ dataset, vì các dataset được sử dụng để pretrain transformers có thể chứa hàng trăm gigabyte dữ liệu. Để giải quyết điều đó, người ta có thể sử dụng các công cụ streaming dataset, chẳng hạn như thư viện datasets (Lhoest et al., 2021).
•Bảo mật. Quan trọng là, các người tham gia chỉ trao đổi tensor và không bao giờ gửi mã để được thực thi trên máy tính của nhau. Vì một người tham gia độc hại cũng có thể ảnh hưởng đến kết quả huấn luyện bằng cách gửi tensor sai, chúng ta nên xác thực người tham gia, như được mô tả trong Diskin et al. (2021), và/hoặc sử dụng các kỹ thuật tổng hợp gradient mạnh mẽ đối với các outlier (Karimireddy et al., 2020; Gorbunov et al., 2021).
Cuối cùng, chúng tôi cung cấp một công thức về cách kết hợp tất cả những điều đó và thiết lập một cuộc huấn luyện hợp tác mới sử dụng thư viện hivemind (hivemind, 2020).
2.2. Máy tính bộ nhớ
Trang web thể hiện bao gồm một "máy tính" tương tác hiển thị lợi ích của các kỹ thuật hiệu quả bộ nhớ khác nhau và sự kết hợp của chúng. Nó có thể tính toán tiêu thụ RAM và bộ nhớ GPU cho BERT (Devlin et al., 2019), T5 (Raffel et al., 2020), GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), GPT-J (Wang and Komatsuzaki, 2021), và DALL-E (Ramesh et al., 2021) trong trường hợp sử dụng optimizer 8-bit, chuyển thống kê optimizer sang CPU, sử dụng gradient checkpointing và chia sẻ tham số.
2.3. Hướng dẫn về các kỹ thuật hiệu quả bộ nhớ
Trang web thể hiện tham khảo đến một hướng dẫn về thiết lập streaming dataset với thư viện datasets (Lhoest et al., 2021) và nén mô hình với thư viện bitsandbytes (Dettmers et al., 2021). Mục tiêu của hướng dẫn là fine-tune mô hình GPT-2 Large (Radford et al., 2019) trên dataset C4 (Raffel et al., 2020) chỉ sử dụng GPU cấp thấp, điều này có thể thực hiện được với optimizer Adam 8-bit.
3

--- TRANG 4 ---
Borzunov Ryabinin Dettmers Lhoest Saulnier Diskin Jernite Wolf
3. Cuộc huấn luyện hợp tác
3.1. Mô hình
Đối với ví dụ thực tế của một cuộc huấn luyện hợp tác, chúng tôi chọn huấn luyện một transformer chuyển đổi văn bản thành hình ảnh tương tự như DALL-E (Ramesh et al., 2021), dựa trên mã từ Wang (2021). Cụ thể, chúng tôi sử dụng một transformer chỉ decoder với 1024 đơn vị ẩn và 64 lớp, mỗi lớp sử dụng 16 attention head với kích thước trạng thái mỗi head là 64 (≈ 1.1B tham số tổng cộng). Chúng tôi xen kẽ các attention mask như trong bài báo gốc, tức là lặp lại mask "hàng, cột, hàng, hàng" cho đến lớp cuối cùng, có mask tích chập.
Để cải thiện hiệu quả giao tiếp và bộ nhớ, chúng tôi liên kết trọng số của tất cả các nhóm lớp "hàng, cột, hàng, hàng" (Lan et al., 2020) và liên kết embedding đầu vào và đầu ra (Press and Wolf, 2016), vì vậy mô hình sử dụng ít hơn 8 lần tham số (nhưng cùng lượng tính toán).
Chúng tôi cũng sử dụng các lớp có thể đảo ngược (Brügger et al., 2019) để giảm sử dụng bộ nhớ và rotary embedding (Su et al., 2021) để cải thiện tính ổn định huấn luyện.
Chúng tôi thay thế dVAE bằng VQ-GAN (Esser et al., 2021), vì nó có lỗi tái tạo nhỏ hơn. Chúng tôi sử dụng checkpoint với f=8 và kích thước codebook 8192. Cuối cùng, chúng tôi sử dụng CLIP ViT/B-32 (Radford et al., 2021) để chọn 4 hình ảnh tốt nhất trong số 128 hình ảnh được tạo.
3.2. Dataset
Chúng tôi huấn luyện mô hình trên 100 triệu cặp hình ảnh-văn bản đầu tiên từ LAION-400M (Schuhmann et al., 2021). Chúng tôi bỏ qua 10% hình ảnh do caption ngắn, tỷ lệ khung hình cực đoan, và nhãn NSFW.
Trước khi huấn luyện, chúng tôi xử lý trước tất cả hình ảnh bằng VQGAN và tải lên các mã VQGAN và caption, cả hai đều được nén bằng Brotli (Alakuijala et al., 2018), lên Hugging Face Dataset Hub (Lhoest et al., 2021). Trong quá trình huấn luyện, chúng tôi stream các mã đã nén thay vì hình ảnh gốc, do đó tiêu thụ ít băng thông hơn 18 lần.
3.3. Quy trình huấn luyện
Chúng tôi tuân theo quy trình huấn luyện phân tán từ Diskin et al. (2021) và sử dụng optimizer LAMB 8-bit (You et al., 2020; Dettmers et al., 2021) được chuyển sang CPU. Chúng tôi sử dụng lịch huấn luyện tuyến tính với 31250 bước (10% đầu là warm-up) và tốc độ học tối đa là 2.5×10^-3. Trong khi trao đổi gradient và tham số, chúng tôi sử dụng lượng tử hóa 8-bit (Dettmers, 2015) cho tensor có ≥2^16 phần tử và độ chính xác 16-bit cho các tensor khác. Không giống như bài báo gốc, chúng tôi không sử dụng PowerSGD (Vogels et al., 2019).
3.4. Kết quả
Cuộc huấn luyện kéo dài 2.5 tháng và hoàn thành 80% lịch huấn luyện. Ngoài các tác giả, 37 tình nguyện viên đã đóng góp ít nhất 10 phút (xem Phụ lục A).
Trong quá trình suy luận, chúng tôi lưu ý rằng việc giới hạn sampling đến top 256 logit hoặc top logit có tổng xác suất p=0.75 cải thiện đáng kể chất lượng hình ảnh. Mô hình cuối cùng tạo ra hình ảnh thực tế cho một số lời nhắc nhưng thất bại trong việc vẽ hình dạng chính xác cho những lời nhắc khác, trong khi sử dụng phong cách hình ảnh, kết cấu và màu sắc phù hợp (xem Phụ lục B). Chúng tôi cho rằng điều này do mô hình của chúng tôi quá nhỏ để ghi nhớ toàn bộ sự đa dạng của hình ảnh trong LAION-400M. Tuy nhiên, mô hình có thể tổng quát hóa các khái niệm không có trong dataset.
4

--- TRANG 5 ---
Huấn luyện Transformers cùng nhau
Tài liệu tham khảo
Jyrki Alakuijala, Andrea Farruggia, Paolo Ferragina, Eugene Kliuchnikov, Robert Obryk,
Zoltan Szabadka, and Lode Vandevenne. Brotli: A general-purpose data compressor.
ACM Transactions on Information Systems (TOIS) , 37(1):1{30, 2018.
Medha Atre, Birendra Jha, and Ashwini Rao. Distributed deep learning using volunteer
computing-like paradigm, 2021.
Romain Beaumont. Easily compute CLIP embeddings and build a CLIP retrieval system
with them. https://github.com/rom1504/clip-retrieval , 2021.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020.
Robin Brügger, Christian F. Baumgartner, and Ender Konukoglu. A partially reversible
u-net for memory-ecient volumetric image segmentation. arXiv:1906.06148 , 2019.
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv e J egou, Julien Mairal, Piotr Bojanowski,
and Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv
preprint arXiv:2104.14294 , 2021.
Boris Dayma. DALL ·E Mega - Training Journal, Sample Pre-
dictions. https://wandb.ai/dalle-mini/dalle-mini/reports/
DALL-E-Mega-Training-Journal--VmlldzoxODMxMDI2#sample-predictions , 2022.
Tim Dettmers. 8-bit approximations for parallelism in deep learning. ICLR , 2015.
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-
wise quantization, 2021.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training
of deep bidirectional transformers for language understanding. In NAACL-HLT , 2019.
Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Anton Sinitsin, Dmitry
Popov, Dmitry V Pyrkin, Maxim Kashirin, Alexander Borzunov, Albert Villanova del
Moral, et al. Distributed deep learning in open collaborations. Advances in Neural
Information Processing Systems , 34:7879{7897, 2021.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain
Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929 , 2020.
Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution
image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 12873{12883, 2021.
Eduard Gorbunov, Alexander Borzunov, Michael Diskin, and Max Ryabinin. Secure dis-
tributed training at scale. arXiv preprint arXiv:2106.11257 , 2021.
5

--- TRANG 6 ---
Borzunov Ryabinin Dettmers Lhoest Saulnier Diskin Jernite Wolf
Anmol Gulati, Chung-Cheng Chiu, James Qin, Jiahui Yu, Niki Parmar, Ruoming Pang,
Shibo Wang, Wei Han, Yonghui Wu, Yu Zhang, and Zhengdong Zhang, editors. Con-
former: Convolution-augmented Transformer for Speech Recognition , 2020.
hivemind. Hivemind: a Library for Decentralized Deep Learning. https://github.com/
learning-at-home/hivemind , 2020.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jerey Wu, and Dario Amodei. Scaling laws for neural
language models, 2020.
Sai Praneeth Karimireddy, Lie He, and Martin Jaggi. Learning from history for byzantine
robust optimization. arXiv preprint arXiv:2012.10333v1 , 2020.
Ekasit Kijsipongse, Apivadee Piyatumrong, and Suriya U-ruekolan. A hybrid gpu cluster
and volunteer computing platform for scalable deep learning. The Journal of Supercom-
puting , 04 2018. doi: 10.1007/s11227-018-2375-9.
Thomas Kluyver, Benjamin Ragan-Kelley, Fernando P erez, Brian E Granger, Matthias Bus-
sonnier, Jonathan Frederic, Kyle Kelley, Jessica B Hamrick, Jason Grout, Sylvain Corlay,
et al. Jupyter Notebooks-a publishing format for reproducible computational work
ows. ,
volume 2016. 2016.
Zhen-Zhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
Radu Soricut. Albert: A lite bert for self-supervised learning of language representations.
InInternational Conference on Learning Representations , 2020.
Dmitry Lepikhin, H. Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Y. Huang, M. Krikun,
Noam Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation
and automatic sharding. ArXiv , abs/2006.16668, 2020.
Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick
von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall,
Joe Davison, Mario Sa sko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven
Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp
Schmid, Sylvain Gugger, Cl ement Delangue, Th eo Matussi ere, Lysandre Debut, Stas
Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran cois Lagunas, Alexan-
der Rush, and Thomas Wolf. Datasets: A community library for natural language
processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations , pages 175{184, Online and Punta Cana,
Dominican Republic, November 2021. Association for Computational Linguistics. URL
https://aclanthology.org/2021.emnlp-demo.21 .
Chuan Li. Demystifying gpt-3 language model: A technical overview, 2020. " https:
//lambdalabs.com/blog/demystifying-gpt-3 ".
N. Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with
transformer network. In AAAI , 2019.
6

--- TRANG 7 ---
Huấn luyện Transformers cùng nhau
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep Gradient Compres-
sion: Reducing the communication bandwidth for distributed training. In The Interna-
tional Conference on Learning Representations , 2018.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized
bert pretraining approach. ArXiv , abs/1907.11692, 2019.
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Pat-
wary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan
Catanzaro, et al. Ecient large-scale language model training on gpu clusters. arXiv
preprint arXiv:2104.04473 , 2021.
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine trans-
lation. In Proceedings of the Third Conference on Machine Translation: Research Papers ,
pages 1{9, Brussels, Belgium, October 2018. Association for Computational Linguistics.
doi: 10.18653/v1/W18-6301. URL https://www.aclweb.org/anthology/W18-6301 .
Gian-Carlo Pascutto and Gary Linscott. Leela chess zero, 2019. URL http://lczero.org/ .
M. Popel and Ondrej Bojar. Training tips for the transformer model. The Prague Bulletin
of Mathematical Linguistics , 110:43 { 70, 2018.
Or Press and Lior Wolf. Using the output embedding to improve language models. arXiv
preprint arXiv:1608.05859 , 2016.
Alec Radford, Je Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
Language models are unsupervised multitask learners. 2019.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervision. In International Conference
on Machine Learning , pages 8748{8763. PMLR, 2021.
Colin Rael, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, W. Li, and Peter J. Liu. Exploring the limits of transfer learning
with a unied text-to-text transformer. ArXiv , abs/1910.10683, 2020.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford,
Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International
Conference on Machine Learning , pages 8821{8831. PMLR, 2021.
Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan
Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-oload: Democratizing billion-scale
model training, 2021.
Max Ryabinin and Anton Gusev. Towards crowdsourced training of large neural networks
using decentralized mixture-of-experts. Advances in Neural Information Processing Sys-
tems, 33:3659{3672, 2020.
7

--- TRANG 8 ---
Borzunov Ryabinin Dettmers Lhoest Saulnier Diskin Jernite Wolf
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton
Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-
400m: Open dataset of clip-ltered 400 million image-text pairs. arXiv preprint
arXiv:2111.02114 , 2021.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and
Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using
gpu model parallelism. arXiv preprint arXiv:1909.08053 , 2019.
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced trans-
former with rotary position embedding. arXiv preprint arXiv:2104.09864 , 2021.
Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li,
Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 1-bit adam: Communication e-
cient large-scale training with adam's convergence speed. In International Conference on
Machine Learning , pages 10118{10129. PMLR, 2021.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles,
and Herv e J egou. Training data-ecient image transformers & distillation through at-
tention. In International Conference on Machine Learning , pages 10347{10357. PMLR,
2021.
Elliot Turner. Estimate of GPT-3 training cost based on public cloud GPU/TPU cost
models, from Elliot Turner's personal page (accessed on May 29, 2020).
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in
neural information processing systems , 30, 2017.
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi. Powersgd: Practical low-rank
gradient compression for distributed optimization. Advances in Neural Information Pro-
cessing Systems , 32, 2019.
Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Lan-
guage Model. https://github.com/kingoflolz/mesh-transformer-jax , May 2021.
Phil Wang. DALLE-pytorch. Implementation / replication of DALL-E, OpenAI's Text
to Image Transformer, in Pytorch. https://github.com/lucidrains/DALLE-pytorch ,
2021.
Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, and Yang You. Go wider instead
of deeper. arXiv preprint arXiv:2107.11817 , 2021.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli,
Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimiza-
tion for deep learning: Training bert in 76 minutes, 2020.
8

--- TRANG 9 ---
Huấn luyện Transformers cùng nhau
Phụ lục A. Những tình nguyện viên hàng đầu theo thời gian tính toán đóng góp
0 5 10 15 20 25
GPU-ngàybrickfrogExpl0dingCatnaclbitNikiTrickygenerallyintelligent-ainicobossAnnasflyyufelixjkintree
Hình 1: Tên người dùng Hugging Face của các tình nguyện viên đóng góp nhiều thời gian tính toán nhất.
Phụ lục B. Kết quả suy luận mô hình
(a) góc nhìn từ trên của bãi biển vào ban ngày
(b) hoàng hôn đẹp tại bãi biển với vỏ sò trên bờ
(c) váy hoa, size M
(d) ảnh của cầu cổng vàng san francisco
9

--- TRANG 10 ---
Borzunov Ryabinin Dettmers Lhoest Saulnier Diskin Jernite Wolf
(e) bản phác thảo than chì của nhà thờ gothic
(f) Pele và Maradona trong một trận đấu
(g) khối đỏ nhỏ ngồi trên khối xanh lá lớn
(h) nửa người nửa tháp Eiffel
(i) tượng nữ thần tự do cộng sản
Hình 2: Kết quả suy luận của mô hình cuối cùng (các lời nhắc được lấy từ Dayma (2022)):
(a){(c) Lời nhắc dẫn đến đầu ra thực tế.
(d){(f) Lời nhắc mà mô hình thất bại trong việc vẽ hình dạng đối tượng chính xác, nhưng
sử dụng phong cách hình ảnh, kết cấu và màu sắc phù hợp.
(g){(i) Lời nhắc mà mô hình có thể tổng quát hóa và vẽ các khái niệm
không có trong tập huấn luyện. Điều này được kiểm tra bằng cách kiểm tra hình ảnh tập huấn luyện
có embedding CLIP gần với embedding lời nhắc (Beaumont, 2021).
10
