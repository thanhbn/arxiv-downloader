# 2309.02654.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/hallucination/2309.02654.pdf
# File size: 597591 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ZERO-RESOURCE HALLUCINATION PREVENTION FOR
LARGE LANGUAGE MODELS
Junyu Luo
Pennsylvania State UniversityCao Xiao
GE HealthCare
Fenglong Ma
Pennsylvania State University
ABSTRACT
The prevalent use of large language models (LLMs) in various domains has drawn
attention to the issue of ”hallucination,” which refers to instances where LLMs
generate factually inaccurate or ungrounded information. Existing techniques for
hallucination detection in language assistants rely on intricate fuzzy, specific free-
language-based chain of thought (CoT) techniques or parameter-based methods
that suffer from interpretability issues. Additionally, the methods that identify
hallucinations post-generation could not prevent their occurrence and suffer from
inconsistent performance due to the influence of the instruction format and model
style. In this paper, we introduce a novel pre-detection self-evaluation technique,
referred to as S ELF-FAMILIARITY , which focuses on evaluating the model’s fa-
miliarity with the concepts present in the input instruction and withholding the
generation of response in case of unfamiliar concepts. This approach emulates the
human ability to refrain from responding to unfamiliar topics, thus reducing hal-
lucinations. We validate S ELF-FAMILIARITY across four different large language
models, demonstrating consistently superior performance compared to existing
techniques. Our findings propose a significant shift towards preemptive strate-
gies for hallucination mitigation in LLM assistants, promising improvements in
reliability, applicability, and interpretability1.
1 I NTRODUCTION
The widespread adoption of Large Language Models (LLMs) has generated significant interest in
their application across diverse use cases, e.g., healthcare and medicine. However, a major challenge
hindering their full potential is the issue of hallucination , where the models produce inaccurate or
fabricated information, leading to a substantial gap in their reliability and trustworthiness. Figure 6
shows an example to demonstrate the problem of hallucination when a user queries ChatGPT about
the drug “ Skytrofa ” that aims to treat short stature due to inadequate secretion of endogenous growth
hormone.2We can observe that ChatGPT generates inaccurate responses on essential factors of this
drug.
Recently, several approaches have been proposed to detect (and even correct) the hallucination re-
sponses generated by LLMs in open conversations, which can be roughly classified into two cate-
gories: The first category of methods hinges on a third-party knowledge base, combined with the
Chain-of-Thought (CoT) technique, to identify and rectify potential hallucination responses Gou
et al. (2023); Zhang et al. (2023a); Min et al. (2023); Zhang et al. (2023b); M ¨undler et al. (2023).
The other research direction focuses on designing parameter-based methods Manakul et al. (2023);
Min et al. (2023), which mainly utilize specific metrics such as perplexity to evaluate the correctness
of responses.
1code and data can be found in https://github.com/soap117/Self-evaluation
2https://www.accessdata.fda.gov/drugsatfda_docs/label/2021/761177lbl.
pdf
1arXiv:2309.02654v3  [cs.CL]  8 Oct 2023

--- PAGE 2 ---
User: I am writing a paper about the drug Skytrofa . 
GPT-3.5:That sounds interesting! Skytrofa (molgradex )isamedication
developed for the treatment ofautoimmune pulmonary alveolar
proteinosis (aPAP ).It'sarare lung disorder where atype ofprotein called
surfactant accumulates inthe airsacs ofthe lungs, leading tobreathing
difficulties .Figure 1: A hallucination example. Red color indicates the incorrect information.
All the aforementioned methods primarily focus on the post-detection of hallucination responses.
These approaches can merely ascertain if a response is a hallucination, lacking the capacity to in-
hibit the production of such responses in the future, thereby diminishing reliability. Moreover, the
performance of existing methods is deeply influenced by instruction and model styles, leading to
challenges in maintaining robustness across open conversation scenarios. This complexity hinders
the ability to establish a clear threshold for distinguishing hallucination responses. For instance,
binary queries often prompt brief response sequences, leading to metrics that diverge significantly
from those derived from more extensive response sequences. Therefore, a proactive, preventative
strategy for hallucination responses is essential for practical and efficient applications of artificial
intelligence (AI) language assistants.
Designing such an effective prevention method faces several challenges. Firstly, the proposed ap-
proach must navigate a zero-resource environment, precluding any reliance on external knowledge
gleaned from search engines. Neglecting this imperative compromises the method’s universality
and applicability, rendering it unsuitable for situations with budgetary constraints or contexts lack-
ing external access. Consequently, a profound comprehension of the language model’s intrinsic
knowledge becomes essential. Furthermore, the task of ensuring robustness is of paramount signif-
icance. The envisaged system must exhibit resilience against diverse instruction types, contextual
variations, and model styles. Given the open-ended and dynamic nature of human language, achiev-
ing consistent performance and unwavering resilience across a wide array of scenarios presents an
undeniably formidable challenge.
To tackle these challenges simultaneously, we propose a novel zero-resource, pre-detection method
named S ELF-FAMILIARITY , illustrated in Figure 2. This approach mimics human self-assessment
capabilities by refraining from discussing unfamiliar concepts, thereby reducing the risk of creating
hallucinated information. This method sets it apart from conventional post-detection techniques.
Initially, our method extracts and processes concept entities from the instruction during the Concept
Extraction stage. Following this, the Concept Guessing stage individually examines the extracted
concepts through prompt engineering to obtain each concept’s familiarity score. Lastly, during the
Aggregation stage, the familiarity scores from different concepts are combined to generate the final
instruction-level familiarity score.
Compared to existing methods, our algorithm presents the following advantages. Primarily, S ELF-
FAMILIARITY integrates the strengths of both CoT techniques and parameter-based methods. Like
the CoT methods, our algorithm can offer constructive responses by identifying concepts unfamiliar
to the model. Yet, our algorithm solely employs prompt engineering, eliminating the need for the
model to possess strong inference abilities and avoiding their shortcomings while combining their
advantages. Additionally, our algorithm remains unaffected by instruction style and type and is
proactive and preventative, thereby increasing its reliability and robustness. Finally, it does not
require any outside knowledge.
We assessed our method across four large language models using a newly proposed pre-detection
hallucinatory instruction classification dataset, Concept-7. Experimental results show that the pro-
posed S ELF-FAMILIARITY3consistently outperforms other methods across all models, demonstrat-
ing its huge application value.
3The data and code can be found in Supplementary Material .
2

--- PAGE 3 ---
German operation overload 
blitzkrieg attack0.3
Is part of the operation overload 0.4
blitzkrieg attack during world 
war II is Operation overload 0.5… …What is the purpose of Operation Overload in World War II?
Concept Extraction
 Operation Overload 
is part of the 
German blitzkrieg 
attack …Explain the 
“Operation Overload ” 
within one short 
paragraph.Concept Explanation
Concept InferenceLanguage 
Model
Masking
What  is related to 
“[MASK ] is part of 
the German 
blitzkrieg attack …”?
Selecting Max Scoreby Constrained 
Beam Searchby Greedy Search
“[MASK ] is part of the 
German blitzkrieg attack …”
… …Concept Frequency 
Score  Calculation
Weighted Averaging…
PreventionConcept GuessingConcept 
ExtractionAggregationOperation 
Overload
Language 
Model
< h?Yes Output
ResponseNoFamiliarity 
Score…Figure 2: Example procedure of the S ELF-FAMILIARITY .
2 R ELATED WORK
To the best of our knowledge, no existing work has been devoted to preventing hallucinated re-
sponses in open conversations by analyzing the instruction itself under the zero-resource setting.
Consequently, the contexts we address are distinct from those of previous studies.
2.1 H ALLUCINATION DETECTION AND CORRECTION METHODS
Previous studies in hallucination detection and correction mainly concentrated on conditional text
generation for specific tasks such as abstract summarization Maynez et al. (2020); Wang et al.
(2020a); Cao et al. (2021), image captioning Rohrbach et al. (2018); Biten et al. (2022), dialogue
generation Shuster et al. (2021), machine translation Zhou et al. (2020); Wang & Sennrich (2020),
and table-to-text generation Wang et al. (2020b; 2021). Since these works are highly task-specific,
they fail to tackle hallucination issues in open conversations.
For an open conversation setting, the methodologies are typically categorized into two groups based
on the employed strategies. The first group utilizes the Chain of Thoughts (CoT) or prompt program-
ming to evaluate and amend responses Lee et al. (2022); Gou et al. (2023); Zhang et al. (2023a); Min
et al. (2023); Peng et al. (2023); Huang et al. (2023); Xie et al. (2023); Yue et al. (2023). A notewor-
thy example is CRITIC Gou et al. (2023), wherein a CoT process is deployed with supplementary
inputs from an external search engine to enhance response quality. Certain works do not necessitate
external knowledge Zhang et al. (2023b); M ¨undler et al. (2023), often directly asking the model to
assess the output’s faithfulness. Nonetheless, such methods can be limited as they are engineered for
specific responses, and highly depend on the inner inference ability of the model. Another challenge
lies in the fact that the algorithm output is usually free text, which can make the actual classification
threshold ambiguous.
The second category of methods Manakul et al. (2023); Min et al. (2023) emphasizes using language
model parameters, such as token probability sequence, to determine the hallucination level. These
methods generally exhibit superior generalization capability and can provide precise output scores.
Self-check GPT Manakul et al. (2023) pioneers the use of parameter-based methods for open-ended
text generation. In Self-check GPT, the perplexity, sampling, and unconditional probability are used
together to estimate the hallucination level. However, this work only assesses biography-related
issues, and compared to CoT techniques, the model’s interpretability is significantly reduced.
3

--- PAGE 4 ---
2.2 H ALLUCINATION DETECTION DATASETS
Current datasets for hallucination detection in open conversations majorly focus on post-detection
scenarios. In these datasets Lin et al. (2021); Liu et al. (2022); Muhlgay et al. (2023); Li et al.
(2023); Manakul et al. (2023); Min et al. (2023); M ¨undler et al. (2023); Zhang et al. (2023b), the task
involves selecting the correct response or ascertaining whether responses are incorrect. However,
these datasets are subject to certain constraints. Primarily, the datasets often originate from a single
task like biography writing, without accounting for variations across different language models.
Each model can have distinct background knowledge, enabling one model to identify a hallucinated
response that another model might overlook. Furthermore, even if a model can accurately classify
a specific hallucinated response, it does not guarantee that the model will refrain from generating a
different hallucinated response in the future. Therefore, it is important to create a new dataset used
for validating the pre-detection setting.
3 M ETHODOLOGY
The aim of our algorithm is to evaluate if the target instruction PTis a potential hallucinatory
instruction by checking the familiarity of the language model with the concepts that exist in the PT
under the zero-resource setting. Our method, as depicted in Figure 2, comprises three major steps:
(1) Concept Extraction, (2) Concept Guessing, and (3) Aggregation. The details of each step will be
elucidated in the subsequent sections.
3.1 C ONCEPT EXTRACTION
To assess familiarity, it is necessary to first extract the concept entities from the free-text instruction,
otherwise, the score will be greatly influenced by the “ noise ,” i.e., the stylistic and formatting ele-
ments of the instruction that do not contribute to its understanding. For example, the transformation
of the question “Can sound travel in a vacuum?” into the statement “Sound can travel in a vacuum.
Please judge the statement. ” doesn’t alter the knowledge requisite, but it can substantially modify
the style of the subsequent response. In addition, if the instructions contain multiple concepts, it
will greatly increase the difficulty of the following prompt engineering. By isolating and individ-
ually evaluating these concepts, we can minimize such stylistic influence, thereby enhancing the
robustness of subsequent procedures. We achieve this extraction through the utilization of a Named
Entity Recognition (NER) model on the given instruction:
[c1,···, cN] =NER(PT). (1)
We consider these extracted entities [c1,···, cN]to be the key concepts of the instruction. Nstands
for the number of extracted concepts. However, NER models frequently produce extraneous noise
and fail to completely capture some concepts. Consequently, we introduce subsequent processing
steps to refine these extracted concepts, as outlined in the following sections.
Concept Grouping . The extracted concepts frequently exhibit a degree of incompleteness. For in-
stance, the term “2023 United States debt-ceiling crisis” could be inadvertently divided into [“2023”,
“United States”, “debt-ceiling crisis”]. To address this issue, we propose to sequentially fuse the ad-
jacent concepts. We sort the concepts based on their position in PTand attempt to fuse one concept
with the adjacent concept, if the newly combined concept is found within the original PT, we inte-
grate the original pair to create an extended, unified concept.
Concept Filtering . After merging the concepts, the subsequent step entails the exclusion of simple
concepts that should not be examined to improve efficiency. These include common concepts such
as “year” and “age”, which are generally well comprehended by the models. To address this, we
identify the top frequently used words in Wiktionary4, which we designate as “common concepts.”
Any concepts encompassed within these common words are subsequently eliminated.
3.2 C ONCEPT GUESSING
Our next task is to examine the familiarity of the Language Model (LM) with the extracted concepts
in a zero-resource setting. Undertaking this task in a zero-resource setting magnifies its complexity,
4https://en.wiktionary.org/wiki/Wiktionary:Main_Page
4

--- PAGE 5 ---
as it precludes reliance on external concept knowledge. As such, comparing the model’s under-
standing with an established gold definition to derive results becomes an impracticable approach.
Meanwhile, a direct query to the model regarding its familiarity with specific methods, such as CoT,
presents one possible solution. However, this approach demands a robust inference capability on the
part of the model and frequently results in ambiguous responses. Consequently, there is a need for a
more universally applicable and precise method.
In response to this need, we introduce a novel self-evaluation technique, denoted as Concept Guess-
ing. Our approach begins by prompting the model to generate an explanation for a given concept.
Subsequently, through prompt engineering, we ask the model to recreate the original concept based
on this explanation. Should the model generate the initial concept successfully, the probability score
of the response sequence can be interpreted as the connection strength between the concept and
the explanation, serving as a familiarity score. This entire process can be likened to a specialized
Charades or Pictionary game. If a language model can proficiently derive the original concept from
its generated explanation, this not only suggests the sufficiency of the explanation but also reflects
the model’s adeptness with the concept. Importantly, our method does not necessitate the acquisi-
tion of a gold definition of the concept. We outline the following steps to transform this conceptual
approach into a standardized metric:
3.2.1 C ONCEPT EXPLANATION
First, we use a standard explanation prompt PEin conjunction with the target concept cito query
the tested LM. This inquiry prompts the LM to generate an explanation for each concept through
greedy search (GreedySearch), which selects the next word with the highest probability to generate
the response. This process continues until it encounters the end of the sentence token or reaches the
maximum defined length, denoted as lF:
Ri=GreedySearch (LM(PE⊕ci)), (2)
where ⊕denotes inserting ciinto the pre-defined position of PE. In many scenarios, the original
concept may be directly incorporated into the generated explanation, as illustrated in Figure 2. Con-
sequently, the model could simply “copy” the original concept to “cheat”. To prevent this, we mask
the words of the ciwithin the Ri:
R∗
i=Mask (Ri). (3)
3.2.2 C ONCEPT INFERENCE
Given the masked explanation R∗
iand the concept inference prompt represented by PI, we can ask
the model to generate the original concept ci. However, the response from the model is produced
as open-ended free text, which poses a challenge when attempting to transform it into a standard
score. Consider an instance where the model might correctly generate the original concept, but in
a different format such as “Coca-Cola’s biggest competitor” rather than “Pepsi” . This discrepancy
complicates the determination of whether the original concept has been successfully regenerated.
To resolve this, we apply the constrained beam search (ConsBeamSearch) Anderson et al. (2017)
approach, instructing the model to seek responses incorporating the original concept through beam
search and providing the probability score of the responses in the meanwhile:
[< R1
i, s1
i>,···, < RTB
i, sTB
i>]
=ConsBeamSearch (LM(PI⊕R∗
i), ci),(4)
where each < Rj
i, sj
i>corresponds to a response Rj
iincluding the concept entity ci5, with sj
i
representing the corresponding response probability score. TBis the beam search size of the Cons-
BeamSearch algorithm. We set the stopping criteria to hit the end of the sentence token or reach the
maximum length lB. Beam search will return multiple results, however, the understanding of the
model is only related to the highest one. Therefore, we choose the highest response score, si, from
[s1
i,···, sTB
i]as the familiarity score of the concept ci:
si=Max(s1
i,···, sTB
i). (5)
5We consider the lowercase, uppercase, and capitalized forms in searching.
5

--- PAGE 6 ---
Table 1: Statics information of the Concept-7 dataset.
# of basic concepts 192
# of basic instructions 451
# of test concepts 180
# of real test concepts 106
# of fictional test concepts 74
# of test instructions 515
3.3 A GGREGATION
In many situations, the number of final extracted concepts can be greater than one. As a result, we
need to rank the importance of each concept and merge the familiarity scores of the concepts based
on their importance to generate a final, instruction-level outcome.
3.3.1 C ONCEPT FREQUENCY SCORE
In order to evaluate the importance of a concept, we propose a method for calculating a score based
on the frequency rank of words contained within that concept. Our expectation is that the concept
with more infrequent words will correspond to a lower score fi. To do so, we retrieve the frequency
rankpj
iof the j-th word of concept cifrom the Wiktionary. We set the index to the length of the
dictionary if the word is outside the dictionary or is capitalized. Given that word distribution tends to
follow a long-tail distribution, we employ the exponential function to transform the frequency rank
back to a frequency score and multiply them together to obtain the concept level frequency score:
fi=MiY
j=1e−pj
i
H. (6)
Here, Miis the number of words in concept ci. The term His introduced as a normalization factor
to guarantee that the resulting score resides within a reasonable range.
3.3.2 W EIGHTED AGGREGATION
Next, we average the familiarity scores based on their frequency scores through a weighted average.
This approach offers robustness in multi-entity situations when compared to simply selecting a sin-
gle score as the final value. To make the important parts contribute more than the less important tail
parts, we establish a geometrically decreasing weighting scheme with a ratio of1
r:
sf=PN
i=1(rθ(fi))−1siPN
i=1(rθ(fi))−1. (7)
Here, θ(fi)denotes the rank position of the fiwithin [f1,···, fN]when sorted by magnitude of
fi. We utilize the derived sfto represent the hallucination level of the instruction and terminate the
response process if the score falls below the predetermined threshold h.
4 E XPERIMENTS
In this section, we introduce the experimental settings and results. Due to space limitations, we
have included the implementation details , and concept only experiment results in supplementary
material.
4.1 D ATASET
Most existing datasets predominantly concentrate on the classification of hallucinatory responses.
To effectively evaluate our method, we introduce the Concept-7 dataset, which focuses on the clas-
sification of potential hallucinatory instructions. This dataset encompasses 192 basic concepts with
451 basic instructions and 180 test concepts with 515 test instructions sourced from seven expert
domains. A comprehensive overview of the dataset is displayed in Table 1.
6

--- PAGE 7 ---
Table 2: Hallucinatory instruction classification results on the four models.
MethodsVicuna-13b-v1.3 Falcon-7b-instruct mpt-7b-instruct Alpaca-7b
AUC ACC F1 PEA AUC ACC F1 PEA AUC ACC F1 PEA AUC ACC F1 PEA
Greedy-Perplexity 0.867 0.497 0.651 0.266 0.697 0.487 0.553 0.299 0.639 0.616 0.000 0.182 0.701 0.470 0.614 0.314
Greedy-AvgLogp 0.806 0.676 0.728 0.393 0.621 0.381 0.504 0.180 0.558 0.384 0.555 0.034 0.641 0.454 0.590 0.204
Greedy-MinLogp 0.760 0.604 0.693 0.398 0.631 0.670 0.000 0.227 0.621 0.616 0.000 0.170 0.591 0.460 0.596 0.121
Greedy-Significance 0.585 0.503 0.647 0.031 0.706 0.417 0.531 0.211 0.721 0.487 0.596 0.181 0.532 0.427 0.596 -0.091
Sample-BERTScore 0.872 0.779 0.807 0.640 0.666 0.416 0.526 0.180 0.724 0.497 0.602 0.299 0.650 0.517 0.634 0.196
Sample-SentenceScore 0.831 0.718 0.757 0.628 0.656 0.421 0.527 0.270 0.736 0.480 0.590 0.326 0.653 0.497 0.621 0.265
Forward-Inference 0.809 0.720 0.744 0.402 0.511 0.330 0.495 0.099 0.638 0.470 0.563 -0.081 0.520 0.515 0.432 0.070
SELF-FAMILIARITY 0.927 0.868 0.854 0.693 0.926 0.882 0.822 0.687 0.921 0.850 0.817 0.661 0.918 0.866 0.848 0.685
4.1.1 D ATASET CREATION
In the subsequent section, we delve into the specifics of dataset construction.
•Concept Selection. Initially, we select 192 fundamental concepts, guided by the popular pages on
Wikipedia6and domain diversity. These concepts are considered universally known for all language
models and serve as a benchmark for the classification of familiar concepts. For the test concepts, we
choose 106 concepts from seven expert domains: Medical, Finance, Music, Art, Legal, Physics, and
History. To maintain a balanced proportion between familiar and unfamiliar concepts, we fabricate
74 fictional concepts built upon real ones. As these fictional concepts don’t exist, they’re inherently
deemed unfamiliar to all language models. However, as the training sources vary for each language
model, it is necessary to annotate familiarity scores for the real concepts for each model under test.
•Familiarity Annotation. To assign familiarity scores, we propose a comparison between a crafted
gold explanation of concepts and each language model’s generated explanation. This familiarity as-
sessment is conducted automatically via GPT-4. We instruct GPT-4 to provide a familiarity score for
the model in relation to the concept on a scale of “1-9”, as described in supplementary material . A
threshold of “5” is chosen; concepts with a score exceeding “5” are deemed familiar, whereas those
scoring less are deemed unfamiliar. Concepts scoring exactly “5” are manually reviewed for final
labeling. Additionally, we employ Amazon Mechanical Turk7to gather pure human annotation re-
sults for the highest-performing model as a secondary evaluation method, ensuring the effectiveness
of the GPT-4 annotated results. For human-annotated results, we have three different annotators for
each concept and the final label is decided based on the average score. If the average score happens
to be “5”, a majority vote is applied to decide the label.
•Generating Instructions To replicate general conversational scenarios, we use a prompt (as shown
insupplementary material ) to generate three related questions for each concept via GPT-3.5. This
includes two open-ended questions and one yes-or-no question. We subsequently discard questions
that fail to mention the original concept to maintain strong relevance. Instructions comprising unfa-
miliar concepts are regarded as hallucinatory instructions.
4.2 B ASELINE METHODS
Considering that prior methodologies primarily focused on the detection of hallucinatory responses,
their settings could not be directly applied in this context. Nevertheless, we strived to adjust these
settings to establish the following baseline methods. Owing to space limitations, we only discuss the
core concepts here, leaving comprehensive details of each baseline method for the supplementary
material . It is important to note that as our focus lies on zero-resource prevention settings, we
excluded methods that necessitate external knowledge.
• Greedy-Perplexity: For each input, we utilize greedy search to generate a response and then cal-
culate the response’s perplexity. The negative perplexity score is considered the familiarity score,
similar to the approach in Manakul et al. (2023).
• Greedy-AvgLogp: For each input, we utilize greedy search to generate a response and then calcu-
late the response’s average log token probability score, similar to the approach in Manakul et al.
(2023).
6https://en.wikipedia.org/wiki/Wikipedia:Popular_pages
7https://www.mturk.com/
7

--- PAGE 8 ---
• Greedy-MinLogp: Similar to Greedy-AvgLogp, but take the minimal probability score, akin to
Manakul et al. (2023).
• Greedy-Significance: We first ask the model to generate an explanation response using greedy
search, after which the prompt’s core concepts are masked. We then regenerate the previous
explanation through force decoding and compare the KL divergence of the output probability se-
quences between the original and masked instruction. This divergence is treated as the familiarity
score.
• Sample-BERTScore: We sample TSresponses from the prompt and evaluate the
BERTScore Zhang et al. (2019) similarity between each pair of sentences. We then select the
sentence with the highest average similarity score to the remaining sentences as the central sen-
tence. The highest average similarity is treated as the familiarity score, following the method in
Manakul et al. (2023).
• Sample-SentenceScore: Like Sample-BERTScore, we sample TSresponses from the prompt and
compare their sentence embedding cosine similarity, as calculated by a Sentence-BERT Reimers
& Gurevych (2019) encoder.
• Forward-Inference: We directly inquire if the language model recognizes the domain-related con-
cepts within the instruction, resembling the CoT methods Zhang et al. (2023b); M ¨undler et al.
(2023). The likelihood of a “Yes” response sequence is considered the familiarity score. If the
model responds with “No” or anything other than “Yes”, the familiarity score is determined as one
minus the probability of the response sequence.
4.3 T ESTED LARGE LANGUAGE MODELS
In order to enable an in-depth comparison between different styles of instruction-aligned large lan-
guage models, we have selected four distinct models for evaluation: Vicuna-13b-v1.3 Zheng et al.
(2023), Falcon-7b-instruct Almazrouei et al. (2023), mpt-7b-instruct8, and Alpaca-7b Taori et al.
(2023). Given our approach’s requirement for constrained beam search generation control, models
that exclusively offer API access were not considered in this study.
4.4 E VALUATION METRICS
We adopt the area under the curve (AUC), accuracy (ACC), F-score (F1), and the Pearson Correla-
tion Coefficient (PEA) between the predicted and annotated familiarity scores.
4.5 R ESULTS
We begin our discussion by analyzing the hallucinatory instruction classification results presented in
Table 2. The table reveals two primary insights. Firstly, apart from our method, all other baseline ap-
proaches demonstrate notable performance inconsistency across the various models tested. Further-
more, the favored methods among different Language Models (LMs) significantly differ from one
another. As highlighted in the introduction, many current methods are easily influenced by model
styles. As a consequence, the performance of certain methods can vary based on the models in use.
This variability renders these methods less versatile across different settings. A notable example is
the Forward-Inference method. While it showcases commendable performance on Vicuna-13b-v1.3,
its efficacy diminishes with other models. This observation supports the hypothesis that techniques
like the CoT or prompt programming, though often capable of delivering high-quality results, are
heavily reliant on the model’s intrinsic CoT capacity. Since many LMs aren’t specifically fine-tuned
for this purpose, it restricts their widespread utility. On the other hand, Greedy-Perplexity performs
well across various models but fails to detect any hallucinated instructions on mpt-7b-instruct, re-
sulting in an F1 score of 0. This underscores the idea that even parameter-based methods are not
immune to robustness issues. Similarly, other methods exhibit this same challenge. In contrast to
existing approaches, our method not only delivers superior performance but also ensures consistent
results across various LMs. Additionally, the PEA correlation score demonstrates that the evalua-
tions produced by our algorithm align closely with the familiarity scores based on gold explanations
of concepts. These results underscore the robustness and reliability of our proposed approach.
8https://github.com/mosaicml/llm-foundry/
8

--- PAGE 9 ---
Table 3: Human annotated result evaluation.
MethodsVicuna-13b-v1.3
AUC ACC F1 PEA
Greedy-Perplexity 0.847 0.511 0.665 0.201
Greedy-AvgLogP 0.781 0.674 0.730 0.300
Greedy-MinLogp 0.733 0.493 0.661 0.264
Greedy-Significance 0.581 0.517 0.661 0.082
Sample-BERTScore 0.838 0.753 0.789 0.494
Sample-SentenceScore 0.792 0.701 0.746 0.543
Forward-Inference 0.766 0.691 0.762 0.290
SELF-FAMILIARITY 0.892 0.827 0.813 0.526
4.6 H UMAN EVALUATION RESULTS
In addition to evaluations based on GPT-4, we further utilized crowd-sourcing for the annotation of
the concept familiarity scores for Vicuna-13b-v1.3 and then assessed these human-annotated results.
The outcomes are delineated in Table 6. The outcome and ranking of different methods are similar
to the GPT-4 based results, proving the effectiveness of our auto-evaluation methodology utilizing
GPT-4. Finally, under the human-based evaluation, our approach still consistently exhibits superior
performance across all evaluated metrics.
4.7 A BLATION STUDY
Table 4: Entity processing ablation study
MethodsVicuna-13b-v1.3
AUC ACC F1
SELF-FAMILIARITY 0.927 0.868 0.854
W/O Grouping 0.918 0.856 0.841
W/O Filtration 0.923 0.856 0.841
W/O Ranking 0.926 0.86 0.845
Minimal Only 0.902 0.808 0.767
Most Infrequent Only 0.921 0.866 0.858
In this section, we present an ablation study in Table 4 to examine the contribution of the proposed
concept processing methods and score aggregation methods to the overall performance of our model.
We first examine the different concept processing strategies. The following notations represent dif-
ferent configurations of our algorithm: (1) W/O Grouping denotes processing without grouping the
extracted concepts. (2) W/O Filtering denotes processing without filtering out common concepts.
(3)W/O Ranking means that concepts are not ranked based on their frequency scores. Instead, the
position of the concepts within the instruction determines their order. Results indicate that excluding
any of these techniques leads to a drop in final performance. This underscores the efficacy of each
proposed processing strategy.
Next, we benchmark the efficacy of other instruction-level familiarity scores aggregation techniques
against our weighted averaging method: (1) Minimal Only selects the smallest concept familiarity
score as the final outcome. (2) Most Infrequent Only chooses the familiarity score of the concept
with the least frequency score fias the final result. It is evident from the table that our proposed
method demonstrates the best overall performance except for the F1 metric. This is attributable to
its capability of accounting for both the importance rank of the diverse concepts and the aggregate
performance. Conversely, the other two methods solely consider segments of the concept familiarity
scores.
4.8 C ASE STUDY
In this section, we perform a real-world case study on the medical domain to demonstrate the ap-
plicability of our algorithm in preventing hallucinations. The details can be found in Figure 3.
9

--- PAGE 10 ---
User: What is the usage of recently approved Beyfortus ?
Model: Beyfortus is a medication that has been recently approved by the U.S. 
Food and Drug Administration (FDA) for the treatment of type 2 diabetes .
Guard: Explain the " Beyfortus " within one short paragraph. User: What is the usage of recently approved Beyfortus ?
Model: Beyfortus is a hypothetical term that combines the words "beyond" and 
"fortune." …
Guard: " ... is a hypothetical term that combines the words "beyond" and 
"fortune." … " is related to what? 
Model: The term " beyondwealth " is related to the idea of achieving a higher level 
Beyfortus
Familiarity score: 0.681 
Guard: The model’s familiarity score of “ Beyfortus ” is below the standard value, 
search for additional information for “ Beyfortus ”.
Search Engine: “Today, the U.S. Food and Drug Administration approved 
Beyfortus (nirsevimab -alip) for the prevention of Respiratory Syncytial Virus 
(RSV) …”
Model: Beyfortus (nirsevimab -alip) is a recently approved medication used for 
the prevention of Respiratory Syncytial Virus (RSV) lower respiratory tract 
disease in neonates and infants born during or entering their first RSV season, …Self-Familiarity
Enhance with Search Engine
Providing BackgroundConcept Explanation
Concep tInference
Protective Intervention-------------------------------------------- Without Guard ----------------------------------------------
------------------------------------------------ With Guard -----------------------------------------------
Searching OnlineMaskingFigure 3: The Red color denotes misinformation, while Green signifies correct information. Blue is
the concept and Gold is the generated explanation. The tested model is Vicuna-13b-v1.3.
“Beyfortus”9is newly approved drug. We initially examine the response in the absence of our al-
gorithm in “Without Guard”, wherein the model unhesitatingly disseminates misinformation. This
kind of misinformation is challenging to detect unless one proactively seeks the underlying back-
ground information. Subsequently, in “With Guard”, our algorithm serves as a guard, assisting us
9https://www.fda.gov/news-events/press-announcements/
fda-approves-new-drug-prevent-rsv-babies-and-toddlers
10

--- PAGE 11 ---
in evaluating the model’s comprehension of “Beyfortus” utilizing the S ELF-FAMILIARITY . It is
evident that the model encounters considerable difficulty in generating a response associated with
“Beyfortus” based on the masked explanation. This is because the model lacks an intrinsic learned
connection between the concept and the fabricated explanation. Furthermore, we can readily address
these issues by introducing background knowledge of unfamiliar concepts. In the subsequent step,
the search engine is activated to retrieve information related to “Beyfortus” as background data, and
the model is then capable of rendering the correct response. These results suggest that our approach
is not only potent in prevention but can also offer great interpretability and serve as a valuable tool
in correcting hallucinated responses.
5 C ONCLUSION
We have introduced a cutting-edge pre-detection mechanism for potential hallucination instruction,
which we refer to as S ELF-FAMILIARITY . Our approach leverages Concept Guessing to assess the
model’s quality of concept explanation, thereby determining the model’s level of understanding.
SELF-FAMILIARITY consistently achieves state-of-the-art results in the pre-detection of halluci-
natory instruction across four distinct language models using only self-evaluation under the zero-
resource setting. Additionally, our method demonstrates superior interpretability by identifying the
particular concept that led to the hallucination. This unique feature enables the integration of our
method with post-detection and correction techniques, enhancing its versatility. In future work, we
plan to investigate how to evaluate the understanding of more granular sub-concepts to further refine
the precision of the current algorithm.
REFERENCES
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojo-
caru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, et al.
Falcon-40b: an open large language model with state-of-the-art performance, 2023.
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Guided open vocabulary
image captioning with constrained beam search. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing , pp. 936–945, 2017.
Ali Furkan Biten, Llu ´ıs G´omez, and Dimosthenis Karatzas. Let there be a clock on the beach:
Reducing object hallucination in image captioning. In Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision , pp. 1381–1390, 2022.
Meng Cao, Yue Dong, and Jackie Chi Kit Cheung. Hallucinated but factual! inspecting the factuality
of hallucinations in abstractive summarization. arXiv preprint arXiv:2109.09784 , 2021.
Bradley Efron and Robert J Tibshirani. An introduction to the bootstrap . CRC press, 1994.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint
arXiv:2305.11738 , 2023.
Kung-Hsiang Huang, Hou Pong Chan, and Heng Ji. Zero-shot faithful factual error correction. arXiv
preprint arXiv:2305.07982 , 2023.
Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan
Catanzaro. Factuality enhanced language models for open-ended text generation. Advances in
Neural Information Processing Systems , 35:34586–34599, 2022.
Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Halueval: A large-scale
hallucination evaluation benchmark for large language models. arXiv e-prints , pp. arXiv–2305,
2023.
Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human
falsehoods. arXiv preprint arXiv:2109.07958 , 2021.
11

--- PAGE 12 ---
Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, and William B Dolan.
A token-level reference-free hallucination detection benchmark for free-form text generation. In
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) , pp. 6723–6737, 2022.
Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallu-
cination detection for generative large language models. arXiv preprint arXiv:2303.08896 , 2023.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality
in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics , pp. 1906–1919, 2020.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual
precision in long form text generation. arXiv preprint arXiv:2305.14251 , 2023.
Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend,
Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. Generating benchmarks for factuality
evaluation of language models. arXiv preprint arXiv:2307.06908 , 2023.
Niels M ¨undler, Jingxuan He, Slobodan Jenko, and Martin Vechev. Self-contradictory hallucinations
of large language models: Evaluation, detection and mitigation. arXiv preprint arXiv:2305.15852 ,
2023.
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars
Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language
models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813 ,
2023.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-
networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing . Association for Computational Linguistics, 11 2019. URL https://arxiv.
org/abs/1908.10084 .
Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object
hallucination in image captioning. In Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing , pp. 4035–4045, 2018.
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation
reduces hallucination in conversation. In Findings of the Association for Computational Linguis-
tics: EMNLP 2021 , pp. 3784–3803, 2021.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,
Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-
following model. Stanford Center for Research on Foundation Models. https://crfm. stanford.
edu/2023/03/13/alpaca. html , 3(6):7, 2023.
Alex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the
factual consistency of summaries. arXiv preprint arXiv:2004.04228 , 2020a.
Chaojun Wang and Rico Sennrich. On exposure bias, hallucination and domain shift in neural ma-
chine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics , pp. 3544–3552, 2020.
Peng Wang, Junyang Lin, An Yang, Chang Zhou, Yichang Zhang, Jingren Zhou, and Hongxia Yang.
Sketch and refine: Towards faithful and informative table-to-text generation. In Findings of the
Association for Computational Linguistics: ACL-IJCNLP 2021 , pp. 4831–4843, 2021.
Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and Changyou Chen. Towards faithful neu-
ral table-to-text generation with content-matching constraints. arXiv preprint arXiv:2005.00969 ,
2020b.
12

--- PAGE 13 ---
Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn
sloth: Unraveling the behavior of large language models in knowledge conflicts. arXiv preprint
arXiv:2305.13300 , 2023.
Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of
attribution by large language models. arXiv preprint arXiv:2305.06311 , 2023.
Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. Mitigating language model
hallucination with interactive question-knowledge alignment. arXiv preprint arXiv:2305.13669 ,
2023a.
Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen,
Xixin Wu, Danny Fox, Helen Meng, and James Glass. Interpretable unified language checking.
arXiv preprint arXiv:2304.03728 , 2023b.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluat-
ing text generation with bert. arXiv preprint arXiv:1904.09675 , 2019.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. arXiv preprint arXiv:2306.05685 , 2023.
Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and
Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation.
arXiv preprint arXiv:2011.02593 , 2020.
13

--- PAGE 14 ---
A P ROMPTS
Using the background information provided, assess the participant's understanding of the target concept from their explanatio n. 
Please provide a rating between 1 and 9, with 9 being the best score (9 = Excellent, 5 = Average, 1 = Poor).
Concept:
{concept}
Background:
{background}
Participant's Explanation:
{response}
Scoring Guide:
Award a score of "7 -9 (Excellent)" if the participant's explanation of the concept is correct, with the inclusion of essential 
details demonstrating a comprehensive understanding of the concept. Minor inaccuracies can be ignored.
Award a score of "4 -6 (Average)" if the participant's explanation of the concept is partially correct but lacks essential detail s.
Award a score of "1 -3 (Poor)" if the participant's explanation of the concept is incorrect or includes obvious errors.
The term "essential details" refers to those details that only someone knowledgeable in this concept would suggest or that 
capture the core idea of the concept. These details cannot be deduced from a simple literal understanding.
Please note that the length of the explanation is not a determinant of the score. A concise yet accurate explanation with 
essential details is worthy of a high score.
Your Score (please provide a number between 1 and 9):
Figure 4: Scoring prompt used by GPT-4 and human annotators for annotating the familiarity score.
Please list two open -ended questions and one yes or no question that a user may ask about the following concept in the 
{domain} domain. Please number your questions as follows: 1., 2., and 3.
Note that the questions should be highly relevant to the concept and not involve the second concept.
Concept:
{concept}
Questions:
Figure 5: Question generation prompt used by GPT-3.5 for generating the instructions.
B I MPLEMENTATION DETAILS
For all baselines, we limit the maximum length of the response to 200 tokens. For sampling methods,
the number of samples, Ts, is set to 10. For our method, we set lFto 200 and lBto 15, and the search
beam size TBis set to 30. We use “...” as the mask token since not every language model has the
”[MASK]” token and we select the top 10,000 words in Wiktionary as the “common words”. The
decay ratio ris set to 2 and the normalization factor His set to 100. Finally, We use GPT-3.5
to extract critical concepts from the sampled questions as a zero-resource domain named entity
14

--- PAGE 15 ---
Analyze the provided question, identifying and extracting all named entities in {domain} domain. The output should include 
only the exact named entities from the text, without any additional details or modifications. Refrain from adding any further
explanatory information or details. Remember, standard nouns don't qualify as named entities.
Return them in the order of importance.
Question:
{question}
Named Entities (separated by a comma and return in the order of importance):Figure 6: Named entity extraction prompt used by GPT-3.5 for extracting entities from the general
instructions.
extractor. The prompt used for named entity extraction is displayed in supplementary material . The
following prompts are employed to perform Concept Explanation and conduct Concept Inference
for concept only and general-level experiments:
•Concept Explanation (concept only) PE:Explain the {concept }in the {domain }domain within one short
paragraph.
•Concept Explanation PE:Explain the “ {concept }” within one short paragraph.
•Concept Inference PI:“{masked explanation }” is related to what?
For all settings and methods, we employ 192 basic concepts-related instructions to estimate the
hallucination threshold hfor each method and model. We use bootstrap sampling Efron & Tibshirani
(1994) to sample the 95% threshold interval of the scores of basic concepts and use the midpoint of
the interval as the threshold. Finally, all methods were tested on a PyTorch-based system running
Ubuntu 20.04, equipped with 128 GB of memory and two NVIDIA A100 GPUs. To increase the
reproducibility, we fix the random seed to 42 to run the experiment and use the max probability to
prompt the LM to generate responses unless the sampling is required to avoid randomness.
C B ASELINES
C.1 G REEDY -PERPLEXITY
A model with higher perplexity typically indicates greater uncertainty in its responses, which can
suggest potential hallucinations. Initially, the response is generated using Greedy Search:
R=GreedySearch (LM(PT)) (8)
Subsequently, the perplexity score of response Ris determined:
s=−Perplexity (R) (9)
The inverse of this perplexity score is considered the faithfulness score.
C.2 G REEDY -AVGLOGP
Lower probability values generally signify that the model is more uncertain, hinting at a potential
hallucination. The response is generated using Greedy Search:
R=GreedySearch (LM(PT)) (10)
The probability sequence of Ris then determined:
P=ProSeq (R) (11)
The mean log value of Pis computed as:
s=Avg(log(P)) (12)
This average, s, is considered the faithfulness score.
15

--- PAGE 16 ---
C.3 G REEDY -MINLOGP
This method mirrors the Greedy-AvgLogp but employs the minimal score. The response generation
and probability sequence determination are consistent:
R=GreedySearch (LM(PT)) (13)
P=ProSeq (R) (14)
The minimum log value of Pis:
s=Min(log(P)) (15)
Here, sdenotes the faithfulness score.
C.4 G REEDY -SIGNIFICANCE
A small difference between the generated explanation and an explanation generated without condi-
tions can hint that the concept entities have minimal impact on the final outcome, signaling potential
hallucinations. We Use the same concept entity extraction technique as our method to extract the
concept entities. Then, the response is generated using Greedy Search:
R=GreedySearch (LM(PF⊕ei)) (16)
Simultaneously, the probability sequence of Ris determined:
P=ProSeq (R) (17)
To regenerate the prior output Runder an unconditional setting (where concepts are masked), the
force decode is applied, obtaining:
P∗=ForceDecode (LM(PF⊕[MASK] ), R) (18)
We then compute the difference between conditional probability sequence P∗and unconditional
probability sequence P:
s=KL-divergence (R∗, R) (19)
C.5 S AMPLE -BERTS CORE
If there is high similarity between generated responses, it indicates the sampled responses are con-
sistent. Conversely, diverse responses suggest the model’s uncertainty about the instruction. We first
sample Tsresponses based on the instruction:
[R1,···, RTs] =Sample (LM(PT)) (20)
Next, we compute the similarity between any two responses:
ssim
i,j=BERTScore (Ri, Rj) (21)
For each response, we calculate the aver1age similarity to all the responses:
ssim
i=1
mmX
j=1ssim
i,j (22)
Each response’s average similarity to all others is determined, with the highest average similarity
representing the final score:
s=max(ssim
1,···, ssim
Ts) (23)
C.6 S AMPLE -SENTENCE SCORE
Similar to the Sample-BERTScore, the only difference is that we use the sentence transformer to
obtain the sentence embedding of each sampled response Ri:
ri=SentenceTransformer (Ri) (24)
Next, we utilize the Cosine Similarity to obtain the similarity score between any two responses based
on the sentence embedding:
ssim
i,j=CosineSimilarity (ri,rj) (25)
The following parts are identical to Eq.(22)-(23).
16

--- PAGE 17 ---
C.7 D IRECT -INFERENCE
We directly ask if the model is familiar with the concepts through the following prompts:
•PD(concept only): ”Are you familiar with the {concept }in{domain }? Answer yes or no. ”
•PD:”Are you familiar with all the {domain }concepts in ” {instruction }”? Answer yes or no. ”
R=GreedySearch (LM(PD)) (26)
Based on the response, we calculate the hallucination score based on the overall probability score of
the response R:
s=Prob(R),”Yes” in R
1−Prob(R),else(27)
Note that keywords are considered in their lowercase, uppercase, and capitalized forms.
Table 5: Results of concept only on the four models.
MethodsVicuna-13b-v1.3 Falcon-7b-instruct mpt-7b-instruct Alpaca-7b
AUC ACC F1 Pearson AUC ACC F1 Pearson AUC ACC F1 Pearson AUC ACC F1 Pearson
Greedy-Perplexity 0.651 0.511 0.645 0.216 0.718 0.467 0.551 0.409 0.536 0.617 0.000 -0.053 0.653 0.494 0.609 0.162
Greedy-AvgLogp 0.784 0.478 0.647 0.467 0.747 0.550 0.571 0.439 0.652 0.383 0.554 0.234 0.673 0.433 0.605 0.280
Greedy-MinLogp 0.724 0.478 0.647 0.394 0.725 0.422 0.519 0.427 0.758 0.383 0.554 0.419 0.659 0.433 0.605 0.270
Greedy-Significance 0.593 0.556 0.675 0.042 0.687 0.583 0.611 0.033 0.597 0.528 0.585 -0.039 0.454 0.45 0.599 -0.260
Sample-BERTScore 0.920 0.678 0.748 0.718 0.863 0.600 0.617 0.711 0.802 0.528 0.615 0.393 0.876 0.722 0.742 0.700
Sample-SentenceScore 0.883 0.639 0.726 0.788 0.850 0.556 0.592 0.634 0.833 0.606 0.657 0.538 0.809 0.639 0.700 0.666
Forward-Inference 0.902 0.722 0.769 0.255 0.564 0.356 0.508 0.005 0.568 0.556 0.474 0.206 0.783 0.722 0.662 0.486
SELF-FAMILIARITY 0.966 0.928 0.921 0.844 0.968 0.911 0.864 0.772 0.935 0.911 0.892 0.631 0.905 0.883 0.863 0.680
D C ONCEPT ONLY EVALUATION
This scenario aims to evaluate the Concept Guessing stage solely. The results can be found in
Table 5 and the Table 6. The target concept is directly provided and the concept explanation prompt
PF“Explain the {concept }in the{domain }domain within one short paragraph. ” is uniformly
applied to the baselines as the input instruction, which is also the concept explanation prompt of
our Concept Guessing. In this scenario, our method consistently achieves state-of-the-art (SOTA)
performance. This demonstrates that even in less-noisy situations, our approach still surpasses the
baseline methods.
Table 6: Human annotated result evaluation on concept only.
MethodsVicuna-13b-v1.3
AUC ACC F1 Pearson
Greedy-Perplexity 0.681 0.528 0.661 0.189
Greedy-AvgLogp 0.765 0.494 0.662 0.361
Greedy-MinLogp 0.712 0.494 0.662 0.240
Greedy-Significance 0.600 0.572 0.691 0.071
Sample-BERTScore 0.884 0.683 0.755 0.496
Sample-SentenceScore 0.841 0.644 0.733 0.613
Forward-Inference 0.895 0.717 0.767 0.362
SELF-FAMILIARITY 0.920 0.889 0.881 0.678
17
