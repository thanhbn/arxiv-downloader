# 2305.14739.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/hallucination/2305.14739.pdf
# File size: 873576 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Trusting Your Evidence:
Hallucinate Less with Context-aware Decoding
Weijia Shi1∗Xiaochuang Han1∗
Mike Lewis2Yulia Tsvetkov1Luke Zettlemoyer1Scott Yih2
1University of Washington, Seattle, WA,2Meta AI
{swj0419, xhan77}@cs.washington.edu
Abstract
Language models (LMs) often struggle to pay
enough attention to the input context, and gen-
erate texts that are unfaithful or contain hallu-
cinations. To mitigate this issue, we present
context-aware decoding (CAD), which follows
a contrastive output distribution that amplifies
the difference between the output probabilities
when a model is used with and without con-
text. Our experiments show that CAD, without
additional training, significantly improves the
faithfulness of different LM families, including
OPT, GPT, LLaMA and FLAN-T5 for summa-
rization tasks (e.g., 14.3% gain for LLaMA in
factuality metrics). Furthermore, CAD is par-
ticularly effective in overriding a model’s prior
knowledge when it contradicts the provided
context, leading to substantial improvements in
tasks where resolving the knowledge conflict is
essential.
1 Introduction
Language models (LMs) are remarkably effective
in generating coherent and fluent continuations of
a prompt or document prefix. During generation,
they mostly rely on two sources of knowledge: (1)
prior knowledge , which is learned during pretrain-
ing and stored implicitly within the model parame-
ters; (2) context knowledge , which is passed as in-
puts in the prefix context (Chan et al., 2022). How-
ever, it remains an open question how a pretrained
LM, particularly a vanilla LM without task-specific
finetuning, balances these two knowledge sources
during generation.
Previous research shows that LMs can fail to pay
enough attention to new information introduced
in the context knowledge. This can lead to hallu-
cination in summarization (Maynez et al., 2020;
Pagnoni et al., 2021), where the generated sum-
maries include facts not present in the input doc-
ument. Insufficient attention to context is espe-
cially problematic when the context knowledge
∗Equal contribution. Order randomly determined.
Figure 1: An illustration of context-aware decoding.
contradicts with the prior knowledge (Longpre
et al., 2021; Zhou et al., 2023). For instance, when
LLaMA (Touvron et al., 2023) is presented with a
latest document “Argentina won the FIFA World
Cups in 1978,1986 and 2022 ...” in its context (Fig-
ure 1), it still predicts “Two” in response to the
question “How many World Cups have Argentina
won?”, due in part to the outdated training data.
In this work, we present a simple context- aware
decoding (CAD) method to encourage the LM to
attend to its context during generation. As shown
in Figure 1, CAD samples from a new output dis-
tribution, which amplifies the difference between
output probabilities with and without the context
document. This provides a new form of contrastive
decoding (Li et al., 2022), which effectively down-
weights the prior knowledge when more relevant
contextual information is provided. CAD can be
used with off-the-shelf pretrained language models
without any additional training.
Experimental results from summarization tasks
show that context-aware decoding significantly
enhances the generation faithfulness of various
vanilla LMs including OPT (Zhang et al., 2022),
GPT-Neo (Black et al., 2021), LLaMA (Touvron
et al., 2023) and instruction-finetuned LMs such as
FLAN (Chung et al., 2022). For instance, when ap-
plied to LLaMA-30B in CNN-DM, CAD leads to
substantial improvement in both ROUGE-L (21%)arXiv:2305.14739v1  [cs.CL]  24 May 2023

--- PAGE 2 ---
and summary factuality evaluation metrics (14.3%).
More notably, CAD is especially beneficial for
knowledge conflicting tasks, where the context
contains information contradictory to the model’s
prior knowledge. CAD brings a 2.9x improve-
ment to LLaMA-30B on a knowledge conflicts QA
dataset (Longpre et al., 2021). Furthermore, we
observe that this gain brought by CAD increases
as the model size grows in knowledge conflicts
tasks. These results demonstrate the potential of
CAD in mitigating hallucinations in text generation
and overriding prior knowledge with reliable and
trusted information.
2 Method
2.1 Background
Given a language model θ, an input query x, and
a context cthat contains some external knowledge
unfamiliar orin conflict to the model’s prior knowl-
edge, we ask our model θto generate a response
ygiven the the query and context. The response
can be directly sampled (autoregressively) from the
probability distribution conditioned on query xand
context c:
yt∼pθ(yt∣c,x,y<t)
∝exp logitθ(yt∣c,x,y<t)
However, in cases where the context ccontains
knowledge that is out-of-distribution with respect
toθ, we hypothesize that the model can struggle
to effectively attend to cand overly rely on the
prior knowledge encoded in θ. For instance, as
illustrated in Figure 1, when the context cstates
“Argentina won the FIFA World Cups in 1978, 1986
and 2022 ...”, it contradicts the LM’s outdated prior
knowledge that Argentina has won the World Cup
twice. The language model may still incorrectly
predict “Two” even when presented with the con-
textcand the query x.
2.2 Context-aware Decoding
To mitigate such issues, we factor out the prior
knowledge from the model’s original output dis-
tribution contrastively. Here, we model the prior
knowledge as pθ(yt∣x,y<t)and adjust the
model’s original output probability distribution us-
ing the pointwise mutual information (PMI) be-
tween the context cand the generation yt, condi-tioned on x,y<t. Formally, we have:
yt∼˜pθ(yt∣c,x,y<t)
∝pθ(yt∣c,x,y<t)(pθ(yt∣c,x,y<t)
pθ(yt∣x,y<t))α
where the output probability is a product-of-experts
of the original output probability and PMI weighted
byα. Essentially, outputs that become much more
likely when the context is included are preferred
(Figure 1).
This expression is not a valid probability distribu-
tion and needs to be normalized across all possible
values of yt. By rearranging the terms, we obtain
the final form:
yt∼softmax [(1+α)logitθ(yt∣c,x,y<t)
−αlogitθ(yt∣x,y<t)]
Larger αmeans more weight on our adjustment
(α=0reduces to regular decoding).1We refer
to this simple method as context-aware decoding.
From the adjusted output distribution ˜p, we can
apply various sampling strategies, such as nucleus
sampling (Holtzman et al., 2019).
Essentially, context-aware decoding is just a con-
trastive ensemble between the logits of pθ(yt∣
c,x,y<t)andpθ(yt∣x,y<t). A similar con-
trastive objective is universal in image genera-
tion, where classifier-free diffusion models (Ho
and Salimans, 2022) predict diffusion noise with
(1+α)ϵθ(x,c)−αϵθ(x), withcbeing a control to
the image. In text generation, Malkin et al. (2021)
propose coherence boosting with the same intu-
ition, with a focus on contrasting the full input and
a short premise-free input, promoting coherence
w.r.t. the long context. Instead of using a single
model θin this work, different models can also
be used in the distribution adjustments to demote
unwanted model behaviors or distill expert model’s
capability (Liu et al., 2021; Li et al., 2022).
3 Experimental Setup
We perform evaluation on tasks that require LMs to
read and reason over contexts and produce outputs
that are faithful to the contexts. Following prior
work (Zhang et al., 2023; Zhou et al., 2023), we
evaluate the models using prompting.
1If we identify an external knowledge cconditionally in-
dependent to the generation, pθ(yt∣c,x,y<t)=pθ(yt∣
x,y<t), even a non-zero αwould not have an impact to the
original output distribution.

--- PAGE 3 ---
3.1 Datasets and Metrics
Summarization We conduct summarization ex-
periments on two news datasets: CNN-DM (See
et al., 2017) and XSUM (Narayan et al., 2018). We
use ROUGE-L (Lin, 2004) to evaluate summariza-
tion quality. To measure the factual consistency
of summaries, we adopt BERT-Precision (Pagnoni
et al., 2021) as well as FactKB (Feng et al., 2023),
which has been demonstrated to achieve high cor-
relations with human judgment on the two summa-
rization datasets.
Knowledge Conflicts We evaluate performance
on two knowledge conflict datasets: MemoTrap
(Liu and Liu, 2023) and NQ-Swap (Longpre et al.,
2021). MemoTrap is created to investigate whether
language models could fall into memorization traps.
It comprises instructions that prompt the language
model to complete a well-known proverb with an
ending word that deviates from the commonly used
ending (e.g., Write a quote that ends in the word
“early”: Better late than ). NQ-Swap is based on
a QA dataset, natural questions (NQ) (Kwiatkowski
et al., 2019), where the objective is to answer ques-
tions based on a reliable gold document. To gener-
ate NQ-Swap, Longpre et al. (2021) first identify
questions in NQ with named entity answers, find
the supportive document for each question and then
replace the gold answer entity in the document with
a random entity. A faithful LM should generate the
replaced entity as the answer when given the ques-
tion and modified document. We also include the
original NQ dataset with the question and origi-
nal document for evaluation. We use Exact Match
(EM) as the evaluation metric for NQ-Swap, NQ
and MemoTrap.
In Table 1, we show illustrative examples of the
contexts we aim to upweight for the model and the
queries across different datasets. We hope LMs pay
more attention to the source document in XSUM
and NQ-Swap. On the other hand, we hope LMs
focus more on the instruction in MemoTrap.
3.2 Models and Baselines
We apply CAD to pretrained language models in-
cluding OPT (13B and 30B) (Zhang et al., 2022),
GPT-Neo (2.7B and 20B) (Black et al., 2021),
LLaMA (13B and 30B) (Touvron et al., 2023)
and instruction-finetuned language models such
as FLAN-T5 (XL 3B and XXL 11B) (Chung et al.,
2022).XSUM
c Article: Prison Link Cymru had 1,099 referrals in
2015-16 and said some ex-offenders were living
rough for up to a year before finding suitable accom-
modation ...
x Summarize the article in one sentence. Summary:
NQ-SWAP
c Tesla CEO Elon Musk is now in charge of Twitter ,
CNBC has learned ...
x Who is Twitter CEO now?
MemoTrap
c Write a quote that ends in the word "early":
x Better late than
Table 1: An illustation of the inputs to CAD applied to
each dataset. CAD upweights the context c(in red) by
sampling each token from softmax [(1+α)logitθ(yt∣
c,x,y<t)−αlogitθ(yt∣x,y<t)].
CAD introduces a hyperparameter αto control
the adjustment level. We set α=0.5for all mod-
els evaluated on the summarization datasets and
α=1for all models evaluated on the knowledge
conflict datasets. We observed that α=0.5gen-
erally yielded good results across all settings and
all datasets, but a slightly higher αis more effec-
tive in the knowledge conflict setting, where the
prior knowledge needs to be factored out more. We
investigate the effect of αin Section 4.2.
For the baselines, we use regular decoding
following prior work (Longpre et al., 2021;
Kwiatkowski et al., 2019) to use greedy decod-
ing for knowledge conflict tasks and top- psam-
pling with p=0.9 for summarization tasks (Holtz-
man et al., 2019). For CAD, we use the same
sampling strategies on top of the adjusted output
probability distribution.
4 Results
4.1 Main Results
Summarization Table 2 reports the results on
CNN-DM and XSUM. We observe that CAD
outperforms the standard decoding algorithm by
a large margin in all eight models across both
datasets. Specifically, when applied to LLAMA-
30B in CNN-DM, CAD leads to 21% increase in
ROUGE-L, 14.3% increase in factKB and 7.8%
increase in BERT-P. This result demonstrates that
CAD could effectively improve the quality and fac-
tuality of the generated summaries from a diverse
set of language models.

--- PAGE 4 ---
CNN-DM XSUM
Model Decoding ROUGE-L factKB BERT-P ROUGE-L factKB BERT-P
OPT13BRegular 22.0 77.8 86.5 16.4 47.2 85.2
CAD 27.4 84.1 90.8 18.2 64.9 87.5
30BRegular 22.2 81.7 87.0 17.4 38.2 86.1
CAD 28.4 87.0 90.2 19.5 45.6 89.3
GPT-Neo3BRegular 24.3 80.5 87.5 17.6 54.0 86.6
CAD 27.7 87.5 90.6 18.1 65.1 89.1
20BRegular 18.7 68.3 85.2 14.9 42.2 85.7
CAD 24.5 77.5 89.4 19.0 63.3 90.6
LLaMA13BRegular 27.1 80.2 89.5 19.0 53.5 87.8
CAD 32.6 90.8 93.0 21.1 73.4 91.7
30BRegular 25.8 76.8 88.5 18.7 47.7 87.1
CAD 31.8 87.8 92.2 22.0 66.4 90.3
FLAN3BRegular 25.5 90.2 91.6 18.8 31.9 88.2
CAD 26.1 93.9 92.1 19.5 35.9 88.8
11BRegular 25.4 90.4 91.4 19.4 29.8 88.3
CAD 27.1 93.1 92.2 20.0 35.0 88.8
Table 2: CAD consistently outperform the regular decoding method in terms of both summary quality metric
(ROUGE-L) and summary factuality (factKB and BERT-P). The best scores for each setting are boldfaced.
FLAN 3B and 11B refer to FLAN-T5 XL and FLAN-T5 XXL respectively.
Model Decoding Memo. NQ NQ-SWAP
OPT13BReg. 32.5 29.2 18.8
CAD 44.5 32.2 36.9
30BReg. 28.4 29.4 14.7
CAD 41.0 35.5 29.0
GPT.3BReg. 22.5 31.9 19.1
CAD 47.3 39.9 41.2
20BReg. 37.1 22.8 16.1
CAD 57.3 32.1 36.8
LLAMA13BReg. 23.8 22.3 11.7
CAD 57.1 33.6 36.7
30BReg. 25.8 23.8 9.6
CAD 50.6 34.0 37.7
FLAN3BReg. 69.2 81.8 71.4
CAD 72.2 80.3 73.3
11BReg. 82.0 85.5 73.0
CAD 88.7 82.5 77.1
Table 3: CAD outperforms the regular decoding method
(Reg.) in all settings except for FLAN-T5 on NQ.
Note that FLAN-T5 is trained on NQ dataset during
instruction-finetuning.
Knowledge Conflicts Our results for the knowl-
edge conflict datasets, NQ-SWAP and MemoTrap,
as well as the original NQ are detailed in Table 3.
CAD is significantly better than the regular decod-
ing in all settings, with the exception of a minor de-
crease observed for FLAN-T5 on the non-conflict
NQ dataset.2Despite this, CAD achieves substan-
2This slight decline can be attributed to the fact that this
particular NQ dataset is included in the instruction-finetuning
sets used by FLAN-T5, and hence, the model has been previ-
ously trained on it.tially better performance on the knowledge con-
flict datasets, e.g., CAD improve GPT-Neo 20B by
54.4% on Memotrap and by 128% on NQ-SWAP.
This substantial improvement suggests that context-
aware decoding is particularly beneficial for LMs
to adhere to the given context, in scenarios where
the model’s prior knowledge contradicts with the
context knowledge.
4.2 Analysis
Qualitative analyais We provide qualitative ex-
amples for XSUM and Memotrap in Table 4. In
XSUM, the regular decoding generates texts that
is not mentioned in the article, whereas CAD pro-
duces output exclusively based on the information
in the input article. For MemoTrap, the standard
decoding disregards the instruction and generates
the memorized ending, while CAD adheres to the
instruction within the given context and produces
the desired output.
CAD brings consistent improvement to LMs
with different sizes. In Tables 2 and 3, we show
that CAD could be used to enhance a diverse set of
LM families, including OPT, GPT-Neo, LLaMA,
and FLAN-T5. Here we further investigate whether
CAD is effective in improving language models of
different sizes. Specifically, we focus on OPT mod-
els across a range of sizes: 125M, 350M, 1.3B,
2.7B, 6.7B, 13B, 30B. As depicted in Figure 2,
we observe that the performance gain brought by

--- PAGE 5 ---
Figure 2: OPT models of varying sizes consistently benefit from CAD. The x-axis indicates the size of language
models and the y-axis is the performance.
XSUM
Article He passed away peacefully in hospital on Tues-
day after a short illness. Born in Tourmakeady,
County Mayo, he worked as a teacher before se-
curing a part in the premiere of the Brian Friel
play Translations in 1980. Lally became a house-
hold name in Ireland for his role as Miley Byrne
in the RTE soap opera Glenroe and later starred in
the BBC series Ballykissangel. He also appeared
in the Hollywood movie Alexander and provided
the voice for the Oscar-nominated, animated Irish
film, The Secret of Kells. As a fluent Irish speaker
and advocate of the language, Lally had roles in
several Irish language films ...
Regular Westminister actor Pat Lally died in hospital on
Tuesday night aged 82
CAD Actor Lally, best known for Glenroe and Bal-
lykissangel, has died in hospital on Tuesday
MemoTrap
Input Write a quote that ends in the word “early”. Better
late than
Regular never
CAD early
Table 4: Qualitative examples of contrast-aware decod-
ing. The nonfactual or inconsistent texts are highlighted
in yellow.
.
CAD stays consistent with different model sizes in
CNN-DM. In Memotrap and NQSWAP, this gain
increases as the model size grows, indicating that
larger LMs can have a greater tendency to rely on
their prior knowledge instead of reading the con-
texts, thereby benefiting more from CAD.
Effect of adjustment level αContext-aware
decoding introduces a hyperparameter α, which
serves to control the adjustment level of CAD (a
small αmakes the distribution closer to the original
next token distribution). We conduct experimentswith various values of αand present the results in
Figure 3. Across all three datasets, we find λ=0.5
consistently provide robust improvements over reg-
ular decoding. Further increasing the value of α
yields additional improvement in tasks involving
knowledge conflicts.
5 Related Work
Summarization Factuality Summarization mod-
els have shown a tendency to generate hallucinated
texts (Maynez et al., 2020; Pagnoni et al., 2021).
This has led to growing efforts to improve the fac-
tual consistency, including applying attentions to
fact triples extracted from source documents (Cao
et al., 2018; Zhu et al., 2021), optimizing sum-
marization models towards a factual consistency
metrics (Nan et al., 2021; Cao and Wang, 2021),
learning a post-editing error corrector (Dong et al.,
2020) and removing noisy training samples (Kang
and Hashimoto, 2020; Goyal and Durrett, 2021).
However, all these methods require additional fine-
tuning and are not directly suitable for zero-shot
and few-shot prompting scenarios.
Knowledge Conflicts When presented with an
updated document with conflicting knowledge,
we expect language models to generate responses
based on the provided contexts rather than relying
solely on outdated parametric knowledge. This set-
ting is especially valuable to retrieval-augmented
language models (Khandelwal et al., 2020; Shi
et al., 2023; Min et al., 2022; Yasunaga et al., 2023),
where documents retrieved from external databases
are used as additional input to provide LMs addi-
tional knowledge. However, simply adding docu-
ments does not always change the model predic-
tions, as current LMs often overlook the contexts

--- PAGE 6 ---
Figure 3: Effect of the adjustment level α. The y-axis is the performance and the x-axis is α.
and rely heavily on their prior parametric knowl-
edge (Longpre et al., 2021; Chen et al., 2022). Ex-
isting approaches for improving model’s faithful-
ness to the context, such as the prompting-based
method (Zhou et al., 2023), are limited in that
they could only apply to large-scale instruction-
finetuned LMs like OpenAI’s text-davinci-003. In
contrast, our work investigates a decoding strategy
to tackle this problem, which is applicable to any
LM.
Contrastive Decoding Methods Contrastive de-
coding methods have been extensively explored for
text generation. Coherence boosting (Malkin et al.,
2021) demotes a short context from a full context,
focusing on the longer-range context for coherence
and overall better generation quality. MMI-based
decoding (Li et al., 2015) uses a contrastive formu-
lation to improve output diversity in dialog genera-
tion. In this work, we adopt a same intuition and
focus on analyzing the knowledge conflict scenar-
ios where the faithfulness to the context is particu-
larly important but difficult for the regular decoding
methods. DExperts (Liu et al., 2021) demotes the
output distribution of an anti-expert (e.g., exposed
to toxic language) to help lead the generations free
from the unwanted attributes. Contrastive decoding
(Li et al., 2022) demotes an amateur model (e.g.,
models with a very small number of parameters)
to help distill the expert knowledge learned in the
larger, more competitive models. In general, con-
trastive decoding has shown to be a general way to
control model outputs, which we reinforce by con-
sidering the new case of factual consistency with
the textual context.6 Conclusion
Off-the-shelf language models may suffer from an
insufficient attention to the supplied context com-
pared to its learned prior knowledge, leading to
an unfaithful generation to the input context. We
present context-aware decoding, a simple inference-
time method that downweights an output probabil-
ity associated with the model’s prior knowledge to
promote models’ attention to the contextual infor-
mation. We experiment on two families of tasks
that require a strong attention to the context, sum-
marization and knowledge conflicts tasks. We show
that CAD provides more reliable and factual out-
puts across different language models of various
sizes.
References
Sid Black, Gao Leo, Phil Wang, Connor Leahy,
and Stella Biderman. 2021. GPT-Neo: Large
Scale Autoregressive Language Modeling with Mesh-
Tensorflow.
Shuyang Cao and Lu Wang. 2021. Cliff: Con-
trastive learning for improving faithfulness and fac-
tuality in abstractive summarization. arXiv preprint
arXiv:2109.09209 .
Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.
Faithful to the original: Fact aware neural abstractive
summarization. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence , volume 32.
Stephanie C. Y . Chan, Adam Santoro, Andrew Kyle
Lampinen, Jane X. Wang, Aaditya K Singh,
Pierre H. Richemond, Jay Mcclelland, and Felix
Hill. 2022. Data distributional properties drive emer-
gent in-context learning in transformers. ArXiv ,
abs/2205.05055.
Hung-Ting Chen, Michael Zhang, and Eunsol Choi.
2022. Rich knowledge sources bring complex knowl-

--- PAGE 7 ---
edge conflicts: Recalibrating models to reflect con-
flicting evidence. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing , pages 2292–2307, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .
Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie
Chi Kit Cheung, and Jingjing Liu. 2020. Multi-
fact correction in abstractive text summarization. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 9320–9331, Online. Association for Computa-
tional Linguistics.
Shangbin Feng, Vidhisha Balachandran, Yuyang Bai,
and Yulia Tsvetkov. 2023. Factkb: Generalizable fac-
tuality evaluation using language models enhanced
with factual knowledge.
Tanya Goyal and Greg Durrett. 2021. Annotating and
modeling fine-grained factuality in summarization.
InProceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 1449–1462, Online. Association for Computa-
tional Linguistics.
Jonathan Ho and Tim Salimans. 2022. Classifier-free
diffusion guidance. ArXiv , abs/2207.12598.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2019. The curious case of neural text de-
generation. In International Conference on Learning
Representations .
Daniel Kang and Tatsunori B. Hashimoto. 2020. Im-
proved natural language generation via loss trunca-
tion. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
718–731, Online. Association for Computational Lin-
guistics.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020. Generalization
through memorization: Nearest neighbor language
models. In International Conference on Learning
Representations .
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, et al. 2019. Natural questions: a benchmark
for question answering research. Transactions of the
Association for Computational Linguistics , 7:452–
466.
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and William B. Dolan. 2015. A diversity-promoting
objective function for neural conversation models.
ArXiv , abs/1510.03055.Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang,
Jason Eisner, Tatsunori Hashimoto, Luke Zettle-
moyer, and Mike Lewis. 2022. Contrastive decoding:
Open-ended text generation as optimization. ArXiv ,
abs/2210.15097.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out , pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Alisa Liu and Jiacheng Liu. 2023. The
memotrap dataset. https://github.
com/inverse-scaling/prize/blob/main/
data-release/README.md .
Alisa Liu, Maarten Sap, Ximing Lu, Swabha
Swayamdipta, Chandra Bhagavatula, Noah A. Smith,
and Yejin Choi. 2021. Dexperts: Decoding-time con-
trolled text generation with experts and anti-experts.
InAnnual Meeting of the Association for Computa-
tional Linguistics .
Shayne Longpre, Kartik Perisetla, Anthony Chen,
Nikhil Ramesh, Chris DuBois, and Sameer Singh.
2021. Entity-based knowledge conflicts in question
answering. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 7052–7063, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Nikolay Malkin, Zhen Wang, and Nebojsa Jojic. 2021.
Coherence boosting: When your pretrained language
model is not paying enough attention. In Annual
Meeting of the Association for Computational Lin-
guistics .
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 1906–1919, On-
line. Association for Computational Linguistics.
Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen
tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer.
2022. Nonparametric masked language modeling.
Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu,
Patrick Ng, Kathleen McKeown, Ramesh Nallapati,
Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, and
Bing Xiang. 2021. Improving factual consistency
of abstractive summarization via question answering.
InProceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
6881–6894, Online. Association for Computational
Linguistics.
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018

--- PAGE 8 ---
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 1797–1807, Brussels, Bel-
gium. Association for Computational Linguistics.
Artidoro Pagnoni, Vidhisha Balachandran, and Yulia
Tsvetkov. 2021. Understanding factuality in abstrac-
tive summarization with FRANK: A benchmark for
factuality metrics. In Proceedings of the 2021 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies , pages 4812–4829, Online. As-
sociation for Computational Linguistics.
Abigail See, Peter J Liu, and Christopher D Man-
ning. 2017. Get to the point: Summarization
with pointer-generator networks. arXiv preprint
arXiv:1704.04368 .
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon
Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and
Wen tau Yih. 2023. Replug: Retrieval-augmented
black-box language models.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. LLaMA: Open and ef-
ficient foundation language models. arXiv preprint
arXiv:2302.13971 .
Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi,
Rich James, Jure Leskovec, Percy Liang, Mike Lewis,
Luke Zettlemoyer, and Wen-tau Yih. 2023. Retrieval-
augmented multimodal language modeling. In Inter-
national Conference on Machine Learning (ICML) .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .
Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,
Kathleen McKeown, and Tatsunori B. Hashimoto.
2023. Benchmarking large language models for news
summarization.
Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and
Muhao Chen. 2023. Context-faithful prompting for
large language models. ArXiv , abs/2303.11315.
Chenguang Zhu, William Hinthorn, Ruochen Xu,
Qingkai Zeng, Michael Zeng, Xuedong Huang, and
Meng Jiang. 2021. Enhancing factual consistency
of abstractive summarization. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 718–733, Online.
Association for Computational Linguistics.
