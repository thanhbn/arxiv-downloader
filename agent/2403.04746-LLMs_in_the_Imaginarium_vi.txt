# 2403.04746.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/agent/2403.04746.pdf
# Kích thước tệp: 577504 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
LLM trong Imaginarium:
Học Công cụ thông qua Thử và Sai Mô phỏng
Boshi Wang♠*Hao Fang3Jason Eisner3Benjamin Van Durme3Yu Su3
♠Đại học Bang Ohio 3Microsoft Semantic Machines
wang.13930@osu.edu ,{hao.fang,jason.eisner,bevandur,yusu2}@microsoft.com
Tóm tắt
Các công cụ rất cần thiết cho các mô hình ngôn ngữ lớn
(LLM) để có được thông tin cập nhật và
thực hiện các hành động có hậu quả trong môi trường
bên ngoài. Các nghiên cứu hiện tại về LLM được
tăng cường công cụ chủ yếu tập trung vào phạm vi
bao phủ rộng của các công cụ và tính linh hoạt trong
việc thêm các công cụ mới. Tuy nhiên, một khía cạnh
quan trọng mà đáng ngạc nhiên là đã bị nghiên cứu
ít là đơn giản là LLM sử dụng các công cụ chính xác
như thế nào mà nó đã được huấn luyện. Chúng tôi
phát hiện rằng các LLM hiện tại, bao gồm GPT-4
và các LLM mã nguồn mở được tinh chỉnh đặc biệt
cho việc sử dụng công cụ, chỉ đạt tỷ lệ chính xác
trong khoảng 30% đến 60%, còn xa mức đáng tin
cậy trong thực tế. Chúng tôi đề xuất một phương pháp
lấy cảm hứng từ sinh học cho LLM được tăng cường
công cụ, thử và sai mô phỏng (STE), điều phối ba
cơ chế chính cho hành vi sử dụng công cụ thành công
trong hệ thống sinh học: thử và sai, tưởng tượng,
và trí nhớ. Cụ thể, STE tận dụng 'trí tưởng tượng'
của LLM để mô phỏng các tình huống hợp lý cho
việc sử dụng công cụ, sau đó LLM tương tác với
công cụ để học từ phản hồi thực thi. Cả trí nhớ
ngắn hạn và dài hạn đều được sử dụng để cải thiện
độ sâu và độ rộng của việc khám phá, tương ứng.
Các thí nghiệm toàn diện trên ToolBench cho thấy
STE cải thiện đáng kể việc học công cụ cho LLM
trong cả hai thiết lập học trong ngữ cảnh và tinh
chỉnh, mang lại mức tăng 46.7% cho Mistral-Instruct-7B
và cho phép nó vượt trội hơn GPT-4. Chúng tôi cũng
cho thấy việc học liên tục hiệu quả của các công cụ
thông qua chiến lược phát lại kinh nghiệm đơn giản.1
1 Giới thiệu
Các công cụ đóng vai trò thiết yếu trong việc mở rộng
con người (Gibson et al., 1993) và các động vật khác
(Shumaker et al., 2011) vượt ra ngoài giới hạn của
cơ thể vật lý để cảm nhận tốt hơn và tác động lên
môi trường của chúng. Gần đây có sự quan tâm
tăng vọt trong việc tăng cường các mô hình ngôn ngữ
lớn (LLM) với các công cụ để vượt qua giới hạn của
kiến thức tham số tĩnh và giao diện văn bản-vào-văn
bản-ra, trao quyền cho chúng để có được thông tin
cập nhật, gọi các bộ lý luận bên ngoài, và thực hiện
các hành động có hậu quả trong môi trường bên ngoài
(Schick et al., 2023; Mialon et al., 2023; Qin et al., 2023).

Các nghiên cứu hiện tại về LLM được tăng cường
công cụ chủ yếu nhằm tăng tính dễ dàng trong việc
thêm các công cụ mới, hoặc khả năng truy cập nhiều
công cụ (ví dụ, lên đến 16.000 API (Qin et al., 2024)).
Điều này được thực hiện thông qua một trong hai
phương pháp phổ biến: 1) Học trong ngữ cảnh (ICL),
khuyến khích các LLM đông lạnh với đặc tả API
và các ví dụ sử dụng công cụ (tức là các cặp hướng
dẫn-lời gọi API) (Lu et al., 2023; Song et al., 2023;
Shen et al., 2023; Liang et al., 2023b), hoặc 2) tinh
chỉnh với các ví dụ sử dụng công cụ được tổng hợp
bởi LLM (Schick et al., 2023; Patil et al., 2023;
Qin et al., 2024; Tang et al., 2023).

Trong khi phạm vi bao phủ và tính linh hoạt là quan
trọng đối với việc sử dụng công cụ, một khía cạnh
quan trọng mà có lẽ đáng ngạc nhiên là đã bị nghiên
cứu ít là đơn giản là LLM sử dụng các công cụ chính
xác như thế nào mà nó đã được huấn luyện. ICL
linh hoạt nhưng khó để đưa đến mức chính xác cấp
độ sản xuất. Tinh chỉnh có thể dẫn đến độ chính xác
tốt hơn bằng cách tích hợp số lượng lớn hơn các ví
dụ, nhưng các nghiên cứu hiện tại chủ yếu tập trung
vào việc tổng quát hóa đến các công cụ chưa thấy
thay vì tối ưu hóa khả năng của LLM trong việc sử
dụng các công cụ đã thấy trong quá trình huấn luyện
(Qin et al., 2024; Patil et al., 2023; Tang et al., 2023).
Trong khi đó, việc triển khai thực tế của LLM được
tăng cường công cụ đòi hỏi mức độ chính xác cao vì
chúng cho phép các hành động có hậu quả, ví dụ,
giao dịch tài chính hoặc các hoạt động ràng buộc
pháp lý khác. Việc sử dụng công cụ không chính xác
có thể dẫn đến kết quả không mong muốn hoặc có
hại và nhanh chóng làm suy giảm niềm tin của người
dùng.

Làm thế nào để thực sự thành thạo một công cụ?
Chúng tôi hướng đến các tiền lệ thành công trong
hệ thống sinh học như con người (Gibson et al., 1993),
vượn và loài quạ (Emery and Clayton, 2004). Học để

--- TRANG 2 ---
Tài liệu API
Tên: forecast_weather
Mô tả: Dự báo thời tiết trong những ngày sắp tới. Đầu vào của bạn nên là ...Trí nhớ Ngắn hạn
Trí nhớ Dài hạnTài liệu API Thử Thử ...×N
Truy vấn
Suy nghĩ
Lời gọi API
Quan sát
Phản hồi
Tự phản tỉnh
...Liệu có tuyết ở Iowa trong một tuần tới?
Tôi có thể làm gì ở thành phố New York hôm nay?Các Truy vấn Đã Khám phá Được Hoàn thành?
...a)b)
c)
Khám phá Khai thácCác Ví dụ
Sử dụng Công cụ lọc & diễn giải
Tinh chỉnh
đặc biệt cho
Công cụPool Minh họa
cho ICLThử
ThửThử
Tôi muốn xem bình minh ngày mai ở đây
ở Paris. Khi nào sẽ có?...Hình 1: Minh họa thử và sai mô phỏng. Trong giai đoạn khám phá, một LLM tương tác với công cụ và
dần dần thu thập kinh nghiệm sử dụng công cụ thông qua thử và sai. Cụ thể, a) trong mỗi thử nghiệm, LLM
tưởng tượng các tình huống hợp lý liên quan đến công cụ mục tiêu, lặp đi lặp lại tương tác với công cụ để hoàn
thành truy vấn của người dùng, và cuối cùng tự phản tỉnh về thử nghiệm; b) một trí nhớ ngắn hạn bao gồm các
quỹ đạo thử nghiệm gần đây khuyến khích học từ thành công và thất bại chi tiết và khám phá API sâu hơn;
c) một trí nhớ dài hạn về các kinh nghiệm thử và sai trong quá khứ ở mức độ thô duy trì việc học tiến bộ trong
khoảng thời gian dài. Trong giai đoạn khai thác, các kinh nghiệm khám phá được chưng cất thành một tập hợp
các ví dụ sử dụng công cụ cho ICL hoặc tinh chỉnh.

sử dụng một công cụ là một chức năng nhận thức khá
tiên tiến phụ thuộc vào nhiều chức năng nhận thức
khác. Trước hết, thử và sai là thiết yếu cho việc học
công cụ (Beck, 1973; Auersperg et al., 2011). Chúng
ta không thành thạo một công cụ chỉ bằng cách đọc
'hướng dẫn sử dụng'; thay vào đó, chúng ta khám phá
các cách khác nhau để sử dụng công cụ, quan sát kết
quả, và học từ cả thành công lẫn thất bại. Hơn nữa,
các động vật thông minh không chỉ thử và sai ngẫu
nhiên - chúng ta chủ động tưởng tượng hoặc mô phỏng
các tình huống hợp lý mà hiện tại không có sẵn để
cảm nhận cho việc khám phá (Emery and Clayton,
2004; Redish, 2016). Cuối cùng, trí nhớ, cả ngắn hạn
và dài hạn, là công cụ cho việc học tiến bộ và sử dụng
lặp lại các công cụ (Vaesen, 2012; Emery and Clayton,
2004; Clayton and Dickinson, 1998).

Để đạt được điều này, chúng tôi đề xuất thử và sai
mô phỏng (STE; được minh họa trong Hình 1), một
phương pháp lấy cảm hứng từ sinh học cho LLM được
tăng cường công cụ. Được cho một công cụ (ví dụ,
một API với đặc tả của nó), STE tận dụng LLM để
mô phỏng, hoặc 'tưởng tượng', các tình huống hợp
lý (tức là, hướng dẫn) cho việc sử dụng công cụ.
Sau đó nó lặp đi lặp lại tương tác với API để hoàn
thành tình huống bằng cách tổng hợp, thực thi, và
quan sát phản hồi từ các lời gọi API, và sau đó phản
tỉnh về thử nghiệm hiện tại (Shinn et al., 2023).
Chúng tôi thiết kế các cơ chế trí nhớ để cải thiện chất
lượng của các hướng dẫn mô phỏng. Một trí nhớ ngắn
hạn bao gồm các quỹ đạo thử và sai gần đây được

sử dụng để tạo điều kiện khám phá sâu hơn trong một
tập đơn, trong khi một trí nhớ dài hạn chứa các khám
phá và phản tỉnh trong quá khứ được chưng cất duy
trì việc học tiến bộ trong khoảng thời gian dài. Trong
giai đoạn khai thác, người ta có thể sử dụng các ví
dụ sử dụng công cụ từ các thử nghiệm đã khám phá
để tinh chỉnh LLM, hoặc đơn giản là làm ICL bằng
cách truy xuất từ những ví dụ đó.

Chúng tôi tiến hành các thí nghiệm toàn diện trên
các API từ ToolBench (Qin et al., 2024) và tóm tắt
các phát hiện chính như sau:

•Các LLM hiện tại còn xa mới đạt được hiệu suất
sử dụng công cụ đáng tin cậy: GPT-4 (OpenAI,
2023) đạt 60.8% độ chính xác, và ToolLLaMA-
v2 (Qin et al., 2024) được tinh chỉnh đặc biệt
cho việc sử dụng công cụ chỉ đạt 37.3%.

•STE tỏ ra cực kỳ hiệu quả trong việc tăng cường
LLM với các công cụ, trong cả hai thiết lập ICL
và tinh chỉnh. STE cải thiện khả năng sử dụng
công cụ của Mistral-Instruct-7B (Jiang et al.,
2023) lên 76.8% (tăng 46.7% tuyệt đối), làm
cho nó vượt trội hơn GPT-4 với ICL.

•Trong thực tế, các công cụ mới được thêm liên
tục. Tinh chỉnh (có hoặc không có STE) mang
lại thách thức quên thảm khốc, nơi việc học các
công cụ mới có thể khiến LLM mất khả năng
sử dụng công cụ hiện có hoặc khả năng ngôn
ngữ chung. Chúng tôi chứng minh rằng một chiến
lược phát lại kinh nghiệm đơn giản (Scialom
et al., 2022) có thể phần lớn giảm thiểu vấn đề,
cho phép

--- TRANG 3 ---
Tên API : forecast_weather
Mô tả : Dự báo thời tiết trong những ngày sắp tới. Đầu vào của bạn nên là ...
-----------------------------------------------------------------------------------------
Truy vấn: Liệu có tuyết ở Iowa trong một tuần tới?
Suy nghĩ: Người dùng muốn biết liệu có tuyết ở Iowa hay không trong...
API: forecast_weather | args : {"location": "Iowa", "days": " one week" } 
Quan sát : "Mã lỗi 422: thực thể không thể xử lý "
Suy nghĩ: Tôi đã mắc lỗi trong lời gọi API. Tôi quên sử dụng số cho trường "days". 
Hãy để tôi sửa lại.API: forecast_weather | args : {"location": "Iowa", "days": " 7"} 
Quan sát : "Dự báo thời tiết cho Iowa...tất cả thời tiết: có mây một phần, ..., 
tổng tuyết: 0.0 (cm) , ..., thời gian mặt trời mọc: 07:05 AM, ..."
Phản hồi cuối cùng : Dự báo thời tiết cho thấy sẽ không có tuyết ở Iowa trong một tuần tới.Tự phản tỉnh : Thành côngTập  1 - Thử 1
Hướng dẫn: Bây giờ dựa trên những gì bạn đã khám phá về API này, bạn có thể khám phá thêm một chút và củng cố hiểu biết của mình...-----------------------------------------------------------------------------------------Truy vấn: Tôi muốn xem bình minh ngày mai ở đây ở Paris. Khi nào sẽ có?
Suy nghĩ: Người dùng muốn...
API: forecast_weather | args : {"location": "Paris", "days": " 1"} Tập  1 - Thử MTên API : forecast_weather
Mô tả : Dự báo thời tiết trong những ngày sắp tới. Đầu vào của bạn nên là ...
-----------------------------------------------------------------------------------------Hướng dẫn: Dưới đây là các truy vấn bạn đã khám phá và liệu bạn có thành công giải quyết chúng với sự giúp đỡ của API:                   
                                Các Truy vấn Đã Khám phá                                                     Được Hoàn thành?
  Liệu có tuyết ở Iowa trong một tuần tới?                                           Tôi có thể làm gì ở thành phố New York hôm nay?  Tôi muốn xem bình minh ngày mai... khi nào sẽ có ?                
Dựa trên những điều này, hãy thử khám phá các truy vấn có thể giúp bạn hiểu API hơn; tránh tổng hợp các truy vấn quá gần với những cái hiện có.-----------------------------------------------------------------------------------------Truy vấn: ...
Suy nghĩ: ...
API: ...
Phản hồi cuối cùng : ...
Tự phản tỉnh : ...Tập  N - Thử 1
... ... ...
Trí nhớ Dài hạnTrí nhớ Ngắn hạn...

Hình 2: Khám phá với thử và sai mô phỏng, nổi bật các cơ chế trí nhớ. Mỗi tập bắt đầu
với đặc tả API (chỉ trong thử nghiệm đầu tiên), tiếp theo là một loạt các thử nghiệm được thêm động vào trí nhớ
ngắn hạn. Trí nhớ dài hạn được tải vào ngữ cảnh ở đầu mỗi thử nghiệm để cho phép LLM
tiến bộ tưởng tượng các tình huống mới, và sau đó được giảm tải (bỏ qua trong hình).

mô hình học liên tục các công cụ mới trong khi
bảo tồn các kỹ năng đã có trước đó.

2 Thử và Sai Mô phỏng
Chúng tôi giới thiệu thử và sai mô phỏng đề xuất của
chúng tôi (STE) cho việc học công cụ. STE bao gồm
một giai đoạn khám phá và một giai đoạn khai thác,
được thảo luận tiếp theo.

2.1 Khám phá
Trong giai đoạn khám phá, đối với mỗi API mới,
LLM tương tác với API trong một ngân sách để thu
thập càng nhiều thông tin càng tốt về API. Giai đoạn
khám phá bao gồm một loạt các thử nghiệm (Hình 1)
giống như việc học tiến bộ của con người về một công
cụ. Trong mỗi thử nghiệm, có điều kiện trên mô tả
API, LLM 1) tưởng tượng một truy vấn người dùng
hợp lý liên quan đến API; 2) cố gắng hoàn thành truy
vấn bằng cách tương tác với API; 3) phản tỉnh về
thử nghiệm để tạo điều kiện cho việc khám phá tiếp
theo. Ba thành phần thiết kế cốt lõi được tích hợp
với các thử nghiệm để tăng cường tính hợp lệ, toàn
diện và đa dạng của việc khám phá, được giới thiệu
tiếp theo.

Tinh chỉnh lặp đi lặp lại với phản hồi thực thi. Để
cải thiện tính hợp lệ của việc khám phá, chúng tôi
sử dụng một chiến lược tương tự như ý tưởng của
Chen et al. (2024); Qin et al. (2024); Madaan et al.
(2023); Shinn et al. (2023) nơi LLM học từ phản hồi
thực thi để tinh chỉnh các đầu ra của chính nó (Hình
2, góc trên bên trái). Cụ thể, chúng tôi áp dụng định
dạng ReAct (Yao et al., 2023) nơi trong mỗi bước,

LLM đầu tiên diễn đạt suy nghĩ nội tại của nó, sau
đó thực hiện một hành động (lời gọi API) và quan
sát phản hồi thực thi tương ứng, và sau đó lặp lại
quá trình suy nghĩ→hành động→quan sát cho đến
khi mô hình quyết định rằng lời gọi API đã trả về
thông tin đầy đủ hoặc một số lần gọi tối đa được
xác định trước. Trong giai đoạn này, LLM học từ
môi trường thực thi để sửa chữa các lỗi cú pháp và
ngữ nghĩa của chính nó trong các lời gọi API, thu
thập kinh nghiệm sử dụng công cụ như các quỹ đạo
thử-và-sai chi tiết. Sau đó, mô hình phản hồi truy
vấn của người dùng và tự phản tỉnh về việc liệu truy
vấn đã khám phá có được hoàn thành thành công
hay không.

Trí nhớ ngắn hạn. Một triển khai trực tiếp của việc
khám phá nơi mỗi thử nghiệm được tiến hành trong
một tập riêng biệt chỉ cho phép khám phá nông về
API. Chúng tôi tăng cường LLM với một trí nhớ
ngắn hạn bao gồm các quỹ đạo khám phá của các
thử nghiệm gần đây, nơi LLM được hướng dẫn tiến
hành các thử nghiệm tiếp theo có điều kiện trên trí
nhớ (Hình 2, bên trái). Mỗi tập bắt đầu với một trí
nhớ ngắn hạn mới, nơi các thử nghiệm mới được
tiến hành được thêm động vào trí nhớ cho một số
thử nghiệm nhất định. Điều này cho phép mô hình
học từ các thành công và thất bại chi tiết gần đây
(ví dụ, lỗi cú pháp/ngữ nghĩa), và cũng khám phá
API sâu hơn trong các thử nghiệm sắp tới dựa trên
các quan sát trước đó của nó về API (ví dụ, các
chức năng không mong đợi).

--- TRANG 4 ---
Trí nhớ dài hạn. Chỉ một số lượng nhỏ các thử nghiệm
có thể được lưu trữ trong trí nhớ ngắn hạn vì các
quỹ đạo chi tiết nhanh chóng tiêu thụ khả năng ngữ
cảnh của LLM. Chúng tôi tăng cường LLM với một
trí nhớ dài hạn lưu trữ các kinh nghiệm thử-và-sai
được chưng cất từ các tập trong quá khứ, để hỗ trợ
việc học tiến bộ trong khoảng thời gian dài. Cụ thể,
trí nhớ dài hạn ghi lại các truy vấn đã khám phá
trong quá khứ và liệu chúng có được đánh giá là
hoàn thành thành công hay không (Hình 2, bên phải).
Nó chỉ được tải vào ngữ cảnh ở đầu mỗi thử nghiệm
mới, nơi mô hình được hướng dẫn tưởng tượng các
tình huống xa cách với những cái đã khám phá trước
đây để cải thiện thu thập thông tin. Bằng cách này,
trí nhớ dài hạn phục vụ như một kho tích lũy các
thành công và thất bại trong quá khứ, cho phép LLM
liên tục mở rộng việc khám phá để tạo ra tiến bộ
qua các tập khác nhau.

2.2 Khai thác
Trong giai đoạn khai thác, các thử nghiệm thu được
từ giai đoạn khám phá được sử dụng để tăng cường
khả năng sử dụng công cụ của LLM thông qua tinh
chỉnh hoặc học trong ngữ cảnh (ICL). Đối với mỗi
thử nghiệm, chúng tôi trích xuất truy vấn người dùng
đã tổng hợp, lời gọi API cuối cùng của LLM và kết
quả thực thi của nó, và phản hồi cuối cùng từ quỹ
đạo thử nghiệm. Sau đó, chúng tôi thực hiện lọc
bằng cách sử dụng GPT-4 để đánh giá tính hợp lệ
của mỗi ví dụ, và sau đó diễn giải các ví dụ hợp lệ
cho mỗi API mới thành xấp xỉ cùng một lượng
(Phụ lục E), điều này duy trì sự cân bằng qua các
API khác nhau và thêm các biến thể ngôn ngữ vào
các ví dụ sử dụng công cụ đã tổng hợp.

Đối với tinh chỉnh, chúng tôi sử dụng mục tiêu mô
hình hóa ngôn ngữ tiêu chuẩn nơi tổn thất được tính
toán chỉ cho phần sử dụng công cụ/tạo phản hồi,
và không bao gồm tài liệu API trong ngữ cảnh. Đối
với ICL, các ví dụ đã tổng hợp được sử dụng như
kho minh họa từ đó các ví dụ trong ngữ cảnh được
truy xuất và thêm vào tài liệu API trong ngữ cảnh
của LLM. Chúng tôi sử dụng chiến lược lựa chọn
minh họa láng giềng gần nhất động nơi các ví dụ
gần nhất về mặt ngữ nghĩa với truy vấn người dùng
thử nghiệm được truy xuất như các ví dụ trong ngữ
cảnh, một trong những chiến lược hoạt động tốt nhất
cho ICL (Liu et al., 2022; Rubin et al., 2022).

3 Thiết lập Thí nghiệm
Công cụ. Chúng tôi tiến hành thí nghiệm sử dụng
các API từ ToolBench (Qin et al., 2024), một kho
tàng quy mô lớn của các API thế giới thực được thu
thập từ RapidAPI

và BMTools. Chúng tôi lọc xuống các API miễn phí
sử dụng với độ trễ thực thi thấp. Cuối cùng, chúng
tôi thu được 50 API bao phủ các công cụ tìm kiếm
(ví dụ, Google Search & Places), các API tìm kiếm
thông tin đặc thù miền (ví dụ, Wikipedia, Weather,
Sports, Gaming), và cũng các API giải quyết vấn
đề như WolframAlpha, Number Translator, v.v.
Chi tiết thêm ở Phụ lục A.

Thiết lập cho khám phá. Trong giai đoạn khám phá,
chúng tôi sử dụng ChatGPT (16k-0613) cho khám
phá và diễn giải, và GPT-4 (8k-0613) (OpenAI,
2023) cho lọc ví dụ cuối cùng. Chúng tôi đặt số
lần gọi API tối đa cho mỗi thử nghiệm là 4. Đối
với mỗi API, giai đoạn khám phá kéo dài 15 tập
với 4 thử nghiệm mỗi tập, dẫn đến tổng cộng 60
ví dụ trước khi lọc và diễn giải. Sau khi lọc, 15
ví dụ cho mỗi API được chọn ngẫu nhiên vào tập
thử nghiệm, nơi những cái còn lại được diễn giải
thành ~140 ví dụ, tạo tổng cộng ~7K ví dụ sử dụng
công cụ. Đối với các ví dụ thử nghiệm, chúng tôi
kiểm tra thủ công và sửa chữa bất kỳ vấn đề nào,
nếu có, để đảm bảo chất lượng tập thử nghiệm.

Đường cơ sở & khai thác với STE. Chúng tôi thí
nghiệm với Llama-2-Chat-7B/13B (Touvron et al.,
2023), Mistral-Instruct-7B (Jiang et al., 2023),
và GPT-3.5-turbo/GPT-4 (chỉ ICL) và so sánh hiệu
suất của chúng có và không có STE. Chúng tôi so
sánh với ToolLLaMA-v2 (Qin et al., 2024) như
đường cơ sở chính cho các chiến lược học công cụ
hiện tại. Nó dựa trên Llama-2-7B và được tinh chỉnh
trên 126K ví dụ sử dụng công cụ được tổng hợp bởi
ChatGPT-3.5-turbo cho việc sử dụng công cụ chung,
bao phủ một số lượng lớn các công cụ từ RapidAPI
bao gồm những cái được sử dụng trong các thí nghiệm
của chúng tôi.

Đối với ICL với lựa chọn minh họa láng giềng gần
nhất, theo các nghiên cứu trước (Liu et al., 2022;
Rubin et al., 2022), chúng tôi sử dụng mô hình
paraphrase-mpnet-base-v2 từ SentenceBERT
(Reimers and Gurevych, 2019) để tính toán độ
tương tự ngữ nghĩa, và chọn 8 ví dụ hàng đầu gần
nhất với truy vấn thử nghiệm như các minh họa
trong ngữ cảnh. Đối với Llama-2 với ICL, vì độ
dài token của toàn bộ 50 tài liệu API (khoảng 7K
token) vượt quá độ dài ngữ cảnh của nó (4,096),2
chúng tôi tăng cường mô hình với một bộ truy xuất
công cụ oracle truy xuất 15 API tương tự hàng đầu
so với API thực tế sử dụng tài liệu liên quan. Chúng
tôi tăng cường các mô hình khác có quy mô tương
tự (7B/13B) với cùng

2Trong khi có các biến thể của Llama-2 với ngữ cảnh dài
hơn (ví dụ, Xiong et al. (2023)), chúng tôi tuân thủ mô hình
gốc trong Touvron et al. (2023) để so sánh công bằng.

--- TRANG 5 ---
Thiết lập Mô hình Cơ sở Định dạng Tốt? API Khớp Độ Chính xác
Đường cơ sốToolLLaMA-v2 98.1 49.0 37.3
Llama-2-Chat-7B 34.5 40.2 10.7
Llama-2-Chat-13B 79.3 53.6 32.7
Mistral-Instruct-7B 61.7 69.3 30.1
GPT-3.5-turbo (16k-0613) 96.9 77.6 60.5
GPT-4 (8k-0613) 96.1 78.1 60.8
ICL w/ STELlama-2-Chat-7B 58.3 86.7 41.7
Llama-2-Chat-13B 87.5 86.6 62.9
Mistral-Instruct-7B 69.9 88.4 47.9
GPT-3.5-turbo (16k-0613) 97.6 90.8 75.6
GPT-4 (8k-0613) 97.7 92.8 76.3
Tinh chỉnh w/ STELlama-2-Chat-7B 99.2 94.9 73.3
Llama-2-Chat-13B 98.9 95.1 74.3
Mistral-Instruct-7B 99.1 95.8 76.8

Bảng 1: Hiệu suất sử dụng công cụ tổng thể. STE hiệu quả khi được sử dụng trong cả ICL và tinh chỉnh. Kết quả
tốt nhất tổng thể được in đậm, và kết quả tốt nhất dưới mỗi thiết lập được gạch chân.

Thiết lập Định dạng Tốt? API Khớp Độ Chính xác
STE Đầy đủ 99.2 94.9 73.3
- Phản hồi Thực thi 89.9 79.4 50.5
- Trí nhớ Ngắn 99.7 70.6 53.9
- Trí nhớ Dài 98.7 79.9 59.7
- Tự phản tỉnh 99.3 81.7 60.1

Bảng 2: Kết quả cho các nghiên cứu loại bỏ. Chúng tôi loại bỏ riêng lẻ từng thành phần chính của thiết kế khám
phá của chúng tôi. Khai thác được thực hiện bằng cách tinh chỉnh Llama-2-Chat-7B.

bộ truy xuất công cụ (khi ICL được sử dụng cho
khai thác) để so sánh công bằng. Các LLM được
tinh chỉnh trên STE không cần tài liệu API như vậy
trong ngữ cảnh, điều này giảm đáng kể chi phí suy
luận.

Thước đo đánh giá. Chúng tôi đánh giá mô hình
bằng cách khớp lời gọi API được dự đoán với thực
tế. Đối với các API có phạm vi giá trị nghiêm ngặt
cho các đối số, chúng tôi thực hiện khớp chuỗi trên
các đối số tương ứng trực tiếp. Đối với các API chấp
nhận đầu vào ngôn ngữ tự nhiên tự do, chúng tôi
sử dụng ChatGPT để đánh giá tính chính xác của
các đối số API được dự đoán. Chúng tôi báo cáo
độ chính xác tổng thể xem xét cả tên API và đối số
(Độ Chính xác) như thước đo chính, cùng với tỷ lệ
phần trăm các ví dụ với lời gọi API hợp lệ và không
có lỗi cú pháp/định dạng (Định dạng Tốt) và tỷ lệ
phần trăm các ví dụ chọn đúng sử dụng API thực
tế (API Khớp). Trong khi việc đánh giá mô hình
về việc liệu mô hình có giải quyết thành công truy
vấn người dùng dựa trên kết quả thực thi hay không
cũng là mong muốn, phần lớn các API trong thí
nghiệm của chúng tôi được kết nối với thông tin
thế giới thực động (ví dụ, thời tiết 'ngày mai' nơi
ngày phụ thuộc vào thời gian thực tế thực hiện lời
gọi API), điều này làm cho việc đánh giá như vậy
không khả thi. Chúng tôi để lại thách thức này cho
nghiên cứu tương lai.

4 Kết quả
4.1 Hiệu quả của STE
Kết quả được bao gồm trong Bảng 1. Chúng tôi tóm
tắt các phát hiện chính dưới đây.

Không có mô hình cơ sở nào hiển thị hiệu suất
thỏa đáng. Đối với tất cả các mô hình cơ sở mà
chúng tôi thử nghiệm, không có cái nào đạt được
hiệu suất sử dụng công cụ thỏa đáng. Mô hình tốt
nhất là GPT-4, chỉ đạt được tỷ lệ chính xác tổng
thể 60.8%. Llama-2 và Mistral đạt được hiệu suất
thấp hơn nhiều, chủ yếu do mô hình không thể tuân
theo các yêu cầu cú pháp/định dạng được chỉ định
khi thực hiện lời gọi API.3 ToolLLaMA-v2 (Qin
et al., 2024), mặc dù được tinh chỉnh rộng rãi cho
việc sử dụng công cụ, vẫn chủ yếu kém hiệu suất
so với GPT-3.5/4. Cải thiện hiệu suất của nó so với
các đường cơ sở không được tinh chỉnh như Llama-2
dường như chủ yếu đến từ định dạng tốt, và nó vẫn
gặp khó khăn nghiêm trọng trong việc chọn công
cụ đúng và dự đoán các đối số đúng. Điều này cho
thấy rằng tinh chỉnh cho việc sử dụng công cụ chung
không đủ để đạt được mức hiệu suất cần thiết cho
triển khai thực tế.

STE hiệu quả với cả ICL và tinh chỉnh. Những cải
thiện đáng kể được quan sát trong cả hai thiết lập.
Khi truy xuất từ các ví dụ sử dụng công cụ được
tạo bởi STE cho ICL, chúng tôi thấy cải thiện trên
toàn diện, với lên đến 30.2% (cho Llama-2-Chat-13B)
về độ chính xác cho các LLM mã nguồn mở. Nó
cũng tăng đáng kể hiệu suất đã mạnh của GPT-3.5/4.
Tinh chỉnh với các ví dụ STE cải thiện khả năng
sử dụng công cụ của các LLM mã nguồn mở

3Khả năng vượt trội của GPT-3.5/GPT-4 trong việc tuân theo
cú pháp có thể một phần do sự tăng cường đặc biệt của chúng
về gọi hàm (https://openai.com/blog/function-calling-
and-other-api-updates).

--- TRANG 6 ---
bằng một biên độ thậm chí lớn hơn, tăng Mistral-
Instruct-7B 46.7% về độ chính xác và cho phép nó
vượt trội hơn GPT-4. Tinh chỉnh với STE cũng làm
cho các LLM gần như hoàn hảo trong định dạng tốt
và chọn các công cụ đúng. Điều này có thể do tinh
chỉnh cho phép tiêm một phạm vi rộng hơn nhiều
các ví dụ sử dụng công cụ vào một mô hình so với
ICL. Trong khi chúng tôi không thể tinh chỉnh GPT-
3.5/4 do chi phí và tính khả dụng, việc giả định
rằng STE có thể cải thiện thêm khả năng sử dụng
công cụ của chúng vượt ra ngoài hiệu suất ICL
hiện tại của chúng là hợp lý.

4.2 Nghiên cứu Loại bỏ
Chúng tôi tiến hành một nghiên cứu loại bỏ cho
thiết kế khám phá của chúng tôi, với khai thác được
thực hiện bằng cách tinh chỉnh Llama-2-Chat-7B.
Cụ thể, chúng tôi loại bỏ phản hồi thực thi, trí nhớ
ngắn hạn/dài hạn, và thành phần tự phản tỉnh. Chúng
tôi mở rộng số lượng tập để bảo tồn tổng số thử
nghiệm nếu cần. Kết quả trong Bảng 2 cho thấy
rằng 1) khám phá không có phản hồi thực thi có
thể đưa ra một lượng đáng kể các ví dụ định dạng
sai nơi các lời gọi API không tuân theo các yêu
cầu cú pháp/định dạng; 2) cả trí nhớ ngắn hạn và
dài hạn đều tỏ ra thiết yếu cho thử và sai hiệu quả;
3) tự phản tỉnh quan trọng trong việc duy trì một
trí nhớ dài hạn thông tin cho việc khám phá. Để
hiểu rõ hơn về lợi ích của thiết kế trí nhớ của chúng
tôi, chúng tôi tiến hành một nghiên cứu trường hợp
với API forecast_weather (các ví dụ trong Phụ
lục C), điều này cho thấy rõ ràng rằng cả hai cơ
chế trí nhớ đều cải thiện đáng kể tính đa dạng và
toàn diện của việc khám phá:

•Trí nhớ ngắn hạn tăng cường tính cụ thể và
toàn diện. So sánh các thử nghiệm có/không
có trí nhớ ngắn hạn, có thể thấy rằng trí nhớ
ngắn hạn hiệu quả thúc đẩy LLM khám phá
toàn diện thông tin chi tiết từ công cụ, bao
trùm 16 thuộc tính khác nhau (ví dụ, độ ẩm,
lượng mưa, chỉ số UV, tầm nhìn, và thời gian
mặt trời mọc/lặn) tổng cộng. Trong khi đó,
khi trí nhớ ngắn hạn bị vô hiệu hóa, các ví
dụ kém cụ thể hơn nhiều và chủ yếu về điều
kiện thời tiết chung (ví dụ, "Thời tiết sẽ như
thế nào...") do không thể tận dụng thông tin
mới thu được từ kết quả thực thi. Ngoài ra,
việc khám phá không có trí nhớ ngắn hạn dẫn
đến tỷ lệ phần trăm thấp hơn đáng kể các ví
dụ sử dụng công cụ tích cực (78.3%→51.7%),
vì mô hình không thể học từ các lỗi chi tiết
trong quá khứ để tạo điều kiện cho các thử
nghiệm tương lai. Như một ví dụ, mô hình
tổng hợp một lượng đáng kể các truy vấn nơi

thời gian được chỉ định như ngày trong tuần,
đây không phải là loại tham số được hỗ trợ
của API và do đó liên tục dẫn đến thất bại.

•Trí nhớ dài hạn cải thiện tính đa dạng tổng
thể trong khoảng thời gian dài. Với trí nhớ
dài hạn, LLM khám phá các ví dụ bao phủ
một phạm vi rộng hơn các chủ đề, và duy trì
tiến bộ qua các tập khác nhau. Khi trí nhớ
dài hạn bị vô hiệu hóa, các thử nghiệm qua
các tập trở nên lặp lại và ít thông tin hơn.
Để đặc tính định lượng, chúng tôi trích xuất
các chủ đề cốt lõi (vị trí, thời gian, thuộc tính)
từ các truy vấn, đo lường tính đa dạng của
chúng và cũng vẽ biểu đồ phân phối thuộc
tính (Phụ lục C). Với trí nhớ dài hạn, tất cả
các truy vấn đều riêng biệt và các thử nghiệm
được cân bằng qua các thuộc tính khác nhau.
Không có trí nhớ dài hạn, chỉ 71.7% các thử
nghiệm liên quan đến các chủ đề riêng biệt
và phân phối qua các thuộc tính lệch nhiều
hơn, cho thấy hiệu quả của trí nhớ dài hạn
trong việc duy trì tính đa dạng của việc khám
phá trong khoảng thời gian dài.

4.3 Phân tích Lỗi
Lỗi của GPT-4. Như một trong những LLM có khả
năng nhất, GPT-4 (8k-0613) chỉ có thể đạt được
độ chính xác tổng thể 60.8%. Chúng tôi tiến hành
phân tích lỗi của GPT-4. Chúng tôi lấy mẫu ngẫu
nhiên và kiểm tra 30 ví dụ lỗi của GPT-4, có thể
được phân loại thành ba loại sau, với tỷ lệ phần
trăm tương ứng không có→có ICL với các ví dụ
STE. Các ví dụ cho mỗi danh mục được hiển thị
trong Phụ lục D.

•Lựa chọn sai API (36.7%→19.0%). GPT-4
gọi API sai không thể giải quyết truy vấn người
dùng. Bảng 4 cho thấy một ví dụ nơi truy vấn
người dùng là về các công viên có đường
mòn đi bộ ở San Francisco. Ở đây mô hình
gọi một API truy xuất tọa độ địa lý của San
Francisco, bỏ qua API "Places" thực tế. ICL
với các ví dụ STE giúp giải quyết khoảng một
nửa các lỗi như vậy bằng cách minh họa tốt
hơn ngữ nghĩa chi tiết của các API.

•Thiếu/sai đối số (26.7%→10.0%). Ở đây,
GPT-4 không cung cấp tập hợp đối số đúng
mặc dù chọn công cụ đúng. Bảng 5 cho thấy
một ví dụ nơi mô hình không cung cấp từ
khóa "lang" cần thiết. STE đặc biệt hiệu quả
cho các lỗi như vậy.

•Các ví dụ khó đánh giá (36.7%→16.7%).
Chúng tôi phát hiện rằng thật khó để đánh
giá tính chính xác

--- TRANG 7 ---
-------------------------------------------------------------------
Lời gọi API Vàng: Football Dolphin - Thống kê đối đầu
Args: { "first_team" : "Leeds" , "second_team": "Sheffield 
Weds", "type_of_statistics": ...}
 Mô tả : API này trả về dữ liệu thống kê về 
Giải Ngoại hạng Anh... Tham số bắt buộc: [{"name": 
"first_team", type: "STRING", description: Nhập đội đầu tiên 
từ tất cả các đội có sẵn: Arsenal, ..., Ipswich, Leeds , 
Leicester, Liverpool, ...,}, {"name": ...}]
------------------------------------------------------------------Lời gọi API Dự đoán: Football Dolphin - Thống kê đối đầu
Args: { "first_team": "Leeds United" , "second_team": "Sheffield 
Weds", "type_of_statistics": ...}-----------------------------------------------------------------
Lời gọi API Vàng: GamerPower - Lọc & Nhóm Giveaway
Args: { "platform": "gog", "type": "game.key"}
 Mô tả : "Tìm tất cả trò chơi miễn phí, loot và giveaway 
với API theo dõi giveaway này được cung cấp bởi GamerPower.com! Truy cập theo chương trình các 
giveaway tốt nhất trong... Tham số bắt buộc: [{"name" : "platform", "type": "STRING"}, {"name": "type", "type": 
"STRING", "description": ...}]
------------------------------------------------------------------------------------------------------------------------------
Lời gọi API Vàng: BART - Thông tin tư vấn
Args : {"cmd" : "bsa", "orig": "UCTY"}
 Mô tả : API BART cung cấp cho bạn quyền truy cập vào hầu hết 
tất cả dịch vụ BART và dữ liệu trạm 
có sẵn... Tham số bắt buộc: [{"name": "cmd", "type": "STRING", ... name": "orig", "type": "STRING", "description": "Bộ lọc trạm tùy chọn. Sử dụng 
viết tắt trạm BART 4 ký tự ...}]
-------------------------------------------------------------
Lời gọi API Dự đoán: BART - Thông tin tư vấn
Args : {"cmd" : "bsa", "orig": "UNION" }Truy vấn người dùng : Các giveaway đang diễn ra cho khóa 
trò chơi PC trên nền tảng GOG là gì ?Truy vấn người dùng : Thông tin tư vấn hiện tại cho 
trạm Union City là gì?Truy vấn người dùng : Bạn có thể cung cấp tổng số trận đấu 
đã chơi giữa Leeds United và Sheffield Wednesday trong 
Giải Ngoại hạng Anh không?
(a) (b) (c)Lời gọi API Dự đoán: GamerPower - Lọc & Nhóm Giveaway
Args: { "platform": "pc", "type": "game.key"}

Hình 3: Các ví dụ lỗi của Mistral-Instruct-7B sau tinh chỉnh: (a) kiến thức thường thức/thế giới, (b) hiểu biết
ngôn ngữ, và (c) grounding.

Lô 1 Lô 2 Lô 3 Lô 4 Tất cả API MMLU BBH
Llama-Flan - - - - - 37.2 39.5
CL-Vòng 1 80.6 - - - - 39.6 36.8
CL-Vòng 2 1.7→76.1 87.7→84.1 - - - 40.2 38.9
CL-Vòng 3 0.0→70.6 56.9→84.1 68.9→65.6 - - 39.2 37.5
CL-Vòng 4 0.0→65.0 38.5→88.7 25.0→66.1 71.8→70.3 34.7→72.8 38.5 39.1
Llama-FT 73.3 87.2 68.3 67.2 74.1 38.7 40.8

Bảng 3: Kết quả cho học liên tục. Llama-Flan là LLM cơ sở và Llama-FT là Llama-Flan được tinh chỉnh trên
tất cả các công cụ cùng một lúc. Đối với CL, các công cụ được chia thành bốn lô và LLM cần học liên tục một
lô mới trong mỗi vòng. Điểm số bên trái/phải của mỗi mũi tên ("→") là độ chính xác sử dụng công cụ không
có/có rehearsal. Ví dụ, 1.7→76.1 có nghĩa là mô hình được tinh chỉnh chỉ đạt 1.7% trên các công cụ Lô 1
sau CL-Vòng 2 không có rehearsal, và 76.1% với rehearsal. Trong khi tinh chỉnh vanilla gây ra quên thảm khốc,
rehearsal có thể phần lớn giảm thiểu vấn đề này và cho phép mô hình học liên tục các công cụ mới trong khi
bảo tồn các kỹ năng đã có trước đó.

của các dự đoán mô hình cho khoảng một phần
ba các ví dụ lỗi (một ví dụ được bao gồm trong
Bảng 6). Các lý do chính đằng sau điều này là
1) sự tồn tại của các công cụ có chức năng chồng
chéo làm cho thực tế không duy nhất và 2) bản
chất nhạy cảm với thời gian của một số công cụ
ngăn cản thực tế nhất quán. Những khó khăn
như vậy trong việc đánh giá việc sử dụng công
cụ cũng được ghi nhận trong các nghiên cứu hiện
tại (Qin et al., 2024; Patil et al., 2023), đây là
một thách thức mở cho nghiên cứu tương lai.

Lỗi sau tinh chỉnh. Chúng tôi cũng kiểm tra các
lỗi của mô hình được tinh chỉnh có hiệu suất tốt
nhất (Mistral-Instruct-7B) và tóm tắt các nguyên
nhân lỗi đáng chú ý so với GPT-4, điều này làm
sáng tỏ các hướng cải thiện cho tương lai.

•Kiến thức thường thức/thế giới (47.4%). Nhiều
công cụ yêu cầu kiến thức thường thức/thế
giới. Hình 3(a) cho thấy một ví dụ nơi việc
gọi API yêu cầu biết viết tắt 4 ký tự của trạm
giao thông mục tiêu, và ở đây mô hình ảo
tưởng một viết tắt sai. Vấn đề này có thể được
giảm thiểu bằng cách mở rộng quy mô hoặc
truy xuất kiến thức bổ sung.

•Hiểu biết ngôn ngữ (31.6%). Một số lỗi được
gây ra bởi thiếu khả năng hiểu ngôn ngữ cơ
bản. Hình 3(b) cho thấy một ví

dụ nơi mô hình hiểu sai truy vấn người dùng
dẫn đến đối số sai. Sử dụng LLM cơ sở mạnh
hơn có thể giảm thiểu các lỗi như vậy.

•Grounding (21.1%). Chúng tôi phát hiện rằng
một số lỗi do thiếu grounding, nơi LLM tạo
ra các lời gọi API đúng về mặt ngữ nghĩa
nhưng không được grounded với các ràng buộc
API. Một ví dụ được đưa ra trong Hình 3(c),
nơi mô hình trích xuất đúng thực thể mục tiêu
nhưng không liên kết nó với tên thực thể được
hỗ trợ bởi API. Điều này có thể được cải thiện
bằng cách tích hợp các ràng buộc trong quá
trình giải mã (Zhang et al., 2023; Shin et al.,
2021; Fang et al., 2023) hoặc sử dụng các cơ
chế khớp mờ.

4.4 Học Công cụ Liên tục
Trong khi tinh chỉnh vượt trội đáng kể so với ICL
đối với việc sử dụng công cụ, một nhược điểm là
khả năng giảm tính linh hoạt như đã thảo luận trong
§1 do quên thảm khốc (Kirkpatrick et al., 2017;
Howard and Ruder, 2018; Kumar et al., 2022;
Luo et al., 2023). Vì việc huấn luyện lại mô hình
từ đầu tốn kém và làm tổn hại tính linh hoạt, chúng
tôi khám phá học liên tục (CL) và cho thấy rằng
rehearsal đơn giản (Scialom et al., 2022) dường
như đủ cho việc học công cụ liên tục với STE.

--- TRANG 8 ---
Chúng tôi chia ngẫu nhiên các công cụ thành 4 lô
liên tiếp để mô phỏng thiết lập liên tục. Đối với
rehearsal, trong mỗi vòng, chúng tôi thêm 10%
ví dụ sử dụng công cụ cho mỗi API từ các lô trước
vào bộ đệm phát lại. Để bảo tồn khả năng không
sử dụng công cụ chung, chúng tôi cũng thêm vào
mỗi vòng huấn luyện 2.000 ví dụ ngẫu nhiên từ
Flan-V2 (Longpre et al., 2023; Chung et al., 2022),
một trong những tập dữ liệu hướng dẫn chung chất
lượng cao nhất (Wang et al., 2023a), và đánh giá
mô hình trên MMLU (Hendrycks et al., 2021) và
Big-Bench-Hard (BBH) (Suzgun et al., 2023).
Chúng tôi sử dụng Llama-Flan làm mô hình cơ
sở để đảm bảo so sánh công bằng các khả năng
chung trên MMLU và BBH (chi tiết thêm trong
Phụ lục B). Kết quả trong Bảng 3 cho thấy rằng
mô hình có thể quên drastic các công cụ đã học
trước đó mà không có rehearsal, với những cái xa
hơn bị quên nghiêm trọng hơn. Rehearsal phần
lớn giảm thiểu việc quên - mô hình được huấn
luyện CL đạt được hiệu suất tương đương với
Llama-FT. Khả năng ngôn ngữ chung cũng được
giữ lại như được đo trên MMLU và BBH. Tổng
thể, chúng tôi mở rộng các phát hiện của Scialom
et al. (2022) về hiệu quả của experience replay
đến lĩnh vực mới của việc học công cụ LLM, chứng
minh một cách khả thi để linh hoạt thêm các công
cụ mới với phương pháp STE đề xuất.

5 Nghiên cứu Liên quan
Các mô hình ngôn ngữ được tăng cường công cụ.
Một trong những trọng tâm của nghiên cứu rộng
rãi trong NLP là tăng cường các mô hình với các
công cụ tìm kiếm/truy xuất có thể bổ sung kiến
thức thêm (Guu et al., 2020; Lewis et al., 2020;
Izacard et al., 2022; Borgeaud et al., 2022, inter
alia). Gần đây, có xu hướng tăng cường LLM với
các loại công cụ đa dạng hơn, như bộ thực thi
chương trình, mô hình dịch và QA (Chen et al.,
2023b; Gao et al., 2023; Parisi et al., 2022; Schick
et al., 2023), API từ nhà phát triển và kho lưu trữ
công cộng (Patil et al., 2023; Qin et al., 2024;
Xie et al., 2023), và các công cụ được tuyển chọn
cho các môi trường cụ thể (Gu et al., 2024) để
mở rộng thêm phạm vi các vấn đề mà LLM có
thể hỗ trợ.

Cả tinh chỉnh và ICL đều được sử dụng để thích
ứng LLM để sử dụng công cụ. Các phương pháp
dựa trên tinh chỉnh huấn luyện LLM để sử dụng
các công cụ trên một tập hợp các ví dụ minh họa
đặc thù công cụ (Schick et al., 2023; Parisi et al.,
2022), trong khi các phương pháp dựa trên ICL
(Lu et al., 2023; Song et al., 2023; Shen et al.,
2023; Liang et al., 2023b; Gao et al., 2023) trực
tiếp đặt các mô tả công cụ và tùy chọn (một lượng
nhỏ) các minh họa sử dụng công cụ trong ngữ cảnh.
Hao et al. (2023) đề xuất một phương pháp thích
ứng nhẹ mở rộng từ vựng của LLM với các embedding
công cụ được huấn luyện. Qin et al. (2024); Patil
et al. (2023); Tang et al. (2023) khám phá huấn
luyện các mô hình để tận dụng tốt hơn các mô tả
API cho việc sử dụng công cụ. Công việc của chúng
tôi nhằm phát triển một khuôn khổ cho phép trang
bị LLM với khả năng sử dụng công cụ mạnh hơn,
được thúc đẩy bởi cách con người thường học công
cụ thông qua thử và sai liên tục.

LLM có thể học từ phản hồi. Nghiên cứu gần đây
phát hiện rằng LLM có khả năng cải thiện/sửa chữa
các dự đoán của chúng với phản hồi (Shinn et al.,
2023; Madaan et al., 2023; Ganguli et al., 2023;
Chen et al., 2024; Peng et al., 2023; Kim et al.,
2023; Pan et al., 2023). Công việc của chúng tôi
được xây dựng dựa trên những phát hiện này và
sử dụng LLM để học tiến bộ các công cụ bằng cách
tận dụng phản hồi từ việc thực thi công cụ và tự
phản tỉnh của LLM.

Tổng hợp dữ liệu & bootstrapping với LLM. Do
việc tiếp xúc của LLM với các miền rộng trong
quá trình pretraining và khả năng tạo sinh được
cải thiện nhanh chóng, các nghiên cứu gần đây đã
khám phá việc sử dụng LLM cho tổng hợp tập dữ
liệu, điều này làm giảm bớt gánh nặng của các chú
thích con người tốn kém (Schick and Schütze,
2021; Wang et al., 2023b; Honovich et al., 2023;
Li et al., 2023; Zelikman et al., 2022; Huang
et al., 2023). Dữ liệu được tổng hợp bởi mô hình
như vậy sau đó có thể được sử dụng để cải thiện
các mô hình bao gồm chính chúng. Trong lĩnh vực
học công cụ, các ý tưởng tương tự đã được khám
phá cho tổng hợp dữ liệu đặc thù công cụ (Schick
et al., 2023; Patil et al., 2023; Qin et al., 2024).
Phương pháp của chúng tôi theo dòng nghiên cứu
này và thực hiện một bước hướng tới tính toàn
diện và đa dạng tốt hơn của các ví dụ sử dụng
công cụ được tổng hợp.

Tăng cường các mô hình với trí nhớ động. Việc
sử dụng các cơ chế trí nhớ để cho phép các mô
hình động thu thập và sử dụng kinh nghiệm là
một ý tưởng cũ, ví dụ, Riesbeck (1981); Schank
(1983). Nghiên cứu gần đây cũng khám phá việc
tăng cường các mô hình với trí nhớ tăng trưởng
của phản hồi người dùng và môi trường (Madaan
et al., 2022; Shinn et al., 2023; Zhong et al.,
2023; Liang et al., 2023a; Zhao et al., 2023;
Modarressi et al., 2023; Hu et al., 2023). Chúng
tôi lấy cảm hứng từ những nghiên cứu này và tăng
cường LLM với trí nhớ ngắn hạn chi tiết và trí
nhớ dài hạn được chưng cất để tăng cường việc
học tiến bộ các công cụ của LLM.

--- TRANG 9 ---
6 Kết luận
Được thúc đẩy bởi cách con người thành thạo các
công cụ thông qua tương tác liên tục và củng cố,
chúng tôi đề xuất thử và sai mô phỏng, một phương
pháp học công cụ LLM được xây dựng dựa trên
thử và sai dựa trên trí nhớ tiến bộ. Các thí nghiệm
trên các API được rút ra từ ToolBench cho thấy
hiệu quả của phương pháp đề xuất, và cũng rằng
tinh chỉnh dựa trên rehearsal có thể cho phép học
liên tục các công cụ mới với các kỹ năng trước
được bảo tồn.

Hạn chế
Cải thiện lặp đi lặp lại. Hiện tại, chúng tôi sử
dụng các mô hình mạnh cho khám phá và các mô
hình yếu nhỏ hơn cho khai thác. Việc khám phá-
khai thác cũng có thể được thực hiện lặp đi lặp
lại như trong các nghiên cứu trước (Aksitov et al.,
2023; Zelikman et al., 2022), nơi sự phụ thuộc
vào các mô hình mạnh có thể được giảm dần (ví
dụ, chỉ như các đánh giá viên) khi khả năng của
các mô hình được tăng cường cải thiện.

Sử dụng công cụ tổng hợp & lập kế hoạch. Một
khả năng quan trọng khác trong bối cảnh sử dụng
công cụ là sáng tác/lập kế hoạch nhiều lời gọi
công cụ để hoàn thành các truy vấn phức tạp,
điều này đi theo hướng trực giao như trọng tâm
của chúng tôi ở đây. Các nghiên cứu gần đây cho
thấy rằng các khả năng cốt lõi của LLM được mã
hóa và gợi ra từ pretraining thay vì được tiêm
thông qua tinh chỉnh/alignment (Zhou et al.,
2023; Lin et al., 2023), điều này cho thấy rằng
việc chuẩn bị dữ liệu rộng rãi có thể không cần
thiết để thích ứng LLM cho việc sử dụng công
cụ phức tạp, khác với trọng tâm của chúng tôi
nơi việc học và khám phá rộng rãi luôn được mong
muốn vì thông tin được thu thập từ phía công cụ.

Khả năng trí nhớ lớn hơn vượt ra ngoài giới hạn
ngữ cảnh. Khả năng của trí nhớ được tăng cường
bị giới hạn bởi độ dài ngữ cảnh của LLM. Có các
loại phương pháp khác nhau có thể được sử dụng
để mở rộng thêm trí nhớ, như sử dụng các mô-đun
truy xuất bổ sung (Wang and Li, 2023) hoặc có
các biểu diễn phân cấp/nén hơn của trí nhớ (Chen
et al., 2023a).

Unlearning công cụ? Trong khi chúng tôi khám
phá việc học liên tục các công cụ mới, vấn đề
unlearning cũng quan trọng vì các công cụ có thể
liên tục được gỡ bỏ/lỗi thời. Knowledge unlearning
nói chung là một vấn đề thách thức (Si et al.,
2023), và có thể có các thiết kế cụ thể hỗ trợ
unlearning công cụ dễ dàng hơn, như ToolkenGPT
(Hao et al., 2023) cho phép thích ứng cắm và chơi
trong khi cho phép học với các ví dụ quy mô lớn.

Hạn chế của tinh chỉnh dựa trên ví dụ. Cuối cùng,
cũng có những hạn chế vốn có của các phương pháp
dựa trên ví dụ cho việc học công cụ, đặc biệt, khó
khăn trong việc dạy mô hình khi nào không sử dụng
công cụ thông qua chỉ các ví dụ sử dụng công cụ
tích cực. Một số cách tiềm năng để cải thiện vấn
đề này là tích hợp các ví dụ tiêu cực (ví dụ, sử
dụng các mục tiêu contrastive) hoặc mang các
phần như vậy của API cùng với huấn luyện dựa
trên ví dụ. Chúng tôi để lại những nghiên cứu này
cho công việc tương lai.

Tài liệu tham khảo
Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang
Li, Sheila Babayan, Kavya Kopparapu, Zachary
Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srini-
vasan, Manzil Zaheer, Felix Yu, and Sanjiv Kumar.
2023. Rest meets react: Self-improvement for multi-
step reasoning llm agent.

Alice MI Auersperg, Auguste MP V on Bayern, Gyula K
Gajdon, Ludwig Huber, and Alex Kacelnik. 2011.
Flexibility in problem solving and tool use of kea and
new caledonian crows in a multi access box paradigm.
PloS one , 6(6):e20231.

Benjamin B Beck. 1973. Observation learning of tool
use by captive guinea baboons (papio papio). Amer-
ican Journal of Physical Anthropology , 38(2):579-
582.

Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.
Improving language models by retrieving from tril-
lions of tokens. In International conference on ma-
chine learning , pages 2206-2240. PMLR.

Howard Chen, Ramakanth Pasunuru, Jason Weston, and
Asli Celikyilmaz. 2023a. Walking down the mem-
ory maze: Beyond context limit through interactive
reading.

Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W. Cohen. 2023b. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. Transactions on
Machine Learning Research .

Xinyun Chen, Maxwell Lin, Nathanael Schärli, and
Denny Zhou. 2024. Teaching large language models
to self-debug. In The Twelfth International Confer-
ence on Learning Representations .

Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,

--- TRANG 10 ---
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,
and Jason Wei. 2022. Scaling instruction-finetuned
language models.

Nicola S Clayton and Anthony Dickinson. 1998.
Episodic-like memory during cache recovery by
scrub jays. Nature , 395(6699):272-274.

Nathan J Emery and Nicola S Clayton. 2004. The men-
tality of crows: convergent evolution of intelligence
in corvids and apes. science , 306(5703):1903-1907.

Hao Fang, Anusha Balakrishnan, Harsh Jhamtani, John
Bufe, Jean Crawford, Jayant Krishnamurthy, Adam
Pauls, Jason Eisner, Jacob Andreas, and Dan Klein.
2023. The whole truth and nothing but the truth:
Faithful and controllable dialogue response genera-
tion with dataflow transduction and constrained de-
coding. In Findings of the Association for Compu-
tational Linguistics: ACL 2023 , pages 5682-5700,
Toronto, Canada. Association for Computational Lin-
guistics.

Deep Ganguli, Amanda Askell, Nicholas Schiefer,
Thomas Liao, Kamil ˙e Lukoši ¯ut˙e, Anna Chen, Anna
Goldie, Azalia Mirhoseini, Catherine Olsson, Danny
Hernandez, et al. 2023. The capacity for moral self-
correction in large language models. arXiv preprint
arXiv:2302.07459 .

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2023. Pal: Program-aided language
models. In International Conference on Machine
Learning , pages 10764-10799. PMLR.

Kathleen R Gibson, Kathleen Rita Gibson, and Tim In-
gold. 1993. Tools, language and cognition in human
evolution . Cambridge University Press.

Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong,
Jie Tang, Jayanth Srinivasa, Hugo Latapie, and Yu Su.
2024. Middleware for llms: Tools are instrumental
for language agents in complex environments. arXiv
preprint arXiv:2402.14672 .

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Mingwei Chang. 2020. Retrieval augmented
language model pre-training. In International confer-
ence on machine learning , pages 3929-3938. PMLR.

Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu.
2023. ToolkenGPT: Augmenting frozen language
models with massive tools via tool embeddings. In
Thirty-seventh Conference on Neural Information
Processing Systems .

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2021. Measuring massive multitask language under-
standing. In International Conference on Learning
Representations .

Or Honovich, Thomas Scialom, Omer Levy, and Timo
Schick. 2023. Unnatural instructions: Tuning lan-
guage models with (almost) no human labor. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 14409-14428, Toronto, Canada.
Association for Computational Linguistics.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification.
InProceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers) , pages 328-339, Melbourne, Australia.
Association for Computational Linguistics.

Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo
Zhao, and Hang Zhao. 2023. Chatdb: Augmenting
llms with databases as their symbolic memory.

Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi
Wang, Hongkun Yu, and Jiawei Han. 2023. Large
language models can self-improve. In Proceedings
of the 2023 Conference on Empirical Methods in Nat-
ural Language Processing , pages 1051-1068, Singa-
pore. Association for Computational Linguistics.

Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-
Yu, Armand Joulin, Sebastian Riedel, and Edouard
Grave. 2022. Atlas: Few-shot learning with retrieval
augmented language models.

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, Lélio Renard Lavaud,
Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
and William El Sayed. 2023. Mistral 7b.

Geunwoo Kim, Pierre Baldi, and Stephen McAleer.
2023. Language models can solve computer tasks.
arXiv preprint arXiv:2303.17491 .

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,
Joel Veness, Guillaume Desjardins, Andrei A. Rusu,
Kieran Milan, John Quan, Tiago Ramalho, Ag-
nieszka Grabska-Barwinska, Demis Hassabis, Clau-
dia Clopath, Dharshan Kumaran, and Raia Hadsell.
2017. Overcoming catastrophic forgetting in neural
networks. Proceedings of the National Academy of
Sciences , 114(13):3521-3526.

Ananya Kumar, Aditi Raghunathan, Robbie Matthew
Jones, Tengyu Ma, and Percy Liang. 2022. Fine-
tuning can distort pretrained features and underper-
form out-of-distribution. In International Conference
on Learning Representations .

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems , 33:9459-9474.

--- TRANG 11 ---
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke
Zettlemoyer, Omer Levy, Jason Weston, and Mike
Lewis. 2023. Self-alignment with instruction back-
translation.

Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu,
Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023a.
Unleashing infinite-length input capacity for large-
scale language models with self-controlled memory
system.

Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,
Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,
Shaoguang Mao, et al. 2023b. Taskmatrix. ai: Com-
pleting tasks by connecting foundation models with
millions of apis. arXiv preprint arXiv:2303.16434 .

Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu,
Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chan-
dra Bhagavatula, and Yejin Choi. 2023. The unlock-
ing spell on base llms: Rethinking alignment via
in-context learning.

Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2022. What
makes good in-context examples for GPT-3? In
Proceedings of Deep Learning Inside Out (DeeLIO
2022): The 3rd Workshop on Knowledge Extrac-
tion and Integration for Deep Learning Architectures ,
pages 100-114, Dublin, Ireland and Online. Associa-
tion for Computational Linguistics.

Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V . Le,
Barret Zoph, Jason Wei, and Adam Roberts. 2023.
The flan collection: Designing data and methods for
effective instruction tuning.

Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In International Confer-
ence on Learning Representations .

Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-
Wei Chang, Ying Nian Wu, Song-Chun Zhu, and
Jianfeng Gao. 2023. Chameleon: Plug-and-play
compositional reasoning with large language models.
InThirty-seventh Conference on Neural Information
Processing Systems .

Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie
Zhou, and Yue Zhang. 2023. An empirical study
of catastrophic forgetting in large language mod-
els during continual fine-tuning. arXiv preprint
arXiv:2308.08747 .

Aman Madaan, Niket Tandon, Peter Clark, and Yim-
ing Yang. 2022. Memory-assisted prompt editing
to improve GPT-3 after deployment. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 2833-2861,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,

Shashank Gupta, Bodhisattwa Prasad Majumder,
Katherine Hermann, Sean Welleck, Amir Yazdan-
bakhsh, and Peter Clark. 2023. Self-refine: Itera-
tive refinement with self-feedback. In Thirty-seventh
Conference on Neural Information Processing Sys-
tems.

Grégoire Mialon, Roberto Dessi, Maria Lomeli, Christo-
foros Nalmpantis, Ramakanth Pasunuru, Roberta
Raileanu, Baptiste Roziere, Timo Schick, Jane
Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann
LeCun, and Thomas Scialom. 2023. Augmented lan-
guage models: a survey. Transactions on Machine
Learning Research . Survey Certification.

Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and
Hinrich Schütze. 2023. Ret-llm: Towards a general
read-write memory for large language models.

OpenAI. 2023. Gpt-4 technical report.

Liangming Pan, Michael Saxon, Wenda Xu, Deepak
Nathani, Xinyi Wang, and William Yang Wang. 2023.
Automatically correcting large language models: Sur-
veying the landscape of diverse self-correction strate-
gies. arXiv preprint arXiv:2308.03188 .

Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:
Tool augmented language models.

Shishir G Patil, Tianjun Zhang, Xin Wang, and
Joseph E Gonzalez. 2023. Gorilla: Large language
model connected with massive apis. arXiv preprint
arXiv:2305.15334 .

Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng,
Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou
Yu, Weizhu Chen, et al. 2023. Check your facts and
try again: Improving large language models with
external knowledge and automated feedback. arXiv
preprint arXiv:2302.12813 .

Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen,
Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,
Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,
Huadong Wang, Cheng Qian, Runchu Tian, Kunlun
Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen
Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,
Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,
Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,
Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng
Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and
Maosong Sun. 2023. Tool learning with foundation
models.

Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan
Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,
Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian,
Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li,
Zhiyuan Liu, and Maosong Sun. 2024. ToolLLM:
Facilitating large language models to master 16000+
real-world APIs. In The Twelfth International Con-
ference on Learning Representations .

A David Redish. 2016. Vicarious trial and error. Nature
Reviews Neuroscience , 17(3):147-159.

--- TRANG 12 ---
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
3982-3992, Hong Kong, China. Association for Com-
putational Linguistics.

Christopher Riesbeck. 1981. Failure-driven reminding
for incremental learning. In IJCAI , pages 115-120.
Citeseer.

Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2022. Learning to retrieve prompts for in-context
learning. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 2655-2671, Seattle, United States.
Association for Computational Linguistics.

Roger C. Schank. 1983. Dynamic Memory: A Theory of
Reminding and Learning in Computers and People .
Cambridge University Press, USA.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta
Raileanu, Maria Lomeli, Eric Hambro, Luke Zettle-
moyer, Nicola Cancedda, and Thomas Scialom. 2023.
Toolformer: Language models can teach themselves
to use tools. In Thirty-seventh Conference on Neural
Information Processing Systems .

Timo Schick and Hinrich Schütze. 2021. Generating
datasets with pretrained language models. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 6943-
6951, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.

Thomas Scialom, Tuhin Chakrabarty, and Smaranda
Muresan. 2022. Fine-tuned language models are
continual learners. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing , pages 6107-6122, Abu Dhabi, United
Arab Emirates. Association for Computational Lin-
guistics.

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2023. Hugging-
GPT: Solving AI tasks with chatGPT and its friends
in hugging face. In Thirty-seventh Conference on
Neural Information Processing Systems .

Richard Shin, Christopher Lin, Sam Thomson, Charles
Chen, Subhro Roy, Emmanouil Antonios Platanios,
Adam Pauls, Dan Klein, Jason Eisner, and Benjamin
Van Durme. 2021. Constrained language models
yield few-shot semantic parsers. In Proceedings of
the 2021 Conference on Empirical Methods in Natu-
ral Language Processing , pages 7699-7715, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.

Noah Shinn, Federico Cassano, Ashwin Gopinath,
Karthik R Narasimhan, and Shunyu Yao. 2023. Re-
flexion: language agents with verbal reinforcement

learning. In Thirty-seventh Conference on Neural
Information Processing Systems .

Robert W Shumaker, Kristina R Walkup, and Ben-
jamin B Beck. 2011. Animal tool behavior: the use
and manufacture of tools by animals . JHU Press.

Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang,
Dan Qu, and Weiqiang Zhang. 2023. Knowledge
unlearning for llms: Tasks, methods, and challenges.

Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li,
Ke Wang, Ye Tian, and Sujian Li. 2023. Rest-
gpt: Connecting large language models with real-
world applications via restful apis. arXiv preprint
arXiv:2306.06624 .

Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny
Zhou, and Jason Wei. 2023. Challenging BIG-bench
tasks and whether chain-of-thought can solve them.
InFindings of the Association for Computational Lin-
guistics: ACL 2023 , pages 13003-13051, Toronto,
Canada. Association for Computational Linguistics.

Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,
Qiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca:
Generalized tool learning for language models with
3000 simulated cases.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.

Krist Vaesen. 2012. The cognitive bases of human tool
use. Behavioral and brain sciences , 35(4):203-218.

Danqing Wang and Lei Li. 2023. Learning from mis-
takes via cooperative study assistant for large lan-
guage models. In Proceedings of the 2023 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pages 10667-10685, Singapore. Association
for Computational Linguistics.

--- TRANG 13 ---
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack
Hessel, Tushar Khot, Khyathi Chandu, David Wad-
den, Kelsey MacMillan, Noah A. Smith, Iz Beltagy,
and Hannaneh Hajishirzi. 2023a. How far can camels
go? exploring the state of instruction tuning on open
resources. In Thirty-seventh Conference on Neural
Information Processing Systems Datasets and Bench-
marks Track .

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. 2023b. Self-instruct: Aligning language
models with self-generated instructions. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 13484-13508, Toronto, Canada. Association
for Computational Linguistics.

Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Lu-
oxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao,
Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin
Su, Dongchan Shin, Caiming Xiong, and Tao Yu.
2023. Openagents: An open platform for language
agents in the wild.

Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang,
Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi
Rungta, Karthik Abinav Sankararaman, Barlas Oguz,
Madian Khabsa, Han Fang, Yashar Mehdad, Sharan
Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale,
Sergey Edunov, Mike Lewis, Sinong Wang, and Hao
Ma. 2023. Effective long-context scaling of founda-
tion models.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R Narasimhan, and Yuan Cao. 2023.
React: Synergizing reasoning and acting in language
models. In The Eleventh International Conference
on Learning Representations .

Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-
man. 2022. STar: Bootstrapping reasoning with rea-
soning. In Advances in Neural Information Process-
ing Systems .

Kexun Zhang, Hongqiao Chen, Lei Li, and William
Wang. 2023. Syntax error-free and generalizable tool
use for llms via finite-state decoding.

Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu
Lin, Yong-Jin Liu, and Gao Huang. 2023. Expel:
Llm agents are experiential learners.

Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and
Yanlin Wang. 2023. Memorybank: Enhancing large
language models with long-term memory.

Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis,
Luke Zettlemoyer, and Omer Levy. 2023. LIMA:
Less is more for alignment. In Thirty-seventh Con-
ference on Neural Information Processing Systems .

--- TRANG 14 ---
A Lựa chọn API
Chúng tôi chọn 50 API từ ToolBench4, một bộ sưu
tập lớn các API từ BMTools5 và RapidAPI6. Đối
với BMTools, chúng tôi thủ công chọn 10 API chất
lượng cao. Đối với RapidAPI, do quy mô lớn của
kho API (hơn 16k), chúng tôi thực hiện lọc bằng
cách 1) chọn các API miễn phí sử dụng mà không
cần đăng ký; 2) sắp xếp các API dựa trên giới hạn
tốc độ và độ trễ và chọn 400 cái hàng đầu; 3) chạy
một khám phá quy mô nhỏ sử dụng ChatGPT để
tương tác với môi trường thực thi API, và chọn 40
cái hàng đầu dựa trên tỷ lệ thành công lời gọi API.

B Chi tiết Thí nghiệm
Tinh chỉnh. Tất cả việc tinh chỉnh mô hình được
thực hiện sử dụng bộ tối ưu AdamW (Loshchilov
and Hutter, 2019) với beta=(0.9,0.999), ε=10−8
và không có weight decay. Chúng tôi tinh chỉnh
mô hình trong một epoch với tốc độ học 2×10−5,
và một scheduler cosine với tỷ lệ warmup 0.03,
và kích thước batch 64. Tất cả các lần chạy tinh
chỉnh được thực hiện với 4 GPU NVIDIA A100/A6000.
Các ví dụ huấn luyện được cắt ngắn xuống 2.048
token.

Dữ liệu Flan-V2 & rehearsal. Chúng tôi sử dụng
Flan-V2 (Chung et al., 2022; Longpre et al., 2023)
cho rehearsal là một trong những dữ liệu điều chỉnh
hướng dẫn miền chung chất lượng cao nhất (Wang
et al., 2023a). Chúng tôi sử dụng dữ liệu được
phát hành bởi Wang et al. (2023a).7

C Các truy vấn ví dụ cho tất cả các thử nghiệm đã khám phá
Bảng 14, 15, 16 bao gồm các truy vấn cho tất cả
các thử nghiệm đã khám phá cho API forecast_weather
qua các thiết lập khác nhau (như được chỉ ra trong
chú thích). Hình 4 bao gồm biểu đồ của các thử
nghiệm đã khám phá được nhóm theo thuộc tính
được hỏi, so sánh STE và STE không có trí nhớ
dài hạn. Kết quả cho thấy rõ ràng rằng các cơ chế
trí nhớ trong STE cải thiện đáng kể tính đa dạng
và toàn diện của việc khám phá.

D Các Ví dụ Lỗi của GPT-4 và Mistral-Instruct-7B
Bảng 4, 5, 6 bao gồm các ví dụ lỗi của GPT-4,
và Bảng 7, 8, 9 bao gồm các ví dụ lỗi của Mistral-
Instruct-7B được tinh chỉnh được tham chiếu bởi
§4.2.

4https://github.com/OpenBMB/ToolBench
5https://github.com/OpenBMB/BMTools
6https://rapidapi.com/
7https://github.com/allenai/open-instruct

E Prompts
Bảng 10, 11 bao gồm prompt đầy đủ cho thử và
sai mô phỏng. Bảng 10 bao gồm prompt cho tự
tinh chỉnh với phản hồi thực thi và trí nhớ ngắn
hạn, và Bảng 11 bao gồm prompt cho trí nhớ dài
hạn. Bảng 12 và 13 bao gồm các prompt cho lọc
ví dụ và diễn giải tương ứng.

--- TRANG 15 ---
0.0 0.1 0.2 0.3 0.4 0.5 0.6
% của các thử nghiệmthời tiết
tốc độ gió tối đa
nhiệt độ hiện tại
chỉ số UV
độ ẩm trung bình
tốc độ gió trung bình
tốc độ gió
mức độ ẩm
thời gian mặt trời mọc và lặn
khả năng có tuyết
nhiệt độ
khả năng có mưa
nhiệt độ tối đa và tối thiểu
nhiệt độ trung bình
lượng mưa
nhiệt độ tối đa
tầm nhìn trung bình
nhiệt độ hàng giờthuộc tính
STE
STE không có trí nhớ dài hạn

Hình 4: Biểu đồ của các thử nghiệm đã khám phá được nhóm theo thuộc tính được hỏi. Trí nhớ dài hạn cải thiện
đáng kể tính đa dạng của các thuộc tính đã khám phá.

Truy vấn người dùng : Những công viên nào ở San Francisco có đường mòn đi bộ?
Tên API thực tế :search_places
Mô tả cho API thực tế : Chạy tìm kiếm Places. Đầu vào của bạn nên là một json (args json schema): {"query":
string, }
Đối số thực tế : {"query": "parks in San Francisco with hiking trails"}
API Dự đoán :Geographic coordinates by placename
Mô tả cho API dự đoán : Trả về tọa độ địa lý cho tên địa điểm đã cho (thành phố, làng, v.v.). Phương pháp trả về
địa điểm có tên tương tự nhất với chuỗi tìm kiếm.", "required_parameters": [{"name": "name", "type": "STRING",
"description": "Placename", "default": "London"}, {"name": "lang", "type": "ENUM", "description": "Two-letter
language code (ISO639-1). The following values are available: en (english), ru (russian)", "default": ""}],
"optional_parameters": [{"name": "country", "type": "STRING", "description": "Two-letter country code, ISO-3166
(optional). Default is all countries.", "default": ""}]
Đối số Dự đoán : {"name": "San Francisco", "lang": "en"}

Bảng 4: Ví dụ lỗi cho GPT-4 (danh mục: lời gọi API sai). Ở đây GPT-4 "bỏ qua" API Google Places thỏa mãn
truy vấn, và thay vào đó gọi một API truy xuất tọa độ địa lý của địa điểm mục tiêu.

--- TRANG 16 ---
Truy vấn người dùng : Tọa độ địa lý của thành phố Sydney, Canada là gì?
Tên API thực tế : Geographic coordinates by placename
Mô tả cho API thực tế : Trả về tọa độ địa lý cho tên địa điểm đã cho (thành phố, làng,
v.v.). Phương pháp trả về địa điểm có tên tương tự nhất với chuỗi tìm kiếm. Tham số bắt buộc:
[{"name": "name", "type": "STRING", "description": "Placename", "default": "London"}, {"name": "lang",
"type": "ENUM", "description": "Two-letter language code (ISO639-1). The following values are available:
en (english), ru (russian)", "default": ""}]. Tham số tùy chọn: [{"name": "country", "type": "STRING",
"description": "Two-letter country code, ISO-3166 (optional). Default is all countries.", "default": ""}]
Đối số thực tế :{"name": "Sydney", "country": "CA", "lang": "en"}
API Dự đoán : Geographic coordinates by placename
Mô tả cho API dự đoán : Trả về tọa độ địa lý cho tên địa điểm đã cho (thành phố, làng, v.v.). Phương pháp trả về
địa điểm có tên tương tự nhất với chuỗi tìm kiếm.", "required_parameters": [{"name": "name", "type": "STRING",
"description": "Placename", "default": "London"}, {"name": "lang", "type": "ENUM", "description": "Two-letter
language code (ISO639-1). The following values are available: en (english), ru (russian)", "default": ""}],
"optional_parameters": [{"name": "country", "type": "STRING", "description": "Two-letter country code, ISO-3166
(optional). Default is all countries.", "default": ""}]
Đối số Dự đoán :[{"name": "Sydney", "country": "CA"}

Bảng 5: Ví dụ lỗi cho GPT-4 (danh mục: đối số API sai). Ở đây GPT-4 chọn công cụ đúng nhưng không điền
từ khóa "lang" bắt buộc.

Truy vấn người dùng : Nguyên nhân chính của biến đổi khí hậu là gì?
Tên API thực tế :search
Mô tả cho API thực tế : Đầu vào là một tên thực thể chính xác. Hành động sẽ tìm kiếm tên thực thể này
trên Wikipedia và trả về năm câu đầu tiên nếu nó tồn tại. Nếu không, nó sẽ trả về một số thực thể liên quan để
tìm kiếm tiếp theo. Đầu vào của bạn nên là một json (args json schema): {"entity": string, }
Đối số thực tế : {"entity": "causes of climate change"}
API Dự đoán :search_general
Mô tả cho API dự đoán : Chạy truy vấn qua GoogleSearch và phân tích kết quả. Đầu vào của bạn nên là một json
(args json schema): {"query": string, }
Đối số Dự đoán : {"query": "main causes of climate change"}

Bảng 6: Ví dụ lỗi cho GPT-4 (danh mục: khó đánh giá độ chính xác). Ở đây GPT-4 gọi API Google Search cũng
có thể là một lựa chọn hợp lệ bên cạnh thực tế.

Truy vấn người dùng : Thông tin tư vấn hiện tại cho trạm Union City là gì ?
Tên API thực tế : BART - Advisory information
Mô tả cho API thực tế : API BART cung cấp cho bạn quyền truy cập vào hầu hết tất cả dịch vụ BART
và dữ liệu trạm có sẵn... "required_parameters": [{"name": "cmd", "type": "STRING", "description": "See more
examples http://api.bart.gov/docs/overview/examples.aspx", "default": "bsa"}], "optional_parameters": [{"name": "orig", "type": "STRING", "description": "Optional station filter. Uses 4
character BART station abbreviations (http://api.bart.gov/docs/overview/abbrev.aspx)", "default": ""}]
Đối số thực tế : {"cmd": "bsa", "orig": "UCTY" }
API Dự đoán : BART - Advisory information
Đối số Dự đoán : {"cmd": "bsa", "orig": "UNION" }

Bảng 7: Ví dụ lỗi của Mistral-Instruct (7B) sau tinh chỉnh. Ở đây mô hình ảo tưởng một viết tắt sai cho trạm
mục tiêu do thiếu kiến thức thường thức/thế giới.

--- TRANG 17 ---
Truy vấn người dùng : Các giveaway đang diễn ra cho khóa trò chơi PC trên nền tảng GOG là gì ?
Tên API thực tế : GamerPower - Filter & Group Giveaways
Mô tả cho API thực tế : "Tìm tất cả trò chơi miễn phí, loot và giveaway với API theo dõi giveaway này
được cung cấp bởi GamerPower.com! Truy cập theo chương trình các giveaway tốt nhất trong gaming! Lọc và nhóm
nền tảng và loại giveaway để có kết quả cá nhân hóa.", "required_parameters": [{"name": "platform", "type":
"STRING", "description": "", "default": "epic-games-store.steam.android"}], "optional_parameters": [{"name":
"type", "type": "STRING", "description": "", "default": "game.loot"}]
Đối số thực tế : {"platform": "gog" , "type": "game.key"}
API Dự đoán : GamerPower - Filter & Group Giveaways
Đối số Dự đoán : {"platform": "pc" , "type": "game.key"}

Bảng 8: Ví dụ lỗi của Mistral-Instruct (7B) sau tinh chỉnh. Ở đây mô hình mắc lỗi trong việc hiểu truy vấn người
dùng và dự đoán đối số sai cho "platform".

Truy vấn người dùng : Bạn có thể cung cấp tổng số trận đấu đã chơi giữa Leeds United và Sheffield Wednesday
trong Giải Ngoại hạng Anh không?
Tên API thực tế : Football Dolphin - Head to head statistics
Mô tả cho API thực tế : Api này trả về dữ liệu thống kê về Giải Ngoại hạng Anh. Nhấp vào liên kết để xem tất cả
endpoints trong một ứng dụng web https://football-dolphin-web-app.up.railway.app/. "required_parameters": [{"name": "first_team", "type": "STRING", "description": "**Enter first team from
all available teams:** Arsenal, Aston Villa, Barnsley, Birmingham, Blackburn, Blackpool, Bolton, Bournemouth,
Bradford, Brighton, Burnley, Cardiff, Charlton, Chelsea, Coventry, Crystal Palace, Derby, Everton, Fulham,
Huddersfield, Hull, Ipswich, Leeds, Leicester, Liverpool, Man City, Man United, Middlesbrough, Newcastle,
Norwich, Nott'm Forest, Portsmouth, QPR, Reading, Sheffield United, Sheffield Weds, Southampton, Stoke,
Sunderland, Swansea, Tottenham, Watford, West Brom, West Ham, Wigan, Wimbledon, Wolves", "default":
"Man United"}, {"name": "second_team", "type": "STRING", "description": "**Enter second team from all
available teams:** Arsenal, Aston Villa, Barnsley, Birmingham, Blackburn, Blackpool, Bolton, Bournemouth,
Bradford, Brighton, Burnley, Cardiff, Charlton, Chelsea, Coventry, Crystal Palace, Derby, Everton, Fulham,
Huddersfield, Hull, Ipswich, Leeds , Leicester, Liverpool, Man City, Man United, Middlesbrough, Newcastle,
Norwich, Nott'm Forest, Portsmouth, QPR, Reading, Sheffield United, Sheffield Weds, Southampton, Stoke,
Sunderland, Swansea, Tottenham, Watford, West Brom, West Ham, Wigan, Wimbledon, Wolves", "default":
"Liverpool"}, {"name": "type_of_statistics", "type": "STRING", "description": "**Enter one from available
types of statistics:** full time result, home vs away full time result, result first half and the match,exact number
of goals in the match, goals over, goals under", "default": "full time result"}]
Đối số thực tế : {"first_team": " Leeds ", "second_team": "Sheffield Weds", "type_of_statistics": "full
time result"}
API Dự đoán : Football Dolphin - Head to head statistics
Đối số Dự đoán : {"first_team": " Leeds United ", "second_team": "Sheffield Weds", "type_of_statistics":
"full time result"}

Bảng 9: Ví dụ lỗi của Mistral-Instruct (7B) sau tinh chỉnh. Ở đây mô hình trích xuất đúng thực thể mục tiêu
nhưng không liên kết nó với tên thực thể được hỗ trợ bởi API.

--- TRANG 18 ---
Nhiệm vụ của bạn là trả lời truy vấn của người dùng tốt nhất có thể. Bạn có quyền truy cập vào các công cụ sau mà bạn có thể sử dụng thông qua lời gọi API để
giúp với phản hồi của bạn:
{api_descriptions}
Bây giờ bạn có cơ hội khám phá các API có sẵn. Bạn có thể làm điều này bằng cách 1) tổng hợp một số truy vấn người dùng tự nhiên mà việc gọi
API có thể giúp, và 2) cố gắng phản hồi truy vấn người dùng với sự giúp đỡ của các API. Ở đây, bạn có thể tập trung vào các truy vấn chỉ yêu cầu
gọi API một lần.
Bây giờ, đầu tiên nhập truy vấn người dùng đã tổng hợp của bạn. Bạn nên làm cho truy vấn tự nhiên - ví dụ, cố gắng tránh sử dụng các mô tả API hoặc tên API đã cung cấp
trong truy vấn, vì người dùng không biết bạn có quyền truy cập vào API nào. Ngoài ra, cố gắng làm cho truy vấn cụ thể nhất có thể. Chỉ nhập truy vấn người dùng thôi; ĐỪNG giải quyết truy vấn ngay bây giờ.
Truy vấn Người dùng:
=========
Bây giờ, cố gắng phản hồi truy vấn sử dụng các API có sẵn.
Định dạng bạn sử dụng API là bằng cách chỉ định 1) Action: tên hàm API bạn muốn gọi 2) Action Input: các tham số đầu vào
của lời gọi API ở định dạng chuỗi json. Kết quả của lời gọi API sẽ được trả về bắt đầu với "Observation:". Nhớ rằng bạn
chỉ nên thực hiện một hành động duy nhất tại một thời điểm, ĐỪNG trả về danh sách nhiều hành động.
Lời nhắc:
1) các giá trị duy nhất nên theo sau "Action:" là: {api_names}
2) sử dụng định dạng chuỗi json sau cho các đối số API:
Action Input:
{
"key_1": "value_1",
...
"key_n": "value_n"
}
Nhớ LUÔN sử dụng định dạng sau:
Thought: bạn nên luôn suy nghĩ về những gì cần làm tiếp theo
Action: tên hàm API
Action Input: các tham số đầu vào của lời gọi API ở định dạng chuỗi json
Observation: kết quả trả về của lời gọi API. Đây là những gì tôi sẽ cung cấp cho bạn; bạn không cần lặp lại nó trong phản hồi của mình.
... (Thought/Action/Action Input/Observation này có thể lặp lại N lần)
Thought: Bây giờ tôi biết câu trả lời cuối cùng
Final Answer: phản hồi cho truy vấn người dùng
Bắt đầu! Nhớ rằng phản hồi của bạn không bao giờ nên bắt đầu với "Observation:" vì đó là những gì tôi sẽ cung cấp cho bạn. Một khi bạn
có đủ thông tin, hãy ngay lập tức sử dụng
Thought: Bây giờ tôi biết câu trả lời cuối cùng
Final Answer:
Truy vấn Người dùng (giống như bạn vừa tổng hợp): {query}
=========
Bạn có nghĩ mình đã trả lời thành công truy vấn này cuối cùng không? Phản hồi bằng "Yes" hoặc "No".
=========
Bây giờ bạn biết thêm một chút về API. Bạn có thể tổng hợp một truy vấn người dùng khác để khám phá API thêm một chút và củng cố
hiểu biết của bạn về API, dựa trên những điều bạn đã khám phá về API này. Một lần nữa, chỉ nhập truy vấn người dùng thôi; ĐỪNG
giải quyết truy vấn ngay bây giờ.
Truy vấn Người dùng:
=========
Bây giờ cố gắng giải quyết truy vấn sử dụng API. Nhớ tuân theo cùng định dạng, tức là,
Thought:
Action:
Action Input:
Observation:
Final Answer:

Bảng 10: Prompt cho tự tinh chỉnh với phản hồi thực thi và trí nhớ ngắn hạn.

--- TRANG 19 ---
Dưới đây là các truy vấn bạn đã khám phá và liệu bạn có thành công giải quyết chúng với sự giúp đỡ của API:
{long_term_memory}
Dựa trên những điều này, hãy cố gắng khám phá các truy vấn có thể giúp bạn hiểu API hơn; tránh tổng hợp các truy vấn quá gần với
những cái hiện có.

Bảng 11: Prompt cho trí nhớ dài hạn.

Một trợ lý đang cố gắng phản hồi truy vấn người dùng với sự giúp đỡ của một số API. Các API mà trợ lý có quyền truy cập như sau:
{api_descriptions}
Bây giờ, nhiệm vụ của bạn là đánh giá trợ lý đã làm tốt công việc như thế nào. Kiểm tra cẩn thận các khía cạnh sau của phản hồi của trợ lý:
1) liệu phản hồi có trả lời truy vấn của người dùng một cách thông tin hay không. Ví dụ, nếu các lời gọi API không thành công và agent
không thể tìm thấy câu trả lời cho yêu cầu, bạn nên nói "No."
2) liệu phản hồi có trung thực so với kết quả thực thi của các lời gọi API hay không. Phản hồi không nên bao gồm thông tin không thể được hỗ trợ bởi phản hồi lời gọi API,
3) liệu trợ lý có sử dụng các lời gọi API một cách thích hợp hay không. Ví dụ, trợ lý nên luôn sử dụng các lời gọi API liên quan cho các truy vấn
về thông tin cập nhật hoặc tính toán phức tạp,
Đối với mỗi trong ba khía cạnh, bạn nên nói "Yes" hoặc "No" chỉ ra liệu trợ lý có làm tốt khía cạnh đó hay không, và
giải thích lý do đằng sau đánh giá của bạn. Đầu ra của bạn nên tuân theo định dạng dưới đây, nơi "<explanation>" nên là giải thích thực tế của bạn cho
đánh giá tương ứng:
1) Yes/No. <explanation>
2) Yes/No. <explanation>
3) Yes/No. <explanation>
Bây giờ, truy vấn người dùng là:
{query}
Các lời gọi API của trợ lý và kết quả thực thi tương ứng là:
{chains}
Phản hồi cuối cùng của trợ lý là:
{final_ans}
Bây giờ, đánh giá của bạn là (nhớ tuân theo định dạng trước):

Bảng 12: Prompt cho lọc ví dụ.

Dưới đây bạn sẽ được đưa ra một truy vấn người dùng. Hãy cố gắng diễn giải nó theo một cách khác trong khi bảo tồn ý nghĩa của nó. Truy vấn là:
{query}
Cách diễn giải truy vấn của bạn:
=========
Bạn có thể cố gắng diễn giải nó lại theo một cách mới không? Tránh đưa ra thứ gì đó quá gần với những cái trước đó của bạn. Cách diễn giải của bạn:

Bảng 13: Prompt cho diễn giải truy vấn.

--- TRANG 20 ---
Chỉ số UV ở San Francisco trong 3 ngày tới là gì?
Chỉ số UV ở Sydney trong 3 ngày tới là gì?
Độ ẩm trung bình ở Miami Beach trong 5 ngày tới là gì?
Nhiệt độ trung bình ở Los Angeles trong 3 ngày tới là gì?
Nhiệt độ trung bình ở Miami trong 5 ngày tới là gì?
Nhiệt độ trung bình ở San Francisco trong 3 ngày tới là gì?
Nhiệt độ trung bình ở Sydney trong 10 ngày tới là gì?
Tầm nhìn trung bình ở thành phố New York trong 3 ngày tới là gì?
Tầm nhìn trung bình ở Tokyo trong 4 ngày tới là gì?
Tốc độ gió trung bình ở Chicago trong 7 ngày tới là gì?
Khả năng có mưa ở London trong 3 ngày tới là gì?
Khả năng có mưa ở Los Angeles trong 7 ngày tới là gì?
Khả năng có mưa ở Miami Beach trong 3 ngày tới là gì?
Khả năng có mưa ở San Francisco trong 3 ngày tới là gì?
Khả năng có mưa ở San Francisco trong 5 ngày tới là gì?
Khả năng có mưa ở Seattle trong 7 ngày tới là gì?
Khả năng có mưa ở Sydney trong 5 ngày tới là gì?
Khả năng có mưa ở Tokyo trong 7 ngày tới là gì?
Khả năng có tuyết ở Boston ngày mai là gì?
Khả năng có tuyết ở Chicago trong 5 ngày tới là gì?
Khả năng có tuyết ở Denver trong 3 ngày tới là gì?
Khả năng có tuyết ở thành phố New York trong 5 ngày tới là gì?
Khả năng có tuyết ở Paris trong 3 ngày tới là gì?
Tốc độ gió dự báo ở London trong 5 ngày tới là gì?
Dự báo lượng mưa hàng giờ ở thành phố New York trong 24 giờ tới là gì?
Dự báo lượng mưa hàng giờ ở Seattle trong 24 giờ tới là gì?
Dự báo nhiệt độ hàng giờ cho Chicago ngày mai là gì?
Dự báo nhiệt độ hàng giờ ở Los Angeles trong 24 giờ tới là gì?
Dự báo nhiệt độ hàng giờ ở thành phố New York trong 12 giờ tới là gì?
Dự báo nhiệt độ hàng giờ ở San Francisco trong 12 giờ tới là gì?
Dự báo tốc độ gió hàng giờ ở Miami ngày mai là gì?
Nhiệt độ tối đa ở Sydney trong 7 ngày tới là gì?
Tốc độ gió tối đa ở Los Angeles trong 2 ngày tới là gì?
Thời gian mặt trời mọc và lặn ở Paris ngày mai là gì?
Thời gian mặt trời mọc và lặn ở Tokyo ngày mai là gì?
Phạm vi nhiệt độ ở Sydney trong 3 ngày tới là gì?
Tổng lượng mưa ở Sydney trong 10 ngày tới là gì?
Tổng lượng mưa ở Tokyo trong 7 ngày tới là gì?
Dự báo thời tiết ở Miami trong tuần tới là gì?
Dự báo gió ở San Francisco trong 7 ngày tới là gì?
Nhiệt độ trung bình ở London trong 5 ngày tới sẽ là gì?
Nhiệt độ trung bình ở Seattle trong 7 ngày tới sẽ là gì?
Nhiệt độ trung bình ở Tokyo trong 7 ngày tới sẽ là gì?
Tầm nhìn trung bình ở Miami trong 7 ngày tới sẽ là gì?
Tốc độ gió trung bình ở San Francisco trong 3 ngày tới sẽ là gì?
Khả năng có mưa ở San Diego trong 7 ngày tới sẽ là gì?
Mức độ ẩm ở London trong 5 ngày tới sẽ là gì?
Nhiệt độ tối đa ở Chicago trong 5 ngày tới sẽ là gì?
Nhiệt độ tối đa ở London trong 10 ngày tới sẽ là gì?
Nhiệt độ tối đa ở Los Angeles trong 7 ngày tới sẽ là gì?
Nhiệt độ tối đa ở Madrid trong 7 ngày tới sẽ là gì?
Nhiệt độ tối đa ở Paris trong 7 ngày tới sẽ là gì?
Nhiệt độ tối đa ở Sydney trong 5 ngày tới sẽ là gì?
Nhiệt độ tối đa ở Tokyo trong 10 ngày tới sẽ là gì?
Tốc độ gió tối đa ở San Francisco trong 5 ngày tới sẽ là gì?
Thời tiết ở thành phố New York trong 5 ngày tới sẽ như thế nào?
Dự báo tốc độ gió ở Boston trong 3 ngày tới sẽ là gì?
Tốc độ gió ở Chicago vào sáng mai sẽ là gì?
Liệu có mưa ở Seattle ngày mai không?
Liệu có tuyết ở Denver tuần tới không?

Bảng 14: Các truy vấn trong tất cả các thử nghiệm đã khám phá (sắp xếp theo tiền tố). Thiết lập: STE (cả trí nhớ ngắn hạn và dài hạn).

--- TRANG 21 ---
Liệu có mưa ở London ngày mai không?
Nhiệt độ ở San Francisco vào thứ Năm sẽ là gì?
Thời tiết ở Barcelona vào thứ Sáu sẽ như thế nào?
Thời tiết ở Berlin vào thứ Tư tới sẽ như thế nào?
Thời tiết ở Chicago trong 7 ngày tới sẽ như thế nào?
Thời tiết ở Denver trong 5 ngày tới sẽ như thế nào?
Thời tiết ở London trong 2 ngày tới sẽ như thế nào?
Thời tiết ở London trong 3 ngày tới sẽ như thế nào?
Thời tiết ở London trong 5 ngày tới sẽ như thế nào?
Thời tiết ở London vào thứ Sáu tới sẽ như thế nào?
Thời tiết ở London vào Chủ nhật sẽ như thế nào?
Thời tiết ở Los Angeles trong 3 ngày tới sẽ như thế nào?
Thời tiết ở Los Angeles vào thứ Hai sẽ như thế nào?
Thời tiết ở Madrid trong 5 ngày tới sẽ như thế nào?
Thời tiết ở Madrid vào Chủ nhật sẽ như thế nào?
Thời tiết ở Miami trong 7 ngày tới sẽ như thế nào?
Thời tiết ở Miami vào thứ Sáu sẽ như thế nào?
Thời tiết ở Miami vào Chủ nhật sẽ như thế nào?
Thời tiết ở Miami vào thứ Tư sẽ như thế nào?
Thời tiết ở thành phố New York trong 5 ngày tới sẽ như thế nào?
Thời tiết ở thành phố New York vào thứ Bảy tới sẽ như thế nào?
Thời tiết ở thành phố New York vào thứ Năm sẽ như thế nào?
Thời tiết ở Paris trong 3 ngày tới sẽ như thế nào?
Thời tiết ở Paris vào thứ Hai tới sẽ như thế nào?
Thời tiết ở Paris vào Chủ nhật sẽ như thế nào?
Thời tiết ở San Antonio trong 10 ngày tới sẽ như thế nào?
Thời tiết ở San Francisco vào thứ Sáu sẽ như thế nào?
Thời tiết ở San Francisco vào thứ Bảy sẽ như thế nào?
Thời tiết ở Seattle trong 5 ngày tới sẽ như thế nào?
Thời tiết ở Sydney trong 5 ngày tới sẽ như thế nào?
Thời tiết ở Sydney trong 7 ngày tới sẽ như thế nào?
Thời tiết ở Sydney vào thứ Hai tới sẽ như thế nào?
Thời tiết ở Sydney vào thứ Ba tới sẽ như thế nào?
Thời tiết ở Sydney vào thứ Tư tới sẽ như thế nào?
Thời tiết ở Sydney vào thứ Hai sẽ như thế nào?
Thời tiết ở Sydney vào thứ Bảy sẽ như thế nào?
Thời tiết ở Tokyo trong 3 ngày tới sẽ như thế nào?
Thời tiết ở Tokyo trong 4 ngày tới sẽ như thế nào?
Thời tiết ở Tokyo trong 7 ngày tới sẽ như thế nào?
Thời tiết ở Tokyo vào Chủ nhật tới sẽ như thế nào?
Thời tiết ở Tokyo vào thứ Ba tới sẽ như thế nào?
Thời tiết ở Tokyo vào thứ Sáu sẽ như thế nào?
Thời tiết ở Tokyo vào thứ Hai sẽ như thế nào?
Thời tiết ở Tokyo vào thứ Bảy sẽ như thế nào?
Thời tiết ở Tokyo vào Chủ nhật sẽ như thế nào?
Thời tiết ở Tokyo vào thứ Năm sẽ như thế nào?
Thời tiết ở Tokyo vào thứ Tư sẽ như thế nào?
Thời tiết ở Barcelona trong 7 ngày tới sẽ như thế nào?
Thời tiết ở Miami vào thứ Hai tới sẽ như thế nào?
Thời tiết ở Paris vào thứ Hai sẽ như thế nào?
Thời tiết ở Sydney vào thứ Tư tới sẽ như thế nào?
Thời tiết ở Tokyo ngày mai sẽ như thế nào?
Liệu có mây ở Tokyo ngày mai không?
Liệu có nóng ở Miami tuần tới không?
Liệu có mưa ở Seattle ngày mai không?
Liệu có nắng ở Madrid ngày mai không?
Liệu có nắng ở San Francisco vào thứ Năm tới không?
Liệu có nắng ở San Francisco vào thứ Tư không?
Liệu có mưa ở London trong 3 ngày tới không?
Liệu có mưa ở Sydney ngày mai không?

Bảng 15: Các truy vấn trong tất cả các thử nghiệm đã khám phá (sắp xếp theo tiền tố). Thiết lập: STE không có trí nhớ ngắn hạn.

--- TRANG 22 ---
Tôi có thể lấy dự báo thời tiết hàng giờ cho ngày mai ở thành phố New York không?
Bạn có thể cung cấp dự báo thời tiết cho 10 ngày tới ở Los Angeles không?
Dự báo thời tiết cho tuần tới chính xác như thế nào?
Tầm nhìn trung bình ở Paris trong 7 ngày tới là gì?
Khả năng có mưa ở Los Angeles ngày mai là gì?
Khả năng có mưa ở Miami trong 3 ngày tới là gì?
Khả năng có mưa ở Seattle ngày mai là gì?
Khả năng có mưa ở Sydney trong 7 ngày tới là gì?
Khả năng có mưa ở Tokyo trong 5 ngày tới là gì?
Nhiệt độ hiện tại ở Los Angeles là gì?
Nhiệt độ hiện tại ở thành phố New York là gì?
Điều kiện thời tiết hiện tại ở Sydney là gì?
Nhiệt độ dự báo cho Paris ngày mai là gì?
Nhiệt độ dự báo ở Los Angeles trong 10 ngày tới là gì?
Thời tiết dự báo cho thành phố New York trong 5 ngày tới là gì?
Tốc độ gió dự báo cho Los Angeles ngày mai là gì?
Dự báo thời tiết hàng giờ cho Los Angeles ngày mai là gì?
Dự báo thời tiết hàng giờ cho Los Angeles ngày mai là gì?
Dự báo thời tiết cho London ngày mai là gì?
Dự báo thời tiết cho Los Angeles ngày mai là gì?
Dự báo thời tiết cho Tokyo trong 10 ngày tới là gì?
Dự báo thời tiết cho 10 ngày tới ở London là gì?
Dự báo thời tiết cho 2 ngày tới ở London là gì?
Dự báo thời tiết cho 3 ngày tới ở Paris là gì?
Dự báo thời tiết cho 5 ngày tới ở Los Angeles là gì?
Dự báo thời tiết cho ngày mai ở thành phố New York là gì?
Tốc độ gió ở Paris ngày mai là gì?
Độ ẩm trung bình ở Tokyo trong 3 ngày tới sẽ là gì?
Tốc độ gió trung bình ở Seattle trong 3 ngày tới sẽ là gì?
Nhiệt độ tối đa và tối thiểu ở London ngày mai sẽ là gì?
Nhiệt độ tối đa ở London ngày mai sẽ là gì?
Tốc độ gió tối đa ở Paris ngày mai sẽ là gì?
Nhiệt độ ở London trong 3 ngày tới sẽ là gì?
Nhiệt độ ở San Francisco ngày mai sẽ là gì?
Điều kiện thời tiết ở London trong 7 ngày tới sẽ như thế nào?
Dự báo thời tiết cho London ngày mai sẽ là gì?
Dự báo thời tiết cho thành phố New York trong 5 ngày tới sẽ là gì?
Dự báo thời tiết cho Seattle trong 5 ngày tới sẽ là gì?
Dự báo thời tiết cho Sydney tuần tới sẽ là gì?
Dự báo thời tiết cho 5 ngày tới ở thành phố New York sẽ là gì?
Thời tiết ở thành phố New York trong 5 ngày tới sẽ là gì?
Thời tiết ở London trong 3 ngày tới sẽ như thế nào?
Thời tiết ở London trong 7 ngày tới sẽ như thế nào?
Thời tiết ở thành phố New York trong 5 ngày tới sẽ như thế nào?
Thời tiết ở thành phố New York trong 5 ngày tới sẽ như thế nào?
Thời tiết ở thành phố New York trong 5 ngày tới sẽ như thế nào?
Thời tiết ở thành phố New York trong ba ngày tới sẽ như thế nào?
Thời tiết ở thành phố New York trong 5 ngày tới sẽ như thế nào?
Thời tiết ở Paris trong 10 ngày tới sẽ như thế nào?
Thời tiết ở San Francisco trong 5 ngày tới sẽ như thế nào?
Thời tiết ở San Francisco ngày mai sẽ như thế nào?
Thời tiết ở Tokyo trong 7 ngày tới sẽ như thế nào?
Thời tiết ở London trong 5 ngày tới sẽ như thế nào?
Thời tiết ở London trong 7 ngày tới sẽ như thế nào?
Thời tiết ở Los Angeles trong 3 ngày tới sẽ như thế nào?
Thời tiết ở thành phố New York trong 5 ngày tới sẽ như thế nào?
Thời tiết ở thành phố New York trong 5 ngày tới sẽ như thế nào?
Thời tiết ở thành phố New York ngày mai sẽ như thế nào?
Liệu có mưa ở Los Angeles ngày mai không?
Liệu có mưa ở San Francisco ngày mai không?

Bảng 16: Các truy vấn trong tất cả các thử nghiệm đã khám phá (sắp xếp theo tiền tố). Thiết lập: STE không có trí nhớ dài hạn.
