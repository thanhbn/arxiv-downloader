OpenAGI: Khi LLM Gặp Gỡ Các Chuyên Gia Miền

Yingqiang Ge
Đại học Rutgers

Wenyue Hua
Đại học Rutgers

Kai Mei
Đại học Rutgers

Jianchao Ji
Đại học Rutgers

Juntao Tan
Đại học Rutgers

Shuyuan Xu
Đại học Rutgers

Zelong Li
Đại học Rutgers

Yongfeng Zhang∗
Đại học Rutgers

Tóm tắt

Trí tuệ Con người (HI) xuất sắc trong việc kết hợp các kỹ năng cơ bản để giải quyết các nhiệm vụ phức tạp. Khả năng này rất quan trọng đối với Trí tuệ Nhân tạo (AI) và nên được tích hợp vào các Tác nhân AI toàn diện, cho phép chúng khai thác các mô hình chuyên gia để giải quyết nhiệm vụ phức tạp hướng tới Trí tuệ Nhân tạo Tổng quát (AGI). Các Mô hình Ngôn ngữ Lớn (LLM) cho thấy khả năng học tập và lý luận đầy hứa hẹn, và có thể sử dụng hiệu quả các mô hình, công cụ, plugin hoặc API bên ngoài để giải quyết các vấn đề phức tạp. Trong công trình này, chúng tôi giới thiệu OpenAGI, một nền tảng nghiên cứu và phát triển AGI mã nguồn mở được thiết kế để giải quyết các nhiệm vụ thực tế đa bước. Cụ thể, OpenAGI sử dụng chiến lược kép, tích hợp các nhiệm vụ benchmark tiêu chuẩn để đánh giá và đánh giá, và các nhiệm vụ mở bao gồm nhiều mô hình, công cụ, plugin hoặc API có thể mở rộng hơn để giải quyết vấn đề sáng tạo. Các nhiệm vụ được trình bày dưới dạng truy vấn ngôn ngữ tự nhiên cho LLM, sau đó chọn và thực thi các mô hình phù hợp. Chúng tôi cũng đề xuất cơ chế Học Tăng cường từ Phản hồi Nhiệm vụ (RLTF) sử dụng kết quả nhiệm vụ để cải thiện khả năng giải quyết nhiệm vụ của LLM, tạo ra một vòng lặp phản hồi AI tự cải thiện. Mặc dù chúng tôi thừa nhận rằng AGI là một thách thức nghiên cứu rộng lớn và đa diện không có con đường giải pháp được xác định duy nhất, việc tích hợp LLM với các mô hình chuyên gia miền cụ thể, lấy cảm hứng từ việc phản ánh sự pha trộn giữa trí tuệ tổng quát và chuyên môn ở con người, mang lại một cách tiếp cận đầy hứa hẹn hướng tới AGI. Chúng tôi đang mở mã nguồn code, dataset, benchmark, phương pháp đánh giá và demo UI của dự án OpenAGI để thúc đẩy sự tham gia của cộng đồng trong việc phát triển AGI: https://github.com/agiresearch/OpenAGI.

1 Giới thiệu

Việc thu thập và tái sử dụng kỹ năng là một khía cạnh cơ bản của trí tuệ con người cho phép hình thành các kỹ năng phức tạp để giải quyết các vấn đề mới hoặc phức tạp [19,4,57]. Chúng tôi cho rằng trí tuệ máy nên kết hợp khả năng này để tổng hợp các kỹ năng khác nhau bằng cách kết hợp chúng thành các kỹ năng phức tạp để giải quyết nhiệm vụ phức tạp. Trong thuật ngữ khoa học máy tính, mỗi kỹ năng được gọi là một "mô hình" chuyên gia miền - một công cụ, module, mạng, plugin hoặc API có thể tái sử dụng với chức năng được xác định. Các mô hình chuyên gia miền có thể được tổng hợp thành một "kế hoạch" lớn hơn để thực hiện các nhiệm vụ phức tạp hơn. Quá trình tổng hợp mô hình có thể thích ứng với đầu vào hoặc nhiệm vụ, sao cho đối với một nhiệm vụ nhất định, các mô hình được tổng hợp thành kế hoạch phù hợp nhất để giải quyết nhiệm vụ đó. Do đó, các đầu vào hoặc nhiệm vụ khác nhau có thể cần các mô hình tổng hợp khác biệt như một kế hoạch để giải quyết nhiệm vụ.

Những tiến bộ gần đây trong Mô hình Ngôn ngữ Lớn (LLM) đã thể hiện khả năng học tập và lý luận đặc biệt, khiến chúng rất phù hợp để chọn, tổng hợp và thực thi các mô hình chuyên gia bên ngoài để giải quyết các nhiệm vụ phức tạp. Những LLM này, như dòng GPT [32,2], dòng LLaMA [45,44] và dòng T5 [33,8], đã thể hiện hiểu biết sâu sắc về ngôn ngữ tự nhiên và khả năng tạo ra các phản hồi mạch lạc và phù hợp về mặt ngữ cảnh. Điều này đã mở ra những khả năng mới cho việc ứng dụng chúng trong các nhiệm vụ phức tạp liên quan đến dữ liệu đa phương thức, như xử lý hình ảnh và văn bản, cũng như tích hợp kiến thức miền cụ thể. Trong quá trình này, LLM đóng vai trò quan trọng vì chúng có thể hiểu và tạo ra ngôn ngữ tự nhiên, giúp AI hiểu và xử lý tốt hơn các vấn đề khác nhau. Bằng cách tích hợp kiến thức và kỹ năng từ các miền khác nhau, Tổng hợp Mô hình Miền mở (OMS) có tiềm năng thúc đẩy sự phát triển của trí tuệ nhân tạo tổng quát (AGI), cho phép AI giải quyết một loạt đa dạng các vấn đề và nhiệm vụ. Mặc dù thừa nhận tính phức tạp và thiếu con đường xác định hướng tới AGI, sự kết hợp giữa LLM và các mô hình chuyên gia miền cụ thể, lấy cảm hứng từ sự tương tác giữa trí tuệ tổng quát và chuyên môn ở con người, cung cấp một hướng đi đầy hứa hẹn [19]. Tuy nhiên, lĩnh vực nghiên cứu hiện tại, mặc dù có những nỗ lực ban đầu, vẫn gặp một số thách thức đáng kể: 1) Khả năng mở rộng: Một số công trình hiện có sử dụng số lượng mô hình cố định, như WebGPT [25] và ToolFormer [40], dẫn đến khó khăn khi cố gắng mở rộng khả năng của chúng; 2) Lập kế hoạch Nhiệm vụ Phi tuyến: Phần lớn nghiên cứu hiện tại bị giới hạn trong việc giải quyết các nhiệm vụ với giải pháp lập kế hoạch nhiệm vụ tuyến tính [49,18], có nghĩa là mỗi nhiệm vụ con phải được hoàn thành trước khi nhiệm vụ con tiếp theo có thể bắt đầu. Tuy nhiên, lập kế hoạch tuyến tính của các mô hình có thể không đủ để giải quyết các nhiệm vụ phức tạp, bên cạnh đó, nhiều nhiệm vụ liên quan đến nhiều đầu vào đa phương thức; 3) Đánh giá Định lượng: Nhiều công trình hiện có chỉ cung cấp kết quả định tính, như HuggingGPT [42]. Điều này khiến việc đánh giá khả năng lập kế hoạch của LLM trở nên khó khăn để xác định liệu các chiến lược được sử dụng có tối ưu hay không.

Để giảm thiểu những hạn chế trên, chúng tôi phát triển một nền tảng bao gồm một loạt đa dạng các mô hình chuyên gia miền cụ thể và các nhiệm vụ đa bước phức tạp với một hoặc nhiều đầu vào đa phương thức. Hơn nữa, để thúc đẩy sự phát triển và đánh giá dài hạn của cộng đồng về khả năng AGI, chúng tôi mở mã nguồn tất cả code và dataset, và do đó, đặt tên nền tảng này là OpenAGI. Một ví dụ đồ chơi, thể hiện toàn bộ quy trình của OpenAGI, được mô tả trong Hình 1. Cụ thể, 1) một hướng dẫn ngôn ngữ tự nhiên của một nhiệm vụ cụ thể được đưa ra; 2) hướng dẫn được bổ sung bằng prompt được thiết kế thủ công và sau đó được đưa vào LLM để tạo ra một kế hoạch; 3) các mô hình chuyên gia được chọn và tổng hợp dựa trên kế hoạch được tạo ra, và sau đó được thực thi để xử lý các mẫu dữ liệu; 4) khả năng giải quyết nhiệm vụ của LLM có thể được đánh giá bằng cách so sánh giữa đầu ra và nhãn chuẩn hoặc thông qua đánh giá của con người.

OpenAGI thể hiện cách tiếp cận kép để giải quyết các yêu cầu đa dạng - nhiệm vụ benchmark và nhiệm vụ mở. Một mặt, chúng tôi đã tích hợp các nhiệm vụ benchmark, mỗi nhiệm vụ được hỗ trợ bởi các dataset và metric đánh giá cụ thể theo nhiệm vụ. Việc bao gồm này cung cấp cho các nhà nghiên cứu một nền tảng nhất quán để đánh giá và so sánh hiệu suất của các mô hình khác nhau, kích thích cải tiến liên tục và đổi mới cạnh tranh. Đối với các nhiệm vụ benchmark, như được mô tả trong Hình 1, chúng tôi sử dụng một lựa chọn các mô hình chuyên gia có nguồn gốc từ các thư viện uy tín như transformers và diffusers của Hugging Face, cũng như từ các repository GitHub, từ đó dễ dàng tạo điều kiện cho việc mở rộng bộ mô hình của chúng tôi. Ngoài ra, các dataset đã được lựa chọn cẩn thận để phù hợp hoặc tương tự với các dataset huấn luyện của các mô hình tương ứng. Sau đó chúng tôi triển khai nhiều kỹ thuật tăng cường dữ liệu để nâng cao các dataset gốc này, cho phép xây dựng các nhiệm vụ đa bước tinh vi được thiết kế để đánh giá khả năng lập kế hoạch và giải quyết nhiệm vụ của một LLM nhất định. Mặt khác, OpenAGI cũng cung cấp các nhiệm vụ mở sử dụng nhiều mô hình có thể mở rộng. Những nhiệm vụ này mở ra cánh cửa cho sự sáng tạo và giải quyết vấn đề giàu trí tưởng tượng, cho phép khám phá các giải pháp đổi mới có thể không xuất hiện trong các khung nhiệm vụ bị ràng buộc hơn. Đối với các nhiệm vụ mở, như được mô tả trong Hình 2, được thiết kế để đáp ứng một phạm vi rộng hơn các nhu cầu, chúng tôi cũng bao gồm LangChain để cung cấp các mô hình chuyên gia bổ sung, như Google Search, Wikipedia, Wolfram Alpha và nhiều hơn nữa. Thật vậy, việc chỉ dựa vào văn bản đầu vào để học tập là không đủ đối với LLM khi đối mặt với các nhiệm vụ thực tế. Để cải thiện hiệu suất của nó, chúng tôi giới thiệu một cơ chế được gọi là Học Tăng cường từ Phản hồi Nhiệm vụ (RLTF). Cách tiếp cận này tận dụng phản hồi hiệu suất thu được từ các nhiệm vụ sau khi thực thi giải pháp được thiết kế bởi LLM. Do đó, cơ chế RLTF hiệu quả tinh chỉnh chiến lược lập kế hoạch của LLM, dẫn đến một hệ thống nâng cao và thích ứng hơn. Tóm lại, những đóng góp chính của công trình bao gồm:

• Chúng tôi giới thiệu OpenAGI, một nền tảng nghiên cứu AGI, được thiết kế đặc biệt để cung cấp các nhiệm vụ phức tạp, đa bước cùng với các dataset, phương pháp đánh giá và một loạt đa dạng các mô hình có thể mở rộng tương ứng có thể được tổng hợp để giải quyết hiệu quả các nhiệm vụ này. Mục đích của nền tảng này là hỗ trợ trong việc định lượng khả năng lập kế hoạch và giải quyết nhiệm vụ tổng thể của LLM. OpenAGI ôm lấy AGI bằng cách tập trung vào tổng hợp mô hình được điều khiển bởi LLM, (miền mở), chủ yếu sử dụng các mô hình và dataset trên Hugging Face, GitHub và LangChain.

• Chúng tôi đề xuất phương pháp LLM+RLTF cho OpenAGI, tận dụng Mô hình Ngôn ngữ Lớn như một bộ điều khiển để chọn, tổng hợp và thực thi các mô hình chuyên gia bên ngoài khác nhau để giải quyết nhiệm vụ phức tạp. Phản hồi thu được từ các nhiệm vụ này sau đó được sử dụng để tinh chỉnh chiến lược lập kế hoạch của LLM, từ đó nâng cao hiệu suất tổng thể và khả năng giải quyết nhiệm vụ của LLM.

• Chúng tôi đánh giá cả LLM mã nguồn mở và mã nguồn đóng với các quy mô khác nhau dưới các lược đồ học tập khác biệt và quy trình OpenAGI. Phát hiện của chúng tôi cho thấy rằng ngay cả LLM quy mô nhỏ hơn, khi được kết hợp với lược đồ học tập phù hợp như RLTF, có thể có tiềm năng vượt trội hơn các đối thủ cạnh tranh có số lượng tham số mô hình lớn hơn đáng kể.

2 Công trình Liên quan

2.1 Mô hình Ngôn ngữ Lớn và Tác nhân AI

Với sự tiến bộ của kiến trúc transformer có thể song song hóa cao, các mô hình ngôn ngữ được tiền huấn luyện (PLM) đã thể hiện khả năng đáng chú ý trong việc hiểu, tạo ra và thao tác ngôn ngữ tự nhiên [31,24]. Những mô hình này được tiền huấn luyện trên kho dữ liệu văn bản lớn và thường được tinh chỉnh cho các nhiệm vụ downstream cụ thể. Ngay sau đó, PLM được mở rộng quy mô, được biết đến như Mô hình Ngôn ngữ Lớn (LLM) [34,2,27,6,55,45], bao gồm số lượng tham số lớn hơn đáng kể và tận dụng lượng dữ liệu huấn luyện khổng lồ. Do đó, LLM thể hiện khả năng nâng cao để học các mẫu và cấu trúc ngôn ngữ phức tạp, cùng với khả năng lý luận đáng chú ý, dẫn đến hiệu suất vượt trội trên các nhiệm vụ xử lý ngôn ngữ tự nhiên đa dạng [2,45,55,6,5,30,14,52]. Ngoài ưu điểm trên, LLM đôi khi có thể tạo ra các dự đoán có vẻ hợp lý nhưng không chính xác và đối mặt với thách thức khi giải quyết các vấn đề đòi hỏi chuyên môn miền chuyên biệt [23]. Do đó, lĩnh vực mới nổi của Mô hình Ngôn ngữ Tăng cường (ALM) tập trung vào việc giải quyết những hạn chế của LLM thông thường [8,6,2] bằng cách trang bị cho chúng khả năng lý luận nâng cao và khả năng sử dụng tài nguyên bên ngoài [23]. Quá trình lý luận bao gồm việc chia nhỏ các nhiệm vụ phức tạp thành các nhiệm vụ con nhỏ hơn, dễ quản lý hơn có thể được giải quyết độc lập hoặc cộng tác bởi LLM với sự hỗ trợ của các công cụ. Hơn nữa, LLM cũng có thể gọi các công cụ hoặc mô hình bên ngoài để hoàn thành các nhiệm vụ liên quan. Ví dụ, ToolFormer [40] giới thiệu các thẻ API bên ngoài trong chuỗi văn bản, tạo điều kiện cho LLM truy cập các công cụ bên ngoài. Visual ChatGPT [51] kết hợp ChatGPT với Mô hình Nền tảng Thị giác (VFM) như Transformers, ControlNet và Stable Diffusion, hoạt động như một cầu nối giữa người dùng, cho phép họ giao tiếp qua chat và tạo ra hình ảnh. HuggingGPT [42] tích hợp hub Hugging Face với các mô hình cụ thể theo nhiệm vụ xung quanh ChatGPT để giải quyết các nhiệm vụ AI. ChatGPT for Robotics [47] sử dụng ChatGPT cho một loạt rộng các nhiệm vụ robot thông qua kỹ thuật prompt chiến lược. Bên cạnh đó, một số repository GitHub mã nguồn mở liên quan đến chủ đề này, như BabyAGI và AutoGPT. Đáng chú ý, AutoGPT [15] là một tác nhân tự động, được thiết kế để đặt nhiều mục tiêu, chia nhỏ chúng thành các nhiệm vụ liên quan, và lặp lại các nhiệm vụ này cho đến khi các mục tiêu được đạt. Các mô hình ngôn ngữ tăng cường có thể sử dụng những cải tiến này riêng biệt hoặc kết hợp chúng theo một thứ tự cụ thể để hoàn thành nhiệm vụ cụ thể, cuối cùng dẫn đến khả năng tổng quát hóa vượt trội.

Khác với các công trình khác, chúng tôi đề xuất OpenAGI, một nền tảng nghiên cứu và phát triển AGI mã nguồn mở được thiết kế để giải quyết các thách thức thường gặp trong các công trình hiện có, như khả năng mở rộng, lập kế hoạch nhiệm vụ phi tuyến và đánh giá định lượng. Hơn nữa, chúng tôi giới thiệu các phương pháp đổi mới vào lược đồ học tập của LLM, bao gồm Học Tăng cường từ Phản hồi Nhiệm vụ (RLTF) và lập kế hoạch nhiệm vụ phi tuyến, nhằm giải quyết các thách thức về tổng quát hóa ngoài phân phối (OOD), lập kế hoạch nhiệm vụ tối ưu và tự cải thiện của AI (vui lòng xem Mục A.1 trong tài liệu bổ sung để thảo luận mở rộng về những vấn đề này). Chúng tôi hy vọng nền tảng OpenAGI có thể tạo điều kiện cho sự phát triển và đánh giá mở và dài hạn các khả năng AGI trong cộng đồng.

2.2 Học Tăng cường từ Phản hồi Con người (RLHF)

Để căn chỉnh tốt hơn Mô hình Ngôn ngữ Lớn (LLM) với các giá trị con người, Học Tăng cường từ Phản hồi Con người (RLHF) đã được giới thiệu [7,58], tinh chỉnh LLM bằng phản hồi con người được thu thập, hiệu quả nâng cao các tiêu chí căn chỉnh như tính hữu ích, trung thực và vô hại. Về cốt lõi, RLHF triển khai các thuật toán học tăng cường (RL), đặc biệt là Tối ưu hóa Chính sách Gần (PPO) [41], để điều chỉnh LLM theo phản hồi này thông qua một mô hình phần thưởng. Quan trọng là, cách tiếp cận này tích cực liên quan đến giám sát của con người trong quá trình huấn luyện, được minh họa bởi các mô hình đáng chú ý như InstructGPT [27]. Tuy nhiên, hiệu quả của RLHF phụ thuộc vào chất lượng phản hồi từ các người gắn nhãn thành thạo, khiến việc triển khai thực tế trở nên thách thức [13,29]. Do đó, có một yêu cầu cấp thiết để tinh chỉnh khung RLHF nhằm giảm thiểu sự phụ thuộc vào việc gắn nhãn thủ công và khám phá các phương pháp chú thích đổi mới, hiệu quả đảm bảo tính toàn vẹn dữ liệu. So với RLHF, RLTF được đề xuất nhận phản hồi nhiệm vụ để cung cấp thông tin hướng dẫn hướng học tập của LLM, dẫn đến các chiến lược cải tiến và hiệu quả hơn, không đòi hỏi sự can thiệp của con người.

3 Nền tảng OpenAGI

OpenAGI bao gồm một loạt rộng các tính năng được điều chỉnh theo các nhu cầu khác nhau. Một thành phần chính là các nhiệm vụ benchmark của nó, được mô tả chi tiết trong Mục 3.1, một công cụ đặc biệt có giá trị cho các nhà nghiên cứu. Những nhiệm vụ này được trang bị các dataset cụ thể theo nhiệm vụ và metric đánh giá. Điều này khiến các nhà nghiên cứu có thể đánh giá hiệu suất của các LLM khác nhau một cách có cấu trúc và thống nhất, cung cấp cái nhìn sâu sắc về hiệu quả và các lĩnh vực tiềm năng cần cải thiện. Ngoài các nhiệm vụ benchmark, OpenAGI cũng cung cấp các nhiệm vụ mở, được mô tả chi tiết trong Mục 3.2. Những nhiệm vụ này cho phép mức độ sáng tạo và trí tưởng tượng cao hơn, thoát khỏi các ràng buộc thông thường để cho phép nghiên cứu khám phá hơn. Chúng tôi tin rằng sự kết hợp này giữa các nhiệm vụ benchmark có cấu trúc và các nhiệm vụ mở linh hoạt làm cho OpenAGI trở thành một nền tảng mạnh mẽ và linh hoạt có thể đáp ứng một loạt đa dạng các yêu cầu nghiên cứu.

3.1 Nhiệm vụ Benchmark

Đối với các nhiệm vụ benchmark, mục tiêu của chúng tôi là cung cấp cho cộng đồng một công cụ có giá trị để đánh giá khả năng lập kế hoạch của LLM cho các nhiệm vụ phức tạp, đa bước. Cụ thể, thay vì xây dựng các nhiệm vụ phức tạp, đa bước từ đầu, trước tiên chúng tôi khám phá các mô hình chuyên gia miền (Mục 3.1.1) có thể được sử dụng như các khối xây dựng, sau đó giới thiệu cách chúng tôi tạo ra các nhiệm vụ như vậy dựa trên chúng (Mục 3.1.2).

3.1.1 Tập Mô hình Chuyên gia Miền

Bây giờ chúng tôi trình bày các nhiệm vụ miền và các mô hình tương ứng có thể được sử dụng trong nền tảng của chúng tôi. Tập hợp này được thiết kế để linh hoạt, cho phép người dùng dễ dàng tích hợp các nhiệm vụ miền và mô hình của riêng họ. Các nhiệm vụ miền của chúng tôi như sau:

• Mô hình Liên quan đến Ngôn ngữ: Phân tích Cảm xúc phân loại cực tính cảm xúc của một câu nhất định [1]; Tóm tắt Văn bản tạo ra một bản tóm tắt văn bản đại diện cho thông tin quan trọng hoặc liên quan nhất trong nội dung văn bản gốc [20]; Dịch Máy chuyển đổi một câu từ ngôn ngữ nguồn sang ngôn ngữ đích [34]; Điền Mask liên quan đến việc thay thế các từ bị che trong một văn bản nhất định [22]; Trả lời Câu hỏi (QA) cung cấp một câu trả lời văn bản cho một câu hỏi dựa trên ngữ cảnh nhất định [39].

• Mô hình Liên quan đến Thị giác: Phân loại Hình ảnh nhằm hiểu toàn bộ hình ảnh như một tổng thể và gán nó cho một nhãn cụ thể [12]; Phát hiện Đối tượng xác định và định vị các đối tượng cụ thể trong hình ảnh bằng cách phát hiện các thể hiện của chúng thuộc một lớp cụ thể [3]; Tô màu đề cập đến kỹ thuật thêm thông tin màu sắc hợp lý vào các bức ảnh hoặc video đơn sắc [54]; Siêu phân giải Hình ảnh tạo ra hình ảnh độ phân giải cao (HR) từ hình ảnh độ phân giải thấp (LR) [10]; Khử nhiễu Hình ảnh nhằm loại bỏ nhiễu không mong muốn khỏi hình ảnh trong khi bảo toàn các đặc điểm quan trọng của nó [53]; Khử mờ Hình ảnh nhằm khôi phục hình ảnh rõ nét từ hình ảnh đầu vào bị mờ [53].

• Mô hình Thị giác-Ngôn ngữ: Trả lời Câu hỏi Thị giác (VQA) liên quan đến việc trả lời câu hỏi dựa trên hình ảnh [48]; Mô tả Hình ảnh tạo ra mô tả văn bản về nội dung thị giác được mô tả trong hình ảnh; Tạo Hình ảnh từ Văn bản tạo ra hình ảnh từ câu đầu vào hoặc chuỗi từ nhất định [36].

Chi tiết của các mô hình tương ứng được thể hiện trong Bảng A.1, A.2 và A.3 trong tài liệu bổ sung. Sau khi chọn các mô hình chuyên gia miền, việc chọn các dataset thô trở nên đơn giản hơn, với điều kiện chúng ta cần đảm bảo sự căn chỉnh phù hợp giữa các dataset và bộ huấn luyện của các mô hình chuyên gia miền. Các dataset thô được cung cấp như sau: ImageNet-1K [38], Common Objects in Context (COCO) [21], CNN/Daily Mail [26], Stanford Sentiment Treebank (SST2) [28], TextVQA [43], Stanford Question Answering Dataset (SQuAD) [35]. Chi tiết thêm về các dataset này có thể được tìm thấy trong Mục A.2 trong tài liệu bổ sung.

3.1.2 Xây dựng Nhiệm vụ Đa bước và Dataset Tương ứng

Một nhiệm vụ đa bước, như tên gọi, đề cập đến một vấn đề phức tạp không thể được giải quyết trong một bước đơn giản. Nó cần một số quy trình phụ hoặc giai đoạn, mỗi giai đoạn yêu cầu một loại kỹ năng giải quyết vấn đề cụ thể, nói cách khác, mô hình chuyên gia miền. Để xây dựng các nhiệm vụ phức tạp, đa bước như vậy, chúng tôi giới thiệu một số phương pháp tăng cường dữ liệu thường được sử dụng, đó là Gaussian Blur, Gaussian Noise, Grayscale, Low Resolution, Translation, Word Mask, để tăng cường dataset thô. Chi tiết thêm về các phương pháp này có thể được tìm thấy trong Mục A.3 trong tài liệu bổ sung.

Nhằm mục đích nghiên cứu của chúng tôi, chúng tôi đã sắp xếp các nhiệm vụ này thành sáu danh mục chính theo phương thức của đầu vào và đầu ra:

• Hình ảnh vào, hình ảnh ra: Trong những nhiệm vụ này, hình ảnh trải qua nhiều giai đoạn biến đổi. Một ví dụ có thể là nhiệm vụ bao gồm "Khử nhiễu và nâng cao độ phân giải của hình ảnh độ phân giải thấp, nhiều nhiễu". Ở đây, quá trình đa bước bao gồm khử nhiễu hình ảnh tiếp theo là siêu phân giải.

• Hình ảnh vào, văn bản ra: Những nhiệm vụ này thường liên quan đến việc diễn giải nội dung của hình ảnh. Ví dụ, "Phát hiện đối tượng trong hình ảnh và mô tả chúng trong một câu" yêu cầu phát hiện đối tượng tiếp theo là tạo văn bản.

• Văn bản vào, hình ảnh ra: Các nhiệm vụ trong danh mục này có thể bao gồm tạo hình ảnh dựa trên mô tả văn bản, như "Tạo ra một biểu diễn đồ họa của căn phòng được mô tả trong văn bản đã cho", đòi hỏi các bước hiểu văn bản và tạo hình ảnh.

• Văn bản vào, văn bản ra: Những nhiệm vụ này tham gia vào việc biến đổi hoặc diễn giải văn bản. Ví dụ, "Dịch một đoạn văn từ tiếng Anh sang tiếng Đức và tóm tắt nó bằng tiếng Anh" yêu cầu hai bước - dịch và tóm tắt.

• Cặp hình ảnh-văn bản vào, văn bản ra: Những nhiệm vụ này xử lý sự tương tác phức tạp giữa dữ liệu thị giác và văn bản. Ví dụ, "Cho một hình ảnh và một câu hỏi về hình ảnh bằng tiếng Anh, trả lời câu hỏi bằng tiếng Đức." Nhiệm vụ này bao gồm hiểu hình ảnh-văn bản, trả lời câu hỏi và dịch.

• Cặp văn bản-văn bản vào, văn bản ra: Những nhiệm vụ này có thể liên quan đến so sánh, tổng hợp hoặc trích xuất thông tin từ hai đầu vào văn bản. Ví dụ, "Cho hai đánh giá phim bằng tiếng Anh, dịch chúng sang tiếng Đức và cung cấp một bản tóm tắt."

Tổng cộng, chúng tôi đã thiết kế 185 nhiệm vụ đa bước, trong đó 117 nhiệm vụ duy trì cấu trúc nhiệm vụ tuyến tính với các bước theo một chuỗi đơn giản, trong khi 68 nhiệm vụ còn lại thể hiện cấu trúc nhiệm vụ phi tuyến, nơi các bước có thể được thực hiện đồng thời hoặc theo thứ tự phức tạp. Trong số các danh mục này, các nhiệm vụ như Trả lời Câu hỏi (QA) và Trả lời Câu hỏi Thị giác (VQA), liên quan đến nhiều hoặc thậm chí nhiều đầu vào đa phương thức, đặc biệt phức tạp và thách thức các giải pháp lập kế hoạch nhiệm vụ tuyến tính đơn giản. Để có cái nhìn toàn diện, chúng tôi cung cấp các ví dụ nhiệm vụ và các mẫu dữ liệu đầu vào và đầu ra của chúng trong Bảng A.4 của tài liệu bổ sung. Ngoài ra, danh sách đầy đủ các mô tả nhiệm vụ, kèm theo mức độ khó ước tính của chúng, có thể được tìm thấy trong Bảng A.5 trong tài liệu bổ sung.

3.1.3 Metric Đánh giá

Cho rằng các nhiệm vụ benchmark của OpenAGI bao gồm một loạt đa dạng các nhiệm vụ miền với dữ liệu đa phương thức, chúng tôi phân loại chúng theo các nhiệm vụ miền cũng như loại đầu vào và đầu ra. Sau đó chúng tôi đánh giá hiệu suất của chúng bằng ba metric sau dựa trên danh mục của chúng: CLIP Score [16], BERT Score [56] và ViT Score (chi tiết thêm có thể được tìm thấy trong tài liệu bổ sung). Cụ thể, chúng tôi chỉ sử dụng CLIP Score cho các nhiệm vụ dựa trên Tạo Hình ảnh từ Văn bản, BERT Score được sử dụng để đánh giá các nhiệm vụ có đầu ra văn bản, và ViT score được áp dụng để đo độ tương tự hình ảnh cho các nhiệm vụ còn lại có đầu ra hình ảnh. Chúng tôi cũng chuẩn hóa điểm BERT và CLIP.

3.2 Nhiệm vụ Mở

Các nhiệm vụ mở đòi hỏi mức độ khả năng sáng tạo và trí tưởng tượng cao, vì chúng lệch khỏi các ràng buộc thông thường để kích thích nghiên cứu khám phá hơn. Những nhiệm vụ này được thiết kế để đáp ứng một phạm vi rộng các nhu cầu, như được minh họa trong Hình 2. Để đạt được điều này, LangChain được tích hợp để cung cấp các mô hình chuyên gia bổ sung từ các nguồn nổi tiếng như Google Search, Wikipedia, Wolfram Alpha và nhiều hơn nữa. Quan trọng là, những mô hình này cung cấp khả năng mở rộng, đảm bảo rằng các nhiệm vụ mở không bị giới hạn trong các hướng dẫn hoặc metric hiệu suất cụ thể. Để minh họa quá trình này, Hình 2 làm rõ cách OpenAGI được hướng dẫn tạo ra một bức tranh Trung Quốc truyền thống với "Gao Shan Liu Shui" (dịch là "Núi Cao và Nước Chảy" trong tiếng Anh) làm chủ đề. Quá trình được làm phong phú thêm với việc bổ sung một bài thơ cổ Trung Quốc được tạo ra và một bản nhạc hài hòa với bức tranh. Để hiệu quả thực hiện theo hướng dẫn này, OpenAGI đầu tiên tiến hành tìm kiếm trực tuyến để hiểu câu chuyện lịch sử của "Gao Shan Liu Shui". Tiếp theo, bức tranh, bài thơ và âm nhạc được tạo ra theo cách từng bước, tận dụng sự hợp tác giữa các mô hình ngôn ngữ mở rộng và các mô hình chuyên gia miền cụ thể. Sản phẩm cuối cùng - một tổng thể nghệ thuật mạch lạc gồm tranh, thơ và nhạc - thành công cộng hưởng với câu chuyện cổ xưa cơ bản, thể hiện hiệu quả của cách tiếp cận này trong các nhiệm vụ mở. Nhiều ví dụ khác được cung cấp trong tài liệu bổ sung.

4 Học Tăng cường từ Phản hồi Nhiệm vụ (RLTF)

Mặc dù học chỉ từ văn bản đầu vào là một phương pháp mạnh mẽ để huấn luyện LLM, nhưng nó không đủ để xử lý các nhiệm vụ thực tế đòi hỏi hiểu biết sâu hơn về ngữ cảnh và môi trường. Một phương pháp tiềm năng để cải thiện khả năng của LLM là tích hợp các kỹ thuật học tăng cường (RL). Bằng cách tận dụng điểm mạnh của RL, LLM có thể thu được thêm cái nhìn sâu sắc từ kinh nghiệm thử và sai. Điều này dẫn đến các mô hình mạnh mẽ và thích ứng hơn, đặc biệt trong các tình huống có ít dữ liệu được gắn nhãn hoặc khi các nhiệm vụ liên quan đến tương tác vật lý. Trong công trình này, chúng tôi đề xuất Học Tăng cường từ Phản hồi Nhiệm vụ (RLTF), được thể hiện trong Hình 3, sử dụng phản hồi nhiệm vụ để cung cấp thêm thông tin hướng dẫn hướng học tập của LLM, dẫn đến các chiến lược cải thiện và hiệu quả hơn. Chúng tôi chọn sử dụng REINFORCE [50] trong công trình này và chi tiết thêm về thuật toán được cung cấp trong Mục A.6 trong tài liệu bổ sung.

5 Thí nghiệm

5.1 LLM Nền tảng

• GPT-3.5-turbo. Dòng GPT (Generative Pre-trained Transformer) [2] bao gồm các mô hình ngôn ngữ tiên tiến. Trong công trình này, chúng tôi sử dụng snapshot GPT-3.5-turbo-0301.

• Claude-2. Claude-2 [9] là một LLM transformer được huấn luyện với học không giám sát và RLHF.

• GPT-4. GPT-4 là phiên bản tiếp theo của GPT-3.5, mạnh mẽ hơn các phiên bản trước. Trong công trình này, chúng tôi sử dụng snapshot GPT-4-0613.

• Flan-T5-Large. Flan-T5 [8] là một dòng mô hình ngôn ngữ được tinh chỉnh bằng kỹ thuật gọi là tinh chỉnh hướng dẫn. Flan-T5-Large có 770 triệu tham số.

• Vicuna-7B. Vicuna [5] là một chatbot mã nguồn mở được huấn luyện bằng cách tinh chỉnh mô hình LLaMA [45] với các cuộc trò chuyện được chia sẻ bởi người dùng. Trong công trình này, chúng tôi sử dụng mô hình Vicuna kích thước 7 tỷ tham số.

• LLaMA-2. LLaMA-2 [46] là phiên bản kế thừa của mô hình LLaMA gốc, nó mạnh mẽ hơn đáng kể. Trong công trình này, chúng tôi sử dụng mô hình kích thước 13 tỷ tham số.

Tổng thể, chúng tôi bao gồm ba LLM mã nguồn đóng (GPT-3.5-turbo, Claude-2 và GPT-4) cũng như ba LLM mã nguồn mở (Flan-T5-Large, Vicuna-7B và LLaMA-2-13B).

5.2 Lược đồ Học tập của LLM

Chúng tôi sử dụng các lược đồ học tập LLM sau để thử nghiệm.

• Học Zero-shot (Zero) trực tiếp đưa prompt vào LLM.

• Học Few-shot (Few) trình bày một tập hợp các minh chứng chất lượng cao, mỗi minh chứng bao gồm cả đầu vào và đầu ra mong muốn, trên nhiệm vụ đích. Khi mô hình đầu tiên thấy các ví dụ tốt, nó có thể hiểu rõ hơn ý định của con người và tiêu chí cho loại câu trả lời nào được mong muốn.

• Fine-tuning liên quan đến việc sử dụng các mẫu dữ liệu được gắn nhãn thủ công như tín hiệu huấn luyện bổ sung để tinh chỉnh và thích ứng LLM được tiền huấn luyện cho các nhiệm vụ hoặc miền cụ thể.

• RLTF là phương pháp được đề xuất của chúng tôi trong Mục 4, sử dụng thêm phương pháp RL để điều chỉnh LLM được tinh chỉnh với dữ liệu được gắn nhãn bởi con người.

Chúng tôi sử dụng Low-Rank Adaptation (LoRA) [17] để tối ưu hóa tất cả LLM mã nguồn mở trên cả lược đồ học tập Fine-tuning và RLTF.

5.3 Trình Phân tích Giải pháp Lập kế hoạch

Để chuyển đổi đầu ra LLM gốc thành giải pháp lập kế hoạch nhiệm vụ khả thi, chúng tôi sử dụng một trình phân tích được xây dựng trên GPT-3.5. Prompt chúng tôi sử dụng như sau: "Bạn là một trình trích xuất cụm từ khóa có thể trích xuất tên module tiềm năng từ ngữ cảnh đã cho. Bạn đã biết tất cả tên module trong danh sách module đầy đủ. Danh sách module đầy đủ là: [Image Classification, Colorization, Object Detection, Image Deblurring, Image Denoising, Image Super Resolution, Image Captioning, Text to Image Generation, Visual Question Answering, Sentiment Analysis, Question Answering, Text Summarization, Machine Translation]. Cho ngữ cảnh sau: '{}'. Vui lòng trích xuất một chuỗi module từ ngữ cảnh này và loại bỏ tên module không tồn tại trong danh sách module đầy đủ khỏi chuỗi này. Xuất chuỗi module sau khi lọc theo định dạng 'module: module1, module: module2, module: module3, v.v...'." Khi prompt này được thực thi trên đầu ra văn bản gốc của LLM, một giải pháp lập kế hoạch nhiệm vụ sẽ được tạo ra bao gồm một giải pháp đa bước của vấn đề.

5.4 Dataset

Xem xét thực tế rằng số lượng không cân bằng của các nhiệm vụ với các phương thức đầu vào và đầu ra khác nhau có thể dẫn đến kết quả đo lường lệch, chúng tôi chọn các nhiệm vụ trong OpenAGI để tạo thành tập huấn luyện. Cụ thể, chúng tôi ngẫu nhiên chọn 10% nhiệm vụ, cùng với các dataset tương ứng của chúng, dựa trên phương thức đầu vào và đầu ra cho mục đích huấn luyện. Đối với few-shot, fine-tuning và RLTF, chúng tôi cung cấp các giải pháp khả thi được tuyển chọn thủ công như nhãn chuẩn. Trong trường hợp RLTF, chúng tôi sử dụng checkpoint fine-tuning như một khởi tạo hợp lý cho LLM và sử dụng tìm kiếm beam có ràng buộc [11,37] để giảm khả năng tạo ra các giải pháp không khả thi (chi tiết có thể được tìm thấy trong Mục A.7 trong tài liệu bổ sung). Hơn nữa, chúng tôi chọn thêm 10% nhiệm vụ, tuân theo cùng tiêu chí lựa chọn như đã đề cập ở trên, để phục vụ như tập kiểm tra.

5.5 Phân tích Thí nghiệm

Kết quả thí nghiệm chính được lập bảng trong Bảng 1 và 2, thể hiện kết quả cho LLM mã nguồn đóng và mã nguồn mở, tương ứng. Hiệu suất tổng thể được tính như trung bình của điểm CLIP, BERT và ViT. Ở đây, chỉ có mô tả nhiệm vụ của các nhiệm vụ benchmark được đưa vào LLM (thông tin bổ sung, như prompt đầu vào và đầu ra của LLM, được cung cấp trong Hình A.4 và A.5 trong tài liệu bổ sung). Nói chung, LLM mã nguồn đóng thể hiện hiệu suất vượt trội trên các nhiệm vụ OpenAGI, với GPT-4 dẫn đầu trong cả kịch bản zero- và few-shot. Trong danh mục mã nguồn mở, LLaMA-2-13B dẫn đầu, liên tục đạt kết quả hàng đầu trên các lược đồ học tập khác nhau - hiệu suất có thể bị ảnh hưởng bởi kích thước mô hình lớn hơn. Đáng chú ý, LLM mã nguồn mở hưởng lợi đáng kể từ các phương pháp điều chỉnh, đặc biệt là Fine-tuning và RLTF. Những phương pháp này đánh dấu những cải thiện đáng chú ý cho Flan-T5-Large, Vicuna-7B và LLaMA-2-13B khi so sánh với lược đồ học tập zero-shot và few-shot. Thực tế, mỗi mô hình mã nguồn mở này đều đạt đỉnh cao dưới cách tiếp cận RLTF. Kết luận, với điều chỉnh RLTF, hiệu suất của LLaMA-2-13B tiếp cận của GPT-3.5, minh họa tiềm năng của nó.

5.6 Hiệu ứng của Prompt

Chúng tôi thiết kế hai loại prompt kết hợp với các mức độ mô tả mô hình khác nhau để kiểm tra hiệu suất zero-shot của LLM. Đầu tiên, Prompt-1, chỉ kết hợp mô tả nhiệm vụ với tên mô hình, trong khi thứ hai, Prompt-2, tích hợp mô tả nhiệm vụ với mô tả mô hình toàn diện, chi tiết cách sử dụng mô hình, loại đầu vào và đầu ra (thông tin bổ sung về hai prompt này được cung cấp trong Hình A.6 trong tài liệu bổ sung). Chúng tôi phân tích kết quả trong Bảng 3 và 4 kết hợp với kết quả zero-shot trước đó trong Bảng 1 và 2. So với prompt gốc chỉ sử dụng mô tả nhiệm vụ để tạo ra kết quả trong Bảng 1 và 2, rõ ràng là trong hầu hết các trường hợp, LLM mã nguồn đóng, như dòng GPT và Claude-2, có xu hướng vượt trội khi được cung cấp thông tin chi tiết liên quan đến mô hình như thấy trong Prompt-1 và Prompt-2. Ngược lại, LLM mã nguồn mở, có khả năng hiểu và lý luận có thể yếu hơn những mô hình mã nguồn đóng khổng lồ, dường như bị dẫn lầm bởi các chi tiết mơ hồ trong Prompt-1 và Prompt-2 trong quá trình lựa chọn mô hình. Tổng thể, prompt chi tiết có thể hỗ trợ cải thiện hiệu suất zero-shot ở một mức độ nhất định, tùy thuộc vào mô hình cụ thể. Tuy nhiên, chúng có thể không mạnh mẽ như các kịch bản huấn luyện khác đối với LLM kích thước nhỏ hơn, như fine-tuning hoặc RLTF.

5.7 Nghiên cứu Trường hợp về Lập kế hoạch Phi tuyến

Chúng tôi đánh giá định tính khả năng lập kế hoạch nhiệm vụ phi tuyến của LLM. Hình 4 minh họa phản hồi của GPT-3.5, Vicuna-7B và Flan-T5-Large với Prompt-2. Mô tả nhiệm vụ đã cho yêu cầu LLM trả lời một truy vấn được đặt bằng tiếng Anh về một hình ảnh nhiều nhiễu, mờ và thang độ xám đã cho bằng tiếng Đức. Có thể quan sát từ kết quả rằng hiệu suất của các mô hình khác nhau đáng kể. Flan-T5-Large, chẳng hạn, thể hiện sự hiểu biết gặp khó khăn về truy vấn, trong khi câu trả lời của Vicuna-7B kết hợp tất cả các mô hình được cung cấp trong nỗ lực giải quyết nhiệm vụ. GPT-3.5 thành công hiểu nhiệm vụ và do đó đưa ra một kế hoạch hợp lý. Kế hoạch được tạo ra bởi mô hình này đặc biệt phi tuyến, và nó hướng dẫn sử dụng mô hình Trả lời Câu hỏi Thị giác (VQA) với truy vấn tiếng Anh và hình ảnh đã xử lý như đầu vào trong bước 1 và 2 để hoàn thành nhiệm vụ. Tương tự, một kịch bản nhiệm vụ khác được thể hiện trong Hình 2, đây là một nhiệm vụ mở với GPT được hướng dẫn tạo ra một bức tranh theo phong cách truyền thống Trung Quốc mô tả "Gao Shan Liu Shui". Ban đầu, GPT dường như thiếu hiểu biết về những gì tạo nên một bức tranh phong cách truyền thống Trung Quốc và nó cũng không quen thuộc với khái niệm "Gao Shan Liu Shui". Như một biện pháp khắc phục, GPT sử dụng tìm kiếm Google trong hai bước đầu tiên để thu thập thông tin về những chủ đề không quen thuộc này. Tiếp theo, nó tích hợp thông tin được lấy để xây dựng một prompt toàn diện hướng dẫn mô hình Tạo Hình ảnh từ Văn bản tạo ra tác phẩm nghệ thuật mong muốn.

6 Kết luận và Công việc Tương lai

Trong công trình này, chúng tôi giới thiệu OpenAGI, một nền tảng nghiên cứu AGI mã nguồn mở được thiết kế để tạo điều kiện phát triển và đánh giá LLM trong việc giải quyết các nhiệm vụ phức tạp, đa bước thông qua thao tác các mô hình chuyên gia miền khác nhau, công cụ, plugin hoặc API. OpenAGI cung cấp một loạt rộng các nhiệm vụ, mô hình, dataset, benchmark và phương pháp đánh giá. Chúng tôi cũng đề xuất phương pháp LLM+RLTF, kết hợp LLM với học tăng cường để tối ưu hóa hiệu suất giải quyết nhiệm vụ. Việc đánh giá các LLM khác nhau sử dụng quy trình OpenAGI và các lược đồ học tập khác nhau chứng minh rằng LLM quy mô nhỏ hơn có thể vượt trội hơn các mô hình lớn hơn khi kết hợp với cách tiếp cận học tập phù hợp, như RLTF. Trong tương lai, chúng tôi hướng tới khám phá 1) Tác nhân Có con người trong vòng lặp, nơi LLM có thể nhắc nhở các chuyên gia con người để có câu trả lời như một bước của kế hoạch giải quyết nhiệm vụ khi không có mô hình phù hợp, do đó cho phép hợp tác Người-AI tốt hơn; 2) Tác nhân Đáng tin cậy, đảm bảo sự an toàn và tiêu chuẩn đạo đức của tác nhân trong quá trình giải quyết nhiệm vụ; và 3) Tác nhân Tự cải thiện, cho phép tạo nhiệm vụ và huấn luyện tự động tạo điều kiện cho OpenAGI trong việc khám phá nhiệm vụ độc lập, trao quyền cho sự tự phản ánh, tự nhắc nhở và tự cải thiện của các tác nhân thông minh.
