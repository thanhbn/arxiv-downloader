# 2311.05657.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/agent/2311.05657.pdf
# Kích thước file: 1386086 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Hội nghị chính ACL 2024
Agent LUMOS:
Huấn luyện Thống nhất và Mô-đun cho các Agent Ngôn ngữ Mã nguồn Mở
Da Yin♡*Faeze Brahman♠Abhilasha Ravichander♠Khyathi Chandu♠
Kai-Wei Chang♡Yejin Choi♠♢Bill Yuchen Lin♠
♠Viện Allen về AI
♡UCLA♢Đại học Washington
https://allenai.github.io/lumos/
da.yin@cs.ucla.edu yuchenl@allenai.org
Tóm tắt
Các agent mã nguồn đóng gặp phải một số vấn đề
như thiếu khả năng chi trả, tính minh bạch,
và khả năng tái tạo, đặc biệt trên các tác vụ tương tác
phức tạp. Điều này thúc đẩy việc phát triển
các lựa chọn thay thế mã nguồn mở. Chúng tôi giới thiệu
LUMOS, một trong những khung làm việc đầu tiên để huấn
luyện các agent dựa trên LLM mã nguồn mở. LUMOS
có một kiến trúc có thể học, thống nhất và mô-đun
với một mô-đun lập kế hoạch học cách tạo ra các mục tiêu con
cấp cao, và một mô-đun định căn được huấn luyện để dịch
chúng thành các hành động sử dụng các công cụ khác nhau
trong mô-đun thực thi. Thiết kế này cho phép nâng cấp
mô-đun và khả năng áp dụng rộng rãi hơn cho các tác vụ
tương tác đa dạng. Để thúc đẩy việc học agent có thể tổng
quát hóa, chúng tôi thu thập các chú thích huấn luyện quy
mô lớn, thống nhất và chất lượng cao được dẫn xuất từ
các lý luận chân lý nền đa dạng qua nhiều tác vụ tương tác
phức tạp khác nhau. Trên 9 bộ dữ liệu, LUMOS thể hiện
một số lợi thế chính: (1) LUMOS vượt trội so với nhiều
agent mã nguồn mở lớn hơn trên các bộ dữ liệu được giữ lại
(không được sử dụng để huấn luyện) cho mỗi loại tác vụ.
LUMOS thậm chí còn vượt qua các agent GPT trên các tác vụ
QA và web; (2) LUMOS vượt trội so với các agent mã nguồn
mở được tạo ra bởi chuỗi suy nghĩ và huấn luyện tích hợp
không mô-đun; và (3) LUMOS hiệu quả tổng quát hóa cho
các tác vụ chưa thấy, vượt trội so với các agent quy mô 33B
và các agent chuyên biệt về miền.

1 Giới thiệu
Các agent ngôn ngữ thực hiện các hành động và tương tác với
các công cụ hoặc môi trường bên ngoài, phục vụ cho một mục tiêu.
Chúng đã phát triển thành những yếu tố quan trọng của các hệ thống AI
nhắm vào việc giải quyết các tác vụ tương tác phức tạp.
Các tác vụ này thường đòi hỏi các agent phải thực hiện việc lập
kế hoạch tầm xa và lý luận tương tác, và có thể dao động từ QA
(Yang et al., 2018; Geva et al., 2021), đến các tác vụ web (Deng et al., 2023; Zhou et al.,

*Công việc được thực hiện trong thời gian thực tập của Da tại AI2. Mã nguồn:
https://github.com/allenai/lumos. Mô hình & Dữ liệu:
https://huggingface.co/ai2lumos

QA Phức tạp
Toán học
Duyệt Web Trò chơi Văn bản
Lập trình Tương tác Đa phương tiện
…
Các Tác vụ Tương tác Phức tạp Đa dạng Chúng ta có thể huấn luyện agent với dữ liệu mở và LLM không?
Có, chúng tôi giới thiệu 🪄 Lumos!
LLM Đóng (GPT-3.5/4) như bộ điều khiển cho agent?
Nhược điểm: Đắt đỏ, hộp đen, không tái tạo được, ít có thể kiểm soát, v.v.
🪄 Lumos
Một khung huấn luyện thống nhất, mô-đun và mở
Dựa trên LLAMA-2-7B/13B ✅ Có khả năng chi trả ✅ Minh bạch
✅ Dễ cập nhật ✅ Tổng quát cho tác vụ ✅ Có thể tái tạo

Hình 1: LUMOS là một khung huấn luyện agent thống nhất, mô-đun và mã nguồn mở cho phép tổng quát hóa hiệu quả qua các tác vụ trong khi dễ dàng được cập nhật. Nó cũng có những lợi thế so với các agent mã nguồn đóng về khía cạnh khả năng chi trả, tính minh bạch và khả năng tái tạo.

2023), toán học (Cobbe et al., 2021), và lý luận đa phương tiện (Schwenk et al., 2022; Lu et al., 2022).
Các khung agent trước đây (Yao et al., 2022b; Shinn et al., 2023; Lin et al., 2023; Lu et al., 2023; Liu et al., 2023c) chủ yếu dựa vào các API mô hình ngôn ngữ lớn (LLM) mã nguồn đóng như GPT-4 và ChatGPT (OpenAI, 2023a, 2022). Mặc dù mạnh mẽ, chúng có thể cực kỳ đắt đỏ, đặc biệt đối với các tác vụ có ngữ cảnh dài như các tác vụ web (bao gồm việc mã hóa mã HTML dài). Hơn nữa, việc thiếu tính minh bạch trong các LLM mã nguồn đóng cản trở sự hiểu biết khoa học về kiến trúc và hiệu quả của chúng, và cung cấp khả năng tái tạo và kiểm soát hạn chế đối với hành vi của chúng. Chúng tôi cho rằng việc phụ thuộc quá mức vào các agent dựa trên LLM mã nguồn đóng không có lợi cho sự phát triển của nghiên cứu về các agent ngôn ngữ.

Trong bài báo này, chúng tôi đề xuất LUMOS, một khung agent ngôn ngữ có thể tổng quát hóa thông qua Huấn luyện Thống nhất, Mô-đun và Mã nguồn Mở. LUMOS sử dụng một kiến trúc thống nhất và mô-đun có thể áp dụng rộng rãi cho các tác vụ tương tác phức tạp: một mô-đun lập kế hoạch ♂list-alt, một mô-đun định căn ⚛, và một mô-arXiv:2311.05657v3 [cs.AI] 10 Jul 2024

--- TRANG 2 ---
📝 s1: Tìm kiếm chuyến bay từ honolulu đến nyc...
⚛ a1-1: Type([box-id], HNL) ⚒ ⇒ Trình duyệt
⚛ a1-2: Type([box-id], JFK) ⚒ ⇒ Trình duyệt
⚛ a1-3: Click([button-id]) ⚒ ⇒ Trình duyệt
📝 s2: Đặt bộ lọc để giữ ... giá ≤ 1300
⚛ .... ⚒
....
📝 s3: Đặt bộ lọc để giữ hạng thương gia cao cấp Lumos-OnePass (Lumos-O) Lumos-Iterative (Lumos-I) (lần lặp thứ t)
Mô tả tác vụ: T
Mục tiêu con trước: 
Hành động trước:
Giao diện hành động: I
Mô tả tác vụ: T
Kết quả trước:
Mục tiêu con trước:
📝 Lập kế hoạch
⚛ Định căn
⚒ Thực thi Mô tả tác vụ: T
Giao diện hành động: I Mô tả tác vụ: T
Tất cả Hành động
Kết quả thực thi
Tất cả Mục tiêu con 📝 Lập kế hoạch
⚛ Định căn
⚒ Thực thi Hành động Tiếp theo
Kết quả thực thi
Mục tiêu con Tiếp theo

Tác vụ Đa phương tiện (A-OKVQA): Thiết bị trong tay cô ấy đến từ quốc gia nào?
📝 s1: Xác định thương hiệu của thiết bị ...
⚛ a1: VQA(<img>, Thương hiệu gì..?)
⚒ e1: LLAVA(...) ⇒ Nintendo

Tác vụ Web (Mind2Web): Tìm chuyến bay từ honolulu đến NYC với ngân sách $1,300 cho hạng thương gia cao cấp.
📝 s2: Trả lời quốc gia của Nintendo
⚛ a2: QA(context, Quốc gia nào của ...?)
⚒ e2: LLM(...) ⇒ Nhật Bản

Hình 2: Khung tổng thể của LUMOS. LUMOS được huấn luyện với 56K chú thích huấn luyện chất lượng cao. Chúng tôi đề xuất hai công thức huấn luyện và suy luận agent, LUMOS-O(§2.2) và LUMOS-I(§2.3). LUMOS-O là một công thức hiệu quả cho phép suy luận một lần; LUMOS-I là một công thức thích ứng giúp các agent lập kế hoạch linh hoạt dựa trên phản hồi thực thi. Chúng tôi trình bày hai ví dụ chạy LUMOS-I trong A-OKVQA và Mind2Web.

đun thực thi /tools. Mô-đun lập kế hoạch học cách phân tách các tác vụ phức tạp đa dạng thành một chuỗi các mục tiêu con cấp cao. Mô-đun định căn được huấn luyện để giao tiếp với mô-đun lập kế hoạch và dịch các mục tiêu con được tạo ra thành các hành động có thể được thực thi thông qua một tập hợp các công cụ trong mô-đun thực thi. Thiết kế LUMOS cho phép nâng cấp mô-đun dễ dàng để tăng cường lập kế hoạch tác vụ mới, định căn hành động mới và bổ sung các công cụ mới, mà không ảnh hưởng đến các mô-đun khác. Để giải quyết các tác vụ thông qua các mô-đun agent, chúng tôi đề xuất hai công thức tương tác để triển khai các agent ngôn ngữ, LUMOS-OnePass (LUMOS-O) và LUMOS-Iterative (LUMOS-I). Được phác thảo trong Hình 2, LUMOS-O là một công thức hiệu quả tạo ra tất cả các mục tiêu con và hành động thông qua một lần gọi suy luận duy nhất, hoàn thành tác vụ theo cách một lần. LUMOS-I là một công thức lặp tạo ra một mục tiêu con tại một thời điểm dựa trên kết quả thực thi trước đó và cập nhật môi trường, từ đó cho phép một agent thích ứng.

Ngoài ra, LUMOS sử dụng một định dạng dữ liệu thống nhất bao gồm nhiều loại tác vụ, từ đó cho phép khung agent được đề xuất hỗ trợ thuận tiện một loạt các tác vụ tương tác. Những tác vụ này bao gồm, nhưng không giới hạn ở: trả lời câu hỏi, toán học, lập trình, duyệt web, lý luận đa phương tiện, và trò chơi văn bản. Để có được các chú thích chất lượng cao để huấn luyện LUMOS, chúng tôi tận dụng các lý luận chân lý nền trong các điểm chuẩn hiện có qua nhiều loại tác vụ khác nhau, và chuyển đổi chúng thành một định dạng thống nhất (§3). Việc chuyển đổi này được thực hiện với sự hỗ trợ của các LLM mạnh, đảm bảo rằng các chú thích được chuyển đổi tuân thủ một định dạng có thể áp dụng phổ quát phù hợp với thiết kế mô-đun của chúng tôi.

Phương pháp chuyển đổi chú thích được đề xuất của chúng tôi tạo ra khoảng 56K chú thích huấn luyện agent đa tác vụ đa miền, một trong những tài nguyên mã nguồn mở lớn nhất để tinh chỉnh agent. Các chú thích huấn luyện có thể phục vụ như một tài nguyên để nâng cao một cách phổ quát bất kỳ LLM mã nguồn mở nào với khả năng agent.

Đánh giá của chúng tôi chứng minh rằng LUMOS cung cấp hiệu suất được cải thiện hoặc có thể so sánh với các agent dựa trên GPT hoặc agent mã nguồn mở lớn hơn qua nhiều tác vụ tương tác phức tạp được sử dụng phổ biến để đánh giá agent, bao gồm QA, web, toán học, và các tác vụ đa phương tiện. Chúng tôi tóm tắt các đóng góp và kết quả của chúng tôi như sau:

Khung Agent Tổng quát với Dữ liệu Chất lượng Cao. Chúng tôi giới thiệu một khung học agent mã nguồn mở huấn luyện LLM với dữ liệu thống nhất, nhằm thống nhất các tác vụ tương tác phức tạp và tăng cường tổng quát hóa trên các tác vụ chưa thấy với môi trường và hành động mới. Chúng tôi hy vọng khung và chú thích của chúng tôi có thể tạo điều kiện cho nghiên cứu tương lai trong việc phát triển các agent ngôn ngữ mã nguồn mở.

Hiệu suất Cạnh tranh Qua các Tác vụ và Công thức Huấn luyện Agent. LUMOS vượt trội so với một số lượng lớn các agent mã nguồn mở trên các bộ dữ liệu được giữ lại của LUMOS không được sử dụng trong dữ liệu huấn luyện LUMOS qua bốn loại tác vụ huấn luyện. LUMOS thậm chí còn vượt qua các agent dựa trên GPT trong các tác vụ web và QA. Cụ thể, LUMOS cho thấy cải thiện 5.0% so với GPT-4 trên Mind2Web, và cải thiện độ chính xác LLM 4.1% và 3.5% trên HotpotQA so với các agent ReAct và ReWOO hoàn toàn dựa trên GPT-3.5-turbo, tương ứng. Hơn nữa, chúng tôi quan sát thấy rằng các công thức huấn luyện LUMOS vượt trội so với các phương pháp huấn luyện agent tiềm năng khác, như chuỗi suy nghĩ và huấn luyện tích hợp, hướng dẫn một mô-đun duy nhất để vừa lập kế hoạch vừa định căn.

Tổng quát hóa Qua Tác vụ. Chúng tôi đánh giá LUMOS trên hai tác vụ chưa thấy, WebShop (Yao et al., 2022a), một trò chơi văn bản để mua sắm trực tuyến, và InterCode SQL (Yang et al., 2023), một tác vụ tạo mã tương tác. LUMOS thậm chí còn vượt qua các agent quy mô 30B, đặc biệt là gần 20 điểm thưởng trên WebShop. LUMOS cũng mang lại cải thiện thưởng nhất quán so với các agent chuyên biệt về miền.

1Một chỉ số được định nghĩa trong Xu et al. (2023) để xác định sự tương đương ngữ nghĩa giữa dự đoán và câu trả lời vàng.

--- TRANG 3 ---
Điều này cho thấy LUMOS có thể tổng quát hóa qua các tác vụ, gợi ý về những lợi ích tiềm năng cho một phổ rộng các ứng dụng agent ngôn ngữ.

2 LUMOS: Một Khung Agent dựa trên LLM Mã nguồn Mở Mô-đun

Chúng tôi giới thiệu thiết kế tổng thể và hai công thức để phát triển agent trong khung này.

2.1 Kiến trúc Agent LUMOS

Đối với các tác vụ tương tác phức tạp khác nhau, một giải pháp chung sẽ bao gồm: (1) phân tách tác vụ thành một loạt các mục tiêu con, (2) chuyển đổi các mục tiêu con thành các hành động cụ thể, (3) thực thi những hành động đó. Quá trình này tương ứng với các mô-đun lập kế hoạch, định căn, và thực thi trong khung của chúng tôi.

♂list-altMô-đun Lập kế hoạch (PM). Mô-đun này được thiết kế để phân tích một tác vụ phức tạp thành một loạt các mục tiêu con cấp cao, được biểu đạt bằng ngôn ngữ tự nhiên. Ví dụ, một câu hỏi đa phương tiện như "Thiết bị trong tay cô ấy đến từ quốc gia nào?" đòi hỏi hai mục tiêu con: (1) Xác định thương hiệu của thiết bị trong tay cô ấy; (2) Trả lời quốc gia của thương hiệu thiết bị. Các mục tiêu con được thiết kế hỗ trợ trong việc phân tách một tác vụ phức tạp thành các hành động cấp thấp theo cách có thể diễn giải và bất khả tri về công cụ. Mô-đun lập kế hoạch được thiết kế để dễ dàng gỡ lỗi và học lập kế hoạch tác vụ mới, mà không ảnh hưởng đến các mô-đun khác.

⚛Mô-đun Định căn (GM). Mô-đun này chuyển đổi các mục tiêu con cấp cao được tạo ra bởi PM thành các hành động có thể thực thi cấp thấp. Ví dụ, GM dịch mục tiêu con, "Truy vấn thời gian sống của Lowell Sherman," thành một hoặc nhiều hành động, như KnowledgeQuery(Lowell Sherman) và QA([R2], Query:"What is the living period of Lowell Sherman?"). Ở đây, R2 đề cập đến kiến thức đã được truy xuất trước đó có thể hữu ích trong việc trả lời truy vấn. Mô-đun định căn có thể được tùy chỉnh dễ dàng để học các hành động mới mà không ảnh hưởng đến mô-đun lập kế hoạch.

/toolsMô-đun Thực thi (EM). Mô-đun Thực thi (EM) là một chương trình triển khai các hành động được tạo ra bởi mô-đun định căn và nhận kết quả thực thi. Nó triển khai nhiều công cụ có sẵn, bao gồm API, mô hình thần kinh, và trình mô phỏng ảo. Ví dụ, mô-đun thực thi có thể gọi các API Wikipedia hoặc Google Search để hoàn thành hành động KnowledgeQuery.

Đặc điểm chính của khung LUMOS là sự tương tác giữa ba mô-đun. Chúng tôi đề xuất hai công thức thúc đẩy giao tiếp: LUMOS-OnePass (LUMOS-O) và LUMOS-Iterative (LUMOS-I).

2.2 LUMOS-OnePass (LUMOS-O)

Công thức LUMOS-OnePass (LUMOS-O) là một phương pháp hiệu quả tạo ra tất cả các mục tiêu con và hành động được định căn cùng một lúc (nghiên cứu hiệu quả trong App. E). Như được mô tả trong Hình 2, công thức này sử dụng mô-đun lập kế hoạch để tạo ra tất cả n mục tiêu con trong một lần gọi suy luận duy nhất. Sau đó chúng tôi chuyển tất cả các mục tiêu con được tạo ra cho mô-đun định căn, mô-đun này dịch chúng thành một chuỗi m hành động cấp thấp. Lưu ý rằng ngoài mô tả tác vụ và các mục tiêu con, chúng tôi cũng cung cấp các giao diện hành động I cho mô-đun định căn như đầu vào. Những giao diện hành động này (ví dụ, "VQA(Image_Context, Query): Given the image context, answer the query.") định nghĩa các chức năng của hành động và các đối số hợp lệ của chúng, hướng dẫn mô-đun định căn để tạo ra các hành động có thể thực thi. Cuối cùng, ví dụ, mô-đun định căn có thể tạo ra tất cả các hành động tương ứng, từ VQA([IMG], Question: What's the brand of the device in her hand?) đến hành động cuối cùng QA(..., Question: What's the country of ...?).

Chính thức, quá trình lập kế hoạch và định căn tổng thể của LUMOS-O được minh họa trong Hình 2. Trong giai đoạn lập kế hoạch, mô tả tác vụ T được đưa vào mô-đun lập kế hoạch. Điều này tạo ra một chuỗi đầu ra các mục tiêu con, được biểu đạt như S = πplan(T) = {s1, ..., sn}, trong đó πplan là các tham số của mô-đun lập kế hoạch đã được huấn luyện. Các hành động được định căn được thu được thông qua A = πground(T, I, S), với sự phụ thuộc vào mô tả tác vụ, giao diện hành động I = {i1, ..., ik}, và các mục tiêu con được tạo ra S. πground đại diện cho các tham số của mô-đun định căn. Chúng tôi lấy kết quả thực thi cuối cùng en làm kết quả suy luận cuối cùng cho tác vụ đã cho.

2.3 LUMOS-Iterative (LUMOS-I)

LUMOS-Iterative (LUMOS-I) là một công thức tạo ra một mục tiêu con và các hành động có thể thực thi tương ứng của nó trong mỗi lần lặp. Khi tạo ra mục tiêu con thứ t, mô-đun lập kế hoạch yêu cầu các mục tiêu con đã lập kế hoạch trước đó và kết quả thực thi của các hành động được định căn của chúng làm đầu vào. Kết quả thực thi hỗ trợ mô-đun lập kế hoạch để nhận thức về sự thay đổi môi trường và quyết định mục tiêu con tiếp theo theo môi trường cập nhật.

--- TRANG 4 ---
Lấy câu hỏi VQA "Thiết bị trong tay cô ấy đến từ quốc gia nào?" trong Hình 2 làm ví dụ. Trong lần lặp đầu tiên, mô-đun lập kế hoạch sẽ tạo ra "Mục tiêu con 1: Xác định thương hiệu của thiết bị trong tay cô ấy". Mục tiêu con này được chuyển cho mô-đun định căn để tạo ra các hành động truy vấn, và thu được kết quả thực thi Nintendo. Mô-đun lập kế hoạch sau đó lấy Nintendo cùng với ngữ cảnh lập kế hoạch trước đó làm đầu vào để tạo ra mục tiêu con tiếp theo "Mục tiêu con 2: Trả lời quốc gia của Nintendo". Lập kế hoạch dựa trên kết quả thực thi mới nhất sẽ giảm thiểu rủi ro giới thiệu một đối tượng không tồn tại trong môi trường hoặc một thực thể ngẫu nhiên trong quá trình lý luận (một nghiên cứu trường hợp trong App. E).

Chúng tôi trình bày một lần lặp duy nhất của quá trình lập kế hoạch và định căn của LUMOS-I trong Hình 2. Để lập kế hoạch mục tiêu con thứ t, chúng tôi đưa vào 1) mô tả tác vụ T, 2) các mục tiêu con trước đó {s1, ..., st-1}, và 3) kết quả thực thi của chúng {e1, ..., et-1} vào mô-đun lập kế hoạch. Chúng tôi nối chúng theo thứ tự T, s1, e1, ..., st-1, et-1 trong đó các mục tiêu con gần đây nhất và kết quả của chúng được đặt ở cuối, vì chúng có ảnh hưởng cao hơn đối với việc lập kế hoạch mục tiêu con thứ t. Đầu ra sẽ là mục tiêu con thứ t, st = πplan(T, s1, e1, ..., st-1, et-1). Sau đó, mục tiêu con thứ t sẽ được kết hợp trực tiếp vào mô-đun định căn cùng với lịch sử định căn trước đó và giao diện hành động I để tạo ra các hành động tương ứng, At = πground(T, I, s1, A1, ..., st-1, At-1, st). Lưu ý rằng At là một danh sách hành động có thể thực thi, vì mục tiêu con cấp cao có thể được phân tách thành nhiều hành động cấp thấp. Cuối cùng chúng tôi đưa các hành động cấp thấp At vào mô-đun thực thi. Kết quả thực thi cuối cùng et có thể được gửi lại để lập kế hoạch mục tiêu con thứ (t+1).

3 Học Lập kế hoạch & Định căn với LLM Mã nguồn Mở

Để hướng dẫn các mô-đun lập kế hoạch và định căn tạo ra các mục tiêu con và hành động cấp thấp hợp lệ dưới các giao diện hành động được chỉ định của chúng tôi, chúng tôi tinh chỉnh hai mô-đun để tạo ra các đầu ra mong đợi.

Huấn luyện các mô-đun đòi hỏi các giám sát bao gồm các tác vụ, mục tiêu con, và hành động cấp thấp chất lượng cao. Để trang bị cho các LLM nhỏ hơn khả năng tuân theo hướng dẫn, các công trình trước đây tận dụng các phương pháp như Self-Instruct (Wang et al., 2023b) để tổng hợp các tác vụ huấn luyện và đầu vào, và trực tiếp tạo ra các đầu ra tác vụ chân lý nền dựa trên các tác vụ đã tạo của nó. Tuy nhiên, các phương pháp này không phù hợp để tạo ra các chú thích chất lượng cao để huấn luyện agent. Ví dụ, đối với một tác vụ duyệt web trong Mind2Web, GPT-4 chỉ đạt được khoảng 20% tỷ lệ thành công bước (Liu et al., 2023b) khi hoàn thành tác vụ. Việc dựa vào các phương pháp như vậy để tạo ra các chú thích tác vụ tương tác phức tạp có thể làm giảm chất lượng chú thích.

Thay vì tạo ra các chú thích với LLM từ đầu, chúng tôi khai thác LLM như một công cụ "chuyển đổi phong cách" để chuyển đổi các bước lý luận chân lý nền trong các điểm chuẩn hiện có thành định dạng mong đợi trong các công thức LUMOS. Có một số lượng đáng kể các bộ dữ liệu được chú thích với các giải pháp do con người viết hoặc các chuỗi hành động có cấu trúc2. Ví dụ, PRM800K (Lightman et al., 2023) là một bộ dữ liệu toán học chứa các giải pháp ngôn ngữ tự nhiên xen kẽ với các công thức; StrategyQA (Geva et al., 2021) là một bộ dữ liệu QA với các câu hỏi được phân tách, các sự kiện hỗ trợ, và các chỉ số đoạn văn Wikipedia có liên quan; Mind2Web bao gồm các chuỗi hành động chân lý nền, v.v. Chúng cung cấp cho LLM thông tin cơ bản đủ đóng góp vào việc chuyển đổi chú thích.

Tiếp theo, chúng tôi giới thiệu 1) cách chúng tôi nhắc LLM để có được các giám sát mục tiêu con và hành động để huấn luyện các mô-đun; 2) cách tổ chức các mục tiêu con và hành động thành các dạng đối thoại phù hợp với các công thức LUMOS; 3) cách chúng tôi huấn luyện các mô-đun với các chú thích cuối cùng.

3.1 Lời nhắc Chuyển đổi Chú thích

Để giúp LLM tuân theo tốt hơn các hướng dẫn chuyển đổi chú thích, chúng tôi thêm 4-/5-shot ví dụ trong các lời nhắc chuyển đổi (xem App. I để biết chi tiết lời nhắc). Chúng tôi thảo luận về các yếu tố quan trọng trong những ví dụ trong ngữ cảnh này. Các ký hiệu của các chú thích được chuyển đổi có dấu mũ trên các chữ cái.

Giao diện Hành động. Giao diện hành động định nghĩa các hành động có sẵn mà LLM có thể định căn đến. Bảng 8 cho thấy một danh sách toàn diện các định nghĩa hành động và việc triển khai của chúng.

Các Bước Lý luận Trung gian Chân lý Nền. Chúng tôi cung cấp cho LLM các bước lý luận trung gian chân lý nền trong các điểm chuẩn hiện có. Với những điều này làm tài liệu tham khảo, LLM có thể tóm tắt các mục tiêu con cấp cao và tổng hợp các hành động tương ứng theo các giao diện hành động đã cho.

2Nhiều tài nguyên có sẵn để mở rộng trong tương lai và thảo luận về khả năng mở rộng của các phương pháp chuyển đổi chú thích của chúng tôi được mô tả trong App. C.

--- TRANG 5 ---
Các Mục tiêu con và Hành động Tương ứng. Khi chuyển đổi các bước lý luận chân lý nền thành các chú thích mong đợi của chúng tôi, chúng tôi cung cấp cho LLM các ví dụ về cách chưng cất các mục tiêu con cấp cao từ các bước lý luận và ánh xạ chúng thành các hành động tương ứng. Trong các ví dụ trong ngữ cảnh, chúng tôi thủ công phân tách một tác vụ phức tạp thành các mục tiêu con cấp cao theo ngữ cảnh của các bước lý luận chân lý nền. Dưới mỗi mục tiêu con cấp cao, chúng tôi viết ra nhiều hành động tương ứng giúp hoàn thành mục tiêu con (được hiển thị trong App. I). Với các mục tiêu con và hành động mẫu trong lời nhắc, LLM sẽ bắt chước để tạo ra các mục tiêu con và các hành động được ghép cặp của chúng khi thực hiện chuyển đổi cho các tác vụ mới.

Vì kết quả thực thi của các mục tiêu con trước đó có thể hữu ích trong việc triển khai hành động trong tương lai, chúng tôi liên kết các hành động được định căn trong các ví dụ trong ngữ cảnh để cho phép thực thi phụ thuộc vào ngữ cảnh. Một ví dụ về các hành động liên kết là R1 = KnowledgeQuery(Zombies); R2 = ParagraphRetrieve(R1, Query: What color skin are zombies typically depicted with?). Agent có thể đầu tiên tìm trang kiến thức zombie (R1). Được viết theo phong cách liên kết, hành động ParagraphRetrieve có thể nhận kiến thức về zombie R1 làm ngữ cảnh, và thực hiện truy xuất dựa trên truy vấn.

Kết quả Thực thi Trung gian của Các Mục tiêu con. Kết quả thực thi trung gian Ê đóng một vai trò quan trọng trong việc tăng khả năng thích ứng của LUMOS với các thay đổi môi trường. Một số bộ dữ liệu (ví dụ, GSM8K) cung cấp kết quả thực thi trong các bước lý luận của chúng, tức là kết quả tính toán của các công thức. Đối với các bộ dữ liệu không có bất kỳ kết quả thực thi nào, các bước lý luận của chúng thực sự chứa các manh mối liên quan cho kết quả thực thi. Chúng tôi lấy một ví dụ trong StrategyQA. Mặc dù câu trả lời của câu hỏi được phân tách đã được chú thích "Zombies thường được mô tả với màu da gì?" không được cung cấp trực tiếp, chú thích chứa một sự kiện liên quan "Zombies thường được mô tả là có màu xanh lá cây nhợt nhạt." đề cập đến câu trả lời "xanh lá cây". Do đó, đối với mỗi ví dụ trong ngữ cảnh, chúng tôi nối các tài liệu liên quan cũng như kết quả thực thi được ghi lại thủ công của chúng tôi trong các lời nhắc chuyển đổi. Khi chuyển đổi các mẫu mới thành chú thích LUMOS, LLM sẽ tự động trích xuất kết quả thực thi từ các tài liệu đã cho.

Sau khi nhắc LLM với các lời nhắc chuyển đổi, chúng tôi có thể có được các yếu tố chính trong các chú thích huấn luyện, bao gồm các mục tiêu con Ŝ, các hành động tương ứng Â và kết quả thực thi Ê của chúng.

3.2 Tổ chức Chú thích Đối thoại

Cuối cùng, để xây dựng tương tác giữa các mô-đun lập kế hoạch và định căn, chúng tôi tổ chức các chú thích thành định dạng đối thoại.

Chú thích Mô-đun Lập kế hoạch Đối thoại. Như được hiển thị trong Hình 3a của App. A, chúng tôi đầu tiên đóng vai trò người dùng để cung cấp tác vụ T̂ trong lời nhắc người dùng. Đối với LUMOS-O, tất cả các mục tiêu con Ŝ là đầu ra cuối cùng của mô-đun lập kế hoạch. LUMOS-I yêu cầu phong cách đối thoại nhiều lượt. Từ Hình 3a, mô-đun lập kế hoạch thêm mục tiêu con chân lý nền đầu tiên ŝ1 với chỉ số "Mục tiêu con 1" làm giám sát phản hồi đầu tiên. Sau đó chúng tôi đặt kết quả thực thi ê1 của Mục tiêu con 1 với tiền tố "Kết quả thực thi cho Mục tiêu con 1 là" làm đầu vào người dùng thứ hai. Đối với các lượt còn lại, chúng tôi đóng vai trò người dùng, cung cấp kết quả thực thi êt-1 của mục tiêu con cuối cùng ŝt-1 cho mô-đun lập kế hoạch, và hỏi liệu việc lập kế hoạch có nên được dừng lại không. Các giám sát phản hồi bao gồm việc liệu lập kế hoạch có nên được chấm dứt; nếu không, phản hồi nên chứa một mục tiêu con vàng mới ŝt.

Chú thích Mô-đun Định căn Đối thoại. Như được hiển thị trong Hình 3b của App. A, chúng tôi cũng đầu tiên cung cấp tác vụ T̂ và giao diện hành động Î cho mô-đun định căn trong lượt người dùng đầu tiên. Đối với LUMOS-O, chúng tôi đưa tất cả các chú thích mục tiêu con đã chuyển đổi Ŝ vào lời nhắc người dùng. Tất cả các chú thích hành động Â sẽ là phản hồi lời nhắc người dùng. Đối với LUMOS-I, chúng tôi đưa mục tiêu con vàng hiện tại ŝt, với tiền tố "Mục tiêu con cần được định căn:". Phản hồi của nó sẽ là các hành động tương ứng Ât của ŝt.

3.3 Huấn luyện với Chú thích Đã chuyển đổi

Vì các chú thích LUMOS có tính đối thoại, chúng tôi công thức hóa chúng như {x1, y1, ..., xi, yi, ..., xn, yn}, trong đó xi là lời nhắc người dùng thứ i và yi chỉ ra các phản hồi chân lý nền của nó. Theo Wang et al. (2023a), trong quá trình huấn luyện, chúng tôi đưa mỗi chú thích nhiều lượt hoàn chỉnh vào một mô hình chỉ giải mã trong khi chỉ tính toán tổn thất giải mã trên các token của các phản hồi chân lý nền Y = {y1, ..., yi, ..., yn}. Chúng tôi áp dụng che nhị phân trên các token lời nhắc người dùng để ngăn tính toán tổn thất trên chúng. Hàm tổn thất cuối cùng là L = -∑j log pπ(tj|t<j) × 1(tj∈Y) trong đó tj biểu thị token đầu vào thứ j và 1(·) là một hàm chỉ thị Boolean.

--- TRANG 6 ---
4 Thí nghiệm

Chúng tôi bắt đầu với các chi tiết của thiết lập thí nghiệm, bao gồm chuyển đổi chú thích, huấn luyện mô-đun, và các công cụ được sử dụng trong mô-đun thực thi. Sau đó chúng tôi đánh giá LUMOS bằng cách: 1) so sánh LUMOS với các agent LLM mã nguồn mở hiện có và các agent dựa trên GPT, 2) đối chiếu LUMOS với các phương pháp huấn luyện agent khác, 3) thể hiện khả năng tổng quát hóa của LUMOS trên hai tác vụ chưa thấy liên quan đến môi trường và hành động mới, và 4) đánh giá chất lượng chú thích LUMOS.

4.1 Thiết lập Thí nghiệm

Thu thập Dữ liệu. Sử dụng các lời nhắc chuyển đổi được thảo luận trong §3.1, chúng tôi sử dụng các phiên bản GPT-4 (Achiam et al., 2023) vào 8/13/2023 và 9/13/2023, và phiên bản GPT-4V (OpenAI, 2023b) vào 1/24/2023 để thực hiện chuyển đổi chú thích trên các bước lý luận chân lý nền trong các điểm chuẩn hiện có. App. B cung cấp các nguồn dữ liệu được sử dụng để chuyển đổi chú thích. Những nguồn này bao gồm các bộ dữ liệu của các tác vụ toán học, QA, web và đa phương tiện. Để giúp LUMOS nhận thức về các đầu vào hình ảnh trong các tác vụ đa phương tiện, chúng tôi thêm một chú thích hình ảnh chi tiết được tạo ra bởi LLAMA-1.5-7B (Liu et al., 2023a) vào mô tả tác vụ trong các bộ huấn luyện và kiểm tra. Sau khi lọc ra những cái chứa dấu ngoặc đơn không khớp, đầu ra thực thi không hợp lệ hoặc đầu ra quá dài, chúng tôi thu được 55,382 và 55,499 chú thích để huấn luyện các mô-đun lập kế hoạch và định căn, tương ứng.

Huấn luyện và Giao diện Hành động. Chúng tôi áp dụng LLAMA-2-7B và LLAMA-2-13B (Touvron et al., 2023a) làm các mô hình cơ sở cho cả các mô-đun lập kế hoạch và định căn. Chi tiết về quá trình huấn luyện có thể được tìm thấy trong App. D. Để giải quyết các tác vụ tương tác, chúng tôi tích hợp các hành động được sử dụng phổ biến cho mỗi tác vụ vào các giao diện hành động được định nghĩa trước. Chi tiết về các hành động có thể thực thi được hỗ trợ được bao gồm trong App. G.

4.2 Hiệu suất Tác vụ Huấn luyện

Chúng tôi đánh giá LUMOS qua một loạt các tác vụ tương tác phức tạp - QA, web, toán học và các tác vụ đa phương tiện. Đánh giá chủ yếu tuân theo các thiết lập được thiết lập bởi AgentBench (Liu et al., 2023b) và ReWOO (Xu et al., 2023) (xem App. H). Đối với mỗi loại tác vụ, ngoại trừ các tác vụ web, chúng tôi bao gồm một bộ dữ liệu được giữ lại để đánh giá khả năng tổng quát hóa của mô hình qua các miền trong cùng một danh mục tác vụ. Hiệu suất được hiển thị trong Bảng 1. Lưu ý rằng trong Bảng 1, các agent chuyên biệt tác vụ LUMOS-IX được huấn luyện sử dụng dữ liệu chuyên biệt tác vụ thuộc loại tác vụ X (ví dụ, Web, Math, QA, MM). LUMOS-IAll đại diện cho agent sau khi huấn luyện thống nhất với sự kết hợp của bốn loại chú thích tác vụ. Chi tiết đánh giá thêm được hiển thị trong App. H.

LUMOS so với Các Agent Mã nguồn Mở. Tổng thể, chúng tôi thấy rằng LUMOS liên tục vượt trội so với các agent mã nguồn mở khác nhau qua bảy bộ dữ liệu. Mặc dù các mô hình cơ sở của một số agent được so sánh lớn hơn 2-10× so với LUMOS, LUMOS vượt trội đáng kể về hiệu suất. Cụ thể, LUMOS-I 7B đạt được cải thiện tỷ lệ thành công bước 24.5% và 14.1% so với WizardLM-30B và AgentLM-70B trên Mind2Web.

Hiệu quả của LUMOS đặc biệt được thể hiện trên các bộ dữ liệu được giữ lại không được sử dụng trong dữ liệu huấn luyện LUMOS. Quan sát của chúng tôi cho thấy LUMOS vượt trội so với nhiều agent mã nguồn mở cơ sở qua tất cả các bộ dữ liệu được giữ lại. Đáng chú ý, mặc dù Orca-Platypus-13B (Lee et al., 2023) đã được huấn luyện trên một corpus toán học bao gồm GSM8K, hiệu suất của nó vẫn thấp hơn 8.6% so với LUMOS-O trên SVAMP (Patel et al., 2021). Hơn nữa, mặc dù ReWOO và FiReAct được tinh chỉnh với các chú thích HotpotQA trong miền, LUMOS-I, mà không có bất kỳ tinh chỉnh nào trên HotpotQA, vẫn thể hiện một cải thiện ấn tượng. Một xu hướng tương tự có thể được quan sát trên ScienceQA. Chúng tôi so sánh AutoAct-7B (Qiao et al., 2024), một agent mã nguồn mở được huấn luyện đặc biệt trên ScienceQA. LUMOS đạt được 67.3% trên toàn bộ bộ kiểm tra của ScienceQA, trong khi hiệu suất của AutoAct-7B chỉ là 53.3%.

LUMOS so với Các Agent dựa trên GPT. Mặc dù LUMOS được xây dựng trên LLAMA-2-7B/13B, LUMOS-I mang lại hiệu suất vượt trội 5.0-8.7% so với GPT-4 trên Mind2Web. Chúng tôi cũng nhận thấy tăng 3.5-7.8% về độ chính xác LLM so với ReWOO dựa trên GPT trên bộ dữ liệu HotpotQA khi sử dụng GPT-3.5-turbo làm việc triển khai công cụ QA để đảm bảo so sánh công bằng.

4.3 So sánh Công thức Agent

Chúng tôi huấn luyện các mô hình sử dụng cùng mô hình cơ sở và dữ liệu, nhưng với các phương pháp huấn luyện khác nhau - Huấn luyện Chuỗi Suy nghĩ (CoT): Đối với một tác vụ T đã cho, agent học cách tạo ra cả giải pháp chuỗi suy nghĩ và câu trả lời cuối cùng trực tiếp; Huấn luyện Agent Tích hợp: Đối với một tác vụ T đã cho, agent học cách tạo ra tất cả các mục tiêu con và hành động

--- TRANG 7 ---
[Table content showing performance comparison between different agents across various tasks - translated as before but maintaining tabular format]

Bảng 1: Hiệu suất tổng thể của các agent ngôn ngữ trên các tác vụ tương tác phức tạp đa dạng. Các tác vụ được đánh dấu bằng màu đỏ và xanh dương là các bộ dữ liệu được giữ trong/ngoài cho các loại tác vụ được huấn luyện. ⋆ chỉ ra rằng các kết quả được thu được sau khi tinh chỉnh trên bộ huấn luyện của tác vụ. Chúng tôi áp dụng thiết lập nhiều lựa chọn cho A-OKVQA. IMG biểu thị tập con với đầu vào hình ảnh trong bộ kiểm tra ScienceQA. GPT-3 trong Bảng 1c chỉ ra text-davinci-002.

sử dụng cùng mô hình. Các mô-đun thực thi vẫn giữ nguyên. Mô hình huấn luyện này được áp dụng trong ReWOO-open, FiReAct và AgentLM.

Từ Bảng 3, cả LUMOS-I và LUMOS-O đều vượt trội so với Huấn luyện CoT3. Chúng cũng vượt qua công thức tích hợp dựa trên một mô-đun duy nhất để vận hành lập kế hoạch và định căn. Điều này làm nổi bật tầm quan trọng của việc tách biệt các kỹ năng lập kế hoạch mục tiêu con và định căn hành động trong quá trình huấn luyện agent.

4.4 Đánh giá Khả năng Tổng quát hóa LUMOS

Vì LUMOS sử dụng một định dạng thống nhất để biểu diễn các tác vụ tương tác phức tạp, chúng tôi hình dung rằng sau khi được huấn luyện với các chú thích kết hợp qua bốn loại tác vụ huấn luyện, tức là huấn luyện thống nhất, khi đối mặt với một loại tác vụ chưa thấy, LUMOS có thể thích ứng với nó hiệu quả hơn.

Để kiểm tra khả năng tổng quát hóa của LUMOS, chúng tôi đánh giá nó trên các tác vụ chưa thấy, WebShop (Yao et al., 2022a) và InterCode SQL. WebShop giống

3Chúng tôi không triển khai huấn luyện CoT trên các tác vụ web, vì các cập nhật cho môi trường (ví dụ, thay đổi HTML) là những trung gian cần thiết để lập kế hoạch các hành động tiếp theo.

[THIS IS TABLE: Shows comparison between agents trained with different annotations for QA tasks]

Bảng 2: So sánh giữa các agent kích thước 7B được huấn luyện với các chú thích khác nhau.

như một trò chơi văn bản4, với môi trường mua sắm và không gian hành động của nó khác biệt đáng kể so với những gì được bao gồm trong các chú thích huấn luyện của LUMOS. InterCode SQL (Yang et al., 2023) là một tác vụ tạo mã tương tác đòi hỏi agent tạo ra mã SQL dựa trên các cơ sở dữ liệu bên ngoài và liên quan đến các lệnh SQL chưa thấy. Để làm cho LUMOS thích ứng với các tác vụ chưa thấy này, chúng tôi bổ sung 2-/3-shot ví dụ trong các đầu vào mô-đun, cho phép

4WebShop sử dụng bốn hành động trong các chú thích huấn luyện của nó: Search, FeatureRetrieve, Pick, và Click. Đối số của Click là một mặt hàng mua sắm, khác với Click trong Mind2Web bao gồm mô tả phần tử HTML.

--- TRANG 8 ---
chúng học cách tạo ra các mục tiêu con và định căn đến các bộ hành động có sẵn mới (xem App. J).

Như được phác thảo trong Bảng 1e, LUMOS-I đạt được thưởng trung bình cao hơn 3-7 so với các agent chuyên biệt miền trên WebShop. Nó cũng vượt trội đáng kể so với các agent lớn hơn như WizardLM-30B và Vicuna-v1.3-33B (Chiang et al., 2023). Chúng tôi quan sát xu hướng tương tự trên InterCode SQL. Điều này cho thấy rằng huấn luyện thống nhất cho phép các agent tăng cường khả năng tổng quát hóa trên các tác vụ chưa thấy và các hành động mới. Chúng tôi cung cấp thêm kết quả huấn luyện thống nhất trong App. F để cho thấy huấn luyện thống nhất cũng tăng cường hiệu suất trên hầu hết các loại tác vụ huấn luyện.

4.5 Phân tích Thêm về Chú thích Huấn luyện

Chúng tôi nhằm giải quyết ba câu hỏi liên quan đến chất lượng và quyết định định dạng. Q1: Các chú thích huấn luyện đã chuyển đổi của chúng tôi tốt như thế nào? Q2: Việc áp dụng các mục tiêu con cấp thấp có hiệu quả hơn việc sử dụng các mục tiêu con cấp cao không? Q3: Việc kết hợp chú thích LUMOS với dữ liệu huấn luyện hướng dẫn chung có đạt được hiệu suất tương đương trên các điểm chuẩn tuân theo hướng dẫn không?

Đánh giá Chất lượng Chú thích. Chúng tôi đánh giá chất lượng chú thích của chúng tôi bằng cách huấn luyện các mô hình với chúng và đánh giá hiệu suất của các agent. Chúng tôi so sánh chúng với dữ liệu ReWOO-open, được xây dựng dựa trên HotpotQA và TriviaQA (Joshi et al., 2017) sử dụng phương pháp Self-Instruct. Để so sánh công bằng, chúng tôi huấn luyện mô hình cơ sở của ReWOO-open, LLAMA-7B, sử dụng các chú thích LUMOS. Chúng tôi cũng áp dụng công thức huấn luyện tích hợp và lấy mẫu 2,000 dữ liệu huấn luyện để giữ các thiết lập huấn luyện giống như ReWOO-open. Cho rằng dữ liệu ReWOO-open chỉ dựa vào các điểm chuẩn QA, chúng tôi chủ yếu tập trung vào đánh giá tác vụ QA. Được hiển thị trong Bảng 2, dữ liệu LUMOS mang lại cải thiện khi so sánh với dữ liệu ReWOO-open trên StrategyQA và HotpotQA. Lưu ý rằng ngay cả khi dữ liệu ReWOO-open dựa trên HotpotQA, nó vẫn kém hiệu suất hơn LUMOS trên HotpotQA.

Mục tiêu con Cấp thấp so với Mục tiêu con Cấp cao. Như được mô tả trong §2, chúng tôi yêu cầu LLM tạo ra các mục tiêu con cấp cao tương ứng với một hoặc nhiều hành động cấp thấp. Một chú thích thay thế có thể là một chú thích trong đó mỗi mục tiêu con chỉ tương ứng với một hành động cấp thấp, tức là mục tiêu con là "cấp thấp". Chúng tôi hướng dẫn LLM tạo ra các mục tiêu con cấp thấp bằng cách sửa đổi lời nhắc chuyển đổi chú thích để phù hợp với định dạng trong đó một mục tiêu con được liên kết chặt chẽ với một hành động. Bảng 2 cho thấy sự sụt giảm sau khi thay thế các mục tiêu con cấp cao bằng các mục tiêu con cấp thấp trên cả hai bộ dữ liệu QA. Kết quả này do đó khẳng định lại sự phù hợp của thiết kế mục tiêu con ban đầu của chúng tôi.

Tác động lên Điểm chuẩn Tuân theo Hướng dẫn Chung. Vì LUMOS áp dụng phong cách đối thoại được thảo luận trong Phần 3, bất kỳ tác vụ nào có thể được chính thức hóa như các tác vụ đối thoại đều có thể được hỗ trợ. Do đó, LUMOS hỗ trợ tất cả các tác vụ huấn luyện hướng dẫn và các tác vụ tương tác phức tạp. Chúng tôi tiến hành một thí nghiệm để cho thấy hiệu quả của LUMOS trong các tác vụ huấn luyện hướng dẫn. Bằng cách tích hợp dữ liệu huấn luyện hướng dẫn chung, Alpaca, với các chú thích huấn luyện thống nhất của LUMOS, chúng tôi tạo ra một bộ dữ liệu huấn luyện mở rộng. Để hướng dẫn LUMOS tạo ra các phản hồi đơn giản, chúng tôi thêm cụm từ "Respond to me directly" vào mỗi thể hiện đầu vào từ Alpaca, hướng dẫn mô hình tránh elaborating các quá trình lý luận cho các tác vụ huấn luyện hướng dẫn chung. Được kiểm tra trên điểm chuẩn SuperNaturalInstruction (Wang et al., 2022), Lumos đạt được điểm ROUGE-L 39.3, gần với LLAMA-2-7B được huấn luyện chỉ trên Alpaca (39.8), điều này cho thấy khả năng phát triển khả năng lý luận agent và khả năng tuân theo hướng dẫn chung đồng thời.

5 Công trình Liên quan

Agent LLM. Các agent ngôn ngữ đã cho thấy tiềm năng trong việc giải quyết các tác vụ tương tác phức tạp đa dạng. ReAct (Yao et al., 2022b) giới thiệu một phương pháp nhắc nhở hình thành LLM như các agent ngôn ngữ và định căn chúng trong các môi trường bên ngoài. Tiếp theo, một số phương pháp (Shen et al., 2023; Lu et al., 2023; Xu et al., 2023; Lin et al., 2023; Liu et al., 2023c) nhằm cải thiện hiệu suất agent và tăng khả năng áp dụng của chúng trong các tình huống đa dạng. Những agent này chủ yếu dựa vào các LLM mã nguồn đóng, thiếu sự xem xét về các vấn đề khả năng chi trả, khả năng tái tạo và tính minh bạch trên các tác vụ tương tác phức tạp.

Cải thiện Các Mô hình Nhỏ để Xây dựng Agent. Các công trình gần đây đã sử dụng các mô hình lớn hơn để tạo ra dữ liệu huấn luyện để tinh chỉnh các mô hình nhỏ hơn (Bosselut et al., 2019; West et al., 2022; Wang et al., 2023b; Hsieh et al., 2023; Brahman et al., 2023) để cho phép chúng tuân theo hướng dẫn và thực hiện lý luận chuỗi suy nghĩ. Chúng tôi cũng quan sát các nỗ lực đồng thời ReWOO-open (Xu et al., 2023), FireAct (Chen et al., 2023), AgentLM

--- TRANG 9 ---
[Bảng so sánh hiệu suất của các công thức huấn luyện agent khác nhau được giữ nguyên định dạng]

Bảng 3: So sánh giữa các công thức khác nhau của huấn luyện agent ngôn ngữ. Chỉ số cho HotpotQA là độ chính xác LLM (%). Tất cả các thí nghiệm đều dựa trên LLAMA-2-7B.

(Zeng et al., 2023), và AutoAct (Qiao et al., 2024), tập trung vào huấn luyện agent trên các LLM nhỏ hơn. Không giống như FireAct và AutoAct, công việc của chúng tôi đi sâu vào phân tích chuyên sâu hơn, nhằm khám phá một biểu diễn tác vụ thống nhất cho phép các agent tổng quát hóa hiệu quả qua các tác vụ tương tác chưa thấy. Trái ngược với ReWOO-open và AgentLM, chúng tôi mở rộng để kiểm tra các công thức huấn luyện phù hợp và nghiên cứu nhiều chiến lược để tạo ra các bộ dữ liệu quy mô lớn, chất lượng cao cho huấn luyện agent. Chúng tôi chứng minh hiệu suất vượt trội của LUMOS trong §4.

6 Kết luận

Chúng tôi giới thiệu LUMOS, một khung huấn luyện agent ngôn ngữ mã nguồn mở, có thể tổng quát hóa. Chúng tôi đề xuất hai công thức, LUMOS-I và LUMOS-O, thúc đẩy sự hợp tác giữa các mô-đun agent để giải quyết các tác vụ phức tạp. Đối với dữ liệu huấn luyện mô-đun, chúng tôi sử dụng LLM để chuyển đổi các bước lý luận trong các điểm chuẩn hiện có thành một định dạng thống nhất có thể áp dụng trong khung LUMOS. LUMOS vượt trội so với nhiều agent mã nguồn mở qua 9 bộ dữ liệu. Nó thậm chí còn hoạt động tốt hơn các agent GPT trên các tác vụ QA và web. LUMOS cũng vượt qua các công thức huấn luyện agent tiềm năng và thể hiện khả năng tổng quát hóa vượt trội trên hai tác vụ tương tác chưa thấy.

Hạn chế

Các Loại Tác vụ Huấn luyện Được Bao gồm. Hiện tại, LUMOS được huấn luyện sử dụng các chú thích cho bốn loại tác vụ tương tác phức tạp cụ thể, điều này có thể vẫn hạn chế khả năng tổng quát hóa của nó cho các tác vụ mới. Để giải quyết điều này, chúng tôi nhằm làm phong phú dữ liệu huấn luyện cho LUMOS bằng cách kết hợp một loạt loại tác vụ rộng hơn. Như được phác thảo trong §3 và App. C, một loạt điểm chuẩn đáng kể đã tồn tại, cung cấp các bước lý luận chân lý nền có thể phục vụ như một nền tảng để mở rộng các chú thích của LUMOS. Bằng cách mở rộng phạm vi chú thích, chúng tôi không chỉ nâng cao các agent ngôn ngữ mà còn cung cấp một tài nguyên có giá trị cho các nhà thực hành muốn phát triển các mô hình của riêng họ.

Khả năng Quay lại và Lập lại Kế hoạch. Trong các tình huống mà các agent ngôn ngữ gặp phải kết quả thực thi không hợp lệ hoặc điều hướng các đường dẫn giải pháp sai lầm, điều quan trọng là chúng phải có khả năng tự chẩn đoán và lập lại kế hoạch quá trình lý luận của chúng. LUMOS hiện tại thiếu những tính năng tự sửa chữa tinh vi này. Các phiên bản tương lai nên được thiết kế với các cơ chế tiên tiến cho phép các agent nhận ra và khắc phục các lỗi lập kế hoạch của chúng.

Thay thế Công cụ Mã nguồn Mở. Đối với một phần của các thí nghiệm QA của chúng tôi, chúng tôi sử dụng các mô hình GPT để giải quyết các câu hỏi con được phân tách. Nó được thiết kế để so sánh công bằng với các agent cũng sử dụng các mô hình GPT làm công cụ QA, như được elaborated trong §4.2. Chiến lược tương lai của chúng tôi liên quan đến việc chuyển đổi sang các khung QA hoàn toàn mã nguồn mở tận dụng các mô hình như LLAMA-2-70B, nhằm thiết lập một khung hoàn toàn mã nguồn mở.

Lời cảm ơn

Công việc này được tài trợ một phần bởi chương trình DARPA MCS thông qua NIWC Pacific (N66001-19-2-4031), DARPA ECOLE (#HR00112390060), và Viện Allen về AI. Chúng tôi cảm ơn các thành viên nhóm Mosaic, và các nhà đánh giá ẩn danh cho các cuộc thảo luận hữu ích.

Tài liệu tham khảo

[Danh sách tài liệu tham khảo được giữ nguyên định dạng và nội dung như bản gốc]
