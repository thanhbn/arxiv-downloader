DARTS+: Tìm kiếm Kiến trúc Khả vi Cải tiến với Dừng Sớm
Hanwen Liang1Shifeng Zhang2*Jiacheng Sun1†Xingqiu He1
Weiran Huang1Kechen Zhuang1Zhenguo Li1
1Phòng thí nghiệm Noah's Ark của Huawei2TNList, Đại học Tsinghua
Tóm tắt
Gần đây, đã có sự quan tâm ngày càng tăng trong việc tự động hóa
quá trình thiết kế kiến trúc mạng nơ-ron, và phương pháp Tìm kiếm
Kiến trúc Khả vi (DARTS) làm cho quá trình này có thể thực hiện
được trong vài ngày GPU. Tuy nhiên, hiệu suất của DARTS thường
bị quan sát thấy sụp đổ khi số lượng epoch tìm kiếm trở nên lớn.
Đồng thời, rất nhiều "kết nối bỏ qua" được tìm thấy trong các kiến
trúc được chọn. Trong bài báo này, chúng tôi khẳng định rằng nguyên
nhân của sự sụp đổ là do tồn tại hiện tượng overfitting trong việc tối
ưu hóa của DARTS. Do đó, chúng tôi đề xuất một thuật toán đơn giản
và hiệu quả, được gọi là "DARTS+", để tránh sụp đổ và cải thiện
DARTS gốc, bằng cách "dừng sớm" quy trình tìm kiếm khi đáp ứng
một tiêu chí nhất định. Chúng tôi cũng tiến hành các thí nghiệm toàn
diện trên các tập dữ liệu chuẩn và các không gian tìm kiếm khác nhau
và cho thấy hiệu quả của thuật toán DARTS+ của chúng tôi, và
DARTS+ đạt được 2:32% lỗi test trên CIFAR10, 14:87% trên
CIFAR100, và 23:7% trên ImageNet. Chúng tôi tiếp tục lưu ý rằng
ý tưởng về "dừng sớm" được bao gồm một cách ngầm định trong một
số biến thể DARTS hiện có bằng cách thiết lập thủ công một số lượng
nhỏ các epoch tìm kiếm, trong khi chúng tôi đưa ra một tiêu chí rõ
ràng cho "dừng sớm".

1 Giới thiệu
Tìm kiếm Kiến trúc Mạng nơ-ron (NAS) đóng vai trò quan trọng
trong Học máy Tự động (AutoML), đã thu hút rất nhiều sự chú ý
gần đây [3, 8, 20–22, 26, 35, 42]. Tìm kiếm Kiến trúc Khả vi
(DARTS) [21] nhận được sự chú ý rộng rãi vì nó có thể thực hiện
tìm kiếm rất nhanh trong khi đạt được hiệu suất mong muốn. Cụ thể,
nó mã hóa không gian tìm kiếm kiến trúc với các tham số liên tục
để tạo thành một mô hình một lần và thực hiện tìm kiếm bằng cách
huấn luyện mô hình một lần với tối ưu hóa bi-level dựa trên gradient.

Mặc dù DARTS hiệu quả, một vấn đề nghiêm trọng của DARTS đã
được tìm thấy [3, 4, 37, 41]. Cụ thể, sau một số epoch tìm kiếm
nhất định, số lượng kết nối bỏ qua tăng lên đáng kể trong kiến trúc
được chọn, dẫn đến hiệu suất kém. Chúng tôi gọi hiện tượng giảm
hiệu suất sau một số epoch nhất định là "sụp đổ" của DARTS.

*Đóng góp ngang nhau. Công việc này được thực hiện khi hai tác giả
đầu tiên là thực tập sinh tại Phòng thí nghiệm Noah's Ark của Huawei.
†Email liên hệ: sunjiacheng1@huawei.com.

Để giải quyết vấn đề này, một số công trình như P-DARTS [3] thiết
kế regularization không gian tìm kiếm để giảm bớt sự thống trị của
kết nối bỏ qua trong quá trình tìm kiếm. Tuy nhiên, các cách tiếp
cận này liên quan đến nhiều siêu tham số hơn, cần được điều chỉnh
cẩn thận bởi các chuyên gia con người. Hơn nữa, Single-Path NAS
[30], StacNAS [17], và SNAS [34] sử dụng tối ưu hóa một cấp thay
vì tối ưu hóa bi-level trong DARTS, trong đó các tham số kiến trúc
và trọng số mô hình được cập nhật đồng thời. Tuy nhiên, các không
gian tìm kiếm của những thuật toán này cần được thiết kế cẩn thận
[17, 21, 34]. Tóm lại, cơ chế sụp đổ của DARTS vẫn còn mở.

Trong bài báo này, trước tiên chúng tôi cho thấy rằng sự sụp đổ
của DARTS là do overfitting trong giai đoạn tìm kiếm, dẫn đến khoảng
cách lớn giữa lỗi huấn luyện và validation. Cụ thể, chúng tôi giải
thích tại sao overfitting dẫn đến số lượng lớn kết nối bỏ qua trong
các kiến trúc được chọn trong DARTS, làm tổn hại hiệu suất của
kiến trúc được chọn. Để tránh sụp đổ của DARTS, chúng tôi thêm
một mô hình "dừng sớm" đơn giản và hiệu quả, gọi là "DARTS+",
trong đó quy trình tìm kiếm dừng lại theo một tiêu chí nhất định,
được minh họa trong Hình 1.(a). Chúng tôi chỉ ra rằng việc tìm kiếm
đạt bão hòa khi nó được "dừng sớm". Chúng tôi lưu ý rằng một số
tiến bộ của DARTS, bao gồm P-DARTS [3], Auto-DeepLab [19],
và PC-DARTS [36], cũng áp dụng ý tưởng dừng sớm một cách
ngầm định trong đó ít epoch tìm kiếm hơn được thiết lập thủ công
trong các phương pháp của họ.

Hơn nữa, chúng tôi tiến hành các thí nghiệm đầy đủ để chứng minh
hiệu quả của thuật toán DARTS+ được đề xuất. Cụ thể, DARTS+
thành công trong việc tìm kiếm trên các không gian khác nhau bao
gồm DARTS, MobileNetV2, ResNet. Trong không gian tìm kiếm
DARTS, DARTS+ đạt được 2:32% lỗi test trên CIFAR10 và 14:87%
lỗi test trên CIFAR100, trong khi thời gian tìm kiếm nhỏ hơn 0:4
ngày GPU. Khi chuyển sang ImageNet, DARTS+ đạt được 23:7%
lỗi top-1 và 22:5% lỗi top-1 ấn tượng nếu SE-Module [12] được
giới thiệu. DARTS+ cũng có thể tìm kiếm trực tiếp trên ImageNet
và đạt được 23:9% lỗi top-1.

Tóm lại, các đóng góp chính của chúng tôi như sau:
• Chúng tôi nghiên cứu vấn đề sụp đổ của DARTS, và chỉ ra rằng
lý do cơ bản là overfitting của trọng số mô hình trong huấn luyện
DARTS.

• Chúng tôi giới thiệu một mô hình "dừng sớm" hiệu quả cho DARTS
để tránh sụp đổ và đề xuất các tiêu chí hiệu quả và thích ứng cho
dừng sớm.

• Chúng tôi tiến hành các thí nghiệm mở rộng trên các tập dữ liệu
chuẩn và các không gian tìm kiếm khác nhau để chứng minh hiệu
quả của thuật toán được đề xuất, đạt được kết quả tốt nhất hiện
tại trên tất cả chúng.

2 Sụp đổ của DARTS

Có một hành vi không mong muốn của DARTS [21] là quá nhiều
kết nối bỏ qua có xu hướng xuất hiện trong kiến trúc được chọn
khi số lượng epoch tìm kiếm lớn, làm cho hiệu suất kém. Hiện tượng
giảm hiệu suất được gọi là "sụp đổ" của DARTS trong bài báo của
chúng tôi. Trong phần này, trước tiên chúng tôi đưa ra một đánh
giá nhanh về DARTS gốc, sau đó chỉ ra vấn đề sụp đổ của DARTS
và thảo luận về các nguyên nhân cơ bản của nó.

2.1 DARTS

Mục tiêu của DARTS là tìm kiếm một cell, có thể được xếp chồng
để tạo thành một mạng tích chập hoặc một mạng hồi quy. Mỗi cell
là một đồ thị có hướng không có chu trình (DAG) của N nodes
{xi}N-1i=0, trong đó mỗi node đại diện cho một lớp mạng. Chúng
tôi ký hiệu không gian operation là O, và mỗi phần tử là một operation
ứng viên, ví dụ, zero, skip-connect, convolution, max-pool, v.v.
Mỗi cạnh (i,j) của DAG đại diện cho luồng thông tin từ node xi
đến xj, bao gồm các operation ứng viên được trọng số bởi tham số
kiến trúc α(i,j). Cụ thể, mỗi cạnh (i,j) có thể được công thức hóa
bởi một hàm o(i,j) trong đó o(i,j)(xi) = ∑o∈O p(i,j)o · o(xi);
và trọng số của mỗi operation o ∈ O là một softmax của tham số
kiến trúc α(i,j), tức là p(i,j)o = exp(α(i,j)o)/∑o'∈O exp(α(i,j)o').

Một node trung gian là xj = ∑i<j o(i,j)(xi), và node đầu ra
xN-1 là concatenation theo chiều sâu của tất cả các node trung
gian ngoại trừ các node đầu vào. Siêu mạng ở trên được gọi là
mô hình một lần, và chúng tôi ký hiệu w là trọng số của siêu mạng.

Đối với quy trình tìm kiếm, chúng tôi ký hiệu Ltrain và Lval là
mất mát huấn luyện và validation tương ứng. Sau đó, các tham số
kiến trúc được học với bài toán tối ưu hóa bi-level sau:
min α Lval(w*(α), α);
s.t: w*(α) = arg minw Ltrain(w, α).

Sau khi thu được các tham số kiến trúc α, kiến trúc rời rạc cuối
cùng được tạo bởi: 1) thiết lập o(i,j) = arg maxo∈O,o≠zero p(i,j)o,
và 2) đối với mỗi node trung gian, chọn hai cạnh đến có hai giá trị
lớn nhất của maxo∈O,o≠zero p(i,j)o. Các chi tiết kỹ thuật hơn có
thể được tìm thấy trong bài báo DARTS gốc [21].

2.2 Vấn đề Sụp đổ

Đã được quan sát trong DARTS rằng rất nhiều kết nối bỏ qua được
liên quan đến kiến trúc được chọn, làm cho kiến trúc trở nên nông
và hiệu suất kém. Như một ví dụ, hãy xem xét tìm kiếm trên
CIFAR100. Giá trị của kết nối bỏ qua (đường màu xanh lá cây
trong Hình 2(i)(c)) trở nên lớn khi số lượng epoch tìm kiếm lớn,
do đó số lượng kết nối bỏ qua tăng lên trong kiến trúc được chọn
như được hiển thị trong đường màu xanh lá cây trong Hình 2(i)(a).
Một mạng nông như vậy có ít tham số có thể học được hơn so với
các mạng sâu, và do đó nó có sức mạnh biểu đạt yếu hơn. Kết
quả là, các kiến trúc có nhiều kết nối bỏ qua có hiệu suất kém,
tức là sụp đổ, được chỉ ra như đường màu xanh lam trong Hình
2(i)(a)¹, xem thêm thí nghiệm trong Phụ lục C. Để trực quan hơn,
chúng tôi vẽ các kiến trúc được chọn từ các epoch tìm kiếm khác
nhau trên CIFAR100 trong

¹Tất cả các kiến trúc được huấn luyện và đánh giá đầy đủ trong
cùng một thiết lập.

Hình 1(b). Khi số lượng epoch tìm kiếm tăng lên, số lượng kết nối
bỏ qua trong kiến trúc được chọn cũng tăng lên. Hiện tượng như
vậy cũng có thể được quan sát trên các tập dữ liệu khác, như
CIFAR10 và Tiny-ImageNet-200.

Để tránh sụp đổ, người ta có thể đề xuất điều chỉnh các siêu tham
số tìm kiếm, chẳng hạn như 1) điều chỉnh tỷ lệ học, 2) thay đổi
tỷ lệ dữ liệu huấn luyện và validation, và 3) thêm regularization
trên kết nối bỏ qua như dropout. Thật không may, các phương pháp
như vậy chỉ làm giảm bớt sự sụp đổ tại một số epoch tìm kiếm
nhất định nhưng sự sụp đổ cuối cùng sẽ xuất hiện, ngụ ý rằng
việc chọn siêu tham số không phải là nguyên nhân cơ bản của sự
sụp đổ.

2.3 Overfitting và Phân tích

Để tìm hiểu sự sụp đổ của DARTS, chúng tôi quan sát rằng trọng
số mô hình trong mô hình một lần gặp phải "overfitting" trong
quá trình tìm kiếm. Trong tối ưu hóa bi-level của DARTS, trọng
số mô hình w được cập nhật với dữ liệu huấn luyện trong khi các
tham số kiến trúc α được cập nhật với dữ liệu validation. Vì trọng
số mô hình w trong mô hình một lần được tham số hóa quá mức,
w có xu hướng khớp dữ liệu huấn luyện tốt, trong khi dữ liệu
validation bị underfitting vì số lượng α bị giới hạn. Cụ thể, trong
tập dữ liệu CIFAR10/100, độ chính xác huấn luyện có thể đạt 99%
trong khi độ chính xác validation chỉ là 88% trong CIFAR10 và
60% trong CIFAR100. Điều này ngụ ý "overfitting" vì khoảng cách
giữa lỗi huấn luyện và validation lớn.²

Chúng tôi cho thấy rằng overfitting của trọng số mô hình là nguyên
nhân chính của sự sụp đổ của DARTS. Cụ thể, ở trạng thái ban
đầu, trọng số mô hình underfit dữ liệu huấn luyện và khoảng cách
giữa lỗi huấn luyện và validation nhỏ. Do đó, các tham số kiến
trúc α và trọng số mô hình w trở nên tốt hơn cùng nhau. Sau một
số epoch tìm kiếm nhất định, trọng số mô hình overfit dữ liệu
huấn luyện. Tuy nhiên, trên dữ liệu validation, chúng không khớp
tốt như dữ liệu huấn luyện và các cell đầu tiên của mô hình có
thể thu được biểu diễn đặc trưng cấp thấp tương đối tốt hơn so
với những cell trong các cell cuối cùng.

Nếu chúng ta cho phép các cell khác nhau có các kiến trúc riêng
biệt trong mô hình một lần, các cell cuối cùng có nhiều khả năng
chọn nhiều kết nối bỏ qua hơn để thu được biểu diễn đặc trưng
tốt trực tiếp từ các cell đầu tiên. Hình 2(ii) cho thấy các kiến trúc
cell bình thường đã học tại các lớp khác nhau nếu chúng ta tìm
kiếm các kiến trúc khác nhau tại các giai đoạn khác nhau³. Có
thể thấy rằng

²Định nghĩa "overfitting" hơi khác so với định nghĩa chung như
là lỗi huấn luyện thấp hơn dẫn đến lỗi validation cao hơn. Tuy
nhiên, trong cả hai trường hợp, khoảng cách giữa lỗi huấn luyện
và validation đều lớn.

³Các giai đoạn được phân tách bằng các cell reduction, và mỗi
giai đoạn bao gồm một số cell được xếp chồng.

thuật toán có xu hướng chọn các kiến trúc sâu với các operation
có thể học được trong các cell đầu tiên (Hình 2(ii)(a)), trong khi
các kiến trúc có nhiều kết nối bỏ qua được ưa thích trong các cell
cuối cùng (Hình 2(ii)(c)). Nó phù hợp với phân tích trước đó rằng
các lớp cuối cùng sẽ chọn kết nối bỏ qua. Nếu các cell khác nhau
bị buộc phải có cùng kiến trúc, như DARTS làm, việc tiếp tục tìm
kiếm và fitting sẽ đẩy kết nối bỏ qua được phát sóng từ các cell
cuối cùng đến các cell đầu tiên, làm cho số lượng kết nối bỏ qua
trong kiến trúc được chọn tăng lên dần dần. Chúng ta có thể thấy
hiện tượng overfitting của trọng số mô hình gây ra sự suy giảm
của kiến trúc mô hình được chọn.

Hiện tượng overfitting này có thể được minh họa thêm như một
bài toán phân loại nhị phân tổng hợp được hiển thị trong Hình 3.
Mô hình một lần là một mạng 2 lớp được định nghĩa là o(x) =
w⊤r(α0x + (1-α0)Wx), trong đó W, wr là trọng số mô hình với
∥wr∥ = r, và α0 là tham số kiến trúc (được hiển thị trong Hình
3(a)). Dữ liệu huấn luyện T và dữ liệu validation V được sử dụng
cho tìm kiếm kiến trúc là các biểu diễn đặc trưng 2-D và chúng
là hỗn hợp các Gaussian sao cho T = {(xi, yi); yi · xi ∼ N(te,
σ²tI)}, V = {(xi, yi); yi · xi ∼ N(ve, σ²vI)} trong đó e = 1/√2(1,1)⊤.
Cả nhãn huấn luyện và validation đều cân bằng sao cho số lượng
dữ liệu với nhãn 1 giống như số lượng dữ liệu với nhãn -1. Nếu
mô hình một lần được tìm kiếm với DARTS trong đó mất mát
huấn luyện và validation là Ltrain = ∑(xi,yi)∈T l(o(xi), yi),
Lval = ∑(xi,yi)∈V l(o(xi), yi), l(o, y) = log(1 + exp(-yo)),
thì trong những điều kiện nhất định, kết nối bỏ qua sẽ được chọn,
điều này được tóm tắt trong bổ đề sau.

Bổ đề 1 Xem xét tìm kiếm với một bài toán phân loại nhị phân
trong đó dữ liệu và mô hình một lần được định nghĩa ở trên (được
hiển thị trong Hình 3). Giả sử (1) các biểu diễn đặc trưng được
chuẩn hóa sao cho 1/2 · σ²t + σ²t = 1/2 · σ²v + σ²v = 1/4⁴,

⁴Ký hiệu x[i] là chiều thứ i của x. Vì các đặc trưng huấn luyện
được chuẩn hóa, đối với dữ liệu huấn luyện, nó phải là ET[x] = 0
và DT[x[i]] = 1 đối với bất kỳ i = 0, 1 nào. Đối với cả hai chiều,
rõ ràng là ET[x] = 0 vì các nhãn cân bằng, và DT[x[i]] = 1/2 · σ²t
+ σ²t đối với bất kỳ i = 0, 1 nào vì y · x[i] ∼ N(t/√2, σ²t); (x, y) ∈ T.
Sau đó chúng ta có 1/2 · σ²t + σ²t = 1. Điều tương tự cũng đúng
với dữ liệu validation sao cho 1/2 · σ²v + σ²v = 1.

và các đầu ra của lớp fully-connected (fc) {Wx} được chuẩn hóa
như trên; (2) các mất mát trên được định nghĩa và được huấn
luyện với tối ưu hóa bi-level. Sau đó chúng ta có

P1: Nếu σt nhỏ, đối với bất kỳ α0 nào, wr → re, W → W*,
trong đó W*x = tx · e, và tx = 2√(2+σ²t) · e⊤x.

P2: Nếu σt nhỏ và σv > ε₀(r) trong đó ε₀(r) là một hàm đơn
điệu giảm, thì dLval/dα0 < 0, điều này ngụ ý rằng α0 sẽ trở nên
lớn hơn với gradient descent.

Chứng minh có thể được tìm thấy trong Phụ lục A. Trong bài
báo này, các biểu diễn đặc trưng được thảo luận trong Bổ đề 1
tương ứng với các biểu diễn đặc trưng lớp cuối cùng trong mô
hình một lần. Cụ thể, ở đầu quy trình tìm kiếm, σv lớn và r nhỏ;
Khi overfitting xảy ra, r trở nên lớn hơn để làm cho dữ liệu huấn
luyện có thể phân tách được hơn trong khi σv vẫn lớn. σt có xu
hướng nhỏ trong quá trình tìm kiếm. Theo Bổ đề 1, ở đầu tìm
kiếm, σv lớn trong khi r nhỏ và do đó σv < ε₀(r), sau đó các
operation có thể học được được ưa thích (hình bên trái trong
Hình 3(b)). Khi overfitting xảy ra, σv vẫn tương đối lớn sao cho
σv > ε₀(r) vì r lớn hơn làm cho ε₀(r) nhỏ, sau đó kết nối bỏ qua
có xu hướng được chọn (hình bên phải trong Hình 3(b)).

3 Phương pháp Dừng Sớm

Vì vấn đề sụp đổ của DARTS được gây ra bởi "overfitting" của
mô hình một lần trong tối ưu hóa bi-level như đã chỉ ra trong
Phần 2.2, chúng tôi đề xuất một mô hình "dừng sớm" đơn giản
và hiệu quả dựa trên DARTS để tránh sụp đổ. Cụ thể, quy trình
tìm kiếm nên được dừng sớm tại một tiêu chí thích ứng, khi
DARTS bắt đầu sụp đổ. Mô hình như vậy dẫn đến cả hiệu suất
tốt hơn và chi phí tìm kiếm ít hơn so với DARTS gốc. Chúng tôi
sử dụng DARTS+ để ký hiệu thuật toán DARTS với tiêu chí dừng
sớm của chúng tôi.

Chúng tôi muốn nhấn mạnh rằng dừng sớm là cần thiết và cần
được chú ý nhiều hơn. Đã được tìm thấy rằng các kết nối quan
trọng được xác định trong giai đoạn đầu của huấn luyện [1]. Các
thay đổi đáng kể và quan trọng xảy ra trong giai đoạn sớm nhất
của huấn luyện [7].

Bên cạnh vấn đề "overfitting", một động lực khác của "dừng sớm"
là thứ hạng của các tham số kiến trúc α của các operation quan
trọng vì chỉ có operation với α tối đa được chọn trong kiến trúc
được chọn. Trong quá trình tìm kiếm, dữ liệu validation có sở
thích khác nhau đối với các operation có thể học được, tương
ứng với thứ hạng của các giá trị α. Nếu thứ hạng của α không
ổn định, kiến trúc quá nhiễu để được chọn; trong khi khi nó trở
nên ổn định, các operation có thể học được trong kiến trúc được
chọn cuối cùng không thay đổi, và chúng ta có thể coi điểm này
là điểm tìm kiếm bão hòa. Các vòng tròn màu đỏ trong Hình
2(i)(a-b) ký hiệu các điểm tìm kiếm bão hòa trên các tập dữ liệu
khác nhau. Nó xác minh rằng sau điểm này, độ chính xác validation
của các kiến trúc được chọn trên tất cả các tập dữ liệu (các đường
màu xanh lam) có xu hướng giảm, tức là sụp đổ. Để kết luận, quy
trình tìm kiếm có thể được "dừng sớm" tại điểm tìm kiếm bão hòa
để chọn các kiến trúc mong muốn cũng như tránh overfitting, và
chúng tôi nhấn mạnh rằng điểm này không có nghĩa là sự hội tụ
của mô hình một lần.

Trước hết, chúng tôi tuân theo kiến trúc dựa trên cell được sử
dụng bởi DARTS. Tiêu chí đầu tiên được phát biểu như sau.

Tiêu chí 1 Quy trình tìm kiếm dừng lại khi có hai hoặc nhiều hơn
hai kết nối bỏ qua trong một cell bình thường.

Lợi thế chính của tiêu chí dừng được đề xuất là sự đơn giản của
nó. So với các biến thể DARTS khác, DARTS+ chỉ cần một vài
sửa đổi dựa trên DARTS, và có thể tăng đáng kể hiệu suất với
thời gian tìm kiếm ít hơn. Vì quá nhiều kết nối bỏ qua sẽ làm
tổn hại hiệu suất của DARTS, trong khi một số lượng phù hợp
kết nối bỏ qua giúp chuyển thông tin từ các lớp đầu tiên đến các
lớp cuối cùng và ổn định quá trình huấn luyện, ví dụ, ResNet
[10], làm cho các kiến trúc đạt hiệu suất tốt hơn. Do đó, dừng
lại bằng Tiêu chí 1 là một lựa chọn hợp lý.

Siêu tham số hai trong Tiêu chí 1 được thúc đẩy bởi P-DARTS
[3], trong đó số lượng kết nối bỏ qua trong cell của kiến trúc
cuối cùng được cắt xuống còn hai một cách thủ công. Tuy nhiên,
DARTS+ về cơ bản khác với P-DARTS trong việc xử lý kết nối
bỏ qua. P-DARTS không can thiệp vào số lượng kết nối bỏ qua
trong quá trình tìm kiếm, mà chỉ thay thế các kết nối bỏ qua dư
thừa bằng các operation khác như hậu xử lý sau khi quy trình
tìm kiếm kết thúc. Ngược lại, DARTS+ của chúng tôi kết thúc
với các kiến trúc mong muốn với số lượng kết nối bỏ qua phù
hợp để tránh sụp đổ của DARTS. Nó kiểm soát số lượng kết nối
bỏ qua trực tiếp hơn và cũng hiệu quả hơn (Xem Bảng 1 để so
sánh hiệu suất giữa DARTS+ và P-DARTS).

Vì thứ hạng ổn định của các tham số kiến trúc cho các operation
có thể học được chỉ ra quy trình tìm kiếm bão hòa trong DARTS,
chúng ta cũng có thể sử dụng tiêu chí dừng sau:

Tiêu chí 2 Quy trình tìm kiếm dừng lại khi thứ hạng của các tham
số kiến trúc cho các operation có thể học được trở nên ổn định
trong một số epoch xác định (ví dụ, 10 epoch).

Có thể thấy từ Hình 2(i) rằng điểm huấn luyện bão hòa (điểm
dừng với Tiêu chí 2) gần với điểm dừng khi Tiêu chí 1 thỏa mãn
(đường nét đứt màu đỏ trong Hình 2(i)(a)). Chúng tôi cũng lưu
ý rằng cả hai tiêu chí đều có thể được sử dụng tự do vì các điểm
dừng gần nhau. Tuy nhiên, Tiêu chí 1 dễ thao tác hơn nhiều,
nhưng nếu người ta cần dừng chính xác hơn hoặc các không gian
tìm kiếm khác được liên quan, Tiêu chí 2 có thể được sử dụng
thay thế. Mười epoch trong Tiêu chí 2 là một siêu tham số và
theo các thí nghiệm của chúng tôi, khi thứ hạng của các toán tử
vẫn giữ nguyên trong hơn 6 epoch, nó có thể được coi là ổn định,
ngụ ý rằng siêu tham số này không nhạy cảm và linh hoạt để
chọn. Chúng tôi tiếp tục lưu ý rằng mô hình dừng sớm của chúng
tôi giải quyết một vấn đề nội tại của DARTS và trực giao với
các thủ thuật khác, do đó nó có tiềm năng được sử dụng trong
các thuật toán dựa trên DARTS khác để đạt hiệu suất tốt hơn.
Hơn nữa, phương pháp của chúng tôi rất dễ bổ sung hơn so với
các phương pháp khác như tính toán eigenvalue của Hessian
trong mất mát validation [39].

Chúng tôi lưu ý rằng các phương pháp tìm kiếm kiến trúc khả vi
tiên tiến gần đây cũng giới thiệu ý tưởng dừng sớm một cách
đặc biệt. Để tránh sụp đổ, P-DARTS [3] sử dụng 1) tìm kiếm
trong 25 epoch thay vì 50 epoch, 2) áp dụng dropout sau kết nối
bỏ qua, và 3) giảm thủ công số lượng kết nối bỏ qua xuống còn
hai. Auto-DeepLab [19] sử dụng ít epoch hơn để tìm kiếm các
tham số kiến trúc và thấy rằng tìm kiếm trong nhiều epoch hơn
không mang lại lợi ích.

PC-DARTS [36] sử dụng kết nối kênh một phần để giảm thời
gian tìm kiếm, và do đó cần nhiều epoch hơn để hội tụ tìm kiếm.
Do đó, thiết lập 50 epoch huấn luyện cũng là một mô hình dừng
sớm ngầm định, xem Phụ lục C.

4 Thí nghiệm và Phân tích

4.1 Tập dữ liệu

Trong phần này, chúng tôi tiến hành các thí nghiệm mở rộng trên
các tập dữ liệu phân loại chuẩn để đánh giá hiệu quả của thuật
toán DARTS+ được đề xuất. Chúng tôi sử dụng bốn tập dữ liệu
phổ biến bao gồm CIFAR10 [15], CIFAR100 [15], Tiny-ImageNet-200⁵
và ImageNet [6]. CIFAR10/100 bao gồm 50K hình ảnh huấn luyện
và 10K hình ảnh thử nghiệm và độ phân giải là 32×32. Tiny-ImageNet-200
chứa 100K hình ảnh huấn luyện 64×64 và 10K hình ảnh thử nghiệm.
ImageNet được lấy từ ILSVRC2012 [28], chứa hơn 1.2M hình
ảnh huấn luyện và 50K hình ảnh validation. Chúng tôi tuân theo
thiết lập chung trên tập dữ liệu ImageNet trong đó các hình ảnh
được thay đổi kích thước thành 224×224 để huấn luyện và thử
nghiệm.

4.2 Hiệu quả của Dừng Sớm trên Các Không gian Tìm kiếm
Khác nhau

Để xác minh hiệu quả của dừng sớm trong DARTS+, chúng tôi
tiến hành các thí nghiệm mở rộng với các tập dữ liệu khác nhau
trên các kiến trúc được chọn tại các epoch khác nhau. Các thí
nghiệm được thực hiện trong hai giai đoạn: tìm kiếm kiến trúc
và đánh giá kiến trúc.

Không gian Tìm kiếm DARTS. Trong không gian tìm kiếm DARTS,
các thiết lập thí nghiệm tương tự như DARTS. Đối với CIFAR10
và CIFAR100, trong giai đoạn tìm kiếm kiến trúc, chúng tôi sử
dụng cùng mô hình một lần như DARTS gốc và các siêu tham số
gần như giống với DARTS ngoại trừ tối đa 60 epoch được áp dụng.
Trong giai đoạn đánh giá kiến trúc, các thiết lập thí nghiệm tuân
theo DARTS gốc

⁵https://tiny-ImageNet.herokuapp.com/

DARTS gốc, ngoại trừ 2000 epoch được sử dụng để hội tụ tốt
hơn. Đối với Tiny-ImageNet-200, trong giai đoạn tìm kiếm, mô
hình một lần gần như giống với CIFAR10/100 ngoại trừ một lớp
convolution 3×3 với stride 2 được thêm vào lớp đầu tiên để giảm
độ phân giải đầu vào từ 64×64 xuống 32×32. Các thiết lập khác
giống như những thiết lập được sử dụng trong CIFAR10/100.

Chúng tôi sử dụng Tiêu chí 1 và 2 cho dừng sớm trong không
gian tìm kiếm DARTS. Các chi tiết khác về thiết lập thí nghiệm
có thể được tham khảo trong Phụ lục B.

Kết quả phân loại của các kiến trúc được chọn tại các epoch
khác nhau được hiển thị trong Hình 2(i). Chúng tôi cũng đánh
dấu điểm "dừng sớm" theo hai tiêu chí tương ứng là "đường nét
đứt màu đỏ" và "vòng tròn màu đỏ". Chúng tôi quan sát thấy
rằng kiến trúc được chọn hoạt động tệ hơn với các epoch lớn
hơn, ngụ ý rằng DARTS gốc gặp phải vấn đề sụp đổ. Ngược lại,
"dừng sớm" có thể tạo ra các kiến trúc tốt tại cả hai tiêu chí dừng,
bất kể loại tập dữ liệu.

Chúng tôi cũng so sánh "dừng sớm" Tiêu chí 1 và 2 trong Bảng
1 và Hình 2(i). Chúng tôi quan sát thấy rằng cả hai tiêu chí đều
đạt hiệu suất tương đương trên tất cả các tập dữ liệu vì các điểm
dừng rất gần nhau.

Không gian Tìm kiếm MobileNetV2 và ResNet. Để xác minh thêm
hiệu quả của DARTS+, chúng tôi sử dụng MobileNetV2 [29] và
ResNet [10] làm backbone để xây dựng không gian kiến trúc [2].
Đối với không gian tìm kiếm MobileNetV2, chúng tôi giới thiệu
một tập hợp mobile inverted bottleneck convolution (MBConv)
với các kích thước kernel và tỷ lệ mở rộng khác nhau để xây
dựng khối tìm kiếm. Đối với không gian tìm kiếm ResNet, chúng
tôi xây dựng mô hình một lần bằng cách thay thế khối residual
bằng một tập hợp các operation ứng viên, trong đó chúng tôi giữ
kết nối bỏ qua trong khối residual và liên quan đến 10 operation
ứng viên. Trong cả hai không gian tìm kiếm, softmax được áp
dụng cho các tham số kiến trúc để tính toán trọng số, được sử
dụng để xác định kiến trúc được chọn. Các thí nghiệm được tiến
hành trên tập dữ liệu CIFAR100. Chi tiết về các không gian tìm
kiếm và thiết lập thí nghiệm về tìm kiếm kiến trúc và đánh giá
kiến trúc được tóm tắt trong Phụ lục B.

Vì kết nối bỏ qua không được liên quan đến các không gian tìm
kiếm, chúng tôi sử dụng "dừng sớm" Tiêu chí 2. Kết quả phân
loại của các kiến trúc được chọn tại các epoch khác nhau được
hiển thị trong Hình 4. Thời gian "dừng sớm" với Tiêu chí 2 được
đánh dấu là "vòng tròn màu đỏ". Có thể thấy rằng kiến trúc được
chọn với "dừng sớm" đạt hiệu suất tương đối tốt nhất, so với
kiến trúc tìm kiếm ngẫu nhiên (epoch 0) và kiến trúc trong các
epoch lớn.

4.3 So sánh với Hiện đại

Trừ khi được chỉ định, chúng tôi sử dụng không gian tìm kiếm
DARTS và "dừng sớm" Tiêu chí 1 để đánh giá DARTS+. Lưu ý
rằng các điểm dừng bởi Tiêu chí 1 và 2 gần như giống nhau trong
không gian tìm kiếm được đề xuất, như đã thảo luận trong Phần
4.2.

Đối với các tập dữ liệu CIFAR10, CIFAR100, và Tiny-Imagenet-200,
các thiết lập thí nghiệm về cả giai đoạn tìm kiếm kiến trúc và
đánh giá kiến trúc có thể được tìm thấy trong Phần 4.2 và Phụ
lục B. Đối với ImageNet, theo [36], mô hình một lần bắt đầu với
ba lớp convolution 3×3 với stride 2 để giảm độ phân giải từ
224×224 xuống 28×28, và phần còn lại của mạng bao gồm 8 cell.
Chúng tôi chọn 10% dữ liệu từ tập huấn luyện để cập nhật trọng
số mô hình, và 10% khác để cập nhật các tham số kiến trúc. Trong
giai đoạn đánh giá kiến trúc, chúng tôi huấn luyện mô hình trong
800 epoch với kích thước batch 2048 để hội tụ tốt hơn. Các thiết
lập thí nghiệm khác gần như giống với những thiết lập trong
DARTS, có thể được tìm thấy trong Phụ lục B.

Kết quả Tìm kiếm và Phân tích. DARTS+ được đề xuất cần ít
thời gian tìm kiếm hơn vì "dừng sớm" được áp dụng. Đối với
CIFAR10, quy trình tìm kiếm yêu cầu 0.4 ngày GPU với một GPU
Tesla V100 duy nhất và dừng lại khoảng epoch 35. Đối với CIFAR100,
thời gian tìm kiếm là 0.2 ngày GPU và quy trình tìm kiếm dừng
lại khoảng epoch 18. Đối với Tiny-ImageNet-200, tìm kiếm dừng
lại khoảng epoch 10. Đối với ImageNet, quy trình tìm kiếm liên
quan đến 200 epoch và yêu cầu 6.8 ngày GPU trên GPU Tesla
P100.

Số lượng kết nối bỏ qua được hiển thị trong Hình 2(i) ngụ ý
rằng các cell được tìm kiếm bởi DARTS+ chứa một số lượng nhỏ
kết nối bỏ qua, cho thấy rằng DARTS+ thành công trong việc tìm
kiếm với tất cả ba tập dữ liệu bao gồm CIFAR10/100, Tiny-ImageNet-200,
và ImageNet. Tuy nhiên, DARTS gốc thất bại trong việc tìm kiếm
trên CIFAR100 vì kiến trúc được chọn đầy kết nối bỏ qua, và
hầu hết các công trình trước đó về tìm kiếm khả vi [3, 21, 34]
không tìm kiếm trên ImageNet. Kiến trúc được chọn có thể được
tìm thấy trong Phụ lục B.

Đánh giá Kiến trúc trên CIFAR10 và CIFAR100. Kết quả đánh
giá được tóm tắt trong Bảng 1. Đối với mỗi cell được chọn từ
CIFAR10 hoặc CIFAR100, chúng tôi báo cáo hiệu suất trên cả
hai tập dữ liệu. Với mô hình "dừng sớm" đơn giản, chúng tôi
đạt được kết quả tốt nhất với 2:32% lỗi test trên CIFAR10 và
14:87% lỗi test trên CIFAR100. DARTS+ được đề xuất đơn giản
và tốt hơn nhiều so với các thuật toán DARTS đã sửa đổi khác
như P-DARTS và PC-DARTS. ProxylessNAS sử dụng không gian
tìm kiếm khác nhau, và nó liên quan đến thời gian tìm kiếm nhiều
hơn. Hơn nữa, DARTS+ dễ thực hiện hơn nhiều so với các biến
thể DARTS đã sửa đổi khác bao gồm ASAP.

Chúng tôi tiếp tục tăng số kênh ban đầu từ 36 lên 50 và thêm
nhiều thủ thuật tăng cường hơn bao gồm AutoAugment [5] và
mixup [40] để đạt kết quả tốt hơn. Bảng 1 cho thấy rằng DARTS+
đạt được 1:68% lỗi test ấn tượng trên CIFAR10 và 13:03% trên
CIFAR100, chứng minh hiệu quả của DARTS+.

Đánh giá Kiến trúc trên Tiny-ImageNet-200. Đối với DARTS và
DARTS+, chúng tôi sử dụng kiến trúc tìm kiếm trực tiếp từ Tiny-ImageNet-200
để đánh giá. Chúng tôi cũng chuyển các kiến trúc tìm kiếm từ
các thuật toán khác để so sánh công bằng. Kết quả được hiển
thị trong Bảng 2. DARTS+ đạt được 28:3% lỗi test hiện đại với
Tiêu chí 1 và 27:6% lỗi test với Tiêu chí 2. Lưu ý rằng kiến trúc
tìm kiếm trên Tiny-ImageNet-200 với DARTS có kích thước tham
số ít hơn và hoạt động kém hơn nhiều, bởi vì DARTS gặp phải
sự sụp đổ và kiến trúc tìm kiếm với DARTS chứa rất nhiều kết
nối bỏ qua.

Đánh giá Kiến trúc trên ImageNet. Chúng tôi sử dụng kiến trúc
tìm kiếm trực tiếp từ ImageNet để đánh giá, và kiến trúc từ
CIFAR100 để kiểm tra khả năng chuyển đổi của kiến trúc được
chọn. Kết quả thí nghiệm được hiển thị trong Bảng 3. Lưu ý rằng
chúng tôi tái thực hiện PC-DARTS và các kết quả được báo cáo.
Khi tìm kiếm trên ImageNet với DARTS+ được đề xuất, kiến trúc
được chọn đạt được 23:9%=7:4% lỗi top-1/top-5 ấn tượng, và
kiến trúc chuyển từ CIFAR100 đạt được 23:7%=7:2% lỗi hiện
đại. Kết quả ngụ ý rằng DARTS với "dừng sớm" thành công trong
việc tìm kiếm kiến trúc tốt với hiệu suất ấn tượng trên các tập
dữ liệu quy mô lớn với thời gian hạn chế.

Chúng tôi cũng áp dụng SE-module [12] trong kiến trúc chuyển
từ CIFAR100, và giới thiệu AutoAugment [5] và mixup [40] để
huấn luyện nhằm có được mô hình tốt hơn. Kết quả được hiển
thị trong Bảng 3, và chúng tôi đạt được 22:5%=6:4% lỗi top-1/top-5
chỉ với thêm 3M flops, cho thấy hiệu quả của kiến trúc được chọn.

5 Kết luận

Trong bài báo này, chúng tôi tiến hành phân tích toàn diện và
các thí nghiệm mở rộng để cho thấy rằng DARTS gặp phải vấn
đề sụp đổ, chủ yếu được gây ra bởi overfitting của mô hình một
lần trong DARTS. Chúng tôi đề xuất "DARTS+", trong đó mô hình
"dừng sớm" được giới thiệu để tránh sụp đổ của DARTS. Các thí
nghiệm cho thấy rằng chúng tôi thành công trong việc tìm kiếm
trên các tập dữ liệu chuẩn khác nhau bao gồm ImageNet quy mô
lớn với số ngày GPU hạn chế, và các kiến trúc kết quả đạt hiệu
suất hiện đại trên tất cả các tập dữ liệu chuẩn. Hơn nữa, các tiêu
chí "dừng sớm" được đề xuất có thể được áp dụng cho các không
gian tìm kiếm khác nhau và nhiều dấu hiệu tiến bộ gần đây của
DARTS có thể sử dụng "dừng sớm" để đạt kết quả tốt hơn.
