# 2310.05424.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/early-stopping/2310.05424.pdf
# Kích thước tệp: 1071223 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Khung Thoát Sớm Nhanh và Mạnh mẽ cho
Mô hình Ngôn ngữ Tự hồi quy với Giải mã Đồng bộ Song song
Sangmin Bae1∗Jongwoo Ko1∗Hwanjun Song2†Se-Young Yun1†
1KAIST AI2AWS AI
{bsmn0223, jongwoo.ko, yunseyoung}@kaist.ac.kr hwanjuns@amazon.com
https://github.com/raymin0223/fast_robust_early_exit
Tóm tắt
Để giải quyết độ trễ suy luận cao được thể hiện
bởi các mô hình ngôn ngữ tự hồi quy, các nghiên cứu
trước đây đã đề xuất một khung thoát sớm
phân bổ các đường dẫn tính toán thích ứng
cho từng token dựa trên độ phức tạp của việc tạo
token tiếp theo. Tuy nhiên, chúng tôi
quan sát thấy một số thiếu sót, bao gồm suy giảm
hiệu suất do cơ chế sao chép trạng thái
hoặc nhiều đường thoát, và độ nhạy cảm
với ngưỡng tin cậy thoát. Do đó,
chúng tôi đề xuất khung Thoát Sớm Nhanh và
Mạnh mẽ (FREE), kết hợp mô-đun nông-sâu
và giải mã đồng bộ song song. Khung của chúng tôi
cho phép suy luận nhanh hơn bằng cách đồng bộ hóa
quá trình giải mã của token hiện tại với
các token thoát sớm đã được xếp chồng trước đó.
Hơn nữa, vì giải mã song song cho phép chúng ta
quan sát các dự đoán từ cả mô hình nông và sâu,
chúng tôi trình bày một bộ ước tính ngưỡng thích ứng
mới khai thác mô hình hỗn hợp Beta để xác định
ngưỡng tin cậy phù hợp. Chúng tôi đã chứng minh
thực nghiệm tính ưu việt của khung đề xuất
trên các nhiệm vụ tạo sinh mở rộng.

1 Giới thiệu
Những tiến bộ gần đây trong các mô hình ngôn ngữ
tự hồi quy (Brown et al., 2020; Raffel et al., 2020;
Hoffmann et al., 2022; Touvron et al., 2023) đã
cách mạng hóa chất lượng tạo sinh ngôn ngữ
trong các nhiệm vụ tạo sinh khác nhau, bao gồm trả lời
câu hỏi (Rajpurkar et al., 2016a), tóm tắt
(Nallapati et al., 2016; Fabbri et al., 2019b),
và dịch máy (Cettolo et al., 2017a).
Tuy nhiên, những mô hình transformer lớn này đã
cho thấy độ trễ suy luận cao do số lượng
lớp đáng kể và bước giải mã tự hồi quy.
Vì nhiều chồng lớp transformer phải được
tính toán tuần tự cho từng token riêng lẻ,
quá trình suy luận gây ra gánh nặng tính toán
đáng kể và cản trở khả năng thích ứng thời gian
thực của chúng (Jiao et al., 2020).

Với sự cần thiết phải đẩy nhanh độ trễ suy
luận, khung thoát sớm (Elbayad et al.,
2020; Liu et al., 2021; Schuster et al., 2022)
nổi lên như một phương pháp đầy hứa hẹn
phân bổ động các đường dẫn tính toán dựa trên
độ phức tạp tạo sinh cho từng token. Như được
minh họa trong Hình 1a, các token tương đối dễ
dự đoán token tiếp theo tạo ra các dự đoán
nhất quán chỉ với vài phép tính lớp, trong khi
những token có độ khó cao hơn yêu cầu tính toán
qua số lượng lớp lớn hơn để tạo ra dự đoán
chính xác. Trong kịch bản lý tưởng, phương pháp
thoát sớm cho phép các mô hình đạt được gia tốc
đáng kể trong suy luận mà không ảnh hưởng đến
chất lượng tạo sinh so với mô hình đầy đủ.

Tuy nhiên, phân tích mở rộng của chúng tôi đã
xác định bốn thách thức trong khung thoát sớm.
Thứ nhất, mặc dù có tiềm năng thoát ở các lớp
sớm hơn, các trạng thái key và value cho các lớp
còn lại vẫn cần thiết để xử lý các token tiếp theo.
Trong khi các công trình trước đây đã đề xuất
cơ chế sao chép trạng thái (Elbayad et al., 2020;
Schuster et al., 2022) để tính toán hiệu quả
các trạng thái này bằng cách tái sử dụng trạng thái
ẩn từ lớp thoát sớm, các phát hiện của chúng tôi
cho thấy phương pháp này hoạt động kém với
các mô hình lớn hơn và chuỗi đầu ra dài hơn
(xem Phần 4.1). Ngoài ra, việc đặt tất cả các lớp
làm vị trí thoát có thể không đảm bảo suy luận
nhanh hơn do (1) hiệu suất kém của các lớp
sớm hơn có thể tạo ra đầu ra chuỗi dài bất thường,
và (2) chi phí tính toán từ đo lường tin cậy
ở mọi lớp (xem Phần 4.2 và 4.3). Cuối cùng,
việc đạt được mức độ độ trễ và độ chính xác
mong muốn với thoát sớm phụ thuộc nhiều vào
việc chọn ngưỡng tin cậy phù hợp cho nhiệm vụ
đích. Điều này thường đòi hỏi nỗ lực đáng kể
và chi phí tính toán bổ sung (xem Phần 4.4).
Do đó, những thách thức này đòi hỏi một phương pháp
mới nhất quán arXiv:2310.05424v1  [cs.CL]  9 Oct 2023

--- TRANG 2 ---
𝐡!"𝐡"!𝐡#$%!𝐡&!
<S>Earth𝐡#!𝐡#"
isflat𝐡&!Lớp 𝟏𝐡!!𝐡"!𝐡&!Earthisflat<E>Lớp 𝑳𝐡!"
🤨
Lớp 𝟐𝐡"!(a) Thoát Sớm Thông thường
isDobby𝐡!"#!𝐡$"#!𝐡%"#!
<S>Earth𝐡$!𝐡%!𝐡!$𝐡%$
is𝐡&$Lớp 𝟏𝐡!!𝐡$$Earthisround<E>0.850.950.1Conf (nông) :Giống nhau? :Tập Hiệu chuẩn (𝓓𝒄)
Mô hình Hỗn hợp Betatìm MLEFalseTrueTrueEarthisflatPred(nông) :EarthisPred(sâu) :Lớp 𝑳(sâu)
🤩
roundroundLớp 𝟐(nông)𝐡&! (b) Thoát Sớm Nhanh và Mạnh mẽ (FREE)

Hình 1: Tổng quan về khung FREE của chúng tôi so với khung thoát sớm thông thường. FREE thể hiện
ba khác biệt chính: (1) FREE sử dụng mô-đun nông-sâu tận dụng hai điểm thoát thay vì sử dụng
tất cả các lớp làm điểm thoát, (2) FREE thay thế cơ chế sao chép trạng thái (màu vàng) bằng giải mã
đồng bộ song song (màu đỏ) để ngăn suy giảm hiệu suất đồng thời tăng tốc độ suy luận, và (3) FREE
sử dụng bộ ước tính ngưỡng thích ứng để xác định giá trị ngưỡng phù hợp cho mỗi tập dữ liệu trong suy luận.

thể hiện hiệu suất cao và độ trễ thấp qua các
mô hình ngôn ngữ và tập dữ liệu đa dạng.

Trong bài báo này, chúng tôi giới thiệu khung
Thoát Sớm Nhanh và Mạnh mẽ (FREE) kết hợp
mô-đun nông-sâu và giải mã đồng bộ song song.
Khung của chúng tôi không chỉ cung cấp tăng tốc
và hiệu suất nhất quán ngay cả đối với các mô hình
lớn hơn và chuỗi đầu ra dài hơn, mà còn loại bỏ
nhu cầu cho quá trình tính toán đắt đỏ để tìm
ngưỡng thoát phù hợp.

Cụ thể, mô-đun nông-sâu chia nhỏ các đường dẫn
tính toán thành mô hình nông (với số lượng lớp
sớm được chỉ định) và mô hình sâu (bao gồm tất cả
các lớp). Giải mã đồng bộ song song của chúng tôi
tích lũy các token thoát sớm liên tiếp chỉ đi qua
mô hình nông cho đến khi gặp token không thoát.
Do đó, chúng tôi đồng bộ hóa quá trình giải mã
của token không thoát hiện tại với các token
đã xếp chồng trước đó, như được thể hiện ở bên
trái Hình 1b. Điều này ngăn suy giảm hiệu suất
bằng cách sử dụng key và value attention thực tế
được tính toán thay vì các trạng thái xấp xỉ
thông qua sao chép trạng thái, đồng thời đạt được
phương pháp hiệu quả hơn so với giải mã từng
token tự hồi quy. Hơn nữa, chúng tôi thiết kế
một bộ ước tính ngưỡng thích ứng mới, như được
thể hiện ở bên phải Hình 1b, bằng cách tận dụng
thực tế rằng giải mã song song xuất ra dự đoán
ngay cả cho các token thoát sớm từ mô hình sâu.
Bộ ước tính này sử dụng mô hình hỗn hợp Beta (BMM)
để nắm bắt mối tương quan giữa điểm tin cậy
và sự phù hợp dự đoán của hai mô hình, xác định
ngưỡng tin cậy phù hợp cho từng tập dữ liệu.

Trong thực tế, chúng tôi chứng minh hiệu quả
của khung FREE trên các nhiệm vụ tạo sinh mở rộng.

2 Công trình Liên quan

2.1 Khung Thoát Sớm
Khi kích thước của các mô hình ngôn ngữ tăng
đáng kể, đã có nhiều nỗ lực để phát triển các
phương pháp giải mã hiệu quả giảm chi phí
tính toán của các nhiệm vụ tạo sinh ngôn ngữ.
Được thúc đẩy bởi các tài liệu trước đây (Teerapittayanon et al.,
2016; Graves, 2016; Zhang et al., 2019a), Elbayad
et al. (2020) đã giới thiệu khung thoát sớm
cho suy luận nhanh hơn, điều chỉnh động
độ sâu của bộ giải mã cho từng tạo sinh token
bằng cách đưa ra dự đoán ở lớp trung gian.
Để đạt được sự cân bằng tốt hơn giữa tốc độ
và độ chính xác, Schuster et al. (2022) gần đây
đã khám phá các phương pháp ngưỡng tin cậy,
bao gồm các biện pháp tin cậy khác nhau, hàm
ngưỡng giảm dần, và phương pháp hiệu chuẩn.

Tuy nhiên, các thí nghiệm của họ chủ yếu được
thực hiện trên các mô hình giải mã kích thước nhỏ,
cần thiết xác thực thêm trên các mô hình lớn hơn.
Ngoài ra, các phương pháp của họ yêu cầu thời gian
đào tạo bổ sung cho các kiểm tra thống kê trên
tập hiệu chuẩn bổ sung, điều này ngăn chúng
khỏi các kịch bản triển khai thực tế.

2.2 Giải mã Song song
Giải mã không tự hồi quy, tạo ra nhiều token
đầu ra song song, ban đầu được đề xuất bởi
Gu et al. (2018). Một số công trình (Ghazvininejad et al.,
2019; Gu and Kong, 2021; Savinov et al., 2022;
Santilli et al., 2023) từ đó đã tập trung vào
việc nâng cao chất lượng tạo sinh trong các
nhiệm vụ dịch máy. Tiếp theo, Leviathan
et al. (2023) đã giới thiệu giải mã suy đoán
cho các nhiệm vụ tạo sinh chuỗi. Trong phương pháp này,

--- TRANG 3 ---
mô hình xấp xỉ (kích thước nhỏ) dự đoán đầu ra
tự hồi quy, trong khi mô hình đích (kích thước lớn)
chạy song song để xác minh việc chấp nhận các
dự đoán được thực hiện bởi mô hình xấp xỉ.
Chỉ với các token được chấp nhận, họ lấy mẫu lại
token tiếp theo từ phân phối điều chỉnh. Các
phương pháp liên quan đã được đề xuất bởi Chen et al.
(2023) và Kim et al. (2023), nơi họ cũng sử dụng
hai mô hình có độ sâu khác nhau và tập trung vào
việc tinh chỉnh mô hình nhỏ thông qua lấy mẫu
suy đoán hoặc chính sách rollback theo cách
không tự hồi quy.

Phương pháp của chúng tôi khác biệt đáng kể
so với các công trình nêu trên vì chúng tôi tập trung
vào khung thoát sớm bằng cách giới thiệu giải mã
đồng bộ song song trong một mạng duy nhất,
kết hợp mô-đun nông-sâu. Trong khi chúng tôi
cũng tận dụng lợi thế của việc đồng thời thu được
dự đoán từ các mô hình có độ sâu khác nhau,
chúng tôi nhằm phát triển phương pháp ước tính
mới và hiệu quả để xác định thích ứng ngưỡng
tối ưu cho từng tập dữ liệu. Đáng chú ý rằng
các chiến lược tinh chỉnh của họ có thể dẫn đến
tăng độ trễ không giới hạn vì họ khởi động lại
từ các dự đoán không chính xác.

3 Kiến thức Cơ bản
Mạng Transformer (Vaswani et al., 2017) được
cấu thành từ L lớp, trong đó mỗi lớp bao gồm
hai lớp con, lớp multi-head attention (MHA)
và lớp feed-forward network (FFN). Phép tính
cho trạng thái ẩn tại bước thời gian t+1 thông qua
các khối Transformer xếp chồng như sau:

hℓ_{t+1} = Transformer^ℓ(h^{ℓ-1}_{t+1}), ℓ ∈ [1, L],

trong đó h^0_{t+1} là đầu ra lớp embedding của y_t
đại diện cho token được tạo ra tại bước thời gian t.
Sau lớp thứ L của mạng giải mã, token dự đoán
ŷ_{t+1} được xác định bởi đầu ra xác suất từ
bộ phân loại softmax W_L:

p(y_{t+1}|h^L_{t+1}) = softmax(W^⊺_L h^L_{t+1})

Tuy nhiên, không giống như LM tiêu chuẩn,
khung thoát sớm cho phép tạo ra token tiếp theo
ở các lớp sớm hơn bằng cách sử dụng p(y_{t+1}|h^ℓ_{t+1}).
Nếu điểm tin cậy c^ℓ lớn hơn ngưỡng được định
trước, chúng ta có thể đưa ra dự đoán tại bước
thời gian t+1 là arg max p(y_{t+1}|h^ℓ_{t+1}).
Trong khi các bộ phân loại có thể được tham số hóa
độc lập hoặc chia sẻ qua L lớp, hầu hết các
phương pháp thoát sớm (Elbayad et al., 2020;
Liu et al., 2021; Schuster et al., 2022) sử dụng
bộ phân loại chia sẻ do số lượng lớn tham số
gây ra bởi kích thước từ vựng khổng lồ.

Bảng 1: So sánh điểm ROUGE-L giữa mô hình đầy đủ,
được tinh chỉnh sử dụng tất cả đầu ra lớp, và oracle-exiting.
Chúng tôi cũng đo độ tương tự cosine giữa trạng thái ẩn
của lớp cuối và lớp oracle-exited.

Dataset | Model     | Full M. | Oracle | Sim.
--------|-----------|---------|--------|-------
SAMSum  | T5-small  | 44.84   | 44.17 (-0.67) | 0.913
        | T5-large  | 48.82   | 47.58 (-1.24) | 0.809
CNN/DM  | T5-small  | 37.82   | 37.60 (-0.22) | 0.902
        | T5-large  | 41.15   | 40.15 (-1.00) | 0.792
Multi-News | LongT5-base | 37.62 | 29.63 (-7.99) | 0.724
BIGPATENT | LongT5-base | 49.68 | 44.99 (-4.69) | 0.686

Sau khi token hiện tại được thoát sớm ở lớp thứ ℓ,
chúng ta cần tính toán các trạng thái key và value
cho tất cả các khối sâu hơn để thực hiện
self-attention cho các token tiếp theo đi qua
các khối sâu hơn. Để có phương pháp hiệu quả
hơn trong việc lưu trữ các trạng thái key và value,
các khung thoát sớm sử dụng cơ chế sao chép trạng thái.
Nó nhân đôi các trạng thái ẩn của lớp thoát sớm
(tức là, h^i_{t+1} = h^ℓ_{t+1}, ∀i ∈ [ℓ+1, L]),
cho phép chúng ta tính toán các trạng thái key
và value xấp xỉ cần thiết cho self-attention
của mạng Transformer. Schuster et al. (2022)
đã xác minh rằng sao chép trạng thái từ các lớp
thấp hơn không có tác động có hại đến hiệu suất
trong trường hợp các mô hình T5 kích thước nhỏ
(Raffel et al., 2020).

4 Đánh giá lại Khung Thoát Sớm
Trong phần này, chúng tôi trình bày bốn phát hiện
mới từ việc đánh giá lại khung thoát sớm.
Chúng tôi sử dụng các kích thước mô hình T5
khác nhau (Raffel et al., 2020) trên SAMSum
(Gliwa et al., 2019) và CNN/DailyMail (See et al., 2017),
và kiến trúc LongT5-base (Guo et al., 2022)
trên Multi-News (Fabbri et al., 2019a) và
BIGPATENT (Sharma et al., 2019).

4.1 Thiếu Tính Mạnh mẽ đối với Kích thước
Mô hình và Độ dài Chuỗi Đầu ra

Chúng tôi đầu tiên đánh giá lại cơ chế sao chép
trạng thái là thành phần thiết yếu của khung
thoát sớm. Theo Schuster et al. (2022), chúng tôi
sử dụng biện pháp tin cậy oracle cho phép các
token thoát ở lớp sớm nhất, sao cho dự đoán
của chúng giống hệt với dự đoán của lớp cuối.
Đáng chú ý, như quan sát trong Bảng 1, sự suy giảm
chất lượng tạo sinh với sao chép trạng thái trở nên
nghiêm trọng trên các mô hình lớn hơn và tập dữ liệu
với chuỗi dài hơn (▷Obs. 1). Ví dụ, khi xem xét
kết quả oracle-exiting, mô hình T5-small

--- TRANG 4 ---
chỉ thể hiện suy giảm 0.67 trên tập dữ liệu SAMSum,
trong khi mô hình T5-large trải qua sự giảm
lớn hơn nhiều là 1.24. Tương tự, trên các tập dữ liệu
như Multi-News và BIGPATENT, bao gồm các
chuỗi đầu ra tương đối dài, kết quả oracle-exiting
thể hiện sự giảm 7.99 và 4.69, tương ứng.

Để củng cố bằng chứng hỗ trợ, chúng tôi tiếp tục
khám phá sự biến động đáng kể trong phân phối
trạng thái ẩn qua các lớp khác nhau. Trong Bảng 1,
chúng tôi cũng báo cáo độ tương tự cosine giữa
trạng thái ẩn của lớp cuối và lớp oracle-exited.
Mặc dù trạng thái ẩn của lớp cuối và oracle-exited
tạo ra cùng dự đoán, độ tương tự cosine giữa chúng
giảm đáng kể khi mạng giải mã trở nên lớn hơn
và chuỗi đầu ra trở nên dài hơn.

4.2 Suy giảm Hiệu suất do Vị trí Thoát

Để tạo điều kiện thoát sớm cho tất cả các lớp
giải mã, các mục tiêu đào tạo cần là sự kết hợp
của các mục tiêu đào tạo cho từng lớp riêng lẻ.
Chúng ta có thể trình bày như sau:

L = ∑^L_{i=1} α_i L_i where ∑_i α_i = 1, (1)

L_i và α_i là hàm mất mát negative log-likelihood
và hệ số trọng số cho lớp thứ i, tương ứng. Đặc biệt,
công trình trước đây đặt α_i là 1/L (trung bình
không trọng số; Elbayad et al. 2020) hoặc i/∑_i i
(trung bình có trọng số; Schuster et al. 2022).
Họ chứng minh rằng những quy tắc trọng số này
hiệu quả tạo điều kiện học tập ở các lớp sớm hơn
mà không ảnh hưởng đến hiệu suất tổng thể
của mô hình đầy đủ trên các mô hình giải mã
kích thước nhỏ.

Tuy nhiên, như được thể hiện trong Hình 2, chúng tôi
quan sát thấy sự giảm đáng kể trong hiệu suất
của static-exiting, sử dụng cùng số lượng lớp
cho tất cả token, khi chỉ sử dụng một phần nhỏ
các lớp sớm từ mô hình T5-large. (▷Obs. 2).
Ví dụ, nếu tất cả token được thoát ở lớp một
hoặc hai, mô hình đạt được điểm ROUGE-L gần như
bằng không. Hơn nữa, khi chúng tôi áp dụng khung
thoát sớm cho các mô hình này trong suy luận,
chúng tôi xác minh rằng mô hình T5-large tạo ra
câu dài bất thường, thực sự tiêu tốn nhiều thời gian
suy luận hơn. Dựa trên những kết quả này, trong
các thí nghiệm tiếp theo, chúng tôi đã loại trừ
hai hoặc bốn lớp đầu tiên khỏi các ứng viên
cho các lớp thoát sớm của mô hình base và large,
tương ứng.

[Hình 2 và các biểu đồ: Minh họa điểm ROUGE-L và độ dài chuỗi được tạo từ phương pháp static-exiting trong T5-small (trái) và T5-large (phải) trên tập dữ liệu SAMSum. Đường ngang đứt nét đại diện cho độ dài chuỗi trung bình của ground truth.]

[Hình 3: Chi phí tính toán theo thành phần trên ba tập dữ liệu. Bốn thanh tương ứng với mô hình đầy đủ và thoát sớm với ngưỡng 0.9, 0.7, và 0.5. Màu gạch chéo biểu thị thời gian trôi qua sau khi token thoát, liên quan đến cơ chế sao chép trạng thái. Các số trên thanh đại diện cho điểm ROUGE-L. SA và CA biểu thị self- và cross-attention, tương ứng.]

4.3 Chi phí Tính toán Không thể Bỏ qua

Trong quá trình phân tích, chúng tôi quan sát thấy
khung thoát sớm thông thường không chỉ thể hiện
những bất lợi về hiệu suất mà còn đặt ra thách thức
cho độ trễ suy luận. Trong Hình 3, chúng tôi thực hiện
phân tích chi phí tính toán liên quan đến mô hình
giải mã qua ba tập dữ liệu tóm tắt. Đáng ngạc nhiên,
thoát sớm thường cho thấy sự tăng bất ngờ trong
tổng thời gian giải mã so với mô hình cơ sở
không sử dụng thoát sớm.

Điều này có thể được quy cho chi phí tính toán
không thể bỏ qua trong việc đo lường tin cậy
tại mỗi lớp, đặc biệt do các phép toán softmax
với kích thước từ vựng lớn. Ngoài ra, mặc dù
phương pháp sao chép trạng thái nhằm giảm
thời gian tính toán trong các lớp MHA và FFN
của các lớp còn lại, việc tính toán các trạng thái
key và value sử dụng trạng thái ẩn nhân đôi
phát sinh chi phí bổ sung không thể bỏ qua
(▷Obs. 3).

--- TRANG 5 ---
Bảng 2: Ngưỡng tin cậy tối ưu để đạt được hiệu suất
mong muốn. Chúng tôi chọn giá trị tốt nhất trong số
các giá trị ngưỡng từ 0 đến 1 với bước 0.1. Các số
tuần tự đại diện cho ngưỡng được chọn và hiệu suất
tương ứng (màu xám).

Performance Drop
Task    Dataset      ∼1%        ∼5%        ∼10%
SUM     SAMSum      1.0 (48.8) 0.7 (46.8) 0.5 (45.0)
        CNN/DM      1.0 (41.2) 0.5 (39.2) 0.3 (37.3)
        Multi-News  0.8 (37.3) 0.5 (35.9) 0.4 (34.9)
        BIGPATENT   1.0 (49.7) 0.8 (47.3) 0.6 (45.2)
QA      SQuAD       0.1 (90.1) 0.0 (88.3) 0.0 (88.3)
MT      IWSLT       1.0 (39.4) 1.0 (39.4) 1.0 (39.4)

4.4 Ngưỡng Tin cậy Tối ưu Khác biệt

Xác định ngưỡng phù hợp cho tin cậy thoát
là thách thức quan trọng trong khung thoát sớm
vì nó ảnh hưởng trực tiếp đến sự cân bằng
giữa hiệu suất và độ trễ (Zhang et al., 2019b;
Schuster et al., 2022). Như được tóm tắt trong
Bảng 2, các quan sát của chúng tôi chỉ ra rằng
các ngưỡng tin cậy tối ưu để đạt được độ trễ
thấp nhất trong cùng hiệu suất khác nhau đáng kể
qua các tập dữ liệu (▷Obs. 4). Ví dụ, tập dữ liệu
SQuAD và CNN/DailyMail có thể duy trì hiệu suất
với ngưỡng thoát tương đối thấp hơn, trong khi
giá trị ngưỡng cao hơn được yêu cầu trong trường hợp
tập dữ liệu IWSLT. Công trình trước đây (Schuster et al.,
2022) đã tận dụng các kỹ thuật kiểm soát rủi ro
distribution-free cho tạo sinh tin cậy. Tuy nhiên,
những phương pháp này yêu cầu thời gian đào tạo
bổ sung cho các kiểm tra thống kê trên tập
hiệu chuẩn bổ sung trước triển khai, nơi thời gian
cũng có thể bị ảnh hưởng bởi kích thước của
tập ứng viên ngưỡng.

5 Khung Thoát Sớm Mới: FREE

Dựa trên các khám phá trong Phần 4, chúng tôi
giới thiệu khung Thoát Sớm Nhanh và Mạnh mẽ
có tên FREE, tận dụng mô-đun nông-sâu và
tận dụng cấu trúc của giải mã song song. Hơn nữa,
chúng tôi trình bày thuật toán ước tính tin cậy
được thiết kế để nâng cao tính mạnh mẽ của
thoát sớm trong khung FREE.

5.1 Mô-đun Nông-Sâu

Chúng tôi trình bày mô-đun nông-sâu hiệu quả,
chiến lược phân bổ một số lượng lớp sớm được
định trước (L_S) làm mô hình nông, trong khi
tất cả các lớp làm mô hình sâu. Mô-đun này
giải quyết suy giảm hiệu suất liên quan đến

[Hình 4: Tổng quan về giải mã đồng bộ song song. Chúng tôi tô màu các token được sử dụng để tạo token tiếp theo dựa trên mô hình mà chúng chuyển tiếp.]

đồng đào tạo nhiều lớp thoát trong khung
thoát sớm thông thường.

Để nâng cao hiệu suất của mô hình nông, chúng tôi
khai thác knowledge distillation (KD) theo lớp
như một số hạng mất mát bổ sung cho Eq. (1)
với α_{L_s} = L_s/(L+L_s) và α_L = L/(L+L_s):

L_{KD} = 1/|L_S| ∑^{L_S}_{i=1} MSE(H^i_S, H^{m(i)}_D),

trong đó m(i) chỉ ra lớp trong mô hình sâu
trích xuất kiến thức vào lớp tương ứng i
của mô hình nông. H_S và H_D là trạng thái ẩn
từ mô hình nông và sâu.

Chúng tôi đã thí nghiệm với chưng cất từ lớp cuối
(KD-last; Wang et al. 2020; Ko et al. 2023),
từ các lớp ánh xạ đồng nhất cố định (KD-unif;
Jiao et al. 2020; Park et al. 2021), và từ
các lớp ánh xạ động (KD-dyna; Xia et al. 2022).
Đặc biệt, hàm ánh xạ động cho phép chúng ta
căn chỉnh mỗi lớp mô hình sâu với đối tác
gần nhất trong mô hình nông:

m(i) = arg min_j MSE(H^i_S, H^j_D)

trong đó j biểu thị chỉ số lớp của mô hình sâu
được chọn bởi tổng số L_S, và điều kiện
m(1) ≤ ··· ≤ m(L_S) phải được thỏa mãn.
Dựa trên hiệu suất ưu việt nhất quán của
mất mát KD-dyna (xem Phụ lục D.2), chúng tôi
sử dụng nó cho tất cả thí nghiệm với mô-đun nông-sâu.

5.2 Giải mã Đồng bộ Song song

Chúng tôi trình bày giải mã đồng bộ song song
như một thay thế cho cơ chế sao chép trạng thái,
là thành phần chính của khung thoát sớm thông thường
nhưng có thể dẫn đến suy giảm hiệu suất đáng kể,
như được chứng minh trong Phần 4.1. Trái ngược
với các phương pháp truyền thống có nhiều điểm thoát,
phương pháp của chúng tôi kết hợp mô-đun nông-sâu,
cho phép chúng ta xếp chồng các token thoát sớm
liên tiếp trong mô hình nông cho đến khi gặp
token không thoát. Khi giải mã token với mô hình sâu,
chúng tôi nâng cao hiệu quả và hiệu suất thông qua
giải mã song song, tính toán đồng bộ các trạng thái
key và value của các token đã xếp chồng trước đó.
Ví dụ về quá trình giải mã song song được mô tả
trong Hình 4.

Nguyên lý cơ bản của phương pháp này là tận dụng
khả năng song song tăng cường được cung cấp bởi
các bộ gia tốc phần cứng hiện đại. Điều này cho phép
các phép tính hiệu quả được thực hiện đồng thời
trên số lượng lớn chuỗi. Do đó, bằng cách sử dụng
giải mã đồng bộ song song, chúng ta có thể tính toán
trực tiếp nhiều trạng thái ẩn tương tự như thời gian
xử lý token đơn. Bên cạnh đó, điều này có thể loại bỏ
suy giảm hiệu suất tiềm năng có thể phát sinh từ
xấp xỉ không chính xác của trạng thái ẩn do
cơ chế sao chép trạng thái.

5.3 Ước tính Ngưỡng Thích ứng

Chúng tôi đề xuất phương pháp ước tính ngưỡng
thích ứng mới cập nhật ngưỡng để được giữ lại
cho các tập dữ liệu khác nhau. Không giống như
các phương pháp trước đây sử dụng tập hiệu chuẩn
bổ sung (Schuster et al., 2022), chúng tôi nhanh chóng
thích ứng ngưỡng bằng cách sử dụng thông tin
của các instance giai đoạn đầu, bất kể giá trị
ngưỡng ban đầu. Đặc biệt, trong quá trình giải mã
song song, chúng tôi thu thập mẫu để đánh giá
sự tương ứng giữa điểm tin cậy của mô hình nông
và sự phù hợp dự đoán giữa mô hình nông và sâu.

Như được mô tả trong Hình 1b, chúng tôi quan sát
rằng khi dự đoán của mô hình sâu và nông giống hệt,
tin cậy có xu hướng nghiêng về một, ngược lại
nó nghiêng về không. Để mô hình hóa phân phối
nghiêng này trên [0,1], chúng tôi sử dụng mô hình
hỗn hợp beta (BMM; Ma and Leijon 2011) do tính
linh hoạt và tập hỗ trợ phù hợp của phân phối beta.
Hàm mật độ xác suất của phân phối beta trên x ∈ [0,1]
được định nghĩa là:

p(x|α, β) = Γ(α+β)/(Γ(α)Γ(β)) x^{α-1}(1-x)^{β-1}

Các tham số của BMM được cập nhật bằng cách sử dụng
bộ ước tính maximum likelihood (MLE; Norden 1972)
với các điểm dữ liệu quan sát được.

α_k = ̄c_k/( ̄c_k(1- ̄c_k)/s^2_k - 1), β_k = α_k(1- ̄c_k)/ ̄c_k, (2)

trong đó ̄c_k là trung bình của tin cậy {c^{L_s}_i}^{|D_c|}_{i=1}
cho k tương ứng. k được đặt là 1 nếu dự đoán
của hai mô hình giống hệt, và 0

Thuật toán 1 Ước tính Ngưỡng Thích ứng
Input : tập dữ liệu hiệu chuẩn trống D_c, ngưỡng
tin cậy ban đầu λ^0_c, điều kiện posterior ζ, số
cập nhật T
Output : ngưỡng tin cậy được cập nhật λ_c
1: khởi tạo t ← 0, λ_c ← λ^0_c
2: while t ≤ T do
3:   Tạo câu thứ t với N_t token
4:   /* Cập nhật D_c*/
5:   D_c ← D_c ∪ {c^{L_s}_i, I( ̂y^{L_s}_i =  ̂y^L_i)}^{N_t}_{i=1}
6:   /* Tìm ngưỡng với Eq.(2)-(4)*/
7:   α_k, β_k ← MLE BMM(D_c) for k ∈ {0,1}
8:   λ_c ← arg min_{λ:p(k=1|λ)≥ζ} λ
9:   cập nhật t ← t + 1
10: end while

ngược lại. Tương tự, s_k là độ lệch chuẩn của
tin cậy của k liên quan.

̄c_k = (∑^N_{i=1} γ_i c^{L_s}_i)/(∑^N_{i=1} γ_i), ̄s^2_k = (∑^N_{i=1} γ_i(c^{L_s}_i - ̄c_k)^2)/(∑^N_{i=1} γ_i), (3)

trong đó γ_i := I( ̂y^{L_s}_i =  ̂y^L_i) biểu thị liệu
dự đoán của hai mô hình có giống nhau hay không.

Sau khi cập nhật BMM, chúng tôi tìm ngưỡng phù hợp
cho các token tương lai bằng cách xác định điểm
tại đó xác suất posterior, được định nghĩa dưới đây,
đạt ζ:

p(k=1|λ_c) = p(k=1)p(λ_c|α_1,β_1)/(∑_{j∈{0,1}} p(k=j)p(λ_c|α_j,β_j)). (4)

Ở đây, vì chúng tôi quan sát sự mất cân bằng nghiêm trọng
giữa trường hợp k=0 và 1, chúng tôi hạn chế giá trị
prior của mỗi lớp là 0.5 để cân bằng giữa hai
trường hợp (tức là, p(k=j) = 0.5 ∀j). Vì hạn chế
này khiến chúng ta sử dụng giá trị ζ nhỏ hơn,
chúng tôi naïvely đặt nó là 0.4. Thuật toán chi tiết
có thể được tìm thấy trong Thuật toán 1.

6 Thí nghiệm

6.1 Thiết lập Thí nghiệm

Chúng tôi thực hiện thí nghiệm trên các nhiệm vụ
mô hình hóa chuỗi khác nhau, bao gồm trả lời câu hỏi
(SQuAD; Rajpurkar et al. 2016b), dịch máy
(IWSLT 2017 En-De; Cettolo et al. 2017b),
và các nhiệm vụ tóm tắt văn bản sử dụng
SAMSum, CNN/DailyMail, Multi-News, và
BIGPATENT datasets. Mô hình LongT5-base
được sử dụng cho tập dữ liệu Multi-News và
BIGPATENT, trong khi mô hình T5-large được
sử dụng cho các tập dữ liệu khác. Tất cả triển khai
dựa trên PyTorch sử dụng Huggingface (Wolf et al.,
2020; Lhoest et al., 2021). Chi tiết thêm có thể
được tìm thấy trong Phụ lục B.

--- TRANG 7 ---
[Hình 5: Sự cân bằng giữa chất lượng đầu ra được tạo và độ trễ được chuẩn hóa dưới các điều kiện thoát khác nhau. Chúng tôi thay đổi giá trị ngưỡng thoát giữa 0 và 1 cho cả CALM và FREE† và số lượng lớp thoát cho khung static-exiting. Chúng tôi loại trừ điểm bên trong của đường cong Pareto, và đường đứt nét đại diện cho điểm ROUGE-L của mô hình đầy đủ, là mô-đun nông-sâu được tinh chỉnh.]

Bảng 3: So sánh giữa các khung thoát sớm trên các tập dữ liệu khác nhau. Đối với CALM và FREE†, chúng tôi báo cáo hiệu suất sử dụng giá trị ngưỡng nhỏ nhất đạt được 99% hiệu suất của mô hình đầy đủ, được tinh chỉnh bởi mất mát trung bình có trọng số hoặc KD-dyna, tương ứng. Dấu ngoặc đơn biểu thị tăng tốc tương đối dựa trên hàng đầu tiên.

SUM                                    QA         MT
Method      SAMSum         CNN/DailyMail    Multi-News      BIGPATENT      SQuAD        IWSLT De-En
Full Model  48.82 (×1.00)  41.15 (×1.00)    37.62 (×1.00)   49.68 (×1.00)  90.63 (×1.00) 39.19 (×1.00)
CALM        48.37 (×0.72)  40.78 (×0.86)    37.27 (×0.85)   49.21 (×0.65)  90.09 (×2.03) 39.19 (×1.00)
Full Model  49.11 (×1.00)  41.09 (×1.00)    39.20 (×1.00)   49.68 (×1.00)  91.90 (×1.00) 39.39 (×1.00)
FREE†       48.65 (×1.50)  40.89 (×1.80)    38.93 (×1.07)   49.51 (×1.62)  91.31 (×2.76) 39.04 (×1.07)
FREE        48.66 (×1.47)  40.99 (×1.65)    38.66 (×1.23)   49.47 (×1.58)  91.82 (×2.16) 38.17 (×1.18)

6.2 Kết quả Thí nghiệm

Để điều tra tác động của từng thành phần riêng lẻ
của khung đề xuất, chúng tôi đánh giá cả FREE
không có và có bộ ước tính ngưỡng thích ứng,
được ký hiệu là FREE† và FREE.

Hiệu suất tổng thể. Trong Hình 5, chúng tôi trình bày
so sánh chất lượng đầu ra được tạo (ROUGE-L)
và độ trễ suy luận giữa khung FREE và các baseline,
bao gồm static-exiting và phương pháp thoát sớm
thông thường (CALM; Schuster et al. 2022). Phương pháp
CALM thể hiện hiệu suất kém hơn so với phương pháp
static-exiting đơn giản trên tất cả tập dữ liệu,
có thể do cơ chế sao chép trạng thái và sự hiện diện
của nhiều vị trí thoát, như quan sát trong Phần 4.
Ngược lại, FREE† thể hiện hiệu suất mạnh mẽ
và AUC (diện tích dưới đường cong) lớn hơn
qua các tập dữ liệu bằng cách điều chỉnh ngưỡng thoát.

Đánh giá ngưỡng thích ứng. Trong khung thoát sớm,
việc chọn ngưỡng tin cậy phù hợp là quan trọng
để đạt được sự cân bằng tốt nhất giữa chất lượng
tạo sinh và độ trễ. Không giống như các phương pháp
hiệu chuẩn trước đây (Schuster et al., 2022) yêu cầu
tập hiệu chuẩn bổ sung

[Hình 6: Sự cân bằng giữa chất lượng đầu ra được tạo và độ trễ được chuẩn hóa trên mô hình T5-3B.]

và thời gian đào tạo, phương pháp của chúng tôi
hiệu quả giải quyết thách thức này bằng cách tận dụng
sản phẩm phụ của giải mã song song. Như được tóm tắt
trong Bảng 3, FREE với ước tính ngưỡng thích ứng
thành công đạt được tăng tốc đáng kể, lên đến ×2.16,
khi bảo tồn 99% hiệu suất mô hình đầy đủ. Hơn nữa,
trong Hình 5, ngưỡng được ước tính thể hiện gần như
cải thiện tốc độ tối đa có thể đạt được mà không
hy sinh hiệu suất, được đại diện bởi các ngôi sao đỏ.

Mô hình ngôn ngữ lớn. Gần đây, các nghiên cứu khác nhau
(Dettmers et al., 2022; Xiao et al., 2023;
Leviathan et al., 2023; Liu et al., 2023b) đã

--- TRANG 8 ---
Bảng 4: So sánh ROUGE-L và tăng tốc dựa trên số lượng lớp khác nhau cho mô hình nông và ngưỡng tin cậy.

                   Threshold
Dataset  LS  0.7                0.5                0.3
SAMSum   4   48.27 (×1.04)     46.95 (×1.09)     44.72 (×1.15)
         6   48.89 (×1.32)     48.65 (×1.50)     47.60 (×1.80)
         8   48.74 (×1.11)     47.97 (×1.17)     47.09 (×1.31)
         12  48.97 (×1.21)     48.74 (×1.28)     48.10 (×1.37)
CNN/DM   4   41.03 (×1.45)     40.59 (×1.68)     39.88 (×1.86)
         6   41.08 (×1.53)     41.00 (×1.69)     40.60 (×2.07)
         8   41.19 (×1.44)     41.15 (×1.64)     40.95 (×1.69)
         12  41.11 (×1.33)     41.09 (×1.47)     40.95 (×1.55)

nhằm tăng tốc độ suy luận của các mô hình ngôn ngữ lớn (LLM). Để xác thực khả năng áp dụng của khung FREE trên LLM, chúng tôi thực hiện thí nghiệm sử dụng mô hình T5-3B (Raffel et al., 2020) trên tập dữ liệu SAMSum và CNN/DailyMail. Do chi phí tính toán đáng kể, chúng tôi sử dụng bộ chuyển đổi LoRA (Hu et al., 2022), nhắm mục tiêu cả lớp self-attention và feed-forward với rank 64. Hình 6 tóm tắt so sánh toàn diện các phương pháp thoát sớm. Phương pháp của chúng tôi duy trì sự ưu việt so với các baseline về độ trễ và điểm ROUGE-L, cho thấy xu hướng hiệu suất nhất quán được quan sát trong mô hình T5-large. Do đó, chúng tôi tin tưởng rằng khung đề xuất sẽ thể hiện mức độ gia tốc suy luận nhất quán, ngay cả với các mô hình ngôn ngữ lớn hơn.

6.3 Nghiên cứu Ablation

Độ sâu khác nhau của mô hình nông. Trong Bảng 4, chúng tôi cũng ablate về số lượng lớp cho mô hình nông để quan sát các sự cân bằng. Trong khi phương pháp của chúng tôi thể hiện xu hướng tăng tốc cao hơn khi độ sâu của mô hình nông giảm, chúng tôi trải qua một số giảm hiệu suất và tăng tốc khi độ sâu của mô hình giảm quá nhiều (ví dụ, bốn lớp). Chúng tôi giả định rằng điều này do các câu đầu ra không chính xác và dư thừa, tương tự được quan sát trong khung thoát sớm thông thường. Do đó, với độ sâu đủ (ví dụ, sáu lớp), FREE nhất quán cho thấy hiệu suất mạnh mẽ và tăng tốc suy luận.

Tính mạnh mẽ của giải mã song song. Để xác minh tính mạnh mẽ của cơ chế giải mã của chúng tôi, chúng tôi thực hiện phân tích so sánh giữa giải mã đồng bộ song song (SPD) và sao chép trạng thái (SC), cả hai đều được triển khai với mô-đun nông-sâu

Bảng 5: So sánh giữa giải mã đồng bộ song song (SPC) và sao chép trạng thái (SC). Mô-đun nông-sâu được sử dụng trong cả hai phương pháp giải mã.

Method    Threshold
Dataset  SC  SPD  0.9    0.7    0.5    0.3    0.1
SAMSum   ✓   ✗    46.35  44.59  43.92  42.36  41.27
         ✗   ✓    48.89  48.89  48.65  47.60  45.27
CNN/DM   ✓   ✗    40.92  40.92  40.71  39.99  38.17
         ✗   ✓    41.12  41.08  41.00  40.60  39.30
Multi-News ✓   ✗    38.43  37.61  36.55  33.99  29.34
         ✗   ✓    39.16  39.06  38.78  37.87  33.98

Bảng 6: Kết quả thí nghiệm của khung FREE dựa trên kích thước khác nhau của tập hiệu chuẩn.

         3%              10%             100%
Dataset  Thr. Perf. Speed Thr. Perf. Speed Thr.
SAMSum   0.51 48.66 ×1.47 0.49 48.69 ×1.51 0.48
BIGPATENT 0.54 49.47 ×1.58 0.54 49.39 ×1.63 0.54

mô-đun. Giải mã đồng bộ song song nhất quán vượt trội hơn sao chép trạng thái qua tất cả ba tập dữ liệu với các chỉ số ROUGE-L cao hơn nhiều, như được tóm tắt trong Bảng 5. Cải thiện này có thể được quy cho các trạng thái ẩn được cập nhật thu được thông qua tính toán chính xác của các lớp Transformer trong giải mã song song. Những phát hiện này cho thấy rằng phương pháp giải mã hiệu quả của chúng tôi cho các token thoát sớm có thể nâng cao hiệu suất tổng thể của khung thoát sớm.

Phụ thuộc vào kích thước tập hiệu chuẩn. Bằng cách sử dụng các instance giai đoạn đầu làm tập hiệu chuẩn, chúng tôi cập nhật lặp lại ngưỡng tin cậy thích ứng để hội tụ về giá trị phù hợp. Ở đây, chúng tôi đã quan sát hiệu quả mẫu của bộ ước tính ngưỡng thích ứng bằng cách thay đổi kích thước của tập hiệu chuẩn này. Thú vị là, ngay cả chỉ với 3% tổng số mẫu, bộ ước tính của chúng tôi có thể xấp xỉ ngưỡng, được đo bởi tập mẫu đầy đủ, như được thể hiện trong Bảng 6. Điều này đảm bảo thời gian tính toán bổ sung tối thiểu cần thiết cho ước tính ngưỡng.

Tinh chỉnh dự đoán mô hình nông. Các công trình trước đây (Leviathan et al., 2023; Chen et al., 2023; Kim et al., 2023) đã đề xuất các phương pháp tinh chỉnh để sửa chữa đầu ra sai từ mô hình xấp xỉ. Cụ thể, khi token sai được phát hiện trong các chuỗi trước đó, họ loại bỏ tất cả các token được tạo sau đó và khởi động lại quá trình tạo sinh từ điểm đó. Trong Bảng 7, chúng tôi thực hiện thí nghiệm để đánh giá tác động của phương pháp tinh chỉnh này (Kim et al., 2023)

--- TRANG 9 ---
Bảng 7: Đánh giá các phương pháp tinh chỉnh trong khung FREE. Ngưỡng tinh chỉnh kiểm soát mức độ chấp nhận cho dự đoán từ mô hình nông.

         Thr. 0.7           Thr. 0.3
Dataset  Ref. Thr. Perf. Speed Perf. Speed
SAMSum   ✗    -    48.89 ×1.33 47.60 ×1.80
         ✓    1.0  49.08 ×1.26 48.31 ×1.50
         ✓    0.1  49.06 ×1.17 48.27 ×1.12
CNN/DM   ✗    -    41.08 ×1.53 40.60 ×2.07
         ✓    1.0  40.86 ×1.51 40.78 ×1.67
         ✓    0.1  40.85 ×1.35 40.75 ×1.21

trong khung thoát sớm của chúng tôi. Chúng tôi quan sát thấy rằng khi ngưỡng tinh chỉnh được đặt thấp, cho phép nhiều sửa chữa hơn bởi mô hình sâu, cải thiện hiệu suất là tối thiểu so với sự tăng độ trễ. Các phát hiện của chúng tôi cho thấy rằng những phương pháp này không thể đảm bảo giới hạn trên cho sự tăng độ trễ có thể không phù hợp để tích hợp vào khung thoát sớm.

Đánh giá tóm tắt giống con người. Các nghiên cứu gần đây (Gao et al., 2023; Liu et al., 2023a; Zhang et al., 2023) đã lập luận rằng các chỉ số đánh giá tóm tắt hiện tại như ROUGE-L không đại diện chính xác cho khả năng tóm tắt thực sự. Thay vào đó, họ khám phá đánh giá giống con người sử dụng LLM dựa trên mối tương quan mạnh mẽ với phán đoán của con người. Do đó, chúng tôi thực hiện hai phương pháp đánh giá giống con người, chấm điểm thang Likert và so sánh theo cặp (Gao et al., 2023), sử dụng ChatGPT API (gpt-3.5-turbo-0613). Chúng tôi so sánh mô hình đầy đủ và khung FREE của chúng tôi trên 100 instance, được rút ngẫu nhiên từ tập dữ liệu CNN/DailyMail. Hình 7 và 8 cung cấp các mẫu được sử dụng cho từng nhiệm vụ đánh giá. Đối với mô hình đầy đủ, chúng tôi quan sát điểm [4.73, 3.83, 3.87, 3.77], trong khi phương pháp FREE của chúng tôi trả về điểm [4.68, 3.84, 3.84, 3.72] qua bốn chiều. Bên cạnh đó, số lần thắng cho mỗi phương pháp là 101 và 99, tương ứng. Với điểm ROUGE-L 41.09 (×1.00) cho mô hình đầy đủ và 40.99 (×1.65) cho phương pháp FREE, phương pháp của chúng tôi chắc chắn có khả năng tạo ra dự đoán chất lượng tương tự, đồng thời giảm đáng kể chi phí tính toán.

7 Kết luận

Chúng tôi đề xuất khung FREE để giải quyết các thách thức của khung thoát sớm thông thường cho các mô hình ngôn ngữ tự hồi quy. Phương pháp của chúng tôi kết hợp ba thành phần chính: (1) mô-đun nông-sâu, (2) giải mã đồng bộ song song,

Đánh giá chất lượng tóm tắt được viết cho một bài báo tin tức. Chấm điểm mỗi tóm tắt trên bốn chiều: {Dimension_1}, {Dimension_2}, {Dimension_3}, và {Dimension_4}. Bạn nên chấm điểm trên thang từ 1 (tệ nhất) đến 5 (tốt nhất).

Bài báo: {Article}
Tóm tắt: {Summary}

Hình 7: Mẫu cho chấm điểm thang Likert. Bốn chiều là liên quan, thông tin, trôi chảy, và mạch lạc.

Cho một bài báo tin tức, tóm tắt nào tốt hơn? Trả lời "Summary 0" hoặc "Summary 1". Bạn không cần giải thích lý do.

Bài báo: {Article}
Tóm tắt 0: {Summary_0}
Tóm tắt 1: {Summary_1}

Hình 8: Mẫu cho so sánh theo cặp. Chúng tôi đo hai lần bằng cách thay đổi thứ tự tóm tắt để so sánh công bằng.

và (3) ước tính ngưỡng thích ứng. Thông qua các thí nghiệm mở rộng trên các nhiệm vụ tạo sinh khác nhau, chúng tôi chứng minh thực nghiệm hiệu suất ưu việt của khung FREE, đạt được gia tốc đáng kể về độ trễ mà không ảnh hưởng đến chất lượng đầu ra được tạo.

Hạn chế. Công trình của chúng tôi giải quyết khung hiện tại nhanh và mạnh mẽ có thể được sử dụng hiệu quả mà không lo ngại về suy giảm hiệu suất. Tuy nhiên, phương pháp của chúng tôi có một số hạn chế mà chúng tôi thảo luận dưới đây: (1) Phương pháp của chúng tôi yêu cầu tài nguyên tính toán bổ sung để tinh chỉnh mô hình nông. Tuy nhiên, như chúng tôi đã chứng minh, các phương pháp tinh chỉnh hiệu quả tham số sẽ là giải pháp đầy hứa hẹn để khắc phục hạn chế này. (2) Trong khi công trình của chúng tôi thể hiện tính mạnh mẽ về độ sâu của mô hình nông, cần điều tra thêm để xác định độ sâu phù hợp cho các mô hình ngôn ngữ khác nhau. Khía cạnh này vẫn là lĩnh vực cần nghiên cứu bổ sung.

Lời cảm ơn. Công trình này được hỗ trợ bởi viện Công nghệ Thông tin & Truyền thông Kế hoạch & Đánh giá (IITP) tài trợ bởi chính phủ Hàn Quốc (MSIT) [Số 2021-0-00907, Phát triển Công nghệ Phân tích Cộng tác Edge Thích ứng và Nhẹ để Kích hoạt Phản ứng Ngay lập tức Chủ động và Học tập Nhanh, 90% và Số 2019-0-00075, Chương trình Trường Đại học Trí tuệ Nhân tạo (KAIST), 10%].

--- TRANG 10 ---
Tài liệu Tham khảo

Sangmin Bae, Sungnyun Kim, Jongwoo Ko, Gihun Lee, Seungjong Noh, và Se-Young Yun. 2021. Self-contrastive learning: Single-viewed supervised contrastive framework using sub-network. arXiv preprint arXiv:2106.15499.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.

Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Niehues Jan, Stüker Sebastian, Sudoh Katsuitho, Yoshino Koichiro, và Federmann Christian. 2017a. Overview of the iwslt 2017 evaluation campaign. In Proceedings of the 14th International Workshop on Spoken Language Translation, pages 2–14.

Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, và Christian Federmann. 2017b. Overview of the IWSLT 2017 evaluation campaign. In Proceedings of the 14th International Conference on Spoken Language Translation, pages 2–14, Tokyo, Japan. International Workshop on Spoken Language Translation.

Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, và John Jumper. 2023. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318.

Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. 2022. Llm. int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339.

Maha Elbayad, Jiatao Gu, Edouard Grave, và Michael Auli. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.

Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, và Dragomir Radev. 2019a. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074–1084, Florence, Italy. Association for Computational Linguistics.

Alexander R Fabbri, Irene Li, Tianwei She, Suyi Li, và Dragomir R Radev. 2019b. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. arXiv preprint arXiv:1906.01749.

Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, và Xiaojun Wan. 2023. Human-like summarization evaluation with chatgpt. arXiv preprint arXiv:2304.02554.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, và Luke Zettlemoyer. 2019. Mask-predict: Parallel decoding of conditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 6111–6120. Association for Computational Linguistics.

Bogdan Gliwa, Iwona Mochol, Maciej Biesek, và Aleksander Wawer. 2019. SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70–79, Hong Kong, China. Association for Computational Linguistics.

Alex Graves. 2016. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983.

Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, và Richard Socher. 2018. Non-autoregressive neural machine translation. In International Conference on Learning Representations.

Jiatao Gu và Xiang Kong. 2021. Fully non-autoregressive neural machine translation: Tricks of the trade. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 120–133, Online. Association for Computational Linguistics.

Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, và Yinfei Yang. 2022. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724–736, Seattle, United States. Association for Computational Linguistics.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, và Laurent Sifre. 2022. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, và Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.

--- TRANG 11 ---
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, và Qun Liu. 2020. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163–4174, Online. Association for Computational Linguistics.

Sehoon Kim, Karttikeya Mangalam, Jitendra Malik, Michael W Mahoney, Amir Gholami, và Kurt Keutzer. 2023. Big little transformer decoder. arXiv preprint arXiv:2302.07863.

Jongwoo Ko, Seungjoon Park, Minchan Jeong, Sukjin Hong, Euijai Ahn, Du-Seong Chang, và Se-Young Yun. 2023. Revisiting intermediate layer distillation for compressing language models: An overfitting perspective. In Findings of the Association for Computational Linguistics: EACL 2023, pages 158–175, Dubrovnik, Croatia. Association for Computational Linguistics.

Yaniv Leviathan, Matan Kalman, và Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 19274–19286. PMLR.

Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander M. Rush, và Thomas Wolf. 2021. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2021, Online and Punta Cana, Dominican Republic, 7-11 November, 2021, pages 175–184. Association for Computational Linguistics.

Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.

Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, và Chenguang Zhu. 2023a. Gpteval: nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634.

Yijin Liu, Fandong Meng, Jie Zhou, Yufeng Chen, và Jinan Xu. 2021. Faster depth-adaptive transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13424–13432.

Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Ré, và Beidi Chen. 2023b. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 22137–22176. PMLR.

Zhanyu Ma và Arne Leijon. 2011. Bayesian estimation of beta mixture models with variational inference. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(11):2160–2173.

Ramesh Nallapati, Bowen Zhou, Cícero Nogueira dos Santos, Çaglar Gülçehre, và Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, pages 280–290. ACL.

RH Norden. 1972. A survey of maximum likelihood estimation. International Statistical Review/Revue Internationale de Statistique, pages 329–354.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Geondo Park, Gyeongman Kim, và Eunho Yang. 2021. Distilling linguistic context for language model compression. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 364–378, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. 2016a. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.

--- TRANG 12 ---
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. 2016b. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas. Association for Computational Linguistics.

Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, và Emanuele Rodolà. 2023. Accelerating transformer inference for translation via parallel decoding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 12336–12355. Association for Computational Linguistics.

Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, và Aaron van den Oord. 2022. Step-unrolled denoising autoencoders for text generation. In International Conference on Learning Representations.

Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, và Donald Metzler. 2022. Confident adaptive language modeling. Advances in Neural Information Processing Systems, 35:17456–17472.

Abigail See, Peter J. Liu, và Christopher D. Manning. 2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073–1083, Vancouver, Canada. Association for Computational Linguistics.

Eva Sharma, Chen Li, và Lu Wang. 2019. BIGPATENT: A large-scale dataset for abstractive and coherent summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204–2213, Florence, Italy. Association for Computational Linguistics.

Noam Shazeer và Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596–4604. PMLR.

Surat Teerapittayanon, Bradley McDanel, và Hsiang-Tsung Kung. 2016. Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd International Conference on Pattern Recognition (ICPR), pages 2464–2469. IEEE.

Yonglong Tian, Dilip Krishnan, và Phillip Isola. 2019. Contrastive representation distillation. arXiv preprint arXiv:1910.10699.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.

Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, và Ming Zhou. 2020. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776–5788.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, và Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online. Association for Computational Linguistics.

Mengzhou Xia, Zexuan Zhong, và Danqi Chen. 2022. Structured pruning learns compact and accurate models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1513–1528, Dublin, Ireland. Association for Computational Linguistics.

Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, và Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 38087–38099. PMLR.

Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, và Kaisheng Ma. 2019a. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3713–3722.

Linfeng Zhang, Zhanhong Tan, Jiebo Song, Jingwei Chen, Chenglong Bao, và Kaisheng Ma. 2019b. Scan: A scalable neural networks framework towards compact and efficient models. Advances in Neural Information Processing Systems, 32.

Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, và Yongbin Li. 2023. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862.

--- TRANG 13 ---
A Mô tả Tập dữ liệu

Chúng tôi áp dụng FREE trên các nhiệm vụ tạo sinh khác nhau bao gồm tóm tắt, trả lời câu hỏi, và dịch máy. Chúng tôi cung cấp mô tả chi tiết về các tập dữ liệu được sử dụng.

• SAMSum (Tóm tắt): SAMSum (Gliwa et al., 2019) bao gồm 16K cuộc trò chuyện giống messenger được chú thích với tóm tắt để cung cấp tổng quan ngắn gọn về nội dung cuộc trò chuyện ở ngôi thứ ba.

• CNN/DailyMail (Tóm tắt): CNN/DailyMail (See et al., 2017) bao gồm hơn 300K bài báo tin tức tiếng Anh ban đầu được thiết kế cho đọc hiểu máy và trả lời câu hỏi trừu tượng, nhưng hiện cũng hỗ trợ tóm tắt trích xuất và trừu tượng.

• Multi-News (Tóm tắt): Multi-News (Fabbri et al., 2019a) bao gồm 45K bài báo tin tức và tóm tắt tương ứng, nơi mỗi tóm tắt được tạo chuyên nghiệp và cung cấp liên kết đến các bài báo gốc được tham chiếu.

• BIGPATENT (Tóm tắt): BIGPATENT (Sharma et al., 2019) chứa 1.3M hồ sơ tài liệu bằng sáng chế Hoa Kỳ, mỗi hồ sơ đi kèm với tóm tắt trừu tượng được viết bởi con người. Trong công trình của chúng tôi, chúng tôi đặc biệt tập trung vào danh mục Fixed Constructions, là một trong chín danh mục phân loại có sẵn trong tập dữ liệu.

• SQuAD (Trả lời Câu hỏi): Stanford Question Answering (SQuAD, Rajpurkar et al. 2016b) là tập hợp 87.6K nhiệm vụ đọc hiểu. Nó bao gồm các câu hỏi được tạo bởi nhân viên đám đông dựa trên tập hợp các bài báo Wikipedia.

• IWSLT 2017 (Dịch Máy): IWSLT 2017 (Cettolo et al., 2017b) giải quyết dịch văn bản, sử dụng hệ thống dịch máy (MT) đơn cho nhiều hướng ngôn ngữ như tiếng Anh và tiếng Đức. Ở đây, chúng tôi đặc biệt tập trung vào nhiệm vụ dịch Đức-sang-Anh.

B Thiết lập Thí nghiệm Chi tiết

Siêu tham số đào tạo. Trong phần này, chúng tôi mô tả các giá trị siêu tham số chi tiết cho công trình của chúng tôi. Chúng tôi sử dụng GPU NVIDIA RTX 3090 để đào tạo các mô hình ngôn ngữ, và chúng tôi tóm tắt

Bảng 8: Siêu tham số được tối ưu hóa để đào tạo mô hình T5 nông-sâu. Cột có nhãn '# Batch' chỉ ra tích của kích thước batch mỗi GPU và số lượng GPU. 'In len.' và 'Out len.' đại diện cho độ dài tối đa của đầu vào và đầu ra, tương ứng.

Dataset     Model         # Batch  Epochs  In len.  Out len.
SAMSum      T5-large      4×2      20      512      128
CNN/DM      T5-large      4×4      3       512      128
Multi-News  LongT5-base   2×2      3       2048     512
BIGPATENT   LongT5-base   2×2      3       2048     512
SQuAD       T5-large      4×2      10      512      30
IWSLT 2017  mT5-large     4×4      2       1024     128

cấu hình đào tạo trong Bảng 8. Đối với tất cả tập dữ liệu, chúng tôi sử dụng bộ tối ưu AdaFactor (Shazeer và Stern, 2018) với tốc độ học 1e-4. Đối với ước tính ngưỡng thích ứng, chúng tôi đặt giá trị ngưỡng ban đầu λ₀c là 0.9, ζ là 0.4, T là 3% tổng số mẫu (tham khảo Thuật toán 1).

Chỉ số hiệu suất. Để đo lường số bằng chất lượng đầu ra của phương pháp chúng tôi, chúng tôi sử dụng điểm F1 cho SQuAD, điểm BLEU (Papineni et al., 2002) cho IWSLT2017, và điểm ROUGE (Lin, 2004) cho bốn nhiệm vụ tóm tắt.

C Đánh giá Độ trễ Suy luận

Để đo tốc độ suy luận, chúng tôi thực hiện 500 dự đoán suy luận cho mỗi tập dữ liệu dưới mỗi cấu hình được kiểm tra trong hàm được biên dịch PyTorch (Paszke et al., 2019) trong một máy chủ duy nhất với GPU NVIDIA GeForce RTX 3039 đơn và CPU 12th Gen Intel(R) Core(TM) i7-12700K. Đối với mỗi dự đoán suy luận, chúng tôi sử dụng kích thước batch 1, là trường hợp sử dụng phổ biến cho phục vụ trực tuyến (Schuster et al., 2022). Ngoài ra, chúng tôi sử dụng để tạo chuỗi đầu ra thông qua lấy mẫu tham lam với kích thước beam là 1. Chúng tôi đo thời gian bao gồm tất cả các bước giải mã cho đến hoàn thành.

--- TRANG 14 ---
[Hình 9: Sự cân bằng giữa chất lượng đầu ra được tạo và độ trễ được chuẩn hóa dưới các điều kiện thoát khác nhau. Đường đứt nét đại diện cho điểm F1 và BLEU của mô hình đầy đủ, là mô-đun nông-sâu được tinh chỉnh, tương ứng. Tương tự như Hình 5, chúng tôi loại trừ điểm bên trong của đường cong Pareto.]

[Hình 10: Sự cân bằng giữa hiệu suất và độ trễ được chuẩn hóa mỗi câu. Chúng tôi thay đổi ngưỡng thoát trong khoảng {0.0, 0.1, 0.3, 0.5, 0.7, 0.9}. Các giá trị độ trễ được chuẩn hóa bởi độ trễ của baseline, là mô hình đầy đủ được tinh chỉnh đơn giản.]

D Kết quả Thí nghiệm Bổ sung

Trong phần này, chúng tôi cung cấp kết quả thí nghiệm bổ sung để chứng minh hiệu quả của phương pháp đề xuất và các thành phần riêng lẻ của nó.

D.1 Hiệu suất trên Các Tập dữ liệu Khác nhau

Trong phần này, chúng tôi trình bày so sánh chất lượng đầu ra được tạo (F1 hoặc BLEU) và độ trễ suy luận trên tập dữ liệu SQuAD và IWSLT 2017, tương tự như các thí nghiệm trong Hình 5. Hình 9 minh họa rằng cả FREE† và FREE đều nhất quán vượt trội hơn các baseline CALM và static-exiting trong tập dữ liệu SQuAD, phù hợp với các phát hiện trước đây của chúng tôi.

Tuy nhiên, lợi thế hiệu suất của chúng trong tập dữ liệu IWSLT bị giảm nhẹ so với các tập dữ liệu khác. Điều này có thể được quy cho kích thước từ vựng lớn hơn của mT5 so với T5, dẫn đến thời gian xử lý dài hơn cho đo lường tin cậy. Phương pháp CALM, cũng sử dụng các bộ phân loại tuyến tính lớn, thể hiện hiệu quả thấp hơn nhiều trong tập dữ liệu này. Chúng tôi tin rằng thách thức này, liên quan đến kích thước từ vựng lớn, có thể được giảm thiểu bằng cách sử dụng biện pháp tin cậy độc lập với kích thước từ vựng được đề xuất trong công trình trước đây (Schuster et al., 2022). Tuy nhiên, thuật toán đề xuất của chúng tôi vẫn vượt trội hơn các baseline khác trên các tập dữ liệu khác nhau.

Bảng 9: So sánh giữa FREE với T5-large và T5-base kích thước nhỏ được đào tạo trực tiếp. Chúng tôi áp dụng giá trị ngưỡng của FREE† là 0.1 cho SQuAD và 0.2 cho CNN/DailyMail.

                SQuAD                  CNN/DailyMail
Method    Model    F1    Speedup    ROUGE-L  Speedup
Full Model T5-large 91.82  ×1.00     41.09    ×1.00
Full Model T5-base  90.50  ×1.86     40.22    ×2.06
FREE†     T5-large 90.95  ×2.76     40.17    ×2.07

D.2 Knowledge Distillation Theo lớp

Với chỉ hai vị trí thoát trong mô-đun nông-sâu của chúng tôi, vì hiệu suất của chúng ảnh hưởng đáng kể đến tính mạnh mẽ tổng thể của phương pháp thoát sớm, chúng tôi thiết kế cẩn thận hàm mất mát để đào tạo. Trong Hình 10, chúng tôi quan sát xu hướng hiệu suất của bốn hàm mất mát khác nhau khi chúng tôi thay đổi ngưỡng thoát. Trong khi sự khác biệt không đáng kể, mất mát KD-dyna thể hiện sự cân bằng tốt hơn so với trung bình có trọng số hoặc các mất mát dựa trên KD khác. Cụ thể, hiệu suất thấp hơn của KD-unif trên tập dữ liệu SAMSum cho thấy rằng việc xác định ánh xạ lớp động có thể tạo điều kiện chuyển giao kiến thức hiệu quả hơn giữa mô hình sâu và nông. Do đó, chúng tôi đào tạo mô-đun nông-sâu sử dụng mất mát KD-dyna cho tất cả thí nghiệm, và để lại việc khám phá các hàm mất mát bổ sung, như mất mát chưng cất tương phản (Tian et al., 2019; Bae et al., 2021), cho công trình tương lai.

D.3 So sánh với Mô hình Kích thước Nhỏ

Chúng tôi thực hiện so sánh giữa tốc độ suy luận của FREE sử dụng mô hình T5-large và mô hình T5-base được đào tạo trực tiếp. Để đảm bảo so sánh công bằng, chúng tôi chọn thủ công ngưỡng tin cậy phù hợp cho FREE† (không dựa vào bộ ước tính ngưỡng thích ứng) để căn chỉnh hiệu suất của nó gần với T5-base. Kết quả, được trình bày trong Bảng 9, chứng minh rằng phương pháp đề xuất thể hiện tăng tốc cạnh tranh trong hiệu suất suy luận trên tập dữ liệu CNN/DailyMail. Hơn nữa, nó thể hiện điểm F1 ưu việt và tăng tốc cao hơn đáng kể trên tập dữ liệu SQuAD.

--- TRANG 15 ---
Bảng 10: So sánh giữa các khung thoát sớm trên SAMSum với các chiến lược giải mã khác nhau.

                top-k (k=50)          nucleus (p=0.92)
Method         ROUGE-L  Speedup     ROUGE-L  Speedup
Full Model     44.34    ×1.00       45.84    ×1.00
CALM           42.35    ×0.78       44.48    ×0.82
FREE           43.58    ×1.30       45.78    ×1.31

Chúng tôi tin rằng sự biến động trong tăng tốc qua các tập dữ liệu có thể được quy cho hiệu suất có thể đạt được bởi mô hình nhỏ hơn được đào tạo trực tiếp, cũng như mô hình nông trong khung FREE. Trong trường hợp SQuAD, mô hình T5-base (12 lớp) đạt được điểm ROUGE-L 90.50, trong khi mô hình nông (6 lớp) của khung FREE chúng tôi cho điểm tương tự 90.24. Phương pháp của chúng tôi hiệu quả tận dụng những lợi ích vốn có này, do đó tạo điều kiện tăng tốc suy luận thông qua thoát ở các lớp thấp hơn.

D.4 Các Chiến lược Giải mã Khác nhau

Để đánh giá khả năng áp dụng của FREE trên các phương pháp giải mã khác nhau, chúng tôi thực hiện thí nghiệm với lấy mẫu top-k (Radford et al., 2019) và lấy mẫu nucleus (lấy mẫu top-p; Holtzman et al. 2020). Lấy mẫu top-k lấy mẫu từ tiếp theo từ k lựa chọn có xác suất cao nhất, thay vì nhằm giải mã văn bản tối đa hóa likelihood. Mặt khác, lấy mẫu nucleus chọn từ tập hợp nhỏ nhất có thể của các từ có xác suất tích lũy vượt quá xác suất p. Như được chi tiết trong Bảng 10, phương pháp FREE thể hiện hiệu suất nhất quán và mạnh mẽ đồng thời đạt được tăng tốc lớn hơn so với CALM. Những kết quả này khẳng định rằng khung FREE của chúng tôi có thể được áp dụng rộng rãi, bất kể phương pháp giải mã được chọn.
