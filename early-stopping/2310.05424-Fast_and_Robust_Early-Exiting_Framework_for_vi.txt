# 2310.05424.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/early-stopping/2310.05424.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 1071223 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
Khung ThoÃ¡t Sá»›m Nhanh vÃ  Máº¡nh máº½ cho
MÃ´ hÃ¬nh NgÃ´n ngá»¯ Tá»± há»“i quy vá»›i Giáº£i mÃ£ Äá»“ng bá»™ Song song
Sangmin Bae1âˆ—Jongwoo Ko1âˆ—Hwanjun Song2â€ Se-Young Yun1â€ 
1KAIST AI2AWS AI
{bsmn0223, jongwoo.ko, yunseyoung}@kaist.ac.kr hwanjuns@amazon.com
https://github.com/raymin0223/fast_robust_early_exit
TÃ³m táº¯t
Äá»ƒ giáº£i quyáº¿t Ä‘á»™ trá»… suy luáº­n cao Ä‘Æ°á»£c thá»ƒ hiá»‡n
bá»Ÿi cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± há»“i quy, cÃ¡c nghiÃªn cá»©u
trÆ°á»›c Ä‘Ã¢y Ä‘Ã£ Ä‘á» xuáº¥t má»™t khung thoÃ¡t sá»›m
phÃ¢n bá»• cÃ¡c Ä‘Æ°á»ng dáº«n tÃ­nh toÃ¡n thÃ­ch á»©ng
cho tá»«ng token dá»±a trÃªn Ä‘á»™ phá»©c táº¡p cá»§a viá»‡c táº¡o
token tiáº¿p theo. Tuy nhiÃªn, chÃºng tÃ´i
quan sÃ¡t tháº¥y má»™t sá»‘ thiáº¿u sÃ³t, bao gá»“m suy giáº£m
hiá»‡u suáº¥t do cÆ¡ cháº¿ sao chÃ©p tráº¡ng thÃ¡i
hoáº·c nhiá»u Ä‘Æ°á»ng thoÃ¡t, vÃ  Ä‘á»™ nháº¡y cáº£m
vá»›i ngÆ°á»¡ng tin cáº­y thoÃ¡t. Do Ä‘Ã³,
chÃºng tÃ´i Ä‘á» xuáº¥t khung ThoÃ¡t Sá»›m Nhanh vÃ 
Máº¡nh máº½ (FREE), káº¿t há»£p mÃ´-Ä‘un nÃ´ng-sÃ¢u
vÃ  giáº£i mÃ£ Ä‘á»“ng bá»™ song song. Khung cá»§a chÃºng tÃ´i
cho phÃ©p suy luáº­n nhanh hÆ¡n báº±ng cÃ¡ch Ä‘á»“ng bá»™ hÃ³a
quÃ¡ trÃ¬nh giáº£i mÃ£ cá»§a token hiá»‡n táº¡i vá»›i
cÃ¡c token thoÃ¡t sá»›m Ä‘Ã£ Ä‘Æ°á»£c xáº¿p chá»“ng trÆ°á»›c Ä‘Ã³.
HÆ¡n ná»¯a, vÃ¬ giáº£i mÃ£ song song cho phÃ©p chÃºng ta
quan sÃ¡t cÃ¡c dá»± Ä‘oÃ¡n tá»« cáº£ mÃ´ hÃ¬nh nÃ´ng vÃ  sÃ¢u,
chÃºng tÃ´i trÃ¬nh bÃ y má»™t bá»™ Æ°á»›c tÃ­nh ngÆ°á»¡ng thÃ­ch á»©ng
má»›i khai thÃ¡c mÃ´ hÃ¬nh há»—n há»£p Beta Ä‘á»ƒ xÃ¡c Ä‘á»‹nh
ngÆ°á»¡ng tin cáº­y phÃ¹ há»£p. ChÃºng tÃ´i Ä‘Ã£ chá»©ng minh
thá»±c nghiá»‡m tÃ­nh Æ°u viá»‡t cá»§a khung Ä‘á» xuáº¥t
trÃªn cÃ¡c nhiá»‡m vá»¥ táº¡o sinh má»Ÿ rá»™ng.

1 Giá»›i thiá»‡u
Nhá»¯ng tiáº¿n bá»™ gáº§n Ä‘Ã¢y trong cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯
tá»± há»“i quy (Brown et al., 2020; Raffel et al., 2020;
Hoffmann et al., 2022; Touvron et al., 2023) Ä‘Ã£
cÃ¡ch máº¡ng hÃ³a cháº¥t lÆ°á»£ng táº¡o sinh ngÃ´n ngá»¯
trong cÃ¡c nhiá»‡m vá»¥ táº¡o sinh khÃ¡c nhau, bao gá»“m tráº£ lá»i
cÃ¢u há»i (Rajpurkar et al., 2016a), tÃ³m táº¯t
(Nallapati et al., 2016; Fabbri et al., 2019b),
vÃ  dá»‹ch mÃ¡y (Cettolo et al., 2017a).
Tuy nhiÃªn, nhá»¯ng mÃ´ hÃ¬nh transformer lá»›n nÃ y Ä‘Ã£
cho tháº¥y Ä‘á»™ trá»… suy luáº­n cao do sá»‘ lÆ°á»£ng
lá»›p Ä‘Ã¡ng ká»ƒ vÃ  bÆ°á»›c giáº£i mÃ£ tá»± há»“i quy.
VÃ¬ nhiá»u chá»“ng lá»›p transformer pháº£i Ä‘Æ°á»£c
tÃ­nh toÃ¡n tuáº§n tá»± cho tá»«ng token riÃªng láº»,
quÃ¡ trÃ¬nh suy luáº­n gÃ¢y ra gÃ¡nh náº·ng tÃ­nh toÃ¡n
Ä‘Ã¡ng ká»ƒ vÃ  cáº£n trá»Ÿ kháº£ nÄƒng thÃ­ch á»©ng thá»i gian
thá»±c cá»§a chÃºng (Jiao et al., 2020).

Vá»›i sá»± cáº§n thiáº¿t pháº£i Ä‘áº©y nhanh Ä‘á»™ trá»… suy
luáº­n, khung thoÃ¡t sá»›m (Elbayad et al.,
2020; Liu et al., 2021; Schuster et al., 2022)
ná»•i lÃªn nhÆ° má»™t phÆ°Æ¡ng phÃ¡p Ä‘áº§y há»©a háº¹n
phÃ¢n bá»• Ä‘á»™ng cÃ¡c Ä‘Æ°á»ng dáº«n tÃ­nh toÃ¡n dá»±a trÃªn
Ä‘á»™ phá»©c táº¡p táº¡o sinh cho tá»«ng token. NhÆ° Ä‘Æ°á»£c
minh há»a trong HÃ¬nh 1a, cÃ¡c token tÆ°Æ¡ng Ä‘á»‘i dá»…
dá»± Ä‘oÃ¡n token tiáº¿p theo táº¡o ra cÃ¡c dá»± Ä‘oÃ¡n
nháº¥t quÃ¡n chá»‰ vá»›i vÃ i phÃ©p tÃ­nh lá»›p, trong khi
nhá»¯ng token cÃ³ Ä‘á»™ khÃ³ cao hÆ¡n yÃªu cáº§u tÃ­nh toÃ¡n
qua sá»‘ lÆ°á»£ng lá»›p lá»›n hÆ¡n Ä‘á»ƒ táº¡o ra dá»± Ä‘oÃ¡n
chÃ­nh xÃ¡c. Trong ká»‹ch báº£n lÃ½ tÆ°á»Ÿng, phÆ°Æ¡ng phÃ¡p
thoÃ¡t sá»›m cho phÃ©p cÃ¡c mÃ´ hÃ¬nh Ä‘áº¡t Ä‘Æ°á»£c gia tá»‘c
Ä‘Ã¡ng ká»ƒ trong suy luáº­n mÃ  khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n
cháº¥t lÆ°á»£ng táº¡o sinh so vá»›i mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§.

Tuy nhiÃªn, phÃ¢n tÃ­ch má»Ÿ rá»™ng cá»§a chÃºng tÃ´i Ä‘Ã£
xÃ¡c Ä‘á»‹nh bá»‘n thÃ¡ch thá»©c trong khung thoÃ¡t sá»›m.
Thá»© nháº¥t, máº·c dÃ¹ cÃ³ tiá»m nÄƒng thoÃ¡t á»Ÿ cÃ¡c lá»›p
sá»›m hÆ¡n, cÃ¡c tráº¡ng thÃ¡i key vÃ  value cho cÃ¡c lá»›p
cÃ²n láº¡i váº«n cáº§n thiáº¿t Ä‘á»ƒ xá»­ lÃ½ cÃ¡c token tiáº¿p theo.
Trong khi cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y Ä‘Ã£ Ä‘á» xuáº¥t
cÆ¡ cháº¿ sao chÃ©p tráº¡ng thÃ¡i (Elbayad et al., 2020;
Schuster et al., 2022) Ä‘á»ƒ tÃ­nh toÃ¡n hiá»‡u quáº£
cÃ¡c tráº¡ng thÃ¡i nÃ y báº±ng cÃ¡ch tÃ¡i sá»­ dá»¥ng tráº¡ng thÃ¡i
áº©n tá»« lá»›p thoÃ¡t sá»›m, cÃ¡c phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i
cho tháº¥y phÆ°Æ¡ng phÃ¡p nÃ y hoáº¡t Ä‘á»™ng kÃ©m vá»›i
cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n vÃ  chuá»—i Ä‘áº§u ra dÃ i hÆ¡n
(xem Pháº§n 4.1). NgoÃ i ra, viá»‡c Ä‘áº·t táº¥t cáº£ cÃ¡c lá»›p
lÃ m vá»‹ trÃ­ thoÃ¡t cÃ³ thá»ƒ khÃ´ng Ä‘áº£m báº£o suy luáº­n
nhanh hÆ¡n do (1) hiá»‡u suáº¥t kÃ©m cá»§a cÃ¡c lá»›p
sá»›m hÆ¡n cÃ³ thá»ƒ táº¡o ra Ä‘áº§u ra chuá»—i dÃ i báº¥t thÆ°á»ng,
vÃ  (2) chi phÃ­ tÃ­nh toÃ¡n tá»« Ä‘o lÆ°á»ng tin cáº­y
á»Ÿ má»i lá»›p (xem Pháº§n 4.2 vÃ  4.3). Cuá»‘i cÃ¹ng,
viá»‡c Ä‘áº¡t Ä‘Æ°á»£c má»©c Ä‘á»™ Ä‘á»™ trá»… vÃ  Ä‘á»™ chÃ­nh xÃ¡c
mong muá»‘n vá»›i thoÃ¡t sá»›m phá»¥ thuá»™c nhiá»u vÃ o
viá»‡c chá»n ngÆ°á»¡ng tin cáº­y phÃ¹ há»£p cho nhiá»‡m vá»¥
Ä‘Ã­ch. Äiá»u nÃ y thÆ°á»ng Ä‘Ã²i há»i ná»— lá»±c Ä‘Ã¡ng ká»ƒ
vÃ  chi phÃ­ tÃ­nh toÃ¡n bá»• sung (xem Pháº§n 4.4).
Do Ä‘Ã³, nhá»¯ng thÃ¡ch thá»©c nÃ y Ä‘Ã²i há»i má»™t phÆ°Æ¡ng phÃ¡p
má»›i nháº¥t quÃ¡n arXiv:2310.05424v1  [cs.CL]  9 Oct 2023

--- TRANG 2 ---
ğ¡!"ğ¡"!ğ¡#$%!ğ¡&!
<S>Earthğ¡#!ğ¡#"
isflatğ¡&!Lá»›p ğŸğ¡!!ğ¡"!ğ¡&!Earthisflat<E>Lá»›p ğ‘³ğ¡!"
ğŸ¤¨
Lá»›p ğŸğ¡"!(a) ThoÃ¡t Sá»›m ThÃ´ng thÆ°á»ng
isDobbyğ¡!"#!ğ¡$"#!ğ¡%"#!
<S>Earthğ¡$!ğ¡%!ğ¡!$ğ¡%$
isğ¡&$Lá»›p ğŸğ¡!!ğ¡$$Earthisround<E>0.850.950.1Conf (nÃ´ng) :Giá»‘ng nhau? :Táº­p Hiá»‡u chuáº©n (ğ““ğ’„)
MÃ´ hÃ¬nh Há»—n há»£p BetatÃ¬m MLEFalseTrueTrueEarthisflatPred(nÃ´ng) :EarthisPred(sÃ¢u) :Lá»›p ğ‘³(sÃ¢u)
ğŸ¤©
roundroundLá»›p ğŸ(nÃ´ng)ğ¡&! (b) ThoÃ¡t Sá»›m Nhanh vÃ  Máº¡nh máº½ (FREE)

HÃ¬nh 1: Tá»•ng quan vá» khung FREE cá»§a chÃºng tÃ´i so vá»›i khung thoÃ¡t sá»›m thÃ´ng thÆ°á»ng. FREE thá»ƒ hiá»‡n
ba khÃ¡c biá»‡t chÃ­nh: (1) FREE sá»­ dá»¥ng mÃ´-Ä‘un nÃ´ng-sÃ¢u táº­n dá»¥ng hai Ä‘iá»ƒm thoÃ¡t thay vÃ¬ sá»­ dá»¥ng
táº¥t cáº£ cÃ¡c lá»›p lÃ m Ä‘iá»ƒm thoÃ¡t, (2) FREE thay tháº¿ cÆ¡ cháº¿ sao chÃ©p tráº¡ng thÃ¡i (mÃ u vÃ ng) báº±ng giáº£i mÃ£
Ä‘á»“ng bá»™ song song (mÃ u Ä‘á») Ä‘á»ƒ ngÄƒn suy giáº£m hiá»‡u suáº¥t Ä‘á»“ng thá»i tÄƒng tá»‘c Ä‘á»™ suy luáº­n, vÃ  (3) FREE
sá»­ dá»¥ng bá»™ Æ°á»›c tÃ­nh ngÆ°á»¡ng thÃ­ch á»©ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh giÃ¡ trá»‹ ngÆ°á»¡ng phÃ¹ há»£p cho má»—i táº­p dá»¯ liá»‡u trong suy luáº­n.

thá»ƒ hiá»‡n hiá»‡u suáº¥t cao vÃ  Ä‘á»™ trá»… tháº¥p qua cÃ¡c
mÃ´ hÃ¬nh ngÃ´n ngá»¯ vÃ  táº­p dá»¯ liá»‡u Ä‘a dáº¡ng.

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i giá»›i thiá»‡u khung
ThoÃ¡t Sá»›m Nhanh vÃ  Máº¡nh máº½ (FREE) káº¿t há»£p
mÃ´-Ä‘un nÃ´ng-sÃ¢u vÃ  giáº£i mÃ£ Ä‘á»“ng bá»™ song song.
Khung cá»§a chÃºng tÃ´i khÃ´ng chá»‰ cung cáº¥p tÄƒng tá»‘c
vÃ  hiá»‡u suáº¥t nháº¥t quÃ¡n ngay cáº£ Ä‘á»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh
lá»›n hÆ¡n vÃ  chuá»—i Ä‘áº§u ra dÃ i hÆ¡n, mÃ  cÃ²n loáº¡i bá»
nhu cáº§u cho quÃ¡ trÃ¬nh tÃ­nh toÃ¡n Ä‘áº¯t Ä‘á» Ä‘á»ƒ tÃ¬m
ngÆ°á»¡ng thoÃ¡t phÃ¹ há»£p.

Cá»¥ thá»ƒ, mÃ´-Ä‘un nÃ´ng-sÃ¢u chia nhá» cÃ¡c Ä‘Æ°á»ng dáº«n
tÃ­nh toÃ¡n thÃ nh mÃ´ hÃ¬nh nÃ´ng (vá»›i sá»‘ lÆ°á»£ng lá»›p
sá»›m Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh) vÃ  mÃ´ hÃ¬nh sÃ¢u (bao gá»“m táº¥t cáº£
cÃ¡c lá»›p). Giáº£i mÃ£ Ä‘á»“ng bá»™ song song cá»§a chÃºng tÃ´i
tÃ­ch lÅ©y cÃ¡c token thoÃ¡t sá»›m liÃªn tiáº¿p chá»‰ Ä‘i qua
mÃ´ hÃ¬nh nÃ´ng cho Ä‘áº¿n khi gáº·p token khÃ´ng thoÃ¡t.
Do Ä‘Ã³, chÃºng tÃ´i Ä‘á»“ng bá»™ hÃ³a quÃ¡ trÃ¬nh giáº£i mÃ£
cá»§a token khÃ´ng thoÃ¡t hiá»‡n táº¡i vá»›i cÃ¡c token
Ä‘Ã£ xáº¿p chá»“ng trÆ°á»›c Ä‘Ã³, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n á»Ÿ bÃªn
trÃ¡i HÃ¬nh 1b. Äiá»u nÃ y ngÄƒn suy giáº£m hiá»‡u suáº¥t
báº±ng cÃ¡ch sá»­ dá»¥ng key vÃ  value attention thá»±c táº¿
Ä‘Æ°á»£c tÃ­nh toÃ¡n thay vÃ¬ cÃ¡c tráº¡ng thÃ¡i xáº¥p xá»‰
thÃ´ng qua sao chÃ©p tráº¡ng thÃ¡i, Ä‘á»“ng thá»i Ä‘áº¡t Ä‘Æ°á»£c
phÆ°Æ¡ng phÃ¡p hiá»‡u quáº£ hÆ¡n so vá»›i giáº£i mÃ£ tá»«ng
token tá»± há»“i quy. HÆ¡n ná»¯a, chÃºng tÃ´i thiáº¿t káº¿
má»™t bá»™ Æ°á»›c tÃ­nh ngÆ°á»¡ng thÃ­ch á»©ng má»›i, nhÆ° Ä‘Æ°á»£c
thá»ƒ hiá»‡n á»Ÿ bÃªn pháº£i HÃ¬nh 1b, báº±ng cÃ¡ch táº­n dá»¥ng
thá»±c táº¿ ráº±ng giáº£i mÃ£ song song xuáº¥t ra dá»± Ä‘oÃ¡n
ngay cáº£ cho cÃ¡c token thoÃ¡t sá»›m tá»« mÃ´ hÃ¬nh sÃ¢u.
Bá»™ Æ°á»›c tÃ­nh nÃ y sá»­ dá»¥ng mÃ´ hÃ¬nh há»—n há»£p Beta (BMM)
Ä‘á»ƒ náº¯m báº¯t má»‘i tÆ°Æ¡ng quan giá»¯a Ä‘iá»ƒm tin cáº­y
vÃ  sá»± phÃ¹ há»£p dá»± Ä‘oÃ¡n cá»§a hai mÃ´ hÃ¬nh, xÃ¡c Ä‘á»‹nh
ngÆ°á»¡ng tin cáº­y phÃ¹ há»£p cho tá»«ng táº­p dá»¯ liá»‡u.

Trong thá»±c táº¿, chÃºng tÃ´i chá»©ng minh hiá»‡u quáº£
cá»§a khung FREE trÃªn cÃ¡c nhiá»‡m vá»¥ táº¡o sinh má»Ÿ rá»™ng.

2 CÃ´ng trÃ¬nh LiÃªn quan

2.1 Khung ThoÃ¡t Sá»›m
Khi kÃ­ch thÆ°á»›c cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ tÄƒng
Ä‘Ã¡ng ká»ƒ, Ä‘Ã£ cÃ³ nhiá»u ná»— lá»±c Ä‘á»ƒ phÃ¡t triá»ƒn cÃ¡c
phÆ°Æ¡ng phÃ¡p giáº£i mÃ£ hiá»‡u quáº£ giáº£m chi phÃ­
tÃ­nh toÃ¡n cá»§a cÃ¡c nhiá»‡m vá»¥ táº¡o sinh ngÃ´n ngá»¯.
ÄÆ°á»£c thÃºc Ä‘áº©y bá»Ÿi cÃ¡c tÃ i liá»‡u trÆ°á»›c Ä‘Ã¢y (Teerapittayanon et al.,
2016; Graves, 2016; Zhang et al., 2019a), Elbayad
et al. (2020) Ä‘Ã£ giá»›i thiá»‡u khung thoÃ¡t sá»›m
cho suy luáº­n nhanh hÆ¡n, Ä‘iá»u chá»‰nh Ä‘á»™ng
Ä‘á»™ sÃ¢u cá»§a bá»™ giáº£i mÃ£ cho tá»«ng táº¡o sinh token
báº±ng cÃ¡ch Ä‘Æ°a ra dá»± Ä‘oÃ¡n á»Ÿ lá»›p trung gian.
Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± cÃ¢n báº±ng tá»‘t hÆ¡n giá»¯a tá»‘c Ä‘á»™
vÃ  Ä‘á»™ chÃ­nh xÃ¡c, Schuster et al. (2022) gáº§n Ä‘Ã¢y
Ä‘Ã£ khÃ¡m phÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p ngÆ°á»¡ng tin cáº­y,
bao gá»“m cÃ¡c biá»‡n phÃ¡p tin cáº­y khÃ¡c nhau, hÃ m
ngÆ°á»¡ng giáº£m dáº§n, vÃ  phÆ°Æ¡ng phÃ¡p hiá»‡u chuáº©n.

Tuy nhiÃªn, cÃ¡c thÃ­ nghiá»‡m cá»§a há» chá»§ yáº¿u Ä‘Æ°á»£c
thá»±c hiá»‡n trÃªn cÃ¡c mÃ´ hÃ¬nh giáº£i mÃ£ kÃ­ch thÆ°á»›c nhá»,
cáº§n thiáº¿t xÃ¡c thá»±c thÃªm trÃªn cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n.
NgoÃ i ra, cÃ¡c phÆ°Æ¡ng phÃ¡p cá»§a há» yÃªu cáº§u thá»i gian
Ä‘Ã o táº¡o bá»• sung cho cÃ¡c kiá»ƒm tra thá»‘ng kÃª trÃªn
táº­p hiá»‡u chuáº©n bá»• sung, Ä‘iá»u nÃ y ngÄƒn chÃºng
khá»i cÃ¡c ká»‹ch báº£n triá»ƒn khai thá»±c táº¿.

2.2 Giáº£i mÃ£ Song song
Giáº£i mÃ£ khÃ´ng tá»± há»“i quy, táº¡o ra nhiá»u token
Ä‘áº§u ra song song, ban Ä‘áº§u Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi
Gu et al. (2018). Má»™t sá»‘ cÃ´ng trÃ¬nh (Ghazvininejad et al.,
2019; Gu and Kong, 2021; Savinov et al., 2022;
Santilli et al., 2023) tá»« Ä‘Ã³ Ä‘Ã£ táº­p trung vÃ o
viá»‡c nÃ¢ng cao cháº¥t lÆ°á»£ng táº¡o sinh trong cÃ¡c
nhiá»‡m vá»¥ dá»‹ch mÃ¡y. Tiáº¿p theo, Leviathan
et al. (2023) Ä‘Ã£ giá»›i thiá»‡u giáº£i mÃ£ suy Ä‘oÃ¡n
cho cÃ¡c nhiá»‡m vá»¥ táº¡o sinh chuá»—i. Trong phÆ°Æ¡ng phÃ¡p nÃ y,

--- TRANG 3 ---
mÃ´ hÃ¬nh xáº¥p xá»‰ (kÃ­ch thÆ°á»›c nhá») dá»± Ä‘oÃ¡n Ä‘áº§u ra
tá»± há»“i quy, trong khi mÃ´ hÃ¬nh Ä‘Ã­ch (kÃ­ch thÆ°á»›c lá»›n)
cháº¡y song song Ä‘á»ƒ xÃ¡c minh viá»‡c cháº¥p nháº­n cÃ¡c
dá»± Ä‘oÃ¡n Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi mÃ´ hÃ¬nh xáº¥p xá»‰.
Chá»‰ vá»›i cÃ¡c token Ä‘Æ°á»£c cháº¥p nháº­n, há» láº¥y máº«u láº¡i
token tiáº¿p theo tá»« phÃ¢n phá»‘i Ä‘iá»u chá»‰nh. CÃ¡c
phÆ°Æ¡ng phÃ¡p liÃªn quan Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Chen et al.
(2023) vÃ  Kim et al. (2023), nÆ¡i há» cÅ©ng sá»­ dá»¥ng
hai mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u khÃ¡c nhau vÃ  táº­p trung vÃ o
viá»‡c tinh chá»‰nh mÃ´ hÃ¬nh nhá» thÃ´ng qua láº¥y máº«u
suy Ä‘oÃ¡n hoáº·c chÃ­nh sÃ¡ch rollback theo cÃ¡ch
khÃ´ng tá»± há»“i quy.

PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i khÃ¡c biá»‡t Ä‘Ã¡ng ká»ƒ
so vá»›i cÃ¡c cÃ´ng trÃ¬nh nÃªu trÃªn vÃ¬ chÃºng tÃ´i táº­p trung
vÃ o khung thoÃ¡t sá»›m báº±ng cÃ¡ch giá»›i thiá»‡u giáº£i mÃ£
Ä‘á»“ng bá»™ song song trong má»™t máº¡ng duy nháº¥t,
káº¿t há»£p mÃ´-Ä‘un nÃ´ng-sÃ¢u. Trong khi chÃºng tÃ´i
cÅ©ng táº­n dá»¥ng lá»£i tháº¿ cá»§a viá»‡c Ä‘á»“ng thá»i thu Ä‘Æ°á»£c
dá»± Ä‘oÃ¡n tá»« cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u khÃ¡c nhau,
chÃºng tÃ´i nháº±m phÃ¡t triá»ƒn phÆ°Æ¡ng phÃ¡p Æ°á»›c tÃ­nh
má»›i vÃ  hiá»‡u quáº£ Ä‘á»ƒ xÃ¡c Ä‘á»‹nh thÃ­ch á»©ng ngÆ°á»¡ng
tá»‘i Æ°u cho tá»«ng táº­p dá»¯ liá»‡u. ÄÃ¡ng chÃº Ã½ ráº±ng
cÃ¡c chiáº¿n lÆ°á»£c tinh chá»‰nh cá»§a há» cÃ³ thá»ƒ dáº«n Ä‘áº¿n
tÄƒng Ä‘á»™ trá»… khÃ´ng giá»›i háº¡n vÃ¬ há» khá»Ÿi Ä‘á»™ng láº¡i
tá»« cÃ¡c dá»± Ä‘oÃ¡n khÃ´ng chÃ­nh xÃ¡c.

3 Kiáº¿n thá»©c CÆ¡ báº£n
Máº¡ng Transformer (Vaswani et al., 2017) Ä‘Æ°á»£c
cáº¥u thÃ nh tá»« L lá»›p, trong Ä‘Ã³ má»—i lá»›p bao gá»“m
hai lá»›p con, lá»›p multi-head attention (MHA)
vÃ  lá»›p feed-forward network (FFN). PhÃ©p tÃ­nh
cho tráº¡ng thÃ¡i áº©n táº¡i bÆ°á»›c thá»i gian t+1 thÃ´ng qua
cÃ¡c khá»‘i Transformer xáº¿p chá»“ng nhÆ° sau:

hâ„“_{t+1} = Transformer^â„“(h^{â„“-1}_{t+1}), â„“ âˆˆ [1, L],

trong Ä‘Ã³ h^0_{t+1} lÃ  Ä‘áº§u ra lá»›p embedding cá»§a y_t
Ä‘áº¡i diá»‡n cho token Ä‘Æ°á»£c táº¡o ra táº¡i bÆ°á»›c thá»i gian t.
Sau lá»›p thá»© L cá»§a máº¡ng giáº£i mÃ£, token dá»± Ä‘oÃ¡n
Å·_{t+1} Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi Ä‘áº§u ra xÃ¡c suáº¥t tá»«
bá»™ phÃ¢n loáº¡i softmax W_L:

p(y_{t+1}|h^L_{t+1}) = softmax(W^âŠº_L h^L_{t+1})

Tuy nhiÃªn, khÃ´ng giá»‘ng nhÆ° LM tiÃªu chuáº©n,
khung thoÃ¡t sá»›m cho phÃ©p táº¡o ra token tiáº¿p theo
á»Ÿ cÃ¡c lá»›p sá»›m hÆ¡n báº±ng cÃ¡ch sá»­ dá»¥ng p(y_{t+1}|h^â„“_{t+1}).
Náº¿u Ä‘iá»ƒm tin cáº­y c^â„“ lá»›n hÆ¡n ngÆ°á»¡ng Ä‘Æ°á»£c Ä‘á»‹nh
trÆ°á»›c, chÃºng ta cÃ³ thá»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n táº¡i bÆ°á»›c
thá»i gian t+1 lÃ  arg max p(y_{t+1}|h^â„“_{t+1}).
Trong khi cÃ¡c bá»™ phÃ¢n loáº¡i cÃ³ thá»ƒ Ä‘Æ°á»£c tham sá»‘ hÃ³a
Ä‘á»™c láº­p hoáº·c chia sáº» qua L lá»›p, háº§u háº¿t cÃ¡c
phÆ°Æ¡ng phÃ¡p thoÃ¡t sá»›m (Elbayad et al., 2020;
Liu et al., 2021; Schuster et al., 2022) sá»­ dá»¥ng
bá»™ phÃ¢n loáº¡i chia sáº» do sá»‘ lÆ°á»£ng lá»›n tham sá»‘
gÃ¢y ra bá»Ÿi kÃ­ch thÆ°á»›c tá»« vá»±ng khá»•ng lá»“.

Báº£ng 1: So sÃ¡nh Ä‘iá»ƒm ROUGE-L giá»¯a mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§,
Ä‘Æ°á»£c tinh chá»‰nh sá»­ dá»¥ng táº¥t cáº£ Ä‘áº§u ra lá»›p, vÃ  oracle-exiting.
ChÃºng tÃ´i cÅ©ng Ä‘o Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a tráº¡ng thÃ¡i áº©n
cá»§a lá»›p cuá»‘i vÃ  lá»›p oracle-exited.

Dataset | Model     | Full M. | Oracle | Sim.
--------|-----------|---------|--------|-------
SAMSum  | T5-small  | 44.84   | 44.17 (-0.67) | 0.913
        | T5-large  | 48.82   | 47.58 (-1.24) | 0.809
CNN/DM  | T5-small  | 37.82   | 37.60 (-0.22) | 0.902
        | T5-large  | 41.15   | 40.15 (-1.00) | 0.792
Multi-News | LongT5-base | 37.62 | 29.63 (-7.99) | 0.724
BIGPATENT | LongT5-base | 49.68 | 44.99 (-4.69) | 0.686

Sau khi token hiá»‡n táº¡i Ä‘Æ°á»£c thoÃ¡t sá»›m á»Ÿ lá»›p thá»© â„“,
chÃºng ta cáº§n tÃ­nh toÃ¡n cÃ¡c tráº¡ng thÃ¡i key vÃ  value
cho táº¥t cáº£ cÃ¡c khá»‘i sÃ¢u hÆ¡n Ä‘á»ƒ thá»±c hiá»‡n
self-attention cho cÃ¡c token tiáº¿p theo Ä‘i qua
cÃ¡c khá»‘i sÃ¢u hÆ¡n. Äá»ƒ cÃ³ phÆ°Æ¡ng phÃ¡p hiá»‡u quáº£
hÆ¡n trong viá»‡c lÆ°u trá»¯ cÃ¡c tráº¡ng thÃ¡i key vÃ  value,
cÃ¡c khung thoÃ¡t sá»›m sá»­ dá»¥ng cÆ¡ cháº¿ sao chÃ©p tráº¡ng thÃ¡i.
NÃ³ nhÃ¢n Ä‘Ã´i cÃ¡c tráº¡ng thÃ¡i áº©n cá»§a lá»›p thoÃ¡t sá»›m
(tá»©c lÃ , h^i_{t+1} = h^â„“_{t+1}, âˆ€i âˆˆ [â„“+1, L]),
cho phÃ©p chÃºng ta tÃ­nh toÃ¡n cÃ¡c tráº¡ng thÃ¡i key
vÃ  value xáº¥p xá»‰ cáº§n thiáº¿t cho self-attention
cá»§a máº¡ng Transformer. Schuster et al. (2022)
Ä‘Ã£ xÃ¡c minh ráº±ng sao chÃ©p tráº¡ng thÃ¡i tá»« cÃ¡c lá»›p
tháº¥p hÆ¡n khÃ´ng cÃ³ tÃ¡c Ä‘á»™ng cÃ³ háº¡i Ä‘áº¿n hiá»‡u suáº¥t
trong trÆ°á»ng há»£p cÃ¡c mÃ´ hÃ¬nh T5 kÃ­ch thÆ°á»›c nhá»
(Raffel et al., 2020).

4 ÄÃ¡nh giÃ¡ láº¡i Khung ThoÃ¡t Sá»›m
Trong pháº§n nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y bá»‘n phÃ¡t hiá»‡n
má»›i tá»« viá»‡c Ä‘Ã¡nh giÃ¡ láº¡i khung thoÃ¡t sá»›m.
ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh T5
khÃ¡c nhau (Raffel et al., 2020) trÃªn SAMSum
(Gliwa et al., 2019) vÃ  CNN/DailyMail (See et al., 2017),
vÃ  kiáº¿n trÃºc LongT5-base (Guo et al., 2022)
trÃªn Multi-News (Fabbri et al., 2019a) vÃ 
BIGPATENT (Sharma et al., 2019).

4.1 Thiáº¿u TÃ­nh Máº¡nh máº½ Ä‘á»‘i vá»›i KÃ­ch thÆ°á»›c
MÃ´ hÃ¬nh vÃ  Äá»™ dÃ i Chuá»—i Äáº§u ra

ChÃºng tÃ´i Ä‘áº§u tiÃªn Ä‘Ã¡nh giÃ¡ láº¡i cÆ¡ cháº¿ sao chÃ©p
tráº¡ng thÃ¡i lÃ  thÃ nh pháº§n thiáº¿t yáº¿u cá»§a khung
thoÃ¡t sá»›m. Theo Schuster et al. (2022), chÃºng tÃ´i
sá»­ dá»¥ng biá»‡n phÃ¡p tin cáº­y oracle cho phÃ©p cÃ¡c
token thoÃ¡t á»Ÿ lá»›p sá»›m nháº¥t, sao cho dá»± Ä‘oÃ¡n
cá»§a chÃºng giá»‘ng há»‡t vá»›i dá»± Ä‘oÃ¡n cá»§a lá»›p cuá»‘i.
ÄÃ¡ng chÃº Ã½, nhÆ° quan sÃ¡t trong Báº£ng 1, sá»± suy giáº£m
cháº¥t lÆ°á»£ng táº¡o sinh vá»›i sao chÃ©p tráº¡ng thÃ¡i trá»Ÿ nÃªn
nghiÃªm trá»ng trÃªn cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n vÃ  táº­p dá»¯ liá»‡u
vá»›i chuá»—i dÃ i hÆ¡n (â–·Obs. 1). VÃ­ dá»¥, khi xem xÃ©t
káº¿t quáº£ oracle-exiting, mÃ´ hÃ¬nh T5-small

--- TRANG 4 ---
chá»‰ thá»ƒ hiá»‡n suy giáº£m 0.67 trÃªn táº­p dá»¯ liá»‡u SAMSum,
trong khi mÃ´ hÃ¬nh T5-large tráº£i qua sá»± giáº£m
lá»›n hÆ¡n nhiá»u lÃ  1.24. TÆ°Æ¡ng tá»±, trÃªn cÃ¡c táº­p dá»¯ liá»‡u
nhÆ° Multi-News vÃ  BIGPATENT, bao gá»“m cÃ¡c
chuá»—i Ä‘áº§u ra tÆ°Æ¡ng Ä‘á»‘i dÃ i, káº¿t quáº£ oracle-exiting
thá»ƒ hiá»‡n sá»± giáº£m 7.99 vÃ  4.69, tÆ°Æ¡ng á»©ng.

Äá»ƒ cá»§ng cá»‘ báº±ng chá»©ng há»— trá»£, chÃºng tÃ´i tiáº¿p tá»¥c
khÃ¡m phÃ¡ sá»± biáº¿n Ä‘á»™ng Ä‘Ã¡ng ká»ƒ trong phÃ¢n phá»‘i
tráº¡ng thÃ¡i áº©n qua cÃ¡c lá»›p khÃ¡c nhau. Trong Báº£ng 1,
chÃºng tÃ´i cÅ©ng bÃ¡o cÃ¡o Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a
tráº¡ng thÃ¡i áº©n cá»§a lá»›p cuá»‘i vÃ  lá»›p oracle-exited.
Máº·c dÃ¹ tráº¡ng thÃ¡i áº©n cá»§a lá»›p cuá»‘i vÃ  oracle-exited
táº¡o ra cÃ¹ng dá»± Ä‘oÃ¡n, Ä‘á»™ tÆ°Æ¡ng tá»± cosine giá»¯a chÃºng
giáº£m Ä‘Ã¡ng ká»ƒ khi máº¡ng giáº£i mÃ£ trá»Ÿ nÃªn lá»›n hÆ¡n
vÃ  chuá»—i Ä‘áº§u ra trá»Ÿ nÃªn dÃ i hÆ¡n.

4.2 Suy giáº£m Hiá»‡u suáº¥t do Vá»‹ trÃ­ ThoÃ¡t

Äá»ƒ táº¡o Ä‘iá»u kiá»‡n thoÃ¡t sá»›m cho táº¥t cáº£ cÃ¡c lá»›p
giáº£i mÃ£, cÃ¡c má»¥c tiÃªu Ä‘Ã o táº¡o cáº§n lÃ  sá»± káº¿t há»£p
cá»§a cÃ¡c má»¥c tiÃªu Ä‘Ã o táº¡o cho tá»«ng lá»›p riÃªng láº».
ChÃºng ta cÃ³ thá»ƒ trÃ¬nh bÃ y nhÆ° sau:

L = âˆ‘^L_{i=1} Î±_i L_i where âˆ‘_i Î±_i = 1, (1)

L_i vÃ  Î±_i lÃ  hÃ m máº¥t mÃ¡t negative log-likelihood
vÃ  há»‡ sá»‘ trá»ng sá»‘ cho lá»›p thá»© i, tÆ°Æ¡ng á»©ng. Äáº·c biá»‡t,
cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y Ä‘áº·t Î±_i lÃ  1/L (trung bÃ¬nh
khÃ´ng trá»ng sá»‘; Elbayad et al. 2020) hoáº·c i/âˆ‘_i i
(trung bÃ¬nh cÃ³ trá»ng sá»‘; Schuster et al. 2022).
Há» chá»©ng minh ráº±ng nhá»¯ng quy táº¯c trá»ng sá»‘ nÃ y
hiá»‡u quáº£ táº¡o Ä‘iá»u kiá»‡n há»c táº­p á»Ÿ cÃ¡c lá»›p sá»›m hÆ¡n
mÃ  khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n hiá»‡u suáº¥t tá»•ng thá»ƒ
cá»§a mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§ trÃªn cÃ¡c mÃ´ hÃ¬nh giáº£i mÃ£
kÃ­ch thÆ°á»›c nhá».

Tuy nhiÃªn, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong HÃ¬nh 2, chÃºng tÃ´i
quan sÃ¡t tháº¥y sá»± giáº£m Ä‘Ã¡ng ká»ƒ trong hiá»‡u suáº¥t
cá»§a static-exiting, sá»­ dá»¥ng cÃ¹ng sá»‘ lÆ°á»£ng lá»›p
cho táº¥t cáº£ token, khi chá»‰ sá»­ dá»¥ng má»™t pháº§n nhá»
cÃ¡c lá»›p sá»›m tá»« mÃ´ hÃ¬nh T5-large. (â–·Obs. 2).
VÃ­ dá»¥, náº¿u táº¥t cáº£ token Ä‘Æ°á»£c thoÃ¡t á»Ÿ lá»›p má»™t
hoáº·c hai, mÃ´ hÃ¬nh Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm ROUGE-L gáº§n nhÆ°
báº±ng khÃ´ng. HÆ¡n ná»¯a, khi chÃºng tÃ´i Ã¡p dá»¥ng khung
thoÃ¡t sá»›m cho cÃ¡c mÃ´ hÃ¬nh nÃ y trong suy luáº­n,
chÃºng tÃ´i xÃ¡c minh ráº±ng mÃ´ hÃ¬nh T5-large táº¡o ra
cÃ¢u dÃ i báº¥t thÆ°á»ng, thá»±c sá»± tiÃªu tá»‘n nhiá»u thá»i gian
suy luáº­n hÆ¡n. Dá»±a trÃªn nhá»¯ng káº¿t quáº£ nÃ y, trong
cÃ¡c thÃ­ nghiá»‡m tiáº¿p theo, chÃºng tÃ´i Ä‘Ã£ loáº¡i trá»«
hai hoáº·c bá»‘n lá»›p Ä‘áº§u tiÃªn khá»i cÃ¡c á»©ng viÃªn
cho cÃ¡c lá»›p thoÃ¡t sá»›m cá»§a mÃ´ hÃ¬nh base vÃ  large,
tÆ°Æ¡ng á»©ng.

[HÃ¬nh 2 vÃ  cÃ¡c biá»ƒu Ä‘á»“: Minh há»a Ä‘iá»ƒm ROUGE-L vÃ  Ä‘á»™ dÃ i chuá»—i Ä‘Æ°á»£c táº¡o tá»« phÆ°Æ¡ng phÃ¡p static-exiting trong T5-small (trÃ¡i) vÃ  T5-large (pháº£i) trÃªn táº­p dá»¯ liá»‡u SAMSum. ÄÆ°á»ng ngang Ä‘á»©t nÃ©t Ä‘áº¡i diá»‡n cho Ä‘á»™ dÃ i chuá»—i trung bÃ¬nh cá»§a ground truth.]

[HÃ¬nh 3: Chi phÃ­ tÃ­nh toÃ¡n theo thÃ nh pháº§n trÃªn ba táº­p dá»¯ liá»‡u. Bá»‘n thanh tÆ°Æ¡ng á»©ng vá»›i mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§ vÃ  thoÃ¡t sá»›m vá»›i ngÆ°á»¡ng 0.9, 0.7, vÃ  0.5. MÃ u gáº¡ch chÃ©o biá»ƒu thá»‹ thá»i gian trÃ´i qua sau khi token thoÃ¡t, liÃªn quan Ä‘áº¿n cÆ¡ cháº¿ sao chÃ©p tráº¡ng thÃ¡i. CÃ¡c sá»‘ trÃªn thanh Ä‘áº¡i diá»‡n cho Ä‘iá»ƒm ROUGE-L. SA vÃ  CA biá»ƒu thá»‹ self- vÃ  cross-attention, tÆ°Æ¡ng á»©ng.]

4.3 Chi phÃ­ TÃ­nh toÃ¡n KhÃ´ng thá»ƒ Bá» qua

Trong quÃ¡ trÃ¬nh phÃ¢n tÃ­ch, chÃºng tÃ´i quan sÃ¡t tháº¥y
khung thoÃ¡t sá»›m thÃ´ng thÆ°á»ng khÃ´ng chá»‰ thá»ƒ hiá»‡n
nhá»¯ng báº¥t lá»£i vá» hiá»‡u suáº¥t mÃ  cÃ²n Ä‘áº·t ra thÃ¡ch thá»©c
cho Ä‘á»™ trá»… suy luáº­n. Trong HÃ¬nh 3, chÃºng tÃ´i thá»±c hiá»‡n
phÃ¢n tÃ­ch chi phÃ­ tÃ­nh toÃ¡n liÃªn quan Ä‘áº¿n mÃ´ hÃ¬nh
giáº£i mÃ£ qua ba táº­p dá»¯ liá»‡u tÃ³m táº¯t. ÄÃ¡ng ngáº¡c nhiÃªn,
thoÃ¡t sá»›m thÆ°á»ng cho tháº¥y sá»± tÄƒng báº¥t ngá» trong
tá»•ng thá»i gian giáº£i mÃ£ so vá»›i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ
khÃ´ng sá»­ dá»¥ng thoÃ¡t sá»›m.

Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c quy cho chi phÃ­ tÃ­nh toÃ¡n
khÃ´ng thá»ƒ bá» qua trong viá»‡c Ä‘o lÆ°á»ng tin cáº­y
táº¡i má»—i lá»›p, Ä‘áº·c biá»‡t do cÃ¡c phÃ©p toÃ¡n softmax
vá»›i kÃ­ch thÆ°á»›c tá»« vá»±ng lá»›n. NgoÃ i ra, máº·c dÃ¹
phÆ°Æ¡ng phÃ¡p sao chÃ©p tráº¡ng thÃ¡i nháº±m giáº£m
thá»i gian tÃ­nh toÃ¡n trong cÃ¡c lá»›p MHA vÃ  FFN
cá»§a cÃ¡c lá»›p cÃ²n láº¡i, viá»‡c tÃ­nh toÃ¡n cÃ¡c tráº¡ng thÃ¡i
key vÃ  value sá»­ dá»¥ng tráº¡ng thÃ¡i áº©n nhÃ¢n Ä‘Ã´i
phÃ¡t sinh chi phÃ­ bá»• sung khÃ´ng thá»ƒ bá» qua
(â–·Obs. 3).

--- TRANG 5 ---
Báº£ng 2: NgÆ°á»¡ng tin cáº­y tá»‘i Æ°u Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t
mong muá»‘n. ChÃºng tÃ´i chá»n giÃ¡ trá»‹ tá»‘t nháº¥t trong sá»‘
cÃ¡c giÃ¡ trá»‹ ngÆ°á»¡ng tá»« 0 Ä‘áº¿n 1 vá»›i bÆ°á»›c 0.1. CÃ¡c sá»‘
tuáº§n tá»± Ä‘áº¡i diá»‡n cho ngÆ°á»¡ng Ä‘Æ°á»£c chá»n vÃ  hiá»‡u suáº¥t
tÆ°Æ¡ng á»©ng (mÃ u xÃ¡m).

Performance Drop
Task    Dataset      âˆ¼1%        âˆ¼5%        âˆ¼10%
SUM     SAMSum      1.0 (48.8) 0.7 (46.8) 0.5 (45.0)
        CNN/DM      1.0 (41.2) 0.5 (39.2) 0.3 (37.3)
        Multi-News  0.8 (37.3) 0.5 (35.9) 0.4 (34.9)
        BIGPATENT   1.0 (49.7) 0.8 (47.3) 0.6 (45.2)
QA      SQuAD       0.1 (90.1) 0.0 (88.3) 0.0 (88.3)
MT      IWSLT       1.0 (39.4) 1.0 (39.4) 1.0 (39.4)

4.4 NgÆ°á»¡ng Tin cáº­y Tá»‘i Æ°u KhÃ¡c biá»‡t

XÃ¡c Ä‘á»‹nh ngÆ°á»¡ng phÃ¹ há»£p cho tin cáº­y thoÃ¡t
lÃ  thÃ¡ch thá»©c quan trá»ng trong khung thoÃ¡t sá»›m
vÃ¬ nÃ³ áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n sá»± cÃ¢n báº±ng
giá»¯a hiá»‡u suáº¥t vÃ  Ä‘á»™ trá»… (Zhang et al., 2019b;
Schuster et al., 2022). NhÆ° Ä‘Æ°á»£c tÃ³m táº¯t trong
Báº£ng 2, cÃ¡c quan sÃ¡t cá»§a chÃºng tÃ´i chá»‰ ra ráº±ng
cÃ¡c ngÆ°á»¡ng tin cáº­y tá»‘i Æ°u Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ trá»…
tháº¥p nháº¥t trong cÃ¹ng hiá»‡u suáº¥t khÃ¡c nhau Ä‘Ã¡ng ká»ƒ
qua cÃ¡c táº­p dá»¯ liá»‡u (â–·Obs. 4). VÃ­ dá»¥, táº­p dá»¯ liá»‡u
SQuAD vÃ  CNN/DailyMail cÃ³ thá»ƒ duy trÃ¬ hiá»‡u suáº¥t
vá»›i ngÆ°á»¡ng thoÃ¡t tÆ°Æ¡ng Ä‘á»‘i tháº¥p hÆ¡n, trong khi
giÃ¡ trá»‹ ngÆ°á»¡ng cao hÆ¡n Ä‘Æ°á»£c yÃªu cáº§u trong trÆ°á»ng há»£p
táº­p dá»¯ liá»‡u IWSLT. CÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y (Schuster et al.,
2022) Ä‘Ã£ táº­n dá»¥ng cÃ¡c ká»¹ thuáº­t kiá»ƒm soÃ¡t rá»§i ro
distribution-free cho táº¡o sinh tin cáº­y. Tuy nhiÃªn,
nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y yÃªu cáº§u thá»i gian Ä‘Ã o táº¡o
bá»• sung cho cÃ¡c kiá»ƒm tra thá»‘ng kÃª trÃªn táº­p
hiá»‡u chuáº©n bá»• sung trÆ°á»›c triá»ƒn khai, nÆ¡i thá»i gian
cÅ©ng cÃ³ thá»ƒ bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi kÃ­ch thÆ°á»›c cá»§a
táº­p á»©ng viÃªn ngÆ°á»¡ng.

5 Khung ThoÃ¡t Sá»›m Má»›i: FREE

Dá»±a trÃªn cÃ¡c khÃ¡m phÃ¡ trong Pháº§n 4, chÃºng tÃ´i
giá»›i thiá»‡u khung ThoÃ¡t Sá»›m Nhanh vÃ  Máº¡nh máº½
cÃ³ tÃªn FREE, táº­n dá»¥ng mÃ´-Ä‘un nÃ´ng-sÃ¢u vÃ 
táº­n dá»¥ng cáº¥u trÃºc cá»§a giáº£i mÃ£ song song. HÆ¡n ná»¯a,
chÃºng tÃ´i trÃ¬nh bÃ y thuáº­t toÃ¡n Æ°á»›c tÃ­nh tin cáº­y
Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ nÃ¢ng cao tÃ­nh máº¡nh máº½ cá»§a
thoÃ¡t sá»›m trong khung FREE.

5.1 MÃ´-Ä‘un NÃ´ng-SÃ¢u

ChÃºng tÃ´i trÃ¬nh bÃ y mÃ´-Ä‘un nÃ´ng-sÃ¢u hiá»‡u quáº£,
chiáº¿n lÆ°á»£c phÃ¢n bá»• má»™t sá»‘ lÆ°á»£ng lá»›p sá»›m Ä‘Æ°á»£c
Ä‘á»‹nh trÆ°á»›c (L_S) lÃ m mÃ´ hÃ¬nh nÃ´ng, trong khi
táº¥t cáº£ cÃ¡c lá»›p lÃ m mÃ´ hÃ¬nh sÃ¢u. MÃ´-Ä‘un nÃ y
giáº£i quyáº¿t suy giáº£m hiá»‡u suáº¥t liÃªn quan Ä‘áº¿n

[HÃ¬nh 4: Tá»•ng quan vá» giáº£i mÃ£ Ä‘á»“ng bá»™ song song. ChÃºng tÃ´i tÃ´ mÃ u cÃ¡c token Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o token tiáº¿p theo dá»±a trÃªn mÃ´ hÃ¬nh mÃ  chÃºng chuyá»ƒn tiáº¿p.]

Ä‘á»“ng Ä‘Ã o táº¡o nhiá»u lá»›p thoÃ¡t trong khung
thoÃ¡t sá»›m thÃ´ng thÆ°á»ng.

Äá»ƒ nÃ¢ng cao hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh nÃ´ng, chÃºng tÃ´i
khai thÃ¡c knowledge distillation (KD) theo lá»›p
nhÆ° má»™t sá»‘ háº¡ng máº¥t mÃ¡t bá»• sung cho Eq. (1)
vá»›i Î±_{L_s} = L_s/(L+L_s) vÃ  Î±_L = L/(L+L_s):

L_{KD} = 1/|L_S| âˆ‘^{L_S}_{i=1} MSE(H^i_S, H^{m(i)}_D),

trong Ä‘Ã³ m(i) chá»‰ ra lá»›p trong mÃ´ hÃ¬nh sÃ¢u
trÃ­ch xuáº¥t kiáº¿n thá»©c vÃ o lá»›p tÆ°Æ¡ng á»©ng i
cá»§a mÃ´ hÃ¬nh nÃ´ng. H_S vÃ  H_D lÃ  tráº¡ng thÃ¡i áº©n
tá»« mÃ´ hÃ¬nh nÃ´ng vÃ  sÃ¢u.

ChÃºng tÃ´i Ä‘Ã£ thÃ­ nghiá»‡m vá»›i chÆ°ng cáº¥t tá»« lá»›p cuá»‘i
(KD-last; Wang et al. 2020; Ko et al. 2023),
tá»« cÃ¡c lá»›p Ã¡nh xáº¡ Ä‘á»“ng nháº¥t cá»‘ Ä‘á»‹nh (KD-unif;
Jiao et al. 2020; Park et al. 2021), vÃ  tá»«
cÃ¡c lá»›p Ã¡nh xáº¡ Ä‘á»™ng (KD-dyna; Xia et al. 2022).
Äáº·c biá»‡t, hÃ m Ã¡nh xáº¡ Ä‘á»™ng cho phÃ©p chÃºng ta
cÄƒn chá»‰nh má»—i lá»›p mÃ´ hÃ¬nh sÃ¢u vá»›i Ä‘á»‘i tÃ¡c
gáº§n nháº¥t trong mÃ´ hÃ¬nh nÃ´ng:

m(i) = arg min_j MSE(H^i_S, H^j_D)

trong Ä‘Ã³ j biá»ƒu thá»‹ chá»‰ sá»‘ lá»›p cá»§a mÃ´ hÃ¬nh sÃ¢u
Ä‘Æ°á»£c chá»n bá»Ÿi tá»•ng sá»‘ L_S, vÃ  Ä‘iá»u kiá»‡n
m(1) â‰¤ Â·Â·Â· â‰¤ m(L_S) pháº£i Ä‘Æ°á»£c thá»a mÃ£n.
Dá»±a trÃªn hiá»‡u suáº¥t Æ°u viá»‡t nháº¥t quÃ¡n cá»§a
máº¥t mÃ¡t KD-dyna (xem Phá»¥ lá»¥c D.2), chÃºng tÃ´i
sá»­ dá»¥ng nÃ³ cho táº¥t cáº£ thÃ­ nghiá»‡m vá»›i mÃ´-Ä‘un nÃ´ng-sÃ¢u.

5.2 Giáº£i mÃ£ Äá»“ng bá»™ Song song

ChÃºng tÃ´i trÃ¬nh bÃ y giáº£i mÃ£ Ä‘á»“ng bá»™ song song
nhÆ° má»™t thay tháº¿ cho cÆ¡ cháº¿ sao chÃ©p tráº¡ng thÃ¡i,
lÃ  thÃ nh pháº§n chÃ­nh cá»§a khung thoÃ¡t sá»›m thÃ´ng thÆ°á»ng
nhÆ°ng cÃ³ thá»ƒ dáº«n Ä‘áº¿n suy giáº£m hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ,
nhÆ° Ä‘Æ°á»£c chá»©ng minh trong Pháº§n 4.1. TrÃ¡i ngÆ°á»£c
vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng cÃ³ nhiá»u Ä‘iá»ƒm thoÃ¡t,
phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i káº¿t há»£p mÃ´-Ä‘un nÃ´ng-sÃ¢u,
cho phÃ©p chÃºng ta xáº¿p chá»“ng cÃ¡c token thoÃ¡t sá»›m
liÃªn tiáº¿p trong mÃ´ hÃ¬nh nÃ´ng cho Ä‘áº¿n khi gáº·p
token khÃ´ng thoÃ¡t. Khi giáº£i mÃ£ token vá»›i mÃ´ hÃ¬nh sÃ¢u,
chÃºng tÃ´i nÃ¢ng cao hiá»‡u quáº£ vÃ  hiá»‡u suáº¥t thÃ´ng qua
giáº£i mÃ£ song song, tÃ­nh toÃ¡n Ä‘á»“ng bá»™ cÃ¡c tráº¡ng thÃ¡i
key vÃ  value cá»§a cÃ¡c token Ä‘Ã£ xáº¿p chá»“ng trÆ°á»›c Ä‘Ã³.
VÃ­ dá»¥ vá» quÃ¡ trÃ¬nh giáº£i mÃ£ song song Ä‘Æ°á»£c mÃ´ táº£
trong HÃ¬nh 4.

NguyÃªn lÃ½ cÆ¡ báº£n cá»§a phÆ°Æ¡ng phÃ¡p nÃ y lÃ  táº­n dá»¥ng
kháº£ nÄƒng song song tÄƒng cÆ°á»ng Ä‘Æ°á»£c cung cáº¥p bá»Ÿi
cÃ¡c bá»™ gia tá»‘c pháº§n cá»©ng hiá»‡n Ä‘áº¡i. Äiá»u nÃ y cho phÃ©p
cÃ¡c phÃ©p tÃ­nh hiá»‡u quáº£ Ä‘Æ°á»£c thá»±c hiá»‡n Ä‘á»“ng thá»i
trÃªn sá»‘ lÆ°á»£ng lá»›n chuá»—i. Do Ä‘Ã³, báº±ng cÃ¡ch sá»­ dá»¥ng
giáº£i mÃ£ Ä‘á»“ng bá»™ song song, chÃºng ta cÃ³ thá»ƒ tÃ­nh toÃ¡n
trá»±c tiáº¿p nhiá»u tráº¡ng thÃ¡i áº©n tÆ°Æ¡ng tá»± nhÆ° thá»i gian
xá»­ lÃ½ token Ä‘Æ¡n. BÃªn cáº¡nh Ä‘Ã³, Ä‘iá»u nÃ y cÃ³ thá»ƒ loáº¡i bá»
suy giáº£m hiá»‡u suáº¥t tiá»m nÄƒng cÃ³ thá»ƒ phÃ¡t sinh tá»«
xáº¥p xá»‰ khÃ´ng chÃ­nh xÃ¡c cá»§a tráº¡ng thÃ¡i áº©n do
cÆ¡ cháº¿ sao chÃ©p tráº¡ng thÃ¡i.

5.3 Æ¯á»›c tÃ­nh NgÆ°á»¡ng ThÃ­ch á»©ng

ChÃºng tÃ´i Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p Æ°á»›c tÃ­nh ngÆ°á»¡ng
thÃ­ch á»©ng má»›i cáº­p nháº­t ngÆ°á»¡ng Ä‘á»ƒ Ä‘Æ°á»£c giá»¯ láº¡i
cho cÃ¡c táº­p dá»¯ liá»‡u khÃ¡c nhau. KhÃ´ng giá»‘ng nhÆ°
cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y sá»­ dá»¥ng táº­p hiá»‡u chuáº©n
bá»• sung (Schuster et al., 2022), chÃºng tÃ´i nhanh chÃ³ng
thÃ­ch á»©ng ngÆ°á»¡ng báº±ng cÃ¡ch sá»­ dá»¥ng thÃ´ng tin
cá»§a cÃ¡c instance giai Ä‘oáº¡n Ä‘áº§u, báº¥t ká»ƒ giÃ¡ trá»‹
ngÆ°á»¡ng ban Ä‘áº§u. Äáº·c biá»‡t, trong quÃ¡ trÃ¬nh giáº£i mÃ£
song song, chÃºng tÃ´i thu tháº­p máº«u Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
sá»± tÆ°Æ¡ng á»©ng giá»¯a Ä‘iá»ƒm tin cáº­y cá»§a mÃ´ hÃ¬nh nÃ´ng
vÃ  sá»± phÃ¹ há»£p dá»± Ä‘oÃ¡n giá»¯a mÃ´ hÃ¬nh nÃ´ng vÃ  sÃ¢u.

NhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong HÃ¬nh 1b, chÃºng tÃ´i quan sÃ¡t
ráº±ng khi dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh sÃ¢u vÃ  nÃ´ng giá»‘ng há»‡t,
tin cáº­y cÃ³ xu hÆ°á»›ng nghiÃªng vá» má»™t, ngÆ°á»£c láº¡i
nÃ³ nghiÃªng vá» khÃ´ng. Äá»ƒ mÃ´ hÃ¬nh hÃ³a phÃ¢n phá»‘i
nghiÃªng nÃ y trÃªn [0,1], chÃºng tÃ´i sá»­ dá»¥ng mÃ´ hÃ¬nh
há»—n há»£p beta (BMM; Ma and Leijon 2011) do tÃ­nh
linh hoáº¡t vÃ  táº­p há»— trá»£ phÃ¹ há»£p cá»§a phÃ¢n phá»‘i beta.
HÃ m máº­t Ä‘á»™ xÃ¡c suáº¥t cá»§a phÃ¢n phá»‘i beta trÃªn x âˆˆ [0,1]
Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ :

p(x|Î±, Î²) = Î“(Î±+Î²)/(Î“(Î±)Î“(Î²)) x^{Î±-1}(1-x)^{Î²-1}

CÃ¡c tham sá»‘ cá»§a BMM Ä‘Æ°á»£c cáº­p nháº­t báº±ng cÃ¡ch sá»­ dá»¥ng
bá»™ Æ°á»›c tÃ­nh maximum likelihood (MLE; Norden 1972)
vá»›i cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u quan sÃ¡t Ä‘Æ°á»£c.

Î±_k = Ì„c_k/( Ì„c_k(1- Ì„c_k)/s^2_k - 1), Î²_k = Î±_k(1- Ì„c_k)/ Ì„c_k, (2)

trong Ä‘Ã³ Ì„c_k lÃ  trung bÃ¬nh cá»§a tin cáº­y {c^{L_s}_i}^{|D_c|}_{i=1}
cho k tÆ°Æ¡ng á»©ng. k Ä‘Æ°á»£c Ä‘áº·t lÃ  1 náº¿u dá»± Ä‘oÃ¡n
cá»§a hai mÃ´ hÃ¬nh giá»‘ng há»‡t, vÃ  0

Thuáº­t toÃ¡n 1 Æ¯á»›c tÃ­nh NgÆ°á»¡ng ThÃ­ch á»©ng
Input : táº­p dá»¯ liá»‡u hiá»‡u chuáº©n trá»‘ng D_c, ngÆ°á»¡ng
tin cáº­y ban Ä‘áº§u Î»^0_c, Ä‘iá»u kiá»‡n posterior Î¶, sá»‘
cáº­p nháº­t T
Output : ngÆ°á»¡ng tin cáº­y Ä‘Æ°á»£c cáº­p nháº­t Î»_c
1: khá»Ÿi táº¡o t â† 0, Î»_c â† Î»^0_c
2: while t â‰¤ T do
3:   Táº¡o cÃ¢u thá»© t vá»›i N_t token
4:   /* Cáº­p nháº­t D_c*/
5:   D_c â† D_c âˆª {c^{L_s}_i, I( Ì‚y^{L_s}_i =  Ì‚y^L_i)}^{N_t}_{i=1}
6:   /* TÃ¬m ngÆ°á»¡ng vá»›i Eq.(2)-(4)*/
7:   Î±_k, Î²_k â† MLE BMM(D_c) for k âˆˆ {0,1}
8:   Î»_c â† arg min_{Î»:p(k=1|Î»)â‰¥Î¶} Î»
9:   cáº­p nháº­t t â† t + 1
10: end while

ngÆ°á»£c láº¡i. TÆ°Æ¡ng tá»±, s_k lÃ  Ä‘á»™ lá»‡ch chuáº©n cá»§a
tin cáº­y cá»§a k liÃªn quan.

Ì„c_k = (âˆ‘^N_{i=1} Î³_i c^{L_s}_i)/(âˆ‘^N_{i=1} Î³_i), Ì„s^2_k = (âˆ‘^N_{i=1} Î³_i(c^{L_s}_i - Ì„c_k)^2)/(âˆ‘^N_{i=1} Î³_i), (3)

trong Ä‘Ã³ Î³_i := I( Ì‚y^{L_s}_i =  Ì‚y^L_i) biá»ƒu thá»‹ liá»‡u
dá»± Ä‘oÃ¡n cá»§a hai mÃ´ hÃ¬nh cÃ³ giá»‘ng nhau hay khÃ´ng.

Sau khi cáº­p nháº­t BMM, chÃºng tÃ´i tÃ¬m ngÆ°á»¡ng phÃ¹ há»£p
cho cÃ¡c token tÆ°Æ¡ng lai báº±ng cÃ¡ch xÃ¡c Ä‘á»‹nh Ä‘iá»ƒm
táº¡i Ä‘Ã³ xÃ¡c suáº¥t posterior, Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a dÆ°á»›i Ä‘Ã¢y,
Ä‘áº¡t Î¶:

p(k=1|Î»_c) = p(k=1)p(Î»_c|Î±_1,Î²_1)/(âˆ‘_{jâˆˆ{0,1}} p(k=j)p(Î»_c|Î±_j,Î²_j)). (4)

á» Ä‘Ã¢y, vÃ¬ chÃºng tÃ´i quan sÃ¡t sá»± máº¥t cÃ¢n báº±ng nghiÃªm trá»ng
giá»¯a trÆ°á»ng há»£p k=0 vÃ  1, chÃºng tÃ´i háº¡n cháº¿ giÃ¡ trá»‹
prior cá»§a má»—i lá»›p lÃ  0.5 Ä‘á»ƒ cÃ¢n báº±ng giá»¯a hai
trÆ°á»ng há»£p (tá»©c lÃ , p(k=j) = 0.5 âˆ€j). VÃ¬ háº¡n cháº¿
nÃ y khiáº¿n chÃºng ta sá»­ dá»¥ng giÃ¡ trá»‹ Î¶ nhá» hÆ¡n,
chÃºng tÃ´i naÃ¯vely Ä‘áº·t nÃ³ lÃ  0.4. Thuáº­t toÃ¡n chi tiáº¿t
cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong Thuáº­t toÃ¡n 1.

6 ThÃ­ nghiá»‡m

6.1 Thiáº¿t láº­p ThÃ­ nghiá»‡m

ChÃºng tÃ´i thá»±c hiá»‡n thÃ­ nghiá»‡m trÃªn cÃ¡c nhiá»‡m vá»¥
mÃ´ hÃ¬nh hÃ³a chuá»—i khÃ¡c nhau, bao gá»“m tráº£ lá»i cÃ¢u há»i
(SQuAD; Rajpurkar et al. 2016b), dá»‹ch mÃ¡y
(IWSLT 2017 En-De; Cettolo et al. 2017b),
vÃ  cÃ¡c nhiá»‡m vá»¥ tÃ³m táº¯t vÄƒn báº£n sá»­ dá»¥ng
SAMSum, CNN/DailyMail, Multi-News, vÃ 
BIGPATENT datasets. MÃ´ hÃ¬nh LongT5-base
Ä‘Æ°á»£c sá»­ dá»¥ng cho táº­p dá»¯ liá»‡u Multi-News vÃ 
BIGPATENT, trong khi mÃ´ hÃ¬nh T5-large Ä‘Æ°á»£c
sá»­ dá»¥ng cho cÃ¡c táº­p dá»¯ liá»‡u khÃ¡c. Táº¥t cáº£ triá»ƒn khai
dá»±a trÃªn PyTorch sá»­ dá»¥ng Huggingface (Wolf et al.,
2020; Lhoest et al., 2021). Chi tiáº¿t thÃªm cÃ³ thá»ƒ
Ä‘Æ°á»£c tÃ¬m tháº¥y trong Phá»¥ lá»¥c B.

--- TRANG 7 ---
[HÃ¬nh 5: Sá»± cÃ¢n báº±ng giá»¯a cháº¥t lÆ°á»£ng Ä‘áº§u ra Ä‘Æ°á»£c táº¡o vÃ  Ä‘á»™ trá»… Ä‘Æ°á»£c chuáº©n hÃ³a dÆ°á»›i cÃ¡c Ä‘iá»u kiá»‡n thoÃ¡t khÃ¡c nhau. ChÃºng tÃ´i thay Ä‘á»•i giÃ¡ trá»‹ ngÆ°á»¡ng thoÃ¡t giá»¯a 0 vÃ  1 cho cáº£ CALM vÃ  FREEâ€  vÃ  sá»‘ lÆ°á»£ng lá»›p thoÃ¡t cho khung static-exiting. ChÃºng tÃ´i loáº¡i trá»« Ä‘iá»ƒm bÃªn trong cá»§a Ä‘Æ°á»ng cong Pareto, vÃ  Ä‘Æ°á»ng Ä‘á»©t nÃ©t Ä‘áº¡i diá»‡n cho Ä‘iá»ƒm ROUGE-L cá»§a mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§, lÃ  mÃ´-Ä‘un nÃ´ng-sÃ¢u Ä‘Æ°á»£c tinh chá»‰nh.]

Báº£ng 3: So sÃ¡nh giá»¯a cÃ¡c khung thoÃ¡t sá»›m trÃªn cÃ¡c táº­p dá»¯ liá»‡u khÃ¡c nhau. Äá»‘i vá»›i CALM vÃ  FREEâ€ , chÃºng tÃ´i bÃ¡o cÃ¡o hiá»‡u suáº¥t sá»­ dá»¥ng giÃ¡ trá»‹ ngÆ°á»¡ng nhá» nháº¥t Ä‘áº¡t Ä‘Æ°á»£c 99% hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§, Ä‘Æ°á»£c tinh chá»‰nh bá»Ÿi máº¥t mÃ¡t trung bÃ¬nh cÃ³ trá»ng sá»‘ hoáº·c KD-dyna, tÆ°Æ¡ng á»©ng. Dáº¥u ngoáº·c Ä‘Æ¡n biá»ƒu thá»‹ tÄƒng tá»‘c tÆ°Æ¡ng Ä‘á»‘i dá»±a trÃªn hÃ ng Ä‘áº§u tiÃªn.

SUM                                    QA         MT
Method      SAMSum         CNN/DailyMail    Multi-News      BIGPATENT      SQuAD        IWSLT De-En
Full Model  48.82 (Ã—1.00)  41.15 (Ã—1.00)    37.62 (Ã—1.00)   49.68 (Ã—1.00)  90.63 (Ã—1.00) 39.19 (Ã—1.00)
CALM        48.37 (Ã—0.72)  40.78 (Ã—0.86)    37.27 (Ã—0.85)   49.21 (Ã—0.65)  90.09 (Ã—2.03) 39.19 (Ã—1.00)
Full Model  49.11 (Ã—1.00)  41.09 (Ã—1.00)    39.20 (Ã—1.00)   49.68 (Ã—1.00)  91.90 (Ã—1.00) 39.39 (Ã—1.00)
FREEâ€        48.65 (Ã—1.50)  40.89 (Ã—1.80)    38.93 (Ã—1.07)   49.51 (Ã—1.62)  91.31 (Ã—2.76) 39.04 (Ã—1.07)
FREE        48.66 (Ã—1.47)  40.99 (Ã—1.65)    38.66 (Ã—1.23)   49.47 (Ã—1.58)  91.82 (Ã—2.16) 38.17 (Ã—1.18)

6.2 Káº¿t quáº£ ThÃ­ nghiá»‡m

Äá»ƒ Ä‘iá»u tra tÃ¡c Ä‘á»™ng cá»§a tá»«ng thÃ nh pháº§n riÃªng láº»
cá»§a khung Ä‘á» xuáº¥t, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cáº£ FREE
khÃ´ng cÃ³ vÃ  cÃ³ bá»™ Æ°á»›c tÃ­nh ngÆ°á»¡ng thÃ­ch á»©ng,
Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  FREEâ€  vÃ  FREE.

Hiá»‡u suáº¥t tá»•ng thá»ƒ. Trong HÃ¬nh 5, chÃºng tÃ´i trÃ¬nh bÃ y
so sÃ¡nh cháº¥t lÆ°á»£ng Ä‘áº§u ra Ä‘Æ°á»£c táº¡o (ROUGE-L)
vÃ  Ä‘á»™ trá»… suy luáº­n giá»¯a khung FREE vÃ  cÃ¡c baseline,
bao gá»“m static-exiting vÃ  phÆ°Æ¡ng phÃ¡p thoÃ¡t sá»›m
thÃ´ng thÆ°á»ng (CALM; Schuster et al. 2022). PhÆ°Æ¡ng phÃ¡p
CALM thá»ƒ hiá»‡n hiá»‡u suáº¥t kÃ©m hÆ¡n so vá»›i phÆ°Æ¡ng phÃ¡p
static-exiting Ä‘Æ¡n giáº£n trÃªn táº¥t cáº£ táº­p dá»¯ liá»‡u,
cÃ³ thá»ƒ do cÆ¡ cháº¿ sao chÃ©p tráº¡ng thÃ¡i vÃ  sá»± hiá»‡n diá»‡n
cá»§a nhiá»u vá»‹ trÃ­ thoÃ¡t, nhÆ° quan sÃ¡t trong Pháº§n 4.
NgÆ°á»£c láº¡i, FREEâ€  thá»ƒ hiá»‡n hiá»‡u suáº¥t máº¡nh máº½
vÃ  AUC (diá»‡n tÃ­ch dÆ°á»›i Ä‘Æ°á»ng cong) lá»›n hÆ¡n
qua cÃ¡c táº­p dá»¯ liá»‡u báº±ng cÃ¡ch Ä‘iá»u chá»‰nh ngÆ°á»¡ng thoÃ¡t.

ÄÃ¡nh giÃ¡ ngÆ°á»¡ng thÃ­ch á»©ng. Trong khung thoÃ¡t sá»›m,
viá»‡c chá»n ngÆ°á»¡ng tin cáº­y phÃ¹ há»£p lÃ  quan trá»ng
Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c sá»± cÃ¢n báº±ng tá»‘t nháº¥t giá»¯a cháº¥t lÆ°á»£ng
táº¡o sinh vÃ  Ä‘á»™ trá»…. KhÃ´ng giá»‘ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p
hiá»‡u chuáº©n trÆ°á»›c Ä‘Ã¢y (Schuster et al., 2022) yÃªu cáº§u
táº­p hiá»‡u chuáº©n bá»• sung

[HÃ¬nh 6: Sá»± cÃ¢n báº±ng giá»¯a cháº¥t lÆ°á»£ng Ä‘áº§u ra Ä‘Æ°á»£c táº¡o vÃ  Ä‘á»™ trá»… Ä‘Æ°á»£c chuáº©n hÃ³a trÃªn mÃ´ hÃ¬nh T5-3B.]

vÃ  thá»i gian Ä‘Ã o táº¡o, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i
hiá»‡u quáº£ giáº£i quyáº¿t thÃ¡ch thá»©c nÃ y báº±ng cÃ¡ch táº­n dá»¥ng
sáº£n pháº©m phá»¥ cá»§a giáº£i mÃ£ song song. NhÆ° Ä‘Æ°á»£c tÃ³m táº¯t
trong Báº£ng 3, FREE vá»›i Æ°á»›c tÃ­nh ngÆ°á»¡ng thÃ­ch á»©ng
thÃ nh cÃ´ng Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ, lÃªn Ä‘áº¿n Ã—2.16,
khi báº£o tá»“n 99% hiá»‡u suáº¥t mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§. HÆ¡n ná»¯a,
trong HÃ¬nh 5, ngÆ°á»¡ng Ä‘Æ°á»£c Æ°á»›c tÃ­nh thá»ƒ hiá»‡n gáº§n nhÆ°
cáº£i thiá»‡n tá»‘c Ä‘á»™ tá»‘i Ä‘a cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c mÃ  khÃ´ng
hy sinh hiá»‡u suáº¥t, Ä‘Æ°á»£c Ä‘áº¡i diá»‡n bá»Ÿi cÃ¡c ngÃ´i sao Ä‘á».

MÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n. Gáº§n Ä‘Ã¢y, cÃ¡c nghiÃªn cá»©u khÃ¡c nhau
(Dettmers et al., 2022; Xiao et al., 2023;
Leviathan et al., 2023; Liu et al., 2023b) Ä‘Ã£

--- TRANG 8 ---
Báº£ng 4: So sÃ¡nh ROUGE-L vÃ  tÄƒng tá»‘c dá»±a trÃªn sá»‘ lÆ°á»£ng lá»›p khÃ¡c nhau cho mÃ´ hÃ¬nh nÃ´ng vÃ  ngÆ°á»¡ng tin cáº­y.

                   Threshold
Dataset  LS  0.7                0.5                0.3
SAMSum   4   48.27 (Ã—1.04)     46.95 (Ã—1.09)     44.72 (Ã—1.15)
         6   48.89 (Ã—1.32)     48.65 (Ã—1.50)     47.60 (Ã—1.80)
         8   48.74 (Ã—1.11)     47.97 (Ã—1.17)     47.09 (Ã—1.31)
         12  48.97 (Ã—1.21)     48.74 (Ã—1.28)     48.10 (Ã—1.37)
CNN/DM   4   41.03 (Ã—1.45)     40.59 (Ã—1.68)     39.88 (Ã—1.86)
         6   41.08 (Ã—1.53)     41.00 (Ã—1.69)     40.60 (Ã—2.07)
         8   41.19 (Ã—1.44)     41.15 (Ã—1.64)     40.95 (Ã—1.69)
         12  41.11 (Ã—1.33)     41.09 (Ã—1.47)     40.95 (Ã—1.55)

nháº±m tÄƒng tá»‘c Ä‘á»™ suy luáº­n cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM). Äá»ƒ xÃ¡c thá»±c kháº£ nÄƒng Ã¡p dá»¥ng cá»§a khung FREE trÃªn LLM, chÃºng tÃ´i thá»±c hiá»‡n thÃ­ nghiá»‡m sá»­ dá»¥ng mÃ´ hÃ¬nh T5-3B (Raffel et al., 2020) trÃªn táº­p dá»¯ liá»‡u SAMSum vÃ  CNN/DailyMail. Do chi phÃ­ tÃ­nh toÃ¡n Ä‘Ã¡ng ká»ƒ, chÃºng tÃ´i sá»­ dá»¥ng bá»™ chuyá»ƒn Ä‘á»•i LoRA (Hu et al., 2022), nháº¯m má»¥c tiÃªu cáº£ lá»›p self-attention vÃ  feed-forward vá»›i rank 64. HÃ¬nh 6 tÃ³m táº¯t so sÃ¡nh toÃ n diá»‡n cÃ¡c phÆ°Æ¡ng phÃ¡p thoÃ¡t sá»›m. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i duy trÃ¬ sá»± Æ°u viá»‡t so vá»›i cÃ¡c baseline vá» Ä‘á»™ trá»… vÃ  Ä‘iá»ƒm ROUGE-L, cho tháº¥y xu hÆ°á»›ng hiá»‡u suáº¥t nháº¥t quÃ¡n Ä‘Æ°á»£c quan sÃ¡t trong mÃ´ hÃ¬nh T5-large. Do Ä‘Ã³, chÃºng tÃ´i tin tÆ°á»Ÿng ráº±ng khung Ä‘á» xuáº¥t sáº½ thá»ƒ hiá»‡n má»©c Ä‘á»™ gia tá»‘c suy luáº­n nháº¥t quÃ¡n, ngay cáº£ vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n hÆ¡n.

6.3 NghiÃªn cá»©u Ablation

Äá»™ sÃ¢u khÃ¡c nhau cá»§a mÃ´ hÃ¬nh nÃ´ng. Trong Báº£ng 4, chÃºng tÃ´i cÅ©ng ablate vá» sá»‘ lÆ°á»£ng lá»›p cho mÃ´ hÃ¬nh nÃ´ng Ä‘á»ƒ quan sÃ¡t cÃ¡c sá»± cÃ¢n báº±ng. Trong khi phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i thá»ƒ hiá»‡n xu hÆ°á»›ng tÄƒng tá»‘c cao hÆ¡n khi Ä‘á»™ sÃ¢u cá»§a mÃ´ hÃ¬nh nÃ´ng giáº£m, chÃºng tÃ´i tráº£i qua má»™t sá»‘ giáº£m hiá»‡u suáº¥t vÃ  tÄƒng tá»‘c khi Ä‘á»™ sÃ¢u cá»§a mÃ´ hÃ¬nh giáº£m quÃ¡ nhiá»u (vÃ­ dá»¥, bá»‘n lá»›p). ChÃºng tÃ´i giáº£ Ä‘á»‹nh ráº±ng Ä‘iá»u nÃ y do cÃ¡c cÃ¢u Ä‘áº§u ra khÃ´ng chÃ­nh xÃ¡c vÃ  dÆ° thá»«a, tÆ°Æ¡ng tá»± Ä‘Æ°á»£c quan sÃ¡t trong khung thoÃ¡t sá»›m thÃ´ng thÆ°á»ng. Do Ä‘Ã³, vá»›i Ä‘á»™ sÃ¢u Ä‘á»§ (vÃ­ dá»¥, sÃ¡u lá»›p), FREE nháº¥t quÃ¡n cho tháº¥y hiá»‡u suáº¥t máº¡nh máº½ vÃ  tÄƒng tá»‘c suy luáº­n.

TÃ­nh máº¡nh máº½ cá»§a giáº£i mÃ£ song song. Äá»ƒ xÃ¡c minh tÃ­nh máº¡nh máº½ cá»§a cÆ¡ cháº¿ giáº£i mÃ£ cá»§a chÃºng tÃ´i, chÃºng tÃ´i thá»±c hiá»‡n phÃ¢n tÃ­ch so sÃ¡nh giá»¯a giáº£i mÃ£ Ä‘á»“ng bá»™ song song (SPD) vÃ  sao chÃ©p tráº¡ng thÃ¡i (SC), cáº£ hai Ä‘á»u Ä‘Æ°á»£c triá»ƒn khai vá»›i mÃ´-Ä‘un nÃ´ng-sÃ¢u

Báº£ng 5: So sÃ¡nh giá»¯a giáº£i mÃ£ Ä‘á»“ng bá»™ song song (SPC) vÃ  sao chÃ©p tráº¡ng thÃ¡i (SC). MÃ´-Ä‘un nÃ´ng-sÃ¢u Ä‘Æ°á»£c sá»­ dá»¥ng trong cáº£ hai phÆ°Æ¡ng phÃ¡p giáº£i mÃ£.

Method    Threshold
Dataset  SC  SPD  0.9    0.7    0.5    0.3    0.1
SAMSum   âœ“   âœ—    46.35  44.59  43.92  42.36  41.27
         âœ—   âœ“    48.89  48.89  48.65  47.60  45.27
CNN/DM   âœ“   âœ—    40.92  40.92  40.71  39.99  38.17
         âœ—   âœ“    41.12  41.08  41.00  40.60  39.30
Multi-News âœ“   âœ—    38.43  37.61  36.55  33.99  29.34
         âœ—   âœ“    39.16  39.06  38.78  37.87  33.98

Báº£ng 6: Káº¿t quáº£ thÃ­ nghiá»‡m cá»§a khung FREE dá»±a trÃªn kÃ­ch thÆ°á»›c khÃ¡c nhau cá»§a táº­p hiá»‡u chuáº©n.

         3%              10%             100%
Dataset  Thr. Perf. Speed Thr. Perf. Speed Thr.
SAMSum   0.51 48.66 Ã—1.47 0.49 48.69 Ã—1.51 0.48
BIGPATENT 0.54 49.47 Ã—1.58 0.54 49.39 Ã—1.63 0.54

mÃ´-Ä‘un. Giáº£i mÃ£ Ä‘á»“ng bá»™ song song nháº¥t quÃ¡n vÆ°á»£t trá»™i hÆ¡n sao chÃ©p tráº¡ng thÃ¡i qua táº¥t cáº£ ba táº­p dá»¯ liá»‡u vá»›i cÃ¡c chá»‰ sá»‘ ROUGE-L cao hÆ¡n nhiá»u, nhÆ° Ä‘Æ°á»£c tÃ³m táº¯t trong Báº£ng 5. Cáº£i thiá»‡n nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c quy cho cÃ¡c tráº¡ng thÃ¡i áº©n Ä‘Æ°á»£c cáº­p nháº­t thu Ä‘Æ°á»£c thÃ´ng qua tÃ­nh toÃ¡n chÃ­nh xÃ¡c cá»§a cÃ¡c lá»›p Transformer trong giáº£i mÃ£ song song. Nhá»¯ng phÃ¡t hiá»‡n nÃ y cho tháº¥y ráº±ng phÆ°Æ¡ng phÃ¡p giáº£i mÃ£ hiá»‡u quáº£ cá»§a chÃºng tÃ´i cho cÃ¡c token thoÃ¡t sá»›m cÃ³ thá»ƒ nÃ¢ng cao hiá»‡u suáº¥t tá»•ng thá»ƒ cá»§a khung thoÃ¡t sá»›m.

Phá»¥ thuá»™c vÃ o kÃ­ch thÆ°á»›c táº­p hiá»‡u chuáº©n. Báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c instance giai Ä‘oáº¡n Ä‘áº§u lÃ m táº­p hiá»‡u chuáº©n, chÃºng tÃ´i cáº­p nháº­t láº·p láº¡i ngÆ°á»¡ng tin cáº­y thÃ­ch á»©ng Ä‘á»ƒ há»™i tá»¥ vá» giÃ¡ trá»‹ phÃ¹ há»£p. á» Ä‘Ã¢y, chÃºng tÃ´i Ä‘Ã£ quan sÃ¡t hiá»‡u quáº£ máº«u cá»§a bá»™ Æ°á»›c tÃ­nh ngÆ°á»¡ng thÃ­ch á»©ng báº±ng cÃ¡ch thay Ä‘á»•i kÃ­ch thÆ°á»›c cá»§a táº­p hiá»‡u chuáº©n nÃ y. ThÃº vá»‹ lÃ , ngay cáº£ chá»‰ vá»›i 3% tá»•ng sá»‘ máº«u, bá»™ Æ°á»›c tÃ­nh cá»§a chÃºng tÃ´i cÃ³ thá»ƒ xáº¥p xá»‰ ngÆ°á»¡ng, Ä‘Æ°á»£c Ä‘o bá»Ÿi táº­p máº«u Ä‘áº§y Ä‘á»§, nhÆ° Ä‘Æ°á»£c thá»ƒ hiá»‡n trong Báº£ng 6. Äiá»u nÃ y Ä‘áº£m báº£o thá»i gian tÃ­nh toÃ¡n bá»• sung tá»‘i thiá»ƒu cáº§n thiáº¿t cho Æ°á»›c tÃ­nh ngÆ°á»¡ng.

Tinh chá»‰nh dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh nÃ´ng. CÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y (Leviathan et al., 2023; Chen et al., 2023; Kim et al., 2023) Ä‘Ã£ Ä‘á» xuáº¥t cÃ¡c phÆ°Æ¡ng phÃ¡p tinh chá»‰nh Ä‘á»ƒ sá»­a chá»¯a Ä‘áº§u ra sai tá»« mÃ´ hÃ¬nh xáº¥p xá»‰. Cá»¥ thá»ƒ, khi token sai Ä‘Æ°á»£c phÃ¡t hiá»‡n trong cÃ¡c chuá»—i trÆ°á»›c Ä‘Ã³, há» loáº¡i bá» táº¥t cáº£ cÃ¡c token Ä‘Æ°á»£c táº¡o sau Ä‘Ã³ vÃ  khá»Ÿi Ä‘á»™ng láº¡i quÃ¡ trÃ¬nh táº¡o sinh tá»« Ä‘iá»ƒm Ä‘Ã³. Trong Báº£ng 7, chÃºng tÃ´i thá»±c hiá»‡n thÃ­ nghiá»‡m Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a phÆ°Æ¡ng phÃ¡p tinh chá»‰nh nÃ y (Kim et al., 2023)

--- TRANG 9 ---
Báº£ng 7: ÄÃ¡nh giÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p tinh chá»‰nh trong khung FREE. NgÆ°á»¡ng tinh chá»‰nh kiá»ƒm soÃ¡t má»©c Ä‘á»™ cháº¥p nháº­n cho dá»± Ä‘oÃ¡n tá»« mÃ´ hÃ¬nh nÃ´ng.

         Thr. 0.7           Thr. 0.3
Dataset  Ref. Thr. Perf. Speed Perf. Speed
SAMSum   âœ—    -    48.89 Ã—1.33 47.60 Ã—1.80
         âœ“    1.0  49.08 Ã—1.26 48.31 Ã—1.50
         âœ“    0.1  49.06 Ã—1.17 48.27 Ã—1.12
CNN/DM   âœ—    -    41.08 Ã—1.53 40.60 Ã—2.07
         âœ“    1.0  40.86 Ã—1.51 40.78 Ã—1.67
         âœ“    0.1  40.85 Ã—1.35 40.75 Ã—1.21

trong khung thoÃ¡t sá»›m cá»§a chÃºng tÃ´i. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng khi ngÆ°á»¡ng tinh chá»‰nh Ä‘Æ°á»£c Ä‘áº·t tháº¥p, cho phÃ©p nhiá»u sá»­a chá»¯a hÆ¡n bá»Ÿi mÃ´ hÃ¬nh sÃ¢u, cáº£i thiá»‡n hiá»‡u suáº¥t lÃ  tá»‘i thiá»ƒu so vá»›i sá»± tÄƒng Ä‘á»™ trá»…. CÃ¡c phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i cho tháº¥y ráº±ng nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y khÃ´ng thá»ƒ Ä‘áº£m báº£o giá»›i háº¡n trÃªn cho sá»± tÄƒng Ä‘á»™ trá»… cÃ³ thá»ƒ khÃ´ng phÃ¹ há»£p Ä‘á»ƒ tÃ­ch há»£p vÃ o khung thoÃ¡t sá»›m.

ÄÃ¡nh giÃ¡ tÃ³m táº¯t giá»‘ng con ngÆ°á»i. CÃ¡c nghiÃªn cá»©u gáº§n Ä‘Ã¢y (Gao et al., 2023; Liu et al., 2023a; Zhang et al., 2023) Ä‘Ã£ láº­p luáº­n ráº±ng cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ tÃ³m táº¯t hiá»‡n táº¡i nhÆ° ROUGE-L khÃ´ng Ä‘áº¡i diá»‡n chÃ­nh xÃ¡c cho kháº£ nÄƒng tÃ³m táº¯t thá»±c sá»±. Thay vÃ o Ä‘Ã³, há» khÃ¡m phÃ¡ Ä‘Ã¡nh giÃ¡ giá»‘ng con ngÆ°á»i sá»­ dá»¥ng LLM dá»±a trÃªn má»‘i tÆ°Æ¡ng quan máº¡nh máº½ vá»›i phÃ¡n Ä‘oÃ¡n cá»§a con ngÆ°á»i. Do Ä‘Ã³, chÃºng tÃ´i thá»±c hiá»‡n hai phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ giá»‘ng con ngÆ°á»i, cháº¥m Ä‘iá»ƒm thang Likert vÃ  so sÃ¡nh theo cáº·p (Gao et al., 2023), sá»­ dá»¥ng ChatGPT API (gpt-3.5-turbo-0613). ChÃºng tÃ´i so sÃ¡nh mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§ vÃ  khung FREE cá»§a chÃºng tÃ´i trÃªn 100 instance, Ä‘Æ°á»£c rÃºt ngáº«u nhiÃªn tá»« táº­p dá»¯ liá»‡u CNN/DailyMail. HÃ¬nh 7 vÃ  8 cung cáº¥p cÃ¡c máº«u Ä‘Æ°á»£c sá»­ dá»¥ng cho tá»«ng nhiá»‡m vá»¥ Ä‘Ã¡nh giÃ¡. Äá»‘i vá»›i mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§, chÃºng tÃ´i quan sÃ¡t Ä‘iá»ƒm [4.73, 3.83, 3.87, 3.77], trong khi phÆ°Æ¡ng phÃ¡p FREE cá»§a chÃºng tÃ´i tráº£ vá» Ä‘iá»ƒm [4.68, 3.84, 3.84, 3.72] qua bá»‘n chiá»u. BÃªn cáº¡nh Ä‘Ã³, sá»‘ láº§n tháº¯ng cho má»—i phÆ°Æ¡ng phÃ¡p lÃ  101 vÃ  99, tÆ°Æ¡ng á»©ng. Vá»›i Ä‘iá»ƒm ROUGE-L 41.09 (Ã—1.00) cho mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§ vÃ  40.99 (Ã—1.65) cho phÆ°Æ¡ng phÃ¡p FREE, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cháº¯c cháº¯n cÃ³ kháº£ nÄƒng táº¡o ra dá»± Ä‘oÃ¡n cháº¥t lÆ°á»£ng tÆ°Æ¡ng tá»±, Ä‘á»“ng thá»i giáº£m Ä‘Ã¡ng ká»ƒ chi phÃ­ tÃ­nh toÃ¡n.

7 Káº¿t luáº­n

ChÃºng tÃ´i Ä‘á» xuáº¥t khung FREE Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c cá»§a khung thoÃ¡t sá»›m thÃ´ng thÆ°á»ng cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± há»“i quy. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i káº¿t há»£p ba thÃ nh pháº§n chÃ­nh: (1) mÃ´-Ä‘un nÃ´ng-sÃ¢u, (2) giáº£i mÃ£ Ä‘á»“ng bá»™ song song,

ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng tÃ³m táº¯t Ä‘Æ°á»£c viáº¿t cho má»™t bÃ i bÃ¡o tin tá»©c. Cháº¥m Ä‘iá»ƒm má»—i tÃ³m táº¯t trÃªn bá»‘n chiá»u: {Dimension_1}, {Dimension_2}, {Dimension_3}, vÃ  {Dimension_4}. Báº¡n nÃªn cháº¥m Ä‘iá»ƒm trÃªn thang tá»« 1 (tá»‡ nháº¥t) Ä‘áº¿n 5 (tá»‘t nháº¥t).

BÃ i bÃ¡o: {Article}
TÃ³m táº¯t: {Summary}

HÃ¬nh 7: Máº«u cho cháº¥m Ä‘iá»ƒm thang Likert. Bá»‘n chiá»u lÃ  liÃªn quan, thÃ´ng tin, trÃ´i cháº£y, vÃ  máº¡ch láº¡c.

Cho má»™t bÃ i bÃ¡o tin tá»©c, tÃ³m táº¯t nÃ o tá»‘t hÆ¡n? Tráº£ lá»i "Summary 0" hoáº·c "Summary 1". Báº¡n khÃ´ng cáº§n giáº£i thÃ­ch lÃ½ do.

BÃ i bÃ¡o: {Article}
TÃ³m táº¯t 0: {Summary_0}
TÃ³m táº¯t 1: {Summary_1}

HÃ¬nh 8: Máº«u cho so sÃ¡nh theo cáº·p. ChÃºng tÃ´i Ä‘o hai láº§n báº±ng cÃ¡ch thay Ä‘á»•i thá»© tá»± tÃ³m táº¯t Ä‘á»ƒ so sÃ¡nh cÃ´ng báº±ng.

vÃ  (3) Æ°á»›c tÃ­nh ngÆ°á»¡ng thÃ­ch á»©ng. ThÃ´ng qua cÃ¡c thÃ­ nghiá»‡m má»Ÿ rá»™ng trÃªn cÃ¡c nhiá»‡m vá»¥ táº¡o sinh khÃ¡c nhau, chÃºng tÃ´i chá»©ng minh thá»±c nghiá»‡m hiá»‡u suáº¥t Æ°u viá»‡t cá»§a khung FREE, Ä‘áº¡t Ä‘Æ°á»£c gia tá»‘c Ä‘Ã¡ng ká»ƒ vá» Ä‘á»™ trá»… mÃ  khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n cháº¥t lÆ°á»£ng Ä‘áº§u ra Ä‘Æ°á»£c táº¡o.

Háº¡n cháº¿. CÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i giáº£i quyáº¿t khung hiá»‡n táº¡i nhanh vÃ  máº¡nh máº½ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng hiá»‡u quáº£ mÃ  khÃ´ng lo ngáº¡i vá» suy giáº£m hiá»‡u suáº¥t. Tuy nhiÃªn, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i cÃ³ má»™t sá»‘ háº¡n cháº¿ mÃ  chÃºng tÃ´i tháº£o luáº­n dÆ°á»›i Ä‘Ã¢y: (1) PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i yÃªu cáº§u tÃ i nguyÃªn tÃ­nh toÃ¡n bá»• sung Ä‘á»ƒ tinh chá»‰nh mÃ´ hÃ¬nh nÃ´ng. Tuy nhiÃªn, nhÆ° chÃºng tÃ´i Ä‘Ã£ chá»©ng minh, cÃ¡c phÆ°Æ¡ng phÃ¡p tinh chá»‰nh hiá»‡u quáº£ tham sá»‘ sáº½ lÃ  giáº£i phÃ¡p Ä‘áº§y há»©a háº¹n Ä‘á»ƒ kháº¯c phá»¥c háº¡n cháº¿ nÃ y. (2) Trong khi cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i thá»ƒ hiá»‡n tÃ­nh máº¡nh máº½ vá» Ä‘á»™ sÃ¢u cá»§a mÃ´ hÃ¬nh nÃ´ng, cáº§n Ä‘iá»u tra thÃªm Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘á»™ sÃ¢u phÃ¹ há»£p cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ khÃ¡c nhau. KhÃ­a cáº¡nh nÃ y váº«n lÃ  lÄ©nh vá»±c cáº§n nghiÃªn cá»©u bá»• sung.

Lá»i cáº£m Æ¡n. CÃ´ng trÃ¬nh nÃ y Ä‘Æ°á»£c há»— trá»£ bá»Ÿi viá»‡n CÃ´ng nghá»‡ ThÃ´ng tin & Truyá»n thÃ´ng Káº¿ hoáº¡ch & ÄÃ¡nh giÃ¡ (IITP) tÃ i trá»£ bá»Ÿi chÃ­nh phá»§ HÃ n Quá»‘c (MSIT) [Sá»‘ 2021-0-00907, PhÃ¡t triá»ƒn CÃ´ng nghá»‡ PhÃ¢n tÃ­ch Cá»™ng tÃ¡c Edge ThÃ­ch á»©ng vÃ  Nháº¹ Ä‘á»ƒ KÃ­ch hoáº¡t Pháº£n á»©ng Ngay láº­p tá»©c Chá»§ Ä‘á»™ng vÃ  Há»c táº­p Nhanh, 90% vÃ  Sá»‘ 2019-0-00075, ChÆ°Æ¡ng trÃ¬nh TrÆ°á»ng Äáº¡i há»c TrÃ­ tuá»‡ NhÃ¢n táº¡o (KAIST), 10%].

--- TRANG 10 ---
TÃ i liá»‡u Tham kháº£o

Sangmin Bae, Sungnyun Kim, Jongwoo Ko, Gihun Lee, Seungjong Noh, vÃ  Se-Young Yun. 2021. Self-contrastive learning: Single-viewed supervised contrastive framework using sub-network. arXiv preprint arXiv:2106.15499.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877â€“1901.

Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Niehues Jan, StÃ¼ker Sebastian, Sudoh Katsuitho, Yoshino Koichiro, vÃ  Federmann Christian. 2017a. Overview of the iwslt 2017 evaluation campaign. In Proceedings of the 14th International Workshop on Spoken Language Translation, pages 2â€“14.

Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian StÃ¼ker, Katsuhito Sudoh, Koichiro Yoshino, vÃ  Christian Federmann. 2017b. Overview of the IWSLT 2017 evaluation campaign. In Proceedings of the 14th International Conference on Spoken Language Translation, pages 2â€“14, Tokyo, Japan. International Workshop on Spoken Language Translation.

Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, vÃ  John Jumper. 2023. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318.

Tim Dettmers, Mike Lewis, Younes Belkada, vÃ  Luke Zettlemoyer. 2022. Llm. int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339.

Maha Elbayad, Jiatao Gu, Edouard Grave, vÃ  Michael Auli. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.

Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, vÃ  Dragomir Radev. 2019a. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074â€“1084, Florence, Italy. Association for Computational Linguistics.

Alexander R Fabbri, Irene Li, Tianwei She, Suyi Li, vÃ  Dragomir R Radev. 2019b. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. arXiv preprint arXiv:1906.01749.

Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, vÃ  Xiaojun Wan. 2023. Human-like summarization evaluation with chatgpt. arXiv preprint arXiv:2304.02554.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, vÃ  Luke Zettlemoyer. 2019. Mask-predict: Parallel decoding of conditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 6111â€“6120. Association for Computational Linguistics.

Bogdan Gliwa, Iwona Mochol, Maciej Biesek, vÃ  Aleksander Wawer. 2019. SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70â€“79, Hong Kong, China. Association for Computational Linguistics.

Alex Graves. 2016. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983.

Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, vÃ  Richard Socher. 2018. Non-autoregressive neural machine translation. In International Conference on Learning Representations.

Jiatao Gu vÃ  Xiang Kong. 2021. Fully non-autoregressive neural machine translation: Tricks of the trade. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 120â€“133, Online. Association for Computational Linguistics.

Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, vÃ  Yinfei Yang. 2022. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724â€“736, Seattle, United States. Association for Computational Linguistics.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, vÃ  Laurent Sifre. 2022. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, vÃ  Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, vÃ  Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.

--- TRANG 11 ---
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, vÃ  Qun Liu. 2020. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163â€“4174, Online. Association for Computational Linguistics.

Sehoon Kim, Karttikeya Mangalam, Jitendra Malik, Michael W Mahoney, Amir Gholami, vÃ  Kurt Keutzer. 2023. Big little transformer decoder. arXiv preprint arXiv:2302.07863.

Jongwoo Ko, Seungjoon Park, Minchan Jeong, Sukjin Hong, Euijai Ahn, Du-Seong Chang, vÃ  Se-Young Yun. 2023. Revisiting intermediate layer distillation for compressing language models: An overfitting perspective. In Findings of the Association for Computational Linguistics: EACL 2023, pages 158â€“175, Dubrovnik, Croatia. Association for Computational Linguistics.

Yaniv Leviathan, Matan Kalman, vÃ  Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 19274â€“19286. PMLR.

Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, ClÃ©ment Delangue, ThÃ©o MatussiÃ¨re, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, FranÃ§ois Lagunas, Alexander M. Rush, vÃ  Thomas Wolf. 2021. Datasets: A community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2021, Online and Punta Cana, Dominican Republic, 7-11 November, 2021, pages 175â€“184. Association for Computational Linguistics.

Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74â€“81, Barcelona, Spain. Association for Computational Linguistics.

Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, vÃ  Chenguang Zhu. 2023a. Gpteval: nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634.

Yijin Liu, Fandong Meng, Jie Zhou, Yufeng Chen, vÃ  Jinan Xu. 2021. Faster depth-adaptive transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13424â€“13432.

Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher RÃ©, vÃ  Beidi Chen. 2023b. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 22137â€“22176. PMLR.

Zhanyu Ma vÃ  Arne Leijon. 2011. Bayesian estimation of beta mixture models with variational inference. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(11):2160â€“2173.

Ramesh Nallapati, Bowen Zhou, CÃ­cero Nogueira dos Santos, Ã‡aglar GÃ¼lÃ§ehre, vÃ  Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, pages 280â€“290. ACL.

RH Norden. 1972. A survey of maximum likelihood estimation. International Statistical Review/Revue Internationale de Statistique, pages 329â€“354.

Kishore Papineni, Salim Roukos, Todd Ward, vÃ  Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311â€“318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Geondo Park, Gyeongman Kim, vÃ  Eunho Yang. 2021. Distilling linguistic context for language model compression. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 364â€“378, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, vÃ  Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485â€“5551.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, vÃ  Percy Liang. 2016a. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.

--- TRANG 12 ---
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, vÃ  Percy Liang. 2016b. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383â€“2392, Austin, Texas. Association for Computational Linguistics.

Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, vÃ  Emanuele RodolÃ . 2023. Accelerating transformer inference for translation via parallel decoding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 12336â€“12355. Association for Computational Linguistics.

Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, vÃ  Aaron van den Oord. 2022. Step-unrolled denoising autoencoders for text generation. In International Conference on Learning Representations.

Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, vÃ  Donald Metzler. 2022. Confident adaptive language modeling. Advances in Neural Information Processing Systems, 35:17456â€“17472.

Abigail See, Peter J. Liu, vÃ  Christopher D. Manning. 2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073â€“1083, Vancouver, Canada. Association for Computational Linguistics.

Eva Sharma, Chen Li, vÃ  Lu Wang. 2019. BIGPATENT: A large-scale dataset for abstractive and coherent summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204â€“2213, Florence, Italy. Association for Computational Linguistics.

Noam Shazeer vÃ  Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596â€“4604. PMLR.

Surat Teerapittayanon, Bradley McDanel, vÃ  Hsiang-Tsung Kung. 2016. Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd International Conference on Pattern Recognition (ICPR), pages 2464â€“2469. IEEE.

Yonglong Tian, Dilip Krishnan, vÃ  Phillip Isola. 2019. Contrastive representation distillation. arXiv preprint arXiv:1910.10699.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, vÃ  Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.

Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, vÃ  Ming Zhou. 2020. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776â€“5788.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, vÃ  Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38â€“45, Online. Association for Computational Linguistics.

Mengzhou Xia, Zexuan Zhong, vÃ  Danqi Chen. 2022. Structured pruning learns compact and accurate models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1513â€“1528, Dublin, Ireland. Association for Computational Linguistics.

Guangxuan Xiao, Ji Lin, MickaÃ«l Seznec, Hao Wu, Julien Demouth, vÃ  Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 38087â€“38099. PMLR.

Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, vÃ  Kaisheng Ma. 2019a. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3713â€“3722.

Linfeng Zhang, Zhanhong Tan, Jiebo Song, Jingwei Chen, Chenglong Bao, vÃ  Kaisheng Ma. 2019b. Scan: A scalable neural networks framework towards compact and efficient models. Advances in Neural Information Processing Systems, 32.

Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, vÃ  Yongbin Li. 2023. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862.

--- TRANG 13 ---
A MÃ´ táº£ Táº­p dá»¯ liá»‡u

ChÃºng tÃ´i Ã¡p dá»¥ng FREE trÃªn cÃ¡c nhiá»‡m vá»¥ táº¡o sinh khÃ¡c nhau bao gá»“m tÃ³m táº¯t, tráº£ lá»i cÃ¢u há»i, vÃ  dá»‹ch mÃ¡y. ChÃºng tÃ´i cung cáº¥p mÃ´ táº£ chi tiáº¿t vá» cÃ¡c táº­p dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng.

â€¢ SAMSum (TÃ³m táº¯t): SAMSum (Gliwa et al., 2019) bao gá»“m 16K cuá»™c trÃ² chuyá»‡n giá»‘ng messenger Ä‘Æ°á»£c chÃº thÃ­ch vá»›i tÃ³m táº¯t Ä‘á»ƒ cung cáº¥p tá»•ng quan ngáº¯n gá»n vá» ná»™i dung cuá»™c trÃ² chuyá»‡n á»Ÿ ngÃ´i thá»© ba.

â€¢ CNN/DailyMail (TÃ³m táº¯t): CNN/DailyMail (See et al., 2017) bao gá»“m hÆ¡n 300K bÃ i bÃ¡o tin tá»©c tiáº¿ng Anh ban Ä‘áº§u Ä‘Æ°á»£c thiáº¿t káº¿ cho Ä‘á»c hiá»ƒu mÃ¡y vÃ  tráº£ lá»i cÃ¢u há»i trá»«u tÆ°á»£ng, nhÆ°ng hiá»‡n cÅ©ng há»— trá»£ tÃ³m táº¯t trÃ­ch xuáº¥t vÃ  trá»«u tÆ°á»£ng.

â€¢ Multi-News (TÃ³m táº¯t): Multi-News (Fabbri et al., 2019a) bao gá»“m 45K bÃ i bÃ¡o tin tá»©c vÃ  tÃ³m táº¯t tÆ°Æ¡ng á»©ng, nÆ¡i má»—i tÃ³m táº¯t Ä‘Æ°á»£c táº¡o chuyÃªn nghiá»‡p vÃ  cung cáº¥p liÃªn káº¿t Ä‘áº¿n cÃ¡c bÃ i bÃ¡o gá»‘c Ä‘Æ°á»£c tham chiáº¿u.

â€¢ BIGPATENT (TÃ³m táº¯t): BIGPATENT (Sharma et al., 2019) chá»©a 1.3M há»“ sÆ¡ tÃ i liá»‡u báº±ng sÃ¡ng cháº¿ Hoa Ká»³, má»—i há»“ sÆ¡ Ä‘i kÃ¨m vá»›i tÃ³m táº¯t trá»«u tÆ°á»£ng Ä‘Æ°á»£c viáº¿t bá»Ÿi con ngÆ°á»i. Trong cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i, chÃºng tÃ´i Ä‘áº·c biá»‡t táº­p trung vÃ o danh má»¥c Fixed Constructions, lÃ  má»™t trong chÃ­n danh má»¥c phÃ¢n loáº¡i cÃ³ sáºµn trong táº­p dá»¯ liá»‡u.

â€¢ SQuAD (Tráº£ lá»i CÃ¢u há»i): Stanford Question Answering (SQuAD, Rajpurkar et al. 2016b) lÃ  táº­p há»£p 87.6K nhiá»‡m vá»¥ Ä‘á»c hiá»ƒu. NÃ³ bao gá»“m cÃ¡c cÃ¢u há»i Ä‘Æ°á»£c táº¡o bá»Ÿi nhÃ¢n viÃªn Ä‘Ã¡m Ä‘Ã´ng dá»±a trÃªn táº­p há»£p cÃ¡c bÃ i bÃ¡o Wikipedia.

â€¢ IWSLT 2017 (Dá»‹ch MÃ¡y): IWSLT 2017 (Cettolo et al., 2017b) giáº£i quyáº¿t dá»‹ch vÄƒn báº£n, sá»­ dá»¥ng há»‡ thá»‘ng dá»‹ch mÃ¡y (MT) Ä‘Æ¡n cho nhiá»u hÆ°á»›ng ngÃ´n ngá»¯ nhÆ° tiáº¿ng Anh vÃ  tiáº¿ng Äá»©c. á» Ä‘Ã¢y, chÃºng tÃ´i Ä‘áº·c biá»‡t táº­p trung vÃ o nhiá»‡m vá»¥ dá»‹ch Äá»©c-sang-Anh.

B Thiáº¿t láº­p ThÃ­ nghiá»‡m Chi tiáº¿t

SiÃªu tham sá»‘ Ä‘Ã o táº¡o. Trong pháº§n nÃ y, chÃºng tÃ´i mÃ´ táº£ cÃ¡c giÃ¡ trá»‹ siÃªu tham sá»‘ chi tiáº¿t cho cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i. ChÃºng tÃ´i sá»­ dá»¥ng GPU NVIDIA RTX 3090 Ä‘á»ƒ Ä‘Ã o táº¡o cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯, vÃ  chÃºng tÃ´i tÃ³m táº¯t

Báº£ng 8: SiÃªu tham sá»‘ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a Ä‘á»ƒ Ä‘Ã o táº¡o mÃ´ hÃ¬nh T5 nÃ´ng-sÃ¢u. Cá»™t cÃ³ nhÃ£n '# Batch' chá»‰ ra tÃ­ch cá»§a kÃ­ch thÆ°á»›c batch má»—i GPU vÃ  sá»‘ lÆ°á»£ng GPU. 'In len.' vÃ  'Out len.' Ä‘áº¡i diá»‡n cho Ä‘á»™ dÃ i tá»‘i Ä‘a cá»§a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra, tÆ°Æ¡ng á»©ng.

Dataset     Model         # Batch  Epochs  In len.  Out len.
SAMSum      T5-large      4Ã—2      20      512      128
CNN/DM      T5-large      4Ã—4      3       512      128
Multi-News  LongT5-base   2Ã—2      3       2048     512
BIGPATENT   LongT5-base   2Ã—2      3       2048     512
SQuAD       T5-large      4Ã—2      10      512      30
IWSLT 2017  mT5-large     4Ã—4      2       1024     128

cáº¥u hÃ¬nh Ä‘Ã o táº¡o trong Báº£ng 8. Äá»‘i vá»›i táº¥t cáº£ táº­p dá»¯ liá»‡u, chÃºng tÃ´i sá»­ dá»¥ng bá»™ tá»‘i Æ°u AdaFactor (Shazeer vÃ  Stern, 2018) vá»›i tá»‘c Ä‘á»™ há»c 1e-4. Äá»‘i vá»›i Æ°á»›c tÃ­nh ngÆ°á»¡ng thÃ­ch á»©ng, chÃºng tÃ´i Ä‘áº·t giÃ¡ trá»‹ ngÆ°á»¡ng ban Ä‘áº§u Î»â‚€c lÃ  0.9, Î¶ lÃ  0.4, T lÃ  3% tá»•ng sá»‘ máº«u (tham kháº£o Thuáº­t toÃ¡n 1).

Chá»‰ sá»‘ hiá»‡u suáº¥t. Äá»ƒ Ä‘o lÆ°á»ng sá»‘ báº±ng cháº¥t lÆ°á»£ng Ä‘áº§u ra cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i, chÃºng tÃ´i sá»­ dá»¥ng Ä‘iá»ƒm F1 cho SQuAD, Ä‘iá»ƒm BLEU (Papineni et al., 2002) cho IWSLT2017, vÃ  Ä‘iá»ƒm ROUGE (Lin, 2004) cho bá»‘n nhiá»‡m vá»¥ tÃ³m táº¯t.

C ÄÃ¡nh giÃ¡ Äá»™ trá»… Suy luáº­n

Äá»ƒ Ä‘o tá»‘c Ä‘á»™ suy luáº­n, chÃºng tÃ´i thá»±c hiá»‡n 500 dá»± Ä‘oÃ¡n suy luáº­n cho má»—i táº­p dá»¯ liá»‡u dÆ°á»›i má»—i cáº¥u hÃ¬nh Ä‘Æ°á»£c kiá»ƒm tra trong hÃ m Ä‘Æ°á»£c biÃªn dá»‹ch PyTorch (Paszke et al., 2019) trong má»™t mÃ¡y chá»§ duy nháº¥t vá»›i GPU NVIDIA GeForce RTX 3039 Ä‘Æ¡n vÃ  CPU 12th Gen Intel(R) Core(TM) i7-12700K. Äá»‘i vá»›i má»—i dá»± Ä‘oÃ¡n suy luáº­n, chÃºng tÃ´i sá»­ dá»¥ng kÃ­ch thÆ°á»›c batch 1, lÃ  trÆ°á»ng há»£p sá»­ dá»¥ng phá»• biáº¿n cho phá»¥c vá»¥ trá»±c tuyáº¿n (Schuster et al., 2022). NgoÃ i ra, chÃºng tÃ´i sá»­ dá»¥ng Ä‘á»ƒ táº¡o chuá»—i Ä‘áº§u ra thÃ´ng qua láº¥y máº«u tham lam vá»›i kÃ­ch thÆ°á»›c beam lÃ  1. ChÃºng tÃ´i Ä‘o thá»i gian bao gá»“m táº¥t cáº£ cÃ¡c bÆ°á»›c giáº£i mÃ£ cho Ä‘áº¿n hoÃ n thÃ nh.

--- TRANG 14 ---
[HÃ¬nh 9: Sá»± cÃ¢n báº±ng giá»¯a cháº¥t lÆ°á»£ng Ä‘áº§u ra Ä‘Æ°á»£c táº¡o vÃ  Ä‘á»™ trá»… Ä‘Æ°á»£c chuáº©n hÃ³a dÆ°á»›i cÃ¡c Ä‘iá»u kiá»‡n thoÃ¡t khÃ¡c nhau. ÄÆ°á»ng Ä‘á»©t nÃ©t Ä‘áº¡i diá»‡n cho Ä‘iá»ƒm F1 vÃ  BLEU cá»§a mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§, lÃ  mÃ´-Ä‘un nÃ´ng-sÃ¢u Ä‘Æ°á»£c tinh chá»‰nh, tÆ°Æ¡ng á»©ng. TÆ°Æ¡ng tá»± nhÆ° HÃ¬nh 5, chÃºng tÃ´i loáº¡i trá»« Ä‘iá»ƒm bÃªn trong cá»§a Ä‘Æ°á»ng cong Pareto.]

[HÃ¬nh 10: Sá»± cÃ¢n báº±ng giá»¯a hiá»‡u suáº¥t vÃ  Ä‘á»™ trá»… Ä‘Æ°á»£c chuáº©n hÃ³a má»—i cÃ¢u. ChÃºng tÃ´i thay Ä‘á»•i ngÆ°á»¡ng thoÃ¡t trong khoáº£ng {0.0, 0.1, 0.3, 0.5, 0.7, 0.9}. CÃ¡c giÃ¡ trá»‹ Ä‘á»™ trá»… Ä‘Æ°á»£c chuáº©n hÃ³a bá»Ÿi Ä‘á»™ trá»… cá»§a baseline, lÃ  mÃ´ hÃ¬nh Ä‘áº§y Ä‘á»§ Ä‘Æ°á»£c tinh chá»‰nh Ä‘Æ¡n giáº£n.]

D Káº¿t quáº£ ThÃ­ nghiá»‡m Bá»• sung

Trong pháº§n nÃ y, chÃºng tÃ´i cung cáº¥p káº¿t quáº£ thÃ­ nghiá»‡m bá»• sung Ä‘á»ƒ chá»©ng minh hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p Ä‘á» xuáº¥t vÃ  cÃ¡c thÃ nh pháº§n riÃªng láº» cá»§a nÃ³.

D.1 Hiá»‡u suáº¥t trÃªn CÃ¡c Táº­p dá»¯ liá»‡u KhÃ¡c nhau

Trong pháº§n nÃ y, chÃºng tÃ´i trÃ¬nh bÃ y so sÃ¡nh cháº¥t lÆ°á»£ng Ä‘áº§u ra Ä‘Æ°á»£c táº¡o (F1 hoáº·c BLEU) vÃ  Ä‘á»™ trá»… suy luáº­n trÃªn táº­p dá»¯ liá»‡u SQuAD vÃ  IWSLT 2017, tÆ°Æ¡ng tá»± nhÆ° cÃ¡c thÃ­ nghiá»‡m trong HÃ¬nh 5. HÃ¬nh 9 minh há»a ráº±ng cáº£ FREEâ€  vÃ  FREE Ä‘á»u nháº¥t quÃ¡n vÆ°á»£t trá»™i hÆ¡n cÃ¡c baseline CALM vÃ  static-exiting trong táº­p dá»¯ liá»‡u SQuAD, phÃ¹ há»£p vá»›i cÃ¡c phÃ¡t hiá»‡n trÆ°á»›c Ä‘Ã¢y cá»§a chÃºng tÃ´i.

Tuy nhiÃªn, lá»£i tháº¿ hiá»‡u suáº¥t cá»§a chÃºng trong táº­p dá»¯ liá»‡u IWSLT bá»‹ giáº£m nháº¹ so vá»›i cÃ¡c táº­p dá»¯ liá»‡u khÃ¡c. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c quy cho kÃ­ch thÆ°á»›c tá»« vá»±ng lá»›n hÆ¡n cá»§a mT5 so vá»›i T5, dáº«n Ä‘áº¿n thá»i gian xá»­ lÃ½ dÃ i hÆ¡n cho Ä‘o lÆ°á»ng tin cáº­y. PhÆ°Æ¡ng phÃ¡p CALM, cÅ©ng sá»­ dá»¥ng cÃ¡c bá»™ phÃ¢n loáº¡i tuyáº¿n tÃ­nh lá»›n, thá»ƒ hiá»‡n hiá»‡u quáº£ tháº¥p hÆ¡n nhiá»u trong táº­p dá»¯ liá»‡u nÃ y. ChÃºng tÃ´i tin ráº±ng thÃ¡ch thá»©c nÃ y, liÃªn quan Ä‘áº¿n kÃ­ch thÆ°á»›c tá»« vá»±ng lá»›n, cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£m thiá»ƒu báº±ng cÃ¡ch sá»­ dá»¥ng biá»‡n phÃ¡p tin cáº­y Ä‘á»™c láº­p vá»›i kÃ­ch thÆ°á»›c tá»« vá»±ng Ä‘Æ°á»£c Ä‘á» xuáº¥t trong cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y (Schuster et al., 2022). Tuy nhiÃªn, thuáº­t toÃ¡n Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i váº«n vÆ°á»£t trá»™i hÆ¡n cÃ¡c baseline khÃ¡c trÃªn cÃ¡c táº­p dá»¯ liá»‡u khÃ¡c nhau.

Báº£ng 9: So sÃ¡nh giá»¯a FREE vá»›i T5-large vÃ  T5-base kÃ­ch thÆ°á»›c nhá» Ä‘Æ°á»£c Ä‘Ã o táº¡o trá»±c tiáº¿p. ChÃºng tÃ´i Ã¡p dá»¥ng giÃ¡ trá»‹ ngÆ°á»¡ng cá»§a FREEâ€  lÃ  0.1 cho SQuAD vÃ  0.2 cho CNN/DailyMail.

                SQuAD                  CNN/DailyMail
Method    Model    F1    Speedup    ROUGE-L  Speedup
Full Model T5-large 91.82  Ã—1.00     41.09    Ã—1.00
Full Model T5-base  90.50  Ã—1.86     40.22    Ã—2.06
FREEâ€      T5-large 90.95  Ã—2.76     40.17    Ã—2.07

D.2 Knowledge Distillation Theo lá»›p

Vá»›i chá»‰ hai vá»‹ trÃ­ thoÃ¡t trong mÃ´-Ä‘un nÃ´ng-sÃ¢u cá»§a chÃºng tÃ´i, vÃ¬ hiá»‡u suáº¥t cá»§a chÃºng áº£nh hÆ°á»Ÿng Ä‘Ã¡ng ká»ƒ Ä‘áº¿n tÃ­nh máº¡nh máº½ tá»•ng thá»ƒ cá»§a phÆ°Æ¡ng phÃ¡p thoÃ¡t sá»›m, chÃºng tÃ´i thiáº¿t káº¿ cáº©n tháº­n hÃ m máº¥t mÃ¡t Ä‘á»ƒ Ä‘Ã o táº¡o. Trong HÃ¬nh 10, chÃºng tÃ´i quan sÃ¡t xu hÆ°á»›ng hiá»‡u suáº¥t cá»§a bá»‘n hÃ m máº¥t mÃ¡t khÃ¡c nhau khi chÃºng tÃ´i thay Ä‘á»•i ngÆ°á»¡ng thoÃ¡t. Trong khi sá»± khÃ¡c biá»‡t khÃ´ng Ä‘Ã¡ng ká»ƒ, máº¥t mÃ¡t KD-dyna thá»ƒ hiá»‡n sá»± cÃ¢n báº±ng tá»‘t hÆ¡n so vá»›i trung bÃ¬nh cÃ³ trá»ng sá»‘ hoáº·c cÃ¡c máº¥t mÃ¡t dá»±a trÃªn KD khÃ¡c. Cá»¥ thá»ƒ, hiá»‡u suáº¥t tháº¥p hÆ¡n cá»§a KD-unif trÃªn táº­p dá»¯ liá»‡u SAMSum cho tháº¥y ráº±ng viá»‡c xÃ¡c Ä‘á»‹nh Ã¡nh xáº¡ lá»›p Ä‘á»™ng cÃ³ thá»ƒ táº¡o Ä‘iá»u kiá»‡n chuyá»ƒn giao kiáº¿n thá»©c hiá»‡u quáº£ hÆ¡n giá»¯a mÃ´ hÃ¬nh sÃ¢u vÃ  nÃ´ng. Do Ä‘Ã³, chÃºng tÃ´i Ä‘Ã o táº¡o mÃ´-Ä‘un nÃ´ng-sÃ¢u sá»­ dá»¥ng máº¥t mÃ¡t KD-dyna cho táº¥t cáº£ thÃ­ nghiá»‡m, vÃ  Ä‘á»ƒ láº¡i viá»‡c khÃ¡m phÃ¡ cÃ¡c hÃ m máº¥t mÃ¡t bá»• sung, nhÆ° máº¥t mÃ¡t chÆ°ng cáº¥t tÆ°Æ¡ng pháº£n (Tian et al., 2019; Bae et al., 2021), cho cÃ´ng trÃ¬nh tÆ°Æ¡ng lai.

D.3 So sÃ¡nh vá»›i MÃ´ hÃ¬nh KÃ­ch thÆ°á»›c Nhá»

ChÃºng tÃ´i thá»±c hiá»‡n so sÃ¡nh giá»¯a tá»‘c Ä‘á»™ suy luáº­n cá»§a FREE sá»­ dá»¥ng mÃ´ hÃ¬nh T5-large vÃ  mÃ´ hÃ¬nh T5-base Ä‘Æ°á»£c Ä‘Ã o táº¡o trá»±c tiáº¿p. Äá»ƒ Ä‘áº£m báº£o so sÃ¡nh cÃ´ng báº±ng, chÃºng tÃ´i chá»n thá»§ cÃ´ng ngÆ°á»¡ng tin cáº­y phÃ¹ há»£p cho FREEâ€  (khÃ´ng dá»±a vÃ o bá»™ Æ°á»›c tÃ­nh ngÆ°á»¡ng thÃ­ch á»©ng) Ä‘á»ƒ cÄƒn chá»‰nh hiá»‡u suáº¥t cá»§a nÃ³ gáº§n vá»›i T5-base. Káº¿t quáº£, Ä‘Æ°á»£c trÃ¬nh bÃ y trong Báº£ng 9, chá»©ng minh ráº±ng phÆ°Æ¡ng phÃ¡p Ä‘á» xuáº¥t thá»ƒ hiá»‡n tÄƒng tá»‘c cáº¡nh tranh trong hiá»‡u suáº¥t suy luáº­n trÃªn táº­p dá»¯ liá»‡u CNN/DailyMail. HÆ¡n ná»¯a, nÃ³ thá»ƒ hiá»‡n Ä‘iá»ƒm F1 Æ°u viá»‡t vÃ  tÄƒng tá»‘c cao hÆ¡n Ä‘Ã¡ng ká»ƒ trÃªn táº­p dá»¯ liá»‡u SQuAD.

--- TRANG 15 ---
Báº£ng 10: So sÃ¡nh giá»¯a cÃ¡c khung thoÃ¡t sá»›m trÃªn SAMSum vá»›i cÃ¡c chiáº¿n lÆ°á»£c giáº£i mÃ£ khÃ¡c nhau.

                top-k (k=50)          nucleus (p=0.92)
Method         ROUGE-L  Speedup     ROUGE-L  Speedup
Full Model     44.34    Ã—1.00       45.84    Ã—1.00
CALM           42.35    Ã—0.78       44.48    Ã—0.82
FREE           43.58    Ã—1.30       45.78    Ã—1.31

ChÃºng tÃ´i tin ráº±ng sá»± biáº¿n Ä‘á»™ng trong tÄƒng tá»‘c qua cÃ¡c táº­p dá»¯ liá»‡u cÃ³ thá»ƒ Ä‘Æ°á»£c quy cho hiá»‡u suáº¥t cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c bá»Ÿi mÃ´ hÃ¬nh nhá» hÆ¡n Ä‘Æ°á»£c Ä‘Ã o táº¡o trá»±c tiáº¿p, cÅ©ng nhÆ° mÃ´ hÃ¬nh nÃ´ng trong khung FREE. Trong trÆ°á»ng há»£p SQuAD, mÃ´ hÃ¬nh T5-base (12 lá»›p) Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm ROUGE-L 90.50, trong khi mÃ´ hÃ¬nh nÃ´ng (6 lá»›p) cá»§a khung FREE chÃºng tÃ´i cho Ä‘iá»ƒm tÆ°Æ¡ng tá»± 90.24. PhÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i hiá»‡u quáº£ táº­n dá»¥ng nhá»¯ng lá»£i Ã­ch vá»‘n cÃ³ nÃ y, do Ä‘Ã³ táº¡o Ä‘iá»u kiá»‡n tÄƒng tá»‘c suy luáº­n thÃ´ng qua thoÃ¡t á»Ÿ cÃ¡c lá»›p tháº¥p hÆ¡n.

D.4 CÃ¡c Chiáº¿n lÆ°á»£c Giáº£i mÃ£ KhÃ¡c nhau

Äá»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng Ã¡p dá»¥ng cá»§a FREE trÃªn cÃ¡c phÆ°Æ¡ng phÃ¡p giáº£i mÃ£ khÃ¡c nhau, chÃºng tÃ´i thá»±c hiá»‡n thÃ­ nghiá»‡m vá»›i láº¥y máº«u top-k (Radford et al., 2019) vÃ  láº¥y máº«u nucleus (láº¥y máº«u top-p; Holtzman et al. 2020). Láº¥y máº«u top-k láº¥y máº«u tá»« tiáº¿p theo tá»« k lá»±a chá»n cÃ³ xÃ¡c suáº¥t cao nháº¥t, thay vÃ¬ nháº±m giáº£i mÃ£ vÄƒn báº£n tá»‘i Ä‘a hÃ³a likelihood. Máº·t khÃ¡c, láº¥y máº«u nucleus chá»n tá»« táº­p há»£p nhá» nháº¥t cÃ³ thá»ƒ cá»§a cÃ¡c tá»« cÃ³ xÃ¡c suáº¥t tÃ­ch lÅ©y vÆ°á»£t quÃ¡ xÃ¡c suáº¥t p. NhÆ° Ä‘Æ°á»£c chi tiáº¿t trong Báº£ng 10, phÆ°Æ¡ng phÃ¡p FREE thá»ƒ hiá»‡n hiá»‡u suáº¥t nháº¥t quÃ¡n vÃ  máº¡nh máº½ Ä‘á»“ng thá»i Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c lá»›n hÆ¡n so vá»›i CALM. Nhá»¯ng káº¿t quáº£ nÃ y kháº³ng Ä‘á»‹nh ráº±ng khung FREE cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng rá»™ng rÃ£i, báº¥t ká»ƒ phÆ°Æ¡ng phÃ¡p giáº£i mÃ£ Ä‘Æ°á»£c chá»n.
