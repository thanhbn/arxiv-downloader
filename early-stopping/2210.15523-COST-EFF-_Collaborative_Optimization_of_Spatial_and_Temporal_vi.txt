# 2210.15523.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/early-stopping/2210.15523.pdf
# Kích thước tệp: 700520 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
COST-EFF: Tối ưu hóa Cộng tác về Hiệu quả Không gian và Thời gian
với Mô hình Ngôn ngữ Đa lối thoát được Thu hẹp
Bowen Shen1;2, Zheng Lin1;2, Yuanxin Liu1;3, Zhengxiao Liu1,
Lei Wang1, Weiping Wang1
1Viện Kỹ thuật Thông tin, Viện Hàn lâm Khoa học Trung Quốc
2Trường An ninh mạng, Đại học Viện Hàn lâm Khoa học Trung Quốc
3Phòng thí nghiệm Trọng điểm MOE về Ngôn ngữ học Tính toán, Đại học Bắc Kinh
{shenbowen, linzheng, liuzhengxiao, wanglei, wangweiping}@iie.ac.cn
liuyuanxin@stu.pku.edu.cn
Tóm tắt
Các mô hình ngôn ngữ được huấn luyện trước dựa trên Transformer (PLM) chủ yếu bị ảnh hưởng bởi chi phí quá mức mặc dù có khả năng tiên tiến. Đối với các thiết bị có tài nguyên hạn chế, có nhu cầu cấp thiết về một mô hình hiệu quả về không gian và thời gian vẫn giữ được khả năng chính của PLM. Tuy nhiên, các mô hình nén tĩnh hiện tại không nhận thức được sự đa dạng phức tạp giữa các thực thể đầu vào, có thể dẫn đến dư thừa và không đầy đủ cho các đầu vào đơn giản và phức tạp. Ngoài ra, các mô hình thu nhỏ với thoát sớm gặp thách thức trong việc cân bằng giữa đưa ra dự đoán và phục vụ các lớp sâu hơn. Được thúc đẩy bởi những cân nhắc như vậy, chúng tôi đề xuất một tối ưu hóa cộng tác cho PLM tích hợp nén mô hình tĩnh và gia tốc suy luận động. Cụ thể, PLM được thu hẹp về chiều rộng trong khi độ sâu được giữ nguyên, bổ sung cho việc thoát sớm theo lớp để tăng tốc suy luận một cách động. Để giải quyết sự cân bằng của việc thoát sớm, chúng tôi đề xuất một phương pháp huấn luyện kết hợp hiệu chỉnh việc thu hẹp và bảo tồn các cấu trúc đóng góp cho từng lối thoát thay vì chỉ lớp cuối cùng. Các thí nghiệm được tiến hành trên benchmark GLUE và kết quả xác minh tính tối ưu Pareto của phương pháp chúng tôi ở tỷ lệ nén và gia tốc cao với 1/8 tham số và 1/19 FLOP của BERT.

1 Giới thiệu
Huấn luyện trước các mô hình ngôn ngữ tổng quát và tinh chỉnh chúng trên các tác vụ hạ nguồn cụ thể đã trở thành mô hình chủ đạo trong xử lý ngôn ngữ tự nhiên (NLP) kể từ sự ra đời của Transformer (Vaswani et al., 2017) và BERT (Devlin et al., 2019). Tuy nhiên, các mô hình ngôn ngữ được huấn luyện trước (PLM) chủ yếu được thiết kế để có quy mô lớn nhằm theo đuổi khả năng mô hình và khả năng tổng quát hóa. Với mối quan tâm như vậy, việc lưu trữ mô hình và thời gian suy luận của PLM thường cao, khiến chúng khó triển khai trên các thiết bị có tài nguyên hạn chế (Sun et al., 2020).

Zheng Lin và Lei Wang là các tác giả liên hệ.

Tfm Layer 1Tfm Layer 6…
Emb LayerClf 1Clf 6Clf 12 Tfm Layer 12
…Clf
BER TBase COST -EFF (ours)Tfm Layer 12
Tfm Layer 6
Tfm Layer 1
Emb Layer
Distillation
Dynamic acceleration  
for temporal ef ficiency  Static slenderization  
for spatial efficiency  Computation demands
… …Hình 1: Minh họa cấu trúc mô hình COST-EFF và quy trình suy luận. Emb, Tfm và Clf là viết tắt của embedding, Transformer và classifier, tương ứng. Biểu đồ cột màu xanh biểu thị phân phối xác suất đầu ra bởi các bộ phân loại.

Các nghiên cứu gần đây chỉ ra rằng PLM dựa trên Transformer có sự dư thừa về không gian và thời gian đến từ chiều rộng và độ sâu quá mức của mô hình (Michel et al., 2019; Xin et al., 2021). Với các phương pháp nén tĩnh bao gồm tỉa mạng (Xia et al., 2022) và chưng cất kiến thức (Jiao et al., 2020), chi phí không gian của PLM (tức là tham số mô hình) có thể được giảm xuống một cài đặt cố định. Từ góc độ các thực thể đầu vào hơn là mô hình, việc thoát sớm mà không đi qua tất cả các lớp mô hình cho phép gia tốc động tại thời điểm suy luận và giảm chi phí thời gian (Zhou et al., 2020).

Tuy nhiên, nén tĩnh khó có thể tìm được cài đặt tối ưu vừa hiệu quả trên các thực thể đầu vào đơn giản vừa chính xác trên các thực thể phức tạp, trong khi việc thoát sớm không thể giảm sự dư thừa trong chiều rộng mô hình và không có khả năng giảm khối lượng thực tế của mô hình. Hơn nữa, các nghiên cứu về khả năng diễn giải chỉ ra rằng các đặc trưng chú ý và ngữ nghĩa qua các lớp là khác nhau trong BERT (Clark et al., 2019). Do đó, việc tạo ra một mô hình đa lối thoát từ một mô hình đơn lối thoát được huấn luyện trước như BERT gây ra sự không nhất quán trong mục tiêu huấn luyện, nơi mỗi lớp đồng thời đưa ra dự đoán và phục vụ các lớp sâu hơn (Xin et al., 2021). Về mặt thực nghiệm, chúng tôi thấy rằng BERT không nén không bị ảnh hưởng nghiêm trọng bởi sự không nhất quán như vậy, trong khi các mô hình có khả năng nhỏ không có khả năng cân bằng các lớp nông và sâu. Việc cắm thêm các lối thoát sau khi nén sẽ dẫn đến suy giảm hiệu suất nghiêm trọng, điều này cản trở việc bổ sung của hai tối ưu hóa.

Để khai thác đầy đủ hiệu quả của PLM và giảm thiểu các vấn đề nêu trên, chúng tôi thiết kế một mô hình đa lối thoát được thu hẹp và đề xuất một phương pháp Tối ưu hóa Cộng tác về Hiệu quả Không gian và Thời gian (COST-EFF) như được mô tả trong Hình 1. Không giống như các công trình trước đây, ví dụ DynaBERT (Hou et al., 2020) và CoFi (Xia et al., 2022), tạo ra một mô hình thấp, chúng tôi giữ nguyên độ sâu trong khi thu hẹp PLM. Ưu thế của kiến trúc thanh mảnh so với kiến trúc thấp được hỗ trợ bởi (Bengio et al., 2007) và (Turc et al., 2019) trong học máy tổng quát và thiết kế PLM. Để giải quyết sự không nhất quán trong mô hình đa lối thoát được nén, trước tiên chúng tôi chưng cất một BERT đa lối thoát từ PLM gốc làm cả trợ lý giảng dạy (TA) và xương sống thu hẹp, điều này hiệu quả hơn trong việc cân bằng sự cân bằng giữa các lớp so với các mô hình nén. Sau đó, chúng tôi đề xuất một phương pháp cộng tác thu hẹp xương sống với việc hiệu chỉnh các lối thoát. Việc thu hẹp như vậy giảm bớt các cấu trúc ít đóng góp cho từng lối thoát cũng như sự dư thừa trong chiều rộng. Sau khi thu hẹp, chưng cất kiến thức cụ thể cho tác vụ được tiến hành với các mục tiêu biểu diễn ẩn và dự đoán của từng lớp như khôi phục.

Cụ thể, các đóng góp của bài báo này như sau:
• Để tối ưu hóa toàn diện hiệu quả không gian và thời gian của PLM, chúng tôi tận dụng cả thu hẹp tĩnh và gia tốc động từ góc độ quy mô mô hình và tính toán biến đổi.
• Chúng tôi đề xuất một phương pháp huấn luyện cộng tác hiệu chỉnh việc thu hẹp dưới sự hướng dẫn của các lối thoát trung gian và giảm thiểu sự không nhất quán của việc thoát sớm.
• Các thí nghiệm được tiến hành trên benchmark GLUE xác minh tính tối ưu Pareto của phương pháp chúng tôi. COST-EFF đạt 96.5% hiệu suất của BERT Base được tinh chỉnh với khoảng 1/8 tham số và 1/19 FLOP mà không có bất kỳ hình thức tăng cường dữ liệu nào.1

2 Công trình Liên quan
Việc nén và gia tốc PLM gần đây đã được nghiên cứu để vô hiệu hóa chi phí của các mô hình lớn bằng các phương tiện khác nhau.

Việc tỉa có cấu trúc bao gồm, từ nhỏ đến lớn, các chiều ẩn (Wang et al., 2020), các đầu chú ý (Michel et al., 2019), các mô-đun chú ý đa đầu (MHA) và mạng feed-forward (FFN) (Xia et al., 2022) và toàn bộ các lớp Transformer (Fan et al., 2020). Xem xét lợi ích của cấu trúc tổng thể, chúng tôi giữ tất cả các mô-đun trong khi giảm kích thước của chúng. Bên cạnh việc tỉa bỏ các cấu trúc, một phương pháp tinh vi là tỉa không có cấu trúc, tỉa bỏ các trọng số. Tỉa không có cấu trúc có thể đạt được độ thưa thớt cao 97% (Xu et al., 2022) nhưng chưa thích ứng với các nền tảng tính toán tổng quát và phần cứng.

Trong quá trình huấn luyện khôi phục của các mô hình nén, các mục tiêu chưng cất kiến thức bao gồm dự đoán của các bộ phân loại (Sanh et al., 2020), đặc trưng của các biểu diễn trung gian (Jiao et al., 2020) và mối quan hệ giữa các mẫu (Tung và Mori, 2019). Ngoài ra, dịp chưng cất khác nhau từ huấn luyện trước tổng quát và tinh chỉnh cụ thể cho tác vụ (Turc et al., 2019). Chưng cất cho phép huấn luyện mà không cần nhãn sự thật cơ bản bổ sung cho việc tăng cường dữ liệu. Trong bài báo này, việc tăng cường dữ liệu không được tận dụng vì nó yêu cầu thời gian huấn luyện dài, nhưng phương pháp của chúng tôi thích ứng tốt với nó nếu muốn theo đuổi hiệu suất tốt hơn.

Các lối thoát sớm động xuất phát từ BranchyNet (Teerapittayanon et al., 2016), giới thiệu các nhánh thoát sau các lớp tích chập cụ thể của mô hình CNN. Ý tưởng được áp dụng cho PLM như việc thoát sớm theo lớp Transformer (Xin et al., 2021; Zhou et al., 2020; Liu et al., 2020). Tuy nhiên, việc thoát sớm chỉ tăng tốc suy luận nhưng không giảm kích thước mô hình và sự dư thừa trong chiều rộng. Hơn nữa, do sự không nhất quán giữa các lớp nông và sâu, khó đạt được tốc độ cao chỉ bằng việc thoát sớm.

Các PLM phổ biến, ví dụ RoBERTa (Liu et al., 2019) và XLNet (Yang et al., 2019) là các biến thể của Transformer với cấu trúc tổng thể tương tự, thích ứng tốt với các tối ưu hóa mà chúng tôi đề xuất. Ngoài PLM với kích thước tăng, ALBERT (Lan et al., 2020) khác biệt với khối lượng nhỏ 18M (triệu) tham số thu được từ việc chia sẻ trọng số của các lớp Transformer. Việc chia sẻ trọng số cho phép mô hình lưu trữ tham số chỉ một lần, giảm đáng kể chi phí lưu trữ. Tuy nhiên, các trọng số được chia sẻ không đóng góp vào tăng tốc suy luận. Thay vào đó, thời gian cần thiết để ALBERT đạt được độ chính xác giống BERT tăng lên.

1Code có sẵn tại https://github.com/sbwww/COST-EFF .

--- TRANG 2 ---
3 Phương pháp luận
Trong phần này, chúng tôi phân tích các cấu trúc chính của PLM dựa trên Transformer và đưa ra các tối ưu hóa tương ứng. COST-EFF được đề xuất có ba thuộc tính chính, cụ thể là thu hẹp tĩnh, gia tốc động và huấn luyện cộng tác.

3.1 Kiến thức cơ bản
Trong bài báo này, chúng tôi tập trung vào việc tối ưu hóa PLM dựa trên Transformer chủ yếu bao gồm embedding, MHA và FFN. Cụ thể, embedding chuyển đổi mỗi token đầu vào thành một tensor có kích thước H (tức là chiều ẩn). Với kích thước từ vựng chung |V| = 30,522, ma trận word embedding chiếm <22% tham số của BERT Base.

Bên trong Transformer, MHA có bốn ma trận WQ, WK, WV và WO, tất cả đều có kích thước đầu vào và đầu ra là H. FFN có hai ma trận WFI và WFO với kích thước H×F. Là các thành phần chính của Transformer, MHA và FFN chiếm <26% và <52% tham số của BERT Base, tương ứng.

Dựa trên phân tích, chúng tôi có các sơ đồ thu hẹp và gia tốc sau. (1) Ma trận word embedding Wt được phân tách thành phép nhân của hai ma trận theo (Lan et al., 2020). Do đó, kích thước từ vựng |V| và kích thước ẩn H không thay đổi. (2) Đối với các ma trận biến đổi của MHA và FFN, tỉa có cấu trúc được áp dụng để giảm chiều đầu vào hoặc đầu ra của chúng. (3) Suy luận được tăng tốc thông qua việc thoát sớm vì chúng tôi giữ nguyên độ sâu mô hình được huấn luyện trước. Để tránh đưa vào các tham số bổ sung, chúng tôi loại bỏ ma trận pooler được huấn luyện trước trước các bộ phân loại. (4) Chưng cất kiến thức về logit dự đoán và trạng thái ẩn của từng lớp được tận dụng như một sự thay thế cho việc tinh chỉnh thông thường. Kiến trúc tổng thể của COST-EFF được mô tả trong Hình 2.

3.2 Thu hẹp Tĩnh

3.2.1 Phân tách Ma trận của Embedding
Như đã đề cập trước đây, word embedding chiếm hơn 1/5 tham số của BERT Base. Chiều đầu ra của word embedding bằng kích thước ẩn, mà chúng tôi không sửa đổi, chúng tôi sử dụng phân tách giá trị đơn bị cắt xén (TSVD) để nén nội bộ ma trận word embedding.

TSVD đầu tiên phân tách ma trận như Am×n = Um×m Σm×n VTn×n, trong đó Σm×n là ma trận đường chéo giá trị đơn. Sau đó, ba ma trận được cắt xén đến rank đã cho. Do đó, việc phân tách word embedding là
W|V|×H t ≈ W|V|×R t1 W R×H t2 = ŨU|V|×R Σ̃R×R ṼR×H;    (1)
trong đó chúng tôi nhân Ũ và Σ̃ làm ma trận embedding đầu tiên W|V|×R t1 và W R×H t2 = ṼT là một biến đổi tuyến tính không có bias.

3.2.2 Tỉa Có cấu trúc của MHA và FFN
Để nén các ma trận trong MHA và FFN đóng góp vào hầu hết các tham số của PLM, chúng tôi áp dụng tỉa có cấu trúc để nén một chiều của các ma trận. Như được mô tả trong Hình 2, độ chi tiết tỉa của MHA và FFN lần lượt là đầu chú ý và chiều ẩn.

Theo (Molchanov et al., 2017), COST-EFF có mục tiêu tỉa là tối thiểu hóa sự khác biệt giữa mô hình đã tỉa và gốc, được tính bằng khai triển Taylor bậc nhất
|J(S)| = |L(X) - L(X|hi = 0, hi ∈ S)|
= |∑hi∈S ∂L/∂hi (hi - 0) + R(1)|
≈ |∑hi∈S ∂L/∂hi hi|,    (2)
trong đó S biểu thị một cấu trúc cụ thể, tức là một tập hợp các trọng số, L(·) là hàm mất mát và ∂L/∂hi là gradient của mất mát đối với trọng số hi. |J(S)| là tầm quan trọng của cấu trúc S được đo bằng giá trị tuyệt đối của số hạng bậc nhất. Để đơn giản, chúng tôi bỏ qua phần dư R(1) trong khai triển Taylor.

Trong mỗi lớp Transformer, cấu trúc S của MHA là đầu chú ý trong khi của FFN là chiều ẩn như được mô tả ở phần dưới của Hình 2. Cụ thể, các chiều đầu ra của WQ, WK, WV và WFI được nén. Ngược lại, các chiều đầu vào của WO và WFO được nén. Do đó, chiều của các trạng thái ẩn vẫn nguyên vẹn trong COST-EFF. Ngoài ra, vì một lần tỉa đơn nhưng mạnh mẽ thường gây ra thiệt hại khó khôi phục, chúng tôi sử dụng tỉa lặp (Tan và Motani, 2020) trong COST-EFF, dần dần tỉa bỏ các mô-đun không quan trọng.

3.3 Gia tốc Động

3.3.1 Suy luận với Thoát Sớm
Không giống như nén tĩnh, việc thoát sớm xác định tính toán một cách động tại thời điểm suy luận, tùy thuộc vào độ phức tạp của đầu vào và độ khó hiểu của mô hình. Cụ thể, chúng tôi sử dụng thoát sớm theo lớp, như được hiển thị trong Hình 1, bằng cách cắm một bộ phân loại tại mỗi lớp Transformer.

Theo kết quả thí nghiệm của ElasticBERT (Liu et al., 2022), việc thoát dựa trên entropy nói chung vượt trội hơn dựa trên patience, chúng tôi sử dụng entropy của đầu ra bộ phân loại làm điều kiện thoát được định nghĩa là H(x) = -∑Ci=1 p(i) ln p(i), trong đó p(·) là phân phối xác suất được tính bằng hàm softmax và H(x) là entropy của phân phối xác suất x. Nếu entropy lớn hơn ngưỡng cho trước HT, mô hình khó đưa ra dự đoán ở trạng thái đó. Ngược lại, mô hình có xu hướng đưa ra dự đoán chắc chắn với entropy nhỏ, nơi sự khác biệt trong phân phối xác suất lớn và chiếm ưu thế.

3.3.2 Huấn luyện Nhiều Lối thoát
Khi huấn luyện mô hình với nhiều lối thoát, hàm mất mát của mỗi lối thoát được tính đến. DeeBERT (Xin et al., 2020) giới thiệu một sơ đồ huấn luyện hai giai đoạn trong đó mô hình xương sống và các lối thoát được huấn luyện riêng biệt. Tuy nhiên, chỉ với mất mát của bộ phân loại cuối cùng và các gradient lan truyền ngược, các lớp nông của mô hình xương sống không có khả năng đưa ra dự đoán tự tin mà thay vào đó phục vụ các lớp sâu. Do đó, cần thiết phải đưa vào mất mát của các bộ phân loại trung gian trong khi huấn luyện và tính toán tầm quan trọng cấu trúc dựa trên khai triển Taylor như Phương trình 2 trong COST-EFF.

Để cân bằng gradient từ nhiều mất mát bộ phân loại, chúng tôi sử dụng cân bằng gradient theo (Li et al., 2019) và thay đổi tỷ lệ gradient của lớp k thành
∇'wk L = 1/(L-k+1) ∑i=k L ∇wk Li,    (3)
trong đó L là độ sâu mô hình, ∇wk Li là gradient lan truyền từ lớp i xuống lớp k và ∇'wk L là gradient được thay đổi tỷ lệ.

3.4 Huấn luyện Cộng tác của COST-EFF

3.4.1 Huấn luyện với Chưng cất Kiến thức
Kích thước nhỏ và khả năng của mô hình nén làm cho việc khôi phục hiệu suất chỉ bằng tinh chỉnh trở nên khó khăn. Trong khi chưng cất kiến thức được sử dụng như một bổ sung chuyển giao kiến thức từ mô hình giáo viên gốc sang mô hình học sinh nén. Trong bài báo này, chúng tôi nhằm chưng cất dự đoán và đặc trưng trung gian (tức là trạng thái ẩn) như được mô tả trong Hình 2.

Vì sự không nhất quán giữa các lớp được quan sát (Xin et al., 2021), việc đơn giản sử dụng nhãn sự thật cơ bản để huấn luyện một mô hình đa lối thoát nén sẽ dẫn đến mâu thuẫn nghiêm trọng. Với điều này, trước tiên chúng tôi chưng cất mô hình gốc thành một mô hình BERT Base đa lối thoát với cùng số lớp làm TA. Sau đó, mỗi đầu ra lớp của TA được sử dụng làm nhãn mềm của lớp tương ứng trong COST-EFF như
Lpred = ∑i=1 L CELoss(zTA i /T, zCE i /T),    (4)
trong đó zTA i và zCE i là đầu ra dự đoán của TA và COST-EFF tại lớp thứ i, tương ứng. T là yếu tố nhiệt độ thường được đặt là 1. Bên cạnh chưng cất các dự đoán, COST-EFF chưng cất các trạng thái ẩn để chuyển giao hiệu quả các biểu diễn của TA sang mô hình học sinh. Các đầu ra trạng thái ẩn, ký hiệu là Hi (i = 0, 1, ..., L+1) bao gồm đầu ra embedding H0 và mỗi đầu ra lớp Transformer, được tối ưu hóa như
Lfeat = ∑i=0 L+1 MSELoss(HTA i , HCE i ).    (5)

3.4.2 Quy trình COST-EFF
Như đã đề cập trong Phần 3.4.1, COST-EFF đầu tiên chưng cất mô hình thành một mô hình TA đa lối thoát với cùng số lớp. Cụ thể, chúng tôi chưng cất các dự đoán ở giai đoạn này. Mặc dù chưng cất đặc trưng thường mạnh mẽ hơn, các biểu diễn của mô hình đơn lối thoát không được căn chỉnh với mô hình đa lối thoát và sẽ đưa vào sự không nhất quán trong quá trình huấn luyện. Việc chưng cất như vậy che giấu các triển khai tầm thường của các PLM khác nhau được nén, cũng như giảm thiểu sơ bộ sự không nhất quán giữa các lớp với một mô hình lớn hơn và mạnh mẽ hơn. Sau đó, mô hình TA được sử dụng làm cả xương sống thu hẹp và giáo viên của việc chưng cất kiến thức tiếp theo.

Trong quá trình thu hẹp, chúng tôi tích hợp mất mát của các lối thoát vào tính toán tầm quan trọng cấu trúc dựa trên khai triển Taylor. So với việc đơn giản sử dụng mất mát của bộ phân loại cuối cùng, mất mát đa lối thoát giúp hiệu chỉnh việc thu hẹp bằng cách cân nhắc đóng góp của các cấu trúc cho từng lối thoát tiếp theo thay vì chỉ lớp cuối cùng. Bằng cách này, sự cân bằng giữa các lớp có thể được cân bằng tốt hơn trong mô hình được thu hẹp. Sau khi thu hẹp, huấn luyện khôi phục là một chuyển giao kiến thức theo lớp từ TA sang COST-EFF với mục tiêu tối thiểu hóa tổng của Lpred và Lfeat, điều này giảm thiểu các mâu thuẫn của huấn luyện nhãn sự thật cơ bản trên mô hình đa lối thoát được thu hẹp.

4 Đánh giá Thí nghiệm

4.1 Thiết lập Thí nghiệm
Bộ dữ liệu Chúng tôi sử dụng bốn tác vụ của benchmark GLUE (Wang et al., 2019), cụ thể là SST-2, MRPC, QNLI và MNLI. Chi tiết của các tác vụ này được hiển thị trong Bảng 1 và hầu hết các danh mục của GLUE được bao gồm.

Tác vụ Danh mục Nhãn Thước đo
SST-2 Câu đơn 2 Acc
MRPC Paraphrase 2 F1
QNLI Suy luận 2 Acc
MNLI Suy luận 3 Acc

Bảng 1: Chi tiết của các bộ dữ liệu.

Phương pháp So sánh Chúng tôi so sánh các baseline và phương pháp sau. (1) Các kích thước khác nhau của mô hình BERT, cụ thể là BERT Base, BERT 6L-768H và BERT 8L-256H, được tinh chỉnh dựa trên các mô hình được huấn luyện trước của (Turc et al., 2019). (2) Các phương pháp nén tĩnh đại diện. DistilBERT (Sanh et al., 2020) và TinyBERT (Jiao et al., 2020). (3) Các phương pháp gia tốc động. DeeBERT (Xin et al., 2020), PABEE (Zhou et al., 2020) và mô hình đa lối thoát được huấn luyện trước ElasticBERT (Liu et al., 2022).

Cài đặt Mô hình Vì số lượng tham số ảnh hưởng sâu sắc đến khả năng và hiệu suất, chúng tôi có hai nhóm so sánh với kích thước mô hình tương tự bên trong mỗi nhóm. Các mô hình trong nhóm đầu tiên có ít hơn 20M tham số và nhóm mô hình thứ hai có kích thước lớn hơn trên 50M tham số. Chi tiết của cài đặt mô hình có thể được tìm thấy trong Bảng 2. Đáng chú ý, kết quả của DistilBERT được trích xuất từ bài báo gốc và những kết quả khác được thực hiện bởi chúng tôi vì các thí nghiệm liên quan đến các mô hình xương sống và dữ liệu huấn luyện khác nhau. Việc triển khai với bộ tối ưu AdamW trên một GPU RTX 3090 24GB, trong khi kích thước batch huấn luyện nằm trong {32, 64} và tỷ lệ học nằm trong {2e-5, 3e-5, 4e-5} khác nhau từ các tác vụ.

--- TRANG 3 ---
Mô hình Kích thước EE L H A F
BERT Base 12 768 12 64 3072
BERT 8L-256H 8 256 4 64 1024
TinyBERT 4 4 312 12 26 1200
DeeBERT 12L-256H 12 256 4 64 1024 X
PABEE 12L-256H 12 256 4 64 1024 X
COST-EFF 8 12 768 2 64 256 X
BERT 6L-768H 6 768 12 64 3072
TinyBERT 6 6 768 12 64 3072
DistilBERT 6 6 768 12 64 3072
ElasticBERT 6L 6 768 12 64 3072 X
DeeBERT 12L-512H 12 512 8 64 2048 X
PABEE 12L-512H 12 512 8 64 2048 X
COST-EFF 2 12 768 6 64 1536 X

Bảng 2: Cài đặt của các mô hình nén. L là số lớp và H là chiều của các trạng thái ẩn. A biểu thị kích thước MHA là số_đầu × kích_thước_đầu, và kích thước trung gian của FFN là F. Các mô hình có dấu kiểm tra trong cột EE áp dụng thoát sớm.

4.2 Kết quả Thí nghiệm

4.2.1 Kết quả Tổng thể
Kết quả của COST-EFF và các phương pháp so sánh được liệt kê trong Bảng 3. Khi đếm tham số, chúng tôi bao gồm các tham số của embedding và sử dụng kích thước từ vựng 30,522 làm mặc định. FLOP được đánh giá bởi PyTorch profiler với các chuỗi đầu vào được đệm hoặc cắt đến độ dài mặc định 128 token và được tính trung bình theo các tác vụ.

Trong nhóm đầu tiên, các mô hình được nén và gia tốc cao, trong khi hiệu suất được giữ lại ở khoảng 96.5% bởi COST-EFF 8, tốt hơn nhiều so với việc huấn luyện trước và tinh chỉnh thông thường của BERT 8L-256H. Cụ thể, COST-EFF 8 vượt trội hơn TinyBERT 4 trong tất cả bốn tác vụ, cho thấy rằng một mô hình được thu hẹp bảo tồn tất cả các lớp vượt trội hơn một mô hình thấp. Kiến trúc được thu hẹp có nhiều khả năng trích xuất các đặc trưng phân cấp cho các thực thể khó trong khi xử lý nhanh các thực thể đơn giản. Đối với các mô hình lớn hơn, TinyBERT 6 với chưng cất tổng quát có một chút lợi thế so với COST-EFF 2. Trong khi COST-EFF 2 có khối lượng nhỏ hơn TinyBERT 6 và không yêu cầu chưng cất tổng quát, khoảng cách hiệu suất không đáng kể. Đồng thời, TinyBERT 6 không có chưng cất tổng quát bị chi phối bởi COST-EFF 2 cả về hiệu quả và hiệu quả, cho thấy sự cần thiết của chưng cất tổng quát TinyBERT. Tuy nhiên, một nỗ lực lớn được yêu cầu bởi chưng cất tổng quát, huấn luyện trước một mô hình đơn có kích thước và tính toán cố định. Trong trường hợp nhu cầu tính toán thay đổi, việc huấn luyện trước một mô hình khác có thể cực kỳ tốn thời gian. So với TinyBERT, COST-EFF có lợi thế cả về hiệu suất và suy luận linh hoạt.

Để chứng minh hiệu quả của gia tốc động, chúng tôi chọn các thực thể đơn giản từ tập phát triển ngắn hơn (tức là thấp hơn độ dài không đệm trung vị sau khi token hóa). Kết quả trên các thực thể đơn giản thể hiện những cải thiện bổ sung được quy cho suy luận động, khó thu được với các mô hình tĩnh. Đáng chú ý, độ dài ngắn hơn không phải lúc nào cũng chỉ ra sự đơn giản. Đối với các tác vụ entailment như QNLI, đầu vào ngắn hơn sẽ chứa ít thông tin hơn, có thể làm trầm trọng thêm độ khó hiểu của các mô hình ngôn ngữ. Ngoài ra, chúng tôi vẽ các đường cong hiệu suất đối với điểm GLUE và FLOP trong Hình 3 và 4. Các đường cong hiệu suất là hai chiều và thể hiện tính tối ưu của các phương pháp khác nhau. Nhằm thu được mô hình với tính toán và hiệu suất nhỏ hơn, chúng tôi tập trung vào các mô hình ở phần trên bên trái của hình, đó là biên Pareto được vẽ bằng các đường xanh đứt nét.

Như được mô tả trong Hình 3 và 4, cả COST-EFF 8 và COST-EFF 2 đều vượt trội hơn DistilBERT, DeeBERT, PABEE và các baseline BERT. So với TinyBERT và ElasticBERT, COST-EFF nói chung là tối ưu. Chúng tôi thấy rằng việc thoát sớm làm giảm giới hạn trên của hiệu suất NLI, nơi cả COST-EFF 2 và ElasticBERT 6L đều kém hơn TinyBERT 6. Vấn đề này có thể xuất phát từ sự không nhất quán giữa các lớp. Cho rằng các mẫu phức tạp trong tác vụ NLI dựa vào ngữ nghĩa cấp cao, các lớp nông nên phục vụ các lớp sâu hơn thay vì tự giải quyết tác vụ. Tuy nhiên, vấn đề này không ảnh hưởng đến tính tối ưu toàn cục. Như được hiển thị trong Hình 3, COST-EFF 8 có hiệu suất không bị chi phối so với TinyBERT 4 trên QNLI và MNLI, chứng minh tính linh hoạt của phương pháp chúng tôi.

Hiệu suất của các mô hình kết hợp thoát sớm bị ảnh hưởng đáng kể bởi từng lối thoát. Trong Hình 5, chúng tôi vẽ hiệu suất theo lớp của các mô hình với thoát sớm trong nhóm đầu tiên và hiệu suất cuối cùng của TinyBERT 4. COST-EFF 8 đạt hiệu suất chiếm ưu thế so với DeeBERT và PABEE. So với TinyBERT 4, COST-EFF 8 có thể đạt hiệu suất tốt hơn từ lớp 7 đến 12, tiếp tục xác minh tuyên bố của chúng tôi rằng các mô hình thanh mảnh vượt trội hơn các mô hình thấp về hiệu suất, hưởng lợi từ kiến trúc được bảo tồn và khả năng trích xuất ngữ nghĩa cấp cao. Một cách khác để thu được các mô hình đa lối thoát mạnh mẽ là thay đổi xương sống từ BERT sang ElasticBERT được huấn luyện trước (Liu et al., 2022). Về mặt công bằng, chúng tôi thống nhất sử dụng BERT làm xương sống của COST-EFF và các phương pháp so sánh. Đáng chú ý, phương pháp của chúng tôi thích ứng tốt với ElasticBERT và hiệu suất tiên tiến được thể hiện trong Phụ lục A.

4.2.2 Nghiên cứu Ablation

Tác động của chưng cất kiến thức Các thí nghiệm ablation của các chiến lược chưng cất nhằm đánh giá hiệu quả của chưng cất dự đoán và đặc trưng. Trong nghiên cứu ablation này, các phương pháp so sánh là (1) ablation chưng cất đặc trưng và (2) thay thế chưng cất dự đoán bằng huấn luyện nhãn sự thật cơ bản. Kết quả được hiển thị trong Bảng 4 chỉ ra rằng cả hai mục tiêu đều quan trọng.

Quy cho việc bắt chước các biểu diễn ẩn, COST-EFF 8 có lợi thế 1.6% về hiệu suất so với huấn luyện không có chưng cất đặc trưng. Không có chưng cất dự đoán, hiệu suất giảm hơn 3.4%. Các công trình trước về nén tĩnh, ví dụ TinyBERT (Jiao et al., 2020) và CoFi (Xia et al., 2022), nói chung không nhạy cảm với chưng cất dự đoán trong các tác vụ GLUE, vì phân phối đầu ra của mô hình giáo viên đơn lối thoát nói chung phù hợp với nhãn sự thật cơ bản. Tuy nhiên, một sự giảm lớn trong hiệu suất COST-EFF được quan sát trong Bảng 4 nếu phân phối dự đoán bị ablation. Kết quả chỉ ra rằng việc theo đuổi sự thật cơ bản ở các lớp nông có thể làm xấu đi hiệu suất của các lớp sâu. Sự không nhất quán như vậy giữa các lớp nông và sâu thường tồn tại trong các mô hình thoát sớm, đặc biệt khó cân bằng bởi các mô hình nén với khả năng nhỏ. Thay vào đó, COST-EFF đưa vào một mô hình TA không nén để giảm thiểu mâu thuẫn ở giai đoạn đầu và chuyển giao sự cân bằng thông qua chưng cất dự đoán.

Mô hình SST-2 MRPC QNLI
COST-EFF 8 90.6 87.1 87.8
∅Lfeat 87.5 86.8 86.4
∅Lpred 88.6 82.4 84.2

Bảng 4: Kết quả ablation trên tập phát triển GLUE với nén 8×. Chưng cất đặc trưng được ablation trong ∅Lfeat, trong khi nhãn sự thật cơ bản được sử dụng để thay thế chưng cất dự đoán trong ∅Lpred. FLOP của hai phương pháp ablation được đảm bảo nhiều hơn COST-EFF 8.

Tác động của huấn luyện cộng tác Trong bài báo này, chúng tôi đề xuất một phương pháp cộng tác cho việc thu hẹp mô hình và huấn luyện thoát, nhằm hiệu chỉnh việc tỉa các mô-đun nông. Để xác thực hiệu quả của chiến lược huấn luyện, chúng tôi ablation huấn luyện cộng tác tại các thời điểm khác nhau. Trước tiên, chúng tôi triển khai một chế độ huấn luyện hai giai đoạn như DeeBERT làm. Ngoài ra, chúng tôi triển khai COST-EFF 8 với mất mát thoát được ablation trước và trong quá trình thu hẹp. Việc so sánh theo lớp của các phương pháp trên được hiển thị trong Hình 6.

Một cách trực quan, huấn luyện hai giai đoạn có lợi thế trên lớp cuối cùng so với huấn luyện cộng tác, vì sự không nhất quán giữa các lớp không được đưa vào. Tuy nhiên, lợi thế giảm dần ở các lớp nông, khiến hiệu suất tổng thể không thể chấp nhận được. So với việc thu hẹp không có mất mát thoát, phương pháp của chúng tôi có lợi thế từ 1.1% đến 2.3%. Đáng chú ý, việc thu hẹp không có hiệu chỉnh của thoát vẫn có thể đạt hiệu suất tương tự như COST-EFF ở các lớp nông, cho thấy rằng huấn luyện dựa trên chưng cất có hiệu quả trong việc khôi phục hiệu suất. Tuy nhiên, hiệu suất kém của các lớp sâu chỉ ra rằng sự cân bằng giữa các lớp không được cân bằng tốt, vì việc thu hẹp được tiến hành nhằm tối ưu hóa hiệu suất của bộ phân loại cuối cùng.

5 Kết luận
Trong bài báo này, chúng tôi thu hẹp tĩnh và gia tốc động PLM nhằm theo đuổi hiệu quả suy luận cũng như bảo tồn khả năng. Để tích hợp các góc nhìn, chúng tôi đề xuất một phương pháp tối ưu hóa cộng tác đạt được lợi ích tương hỗ của việc thu hẹp tĩnh và gia tốc động. Cụ thể, kích thước của PLM được giảm trong chiều rộng mô hình, và suy luận có thể thích ứng với độ phức tạp của đầu vào mà không đưa vào sự dư thừa cho đầu vào đơn giản và không đầy đủ cho đầu vào khó. Các thí nghiệm so sánh được tiến hành trên benchmark GLUE và xác minh tính tối ưu Pareto của phương pháp chúng tôi ở tỷ lệ nén và gia tốc cao.

Hạn chế
COST-EFF hiện tại có những hạn chế sau. Nếu chúng được giải quyết trong các công trình tương lai, khả năng tiềm năng của COST-EFF có thể được giải phóng. (1) Trong quá trình suy luận của các mô hình thoát sớm động, thực hành thông thường là đặt kích thước batch là 1 để điều chỉnh tính toán tốt hơn theo các mẫu đầu vào riêng lẻ. Tuy nhiên, cài đặt như vậy không phải lúc nào cũng hiệu quả vì kích thước batch lớn hơn có khả năng giảm thời gian suy luận, trong khi độ phức tạp đầu vào bên trong một batch có thể khác nhau đáng kể. Do đó, việc nghiên cứu một pipeline thu thập các mẫu với kỳ vọng tương tự về độ phức tạp vào một batch trong khi kiểm soát ưu tiên của các batch với độ phức tạp khác nhau để đạt được song song là điều đáng khuyến khích. (2) Chúng tôi chọn các tác vụ hiểu ngôn ngữ tự nhiên (NLU) để nghiên cứu nén và gia tốc theo các baseline mạnh TinyBERT (Jiao et al., 2020) và ElasticBERT (Liu et al., 2022). Tuy nhiên, khả năng mở rộng của COST-EFF vẫn chưa được khám phá trong các tác vụ phức tạp hơn bao gồm tạo ngôn ngữ tự nhiên, dịch thuật, v.v. Cho đến nay, nén mô hình tĩnh được chứng minh là hiệu quả trong các tác vụ phức tạp (Gupta và Agrawal, 2022) và chúng tôi đang tìm kiếm sự mở rộng của gia tốc suy luận động trên các tác vụ khác nhau sử dụng các mô hình với một quá trình lặp.

--- TRANG 4 ---
[Tiếp tục các trang còn lại...]
