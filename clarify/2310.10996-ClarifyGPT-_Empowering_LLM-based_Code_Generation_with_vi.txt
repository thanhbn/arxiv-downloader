vào framework ClarifyGPT. Trong công trình này, chúng tôi chọn hai chat-LLM đại diện (tức là ChatGPT và GPT4) làm mô hình cơ sở để đánh giá framework ClarifyGPT.

• ChatGPT [34] là một trong những mô hình chat mạnh mẽ nhất được OpenAI trao quyền. Nó được huấn luyện bằng một phương pháp mới gọi là Reinforcement Learning from Human Feedback (RLHF), tích hợp một cách liền mạch reinforcement learning và phản hồi của con người. Cụ thể, ChatGPT đầu tiên được huấn luyện với lượng lớn văn bản ngôn ngữ tự nhiên và tệp mã. Sau đó, nó được tinh chỉnh thông qua reinforcement learning, cho phép nó hiểu và thực hiện hướng dẫn của con người một cách thành thạo. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng API của OpenAI để truy cập mô hình ChatGPT, tức là gpt-3.5-turbo.

• GPT-4 [35] là LLM tiên tiến nhất của OpenAI, có thể chấp nhận đầu vào hình ảnh và văn bản, đưa ra đầu ra văn bản. Nó cũng được huấn luyện với reinforcement learning và học cách tuân theo hướng dẫn của con người. GPT-4 đã chứng minh khả năng hiểu ngôn ngữ được cải thiện, cho phép nó hiểu các ngữ cảnh phức tạp và tinh tế, làm cho nó rất hiệu quả trong nhiều nhiệm vụ downstream, bao gồm tóm tắt văn bản, dịch thuật, và tạo sinh mã [6]. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng API của OpenAI để truy cập mô hình GPT-4, tức là gpt-4-turbo.

4.3 Benchmark

Theo công trình trước đây [7,11,17,26], chúng tôi tiến hành thí nghiệm trên bốn benchmark tạo sinh mã công khai: HumanEval [8], MBPP-sanitized [4], cùng với các phiên bản test case mở rộng của chúng (tức là HumanEval-ET và MBPP-ET [10]). Thống kê của những benchmark này được hiển thị trong Bảng 1.

• HumanEval [8] là một bộ dữ liệu giải quyết vấn đề được viết bằng tay được tạo ra sau ngày cắt dữ liệu huấn luyện của Codex, bao gồm 164 bài toán lập trình Python. Các bài toán lập trình trong HumanEval liên quan đến hiểu ngôn ngữ, thuật toán, và toán học. Mỗi bài toán bao gồm một chữ ký hàm, một yêu cầu ngôn ngữ tự nhiên, và một số unit test. Một bài toán được coi là được giải quyết bởi code-LLM khi tất cả unit test đều pass.

• MBPP-sanitized [4] là một tập con được xác minh thủ công của bộ dữ liệu MBPP (Mostly Basic Programming Problems), chứa 427 bài toán lập trình Python được crowdsource, liên quan đến thao tác số, chức năng thư viện tiêu chuẩn, và nhiều hơn nữa. Mỗi bài toán chứa một chữ ký hàm, một yêu cầu người dùng, và ba test case.

• HumanEval-ET và MBPP-ET [10] là hai phiên bản mở rộng của các benchmark HumanEval và MBPP với trung bình 100+ test case bổ sung cho mỗi bài toán. Để cải thiện độ tin cậy của việc đánh giá mã được tạo, chúng thu thập nhiều edge test case không được bao gồm trong các benchmark gốc.

4.4 Metric Đánh giá

Chúng tôi đánh giá độ chính xác của mã được tạo bằng metric Pass@k [22]. Metric này phục vụ như một ước lượng của khả năng tạo sinh dưới một ngân sách cụ thể, được sử dụng rộng rãi trong các nghiên cứu liên quan đến LLM trước đây [7,23,51]. Đối với mỗi bài toán trong các benchmark, chúng tôi tạo ra k giải pháp mã, và nếu bất kỳ giải pháp mã nào trong k giải pháp pass tất cả các test, bài toán này được coi là đã giải quyết. Trong các tình huống phát triển thế giới thực, việc tạo ra k mã sẽ đặt gánh nặng lên các nhà phát triển, tức là họ cần đọc và hiểu k mã khác nhau và chọn một cái làm mã mục tiêu. Do đó, trong bài báo này, k được đặt là 1, thỏa mãn hầu hết các tình huống mà các nhà phát triển chỉ xem xét mã được tạo đơn lẻ [11,17]. Để tránh phương sai cao và tính ngẫu nhiên, chúng tôi chạy mỗi phương pháp ba lần và báo cáo kết quả trung bình làm kết quả cuối cùng.

4.5 Baseline So sánh

• LLM Mặc định: lấy các yêu cầu ban đầu trực tiếp từ benchmark làm đầu vào để nhắc LLM tạo sinh mã.

• CoT (Chain-of-Thought) [47]: tạo ra một chuỗi các bước lý luận cho mỗi yêu cầu bằng cách sử dụng prompt CoT và sau đó tạo ra mã tương ứng. Để đảm bảo tính công bằng của so sánh, baseline CoT có cùng số lượng minh họa (tức là ba minh họa) và seed minh họa.

• GPT-Engineer²: là một kho lưu trữ Github mã nguồn mở gần đây. Nó sử dụng các hướng dẫn được thiết kế thủ công để khơi gợi LLM đặt câu hỏi làm rõ cho các yêu cầu đầu vào của người dùng và sau đó tạo ra các đoạn mã dựa trên phản hồi của người dùng.

4.6 Chi tiết Triển khai

Chi tiết triển khai của việc xây dựng prompt và cấu hình mô hình trong ClarifyGPT như sau.

Xây dựng Prompt. Vì bốn benchmark không có tập huấn luyện, theo công trình trước đây [44,47], chúng tôi chọn ba bài toán đầu tiên từ mỗi benchmark và trích xuất các yêu cầu người dùng từ những bài toán này làm seed minh họa. Sau đó, chúng tôi tạo thủ công các minh họa khác biệt cho các prompt khác nhau, như được minh họa trong Hình 3. Cần lưu ý rằng lý do chúng tôi chỉ tạo ba minh họa cho mỗi prompt là do giới hạn độ dài đầu vào của LLM.

Cấu hình Mô hình. Chúng tôi coi hai LLM được sử dụng trong thí nghiệm như các trình tạo black box và chỉ đặt một số tham số giao diện mà chúng cung cấp mà không truy cập các tham số nội bộ. Đối với tất cả LLM, chúng tôi đặt top p thành 0.95, frequency_penalty thành 0. max_tokens đại diện cho số lượng tối đa token được tạo ra, được đặt thành 800 cho prompt đặt câu hỏi làm rõ và 300 cho các prompt khác. Cụ thể, chúng tôi đặt temperature thành 0, ngoại trừ khi lấy mẫu các giải pháp mã, temperature được đặt thành 0.8. Chúng tôi theo Chen et al. [8] để cắt bớt nội dung được tạo trong HumanEval và MBPP bằng năm chuỗi dừng: "\nclass", "\ndef", "\n#", "\nif", và "\nprint".

5 KẾT QUẢ VÀ PHÂN TÍCH

5.1 RQ1: ClarifyGPT hoạt động như thế nào khi nhận phản hồi thực tế của người dùng so với các phương pháp baseline?

Thiết lập. Trong RQ này, chúng tôi khám phá cách ClarifyGPT hoạt động trong các tình huống thế giới thực, tức là liệu ClarifyGPT có thể đạt được hiệu suất cao hơn các baseline tạo sinh mã hiện có khi nhận phản hồi thực tế của người dùng hay không. Cụ thể, chúng tôi áp dụng ClarifyGPT cho mô hình GPT-4. Vì benchmark MBPP-ET chia sẻ cùng yêu cầu người dùng với MBPP-sanitized, chúng tôi chỉ áp dụng ClarifyGPT cho các phiên bản gốc của benchmark (tức là MBPP-sanitized) và báo cáo hiệu suất của ClarifyGPT trên hai benchmark này sử dụng các unit test tương ứng của chúng. ClarifyGPT đầu tiên lấy yêu cầu người dùng của mỗi bài toán trong benchmark làm đầu vào và xác định chúng là mơ hồ hoặc không mơ hồ. Sau đó, nó tạo ra các câu hỏi làm rõ cho các yêu cầu mơ hồ (như được hiển thị trong Hình 1). Tổng cộng, chúng tôi thu được 140 bài toán với yêu cầu mơ hồ từ benchmark MBPP-sanitized. Số lượng câu hỏi làm rõ trung bình cho mỗi bài toán là 2.85. Chúng tôi tạo ra ba bảng câu hỏi giống hệt nhau cho mỗi bài toán, đảm bảo rằng mỗi bài toán sẽ được đánh giá bởi ba người tham gia khác nhau. Mỗi bảng câu hỏi bao gồm ba yếu tố: (1) yêu cầu (mơ hồ) của bài toán, mô tả ý định của bài toán; (2) các unit test case chứa các ví dụ đầu vào-đầu ra mong đợi, hỗ trợ người tham gia hiểu ý định của bài toán; (3) các câu hỏi làm rõ được tạo, mà người tham gia được yêu cầu trả lời.

Chúng tôi tuyển mười người tham gia, bao gồm ba sinh viên tiến sĩ, hai học viên thạc sĩ, hai nhà nghiên cứu cao cấp, và ba nhà phát triển trong ngành. Không ai trong số họ là đồng tác giả của bài báo này. Tất cả người tham gia có ít nhất ba năm kinh nghiệm phát triển Python, với sáu người trong số họ có hơn năm năm kinh nghiệm. Người tham gia ban đầu được cung cấp mô tả nhiệm vụ và bảng câu hỏi ví dụ chứa các câu trả lời phù hợp. Sau khi hoàn thành bài tập luyện tập, chúng tôi giao 42 bài toán cho mỗi người tham gia và yêu cầu họ trả lời các câu hỏi làm rõ dựa trên thông tin được cung cấp trong bảng câu hỏi. Mỗi bài toán sẽ được giải quyết bởi ba người tham gia.

Chúng tôi thu thập các câu trả lời do người tham gia cung cấp và đưa chúng vào ClarifyGPT để tạo ra các giải pháp mã cuối cùng. Như đã đề cập trước đó, chúng tôi đánh giá độ chính xác của mã được tạo trên hai benchmark bằng các unit test case. Vì các câu hỏi làm rõ của mỗi bài toán được trả lời bởi ba người tham gia, chúng tôi báo cáo kết quả Pass@1 trung bình.

Kết quả. Kết quả so sánh giữa hiệu suất của ClarifyGPT và các baseline khác được mô tả trong Bảng 2. Các giá trị màu đỏ là những cải thiện tương đối của ClarifyGPT (Human Feedback) so với baseline Default.

Chúng ta có thể thấy rằng ClarifyGPT (Human Feedback) đạt được hiệu suất cao nhất trên tất cả bốn benchmark. So với Default, ClarifyGPT (Human Feedback) chứng minh hiệu suất vượt trội về metric Pass@1, đạt được mức tăng 13.87% trên MBPP-sanitized và 16.83% trên MBPP-ET. Hơn nữa, khi so sánh với các baseline hoạt động tốt nhất (tức là CoT hoặc GPT-Engineer), ClarifyGPT (Human Feedback) cũng cải thiện hiệu suất Pass@1 lần lượt 9.53% trên MBPP-sanitized và 9.52% trên MBPP-ET. Điều này chủ yếu là do ClarifyGPT có thể thành thạo xác định các yêu cầu mơ hồ và đưa ra các câu hỏi làm rõ có mục tiêu. Người dùng dễ dàng làm rõ ý định của họ bằng cách trả lời những câu hỏi này, do đó tạo thuận lợi cho việc tạo ra mã chính xác hơn bởi LLM. Điều này chỉ ra rằng ClarifyGPT, như một framework tạo sinh mã tương tác, có thể hỗ trợ các nhà phát triển viết mã trong các bối cảnh phát triển thế giới thực.

Trả lời RQ1: Trong đánh giá của con người, ClarifyGPT nâng cao hiệu suất (Pass@1) của GPT-4 trên MBPP-sanitized từ 70.96% lên 80.8%; nâng cao hiệu suất của nó trên MBPP-ET từ 51.52% lên 60.19%. Cải thiện tương đối là 15.35% trung bình, vượt trội hơn các baseline.

5.2 RQ2: ClarifyGPT hoạt động như thế nào khi nhận phản hồi mô phỏng của người dùng so với các phương pháp baseline hiện đại?

Thiết lập. Do sự tham gia của người tham gia con người, việc đánh giá framework tạo sinh mã tương tác ClarifyGPT rất tốn kém và khó tái tạo. Một giải pháp tương đối đơn giản là tiến hành đánh giá offline [3]. Tuy nhiên, nó giới hạn hệ thống trong việc chọn các câu hỏi làm rõ từ một tập hợp các câu hỏi được định nghĩa trước hoặc được gán nhãn, điều này không chuyển giao tốt sang môi trường phát triển thực tế. Trong RQ này, chúng tôi áp dụng phương pháp Mô phỏng Người dùng để Đánh giá [15,39] để tạo thuận lợi cho việc đánh giá tự động của ClarifyGPT trên các LLM và benchmark khác nhau, loại bỏ sự cần thiết của việc tham gia trực tiếp của người dùng.

Khía cạnh quan trọng nhất của việc mô phỏng phản hồi người dùng là đảm bảo rằng phản hồi người dùng được tạo ra gần giống với phản hồi thực tế mà người dùng sẽ cung cấp trong cùng môi trường. Các mô phỏng độ trung thực thấp có thể dẫn đến ClarifyGPT nhận được phản hồi khó gặp trong thực tế thực tế, do đó mang lại kết quả gây hiểu lầm và ảnh hưởng đến đánh giá của chúng tôi về hiệu suất của ClarifyGPT. Do đó, chúng tôi đề xuất một phương pháp mô phỏng người dùng độ trung thực cao tận dụng LLM để tạo ra phản hồi của người dùng bằng cách cung cấp cho LLM các câu hỏi làm rõ và ground-truth test case. Hiểu biết chính của chúng tôi là các ground-truth test case chứa các ví dụ đầu vào-đầu ra mong đợi, phản ánh chức năng mong muốn mà người dùng tìm kiếm. Việc trao cho LLM kiến thức tiên nghiệm này tạo thuận lợi cho việc hiểu ý định của người dùng và cho phép tạo ra phản hồi người dùng mô phỏng độ trung thực cao. Để hướng dẫn LLM giải quyết nhiệm vụ này, chúng tôi thiết kế một prompt (như được hiển thị trong Hình 3), cũng bao gồm ba phần: (1) một hướng dẫn, mô tả nhiệm vụ (tức là mô phỏng phản hồi của người dùng) mà chúng tôi muốn LLM giải quyết; (2) các bộ tứ <yêu cầu, ground-truth test, câu hỏi làm rõ, câu trả lời> few-shot như minh họa, giúp LLM hiểu và giải quyết nhiệm vụ; (3) một truy vấn, chứa một yêu cầu người dùng và ground-truth test của nó, được đưa vào LLM để tạo ra phản hồi mô phỏng.

Chúng tôi áp dụng ba baseline (Mục 4.5) và ClarifyGPT của chúng tôi cho hai SOTA LLM (Mục 4.2). Chúng tôi đánh giá chúng trên bốn benchmark (Mục 4.3) và so sánh hiệu suất của chúng bằng cách tính toán metric Pass@1 (Mục 4.4). Để so sánh công bằng, tất cả baseline áp dụng cùng thiết lập thí nghiệm như ClarifyGPT của chúng tôi.

Kết quả. Bảng 3 trình bày kết quả so sánh giữa hiệu suất của ClarifyGPT (Simulated Feedback) và các baseline khác về việc tạo sinh mã. Các giá trị màu đỏ là những cải thiện tương đối của ClarifyGPT (Simulated Feedback) so với baseline Default.

Nhìn chung, ClarifyGPT (Simulated Feedback) có thể cải thiện đáng kể hiệu suất của việc tạo sinh mã, đạt được mức tăng trên các LLM và dataset khác nhau. Đối với mô hình GPT-4, so với baseline Default, ClarifyGPT (Simulated Feedback) chứng minh những cải thiện đáng chú ý trong hiệu suất Pass@1, đạt được mức tăng 11.34% trên dataset HumanEval, 10.35% trên HumanEval-ET, 10.89% trên MBPP-sanitized, và 13.49% trên MBPP-ET. Đối với mô hình ChatGPT, khi so sánh với baseline Default, ClarifyGPT (Simulated Feedback) cải thiện hiệu suất Pass@1 lần lượt 15.10%, 13.12%, 12.98%, và 19.07% trên bốn benchmark. Kết quả chứng minh rằng ClarifyGPT, trao quyền cho LLM tự động tạo ra câu hỏi làm rõ và tinh chỉnh yêu cầu người dùng dựa trên phản hồi của người dùng, tạo thuận lợi cho người dùng làm rõ ý định của họ, do đó nâng cao hiệu suất tạo sinh mã bằng cách nắm bắt ý định của người dùng.

Chúng tôi cũng lưu ý rằng, so với baseline liên quan nhất (tức là GPT-Engineer), thể hiện hiệu suất vượt trội về metric Pass@1, đạt được cải thiện trung bình 11.45%, 8.65%, 6.95%, và 8.56% trên bốn benchmark. Chúng tôi quy những cải thiện này cho các kỹ thuật mới của chúng tôi, tức là xác định yêu cầu mơ hồ và tạo sinh câu hỏi làm rõ. Việc đặt câu hỏi làm rõ cho mọi yêu cầu của người dùng dẫn đến các tương tác LLM-Human không cần thiết về các yêu cầu không mơ hồ, điều này đặt gánh nặng bổ sung lên người dùng và làm hại hiệu suất tạo sinh mã khi tạo ra các câu hỏi ngoài chủ đề. Trong khi ClarifyGPT có thể xác định hiệu quả các yêu cầu mơ hồ mà không cần bất kỳ huấn luyện có giám sát nào bằng cách tiến hành kiểm tra tính nhất quán mã. Các đoạn mã không nhất quán được lấy làm đầu vào để giúp ClarifyGPT xây dựng các câu hỏi có mục tiêu hướng dẫn người dùng làm rõ sự mơ hồ.

Bên cạnh đó, chúng tôi quan sát thấy rằng hiệu suất của ClarifyGPT (Human Feedback) cao hơn một chút so với ClarifyGPT (Simulated Feedback). Điều này cho thấy rằng phương pháp mô phỏng người dùng của chúng tôi có thể tạo ra các phản hồi của người dùng không thỏa mãn ý định của người dùng. Tuy nhiên, cả hai phương pháp đều có thể cải thiện đáng kể hiệu suất của việc tạo sinh mã và đạt được mức tăng nhất quán trên các LLM và benchmark khác nhau, chứng minh độ tin cậy của kết quả đánh giá phương pháp mô phỏng của chúng tôi.

Trả lời RQ2: ClarifyGPT (Simulated Feedback) cải thiện hiệu suất trung bình (Pass@1) của GPT-4 trên bốn benchmark từ 68.02% lên 75.75%, cải thiện hiệu suất trung bình của ChatGPT trên bốn benchmark từ 58.55% lên 67.22%. Cải thiện tương đối của chúng lần lượt là 11.52% và 15.07%, và cải thiện trung bình là 13.27%.

5.3 RQ3: Số lượng minh họa trong một prompt ảnh hưởng như thế nào đến hiệu suất của ClarifyGPT?

Thiết lập. Trong RQ này, chúng tôi điều tra liệu việc tăng hoặc giảm số lượng minh họa có ảnh hưởng đến hiệu suất của ClarifyGPT (Simulated Feedback) trong nhiệm vụ tạo sinh mã hay không. Cụ thể, do giới hạn độ dài đầu vào của LLM, chúng tôi thay đổi số lượng minh họa trong prompt từ không đến ba. Sau đó, chúng tôi áp dụng hai LLM cho ClarifyGPT và các biến thể của nó, và đánh giá hiệu suất của chúng trên bốn benchmark. Chúng tôi chạy những phương pháp này ba lần và báo cáo kết quả Pass@1 trung bình làm báo cáo cuối cùng.

Kết quả. Bảng 4 trình bày so sánh hiệu suất giữa ClarifyGPT và các biến thể của nó. Nhìn chung, ClarifyGPT chứng minh tính bền vững đối với số lượng minh họa trong các prompt. Khi thay đổi số lượng minh họa từ không đến ba, ClarifyGPT liên tục vượt trội hơn baseline Default trên hai LLM và bốn benchmark.

Chúng ta có thể quan sát thấy rằng, như mong đợi, hiệu suất của ClarifyGPT tăng theo số lượng minh họa. Cụ thể, khi số lượng minh họa trong prompt được tăng từ không lên ba, liên quan đến ChatGPT, ClarifyGPT đạt được mức tăng hiệu suất trung bình từ 59.77% lên 67.22% trên bốn benchmark. Đối với mô hình GPT-4, hiệu suất trung bình của ClarifyGPT tăng từ 68.59% lên 75.75%. Điều này chủ yếu là do nhiều minh họa hơn có thể cung cấp nhiều tình huống và thông tin khác nhau cho LLM, cho phép chúng hiểu rõ hơn ngữ cảnh của vấn đề và giải pháp được yêu cầu. Hơn nữa, LLM có thể học cách tổng quát hóa tốt hơn thông qua các minh họa, tức là suy ra giải pháp cho một tình huống mới từ một minh họa đã biết. Điều này cho phép LLM thích ứng tốt hơn với các đầu vào và yêu cầu khác nhau.

Chúng tôi cũng nhận thấy rằng hiệu suất của ClarifyGPT trong thiết lập zero-shot thể hiện cải thiện nhỏ so với baseline Default, trong khi hiệu suất của nó trong thiết lập one-shot được nâng cao đáng kể so với baseline Default. Chúng tôi quy sự khác biệt này cho thực tế là trong thiết lập zero-shot, ClarifyGPT được mong đợi tạo ra các phản hồi có ý nghĩa mà không có bất kỳ minh họa nào, điều này có thể đặc biệt thách thức đối với các nhiệm vụ phức tạp (ví dụ: yêu cầu LLM tạo ra các câu hỏi làm rõ có mục tiêu). Hơn nữa, zero-shot prompting chỉ dựa vào kiến thức được huấn luyện trước của LLM và cách diễn đạt của các prompt được cho, có thể không cung cấp đủ hướng dẫn hoặc ràng buộc cho LLM để tạo ra các phản hồi chính xác hoặc liên quan đến ngữ cảnh. Ngược lại, hiệu suất của ClarifyGPT với thiết lập one-shot cao hơn đáng kể so với thiết lập zero-shot và gần với hiệu suất của ClarifyGPT với thiết lập three-shot. Điều này chỉ ra rằng ClarifyGPT có hiệu suất tổng quát hóa mạnh khi chỉ có một minh họa được cung cấp. Chúng tôi tin rằng trong các tình huống sử dụng thực tế, việc sử dụng ClarifyGPT trong thiết lập one-shot có thể đóng vai trò như một sự đánh đổi giữa hiệu quả và hiệu suất.

Trả lời RQ3: Nhìn chung, ClarifyGPT chứng minh tính bền vững đối với số lượng minh họa trong các prompt. Khi thay đổi số lượng minh họa từ không đến ba, ClarifyGPT liên tục vượt trội hơn baseline Default trên hai LLM và bốn benchmark.

6 THẢO LUẬN

6.1 Nghiên cứu Tình huống

Để đánh giá thêm hiệu quả của phương pháp của chúng tôi, chúng tôi tiến hành phân tích định tính. Như được hiển thị trong Hình 4, chúng tôi chọn hai ví dụ đại diện từ hai benchmark tạo sinh mã phổ biến (tức là HumanEval và MBPP). Mỗi yêu cầu đầu vào bao gồm một chữ ký hàm và một mô tả NL. Chúng tôi lấy ChatGPT [34] làm mô hình cơ sở và sử dụng hai baseline (tức là Default và GPT-Engineer) và ClarifyGPT để tạo ra một giải pháp mã cho mỗi yêu cầu đầu vào.

Đối với ví dụ đầu tiên lấy từ MBPP-sa, mô tả "viết một hàm để sắp xếp một danh sách các phần tử" không chỉ rõ liệu hàm này nên được sắp xếp theo thứ tự tăng dần hay giảm dần. ChatGPT mặc định trực tiếp tạo ra một giải pháp mã sắp xếp danh sách đã cho theo thứ tự giảm dần, không thể pass các ground-truth test case. GPT-Engineer đặt năm câu hỏi làm rõ và tạo ra một giải pháp mã chính xác dựa trên phản hồi của người dùng. Tuy nhiên, một số trong những câu hỏi đó không có thông tin và có thể được trả lời bằng thông tin trong yêu cầu đã cho. Ví dụ, câu trả lời cho câu hỏi thứ ba "Có ràng buộc cụ thể nào cho thuật toán sắp xếp không?" có thể được suy ra từ tên hàm comb_sort được đề cập trong chữ ký hàm. Câu hỏi thứ năm "Có ngôn ngữ lập trình ưa thích nào không?" cũng có vẻ tầm thường, vì chúng ta có thể dễ dàng biết rằng hàm nên được triển khai bằng Python dựa trên cú pháp của chữ ký hàm. Việc trả lời những câu hỏi này không thể thu được thông tin bổ sung; thay vào đó, nó tạo ra các cuộc đối thoại thừa có hại cho trải nghiệm người dùng. Hơn nữa, nó dẫn đến việc tăng số lượng token cho cả đầu vào và đầu ra của LLM, do đó làm tăng chi phí hoạt động. Ngược lại, ClarifyGPT có thể xác định các điểm mơ hồ trong yêu cầu bằng cách so sánh các triển khai mã khác nhau, do đó đặt câu hỏi làm rõ có mục tiêu. Kết quả là, ClarifyGPT chỉ đặt một câu hỏi "Việc sắp xếp nên theo thứ tự tăng dần hay giảm dần?" và tạo ra một giải pháp mã chính xác.

Đối với ví dụ thứ hai, yêu cầu người dùng được định nghĩa rõ ràng. ChatGPT mặc định tạo ra một giải pháp chính xác, trong khi GPT-Engineer tạo ra một giải pháp mã không chính xác. Sự khác biệt này chủ yếu phát sinh từ việc GPT-Engineer không thể xác định liệu một yêu cầu có mơ hồ hay không. Do đó, ngay cả đối với yêu cầu không mơ hồ này, GPT-Engineer vẫn đặt ba câu hỏi, hóa ra không có thông tin. Hơn nữa, những câu hỏi này góp phần làm cho prompt đã tinh chỉnh trở nên quá dài, có thể gây nhầm lẫn cho LLM. Ngược lại, ClarifyGPT của chúng tôi có thể xác định liệu một yêu cầu có cần làm rõ hay không bằng cách tiến hành kiểm tra tính nhất quán mã. Vì vậy chúng ta có thể thấy ClarifyGPT không đưa ra bất kỳ câu hỏi nào cho yêu cầu này mà thay vào đó trực tiếp tạo ra một giải pháp chính xác.

6.2 Lợi ích và Hạn chế

Trong mục này, chúng tôi thảo luận về một số lợi ích và hạn chế tiềm ẩn của ClarifyGPT.

Lợi ích. (1) Ngược lại với các phương pháp tạo sinh mã dựa trên LLM hiện tại [7,23,27] tận dụng các kỹ thuật xử lý hậu kỳ để lấy mẫu một pool ứng viên mã đáng kể và sau đó chọn một cái, ClarifyGPT nhằm mục đích trực tiếp làm rõ các yêu cầu đầu vào bằng cách đặt câu hỏi làm rõ. Do đó, framework của chúng tôi góp phần vào việc tăng cường khả năng diễn giải trong mã được tạo bởi LLM. Bằng cách làm rõ các chi tiết cụ thể trong yêu cầu hoặc thêm kiến thức bổ sung vào chúng, người dùng có thể dễ dàng phân biệt các thay đổi tương ứng trong mã kết quả. Điều này góp phần cung cấp cho người dùng hướng dẫn về cách xây dựng yêu cầu để cải thiện việc tạo sinh mã, do đó tạo thuận lợi cho việc hiểu rõ hơn về mã được tạo. (2) ClarifyGPT của chúng tôi cải thiện các kỹ năng tương tác của LLM bằng cách trao quyền cho chúng khả năng tự động đặt câu hỏi làm rõ cho các yêu cầu mơ hồ. Theo cách này, nó phục vụ để tạo thuận lợi cho người dùng xác định sự mơ hồ trong yêu cầu và cung cấp hướng dẫn trong việc làm rõ ý định của họ mà không yêu cầu người dùng ban đầu tạo ra mã và sau đó đọc và phân tích mã để tinh chỉnh yêu cầu. Do đó, ClarifyGPT nâng cao trải nghiệm người dùng và hiệu quả sản xuất.

Hạn chế. (1) Lý tưởng nhất, framework của chúng tôi có thể áp dụng cho tất cả LLM. Tuy nhiên, ClarifyGPT đòi hỏi LLM phải có một mức độ năng lực giao tiếp nhất định, tức là khả năng hiểu hướng dẫn của con người và xây dựng câu hỏi làm rõ. Do đó, các LLM có thể áp dụng cho framework của chúng tôi bị hạn chế, tức là các LLM không có instruction tuning (ví dụ: InCoder [13] và CodeGen [33]) không phù hợp làm mô hình cơ sở được áp dụng cho framework ClarifyGPT. (2) Do việc sử dụng kiểm tra tính nhất quán mã để xác định liệu một yêu cầu có cần làm rõ hay không, ClarifyGPT được yêu cầu tạo ra đầu vào kiểm tra cho yêu cầu và so sánh đầu ra kiểm tra của các giải pháp được lấy mẫu. Do đó, ClarifyGPT không phù hợp để tạo ra mã với đầu vào phức tạp (ví dụ: hình ảnh hoặc tệp). Ngoài ra, đối với một số mã không trả về giá trị đầu ra (ví dụ: chương trình deep learning), việc sử dụng ClarifyGPT cũng có thể chịu một số hạn chế.

6.3 Mối đe dọa đến Tính hợp lệ

Mối đe dọa đầu tiên đến tính hợp lệ là khả năng rò rỉ dữ liệu. Vì những LLM này được huấn luyện trên các kho lưu trữ mã nguồn mở, có thể một số benchmark công khai đã được bao gồm trong dữ liệu huấn luyện của chúng. Điều này có thể làm thiên lệch đánh giá của chúng tôi về phương pháp được đề xuất, vì một số đầu ra mô hình có thể bị ảnh hưởng bởi việc tiếp xúc trước với những benchmark này. Để giảm thiểu mối đe dọa này, chúng tôi cẩn thận chọn HumanEval [8], MBPP-sanitized [4], và các phiên bản mở rộng tương ứng của chúng cho đánh giá của chúng tôi. HumanEval là một bộ dữ liệu giải quyết vấn đề được tạo thủ công, được OpenAI giới thiệu để đánh giá hiệu suất của Codex. MBPP-sanitized, mặt khác, là một tập con được xác minh thủ công của bộ dữ liệu MBPP, bao gồm 427 bài toán Python đã trải qua xác minh crowdsource. Những dataset này đã trải qua xem xét thủ công tỉ mỉ và đã được sử dụng rộng rãi trong các nghiên cứu trước đây [7, 23, 46].

Mối đe dọa thứ hai đến tính hợp lệ là mô phỏng người dùng để đánh giá. Do sự tham gia của người tham gia con người, việc đánh giá ClarifyGPT, một framework tạo sinh mã tương tác, rất tốn kém và khó tái tạo. Do đó, chúng tôi đề xuất một phương pháp mô phỏng người dùng để tạo thuận lợi cho việc đánh giá tự động của ClarifyGPT trên các LLM và benchmark khác nhau. Tuy nhiên, các mô phỏng độ trung thực thấp có thể dẫn đến ClarifyGPT nhận được phản hồi khó gặp trong thực tế thực tế, do đó mang lại kết quả gây hiểu lầm và ảnh hưởng đến đánh giá của chúng tôi về hiệu suất của ClarifyGPT. Để giảm thiểu mối đe dọa này, chúng tôi thiết kế một prompt đặc biệt để cung cấp cho LLM các câu hỏi làm rõ và ground-truth test case. Bằng cách trao cho LLM kiến thức tiên nghiệm này, ClarifyGPT tạo thuận lợi cho việc hiểu ý định của người dùng của LLM và cho phép tạo ra phản hồi người dùng mô phỏng độ trung thực cao. Kết quả cho thấy rằng hiệu suất của ClarifyGPT (Simulated Feedback) rất gần với ClarifyGPT (Human Feedback), chứng minh rằng phương pháp mô phỏng được đề xuất của chúng tôi có thể đóng vai trò như một proxy tốt cho việc đánh giá tự động của ClarifyGPT, loại bỏ sự cần thiết của việc tham gia trực tiếp của người dùng.

Mối đe dọa thứ ba liên quan đến khả năng tổng quát hóa của kết quả thí nghiệm của chúng tôi. Để giải quyết vấn đề này, một mặt, chúng tôi đã cẩn thận chọn hai chat LLM đại diện (ChatGPT và GPT-4) làm mô hình cơ sở và bốn dataset phổ biến làm đối tượng đánh giá. Chúng tôi áp dụng hai LLM cho ClarifyGPT của chúng tôi và đánh giá hiệu suất của chúng trên bốn dataset này. Mặt khác, xem xét độ nhạy cảm vốn có của LLM đối với prompt, chúng tôi chạy baseline và ClarifyGPT ba lần để giúp giảm thiểu phương sai cao và tính ngẫu nhiên. Chúng tôi báo cáo kết quả trung bình làm kết quả cuối cùng. Kết quả cho thấy rằng ClarifyGPT của chúng tôi có thể cải thiện đáng kể hiệu suất của tất cả LLM, đạt được mức tăng nhất quán trên các dataset khác nhau. Do đó, chúng tôi tin rằng ClarifyGPT có khả năng tổng quát hóa tốt, và có thể hoạt động hiệu quả trong nhiều bối cảnh liên quan.

7 KẾT LUẬN

Trong bài báo này, được thúc đẩy bởi quan sát rằng các nhà phát triển con người thường đặt câu hỏi làm rõ khi họ đối mặt với các yêu cầu mơ hồ, chúng tôi cho rằng việc trao quyền cho LLM khả năng tự động làm rõ các yêu cầu mơ hồ có thể cải thiện việc tạo sinh mã. Để đạt được mục tiêu này, chúng tôi đề xuất ClarifyGPT, một framework tạo sinh mã cho phép LLM xác định các yêu cầu mơ hồ và tạo ra các câu hỏi làm rõ có mục tiêu. Cụ thể, ClarifyGPT bao gồm bốn giai đoạn chính, tức là tạo sinh đầu vào kiểm tra, kiểm tra tính nhất quán mã, tạo sinh câu hỏi dựa trên lý luận, và tạo sinh mã nâng cao. Đối với một yêu cầu nhất định, ClarifyGPT đầu tiên tạo ra các đầu vào kiểm tra chất lượng cao bằng cách sử dụng các kỹ thuật prompting và đột biến heuristic. Sau đó, nó sử dụng các đầu vào kiểm tra đã tạo để tiến hành đánh giá tính nhất quán và xác định các yêu cầu mơ hồ. Tiếp theo, ClarifyGPT xây dựng các câu hỏi làm rõ có mục tiêu cho các yêu cầu mơ hồ đã xác định bằng cách nhắc LLM tham gia vào lý luận trung gian. Cuối cùng, nó kết hợp các câu hỏi làm rõ và phản hồi của chúng để tinh chỉnh yêu cầu ban đầu và tạo ra giải pháp mã cuối cùng dựa trên prompt đã tinh chỉnh. Trong phần đánh giá, trước tiên chúng tôi áp dụng GPT-4 cho ClarifyGPT và tuyển mười người tham gia để đánh giá hiệu suất của nó trên hai benchmark công khai. Kết quả đánh giá của con người cho thấy ClarifyGPT đạt được cải thiện tương đối lên đến 16.83% trong Pass@1 so với baseline Default. Ngoài ra, để tự động hóa việc đánh giá ClarifyGPT, chúng tôi giới thiệu một phương pháp mô phỏng độ trung thực cao để mô phỏng phản hồi của người dùng. Chúng tôi tiến hành các thí nghiệm toàn diện trên bốn benchmark (tức là HumanEval, HumanEval-ET, MBPP-sanitized, và MBPP-ET) sử dụng hai LLM (tức là GPT-4 và ChatGPT). Kết quả rộng rãi minh họa rằng ClarifyGPT cải thiện hiệu suất trung bình của GPT-4 trên bốn benchmark từ 68.02% lên 75.75%, và cải thiện hiệu suất trung bình của ChatGPT trên bốn benchmark từ 58.55% lên 67.22%. Do đó, chúng tôi tin rằng ClarifyGPT có thể tạo thuận lợi đáng kể cho việc ứng dụng thực tế của LLM trong môi trường phát triển thế giới thực.

TÀI LIỆU THAM KHẢO

[1] 2023. website. https://github.com/ClarifyGPT/ClarifyGPT.
[2] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2021. Unified Pre-training for Program Understanding and Generation. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021. 2655–2668. https://doi.org/10.18653/v1/2021.naacl-main.211
[3]Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, và W Bruce Croft. 2019. Asking clarifying questions in open-domain information-seeking conversations. Trong Proceedings of the 42nd international acm sigir conference on research and development in information retrieval. 475–484.
[4]Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, và Charles Sutton. 2021. Program Synthesis with Large Language Models. CoRR abs/2108.07732 (2021). arXiv:2108.07732 https://arxiv.org/abs/2108.07732
[5]Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, và Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. CoRR abs/2204.06745 (2022). https://doi.org/10.48550/arXiv.2204.06745 arXiv:2204.06745
[6]Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al .2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023).
[7]Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, và Weizhu Chen. 2022. CodeT: Code Generation with Generated Tests. CoRR abs/2207.10397 (2022). https://doi.org/10.48550/arXiv.2207.10397 arXiv:2207.10397
[8]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. CoRR abs/2107.03374 (2021). arXiv:2107.03374 https://arxiv.org/abs/2107.03374
[9]Kaustubh D. Dhole. 2020. Resolving Intent Ambiguities by Retrieving Discriminative Clarifying Questions. CoRR abs/2008.07559 (2020). arXiv:2008.07559 https://arxiv.org/abs/2008.07559

[TIẾP TỤC VỚI PHẦN TÀI LIỆU THAM KHẢO CÒN LẠI...]
