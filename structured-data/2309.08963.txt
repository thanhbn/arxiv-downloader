# 2309.08963.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/structured-data/2309.08963.pdf
# File size: 4670099 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
STRUC -BENCH : Are Large Language Models Good at Generating
Complex Structured Tabular Data?
Xiangru Tang♠Yiming Zong♡Jason Phang♢Yilun Zhao♠Wangchunshu Zhou♠
Arman Cohan♠Mark Gerstein♠
♠Yale University♡Zhejiang University♢New York University
xiangru.tang@yale.edu
Abstract
Despite the remarkable capabilities of Large Lan-
guage Models (LLMs) like GPT-4, producing com-
plex, structured tabular data remains challenging.
Our study assesses LLMs’ proficiency in struc-
turing tables and introduces a novel fine-tuning
method, cognizant of data structures, to bolster
their performance. We unveil STRUC -BENCH ,
a comprehensive benchmark featuring prominent
LLMs (GPT-NeoX-20B, GPT-3.5, GPT-4, and Vi-
cuna), which spans text tables, HTML, and LaTeX
formats. Our proposed FORMAT COTaids in craft-
ing format-specific instructions from the intended
outputs to populate this benchmark. Addressing
the gap in task-centered evaluation, we propose two
innovative metrics, P-Score ( Prompting Score) and
H-Score ( Heuristical Score), to more accurately
gauge LLM performance. Our experiments show
that applying our structure-aware fine-tuning to
LLaMA-7B leads to substantial performance gains,
outshining its LLM counterparts across most mea-
sures. In-depth error analysis and creating an abil-
ity map across six dimensions—coverage, format-
ting, reasoning, comprehension, pragmatics, and
hallucination—highlight areas for future enhance-
ments and suggest forthcoming research trajecto-
ries. Our code and models can be found at https:
//github.com/gersteinlab/Struc-Bench .
1 Introduction
Significant advancements have been made in var-
ious natural language processing tasks by Large
Language Models (LLMs) (Brown et al., 2020;
Scao et al., 2022; Ouyang et al., 2022; Muennighoff
et al., 2022; OpenAI, 2023; Zhao et al., 2023a), es-
pecially in text generation tasks (Qin et al., 2023).
The ability to output structured data, one of the key
aspects of generative capability, has also attracted
great interest in previous studies (Wu et al., 2022;Zhao et al., 2023c,b; Zha et al., 2023).
Despite their advanced capabilities, LLMs have
problems with generating complex structured ta-
bles, an indispensable skill for practical applica-
tions like coding copilot and automated report gen-
eration. This proficiency entails the organization of
information from multifarious sources into coher-
ent structures. Generating structured tables as out-
puts not only helps human understanding but also
facilitates the automated data processing pipeline
in autonomous language agents. Furthermore, gen-
erating structured tables can also serve as a criti-
cal preprocessing procedure for downstream tasks
such as decision-making and knowledge extraction.
However, the current landscape of LLM evaluation
often neglects this aspect of table generation, which
casts uncertainty on their full potential and utility
in such scenarios. Our research seeks to thoroughly
investigate these gaps.
First, there is a lack of systematic analysis and
comprehensive benchmarks of the ability of LLMs
to output complex structured tabular data. Previous
efforts on evaluating LLMs (Qin et al., 2023; Ma
et al., 2023) on structured data primarily centered
around simple Information Extraction (IE) tasks:
recognizing named entities, extracting relations,
and detecting events. Here the goal of IE tasks is
to gather the extracted data in a highly structured
form (Zhong and Chen, 2020). Much earlier work
was considerably more task-centric as opposed to
LLM-centric. The focus was predominantly on gen-
erating structured data from text (text-to-data) tasks
with pre-trained models (He et al., 2023; Rossiello
et al., 2022; Whitehouse et al., 2023; Pietruszka
et al., 2022) like BART (Lewis et al., 2019) and
T5 (Raffel et al., 2020).
Second, there is a lack of evaluation metrics of
structured tabular data generation. Existing bench-
marks often rely on rudimentary objective metrics
such as word overlap to measure the accuracy of
the content generated by the model (Li et al., 2023;arXiv:2309.08963v3  [cs.CL]  4 Apr 2024

--- PAGE 2 ---
Dataset
Curation
FormatCoT self-instruct with
in-context examples
Train LLaMA-7B
Guiding Questions
for PromptingInput:
###Task: Generate a LaTex table from given text
###Text
Input:
###Task: Generate a LaTex table from given text
and format description
###Text
###Format Instruction###Data
Demo/examples:...
###Describe the detailed format of a given latex table according
to the commands and tags with more than 500 words
Whether there are table borderlines?
How is text alignment? 
What are table attributes? 
Whether to bold? 
Whether to add \ref? 
Whether there are horizontal and vertical lines bordering each row
and column? 
Say anything about special \" \ \" format token in latex. Benchmark and
metricsFigure 1: Overview of our workflow: We commenced by creating datasets of raw text tables, HTML tables, and
LATEX tables. Subsequently, LLaMA-7B was trained using the training data constructed by FORMAT COT. Finally,
our benchmarks validate the effectiveness of the current LLMs to generate such tables.
Wu et al., 2022; Pietruszka et al., 2022). This may
be insufficient for evaluating whether LLMs can
generate structured output, as an ideal evaluation
metric ought to also consider the format of gener-
ated content.
Third, there is a lack of methods to enhance the
performance of current LLMs to better follow nat-
ural language inputs and generate tabular outputs
with the correct format.
Our contributions can be encapsulated as out-
lined in Figure 1: (1) We introduce STRUC -BENCH ,
a benchmark specifically constructed for generat-
ing structured tabular data. (2) We evaluate popular
LLMs on STRUC -BENCH using two proposed met-
rics, providing a comprehensive insight into the
prevailing limitations and common error types. (3)
We propose FORMAT COTto generate instruction
tuning data, wherein we utilize GPT-3.5 to generate
format instructions and then fine-tune LLaMA-7B
model to follow these formats. The resulting im-
pressive performance demonstrates that with FOR-
MATCOTsmall models can indeed surpass the per-
formance of a larger model in this particular task.
2 Problem Analysis and Benchmark
2.1 Problem Definition and Motivation
LLMs are tasked with generating complex struc-
tured tables, a process that involves understanding
both the content and the specific format require-
ments, such as LaTeX syntax. This task extends
beyond simple text generation as it demands preci-
sion not just in content creation but also in adhering
to a detailed and precise structural format. Spe-
cially, we aim to convert unstructured textual data
into structured tabular data, by extracting neces-
sary contents from text and following the required
structure or format.
2.2 Problem Analysis
To assess LLMs’ capability to convert textual de-
scriptions to structured tables, we utilized the Ro-
toWire dataset (Wiseman et al., 2017), originally
a table-to-text dataset, in reverse as a text-to-tabletask. After ensuring that the descriptions contained
adequate information for table generation through
a review of 20 samples, we found significant limi-
tations in the performance of GPT-3.5 and GPT-4,
especially when dealing with complex structures
as detailed in Appendix A.
When put to the test of generating data in precise
formats, such as tables, both GPT-3.5 and GPT-
4, despite their advanced capabilities, frequently
erred, as evidenced by a systematic MTurk human
annotation study (refer to Appendix B). The types
of errors, categorized into ‘Element Errors’, ‘Ele-
ment Format Errors’, ‘Structure Error’, and ‘Struc-
ture Naming Errors’, are quantified in Figure 2.
A mere 3% of GPT-3.5’s outputs were fully accu-
rate, with GPT-4 only slightly better at 9%. These
results suggest design limitations within the GPT
architecture, which, although effective at mimick-
ing language patterns, falter in tasks requiring sus-
tained structural coherence over longer sequences.
2.3 Benchmark Construction
We begin by selectively sourcing tables larger than
3x3 from the RotoWire (Wiseman et al., 2017)
dataset to present a baseline of complexity. Then,
to broaden our dataset diversity across various do-
mains, from The Stack (Kocetkov et al., 2022),
which includes GitHub code in 358 programming
languages from the BigCode project, we first se-
lect LaTeX and HTML formats. Further refining
our dataset, we extract elements relevant to table
representations to ensure focused complexity and
relevance to our structured data generation task. An
example of our benchmark is shown in Figure 4.
Table 1 gives statistics for the Rotowire dataset
and our constructed datasets. Then we eval-
uate 4 popular LLMs, including GPT-NeoX-
20B (Black et al., 2022), GPT-3.5, GPT-4, and
Vicuna-13B (Chiang et al., 2023). For LaTeX and
HTML data without paired text, we harness GPT-
3.5 to construct synthetic descriptions to be uti-
lized as input. To guarantee the quality of our
benchmark, we sample 50 tables for each format

--- PAGE 3 ---
9%3%91%97%
0%10%20%30%40%50%60%70%80%90%100%GPT4GPT-3.5
CorrectError82%88%81%89%73%75%79%81%
Element ErrorsElement Format ErrorsStructure Naming ErrorStructure ErrorsFigure 2: Error analysis by human annotation. Some error types are explained in Appendix A.
to ensure the correctness of the descriptions. Ini-
tially, we achieved a satisfaction rate of 76%. How-
ever, upon incorporating a manual interpretation
template (e.g. tab names for HTML) tailored to
each format (Appendix D), our satisfaction rate im-
proved significantly, reaching 96%. For example,
HTML tables possess their unique tags and struc-
ture, conforming faithfully to the syntax rules of
HTML language.
Dataset # Train # Test Format Rows & Columns
STRUC -BENCH Table 3.4k 728 Raw tex 7.26 & 8.75
STRUC -BENCH LATEX 5.3k 500 L ATEX 2.75 & 4.47
STRUC -BENCH HTML 5.4k 499 HTML 5.50 & 3.54
Table 1: STRUC -BENCH data statistics. The number of
Rows & Columns has been averaged.
3 Methodology
3.1 Data Generation
As shown in Figure 3, we propose FORMAT COT
with GPT-3.5, a self-instruct method to generate
{data, instruction} pairs for fine-tuning purposes.
Specifically, our prompt of FORMAT COTinvolves
guiding models to accurately describe and inter-
pret the format elements presented in the output
table, inspired by Wang et al. (2023b) in the sum-
marization task. To verify the effectiveness of ourproposed FORMAT COT, we conduct an ablation
study in Appendix F.
3.2 Instruction Tuning
We introduce an instruction tuning approach de-
signed specifically to enhance LLMs’ abilities in
generating structured text (Touvron et al., 2023;
Patil et al., 2023). Specifically, we combine GPT-
3.5-generated format descriptions of output tables
and the original text input as the new input of
LLaMA fine-tuning. In other words, we start with
GPT-3.5 processing table data and synthesizing
comprehensive format instructions. The LLaMA
model is then fine-tuned on these enriched instruc-
tions we generate. This approach simulates a user-
agent interaction where GPT-3.5 effectively fetches
and consolidates table information, conversation-
ally instructing LLaMA for the final text generation,
outlined in Figure 3.
3.3 Evaluation Metrics
Assessing the accuracy of generated tables against
ground truth is complex due to the variability in
formatting, like HTML. An ideal evaluation met-
ric needs to discern substantial data discrepancies
while disregarding trivial formatting variations.
We propose to break down the similarity of two
Guiding Questions for PromptingInput:Task:GenerateaLaTextablefromgiventext <TEXT>: …Input:Task:GenerateaLaTextablefromgiventextand format description <TEXT>: …<FORMATINSTRUCTION>: …FormatCoT:<TABLES>: …Describethedetailedformatofagivenlatextableaccording tothecommandsandtagswithmorethan500wordsWhethertherearetableborderlines?Howistextalignment?Whataretableattributes? Whethertobold?Whethertoadd\ref?Whethertherearehorizontalandverticallinesborderingeachrowand column?Sayanythingaboutspecial\"\\"formattokeninlatex.
Figure 3: The upper-left corner box represents the original input, which notably lacks a description of the format.
To explicitly instruct the model on format understanding, we employ the FORMAT COTlocated on the right, which
produces the <FORMAT INSTRUCTION>. The lower-left box illustrates what the input for LLaMA fine-tuning
looks like after passing through FORMAT COT. <TEXT> provides a descriptive text for the expected table output
(original input), <TABLE> serves as a reference table (output), and the <FORMAT INSTRUCTION> is a format
guideline generated through F ORMAT COT (added into input). Detailed prompts are displayed in Appendix D.1.

--- PAGE 4 ---
tables into two coarse components: content and
format . In scoring content similarity, we attempt
to parse content out the data within the table cells,
and compute the similarity. This similarity is com-
puted between the generated and ground-truth table
cells by commonly used similarity metrics. In scor-
ingformat similarity, we place higher emphasis on
components such as the number of columns and
rows, cell alignment, and the table caption. We find
that these two scores allow us to perform a more
involved analysis of where predicted and ground-
truth tables differ. The implementation of these
two scores can be found in Appendix C.
3.3.1 P-Score
We take two approaches to score each metric. First,
we perform model-based evaluation, querying GPT-
3.5 with both tables and having it score the simi-
larity of content and format separately. Following
Wang et al. (2023a), we prompt the model to per-
form Chain-of-Thought (Wei et al., 2023) reason-
ing before outputting its scores, and we query the
model with the predicted and ground-truth tables
in both orders and average the scores. We report
these as the P-Score (Prompting Score).
3.3.2 H-Score
In addition, we also implement hand-crafted scor-
ing functions to score the similarity of the tables.
Since the tables can be presented in different for-
mats, we implement several heuristics to normalize
the tables and to compute their similarity. We use
an average of Levenshtein distance and the Ratclif-
f/Obershelp similarity metric to compute the sim-ilarities between strings or data structures. These
heuristically normalized metrics are reported as the
H-Score (Heuristical Score). The analysis can be
found in Appendix A.3.
4 Experiments
4.1 Basic Settings
For metrics, we use SacreBLEU, ROUGE-L,
BERTScore, BARTScore, and BLEURT metrics as
they are all classical metrics to evaluate text sim-
ilarity, as well as two proposed metrics: P-Score
and H-score. qWe evaluate the following models:
GPT-NeoX-20B, GPT-3.5, GPT-4, Vicuna-13B,
LLaMA-7B, and our finetuning LLaMa-7B. GPT-
NeoX-20B, GPT-3.5 and GPT-4 represent the state-
of-art performance of current LLMs and Vicuna-
13B is another version finetuned on LLaMA, which
can reach 90% of the capacity of GPT-3.5. We
think these models are strong enough to be persua-
sive. For the first 4 models, we simply call their
APIs from OpenAI or HuggingFace to generate
results without further finetuning. In our dataset,
each item consists of three parts: instruction, input,
and output. When generating results, we put each
item’s instruction and input together as the final in-
put to models. During inference, the user provides
the prompt in natural language, this can be for a
simple task (e.g., “please generate a table given by
the following information and format”). During
the inference process, we provide the model with
a natural language prompt to describe the format
and content of our task, as well as the expected
response.
Model SacreBLEU ROUGE-L BERTScore BARTScore BLEURT Content P-Score Format P-Score Content H-Score Format H-Score
Tables from Raw Text
GPT-NeoX-20B 35.24 55.78 68.91 -2.34 33.51 3.86 6.10 0.50 -1.32
GPT-3.5 56.92 70.97 91.35 -1.68 36.85 6.19 8.16 0.52 -1.27
GPT-4 68.13 75.44 94.89 -0.99 55.24 6.88 8.30 0.85 0.53
Vicuna-13B 40.12 50.77 75.21 -2.05 40.02 4.07 6.33 0.55 -1.38
Ours-7B 90.6 88.98 98.54 -0.69 66.07 7.69 8.60 1.65 3.61
w.o.finetune 9.9 36.56 81.63 -2.50 70.24 4.58 6.00 0.51 -1.01
LaTeX
GPT-NeoX-20B 45.92 65.10 76.09 -2.05 40.87 7.23 7.02 0.56 0.72
GPT-3.5 56.94 75.99 86.25 -1.30 42.89 8.22 8.41 0.99 1.27
GPT-4 78.15 85.34 88.07 -1.09 67.11 8.78 8.81 1.10 1.35
Vicuna-13B 50.80 69.48 80.44 -1.07 36.74 7.70 8.10 0.78 1.06
Ours-7B 89.13 88.99 98.55 -0.69 66.07 8.94 9.05 1.14 1.52
w.o.finetune 47.24 70.89 73.27 -2.13 38.13 7.10 6.98 0.51 0.69
HTML
GPT-NeoX-20B 60.36 72.13 86.88 -1.59 30.06 8.42 8.94 0.81 0.92
GPT-3.5 73.80 85.19 96.76 -1.46 34.81 9.11 9.35 1.10 2.15
GPT-4 79.25 85.95 97.22 -1.31 41.59 9.17 9.62 1.15 2.29
Vicuna-13B 58.75 70.37 88.65 -1.58 31.11 8.55 8.88 0.79 0.93
Ours-7B 77.50 86.08 96.25 -1.30 42.89 9.20 9.70 1.18 2.49
w.o.finetune 65.30 78.24 88.12 -1.57 32.78 8.22 8.81 0.92 0.96
Table 2: Automated evaluation results on the test set, involving five types of previous metrics and four proposed
ones. w.o.finetune means that we also compared the performance of our model without finetuning as an ablation
study. ‘Ours-7B’ is finetuned LLaMA.

--- PAGE 5 ---
Figure 4: An exemplification of our benchmark. The
input is made up of the instruction and the input text,
whereas the output aims to present the target table. No-
tably, there are some inaccuracies in the predicted out-
put; for instance, ‘Points in 4th quarter’ under ‘Hawks’
should be vacant, and correspondingly, ‘Points in 4th
quarter’ for ‘Magic’ should be 21.
4.2 Human Evaluation
Table 3 displays human evaluation results on two
proposed metrics with instance-level Pearson cor-
relation, reflecting a purposeful design that caters
to the specific demands of structured output assess-
ment. We engaged five undergraduate students to
annotate 200 examples focusing on content and for-
mat quality. Equipped with the input description
(with reference appended) and generated outputs,
they scored each aspect on a 10-point scale. Both
the P-score and H-score showcase a significant cor-
relation with human judgment, indicating their rel-
ative robustness and effectiveness in this evaluation
space. This level of correlation, which surpasses
that of many prior meta-evaluation efforts (Fab-
bri et al., 2020; Tang et al., 2021), reinforces the
value of our metrics and addresses concerns about
their ability to reliably reflect human evaluation.
Additionally, we evaluated well-known metrics
including ROUGE-L, BERTScore, BARTScore,and BLEURT. Limited space precluded a full dis-
cussion, yet our Content P-score showed the best
instance-level correlation.
Metrics Content Correlation Format Correlation
Content P-score 0.5301 -
Format P-score - 0.3812
Content H-score 0.1059 -
Format H-score - 0.3021
Table 3: Human evaluation results.
4.3 Results
Table 2 provides a comparative analysis of differ-
ent LLMs based on several metrics. For ‘Tables
from Raw Text’, the Ours-7B outperforms the other
models in every metric. Interestingly, without fine-
tuning, the performance drops significantly, partic-
ularly in SacreBLEU, ROUGE-L, and BERTScore.
The results for ‘LaTeX’ reveal a similar trend and
in the ‘HTML’ category, GPT-4 scores the highest
in SacreBLEU and BERTScore. However, these
differences are slight and our 7B model comes
out on top for the rest of the metrics. The results
demonstrate that our approach exhibits superior per-
formance, highlighting the efficacy of fine-tuning
smaller models in surpassing much larger models.
Moreover, we delve into an analysis based on our
Mturk annotation, attributing observed shortcom-
ings to several error types. And we present an
ability map in Figure 5 and Appendix E.
00.10.20.30.40.50.60.70.80.9CoverageFormatting
ReasoningComprehensionPragmaticsHallucinationControlVicunaChatGPTGPT-4Ours
Figure 5: Visualization of LLMs’ capability.
5 Conclusion
In summary, our study provides a thorough analysis
of LLMs’ challenges in structured table generation,
introduces novel evaluation metrics, and assem-
bles a specific benchmark covering a range of data
types. We pinpoint key issues including content fi-
delity, format adherence, numerical reasoning, and
management of extensive tables.

--- PAGE 6 ---
6 Limitations
Although we present a comprehensive analysis, the
exploration of LLMs in structured text generation
presented in this paper has several limitations:
Investigating Optimal Format for Tabular Rep-
resentation In this study, we did not investigate
which table formats are most effective. Different
presentations of the same information can be rea-
sonable, and table normalization strategies, such
as determining the best way to tabulate given facts
or how to interconnect multiple tables, remain un-
explored. Future research could engage in the study
of table normalization to ascertain optimal strate-
gies for tabular data structuring and representation.
Domain-Specific Benchmark Development
While we’ve made strides in constructing bench-
marks for structured text generation, it may be
beneficial to develop benchmarks that cater to
specific domains. Different fields might have
unique structural requirements and understanding
these nuances can significantly improve the
models’ applicability across diverse contexts.
Expand the Range of Datasets There are end-
less data types and sources that can be explored.
Incorporating a broader variety of datasets could
expose the models to an even wider range of struc-
tural formats, ultimately enhancing their overall
performance.
Enhancing Numerical Reasoning Capabilities
Our study identified inadequate numerical reason-
ing as one of the challenges faced by LLMs. Inves-
tigating techniques to bolster numerical reasoning
in these models could lead to significant improve-
ments in their performance.
Developing Advanced Methods While our
structure-aware instruction tuning method showed
promising results, more sophisticated techniques
could be developed. For instance, future work
could explore ways of incorporating more explicit
structural information into the model or developing
methods that allow the model to learn structural
patterns more effectively.
Exploring Multimodal LLMs As LLMs con-
tinue to evolve, there are opportunities to explore
multimodal models that can process and generate
both text and other forms of data, such as sound
or images (Kamigaito et al., 2023), in a structured
manner.References
Sid Black, Stella Biderman, Eric Hallahan, Quentin
Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, et al.
2022. Gpt-neox-20b: An open-source autoregressive
language model. arXiv preprint arXiv:2204.06745 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.
2023. Vicuna: An open-source chatbot impressing
gpt-4 with 90%* chatgpt quality. See https://vicuna.
lmsys. org (accessed 14 April 2023) .
Alexander R Fabbri, Wojciech Kry ´sci´nski, Bryan Mc-
Cann, Caiming Xiong, Richard Socher, and Dragomir
Radev. 2020. Summeval: Re-evaluating summariza-
tion evaluation. arXiv preprint arXiv:2007.12626 .
Yuxin He, Jingyue Hu, and Buzhou Tang. 2023. Revisit-
ing event argument extraction: Can eae models learn
better when being aware of event co-occurrences?
arXiv preprint arXiv:2306.00502 .
Hidetaka Kamigaito, Katsuhiko Hayashi, and Taro
Watanabe. 2023. Table and image generation
for investigating knowledge of entities in pre-
trained vision and language models. arXiv preprint
arXiv:2306.02115 .
Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia
Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine
Jernite, Margaret Mitchell, Sean Hughes, Thomas
Wolf, Dzmitry Bahdanau, Leandro von Werra, and
Harm de Vries. 2022. The stack: 3 tb of permissively
licensed source code. Preprint .
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-
noising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
arXiv preprint arXiv:1910.13461 .
Tong Li, Zhihao Wang, Liangying Shao, Xuling Zheng,
Xiaoli Wang, and Jinsong Su. 2023. A sequence-
to-sequence&set model for text-to-table generation.
arXiv preprint arXiv:2306.00137 .
Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun.
2023. Large language model is not a good few-shot
information extractor, but a good reranker for hard
samples! arXiv preprint arXiv:2303.08559 .
Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
Adam Roberts, Stella Biderman, Teven Le Scao,
M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey

--- PAGE 7 ---
Schoelkopf, et al. 2022. Crosslingual generaliza-
tion through multitask finetuning. arXiv preprint
arXiv:2211.01786 .
OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Shishir G. Patil, Tianjun Zhang, Xin Wang, and
Joseph E. Gonzalez. 2023. Gorilla: Large language
model connected with massive apis. arXiv preprint
arXiv:2305.15334 .
Michał Pietruszka, Michał Turski, Łukasz Borchmann,
Tomasz Dwojak, Gabriela Pałka, Karolina Szyndler,
Dawid Jurkiewicz, and Łukasz Garncarek. 2022. Sta-
ble: Table generation framework for encoder-decoder
models. arXiv preprint arXiv:2206.04045 .
Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao
Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is
chatgpt a general-purpose natural language process-
ing task solver? arXiv preprint arXiv:2302.06476 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485–5551.
Gaetano Rossiello, Faisal Chowdhury, Nandana Mi-
hindukulasooriya, Owen Cornec, and Alfio Gliozzo.
2022. Knowgl: Knowledge generation and linking
from text. arXiv preprint arXiv:2210.13952 .
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .
Xiangru Tang, Alexander R Fabbri, Ziming Mao, Griffin
Adams, Borui Wang, Haoran Li, Yashar Mehdad, and
Dragomir Radev. 2021. Investigating crowdsourcing
protocols for evaluating the factual consistency of
summaries. arXiv preprint arXiv:2109.09195 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models.
Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai
Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.
2023a. Large language models are not fair evaluators.Yiming Wang, Zhuosheng Zhang, and Rui Wang. 2023b.
Element-aware summarization with large language
models: Expert-aligned evaluation and chain-of-
thought method. arXiv preprint arXiv:2305.13412 .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and
Denny Zhou. 2023. Chain-of-thought prompting elic-
its reasoning in large language models.
Chenxi Whitehouse, Clara Vania, Alham Fikri Aji,
Christos Christodoulopoulos, and Andrea Pierleoni.
2023. Webie: Faithful and robust information extrac-
tion on the web. arXiv preprint arXiv:2305.14293 .
Sam Wiseman, Stuart M Shieber, and Alexander M
Rush. 2017. Challenges in data-to-document genera-
tion. arXiv preprint arXiv:1707.08052 .
Xueqing Wu, Jiacheng Zhang, and Hang Li. 2022. Text-
to-table: A new way of information extraction. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 2518–2533, Dublin, Ireland. As-
sociation for Computational Linguistics.
Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi
Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang
Li, Aofeng Su, et al. 2023. Tablegpt: Towards unify-
ing tables, nature language and commands into one
gpt.arXiv preprint arXiv:2307.08674 .
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
Zhang, Junjie Zhang, Zican Dong, et al. 2023a. A
survey of large language models. arXiv preprint
arXiv:2303.18223 .
Yilun Zhao, Haowei Zhang, Shengyun Si, Linyong Nan,
Xiangru Tang, and Arman Cohan. 2023b. Large lan-
guage models are effective table-to-text generators,
evaluators, and feedback providers. arXiv preprint
arXiv:2305.14987 .
Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting
Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, and
Dragomir Radev. 2023c. Robut: A system-
atic study of table qa robustness against human-
annotated adversarial perturbations. arXiv preprint
arXiv:2306.14321 .
Zexuan Zhong and Danqi Chen. 2020. A frustrat-
ingly easy approach for entity and relation extraction.
arXiv preprint arXiv:2010.12812 .

--- PAGE 8 ---
A Analysis with Examples
A.1 Example Table A
The main difference between the reference tables
and the tables generated by GPT-3.5 and GPT4,
shown in figure 6, is in the completeness and preci-
sion of the data provided.
In the reference tables, all relevant data is fully
represented: For the teams (Table 1), each team has
a precise number or percentage for every statistic.
Similarly, for the players (Table 2), each player
has a definite number for every statistic, including
minutes played in the format “mm:ss”.
In contrast, the generated tables show data that is
incomplete and imprecise. For GPT-3.5 generated
one, the team statistics table has some statistics
missing, as represented by empty cells, and some
are not presented as percentages. The player statis-
tics table also has missing data similarly, and it
lacks the "minutes played" statistics entirely. For
instance, in the ‘team’ table, the "Percentage of
field goals" column for the Suns is missing. Sim-
ilarly, in the ‘player’ table, many key statistics
such as "3-pointers attempted", "3-pointers made",
"Field goals attempted", "Field goals made", and
"Minutes played" are missing for various players.
Regarding the format, we observe a lot of format
errors. For example, the ‘Percentage of field goals’
column for Grizzlies is represented as "50" instead
of "50.0%". Moreover, the ‘Wins’ column for the
Suns is represented as "3" instead of "0". This
misrepresentation can lead to significant misunder-
standing of the data. The ‘Player’ table also has
format errors. For instance, the ‘Minutes played’
column is missing the time format (i.e., “00:00”).On the other hand, the reference tables adhere to
a standard format. Percentage data is represented
with a ‘%’ sign, time data uses the ‘00:00’ format,
and numeric data correctly represents each statistic.
For Vicuna-13B results shown in figure 7, al-
though it has the correct format for both tables,
there are still many element errors. For instance,
the ‘team’ table has wrong statistics such as
“Losses” and “Win” for the Suns. Besides, in the
‘player’ table, many cells shouldn’t have data. How-
ever, they have, which is a mistake. Some cells
like Isaiah Thomas’s and Eric Bledsoe’s ‘Assists’
should be 2 and 4, but they are not in the Vicuna-
13B ‘player’ table. Similarly, LLaMA2-7B results,
have the same element errors in the ‘team’ table
and worse errors in the ‘player’ table. It fills all
cells, many of which should be none. As for some
cells that should have data, their data are wrongly
filled in like Eric Bledsoe’s ‘Assists’ and ‘Field
goals made’.
PlayerAssists3-pointers attempted3-pointers madeField goals attemptedField goals madeMinutes playedPointsTotal reboundsMarc Gasol6-----185Courtney Lee-54149-22-Mike Conley1143149-24-Markieff Morris------205Goran Dragic-----266-Eric Bledsoe4--129-235Isaiah Thomas2-----15-TeamNumber of team assistsPercentage of field goalsLossesTotal pointsPoints in 3rd quarterPoints in 4th quarterReboundsWinsSuns13-2911920353Grizzlies2550-102302637-TeamNumber of team assistsPercentage of field goalsLossesTotal pointsPoints in 3rd quarterPoints in 4th quarterReboundsWinsSuns1347.7%2911920350Grizzlies2550.0%01023026371
PlayerAssists3-pointers attempted3-pointers madeField goals attemptedField goals madeMinutes playedPointsTotal reboundsMarc Gasol60012835:00185Courtney Lee15414934:00222Mike Conley114314936:00240Markieff Morris15214834:00205Goran Dragic42112926:0065Eric Bledsoe43112934:00235Isaiah Thomas25211526:00150
Reference
Reference
GPT
-
3.5
GPT
-
4
GPT
-
3.5
GPT
-
4
Table 1: Team 
Summary
Table 2: Player 
StatisticsTheGrizzlies(50)usedastrongsecondhalftooutlasttheSuns(3-2)102-91inPhoenixonWednesdaynight.MemphisfounditselfbehindsixathalftimebutoutscoredPhoenix30-19inthethirdquarterand26-20inthefinalperiod.TheGrizzliesshot50percentfromthefield,ledbystrongperformancesfromCourtneyLeeandMikeConley.Leescored22points(9-14FG,4-53Pt),whileConleyledallscorerswith24(9-14FG,3-43Pt)and11assists.MarcGasoladded18points,sixassists,andfiverebounds.TheSuns,whobeattheLakers112-106onTuesday,werepacedby23points(9-12FG),fivereboundsandfourassistsfromEricBledsoe.ItwasaquietnightforGoranDragic,whoscoredjustsixpointsin26minutes.Thethirdmemberofthebackcourttrio,IsaiahThomas,had15pointsandtwoassistsoffthebench,whileMarkieffMorrisadded20pointsandfiverebounds.TheGrizzliesout-reboundedPhoenix37-35andoutscoredtheSunsinthepaint46-32.Memphisalsoregistered25assistscomparedtoonly13-on32fieldgoals-fortheSuns.MemphisnowheadstoOklahomaCitytotakeontheThunderonFriday.Phoenix,meanwhile,hoststheKingsonFriday.
TeamNumber of team assistsPercentage of field goalsLossesTotal pointsPoints in 3rd quarterPoints in 4th quarterReboundsWinspoints in the paint Grizzlies2550.0%-1023026375046Suns13-291192035332PlayerAssists3-pointers attempted3-pointers madeField goals attemptedField goals madeMinutes playedPointsTotal reboundsMarc Gasol6-----185Courtney Lee-54149-22-Mike Conley1143149-24-Markieff Morris------205Goran Dragic-----266-Eric Bledsoe4--129-235Isaiah Thomas2-----15-
Figure 6: Examples of generating tables with GPT-3.5 and GPT-4 based on the input text. The generated results
contain a large number of errors, including format errors and content errors.

--- PAGE 9 ---
PlayerAssists3-pointers attempted3-pointers madeField goals attemptedField goals madeMinutes playedPointsTotal reboundsMarc Gasol6-----185Courtney Lee-54149-22-Mike Conley1143149-24-Markieff Morris------205Goran Dragic-----266-Eric Bledsoe4--129-235Isaiah Thomas2-----15-TeamNumber of team assistsPercentage of field goalsLossesTotal pointsPoints in 3rd quarterPoints in 4th quarterReboundsWinsSuns13-2911920353Grizzlies2550-102302637-TeamNumber of team assistsPercentage of field goalsLossesTotal pointsPoints in 3rd quarterPoints in 4th quarterReboundsWinsSuns13-3911920355Grizzlies2550.0%-1023026377
PlayerAssists3-pointers attempted3-pointers madeField goals attemptedField goals madeMinutes playedPointsTotal reboundsMarc Gasol6--14932185Courtney Lee-451493222-Mike Conley11341493224-Markieff Morris-2112626205Goran Dragic-20126266-Eric Bledsoe-1112926235Isaiah Thomas-108418152
Reference
Reference
Vicuna
-
13B
LLaMA
2
-
7B
Vicuna
-
13B
LLaMA2
-
7B
Table 1: Team 
Summary
Table 2: Player 
StatisticsTheGrizzlies(50)usedastrongsecondhalftooutlasttheSuns(3-2)102-91inPhoenixonWednesdaynight.MemphisfounditselfbehindsixathalftimebutoutscoredPhoenix30-19inthethirdquarterand26-20inthefinalperiod.TheGrizzliesshot50percentfromthefield,ledbystrongperformancesfromCourtneyLeeandMikeConley.Leescored22points(9-14FG,4-53Pt),whileConleyledallscorerswith24(9-14FG,3-43Pt)and11assists.MarcGasoladded18points,sixassists,andfiverebounds.TheSuns,whobeattheLakers112-106onTuesday,werepacedby23points(9-12FG),fivereboundsandfourassistsfromEricBledsoe.ItwasaquietnightforGoranDragic,whoscoredjustsixpointsin26minutes.Thethirdmemberofthebackcourttrio,IsaiahThomas,had15pointsandtwoassistsoffthebench,whileMarkieffMorrisadded20pointsandfiverebounds.TheGrizzliesout-reboundedPhoenix37-35andoutscoredtheSunsinthepaint46-32.Memphisalsoregistered25assistscomparedtoonly13-on32fieldgoals-fortheSuns.MemphisnowheadstoOklahomaCitytotakeontheThunderonFriday.Phoenix,meanwhile,hoststheKingsonFriday.
TeamNumber of team assistsPercentage of field goalsLossesTotal pointsPoints in 3rd quarterPoints in 4th quarterReboundsWinsSuns1340.6%2911920350Grizzlies2550.0%11023026371
PlayerAssists3-pointers attempted3-pointers madeField goals attemptedField goals madeMinutes playedPointsTotal reboundsMarc Gasol6319434185Courtney Lee35414936223Mike Conley114314936243Markieff Morris22012531205Goran Dragic4311262662Eric Bledsoe52112735234Isaiah Thomas22111423152Figure 7: Examples of generating tables with Vicuna-13B and LLaMA2-7B based on the input text, the generated
results contain a large number of errors, including format errors and content errors.

--- PAGE 10 ---
PlayerAssists3-pointers attempted3-pointers madeField goals attemptedField goals madeMinutes playedPointsTotal reboundsMarc Gasol6-----185Courtney Lee-54149-22-Mike Conley1143149-24-Markieff Morris------205Goran Dragic-----266-Eric Bledsoe4--129-235Isaiah Thomas2-----15-TeamNumber of team assistsPercentage of field goalsLossesTotal pointsPoints in 3rd quarterPoints in 4th quarterReboundsWinsSuns13-2911920353Grizzlies2550-102302637-TeamHalf-Time ScoreFinal ScoreField Goals (%)3Pt (%)ReboundsAssistsPoints in the PaintGrizzlies4610250N/A372546Suns5291N/AN/A351332
PlayerTeamPointsAssistsReboundsField Goals (FG)Three-Points (3Pt)Courtney LeeGrizzlies22N/AN/A9 -144 -5Mike ConleyGrizzlies2411N/A9 -143 -4Marc GasolGrizzlies1865N/AN/AEric BledsoeSuns23459 -12N/AGoran DragicSuns6N/AN/AN/AN/AIsaiah ThomasSuns152N/AN/AN/AMarkieff MorrisSuns20N/A5N/AN/A
Reference
Reference
GPT
-
4
GPT
-
4
Table 1: Team 
Summary
Table 2: Player 
StatisticsTheGrizzlies(50)usedastrongsecondhalftooutlasttheSuns(3-2)102-91inPhoenixonWednesdaynight.MemphisfounditselfbehindsixathalftimebutoutscoredPhoenix30-19inthethirdquarterand26-20inthefinalperiod.TheGrizzliesshot50percentfromthefield,ledbystrongperformancesfromCourtneyLeeandMikeConley.Leescored22points(9-14FG,4-53Pt),whileConleyledallscorerswith24(9-14FG,3-43Pt)and11assists.MarcGasoladded18points,sixassists,andfiverebounds.TheSuns,whobeattheLakers112-106onTuesday,werepacedby23points(9-12FG),fivereboundsandfourassistsfromEricBledsoe.ItwasaquietnightforGoranDragic,whoscoredjustsixpointsin26minutes.Thethirdmemberofthebackcourttrio,IsaiahThomas,had15pointsandtwoassistsoffthebench,whileMarkieffMorrisadded20pointsandfiverebounds.TheGrizzliesout-reboundedPhoenix37-35andoutscoredtheSunsinthepaint46-32.Memphisalsoregistered25assistscomparedtoonly13-on32fieldgoals-fortheSuns.MemphisnowheadstoOklahomaCitytotakeontheThunderonFriday.Phoenix,meanwhile,hoststheKingsonFriday.Figure 8: Using GPT-4 to generate a table based on the input text without FORMAT COT, the generated results
contain a large number of errors, including format errors and content errors.

--- PAGE 11 ---
A.2 Error Type
Structure Errors: These errors pertain to the
structural integrity of the generated tables. Specifi-
cally, they include instances where there are excess
or missing rows or columns in comparison to the
correct table structure. For instance, in figure 8
GPT4 generated result has missing columns like
“Win” and “Losse” in the ‘team’ table.
Structure Naming Errors: This category cap-
tures errors related to the naming conventions used
for rows or columns. Any discrepancies in a row or
column names between the generated and correct
table are flagged as structure naming errors. For
instance, in figure 8, the GPT-4 generated result
has wrong column names like “Half-Time Score”
in the ‘team’ table.
Element Errors: These are inaccuracies ob-
served at the element level within the generated
table. Element errors encompass incorrect num-
bers, values, or inappropriately empty cells, reflect-
ing discrepancies in individual table entries relative
to the correct table. In figure 6 and figure 7, most
errors are element errors.

--- PAGE 12 ---
A.3 Metric Effectiveness
Table 4 crystallizes different results of examples in
Appendix A.1 based on our H-score metric. Taking
GPT-4 as an example, there exist slight drops from
2.0 to 1.86 in content H-score, and very accurately,
this trend is followed by the results of GPT-4 re-
vealing slight errors in its content. For the format
H-score, GPT-4 also cannot do very well for its
numeric performance, which is not close enough
to the full score, matched by the poor performance
of GPT-4 in this area. Other models’ H-scores fol-
low this trend as well. Therefore, the H-score can
respond to differences in both content and format
accurately.
Model Content H-score Format H-score
GPT-3.5 1.39 4.0
GPT-4 1.86 3.44
Vicuna-13B 1.51 4.0
LLaMA2-7B 1.37 4.0
Ours-7B 2.0 4.0
Reference 2.0 4.0
Table 4: H-scores for different results of Ours-7B, GPT-
3.5, GPT-4, Vicuna-13B and LLaMA2-7B.

--- PAGE 13 ---
B MTurk
To facilitate a comprehensive analysis of the LLM
output, we designed a task on Amazon Mechan-
ical Turk (MTurk) to gather detailed annotations
about various error types encountered in the gener-
ated structured tables. The task was structured as
follows:
From the RotoWire dataset, we randomly chose
100 instances of LLM-generated output, ensuring a
representative mix of quality based on preliminary
assessment.
Each Human Intelligence Task (HIT) presented
the annotators with a side-by-side view of the LLM
output and the expected structured table format.
Annotators were instructed to identify and catego-
rize errors according to predefined types: ‘Element
Errors’, ‘Element Format Errors’, ‘Structure Error’,
and ‘Structure Naming Errors’.
We provided extensive guidelines, exemplified
with step-by-step instructions, to clarify typical
instances of each error type. These guidelines were
reviewed and iteratively improved through a pilot
study conducted with a small set of annotators.
About the qualifications of Amazon Mechanical
Turk (MTurk) workers, we use the following qual-
ifications to recruit in total of 10 MTurk workers
with good track records: HIT approval rate greater
than or equal to 98%, number of HITs approved
greater than or equal to 500, and located in one
of the following English native-speaking countries:
Australia, Canada, New Zealand, United Kingdom,
United States. Each annotator is limited to anno-
tating 10 examples, including both the output of
GPT-3.5 and GPT-4.
Annotators workers were compensated $7, cali-
brated to equal a $42/hour pay rate. We first anno-
tated examples in-house to determine the required
annotation speed. A summary block usually takes
around 10 minutes.
To demonstrate our annotation template and fa-
cilitate future research, we show the interface for
annotations.
Figure 9: Our interface of Mturk.
Figure 10: Our Interface of Mturk.

--- PAGE 14 ---
C Scoring
C.1 P-Score
Our approach involves prompting the model to en-
gage in Chain-of-Thought reasoning before issu-
ing its scores. Firstly, we instruct GPT on how to
evaluate both "content similarity" and "structure
similarity". Following this, the model is guided
on the correct procedure to output its answer. To
calculate the scores, the model is queried with both
the predicted table and the ground truth table in
varying sequences, after which the scores are av-
eraged. We’ll illustrate this process using the P-
Scores prompt for raw text tables as an illustrative
example:
“Based on the above, we wanted to determine
if the above tables are similar. Ideally, they should
have identical content and structure. Score the
"content similarity" and "structural similarity" be-
tween 0 and 10.
- Content similarity: 10 if the contents of the
table cells are identical, 0 if they are entirely differ-
ent. If about 50% of the cells have the same data,
the score should be 5.
- Structural similarity: 10 if the tables have
the same structure (e.g. same column and rows
with identical ordering, same alignment, etc.) al-
though text formatting differences can be ignored
(e.g. colors, font).
Output a JSON object such as the following:
"""json
{{
"content_similarity": ...
"structural_similarity": ...
}}
"""
Think carefully, and then output the scores. ”
For instance, in figure 11, both tables have iden-
tical structures, so their structural similarity score
is 10. The contents of the first table of Table1 and
Table2 are the same, and the second table of Table1
and Table2 are almost 10% similar. Therefore their
content similarity score is 5.
C.2 H-Score
We attach our algorithm to calculate H-Score as
Algorithm 1.
LATEXWe use the pylatexenc library to parse a
given L ATEX table, and walk through the parse-tree
structure in the tabular environment to identify
the table “cells”. We score the content similaritybased on strings within the cells, and score struc-
tural similarity based on having the matching num-
ber of rows and columns, the same caption, and the
same cell alignment.
HTML We use the beautifulsoup4 library to
parse a given L ATEX HTML snippet and walk
through the parse-tree structure in <table> ,<ul>
or<ol> tags to identify data cells. We separately
build a tree of white-listed HTML tags to score
the structural similarity, traversing an HTML doc-
ument tree structure, disregarding the actual con-
tent within the tags and simplifying it by focusing
only on specific HTML tags (defined in RECOG-
NIZED_HTML_TAGS). We score the content sim-
ilarity based on strings within the cells and score
structural similarity based on the similarity of the
structure tree and the total number of cells match-
ing.
White-listed HTML tags:
RECOGNIZED_HTML_TAGS = [
" table ", "tr", "th", "td",
"ul", "ol", "li",
" div ", " span ", "p",
"a", " img ", " embed ", " pre ",
"h1", "h2", "h3", "h4", "h5", "h6",
" input ", " button ",
]
Raw Text Tables In our evaluated dataset, each
example consists of two tables (Team and Player).
We do a string search for "Team" and"Player"
headers to identify the two tables. We then parse
the tables according to Markdown formatting, with
newlines and pipes as row and column dividers
respectively, to identify the table cells. We score
the content similarity based on strings within the
cells, and score structural similarity based on the
similarity of column names and the number of rows
and columns matching.
String Similarity Measurement: Our script in-
cludes methods to calculate the similarity between
two strings. These methods can be used to com-
pare the structure or content of HTML, latex docu-
ments, or any other pair of strings. The similarity
is evaluated using well-established algorithms in
text analysis: the Levenshtein distance and the Se-
quenceMatcher from Python’s difflib module.

--- PAGE 15 ---
TeamPercentage of Field GoalsLossesTotal PointsPoints in 3rd QuarterWinsBucks54%1295464Cavaliers39%41882119
PlayerAssistsDefensive Rebounds3-Pointers Attempted3-Pointers MadeOffensive ReboundsPersonal FoulsPointsTotal ReboundsStealsLeBron James10422444221313Kevin Love1313131344131313Kyrie Irving44204444204Jabari Parker55185555185Michael Beasley55175555175
Table 1TeamPercentage of Field GoalsLossesTotal PointsPoints in 3rd QuarterWinsBucks54%1295464Cavaliers39%41882119
Table 2
PlayerAssistsDefensive rebounds3-pointers attempted3-pointers madeOffensive reboundsPersonal foulsPointsTotal reboundsStealsLeBron James4224Kevin Love1313Kyrie Irving20Jabari Parker111118Michael Beasley1111171
Figure 11: An example of P-score calculation. The top two figures displays two tables, accompanied by a prompt
below to calculate the content and structural similarity between them. The goal is to determine if the tables have
identical content and structure. The content similarity will be scored based on the percentage of cells with identical
data, ranging from 0 (no similarity) to 10 (complete similarity). The structural similarity will be scored 10 if the
tables share the same structure, including columns, rows, and alignment. The output will be a JSON object with the
calculated scores.

--- PAGE 16 ---
Algorithm 1: The provided algorithm is the H-score algorithm with pseudocode. It aims to
calculate the content similarity (Content H-score) and format similarity (Format H-score) between
predictions and references. The algorithm begins by defining functions for Levenshtein similarity
and Difflib similarity to compute string similarities. It then iterates over each pair of predictions
and references, parsing their structures and data to determine the similarity in terms of the number
of columns, rows, column names, and data rows. The final step involves averaging the similarity
scores to obtain the Content H-score and Format H-score.
Data: predictions α, references β
Result: Content H-score γ, Format H-score δ
1Function LevenshteinSimilarity( p, q):
2 ifq is empty then
3 return 0.0
4 end
5 s←1 - LevenshteinDistance( p,q) /(2 * length(p) )
6 return s
7Function DifflibSimilarity( p, q):
8 s←difflib.SequenceMatcher( None ,p,q)
9 return s
10γ←0
11δ←0
12forσ, ϕinzip(α, β)do
13 Parse σto get its number of columns λc, number of rows λr, column names µand data rows κ
14 Parse ϕto get its number of columns θc, number of rows θr, column names τand data rows ω
15 ρ1←ifλc==θcthen 1.0else0.0
// Whether their number of columns are same
16 ρ2←ifλr==θrthen 1.0else0.0
// Whether their number of rows are same
17 ρ3←LevenshteinSimilarity (µ, τ)
// Compute their columns Levenshtein Similarity
18 ρ4←DifflibSimilarity (µ, τ)
// Compute their columns Difflib Similarity
19 ρ5←LevenshteinSimilarity (κ, ω)
// Compute their data Levenshtein Similarity
20 ρ6←DifflibSimilarity (κ, ω)
// Compute their data Difflib Similarity
21 γ←γ+average (ρ5, ρ6)
22 δ←δ+average (ρ1, ρ2, ρ3, ρ4)
23end
24γ←γ/length (α)
25δ←δ/length (α)

--- PAGE 17 ---
D Prompt for FormatCoT and Inference
D.1 Prompt for FormatCoT
Raw Text Table Description Traditional data-
to-text datasets only have raw text for each table.
However, it is not enough for GPT-3.5 or other
LLMs to generate correct tables. As a result, we
added some format descriptions to help them gen-
erate the correct tables. We use GPT-3.5 to achieve
this. We want to get detailed format information
without concrete contents in cells, so we explicitly
include these requirements in the prompt. Here
is our prompt: “Describe details about the given
text. First, give the number of tables, and then for
each table, describe its format such as the num-
ber of columns and rows, column names, and row
names.”
HTML Table Description Unlike data-to-text
datasets, HTML datasets only have final outputs,
so we are required to generate a detailed description
of their format and content. For content descrip-
tions, we can simply ask GPT-3.5 to output raw text
without HTML tags. For format descriptions, how-
ever, we need to ask GPT-3.5 to describe each tag,
otherwise, it will leave out some tags and describe
the table in general rather than detailed information.
Moreover, it is necessary to ask it to use specific
numbers instead of ‘several’ or ‘multiple’. Here is
our prompt for HTML format descriptions: “De-
scribe the format of this HTML in detail according
to each HTML tag of the following HTML code.
Be careful and make sure don’t miss any HTML
tags. Please use more than 300 words to explain
the format. Use specific numbers rather than being
vague about several.”
LaTeX Table Description Similar to HTML
prompt generation, it is necessary to ask GPT-3.5
to generate both format descriptions and content de-
scriptions as latex datasets only have final outputs.
For content descriptions, we can simply ask GPT-
3.5 to describe the given latex table as detailed as
it can and include all cells. For format description,
since the latex format is too complex, we need to
give it a small example to learn. Then we ask GPT-
3.5 to describe the detailed format of a given latex
table, including specific questions to help it gen-
erate format descriptions. Here is our prompt for
latex format descriptions: “Describe the detailed
format of a given latex table according to the com-
mands and tags with more than 500 words. Include:
Whether there is table border lines? How is textalignment? What are table attributes? Whether to
bold? Whether to add \ref? Please clearly explain
whether there are horizontal and vertical lines bor-
dering each row and column. Say anything about
a special "\" format token in latex if there is one.
Don’t display latex code directly. Use natural lan-
guage. And provide enough format information
for me to recreate this table based on your output
description.”
D.2 Prompt for Inference
When inferencing raw text tables, LLMs tend to
output tabular results rather than raw text tables.
As a result, we need to give it an example output
first, then tell the model that the input consists of
two parts, text and format descriptions, and ask
the model to generate the output based on them.
For HTML and Latex inference, we can simply ask
models to infer from the input and specify the for-
mat and content sections in the input, since models
can generate correct syntax.

--- PAGE 18 ---
FORMAT COT Prompt for Raw Text
TableDescribe details about the given text. First, give the
number of tables, and then for each table, describe
its format such as the number of columns and rows,
column names, and row names.
FORMAT COTPrompt for HTML TableDescribe the format of this HTML in detail
according to each HTML tag of the following
HTML code. Be careful and make sure don’t miss
any HTML tags. Please use more than 300 words to
explain the format. Use specific numbers rather than
being vague about several.
FORMAT COTPrompt for LaTeX TableDescribe the detailed format of a given latex table
according to the commands and tags with more than
500 words. Include: Whether there are table border
lines? How is text alignment? What are table
attributes? Whether to bold? Whether to add \ref?
Please clearly explain whether there are horizontal
and vertical lines bordering each row and column.
Say anything about a special "\" format token in
latex if there is one. Don’t display latex code
directly. Use natural language. And provide enough
format information for me to recreate this table
based on your output description.
Prompt for InferenceBased on the example output above, generate the
raw text/HTML/LaTeX table according to the
following description.
Table 5: Our prompts for FORMAT COTand Inference. The prompt requests an overview of table formatting in
raw text, HTML, and LaTeX formats, including descriptions of the number of tables, column and row structures,
formatting elements, and specific instructions. The prompt for inference illustrates the request to generate the tables
based on the provided details.

--- PAGE 19 ---
E Ability Map
Based on our automated evaluation, we selected Vi-
cuna, GPT-3.5, GPT-4, and Ours as representative
models and conducted an in-depth analysis of the
causes of model errors.
We identified content accuracy, formatting, nu-
merical reasoning, and handling of long tables as
the main sources of these errors.
At the fundamental level, we decompose the pro-
cess of model-generated complex structured out-
puts into two parts: Content Selection and Format
Planning. Initially, the model needs to identify key
information from a given vast amount of unstruc-
tured input, extract this information, understand
it, and organize it. Subsequently, it needs to plan
how to summarize these extracted details, devise
the format of the table to be generated, and then fill
in the information.
Accordingly, we can break down the model’s ca-
pabilities into Coverage, Formatting Reasoning,
Comprehension, Pragmatics, and Hallucination
Control.
Coverage entails the model’s ability to accurately
cover the content in the input. Formatting Reason-
ing pertains to judgment about the output format,
assessing if the model can find the most appropriate
and reasonable structured format.
Comprehension reflects whether the model can
understand the content of the input, as there are
times when it is necessary to infer from a large
amount of data (including performing addition or
subtraction or comparing multiple elements).
Pragmatics involves the ability to utilize special
formats, such as HTML tags and specific syntax in
LaTeX.
Finally, Hallucination Control signifies the
model’s ability to refrain from generating content
not present in the input.
We carried out manual annotations and obtained
visualized results to demonstrate these aspects.

--- PAGE 20 ---
F Ablation Study for FormatCoT
F.1 Contrast between descriptions
In this section, we conduct an ablation study to
examine the impact of our proposed FORMAT COT.
In the generation of table descriptions sans FOR-
MATCOT, we simply utilize the prompt: “Provide
a description of the following tables.” We display
the results in figure 12. The primary differentia-
tion between results pivots on the extent of details
incorporated.
For instance, in the FORMAT COTresult, the
description comprises an array of detailed format
information - encompassing row names, column
names, and table count. The precision in these
details proves substantial enough for models to
accurately recreate the tables in question.
Contrastingly, the outcome bereft of FORMAT -
COTconveys considerably less information - pro-
viding incomplete column names without the ac-
companiment of row names. This sparse degree
of detail proves insufficient for models seeking to
faithfully regenerate the corresponding tables.
F.2 Contrast between results
In this section, we draw a comparison between two
sets of description results, shown in figure 13. The
FORMAT COTresult showcases a table that stands
remarkably close to the correct table, albeit with mi-
nor errors. It contains an extra row termed “Player”
in the initial table, a discrepancy potentially at-
tributable to the fact that the result comprises two
tables, with “Player” denoting the header of the
subsequent table. We posit that this error could po-
tentially be circumvented with a different method
of integrating table names.
Furthermore, an additional column surfaces in
the second table, which in reality represents the fi-
nal row of that table. Besides these minor inaccura-
cies, the FORMAT COTresult accurately replicates
the content in each cell as well as maintaining the
overall format.
Conversely, the alternative result contains mul-
tiple errors that span both content and format. Ini-
tially, an additional row is present in the first ta-
ble, introducing an unrelated basketball team that
bears no relevance to the game under considera-
tion. Following this, the second table possesses an
excessive number of player names, encompassing
unnecessary players along with coaches who did
not participate in the game.Furthermore, its content is not entirely accu-
rate, with discrepancies present in the statistics at-
tributed to both Gordon Hayward and Gerald Green.
These shortcomings underscore the efficiency and
essentiality of implementing the FORMAT COTto
ensure accuracy and precision.

--- PAGE 21 ---
TeamLossesTotal PointsWinsSuns388739Jazz428534PlayerAssistsBlocks3-Pointers MadeField Goals AttemptedMinutes PlayedPointsTotal ReboundsStealsGordon Hayward---18-21--Rudy Gobert21--388151Rodney Hood-----17--Brandan Wright14--331433TJ Warren-----4--Gerald Green--3-2224--
FormatCoTTherearetwotables.Thefirsttablehasfourcolumnsandthreerows.Thecolumnnamesare"Team","Losses","Totalpoints",and"Wins".Therownamesare"Suns","Jazz",and"Player".Thesecondtablehasninecolumnsandsixrows.Thecolumnnamesare"Assists","Blocks","3-pointersmade","Fieldgoalsattempted","Minutesplayed","Points","Totalrebounds","Steals",and"GeraldGreen".Therownamesare"GordonHayward","RudyGobert","RodneyHood","BrandanWright","TJWarren",and"GeraldGreen".
Without 
FormatCoTThefirsttablelistsbasketballteamswiththeirnumberoflosses,totalpoints,andwins.Thesecondtablelistsbasketballplayerswiththeirnumberofassists,blocks,3-pointersmade,fieldgoalsattempted,minutesplayed,points,totalrebounds,andsteals.Somecellswithinthesecondtableareempty,meaningthatdataisnotavailableforthosespecificcategoriesforcertainplayers.
Target tablesFigure 12: Using FORMAT COTand normal instructions to ask GPT-3.5 to generate table descriptions based on the
input text, F ORMAT COT results contain more detailed information about row names.
TeamLossesTotal PointsWinsSuns388739Jazz428534PlayerPlayerAssistsBlocks3-Pointers MadeField Goals AttemptedMinutes PlayedPointsTotal ReboundsStealsGerald GreensGordon Hayward-----21--Rudy Gobert21--388151Rodney Hood--1--17--Brandan Wright14--331433TJ Warren-----4--Gerald Green--3-2224--
FormatCoTTeamLossesTotal PointsWinsPhoenix Suns388739Utah Jazz428534Oklahoma City ThunderPlayerAssistsBlocks3-Pointers MadeField Goals AttemptedMinutes PlayedPointsTotal ReboundsStealsRodney Hood--------Gerald Green-----24--Brandan Wright14--331433Trey Burke--------T.J. Warren-----4--Dante Exum--------Joe Ingles--------Gordon Hayward211821151Rudy Gobert21388151
Without 
FormatCoT
Figure 13: Using two descriptions to regenerate table descriptions based on the input text and descriptions,
FORMAT COT result is more correct in both format and content.
