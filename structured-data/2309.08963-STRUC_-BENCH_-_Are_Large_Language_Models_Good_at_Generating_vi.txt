# 2309.08963.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/structured-data/2309.08963.pdf
# Kích thước tệp: 4670099 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
STRUC-BENCH: Các Mô hình Ngôn ngữ Lớn có Giỏi trong việc Tạo ra 
Dữ liệu Bảng Có cấu trúc Phức tạp không?
Xiangru Tang♠Yiming Zong♡Jason Phang♢Yilun Zhao♠Wangchunshu Zhou♠
Arman Cohan♠Mark Gerstein♠
♠Đại học Yale♡Đại học Chiết Giang♢Đại học New York
xiangru.tang@yale.edu

Tóm tắt
Bất chấp khả năng đáng kể của các Mô hình Ngôn ngữ Lớn (LLMs) như GPT-4, việc tạo ra dữ liệu bảng có cấu trúc phức tạp vẫn là thách thức. Nghiên cứu của chúng tôi đánh giá khả năng thành thạo của LLMs trong việc cấu trúc hóa bảng và giới thiệu một phương pháp tinh chỉnh mới, có nhận thức về cấu trúc dữ liệu, để tăng cường hiệu suất của chúng. Chúng tôi ra mắt STRUC-BENCH, một benchmark toàn diện bao gồm các LLMs nổi bật (GPT-NeoX-20B, GPT-3.5, GPT-4, và Vicuna), trải dài qua các định dạng bảng văn bản, HTML, và LaTeX. FORMAT COT được chúng tôi đề xuất hỗ trợ trong việc tạo ra các hướng dẫn cụ thể theo định dạng từ các đầu ra mong muốn để điền vào benchmark này. Để giải quyết khoảng trống trong đánh giá tập trung vào nhiệm vụ, chúng tôi đề xuất hai chỉ số đổi mới, P-Score (Điểm Prompting) và H-Score (Điểm Heuristic), để đo lường hiệu suất LLM chính xác hơn. Các thí nghiệm của chúng tôi cho thấy việc áp dụng tinh chỉnh có nhận thức cấu trúc của chúng tôi cho LLaMA-7B dẫn đến những cải thiện hiệu suất đáng kể, vượt trội hơn các đối tác LLM qua hầu hết các thước đo. Phân tích lỗi sâu và việc tạo ra bản đồ khả năng qua sáu chiều - phạm vi, định dạng, lý luận, hiểu biết, thực dụng, và ảo giác - làm nổi bật các khu vực cần cải thiện trong tương lai và gợi ý các hướng nghiên cứu sắp tới. Mã và mô hình của chúng tôi có thể được tìm thấy tại https://github.com/gersteinlab/Struc-Bench.

1 Giới thiệu
Những tiến bộ đáng kể đã được thực hiện trong các nhiệm vụ xử lý ngôn ngữ tự nhiên khác nhau bởi các Mô hình Ngôn ngữ Lớn (LLMs) (Brown et al., 2020; Scao et al., 2022; Ouyang et al., 2022; Muennighoff et al., 2022; OpenAI, 2023; Zhao et al., 2023a), đặc biệt trong các nhiệm vụ tạo văn bản (Qin et al., 2023). Khả năng đầu ra dữ liệu có cấu trúc, một trong những khía cạnh chính của khả năng tạo sinh, cũng đã thu hút sự quan tâm lớn trong các nghiên cứu trước đây (Wu et al., 2022; Zhao et al., 2023c,b; Zha et al., 2023).

Bất chấp khả năng tiên tiến của chúng, LLMs gặp vấn đề với việc tạo ra các bảng có cấu trúc phức tạp, một kỹ năng không thể thiếu cho các ứng dụng thực tế như coding copilot và tạo báo cáo tự động. Khả năng thành thạo này đòi hỏi việc tổ chức thông tin từ nhiều nguồn khác nhau thành các cấu trúc mạch lạc. Tạo ra các bảng có cấu trúc như đầu ra không chỉ giúp con người hiểu mà còn tạo điều kiện thuận lợi cho quy trình xử lý dữ liệu tự động trong các tác nhân ngôn ngữ tự chủ. Hơn nữa, tạo ra các bảng có cấu trúc cũng có thể phục vụ như một quy trình tiền xử lý quan trọng cho các nhiệm vụ downstream như ra quyết định và trích xuất kiến thức.

Tuy nhiên, bối cảnh hiện tại của đánh giá LLM thường bỏ qua khía cạnh tạo bảng này, điều này tạo ra sự không chắc chắn về tiềm năng đầy đủ và tính hữu ích của chúng trong các tình huống như vậy. Nghiên cứu của chúng tôi tìm cách điều tra kỹ lưỡng những khoảng trống này.

Thứ nhất, thiếu phân tích có hệ thống và các benchmark toàn diện về khả năng của LLMs trong việc đầu ra dữ liệu bảng có cấu trúc phức tạp. Các nỗ lực trước đây trong việc đánh giá LLMs (Qin et al., 2023; Ma et al., 2023) trên dữ liệu có cấu trúc chủ yếu tập trung vào các nhiệm vụ Trích xuất Thông tin (IE) đơn giản: nhận dạng thực thể được đặt tên, trích xuất quan hệ, và phát hiện sự kiện. Ở đây mục tiêu của các nhiệm vụ IE là thu thập dữ liệu được trích xuất dưới dạng có cấu trúc cao (Zhong and Chen, 2020). Công việc trước đây sớm hơn đáng kể tập trung vào nhiệm vụ hơn là tập trung vào LLM. Trọng tâm chủ yếu là tạo ra dữ liệu có cấu trúc từ văn bản (text-to-data) với các mô hình được huấn luyện trước (He et al., 2023; Rossiello et al., 2022; Whitehouse et al., 2023; Pietruszka et al., 2022) như BART (Lewis et al., 2019) và T5 (Raffel et al., 2020).

Thứ hai, thiếu các chỉ số đánh giá cho việc tạo ra dữ liệu bảng có cấu trúc. Các benchmark hiện có thường dựa vào các chỉ số khách quan cơ bản như sự trùng lặp từ để đo độ chính xác của nội dung được tạo ra bởi mô hình (Li et al., 2023;

--- TRANG 2 ---
Curation
Tập dữ liệu
FormatCoT self-instruct với
các ví dụ trong ngữ cảnh
Huấn luyện LLaMA-7B
Câu hỏi Hướng dẫn
cho Prompting

Đầu vào:
###Nhiệm vụ: Tạo bảng LaTeX từ văn bản cho trước
###Văn bản

Đầu vào:
###Nhiệm vụ: Tạo bảng LaTeX từ văn bản cho trước
và mô tả định dạng
###Văn bản
###Hướng dẫn Định dạng###Dữ liệu
Demo/ví dụ:...

###Mô tả định dạng chi tiết của bảng latex cho trước theo
các lệnh và thẻ với hơn 500 từ
Có đường viền bảng không?
Căn chỉnh văn bản như thế nào?
Thuộc tính bảng là gì?
Có in đậm không?
Có thêm \ref không?
Có đường ngang và dọc bao quanh mỗi hàng
và cột không?
Nói gì về token định dạng đặc biệt "\" trong latex.

Benchmark và
chỉ số

Hình 1: Tổng quan về quy trình của chúng tôi: Chúng tôi bắt đầu bằng việc tạo các tập dữ liệu bảng văn bản thô, bảng HTML, và bảng LATEX. Tiếp theo, LLaMA-7B được huấn luyện sử dụng dữ liệu huấn luyện được xây dựng bởi FORMAT COT. Cuối cùng, các benchmark của chúng tôi xác nhận hiệu quả của các LLMs hiện tại để tạo ra các bảng như vậy.

Wu et al., 2022; Pietruszka et al., 2022). Điều này có thể không đủ để đánh giá liệu LLMs có thể tạo ra đầu ra có cấu trúc hay không, vì một chỉ số đánh giá lý tưởng cũng nên xem xét định dạng của nội dung được tạo ra.

Thứ ba, thiếu các phương pháp để tăng cường hiệu suất của các LLMs hiện tại để tuân thủ tốt hơn các đầu vào ngôn ngữ tự nhiên và tạo ra các đầu ra bảng với định dạng chính xác.

Đóng góp của chúng tôi có thể được tóm tắt như được trình bày trong Hình 1: (1) Chúng tôi giới thiệu STRUC-BENCH, một benchmark được xây dựng đặc biệt để tạo ra dữ liệu bảng có cấu trúc. (2) Chúng tôi đánh giá các LLMs phổ biến trên STRUC-BENCH sử dụng hai chỉ số được đề xuất, cung cấp cái nhìn toàn diện về các hạn chế hiện tại và các loại lỗi phổ biến. (3) Chúng tôi đề xuất FORMAT COT để tạo ra dữ liệu tinh chỉnh hướng dẫn, trong đó chúng tôi sử dụng GPT-3.5 để tạo ra các hướng dẫn định dạng và sau đó tinh chỉnh mô hình LLaMA-7B để tuân theo các định dạng này. Hiệu suất ấn tượng thu được chứng minh rằng với FORMAT COT, các mô hình nhỏ thực sự có thể vượt qua hiệu suất của một mô hình lớn hơn trong nhiệm vụ cụ thể này.

2 Phân tích Vấn đề và Benchmark

2.1 Định nghĩa Vấn đề và Động lực

LLMs được giao nhiệm vụ tạo ra các bảng có cấu trúc phức tạp, một quá trình bao gồm việc hiểu cả nội dung và các yêu cầu định dạng cụ thể, chẳng hạn như cú pháp LaTeX. Nhiệm vụ này vượt ra ngoài việc tạo văn bản đơn giản vì nó đòi hỏi độ chính xác không chỉ trong việc tạo nội dung mà còn trong việc tuân thủ một định dạng cấu trúc chi tiết và chính xác. Cụ thể, chúng tôi nhằm chuyển đổi dữ liệu văn bản không có cấu trúc thành dữ liệu bảng có cấu trúc, bằng cách trích xuất nội dung cần thiết từ văn bản và tuân theo cấu trúc hoặc định dạng yêu cầu.

2.2 Phân tích Vấn đề

Để đánh giá khả năng của LLMs trong việc chuyển đổi mô tả văn bản thành bảng có cấu trúc, chúng tôi đã sử dụng tập dữ liệu RotoWire (Wiseman et al., 2017), ban đầu là tập dữ liệu table-to-text, theo hướng ngược lại như một nhiệm vụ text-to-table. Sau khi đảm bảo rằng các mô tả chứa đủ thông tin để tạo bảng thông qua việc xem xét 20 mẫu, chúng tôi đã phát hiện những hạn chế đáng kể trong hiệu suất của GPT-3.5 và GPT-4, đặc biệt khi xử lý các cấu trúc phức tạp như được chi tiết trong Phụ lục A.

Khi được kiểm tra việc tạo ra dữ liệu trong các định dạng chính xác, chẳng hạn như bảng, cả GPT-3.5 và GPT-4, bất chấp khả năng tiên tiến của chúng, thường xuyên mắc lỗi, như được chứng minh bởi một nghiên cứu chú thích con người có hệ thống trên MTurk (tham khảo Phụ lục B). Các loại lỗi được phân loại thành 'Lỗi Phần tử', 'Lỗi Định dạng Phần tử', 'Lỗi Cấu trúc', và 'Lỗi Đặt tên Cấu trúc', được định lượng trong Hình 2. Chỉ có 3% đầu ra của GPT-3.5 hoàn toàn chính xác, với GPT-4 chỉ tốt hơn một chút ở mức 9%. Những kết quả này gợi ý những hạn chế thiết kế trong kiến trúc GPT, mặc dù hiệu quả trong việc bắt chước các mẫu ngôn ngữ, nhưng lại gặp khó khăn trong các nhiệm vụ đòi hỏi sự mạch lạc cấu trúc duy trì qua các chuỗi dài hơn.

2.3 Xây dựng Benchmark

Chúng tôi bắt đầu bằng việc chọn lọc các bảng lớn hơn 3x3 từ tập dữ liệu RotoWire (Wiseman et al., 2017) để trình bày mức độ phức tạp cơ bản. Sau đó, để mở rộng sự đa dạng tập dữ liệu của chúng tôi qua các lĩnh vực khác nhau, từ The Stack (Kocetkov et al., 2022), bao gồm mã GitHub trong 358 ngôn ngữ lập trình từ dự án BigCode, chúng tôi đầu tiên chọn các định dạng LaTeX và HTML. Tinh chỉnh thêm tập dữ liệu của chúng tôi, chúng tôi trích xuất các phần tử liên quan đến biểu diễn bảng để đảm bảo độ phức tạp tập trung và sự liên quan đến nhiệm vụ tạo dữ liệu có cấu trúc của chúng tôi. Một ví dụ về benchmark của chúng tôi được hiển thị trong Hình 4.

Bảng 1 cung cấp thống kê cho tập dữ liệu Rotowire và các tập dữ liệu được xây dựng của chúng tôi. Sau đó chúng tôi đánh giá 4 LLMs phổ biến, bao gồm GPT-NeoX-20B (Black et al., 2022), GPT-3.5, GPT-4, và Vicuna-13B (Chiang et al., 2023). Đối với dữ liệu LaTeX và HTML không có văn bản ghép đôi, chúng tôi khai thác GPT-3.5 để xây dựng các mô tả tổng hợp để được sử dụng làm đầu vào. Để đảm bảo chất lượng của benchmark của chúng tôi, chúng tôi lấy mẫu 50 bảng cho mỗi định dạng

--- TRANG 3 ---
9% 3% 91% 97%
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% GPT4 GPT-3.5
Đúng Lỗi 82% 88% 81% 89% 73% 75% 79% 81%
Lỗi Phần tử Lỗi Định dạng Phần tử Lỗi Đặt tên Cấu trúc Lỗi Cấu trúc

Hình 2: Phân tích lỗi bằng chú thích con người. Một số loại lỗi được giải thích trong Phụ lục A.

để đảm bảo tính chính xác của các mô tả. Ban đầu, chúng tôi đạt được tỷ lệ hài lòng 76%. Tuy nhiên, khi kết hợp một mẫu diễn giải thủ công (ví dụ: tên tab cho HTML) được thiết kế riêng cho từng định dạng (Phụ lục D), tỷ lệ hài lòng của chúng tôi đã cải thiện đáng kể, đạt 96%. Ví dụ, các bảng HTML có các thẻ và cấu trúc độc đáo của chúng, tuân thủ trung thành với các quy tắc cú pháp của ngôn ngữ HTML.

[THIS IS TABLE: Thống kê dữ liệu STRUC-BENCH]
Tập dữ liệu | # Huấn luyện | # Kiểm tra | Định dạng | Hàng & Cột
STRUC-BENCH Bảng | 3.4k | 728 | Văn bản thô | 7.26 & 8.75
STRUC-BENCH LATEX | 5.3k | 500 | LATEX | 2.75 & 4.47
STRUC-BENCH HTML | 5.4k | 499 | HTML | 5.50 & 3.54

Bảng 1: Thống kê dữ liệu STRUC-BENCH. Số lượng Hàng & Cột đã được tính trung bình.

3 Phương pháp

3.1 Tạo Dữ liệu

Như được hiển thị trong Hình 3, chúng tôi đề xuất FORMAT COT với GPT-3.5, một phương pháp self-instruct để tạo ra các cặp {dữ liệu, hướng dẫn} cho mục đích tinh chỉnh. Cụ thể, prompt FORMAT COT của chúng tôi bao gồm việc hướng dẫn các mô hình mô tả và diễn giải chính xác các phần tử định dạng được trình bày trong bảng đầu ra, lấy cảm hứng từ Wang et al. (2023b) trong nhiệm vụ tóm tắt. Để xác minh hiệu quả của FORMAT COT được đề xuất của chúng tôi, chúng tôi tiến hành một nghiên cứu ablation trong Phụ lục F.

3.2 Tinh chỉnh Hướng dẫn

Chúng tôi giới thiệu một phương pháp tinh chỉnh hướng dẫn được thiết kế đặc biệt để tăng cường khả năng của LLMs trong việc tạo ra văn bản có cấu trúc (Touvron et al., 2023; Patil et al., 2023). Cụ thể, chúng tôi kết hợp các mô tả định dạng do GPT-3.5 tạo ra của các bảng đầu ra và đầu vào văn bản gốc làm đầu vào mới của việc tinh chỉnh LLaMA. Nói cách khác, chúng tôi bắt đầu với GPT-3.5 xử lý dữ liệu bảng và tổng hợp các hướng dẫn định dạng toàn diện. Mô hình LLaMA sau đó được tinh chỉnh trên các hướng dẫn phong phú mà chúng tôi tạo ra. Phương pháp này mô phỏng tương tác người dùng-tác nhân trong đó GPT-3.5 hiệu quả thu thập và hợp nhất thông tin bảng, hướng dẫn LLaMA một cách đối thoại cho việc tạo văn bản cuối cùng, được nêu trong Hình 3.

3.3 Chỉ số Đánh giá

Đánh giá độ chính xác của các bảng được tạo ra so với ground truth là phức tạp do sự biến đổi trong định dạng, như HTML. Một chỉ số đánh giá lý tưởng cần phân biệt những khác biệt dữ liệu đáng kể trong khi bỏ qua những biến đổi định dạng tầm thường.

Chúng tôi đề xuất chia nhỏ sự tương đồng của hai

[HÌNH VẼ PHỨC TẠP HIỂN THỊ CÂUHỎI HƯỚNG DẪN CHO PROMPTING]

Hình 3: Hộp góc trên bên trái đại diện cho đầu vào gốc, đáng chú ý là thiếu mô tả về định dạng. Để hướng dẫn mô hình hiểu rõ về định dạng một cách rõ ràng, chúng tôi sử dụng FORMAT COT nằm ở bên phải, tạo ra <HƯỚNG DẪN ĐỊNH DẠNG>. Hộp dưới bên trái minh họa đầu vào cho việc tinh chỉnh LLaMA trông như thế nào sau khi đi qua FORMAT COT. <VĂN BẢN> cung cấp một văn bản mô tả cho đầu ra bảng mong đợi (đầu vào gốc), <BẢNG> phục vụ như một bảng tham chiếu (đầu ra), và <HƯỚNG DẪN ĐỊNH DẠNG> là một hướng dẫn định dạng được tạo ra thông qua FORMAT COT (được thêm vào đầu vào). Các prompt chi tiết được hiển thị trong Phụ lục D.1.

--- TRANG 4 ---
bảng thành hai thành phần thô: nội dung và định dạng. Trong việc tính điểm tương đồng nội dung, chúng tôi cố gắng phân tích nội dung ra dữ liệu bên trong các ô bảng, và tính toán sự tương đồng. Sự tương đồng này được tính toán giữa các ô bảng được tạo ra và ground-truth bằng các chỉ số tương đồng thường được sử dụng. Trong việc tính điểm tương đồng định dạng, chúng tôi đặt trọng số cao hơn lên các thành phần như số lượng cột và hàng, căn chỉnh ô, và tiêu đề bảng. Chúng tôi thấy rằng hai điểm số này cho phép chúng tôi thực hiện phân tích chi tiết hơn về nơi các bảng được dự đoán và ground-truth khác nhau. Việc triển khai hai điểm số này có thể được tìm thấy trong Phụ lục C.

3.3.1 P-Score

Chúng tôi thực hiện hai cách tiếp cận để tính điểm cho mỗi chỉ số. Đầu tiên, chúng tôi thực hiện đánh giá dựa trên mô hình, truy vấn GPT-3.5 với cả hai bảng và để mô hình tính điểm tương đồng của nội dung và định dạng riêng biệt. Theo Wang et al. (2023a), chúng tôi nhắc mô hình thực hiện lý luận Chain-of-Thought (Wei et al., 2023) trước khi đưa ra điểm số, và chúng tôi truy vấn mô hình với bảng được dự đoán và ground-truth theo cả hai thứ tự và tính trung bình điểm số. Chúng tôi báo cáo những điểm này là P-Score (Điểm Prompting).

3.3.2 H-Score

Ngoài ra, chúng tôi cũng triển khai các hàm tính điểm thủ công để tính điểm tương đồng của các bảng. Vì các bảng có thể được trình bày trong các định dạng khác nhau, chúng tôi triển khai một số heuristics để chuẩn hóa các bảng và tính toán sự tương đồng của chúng. Chúng tôi sử dụng trung bình của khoảng cách Levenshtein và chỉ số tương đồng Ratcliff/Obershelp để tính toán sự tương đồng giữa các chuỗi hoặc cấu trúc dữ liệu. Những chỉ số được chuẩn hóa heuristically này được báo cáo là H-Score (Điểm Heuristic). Phân tích có thể được tìm thấy trong Phụ lục A.3.

4 Thí nghiệm

4.1 Cài đặt Cơ bản

Đối với các chỉ số, chúng tôi sử dụng SacreBLEU, ROUGE-L, BERTScore, BARTScore, và các chỉ số BLEURT vì chúng đều là các chỉ số cổ điển để đánh giá tương đồng văn bản, cũng như hai chỉ số được đề xuất: P-Score và H-score. Chúng tôi đánh giá các mô hình sau: GPT-NeoX-20B, GPT-3.5, GPT-4, Vicuna-13B, LLaMA-7B, và LLaMA-7B được tinh chỉnh của chúng tôi. GPT-NeoX-20B, GPT-3.5 và GPT-4 đại diện cho hiệu suất tiên tiến nhất của các LLMs hiện tại và Vicuna-13B là một phiên bản khác được tinh chỉnh trên LLaMA, có thể đạt được 90% khả năng của GPT-3.5. Chúng tôi nghĩ những mô hình này đủ mạnh để có tính thuyết phục. Đối với 4 mô hình đầu tiên, chúng tôi đơn giản gọi APIs của chúng từ OpenAI hoặc HuggingFace để tạo ra kết quả mà không cần tinh chỉnh thêm. Trong tập dữ liệu của chúng tôi, mỗi mục bao gồm ba phần: hướng dẫn, đầu vào, và đầu ra. Khi tạo ra kết quả, chúng tôi kết hợp hướng dẫn và đầu vào của mỗi mục làm đầu vào cuối cùng cho các mô hình. Trong quá trình suy luận, người dùng cung cấp prompt bằng ngôn ngữ tự nhiên, điều này có thể dành cho một nhiệm vụ đơn giản (ví dụ: "vui lòng tạo ra một bảng dựa trên thông tin và định dạng sau"). Trong quá trình suy luận, chúng tôi cung cấp cho mô hình một prompt ngôn ngữ tự nhiên để mô tả định dạng và nội dung của nhiệm vụ, cũng như phản hồi mong đợi.

[THIS IS TABLE: Kết quả đánh giá tự động trên tập kiểm tra với nhiều loại chỉ số - bảng phức tạp hiển thị hiệu suất của các mô hình khác nhau trên các định dạng bảng khác nhau]

Bảng 2: Kết quả đánh giá tự động trên tập kiểm tra, bao gồm năm loại chỉ số trước đây và bốn chỉ số được đề xuất. w.o.finetune có nghĩa là chúng tôi cũng so sánh hiệu suất của mô hình không có tinh chỉnh như một nghiên cứu ablation. 'Ours-7B' là LLaMA được tinh chỉnh.

--- TRANG 5 ---
Hình 4: Một ví dụ minh họa cho benchmark của chúng tôi. Đầu vào bao gồm hướng dẫn và văn bản đầu vào, trong khi đầu ra nhằm trình bày bảng mục tiêu. Đáng chú ý, có một số sai sót trong đầu ra được dự đoán; ví dụ, 'Điểm trong hiệp 4' dưới 'Hawks' nên để trống, và tương ứng, 'Điểm trong hiệp 4' cho 'Magic' nên là 21.

4.2 Đánh giá Con người

Bảng 3 hiển thị kết quả đánh giá con người trên hai chỉ số được đề xuất với tương quan Pearson ở cấp độ thể hiện, phản ánh một thiết kế có mục đích phục vụ những nhu cầu cụ thể của đánh giá đầu ra có cấu trúc. Chúng tôi thuê năm sinh viên đại học để chú thích 200 ví dụ tập trung vào chất lượng nội dung và định dạng. Được trang bị mô tả đầu vào (với tham chiếu được đính kèm) và các đầu ra được tạo ra, họ chấm điểm từng khía cạnh trên thang điểm 10. Cả P-score và H-score đều thể hiện tương quan đáng kể với đánh giá con người, cho thấy tính mạnh mẽ và hiệu quả tương đối của chúng trong không gian đánh giá này. Mức độ tương quan này, vượt qua nhiều nỗ lực meta-evaluation trước đây (Fabbri et al., 2020; Tang et al., 2021), củng cố giá trị của các chỉ số của chúng tôi và giải quyết mối lo ngại về khả năng phản ánh đánh giá con người một cách đáng tin cậy.

Ngoài ra, chúng tôi đánh giá các chỉ số nổi tiếng bao gồm ROUGE-L, BERTScore, BARTScore, và BLEURT. Không gian hạn chế đã loại trừ một cuộc thảo luận đầy đủ, tuy nhiên Content P-score của chúng tôi cho thấy tương quan ở cấp độ thể hiện tốt nhất.

[THIS IS TABLE: Bảng 3 hiển thị kết quả đánh giá con người với các chỉ số tương quan nội dung và định dạng]

4.3 Kết quả

Bảng 2 cung cấp phân tích so sánh của các LLMs khác nhau dựa trên một số chỉ số. Đối với 'Bảng từ Văn bản Thô', Ours-7B vượt trội hơn các mô hình khác trong mỗi chỉ số. Thú vị là, không có tinh chỉnh, hiệu suất giảm đáng kể, đặc biệt trong SacreBLEU, ROUGE-L, và BERTScore. Kết quả cho 'LaTeX' cho thấy xu hướng tương tự và trong danh mục 'HTML', GPT-4 đạt điểm cao nhất trong SacreBLEU và BERTScore. Tuy nhiên, những khác biệt này là nhỏ và mô hình 7B của chúng tôi dẫn đầu trong phần còn lại của các chỉ số. Kết quả chứng minh rằng phương pháp của chúng tôi thể hiện hiệu suất vượt trội, làm nổi bật hiệu quả của việc tinh chỉnh các mô hình nhỏ hơn trong việc vượt qua các mô hình lớn hơn nhiều.

Hơn nữa, chúng tôi đi sâu vào phân tích dựa trên chú thích Mturk của chúng tôi, quy kết những thiếu sót quan sát được cho một số loại lỗi. Và chúng tôi trình bày một bản đồ khả năng trong Hình 5 và Phụ lục E.

[THIS IS FIGURE: Hình 5 hiển thị biểu đồ hình tròn thể hiện khả năng của các LLMs với các trục: Coverage, Formatting, Reasoning, Comprehension, Pragmatics, Hallucination Control cho Vicuna, ChatGPT, GPT-4, và Ours]

5 Kết luận

Tóm lại, nghiên cứu của chúng tôi cung cấp một phân tích kỹ lưỡng về những thách thức của LLMs trong việc tạo bảng có cấu trúc, giới thiệu các chỉ số đánh giá mới, và tập hợp một benchmark cụ thể bao gồm nhiều loại dữ liệu. Chúng tôi xác định các vấn đề chính bao gồm độ trung thực nội dung, tuân thủ định dạng, lý luận số học, và quản lý các bảng mở rộng.

--- TRANG 6 ---
6 Hạn chế

Mặc dù chúng tôi trình bày một phân tích toàn diện, việc khám phá LLMs trong tạo văn bản có cấu trúc được trình bày trong bài báo này có một số hạn chế:

Điều tra Định dạng Tối ưu cho Biểu diễn Bảng Trong nghiên cứu này, chúng tôi không điều tra định dạng bảng nào hiệu quả nhất. Các cách trình bày khác nhau của cùng một thông tin có thể hợp lý, và các chiến lược chuẩn hóa bảng, chẳng hạn như xác định cách tốt nhất để lập bảng các sự kiện cho trước hoặc cách kết nối nhiều bảng, vẫn chưa được khám phá. Nghiên cứu tương lai có thể tham gia vào nghiên cứu chuẩn hóa bảng để xác định các chiến lược tối ưu cho cấu trúc và biểu diễn dữ liệu bảng.

Phát triển Benchmark Cụ thể theo Lĩnh vực
Trong khi chúng tôi đã đạt được tiến bộ trong việc xây dựng benchmark cho tạo văn bản có cấu trúc, có thể có lợi khi phát triển các benchmark phục vụ các lĩnh vực cụ thể. Các lĩnh vực khác nhau có thể có yêu cầu cấu trúc độc đáo và hiểu những sắc thái này có thể cải thiện đáng kể khả năng áp dụng của các mô hình qua các bối cảnh đa dạng.

Mở rộng Phạm vi Tập dữ liệu Có vô số loại và nguồn dữ liệu có thể được khám phá. Kết hợp một loạt tập dữ liệu rộng hơn có thể tiếp xúc các mô hình với một phạm vi định dạng cấu trúc thậm chí rộng hơn, cuối cùng tăng cường hiệu suất tổng thể của chúng.

Tăng cường Khả năng Lý luận Số học
Nghiên cứu của chúng tôi xác định lý luận số học không đủ là một trong những thách thức mà LLMs phải đối mặt. Điều tra các kỹ thuật để tăng cường lý luận số học trong các mô hình này có thể dẫn đến những cải thiện đáng kể trong hiệu suất của chúng.

Phát triển Phương pháp Tiên tiến Trong khi phương pháp tinh chỉnh hướng dẫn có nhận thức cấu trúc của chúng tôi cho thấy kết quả đầy hứa hẹn, các kỹ thuật tinh vi hơn có thể được phát triển. Ví dụ, công việc tương lai có thể khám phá các cách kết hợp thông tin cấu trúc rõ ràng hơn vào mô hình hoặc phát triển các phương pháp cho phép mô hình học các mẫu cấu trúc hiệu quả hơn.

Khám phá LLMs Đa phương thức Khi LLMs tiếp tục phát triển, có cơ hội khám phá các mô hình đa phương thức có thể xử lý và tạo ra cả văn bản và các dạng dữ liệu khác, chẳng hạn như âm thanh hoặc hình ảnh (Kamigaito et al., 2023), theo cách có cấu trúc.

Tài liệu tham khảo

[Danh sách tài liệu tham khảo tiếp tục với các công trình nghiên cứu được trích dẫn...]

--- TRANG 7 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 8 ---
A Phân tích với Ví dụ

A.1 Ví dụ Bảng A

Sự khác biệt chính giữa các bảng tham chiếu và các bảng được tạo ra bởi GPT-3.5 và GPT4, được hiển thị trong hình 6, là về tính đầy đủ và chính xác của dữ liệu được cung cấp.

Trong các bảng tham chiếu, tất cả dữ liệu liên quan được biểu diễn đầy đủ: Đối với các đội (Bảng 1), mỗi đội có một số hoặc phần trăm chính xác cho mỗi thống kê. Tương tự, đối với các cầu thủ (Bảng 2), mỗi cầu thủ có một số xác định cho mỗi thống kê, bao gồm số phút chơi theo định dạng "mm:ss".

Ngược lại, các bảng được tạo ra cho thấy dữ liệu không đầy đủ và không chính xác. Đối với bảng được tạo ra bởi GPT-3.5, bảng thống kê đội có một số thống kê bị thiếu, được biểu diễn bằng các ô trống, và một số không được trình bày dưới dạng phần trăm. Bảng thống kê cầu thủ cũng có dữ liệu bị thiếu tương tự, và nó thiếu hoàn toàn thống kê "số phút chơi". Ví dụ, trong bảng 'đội', cột "Phần trăm ném ghi điểm" cho Suns bị thiếu. Tương tự, trong bảng 'cầu thủ', nhiều thống kê chính như "3 điểm thực hiện", "3 điểm ghi được", "Số lần ném thực hiện", "Số lần ném ghi được", và "Số phút chơi" bị thiếu cho nhiều cầu thủ khác nhau.

Về định dạng, chúng tôi quan sát nhiều lỗi định dạng. Ví dụ, cột 'Phần trăm ném ghi điểm' cho Grizzlies được biểu diễn là "50" thay vì "50.0%". Hơn nữa, cột 'Thắng' cho Suns được biểu diễn là "3" thay vì "0". Sự biểu diễn sai này có thể dẫn đến hiểu lầm đáng kể về dữ liệu. Bảng 'Cầu thủ' cũng có lỗi định dạng. Ví dụ, cột 'Số phút chơi' thiếu định dạng thời gian (tức là, "00:00").

Mặt khác, các bảng tham chiếu tuân thủ định dạng tiêu chuẩn. Dữ liệu phần trăm được biểu diễn với dấu '%', dữ liệu thời gian sử dụng định dạng '00:00', và dữ liệu số biểu diễn chính xác mỗi thống kê.

Đối với kết quả Vicuna-13B được hiển thị trong hình 7, mặc dù nó có định dạng đúng cho cả hai bảng, vẫn còn nhiều lỗi phần tử. Ví dụ, bảng 'đội' có thống kê sai như "Thua" và "Thắng" cho Suns. Bên cạnh đó, trong bảng 'cầu thủ', nhiều ô không nên có dữ liệu. Tuy nhiên, chúng có, đây là một lỗi. Một số ô như 'Kiến tạo' của Isaiah Thomas và Eric Bledsoe nên là 2 và 4, nhưng chúng không có trong bảng 'cầu thủ' của Vicuna-13B. Tương tự, kết quả LLaMA2-7B có cùng lỗi phần tử trong bảng 'đội' và lỗi tệ hơn trong bảng 'cầu thủ'. Nó điền tất cả các ô, nhiều ô trong số đó nên không có gì. Đối với một số ô nên có dữ liệu, dữ liệu của chúng được điền sai như 'Kiến tạo' và 'Số lần ném ghi được' của Eric Bledsoe.

[Tiếp tục với các bảng và mô tả chi tiết về lỗi trong các kết quả tạo bảng của GPT-3.5 và GPT-4]

--- TRANG 9 ---
[Tiếp tục với ví dụ và phân tích lỗi của Vicuna-13B và LLaMA2-7B]

--- TRANG 10 ---
[Tiếp tục với ví dụ GPT-4 và phân tích lỗi]

--- TRANG 11 ---
A.2 Loại Lỗi

Lỗi Cấu trúc: Những lỗi này liên quan đến tính toàn vẹn cấu trúc của các bảng được tạo ra. Cụ thể, chúng bao gồm các trường hợp có hàng hoặc cột thừa hoặc thiếu so với cấu trúc bảng đúng. Ví dụ, trong hình 8, kết quả được tạo ra bởi GPT4 thiếu các cột như "Thắng" và "Thua" trong bảng 'đội'.

Lỗi Đặt tên Cấu trúc: Danh mục này ghi lại các lỗi liên quan đến các quy ước đặt tên được sử dụng cho hàng hoặc cột. Bất kỳ sự khác biệt nào trong tên hàng hoặc cột giữa bảng được tạo ra và bảng đúng được đánh dấu là lỗi đặt tên cấu trúc. Ví dụ, trong hình 8, kết quả được tạo ra bởi GPT-4 có tên cột sai như "Điểm Hiệp Một" trong bảng 'đội'.

Lỗi Phần tử: Đây là những sai sót được quan sát ở cấp độ phần tử trong bảng được tạo ra. Lỗi phần tử bao gồm các số, giá trị không chính xác, hoặc các ô trống không phù hợp, phản ánh sự khác biệt trong các mục bảng riêng lẻ so với bảng đúng. Trong hình 6 và hình 7, hầu hết các lỗi là lỗi phần tử.

--- TRANG 12 ---
A.3 Hiệu quả Chỉ số

Bảng 4 thể hiện các kết quả khác nhau của các ví dụ trong Phụ lục A.1 dựa trên chỉ số H-score của chúng tôi. Lấy GPT-4 làm ví dụ, có sự giảm nhẹ từ 2.0 xuống 1.86 trong content H-score, và rất chính xác, xu hướng này được theo sau bởi kết quả của GPT-4 cho thấy các lỗi nhẹ trong nội dung của nó. Đối với format H-score, GPT-4 cũng không thể làm tốt cho hiệu suất số của nó, không đủ gần với điểm số đầy đủ, phù hợp với hiệu suất kém của GPT-4 trong lĩnh vực này. H-scores của các mô hình khác cũng theo xu hướng này. Do đó, H-score có thể phản ứng với sự khác biệt trong cả nội dung và định dạng một cách chính xác.

[THIS IS TABLE: Bảng 4 hiển thị H-scores cho các kết quả khác nhau của các mô hình]

--- TRANG 13 ---
B MTurk

Để thúc đẩy phân tích toàn diện về đầu ra LLM, chúng tôi đã thiết kế một nhiệm vụ trên Amazon Mechanical Turk (MTurk) để thu thập các chú thích chi tiết về các loại lỗi khác nhau gặp phải trong các bảng có cấu trúc được tạo ra. Nhiệm vụ được cấu trúc như sau:

Từ tập dữ liệu RotoWire, chúng tôi ngẫu nhiên chọn 100 trường hợp đầu ra được tạo ra bởi LLM, đảm bảo sự kết hợp đại diện về chất lượng dựa trên đánh giá sơ bộ.

Mỗi Human Intelligence Task (HIT) trình bày cho người chú thích một cái nhìn cạnh nhau về đầu ra LLM và định dạng bảng có cấu trúc mong đợi. Người chú thích được hướng dẫn xác định và phân loại lỗi theo các loại được định nghĩa trước: 'Lỗi Phần tử', 'Lỗi Định dạng Phần tử', 'Lỗi Cấu trúc', và 'Lỗi Đặt tên Cấu trúc'.

Chúng tôi cung cấp hướng dẫn mở rộng, được minh họa bằng hướng dẫn từng bước, để làm rõ các trường hợp điển hình của mỗi loại lỗi. Những hướng dẫn này đã được xem xét và cải thiện lặp đi lặp lại thông qua một nghiên cứu thí điểm được tiến hành với một nhóm nhỏ người chú thích.

Về trình độ của các nhân viên Amazon Mechanical Turk (MTurk), chúng tôi sử dụng các trình độ sau để tuyển dụng tổng cộng 10 nhân viên MTurk có thành tích tốt: Tỷ lệ phê duyệt HIT lớn hơn hoặc bằng 98%, số HITs được phê duyệt lớn hơn hoặc bằng 500, và nằm ở một trong các quốc gia nói tiếng Anh bản địa sau: Úc, Canada, New Zealand, Vương quốc Anh, Hoa Kỳ. Mỗi người chú thích được giới hạn chú thích 10 ví dụ, bao gồm cả đầu ra của GPT-3.5 và GPT-4.

Các nhân viên chú thích được trả $7, hiệu chỉnh để bằng mức lương $42/giờ. Chúng tôi đầu tiên chú thích các ví dụ trong nội bộ để xác định tốc độ chú thích cần thiết. Một khối tóm tắt thường mất khoảng 10 phút.

Để chứng minh mẫu chú thích của chúng tôi và tạo điều kiện cho nghiên cứu tương lai, chúng tôi hiển thị giao diện cho các chú thích.

[Các hình ảnh giao diện MTurk được hiển thị]

--- TRANG 14 ---
C Tính điểm

C.1 P-Score

Phương pháp của chúng tôi bao gồm việc nhắc mô hình tham gia vào lý luận Chain-of-Thought trước khi đưa ra điểm số. Đầu tiên, chúng tôi hướng dẫn GPT về cách đánh giá cả "tương đồng nội dung" và "tương đồng cấu trúc". Sau đó, mô hình được hướng dẫn về quy trình đúng để đưa ra câu trả lời. Để tính điểm số, mô hình được truy vấn với cả bảng được dự đoán và bảng ground truth theo các chuỗi khác nhau, sau đó điểm số được tính trung bình. Chúng tôi sẽ minh họa quá trình này bằng cách sử dụng prompt P-Scores cho bảng văn bản thô làm ví dụ minh họa:

"Dựa trên những điều trên, chúng tôi muốn xác định xem các bảng trên có tương tự không. Lý tưởng nhất, chúng nên có nội dung và cấu trúc giống hệt nhau. Chấm điểm "tương đồng nội dung" và "tương đồng cấu trúc" từ 0 đến 10.

- Tương đồng nội dung: 10 nếu nội dung của các ô bảng giống hệt nhau, 0 nếu chúng hoàn toàn khác nhau. Nếu khoảng 50% các ô có cùng dữ liệu, điểm số nên là 5.

- Tương đồng cấu trúc: 10 nếu các bảng có cùng cấu trúc (ví dụ: cùng cột và hàng với thứ tự giống hệt nhau, cùng căn chỉnh, v.v.) mặc dù sự khác biệt về định dạng văn bản có thể được bỏ qua (ví dụ: màu sắc, phông chữ).

Đưa ra một đối tượng JSON như sau:
"""json
{{
"content_similarity": ...
"structural_similarity": ...
}}
"""

Suy nghĩ cẩn thận, sau đó đưa ra điểm số."

Ví dụ, trong hình 11, cả hai bảng đều có cấu trúc giống hệt nhau, vì vậy điểm tương đồng cấu trúc của chúng là 10. Nội dung của bảng đầu tiên của Table1 và Table2 giống nhau, và bảng thứ hai của Table1 và Table2 tương tự khoảng 10%. Do đó điểm tương đồng nội dung của chúng là 5.

C.2 H-Score

Chúng tôi đính kèm thuật toán của chúng tôi để tính H-Score như Thuật toán 1.

LATEX Chúng tôi sử dụng thư viện pylatexenc để phân tích một bảng LATEX cho trước, và đi qua cấu trúc parse-tree trong môi trường tabular để xác định các "ô" bảng. Chúng tôi tính điểm tương đồng nội dung dựa trên các chuỗi trong các ô, và tính điểm tương đồng cấu trúc dựa trên việc có số hàng và cột khớp, cùng tiêu đề, và cùng căn chỉnh ô.

HTML Chúng tôi sử dụng thư viện beautifulsoup4 để phân tích một đoạn HTML LATEX cho trước và đi qua cấu trúc parse-tree trong các thẻ <table>, <ul> hoặc <ol> để xác định các ô dữ liệu. Chúng tôi riêng biệt xây dựng một cây các thẻ HTML được liệt kê trắng để tính điểm tương đồng cấu trúc, duyệt qua cấu trúc cây tài liệu HTML, bỏ qua nội dung thực tế trong các thẻ và đơn giản hóa nó bằng cách chỉ tập trung vào các thẻ HTML cụ thể (được định nghĩa trong RECOGNIZED_HTML_TAGS). Chúng tôi tính điểm tương đồng nội dung dựa trên các chuỗi trong các ô và tính điểm tương đồng cấu trúc dựa trên sự tương đồng của cây cấu trúc và tổng số ô khớp.

Các thẻ HTML được liệt kê trắng:
RECOGNIZED_HTML_TAGS = [
"table", "tr", "th", "td",
"ul", "ol", "li",
"div", "span", "p",
"a", "img", "embed", "pre",
"h1", "h2", "h3", "h4", "h5", "h6",
"input", "button",
]

Bảng Văn bản Thô Trong tập dữ liệu được đánh giá của chúng tôi, mỗi ví dụ bao gồm hai bảng (Team và Player). Chúng tôi thực hiện tìm kiếm chuỗi cho tiêu đề "Team" và "Player" để xác định hai bảng. Sau đó chúng tôi phân tích các bảng theo định dạng Markdown, với dòng mới và ống làm dấu phân cách hàng và cột tương ứng, để xác định các ô bảng. Chúng tôi tính điểm tương đồng nội dung dựa trên các chuỗi trong các ô, và tính điểm tương đồng cấu trúc dựa trên sự tương đồng của tên cột và số hàng và cột khớp.

Đo lường Tương đồng Chuỗi: Script của chúng tôi bao gồm các phương pháp để tính toán sự tương đồng giữa hai chuỗi. Những phương pháp này có thể được sử dụng để so sánh cấu trúc hoặc nội dung của các tài liệu HTML, latex, hoặc bất kỳ cặp chuỗi nào khác. Sự tương đồng được đánh giá bằng cách sử dụng các thuật toán được thiết lập tốt trong phân tích văn bản: khoảng cách Levenshtein và SequenceMatcher từ module difflib của Python.

--- TRANG 15 ---
[Hình 11 và ví dụ tính toán P-score]

--- TRANG 16 ---
[Thuật toán 1: Thuật toán H-score với mã giả]

--- TRANG 17 ---
D Prompt cho FormatCoT và Suy luận

D.1 Prompt cho FormatCoT

Mô tả Bảng Văn bản Thô Các tập dữ liệu data-to-text truyền thống chỉ có văn bản thô cho mỗi bảng. Tuy nhiên, điều này không đủ để GPT-3.5 hoặc các LLMs khác tạo ra các bảng đúng. Kết quả là, chúng tôi đã thêm một số mô tả định dạng để giúp chúng tạo ra các bảng đúng. Chúng tôi sử dụng GPT-3.5 để đạt được điều này. Chúng tôi muốn có thông tin định dạng chi tiết mà không có nội dung cụ thể trong các ô, vì vậy chúng tôi rõ ràng bao gồm những yêu cầu này trong prompt. Đây là prompt của chúng tôi: "Mô tả chi tiết về văn bản cho trước. Đầu tiên, đưa ra số lượng bảng, và sau đó cho mỗi bảng, mô tả định dạng của nó như số lượng cột và hàng, tên cột, và tên hàng."

Mô tả Bảng HTML Khác với các tập dữ liệu data-to-text, các tập dữ liệu HTML chỉ có đầu ra cuối cùng, vì vậy chúng tôi được yêu cầu tạo ra một mô tả chi tiết về định dạng và nội dung của chúng. Đối với mô tả nội dung, chúng tôi có thể đơn giản yêu cầu GPT-3.5 đưa ra văn bản thô không có thẻ HTML. Đối với mô tả định dạng, tuy nhiên, chúng tôi cần yêu cầu GPT-3.5 mô tả từng thẻ, nếu không, nó sẽ bỏ qua một số thẻ và mô tả bảng một cách tổng quát thay vì thông tin chi tiết. Hơn nữa, cần thiết phải yêu cầu nó sử dụng các số cụ thể thay vì 'một số' hoặc 'nhiều'. Đây là prompt của chúng tôi cho mô tả định dạng HTML: "Mô tả định dạng của HTML này một cách chi tiết theo từng thẻ HTML của mã HTML sau. Hãy cẩn thận và đảm bảo không bỏ lỡ bất kỳ thẻ HTML nào. Vui lòng sử dụng hơn 300 từ để giải thích định dạng. Sử dụng các số cụ thể thay vì mơ hồ về một số."

Mô tả Bảng LaTeX Tương tự như tạo prompt HTML, cần thiết phải yêu cầu GPT-3.5 tạo ra cả mô tả định dạng và mô tả nội dung vì các tập dữ liệu latex chỉ có đầu ra cuối cùng. Đối với mô tả nội dung, chúng tôi có thể đơn giản yêu cầu GPT-3.5 mô tả bảng latex cho trước một cách chi tiết như nó có thể và bao gồm tất cả các ô. Đối với mô tả định dạng, vì định dạng latex quá phức tạp, chúng tôi cần đưa cho nó một ví dụ nhỏ để học. Sau đó chúng tôi yêu cầu GPT-3.5 mô tả định dạng chi tiết của một bảng latex cho trước, bao gồm các câu hỏi cụ thể để giúp nó tạo ra mô tả định dạng. Đây là prompt của chúng tôi cho mô tả định dạng latex: "Mô tả định dạng chi tiết của một bảng latex cho trước theo các lệnh và thẻ với hơn 500 từ. Bao gồm: Có đường viền bảng không? Căn chỉnh văn bản như thế nào? Thuộc tính bảng là gì? Có in đậm không? Có thêm \ref không? Vui lòng giải thích rõ ràng có đường ngang và dọc bao quanh mỗi hàng và cột không. Nói gì về token định dạng đặc biệt "\" trong latex nếu có. Không hiển thị mã latex trực tiếp. Sử dụng ngôn ngữ tự nhiên. Và cung cấp đủ thông tin định dạng cho tôi để tái tạo bảng này dựa trên mô tả đầu ra của bạn."

D.2 Prompt cho Suy luận

Khi suy luận các bảng văn bản thô, LLMs có xu hướng đưa ra kết quả dạng bảng thay vì bảng văn bản thô. Kết quả là, chúng tôi cần đưa cho nó một ví dụ đầu ra trước, sau đó nói với mô hình rằng đầu vào bao gồm hai phần, văn bản và mô tả định dạng, và yêu cầu mô hình tạo ra đầu ra dựa trên chúng. Đối với suy luận HTML và Latex, chúng tôi có thể đơn giản yêu cầu các mô hình suy luận từ đầu vào và chỉ định các phần định dạng và nội dung trong đầu vào, vì các mô hình có thể tạo ra cú pháp đúng.

--- TRANG 18 ---
[Bảng 5: Các prompt của chúng tôi cho FORMAT COT và Suy luận]

--- TRANG 19 ---
E Bản đồ Khả năng

Dựa trên đánh giá tự động của chúng tôi, chúng tôi đã chọn Vicuna, GPT-3.5, GPT-4, và Ours làm các mô hình đại diện và tiến hành phân tích sâu về nguyên nhân của các lỗi mô hình.

Chúng tôi xác định độ chính xác nội dung, định dạng, lý luận số học, và xử lý các bảng dài là những nguồn chính của các lỗi này.

Ở cấp độ cơ bản, chúng tôi phân tách quá trình các đầu ra có cấu trúc phức tạp được tạo ra bởi mô hình thành hai phần: Lựa chọn Nội dung và Lập kế hoạch Định dạng. Ban đầu, mô hình cần xác định thông tin quan trọng từ một lượng lớn đầu vào không có cấu trúc cho trước, trích xuất thông tin này, hiểu nó, và tổ chức nó. Tiếp theo, nó cần lập kế hoạch cách tóm tắt những chi tiết được trích xuất này, thiết kế định dạng của bảng cần tạo ra, và sau đó điền thông tin.

Tương ứng, chúng tôi có thể chia nhỏ khả năng của mô hình thành Phạm vi, Lý luận Định dạng, Hiểu biết, Thực dụng, và Kiểm soát Ảo giác.

Phạm vi bao gồm khả năng của mô hình để bao phủ chính xác nội dung trong đầu vào. Lý luận Định dạng liên quan đến đánh giá về định dạng đầu ra, đánh giá xem mô hình có thể tìm thấy định dạng có cấu trúc phù hợp và hợp lý nhất hay không.

Hiểu biết phản ánh liệu mô hình có thể hiểu nội dung của đầu vào hay không, vì có những lúc cần thiết phải suy luận từ một lượng lớn dữ liệu (bao gồm thực hiện phép cộng hoặc trừ hoặc so sánh nhiều phần tử).

Thực dụng bao gồm khả năng sử dụng các định dạng đặc biệt, chẳng hạn như thẻ HTML và cú pháp cụ thể trong LaTeX.

Cuối cùng, Kiểm soát Ảo giác biểu thị khả năng của mô hình để tránh tạo ra nội dung không có trong đầu vào.

Chúng tôi đã thực hiện chú thích thủ công và thu được kết quả trực quan để chứng minh những khía cạnh này.

--- TRANG 20 ---
F Nghiên cứu Ablation cho FormatCoT

F.1 Tương phản giữa các mô tả

Trong phần này, chúng tôi tiến hành một nghiên cứu ablation để kiểm tra tác động của FORMAT COT được đề xuất của chúng tôi. Trong việc tạo ra mô tả bảng không có FORMAT COT, chúng tôi đơn giản sử dụng prompt: "Cung cấp mô tả về các bảng sau." Chúng tôi hiển thị kết quả trong hình 12. Sự khác biệt chính giữa các kết quả tập trung vào mức độ chi tiết được kết hợp.

Ví dụ, trong kết quả FORMAT COT, mô tả bao gồm một loạt thông tin định dạng chi tiết - bao gồm tên hàng, tên cột, và số lượng bảng. Độ chính xác trong những chi tiết này đủ đáng kể để các mô hình tái tạo chính xác các bảng đang được xem xét.

Ngược lại, kết quả thiếu FORMAT COT truyền đạt ít thông tin hơn đáng kể - cung cấp tên cột không đầy đủ mà không có sự đi kèm của tên hàng. Mức độ chi tiết thưa thớt này không đủ cho các mô hình tìm cách tái tạo trung thành các bảng tương ứng.

F.2 Tương phản giữa các kết quả

Trong phần này, chúng tôi so sánh giữa hai bộ kết quả mô tả, được hiển thị trong hình 13. Kết quả FORMAT COT thể hiện một bảng đứng rất gần với bảng đúng, mặc dù có những lỗi nhỏ. Nó chứa một hàng thừa có tên là "Player" trong bảng đầu tiên, một sự khác biệt có thể được quy cho thực tế là kết quả bao gồm hai bảng, với "Player" biểu thị tiêu đề của bảng tiếp theo. Chúng tôi cho rằng lỗi này có thể được tránh với một phương pháp khác để tích hợp tên bảng.

Hơn nữa, một cột bổ sung xuất hiện trong bảng thứ hai, trong thực tế đại diện cho hàng cuối cùng của bảng đó. Bên cạnh những sai sót nhỏ này, kết quả FORMAT COT tái tạo chính xác nội dung trong mỗi ô cũng như duy trì định dạng tổng thể.

Ngược lại, kết quả thay thế chứa nhiều lỗi trải dài cả nội dung và định dạng. Ban đầu, một hàng bổ sung có mặt trong bảng đầu tiên, giới thiệu một đội bóng rổ không liên quan không có mối liên hệ với trận đấu đang được xem xét. Sau đó, bảng thứ hai có số lượng tên cầu thủ quá mức, bao gồm các cầu thủ không cần thiết cùng với các huấn luyện viên không tham gia trận đấu.

Hơn nữa, nội dung của nó không hoàn toàn chính xác, với sự khác biệt có mặt trong thống kê được quy cho cả Gordon Hayward và Gerald Green. Những thiếu sót này nhấn mạnh hiệu quả và sự cần thiết của việc triển khai FORMAT COT để đảm bảo độ chính xác và chính xác.

--- TRANG 21 ---
[Hình 12 và 13 hiển thị so sánh giữa FORMAT COT và không có FORMAT COT]
