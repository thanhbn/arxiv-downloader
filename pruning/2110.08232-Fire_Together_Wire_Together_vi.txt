# 2110.08232.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2110.08232.pdf
# Kích thước tệp: 3058739 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bắn Cùng Nhau Nối Dây Cùng Nhau:
Một Phương Pháp Cắt Tỉa Động với Dự Đoán Mặt Nạ Tự Giám Sát
Sara Elkerdawy1Mostafa Elhoushi2Hong Zhang1
Nilanjan Ray1
1Đại học Alberta,2Phòng thí nghiệm Trình biên dịch Dị thể Toronto, Huawei
{elkerdaw, hzhang, nray1 }@ualberta.ca
Tóm tắt
Cắt tỉa mô hình động là một hướng nghiên cứu gần đây cho phép suy luận một mạng con khác nhau cho mỗi mẫu đầu vào trong quá trình triển khai. Tuy nhiên, các phương pháp động hiện tại dựa vào việc học một cổng kênh liên tục thông qua việc chính quy hóa bằng cách tạo ra tổn thất thưa thớt. Công thức này gây ra độ phức tạp trong việc cân bằng các tổn thất khác nhau (ví dụ tổn thất nhiệm vụ, tổn thất chính quy hóa). Ngoài ra, các phương pháp dựa trên chính quy hóa thiếu sự lựa chọn siêu tham số đánh đổi minh bạch để thực hiện một ngân sách tính toán. Đóng góp của chúng tôi có hai khía cạnh: 1) tách biệt tổn thất nhiệm vụ và cắt tỉa. 2) Lựa chọn siêu tham số đơn giản cho phép ước tính giảm FLOPs trước khi huấn luyện. Lấy cảm hứng từ lý thuyết Hebbian trong Khoa học thần kinh: "các tế bào thần kinh bắn cùng nhau sẽ nối dây cùng nhau", chúng tôi đề xuất dự đoán một mặt nạ để xử lý k bộ lọc trong một lớp dựa trên việc kích hoạt của lớp trước đó. Chúng tôi đặt vấn đề như một bài toán phân loại nhị phân tự giám sát. Mỗi mô-đun dự đoán mặt nạ được huấn luyện để dự đoán liệu log-likelihood cho mỗi bộ lọc trong lớp hiện tại có thuộc về top-k bộ lọc được kích hoạt hay không. Giá trị k được ước tính động cho mỗi đầu vào dựa trên một tiêu chí mới sử dụng khối lượng của bản đồ nhiệt. Chúng tôi trình bày các thí nghiệm trên nhiều kiến trúc mạng nơ-ron như VGG, ResNet và MobileNet trên các bộ dữ liệu CIFAR và ImageNet. Trên CIFAR, chúng tôi đạt độ chính xác tương tự với các phương pháp SOTA với việc giảm FLOPs cao hơn 15% và 24%. Tương tự trên ImageNet, chúng tôi đạt được sự sụt giảm độ chính xác thấp hơn với cải thiện lên đến 13% trong việc giảm FLOPs. Mã nguồn có sẵn tại https://github.com/selkerdawy/FTWT

1. Giới thiệu
Mạng Nơ-ron Tích chập (CNNs) đã cho thấy sự tăng trưởng chưa từng có trong thập kỷ qua, đại diện cho công nghệ tiên tiến trong nhiều lĩnh vực. Tuy nhiên, CNNs đòi hỏi tính toán và tiêu thụ bộ nhớ lớn, điều này hạn chế việc triển khai trên các nền tảng edge và nhúng.

25 30 35 40 45 50
Giảm FLOPs (%)0.00.51.01.52.02.53.03.5Sụt giảm độ chính xác (%)
FTWT (của chúng tôi)
Taylor
LCCL
SFP
FPGM
ResNet18Hình 1. Giảm FLOPs so với sụt giảm độ chính xác từ baseline cho các mô hình động và tĩnh khác nhau trên ResNet34 ImageNet.

Có nhiều tiến bộ trong nghiên cứu nén mô hình bao gồm các mô hình nhẹ được thiết kế thủ công [16, 17], độ chính xác bit thấp [24, 51], tìm kiếm kiến trúc [4, 42], và cắt tỉa mô hình [9,22,31,36]. Hầu hết các kỹ thuật nén đều không phụ thuộc vào dữ liệu đầu vào và tối ưu hóa cho một mô hình hiệu quả tĩnh. Những nỗ lực gần đây trong tài liệu cắt tỉa đề xuất giữ nguyên backbone cho mô hình baseline và thực hiện suy luận sử dụng các mạng con khác nhau có điều kiện trên đầu vào. Điều này được gọi là cắt tỉa động, trong đó các tuyến đường khác nhau được kích hoạt dựa trên đầu vào, cho phép mức độ tự do cao hơn và linh hoạt hơn so với cắt tỉa tĩnh.

Các phương pháp cắt tỉa động hiện tại thường giới thiệu một thuật ngữ chính quy hóa để tạo ra tính thưa thớt trên một tham số liên tục cho việc gating/masking kênh [6, 10, 44]. Những phương pháp khác áp dụng gradient chính sách được giới thiệu trong học tăng cường [46] để học các tuyến đường khác nhau. Các phương pháp này đòi hỏi việc điều chỉnh cẩn thận trong huấn luyện để giải quyết các vấn đề như tính ổn định huấn luyện với việc ủ lịch trình [44], xử lý huấn luyện thiên vị [18], hoặc tỷ lệ cắt tỉa được định trước cho mỗi lớp [10, 27]. Ngoài ra, như được ghi nhận trong [6], tổn thất thưa thớt bổ sung làm giảm chất lượng nhiệm vụ

--- TRANG 2 ---
xe hơi
hươu
ngựa
xe tải
tàu
máy bay
 chim
chó
 mèo
ếch
0.00.20.40.60.81.0
Kích hoạt mỗi bộ lọcMẫu đầu vào(a) Lớp tích chập cuối cùng
ếch
xe hơi
ngựa
hươu
 mèo
chim
tàu
máy bay
xe tải
chó
0.00.20.40.60.81.0
Kích hoạt mỗi bộ lọcMẫu đầu vào (b) Lớp tích chập theo chiều sâu thứ 8

Hình 2. Kích hoạt tối đa trong tất cả các đặc trưng tại lớp tích chập cuối cùng và một lớp giữa trong mobilenetv1 CIFAR-10. Mỗi hàng trong một biểu đồ con đại diện cho một mẫu đầu vào. Các mẫu thuộc cùng một lớp kích hoạt cùng nhóm bộ lọc. Được hiển thị tốt hơn trong màu.

tổn thất vì khó cân bằng tổn thất nhiệm vụ và tổn thất cắt tỉa đặc biệt dưới tỷ lệ cắt tỉa cao như được hiển thị trong Hình 1. Hơn nữa, việc giảm FLOPs của các phương pháp động này phụ thuộc vào siêu tham số thưa thớt mục tiêu được đặt trước. Việc lựa chọn siêu tham số này thiếu mối quan hệ minh bạch giữa siêu tham số thưa thớt và FLOPs đạt được; do đó, cản trở việc huấn luyện hiệu quả thực tế với nhiều lần lặp thử và sai để đạt được một mục tiêu giảm FLOPs.

Trong bài báo này, chúng tôi giải quyết những vấn đề này bằng cách xây dựng vấn đề như một nhiệm vụ phân loại nhị phân tự giám sát. Chúng tôi tạo ra mặt nạ nhị phân của lớp hiện tại (nối dây) dựa trên việc kích hoạt (bắn) của lớp trước đó. Chúng tôi lấy cảm hứng từ lý thuyết Hebbian [33] trong Khoa học thần kinh với một sự xoắn là chúng tôi thực thi mối quan hệ nối dây-bắn này thay vì một nghiên cứu về nguyên nhân như trong lý thuyết. Hình 2 vẽ đồ thị phản ứng tối đa cho mỗi bộ lọc (trục x) của lớp tích chập cuối cùng và một lớp giữa của MobileNet-V1 cho các mẫu đầu vào ngẫu nhiên (trục y) được nhóm theo lớp của chúng. Biểu đồ cho thấy rằng các mẫu thuộc cùng một lớp có xu hướng kích hoạt cùng một tổ hợp bộ lọc và do đó chúng tôi chỉ cần xử lý một số ít bộ lọc. Điều đáng chú ý là số lượng cụm khác nhau theo từng lớp. Tương tự như các phương pháp cắt tỉa động khác, chúng tôi học một đầu quyết định cho việc gating kênh. Tuy nhiên, chúng tôi học gating sử dụng tổn thất entropy chéo nhị phân cho mỗi kênh. Mỗi lớp dự đoán các bộ lọc có khả năng cao nhất được kích hoạt mạnh mẽ với các kích hoạt đầu vào của lớp. Chúng tôi tạo ra mặt nạ nhị phân ground truth cho mỗi lớp dựa trên khối lượng của bản đồ nhiệt cho mỗi mẫu. Công thức này cung cấp lợi thế trong hai khía cạnh. Thứ nhất, tổn thất gating kênh tuân thủ ngầm và thích ứng với trạng thái của backbone, điều này ổn định việc huấn luyện so với trường hợp với chính quy hóa thưa thớt hoặc huấn luyện dựa trên RL. Thứ hai, việc giảm FLOPs có thể được ước tính trước khi huấn luyện, vì mặt nạ mục tiêu được kiểm soát bởi mặt nạ ground truth được tạo ra, điều này đưa ra một ước tính về việc giảm. Điều này đơn giản hóa việc lựa chọn siêu tham số kiểm soát tỷ lệ cắt tỉa. Các đóng góp chính được tóm tắt như sau:

• Một công thức tổn thất mới với việc tạo mặt nạ ground truth tự giám sát thân thiện với gradient descent ngẫu nhiên (SGD) mà không có các thủ thuật trọng số gradient.

• Chúng tôi đề xuất một chữ ký động mới dựa trên khối lượng bản đồ nhiệt mà không có tỷ lệ cắt tỉa được định trước cho mỗi lớp.

• Lựa chọn siêu tham số đơn giản cho phép ước tính giảm FLOPs trước khi huấn luyện. Điều này đơn giản hóa việc thực hiện một mục tiêu ngân sách trước với không gian tìm kiếm siêu tham số có giới hạn.

2. Công trình liên quan
Cắt tỉa Tĩnh. Cắt tỉa tĩnh loại bỏ các trọng số trong giai đoạn huấn luyện ngoại tuyến và áp dụng cùng một mô hình nén cho tất cả các mẫu. Cắt tỉa không có cấu trúc [9, 11, 32, 40] nhắm mục tiêu loại bỏ các trọng số riêng lẻ với đóng góp tối thiểu. Hạn chế của việc cắt tỉa trọng số không có cấu trúc là cần phần cứng và thư viện chuyên dụng [40] để đạt được tăng tốc từ việc nén. Cắt tỉa có cấu trúc đang trở thành một giải pháp thực tế hơn trong đó các bộ lọc hoặc khối được xếp hạng và cắt tỉa dựa trên một tiêu chí [7,13,15,29,34,36]. Các phương pháp cắt tỉa bộ lọc sớm hơn [23,34] đòi hỏi tính toán

--- TRANG 3 ---
Hình 3. Đường ống được đề xuất để huấn luyện định tuyến động cho một lớp. Đối với một lớp l, đầu dự đoán fl
p(Il;Wpl) nhận một đầu vào Il, áp dụng global max pooling (GMP), chuẩn hóa với Softmax, sau đó đưa vào tích chập 1x1 để tạo ra logits Pl cho mặt nạ nhị phân Ml. Tổn thất Binary Cross Entropy (BCEWithLogits) phạt việc dự đoán mặt nạ dựa trên top-k thu được từ các bản đồ đặc trưng không bị cắt tỉa Ol.

phân tích độ nhạy theo lớp để tạo ra chữ ký mô hình (tức là số lượng bộ lọc cho mỗi lớp) trước khi cắt tỉa. Phân tích độ nhạy tốn kém về mặt tính toán, đặc biệt khi các mô hình trở nên sâu hơn. Các phương pháp gần đây [30, 36, 45] học một thước đo tầm quan trọng toàn cục. Molchanov et al. [36] đề xuất một xấp xỉ Taylor trên các trọng số của mạng trong đó gradient và chuẩn của bộ lọc được sử dụng để xấp xỉ điểm tầm quan trọng toàn cục của nó. Liu et al. [30] và Wen et al. [45] giới thiệu một tổn thất thưa thớt ngoài tổn thất nhiệm vụ như một chính quy hóa sau đó cắt tỉa các bộ lọc có tiêu chí ít hơn một ngưỡng.

Cắt tỉa Động. Trái ngược với việc triển khai một-mô-hình-phù-hợp-cho-tất-cả như trong cắt tỉa tĩnh, cắt tỉa động xử lý các tuyến đường khác nhau cho mỗi mẫu đầu vào. Tương tự như cắt tỉa tĩnh, các phương pháp có thể áp dụng độ chi tiết khác nhau để cắt tỉa. Channel gating network (CGNet) [18] là một phương pháp chi tiết mịn bỏ qua các vị trí không trong bản đồ đặc trưng. Một tập con của các kênh đầu vào được xử lý bởi mỗi kernel. Việc quyết định gating được học thông qua chính quy hóa với một hàm không khả vi xấp xỉ phức tạp. Họ áp dụng tích chập nhóm và thao tác xáo trộn để cân bằng tần số cập nhật bộ lọc từ các nhóm đặc trưng khác nhau. Gần nhất với công trình của chúng tôi, chúng tôi tập trung vào các phương pháp cắt tỉa bộ lọc động. Trong Runtime Neural Pruning (RNP) [27], một đơn vị quyết định được mô hình hóa như một lớp hồi quy toàn cục, tạo ra các hành động rời rạc tương ứng với bốn nhóm lựa chọn kênh được đặt trước. Việc lựa chọn nhóm được huấn luyện với học tăng cường. Tương tự, trong BlockDrop [47], một mạng chính sách được huấn luyện để bỏ qua các khối trong mạng dư thay vì chỉ các kênh. D2NN [28] định nghĩa một tập hợp biến thể của các nhánh có điều kiện trong DNN, và sử dụng Q-learning để huấn luyện các chính sách phân nhánh. Các phương pháp này huấn luyện các hàm chính sách của chúng bằng học tăng cường, có thể là một nhiệm vụ tối ưu hóa tốn kém không tầm thường cùng với backbone CNN. Feature Boosting and Suppression (FBS) [10] tạo ra tính nổi bật kênh liên tục và sử dụng tỷ lệ cắt tỉa được định trước cho mỗi lớp để có được một gating nhị phân rời rạc. LCS [44] đề xuất có được một hành động rời rạc từ N nhóm kênh đã học được lấy mẫu từ phân phối Gumbel. Họ áp dụng nhiệt độ ủ để ổn định việc huấn luyện và giới thiệu tính đa dạng trong các tuyến đường đã học bằng thủ thuật chuẩn hóa gradient. Các phương pháp dựa trên chính quy hóa hiện tại này đòi hỏi việc điều chỉnh cẩn thận bổ sung để ổn định việc huấn luyện, điều này được gây ra bởi gradient chính sách hoặc để thực thi việc học các tuyến đường đa dạng để không hội tụ thành một cắt tỉa tĩnh. Do đó, chúng tôi đề xuất xây dựng việc lựa chọn kênh như một phân loại nhị phân tự giám sát trong đó các tuyến đường có thể diễn giải có thể được nghiên cứu và đơn giản được huấn luyện với SGD thông thường.

3. Phương pháp luận
Trong phần này, đầu tiên chúng tôi giải thích cơ chế cho gating động. Sau đó, chúng tôi thảo luận về cách chúng tôi thiết kế các đầu quyết định và tổn thất có giám sát

--- TRANG 4 ---
3.1. Channel Gating
Gọi Il,Wl là các đặc trưng đầu vào và trọng số của một lớp tích chập l, tương ứng, trong đó Il∈Rcl−1×wl×hl, Wl∈Rcl×cl−1×kl×kl, và cl là số lượng bộ lọc trong lớp l. Một khối CNN điển hình bao gồm một phép toán tích chập (∗), chuẩn hóa batch (BN), và một hàm kích hoạt (f) như ReLU thường được sử dụng. Không mất tính tổng quát, chúng tôi bỏ qua thuật ngữ bias vì có BN, do đó, bản đồ đặc trưng đầu ra Ol có thể được viết là Ol=f(BN(Il∗Wl)). Chúng tôi dự đoán một mặt nạ nhị phân Ml∈Rcl biểu thị các bản đồ đặc trưng đầu ra được kích hoạt cao Ol từ bản đồ kích hoạt đầu vào Il bằng cách áp dụng một đầu quyết định fl
p với các tham số có thể học Wl
p. Đầu ra được che mặt Il+1 sau đó được biểu diễn là Il+1=Ol⊙Binarize(fl
p(Il;Wp)). Hàm Binarize(.) là round(Sigmoid(.)) để chuyển đổi logits thành một mặt nạ nhị phân. Việc dự đoán các bản đồ đặc trưng đầu ra được kích hoạt cao cho phép xử lý các bộ lọc f trong đó Ml
f=1 trong thời gian suy luận và bỏ qua phần còn lại. Đầu quyết định của chúng tôi có chi phí cl−1×cl FLOPs cho mỗi lớp l, điều này là không đáng kể.

3.2. Self-Supervised Binary Gating
Phương pháp được đề xuất của chúng tôi như được hiển thị trong Hình 3 học định tuyến động này theo cách tự giám sát bằng cách chèn một đầu dự đoán sau mỗi khối tích chập để dự đoán k bộ lọc được kích hoạt cao của lớp tiếp theo. Giá trị k được tính toán tự động cho mỗi đầu vào dựa trên khối lượng của bản đồ nhiệt.

Hàm tổn thất Mặt nạ nhị phân ground truth của các đặc trưng được kích hoạt cao có thể đạt được bằng cách sắp xếp chuẩn của các đặc trưng. Mục tiêu huấn luyện tổng thể là:
min
{W,Wp}Ltotal=Lent(fn(x;W),yk)
+Lpred({fl
p(Il;Wpl),gl}L)(1)
trong đó fn là backbone của mô hình baseline, Lent là tổn thất entropy chéo của nhiệm vụ, Lpred là tổng tổn thất dự đoán cho tất cả các lớp l∈1...L. Trong chi tiết, chúng tôi định nghĩa Lpred như sau:
Lpred({Pl,gl}L) =
LX
lFlX
fBCEWithLogits(Pl
f,gl
f)(2)
trong đó Pl là đầu ra của đầu quyết định fl
p(Il;Wpl), gl là mặt nạ ground truth được tạo ra dựa trên top-k đầu ra được kích hoạt cao Ol, BCEWithLogits là một Sigmoid theo sau bởi tổn thất entropy chéo nhị phân BCE(p, g) =−(glog(p) + (1−g) log(1−p)).

Số lượng k được kích hoạt Chúng tôi tự động tính toán k bằng cách giữ một tỷ lệ phần trăm r không đổi của khối lượng bản đồ nhiệt. Đối với mỗi kênh i=1, ..., cl, chúng tôi giữ phản ứng tối đa bằng cách áp dụng một global maximum pooling (GMP) (GMP(Ol
i). Đối với mỗi ví dụ đầu vào, k là số lượng bộ lọc được giữ sao cho khối lượng tích lũy của việc kích hoạt được chuẩn hóa đã sắp xếp đạt r%. Thuật toán tạo ground truth được hiển thị trong Thuật toán 1. Chúng tôi sử dụng cùng r cho tất cả các lớp, tuy nhiên, mỗi mẫu sẽ có tỷ lệ cắt tỉa khác nhau cho mỗi lớp dựa trên việc kích hoạt của nó. Vì groundtruth nhị phân mục tiêu được tạo ra từ các kích hoạt của các bộ lọc không bị che, việc giảm FLOPs có thể được ước tính lỏng lẻo trước khi huấn luyện để điều chỉnh r phù hợp. Lợi thế này thêm vào tính thực tế của phương pháp của chúng tôi, không dựa vào việc điều chỉnh siêu tham số gián tiếp để đạt được một mục tiêu ngân sách trong việc giảm FLOPs. Cũng đáng đề cập rằng r=1 là một trường hợp đặc biệt cho biết đầu quyết định sẽ dự đoán các đặc trưng hoàn toàn bị hủy kích hoạt (ví dụ phản ứng tối đa bằng không). Điều này cho phép duy trì độ chính xác của baseline với đầu quyết định được huấn luyện lý tưởng cho các backbone rất thưa thớt.

Thuật toán 1 Tạo ground truth mặt nạ nhị phân
Đầu vào: I1...IL,r
Đầu ra: g ground truth nhị phân với 0 là cần cắt tỉa
1: gt←ones(L, cl)
2: for l←1 to L do
3: acts←GMP((Ol))
4: normalized←acts/Pacts
5: sorted, idx←SORT(normalized, "descend")
6: cumulative←CUMSUM(sorted)
7: prune idx←WHERE(cumulative > r)
8: gt[l][prune idx]←0
9: end for

3.3. Thiết kế Đầu Dự đoán
Thiết kế đầu dự đoán nên được mô hình hóa theo cách đơn giản để giảm overhead so với mạng baseline. Trong quá trình forward pass, chúng tôi áp dụng GMP để giảm bản đồ đặc trưng Il cho mỗi lớp thành El∈Rcl−1×1×1. Tiếp theo, chúng tôi áp dụng tích chập 1x1 trên embedding được làm phẳng El để tạo ra logits của mặt nạ. Chúng tôi thử nghiệm với hai chế độ huấn luyện: 1) tách biệt, và 2) liên kết. Trong cả hai chế độ, chúng tôi huấn luyện các trọng số backbone và các đầu quyết định song song. Sự phân biệt là liệu chúng tôi có thực hiện huấn luyện hoàn toàn khả vi (liên kết) hay dừng gradient từ các đầu để lan truyền ngược về backbone và ngược lại (tách biệt). Trong huấn luyện liên kết, đầu quyết định hoàn toàn khả vi ngoại trừ phần nhị phân hóa. Tương tự như các công trình trước [8, 35, 50], chúng tôi sử dụng straight-through estimator (STE) để bỏ qua hàm không khả vi. Một vấn đề cần xem xét là sự can thiệp của tổn thất với nhiều tổn thất ở độ sâu khác nhau trong mạng như được chỉ ra trong [19]. Sự can thiệp tổn thất làm nổi bật rằng các bản đồ đặc trưng có thể bị thiên vị hướng tới

--- TRANG 5 ---
Mô hình Động? Top-1 Acc. (%) Giảm FLOPs (%)
VGG16-BNBaseline – 93.82 —
L1-norm [22] N 93.00 34
ThiNet [34] N 93.36 50
CP [15] N 93.18 50
Taylor-50 [36] N 92.00 51
RNP [27] Y 92.65 50
FBS [10] Y 93.03 50
LCS [44] Y 93.45 50
FTWT J(r= 0.92) Y 93.55 65
FTWT D(r= 0.92) Y 93.73 56
Taylor-59 [36] N 91.50 59
FTWT D(r= 0.85) Y 93.19 73
FTWT J(r= 0.88) Y 92.65 74
ResNet56Baseline – 93.66 –
Uniform from [44] N 74.39 50
ThiNet [34] N 91.98 50
SFP [13] N 92.56 48
LCS [44] Y 92.57 52
FTWT D(r= 0.80) Y 92.63 66
FTWT J(r= 0.88) Y 92.28 54
MobileNetV1Baseline – 90.89 –
MobileNet 75 [16] N 89.79 42
MobileNet 50 [16] N 87.58 73
FTWT D(r= 1.0) Y 91.06 78
FTWT J(r= 1.0) Y 91.21 78

Bảng 1. Kết quả trên CIFAR-10. Giảm FLOPs chỉ ra việc giảm FLOPs theo phần trăm. r trong phương pháp của chúng tôi biểu thị tỷ lệ siêu tham số trong Thuật toán 1. x trong FTWT x chỉ ra huấn luyện liên kết (J) hoặc tách biệt (D).

đạt độ chính xác cao cho nhiệm vụ địa phương hơn là kiến trúc tổng thể. Không giống như các phương pháp khác dựa vào việc điều chỉnh huấn luyện cẩn thận để quản lý gradient từ các tổn thất khác nhau, chúng tôi huấn luyện các đầu cùng với backbone song song nhưng cộng tác vì các mặt nạ được tạo ra từ trạng thái hiện tại của mô hình. Các mặt nạ nhị phân groundtruth được điều chỉnh một cách rõ ràng bởi các trọng số backbone được cập nhật, do đó, tuân thủ ngầm tốc độ học của backbone.

4. Thí nghiệm và Phân tích
Chúng tôi đánh giá phương pháp của mình trên các bộ dữ liệu CIFAR [20] và ImageNet [5] trên nhiều kiến trúc khác nhau như VGG [41], ResNet [12] và MobileNet [16]. Trong tất cả các kiến trúc, các mặt nạ ground truth được tạo ra sau mỗi khối conv-BN-ReLU. Đối với các mô hình baseline CIFAR, chúng tôi huấn luyện trong 200 epochs sử dụng batch-size là 128 với tối ưu hóa SGD. Tốc độ học ban đầu (lr) 0.1 được chia cho 10 tại các epochs 80, 120, và 150. Chúng tôi sử dụng momentum là 0.9 với weight decay là 5−4. Đối với ImageNet, chúng tôi sử dụng các mô hình được huấn luyện trước trong PyTorch [39] làm baseline. Các trọng số của đầu quyết định được huấn luyện với 0.1 làm lr ban đầu và cùng lịch trình lr như backbone. Chúng tôi sử dụng máy 4 V100-GPU trong các thí nghiệm của mình.

4.1. Thí nghiệm trên CIFAR
Chúng tôi tuân theo các cài đặt huấn luyện tương tự được sử dụng trong baseline cho huấn luyện động, nhưng chúng tôi huấn luyện tất cả các mô hình với tốc độ học ban đầu là 1e−2. Chúng tôi báo cáo độ chính xác trung bình trên ba thí nghiệm lặp lại và việc giảm FLOPs trên CIFAR-10 trên nhiều kiến trúc trong Bảng 1. Phương pháp của chúng tôi (FTWT) đạt được việc giảm FLOPs cao hơn trên độ chính xác top1 tương tự so với các phương pháp cắt tỉa tĩnh và động. Chúng tôi đạt được lên đến 66% giảm FLOPs trên VGG-16 và ResNet-56, cao hơn các phương pháp cắt tỉa bộ lọc động RNP [27], FBS [10], LCS [44] lên đến 15%. Huấn luyện liên kết hoạt động tốt như huấn luyện tách biệt trên các ngưỡng r cao. Tuy nhiên, độ chính xác giảm so với huấn luyện tách biệt trên các ngưỡng thấp hơn. Điều đó là do xung đột tăng giữa các tổn thất như có thể thấy trên việc giảm FLOPs ≈73% trên VGG. Chúng tôi tiếp tục đạt được 73% giảm FLOPs trên VGG với chỉ 0.63% sụt giảm độ chính xác. Hơn nữa, FTWT vượt trội hơn các biến thể nhỏ hơn của MobileNet về độ chính xác 3.42% với việc giảm FLOPs cao hơn.

Chúng tôi hiển thị số lượng tổ hợp bộ lọc duy nhất (cụm) được kích hoạt trên toàn bộ tập dữ liệu D và tỷ lệ cắt tỉa cho mỗi lớp trong Hình 4. Có nghĩa là, mỗi

--- TRANG 6 ---
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
ID Lớp0246810Số cụm (log)0123456789012N cụm(a) Số nhóm bộ lọc duy nhất (cụm) cho mỗi lớp.
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
ID Lớp0.00.20.40.60.8Tỷ lệ Cắt tỉa (b) Tỷ lệ cắt tỉa cho mỗi lớp.

Hình 4. Phân phối MobileNetV1 CIFAR10

Phương pháp Động? Top-1 Acc. (%) Giảm FLOPs (%)
Baseline Đã cắt tỉa Delta
ResNet34Taylor [36] N 73.31 72.83 0.48 22.25
LCCL [6] Y 73.42 72.99 0.43 24.80
FTWT ( r= 0.97) Y 73.30 73.25 0.05 25.86
FTWT ( r= 0.95) Y 73.30 72.79 0.51 37.77
SFP [13] N 73.92 71.83 2.09 41.10
FPGM [14] N 73.92 72.54 1.38 41.10
FTWT ( r= 0.93) Y 73.30 72.17 1.13 47.42
ResNet18 [12] N 73.30 69.76 3.54 50.04
FTWT ( r= 0.92) Y 73.30 71.71 1.59 52.24
ResNet18PFP-B [26] N 69.74 65.65 4.09 43.12
SFP [13] N 70.28 67.10 3.18 41.80
LCCL [6] Y 69.98 66.33 3.65 34.60
FBS [10] Y 70.70 68.20 2.50 49.49
FTWT ( r= 0.91) Y 69.76 67.49 2.27 51.56
MobileNetV1MobileNetV1-75 [16] N 69.76 67.00 2.76 42.85
FTWT ( r= 1) Y 69.57 69.66 -0.09 41.07

Bảng 2. Kết quả trên ImageNet. Độ chính xác baseline cho mỗi phương pháp được báo cáo cùng với độ chính xác của mô hình đã cắt tỉa và sự thay đổi độ chính xác từ baseline. Giảm FLOPs đại diện cho việc giảm FLOPs theo phần trăm. Delta âm cho biết sự tăng độ chính xác từ baseline. r trong phương pháp của chúng tôi biểu thị tỷ lệ siêu tham số trong Thuật toán 1

mẫu i tạo ra một mặt nạ nhị phân mi,j cho mỗi lớp j, các cụm duy nhất cho mỗi lớp là set(m0,j, ..., mi,j, ...m|D|,j). Trong LCS và RNP, một số lượng cụm cố định được đặt trước như một siêu tham số cho tất cả các lớp, chúng tôi hiển thị trong Hình 4a rằng các lớp khác nhau về số lượng cụm đa dạng. Phương pháp của chúng tôi điều chỉnh số lượng cụm khác nhau cho mỗi lớp tự động do cơ chế tạo mặt nạ tự giám sát. Để dễ hình dung, trục y được hiển thị trong thang log. Các lớp sớm có tính đa dạng nhỏ trong nhóm bộ lọc được kích hoạt, do đó, hoạt động tương tự như cắt tỉa tĩnh. Điều này hợp lý vì các lớp sớm phát hiện các đặc trưng cấp thấp và ít phụ thuộc vào đầu vào. Mặt khác, số lượng cụm tăng khi chúng ta đi sâu hơn vào mạng. Đáng chú ý rằng những cụm khác nhau này là chi tiết mịn, có nghĩa là các cụm có thể khác nhau chỉ trong một bộ lọc. Chúng tôi cũng tính toán tỷ lệ phần trăm của các bộ lọc cốt lõi được chia sẻ giữa tất cả các cụm cho mỗi lớp. Chúng tôi thấy rằng phạm vi của tỷ lệ phần trăm bộ lọc cốt lõi so với tổng số bộ lọc dao động từ 0.4 đến 1.0. Điều này cung cấp cái nhìn sâu sắc về lý do tại sao các phương pháp cắt tỉa tĩnh dẫn đến sụt giảm lớn về độ chính xác với việc cắt tỉa lớn. Vì tỷ lệ cắt tỉa có thể đạt được bị giới hạn bởi số lượng bộ lọc cốt lõi và việc cắt tỉa thêm sẽ hạn chế khả năng của mô hình. Một câu hỏi nghiên cứu thú vị trong tương lai sẽ là liệu chúng ta có thể xác định khả năng nén của một mô hình dựa trên khái niệm tỷ lệ bộ lọc cốt lõi. Cuối cùng, Hình 4b hiển thị tỷ lệ cắt tỉa cho mỗi lớp, như mong đợi, các lớp sau được cắt tỉa nhiều hơn các lớp sớm vì các lớp trở nên rộng hơn và có thể nén được hơn. Chúng tôi nhận thấy việc cắt tỉa nặng đạt 85% trong các lớp giữa với một chuỗi các lớp có 512 bộ lọc.

4.2. Thí nghiệm trên ImageNet
Đối với ImageNet, chúng tôi huấn luyện trong 90 epochs với tốc độ học ban đầu là 10−2 giảm mỗi 30 epochs 0.1. Thí nghiệm trên ImageNet được thực hiện với chế độ huấn luyện tách biệt. Bảng 3 hiển thị sụt giảm độ chính xác từ baseline cho mỗi phương pháp để tính đến sự khác biệt huấn luyện do augmentation. Kết quả cho thấy phương pháp của chúng tôi đạt được sụt giảm độ chính xác nhỏ hơn với việc giảm FLOPs cao hơn so với các phương pháp SOTA khác. Chúng tôi đạt được giảm độ chính xác tương tự như LCCL với 13% giảm FLOPs cao hơn trên ResNet34. Mặt khác, trên việc giảm FLOPs tương tự (≈25), chúng tôi có sụt giảm tối thiểu về độ chính xác (≈0.05%). Mặc dù chúng tôi đạt được độ chính xác tương tự trên ResNet18 với FBS, phương pháp sau đòi hỏi một số lượng bộ lọc được định trước để giữ cho mỗi lớp. Mặt khác, phương pháp của chúng tôi gán động tỷ lệ cắt tỉa cho mỗi lớp, điều này cho thấy hiệu quả của khối lượng bản đồ nhiệt như một tiêu chí. Chúng tôi cũng so sánh với các biến thể nhỏ hơn của kiến trúc như ResNet18 và MobileNet-75. Chúng tôi vượt trội hơn ResNet18 và MobileNet-75 ≈2% về độ chính xác trên ngân sách tính toán tương tự.

4.3. Nghiên cứu Ablation
4.3.1 Sự Không chắc chắn dưới Dataset Shift
Trong phần này, chúng tôi đo lường độ nhạy của định tuyến của chúng tôi đối với dataset shift. Các chỉ số dưới dataset shift hiếm khi được kiểm tra trong tài liệu cắt tỉa mô hình. Chúng tôi tin vào tầm quan trọng của nó vì độ phức tạp suy luận tăng và do đó muốn khởi xướng báo cáo những so sánh như vậy. Chúng tôi tiến hành thí nghiệm cho VGG16-bn CIFAR-10 với tỷ lệ cắt tỉa cao 73% cho tất cả các mô hình đã cắt tỉa. Lấy cảm hứng từ [37], chúng tôi báo cáo điểm Brier [3] dưới các loại nhiễu khác nhau như làm mờ Gaussian Bảng 3a và nhiễu cộng Bảng 3b với mô hình dense baseline làm tham chiếu. Như có thể thấy, phương pháp của chúng tôi có khả năng phục hồi hơn so với cắt tỉa Taylor tĩnh với điểm brier thấp hơn. Chúng tôi cũng so sánh với cắt tỉa đồng đều tĩnh, chúng tôi đạt được điểm Brier tương tự (đôi khi thấp hơn một chút). Điều này cho thấy khả năng phục hồi của mô hình của chúng tôi đối với data shift ngay cả khi so sánh với quyết định cắt tỉa tĩnh không phụ thuộc vào dữ liệu. Cuối cùng, như mong đợi, mô hình dense có khả năng phục hồi nhất đối với nhiễu. Tuy nhiên, phương pháp của chúng tôi vẫn cho thấy chất lượng khá tốt nhìn chung. Chúng tôi quy cho sự ổn định phân phối này cho softmax trong đầu. Softmax hoạt động như một bộ chuẩn hóa giảm độ nhạy đối với dataset shift. Chúng tôi so sánh phương pháp của mình có và không có chuẩn hóa softmax trong đầu quyết định để xác minh giả thuyết này. Bảng 3c hiển thị điểm Brier với nhiễu cộng và làm mờ cho so sánh này. Như có thể thấy, thực sự, bộ chuẩn hóa ổn định đầu ra mặt nạ quyết định đặc biệt trong trường hợp làm mờ.

4.3.2 Chữ ký Động và Định tuyến Động
Chúng tôi nghiên cứu việc tách biệt hiệu ứng từ chữ ký động (tức là tỷ lệ cắt tỉa cho mỗi lớp) cho mỗi mẫu từ định tuyến động (tức là nhóm bộ lọc được kích hoạt). Chúng tôi khám phá hiệu quả của định tuyến động với một chữ ký được định trước cho tất cả đầu vào. Trong những thí nghiệm này, chữ ký được định trước sử dụng tiêu chí Taylor được đề xuất trong [36] như một nghiên cứu trường hợp. Như trong thiết lập trước, chúng tôi chọn k đặc trưng được kích hoạt cao trong đó k được định nghĩa bởi chữ ký trong khi các mẫu khác nhau trong việc chọn k bộ lọc nào. Bảng 6 hiển thị kết quả của định tuyến động dưới các tỷ lệ cắt tỉa khác nhau. Như có thể thấy, định tuyến động hoạt động tốt hơn suy luận tĩnh đặc biệt trên tỷ lệ cắt tỉa cao lên đến 4%. Thiết lập huấn luyện giống như cho CIFAR được giải thích trong bài báo, tuy nhiên, chúng tôi huấn luyện các mô hình ImageNet trong 30 epochs sử dụng thiết lập finetune thay vì thiết lập huấn luyện với 90 epochs như được phân biệt trong [31] để tăng tốc thí nghiệm.

4.3.3 Lựa chọn Siêu tham số r
Siêu tham số r (tức là tỷ lệ khối lượng) được chọn dựa trên một đánh giá đơn giản trước khi huấn luyện. Chúng tôi ước tính FLOPs trước khi huấn luyện bằng cách áp dụng các mặt nạ groundtruth trên mô hình dense được huấn luyện trước đóng băng trên tập huấn luyện (một lần qua). Tiếp theo, chúng tôi có một ước tính trước khi huấn luyện được khởi xướng về việc giảm FLOPs mong đợi dưới các giá trị r khác nhau. Điều này đơn giản hóa việc lựa chọn siêu tham số để đạt được một mục tiêu giảm FLOPs. Mặt khác, siêu tham số chính quy hóa thưa thớt thường được tinh chỉnh với một quá trình cross-validation và đòi hỏi một thử và sai của nhiều huấn luyện đầy đủ để đạt được một ngân sách mục tiêu. Không có mối quan hệ trực tiếp giữa trọng số chính quy hóa và việc giảm FLOPs cuối cùng đạt được một cách biết trước khi huấn luyện. Phương pháp của chúng tôi đơn giản hóa việc lựa chọn và làm cho nó trở thành một lựa chọn thực tế hơn khi một ngân sách mục tiêu được đưa ra trước. Bảng 4 hiển thị FLOPs ước tính trước khi huấn luyện sử dụng các ngưỡng khác nhau, r, và việc giảm FLOPs thực tế đạt được sau khi huấn luyện. Sự khác biệt trong việc giảm là do sự không chính xác của các đầu quyết định. Tuy nhiên, FLOPs ước tính đưa ra một xấp xỉ tốt cho FLOPs cuối cùng đạt được và do đó giảm tìm kiếm siêu tham số.

4.4. Tăng tốc Lý thuyết vs Thực tế
Đối với tất cả các phương pháp nén, bao gồm cắt tỉa tĩnh và động, thường có một khoảng cách rộng giữa việc giảm FLOPs và tăng tốc thực tế do các yếu tố khác như độ trễ I/O và thư viện BLAS. Tăng tốc phụ thuộc vào phần cứng và backend như được hiển thị trong các công trình trước [1, 7, 49]. Chúng tôi kiểm tra tốc độ thực tế trên PyTorch [39] sử dụng backend MKL trên AMD CPU sử dụng một luồng đơn như được hiển thị trong Bảng 5.

Hạn chế. Tăng tốc thực tế của chúng tôi ít hơn việc giảm FLOPs và điều đó được quy cho hai yếu tố: 1) Overhead truyền dữ liệu từ việc cắt ma trận trọng số dense dựa

--- TRANG 7 ---
σMô hình
DenseFTWT
(của chúng tôi)Taylor
PruningUniform
Pruning
0.5 0.11 0.12 0.20 0.12
0.7 0.16 0.18 0.39 0.19
0.9 0.38 0.39 0.57 0.42
1.09 0.69 0.58 0.66 0.61
1.27 0.74 0.68 0.69 0.71
1.45 0.76 0.73 0.74 0.75
(a) Nhiễu làm mờ Gaussian.σMô hình
DenseFTWT
(của chúng tôi)Taylor
PruningUniform
Pruning
0.00 0.11 0.12 0.16 0.12
0.02 0.11 0.12 0.16 0.12
0.05 0.11 0.12 0.17 0.13
0.11 0.13 0.14 0.20 0.15
0.14 0.19 0.21 0.30 0.22
0.20 0.39 0.43 0.51 0.42
(b) Nhiễu Gaussian cộng.

Làm mờ Gaussian Nhiễu Cộng
FTWT
với chuẩn hóaFTWT
không chuẩn hóaFTWT
với chuẩn hóaFTWT
không chuẩn hóa
0.12 0.19 0.12 0.12
0.18 0.48 0.12 0.13
0.39 0.73 0.14 0.15
0.58 0.85 0.21 0.24
0.68 0.90 0.43 0.47
(c) Phương pháp của chúng tôi có và không có chuẩn hóa softmax trong các đầu quyết định.

Bảng 3. Thí nghiệm dataset shift: Các số đại diện cho điểm Brier trên CIFAR-10 VGG16

Mô hìnhEst.
FLOPs (%)Final
FLOPs (%)
MobileNet (1.0) 42.3 41.07
Resnet34 (0.97) 23.32 25.86
Resnet34 (0.95) 31.77 37.77

Bảng 4. FLOPs ước tính trước khi huấn luyện dưới các ngưỡng khác nhau (được chỉ ra trong ngoặc đơn) vs FLOPs đạt được sau khi huấn luyện.

Mô hìnhGiảm FLOPs
(%)Giảm Độ trễ
(%)
ResNet3452.18 27.17
37.77 19.78
25.86 11.08

Bảng 5. Tăng tốc thực tế vs lý thuyết trên ImageNet trên AMD Ryzen Threadripper 2970WX CPU với batch size là 1.

trên dự đoán mặt nạ, có thể được giảm thiểu bởi các backend với suy luận thưa thớt tại chỗ hiệu quả. 2) Tăng tốc phụ thuộc vào chữ ký của mô hình và thông số kỹ thuật của phần cứng. Cắt tỉa từ các lớp sau xử lý độ phân giải đầu vào nhỏ hơn có thể không đạt được tăng tốc nhiều như cắt tỉa từ các lớp sớm. Tối ưu hóa nhận thức về ràng buộc sử dụng Alternating Direction Method of Multipliers (ADMM) [2] như được đề xuất trong [48] có thể được tích hợp thêm với phương pháp của chúng tôi để tối ưu hóa theo độ trễ thay vì FLOPs.

Tập dữ liệu Mô hìnhFLOPs
(%)Top-1 acc. (%)
Tĩnh Động
CIFAR-10VGG16-BN50 92.00 93.80
85 91.12 92.75
ResNet56 70 91.61 92.09
CIFAR-100 VGG16-BN30 72.65 73.67
65 68.17 72.18
93 58.74 60.05
ImageNet ResNet18 45 64.89 65.11

Bảng 6. So sánh độ chính xác của định tuyến động với một chữ ký được định trước và đối tác của nó với suy luận tĩnh.

5. Kết luận
Trong bài báo này, chúng tôi đề xuất một công thức mới cho cắt tỉa mô hình động. Tương tự như các phương pháp cắt tỉa động khác, chúng tôi trang bị một đầu quyết định rẻ cho lớp tích chập gốc. Tuy nhiên, chúng tôi đề xuất huấn luyện các đầu quyết định trong một mô hình tự giám sát. Đầu này dự đoán các bộ lọc có khả năng cao nhất được kích hoạt mạnh mẽ với việc kích hoạt đầu vào của lớp. Các mặt nạ được huấn luyện sử dụng tổn thất entropy chéo nhị phân tách biệt khỏi tổn thất nhiệm vụ để loại bỏ sự can thiệp tổn thất. Chúng tôi tạo ra ground truth mặt nạ dựa trên một tiêu chí mới sử dụng khối lượng bản đồ nhiệt cho mỗi mẫu đầu vào. Trong các thí nghiệm của chúng tôi, chúng tôi đã cho thấy kết quả trên các kiến trúc khác nhau trên các bộ dữ liệu CIFAR và ImageNet, và phương pháp của chúng tôi vượt trội hơn các phương pháp cắt tỉa động và tĩnh khác dưới việc giảm FLOPs tương tự.

--- TRANG 8 ---
Lời cảm ơn
Chúng tôi cảm ơn các reviewers vì phản hồi quý báu của họ. Chúng tôi cũng muốn cảm ơn Compute Canada vì các siêu máy tính của họ để tiến hành các thí nghiệm của chúng tôi.

Tài liệu tham khảo
[1] Simone Bianco, Remi Cadene, Luigi Celona, và Paolo Napoletano. Benchmark analysis of representative deep neural network architectures. IEEE Access, 6:64270–64277, 2018. 7
[2] Stephen Boyd, Neal Parikh, và Eric Chu. Distributed optimization and statistical learning via the alternating direction method of multipliers. Now Publishers Inc, 2011. 8
[3] Glenn W Brier et al. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):1–3, 1950. 7
[4] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, và Song Han. Once-for-all: Train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791, 2019. 1
[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 5
[6] Xuanyi Dong, Junshi Huang, Yi Yang, và Shuicheng Yan. More is less: A more complicated network with less inference complexity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5840–5848, 2017. 1, 6
[7] Sara Elkerdawy, Mostafa Elhoushi, Abhineet Singh, Hong Zhang, và Nilanjan Ray. To filter prune, or to layer prune, that is the question. In Proceedings of the Asian Conference on Computer Vision, 2020. 2, 7
[8] Sara Elkerdawy, Hong Zhang, và Nilanjan Ray. Lightweight monocular depth estimation model by joint end-to-end filter pruning. In 2019 IEEE International Conference on Image Processing (ICIP), pages 4290–4294. IEEE, 2019. 4
[9] Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. 1, 2
[10] Xitong Gao, Yiren Zhao, Łukasz Dudziak, Robert Mullins, và Cheng-zhong Xu. Dynamic channel pruning: Feature boosting and suppression. arXiv preprint arXiv:1810.05331, 2018. 1, 3, 5, 6
[11] Song Han, Jeff Pool, John Tran, và William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems (NIPS), pages 1135–1143, 2015. 2
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5, 6
[13] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, và Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018. 2, 5, 6
[14] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, và Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4340–4349, 2019. 6
[15] Yihui He, Xiangyu Zhang, và Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 1389–1397, 2017. 2, 5
[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, và Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 1, 5, 6
[17] Jie Hu, Li Shen, và Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132–7141, 2018. 1
[18] Weizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang, và G Edward Suh. Channel gating neural networks. arXiv preprint arXiv:1805.12549, 2018. 1, 3
[19] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, và Kilian Q Weinberger. Multi-scale dense networks for resource efficient image classification. arXiv preprint arXiv:1703.09844, 2017. 4
[20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 5
[21] Bailin Li, Bowen Wu, Jiang Su, và Guangrun Wang. Eagleeye: Fast sub-net evaluation for efficient neural network pruning. In European conference on computer vision, pages 639–654. Springer, 2020. 11
[22] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016. 1, 5
[23] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for efficient convnets. ICLR, 2017. 2
[24] Yuhang Li, Xin Dong, và Wei Wang. Additive powers-of-two quantization: An efficient non-uniform discretization for neural networks. arXiv preprint arXiv:1909.13144, 2019. 1
[25] Lucas Liebenwein, Cenk Baykal, Brandon Carter, David Gifford, và Daniela Rus. Lost in pruning: The effects of pruning neural networks beyond test accuracy. arXiv preprint arXiv:2103.03014, 2021. 12
[26] Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, và Daniela Rus. Provable filter pruning for efficient neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. 6
[27] Ji Lin, Yongming Rao, Jiwen Lu, và Jie Zhou. Runtime neural pruning. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 2178–2188, 2017. 1, 3, 5
[28] Lanlan Liu và Jia Deng. Dynamic deep neural networks: Optimizing accuracy-efficiency trade-offs by selective execution. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. 3

--- TRANG 9 ---
[29] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE International Conference on Computer Vision, pages 2736–2744, 2017. 2
[30] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE ICCV, pages 2736–2744, 2017. 3
[31] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, và Trevor Darrell. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. 1, 7
[32] Christos Louizos, Max Welling, và Diederik P Kingma. Learning sparse neural networks through l0 regularization. arXiv preprint arXiv:1712.01312, 2017. 2
[33] Siegrid Lowel và Wolf Singer. Selection of intrinsic horizontal connections in the visual cortex by correlated neuronal activity. Science, 255(5041):209–212, 1992. 2
[34] Jian-Hao Luo, Jianxin Wu, và Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In Proceedings of the IEEE international conference on computer vision, pages 5058–5066, 2017. 2, 5
[35] Arun Mallya, Dillon Davis, và Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV), pages 67–82, 2018. 4
[36] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, và Jan Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11264–11272, 2019. 1, 2, 3, 5, 6, 7
[37] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V Dillon, Balaji Lakshminarayanan, và Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019. 7
[38] Michela Paganini. Prune responsibly. arXiv preprint arXiv:2009.09936, 2020. 12
[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019. 5, 7
[40] Sayeh Sharify, Alberto Delmas Lascorz, Mostafa Mahmoud, Milos Nikolic, Kevin Siu, Dylan Malone Stuart, Zissis Poulos, và Andreas Moshovos. Laconic deep learning inference acceleration. In 2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA), pages 304–317. IEEE, 2019. 2
[41] Karen Simonyan và Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio và Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. 5
[42] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, và Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820–2828, 2019. 1
[43] Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, và Chang Xu. Scop: Scientific control for reliable neural network pruning. Advances in Neural Information Processing Systems, 33:10936–10947, 2020. 11
[44] Yulong Wang, Xiaolu Zhang, Xiaolin Hu, Bo Zhang, và Hang Su. Dynamic network pruning with interpretable layer-wise channel selection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 6299–6306, 2020. 1, 3, 5
[45] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, và Hai Li. Learning structured sparsity in deep neural networks. In Advances in neural information processing systems, pages 2074–2082, 2016. 3
[46] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992. 1
[47] Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S Davis, Kristen Grauman, và Rogerio Feris. Blockdrop: Dynamic inference paths in residual networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8817–8826, 2018. 3
[48] Haichuan Yang, Yuhao Zhu, và Ji Liu. Ecc: Platform-independent energy-constrained deep neural network compression via a bilinear regression model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11206–11215, 2019. 8
[49] Tien-Ju Yang, Yu-Hsin Chen, và Vivienne Sze. Designing energy-efficient convolutional neural networks using energy-aware pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5687–5695, 2017. 7
[50] Xucheng Ye, Pengcheng Dai, Junyu Luo, Xin Guo, Yingjie Qi, Jianlei Yang, và Yiran Chen. Accelerating cnn training by pruning activation gradients. In European Conference on Computer Vision, pages 322–338. Springer, 2020. 4
[51] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, và Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pages 365–382, 2018. 1
[52] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, và Jinhui Zhu. Discrimination-aware channel pruning for deep neural networks. Advances in neural information processing systems, 31, 2018. 11

--- TRANG 10 ---
Phụ lục
1. Thông tin Tính toán
Bảng 1 hiển thị giờ tính toán cho việc huấn luyện các mô hình được báo cáo trong bài báo của chúng tôi sử dụng máy 4 V100 GPU.

Tập dữ liệu Mô hình Giờ tính toán
CIFAR-10VGG 2.0
ResNet56 8.0
MobileNet 3.5
ImageNetResNet18 40.0
ResNet34 55.5
MobileNetv1 41.0

Bảng 1. Giờ tính toán trên máy 4 V100 GPU

2. CIFAR
2.1. MobileNet
Chúng tôi so sánh phương pháp của mình trên MobileNetv1/v2 dưới tỷ lệ cắt tỉa khác nhau với các phương pháp cắt tỉa khác như EagleEye [21], SCOP [43] và DCP [52]. Lưu ý rằng EagleEye báo cáo tốt nhất trong hai mô hình ứng viên khác nhau về chữ ký, do đó gấp đôi thời gian huấn luyện. Chúng tôi vượt trội hơn SOTA 37% giảm FLOPs cao hơn trên độ chính xác tương tự như được hiển thị trong 1.

30
40
50
60
70
80
90
Giảm FLOPs (%)(cao hơn là tốt hơn)
0
1
2
3
4Khoảng cách độ chính xác (%)(thấp hơn là tốt hơn)
Mbnetv1-FTWT (của chúng tôi)
Mbnetv1-EagleEye
Mbnetv1-50
Mbnetv1-25
Mbnetv2-FTWT (của chúng tôi)
Mbnetv2-SCOP
Mbnetv2-DCP
Mbnetv2-25
Mbnetv2-50

Hình 1. MobileNetV1/V2 trên CIFAR10.

2.2. Hình ảnh Bộ lọc Cốt lõi
Chúng tôi hiển thị hình ảnh số lượng bộ lọc cốt lõi cho mỗi lớp như được giải thích trong bài báo chính. Bộ lọc cốt lõi cho mỗi lớp chỉ ra các bộ lọc được kích hoạt trong tất cả các tuyến đường khác nhau. Tính đa dạng cao nhất ở các lớp giữa, điều này giải thích sự sụt giảm độ chính xác trong cắt tỉa đồng đều tĩnh (Bảng 1 bài báo chính). MobileNet 50 tĩnh dẫn đến 3.31% sụt giảm độ chính xác so với phương pháp động của chúng tôi cải thiện baseline với tăng nhẹ độ chính xác 0.17%.

Hình 2. Số lượng bộ lọc cốt lõi cho mỗi lớp trong MobileNet.

2.3. Thanh Lỗi
Bảng 2 hiển thị chi tiết số của các thí nghiệm CIFAR với trung bình và độ lệch chuẩn trên 3 lần chạy.

2.4. Hình ảnh Bản đồ Nhiệt
Chúng tôi hình ảnh bản đồ nhiệt của một lớp được cắt tỉa nhiều so với mô hình baseline. Hình 3 hiển thị so sánh giữa bản đồ nhiệt từ baseline với tất cả bộ lọc được kích hoạt và bản đồ nhiệt của các bộ lọc được chọn động. Như có thể thấy, cắt tỉa động xấp xỉ baseline với sự chú ý cao vào các đối tượng tiền cảnh. Điều này cho thấy rằng ngay cả với tỷ lệ cắt tỉa 70% trong lớp đó, chúng tôi có thể xấp xỉ hành vi của mô hình gốc.

3. ImageNet
3.1. Liên kết vs Tách biệt
Vì việc huấn luyện các mô hình ImageNet tốn kém, chúng tôi hiển thị một vài thí nghiệm để so sánh kết quả với các chế độ huấn luyện liên kết và tách biệt trong Bảng 3. Tương tự như kết quả trên CIFAR, huấn luyện tách biệt vượt trội hơn huấn luyện liên kết dưới việc giảm FLOPs tương tự.

Tác động Rộng lớn hơn
Cắt tỉa Mạng Nơ-ron có tiềm năng tăng hiệu quả triển khai về mặt năng lượng và thời gian phản hồi. Tuy nhiên, việc có được những mô hình đã cắt tỉa này vẫn chưa được tối ưu hóa cho một tiêu thụ tính toán tổng thể tốt hơn và thân thiện với môi trường hơn. Hơn nữa, cắt tỉa đòi hỏi hiểu biết cẩn thận về các tình huống triển khai như

--- TRANG 11 ---
Hình 3. Hình ảnh bản đồ nhiệt của các mẫu đầu vào ngẫu nhiên từ CIFAR cho lớp thứ 10 trong MobileNetV1. Mỗi bộ ba đại diện cho hình ảnh đầu vào, bản đồ nhiệt baseline, bản đồ nhiệt đã cắt tỉa. Giảm FLOPs trong lớp là ≈70%, nhưng bản đồ nhiệt đã cắt tỉa xấp xỉ cao bản đồ nhiệt với các bộ lọc được kích hoạt đầy đủ.

Chạy VGG16-BN VGG16-BN ResNet56 MobileNet
Acc. (%) FLOPs (%) Acc. (%) FLOPs (%) Acc. (%) FLOPs (%) Acc. (%) FLOPs (%)
93.26 73.10 93.66 65.41 92.61 66.45 91.01 78.03
93.21 73.00 93.49 64.37 92.63 66.43 91.16 79.52
93.11 73.47 93.51 65.27 92.65 66.41 91.03 78.04
Trung bình 93.19 73.19 93.55 65.01 92.63 66.43 91.06 78.53
Std 0.07 0.24 0.09 0.56 0.02 0.02 0.08 0.8

Bảng 2. Trung bình và độ lệch chuẩn qua các lần chạy khác nhau trên CIFAR.

Mô hình Liên kết Tách biệt Giảm FLOPs (%)
Resnet3470.06 71.71 37
71.52 72.79 52

Bảng 3. Huấn luyện liên kết vs tách biệt trên ResNet34 ImageNet

việc đặt câu hỏi về khái quát hóa ngoài phân phối [25] hoặc thay đổi hành vi của mạng theo cách không công bằng [38]. Chúng tôi đã hiển thị kết quả về dịch chuyển ngoài phân phối để giải quyết phần đầu tiên. Chúng tôi không điều tra tính công bằng của dự đoán của mô hình vì cả hai tập dữ liệu (tức là CIFAR và ImageNet) đều cân bằng. Mặc dù, chúng tôi cắt tỉa dựa trên khối lượng của bản đồ nhiệt như nhau cho tất cả các mẫu. Chúng tôi giả định đặc điểm này có thể mang lại lợi thế cho phương pháp của chúng tôi so với tỷ lệ cắt tỉa cố định có thể làm tổn thương một số mẫu đầu vào hơn một số mẫu khác.
