# 2304.13718.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2304.13718.pdf
# File size: 1706921 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Sparsiﬁed Model Zoo Twins:
Investigating Populations of Sparsiﬁed Neural Network Models
Dominik Honegger1Konstantin Sch ¨urholt1Damian Borth1
Abstract
With growing size of Neural Networks (NNs),
model sparsiﬁcation to reduce the computational
cost and memory demand for model inference
has become of vital interest for both research and
production. While many sparsiﬁcation methods
have been proposed and successfully applied on
individual models, to the best of our knowledge
their behavior and robustness has not yet been
studied on large populations of models. With
this paper, we address that gap by applying two
popular sparsiﬁcation methods on populations of
models (so called model zoos) to create sparsi-
ﬁed versions of the original zoos. We investigate
the performance of these two methods for each
zoo, compare sparsiﬁcation layer-wise, and anal-
yse agreement between original and sparsiﬁed
populations. We ﬁnd both methods to be very
robust with magnitude pruning able outperform
variational dropout with the exception of high
sparsiﬁcation ratios above 80%. Further, we ﬁnd
sparsiﬁed models agree to a high degree with their
original non-sparsiﬁed counterpart, and that the
performance of original and sparsiﬁed model is
highly correlated. Finally, all models of the model
zoos and their sparsiﬁed model twins are publicly
available: modelzoos.cc .
1. Introduction
In recent years, deep neural networks have gained signif-
icant momentum and popularity with the general trend
of growing in size. This is mainly due to the observed
relationship between model size and performance i.e. larger
models tend to have an improved performance over their
smaller counterparts as reported by (Kaplan et al., 2020;
Tan & Le, 2019; Brock et al., 2018). Unfortunately, the
increasing performance results in very high computational
and environmental costs for training and inference, as the
1AIML Lab, School of Computer Science, University of
St.Gallen, St.Gallen, Switzerland. Correspondence to: Konstantin
Sch¨urholt<konstantin.schuerholt@unisg.ch >.size of the models continuous to increases (Hoeﬂer et al.,
2021; Strubell et al., 2019). As an example, the image
classiﬁcation model CoCa, which currently achieves the
highest accuracy (91.0%) on the ImageNet dataset, has 2.1
billion parameters (Yu et al., 2022). Forecasts predict that
by 2025 models will exist able to achieve a performance
of 95% on the ImageNet object classiﬁcation but demand
as much electricity for training as New York City emits in
CO2in a month (Thompson et al., 2022).
One approach to tackle this issue is to exploit the over-
parameterization of large models of neural networks (NNs)
to successfully train, but to reduce their size signiﬁcantly
for inference. According to (Hoeﬂer et al., 2021) sparsiﬁ-
cation of neural network models can achieve reductions of
10-100x without signiﬁcant losses in performance, even for
extremely large models (Frantar & Alistarh, 2023). By prun-
ing parameters after training, it becomes possible to reduce
the required computational power for inference, save energy
or deploy models on mobile devices, on embedded systems
or satellites with limited storage capabilities (Giuffrida et al.,
2021; Hoeﬂer et al., 2021; Howard et al., 2017).
Related work has investigated individual methods of spar-
siﬁcation extensively (Blalock et al., 2020; Hoeﬂer et al.,
2021) and run large scale studies rigorously evaluating per-
formance differences between different methods (Gale et al.,
2019). Contrary to (Gale et al., 2019), who evaluate sparsiﬁ-
cation on ﬁxed seeds and optimize hyper-parameters for best
sparsiﬁcation, this work evaluates the effect on speciﬁcation
on populations of neural network models (so called “model
zoos”). Since neural networks follow a non-convex opti-
mization and are sensitive to hyper-parameter selection, to
achieve more robust results in studying sparsity we propose
to shift the focus from individual models to populations
of neural networks (Sch ¨urholt et al., 2021a; 2022a), which
are trained according to controlled generating factors i.e.,
selection of hyper-parameters, seeds, initialization meth-
ods. To the best of our knowledge, there are no studies on
sparsiﬁcation on a population of neural network models.
Our contributions: (1) We generate a sparsiﬁed version
of an available model zoo (Sch ¨urholt et al., 2022c) using two
popular sparsiﬁcation methods, namely Variational DropoutarXiv:2304.13718v1  [cs.LG]  26 Apr 2023

--- PAGE 2 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
Figure 1. An overview of the approach. (Left:) A population of neural network models is trained according to latent generating factors
such as dataset, architecture and hyperparameter. (Middle:) The given model zoos are sparsiﬁed given magnitude pruning and variational
dropout. (Right:) Models in the populations are analyzed and entire model zoos are compared with each other and its sparsiﬁed counterpart.
Additional representation of the zoos are trained to analyse the underlying structure of the sparsiﬁed zoos.
(VD) (Molchanov et al., 2017) and Magnitude Pruning
(MP) (Han et al., 2015; Str ¨om, 1997) and thus generate
a dataset consisting of 33’920 trained and sparsiﬁed CNNs
with 1’721’600 unique model states representing their spar-
siﬁcation trajectories. (2) We conduct an in-depth analysis
and comparison of the sparsiﬁed model zoos and the utilized
sparsiﬁcation methods and ﬁnd that i) both methods perform
robustly on all populations, ii) MP outperforms VD except
for some very high sparsity ratios and iii) higher sparsity
ratios are achieved in larger layers consistently in the popu-
lations of the model zoos. Since for each individual model a
dense and fully parameterised as well as a sparsiﬁed version
exists, their relationships can be investigated. Particular
attention is paid to investigating how robustly the methods
perform on the model zoos trained on different datasets
and with varying hyperparameter conﬁgurations. (3) As ex-
pected, on average with increased sparsiﬁcation, we observe
a performance drop in the populations. However, within
the population, we can ﬁnd individual models, which are
less prone to the performance drop (they are sparsiﬁcation-
friendly) or vice verse, are affected stronger by the perfor-
mance drop (they are sparsiﬁcation-hard). (4) Furthermore,
we examine the weight spaces of the sparsiﬁed model zoos
by learning hyper-representations of the individual model
parameters and are able to show that model properties such
as accuracy and sparsity disentangle very well in the latent
space and can be predicted from its latent representation.
2. Related Work
Sparsiﬁcation of Neural Networks Model sparsiﬁcation
has been studied in depth, (Hoeﬂer et al., 2021) provides a
survey over the different approaches. Most sparsiﬁcation
approaches can be categorized as ’data-free’ or ’training-
aware’. Data-free approaches prune models based on the
structure of the neural networks. Magnitude Pruning (MP)(Han et al., 2015; Str ¨om, 1997) as the most common repre-
sentative uses the absolute value or parameters as indicator
for importance, but several other approaches have been pro-
posed (Kusupati et al., 2020; Bellec et al., 2017). Training-
aware rely on data to identify parameters that have the least
impact on the output, based on, e.g., ﬁrst (Xiao et al., 2019;
Ding et al., 2019; Lis et al., 2019; Lee et al., 2018; Srinivas
& Babu, 2015) or second order (Hassibi et al., 1993; Cun
et al., 1990; Dong et al., 2017; Wang et al., 2019; Theis
et al., 2018; Ba et al., 2016; Martens & Grosse, 2015) ap-
proximations of the loss functions. Variational methods like
Variational Dropout (VD) (Molchanov et al., 2017) explic-
itly model the distribution of parameters and remove those
with high amount of noise.
A large comparative study of sparsiﬁcation methods
found that simple MP can match or outperform more
complicated VD on large models (Gale et al., 2019).
Similarly, (neuralmagic) offers a selection of sparsiﬁed
large-scale NNs. Despite the great diversity of sparsiﬁcation
methods and the application of those methods to a diverse
range of NNs, sparsiﬁcation has not yet been applied and
studied on a large population of CNNs.
Populations of Neural Networks Recently, populations
of models have become an object of study. Several ap-
proaches predict model properties from model features (Yak
et al., 2019; Jiang et al., 2019; Corneanu et al., 2020; Martin
& Mahoney, 2019; Unterthiner et al., 2020; Eilertsen et al.,
2020) or compare models based on their activations (Raghu
et al., 2017; Morcos et al., 2018; Nguyen et al., 2020). Other
methods leverage zoos for transfer or meta learning (Liu
et al., 2019; Shu et al., 2021; Ramesh & Chaudhari, 2022).
Another line of work investigates the weight space of trained
models (Lucas et al.; Wortsman et al., 2021; Benton et al.,
2021; Ainsworth et al., 2022; Ilharco et al., 2022). Recently,

--- PAGE 3 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
Figure 2. (Left:) Test accuracy over the initial training over a ﬁxed number of epochs in the original MNIST Seed model zoo. (Right:)
Test accuracy and sparsity over the 25 epochs of VD sparsiﬁcation starting from the last epoch of the original training.
several methods have been proposed to learn representa-
tions of trained models (Denil et al., 2013; Berardi et al.,
2022; Peebles et al., 2022; Ashkenazi et al., 2022; Wang
et al., 2023; Navon et al., 2023). (Sch ¨urholt et al., 2021a;
2022a) proposed a self-supervised approach to learn repre-
sentations of populations of models, which they dub hyper-
representations and show to disentangle model properties
and be useful to generate new models. Nonetheless, there
are only few structured datasets of model zoos. (Gavrikov
& Keuper, 2022) publish and analyse a dataset of convo-
lutional ﬁlters. (Sch ¨urholt et al., 2022c) provide a large
dataset of diverse, pre-trained models, which form the basis
for our sparsiﬁcation work.
3. Generating Sparsiﬁed Model Zoo Twins
To analyse sparsity on populations, we apply two sparsi-
ﬁcation methods on existing pre-trained model zoos, as
outlined in Figures 1 and 2. We select magnitude pruning
and variational dropout as representative for data-free
and training-aware methods, since they can be applied to
small or medium sized CNNs that were already trained to
convergence and the methods are appropriate for scaling
to large populations of models.
The model zoos of (Sch ¨urholt et al., 2022c) serve as a
starting point of the sparsiﬁcation process. We refer to these
model zoos as original model zoos. (Sch ¨urholt et al., 2022c)
establish a setting of varying architectures Aand hyperpa-
rameterson different datasets Dfor the generation of their
zoos, which we adopt for this work. The zoos were trained
onMNIST (LeCun et al., 1998), Fashion-MNIST (Xiao
et al., 2017), SVHN (Netzer et al., 2011), USPS (Hull, 1994),CIFAR-10 (Krizhevsky, 2009) and STL-10 (Coates
et al., 2011) using a small CNN architecture. To sparsify the
model zoos, we apply both sparsiﬁcation methods to the last
state of each model in the zoos. To ensure that the sparsiﬁed
versions of the CNNs can be compared with their original
versions, the generating factors A,andDof the original
models remain unchanged, except for the learning rate.
Magnitude Pruning To sparsify model zoos with MP,
we select several sparsity ratios and sparsify each model in
the zoo accordingly. The corresponding fraction of weights
with smallest absolute value is set to zero and removed from
the set of learnable parameters. We use global unstructured
MP and rely on the pytorch implementation (Paganini,
2019). MP generally hurts the performance, so we ﬁne-tune
the pruned models on their original dataset to recover for
a ﬁxed number of epochs. During ﬁne-tuning we document
each epoch by saving the current state dict of the model and
report the test accuracy and generalization gap.
Variational Dropout Following a similar setup, we apply
VD for deﬁned number of epochs on the last state of every
model in the model zoos. Following (Gale et al., 2019),
we reduce the learning rate compared to the original zoos.
After training, the parameters with high variance ( 3)
are removed from the NNs. As VD includes training,
we do not ﬁne-tuning the models further. We document
each training epoch by saving the state dict as well es the
accuracy ratio, test accuracy and generalization gap.

--- PAGE 4 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
4. Experiments
This section outlines the experimental setup, evaluation, and
analysis of generated sparsiﬁed populations of NN models.
4.1. Experimental Setup
We sparsify 14 model zoos with VD and 10 model zoos
with MP using the the methods introduced above. In
the case of MP, we sparsify each zoo with sparsity levels
[10;20;30;40;50;60;70;80;90]%. This is followed by 15
epochs of ﬁne-tuning, in which the pruned weights do not
receive a weight update. For our experiments, we use the
pruning library of PyTorch (Paganini, 2019). For VD, each
weight parameter of the model receives an additional param-
eter. Each model is trained for 25 epochs and the learnable
parameters wandare optimized. Both wandare ag-
gregated in a per-parameter value . Weights are pruned
for>3. For the implementation of VD, we adapted the
fully-connected and convolutional layers of PyTorch based
on the code of several previous works (Ryzhikov, 2021;
Gale et al., 2019; Molchanov et al., 2017).
Computing Infrastructure: The model zoos were
sparsiﬁed on nodes with up to 4 CPUs and 64g RAM.
Sparsifying a zoo of 1000 models takes 2-3 days. Large
and more complex model zoos consisting of roughly 2600
models and greater diversity in terms of hyperparameters
may take up to 11 days. Hyper-representations are trained
on a GPU of a NVIDIA DGX2 station for up to 12 hours.
4.2. Evaluation
For every model at every state, we record test accuracy, gen-
eralization gap and sparsity ratio as fundamental meterics
to evaluate models. Further, we compute the agreement
between original and sparisiﬁed models and learn hyper-
representations, to evaluate the structure of populations of
sparsiﬁed models.
Model Agreement As one measure for evaluation, we
compute the pairwise agreement of models within the spar-
siﬁed and original model zoos. The models agree when both
predict the same class given same test data. Per model pair
(kandl) this is summed up as follows:
aggr=1
NNX
i=1yi; (1)
for test samples i= 1;:::;N , whereyi= 1, ifyk
i=yl
iand
yi= 0 otherwise.
Hyper-Representation Learning For a deeper under-
standing of the weight spaces of the model zoos created
with VD, we train a attention based auto-encoder (AE) pro-posed by (Sch ¨urholt et al., 2022a;b). We learn task-agnostic
hyper-representations in a self-supervised learning setting.
Such representations can provide a proxy to how structured
the sparsiﬁcation process is. Explicitly, it provides insights
in how well weights and alphas can be compressed and how
well the latent space disentangles model properties like ac-
curacy or sparsity. We adapt the AE to take non-sparsiﬁed
weights as input and reconstruct to weights and sparsiﬁca-
tion maps (). To improve the reconstruction quality, we
introduce a new loss normalisation for the reconstruction of
the alpha parameters deﬁned as
L
MSE =1
MMX
i=1tanh ^i t
r
 tanh i t
r2
2;
(2)
wheretrefers to the pruning threshold and rto the selected
range of interest. With that, we force the model to pay
attention to the active range around the threshold that
determines sparsiﬁcation. Details of the model are shown
in Appendix G and G.
4.3. Experimental Results and Analysis
In this section we analyze the 24 sparsiﬁed model zoos. Due
to the large scope of the results we only show highlights
here and provide full details in Appendix B and C.
Robust Performance on Population Level: As previous
work investigated sparsiﬁcation on single models, or hyper-
parameter optimization of sparsiﬁcation, the robustness of
sparsiﬁcation methods on populations has not yet been eval-
uated. Related work indicates that pruning the excess param-
eters of a model reduces overﬁtting and thus improves test
accuracy and generalization (Hoeﬂer et al., 2021; Bartold-
son et al., 2020). With further increasing sparsity, functional
parts of the models are removed and the performance drops.
To investigate the performance of the methods we consider
the sparsity-ratio, test accuracy and generalization gap (train
accuracy - test accuracy) as metrics. In our experiments,
magnitude pruning and variational dropout have showed
remarkably robust sparsiﬁcation performance on a popula-
tion basis, preserving the original accuracy for considerable
levels of sparsity. As illustrated in 2, the distribution of the
performance metrics of the individual models in the zoo is
very consistent and the variation from the top to the worst
performing models is low. Although the standard deviation
of the performance is higher on model zoos trained on a
more sophisticated image dataset (e.g. CIFAR-10), compa-
rable results are achieved. The results furthermore conﬁrm
on a population level, that the generalization gap of is lower
for models with moderate sparsiﬁcation levels.

--- PAGE 5 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
Figure 3. Sparsiﬁcation Frequency per weight for the MNIST zoo
at different VD epochs. Within layers, there is remarkable consis-
tency. Further, different layers are pruned in different phases
Larger Layers Achieve Higher Sparsity Ratios The
previous results indicate considerable robustness and consis-
tency in the sparsiﬁcation results within and between model
zoos. To shed further light on sparsiﬁcation patterns, we
investigate the sparsiﬁcation per layer. Within zoos, the
sparsiﬁcation ratios per layer are remarkably consistent, see
Figure 3. Across all zoos, our experiments show that larger
layers are more strongly pruned, since a positive relationship
between the number of parameters of a layer and the corre-
sponding sparsity ratio exists. This relationship is shown in
Figure 4. Detailed results regarding the sparsity per layer
can be found in Appendix 4, D and E. This may indicate
that the allocation of parameters in the architecture for the
original model zoos of (Sch ¨urholt et al., 2022c) was not
optimal. This is in line with the literature (Hoeﬂer et al.,
2021), which states that pruning works particularly well for
over-parameterized models.
Figure 4. Binned scatter plot of all model zoos sparsiﬁed with
variational dropout at epoch 5, 10, 15 and 20. Epoch 25 is not
shown because certain model zoos collapsed at high sparsity ratios
and this would distort the plot. The x-axis shows the logarithmized
layer size, the y-axis the logarithmized mean sparsity level. The
error bar represents the standard deviation of the sparsity.
Magnitude Pruning outperforms Variational Dropout
Related work found that MP can outperform VD, especially
for moderate sparsity ratios (Gale et al., 2019). Our results
conﬁrm that on population level. MP outperforms VD for
sparsiﬁcation levels of up to 80% consistently, see Figure5, Appendix B and C. At higher sparsiﬁcation levels, MP
shows steep drops in performance. VD on some zoos is
more stable and thus shows higher performance at higher
sparsiﬁcation levels, justifying the larger parameter count
and computational load.
Figure 5. Mean accuracy per zoo over sparsity for a selection of
model zoos sparsiﬁed with VD and MP. MP outperforms VD up to
sparsity levels of 80%. At higher sparsity, MP performance drops,
VD performance is more stable.
Table 1. Agreement overview between original and twin Seed
model zoos. The values mean (std) are reported in %. Higher
values indicate higher agreement. Agreem denotes Agreement.
Magnitude Pruning Variational Dropout
Model Zoo Accuracy Sparsity Agreem Accuracy Sparsity Agreem
MNIST (s) 83.7 (13.5) 80.0 (0.0) 82.1 (13.0) 87.6 (1.2) 78.0 (1.1) 83.4 (1.4)
USPS (s) 73.8 (17.3) 90.0 (0.0) 74.3 (17.3) 82.3 (1.5) 88.5 (0.6) 86.6 (1.1)
SVHN (s) 70.7 (7.7) 60.0 (0.0) 74.9 (2.5) 62.2 (7.2) 62.8 (2.9) 57.5 (6.0)
FMNIST (s) 73.6 (1.3) 70.0 (0.0) 79.7 (1.9) 69.3 (1.2) 72.0 (1.0) 76.2 (2.1)
CIFAR-10 (s) 47.3 (1.2) 70.0 (0.0) 78.4 (1.2) 40.5 (1.5) 67.1 (2.8) 61.0 (2.7)
STL-10 (s) 40.6 (0.8) 70.0 (0.0) 55.9 (2.8) 35.9 (1.2) 66.5 (1.3) 54.4 (2.8)
Agreement between Twin and Original Model Zoos
By analysing the agreement between the original and
twin models, we investigate how well the two methods
preserve the behavior of the original models, beyond loss
or accuracy. The agreement is evaluated for six model zoos
at the sparsity ratio 60%, 70%, 80% or 90%. The sparsity
ratio was selected such that a favourable accuracy-sparsity
trade-off is achieved in variational dropout.
The results show relatively high levels of agreement be-
tween 60 and 80 % for both methods. Unsurprisingly, the
agreement is higher for overall higher levels of accuracy.
Generally, MP achieves higher accuracy and agreement, and
appears to therefore preserve the original behavior of the
model better. Our results indicate that simple performance
metrics like accuracy may be a good proxy to estimate pre-
served behavior like agreement.

--- PAGE 6 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
Performance of Original and Sparsiﬁed Models are Cor-
related The sparsiﬁcation of populations show remark-
ably robust results, as indicated above. Nonetheless, there
is a spread in the performance of sparsiﬁed models, see
Figure 2. In practice, it is relevant to identify candidates
for high performance at high sparsity before sparsiﬁcation.
As ﬁrst approximation, we compute the correlation between
model performance before and after sparsiﬁcation. We use
Pearson’s r as well as Kendall’s tau coefﬁcients, the former
measures the covariance normalized by the product of vari-
ances, the latter measures agreement in rank order between
the two paired samples. The results show a remarkable
high correlation between original and sparsiﬁed models, see
Table 2. For ﬁxed sparsity levels with MP, the Pearson’s
r correlation is above 90% with a single exception. The
Kendall’s tau is similarly high, indicating that the rank order
of samples remains preserved to a high degree. Since the
sparsiﬁcation levels of VD zoos are not as consistent, the
correlation values are lower, but conﬁrm the ﬁnding. Conse-
quently, based on the results of the sparsiﬁed populations,
the best performing models will likely be the best or among
the best sparsiﬁed models.
Table 2. Correlation between the per-model performance of orig-
inal and sparsiﬁed accuracy. Values are Pearson correlation and
Kendall’s tau in %. Original model performance and sparsiﬁed
performance are highly correlated.
Magnitude Pruning Variational Dropout
Model Zoo Sparsity Pearson’s r Kendall’s tau Sparsity Pearson’s r Kendall’s tau
MNIST 80.0 99.7 89.4 78.0 51.4 35.2
USPS 80.0 90.0 71.6 88.5 52.1 35.0
SVHN 80.0 98.7 92.0 64.7 91.6 54.7
FMNIST 80.0 95.7 72.7 72.6 58.6 40.9
CIFAR s 80.0 97.2 82.7 67.1 72.6 45.2
CIFAR l 80.0 93.4 76.3 67.3 50.0 33.3
STL s 80.0 96.7 82.3 66.5 56.9 37.7
STL l 80.0 73.7 52.8 49.0 80.9 61.3
Disentangled Representations learned from Weight
Space With the revised AE and its novel loss normali-
sation we are able to not only reconstruct the weight spaces
of the CNNs but also the alpha parameters needed for the
pruning decision in VD. The results are remarkable in that
both accuracy and sparsity are highly predictable and thus
disentangled very well in latent space. What is more, both
weights as well as alphas are reconstructed well, indicat-
ing a high degree of structure in populations of sparsiﬁed
models. This opens the door for future attempts to zero-shot
sparsify models impressing such structure on pre-trained
models. The results are shown in Table 3.
5. Conclusion
In this work, we have analyzed sparsiﬁcation on large popu-
lations of neural networks. Using magnitude pruning and
variational dropout as underlying sparsiﬁcation approach,Table 3. Results overview of the hyper-representation learning task
on the MNIST Seed and SVHN Seed model zoos. The AE is
trained with the weight spaces of the original and sparsiﬁed model
zoos. All values are R2reported in %. Higher values are better.
Weights and alphas are the reconstructions R2. Accuracy, spar-
sity, epoch and generalization gap (GGap) are predicted from the
embeddings as in (Sch ¨urholt et al., 2021b).
Zoo Weights Alphas Accuracy Sparsity Epoch GGap
MNIST (s) 78.7 79.4 96.3 98.0 53.2 19.1
SVHN (s) 74.8 74.2 94.5 97.6 30.2 10.5
we have created ten sparsiﬁed model zoo twins representing
common computer vision datasets. In total, we have cre-
ated 23’920 sparsiﬁed models with 1’726’000 documented
model states. We can conﬁrm, that both approaches - magni-
tude pruning (MP) and variational dropout (VD) - perform
well on population level with respect to sparsiﬁcation ratio
and accuracy. For sparsiﬁcation ratios below 80%, MP out-
performs VD. At higher sparsiﬁcation ratios, both methods
degrade, but VD is more stable. Sparsiﬁed models show
high agreement with their original models, with no clear
preference between the two sparsiﬁcation approaches. We
further ﬁnd that performance before and after sparsiﬁca-
tion is highly correlated, indicating that the best performing
model is the best candidate for sparsiﬁcation. The sparsiﬁca-
tion characteristics per layer within the zoos are surprisingly
consistent. This gives rise to learning hyper-representations
on sparsiﬁed model zoos, which shows to be unexpectedly
successful. That indicates that sparsiﬁcation is highly struc-
tured, which may be exploited for zero-shot sparsiﬁcation.
References
Ainsworth, S. K., Hayase, J., and Srinivasa, S. Git Re-
Basin: Merging Models modulo Permutation Symmetries,
September 2022.
Ashkenazi, M., Rimon, Z., Vainshtein, R., Levi, S., Richard-
son, E., Mintz, P., and Treister, E. Nern–learning neu-
ral representations for neural networks. arXiv preprint
arXiv:2212.13554 , 2022.
Ba, J., Grosse, R., and Martens, J. Distributed second-order
optimization using kronecker-factored approximations.
2016.
Bartoldson, B., Morcos, A., Barbu, A., and Erlebacher, G.
The generalization-stability tradeoff in neural network
pruning. Advances in Neural Information Processing
Systems , 33:20852–20864, 2020.
Bellec, G., Kappel, D., Maass, W., and Legenstein, R. Deep
rewiring: Training very sparse deep networks, 2017. URL
https://arxiv.org/abs/1711.05136 .

--- PAGE 7 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
Benton, G. W., Maddox, W. J., Lotﬁ, S., and Wilson, A. G.
Loss Surface Simplexes for Mode Connecting V olumes
and Fast Ensembling. In PMLR , 2021.
Berardi, G., De Luigi, L., Salti, S., and Di Stefano, L. Learn-
ing the Space of Deep Models, June 2022.
Blalock, D., Gonzalez Ortiz, J. J., Frankle, J., and Guttag, J.
What is the state of neural network pruning? Proceedings
of machine learning and systems , 2:129–146, 2020.
Brock, A., Donahue, J., and Simonyan, K. Large scale gan
training for high ﬁdelity natural image synthesis. arXiv
preprint arXiv:1809.11096 , 2018.
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A
simple framework for contrastive learning of visual repre-
sentations, 2020. URL https://arxiv.org/abs/
2002.05709 .
Coates, A., Lee, H., and Ng, A. Y . An Analysis of Single-
Layer Networks in Unsupervised Feature Learning. In
Proceedings of the 14th International Con- Ference on
Artiﬁcial Intelligence and Statistics (AISTATS) , pp. 9,
2011.
Corneanu, C. A., Escalera, S., and Martinez, A. M. Comput-
ing the Testing Error Without a Testing Set. In 2020
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , pp. 2674–2682, Seattle, WA,
USA, June 2020. IEEE. ISBN 978-1-72817-168-5. doi:
10.1109/CVPR42600.2020.00275.
Cun, Y . L., Denker, J. S., and Solla, S. A. Optimal Brain
Damage , pp. 598–605. Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA, 1990. ISBN 1558601007.
Denil, M., Shakibi, B., Dinh, L., and Ranzato, M. Predicting
Parameters in Deep Learning. In Neural Information
Processing Systems (NeurIPS) , pp. 9, 2013.
Ding, X., Ding, G., Guo, Y ., and Han, J. Centripetal sgd
for pruning very deep convolutional networks with com-
plicated structure. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pp.
4943–4953, 2019.
Dong, X., Chen, S., and Pan, S. Learning to prune deep
neural networks via layer-wise optimal brain surgeon.
Advances in Neural Information Processing Systems , 30,
2017.
Eilertsen, G., J ¨onsson, D., Ropinski, T., Unger, J., and
Ynnerman, A. Classifying the classiﬁer: Dissecting the
weight space of neural networks. arXiv:2002.05688 [cs] ,
February 2020.Frantar, E. and Alistarh, D. Sparsegpt: Massive language
models can be accurately pruned in one-shot, 2023. URL
https://arxiv.org/abs/2301.00774 .
Gale, T., Elsen, E., and Hooker, S. The state of sparsity in
deep neural networks. arXiv preprint arXiv:1902.09574 ,
2019.
Gavrikov, P. and Keuper, J. CNN Filter DB: An Empir-
ical Investigation of Trained Convolutional Filters. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pp. 11, 2022.
Giuffrida, G., Fanucci, L., Meoni, G., Bati ˇc, M., Buckley,
L., Dunne, A., van Dijk, C., Esposito, M., Hefele, J.,
Vercruyssen, N., et al. The -sat-1 mission: the ﬁrst
on-board deep neural network demonstrator for satellite
earth observation. IEEE Transactions on Geoscience and
Remote Sensing , 60:1–14, 2021.
Han, S., Mao, H., and Dally, W. J. Deep compres-
sion: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149 , 2015.
Hassibi, B., Stork, D. G., Wolff, G., and Watanabe, T. Opti-
mal brain surgeon: Extensions and performance compar-
isons. In Proceedings of the 6th International Conference
on Neural Information Processing Systems , NIPS’93, pp.
263–270, San Francisco, CA, USA, 1993. Morgan Kauf-
mann Publishers Inc.
Hoeﬂer, T., Alistarh, D., Ben-Nun, T., Dryden, N., and
Peste, A. Sparsity in deep learning: Pruning and growth
for efﬁcient inference and training in neural networks. J.
Mach. Learn. Res. , 22(241):1–124, 2021.
Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang,
W., Weyand, T., Andreetto, M., and Adam, H. Mobilenets:
Efﬁcient convolutional neural networks for mobile vision
applications. arXiv preprint arXiv:1704.04861 , 2017.
Hull, J. A database for handwritten text recognition research.
IEEE Transactions on Pattern Analysis and Machine In-
telligence , 16(5):550–554, May 1994. ISSN 1939-3539.
doi: 10.1109/34.291440.
Ilharco, G., Ribeiro, M. T., Wortsman, M., Gururangan, S.,
Schmidt, L., Hajishirzi, H., and Farhadi, A. Editing mod-
els with task arithmetic. arXiv preprint arXiv:2212.04089 ,
2022.
Jiang, Y ., Krishnan, D., Mobahi, H., and Bengio, S. Pre-
dicting the Generalization Gap in Deep Networks with
Margin Distributions. arXiv:1810.00113 [cs, stat] , June
2019.

--- PAGE 8 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and
Amodei, D. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361 , 2020.
Krizhevsky, A. Learning Multiple Layers of Features from
Tiny Images. pp. 60, 2009.
Kusupati, A., Ramanujan, V ., Somani, R., Wortsman, M.,
Jain, P., Kakade, S., and Farhadi, A. Soft threshold weight
reparameterization for learnable sparsity. In Interna-
tional Conference on Machine Learning , pp. 5544–5555.
PMLR, 2020.
LeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P. Gradient-
Based Learning Applied to Document Recognition. Pro-
ceedings of the IEEE , 86(11):2278–2324, November
1998.
Lee, N., Ajanthan, T., and Torr, P. H. S. Snip: Single-shot
network pruning based on connection sensitivity, 2018.
URL https://arxiv.org/abs/1810.02340 .
Lis, M., Golub, M., and Lemieux, G. Full deep neural
network training on a pruned weight budget. Proceedings
of Machine Learning and Systems , 1:252–263, 2019.
Liu, I.-J., Peng, J., and Schwing, A. G. Knowledge Flow:
Improve Upon Your Teachers. In International Confer-
ence on Learning Representations (ICLR) , April 2019.
Lucas, J., Bae, J., Zhang, M. R., Fort, S., Zemel, R., and
Grosse, R. Analyzing Monotonic Linear Interpolation in
Neural Network Loss Landscapes. pp. 12.
Martens, J. and Grosse, R. Optimizing neural networks with
kronecker-factored approximate curvature. In Interna-
tional conference on machine learning , pp. 2408–2417.
PMLR, 2015.
Martin, C. H. and Mahoney, M. W. Traditional and Heavy-
Tailed Self Regularization in Neural Network Models.
arXiv:1901.08276 [cs, stat] , January 2019.
Molchanov, D., Ashukha, A., and Vetrov, D. Variational
dropout sparsiﬁes deep neural networks. In Interna-
tional Conference on Machine Learning , pp. 2498–2507.
PMLR, 2017.
Morcos, A. S., Raghu, M., and Bengio, S. Insights on repre-
sentational similarity in neural networks with canonical
correlation. arXiv:1806.05759 [cs, stat] , June 2018.
Navon, A., Shamsian, A., Achituve, I., Fetaya, E., Chechik,
G., and Maron, H. Equivariant architectures for learning
in deep weight spaces. arXiv preprint arXiv:2301.12780 ,
2023.Netzer, Y ., Wang, T., Coates, A., Bissacco, A., Wu, B.,
and Ng, A. Y . Reading Digits in Natural Images with
Unsupervised Feature Learning. In NIPS Workshop on
Deep Learning and Unsupervised Feature Learning 2011 ,
pp. 9, 2011.
neuralmagic. Sparsezoo. URL https://sparsezoo.
neuralmagic.com/ .
Nguyen, T., Raghu, M., and Kornblith, S. Do Wide and
Deep Networks Learn the Same Things? Uncovering
How Neural Network Representations Vary with Width
and Depth. arXiv:2010.15327 [cs] , October 2020.
Paganini, M. Pruning tutorial ¶, 2019. URL
https://pytorch.org/tutorials/
intermediate/pruning_tutorial.html .
Peebles, W., Radosavovic, I., Brooks, T., Efros, A. A., and
Malik, J. Learning to Learn with Generative Models of
Neural Network Checkpoints, September 2022.
Raghu, M., Gilmer, J., Yosinski, J., and Sohl-Dickstein, J.
SVCCA: Singular Vector Canonical Correlation Anal-
ysis for Deep Learning Dynamics and Interpretability.
arXiv:1706.05806 [cs, stat] , June 2017.
Ramesh, R. and Chaudhari, P. Model Zoo: A Growing
”Brain” That Learns Continually. In International Con-
ference on Learning Representations ICLR , 2022.
Ryzhikov, A. Pytorch ard: Pytorch implementation of
variational dropout sparsiﬁes deep neural networks, Nov
2021. URL https://github.com/HolyBayes/
pytorch_ard .
Sch¨urholt, K., Kostadinov, D., and Borth, D. Self-supervised
representation learning on neural network weights for
model characteristic prediction. Advances in Neural In-
formation Processing Systems , 34:16481–16493, 2021a.
Sch¨urholt, K., Kostadinov, D., and Borth, D. Self-supervised
representation learning on neural network weights for
model characteristic prediction. Advances in Neural In-
formation Processing Systems , 34:16481–16493, 2021b.
Sch¨urholt, K., Knyazev, B., Gir ´o-i Nieto, X., and Borth, D.
Hyper-representations as generative models: Sampling
unseen neural network weights. Conference on Neural
Information Processing Systems (NeurIPS), 2022 , 2022a.
Sch¨urholt, K., Knyazev, B., Gir ´o-i Nieto, X., and Borth,
D. Hyper-representation for pre-training and transfer
learning. In First Workshop on Pre-training: Perspectives,
Pitfalls, and Paths Forward at ICML 2022 , 2022b.
Sch¨urholt, K., Taskiran, D., Knyazev, B., Gir ´o-i Nieto, X.,
and Borth, D. Model zoos: A dataset of diverse popula-
tions of neural network models. Conference on Neural

--- PAGE 9 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
Information Processing Systems (NeurIPS), Datasets and
Benchmarks Track, 2022 , 2022c.
Shu, Y ., Kou, Z., Cao, Z., Wang, J., and Long, M. Zoo-
Tuning: Adaptive Transfer from a Zoo of Models. In
International Conference on Machine Learning (ICML) ,
pp. 12, 2021.
Srinivas, S. and Babu, R. V . Learning neural network ar-
chitectures using backpropagation, 2015. URL https:
//arxiv.org/abs/1511.05497 .
Str¨om, N. Sparse connection and pruning in large dynamic
artiﬁcial neural networks. In Fifth European Confer-
ence on Speech Communication and Technology . Citeseer,
1997.
Strubell, E., Ganesh, A., and McCallum, A. Energy and
policy considerations for deep learning in nlp, 2019. URL
https://arxiv.org/abs/1906.02243 .
Tan, M. and Le, Q. Efﬁcientnet: Rethinking model scal-
ing for convolutional neural networks. In International
conference on machine learning , pp. 6105–6114. PMLR,
2019.
Theis, L., Korshunova, I., Tejani, A., and Husz ´ar, F. Faster
gaze prediction with dense networks and ﬁsher prun-
ing, 2018. URL https://arxiv.org/abs/1801.
05787 .
Thompson, N. C., Greenewald, K., Lee, K., and
Manso, G. F. Deep learning’s diminishing returns,
Nov 2022. URL https://spectrum.ieee.org/
deep-learning-computational-cost .
Unterthiner, T., Keysers, D., Gelly, S., Bousquet, O., and
Tolstikhin, I. Predicting Neural Network Accuracy from
Weights. arXiv:2002.11448 [cs, stat] , February 2020.
Wang, C., Grosse, R., Fidler, S., and Zhang, G. Eigendam-
age: Structured pruning in the kronecker-factored eigen-
basis. In International Conference on Machine Learning ,
pp. 6566–6575. PMLR, 2019.
Wang, J., Chen, Y ., Yu, S. X., Cheung, B., and LeCun,
Y . Compact and optimal deep learning with recurrent
parameter generators. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision ,
pp. 3900–3910, 2023.
Wortsman, M., Horton, M. C., Guestrin, C., Farhadi, A.,
and Rastegari, M. Learning Neural Network Subspaces.
InInternational Conference on Machine Learning , pp.
11217–11227. PMLR, July 2021.
Xiao, H., Rasul, K., and V ollgraf, R. Fashion-MNIST: A
Novel Image Dataset for Benchmarking Machine Learn-
ing Algorithms, September 2017.Xiao, X., Wang, Z., and Rajasekaran, S. Autoprune: Auto-
matic network pruning by regularizing auxiliary parame-
ters. Advances in neural information processing systems ,
32, 2019.
Yak, S., Gonzalvo, J., and Mazzawi, H. Towards Task and
Architecture-Independent Generalization Gap Predictors.
arXiv:1906.01550 [cs, stat] , June 2019.
Yu, J., Wang, Z., Vasudevan, V ., Yeung, L., Seyedhosseini,
M., and Wu, Y . Coca: Contrastive captioners are image-
text foundation models, 2022. URL https://arxiv.
org/abs/2205.01917 .

--- PAGE 10 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
A. Experimental Overview
Model Zoo W Activation Optim LR VD LR MP WD Dropout
MNIST (s) Seed 2416 T AD 3e-4 1e-3 0 0
MNIST (s) Random 2416 T, S, R, G AD, SGD 1e-3, 1e-4 1e-3, 1e-4 1e-3, 1e-4 0, 0.5
MNIST (s) Fixed 2416 T, S, R, G AD, SGD 1e-3, 1e-4 1e-3, 1e-4 1e-3, 1e-4 0, 0.5
SVHN (s) Seed 2416 T AD 3e-3 1e-3 0 0
FMNIST (s) Seed 2416 T AD 3e4 1e-3 0 0
FMNIST (s) Random 2416 T, S, R, G AD, SGD 1e-6 - 1e-3, 1e-4 0, 0.5
FMNIST (s) Fixed 2416 T, S, R, G AD, SGD 1e-6 - 1e-3, 1e-4 0, 0.5
CIFAR-10 (l) Seed 10760 G AD 1e-5 1e-4 1.00E-02 0
CIFAR-10 (l) Random 10760 T, S, R, G AD, SGD 5e-6 - 1e-2,1e-3 0, 0.5
CIFAR-10 (l) Fixed 10760 T, S, R, G AD, SGD 5e-6 - 1e-2,1e-3 0, 0.5
CIFAR-10 (s) Seed 2816 G AD 5e-6 1e-4 1e-2 0
USPS (s) Seed 2416 T AD 3e-4 1e-3 1e-3 0
STL-10 (l) Seed 10760 T AD 1e-4 1e-3 1e-3 0
STL-10 (s) Seed 2816 T AD 1e-4 1e-3 1e-3 0
Table 4. Model Conﬁguration for Sparsiﬁcation. Model Zoo contains information about the dataset used for training, the architecture of
the models in the zoo (CNN (s) - small or CNN (l) - large) and the conﬁguration of the original model zoo (Seed, Fixed Seed or Random
Seed). Wdenotes the number of the parameters of the models in a zoo. Activation denotes the activation function used: T - Tanh, S
- Sigmoid, R - ReLU, G - GeLU. Optim denotes the optimizer: AD - Adam, SGD - Stochastic Gradient Descent. LR VD denotes the
learning rate used in VD. LR MP denotes the learning rate used in magnitude pruning. WDdenotes weight decay used. Dropout deﬁnes
the dropout rate used.
B. Results Magnitude Pruning
Model Zoo Metric Epoch
0 15 0 15 0 15 0 15 0 15 0 15
All Spars 0.0 (0.0) 0.0 (0.0) 50.0 (0.0) 50.0 (0.0) 60.0 (0.0) 60.0 (0.0) 70.0 (0.0) 70.0 (0.0) 80.0 (0.0) 80.0 (0.0) 90.0 (0.0) 90.0 (0.0)
MNIST (s) S Acc 91.1 (0.9) 94.5 (0.5) 56.1 (13.3) 93.3 (0.7) 41.2 (13.0) 92.6 (1.0) 28.6 (10.7) 91.1 (1.8) 19.1 (7.7) 83.7 (13.5) 12.4 (4.0) 46.6 (25.1)
GGap 0.6 (0.3) 0.4 (0.2) -0.4 (0.6) 0.1 (0.2) -0.2 (0.6) -0.1 (0.3) -0.1 (0.5) -0.4 (0.3) -0.1 (0.4) -0.7 (0.4) -0.0 (0.3) -0.5 (1.0)
MNIST (s) F Acc 75.1 (34.6) 76.2 (34.4) 61.8 (33.5) 73.7 (34.9) 50.0 (31.0) 72.2 (35.1) 35.4 (25.7) 69.0 (35.1) 19.7 (14.4) 59.8 (34.4) 11.5 (3.7) 33.7 (29.2)
GGap -0.1 (0.5) -5.5 (8.1) -0.4 (0.5) -6.1 (8.5) -0.4 (0.6) -6.5 (9.0) -0.3 (0.6) -7.1 (10.2) -0.1 (0.4) -7.2 (11.4) -0.0 (0.3) -2.4 (6.3)
MNIST (s) R Acc 74.4 (35.0) 75.5 (34.8) 62.0 (33.8) 73.3 (35.1) 50.3 (31.2) 71.8 (35.3) 35.1 (25.1) 69.5 (35.1) 20.1 (14.2) 60.5 (34.7) 11.6 (3.8) 33.7 (29.2)
GGap -0.1 (0.5) -5.6 (8.0) -0.4 (0.5) -5.9 (8.4) -0.4 (0.6) -6.3 (8.9) -0.3 (0.6) -7.0 (10.1) -0.1 (0.4) -7.2 (11.6) -0.0 (0.4) -2.2 (6.2)
SVHN (s) S Acc 71.1 (8.0) 74.4 (8.4) 44.8 (8.7) 72.6 (7.9) 30.1 (7.4) 70.7 (7.7) 19.0 (4.9) 67.1 (7.2) 13.2 (3.6) 58.5 (7.3) 11.7 (3.8) 34.0 (5.7)
GGap 2.8 (0.7) 2.8 (0.7) 2.2 (1.3) 2.7 (0.7) 1.0 (1.2) 2.5 (0.8) 0.0 (0.9) 2.3 (0.8) -0.4 (0.8) 1.8 (0.9) -0.4 (0.8) -0.1 (1.0)
FMNIST (s) S Acc 72.7 (1.0) 75.5 (1.3) 52.1 (8.4) 74.4 (1.2) 43.7 (8.7) 74.2 (1.2) 34.5 (8.6) 73.6 (1.3) 24.2 (8.0) 71.9 (2.7) 14.5 (5.4) 57.1 (14.4)
GGap 1.7 (0.3) 2.3 (0.6) 0.6 (0.4) 1.9 (0.5) 0.4 (0.4) 1.7 (0.5) 0.3 (0.4) 1.5 (0.5) 0.1 (0.3) 1.3 (0.5) 0.0 (0.2) 0.8 (1.0)
CIFAR-10 (s) S Acc 48.7 (1.4) 49.7 (1.3) 39.4 (3.5) 49.0 (1.3) 32.8 (4.5) 48.5 (1.3) 25.2 (4.8) 47.3 (1.2) 17.8 (3.8) 44.4 (1.1) 11.6 (1.9) 34.5 (2.5)
GGap 0.7 (0.4) 0.3 (0.4) 0.2 (0.4) 0.4 (0.4) 0.0 (0.4) 0.4 (0.4) -0.0 (0.4) 0.3 (0.4) -0.0 (0.3) 0.1 (0.4) 0.0 (0.2) -0.3 (0.4)
CIFAR-10 (l) S Acc 61.5 (0.7) 62.4 (0.7) 53.2 (3.9) 61.8 (0.7) 43.7 (6.6) 61.5 (0.6) 32.1 (7.4) 60.8 (0.6) 22.2 (5.9) 59.0 (0.6) 15.9 (3.5) 51.4 (1.0)
GGap 2.0 (0.3) 1.4 (0.4) 1.2 (0.5) 1.7 (0.3) 0.7 (0.5) 1.6 (0.3) 0.2 (0.4) 1.5 (0.3) 0.1 (0.3) 1.1 (0.4) 0.0 (0.3) 0.4 (0.4)
USPS (s) S Acc 87.0 (1.7) 91.0 (0.8) 82.1 (5.0) 90.1 (1.0) 77.4 (7.6) 89.9 (1.0) 70.0 (11.4) 89.5 (1.0) 54.9 (15.7) 88.5 (0.9) 13.5 (9.8) 73.8 (17.3)
GGap 5.1 (0.6) 5.1 (0.6) 4.9 (0.7) 5.0 (0.6) 4.7 (0.8) 4.9 (0.6) 4.3 (0.9) 4.8 (0.6) 3.6 (1.2) 4.6 (0.5) 1.2 (0.9) 3.3 (1.6)
STL-10 (s) S Acc 39.0 (1.1) 41.0 (1.1) 23.7 (4.7) 41.4 (0.9) 20.4 (3.8) 41.3 (0.9) 15.3 (3.6) 40.6 (0.8) 11.9 (2.6) 36.6 (1.7) 10.1 (0.6) 10.9 (3.5)
GGap 4.9 (0.8) 8.6 (1.1) 1.3 (0.8) 5.3 (0.8) 0.7 (0.7) 4.2 (0.9) 0.0 (0.5) 2.6 (0.8) -0.2 (0.5) 1.0 (0.7) -0.2 (0.3) 0.1 (0.3)
STL-10 (l) S Acc 47.4 (0.9) 46.4 (0.9) 36.2 (3.2) 47.4 (0.9) 30.8 (3.9) 48.0 (0.9) 25.2 (3.9) 48.4 (0.9) 19.0 (3.6) 47.9 (0.8) 12.3 (2.6) 42.8 (1.0)
GGap 15.3 (1.0) 29.5 (1.6) 6.0 (1.3) 20.8 (1.2) 3.6 (1.1) 17.6 (1.1) 1.9 (0.9) 13.6 (0.9) 0.7 (0.7) 8.9 (0.8) -0.1 (0.5) 3.5 (0.8)
Table 5. Magnitude Pruning Results. The results are reported as mean (std) in %. Metric denotes the performance metric: Acc - Accuracy
and GGAP - Generalization Gap. The table contains the results of all Seed, Fixed Seed and Random Seed Model Zoos before and after
ﬁne-tuning. The performance is reported for the sparsity levels 0%, 50%, 60%, 70%, 80% and 90%. Within the declaration of the model
zooSrepresents Seed, Frepresents Fixed Seed and Rrepresents Random Seed.

--- PAGE 11 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
C. Results Variational Dropout
Model Zoo Metric Epoch
0 2 4 6 8 10 13 16 19 22 25
MNIST (s) Seed Acc 91.1 (0.9) 91.1 (0.9) 90.5 (0.9) 89.6 (0.9) 88.9 (0.9) 88.4 (1.0) 87.9 (1.1) 87.7 (1.1) 87.6 (1.2) 87.6 (1.2) 87.6 (1.2)
Spar 0.2 (0.1) 26.1 (1.3) 51.3 (1.3) 58.7 (1.2) 63.2 (1.1) 66.6 (1.1) 70.5 (1.0) 73.3 (1.0) 75.4 (1.1) 77.1 (1.1) 78.5 (1.1)
GGap 0.6 (0.3) 0.3 (0.3) -0.6 (0.3) -3.2 (0.4) -5.0 (0.6) -5.4 (0.7) -4.9 (0.7) -4.6 (0.7) -4.2 (0.7) -4.0 (0.7) -3.9 (0.7)
MNIST (s) Fixed Acc 73.8 (35.2) 69.0 (35.2) 64.4 (34.9) 61.3 (34.2) 57.6 (33.6) 53.2 (33.5) 47.4 (33.6) 43.1 (32.8) 39.4 (32.3) 37.0 (31.3) 35.3 (30.3)
Spar 1.5 (7.7) 28.2 (16.7) 46.7 (21.0) 56.6 (21.4) 63.8 (21.1) 68.8 (21.1) 74.0 (22.1) 77.4 (22.2) 80.3 (22.5) 82.3 (22.7) 83.4 (23.4)
GGap -0.1 (0.5) -5.5 (10.0) -8.0 (10.8) -8.1 (11.2) -7.5 (11.2) -6.2 (12.1) -5.4 (11.9) -5.4 (11.4) -5.2 (11.0) -5.1 (11.0) -5.2 (10.5)
MNIST (s) Rand Acc 73.6 (35.3) 68.6 (35.3) 64.0 (35.0) 61.0 (34.3) 57.3 (33.7) 52.9 (33.7) 47.4 (33.6) 42.9 (32.8) 39.4 (32.1) 37.2 (31.2) 35.4 (30.2)
Spar 1.5 (7.0) 28.5 (17.3) 46.8 (21.2) 56.8 (21.4) 64.1 (21.3) 69.0 (21.4) 74.2 (22.1) 77.5 (22.3) 80.3 (22.7) 82.4 (22.8) 83.5 (23.2)
GGap -0.1 (0.5) -5.2 (10.0) -7.8 (10.8) -7.8 (11.1) -7.2 (11.0) -5.8 (12.2) -5.2 (11.8) -5.2 (11.1) -5.0 (10.9) -5.0 (11.0) -5.0 (10.5)
SVHN (s) Seed Acc 71.1 (8.0) 70.6 (8.1) 67.6 (7.5) 65.6 (7.3) 64.0 (7.3) 63.7 (7.3) 62.8 (7.3) 61.8 (7.2) 60.5 (7.0) 59.4 (6.8) 57.9 (6.6)
Spar 0.2 (0.1) 16.7 (4.6) 34.2 (4.6) 44.3 (3.8) 51.6 (3.3) 56.1 (3.0) 60.7 (2.9) 63.9 (2.9) 66.1 (2.9) 67.9 (2.9) 69.5 (2.9)
GGap 2.8 (0.7) 0.3 (0.8) -2.9 (0.9) -3.7 (1.0) -3.8 (1.1) -4.6 (1.2) -5.2 (1.2) -5.5 (1.3) -5.8 (1.4) -6.0 (1.5) -6.0 (1.5)
FMNIST (s) Seed Acc 72.7 (1.0) 72.8 (1.0) 72.1 (1.0) 70.9 (1.0) 70.2 (1.0) 69.6 (1.1) 69.4 (1.1) 69.3 (1.2) 69.3 (1.2) 69.3 (1.3) 69.4 (1.3)
Spar 0.2 (0.1) 26.1 (1.2) 49.5 (1.3) 57.4 (1.2) 62.4 (1.1) 66.0 (1.0) 69.6 (1.0) 72.0 (1.0) 73.7 (1.0) 75.0 (1.0) 76.1 (1.1)
GGap 1.7 (0.3) 1.4 (0.3) 0.7 (0.4) -1.6 (0.5) -3.5 (0.7) -3.3 (0.7) -2.6 (0.8) -2.0 (0.8) -1.3 (0.8) -1.0 (0.9) -1.0 (0.9)
FMNIST (s) Fixed Acc 64.2 (27.3) 58.4 (27.1) 52.3 (27.6) 48.7 (27.0) 44.9 (26.2) 41.1 (25.8) 35.8 (25.1) 31.8 (24.0) 29.2 (23.2) 27.3 (22.4) 25.8 (21.7)
Spar 1.3 (7.3) 29.5 (15.3) 49.9 (19.3) 61.3 (20.7) 68.5 (20.5) 72.7 (20.4) 76.9 (20.5) 79.9 (20.9) 82.5 (20.9) 84.2 (21.1) 85.7 (21.0)
GGap 1.0 (0.8) -2.0 (7.3) -2.8 (8.4) -2.1 (9.1) -0.9 (9.7) 0.5 (10.6) 1.8 (11.5) 2.0 (11.9) 1.8 (11.9) 1.8 (11.9) 1.7 (11.3)
FMNIST (s) Rand Acc 64.1 (27.4) 58.8 (26.9) 52.8 (27.3) 49.1 (26.7) 45.5 (26.1) 41.7 (25.7) 36.1 (24.9) 32.1 (23.9) 29.5 (23.1) 27.6 (22.3) 25.9 (21.6)
Spar 1.6 (8.8) 29.4 (15.2) 49.8 (19.2) 61.5 (20.3) 68.7 (20.1) 73.1 (19.9) 77.3 (19.9) 80.4 (20.2) 82.9 (20.2) 84.9 (19.8) 86.2 (20.0)
GGap 1.0 (0.8) -2.2 (7.0) -3.1 (8.3) -2.3 (9.3) -1.2 (9.8) -0.0 (10.6) 1.6 (11.2) 1.8 (11.7) 1.6 (11.9) 1.6 (11.7) 1.7 (11.3)
CIFAR-10 (l) Seed Acc 61.5 (0.7) 61.7 (0.7) 60.1 (0.7) 56.7 (0.8) 51.5 (1.1) 44.4 (1.8) 30.7 (3.3) 18.2 (3.8) 12.6 (2.8) 10.7 (1.6) 10.2 (0.9)
Spar 4.0 (1.4) 18.8 (3.2) 45.2 (2.1) 61.2 (1.4) 72.5 (1.0) 80.5 (0.7) 88.4 (0.5) 93.2 (0.4) 96.0 (0.3) 97.7 (0.3) 98.7 (0.2)
GGap 2.0 (0.3) 1.2 (0.3) 1.4 (0.4) 1.6 (0.4) 1.7 (0.5) 2.0 (0.7) 2.9 (1.1) 2.4 (1.3) 0.8 (0.9) 0.3 (0.6) 0.1 (0.4)
CIFAR (l) Fixed Acc 38.8 (21.9) 19.6 (14.1) 14.0 (10.1) 13.3 (8.6) 12.5 (7.1) 12.0 (5.9) 11.4 (4.2) 10.9 (2.9) 10.7 (2.3) 10.5 (1.7) 10.2 (1.1)
Spar 22.6 (33.0) 74.1 (19.3) 84.7 (19.6) 84.5 (21.7) 83.1 (25.4) 82.5 (27.7) 82.1 (29.6) 82.3 (30.5) 82.6 (30.5) 83.0 (30.5) 83.3 (30.6)
GGap 1.4 (2.5) 6.3 (7.5) 0.3 (1.8) 0.1 (1.9) 0.1 (1.4) 0.1 (1.3) 0.3 (1.4) 0.2 (1.4) 0.2 (1.2) 0.2 (1.1) 0.2 (1.1)
CIFAR (l) Rand Acc 39.4 (21.9) 19.5 (14.1) 14.0 (10.2) 13.3 (8.7) 12.6 (7.4) 12.1 (6.1) 11.4 (4.2) 10.9 (3.0) 10.7 (2.4) 10.5 (1.8) 10.3 (1.3)
Spar 23.3 (33.7) 74.7 (17.9) 85.3 (17.6) 85.4 (19.6) 84.1 (23.5) 83.5 (26.1) 83.4 (27.8) 83.7 (28.2) 84.4 (28.0) 84.6 (28.2) 84.8 (28.4)
GGap 1.4 (2.5) 6.5 (7.4) 0.4 (1.8) 0.0 (1.9) 0.2 (1.5) 0.2 (1.3) 0.2 (1.5) 0.3 (1.4) 0.2 (1.4) 0.2 (1.2) 0.1 (0.9)
CIFAR-10 (s) Seed Acc 48.7 (1.4) 48.9 (1.4) 48.1 (1.4) 46.0 (1.5) 42.7 (1.6) 38.0 (1.7) 29.6 (2.3) 22.4 (2.7) 17.0 (3.6) 13.1 (3.4) 11.2 (2.4)
Spar 1.4 (0.4) 12.4 (2.6) 34.1 (3.2) 49.5 (3.0) 61.9 (2.6) 71.8 (2.1) 82.6 (1.5) 89.6 (1.1) 93.9 (0.8) 96.5 (0.8) 98.2 (0.7)
GGap 0.7 (0.4) 0.1 (0.4) 0.3 (0.4) 0.4 (0.4) 0.6 (0.5) 0.9 (0.5) 1.1 (0.7) 1.2 (0.8) 1.3 (1.1) 0.8 (1.1) 0.3 (0.8)
USPS (s) Seed Acc 87.0 (1.7) 87.4 (1.6) 87.0 (1.5) 86.0 (1.4) 84.8 (1.4) 83.4 (1.4) 81.7 (1.6) 79.7 (2.3) 77.2 (3.7) 74.5 (4.8) 71.4 (6.1)
Spar 0.6 (0.2) 28.5 (2.2) 55.9 (1.8) 71.2 (1.2) 79.4 (1.0) 84.9 (0.8) 89.7 (0.5) 92.3 (0.5) 93.5 (0.5) 94.2 (0.6) 94.5 (0.6)
GGap 5.1 (0.6) 4.8 (0.6) 4.6 (0.5) 4.5 (0.6) 4.6 (0.6) 4.6 (0.8) 4.7 (1.2) 4.9 (2.0) 5.5 (3.4) 5.8 (4.5) 6.0 (5.7)
STL-10 (l) Seed Acc 47.4 (0.9) 47.7 (0.9) 47.7 (0.9) 47.1 (0.9) 45.1 (1.0) 40.2 (1.1) 28.0 (2.4) 19.4 (2.7) 16.3 (3.2) 14.8 (3.5) 13.9 (3.6)
Spar 0.8 (0.1) 12.2 (0.5) 31.7 (0.8) 49.0 (1.0) 64.5 (1.2) 77.5 (1.1) 90.3 (0.6) 95.4 (0.3) 97.3 (0.2) 98.5 (0.2) 99.3 (0.2)
GGap 15.3 (1.0) 13.4 (1.0) 12.2 (1.0) 9.4 (0.9) 6.3 (0.9) 4.2 (0.8) 2.8 (1.1) 1.0 (1.0) 0.4 (0.9) 0.2 (0.7) 0.1 (0.6)
STL-10 (s) Seed Acc 39.0 (1.0) 39.1 (1.0) 39.4 (1.0) 39.3 (1.0) 39.1 (0.9) 38.5 (1.0) 35.9 (1.2) 31.4 (1.6) 26.4 (2.0) 22.0 (2.6) 18.9 (2.8)
Spar 0.7 (0.2) 6.7 (0.6) 20.1 (1.0) 31.7 (1.3) 42.7 (1.4) 53.0 (1.5) 66.5 (1.3) 77.1 (1.2) 85.3 (1.1) 91.4 (0.9) 95.3 (0.6)
GGap 5.0 (0.8) 4.5 (0.8) 4.2 (0.8) 3.9 (0.7) 3.3 (0.7) 2.6 (0.7) 1.8 (0.7) 1.4 (0.8) 1.1 (0.9) 1.0 (1.0) 0.8 (0.9)
Table 6. Variational Dropout Results. The results are reported as mean (std) in %. Metric denotes the performance metric: Acc- Accuracy,
Spar - Sparsity and GGAP - Generalization Gap. The table contains the results of all Seed, Fixed Seed and Random Seed Model Zoos at
epochs 0, 2, 4, 6, 8, 10, 13, 16, 19, 22, 25.

--- PAGE 12 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
D. Sparsity per Layer VD
Model Zoo Epoch Conv 1 Conv 2 Conv 3 FC 1 FC 2 Accuracy GGAP
MNIST (s) Seed 0 0.2 (0.2) 0.2 (0.2) 0.4 (0.2) 0.2 (0.2) 0.1 (0.1) 91.1 (0.9) 0.6 (0.3)
5 41.4 (10.6) 55.2 (7.6) 40.7 (2.3) 67.6 (2.9) 37.7 (4.8) 90.1 (0.9) -1.8 (0.4)
10 48.9 (10.6) 60.7 (7.4) 62.8 (2.3) 84.1 (2.5) 58.9 (4.7) 88.4 (1.0) -5.4 (0.7)
15 52.7 (10.0) 65.1 (7.1) 74.2 (2.3) 90.6 (1.8) 70.2 (3.2) 87.8 (1.1) -4.8 (0.7)
20 55.7 (9.4) 68.8 (6.8) 80.0 (2.1) 93.3 (1.4) 75.2 (2.9) 87.6 (1.2) -4.2 (0.7)
25 58.4 (8.8) 71.9 (6.5) 83.1 (2.1) 94.7 (1.3) 77.5 (3.0) 87.6 (1.2) -3.9 (0.7)
SVHN (s) Seed 0 0.1 (0.1) 0.1 (0.1) 1.2 (0.3) 0.3 (0.2) 0.1 (0.1) 71.1 (8.0) 2.8 (0.7)
5 20.0 (2.9) 34.6 (5.8) 19.9 (1.1) 58.1 (2.9) 37.0 (4.0) 66.5 (7.4) -3.4 (1.0)
10 27.4 (3.8) 45.2 (6.8) 39.6 (1.5) 82.8 (2.0) 62.0 (3.7) 63.7 (7.3) -4.6 (1.2)
15 31.7 (4.1) 51.3 (7.2) 53.3 (2.1) 89.4 (1.5) 72.3 (2.8) 62.2 (7.2) -5.5 (1.3)
20 35.1 (4.5) 55.2 (7.2) 62.1 (1.7) 92.6 (1.1) 77.7 (3.4) 60.1 (6.9) -5.8 (1.4)
25 37.1 (4.7) 58.0 (7.1) 69.3 (1.9) 94.4 (1.0) 80.8 (3.2) 57.9 (6.6) -6.0 (1.5)
FMNIST (s) Seed 0 0.2 (0.2) 0.2 (0.2) 0.3 (0.2) 0.2 (0.2) 0.1 (0.1) 72.7 (1.0) 1.7 (0.3)
5 44.2 (10.6) 51.7 (7.5) 47.5 (4.9) 67.8 (5.5) 32.1 (6.0) 71.6 (1.0) -0.3 (0.4)
10 54.8 (12.0) 56.1 (7.2) 68.3 (4.7) 87.0 (4.1) 59.8 (5.4) 69.6 (1.1) -3.3 (0.7)
15 57.3 (12.1) 59.8 (6.8) 75.6 (4.1) 92.9 (2.9) 74.5 (5.4) 69.2 (1.2) -2.0 (0.8)
20 59.3 (12.1) 62.9 (6.5) 79.5 (3.6) 94.8 (2.3) 79.5 (5.2) 69.2 (1.3) -1.2 (0.9)
25 61.1 (12.1) 65.5 (6.2) 81.7 (3.4) 95.7 (2.0) 81.5 (5.2) 69.4 (1.3) -1.0 (0.9)
CIFAR-10 (l) Seed 0 0.8 (0.3) 3.0 (0.6) 2.9 (0.6) 10.9 (2.8) 11.2 (1.8) 61.5 (0.7) 2.0 (0.3)
5 15.8 (2.7) 62.0 (1.9) 55.1 (4.1) 40.2 (5.3) 30.6 (9.3) 58.6 (0.8) 1.6 (0.4)
10 35.3 (4.4) 88.0 (1.3) 84.8 (3.5) 63.3 (7.3) 48.1 (15.0) 44.4 (1.8) 2.0 (0.7)
15 55.9 (4.6) 96.5 (0.8) 96.1 (1.7) 81.4 (5.3) 64.2 (17.8) 21.7 (3.9) 2.9 (1.4)
20 74.5 (3.6) 99.1 (0.4) 99.3 (0.6) 93.1 (2.6) 78.0 (16.8) 11.7 (2.4) 0.6 (0.9)
25 88.5 (2.5) 99.8 (0.2) 99.9 (0.2) 98.2 (0.9) 87.6 (12.5) 10.2 (0.9) 0.1 (0.4)
CIFAR-10 (s) Seed 0 1.0 (0.4) 1.8 (0.5) 0.5 (0.2) 1.6 (0.4) 0.7 (0.3) 48.7 (1.4) 0.7 (0.4)
5 32.6 (5.5) 53.7 (5.0) 15.1 (1.2) 40.6 (3.9) 21.4 (2.9) 47.2 (1.4) 0.4 (0.4)
10 64.9 (6.1) 84.7 (4.4) 30.0 (1.6) 69.8 (5.0) 42.2 (5.1) 38.0 (1.7) 0.9 (0.5)
15 87.1 (3.9) 95.3 (2.2) 48.0 (1.9) 87.5 (3.7) 63.0 (6.2) 24.6 (2.5) 1.2 (0.8)
20 96.1 (2.1) 98.4 (0.9) 68.1 (2.0) 95.7 (1.9) 80.4 (5.8) 15.5 (3.7) 1.2 (1.0)
25 98.9 (0.9) 99.5 (0.4) 84.5 (1.8) 98.9 (0.7) 92.1 (4.0) 11.2 (2.4) 0.3 (0.8)
USPS (s) Seed 0 0.5 (0.2) 0.9 (0.3) 0.4 (0.2) 0.5 (0.2) 0.1 (0.1) 87.0 (1.7) 5.1 (0.6)
5 55.4 (13.0) 80.8 (2.6) 30.6 (4.2) 58.2 (4.8) 19.2 (4.4) 86.5 (1.4) 4.6 (0.5)
10 79.6 (7.9) 94.6 (1.7) 62.2 (6.1) 85.4 (3.9) 41.1 (6.3) 83.4 (1.4) 4.6 (0.8)
15 89.4 (4.8) 97.4 (1.1) 79.5 (5.2) 92.9 (2.6) 59.9 (5.6) 80.4 (2.0) 4.8 (1.6)
20 92.9 (3.5) 97.5 (0.9) 86.1 (3.9) 95.3 (1.9) 71.6 (4.4) 76.5 (3.8) 5.4 (3.5)
25 93.5 (3.3) 96.9 (1.0) 88.8 (3.2) 96.0 (1.5) 78.3 (3.7) 71.4 (6.1) 6.0 (5.7)
STL-10 (l) Seed 0 0.3 (0.2) 0.8 (0.3) 1.1 (0.3) 0.5 (0.3) 0.3 (0.2) 47.4 (0.9) 15.3 (1.0)
5 9.7 (1.0) 37.9 (1.5) 49.8 (2.5) 33.4 (5.5) 22.9 (10.5) 47.6 (0.9) 10.8 (0.9)
10 22.6 (1.6) 77.4 (1.3) 88.5 (2.2) 67.6 (7.0) 47.7 (20.5) 40.2 (1.1) 4.2 (0.8)
15 41.9 (2.2) 98.0 (0.5) 99.0 (0.4) 91.2 (2.7) 68.7 (24.7) 21.3 (2.7) 1.5 (1.0)
20 69.8 (2.4) 99.8 (0.1) 99.8 (0.1) 98.5 (0.6) 82.0 (19.5) 15.7 (3.3) 0.3 (0.9)
25 93.6 (1.3) 99.9 (0.1) 99.9 (0.1) 99.6 (0.2) 91.1 (11.5) 13.9 (3.6) 0.1 (0.6)
STL-10 (s) Seed 0 0.6 (0.2) 1.0 (0.3) 0.3 (0.2) 0.4 (0.2) 0.3 (0.2) 39.0 (1.0) 5.0 (0.8)
5 13.0 (1.1) 34.5 (1.4) 11.0 (0.9) 27.9 (1.6) 13.4 (1.6) 39.4 (1.0) 4.1 (0.7)
10 29.0 (1.7) 68.6 (1.6) 23.9 (1.5) 57.7 (2.2) 29.1 (3.1) 38.5 (1.0) 2.6 (0.7)
15 48.3 (2.1) 90.0 (1.5) 39.3 (1.6) 81.0 (2.6) 45.9 (5.5) 33.1 (1.4) 1.4 (0.8)
20 74.0 (2.3) 97.5 (0.9) 58.5 (1.6) 93.2 (2.0) 63.4 (8.1) 24.8 (2.2) 1.1 (1.0)
25 93.5 (1.4) 99.2 (0.5) 77.3 (1.4) 97.5 (1.2) 78.4 (8.9) 18.9 (2.8) 0.8 (0.9)
Table 7. Sparsity per layer at epoch 0, 5, 10, 15, 20 and 25 for all Seed model zoos. The mean (std) are reported in %. Conv is the
abbreviation for convolutional layer, FCis the abbreviation for fully-connected layer.

--- PAGE 13 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
E. Sparsity per Layer MP
Model Zoo Spars Conv1 Conv2 Conv3 FC1 FC2 Accuracy GGAP
MNIST (s) Seed 50 42.5 (11.5) 51.1 (7.6) 76.4 (3.0) 53.6 (2.3) 25.5 (2.3) 93.3 (0.7) 0.1 (0.2)
70 62.7 (14.5) 72.1 (8.5) 91.3 (1.8) 73.8 (2.3) 40.7 (3.1) 91.1 (1.8) -0.4 (0.3)
90 86.6 (12.1) 92.2 (5.3) 98.6 (0.5) 92.4 (1.4) 67.1 (4.3) 46.6 (25.1) -0.5 (1.0)
MNIST (s) Fixed 50 37.8 (4.0) 63.2 (3.5) 35.1 (2.2) 40.6 (3.4) 23.3 (3.3) 73.7 (34.9) -6.1 (8.5)
70 58.0 (4.7) 83.7 (2.6) 51.1 (2.6) 62.2 (4.0) 35.9 (3.8) 69.0 (35.1) -7.1 (10.2)
90 87.2 (3.1) 96.6 (1.2) 75.9 (2.3) 90.2 (2.7) 58.5 (4.7) 33.7 (29.2) -2.4 (6.3)
MNIST (s) Random 50 37.9 (2.7) 63.0 (2.6) 35.4 (1.0) 40.9 (2.3) 24.1 (2.8) 73.3 (35.1) -5.9 (8.4)
70 58.1 (2.8) 83.7 (1.7) 51.5 (1.2) 62.4 (3.0) 36.1 (3.2) 69.5 (35.1) -7.0 (10.1)
90 87.4 (1.7) 96.5 (0.6) 76.1 (1.3) 90.4 (1.9) 58.9 (4.1) 33.7 (29.2) -2.2 (6.2)
SVHN (s) Seed 50 50.0 (0.0) 50.0 (0.0) 50.0 (0.0) 50.0 (0.0) 50.0 (0.0) 72.6 (7.9) 2.7 (0.7)
70 70.0 (0.0) 70.0 (0.0) 70.0 (0.0) 70.0 (0.0) 70.0 (0.0) 67.1 (7.2) 2.3 (0.8)
90 90.0 (0.0) 90.0 (0.0) 90.0 (0.0) 90.0 (0.0) 90.0 (0.0) 34.0 (5.7) -0.1 (1.0)
USPS (s) Seed 50 41.4 (9.2) 64.4 (3.8) 29.0 (4.4) 42.0 (4.0) 11.2 (2.6) 90.1 (1.0) 5.0 (0.6)
70 62.9 (11.1) 86.5 (2.8) 45.6 (6.3) 62.0 (5.0) 18.4 (3.9) 89.5 (1.0) 4.8 (0.6)
90 94.0 (4.4) 99.5 (0.3) 77.6 (7.0) 89.2 (3.9) 37.4 (5.7) 73.8 (17.3) 3.3 (1.6)
FMNIST (s) Seed 50 51.8 (11.6) 50.5 (7.8) 67.3 (4.6) 52.3 (5.0) 28.5 (4.1) 74.4 (1.2) 1.9 (0.5)
70 72.3 (13.2) 71.1 (8.6) 85.0 (3.3) 72.0 (5.2) 46.6 (6.5) 73.6 (1.3) 1.5 (0.5)
90 91.2 (8.5) 91.3 (5.1) 97.1 (1.1) 91.0 (3.2) 73.7 (8.3) 57.1 (14.4) 0.8 (1.0)
CIFAR-10 (s) Seed 50 40.2 (5.8) 59.2 (10.1) 63.0 (5.2) 46.2 (12.6) 47.9 (4.8) 49.0 (1.3) 0.4 (0.4)
70 61.2 (6.2) 80.2 (9.6) 84.0 (4.4) 65.5 (15.5) 68.1 (5.4) 47.3 (1.2) 0.3 (0.4)
90 89.0 (3.5) 96.5 (3.7) 98.0 (1.3) 86.5 (14.6) 90.8 (3.4) 34.5 (2.5) -0.3 (0.4)
CIFAR-10 (l) Seed 50 13.8 (2.2) 55.0 (2.0) 53.0 (4.1) 38.2 (1.8) 20.0 (1.9) 61.8 (0.7) 1.7 (0.3)
70 23.0 (3.3) 76.5 (1.8) 74.9 (4.4) 51.6 (2.1) 25.8 (2.6) 60.8 (0.6) 1.5 (0.3)
90 43.4 (4.7) 95.0 (0.9) 95.7 (2.0) 75.3 (2.2) 38.6 (4.2) 51.4 (1.0) 0.4 (0.4)
STL-10 (s) Seed 50 43.5 (1.7) 66.9 (10.6) 71.6 (1.5) 36.1 (14.9) 31.5 (1.6) 41.4 (0.9) 5.3 (0.8)
70 66.5 (1.6) 89.6 (10.4) 94.2 (1.2) 52.9 (17.5) 48.1 (1.8) 40.6 (0.8) 2.6 (0.8)
90 99.2 (0.3) 99.9 (0.3) 99.9 (0.1) 80.1 (10.3) 79.5 (2.2) 10.9 (3.5) 0.1 (0.3)
STL-10 (l) Seed 50 19.7 (1.3) 47.0 (1.6) 63.9 (1.6) 28.7 (1.4) 12.4 (1.8) 47.4 (0.9) 20.8 (1.2)
70 28.8 (1.5) 68.2 (1.5) 86.3 (1.4) 41.7 (1.6) 18.1 (2.6) 48.4 (0.9) 13.6 (0.9)
90 44.4 (1.7) 95.3 (0.7) 99.2 (0.4) 63.2 (1.5) 28.1 (3.8) 42.8 (1.0) 3.5 (0.8)
Table 8. Overview of the sparsity per layer per model zoo. The values mean (std) are reported in % for the sparsity ratios 50%, 70% and
90%. Conv is the abbreviation for convolutional layer. FCis the abbreviation for fully-connected layer.
F. Model Agreement
Model Zoo Acc MP Spar MP Aggr MP Acc VD Spar VD Aggr VD
MNIST (s) Seed 83.7 (13.5) 80.0 (0.0) 82.1 (13.0) 87.6 (1.2) 78.0 (1.1) 83.4 (1.4)
SVHN (s) Seed 70.7 (7.7) 60.0 (0.0) 74.9 (2.5) 62.2 (7.2) 62.8 (2.9) 57.5 (6.0)
FMNIST (s) Seed 73.6 (1.3) 70.0 (0.0) 79.7 (1.9) 69.3 (1.2) 72.0 (1.0) 76.2 (2.1)
CIFAR-10 (s) Seed 47.3 (1.2) 70.0 (0.0) 78.4 (1.2) 40.5 (1.5) 67.1 (2.8) 61.0 (2.7)
USPS (s) Seed 73.8 (17.3) 90.0 (0.0) 74.3 (17.3) 82.3 (1.5) 88.5 (0.6) 86.6 (1.1)
STL-10 (s) Seed 40.6 (0.8) 70.0 (0.0) 55.9 (2.8) 35.9 (1.2) 66.5 (1.3) 54.4 (2.8)
Table 9. Agreement overview between original and twin Seed model zoos. The values mean (std) are reported in %. Higher values indicate
higher agreement. Aggr denotes Agreement. Accdenotes Accuracy. Spar denotes Sparsity. MPdenotes Magnitude Pruning. VDdenotes
Variational Dropout.

--- PAGE 14 ---
Sparsiﬁed Model Zoo Twins: Investigating Populations of Sparsiﬁed Neural Network Models
G. Auto-Encoder Model
Figure 6. Overview of the adapted multi-head attention-based autoencoder based on (Sch ¨urholt et al., 2022a). Weights and biases are put
into vector form. This sequence is converted to token embeddings, a position encoding is added and a CLS token is appended to the
sequence. This sequence is passed through several layers of multi-head self-attention before the CLS token is linearly compressed by the
encoder. The bottleneck zis linearly decompressed by the decoder to two sequences of token embeddings. One sequence for wand one
sequence for . A position and type encoding is added, to tell the decoder if it reconstructs wor. The two sequences are merged and
and passed through another stack of multi-head self-attention. This enables the transformer to beneﬁt from mutual information as it takes
into account both sequences at once. In the end, the sequences are split again and mapped back to the original parameters.
H. Self-Supervised Losses
The implemented loss is composed of a contrastive part and a reconstruction part. The contrastive loss implemented is the
regular NTXent loss of (Chen et al., 2020). In this case each sample is randomly augmented to obtain the views iandj.
Using cosine similarity, the loss can be written as
LC=X
(i;j) logexp(sim( zi; zj))=TP2MB
k=1 Ik 6=iexp(sim( zi; zj))=T; (3)
whereMBis a batch of model parameter, Ik 6=i= 1 ifk6=iand 0 otherwise and Tis the temperature parameter (Sch ¨urholt
et al., 2021a; Chen et al., 2020).
For the reconstruction of wthe layer-wise reconstruction loss of (Sch ¨urholt et al., 2022a) is utilized
Lw
MSE=1
MNMX
i=1LX
l=1^ w(l)
i l
l w(l)
i l
l2
2=1
MNMX
i=1LX
l=1jj^ w(l)
i w(l)
ijj2
2
2
l; (4)
where^w(l)
iis the reconstruction of the parameters in layer lof modeliandlandlare the standard deviation and mean of
the weights in layer l.
The novel normalization of focuses more on the values around the pruning threshold of 3 and less around the values
around the zero point. Accordingly the reconstruction loss for is deﬁned as
L
MSE =1
MMX
i=1tanh ^i t
r
 tanh i t
r2
2; (5)
wheretrefers to the threshold and rto the selected range of interest.
Putting the individual parts together gives the following loss:
LEcD=LC+ (1 )(Lw
MSE+L
MSE ); (6)
whereis the weighting between the contrastive and reconstruction part.
