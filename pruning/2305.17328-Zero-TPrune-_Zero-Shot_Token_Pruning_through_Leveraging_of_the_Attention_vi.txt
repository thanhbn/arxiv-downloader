Zero-TPrune: Cắt tỉa Token Zero-Shot thông qua Khai thác Đồ thị Chú ý trong các Transformer Đã được Huấn luyện Trước

Hongjie Wang, Bhishma Dedhia, Niraj K. Jha
Đại học Princeton
Princeton, NJ 08540, USA
{hongjiewang, bdedhia, jha }@princeton.edu

Tóm tắt
Việc triển khai các mô hình Transformer trên các thiết bị biên ngày càng trở nên thách thức do chi phí suy luận tăng theo cấp số nhân, tỷ lệ thuận với bình phương số lượng token trong chuỗi đầu vào. Cắt tỉa token là một giải pháp mới nổi để giải quyết thách thức này do tính dễ triển khai trên các backbone Transformer khác nhau. Tuy nhiên, hầu hết các phương pháp cắt tỉa token đều yêu cầu fine-tuning tốn kém về tính toán, điều này không mong muốn trong nhiều trường hợp triển khai biên. Trong nghiên cứu này, chúng tôi đề xuất Zero-TPrune, phương pháp zero-shot đầu tiên xem xét cả tầm quan trọng và độ tương tự của các token khi thực hiện cắt tỉa token. Nó tận dụng đồ thị chú ý của các mô hình Transformer đã được huấn luyện trước để tạo ra phân phối tầm quan trọng cho các token thông qua thuật toán Weighted Page Rank (WPR) đề xuất của chúng tôi. Phân phối này tiếp tục hướng dẫn việc phân chia token để cắt tỉa hiệu quả dựa trên độ tương tự. Do loại bỏ chi phí fine-tuning, Zero-TPrune có thể cắt tỉa các mô hình lớn với chi phí tính toán không đáng kể, chuyển đổi giữa các cấu hình cắt tỉa khác nhau mà không mất chi phí tính toán, và thực hiện điều chỉnh siêu tham số một cách hiệu quả. Chúng tôi đánh giá hiệu suất của Zero-TPrune trên các tác vụ thị giác bằng cách áp dụng nó vào các backbone vision Transformer khác nhau và kiểm tra chúng trên ImageNet. Không cần fine-tuning, Zero-TPrune giảm chi phí FLOPs của DeiT-S 34.7% và cải thiện thông lượng 45.3% chỉ với 0.4% mất độ chính xác. So với các phương pháp cắt tỉa tiên tiến yêu cầu fine-tuning, Zero-TPrune không chỉ loại bỏ nhu cầu fine-tuning sau cắt tỉa mà còn làm như vậy chỉ với 0.1% mất độ chính xác. So với các phương pháp cắt tỉa không cần fine-tuning tiên tiến, Zero-TPrune giảm mất độ chính xác tới 49% với ngân sách FLOPs tương tự.

1. Giới thiệu
Kiến trúc Transformer [37] đã nổi lên như một công cụ chủ đạo của các mô hình học máy đương đại, cho thấy khả năng tổng quát hóa ấn tượng qua nhiều tác vụ bao gồm Thị giác Máy tính (CV) [34], xử lý ngôn ngữ tự nhiên (NLP) [10], robot [31], và trò chơi [26]. Trung tâm của kiến trúc này là cơ chế self-attention đa đầu động tập hợp các token được xử lý song song, tạo ra một khung tính toán mục đích chung rất hiệu quả. Những tác động đặc biệt rõ ràng trong trường hợp CV nơi khả năng của Transformer trong việc tiếp thu các trừu tượng phong phú từ dữ liệu quy mô lớn tạo điều kiện chuyển giao mạnh mẽ cho các tác vụ downstream, vượt trội hơn các Mạng nơ-ron tích chập (CNN) tiên tiến [12].

Các nghiên cứu về quy luật tỷ lệ thực nghiệm cho Vision Transformers (ViTs) [41] chỉ ra khả năng cải thiện hiệu suất mô hình với dung lượng mô hình; các mô hình gần đây thực sự đã được mở rộng lên hàng tỷ tham số. Trong khi việc mở rộng mô hình mang lại lời hứa về khả năng tổng quát hóa đáng chú ý, nó đặt ra những trở ngại nghiêm trọng cho việc triển khai các kiến trúc như vậy trên các thiết bị có hạn chế tính toán như biên và thực hiện các khối lượng công việc suy luận thời gian thực dưới năng lượng và bộ nhớ hạn chế. Làm thế nào để giảm độ phức tạp tính toán của lượt truyền xuôi trong khi vẫn duy trì sự phong phú của các biểu diễn đã học? Để đạt được mục tiêu này, Cắt tỉa Token mở ra một hướng đi đầy triển vọng. Rút ra một phép tương tự đơn giản với hệ thống thị giác con người, khi cố gắng nhận dạng một loài chim kỳ lạ đậu trên cửa sổ vào một buổi chiều điền viên, chúng ta có xu hướng cắt tỉa những chi tiết thị giác không quan trọng như cốc trà nóng bốc khói nằm gần đó, những người đi bộ tản bộ trên lối đi hoặc tán lá được ánh nắng chiếu sáng ở phía sau. Các đầu chú ý tạo ra độ phức tạp tính toán bậc hai so với độ dài chuỗi đầu vào. Do đó, việc cắt tỉa các token không quan trọng có thể dẫn đến tăng tốc đáng kể, đặc biệt trong trường hợp các chuỗi dài hơn. Vì cắt tỉa token chỉ cắt tỉa thông tin đi qua các lớp tuần tự của Transformer và không đòi hỏi sửa đổi kiến trúc backbone, nó có thể được triển khai rộng rãi trên hầu hết các backbone Transformer và bất kỳ phần cứng tính toán nào cũng có thể khai thác đầy đủ độ thưa thớt kết quả. Tuy nhiên, hầu hết các phương pháp cắt tỉa token hiện có dựa vào các mô-đun chấm điểm token phải được huấn luyện cùng với backbone, yêu cầu việc huấn luyện lại hoặc fine-tuning tốn kém về tính toán để triển khai. Điều này không thực tế cho các ứng dụng và người dùng biên, do sự khan hiếm tài nguyên tính toán. Ví dụ, phương pháp cắt tỉa token tiên tiến DynamicViT [30] yêu cầu 150 giờ fine-tuning trên GPU NVIDIA A100 để cắt tỉa mô hình DeiT-S [36]. Hơn nữa, tài nguyên bộ nhớ và tính toán có sẵn có thể khác nhau rất nhiều giữa các thiết bị biên; chúng cũng có thể có sự biến động lớn về yêu cầu thông lượng. Các phương pháp cắt tỉa yêu cầu fine-tuning cần huấn luyện mô hình nhiều lần dưới các cấu hình cắt tỉa khác nhau do các ràng buộc phần cứng áp đặt, như được hiển thị trong Hình 1, làm cho quá trình cắt tỉa trở nên đắt đỏ hơn. Ngoài ra, các phương pháp này không thực tế để cắt tỉa các mô hình rất lớn do chi phí tính toán cao của việc huấn luyện sau cắt tỉa. Ví dụ, cần hàng nghìn giờ GPU A100 để áp dụng DynamicViT [30] cho các mô hình DeiT-B và DeiT-L [36].

Trong nghiên cứu này, chúng tôi đề xuất một phương pháp cắt tỉa token zero-shot không cần huấn luyện có tên Zero-TPrune. Làm thế nào để cắt tỉa token mà không cần fine-tuning? Sự chú ý mềm giữa các token tạo ra một đồ thị có hướng với các token là các nút và chú ý là các cạnh. Độ mạnh của cạnh biểu thị giá trị chú ý. Chúng tôi đặt giả thuyết và sau đó chứng minh thông qua một tập hợp thí nghiệm nghiêm ngặt và toàn diện rằng đồ thị chú ý là một nguồn thông tin phong phú để suy luận các token quan trọng và ngược lại, các token có thể dễ dàng bị cắt tỉa. Làm thế nào để xác định các token quan trọng từ đồ thị chú ý? Trọng số của các cạnh có hướng trên đồ thị chú ý có thể được hiểu là khối lượng định tuyến thông tin giữa các nút. Sử dụng giả định cơ bản rằng các token quan trọng khác chú ý đến các token quan trọng, chúng tôi lặp đi lặp lại gán tầm quan trọng tương đối cho các token. Các phương pháp xếp hạng như vậy [4] đã được sử dụng phổ biến bởi các công cụ tìm kiếm để tổ chức các trang web trên Internet. Có thể khai thác thêm sự dư thừa giữa các token không? Các thí nghiệm của chúng tôi cho thấy các token thường học các trừu tượng tương tự và do đó, các bản sao của cùng một đặc trưng có thể được cắt tỉa mà không mất thông tin. Chúng tôi bổ sung xếp hạng tầm quan trọng với cắt tỉa dựa trên độ tương tự để tính đến các token tương tự. Mặc dù Zero-TPrune có thể được áp dụng cho bất kỳ tác vụ dựa trên Transformer nào, chúng tôi tập trung vào các tác vụ thị giác để đánh giá hiệu suất của nó trong bài báo này.

Các đóng góp chính của nghiên cứu này có thể được tóm tắt như sau. (1) Chúng tôi trình bày Zero-TPrune, một phương pháp cắt tỉa token zero-shot khai thác hiệu quả khả năng nhận dạng đặc trưng (xem xét ma trận chú ý như ma trận kề của một đồ thị có hướng) của các Transformer đã được huấn luyện trước. Nó khai thác cả tầm quan trọng và độ tương tự của các token để thực hiện cắt tỉa. (2) Chúng tôi sử dụng tín hiệu đồ thị để biểu diễn phân phối điểm tầm quan trọng trên các token và đề xuất thuật toán Weighted Page Rank (WPR) để suy luận các token không quan trọng trong quá trình gán tầm quan trọng lặp. Lược đồ lặp này giảm nhiễu từ các token không quan trọng trong quá trình gán. (3) Được hướng dẫn bởi phân phối tầm quan trọng, chúng tôi phân chia các token thành hai nhóm và thực hiện cắt tỉa dựa trên độ tương tự. Việc phân chia phụ thuộc đầu vào kiểm soát phân phối tầm quan trọng của các token được cắt tỉa bởi metric độ tương tự. (4) Chúng tôi áp dụng Zero-TPrune và các phương pháp baseline cho các backbone Transformer khác nhau và đánh giá hiệu suất của chúng trên ImageNet [9]. Các backbone được sử dụng bao gồm DeiT [36], LV-ViT [18], AugReg [33], v.v. So với các phương pháp cắt tỉa Transformer tiên tiến yêu cầu fine-tuning, Zero-TPrune loại bỏ nhu cầu fine-tuning sau khi cắt tỉa DeiT-S chỉ với khoảng 0.1% giảm độ chính xác trong khi đạt được cùng mức tiết kiệm FLOPs. Hơn nữa, Zero-TPrune vượt trội hơn các phương pháp không cần fine-tuning tiên tiến về cả độ chính xác và thông lượng. Zero-TPrune giảm mất độ chính xác 33% trên DeiT-S khi so sánh với các phương pháp không cần fine-tuning tiên tiến. Về thông lượng, Zero-TPrune cung cấp tăng tốc 45.3% ngay lập tức với chi phí chỉ 0.4% độ chính xác.

2. Các Công trình Liên quan
Trong vài năm đầu sau khi mô hình Transformer được đề xuất năm 2017, nó chủ yếu được sử dụng trong lĩnh vực NLP [10]. ViT [12] là công trình đầu tiên áp dụng trực tiếp kiến trúc Transformer chỉ có encoder cho các patch hình ảnh không chồng lấp trong tác vụ phân loại hình ảnh mà không sử dụng bất kỳ phép toán tích chập nào. So với các CNN tiên tiến, ViT có thể đạt được hiệu suất tốt hơn thông qua huấn luyện trước quy mô lớn. DeiT [36] là một Transformer khác không có tích chập được huấn luyện chỉ trên ImageNet [9] và đạt được hiệu suất tốt hơn ViT bằng cách dựa vào một số kỹ thuật huấn luyện. Cả ViT và các kiến trúc tiếp theo đều chia hình ảnh đầu vào thành nhiều patch hình ảnh không chồng lấp và biến đổi chúng thành token để xử lý tiếp. Điều này cung cấp một chiều kích mới cho việc khai thác độ thưa thớt khá khác với các kỹ thuật tăng cường độ thưa thớt được sử dụng trong CNN.

Hầu hết các công trình cắt tỉa token trước đây tập trung vào các tác vụ NLP, bao gồm PoWER-BERT [15], Length-Adaptive Transformer [19], SpAtten [39], TR-BERT [42], và Learned Token Pruning [20]. Đối với các tác vụ CV, một công trình cắt tỉa token điển hình là DynamicViT [30]. Nó chèn các mô-đun dự đoán giữa các khối transformer để dự đoán và loại bỏ các token ít thông tin hơn. Các mô-đun dự đoán là những mạng nơ-ron có thể được huấn luyện chung với backbone vision Transformer. Thay vì sử dụng chiến lược xác định để cắt tỉa token, A-ViT [43] giới thiệu một quá trình cắt tỉa ngẫu nhiên. Nó sử dụng các mô-đun dừng thích ứng để tính xác suất dừng cho mỗi token. Một token được cắt tỉa (tức là loại bỏ) khi đạt điều kiện dừng. Kết quả là, số lượng token giảm dần, dẫn đến suy luận nhanh hơn. Các công trình gần đây khác về cắt tỉa token cho ViT bao gồm SPViT [21], TPS [40], Adaptive Sparse ViT [24], DToP [35], và HeatViT [11]. Mặc dù các phương pháp cắt tỉa được đề cập ở trên yêu cầu ít hoặc thậm chí không có tham số bổ sung để cắt tỉa, chúng yêu cầu fine-tuning tốn kém về tính toán sau cắt tỉa. Ngược lại, Zero-TPrune đề xuất của chúng tôi có thể loại bỏ quá trình huấn luyện sau cắt tỉa chỉ với 0.1% giảm độ chính xác.

Có một số công trình trước đây đã khám phá việc cắt tỉa token mà không yêu cầu fine-tuning. ATS [14] sử dụng phép biến đổi nghịch đảo để thực hiện lấy mẫu token thích ứng dựa trên phân phối điểm tầm quan trọng. Khi các điểm tầm quan trọng tập trung vào một số token, số lượng token được lấy mẫu tự động giảm. Tuy nhiên, ATS chỉ sử dụng xác suất chú ý của token phân loại (CLS) trong ma trận chú ý và bỏ qua tác động của độ tương tự giữa các token. Mặt khác, ToMe [3] tập trung vào việc hợp nhất token thay vì cắt tỉa chúng, do đó giảm chi phí suy luận của các Transformer đã được huấn luyện trước mà không cần fine-tuning. Các token được hợp nhất dần dần dựa trên độ tương tự của chúng khi các lớp trở nên sâu hơn. Tuy nhiên, ToMe chỉ dựa vào các vector nhúng từ các mô hình đã được huấn luyện trước và quá trình khớp của nó thiếu hướng dẫn phù hợp (chi tiết hơn trong Phần 3.3). Ngược lại, Zero-TPrune sử dụng hiệu quả cả ma trận chú ý hoàn chỉnh và các vector nhúng từ các Transformer đã được huấn luyện trước, đồng thời xem xét tầm quan trọng và độ tương tự của các token.

3. Phương pháp luận
Trong phần này, chúng tôi đầu tiên cung cấp tổng quan về Zero-TPrune trong Phần 3.1, sau đó mô tả các thành phần của nó, I-stage (Phần 3.2) và S-stage (Phần 3.3). Lưu ý rằng Zero-TPrune có thể vi phân, điều này cho phép mô hình đã cắt tỉa được fine-tune thêm để có hiệu suất tốt hơn. Mô hình huấn luyện-sau-cắt tỉa tùy chọn này được mô tả trong Tài liệu Bổ sung Phần C.

3.1. Tổng quan: Zero-TPrune
Khung Zero-TPrune tổng thể được hiển thị trong Hình 2. Mỗi lớp cắt tỉa bao gồm nhiều giai đoạn và có thể được chèn bất cứ đâu giữa các khối Transformer. I-stage và S-stage cho phép Zero-TPrune xem xét cả tầm quan trọng và độ tương tự. Mục tiêu của I-stage là thu được phân phối điểm tầm quan trọng trên các token và giữ lại k token quan trọng nhất. Để đạt được mục tiêu này, chúng tôi đề xuất thuật toán WPR và sử dụng ma trận chú ý từ khối Transformer đã được huấn luyện trước. Trong S-stage, chúng tôi đo lường độ tương tự giữa các token dựa trên các vector nhúng của chúng và chỉ giữ lại một token trong các cặp tương tự nhất top-r. Để giảm chi phí tính toán từ tất cả các kết hợp theo cặp, chúng tôi phân chia các token thành các nhóm song song. Các token trong cùng nhóm không bao giờ được ghép cặp để đo lường độ tương tự. Để có quyền kiểm soát tốt hơn phân phối tầm quan trọng của các token đã cắt tỉa, chúng tôi hướng dẫn việc phân chia bằng thứ hạng tầm quan trọng của chúng.

Một cách đơn giản để kết hợp hai giai đoạn là nối liên tiếp I-stage và S-stage: một số token được cắt tỉa dựa trên phân phối điểm tầm quan trọng thu được trong I-stage; phân phối này sau đó được sử dụng để hướng dẫn việc phân chia trong S-stage và một số token khác được cắt tỉa dựa trên độ tương tự. Tuy nhiên, chúng tôi quan sát thực nghiệm rằng sự kết hợp tầm thường như vậy có thể khiến các token không quan trọng về mặt ngữ nghĩa cuối cùng đẩy ra các token có ý nghĩa ngữ nghĩa trong I-stage. Ví dụ, đôi khi các token nền nhận được điểm tầm quan trọng cao so với các token đối tượng chính. Chi tiết về hiện tượng này có thể được tìm thấy trong Tài liệu Bổ sung Phần A.1. Chúng tôi giải quyết vấn đề này bằng cách hoán đổi I-stage và S-stage. Phương pháp này cho phép loại bỏ sớm các token tương tự trong S-stage, do đó giảm đáng kể tác động tiêu cực của độ tương tự trong I-stage. Chúng tôi trình bày so sánh hai mô hình trong Tài liệu Bổ sung Phần A.2. Để tạo điều kiện phân chia trong S-stage, chúng tôi giới thiệu I′-stage tiền xếp hạng để gán điểm tầm quan trọng cho các token với một vòng bỏ phiếu duy nhất. Đáng chú ý, không có token nào được cắt tỉa trong I′-stage. Do đó, lớp cắt tỉa bao gồm việc áp dụng tuần tự I′-stage, S-stage và I-stage.

3.2. I-stage: Cắt tỉa dựa trên Tầm quan trọng
Để giữ lại k token quan trọng nhất, chúng tôi giới thiệu một metric xếp hạng gọi là điểm tầm quan trọng. Để thu được điểm tầm quan trọng, chúng tôi coi ma trận chú ý A(h,l) như ma trận kề của một đồ thị hoàn chỉnh, có hướng, được gọi là đồ thị chú ý, như được hiển thị trong Hình 3(a). Việc xếp hạng các nút trong đồ thị là thách thức vì một số lý do. (i) Đồ thị chú ý dày đặc, thường bao gồm hàng trăm nút và nhiều cạnh hơn khi đầu vào là một hình ảnh. (ii) Chúng tôi có ngân sách nghiêm ngặt cho chi phí tính toán phát sinh bởi thuật toán trên mỗi hình ảnh trong quá trình suy luận.

Lấy cảm hứng từ thuật toán Page Rank [4], chúng tôi đề xuất thuật toán WPR để có được điểm tầm quan trọng. Page Rank được sử dụng trong Google Search để xếp hạng các trang web. Trong thuật toán Page Rank ban đầu, các liên kết giữa các trang web không có trọng số. Để áp dụng nó vào đồ thị chú ý có trọng số và có hướng, chúng tôi xem xét tín hiệu của mỗi nút trong đồ thị này như tầm quan trọng của mỗi token. Chúng tôi khởi tạo tín hiệu đồ thị đồng đều và sử dụng ma trận kề như một Toán tử Dịch chuyển Đồ thị (GSO). Khi GSO được áp dụng cho tín hiệu đồ thị, mỗi nút bỏ phiếu cho nút nào quan trọng hơn thông qua trọng số được gán cho các cạnh đầu ra, tức là sự chú ý mà một token dành cho các token khác. Nếu bản thân nút quan trọng hơn, việc bỏ phiếu của nút này quan trọng hơn. Điều này được hiển thị trong Thuật toán 1. Quá trình chuyển từ khởi tạo đến hội tụ được hiển thị trong Hình 3(b).

Chúng tôi thu được biểu thức cho điểm tầm quan trọng của mỗi nút (tức là token) trong lớp l thứ l, đầu h thứ h như sau:

s(h,l)(xi) = 1/N ∑(j=1 to N) A(h,l)(xi, xj)·s(h,l)(xj)    (1)

trong đó s(h,l)(xj) là điểm tầm quan trọng của nút xi trong đầu h thứ h của lớp l thứ l, và N là số lượng token trong lớp l thứ l. s(h,l)(xi) được suy ra từ tổng có trọng số của sự chú ý nhận được. WPR do đó đệ quy gán tầm quan trọng cao cho các token có ý nghĩa ngữ nghĩa và giảm nhiễu từ các token không quan trọng có ngữ nghĩa yếu. Chúng tôi giữ lại k token quan trọng nhất (k được xác định bởi tỷ lệ giữ lại và tổng số token).

Đơn giản là lấy trung bình điểm tầm quan trọng qua các đầu khác nhau không phải là lựa chọn tối ưu. Các đầu khác nhau trong một lớp encoder thường chú ý đến các phần khác nhau của hình ảnh đầu vào (một ví dụ trực quan được đưa ra trong Tài liệu Bổ sung Phần E.1). Do đó, có một số token rất quan trọng trong một hoặc hai đầu, nhưng không có trong các đầu khác. Mặt khác, một số token có tầm quan trọng thấp đến trung bình trong tất cả các đầu. Các token trước thường mang nhiều thông tin hơn các token sau. Tuy nhiên, nếu có nhiều đầu và điểm tầm quan trọng được lấy trung bình trực tiếp qua tất cả các đầu, các token sau có thể nhận được điểm tương tự hoặc thậm chí cao hơn các token trước, dẫn đến xếp hạng và cắt tỉa token không chính xác. Để giải quyết vấn đề này, chúng tôi tổng hợp điểm tầm quan trọng qua các đầu thông qua trung bình căn của tổng bình phương. Chúng tôi gọi đây là tổng hợp Nhấn mạnh Vùng Thông tin (EIR). Chúng tôi quan sát thấy EIR phân biệt hiệu quả các vùng thông tin từ các vùng không thông tin. Một ví dụ cụ thể so sánh EIR với các phương pháp khác (như argmax và average) được đưa ra trong Tài liệu Bổ sung Phần E.1.

Bên cạnh vấn đề được đề cập ở trên, đôi khi điểm tầm quan trọng được cung cấp bởi thuật toán WPR có thể hội tụ về một phân phối không mong muốn trong một số đầu: (1) các token ở rìa hình ảnh đầu vào có thể nhận được điểm tầm quan trọng rất cao; (2) phân phối điểm tầm quan trọng có thể trở nên gần như đồng đều. Chúng tôi cung cấp các ví dụ trực quan về những trường hợp này trong Tài liệu Bổ sung Phần E.2. Cả hai đầu trong những trường hợp này đều không cung cấp thông tin hữu ích và thậm chí còn gây hiểu lầm. Để giảm thiểu tác động tiêu cực của những đầu này, chúng tôi giới thiệu Bộ lọc Đầu dựa trên Phương sai (VHF). Chúng tôi tính phương sai của phân phối trong mỗi đầu và đặt cả ngưỡng tối thiểu và tối đa cho phương sai. Các đầu có phương sai phân phối vượt quá ngưỡng tối đa hoặc thấp hơn ngưỡng tối thiểu bị loại khỏi tính toán. Sau đó phương trình điểm tầm quan trọng cuối cùng trở thành:

s(l)(xi) = √(∑(h=1 to Nh) s(h,l)(xi)² · η(vmin≤Varh≤vmax) / ∑(h=1 to Nh) η(vmin≤Varh≤vmax))    (2)

trong đó η(vmin≤Varh≤vmax) bằng 1 nếu vmin≤Varh≤vmax, ngược lại bằng 0; vmin và vmax đại diện cho ngưỡng tối thiểu và tối đa, tương ứng; Varh là phương sai điểm tầm quan trọng của các token trong đầu h thứ h; Nh là số lượng đầu trong lớp l thứ l. Độ phức tạp của I-stage, bao gồm WPR, EIR và VHF, là O(N²), trong đó N là số lượng token.

3.3. S-stage: Cắt tỉa dựa trên Độ tương tự
Như đã thảo luận trước đây, việc đo lường độ tương tự ngay cả giữa các token quan trọng và thực hiện cắt tỉa thêm là có giá trị. Công trình trước đây [3] sử dụng phân chia token không phụ thuộc hình ảnh để đo lường độ tương tự theo cặp. Thay vào đó, chúng tôi đề xuất phân chia dựa trên tầm quan trọng trên mỗi hình ảnh để cắt tỉa độ tương tự, như được hiển thị trong Hình 4.

Hình 4 (1&2): Dựa trên điểm tầm quan trọng của các token, chúng tôi tuần tự phân chia chúng thành các nhóm có kích thước gần bằng nhau, A và B, và cắt tỉa nhóm ít quan trọng hơn. Chúng tôi khám phá các lược đồ phân chia hướng dẫn tầm quan trọng khác, bao gồm phân chia thay thế và phân chia ngẫu nhiên, và cung cấp kết quả ablation trong Tài liệu Bổ sung Phần G.3. Hình 4 (3): Sau đó chúng tôi xác định token tương tự nhất trong Nhóm B cho mỗi token trong Nhóm A và ghi lại độ tương tự tương ứng của mỗi cặp. Để hoàn thành điều này, chúng tôi biểu diễn mỗi token bằng một vector đặc trưng, có thể được suy ra từ một số lựa chọn có sẵn, chẳng hạn như các vector tương ứng trong ma trận Key, Query hoặc Value. Các thí nghiệm ablation của chúng tôi chỉ ra rằng sử dụng các vector từ ma trận Key là lựa chọn tối ưu. Chúng tôi tính độ tương tự trên những vector này bằng một metric được chỉ định, chẳng hạn như độ tương tự cosine, khoảng cách Manhattan hoặc khoảng cách Euclidean. Theo kết quả của các thí nghiệm ablation, chúng tôi sử dụng độ tương tự cosine. Chúng tôi cung cấp tài khoản chi tiết về kết quả thí nghiệm ablation trong Tài liệu Bổ sung Phần G.3.

Hình 4 (4&5): Trong bước tiếp theo, chúng tôi chọn r cặp tương tự nhất và cắt tỉa các token tương ứng trong Nhóm A. Chúng tôi cắt tỉa một token trong mỗi cặp được chọn thay vì hợp nhất chúng, do các lý do sau: (i) vì các token trong các cặp được chọn tương tự, việc cắt tỉa một trong số chúng dẫn đến mất thông tin tối thiểu; (ii) các token được hợp nhất nên có trọng số cao hơn trong tính toán sau [3], điều này làm cho nó không tương thích với một số backbone nhất định, chẳng hạn như Sparse Transformer [6]. Cuối cùng, chúng tôi chuyển các token còn lại sang giai đoạn tiếp theo. Độ phức tạp của S-stage là O(N²×d), trong đó N là số lượng token và d là chiều của nhúng token.

Phân chia hướng dẫn tầm quan trọng trong S-stage tạo điều kiện kiểm soát ổn định tầm quan trọng của các token đã cắt tỉa cho các hình ảnh đầu vào khác nhau. Bằng cách cắt tỉa các token tương tự thay vì hợp nhất chúng, phương pháp của chúng tôi duy trì khả năng tương thích với một số backbone chuyên biệt [6] trong khi chỉ phát sinh mất thông tin tối thiểu.

4. Kết quả Thí nghiệm
Trong phần này, chúng tôi đầu tiên mô tả quá trình cắt tỉa token được trực quan hóa của một số hình ảnh trong tập dữ liệu xác thực ImageNet, như được hiển thị trong Hình 5. Chúng tôi cũng trình bày các thí nghiệm ablation để xác thực các lựa chọn thiết kế và hiệu quả của các phương pháp đề xuất. Sau đó, chúng tôi so sánh Zero-TPrune với các phương pháp cắt tỉa token tiên tiến.

Thiết lập Thí nghiệm: Để so sánh các phương pháp cắt tỉa khác nhau, chúng tôi áp dụng chúng vào các backbone vision Transformer khác nhau và đánh giá hiệu suất của các mô hình đã cắt tỉa trên ImageNet [9]. Chúng tôi đánh giá các mô hình trên hình ảnh 224px trừ khi có ghi chú khác. Chúng tôi ước tính GFLOPS suy luận trên GPU V100 bằng thư viện fvcore¹. Chúng tôi đo lường thông lượng suy luận của các mô hình đã cắt tỉa trên một GPU A100 và thực hiện fine-tuning sau cắt tỉa trên GPU A100.

Mặc dù các thí nghiệm của chúng tôi tập trung vào tác vụ phân loại, Zero-TPrune có thể được áp dụng cho các tác vụ khác, chẳng hạn như tạo sinh và phân đoạn. Chúng tôi gọi thiết kế có thể được chuyển giao cho các tác vụ khác là "Zero-TPrune-uni", có nghĩa là "Zero-TPrune cho mục đích phổ quát." Đối với tác vụ phân loại, token CLS được biết là quan trọng hơn nhiều so với các token khác, và trọng số chú ý của nó là tín hiệu mạnh hơn nhiều để chọn token [14]. Do đó, thay vì khởi tạo điểm tầm quan trọng của các token đồng đều, chúng tôi gán cho token CLS một điểm tầm quan trọng lớn hơn √N lần so với các token khác trong quá trình khởi tạo trong I-stage, trong đó N là số lượng token. Chúng tôi gọi thiết kế này là "Zero-TPrune" trong các thí nghiệm sau.

Đối với các thí nghiệm ablation, chúng tôi triển khai Zero-TPrune trên mô hình DeiT-S [36] với các cấu hình và lựa chọn thiết kế khác nhau. Để so sánh với các công trình cắt tỉa token tiên tiến, chúng tôi chia chúng thành hai loại: (1) các phương pháp yêu cầu fine-tuning của mô hình đã cắt tỉa, bao gồm DynamicViT [30] và A-ViT [43]; (2) các phương pháp không cần fine-tuning, bao gồm ATS [14] và ToMe [3] (đây là phương pháp hợp nhất token thay vì cắt tỉa token, nhưng cũng không cần fine-tuning). Đối với loại thứ nhất, chúng tôi so sánh các triển khai trên các mô hình DeiT. Chúng tôi sử dụng triển khai chính thức của DynamicViT để tái tạo kết quả của nó và cũng tạo ra một số kết quả mới để so sánh. Đối với A-ViT, chúng tôi sử dụng trực tiếp kết quả được trình bày trong bài báo đó. Đối với loại thứ hai, chúng tôi sử dụng mã nguồn mở chính thức của ATS và ToMe để triển khai chúng trên các backbone Transformer đã được huấn luyện trước khác nhau, bao gồm DeiT [36], LV-ViT [18], MAE [17], AugReg [33] và SWAG [32]. Chúng tôi so sánh hiệu suất ngay lập tức của Zero-TPrune với của chúng. Ngoài ra, chúng tôi so sánh hiệu suất của các mô hình đã cắt tỉa trên các tác vụ downstream để kiểm tra khả năng học chuyển giao của chúng, theo việc lựa chọn tập dữ liệu trong [5]. Chúng tôi cung cấp chi tiết về các tập dữ liệu đã chọn trong Tài liệu Bổ sung Phần F. Lưu ý rằng ToMe có một thiết kế tùy chọn, Proportional Attention (PA), dành riêng cho phân loại [2] và không tương thích với thiết kế chú ý thưa thớt [6]. Chúng tôi gọi ToMe với PA tắt là "ToMe-uni" và ToMe với PA bật là "ToMe". ATS là một phương pháp chỉ dựa trên token CLS; do đó, nó không có phiên bản phổ quát.

Để xác thực thêm hiệu quả của Zero-TPrune, chúng tôi bổ sung so sánh với các phương pháp thích ứng độ sâu và các phương pháp xếp hạng token dựa trên chú ý đơn giản khác (ví dụ: lấy trung bình chú ý nhận được) trong Tài liệu Bổ sung Phần H.1 và H.2.

4.1. Thí nghiệm Ablation
Chúng tôi sử dụng các thí nghiệm ablation để xác định các siêu tham số tối ưu cho Zero-TPrune. Đầu tiên, việc kiểm tra xem thuật toán WPR có hội tụ sau mỗi lần lặp hay không là tốn kém về tính toán. Do đó, sẽ rất mong muốn nếu chúng tôi có thể xác định số lần lặp của nó trước. Bằng cách kiểm tra phân phối tầm quan trọng sau các số lần lặp khác nhau và tính độ phân kỳ Kullback-Liebler (KL) giữa chúng, chúng tôi thấy 30-50, 5-10 và 1 lần lặp là đủ để đảm bảo hội tụ trong ba lớp đầu tiên, các lớp trung bình và ba lớp cuối cùng, tương ứng. Chúng tôi cung cấp so sánh trực quan và định lượng trong Tài liệu Bổ sung Phần G.1. Thứ hai, chúng tôi xác định các ngưỡng tối thiểu và tối đa đủ tốt cho VHF thông qua khởi tạo ngẫu nhiên và tìm kiếm tham lam. Chúng tôi cung cấp cấu hình tìm kiếm chi tiết và kết quả trong Tài liệu Bổ sung Phần G.2. Phạm vi tìm thấy là [0.01,0.7], đây là cài đặt mặc định trong các thí nghiệm của chúng tôi. Thứ ba, chúng tôi khám phá các lựa chọn thiết kế tối ưu trong S-stage với các thí nghiệm ablation được trình bày trong Tài liệu Bổ sung Phần G.3.

Để minh họa hiệu quả của các kỹ thuật khác nhau được sử dụng trong Zero-TPrune, chúng tôi phân tích đóng góp của chúng. Chúng tôi áp dụng các kết hợp khác nhau của các kỹ thuật được sử dụng trong Zero-TPrune cho mô hình DeiT-S và đánh giá hiệu suất của các mô hình đã cắt tỉa. Chúng tôi chèn các lớp cắt tỉa sau lớp [1,3,6,9,11] với tỷ lệ giữ lại [1,0.9,0.8,0.7,1] và số lần lặp [30,5,5,1,1] trong I-stage, và cắt tỉa 10 token trong mỗi S-stage. Trước khi thêm S-stage, chúng tôi chèn các lớp cắt tỉa sau lớp [3,6,9,11] với tỷ lệ giữ lại [0.8,0.7,0.7,0.6] và số lần lặp [5,5,1,1]. Chúng tôi hiển thị kết quả trong Bảng 1 (chúng tôi cung cấp kết quả tương ứng của Zero-TPrune-uni trong Tài liệu Bổ sung Phần G.4). Thuật toán WPR cải thiện hiệu suất đáng kể. Các kỹ thuật EIR/VHF và S-stage cải thiện hiệu suất thêm.

Để cải thiện thêm hiệu suất của các mô hình đã cắt tỉa, chúng tôi có thể sử dụng Mô phỏng Monte Carlo (MCS) để khám phá ngẫu nhiên không gian siêu tham số, bao gồm số lượng và vị trí của các lớp cắt tỉa, tỷ lệ giữ lại tương ứng, số lần lặp trong mỗi lớp và số token được cắt tỉa trong mỗi S-stage. Sau khi tiến hành hàng nghìn thử nghiệm, chúng tôi chọn cài đặt tối ưu thể hiện hiệu suất tốt nhất đạt được bởi Zero-TPrune trong khi duy trì ngân sách GFLOPS cố định. Trong trường hợp được hiển thị trong Bảng 1, MCS giúp đạt được độ chính xác 79.5% với 3.08 GFLOPS. Để đảm bảo so sánh công bằng, chúng tôi không sử dụng MCS trong Zero-TPrune trong các so sánh tiếp theo với các phương pháp tiên tiến. Zero-TPrune không nhạy cảm với lựa chọn siêu tham số, như được minh họa với kết quả thí nghiệm trong Tài liệu Bổ sung Phần G.5.

4.2. So sánh với Các Phương pháp Tiên tiến
Trong phần này, chúng tôi chọn số lượng và vị trí của các lớp cắt tỉa với tỷ lệ giữ lại không đổi hoặc giảm đều để phù hợp với ngân sách GFLOPS đã cho. Chúng tôi giữ số lượng token được cắt tỉa trong mỗi S-stage không đổi. Chúng tôi cố định số lần lặp là 30, 5 và 1 cho ba lớp đầu tiên, các lớp trung gian và ba lớp cuối cùng, tương ứng.

So sánh với Các Phương pháp Yêu cầu Fine-Tuning: Để minh họa ưu điểm của Zero-TPrune, chúng tôi so sánh hiệu suất của nó với DynamicViT và A-ViT có/không có fine-tuning sau cắt tỉa. Do khởi tạo ngẫu nhiên và thực tế là các mô-đun cắt tỉa trong DynamicViT và A-ViT cần được huấn luyện, hiệu suất của DynamicViT và A-ViT mà không có fine-tuning sau cắt tỉa dựa trên các token được cắt tỉa ngẫu nhiên. Hình 6 rõ ràng thể hiện ưu điểm của Zero-TPrune so với các phương pháp cắt tỉa yêu cầu fine-tuning tiên tiến, tức là DynamicViT và A-ViT. Không có fine-tuning sau cắt tỉa, Zero-TPrune vượt trội hơn DynamicViT và A-ViT (sử dụng loại bỏ ngẫu nhiên trong trường hợp này) khoảng 1%. Điều này có nghĩa là Zero-TPrune giảm độ sụt giảm độ chính xác hơn 60%. Hiệu suất của Zero-TPrune mà không có fine-tuning sau cắt tỉa có thể so sánh với DynamicViT và A-ViT có fine-tuning sau cắt tỉa (ví dụ: 0.1% mất độ chính xác so với tốt nhất, với ngân sách 3.5 GFLOPS trên DeiT-S). Với fine-tuning sau cắt tỉa, Zero-TPrune vượt trội hơn cả DynamicViT và A-ViT. Zero-TPrune cũng có thể dễ dàng được áp dụng cho các mô hình lớn hơn (ví dụ: với ngân sách 13.6 GFLOPS trên DeiT-B) để có độ chính xác cao hơn. Ngược lại, việc áp dụng DynamicViT và A-ViT cho các mô hình lớn rất tốn kém về tính toán do fine-tuning đắt đỏ sau cắt tỉa.

So sánh với Các Phương pháp Không cần Fine-Tuning: ATS và ToMe cung cấp tùy chọn ngay lập tức để cắt tỉa các mô hình Transformer mà không yêu cầu fine-tuning sau cắt tỉa. Chúng tôi đầu tiên áp dụng chúng và Zero-TPrune cho mô hình DeiT-S để so sánh hiệu suất ngay lập tức sau cắt tỉa mà không có fine-tuning. Kết quả được hiển thị trong Hình 7 và Bảng 2. Chúng tôi cung cấp thêm kết quả liên quan đến thông lượng trong Tài liệu Bổ sung Phần H.3. Như được hiển thị trong Hình 7, so với các phương pháp không cần fine-tuning tiên tiến, Zero-TPrune giảm mất độ chính xác 33% trên mô hình DeiT-S với ngân sách 3 GFLOPS. Nếu chúng tôi thay đổi cấu hình cắt tỉa và đưa ra ngân sách thấp hơn (ví dụ: giảm GFLOPS 45%), mất độ chính xác do Zero-TPrune gây ra vẫn chỉ là 0.7%. Zero-TPrune có thể giảm GFLOPS 13% gần như không có chi phí. Lưu ý rằng những kết quả này được thu được mà không có fine-tuning.

Chúng tôi tiếp tục đánh giá Zero-TPrune và baseline trên các backbone khác nhau với kích thước khác nhau. Kết quả được hiển thị trong Bảng 3. Chúng tôi thấy rằng khi mô hình ban đầu có kích thước trung bình, ví dụ: AugReg và LV-ViT-S, Zero-TPrune vượt trội hơn các phương pháp baseline với biên độ lớn (nó giảm mất độ chính xác tới 49%). Đối với các mô hình lớn, nếu việc cắt tỉa vừa phải (tức là giảm GFLOPS 20%), Zero-TPrune vẫn vượt trội hơn các phương pháp baseline. Tuy nhiên, chúng tôi thấy khi các mô hình lớn được cắt tỉa mạnh (tức là giảm GFLOPS 50%), Zero-TPrune không vượt trội hơn baseline. Lưu ý rằng việc cắt tỉa mạnh các mô hình lớn thường không phải là ý tưởng tốt, điều này được chỉ ra bằng cách so sánh mô hình LV-ViT-M đã cắt tỉa tối ưu (ToMe, 81.6% với 6.3 GFLOPS) và mô hình LV-ViT-S đã cắt tỉa tối ưu (Zero-TPrune, 81.5% với 3.5 GFLOPS). Mô hình sau chỉ cần 60% GFLOPS với chi phí 0.1% mất độ chính xác. So với các mô hình lớn được cắt tỉa mạnh, việc sử dụng mô hình đã được huấn luyện trước nhỏ hơn thường là lựa chọn tốt hơn. Chúng tôi cung cấp thảo luận chi tiết trong Tài liệu Bổ sung Phần I.

Chúng tôi cũng đánh giá hiệu suất của các mô hình đã cắt tỉa trên các tác vụ downstream để đo lường khả năng học chuyển giao của chúng. Chúng tôi chọn một số tập dữ liệu hình ảnh nhỏ cho mục đích này. Zero-TPrune vượt trội hơn baseline trên hầu hết các tập dữ liệu, cho thấy khả năng học chuyển giao mạnh mẽ sau cắt tỉa. Chúng tôi giới thiệu các tập dữ liệu đã chọn và trình bày kết quả thí nghiệm chi tiết trong Tài liệu Bổ sung Phần F.

5. Kết luận
Trong bài báo này, chúng tôi đề xuất Zero-TPrune, một phương pháp cắt tỉa token zero-shot khai thác cả tầm quan trọng và độ tương tự của các token để loại bỏ quá trình fine-tuning cho việc cắt tỉa. Trong I-stage, nó xem xét ma trận chú ý là ma trận kề của một đồ thị chú ý, giảm nhiễu từ các token không quan trọng. Trong S-stage, nó sử dụng phân phối tầm quan trọng để hướng dẫn phân chia token và cắt tỉa dựa trên độ tương tự, làm cho chúng ổn định và chính xác hơn. Thông qua việc triển khai Zero-TPrune và các phương pháp baseline trên các backbone Transformer khác nhau và đánh giá trên ImageNet, chúng tôi cho thấy rằng nó có thể loại bỏ quá trình fine-tuning cho việc cắt tỉa với độ giảm độ chính xác rất nhỏ. Hơn nữa, khi so sánh với các phương pháp cắt tỉa ngay lập tức tiên tiến, Zero-TPrune không chỉ vượt trội hơn chúng bằng cách giảm mất độ chính xác tới 49% mà còn tăng cường khả năng học chuyển giao của các mô hình đã cắt tỉa. Những phát hiện này nhấn mạnh hiệu quả của Zero-TPrune trong việc cân bằng nén mô hình và bảo tồn hiệu suất, làm cho nó trở thành một cách tiếp cận đầy hứa hẹn cho việc cắt tỉa hiệu quả và chính xác các mô hình Transformer. Nghiên cứu tương lai có thể nâng cao thêm khả năng của Zero-TPrune. Một chủ đề hấp dẫn cho nghiên cứu tương lai là kiểm tra tính ứng dụng của Zero-TPrune trên các tác vụ như tái tạo hình ảnh, phân đoạn và tạo sinh. Việc điều tra các lợi ích tiềm năng và tăng hiệu quả của việc sử dụng Zero-TPrune trong những lĩnh vực này mang lại triển vọng để thúc đẩy lĩnh vực này phát triển hơn nữa.
