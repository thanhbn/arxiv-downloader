# 2308.07633.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2308.07633.pdf
# Kích thước tệp: 504718 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Một Khảo Sát về Nén Mô Hình cho Các Mô Hình Ngôn Ngữ Lớn
Xunyu Zhu1,2, Jian Li1,2*, Yong Liu3, Can Ma1,2, Weiping Wang1,2
1Viện Kỹ thuật Thông tin, Viện Hàn lâm Khoa học Trung Quốc
2Khoa An ninh mạng, Đại học Viện Hàn lâm Khoa học Trung Quốc
3Trường Trí tuệ nhân tạo Gaoling, Đại học Nhân dân Trung Quốc
{zhuxunyu, lijian9026, macan, wangweiping}@iie.ac.cn, liuyonggsai@ruc.edu.cn

Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLMs) đã biến đổi thành công các tác vụ xử lý ngôn ngữ tự nhiên. Tuy nhiên, kích thước lớn và nhu cầu tính toán cao của chúng đặt ra những thách thức cho việc sử dụng thực tế, đặc biệt trong các môi trường hạn chế tài nguyên. Nén mô hình đã nổi lên như một lĩnh vực nghiên cứu chủ chốt để giải quyết những thách thức này. Bài báo này trình bày một khảo sát về các kỹ thuật nén mô hình cho LLMs. Chúng tôi bao gồm các phương pháp như lượng tử hóa, tỉa cành và chưng cất kiến thức, làm nổi bật những tiến bộ gần đây. Chúng tôi cũng thảo luận về các chiến lược đánh giá hiệu suất và các chỉ số đánh giá quan trọng để đánh giá LLMs được nén. Khảo sát này cung cấp những hiểu biết có giá trị cho các nhà nghiên cứu và người thực hành, nhằm nâng cao hiệu quả và khả năng ứng dụng thực tế của LLMs đồng thời đặt nền tảng cho những tiến bộ trong tương lai.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLMs) (Touvron et al., 2023a,b; Zhang et al., 2022; Scao et al., 2022; Wang and Komatsuzaki, 2021; OpenAI, 2024) đề cập đến các mô hình ngôn ngữ Transformer chứa hàng tỷ (hoặc nhiều hơn) tham số, được huấn luyện trên dữ liệu văn bản khổng lồ. LLMs liên tục thể hiện hiệu suất đáng chú ý trên nhiều tác vụ khác nhau, nhưng khả năng đặc biệt của chúng đi kèm với những thách thức đáng kể phát sinh từ kích thước rộng lớn và yêu cầu tính toán của chúng. Ví dụ, mô hình GPT-175B (Brown et al., 2020), với 175 tỷ tham số ấn tượng, đòi hỏi tối thiểu 350GB bộ nhớ ở định dạng bán chính xác (FP16). Hơn nữa, triển khai mô hình này để suy luận cần ít nhất năm GPU A100, mỗi chiếc có 80GB bộ nhớ, để quản lý các hoạt động một cách hiệu quả. Để giải quyết những vấn đề này, một phương pháp phổ biến được gọi là nén mô hình (Han et al., 2016) cung cấp một giải pháp. Nén mô hình bao gồm việc biến đổi một mô hình lớn, tốn nhiều tài nguyên thành một phiên bản nhỏ gọn phù hợp để triển khai trên các thiết bị hạn chế tài nguyên. Ngoài ra, nén mô hình có thể nâng cao tốc độ suy luận LLM và tối ưu hóa hiệu quả tài nguyên.

Trong bài báo của chúng tôi, mục tiêu chính của chúng tôi là làm sáng tỏ những bước tiến gần đây trong lĩnh vực các kỹ thuật nén mô hình được thiết kế riêng cho LLMs. Công trình của chúng tôi tiến hành một khảo sát toàn diện về các phương pháp luận, chỉ số và điểm chuẩn của nén mô hình cho LLMs. Hình 1 cho thấy phân loại các phương pháp nén mô hình cho LLMs, bao gồm lượng tử hóa, tỉa cành, chưng cất kiến thức và phân tích nhân tử hạng thấp. Hình 2 tiếp tục cho thấy luồng cơ bản của các phương pháp nén mô hình này cho LLMs. Hơn nữa, nghiên cứu của chúng tôi làm sáng tỏ các thách thức hiện tại và cung cấp cái nhìn về các quỹ đạo nghiên cứu tiềm năng trong tương lai trong lĩnh vực đang phát triển này. Chúng tôi ủng hộ các nỗ lực hợp tác trong cộng đồng để mở đường cho một tương lai có ý thức sinh thái, toàn diện và bền vững cho LLMs. Mặc dù đã có các khảo sát trước đây về nén mô hình mạng nơ-ron (Li et al., 2023c) và nó đã được thảo luận nhẹ trong các khảo sát trước về LMs (Rogers et al., 2020) và LLMs (Zhao et al., 2023), công trình của chúng tôi là khảo sát đầu tiên được dành riêng cho nén mô hình cho LLMs.

2 Chỉ số và Điểm chuẩn
2.1 Chỉ số
Nén mô hình của LLMs có thể được đo lường bằng nhiều chỉ số khác nhau, chúng nắm bắt các khía cạnh khác nhau của hiệu suất. Những chỉ số này thường được trình bày cùng với độ chính xác và khả năng zero-shot để đánh giá toàn diện LLM.

Kích thước Mô hình trong một LLM thường được đo bằng tổng số tham số của LLM. Nói chung, LLMs với nhiều tham số hơn thường yêu cầu nhiều tài nguyên tính toán và bộ nhớ hơn cho cả huấn luyện và suy luận.

Phép toán Dấu phẩy động (FLOPs) là một chỉ báo đo lường hiệu quả tính toán của LLMs, đại diện cho số lượng phép toán dấu phẩy động cần thiết để LLM thực hiện một thể hiện. Trong nén mô hình, việc giảm FLOPs giúp làm cho LLM chạy nhanh hơn và hiệu quả hơn.

Sử dụng FLOPS Trung bình (MFU) định lượng hiệu quả thực tế của việc sử dụng tài nguyên tính toán bởi LLMs trong các tác vụ. MFU đo tỷ lệ FLOPS thực tế được sử dụng bởi LLM so với FLOPS lý thuyết tối đa của một thiết bị. Không giống như FLOPs, ước tính các phép toán tối đa mà một LLM có thể thực hiện, MFU đánh giá hiệu quả thực tế của việc sử dụng tài nguyên trong hoạt động. Về cơ bản, trong khi FLOPs đo nhu cầu tính toán lý thuyết của LLM, MFU cho thấy mức độ hiệu quả của những tính toán này được sử dụng trong thực tế.

Thời gian suy luận (tức là độ trễ) đo thời gian mà LLM mất để xử lý và tạo ra các phản hồi cho dữ liệu đầu vào trong quá trình suy luận. Thời gian suy luận đặc biệt quan trọng đối với các ứng dụng thực tế nơi LLM cần phản hồi cho các truy vấn của người dùng hoặc xử lý lượng lớn dữ liệu trong thời gian thực.

Tỷ lệ Tăng tốc đo mức độ nhanh hơn của một LLM được nén khi thực hiện các tác vụ so với LLM gốc. Cụ thể, nó đo tỷ lệ thời gian suy luận của mô hình không nén so với thời gian suy luận của mô hình đã nén. Tỷ lệ cao hơn có nghĩa là hiệu quả lớn hơn và thời gian tính toán giảm, làm nổi bật việc nén hiệu quả.

Tỷ lệ Nén đo mức độ giảm kích thước của LLM thông qua nén, được tính bằng kích thước gốc chia cho kích thước đã nén. Tỷ lệ cao hơn có nghĩa là giảm kích thước lớn hơn, cho thấy hiệu quả của việc nén trong việc tiết kiệm lưu trữ và bộ nhớ.

2.2 Điểm chuẩn và Bộ dữ liệu
Mục tiêu chính của những điểm chuẩn và bộ dữ liệu này là đo lường hiệu quả và hiệu suất của LLMs được nén so với các đối tác chưa nén của chúng. Những điểm chuẩn và bộ dữ liệu này thường bao gồm các tác vụ và bộ dữ liệu đa dạng bao phủ một loạt các thách thức xử lý ngôn ngữ tự nhiên.

2.2.1 Điểm chuẩn và Bộ dữ liệu Phổ biến
Phần lớn nghiên cứu đánh giá LLMs được nén trên các điểm chuẩn và bộ dữ liệu NLP được thiết lập tốt. Ví dụ, WikiText-2 (Merity et al., 2017), C4 (Raffel et al., 2020), và PTB (Marcus et al., 1993) được thiết kế để đánh giá hiệu suất perplexity của các mô hình ngôn ngữ. LAMBADA (Paperno et al., 2016), PIQA (Tata and Patel, 2003), và OpenBookQA (Mihaylov et al., 2018) được thiết kế để đánh giá khả năng zero-shot của các mô hình ngôn ngữ. GSM8K (Cobbe et al., 2021), CommonsenseQA (Talmor et al., 2019) và StrategyQA (Geva et al., 2021) được thiết kế để đánh giá khả năng lập luận của các mô hình ngôn ngữ.

2.2.2 BIG-Bench
BIG-Bench (BBH) (Srivastava et al., 2023) là một bộ điểm chuẩn được thiết kế cho LLMs, bao gồm hơn 200 tác vụ NLP, ví dụ: Tác vụ Hiểu văn bản, Tác vụ Suy luận, Tác vụ Lập luận Toán học. Mục tiêu của BBH là đánh giá hiệu suất của LLMs trên những tác vụ phức tạp đa dạng này. LLMs được nén sử dụng BBH để đo lường khả năng của chúng trên một phổ tác vụ đa chiều.

2.2.3 Bộ dữ liệu Hướng dẫn Chưa thấy
Bộ dữ liệu hướng dẫn chưa thấy được sử dụng để đánh giá hiệu suất của LLMs trên các tác vụ chưa thấy. Ví dụ, bộ dữ liệu Vicuna-Instructions (Zheng et al., 2023) được tạo bởi GPT-4 bao gồm 80 câu hỏi phức tạp trên chín danh mục khác nhau như các tác vụ chung, dựa trên kiến thức và viết. Một bộ dữ liệu khác, User-Oriented-Instructions (Wang et al., 2023e), bao gồm 252 hướng dẫn được chọn lọc cẩn thận lấy cảm hứng từ các ứng dụng tập trung vào người dùng khác nhau như Grammarly, StackOverflow và Overleaf. Những bộ dữ liệu này đánh giá mức độ LLMs nhỏ gọn có thể xử lý và thực hiện các tác vụ mới bằng cách trình bày cho chúng những hướng dẫn không quen thuộc.

2.2.4 EleutherAI LM Harness
EleutherAI LM Harness (Gao et al., 2023) là một khung tiên tiến để đánh giá LLMs, cung cấp một nền tảng kiểm tra thống nhất hỗ trợ hơn 60 điểm chuẩn học thuật tiêu chuẩn cùng với hàng trăm tác vụ con và biến thể. Các tác vụ đánh giá được chuẩn hóa do harness cung cấp đảm bảo tính tái tạo và khả năng so sánh của đánh giá, điều này rất cần thiết để thực hiện các đánh giá công bằng và tái tạo cho LLMs được nén.

3 Lượng tử hóa
Lượng tử hóa (Gray and Neuhoff, 1998) đề cập đến quá trình giảm số bit (tức là độ chính xác) trong các tham số của mô hình với mất mát tối thiểu trong hiệu suất suy luận. Lượng tử hóa có thể được phân loại thành hai phương pháp chính: Huấn luyện Nhận biết Lượng tử hóa (QAT), và Lượng tử hóa Sau Huấn luyện (PTQ). Sự khác biệt chính giữa hai phương pháp nằm ở việc có cần huấn luyện lại trong quá trình lượng tử hóa hay không. PTQ cho phép sử dụng trực tiếp các mô hình đã lượng tử hóa trong suy luận, trong khi QAT yêu cầu huấn luyện lại để sửa chữa các lỗi được giới thiệu bởi lượng tử hóa. Bảng 1 cho thấy hiệu suất của nhiều phương pháp lượng tử hóa LLM đại diện.

3.1 Huấn luyện Nhận biết Lượng tử hóa
QAT bao gồm việc huấn luyện lại một mô hình đã lượng tử hóa để chống lại sự suy giảm hiệu suất do lượng tử hóa gây ra. Ví dụ, LLM-QAT (Liu et al., 2023b) thực hiện khung QAT tiêu chuẩn trực tiếp trên LLMs. LLM-QAT chưng cất kiến thức bằng cách tạo dữ liệu từ chính LLM và huấn luyện LLM đã lượng tử hóa để căn chỉnh với phân phối đầu ra của LLM gốc dựa trên dữ liệu được tạo. BitDistiller (Du et al., 2024) kết hợp QAT với tự chưng cất, nâng cao hiệu suất LLM ở độ chính xác dưới 4-bit. Nó sử dụng lượng tử hóa bất đối xứng được điều chỉnh, cắt xén và một mục tiêu Divergence Kullback-Leibler Nhận biết Độ tin cậy để hội tụ nhanh hơn và kết quả vượt trội. OneBit (Xu et al., 2024) giới thiệu một phương pháp biểu diễn tham số 1-bit mới và một phương pháp khởi tạo tham số hiệu quả để thực hiện lượng tử hóa 1-bit cho ma trận trọng số LLM, mở đường cho việc triển khai độ rộng bit cực thấp của LLMs.

Nhận xét 1. Mặc dù QAT có thể giảm thiểu suy giảm độ chính xác của lượng tử hóa, việc huấn luyện lại đòi hỏi rất nhiều nỗ lực do hàng chục hoặc hàng trăm tỷ tham số trong LLMs. Một giải pháp thực tế là kết hợp Tinh chỉnh Hiệu quả Tham số (PEFT) vào quá trình huấn luyện lại của QAT. Hiện tại, các phương pháp như QLORA (Dettmers et al., 2023), PEQA (Kim et al., 2023a) và LoftQ (Li et al., 2023a) kết hợp lượng tử hóa với PEFT để tinh chỉnh mô hình hiệu quả. Tuy nhiên, những phương pháp này thường phụ thuộc vào tác vụ. L4Q (Jeon et al., 2024) thực hiện một nỗ lực sơ bộ để nâng cao tính tổng quát bằng cách tận dụng kích thước bước lượng tử hóa được học theo LoRA cho LLMs. Chúng tôi nghĩ rằng việc giới thiệu PEFT để nâng cao hiệu quả QAT không chỉ khả thi mà còn có tiềm năng đáng kể, đáng để khám phá kỹ lưỡng.

3.2 Lượng tử hóa Sau Huấn luyện
PTQ chuyển đổi hiệu quả một LLM độ chính xác đầy đủ sang độ chính xác thấp mà không cần huấn luyện lại, tiết kiệm chi phí bộ nhớ và tính toán. Chúng tôi phân loại PTQ cho LLMs thành ba nhóm: Lượng tử hóa Chỉ Trọng số, Lượng tử hóa Trọng số-Kích hoạt và Lượng tử hóa KV Cache. Sự khác biệt giữa những nhóm này nằm ở mục tiêu lượng tử hóa của chúng. Lượng tử hóa chỉ trọng số tập trung duy nhất vào việc lượng tử hóa trọng số, trong khi lượng tử hóa trọng số-kích hoạt mở rộng mục tiêu của nó đến cả trọng số và kích hoạt. Nghiên cứu trước (Yao et al., 2023) chỉ ra rằng lượng tử hóa kích hoạt thường nhạy cảm hơn với lượng tử hóa trọng số, cho phép lượng tử hóa chỉ trọng số đạt được độ rộng bit thấp hơn. Tuy nhiên, vì trọng số đã lượng tử hóa đòi hỏi giải lượng tử hóa trước khi nhân với kích hoạt, lượng tử hóa chỉ trọng số không tránh khỏi gây ra chi phí tính toán bổ sung trong quá trình suy luận và không thể tận hưởng hoạt động bit thấp được tăng tốc được hỗ trợ bởi phần cứng cụ thể. Hơn nữa, lượng tử hóa kv cache nhắm vào KV cache, lưu trữ các khóa và giá trị của các lớp attention. KV cache thường tiêu thụ nhiều bộ nhớ, hoạt động như một nút cổ chai cho các luồng đầu vào chứa các token dài. Bằng cách thực hiện lượng tử hóa kv cache, có thể tăng thông lượng và chứa các đầu vào với token dài hơn một cách hiệu quả hơn.

3.2.1 Lượng tử hóa Chỉ Trọng số
Lượng tử hóa chỉ trọng số là phương pháp thông thường và phổ biến nhất. Ví dụ, LUT-GEMM (Park et al., 2024) sử dụng định dạng lượng tử hóa mã hóa nhị phân (BCQ) (Rastegari et al., 2016), phân tích các tham số của LLMs thành các tham số nhị phân và một tập hợp các yếu tố tỷ lệ, để tăng tốc các phép nhân ma trận đã lượng tử hóa trong lượng tử hóa chỉ trọng số. GPTQ (Frantar et al., 2023) đề xuất một phương pháp lượng tử hóa theo lớp dựa trên Lượng tử hóa Não bộ Tối ưu (OBQ) (Frantar and Alistarh, 2022), cập nhật trọng số với thông tin Hessian nghịch đảo, và lượng tử hóa LLMs thành 3/4-bit. QuIP (Chee et al., 2023) điều chỉnh trọng số một cách tối ưu bằng cách sử dụng phân tích LDL của ma trận Hessian được dẫn xuất từ các vector được rút ngẫu nhiên từ một tập hiệu chuẩn, và nhân ma trận trọng số và Hessian với một tích Kronecker của các ma trận trực giao ngẫu nhiên để đảm bảo sự không mạch lạc giữa ma trận trọng số và Hessian. Kết hợp hai bước này, QuIP thành công lượng tử hóa LLMs thành 2-bits với mất mát hiệu suất tối thiểu.

Để giảm thiểu hơn nữa các lỗi lượng tử hóa trong lượng tử hóa chỉ trọng số của LLMs, nhiều công trình xác định các trọng số nhạy cảm, có tác động quan trọng đến hiệu suất lượng tử hóa của LLMs, và lưu trữ những trọng số nhạy cảm này ở độ chính xác cao. Ví dụ, AWQ (Lin et al., 2023) lưu trữ 1% hàng đầu của các trọng số có tác động đáng kể nhất đến hiệu suất LLM ở độ chính xác cao, và tích hợp một phương pháp tỷ lệ theo kênh để xác định các yếu tố tỷ lệ tối ưu. Ở đây, "kênh" biểu thị các chiều riêng lẻ hoặc bản đồ đặc trưng trong mô hình. Tương tự như AWQ, OWQ (Lee et al., 2024) lưu trữ các trọng số nhạy cảm với các outlier kích hoạt ở độ chính xác cao, và lượng tử hóa các trọng số không nhạy cảm khác. Khác với OWQ, SpQR (Dettmers et al., 2024) sử dụng lỗi L2 giữa các dự đoán gốc và đã lượng tử hóa như một chỉ số độ nhạy cảm trọng số. Hơn nữa, SqueezeLLM (Kim et al., 2023b) giới thiệu một thuật toán cụm trọng số dựa trên độ nhạy cảm, sử dụng các centroid k-means như các giá trị trọng số đã lượng tử hóa, để xác định các trọng số nhạy cảm. Độ nhạy cảm được xấp xỉ bởi ma trận Hessian của trọng số. Sau đó, SqueezeLLM lưu trữ các trọng số nhạy cảm trong một định dạng thưa thớt hiệu quả, và lượng tử hóa các trọng số khác. SqueezeLLM lượng tử hóa LLMs trong 3-bit, và đạt được tăng tốc hơn 2× so với baseline FP16.

3.2.2 Lượng tử hóa Trọng số-Kích hoạt
Bên cạnh các công trình tập trung vào lượng tử hóa chỉ trọng số trong LLMs, có rất nhiều nghiên cứu tập trung chủ yếu vào lượng tử hóa trọng số-kích hoạt trong LLMs. Ví dụ, ZeroQuant (Yao et al., 2022) là công trình đầu tiên thực hiện lượng tử hóa trọng số-kích hoạt cho LLMs, sử dụng lượng tử hóa theo nhóm cho trọng số và lượng tử hóa theo token cho kích hoạt, và giảm độ chính xác cho trọng số và kích hoạt của LLMs xuống INT8.

LLMs có các outlier trong kích hoạt, và hiệu suất của LLMs giảm nhiều, nếu những kích hoạt với outlier này được lượng tử hóa trực tiếp. Các công trình gần đây cố gắng xử lý những outlier này một cách đặc biệt để giảm lỗi lượng tử hóa trong lượng tử hóa trọng số-kích hoạt. Ví dụ, LLM.int8() (Dettmers et al., 2022) lưu trữ những chiều đặc trưng outlier này ở độ chính xác cao, và sử dụng lượng tử hóa theo vector, gán các hằng số chuẩn hóa riêng biệt cho mỗi tích vô hướng trong phép nhân ma trận, để lượng tử hóa các đặc trưng khác. LLM.int8() lượng tử hóa trọng số và kích hoạt của LLMs thành 8-bit mà không có bất kỳ suy giảm hiệu suất nào. SmoothQuant (Xiao et al., 2023) thiết kế một phép biến đổi tỷ lệ theo kênh để làm mịn các outlier kích hoạt dựa trên phát hiện rằng các token khác nhau có biến thiên tương tự trên các kênh kích hoạt. RPTQ (Yuan et al., 2023a) phát hiện rằng phạm vi giá trị thay đổi rất nhiều giữa các kênh khác nhau, và tích hợp một phương pháp sắp xếp lại kênh, cụm và sắp xếp lại các kênh trong kích hoạt và sử dụng các tham số lượng tử hóa giống nhau để lượng tử hóa các giá trị trong mỗi cụm, vào trọng số lớp chuẩn hóa và lớp tuyến tính để giảm hiệu quả tác động của sự khác biệt phạm vi số giữa các kênh. OliVe (Guo et al., 2023) nghĩ rằng các outlier quan trọng hơn các giá trị bình thường, và sử dụng lượng tử hóa cặp outlier-victim (OVP) để xử lý các giá trị outlier cục bộ với chi phí phần cứng thấp và lợi ích hiệu suất đáng kể. OS+ (Wei et al., 2023) tiếp tục phát hiện rằng các outlier tập trung trong các kênh cụ thể và bất đối xứng. Dựa trên các phát hiện, OS+ kết hợp dịch chuyển theo kênh để loại bỏ tác động của bất đối xứng và tỷ lệ theo kênh để cân bằng phân phối các outlier. LLM-FP4 (Liu et al., 2023a) sử dụng các định dạng dấu phẩy động (cụ thể là FP8 và FP4) để giải quyết các hạn chế của lượng tử hóa số nguyên truyền thống (như INT8 và INT4) để xử lý các outlier. Hơn nữa, LLM-FP4 (Liu et al., 2023a) chỉ ra rằng các bit số mũ và phạm vi cắt xén là các yếu tố quan trọng ảnh hưởng đến hiệu suất của lượng tử hóa FP, và giới thiệu một khung dựa trên tìm kiếm để xác định độ lệch số mũ tối ưu và giá trị lượng tử hóa tối đa. OmniQuant (Shao et al., 2024b) xử lý các outlier kích hoạt bằng cách chuyển đổi một cách tương đương thách thức lượng tử hóa từ kích hoạt sang trọng số, và tối ưu hóa ngưỡng cắt xén để điều chỉnh các giá trị cực trị của trọng số.

3.2.3 Lượng tử hóa KV Cache
Với số lượng token đầu vào được hỗ trợ bởi LLMs tăng lên, việc sử dụng bộ nhớ của KV cache cũng tăng. Các nỗ lực gần đây bắt đầu tập trung vào lượng tử hóa kv cache để giảm dấu chân bộ nhớ của LLMs và tăng tốc suy luận của chúng. Ví dụ, KVQuant (Hooper et al., 2024) đề xuất một số phương pháp Lượng tử hóa KV Cache, như Lượng tử hóa Khóa Theo Kênh, Lượng tử hóa Khóa PreRoPE và lượng tử hóa kv cache Không đồng nhất, để thực hiện suy luận LLM với độ dài ngữ cảnh 10 triệu. Thông qua phân tích sâu về phân phối phần tử trong KV cache, KIVI (Liu et al., 2024) phát hiện rằng cache khóa nên được lượng tử hóa theo kênh, trong khi cache giá trị nên được lượng tử hóa theo token. Cuối cùng, KIVI thành công lượng tử hóa KV cache xuống 2 bits mà không cần tinh chỉnh. WKVQuant (Yue et al., 2024) trình bày một phương pháp sáng tạo để lượng tử hóa các mô hình ngôn ngữ lớn (LLMs) bằng cách tích hợp lượng tử hóa chỉ quá khứ để tinh chỉnh các tính toán attention, sử dụng chiến lược lượng tử hóa hai chiều để quản lý phân phối cache key/value (KV) một cách hiệu quả, và sử dụng chính quy hóa tái tạo lại chéo khối để tối ưu hóa tham số. Phương pháp này cho phép lượng tử hóa cả trọng số và KV cache, dẫn đến tiết kiệm bộ nhớ cạnh tranh với lượng tử hóa trọng số-kích hoạt, trong khi gần như khớp với mức hiệu suất của lượng tử hóa chỉ trọng số.

4 Tỉa cành
Tỉa cành (LeCun et al., 1989) là một kỹ thuật mạnh mẽ để giảm kích thước hoặc độ phức tạp của một mô hình bằng cách loại bỏ các thành phần dư thừa. Tỉa cành có thể được chia thành Tỉa cành Không có cấu trúc, Tỉa cành Bán cấu trúc và Tỉa cành Có cấu trúc. Tỉa cành có cấu trúc loại bỏ toàn bộ các thành phần như nơ-ron, đầu attention hoặc lớp dựa trên các quy tắc cụ thể trong khi bảo tồn cấu trúc mạng tổng thể. Mặt khác, tỉa cành không có cấu trúc tỉa các tham số riêng lẻ, dẫn đến một cấu trúc thưa thớt bất thường. Tỉa cành bán cấu trúc là một phương pháp nằm giữa tỉa cành có cấu trúc và tỉa cành không có cấu trúc, có khả năng đạt được tỉa cành tinh vi và chính quy hóa cấu trúc đồng thời. Nó tỉa các tham số một phần dựa trên các mẫu cụ thể thay vì toàn bộ kênh, bộ lọc hoặc nơ-ron, làm cho nó trở thành một dạng tỉa cành có cấu trúc tinh vi. Bảng 2 cho thấy hiệu suất của nhiều phương pháp tỉa cành LLM đại diện.

4.1 Tỉa cành Không có cấu trúc
Tỉa cành không có cấu trúc bảo tồn hiệu suất của mô hình được tỉa, do đó, các công trình liên quan đến tỉa cành không có cấu trúc của LLMs thường bỏ qua việc huấn luyện lại để khôi phục hiệu suất. Tuy nhiên, tỉa cành không có cấu trúc làm cho mô hình được tỉa trở nên bất thường, đòi hỏi xử lý đặc biệt hoặc tối ưu hóa phần mềm để tăng tốc suy luận. Một phương pháp sáng tạo trong lĩnh vực này là SparseGPT (Frantar and Alistarh, 2023), giới thiệu một chiến lược tỉa cành một lần mà không cần huấn luyện lại. SparseGPT định khung tỉa cành như một bài toán hồi quy thưa thớt rộng lớn và giải quyết nó bằng cách sử dụng một bộ giải hồi quy thưa thớt xấp xỉ. SparseGPT đạt được độ thưa thớt không có cấu trúc đáng kể, thậm chí lên đến hơn 50% trên các mô hình GPT lớn nhất như OPT-175B và BLOOM-176B, với sự gia tăng tối thiểu trong perplexity. Để giảm chi phí về quá trình cập nhật trọng số cần thiết bởi SparseGPT, Wanda (Sun et al., 2024) đạt được độ thưa thớt mô hình bằng cách tỉa các trọng số có độ lớn nhỏ nhất nhân với chuẩn của kích hoạt đầu vào tương ứng, mà không cần huấn luyện lại hoặc cập nhật trọng số. Để giảm thiểu hơn nữa các lỗi gây ra bởi tỉa cành trong khi duy trì mức độ thưa thớt tổng thể mong muốn, SAMSP (Shao et al., 2024a) sử dụng ma trận Hessian như một chỉ số để đánh giá độ nhạy cảm ma trận trọng số, và điều chỉnh động việc phân bổ độ thưa thớt dựa trên độ nhạy cảm. Hơn nữa, DSnoT (Zhang et al., 2024) giảm thiểu lỗi tái tạo giữa các mô hình dày đặc và thưa thớt thông qua tỉa cành trọng số lặp đi lặp lại và phát triển trên các LLM thưa thớt để nâng cao hiệu suất LLM trên các tỷ lệ độ thưa thớt khác nhau, đặc biệt ở mức độ thưa thớt cao. Để cung cấp hỗ trợ phần cứng cho việc xử lý tỉa cành không có cấu trúc trên phần cứng GPU Tensor Core, Flash-LLM (Xia et al., 2023) giới thiệu một phương pháp nhân ma trận thưa thớt không có cấu trúc, tải ma trận trọng số ở định dạng thưa thớt từ bộ nhớ toàn cục và tái tạo chúng ở định dạng dày đặc trong các bộ đệm tốc độ cao trên chip để tính toán bằng tensor core.

4.2 Tỉa cành Có cấu trúc
So với tỉa cành không có cấu trúc, tỉa cành có cấu trúc cung cấp lợi thế không phụ thuộc vào phần cứng, cho phép suy luận được tăng tốc trên phần cứng truyền thống sau tỉa cành. Tuy nhiên, việc loại bỏ các thành phần lớn hơn và có thể quan trọng hơn trong tỉa cành có cấu trúc có thể dẫn đến suy giảm hiệu suất, thường yêu cầu tinh chỉnh tham số hiệu quả để phục hồi. Chúng tôi chia các công trình tỉa cành có cấu trúc LLMs thành một số nhóm dựa trên chỉ số tỉa cành: Tỉa cành Dựa trên Mất mát, Tỉa cành Dựa trên Độ lớn, Tỉa cành Dựa trên Chính quy hóa.

Tỉa cành Dựa trên Mất mát (Molchanov et al., 2019) đánh giá tầm quan trọng của một đơn vị tỉa cành bằng cách đo tác động của nó đến mất mát hoặc thông tin gradient (ví dụ: đạo hàm bậc nhất hoặc bậc hai của mất mát). Ví dụ, LLM-Pruner (Ma et al., 2023) giới thiệu tỉa cành có cấu trúc một lần trên LLMs dựa trên thông tin gradient. Cụ thể, LLM-Pruner xác định các cấu trúc phụ thuộc thông qua một thuật toán phát hiện phụ thuộc và chọn các nhóm tỉa cành tối ưu bằng thông tin gradient, thay vì chỉ dựa vào thay đổi mất mát, theo cách bất khả tri tác vụ. Khác với LLM-Pruner, tập trung vào thu hẹp chiều rộng của LLMs, Shortened LLaMA (Kim et al., 2024) giới thiệu tỉa cành chiều sâu một lần trên LLMs. Shortened LLaMA chọn khối Transformer làm đơn vị có thể tỉa, và tỉa những khối Transformer không quan trọng, nơi tầm quan trọng của các khối Transformer được đánh giá bằng mất mát và đạo hàm bậc hai của nó. Sau Tỉa cành, cả LLM-Pruner và Shortened LLaMA đều sử dụng LoRA để nhanh chóng khôi phục hiệu suất của mô hình được tỉa.

Tỉa cành Dựa trên Độ lớn (Han et al., 2015) bao gồm việc đưa ra một chỉ số heuristic dựa trên độ lớn của các đơn vị tỉa cành, và sử dụng chỉ số để đánh giá tầm quan trọng của các đơn vị tỉa cành, sau đó tỉa những đơn vị có điểm số dưới ngưỡng được xác định trước. Ví dụ, FLAP (An et al., 2024) sử dụng một chỉ số biến động có cấu trúc để đánh giá và xác định các cột trong ma trận trọng số phù hợp để tỉa cành, đo sự biến thiên của mỗi đặc trưng đầu vào so với một giá trị cơ sở để ước tính tác động của việc loại bỏ một cột trọng số. Ngoài ra, FLAP sử dụng tìm kiếm cấu trúc thích ứng để tối ưu hóa nén mô hình toàn cục, và khôi phục hiệu suất của mô hình sau tỉa cành thông qua một cơ chế bù độ lệch cơ sở, tránh nhu cầu tinh chỉnh. Để duy trì hơn nữa hiệu suất của mô hình được tỉa, SliceGPT (Ashkboos et al., 2024) tận dụng tính bất biến tính toán của mạng transformer và tối ưu hóa quá trình tỉa cành thông qua Phân tích Thành phần Chính (PCA). Cụ thể, SliceGPT sử dụng PCA như chỉ số tỉa cành, áp dụng nó ở mỗi lớp của mạng transformer để chiếu ma trận tín hiệu lên các thành phần chính của nó và loại bỏ các cột hoặc hàng không đáng kể từ các ma trận trọng số được biến đổi, cuối cùng nhằm nén mô hình một cách hiệu quả.

Tỉa cành Dựa trên Chính quy hóa (Wen et al., 2016) thường thêm một số hạng chính quy hóa (ví dụ: chính quy hóa L0, L1 và L2) vào hàm mất mát để tạo ra độ thưa thớt cho LLMs. Ví dụ, Sheared LLaMA (Xia et al., 2024) sử dụng một cặp nhân tử Lagrange dựa trên mặt nạ tỉa cành để áp đặt các ràng buộc trên hình dạng mô hình được tỉa trực tiếp, do đó công thức hóa tỉa cành như một bài toán tối ưu hóa có ràng buộc. Thông qua giải quyết bài toán tối ưu hóa này, Sheared LLaMA tìm ra các mặt nạ tỉa cành tối ưu. Ngoài ra, Sheared LLaMA giới thiệu tải batch động, một chiến lược điều chỉnh việc tải dữ liệu huấn luyện dựa trên tỷ lệ giảm mất mát của mỗi lĩnh vực, nâng cao hiệu quả sử dụng dữ liệu trong quá trình huấn luyện.

Nhận xét 2. Tỉa cành có cấu trúc thường giảm kích thước mô hình bằng cách loại bỏ các tham số dư thừa, nhưng nó có thể làm giảm hiệu suất mô hình. Một phương pháp mới là kết hợp chưng cất kiến thức (Hinton et al., 2015) với tỉa cành có cấu trúc. Chưng cất kiến thức cho phép kiến thức được trích xuất từ LLM được chuyển đến một mô hình nhỏ hơn, giúp mô hình nhỏ hơn duy trì hiệu suất của nó trong khi giảm kích thước.

4.3 Tỉa cành Bán cấu trúc
Ngoài tỉa cành không có cấu trúc và tỉa cành có cấu trúc, có nhiều công trình sử dụng tỉa cành bán cấu trúc để tỉa các trọng số một phần của LLMs dựa trên các mẫu cụ thể. Độ thưa thớt N:M, nơi mỗi M phần tử liền kề để lại N phần tử khác không, là một ví dụ về tỉa cành bán cấu trúc. Ví dụ, E-Sparse (Li et al., 2023b) thực hiện độ thưa thớt N:M bằng cách giới thiệu entropy thông tin như một chỉ số để đánh giá tầm quan trọng tham số để nâng cao tầm quan trọng của trọng số tham số và chuẩn đặc trưng đầu vào. E-Sparse kết hợp xáo trộn naïve toàn cục và xáo trộn khối cục bộ để tối ưu hóa hiệu quả phân phối thông tin và giảm thiểu tác động của độ thưa thớt N:M đến độ chính xác LLM. Hơn nữa, nhiều công trình tỉa cành cũng có thể được tổng quát hóa thành các mẫu bán cấu trúc. Ví dụ, SparseGPT (Frantar and Alistarh, 2023) và Wanda (Sun et al., 2024) cũng khám phá độ thưa thớt N:M của LLMs. SparseGPT (Frantar and Alistarh, 2023) sử dụng phân vùng trọng số theo khối, với mỗi khối chứa M trọng số. Nó xác định và tỉa N trọng số với lỗi tái tạo thấp nhất (dựa trên thông tin Hessian), đảm bảo tỷ lệ độ thưa thớt N:M. Quá trình này lặp đi lặp lại tỉa và cập nhật trọng số mô hình, giải quyết từng khối một cho đến khi đạt được mức độ thưa thớt mong muốn trên toàn bộ mô hình. Wanda (Sun et al., 2024) đạt được tỉa cành N:M có cấu trúc bằng cách chia ma trận trọng số thành các nhóm M trọng số liên tiếp và tính điểm tầm quan trọng cho mỗi trọng số. Điểm được xác định bởi tích của độ lớn trọng số và chuẩn của kích hoạt đầu vào tương ứng. Trong mỗi nhóm trọng số, N trọng số có điểm cao nhất được giữ lại, trong khi phần còn lại được đặt về không, do đó thực hiện tỉa cành N:M có cấu trúc. Hơn nữa, việc chọn chiến lược tỉa cành tối ưu là quan trọng để tương thích với phần cứng mục tiêu. Ví dụ, Choquette et al. (2021) giới thiệu kiến trúc GPU Ampere Tensor Core (ví dụ: GPU A100) và đề xuất độ thưa thớt bán cấu trúc tinh vi 2:4 để tăng tốc Mạng Nơ-ron Thưa thớt trên phần cứng này. Tuy nhiên, việc triển khai hiện tại của kiến trúc Ampere chỉ hỗ trợ tỷ lệ 2:4, để lại các tỷ lệ khác mà không có tăng tốc.

Nhận xét 3. LLMs thường hoạt động tốt trên nhiều tác vụ, có nghĩa là chúng chứa vô số tham số cho các tác vụ khác nhau. Các phương pháp tỉa cành động (Xia et al., 2020) có thể tỉa động các phần khác nhau của mô hình dựa trên yêu cầu của tác vụ hiện tại để cung cấp hiệu suất tốt hơn trên các tác vụ cụ thể. Điều này giúp đạt được sự cân bằng giữa hiệu suất và hiệu quả.

Nhận xét 4. Đối với PTQ và tỉa cành, việc chuẩn bị một bộ dữ liệu hiệu chuẩn chất lượng cao để hỗ trợ cải thiện hiệu suất của LLMs được nén là rất quan trọng. Cụ thể, Williams và Aletras (2023) thực hiện một nghiên cứu thực nghiệm rộng rãi về tác động của dữ liệu hiệu chuẩn đối với các phương pháp nén mô hình, và phát hiện rằng hiệu suất của các tác vụ downstream có thể thay đổi đáng kể tùy thuộc vào dữ liệu hiệu chuẩn được chọn. Dữ liệu hiệu chuẩn chất lượng cao có thể cải thiện hiệu suất và độ chính xác của mô hình được nén, vì vậy việc lựa chọn và chuẩn bị dữ liệu hiệu chuẩn cẩn thận là cần thiết.

5 Chưng cất Kiến thức
Chưng cất Kiến thức (KD) (Hinton et al., 2015) là một kỹ thuật nhằm chuyển giao kiến thức từ một mô hình lớn và phức tạp (tức là mô hình giáo viên) sang một mô hình nhỏ hơn và đơn giản hơn (tức là mô hình học sinh). Chúng tôi phân loại những phương pháp này thành hai danh mục rõ ràng (Gu et al., 2024): KD Hộp đen, nơi chỉ có đầu ra của giáo viên có thể truy cập, thường từ LLMs nguồn đóng, và KD Hộp trắng, nơi các tham số hoặc phân phối đầu ra của giáo viên có sẵn, thường từ LLMs nguồn mở.

5.1 KD Hộp đen
KD Hộp đen thường nhắc LLM giáo viên tạo ra một bộ dữ liệu chưng cất để tinh chỉnh LM học sinh, do đó chuyển giao khả năng từ LLM giáo viên sang LM học sinh. Trong KD Hộp đen, các LLM giáo viên như ChatGPT (gpt-3.5-turbo) và GPT4 (OpenAI, 2024) thường được sử dụng, trong khi các LM nhỏ hơn (SLMs), như GPT-2 (Radford et al., 2019), T5 (Raffel et al., 2020), FlanT5 (Chung et al., 2024), và CodeT5 (Wang et al., 2021), thường được sử dụng như LM học sinh. Mặt khác, các nhà nghiên cứu phát hiện rằng LLMs có khả năng nổi lên, đề cập đến sự cải thiện đáng kể trong hiệu suất khi mô hình đạt đến một quy mô nhất định, thể hiện những khả năng đáng ngạc nhiên. Nhiều phương pháp KD Hộp đen cố gắng chưng cất các khả năng nổi lên từ LLMs sang LM học sinh, và chúng tôi giới thiệu ba phương pháp chưng cất khả năng nổi lên thường được sử dụng: Chưng cất Chuỗi Suy nghĩ (CoT), Chưng cất Học trong Ngữ cảnh (ICL), và Chưng cất Tuân theo Hướng dẫn (IF).

5.1.1 Chưng cất Chuỗi Suy nghĩ
CoT (Wei et al., 2022; Wang et al., 2023c) nhắc LLMs tạo ra các bước suy luận trung gian, cho phép chúng giải quyết các tác vụ lập luận phức tạp từng bước một. Li et al. (2024b) và Hsieh et al. (2023) sử dụng LLMs để nhắc tạo ra các giải thích và tận dụng khung học đa tác vụ để tăng cường khả năng lập luận của các mô hình nhỏ hơn trong khi nâng cao khả năng tạo ra giải thích của chúng. Magister et al. (2023) cho thấy rằng khả năng lập luận của LLMs có thể được chuyển sang SLMs thông qua chưng cất kiến thức, nhưng có sự đánh đổi giữa kích thước mô hình và bộ dữ liệu trong khả năng lập luận. Ho et al. (2023) sử dụng các kỹ thuật CoT zero-shot để nhắc LLMs tạo ra các lý luận đa dạng để làm phong phú bộ dữ liệu chưng cất cho các mô hình học sinh. Shridhar et al. (2023) chưng cất hai mô hình học sinh: một bộ phân tách vấn đề và một bộ giải vấn đề con, trong đó bộ phân tách vấn đề phân tách các vấn đề phức tạp thành một chuỗi các vấn đề con, và bộ giải vấn đề con giải quyết những vấn đề con này từng bước một. Wang et al. (2023a) kết hợp giải mã tương phản trong quá trình tạo lý luận cho các mô hình giáo viên và giải quyết các vấn đề đường tắt bằng cách giới thiệu một mục tiêu lập luận phản thực tế trong quá trình huấn luyện mô hình học sinh. Fu et al. (2023) chứng minh rằng việc tăng khả năng cụ thể cho tác vụ thông qua chưng cất có thể vô tình dẫn đến giảm hiệu suất trong việc giải quyết các vấn đề tổng quát, và tập trung vào cải thiện khả năng toán học của LM học sinh thông qua chưng cất. PaD (Zhu et al., 2024) nhắc LLMs tạo ra các lý luận Chương trình Suy nghĩ (PoT) thay vì các lý luận Chuỗi Suy nghĩ (CoT) để xây dựng bộ dữ liệu chưng cất, và tinh chỉnh SLMs với bộ dữ liệu chưng cất. Wang et al. (2023f) thiết lập một mô hình học tương tác đa vòng cho phép LM học sinh cung cấp phản hồi cho LLM giáo viên trong quá trình chưng cất, do đó thu được dữ liệu huấn luyện được điều chỉnh. Ngoài ra, DRA giới thiệu một cơ chế học tự phản tư, cho phép LM học sinh học từ những sai lầm của mình và nâng cao khả năng lập luận. Li et al. (2024c) phát hiện rằng dữ liệu tiêu cực được tạo từ LM giáo viên cũng có kiến thức lập luận, và hướng dẫn LM học sinh học kiến thức từ cả mẫu tiêu cực bên cạnh mẫu tích cực.

5.1.2 Chưng cất Học trong Ngữ cảnh
ICL (Dong et al., 2023; Wang et al., 2023b) sử dụng các lời nhắc có cấu trúc với mô tả tác vụ và ví dụ cho LLMs để học các tác vụ mới mà không cần cập nhật gradient. Huang et al. (2022) giới thiệu một phương pháp gọi là chưng cất học trong ngữ cảnh, chuyển giao khả năng học trong ngữ cảnh từ LLMs sang các mô hình nhỏ hơn bằng cách kết hợp các mục tiêu học trong ngữ cảnh với các mục tiêu mô hình hóa ngôn ngữ. Cụ thể, nó huấn luyện mô hình học sinh để cải thiện khả năng tổng quát hóa trên các tác vụ khác nhau bằng cách bắt chước các dự đoán nhãn mềm của mô hình giáo viên và các giá trị sự thật nhãn cứng. Ngoài ra, phương pháp kết hợp hai mô hình học few-shot: Meta In-context Tuning (Meta-ICT) và Multitask In-context Tuning (Multitask-ICT). Trong Meta-ICT, mô hình học sinh thích ứng với các tác vụ mới với học trong ngữ cảnh và hướng dẫn từ giáo viên. Ngược lại, Multitask-ICT coi tất cả các tác vụ mục tiêu như các tác vụ huấn luyện, sử dụng trực tiếp các ví dụ từ chúng trong chưng cất. Kết quả cho thấy rằng Multitask-ICT hiệu quả hơn, mặc dù yêu cầu tính toán tăng lên. AICD (Liu, 2024) tận dụng bản chất tự hồi quy của LLMs để thực hiện meta-teacher forcing trên CoTs trong ngữ cảnh, tối ưu hóa đồng thời khả năng của tất cả CoTs trong ngữ cảnh, do đó chưng cất khả năng học trong ngữ cảnh và lập luận vào các mô hình nhỏ hơn.

5.1.3 Chưng cất Tuân theo Hướng dẫn
IF (Ouyang et al., 2022; Brooks et al., 2023) nhằm tăng cường khả năng zero-shot của LLMs thông qua tinh chỉnh bằng cách sử dụng một tập hợp các cặp lời nhắc-phản hồi giống như hướng dẫn. Ví dụ, Lion (Jiang et al., 2023) nhắc LLM xác định và tạo ra các hướng dẫn "khó", sau đó được sử dụng để nâng cao khả năng của mô hình học sinh. LaMini-LM (Wu et al., 2024) phát triển một bộ sưu tập rộng lớn gồm 2,58 triệu hướng dẫn, bao gồm cả hướng dẫn hiện có và mới được tạo, và tinh chỉnh một loạt các mô hình đa dạng bằng cách sử dụng những hướng dẫn này. SELF-INSTRUCT (Wang et al., 2023d) sử dụng chính LM học sinh như giáo viên để tạo ra bộ dữ liệu tuân theo hướng dẫn, và tinh chỉnh chính học sinh với bộ dữ liệu. Selective Reflection-Tuning (Li et al., 2024a) tận dụng LLM giáo viên để phản tư và cải thiện dữ liệu hiện có, trong khi LM học sinh đánh giá và kết hợp một cách có chọn lọc những cải thiện này, do đó tăng chất lượng dữ liệu và khả năng tương thích với LM học sinh.

Nhận xét 5. Chưng cất Hộp đen sử dụng đầu ra của mô hình giáo viên như giám sát, nhưng đầu ra của mô hình giáo viên có thể không bao phủ tất cả các tình huống đầu vào có thể. Do đó, hiểu cách xử lý khả năng tổng quát hóa của mô hình học sinh trên dữ liệu chưa biết và cách tăng đa dạng dữ liệu là một lĩnh vực cần nghiên cứu thêm.

5.2 KD Hộp trắng
KD Hộp trắng cho phép LM học sinh hiểu sâu hơn về cấu trúc nội bộ và biểu diễn kiến thức của LLM giáo viên, thường dẫn đến cải thiện hiệu suất ở mức cao hơn. Một ví dụ đại diện là MINILLM (Gu et al., 2024), công trình đầu tiên nghiên cứu chưng cất từ các LLM tạo sinh nguồn mở. MINILLM sử dụng một mục tiêu divergence Kullback-Leibler ngược, phù hợp hơn cho KD trên các mô hình ngôn ngữ tạo sinh, để ngăn mô hình học sinh đánh giá quá cao các vùng xác suất thấp của phân phối giáo viên, và rút ra một phương pháp tối ưu hóa hiệu quả để học mục tiêu. Hơn nữa, GKD (Agarwal et al., 2024) khám phá chưng cất từ các mô hình tự hồi quy, nơi các mô hình ngôn ngữ tạo sinh là một tập con. GKD huấn luyện học sinh bằng đầu ra tự tạo, kết hợp phản hồi của giáo viên, và cho phép linh hoạt trong việc sử dụng các hàm mất mát khác nhau khi học sinh không thể sao chép hoàn toàn phân phối của giáo viên. Khác với các công trình trên, tập trung vào học phân phối giáo viên, TED (Liang et al., 2023) đề xuất một phương pháp chưng cất theo lớp nhận biết tác vụ, thiết kế các bộ lọc nhận biết tác vụ, căn chỉnh các biểu diễn ẩn của mô hình giáo viên và học sinh ở mỗi lớp trung gian, để giảm khoảng cách kiến thức giữa mô hình học sinh và giáo viên.

Nhận xét 6. Mặc dù chưng cất hộp trắng cho phép LM học sinh học kiến thức của LLM giáo viên sâu hơn so với chưng cất hộp đen, hiện tại, LLM nguồn mở hoạt động kém hơn so với những LLM nguồn đóng, hạn chế việc cải thiện hiệu suất LM học sinh trong chưng cất hộp trắng. Đây là một trong những yếu tố cằn cỗi cản trở sự phát triển của chưng cất hộp trắng. Một giải pháp khả thi là chưng cất kiến thức từ LLM nguồn đóng sang LLM nguồn mở thông qua chưng cất hộp đen, sau đó sử dụng chưng cất hộp trắng để chuyển giao kiến thức từ LLM nguồn mở sang LLM học sinh.

Nhận xét 7. Chưng cất hộp trắng thường liên quan đến việc hiểu và sử dụng cấu trúc nội bộ của LLMs, như kết nối lớp và cài đặt tham số. Một khám phá sâu hơn về các cấu trúc mạng khác nhau và tương tác giữa các lớp có thể cải thiện hiệu quả của chưng cất hộp trắng.

6 Phân tích Nhân tử Hạng thấp
Phân tích Nhân tử Hạng thấp (Srebro and Jaakkola, 2003) giảm một ma trận lớn thành các ma trận nhỏ hơn để tiết kiệm không gian và nỗ lực tính toán. Ví dụ, nó phân tích một ma trận lớn W thành hai ma trận nhỏ U và V (tức là W ~ = UV), trong đó U là m×k và V là k×n, với k nhỏ hơn nhiều so với m và n. Các công trình gần đây cố gắng sử dụng phân tích nhân tử hạng thấp để nén LLMs và đạt được thành công đáng kể trong lĩnh vực này. Ví dụ, LPLR (Saha et al., 2023) nén ma trận trọng số của LLMs thông qua phân tích nhân tử hạng thấp và độ chính xác thấp ngẫu nhiên. Cụ thể, LPLR xấp xỉ không gian cột của ma trận bằng các kỹ thuật phác thảo ngẫu nhiên, lượng tử hóa những cột này, và sau đó chiếu các cột gốc lên không gian đã lượng tử hóa này để tạo ra hai nhân tử hạng thấp được lưu trữ ở độ chính xác thấp. ASVD (Yuan et al., 2023b) phát hiện rằng phân phối kích hoạt có tác động đến hiệu suất nén. Để giải quyết vấn đề, ASVD đề xuất tỷ lệ ma trận trọng số với một ma trận đường chéo chứa các yếu tố tỷ lệ tương ứng với phân phối kích hoạt của các kênh đặc trưng đầu vào. Hơn nữa, ASVD gán tỷ lệ nén phù hợp nhất cho các lớp khác nhau bằng cách phân tích phân phối giá trị đơn lẻ trong ma trận trọng số của mỗi lớp, đảm bảo mất mát tối thiểu hiệu suất mô hình trong quá trình nén. Hơn nữa, Sharma et al. (2024) chứng minh rằng hiệu suất của LLMs có thể được cải thiện đáng kể bằng cách áp dụng Giảm Hạng Chọn lọc Lớp (LASER) cho các lớp cụ thể của mô hình Transformer. LASER bao gồm việc giảm hạng có chọn lọc các thành phần bậc cao hơn của ma trận trọng số, được cho là cải thiện khả năng xử lý dữ liệu huấn luyện hiếm của mô hình và khả năng chống lại việc diễn giải lại câu hỏi.

7 Thách thức và Hướng Tương lai
7.1 Các Phương pháp Tiên tiến hơn
Nghiên cứu về các kỹ thuật nén mô hình cho LLMs vẫn đang trong giai đoạn đầu. Những LLM được nén này, như được chứng minh trong các nghiên cứu trước (Frantar and Alistarh, 2023; Liu et al., 2023b; Ho et al., 2023), tiếp tục thể hiện khoảng cách hiệu suất đáng kể khi so sánh với các đối tác chưa nén của chúng. Bằng cách đi sâu vào các phương pháp nén mô hình tiên tiến hơn được thiết kế riêng cho LLMs, chúng ta có tiềm năng nâng cao hiệu suất của những LLM chưa nén này.

7.2 Mở rộng Các Phương pháp Nén Mô hình từ Các Mô hình Khác
Trong bài báo của chúng tôi, chúng tôi giới thiệu một số phương pháp nén mô hình đại diện cho LLMs. Tuy nhiên, nhiều phương pháp nén mô hình cổ điển vẫn phổ biến trong các mô hình nhỏ truyền thống. Ví dụ, vé số (Frankle and Carbin, 2019) và chia sẻ tham số (Savarese and Maire, 2019) là các phương pháp nén mô hình được sử dụng rộng rãi trong các mô hình nhỏ. Những phương pháp này vẫn có tiềm năng đáng kể trong kỷ nguyên LLMs. Công việc tương lai nên tập trung vào khám phá cách mở rộng những phương pháp nén này đến LLMs để đạt được nén thêm.

7.3 Suy luận và Triển khai LLM
Hiệu quả của LLMs được nén trong quá trình triển khai cũng là một lĩnh vực đáng kể để khám phá. Điều này bao gồm nhiều chỉ số đánh giá, bao gồm cường độ số học, kích thước bộ nhớ và thông lượng. Hơn nữa, chúng ta có thể sử dụng một công cụ phân tích, Mô hình Roofline (Williams et al., 2009), để đánh giá hiệu quả tài nguyên của LLMs được nén trên phần cứng cụ thể. Đánh giá hiệu quả triển khai của LLMs được nén trên phần cứng cụ thể có thể hướng dẫn các nhà nghiên cứu trong việc lựa chọn và phân tích ưu điểm và nhược điểm của các phương pháp nén mô hình khác nhau và tối ưu hóa thêm những phương pháp này.

7.4 Tác động của Quy luật Mở rộng
Quy luật mở rộng (Kaplan et al., 2020) nhấn mạnh tác động đáng kể của kích thước mô hình, kích thước bộ dữ liệu và tài nguyên tính toán đến hiệu suất của LLMs. Tuy nhiên, quy luật mở rộng đặt ra một thách thức cơ bản cho việc nén LLM, tức là có sự đánh đổi giữa kích thước mô hình và hiệu suất trong LLMs được nén. Việc đi sâu vào các cơ chế và lý thuyết làm nền tảng cho quy luật mở rộng là rất quan trọng để làm sáng tỏ và có thể vượt qua hạn chế này.

7.5 AutoML cho Nén LLM
Các kỹ thuật nén hiện có đã có tiến bộ đáng chú ý, nhưng chúng vẫn phụ thuộc nhiều vào thiết kế thủ công. Ví dụ, thiết kế các kiến trúc học sinh phù hợp cho chưng cất kiến thức đòi hỏi một lượng lớn nỗ lực con người. Để giảm sự phụ thuộc này vào thiết kế thủ công, một giải pháp khả thi là kết hợp các kỹ thuật Học Máy Tự động (AutoML) như Meta-Learning (Finn et al., 2017) và Tìm kiếm Kiến trúc Mạng nơ-ron (NAS) (Zoph and Le, 2017) với nén mô hình. Bằng cách kết hợp với các kỹ thuật AutoML, nén mô hình có thể tự động chọn các siêu tham số phù hợp và điều chỉnh kiến trúc và quy mô của các mô hình được nén, do đó giảm thiểu sự tham gia của con người và giảm chi phí liên quan. Hơn nữa, AutoML có thể xác định các chiến lược nén mô hình tối ưu được điều chỉnh theo yêu cầu tác vụ cụ thể, do đó nâng cao hơn nữa tỷ lệ nén mà không làm tổn hại đến hiệu suất mô hình.

7.6 Khả năng Giải thích của Nén LLM
Nghiên cứu trước (Stanton et al., 2021; Xu et al., 2021) đã nêu ra những lo ngại đáng kể về khả năng giải thích của các kỹ thuật nén mô hình được áp dụng cho Mô hình Ngôn ngữ Được huấn luyện trước (PLMs). Đáng chú ý, những thách thức tương tự này cũng mở rộng đến các phương pháp nén LLM. Ví dụ, chưng cất CoT có thể nâng cao hiệu suất lập luận của SLMs, nhưng cơ chế mà nó truyền đạt khả năng CoT vẫn chưa rõ ràng. Thách thức này nhấn mạnh tầm quan trọng của việc tích hợp khả năng giải thích với các phương pháp nén mô hình để tiến bộ của các ứng dụng nén LLM. Khả năng giải thích không chỉ làm rõ những thay đổi và đánh đổi trong quá trình nén mà còn nâng cao hiệu quả và độ chính xác. Ngoài ra, khả năng diễn giải giúp đánh giá hiệu suất của mô hình được nén để đảm bảo nó phù hợp với yêu cầu thực tế.

8 Kết luận
Trong khảo sát, chúng tôi đã khám phá các kỹ thuật nén mô hình cho LLMs. Phạm vi bao quát của chúng tôi bao gồm các phương pháp nén, chỉ số và bộ dữ liệu điểm chuẩn. Bằng cách đi sâu vào nén LLM, chúng tôi đã làm nổi bật những thách thức và cơ hội của nó. Khảo sát này nhằm trở thành một tài liệu tham khảo có giá trị, cung cấp hiểu biết về bối cảnh hiện tại và thúc đẩy việc khám phá liên tục chủ đề quan trọng này.

Lời cảm ơn
Chúng tôi muốn cảm ơn các nhà phản biện ẩn danh và Biên tập viên Hành động cho phản hồi có giá trị và thảo luận của họ. Công việc của Jian Li được hỗ trợ một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62106257). Công việc của Yong Liu được hỗ trợ một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62076234), Chương trình Nhà khoa học Trẻ Xuất sắc Bắc Kinh (Số BJJWZYJH012019100020098), Kế hoạch Hợp tác Sinh thái Đổi mới Unicom, và Quỹ Populus Grove CCF-Huawei.

Tài liệu tham khảo
[Phần tài liệu tham khảo được dịch tương tự, giữ nguyên cấu trúc và định dạng...]
