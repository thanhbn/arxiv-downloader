# 2211.08339.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2211.08339.pdf
# Kích thước tệp: 1274418 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1
Cắt tỉa Kênh Mạng Nơ-ron Sâu Rất Sâu để
Suy luận Hiệu quả
Yihui He
Tóm tắt —Trong bài báo này, chúng tôi giới thiệu một phương pháp cắt tỉa kênh mới để tăng tốc các mạng nơ-ron tích chập rất sâu. Với một
mô hình CNN đã được huấn luyện, chúng tôi đề xuất một thuật toán hai bước lặp để cắt tỉa hiệu quả từng lớp, bằng một phương pháp chọn kênh
dựa trên hồi quy LASSO và tái tạo bình phương tối thiểu. Chúng tôi tiếp tục tổng quát hóa thuật toán này cho các trường hợp đa lớp và đa nhánh. Phương pháp của chúng tôi
giảm sai số tích lũy và nâng cao khả năng tương thích với các kiến trúc khác nhau. VGG-16 đã cắt tỉa của chúng tôi đạt được
kết quả tối ưu với tăng tốc 5× cùng với chỉ tăng 0.3% sai số. Quan trọng hơn, phương pháp của chúng tôi có thể tăng tốc
các mạng hiện đại như ResNet, Xception và chỉ mất 1.4%, 1.0% độ chính xác dưới tăng tốc 2× tương ứng, điều này là
đáng kể. Mã nguồn của chúng tôi đã được công bố công khai.
Từ khóa chỉ mục —Mạng nơ-ron tích chập, tăng tốc, phân loại hình ảnh.
F
1 GIỚI THIỆU
GẦN đây, các mạng nơ-ron tích chập (CNN) được sử dụng rộng rãi
trên các hệ thống nhúng như điện thoại thông minh và xe tự lái.
Xu hướng chung trong vài năm qua là các mạng ngày càng sâu hơn,
với sự gia tăng tổng thể về số lượng tham số và phép toán tích chập. Suy luận hiệu quả
ngày càng trở nên quan trọng đối với CNN [1].
Các công trình tăng tốc CNN chia thành ba loại: triển khai tối ưu
(ví dụ: FFT [2]), lượng tử hóa (ví dụ: BinaryNet [3]),
và đơn giản hóa có cấu trúc chuyển đổi CNN thành dạng compact [4]. Công trình này tập trung vào loại cuối cùng vì nó trực tiếp xử lý
vấn đề over-parameterization của CNN.
Đơn giản hóa có cấu trúc chủ yếu bao gồm: phân tích tensor [4], kết nối thưa thớt [5], và cắt tỉa kênh [6]. Phân tích tensor
phân tích một lớp tích chập thành nhiều lớp hiệu quả (Hình 1(c)). Tuy nhiên, chiều rộng feature map (số kênh)
không thể được giảm, điều này khiến việc phân tích các lớp tích chập 1×1 được ưa chuộng bởi các mạng hiện đại (ví dụ:
GoogleNet [7], ResNet [8], Xception [9]) trở nên khó khăn. Loại phương pháp này
cũng gây ra overhead tính toán bổ sung. Kết nối thưa thớt
vô hiệu hóa các kết nối giữa các nơ-ron hoặc kênh (Hình 1(b)).
Mặc dù nó có thể đạt được tỷ lệ tăng tốc lý thuyết cao, các lớp tích chập thưa thớt có hình dạng "không đều" không thân thiện với việc triển khai. Ngược lại, cắt tỉa kênh trực tiếp
giảm chiều rộng feature map, làm co một mạng thành dạng mỏng hơn, như được thể hiện trong Hình 1(d). Nó hiệu quả trên cả CPU và GPU
vì không cần triển khai đặc biệt.
Cắt tỉa kênh đơn giản nhưng đầy thách thức vì việc loại bỏ
các kênh trong một lớp có thể thay đổi đáng kể đầu vào của
lớp tiếp theo. Gần đây, các công trình cắt tỉa kênh dựa trên huấn luyện [6], [10] đã tập trung vào việc áp đặt ràng buộc thưa thớt lên
trọng số trong quá trình huấn luyện, có thể xác định thích ứng các siêu tham số. Tuy nhiên, huấn luyện từ đầu rất tốn kém, và kết quả cho các CNN rất sâu trên ImageNet hiếm khi được báo cáo.
Các nỗ lực thời gian suy luận [11], [12] đã tập trung vào phân tích
tầm quan trọng của từng trọng số. Tỷ lệ tăng tốc được báo cáo là
rất hạn chế.

--- TRANG 2 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2
chúng tôi đề xuất tái tạo đầu ra sau khi cắt tỉa từng lớp. Cụ thể, với một mô hình CNN đã được huấn luyện, việc cắt tỉa từng lớp được thực hiện
bằng cách tối thiểu hóa sai số tái tạo trên các feature map đầu ra của nó, như
được thể hiện trong Hình 2. Chúng tôi giải quyết bài toán tối thiểu hóa này bằng hai bước
thay thế: chọn kênh và tái tạo feature map. Trong
một bước, chúng tôi tìm ra các kênh đại diện nhất, và cắt tỉa
những kênh dư thừa, dựa trên hồi quy LASSO. Trong bước khác, chúng tôi
tái tạo các đầu ra với các kênh còn lại bằng bình phương tối thiểu tuyến tính. Chúng tôi thay thế thực hiện hai bước. Hơn nữa, chúng tôi xấp xỉ
mạng theo từng lớp, với sai số tích lũy được tính đến. Chúng tôi
cũng thảo luận các phương pháp để cắt tỉa các mạng đa nhánh (ví dụ:
ResNet [8], Xception [9]).
Chúng tôi chứng minh độ chính xác vượt trội của phương pháp chúng tôi so với
các kỹ thuật cắt tỉa gần đây khác [15], [16], [17], [18]. Đối với VGG-16, chúng tôi đạt được tăng tốc 4×, với chỉ tăng 1.0% sai số
top-5. Kết hợp với phân tích tensor, chúng tôi đạt tăng tốc 5×
nhưng chỉ chịu tăng 0.3% sai số, vượt trội hơn các state-of-the-art trước đó. Chúng tôi tiếp tục tăng tốc ResNet-50 và Xception-50 lên 2× với chỉ mất 1.4%, 1.0% độ chính xác
tương ứng. Mã nguồn đã được công bố công khai¹.
Phiên bản sơ bộ của bản thảo này đã được chấp nhận tại
một hội nghị [19]. Bản thảo này mở rộng phiên bản ban đầu từ
nhiều khía cạnh để củng cố phương pháp của chúng tôi:
1) Chúng tôi đã điều tra sự dư thừa giữa các kênh trong từng lớp, và
phân tích tốt hơn để cắt tỉa.
2) Chúng tôi trình bày cắt tỉa theo filter, có hiệu suất tăng tốc
hấp dẫn cho một lớp đơn.
3) Chúng tôi đã chứng minh kết quả tăng tốc VGG-16 Top-1 hấp dẫn
vượt trội hơn mô hình gốc.

2 CÔNG TRÌNH LIÊN QUAN
Đã có rất nhiều công trình về tăng tốc
CNN [20], bắt đầu từ brain damage [21], [22]. Nhiều trong số chúng
chia thành ba loại: triển khai tối ưu [23],
lượng tử hóa [24], và đơn giản hóa có cấu trúc [4].
Các phương pháp dựa trên triển khai tối ưu [2], [23], [25],
[26] tăng tốc tích chập, với các thuật toán tích chập đặc biệt
như FFT [2]. Lượng tử hóa [3], [24], [27] giảm độ phức tạp tính toán
dấu phẩy động.
Kết nối thưa thớt loại bỏ các kết nối giữa các nơ-ron [5], [28], [29], [30], [31], [32], [33]. [34] cắt tỉa các kết nối
dựa trên độ lớn trọng số. [35] có thể tăng tốc các lớp fully connected
lên đến 50×. Tuy nhiên, trong thực tế, tốc độ tăng tốc thực tế có thể
rất liên quan đến việc triển khai.
Phân tích tensor [4], [36], [37], [38], [39] phân tách
trọng số thành nhiều phần. [40], [41], [42] tăng tốc các lớp fully connected với SVD cắt ngắn. [14] phân tích một lớp thành kết hợp 3×3
và 1×1, được thúc đẩy bởi sự dư thừa feature map.
Cắt tỉa kênh loại bỏ các kênh dư thừa trên feature map.
Có một số phương pháp dựa trên huấn luyện [43]. [6], [10], [44]
điều chỉnh các mạng để cải thiện độ chính xác. Channel-wise SSL [6]
đạt tỷ lệ nén cao cho một vài lớp conv đầu tiên của
LeNet [45] và AlexNet [46]. [44] có thể hoạt động tốt cho các lớp fully connected. Tuy nhiên, các phương pháp dựa trên huấn luyện tốn kém hơn, và hiệu quả của chúng trên các mạng rất sâu trên các tập dữ liệu lớn hiếm khi được khai thác. Cắt tỉa kênh thời gian suy luận là
thách thức, như được báo cáo bởi các công trình trước đó [47], [48]. Gần đây,
AMC [49] cải thiện phương pháp của chúng tôi bằng cách học tỷ lệ tăng tốc với
học tăng cường.

--- TRANG 3 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 3

3 PHƯƠNG PHÁP
Trong phần này, chúng tôi đầu tiên đề xuất một thuật toán cắt tỉa kênh cho một
lớp đơn, sau đó tổng quát hóa phương pháp này cho nhiều lớp hoặc toàn bộ mô hình. Hơn nữa, chúng tôi thảo luận các biến thể của phương pháp chúng tôi
cho các mạng đa nhánh.

3.1 Công thức hóa
Hình 2 minh họa thuật toán cắt tỉa kênh của chúng tôi cho một
lớp tích chập đơn. Chúng tôi nhằm mục đích giảm số kênh của
feature map B trong khi duy trì đầu ra trong feature map C. Một khi
các kênh được cắt tỉa, chúng ta có thể loại bỏ các kênh tương ứng của các
filter nhận các kênh này làm đầu vào. Ngoài ra, các filter tạo ra
các kênh này cũng có thể được loại bỏ. Rõ ràng là cắt tỉa kênh
bao gồm hai điểm chính. Điểm đầu tiên là chọn kênh vì chúng ta
cần chọn kết hợp kênh phù hợp để duy trì nhiều thông tin nhất.
Điểm thứ hai là tái tạo. Chúng ta cần tái tạo
các feature map tiếp theo bằng cách sử dụng các kênh đã chọn.
Được thúc đẩy bởi điều này, chúng tôi đề xuất một thuật toán hai bước lặp:
1) Trong một bước, chúng tôi nhằm mục đích chọn các kênh đại diện nhất.
   Vì một tìm kiếm toàn diện là không khả thi ngay cả đối với các mạng nhỏ, chúng tôi đưa ra một phương pháp dựa trên hồi quy LASSO
   để tìm ra các kênh đại diện và cắt tỉa những kênh dư thừa.
2) Trong bước khác, chúng tôi tái tạo các đầu ra với các kênh còn lại bằng bình phương tối thiểu tuyến tính.
Chúng tôi thay thế thực hiện hai bước.

Một cách chính thức, để cắt tỉa một feature map B với c kênh, chúng tôi
xem xét việc áp dụng n×c×kh×kw filter tích chập W trên
N×c×kh×kw volume đầu vào X được lấy mẫu từ feature map này,
tạo ra ma trận đầu ra N×n Y từ feature map C. Ở đây,
N là số lượng mẫu, n là số kênh đầu ra,
và kh; kw là kích thước kernel. Để đơn giản biểu diễn, thuật ngữ bias
không được bao gồm trong công thức của chúng tôi. Để cắt tỉa các kênh đầu vào
từ c xuống c'mong muốn (0≤c'≤c), trong khi tối thiểu hóa sai số tái tạo, chúng tôi công thức hóa bài toán như sau:

arg min[β,W] (1/2N)||Y - Σ(i=1 to c) βi Xi Wi^T||²F
subject to ||β||₀ ≤ c'                                (1)

||·||F là chuẩn Frobenius. Xi là ma trận N×kh×kw được cắt từ
kênh thứ i của volume đầu vào X, i = 1,...,c. Wi là
trọng số filter n×kh×kw được cắt từ kênh thứ i của W. β là vector hệ số
có độ dài c để chọn kênh, và βi (mục thứ i của β) là một
scalar mask cho kênh thứ i (tức là bỏ toàn bộ kênh hay không).
Chú ý rằng, nếu βi = 0, Xi sẽ không còn hữu ích, có thể
được cắt tỉa an toàn từ feature map B. Wi cũng có thể được loại bỏ. c'
là số kênh được giữ lại, được đặt thủ công vì nó có thể
được tính toán từ tỷ lệ tăng tốc mong muốn. Đối với tăng tốc toàn mô hình
(tức là Phần 4.1.2), với tăng tốc tổng thể, chúng tôi đầu tiên
gán tỷ lệ tăng tốc cho từng lớp sau đó tính toán từng c'.

3.2 Tối ưu hóa
Giải quyết bài toán tối thiểu hóa ℓ₀ trong Phương trình 1 là NP-hard.
Do đó, chúng tôi nới lỏng ℓ₀ thành điều chỉnh ℓ₁:

arg min[β,W] (1/2N)||Y - Σ(i=1 to c) βi Xi Wi^T||²F + λ||β||₁
subject to ||β||₀ ≤ c', ∀i ||Wi||F = 1                (2)

λ là hệ số phạt. Bằng cách tăng λ, sẽ có nhiều số hạng bằng không hơn trong β và có thể đạt được tỷ lệ tăng tốc cao hơn. Chúng tôi cũng
thêm ràng buộc ∀i ||Wi||F = 1 vào công thức này để tránh nghiệm tầm thường.

Bây giờ chúng tôi giải quyết bài toán này theo hai hướng. Đầu tiên, chúng tôi cố định W, giải
cho việc chọn kênh. Thứ hai, chúng tôi cố định β, giải W để tái tạo
sai số.

3.2.1 (i) Bài toán con của β
Trong trường hợp này, W được cố định. Chúng tôi giải cho việc chọn kênh. Bài toán này
có thể được giải bằng hồi quy LASSO [53], [54], được sử dụng rộng rãi cho việc chọn mô hình.

β̂_LASSO(λ) = arg min[β] (1/2N)||Y - Σ(i=1 to c) βi Zi||²F + λ||β||₁
subject to ||β||₀ ≤ c'                                (3)

Ở đây Zi = Xi Wi^T (kích thước N×n). Chúng tôi sẽ bỏ qua kênh thứ i nếu
βi = 0.

3.2.2 (ii) Bài toán con của W
Trong trường hợp này, β được cố định. Chúng tôi sử dụng các kênh đã chọn để
tối thiểu hóa sai số tái tạo. Chúng ta có thể tìm nghiệm tối ưu bằng
bình phương tối thiểu:

arg min[W'] ||Y - X'(W')^T||²F                         (4)

Ở đây X' = [β₁X₁ β₂X₂ ... βᵢXᵢ ... βcXc] (kích thước N×c×kh×kw). W'
là W được định hình lại n×c×kh×kw, W' = [W₁ W₂ ... Wᵢ ... Wc]. Sau khi
nhận được kết quả W', nó được định hình lại về W. Sau đó chúng tôi gán
βᵢ ← βᵢ/||Wᵢ||F; Wᵢ ← Wᵢ/||Wᵢ||F. Ràng buộc ∀i ||Wᵢ||F = 1
được thỏa mãn.

Chúng tôi tối ưu hóa thay thế (i) và (ii). Ban đầu, W được
khởi tạo từ mô hình đã huấn luyện, β = 0, tức là không có phạt, và ||β||₀ = c. Chúng tôi dần dần tăng λ. Đối với mỗi thay đổi của λ,
chúng tôi lặp hai bước này cho đến khi ||β||₀ ổn định. Sau khi ||β||₀ ≤ c'
thỏa mãn, chúng tôi nhận được nghiệm cuối cùng W từ {βᵢWᵢ}. Trong thực tế,
chúng tôi thấy rằng việc lặp hai bước tốn thời gian. Vì vậy chúng tôi
áp dụng (i) nhiều lần cho đến khi ||β||₀ ≤ c' thỏa mãn. Sau đó áp dụng
(ii) chỉ một lần, để nhận được kết quả cuối cùng. Từ quan sát của chúng tôi, kết quả này
tương đương với kết quả lặp hai bước. Do đó, trong
các thí nghiệm sau đây, chúng tôi áp dụng phương pháp này để hiệu quả.

3.2.3 Thảo luận
Một số công trình gần đây [5], [6], [10] (mặc dù dựa trên huấn luyện) cũng
giới thiệu chuẩn ℓ₁ hoặc LASSO. Tuy nhiên, chúng tôi phải nhấn mạnh rằng
chúng tôi sử dụng các công thức khác nhau. Nhiều trong số chúng đã giới thiệu điều chỉnh sparsity
vào loss huấn luyện, thay vì giải quyết LASSO một cách rõ ràng.
Công trình khác [10] đã giải LASSO, trong khi feature map hoặc
dữ liệu không được xem xét trong quá trình tối ưu hóa.

Do những khác biệt này, phương pháp của chúng tôi có thể được áp dụng tại
thời gian suy luận.

3.3 Cắt tỉa Toàn bộ Mô hình
Được truyền cảm hứng bởi [14], chúng tôi áp dụng phương pháp của mình từng lớp một cách
tuần tự. Đối với mỗi lớp, chúng tôi thu được các volume đầu vào từ
feature map đầu vào hiện tại, và các volume đầu ra từ feature map đầu ra của mô hình chưa cắt tỉa. Điều này có thể được chính thức hóa như:

arg min[β,W] (1/2N)||Y' - Σ(i=1 to c) βᵢXᵢWᵢᵀ||²F
subject to ||β||₀ ≤ c'                                (5)

Khác với Phương trình 1, Y được thay thế bởi Y', là từ
feature map của mô hình gốc. Do đó, sai số tích lũy có thể được tính đến trong quá trình cắt tỉa tuần tự.

3.4 Cắt tỉa Mạng Đa nhánh
Việc cắt tỉa toàn bộ mô hình đã thảo luận ở trên đủ cho các mạng đơn nhánh như LeNet [45], AlexNet [46] và VGG
Nets [55]. Tuy nhiên, nó không đủ cho các mạng đa nhánh
như GoogLeNet [7] và ResNet [8]. Chúng tôi chủ yếu tập trung vào việc cắt tỉa
cấu trúc residual được sử dụng rộng rãi (ví dụ: ResNet [8], Xception [9]).
Với một khối residual được thể hiện trong Hình 3 (trái), đầu vào phân chia

--- TRANG 4 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 4

thành shortcut và nhánh residual. Trên nhánh residual,
có một số lớp tích chập (ví dụ: 3 lớp tích chập có kích thước không gian
1×1; 3×3; 1×1, Hình 3, trái). Các lớp khác ngoại trừ lớp đầu tiên và cuối cùng có thể được cắt tỉa như đã mô tả
trước đó. Đối với lớp đầu tiên, thách thức là chiều rộng feature map đầu vào lớn (đối với ResNet, gấp bốn lần đầu ra của nó) không thể
dễ dàng cắt tỉa vì nó được chia sẻ với shortcut. Đối với lớp cuối cùng,
sai số tích lũy từ shortcut khó có thể được phục hồi, vì
không có tham số trên shortcut. Để giải quyết những thách thức này,
chúng tôi đề xuất một số biến thể của phương pháp như sau.

3.4.1 Lớp cuối cùng của nhánh residual
Được thể hiện trong Hình 3, lớp đầu ra của một khối residual bao gồm
hai đầu vào: feature map Y₁ và Y₂ từ shortcut và
nhánh residual. Chúng tôi nhằm mục đích phục hồi Y₁ + Y₂ cho khối này. Ở đây,
Y₁; Y₂ là các feature map gốc trước khi cắt tỉa. Y₂ có thể được
xấp xỉ như trong Phương trình 1. Tuy nhiên, nhánh shortcut không có tham số,
do đó Y₁ không thể được phục hồi trực tiếp. Để bù đắp
sai số này, mục tiêu tối ưu hóa của lớp cuối cùng được thay đổi từ Y₂
thành Y₁ - Y'₁ + Y₂, điều này không thay đổi việc tối ưu hóa của chúng ta. Ở đây,
Y'₁ là feature map hiện tại sau khi các lớp trước đó đã cắt tỉa. Khi
cắt tỉa, các volume nên được lấy mẫu tương ứng từ hai
nhánh này.

3.4.2 Lớp đầu tiên của nhánh residual
Được minh họa trong Hình 3(trái), feature map đầu vào của khối residual không thể được cắt tỉa, vì nó cũng được chia sẻ với nhánh
shortcut. Trong tình huống này, chúng ta có thể thực hiện lấy mẫu feature map
trước khi tích chập đầu tiên để tiết kiệm tính toán. Chúng tôi vẫn áp dụng thuật toán của mình như Phương trình 1. Khác biệt là, chúng tôi lấy mẫu các kênh đã chọn
trên các feature map chia sẻ để xây dựng đầu vào mới cho phép tích chập sau đó, được thể hiện trong Hình 3(phải). Chi phí tính toán cho
thao tác này có thể được bỏ qua. Quan trọng hơn, sau khi giới thiệu
việc lấy mẫu feature map, phép tích chập vẫn "đều".

Cắt tỉa theo filter là một lựa chọn khác cho phép tích chập đầu tiên
trên nhánh residual, được thể hiện trong Hình 4. Vì các kênh đầu vào
của nhánh shortcut không có tham số không thể được cắt tỉa, chúng tôi áp dụng
Phương trình 1 của chúng tôi cho từng filter độc lập (mỗi filter chọn
các kênh đầu vào đại diện riêng của nó). Nó đầu ra các lớp tích chập "không đều", cần hỗ trợ triển khai thư viện đặc biệt.

3.5 Kết hợp với Phân tích Tensor
Cắt tỉa kênh có thể dễ dàng kết hợp với phân tích tensor,
lượng tử hóa, và lowbits, v.v. Chúng tôi tập trung vào kết hợp
với phân tích tensor.

Nói chung, phân tích tensor có thể được biểu diễn như:
W_ln = W₁ * W₂ * ... * W_n                           (6)

Ở đây, W_ln là các filter lớp tích chập gốc cho lớp n,
và W₁ * W₂ * ... * W_n là một số trọng số được phân tách
có cùng kích thước với W. Vì các kênh đầu vào và đầu ra của các phương pháp phân tích tensor không thể co lại, nó trở thành nút thắt cổ chai
khi đạt tỷ lệ tăng tốc cao. Chúng tôi áp dụng giảm kênh
trên các lớp trọng số được phân tách đầu tiên và cuối cùng, cụ thể là đầu ra
của W_n và đầu vào của W₁. Trong các thí nghiệm của chúng tôi (Mục 4.1.3), chúng tôi
đã kết hợp [4], [14] và phương pháp của chúng tôi. Đầu tiên, một trọng số 3×3 được
phân tách thành 3×1; 1×3; 1×1. Sau đó phương pháp của chúng tôi được áp dụng
cho trọng số 3×1 và 1×1.

3.6 Fine-tuning
Chúng tôi fine-tune mô hình xấp xỉ end-to-end trên dữ liệu huấn luyện,
có thể đạt được độ chính xác cao hơn sau khi giảm. Chúng tôi thấy rằng
vì mạng đang trong trạng thái khá không ổn định, fine-tuning rất
nhạy cảm với learning rate. Learning rate cần đủ nhỏ.
Nếu không, độ chính xác nhanh chóng giảm. Nếu learning rate lớn,
quá trình fine-tuning có thể nhảy ra khỏi local optimum được khởi tạo
bởi mạng đã cắt tỉa và hoạt động rất giống với việc huấn luyện kiến trúc đã cắt tỉa từ đầu (Bảng 5).

Trên ImageNet, chúng tôi sử dụng learning rate 1e-5 và kích thước mini-batch
là 128. Fine-tune các mô hình trong mười epoch trên dữ liệu huấn luyện Imagenet (1/12 lần lặp của việc huấn luyện từ đầu). Trên CIFAR-10, chúng tôi sử dụng learning rate 1e-4 và kích thước mini-batch là 128 và
fine-tune các mô hình trong 6000 lần lặp (huấn luyện từ đầu
cần 64000 lần lặp).

--- TRANG 5 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 5

4 THÍ NGHIỆM
Chúng tôi đánh giá phương pháp của mình cho các VGG Nets [55] phổ biến,
ResNet [8], Xception [9] trên ImageNet [56], CIFAR-10 [57] và
PASCAL VOC 2007 [58].

Đối với Batch Normalization [59], chúng tôi đầu tiên gộp nó vào các
trọng số tích chập, điều này không ảnh hưởng đến đầu ra của các mạng.
Để mỗi lớp tích chập được theo sau bởi ReLU [60]. Chúng tôi
sử dụng Caffe [61]² cho đánh giá mạng sâu, TensorFlow [62] cho
triển khai SGD (Mục 4.1.1) và scikit-learn [63] cho triển khai
solver. Đối với cắt tỉa kênh, chúng tôi thấy rằng đủ để
trích xuất 5000 hình ảnh, và mười mẫu mỗi hình ảnh, điều này
cũng hiệu quả (tức là vài phút cho VGG-16³, Mục 4.1.2). Trên
ImageNet, chúng tôi đánh giá độ chính xác top-5 với single view.
Hình ảnh được thay đổi kích thước sao cho cạnh ngắn hơn là 256. Việc kiểm tra
trên center crop 224×224 pixel. Việc augmentation cho
fine-tuning là random crop 224×224 và mirror.

4.1 Thí nghiệm với VGG-16
VGG-16 [55] là một mạng nơ-ron tích chập đơn nhánh 16 lớp,
với 13 lớp tích chập. Nó được sử dụng rộng rãi cho
nhận dạng, phát hiện và phân đoạn, v.v. Độ chính xác top-5 single view
cho VGG-16 là 89.9%⁴.

4.1.1 Cắt tỉa Lớp Đơn
Trong tiểu mục này, chúng tôi đánh giá hiệu suất tăng tốc lớp đơn
sử dụng thuật toán của chúng tôi trong Mục 3.1. Để hiểu rõ hơn,
chúng tôi so sánh thuật toán của mình với ba chiến lược chọn kênh
ngây thơ. first k chọn k kênh đầu tiên. max response chọn
các kênh dựa trên các filter tương ứng có tổng trọng số tuyệt đối
cao [11]. SGD là một thay thế đơn giản cho phương pháp của chúng tôi để
sử dụng các trọng số gốc làm khởi tạo, và giải quyết bài toán điều chỉnh ℓ₁
trong Phương trình 2 (w.r.t. cả trọng số và kết nối)
bằng stochastic gradient descent.

Để so sánh công bằng, chúng tôi thu được các chỉ số feature map được
chọn bởi mỗi phương pháp, sau đó thực hiện tái tạo (ngoại trừ SGD,
Mục 3.2.2). Chúng tôi hy vọng rằng điều này có thể chứng minh tầm quan trọng
của việc chọn kênh. Hiệu suất được đo bằng sự gia tăng
sai số sau khi một lớp nhất định được cắt tỉa mà không có fine-tuning, được thể hiện trong
Hình 5.

Như mong đợi, sai số tăng khi tỷ lệ tăng tốc tăng. Phương pháp của chúng tôi
liên tục tốt hơn các phương pháp khác trong các

--- TRANG 6 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 6

lớp tích chập khác nhau dưới tỷ lệ tăng tốc khác nhau. Không như mong đợi,
đôi khi max response thậm chí còn tệ hơn first k. Chúng tôi lập luận rằng
max response bỏ qua các mối tương quan giữa các filter khác nhau. Các filter
với trọng số tuyệt đối lớn có thể có mối tương quan mạnh. Do đó
việc chọn dựa trên trọng số filter ít có ý nghĩa hơn. Mối tương quan
trên feature map đáng được khai thác. Chúng ta có thể thấy rằng việc chọn kênh
ảnh hưởng rất nhiều đến sai số tái tạo. Do đó, nó quan trọng
cho việc cắt tỉa kênh.

Đối với SGD, chúng tôi chỉ thực hiện thí nghiệm dưới tăng tốc 4×
do hạn chế thời gian. Mặc dù nó chia sẻ cùng mục tiêu tối ưu hóa
với phương pháp của chúng tôi, SGD đơn giản dường như khó tối ưu hóa đến
một local minimal lý tưởng. Được thể hiện trong Hình 5, SGD rõ ràng tệ hơn
phương pháp tối ưu hóa của chúng tôi.

Cũng chú ý rằng việc cắt tỉa kênh dần trở nên khó khăn,
từ các lớp nông hơn đến sâu hơn. Nó chỉ ra rằng các lớp nông hơn
có nhiều dư thừa hơn, điều này phù hợp với [14]. Chúng ta
có thể cắt tỉa tích cực hơn trên các lớp nông hơn trong việc tăng tốc toàn mô hình.

4.1.2 Cắt tỉa Toàn bộ Mô hình
Được thể hiện trong Bảng 1, chúng tôi đã phân tích PCA energy của VGG-16. Nó
chỉ ra rằng các lớp nông hơn của VGG-16 dư thừa hơn,
điều này trùng khớp với các thí nghiệm lớp đơn của chúng tôi ở trên. Vì vậy chúng tôi
cắt tỉa tích cực hơn cho các lớp nông hơn. Tỷ lệ bảo tồn kênh
cho các lớp nông (conv1_x đến conv3_x) và các lớp sâu
(conv4_x) là 1:1:5. conv5_x không được cắt tỉa, vì chúng chỉ
đóng góp 9% tính toán tổng cộng và không dư thừa, được thể hiện
trong Bảng 1.

Chúng tôi áp dụng việc cắt tỉa toàn mô hình từng lớp được đề xuất trong
Mục 3.3. Hình 6 thể hiện việc cắt tỉa VGG-16 dưới tăng tốc 4×, cuối cùng
đạt 1.0% tăng sai số sau fine-tuning. Dễ dàng thấy rằng
sai số tích lũy tăng từng lớp. Và các sai số chủ yếu được giới thiệu
bởi việc cắt tỉa các lớp sau, điều này trùng khớp với quan sát của chúng tôi
từ việc cắt tỉa lớp đơn và phân tích PCA.

Ngoài mô hình suy luận hiệu quả mà chúng tôi đạt được, thuật toán của chúng tôi
cũng hiệu quả. Được thể hiện trong Hình 7, thuật toán của chúng tôi có thể
hoàn thành việc cắt tỉa VGG-16 dưới 4× trong vòng 5 phút.

Được thể hiện trong Bảng 2, kết quả tăng tốc toàn mô hình dưới 2×,
4×, 5× được chứng minh. Sau fine-tuning, chúng tôi có thể đạt tăng tốc 2×
mà không mất độ chính xác. Dưới 4×, chúng tôi chỉ chịu
giảm 1.0%. Phù hợp với phân tích lớp đơn, phương pháp của chúng tôi
vượt trội hơn các phương pháp cắt tỉa gần đây khác (Filter Pruning [11],
Runtime Neural Pruning [16] và Structured Probabilistic Pruning [17]) với biên độ lớn. Điều này là do chúng tôi khai thác đầy đủ
sự dư thừa kênh trong feature map. So với các thuật toán phân tích tensor, phương pháp của chúng tôi tốt hơn Jaderberg et
al. [4], mà không có fine-tuning. Mặc dù tệ hơn Asym. [14], mô hình kết hợp của chúng tôi vượt trội hơn Asym. 3D kết hợp của nó (Bảng 3).
Điều này có thể chỉ ra rằng việc cắt tỉa kênh thách thức hơn
phân tích tensor, vì việc loại bỏ các kênh trong một lớp có thể
thay đổi đáng kể đầu vào của lớp tiếp theo. Tuy nhiên,
việc cắt tỉa kênh giữ nguyên kiến trúc mô hình gốc, không giới thiệu thêm lớp, và tỷ lệ tăng tốc tuyệt đối trên
GPU cao hơn nhiều (Bảng 7).

--- TRANG 7 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 7

4.1.3 Kết hợp với Các Phương pháp Trực giao
Vì phương pháp của chúng tôi khai thác một cardinality mới, chúng tôi tiếp tục kết hợp
việc cắt tỉa kênh của chúng tôi với phân tích không gian [4] và phân tích kênh [14] (Mục 3.5). Được chứng minh trong Bảng 3, tăng tốc 3 cardinalities của chúng tôi (không gian, phân tích kênh, và cắt tỉa kênh, ký hiệu là 3C) vượt trội hơn các state-of-the-art trước đó.
Asym. 3D [14] (phân tích không gian và kênh), phân tích một
lớp tích chập thành ba phần: 1×3; 3×1; 1×1.

Chúng tôi áp dụng phân tích không gian, phân tích kênh, và
cắt tỉa kênh của chúng tôi cùng nhau tuần tự từng lớp. Chúng tôi fine-tune các mô hình được tăng tốc trong 20 epoch, vì chúng sâu gấp ba lần so với các mô hình gốc. Sau fine-tuning, mô hình 4× của chúng tôi
không chịu suy giảm nào. Rõ ràng, sự kết hợp của các kỹ thuật tăng tốc khác nhau tốt hơn bất kỳ kỹ thuật đơn lẻ nào. Điều này chỉ ra rằng
một mô hình dư thừa trong mỗi cardinality.

4.1.4 Hiệu suất mà không có Tái tạo Đầu ra
Chúng tôi đánh giá hiệu suất cắt tỉa toàn mô hình mà không có tái tạo đầu ra, để xác minh hiệu quả của bài toán con của W (Mục 3.2.2). Được thể hiện trong Bảng 4, không có tái tạo, sai số tích lũy sẽ không thể chấp nhận được cho việc cắt tỉa đa lớp.
Không có tái tạo, sai số tăng lên 99%. Ngay cả sau
fine-tuning, điểm số vẫn tệ hơn nhiều so với các đối tác.
Điều này là do bước LASSO (Mục 3.2.1) chỉ cập nhật với
tự do hạn chế (dimensionality = c), do đó không đủ cho
tái tạo. Vì vậy chúng ta phải điều chỉnh trọng số gốc W (n×c×
kh×kw) cho các kênh đầu vào đã cắt tỉa.

4.1.5 So sánh với Huấn luyện từ Đầu
Mặc dù huấn luyện một mô hình compact từ đầu tốn thời gian
(thường 120 epoch), điều đáng để so sánh phương pháp của chúng tôi và

--- TRANG 8 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 8

các đối tác từ đầu. Để công bằng, chúng tôi đánh giá cả đối tác từ đầu, và mạng thiết lập bình thường có cùng độ phức tạp tính toán và cùng kiến trúc.

Được thể hiện trong Bảng 5, chúng tôi quan sát rằng khó cho các đối tác từ đầu đạt được độ chính xác cạnh tranh. Mô hình của chúng tôi vượt trội hơn mô hình từ đầu. Phương pháp của chúng tôi thành công chọn ra các kênh thông tin và xây dựng các mô hình rất compact.
Chúng ta có thể rút ra kết luận an toàn rằng cùng một mô hình khó
có thể đạt được từ đầu. Điều này trùng khớp với các nghiên cứu thiết kế kiến trúc [10], [64] rằng mô hình có thể dễ huấn luyện hơn
nếu có nhiều kênh hơn trong các lớp nông hơn. Tuy nhiên, việc cắt tỉa kênh ưa chuộng các lớp nông hơn.

Đối với từ đầu (uniformed), các filter trong mỗi lớp được
giảm một nửa (ví dụ: giảm conv1_1 từ 64 xuống 32). Chúng ta
có thể quan sát rằng các mạng thiết lập bình thường có cùng độ phức tạp
cũng không thể đạt được cùng độ chính xác. Điều này củng cố ý tưởng của chúng tôi rằng có nhiều dư thừa trong các mạng trong khi huấn luyện.
Tuy nhiên, sự dư thừa có thể được opt-out tại thời gian suy luận. Điều này có thể
là một lợi thế của các phương pháp tăng tốc thời gian suy luận so với
các phương pháp dựa trên huấn luyện.

Chú ý rằng có khoảng cách 0.6% giữa mô hình từ đầu và
mô hình uniformed, điều này chỉ ra rằng có chỗ cho
việc khám phá mô hình. Việc áp dụng phương pháp của chúng tôi nhanh hơn nhiều so với việc huấn luyện một mô hình từ đầu, ngay cả cho một mô hình mỏng hơn. Các nghiên cứu tiếp theo có thể giảm nhẹ phương pháp của chúng tôi để khám phá mô hình mỏng.

4.1.6 Độ chính xác Top-1 vs Top-5
Mặc dù phương pháp của chúng tôi đã đạt được hiệu suất tốt với
độ chính xác Top-5, vẫn có thể dẫn đến giảm đáng kể
độ chính xác Top-1. Được thể hiện trong Bảng 6, chúng tôi so sánh sự gia tăng
sai số Top-1 và Top-5 cho việc tăng tốc VGG-16 trên ImageNet.
Mặc dù việc giảm tuyệt đối hơi lớn hơn, Top-1 vẫn phù hợp
với kết quả top-5. Đối với 3C 4× và 5×, độ chính xác Top-1 thậm chí
tốt hơn. Độ chính xác Top-1 của 3C 4× vượt trội hơn mô hình VGG-16 gốc 0.3%.

4.1.7 So sánh Hiệu suất Tuyệt đối
Chúng tôi tiếp tục đánh giá hiệu suất tuyệt đối của việc tăng tốc trên GPU.
Kết quả trong Bảng 7 được thu được dưới Caffe [61], CUDA8 [65]
và cuDNN5 [66], với mini-batch 32 trên GPU⁵. Kết quả được tính trung bình từ 50 lần chạy. Các phương pháp phân tích tensor
phân tách trọng số thành quá nhiều phần, làm tăng nặng overhead. Chúng không thể đạt được nhiều tăng tốc tuyệt đối.
Mặc dù phương pháp của chúng tôi cũng gặp phải suy giảm hiệu suất,
nó tổng quát hóa tốt hơn trên GPU so với các phương pháp khác. Kết quả của chúng tôi cho phân tích tensor khác với nghiên cứu trước đó [4], [14],
có thể vì thư viện và phần cứng hiện tại ưa chuộng tích chập lớn đơn thay vì nhiều tích chập nhỏ.

4.1.8 Tăng tốc cho Phát hiện
VGG-16 phổ biến trong các tác vụ phát hiện đối tượng [67], [68], [69],
[70], [71], [72], [73]. Chúng tôi đánh giá khả năng transfer learning của
VGG-16 đã cắt tỉa 2×/4× của chúng tôi, cho việc phát hiện đối tượng Faster R-CNN [74]⁶. Benchmark phát hiện đối tượng PASCAL VOC 2007 [58]
chứa 5k hình ảnh trainval và 5k hình ảnh test. Hiệu suất
được đánh giá bằng mean Average Precision (mAP) và mmAP (AP
tại IoU=.50:.05:.95, metric thách thức chính của COCO [75]). Trong
các thí nghiệm của chúng tôi, chúng tôi đầu tiên thực hiện cắt tỉa kênh cho VGG-16 trên
ImageNet. Sau đó chúng tôi sử dụng mô hình đã cắt tỉa làm mô hình pre-trained cho Faster R-CNN.

Thời gian chạy thực tế của Faster R-CNN là 220ms / hình ảnh.
Các lớp tích chập đóng góp khoảng 64%. Chúng tôi có thời gian thực tế
94ms cho tăng tốc 4×. Từ Bảng 8, chúng tôi quan sát
0.4% mAP và 0.0% mmAP giảm của mô hình 2× của chúng tôi, điều này
không có hại cho việc xem xét thực tế. Quan sát từ mmAP,
Đối với các yêu cầu localization cao hơn, mô hình tăng tốc của chúng tôi không
chịu suy giảm lớn.

4.2 Thí nghiệm với Mạng Kiến trúc Residual
Đối với các mạng đa đường [7], [8], [9], chúng tôi tiếp tục khám phá
ResNet [8] phổ biến và Xception [9] mới nhất, trên ImageNet và
CIFAR-10. Việc cắt tỉa các mạng kiến trúc residual thách thức hơn.
Các mạng này được thiết kế cho cả hiệu quả và độ chính xác cao.
Các thuật toán phân tích tensor [4], [14] không áp dụng được cho các
mô hình này. Về mặt không gian, tích chập 1×1 được ưa chuộng, khó có thể
được phân tích.

4.2.1 Cắt tỉa theo Filter
Dưới tăng tốc lớp đơn, cắt tỉa theo filter (Mục 3.4) chính xác hơn phương pháp gốc của chúng tôi, vì nó linh hoạt hơn,
được thể hiện trong Hình 8. Từ các thí nghiệm cắt tỉa ResNet của chúng tôi trong phần tiếp theo, nó cải thiện 0.5% độ chính xác top-5 cho ResNet-50 2× (áp dụng
trên lớp đầu tiên của mỗi nhánh residual) mà không có fine-tuning.
Tuy nhiên, sau fine-tuning, không có cải thiện đáng chú ý.
Bên cạnh đó, nó đầu ra các lớp tích chập "không đều", cần
hỗ trợ triển khai thư viện đặc biệt để đạt được tăng tốc thực tế.
Do đó, chúng tôi không áp dụng nó cho các mạng kiến trúc residual của chúng tôi.

4.2.2 Cắt tỉa ResNet
Độ phức tạp ResNet giảm đều trên mỗi khối residual, như được
thể hiện trong Bảng 9. Được hướng dẫn bởi các thí nghiệm lớp đơn (Mục 4.1.1),
chúng tôi vẫn ưa chuộng việc giảm các lớp nông hơn nhiều hơn các lớp sâu hơn.
Theo thiết lập tương tự như Filter pruning [11], chúng tôi giữ 70%
kênh cho các khối residual nhạy cảm (res5 và các khối gần
vị trí thay đổi kích thước không gian, ví dụ: res3a, res3d).
Đối với các khối khác, chúng tôi giữ 30% kênh. Với việc tăng cường đa nhánh, chúng tôi cắt tỉa branch2a tích cực hơn

--- TRANG 9 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 9

trong mỗi khối residual. Tỷ lệ bảo tồn kênh cho
branch2a, branch2b, branch2c là 2:4:3 (ví dụ: Với
30%, chúng tôi giữ 40%, 80%, 60% tương ứng).

Chúng tôi đánh giá hiệu suất của các biến thể đa nhánh của phương pháp chúng tôi (Mục 3.4). Phương pháp của chúng tôi vượt trội hơn các phương pháp gần đây

(ThiNet [15], Structured Probabilistic Pruning [17]) với biên độ lớn. Từ Bảng 10, chúng tôi cải thiện 4.0% với việc tăng cường đa nhánh của chúng tôi. Điều này là do chúng tôi đã tính đến sai số tích lũy
từ kết nối shortcut có thể phát sóng đến mọi lớp sau nó. Và chiều rộng feature map đầu vào lớn tại lối vào của mỗi
khối residual được giảm tốt bởi việc lấy mẫu feature map của chúng tôi.

4.2.3 Cắt tỉa Xception
Vì độ phức tạp tính toán trở nên quan trọng trong thiết kế mô hình, tích chập separable đã được chú ý nhiều [9],
[76]. Xception [9] đã được tối ưu hóa về mặt không gian và phân tích tensor
trên lớp tích chập 1×1 là phá hoại. Nhờ phương pháp của chúng tôi, nó vẫn có thể được tăng tốc với suy giảm duyên dáng.
Để dễ so sánh, chúng tôi áp dụng tích chập Xception trên
ResNet-50, ký hiệu là Xception-50. Dựa trên ResNet-50, chúng tôi
thay thế tất cả các lớp tích chập bằng các khối conv không gian. Để giữ
cùng độ phức tạp tính toán, chúng tôi tăng các kênh đầu vào của
tất cả các lớp branch2b lên 2×. Xception-50⁷ baseline có
độ chính xác top-5 là 92.8% và độ phức tạp 4450 MFLOP.

Chúng tôi áp dụng các biến thể đa nhánh của phương pháp như được mô tả trong
Mục 3.4, và áp dụng cùng thiết lập tỷ lệ cắt tỉa như ResNet trong
phần trước. Có thể vì khối Xception không ổn định,
các lớp Batch Normalization phải được duy trì trong quá trình cắt tỉa.
Nếu không, việc fine-tune mô hình đã cắt tỉa trở nên không tầm thường.
Được thể hiện trong Bảng 11, sau fine-tuning, chúng tôi chỉ chịu tăng 1.0%
sai số dưới 2×. Filter pruning [11] cũng có thể áp dụng trên
Xception, mặc dù nó được thiết kế cho tỷ lệ tăng tốc nhỏ. Không có
fine-tuning, sai số top-5 là 100%. Sau huấn luyện 20 epoch, sai số tăng đạt 4.3% giống như huấn luyện từ đầu. Kết quả của chúng tôi cho Xception-50 không duyên dáng như kết quả cho VGG-16
vì các mạng hiện đại có xu hướng ít dư thừa hơn theo thiết kế.

4.2.4 Thí nghiệm trên CIFAR-10
Mặc dù phương pháp của chúng tôi được thiết kế cho các tập dữ liệu lớn, nó có thể
tổng quát hóa tốt trên các tập dữ liệu nhỏ. Chúng tôi thực hiện thí nghiệm trên
tập dữ liệu CIFAR-10 [57], được ưa chuộng bởi nhiều nghiên cứu tăng tốc. Nó bao gồm 50k hình ảnh để huấn luyện và 10k để kiểm tra
trong 10 lớp. Hình ảnh 32×32 gốc được zero pad với 4
pixel ở mỗi bên, sau đó random crop thành 32×32 tại thời gian huấn luyện. Phương pháp của chúng tôi có thể được hoàn thành trong vài phút.

Chúng tôi tái tạo ResNet-56⁸, có độ chính xác 92.8%
(Làm tham chiếu, ResNet-56 chính thức [8] có độ chính xác
93.0%). Đối với tăng tốc 2×, chúng tôi theo thiết lập tương tự như
Mục 4.2.2 (giữ nguyên giai đoạn cuối, nơi kích thước không gian
là 8×8). Được thể hiện trong Bảng 12, phương pháp của chúng tôi cạnh tranh với
mô hình huấn luyện từ đầu, không có fine-tuning, dưới cả tăng tốc 1.4× và 2×. Sau fine-tuning, kết quả của chúng tôi tốt hơn đáng kể so với
Filter pruning [11] và mô hình huấn luyện từ đầu cho cả tăng tốc 1.4× và 2×.

Giảm Lớp Nông vs Sâu: Được ký hiệu là (uniform) trong
Bảng 12, giải pháp uniformed cắt tỉa từng lớp với cùng
tỷ lệ cắt tỉa. Rõ ràng, kết quả uniformed của chúng tôi tệ hơn những kết quả giảm mạnh các lớp nông. Tuy nhiên, mô hình uniformed từ
đầu tốt hơn đối tác của nó. Điều này là do việc cắt tỉa kênh
ưa chuộng ít kênh hơn trên các lớp nông, tuy nhiên các mô hình từ đầu hoạt động tốt hơn với nhiều lớp nông hơn. Nó chỉ ra rằng sự dư thừa trên các lớp nông là cần thiết trong khi huấn luyện, có thể được loại bỏ tại thời gian suy luận.

5 KẾT LUẬN
Tóm lại, các CNN sâu hiện tại chính xác với chi phí suy luận cao. Trong bài báo này, chúng tôi đã trình bày một phương pháp cắt tỉa kênh thời gian suy luận cho các mạng rất sâu. Các CNN được giảm
là các mạng hiệu quả suy luận trong khi duy trì độ chính xác, và chỉ
yêu cầu các thư viện có sẵn. Tốc độ tăng tốc và độ chính xác hấp dẫn được chứng minh cho cả VGG Net và các mạng giống ResNet trên ImageNet, CIFAR-10 và PASCAL VOC 2007.

Trong tương lai, chúng tôi dự định đưa các phương pháp của mình vào thời gian huấn luyện để tăng tốc quy trình huấn luyện, thay vì chỉ thời gian suy luận.

TÀI LIỆU THAM KHẢO
[Các tài liệu tham khảo từ [1] đến [76] được liệt kê theo thứ tự như trong bản gốc]

--- TRANG 12 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 12

[Tiếp tục danh sách tài liệu tham khảo từ [58] đến [76]]
