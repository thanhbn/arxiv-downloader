# Đào tạo có khả năng mở rộng của Mạng Neural Nhân tạo với Kết nối Thưa Thích ứng lấy cảm hứng từ Khoa học Mạng

Decebal Constantin Mocanu1,3,*, Elena Mocanu2,3, Peter Stone4, Phuong H. Nguyen3,
Madeleine Gibescu3, và Antonio Liotta5

1Khoa Toán học và Khoa học Máy tính, Đại học Công nghệ Eindhoven, De Rondom 70, 5612 AP,
Eindhoven, Hà Lan
2Khoa Cơ khí, Đại học Công nghệ Eindhoven, De Rondom 70, 5612 AP, Eindhoven,
Hà Lan
3Khoa Kỹ thuật Điện, Đại học Công nghệ Eindhoven, De Rondom 70, 5612 AP, Eindhoven,
Hà Lan
4Khoa Khoa học Máy tính, Đại học Texas tại Austin, 2317 Speedway, Stop D9500 Austin, Texas
78712-1757, Hoa Kỳ
5Trung tâm Khoa học Dữ liệu, Đại học Derby, Lonsdale House, Quaker Way, Derby DE1 3HD, Vương quốc Anh
*Tác giả liên hệ: d.c.mocanu@tue.nl

## TÓM TẮT

Thông qua thành công của học sâu trong các lĩnh vực khác nhau, mạng neural nhân tạo hiện đang là một trong những phương pháp trí tuệ nhân tạo được sử dụng nhiều nhất. Lấy cảm hứng từ các tính chất mạng của mạng neural sinh học (ví dụ: tính thưa, không có tỷ lệ), chúng tôi lập luận rằng (trái với thực hành chung) mạng neural nhân tạo cũng không nên có các lớp kết nối đầy đủ. Ở đây chúng tôi đề xuất đào tạo tiến hóa thưa của mạng neural nhân tạo, một thuật toán tiến hóa một topology thưa ban đầu (đồ thị ngẫu nhiên Erdős-Rényi) của hai lớp neuron liên tiếp thành một topology không có tỷ lệ, trong quá trình học tập. Phương pháp của chúng tôi thay thế các lớp kết nối đầy đủ của mạng neural nhân tạo bằng các lớp thưa trước khi đào tạo, giảm bậc hai số lượng tham số, mà không làm giảm độ chính xác. Chúng tôi chứng minh tuyên bố của mình trên máy Boltzmann hạn chế, perceptron nhiều lớp, và mạng neural tích chập cho học không giám sát và có giám sát trên 15 bộ dữ liệu. Cách tiếp cận của chúng tôi có tiềm năng cho phép mạng neural nhân tạo mở rộng vượt ra ngoài những gì hiện tại có thể.

## Giới thiệu

Mạng Neural Nhân tạo (ANN) là một trong những phương pháp trí tuệ nhân tạo thành công nhất ngày nay. ANN đã dẫn đến những đột phá lớn trong các lĩnh vực khác nhau, như vật lý hạt1, học tăng cường sâu2, nhận dạng giọng nói, thị giác máy tính, và những lĩnh vực khác3. Thông thường, ANN có các lớp neuron kết nối đầy đủ3, chứa hầu hết các tham số mạng (tức là các kết nối có trọng số), dẫn đến số lượng kết nối bậc hai đối với số lượng neuron của chúng. Ngược lại, kích thước mạng bị hạn chế nghiêm trọng, do các hạn chế tính toán.

Trái ngược với ANN, mạng neural sinh học đã được chứng minh có topology thưa (thay vì dày đặc)4, 5, và cũng có các tính chất quan trọng khác là công cụ cho hiệu quả học tập. Những điều này đã được nghiên cứu rộng rãi trong6 và bao gồm không có tỷ lệ7 (chi tiết trong phần Phương pháp) và tính chất thế giới nhỏ8. Tuy nhiên, ANN chưa tiến hóa để bắt chước những đặc điểm topology này9, 10, đó là lý do tại sao trong thực tế chúng dẫn đến các mô hình cực kỳ lớn. Các nghiên cứu trước đây đã chứng minh rằng, sau giai đoạn đào tạo, các mô hình ANN kết thúc với các biểu đồ trọng số đạt đỉnh xung quanh số không11–13. Hơn nữa, trong công trình trước đây của chúng tôi14, chúng tôi đã quan sát thấy một thực tế tương tự. Tuy nhiên, trong nghệ thuật tiên tiến của máy học, kết nối topology thưa chỉ được theo đuổi như một hậu quả của giai đoạn đào tạo13, điều này mang lại lợi ích chỉ trong giai đoạn suy luận.

Trong một bài báo gần đây, chúng tôi đã giới thiệu máy Boltzmann phức tạp (XBM), một biến thể thưa của máy Boltzmann hạn chế (RBM), được thiết kế với topology không có tỷ lệ thưa10. XBM vượt trội hơn các đối tác RBM kết nối đầy đủ và nhanh hơn nhiều, cả trong giai đoạn đào tạo và suy luận. Tuy nhiên, dựa trên một mẫu thưa cố định, XBM có thể không mô hình hóa đúng phân phối dữ liệu. Để khắc phục hạn chế này, trong bài báo này chúng tôi giới thiệu một quy trình Đào tạo Tiến hóa Thưa (SET), có tính đến phân phối dữ liệu và tạo ra các lớp lưỡng phân thưa phù hợp để thay thế các lớp lưỡng phân kết nối đầy đủ trong bất kỳ loại ANN nào.

SET được lấy cảm hứng rộng rãi từ sự đơn giản tự nhiên của các cách tiếp cận tiến hóa, được khám phá thành công trong công trình trước đây của chúng tôi về xấp xỉ hàm tiến hóa15. Cùng các cách tiếp cận tiến hóa đã được khám phá cho kết nối mạng trong16, và cho kiến trúc lớp của mạng neural sâu17. Thường thì, trong não sinh học, các quá trình tiến hóa được chia thành bốn cấp độ: tiến hóa chủng loại ở quy mô thời gian thế hệ, tiến hóa cá thể ở quy mô hàng ngày (hoặc hàng năm), biểu sinh ở quy mô giây đến ngày, và suy luận ở quy mô mili giây đến giây18. Một ví dụ cổ điển giải quyết tất cả các cấp độ này là Tiến hóa Neural của Topology Tăng cường (NEAT)19. Tóm lại, NEAT là một thuật toán tiến hóa tìm cách tối ưu hóa cả các tham số (trọng số) và topology của ANN cho một nhiệm vụ nhất định. Nó bắt đầu với ANN nhỏ với ít nút và liên kết, và dần dần xem xét thêm các nút và liên kết mới để tạo ra các cấu trúc phức tạp hơn trong chừng mực chúng cải thiện hiệu suất. Trong khi NEAT đã cho thấy một số kết quả thực nghiệm ấn tượng20, trong thực tế, NEAT và hầu hết các biến thể trực tiếp của nó gặp khó khăn trong việc mở rộng do không gian tìm kiếm rất lớn của chúng. Theo hiểu biết tốt nhất của chúng tôi, chúng chỉ có khả năng giải quyết các vấn đề nhỏ hơn nhiều so với những vấn đề hiện tại được giải quyết bởi các kỹ thuật học sâu tiên tiến, ví dụ: nhận dạng đối tượng từ dữ liệu pixel thô của hình ảnh lớn. Trong21, Miconi đã cố gắng sử dụng các nguyên tắc giống NEAT (ví dụ: thêm, xóa) kết hợp với Gradient Descent Ngẫu nhiên (SGD) để đào tạo mạng neural tái phát cho các vấn đề nhỏ, do không gian tìm kiếm vẫn lớn. Gần đây trong22 và trong23, đã được chỉ ra rằng các chiến lược tiến hóa và thuật toán di truyền, tương ứng, có thể đào tạo thành công mạng neural nhân tạo với tới 4 triệu tham số như một thay thế khả thi cho DQN2 cho các nhiệm vụ học tăng cường, nhưng chúng cần hơn 700 CPU để làm như vậy. Để tránh bị mắc kẹt trong cùng loại vấn đề khả năng mở rộng, trong SET, chúng tôi tập trung vào việc sử dụng điều tốt nhất từ cả hai thế giới (tức là neuroevolution truyền thống và học sâu). Ví dụ: tiến hóa chỉ ở quy mô biểu sinh cho các kết nối để tạo ra kết nối thích ứng thưa, kiến trúc đa lớp có cấu trúc với số lượng lớp và neuron cố định để có được các mô hình ANN dễ dàng được đào tạo bởi các thuật toán đào tạo tiêu chuẩn, ví dụ: gradient descent ngẫu nhiên, và những thứ khác.

Ở đây, chúng tôi tuyên bố rằng tính thưa topology phải được theo đuổi bắt đầu từ giai đoạn thiết kế ANN, dẫn đến giảm đáng kể các kết nối và, lần lượt, đến hiệu quả bộ nhớ và tính toán. Chúng tôi cho thấy ANN hoạt động hoàn hảo với các lớp kết nối thưa. Chúng tôi phát hiện rằng các lớp kết nối thưa, được đào tạo với SET, có thể thay thế bất kỳ lớp kết nối đầy đủ nào trong ANN, mà không làm giảm độ chính xác, trong khi có ít tham số hơn bậc hai ngay cả trong giai đoạn thiết kế ANN (trước khi đào tạo). Điều này dẫn đến giảm yêu cầu bộ nhớ và có thể dẫn đến thời gian tính toán nhanh hơn bậc hai trong cả hai giai đoạn (tức là đào tạo và suy luận). Chúng tôi chứng minh tuyên bố của mình trên ba loại ANN phổ biến (máy Boltzmann hạn chế, perceptron nhiều lớp, và mạng neural tích chập), trên hai loại nhiệm vụ (học có giám sát và không giám sát), và trên 15 bộ dữ liệu chuẩn. Chúng tôi hy vọng rằng cách tiếp cận của chúng tôi sẽ cho phép ANN có hàng tỷ neuron và topology tiến hóa có khả năng xử lý các nhiệm vụ thế giới thực phức tạp không thể giải quyết bằng các phương pháp tiên tiến.

## Kết quả

**Phương pháp Đào tạo Tiến hóa Thưa** Với SET, các lớp ANN lưỡng phân bắt đầu từ một topology thưa ngẫu nhiên (tức là đồ thị ngẫu nhiên Erdős-Rényi24), tiến hóa thông qua một quá trình ngẫu nhiên trong giai đoạn đào tạo hướng tới một topology không có tỷ lệ. Đáng chú ý, quá trình này không phải kết hợp bất kỳ ràng buộc nào để buộc topology không có tỷ lệ. Nhưng thuật toán tiến hóa của chúng tôi không tùy ý: nó tuân theo một hiện tượng xảy ra trong các mạng phức tạp thế giới thực (như mạng neural sinh học, và mạng tương tác protein). Bắt đầu từ topology đồ thị ngẫu nhiên Erdős-Rényi và qua hàng thiên niên kỷ tiến hóa tự nhiên, các mạng kết thúc với kết nối có cấu trúc hơn, tức là topology không có tỷ lệ7 hoặc thế giới nhỏ8.

Thuật toán SET được mô tả chi tiết trong Phương pháp và được minh họa trong Hình 1. Chính thức, hãy định nghĩa một lớp Kết nối Thưa (SCk) trong ANN. Lớp này có nk neuron, được thu thập trong một vector hk = [hk1; hk2; :::; hknk]. Bất kỳ neuron nào từ hk được kết nối đến một số lượng tùy ý neuron thuộc lớp bên dưới, hk−1. Các kết nối giữa hai lớp được thu thập trong một ma trận trọng số thưa Wk ∈ Rnk−1×nk. Ban đầu, Wk là một đồ thị ngẫu nhiên Erdős-Rényi, trong đó xác suất của một kết nối giữa neuron hki và hk−1j được cho bởi:

p(Wki,j) = ε(nk + nk−1)/(nk nk−1)                    (1)

trong đó ε ∈ R+ là một tham số của SET kiểm soát mức độ thưa. Nếu ε ≈ nk và ε ≈ nk+1 thì có một số lượng kết nối tuyến tính (tức là các phần tử khác không), nW = |Wk| = ε(nk + nk−1), đối với số lượng neuron trong các lớp thưa. Trong trường hợp các lớp kết nối đầy đủ, số lượng kết nối là bậc hai, tức là nk nk−1.

Tuy nhiên, có thể topology được tạo ngẫu nhiên này không phù hợp với các đặc điểm cụ thể của dữ liệu mà mô hình ANN cố gắng học. Để khắc phục tình huống này, trong quá trình đào tạo, sau mỗi epoch đào tạo, một phần z của các trọng số dương nhỏ nhất và của các trọng số âm lớn nhất của SCk được loại bỏ. Những trọng số bị loại bỏ này là những trọng số gần nhất với số không, vì vậy chúng tôi không mong đợi rằng việc loại bỏ chúng sẽ thay đổi đáng kể hiệu suất mô hình. Điều này đã được chỉ ra, chẳng hạn, trong13, 25 sử dụng các cách tiếp cận phức tạp hơn để loại bỏ trọng số không quan trọng. Tiếp theo, để cho phép topology của SCk tiến hóa để phù hợp với dữ liệu, một lượng kết nối ngẫu nhiên mới, bằng với lượng trọng số đã loại bỏ trước đó, được thêm vào SCk. Bằng cách này, số lượng kết nối trong SCk vẫn không đổi trong quá trình đào tạo. Sau khi đào tạo kết thúc, chúng tôi giữ topology của SCk như được thu được sau bước loại bỏ trọng số cuối cùng, mà không thêm các kết nối ngẫu nhiên mới. Để minh họa tốt hơn các quá trình này, chúng tôi đưa ra sự tương tự sau. Nếu chúng ta giả định một kết nối như thực thể tiến hóa theo thời gian, việc loại bỏ các kết nối ít quan trọng nhất tương ứng, đại khái, với giai đoạn chọn lọc của tiến hóa tự nhiên, trong khi việc thêm ngẫu nhiên các kết nối mới tương ứng, đại khái, với giai đoạn đột biến của tiến hóa tự nhiên.

Đáng chú ý rằng trong giai đoạn ban đầu của việc hình thành quy trình SET, các bước loại bỏ trọng số và thêm trọng số sau mỗi epoch đào tạo được giới thiệu dựa trên trực giác của chúng tôi. Tuy nhiên, trong các giai đoạn cuối của việc chuẩn bị bài báo này, chúng tôi đã phát hiện rằng có sự tương đồng giữa SET và một hiện tượng xảy ra trong não sinh học, được gọi là co rút synapse trong giấc ngủ. Hiện tượng này đã được chứng minh trong hai bài báo gần đây26, 27. Tóm lại, người ta phát hiện rằng trong giấc ngủ, các synapse yếu nhất trong não co rút, trong khi các synapse mạnh nhất vẫn không thay đổi, ủng hộ giả thuyết rằng một trong những chức năng cốt lõi của giấc ngủ là chuẩn hóa lại sức mạnh synapse tổng thể tăng lên khi thức27. Bằng cách giữ sự tương tự, điều này là - theo một cách nào đó - những gì cũng xảy ra với ANN trong quy trình SET.

Chúng tôi đánh giá SET trong ba loại ANN, máy Boltzmann hạn chế28, Perceptron Nhiều Lớp (MLP), và Mạng Neural Tích chập (CNN)3 (cả ba đều được mô tả chi tiết trong phần Phương pháp), để thử nghiệm với cả học không giám sát và có giám sát. Tổng cộng, chúng tôi đánh giá SET trên 15 bộ dữ liệu chuẩn, như được mô tả chi tiết trong Bảng 1, bao gồm một phạm vi rộng các lĩnh vực mà ANN được sử dụng, như sinh học, vật lý, thị giác máy tính, khai thác dữ liệu, và kinh tế. Chúng tôi cũng đánh giá SET kết hợp với hai phương pháp đào tạo khác nhau, tức là divergence tương phản29 và gradient descent ngẫu nhiên3.

**Hiệu suất trên máy Boltzmann hạn chế** Đầu tiên, chúng tôi đã phân tích hiệu suất của SET trên một mô hình ANN ngẫu nhiên lưỡng phân không có hướng, tức là RBM28, phổ biến vì khả năng học không giám sát30 và hiệu suất cao như một trích xuất đặc trưng và ước lượng mật độ31. Mô hình mới xuất phát từ quy trình SET được đặt tên là SET-RBM. Trong tất cả các thử nghiệm, chúng tôi đặt ε = 11, và z = 0.3, thực hiện một tìm kiếm ngẫu nhiên nhỏ chỉ trên bộ dữ liệu MNIST, để có thể đánh giá xem hai siêu tham số này có đặc trưng cho bộ dữ liệu hay các giá trị của chúng đủ chung để hoạt động tốt cũng trên các bộ dữ liệu khác nhau.

Có ít nghiên cứu về tính thưa kết nối RBM10. Tuy nhiên, để có được ước lượng tốt về khả năng SET-RBM, chúng tôi đã so sánh nó với RBM FixProb10 (một mô hình RBM thưa với topology Erdős-Rényi cố định), RBM kết nối đầy đủ, và với kết quả tiên tiến của XBM từ10. Chúng tôi chọn RBM FixProb như một mô hình cơ sở thưa để có thể hiểu rõ hơn tác động của kết nối thích ứng SET-RBM đối với khả năng học của nó, vì cả hai mô hình, tức là SET-RBM và RBM FixProb, đều được khởi tạo với topology Erdős-Rényi. Chúng tôi thực hiện thử nghiệm trên 11 bộ dữ liệu chuẩn đến từ các lĩnh vực khác nhau, như được mô tả trong Bảng 1, sử dụng cùng phép chia cho dữ liệu đào tạo và kiểm tra như trong10. Tất cả các mô hình được đào tạo trong 5000 epoch sử dụng Divergence Tương phản29 (CD) với 1, 3, và 10 bước CD, tốc độ học 0.01, momentum 0.9, và weight decay 0.0002, như được thảo luận trong32. Chúng tôi đánh giá hiệu suất sinh của các mô hình được xem xét bằng cách tính log-xác suất trên dữ liệu kiểm tra sử dụng Annealed Importance Sampling (AIS)33, đặt tất cả tham số như trong10, 33. Chúng tôi đã sử dụng MATLAB cho tập hợp thử nghiệm này. Chúng tôi tự triển khai SET-RBM và RBM FixProb; trong khi cho RBM và AIS chúng tôi đã điều chỉnh mã được cung cấp bởi33.

Hình 2 mô tả hiệu suất của mô hình trên bộ dữ liệu DNA; trong khi Hình 8 trình bày kết quả trên tất cả bộ dữ liệu, sử dụng số lượng neuron ẩn khác nhau (tức là 100, 250, và 500 neuron ẩn cho bộ dữ liệu đánh giá UCI; và 500, 2500, và 5000 neuron ẩn cho bộ dữ liệu CalTech 101 Silhouettes và MNIST). Bảng 2 tóm tắt kết quả, trình bày người thực hiện tốt nhất cho mỗi loại mô hình cho mỗi bộ dữ liệu. Trong 7 trên 11 bộ dữ liệu, SET-RBM vượt trội hơn RBM kết nối đầy đủ, trong khi giảm tham số theo vài bậc độ lớn. Ví dụ, trên bộ dữ liệu MNIST, SET-RBM đạt -86.41 nats (đơn vị thông tin tự nhiên), với cải thiện 5.29 lần so với RBM kết nối đầy đủ, và giảm tham số xuống 2%. Trong 10 trên 11 bộ dữ liệu, SET-RBM vượt trội hơn XBM, đại diện cho kết quả tiên tiến trên các bộ dữ liệu này cho các biến thể thưa của RBM10. Thật thú vị khi thấy trong Bảng 2 rằng RBM FixProb đạt hiệu suất tốt nhất trên mỗi bộ dữ liệu trong trường hợp khi số lượng neuron ẩn tối đa được xem xét. Mặc dù SET-RBM có cùng lượng trọng số với RBM FixProb, nó đạt hiệu suất tối đa trên 3 trong 11 bộ dữ liệu được nghiên cứu chỉ khi một số lượng neuron ẩn trung bình được xem xét (tức là DNA, Mushrooms, và CalTech 101 Silhouettes 28x28).

Hình 2 và Hình 8 trình bày kết quả nổi bật về tính ổn định. RBM kết nối đầy đủ cho thấy các vấn đề không ổn định và overfitting. Ví dụ, sử dụng một bước CD trên bộ dữ liệu DNA, các RBM có đường cong học nhanh, đạt tối đa sau vài epoch. Sau đó, hiệu suất bắt đầu giảm cho thấy dấu hiệu rằng các mô hình bắt đầu bị overfitting. Hơn nữa, như mong đợi, các mô hình RBM với nhiều neuron ẩn hơn (Hình 2, b, c, e, f, h, i) overfitting thậm chí nhanh hơn so với những mô hình có ít neuron ẩn hơn (Hình 2, a, d, g). Hành vi tương tự có thể được thấy trong hầu hết các trường hợp được xem xét, đỉnh điểm là hành vi học rất gai góc trong một số trường hợp (Hình 8). Trái ngược với RBM kết nối đầy đủ, quy trình SET ổn định SET-RBM và tránh overfitting. Tình huống này có thể được quan sát thường xuyên hơn khi một số lượng neuron ẩn cao được chọn. Ví dụ, nếu chúng ta nhìn vào bộ dữ liệu DNA, độc lập với các giá trị của nh và nCD (Hình 2), chúng ta có thể quan sát rằng SET-RBM rất ổn định sau khi chúng đạt khoảng -85 nats, có hành vi học gần như phẳng sau điểm đó. Ngược lại, trên cùng bộ dữ liệu, RBM kết nối đầy đủ có hành vi học tốt ban đầu rất ngắn (cho vài epoch) và, sau đó, chúng đi lên và xuống trong 5000 epoch được phân tích, đạt hiệu suất tối thiểu là -160 nats (Hình 2, i). Lưu ý rằng những khả năng ổn định tốt và tránh overfitting này được tạo ra không chỉ bởi quy trình SET, mà còn bởi chính tính thưa, vì RBM FixProb cũng có hành vi ổn định trong hầu hết các trường hợp. Điều này xảy ra do số lượng tham số được tối ưu hóa rất nhỏ của các mô hình thưa so với số lượng tham số cao của các mô hình kết nối đầy đủ (như được phản ánh bởi thanh xếp chồng từ trục y bên phải của mỗi panel của Hình 2 và Hình 8) không cho phép quy trình học overfitting các mô hình thưa trên dữ liệu đào tạo.

Hơn nữa, chúng tôi đã xác minh giả thuyết ban đầu của chúng tôi về kết nối thưa trong SET-RBM. Hình 3 và Hình 9 cho thấy cách kết nối của neuron ẩn tự nhiên tiến hóa hướng tới topology không có tỷ lệ. Để đánh giá thực tế này, chúng tôi đã sử dụng giả thuyết null từ thống kê34, giả định rằng không có mối quan hệ giữa hai hiện tượng được đo. Để xem liệu giả thuyết null giữa phân phối độ của neuron ẩn và phân phối luật lũy thừa có thể bị bác bỏ hay không, chúng tôi đã tính p-value35, 36 giữa chúng sử dụng kiểm định một đuôi. Để bác bỏ giả thuyết null, p-value phải thấp hơn ngưỡng có ý nghĩa thống kê là 0.05. Trong tất cả các trường hợp (tất cả panel của Hình 3), nhìn vào p-values (trục y bên phải của các panel), chúng ta có thể thấy rằng ở đầu giai đoạn học, giả thuyết null không bị bác bỏ. Điều này là mong đợi, vì phân phối độ ban đầu của neuron ẩn là nhị thức do tính ngẫu nhiên của đồ thị ngẫu nhiên Erdős-Rényi37 được sử dụng để khởi tạo topology SET-RBM. Tiếp theo, trong giai đoạn học, chúng ta có thể thấy rằng, trong nhiều trường hợp, p-values giảm đáng kể dưới ngưỡng 0.05. Khi những tình huống này xảy ra, có nghĩa là phân phối độ của neuron ẩn trong SET-RBM bắt đầu xấp xỉ phân phối luật lũy thừa. Như mong đợi, các trường hợp với ít neuron hơn (Hình 3, a, b, d, e, g) không tiến hóa thành topology không có tỷ lệ, trong khi các trường hợp với nhiều neuron hơn luôn tiến hóa hướng tới topology không có tỷ lệ (Hình 3, c, f, h, i). Tóm lại, trong 70 trên 99 trường hợp được nghiên cứu (tất cả panel của Hình 9), kết nối neuron ẩn SET-RBM tiến hóa rõ ràng trong giai đoạn học từ topology Erdős-Rényi hướng tới topology không có tỷ lệ.

Hơn nữa, trong trường hợp neuron hiển thị, chúng tôi đã quan sát rằng kết nối của chúng có xu hướng tiến hóa thành một mẫu phụ thuộc vào dữ liệu miền. Để minh họa hành vi này, Hình 4 cho thấy những gì xảy ra với lượng kết nối cho mỗi neuron hiển thị trong quá trình đào tạo SET-RBM trên bộ dữ liệu MNIST và CalTech 101. Có thể quan sát rằng ban đầu các mẫu kết nối hoàn toàn ngẫu nhiên, như được cho bởi phân phối nhị thức của topology Erdős-Rényi. Sau khi các mô hình được đào tạo trong vài epoch, một số neuron hiển thị bắt đầu có nhiều kết nối hơn và những neuron khác có ít hơn và ít hơn. Cuối cùng, vào cuối quá trình đào tạo, một số cụm neuron hiển thị với kết nối rõ ràng khác nhau xuất hiện. Nhìn vào bộ dữ liệu MNIST, chúng ta có thể quan sát rõ ràng rằng trong cả hai trường hợp được phân tích (tức là 500 và 2500 neuron ẩn) một cụm với nhiều kết nối xuất hiện ở trung tâm. Đồng thời, ở các cạnh, một cụm khác xuất hiện trong đó mỗi neuron hiển thị có không hoặc rất ít kết nối. Cụm với nhiều kết nối tương ứng chính xác với vùng mà các chữ số xuất hiện trong hình ảnh. Trên bộ dữ liệu Caltech 101, hành vi tương tự có thể được quan sát, ngoại trừ thực tế rằng do tính biến đổi cao của hình dạng trên bộ dữ liệu này, cụm ít kết nối vẫn có một lượng kết nối đáng kể. Hành vi này của kết nối neuron hiển thị có thể được sử dụng, chẳng hạn, để thực hiện giảm chiều bằng cách phát hiện các đặc trưng quan trọng nhất trên các bộ dữ liệu chiều cao, hoặc để làm cho quá trình đào tạo SET-RBM nhanh hơn.

**Hiệu suất trên perceptron nhiều lớp** Để khám phá tốt hơn khả năng của SET, chúng tôi cũng đã đánh giá hiệu suất của nó trên các nhiệm vụ phân loại dựa trên học có giám sát. Chúng tôi phát triển một biến thể của MLP3, được gọi là SET-MLP, trong đó các lớp kết nối đầy đủ đã được thay thế bằng các lớp thưa thu được thông qua quy trình SET, với ε = 20, và z = 0.3. Chúng tôi giữ tham số z như trong trường hợp trước của SET-RBM, trong khi cho tham số ε chúng tôi thực hiện một tìm kiếm ngẫu nhiên nhỏ chỉ trên bộ dữ liệu MNIST. Chúng tôi so sánh SET-MLP với MLP kết nối đầy đủ tiêu chuẩn, và với một biến thể thưa của MLP có topology Erdős-Rényi cố định, được gọi là MLP FixProb. Để đánh giá, chúng tôi đã sử dụng ba bộ dữ liệu chuẩn (Bảng 1), hai đến từ lĩnh vực thị giác máy tính (MNIST và CIFAR10), và một từ vật lý hạt (bộ dữ liệu HIGGS1). Trong tất cả các trường hợp, chúng tôi đã sử dụng cùng kỹ thuật xử lý dữ liệu, kiến trúc mạng, phương pháp đào tạo (tức là SGD3 với tốc độ học cố định 0.01, momentum 0.9, và weight decay 0.0002), và tỷ lệ dropout 0.3 (Bảng 3). Sự khác biệt duy nhất giữa MLP, MLP FixProb, và SET-MLP, bao gồm kết nối topology của chúng. Chúng tôi đã sử dụng Python và thư viện Keras (https://github.com/fchollet/keras) với backend Theano38 cho tập hợp thử nghiệm này. Đối với MLP chúng tôi đã sử dụng triển khai Keras tiêu chuẩn, trong khi chúng tôi tự triển khai SET-MLP và MLP FixProb trên các thư viện Keras tiêu chuẩn.

Kết quả được mô tả trong Hình 5 cho thấy SET-MLP vượt trội hơn MLP FixProb. Hơn nữa, SET-MLP luôn vượt trội hơn MLP, trong khi có ít tham số hơn hai bậc độ lớn. Nhìn vào bộ dữ liệu CIFAR10, chúng ta có thể thấy rằng chỉ với 1% trọng số của MLP, SET-MLP dẫn đến những lợi ích đáng kể. Đồng thời, SET-MLP có kết quả so sánh được với các mô hình MLP tiên tiến sau khi chúng được tinh chỉnh cẩn thận. Để định lượng, mô hình MLP tốt thứ hai trong tài liệu trên CIFAR10 đạt khoảng 74.1% độ chính xác phân loại39 và có 31 triệu tham số: trong khi SET-MLP đạt độ chính xác tốt hơn (74.84%) chỉ có khoảng 0.3 triệu tham số. Hơn nữa, mô hình MLP tốt nhất trong tài liệu trên CIFAR10 có 78.62% độ chính xác40, với khoảng 12 triệu tham số, trong khi cũng hưởng lợi từ giai đoạn pre-training41, 42. Mặc dù chúng tôi chưa pre-train các mô hình MLP được nghiên cứu ở đây, chúng tôi nên đề cập rằng SET-RBM có thể dễ dàng được sử dụng để pre-train mô hình SET-MLP để cải thiện hiệu suất hơn nữa.

Đối với các vấn đề ổn định và overfitting, Hình 5 cho thấy SET-MLP cũng rất ổn định, tương tự như SET-RBM. Lưu ý rằng do việc sử dụng kỹ thuật dropout, MLP kết nối đầy đủ cũng khá ổn định. Về các đặc trưng topology, chúng ta có thể thấy từ Hình 5 rằng, tương tự như những gì được tìm thấy trong các thử nghiệm SET-RBM (Hình 3), các kết nối neuron ẩn trong SET-MLP nhanh chóng tiến hóa hướng tới phân phối luật lũy thừa.

Để hiểu rõ hơn tác động của các kỹ thuật chính quy hóa khác nhau, và hàm kích hoạt, chúng tôi thực hiện một thử nghiệm kiểm soát nhỏ trên bộ dữ liệu Fashion-MNIST. Chúng tôi chọn bộ dữ liệu này vì nó có kích thước tương tự với bộ dữ liệu MNIST, đồng thời là một vấn đề phân loại khó hơn. Chúng tôi sử dụng MLP, MLP FixProb, và SET-MLP với ba lớp ẩn mỗi lớp 1000 neuron ẩn. Sau đó, chúng tôi thay đổi cho mỗi mô hình những điều sau: (1) phương pháp chính quy hóa trọng số (tức là chính quy hóa L1 với tỷ lệ 0.0000001, chính quy hóa L2 với tỷ lệ 0.0002, và không chính quy hóa), (2) việc sử dụng (hoặc không sử dụng) momentum Nesterov, và (3) hai hàm kích hoạt (tức là SReLU43 và ReLU44). Các tỷ lệ chính quy hóa được tìm thấy bằng cách thực hiện một quy trình tìm kiếm ngẫu nhiên nhỏ với các mức L1 và L2 giữa 0.01 và 0.0000001 để cố gắng tối đa hóa hiệu suất của cả ba mô hình. Trong tất cả các trường hợp, chúng tôi sử dụng SGD với tốc độ học 0.01 để đào tạo các mô hình. Kết quả được mô tả trong Hình 6 cho thấy rằng, trong kịch bản cụ thể này, SET-MLP đạt hiệu suất tốt nhất nếu không có chính quy hóa hoặc chính quy hóa L2 được sử dụng cho trọng số, trong khi chính quy hóa L1 không cung cấp cùng mức hiệu suất. Tóm lại, SET-MLP đạt kết quả tốt nhất trên bộ dữ liệu Fashion-MNIST với các cài đặt sau: hàm kích hoạt SReLU, không có momentum Nesterov, và không có (hoặc với L2) chính quy hóa trọng số. Đây thực tế là các cài đặt mà chúng tôi sử dụng trong các thử nghiệm MLP được thảo luận ở trên. Đáng chú ý rằng độc lập với cài đặt cụ thể, kết luận chung được rút ra cho đến nay vẫn đúng. SET-MLP đạt hiệu suất tương tự (hoặc tốt hơn) so với MLP, trong khi có số lượng kết nối nhỏ hơn nhiều. Ngoài ra, SET-MLP luôn vượt trội rõ ràng so với MLP FixProb.

**Hiệu suất trên mạng neural tích chập** Vì một trong những mô hình ANN được sử dụng nhiều nhất ngày nay là CNN3, chúng tôi đã nghiên cứu ngắn gọn cách SET có thể được sử dụng trong kiến trúc CNN để thay thế các lớp kết nối đầy đủ của chúng bằng các đối tác tiến hóa thưa. Chúng tôi xem xét một kiến trúc CNN nhỏ tiêu chuẩn, tức là conv(32,(3,3))-dropout(0.3)-conv(32,(3,3))-pooling-conv(64,(3,3))-dropout(0.3)-conv(64,(3,3))-pooling-conv(128,(3,3))-dropout(0.3)-conv(128,(3,3))-pooling), trong đó các số trong ngoặc cho các lớp tích chập có nghĩa là (số lượng bộ lọc, (kích thước kernel)), và cho các lớp dropout đại diện cho tỷ lệ dropout. Sau đó, trên đỉnh các lớp tích chập, chúng tôi đã sử dụng: (1) hai lớp kết nối đầy đủ để tạo CNN tiêu chuẩn, (2) hai lớp thưa với topology Erdős-Rényi cố định để tạo CNN FixProb, và (3) hai lớp thưa tiến hóa để tạo SET-CNN. Đối với mỗi mô hình, mỗi lớp trong hai lớp trên đỉnh được theo sau bởi một lớp dropout(0.3). Trên đỉnh những lớp này, CNN, CNN FixProb, và SET-CNN cũng chứa một lớp softmax. Mặc dù SReLU dường như cung cấp hiệu suất hơi tốt hơn, chúng tôi sử dụng ReLU như hàm kích hoạt cho các neuron ẩn do việc sử dụng rộng rãi của nó. Chúng tôi sử dụng SGD để đào tạo các mô hình. Các thử nghiệm được thực hiện trên bộ dữ liệu CIFAR10. Kết quả được mô tả trong Hình 7. Chúng cho thấy, giống như trong các thử nghiệm trước đây với máy Boltzmann hạn chế và perceptron nhiều lớp, rằng SET-CNN có thể đạt độ chính xác tốt hơn CNN, mặc dù nó chỉ có khoảng 4% kết nối CNN. Để định lượng điều này, chúng tôi đề cập rằng trong các thử nghiệm của chúng tôi SET-CNN đạt tối đa 90.02% độ chính xác, CNN FixProb đạt tối đa 88.26% độ chính xác, trong khi CNN đạt tối đa 87.48% độ chính xác. Tương tự với các thử nghiệm RBM, chúng ta có thể quan sát rằng CNN bị ảnh hưởng bởi hành vi overfitting nhỏ, trong khi CNN FixProb và SET-CNN rất ổn định. Mặc dù mục tiêu của chúng tôi chỉ là cho thấy rằng SET có thể được kết hợp cũng với CNN được sử dụng rộng rãi và không tối ưu hóa kiến trúc các biến thể CNN để tăng hiệu suất, chúng tôi nhấn mạnh rằng, thực tế, SET-CNN đạt hiệu suất so sánh được với kết quả tiên tiến. Lợi ích của việc sử dụng SET trong CNN có hai mặt: để giảm tổng số tham số trong CNN, và để cho phép sử dụng các mô hình CNN lớn hơn.

Cuối cùng nhưng không kém phần quan trọng, trong tất cả các thử nghiệm được thực hiện, chúng tôi quan sát rằng SET khá ổn định đối với việc lựa chọn siêu tham số ε và z. Không có cách nào để nói rằng lựa chọn của chúng tôi đã mang lại hiệu suất tốt nhất có thể, mặc dù chúng tôi chỉ tinh chỉnh chúng trên một bộ dữ liệu, tức là MNIST, và chúng tôi đánh giá hiệu suất của chúng trên tất cả 15 bộ dữ liệu. Tuy nhiên, chúng tôi có thể nói rằng z = 0.3 cho cả SET-RBM và SET-MLP, và ε cụ thể cho mỗi loại mô hình, SET-RBM (ε = 11), SET-MLP (ε = 20), và SET-CNN (ε = 20) đủ tốt để vượt trội hơn tiên tiến.

Xem xét các bộ dữ liệu khác nhau được xem xét, chúng tôi nhấn mạnh rằng chúng tôi đã đánh giá cả tập hình ảnh tăng cường và tập không phải hình ảnh. Trên các bộ dữ liệu hình ảnh, CNN3 thường vượt trội hơn MLP. Tuy nhiên, CNN không khả thi trên các loại dữ liệu chiều cao khác, như dữ liệu sinh học (ví dụ: 45), hoặc dữ liệu vật lý lý thuyết (ví dụ: 1). Trong những trường hợp đó, MLP sẽ là lựa chọn tốt hơn. Đây thực tế là trường hợp của bộ dữ liệu HIGGS (Hình 5, e, f), trong đó SET-MLP đạt 78.47% độ chính xác phân loại và có khoảng 90000 tham số. Trong khi đó, một trong những mô hình MLP tốt nhất trong tài liệu đạt 78.54% độ chính xác, trong khi có gấp ba lần tham số hơn40.

## Thảo luận

Trong bài báo này chúng tôi đã giới thiệu SET, một quy trình đơn giản và hiệu quả để thay thế các lớp lưỡng phân kết nối đầy đủ của ANN bằng các lớp thưa. Chúng tôi đã xác thực cách tiếp cận của mình trên 15 bộ dữ liệu (từ các lĩnh vực khác nhau) và trên ba mô hình ANN được sử dụng rộng rãi, tức là RBM, MLP, và CNN. Chúng tôi đã đánh giá SET kết hợp với hai phương pháp đào tạo khác nhau, tức là divergence tương phản và gradient descent ngẫu nhiên, cho học không giám sát và có giám sát. Chúng tôi đã chỉ ra rằng SET có khả năng giảm bậc hai số lượng tham số của các lớp mạng neural lưỡng phân từ giai đoạn thiết kế ANN, mà không làm giảm độ chính xác. Trong hầu hết các trường hợp, SET-RBM, SET-MLP, và SET-CNN vượt trội hơn các đối tác kết nối đầy đủ của chúng. Hơn nữa, chúng luôn vượt trội hơn các đối tác không tiến hóa của chúng, tức là RBM FixProb, MLP FixProb, và CNN FixProb.

Chúng tôi có thể kết luận rằng quy trình SET phù hợp với các mạng phức tạp thế giới thực, trong đó kết nối các nút có xu hướng tiến hóa thành topology không có tỷ lệ46. Đặc trưng này có ý nghĩa quan trọng trong ANN: chúng ta có thể hình dung việc giảm thời gian tính toán bằng cách giảm số epoch đào tạo, nếu chúng ta sử dụng chẳng hạn các thuật toán gắn kết ưu tiên47 để tiến hóa nhanh hơn topology của các lớp ANN lưỡng phân hướng tới topology không có tỷ lệ. Tất nhiên, cải tiến có thể này phải được xử lý cẩn thận, vì việc buộc topology mô hình tiến hóa nhanh hơn không tự nhiên thành topology không có tỷ lệ có thể dễ bị lỗi - chẳng hạn, phân phối dữ liệu có thể không được khớp hoàn hảo. Một cải tiến có thể khác sẽ là phân tích cách loại bỏ các trọng số không quan trọng. Trong bài báo này, chúng tôi đã chỉ ra rằng việc SET trực tiếp loại bỏ các kết nối với trọng số gần nhất với số không là hiệu quả. Lưu ý rằng chúng tôi cũng đã thử loại bỏ các kết nối ngẫu nhiên, và, như mong đợi, điều này dẫn đến giảm độ chính xác đáng kể. Tương tự, khi chúng tôi thử loại bỏ các kết nối với trọng số lớn nhất, mô hình SET-MLP không thể học gì cả, hoạt động tương tự như một bộ phân loại ngẫu nhiên. Tuy nhiên, chúng tôi không loại trừ khả năng có thể có những cách tiếp cận tốt hơn, tinh vi hơn để loại bỏ kết nối, ví dụ: sử dụng các phương pháp gradient25, hoặc các số liệu trung tâm từ khoa học mạng48.

SET có thể được áp dụng rộng rãi để giảm các lớp kết nối đầy đủ thành topology thưa trong các loại ANN khác, ví dụ: mạng neural tái phát3, mạng học tăng cường sâu2, 49, và những loại khác. Để sử dụng quy mô lớn SET, từ môi trường học thuật đến công nghiệp, một bước nữa phải được đạt được. Hiện tại, tất cả các triển khai học sâu tiên tiến đều dựa trên các phép nhân ma trận dày đặc được tối ưu hóa rất tốt trên Đơn vị Xử lý Đồ họa (GPU), trong khi các phép nhân ma trận thưa cực kỳ hạn chế về hiệu suất50, 51. Do đó, cho đến khi phần cứng được tối ưu hóa cho các hoạt động giống SET xuất hiện (ví dụ: phép nhân ma trận thưa), người ta sẽ phải tìm một số giải pháp thay thế. Ví dụ: tính toán song song cấp thấp của kích hoạt neuron dựa chỉ trên các kết nối đến và batch dữ liệu của chúng để vẫn thực hiện phép nhân ma trận dày đặc và có dấu chân bộ nhớ thấp. Nếu những thách thức kỹ thuật phần mềm này được giải quyết, SET có thể chứng minh là cơ sở cho ANN lớn hơn nhiều, có lẽ ở quy mô tỷ nút, để chạy trong siêu máy tính. Ngoài ra, nó có thể dẫn đến việc xây dựng ANN nhỏ nhưng mạnh mẽ có thể được đào tạo trực tiếp trên các thiết bị tài nguyên thấp (ví dụ: các nút cảm biến không dây, điện thoại di động), mà không cần đầu tiên đào tạo chúng trên siêu máy tính và sau đó chuyển các mô hình được đào tạo đến các thiết bị tài nguyên thấp, như hiện tại được thực hiện bởi các cách tiếp cận tiên tiến13. Những khả năng mạnh mẽ này sẽ được kích hoạt bởi mối quan hệ tuyến tính giữa số lượng neuron và lượng kết nối giữa chúng được tạo ra bởi SET. ANN được xây dựng với SET sẽ có nhiều sức mạnh đại diện hơn, và khả năng thích ứng tốt hơn so với ANN tiên tiến hiện tại, và chúng tôi hy vọng rằng chúng sẽ tạo ra một hướng nghiên cứu mới trong trí tuệ nhân tạo.

## Phương pháp

**Mạng neural nhân tạo** ANN52 là các mô hình toán học, lấy cảm hứng từ mạng neural sinh học, có thể được sử dụng trong cả ba paradigm máy học (tức là học có giám sát53, học không giám sát53, và học tăng cường54). Điều này làm cho chúng rất linh hoạt và mạnh mẽ, như có thể định lượng bởi thành công đáng chú ý được ghi nhận gần đây bởi thế hệ ANN cuối cùng (còn được gọi là mạng neural nhân tạo sâu hoặc học sâu3) trong nhiều lĩnh vực từ thị giác máy tính3 đến chơi game2, 49. Giống như các đối tác sinh học của chúng, ANN được cấu thành bởi các neuron và kết nối có trọng số giữa các neuron này. Dựa trên mục đích và kiến trúc của chúng, có nhiều mô hình ANN, như máy Boltzmann hạn chế28, perceptron nhiều lớp55, mạng neural tích chập56, mạng neural tái phát57, và những loại khác. Nhiều mô hình ANN này chứa các lớp kết nối đầy đủ. Một lớp neuron kết nối đầy đủ có nghĩa là tất cả neuron của nó được kết nối đến tất cả neuron thuộc lớp kề của nó trong kiến trúc ANN. Cho mục đích của bài báo này, trong phần này chúng tôi mô tả ngắn gọn ba mô hình chứa các lớp kết nối đầy đủ, tức là máy Boltzmann hạn chế28, perceptron nhiều lớp55, và mạng neural tích chập3.

Máy Boltzmann hạn chế là một mạng neural sinh, ngẫu nhiên hai lớp có khả năng học phân phối xác suất trên một tập đầu vào28 theo cách không giám sát. Từ góc độ topology, nó chỉ cho phép kết nối giữa các lớp. Hai lớp của nó là: lớp hiển thị, trong đó các neuron đại diện cho dữ liệu đầu vào; và lớp ẩn, trong đó các neuron đại diện cho các đặc trưng được trích xuất tự động bởi mô hình RBM từ dữ liệu đầu vào. Mỗi neuron hiển thị được kết nối đến tất cả neuron ẩn thông qua một kết nối không có hướng có trọng số, dẫn đến topology kết nối đầy đủ giữa hai lớp. Do đó, dòng thông tin là hai chiều trong RBM, từ lớp hiển thị đến lớp ẩn, và từ lớp ẩn đến lớp hiển thị, tương ứng. RBM, bên cạnh việc rất thành công trong việc cung cấp trọng số khởi tạo rất tốt cho đào tạo có giám sát của kiến trúc mạng neural nhân tạo sâu42, cũng rất thành công như các mô hình độc lập trong nhiều nhiệm vụ khác nhau, như ước lượng mật độ để mô hình hóa lựa chọn con người31, lọc cộng tác58, truy xuất thông tin59, phân loại đa lớp60, và những loại khác.

Perceptron Nhiều Lớp55 là một mô hình ANN truyền thẳng cổ điển ánh xạ một tập dữ liệu đầu vào đến tập dữ liệu đầu ra tương ứng. Do đó, nó được sử dụng cho học có giám sát. Nó được cấu thành bởi một lớp đầu vào trong đó các neuron đại diện cho dữ liệu đầu vào, một lớp đầu ra trong đó các neuron đại diện cho dữ liệu đầu ra, và một số lượng tùy ý các lớp ẩn ở giữa, với các neuron đại diện cho các đặc trưng ẩn của dữ liệu đầu vào (được khám phá tự động). Dòng thông tin trong MLP là một chiều, bắt đầu từ lớp đầu vào hướng tới lớp đầu ra. Do đó, các kết nối là một chiều và tồn tại chỉ giữa các lớp liên tiếp. Bất kỳ hai lớp liên tiếp nào trong MLP đều kết nối đầy đủ. Không có kết nối giữa các neuron thuộc cùng lớp, hoặc giữa các neuron thuộc các lớp không liên tiếp. Trong61, đã được chứng minh rằng MLP là bộ xấp xỉ hàm phổ quát, vì vậy chúng có thể được sử dụng để mô hình hóa bất kỳ loại vấn đề hồi quy hoặc phân loại nào.

Mạng Neural Tích chập3 là một lớp mạng neural truyền thẳng chuyên biệt cho nhận dạng hình ảnh, đại diện cho tiên tiến trên các loại vấn đề này. Chúng thường chứa một lớp đầu vào, một lớp đầu ra, và một số lớp ẩn ở giữa. Từ dưới lên trên, các lớp ẩn đầu tiên là các lớp tích chập, lấy cảm hứng từ vỏ não thị giác sinh học, trong đó mỗi neuron nhận thông tin chỉ từ các neuron lớp trước thuộc trường tiếp nhận của nó. Sau đó, các lớp ẩn cuối cùng là những lớp kết nối đầy đủ.

Nói chung, làm việc với các mô hình ANN bao gồm hai giai đoạn: 1) đào tạo (hoặc học), trong đó các kết nối có trọng số giữa các neuron được tối ưu hóa sử dụng các thuật toán khác nhau (ví dụ: quy trình lan truyền ngược kết hợp với gradient descent ngẫu nhiên62, 63 được sử dụng trong MLP hoặc CNN, divergence tương phản29 được sử dụng trong RBM) để giảm thiểu hàm mất được định nghĩa bởi mục đích của chúng; và 2) suy luận, trong đó mô hình ANN được tối ưu hóa được sử dụng để hoàn thành mục đích của nó.

**Mạng phức tạp không có tỷ lệ** Mạng phức tạp (ví dụ: mạng neural sinh học, diễn viên và phim, lưới điện, mạng giao thông) có ở khắp mọi nơi, ở những hình thức khác nhau, và các lĩnh vực khác nhau (từ sinh học thần kinh đến vật lý thống kê4). Chính thức, mạng phức tạp là một đồ thị với các đặc trưng topology không tầm thường, do con người hoặc tự nhiên tạo ra. Một trong những loại đặc trưng topology được biết đến rộng rãi nhất và được nghiên cứu sâu sắc trong mạng phức tạp là không có tỷ lệ, do thực tế rằng một phạm vi rộng các mạng phức tạp thế giới thực có topology này. Một mạng với topology không có tỷ lệ7 là một đồ thị thưa64 có phân phối độ luật lũy thừa xấp xỉ P(d) ∼ d^−γ, trong đó phần P(d) từ tổng số nút của mạng có d kết nối đến các nút khác, và tham số γ thường nằm trong khoảng γ ∈ (2; 3).

**Hộp 1 Mã giả Đào tạo Tiến hóa Thưa (SET) được mô tả chi tiết trong Thuật toán 1.**

```
1 % Khởi tạo;
2 khởi tạo mô hình ANN;
3 đặt ε và z;
4 foreach lớp kết nối đầy đủ (FC) lưỡng phân của ANN do
5   thay thế FC bằng lớp Kết nối Thưa (SC) có topology Erdős-Rényi được cho bởi ε và Eq.1;
6 end
7 khởi tạo tham số thuật toán đào tạo;
8 % Đào tạo;
9 foreach epoch đào tạo e do
10   thực hiện quy trình đào tạo tiêu chuẩn;
11   thực hiện cập nhật trọng số;
12   foreach lớp SC lưỡng phân của ANN do
13     loại bỏ một phần z của các trọng số dương nhỏ nhất;
14     loại bỏ một phần z của các trọng số âm lớn nhất;
15     if e không phải là epoch đào tạo cuối cùng then
16       thêm ngẫu nhiên trọng số mới (kết nối) với cùng lượng như những trọng số đã loại bỏ trước đó;
17     end
18   end
19 end
```

**Thuật toán 1: Mã giả SET**

**Tính khả dụng dữ liệu** Dữ liệu được sử dụng trong bài báo này là các bộ dữ liệu công khai, có sẵn miễn phí trực tuyến, như được phản ánh bởi các trích dẫn tương ứng của chúng từ Bảng 1. Các triển khai phần mềm nguyên mẫu của các mô hình được sử dụng trong nghiên cứu này có sẵn miễn phí trực tuyến tại https://github.com/dcmocanu/sparse-evolutionary-artificial-neural-networks

## Tài liệu tham khảo

1. Baldi, P., Sadowski, P. & Whiteson, D. Searching for exotic particles in high-energy physics with deep learning. Nat. Commun. 5, 4308 (2014).
2. Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015).
3. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436–444 (2015).
4. Strogatz, S. H. Exploring complex networks. Nature 410, 268–276 (2001).
5. Pessoa, L. Understanding brain networks and brain organization. Phys. Life Rev. 11, 400 – 435 (2014).
6. Bullmore, E. & Sporns, O. Complex brain networks: graph theoretical analysis of structural and functional systems. Nat. Rev. Neurosci. 10, 186–198 (2009).
7. Barabási, A.-L. & Albert, R. Emergence of scaling in random networks. Science 286, 509–512 (1999).
8. Watts, D. J. & Strogatz, S. H. Collective dynamics of 'small-world' networks. Nature 393, 440–442 (1998).
9. Mocanu, D. C. On the synergy of network science and artificial intelligence. In Proc. 25th International Joint Conference on Artificial Intelligence, 4020–4021 (2016).
10. Mocanu, D. C., Mocanu, E., Nguyen, P. H., Gibescu, M. & Liotta, A. A topological insight into restricted boltzmann machines. Mach. Learn. 104, 243–270 (2016).
11. Dieleman, S. & Schrauwen, B. Accelerating sparse restricted boltzmann machine training using non-gaussianity measures. In Proc. Deep Learning and Unsupervised Feature Learning, 9 (2012).
12. Yosinski, J. & Lipson, H. Visually debugging restricted boltzmann machine training with a 3d example. In Representation Learning Workshop, 29th International Conference on Machine Learning (2012).
13. Han, S., Pool, J., Tran, J. & Dally, W. Learning both weights and connections for efficient neural network. In Proc. Advances in Neural Information Processing Systems 28, 1135–1143 (2015).
14. Mocanu, D. C. et al. No-reference video quality measurement: added value of machine learning. J. Electron. Imaging 24, 061208 (2015).
15. Whiteson, S. & Stone, P. Evolutionary function approximation for reinforcement learning. J. Mach. Learn. Res. 7, 877–917 (2006).
16. McDonnell, J. R. & Waagen, D. Evolving neural network connectivity. In Proc. IEEE International Conference on Neural Networks, 863–868 vol.2 (1993).
17. Miikkulainen, R. et al. Evolving deep neural networks. Preprint at https://arxiv.org/abs/1703.00548 (2017).
18. Kowaliw, T., Bredeche, N., Chevallier, S. & Doursat, R. Artificial neurogenesis: An introduction and selective review. In Growing Adaptive Machines: Combining Development and Learning in Artificial Neural Networks, 1–60 (Springer Berlin Heidelberg, Berlin, Heidelberg, 2014).
19. Stanley, K. O. & Miikkulainen, R. Evolving neural networks through augmenting topologies. Evol. Comput. 10, 99–127 (2002).
20. Hausknecht, M., Lehman, J., Miikkulainen, R. & Stone, P. A neuroevolution approach to general atari game playing. IEEE T. Comp. Intel. AI 6, 355–366 (2014).
21. Miconi, T. Neural networks with differentiable structure. Preprint at https://arxiv.org/abs/1606.06216 (2016).
22. Salimans, T., Ho, J., Chen, X., Sidor, S. & Openai, I. S. Evolution Strategies as a Scalable Alternative to Reinforcement Learning. Preprint at https://arxiv.org/abs/1703.03864 (2017).
23. Such, F. P. et al. Deep Neuroevolution: Genetic Algorithms are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning. Preprint at https://arxiv.org/abs/1712.06567 (2018).
24. Erdős, P. & Rényi, A. On random graphs i. Publ. Math-Debrecen 6, 290–297 (1959).
25. Weigend, A. S., Rumelhart, D. E. & Huberman, B. A. Generalization by weight-elimination with application to forecasting. In Proc. Advances in Neural Information Processing Systems 3, 875–882 (Morgan-Kaufmann, 1991).
26. Diering, G. H. et al. Homer1a drives homeostatic scaling-down of excitatory synapses during sleep. Science 355, 511–515 (2017).
27. de Vivo, L. et al. Ultrastructural evidence for synaptic scaling across the wake/sleep cycle. Science 355, 507–510 (2017).
28. Smolensky, P. Information processing in dynamical systems: Foundations of harmony theory. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1, 194–281 (MIT Press, Cambridge, 1986).
29. Hinton, G. E. Training Products of Experts by Minimizing Contrastive Divergence. Neural Comput. 14, 1771–1800 (2002).
30. Bengio, Y. Learning deep architectures for ai. Found. Trends Mach. Learn. 2, 1–127 (2009).
31. Osogami, T. & Otsuka, M. Restricted boltzmann machines modeling human choice. In Proc. Advances in Neural Information Processing Systems 27, 73–81 (2014).
32. Hinton, G. A practical guide to training restricted boltzmann machines. In Neural Networks: Tricks of the Trade, vol. 7700 of Lecture Notes in Computer Science, 599–619 (Springer, 2012).
33. Salakhutdinov, R. & Murray, I. On the quantitative analysis of deep belief networks. In Proc. 25th International Conference on Machine Learning, 872–879 (2008).
34. Everitt, B. The Cambridge dictionary of statistics (Cambridge University Press, Cambridge, UK; New York, 2002).
35. Nuzzo, R. Scientific method: Statistical errors. Nature 506, 150–152 (2014).
36. Clauset, A., Shalizi, C. R. & Newman, M. E. J. Power-law distributions in empirical data. SIAM Rev. 51, 661–703 (2009).
37. Newman, M. E., Strogatz, S. H. & Watts, D. J. Random graphs with arbitrary degree distributions and their applications. Phys. Rev. E 64, 026118 (2001).
38. Al-Rfou, R., Alain, G., Almahairi, A. & et al., C. A. Theano: A Python framework for fast computation of mathematical expressions. Preprint at https://arxiv.org/abs/1605.02688 (2016).
39. Urban, G. et al. Do deep convolutional nets really need to be deep and convolutional? Proc. 5th International Conference on Learning Representations (2016).
40. Lin, Z., Memisevic, R. & Konda, K. How far can we go without convolution: Improving fully-connected networks. Preprint at https://arxiv.org/abs/1511.02580 (2015).
41. Hinton, G. E. & Salakhutdinov, R. R. Reducing the Dimensionality of Data with Neural Networks. Science 313, 504–507 (2006).
42. Hinton, G. E., Osindero, S. & Teh, Y.-W. A fast learning algorithm for deep belief nets. Neural Comput. 18, 1527–1554 (2006).
43. Jin, X. et al. Deep learning with s-shaped rectified linear activation units. In Proc. 30th AAAI Conference on Artificial Intelligence, 1737–1743 (2016).
44. Nair, V. & Hinton, G. E. Rectified linear units improve restricted boltzmann machines. In Proc. 27th International Conference on Machine Learning, 807–814 (2010).
45. Danziger, S. A. et al. Functional census of mutation sequence spaces: the example of p53 cancer rescue mutants. IEEE ACM T. Comput. Bi. 3, 114–125 (2006).
46. Barabási, A.-L. Network science (Cambridge University Press, 2016).
47. Albert, R. & Barabási, A.-L. Statistical mechanics of complex networks. Rev. Mod. Phys. 74, 47–97 (2002).
48. Mocanu, D. C., Exarchakos, G. & Liotta, A. Decentralized dynamic understanding of hidden relations in complex networks. Scientific Reports 8, 1571 (2018).
49. Silver, D. et al. Mastering the game of go with deep neural networks and tree search. Nature 529, 484–489 (2016).
50. Lebedev, V. & Lempitsky, V. Fast convnets using group-wise brain damage. In Proc. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2554–2564 (2016).
51. Changpinyo, S., Sandler, M. & Zhmoginov, A. The power of sparsity in convolutional neural networks. Preprint at https://arxiv.org/abs/1702.06257 (2017).
52. Bishop, C. M. Pattern Recognition and Machine Learning (Information Science and Statistics) (Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006).
53. Hastie, T., Tibshirani, R. & Friedman, J. The Elements of Statistical Learning (Springer New York Inc., New York, NY, USA, 2001).
54. Sutton, R. S. & Barto, A. G. Introduction to Reinforcement Learning (MIT Press, Cambridge, MA, USA, 1998).
55. Rosenblatt, F. Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms (Spartan, 1962).
56. LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86, 2278–2324 (1998).
57. Graves, A. et al. A novel connectionist system for unconstrained handwriting recognition. IEEE Trans. Pattern Anal. Mach. Intell. 31, 855–868 (2009).
58. Salakhutdinov, R., Mnih, A. & Hinton, G. Restricted boltzmann machines for collaborative filtering. In Proc. 24th International Conference on Machine Learning, 791–798 (2007).
59. Gehler, P. V., Holub, A. D. & Welling, M. The rate adapting poisson model for information retrieval and object recognition. In Proc. 23rd International Conference on Machine Learning, 337–344 (2006).
60. Larochelle, H. & Bengio, Y. Classification using discriminative restricted boltzmann machines. In Proc. 25th International Conference on Machine Learning, 536–543 (2008).
61. Cybenko, G. Approximation by superpositions of a sigmoidal function. Math. Control Signal 2, 303–314 (1989).
62. Rumelhart, D., Hintont, G. & Williams, R. Learning representations by back-propagating errors. Nature 323, 533–536 (1986).
63. Bottou, L. & Bousquet, O. The tradeoffs of large scale learning. In Proc. Advances in Neural Information Processing Systems 20, 161–168 (2008).
64. Del Genio, C. I., Gross, T. & Bassler, K. E. All scale-free networks are sparse. Phys. Rev. Lett. 107, 178701 (2011).
65. Larochelle, H. & Murray, I. The neural autoregressive distribution estimator. In Proc. 14th International Conference on Artificial Intelligence and Statistics, 29–37 (2011).
66. Marlin, B. M., Swersky, K., Chen, B. & de Freitas, N. Inductive principles for restricted boltzmann machine learning. In Proc. 13th International Conference on Artificial Intelligence and Statistics, 509–516 (2010).
67. LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, vol. 86, 2278–2324 (1998).
68. Krizhevsky, A. Learning Multiple Layers of Features from Tiny Images. Master's thesis (2009).
69. Xiao, H., Rasul, K. & Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. Preprint at https://arxiv.org/abs/1708.07747 (2017).

**Tuyên bố đóng góp tác giả**
D.C.M. và E.M. hình thành ý tưởng ban đầu. D.C.M., E.M., P.S., P.H.N., M.G., và A.L. thiết kế các thử nghiệm và phân tích kết quả. D.C.M. thực hiện các thử nghiệm. D.C.M., E.M., P.S., P.H.N., M.G., và A.L. viết bản thảo.

**Lợi ích cạnh tranh**
Các tác giả tuyên bố không có lợi ích cạnh tranh.

**Vật liệu và Thư từ**
Thư từ và yêu cầu vật liệu nên được gửi đến D.C. Mocanu (d.c.mocanu@tue.nl).
