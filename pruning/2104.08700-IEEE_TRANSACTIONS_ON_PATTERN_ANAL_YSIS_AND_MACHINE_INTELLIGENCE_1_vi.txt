# 2104.08700.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2104.08700.pdf
# Kích thước tệp: 2017851 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1
Lottery Jackpots Tồn Tại Trong Các Mô Hình Đã Được Huấn Luyện Trước
Yuxin Zhang, Mingbao Lin, Yunshan Zhong, Fei Chao, Member, IEEE , Rongrong Ji, Senior Member, IEEE
Tóm tắt—Tỉa mạng (Network pruning) là một phương pháp hiệu quả để giảm độ phức tạp của mạng với sự thỏa hiệp hiệu suất có thể chấp nhận được. Các nghiên cứu hiện tại đạt được độ thưa thớt của mạng nơ-ron thông qua việc huấn luyện trọng số tốn thời gian hoặc tìm kiếm phức tạp trên các mạng có độ rộng mở rộng, điều này hạn chế rất nhiều các ứng dụng của việc tỉa mạng. Trong bài báo này, chúng tôi chỉ ra rằng các mạng con hiệu suất cao và thưa thớt mà không cần đến việc huấn luyện trọng số, được gọi là "lottery jackpots", tồn tại trong các mô hình đã được huấn luyện trước với độ rộng không mở rộng. Các lottery jackpots của chúng tôi có thể truy vết được thông qua các kết quả thực nghiệm và lý thuyết. Ví dụ, chúng tôi thu được một lottery jackpot chỉ có 10% tham số và vẫn đạt được hiệu suất của VGGNet-19 dày đặc gốc mà không có bất kỳ sửa đổi nào trên các trọng số đã được huấn luyện trước trên CIFAR-10. Hơn nữa, chúng tôi cải thiện hiệu quả tìm kiếm lottery jackpots từ hai góc độ. Thứ nhất, chúng tôi quan sát thấy rằng các mask thưa thớt được tạo ra từ nhiều tiêu chí tỉa hiện tại có sự chồng lấp cao với mask được tìm kiếm của lottery jackpot của chúng tôi, trong đó, việc tỉa dựa trên độ lớn tạo ra mask tương tự nhất với của chúng tôi. Tuân theo thông tin này, chúng tôi khởi tạo mask thưa thớt của mình sử dụng việc tỉa dựa trên độ lớn, dẫn đến giảm chi phí ít nhất 3× trong việc tìm kiếm lottery jackpot trong khi đạt được hiệu suất tương đương hoặc thậm chí tốt hơn. Thứ hai, chúng tôi tiến hành phân tích sâu về quá trình tìm kiếm lottery jackpots. Kết quả lý thuyết của chúng tôi gợi ý rằng việc giảm loss huấn luyện trong quá trình tìm kiếm trọng số có thể bị cản trở bởi sự phụ thuộc giữa các trọng số trong các mạng hiện đại. Để giảm thiểu điều này, chúng tôi đề xuất một phương pháp hạn chế ngắn mới để hạn chế thay đổi các mask có thể có tác động tiêu cực tiềm ẩn đến loss huấn luyện, dẫn đến hội tụ nhanh hơn và giảm dao động cho việc tìm kiếm lottery jackpots. Do đó, lottery jackpot được tìm kiếm của chúng tôi loại bỏ 90% trọng số trong ResNet-50, trong khi dễ dàng đạt được hơn 70% độ chính xác top-1 chỉ sử dụng 5 epoch tìm kiếm trên ImageNet. Mã nguồn của chúng tôi có sẵn tại https://github.com/zyxxmu/lottery-jackpots.
Thuật ngữ chỉ mục—Mạng nơ-ron tích chập, Tỉa mạng, Giả thuyết vé số may mắn.
✦
1 GIỚI THIỆU
ĐỘ phức tạp mô hình ngày càng tăng đã hạn chế rất nhiều các ứng dụng thực tế của mạng nơ-ron sâu (DNNs) trên các thiết bị edge. Nhiều phương pháp đã được đề xuất để giảm thiểu trở ngại này bởi cộng đồng học sâu. Nói chung, nghiên cứu hiện tại có thể được chia thành tỉa mạng [1], [2], lượng tử hóa tham số [3], [4], [5], phân tách hạng thấp [6], [7] và chưng cất tri thức [8], [9]. Trong số các kỹ thuật này, tỉa mạng đã được biết đến như một trong những phương pháp hàng đầu với việc giảm đáng kể độ phức tạp mạng và suy giảm hiệu suất có thể chấp nhận được [10], [11], [12].

Với một mạng nơ-ron quy mô lớn, tỉa mạng loại bỏ một phần các kết nối mạng để thu được một mạng con thưa thớt. Các thuật toán tỉa mở rộng đã được đề xuất trong vài năm qua [13], [14], [15]. Các phương pháp điển hình tạo ra các tiêu chí quan trọng khác nhau để tỉa trọng số dựa trên các mô hình đã được huấn luyện trước, điều này hợp lý vì các mô hình đã được huấn luyện trước hầu hết có thể nhìn thấy trên Internet, hoặc có sẵn từ client [1], [16], [17]. Các nghiên cứu khác tiến hành tỉa từ đầu bằng cách áp đặt một điều chỉnh độ thưa thớt lên loss mạng, hoặc trực tiếp tỉa từ các trọng số được khởi tạo ngẫu nhiên [18], [19], [20].

Mặc dù đã có tiến bộ trong việc giảm kích thước của các tham số mạng với ít suy giảm độ chính xác, các phương pháp hiện tại vẫn yêu cầu một quá trình huấn luyện trọng số tốn thời gian để khôi phục hiệu suất của các mô hình đã được tỉa như được hiển thị trong Hình 1. Ví dụ, khi tỉa ResNet-50 [21] nổi tiếng trên ImageNet [22], hầu hết các phương pháp yêu cầu hơn 100 epoch để huấn luyện mô hình đã được tỉa [18], [19], [23]. Do đó, các mô hình nén đi kèm với chi phí huấn luyện trọng số đắt đỏ, điều này hạn chế rất nhiều các ứng dụng thực tế của các nghiên cứu hiện tại.

Giả thuyết vé số may mắn [26] tiết lộ rằng một mạng được khởi tạo ngẫu nhiên chứa các mạng con vé số may mắn có thể đạt được hiệu suất tốt sau khi huấn luyện trọng số thích hợp. Dựa trên điều này, các nghiên cứu gần đây hơn phát hiện thêm rằng những vé số may mắn này xuất hiện ngay cả khi không cần thiết phải huấn luyện trọng số [27], [28], [29]. Tuy nhiên, một thuật toán tìm kiếm phức tạp phải được tiến hành trên các trọng số được khởi tạo ngẫu nhiên, chi phí của điều này thậm chí còn cao hơn so với việc huấn luyện các trọng số vì độ rộng của mạng gốc thường được mở rộng theo cấp số nhân để đảm bảo việc tìm ra các vé số may mắn hiệu suất cao. Hơn nữa, hiệu suất của các mạng con như vậy vẫn còn kém xa so với các phương pháp tỉa huấn luyện trọng số hiện tại [18], [19], [23].

Trong bài báo này, chúng tôi tiết lộ một cách sáng tạo rằng các mạng con hiệu suất cao có thể được định vị trong các mô hình đã được huấn luyện trước mà không cần đến việc huấn luyện trọng số từ các góc độ thực nghiệm và lý thuyết. Chúng tôi gọi các mạng con này là lottery jackpots trong bài báo này. Thứ nhất, khác với các công trình hiện tại [27], [28], [29] tìm kiếm các vé số may mắn trong một mạng được khởi tạo ngẫu nhiên với việc mở rộng độ rộng, các lottery jackpots của chúng tôi được xây dựng trên một mô hình đã được huấn luyện trước không mở rộng. Ví dụ, một lottery jackpot có thể được tìm thấy trong VGGNet-19 [30] đã được huấn luyện trước trên CIFAR-10 [31], chỉ có 10% tham số của mạng gốc trong khi vẫn đạt được hiệu suất của mô hình đầy đủ mà không có bất kỳ sửa đổi nào trên các trọng số đã được huấn luyện.

--- TRANG 2 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2

Tuy nhiên, thuật toán edge-popup có sẵn [27] mà chúng tôi tận dụng để tìm lottery jackpots tốn thời gian. Nó mất gần như cùng chi phí tính toán so với các phương pháp huấn luyện trọng số hiện tại [25], [32], điều này cản trở rất nhiều giá trị ứng dụng của lottery jackpots.

Để giảm bớt vấn đề trên, chúng tôi tiếp tục đề xuất cải thiện hiệu quả tìm kiếm của lottery jackpots từ hai góc độ, được triển khai như khởi tạo mask và tìm kiếm mask. Hướng tới mục tiêu đầu tiên, chúng tôi xem xét các tiêu chí tỉa dựa trên huấn luyện trọng số hiện tại [1], [25], [32], và sau đó quan sát thấy sự chồng lấp cao giữa các mask thưa thớt từ các tiêu chí tỉa hiện tại này và mask được tìm kiếm từ lottery jackpot của chúng tôi. Trong số đó, tiêu chí tỉa dựa trên độ lớn tạo ra mẫu thưa thớt tương tự nhất. Điều này truyền cảm hứng cho chúng tôi khởi tạo mask của mình với việc tỉa độ lớn như một sự khởi động để tìm kiếm lottery jackpots của chúng tôi. Kết quả là, ít nhất 3× giảm chi phí tìm kiếm được quan sát khi so sánh với các phương pháp tìm kiếm trọng số hiện tại trên các mạng được khởi tạo ngẫu nhiên [27], [28] hoặc các phương pháp huấn luyện trọng số [18], [19], [24].

Tiếp theo, chúng tôi nghiên cứu cách tăng hiệu quả của quá trình tìm kiếm lottery jackpots. Vì các trọng số đã được huấn luyện trước được cố định trong giai đoạn tìm kiếm, cơ hội duy nhất để giảm loss huấn luyện rơi vào việc tỉa các trọng số được bảo tồn trong các lần lặp tìm kiếm trước đó và hồi sinh lại cùng số lượng trọng số đã được tỉa, điều mà chúng tôi gọi là hoán đổi trọng số trong bài báo này. Sau đó, chúng tôi chứng minh toán học rằng việc giảm loss được thực hiện bởi hoán đổi trọng số như vậy trong thuật toán edge-popup bị giới hạn bởi một lỗi biến dạng liên quan đến sự phụ thuộc giữa các trọng số trong các DNN hiện đại, hạn chế hiệu quả tìm kiếm của lottery jackpots. Để đối phó với nhược điểm này, chúng tôi trình bày một hạn chế ngắn popup trực quan nhưng hiệu quả, ngăn chặn một cách tham lam việc hoán đổi trọng số nhận được ít hoặc thậm chí tác động tiêu cực để tối thiểu hóa loss huấn luyện. Do đó, hội tụ nhanh hơn và ít dao động hơn có thể đạt được để tìm kiếm lottery jackpots.

Các thí nghiệm mở rộng đã chứng minh rằng hạn chế ngắn popup được đề xuất của chúng tôi, được gọi là SR-popup, có thể định vị hiệu quả các lottery jackpots hiệu suất cao trong nhiều DNN đại diện bao gồm ResNet [21], MobileNet-V1 [33], v.v. Ví dụ, SR-popup thành công tìm kiếm một lottery jackpot loại bỏ 90% trọng số của ResNet-50 [21] trong khi đạt độ chính xác top-1 là 70% chỉ sử dụng 5 epoch tìm kiếm trên ImageNet và đạt được hiệu suất tương đương với các phương pháp tiên tiến chỉ sử dụng 30 epoch tìm kiếm, như được hiển thị trong Hình 1.

Nhìn chung, đóng góp của chúng tôi được tóm tắt như sau:
• Chúng tôi phát hiện rằng các mô hình đã được huấn luyện trước chứa các mạng con hiệu suất cao mà không cần thiết phải huấn luyện trọng số. Hơn nữa, việc tìm kiếm của chúng tôi cho các mạng con này được xây dựng dựa trên việc không mở rộng độ rộng mạng.
• Chúng tôi phát hiện rằng các tiêu chí tỉa dựa trên huấn luyện trọng số hiện tại thường tạo ra một mask thưa thớt tương tự với mask được tìm kiếm của chúng tôi. Được truyền cảm hứng bởi khám phá này, chúng tôi đề xuất sử dụng mask thưa thớt dựa trên độ lớn như một sự khởi động cho việc tìm kiếm trọng số, dẫn đến tìm kiếm hiệu quả hơn cho lottery jackpots của chúng tôi.
• Với đảm bảo lý thuyết, chúng tôi đề xuất một hạn chế ngắn popup mới, thích ứng bảo tồn các thay đổi mask mang lại những giảm loss dự kiến lớn nhất, sao cho tính bất ổn tìm kiếm của phương pháp edge-popup trước đó được giảm bớt hiệu quả.
• Các thí nghiệm mở rộng chứng minh tính hiệu quả và hiệu suất của phương pháp được đề xuất của chúng tôi cho việc tỉa mạng. Các mạng con hiệu suất cao có thể được tìm thấy nhanh chóng trong các mô hình đã được huấn luyện trước với ít nhất 3× giảm chi phí tính toán so với các phương pháp hiện tại.

2 CÔNG TRÌNH LIÊN QUAN
Tỉa Mạng Nơ-ron. Tỉa mạng nơ-ron đã được chứng minh là một phương pháp hiệu quả để nén các mô hình lớn trong các thập kỷ qua [16], [34], [35]. Các kỹ thuật sớm hơn thường thực hiện tỉa bằng cách thiết kế các tiêu chí quan trọng khác nhau trên các mô hình đã được huấn luyện trước [1], [13], [16]. Ví dụ, Han et al. [1] xem xét độ lớn của trọng số và Molchanov et al. [36] coi khai triển Taylor xấp xỉ các thay đổi trong hàm loss như nguyên tắc tỉa.

Gần đây, nhiều nghiên cứu đã đặt câu hỏi về vai trò của các mô hình đã được huấn luyện trước bằng cách thiết kế các thuật toán tỉa hiệu suất tốt mà không phụ thuộc vào các mô hình đã được huấn luyện trước. Ví dụ, Sparse Evolutionary Training (SET) [24] trồng trọt các trọng số thưa thớt trong suốt quá trình huấn luyện theo cách tỉa-phân phối lại-tái sinh trong khi huấn luyện các mô hình từ đầu. Kusupati et al. [23] đã thiết kế Soft Threshold Reparameterization (STR) để học ngân sách thưa thớt không đồng nhất qua các lớp. Evci et al. [19] tiếp tục đề xuất RigL,

--- TRANG 3 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 3

sử dụng thông tin độ lớn trọng số và gradient để cải thiện tối ưu hóa mạng thưa thớt. Các công trình khác trực tiếp tỉa một mô hình được khởi tạo ngẫu nhiên bằng cách xem xét độ nhạy kết nối như một thước đo quan trọng [25] hoặc tối đa hóa gradient của mạng con đã được tỉa [32].

Tỉa mạng cũng có thể được xem như việc tìm các mask nhị phân chỉ ra việc loại bỏ hoặc bảo tồn trọng số. Ví dụ, Guo et al. [37] đề xuất cập nhật lặp đi lặp lại các mask nhị phân và huấn luyện các tham số mạng để tránh tỉa không chính xác. Mask nhị phân như vậy cũng có thể được học trong quá trình huấn luyện với các biến cổng bổ sung [38] hoặc tham số phụ trợ [39]. Savarese et al. [40] tiếp tục đề xuất Continuous Sparsification để học các mask nhị phân sử dụng một xấp xỉ có thể vi phân đối với phạt ℓ0-regularization cho số lượng tham số của các mạng thưa thớt.

Cũng có nhiều nghiên cứu loại bỏ toàn bộ các nơ-ron hoặc bộ lọc tích chập để đạt được tăng tốc cấp cao thông qua song song hóa [41], [42], [43], [44], [45], [46], [47], [48]. Tuy nhiên, các mô hình được nén bởi các phương pháp này thường bị suy giảm hiệu suất nghiêm trọng, tức là độ chính xác phân loại giảm ở tỷ lệ tỉa cao, và do đó việc giảm độ phức tạp thường rất hạn chế. Ngoài ra, một số nghiên cứu gần đây [49], [50] đã trình bày các phương pháp tỉa trọng số thân thiện với phần cứng để cho phép giảm độ phức tạp thực tế trên các nền tảng có sẵn. Trong bài báo này, chúng tôi tập trung vào tỉa trọng số loại bỏ các trọng số riêng lẻ để đạt được mức độ thưa thớt cao.

Giả thuyết Vé số May mắn. Giả thuyết vé số may mắn ban đầu được đề xuất trong [26] tiết lộ sự tồn tại của các mạng con (a.k.a., "vé thắng") trong một mạng nơ-ron được khởi tạo ngẫu nhiên có thể khớp với độ chính xác kiểm tra của mạng gốc khi được huấn luyện riêng biệt. Zhou et al. [28] tiếp tục chứng minh sự tồn tại tiềm năng của các vé số may mắn có thể đạt được hiệu suất tốt mà không huấn luyện các trọng số. Được truyền cảm hứng bởi tiến bộ này, Ramanujan et al.[27] đã thiết kế một thuật toán edge-popup để tìm kiếm các mạng con này trong các trọng số được khởi tạo ngẫu nhiên. Họ tìm thấy một mạng con của Wide ResNet-50 [51] nhỏ hơn một chút, nhưng khớp với hiệu suất của ResNet-34 [21] trên ImageNet. Sau đó, các mạng con như vậy được xác nhận thêm bởi [29] và [52].

Thật không may, việc tìm các mạng con mà không huấn luyện trọng số phụ thuộc mạnh vào các mạng gốc được tham số hóa quá mức đủ. Để đạt được điều này, độ rộng của các mạng được khởi tạo ngẫu nhiên thường được mở rộng theo cấp số nhân, tuy nhiên điều này tăng độ phức tạp tìm kiếm [14], [29]. Kết quả là, chi phí tìm kiếm các mạng con này thậm chí còn cao hơn so với huấn luyện trọng số. Hơn nữa, các mạng con được tìm kiếm vẫn còn dư thừa với độ chính xác không thỏa đáng, xa rời mục đích của việc tỉa mạng [27]. Do đó, việc tìm ra các vé số may mắn hiệu suất cao với độ phức tạp tìm kiếm nhỏ là rất cấp thiết.

3 PHƯƠNG PHÁP LUẬN
3.1 Khái niệm cơ bản
Hãy để vector trọng số của một mạng đầy đủ là w∈R^k trong đó k là kích thước trọng số. Về mặt kỹ thuật, tỉa mạng có thể được xem như việc áp dụng một mask m∈{0,1}^k trên w để chỉ ra có nên bảo tồn hay loại bỏ một số trọng số. Với một độ thưa thớt toàn cục mong muốn p, mục tiêu thông thường của việc tỉa mạng có thể được công thức hóa như sau:

min_{w,m} L(m⊙w;D), s.t. 1-||m||_0/k ≥ p, (1)

trong đó D là tập dữ liệu quan sát, L đại diện cho hàm loss, ||·||_0 có nghĩa là chuẩn ℓ0 tiêu chuẩn, và ⊙ biểu thị phép nhân từng phần tử.

Như có thể thấy từ Eq. (1), hầu hết các phương pháp trước đó [18], [19], [24] theo đuổi các DNN thưa thớt bằng cách huấn luyện vector trọng số đã cho w trong khi học mask m. Mặc dù việc huấn luyện các trọng số tăng hiệu suất của các mạng thưa thớt được theo đuổi, việc tiêu thụ thời gian nặng nề của nó trở thành một nút thắt nghiêm trọng cho các triển khai thực tế như đã thảo luận trước đây.

Được truyền cảm hứng bởi giả thuyết vé số may mắn [26] chỉ ra rằng một mạng được khởi tạo ngẫu nhiên chứa các mạng con (vé số may mắn) có thể đạt được độ chính xác đáng kể, các nghiên cứu gần đây [27], [28], [29] đề xuất tìm kiếm các vé số may mắn này trên các trọng số được khởi tạo mà không cần thiết phải huấn luyện trọng số. Về cơ bản, mục tiêu học tập của họ có thể được đưa ra như sau:

min_m L(m⊙w;D), s.t. 1-||m||_0/k ≥ p. (2)

Sự khác biệt chính giữa Eq. (1) và Eq. (2) là w được coi như một vector hằng số để loại bỏ sự phụ thuộc vào việc huấn luyện trọng số. Tuy nhiên, Eq. (2) không thể phá vỡ giới hạn của việc tiêu thụ thời gian: Vé thắng hiệu suất cao mà không huấn luyện trọng số hiếm khi tồn tại trong một mạng được khởi tạo ngẫu nhiên, do đó độ rộng mạng thường được mở rộng theo cấp số nhân để đảm bảo sự tồn tại của các vé số may mắn [27], [28], [29]. Điều này tăng không gian tìm kiếm, và do đó cần nhiều chu kỳ tìm kiếm hơn. Kết quả là, chi phí tìm kiếm thậm chí còn cao hơn so với việc huấn luyện các trọng số trong Eq. (1). Bài báo này cố gắng giải quyết vấn đề tốn thời gian như vậy. Trong phần tiếp theo, chúng tôi đầu tiên tiết lộ rằng các vé số may mắn tồn tại trong các mô hình đã được huấn luyện trước mà không cần mở rộng độ rộng mạng, và sau đó cung cấp một cách thức nhanh chóng để tìm kiếm các vé số may mắn này.

3.2 Lottery Jackpots trong Các Mô hình Đã được Huấn luyện Trước
Chúng tôi nhằm mục đích xác minh sự tồn tại của các vé số may mắn mà không huấn luyện trọng số trong các mô hình đã được huấn luyện trước, được gọi là lottery jackpots. Chúng tôi tập trung vào các mô hình đã được huấn luyện trước vì các mô hình này có thể nhìn thấy rộng rãi trên Internet, hoặc có sẵn từ client, điều này nên được tận dụng đầy đủ và cũng loại bỏ sự cần thiết của việc huấn luyện trước một mô hình từ đầu.

Được xây dựng dựa trên một mô hình đã được huấn luyện trước, mục tiêu của chúng tôi để tìm lottery jackpots có thể được tái công thức hóa như sau:

min_m L(m⊙w̃;D), s.t. 1-||m||_0/k ≥ p, (3)

trong đó w̃ đại diện cho vector trọng số đã được huấn luyện trước, điều này khác biệt phương pháp của chúng tôi với công trình trước đó [27], [28] tìm kiếm các vé số may mắn trong một mạng được khởi tạo ngẫu nhiên với yêu cầu mở rộng độ rộng mạng.

Để chứng minh sự tồn tại của lottery jackpots trong một mô hình đã được huấn luyện trước không được mở rộng, chúng tôi đầu tiên áp dụng edge-popup [27] để tìm kiếm lottery jackpots. Cụ thể, mask m∈{0,1}^k đầu tiên được nới lỏng thành một

--- TRANG 4 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 4

m̄∈[0,1]^k được khởi tạo ngẫu nhiên. Ký hiệu đầu vào của mạng là X, và sau đó đầu ra Y được thu được như sau:

Y=F(h(m̄)⊙w̃,X), (4)

trong đó F() đại diện cho hàm mạng nơ-ron, và h(·) được định nghĩa như:

h(m̄_i) = {0, nếu m̄_i trong top-p% nhỏ nhất của m̄,
          {1, ngược lại, (5)

trong đó i∈{1,2,...,k}.

Trong quá trình lan truyền ngược của mạng, straight-through-estimator (STE) [53] được sử dụng để tính gradient của loss L w.r.t. mask nới lỏng m̄ như:

∂L/∂m̄ = ∂L/∂h(m̄) · ∂h(m̄)/∂m̄ ≈ ∂L/∂h(m̄) · 1. (6)

Theo cách này, chúng tôi tiến hành huấn luyện mask, tức là tìm kiếm trọng số trên các mô hình đã được huấn luyện trước khác nhau bằng cách tối ưu hóa m̄ với bộ tối ưu hóa Stochastic gradient descent (SGD) để xác minh sự tồn tại của lottery jackpots. Như có thể được quan sát một cách đáng ngạc nhiên trong Hình 2, với bất kỳ mức độ thưa thớt nào, lottery jackpots có thể được tìm thấy trong tất cả các mạng đã được huấn luyện trước. Ví dụ, một lottery jackpot có thể được tìm thấy trong ResNet-32 [21] được huấn luyện trước trên CIFAR-10 [31], chỉ có 10% tham số trong khi vẫn giữ lại hơn 94% độ chính xác top-1. Lottery jackpots cũng vượt trội hơn các mạng con được tìm kiếm từ các mạng được khởi tạo ngẫu nhiên với một khoảng cách lớn, điều này yêu cầu các mạng gốc được mở rộng theo cấp số nhân để có hiệu suất tốt hơn [27], [28], [29]. Hơn nữa, các lottery jackpots được tìm thấy có thể đạt được hiệu suất tương đương hoặc thậm chí tốt hơn so với các phương pháp tiên tiến trong việc tỉa mạng [19], [25], [32], điều này sẽ được thể hiện định lượng trong phần tiếp theo.

Thảo luận. Một số nghiên cứu gần đây có quan điểm khác rằng các mô hình đã được huấn luyện trước không cần thiết. Ví dụ, giả thuyết vé số may mắn nổi tiếng [26] tìm thấy rằng các mạng thưa thớt có thể được huấn luyện riêng biệt để đạt được độ chính xác đáng kể ngay cả khi không có mô hình đã được huấn luyện trước. Liu et al. [54] tuyên bố rằng việc kế thừa trọng số từ các mô hình đã được huấn luyện trước không nhất thiết là tối ưu. Tuy nhiên, các tuyên bố của họ được xây dựng trên tiền đề của một việc huấn luyện trọng số tốn thời gian giúp khôi phục hiệu suất độ chính xác. Ngược lại, lottery jackpots của chúng tôi trong bài báo này tái biện minh tầm quan trọng của các mô hình đã được huấn luyện trước nơi các mạng con hiệu suất cao đã tồn tại mà không cần thiết phải huấn luyện trọng số và mở rộng độ rộng mạng.

Khả năng Truy vết của Lottery Jackpots. Chúng tôi tiếp tục đi sâu vào các nguyên lý đằng sau lottery jackpots bằng cách khởi động một phân tích lỗi trong việc tỉa mạng. Để đơn giản, chúng tôi xem xét mạng kết nối đầy đủ một lớp, theo sau bởi một hàm kích hoạt ReLU. Phân tích có thể dễ dàng được mở rộng cho các mạng nhiều lớp và các loại mạng khác. Hãy để đầu vào là z^{l-1}∈R^{n_{l-1}} và các trọng số là w^l∈R^{n_l×n_{l-1}}, trong đó n_{l-1} và n_l là số lượng nơ-ron của lớp l-1 và lớp l. Sau đó, đầu ra z^l∈R^{n_l} là:

z^l=σ(w^l·z^{l-1}), (7)

trong đó σ là hàm ReLU. Đối với một lớp được nén với mask nhị phân m^l, đầu ra mới ẑ^l là:

ẑ^l=σ((m^l⊙w^l)·z^{l-1}). (8)

Mệnh đề 1. Ký hiệu ξ=||w^l_{i,:}-(m^l⊙w^l)_{i,:}||_2 và z^{Max}_{l-1}=max|z^{l-1}|. Đối với nơ-ron thứ i, chúng ta có:

0≤|z^l_i-ẑ^l_i|≤ξ√{n_{l-1}}z^{Max}_{l-1}. (9)

Chứng minh. |z^l_i-ẑ^l_i| có thể được suy ra như:

|σ(w^l_{i,:}·z^{l-1})-σ((m^l_{i,:}⊙w^l_{i,:})·z^{l-1})|
(a)
≤|w^l_{i,:}·z^{l-1}-(m^l⊙w^l)_{i,:}·z^{l-1}|
=|((1-m^l)⊙w^l)_{i,:}·z^{l-1}|
(b)
=|Cos(((1-m^l)⊙w^l)_{i,:},z^{l-1})|
||((1-m^l)⊙w^l)_{i,:}||_2||z^{l-1}||_2
(c)
≤ξ√{n_{l-1}}z^{Max}_{l-1}|Cos(((1-m^l)⊙w^l)_{i,:},z^{l-1})|, (10)

trong đó (a) theo |σ(x)-σ(y)|≤|x-y| cho hàm ReLU, (b) theo |a·b|=|Cos(a,b)||a||_2||b||_2 trong đó Cos(a,b) trả về độ tương tự cosine, (c) theo ||a||_2≤√ck cho bất kỳ a∈[-k,k]^c. Xem xét 0≤|Cos(a,b)|≤1, do đó chúng tôi hoàn thành chứng minh. ■

Chúng ta có thể quan sát từ Eq. (10) rằng ranh giới của sự khác biệt đầu ra phụ thuộc vào sự khác biệt độ lớn ξ và độ tương tự cosine |Cos(((1-m^l)⊙w^l)_{i,:},z^{l-1})|. Việc xem xét cả hai yếu tố sẽ làm giảm tốt sự khác biệt đầu ra. Như có thể được tham chiếu đến Hình 3, việc tỉa dựa trên độ lớn [1] không thể bảo tồn hiệu suất tốt hơn vì nó

--- TRANG 5 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 5

chỉ nhấn mạnh sự khác biệt độ lớn trong khi bỏ qua độ tương tự cosine. Tuy nhiên, chúng tôi nhấn mạnh rằng trong các tình huống mà độ tương tự cosine tiếp cận zero, thậm chí không có sự khác biệt đầu ra trong việc tỉa mạng, bất kể sự khác biệt về độ lớn giữa các trọng số dày đặc và thưa thớt.

Dưới đây chúng tôi đưa ra một ví dụ mẫu để hiểu rõ hơn. Để w^l_{i,:}=[1,-1,0.2,0.3], z^{l-1}=[1,1,1,1], rõ ràng rằng việc tỉa dựa trên độ lớn tạo ra một mask nhị phân [1,1,0,0] ở tỷ lệ tỉa 50% để loại bỏ các trọng số có độ lớn nhỏ nhất (0.2 và 0.3). Điều này tạo ra sự khác biệt độ lớn nhỏ nhất ξ=0.3606, độ tương tự cosine Cos([0,0,0.2,0.3],[1,1,1,1])=0.6918, và một sai lệch đầu ra |z^l_i-ẑ^l_i|=0.5. Tuy nhiên, cơ hội tồn tại trong một mask nhị phân [0,0,1,1] dẫn đến độ tương tự cosine Cos([1,-1,0,0],[1,1,1,1])=0. Điều này tạo ra một sai lệch đầu ra |z^l_i-ẑ^l_i|=0, ngay cả với sai lệch độ lớn lớn hơn ξ=1.4142.

Ví dụ trên giải thích rõ ràng lý do đằng sau sự tồn tại của lottery jackpots trong các mạng đã được huấn luyện trước. Mà không cần bất kỳ việc huấn luyện lại trọng số nào, một sự khác biệt đầu ra thấp hơn đáng kể có thể đạt được thông qua việc xem xét đồng thời cả độ tương tự cosine và sự khác biệt độ lớn. Mặc dù sự tồn tại của lottery jackpots làm nổi bật rất nhiều giá trị của các mô hình đã được huấn luyện trước trong việc tỉa mạng, việc tìm kiếm trọng số vẫn dẫn đến một tiêu thụ thời gian đáng kể ngay cả khi độ rộng mạng không cần thiết phải được mở rộng trong lottery jackpots của chúng tôi. Ví dụ, nó mất khoảng 100 epoch tìm kiếm trên ImageNet [22] để thành công tìm lottery jackpots. Do đó, rất cần thiết để rút ngắn quá trình tìm kiếm trọng số để tìm lottery jackpots một cách nhanh chóng. Cộng đồng có thể tập trung nhiều hơn vào cách định vị các lottery jackpots một cách hiệu quả trong các mô hình mạng đã được huấn luyện trước, điều này cũng là nghiên cứu quan trọng của chúng tôi trong Sec. 3.3, Sec. 3.4 và Sec. 3.5.

3.3 Sự Chồng lấp Mask
Han et al. [1] quan sát một hiện tượng thú vị của "bữa trưa miễn phí" rằng việc sử dụng độ lớn để tỉa AlexNet [55] đã được huấn luyện trước trên ImageNet mà không huấn luyện trọng số dẫn đến không có giảm hiệu suất khi độ thưa thớt ít hơn 50%. Mặc dù đơn giản, hiệu suất mô hình suy giảm mạnh ở độ thưa thớt cao hơn. Ngược lại, lottery jackpots của chúng tôi có thể giữ hiệu suất tốt ở hầu hết các mức độ thưa thớt như được hiển thị trong Hình 2. Điều này truyền cảm hứng cho chúng tôi khám phá mối liên kết tiềm năng để tăng tốc việc tìm kiếm lottery jackpots hiệu suất cao của chúng tôi. Để đạt được điều này, chúng tôi đầu tiên định nghĩa tỷ lệ chồng lấp mask thưa thớt sau để đo độ tương tự của hai mask m_1 và m_2 như:

Overlap(m_1,m_2) = 1-||h(m̄_1)-h(m̄_2)||_1/k. (11)

Dễ hiểu rằng tỷ lệ chồng lấp lớn hơn chỉ ra hai mask tương tự hơn. Sau đó, chúng tôi xem xét các tiêu chí tỉa khác nhau trong các công trình hiện tại dựa trên huấn luyện trọng số và so sánh mask của họ với mask được tìm kiếm của lottery jackpot của chúng tôi. Chúng tôi xem xét ngắn gọn các tiêu chí điển hình này như sau:

Magnitude [1]. Phương pháp này sử dụng độ lớn trọng số như điểm quan trọng s và suy ra một vector mask 0-1 m̂ bằng cách loại bỏ những trọng số có độ lớn nhỏ.

SNIP [25]. Độ nhạy kết nối đối với loss được xem xét như điểm quan trọng trọng số s và sau đó mask m̂ được suy ra bằng cách loại bỏ các kết nối không quan trọng. Cụ thể, điểm quan trọng được định nghĩa là ||w⊙g||_1, trong đó g biểu thị gradient của w.

GraSP [32]. Đầu tiên nó tính tích Hessian-gradient h của lớp thứ l sử dụng dữ liệu huấn luyện được lấy mẫu. Sau đó, điểm quan trọng s được định nghĩa là -w⊙h để bảo tồn dòng gradient, và vector mask m̂ được thu được bằng cách loại bỏ các trọng số có điểm thấp.

Random. Chúng tôi đầu tiên khởi tạo ngẫu nhiên m̄∈[0,1], và sau đó loại bỏ các trọng số tuân theo các giá trị mask nới lỏng có điểm thấp này.

Trong Hình 3, chúng tôi cho thấy hiệu suất của các mô hình đã được tỉa bởi các tiêu chí tỉa trên mà không huấn luyện trọng số. Như có thể quan sát, các vé số may mắn của chúng tôi vượt trội hơn các mô hình đã được tỉa với một khoảng cách lớn khi độ thưa thớt mạng tăng lên. Hơn nữa, Hình 4 chỉ ra một tỷ lệ chồng lấp lớn giữa mask của các phương pháp hiện tại m̂ và kết quả tìm kiếm của chúng tôi h(m̄). Trong số đó, việc tỉa dựa trên độ lớn đứng ở vị trí hàng đầu. Ví dụ, sự chồng lấp giữa mask dựa trên độ lớn và lottery jackpot của chúng tôi là hơn 95% khi loại bỏ khoảng 90% tham số của ResNet-50 [21]. Khám phá này chỉ ra rằng một phần rất nhỏ của mask đã được tỉa từ các tiêu chí tỉa hiện tại cần được sửa chữa để phù hợp với mask được tìm kiếm của chúng tôi mà không cần thiết một quá trình huấn luyện trọng số tốn thời gian trong công trình hiện tại.

Do đó, chúng tôi đề xuất tận dụng điểm quan trọng s từ các tiêu chí tỉa hiện tại như một khởi tạo khởi động của mask nới lỏng m̄ của chúng tôi để tìm kiếm lottery jackpots. Mask nới lỏng m̄ được khởi tạo như:

m̄_i = {η, nếu s_i trong top-p% nhỏ nhất của s,
       {1, ngược lại, (12)

trong đó η=0.99 trong bài báo này. Lý do đằng sau điều này là các giá trị mask ban đầu của các trọng số đã được tỉa gần hơn với các đối tác được bảo tồn chỉ ra khoảng cách nhỏ hơn để vượt qua các phần không chồng lấp còn lại giữa mask được khởi tạo và lottery jackpot, dẫn đến hội tụ nhanh như chúng tôi chứng minh định lượng trong Sec. 4.4.

--- TRANG 6 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 6

Do đó, mask nhị phân được khởi tạo h(m̄) được tạo ra bởi Eq. (5) giống với mask m̂ từ các tiêu chí tỉa hiện tại. Nhìn chung, mục tiêu của chúng tôi để tìm lottery jackpots có thể được tái công thức hóa như:

min_{m̄} L(h(m̄)⊙w̃;D), s.t. 1-||h(m̄)||_0/k ≥ p. (13)

Sau đó, chúng tôi tiến hành thí nghiệm sử dụng các mask được khởi tạo khác nhau từ các tiêu chí tỉa được đề cập ở trên. Như có thể thấy từ Hình 5, các khởi tạo khởi động khác nhau mang lại hội tụ nhanh hơn của việc tìm kiếm trọng số trong việc tìm lottery jackpots của chúng tôi. Đặc biệt, với mask dựa trên độ lớn như khởi tạo, một lottery jackpot hiệu suất cao loại bỏ khoảng 90% tham số của VGGNet-19, được định vị nhanh chóng chỉ sử dụng 30 epoch tìm kiếm. Ngược lại, thường là 300 epoch tìm kiếm được sử dụng để tìm ra một lottery jackpot tương đương với các trọng số được khởi tạo ngẫu nhiên. Do đó, một độ phức tạp tìm kiếm đáng kể có thể được giảm khi sử dụng mask dựa trên độ lớn như khởi tạo. Dễ hiểu hiện tượng này vì mask tỉa dựa trên độ lớn tạo ra tỷ lệ chồng lấp cao nhất với mask được tìm kiếm của chúng tôi như được hiển thị trong Hình 4.

Chúng tôi tiếp tục nghiên cứu cách cải thiện hiệu quả tìm kiếm của lottery jackpots sau khi khởi tạo mask của chúng tôi. Trước điều đó, chúng tôi tiến hành phân tích sâu về tính bất ổn tìm kiếm của thuật toán edge-popup [27], mà như chúng tôi chứng minh, chủ yếu bắt nguồn từ sự phụ thuộc lẫn nhau của trọng số trong các mạng nơ-ron hiện đại [27].

3.4 Sự Phụ thuộc Trọng số
Một cách trực quan, Eq. (13) tối thiểu hóa loss mạng L bằng cách tối ưu hóa mask nới lỏng m̄ tại mỗi lần lặp huấn luyện. Hãy để mask nới lỏng tại lần lặp huấn luyện thứ t là m̄^t, tạo ra các mask nhị phân để tỉa hoặc bảo tồn trọng số thông qua Eq. (5). Vì chúng tôi áp dụng mask dựa trên độ lớn, các giá trị mask cho các trọng số được bảo tồn lớn hơn những trọng số đã được tỉa. Ký hiệu các chỉ số mask tương ứng với các trọng số được bảo tồn và đã được tỉa là Φ^t và Ψ^t, chúng ta có thể có mối quan hệ sau:

min(m̄^t_{Φ^t}) > max(m̄^t_{Ψ^t}), (14)

trong đó min(·) và max(·) trả về giá trị thấp nhất và giá trị cao nhất, tương ứng.

Tuy nhiên, bất đẳng thức trên có thể bị phá vỡ sau khi cập nhật mask tại lần lặp t+1 vì một số phần tử trong m̄^{t+1}_{Ψ^t} trở nên lớn hơn một số phần tử trong m̄^{t+1}_{Φ^t}. Do đó, các phần tử này trong m̄^{t+1}_{Ψ^t} được hồi sinh trong khi những phần tử trong m̄^{t+1}_{Φ^t} được tỉa lại tại lần lặp t+1, điều mà chúng tôi gọi là hoán đổi trọng số trong bài báo này. Hãy để các chỉ số của những trọng số được tỉa lại và hồi sinh này là Φ^*_t⊆Φ^t và Ψ^*_t⊆Ψ^t, Mệnh đề 1 tiết lộ rằng hoán đổi trọng số giảm loss huấn luyện L dưới Giả định 1.

Giả định 1: Hãy để Δw̃_i biểu thị một nhiễu trọng số trên một trọng số cụ thể w̃_i và L(w̃_i+Δw̃_i) biểu thị loss mới sau khi thêm nhiễu. Các trọng số w̃_i và w̃_j độc lập đóng góp vào hàm loss L và ràng buộc sau thỏa mãn:

L(w̃_i,w̃_j) - L(w̃_i+Δw̃_i,w̃_j+Δw̃_j)
= L(w̃_i,w̃_j) - L(w̃_i+Δw̃_i,w̃_j)
+ L(w̃_i,w̃_j) - L(w̃_j+Δw̃_j,w̃_i). (15)

Mệnh đề 2: Với ∀i∈Φ^*_t, ∀j∈Ψ^*_t, bất đẳng thức sau luôn có thể được thỏa mãn:

L(h(m̄^{t+1}_i)w̃_i, h(m̄^{t+1}_j)w̃_j) < L(h(m̄^t_i)w̃_i, h(m̄^t_j)w̃_j). (16)

Chứng minh. L(h(m̄^{t+1}_i)w̃_i) chỉ ra loss sau khi loại bỏ w̃_i và nó có thể được xấp xỉ theo chuỗi Taylor bậc nhất như sau:

L(h(m̄^{t+1}_i)w̃_i) ≈ L(h(m̄^t_i)w̃_i) - ∂L/∂h(m̄^t_i)w̃_i(h(m̄^t_i)w̃_i - h(m̄^{t+1}_i)w̃_i)
= L(h(m̄^t_i)w̃_i) - ∂L/∂h(m̄^t_i)w̃_i · w̃_i. (17)

Và L(h(m̄^{t+1}_j)w̃_j) biểu thị loss sau khi hồi sinh w̃_j. Tương tự, nó có thể được xấp xỉ như:

L(h(m̄^{t+1}_j)w̃_j) ≈ L(h(m̄^t_j)w̃_j) - ∂L/∂h(m̄^t_j)w̃_j(h(m̄^t_j)w̃_j - h(m̄^{t+1}_j)w̃_j)
= L(h(m̄^t_j)w̃_j) + ∂L/∂h(m̄^t_j)w̃_j · w̃_j. (18)

Kết hợp hai đẳng thức trên tạo ra:

L(h(m̄^{t+1}_j)w̃_j) - L(h(m̄^t_j)w̃_j) + L(h(m̄^{t+1}_i)w̃_i) - L(h(m̄^t_i)w̃_i)
= ∂L/∂h(m̄^t_j)w̃_j · w̃_j - ∂L/∂h(m̄^t_i)w̃_i · w̃_i. (19)

--- TRANG 7 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 7

Xem xét Giả định 3 chúng ta có:

L(h(m̄^{t+1}_i)w̃_i, h(m̄^{t+1}_j)w̃_j) - L(h(m̄^t_i)w̃_i, h(m̄^t_j)w̃_j)
= ∂L/∂h(m̄^t_j)w̃_j · w̃_j - ∂L/∂h(m̄^t_i)w̃_i · w̃_i. (20)

Bởi Eq. (14) chúng ta có m̄^t_i > m̄^t_j, và cho i∈Φ^*_t, j∈Ψ^*_t chúng ta thu được m̄^{t+1}_j > m̄^{t+1}_i. Kết hợp hai bất đẳng thức này do đó dẫn đến:

m̄^t_i - m̄^{t+1}_i > m̄^t_j - m̄^{t+1}_j. (21)

Nhìn lại Eq. (6), trạng thái cập nhật của m̄ từ lần lặp thứ t đến lần lặp (t+1) có thể được suy ra như:

m̄^{t+1} = m̄^t - ∂L/∂m̄^t = m̄^t - ∂L/∂h(m̄^t)⊙w̃ · ∂h(m̄^t)⊙w̃/∂m̄^t
= m̄^t - ∂L/∂h(m̄^t)⊙w̃ · ∂h(m̄^t)/∂m̄^t · w̃ ≈ m̄^t - ∂L/∂h(m̄^t)⊙w̃ · w̃, (22)

trong đó tỷ lệ học và các mục tối ưu hóa như weight decay được bỏ qua để dễ dàng biểu diễn. Bằng cách đưa suy dẫn này vào Eq. (21), chúng ta thu được:

∂L/∂h(m̄^t_i)w̃_i · w̃_i > ∂L/∂h(m̄^t_j)w̃_j · w̃_j. (23)

Bất đẳng thức này và Eq. (20) dẫn đến:

L(h(m̄^{t+1}_i)w̃_i, h(m̄^{t+1}_j)w̃_j) - L(h(m̄^t_i)w̃_i, h(m̄^t_j)w̃_j)
= ∂L/∂h(m̄^t_j)w̃_j · w̃_j - ∂L/∂h(m̄^t_i)w̃_i · w̃_i < 0. (24)

Do đó, chúng ta có:

L(h(m̄^{t+1}_i)w̃_i, h(m̄^{t+1}_j)w̃_j) < L(h(m̄^t_i)w̃_i, h(m̄^t_j)w̃_j), (25)

điều này cuối cùng hoàn thành chứng minh Mệnh đề 1 của chúng tôi. ■

Cho đến nay, chúng tôi đã chứng minh rằng Mệnh đề 1 đảm bảo việc giảm loss cho hoán đổi trọng số giữa w̃_i và w̃_j dưới Giả định 1, và việc giảm loss dự kiến được ký hiệu bởi ΔL(w̃_i,w̃_j) là:

ΔL(w̃_i,w̃_j) = ∂L/∂h(m̄^t_j)w̃_j · w̃_j - ∂L/∂h(m̄^t_i)w̃_i · w̃_i. (26)

Thật không may, Giả định 1 không luôn đúng trong các mạng nơ-ron hiện đại do sự tồn tại của các lớp Batch normalization, hàm Softmax, v.v. Ngược lại, sự phụ thuộc lẫn nhau thường phát sinh giữa các trọng số khác nhau như được thảo luận trong [32], [56], [57]. Trong trường hợp này, Eq. (15) thực sự được trình bày như:

L(w̃_i,w̃_j) - L(w̃_i+Δw̃_i,w̃_j+Δw̃_j)
= L(w̃_i,w̃_j) - L(w̃_i+Δw̃_i,w̃_j)
+ L(w̃_i,w̃_j) - L(w̃_j+Δw̃_j,w̃_i) + ϵ, (27)

trong đó ϵ∈R là lỗi biến dạng gây ra bởi sự phụ thuộc của các trọng số. Lặp lại các thủ tục suy dẫn cho Mệnh đề 1, chúng ta thu được:

L(h(m̄^{t+1}_i)w̃_i, h(m̄^{t+1}_j)w̃_j) - L(h(m̄^t_i)w̃_i, h(m̄^t_j)w̃_j)
= ∂L/∂h(m̄^t_j)w̃_j · w̃_j - ∂L/∂h(m̄^t_i)w̃_i · w̃_i + ϵ
|{z}
ΔL(w̃_i,w̃_j) trong Eq. (26). (28)

--- TRANG 8 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 8

Do đó, việc giảm loss thực tế cho hoán đổi trọng số rơi vào ΔL(w̃_i,w̃_j) + ϵ. Dễ hiểu rằng Eq. (24) luôn đúng và loss vẫn giảm từ hoán đổi trọng số nếu ràng buộc sau thỏa mãn:

ϵ < -ΔL(w̃_i,w̃_j). (29)

Ngược lại, Eq. (24) bị phá vỡ và loss thậm chí trở nên lớn hơn sau khi cập nhật, đặc biệt đối với hoán đổi trọng số thường xuyên. Hình 6 hiển thị các phân phối định lượng của ϵ bằng cách thực hiện ngẫu nhiên hoán đổi trọng số trong các số lượng khác nhau của trọng số được hoán đổi cho ResNet-32 [21]. Chúng ta có thể thấy rằng phương sai của lỗi biến dạng ϵ tăng tỷ lệ thuận với số lượng trọng số được hoán đổi, phá hủy ràng buộc trên. Kết quả là, tính ổn định tìm kiếm của thuật toán edge-popup bị giới hạn bởi lỗi biến dạng như vậy bắt nguồn từ sự phụ thuộc lẫn nhau của trọng số.

3.5 Short Restriction Popup
Để giải quyết vấn đề trên, trên tiền đề của khởi tạo mask được giới thiệu trong Sec. 3.3, chúng tôi tiếp tục đề xuất một short restriction popup để cải thiện hiệu quả tìm kiếm cho lottery jackpots. Động lực chính của chúng tôi là khuyến khích hoán đổi trọng số nếu chúng tôi quan sát một giá trị nhỏ của việc giảm loss dự kiến ΔL(w̃_i,w̃_j) mạnh mẽ đối với lỗi biến dạng theo Eq. (29); ngược lại, hoán đổi trọng số sẽ được ngăn chặn. Do đó, loss giảm liên tục dẫn đến hội tụ huấn luyện ổn định.

Cụ thể, chúng tôi đầu tiên thu được các chỉ số của các trọng số được bảo tồn và đã được tỉa từ mask được khởi tạo m̄^0 trong Eq. (12):

Φ^0 = {i|m̄^0_i = 1, i = 1,2,...,k},
Ψ^0 = {i|m̄^0_i = η, i = 1,2,...,k}, (30)

trong đó k là kích thước trọng số như được nhắc lại trong Sec. 3.1. Trong lan truyền thuận thứ t, chúng tôi thu được mask nhị phân bởi:

h^*(m̄_i) = {1, nếu i∈Φ^t,
            {0, ngược lại, (31)

Lan truyền ngược giống như Eq. (6). Sau khi cập nhật thứ t, các chỉ số của các trọng số được tỉa lại và hồi sinh lại Φ^*_t và Ψ^*_t được thu được bởi:

Φ^*_t = {i|m̄^{t+1}_i ≤ TopK(m̄^{t+1}, ||Ψ^t||_0), i∈Φ^t},
Ψ^*_t = {i|m̄^{t+1}_i > TopK(m̄^{t+1}, ||Ψ^t||_0), i∈Ψ^t}, (32)

trong đó TopK(v,K) trả về giá trị lớn thứ K trong vector v. Sau đó, chúng tôi dần dần giảm số lượng hoán đổi trọng số q^t giữa m̄^t_{Ψ^*_t} và m̄^t_{Φ^*_t} như:

q^t = ⌈||Ψ^*_t||_0(1-t/t_f)^4⌉, (33)

trong đó ⌈·⌉ là phép toán ceiling và t_f là tổng số lần lặp huấn luyện. Để giải thích, chúng tôi khuyến khích nhiều hoán đổi trọng số hơn để đạt được đủ khám phá ở giai đoạn đầu của huấn luyện, và sau đó dần dần hạn chế số lượng hoán đổi trọng số để đạt được hội tụ hiệu quả bằng nhiều giảm loss hơn. Chúng tôi chọn hoán đổi các trọng số của top-q^t nhỏ nhất m̄^t_{Φ^*_t} và trọng số của top-q^t lớn nhất m̄^t_{Ψ^*_t}. Do đó, các chỉ số mask tương ứng với các trọng số được bảo tồn và các trọng số đã được tỉa tại lần lặp t+1 được thu được như:

Φ^{t+1} = Φ^t\ArgBotK(m̄^{t+1}_{Φ^*_t}, q^t) ∪ ArgTopK(m̄^{t+1}_{Ψ^*_t}, q^t),
Ψ^{t+1} = Ψ^t\ArgTopK(m̄^{t+1}_{Ψ^*_t}, q^t) ∪ ArgBotK(m̄^{t+1}_{Φ^*_t}, q^t), (34)

trong đó ArgTopK(v,K), ArgBotK(v,K) trả về top-K giá trị lớn nhất và nhỏ nhất trong vector v.

Short restriction pop-up được đề xuất của chúng tôi, được gọi là SR-popup để tìm kiếm lottery jackpots, được liệt kê trong Alg. 1. Hình 7 cho thấy các đường cong loss của SR-popup và Edge-popup khi tìm kiếm lottery jackpots trên các tập dữ liệu khác nhau. Bằng cách thoát khỏi hoán đổi trọng số không mạnh mẽ đối với lỗi biến dạng, SR-popup hiệu quả giảm dao động loss và dẫn đến một quá trình tìm kiếm ổn định hơn.

--- TRANG 9 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 9

Thuật toán 1: Short Restriction Popup để Định vị Lottery Jackpots
Đầu vào: Trọng số đã được huấn luyện trước w̃, tỷ lệ tỉa p, lần lặp tìm kiếm t_f.
1 Khởi tạo mask nới lỏng m̄^0 qua Eq. (12).
2 Thu được các chỉ số trọng số được bảo tồn và đã được tỉa Φ^0 và Ψ^0 qua Eq. (30).
3 for t = 1 → t_f do
4   Lan truyền thuận qua Eq. (31).
5   Lan truyền ngược qua Eq. (6).
6   Cập nhật m̄ sử dụng bộ tối ưu hóa SGD.
7   Tiến hành hoán đổi trọng số qua Eq. (34).
8 end
9 Trả về mô hình nén h^*(m̄)⊙w̃.

4 THÍ NGHIỆM
4.1 Cài đặt
Tập dữ liệu. Chúng tôi xem xét các benchmark đại diện với các quy mô khác nhau cho phân loại hình ảnh. Đối với tập dữ liệu quy mô nhỏ, chúng tôi chọn các tập dữ liệu CIFAR-10 và CIFAR-100 [31]. CIFAR-10 chứa 60,000 hình ảnh màu 32×32 từ 10 lớp khác nhau, với 6,000 hình ảnh trong mỗi lớp. Tập dữ liệu CIFAR-100 tương tự như CIFAR-10, ngoại trừ việc nó có 100 lớp, mỗi lớp chứa 600 hình ảnh. Đối với tập dữ liệu quy mô lớn, chúng tôi chọn ImageNet-1K [22] đầy thử thách có hơn 1.2 triệu hình ảnh để huấn luyện và 50,000 hình ảnh xác thực với 1,000 danh mục.

Mạng. Đối với CIFAR-10 và CIFAR-100, chúng tôi tìm lottery jackpots trong VGGNet-19 [30] và ResNet-32 [21] cổ điển. Theo các nghiên cứu trước [32], [58], chúng tôi nhân đôi số lượng bộ lọc của mỗi lớp tích chập của ResNet-32 để làm cho nó phù hợp để fit tập dữ liệu. Đối với tập dữ liệu ImageNet, chúng tôi chứng minh hiệu quả của lottery jackpots để tỉa ResNet-50 [21], phục vụ như một backbone điển hình cho cộng đồng tỉa mạng. Bên cạnh đó, chúng tôi tiến hành thí nghiệm để tìm lottery jackpots để tỉa các mạng rất nhẹ bao gồm MobileNet-V1 [33], EfficientNet-B4 [59].

Chi tiết Triển khai. Chúng tôi áp dụng SR-popup được đề xuất để tìm kiếm lottery jackpots sử dụng bộ tối ưu hóa SGD với tỷ lệ học ban đầu là 0.1 cho tất cả các thí nghiệm. Với các epoch tổng khác nhau để tìm kiếm, tức là 10 và 30, chúng tôi điều chỉnh tỷ lệ học với bộ lập lịch cosine [61]. Momentum được đặt thành 0.9 và kích thước batch được đặt thành 256. Weight decay được đặt thành 5×10^{-4} trên CIFAR-10 và 1×10^{-4} trên ImageNet. Để so sánh công bằng, chúng tôi tiến hành tăng cường dữ liệu cho tiền xử lý hình ảnh bao gồm cắt và lật theo chiều ngang, giống như triển khai chính thức trong Pytorch [62]. Chúng tôi lặp lại tất cả các thí nghiệm của mình ba lần trên CIFAR-10 với các seed khác nhau,

BẢNG 1: So sánh với các phương pháp huấn luyện trọng số hiện tại để tỉa VGGNet-19 [30] và ResNet-32 [21] trên CIFAR-10 [31]. Chúng tôi báo cáo độ chính xác top-1 (%), chi phí huấn luyện (T) và chi phí tìm kiếm (S).

và báo cáo trung bình và độ lệch chuẩn của độ chính xác phân loại top-1. Trên ImageNet, chúng tôi chạy tất cả các thí nghiệm một lần xem xét việc tiêu thụ tài nguyên nặng nề và hiệu suất ổn định trên tập dữ liệu lớn, và báo cáo cả độ chính xác phân loại top-1 và top-5. Tất cả các thí nghiệm được chạy trên GPU NVIDIA Tesla V100.

4.2 So sánh với Các Phương pháp Huấn luyện Trọng số
CIFAR-10/100. Trên CIFAR-10/100, chúng tôi so sánh lottery jackpots với nhiều đối thủ huấn luyện trọng số bao gồm OBD [16], SET [24], Deep-R [60], Lottery Tickets (LT) [26], SNIP [25], và GraSP [32]. Chúng tôi báo cáo định lượng độ chính xác phân loại top-1 để tỉa VGGNet-19 và ResNet-32 dưới hai mức độ thưa thớt {90%,95%} trong Bảng 1 và Bảng 2. Chi phí huấn luyện hoặc tìm kiếm trọng số, đề cập đến chi phí epoch cho việc thu được mô hình đã được tỉa cuối cùng, cũng được liệt kê để so sánh hiệu quả tỉa. Bên cạnh đó, chúng tôi vẽ hiệu suất của các phương pháp khác nhau sử dụng cùng epoch huấn luyện/tìm kiếm ở độ thưa thớt 95% trong Hình 8.

Như có thể quan sát trong Bảng 1, lottery jackpots cung cấp hiệu suất tiên tiến trong khi tận dụng chi phí tính toán tỉa hiếm trên CIFAR-10. Ví dụ, chỉ 10 epoch được yêu cầu để tìm một lottery jackpot chứa chỉ 10% tham số của VGGNet-19, trong khi đạt được độ chính xác top-1 vượt trội là 93.70%. Ngược lại, SNIP mất 160 epoch tổng cộng để đạt độ chính xác thấp hơn là 93.65%. Lottery jackpot cũng phục vụ như một người dẫn đầu để tỉa ResNet-32. Khác với các vé số may mắn [26] cần một quá trình huấn luyện trọng số hoàn chỉnh để khôi phục hiệu suất, chúng tôi trực tiếp tìm kiếm lottery jackpots mà không sửa đổi các trọng số đã được huấn luyện trước, dẫn đến các mô hình đã được tỉa thậm chí tốt hơn (94.30% so với 92.61% cho độ chính xác top-1) với hơn 5× giảm chi phí tỉa (30 epoch so với 160 epoch).

Kết quả của CIFAR-100 trong Bảng 2 cũng cho thấy rằng lottery jackpots của chúng tôi giữ lại độ chính xác top-1 tốt hơn so với các đối thủ

--- TRANG 10 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 10

dưới các tỷ lệ tỉa khác nhau với chi phí ít hơn. Ví dụ, phương pháp của chúng tôi có thể đạt được độ chính xác top-1 là 74.63% khi tỉa VGGNet-19 ở độ thưa thớt 90%, vượt trội hơn các phương pháp tiên tiến khác với khoảng cách lớn là 2.27%, 2.05%, và 1.80% cao hơn SET [24], LT [26], và SNIP [25], tương ứng. Lưu ý rằng nó thậm chí vượt quá mạng dày đặc gốc 0.47%. So với OBD [16] tiến hành fine-training sau khi tỉa ResNet-32 đã được huấn luyện trước, lottery jackpots có thể đạt được độ chính xác tốt hơn dưới tất cả các mức độ thưa thớt.

Từ Hình 8, chúng tôi tiếp tục quan sát hiệu suất tối cao của lottery jackpots khi duy trì chi phí tỉa tương tự. Chi tiết, các phương pháp trước [19], [25], [32] bị suy giảm hiệu suất đáng kể với epoch hạn chế, chủ yếu do việc huấn luyện trọng số không đủ. Ngược lại, chúng tôi trực tiếp tìm kiếm lottery jackpots trên các trọng số đã được huấn luyện trước, dẫn đến hiệu suất thỏa đáng với chi phí ít hơn nhiều.

ImageNet. Chúng tôi tiếp tục chứng minh hiệu quả của lottery jackpots của chúng tôi để tỉa ResNet-50 trên tập dữ liệu ImageNet đầy thử thách. Hai tỷ lệ tỉa {80%,90%} được xem xét để so sánh công bằng với các phương pháp tiên tiến khác bao gồm SET [24], Deep-R [60], Dynamic Sparse [18], SNIP [25], GraSP [32], OBD [16] và RigL [19]. Trong Bảng 3, lottery jackpots đánh bại tất cả các đối thủ trong cả độ chính xác top-1 và top-5 dưới cùng tỷ lệ thưa thớt. Chúng tôi có thể tìm kiếm một lottery jackpot chỉ với 1/5 chi phí tính toán so với GraSP [32] (30 epoch cho chúng tôi và 150 epoch cho GraSP), trong khi thu được mô hình đã được tỉa với tỷ lệ tỉa 80% với độ chính xác top-1 và top-5 cao hơn (75.19% và 92.62% cho lottery jackpot so với 72.06% và 90.82% cho GraSP). Khi tăng mức độ thưa thớt lên 90%, một lottery jackpot đạt được 73.04% độ chính xác top-1 với cải thiện 5.80% và 1.04% so với GraSP [32] và RigL [19], có thể được tìm kiếm thành công với ít epoch hơn (60, 100, và 150

BẢNG 3: So sánh với các phương pháp huấn luyện trọng số có sẵn để tỉa ResNet-50 [21] trên ImageNet [22]. Chúng tôi báo cáo độ chính xác top-1 và top-5 (%), chi phí huấn luyện (T) và chi phí tìm kiếm (S).

epoch cho lottery jackpots, RigL và GraSP, tương ứng). Những kết quả này chứng minh tính khả thi của việc trực tiếp tìm mạng con hiệu suất tốt mà không yêu cầu quá trình huấn luyện trọng số tốn thời gian.

Chúng tôi cũng tiến hành thí nghiệm để tỉa các mạng được thiết kế nhẹ

--- TRANG 11 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 11

BẢNG 4: So sánh với các phương pháp huấn luyện trọng số hiện tại để tỉa MobileNet-V1 [33] và EfficientNet-B4 [59] trên ImageNet [22]. Chúng tôi báo cáo độ chính xác top-1 (%), chi phí huấn luyện (T) và chi phí tìm kiếm (S).

bao gồm MobileNet-V1 [33] và EfficientNet-B4 [59] ở tỷ lệ tỉa 80%. Bảng 4 liệt kê hiệu suất của lottery jackpots so với SNIP [25], GraSP [32], và RigL [19] dựa trên việc tái triển khai của chúng tôi. Kết quả một lần nữa gợi ý sự vượt trội của lottery jackpots cho độ chính xác top-1 cao hơn ngay cả khi không sửa đổi bất kỳ trọng số đã được huấn luyện trước nào. Trên MobileNet-V1, một lottery jackpot vượt trội đáng kể hơn RigL 2.21% có thể được tìm thấy với 6× giảm chi phí tỉa. Đối với EfficientNet-B4 nhỏ gọn hơn, chúng tôi quan sát sự giảm hiệu suất đáng kể cho tất cả các phương pháp. Tuy nhiên, lottery jackpots vẫn hoạt động tốt nhất. Do đó, nó chứng minh rõ ràng lợi thế của việc tìm lottery jackpots trong các mạng nhẹ đã được huấn luyện trước được trao quyền với khả năng giảm hiệu quả độ phức tạp mạng.

4.3 So sánh với Các Phương pháp Tìm kiếm Trọng số
Trong phần này, chúng tôi so sánh hiệu suất của lottery jackpots được tìm thấy bởi SR-popup được đề xuất của chúng tôi và các phương pháp tìm kiếm trọng số của Zhou et al. [28] và Edge-popup [27] ban đầu được thiết kế để tìm kiếm các mạng con hiệu suất tốt trong các mạng được khởi tạo ngẫu nhiên. Kết quả để tỉa ResNet-50 đã được huấn luyện trước trên ImageNet được liệt kê trong Bảng 5. SR-popup được đề xuất của chúng tôi vượt trội đáng kể hơn các phương pháp khác khi tiêu thụ cùng epoch tìm kiếm. Đặc biệt, sự giảm hiệu suất nghiêm trọng được quan sát cho tất cả các phương pháp ngoại trừ SR-popup của chúng tôi với epoch hạn chế là 30, điều này chứng minh hiệu quả của việc tận dụng tỉa dựa trên độ lớn như một khởi động cho các mask nới lỏng. Mặc dù thêm epoch tìm kiếm mang lại cải thiện rõ ràng cho Edge-popup, hiệu suất của nó vẫn tụt hậu so với SR-popup với một khoảng cách đáng chú ý. Ví dụ, khi tiêu thụ 60 epoch tìm kiếm, SR-popup thành công định vị một lottery jackpot với độ chính xác top-1 73.04%, vượt trội hơn edge-popup 2.15% mà quá trình tìm kiếm của nó bị cản trở nặng nề bởi lỗi biến dạng của sự phụ thuộc lẫn nhau trọng số như được thảo luận trong Sec. 3.4.

BẢNG 5: So sánh với các phương pháp tìm kiếm trọng số hiện tại để tỉa ResNet-50 [21] trên ImageNet [22]. Tỷ lệ tỉa được đặt thành 90%. Chúng tôi báo cáo độ chính xác top-1 và top-5 (%), và chi phí tìm kiếm (S).

4.4 Phân tích Hiệu suất
Trong phần này, chúng tôi tiến hành nhiều thí nghiệm để điều tra hiệu suất của lottery jackpots. Chúng tôi đầu tiên nghiên cứu hiệu ứng của quá trình huấn luyện trọng số cho lottery jackpots. Sau khi tìm kiếm lottery jackpots, chúng tôi đóng băng mask được tìm thấy và fine-tune các trọng số để điều tra liệu quá trình huấn luyện trọng số như vậy có thể mang lại lợi ích thêm cho hiệu suất của mô hình đã được tỉa hay không. Mục tiêu huấn luyện trọng số được công thức hóa như:

min_{w̃} L(h(m̂)⊙w̃;D). (35)

Chúng tôi tiến hành thí nghiệm cho ResNet-32 trên CIFAR-10 với các tỷ lệ tỉa khác nhau. Các trọng số được fine-tune với 150 epoch và các cài đặt khác được mô tả trong Sec. 4.1. Hình 9 cho thấy rằng cải thiện của quá trình huấn luyện trọng số là không đáng kể. Nó không thể cải thiện thêm hiệu suất của lottery jackpots, và thậm chí dẫn đến suy giảm độ chính xác nhỏ dưới độ thưa thớt cao. Do đó, chúng tôi có thể kết luận rằng quá trình huấn luyện trọng số tốn thời gian không cần thiết để có được các mô hình thưa thớt hiệu suất tốt ở một mức độ nào đó. Ngược lại, lottery jackpots đã tồn tại trong các mô hình đã được huấn luyện trước.

Sau đó, chúng tôi điều tra ảnh hưởng của epoch tìm kiếm cho lottery jackpots. Chi tiết, trong khi so sánh hiệu suất, chúng tôi cũng quan sát sự chồng lấp kiến trúc thưa thớt

--- TRANG 12 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 12

giữa lottery jackpots và mask ban đầu m̂ thu được bằng tỉa độ lớn. Chúng tôi hiển thị kết quả định lượng trong Bảng 6. Khi epoch tìm kiếm bắt đầu tăng, hiệu suất của lottery jackpots được tìm thấy có thể được tăng cường và kiến trúc thưa thớt cũng thay đổi ở một mức độ nào đó. Tuy nhiên, hiện tượng như vậy không nhất quán với nhiều epoch tìm kiếm hơn. Hiệu suất của lottery jackpots được tìm kiếm với 100 và 1000 epoch ở cùng mức. Hơn nữa, cấu trúc thưa thớt của lottery jackpots dần dần có xu hướng ổn định, và sẽ không thay đổi đáng kể với việc tăng chi phí tìm kiếm. Do đó, chỉ cần một lượng chi phí tính toán tương đối nhỏ để tìm lottery jackpots, điều này chứng minh rõ hiệu quả của phương pháp của chúng tôi.

Cuối cùng, chúng tôi thực hiện bốn nghiên cứu loại bỏ cho phương pháp SR-popup được đề xuất. Các thí nghiệm được tiến hành để tỉa ResNet-32 và VGGNet-19 trên CIFAR-10 ở mức thưa thớt 90% sử dụng 30 epoch tìm kiếm. 1) Bảng 7 cho thấy so sánh hiệu suất giữa không sử dụng khởi tạo dựa trên độ lớn (w/o W) hoặc không sử dụng short-restriction (w/o S), và sử dụng cả hai (SR) để tìm kiếm lottery jackpots. Kết quả gợi ý rằng cả hai thành phần trong SR-popup được đề xuất đều cần thiết cho hiệu suất của lottery jackpots. 2) Đối với khởi tạo dựa trên độ lớn trong Eq. (12), chúng tôi điều tra cách giá trị ban đầu η của các mask nới lỏng ảnh hưởng đến các trọng số đã được tỉa.

BẢNG 6: Độ chính xác top-1 (%) của lottery jackpots với các epoch khác nhau và sự chồng lấp kiến trúc thưa thớt với tỉa dựa trên độ lớn để tỉa ResNet-32 [21] trên CIFAR-10 [31] ở mức thưa thớt 90%.

Chi tiết, chúng tôi vẽ độ chính xác top-1 so với epoch tìm kiếm sử dụng η khác nhau để tìm kiếm lottery jackpots. Hình 10 cho thấy rằng các giá trị ban đầu nhỏ hơn dẫn đến giảm chậm hơn trong loss huấn luyện. Để giải thích, khoảng cách giữa các mask tương ứng với các trọng số đã được tỉa và được bảo tồn quá lớn, dẫn đến hội tụ chậm. 3) Chúng tôi khai thác sự lựa chọn cho các cặp hoán đổi trọng số được bầu chọn. Hai biến thể bao gồm việc chọn những cái có việc giảm loss nhỏ nhất và lựa chọn ngẫu nhiên được xem xét để so sánh với phương pháp được đề xuất của chúng tôi. Kết quả trong Bảng 8 gợi ý rằng phương pháp hạn chế được đề xuất của chúng tôi vượt trội hơn các biến thể khác với một khoảng cách lớn, điều này chứng minh quan điểm của chúng tôi về việc chỉ bảo tồn hoán đổi trọng số đóng góp những giảm loss dự kiến đáng kể nhất để tìm kiếm lottery jackpots. 4) Lịch trình giảm cho số lượng hạn chế của hoán đổi trọng số, tức là Eq. (33), được so sánh với phiên bản nghịch đảo của nó:

q^t = ⌈||Ψ^*_t||_0(t/t_f)^4⌉, (36)

và một lịch trình hằng số cũng được xem xét, w.r.t. q^t = 1. Tình huống này tương đương với việc áp dụng thuật toán edge-popup trên nền tảng khởi tạo dựa trên độ lớn được đề xuất của chúng tôi. Kết quả được liệt kê trong Bảng 9 gợi ý rằng lịch trình được đề xuất của chúng tôi dẫn đến hiệu suất tốt nhất nhờ vào việc khám phá đầy đủ cho hoán đổi trọng số trong các lần lặp tìm kiếm sớm và hạn chế tăng dần của hoán đổi trọng số không ổn định để hội tụ hiệu quả.

BẢNG 7: Nghiên cứu loại bỏ cho các thành phần trong SR-popup được đề xuất của chúng tôi.

BẢNG 8: Nghiên cứu loại bỏ cho việc lựa chọn các cặp hoán đổi trọng số.

BẢNG 9: Nghiên cứu loại bỏ cho lịch trình của số lượng hạn chế cho hoán đổi trọng số.

5 HẠN CHẾ
Như được nhấn mạnh xuyên suốt bài báo, lottery jackpots của chúng tôi được đề xuất trên tiền đề của các mô hình đã được huấn luyện trước, mà chúng tôi tin rằng hầu hết có sẵn từ Internet hoặc client. Tuy nhiên, cơ hội tồn tại để không thể truy cập các mô hình đã được huấn luyện trước trong một số tình huống, chỉ ra tính không áp dụng được của lottery jackpots của chúng tôi. Bên cạnh đó, tài nguyên phần cứng hạn chế của chúng tôi không cho phép chúng tôi khám phá sự tồn tại của lottery jackpots ngoài các mạng nơ-ron tích chập. Chúng tôi hy vọng sẽ hiển thị nhiều kết quả hơn về các nhiệm vụ khác như xử lý ngôn ngữ tự nhiên trong công trình tương lai của chúng tôi.

--- TRANG 13 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 13

6 THẢO LUẬN VÀ KẾT LUẬN
Trong lĩnh vực tỉa mạng, trạng thái của các mô hình đã được huấn luyện trước ngày càng bị bỏ qua. Thay vào đó, nhiều nhà nghiên cứu đã thiết kế các cơ chế phức tạp và tốn thời gian để huấn luyện các mạng thưa thớt từ đầu. Ngược lại, trong bài báo này, chúng tôi tái biện minh tầm quan trọng của các mô hình đã được huấn luyện trước bằng cách tiết lộ sự tồn tại của lottery jackpots mà các mạng con hiệu suất cao xuất hiện trong các mô hình đã được huấn luyện trước mà không cần thiết phải huấn luyện trọng số.

Để giảm thiểu vấn đề kém hiệu quả của các phương pháp tìm kiếm hiện tại, chúng tôi tiếp tục trình bày một phương pháp SR-popup mới tăng cường cả quá trình khởi tạo và tìm kiếm của lottery jackpots. Đối với khởi tạo mask, chúng tôi quan sát thực nghiệm rằng việc tận dụng trực tiếp các tiêu chí tỉa hiện tại dẫn đến các mask thưa thớt chồng lấp với lottery jackpot của chúng tôi ở mức độ đáng chú ý. Trong số các tiêu chí tỉa đó, tỉa dựa trên độ lớn tạo ra các mask tương tự nhất với lottery jackpots. Dựa trên thông tin này, chúng tôi khởi tạo mask thưa thớt sử dụng tỉa độ lớn. Mặt khác, một cơ chế hạn chế ngắn được đề xuất để hạn chế thay đổi của các mask có thể có tác động tiêu cực tiềm ẩn đến loss huấn luyện trong quá trình tìm kiếm dưới đảm bảo lý thuyết. Các thí nghiệm mở rộng chứng minh rằng lottery jackpots của chúng tôi có thể đạt được hiệu suất tương đương hoặc thậm chí tốt hơn với nhiều phương pháp tiên tiến mà không cần kiến thức chuyên gia phức tạp để huấn luyện các mạng thưa thớt, trong khi giảm đáng kể chi phí tỉa.

LỜI CẢM ƠN
Công trình này được hỗ trợ bởi Chương trình R&D Chính quốc gia của Trung Quốc (Số 2022ZD0118202), Quỹ Khoa học Quốc gia dành cho Các Học giả Trẻ Xuất sắc (Số 62025603), Quỹ Khoa học Tự nhiên Quốc gia của Trung Quốc (Số U21B2037, Số U22B2051, Số 62176222, Số 62176223, Số 62176226, Số 62072386, Số 62072387, Số 62072389, Số 62002305 và Số 62272401), và Quỹ Khoa học Tự nhiên của Tỉnh Phúc Kiến của Trung Quốc (Số 2021J01002, Số 2022J06001).

TÀI LIỆU THAM KHẢO
[1] S. Han, J. Pool, J. Tran, and W. Dally, "Learning both weights and connections for efficient neural network," in Advances in Neural Information Processing Systems (NeurIPS), 2015, pp. 1135–1143.
[2] Y. He, P. Liu, Z. Wang, Z. Hu, and Y. Yang, "Filter pruning via geometric median for deep convolutional neural networks acceleration," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4340–4349.
[3] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, "Binarized neural networks," in Advances in Neural Information Processing Systems (NeurIPS), 2016, pp. 4107–4115.
[4] Y. Zhong, M. Lin, M. Chen, K. Li, Y. Shen, F. Chao, Y. Wu, and R. Ji, "Fine-grained data distribution alignment for post-training quantization," in European Conference on Computer Vision (ECCV). Springer, 2022, pp. 70–86.
[5] Y. Zhong, M. Lin, G. Nan, J. Liu, B. Zhang, Y. Tian, and R. Ji, "IntraQ: Learning synthetic images with intra-class heterogeneity for zero-shot network quantization," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 12 339–12 348.
[6] B. Peng, W. Tan, Z. Li, S. Zhang, D. Xie, and S. Pu, "Extreme network compression via filter group approximation," in European Conference on Computer Vision (ECCV), 2018, pp. 300–316.
[7] K. Hayashi, T. Yamaguchi, Y. Sugawara, and S.-i. Maeda, "Exploring unexplored tensor network decompositions for convolutional neural networks," in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 5552–5562.
[8] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio, "Fitnets: Hints for thin deep nets," arXiv preprint arXiv:1412.6550, 2014.
[9] G. Hinton, O. Vinyals, and J. Dean, "Distilling the knowledge in a neural network," arXiv preprint arXiv:1503.02531, 2015.
[10] M. Ashby, C. Baaij, P. Baldwin, M. Bastiaan, O. Bunting, A. Cairncross, C. Chalmers, L. Corrigan, S. Davis, N. van Doorn et al., "Exploiting unstructured sparsity on next-generation datacenter hardware," 2019.
[11] E. Elsen, M. Dukhan, T. Gale, and K. Simonyan, "Fast sparse convnets," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 14 629–14 638.
[12] J. Park, S. R. Li, W. Wen, H. Li, Y. Chen, and P. Dubey, "Holistic sparsecnn: Forging the trident of accuracy, speed, and size," arXiv preprint arXiv:1608.01409, 2016.
[13] D. Molchanov, A. Ashukha, and D. Vetrov, "Variational dropout sparsifies deep neural networks," in International Conference on Machine Learning (ICML), 2017, pp. 2498–2507.
[14] X. Chang, Y. Li, S. Oymak, and C. Thrampoulidis, "Provable benefits of overparameterization in model compression: From double descent to pruning neural networks," in AAAI Conference on Artificial Intelligence (AAAI), 2020, pp. 6974–6983.
[15] D. Joo, E. Yi, S. Baek, and J. Kim, "Linearly replaceable filters for deep network channel pruning," in AAAI Conference on Artificial Intelligence (AAAI), 2021, pp. 8021–8029.
[16] Y. LeCun, J. Denker, and S. Solla, "Optimal brain damage," in Advances in Neural Information Processing Systems (NeurIPS), 1989, pp. 598–605.
[17] Y. He, Y. Ding, P. Liu, L. Zhu, H. Zhang, and Y. Yang, "Learning filter pruning criteria for deep convolutional neural networks acceleration," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 2009–2018.
[18] H. Mostafa and X. Wang, "Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization," in International Conference on Machine Learning (ICML), 2019, pp. 4646–4655.
[19] U. Evci, T. Gale, J. Menick, P. S. Castro, and E. Elsen, "Rigging the lottery: Making all tickets winners," in International Conference on Machine Learning (ICML), 2020, pp. 2943–2952.
[20] Y. Wang, X. Zhang, L. Xie, J. Zhou, H. Su, B. Zhang, and X. Hu, "Pruning from scratch," in AAAI Conference on Artificial Intelligence (AAAI), 2020, pp. 12 273–12 280.
[21] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770–778.
[22] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, "Imagenet: A large-scale hierarchical image database," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 248–255.
[23] A. Kusupati, V. Ramanujan, R. Somani, M. Wortsman, P. Jain, S. Kakade, and A. Farhadi, "Soft threshold weight reparameterization for learnable sparsity," in International Conference on Machine Learning (ICML), 2020, pp. 5544–5555.
[24] D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu, and A. Liotta, "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science," Nature Communications, vol. 9, pp. 1–12, 2018.
[25] N. Lee, T. Ajanthan, and P. Torr, "Snip: Single-shot network pruning based on connection sensitivity," in International Conference on Learning Representations (ICLR), 2019.
[26] J. Frankle and M. Carbin, "The lottery ticket hypothesis: Finding sparse, trainable neural networks," in International Conference on Learning Representations (ICLR), 2019.
[27] V. Ramanujan, M. Wortsman, A. Kembhavi, A. Farhadi, and M. Rastegari, "What's hidden in a randomly weighted neural network?" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 11 893–11 902.
[28] H. Zhou, J. Lan, R. Liu, and J. Yosinski, "Deconstructing lottery tickets: Zeros, signs, and the supermask," in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 3597–3607.
[29] L. Orseau, M. Hutter, and O. Rivasplata, "Logarithmic pruning is all you need," in Advances in Neural Information Processing Systems (NeurIPS), 2020, pp. 2925–2934.
[30] K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition," in International Conference on Learning Representations (ICLR), 2015.

--- TRANG 14 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 14

[31] A. Krizhevsky, G. Hinton et al., "Learning multiple layers of features from tiny images," 2009.
[32] C. Wang, G. Zhang, and R. Grosse, "Picking winning tickets before training by preserving gradient flow," in International Conference on Learning Representations (ICLR), 2020.
[33] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, "Mobilenets: Efficient convolutional neural networks for mobile vision applications," arXiv preprint arXiv:1704.04861, 2017.
[34] B. Hassibi and D. Stork, "Second order derivatives for network pruning: Optimal brain surgeon," in Advances in Neural Information Processing Systems (NeurIPS), 1992, pp. 164–171.
[35] G. Thimm and E. Fiesler, "Evaluating pruning methods," in International Symposium on Artificial Neural Networks (ISANN), 1995, pp. 20–25.
[36] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, "Pruning convolutional neural networks for resource efficient inference," in International Conference on Learning Representations (ICLR), 2017.
[37] Y. Guo, A. Yao, and Y. Chen, "Dynamic network surgery for efficient dnns," arXiv preprint arXiv:1608.04493, 2016.
[38] S. Srinivas, A. Subramanya, and R. Venkatesh Babu, "Training sparse neural networks," in IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2017, pp. 138–145.
[39] X. Xiao, Z. Wang, and S. Rajasekaran, "Autoprune: Automatic network pruning by regularizing auxiliary parameters," Advances in Neural Information Processing Systems (NeurIPS), vol. 32, 2019.
[40] P. Savarese, H. Silva, and M. Maire, "Winning the lottery with continuous sparsification," Advances in Neural Information Processing Systems (NeurIPS), vol. 33, pp. 11 380–11 390, 2020.
[41] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, "Pruning filters for efficient convnets," in International Conference on Learning Representations (ICLR), 2017.
[42] X. Ding, G. Ding, Y. Guo, and J. Han, "Centripetal sgd for pruning very deep convolutional networks with complicated structure," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4943–4953.
[43] Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, T. K.-T. Cheng, and J. Sun, "Metapruning: Meta learning for automatic neural network channel pruning," in International Conference on Computer Vision (ICCV), 2019, pp. 3296–3305.
[44] M. Lin, R. Ji, Y. Wang, Y. Zhang, B. Zhang, Y. Tian, and L. Shao, "Hrank: Filter pruning using high-rank feature map," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 1529–1538.
[45] X. Ruan, Y. Liu, B. Li, C. Yuan, and W. Hu, "Dpfps: Dynamic and progressive filter pruning for compressing convolutional neural networks from scratch," in AAAI Conference on Artificial Intelligence (AAAI), 2021, pp. 2495–2503.
[46] X. Ding, T. Hao, J. Tan, J. Liu, J. Han, Y. Guo, and G. Ding, "Resrep: Lossless cnn pruning via decoupling remembering and forgetting," in International Conference on Computer Vision (ICCV), 2021, pp. 4510–4520.
[47] S. Guo, Y. Wang, Q. Li, and J. Yan, "Dmcp: Differentiable markov channel pruning for neural networks," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 1539–1547.
[48] B. Li, B. Wu, J. Su, and G. Wang, "Eagleeye: Fast sub-net evaluation for efficient neural network pruning," in European Conference on Computer Vision (ECCV). Springer, 2020, pp. 639–654.
[49] J. Yu, A. Lukefahr, D. Palframan, G. Dasika, R. Das, and S. Mahlke, "Scalpel: Customizing dnn pruning to the underlying hardware parallelism," ACM SIGARCH Computer Architecture News, vol. 45, no. 2, pp. 548–560, 2017.
[50] H. Mao, S. Han, J. Pool, W. Li, X. Liu, Y. Wang, and W. J. Dally, "Exploring the regularity of sparse structure in convolutional neural networks," arXiv preprint arXiv:1705.08922, 2017.
[51] S. Zagoruyko and N. Komodakis, "Wide residual networks," arXiv preprint arXiv:1605.07146, 2016.
[52] M. Ye, L. Wu, and Q. Liu, "Greedy optimization provably wins the lottery: Logarithmic number of winning tickets is enough," in Advances in Neural Information Processing Systems (NeurIPS), 2020, pp. 16 409–16 420.
[53] Y. Bengio, N. Léonard, and A. Courville, "Estimating or propagating gradients through stochastic neurons for conditional computation," arXiv preprint arXiv:1308.3432, 2013.
[54] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, "Rethinking the value of network pruning," in International Conference on Learning Representations (ICLR), 2019.
[55] A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classification with deep convolutional neural networks," in Advances in Neural Information Processing Systems (NeurIPS), 2012, pp. 1106–1114.
[56] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort, "Up or down? adaptive rounding for post-training quantization," in International Conference on Machine Learning (ICML). PMLR, 2020, pp. 7197–7206.
[57] Y. Zhang, M. Lin, M. Chen, F. Chao, and R. Ji, "Optimizing gradient-driven criteria in network sparsity: Gradient is all you need," arXiv preprint arXiv:2201.12826, 2022.
[58] S. Hayou, J.-F. Ton, A. Doucet, and Y. W. Teh, "Pruning untrained neural networks: Principles and analysis," arXiv preprint arXiv:2002.08797, 2020.
[59] M. Tan and Q. Le, "Efficientnet: Rethinking model scaling for convolutional neural networks," in International conference on machine learning (ICML). PMLR, 2019, pp. 6105–6114.
[60] G. Bellec, D. Kappel, W. Maass, and R. Legenstein, "Deep rewiring: Training very sparse deep networks," in International Conference on Learning Representations (ICLR), 2018.
[61] I. Loshchilov and F. Hutter, "Sgdr: Stochastic gradient descent with warm restarts," in International Conference on Learning Representations (ICLR), 2017.
[62] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., "Pytorch: An imperative style, high-performance deep learning library," in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 8026–8037.

Yuxin Zhang nhận bằng Cử nhân Khoa học Máy tính, Trường Tin học, Đại học Xiamen, Xiamen, Trung Quốc, năm 2020. Hiện tại anh đang theo đuổi bằng Tiến sĩ tại Đại học Xiamen, Trung Quốc. Các xuất bản của anh trên các hội nghị/tạp chí hàng đầu bao gồm IEEE TPAMI, IEEE TNNLS, NeurIPS, ICLR, ICML, ICCV, IJCAI và vv. Sở thích nghiên cứu của anh bao gồm thị giác máy tính và nén & tăng tốc mạng nơ-ron.

Mingbao Lin hoàn thành nghiên cứu Thạc sĩ-Tiến sĩ và đạt được bằng Tiến sĩ về khoa học và công nghệ thông minh từ Đại học Xiamen, Xiamen, Trung Quốc, năm 2022. Trước đó, anh nhận bằng Cử nhân từ Đại học Fuzhou, Fuzhou, Trung Quốc, năm 2016.
Hiện tại anh là một nhà nghiên cứu cấp cao tại Phòng thí nghiệm Youtu của Tencent, Thượng Hải, Trung Quốc. Các xuất bản của anh trên các hội nghị/tạp chí hàng đầu bao gồm IEEE TPAMI, IJCV, IEEE TIP, IEEE TNNLS, CVPR, NeurIPS, AAAI, IJCAI, ACM MM và vv. Sở thích nghiên cứu hiện tại của anh là phát triển mô hình thị giác hiệu quả, cũng như truy xuất thông tin.

--- TRANG 15 ---
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 15

Yunshan Zhong nhận bằng Cử nhân Kỹ thuật Phần mềm từ Viện Công nghệ Bắc Kinh, Bắc Kinh, Trung Quốc năm 2017, và bằng Thạc sĩ Kỹ thuật Phần mềm từ Đại học Bắc Kinh, Bắc Kinh, Trung Quốc năm 2020. Hiện tại anh là nghiên cứu sinh năm thứ hai tại phòng thí nghiệm MAC, Viện Trí tuệ Nhân tạo, Đại học Xiamen, Trung Quốc, dưới sự hướng dẫn của Giáo sư Rongrong Ji. Anh đã xuất bản nhiều bài báo được đánh giá ngang hàng trên các hội nghị hàng đầu bao gồm CVPR, ICCV, và ECCV. Sở thích nghiên cứu hiện tại của anh là nén mô hình.

Fei Chao nhận bằng Cử nhân kỹ thuật cơ khí từ Đại học Fuzhou, Cộng hòa Nhân dân Trung Hoa, và bằng Thạc sĩ với danh hiệu xuất sắc về khoa học máy tính từ Đại học Wales, Vương quốc Anh, lần lượt vào năm 2004 và 2005, và bằng Tiến sĩ về robot học từ Đại học Aberystwyth, Wales, Vương quốc Anh năm 2009. Anh từng là Nghiên cứu viên dưới sự giám sát của Giáo sư Mark H. Lee tại Đại học Aberystwyth từ 2009 đến 2010. Hiện tại anh là Phó Giáo sư tại Khoa Khoa học Nhận thức, Đại học Xiamen, Cộng hòa Nhân dân Trung Hoa. Anh đã xuất bản khoảng 20 bài báo tạp chí và hội nghị được đánh giá ngang hàng. Sở thích nghiên cứu của anh bao gồm robot học phát triển, học máy, và thuật toán tối ưu hóa. Anh là Phó Chủ tịch của IEEE Computer Intelligence Society Xiamen Chapter. Ngoài ra, anh cũng là thành viên của ACM và CCF.

Rongrong Ji (Thành viên Cấp cao, IEEE) là Giáo sư Đặc biệt Nanqiang tại Đại học Xiamen, Phó Giám đốc Văn phòng Khoa học và Công nghệ tại Đại học Xiamen, và Giám đốc Phòng thí nghiệm Phân tích và Tính toán Phương tiện. Ông được trao giải là Quỹ Khoa học Quốc gia dành cho Các Học giả Trẻ Xuất sắc (2014), Kế hoạch Mười nghìn Quốc gia dành cho Tài năng Trẻ Hàng đầu (2017), và Quỹ Khoa học Quốc gia dành cho Các Học giả Trẻ Xuất sắc (2020). Nghiên cứu của ông thuộc lĩnh vực thị giác máy tính, phân tích đa phương tiện, và học máy. Ông đã xuất bản 50+ bài báo trong ACM/IEEE Transactions, bao gồm TPAMI và IJCV, và 100+ bài báo đầy đủ tại các hội nghị hàng đầu, như CVPR và NeurIPS. Các xuất bản của ông đã nhận được hơn 10K trích dẫn trong Google Scholar. Ông là người nhận Giải thưởng Bài báo Xuất sắc nhất của ACM Multimedia 2011. Ông đã phục vụ như Chủ tịch Khu vực trong các hội nghị hàng đầu như CVPR và ACM Multimedia. Ông cũng là Thành viên Tư vấn cho việc Xây dựng Trí tuệ Nhân tạo trong Ủy ban Giáo dục Thông tin Điện tử của Bộ Giáo dục Quốc gia.
