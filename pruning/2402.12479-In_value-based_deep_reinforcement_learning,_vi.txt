Trong học tăng cường sâu dựa trên giá trị,
một mạng đã cắt tỉa là một mạng tốt

Johan Obando-Ceron1 2 3Aaron Courville2 3Pablo Samuel Castro1 2 3

Tóm tắt
Nghiên cứu gần đây đã chỉ ra rằng các agent học tăng cường sâu gặp khó khăn trong việc sử dụng hiệu quả các tham số mạng của chúng. Chúng tôi tận dụng những hiểu biết trước đó về lợi ích của các kỹ thuật huấn luyện thưa thớt và chứng minh rằng cắt tỉa độ lớn dần dần cho phép các agent dựa trên giá trị tối đa hóa hiệu quả tham số. Điều này dẫn đến các mạng mang lại cải thiện hiệu suất đáng kể so với các mạng truyền thống, chỉ sử dụng một phần nhỏ của toàn bộ tham số mạng. Mã nguồn của chúng tôi được công khai, xem Phụ lục A để biết chi tiết.

1. Giới thiệu

Mặc dù có những ví dụ thành công về học tăng cường sâu (RL) được áp dụng cho các vấn đề thực tế (Mnih et al., 2015; Berner et al., 2019; Vinyals et al., 2019; Fawzi et al., 2022; Bellemare et al., 2020), có bằng chứng ngày càng tăng về các thách thức và bệnh lý phát sinh khi huấn luyện những mạng này (Ostrovski et al., 2021; Kumar et al., 2021a; Lyle et al., 2022; Graesser et al., 2022; Nikishin et al., 2022; Sokar et al., 2023; Ceron et al., 2023). Đặc biệt, đã được chỉ ra rằng các agent RL sâu không khai thác đầy đủ các tham số của mạng: Kumar et al. (2021a) đã chứng minh rằng có một sự thiếu tham số hóa ngầm, Sokar et al. (2023) tiết lộ rằng một số lượng lớn neuron trở nên không hoạt động trong quá trình huấn luyện, và Graesser et al. (2022) chỉ ra rằng các phương pháp huấn luyện thưa thớt có thể duy trì hiệu suất với một phần rất nhỏ của các tham số mạng ban đầu.

Một trong những phát hiện đáng ngạc nhiên nhất của công trình cuối này là việc áp dụng kỹ thuật cắt tỉa độ lớn dần dần được đề xuất bởi Zhu & Gupta (2017) trên DQN (Mnih et al., 2015) với backbone ResNet (như được giới thiệu trong Impala (Espeholt et al., 2018)), dẫn đến cải thiện hiệu suất 50% so với đối tác dày đặc, chỉ với 10% tham số ban đầu (xem panel dưới bên phải của Hình 1 của Graesser et al. (2022)). Thật thú vị, khi cùng một kỹ thuật cắt tỉa được áp dụng cho kiến trúc CNN ban đầu thì không có cải thiện hiệu suất, nhưng cũng không có suy giảm.

Việc cùng một kỹ thuật cắt tỉa có thể có những kết quả khác nhau về mặt chất lượng, nhưng không âm tính, chỉ đơn giản bằng cách thay đổi kiến trúc cơ bản là thú vị. Điều này gợi ý rằng việc huấn luyện các agent RL sâu với các topology mạng không chuẩn (như được tạo ra bởi các kỹ thuật như cắt tỉa độ lớn dần dần) có thể hữu ích một cách tổng quát, và đáng để nghiên cứu sâu hơn.

Trong bài báo này, chúng tôi khám phá cắt tỉa độ lớn dần dần như một kỹ thuật tổng quát để cải thiện hiệu suất của các agent RL. Chúng tôi chứng minh rằng ngoài việc cải thiện hiệu suất của các kiến trúc mạng chuẩn cho các agent dựa trên giá trị, các lợi ích tăng tỷ lệ thuận với kích thước của kiến trúc mạng cơ sở. Điểm cuối này có ý nghĩa, vì các mạng RL sâu được biết là gặp khó khăn khi mở rộng kiến trúc (Ota et al., 2021; Farebrother et al., 2023; Taiga et al., 2023; Schwarzer et al., 2023).

Các đóng góp chính của chúng tôi như sau. Chúng tôi:
• trình bày cắt tỉa độ lớn dần dần như một kỹ thuật tổng quát để tối đa hóa hiệu quả tham số trong RL dựa trên giá trị;
• chứng minh rằng các mạng được huấn luyện với kỹ thuật này tạo ra các agent mạnh hơn so với các đối tác dày đặc của chúng, và tiếp tục cải thiện khi chúng ta mở rộng kích thước của mạng ban đầu;
• điều tra kỹ thuật này trên một tập hợp đa dạng các agent và chế độ huấn luyện, bao gồm các phương pháp actor-critic;
• trình bày các phân tích chuyên sâu để hiểu rõ hơn lý do đằng sau lợi ích của chúng.

2. Công trình liên quan

Mở rộng trong RL Sâu Các mạng neural sâu đã là yếu tố thúc đẩy đằng sau nhiều ứng dụng thành công của học tăng cường vào các nhiệm vụ thực tế. Tuy nhiên, về mặt lịch sử, việc mở rộng những mạng này theo cách tương tự như những gì đã dẫn đến "quy luật mở rộng" trong học có giám sát mà không bị suy giảm hiệu suất là khó khăn; điều này phần lớn do các bất ổn định huấn luyện gia tăng vốn có trong học tăng cường (Van Hasselt et al., 2018; Sinha et al., 2020; Ota et al., 2021). Các công trình gần đây đã có thể làm được điều này thành công phải dựa vào một số kỹ thuật có mục tiêu và lựa chọn siêu tham số cẩn thận (Farebrother et al., 2023; Taiga et al., 2023; Schwarzer et al., 2023; Ceron et al., 2023).

Cobbe et al. (2020); Farebrother et al. (2023) và Schwarzer et al. (2023) đã chuyển từ kiến trúc CNN ban đầu của Mnih et al. (2015) sang kiến trúc dựa trên ResNet, như được đề xuất bởi Espeholt et al. (2018), điều này tỏ ra phù hợp hơn để mở rộng. Cobbe et al. (2020) và Farebrother et al. (2023) quan sát thấy lợi thế khi tăng số lượng features trong mỗi lớp của kiến trúc ResNet. Schwarzer et al. (2023) chỉ ra rằng hiệu suất của agent của họ (BBF) tiếp tục tăng tỷ lệ thuận với độ rộng của mạng. Bjorck et al. (2021) đề xuất chuẩn hóa phổ để giảm thiểu bất ổn định huấn luyện và cho phép mở rộng kiến trúc của họ. Ceron et al. (2023) đề xuất giảm kích thước batch để cải thiện hiệu suất, ngay cả khi mở rộng mạng.

Ceron et al. (2024) chứng minh rằng trong khi việc mở rộng tham số với mạng tích chập làm tổn hại hiệu suất RL đơn nhiệm vụ trên Atari, việc kết hợp các mô-đun Mixture-of-Expert (MoE) trong những mạng như vậy cải thiện hiệu suất. Farebrother et al. (2024) chứng minh rằng các hàm giá trị được huấn luyện với categorical cross-entropy cải thiện đáng kể hiệu suất và khả năng mở rộng trong nhiều lĩnh vực khác nhau.

Mô hình Thưa thớt trong RL Sâu Các nghiên cứu trước đây (Schmitt et al., 2018; Zhang et al., 2019) đã sử dụng chưng cất kiến thức với dữ liệu tĩnh để giảm thiểu bất ổn định, dẫn đến các agent nhỏ, nhưng dày đặc. Livne & Cohen (2020) giới thiệu cắt tỉa và thu nhỏ chính sách, sử dụng cắt tỉa chính sách lặp lại tương tự như cắt tỉa độ lớn lặp lại (Han et al., 2015), để có được một agent DRL thưa thớt. Việc khám phá giả thuyết vé số trong DRL ban đầu được thực hiện bởi Yu et al. (2019), và sau đó Vischer et al. (2021) chứng minh rằng một vé trúng thưa thớt cũng có thể được xác định thông qua behavior cloning. Sokar et al. (2021) đề xuất sử dụng tiến hóa cấu trúc của topology mạng trong DRL, đạt được 50% độ thưa thớt mà không suy giảm hiệu suất. Arnob et al. (2021) giới thiệu cắt tỉa một lần cho Học tăng cường offline.

Graesser et al. (2022) phát hiện ra rằng cắt tỉa thường mang lại kết quả cải thiện, và các phương pháp huấn luyện thưa thớt động, nơi topology thưa thớt thay đổi trong suốt quá trình huấn luyện (Mocanu et al., 2018; Evci et al., 2020), có thể vượt trội đáng kể so với huấn luyện thưa thớt tĩnh, nơi topology thưa thớt vẫn cố định trong suốt quá trình huấn luyện. Tan et al. (2023) nâng cao hiệu quả của huấn luyện thưa thớt động thông qua việc giới thiệu một cơ chế target temporal difference đa bước trì hoãn mới và một buffer replay có khả năng động. Grooten et al. (2023) đề xuất một phương pháp lọc tiếng ồn tự động, sử dụng các nguyên tắc của huấn luyện thưa thớt động để điều chỉnh topology mạng tập trung vào các features liên quan đến nhiệm vụ.

Quá tham số hóa trong RL Sâu Song et al. (2019) và Zhang et al. (2018) nổi bật và xu hướng của các mạng RL quá khớp, trong khi Nikishin et al. (2022) và Sokar et al. (2023) chứng minh sự phổ biến của mất tính dẻo trong các mạng RL, dẫn đến suy giảm hiệu suất cuối cùng. Một số chiến lược đã được đề xuất để giảm thiểu điều này, như data augmentation (Yarats et al., 2021; Cetin et al., 2022), dropout (Gal & Ghahramani, 2016), và layer và batch normalization (Ba et al., 2016; Ioffe & Szegedy, 2015). Hiraoka et al. (2021) chứng minh thành công của việc sử dụng dropout và layer normalization trong Soft Actor-Critic (Haarnoja et al., 2018), trong khi Liu et al. (2020) xác định rằng việc áp dụng regularization trọng số ℓ2 trên actors có thể nâng cao cả thuật toán on- và off-policy.

Nikishin et al. (2022) xác định xu hướng của các mạng quá khớp với dữ liệu sớm (the primacy bias), có thể cản trở việc học tiếp theo, và đề xuất khởi tạo lại mạng định kỳ như một phương tiện để giảm thiểu nó. Tương tự, Sokar et al. (2023) đề xuất khởi tạo lại các neuron không hoạt động để cải thiện tính dẻo của mạng, trong khi Nikishin et al. (2023) đề xuất tiêm tính dẻo bằng cách tạm thời đóng băng mạng hiện tại và sử dụng các trọng số được khởi tạo mới để tạo điều kiện cho việc học liên tục.

3. Kiến thức nền

Học tăng cường sâu Mục tiêu trong Học tăng cường là tối ưu hóa tổng return chiết khấu tích lũy trên một horizon dài, và thường được công thức hóa như một quá trình quyết định Markov (MDP) (X,A, P, r, gamma). Một MDP bao gồm một không gian trạng thái X, một không gian hành động A, một mô hình động lực chuyển tiếp P:X ×A → ∆(X) (trong đó ∆(X) là một phân phối trên một tập X), một hàm phần thưởng R:X × A → R, và một hệ số chiết khấu gamma trong [0,1). Một chính sách pi:X → ∆(A) làm chính thức hóa hành vi của agent.

Đối với một chính sách pi, Qpi(x,a) biểu diễn phần thưởng chiết khấu kỳ vọng đạt được bằng cách thực hiện hành động a trong trạng thái x và sau đó tuân theo chính sách pi:
Qpi(x,a) := Epi[∑∞t=0 gammat R(xt,at)|x0=x,a0=a].

Hàm Q tối ưu, ký hiệu là Q⋆(x,a), thỏa mãn phép lặp Bellman
Q⋆(x,a) = Ex′∼P(x′|x,a)[R(x,a) + gamma max a′ Q⋆(x′,a′)].

Hầu hết các phương pháp dựa trên giá trị hiện đại sẽ xấp xỉ Q thông qua một mạng neural với các tham số theta, ký hiệu là Qtheta. Ý tưởng này được giới thiệu bởi Mnih et al. (2015) với agent DQN của họ, đã phục vụ như cơ sở cho hầu hết các thuật toán RL sâu hiện đại. Mạng Qtheta thường được huấn luyện với một temporal difference loss, như:

L(theta) = E(x,a,r,x′)∼D[(r + gamma max a′∈A Q̄(x′,a′) − Qtheta(x,a))2],

Ở đây D biểu diễn một tập hợp các chuyển tiếp (xt,at,rt,xt+1) được lưu trữ mà agent lấy mẫu từ đó để học (được gọi là replay buffer). Q̄ là một mạng tĩnh thỉnh thoảng sao chép các tham số của nó từ Qtheta; mục đích của nó là tạo ra các target học ổn định hơn.

Rainbow (Hessel et al., 2018) đã mở rộng và cải thiện thuật toán DQN ban đầu với double Q-learning (van Hasselt et al., 2016), prioritized experience replay (Schaul et al., 2016), dueling networks (Wang et al., 2016), multi-step returns (Sutton, 1988), distributional reinforcement learning (Bellemare et al., 2017), và noisy networks (Fortunato et al., 2018).

Cắt tỉa dần dần Trong các thiết lập học có giám sát có một sự quan tâm rộng rãi đến các kỹ thuật huấn luyện thưa thớt, theo đó chỉ một tập con của toàn bộ tham số mạng được huấn luyện/sử dụng (Gale et al., 2019). Điều này được thúc đẩy bởi hiệu quả tính toán và không gian, cũng như tốc độ suy luận. Zhu & Gupta (2017) đề xuất một lịch trình đa thức để dần dần làm thưa thớt một mạng dày đặc trong quá trình huấn luyện bằng cách cắt tỉa các tham số mô hình với độ lớn trọng số thấp.

Cụ thể, gọi sF ∈ [0,1] biểu thị mức độ thưa thớt cuối cùng mong muốn (ví dụ 0.95 trong hầu hết các thí nghiệm của chúng tôi) và gọi tstart và tend biểu thị các iteration bắt đầu và kết thúc của cắt tỉa, tương ứng; sau đó mức độ thưa thớt tại iteration t được cho bởi:

st = sF(1 − (1 − (t − tstart)/(tend − tstart))³) nếu tstart ≤ t ≤ tend
     0.0 nếu t < tstart
     sF nếu t > tend

Graesser et al. (2022) áp dụng ý tưởng này cho các mạng RL sâu, thiết lập tstart ở 20% của huấn luyện và tend ở 80% của huấn luyện.

4. Cắt tỉa có thể tăng cường hiệu suất RL sâu

Chúng tôi điều tra tính hữu ích tổng quát của cắt tỉa độ lớn dần dần trong các agent RL sâu trong cả thiết lập online và offline.

4.1. Chi tiết triển khai

Đối với các agent DQN và Rainbow cơ sở, chúng tôi sử dụng các triển khai Jax của thư viện Dopamine¹ (Castro et al., 2018) với các giá trị mặc định của chúng. Đáng chú ý rằng Dopamine cung cấp một phiên bản "compact" của agent Rainbow ban đầu, chỉ sử dụng multi-step updates, prioritized replay, và distributional RL. Đối với tất cả các thí nghiệm, chúng tôi sử dụng kiến trúc Impala được giới thiệu bởi Espeholt et al. (2018), là một ResNet 15 lớp, trừ khi được chỉ định khác. Lý do của chúng tôi cho điều này không chỉ vì kết quả từ Graesser et al. (2022), mà còn do một số công trình gần đây chứng minh rằng kiến trúc này dẫn đến hiệu suất cải thiện nói chung (Schmidt & Schmied, 2021; Kumar et al., 2022; Schwarzer et al., 2023).

Chúng tôi sử dụng thư viện JaxPruner² (Lee et al., 2024) cho cắt tỉa độ lớn dần dần, vì nó đã cung cấp tích hợp với Dopamine. Chúng tôi tuân theo lịch trình của Graesser et al. (2022): bắt đầu cắt tỉa mạng 20% vào quá trình huấn luyện và dừng ở 80%, giữ mạng thưa thớt cuối cùng cố định cho phần còn lại của quá trình huấn luyện. Hình 2 minh họa các lịch trình cắt tỉa được sử dụng trong các thí nghiệm của chúng tôi (cho 95% độ thưa thớt). Chúng tôi đánh giá các agent của mình trên Arcade Learning Environment (ALE) (Bellemare et al., 2013) trên cùng 15 trò chơi được sử dụng bởi Graesser et al. (2022), được chọn vì tính đa dạng của chúng³. Vì các cân nhắc tính toán, hầu hết các thí nghiệm được tiến hành trên 40 triệu frames (thay vì 200 triệu chuẩn); trong các điều tra của chúng tôi, chúng tôi thấy rằng sự khác biệt định tính giữa các thuật toán ở 40 triệu frames hầu như nhất quán với những ở 100 triệu (ví dụ xem Hình 10).

¹Mã Dopamine có tại https://github.com/google/dopamine.
²Mã JaxPruner có tại https://github.com/google-research/jaxpruner.
³Được thảo luận trong A.4 trong Graesser et al. (2022).

Chúng tôi tuân theo các hướng dẫn được phác thảo bởi Agarwal et al. (2021) để đánh giá: mỗi thí nghiệm được chạy với 5 seed độc lập, và chúng tôi báo cáo interquantile mean (IQM) được chuẩn hóa theo con người, tổng hợp trên 15 trò chơi, cấu hình và seed, cùng với khoảng tin cậy bootstrap phân tầng 95%. Tất cả các thí nghiệm được chạy trên GPU NVIDIA Tesla P100, và mỗi thí nghiệm mất khoảng 2 ngày để hoàn thành. Tất cả các siêu tham số được liệt kê trong Phụ lục F.

4.2. RL Online

Trong khi Graesser et al. (2022) chứng minh rằng các mạng thưa thớt có khả năng duy trì hiệu suất của agent, nếu các mức độ thưa thớt này quá cao, hiệu suất cuối cùng sẽ suy giảm. Điều này trực quan, vì với mức độ thưa thớt cao hơn, có ít tham số hoạt động còn lại trong mạng. Một câu hỏi tự nhiên là liệu việc mở rộng mạng ban đầu của chúng ta có cho phép mức độ thưa thớt cao không. Do đó, chúng tôi bắt đầu cuộc điều tra bằng cách áp dụng cắt tỉa độ lớn dần dần trên DQN với kiến trúc Impala, nơi chúng tôi đã mở rộng các lớp tích chập với hệ số 3. Hình 3 xác nhận rằng đây thực sự là trường hợp: 90% và 95% độ thưa thớt tạo ra cải thiện hiệu suất 33%, và 99% độ thưa thớt duy trì hiệu suất.

Độ thưa thớt 95% liên tục mang lại hiệu suất tốt nhất trong các khám phá ban đầu của chúng tôi, vì vậy chúng tôi chủ yếu tập trung vào mức độ thưa thớt này cho các điều tra của mình. Hình 1 là một kết quả ấn tượng: chúng tôi quan sát cải thiện hiệu suất gần 60% (DQN) và 50% (Rainbow) so với các kiến trúc ban đầu (không cắt tỉa và không mở rộng). Ngoài ra, trong khi hiệu suất của các kiến trúc chưa cắt tỉa giảm đơn điệu với độ rộng tăng, hiệu suất của các đối tác đã cắt tỉa tăng đơn điệu. Trong Hình 6, chúng tôi đánh giá cắt tỉa trên DQN và Rainbow trên tất cả 60 trò chơi Atari 2600, xác nhận rằng phát hiện của chúng tôi không cụ thể cho 15 trò chơi được chọn ban đầu.

Khi chuyển cả hai agent sang sử dụng kiến trúc CNN ban đầu của Mnih et al. (2015), chúng tôi thấy một xu hướng tương tự với Rainbow, nhưng thấy ít cải thiện trong DQN (Hình 4). Kết quả này nhất quán với các phát hiện của Graesser et al. (2022), nơi không có cải thiện thực sự nào được quan sát với cắt tỉa trên các kiến trúc CNN. Một quan sát thú vị là, với kiến trúc CNN này, hiệu suất của DQN dường như có lợi từ độ rộng tăng, trong khi hiệu suất của Rainbow chịu suy giảm nhẹ.

Khi đánh giá trên các agent dựa trên giá trị hiện đại hơn, cụ thể là IQN (Dabney et al., 2018) và Munchausen-IQN (Vieillard et al., 2020), chúng tôi quan sát cùng lợi thế phát sinh từ cắt tỉa (xem Phụ lục G.2).

Các phát hiện của chúng tôi cho đến nay gợi ý rằng việc sử dụng cắt tỉa độ lớn dần dần tăng hiệu quả tham số của những agent này. Nếu vậy, thì những mạng thưa thớt này cũng nên có khả năng hưởng lợi từ nhiều cập nhật gradient hơn. Replay ratio⁴, là số lượng cập nhật gradient cho mỗi bước môi trường, đo chính xác điều này; được biết rằng việc tăng giá trị này mà không bị suy giảm hiệu suất là khó khăn (Fedus et al., 2020; Nikishin et al., 2022; Schwarzer et al., 2023; D'Oro et al., 2023).

Trong Hình 5, chúng tôi thực sự có thể xác nhận rằng các kiến trúc đã cắt tỉa duy trì lợi thế hiệu suất so với baseline chưa cắt tỉa ngay cả ở các giá trị replay ratio cao. Tỷ lệ suy giảm nhanh hơn với cắt tỉa có thể gợi ý rằng lịch trình cắt tỉa cần được điều chỉnh lại cho những thiết lập này.

4.3. Chế độ dữ liệu thấp

Kaiser et al. (2020) giới thiệu benchmark Atari 100k để đánh giá các agent RL trong thiết lập hạn chế mẫu, chỉ cho phép agent 100k⁵ tương tác môi trường. Đối với chế độ này, Kostrikov et al. (2020) giới thiệu DrQ, một biến thể của DQN sử dụng data augmentation; các siêu tham số cho agent này được tối ưu hóa thêm bởi Agarwal et al. (2021) trong DrQ(ϵ). Tương tự, Van Hasselt et al. (2019) giới thiệu Data-Efficient Rainbow (DER), tối ưu hóa các siêu tham số của Rainbow (Hessel et al., 2018) cho chế độ dữ liệu thấp này.

Khi được đánh giá trên chế độ dữ liệu thấp này, các agent đã cắt tỉa của chúng tôi không thể hiện lợi ích. Tuy nhiên, khi chúng tôi chạy cho 40M tương tác môi trường (như được đề xuất bởi Ceron et al. (2023)), chúng tôi quan sát lợi ích đáng kể khi sử dụng cắt tỉa độ lớn dần dần, như được hiển thị trong Hình 7. Thú vị, trong DrQ(ϵ) các agent đã cắt tỉa tránh được suy giảm hiệu suất ảnh hưởng đến baseline khi được huấn luyện lâu hơn.

4.4. RL Offline

Học tăng cường offline tập trung vào việc huấn luyện một agent hoàn toàn từ một tập dữ liệu cố định của các mẫu mà không có bất kỳ tương tác môi trường nào. Chúng tôi sử dụng hai phương pháp tiên tiến gần đây từ tài liệu: CQL (Kumar et al., 2020) và CQL+C51 (Kumar et al., 2022), cả hai với kiến trúc ResNet từ Espeholt et al. (2018). Theo Kumar et al. (2021b), chúng tôi huấn luyện những agent này trên 17 trò chơi Atari trong 200 triệu iterations frames, nơi 1 iteration tương ứng với 62,500 cập nhật gradient. Chúng tôi đánh giá các agent bằng cách xem xét một tập dữ liệu bao gồm một mẫu ngẫu nhiên 5% của tất cả các tương tác môi trường được thu thập bởi một agent DQN được huấn luyện cho 200M bước môi trường (Agarwal et al., 2020).

Lưu ý rằng vì chúng tôi đang huấn luyện cho một số bước khác với các thí nghiệm trước đó của chúng tôi, chúng tôi điều chỉnh lịch trình cắt tỉa tương ứng. Như được hiển thị trong Hình 8, cả CQL và CQL+C51 đều quan sát lợi ích đáng kể khi sử dụng các mạng đã cắt tỉa, đặc biệt với các mạng rộng hơn. Thú vị, trong chế độ offline, cắt tỉa cũng giúp tránh sụp đổ hiệu suất khi sử dụng mạng nông (hệ số width scale bằng 1), hoặc thậm chí cải thiện hiệu suất cuối cùng như trong trường hợp của CQL+C51.

4.5. Phương pháp Actor-Critic

Cuộc điều tra của chúng tôi cho đến nay đã tập trung vào các phương pháp dựa trên giá trị. Ở đây chúng tôi điều tra liệu cắt tỉa độ lớn dần dần có thể mang lại lợi ích hiệu suất cho Soft Actor Critic (SAC; Haarnoja et al., 2018), một thuật toán policy-gradient phổ biến. Chúng tôi đánh giá SAC trên năm môi trường điều khiển liên tục từ bộ MuJoCo (Todorov et al., 2012), sử dụng 10 seed độc lập cho mỗi. Trong Hình 9, chúng tôi trình bày kết quả cho Walker2d-v2 và Ant-v2, nơi chúng tôi thấy lợi thế của cắt tỉa độ lớn dần dần vẫn tồn tại; trong ba môi trường còn lại (xem Phụ lục E) không có lợi ích thật sự quan sát được từ cắt tỉa. Trong Phụ lục G.1, chúng tôi thấy một xu hướng tương tự với PPO (Schulman et al., 2017).

4.6. Tính ổn định của mạng đã cắt tỉa

Chúng tôi tuân theo lịch trình cắt tỉa được đề xuất bởi Graesser et al. (2022), thích ứng tự nhiên với các bước huấn luyện khác nhau (như đã thảo luận ở trên cho các thí nghiệm RL offline). Lịch trình này huấn luyện mạng thưa thớt cuối cùng chỉ trong 20% cuối cùng của các bước huấn luyện. Một câu hỏi tự nhiên là liệu mạng thưa thớt kết quả, khi được huấn luyện lâu hơn, vẫn có thể duy trì hiệu suất của nó. Để đánh giá điều này, chúng tôi huấn luyện DQN trong 100 triệu frames và áp dụng hai lịch trình cắt tỉa: lịch trình thông thường chúng tôi sẽ sử dụng cho 100M cũng như lịch trình chúng tôi thường sử dụng cho 40M bước huấn luyện (xem Hình 2).

Như Hình 10 cho thấy, ngay cả với lịch trình nén 40M, mạng đã cắt tỉa có thể duy trì hiệu suất mạnh mẽ của nó. Thú vị, với lịch trình nén, agent đạt được hiệu suất cao hơn nhanh hơn so với lịch trình thông thường. Điều này gợi ý rằng có nhiều chỗ để khám phá các lịch trình cắt tỉa thay thế.

4.7. Mở rộng tỷ lệ học và kích thước Batch

Tỷ lệ học mặc định hoặc kích thước batch có thể không tối ưu cho các mạng neural lớn. Tỷ lệ học mặc định cho DQN là 6.25×10⁻⁵. Chúng tôi chạy các thí nghiệm với tỷ lệ học chia cho hệ số scale width (vậy 2.08×10⁻⁵ cho hệ số width 3, và 1.25×10⁻⁵ cho hệ số width 5). Trong khi những tỷ lệ học này cải thiện hiệu suất của baseline, nó vẫn bị vượt qua bởi cắt tỉa (xem Hình 28). Chúng tôi quan sát một xu hướng tương tự khi đánh giá các giá trị kích thước batch khác nhau. Kích thước batch mặc định là 32 (cho tất cả các agent dựa trên giá trị được sử dụng trong bài báo này), và chúng tôi chạy các thí nghiệm với kích thước batch 16 và 64. Trong tất cả các trường hợp, cắt tỉa duy trì lợi thế mạnh mẽ của nó (xem Hình 29). Những kết quả này nhất quán với luận điểm rằng cắt tỉa có thể phục vụ như một cơ chế drop-in để tăng hiệu suất agent.

5. Tại sao cắt tỉa lại hiệu quả?

Chúng tôi tập trung phân tích của mình vào bốn trò chơi: BeamRider, Breakout, Enduro, và VideoPinball. Đối với mỗi trò chơi, chúng tôi đo phương sai của các ước tính Q (QVariance); norm trung bình của các tham số mạng (ParametersNorm); norm trung bình của các giá trị Q (QNorm); rank hiệu dụng của ma trận (Srank) như được đề xuất bởi Kumar et al. (2021a), và phần trăm neuron không hoạt động như được định nghĩa bởi Sokar et al. (2023).

Chúng tôi trình bày kết quả của mình trong Hình 11. Điều trở nên rõ ràng từ những hình này là cắt tỉa (i) giảm phương sai, (ii) giảm norm của các tham số, (iii) giảm số lượng neuron không hoạt động, và (iv) tăng rank hiệu dụng của các tham số. Một số quan sát này có thể được quy cho một dạng chuẩn hóa, trong khi những quan sát khác có thể phát sinh do tăng tính dẻo của mạng.

Lyle et al. (2024) chỉ ra rằng norm tham số tăng đi kèm với mất tính dẻo trong các kiến trúc neural khác nhau. Trong Hình 11, chúng tôi quan sát một giá trị norm tham số thấp khi áp dụng cắt tỉa dần dần, điều này biểu diễn một return hiệu suất cuối cùng cao.

5.1. So sánh với các phương pháp khác

Để tách biệt tác động của cắt tỉa khỏi chuẩn hóa và tiêm tính dẻo rõ ràng, chúng tôi so sánh với các phương pháp hiện có trong tài liệu.

Baseline lottery ticket Frankle & Carbin (2018) lập luận rằng các mạng neural chứa các mạng con thưa thớt có thể được huấn luyện ở mức độ thưa thớt cao mà không cần cắt tỉa dần dần; các tác giả cung cấp một thuật toán để tìm những winning tickets này. Sau khi huấn luyện một mạng với cắt tỉa, chúng tôi huấn luyện một mạng mới với mask cuối cùng cố định (tức là không được điều chỉnh trong quá trình huấn luyện) và với các tham số được khởi tạo như trong mạng dày đặc ban đầu. Chúng tôi thấy rằng đề xuất này hoạt động kém hơn cả phương pháp cắt tỉa và baseline chưa cắt tỉa. Thú vị khi quan sát rằng cả phương pháp cắt tỉa và thí nghiệm lottery ticket dường như vẫn đang tiến bộ ở 40M, trong khi baseline dường như bắt đầu xấu đi (Hình 13).

Baseline huấn luyện thưa thớt động Evci et al. (2020) đề xuất RigL như một cơ chế huấn luyện thưa thớt động duy trì một mạng thưa thớt trong suốt toàn bộ quá trình huấn luyện. Trong Phụ lục G.3, chúng tôi đánh giá RigL ở các mức độ thưa thớt khác nhau và thấy rằng, mặc dù hiệu quả, RigL không thể sánh bằng hiệu suất của cắt tỉa độ lớn dần dần; những kết quả này nhất quán với những của Graesser et al. (2022).

Baseline chuẩn hóa Để điều tra vai trò mà chuẩn hóa đóng trong các lợi ích hiệu suất được tạo ra bởi cắt tỉa, chúng tôi xem xét hai loại chuẩn hóa ℓ2 đã được chứng minh hiệu quả trong tài liệu. Thứ nhất là weight decay (WD), một kỹ thuật chuẩn thêm một term phụ vào loss để phạt norm ℓ2 của các trọng số, do đó ngăn cản các tham số mạng phát triển quá lớn. Thứ hai là L2, phương pháp regularization được đề xuất bởi Kumar et al. (2022), được thiết kế để thực thi norm ℓ2 bằng 1 cho các tham số.

Baseline tiêm tính dẻo Chúng tôi so sánh với hai công trình gần đây đề xuất các phương pháp để trực tiếp đối phó với mất tính dẻo. Nikishin et al. (2022) quan sát suy giảm hiệu suất với replay ratio tăng, quy nó cho việc overfitting trên các mẫu sớm, một hiệu ứng họ gọi là "primacy bias". Các tác giả đề xuất đặt lại mạng định kỳ và chứng minh rằng nó rất hiệu quả trong việc giảm thiểu primacy bias, và overfitting nói chung (điều này được gán nhãn là Reset trong kết quả của chúng tôi).

Sokar et al. (2023) chứng minh rằng hầu hết các agent RL sâu đều chịu hiện tượng neuron không hoạt động, theo đó các neuron ngày càng "tắt" trong quá trình huấn luyện các agent RL sâu, do đó giảm khả năng biểu đạt của mạng. Để giảm thiểu điều này, họ đề xuất một phương pháp đơn giản và hiệu quả Recycles Dormant neurons (ReDo) trong suốt quá trình huấn luyện.

Như Hình 12 minh họa, cắt tỉa độ lớn dần dần vượt qua tất cả các phương pháp regularization khác ở tất cả các mức scale, và trong suốt toàn bộ quá trình huấn luyện. Thú vị, hầu hết các phương pháp regularization đều bị suy giảm khi tăng độ rộng mạng. Điều này gợi ý rằng hiệu ứng của cắt tỉa không thể chỉ được quy cho một dạng chuẩn hóa hoặc tiêm tính dẻo. Tuy nhiên, như chúng ta sẽ thấy dưới đây, tăng tính dẻo dường như phát sinh từ việc sử dụng nó. Chúng tôi cung cấp sweeps trên các siêu tham số baseline khác nhau trong Phụ lục G.5 đến G.8.

5.2. Tác động lên tính dẻo

Tính dẻo là khả năng của một mạng neural điều chỉnh nhanh chóng để phản ứng với các phân phối dữ liệu thay đổi (Lyle et al., 2022; 2023; Lewandowski et al., 2023); xét tính không dừng của học tăng cường, việc duy trì nó là rất quan trọng để đảm bảo hiệu suất tốt. Tuy nhiên, các mạng RL được biết là mất tính dẻo trong quá trình huấn luyện (Nikishin et al., 2022; Sokar et al., 2023; Lee et al., 2023).

Lyle et al. (2023) đã tiến hành đánh giá cấu trúc hiệp phương sai của gradients để xem xét landscape loss của mạng, và lập luận rằng hiệu suất cải thiện, và tăng tính dẻo, thường được liên kết với tương quan gradient yếu hơn và giảm nhiễu gradient. Các quan sát của chúng tôi phù hợp với những phát hiện này, như được minh họa trong các bản đồ nhiệt hiệp phương sai gradient trong Hình 14. Trong các mạng dày đặc, gradients thể hiện tính đồng tuyến đáng chú ý, trong khi tính đồng tuyến này được giảm đáng kể trong các mạng đã cắt tỉa.

6. Thảo luận và Kết luận

Công trình trước đây đã chứng minh rằng các agent học tăng cường có xu hướng không khai thác đầy đủ các tham số có sẵn của chúng, và việc không khai thác này tăng lên trong suốt quá trình huấn luyện và được khuếch đại khi kích thước mạng tăng (Graesser et al., 2022; Nikishin et al., 2022; Sokar et al., 2023; Schwarzer et al., 2023). Các agent RL đạt được hiệu suất mạnh trong phần lớn các benchmark đã thiết lập với các mạng nhỏ (tương đối so với những mạng được sử dụng trong các mô hình ngôn ngữ, chẳng hạn), vì vậy việc không hiệu quả tham số rõ ràng này có thể bị bỏ qua như ít quan trọng hơn so với các cân nhắc thuật toán khác.

Khi RL tiếp tục phát triển ngoài các benchmark học thuật và vào các nhiệm vụ phức tạp hơn, nó gần như chắc chắn sẽ cần các mạng lớn hơn và biểu đạt hơn. Trong trường hợp này, hiệu quả tham số trở nên quan trọng để tránh sụp đổ hiệu suất mà các công trình trước đã chỉ ra, cũng như để giảm chi phí tính toán (Ceron & Castro, 2021).

Công trình của chúng tôi cung cấp bằng chứng thuyết phục rằng các kỹ thuật huấn luyện thưa thớt như cắt tỉa độ lớn dần dần có thể hiệu quả trong việc tối đa hóa việc sử dụng mạng, đặc biệt khi các mạng ban đầu được mở rộng (xem Hình 1 và 4). Kết quả trong Hình 8, 7, và 10 đều chứng minh rằng các mạng thưa thớt được tạo ra bởi cắt tỉa tốt hơn trong việc duy trì hiệu suất ổn định khi được huấn luyện lâu hơn. Lợi thế của cắt tỉa vẫn tồn tại ngay cả khi sweeping trên các siêu tham số baseline khác nhau (xem Phụ lục G.4 và G.9 đến G.11). Đáng chú ý rằng hiệu suất của các baseline dày đặc cải thiện khi điều chỉnh tỷ lệ học dựa trên hệ số width multiplier (Phụ lục G.9); tuy nhiên, cắt tỉa vẫn là hiệu suất nhất trong những thiết lập này.

Tổng thể, kết quả của chúng tôi chứng minh rằng, bằng cách loại bỏ có ý nghĩa các tham số mạng trong suốt quá trình huấn luyện, chúng ta có thể vượt qua các đối tác dày đặc truyền thống và tiếp tục cải thiện hiệu suất khi chúng ta phát triển các kiến trúc mạng ban đầu. Kết quả của chúng tôi với các agent và chế độ huấn luyện đa dạng ngụ ý rằng cắt tỉa độ lớn dần dần là một kỹ thuật hữu ích tổng quát có thể được sử dụng như một "drop-in" để tối đa hóa hiệu suất agent.

Công trình tương lai Sẽ tự nhiên để khám phá việc kết hợp cắt tỉa độ lớn dần dần vào các agent gần đây được thiết kế cho khái quát hóa đa nhiệm vụ (Taiga et al., 2023; Kumar et al., 2022), hiệu quả mẫu (Schwarzer et al., 2023; D'Oro et al., 2023), và khả năng khái quát hóa (Hafner et al., 2023). Hơn nữa, tính ổn định quan sát được của các mạng đã cắt tỉa có thể có ý nghĩa đối với các phương pháp dựa vào fine-tuning hoặc reincarnation (Agarwal et al., 2022).

Những tiến bộ gần đây trong các bộ tăng tốc phần cứng để huấn luyện các mạng thưa thớt có thể dẫn đến thời gian huấn luyện nhanh hơn, và phục vụ như một động lực cho nghiên cứu thêm về các phương pháp huấn luyện mạng thưa thớt. Hơn nữa, thực tế là một hệ quả của phương pháp này là một mạng với ít tham số hơn so với khi khởi tạo làm cho nó hấp dẫn cho các ứng dụng downstream trên các thiết bị edge.

Ít nhất, chúng tôi hy vọng công trình này phục vụ như một lời mời khám phá các kiến trúc và topology mạng không chuẩn như một cơ chế hiệu quả để tối đa hóa hiệu suất của các agent học tăng cường. Các agent học tăng cường thường sử dụng các mạng ban đầu được thiết kế cho các vấn đề dừng; do đó, các topology khác có thể phù hợp hơn với bản chất không dừng của RL.

Lời cảm ơn

Các tác giả muốn cảm ơn Laura Graesser, Utku Evci, Gopeshh Subbaraj, Evgenii Nikishin, Hugo Larochelle, Ayoub Echchahed, Zhixuan Lin và phần còn lại của team Google DeepMind Montreal cho các cuộc thảo luận có giá trị trong quá trình chuẩn bị công trình này.

Laura Graesser xứng đáng được đề cập đặc biệt vì đã cung cấp cho chúng tôi phản hồi có giá trị về bản thảo sớm của bài báo. Chúng tôi cảm ơn các reviewer ẩn danh vì sự giúp đỡ có giá trị trong việc cải thiện bản thảo của chúng tôi. Chúng tôi cũng muốn cảm ơn cộng đồng Python (Van Rossum & Drake Jr, 1995; Oliphant, 2007) vì đã phát triển các công cụ cho phép công trình này, bao gồm NumPy (Harris et al., 2020), Matplotlib (Hunter, 2007), Jupyter (Kluyver et al., 2016), Pandas (McKinney, 2013) và JAX (Bradbury et al., 2018).
