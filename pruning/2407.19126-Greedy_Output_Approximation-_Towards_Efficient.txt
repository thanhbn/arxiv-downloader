# 2407.19126.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2407.19126.pdf
# File size: 1498823 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Greedy Output Approximation: Towards Efficient
Structured Pruning for LLMs Without Retraining
Jianwei Li∗Yijun Dong†Qi Lei‡
Abstract
To remove redundant components of large language models (LLMs) without in-
curring significant computational costs, this work focuses on single-shot pruning
without a retraining phase. We simplify the pruning process for Transformer-based
LLMs by identifying a depth-2 pruning structure that functions independently.
Additionally, we propose two inference-aware pruning criteria derived from the
optimization perspective of output approximation, which outperforms traditional
training-aware metrics such as gradient and Hessian. We also introduce a two-
step reconstruction technique to mitigate pruning errors without model retraining.
Experimental results demonstrate that our approach significantly reduces computa-
tional costs and hardware requirements while maintaining superior performance
across various datasets and models.
1 Introduction
With the development of LLMs displaying emergent capabilities like sophisticated reasoning, the
focus of the community has shifted to models with billions of parameters, for example, GPT-4 and
Llama2 [ 1,56]. This transition introduces unprecedented computational costs both in the training and
the inference phases [ 55,34,52,5]. To address this challenge, pruning plays a constructive role by
removing redundant components from models, thereby reducing computational costs [ 22,46,59,36].
Notably, designing an optimal pruning strategy is an NP-hard problem (as it reduces to subset
selection) and requires balancing accuracy, sparsity, generalizability, pruning costs, and hardware
compatibility in practice [ 61,37,12]. Traditional pruning methods primarily focus on accuracy and
sparsity, often neglecting other key factors. They typically involve model retraining and knowledge
distillation to mitigate pruning errors. However, with current LLMs featuring billions of parameters,
the training process is already a significant challenge, making the additional cost of model retraining
even more unaffordable [ 18,19]. Given these challenges, there’s a pressing need for more efficient
pruning approaches.
Recently, some works have focused on structured pruning on pre-trained LLMs, directly addressing
hardware compatibility and generalizability. This approach allows them to concentrate on the
remaining trade-off factors: sparsity, accuracy, and pruning cost. For example, methods like LLM-
Pruner, Shortened LLaMA, and Sheared LLaMA use a single-shot pruning strategy that requires
only one round of retraining [ 32,60,40]. On the other hand, strategies such as FLAP, OPTIN,
Sliced GPT, LLM Surgeon, Wanda, ZipLM, and KRP seek to eliminate the need for model retraining
entirely [ 2,4,57,51,33,31,37]. However, these approaches have respective limitations, such as
high computational costs from the calculation of higher-order information, a lack of fully structured
pruning patterns [ 45], or compromised performance in some cases. The development of these methods
marks a crucial phase in the evolution of LLMs, aiming to enhance model capabilities while ensuring
computational efficiency.
∗Department of Computer Science at North Carolina State University, Email: jli265@ncsu.edu
†Courant Institute of Mathematical Sciences at New York University, Email: yd1319@nyu.edu
‡Center for Data Science at New York University, Email: ql518@nyu.edu
Preprint.arXiv:2407.19126v1  [cs.AI]  26 Jul 2024

--- PAGE 2 ---
A B CFigure 1: Pruning metric analysis from the optimization perspective A:Function Approximation; B:
Output Approximation; C:Objective Approximation.
With the existing challenges, we call for a pruning strategy that better trades off the accuracy, structure
preservation, and computational costs. We delve into this kind of strategy by answering the following
essential questions:
Question 1. Does an inherent pruning structure exist in Transformer-based language models?
We discovered depth-2 pruning modules within Transformer architecture by analyzing structured
pruning from both input and output channels. These structures preserve feature knowledge while
reducing pruning complexity from residual connections.
Question 2. Is there an effective pruning criterion that does not require training awareness?
We identified two efficient and high-performing inference-aware pruning metrics based on output
approximation for Transformer models. They are simpler and more computationally efficient than
training-aware metrics.
Question 3. Is there a low-computation performance recovery technique available?
Inspired by layer-wise reconstruction [ 37], we developed a two-step module reconstruction strategy.
This method updates the weights of the depth-2 module without computing parameter gradients,
effectively minimizing pruning errors.
Answering the above questions altogether, this paper proposes an integrated and efficient pruning
strategy with a focus on Transformer-based LLMs [ 58]. Specifically, we categorize all pruning
metrics into three groups based on their implicit purpose: function (weights) approximation, output
approximation, and objective approximation. Following the output approximation route, we introduce
a similarity-based pruning strategy that exploits the redundancy in multi-head attention mechanisms
by removing heads that extract similar information first rather than those with minimal impact. Addi-
tionally, we propose a second-moment-based pruning approach also under the output approximation
category, which stands out for its ability to integrate information across multiple layers. We apply
this metric for both attention and feed-forward modules to remove the elements with minimal impact
on the model’s performance. Finally, we develop an optimization technique that eliminates the need
for higher-order information by greedily reducing pruning error through weight reconstruction of
the subsequent dense module. Our structured pruning experiments on pre-trained LLMs ranging
from millions to billions of parameters demonstrate that our method ensures generalizability, hard-
ware compatibility, and minimal pruning cost. Moreover, it outperforms or achieves comparative
performance to other non-retraining methods and even some methods that require retraining.
2 Related Work
Pruning and Structured Pruning: Pruning is a technique used in machine learning to reduce the size
of a model by eliminating unnecessary parameters, which can lead to reductions in storage require-
ments and computational complexity without significantly affecting the model’s performance [ 18,17].
This process involves identifying and removing the parts of the model that have the least impact on its
output, such as weights in a neural network with small magnitudes. By doing so, pruning discovers a
more efficient model that is faster to execute and easier to deploy on devices with limited resources.
Structured pruning, a method that imposes more constraints, focuses on eliminating entire units or
structures within the model, such as neurons, channels, or layers, instead of individual weights [ 3,15].
Being the focus of our paper, structured pruning is especially advantageous due to its compatibility
with standard hardware, whereas unstructured pruning requires specially designed accelerators to
deploy in practical scenarios.
Data-free/dependent and Training/Inference-aware Metrics: When choosing which redundant
components to remove from a model, the selection is typically guided by specific metrics [ 28].
2

--- PAGE 3 ---
These metrics can be broadly divided into data-free and data-dependent categories, depending on
whether they rely on specific datasets. Additionally, they can be categorized as training-aware or
inference-aware, based on whether they require model backpropagation. This paper focuses on
inference-aware metrics and explores both data-free and data-dependent versions.
Efficient and Low-Resource Pruning: As the number of parameters in LLMs increases, the quest
for efficient pruning has become paramount. Methods such as LLM-Pruner, Sheared LLaMA, and
Shortened LLaMA adopt a single-shot pruning strategy [ 32,60,40]. Yet, these approaches depend
on computationally expensive metrics and still require retraining to minimize pruning-induced errors.
In contrast, methods like OPTIN, Sliced GPT, LLM Surgeo, ZipLM, and KRP eliminate the need
for retraining but still rely heavily on computationally expensive second-order Hessian information,
which is a significant drawback for large-dimensional models [ 4,51,33,31,37]. Meanwhile, FLAP
and Wanda design specific pruning metrics that share similar ideas with the methods from the pre-
deep learning age [14, 49, 13, 54], significantly reducing computational demand [2, 57]. This paper
proposes a method that eliminates the need for both model retraining and computationally expensive
metrics, offering superior or comparative performance compared to other non-retraining methods and
even some methods that require retraining.
3 Methodology
This section outlines our structured pruning scheme, which consists of three key components: pruning
structure recognition, pruning criteria definition, and post-pruning recovery strategy.
3.1 Pruning Structure Recognition
Our approach involves single-shot pruning and targets structured components, such as entire rows or
columns of weight matrices, rather than individual weights. We do not discuss layer or block pruning,
as it disrupts inherent model correlations and requires retraining to restore layer dependencies.
3.1.1 Input or Output Channel Pruning for Sequential Layers
To clarify structured pruning, it is important to understand that pruning neurons can be approached
in two directions: input channels and output channels. Consider a linear function f(X) =XW ,
where X∈Rdinis the input and W∈Rdin×doutis the weight matrix. When we prune neurons,
we typically refer to pruning the output channels of W, since the number of neurons generally
corresponds to the number of output channels in each layer. After pruning, the weight matrix becomes
ˆW∈Rdin×d′
out, where d′
out< dout. Alternatively, pruning the input channels of Wequates to
pruning the input X, also known as feature selection. This paper focuses on a static approach to
feature selection, where the same channels are removed for all samples, making feature selection
equivalent to output channel pruning in the previous layer. An interesting phenomenon arises: in
depth-2 sequential linear layers, pruning the input channels of the second layer simultaneously
pruning the output channels of the first layer, using identical pruning indices. In contrast, pruning
the output channels of the second layer does not affect the first layer. Both input and output channel
pruning contribute to model compression, but they may have different impacts on performance.
3.1.2 Input or Output Channel Pruning for Transformer
Depth-2 Module Identification: In Transformer-based language models, the attention and feed-
forward modules function as sequential layers with a depth of 2. For the attention module, the
first level includes the weight matrices WQ,WK, and WV, which operate in parallel. Pruning the
input channels of any one of these matrices does not affect the output channels of the others. The
second level consists of the weight matrix WO. The symbols WQ,WK,WV, andWOrepresent the
weights for the query, key, value, and output in the attention block, respectively. The feed-forward
module follows a similar structure: the upward projection and gated projection occur at the first level,
while the downward projection occurs at the second level. These depth-2 modules have a unique
characteristic: when pruning the input channels of layers at the second level, the output channel
indices of the first-level layers must correspondingly match. This ensures that the structural integrity
of the model is maintained while pruning.
3

--- PAGE 4 ---
Depth-2 Module
Depth-2 Module
Inner Channel PruningIntra Channel PruningFFD
Attention++
FFD
++Attention+
Attention Depth-2 ModuleLevel-1 LayersLevel-2 Layers
FFD Depth-2 ModuleLevel-1 LayersLevel-2 Layers
A
B
Figure 2: Pruning structure recognition. A: Two pruning strategies for the depth-2 module. B: Depth-2
modules identification in Transformer-based LLMs.
Pruning Strategies for Depth-2 Modules: Given these depth-2 modules, we can employ two
pruning strategies to achieve the same compression ratio. The first strategy involves pruning the
output channels of the layers in the first level while concurrently pruning the input channels of the
layers in the second level. This approach ensures that the dependencies outside the module remain
invariant. The second strategy involves pruning the output channels and the initial input Xto the
entire depth-2 module. In the context of the Transformer architecture, which consists of multiple
such modules in sequence, pruning the initial input Xis effectively equivalent to pruning the output
channels of a preceding module in the sequence. As this dependency propagates backward through
the layers, it ultimately affects the model’s embedding layer, meaning we are directly pruning the
channels of the token embeddings.
Challenge of Residual Connection : Without considering the loss of tokens’ semantic informa-
tion, the two pruning strategies described above should not differ significantly. However, residual
connections impose substantial constraints on the second strategy. In the Transformer architecture,
every depth-2 module determines residual connections. This means that the pruned channels must
be strictly aligned across all modules. If the pruned indices of one of them do not align with others,
it could lead to an unpredictable loss of information. This constraint severely limits the choice of
channels for pruning and could significantly decrease performance. In contrast, the first strategy
maintains a fixed number of output channels across these modules, avoiding this limitation. Each
module can independently select which internal channels to prune based on its needs, resulting in a
larger search space for optimization. This paper will focus on the first pruning strategy.
Additional Structure for Attention Mechanism : The intricate topology of the attention block
introduces an additional constraint: pruning must be conducted at the level of entire heads, encom-
passing continuous portions of the channels. Fortunately, given the design philosophy of multi-head
attention—that each head is designed to capture correlations between tokens independently—this
setup easily leads to redundancy, making it highly amenable to similarity analysis.
3.2 Pruning Criteria Selection
This section begins by examining different pruning criteria from an optimization perspective. Then,
we introduce two specific pruning metrics for the aforementioned depth-2 modules. Finally, an
intuitive magnitude-based pruning method is employed to remove the least important channels.
3.2.1 Pruning Metric Analysis from an Optimization Perspective
Previous work has categorized pruning metrics based on their relationship with data, as discussed
in Section 2. Diverging from these approaches, we analyze these metrics from an optimization
perspective and describe in Fig. 1. Specifically, for a linear operation f(X) =XW , our goal is to
prune Wwhile preserving the accuracy f(X)≈Y. To minimize pruning error, we can approximate
W,f(X), and(f(X), Y). We term these strategies as function approximation, output approximation,
and objective approximation, respectively. Function approximation focuses on directly approximating
W, which is equivalent to approximating the function itself. Typical metrics in this category include
the L1 and L2 norms of weights or neurons. Output approximation seeks to approximate the result
4

--- PAGE 5 ---
ofXW . Known metrics in this category include contribution energy, the sensitivity of f(X)to
deviations in X, and the variance or similarity score of f(X). Objective approximation aims to
directly approximate accuracy. This category encompasses metrics such as first-order or second-order
information and regularization scores. However, this type of metric is computationally expensive as
the optimization process involves backward propagation and calculation of the Hessian Matrix. By
analyzing these strategies, this paper proposes two new metrics to guide the pruning of LLMs.
3.2.2 Similarity-based Metric for Attention Block
B
A
Figure 3: Similarity visualization of at-
tention heads in A: block 4 and B: block
5 for Llama-7B. Heads with divergence
less than τ= 0.20are connected.Previous research on pruning attention heads typically in-
volves removing heads with the lowest importance scores.
Surprisingly, our experiments indicate that random prun-
ing also yields competitive results compared to magnitude-
based pruning, especially when the pruning ratio is be-
low 50%. Further experimentation with different random
seeds, leading to various head indices for pruning, con-
sistently produces comparable results. Notably, nearly
all heads have been selected for removal at some point
during this process, suggesting a potential oversight in
our initial understanding. Recall that different attention
heads are intended to independently capture correlations
between tokens. Thus, it’s common for similar information
to be extracted across different heads. This observation
prompted us to reconsider our strategy: we prioritize re-
moving similar heads before eliminating those with the
least importance score. By identifying and pruning heads
that capture redundant information, we can optimize the
model more effectively while preserving its performance.
Previous studies have conducted similarity analysis be-
tween neurons [ 13,49,14], examining the output differ-
ences across multiple samples to identify similar compo-
nents. The redundant neurons are then removed, and the
remaining neurons scale their weights or biases to mini-
mize the impact of this removal. However, these methods
are primarily effective in smaller neural networks, as the
scaling technique struggles to handle the accumulated error across numerous layers. Fortunately,
due to the parallelism and independence of attention heads, removing redundant heads does not
lead to significant information loss that affects subsequent layers, thus eliminating the need for
costly remedial operations. Based on this observation, we define a pairwise head divergence matrix
D∈Rh×hfor each attention module, where hrefers to the number of heads. Specifically, given an
attention score matrix Attn∈RN×h×s×s, where Nrepresents the number of samples and sis the
sequence length, let Pi∈RN×s×sandQj∈RN×s×sdenote the attention scores of heads hiand
hj, respectively. Then D(Pi∥Qj)is calculated as:
D(Pi∥Qj) =1
N×sN×sX
n=1r
1
2DKL(P(n)
i∥M(n)
ij) +1
2DKL(Q(n)
j∥M(n)
ij) (1)
where M(n)
ij=1
2(P(n)
i+Q(n)
j),DKLdenotes the Kullback-Leibler Divergence, and Dijrepresents
the average Jensen-Shannon divergence between heads hiandhjacross the dataset. By visualizing
the attention heads as graph nodes and connecting nodes with a divergence less than a predefined
threshold τvia an edge, we can clearly illustrate the relationships between these heads. Fig. 3
demonstrates that some heads fall into the same group, signaling information redundancy, whereas
others stand alone, highlighting the uniqueness of their information. We also observe that specific
layers form large groups, indicating higher redundancy. The details of our pruning strategy for the
attention module are outlined in Algo 1.
3.2.3 Second-moment-based Metric for Depth-2 Module
5

--- PAGE 6 ---
Algorithm 1 Attention Heads Pruning.
Input: Pairwise head divergence matrix D∈Rh×h
Input: divergence threshold τ
Output: List of candidate heads for pruning C
Initialize C= []
forrowiandcoljinDdo
ifD[i][j]< τ and(i, j)/∈edges and(j, i)/∈edges and
i̸=jthen
ifi /∈Candj /∈Cthen
C.append (i)
end if
end if
end for
Prune CAlgorithm 2 Pre-Pruning Recovery.
Input: Depth-2 module miwithW1andW2; Input X
Input: Original dense outputs Y1, Y2forX
Input: Preceding pruned modules m1..mi−1
Output: Reconstructed weight ¯W1and¯W2
procedure WEIGHTS RECONSTRUCTION
ˆX1←(mi−1(mi−2..(m1X)))
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
¯W1←(ˆX1ˆX⊤
1)−1ˆX⊤
1Y1
ˆY1←¯W1ˆX1
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
ˆX2←post-process( ˆY1)
¯W2←(ˆX2ˆX⊤
2)−1ˆX⊤
2Y2
end procedure
To prune the identified depth-2 module, we follow the structure mentioned in Sec 3.1, namely pruning
output channels in the first level and input channels in the second level. Since the pruned channel
indexes from these two directions must match, we have to consider them together.
This paper proposes a 2nd-moment-based pruning metric that is simple to calculate and incorporates
information from multiple layers. To facilitate understanding, we use a basic feed-forward module as
an example. Specifically, we describe the module as f(X) =Bσ(AX), where σis the activation
function, AandBrepresent the upward and downward projection weights, respectively. Let X∼
N(0,ΣX)represent the input with zero means. Let Aibe any output channel vector of A, then
A⊤
iX∼ N(0, A⊤
iΣAi). Let Bibe any input channel vector of B, and Bijis a scalar within this
vector. Assuming a linear activation function, the second moment of Yij:=Bijσ(AT
iX)can be
derived as follows:
E[Y2
ij] =E[B2
ijσ(AT
iX)2] =B2
ijE[(AT
iX)(XTAi)] =B2
ij(AT
iE[XXT]Ai) =B2
ij(AT
iΣXAi) (2)
where E[Y2
ij]can determine the contribution of a single weight Bijto the corresponding value of out
channel Bi. However, instead of summing E[Y2
ij]along the input channels direction for all Bij, we
sum along the output channels direction. In this way, we obtain a 2nd moment value corresponding to
a specific index of the inner channel (output channel of A and input channel of B). We further embed
jdirectly into the formula, then the importance score for inner channel iis:
Mi=X
jE[Y2
ij] =∥Bi∥2
2E[(AT
iX)(XTAi)] =∥Bi∥2
2(AT
iΣXAi) (3)
When the activation function is ReLU, we add a coefficient of 1/2 to the above equation. For GeLU or
SiLU, our observations indicate that only a small portion of values fall within the non-linear region;
therefore, we treat them as equivalent to ReLU. This approach offers more valuable information
from the covariance matrix compared to methods based on output energy (first moment) [ 24,25,30].
Unlike some statistical methods that require calibration datasets to collect feature values and then
calculate statistical properties, our method can flexibly integrate information from both input and
output channels, whereas those methods are limited to focusing only on output channels. When there
is no way to estimate ΣX, our method naturally degenerates to a function approximation method
by assuming ΣXis the identity matrix. More details about calculating Mifor attention and the
feed-forward modules can be found in the Appendix.
3.3 Pre-Pruning Recovery Without Retraining
With the selection of the pruning structure and criteria, this paper proposes a module-wise pruning
approach. Similar to layer-wise pruning, we prune these depth-2 modules sequentially. To prune
one of them, we calculate importance scores for its inner channels based on the module’s structure,
weights, and inputs. Notably, due to errors introduced by pruning preceding modules, the input to
the current module inevitably deviates from its dense version. Consequently, even without pruning
the current module, a discrepancy between its output and the original output is unavoidable. Recall
that our design philosophy is to approximate the output as closely as possible. Thus, it is crucial to
reconstruct the weights of the current module before pruning. This reconstruction ensures that the
output of this module can still align as closely as possible with the original, even with the new input.
This way, the pruning criteria for each channel can be optimally up-to-date. Inspired by [ 37], this
paper presents a pre-pruning recovery technique in Algo 2.
6

--- PAGE 7 ---
Table 1: The zero-shot performance of the compressed LLaMA-7B (20% sparsity). Following the
LLM-Pruner methodology [ 40], we only prune the transformer blocks from the 4th to the 30th. The
average performance is calculated across seven classification datasets. ’Bold’ indicates the best
pruning-only performance, while ’underline’ represents the overall best performance.
Pruning Methods WikiText2 ↓ PTB↓ BoolQ PIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Ave↑
Dense [55, 40] 12.62 22.14 73.18 78.35 72.99 67.01 67.45 41.38 42.4 63.5
Data Free Pruning
Random [28] 23.02 40.19 46.21 71.33 59.35 56.51 47.97 32.0 36.30 49.95
L1 norm [28] 179.02 311.75 51.28 60.22 43.14 52.01 36.53 27.89 30.8 43.12
L2 norm [28] 582.41 1022.17 60.18 58.54 37.04 53.27 32.91 27.56 29.8 42.76
Ours (Self-Gen) 21.76 34.3 63.51 72.63 56.54 54.46 51.68 33.79 36.4 52.72
Ours SG w/ remedy 20.32 33.42 64.17 72.67 58.43 57.29 53.32 34.15 37.23 53.89
Data Dependent Pruning
Training-Aware Pruning
LLM-Pruner Vec [40] 22.28 41.78 61.44 71.71 57.27 54.22 55.77 33.96 38.4 53.52
LLM-Pruner E1 [40] 19.09 34.21 57.06 75.68 66.8 59.83 60.94 36.52 40.0 56.69
LLM-Pruner E2 [40] 19.77 36.66 59.39 75.57 65.34 61.33 59.18 37.12 39.8 56.82
Inference-Aware Pruning
Wanda-sp [51] 27.45 49.52 64.16 75.21 68.62 62.27 59.68 36.68 39.2 57.97
Ours (Calibration) 17.48 30.04 66.48 75.78 67.73 62.27 61.4 35.49 39.6 58.39
Ours C w/ remedy 17.90 31.23 70.12 76.86 68.55 65.76 64.23 38.54 40.5 60.65
Retraining-required Pruning
LLM-P. LoRA [40] 17.37 30.39 69.54 76.44 68.11 65.11 63.43 37.88 40.0 60.07
With this technique, we can mitigate the errors accumulated from previously pruned modules without
requiring model retraining. Unlike previous work, which primarily focuses on a single layer, our
approach targets more complex structures, including intricate layer dependencies. We provide more
details on applying this method to the attention and feed-forward modules in the Appendix.
4 Experiment
This section initially presents the fundamental setup for our experiments. Subsequently, we demon-
strate the results of experiments and provide an in-depth analysis from multiple perspectives.
4.1 Setup
Baselines: This paper presents a comprehensive comparison of state-of-the-art pruning methods
across multiple dimensions, aiming for fair evaluations and in-depth analyses to uncover the reasons
behind the observed results. First, we compare our approach with data-free pruning methods,
including random pruning and magnitude-based pruning (L1 and L2 norms) [ 28]. Next, we evaluate
our methods against data-dependent pruning techniques, encompassing training-aware, inference-
aware, and retraining-required methods. In the training-aware category, we compare with various
configurations of LLM-Pruner [ 40], such as Element1, Element2, and Vector-wise magnitude pruning.
Within the inference-aware category, we compare with the structured version of Wanda [ 51] and
FLAP [ 2]. Additionally, we extend our comparisons to include the LLM-Pruner method augmented
with retraining. Such comprehensive evaluations will demonstrate the effectiveness of our pruning
approach.
Models: Our primary experiments are categorized into two series based on the model scale: LLaMA-
7B with 7 billion parameters and GPT-2 with 110 million parameters [ 47,55]. This aligns with
our study’s goal to assess pruning performance across different model sizes and ensure a thorough
examination. Additionally, we extend our experiments to other models, including LLaMA-13B,
Vicuna-7B [ 7]. This comprehensive selection allows us to explore a broader spectrum of capabili-
ties and sizes, enhancing our understanding of how different architectures perform under various
computational constraints. Additional experiment results can be found in the Appendix.
Evaluation and Datasets: To evaluate performance, we adopt LLaMa’s approach by conducting
zero-shot task classification on a range of common sense reasoning datasets: BoolQ [ 8], PIQA [ 6],
HellaSwag [ 63], WinoGrande [ 48], ARC-easy [ 9], ARC-challenge [ 9], and OpenbookQA [ 43].
Following the methodology in [ 21], the model either ranks the options in multiple-choice tasks or
generates answers in open-ended formats. Additionally, we enhance our evaluation by conducting a
zero-shot perplexity (PPL) analysis on WikiText2 [42] and the Penn Treebank (PTB) [41].
7

--- PAGE 8 ---
Implementation: During the pruning phase, we randomly select 16 samples from Wikitext2 and
Bookcorpus, truncated to a sequence length of 128 for LLaMA-7B and 1024 for GPT-2. These
samples serve as calibration data for pruning metric calculation and covariance matrix extraction, re-
spectively. During the recovery phase, we sample an additional 1,024 examples from the downstream
dataset to guide optimization in the data-dependent comparison experiments.
4.2 Results and Analysis
Table 2: Similarity-based analysis for
LLaMA-7B attention heads pruning (all
blocks) with different τ. ’Bold’ indicates
the best performance.
Methods # pruned heads Wiki2↓ PTB↓
Dense 0 12.62 22.14
Ours (τ= 0.16) 88 12.96 22.45
Random 64 14.50 24.13
L2 Norm 64 14.69 25.64
1st+2nd order 64 13.45 24.19
FLAP 88 12.90 22.67
Ours (τ= 0.19) 204 14.69 24.32
Random 192 18.75 35.73
L2 Norm 192 195.84 371.65
1st+2nd order 192 14.81 28.77
FLAP 204 13.22 24.42We present the main results in Tab. 1. For the data-free
comparison experiments, we leverage the inherent ability
of LLMs to generate sentences. Our pruning method uses
these generated sentences as calibration data because,
given that the LLMs are well-trained, these sentences
naturally conform to the semantic and syntactic token
distributions of the training data. Compared to traditional
data-free metrics (L1 or L2), our data-free version, which
relies solely on the model itself, achieves significant im-
provements in perplexity and up to a 20% enhancement
in zero-shot evaluation for downstream tasks. Moreover,
our method surpasses random pruning by at least 6%,
a significant improvement achieved without relying on
existing datasets, while traditional metrics (L1 or L2) fail
to outperform. These results demonstrate the superiority
of our techniques in data-free pruning methods.
A
B
Figure 4: Performance of compressed A:
LLaMA-7B (w/o Remediation) and B:
GPT-2 (w/ Remediation) concerning the
number of calibration samples.Our approach outperforms data-dependent pruning meth-
ods and the inference-only method Wanda-SP. Impres-
sively, it also surpasses the state-of-the-art training-aware
pruning method LLM-Pruner, which includes different
configurations such as Element1, Element2, and Vec-
tor. Our approach consistently demonstrates better prun-
ing results without requiring computationally intensive
first-order and second-order information. Moreover, our
method even achieves better results compared to LLM-
Pruner with LoRA, despite the latter involving model
retraining.
We also compare our method with the state-of-the-art
inference-only method FLAP and present the results in
Tab. 3. Our approach exhibits significantly better results
on the GPT-2 model and achieves comparable perfor-
mance with LLaMA-7B. Overall, our method demon-
strates superior performance in both data-free and data-
dependent pruning categories.
4.3 Ablation Study
We also explore our pruning metrics by exclusively prun-
ing attention heads. The experimental results in Tab. 2 demonstrate that for colossal LLMs like
LLaMA-7B, our similarity analysis effectively identifies redundant attention heads with minimal
negative impact on model performance. Compared to inference-aware metrics such as the L2 norm,
training-aware metrics using first- and second-order information, and random pruning, our similarity-
based metric consistently outperforms. When compared to the specifically designed metric of FLAP,
we achieve better or comparable performance. These results strongly indicate that we should prioritize
pruning redundant information rather than heads with small importance scores.
Additionally, we designed experiments to explore the influence of the number of calibration samples.
Figure 4 shows that in LLaMA-7B pruning-only experiments, our method is insensitive to the number
of calibration samples, achieving comparable results with as few as 8 samples and as many as
128 samples. Conversely, in GPT-2 pruning with remediation experiments, performance improves
8

--- PAGE 9 ---
Table 3: Perplexity of compressed GPT-2 and LLaMA-7B (25% and 50% sparsity) on Wikitext2 and
PTB. We prune the 4th to 30th transformer blocks for LLaMA-7B and all blocks for GPT-2. ’Bold’
indicates the best performance, while ’underline’ represents the second-best performance.
Models GPT-2: [0-12) LLama-7b: [4-30)
Datasets: PPL WikiText2: 29.95 [47]↓ PTB: 40.12 [47]↓ WikiText2: 12.62 [40]↓ PTB: 22.14 [40]↓
Sparsity 25% 50% 25% 50% 25% 50% 25% 50%
Data Free Pruning
Random [28] 189.73 1839.33 245.33 2769.6 23.02 100.42 40.19 133.56
L1 norm [28] 338.3 1226.13 583.2 1290.45 179.02 891.23 311.75 1034.69
L2 norm [28] 227.32 674.52 324.33 800.14 582.41 14000.68 1022.17 28062.45
Ours (Self-Generation w/ remedy) 119.29 586.87 152.93 723.39 21.76 58.61 34.3 64.24
Data Dependent Pruning
Training-Aware Pruning
LLM-Pruner Element1 [40] 9229.32 32453.23 11993.24 8020.87 19.09 48.84 34.21 105.24
LLM-Pruner Element2 [40] 1897.32 14706.23 2258.33 18598.33 19.77 72.89 36.66 138.33
LLM-Pruner Vector [40] 488.32 39025.12 6169.56 18616.87 22.88 55.68 41.76 305.24
Inference-Aware Pruning
Wanda-Structured Pruning [51] 586.34 4147.32 355.17 3246.79 27.45 69.02 49.52 132.52
FLAP UL-UM w/o remed [2] 818.14 3636.23 554.32 2758.37 17.15 36.08 34.96 85.22
FLAP UL-UM w/ remedy [2] 2197.32 3043.35 2199.24 3561.76 15.76 26.87 32.1 66.18
Ours UL-UM (Calibration w/o remedy) 81.96 317.37 186.68 936.57 NA NA NA NA
FLAP AL-AM w/o remedy [2] 126.57 5538.32 135.07 10244.95 17.01 34.09 30.99 71.76
FLAP AL-AM w/ remedy [2] 1349.25 5382.14 1769.56 7476.08 15.06 26.55 29.45 57.89
Ours ML-MM (Calibration w/o remedy) 79.4 251.34 130.54 756.33 17.48 26.87 30.04 57.89
with an increasing number of calibration samples. These findings demonstrate that our pruning
method is robust regardless of the number of calibration samples, while our pre-pruning recovery
method benefits from a higher number of calibration samples. However, this improvement gradually
diminishes once the number of samples reaches a critical threshold.
5 Discussion, Limitation, and Conclusion
Implicit Motivation and Call: In the pre-deep learning era, various pruning metrics and structures
were designed. For example, variance-based pruning and bias-based remedy methods similar to
FLAP were proposed by researchers 30 years ago [ 14,49,13,54]. These early researchers already
recognized that feature information is at least as crucial as model weights in constructing pruning
criteria. In the early stages of deep learning (before 2022), many researchers found that multi-round
model retraining could easily recover the lost performance induced by pruning, even when based
solely on weight magnitudes. As a result, the importance of pruning metrics and structure design was
often overlooked, with reliance placed on retraining to validate methods. However, this paradigm
shifted after 2022, when colossal LLMs became mainstream in the community. Training such models
is prohibitively expensive, making pruning that relies on multi-round retraining impractical. Although
parameter-efficient training methods like LoRA can reduce costs, they still require rigorous data
selection [ 29,40]. Thus, we urge the community to return to designing metrics that better account
for the influence of both weights and features, rather than focusing solely on dataset competition.
Motivated by this, this paper focuses on inference-aware pruning metrics that do not require retraining.
Limitation: This work evaluates the compressed LLMs primarily on perplexity and downstream
tasks. However, we do not assess the emergent abilities of colossal LLMs, such as mathematical
reasoning, safety alignment, common sense reasoning, contextual understanding, and creativity in
text generation. Future research will focus on evaluating and enhancing these emergent abilities to
provide a more comprehensive understanding of the compressed LLMs.
Conclusion: This paper introduces a novel approach to pruning large language models (LLMs) by
identifying a depth-2 pruning structure and developing two inference-aware pruning criteria. These
methods surpass traditional metrics and eliminate the need for computationally expensive retraining.
Our two-step reconstruction technique further mitigates pruning errors, ensuring superior performance
across various datasets and models. Overall, our approach significantly reduces computational costs
and hardware requirements, offering an efficient solution for pruning colossal LLMs.
9

--- PAGE 10 ---
References
[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
technical report. arXiv preprint arXiv:2303.08774 , 2023.
[2]Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. Fluctuation-based adaptive
structured pruning for large language models. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 38, pages 10865–10873, 2024.
[3]Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional
neural networks. ACM Journal on Emerging Technologies in Computing Systems (JETC) , 13
(3):1–18, 2017.
[4]Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and
James Hensman. Slicegpt: Compress large language models by deleting rows and columns.
arXiv preprint arXiv:2401.15024 , 2024.
[5]Tris Warkentin Jeanine Banks and Tris Warkentin. Gemma: Introducing new state-of-the-art
open models, 2024.
[6]Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-
ical commonsense in natural language. In Proceedings of the AAAI conference on artificial
intelligence , volume 34, pages 7432–7439, 2020.
[7]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot
impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April
2023) , 2(3):6, 2023.
[8]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.
arXiv preprint arXiv:1905.10044 , 2019.
[9]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
challenge. arXiv preprint arXiv:1803.05457 , 2018.
[10] Xavier Suau Cuadros, Luca Zappella, and Nicholas Apostoloff. Filter distillation for network
compression. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision , pages 3140–3149, 2020.
[11] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise
optimal brain surgeon. Advances in Neural Information Processing Systems , 30, 2017.
[12] Mengnan Du, Subhabrata Mukherjee, Yu Cheng, Milad Shokouhi, Xia Hu, and Ahmed Hassan
Awadallah. Robustness challenges in model distillation and pruning for natural language under-
standing. In Proceedings of the 17th Conference of the European Chapter of the Association for
Computational Linguistics , pages 1766–1778, Dubrovnik, Croatia, May 2023. Association for
Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.129 .
[13] Andries P Engelbrecht and Ian Cloete. A sensitivity analysis algorithm for pruning feedforward
neural networks. In Proceedings of International Conference on Neural Networks (ICNN’96) ,
volume 2, pages 1274–1278. IEEE, 1996.
[14] Andries P Engelbrecht, L Fletcher, and Ian Cloete. Variance analysis of sensitivity information
for pruning multilayer feedforward neural networks. In IJCNN’99. International Joint Confer-
ence on Neural Networks. Proceedings (Cat. No. 99CH36339) , volume 3, pages 1829–1833.
IEEE, 1999.
[15] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph:
Towards any structural pruning. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 16091–16101, 2023.
10

--- PAGE 11 ---
[16] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In 7th International Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019. URL https://openreview.
net/forum?id=rJl-b3RcF7 .
[17] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode
connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning ,
pages 3259–3269. PMLR, 2020.
[18] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-
training quantization and pruning. Advances in Neural Information Processing Systems , 35:
4475–4488, 2022.
[19] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned
in one-shot. In International Conference on Machine Learning , pages 10323–10337. PMLR,
2023.
[20] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. ArXiv ,
abs/1902.09574, 2019.
[21] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot
language model evaluation. Version v0. 0.1. Sept , page 8, 2021.
[22] Mitchell Gordon, Kevin Duh, and Nicholas Andrews. Compressing BERT: Studying the effects
of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation
Learning for NLP , pages 143–155, Online, July 2020. Association for Computational Lin-
guistics. doi: 10.18653/v1/2020.repl4nlp-1.18. URL https://aclanthology.org/2020.
repl4nlp-1.18 .
[23] M. Hagiwara. Removal of hidden units and weights for back propagation networks. In
Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan) ,
volume 1, pages 351–354 vol.1, 1993. doi: 10.1109/IJCNN.1993.713929.
[24] Masafumi Hagiwara. Removal of hidden units and weights for back propagation networks. In
Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan) ,
volume 1, pages 351–354. IEEE, 1993.
[25] Masafumi Hagiwara. A simple and effective method for removal of hidden units and weights.
Neurocomputing , 6(2):207–218, 1994.
[26] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 ,
2015.
[27] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric
median for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , pages 4340–4349, 2019.
[28] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity
in deep learning: Pruning and growth for efficient inference and training in neural networks.
Journal of Machine Learning Research , 22(241):1–124, 2021.
[29] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021.
[30] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven
neuron pruning approach towards efficient deep architectures. arXiv preprint arXiv:1607.03250 ,
2016.
[31] Samir Khaki and Konstantinos N Plataniotis. The need for speed: Pruning transformers with
one recipe. arXiv preprint arXiv:2403.17921 , 2024.
11

--- PAGE 12 ---
[32] Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin,
and Hyoung-Kyu Song. Shortened llama: A simple depth pruning for large language models.
arXiv preprint arXiv:2402.02834 , 2024.
[33] Eldar Kurti ´c, Elias Frantar, and Dan Alistarh. Ziplm: Inference-aware structured pruning of
language models. Advances in Neural Information Processing Systems , 36, 2024.
[34] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow,
Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A
176b-parameter open-access multilingual language model. arXiv preprint , 2023.
[35] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network
pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.
[36] Jianwei Li, Weizhi Gao, Qi Lei, and Dongkuan Xu. Breaking through deterministic barriers:
Randomized pruning mask generation and selection. arXiv preprint arXiv:2310.13183 , 2023.
[37] Jianwei Li, Qi Lei, Wei Cheng, and Dongkuan Xu. Towards robust pruning: An adaptive
knowledge-retention pruning strategy for language models. arXiv preprint arXiv:2310.13191 ,
2023.
[38] Mieszko Lis, Maximilian Golub, and Guy Lemieux. Full deep neural network training on a
pruned weight budget. Proceedings of Machine Learning and Systems , 1:252–263, 2019.
[39] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep
neural network compression. In Proceedings of the IEEE international conference on computer
vision , pages 5058–5066, 2017.
[40] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large
language models. Advances in neural information processing systems , 36:21702–21720, 2023.
[41] Mitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.
[42] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843 , 2016.
[43] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789 ,
2018.
[44] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance
estimation for neural network pruning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 11264–11272, 2019.
[45] Jeff Pool, Abhishek Sawarkar, and Jay Rodge. Accelerating inference with sparsity us-
ing the nvidia ampere architecture and nvidia tensorrt. NVIDIA Developer Technical
Blog, https://developer. nvidia. com/blog/accelerating-inference-with-sparsityusing-ampere-
and-tensorrt , 2021.
[46] Sai Prasanna, Anna Rogers, and Anna Rumshisky. When BERT Plays the Lottery, All Tickets
Are Winning. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 3208–3229, Online, November 2020. Association
for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.259. URL https://
aclanthology.org/2020.emnlp-main.259 .
[47] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
[48] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106,
2021.
[49] Sietsma and Dow. Neural net pruning-why and how. In IEEE 1988 international conference on
neural networks , pages 325–333. IEEE, 1988.
12

--- PAGE 13 ---
[50] Sidak Pal Singh and Dan Alistarh. Woodfisher: Efficient second-order approximation for neural
network compression. Advances in Neural Information Processing Systems , 33:18098–18109,
2020.
[51] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning
approach for large language models. arXiv preprint arXiv:2306.11695 , 2023.
[52] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly
capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.
[53] Georg Thimm and Emile Fiesler. Evaluating pruning methods. In Proceedings of the Interna-
tional Symposium on Artificial neural networks , pages 20–25, 1995.
[54] Georg Thimm and Emile Fiesler. Evaluating pruning methods. In Proceedings of the Interna-
tional Symposium on Artificial neural networks , pages 20–25, 1995.
[55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
[56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
[57] Tycho FA van der Ouderaa, Markus Nagel, Mart Van Baalen, Yuki M Asano, and Tijmen
Blankevoort. The llm surgeon. arXiv preprint arXiv:2312.17244 , 2023.
[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30, 2017.
[59] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models.
InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP) , pages 6151–6162, Online, November 2020. Association for Computational Lin-
guistics. doi: 10.18653/v1/2020.emnlp-main.496. URL https://aclanthology.org/2020.
emnlp-main.496 .
[60] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating
language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694 , 2023.
[61] Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian McAuley, and Furu Wei. Be-
yond preserved accuracy: Evaluating loyalty and robustness of BERT compression. In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-
ing, pages 10653–10659, Online and Punta Cana, Dominican Republic, November 2021.
Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.832. URL
https://aclanthology.org/2021.emnlp-main.832 .
[62] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han, Mingfei Gao,
Ching-Yung Lin, and Larry S Davis. Nisp: Pruning networks using neuron importance score
propagation. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 9194–9203, 2018.
[63] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a
machine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.
[64] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for
model compression. arXiv preprint arXiv:1710.01878 , 2017.
13

--- PAGE 14 ---
6 Appendix-A
Pruning is a promising method that can effectively reduce model inference costs. In this paper, we
discuss general pruning methods and various classification philosophies. We summarize previous
work and categorize pruning from multiple perspectives: structured and unstructured, data-free and
data-dependent, training-aware and inference-aware, and retraining-free and retraining-dependent.
We also propose an innovative optimization-oriented view of pruning, which includes: A:Function
Approximation, B:Output Approximation, and C:Objective Approximation. Our pruning pattern is
designed based on this perspective.
Additionally, we review more aspects of pruning, including the most popular techniques in the
pre-deep learning era (before 2022), such as Iterative Magnitude Pruning and the comparison
between random pruning and magnitude pruning . We also discussed the relationship between
pruning and quantization . By considering these various dimensions and methodologies, we aim to
provide a comprehensive understanding of pruning and its potential to enhance model efficiency.
6.1 Iterative Magnitude Pruning
Iterative Magnitude Pruning (IMP) is the most renowned strategy for achieving state-of-the-art
results, surpassing other methods such as Single-shot Network Pruning (SNIP) [ 16,17,18,35]. This
approach divides the pruning process into multiple stages by gradually increasing the sparsity. At
each stage, the goal is to identify and remove redundant parameters or neurons. The most intuitive
approach is to assign an importance score to each element and keep only the top-k elements, where
the score can be based on the absolute value of weights, output sensitivity, gradients, or other fine-
designed metrics [ 23,20,53,26,64,10,39]. Weight magnitude is the most straightforward and
data-free method, while other metrics can be computationally expensive as they require training
with data [ 62,27,38,44,50,11]. Moreover, IMP is accompanied by a retraining phase to restore
performance, which can be computationally costly. Therefore, in the era of colossal LLMs, IMP and
other methods that heavily depend on model retraining are no longer effective due to the immense
costs involved.
6.2 Randomized Pruning v.s. Magnitude Pruning
Excluding the influence of model retraining, we discovered an interesting phenomenon for model
pruning. For colossal LLMs such as LLaMA-7B, randomized pruning surprisingly produced com-
petitive results. Specifically, compared to traditional data-free pruning metrics like L1 and L2 norm
values, randomized pruning achieved several times better results, even rivaling data-dependent prun-
ing methods. However, this advantage only existed when the pruning ratio was less than 2x. As the
pruning ratio increased, magnitude pruning gradually yielded better results. Initially, we attributed
this phenomenon to the high redundancy of parameters in LLMs. However, our experiments with
GPT-2 showed that randomized pruning was still weaker than magnitude pruning. Therefore, we
speculate that for colossal LLMs like Llama-7B, feature information plays a more crucial role
in model activations compared to smaller LLMs like GPT-2.
Magnitude-based pruning methods aim to remove weights or neurons from a neural network that
appear least influential, primarily determined by the value of their weights. The rationale behind
these methods is to reduce overall model size and computational requirements without a drastic loss
in performance. However, several challenges arise with this approach, and one major challenge is the
lack of variety if the magnitude is based on data-free metrics (L1 or L2). This kind of metric focuses
solely on the magnitude of the weights for pruning decisions, potentially missing smaller weights
that play pivotal roles, especially in edge cases or rarer instances.
To illustrate this more clearly, consider the following example. The output of a neural network can
be represented as y=P(wi·fi), where yis the network output, firepresents a feature, and wi
is the corresponding weight. In magnitude-based pruning (L1 or L2), if |wi|< τ (τis pruning
threshold), then wi·fiis pruned. However, the impact on yis not solely determined by wi, but by
the combined effect of wiand the sensitivity of fi. For instance, if firepresents the sharpness of an
image, even a small weight |wi|= 0.01can significantly affect yiffiis highly sensitive, such as
affecting object recognition. Conversely, if firepresents the hue of an image background, a large
weight wi= 5might have minimal impact on yiffiis less sensitive, such as the background hue not
14

--- PAGE 15 ---
altering recognition much. The influence on yis thus a joint effect of wiand the sensitivity of fi.
This example indicates that the influence of feature information plays a significant role in identifying
redundant elements.
Figure 5: Mean activation value of Llama-
7B and GPT-2 on Wikitext2.Based on the above observation, we speculate that
LLaMA-7B’s feature information contributes more to
the importance score of removed elements when the prun-
ing ratio is less than 2x. As the pruning ratio gradually
increases, the influence of the features on the activation
values is no longer greater than that of the weights. There-
fore, randomized pruning fails at larger pruning ratios. To
validate our hypothesis, we conducted a statistical anal-
ysis on the feature values of LLaMA-7B, described in
Figure 5. Our results show that colossal LLMs like Llama-
7B have larger activation values than smaller LLMs like
GPT-2. These findings further motivate us to design
the pruning metrics that incorporate both feature and
weight information instead of seeking dataset compe-
tition.
6.3 Pruning v.s. Quantization:
Pruning, though considered less effective than quantization in the era of colossal LLMs, should
not be underestimated. In practice, pruning and quantization can complement each other, yielding
significant benefits when applied together [ 18]. Even pruning a small percentage of parameters, such
as 5%, can be valuable if it meets practical performance requirements. Therefore, integrating pruning
into the optimization process is always worthwhile.
7 Appendix-B
In Section 3.1.2, we introduced the 2nd moment-based pruning metric for a standard depth-2 module.
However, there are different variants of depth-2 modules, including the attention module and the gated
feed-forward module. We describe the calculation of the metric for these variants in the following
section.
Notations: To better demonstrate our method, let us first establish the notations. We focus on the
pruning of Transformer-based large language models, thus we refer to the attention mechanism as
Cath
i=1[σ1(XWK
iWQ
iX⊤)XWV
i]WO, with iindicating the attention head index. The symbols
WK,WQ,WVandWOrepresent the weights for the key, query, value, and output in the atten-
tion block, respectively. For the general and gated feed-forward module, we denote the logic as
WDσ2(WUX)andWD(WUX·σ2(WGX)). Here, WU,WD, andWGstand for the weights for
upward projection, downward projection, and gate projection. σrefers to the activation function for
all of them, which can be SoftMax, ReLU, GeLU, or SiLU function.
7.1 Calculation of 2nd-Moment-Based Metric for Attention Module
Based on the above notations, we can treat the entire output of σ1(XWiKWiQX⊤)Xas the input
toWV
i. Let ˆXrepresent this new input. In this way, the attention module can be viewed as a module
similar to a standard depth-2 module (ˆXWV
i)WO
i, with each level having only one linear layer.
However, we need to view an attention module as mstandard depth-2 modules ( mis the number of
attention heads), as the attention heads operate independently.
7.2 Calculation of 2nd-Moment-Based Metric for Gate Feed-forward Module
For the gated feedforward module WD(WUX·σ2(WGX)), we treat it as a product of two standard
depth-2 modules. Specifically, we can divide it into two modules: WD·WUXandWD(σ2(WGX)),
and calculate the 2nd-moment metric separately for them. Finally, we use the product of their own
metric as the 2nd-moment metric for the entire module.
15

--- PAGE 16 ---
Overall, these approaches allow us to effectively prune channels in both attention and gated feedfor-
ward modules by leveraging the 2nd moment-based metric.
Algorithm 3 Pre-Pruning Recovery for Attention Module.
1:Input: Attention ModuleModel layers with weights {k_proj, q _proj, v _proj, o _proj}
2:Input: Corresponding inputs x
3:Input: Corresponding outputs y_k_proj, y _q_proj, y _v_proj, y _o_proj
4:Output: Reconstructed weights {¯Wk_proj,¯Wq_proj,¯Wv_proj,¯Wo_proj}
5:procedure WEIGHTS RECONSTRUCTION
6: xtx←Matmul (x.T, x )
7: ¯Wk_proj←reconstruct_best_weight (xtx, x, y _k_proj, w =Wk_proj)
8: ¯Wq_proj←reconstruct_best_weight (xtx, x, y _q_proj, w =Wq_proj))
9: ¯Wv_proj←reconstruct_best_weight (xtx, x, y _v_proj, w =Wv_proj))
10: restore_layer_weights (module.k_proj ,¯Wk_proj)
11: restore_layer_weights (module.q_proj ,¯Wq_proj)
12: restore_layer_weights (module.v_proj ,¯Wv_proj)
13: # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
14: x2←module (x)# new input for o proj
15: xtx2←Matmul (x2.T, x 2)
16: ¯Wo_proj←reconstruct_best_weight (xtx2, x2, y_o_proj, w =w=Wo_proj)
17: restore_layer_weights (module.o_proj ,¯Wo_proj)
18:end procedure
8 Appendix-C
In Section 3.3, we only provide the pre-pruning recovery algorithm for the standard depth-2 module,
thus we describe the details of the recovery process for the attention module and gated feed-forward
module in Algo 3 and Algo 4, respectively.
Algorithm 4 Pre-Pruning Recovery for Gate Feed-forward Module.
1:Input: Gated Feed-forward Module with weights {up_proj, gate _proj, down _proj}
2:Input: Corresponding inputs x
3:Input: Corresponding outputs y_up_proj, y _gate _proj, y _down _proj
4:Output: Reconstructed weights {¯Wup_proj,¯Wgate _proj,¯Wdown _proj}
5:procedure WEIGHTS RECONSTRUCTION
6: xtx←Matmul (x.T, x )
7: ¯Wup_proj←reconstruct_best_weight (xtx, x, y _up_proj, w =Wup_proj)
8: ¯Wgate _proj←reconstruct_best_weight (xtx, x, y _gate _proj, w =Wgate _proj))
9: restore_layer_weights (module.up_proj ,¯Wup_proj)
10: restore_layer_weights (module.gate_proj ,¯Wgate _proj)
11: # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
12: x2←module (x)# new input for down proj
13: xtx2←Matmul (x2.T, x 2)
14: ¯Wdown _proj←reconstruct_best_weight (xtx2, x2, y_down _proj, w =Wdown _proj)
15: restore_layer_weights (module.down_proj ,¯Wdown _proj)
16:end procedure
9 Appendix-D: Broader Impact
Our proposed method efficiently prunes large language models with billions of parameters. Our pro-
posal intends to mitigate AI risks from critical perspectives like economic inequality and concentration
of power and further democratize the use of AI models.
16

--- PAGE 17 ---
Table 4: The zero-shot performance of the compressed Vicuna-7B (20% sparsity). Following the
LLM-Pruner methodology [ 40], we only prune the transformer blocks from the 4th to the 30th. The
average performance is calculated across seven classification datasets. ’Bold’ indicates the best
pruning-only performance, while ’underline’ represents the overall best performance.
Pruning Methods WikiText2 ↓ PTB↓ BoolQ PIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Ave↑
Dense [55, 40] 16.11 61.37 76.57 77.75 70.64 67.40 65.11 41.21 40.80 62.78
Data Free Pruning
Random [28] 34.63 112.44 61.47 70.89 54.67 56.27 55.60 31.74 34.60 52.18
L2 norm [28] 3339.98 5882.21 55.90 56.15 32.37 51.85 30.01 28.41 28.20 40.41
Ours SG w/o remedy 28.45 92.3 62.51 72.63 56.54 57.46 58.68 33.29 36.2 53.91
Data Dependent Pruning
LLM-Pruner Vec [40] 27.03 92.51 62.17 71.44 55.80 53.43 55.77 33.28 37.80 52.81
LLM-Pruner E2 [40] 24.70 94.34 62.87 75.41 64.00 58.41 60.98 37.12 39.00 56.83
LLM-Pruner E1 [40] 25.74 92.88 61.70 75.30 63.75 56.20 63.22 36.60 37.00 56.25
Ours (C) w/o remedy 19.88 90.04 62.48 75.68 65.23 61.27 63.4 35.49 37.6 57.31
Data Dependent Pruning w/ Retraining
LLM-Pruner LoRA [40] 18.97 76.78 60.40 75.63 65.45 63.22 63.05 37.71 39.00 57.78
Table 5: The zero-shot performance of the compressed Llama-13B (20% sparsity). The average
performance is calculated across seven classification datasets. ’Bold’ indicates the best pruning-only
performance, while ’underline’ represents the overall best performance.
Pruning Methods WikiText2 ↓ PTB↓ BoolQ PIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Ave↑
Dense [55, 40] 11.58 20.24 68.47 78.89 76.24 70.09 74.58 44.54 42.00 64.97
Data Free Pruning
Random [28] 19.24 31.84 63.33 73.18 63.54 60.85 64.44 36.26 38.00 57.09
L2 norm [28] 61.15 91.43 61.50 67.57 52.90 57.54 50.13 31.14 36.80 51.08
Ours SG w/o remedy 18.47 29.87 66.51 74.63 68.54 61.35 66.80 36.26 38.41 58.92
Data Dependent Pruning
LLM-Pruner Channel [40] 49.03 106.48 62.39 66.87 49.17 58.96 49.62 31.83 33.20 50.29
LLM-Pruner E1 [40] 16.01 29.28 67.68 77.15 73.41 65.11 68.35 38.40 42.40 61.79
Ours (C) w/o remedy 15.90 28.33 68.48 77.78 74.73 65.01 68.90 39.40 43.11 62.48
Data Dependent Pruning w/ Retraining
LLM-Pruner LoRA [40] 15.18 28.08 70.31 77.91 75.16 67.88 71.09 42.41 43.40 64.02
17
