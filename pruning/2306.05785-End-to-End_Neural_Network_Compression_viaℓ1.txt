# 2306.05785.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2306.05785.pdf
# File size: 936584 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
End-to-End Neural Network Compression viaℓ1
ℓ2Regularized Latency Surrogates
Anshul Nasery
Google Research India
anshulnasery@google.comHardik Shah
Google Research India
hardiknshah@google.com
Arun Sai Suggala
Google Research India
arunss@google.comPrateek Jain
Google Research India
prajain@google.com
Abstract
Neural network (NN) compression via techniques such as pruning, quantization re-
quires setting compression hyperparameters ( e.g., number of channels to be pruned,
bitwidths for quantization) for each layer either manually or via neural architecture
search (NAS) which can be computationally expensive. We address this problem
by providing an end-to-end technique that optimizes for model’s Floating Point
Operations (FLOPs) or for on-device latency via a novelℓ1
ℓ2latency surrogate. Our
algorithm is versatile and can be used with many popular compression methods
including pruning, low-rank factorization, and quantization. Crucially, it is fast
and runs in almost the same amount of time as single model training ; which is a
significant training speed-up over standard NAS methods. For BERT compression
on GLUE fine-tuning tasks, we achieve 50% reduction in FLOPs with only 1%
drop in performance. For compressing MobileNetV3 on ImageNet-1K, we achieve
15% reduction in FLOPs, and 11% reduction in on-device latency without drop in
accuracy , while still requiring 3×less training compute than SOTA compression
techniques. Finally, for transfer learning on smaller datasets, our technique iden-
tifies 1.2×-1.4×cheaper architectures than standard MobileNetV3, EfficientNet
suite of architectures at almost the same training cost and accuracy.
1 Introduction
Large-scale neural networks consistently provide state-of-the-art performance on complex learning
tasks [ 1,2,3]. But they place heavy burden on compute resources such as battery, memory or
processor making them hard to deploy on edge devices such as phones, cameras and wearables.
Several recent works have designed techniques to compress ML models and make them efficient for
inference. However, as detailed below, many of these techniques are hard to use in practice, and often
achieve sub-optimal accuracy vsinference time trade-offs.
Hyperparameter search for compression. Existing works typically rely on one of the following
building blocks to design efficient models: unstructured weights sparsity [ 4,5], pruning entire neurons
or low-rank factorization [ 6], quantization [ 7], distillation [ 8]. Figuring out an optimal way to
combine these building blocks (or to figure out hyper-parameters such as amount of sparsity associated
with each block) while satisfying a global latency/FLOPs/resource constraint is difficult and involves
a combinatorial search. This problem is further exacerbated when multiple building blocks are used
for model compression ( e.g., simultaneous low rank factorization, sparsity/pruning of weights).
Preprint. Under review.arXiv:2306.05785v2  [cs.LG]  13 Jun 2023

--- PAGE 2 ---
/uni00000014/uni00000012/uni00000019 /uni00000014/uni00000012/uni0000001a /uni00000014/uni00000012/uni0000001b /uni00000014/uni00000012/uni0000001c /uni00000014/uni00000012/uni0000001d /uni00000015/uni00000012/uni00000014
/uni00000036/uni00000049/uni00000050/uni00000045/uni00000058/uni0000004d/uni0000005a/uni00000049/uni00000004/uni0000002a/uni00000030/uni00000033/uni00000034/uni00000057/uni00000015/uni00000012/uni00000019
/uni00000015/uni00000012/uni00000014
/uni00000014/uni00000012/uni00000019
/uni00000014/uni00000012/uni00000014/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni0000002a/uni0000004d/uni00000052/uni00000049/uni00000058/uni00000059/uni00000052/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000026/uni00000029/uni00000036/uni00000038/uni00000004/uni00000053/uni00000052/uni00000004/uni0000002b/uni00000030/uni00000039/uni00000029
/uni00000033/uni00000059/uni00000056/uni00000057
/uni00000034/uni00000056/uni00000059/uni00000052/uni0000004d/uni00000052/uni0000004b/uni00000004/uni00000037/uni00000033/uni00000038/uni00000025
/uni00000028/uni0000004d/uni00000057/uni00000058/uni0000004d/uni00000050/uni00000050/uni00000045/uni00000058/uni0000004d/uni00000053/uni00000052
/uni0000001b/uni00000019 /uni00000015/uni00000014/uni00000014 /uni00000015/uni00000016/uni00000019 /uni00000015/uni00000019/uni00000014 /uni00000015/uni0000001b/uni00000019 /uni00000016/uni00000014/uni00000014
/uni00000031/uni00000025/uni00000027/uni00000057/uni00000004/uni0000000c/uni0000004d/uni00000052/uni00000004/uni00000031/uni00000052/uni0000000d/uni0000001a/uni00000018/uni0000001a/uni0000001a/uni0000001a/uni0000001c/uni0000001b/uni00000014/uni0000001b/uni00000016/uni0000001b/uni00000018/uni0000001b/uni0000001a /uni00000038/uni00000053/uni00000054/uni00000011/uni00000015/uni00000004/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni00000031/uni00000053/uni00000046/uni0000004d/uni00000050/uni00000049/uni00000032/uni00000049/uni00000058/uni0000005a/uni00000017/uni00000004/uni00000054/uni00000049/uni00000056/uni0000004a/uni00000053/uni00000056/uni00000051/uni00000045/uni00000052/uni00000047/uni00000049/uni00000004/uni00000053/uni00000052/uni00000004/uni0000002d/uni00000051/uni00000045/uni0000004b/uni00000049/uni00000032/uni00000049/uni00000058
/uni00000033/uni00000059/uni00000056/uni00000057
/uni00000031/uni00000053/uni00000046/uni0000004d/uni00000050/uni00000049/uni00000032/uni00000049/uni00000058/uni0000003a/uni00000017
/uni00000038/uni00000059/uni00000032/uni00000025/uni00000037
/uni00000031/uni00000053/uni00000056/uni00000054/uni0000004c/uni00000032/uni00000049/uni00000058Figure 1: Left plot compares various techniques for BERT compression on GLUE tasks (averaged across
tasks). x-axis is the relative number of FLOPs as compared to BERT BASE.y-axis is the relative drop in accuracy
from the baseline. Pruning SOTA numbers are taken from [ 21], while distillation baselines are from [ 22,23].
Right plot compares various techniques for MobileNetV3 compression on ImageNet-1K dataset. MobileNetV3
corresponds to MobileNetV3 models with different width multiplier. TuNAS, MorphNet are SOTA techniques
for scalable compression. TuNAS takes a blackbox approach to model compression, whereas MorphNet takes a
more direct approach by optimizing FLOPs regularized objective.
Over the past few years, there has been a large body of work that addresses the problem of finding
hyperparameters for model compression. Existing literature in this space can be broadly categorized
as: (a) methods to find hyperparameters of a specific building block such as unstructured pruning of
weights, (b) Neural Architecture Search (NAS) techniques to find hyperparameters of anyefficient
block. The first set of techniques are naturally limited, but even for the specific blocks considered, their
performance on real-world benchmarks is unstable and in fact, can be sub-optimal to the more general
approach that we propose in this work (see the left plot in Figure 1). The NAS based techniques are
much more generally applicable, but the computational cost of such methods is prohibitive as they
typically don’t exploit any specific attributes of the popular efficient blocks such as sparsity, pruning.
Neuron Pruning : Among the category (a) techniques mentioned above, a prominent line of work
has focused on unstructured pruning of weights with non-uniform budget allocation across layers [ 4,
9,10,5]. However, any gain in FLOPs using unstructured pruning is hard to translate to real latency
gain as modern hardware – like GPUs, TPUs – are more geared towards dense matrix operations.
So it is more fruitful to focus on neuron pruning, which removes entire neurons/channels, and low-
rank factorization of weights, which is closely related to neuron pruning. Recent techniques in this
line of work add a latency/FLOPs regularizer to the standard cross entropy loss [ 11,12] to bias
the model towards lower number of neurons. Unfortunately the resulting objective is discrete and
difficult to optimize. To alleviate this, existing works have designed continuous surrogates that are
more amenable to SGD style optimization. These methods either work in the space of probability
distributions over pruned models and optimize the “expected objective” [ 12,13,6] or replace the
discontinuous FLOPs regularizer with a continuous surrogate such as ℓ1norms of the weights of
the network [ 11]. However, the former class of techniques are often unstable, hard to implement in
practice, and empirical studies indicate that their performance is similar to that of simple magnitude
based pruning [ 14] (also see left plot of Fig. 1). Furthermore, as we show in this work, the latter class
of techniques fail to enforce sparsity in the presence of batch, layer normalization (see Section 3).
NAS : Several works in category (b) formulate model compression as a black-box Neural Architecture
Search (NAS) problem and rely on state-of-the-art NAS techniques to search for efficient models [ 15,
16,17,18,19]. These techniques directly take the latency/FLOPs into account and have the potential
to identify the optimal per-layer budget allocation for a wide variety of efficient blocks/compression
mechanisms. However, these approaches are often computationally expensive as they take a blackbox
view of the problem and perform combinatorial search over the space of architectures. Despite recent
advances such as TuNAS [ 18] and DARTS [ 20], these techniques can be an order of magnitude
slower and less accurate than our proposed method (see Fig 1).
Our Approach : In this work, we propose an approach that sits right in the middle of the above two
mentioned categories. That is, our approach applies to a large class of efficient building blocks – like
unstructured sparsity, neuron pruning, quantization – for which we can write the FLOPs computation
with a continuous surrogate (see Table 1). Furthermore, to ensure that our FLOPs, latency regularizers
work even in the presence of batchnorm, layernorm, we propose a novel surrogate that is based on
ℓ1
ℓ2norm. While our surrogates are continuous, they are non-differentiable. In such cases standard
2

--- PAGE 3 ---
optimizers such as SGD, Adam can be quite slow to converge [ 24]. To overcome this, we propose a
projection operation on the mask variables, after each SGD step. Our proposed method speeds up the
convergence and also outputs exact sparse solutions thus eradicating need for post-hoc thresholding,
while being simple enough to not increase training time significantly.
We implement our algorithm with multiple building blocks including pruning, low-rank factorization,
quantization, and apply it on multiple problems in the domain of image classification and NLP. In
particular, we demonstrate the effectiveness of our technique for MobileNetV3 compression on
ImageNet (see Fig. 1), where our method can learn an architecture with up to 15% ( 11%) lower
FLOPs (latency) on Pixel 6 mobile phones, without any drop in accuracy. Here our approach is more
accurate than MorphNet, a SOTA technique which focus exclusively on neuron-pruning, as well as,
TuNAS, a SOTA NAS technique. Furthermore, in terms of training time, our method is 3×cheaper
than TuNAS. We would like to highlight that MobileNetv3 is a highly optimized architecture found
using efficient NAS techniques [25], and our technique is able to compress this architecture further.
One exciting application of our work is that we can apply it to optimize certain “foundational” baseline
models for individual fine-tuning tasks. For example, for compression of BERT on GLUE benchmarks,
our method achieved 40−50% reduction in FLOPs with only 1%drop in accuracy (see Fig 1).
Moreover, our technique dominates standard model compression baselines. Similarly for smaller
vision classification tasks, our technique compresses MobileNetV3, EfficientNet suite of architectures
and identifies 1.2×-1.4×cheaper architectures without significant loss in accuracy (see Figure 3). We
would like to note that all these results are obtained at almost the same cost as that of training a single
model for the task. Finally, we also demonstrate the versatility of our method by using it to quantize
a CNN on CIFAR-10, and learning the bit-widths ( 2,4,8,16) for each of its layers. Our technique
found a model that is 55% smaller than the baseline float-16 model, while achieving the same accuracy
(see Figure 5). While low-bit quantization is not usually exploited by general purpose accelerators to
speed up computation, it can still lead to reduction in inference times of large language models such
as GPT as these models are memory bandwidth bound [ 26]. Here is a summary of our contributions:
(1).We provide an end-to-end neural network compression technique that directly optimizes the
FLOPs/latency regularized objective during leading to compression during training. Our algorithm
can be used with many popular efficient building blocks including pruning, low-rank factorization,
quantization, and can optimize for on-device inference latency.
(2). We design a novelℓ1
ℓ2regularized surrogate for latency that works even in the presence of
batchnorm, layernorm. Our algorithm is fast and runs in the same amount of time as single model
training, and doesn’t require any post-processing steps.
(3).We demonstrate the performance of our technique on both language and vision tasks. Moreover,
for transfer learning settings where the goal is to take a baseline architecture and optimize it for
individual tasks, our techniques outperform SOTA techniques in the broad-domain of automated
neural compression.
2 Related Work
2.1 Neural Architecture Search
Early works on NAS treated the problem as a purely blackbox optimization (BO) problem. These
works relied on BO techniques such as random search [ 27], Gaussian process optimization [ 17], and
zeroth-order gradient descent [ 15,16], evolutionary algorithms to optimize the NAS objective and
identify a good architecture. Several works have improved upon these algorithms using heuristics such
as early stopping [ 27]. Nonetheless, these techniques are computationally expensive, as evaluating
the optimization objective at any point requires training a neural network from scratch. Moreover,
due to computational complexity, these techniques perform a very coarse grained search and are not
suited for fine-grained search over sparsity or low-rank structures.
Recent works have tried to open the blackbox a bit. In these techniques, the search space is first
transformed to the space of probability distributions over architectures. Next, a surrogate model
(which takes an architecture as input and tries to output the optimal set of weights for the architecture)
is trained to quickly evaluate the optimization objective at any input [ 18,20,28,29,12]. While these
techniques are fast, they involve joint training of the surrogate model during the search process. This
joint training often makes the optimization process unstable [30].
3

--- PAGE 4 ---
NAS for Efficient ML. Several recent works at the intersection of efficient ML and NAS have realized
the importance of explicitly accounting for the hardware in the search process [ 15,31,32,33,34,35].
These works incorporate the actual inference time in their search objectives, instead of surrogates
such as FLOPs. The inference time maybe estimated using another neural network, or through
latency tables for basic arithmetic operations on the target platform [ 19]. Many of these works rely on
greedy, random search heuristics to solve the resulting objective [32, 33]. However, these heuristics
either take a lot of time to find the optimal architecture or are not guaranteed to converge to an
optimal solution. There are some works that rely on the NAS algorithms described above [ 15,31,18].
However, these techniques face the same issues as previously mentioned.
Hardware, Neural Architecture codesign. Certain hardware level parameters such as tiling configu-
rations of tensors significantly affect the inference time of a model. Recent hardware-aware NAS tech-
niques expose these hardware level parameters to the NAS algorithm and simultaneously search over
neural architectures and hardware configurations [ 35]. These techniques have the potential to achieve
better performance than vanilla NAS techniques which do not search over hardware configurations.
2.2 Model Compression
The field of model compression is vast. Here, we focus on techniques that perform training-time com-
pression (as opposed to post-training compression) using the following building blocks: unstructured
sparsity, pruning and low-rank factorization. Early works in unstructured sparsity and pruning relied
on magnitude, gradient based pruning [ 4,36,14]. Several works have explored more sophisticated
scoring metrics for pruning [ 37,38,39,40,41]. Other techniques include adding sparsity inducing
norms such as ℓ0, ℓ1to the training objective [ 13,5]. A number of works have also explored low-rank
factorization for model compression [ 42,43,44]. Some of these techniques again rely on sparsity
inducing regularizers to induce the low-rank structure [ 6]. Others rely on SVD based pruning. Some
recent works try and optimize FLOPs regularized objective to perform pruning, low-rank factoriza-
tion [ 11,12]. However, as we discussed in the introduction, the resulting optimization techniques are
often unstable and difficult to use in practice.
3 Method
In this section, we describe our approach for model compression. For simplicity of presentation, we
illustrate our technique on feed-forward networks and restrict ourselves to pruning. The ideas here
can be extended to other architectures ( e.g., 1x1 convolutions in CNNs), and other efficient building
blocks ( e.g., unstructured sparsity, low-rank factorization, quantization) in a straightforward manner
(see Table 1 for details). Consider the following problem: we are given a pre-trained feed forward
neural network (FFN) f∗(x) =σ(W∗
Dσ(W∗
D−1σ(. . . σ(W∗
1x)))), where W∗
i∈Rdi+1×difor all
i∈[D], and a dataset {(xi, yi)}n
i=1. Our goal is to compress f∗while simultaneously performing
well on the learning task. This problem can be formulated as the following optimization problem
min
W1
nnX
i=1ℓ(xi, yi;W) +λ×Latency (W). (1)
HereW={Wi}D
i=1,withWi∈Rd′
i+1×d′
ibeing the weight matrix at layer i,λis the regularization
parameter which trades-off latency with accuracy and ℓis the supervised loss.1. Directly optimizing
the above objective is intractable because Latency (W)is a discrete function of the dimensions of
weight matrices, and is hardware specific.
We now present our technique for solving Equation (1). To begin with, we substitute Latency (W)
with FLOPs (W)2. Later, we extend it to actual latency. The objective in this case is given by
min
W1
nnX
i=1ℓ(xi, yi;W) +λDX
i=1d′
id′
i+1. (2)
1In this objective, we search over d′
isuch that d′
i≤di
2FLOPs is also a discrete function of dimensions of Wi, and the resulting optimization problem is still
intractable
4

--- PAGE 5 ---
To solve this objective, we associate masks with each neuron in the network. In particular, we
parameterize the weight matrix in the ithlayer as Wi×diag(αi). Here αi∈ {0,1}diare the mask
variables of layer i. Ifαi,jis set to 0, then the jthneuron in the (i−1)thlayer will be pruned. The
FLOPs regularizer can now be written in terms of masks asPD
i=1∥αi∥0∥αi+1∥0, where αD+1is
the static vector of all 1’s. The resulting objective though is not continuous. To make it continuous
and amenable to gradient based optimization, one class of techniques place a Bernoulli distribution
Bern(pi,j)over each of the masks αi,jand solve the following smoothed objective [12, 13, 6]
min
W,pE"
1
nnX
i=1ℓ(xi, yi;p,W) +λDX
i=1∥αi∥0∥αi+1∥0#
.
The expectation above is taken w.r.t the random masks αi’s. It is easy to see that the above objective
is equivalent to Equation (2), and is consequently as hard as solving the latter. In fact, the above
problem can be shown to be NP-hard using the observation that sparse linear regression is a special
case of it [ 45]. Furthermore, the discrete nature of αi’s makes the optimization process unstable [ 13].
To overcome this, [ 12,13,6] rely on a heuristic which involves relaxing Bernoulli distribution to
a continuous distribution such as LogisticSigmoid. However, the main drawback of the resulting
algorithm is that it is hard to implement in practice and requires very careful annealing of the
parameters of LogisticSigmoid distribution. Another drawback of this class of techniques is that their
performance is not well understood theoretically, even for simple and fundamental problems such as
sparse linear regression.
Another approach to convert the discrete objective in Equation (2)into a continuous function is to
replace the ℓ0norm on αi’s with ℓ1norm
min
W,αi∈Rdi1
nnX
i=1ℓ(xi, yi;α,W) +λDX
i=1∥αi∥1∥αi+1∥1. (3)
This approach is much more attractive than the previous approach as it is known to recover the optimal
sparse solutions for a variety of statistical problems including sparse linear regression, low-rank
matrix completion [ 46,47]. Furthermore, it is much simpler to implement in practice, with numerous
algorithms being proposed for fast convergence to the stationary points of the objective [ 24,48].
Consequently, recent SOTA compression techniques relied on ℓ1norm surrogates to compute the
FLOPs regularizer [ 11]. A major drawback of ℓ1norm though is that it does not promote sparsity
in the presence of batch normalization and layer normalization [ 49,50]. To see this, consider the
following 1-hidden layer network: σ(BN(W2diag(α2)σ(BN(W1diag(α1)x)))).One can scale down
all entries of α1and scale up the weights W1without affecting the output of the network. Doing
this reduces the objective value in Equation (3), but doesn’t induce any sparsity in the network. In
practice, we in fact notice this behaviour during optimization of Equation (3), which leads to sub-
optimal solutions (see Section 3.2). Note that adding ℓ2penalty on the weights ( i.e.,weight decay)
doesn’t mitigate this issue as any scaling of α′s can be absorbed by the batch norm parameters without
changing the output of the network.
3.1 Inducing sparsity throughℓ1
ℓ2regularizer
We now introduce our approach for making the objective in Equation (2)continuous. We replace ℓ0
norm over masks ( ∥αi∥0) withℓ1
ℓ2penalty (√di∥αi∥1/∥αi∥2) and solve the following optimization
problem
min
W,αi∈Rdi1
nnX
i=1ℓ(xi, yi;α,W) +λDX
i=1√di∥αi∥1
∥αi∥2p
di+1∥αi+1∥1
∥αi+1∥2. (4)
The√diterm in the numerator normalizes the penalty to lie between [0, di].When αi’s are all
1’s, the regularizer evaluates to FLOPs. Observe that this regularizer is invariant to scaling of α’s.
Consequently, the value of the regularizer cannot simply be reduced by scaling down αi’s. In our
experiments in sec 3.2 and Appendix C.2, we show that this handles batch, layer normalizations
better than ℓ1regularizer. Several works have studied this regularizer in the context of sparse linear
regression and showed that is recovers the underlying sparse signal under mild conditions on the
5

--- PAGE 6 ---
Figure 2: Comparison of ℓ1,ℓ1
ℓ2induced FLOPs regularizer for pruning on FashionMNIST : Figures (a)
and (b) depict the evolution of the statistics of the mask variables ( α) as training progresses. Figure (c) shows the
relation between the actual FLOPs of the model and the value of the proxy computed by Equations 3, 4. Figure
(d) shows the evolution of the Frobenius norm of the weight matrix.
data [ 51,52,53]. [54] used a similarℓ1
ℓ2regularizer for network pruning, but their technique doesn’t
optimize latency or FLOPs, and relies on post-training thresholding to get sparsity.
For certain technical reasons described later, we add a positivity constraint on αi’s and solve the
following objective
min
W,αi∈Rdi
+1
nnX
i=1ℓ(xi, yi;α,W) +λDX
i=1√diPdi
j=1αi,j
∥αi∥2p
di+1Pdi+1
j=1αi+1,j
∥αi+1∥2. (5)
Note that we consider α∈Rdi
+rather that discrete or bounded values. We would like to highlight
that this change doesn’t reduce the representational power of our model. It is mainly done for
computational reasons. In the sequel, we use the shorthand ∥αi∥1p(pfor positive) to denotePdi
j=1αi,j.
Importance of positivity constraints. The objective in Equation (4)is continuous, but not smooth.
For such losses, standard optimization techniques such as SGD, Adam are slow to converge to
stationary points [ 55]. Furthermore, these algorithms don’t output exact sparse solutions. This forces
additional post-processing steps to be introduced into the compression pipeline. For example, [ 11,54]
rely on Adam optimizer and add a pruning step at the end, where masks that are close to 0are
pruned away. This is quite cumbersome in practice as one needs to choose appropriate thresholds for
pruning, which introduces an additional tunable hyper-parameter, and needs re-training after pruning.
To overcome this, we add a positivity constraint to the mask variables and modify the objective to
Equation (5). This makes the regularizer smooth (except at all 0’s vector), and easy to optimize
using SGD, Adam. After each SGD/Adam update, we simply project the masks back to the space of
positive real numbers. The overall update looks as follows
W ← W − η∇W(L(α,W) +λR(α)), α←max(0 , α−η∇α(L(α,W) +λR(α))).
HereL(α,W)is the empirical risk and R(α)is the regularizer. Notice, the only additional step
compared to traditional optimization, is the clipping of α’s. In our ablation studies in Sec 3.2 and
App C.2, we validate the importance of this projection step, together withℓ1
ℓ2norm, in encouraging
sparse solutions.
3.2 Verification of design choices
To empirically demonstrate the drawbacks of using ℓ1penalty for model compression, we perform
experiments on the FashionMNIST dataset with a single hidden layer fully connected network which
has a batch norm layer after the first linear layer. We prune out the input to the network using a
mask αon the input. We compare the performance of networks compressed using FLOPs regularizer
induced by ℓ1andℓ1
ℓ2norms. We use SGD for optimization of both the objectives. Furthermore, we
pre-train the network using standard CE loss, and initialize α=1. We track the variance of the
absolute values of the entries of α, i.e.Pd
i=1(|αi|−µα)2
d, where µα=Pd
i=1|αi|
d. We also track the
mean µαof the absolute values of the entries of α. Finally, we plot out the curve between FLOPs and
the considered norm of α(i.e.,ℓ1,ℓ1
ℓ2). Figure 2 presents the results from these experiments. We can
see that the ℓ1objective is mis-aligned with the actual value of FLOPs, while the regularizer computed
usingℓ1
ℓ2is a better proxy. We also find that the mean and variance of α′s sharply decreases when ℓ1
induced FLOPs regularizer is used for compression. This indicates that all entries of αare uniformly
6

--- PAGE 7 ---
Table 1: Table describing regularizers used by our technique for various efficient building blocks (refer to
Appendix for details on quantization). One can easily design regularizers for searching over a combination of
building blocks. For example, last row presents regularizer for low-rank + pruning, which we use in our large-
scale experiments.
Efficient
Building BlockParameterization of WiFLOPs
(ithlayer)Regularizer (FLOPs surrogate)
(ithlayer)
Pruning Wi×diag(αi) ∥αi∥0∥αi+1∥0√di∥αi∥1p
∥αi∥2√
di+1∥αi+1∥1p
∥αi+1∥2
Unstructured SparsityWi⊙αi,where αi∈Rdi+1×di
+ ,
⊙is the elementwise
multiplication operator∥Vec(αi)∥0√
didi+1∥Vec(αi)∥1p
∥Vec(αi)∥2
Low-rank FactorizationUidiag(βi)Vi,
where Ui∈Rdi+1×di,∗,
di,∗= min {di, di+1}(di+di+1)∥βi∥0 (di+di+1)√
di,∗∥βi∥1p
∥βi∥2
Quantization
(1,2,4bit quantization)Wi,1+αi,2(∆i,2+αi,4(∆i,4)),
where αi,2, αi,4∈[0,1],are
mask variables, Wi,bis the
b-bit quantization of Wi,
∆i,2=Wi,2−Wi,1,
∆i,4=Wi,4−Wi,2∥(1−αi,2)∥0didi+1+
2∥αi,2(1−αi,4)∥0didi+1+
4∥αi,2αi,4∥0didi+1ℓ1
ℓ2norm over
the vector [ (1−αi,2),
2αi,2(1−αi,4),4αi,2αi,4]
×didi+1
Pruning +
Low-rank FactorizationUidiag(βi)Vidiag(αi),
where Ui∈Rdi+1×di,∗,
di,∗= min {di, di+1}(∥αi∥0+∥αi+1∥0)∥βi∥0√di∥αi∥1p
∥αi∥2+√
di+1∥αi+1∥1p
∥αi+1∥2
×√
di,∗∥βi∥1p
∥βi∥2
scaled down to a small, non-zero value, reducing the value of the regularizer, while not providing any
sparsity. As seen from the figure,ℓ1
ℓ2does not suffer from this drawback. Finally, we note that the
frobenius norm of the weight matrix Wincreases when ℓ1regularization is used on α, suggesting
that the network is simply scaling down α′s and scaling up the weights to evade the regularizer.
3.3 Hardware aware model compression
In this section, we extend the FLOPs regularizer to take the latency on the target hardware into
account. The resulting regularizer is especially useful for performing hardware aware network
compression. Our key observation is that the inference on a neural network can be broken down
into a series of matrix multiplication operations. For example, inference on a depth DFFN involves
Dmatrix-vector multiplications, which take-up majority of the time. So, getting a good estimate
of the inference time of the overall network boils down to having a good estimate of the latency of
matrix-vector multiplication. To this end, we rely on lookup tables. Before the start of the pruning
phase, we construct a 2-dimensional lookup-table Twhose (d1, d2)thentry is the on-device latency
of multiplying a matrix of size d1×d2with a vector of size d2. Such a table is easy to construct, given
access to the target device. Next, to incorporate the look-up table Tinto our pruning algorithm, we
convert it into a continuous function by performing linear interpolation on the entries in the table [ 56].
To be precise, for any (x, y)∈[d1, d1+ 1]×[d2, d2+ 1],where d1, d2∈N∪{0}, we define T(x, y)
as:T(x, y) =t1+ (t2−t1)(y−d2),where t1=T(d1, d2) + (T(d1+ 1, d2)−T(d1, d2))(x−d1),
andt2=T(d1, d2+ 1) + ( T(d1+ 1, d2+ 1)−T(d1, d2+ 1))( x−d1).Note that in contrast to
black-box NAS techniques like [ 19] which search over a discrete space of number of filters for each
block, our approach needs the latency surrogate to be differentiable, and hence we need interpolated
latency tables. See the appendix for details on how we construct the tables.
We use this interpolated lookup table to construct our latency regularizer as follows
DX
i=1T √di∥αi∥1p
∥αi∥2,p
di+1∥αi+1∥1p
∥αi+1∥2!
. (6)
In the above expression, our differentiable surrogate for ∥αi∥0(i.e.,√di∥αi∥1p/∥αi∥2), is used to
index the lookup table. We note thatℓ1
ℓ2norm is very crucial for this technique to be successful.
This is because√di∥αi∥1p
∥αi∥2is normalized and always lies between [0, di].In contrast, using ℓ1norm
surrogate in the regularizer gives us T(∥αi∥1,∥αi+1∥1). Scaling αiby a constant can drastically
change this regularizer, and makes the optimization unstable.
7

--- PAGE 8 ---
/uni00000015/uni00000014/uni00000014 /uni00000015/uni00000019/uni00000014 /uni00000016/uni00000014/uni00000014
/uni00000031/uni00000025/uni00000027/uni00000057/uni00000004/uni0000000c/uni0000004d/uni00000052/uni00000004/uni00000031/uni00000052/uni0000000d/uni0000001b/uni00000019/uni0000001c/uni00000014/uni0000001c/uni00000019 /uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni00000027/uni00000045/uni00000056/uni00000057/uni00000011/uni00000015/uni0000001d/uni0000001a
/uni00000033/uni00000059/uni00000056/uni00000057
/uni00000031/uni00000053/uni00000046/uni0000004d/uni00000050/uni00000049/uni00000032/uni00000049/uni00000058/uni0000003a/uni00000017
/uni00000015/uni00000014/uni00000014 /uni00000015/uni00000019/uni00000014 /uni00000016/uni00000014/uni00000014
/uni00000031/uni00000025/uni00000027/uni00000057/uni00000004/uni0000000c/uni0000004d/uni00000052/uni00000004/uni00000031/uni00000052/uni0000000d/uni0000001c/uni00000014/uni0000001c/uni00000018/uni0000001c/uni0000001c /uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni0000002a/uni00000053/uni00000053/uni00000048/uni00000011/uni00000015/uni00000014/uni00000015
/uni00000014/uni00000012/uni00000019 /uni00000015/uni00000012/uni00000014 /uni00000016/uni00000012/uni00000014 /uni00000018/uni00000012/uni00000014
/uni00000031/uni00000025/uni00000027/uni00000057/uni00000004/uni0000000c/uni0000004d/uni00000052/uni00000004/uni00000026/uni00000052/uni0000000d/uni0000001d/uni00000015/uni0000001d/uni00000016/uni0000001d/uni00000017/uni0000001d/uni00000018 /uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni00000027/uni00000045/uni00000056/uni00000057/uni00000011/uni00000015/uni0000001d/uni0000001a
/uni00000033/uni00000059/uni00000056/uni00000057
/uni00000029/uni0000004a/uni0000004a/uni0000004d/uni00000047/uni0000004d/uni00000049/uni00000052/uni00000058/uni00000032/uni00000049/uni00000058
/uni00000014/uni00000012/uni00000019 /uni00000015/uni00000012/uni00000014 /uni00000016/uni00000012/uni00000014 /uni00000018/uni00000012/uni00000014
/uni00000031/uni00000025/uni00000027/uni00000057/uni00000004/uni0000000c/uni0000004d/uni00000052/uni00000004/uni00000026/uni00000052/uni0000000d/uni0000001c/uni0000001c/uni0000001c/uni0000001d/uni0000001d/uni00000014/uni0000001d/uni00000015/uni0000001d/uni00000016 /uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni0000002a/uni00000053/uni00000053/uni00000048/uni00000011/uni00000015/uni00000014/uni00000015Figure 3: Accuracy-FLOPs trade-off on transfer learning tasks : Figures (a) and (b) depict the the fine-tuning
performance of models found by our method while compressing MobileNetv3Large and baseline MobileNetV3
on Cars-196 and Food-101 datasets. Figures (c) and (d) show the performance on the EfficientNet family of
architectures, where baselines are EfficientNetB0-B4, while our method compresses EfficientNet B4 and B2.
4 Experiments
In this section, we apply our framework to large scale pre-training and transfer learning tasks on
standard language and vision benchmarks. To demonstrate the versatility of our technique, we
perform experiments on multiple model families (MobileNet, EfficientNet [ 2], BERT), and multiple
building blocks (pruning, low-rank factorization, quantization). We also present a case study using
the actual on-device latency instead of FLOPs. See Appendix C.2 for other ablation studies.
4.1 ImageNet Pre-training
We begin by comparing the performance of our technique with baselines on MobileNetV3 compres-
sion, for ImageNet classification. We rely on low-rank factorization + pruning for the compression.
The results from this experiment are presented in Figure 1. By varying the strength of our regulariza-
tion, we obtain models with different MACs and accuracies. We find that models produced by our
method significantly outperform MobileNetV3 and TuNAS in the high and mid-MACs regime. In
particular, for the same accuracy as MobileNetV3Large, our approach finds a model with 15% fewer
MACs. In comparison with TuNAS, we achieve 30% reduction in MACs at the same level of accuracy.
We however find that our model is at par with MobileNetV3Small in the low MACs regime, indicating
that the former is already well-tuned for this task. In terms of compute needed for training, TuNAS is
the most expensive among all the techniques we tried; it took 2 days to train with our hardware setup.
In contrast, our method took 13 hours ( 3−4×faster than TuNAS), and MorphNet took 10 hours.
4.2 Transfer Learning
A common paradigm in deploying machine learning models today is to first pre-train them on a
large scale dataset such as ImageNet, and then fine-tune them for the desired target task. However,
deploying large models is not feasible on edge devices. Our technique provides a light-weight
modification to the standard fine-tuning procedure by producing a compressed model with comparable
transfer learning performance on the specific task. We demonstrate this on vision and language tasks.
Vision tasks. We consider the task of fine-tuning an ImageNet pre-trained model for a smaller
dataset. We consider Cars196 [ 57] and Food101 [ 58] as the target datasets, and compare against
the MobileNetV3 and EfficientNet families of models. We use ImageNet pre-trained models for
initialization. We plot the FLOP-accuracy curves in Fig 3. We compress MobileNetv3Large and
EfficientNet-B4 and EfficientNet-B2 architectures while transferring them to the target target task.
We find that our method consistently improves over baseline architectures across various FLOPs
regimes. This is because our technique is able to adaptively prune the model based on the difficulty
of the classification task. On both the tasks, we see 1% accuracy gains over MobileNetV3 small.
The accuracy gains persist at the latency footprint of MobileNetV3Large-0.75, where we see over
1.5% accuracy gains on both datasets. On EfficientNet, we see upto 40% reduction in FLOPs without
any drop in accuracy on Food101, and around 20% reduction in FLOPs on the Cars196 dataset for
the largest models (B4). We also see around 30% FLOP reduction while maintaining the transfer
learning performance of the B1 and B0 variants. This demonstrates that our learnt models can scale
better than the heuristic scaling described in [2]. See the appendix for additional results.
Fine-tuning BERT on GLUE. We consider 5 datasets of the GLUE benchmark [ 59] that are
commonly used in the literature, and fine-tune a pre-trained BERT-Base model with our FLOPs
regularizer. We re-parameterize the weight matrices of the feed forward network of each transformer
block with our low-rank+sparse parameterization. We compare our approach against model pruning,
8

--- PAGE 9 ---
/uni0000001a /uni0000001b /uni0000001c /uni0000001d /uni00000015/uni00000014 /uni00000015/uni00000015 /uni00000015/uni00000016 /uni00000015/uni00000017 /uni00000015/uni00000018
/uni00000030/uni00000045/uni00000058/uni00000049/uni00000052/uni00000047/uni0000004d/uni00000049/uni00000057/uni00000004/uni0000000c/uni0000004d/uni00000052/uni00000004/uni00000051/uni00000057/uni0000000d/uni0000001a/uni00000019/uni0000001b/uni00000014/uni0000001b/uni00000019 /uni00000038/uni00000053/uni00000054/uni00000011/uni00000015/uni00000004/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d/uni00000004/uni0000000c/uni0000002d/uni00000051/uni00000045/uni0000004b/uni00000049/uni00000032/uni00000049/uni00000058/uni0000000d
/uni00000030/uni00000045/uni00000058/uni00000049/uni00000052/uni00000047/uni0000005d/uni00000004/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d/uni00000004/uni00000038/uni00000056/uni00000045/uni00000048/uni00000049/uni00000033/uni0000004a/uni0000004a
/uni0000002a/uni00000050/uni00000053/uni00000054/uni00000004/uni00000056/uni00000049/uni0000004b/uni00000059/uni00000050/uni00000045/uni00000056/uni0000004d/uni0000005e/uni00000049/uni00000056
/uni00000030/uni00000045/uni00000058/uni00000049/uni00000052/uni00000047/uni0000005d/uni00000004/uni00000056/uni00000049/uni0000004b/uni00000059/uni00000050/uni00000045/uni00000056/uni0000004d/uni0000005e/uni00000049/uni00000056Method Latencies (ms) Top-1 Accuracy MACs (mn)
Latency
Regularizer10.34 74.03 152.8
12.1 75.25 184
13.52 76.05 216.9
MobileNetv310.22 73.3 155
11.42 72.3 209
13.41 75.6 217
Figure 4: Left plot shows the accuracy-latency curves of models obtained using FLOPs, latency regularizers.
Right table compares the performance of our latency regularized models with MobileNetV3 baseline.
Figure 5: Quantization on CIFAR-10 : Figure (a) compares the performance of our technique for dynamic
quantization against fixed-bit quantization for a 4 layer CNN on CIFAR-10. The baselines have weights quantized
to 2,4,8, 16 bits. Fig (b) depicts the learnt bitwidths for different layers of the models found by our technique,
with the labels denoting the number of MACs (in Bn) of the models.
where SOTA numbers are taken from Fig. 6 of [ 21], reporting the maximum accuracy among [ 60,61,
62,63,64,65]. We also report the performance of widely-used distillation based baselines [ 22,23].
Figure 1 presents the average performance on the 5 datasets, and Figure 6 in appendix presents the
individual performance. In both these figures, we plot the relative FLOPs of the compressed model
w.r.t BERT-base against the drop in accuracy w.r.t BERT-base (similar to [21]). We find that on 4 of
the 5 datasets considered, our technique provides a higher accuracy for the same number of FLOPs,
indicating the efficacy of our method. On MRPC, a dataset with very few samples, our method is
worse off on higher FLOPs, but outperforms the baselines in the low FLOP regime.
4.3 Additional Experiments
Using the latency regularizer. In Eq 6, we propose a latency surrogate for optimizing the actual
on-device inference latency. In this section, we provide empirical evidence of the effectiveness of this
approach for MobileNetv3 on Pixel 6. We compare the accuracy-latency curves of models produced
using FLOPs, latency regularizers (see Fig 4). Observe that using the latency regularizer leads to
models with smaller latencies and consequently better latency-accuracy tradeoff compared to using
the FLOP regularizer. We also find these models to have better performance than MobileNetV3
(0.5−2%improvement in accuracy for similar latency), despite MobileNetv3 being hand-crafted for
faster inference on mobile devices.
Quantization. In this set of experiments, we consider CIFAR-10 classification and compress a 3
layer CNN using quantization. We use the quantization formulation presented in Table 1 and search
over{2,4,8,16}bit quantizations for each layer. We compare with a baseline which uses the same
level of quantization at each layer. Fig 5 presents the results from this experiments. The details of the
implementation can be found in the appendix. We find that our technique compresses the model size
by almost 55% without drop in accuracy (as compared to a model with 16-bit weights). Our technique
also outputs a model which is 1.4% more accurate than a 2-bit quantized model with only 4% more
FLOPs. In the plot on the right in Fig 5, we visualize the learned bit-widths of our models. We find
that later layers are assigned a smaller bit width, indicating the importance of learning expressive
filters early in the network. The different models in our plots were found by varying the the value of
the regularizer coefficient, and hence no combinatorial search over bit-widths is required.
5 Conclusion and Future Work
In this work, we presented an end-to-end technique for neural network compression. Our approach
applies to a wide variety of efficient blocks including pruning, unstructured sparsity, quantization. At
9

--- PAGE 10 ---
the core of our algorithm is a novel surrogate for FLOPs, latency that relies onℓ1
ℓ2norms, and works
with batchnorm, layernorm. Our algorithm is computationally efficient and runs in same amount of
time as needed for training a single model. We demonstrated the efficacy of our approach on various
pre-training and transfer learning tasks on standard language and vision benchmarks. As a future
work, it will useful to incorporate more efficient building blocks such as block diagonal matrices into
our framework. Another interesting direction would be to make our technique more hardware aware
by incorporating hardware level parameters such as tiling into our search process.
References
[1]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770–778, 2016.
[2]Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural
networks. In International conference on machine learning , pages 6105–6114. PMLR, 2019.
[3]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361 , 2020.
[4]Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. Advances in neural information processing systems , 28, 2015.
[5]Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham
Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In
International Conference on Machine Learning , pages 5544–5555. PMLR, 2020.
[6]Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models.
arXiv preprint arXiv:1910.04732 , 2019.
[7]Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen,
and Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint
arXiv:2106.08295 , 2021.
[8]Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In
Knowledge Discovery and Data Mining , 2006.
[9]Tao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model
pruning with feedback. arXiv preprint arXiv:2006.07253 , 2020.
[10] Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing fine-tuning and rewinding in
neural network pruning. In International Conference on Learning Representations , 2020.
[11] Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi.
Morphnet: Fast & simple resource-constrained structure learning of deep networks. In Proceed-
ings of the IEEE conference on computer vision and pattern recognition , pages 1586–1595,
2018.
[12] Shraman Ray Chaudhuri, Elad Eban, Hanhan Li, Max Moroz, and Yair Movshovitz-Attias.
Fine-grained stochastic architecture search. arXiv preprint arXiv:2006.09581 , 2020.
[13] Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks
through l_0regularization. arXiv preprint arXiv:1712.01312 , 2017.
[14] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv
preprint arXiv:1902.09574 , 2019.
[15] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2820–2828,
2019.
[16] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv
preprint arXiv:1611.01578 , 2016.
[17] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric Xing.
Neural architecture search with bayesian optimisation and optimal transport. arXiv preprint
arXiv:1802.07191 , 2018.
10

--- PAGE 11 ---
[18] Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan Kindermans,
and Quoc V Le. Can weight sharing outperform random architecture search? an investigation
with tunas. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14323–14332, 2020.
[19] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze,
and Hartwig Adam. Netadapt: Platform-aware neural network adaptation for mobile applications.
InProceedings of the European Conference on Computer Vision (ECCV) , pages 285–300, 2018.
[20] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search.
arXiv preprint arXiv:1806.09055 , 2018.
[21] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer, and
Amir Gholami. A fast post-training pruning framework for transformers. arXiv preprint
arXiv:2204.09656 , 2022.
[22] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter, 2020.
[23] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model
compression. arXiv preprint arXiv:1908.09355 , 2019.
[24] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and trends ®in Optimiza-
tion, 1(3):127–239, 2014.
[25] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan,
Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3.
InProceedings of the IEEE/CVF International Conference on Computer Vision , pages 1314–
1324, 2019.
[26] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and
John Jumper. Accelerating large language model decoding with speculative sampling. arXiv
preprint arXiv:2302.01318 , 2023.
[27] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search.
InUncertainty in artificial intelligence , pages 367–377. PMLR, 2020.
[28] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture
search via parameters sharing. In International Conference on Machine Learning , pages 4095–
4104. PMLR, 2018.
[29] Jieru Mei, Yingwei Li, Xiaochen Lian, Xiaojie Jin, Linjie Yang, Alan Yuille, and Jianchao Yang.
Atomnas: Fine-grained end-to-end neural architecture search. arXiv preprint arXiv:1912.09640 ,
2019.
[30] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey.
The Journal of Machine Learning Research , 20(1):1997–2017, 2019.
[31] Grace Chu, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kin-
dermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew Howard. Discovering multi-
hardware mobile models via architecture search. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 3022–3031, 2021.
[32] Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han. Mcunet: Tiny
deep learning on iot devices. arXiv preprint arXiv:2007.10319 , 2020.
[33] Zhen Dong, Yizhao Gao, Qijing Huang, John Wawrzynek, Hayden KH So, and Kurt Keutzer.
Hao: Hardware-aware neural architecture optimization for efficient inference. In 2021 IEEE
29th Annual International Symposium on Field-Programmable Custom Computing Machines
(FCCM) , pages 50–59. IEEE, 2021.
[34] Li Lyna Zhang, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, and Yunxin Liu. Fast hardware-
aware neural architecture search. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops , pages 692–693, 2020.
[35] Hadjer Benmeziane, Kaoutar El Maghraoui, Hamza Ouarnoughi, Smail Niar, Martin Wistuba,
and Naigang Wang. A comprehensive survey on hardware-aware neural architecture search.
arXiv preprint arXiv:2101.09336 , 2021.
[36] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. arXiv preprint arXiv:1803.03635 , 2018.
11

--- PAGE 12 ---
[37] Ehud D Karnin. A simple procedure for pruning back-propagation trained neural networks.
IEEE transactions on neural networks , 1(2):239–242, 1990.
[38] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440 , 2016.
[39] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance
estimation for neural network pruning. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 11264–11272, 2019.
[40] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns.
Advances in neural information processing systems , 29, 2016.
[41] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-
wise optimal brain surgeon. Advances in Neural Information Processing Systems , 30, 2017.
[42] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural
networks with low rank expansions. arXiv preprint arXiv:1405.3866 , 2014.
[43] Zhiyun Lu, Vikas Sindhwani, and Tara N Sainath. Learning compact recurrent neural networks.
In2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) ,
pages 5960–5964. IEEE, 2016.
[44] Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Wenrui Dai, Yingyong Qi, Yiran Chen,
Weiyao Lin, and Hongkai Xiong. Trained rank pruning for efficient deep neural networks. In
2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS
Edition (EMC2-NIPS) , pages 14–17. IEEE, 2019.
[45] Balas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM journal on
computing , 24(2):227–234, 1995.
[46] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society: Series B (Methodological) , 58(1):267–288, 1996.
[47] Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep Ravikumar. A unified framework
for high-dimensional analysis of m-estimators with decomposable regularizers. Advances in
neural information processing systems , 22, 2009.
[48] Jihun Yun, Aurélie C Lozano, and Eunho Yang. Adaptive proximal gradient methods for
structured neural networks. Advances in Neural Information Processing Systems , 34:24365–
24378, 2021.
[49] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International conference on machine learning , pages
448–456. PMLR, 2015.
[50] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016.
[51] Penghang Yin, Ernie Esser, and Jack Xin. Ratio and difference of l_1andl_2norms and
sparse representation with coherent dictionaries. Communications in Information and Systems ,
14(2):87–109, 2014.
[52] Yaghoub Rahimi, Chao Wang, Hongbo Dong, and Yifei Lou. A scale-invariant approach for
sparse signal recovery. SIAM Journal on Scientific Computing , 41(6):A3649–A3672, 2019.
[53] Chao Wang, Ming Yan, Yaghoub Rahimi, and Yifei Lou. Accelerated schemes for the l_1/l_2
minimization. IEEE Transactions on Signal Processing , 68:2660–2669, 2020.
[54] Huanrui Yang, Wei Wen, and Hai Li. Deephoyer: Learning sparser neural network with
differentiable scale-invariant sparsity measures. arXiv preprint arXiv:1908.09979 , 2019.
[55] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization . Cambridge
university press, 2004.
[56] Helmuth Späth. One dimensional spline interpolation algorithms . AK Peters/CRC Press, 1995.
[57] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for
fine-grained categorization. In 4th International IEEE Workshop on 3D Representation and
Recognition (3dRR-13) , Sydney, Australia, 2013.
[58] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative
components with random forests. In European Conference on Computer Vision , 2014.
12

--- PAGE 13 ---
[59] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding.
InProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP , pages 353–355, Brussels, Belgium, November 2018. Association for
Computational Linguistics.
[60] Zejian Liu, Fanrong Li, Gang Li, and Jian Cheng. EBERT: Efficient BERT inference with
dynamic structured pruning. In Findings of the Association for Computational Linguistics:
ACL-IJCNLP 2021 , pages 4814–4823, Online, August 2021. Association for Computational
Linguistics.
[61] Zi Lin, Jeremiah Liu, Zi Yang, Nan Hua, and Dan Roth. Pruning redundant mappings in
transformer models via spectral-normalized identity prior. In Findings of the Association for
Computational Linguistics: EMNLP 2020 , pages 719–730, Online, November 2020. Association
for Computational Linguistics.
[62] François Lagunas, Ella Charlaix, Victor Sanh, and Alexander Rush. Block pruning for faster
transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing , pages 10619–10629, Online and Punta Cana, Dominican Republic, November 2021.
Association for Computational Linguistics.
[63] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models.
InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP) , pages 6151–6162, Online, November 2020. Association for Computational Linguis-
tics.
[64] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and accurate
models. In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1513–1528, Dublin, Ireland, May 2022. Association
for Computational Linguistics.
[65] Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. On the effect of dropping layers
of pre-trained transformer models. Computer Speech I& Language , 77:101429, jan 2023.
[66] Weihan Chen, Peisong Wang, and Jian Cheng. Towards mixed-precision quantization of
neural networks via constrained optimization. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 5350–5359, 2021.
[67] Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki Yoshiyama, Javier Alonso Garcia,
Stephen Tiedemann, Thomas Kemp, and Akira Nakamura. Mixed precision dnns: All you need
is a good parametrization. arXiv preprint arXiv:1905.11452 , 2019.
[68] Mart Van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen
Blankevoort, and Max Welling. Bayesian bits: Unifying quantization and pruning. Advances in
neural information processing systems , 33:5741–5752, 2020.
[69] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.
[70] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531 , 2(7), 2015.
[71] Maying Shen, Hongxu Yin, Pavlo Molchanov, Lei Mao, Jianna Liu, and Jose M Alvarez. Halp:
hardware-aware latency pruning. arXiv preprint arXiv:2110.10811 , 2021.
[72] Yanyu Li, Pu Zhao, Geng Yuan, Xue Lin, Yanzhi Wang, and Xin Chen. Pruning-as-search:
Efficient neural architecture search via channel pruning and structural reparameterization. arXiv
preprint arXiv:2206.01198 , 2022.
13

--- PAGE 14 ---
Appendix
A Re-parameterization for Quantization
In this section, we present the parameterization of Wifor quantization. Similar to the main paper,
we consider a FFN. For each layer of the network, we would like to search over {1,2,4. . . B}bit
quantizations of its weights3. LetWibe the weight matrix of layer i. Let clip(Wi;ri,l, ri,u)be the
weight matrix restricted to [ri,l, ri,u]
clip(Wi;ri,l, ri,u) =ri,u−ReLU (ri,u−ri,l−ReLU (Wi−ri,l)).
When clear from the context, we use the short hand notation clip(Wi)to denote clip(Wi;ri,l, ri,u).
LetWi,bto be the b-bit quantization of Wi, and let r(b)
i,l, r(b)
i,ube the range parameters associated with
Wi,b.Wi,bis obtained by uniformly gridding the range (r(b)
i,u−r(b)
i,l)into2bpoints and assigning
each element of Wito its nearest grid point
Wi,b=r(b)
i,l+r(b)
i,u−r(b)
i,l
2b−1$
clip(Wi)−r(b)
i,l
(r(b)
i,u−r(b)
i,l)/(2b−1)'
.
Here⌊·⌉denotes the round-to-nearest-integer function. To choose between Wi,1, Wi,2. . . W i,B, we
introduce binary mask variables αi,1, αi,2. . . α i,B. This leads us to the following parameterization
ofWi
αi,1(Wi,1+αi,2(Wi,2−Wi,1+αi,4(Wi,4−Wi,2+αi,8(. . .)))) (7)
αi,b= 0implies the weights can be parameterized with fewer than bbits. Observe that the above
expression can be rewritten as
αi,1(1−αi,2)Wi,1+αi,1αi,2(1−αi,4)Wi,2+αi,1αi,2αi,4(1−αi,8)Wi,4. . .
The FLOPs needed to compute the output of this layer is given by
[∥αi,1(1−αi,2)∥0+ 2∥αi,1αi,2(1−αi,4)∥0+ 4∥αi,1αi,2αi,4(1−αi,8)∥0+. . .]didi+1
Since searching for binary masks is computationally intractable, we make them continuous; that
is, we let αi,b∈[0,1],∀b∈ {1,2,4, . . . B}. We consider a continuous FLOPs surrogate which is
obtained by computing ℓ1/ℓ2norm of
[αi,1(1−αi,2),2αi,1αi,2(1−αi,4),4αi,1αi,2αi,4(1−αi,8). . .].
This leads us the following regularizer
αi,1(1−αi,2) + 2αi,1αi,2(1−αi,4) + 4αi,1αi,2αi,4(1−αi,8). . .p
(αi,1(1−αi,2))2+ (2αi,1αi,2(1−αi,4))2+ (4αi,1αi,2αi,4(1−αi,8))2. . .didi+1.
Remark 1. Ourℓ1/ℓ2regularizer typically outputs αi’s that are close to 0/1. In many cases these
in fact turn out to be exactly equal to 0/1. In the scenario where they are not equal to 0,1, we project
αi,b’s to{0,1}. This makes sure we have quantized weights.
Remark 2. There are several other works that have attempted to learn the amount of quantiza-
tion/precision to use at each layer [ 66,67,68]. However, unlike our work, these works do not directly
optimize for FLOPs, latency. We would like to note that our parameterization is closely related to the
parameterization of [68].
Straight Through Estimator (STE). Note that the training objective for quantization is non-
differentiable. So, in our experiments, we use STE to optimize the objective [ 69]. This is a standard
technique for performing quantization aware training.
3Bis typically a power of 2
14

--- PAGE 15 ---
/uni00000014/uni00000012/uni00000018 /uni00000014/uni00000012/uni0000001a /uni00000014/uni00000012/uni0000001c /uni00000015/uni00000012/uni00000014
/uni00000036/uni00000049/uni00000050/uni00000045/uni00000058/uni0000004d/uni0000005a/uni00000049/uni00000004/uni00000031/uni00000025/uni00000027/uni00000057/uni00000018
/uni00000017
/uni00000016
/uni00000015
/uni00000014/uni00000028/uni0000004d/uni0000004a/uni0000004a/uni00000049/uni00000056/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni0000004d/uni00000052/uni00000004/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni00000031/uni00000032/uni00000030/uni0000002d
/uni00000014/uni00000012/uni00000018 /uni00000014/uni00000012/uni0000001a /uni00000014/uni00000012/uni0000001c /uni00000015/uni00000012/uni00000014
/uni00000036/uni00000049/uni00000050/uni00000045/uni00000058/uni0000004d/uni0000005a/uni00000049/uni00000004/uni00000031/uni00000025/uni00000027/uni00000057/uni00000016/uni00000012/uni00000014
/uni00000015/uni00000012/uni00000019
/uni00000015/uni00000012/uni00000014
/uni00000014/uni00000012/uni00000019
/uni00000014/uni00000012/uni00000014/uni00000028/uni0000004d/uni0000004a/uni0000004a/uni00000049/uni00000056/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni0000004d/uni00000052/uni00000004/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni00000031/uni00000036/uni00000034/uni00000027
/uni00000014/uni00000012/uni00000018 /uni00000014/uni00000012/uni0000001a /uni00000014/uni00000012/uni0000001c /uni00000015/uni00000012/uni00000014
/uni00000036/uni00000049/uni00000050/uni00000045/uni00000058/uni0000004d/uni0000005a/uni00000049/uni00000004/uni00000031/uni00000025/uni00000027/uni00000057/uni0000001a
/uni00000018
/uni00000016
/uni00000014/uni00000028/uni0000004d/uni0000004a/uni0000004a/uni00000049/uni00000056/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni0000004d/uni00000052/uni00000004/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni00000035/uni00000032/uni00000030/uni0000002d
/uni00000014/uni00000012/uni00000018 /uni00000014/uni00000012/uni0000001a /uni00000014/uni00000012/uni0000001c /uni00000015/uni00000012/uni00000014
/uni00000036/uni00000049/uni00000050/uni00000045/uni00000058/uni0000004d/uni0000005a/uni00000049/uni00000004/uni00000031/uni00000025/uni00000027/uni00000057/uni00000015/uni00000012/uni00000014
/uni00000014/uni00000012/uni00000019
/uni00000014/uni00000012/uni00000014/uni00000028/uni0000004d/uni0000004a/uni0000004a/uni00000049/uni00000056/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni0000004d/uni00000052/uni00000004/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni00000035/uni00000035/uni00000034
/uni00000014/uni00000012/uni00000018 /uni00000014/uni00000012/uni0000001a /uni00000014/uni00000012/uni0000001c /uni00000015/uni00000012/uni00000014
/uni00000036/uni00000049/uni00000050/uni00000045/uni00000058/uni0000004d/uni0000005a/uni00000049/uni00000004/uni00000031/uni00000025/uni00000027/uni00000057/uni00000016/uni00000012/uni00000014
/uni00000015/uni00000012/uni00000019
/uni00000015/uni00000012/uni00000014
/uni00000014/uni00000012/uni00000019
/uni00000014/uni00000012/uni00000014/uni00000028/uni0000004d/uni0000004a/uni0000004a/uni00000049/uni00000056/uni00000049/uni00000052/uni00000047/uni00000049/uni00000004/uni0000004d/uni00000052/uni00000004/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni00000037/uni00000037/uni00000038Figure 6: Fine-tuning tradeoffs of BERT on GLUE benchmark.
.
B Implementation and experimental details
In this section, we provide additional details about the implementation of our technique. We warm-
start our pruning procedure with the pre-trained model ( i.e.,f∗in Section 3) that is made available
to us. In our experiments, we noticed that this speeds up the convergence of our algorithm. For
both MobileNetV3 and BERT compression, we rely on simultaneous pruning, low-rank factorization
of weights (see Table 1 for details). Here, we parameterize weights WiasUidiag(βi)Vidiag(αi);
setting entries of βito0helps reduce the rank of the weight matrix, and αihelps in pruning. We
initialize Ui, Vi, βiby performing SVD on the weight matrices of the pre-trained network. In our
experiments, we apply our technique only to the 1×1convolution layers in the network, for which
the formulation of our regularizer remains the same as the one described in the preceding text. We
anneal our regularization strength λ, increasing it linearly in order to stabilize the training. Finally, we
fine-tune the model returned by our pruning algorithm on the training data to improve its performance
(this simply involves setting the FLOPs regularization coefficient to 0). During the fine-tuning and
pruning phases, we leverage the pre-trained model by adding a distillation loss to the standard cross-
entropy loss [70]. We perform distillation between the logits of the pre-trained model and the logits
of the model being fine-tuned.
B.1 ImageNet Pretraining
Our algorithm was implemented using TensorFlow 2.0. We use the pre-trained MobileNetV3 (or
EfficientNet) models provided in this framework to warm-start our models. We initialize Ui, βi, Vi
to the SVD of the 1x1 convolution filters, and the entries of αiuniformly at random between
[0,0.5]. We use Adam for optimization with its default parameters, and searched over learning rates
in the set {10−4,5×10−5,10−5}, with cosine decay, which is standard practice. The distillation
coefficient was searched among {0.1,0.25,0.5,0.9}and the distillation temperature was searched
among {2,3,4}. For our ImageNet experiments, We trained our model for 70000 steps, linearly
annealing the regularizer for the first 50000 steps. We fine-tuned the obtained model for another
50000 steps. For the transfer learning experiments, we reduced these to 25000 for training and 15000
for fine-tuning. We used a batch size of 2048 for all experiments. Our regularizer coefficient was
varied from 10−8to10−6. This range was determined by looking at the magnitude of the cross-
entropy loss and the FLOPs regularizer, and making sure that they are similar. MobileNet pre-training
took up around 13 hours.
B.2 Transfer Learning
BERT. For the BERT fine-tuning experiments, we start off with a pre-trained BERT model and
introduce our parameterization in a similar manner as described above. We use AdamW to optimize,
and search over learning rates among {10−4,5×10−5,10−5}. Our regularizer coefficient was varied
from 10−7to5∗10−6. Each fine-tuning run taking between 20 mins - 1 hour.
EfficientNet, MobileNet. For EfficientNet and MobileNet experiments, we have similar experimen-
tal setup and hyper-parameter search space as MobileNet ImageNet pretraining described in App B.1,
with the exception that we do not do any model distillation. We also use RMSProp for EfficientNet
with exponential decay as the LR schedule, as this was the optimizer of choice for its pre-training.
We train for 25000 steps with the regularizer, and fine-tune for another 25000 steps.
15

--- PAGE 16 ---
B.3 Quantization
We train a CNN with four convolution layers, with [64,128,256,128] filters and kernel size of 3 with
stride being 1 for each layer. We additionally have batch-norm layers after each conv layer. We
search for learning rate over {1e-4, 5e-4, 1e-3, 5e-3} for the baseline and our model, and regularizer
coefficient over {1e-9, 3e-9, 5e-9, 7e-9, 1e-8}. We train for 100 epochs with a batch size of 512 on a
single V100 GPU, and use Adam with CosineDecay for the learning rate.
B.4 Latency Tables
As mentioned in the main paper, the actual on-device latencies are calculated on Pixel6 for our
latency experiments. We populated the latency lookup-table Tspecified in Section 3.3 by profiling
the corresponding 1×1convolution/matrix-vector multiplication latency, on the device. Note that
the convolution operation is much better optimized than matrix multiplication operation on the Pixel6
kernel. Hence, for our latency experiments on MobileNet, the latency table was populated by profiling
the1×1convolution operations.
A 1x1 convoultion operation is identified by input dimension, input channels and the number of filters
(output channels). Strides can also be different but all 1x1 convolutions in the MobileNet architecture
have stride 1. In the MobileNet architecture we encounter feature maps with input dimensions
indim∈I={1,7,14,28,56,112,224}. Moreover, the input ( inc) and output channels ( outc) are
constrained by inc,outc∈D={d|∀d∈Nandd <1281}. Hence we construct the table T, each
member of which can be accessed via T(indim,inc,outc). Note that profiling T(indim,inc,outc). for
every possible value of (indim,inc,outc)∈I×D×Dis expensive. We must therefore pick certain
tuples (inc,outc)for each indim∈Ifor which we calculate actual on-device latencies. The rest of
the table is populated using linear interpolation. We pick these tuples such that they cover the 1×1
convolutions that are encountered in the MobileNet Architecture. For indim=α, letβdenote the
maximum possible value of inc, andγdenote the maximum possible value of outcin MobileNet. We
construct set Pinwhich denotes values that are likely to be encountered by the regularizer for incand
similarly Poutforoutc. Finally, the actual on-device latencies are calculated for T(α, Pin×Pout).
Construction of PinandPoutis done by choosing an appropriate θand adding all values in the range
(β−θ, β]toPin, and(γ−θ, γ]toPout. Also, from the remaining ranges i.e. (0, β−θ]and(0, γ−θ]
points are sampled exponentially by choosing the midpoint of the range every time and changing the
lower limit of the range to the midpoint for certain iterations.
The experimental setting and hyper-parameter configurations we use for latency table experiments is
same as the one for FLOPs experiments (see Section B.1).
C Additional Experimental Results
C.1 Pretraining ResNet on ImageNet
In this section, we present additional experimental results to demonstrate the generality of our
approach. We compress the ResNet architecture for ImageNet classification, using our method.
In particular, we compress the 1×1convolutions using pruning and low-rank factorization. We
compare our method against HALP [ 71] and PAS [ 72], two state of the art methods for architecture
search and compression for ResNet. Our method compresses ResNet-101 to a model with similar
FLOPs as ResNet-50, while simultaneously achieving better performance than the baseline ResNet-
50. Furthremore, our technique outperformns SOTA methods for the same number of FLOPs, as seen
in Fig 7. We use the same hyper-parameters as those described in Sec B.1, but we vary the FLOP
regularizer coefficient between [1e−10,1e−9]since ResNet models have a higher number of FLOPs.
C.2 Ablation Studies
Effect of sparsity norm. In section 3 we provided small scale experiments to justify our design
choices of using projected-Adam andℓ1
ℓ2norm. In this section we perform large-scale ablation studies
on MobileNetV3 for ImageNet training. The results from this experiment are presented in Figure 8.
Without projected-Adam, we notice that the optimization algorithm doesn’t converge to sparse
solutions. Consequently, the resulting models do not have large reduction in MACs. The accuracy of
16

--- PAGE 17 ---
/uni00000016/uni00000012/uni00000014 /uni00000016/uni00000012/uni00000019 /uni00000017/uni00000012/uni00000014 /uni00000017/uni00000012/uni00000019 /uni00000018/uni00000012/uni00000014
/uni00000031/uni00000025/uni00000027/uni00000057/uni00000004/uni0000000c/uni0000004d/uni00000052/uni00000004/uni00000026/uni00000052/uni0000000d/uni0000001b/uni00000019/uni00000012/uni00000019/uni0000001b/uni0000001a/uni00000012/uni00000014/uni0000001b/uni0000001a/uni00000012/uni00000019/uni0000001b/uni0000001b/uni00000012/uni00000014/uni0000001b/uni0000001b/uni00000012/uni00000019/uni0000001b/uni0000001c/uni00000012/uni00000014 /uni00000038/uni00000053/uni00000054/uni00000011/uni00000015/uni00000004/uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d/uni00000004/uni0000000c/uni0000002d/uni00000051/uni00000045/uni0000004b/uni00000049/uni00000032/uni00000049/uni00000058/uni0000000d
/uni00000033/uni00000059/uni00000056/uni00000057
/uni0000002c/uni00000025/uni00000030/uni00000034
/uni00000034/uni00000025/uni00000037Figure 7: Pruning ResNet on ImageNet : We compare against HALP and PAS, two recent SOTA
techniques to prune ResNet-50, and achieve better performance over different FLOP regimes.
/uni00000015/uni00000014/uni00000014 /uni00000015/uni00000019/uni00000014 /uni00000016/uni00000014/uni00000014
/uni00000031/uni00000025/uni00000027/uni00000057/uni00000004/uni0000000c/uni0000004d/uni00000052/uni00000004/uni00000031/uni00000052/uni0000000d/uni0000001a/uni00000019/uni0000001b/uni00000014/uni0000001b/uni00000019 /uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni0000003a/uni00000045/uni00000056/uni0000004d/uni00000053/uni00000059/uni00000057/uni00000004/uni00000052/uni00000053/uni00000056/uni00000051/uni00000057
1p
2
1
2
1p
/uni00000015/uni00000014/uni00000014 /uni00000015/uni00000019/uni00000014 /uni00000016/uni00000014/uni00000014
/uni00000031/uni00000025/uni00000027/uni00000057/uni00000004/uni0000000c/uni0000004d/uni00000052/uni00000004/uni00000031/uni00000052/uni0000000d/uni0000001b/uni00000014/uni0000001b/uni00000019 /uni00000025/uni00000047/uni00000047/uni00000059/uni00000056/uni00000045/uni00000047/uni0000005d
/uni0000003a/uni00000045/uni00000056/uni0000004d/uni00000053/uni00000059/uni00000057/uni00000004/uni00000029/uni0000004a/uni0000004a/uni0000004d/uni00000047/uni0000004d/uni00000049/uni00000052/uni00000058/uni00000004/uni00000026/uni00000050/uni00000053/uni00000047/uni0000004f/uni00000057
/uni00000030/uni00000036/uni00000004/uni0000000f/uni00000004/uni00000034/uni00000056/uni00000059/uni00000052/uni0000004d/uni00000052/uni0000004b
/uni00000034/uni00000056/uni00000059/uni00000052/uni0000004d/uni00000052/uni0000004b
/uni00000031/uni00000053/uni00000046/uni0000004d/uni00000050/uni00000049/uni00000032/uni00000049/uni00000058/uni0000003a/uni00000017
Figure 8: Ablation studies on ImageNet : We compare using ℓ1andℓ1
ℓ2norms in our regularizer,
with subscript pindicating that projected-Adam was used for optimization. We also experiment with
combining low-rank (LR) factorization with channel pruning.
these models also takes a big hit. On the other hand, using ℓ1norm based FLOPs regularizer with
projected-Adam suffers from the scaling issue described in Sec 3.2. This leads to a large fraction of
channels being pruned for some blocks, producing a model with deteriorated accuracy. Our method
has 2-4% better accuracy in the high and mid FLOPs regimes than these alternatives.
Comparing different building blocks. In Table 1, we described ways to integrate various building
blocks into our framework. In Figure 8, we demonstrate the accuracy vsinference time trade-offs
of using two of these building blocks in our framework, namely Pruning and Pruning+Low-rank
Factorization. We find that the extra flexibility provided by the Low-Rank Factorization leads to
models with fewer MACs for the same accuracy, and the difference is even more pronounced for
smaller models. We note that channel pruning alone can give us 10% reduction in MACs over the
MobileNetV3 family at the same accuracy level. In particular, at 73.4%accuracy, our model has
136Mn MACs compared to 155Mn MACs of the MobileNetV3 family model. Similarly, at 75.5%
accuracy, our model has 198Mn MACs comopared to 216Mn MACs of MobileNetV3 family model.
Adding Low-Rank structure introduces another 5% reduction in MACs over the gains from channel
pruning, with no loss in accuracy. This also shows the effectiveness of our algorithm across multiple
building blocks. Combining other efficient blocks such as block structured sparsity, quantization is a
direction for future investigation.
D Combination of building blocks
Table 2 presents the parameterization of weight matrices that lets us search over multiple building
blocks simultaneously.
17

--- PAGE 18 ---
Table 2: Table describing regularizers used by our technique for various efficient building blocks.
Efficient
Building BlockParameterization of WiFLOPs
(ithlayer)Regularizer (FLOPs surrogate)
(ithlayer)
Pruning +
Unstructured Sparsity(Wi⊙βi)×diag(αi),
⊙is the elementwise
multiplication operator∥Vec(βi×diag(αi))∥0√
didi+1∥Vec(βi×diag(αi))∥1p
∥Vec(βi×diag(αi))∥2
Pruning +
Quantization
(1,2,4bit quantization)
Wi,1+αi,2(∆i,2+αi,4(∆i,4))
×diag(βi),
∥(1−αi,2)∥0+
2∥αi,2(1−αi,4)∥0+
4∥αi,2αi,4∥0
× ∥βi∥0∥βi+1∥0
ℓ1
ℓ2norm of
the vector [ (1−αi,2),
2αi,2(1−αi,4),4αi,2αi,4]
×√di∥βi∥1p
∥βi∥2√
di+1∥βi+1∥1p
∥βi+1∥2
E Limitations and Broader Impact
One limitation of our work is that we only study popular building blocks such as sparsity, pruning,
low-rank factorization and quantization. Extending our work to more building blocks such as block
sparsity and other forms of structured sparsity is an interesting future direction. Another limitation,
which is related to the implementation of our technique, is the need to manually implement the
FLOPs regularizer for various architectures. An automated solution that takes in any architecture and
computes the FLOPs regularizer would make our framework easy to use.
In terms of broader impact, we believe our technique can be used to find more efficient architectures
for large language models such as GPT. This can help democratize these models, and also reduce
their carbon footprint.
18
