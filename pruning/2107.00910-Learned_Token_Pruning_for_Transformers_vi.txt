# Tỉa Token Học được cho Transformers

Sehoon Kim∗
sehoonkim@berkeley.edu
Đại học California, Berkeley
Berkeley, CA, Hoa KỳSheng Shen∗
sheng.s@berkeley.edu
Đại học California, Berkeley
Berkeley, CA, Hoa KỳDavid Thorsley∗
d.thorsley@samsung.com
Samsung Semiconductor, Inc.
San Jose, CA, Hoa Kỳ

Amir Gholami∗
amirgh@berkeley.edu
Đại học California, Berkeley
Berkeley, CA, Hoa KỳWoosuk Kwon
woosuk.kwon@berkeley.edu
Đại học California, Berkeley
Berkeley, CA, Hoa KỳJoseph Hassoun
j.hassoun@samsung.com
Samsung Semiconductor, Inc.
San Jose, CA, Hoa Kỳ

Kurt Keutzer
keutzer@berkeley.edu
Đại học California, Berkeley
Berkeley, CA, Hoa Kỳ

TÓM TẮT
Việc triển khai hiệu quả các mô hình transformer trong thực tế là thách thức do chi phí suy luận của chúng bao gồm bộ nhớ, độ trễ và tiêu thụ điện năng, có độ phức tạp tăng theo hàm bậc hai với độ dài chuỗi đầu vào. Để giải quyết vấn đề này, chúng tôi trình bày một phương pháp giảm token mới được gọi là Tỉa Token Học được (LTP) nhằm loại bỏ thích ứng các token không quan trọng khi một chuỗi đầu vào đi qua các lớp transformer. Cụ thể, LTP tỉa các token có điểm attention dưới một ngưỡng, có giá trị được học cho mỗi lớp trong quá trình huấn luyện. Phương pháp dựa trên ngưỡng của chúng tôi cho phép độ dài của chuỗi đã tỉa thay đổi thích ứng dựa trên chuỗi đầu vào, và tránh các phép toán tốn kém về mặt thuật toán như việc chọn top-k token. Chúng tôi thử nghiệm rộng rãi hiệu suất của LTP trên các tác vụ GLUE và SQuAD và chỉ ra rằng phương pháp của chúng tôi vượt trội hơn các phương pháp tỉa token tiên tiến trước đây lên đến ~2.5% độ chính xác cao hơn với cùng lượng FLOPs. Đặc biệt, LTP đạt được giảm FLOPs lên đến 2.1× với độ giảm chính xác dưới 1%, dẫn đến cải thiện thông lượng lên đến 1.9× và 2.0× trên CPU Intel Haswell và GPU NVIDIA V100. Hơn nữa, chúng tôi chứng minh rằng LTP mạnh mẽ hơn các phương pháp trước đây đối với sự biến thiên trong độ dài chuỗi đầu vào. Mã nguồn của chúng tôi đã được phát triển trong PyTorch và mã nguồn mở¹.

KHÁI NIỆM CCS
•Tổ chức hệ thống máy tính →Mạng neural ;Xử lý ngôn ngữ tự nhiên .

∗Đóng góp bằng nhau.
¹https://github.com/kssteven418/LTP

Quyền được cấp để tạo bản sao kỹ thuật số hoặc bản cứng của toàn bộ hoặc một phần công trình này cho mục đích cá nhân hoặc lớp học mà không tính phí với điều kiện các bản sao không được tạo hoặc phân phối vì lợi nhuận hoặc lợi thế thương mại và các bản sao phải ghi rõ thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần của công trình này thuộc sở hữu của các bên khác ngoài ACM phải được tôn trọng. Việc tóm tắt có ghi nguồn được cho phép. Để sao chép nếu không, hoặc tái xuất bản, để đăng trên máy chủ hoặc phân phối lại thành danh sách, cần có sự cho phép cụ thể trước và/hoặc một khoản phí. Yêu cầu quyền từ permissions@acm.org.
KDD '22, 14-18 tháng 8, 2022, Washington, DC, Hoa Kỳ.
©2022 Hiệp hội Máy tính ACM.
ACM ISBN 978-1-4503-9385-0/22/08. . . $15.00
https://doi.org/10.1145/3534678.3539260

TỪ KHÓA
Học Sâu, Tỉa Mạng, Xử lý Ngôn ngữ Tự nhiên

Định dạng Tham khảo ACM:
Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, và Kurt Keutzer. 2022. Tỉa Token Học được cho Transformers. Trong Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), 14-18 tháng 8, 2022, Washington, DC, Hoa Kỳ. ACM, New York, NY, Hoa Kỳ, 11 trang. https://doi.org/10.1145/3534678.3539260

1 GIỚI THIỆU
Các kiến trúc mạng neural sâu dựa trên Transformer [45], như BERT [7] và RoBERTa [28], đạt được kết quả tiên tiến trong các tác vụ Xử lý Ngôn ngữ Tự nhiên (NLP) như phân loại câu và trả lời câu hỏi. Tuy nhiên, việc triển khai hiệu quả các mô hình này ngày càng trở nên thách thức do kích thước lớn, nhu cầu suy luận thời gian thực, và các tài nguyên năng lượng, tính toán và bộ nhớ có hạn sẵn có. Trung tâm của một lớp transformer là cơ chế multi-head self-attention, nơi mỗi token trong chuỗi đầu vào attend đến mọi token khác để tính toán một biểu diễn mới của chuỗi. Bởi vì tất cả các token attend đến nhau, độ phức tạp tính toán là bậc hai đối với độ dài chuỗi đầu vào; do đó khả năng áp dụng các mô hình transformer cho các chuỗi đầu vào dài trở nên hạn chế.

Tỉa là một kỹ thuật phổ biến để giảm kích thước của mạng neural và lượng tính toán cần thiết. Tỉa không có cấu trúc cho phép các mẫu thưa thớt tùy ý cho các tham số và feature map và về lý thuyết có thể tạo ra tiết kiệm tính toán đáng kể trong khi bảo toàn độ chính xác. Tuy nhiên, các bộ gia tốc DNN thông thường không thể khai thác hiệu quả các mẫu thưa thớt không có cấu trúc. Do đó, các phương pháp tỉa có cấu trúc thường được ưa chuộng trong thực tế do tính dễ triển khai tương đối trên phần cứng.

Multi-head self-attention cung cấp một số khả năng cho tỉa có cấu trúc; ví dụ, tỉa head [30,46] giảm kích thước mô hình bằng cách loại bỏ các head không cần thiết trong mỗi lớp transformer. Một cách tiếp cận trực giao khác mà chúng tôi xem xét trong bài báo này là tỉa token, giảm tính toán bằng cách loại bỏ dần các token không quan trọng trong chuỗi trong quá trình suy luận. Đối với các tác vụ NLP như phân loại câu, tỉa token là một cách tiếp cận hấp dẫn để xem xét vì nó khai thác quan sát trực quan rằng không phải tất cả các token (tức là từ) trong một câu đầu vào đều cần thiết để thực hiện suy luận thành công.

Có hai lớp chính của các phương pháp tỉa token. Trong lớp đầu tiên, các phương pháp như PoWER-BERT [13] và Length-Adaptive Transformer (LAT) [21] tìm kiếm một cấu hình tỉa token duy nhất (tức là độ dài chuỗi cho mỗi lớp) cho toàn bộ tập dữ liệu. Nói cách khác, chúng tỉa tất cả các chuỗi đầu vào về cùng một độ dài. Tuy nhiên, độ dài chuỗi đầu vào có thể thay đổi rất nhiều trong các tác vụ và giữa các tập huấn luyện và validation như trong Hình 1, và do đó việc áp dụng một cấu hình tỉa duy nhất cho tất cả các chuỗi đầu vào có thể tỉa thiếu các chuỗi ngắn hơn hoặc tỉa quá các chuỗi dài hơn.

Trong lớp khác, phương pháp tỉa token điều chỉnh cấu hình dựa trên chuỗi đầu vào. SpAtten [49] sử dụng một cấu hình tỉa tỷ lệ với độ dài câu đầu vào; tuy nhiên, nó không điều chỉnh tỷ lệ các token được tỉa dựa trên nội dung của chuỗi đầu vào. TR-BERT [54] gần đây được công bố sử dụng reinforcement learning (RL) để tìm một mạng chính sách giảm động số lượng token dựa trên độ dài và nội dung của chuỗi đầu vào; tuy nhiên, nó yêu cầu huấn luyện tốn kém bổ sung để hội tụ của phương pháp dựa trên RL. Ngoài ra, tất cả các phương pháp trước đây này dựa vào một phần trên việc chọn k token quan trọng nhất trong quá trình suy luận hoặc huấn luyện. Việc chọn này có thể tốn kém về mặt tính toán mà không có sự phát triển phần cứng chuyên dụng, như engine top-k được giới thiệu trong SpAtten [49].

Để đạt được điều này, chúng tôi đề xuất một phương pháp tỉa token dựa trên ngưỡng học được thích ứng với độ dài và nội dung của các ví dụ riêng lẻ và tránh sử dụng các phép toán top-k. Cụ thể, các đóng góp của chúng tôi như sau:

• Chúng tôi đề xuất Tỉa Token Học được (LTP), một phương pháp tỉa token dựa trên ngưỡng, chỉ cần một phép toán ngưỡng đơn giản để phát hiện các token không quan trọng. Ngoài ra, LTP hoàn toàn tự động hóa việc tìm kiếm cấu hình tỉa tối ưu bằng cách giới thiệu một mask nhị phân mềm có thể vi phân cho phép huấn luyện các ngưỡng chính xác cho các lớp và tác vụ khác nhau. (Phần 3.3)

• Chúng tôi áp dụng LTP cho RoBERTa và đánh giá hiệu suất của nó trên các tác vụ GLUE và SQuAD. Chúng tôi chỉ ra LTP đạt được giảm FLOPs lên đến 2.10× với độ suy giảm chính xác dưới 1%, dẫn đến cải thiện thông lượng lên đến 1.93× và 1.97× trên GPU NVIDIA V100 và CPU Intel Haswell, tương ứng, so với baseline FP16 không tỉa. Chúng tôi cũng chỉ ra rằng LTP vượt trội hơn SpAtten và LAT trong hầu hết các trường hợp, đạt được giảm FLOPs bổ sung cho cùng mức độ giảm chính xác. (Phần 4.2 và 4.5)

• Chúng tôi chỉ ra rằng LTP có tính mạnh mẽ cao đối với các biến thiên độ dài câu. LTP thể hiện độ chính xác nhất quán tốt hơn trên các phân phối độ dài câu khác nhau, đạt được khoảng cách chính xác lên đến 16.4% so với LAT. (Phần 4.3)

2 CÔNG TRÌNH LIÊN QUAN

2.1 Transformers Hiệu quả
Nhiều cách tiếp cận khác nhau đã được đề xuất để cải thiện tốc độ và giảm bộ nhớ của transformers. Chúng có thể được phân loại rộng rãi như sau: (i) thiết kế kiến trúc hiệu quả [5,16, 19,23,25,35,44,47,50,57]; (ii) chưng cất kiến thức [18,37,41–43]; (iii) lượng tử hóa [1,2,10,22,39,55,56,58]; và (iv) tỉa. Ở đây, chúng tôi chỉ tập trung vào tỉa và thảo luận ngắn gọn về công trình liên quan.

2.2 Tỉa Transformer
Các phương pháp tỉa có thể được phân loại thành tỉa không có cấu trúc và tỉa có cấu trúc. Đối với tỉa không có cấu trúc, giả thuyết lottery-ticket [11] đã được khám phá cho transformers trong [4,31]. Gần đây, [59] tận dụng tỉa như một cách hiệu quả để tinh chỉnh transformers trên các tác vụ downstream. [38] đề xuất movement pruning, đạt được cải thiện hiệu suất đáng kể so với các phương pháp dựa trên magnitude trước đây bằng cách xem xét việc sửa đổi trọng số trong quá trình tinh chỉnh. Tuy nhiên, thường khá khó khăn để triển khai hiệu quả độ thưa không có cấu trúc trên các bộ gia tốc neural thông thường để có được tăng tốc có ý nghĩa.

Vì lý do này, một số phương pháp tỉa có cấu trúc đã được giới thiệu để loại bỏ các tập tham số có cấu trúc. [30,46] loại bỏ các attention head trong các lớp multi-head attention, và [9,36] tỉa toàn bộ các lớp transformer. [51] tỉa có cấu trúc các ma trận trọng số thông qua phân tích thừa số rank thấp, và [20,27] cố gắng tỉa kết hợp

các attention head và bộ lọc của ma trận trọng số. [15,29] xác định động tỷ lệ tỉa có cấu trúc trong quá trình suy luận. Các sơ đồ tỉa khối gần đây chia các ma trận trọng số thành nhiều khối và tỉa chúng dựa trên tối ưu hóa group Lasso [26], regularization thích ứng [53], và movement pruning [24]. Tất cả các phương pháp này tương ứng với tỉa trọng số, nơi các tham số mô hình (tức là trọng số) được tỉa.

Gần đây, đã có công trình về tỉa các câu đầu vào cho transformers, thay vì các tham số mô hình. Điều này được gọi là tỉa token, nơi các token ít quan trọng hơn được loại bỏ dần trong quá trình suy luận. PoWER-BERT [13], một trong những công trình sớm nhất, đề xuất học trực tiếp các cấu hình tỉa token. LAT [21] mở rộng ý tưởng này bằng cách giới thiệu LengthDrop, một quy trình trong đó một mô hình được huấn luyện với các cấu hình tỉa token khác nhau, sau đó là một tìm kiếm tiến hóa. Phương pháp này có lợi thế là quy trình huấn luyện trước không cần phải lặp lại cho các tỷ lệ tỉa khác nhau của cùng một mô hình. Trong khi các phương pháp này đã cho thấy giảm tính toán lớn trên các tác vụ downstream NLP khác nhau, chúng cố định một cấu hình tỉa token duy nhất cho toàn bộ tập dữ liệu. Nghĩa là, chúng tỉa tất cả các chuỗi đầu vào về cùng độ dài. Tuy nhiên, như được thể hiện trong Hình 1, độ dài chuỗi đầu vào thay đổi rất nhiều trong một tác vụ. Hệ quả là, việc cố định một cấu hình tỉa duy nhất có thể tỉa thiếu các chuỗi ngắn hơn để giữ lại đủ token cho việc xử lý các chuỗi dài hơn hoặc, ngược lại, tỉa quá các chuỗi dài hơn để loại bỏ đủ token để xử lý hiệu quả các chuỗi ngắn hơn. Quan trọng hơn, một cấu hình tỉa duy nhất thiếu tính mạnh mẽ đối với các biến thiên độ dài chuỗi đầu vào, nơi các câu đầu vào tại thời điểm suy luận dài hơn những câu trong tập dữ liệu huấn luyện [32].

Ngược lại, SpAtten [49] vượt qua vấn đề này bằng cách gán một cấu hình tỉa tỷ lệ với độ dài chuỗi đầu vào. Trong khi điều này cho phép tỉa nhiều token hơn từ các chuỗi dài hơn và ít token hơn từ các chuỗi ngắn hơn, nó không thích ứng với các chuỗi đầu vào riêng lẻ vì nó gán cùng một cấu hình cho tất cả các chuỗi có cùng độ dài bất kể nội dung của chúng. Ngoài ra, các cấu hình tỉa được xác định theo kinh nghiệm và do đó có thể dẫn đến một giải pháp không tối ưu. Gần đây, TR-BERT [54] đề xuất học một mạng chính sách RL để áp dụng các cấu hình tỉa thích ứng cho mỗi chuỗi đầu vào. Tuy nhiên, như được lưu ý bởi các tác giả, vấn đề có không gian tìm kiếm lớn có thể khó cho RL giải quyết. Vấn đề này được giảm thiểu bằng các heuristic liên quan đến imitation learning và sampling của các chuỗi hành động, điều này làm tăng đáng kể chi phí huấn luyện. Quan trọng là, tất cả các phương pháp tỉa token nói trên phụ thuộc một phần hoặc toàn bộ vào phép toán top-k để chọn k token quan trọng nhất trong quá trình suy luận hoặc huấn luyện. Phép toán này có thể tốn kém mà không có hỗ trợ phần cứng chuyên dụng, như được thảo luận trong [49]. Mặt khác, LTP dựa trên một chiến lược tỉa dựa trên ngưỡng hoàn toàn có thể học được. Do đó, nó là (i) thích ứng với cả độ dài và nội dung đầu vào, (ii) mạnh mẽ đối với các biến thiên độ dài câu, (iii) hiệu quả về mặt tính toán, và (iv) dễ triển khai.

3 PHƯƠNG PHÁP LUẬN

3.1 Nền tảng
BERT [7] bao gồm nhiều lớp encoder transformer [45] được xếp chồng lên nhau. Một lớp encoder transformer cơ bản bao gồm một khối multi-head attention (MHA) theo sau là một khối feed-forward điểm (FFN), với các kết nối residual xung quanh mỗi khối. Cụ thể, một MHA bao gồm Nh head được tham số hóa độc lập. Một attention head h trong lớp l được tham số hóa bởi W_k^(h,l), W_q^(h,l), W_v^(h,l) ∈ R^(d_h×d), W_o^(h,l) ∈ R^(d×d_h), trong đó d_h thường được đặt thành d/N_h và d là chiều đặc trưng. Chúng tôi bỏ chỉ số trên l để đơn giản trong công thức sau. MHA đo tầm quan trọng theo cặp của mỗi token đối với mọi token khác trong đầu vào:

MHA(x) = Σ(h=1 đến N_h) Att_W_k,q,v,o^(h)(x), (1)

trong đó x ∈ R^(d×n) là chuỗi đầu vào có độ dài chuỗi n, và Att_W_k,q,v,o là:

Att_W_k,q,v,o(x) = W_o Σ(i=1 đến n) W_v x_i softmax(x^T W_q^T W_k x_i / √d), (2)

x_MHA = LN(Att_W_k,q,v,o(x) + x), (3)

trong đó Eq. 3 là kết nối residual và LayerNorm (LN) theo sau. Đầu ra của MHA sau đó được đưa vào khối FFN áp dụng hai lớp feed-forward cho đầu vào này:

FFN(x_MHA) = σ(W_2(W_1 x_MHA + b_1) + b_2), (4)

x_out = LN(FFN(x_MHA) + x_MHA), (5)

trong đó W_1, W_2, b_1 và b_2 là các tham số FFN, và σ là hàm kích hoạt (thường là GELU cho BERT).

3.2 Tỉa Token Ngưỡng
Hãy ký hiệu xác suất attention của head h giữa token x_i và x_j là A^(h,l):

A^(h,l)(x_i, x_j) = softmax(x^T W_q^T W_k x / √d)(i,j) ∈ R. (6)

Chi phí độ phức tạp tính toán để tính ma trận attention là O(d²n + n²d), tăng theo hàm bậc hai với độ dài chuỗi. Do đó, phép toán attention trở thành nút thắt cổ chai khi áp dụng cho các chuỗi dài. Để giải quyết điều này, chúng tôi áp dụng tỉa token loại bỏ các token không quan trọng khi đầu vào đi qua các lớp transformer để giảm độ dài chuỗi n cho các khối sau. Điều này được thể hiện sơ đồ trong Hình 2 (Trái).

Để tỉa token, chúng ta phải xác định một metric để xác định các token không quan trọng. Theo [13,21,49], chúng tôi định nghĩa điểm quan trọng của token x_i trong lớp l là:

s^(l)(x_i) = (1/N_h)(1/n) Σ(h=1 đến N_h) Σ(j=1 đến n) A^(h,l)(x_i, x_j). (7)

Một cách trực quan, xác suất attention A^(h,l)(x_i, x_j) được hiểu là lượng chuẩn hóa mà tất cả các token khác x_j attend đến token x_i. Token x_i do đó được coi là quan trọng nếu nó nhận được nhiều attention hơn từ tất cả các token trên tất cả các head, điều này trực tiếp dẫn chúng ta đến phương trình 7. Quy trình tính điểm quan trọng từ xác suất attention được minh họa trong Hình 2 (Phải).

Trong [13,21,49], các token được xếp hạng theo điểm quan trọng và tỉa bằng chiến lược chọn top-k. Cụ thể, token x_i được tỉa tại lớp l nếu điểm quan trọng s^(l)(x_i) của nó nhỏ hơn k giá trị lớn nhất của điểm quan trọng từ tất cả các token. Tuy nhiên, việc tìm k giá trị lớn nhất của điểm quan trọng là không hiệu quả về mặt tính toán mà không có phần cứng chuyên dụng [49]; chúng tôi cung cấp kết quả thực nghiệm cho thấy điều này trong Phần A.2. Thay vào đó, chúng tôi giới thiệu một cách tiếp cận tỉa token dựa trên ngưỡng mới trong đó một token chỉ được tỉa nếu điểm quan trọng của nó dưới một ngưỡng được ký hiệu bởi θ^(l) ∈ R. Cụ thể, chúng tôi định nghĩa một chiến lược tỉa bằng cách áp đặt một mask nhị phân M^(l)(·): {1,...,n} → {0,1} cho biết liệu một token có nên được giữ hay tỉa:

M^(l)(x_i) = {1 nếu s^(l)(x_i) > θ^(l), 0 nếu không. (8)

Lưu ý rằng phép toán này chỉ yêu cầu một toán tử so sánh đơn giản mà không cần bất kỳ tính toán top-k tốn kém nào. Một khi một token được tỉa, nó bị loại trừ khỏi các tính toán trong tất cả các lớp tiếp theo, do đó giảm dần độ phức tạp tính toán hướng tới các lớp đầu ra.

3.3 Ngưỡng Có thể Học được cho Tỉa Token
Một mối quan tâm chính với phương pháp trên là cách xác định các giá trị ngưỡng cho mỗi lớp. Không chỉ các giá trị ngưỡng thay đổi cho các lớp khác nhau, chúng cũng thay đổi giữa các tác vụ khác nhau. Chúng tôi giải quyết điều này bằng cách làm cho các ngưỡng (tức là θ trong Eq. 8) có thể học được. Tuy nhiên, có một số thách thức cần xem xét. Đầu tiên, do tính chất nhị phân của M không có gradient flow cho các token được tỉa. Thứ hai, toán tử M không khả vi ngăn cản gradient flow vào các ngưỡng. Để giải quyết những thách thức này, chúng tôi sử dụng một sơ đồ tỉa mềm mô phỏng tỉa cứng ban đầu trong khi vẫn truyền gradient đến các ngưỡng như được hiển thị trong Hình 3.

Sơ đồ Tỉa Mềm. Trong sơ đồ tỉa mềm, chúng tôi thay thế mask không khả vi M^(l) bằng một mask mềm khả vi sử dụng phép toán sigmoid σ:

M̃^(l)(x_i) = σ((s^(l)(x_i) - θ^(l))/T), (9)

trong đó T là nhiệt độ, và θ^(l) là giá trị ngưỡng có thể học được cho lớp l. Với nhiệt độ T đủ nhỏ, M̃^(l)(x_i) sẽ xấp xỉ chặt chẽ mask cứng M^(l)(x_i) trong Eq. 8. Ngoài ra, thay vì chọn các token để tỉa hoặc giữ dựa trên mask cứng của Eq. 8, chúng tôi nhân mask mềm với activation đầu ra của lớp l. Nghĩa là,

x̃_out^(l) = M̃^(l)(x^(l)) · x_out^(l) (10)
         = M̃^(l)(x^(l)) · LN(FFN(x_MHA^(l)) + x_MHA^(l)), (11)

trong đó x_MHA^(l) là activation đầu ra của MHA trong lớp l. Nếu điểm quan trọng của token x_i dưới ngưỡng một khoảng lớn, activation đầu ra lớp của nó gần bằng không và do đó có ít tác động đến lớp tiếp theo. Ngoài ra, vì token nhận được điểm quan trọng bằng không trong lớp tiếp theo, tức là s^(l+1)(x_i) = 0, nó có khả năng bị tỉa lại. Do đó, sơ đồ tỉa mềm gần như giống hệt về hành vi với tỉa cứng, nhưng dạng khả vi của nó cho phép backpropagation và tối ưu hóa dựa trên gradient để làm cho θ có thể học được. Sau khi (i) huấn luyện chung các tham số mô hình và ngưỡng trên các tác vụ downstream với sơ đồ tỉa mềm, (ii) chúng tôi cố định các ngưỡng và nhị phân hóa mask mềm, và (iii) thực hiện tinh chỉnh tiếp theo các tham số mô hình. Mã giả cho thuật toán ba bước này được đưa ra trong Thuật toán 1. Một cách trực quan, độ lớn của gradient d M̃^(l)(x_i)/dθ^(l) được tối đa hóa khi điểm quan trọng s^(l)(x_i) đủ gần với ngưỡng θ^(l) và trở nên gần bằng không ở nơi khác. Do đó, ngưỡng chỉ có thể được huấn luyện dựa trên các token sắp bị tỉa hoặc giữ lại.

Regularization. Không thể học θ mà không có regularization, vì optimizer thường nhận được giá trị loss tốt hơn nếu tất cả các token đều có mặt. Do đó, chúng tôi thêm một số hạng regularization để phạt mạng nếu các token được để không tỉa. Điều này được thực hiện bằng cách áp đặt một loss L1 trên toán tử masking M̃:

L_new = L + λL_reg trong đó L_reg = (1/L) Σ(l=1 đến L) ||M̃^(l)(x)||₁. (12)

Ở đây, L là hàm loss ban đầu (ví dụ: cross-entropy loss), và λ là tham số regularization. Các giá trị λ lớn hơn dẫn đến tỷ lệ tỉa cao hơn. Toán tử regularization này tạo ra một gradient bổ sung cho ngưỡng:

dL_reg/dθ^(l) = (1/dθ^(l))||M̃^(l)(x)||₁ = Σ(i=1 đến n) dM̃^(l)(x_i)/dθ^(l) (13)

Nếu có nhiều token gần ngưỡng hơn, thì gradient dL_reg/dθ^(l) lớn hơn. Kết quả là, ngưỡng được đẩy đến một giá trị lớn hơn, tỉa nhiều token hơn gần ranh giới ngưỡng.

Bảng 1: So sánh hiệu suất và hiệu quả chi tiết của LTP áp dụng cho RoBERTa base.

[THIS IS TABLE: Performance comparison showing Task, Accuracy Metric, GFLOPs, and Speedup for various tasks including MNLI-m, MNLI-mm, QQP, QNLI, SST-2, STS-B, MRPC, RTE, and SQuAD 2.0]

4 THÍ NGHIỆM

4.1 Thiết lập Thí nghiệm
Chúng tôi triển khai LTP trên RoBERTa base [28] sử dụng repo của HuggingFace² và thử nghiệm trên các tác vụ GLUE (tiếng Anh) [48] và SQuAD 2.0 [33]. Đối với các tác vụ GLUE [48], chúng tôi sử dụng 6 tác vụ để đánh giá bao gồm tương tự câu (QQP [17], MRPC [8], STS-B [3]), phân loại tình cảm (SST-2 [40]), entailment văn bản (RTE [6]) và suy luận ngôn ngữ tự nhiên (MNLI [52], QNLI [34]). Để đánh giá kết quả, chúng tôi đo độ chính xác phân loại và điểm F1 cho MRPC và QQP, Tương quan Pearson và Tương quan Spearman cho STS-B, và độ chính xác phân loại cho các tác vụ còn lại trên các tập validation. Đối với các tác vụ có nhiều metric (tức là MRPC, QQP, STS-B), chúng tôi báo cáo trung bình của chúng. Đối với SQuAD 2.0 [33], là tác vụ hỏi đáp, chúng tôi đo điểm F1 để đánh giá kết quả.

Như đề cập trong Phần 3.3, quy trình huấn luyện của LTP bao gồm hai giai đoạn: tỉa mềm huấn luyện cả tham số mô hình và ngưỡng trên các tác vụ downstream, theo sau là tỉa cứng tinh chỉnh các tham số mô hình với ngưỡng cố định. Chúng tôi cũng so sánh LTP với các phương pháp tỉa token tiên tiến hiện tại của SpAtten [49] và LAT [21] theo các chi tiết triển khai trong các bài báo của họ. Xem A.1 cho chi tiết của quy trình huấn luyện. Chúng tôi sử dụng PyTorch 1.8 trong tất cả các thí nghiệm. Đối với các thí nghiệm tốc độ suy luận CPU, chúng tôi sử dụng CPU Intel Haswell với bộ nhớ 3.75GB của Google Cloud Platform. Đối với các thí nghiệm tốc độ suy luận GPU, chúng tôi sử dụng instance AWS p3.2xlarge có GPU NVIDIA V100 với CUDA 11.1.

Một vấn đề quan trọng trong công trình trước [13,21] là tất cả các chuỗi đầu vào cho một tác vụ cụ thể được đệm đến lũy thừa gần nhất của 2 từ phần trăm thứ 99 của độ dài chuỗi, và sau đó hiệu suất sau tỉa được so sánh với baseline đã đệm. Điều này dẫn đến lợi ích hiệu suất phóng đại so với baseline. Ví dụ, trong [13], các đầu vào từ tập dữ liệu SST-2 được đệm đến 64, trong khi độ dài câu trung bình của nó là 26 (cf. Hình 1). Với cách tiếp cận này, người ta có thể đạt được tăng tốc khoảng 2.5× chỉ bằng cách loại bỏ padding. Do đó, chúng tôi tránh bất kỳ padding bổ sung nào của các chuỗi đầu vào và tất cả tăng tốc và thông lượng chúng tôi báo cáo được so sánh với các baseline không đệm.

²https://github.com/huggingface/transformers/

[Additional sections continue with detailed experimental results and analysis...]
