# Vượt Ra Ngoài Các Token Chú Ý: Kết Hợp Tầm Quan Trọng và Tính Đa Dạng của Token để Tạo Ra Vision Transformers Hiệu Quả

Sifan Long1,2*Zhen Zhao3,2*Jimin Pi2Shengsheng Wang1†Jingdong Wang2†
1Đại học Cát Lâm2Baidu VIS3Đại học Sydney
longsf22@mails.jlu.edu.cn zhen.zhao@sydney.edu.au
wss@jlu.edu.cn fpijimin01, wangjingdong g@baidu.com

Tóm tắt
Vision transformers đã đạt được những cải thiện đáng kể trong các tác vụ thị giác khác nhau nhưng các tương tác bậc hai giữa các token làm giảm đáng kể hiệu quả tính toán. Nhiều phương pháp cắt tỉa đã được đề xuất để loại bỏ các token dư thừa cho vision transformers hiệu quả gần đây. Tuy nhiên, các nghiên cứu hiện tại chủ yếu tập trung vào tầm quan trọng của token để bảo tồn các token chú ý cục bộ nhưng hoàn toàn bỏ qua tính đa dạng token toàn cục. Trong bài báo này, chúng tôi nhấn mạnh tính quan trọng của ngữ nghĩa đa dạng toàn cục và đề xuất một phương pháp tách rời và hợp nhất token hiệu quả có thể đồng thời xem xét tầm quan trọng và tính đa dạng của token cho việc cắt tỉa token. Theo sự chú ý của class token, chúng tôi tách rời các token chú ý và không chú ý. Ngoài việc bảo tồn các token cục bộ phân biệt nhất, chúng tôi hợp nhất các token không chú ý tương tự và kết nối các token chú ý đồng nhất để tối đa hóa tính đa dạng token. Mặc dù đơn giản, phương pháp của chúng tôi đạt được sự cân bằng đầy hứa hẹn giữa độ phức tạp mô hình và độ chính xác phân loại. Trên DeiT-S, phương pháp của chúng tôi giảm FLOPs 35% chỉ với độ giảm độ chính xác 0.2%. Đáng chú ý, nhờ vào việc duy trì tính đa dạng token, phương pháp của chúng tôi thậm chí có thể cải thiện độ chính xác của DeiT-T lên 0.1% sau khi giảm FLOPs 40%.

1. Giới thiệu
Transformer [29] đã trở thành kiến trúc phổ biến nhất trong cả cộng đồng xử lý ngôn ngữ tự nhiên và thị giác máy tính. Vision transformers (ViTs) [8] đã đạt được hiệu suất vượt trội và vượt qua các CNN tiêu chuẩn trong các tác vụ thị giác khác nhau như phân loại hình ảnh [10, 28, 31, 38], phân đoạn ngữ nghĩa [17, 19, 30, 33], và phát hiện đối tượng [1, 5]. Ưu điểm đáng chú ý nhất của transformer là khả năng nắm bắt hiệu quả các phụ thuộc tầm xa giữa các patch trong hình ảnh đầu vào thông qua cơ chế self-attention [23]. Tuy nhiên, các tương tác bậc hai giữa các token làm giảm đáng kể hiệu quả tính toán [36], điều này thúc đẩy nhiều nghiên cứu khám phá các transformer hiệu quả.

Là một trong những cách trực tiếp và hiệu quả nhất để giảm độ phức tạp tính toán, việc cắt tỉa token đã được nghiên cứu rộng rãi gần đây. Các nghiên cứu hiện tại chủ yếu tập trung vào việc thiết kế các chiến lược đánh giá tầm quan trọng khác nhau để giữ lại các token chú ý và cắt tỉa các token không chú ý [18, 21, 23, 35, 37]. Trong các công trình dựa trên tầm quan trọng này, DyViT [23] giới thiệu một module bổ sung để ước tính tầm quan trọng của mỗi token trong khi EViT [18] tổ chức lại các token hình ảnh dựa trên điểm tầm quan trọng chú ý của class. Tuy nhiên, được truyền cảm hứng từ các nghiên cứu bảo tồn tính đa dạng gần đây trong các biến thể ViT [9, 11–13, 25,27], chúng tôi cho rằng việc thúc đẩy tính đa dạng token cũng quan trọng cho việc cắt tỉa token. Mặc dù các token không chú ý như nền hình ảnh và các kết cấu mức thấp không liên quan trực tiếp đến các đối tượng phân loại, chúng có thể tăng tính đa dạng token và cải thiện khả năng biểu đạt của mô hình.

Như đã thảo luận trong [32], nền hình ảnh (ví dụ, cỏ và lá trong Hình 2) có thể cải thiện độ chính xác phân loại do mối quan hệ tiềm năng của chúng với các đối tượng tiền cảnh. Vì vậy, chúng tôi đầu tiên điều tra một chiến lược cắt tỉa dựa trên tính đa dạng trên DeiT-S [28] với các tỷ lệ giữ lại khác nhau. Cụ thể, thay vì nổi bật tầm quan trọng token, nó trực tiếp nhóm và kết hợp các token tương tự thành một token duy nhất, từ đó tối đa hóa tính đa dạng token. Một cách đáng ngạc nhiên, như được hiển thị trong Hình 1, chiến lược trực quan như vậy có thể đạt được hiệu suất tương đương và thậm chí tốt hơn so với các phương pháp cắt tỉa dựa trên tầm quan trọng SOTA, đặc biệt ở tỷ lệ giữ lại thấp.

Mặc dù có hiệu suất đầy hứa hẹn, chiến lược dựa trên tính đa dạng không thể giữ lại các token chú ý ban đầu và có thể làm yếu khả năng phân biệt của mô hình. Như được hiển thị trong Hình 2 (c), các token đại diện nhất, ví dụ, mắt và tai của chó hoặc mỏ của hai con chim, chứa thông tin ngữ nghĩa quan trọng cho các tác vụ phân loại nhưng không thể được bảo tồn bởi chiến lược dựa trên tính đa dạng. Để giải quyết vấn đề này, chúng tôi tự nhiên có xu hướng giữ tất cả các token chiếm ưu thế này trong khi duy trì tính đa dạng token, như được hiển thị trong Hình 2 (d). Tóm lại, một phương pháp cắt tỉa hài lòng nên đồng thời tính đến tầm quan trọng và tính đa dạng của token, sao cho thông tin cục bộ quan trọng nhất và thông tin đa dạng toàn cục có thể được bảo tồn đồng thời.

Được thúc đẩy bởi những quan sát trên, trong bài báo này, chúng tôi đề xuất một phương pháp cắt tỉa mới kết hợp tầm quan trọng và tính đa dạng token thông qua việc tách rời và hợp nhất token hiệu quả. Như được hiển thị trong Hình 1 (c), chúng tôi đầu tiên tách rời chuỗi token gốc thành các phần chú ý và không chú ý dựa trên sự chú ý của class token. Thay vì loại bỏ hoàn toàn các token không chú ý, chúng tôi áp dụng một thuật toán nhóm đỉnh mật độ đơn giản hóa [24] để nhóm hiệu quả các token không chú ý tương tự và kết hợp các token từ cùng một nhóm thành một token mới. Ngoài ra, không giống như các phương pháp hiện tại bảo tồn tất cả các token chú ý, chúng tôi thiết kế một thuật toán khớp đơn giản để hợp nhất các token chú ý đồng nhất và cải thiện hiệu quả tính toán hơn nữa. Bằng cách này, chúng tôi có thể cắt tỉa token hiệu quả trong khi tối đa hóa việc bảo tồn tính đa dạng token. Chúng tôi tiến hành các thí nghiệm cắt tỉa token mở rộng để xác thực hiệu quả của phương pháp của chúng tôi. Mặc dù đơn giản, phương pháp của chúng tôi đạt được hiệu suất cắt tỉa vượt trội trên ImageNet [6] cho hai vision transformer khác nhau, DeiT [28] và LV-ViT [15]. Những đóng góp chính của chúng tôi được tóm tắt như sau:

• Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên nhấn mạnh tính đa dạng token cho việc cắt tỉa ViT. Chúng tôi cũng chứng minh tính quan trọng của nó thông qua phân tích số và thực nghiệm.

• Chúng tôi đề xuất một phương pháp tách rời và hợp nhất đơn giản nhưng hiệu quả có thể đồng thời bảo tồn các token cục bộ chú ý nhất và ngữ nghĩa đa dạng toàn cục mà không áp đặt các tham số bổ sung.

• Nhờ vào việc kết hợp tầm quan trọng và tính đa dạng token, phương pháp của chúng tôi đạt được hiệu suất SOTA mới về sự cân bằng giữa độ chính xác và FLOPs. Nó cũng có thể được triển khai cho các phương pháp cắt tỉa token khác, đạt được cải thiện hiệu suất xuất sắc.

2. Công trình liên quan

Vision Transformers. Khác với các mạng tích chập, transformer có khả năng đáng kể trong việc mô hình hóa các phụ thuộc tầm xa và thiên kiến quy nạp tối thiểu [35]. Những tiến bộ gần đây cho thấy rằng các biến thể của transformer có thể là một thay thế cạnh tranh cho CNN. Visual transformer (ViT) [8] là công trình đầu tiên áp dụng kiến trúc transformer để đạt được hiệu suất STOA, nhưng nó chỉ thay thế tích chập tiêu chuẩn trong mạng nơ-ron sâu trên các tập dữ liệu hình ảnh quy mô lớn. Để giải phóng ViT khỏi sự phụ thuộc vào các tập dữ liệu lớn, DeiT [28] kết hợp một token bổ sung cho việc chưng cất tri thức để cải thiện hiệu quả huấn luyện của vision transformers. LV-ViT [15] cải thiện hiệu suất hơn nữa bằng cách sử dụng tất cả các token patch hình ảnh để tính toán mất mát huấn luyện một cách chuyên sâu. Nó tương đương với việc chuyển đổi vấn đề phân loại hình ảnh thành vấn đề nhận dạng từng token. Trong khi ViT và các phiên bản tiếp theo đạt được hiệu suất xuất sắc, độ phức tạp bậc hai với số lượng token gây ra chi phí tính toán cao. Cắt tỉa token nhằm mục đích giảm các token dư thừa và cải thiện hiệu quả suy luận của các backbone ViT khác nhau.

Cắt tỉa token ViT. Mặc dù ViT đã đạt được độ chính xác cạnh tranh trong các tác vụ thị giác [1, 5, 10, 28, 31, 38], nó cần bộ nhớ và tài nguyên tính toán khổng lồ. Do đó, cách xây dựng một transformer hiệu quả hơn thu hút sự quan tâm của các nhà nghiên cứu. So với CNN, chi phí tính toán cao hơn của transformers chủ yếu do độ phức tạp thời gian bậc hai của multi-head self-attention (MHSA). Theo đó, một số công trình [18, 21, 23, 35] cố gắng cắt tỉa token dựa trên điểm tầm quan trọng trong transformer. Dựa trên việc có cần giới thiệu các tham số bổ sung vào mô hình hay không, chúng tôi chia các phương pháp cắt tỉa token hiện tại thành hai nhóm sau. Một nhóm thực hiện cắt tỉa token bằng cách chèn các module dự đoán. DyViT [23] thiết kế một module dự đoán nhẹ được chèn vào các lớp khác nhau để ước tính điểm tầm quan trọng của mỗi token để cắt tỉa các token dư thừa cho các đặc trưng hiện tại. IA-RED2 [21] giới thiệu các module có thể diễn giải để xóa động các patch dư thừa không liên quan đến đầu vào. AdaViT [20] kết nối một mạng quyết định nhẹ với backbone để tạo ra các quyết định động. Nhóm khác tận dụng sự chú ý của class token để giữ các token chú ý. EViT [18] chia các token hình ảnh thành các token chú ý và không chú ý theo sự chú ý của class token, giữ lại các token chú ý và loại bỏ các token hình ảnh không chú ý để tổ chức lại các token hình ảnh. Evo-ViT [35] phân biệt các token có thông tin và không có thông tin thông qua sự chú ý class toàn cục để cập nhật chậm và nhanh tương ứng. A-ViT [37] thiết kế một cơ chế cắt tỉa token thích ứng dựa trên sự chú ý của class token, điều chỉnh động chi phí tính toán của hình ảnh với độ phức tạp khác nhau. Khác với các phương pháp cắt tỉa token này, chỉ tập trung vào tầm quan trọng của token, phương pháp của chúng tôi cũng xem xét tính đa dạng của thông tin ngữ nghĩa token. Do đó phương pháp của chúng tôi đạt được hiệu suất đáng kinh ngạc.

3. Kiến thức cơ bản

Trong các vision transformer tiêu chuẩn [8], mỗi hình ảnh đầu vào I∈R^(H×W×C) đầu tiên được chuyển đổi thành một chuỗi patch một chiều X∈R^(N×P²×C). Sau đó tất cả các patch được ánh xạ thành các embedding token D chiều thông qua một lớp tuyến tính có thể huấn luyện. Ngoài ra, một embedding vị trí có thể học E_pos∈R^((N+1)×D) được thêm vào embedding token để giữ thông tin vị trí. Một cách chính thức, chuỗi patch đầu vào có thể được biểu diễn như:

X = [x_cls; x₁; ...; x_N] + E_pos; (1)

trong đó x_cls biểu thị class token có thể học phục vụ như biểu diễn hình ảnh, và x_i biểu thị token của patch thứ i với i≥0. Sau đó, chuỗi token như vậy được đưa vào một mô hình ViT với L khối transformer xếp chồng, mỗi khối bao gồm một module multi-head self-attention (MHSA) và một mạng feed forward (FFN).

3.1. MHSA & FFN

Trong MHSA, chuỗi token đầu vào được ánh xạ tuyến tính thành ba ma trận khác nhau của query Q, key K, và value V tương ứng. MHSA có thể được công thức hóa như:

MHSA(Z) = Concat[softmax(Q_h K_h^T / √d) V_h]_{h=1}^H; (2)

trong đó Z là chuỗi token của N + 1 token. Concat[] xuất ra sự nối đặc trưng của H head. Q_h, K_h và V_h là các ma trận chiếu của Q, K, và V trong head thứ h tương ứng. d là chiều đặc trưng của head đơn.

FFN thường bao gồm hai lớp fully-connected và một lớp ánh xạ phi tuyến, có thể được biểu đạt như:

FFN(Z) = Sigmoid(Linear(GeLU(Linear(Z)))); (3)

trong đó Linear biểu thị các lớp fully-connected và GeLU là một hàm kích hoạt phi tuyến.

3.2. Độ phức tạp tính toán

Chiều của chuỗi token đầu vào là N×D, trong đó N là số lượng token và D là chiều embedding của mỗi token. Do đó chi phí tính toán của các module MSHA và FFN lần lượt là O(4ND² + 2N²D) và O(8ND²). Rõ ràng, vision transformers yêu cầu chi phí tính toán rất chuyên sâu, với tổng độ phức tạp tính toán là O(12ND² + 2N²D). Vì việc giảm chiều kênh D chỉ ảnh hưởng đến việc tính toán phép nhân ma trận hiện tại, hầu hết các công trình liên quan có xu hướng cắt tỉa token, ví dụ giảm số lượng N, để giảm tất cả các phép toán tuyến tính hoặc thậm chí bậc hai.

4. Phương pháp luận

4.1. Tổng quan

Khác với các công trình hiện tại chỉ tập trung vào các token chú ý, phương pháp của chúng tôi kết hợp tầm quan trọng và tính đa dạng token để có được các vision transformer hiệu quả và chính xác. Vì vậy, chúng tôi đề xuất phương pháp tách rời và hợp nhất token, đạt được sự cân bằng đầy hứa hẹn giữa FLOPs và độ chính xác. Như được hiển thị trong Hình 3, chúng tôi chèn phương pháp của chúng tôi tại lớp 4, 7, và 10 của mô hình DeiT-S. Phương pháp có hai thành phần chính: bộ tách rời token và bộ hợp nhất token. Bộ tách rời chia chuỗi token gốc thành các phần chú ý và không chú ý dựa trên sự chú ý của class token. Sau đó bộ hợp nhất nhóm các token không chú ý tương tự và kết nối các token chú ý đồng nhất. Trong phần này, chúng tôi đầu tiên chứng minh cách bảo tồn tính đa dạng token có lợi cho việc cắt tỉa token và sau đó trình bày chi tiết hai thành phần chính.

4.2. Tính đa dạng token quan trọng

Trong tài liệu, hầu hết công trình chỉ nhấn mạnh việc giữ lại các token quan trọng nhưng trực tiếp loại bỏ tất cả những token còn lại để đạt được tỷ lệ giữ token thỏa đáng. Tuy nhiên, được truyền cảm hứng từ các quan sát trong [32], rằng ngay cả nền hình ảnh cũng có thể giúp cải thiện phân loại thể hiện tiền cảnh, chúng tôi cho rằng các token ít quan trọng hơn cũng có thể chứa thông tin ngữ nghĩa hữu ích và là một bổ sung hiệu quả cho tính đa dạng thông tin. Ngoài ra, như đã thảo luận trong [9, 11–13, 25, 27], tính đa dạng token rất quan trọng để tối ưu hóa các cấu trúc transformer. Do đó, việc bảo tồn thích hợp các token không chú ý này tăng cường tính đa dạng của thông tin ngữ nghĩa, có thể có lợi cho việc cắt tỉa token. Ngược lại, việc loại bỏ token một cách mù quáng sẽ gây ra mất mát không thể phục hồi của thông tin ngữ nghĩa, đặc biệt ở tỷ lệ giữ thấp. Tham khảo [7, 11, 26, 27], chúng tôi tận dụng sự khác biệt giữa token và ma trận hạng-1 để đo tính đa dạng của chuỗi token Z. Điểm đa dạng sr(Z) có thể được tính như:

r(Z) = ||Z - 1z^T||_F, trong đó z = argmin_{z'} ||Z - 1z'^T||_F; (4)

trong đó ||·||_F biểu thị chuẩn l1. Z∈R^(N×C) là chuỗi token của N token và z, z'∈R^C là một trong các token. z^T là ma trận chuyển vị của z và 1 là một vector toàn số một. Hạng của ma trận 1z^T là 1. Điểm đa dạng lớn hơn cho thấy chuỗi token đa dạng hơn.

Chúng tôi điều tra cách điểm đa dạng ảnh hưởng đến hiệu suất cắt tỉa token. Trong Hình 4, chúng tôi kiểm tra độ chính xác phân loại và điểm đa dạng ở các tỷ lệ giữ khác nhau. Rõ ràng, chúng ta có thể thấy rằng điểm đa dạng chuỗi token có tương quan tích cực với độ chính xác phân loại. Trong cả phương pháp EViT hoặc phương pháp đề xuất của chúng tôi, điểm đa dạng cao hơn luôn dẫn đến độ chính xác cao hơn. Ngoài ra, cũng có thể quan sát thấy rằng, đối với phương pháp EViT, điểm đa dạng và độ chính xác phân loại giảm nhanh khi tỷ lệ giữ giảm. Khác biệt, nhờ vào chiến lược hợp nhất token bảo tồn đa dạng của chúng tôi, phương pháp của chúng tôi có thể duy trì điểm đa dạng tương đối cao hơn ở các tỷ lệ giữ khác nhau và do đó luôn vượt trội hơn phương pháp EViT, đặc biệt ở tỷ lệ giữ thấp. Do đó, duy trì tính đa dạng token cao hơn là quan trọng để cải thiện độ chính xác phân loại.

4.3. Bộ tách rời token

Để xem xét đầy đủ tầm quan trọng token trong khi duy trì tính đa dạng, chúng tôi ưu tiên các token chú ý để bảo tồn thông tin ngữ nghĩa quan trọng nhất. Do đó, chúng tôi tách rời chuỗi token gốc thành các token chú ý và không chú ý để duy trì tính đa dạng và tầm quan trọng token đồng thời. Giống như [29], chúng tôi chia các token thành hai nhóm bằng cách so sánh độ tương tự của chúng với các class token. Về mặt toán học, điểm tương tự Attn_cls giữa class token và các token khác như sự chú ý của class token có thể được tính bằng

Attn_cls = Softmax(q_cls K^T / √d); (5)

trong đó q_cls biểu thị class token của vector query. Với N token tổng cộng và tỷ lệ giữ α, chúng tôi chọn top-K token như các token chú ý theo điểm chú ý. N-K token còn lại được xác định là các token không chú ý chứa ít thông tin hơn. Hơn nữa, trong lớp multi-head self-attention, chúng tôi tính trung bình của điểm chú ý của tất cả các head.

4.4. Bộ hợp nhất token

Chúng tôi áp dụng các chiến lược khác nhau cho các token chú ý và không chú ý khi hợp nhất token. Đối với các token không chú ý, chúng tôi đầu tiên áp dụng thuật toán nhóm đỉnh mật độ để nhóm các token không chú ý và sau đó kết hợp các token từ cùng một nhóm thành token mới bằng tổng có trọng số. Bằng cách này, chúng tôi có thể tích hợp một chuỗi token không chú ý mới T = [t₁; ...; tₙ]. Đối với các token chú ý, chúng tôi thích ứng một thuật toán khớp đơn giản để hợp nhất các token chú ý đồng nhất. Chuỗi token hợp nhất là P = [p₁; ...; pₘ]. Chúng tôi nối T và P để có được chuỗi token được cắt tỉa Z = [z_cls; p₁; ...; pₘ; t₁; ...; tₙ].

Nhóm token không chú ý. Thuật toán nhóm K-means thông thường yêu cầu nhiều lần lặp để có được kết quả thỏa đáng, giảm thông lượng trong thực tế và đánh bại ý định tăng tốc mô hình. Sau nghiên cứu, chúng tôi phát hiện rằng thuật toán nhóm mật độ có thể nhanh chóng tìm thấy các lớp có hình dạng tùy ý bằng cách khai thác tính kết nối mật độ của các lớp. Do đó, chúng tôi đơn giản hóa một thuật toán nhóm đỉnh mật độ hiệu quả (DPC) không có quy trình lặp lại cũng như không có thêm tham số. Chúng tôi tuân theo thuật toán DPC được đề xuất trong [24]. Nó giả định rằng trung tâm nhóm được bao quanh bởi các láng giềng mật độ thấp, và khoảng cách giữa trung tâm nhóm và bất kỳ điểm mật độ cao nào tương đối lớn. Chúng tôi tính hai biến cho mỗi token i: mật độ ρᵢ và khoảng cách tối thiểu δᵢ từ token mật độ cao hơn. Cho một tập hợp token x, chúng tôi tính mật độ của mỗi token bằng

ρᵢ = exp(-∑_{zⱼ∈Z} ||zᵢ - zⱼ||₂²/2σ²); (6)

trong đó Z biểu thị tập hợp token. zᵢ và zⱼ là các đặc trưng token tương ứng.

Đối với token có mật độ cao nhất, khoảng cách tối thiểu của nó được đặt thành khoảng cách tối đa giữa nó và bất kỳ token nào khác. Chúng tôi định nghĩa δᵢ là khoảng cách tối thiểu giữa token i và bất kỳ token nào khác có mật độ cao hơn. Khoảng cách tối thiểu của mỗi token là:

δᵢ = min_{j:ρⱼ>ρᵢ} ||zᵢ - zⱼ||₂, nếu ∃j sao cho ρⱼ > ρᵢ
     max_j ||zᵢ - zⱼ||₂, ngược lại. (7)

Chúng tôi biểu thị điểm trung tâm nhóm của token thứ i là ρᵢδᵢ. Điểm cao hơn có nghĩa là tiềm năng cao hơn để trở thành trung tâm nhóm. Chúng tôi chọn các token có điểm top-K làm trung tâm nhóm. Thuật toán DPC gán mỗi token còn lại cho nhóm mà trung tâm nhóm gần nhất với token và có mật độ cao hơn.

Khớp token chú ý. Xem hình ảnh ví dụ trong Hình 5. Các token đồng nhất cũng có mặt trong các đối tượng tiền cảnh (token chú ý), chẳng hạn như má của động vật. Sự dư thừa này làm cho việc hợp nhất các token chú ý đồng nhất để giảm số lượng token trong khi duy trì độ chính xác trở nên khả thi. Chúng tôi có thể áp dụng cùng một chiến lược nhóm token như đã làm cho các token không chú ý. Tuy nhiên, vì các token chú ý chứa thông tin ngữ nghĩa quan trọng cho tác vụ phân loại cuối cùng, sẽ tốt nhất nếu chúng ta có thể bảo tồn các token gốc. Để giải quyết vấn đề này, chúng tôi tùy chỉnh một thuật toán khớp đơn giản giữ các token quan trọng nhất trong khi hợp nhất các token đồng nhất. Cụ thể, chúng tôi định nghĩa thước đo tương tự cosine để xác định tương tự giữa các token khác nhau và tính điểm tương tự cosine giữa các token chú ý. Sau đó chúng tôi chọn top-K cặp token tương tự nhất làm token đồng nhất. Cuối cùng, chúng tôi kết hợp mỗi cặp token thành một token mới và kết nối các token chú ý còn lại.

Mặc dù các token trong cùng một tập hợp có thông tin ngữ nghĩa tương tự, tầm quan trọng ngữ nghĩa của mỗi token không nhất thiết phải giống nhau. Thay vì tính trung bình một cách mù quáng các token trong cùng một tập hợp, chúng tôi kết hợp các token này bằng một tổng có trọng số. Bằng cách giới thiệu sự chú ý của class token để biểu thị tầm quan trọng, chúng tôi kết hợp cùng một tập hợp token thành một token mới bằng

pᵢ = ∑_{j∈Cᵢ} sⱼzⱼ; (8)

trong đó sⱼ biểu thị điểm tầm quan trọng của token zⱼ, và Cᵢ biểu thị tập hợp thứ i.

5. Thí nghiệm

5.1. Thiết lập

Tập dữ liệu và thước đo đánh giá. Các thí nghiệm của chúng tôi được tiến hành trên ImageNet-1K [6] với 1.28 triệu hình ảnh huấn luyện và 50000 hình ảnh xác thực. Chúng tôi báo cáo độ chính xác phân loại top-1 và các phép toán dấu phẩy động (FLOPs) để đánh giá hiệu quả mô hình. Ngoài ra, chúng tôi đo thông lượng của các mô hình trên một GPU NVIDIA V100 duy nhất với kích thước batch cố định là 256.

Chi tiết triển khai. Để chứng minh tính tổng quát của phương pháp của chúng tôi, chúng tôi tiến hành cắt tỉa token trên các mô hình ViT khác nhau bao gồm DeiT-T, DeiT-S, DeiT-B [28], và LV-ViT-S [15]. Theo [18], chúng tôi sử dụng phương pháp của chúng tôi tại lớp 4, 7, và 10 của mô hình DeiT-T/S/B và tại lớp 4, 8, và 12 cho LV-ViT-S [15]. Chúng tôi sử dụng cùng cài đặt huấn luyện như các bài báo gốc của DeiT [28] và LV-ViT [15]. Theo [38], chúng tôi kết hợp một bộ lập lịch cosine vào chiến lược học của chúng tôi trong đó tỷ lệ giữ dần dần giảm từ 1 xuống giá trị mục tiêu trong 100 epoch. Để so sánh công bằng, tất cả các mô hình của chúng tôi được huấn luyện từ đầu trong 300 epoch trên 8 NVIDIA V100.

5.2. Kết quả chính

So sánh với các phương pháp tiên tiến. Chúng tôi so sánh phương pháp của chúng tôi với các phương pháp cắt tỉa token SOTA, kết quả được hiển thị trong Bảng 1. Chúng tôi tận dụng λ cho biết tỷ lệ giữ. Chúng tôi báo cáo độ chính xác top-1, FLOPs, và thông lượng cho mỗi mô hình. So với công trình trước đây, phương pháp của chúng tôi đạt được hiệu suất SOTA mới với chi phí tính toán tương tự. Cụ thể, trên mô hình cổ điển DeiT-S [28], sự suy giảm độ chính xác top-1 của các mô hình được cắt tỉa của chúng tôi được kiểm soát trong vòng 0.2% khi chi phí tính toán giảm 35%. Ngoài ra, ưu thế của phương pháp chúng tôi rõ ràng hơn ở tỷ lệ giữ thấp hơn. Khi tỷ lệ nén của DeiT-S tăng lên 50%, chúng tôi cải thiện 0.5% so với đối thủ tốt nhất. Đặc biệt, tỷ lệ nén của phương pháp chúng tôi gần 40% trên mô hình DeiT-T [28], và độ chính xác thậm chí còn tốt hơn mô hình gốc.

Do tầm quan trọng và tính đa dạng token là trực giao cho việc cắt tỉa token, phương pháp của chúng tôi có thể được cắm vào EViT và đạt được cải thiện hiệu suất 0.1%. Như được hiển thị trong Bảng 2, chúng tôi tiến hành thêm các thí nghiệm trên transformer sâu-hẹp LV-ViT [15]. Chúng tôi quan sát thấy rằng phương pháp của chúng tôi đạt được sự cân bằng độ chính xác-độ phức tạp tốt hơn ở các tỷ lệ giữ khác nhau so với các kiến trúc CNN và ViT hàng đầu hiện tại.

Hiệu suất của các phương pháp hiện tại ở mỗi tỷ lệ giữ. Như được hiển thị trong Hình 6, phương pháp của chúng tôi liên tục đạt được hiệu suất tốt nhất trong khi hai phương pháp khác có được hiệu suất gần nhau. Ngoài ra, với việc giảm tỷ lệ giữ, độ chính xác phân loại của các phương pháp cắt tỉa token hiện tại giảm mạnh. May mắn thay, phương pháp của chúng tôi làm giảm hiện tượng này bằng cách bảo tồn tính đa dạng của thông tin ngữ nghĩa token. Đặc biệt khi FLOPs của DyViT giảm xuống 1.6G, độ chính xác phân loại giảm hơn 10%. Điều này là do việc loại bỏ hoàn toàn các token không chú ý làm giảm đáng kể tính đa dạng token, dẫn đến việc giảm thông tin ngữ nghĩa của chuỗi token gốc. Chúng tôi áp dụng việc tách rời và hợp nhất token để đồng thời xem xét tầm quan trọng và tính đa dạng token, đạt được độ chính xác đáng kinh ngạc ở tỷ lệ giữ thấp. Một cách trực quan, khi chúng ta chỉ giữ một vài token, việc hợp nhất token rõ ràng có ý nghĩa hơn việc chỉ giữ các token chú ý top-K.

Trực quan hóa kết quả bộ hợp nhất token. Để hiển thị thêm khả năng diễn giải của phương pháp chúng tôi, chúng tôi trực quan hóa kết quả bộ hợp nhất token cuối cùng trở lại các patch đầu vào gốc của nó trong Hình 5. Chúng tôi nhận thấy rằng phương pháp của chúng tôi chú ý đến các vùng đóng góp nhiều hơn cho dự đoán hình ảnh thay vì nền không có thông tin. ví dụ, năm cơ quan giác quan của động vật được bảo tồn. Nó chứng minh rằng phương pháp của chúng tôi hiệu quả tách rời các token chú ý và không chú ý. Khác với các phương pháp khác che mặt tất cả các token không chú ý, phương pháp của chúng tôi kết hợp các patch nền với ngữ nghĩa tương tự. ví dụ, lông của động vật được hợp nhất thành một token duy nhất. Nó ngụ ý rằng phương pháp của chúng tôi không chỉ tập trung vào các token chú ý mà còn duy trì tính đa dạng của ngữ nghĩa token. Cũng đáng chỉ ra rằng cặp mắt được khớp và kết hợp thành một token trong hình thứ năm và thứ sáu. Nó cho thấy rằng phương pháp của chúng tôi hiệu quả hợp nhất các token chú ý đồng nhất và giảm sự dư thừa tiềm năng.

5.3. Phân tích loại bỏ

Hiệu quả của mỗi module. Như được hiển thị trong Bảng 3, chúng tôi thêm các sub-module từng cái một để đánh giá hiệu quả của mỗi module. i) Bảo tồn token chú ý. Loại bỏ các token không chú ý dựa trên sự chú ý của class token trong các lớp cắt tỉa; ii) Đóng gói token không chú ý. Đóng gói tất cả các token không chú ý thành một token; iii) Nhóm token không chú ý. Nhóm các token không chú ý và kết hợp các token của cùng một nhóm thành một token mới; iv) Khớp token chú ý. Khớp các token chú ý và kết hợp các token của cùng một cặp thành một token mới; Rõ ràng là vì module nhóm bảo tồn tính đa dạng token, độ chính xác được cải thiện 0.2% và 0.8% ở tỷ lệ giữ 0.7 và 0.5 tương ứng. Đáng chú ý, tỷ lệ giữ càng thấp, phương pháp của chúng tôi hoạt động càng tốt. Ngoài ra, module khớp token chú ý tiếp tục giảm FLOPs của mô hình trong khi duy trì độ chính xác bằng cách hợp nhất các token chú ý đồng nhất.

Chiến lược hợp nhất token khác nhau. Như được trình bày trong Bảng 4, chúng tôi so sánh một số chiến lược hợp nhất token phổ biến để đánh giá hiệu quả của phương pháp chúng tôi. i) Chiến lược pooling. Sử dụng phép toán pooling để giảm số lượng token. ii) Chiến lược sub-sampling. Thêm một loạt các lớp tích chập giữa MHSA và FFN để giảm chiều token. iii) Chiến lược nhóm. Nhóm các token và kết hợp các token của cùng một nhóm thành một token mới. Chúng tôi quan sát thấy rằng chiến lược nhóm nói chung cải thiện độ chính xác 0.4% so với các chiến lược hợp nhất token khác. Một lý do có thể là chiến lược nhóm có được thiên kiến quy nạp nhiều hơn với cùng chi phí tính toán. Tuy nhiên, chúng tôi phát hiện rằng thông lượng của thuật toán K-means thấp hơn, cho thấy rằng nó không hoạt động tốt về mặt tăng tốc mô hình trong thực tế. Hơn nữa, chúng tôi khám phá rằng thông lượng của thuật toán K-means giảm với số lần lặp. Do đó chúng tôi đơn giản hóa một thuật toán DPC hiệu quả không có quy trình lặp lại cũng như không có thêm tham số, vượt trội hơn các chiến lược khác về cả độ chính xác và hiệu quả.

6. Kết luận

Trong bài báo này, chúng tôi đề xuất một phương pháp tách rời và hợp nhất token để đồng thời xem xét tầm quan trọng và tính đa dạng token. Vì tầm quan trọng và tính đa dạng token là trực giao cho việc cắt tỉa token, phương pháp của chúng tôi có thể được sử dụng trong các phương pháp cắt tỉa token hiện tại để cải thiện hiệu suất hơn nữa. Chúng tôi chứng minh rằng phương pháp của chúng tôi đạt được sự cân bằng hiệu suất SOTA giữa độ chính xác và FLOPs mà không áp đặt thêm tham số. Chúng tôi hy vọng rằng bài báo này, kết hợp tầm quan trọng và tính đa dạng token, sẽ cung cấp những hiểu biết sâu sắc cho công việc tương lai về cắt tỉa visual transformers.
