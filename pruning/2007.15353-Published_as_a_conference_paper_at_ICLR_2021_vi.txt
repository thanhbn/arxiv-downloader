# 2007.15353.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2007.15353.pdf
# Kích thước tệp: 1205658 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
PHÁT TRIỂN MẠNG SÂU HIỆU QUẢ
BẰNG PHƯƠNG PHÁP THƯA THỚT LIÊN TỤC CÓ CẤU TRÚC
Xin Yuan
Đại học Chicago
yuanx@uchicago.eduPedro Savarese
TTI-Chicago
savarese@ttic.eduMichael Maire
Đại học Chicago
mmaire@uchicago.edu
TÓM TẮT
Chúng tôi phát triển một phương pháp để phát triển kiến trúc mạng sâu trong quá trình huấn luyện, được điều khiển bởi sự kết hợp nguyên tắc của các mục tiêu độ chính xác và độ thưa thớt. Không giống như các kỹ thuật cắt tỉa hoặc tìm kiếm kiến trúc hiện có hoạt động trên các mô hình kích thước đầy đủ hoặc kiến trúc siêu mạng, phương pháp của chúng tôi có thể bắt đầu từ một kiến trúc hạt giống nhỏ, đơn giản và phát triển và cắt tỉa động cả lớp và bộ lọc. Bằng cách kết hợp việc nới lỏng liên tục của tối ưu hóa cấu trúc mạng rời rạc với một sơ đồ lấy mẫu các mạng con thưa thớt, chúng tôi tạo ra các mạng nén, được cắt tỉa, đồng thời cũng giảm đáng kể chi phí tính toán của việc huấn luyện. Ví dụ, chúng tôi đạt được 49.7% tiết kiệm FLOPs suy luận và 47.4% tiết kiệm FLOPs huấn luyện so với một ResNet-50 cơ sở trên ImageNet, trong khi duy trì 75.2% độ chính xác top-1 — tất cả mà không cần bất kỳ giai đoạn tinh chỉnh chuyên dụng nào. Các thí nghiệm trên CIFAR, ImageNet, PASCAL VOC, và Penn Treebank, với mạng tích chập cho phân loại ảnh và phân đoạn ngữ nghĩa, và mạng hồi quy cho mô hình hóa ngôn ngữ, chứng minh rằng chúng tôi vừa huấn luyện nhanh hơn vừa tạo ra các mạng hiệu quả hơn so với các phương pháp cắt tỉa hoặc tìm kiếm kiến trúc cạnh tranh.
1 GIỚI THIỆU
Mạng nơ-ron sâu là phương pháp thống trị cho nhiều tác vụ học máy, bao gồm phân loại ảnh (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015), phát hiện đối tượng (Girshick, 2015; Liu et al., 2016), phân đoạn ngữ nghĩa (Long et al., 2015; Chen et al., 2017) và mô hình hóa ngôn ngữ (Zaremba et al., 2014; Vaswani et al., 2017; Devlin et al., 2019). Các mạng nơ-ron hiện đại được tham số hóa quá mức và việc huấn luyện các mạng lớn hơn thường mang lại độ chính xác tổng quát hóa được cải thiện. Các nghiên cứu gần đây (He et al., 2016; Zagoruyko & Komodakis, 2016; Huang et al., 2017) minh họa xu hướng này thông qua việc tăng độ sâu và chiều rộng của mạng nơ-ron tích chập (CNN). Tuy nhiên, việc huấn luyện tốn nhiều tính toán, và việc triển khai thực tế thường bị giới hạn bởi ngân sách tham số và tính toán.
Các phương pháp tìm kiếm kiến trúc nơ-ron (NAS) (Zoph & Le, 2017; Liu et al., 2019; Luo et al., 2018; Pham et al., 2018; Savarese & Maire, 2019) và cắt tỉa mô hình (Han et al., 2016; 2015; Guo et al., 2016) nhằm giảm những gánh nặng này. NAS giải quyết một vấn đề làm gia tăng thêm chi phí huấn luyện: không gian khổng lồ của các kiến trúc mạng có thể có. Trong khi việc điều chỉnh thủ công các chi tiết kiến trúc, chẳng hạn như cấu trúc kết nối của các lớp tích chập, có thể cải thiện hiệu suất (Iandola et al., 2016; Sifre & Mallat, 2014; Chollet, 2017; Howard et al., 2017; Zhang et al., 2018; Huang et al., 2018), một cách có nguyên tắc để tạo ra những thiết kế như vậy vẫn còn khó nắm bắt. Các phương pháp NAS nhằm tự động hóa việc khám phá các kiến trúc có thể có, tạo ra một thiết kế hiệu quả cho một tác vụ mục tiêu dưới các ràng buộc tài nguyên thực tế. Tuy nhiên, trong quá trình huấn luyện, hầu hết các phương pháp NAS hoạt động trên một kiến trúc siêu mạng lớn, bao gồm các thành phần ứng viên vượt quá những thành phần cuối cùng được chọn để đưa vào mạng kết quả (Zoph & Le, 2017; Liu et al., 2019; Luo et al., 2018; Pham et al., 2018; Savarese & Maire, 2019). Do đó, việc huấn luyện dựa trên NAS có thể thường kỹ lưỡng hơn, nhưng tốn kém tính toán hơn so với việc huấn luyện một kiến trúc được thiết kế thủ công duy nhất.
Các kỹ thuật cắt tỉa mô hình tương tự tập trung vào việc cải thiện hiệu quả tài nguyên của mạng nơ-ron trong quá trình suy luận, có thể với chi phí tăng chi phí huấn luyện. Các chiến lược phổ biến nhằm tạo ra một phiên bản nhẹ hơn của một kiến trúc mạng nhất định bằng cách loại bỏ các trọng số riêng lẻ (Han et al., 2015; 2016; Molchanov et al., 2017) hoặc các tập tham số có cấu trúc (Li et al., 2017; He et al., 2018; Luo et al., 2017). Tuy nhiên, phần lớn các phương pháp này huấn luyện một mô hình kích thước đầy đủ trước khi cắt tỉa và,

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
Cấp độ 1: Theo kênh Epochs Huấn luyện T=0,1,2…
Cấp độ 2: Theo lớp…
(a) Phát triển Lớp và Bộ lọc CNN
Epochs Huấn luyện FLOPs Phương pháp của chúng tôi
Cắt tỉa
Tìm kiếm Kiến trúc (b) Chi phí Huấn luyện theo Epoch
NAS Cắt tỉa Của chúng tôi FLOPs (c) Tổng Chi phí Huấn luyện
Hình 1: Phát triển Mạng trong quá trình Huấn luyện. Chúng tôi định nghĩa một không gian cấu hình kiến trúc và đồng thời điều chỉnh cấu trúc mạng và trọng số. (a) Áp dụng phương pháp của chúng tôi cho CNN, chúng tôi duy trì các biến phụ trợ xác định cách phát triển và cắt tỉa cả bộ lọc (tức là theo kênh) và lớp, tuân theo các ràng buộc tài nguyên thực tế. (b) Bằng cách bắt đầu với một mạng nhỏ và phát triển kích thước của nó, chúng tôi sử dụng ít tài nguyên hơn trong các epoch huấn luyện sớm, so với các phương pháp cắt tỉa hoặc NAS. (c) Do đó, phương pháp của chúng tôi giảm đáng kể tổng chi phí tính toán của việc huấn luyện, trong khi cung cấp các mạng được huấn luyện có kích thước và độ chính xác tương đương hoặc tốt hơn.
sau khi cắt tỉa, sử dụng các giai đoạn tinh chỉnh bổ sung để duy trì độ chính xác. Hubara et al. (2016) và Rastegari et al. (2016) đề xuất sử dụng trọng số và kích hoạt nhị phân, cho phép suy luận hưởng lợi từ việc giảm chi phí lưu trữ và tính toán hiệu quả thông qua các phép toán đếm bit. Tuy nhiên, việc huấn luyện vẫn liên quan đến việc theo dõi trọng số độ chính xác cao cùng với các xấp xỉ độ chính xác thấp hơn.
Chúng tôi có một cái nhìn thống nhất về cắt tỉa và tìm kiếm kiến trúc, coi cả hai như hoạt động trên một không gian cấu hình, và đề xuất một phương pháp để phát triển động các mạng sâu bằng cách liên tục cấu hình lại kiến trúc của chúng trong quá trình huấn luyện. Phương pháp của chúng tôi không chỉ tạo ra các mô hình với đặc tính suy luận hiệu quả, mà còn giảm chi phí tính toán của việc huấn luyện; xem Hình 1. Thay vì bắt đầu với một mạng kích thước đầy đủ hoặc một siêu mạng, chúng tôi bắt đầu từ các mạng hạt giống đơn giản và dần dần điều chỉnh (phát triển và cắt tỉa) chúng. Cụ thể, chúng tôi tham số hóa một không gian cấu hình kiến trúc với các biến chỉ báo điều chỉnh việc thêm hoặc loại bỏ các thành phần cấu trúc. Hình 2(a) hiển thị một ví dụ, dưới dạng không gian cấu hình hai cấp cho các lớp và bộ lọc CNN. Chúng tôi cho phép học các giá trị chỉ báo (và do đó, cấu trúc kiến trúc) thông qua việc kết hợp nới lỏng liên tục với lấy mẫu nhị phân, như được minh họa trong Hình 2(b). Một tham số nhiệt độ cho mỗi thành phần đảm bảo rằng các cấu trúc tồn tại lâu cuối cùng được đưa vào cấu hình kiến trúc rời rạc của mạng.
Trong khi AutoGrow được đề xuất gần đây (Wen et al., 2020) cũng tìm cách phát triển mạng trong quá trình huấn luyện, phương pháp kỹ thuật của chúng tôi khác biệt đáng kể và dẫn đến những lợi thế thực tế quan trọng. Ở cấp độ kỹ thuật, AutoGrow triển khai một quy trình tìm kiếm kiến trúc trên một cấu trúc mô-đun được định nghĩa trước, tuân theo các chính sách phát triển và dừng do con người tạo ra, hướng dẫn bởi độ chính xác. Ngược lại, chúng tôi tham số hóa các cấu hình kiến trúc và sử dụng gradient descent ngẫu nhiên để học các biến phụ trợ chỉ định các thành phần cấu trúc, đồng thời huấn luyện các trọng số trong những thành phần đó. Phương pháp kỹ thuật độc đáo của chúng tôi mang lại những lợi thế sau:
•Huấn luyện Nhanh bằng Phát triển: Huấn luyện là một quy trình thống nhất, từ đó người ta có thể yêu cầu một cấu trúc mạng và các trọng số liên quan bất cứ lúc nào. Không giống như AutoGrow và phần lớn các kỹ thuật cắt tỉa, việc tinh chỉnh để tối ưu hóa trọng số trong một kiến trúc được phát hiện là tùy chọn. Chúng tôi đạt được kết quả xuất sắc ngay cả khi không có bất kỳ giai đoạn tinh chỉnh nào.
•Phương pháp Có nguyên tắc thông qua Học bằng Tiếp tục + Lấy mẫu: Chúng tôi xây dựng phương pháp của mình theo tinh thần của các phương pháp học bằng tiếp tục, nới lỏng một vấn đề tối ưu hóa rời rạc thành một xấp xỉ liên tục ngày càng cứng nhắc. Quan trọng là, chúng tôi giới thiệu một bước lấy mẫu bổ sung cho chiến lược này. Từ sự kết hợp này, chúng tôi có được sự linh hoạt của việc khám phá một kiến trúc siêu mạng, nhưng hiệu quả tính toán của việc chỉ thực sự huấn luyện một mạng con hoạt động nhỏ hơn nhiều.
•Mục tiêu Tối ưu hóa Nhận thức Ngân sách: Các tham số điều chỉnh cấu hình kiến trúc của chúng tôi được cập nhật thông qua gradient descent. Chúng tôi có sự linh hoạt để xây dựng nhiều mất mát nhạy cảm với tài nguyên, chẳng hạn như đếm tổng FLOPs, theo các tham số này.
•Khả năng Áp dụng Rộng: Mặc dù chúng tôi sử dụng việc phát triển tiến bộ của CNN theo chiều rộng và độ sâu như một ví dụ động lực, kỹ thuật của chúng tôi áp dụng cho hầu như bất kỳ kiến trúc nơ-ron nào. Người ta có sự linh hoạt trong cách tham số hóa không gian cấu hình kiến trúc. Chúng tôi cũng hiển thị kết quả với LSTM.
Chúng tôi chứng minh những lợi thế này trong khi so sánh với các phương pháp NAS và cắt tỉa gần đây thông qua các thí nghiệm mở rộng về phân loại, phân đoạn ngữ nghĩa và mô hình hóa ngôn ngữ cấp từ.

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
𝒙𝒊𝒏𝒙𝒐𝒖𝒕Bộ lọc Hoạt động = 11000…𝒙𝒊𝒏𝒙𝒐𝒖𝒕1100…
𝒙𝒊𝒏𝒙𝒐𝒖𝒕𝒙𝒊𝒏𝒙𝒐𝒖𝒕1010…Phát triểnTách rờiPhát triểnChỉ báo KênhChỉ báo KênhChỉ báo KênhT=0T=1Epochs Phát triển T=2…Cấp độ 1: Không gian Cấu hình Theo kênh
Cấp độ 2: Không gian Cấu hình Theo lớpBộ lọc Hoạt động = 2Bộ lọc Hoạt động = 2
1000…Chỉ báo LớpLớp Hoạt động = 1𝒙𝒊𝒏𝒙𝒐𝒖𝒕1100…Chỉ báo LớpLớp Hoạt động = 2𝒙𝒐𝒖𝒕0111…Lớp Hoạt động = 3Chỉ báo Lớp𝒙𝒊𝒏Phát triểnPhát triểnTách rờiPhát triển
(a) Không gian Cấu hình Kiến trúc cho CNN
Biến Mặt nạ Có thể huấn luyện𝒔Không gian Cấu hình Rời rạcNới lỏng Liên tục Tiến bộChỉ báo Nhị phân
11000𝒒∼𝑩𝒆𝒓𝒏(𝝈(𝜷𝒔))Mặt nạ liên tục𝜎(𝛽𝑠)Lấy mẫu000𝝈𝜷𝒔⊙𝒒 (b) Tối ưu hóa với Tiếp tục Có cấu trúc
Hình 2: Khung Kỹ thuật. (a) Chúng tôi định kỳ tái cấu trúc một CNN bằng cách truy vấn các chỉ báo nhị phân định nghĩa một không gian cấu hình hai cấp cho bộ lọc và lớp. (b) Để làm cho tối ưu hóa khả thi trong khi phát triển mạng, chúng tôi lấy các chỉ báo nhị phân này từ các biến mặt nạ liên tục có thể huấn luyện. Chúng tôi sử dụng một phần mở rộng có cấu trúc của thưa thớt hóa liên tục (Savarese et al., 2020), kết hợp với lấy mẫu. Các biến phụ trợ ngẫu nhiên nhị phân q, được lấy mẫu theo σ(βs), tạo ra các thành phần rời rạc hoạt động tại một thời điểm cụ thể.

2 CÔNG TRÌNH LIÊN QUAN
Cắt tỉa Mạng. Các phương pháp cắt tỉa có thể được chia thành hai nhóm: những phương pháp cắt tỉa các trọng số riêng lẻ và những phương pháp cắt tỉa các thành phần có cấu trúc. Các phương pháp cắt tỉa dựa trên trọng số riêng lẻ khác nhau về tiêu chí loại bỏ. Ví dụ, Han et al. (2015) đề xuất cắt tỉa các trọng số mạng có độ lớn nhỏ, và sau đó lượng tử hóa những trọng số còn lại (Han et al., 2016). Louizos et al. (2018) học các mạng thưa thớt bằng cách xấp xỉ chính quy hóa ℓ0 với một tham số hóa lại ngẫu nhiên. Tuy nhiên, chỉ riêng các trọng số thưa thớt thường chỉ dẫn đến tăng tốc trên phần cứng chuyên dụng với các thư viện hỗ trợ.
Trong các phương pháp có cấu trúc, cắt tỉa được áp dụng ở cấp độ nơ-ron, kênh, hoặc thậm chí lớp. Ví dụ, cắt tỉa L1 (Li et al., 2017) loại bỏ các kênh dựa trên chuẩn của các bộ lọc của chúng. He et al. (2018) sử dụng độ thưa thớt nhóm để làm mượt quá trình cắt tỉa sau huấn luyện. MorphNet (Gordon et al., 2018) chính quy hóa các trọng số về zero cho đến khi chúng đủ nhỏ sao cho các kênh đầu ra tương ứng được đánh dấu để loại bỏ khỏi mạng. Độ Thưa thớt Có cấu trúc Nội tại (ISS) (Wen et al., 2018) hoạt động trên LSTM (Hochreiter & Schmidhuber, 1997) bằng cách loại bỏ tập thể các cột và hàng của ma trận trọng số thông qua LASSO nhóm. Mặc dù các phương pháp cắt tỉa có cấu trúc và thuật toán của chúng tôi có cùng tinh thần tạo ra các mô hình hiệu quả, chúng tôi có được tiết kiệm chi phí huấn luyện bằng cách phát triển mạng từ các kiến trúc ban đầu nhỏ thay vì cắt tỉa những mạng kích thước đầy đủ.
Tìm kiếm Kiến trúc Nơ-ron. Các phương pháp NAS đã cải thiện đáng kể hiệu suất đạt được bởi các mô hình mạng nhỏ. Các phương pháp NAS tiên phong sử dụng học tăng cường (Zoph et al., 2018; Zoph & Le, 2017) và thuật toán di truyền (Real et al., 2019; Xie & Yuille, 2017) để tìm kiếm các khối mạng có thể chuyển giao mà hiệu suất vượt qua nhiều khối được thiết kế thủ công. Tuy nhiên, những phương pháp như vậy đòi hỏi tính toán khổng lồ trong quá trình tìm kiếm — thường là hàng nghìn ngày GPU.
Để giảm chi phí tính toán, các nỗ lực gần đây sử dụng các kỹ thuật tìm kiếm hiệu quả hơn, chẳng hạn như tối ưu hóa dựa trên gradient trực tiếp (Liu et al., 2019; Luo et al., 2018; Pham et al., 2018; Tan et al., 2019; Cai et al., 2019; Wortsman et al., 2019). Tuy nhiên, hầu hết các phương pháp NAS thực hiện tìm kiếm trong một không gian siêu mạng đòi hỏi nhiều tính toán hơn so với việc huấn luyện các kiến trúc có kích thước thông thường.
Phát triển Mạng. Network Morphism (Wei et al., 2016) tìm kiếm các mạng sâu hiệu quả bằng cách mở rộng các lớp trong khi bảo toàn các tham số. AutoGrow được đề xuất gần đây (Wen et al., 2020) áp dụng phương pháp AutoML để phát triển các lớp. Những phương pháp này hoặc đòi hỏi một chính sách được chế tạo đặc biệt để dừng phát triển (ví dụ, sau một số lớp cố định) hoặc dựa vào việc đánh giá độ chính xác trong quá trình huấn luyện, gây ra chi phí tính toán bổ sung đáng kể.
Học bằng Tiếp tục. Các phương pháp tiếp tục thường được sử dụng để xấp xỉ các vấn đề tối ưu hóa khó xử lý bằng cách tăng dần độ khó của mục tiêu cơ bản, ví dụ bằng cách áp dụng các nới lỏng dần dần cho các vấn đề nhị phân. Wu et al. (2019); Xie et al. (2019b; 2020) sử dụng gumbel-softmax (Jang et al., 2017) để lan truyền ngược lỗi trong quá trình tìm kiếm kiến trúc và thưa thớt hóa đặc trưng không gian. Savarese et al. (2020) đề xuất thưa thớt hóa liên tục để tăng tốc cắt tỉa và tìm kiếm vé (Frankle & Carbin, 2019). Mặc dù thành công của các phương pháp tiếp tục trong việc tạo ra các mạng thưa thớt khi hoàn thành huấn luyện, chúng không hoạt động trên các mạng thưa thớt

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
trong quá trình huấn luyện và thay vào đó làm việc với một nới lỏng có giá trị thực. Việc trì hoãn loại bỏ thực tế các thành phần gần như được zero hóa ngăn cản việc áp dụng ngây thơ của những phương pháp này khỏi việc giảm chi phí huấn luyện.

3 PHƯƠNG PHÁP
3.1 KHÔNG GIAN CẤU HÌNH KIẾN TRÚC
Một tôpô mạng có thể được xem như một đồ thị acyclic có hướng bao gồm một chuỗi nút được sắp xếp theo thứ tự. Mỗi nút x(i)in là một đặc trưng đầu vào và mỗi cạnh là một ô tính toán với các siêu tham số có cấu trúc (ví dụ, số bộ lọc và lớp trong mạng tích chập). Một không gian cấu hình kiến trúc có thể được tham số hóa bằng cách liên kết một biến mặt nạ m∈ {0,1} với mỗi ô tính toán (cạnh), cho phép động lực cắt tỉa (m= 1→0) và phát triển (m= 0→1) thời gian huấn luyện.
Như một ví dụ đang chạy, chúng tôi xem xét một không gian cấu hình hai cấp cho kiến trúc CNN, được mô tả trong Hình 2(a), cho phép phát triển động các mạng cả theo chiều rộng (theo kênh) và độ sâu (theo lớp). Các không gian cấu hình khác là có thể; chúng tôi hoãn đến Phụ lục các chi tiết về cách chúng tôi tham số hóa thiết kế của kiến trúc LSTM.
Không gian Cấu hình Kênh CNN: Đối với một lớp tích chập với lin kênh đầu vào, lout kênh đầu ra (bộ lọc) và nhân với kích thước k×k, đặc trưng đầu ra thứ i được tính dựa trên bộ lọc thứ i, tức là cho i∈ {1, . . . , l out}:
x(i)out=f(xin,F(i)·m(i)c), (1)
trong đó m(i)c∈ {0,1} là một tham số nhị phân loại bỏ kênh đầu ra thứ i khi được đặt về zero và f biểu thị phép toán tích chập. m(i)c được chia sẻ qua một bộ lọc và truyền phát đến cùng hình dạng như tensor bộ lọc F(i), cho phép phát triển/cắt tỉa toàn bộ bộ lọc. Như Hình 2(a) (trên) hiển thị, chúng tôi bắt đầu từ một cấu hình kênh mỏng. Sau đó chúng tôi truy vấn các biến chỉ báo và thực hiện chuyển đổi trạng thái: (1) Khi lật một biến chỉ báo từ 0 sang 1 lần đầu tiên, chúng tôi phát triển một bộ lọc được khởi tạo ngẫu nhiên và nối nó vào mạng. (2) Nếu một chỉ báo lật từ 1 sang 0, chúng tôi tạm thời tách bộ lọc tương ứng khỏi đồ thị tính toán; nó sẽ được phát triển trở lại vị trí ban đầu nếu chỉ báo của nó lật trở lại 1, hoặc nếu không sẽ bị cắt tỉa vĩnh viễn ở cuối huấn luyện. (3) Đối với các trường hợp khác, các bộ lọc tương ứng hoặc tồn tại và tiếp tục huấn luyện hoặc vẫn bị tách chờ đợi truy vấn tiếp theo cho các chỉ báo của chúng. Phương pháp của chúng tôi tự động hóa việc tiến hóa kiến trúc, với điều kiện chúng tôi có thể huấn luyện các chỉ báo.
Không gian Cấu hình Lớp CNN: Để phát triển độ sâu mạng, chúng tôi thiết kế một không gian cấu hình lớp trong đó một mạng nông ban đầu sẽ dần dần mở rộng thành một mô hình sâu được huấn luyện, như được hiển thị trong Hình 2(a) (dưới). Tương tự như không gian cấu hình kênh, nơi các bộ lọc phục vụ như các đơn vị cấu trúc cơ bản, chúng tôi yêu cầu một công thức thống nhất để hỗ trợ việc phát triển các mạng phổ biến với các kết nối tắt (ví dụ, ResNet) và không có (ví dụ, mạng đơn giản giống VGG). Chúng tôi đầu tiên giới thiệu một lớp lớp trừu tượng flayer như một đơn vị cấu trúc cơ bản, hoạt động trên các đặc trưng đầu vào xin và tạo ra các đặc trưng đầu ra xout. flayer có thể được thể hiện như các lớp tích chập cho mạng đơn giản hoặc các khối dư cho ResNet, tương ứng. Chúng tôi định nghĩa không gian cấu hình lớp như:
xout=g(xin;flayer·m(j)l) =(flayer(xin),nếu m(j)l= 1
xin, nếu m(j)l= 0, (2)
trong đó m(j)l∈ {0,1} là chỉ báo nhị phân cho lớp thứ j flayer, với đó chúng tôi thực hiện chuyển đổi trạng thái tương tự như không gian cấu hình kênh. Các chỉ báo lớp có ưu tiên hơn các chỉ báo kênh: nếu m(j)l được đặt là 0, tất cả các bộ lọc chứa trong lớp tương ứng sẽ bị tách, bất kể trạng thái của các chỉ báo của chúng. Chúng tôi không tách các lớp thực hiện thay đổi độ phân giải (ví dụ, tích chập có stride).
3.2 PHÁT TRIỂN VỚI THƯA THỚT HÓA LIÊN TỤC CÓ CẤU TRÚC
Chúng tôi có thể tối ưu hóa sự đánh đổi giữa độ chính xác và độ thưa thớt có cấu trúc bằng cách xem xét mục tiêu:
min
w,mc,l,flayerLE(g(f(x;w⊙mc);flayer·ml)) +λ1∥mc∥0+λ2∥ml∥0, (3)

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
trong đó f là phép toán trong Eq. (1) hoặc Eq. (9) (trong Phụ lục A.6), trong khi g được định nghĩa trong Eq. (2). w⊙mc và flayer·ml là các biểu thức tổng quát của các bộ lọc và lớp được thưa thớt hóa có cấu trúc và LE biểu thị một hàm mất mát (ví dụ, mất mát cross-entropy cho phân loại). Các số hạng ℓ0 khuyến khích độ thưa thớt, trong khi λ1,2 là các tham số đánh đổi giữa LE và các hình phạt ℓ0.

Thuật toán 1 : Tối ưu hóa
Đầu vào: Dữ liệu X=(xi)ni=1, nhãn Y=(yi)ni=1
Đầu ra: Mô hình hiệu quả đã phát triển G
Khởi tạo: G,w,u,λbase1 và λbase2.
Đặt ts như tất cả các vector 0 liên kết với các hàm σ.
for epoch = 1 to T do
    Đánh giá độ thưa thớt của G uG và tính toán
    ∆u=u−uG
    Cập nhật λ1←λbase1·∆u;λ2←λbase2·∆u
    trong Eq. (6) sử dụng Eq. (4)
    for r= 1 to R do
        Lấy mẫu mini-batch xi, yi từ X,Y
        Huấn luyện G sử dụng Eq. (6) với SGD
    end for
    Lấy mẫu chỉ báo qc,l∼Bern(σ(βsc,l))
    và ghi lại chỉ số idx nơi giá trị q là 1.
    Cập nhật ts[idx] =ts[idx] + 1
    Cập nhật β sử dụng Eq. (7)
end for
return G

Phát triển Nhận thức Ngân sách. Trong thực tế, việc sử dụng Eq. (3) có thể đòi hỏi tìm kiếm lưới trên λ1 và λ2 cho đến khi một mạng với độ thưa thớt mong muốn được tạo ra. Để tránh một quy trình tốn kém như vậy, chúng tôi đề xuất một quá trình phát triển nhận thức ngân sách, được hướng dẫn bởi một ngân sách mục tiêu theo số tham số mô hình hoặc FLOPs. Thay vì coi λ1 và λ2 như các hằng số, chúng tôi định kỳ cập nhật chúng như:
λ1←λbase1·∆u, λ2←λbase2·∆u , (4)
trong đó ∆u được tính như độ thưa thớt mục tiêu u trừ độ thưa thớt mạng hiện tại uG, và λbase1, λbase2 là các hằng số cơ sở ban đầu. Trong các giai đoạn phát triển sớm, vì mạng quá thưa thớt và ∆u âm, trình tối ưu hóa sẽ đẩy mạng về phía một trạng thái có nhiều dung lượng hơn (rộng hơn/sâu hơn). Hiệu ứng chính quy hóa dần yếu đi khi độ thưa thớt của mạng tiến gần đến ngân sách (và ∆u tiến gần đến zero). Điều này cho phép chúng tôi phát triển mạng một cách thích ứng và tự động điều chỉnh mức độ thưa thớt của nó trong khi đồng thời huấn luyện trọng số mô hình. Phụ lục A.1 cung cấp phân tích chi tiết hơn. Các thí nghiệm của chúng tôi mặc định định nghĩa ngân sách theo số lượng tham số, nhưng cũng điều tra các khái niệm ngân sách khác.

Học bằng Tiếp tục. Một vấn đề khác trong việc tối ưu hóa Eq. (3) là ∥mc∥0 và ∥ml∥0 làm cho vấn đề không thể xử lý được về mặt tính toán do tính chất tổ hợp của các trạng thái nhị phân. Để làm cho không gian cấu hình liên tục và việc tối ưu hóa khả thi, chúng tôi mượn khái niệm học bằng tiếp tục (Cao et al., 2017; Wu et al., 2019; Savarese et al., 2020; Xie et al., 2020). Chúng tôi tham số hóa lại m như dấu nhị phân của một biến liên tục s: sign(s) là 1 nếu s >0 và 0 nếu s <0. Chúng tôi viết lại mục tiêu trong Eq. (3) như:
min
w,sc,l≠0,flayerLE(g(f(x;w⊙sign(sc));flayer·sign(sl))) +λ1∥sign(sc)∥1+λ2∥sign(sl)∥1.(5)

Chúng tôi tấn công vấn đề tối ưu hóa khó khăn và không liên tục trong Eq. (5) bằng cách bắt đầu với một mục tiêu dễ hơn trở nên khó hơn khi huấn luyện tiến triển. Chúng tôi sử dụng một chuỗi các hàm có giới hạn là phép toán dấu: đối với bất kỳ s≠ 0, limβ→∞σ(βs) = sign(s) nếu σ là hàm sigmoid hoặc limβ→0σ(βs) = sign(s) nếu σ là gumbel-softmax exp((−log(s0)+g1(s))/β)/∑j∈{0,1}exp((−log(sj)+gj(s))/β) (Jang et al., 2017), trong đó β > 0 là một tham số nhiệt độ và g0,1 là gumbel. Bằng cách thay đổi β định kỳ, σ(βs) trở nên khó tối ưu hóa hơn, trong khi các mục tiêu hội tụ về mục tiêu rời rạc ban đầu.

Duy trì Thưa thớt hóa Bất kỳ lúc nào. Mặc dù các phương pháp tiếp tục có thể làm cho việc tối ưu hóa khả thi, chúng chỉ tiến hành thưa thớt hóa thông qua một tiêu chí ngưỡng trong giai đoạn suy luận. Trong trường hợp này, kiến trúc thời gian huấn luyện dày đặc và không phù hợp trong bối cảnh phát triển một mạng. Để giảm hiệu quả chi phí tính toán của việc huấn luyện, chúng tôi duy trì một kiến trúc thưa thớt bằng cách giới thiệu một biến phụ trợ được lấy mẫu 0-1 q dựa trên giá trị xác suất σ(βs). Mục tiêu cuối cùng của chúng tôi trở thành:
min
w,sc,l≠0,flayerLE(g(f(x;w⊙σ(βsc)⊙qc);flayer·σ(βsl)·ql)) +λ1∥σ(βsc)∥1+λ2∥σ(βsl)∥1,(6)

trong đó qc và ql là các biến ngẫu nhiên được lấy mẫu từ Bern(σ(βsc)) và Bern(σ(βsl)), hiệu quả duy trì thưa thớt hóa bất kỳ lúc nào và tránh ngưỡng dưới tối ưu, như được hiển thị trong Hình 2(b).

Bộ lập lịch Nhiệt độ Cải tiến. Trong các phương pháp tiếp tục hiện có, giá trị β ban đầu thường được đặt là β0= 1 và một bộ lập lịch được sử dụng ở cuối mỗi epoch huấn luyện để cập nhật β trong tất cả các hàm kích hoạt σ, thường theo β=β0·γt, trong đó t là epoch hiện tại và γ là một siêu tham số (>1 khi σ là hàm sigmoid, <1 khi σ là gumbel softmax). Cả γ và t đều kiểm soát tốc độ

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
Bảng 1: So sánh với các phương pháp cắt tỉa kênh L1-Pruning (Li et al., 2017), SoftNet (He et al., 2018), ThiNet (Luo et al., 2017), Provable (Liebenwein et al., 2020) và BAR (Lemaire et al., 2019) trên CIFAR-10.

mà nhiệt độ tăng trong quá trình huấn luyện. Các phương pháp tiếp tục với bộ lập lịch nhiệt độ toàn cục đã được áp dụng thành công trong cắt tỉa và NAS. Tuy nhiên, trong trường hợp của chúng tôi, một lịch trình toàn cục dẫn đến động lực không cân bằng giữa các biến với xác suất lấy mẫu thấp và cao: tăng nhiệt độ của những biến ít được lấy mẫu ở giai đoạn đầu có thể cản trở việc huấn luyện của chúng hoàn toàn, vì về cuối huấn luyện độ khó tối ưu hóa cao hơn. Để khắc phục vấn đề này, chúng tôi đề xuất một bộ lập lịch nhiệt độ riêng biệt, theo cấu trúc bằng cách thực hiện một sửa đổi đơn giản: đối với mỗi biến mặt nạ, thay vì sử dụng số epoch hiện tại t để tính nhiệt độ của nó, chúng tôi đặt một bộ đếm riêng ts chỉ được tăng khi biến chỉ báo liên quan của nó được lấy mẫu là 1 trong Eq. (6). Chúng tôi định nghĩa bộ lập lịch nhiệt độ theo cấu trúc của chúng tôi là
β=β0·γts, (7)
trong đó ts là các vector liên kết với các hàm σ. Các thí nghiệm sử dụng bộ lập lịch riêng biệt này theo mặc định, nhưng cũng so sánh hai lựa chọn thay thế. Thuật toán 1 tóm tắt quy trình tối ưu hóa của chúng tôi.

4 THÍ NGHIỆM
Chúng tôi đánh giá phương pháp của mình so với các phương pháp cắt tỉa kênh, phát triển mạng và tìm kiếm kiến trúc nơ-ron (NAS) hiện có trên: CIFAR-10 (Krizhevsky et al., 2014) và ImageNet (Deng et al., 2009) cho phân loại ảnh, PASCAL (Everingham et al., 2015) cho phân đoạn ngữ nghĩa và Penn Treebank (PTB) (Marcus et al., 1993) cho mô hình hóa ngôn ngữ. Xem chi tiết tập dữ liệu trong Phụ lục A.2. Trong các bảng, kết quả tốt nhất được đánh dấu bằng chữ in đậm và tốt thứ hai được gạch chân.

4.1 SO SÁNH VỚI CÁC PHƯƠNG PHÁP CẮT TỈA KÊNH
Chi tiết Triển khai. Để so sánh công bằng, chúng tôi chỉ phát triển các bộ lọc trong khi giữ các tham số có cấu trúc khác của mạng (số lớp/khối) giống như các mô hình cơ sở chưa được cắt tỉa. Phương pháp của chúng tôi liên quan đến hai loại biến có thể huấn luyện: trọng số mô hình và trọng số mặt nạ. Đối với trọng số mô hình, chúng tôi áp dụng cùng các siêu tham số được sử dụng để huấn luyện các mô hình cơ sở chưa được cắt tỉa tương ứng, ngoại trừ việc đặt xác suất giữ dropout cho mô hình hóa ngôn ngữ là 0.65. Chúng tôi khởi tạo trọng số mặt nạ sao cho một bộ lọc duy nhất được kích hoạt trong mỗi lớp. Chúng tôi huấn luyện với SGD, tốc độ học ban đầu là 0.1, weight decay là 10−6 và momentum 0.9. Tham số đánh đổi λbase1 được đặt là 0.5 trên tất cả các tác vụ; λ2 không được sử dụng vì chúng tôi không thực hiện phát triển lớp ở đây. Chúng tôi đặt σ là hàm sigmoid và γ là 1001/T trong đó T là tổng số epoch.

VGG-16, ResNet-20, và WideResNet-28-10 trên CIFAR-10. Bảng 1 tóm tắt các mô hình được tạo ra bởi phương pháp của chúng tôi và các phương pháp cắt tỉa kênh cạnh tranh. Lưu ý rằng chi phí huấn luyện được tính dựa trên tổng FLOPs trong các giai đoạn cắt tỉa và phát triển. Phương pháp của chúng tôi tạo ra các mạng thưa thớt hơn với độ suy giảm độ chính xác ít hơn, và liên tục tiết kiệm nhiều tính toán hơn trong quá trình huấn luyện — một hệ quả của việc phát triển từ một mạng đơn giản. Đối với một WideResNet-28-10 được cắt tỉa mạnh mẽ, chúng tôi quan sát thấy rằng BAR (Lemaire et al., 2019) có thể không có đủ dung lượng để đạt được độ giảm độ chính xác không đáng kể, ngay cả với chưng cất kiến thức (Hinton et al., 2015) trong quá trình huấn luyện. Lưu ý rằng chúng tôi

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
Bảng 2: So sánh với các phương pháp cắt tỉa kênh: L1-Pruning (Li et al., 2017), SoftNet (He et al., 2018) và Provable (Liebenwein et al., 2020) trên ImageNet.

báo cáo hiệu suất của phương pháp chúng tôi như mean ± standard deviation, được tính trên 5 lần chạy với các hạt giống ngẫu nhiên khác nhau. Phương sai nhỏ quan sát được cho thấy rằng phương pháp của chúng tôi hoạt động nhất quán qua các lần chạy.

ResNet-50 và MobileNetV1 trên ImageNet. Để xác thực hiệu quả trên các tập dữ liệu quy mô lớn, chúng tôi phát triển, từ đầu, các bộ lọc của ResNet-50 được sử dụng rộng rãi trên ImageNet; chúng tôi không tinh chỉnh. Bảng 2 cho thấy kết quả của chúng tôi tốt nhất so với những kết quả được báo cáo trực tiếp trong các bài báo của các phương pháp cạnh tranh tương ứng. Phương pháp của chúng tôi đạt được 49.7% tiết kiệm suy luận và 47.4% tiết kiệm chi phí huấn luyện về FLOPs trong khi duy trì 75.2% độ chính xác top-1, mà không có bất kỳ giai đoạn tinh chỉnh nào. Phụ lục A.4 hiển thị những cải tiến của chúng tôi trên tác vụ thách thức của việc phát triển các kênh của một MobileNetV1 đã nén. Ngoài ra, Hình 3 hiển thị sự đánh đổi độ chính xác top-1/FLOPs cho MobileNetV1 trên ImageNet, chứng minh rằng phương pháp của chúng tôi thống trị các phương pháp cạnh tranh.

Deeplab-v3-ResNet-101 trên PASCAL VOC. Phụ lục A.5 cung cấp kết quả phân đoạn ngữ nghĩa.

2-Stacked-LSTMs trên PTB: Chúng tôi chi tiết các phần mở rộng cho các tế bào hồi quy và so sánh phương pháp đề xuất của chúng tôi với ISS dựa trên LSTM xếp chồng hai lớp vanilla trong Phụ lục A.6. Như được hiển thị trong Bảng 8, phương pháp của chúng tôi tìm thấy cấu trúc mô hình nén hơn với chi phí huấn luyện thấp hơn, trong khi đạt được độ phức tạp tương tự trên cả tập validation và test.

4.2 SO SÁNH VỚI AUTOGROW
Chi tiết Triển khai. Chúng tôi phát triển cả bộ lọc và lớp. Chúng tôi tuân theo các cài đặt của AutoGrow trong việc khám phá các biến thể kiến trúc định nghĩa mạng hạt giống ban đầu, không gian cấu hình theo lớp và các đơn vị cấu trúc cơ bản flayer của chúng tôi: Basic3ResNet, Bottleneck4ResNet, Plain3Net, Plain4Net. Khác với việc khởi tạo của AutoGrow sử dụng các bộ lọc kích thước đầy đủ trong mỗi lớp, không gian cấu hình theo kênh của chúng tôi bắt đầu từ bộ lọc đơn và mở rộng đồng thời với các lớp. Phụ lục A.7 chứa một so sánh chi tiết về các kiến trúc hạt giống. Đối với việc huấn luyện trọng số mô hình, chúng tôi áp dụng các siêu tham số của các mô hình ResNet hoặc VGG tương ứng với các biến thể hạt giống ban đầu. Đối với các biến mặt nạ theo lớp và theo kênh, chúng tôi khởi tạo các trọng số sao cho chỉ có một bộ lọc duy nhất trong mỗi lớp và một đơn vị cơ bản trong mỗi giai đoạn (ví dụ, BasicBlock trong Basic3ResNet) hoạt động. Chúng tôi sử dụng huấn luyện SGD với tốc độ học ban đầu là 0.1, weight decay là 10−6 và momentum 0.9 trên tất cả các tập dữ liệu. Bộ lập lịch tốc độ học giống như đối với trọng số mô hình tương ứng. Các tham số đánh đổi λbase1 và λbase2 được đặt là 1.0 và 0.1 trên tất cả các tập dữ liệu. Để so sánh công bằng, chúng tôi tinh chỉnh các mô hình cuối cùng của mình với 40 epoch và 20 epoch trên CIFAR-10 và ImageNet, tương ứng.

Kết quả trên CIFAR-10 và ImageNet. Bảng 3 so sánh kết quả của chúng tôi với AutoGrow. Đối với tất cả các biến thể phát triển theo lớp trên cả hai tập dữ liệu, phương pháp của chúng tôi liên tục mang lại cấu hình độ sâu và chiều rộng tốt hơn so với AutoGrow, về mặt độ chính xác và sự đánh đổi chi phí huấn luyện/suy luận. Về thời gian huấn luyện của Bottleneck4ResNet trên ImageNet, AutoGrow đòi hỏi 61.6 giờ cho giai đoạn phát triển và 78.6 giờ cho tinh chỉnh trên 4 GPU TITAN V, trong khi phương pháp của chúng tôi mất 48.2 và 31.3 giờ, tương ứng. Phương pháp của chúng tôi cung cấp 43% tiết kiệm thời gian huấn luyện hơn so với AutoGrow. Chúng tôi không chỉ đòi hỏi ít epoch huấn luyện hơn, mà còn phát triển từ một bộ lọc duy nhất thành một mạng tương đối thưa thớt, trong khi AutoGrow luôn giữ các tập bộ lọc kích thước đầy đủ mà không có bất kỳ phân bổ lại nào.

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
4.3 SO SÁNH VỚI CÁC PHƯƠNG PHÁP NAS
Như một so sánh công bằng với các phương pháp NAS liên quan đến các giai đoạn tìm kiếm và huấn luyện lại, chúng tôi cũng chia phương pháp của mình thành các giai đoạn phát triển và huấn luyện. Cụ thể, chúng tôi phát triển các lớp và kênh từ kiến trúc hạt giống Bottleneck4ResNet trực tiếp trên ImageNet bằng cách đặt λbase1= 2.0, λbase2= 0.1 và ngân sách tham số dưới 7M. Sau đó chúng tôi tiếp tục huấn luyện kiến trúc đã phát triển và so sánh với các phương pháp NAS hiện có về mặt tham số, độ chính xác validation top-1 và số giờ GPU V100 được yêu cầu bởi các giai đoạn tìm kiếm hoặc phát triển, như được hiển thị trong Bảng 4. Lưu ý rằng DARTS (Liu et al., 2019) tiến hành tìm kiếm trên CIFAR-10, sau đó chuyển sang ImageNet thay vì tìm kiếm trực tiếp. Điều này là do DARTS hoạt động trên một siêu mạng bằng cách bao gồm tất cả các đường dẫn ứng viên và gặp phải vụ nổ bộ nhớ GPU. Về FLOPs theo epoch, kết quả được hiển thị trong Hình 1(c) là để huấn luyện tương đương một ResNet-20 trên CIFAR-10 so với DARTS và phương pháp cắt tỉa kênh Provable (Liebenwein et al., 2020). Cũng lưu ý rằng kiến trúc EfficientNet-B0, được bao gồm trong Bảng 4, được tạo ra bằng tìm kiếm lưới trong không gian tìm kiếm MnasNet, do đó có cùng chi phí tìm kiếm nặng. Để đạt được hiệu suất được báo cáo, EfficientNet-B0 sử dụng thêm các mô-đun squeeze-and-excitation (SE) (Hu et al., 2018), AutoAugment (Cubuk et al., 2019), cũng như các epoch huấn luyện lại lâu hơn nhiều trên ImageNet.

Bảng 4: So sánh hiệu suất với các phương pháp NAS AmoebaNet-A (Real et al., 2019), MnasNet (Tan et al., 2019), EfficientNet-B0 (Tan & Le, 2019), DARTS (Liu et al., 2019) và ProxylessNet (Cai et al., 2019) trên ImageNet.

ProxylessNet vẫn bắt đầu với một siêu mạng được tham số hóa quá mức, nhưng áp dụng một phương pháp tìm kiếm giống như cắt tỉa bằng cách nhị phân hóa các tham số kiến trúc và buộc chỉ một đường dẫn được kích hoạt tại thời điểm tìm kiếm. Điều này cho phép tìm kiếm trực tiếp trên ImageNet, đạt được tiết kiệm chi phí tìm kiếm 200× so với MnasNet. Tương phản với ProxylessNet, phương pháp của chúng tôi dần dần thêm các bộ lọc và lớp vào các kiến trúc hạt giống đơn giản trong khi duy trì thưa thớt hóa, dẫn đến tiết kiệm không chỉ tính toán theo epoch mà còn tiêu thụ bộ nhớ, cho phép huấn luyện nhanh hơn, batch lớn hơn. Như vậy, chúng tôi tiết kiệm thêm 45% số giờ tìm kiếm GPU, trong khi đạt được sự đánh đổi độ chính xác-tham số tương đương.

4.4 PHÂN TÍCH
Tiết kiệm Chi phí Huấn luyện. Hình 4 minh họa động lực thưa thớt hóa của chúng tôi, hiển thị FLOPs theo epoch trong khi phát triển một ResNet-20. Phụ lục A.8 trình bày các hình ảnh hóa bổ sung. Ngay cả với phần cứng GPU song song hoàn toàn, việc bắt đầu với ít bộ lọc và lớp trong mạng cuối cùng sẽ tiết kiệm thời gian wall-clock, vì việc huấn luyện batch lớn hơn (Goyal et al., 2017) luôn có thể được sử dụng để lấp đầy phần cứng.

Hình 5 hiển thị độ chính xác validation, độ phức tạp mô hình và số lượng lớp trong khi phát triển Basic3ResNet. Độ phức tạp được đo như tỷ lệ tham số mô hình của mô hình mục tiêu của AutoGrow. Ở cuối 160 epoch, độ chính xác validation của phương pháp chúng tôi là 92.36%, cao hơn 84.65% của AutoGrow ở 360 epoch. Do đó chúng tôi đòi hỏi ít epoch tinh chỉnh hơn để đạt được độ chính xác cuối cùng 94.50% trên CIFAR.

Bảng 5: So sánh với baseline cắt tỉa ngẫu nhiên trên CIFAR-10.

Phát triển Nhận thức Ngân sách. Trong Hình 6, đối với ResNet-20 trên CIFAR-10, chúng tôi so sánh các kiến trúc thu được bởi (1) cắt tỉa đồng nhất: một phương pháp cắt tỉa được định nghĩa trước ngây thơ cắt tỉa cùng tỷ lệ phần trăm kênh trong mỗi lớp, (2) của chúng tôi: các biến thể của phương pháp chúng tôi bằng cách đặt các mức độ thưa thớt tham số mô hình khác nhau như ngân sách mục tiêu trong quá trình phát triển, và (3) thiết kế trực tiếp: các kiến trúc đã phát triển của chúng tôi được khởi tạo lại với trọng số ngẫu nhiên và huấn luyện lại. Trong hầu hết các cài đặt ngân sách, phương pháp phát triển của chúng tôi vượt trội hơn thiết kế trực tiếp và cắt tỉa đồng nhất, chứng minh

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2021
[Hình 3-6 và các mô tả tương ứng]

hiệu quả tham số cao hơn. Phương pháp của chúng tôi cũng dường như có hiệu ứng tích cực về mặt chính quy hóa hoặc động lực tối ưu hóa, điều này bị mất nếu người ta cố gắng huấn luyện trực tiếp cấu trúc nền tảng cuối cùng. Phụ lục A.9 điều tra các mục tiêu ngân sách dựa trên FLOPs.

So sánh với Baseline Ngẫu nhiên. Ngoài baseline cắt tỉa đồng nhất trong Hình 6, chúng tôi cũng so sánh với một baseline lấy mẫu ngẫu nhiên để tách biệt thêm sự đóng góp của không gian cấu hình và phương pháp phát triển của chúng tôi, theo tiêu chí trong (Xie et al., 2019a; Li & Talwalkar, 2019; Yu et al., 2020; Radosavovic et al., 2019). Cụ thể, baseline ngẫu nhiên này thay thế quy trình lấy mẫu các mục của q trong Eq. 6. Thay vì sử dụng xác suất lấy mẫu được lấy từ các tham số mặt nạ đã học s, nó lấy mẫu với xác suất cố định. Như được hiển thị trong Bảng 5, phương pháp của chúng tôi liên tục hoạt động tốt hơn nhiều so với baseline ngẫu nhiên này. Những kết quả này, cũng như các baseline tinh vi hơn trong Hình 6, chứng minh hiệu quả của phương pháp phát triển và cắt tỉa của chúng tôi.

Bộ lập lịch Nhiệt độ. Chúng tôi so sánh kiểm soát nhiệt độ theo cấu trúc của chúng tôi với một cái toàn cục trong các thí nghiệm phát triển kênh trên CIFAR-10 sử dụng VGG-16, ResNet-20, và WideResNet-28-10. Kết quả Bảng 1 sử dụng bộ lập lịch theo cấu trúc của chúng tôi. Để đạt được độ thưa thớt tương tự với bộ lập lịch toàn cục, các mô hình tương ứng chịu độ giảm độ chính xác 1.4%, 0.6%, và 2.5%. Với bộ lập lịch toàn cục, việc tối ưu hóa các biến mặt nạ dừng sớm trong huấn luyện và các epoch tiếp theo tương đương với việc huấn luyện trực tiếp một mạng nền tảng cố định. Điều này có thể buộc mạng bị mắc kẹt với một kiến trúc dưới tối ưu. Phụ lục A.10 điều tra các tương tác giữa tốc độ học và lịch trình nhiệt độ.

5 KẾT LUẬN
Chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả để phát triển các mạng sâu hiệu quả thông qua thưa thớt hóa liên tục có cấu trúc, giảm chi phí tính toán không chỉ của suy luận mà còn của huấn luyện. Phương pháp đơn giản để triển khai và nhanh để thực thi; nó tự động hóa quá trình phân bổ lại cấu trúc mạng dưới ngân sách tài nguyên thực tế. Ứng dụng cho các mạng sâu được sử dụng rộng rãi trên nhiều tác vụ khác nhau cho thấy rằng phương pháp của chúng tôi liên tục tạo ra các mô hình với sự đánh đổi độ chính xác-hiệu quả tốt hơn so với các phương pháp cạnh tranh, trong khi đạt được tiết kiệm chi phí huấn luyện đáng kể.

Lời cảm ơn. Công trình này được hỗ trợ bởi Trung tâm CERES của Đại học Chicago cho Tính toán Không thể Dừng và Quỹ Khoa học Quốc gia dưới tài trợ CNS-1956180.

--- TRANG 10-18 ---
[Phần tài liệu tham khảo và phụ lục được dịch tương tự, giữ nguyên định dạng và cấu trúc như bản gốc]
