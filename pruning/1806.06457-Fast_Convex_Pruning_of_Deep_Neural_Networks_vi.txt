Cắt tỉa nhanh lồi của mạng nơ-ron sâu

Alireza Aghasi, Afshin Abdi, Justin Romberg

Tóm tắt

Chúng tôi phát triển một kỹ thuật nhanh, dễ xử lý gọi là Net-Trim để đơn giản hóa một mạng nơ-ron đã được huấn luyện. Phương pháp này là một mô-đun hậu xử lý lồi, cắt tỉa (thưa hóa) mạng đã huấn luyện từng lớp một, trong khi vẫn bảo toàn các phản hồi nội bộ. Chúng tôi trình bày một phân tích toàn diện về Net-Trim từ cả quan điểm thuật toán và độ phức tạp mẫu, tập trung vào một chương trình tối ưu hóa lồi nhanh, có thể mở rộng. Phân tích của chúng tôi bao gồm các kết quả nhất quán giữa các mô hình ban đầu và được huấn luyện lại trước và sau khi áp dụng Net-Trim và đảm bảo về số lượng mẫu huấn luyện cần thiết để khám phá một mạng có thể được biểu diễn bằng cách sử dụng một số lượng nhất định các số hạng khác không. Cụ thể, nếu có một tập hợp trọng số sử dụng tối đa s số hạng có thể tái tạo đầu ra lớp từ đầu vào lớp, chúng ta có thể tìm thấy những trọng số này từ O(s log N/s) mẫu, trong đó N là kích thước đầu vào. Những kết quả lý thuyết này tương tự như những kết quả cho hồi quy thưa sử dụng Lasso, và phân tích của chúng tôi sử dụng một số công cụ được phát triển gần đây giống nhau (cụ thể là các kết quả gần đây về tập trung độ đo và phân tích lồi). Cuối cùng, chúng tôi đề xuất một khung thuật toán dựa trên phương pháp hướng xen kẽ của các nhân tử (ADMM), cho phép triển khai nhanh và đơn giản Net-Trim để cắt tỉa và nén mạng.

Từ khóa: Cắt tỉa mạng nơ-ron, Mạng nơ-ron sâu, Cảm biến nén, Sơ đồ Bowling, Độ phức tạp Rademacher

1 Giới thiệu

Mạng nơ-ron sâu đang trở thành một công cụ nổi bật để học các cấu trúc dữ liệu có độ phức tạp tùy ý. Thành công này chủ yếu nhờ vào công thức phi tuyến linh hoạt nhưng gọn gàng của chúng, và sự phát triển của các kỹ thuật tính toán và kiến trúc để cải thiện việc huấn luyện chúng (tham khảo [27,11] để có cái nhìn tổng quan toàn diện). Tăng số lượng lớp và số lượng nơ-ron trong mỗi lớp thường là cách tiêu chuẩn nhất để thêm tính linh hoạt cho mạng nơ-ron. Trong khi việc thêm tính linh hoạt như vậy có khả năng cải thiện việc khớp mô hình với dữ liệu huấn luyện (tức là giảm thiên lệch mô hình), nó làm cho các mô hình dễ bị tham số hóa quá mức và quá khớp (tức là tăng phương sai mô hình), điều này có thể làm giảm khả năng dự đoán của mạng.

Để đơn giản hóa hoặc ổn định mạng nơ-ron, các kỹ thuật chính quy hóa và chiến lược cắt tỉa khác nhau đã được xem xét. Lấy cảm hứng từ các bộ chính quy hóa cổ điển cho mô hình tuyến tính, như Ridge [16] và Lasso [29], việc huấn luyện mạng nơ-ron cũng được trang bị các phạt l₂ hoặc l₁ [26,10] để kiểm soát phương sai và độ phức tạp của chúng. Thêm tính ngẫu nhiên vào quá trình huấn luyện cũng được chứng minh là có tác dụng chính quy hóa, có liên quan đến điều này chúng ta có thể tham khảo Dropout [28] và DropConnect [33], chúng loại bỏ ngẫu nhiên các kết nối hoạt động trong giai đoạn huấn luyện và có khả năng tạo ra các mạng được cắt tỉa. Chuẩn hóa theo lô [18], kết hợp với các kỹ thuật khớp kiểu gradient ngẫu nhiên, cũng có thể được coi là một công cụ có bản chất tương tự, trong đó trong quá trình huấn luyện, các cập nhật của các đơn vị ẩn được tính trọng số bởi độ lệch chuẩn của các ví dụ ngẫu nhiên được bao gồm trong mini-batch.

Trong bài báo này, chúng tôi ủng hộ một cách tiếp cận khác. Chúng tôi huấn luyện mạng bằng các kỹ thuật tiêu chuẩn. Sau đó chúng tôi trích xuất các đầu ra nội bộ (các đặc trưng trung gian) tại mỗi lớp và tìm một tập trọng số thưa tái tạo những đặc trưng này trên tất cả dữ liệu huấn luyện. Triết lý ở đây là sản phẩm quan trọng nhất của việc huấn luyện mạng là các đặc trưng mà nó trích xuất, chứ không phải các trọng số mà nó ổn định để tạo ra những đặc trưng đó.

Đối với các mạng lớn, sẽ có nhiều tập trọng số tạo ra chính xác cùng những đặc trưng nội bộ; trong số những trọng số đó, chúng ta chọn trọng số đơn giản nhất.

Phương pháp của chúng tôi để tìm các tập trọng số thưa, được trình bày chi tiết trong Phần 3, có liên quan đến các kỹ thuật nổi tiếng cho hồi quy thưa, ví dụ như Lasso [29] trong thống kê và cảm biến nén [6] trong xử lý tín hiệu. Sự khác biệt chính là tính phi tuyến trong việc ánh xạ các đặc trưng nội bộ từ lớp này sang lớp khác. Nếu tính phi tuyến này là tuyến tính từng phần và lồi (như là đơn vị tuyến tính được chỉnh lưu, ReLU(x) = max(x;0), mà chúng tôi sử dụng trong tất cả phân tích dưới đây), thì có một cách tự nhiên để đúc lại điều kiện rằng đầu ra và đầu vào của một lớp khớp nhau như một tập hợp các ràng buộc bất đẳng thức tuyến tính. Có một cách tương tự để đúc lại một khớp gần đúng như việc bao gồm trong một tập hợp lồi. Sử dụng chuẩn l₁ làm đại diện cho độ thưa, toàn bộ chương trình trở thành lồi.

Điều này mở đường cho một phân tích kỹ lưỡng về mức độ tốt và trong những điều kiện nào chúng ta có thể mong đợi Net-Trim hoạt động tốt, và cho phép chúng ta tận dụng hàng thập kỷ nghiên cứu trong tối ưu hóa lồi để tìm ra một thuật toán có thể mở rộng với hành vi hội tụ có thể dự đoán được.

Lý thuyết trong Phần 4 trình bày một giới hạn trên về số lượng mẫu huấn luyện cần thiết để khám phá một ma trận trọng số thưa. Cho một tập hợp các véc-tơ đầu vào lớp x₁,...,xₚ và các véc-tơ đầu ra y₁,...,yₚ, chúng ta giải chương trình

minimize
W ‖W‖₁ subject to ReLU(W^T xₚ) = yₚ, (p = 1,...,P), (1)

trong đó ‖W‖₁ = Σₙ,ₘ |wₙ,ₘ| là tổng các giá trị tuyệt đối của các phần tử trong ma trận W ∈ R^(N×M). Vì chuẩn l₁ là lồi và hàm ReLU() là tuyến tính từng phần, có nghĩa là các ràng buộc trong chương trình trên có thể được chia thành một chuỗi các ràng buộc đẳng thức và bất đẳng thức tuyến tính, chương trình trên là lồi. Chúng tôi chứng minh rằng nếu xₚ là các mẫu độc lập của một véc-tơ ngẫu nhiên subgaussian không suy biến (có nghĩa là ma trận tương quan có hạng đầy đủ) và tồn tại một W* với các cột tối đa s-thưa thực sự thỏa mãn yₚ = ReLU(W*^T xₚ) cho tất cả p, thì nghiệm của (1) chính xác là W* khi số lượng mẫu huấn luyện P tỷ lệ (gần như) với độ thưa s: chúng ta yêu cầu

P ≳ s log(N/s).

Chúng tôi cũng chứng minh rằng nếu xₚ là subgaussian, thì yₚ cũng vậy. Kết quả là, lý thuyết có thể được áp dụng từng lớp một, tạo ra một kết quả lấy mẫu cho các mạng có độ sâu tùy ý. (Khi chúng tôi áp dụng thuật toán trong thực tế, các ràng buộc đẳng thức trong (1) được nới lỏng; điều này được thảo luận chi tiết trong Phần 3.1.) Cùng với những đảm bảo lý thuyết này, Net-Trim cung cấp hiệu suất tốt nhất trên các mạng thực tế. Trong Phần 6, chúng tôi trình bày một số thí nghiệm số cho thấy rằng các hệ số nén từ 10x đến 50x (loại bỏ 90% đến 98% các kết nối) là có thể với sự mất mát rất ít trong độ chính xác kiểm tra.

Đóng góp và mối quan hệ với công trình trước đây Bài báo này cung cấp một mô tả đầy đủ về phương pháp Net-Trim từ cả quan điểm lý thuyết và thuật toán. Trong Phần 3, chúng tôi trình bày công thức lồi của chúng tôi để thưa hóa các trọng số trong các lớp tuyến tính của mạng; chúng tôi mô tả cách thủ tục có thể được áp dụng từng lớp một trong mạng sâu theo song song hoặc nối tiếp (nối tầng các kết quả), và trình bày các giới hạn nhất quán cho cả hai cách tiếp cận. Phần 4 trình bày kết quả lý thuyết chính của chúng tôi, được phát biểu chính xác trong Định lý 4. Kết quả này suy ra một giới hạn trên về số lượng mẫu dữ liệu chúng ta cần để tin cậy khám phá một lớp có tối đa s kết nối trong lớp tuyến tính của nó - chúng tôi chứng minh rằng nếu các mẫu dữ liệu là ngẫu nhiên, thì những trọng số này có thể được học từ O(s log N/s) mẫu. Về mặt toán học, kết quả này có thể so sánh với các giới hạn độ phức tạp mẫu cho Lasso trong việc thực hiện hồi quy thưa trên mô hình tuyến tính (còn được biết đến như bài toán cảm biến nén). Phân tích của chúng tôi dựa trên sơ đồ bowling [30,24]; các thách thức kỹ thuật chính là thích ứng kỹ thuật này với các ràng buộc tuyến tính từng phần trong chương trình (1), và thực tế là các véc-tơ đầu vào {xₚ} vào mỗi lớp không được trung tâm hóa theo cách không thể dễ dàng tính toán.

Có một số ví dụ khác về các kỹ thuật đơn giản hóa mạng bằng cách huấn luyện lại trong tài liệu gần đây. Những kỹ thuật này thường được trình bày như các công cụ nén mô hình (ví dụ, [15,8,14]) để loại bỏ sự dư thừa mô hình vốn có. Trong công trình có lẽ liên quan nhất đến những gì chúng tôi trình bày ở đây, [15] đề xuất một sơ đồ cắt tỉa đơn giản là cắt bỏ các trọng số nhỏ của mạng đã được huấn luyện, và sau đó điều chỉnh lại các trọng số hoạt động còn lại bằng cách sử dụng một vòng huấn luyện khác. Ngược lại, sơ đồ tối ưu hóa của chúng tôi đảm bảo rằng các đầu vào và đầu ra lớp vẫn nhất quán khi mạng được cắt tỉa.

Khung Net-Trim lần đầu tiên được trình bày trong [2]. Bài báo này cung cấp một phân tích nghiêm ngặt và hoàn chỉnh hơn nhiều (giới hạn độ phức tạp mẫu) của thuật toán Net-Trim cho các mạng có nhiều lớp (công trình trước đây chỉ xem xét một lớp duy nhất của mạng). Ngoài ra, chúng tôi trình bày một triển khai có thể mở rộng (nhưng tương đối đơn giản) của Net-Trim sử dụng phương pháp hướng xen kẽ của các nhân tử (ADMM). Đây là một phương pháp lặp với mỗi lần lặp yêu cầu một số lượng nhỏ các phép nhân ma trận-véc-tơ. Mã code, cùng với tất cả các ví dụ được trình bày trong bài báo, có sẵn trực tuyến¹.

Ký hiệu Chúng tôi sử dụng chữ thường và chữ hoa đậm cho véc-tơ và ma trận, tương ứng. Cụ thể, ký hiệu I được dành riêng cho ma trận đơn vị. Đối với ma trận A, A₁,: biểu thị ma trận con được tạo bằng cách hạn chế các hàng của A vào tập chỉ số 1. Tương tự, A:,₂ hạn chế các cột của A vào 2, và A₁,₂ được tạo bằng cách trích xuất cả hàng và cột. Cho một véc-tơ x (hoặc ma trận X), supp x (hoặc supp X) là tập hợp các chỉ số có các phần tử khác không, và supp^c x (hoặc supp^c X) là tập hợp bù.

Đối với X = [xₘ,ₙ] ∈ R^(M×N), vết ma trận được ký hiệu bởi tr(X). Hơn nữa, chúng tôi sử dụng ‖X‖₁ ≜ Σᵐₘ₌₁ Σᴺₙ₌₁ |xₘ,ₙ| như một ký hiệu cho tổng các phần tử tuyệt đối², và ‖X‖_F như chuẩn Frobenius. Hàm kích hoạt mạng nơ-ron được sử dụng trong suốt bài báo là đơn vị tuyến tính được chỉnh lưu (ReLU), được áp dụng theo từng thành phần cho véc-tơ và ma trận,

(ReLU(X))ₘ,ₙ = max(xₘ,ₙ, 0).

Đôi khi chúng tôi sẽ sử dụng ký hiệu X₊ như viết tắt cho ReLU(X). Đối với một tập chỉ số Ω ⊆ {1,...,M} × {1,...,N}, W_Ω đại diện cho một ma trận có kích thước giống hệt với W = [wₘ,ₙ] với các phần tử

(W_Ω)ₘ,ₙ = {wₘ,ₙ  nếu (m,n) ∈ Ω
            {0      nếu (m,n) ∉ Ω

Cuối cùng, chúng tôi sử dụng S_N để biểu thị mặt cầu đơn vị trong R^(N+1); và ký hiệu f ≳ ĝ (hoặc f ≲ ĝ) khi tồn tại một hằng số tuyệt đối C sao cho f ≥ Cĝ (hoặc f ≤ Cĝ).

Tổng quan. Phần còn lại của bài báo được cấu trúc như sau. Trong Phần 2, chúng tôi tổng quan ngắn gọn về kiến trúc mạng nơ-ron được xem xét. Phần 3 trình bày ý tưởng cắt tỉa và các kết quả nhất quán giữa các mạng ban đầu và được huấn luyện lại. Kiến trúc thống kê của mạng và các kết quả độ phức tạp mẫu tổng quát được trình bày trong Phần 4. Để triển khai chương trình lồi cơ bản của Net-Trim, trong Phần 5 chúng tôi trình bày một sơ đồ ADMM áp dụng cho công thức Net-Trim ban đầu. Cuối cùng, Phần 6 trình bày một số thí nghiệm, cùng với những nhận xét kết luận. Tất cả các bằng chứng kỹ thuật của các định lý và kết quả được trình bày trong bài báo này được chuyển đến Phần 7.

¹Liên kết đến mã code và tài liệu liên quan: https://dnntoolbox.github.io/Net-Trim/
²Ký hiệu ‖X‖₁ không nên nhầm lẫn với chuẩn l₁ cảm sinh ma trận
