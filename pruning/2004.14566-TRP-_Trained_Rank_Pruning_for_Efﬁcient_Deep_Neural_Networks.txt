# 2004.14566.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2004.14566.pdf
# File size: 694748 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
TRP: Trained Rank Pruning for EfÔ¨Åcient Deep Neural Networks
Yuhui Xu1,Yuxi Li1,Shuai Zhang2,Wei Wen3,Botao Wang2,
Yingyong Qi2,Yiran Chen3,Weiyao Lin1,Hongkai Xiong1
1Shanghai Jiao Tong University2Qualcomm AI Research3Duke University
fyuhuixu, lyxok1, wylin, xionghongkai g@sjtu.edu.cn,
fshuazhan, botaow, yingyong g@qti.qualcomm.com, fwei.wen, yiran.chen g@duke.edu
Abstract
To enable DNNs on edge devices like mobile
phones, low-rank approximation has been widely
adopted because of its solid theoretical rationale
and efÔ¨Åcient implementations. Several previous
works attempted to directly approximate a pre-
trained model by low-rank decomposition; how-
ever, small approximation errors in parameters can
ripple over a large prediction loss. As a result, per-
formance usually drops signiÔ¨Åcantly and a sophis-
ticated effort on Ô¨Åne-tuning is required to recover
accuracy. Apparently, it is not optimal to sepa-
rate low-rank approximation from training. Un-
like previous works, this paper integrates low rank
approximation and regularization into the train-
ing process. We propose Trained Rank Pruning
(TRP), which alternates between low rank approxi-
mation and training. TRP maintains the capacity of
the original network while imposing low-rank con-
straints during training. A nuclear regularization
optimized by stochastic sub-gradient descent is uti-
lized to further promote low rank in TRP. The TRP
trained network inherently has a low-rank structure,
and is approximated with negligible performance
loss, thus eliminating the Ô¨Åne-tuning process after
low rank decomposition. The proposed method is
comprehensively evaluated on CIFAR-10 and Ima-
geNet, outperforming previous compression meth-
ods using low rank approximation.
1 Introduction
Deep Neural Networks (DNNs) have shown remarkable suc-
cess in many computer vision tasks. Despite the high perfor-
mance in server-based DNNs powered by cutting-edge par-
allel computing hardware, most state-of-the-art architectures
are not yet ready to be deployed on mobile devices due to the
limitations on computational capacity, memory and power.
This work was supported in part by the National Natural Sci-
ence Foundation of China under Grants 61720106001, 61932022,
and in part by the Program of Shanghai Academic Research Leader
under Grant 17XD1401900.To address this problem, many network compression and
acceleration methods have been proposed. Pruning based
methods [Han et al. , 2015b; He et al. , 2017; Liu et al. , 2017;
Luo et al. , 2017 ]explore the sparsity in weights and Ô¨Ål-
ters. Quantization based methods [Han et al. , 2015b; Zhou
et al. , 2017; Courbariaux and Bengio, 2016; Rastegari et
al., 2016; Xu et al. , 2018 ]reduce the bit-width of network
parameters. Low-rank decomposition [Denton et al. , 2014;
Jaderberg et al. , 2014; Guo et al. , 2018; Wen et al. , 2017;
Alvarez and Salzmann, 2017 ]minimizes the channel-wise
and spatial redundancy by decomposing the original network
into a compact one with low-rank layers. In addition, efÔ¨Åcient
architectures [Sandler et al. , 2018; Ma et al. , 2018 ]are care-
fully designed to facilitate mobile deployment of deep neural
networks. Different from precedent works, this paper pro-
poses a novel approach to design low-rank networks.
Low-rank networks can be trained directly from scratch.
However, it is difÔ¨Åcult to obtain satisfactory results for sev-
eral reasons. (1) Low capacity: compared with the original
full rank network, the capacity of a low-rank network is lim-
ited, which causes difÔ¨Åculties in optimizing its performances.
(2)Deep structure: low-rank decomposition typically dou-
bles the number of layers in a network. The additional layers
make numerical optimization much more vulnerable to gradi-
ents explosion and/or vanishing. (3) Heuristic rank selection:
the rank of decomposed network is often chosen as a hyper-
parameter based on pre-trained networks; this may not be the
optimal rank for the network trained from scratch.
Alternatively, several previous works [Zhang et al. , 2016;
Guo et al. , 2018; Jaderberg et al. , 2014 ]attempted to decom-
pose pre-trained models in order to get initial low-rank net-
works. However, the heuristically imposed low-rank could
incur huge accuracy loss and network retraining is needed
to recover the performance of the original network as much
as possible. Some attempts were made to use sparsity regu-
larization [Wen et al. , 2017; Chen et al. , 2015 ]to constrain
the network into a low-rank space. Though sparsity regular-
ization reduces the error incurred by decomposition to some
extent, performance still degrades rapidly when compression
rate increases.
This paper is an extension of [Xuet al. , 2019 ]. In this pa-
per, we propose a new method, namely Trained Rank Prun-
ing (TRP), for training low-rank networks. We embed the
low-rank decomposition into the training process by gradu-arXiv:2004.14566v1  [cs.LG]  30 Apr 2020

--- PAGE 2 ---
ally pushing the weight distribution of a well functioning net-
work into a low-rank form, where all parameters of the orig-
inal network are kept and optimized to maintain its capacity.
We also propose a stochastic sub-gradient descent optimized
nuclear regularization that further constrains the weights in
a low-rank space to boost the TRP. The proposed solution is
illustrated in Fig. 1.
Overall, our contributions are summarized below.
1. A new training method called the TRP is presented by
explicitly embedding the low-rank decomposition into
the network training;
2. A nuclear regularization is optimized by stochastic sub-
gradient descent to boost the performance of the TRP;
3. Improving inference acceleration and reducing approx-
imation accuracy loss in both channel-wise and spatial-
wise decomposition methods.
2 Related Works
A lot of works have been proposed to accelerate the inference
process of deep neural networks. BrieÔ¨Çy, these works could
be categorized into three main categories: quantization, prun-
ing, and low-rank decomposition.
Quantization Weight quantization methods include train-
ing a quantized model from scratch [Chen et al. , 2015;
Courbariaux and Bengio, 2016; Rastegari et al. , 2016 ]or
converting a pre-trained model into quantized representation
[Zhou et al. , 2017; Han et al. , 2015a; Xu et al. , 2018 ]. The
quantized weight representation includes binary value [Raste-
gari et al. , 2016; Courbariaux and Bengio, 2016 ]or hash
buckets [Chen et al. , 2015 ]. Note that our method is inspired
by the scheme of combining quantization with training pro-
cess, i.e.we embed the low-rank decomposition into training
process to explicitly guide the parameter to a low-rank form.
Pruning Non-structured and structured sparsity are intro-
duced by pruning. [Han et al. , 2015b ]proposes to prune
unimportant connections between neural units with small
weights in a pre-trained CNN. [Wen et al. , 2016 ]utilizes
group Lasso strategy to learn the structure sparsity of net-
works. [Liuet al. , 2017 ]adopts a similar strategy by explic-
itly imposing scaling factors on each channel to measure the
importance of each connection and dropping those with small
weights. In [Heet al. , 2017 ], the pruning problem is formu-
lated as a data recovery problem. Pre-trained Ô¨Ålters are re-
weighted by minimizing a data recovery objective function.
Channels with smaller weight are pruned. [Luoet al. , 2017 ]
heuristically selects Ô¨Ålters using change of next layer‚Äôs output
as a criterion.
Low-rank decomposition Original models are decom-
posed into compact ones with more lightweight layers. [Jader-
berg et al. , 2014 ]considers both the spatial-wise and channel-
wise redundancy and proposes decomposing a Ô¨Ålter into two
cascaded asymmetric Ô¨Ålters. [Zhang et al. , 2016 ]further as-
sumes the feature map lie in a low-rank subspace and decom-
pose the convolution Ô¨Ålter into kkfollowed by 11Ô¨Ålters
via SVD. [Guo et al. , 2018 ]exploits the low-rank assump-
tion of convolution Ô¨Ålters and decompose a regular convolu-
tion into several depth-wise and point-wise convolution struc-
tures. Although these works achieved notable performance innetwork compression, all of them are based on the low-rank
assumption. When such assumption is not completely satis-
Ô¨Åed, large prediction error may occur.
Alternatively, some other works [Wen et al. , 2017; Alvarez
and Salzmann, 2017 ]implicitly utilize sparsity regulariza-
tion to direct the neural network training process to learn a
low-rank representation. Our work is similar to this low-rank
regularization method. However, in addition to appending
an implicit regularization during training, we impose an ex-
plicit sparsity constraint in our training process and prove that
our approach can push the weight distribution into a low-rank
form quite effectively.
3 Methodology
3.1 Preliminaries
Formally, the convolution Ô¨Ålters in a layer can be denoted by
a tensorW2Rnckwkh, wherenandcare the number of
Ô¨Ålters and input channels, khandkware the height and width
of the Ô¨Ålters. An input of the convolution layer Fi2Rcxy
generates an output as Fo=WFi. Channel-wise correla-
tion[Zhang et al. , 2016 ]and spatial-wise correlation [Jader-
berg et al. , 2014 ]are explored to approximate convolution
Ô¨Ålters in a low-rank space. In this paper, we focus on these
two decomposition schemes. However, unlike the previous
works, we propose a new training scheme TRP to obtain a
low-rank network without re-training after decomposition.
3.2 Trained Rank Pruning
Trained Rank Pruning (TRP) is motivated by the strategies of
training quantized nets. One of the gradient update schemes
to train quantized networks from scratch [Liet al. , 2017 ]is
wt+1=Q(wt Of(wt)) (1)
whereQ()is the quantization function, wtdenote the pa-
rameter in the tthiteration. Parameters are quantized by Q()
before updating the gradients.
In contrast, we propose a simple yet effective training
scheme called Trained Rank Pruning (TRP) in a periodic
fashion:
Wt+1=
Wt Of(Wt)t%m6= 0
Tz Of(Tz)t%m= 0
Tz=D(Wt); z =t=m(2)
whereD()is a low-rank tensor approximation operator, is
the learning rate, tindexes the iteration and zis the iteration
of the operatorD, withmbeing the period for the low-rank
approximation.
At Ô¨Årst glance, this TRP looks very simple. An immediate
concern arises: can the iterations guarantee the rank of the pa-
rameters converge, and more importantly would not increase
when they are updated in this way? A positive answer (see
Theorem 2) given in our theoretical analysis will certify the
legitimacy of this algorithm.
For the network quantization, if the gradients are smaller
than the quantization, the gradient information would be to-
tally lost and become zero. However, it will not happen in

--- PAGE 3 ---
ùëòùë§ùëò‚Ñéùê∂ùëòùë§ùëò‚Ñéùê∂
ùëòùë§ùëò‚Ñéùê∂Feature flow
Gradient flow
Low-rank approximation
Substitution
‚Ä¶‚Ä¶
‚Ä¶ùëäùë°
ùëáùëß
ùëäùë°
ÔºàaÔºâ ÔºàbÔºâFigure 1: The training of TRP consists of two parts as illustrated in (a) and (b). (a) one normal iteration with forward-backward broadcast
and weight update. (b) one training iteration inserted by TRP, where the low-rank approximation is Ô¨Årst applied on Ô¨Ålters before convolution.
During backward propagation, the gradients are directly added on low-rank Ô¨Ålters and the original weights are substituted by updated low-rank
Ô¨Ålters. (b) is applied once every miterations ( i.e.when gradient update iteration t=zm; z = 0;1;2;  ), otherwise (a) is applied.
TRP because the low-rank operator is applied on the weight
tensor. Furthermore, we apply low-rank approximation ev-
erymSGD iterations. This saves training time to a large
extend. As illustrated in Fig. 1, for every miterations, we
perform low-rank approximation on the original Ô¨Ålters, while
gradients are updated on the resultant low-rank form. Other-
wise, the network is updated via the normal SGD. Our train-
ing scheme could be combined with any low-rank operators.
In the proposed work, we choose the low-rank techniques
proposed in [Jaderberg et al. , 2014 ]and[Zhang et al. , 2016 ],
both of which transform the 4-dimensional Ô¨Ålters into 2D ma-
trix and then apply the truncated singular value decomposi-
tion (TSVD). The SVD of matrix Wtcan be written as:
Wt=rank (Wt)X
i=1iUi(Vi)T(3)
whereiis the singular value of Wtwith12
rank (Wt), andUiandViare the singular vectors. The pa-
rameterized TSVD( Wt;e) is to Ô¨Ånd the smallest integer k
such that
rank (Wt)X
j=k+1(j)2erank (Wt)X
i=1(i)2(4)
whereeis a pre-deÔ¨Åned hyper-parameter of the energy-
pruning ratio, e2(0;1).
After truncating the last n ksingular values, we trans-
form the low-rank 2D matrix back to 4D tensor. Compared
with directly training low-rank structures from scratch, the
proposed TRP has following advantages.
(1) Unlike updating the decomposed Ô¨Ålters independently
of the network training in literature [Zhang et al. , 2016;
Jaderberg et al. , 2014 ], we update the network directly on the
original 4D shape of the decomposed parameters, which en-
able jointly network decomposition and training by preserv-
ing its discriminative capacity as much as possible.
(2) Since the gradient update is performed based on the
original network structure, there will be no exploding and
vanishing gradients problems caused by additional layers.(3) The rank of each layer is automatically selected during
the training. We will prove a theorem certifying the rank of
network weights convergence and would not increase in sec-
tion 3.4.
3.3 Nuclear Norm Regularization
Nuclear norm is widely used in matrix completion problems.
Recently, it is introduced to constrain the network into low-
rank space during the training process [Alvarez and Salz-
mann, 2017 ].
min(
f(x;w) +LX
l=1jjWljj)
(5)
wheref()is the objective loss function, nuclear norm jjWljj
is deÔ¨Åned asjjWljj=Prank (Wl)
i=1i
l, withi
lthe singular
values ofWl.is a hyper-parameter setting the inÔ¨Çuence
of the nuclear norm. In [Alvarez and Salzmann, 2017 ]the
proximity operator is applied in each layer independently to
solve Eq. (5). However, the proximity operator is split from
the training process and doesn‚Äôt consider the inÔ¨Çuence within
layers.
In this paper, we utilize stochastic sub-gradient descent
[Avron et al. , 2012 ]to optimize nuclear norm regularization
in the training process. Let W=UVTbe the SVD of
Wand letUtru;VtrubeU;V truncated to the Ô¨Årst rank (W)
columns or rows, then UtruVT
truis the sub-gradient of jjWjj
[Watson, 1992 ]. Thus, the sub-gradient of Eq. (5) in a layer
is
Of+UtruVT
tru (6)
The nuclear norm and loss function are optimized simul-
taneously during the training of the networks and can further
be combined with the proposed TRP.
3.4 Theoretic Analysis
In this section, we analyze the rank convergence of TRP from
the perspective of matrix perturbation theory [Stewart, 1990 ].
We prove that rank in TRP is monotonously decreasing, i.e.,
the model gradually converges to a more sparse model.

--- PAGE 4 ---
Let A be an mnmatrix, without loss of generality, m
n. =diag(1;;n)and12n.
is the diagonal matrix composed by all singular values of A.
LeteA=A+Ebe a perturbation of A, andEis the noise
matrix.e =diag(e1;;en)ande1e2en.ei
is the singular values of eA. The basic perturbation bounds for
the singular values of a matrix are given by
Theorem 1. Mirsky‚Äôs theorem [Mirsky, 1960 ]:sX
ijei ij2jjEjjF (7)
wherejjjj Fis the Frobenius norm. Then the following
corollary can be inferred from Theorem 1,
Corollary 1. LetBbe anymnmatrix of rank not greater
thank, i.e. the singular values of B can be denoted by '1
'k0and'k+1=='n= 0. Then
jjB AjjFvuutnX
i=1j'i ij2vuutnX
j=k+12
j (8)
Below, we will analyze the training procedure of the pro-
posed TRP. Note that Wbelow are all transformed into 2D
matrix. In terms of Eq. (2), the training process between two
successive TSVD operations can be rewritten as Eq. (9)
Wt=Tz=TSVD (Wt;e)
Wt+m=Tz m 1
i=0Of(Wt+i)
Tz+1=TSVD (Wt+m;e)(9)
whereWtis the weight matrix in the t-th iteration. Tzis the
weight matrix after applying TSVD over Wt.Of(Wt+i)is
the gradient back-propagated during the (t+i)-th iteration.
e2(0;1)is the predeÔ¨Åned energy threshold. Then we have
following theorem.
Theorem 2. Assume thatjjOfjjFhas an upper bound G, if
G<pe
mjjWt+mjjF, thenrank (Tz)rank (Tz+1).
Proof. We denotet
jandt+m
j as the singular values of Wt
andWt+mrespectively. Then at the t-th iteration, given the
energy ratio threshold e, the TSVD operation tries to Ô¨Ånd the
singular value index k2[0;n 1]such that :
nX
j=k+1 
t
j2<ejjWtjj2
F
nX
j=k 
t
j2ejjWtjj2
F(10)
In terms of Eq. (10), Tzis akrank matrix, i.e, the last n k
singular values of Tzare equal to 0. According to Corollary
1, we can derive that:
jjWt+m TzjjF=jjm 1X
i=0Oft+ijjF
vuutnX
j=k+1 
t+m
j2(11)Given the assumption G<pe
mjjWt+mjjF, we can get:
qPn
j=k+1 
t+m
j2
jjWt+mjjFjjPm 1
i=0Oft+ijjF
jjWt+mjjF
Pm 1
i=0jjOft+ijjF
jjWt+mjjF
mG
jjWt+mjjF<pe(12)
Eq. (12) indicates that since the perturbations of singular
values are bounded by the parameter gradients, if we prop-
erly select the TSVD energy ratio threshold e, we could guar-
antee that if n ksingular values are pruned by previous
TSVD iteration, then before the next TSVD, the energy for
the lastn ksingular values is still less than the pre-deÔ¨Åned
energy threshold e. Thus TSVD should keep the number of
pruned singular values or drop more to achieve the criterion
in Eq. (10), consequently a weight matrix with lower or same
rank is obtained, i.e.Rank (Tz)Rank (Tz+1). We further
conÔ¨Årm our analysis about the variation of rank distribution
in Section 4.
Model Top 1 ( %)Speed up
R-20 (baseline) 91.74 1.00
R-20 (TRP1) 90.12 1.97
R-20 (TRP1+Nu) 90.50 2.17
R-20 ( [Zhang et al. , 2016 ]) 88.13 1.41
R-20 (TRP2) 90.13 2.66
R-20 (TRP2+Nu) 90.62 2.84
R-20 ( [Jaderberg et al. , 2014 ]) 89.49 1.66
R-56 (baseline) 93.14 1.00
R-56 (TRP1) 92.77 2.31
R-56 (TRP1+Nu) 91.85 4.48
R-56 ( [Zhang et al. , 2016 ]) 91.56 2.10
R-56 (TRP2) 92.63 2.43
R-56 (TRP2+Nu) 91.62 4.51
R-56 ( [Jaderberg et al. , 2014 ]) 91.59 2.10
R-56 [Heet al. , 2017 ] 91.80 2.00
R-56 [Liet al. , 2016 ] 91.60 2.00
Table 1: Experiment results on CIFAR-10. ‚ÄùR-‚Äú indicates ResNet-.
4 Experiments
4.1 Datasets and Baseline
We evaluate the performance of TRP scheme on two common
datasets, CIFAR-10 [Krizhevsky and Hinton, 2009 ]and Ima-
geNet [Deng et al. , 2009 ]. The CIFAR-10 dataset consists of
colored natural images with 3232resolution and has totally
10 classes. The ImageNet dataset consists of 1000 classes
of images for recognition task. For both of the datasets, we
adopt ResNet [Heet al. , 2016 ]as our baseline model since
it is widely used in different vision tasks.We use ResNet-20,
ResNet-56 for CIFAR-10 and ResNet-18, ResNet-50 for Im-
ageNet. For evaluation metric, we adopt top-1 accuracy on

--- PAGE 5 ---
CIFAR-10 and top-1, top-5 accuracy on ImageNet. To mea-
sure the acceleration performance, we compute the FLOPs ra-
tio between baseline and decomposed models to obtain the Ô¨Å-
nal speedup rate. Wall-clock CPU and GPU time is also com-
pared. Apart from the basic decomposition methods, we com-
pare the performance with other state-of-the-art acceleration
algorithms [Heet al. , 2017; Li et al. , 2016; Luo et al. , 2017;
Zhou et al. , 2019 ].
Method Top1( %)Top5( %)Speed up
Baseline 69.10 88.94 1.00
TRP1 65.46 86.48 1.81
TRP1+Nu 65.39 86.37 2.23
[Zhang et al. , 2016 ]1- 83.69 1.39
[Zhang et al. , 2016 ] 63.10 84.44 1.41
TRP2 65.51 86.74 2:60
TRP2+Nu 65.34 86.61 3.18
[Jaderberg et al. , 2014 ] 62.80 83.72 2.00
Table 2: Results of ResNet-18 on ImageNet.
Method Top1( %)Top5( %)Speed up
Baseline 75.90 92.70 1.00
TRP1+Nu 72.69 91.41 2.30
TRP1+Nu 74.06 92.07 1.80
[Zhang et al. , 2016 ] 71.80 90.2 1.50
[Heet al. , 2017 ] - 90.80 2.00
[Luoet al. , 2017 ] 72.04 90.67 1.58
[Luoet al. , 2018 ] 72.03 90.99 2.26
[Zhou et al. , 2019 ] 71.50 90.20 2.30
Table 3: Results of ResNet-50 on ImageNet.
4.2 Implementation Details
We implement our TRP scheme with NVIDIA 1080 Ti GPUs.
For training on CIFAR-10, we start with base learning rate
of0:1to train 164 epochs and degrade the value by a fac-
tor of 10at the 82-th and 122-th epoch. For ImageNet, we
directly Ô¨Ånetune the model with TRP scheme from the pre-
trained baseline with learning rate 0:0001 for 10 epochs. We
adopt SGD solver to update weight and set the weight decay
value as 10 4and momentum value as 0:9. The accuracy
improvement enabled by data dependent decomposition van-
ishes after Ô¨Åne-tuning. So we simply adopt the retrained data
independent decomposition as our basic methods.
4.3 Results on CIFAR-10
Settings. Experiments on channel-wise decomposition
(TRP1) and spatial-wise decomposition (TRP2) are both con-
sidered. The TSVD energy threshold in TRP and TRP+Nu is
0:02and the nuclear norm weight is set as 0:0003 . We de-
compose both the 11and33layers in ResNet-56.
Results. As shown in Table 1, for both spatial-wise and
channel-wise decomposition, the proposed TRP outperforms
basic methods [Zhang et al. , 2016; Jaderberg et al. , 2014 ]on
1the implementation of [Guo et al. , 2018 ]
510152025303540
TSVD Iteration (z)102030405060Singular Value Index
0.010.020.030.040.050.060.070.08Figure 2: Visualization of rank selection, taken from the res3-1-2
convolution layer in ResNet-20 trained on CIFAR-10.
ResNet-20 and ResNet-56. Results become even better when
nuclear regularization is used. For example, in the channel-
wise decomposition (TRP2) of ResNet-56, results of TRP
combined with nuclear regularization can even achieve 2
speed up rate than [Zhang et al. , 2016 ]with same accuracy
drop. TRP also outperforms Ô¨Ålter pruning [Liet al. , 2016 ]
and channel pruning [Heet al. , 2017 ]. The channel decom-
posed TRP trained ResNet-56 can achieve 92:77% accuracy
with 2:31acceleration, while [Heet al. , 2017 ]is91:80%
and[Liet al. , 2016 ]is91:60%. With nuclear regularization,
our methods can approximately double the acceleration rate
of[Heet al. , 2017 ]and[Liet al. , 2016 ]with higher accuracy.
4.4 Results on ImageNet
Settings. We choose ResNet-18 and ResNet-50 as our base-
line models. The TSVD energy threshold eis set as 0.005.
of nuclear norm regularization is 0.0003 for both ResNet-
18 and ResNet-50. We decompose both the 33and11
Convolution layers in ResNet-50. TRP1 is the channel-wise
decomposition and TRP2 is the spatial-wise decomposition.
Results. The results on ImageNet are shown in Table 2
and Table 3. For ResNet-18, our method outperforms the
basic methods [Zhang et al. , 2016; Jaderberg et al. , 2014 ].
For example, in the channel-wise decomposition, TRP ob-
tains 1.81speed up rate with 86.48% Top5 accuracy on Im-
ageNet which outperforms both the data-driven [Zhang et al. ,
2016 ]1and data independent [Zhang et al. , 2016 ]methods by
a large margin. Nuclear regularization can increase the speed
up rates with the same accuracy.
For ResNet-50, to better validate the effectiveness of our
method, we also compare TRP with pruning based methods.
With 1:80speed up, our decomposed ResNet-50 can ob-
tain74:06% Top1 and 92:07% Top5 accuracy which is much
higher than [Luoet al. , 2017 ]. The TRP achieves 2:23ac-
celeration which is higher than [Heet al. , 2017 ]with the same
1:4%Top5 degrade. Besides, with the same 2:30accelera-
tion rate, our performance is better than [Zhou et al. , 2019 ].
4.5 Rank Variation
To analyze the variation of rank distribution during training,
we further conduct an experiment on the CIFAR-10 dataset
with ResNet-20 and extract the weight from the res3-1-2 con-
volution layer with channel-wise decomposition as our TRP

--- PAGE 6 ---
1.5 2.0 2.5 3.0
Speed up Rate78808284868890Validation Accuracy
basic method
basic method+Nu
TRP
TRP+Nu(a) Channel-wise decomposition
1.5 2.0 2.5 3.0 3.5 4.0 4.5
Speed up Rate7075808590Validation Accuracy
basic method
basic method+Nu
TRP
TRP+Nu (b) Spatial-wise decomposition
Figure 3: Ablation study on ResNet-20. Basic methods are data-independent decomposition methods (channel or spatial) with Ô¨Ånetuning.
scheme. After each TSVD, we compute the normalized en-
ergy ratioER(i)for each singular value ias Eq. (13).
ER(i) =2
iPrank (Tz)
j=02
j(13)
we record for totally 40iterations of TSVD with period m=
20, which is equal to 800training iterations, and our energy
thesholdeis pre-deÔ¨Åned as 0:05. Then we visualize the varia-
tion ofERin Fig. 2. During our training, we observe that the
theoretic bound value max tmG
jjWtjjF0:092<pe0:223,
which indicates that our basic assumption in theorem 2 al-
ways holds for the initial training stage.
And this phenomenon is also reÔ¨Çected in Fig. 2, at the be-
ginning, the energy distribution is almost uniform w.r.t each
singular value, and the number of dropped singular values in-
creases after each TSVD iteration and the energy distribution
becomes more dense among singular values with smaller in-
dex. Finally, the rank distribution converges to a certain point
where the smallest energy ratio exactly reaches our threshold
eand TSVD will not cut more singular values.
4.6 Ablation Study
In order to show the effectiveness of different components of
our method, we compare four training schemes, basic meth-
ods[Zhang et al. , 2016; Jaderberg et al. , 2014 ], basic methods
combined with nuclear norm regularization, TRP and TRP
combined with nuclear norm regularization. The results are
shown in Fig. 3. We can have following observations:
(1)Nuclear norm regularization After combining nuclear
norm regularization, basic methods improve by a large mar-
gin. Since Nuclear norm regularization constrains the Ô¨Ålters
into low rank space, the loss caused by TSVD is smaller than
the basic methods.
(2)Trained rank pruning As depicted in Fig. 3, when the
speed up rate increases, the performance of basic methods
and basic methods combined with nuclear norm regulariza-
tion degrades sharply. However, the proposed TRP degrades
very slowly. This indicates that by reusing the capacity of the
network, TRP can learn a better low-rank feature representa-
tions than basic methods. The gain of nuclear norm regular-ization on TRP is not as big as basic methods because TRP
has already induced the parameters into low-rank space by
embedding TSVD in training process.
Model GPU time (ms) CPU time (ms)
Baseline 0.45 118.02
TRP1+Nu (channel) 0.33 64.75
TRP2+Nu (spatial) 0.31 49.88
Table 4: Actual inference time per image on ResNet-18.
4.7 Runtime Speed up of Decomposed Networks
We further evaluate the actual runtime speed up of the com-
pressed Network as shown in Table 4. Our experiment is con-
ducted on a platform with one Nvidia 1080Ti GPU and Xeon
E5-2630 CPU. The models we used are the original ResNet-
18 and decomposed models by TRP1+Nu and TRP2+Nu.
From the results, we observe that on CPU our TRP scheme
achieves more salient acceleration performance. Overall the
spatial decomposition combined with our TRP+Nu scheme
has better performance. Because cuDNN is not friendly for
13and31kernels, the actual speed up of spatial-wise
decomposition is not as obvious as the reduction of FLOPs.
5 Conclusion
In this paper, we proposed a new scheme Trained Rank Prun-
ing (TRP) for training low-rank networks. It leverages capac-
ity and structure of the original network by embedding the
low-rank approximation in the training process. Furthermore,
we propose stochastic sub-gradient descent optimized nuclear
norm regularization to boost the TRP. The proposed TRP can
be incorporated with any low-rank decomposition method.
On CIFAR-10 and ImageNet datasets, we have shown that
our methods can outperform basic methods and other prun-
ing based methods both in channel-wise decmposition and
spatial-wise decomposition.

--- PAGE 7 ---
References
[Alvarez and Salzmann, 2017 ]Jose M Alvarez and Mathieu
Salzmann. Compression-aware training of deep networks.
InNIPS , 2017.
[Avron et al. , 2012 ]Haim Avron, Satyen Kale, Shiva Prasad
Kasiviswanathan, and Vikas Sindhwani. EfÔ¨Åcient and
practical stochastic subgradient descent for nuclear norm
regularization. In ICML , 2012.
[Chen et al. , 2015 ]Wenlin Chen, James Wilson, Stephen
Tyree, Kilian Weinberger, and Yixin Chen. Compressing
neural networks with the hashing trick. In ICML , 2015.
[Courbariaux and Bengio, 2016 ]Matthieu Courbariaux and
Yoshua Bengio. Binarynet: Training deep neural networks
with weights and activations constrained to +1 or -1. arXiv
preprint arXiv:1602.02830 , 2016.
[Deng et al. , 2009 ]Jia Deng, Wei Dong, Richard Socher, Li-
Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. CVPR , 2009.
[Denton et al. , 2014 ]Emily Denton, Wojciech Zaremba,
Joan Bruna, Yann Lecun, and Rob Fergus. Exploiting
linear structure within convolutional networks for efÔ¨Åcient
evaluation. In NIPS , 2014.
[Guo et al. , 2018 ]Jianbo Guo, Yuxi Li, Weiyao Lin, Yurong
Chen, and Jianguo Li. Network decoupling: From regular
to depthwise separable convolutions. In BMVC , 2018.
[Han et al. , 2015a ]Song Han, Huizi Mao, and William J.
Dally. Deep compression: Compressing deep neural net-
work with pruning, trained quantization and huffman cod-
ing.CoRR , abs/1510.00149, 2015.
[Han et al. , 2015b ]Song Han, Jeff Pool, John Tran, and
William Dally. Learning both weights and connections for
efÔ¨Åcient neural network. In NIPS , 2015.
[Heet al. , 2016 ]Kaiming He, Xiangyu Zhang, Shaoqing
Ren, and Jian Sun. Deep residual learning for image recog-
nition. 2016.
[Heet al. , 2017 ]Yihui He, Xiangyu Zhang, and Jian Sun.
Channel pruning for accelerating very deep neural net-
works. In ICCV , 2017.
[Jaderberg et al. , 2014 ]Max Jaderberg, Andrea Vedaldi, and
Andrew Zisserman. Speeding up convolutional neu-
ral networks with low rank expansions. arXiv preprint
arXiv:1405.3866 , 2014.
[Krizhevsky and Hinton, 2009 ]Alex Krizhevsky and Geof-
frey Hinton. Learning multiple layers of features from tiny
images. Computer Science , 2009.
[Liet al. , 2016 ]Hao Li, Asim Kadav, Igor Durdanovic,
Hanan Samet, and Hans Peter Graf. Pruning Ô¨Ålters for ef-
Ô¨Åcient convnets. arXiv preprint arXiv:1608.08710 , 2016.
[Liet al. , 2017 ]Hao Li, Soham De, Zheng Xu, Christoph
Studer, Hanan Samet, and Tom Goldstein. Training quan-
tized nets: A deeper understanding. In NIPS , 2017.
[Liuet al. , 2017 ]Zhuang Liu, Jianguo Li, Zhiqiang Shen,
Gao Huang, Shoumeng Yan, and Changshui Zhang.Learning efÔ¨Åcient convolutional networks through net-
work slimming. In ICCV , 2017.
[Luoet al. , 2017 ]Jian-Hao Luo, Jianxin Wu, and Weiyao
Lin. Thinet: A Ô¨Ålter level pruning method for deep neural
network compression. ICCV , 2017.
[Luoet al. , 2018 ]Jian-Hao Luo, Hao Zhang, Hong-Yu
Zhou, Chen-Wei Xie, Jianxin Wu, and Weiyao Lin. Thinet:
pruning cnn Ô¨Ålters for a thinner net. TPAMI , 2018.
[Maet al. , 2018 ]Ningning Ma, Xiangyu Zhang, Hai-Tao
Zheng, and Jian Sun. ShufÔ¨Çenet v2: Practical guide-
lines for efÔ¨Åcient cnn architecture design. arXiv preprint
arXiv:1807.11164 , 2018.
[Mirsky, 1960 ]Leon Mirsky. Symmetric gauge functions
and unitarily invariant norms. The quarterly journal of
mathematics , 11(1):50‚Äì59, 1960.
[Rastegari et al. , 2016 ]Mohammad Rastegari, Vicente Or-
donez, Joseph Redmon, and Ali Farhadi. Xnor-net: Ima-
genet classiÔ¨Åcation using binary convolutional neural net-
works. In ECCV , 2016.
[Sandler et al. , 2018 ]Mark Sandler, Andrew Howard, Men-
glong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
Mobilenetv2: Inverted residuals and linear bottlenecks. In
CVPR , June 2018.
[Stewart, 1990 ]Gilbert W Stewart. Matrix perturbation the-
ory. 1990.
[Watson, 1992 ]G Alistair Watson. Characterization of the
subdifferential of some matrix norms. Linear algebra and
its applications , 170:33‚Äì45, 1992.
[Wen et al. , 2016 ]Wei Wen, Chunpeng Wu, Yandan Wang,
Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. In NIPS , 2016.
[Wen et al. , 2017 ]Wei Wen, Cong Xu, Chunpeng Wu, Yan-
dan Wang, Yiran Chen, and Hai Li. Coordinating Ô¨Ålters
for faster deep neural networks. In ICCV , 2017.
[Xuet al. , 2018 ]Yuhui Xu, Yongzhuang Wang, Aojun
Zhou, Weiyao Lin, and Hongkai Xiong. Deep neural net-
work compression with single and multiple level quantiza-
tion. CoRR , abs/1803.03289, 2018.
[Xuet al. , 2019 ]Yuhui Xu, Yuxi Li, Shuai Zhang, Wei Wen,
Botao Wang, Yingyong Qi, Yiran Chen, Weiyao Lin, and
Hongkai Xiong. Trained rank pruning for efÔ¨Åcient deep
neural networks. In NIPS EMC2 workshop , 2019.
[Zhang et al. , 2016 ]Xiangyu Zhang, Jianhua Zou, Kaim-
ing He, and Jian Sun. Accelerating very deep convolu-
tional networks for classiÔ¨Åcation and detection. TPAMI ,
38(10):1943‚Äì1955, 2016.
[Zhou et al. , 2017 ]Aojun Zhou, Anbang Yao, Yiwen Guo,
Lin Xu, and Yurong Chen. Incremental network quanti-
zation: Towards lossless cnns with low-precision weights.
arXiv preprint arXiv:1702.03044 , 2017.
[Zhou et al. , 2019 ]Yuefu Zhou, Ya Zhang, Yanfeng Wang,
and Qi Tian. Accelerate cnn via recursive bayesian prun-
ing. In ICCV , 2019.
