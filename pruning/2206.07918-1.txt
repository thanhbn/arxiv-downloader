# 2206.07918.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2206.07918.pdf
# File size: 4779763 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
“Understanding Robustness Lottery”: A
Geometric Visual Comparative Analysis of
Neural Network Pruning Approaches
Zhimin Li, Shusen Liu, Xin Yu, Kailkhura Bhavya, Jie Cao, James Daniel Diffenderfer,
Peer-Timo Bremer, Member, IEEE, Valerio Pascucci, Member, IEEE
Abstract —Deep learning approaches have provided state-of-the-art performance in many applications by relying on large and
overparameterized neural networks. However, such networks have been shown to be very brittle and are difficult to deploy on
resource-limited platforms. Model pruning, i.e., reducing the size of the network, is a widely adopted strategy that can lead to a more
robust and compact model. Many heuristics exist for model pruning, but our understanding of the pruning process remains limited due
to the black-box nature of a neural network model. Empirical studies show that some heuristics improve performance whereas others
can make models more brittle or have other side effects. This work aims to shed light on how different pruning methods alter the
network’s internal feature representation and the corresponding impact on model performance. To facilitate a comprehensive
comparison and characterization of the high-dimensional model feature space, we introduce a visual geometric analysis of feature
representations. We decomposed and evaluated a set of critical geometric concepts from the common adopted classification loss, and
used them to design a visualization system to compare and highlight the impact of pruning on model performance and feature
representation. The proposed tool provides an environment for in-depth comparison of pruning methods and a comprehensive
understanding of how model response to common data corruption. By leveraging the proposed visualization, machine learning
researchers can reveal the similarities between pruning methods and redundant in robustness evaluation benchmarks, obtain
geometric insights about the differences between pruned models that achieve superior robustness performance, and identify samples
that are robust or fragile to model pruning and common data corruption.
Index Terms —neural network pruning, robustness, XAI, information visualization
✦
1 I NTRODUCTION
RECENT developments in deep learning have produced
significant advances in a variety of application ar-
eas [1], [2], [3]. However, such performance is often achieved
through extremely large neural networks that consume sub-
stantial resources. Further, these models are difficult to de-
ploy and prone to overfitting, leading to poor generalization
and fragile behavior [4]. Network pruning, which removes
neurons and/or connections from a model, is a common ap-
proach to mitigate some of these challenges since compress-
ing models can reduce both their computational footprint
and their inherent redundancy [5], [6] without significant
performance loss. Although model pruning can be accuracy
as the original dense models, some recent works [7], [8] have
demonstrated that the resulting sparse models are brittle
to out-of-distribution shifts [9]. For example, common, real-
world corruptions can reduce the accuracy of such models
by up to 40% for images from ImageNet-C [10]. This degra-
dation of robustness has raised serious concerns about the
•Zhimin Li and Valerio Pascucci are with the Scientific Comput-
ing and Imaging Institute, University of Utah.E-mail: {zhimin, pas-
cucci}@sci.utah.edu
•Xin Yu and Jie Cao are with School of Computing, University of Utah.
E-mail: {xiny, jcao }@cs.utah.edu
•Shusen Liu, Kailkhura Bhavya, Diffenderfer James Daniel, and Peer-
Timo Bremer are with Lawrence Livermore National Laboratory. E-mail:
{liu42, kailkhura1,diffenderfer2, bremer5 }@llnl.govpractical viability of pruned models, especially in safety-
critical applications such as autonomous driving.
Recent results [11] have demonstrated both theoretically
and empirically that these problems are a byproduct of the
pruning methodologies rather than a fundamental limita-
tion of sparse networks. Previous work [11] has theoretically
indicated that sparse networks with accuracy and robust-
ness comparable to dense models exist. Furthermore, in
some instances, it has been empirically demonstrated that
pruning can in fact improve both the accuracy and robust-
ness of models compared to their dense baselines. This find-
ing is especially surprising as making any model, let alone
a pruned version, more robust to out-of-distribution shift
has proven difficult. Nevertheless, it remains unclear why
certain pruning techniques positively or negatively affect
robustness. Providing an in-depth understanding will not
only support a real-world deployment of such models but
also might lead to even more advanced pruning approaches.
To date, it is unclear what properties of these models can be
attributed to their improved performances, and the model
pruning community does not have comprehensive intro-
spection tools to answer these important questions. Such an
effort is hampered by the opaque nature of neural networks
and the lack of a dedicated system for model comparison
and evaluation in the context of neural network pruning.
In this paper, we aim to fill this crucial gap by introduc-
ing a visual analytical system for understanding differences
among representative pruning methods and measuring andarXiv:2206.07918v2  [cs.HC]  25 Oct 2023

--- PAGE 2 ---
2
interpreting model behavior under various pruning strate-
gies. As a general goal, we hope to understand the effect
of model pruning on multiple levels, e.g., why some sam-
ples are more affected by pruning [7], why certain pruned
models [12] can have better generalization performance
than a state-of-the-art dense-weight trained model, and
how the performance of pruned models differs for unseen
or corrupted data, etc. To achieve the goal of building a
comprehensive introspection system for analyzing model
pruning, our tool focuses on both the computation and
visualization fronts.
On the computation side, one essential challenge arises
from the need to compare the latent representations of
models to understand how making changes to them affects
the final prediction. However, a neural network’s feature
representation usually lies in a high-dimensional space that
contains hundreds if not thousands of dimensions without
explicit semantics or labels. Comparing such spaces is a
nontrivial task, especially considering the behavior of a
classifier can be sensitive to small changes (e.g., adversarial
example) in the feature representation. Traditional dimen-
sionality reduction methods [13], [14] are not suitable solu-
tions because, for complex latent spaces, they will invariably
induce information loss that could significantly impact the
trustworthiness of the downstream analysis. A potential
solution to this challenge is to preserve high-dimensional
relationships in the data for our comparison task. Since our
goal is to understand how latent space changes affect the
final prediction, e.g., image classification result, what as-
pects of the feature representation directly contribute to the
prediction is critical. As long as this information is encoded
faithfully, then the comparison of the high-dimensional fea-
ture representations can be more meaningful.
We propose a set of geometrically inspired features
(namely Angle ,L2 Norm ,Margin ) derived from a direct
decomposition of the classification loss function (i.e., cross-
entropy) to evaluate pruning and data corruption. These
geometric features capture aspects of the feature represen-
tation that are directly linked to the model’s prediction,
which allows us to achieve a comparison of network rep-
resentations by isolating the most crucial information while
removing other variations and noises. Moreover, since many
crucial insights can be obtained only through a comparison
between different methods, the overall design of the linked
interfaces is centered around the ability to provide a con-
trastive visualization.
Based on the analytical framework, we design a novel
visualization system which supports comparison with three
level of details: pruning methods and evaluation benchmark
comparison; feature representations’ geometric comparison;
detail sample’s input feature comparison. By utilizing the
proposed visual analytic system, researchers can compare
pruning methods, reveal the similarities of robustness eval-
uations benchmark, understand where and how pruning
methods differ, identify if a subset of samples is vulnera-
ble to model pruning and data perturbation, and provide
insights into why one model is more robust than another.
These observations can provide feedback to streamline our
analysis, improve our understanding of neural network
pruning, and motivate useful hypotheses for domain ex-
perts to further improve a model’s performance.Our key contributions are summarized as follows:
•A geometric description of the structure of a neural
network model’s feature representation with three
geometric metrics to evaluate and compare model
pruning and data corruption techniques (section 5).
•A new dedicated introspective visualization system
with a three-level hierarchical comparison based on
geometric features for analyzing and comparing ma-
jor pruning methods over different architectures, cor-
ruption datasets, and samples (sections 4 and 6).
•Extensive use case that involve state-of-the-art mod-
els to demonstrate the usability of the proposed visu-
alization system for pruning and robustness analysis
(section 7).
2 R ELATED WORK
In this section, we discuss various directions that are related
to neural network pruning and visualization approaches
and discuss their relationship with respect to the proposed
approach.
2.1 Network Pruning
In this work, we focus on evaluating and comparing neural
network pruning approaches [7], [12], [15], [16], [17], most
of which originate in the ML community. LeCun et al. [15]
proposed a pruning method based on the assumption that
an optimized neural network model can reach a function’s
minimum, and its second derivative can indicate the im-
portance of weights. Frankle and Carbin [16] proposed a
lottery ticket hypothesis that a sparse subnetwork with
the same initialization can be as accurate as a dense net-
work after training. Ramanujan et al. [17] and Diffenderfer
and Kailkhura [12] showed that an untrained subnetwork
that has the same performance as a weight-trained model.
Hooker et al. [7] introduced pruning-identified exemplars
(PIE), which highlight a subset of samples that are more
vulnerable to pruning than the other samples. To provide
a more comprehensive understanding of these techniques,
we discuss the pruning problem in depth and explain the
differences among popular pruning methods in Section 3.1.
A notable visualization work in this context is CN-
NPruner [18]. Li et al. designed a visual analytic system that
enables users to interactively perform pruning and explore
the trade-off between the model accuracy and pruning ratio.
However, their motivation and goal arise from the question
of how to design a human-in-the-loop interactive pruning
system. Instead, we aim to evaluate and understand dif-
ferent pruning methods and their robustness in relation to
a model’s internal representation. Particularly, our frame-
work provides a geometric similarity comparison between
pruning methods and geometric insights into samples that
are more vulnerable/robust to the network pruning. Our
system finds that the random untrained subnetworks that
are surprisingly robust to common corruptions have a sig-
nificant geometry shape compared with regular well-trained
models.

--- PAGE 3 ---
3
2.2 Compression/Pruning for Model Explanation
In the visualization community, apart from the aforemen-
tioned interactive network pruning work [18], the majority
of related works on network pruning have focused on the
model interpretation problem. Wang et al. [19] used model
distillation techniques to compress the size of the model and
combined it with a deep generative model to understand the
model’s reaction to a sample’s neighbor. Summit [20] pro-
posed two aggregation techniques (activation and neuron-
influence aggregation) to select critical neurons (e.g., 7.5%
weights) to build the attribution graph. With its feature
visualization, the system tries to show an overview of the
network model. However, the selected attribute graph is a
subnetwork, and the accuracy of the model is not verified.
Liu et al. [21] designed a visualization system to understand
how adversarial examples affect a model’s prediction by
visualizing the critical subnetwork that preserves the same
prediction accuracy of the complete network with these
selected samples. The critical subnetwork is auto-selected
by an optimization process. Kahng et al. [22] designed a
visualization system, ACTIVIS that is used for a model’s
hidden layer activation pattern exploration. The system de-
signs an activation matrix that shows the top n-th activation
neurons one at a time to compare the activation of different
samples or the input of different categories. The authors
used these most active neurons to select the subnetwork.
Rather than explaining the prediction or focus rationale
behind individual prediction, our study focuses on a global
comparison of state-of-the-art model pruning methods in
the context of model robustness under common corruptions.
2.3 Model Evaluation and Model Robustness
Neural networks have shown superhuman performance
on clean test datasets but they fall short on robustness
by performing poorer on out-of-distribution data [9]. This
brittleness issue is more prominent for pruned models [7],
which makes it crucial to evaluate them on common cor-
ruptions arising in real-world applications. To evaluate the
performance on neural networks in the real world, sev-
eral corruption benchmark datasets have been proposed.
Hendrycks and Dietterich [10] developed corruption robust-
ness benchmarking datasets CIFAR-10/100-C, ImageNet-
C, and ImageNet-R to facilitate robustness evaluations of
CIFAR and ImageNet classification models. Sun et al. [23]
and Mintun et al. [24] further designed new corruption
types to complement [10]. In addition to image classifica-
tion, benchmarking datasets for object detection and point
cloud classification were developed in [25] and [26], re-
spectively. Motivated by the work on corruption robustness
benchmarking, in this work we evaluate a range of pruned
classifiers not only on clean test data but also on corrupted
datasets.
2.4 Model Comparison
We perform a short literature review of different visual
model comparison approaches. The common approach com-
pares the input and output of a model to infer its property.
Square [27] designs a visualization of a model’s output
to compare models’ multilabel prediction behaviors. Man-
ifold [28] uses input and output to compare multiple modelbehaviors, and the comparison may not be constrained
by different model architectures or algorithms. Confusion-
Flow [29] deploys a confusion matrix with a temporal visual
encoding that enables users to track class-level temporal
information during model training and comparison. Stack-
GenVis [30] utilizes multiple output metrics to compare
models’ performance and available information to assemble
more powerful models. Many techniques [31], [32] use input
and output analysis to perform the model comparison.
Information extraction from the output of a model is
valuable. It provides a confident score of the model’s deci-
sions and reveals the ambiguity of samples among multiple
categories. However, this information is limited concerning
the stability of the prediction (e.g., adversarial example).
Also, recent research [33] has found that output probability
can be problematic, and a model may be uncalibrated.
Instead of comparing model output, we study the feature
representation extracted by a CNN model before classifi-
cation. By studying the feature representation of a neural
network directly, we are able to have a global intuition
about the prediction behavior of a model and gain more
information about a model’s behavior such as the robustness
of a prediction. Previous studies have often used dimension
reduction that projects high-dimensional data into 2D space
to study the data cluster [34] and sample density or outliers.
However, knowledge learned from the 2D projected space
contains uncertainty [35] because the projection process may
lose a significant amount of information in the original high-
dimensional space. How to properly understand and use
the projected 2D space from high-dimensional space is still
ongoing research.
In convolution neural networks, the last classification
layer is a linear layer, and some of these models are attached
to a softmax operation. Therefore, the last layer’s feature
representation is more accessible and interpretable than the
previous latent space. Limited work has been designed to
understand the latent feature space because of the unknown
mysterious structure of the feature latent representation. In
this work, we leverage the loss function to construct global
geometric features to understand and compare the behavior
of multiple pruning methods.
3 D OMAIN BACKGROUND
In this section, we discuss the basic terminologies, pruning
techniques, and evaluation methods used in this study.
3.1 Network Pruning
Here, we introduce the pruning approaches used in this
study for evaluation and analysis. In this paper, we will
focus on unstructured pruning, which removes redundant
network weights. Nevertheless, the analysis pipeline also
works for structured pruning, which prunes entire neurons
or filters. As summarized in [36], most works in network
pruning start with scoring the model parameters based on
their potential impact on the network performance, selecting
weights of least importance to remove from the network,
and optionally performing retraining to gain back perfor-
mance degradation due to pruning. We will explore the
following pruning methods:

--- PAGE 4 ---
4
Random Pruning : Randomly select a set of weights and
remove them from a neural network model.
Magnitude Pruning : Score the weights with their abso-
lute values and prune the ones with the smallest scores [5].
Gradient-Based Pruning : Prune the weights that would
have the least impact on the loss function of the model if
removed. Given the training data Dand the neural network
with its weights ϕ, the impact on the loss function Lafter
removing a single weight ϕmcan be estimated as Imby the
functional Taylor expansion as
Im=|∇L(D,ϕ)(ˆϕ−ϕ)
+1
2(ˆϕ−ϕ)T∇2L(D,ϕ)(ˆϕ−ϕ)
+O(∥ˆϕ−ϕ∥3)|
where ∇L and∇2Lindicate the first-order gradient of
the weights and second-order gradient (Hessian matrix)
respectively [37], [38], [39].
Multi-prize lottery tickets (MPTs) : This strategy
searches for a performant sparse subnetwork within a ran-
domly initialized network and can further compress the net-
work by applying weight binarization. Counter to the tradi-
tional training paradigm of learning the network weights,
this approach learns which randomly initialized weights
should be retained to improve performance by optimizing
over surrogate scores that indicate the importance of each
weight to network performance. In our experiments, we
make use of biprop (Algorithm 1 in [12]). This methodology
is built on the multi-prize lottery ticket hypothesis [12], which
proposes that sufficiently overparameterized randomly ini-
tialized networks contain sparse subnetworks that, without
any training, can perform comparably to dense networks
and are amenable to weight binarization. Theoretical proofs
supporting this hypothesis have been established [12], [40].
Further experimental efforts have established that these
MPTs learned using biprop are comparably or more ro-
bust than dense networks learned using traditional weight
optimization in work demonstrating that certain model
compression algorithms are capable of producing compact,
accurate, and robust deep neural networks, or CARDs [11].
3.2 Data Corruption Evaluation
Besides accuracy on clean test data, we use the cifar10-C
dataset [10], which is the cifar10 test dataset corrupted with
19 common corruption algorithms from four categories:
noise, blur, weather, and digital corruptions. For an image
dataset, similar corruption operations can be performed to
generate the same corrupted image datasets. For example,
the same corruptions have been performed to generate
MNIST-C, Cifar100-C, and Imagenet-C datasets. We also
perform a quantitative evaluation of these datasets based
on exploration observation. These corruptions preserve the
semantic content of images, and humans can easily recog-
nize these images. Each corruption technique has a severity
level from one to five where a larger number denotes a more
severe corruption.
4 T ASKS ANALYSIS
As previously mentioned, existing model pruning schemes
exhibit diverse behaviors. Specifically, some pruned modelsare more robust to different data corruption than others.
Unfortunately, it is not clear why certain pruning tech-
niques affect a model in one way and not another. In this
work, we aim to provide an in-depth understanding of
this phenomenon that will not only support a real-world
deployment of such models but could also lead to more
advanced pruning approaches. Here, we are specifically
interested in answering the following questions: For given
two models pruned with different pruning techniques, what
properties of the models make their accuracy and robustness
differ from each other? What properties can make certain
samples more vulnerable to the model pruning? Are current
evaluation benchmarks sufficient for the network pruning
methods?
These questions, which are critical for comparing and de-
signing better pruning strategies, are articulated by domain
experts. To help domain experts improve their understand-
ing of model behavior and answer the above questions,
we distill these questions into the following requirements
to drive the design of the visualization system. The re-
quirements of this work are based on a long-term regular
interview (twice a month over 7 months) with four domain
experts who are machine learning researchers and also the
coauthors of this work . Currently, our visualization system
is designed for developers who have a machine learning
background.
R1 - Model Evaluation Overview : Model pruning ex-
perts often perform experiments on multiple architectures,
different pruning methods, and various data corruption
evaluation benchmarks. A succinct presentation of these
results can guide domain experts to figure out the pros and
cons of different pruning solutions and architectures and
narrow down their analysis to the most interesting subset
for a detailed examination.
R2 - Evaluation Similarity Between Different Pruning
and Corruption Operation : Understanding the similarity
between pruning and corruption operations will help do-
main experts quickly figure out whether one approach is
different from another. The difference will lead to further
examination to understand what makes the two approaches
different from each other.
R3 - Model Behavior Over Samples with Different
Pruning and Corruption Configurations : A summary of
a model’s behavior over different subsets of samples is
useful for understanding and diagnosing a model’s behav-
iors under different pruning configurations. Meanwhile, a
summary of certain representative model properties over
a large number of samples is useful for domain experts
to diagnose a model’s behavior. It helps domain experts
understand what mistakes the model will make and what
decisions the model will be confident about.
R4 - Latent Feature Representation Variation : Compar-
ing different models’ latent space provides domain experts
with a visual understanding of how models’ feature repre-
sentation changes after pruning. Latent feature comparisons
are more sensitive than input and output comparisons,
which can capture minute changes. Such a comparison can
provide valuable insights to understand why a model may
be more resilient to input perturbation than another and
why certain samples are vulnerable to pruning or corrup-
tion.

--- PAGE 5 ---
5
Class Direction 
angle 
length Margin Class 1 
probability 
Class n W1
Last Feature 
Encoding Layer …
...
Softmax/Classification W1 is the Class 1 direction 
Fig. 1. This study monitors the last feature encoding a layer’s latent
feature representation to understand the impact of network pruning. The
feature representation of data samples in the last feature encoding layer
with 2 neurons can be visualized directly. The star shape is the sample’s
latent space representation of a model trained with the cross entropy
loss. The decision boundary in the layer can also be approximated and
visualized by sampling the 2D space. Four geometric features that are
connected with the loss function can be identified in the visualization.
R5 - Input Feature’s Sensitivity With Respect to Model
Pruning and Data Corruption : Model pruning leads to
varying impacts on a model’s decisions on different samples
or labels. Certain samples are robust to pruning but others
are fragile. Understanding what input features make a sam-
ple vulnerable or robust to network pruning is a critical task
for domain experts.
5 G EOMETRIC VIEW OF LATENT SPACE
Fulfilling the above-mentioned requirements is a nontriv-
ial task. In particular, how to display a model’s behavior
summary over a large amount of data besides prediction
accuracy and how to perform latent space comparisons are
difficult questions to answer. To address these challenges, in
this section, we define the class direction and corresponding
three geometric metrics ( angle ,length , and margin ) on a
neural network’s last (a network’s layer before classification)
layer’s feature encoding, which is the accumulated result
of previous layers’ transformations and directly used for a
model’s final prediction.
Before getting into a detailed description of the geomet-
ric features, we present an example to explain the intuition
of what the geometric features look like in the last feature
encoding layer. Fig. 1 highlights the critical concepts in the
following discussion. The figure visualizes the feature rep-
resentation of samples in a trained neural network, where
the last feature encoding layer is set with only 2 neurons.
These 2D latent feature vectors display a star shape, in
which samples that are far away from the origin have more
distinguishable features, and samples that are close to the
origin are ambiguous (refer to support material for more
discussion about the geometric shape and loss function).
Class direction is the direction across the middle of samples
that belong to a certain category. Angle and length metrics
are marked in the plot, which is the angle with the classdirection and a sample’s distance to the origin, respectively.
The plot on the right shows the same feature vectors but
with the decision boundaries of the classification. These ge-
ometric properties not only exist in 2D space but also can be
generalized to high-dimensional space to help summarize
crucial aspects of the latent space structure and form the
basis for cross-model comparison.
5.1 Class Direction
The loss function used for training the neural network is
critical for shaping the geometry of the latent feature em-
bedding. Cross-entropy loss and negative log-likelihood loss
are default loss functions used for classification tasks. For a
given example xwith a ground truth label y=i, the loss
function can be formulated as Lloss=−log(P(y=i|x))
where P(y=i|x)is the predicted probability for a model
for the label y=iwith a value range in [0,1]. A large
P(y=i|x)will achieve a small loss Lloss.
Without loss of generality, we assume that the neural net-
work is composed of two parts: an encoder hthat transforms
input xinto a feature representation vector ⃗X=h(x)and a
classifier cthat is used for producing the predicted probabil-
ityP(y=i|x) =c(⃗X). We consider that the classifier part c
consists of a fully connected layer, and a soft-max activation
function, which is a general configuration in convolutional
neural network models. Cis the number of classes for
a classification task and ⃗Xis the m-dimensional feature
embedding of ⃗X∈Rm. The last fully connected layer in
classifier part cparameterized with weight W∈Rm×Cwill
take this m-dimensional feature vector as input and project
it into Cscores, and then output the predicted probability
via the soft-max activation function. These operations can
be summarized as equation (1).
In equation (1), we can rewrite the weight matrix
W={⃗Wi|0< i≤ C,⃗Wi∈Rm}as the set of weights
corresponding to each of the Cclasses. Hence, the predicted
probability P(y=i|x)is determined by the dot products
of feature representation ⃗Xand each target label j’s neuron
weight ⃗Wj.
P(y=i|x) =e⃗Wi·⃗X
PC
j=0e⃗Wj·⃗X=e∥⃗Wi∥∥⃗X∥cosθi
PC
j=0e∥⃗Wj∥∥⃗X∥cosθj(1)
=1
PC
j̸=ie∥⃗Wj∥∥⃗X∥cosθj
e∥⃗Wi∥∥⃗X∥cosθi+ 1(2)
=1
PC
j̸=ie∥⃗X∥(∥⃗Wj∥cosθj−∥⃗Wi∥cosθi)+ 1(3)
=1
PC
j̸=ie∥⃗X∥(Cjcosθj−Cicosθi)+ 1(4)
The dot product operation (denoted as ·) can be inter-
preted as the feature embedding ⃗Xof every input exam-
ple being projected onto each label j’s⃗Wjdirection and
multiplied with ∥⃗Wj∥. Here, ∥⃗Wj∥is a constant, and the
predicted probability is determined by the L2 norm ∥⃗X∥
and the angles θjbetween the directions of ⃗Xand each ⃗Wj.
Based on the above intuition, we define the fixed weight
vector ⃗Wjparameterizing the classifier part cas the class

--- PAGE 6 ---
6
direction of category jin the network’s last layer latent
space.
5.2 Angle, Length, and Margin
Previous discussion has defined the class direction that can
be extracted from the built weights in the neural network
model. Based on the above definition, we introduce three
geometric features of the last layer latent space - angle,
length, and margin.
Length metric is the L2 norm of feature vectors ⃗Xthat
mainly affects the confidence or output probability of a
prediction.
Angle metric is the geometric angle θbetween class
directions and feature vectors. A small angle between the
class direction ⃗Wimeans the sample will be predicted as
label iwith high probability. If a feature vector has similar
angles with respect to two class directions, then the model
will give similar probability to both categories. If the model
makes an incorrect prediction on a sample, then the feature
vector of this sample often has a large angle with the target
class direction. This metric can be affected by the curse of
dimensionality. See support material for more detail.
Margin metric is the minimum distance to the decision
boundary that is often interpreted as a prediction’s robust-
ness. A large margin can tolerate severe data corruption and
large perturbations in the input sample. In equation (1), a
sample with the feature embedding ⃗Xbelonging to label i
needs to have the largest output probability of the model
that needs to satisfy
⃗Wi·⃗X−⃗Wj·⃗X >0, j̸=i, j∈1, ..,C.
The decision boundary of label iis constructed with
n−1hyper-planes (⃗Wi−⃗Wj)·⃗X= 0, j̸=i, j∈1, ..,C.
The minimum distance of a feature vector to the decision
boundary is the minimum distance of the feature vector X
to all n−1hyper-planes:
min{∥(⃗Wi−⃗Wj)·⃗X∥
∥(⃗Wi−⃗Wj)∥, j̸=i, j∈1, ..,C}. (5)
Note that if a model makes a wrong prediction on a
sample, then the margin value will be multiplied by a
negative one to indicate the error.
A similar concept is described as visual angle hard-
ness [41], in which normalized angles and l2 are used as
a metric to describe the generalization of neural network
prediction. Liu et. al [42] proposed a large-margin softmax
loss that was used to maximize the margin between two face
recognition predictions. Compared with previous studies,
ours combines these metrics as a whole to describe the
geometry of a neural network’s high-dimensional feature
representation and embeds these metrics into a visualization
system to compare the similarity between neural network
models and monitor the impact of network pruning and
data corruption.
6 S YSTEM DESIGN
Leveraging previously proposed geometric metrics, we
design a visualization system that generally follows the
overview first (Fig. 2( b⃝) and detail-on-demand (Fig. 2 c⃝,d⃝) mantra [43]. We introduce five key visual components
that integrate these metrics and coordinate with each other
for pruning evaluation and model comparison.
In Fig. 2, we demonstrate a workflow overview of our
visualization system. Fig. 2 ( a⃝) is the classical work pipeline
of machine learning engineers for performing model prun-
ing [44]. Such a pipeline often evaluates the model per-
formance with prediction accuracy but ignores the impact
of pruning over a model’s robustness, latent space vari-
ance, and sample-level impact. Beyond the classical prun-
ing analysis pipeline, our system provides broader evalu-
ation [45], model comparison, and geometric exploration.
Furthermore, the system also supports fine-grain sample-
level analysis from a whole dataset to a single sample. Such
interactions provide domain experts insights about pruning
behaviors and give them feedback to improve the evalua-
tion methods and neural network models. In the following
section, we will discuss the design rationale for these visual
components and explain how they work together to address
the corresponding domain requirements (see section 4).
6.1 Evaluation Table and Geometric Similarity View
Having an overview of pruned models’ performance over
different evaluation benchmark and their feature represen-
tation similarity is an essential first step ( R1). This process
involves comparison among multiple model architectures,
pruning approaches, evaluation metrics, and different sub-
sets of samples’ prediction outcomes. However, efficiently
comparing the feature representations of hundreds of mod-
els visually is a challenge. To address the above require-
ment, we design an evaluation table view for performance
comparison and a geometric similarity view to compare ge-
ometric similarity between models’ feature representations.
In Fig. 3 ( 1⃝), the evaluation table displays an evaluation
summary of the models’ performance over different data
corruption benchmarks [10]. The x-axis of the evaluation
table represents different data corruption types, and the y-
axis represents the architectures and pruning techniques. In
the visualization, each histogram represents the evaluation
outcomes (i.e., accuracy) on a given corruption-type dataset,
for models with increasing pruned rates, which is defined as
the fraction of weights removed by the pruning algorithm.
With the increasing pruned rates, the performance of the
model often worsens except for the model with retraining.
Comparing the similarities among neural network mod-
els is critical for the pruning approach analysis ( R2). In Fig. 3
(2⃝), we design the geometric similarity view to compare
the similarity of pruned or retrained models’ behavior over
different data corruption benchmarks. For each model, we
use each sample’s three geometric features– angle ,margin
and l2norm (Fig. 3 3⃝) as properties to describe a model.
A model is evaluated with 10,000 samples will have 30,000
geometric features. The geometric similarity view projects
all models’ high-dimensional features into two-dimensional
space by the UMAP [46] dimension reduction techniques. In
the visualization, each point represents a model, and nearby
points (models) are more similar than the others.
The evaluation table view and the geometric similarity
view can interact with each other. A set of models that have
similar feature representation is selected in geometric sim-
ilarity view (Fig. 3 2⃝), and related models are highlighted

--- PAGE 7 ---
7
12
344
Pretrain 
Model pruning/ 
Fine-tuning/ 
retrain Our System Feedback Network Pruning and Evaluation Benchmark Comparison 
● Performance comparison 
● Geometric similarity comparison Model Comparison 
● Local geometric comparison 
● Global geometric comparison 
Detail Samples Comparison 
● Geometric feature attribution 
● Provide different level of granularity detail and understanding of  model pruning 
● Identify redundant evaluation benchmarks and pruning approaches. ● Model refinement with pruning or assemble to improve model performance 
● Incorporate additional geometric constraints during training/pruning bc
da
Fig. 2. Compared with classical pruning analysis ( a⃝), the workflow of our visualization system is embedded with three levels of comparison: pruning
method and evaluation benchmark comparison; model comparison; sample comparison. Domain experts can compare pruning techniques and data
corruption benchmarks ( b⃝) through the interaction between the evaluation view and geometric similarity view. For selected models ( c⃝), users can
examine and compare their local and global geometric properties. Furthermore, the sample level comparison ( d⃝) enables users to examine features
that are captured by models. During the exploration process, users can choose a pruning technique to refine the original model, remove redundant
evaluation benchmarks, assemble multiple pruned models for better performance, or incorporate geometric constraints for model retraining.
Architecture & Pruning Evaluation Benchmark 
Angle: xxx 
Margin: xxx 
L2: xxx 
1
23
Select similar feature 
representation 
4
 Display different models’ 
feature representation 
over an evaluation 
benchmark 
Fig. 3. The evaluation table view and geometric similarity view can
coordinate with each other to scalably compare many models at the
same time. Users can select a set of nearby points that are similar
models in the geometric similar view 2⃝, and the related model will be
highlighted in the evaluation view 1⃝. A similar operation can also be
performed in the evaluation table view.
in the evaluation table view (Fig. 3 1⃝). Similarly, the model
selection operation can also be performed in the evaluation
table view and relative models will be highlighted in the
geometric similar view. In Fig. 3 4⃝, we select models that
are evaluated with the impulse noise corruption benchmark.
The geometric similarity view displays only models that
belong to this evaluation. Furthermore, these points can
be connected with a minimum spanning tree for similarity
comparison between different evaluation benchmarks (see
use case 7.1 for more detail).
6.2 Local Geometry View
The model evaluation table view and geometric similarity
view provide a summary of the overall performance and
similarity between network pruning techniques. Beyondthe similarities and differences between network pruning
approaches and evaluation methods, it is important to have
a detailed geometric examination of each feature represen-
tation.
The Local Geometry Plot performs class-specific evalu-
ations, which helps in conveying how well samples from
a specific class are classified and uncovers the cause of
potentially poor performance. The visualization in Fig. 4
1⃝displays geometric metrics (angle and l2 norm) for five
classes, and the deer class is selected to examine more local
features. The accuracy on the top shows that samples from
the cat class have 79.4% accuracy, which is less than the
rest of the samples, and the car class has the best perfor-
mance with 96.8% accuracy. The angle and length metrics
are presented as a scatter plot. Within a single model, these
geometric metric values often have similar ranges for each
class. Among these points, the gray color indicates the
correctly classified samples, and the color red indicates the
incorrectly classified samples. In all five categories, these in-
correct samples have larger angle and smaller length metrics
than the correct samples. By selecting a category, a user can
examine more of a class’s local features, including margin ,
and probability .
In addition to exploring a model’s latent space geometry
and its per-class performance, the local analysis also facil-
itates a comparison between models ( R4). In Fig. 4 2⃝, we
select models with pruning and retrain for comparison. The
visualization shows the variation of the geometric metric
distribution of bird samples and the corresponding geomet-
ric value change. Users can selectively pick samples that
exhibit diverse reactions to network pruning and retraining.
We encode these reactions into four types based on domain
experts’ recommendations. One of these types is that a
sample is predicted correctly in one model but wrong in
the other (red). In the opposite case, a sample is predicted
wrong in the original model but incorrect in the new model,
which is encoded as green. The value in each axis is the
difference (e.g., ∆angle = angle - angle’) of samples’ geo-
metric value between two models. The related scatter plot
matrix encodes all geometric feature distributions and their
distribution shift.

--- PAGE 8 ---
8
1
2
Fig. 4. Local geometric view visually presents the geometric metrics
distribution of each label category 1⃝. The incorrect predictions are
samples often with large angle and small l2norm. Furthermore, the local
geometric view also enables users to compare two models’ geometric
differences ( 2⃝) by displaying the geometric value variation between
samples from different models and providing highlight operations for
samples that are errors in one model but correct in the other.
6.3 Global Geometry View
The local geometry plot provides a detailed view of the
behavior of a single class. However, for the comparison task,
the ability to have a more comprehensive summary is cru-
cial. A global geometry plot gives a geometric overview of
how a model performs on all classes of the currently selected
dataset ( R2), which shows not only how well samples are
classified, but also what other classes the model is confused
with.
The global geometry plot uses a parallel coordinate
display in a n+ 3 dimension configuration, in which nis
the number of classes (for a dataset with a large number
of classes, a pres-election can be applied to focus on a
subset of classes to make visual encoding and exploration
manageable). The first ndimensions represent the angle
metric of a sample with respect to each class direction. As
the previous section described (section 5), a relatively small
angle with a label means a good chance the sample belongs
to this category.
For example, Fig. 5 2⃝displays samples belonging to the
plane class. Most of the samples have smaller angles with
respect to the direction of the plane class in comparison to
other classes. However, these plane samples also have rela-
tively small angles with the bird class (Fig. 5 4⃝) and the ship
class (Fig. 5 5⃝), which can lead to incorrect classification.
By further examining these samples, we can see a similar
shared background, which might be one contributing factor
for this mistake. The other features (l2 norm, margin, and
probability) are displayed as density plots that are separated
from angle features because of the difference in semantic
meaning and value scale.
Similarly with the local geometry plot, the global ge-
ometry plot also enables the geometric comparison among
models and datasets ( R3). In Fig. 5 3⃝, the visualizationdisplays the same plane class but with samples that are
corrupted with fog noise. Before corruption, only a few
plane samples had a small angle with the ship and the bird
classes. However, with the presence of fog corruption, the
number of samples that have small angles with both plane
and bird increases. The density plot (Fig. 5 7⃝) on the top of
each class axis represents the distribution of the angle value.
The x-axis means the number of samples and the y-axis is
the angle of a specific class. The gray color highlights the
reference dataset (original), and the yellow color highlights
the compared dataset (corrupted). In the visualization, the
bird class samples’ angle metrics shift to the smaller values in
the presence of corruption. Moreover, the corrupted dataset
as a whole has a smaller margin and length compared to the
clean datasets. Such a result indicates that the uncertainty
of the model’s prediction between plane samples and bird
samples increases. Our comparative interface allows us to
extensively reason about similarities and differences among
models pruned with various techniques. A detailed case
study is presented in section 7.2.
A confusion matrix [29] aggregates many samples’ pre-
diction results to highlight the ambiguities between label
predictions. However, if a sample is predicted correctly, the
information from the confusion matrix will not help to tell
whether the model may confuse this sample with other
labels or not. The output probability of a model is supposed
to reveal the confusing information of a sample, but many
studies have found that a model’s output probability is often
overcalibrated [33], [47], and researchers should be cautious
about how to read these probability values. Compared
with previous visual encoding, our design visualization and
geometric metrics give more detailed information about a
model’s prediction. In Fig. 5 ( 6⃝), we demonstrate a set
of plane samples that are predicted correctly with high
probability, but have a related small angle with other class
labels. These samples are often fragile to data perturbation
and pruning. The confusion analysis and output probability
do not provide explanations for these vulnerabilities.
6.4 Geometric Attribution View
The geometric feature attribution view shows how each
pixel of an input image contributes to different geometric
features. Our method is designed based on a perturbation
technique that occludes a part of the input image and checks
how much a model changes a sample’s latent geometric
features compared with the original image ( R5). The vari-
ation of these geometric feature values is visualized as a
heatmap to highlight input pixels that can change. During
the design process, we have tried different gradient-based
feature attribution methods [48], [49] to compare the input
features that are captured by different models. However,
we found that gradient-based feature attribution methods
often highlight the same pixel even if the selected models
have significantly different prediction performances, and
this observation is consistent with previous research ob-
servations [50]. Alternatively, we use a perturbation-based
approach to highlight the sensitive region for the model
prediction.
In Fig. 6, we demonstrate the geometric feature attribu-
tion of an image over three geometric metrics and prob-
ability from a well-trained VGG16 with different pruning

--- PAGE 9 ---
9
1
2
3
4
7
5
6
Fig. 5. VGG16 model’s behavior over cifar10 plane samples, and the
same data but corrupted with fog corruption. In the reference dataset
2⃝, these samples are confused with bird and ship samples. In the fog-
corrupted version 3⃝, the confusion is strengthened. In 6⃝, the visual-
ization displays a set of samples that are predicted correctly with high
probability but actually show certain ambiguities with the other labels.
ratios. For example, the heat map of the margin highlights
the input pixels that are critical for prediction robustness.
The color map of each heat map is annotated with a metric
value scale. The green text is the geometric value of an
image without pixels occluding. Blue represents the increase
of the geometric metric and red indicates a decrease of the
value. Removing the blue region will strengthen a certain
geometric feature, and the red region will weaken a geo-
metric feature.
The well-trained VGG16 in Fig. 6 ( 1⃝) predicts the image
belonging to the horse category with high confidence. Mean-
while, all four feature sensitivity views highlight the horse’s
body. This result indicates that for this image, prediction
confidence, margin, angle, and l2 norm features focus on
similar input. With 60% weights pruned, the model still
predicts the image as a horse correctly but with much
lower confidence. Compared with the original prediction,
the new prediction’s four feature sensitivity map highlights
the inconsistent part of the image but with certain portions
focused on the horse’s body.
6.5 Coordination Between Views
In the system, five views coordinate with each other to
address domain requirements. We summarize their relation-
Fig. 6. A geometric feature attribution view gives information about the
sensitivity of input pixels of an image. From left to right are the sensitivity
heat maps with respect to l2, angle, margin, and probability. From top to
bottom are the results of VGG16 model with a different pruning ratios.
Models with different pruning ratios will change the importance of an
input image’s pixel to the final decision.TABLE 1
Composition of visual components to address each domain task.
Visual Components \Tasks R1 R2 R3 R4 R5
Evaluation Table View
Geometric Comparison View
Local Geometry View
Global Geometry View
Geometric Attribution View
ship with each domain task in Table 1, and show how they
interact with each other to expand the exploration ability of
the visualization system in Fig. 2. Users can interact between
the geometric similarity view and evaluation table view
to study and compare the similarity between evaluation
benchmarks and pruning methods (Fig. 2 ( 1⃝)) and select
interesting models for detailed examination and comparison
(Fig. 2 ( 2⃝)). During the pruning analysis, domain experts
are also interested in the performance of different architec-
tures and pruning techniques on a subset of samples or
a specific class (Fig. 2 ( 3⃝)). To achieve this requirement,
during the downstream analysis, our system enables users
to select a subset of samples, and the evaluation table can
automatically reflect the result of current samples ( R3). The
current visualization also enables a comparative analysis,
which shows the performance comparison between cur-
rently selected samples and the entire test dataset. The
length of the green bar indicates that the performance
increases on the selected subset of samples compared to
the accuracy of the overall test dataset. The red bar shows
the opposite concept revealing a decline in the performance.
Such an operation enables users to examine more details of
model behavior and helps them understand model perfor-
mance under different levels of granularity. At the end, users
can select a specific samples for detailed feature attribution
comparison (Fig. 2 ( 4⃝))
7 U SECASES
In this section, we demonstrate the usability of our system
by demonstrating how it can provide valuable insights and
impactful answers regarding behaviors of various pruning
methods, the efficiency of evaluation benchmark, and the
robustness of the pruned models to common data corrup-
tions.
7.1 Comparing Network Pruning Approaches and Iden-
tifying Similar Evaluation Benchmarks
Exploring and testing the performance of network prun-
ing approaches’ over multiple corruption evaluation bench-
marks are often the initial tasks for domain experts to un-
derstand their differences. This process involves questions
such as whether different pruning approaches will yield
similar or different behaviors. The answer to this question
helps researchers understand the pros and cons of different
pruning methods. Domain experts tell us that they often
use similarity metric centered kernel alignment (CKA) [51]
to compare two pruned models’ feature representations one
at a time. However, as the number of feature represen-
tations for comparison increases, this approach becomes
increasingly intractable. Therefore, there is a growing need

--- PAGE 10 ---
10
Vgg16-retrain 
Resnet 18-retrain 
Resnet18-MPTs Vgg16-MPTs 
frostsnow 
Gaussian_blur Impulse noise 1
2 43
56
Fig. 7. In the visualization, 1⃝reveal that MPTs pruning approach and magnitude pruning approach end up with different geometric feature
representations over Gaussian noise corruption.The finding of 2⃝reveals that pruning and retraining with Vgg16 and Resnet18 models have
highly comparable geometric feature representations. 3⃝and4⃝together emphasize the existence of redundancy in common corruption evaluation
benchmarks. 3⃝shows redundancy between the Gaussian blur and the zoom blur benchmarks. 4⃝highlights the similarity among clean, brightness
and saturate benchmark. In 5⃝,6⃝, we select two pairs of evaluation benchmarks on the vgg16 retrain model for comparison. Their geometric
features’ distribution are similar and overlap with each other.
for a scalable methodology that can efficiently assess the
similarity among multiple models simultaneously.
In Fig. 7, we illustrate how domain experts employ a
combination of the geometric similarity view and evaluation
table view to address the above concerns. In Fig. 7 1⃝, we se-
lect the data from the Gaussian noise corruption benchmark
to generate the feature representations of different models.
Specifically, we apply magnitude and MPTs pruning to
VGG16 and Resnet18 network architectures for comparative
analysis. From the visualization, we can tell that magnitude
pruning, when applied with retraining, on both VGG16
and Resnet18 results in feature representations that have a
similar geometry as their dense counterparts. Conversely,
MPT pruning over these two network architectures makes
significant geometric changes. One intriguing observation of
1⃝is that retrained VGG16 and Resnet18 have comparable
feature representation over the Gaussian noise corruption
benchmark. However, MPT pruning applied to VGG16 and
Resnet18 led to significantly different feature representa-
tions. To further validate the consistency of similarity be-
tween retrain VGG16 and Resnet18, we narrow our focus
to these two models and evaluate them with additional
corruptions such as snow, Gaussian blur, frost, and impulse
noise. In Fig. 7 2⃝, the resulting visualization reveals that the
similarities between these two models are consistent across
multiple evaluation benchmarks.
During the comparison and evaluation, understanding
the coverage of the evaluation benchmark is important to
assess the quality of the analysis. In the evaluation table
view, there are 20 distinct evaluation benchmarks, and each
of them is created by different corruption algorithms [10].During the design of these corruption algorithms, whether
these evaluations will lead to similar model behavior has
not been thoroughly explored. Corruption operations that
result in a similar model behavior may be redundant and
unnecessary. In Fig. 7 3⃝,4⃝, we show how such simi-
larity can be revealed by our system. 3⃝highlights two
corruption benchmarks, Gaussian blur, and zoom blur, in
the evaluation table view and geometric similarity view.
Notably, the minimum spanning tree constructed by these
models evaluated with the selected benchmarks exhibits
a significant overlap with each other. Similarly, in 4⃝, the
minimum spanning trees of the models that are evaluated
with the corruption evaluation over brightness, saturation,
and the clean dataset also overlap substantially. We can
further use the local geometric view to compare their local
geometric feature variation. In 5⃝compares the geometric
features distribution of retrain vgg16 over clean and the
brightness corruption data, and 6⃝compares the clean and
the saturate corruption data. In both cases, the difference
between their geometric distribution is subtle. In 5⃝and 6⃝,
each histogram has two color distributions: orange and steel
blue. Because these two histograms are highly overlapped,
only one is color histogram displayed at the end. These
results suggest that for the current image dataset and tasks,
models subjected to the currently available pruning meth-
ods have similar reactions to the evaluation benchmark.
Consequently, there is an opportunity for optimization, such
as removing similar benchmarks, to streamline the analysis
process, reduce computation complexity, and minimize cost.

--- PAGE 11 ---
11
1 
2 
3 
4 
5 
6 
(5), (6) 
(2), (4) 
(1), (3) a b 
c d 
Fig. 8. The geometric similarity view a⃝highlights the position of related models. The visualization result reveals geometric disparities in their feature
representation. The panel c⃝presents a comparison between truck samples of cifar10 on a 90% weight-pruned retrained VGG16, and the same
samples but corrupted with the JPEG compression. The panel b⃝showcases a dense weight well-trained VGG16 and its representation of clean
and corrupted data. The same comparison d⃝is performed on the VGG16, which is generated by the MPT method. The JPEG corruption induces
pronouncedly more angular variations with multiple class directions and minimum distant shift in the weight-trained VGG16 and retrained VGG16
than the VGG16 generated by the MPTs. This observation provides valuable insights into why the models generated by the MPTs’ method tend to
be more robustness to data corruption compared to the regular weight-trained model and weight-pruned retrained model.
7.2 What Is the Representation Difference Between the
Robust MPTs Pruned Models and Others (Dense and
Retrained)?
The use case in section 7.1 gives hints about the reaction
of different pruned models over multiple evaluation bench-
marks. However, it does not give details about the difference
between retrained models and MPTs pruned models. The
MPTs method has been proven [12] to produce compact
models, which not only have high prediction accuracy and
small model size but also models that are significantly more
robust to various data corruptions than regular weight-
trained models1. However, it is still unclear why and how
the MPT approach achieves such a performance gain. Here,
we show that our visualization tool can help develop hy-
potheses about answering this question by comparing the
latent spaces’ geometric structures of the traditional weight-
trained VGG16, the retrained VGG16 model with a certain
pruning ratio, and the VGG16 model generated by MPT.
From Fig. 8 a⃝, the visualization result reveals the sig-
nificant geometric disparity of the model’s feature represen-
tation. The related models are numbered for reference. To
examine this in more geometric detail, we use the global ge-
ometric view to understand the difference between models.
In Fig. 8, the panel ( b⃝) shows the geometric difference
of samples from the truck class with and without the JPGE
corruption in the original weight unpruned trained VGG16
model. The model is trained on the cifar10 dataset with
200 epochs, and the final accuracy on clean test data is
91%. With the same test dataset but corrupted with the
JPEG compression, the accuracy of the model drops to
72%. The models are able to distinguish truck images from
samples of other classes even though some samples may be
slightly confused with car samples ( 1⃝). Once the dataset
is corrupted, the model displays confusion with multiple
1. https://robustbench.github.io/div cifar10 corruptions headingcategories such as plane/ship ( 2⃝), and the models’ global
geometric features on corrupted data are not as coherent as
the clean dataset. The samples belong to the truck class, and
the prediction accuracy dropped from 93.3% to 82.9%.
The panel ( d⃝) illustrates the same comparison, but using
the models generated by the MPTs method, i.e., starting
with an untrained VGG16 model and then 90% of the
weights are pruned, resulting in a model with 91% accu-
racy. The performance of the model declines to 83% when
tested on the corruption dataset, which is significantly better
than the performance of the dense weight trained model
(71.84%). For samples belonging to the truck class, the
performance of the model only slightly drops from 94.3%
to 91.7%. The angle distribution of samples with each class
direction displays a distinct pattern, i.e., the dense VGG16
has a much larger variance but smaller mean angle , whereas
the MPT model has much a smaller variance but larger
mean. For MPTs, the global geometric visualization com-
parison 5⃝,6⃝shows that the difference between corrupted
and clean data is minor.
A simple potential explanation of such an observation
is that the pruned model contains fewer parameters, which
may lead to a latent space that contains much less informa-
tion, and the model’s prediction should be less sensitive to
the input noise, which leads to a more robust model. To
verify this hypothesis, we add an additional comparison
(c⃝), in which the model is pruned with 90% weight but
retrained 50 epochs to gain the original prediction accuracy
around 91.14%. For the truck class, the prediction accuracy
increases to 94.4%, and the accuracy of the corrupted data
increases from 82.9% to 85.0%. The new comparison gives
positive feedback about the hypothesis that decreasing the
number of parameters in a model can improve a model’s
resiliency to data corruption.
However, the model generated by the MPTs model’s
prediction accuracy on corruption is still significantly bet-

--- PAGE 12 ---
12
ter than the retrained model. This behavior indicates that
besides the number of parameters in the model-generated
MPTs, the unique geometric latent space of the MPTs model
can play a significant role.
After presenting this use case to the domain experts, we
collected and summarized their feedback about how this
information is helpful to their research as follows: ”Such an
observation motivates a hypothesis that the representation
geometry of MPTs is less sensitive to different corruption
types than that of the regular weight-trained or retrained
models. These sensitive properties may related to the thin
standard deviation of geometric features and angle scales of
different models’ feature representations. Meanwhile, this
finding has potentially significant implications for robust
machine learning as it suggests that to design a robust
neural network, one should not only optimize for the
training accuracy but also incorporate additional geometric
constraints during training”.
7.3 How Do Samples’ Geometry Impacted by Model
Pruning, Data Corruptions, and Model Retrain?
Model evaluation often aggregates a model’s prediction
over all test samples, but often misses critical details about
the model’s behavior in a specific category or a subset
of samples. The previous network pruning literature [7]
has assessed the impact of network pruning over samples
by proposing a metric called PIE (pruning-identified ex-
emplars) to identify vulnerable samples. The PIE value is
calculated by the disagreement between compressed and
uncompressed models that require a large amount of com-
putation resources. At the same time, this metric does not
reveal what properties cause a sample’s vulnerability. Here,
we demonstrate how we can use our visualization system to
quickly highlight these samples and provide depth analysis
such as how pruning and retraining affect a sample and
what features are captured or forgotten by the model.
In Fig. 9, we demonstrate an interactive exploration case
that compares three subsets of samples’ observations with
different geometric properties. We select these samples from
the frog category of the cifar10 dataset and evaluate their
performance on pruned models over available evaluation
benchmarks. Fig. 9 1⃝displays the evaluation result of sam-
ples that are selected from the frog category with large
length and small angle . In the evaluation table view, we
can tell that these samples perform better than the whole
dataset with high prediction accuracy with different data
corruption benchmarks. Meanwhile, they are less affected
by the different pruning techniques.
Compared with Fig. 9 1⃝, Fig. 9 2⃝is the evaluation re-
sult from samples that have a relatively larger angle and
smaller l2 norms. The overall performance of these sam-
ples declines compared with samples from Fig. 9 1⃝. These
samples demonstrate less resiliency to different pruning
and corruption operations. The last case is Fig. 9 3⃝, which
shows samples with small l2 and a large angle. All these
samples have poor performance in different evaluations
and different model architectures. Meanwhile, the Resnet18
model performs worse than the VGG16 model over these
samples under different pruning techniques and retraining
models.CNN Architecture rc-angle rc-l2 rc-margin.
VGG16 -0.636 0.516 0.685
resnet18 -0.715 0.192 0.696
resnet50 -0.7172 0.053 0.6677
resnet152 -0.7158 -0.035 0.7228
densenet121 -0.726 -0.015 0.6732
TABLE 2
The result of the Imagenet-C validation dataset. This table shows the
Pearson correlation coefficient between different geometric features
and models’ robustness. Angle and margin show a significant
correlation with robustness. The correlation between l2 and robustness
is moderate or subtle.
CNN Architecture rc-angle rc-l2 rc-margin.
VGG16 -0.6739 0.3924 0.6534
VGG19 -0.6667 0.3984 0.6514
resnet18 -0.6795 0.1737 0.7282
resnet50 -0.6933 0.083 0.7058
Densenet121 -0.6816 0.0252 0.7154
TABLE 3
The result of the Imagenet validation dataset. This table shows the
Pearson correlation coefficient between different geometric features
and model magnitude pruning. Angle and margin show a significant
correlation with pruning vulnerability. The correlation between l2 and
pruning vulnerability is subtle.
The above observation reveals that large l2 and small
angle values often show resiliency to the different pruning
operations across multiple network models and different
data corruption evaluations. On the other hand, samples
with small length and large angle values display fragile
behavior, and these samples are affected dramatically by
pruning and data corruption. To further verify such obser-
vations, we follow domain experts’ suggestion to perform a
quantitative evaluation to measure the relationship between
geometric properties and data corruption.
Our experiment measures two relationships: how does a
sample’s geometric properties correlate with different data
corruptions, and how does a sample’s geometric properties
correlate with standard model pruning? In Table 1 and
Table 2, we demonstrate our evaluation result over the
Imagenet dataset. The result finds that the angle and margin
metric are strongly correlated with the samples’ resiliency to
data corruption and model pruning. However, the l2 norm
does not show a consistent correlation relationship between
pruning and data corruption. The support material includes
details of how we performed the experiment and more
dataset evaluation results. This result shows that geometric
properties can be used as a metric to highlight vulnerable
and resilient samples without comparing many models, and
it is a significant advantage compared with the previous
approach [7].
Model pruning and retraining not only recover the per-
formance of the pruned model but also improve it, espe-
cially for certain data samples. We can check the feature
attribution heat map with respect to the margin in Fig. 10.
We compare the geometric feature attribution heat map of
the pretrained model and retrained model with 50% weight
removed. We select Fig. 10( 1⃝) samples (green color) that are
predicted incorrectly in the pretrained model but correctly
in the retrained model. The geometric feature attribution
visualization in Fig. 10( 3⃝) shows that the retrained model
captures better features than the pretrained model.

--- PAGE 13 ---
13
1 3 2
1
2
3
Fig. 9. An overview of the evaluation comparison across multiple architectures and pruning techniques of frog samples. The sample with large
length and small angle are less affected by the pruning, different model architecture, and data corruption. On the other hand, the samples with
smaller length and large angle are fragile over different pruning approaches and models.
Sample 
Error 
Prediction 
Right 
Prediction 12
3
Fig. 10. Model pruning and retraining can improve prediction perfor-
mance. Comparing the retrained and original model ( 1⃝), the samples
that are predicted wrong in the original model but predicted right in the
retrained model have captured better features with respect to margin.
(3⃝).
8 D ISCUSSION AND CONCLUSION
In this work, we introduced three geometrically inspired
metrics in the neural network latent space for a compar-
ative study of how widely adopted (and state-of-the-art)
model pruning approaches impact neural network models’
internal representation and performance. The proposed vi-
sualization system is able to highlight the key differences
between pruning techniques that are unknown to ML re-
searchers. The visualization also provided valuable insights
for explaining model robustness from a geometric perspec-
tive and answered why certain pruning methods produce
surprisingly robust models whereas others reduce model
robustness.
One potential limitation of the current system is the
scalability of the parallel coordinate view. As for more com-
plex datasets, such as Imagenet (1000 classes), visualizing
allclass directions at the same is not practical. However,
practically speaking, it is important to note that even with
a more scalable visual encoding, there is a limit to how
many classes a user can meaningfully examine at the same
time. Accordingly, a pres-election or ranking operation can
greatly mitigate this challenge. In the future, we plan to
develop an easy-to-use interface to help users select a subset
of the interesting classes with different criteria, e.g., classes
mostly influenced by network pruning or data perturbation.
Another potential improvement of the current system forfuture work comes from additional introspection ability,
i.e., can we combine other model explanation tools such as
saliency map [52] or concept-based explanation [53] with
the proposed geometry metric, to better articulate the exact
semantics changes induced by pruning?

--- PAGE 14 ---
14
9 S UPPORTING MATERIALS
In this section, we describe the quantitative evaluation of
three geometric metrics. We measure the correlation be-
tween these geometric metrics and data corruption. Further,
we also measure the correlation between the impact of mag-
nitude pruning and these geometric metrics. We find that
both angle and margin are highly correlated with corrup-
tion operation and model pruning. Finally, we demonstrate
that the loss functions have a strong impact on the geo-
metric shape of a neural network’s feature representation.
We perform our experiment on MNIST, Cifar10, Cifar100,
and ImageNet datasets. For the MNIST dataset, we train
LeNet5 with 50 epochs. The model trained with the cifar10
and cifar100 datasets is trained with 200 epochs. For the
ImageNet dataset, we use the pretrained model from the
PyTorch library to perform the experiments
9.1 Correlation Between Geometric Metrics and Cor-
ruption Robustness
To attribute the robustness to three geometric features, we
perform an experiment to measure the correlation between
corruption robustness and geometric properties. Note that
the robustness is defined as the average accuracy on cor-
rupted data (i.e., applying c= 19 corruption types and
s= 5 severity levels to each clean test image xiand
transforming it to a set of corrupted images xc,s
i):
robustness i=19X
c=15X
s=1I{yi=f(xc,s
i)}, (6)
where Iis an indicator function taking value 1 if the pre-
diction is correct on the corrupted image and 0 otherwise.
Furthermore, robustness idenotes per sample robustness
quantifying the level of invariance to corruptions.
The result of our experiment is summarized in Tables 1,
2, 3, and 4, in which rc-angle, rc-l2, and rc-margin denote
the Pearson correlation coefficient between the robustness
and the angle, length, and margin, respectively. Overall, the
angle and margin to the decision boundary show a strong
correlation with the corruption robustness. However, the
length is moderately correlated. The correlation between
length and robustness is not consistent across different
architectures. ResNet Architecture has a much smaller l2
correlation compared with regular convolution neural net-
works.
9.2 Correlation Between Geometric Metric and Pruning
Vulnerability
To measure the correlation between geometric metrics and
pruning vulnerability (PV), we use magnitude pruning with
a different pruning ratio on the same model to understand
CNN Architecture rc-angle rc-l2 rc-margin.
Lenet 5 -0.6902 0.3516 0.6359
TABLE 4
The result of the MNIST-C dataset. This table shows the Pearson
correlation coefficient between different geometric features and models’
robustness. Angle and margin show a significant correlation with
robustness. The correlation between l2 and robustness is moderate or
weak.the impact of magnitude pruning on different samples. The
operation is performed 10 times with the pruning ratio from
0%to90% (e.g.,10%, 20%,...), which includes 10 ratios. We
collect each sample’s correctness in each pruned model and
aggregate the result as a PV value. A large PV value
indicates that the samples’ features are resilient to model
pruning. We measure the correlation between the PV and
the three geometric metrics.
PVi=10X
s=1I{yi=f(xc,s
i)}, (7)
We perform our experiment on the MNIST, Cifar10, Ci-
far100, and Imagenet datasets. The result of our experiment
is summarized in Tables 6, 7, and 8, in which rc-angle, rc-
l2, and rc-margin denote the Pearson correlation coefficient
between the pruning vulnerability and the angle, length,
and margin, respectively. Overall, the angle and margin
to the decision boundary show a strong correlation with
the pruning vulnerability. Samples with small and large
margins are more resilient to magnitude pruning than the
other samples. However, the length is a weak correlation.
9.3 The Effect of the Curse of Dimensionality
High-dimensional geometry is affected by the curse of di-
mensionality [54] such that the intuitions humans have on
two- or three-dimensional spaces may not apply to high-
dimensional space. For example, the angle between two ran-
dom vectors in high dimensional space will display counter-
intuitive behavior. In Fig. 11, the top plot is the result of
an experiment that is performed on vectors generated by
uniform random sampling. With the increasing dimension,
the mean angle between 10000 uniformly random sample
vectors will converge to 90 (orthogonal) and their standard
deviation will become small. The orthogonality between
two vectors indicates that they do not contain any infor-
mation about each other.
The phenomenon also affects the high-dimensional la-
tent space of well-trained neural network models. For un-
trained LeNet 5 models, the samples’ mean angle and stan-
dard deviation with the class direction are 88.8±6.8, which
indicates an uninformative feature vector. The bottom plot
of Fig. 11 displays the average angle and standard deviation
of the MNIST dataset with their class directions on well-
trained LeNet 5 models. The x-axis represents a dimension
that is the number of neurons placed in the last feature layer
of LeNet 5 models. As the number of neurons increases,
the average angle of samples with the class direction also
increases and the standard deviation will become smaller.
In our study, it is a rare event that a feature vector has
a small angle (e.g., 0 degrees) with its class direction in
CNN Architecture rc-angle rc-l2 rc-margin.
Resnet18 -0.8148 0.3401 0.7794
VGG16 -0.8107 0.5783 0.8234
Alexnet -0.8572 0.5557 0.7681
TABLE 5
The result of the cifar10-C dataset. This table shows the Pearson
correlation coefficient between different geometric features and models’
robustness. Angle and margin show a significant correlation with
robustness. The correlation between l2 and robustness is moderate or
weak.

--- PAGE 15 ---
15
CNN Architecture rc-angle rc-l2 rc-margin.
VGG16 -0.7665 -0.0618 0.6891
resnet18 -0.7673 0.3334 0.7316
Googlenet -0.7686 0.3815 0.7228
TABLE 6
The result of the cifar100-C dataset. This table shows the Pearson
correlation coefficient between different geometric features and models’
robustness. Angle and margin show a significant correlation with
robustness. The correlation between l2 and robustness is moderate or
weak.
CNN Architecture rc-angle rc-l2 rc-margin.
Lenet 5 -0.399 0.238 0.486
TABLE 7
The result of the MNIST dataset. This table shows the Pearson
correlation coefficient between different geometric features and model
magnitude pruning.
the popular well-trained convolution architectures. Most of
the angle between samples and the class direction is above
30 degrees, and the angle of samples with unrelated class
direction is near 90.
Fig. 11. Top plot shows that with increasing dimensions, the angle
between two uniformly sampled random vectors will converge to 90
degrees with smaller standard deviations. The bottom plot shows that
increasing the number of neurons in the last feature layer will lead to a
large angle between samples and their class direction.
CNN Architecture rc-angle rc-l2 rc-margin.
Resnet18 -0.787 0.3067 0.7264
VGG16 -0.838 0.395 0.764
Alexnet -0.739 0.442 0.642
TABLE 8
The result of the cifar10 dataset. This table shows the Pearson
correlation coefficient between different geometric features and model
magnitude pruning. Angle and margin show a significant correlation
with robustness. The correlation between l2 and robustness is
moderate or weak.CNN Architecture rc-angle rc-l2 rc-margin.
Resnet18 -0.814 0.333 0.822
VGG16 -0.8731 0.002 0.8797
Googlenet -0.838 0.3687 0.836
TABLE 9
The result of the cifar100 dataset. This table shows the Pearson
correlation coefficient between different geometric features and model
magnitude pruning. Angle and margin show a significant correlation
with robustness. The correlation between l2 and robustness is
moderate or weak.
REFERENCES
[1] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V . Panneershelvam,
M. Lanctot et al. , “Mastering the game of go with deep neural
networks and tree search,” nature , vol. 529, no. 7587, pp. 484–489,
2016.
[2] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ron-
neberger, K. Tunyasuvunakool, R. Bates, A. ˇZ´ıdek, A. Potapenko
et al. , “Highly accurate protein structure prediction with al-
phafold,” Nature , vol. 596, no. 7873, pp. 583–589, 2021.
[3] A. W. Senior, R. Evans, J. Jumper, J. Kirkpatrick, L. Sifre, T. Green,
C. Qin, A. ˇZ´ıdek, A. W. Nelson, A. Bridgland et al. , “Improved
protein structure prediction using potentials from deep learning,”
Nature , vol. 577, no. 7792, pp. 706–710, 2020.
[4] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee,
J. Ngiam, Q. V . Le, Y. Wu et al. , “Gpipe: Efficient training of giant
neural networks using pipeline parallelism,” Advances in neural
information processing systems , vol. 32, 2019.
[5] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural networks with pruning, trained quantization and
huffman coding,” arXiv preprint arXiv:1510.00149 , 2015.
[6] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, “Learning
efficient convolutional networks through network slimming,” in
Proceedings of the IEEE international conference on computer vision ,
2017, pp. 2736–2744.
[7] S. Hooker, A. Courville, G. Clark, Y. Dauphin, and A. Frome,
“What do compressed deep neural networks forget?” arXiv
preprint arXiv:1911.05248 , 2019.
[8] L. Liebenwein, C. Baykal, B. Carter, D. Gifford, and D. Rus, “Lost
in pruning: The effects of pruning neural networks beyond test
accuracy,” Proceedings of Machine Learning and Systems , vol. 3, pp.
93–138, 2021.
[9] S. Bulusu, B. Kailkhura, B. Li, P . K. Varshney, and D. Song,
“Anomalous example detection in deep learning: A survey,” IEEE
Access , vol. 8, pp. 132 330–132 347, 2020.
[10] D. Hendrycks and T. Dietterich, “Benchmarking neural network
robustness to common corruptions and perturbations,” arXiv
preprint arXiv:1903.12261 , 2019.
[11] J. Diffenderfer, B. R. Bartoldson, S. Chaganti, J. Zhang,
and B. Kailkhura, “A winning hand: Compressing deep
networks can improve out-of-distribution robustness,” CoRR , vol.
abs/2106.09129, 2021. [Online]. Available: https://arxiv.org/abs/
2106.09129
[12] J. Diffenderfer and B. Kailkhura, “Multi-prize lottery ticket hy-
pothesis: Finding accurate binary neural networks by pruning a
randomly weighted network,” 2021.
[13] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.”
Journal of machine learning research , vol. 9, no. 11, 2008.
[14] S. Wold, K. Esbensen, and P . Geladi, “Principal component analy-
sis,” Chemometrics and intelligent laboratory systems , vol. 2, no. 1-3,
pp. 37–52, 1987.
[15] Y. LeCun, J. Denker, and S. Solla, “Optimal brain damage,” Ad-
vances in neural information processing systems , vol. 2, 1989.
[16] J. Frankle and M. Carbin, “The lottery ticket hypothesis: Finding
sparse, trainable neural networks,” arXiv preprint arXiv:1803.03635 ,
2018.
[17] V . Ramanujan, M. Wortsman, A. Kembhavi, A. Farhadi, and
M. Rastegari, “What’s hidden in a randomly weighted neural
network?” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2020, pp. 11 893–11 902.
[18] G. Li, J. Wang, H.-W. Shen, K. Chen, G. Shan, and Z. Lu,
“Cnnpruner: Pruning convolutional neural networks with visual
analytics,” IEEE Transactions on Visualization and Computer Graphics ,
vol. 27, no. 2, pp. 1364–1373, 2020.

--- PAGE 16 ---
16
[19] J. Wang, L. Gou, W. Zhang, H. Yang, and H.-W. Shen, “Deepvid:
Deep visual interpretation and diagnosis for image classifiers
via knowledge distillation,” IEEE transactions on visualization and
computer graphics , vol. 25, no. 6, pp. 2168–2180, 2019.
[20] F. Hohman, H. Park, C. Robinson, and D. H. P . Chau, “S ummit:
Scaling deep learning interpretability by visualizing activation and
attribution summarizations,” IEEE transactions on visualization and
computer graphics , vol. 26, no. 1, pp. 1096–1106, 2019.
[21] M. Liu, S. Liu, H. Su, K. Cao, and J. Zhu, “Analyzing the noise
robustness of deep neural networks,” in 2018 IEEE Conference on
Visual Analytics Science and Technology (V AST) , 2018, pp. 60–71.
[22] M. Kahng, P . Y. Andrews, A. Kalro, and D. H. Chau, “A cti v is:
Visual exploration of industry-scale deep neural network models,”
IEEE transactions on visualization and computer graphics , vol. 24,
no. 1, pp. 88–97, 2017.
[23] J. Sun, A. Mehra, B. Kailkhura, P .-Y. Chen, D. Hendrycks, J. Hamm,
and Z. M. Mao, “Certified adversarial defenses meet out-of-
distribution corruptions: Benchmarking robustness and simple
baselines,” arXiv preprint arXiv:2112.00659 , 2021.
[24] E. Mintun, A. Kirillov, and S. Xie, “On interaction between aug-
mentations and corruptions in natural corruption robustness,”
Advances in Neural Information Processing Systems , vol. 34, 2021.
[25] C. Michaelis, B. Mitzkus, R. Geirhos, E. Rusak, O. Bringmann, A. S.
Ecker, M. Bethge, and W. Brendel, “Benchmarking robustness in
object detection: Autonomous driving when winter is coming,”
arXiv preprint arXiv:1907.07484 , 2019.
[26] J. Sun, Q. Zhang, B. Kailkhura, Z. Yu, C. Xiao, and Z. M. Mao,
“Benchmarking robustness of 3d point cloud recognition against
common corruptions,” arXiv preprint arXiv:2201.12296 , 2022.
[27] D. Ren, S. Amershi, B. Lee, J. Suh, and J. D. Williams, “Squares:
Supporting interactive performance analysis for multiclass clas-
sifiers,” IEEE transactions on visualization and computer graphics ,
vol. 23, no. 1, pp. 61–70, 2016.
[28] J. Zhang, Y. Wang, P . Molino, L. Li, and D. S. Ebert, “Manifold:
A model-agnostic framework for interpretation and diagnosis of
machine learning models,” IEEE transactions on visualization and
computer graphics , vol. 25, no. 1, pp. 364–373, 2018.
[29] A. Hinterreiter, P . Ruch, H. Stitz, M. Ennemoser, J. Bernard,
H. Strobelt, and M. Streit, “Confusionflow: A model-agnostic
visualization for temporal analysis of classifier confusion,” IEEE
Transactions on Visualization and Computer Graphics , 2020.
[30] A. Chatzimparmpas, R. M. Martins, K. Kucher, and A. Kerren,
“Stackgenvis: Alignment of data, algorithms, and models for
stacking ensemble learning using performance metrics,” IEEE
Transactions on Visualization and Computer Graphics , vol. 27, no. 2,
pp. 1547–1557, 2020.
[31] J. Wang, L. Wang, Y. Zheng, C.-C. M. Yeh, S. Jain, and W. Zhang,
“Learning-from-disagreement: A model comparison and visual
analytics framework,” IEEE Transactions on Visualization and Com-
puter Graphics , 2022.
[32] Y. Li, T. Fujiwara, Y. K. Choi, K. K. Kim, and K.-L. Ma, “A
visual analytics system for multi-model comparison on clinical
data predictions,” Visual Informatics , vol. 4, no. 2, pp. 122–131, 2020.
[33] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration
of modern neural networks,” in International Conference on Machine
Learning . PMLR, 2017, pp. 1321–1330.
[34] J. Xia, Y. Zhang, J. Song, Y. Chen, Y. Wang, and S. Liu, “Revisiting
dimensionality reduction techniques for visual cluster analysis: An
empirical study,” IEEE Transactions on Visualization and Computer
Graphics , vol. 28, no. 1, pp. 529–539, 2021.
[35] S. Liu, B. Wang, P .-T. Bremer, and V . Pascucci, “Distortion-guided
structure-driven interactive exploration of high-dimensional
data,” in Computer Graphics Forum , vol. 33, no. 3. Wiley Online
Library, 2014, pp. 101–110.
[36] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag, “What
is the state of neural network pruning?” Proceedings of machine
learning and systems , vol. 2, pp. 129–146, 2020.
[37] N. Lee, T. Ajanthan, and P . H. Torr, “Snip: Single-shot network
pruning based on connection sensitivity,” ICLR , 2019.
[38] P . Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, “Impor-
tance estimation for neural network pruning,” in CVPR , 2019, pp.
11 264–11 272.
[39] B. Hassibi and D. Stork, “Second order derivatives for network
pruning: Optimal brain surgeon,” Advances in neural information
processing systems , vol. 5, 1992.[40] K. Sreenivasan, S. Rajput, J.-Y. Sohn, and D. Papailiopoulos, “Find-
ing everything within random binary networks,” arXiv preprint
arXiv:2110.08996 , 2021.
[41] B. Chen, W. Liu, Z. Yu, J. Kautz, A. Shrivastava, A. Garg, and
A. Anandkumar, “Angular visual hardness,” in International Con-
ference on Machine Learning . PMLR, 2020, pp. 1637–1648.
[42] W. Liu, Y. Wen, Z. Yu, and M. Yang, “Large-margin softmax loss for
convolutional neural networks,” arXiv preprint arXiv:1612.02295 ,
2016.
[43] B. Shneiderman, “The eyes have it: A task by data type taxonomy
for information visualizations,” in The craft of information visualiza-
tion. Elsevier, 2003, pp. 364–371.
[44] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the
value of network pruning,” arXiv preprint arXiv:1810.05270 , 2018.
[45] M. Tulio Ribeiro, T. Wu, C. Guestrin, and S. Singh, “Beyond
accuracy: Behavioral testing of nlp models with checklist,” arXiv
e-prints , pp. arXiv–2005, 2020.
[46] L. McInnes, J. Healy, and J. Melville, “Umap: Uniform manifold
approximation and projection for dimension reduction,” arXiv
preprint arXiv:1802.03426 , 2018.
[47] P . Xenopoulos, J. Rulff, L. G. Nonato, B. Barr, and C. Silva, “Cal-
ibrate: Interactive analysis of probabilistic model output,” IEEE
Transactions on Visualization and Computer Graphics , 2022.
[48] A. Adadi and M. Berrada, “Peeking inside the black-box: a survey
on explainable artificial intelligence (xai),” IEEE access , vol. 6, pp.
52 138–52 160, 2018.
[49] K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside con-
volutional networks: Visualising image classification models and
saliency maps,” arXiv preprint arXiv:1312.6034 , 2013.
[50] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, and
B. Kim, “Sanity checks for saliency maps,” Advances in neural
information processing systems , vol. 31, 2018.
[51] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton, “Similarity of
neural network representations revisited,” in International confer-
ence on machine learning . PMLR, 2019, pp. 3519–3529.
[52] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, “Grad-cam: Visual explanations from deep networks via
gradient-based localization,” in Proceedings of the IEEE international
conference on computer vision , 2017, pp. 618–626.
[53] B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas et al. ,
“Interpretability beyond feature attribution: Quantitative testing
with concept activation vectors (tcav),” in International conference
on machine learning . PMLR, 2018, pp. 2668–2677.
[54] M. Verleysen and D. Franc ¸ois, “The curse of dimensionality in data
mining and time series prediction,” in International work-conference
on artificial neural networks . Springer, 2005, pp. 758–770.
