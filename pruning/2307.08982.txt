# 2307.08982.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2307.08982.pdf
# File size: 5242057 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1111
Neural Network Pruning as Spectrum Preserving Process
SHIBO YAO, New Jersey Institute of Technology, USA
DANTONG YU, New Jersey Institute of Technology, USA
IOANNIS KOUTIS, New Jersey Institute of Technology, USA
Neural networks have achieved remarkable performance in various application domains. Nevertheless, a large
number of weights in pre-trained deep neural networks prohibit them from being deployed on smartphones
and embedded systems. It is highly desirable to obtain lightweight versions of neural networks for inference
in edge devices. Many cost-effective approaches were proposed to prune dense and convolutional layers that
are common in deep neural networks and dominant in the parameter space. However, a unified theoretical
foundation for the problem mostly is missing. In this paper, we identify the close connection between matrix
spectrum learning and neural network training for dense and convolutional layers and argue that weight
pruning is essentially a matrix sparsification process to preserve the spectrum. Based on the analysis, we also
propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning result.
We carefully design and conduct experiments to support our arguments. Hence we provide a consolidated
viewpoint for neural network pruning and enhance the interpretability of deep neural networks by identifying
and preserving the critical neural weights.
CCS Concepts: •Computing methodologies →Spectral methods ;Linear algebra algorithms ;Neural
networks .
Additional Key Words and Phrases: neural network pruning, interpretability, matrix sparsification, randomized
algorithm
ACM Reference Format:
Shibo Yao, Dantong Yu, and Ioannis Koutis. 2022. Neural Network Pruning as Spectrum Preserving Process.
ACM Trans. Knowl. Discov. Data. 37, 4, Article 111 (August 2022), 17 pages. https://doi.org/10.1145/1122445.
1122456
1 INTRODUCTION
Deep neural network pruning[ 17] [20] [19] [29] has been an essential topic in recent years due
to the emerging desire of efficiently deploying pre-trained models on light-weight devices, for
example, smartphones, edge devices (Nvidia Jetson and Raspberry Pi), and Internet of Things (IoTs).
Neural network pruning dates back to last century when the initial attempts were made by optimal
brain damage [28] and optimal brain surgeon [21], and have achieved impressive results.
A larger body of work, namely, neural network compression [ 9][23][11][26], aims at removing a
large number of parameters without significantly deteriorating the performance while benefiting
from the reduced storage footprints for pre-trained networks and computing power. Because the
1Preprint.
Authors’ addresses: Shibo Yao, New Jersey Institute of Technology, Newark, NJ, USA, espoyao@gmail.com; Dantong Yu,
New Jersey Institute of Technology, Newark, NJ, USA, dtyu@njit.edu; Ioannis Koutis, New Jersey Institute of Technology,
Newark, NJ, USA, ikoutis@njit.edu.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
©2018 Association for Computing Machinery.
1556-4681/2022/8-ART111 $15.00
https://doi.org/10.1145/1122445.1122456
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.arXiv:2307.08982v1  [cs.LG]  18 Jul 2023

--- PAGE 2 ---
111:2 Shibo Yao, Dantong Yu, and Ioannis Koutis
dense layers and convolutional layers usually dominate the space and time complexity in neural
networks, multiple approaches have been proposed to compress these two types of network. These
approaches demonstrated surprising simplicity and superior efficacy in many situations of neural
network pruning.
Nonetheless, existing works mostly are experiment-oriented and fall into distinct categories.
Thoroughly studying these related work revealed several questions. Can the same approaches
be applied to both dense layers and convolutional layers? What are the theory and mechanisms
justifying the chosen pruning operations and providing the performance guarantee of the pruned
networks? And can we gain a better insight to interpret deep learning models [ 35][13] from studying
the topics of neural network pruning?
In this paper, we employ spectral theory and matrix sparsification techniques to provide an
alternative perspective for neural network pruning and attempt to answer the questions mentioned
above. Our contributions are as follows:
•We discover the relationship between neural network training and the spectrum learning
process of weight matrices. By tracking the evolution of matrix spectrum, we formalize neural
network pruning as a spectrum preserving process.
•We illustrate in detail the resemblance between dense layer and convolutional layer and
essentially they both are matrix multiplication. Consequently, a unified viewpoint, namely
matrix sparsification, is proposed to prune both types of layer.
•Based on our analysis, we show the potential of customizing matrix sparsification algorithm
for better neural network pruning by proposing a tailored sparsification algorithm.
•We thoroughly conduct experimental tasks, each of which targets a specific argument to
provide appropriate and solid empirical support.
The outline of this paper is as follows: Section 2 briefly reviews related literature. In section 3,
we formulate the problem of neural network pruning and explain how we tackle it from spectral
theory perspective; Section 4 provides matrix sparsification techniques that are suitable for neural
network pruning. In section 5, we generalize the analysis schema to convolutional layers. Section 6
presents detailed empirical study; and finally the conclusions are offered.
2 RELATED WORK
The essential objective of neural network pruning is to remove parameters from pre-trained models
without incurring significant performance loss. Removing parameters from the weight matrix is
equivalent to eliminate neuron connections from a neural layer. Our discussion is mainly concerned
with two research topics: neural network pruning and matrix sparsification.
2.1 Neural Network Pruning
Early attempts in pruning neural networks dated back to optimal brain damage [28] and optimal
brain surgeon [21] where the authors employed the second-order partial derivative matrix to decide
the importance of connections and remove those unimportant ones. The simple yet very effective
magnitude-based approach was examined in [ 20] for both dense layers and convolutional layers,
where small entries in terms of absolute value were removed from the network. The work was
further expanded [ 19] with the quantization techniques,which was previously introduced by [ 17].
Channel-wise pruning [ 29] for convolution was also examined based on the 𝑙1-norm magnitude of
the filters. The lottery ticket hypothesis [ 15] and a consequent work [ 39] have again attracted the
attention of the community on the magnitude-based pruning.
There is another line of efforts [ 11] [38] [26] [23] [30] based on low-rank approximation, which
has a similar taste to pruning but with a more clear theoretical support, falling within a larger scope
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 3 ---
Neural Network Pruning as Spectrum Preserving Process 111:3
of work namely neural network compression [ 9]. The low rank approximation was applied to the
weight matrix or tensors [ 24] in order to reduce the storage requirements or inference time for
large pre-trained models. Some other works on neural network compression also include weight
sharing [7] and hash trick [8], where they also look at the problem in the frequency domain.
2.2 Matrix Sparsification
Matrix sparsification is important in many numerical problems, e.g. low-rank approximation, semi-
definite programming and matrix completion, which widely exist in data mining and machine
learning problems. Matrix sparsification is to reduce the number of nonzero entries in a matrix
without altering its spectrum. The original problem is NP-hard [ 31][18]. The study of approximation
solutions to this problem was pioneered by [ 3], and further expanded in [ 2] [4] [1] [33] [12]. An
extensive study on the error bound was done in [16].
Since the spectrum of the sparsified matrix does not deviate significantly from that of the original
matrix, serving as a linear operator the matrix retains its functionality, i.e. 𝐴∈R𝑚×𝑛is a mapping
R𝑚→R𝑛. We can define the matrix sparsification process as the following optimization problem:
min∥˜𝐴∥0
s.t.∥𝐴−˜𝐴∥≤𝜖(1)
where𝐴is the original matrix, ˜𝐴is the sparsified matrix, ∥·∥ 0is the 0-norm that equals the number
of non-zero entries in a matrix, ∥·∥ denotes matrix norm, 𝜖≥0is the error tolerance.
In matrix sparsification, we often use the spectral norm (2-norm) ∥·∥ 2and the Frobenius norm
(F-norm)∥·∥𝐹to measure the deviation of the sparsified matrix from the original one.
3 PROBLEM FORMULATION
Given a dense layer z𝑡=𝜎(z𝑇
𝑡−1𝐴+b), where z𝑡−1∈R𝑚is the input signal, z𝑡∈R𝑛is the output
signal,𝐴∈R𝑚×𝑛is the weight matrix, b∈R𝑛is the bias,𝜎denotes some activation function,
we desire to obtain a sparse version of 𝐴denoted by ˜𝐴such that𝐴and ˜𝐴have similar spectral
structure and z𝑇
𝑡−1˜𝐴is as close to z𝑇
𝑡−1𝐴as possible.
Similarly for a convolution 𝑧𝑡=𝑇∗𝑧𝑡−1we want to find a sparse version of 𝑇such that the
convolution result is as close as possible, where 𝑇could be a vector, matrix or higher-order array
depending on the order of input signal and the number of output channel. By closeness, we use
norms as metric. We start from the investigation on dense layers and then make generalization on
convolutional layers.
3.1 Neural Net Training as Spectrum Learning Process
In a dense layer, we focus on the z𝑇𝐴part since it contains most parameters. Neural network is
essentially a function simulator that learns some artificial features, which is achieved by linear
mappings, nonlinear activations, and some other customized units (e.g. recurrent unit). For the
linear mapping, the analysis is usually done on the spectral domain.
Recall that Singular Value Decomposition (SVD) is optimal under both spectral norm [ 14] and
Frobenius norm [32]. The weight matrix 𝐴∈R𝑚×𝑛as a linear operator can be decomposed as
𝐴=𝑈Σ𝑉𝑇=𝑚𝑖𝑛(𝑚,𝑛)∑︁
𝑖=1𝜎𝑖u𝑖v𝑇
𝑖
where𝑈=[u1,u2,...,u𝑚]∈R𝑚×𝑚is the left singular matrix, 𝑉=[v1,v2,...,v𝑛]∈R𝑛×𝑛is the
right singular matrix and and Σcontains the singular values 𝑑𝑖𝑎𝑔(𝜎1,𝜎2,...,𝜎𝑛)in a non-increasing
order.
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 4 ---
111:4 Shibo Yao, Dantong Yu, and Ioannis Koutis
Note that a input signal z∈R𝑚can be written into a linear combination of u𝑖, i.e.z𝑖=Í𝑚
𝑖𝑐𝑖u𝑖
where𝑐𝑖are the coefficients. Thus, the mapping from z∈R𝑚toz′∈R𝑛is
z′=z𝑇𝐴=𝑚∑︁
𝑖=1𝑐𝑖u𝑇
𝑖𝑚𝑖𝑛(𝑚,𝑛)∑︁
𝑗=1𝜎𝑗u𝑗v𝑇
𝑗=𝑚𝑖𝑛(𝑚,𝑛)∑︁
𝑗=1𝑐𝑗𝜎𝑗v𝑗
where𝜎𝑗are non-increasingly ordered, since u𝑇
𝑖u𝑗=0,∀𝑖≠𝑗andu𝑇
𝑖u𝑖=1,∀𝑖.
We are especially interested in finding out how the spectrum Σ, i.e., singular values, of the linear
mapping evolve during neural network training. As a matter of fact, our empirical study shows
that the neural network training process is a spectrum learning process. To investigate the spectral
structure, we design the following task:
Task 1 : check the spectra and norms of dense layer weight matrices and how they change during
neural network training.
0 2 4 6 81.501.551.601.651.702-normconv1.weight
0 2 4 6 81.01.52.02.53.0conv2.weight
0 2 4 6 81234567fc1.weight
0 2 4 6 81234fc2.weight
0 2 4 6 83.303.353.403.45f-norm
0 2 4 6 85678
0 2 4 6 8101520
0 2 4 6 8
epoch23456
Fig. 1. (Task1) Matrix Norms in LeNet stabilize during training. y-axis denotes matrix norm and x-axis denotes
training epoch.
0 50 100 150345672-normconv1.weight
0 50 100 1502468conv2.weight
0 50 100 150234567conv3.weight
0 50 100 1502345conv4.weight
0 50 100 15045678910f-norm
0 50 100 1506810121416
0 50 100 150810121416
0 50 100 150
epoch10121416
Fig. 2. (Task1) Matrix Norms in VGG19 stabilize during training.
3.2 Neural Net Pruning as Spectrum Preserving Process
Following the logic, if the neural network training is a spectrum learning process, when we practise
neural network pruning, we would like to preserve the spectrum in order to preserve the neural
network performance. In other words, we want to obtain a sparse ˜𝐴that has similar singular
values to𝐴. How to measure the wellness of spectrum preservation? We can use the spectral norm
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 5 ---
Neural Network Pruning as Spectrum Preserving Process 111:5
(2-norm)∥𝐴∥2=𝜎1which is the largest singular value since we care about the dominant principle
component, and the Frobenius norm (F-norm)
∥𝐴∥𝐹=(𝑚∑︁
𝑖=1𝑛∑︁
𝑗=1𝐴2
𝑖𝑗)1/2=(𝑇𝑟(𝐴𝑇𝐴))1/2=(𝑚𝑖𝑛(𝑚,𝑛)∑︁
𝑖=1𝜎2
𝑖)1/2
which is usually considered an aggregation of the whole spectrum. Note that ∥𝐴∥2≤∥𝐴∥𝐹and
∥𝐴∥𝐹≤√︁
𝑚𝑖𝑛(𝑚,𝑛)∥𝐴∥2.
Therefore the goal is to find a sparse ˜𝐴such that∥𝐴−˜𝐴∥2≤𝜖or∥𝐴−˜𝐴∥𝐹≤𝜖. To show that
the pruning process is a spectrum preserving process, we design the following task.
Task2 : apply magnitude-based thresholding at different sparsity levels and check the relationship
between the resulting weight matrix spectra and the performance of the pruned neural networks.
3.54.04.55.05.596.597.097.598.098.599.0fc12-norm
12141618202296.597.097.598.098.599.0f-norm
1.5 2.0 2.55060708090100conv2
5 6 7 85060708090100
Fig. 3. (Task2)∥𝐴−˜𝐴∥2and∥𝐴−˜𝐴∥𝐹V.S. neural network accuracy as the sparsity increases (LeNet on
MNIST). As we increase sparsity, the 2-norm∥𝐴−˜𝐴∥2and the𝐹-norm∥𝐴−˜𝐴∥𝐹increase while the neural
network performance drops accordingly. y-axis denotes accuracy and x-axis denotes matrix norm.
3.3 Iterative Pruning and Retraining
From the optimization perspective, the pretrained neural network achieved satisfying local optimum.
Once the weight matrix is sparsified, the spectrum to some extent deviates and the optimality no
longer holds. We wish to retrain the neural network such that it returns to the satisfying local
optimum and the performance is preserved. Apparently we don’t want the neural net deviate from
the local optimum too far, otherwise it would be difficult to return to the optimum due to the
nonconvex optimization process of neural network. Hence, iterative pruning and retraining has
been a common practice for neural network pruning. We also design a task to elaborate from the
spectrum perspective.
Task 3: Inspect the weight matrix spectra in each pruning iteration, i.e. the weight matrix spectra
after pruning and after retraining respectively, and compare the corresponding neural network
performances. We also include a comparison to one-shot pruning.
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 6 ---
111:6 Shibo Yao, Dantong Yu, and Ioannis Koutis
0.5 1.0 1.5 2.060708090conv12-norm
2 3 4 560708090f-norm
0.500.751.001.251.501.75405060708090conv2
2 3 4 5 6405060708090
1.001.251.501.752.002.2560708090conv3
6 7 8 960708090
1.501.752.002.252.502.75405060708090conv4
7 8 910 11405060708090
Fig. 4. (Task2)∥𝐴−˜𝐴∥2and∥𝐴−˜𝐴∥𝐹V.S. neural network accuracy as sparsity increases (VGG19 on CIFAR10).
The axis labels are the same as those in Figure 3.
4 MATRIX SPARSIFICATION ALGORITHMS
In the previous section, we identify the relationship between neural network pruning (removing
parameters/connections) and the spectrum preserving process. Matrix sparsification plays a primary
role in the spectrum preserving process and will guide the network pruning process. In this section,
we present the practical techniques of matrix sparsification.
4.1 Magnitude Based Thresholding
Magnitude-based neural network pruning have attracted a lot of attention and show supprisingly
simplicity and superior efficacy. In the context of matrix sparsification, this is a straightforward
approach, namely magnitude-based matrix sparsification or hard thresholding. Given a matrix 𝐴,
let˜𝐴denote its sparsifier. Entry-wise we have
˜𝐴𝑖𝑗=(
𝐴𝑖𝑗|𝐴𝑖𝑗|>𝑡
0𝑒𝑙𝑠𝑒
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 7 ---
Neural Network Pruning as Spectrum Preserving Process 111:7
0 50 1000246fc1.weight100.0
0 50 10030.0
0 50 1009.0
0 50 1002.7
0 50 100one shot 2.7
0 25 500123conv2.weight
0 25 50
 0 25 50
 0 25 50
 0 25 50
λi
Fig. 5. (Task3) Iterative pruning for LeNet on MNIST. The number above each column denotes the percentage
of parameter left in a weight matrix. The right-most are the one-shot pruning results. We can see that the
spectra deviate after pruning (orange) and tend to recover after retraining (blue). Once the sparsity reaches a
certain point, the spectrum collapses which corresponds to model performance significant drop.
0 50 100024fc1.weight100.0
0 50 10030.0
0 50 1009.0
0 50 1002.7
0 50 100one shot 2.7
0 25 500123conv2.weight
0 25 50
 0 25 50
 0 25 50
 0 25 50
λi
Fig. 6. (Task3) Iterative pruning for LeNet with Batch Normalization on MNIST. Comparing to LeNet without
Batch Normalization, the spectra recovery behaviour is insignificant.
.
Fact. Magnitude based thresholding always achieves sparsification optimality in terms of F-norm.
The fact can be trivially verified since using |𝐴𝑖𝑗|and using𝐴2
𝑖𝑗(on which F-norm is based) are
equivalent in terms of deciding small entries in a matrix. However throwing away small entries does
not always guarantee the optimal sparsification result in terms of 2-norm. And in many situations,
we care more about the dominant singular value instead of the whole spectrum.
4.2 Randomized Algorithms
In randomized matrix sparsification, each entry is sampled according to some distribution indepen-
dently and then rescaled. E.g. each entry is sampled according to a Berboulli distribution, and we
either set it to zero or rescale it.
˜𝐴𝑖𝑗=(
𝐴𝑖𝑗/𝑝𝑖𝑗𝑝𝑖𝑗
0 1−𝑝𝑖𝑗
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 8 ---
111:8 Shibo Yao, Dantong Yu, and Ioannis Koutis
where𝑝𝑖𝑗can be a constant or positively correlated to the magnitude of the entry. The following
theorem provides the justification to this type of matrix sparsification.
Theorem 4.1. A matrix where each entry is sampled from a zero-mean bounded-variance distribu-
tion possesses weak spectrum with large probability.
By weak spectrum, it means small matrix norm. To be more concrete, since matrix norm is a
metric and triangle inequality applies, we have
∥𝐴∥≤∥ ˜𝐴∥+∥𝐴−˜𝐴∥
. We need to show that 𝑁=𝐴−˜𝐴falls within the category of matrices described in Theorem 4.1.
Since
𝐸(𝑁𝑖𝑗)=𝐸(𝐴𝑖𝑗−˜𝐴𝑖𝑗)=𝐴𝑖𝑗−𝐴𝑖𝑗/𝑝𝑖𝑗·𝑝𝑖𝑗=0
and
𝑣𝑎𝑟(𝑁𝑖𝑗)=𝑣𝑎𝑟(˜𝐴𝑖𝑗)=(𝐴𝑖𝑗/𝑝𝑖𝑗)2·𝑝𝑖𝑗=𝐴2
𝑖𝑗/𝑝𝑖𝑗
, as long as𝐴2
𝑖𝑗/𝑝𝑖𝑗is upper-bounded, which is true most of time, 𝑣𝑎𝑟(𝑁𝑖𝑗)is bounded. Therefore
the randomized matrix sparsification can guarantee the error bound.
4.3 Customize Matrix Sparsification Algorithm for Neural Network Pruning
In this section, we propose a customized matrix sparsification algorithm to show the potential of
designing a better spectrum preservation process in neural network pruning. We do not intend
to present a new state-of-the-art neural network pruning algorithm. There are two important
points in our proposed algorithm: truncation and sampling based on the principal components of
explicit truncated SVD. To the best of our knowledge, sampling based on probability proportional
to principal components is employed for the first time in designing matrix sparsification for neural
network pruning .
First, we adopt the truncation trick that is common in existing work. As clearly pointed out by
[1], the spectrum of the random matrix 𝑁=𝐴−˜𝐴is determined by its variance bound. Usually, the
larger the variance, the stronger the spectrum of the random matrix. Existing works took advantage
of the finding and proposed truncation [ 4][12] in sparsification, i.e. to set small entries to zero
while leaving large entries as is and sampling on the remaining ones.
˜𝐴𝑖𝑗= 
𝐴𝑖𝑗 |𝐴𝑖𝑗|>𝑡
0 𝑝𝑖𝑗<𝑐
𝐴𝑖𝑗/𝑝𝑖𝑗·𝐵𝑒𝑟𝑛(𝑝𝑖𝑗)𝑒𝑙𝑠𝑒
where𝑝𝑖𝑗∝|𝐴𝑖𝑗|,𝑡is decided by the quantile (leave large entries as is), and 𝑐, the lower threshold
for zeroing weights, as a constant could be set manually, and 𝐵𝑒𝑟𝑛(·)denotes Bernoulli distribution.
Second, instead of sampling based on the probability calculated from the magnitude of the
original matrix entry, we do sampling based on the probability calculated from the principal
component matrix entry magnitude with a little compromise on complexity, in order to better
preserve the dominant singular values. Matrix sparsification was originally proposed for fast
low-rank approximation on very large matrices, due to the fact that sparsity accelerates matrix-
vector multiplication in power iteration. Essentially, we desire to find the sparse sketch ˜𝐴of
𝐴that preserves the dominant singular values well. This coincides with the goal of layer-wise
neural network pruning from the spectrum preserving viewpoint – we desire to preserve the
dominant singular values, based on the fact that we often consider information lies in the low-
frequency domain while noises are in the high-frequency domain. The major difference is that
weight matrices in neural network, either from dense layers or convolutional layers, are usually
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 9 ---
Neural Network Pruning as Spectrum Preserving Process 111:9
not too large, and therefore explicit SVD or truncated SVD on them is fairly affordable. Once we
have access to the principle components of the weight matrices, we are able to preserve them
better in the sparsification process. Note that preserving dominant singular values is a harmonic
approach between preserving the 2-norm and the F-norm, since ∥𝐴∥2=lim𝑝→∞(Í
𝑖𝜎𝑝
𝑖)1/𝑝and
∥𝐴∥𝐹=lim𝑝→2(Í
𝑖𝜎𝑝
𝑖)1/𝑝.
The crutial part is to find the low-rank approximation 𝐵to𝐴, where𝐵=Í𝐾
𝑖=1𝜎𝑖u𝑖v𝑇
𝑖and𝜎𝑖u𝑖v𝑇
𝑖
are from SVD on A. We set the entry-wise sampling probability based on |𝐵𝑖𝑗|, i.e.𝑝𝑖𝑗∝|𝐵𝑖𝑗|. Alg 1
presents the sparification algorithm. The partition function is the one used in quicksort .
Algorithm 1: Sparsify
input : (𝐴,𝑐,𝑞,𝐾)
output: ˜𝐴
; //𝑞: quantile above which remain unchanged
1𝐵=truncated-SVD( 𝐴,𝐾); ; //𝐵: low-rank approx to 𝐴
2𝑚,𝑛 = shape of𝐵;
3𝑡= partition({|𝐵𝑖𝑗|}, int(𝑚×𝑛×𝑞));
4for𝑖←1to𝑚do
5 for𝑗←1to𝑛do
6 if|𝐵𝑖𝑗|<𝑡then
7 𝑝𝑖𝑗=(𝐵𝑖𝑗/𝑡)2;
8 if𝑝𝑖𝑗<𝑐then
9 𝐴𝑖𝑗=0;
10 else
11 𝐴𝑖𝑗=𝐴𝑖𝑗/𝑝𝑖𝑗·𝐵𝑒𝑟𝑛(𝑝𝑖𝑗);
12 end
13end
0.6 0.7 0.8 0.920406080AccuracyK=2
0.700.750.800.850.900.9520406080K=4
0.75 0.80 0.85 0.90 0.9520406080K=6
0.75 0.80 0.85 0.90 0.95
sparsity20406080K=8
Fig. 7. (Task4) Pruned Network Testing Performance given by Magnitude-based Thresholding (orange) v.s.
Algorithm1 (blue). The larger 𝑞, the larger sparsity.
We need to demonstrate that the proposed sparsification algorithm preserves dominant singular
values better and improves the generalization performance of the pruned network. We propose the
following task to check whether it makes improvement based on our analysis.
Task 4 Apply the above algorithm on VGG19 layer-wise weight matrix sparsification, compare the
generalization performance of the pruned network and the pruned network given by thresholding
at the same sparsity level.
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 10 ---
111:10 Shibo Yao, Dantong Yu, and Ioannis Koutis
Here we provide a high level proof on sparsification error being upper bounded. Let 𝐷denote the
sparse sketch generated by setting smallest entries in 𝐴to 0 and ˜𝐴as usual the final sparsfied result.
From Fact 4.1 we know that 𝐷is the optimal sketch of 𝐴in terms of F-norm, i.e. ∥𝐴−𝐷∥𝐹=𝜖∗.
Based on Theorem 4.1 and its illustration we know that 𝑁=˜𝐴−𝐷satisfies the zero-mean and
bounded-variance condition. Hence ∥˜𝐴−𝐷∥𝐹≤𝜖0. Therefore if we apply triangle equality given
matrix norm is a metric,
∥𝐴−˜𝐴∥𝐹≤∥𝐴−𝐷∥𝐹+∥𝐷−˜𝐴∥𝐹≤𝜖∗+𝜖0.
Some other techniques, e.g. quantization[ 17][19], can be used together with sparsification to further
compress matrices and neural networks. Essentially they are also spectrum preservation techniques
[3][4].
5 GENERALIZATION TO CONVOLUTION
HW
KK
O
CKKCWH
OK
K
KKC
Fig. 8. Convolution as Dense Matrix Multiplication
Extensive literatures argue that convolutional layers compression can be formalized as tensor
algebra problems [ 26][11] [23][30][37]. However, it’s advantageous to explain convolutional layer
pruning from the matrix viewpoint since the linear algebra have many nice properties that do not
hold for multilinear algebra. We want to ask: can we still provide theoretical support to convolutional
layer pruning using linear algebra we have discussed so far?
5.1 Pruning on Convolutional Filters
In this section we state and illustrate the following fact.
Fact. Discrete convolution in neural networks can be represented by dot product between two dense
matrices.
To see this, suppose we have a convolutional layer with input signal size of 𝑊×𝐻as width by
height, and with 𝐶input channels and 𝑂output channels. Here we consider a 2-d convolution on
the signal. The kernel is of size 𝐶×𝐾×𝐾and there are 𝑂such kernels. For the sake of simplicity in
notations, suppose the striding step is 1, half-padding is applied and there is no dilation (for even 𝑊
and𝐻the above setting results in output signal of size 𝑊×𝐻as width by height). 2-d convolution
means that the kernel is moving in two directions. Fact 5.1 has been utilized to optimize lower-level
implementation of CNN on hardware [ 6][10]. Here we take advantage of the idea to unify neural
network pruning on dense layers and convolutional layers with matrix sparsification.
Let us focus on one single output channel, one step of the convolution operation is the summation
of element-wise product of two higher-order array, i.e. the kernel 𝐺∈R𝐶×𝐾×𝐾and the receptive
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 11 ---
Neural Network Pruning as Spectrum Preserving Process 111:11
field of the signal of the same size 𝑋∈R𝐶×𝐾×𝐾. Note that taking the summation of element-wise
product is equivalent to vector inner product. Therefore if we unfold the kernel for a single output
channel to a vector and rearrange the receptive field of the signal accordingly to another vector,
a single convolution step can be treated as two vector inner product, i.e. 𝐺∗𝑋=g𝑇xwhere
g,x∈R𝐶𝐾𝐾. Since we have 𝑂output channels in total, there are 𝑂such kernels of the same
size. All of them being unfolded, we then can convert the convolution into a matrix product 𝑍𝑇𝐴,
where𝐴∈R𝐶𝐾𝐾×𝑂being the kernels and 𝑍∈R𝐶𝐾𝐾×𝑊𝐻being the rearranged input signals.
And consequently the output signal 𝑌∈R𝑊𝐻×𝑂(as mentioned before, stride 1, half padding and
no dilation result in input signal and output signal being in the same shape). Figure 8 visualizes
convolution as matrix multiplication.
The matrix multiplication representation of convolution discussed above generalizes to any other
convolution settings. Also note that the way we unfold the filters does not affect the spectrum of the
resulting matrix, since row and column permutations do not change matrix spectrum. Therefore,
all the analyses based on simple linear algebra we have discussed so far generalize to convolutional
layer pruning. To verify our analyses,
Task1,2,3 will also be conducted on convolutional layers. When we say weight matrix in a
context of convolution, it refers to as the matrix unfolded from the convolution higher-order array
in the way described in Figure 8.
5.2 Convolutional Filter Channel Pruning
02468105.96.06.16.26.3conv1
68101214167.827.847.867.887.907.927.94conv2
10 15 20 2510.4610.4810.5010.5210.5410.56conv3
20 25 30 3512.3412.3612.3812.40conv4
Fig. 9. (Task5) VGG19 Channel Pruning based onÍ
𝑖|𝑇𝑖|. y-axis denotes∥˜𝐴∥𝐹and x-axis denotesÍ
𝑖|𝑇𝑖|. The
smallerÍ
𝑖|𝑇𝑖|, the smaller∥𝐴−˜𝐴∥𝐹, the larger∥˜𝐴∥𝐹, the better spectra are preserved. Hence our analysis
bridges the gap between smallÍ
𝑖|𝑇𝑖|and good neural network performance preservation.
Entry-wise pruning almost always results in unstructured sparsity that requires specific data
structure design in network deployment in order to realize the complexity reduction from pruning.
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 12 ---
111:12 Shibo Yao, Dantong Yu, and Ioannis Koutis
Therefore it’s desirable to prune entire channels from convolutional layers to achieve higher
efficiency. There is another important work on pruning channels [ 29]. The approach is to take
smallÍ
𝑖𝑗𝑘|𝑇𝑖𝑗𝑘|where𝑇denotes the filter for a specific channel. This is equivalent to remove a
column in𝐴we just discussed with small-magnitude values. It’s also a spectrum preserving process
asÍ
𝑖𝑗𝑘|𝑇𝑖𝑗𝑘|is fairly a proximity toÍ
𝑖𝑗𝑘(𝑇𝑖𝑗𝑘)2on which the F-norm is based. Hence pruning the
whole filter with smallÍ
𝑖𝑗𝑘|𝑇𝑖𝑗𝑘|is to preserve the F-norm of the convolution matrix we discussed
in the previous subsection. To check the relation betweenÍ
𝑖𝑗𝑘|𝑇𝑖𝑗𝑘|and the F-norm of ˜𝐴, we
propose the following task.
Task5 Pruning different filters and check the relationship betweenÍ
𝑖𝑗𝑘|𝑇𝑖𝑗𝑘|and the resulted
convolution matrix F-norm ∥˜𝐴∥𝐹.
6 EMPIRICAL STUDY DETAILS
In this section, we present the proposed task details and results. The experiments are mainly
based on LeNet [ 27] on MNIST and VGG19 [ 36] on CIFAR10 dataset [ 25]. We trained the neural
networks from scratch based on the official PyTorch [ 34] implementation. Then we conducted our
experiments based on the pre-trained neural networks.
We trained LeNet with a reduced number of epochs of 10. All other hyperparameter settings
are the ones used in the original implementation of the PyTorch example. The VGG19 was trained
with the following hyperparameter setting: batch size 128, momentum 0.9, weight decay 5𝑒−4, and
the learning rates of 0.1 for 50 epochs, 0.01 of 50 epochs, and 0.001 of another 50 epochs. The final
testing accuracy for LeNet on MNIST and VGG19 on FICAR10 was 99.14% and 92.66%, respectively.
We saved the layer weight matrices for each epoch of training, including two fully connected layers
and two convolutional layers for LeNet and the first four convolutional layers for VGG19 (due to
the limitation in the reporting space). We then study the evolvement of 2-norm and F-norm of
weight matrices during training.
In Figures 1 and 2, we observe that the 2-norm and F-norm of a particular weight matrix change
fast at the beginning of training and tend to stabilize as training proceeds. This observation provides
concrete evidence that network training is essentially a spectrum learning process. Note that the
initial spectrum is not necessarily flat (see Figures 10 and 11 in Appendix), but rather depends
on the initialization. The stabilization also has its explanation from the optimization perspective:
as training goes on we are trapped into a satisfying local optimum and the gradients are almost
zero for layers when chain rule applied, which means the weight matrices are not being updated
significantly.
Task 2 We investigated the relationship between spectrum preservation and the performance of
the pruned neural network. Matrix sparsification (hard thresholding) was employed to prune neural
networks. We varied the percentage of parameter preservation from 20% to 1% to get different
sparsities (the sparser, the larger ∥𝐴−˜𝐴∥2and∥𝐴−˜𝐴∥𝐹). We pruned different layers in the
pre-trained LeNet and VGG19 and checked their performance without retraining.
Figure 3 and figure 4 show that when ∥𝐴−˜𝐴∥2increases, the neural network performance
deteriorates almost monotonically. It is also true for ∥𝐴−˜𝐴∥𝐹. The finding confirms that the better
the spectrum of weight matrix is preserved during pruning, the better the performance of the
pruned neural network performance is conserved. It is also interesting to note that the experiment
observations (data points) tend to concentrate on the upper left side, rather than the lower right
of plots, and there is a drop-off in each plot. The pattern indicates that the spectrum and neural
network performance collapse when the sparsity of the matrix sketch goes beyond a certain point.
Task 3 We also inspected the spectra of the weight matrices during the iterative pruning
and retraining. Specifically, we adopted the magnitude-based pruning (hard thresholding matrix
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 13 ---
Neural Network Pruning as Spectrum Preserving Process 111:13
sparsification) to retain 30% parameters in each iteration. We conducted experiments on the second
convolutional layer and the first dense layer which contain most parameters in LeNet. Once the
parameters were pruned, we applied a mask on that certain matrix to fix the zero values which
correspond to the pruned connections between neurons. We also applied masks on all other layers
during retraining in order to get a better assess on certain weight matrix spectrum change due to
sparsification. The retraining process was done with two additional epochs.
Figure 5 shows a clear pattern of how the spectrum of a weight matrix evolves once we iteratively
prune it. The first plot in each row is the spectrum of the original weight matrix. The orange dots
denote the spectra of the weight matrices after each round of pruning, and the blues dots represent
the spectra after retraining in each iteration. In the beginning, we can prune a large number of
parameters from the weight matrix without altering the spectrum significantly, and we still can
recover the spectrum to some extent by retraining. When the sparsity reaches a certain point, the
spectrum seems to collapse and fail to recover its original shape and position even with retraining,
which is consistent with the general observation neural network performance significantly drops
when it’s too sparse.
Such a spectrum recovery behaviour is negligibly insignificant on VGG19. The spectra deviate
from the original ones in pruning yet are rather slightly modified during retraining. This is mainly
due to the widely adopted Batch Normalization[ 22], which rescales training batches to zero-mean
unit-variance batches and hence significantly eliminates the need for shaping the matrix spectra. In
the beginning, we managed to remove the batch normalization that comes after each convolutional
layer in VGG19. However, this leads to a difficult training situation for deep neural networks, as
the community is widely aware of [ 22]. Hence, we adopted an alternative approach to demonstrate
batch normalization effect on the learning process of spectra. We added batch normalization after
each convolutional layer in LeNet, repeated the experiment aforementioned, and compared the
results. For a detailed treatment of batch normalization, please refer to[22][5].
In Figure 6, we observe that when batch normalization is applied, the spectrum recovery is less
significant than that of without batch normalization, although both the trajectory and the ending
status of the spectra for iterative pruning and one-shot pruning surprisingly reassemble to each
other.
Task 4 We applied algorithm1 on all convolutional layers in VGG19 at the same time, varied
algorithm settings to get different sparsities, recorded the corresponding testing performance of
the pruned network, and compared with the performance of the pruned network via thresholding
at the same sparsity level. Due to the randomness in our proposed algorithm, the sparsity in
different layers is also different. We present the aggregated sparsity, i.e. the total number of nonzero
parameters divided by total number of parameters in all convolutional weight matrices, in our
empirical study result. To ease the implementation and focus on our arguments, we fixed parameter
𝑐=0.5, varied the quantile parameter 𝑞and the number of principal components 𝐾.
From Figure 7 we can see that, our proposed algorithm almost always leads to better pruned
network generalization performance without retraining compared to that given by thresholding,
at different sparsity levels. This demonstrates the potential of designing and customizing matrix
sparsification algorithms for better neural network pruning approaches. In addition, we also
observed Alg 1 almost always yield smaller sparsification error compared to thresholding in terms
of 2-norm, which is exactly the motivation of the algorithm design.
Task 5 To support the channel pruning mechanism interpretation, we inspected the relation
betweenÍ
𝑖|𝑇𝑖|and∥˜𝐴∥𝐹. For this task, we checked the first four convolutional layers in VGG19.
Based on our illustration on convolutional layer as dense matrix multiplication, we unfolded each
channel filter from a 3rd-order array to a vector, and then concatenated all the channel vectors into
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 14 ---
111:14 Shibo Yao, Dantong Yu, and Ioannis Koutis
a matrix. Therefore removing a filter channel is equivalent to removing a column in the so called
convolution matrix in figure 8. We have shown the relation between spectrum preservation and
neural network performance preservation. As long as we can show the relation betweenÍ
𝑖|𝑇𝑖|
and∥˜𝐴∥𝐹, we can bridge the gap and associateÍ
𝑖|𝑇𝑖|and neural network performance.
Figure 9 shows that more or less ∥˜𝐴∥𝐹of a convolution matrix is negatively correlated toÍ
𝑖|𝑇𝑖|,
which is consistent to our analysis.
7 CONCLUSION AND FUTURE WORK
In this work, we argue that neural network training has a strong interdependence with spectrum
learning. The relationship provides neural network pruning with a theorical formulation based on
spectrum preserving, to be more specific, matrix sparsification. We reviewed the existing primary
efforts on neural network pruning and proposed a unified viewpoint for both dense layer and
convolutional layer pruning. We also designed and conducted experiments to support the arguments,
and hence provided more interpretability to deep learning related topic.
We anticipate that the superior algorithm design for neural network pruning rests upon the
effective and efficient matrix sparsification using spectral theory and lower-level implementation
of sparse neural network for the targeted systems. For future works, we will undertake further
investigation on how activation functions affect pruning and more pruning algorithms on other
types of network layers.
REFERENCES
[1]Dimitris Achlioptas, Zohar Karnin, and Edo Liberty. 2013. Matrix entry-wise sampling: Simple is best. Submitted to
KDD 2013, 1.1 (2013), 1–4.
[2]Dimitris Achlioptas, Zohar S Karnin, and Edo Liberty. 2013. Near-optimal entrywise sampling for data matrices. In
Advances in Neural Information Processing Systems . 1565–1573.
[3]Dimitris Achlioptas and Frank McSherry. 2007. Fast computation of low-rank matrix approximations. Journal of the
ACM (JACM) 54, 2 (2007), 9.
[4]Sanjeev Arora, Elad Hazan, and Satyen Kale. 2006. A fast random sampling algorithm for sparsifying matrices. In
Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques . Springer, 272–279.
[5]Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. 2018. Understanding batch normalization. In
Advances in Neural Information Processing Systems . 7694–7705.
[6]Kumar Chellapilla, Sidd Puri, and Patrice Simard. 2006. High performance convolutional neural networks for document
processing.
[7]Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. 2015. Compressing neural networks
with the hashing trick. In International conference on machine learning . 2285–2294.
[8]Wenlin Chen, James Wilson, Stephen Tyree, Kilian Q Weinberger, and Yixin Chen. 2016. Compressing convolutional
neural networks in the frequency domain. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining . 1475–1484.
[9]Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2017. A survey of model compression and acceleration for deep
neural networks. arXiv preprint arXiv:1710.09282 (2017).
[10] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan
Shelhamer. 2014. cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759 (2014).
[11] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. 2014. Exploiting linear structure within
convolutional networks for efficient evaluation. In Advances in neural information processing systems . 1269–1277.
[12] Petros Drineas and Anastasios Zouzias. 2011. A note on element-wise matrix sparsification via a matrix-valued
Bernstein inequality. Inform. Process. Lett. 111, 8 (2011), 385–389.
[13] Mengnan Du, Ninghao Liu, and Xia Hu. 2019. Techniques for interpretable machine learning. Commun. ACM 63, 1
(2019), 68–77.
[14] Carl Eckart and Gale Young. 1936. The approximation of one matrix by another of lower rank. Psychometrika 1, 3
(1936), 211–218.
[15] Jonathan Frankle and Michael Carbin. 2018. The lottery ticket hypothesis: Finding sparse, trainable neural networks.
arXiv preprint arXiv:1803.03635 (2018).
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 15 ---
Neural Network Pruning as Spectrum Preserving Process 111:15
[16] Alex Gittens and Joel A Tropp. 2009. Error bounds for random matrix approximation schemes. arXiv preprint
arXiv:0911.4108 (2009).
[17] Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. 2014. Compressing deep convolutional networks using
vector quantization. arXiv preprint arXiv:1412.6115 (2014).
[18] Lee-Ad Gottlieb and Tyler Neylon. 2010. Matrix sparsification and the sparse null space problem. In Approximation,
Randomization, and Combinatorial Optimization. Algorithms and Techniques . Springer, 205–218.
[19] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015).
[20] Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights and connections for efficient neural
network. In Advances in neural information processing systems . 1135–1143.
[21] Babak Hassibi and David G Stork. 1993. Second order derivatives for network pruning: Optimal brain surgeon. In
Advances in neural information processing systems . 164–171.
[22] Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167 (2015).
[23] Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. 2015. Compression of deep
convolutional neural networks for fast and low power mobile applications. arXiv preprint arXiv:1511.06530 (2015).
[24] Tamara G Kolda and Brett W Bader. 2009. Tensor decompositions and applications. SIAM review 51, 3 (2009), 455–500.
[25] Alex Krizhevsky, Geoffrey Hinton, et al .2009. Learning multiple layers of features from tiny images . Technical Report.
Citeseer.
[26] Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky. 2014. Speeding-up convolu-
tional neural networks using fine-tuned cp-decomposition. arXiv preprint arXiv:1412.6553 (2014).
[27] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al .1998. Gradient-based learning applied to document
recognition. Proc. IEEE 86, 11 (1998), 2278–2324.
[28] Yann LeCun, John S Denker, and Sara A Solla. 1990. Optimal brain damage. In Advances in neural information processing
systems . 598–605.
[29] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. 2016. Pruning filters for efficient convnets.
arXiv preprint arXiv:1608.08710 (2016).
[30] Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. 2015. Sparse convolutional neural
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 806–814.
[31] S Thomas McCormick. 1983. A Combinatorial Approach to Some Sparse Matrix Problems. Technical Report. STANFORD
UNIV CA SYSTEMS OPTIMIZATION LAB.
[32] L. Mirsky. 1960. Symmetric gauge functions and unitarily invariant norms. QJ Math., Oxf. II. Ser. 11 (1960), 50–59.
https://doi.org/10.1093/qmath/11.1.50
[33] NH Nguyen, Petros Drineas, and TD Tran. 2009. Matrix sparsification via the Khintchine inequality. (2009).
[34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,
Natalia Gimelshein, Luca Antiga, et al .2019. PyTorch: An imperative style, high-performance deep learning library. In
Advances in Neural Information Processing Systems . 8024–8035.
[35] Wojciech Samek, Thomas Wiegand, and Klaus-Robert Müller. 2017. Explainable artificial intelligence: Understanding,
visualizing and interpreting deep learning models. arXiv preprint arXiv:1708.08296 (2017).
[36] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 (2014).
[37] Yi Sun, Xiaogang Wang, and Xiaoou Tang. 2016. Sparsifying neural network connections for face recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 4856–4864.
[38] M Alex O Vasilescu and Demetri Terzopoulos. 2002. Multilinear analysis of image ensembles: Tensorfaces. In European
Conference on Computer Vision . Springer, 447–460.
[39] Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. 2019. Deconstructing lottery tickets: Zeros, signs, and the
supermask. arXiv preprint arXiv:1905.01067 (2019).
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 16 ---
111:16 Shibo Yao, Dantong Yu, and Ioannis Koutis
A SUPPLEMENTARY MATERIALS FOR REPRODUCIBILITY
A.1 python code for Alg1
def lowRankSampling(A, tao=0.3, n_comp=5):
A_ = np.array([i.flatten() for i in A])
U,sigma,V = randomized_svd(A_, n_comp)
B = np.zeros(A_.shape)
for i in range(n_comp):
B += np.outer(U[:,i],V[i])*sigma[i]
flat_B = abs(B.flatten())
thresh_B = int(flat_B.size*tao)-1
t_B = np.partition(flat_B, thresh_B)[thresh_B]
with np.nditer(A, op_flags=["readwrite"]) as it:
for x in it:
if abs(x) < t_B:
p = np.square(x/t_B)
if p < 0.5:
x[...] = 0
else:
x[...] = np.random.binomial(1,p,1) * x / p
return A
A.2 Additional Empirical Study Results
0 50.51.01.5conv1.weightepoch 1 acc 10.17
0 5epoch 2 acc 97.96
0 5epoch 3 acc 98.88
0 5epoch 4 acc 98.98
0 5epoch 5 acc 99.02
0 5epoch 6 acc 99.12
0 5epoch 7 acc 99.17
0 5epoch 8 acc 99.11
0 5epoch 9 acc 99.17
0 5epoch 10 acc 99.15
0 25 50123conv2.weight
0 25 50
 0 25 50
 0 25 50
 0 25 50
 0 25 50
 0 25 50
 0 25 50
 0 25 50
 0 25 50
0 50 100246fc1.weight
0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
0 51234fc2.weight
0 5
 0 5
 0 5
 0 5
 0 5
 0 5
 0 5
 0 5
 0 5
Fig. 10. (Task1) Matrix Spectra in LeNet stabilize during training. Each dot denotes a singular value of the
weight matrix.
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- PAGE 17 ---
Neural Network Pruning as Spectrum Preserving Process 111:17
0 10 200246conv1.weightepoch 0 acc 10.71
0 10 20epoch 1 acc 25.39
0 10 20epoch 4 acc 55.59
0 10 20epoch 6 acc 65.93
0 10 20epoch 10 acc 73.85
0 10 20epoch 15 acc 77.68
0 10 20epoch 21 acc 80.83
0 10 20epoch 50 acc 90.8
0 10 20epoch 54 acc 91.91
0 10 20epoch 58 acc 92.03
0 25 5002468conv2.weight
0 25 50
 0 25 50
 0 25 50
 0 25 50
 0 25 50
 0 25 50
 0 25 50
 0 25 50
 0 25 50
0 50 1000246conv3.weight
0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
0 50 1000246conv4.weight
0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
 0 50 100
Fig. 11. (Task1) Matrix Spectra in VGG19 stabilize during training.
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.
