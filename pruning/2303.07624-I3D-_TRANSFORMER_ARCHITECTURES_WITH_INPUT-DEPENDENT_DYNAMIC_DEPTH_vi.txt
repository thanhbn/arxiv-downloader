I3D: KIẾN TRÚC TRANSFORMER VỚI ĐỘ SÂU ĐỘNG PHỤ THUỘC ĐẦU VÀO CHO NHẬN DẠNG TIẾNG NÓI

Yifan Peng1, Jaesong Lee2, Shinji Watanabe1
1Đại học Carnegie Mellon 2Tập đoàn NAVER

TÓM TẮT
Nhận dạng tiếng nói đầu-cuối dựa trên Transformer đã đạt được thành công lớn. Tuy nhiên, kích thước lớn và chi phí tính toán cao khiến việc triển khai các mô hình này trong một số ứng dụng thực tế trở nên khó khăn. Các kỹ thuật nén mô hình có thể giảm kích thước mô hình và tăng tốc suy luận, nhưng mô hình được nén có kiến trúc cố định có thể không tối ưu. Chúng tôi đề xuất một bộ mã hóa Transformer mới với Độ sâu Động phụ thuộc Đầu vào (I3D) để đạt được sự cân bằng hiệu suất-hiệu quả mạnh mẽ. Với số lượng lớp tương tự tại thời điểm suy luận, các mô hình dựa trên I3D vượt trội hơn Transformer vanilla và mô hình cắt tỉa tĩnh thông qua cắt tỉa lớp lặp. Chúng tôi cũng trình bày phân tích thú vị về xác suất cổng và sự phụ thuộc đầu vào, giúp hiểu rõ hơn về các bộ mã hóa sâu.

Từ khóa chỉ mục —Độ sâu động, transformer, nhận dạng tiếng nói

1. GIỚI THIỆU
Gần đây, nhận dạng tiếng nói tự động đầu-cuối (ASR) đã trở nên phổ biến. Các khung điển hình bao gồm Phân loại Thời gian Kết nối (CTC) [1], Bộ mã hóa-Giải mã dựa trên Chú ý (AED) [2–4], và Bộ chuyển đổi Mạng Neural Hồi quy (RNN-T) [5]. Nhiều loại mạng có thể được sử dụng làm bộ mã hóa trong các khung này, như Mạng Neural Tích chập (CNN), RNN, Transformer [6] và sự kết hợp của chúng [7–9]. Transformer đã đạt được thành công lớn trong nhiều điểm chuẩn [10]. Tuy nhiên, chúng thường chứa nhiều khối xếp tầng và do đó có chi phí tính toán cao, cản trở việc triển khai trong một số ứng dụng thực tế với tài nguyên hạn chế. Để giảm tính toán và tăng tốc suy luận, các nhà nghiên cứu đã điều tra các phương pháp khác nhau.

Một phương pháp phổ biến là nén một mô hình lớn được huấn luyện trước bằng cách sử dụng chưng cất [11–13], cắt tỉa [14–16], và lượng tử hóa [15]. Tuy nhiên, mô hình được nén có kiến trúc cố định cho tất cả các loại đầu vào, có thể không tối ưu. Ví dụ, mô hình cố định này có thể quá tốn kém cho các phát ngôn rất dễ nhưng không đủ cho những phát ngôn khó. Để cân bằng tốt hơn giữa hiệu suất và tính toán, các nghiên cứu trước đã khám phá các mô hình động [17], có thể thích ứng kiến trúc của chúng với các đầu vào khác nhau. Các mô hình động đã cho thấy hiệu quả trong thị giác máy tính [18–23], chủ yếu dựa trên CNN. Đối với xử lý tiếng nói, [24] huấn luyện hai bộ mã hóa RNN có kích thước khác nhau và chuyển đổi động giữa chúng được hướng dẫn bởi phát hiện từ khóa. [25] đề xuất một bộ chuyển đổi mã hóa động dựa trên bỏ qua lớp và học tập hợp tác. [26] áp dụng hai bộ mã hóa RNN để xử lý tiếng nói gần và xa. [27] cũng thiết kế hai bộ mã hóa RNN được nén ở các mức độ khác nhau và chuyển đổi giữa chúng trên cơ sở từng khung hình. [28] mở rộng ý tưởng này cho Transformer-transducers và xem xét các mạng con linh hoạt hơn, nhưng vẫn tập trung vào ASR streaming và kiến trúc vẫn được xác định trên cơ sở từng khung hình, đòi hỏi thiết kế đặc biệt cho các phép toán khóa và truy vấn chi tiết trong tự chú ý. Đối với ASR không streaming (hoặc streaming dựa trên chunk), dự đoán mức khung hình có thể tốn kém và không tối ưu, vì nó chỉ nắm bắt các đặc trưng cục bộ mức khung hình.

Chúng tôi đề xuất một bộ mã hóa Transformer với Độ sâu Động phụ thuộc Đầu vào (I3D) cho ASR đầu-cuối. Thay vì sử dụng các phép toán chi tiết được thiết kế cẩn thận trong các mô-đun con như chú ý, I3D dự đoán có nên bỏ qua toàn bộ khối tự chú ý hay toàn bộ mạng feed-forward thông qua một loạt các bộ dự đoán cổng cục bộ hoặc một bộ dự đoán cổng toàn cục duy nhất. Dự đoán được thực hiện ở mức phát ngôn (hoặc mức chunk nếu mở rộng cho các trường hợp streaming), dễ thực hiện hơn và giảm chi phí bổ sung. Nó cũng nắm bắt thống kê toàn cục của đầu vào. Như được phân tích trong Phần 3.4, độ dài của một phát ngôn ảnh hưởng đến kiến trúc suy luận. Một số khối có thể hữu ích cho các đầu vào dài hơn. Kết quả cho thấy các mô hình I3D luôn vượt trội hơn Transformer được huấn luyện từ đầu và các mô hình cắt tỉa tĩnh thông qua cắt tỉa lớp lặp [29], khi sử dụng số lượng lớp tương tự cho suy luận. Chúng tôi cũng thực hiện phân tích thú vị về xác suất cổng được dự đoán và sự phụ thuộc đầu vào, giúp hiểu rõ hơn về hành vi của các bộ mã hóa sâu.

2. PHƯƠNG PHÁP

2.1. Bộ mã hóa Transformer
Một lớp bộ mã hóa Transformer [6] chứa một mô-đun tự chú ý đa đầu (MHA) và một mạng feed-forward (FFN), được kết hợp tuần tự. Chức năng của lớp thứ l như sau:

Y(l)=X(l-1)+MHA(l)(X(l-1)); (1)
X(l)=Y(l)+FFN(l)(Y(l)); (2)

trong đó X(l) là đầu ra của lớp Transformer thứ l và X(l-1) là đầu vào của lớp thứ l. Y(l) là đầu ra của MHA tại lớp thứ l, cũng là đầu vào của FFN tại lớp thứ l. Các chuỗi này đều có độ dài T và kích thước đặc trưng d.

2.2. Kiến trúc tổng thể của bộ mã hóa I3D
Hình 1a cho thấy kiến trúc tổng thể của bộ mã hóa I3D. Một dạng sóng đầu tiên được chuyển đổi thành một chuỗi đặc trưng bởi một frontend, và được xử lý thêm và giảm mẫu bởi một CNN, sau đó các embedding vị trí được thêm vào. Sau đó, chuỗi được xử lý bởi một chồng N lớp mã hóa I3D để tạo ra các đặc trưng cấp cao. Thiết kế tổng thể này tuân theo của Transformer. Tuy nhiên, Transformer luôn sử dụng một kiến trúc cố định bất kể đầu vào. I3D của chúng tôi chọn các kết hợp khác nhau của MHA và FFN tùy thuộc vào phát ngôn đầu vào.

Để xác định có nên thực thi hay bỏ qua một mô-đun, một cổng nhị phân được giới thiệu cho mỗi mô-đun MHA hoặc FFN. Chức năng của lớp thứ l (xem Phương trình (1) và (2) cho Transformer vanilla) bây giờ trở thành:

Y(l)=X(l-1)+g(l)_MHA*MHA(l)(X(l-1)); (3)
X(l)=Y(l)+g(l)_FFN*FFN(l)(Y(l)); (4)

trong đó g(l)_MHA, g(l)_FFN ∈ {0,1} là các cổng phụ thuộc đầu vào. Nếu một cổng được dự đoán là 0, thì mô-đun tương ứng sẽ bị bỏ qua, điều này hiệu quả giảm tính toán. Tổng mất mát huấn luyện là:

L_total = L_ASR + λ*L_utility; (5)
L_utility = 1/(2N) * Σ(l=1 to N)[g(l)_MHA + g(l)_FFN]; (6)

trong đó L_ASR là mất mát ASR tiêu chuẩn và L_utility là mất mát chính quy đo tỷ lệ sử dụng của tất cả các mô-đun MHA và FFN. λ > 0 là một siêu tham số để cân bằng độ chính xác nhận dạng và chi phí tính toán. Lưu ý rằng mất mát tiện ích trong Phương trình (6) được định nghĩa cho một phát ngôn riêng lẻ nên chỉ số phát ngôn được bỏ qua. Trong thực tế, một mini-batch được sử dụng và mất mát được trung bình hóa trên các phát ngôn.

Một vấn đề chính với mục tiêu huấn luyện này là các cổng nhị phân không khả vi. Để giải quyết vấn đề này, chúng tôi áp dụng thủ thuật Gumbel-Softmax [30, 31], cho phép lấy mẫu cứng (hoặc mềm) từ một phân phối rời rạc. Xem xét một biến ngẫu nhiên rời rạc Z với xác suất P(Z=k) ∝ π_k cho bất kỳ k = 1,...,K. Để lấy một mẫu từ phân phối này, trước tiên chúng ta có thể lấy K mẫu i.i.d. {g_k}^K_{k=1} từ phân phối Gumbel tiêu chuẩn và sau đó chọn chỉ số có xác suất log bị nhiễu lớn nhất:

z = arg max_{k∈{1,...,K}} log π_k + g_k; (7)

Argmax không khả vi, có thể được nới lỏng thành softmax. Biết rằng bất kỳ mẫu nào từ phân phối rời rạc có thể được ký hiệu như một vector một-nóng, trong đó chỉ số của mục duy nhất khác không là mẫu mong muốn. Với ký hiệu dựa trên vector này, chúng ta có thể lấy một mẫu mềm như sau:

z = softmax((log π + g)/τ); (8)

trong đó π = (π_1,...,π_K), g = (g_1,...,g_K), và τ là một hằng số nhiệt độ. Phương trình (8) là một xấp xỉ của quá trình lấy mẫu ban đầu, nhưng nó khả vi đối với π và do đó phù hợp cho tối ưu hóa dựa trên gradient. Khi τ → 0, xấp xỉ trở nên gần hơn với phiên bản rời rạc. Chúng tôi sử dụng τ = 1 trong các thí nghiệm của mình.

Đối với MHA thứ l, một phân phối xác suất rời rạc p(l)_MHA ∈ R^2 trên hai giá trị cổng có thể (0 và 1) được dự đoán, trong đó 0 có nghĩa là bỏ qua mô-đun này và 1 có nghĩa là thực thi nó. Sau đó, một mẫu mềm được lấy từ phân phối rời rạc này bằng Phương trình (8), được sử dụng làm cổng trong Phương trình (3) trong quá trình huấn luyện. Tương tự, đối với FFN thứ l, một phân phối p(l)_FFN ∈ R^2 được dự đoán, từ đó một cổng mềm được lấy và sử dụng trong Phương trình (4). Các phân phối cổng được tạo bởi một bộ dự đoán cổng dựa trên các đặc trưng đầu vào, như được định nghĩa trong Phần 2.3.

2.3. Bộ dự đoán cổng cục bộ và toàn cục
Chúng tôi đề xuất hai loại bộ dự đoán cổng, cụ thể là bộ dự đoán cổng cục bộ và bộ dự đoán cổng toàn cục. Chúng tôi sử dụng một perceptron đa lớp (MLP) với một lớp ẩn duy nhất có kích thước 32 trong tất cả các thí nghiệm, có chi phí tính toán ít.

Bộ dự đoán cổng cục bộ (LocalGP hoặc LGP) được liên kết với một lớp mã hóa I3D cụ thể, như minh họa trong Hình 1b. Mỗi lớp có bộ dự đoán cổng riêng có tham số độc lập. Xem xét lớp mã hóa thứ l với một chuỗi đầu vào X(l-1) ∈ R^{T×d}. Chuỗi này đầu tiên được chuyển đổi thành một vector d chiều x(l-1) ∈ R^d thông qua pooling trung bình dọc theo chiều thời gian. Sau đó, vector được gộp này được biến đổi thành hai vector xác suất 2 chiều cho cổng MHA và cổng FFN, tương ứng:

p(l)_MHA, p(l)_FFN = LGP(l)(x(l-1)); (9)

trong đó p(l)_MHA, p(l)_FFN ∈ R^2 được giới thiệu trong Phần 2.2, và LGP(l) là bộ dự đoán cổng cục bộ tại lớp thứ l. Với công thức này, quyết định thực thi hoặc bỏ qua bất kỳ mô-đun MHA hoặc FFN nào phụ thuộc vào đầu vào của lớp hiện tại, phụ thuộc thêm vào quyết định được đưa ra tại lớp trước đó. Do đó, các quyết định được đưa ra tuần tự từ lớp thấp đến lớp cao. Trong quá trình suy luận, một ngưỡng cố định θ ∈ [0,1] được sử dụng để tạo ra một cổng nhị phân cho mỗi mô-đun:

g(l)_MHA = 1 nếu (p(l)_MHA)_1 > θ ngược lại 0; (10)
g(l)_FFN = 1 nếu (p(l)_FFN)_1 > θ ngược lại 0; (11)

trong đó (p(l)_MHA)_1 là xác suất thực thi MHA và (p(l)_FFN)_1 là xác suất thực thi FFN. Chúng tôi sử dụng θ = 0.5 theo mặc định, nhưng cũng có thể điều chỉnh chi phí suy luận bằng cách thay đổi θ.

Bộ dự đoán cổng toàn cục (GlobalGP hoặc GGP), mặt khác, được định nghĩa cho toàn bộ bộ mã hóa I3D, như được hiển thị trong Hình 1c. Nó dự đoán các phân phối cổng cho tất cả các lớp dựa trên đầu vào của bộ mã hóa, cũng là đầu vào của lớp đầu tiên: X = X(0) ∈ R^{T×d}. Cụ thể, chuỗi được biến đổi thành một vector duy nhất x = x(0) ∈ R^d bằng pooling trung bình. Sau đó, nó được ánh xạ thành hai tập phân phối xác suất cho tất cả N cổng MHA và FFN, tương ứng:

{p(l)_MHA}^N_{l=1}, {p(l)_FFN}^N_{l=1} = GGP(x); (12)

trong đó p(l)_MHA, p(l)_FFN ∈ R^2 là các phân phối xác suất cổng tại lớp thứ l, và bộ mã hóa I3D có tổng cộng N lớp. Ở đây, các quyết định thực thi hoặc bỏ qua các mô-đun được đưa ra ngay lập tức sau khi nhìn thấy đầu vào của bộ mã hóa, có chi phí tính toán thấp hơn LocalGP và cho phép kiểm soát linh hoạt hơn đối với kiến trúc suy luận. Trong quá trình suy luận, chúng ta vẫn có thể sử dụng một ngưỡng cố định θ ∈ [0,1] để tạo ra các cổng nhị phân như trong Phương trình (10) và (11).

3. THÍ NGHIỆM

3.1. Thiết lập thí nghiệm
Chúng tôi sử dụng PyTorch [32] và tuân theo các công thức ASR trong ESPnet [33] để huấn luyện tất cả các mô hình. Chúng tôi chủ yếu sử dụng khung CTC trên LibriSpeech 100h [34]. Trong Phần 3.5, chúng tôi cũng cho thấy rằng I3D có thể được áp dụng cho AED và một kho dữ liệu khác, Tedlium2 [35]. Các bộ mã hóa I3D của chúng tôi có tổng cộng 36 lớp. Chúng được khởi tạo với các Transformer tiêu chuẩn đã huấn luyện và được tinh chỉnh với tốc độ học giảm (1e-3) và các λ khác nhau (thường từ 1 đến 13) trong Phương trình (5) để cân bằng WER và tính toán. Các epochs tinh chỉnh cho LibriSpeech 100h và Tedlium2 lần lượt là 50 và 35. Chúng tôi so sánh I3D với hai baseline. Đầu tiên, chúng tôi huấn luyện các Transformer tiêu chuẩn với số lượng lớp giảm. Thứ hai, chúng tôi huấn luyện một Transformer 36 lớp với LayerDrop [36, 37] hoặc Intermediate CTC (InterCTC) [38] và thực hiện cắt tỉa lớp lặp [29] sử dụng tập validation để có được nhiều mô hình với kiến trúc nhỏ hơn và cố định. Baseline này được ký hiệu là "LayerDrop" trong Hình 2 và 3. Chúng ta có thể so sánh I3D, có các lớp được giảm động dựa trên đầu vào, với các mô hình cắt tỉa tĩnh.

3.2. Kết quả chính
Hình 2 so sánh các mô hình I3D của chúng tôi với hai baseline. Chúng tôi huấn luyện các mô hình I3D-CTC với λ khác nhau trong Phương trình (5) để điều chỉnh điểm hoạt động. Chúng tôi tính toán số lượng lớp như trung bình của số lượng khối MHA và số lượng khối FFN. Cả I3D-LocalGP và I3D-GlobalGP đều vượt trội hơn Transformer tiêu chuẩn và phiên bản được cắt tỉa sử dụng cắt tỉa lớp lặp [29]. Chúng ta có thể giảm số lượng lớp trung bình xuống khoảng 20 trong khi vẫn phù hợp với Transformer được huấn luyện từ đầu. LocalGP đạt hiệu suất tương tự như GlobalGP, nhưng GlobalGP chỉ có một bộ dự đoán cổng, có thể hiệu quả hơn cho suy luận. Lý do tại sao LocalGP không tốt hơn GlobalGP có thể là LocalGP quyết định có nên thực thi hay bỏ qua một khối dựa trên đầu vào của lớp hiện tại, phụ thuộc vào các quyết định tại các lớp trước đó. Quy trình tuần tự này có thể dẫn đến lan truyền lỗi nghiêm trọng hơn. Chúng tôi cũng cho thấy rằng có thể điều chỉnh chi phí tính toán của một mô hình I3D đã huấn luyện bằng cách thay đổi θ (xem Phương trình (10) (11)) tại thời điểm suy luận. Ba mô hình I3D-GlobalGP được giải mã với θ khác nhau. Khi θ giảm, nhiều khối được sử dụng hơn, và WER thường được cải thiện.

Hình 3 cho thấy kết quả sử dụng InterCTC [38]. Các WER thấp hơn so với Hình 2, nhờ vào mất mát CTC phụ trợ chính quy hóa huấn luyện. Một lần nữa, I3D luôn tốt hơn Transformer được huấn luyện từ đầu và mô hình được cắt tỉa.

3.3. Phân tích các phân phối cổng
Hình 5 cho thấy trung bình và độ lệch chuẩn (std) của các xác suất cổng được tạo bởi một mô hình I3D-GlobalGP sử dụng CTC trên LibriSpeech test-other. Hầu hết các lớp có xác suất ổn định. Một số lớp có biến thiên lớn hơn tùy thuộc vào đầu vào. Đối với cả MHA và FFN, các lớp trên được thực thi với xác suất cao trong khi các lớp dưới có xu hướng bị bỏ qua, phù hợp với [28].

Chúng tôi cũng hiển thị xác suất cổng từ một mô hình I3D-GlobalGP được huấn luyện với InterCTC [38] trong Hình 6. Thú vị, xu hướng tổng thể rất khác so với Hình 5. Bây giờ, các lớp trên hầu như bị bỏ qua trong khi các lớp dưới được thực thi với xác suất rất cao, cho thấy rằng các lớp dưới của bộ mã hóa này có thể học các biểu diễn mạnh mẽ cho nhiệm vụ ASR. Điều này có thể là do các mất mát CTC phụ trợ được chèn tại các lớp trung gian có thể tạo thuận lợi cho việc lan truyền gradient đến các phần thấp hơn của bộ mã hóa sâu, điều này hiệu quả cải thiện khả năng của nó và cả hiệu suất cuối cùng. Chúng tôi tin rằng phân tích cổng này có thể cung cấp một cách để giải thích hành vi theo từng lớp của các mạng sâu.

3.4. Phân tích sự phụ thuộc đầu vào
Đã được chỉ ra rằng các mô hình I3D của chúng tôi có thể điều chỉnh động độ sâu bộ mã hóa dựa trên các đặc điểm của một phát ngôn đầu vào, đạt được hiệu suất mạnh mẽ ngay cả với tính toán giảm. Nhưng không rõ những đặc trưng nào quan trọng để bộ dự đoán cổng xác định các mô-đun được sử dụng trong quá trình suy luận. Chúng tôi đã phát hiện rằng độ dài tiếng nói thường ảnh hưởng đến kiến trúc suy luận. Hình 7 cho thấy phân phối độ dài tiếng nói được phân loại theo số lượng khối MHA hoặc FFN được sử dụng bởi một mô hình I3D-GlobalGP trong quá trình suy luận. Chúng tôi quan sát thấy rằng các phát ngôn sử dụng nhiều khối hơn có xu hướng dài hơn. Điều này có thể là vì các phát ngôn dài hơn chứa thông tin phức tạp hơn và sự phụ thuộc tầm xa hơn giữa các khung hình, đòi hỏi nhiều khối hơn (đặc biệt là MHA) để xử lý.

Chúng tôi cũng xem xét hai yếu tố khác có thể ảnh hưởng đến kiến trúc suy luận, cụ thể là độ khó của các phát ngôn được đo bằng WER, và chất lượng âm thanh được đo bằng điểm DNSMOS [39]. Tuy nhiên, nói chung, chúng tôi không quan sát thấy mối quan hệ rõ ràng giữa các chỉ số này và số lượng lớp được sử dụng cho suy luận.

3.5. Tính tổng quát
Chúng tôi chứng minh rằng các bộ mã hóa I3D được đề xuất có thể được áp dụng trực tiếp cho các tập dữ liệu và khung ASR khác. Hình 4 cho thấy kết quả của các mô hình dựa trên CTC trên Tedlium2. Các mô hình I3D của chúng tôi luôn đạt WER thấp hơn so với Transformer tiêu chuẩn với số lượng lớp tương tự hoặc thậm chí ít hơn trong quá trình suy luận. Chúng tôi tiếp tục áp dụng I3D cho khung bộ mã hóa-giải mã dựa trên chú ý (AED). Chỉ có bộ mã hóa được thay đổi trong khi bộ giải mã vẫn là bộ giải mã Transformer tiêu chuẩn. Bảng 1 trình bày kết quả trên LibriSpeech 100h. Với khoảng 27 lớp trung bình trong quá trình suy luận, các mô hình I3D của chúng tôi vượt trội hơn Transformer 27 lớp được huấn luyện từ đầu trên cả tập dev clean và test clean. I3D với bộ dự đoán cổng toàn cục tốt hơn một chút so với bộ có bộ dự đoán cổng cục bộ.

4. KẾT LUẬN
Trong công trình này, chúng tôi đề xuất I3D, một bộ mã hóa dựa trên Transformer điều chỉnh động độ sâu của nó dựa trên các đặc điểm của các phát ngôn đầu vào để cân bằng hiệu suất và hiệu quả. Chúng tôi thiết kế hai loại bộ dự đoán cổng và cho thấy rằng các mô hình dựa trên I3D luôn vượt trội hơn Transformer vanilla được huấn luyện từ đầu và mô hình cắt tỉa tĩnh. I3D có thể được áp dụng cho nhiều khung ASR đầu-cuối và kho dữ liệu khác nhau. Chúng tôi cũng trình bày phân tích thú vị về các xác suất cổng được dự đoán và sự phụ thuộc đầu vào để giải thích tốt hơn hành vi của các bộ mã hóa sâu và tác động của các kỹ thuật chính quy hóa mất mát trung gian. Trong tương lai, chúng tôi dự định áp dụng phương pháp này cho các mô hình lớn được huấn luyện trước. Chúng tôi sẽ khám phá chỉ tinh chỉnh các bộ dự đoán cổng để giảm đáng kể chi phí huấn luyện.

5. LỜI CẢM ƠN
Công trình này đã sử dụng Bridges2 tại PSC và Delta tại NCSA thông qua phân bổ CIS210014 từ chương trình Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS), được hỗ trợ bởi các khoản tài trợ của Quỹ Khoa học Quốc gia #2138259, #2138286, #2138307, #2137603, và #2138296.

6. TÀI LIỆU THAM KHẢO
[1] A. Graves, S. Fernández, et al., "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks," trong Proc. ICML, 2006.
[2] K. Cho, B. Merrienboer, et al., "Learning phrase representations using RNN encoder-decoder for statistical machine translation," trong Proc. EMNLP, 2014.
[3] D. Bahdanau, K. Cho, et al., "Neural machine translation by jointly learning to align and translate," trong Proc. ICLR, 2015.
[4] W. Chan, N. Jaitly, et al., "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition," trong Proc. ICASSP, 2016.
[5] A. Graves, "Sequence transduction with recurrent neural networks," arXiv:1211.3711, 2012.
[6] A. Vaswani, N. Shazeer, N. Parmar, et al., "Attention is all you need," trong Proc. NeurIPS, 2017.
[7] A. Gulati, J. Qin, C.-C. Chiu, et al., "Conformer: Convolution-augmented Transformer for Speech Recognition," trong Proc. Interspeech, 2020.
[8] Y. Peng, S. Dalmia, et al., "Branchformer: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding," trong Proc. ICML, 2022.
[9] K. Kim, F. Wu, Y. Peng, et al., "E-branchformer: Branchformer with enhanced merging for speech recognition," arXiv:2210.00077, 2022.
[10] S. Karita, N. Chen, T. Hayashi, et al., "A comparative study on transformer vs rnn in speech applications," trong Proc. ASRU, 2019.
[11] G. Hinton, O. Vinyals, J. Dean, et al., "Distilling the knowledge in a neural network," arXiv:1503.02531, 2015.
[12] H. Chang, S. Yang, and H. Lee, "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert," trong Proc. ICASSP, 2022.
[13] R. Wang, Q. Bai, et al., "LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT," trong Proc. Interspeech, 2022.
[14] P. Dong, S. Wang, et al., "RTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition," trong ACM/IEEE Design Automation Conference (DAC), 2020.
[15] K. Tan and D.L. Wang, "Compressing deep neural networks for efficient speech enhancement," trong Proc. ICASSP, 2021.
[16] C. J. Lai, Y. Zhang, et al., "Parp: Prune, adjust and re-prune for self-supervised speech recognition," trong Proc. NeurIPS, 2021.
[17] Y. Han, G. Huang, S. Song, et al., "Dynamic neural networks: A survey," IEEE Trans. Pattern Anal. Mach. Intell., tập 44, số 11, trang 7436–7456, 2022.
[18] E. Bengio, P. Bacon, et al., "Conditional computation in neural networks for faster models," arXiv:1511.06297, 2015.
[19] A. Veit and S. Belongie, "Convolutional networks with adaptive inference graphs," trong Proc. ECCV, 2018.
[20] X. Wang, F. Yu, et al., "Skipnet: Learning dynamic routing in convolutional networks," trong Proc. ECCV, 2018.
[21] Z. Wu, T. Nagarajan, et al., "Blockdrop: Dynamic inference paths in residual networks," trong Proc. CVPR, 2018.
[22] J. Shen, Y. Wang, et al., "Fractional skipping: Towards finer-grained dynamic cnn inference," trong Proc. AAAI, 2020.
[23] C. Li, G. Wang, et al., "Dynamic slimmable network," trong Proc. CVPR, 2021.
[24] J. Macoskey, G. P. Strimel, and A. Rastrow, "Bifocal neural asr: Exploiting keyword spotting for inference optimization," trong Proc. ICASSP, 2021.
[25] Y. Shi, V. Nagaraja, C. Wu, et al., "Dynamic encoder transducer: a flexible solution for trading off accuracy for latency," arXiv:2104.02176, 2021.
[26] F. Weninger, M. Gaudesi, R. Leibold, R. Gemello, and P. Zhan, "Dual-encoder architecture with encoder selection for joint close-talk and far-talk speech recognition," trong Proc. ASRU, 2021.
[27] J. Macoskey, G. P. Strimel, J. Su, and A. Rastrow, "Amortized neural networks for low-latency speech recognition," arXiv:2108.01553, 2021.
[28] Y. Xie, J. J. Macoskey, et al., "Compute Cost Amortized Transformer for Streaming ASR," trong Proc. Interspeech, 2022.
[29] J. Lee, J. Kang, and S. Watanabe, "Layer pruning on demand with intermediate CTC," trong Proc. Interspeech, 2021.
[30] E. Jang, S. Gu, and B. Poole, "Categorical reparameterization with gumbel-softmax," trong Proc. ICLR, 2017.
[31] C. J. Maddison, A. Mnih, and Y. Teh, "The concrete distribution: A continuous relaxation of discrete random variables," trong Proc. ICLR, 2017.
[32] A. Paszke, S. Gross, F. Massa, et al., "Pytorch: An imperative style, high-performance deep learning library," trong Proc. NeurIPS, 2019.
[33] S. Watanabe, T. Hori, S. Karita, et al., "ESPnet: End-to-End Speech Processing Toolkit," trong Proc. Interspeech, 2018.
[34] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, "Librispeech: An ASR corpus based on public domain audio books," trong Proc. ICASSP, 2015.
[35] A. Rousseau, P. Deléglise, Y. Esteve, et al., "Enhancing the ted-lium corpus with selected data for language modeling and more ted talks.," trong Proc. LREC, 2014.
[36] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger, "Deep networks with stochastic depth," trong Proc. ECCV, 2016.
[37] A. Fan, E. Grave, and A. Joulin, "Reducing transformer depth on demand with structured dropout," trong Proc. ICLR, 2020.
[38] J. Lee and S. Watanabe, "Intermediate loss regularization for ctc-based speech recognition," trong Proc. ICASSP, 2021.
[39] C. K. Reddy, V. Gopal, and R. Cutler, "Dnsmos p.835: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors," trong Proc. ICASSP, 2022.
