# 2303.11525.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2303.11525.pdf
# Kích thước tệp: 1076738 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Sparse-IFT: Biến đổi Iso-FLOP Thưa thớt để
Tối đa hóa Hiệu quả Huấn luyện
Vithursan Thangarasa* 1Shreyas Saxena*†Abhay Gupta†Sean Lie1

Tóm tắt
Nghiên cứu gần đây đã tập trung vào độ thưa thớt trọng số
trong huấn luyện mạng nơ-ron sâu để giảm FLOPs,
nhằm cải thiện hiệu quả (độ chính xác kiểm tra so với
FLOPs huấn luyện). Tuy nhiên, huấn luyện trọng số thưa thớt
thường làm giảm độ chính xác, đòi hỏi lịch trình huấn luyện
kéo dài để đạt được độ chính xác của các mô hình dày đặc.
Ngược lại, phương pháp của chúng tôi, Biến đổi Iso-
FLOP Thưa thớt (Sparse-IFT), sử dụng độ thưa thớt
để cải thiện độ chính xác trong khi duy trì FLOPs của
mô hình dày đặc. Sử dụng một siêu tham số duy nhất
(tức là, mức độ thưa thớt), Sparse-IFTs hiệu quả
thay thế các lớp dày đặc, mở rộng không gian tìm kiếm
cho các mặt nạ thưa thớt tối ưu. Ngoài ra, huấn luyện
thưa thớt động (DST) với các mô hình Sparse-IFT có
hiệu quả điều hướng không gian mặt nạ-trọng số thưa thớt
lớn hơn này, được chứng minh bằng phân tích phổ
sử dụng tính chất đồ thị Ramanujan. Nghiên cứu của chúng tôi
tiết lộ một mối tương quan mạnh mẽ giữa cấu trúc mặt nạ,
trọng số, và hiệu suất cuối cùng. Đáng chú ý, không cần
điều chỉnh bất kỳ siêu tham số huấn luyện nào, việc thay thế
các lớp dày đặc bằng Sparse-IFT mang lại những cải thiện
đáng kể, chẳng hạn như tăng +3.5% cho ResNet-
18 trên ImageNet và +0.9% cho GPT-3 Small trên
bảng xếp hạng Open LLM. Theo hiểu biết của chúng tôi,
đây là công trình đầu tiên chứng minh việc sử dụng độ thưa thớt
để cải thiện độ chính xác của các mô hình dày đặc thông qua
một tập hợp các biến đổi thưa thớt đơn giản để sử dụng.
Mã nguồn có sẵn tại: https:
//github.com/CerebrasResearch/Sparse-IFT.

1. Giới thiệu
Sự gia tăng kích thước mô hình và dữ liệu huấn luyện đã dẫn đến nhiều
đột phá trong học sâu (ví dụ: AlexNet (Krizhevsky
et al., 2012), ResNet (He et al., 2016), Transform-
*Đóng góp bằng nhau1Cerebras Systems Inc, California, USA,
†Công việc được thực hiện khi ở Cerebras. Liên hệ: Vithursan
Thangarasa <vithu@cerebras.net >.

Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học máy,
Vienna, Austria. PMLR 235, 2024. Bản quyền 2024 bởi
(các) tác giả.

1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00
FLOPs Huấn luyện (exaFLOPs)71727374757677Độ chính xác Top-1 ImageNet (%)
ResNet-18ResNet-34ResNet-50
s = 50%s = 50%
s = 75%s = 75%
s = 90%s = 90% Dày đặc
Sparse-IFTHình 1: Độ chính xác Top-1 so với FLOPs Huấn luyện cho các
biến thể ResNet trên ImageNet. Sparse-IFT cung cấp những
cải thiện độ chính xác đáng kể trên các mô hình khác nhau và
các mức độ thưa thớt, s∈ {50%, 75%, 90%}, trong khi sử dụng
cùng FLOPs huấn luyện như đối tác dày đặc của nó.

ers (Vaswani et al., 2017), GPT (Radford et al., 2018; 2019),
AlphaGo (Silver et al., 2017), v.v.). Do đó, nhu cầu tính toán
và bộ nhớ để huấn luyện và triển khai mạng nơ-ron sâu
(DNNs) đã tăng vọt một cách đáng kể. Để cho phép triển khai
các mô hình lớn, nhiều kỹ thuật (ví dụ: chưng cất (Hinton et al., 2015),
lượng tử hóa (Han et al., 2015a), cắt tỉa (Han et al., 2015b))
đã được giới thiệu để giảm FLOPs suy luận và yêu cầu bộ nhớ.
Trong khi các kỹ thuật này cải thiện hiệu quả suy luận (độ chính xác
kiểm tra so với FLOPs suy luận), chi phí huấn luyện liên quan
vẫn còn quá cao. Công việc của chúng tôi tập trung vào cải thiện
hiệu quả huấn luyện (độ chính xác kiểm tra so với FLOPs huấn luyện)
của DNNs thông qua độ thưa thớt trọng số. Trong những năm gần đây,
chúng ta đã chứng kiến tiến bộ trong việc sử dụng độ thưa thớt trọng số
để giảm FLOPs huấn luyện (Evci et al., 2020; Liu et al., 2021a;
Jayakumar et al., 2020). Frankle & Carbin (2018) cho thấy rằng
các mạng con thưa thớt ("vé số trúng thưởng") tồn tại tại khởi tạo
và có thể được huấn luyện để đạt độ chính xác của mạng dày đặc.
Các phương pháp huấn luyện thưa thớt động (DST) (Ma et al., 2022;
Evci et al., 2020; Liu et al., 2021b; Jayakumar et al., 2020)
lặp đi lặp lại điều chỉnh các mẫu thưa thớt để tạo điều kiện
khám phá các mạng con thưa thớt tối ưu trong một lần huấn luyện
duy nhất. Tuy nhiên, chúng thường thua kém so với các đường cơ sở
dày đặc hoặc đòi hỏi lịch trình huấn luyện dài hơn (ví dụ: 2-5x
bước huấn luyện) để thu hẹp khoảng cách (Yuan et al., 2021;
Tai et al., 2022; Liu et al., 2021a). Đóng góp độc đáo của chúng tôi
tập trung vào việc sử dụng độ thưa thớt để cải thiện độ chính xác của
một mô hình dày đặc cho trước. Chúng tôi giới thiệu Biến đổi Iso-FLOP Thưa thớt
1arXiv:2303.11525v4  [cs.LG]  17 Jul 2024

--- TRANG 2 ---
Biến đổi Iso-FLOP Thưa thớt để Tối đa hóa Hiệu quả Huấn luyện

Transformations (Sparse-IFT), một họ các kỹ thuật phục vụ
như các thay thế trực tiếp cho các lớp dày đặc trong DNNs.
Sparse-IFTs tăng khả năng biểu diễn của lớp, tạo điều kiện
thuận lợi cho việc khám phá các mạng con thưa thớt tối ưu trong khi
duy trì FLOPs không đổi (tức là Iso-FLOP). Ví dụ,
mở rộng một lớp với độ thưa thớt được duy trì tăng chiều
mà không ảnh hưởng đến FLOPs; mở rộng không gian
mặt nạ-trọng số thưa thớt cho các cấu hình đa dạng hơn. Điều này
cho phép các phương pháp DST điều hướng không gian tìm kiếm một cách
hiệu quả, có khả năng tìm ra các mạng con thưa thớt được cải thiện
cho độ chính xác cao hơn. Lấy cảm hứng từ các công trình trước đây
(Hoang et al., 2023b;a), chúng tôi phân tích khả năng kết nối của
các mô hình Sparse-IFT như các đồ thị Ramanujan và tác động của chúng
đến hiệu suất khi được huấn luyện với DST. Tất cả Sparse-IFTs được
tham số hóa bởi một siêu tham số duy nhất, mức độ thưa thớt.
Hình 1 tóm tắt hiệu suất ImageNet, cho thấy những cải thiện
độ chính xác đáng kể với các biến thể Sparse Wide IFT ResNet.
Sparse Wide ResNet-18 đạt được độ chính xác top-1 +3.5% ở
90% thưa thớt, vượt qua một ResNet-34 dày đặc (74.2%) với
2x ít FLOPs hơn. Những cải thiện này là kết quả của việc thay thế
các lớp dày đặc bằng Sparse-IFTs, không đòi hỏi thay đổi các
siêu tham số huấn luyện. Những đóng góp chính của công việc chúng tôi là:

1. Chúng tôi giới thiệu Biến đổi Iso-FLOP Thưa thớt
(Sparse-IFTs), một họ các kỹ thuật nhằm tăng cường
hiệu quả huấn luyện DNN. Những biến đổi này
tăng cường độ chính xác trong khi duy trì số lượng FLOP
không đổi. Sparse-IFTs được tham số hóa bởi một siêu
tham số duy nhất, mức độ thưa thớt, và có thể được sử dụng
một cách liền mạch như các thay thế trực tiếp cho các lớp dày đặc.

2. Chúng tôi xác nhận thực nghiệm lợi thế nhất quán của
DST so với huấn luyện thưa thớt tĩnh cho các mạng
Sparse-IFT. Cuộc điều tra của chúng tôi về sự phát triển
động của các cấu trúc thưa thớt trong DST thông qua phân tích
phổ đồ thị Ramanujan làm nổi bật các mẫu kết nối được
tối ưu hóa và các đặc tính phổ được cải thiện.

3. Chúng tôi cho thấy những lợi ích nhất quán của Sparse-IFT
trên các lĩnh vực thị giác máy tính và xử lý ngôn ngữ tự nhiên.
Sparse-IFT tăng cường độ chính xác top-1 của ResNet-18 và
ResNet-34 trên ImageNet lần lượt 3.5% và 2.6%. Tinh chỉnh
cho phát hiện đối tượng (MS COCO) và phân đoạn (CityScapes)
mang lại những cải thiện 5.2% mAP và 2.4% mIoU. Sparse-IFT
với GPT-3 dẫn đến cải thiện 0.9% trên bảng xếp hạng Open LLM.

4. Chúng tôi trình bày giá trị thực tế của Sparse-IFT với
thời gian thực tế cho huấn luyện trên Cerebras CS-
2 (Lie, 2023) và suy luận với Neural Magic
DeepSparse (NeuralMagic, 2021) sử dụng độ thưa thớt
không có cấu trúc. Mặc dù rộng hơn 2x ở 75% thưa thớt với
Sparse Wide IFT, chúng tôi quan sát thấy chi phí tính toán
tối thiểu trên cả hai nền tảng so với GPUs.

2. Phương pháp
Trong phần này, chúng tôi đầu tiên giải thích trực giác và giả thuyết
của chúng tôi, tiếp theo là phương pháp luận để cải thiện hiệu quả huấn luyện.

Huấn luyện với Ma trận Dày đặc là Không hiệu quả FLOP Các
DNN hiện đại thường được tham số hóa quá mức, cho thấy độ thưa thớt
trong các đặc trưng và trọng số qua các lớp. Giả thuyết Vé số
Trúng thưởng (Frankle & Carbin, 2018; Chen et al., 2020)
gợi ý rằng các DNN thưa thớt, được khởi tạo với một mặt nạ
thưa thớt hiệu quả ("vé số trúng thưởng"), có thể đạt được
cùng độ chính xác như các đối tác dày đặc. Các phương pháp
huấn luyện thưa thớt về mặt lý thuyết tăng cường hiệu quả
nhưng thường mang lại độ chính xác thấp hơn so với các đường
cơ sở dày đặc. Sự khác biệt này có thể xuất phát từ những
thách thức trong việc xác định các mặt nạ tối ưu trong một
lần huấn luyện duy nhất. Các phương pháp huấn luyện thưa thớt
hiện có (Jayakumar et al., 2020; Evci et al., 2020; Yuan
et al., 2021; Tai et al., 2022; Liu et al., 2021a) đầu tư
những tiết kiệm FLOP này vào các lịch trình huấn luyện dài hơn
để thu hẹp khoảng cách độ chính xác, một cách không hiệu quả
đòi hỏi nhiều FLOPs hơn so với các đường cơ sở dày đặc cho
cùng độ chính xác mục tiêu.

Trong công việc của chúng tôi, chúng tôi áp dụng một phương pháp
trực giao và đầu tư những tiết kiệm FLOP này để (1) tăng cường
khả năng biểu diễn của một lớp và (2) mở rộng không gian tìm kiếm
của nó, nhằm khám phá một mặt nạ thưa thớt tối ưu (Ramanujan
et al., 2020; Stosic & Stosic, 2021). Các mô hình thưa thớt
lớn hơn cho thấy tiềm năng cải thiện độ chính xác, nhưng thiết kế
một kiến trúc phù hợp là thách thức. Ví dụ, việc đạt được hiệu suất
vượt qua ResNet-18 trên ImageNet đòi hỏi sự cân bằng cẩn thận
giữa độ thưa thớt và kích thước mạng. Các nghiên cứu hiện có
khám phá các kết hợp đa dạng nhưng thường thiếu hiệu quả FLOP,
đòi hỏi nhiều lần lặp để có được các cài đặt tối ưu và điều chỉnh
siêu tham số. Để giải quyết điều này, chúng tôi đề xuất họ Biến đổi
Iso-FLOP Thưa thớt (Sparse-IFT), thay thế các lớp dày đặc bằng
các biến đổi thưa thớt tương đương FLOP. Đáng chú ý, Sparse-IFT
được tham số hóa bởi một siêu tham số duy nhất—mức độ thưa thớt,
đơn giản hóa quá trình điều chỉnh.

2.1. Biến đổi Iso-FLOP Thưa thớt

Thiết lập Để rõ ràng, chúng tôi giải thích phương pháp của chúng tôi
trong bối cảnh của một mạng được kết nối đầy đủ. Cho N biểu thị
một DNN L lớp được tham số hóa bởi ΘN. Cho ΘN∈ {θ1, . . . , θ L}
biểu thị các tham số của DNN. Đầu ra của lớp thứ l được định nghĩa
là: zl=σ(fθl(zl−1)) cho một số hàm kích hoạt σ (ví dụ: ReLU
(Nair & Hinton, 2010)) và hàm feedforward fθl. Cụ thể, cho
fθl(zl−1) =θT lzl−1, trong đó θl∈RDin×Dout,zl−1∈RDin×B
và B,Din,Dout biểu thị kích thước batch, chiều đầu vào và đầu ra
của các đặc trưng tương ứng. Tổng FLOPs cần thiết cho fθl được
cho bởi B·Din·Dout. Trong Phụ lục A.1, chúng tôi chi tiết một
mở rộng đơn giản cho các lớp tích chập. Trong thiết lập tiêu chuẩn,
hàm feedforward fθl tính toán các đặc trưng đầu ra thông qua
một phép biến đổi tuyến tính của các đặc trưng đầu vào. Trong khi
về mặt lý thuyết, các phép biến đổi phi tuyến tùy ý có thể được áp dụng,
các triển khai thực tế thường sử dụng việc biểu diễn các biến đổi

--- TRANG 3 ---
Biến đổi Iso-FLOP Thưa thớt để Tối đa hóa Hiệu quả Huấn luyện

Biến đổi ISO FLOP Thưa thớtf/uni03B8l(zl−1)Sparse WideSparse ParallelSparse FactorizedSparse DopedDoutDinBiến đổi Dày đặc
ksw.Doutksw.DinDinidx:0idx:kspDoutDinDoutDindsdDoutDindsf
dsfDout
ksw=1(1−s)ksp=1(1−s)dsf=DinDout(Din+Dout).(1−s)dsd=s.Din.Dout(Din+Dout)

Hình 2: Các thành viên khác nhau của họ Sparse-IFT, mỗi thành viên được tham số hóa bởi một siêu tham số duy nhất (tức là mức độ thưa thớt, s). Các ô vuông đen và trắng biểu thị các trọng số không hoạt động và hoạt động tương ứng. Khối xanh lá cây chỉ ra một hàm kích hoạt phi tuyến (ví dụ: ReLU). Được dẫn xuất với độ thưa thớt được đặt ở 50% như một ví dụ, tất cả các biến đổi đều là Iso-FLOP với hàm feedforward dày đặc fθl, làm cho chúng phù hợp như các thay thế trực tiếp cho fθl. Chi tiết về mỗi thành viên có trong Phần 2.2.

như các phép nhân ma trận dày đặc để hỗ trợ GPU hiệu quả
(Nvidia, 2023). Chúng tôi nhằm tăng cường hiệu quả huấn luyện DNN
bằng cách nâng cao khả năng biểu diễn của hàm feedforward.
Không giống như các phương pháp thông thường tăng khả năng bằng cách
xếp chồng nhiều lớp hơn (Lin et al., 2014a), mở rộng (Zagoruyko &
Komodakis, 2016), hoặc kết hợp (Littwin et al., 2020), phương pháp
của chúng tôi giới thiệu độ thưa thớt không có cấu trúc trong các
ma trận trọng số, đạt được cùng FLOPs như một hàm feedforward dày đặc.

Cho Ψl biểu thị tập hợp các Biến đổi Iso-FLOP Thưa thớt
(Sparse-IFT) cho một lớp cụ thể l:
Ψl:{ψl(s),0≤s <1, g(ψl)≈g(fθl)},
trong đó ψl là một biến đổi, s biểu thị mức độ thưa thớt,
và g(·) trả về FLOPs tính toán. Mỗi biến đổi trong tập hợp này
thỏa mãn các tính chất sau: (1) FLOPs tính toán của biến đổi
ψl giống như của biến đổi dày đặc fθl, và (2) biến đổi được
tham số hóa bởi một siêu tham số duy nhất - mức độ thưa thớt.
Những biến đổi Iso-FLOP này phục vụ như các thay thế trực tiếp
cho các hàm feedforward dày đặc, bảo toàn FLOPs của lớp.
Trong khi có thể có các biến đổi bất biến FLOP khác, trong công
việc này, chúng tôi khám phá: Sparse Wide, Sparse Parallel,
Sparse Factorized, và Sparse Doped.

2.2. Các thành viên của Sparse-IFT

Sparse Wide Biến đổi này tăng cường khả năng biểu diễn của
một lớp bằng cách tăng số lượng đặc trưng đầu ra trong khi
giữ phần s của trọng số thưa thớt. Do đó, nó mở rộng các đặc
trưng đầu vào và đầu ra cho tất cả L lớp của mạng với cùng
hệ số mở rộng, ksw, để tránh không khớp trong chiều đặc trưng
qua các lớp. Cho θsw l∈Rksw·Din×ksw·Dout biểu thị ma trận
biến đổi, với phần s của trọng số là thưa thớt. Vì phần của
trọng số không thưa thớt được cho bởi 1−s, FLOPs cần thiết

bởi biến đổi này là B·(ksw·Din)·(ksw·Dout)·(1−s).
Đặt những điều này bằng FLOPs của fθl dày đặc ban đầu,
chúng ta có được hệ số mở rộng ksw=√1/(1−s). Nếu chúng ta
đặt độ thưa thớt s thành 0, chúng ta có được ksw là 1 và khôi phục
hàm feedforward dày đặc.

Sparse Parallel Biến đổi thưa thớt song song thay thế hàm
feedforward bằng tổng của ksp hàm phi tuyến. Cho θsp l∈
{θsp,1 l, . . . , θsp,ksp l} biểu thị các tham số của biến đổi này,
trong đó θsp,j l∈RDin×Dout biểu thị ma trận biến đổi của hàm
thứ j, trong đó phần s của trọng số là thưa thớt. Biến đổi thưa
thớt song song trong trường hợp này là ψsp l=∑ksp j=1σ((θsp,j l)Tzl),
trong đó σ là một hàm phi tuyến. Trong thực tế, ψsp l được triển
khai như một lớp với ksp nhánh song song. FLOPs tính toán của
biến đổi này là ksp·B·Din·Dout·(1−s). Đặt những FLOPs này
bằng FLOPs của fθ, chúng ta có được ksp= 1/(1−s). Lưu ý, ở
s= 0, số lượng nhánh song song ksp là 1. Nếu chúng ta thay thế
σ bằng Identity, chúng ta có thể khôi phục hàm feedforward
dày đặc ban đầu.

Sparse Factorized Ma trận biến đổi của hàm feedforward fθl
được biểu thị bởi θl∈RDin×Dout. Nhiều công trình đã khám phá
các kỹ thuật phân tích ma trận để biểu diễn ma trận biến đổi
θl như một tích của hai ma trận θl=UVT, trong đó U∈RDin×d,
V∈RDout×d. Khodak et al. (2020); Tai et al. (2016) và Chen
et al. (2021b) đã khám phá phân tích hạng thấp (d << Dout)
như một dạng độ thưa thớt có cấu trúc để cải thiện hiệu quả
huấn luyện và suy luận, trong khi Arora et al. (2018) và Guo
et al. (2020a) đã khám phá các phân tích được tham số hóa quá
mức để có được khái quát hóa tốt hơn và hội tụ nhanh hơn.
Ngược lại, chúng tôi sử dụng phân tích để tăng cường khả năng
biểu diễn mà không giảm hoặc tăng FLOPs. Chính xác hơn, cho
θsf l∈ {Ul, Vl} biểu thị các tham số của biến đổi này, trong đó
Ul∈RDin×dsf,Vl∈Rdsf×Dout là các ma trận thưa thớt với phần s
của trọng số của chúng là

--- TRANG 4 ---
Biến đổi Iso-FLOP Thưa thớt để Tối đa hóa Hiệu quả Huấn luyện

Bảng 1: Tính đa dạng của không gian tìm kiếm cho mặt nạ thưa thớt của các thành viên khác nhau của họ Sparse-IFT.

BIẾN ĐỔI                    TÍNH ĐA DẠNG CỦA
                           KHÔNG GIAN TÌM KIẾM

SPARSE WIDE               (ksw)2·(Din·Dout)
SPARSE PARALLEL           ksp·(Din·Dout)
SPARSE FACTORIZED         dsf·(Din+Dout)
SPARSE DOPED              Din·Dout

thưa thớt. Biến đổi chức năng trong trường hợp này là ψsf l=
VT lσ(UT lzl). FLOPs tính toán của biến đổi này là dsf·B·(Din+Dout)·(1−s).
Đặt những FLOPs này bằng FLOPs của fθl, chúng ta có được dsf=Din·Dout/(Din+Dout)·(1−s).

Lưu ý, đặt độ thưa thớt s= 0, chúng ta khôi phục một phân tích
hạng thấp phi tuyến với các ma trận dày đặc.

Sparse Doped được lấy cảm hứng từ các công trình trước đây
mà tương tự một ma trận dày đặc với một kết hợp của phân tích
hạng thấp và ma trận thưa thớt (Chen et al., 2021a; Thakker
et al., 2021; Udell & Townsend, 2019; Candès et al., 2011).
Trong phương pháp của chúng tôi, chúng tôi thay thế hàm feedforward
bằng phân tích hạng thấp (với hạng dsd) và một ma trận trọng số
thưa thớt không có cấu trúc (với độ thưa thớt s). Cho Ul∈RDin×dsd,
Vl∈Rdsd×Dout biểu thị các ma trận hạng thấp, và θsd l∈RDin×Dout
biểu thị ma trận với độ thưa thớt không có cấu trúc. Biến đổi
chức năng, trong trường hợp này, được cho bởi ψsd l=VT l(UT lzl) +σ((θsd l)Tzl).
FLOPs tính toán liên quan đến biến đổi này là B·dsd·(Din+Dout) + (1 −s)·B·Din·Dout.
Đặt những FLOPs này bằng FLOPs của fθl, chúng ta có được dsd=s·Din·Dout/(Din+Dout).

Lưu ý, khi s→0 và dsd→0, thành phần hạng thấp biến mất,
và chúng ta có thể khôi phục hàm feedforward dày đặc như một
trường hợp đặc biệt bằng cách đặt σ thành Identity.

Tính đa dạng của Không gian Tìm kiếm Việc tăng không gian
tìm kiếm mặt nạ thưa thớt với Sparse-IFT được dự đoán sẽ tăng
cường hiệu quả huấn luyện, như được chỉ ra bởi các công trình
trước đây (Ramanujan et al., 2020; Liu et al., 2022c; Stosic &
Stosic, 2021). Khả năng tìm thấy một vé số trúng thưởng trong
một mạng được khởi tạo ngẫu nhiên tăng lên với độ rộng mạng
(Ramanujan et al., 2020). Cả Liu et al. (2022b) và Stosic &
Stosic (2021) đều cho thấy rằng việc mở rộng không gian tìm
kiếm thông qua tăng độ rộng hoặc độ sâu cải thiện độ chính xác.
Tính đa dạng của không gian tìm kiếm, được định nghĩa là các
trọng số mà một phương pháp huấn luyện thưa thớt có thể khám phá,
được chi tiết trong Bảng 1. Sparse Wide, Sparse Parallel, và
Sparse Factorized tỷ lệ với độ rộng, nhánh song song, và kích
thước chiều ẩn tương ứng. Sparse Doped duy trì không gian tìm
kiếm không đổi bằng cách phân bổ FLOPs giữa một ma trận trọng số
hạng thấp và một ma trận trọng số thưa thớt không có cấu trúc.
Do đó, DST trở nên quan trọng để duyệt hiệu quả không gian con
tham số lớn hơn này, như được thảo luận trong Phần 3.1.

3. Nghiên cứu Loại bỏ Sparse-IFT

Trong phần này, chúng tôi trình bày một phân tích toàn diện về
các mạng Sparse-IFT, tập trung vào phương pháp huấn luyện của chúng và

Bảng 2: Sparse Wide IFT với ResNet-18 được huấn luyện sử dụng các phương pháp huấn luyện thưa thớt khác nhau trên CIFAR-100 qua các mức độ thưa thớt khác nhau (các cột). Độ chính xác tốt nhất cho mỗi phương pháp huấn luyện thưa thớt được tô đậm.

DÀY ĐẶC    PHƯƠNG PHÁP THƯA THỚT    0.50    0.75    0.90
77.0±0.2   STATIC                   78.5±0.3 78.3±0.1 78.2 ±0.3
           SNIP                     77.8±0.3 77.0±0.2 75.8 ±0.2
           GRASP                    77.7±0.3 76.5±0.3 76.5 ±0.3
           FORCE                    77.2±0.3 76.9±0.3 75.4 ±0.4
           SET                      78.8 ±0.1 79.2 ±0.2 79.8±0.2
           RIGL                     79.1 ±0.2 79.5 ±0.1 80.1±0.2
           GRANET                   79.2±0.2 79.6 ±0.2 80.0±0.2

các cân nhắc thiết kế. Đầu tiên, chúng tôi so sánh huấn luyện
thưa thớt tĩnh với DST, làm nổi bật hiệu suất vượt trội của DST
trong việc xử lý các không gian tham số lớn hơn thông qua các
kết quả thực nghiệm sử dụng kiến trúc ResNet-18 trên CIFAR-100.
Sau đó, chúng tôi khám phá các khía cạnh thiết kế quan trọng của
Sparse-IFT, bao gồm vai trò của các tính phi tuyến, và lợi ích
của độ thưa thớt động không có cấu trúc so với độ thưa thớt có
cấu trúc. Cuối cùng, chúng tôi đánh giá hiệu quả của DST bằng
cách so sánh nó với các mô hình Sparse-IFT được huấn luyện dày đặc.

3.1. Tác động của Các kỹ thuật Huấn luyện Thưa thớt

Phần này cung cấp một phân tích so sánh các mạng Sparse-IFT
được huấn luyện với hai lớp phương pháp: huấn luyện thưa thớt
tĩnh và DST. Trọng tâm là chứng minh hiệu quả của DST trong
việc điều hướng các không gian tham số lớn hơn, như được chứng
minh bởi nghiên cứu trước đây (Huang et al., 2023; Tai et al., 2022).
Các kết quả thực nghiệm của chúng tôi nhất quán cho thấy sự
vượt trội của DST so với huấn luyện thưa thớt tĩnh. Tất cả
các thí nghiệm sử dụng kiến trúc ResNet-18 trên CIFAR-100
với các cài đặt đã công bố (DeVries & Taylor, 2017). Thông tin
mô hình chi tiết và các siêu tham số có sẵn trong Phụ lục C.1,
và tất cả kết quả được tính trung bình qua 3 seeds.

Sparse-IFTs sử dụng độ thưa thớt không có cấu trúc trong các
biến đổi của nó. Nghiên cứu này điều tra tác động của các phương
pháp huấn luyện thưa thớt đối với các cấu hình Sparse-IFT khác
nhau, tập trung vào Sparse Wide IFT với độ thưa thớt ∈ {50%,75%,90%}.
Trong Bảng 2, chúng tôi đánh giá: độ thưa thớt tĩnh ngẫu nhiên,
SNIP (Lee et al., 2018), GraSP (Wang et al., 2020a), FORCE
(de Jorge et al., 2020), SET (Mocanu et al., 2018), RigL (Evci
et al., 2020) và GraNet (Liu et al., 2021a). SET, RigL, và
GraNet là các phương pháp DST, với SET cập nhật mặt nạ ngẫu
nhiên, RigL cập nhật nó với thông tin gradient và GraNet kết
hợp cắt tỉa độ lớn dần dần (Zhu & Gupta, 2017) với RigL.
Các phương pháp Cắt tỉa tại Khởi tạo (PaI) (ví dụ: SNIP, GraSP,
FORCE) và GraNet tăng FLOPs huấn luyện do độ thưa thớt không
đồng nhất và huấn luyện từ dày đặc đến thưa thớt. Chúng tôi
giải quyết điều này bằng cách điều chỉnh các mức độ thưa thớt
mục tiêu để căn chỉnh FLOPs huấn luyện Sparse-IFT với đường
cơ sở dày đặc (xem Phụ lục A.2). Trong các tình huống Iso-FLOP,
các phương pháp PaI hoạt động kém vì chúng cắt tỉa mạnh các lớp
giàu tham số để khớp với các mức độ thưa thớt mục tiêu, dẫn đến
sụp đổ lớp và dòng gradient kém. Hơn nữa, các phương pháp DST
nhất quán

--- TRANG 5 ---
Biến đổi Iso-FLOP Thưa thớt để Tối đa hóa Hiệu quả Huấn luyện

0.00 0.50 0.75 0.90
Mức độ Thưa thớt (%)77.077.578.078.579.079.580.0Độ chính xác Kiểm tra (%)
Dày đặc (baseline)
Sparse Wide IFT (Không có cấu trúc)
Sparse Wide IFT (N:M Có cấu trúc)

0.00 0.50 0.75 0.90
Mức độ Thưa thớt (%)77.077.578.078.579.079.580.0Độ chính xác Kiểm tra (%)
Dày đặc (baseline)
Sparse Wide IFT
Sparse Parallel IFT
Sparse Factorized IFT
Sparse Doped IFT

0.50 0.75 0.90
Mức độ Thưa thớt (%)77.578.078.579.079.580.080.5Độ chính xác Kiểm tra (%)
Sparse Wide IFT
Dense Wide
Sparse Parallel IFT
Dense Parallel

Hình 3: Các nghiên cứu loại bỏ với Sparse-IFT trên mô hình ResNet-18 cho CIFAR-100 qua độ thưa thớt ∈ {50%,75%,90%}.
(trái) Sparse Wide IFT được huấn luyện với độ thưa thớt động không có cấu trúc và có cấu trúc. (giữa) Các thành viên họ Sparse-IFT
được huấn luyện với RigL, trong đó Sparse Wide hoạt động tốt nhất. (phải) Sparse Wide IFT được huấn luyện theo cách thưa thớt và dày đặc.

vượt trội hơn độ thưa thớt tĩnh, với những cải thiện duy trì ở
các mức độ thưa thớt cao hơn. Sparse-IFTs mở rộng không gian
mặt nạ-trọng số thưa thớt ∝độ thưa thớt, có lợi cho DST trong
việc khám phá và khai thác triệt để trong không gian này. Trong
khi RigL và GraNet đạt được hiệu suất tương tự, RigL được chọn
làm phương pháp huấn luyện thưa thớt cho sự đơn giản trong tất
cả các thí nghiệm.

3.2. Đánh giá Tác động của Các biến thể Kiến trúc

Phần này phân tích các cân nhắc thiết kế khác nhau cho Sparse-IFT
bằng cách đầu tiên, khám phá vai trò của các tính phi tuyến trong
việc tăng cường khả năng biểu diễn. Sau đó, lợi thế của huấn
luyện với độ thưa thớt động không có cấu trúc so với có cấu trúc
được điều tra. Tiếp theo, chúng tôi so sánh giữa các mô hình
Sparse-IFT được huấn luyện dày đặc và thưa thớt. Cuối cùng,
bằng cách áp dụng các Sparse-IFTs hiệu suất cao nhất cho các
mô hình thị giác hiệu quả, những hiểu biết này đóng góp vào
một khung tổng hợp.

Tầm quan trọng của việc Sử dụng Kích hoạt Phi tuyến Đối với
một số thành viên của Sparse-IFT, chúng tôi lấy cảm hứng từ
các phương pháp tham số hóa quá mức tuyến tính, mà gấp hàm
feedforward thành một ma trận dày đặc sau huấn luyện (Ding
et al., 2021b;a; Guo et al., 2020a; Ding et al., 2019). Phương
pháp của chúng tôi tăng cường khả năng biểu diễn thông qua một
biến đổi Iso-FLOP mà không tăng FLOPs huấn luyện. Duy trì
các mức FLOP dày đặc ban đầu loại bỏ nhu cầu sửa đổi sau
huấn luyện, cho phép suy luận hiệu quả và kết hợp các tính
phi tuyến (tức là ReLU) trong Sparse-IFT. Các thí nghiệm trên
ResNet-18 trên CIFAR-100 cho thấy những cải thiện độ chính xác
đáng kể qua tất cả các mức độ thưa thớt với các kích hoạt phi
tuyến. Ví dụ, ở 90% thưa thớt, việc sử dụng các tính phi tuyến
trong Sparse Factorized IFT mang lại tăng độ chính xác 1.8%
so với đường cơ sở dày đặc, trái ngược với giảm 0.5% khi không
có các tính phi tuyến. Những phát hiện này mở rộng đến tất cả
các thành viên Sparse-IFT (xem Phụ lục C.2 để biết chi tiết).
Những cải thiện độ chính xác ở tất cả các mức độ thưa thớt làm
nổi bật hiệu quả của việc kết hợp các kích hoạt phi tuyến trong
Sparse-IFT.

Độ thưa thớt Không có cấu trúc so với Có cấu trúc Chúng tôi
so sánh độ thưa thớt động không có cấu trúc và có cấu trúc sử
dụng Sparse-IFT. Độ thưa thớt không có cấu trúc khám phá tất
cả các biến thể mặt nạ, nhưng hầu hết các bộ tăng tốc phần cứng
không hỗ trợ tăng tốc thưa thớt không có cấu trúc. Các công
trình trước đây đã điều tra độ thưa thớt có cấu trúc, chẳng hạn
như các ma trận hạng thấp và khối thưa thớt, để tăng tốc thời
gian wall-clock (Khodak et al., 2020; Chen et al., 2021b; Hubara
et al., 2021; Dao et al., 2022). Chúng tôi khám phá độ thưa thớt
có cấu trúc thông qua các cấu hình Iso-FLOP với Sparse Wide
IFT, sử dụng phân tích hạng thấp và độ thưa thớt N:M để tăng
tốc GPU. Trong Hình 3 (biểu đồ trái), chúng tôi so sánh độ thưa
thớt động không có cấu trúc với độ thưa thớt có cấu trúc N:M
có thể chuyển vị (Hubara et al., 2021) sử dụng Sparse-IFT.
Cái sau chứng minh những cải thiện so với đường cơ sở dày đặc
ở mức 75% và 90% thưa thớt. Kết quả cũng chỉ ra rằng độ thưa
thớt khối N:M vượt trội hơn phân tích hạng thấp (xem Phụ lục
C.3.3). Tuy nhiên, độ thưa thớt không có cấu trúc vẫn mang lại
những cải thiện cao nhất, vì độ thưa thớt N:M có sự đa dạng
mặt nạ giảm trong các ma trận khối thưa thớt (Hubara et al., 2021),
do đó, chúng tôi áp dụng độ thưa thớt không có cấu trúc trong
tất cả các thí nghiệm tiếp theo.

Sparse-IFT ResNet-18 Chúng tôi đánh giá tất cả các thành viên
họ Sparse-IFT với ResNet-18 trên CIFAR-100 qua các mức độ
thưa thớt khác nhau. Biểu đồ giữa của Hình 3, làm nổi bật độ
chính xác tốt nhất đạt được bởi mỗi thành viên Sparse-IFT.
Tất cả các thành viên thể hiện những cải thiện độ chính xác
đáng kể so với đường cơ sở dày đặc (77%), sử dụng cùng FLOPs.
Sparse Wide nhất quán hoạt động tốt nhất, trong khi Sparse
Doped là thành viên duy nhất không đạt được độ chính xác ở
độ thưa thớt cao hơn. Điều này được quy cho việc Sparse Doped
duy trì không gian tìm kiếm không đổi bằng cách phân phối
FLOPs giữa các ma trận hạng thấp và thưa thớt không có cấu
trúc (xem Bảng 1), dẫn đến giảm trọng số hoạt động trong ma
trận không có cấu trúc. Trong Phụ lục C.3.1, chúng tôi so sánh
Sparse-IFT với các đường cơ sở DST khác dưới cùng thiết lập
hiệu quả huấn luyện bằng cách mở rộng các bước huấn luyện,
cho thấy Sparse-IFT vượt trội hơn chúng đáng kể ở s∈ {50%,75%,90%}.
Vì, Sparse Parallel và Sparse Wide hoạt động tốt nhất qua các
loại bỏ, chúng tôi sử dụng hai IFTs này cho các thí nghiệm chính.

Sparse-IFT so với Tham số hóa Quá mức Dày đặc Một yếu tố
quan trọng trong thành công của Sparse-IFT nằm ở việc khám
phá hiệu quả không gian tìm kiếm. Trong phần này, để đánh
giá việc khám phá này, chúng tôi thiết lập một giới hạn trên
bằng cách huấn luyện các kiến trúc Sparse-IFT theo cách dày
đặc (với các mức độ thưa thớt s∈50%, 75%, 90%). Trong Hình 3,
biểu đồ phải so sánh các phiên bản thưa thớt và dày đặc của
Sparse Wide và Sparse Parallel IFTs. Cả hai thành viên Sparse-IFT
đều xuất sắc trong việc khám phá một không gian tìm kiếm lớn
với DST, đạt được độ chính xác tương đương với các đối tác dày
đặc của chúng mà không có chi phí tính toán. Những kết quả này
làm nổi bật rằng việc tìm kiếm thưa thớt trong DST tiếp cận tối
ưu và có thể đạt được độ chính xác tương đương với các mô hình
được huấn luyện dày đặc. Hiệu quả này không làm giảm độ chính
xác và cung cấp những lợi ích tính toán đáng kể, đặc biệt trên
phần cứng được tối ưu hóa cho độ thưa thớt (thảo luận thêm
trong Phần 6).

Kiến trúc Hiệu quả Để đánh giá tính mạnh mẽ của Sparse-IFT
qua tập hợp đa dạng các mô hình, chúng tôi đánh giá nó trên các
kiến trúc được tối ưu hóa cho suy luận hiệu quả (MobileNetV2
(Sandler et al., 2018) và MobileViT (Mehta & Rastegari, 2021))
và huấn luyện hiệu quả (BotNet (Srinivas et al., 2021)). Áp
dụng Sparse Wide IFT cho các lớp dày đặc cải thiện đáng kể
độ chính xác kiểm tra qua tất cả các kiến trúc (tham khảo Bảng 3).
Tương tự, việc sử dụng Sparse Parallel IFT nhất quán tăng cường
hiệu suất qua tất cả các kiến trúc (xem Phụ lục C.3.2). Chúng tôi
đánh giá mô hình hoạt động tốt nhất, BotNet-50, trên ImageNet,
nơi biến thể Sparse-IFT vượt trội hơn dày đặc 1% (xem Phần 5.1).
Chúng tôi cung cấp chi tiết thiết lập thí nghiệm bổ sung trong
Phụ lục C.1. Tóm lại, Sparse-IFT cải thiện đáng kể độ chính xác
kiểm tra qua tất cả các kiến trúc hiệu quả, chứng minh tính mạnh
mẽ và hiệu quả của nó.

4. Phân tích Phổ của DST trong Sparse-IFT

Trong nghiên cứu này, chúng tôi điều tra các tính chất phức tạp
của các mạng Sparse-IFT và động lực huấn luyện của chúng.
Chúng tôi phân tích lợi ích của các mạng Sparse-IFT được huấn
luyện với DST bằng cách phân tích các đặc tính Ramanujan Gap
và Spectral Gap. Các cấu trúc đồ thị Ramanujan mà được biết
đến là thể hiện độ thưa thớt và kết nối cao như các đồ thị mở
rộng, được điều tra để tiết lộ mối tương quan của chúng với hiệu
suất cuối cùng của các mạng thưa thớt. Phân tích của chúng tôi
đánh giá tác động của các tham số mô hình và kết nối đồ thị đến
hiệu quả của DNNs với Sparse-IFTs, nhằm cung cấp những hiểu
biết sâu sắc về động lực huấn luyện của các mô hình Sparse-IFT.
Được lấy cảm hứng từ Hoang et al. (2023b;a), trong phân tích
này, chúng tôi diễn giải mô hình ResNet-18 như một loạt các
đồ thị tính toán hai phía, trong đó mỗi lớp, {θ1, . . . , θ L}
trong một DNN thưa thớt L lớp, có dạng của một ma trận kề
vuông A. Hoang et al. (2023b) đề xuất một số chỉ số đồ thị
được lấy cảm hứng từ các tính chất Ramanujan để đặc trưng
cho các mạng thưa thớt, thông qua: 1) Ramanujan Gap:
∆r= 2∗√d−1−μ̂(A), và ∆rimdb=1/|K|∑|K|i=1(2√di−1−μ̂(AKi)),
trong đó d là cạnh trung bình trên mỗi nút, và μ̂(A) là eigenvalue
không tầm thường của A. Ở đây, ∆r là quan điểm thông thường
của việc đo

0.26 0.28 0.30 0.32
imsg
0.38
0.40
0.42
0.44
0.46rimdb
Sparse Wide IFT (s=50%)
0.9410.8520.7530.6640.5650.4760.3870.28Độ chính xác Kiểm tra (%)

0.27 0.28 0.29 0.30
imsg
0.35
0.37
0.38
0.40
0.41rimdb
Sparse Parallel IFT (s=50%)
0.9410.8520.7530.6640.5650.4760.3870.28Độ chính xác Kiểm tra (%)

15.53
 9.27
 3.02
 3.24 9.49
11.98
5.47
1.037.5414.04r
Sparse Wide IFT (s=50%)
0.9410.8520.7530.6640.5650.4760.3870.28Độ chính xác Kiểm tra (%)

Hình 4: Mối quan hệ giữa cấu trúc và trọng số của các mạng
Sparse-IFT ResNet-18 được phân tích thông qua góc độ đồ thị
về hiệu suất. Hàng trên: chúng tôi đánh giá mối quan hệ giữa
∆rimdb và λimsg. Hàng dưới: điều tra mối tương quan giữa
∆r và λ. Bản đồ nhiệt cong Pareto biểu diễn trực quan hiệu
suất phân loại, với các gradient màu sắc khác nhau tượng trưng
cho phổ từ độ chính xác kiểm tra thấp đến cao trên CIFAR-100.

khoảng cách giữa giới hạn trên Ramanujan 2∗√d−1 và μ̂(A).
∆r đo mức độ kết nối của mạng để tiết lộ dòng chảy lan truyền
thông tin. ∆rimdb (Hoang et al., 2023b), Iterative Mean Difference
Bound (imdb), đánh giá ranh giới kết nối trung bình qua tất cả
các đồ thị con K trong A. Một ∆r cao hơn trong các mạng thưa
thớt có nghĩa là dòng chảy thông tin hiệu quả, lan truyền gradient,
và một phổ được phân tách tốt trong ma trận kề của các trọng số
thưa thớt; chỉ ra biểu diễn mạnh mẽ và hiệu quả. Ngoài ra, một
∆rimdb tăng chỉ ra các ranh giới kết nối rộng lớn hơn trong
các đồ thị con, tăng cường giao tiếp giữa các nút và thúc đẩy
các kết nối mạnh mẽ hơn. 2) Weighted Spectral Gap:
λ=μ0(|W|)−μ̂(|W|), và λimsg=1/|K|∑|K|i=1(μ0(|WKi|)−μ̂(|WKi|)).
Ở đây, khoảng cách giữa μ0, các tham số tầm thường, và μ̂,
các eigenvalues không tầm thường của W, ma trận kề có trọng số,
được ký hiệu là λ, khoảng cách phổ có trọng số. Sau đó, λimsg
(Hoang et al., 2023a) là phiên bản lặp mà tính đến tất cả các
đồ thị con K trong W. Một λimsg cao hơn chỉ ra sự phân tách
phổ được tăng cường giữa μ0 và μ̂ của W, ngụ ý một cấu trúc
phổ khác biệt và được định nghĩa rõ ràng hơn trong các đồ thị con.
Sự phân tách được cải thiện trong phổ này, được biểu diễn bởi
một λ cao hơn, tạo điều kiện thuận lợi cho việc cô lập tốt hơn
các tín hiệu có ý nghĩa. Chúng tôi huấn luyện các mô hình Sparse
Wide và Sparse Parallel ResNet-18 ở 50% thưa thớt trên CIFAR-100.
Sau đó, chúng tôi tạo ra một bản đồ nhiệt cong Pareto, xem xét
độ lớn trọng số và chi tiết cấu trúc tô-pô đồ thị (xem Hình 4).
Xem Phụ lục B để có phân tích chi tiết.

Phân tích ∆rimdb và λimsg: Trong các giai đoạn đầu đến giữa
của huấn luyện, việc cắt tỉa và tái sinh động của RigL tăng
∆rimdb cho mạng để khám phá các mẫu kết nối đa dạng (xem
hàng trên, Hình 4). Việc cắt tỉa tiếp theo loại bỏ các kết nối
ít quan trọng hơn, đa dạng hóa các đồ thị con trong ma trận kề A.
Các giai đoạn sau chứng kiến sự giảm ∆rimdb khi mạng hội tụ
đến các mẫu kết nối tập trung và có tổ chức hơn. RigL ưu tiên
các kết nối quan trọng, khai thác một cấu trúc đồ thị con hiệu
quả liên kết với các vùng độ chính xác cao của các mô hình
Sparse-IFT. Sớm trong huấn luyện, λimsg tăng gợi ý việc cô
lập thành công các chế độ khác nhau. Việc cắt tỉa dẫn đến sự
phân tách khác biệt giữa các chế độ thống trị và ít thống trị hơn.
Sự giảm λimsg tiếp theo báo hiệu sự hội tụ của mạng đến một
biểu diễn chuyên biệt hơn, nhấn mạnh các thành phần phổ chính
và giảm bớt ảnh hưởng của các chế độ ít quan trọng hơn. Trong
khi cả Sparse Wide và Sparse Parallel IFTs đều cho thấy ∆rimdb
tăng, tính đa dạng không gian tìm kiếm lớn hơn trong Sparse
Wide tạo điều kiện cho sự xuất hiện của các cấu trúc đồ thị con
đa dạng trong mỗi lớp, cho phép một tập hợp kết nối phong phú
hơn giữa các nút; dẫn đến một ∆rimdb tối đa cao hơn. Tương tự,
Sparse Wide có λimsg tối đa cao hơn so với Sparse Parallel,
chỉ ra sự xuất hiện của các đồ thị con với các tính chất phổ
khác biệt hơn.

Phân tích ∆r và λ: Hàng dưới của Hình 4 tiết lộ một mối
tương quan mạnh giữa ∆r và λ. ∆r ban đầu giảm, chỉ ra sự
nới lỏng tạm thời các ràng buộc phổ trong quá trình cắt tỉa
động với RigL. Tiếp theo, nó tối đa hóa trong các giai đoạn
huấn luyện cuối, có nghĩa là khả năng của RigL hướng dẫn
mạng tái tổ chức kết nối của mình, thúc đẩy các đặc tính phổ
có cấu trúc và thuận lợi hơn. Tương tự, λ theo một mẫu giảm
ban đầu và tối đa hóa sau đó. Điều này ngụ ý rằng độ thưa thớt
động của RigL ban đầu dẫn đến tổ chức trọng số ít tối ưu hơn
liên quan đến các tính chất phổ. Tuy nhiên, việc cắt tỉa và nối
lại lặp đi lặp lại của RigL thích ứng động mạng, căn chỉnh trọng
số để tăng cường các đặc tính phổ và tăng khoảng cách phổ.
Phân tích của chúng tôi chứng minh rằng DST, như được minh
họa bởi RigL, vượt trội hơn huấn luyện thưa thớt tĩnh bằng cách
tối ưu hóa các đặc tính phổ cho Sparse-IFT; tạo điều kiện thuận
lợi cho các mẫu kết nối được cải thiện và một hồ sơ phổ thuận
lợi hơn.

5. Đánh giá Thực nghiệm

Dựa trên những hiểu biết thu được từ các loại bỏ của chúng tôi
được thảo luận trong Phần 3.2, chúng tôi áp dụng Sparse-IFTs
cho ImageNet, đồng thời chứng minh những lợi thế của nó cho
học chuyển giao trong các nhiệm vụ thị giác máy tính khác nhau.
Ngoài ra, chúng tôi làm nổi bật những lợi ích của Sparse-IFT
trong lĩnh vực NLP bằng cách trình bày kết quả về tiền huấn
luyện GPT (Brown et al., 2020).

5.1. ImageNet và Học Chuyển giao

Chúng tôi áp dụng các biến đổi Sparse-IFT hoạt động tốt nhất
(Sparse Wide IFT và Sparse Parallel IFT) từ CIFAR-100 cho
ImageNet sử dụng ResNet-18. Chúng tôi tuân theo các cài đặt
huấn luyện đã công bố cho ImageNet (Nvidia, 2023). Cả hai
họ Sparse-IFT đều đạt được độ chính xác cao hơn đáng kể so với

Bảng 4: Sparse-IFT trên ImageNet. Kết quả tốt nhất cho mỗi
biến đổi và kiến trúc được tô đậm.

MÔ HÌNH      DÀY ĐẶC    BIẾN ĐỔI         0.50    0.75    0.90
RESNET-18    70.9±0.1   SPARSE WIDE      72.7±0.1 73.8 ±0.2 74.4±0.2
                        SPARSE PARALLEL   72.7±0.2 73.2 ±0.2 74.0±0.2
RESNET-34    74.2±0.1   SPARSE WIDE      75.6±0.2 76.4 ±0.1 76.8±0.3
BOTNET-50    77.5±0.1   SPARSE WIDE      77.9±0.2 78.3 ±0.2 78.6±0.3

Bảng 5: Các biến thể Sparse Wide IFT của ResNet-18 làm
backbone cho: (a) phát hiện đối tượng trên MS COCO, (b)
phân đoạn ngữ nghĩa trên Cityscapes.

METRIC       DÀY ĐẶC    0.50    0.75    0.90
MS COCO      AP         29.3±0.1 31.3 ±0.1 32.8 ±0.2 34.5±0.2
             AP50       46.2±0.2 49.0 ±0.2 51.0 ±0.2 53.5±0.2
             AP75       30.9±0.2 33.0 ±0.2 34.8 ±0.2 36.5±0.3
CITYSCAPES   MIOU       76.7±0.2 77.9 ±0.2 78.9 ±0.2 79.1±0.2
             MACC       84.4±0.2 85.1 ±0.2 85.7 ±0.2 86.0±0.2

đường cơ sở dày đặc (xem Bảng 4). Cụ thể, Sparse Wide IFT
ResNet-18 ở 90% thưa thớt cải thiện so với đường cơ sở dày đặc
3.5% và khớp với độ chính xác của một ResNet-34 dày đặc với
2× ít FLOPs huấn luyện hơn (tham khảo Hình 1). Chúng tôi cũng
áp dụng biến đổi hoạt động tốt nhất (Sparse Wide IFT) cho
ResNet-34 và BotNet-50. Việc tăng độ thưa thớt nhất quán cải
thiện độ chính xác, chỉ ra hiệu quả huấn luyện được tăng cường
ở độ thưa thớt cao hơn. Trên BotNet-50, một mô hình ViT lai,
có sự cải thiện 1.1% ở 90% thưa thớt.

Học Chuyển giao trên Downstream Để cho thấy hiệu quả của
việc tiền huấn luyện các backbone phân loại Sparse-IFT của
chúng tôi, chúng tôi đánh giá chúng trên 1) phát hiện đối tượng
trên MS COCO 2017 (Lin et al., 2014b), và 2) phân đoạn ngữ
nghĩa trên CityScapes (Cordts et al., 2016). Đối với phát hiện
đối tượng, chúng tôi áp dụng RetinaNet (Lin et al., 2017b) từ
hộp công cụ mã nguồn mở MMDetection (Chen et al., 2019)
và báo cáo kết quả trong cài đặt huấn luyện tiêu chuẩn hóa.
Đối với phân đoạn ngữ nghĩa, chúng tôi sử dụng DeepLabV3+
(Chen et al., 2018) trong hộp công cụ mã nguồn mở MMSegmenation
(Contributors, 2020). Chúng tôi đánh giá ResNet-18 với Sparse
Wide IFT và để đảm bảo so sánh tương đương FLOP với backbone
dày đặc, các backbone Sparse-IFT vẫn thưa thớt trong quá trình
tinh chỉnh. Phụ lục C.3.4 cung cấp thêm chi tiết về thiết lập
huấn luyện. Chúng tôi tóm tắt các phát hiện của chúng tôi trong
Bảng 5, nơi việc sử dụng backbone Sparse Wide IFT ResNet-18
dẫn đến những cải thiện độ chính xác đáng kể qua tất cả các
chỉ số trên cả hai nhiệm vụ.

5.2. Mô hình hóa Ngôn ngữ

Chúng tôi tiền huấn luyện mô hình Sparse Wide IFT GPT-3 Small
ở s∈ {50%,75%} từ đầu trên tập dữ liệu Pile (Gao et al., 2020)
sử dụng SET (Mocanu et al., 2018), và so sánh với mô hình dày
đặc tiêu chuẩn. Tất cả các mô hình được huấn luyện trên Cerebras
CS-2 (Cerebras, 2023) theo Chinchilla (Hoffmann et al., 2022)
để có được các cấu hình đường cơ sở tiền huấn luyện tối ưu
về mất mát của các mô hình. Chúng tôi đánh giá các mô hình
trên 5 nhiệm vụ từ bảng xếp hạng Open LLM

--- TRANG 8 ---
Biến đổi Iso-FLOP Thưa thớt để Tối đa hóa Hiệu quả Huấn luyện

Bảng 6: Độ chính xác trung bình của Sparse Wide IFT với GPT-3
Small qua các nhiệm vụ ARC, HellaSwag, TruthfulQA, MMLU và
Winogrande trên Bảng xếp hạng Open LLM.

MÔ HÌNH      DÀY ĐẶC    0.50    0.75
GPT-3 SMALL  33.8±0.1   34.1±0.2 34.7±0.2

board (Beeching et al., 2023) (tức là ARC (Clark et al., 2018),
HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021),
TruthfulQA (Lin et al., 2022) và Winogrande (Sakaguchi et al.,
2019)), và cho thấy rằng Sparse Wide IFT GPT-3 Small ở 75%
thưa thớt cải thiện độ chính xác trung bình đáng chú ý 0.9%
(xem Bảng 6). Trong Phụ lục D.1, chúng tôi cung cấp chi tiết
về các mô hình và siêu tham số.

6. Tăng tốc Wall-Clock với Sparse-IFT

Các nghiên cứu của chúng tôi trong Phần 5 cho thấy hiệu quả
huấn luyện được cải thiện đáng chú ý (độ chính xác kiểm tra
so với FLOPs huấn luyện) cho các mô hình Sparse-IFT. Trong
phần này, chúng tôi nhằm trình bày tính thực tế của các mô hình
Sparse-IFT, cung cấp những hiểu biết độc đáo về phần cứng
để tăng tốc DNNs với độ thưa thớt không có cấu trúc, một góc
nhìn đáng chú ý vắng mặt trong hầu hết các công trình hiện có.
Các phát triển gần đây, như các kernel phần mềm chuyên biệt
và phần cứng (ví dụ: DeepSparse (NeuralMagic, 2021) và Cerebras
CS-2 (Lie, 2023)) chỉ ra những cải thiện đầy hứa hẹn trong
việc nhận ra lợi ích độ thưa thớt không có cấu trúc trong quá
trình huấn luyện và suy luận (Thangarasa et al., 2023). Điều
này tạo ra bối cảnh để kiểm tra tác động đến tăng tốc suy luận
và huấn luyện.

Tăng tốc Suy luận Thực tế Chúng tôi đánh giá hiệu quả suy
luận của Sparse-IFT sử dụng DeepSparse¹. Thiết lập của chúng
tôi sử dụng mô hình ResNet-18 và thực hiện suy luận theo batch
của 64 hình ảnh từ ImageNet ở độ phân giải 224×224 trên Intel
Cascade Lake CPUs, được biết đến với hỗ trợ AVX-512 của chúng.
Độ trễ (tức là giây trên mỗi batch) được so sánh giữa mô hình
ResNet-18 dày đặc và các biến thể Sparse Wide IFT ở s∈{50%, 75%, 90%}.
Trên phần cứng lý tưởng, FLOPs nên dịch trực tiếp thành thời
gian wall clock. Do đó, độ trễ suy luận hoặc thời gian huấn luyện
cho tất cả các mô hình Sparse-IFT nên khớp với mô hình dày đặc,
vì tất cả các mô hình đều là Iso-FLOP. Đường cơ sở này được
minh họa bởi đường nét đứt đen trong biểu đồ trái của Hình 5.
Tuy nhiên, đường màu xanh lam cho thấy sự tăng dự kiến trong
độ trễ trên phần cứng không có hỗ trợ tăng tốc thưa thớt không
có cấu trúc, như các CPU chúng tôi đánh giá, với sự tăng đáng
chú ý 19.5x ở s=90%. Ngược lại, đường màu xanh lá cây chứng
minh sự giảm đáng kể trong độ trễ sử dụng DeepSparse, giảm
sự tăng độ trễ từ 19.5x xuống 3.5x, và cho thấy chi phí tối thiểu
lên đến 75% thưa thớt. Điều này nhấn mạnh lợi ích của hỗ trợ
kernel được tối ưu hóa cho tăng tốc suy luận thưa thớt, trình
bày tiềm năng cho triển khai thực tế của các mô hình Sparse-IFT.

¹Neural Magic DeepSparse

50% 75% 90%
Độ thưa thớt1.02.55.07.510.012.515.017.520.0Tăng tương đối 
 trong độ trễ (giây/batch) so với Dày đặc)
Lý thuyết (FLOPs Thưa thớt)
Không có Hỗ trợ Thưa thớt
Có Hỗ trợ Thưa thớt

50% 75% 90%
Độ thưa thớt1.02.55.07.510.0Tăng tương đối 
 trong thời gian huấn luyện (giây/iter) so với Dày đặc
Lý thuyết (FLOPs Thưa thớt)
Không có Hỗ trợ Thưa thớt
Có Hỗ trợ Thưa thớt

Hình 5: Đánh giá độ thưa thớt không có cấu trúc trong (trái)
suy luận trên runtime Neural Magic's DeepSparse và (phải)
tăng tốc huấn luyện trên Cerebras CS-2. Trong cả hai thiết lập,
chúng tôi đo sự tăng tương đối trong độ trễ hoặc tốc độ huấn
luyện cho các biến thể Sparse-IFT so với mô hình dày đặc.

Tăng tốc Huấn luyện Thực tế Trong biểu đồ phải của Hình 5,
chúng tôi đánh giá hiệu quả huấn luyện của Sparse-IFT trên
hệ thống Cerebras CS-2, mà hỗ trợ huấn luyện thưa thớt không
có cấu trúc cho LLMs². Thiết lập thí nghiệm của chúng tôi bao
gồm tiền huấn luyện một mô hình GPT-3 trên CS-2. Chúng tôi
đo và so sánh thông lượng (tức là lần lặp trên giây), giữa mô
hình GPT-3 dày đặc và các biến thể Sparse Wide IFT ở các mức
độ thưa thớt 50%, 75%, và 90%. Như đã đề cập trước đây, đường
cơ sở lý thuyết (đường nét đứt đen) gợi ý rằng vì cả mô hình
dày đặc và các cấu hình Sparse Wide IFT đều là Iso-FLOP,
thời gian huấn luyện không nên tăng với độ thưa thớt tăng.
Đường màu xanh lam cho thấy thông lượng cho các biến thể
Sparse Wide IFT chạy mà không có bất kỳ hỗ trợ tăng tốc thưa
thớt không có cấu trúc nào trên Cerebras CS-2, chỉ ra sự tăng
∼10x trong thời gian huấn luyện cho mô hình ở 90% thưa thớt.
Một sự suy giảm hiệu suất tương tự sẽ được mong đợi trên
phần cứng Nvidia GPU hoặc Google TPU truyền thống. Ngược
lại, đường màu xanh lá cây chứng minh hiệu ứng của việc sử
dụng hỗ trợ huấn luyện thưa thớt không có cấu trúc của Cerebras
CS-2. Ở đây, chúng tôi quan sát sự giảm đáng kể trong thời
gian huấn luyện tương đối, giảm sự tăng từ ∼10x xuống 2.82x
ở 90% thưa thớt. Ngoài ra, đối với các mức độ thưa thớt lên
đến 75%, chúng tôi ghi nhận chi phí tối thiểu so với mô hình
dày đặc. Chi tiết thiết lập đánh giá có trong Phụ lục E.

Trong khi chúng tôi không đạt được dịch FLOPs hoàn hảo với
các mô hình Sparse-IFT, kết quả đầy hứa hẹn của chúng tôi
làm nổi bật tầm quan trọng của việc đồng thiết kế phần mềm
và phần cứng ML để tận dụng độ thưa thớt. Sự tương tác của
chiều lớp, độ thưa thớt, và chi phí, bị ảnh hưởng bởi kiến trúc
phần cứng, đòi hỏi các kỹ thuật thưa thớt được đồng thiết kế
để có hiệu suất tối ưu. Công việc của chúng tôi trình bày những
tiến bộ thuật toán so với các phương pháp thưa thớt trước đây,
nhấn mạnh lợi ích của huấn luyện thưa thớt và thắng xổ số
phần cứng (Hooker, 2020).

7. Công việc Liên quan

Công việc của chúng tôi phù hợp với nghiên cứu về tham số hóa
quá mức và độ thưa thớt trong huấn luyện DNN. Khả năng mô
hình hóa cần thiết cho một nhiệm vụ cho trước thường không
được biết, dẫn đến huấn luyện các mô hình được tham số hóa
quá mức để khai thác đầy đủ khả năng học trước khi nén chúng
thành các mạng con nhỏ hơn.

Tham số hóa Quá mức Nakkiran et al. (2021) cho thấy rằng
DNNs có lợi từ tham số hóa quá mức. Tiếp theo, một số nghiên
cứu đã tận dụng tham số hóa quá mức bằng cách mở rộng kích
thước của các mô hình (Rae et al., 2021; Goyal et al., 2022)
và tăng cường các DNNs hiện có để tăng cường khả năng mô
hình hóa và độ chính xác của các mạng được huấn luyện (Guo
et al., 2020b; Ding et al., 2019; 2021b; Cao et al., 2022; Vasu
et al., 2022; Liu et al., 2022a). Những phương pháp này sử
dụng các tham số hóa tuyến tính của mô hình, làm cho chúng
rất không hiệu quả để huấn luyện, và tập trung vào cải thiện
thông lượng suy luận. Ngược lại, công việc của chúng tôi tập
trung vào cải thiện khả năng mô hình hóa sử dụng các tham số
hóa phi tuyến thưa thớt. Sparse-IFT tăng cường độ chính xác
mà không tăng FLOPs huấn luyện và suy luận so với mô hình
dày đặc cơ sở.

Huấn luyện Mạng Thưa thớt Giả thuyết Vé số Trúng thưởng
(Frankle & Carbin, 2018; Frankle et al., 2020) cho thấy rằng
các mạng con thưa thớt chính xác tồn tại trong các mạng dày
đặc được tham số hóa quá mức nhưng đòi hỏi huấn luyện một
đường cơ sở dày đặc để tìm thấy. Các phương pháp khác đã
đề xuất các khung để xác định vé số trúng thưởng (Zhou et al.,
2019; Ma et al., 2022) nhưng vẫn đòi hỏi nhiều tài nguyên tính
toán. Tiếp theo điều này, nhiều nỗ lực đã được thực hiện để
tìm mạng con thưa thớt tối ưu trong một lần bắn. Những phương
pháp này hoặc cố gắng tìm các mạng con tại khởi tạo (Tanaka
et al., 2020; Wang et al., 2020a; de Jorge et al., 2020; Lee
et al., 2018) hoặc động trong quá trình huấn luyện (Mocanu
et al., 2018; Evci et al., 2020; Jayakumar et al., 2020; Raihan
& Aamodt, 2020). Tuy nhiên, với khả năng mô hình cố định,
những phương pháp này đánh đổi độ chính xác so với đường
cơ sở dày đặc để tiết kiệm FLOPs huấn luyện. Stosic & Stosic
(2021) và Ramanujan et al. (2020) tăng không gian tìm kiếm
trong quá trình huấn luyện thưa thớt để duy trì độ chính xác;
tuy nhiên, không đảm bảo tiết kiệm FLOPs. Ngược lại với những
phương pháp này, công việc của chúng tôi giới thiệu một tập
hợp các biến đổi thưa thớt phi tuyến, mà tăng khả năng biểu
diễn của mạng. Phương pháp này không giới thiệu một thuật
toán huấn luyện thưa thớt mới, mà thay vào đó cải thiện không
gian tìm kiếm của các phương pháp hiện có, dẫn đến khái quát
hóa được cải thiện trong khi hiệu quả để huấn luyện.

Iso-Parameter so với Iso-FLOP Các công trình gần đây đã
tập trung vào cải thiện khái quát hóa ở các mức độ thưa thớt
cao. Các kỹ thuật như Erdős-Rényi-Kernel (Evci et al., 2020),
Ideal Gas Quota (Chen et al., 2022), và parameter leveling
(Golubeva et al., 2021) sử dụng các phân phối độ thưa thớt
theo lớp trong huấn luyện thưa thớt để tăng cường độ chính xác.
Tuy nhiên, những phương pháp này nhắm đến các tình huống
nơi các mô hình có ngân sách tham số cố định (tức là Iso-Parameter),
mà không tương đương với FLOPs huấn luyện tương tự như mô
hình dày đặc ban đầu. Công việc của chúng tôi làm nổi bật
rằng trong khi các mạng NLP dựa trên transformer có thể không
cho thấy sự khác biệt đáng kể giữa tối ưu hóa Iso-Parameter
và Iso-FLOP, sự phân biệt này trở nên quan trọng trong các
mạng CV. Trong CNNs và ViTs không đồng nhất (Wang et al.,
2021; Pan et al., 2021; Wu et al., 2021), sự phân phối không
đồng đều của tham số và chi phí tính toán qua các lớp đòi hỏi
một phương pháp khác biệt. Tối ưu hóa cho Iso-Parameter thường
bao gồm cắt tỉa các lớp sau, giàu tham số, do đó duy trì hiệu
suất nhưng không giảm đáng kể chi phí tính toán. Ngược lại,
tối ưu hóa cho Iso-FLOP chuyển việc cắt tỉa đến các lớp đầu,
tốn FLOP, tăng cường hiệu suất bằng cách giải quyết cả nhu
cầu tính toán và cắt tỉa. Không giống như các kỹ thuật độ thưa
thớt biến đổi mà thích ứng với các nhu cầu tính toán và bộ nhớ
khác nhau qua các lớp, phương pháp của chúng tôi sử dụng một
phương pháp độ thưa thớt đồng nhất, đảm bảo giảm FLOP nhất
quán qua tất cả các lớp. Điều này căn chỉnh chi phí tính toán
chặt chẽ với của một mô hình hoàn toàn dày đặc, đạt được hiệu
quả tính toán đáng kể mà không làm giảm hiệu suất.

Sparse-IFT và Quy luật Mở rộng Những tiến bộ gần đây trong
học sâu làm nổi bật tầm quan trọng của các quy luật mở rộng,
mà cung cấp một khung hệ thống để tối ưu hóa hiệu suất mô
hình khi kích thước mô hình tăng. Các công trình quy luật mở
rộng tiên phong như ConvNeXt (Liu et al., 2022d), EfficientNet
(Tan & Le, 2019), các mô hình ngôn ngữ lớn (Kaplan et al., 2020),
và vision transformers (Alabdulmohsin et al., 2023) chứng minh
rằng việc đạt được hiệu suất tối ưu thường bao gồm điều chỉnh
nhiều siêu tham số huấn luyện (ví dụ: tốc độ học, kích thước
batch, v.v.) và kiến trúc (ví dụ: độ sâu, độ rộng, độ phân giải,
v.v.). Sự cân bằng phức tạp này đòi hỏi thí nghiệm rộng rãi
và điều chỉnh siêu tham số. Sparse-IFT giới thiệu một phương
pháp hợp lý để mở rộng DNNs, tận dụng một siêu tham số duy
nhất, mức độ thưa thớt, để tăng cường hiệu quả và độ chính
xác mô hình. Phương pháp này đơn giản hóa quá trình tối ưu
hóa bằng cách loại bỏ nhu cầu điều chỉnh nhiều yếu tố đồng
thời. Nghiên cứu tương lai sẽ khám phá việc tích hợp Sparse-IFT
với các quy luật mở rộng để giải quyết các thách thức của việc
mở rộng các mô hình lớn. Điều này bao gồm kiểm tra sự tương
tác giữa Sparse-IFT và các yếu tố kiến trúc khác nhau, chẳng
hạn như độ sâu và độ rộng, trong khi duy trì một ngân sách
FLOP tính toán không đổi.

8. Kết luận

Chúng tôi đã giới thiệu Sparse-IFT như một thay thế trực tiếp
cho các lớp dày đặc trong DNNs, tăng cường độ chính xác kiểm
tra so với FLOPs huấn luyện bằng cách tăng khả năng biểu diễn
thông qua độ thưa thớt. Không gian trọng số mở rộng cho phép
khám phá và khai thác hiệu quả bởi các thuật toán DST, tạo
điều kiện thuận lợi cho việc khám phá các mạng con thưa thớt
tối ưu. Phân tích phổ của chúng tôi về các mô hình Sparse-IFT
được huấn luyện với DST tiết lộ kết nối và lan truyền thông
tin hiệu quả, tương quan với các mạng hiệu suất cao. Đáng
chú ý, Sparse-IFT nhất quán vượt trội hơn các mô hình dày
đặc trong các lĩnh vực thị giác và NLP.

Mặc dù có những hạn chế phần cứng hiện tại, các đánh giá
đầy hứa hẹn trên Cerebras CS-2 và runtime Neural Magic
DeepSparse làm nổi bật nhu cầu cải thiện hỗ trợ cho độ thưa
thớt trọng số không có cấu trúc. Chúng tôi hy vọng các phát
hiện của chúng tôi khuyến khích cộng đồng khám phá độ thưa
thớt không có cấu trúc để cải thiện hiệu quả và hiệu suất mô
hình qua các ứng dụng.

--- TRANG 10 ---
Biến đổi Iso-FLOP Thưa thớt để Tối đa hóa Hiệu quả Huấn luyện

Lời cảm ơn
Chúng tôi cảm ơn Anshul Samar vì những bình luận và chỉnh
sửa hữu ích đã cải thiện bản thảo của chúng tôi. Chúng tôi cũng
cảm ơn Chen-Yu Kevin Leong vì đã hỗ trợ với các thí nghiệm
GPT-3 trên Cerebras CS-2 và Dylan Finch vì các đánh giá hiệu
suất trên CS-2. Cuối cùng, chúng tôi cung cấp chi tiết về đóng
góp của mỗi tác giả trong Phụ lục F.

Tuyên bố Tác động

Cảnh quan học máy (ML) đã chứng kiến sự tăng trưởng theo
cấp số nhân trong các mô hình, đặc biệt là trong các lĩnh vực
như xử lý ngôn ngữ tự nhiên và thị giác máy tính. Tuy nhiên,
sự gia tăng kích thước mô hình này đã đi kèm với chi phí đáng
kể về yêu cầu tính toán, bộ nhớ và năng lượng. Phương pháp
của chúng tôi, Biến đổi Iso-FLOP Thưa thớt (Sparse-IFT),
đại diện cho một bước tiến đáng kể hướng tới giảm thiểu những
nhu cầu tiêu tốn tài nguyên này. Sparse-IFT giới thiệu một
phương pháp mới nâng cao hiệu quả huấn luyện các mạng nơ-ron
lớn. Đáng chú ý, nó đạt được độ chính xác được cải thiện trong
khi duy trì cùng FLOPs như mô hình đường cơ sở dày đặc ban đầu.
Phương pháp của chúng tôi hứa hẹn cho những tác động môi
trường tích cực, với các tài nguyên tính toán đáng kể thường
liên quan đến huấn luyện các mạng nơ-ron lớn. Các mô hình
được huấn luyện với Sparse-IFT yêu cầu ít tài nguyên tính
toán và năng lượng hơn để đạt được chất lượng mô hình cao
hơn, dịch trực tiếp thành chi phí triển khai thấp hơn cho các
ứng dụng thực tế. Hơn nữa, huấn luyện các mô hình với độ
thưa thớt đã được chứng minh là dẫn đến khái quát hóa tốt
hơn (Chen et al., 2022), một lợi ích được hỗ trợ bởi kết quả
học chuyển giao của chúng tôi trên các nhiệm vụ thị giác máy
tính. Một lợi thế bổ sung là hiệu quả được tăng cường trong
việc huấn luyện các mô hình thưa thớt lớn hơn, được tạo điều
kiện thuận lợi bởi việc áp dụng rộng rãi phần cứng AI, chẳng
hạn như Cerebras CS-2, mà tăng tốc độ thưa thớt không có
cấu trúc. Chìa khóa ở đây là đạt được tăng tốc độ thưa thớt
thông qua sự hợp tác hài hòa giữa hỗ trợ phần cứng và phát
triển các kỹ thuật ML thưa thớt.

Đóng góp tiềm năng về tính bền vững nằm ở chỗ, khi đồng
thiết kế phần mềm và phần cứng ML thưa thớt tiếp tục phát
triển, chúng ta có thể có khả năng huấn luyện các mạng "thưa
thớt lớn hơn" chính xác hơn trong giới hạn của cùng ngân sách
tính toán như một mô hình dày đặc nhỏ hơn. Sự thay đổi mô
hình này có thể mở ra một phương pháp có ý thức môi trường
hơn đối với học sâu, giải quyết những lo ngại liên quan đến
yêu cầu tài nguyên leo thang của các mô hình mở rộng không
ngừng. Sự tích hợp liền mạch của những yếu tố này đảm bảo
rằng các kiến trúc phần cứng được tối ưu hóa để bổ sung cho
các kỹ thuật thưa thớt, thúc đẩy một quỹ đạo bền vững và
hiệu quả cho tương lai của học sâu.

Khi chúng ta tiếp tục khám phá giao điểm của hỗ trợ phần
cứng cho độ thưa thớt và sự phát triển của các kỹ thuật ML
thưa thớt, phân tích đánh giá của chúng tôi trong Phần 6 và
Phụ lục E phục vụ như một minh họa thực tế về tiềm năng biến
đổi của Sparse-IFT. Nó không chỉ chứng thực những lời hứa
lý thuyết mà còn cung cấp một lộ trình cho các phát triển tương
lai trong việc theo đuổi các thực hành học sâu bền vững và
hiệu quả.

Tài liệu tham khảo

Alabdulmohsin, I., Zhai, X., Kolesnikov, A., and Beyer, L.
Getting vit in shape: Scaling laws for compute-optimal
model design. In NeurIPS, 2023.

Arora, S., Cohen, N., and Hazan, E. On the optimization of
deep networks: Implicit acceleration by overparameteri-
zation. In ICML, 2018.

Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert,
N., Rajani, N., Sanseviero, O., Tunstall, L., and Wolf,
T. Open llm leaderboard. https://huggingface.co/spaces/
HuggingFaceH4/open llmleaderboard, 2023.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
In NeurIPS, 2020.

Candès, E. J., Li, X., Ma, Y., and Wright, J. Robust principal
component analysis? Journal of the ACM, 2011.

Cao, J., Li, Y., Sun, M., Chen, Y., Lischinski, D., Cohen-Or,
D., Chen, B., and Tu, C. Do-conv: Depthwise over-
parameterized convolutional layer. IEEE Transactions on
Image Processing, 2022.

Cerebras. Train a model with weight sparsity. Cere-
bras Wafer-Scale cluster (R2.1.1) Documentation, Jan
2023. URL https://docs.cerebras.net/en/2.1.1/wsc/how
toguides/sparsity.html.

Chen, B., Dao, T., Winsor, E., Song, Z., Rudra, A., and Ré,
C. Scatterbrain: Unifying sparse and low-rank attention
approximation. In NeurIPS, 2021a.

Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun,
S., Feng, W., Liu, Z., Xu, J., Zhang, Z., Cheng, D., Zhu,
C., Cheng, T., Zhao, Q., Li, B., Lu, X., Zhu, R., Wu, Y.,
Dai, J., Wang, J., Shi, J., Ouyang, W., Loy, C. C., and Lin,
D. MMDetection: Open mmlab detection toolbox and
benchmark. arXiv, 2019.

Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., and Adam,
H. Encoder-decoder with atrous separable convolution
for semantic image segmentation. In ECCV, 2018.

Chen, P., Yu, H.-F., Dhillon, I., and Hsieh, C.-J. Drone:
Data-aware low-rank compression for large nlp models.
In NeurIPS, 2021b.

Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Wang,
Z., and Carbin, M. The lottery ticket hypothesis for pre-
trained bert networks. In NeurIPS, 2020.

Chen, T., Zhang, Z., pengjun wang, Balachandra, S., Ma, H.,
Wang, Z., and Wang, Z. Sparsity winning twice: Better
robust generalization from more efficient training. In
ICLR, 2022.

Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,
Schoenick, C., and Tafjord, O. Think you have solved
question answering? try arc, the ai2 reasoning challenge,
2018.

Contributors, M. MMSegmentation: Openmmlab semantic
segmentation toolbox and benchmark. https://github.com/
open-mmlab/mmsegmentation, 2020.

Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler,
M., Benenson, R., Franke, U., Roth, S., and Schiele, B.
The cityscapes dataset for semantic urban scene under-
standing. In CVPR, 2016.

Dao, T., Chen, B., Sohoni, N. S., Desai, A., Poli, M., Grogan,
J., Liu, A., Rao, A., Rudra, A., and Ré, C. Monarch:
Expressive structured matrices for efficient and accurate
training. In ICML, 2022.

de Jorge, P., Sanyal, A., Behl, H. S., Torr, P. H., Rogez, G.,
and Dokania, P. K. Progressive skeletonization: Trim-
ming more fat from a network at initialization. arXiv,
2020.

DeVries, T. and Taylor, G. W. Improved regularization of
convolutional neural networks with cutout. arXiv, 2017.

Ding, X., Guo, Y., Ding, G., and Han, J. Acnet: Strengthen-
ing the kernel skeletons for powerful cnn via asymmetric
convolution blocks. In ICCV, 2019.

Ding, X., Zhang, X., Han, J., and Ding, G. Diverse branch
block: Building a convolution as an inception-like unit.
In CVPR, 2021a.

Ding, X., Zhang, X., Ma, N., Han, J., Ding, G., and Sun,
J. Repvgg: Making vgg-style convnets great again. In
CVPR, 2021b.

Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E.
Rigging the lottery: Making all tickets winners. In ICML,
2020.

Frankle, J. and Carbin, M. The lottery ticket hypothesis:
Finding sparse, trainable neural networks. In ICLR, 2018.

Frankle, J., Dziugaite, G. K., Roy, D., and Carbin, M. Linear
mode connectivity and the lottery ticket hypothesis. In
ICML, 2020.

Gale, T., Elsen, E., and Hooker, S. The state of sparsity in
deep neural networks. arXiv, 2019.

Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,
Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,
et al. The pile: An 800gb dataset of diverse text for
language modeling. arXiv, 2020.

Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster,
C., Golding, L., Hsu, J., McDonell, K., Muennighoff,
N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang,
B., Wang, K., and Zou, A. A framework for few-shot
language model evaluation, September 2021.

Golubeva, A., Gur-Ari, G., and Neyshabur, B. Are wider
nets better given the same number of parameters? In
ICLR, 2021.

Goyal, P., Duval, Q., Seessel, I., Caron, M., Singh, M.,
Misra, I., Sagun, L., Joulin, A., and Bojanowski, P. Vi-
sion models are more robust and fair when pretrained on
uncurated images without supervision. arXiv, 2022.

Guo, S., Alvarez, J. M., and Salzmann, M. Expandnets: Lin-
ear over-parameterization to train compact convolutional
networks. In NeurIPS, 2020a.

Guo, S., Alvarez, J. M., and Salzmann, M. Expandnets: Lin-
ear over-parameterization to train compact convolutional
networks. In NeurIPS, 2020b.

Han, S., Mao, H., and Dally, W. J. Deep compression:
Compressing deep neural networks with pruning, trained
quantization and huffman coding. arXiv, 2015a.

Han, S., Pool, J., Tran, J., and Dally, W. Learning both
weights and connections for efficient neural network. In
NeurIPS, 2015b.

He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings
in deep residual networks. In ECCV, 2016.

He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.
Bag of tricks for image classification with convolutional
neural networks. In CVPR, 2019.

Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
Song, D., and Steinhardt, J. Measuring massive multitask
language understanding, 2021.

Hinton, G., Vinyals, O., and Dean, J. Distilling the knowl-
edge in a neural network. arXiv, 2015.

Hoang, D. N., Kundu, S., Liu, S., and Wang, Z. Don't
just prune by magnitude! your mask topology is a secret
weapon. In NeurIPS, 2023a.

Hoang, D. N., Liu, S., Marculescu, R., and Wang, Z. Revisit-
ing pruning at initialization through the lens of ramanujan
graph. In ICLR, 2023b.

--- TRANG 11 ---
Biến đổi Iso-FLOP Thưa thớt để Tối đa hóa Hiệu quả Huấn luyện

Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,
Cai, T., Rutherford, E., de las Casas, D., Hendricks, L. A.,
Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican,
K., van den Driessche, G., Damoc, B., Guy, A., Osindero,
S., Simonyan, K., Elsen, E., Vinyals, O., Rae, J. W., Sifre,
L., and et al. An empirical analysis of compute-optimal
large language model training. In NeurIPS, 2022.

Hooker, S. The Hardware Lottery. arXiv, 2020.

Huang, S., Lei, B., Xu, D., Peng, H., Sun, Y., Xie, M.,
and Ding, C. Dynamic sparse training via balancing the
exploration-exploitation trade-off. arXiv, 2023.

Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, J.,
and Soudry, D. Accelerated sparse neural training: A
provable and efficient method to find n:m transposable
masks. In NeurIPS, 2021.

Ioffe, S. and Szegedy, C. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
In ICML, 2015.

Iofinova, E., Peste, A., Kurtz, M., and Alistarh, D. How
well do sparse imagenet models transfer? CoRR,
abs/2111.13445, 2021.

Jayakumar, S., Pascanu, R., Rae, J., Osindero, S., and Elsen,
E. Top-kast: Top-k always sparse training. In NeurIPS,
2020.

Jiang, P., Hu, L., and Song, S. Exposing and exploiting
fine-grained block structures for fast and accurate sparse
training. In NeurIPS, 2022.

Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and
Amodei, D. Scaling laws for neural language models.
arXiv, 2020.

Khodak, M., Tenenholtz, N. A., Mackey, L., and Fusi, N.
Initialization and regularization of factorized neural lay-
ers. In ICLR, 2020.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classification with deep convolutional neural networks.
In NeurIPS, 2012.

Kurtz, M., Kopinsky, J., Gelashvili, R., Matveev, A., Carr,
J., Goin, M., Leiserson, W., Moore, S., Nell, B., Shavit,
N., and Alistarh, D. Inducing and exploiting activation
sparsity for fast inference on deep neural networks. In
ICML, 2020.

Lee, N., Ajanthan, T., and Torr, P. H. Snip: Single-shot
network pruning based on connection sensitivity. arXiv,
2018.

Lie, S. Thinking outside the die: Architecting the ml accel-
erator of the future. https://www.microarch.org/micro54/
media/lie-keynote.pdf, 2021.

Lie, S. Cerebras architecture deep dive: First look inside
the hardware/software co-design for deep learning. IEEE
Micro, 2023.

Lin, M., Chen, Q., and Yan, S. Network in network. In
ICLR, 2014a.

Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring
how models mimic human falsehoods, 2022.

Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
Ramanan, D., Dollár, P., and Zitnick, C. L. Microsoft
coco: Common objects in context. In ECCV, 2014b.

Lin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B.,
and Belongie, S. Feature Pyramid Networks for Object
Detection. In CVPR, 2017a.

Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Dollár, P.
Focal loss for dense object detection. In ICCV, 2017b.

Littwin, E., Myara, B., Sabah, S., Susskind, J., Zhai, S., and
Golan, O. Collegial ensembles. In NeurIPS, 2020.

Liu, S., Chen, T., Chen, X., Atashgahi, Z., Yin, L., Kou,
H., Shen, L., Pechenizkiy, M., Wang, Z., and Mocanu,
D. C. Sparse training via boosting pruning plasticity with
neuroregeneration. 2021a.

Liu, S., Mocanu, D. C., Pei, Y., and Pechenizkiy, M. Selfish
sparse rnn training. In ICML, 2021b.

Liu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B.,
Pechenizkiy, M., Mocanu, D., and Wang, Z. More con-
vnets in the 2020s: Scaling up kernels beyond 51x51
using sparsity. arXiv, 2022a.

Liu, S., Chen, T., Chen, X., Shen, L., Mocanu, D. C., Wang,
Z., and Pechenizkiy, M. The unreasonable effectiveness
of random pruning: Return of the most naive baseline for
sparse training. In ICLR, 2022b.

Liu, S., Chen, T., Chen, X., Shen, L., Mocanu, D. C., Wang,
Z., and Pechenizkiy, M. The unreasonable effectiveness
of random pruning: Return of the most naive baseline for
sparse training. arXiv, 2022c.

Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
S., and Guo, B. Swin transformer: Hierarchical vision
transformer using shifted windows. In ICCV, 2021c.

Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T.,
and Xie, S. A convnet for the 2020s. In CVPR, 2022d.

Loshchilov, I. and Hutter, F. Decoupled weight decay regu-
larization. arXiv, 2017.

Ma, X., Qin, M., Sun, F., Hou, Z., Yuan, K., Xu, Y., Wang,
Y., Chen, Y.-K., Jin, R., and Xie, Y. Effective model
sparsification by scheduled grow-and-prune methods. In
ICLR, 2022.

Mehta, S. and Rastegari, M. Mobilevit: Light-weight,
general-purpose, and mobile-friendly vision transformer.
In ICLR, 2021.

Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen,
E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,
Venkatesh, G., and Wu, H. Mixed precision training. In
ICLR, 2018.

Mocanu, D., Mocanu, E., Stone, P., Nguyen, P., Gibescu,
M., and Liotta, A. Scalable training of artificial neural
networks with adaptive sparse connectivity inspired by
network science. Nature Communications, 2018.

Nair, V. and Hinton, G. E. Rectified linear units improve
restricted boltzmann machines. In ICML, 2010.

Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B.,
and Sutskever, I. Deep double descent: Where bigger
models and more data hurt. Journal of Statistical Me-
chanics: Theory and Experiment, 2021.

NeuralMagic. Deepsparse, 2021. URL https://github.com/
neuralmagic/deepsparse.

Nvidia. Resnet v1.5 for pytorch. 2019. URL
https://catalog.ngc.nvidia.com/orgs/nvidia/resources/
resnet 50v15forpytorch.

Nvidia. Nvidia performance documentation. 2023.
URL https://docs.nvidia.com/deeplearning/performance/
dl-performance-matrix-multiplication/index.html.

Pan, Z., Zhuang, B., Liu, J., He, H., and Cai, J. Scalable
vision transformers with hierarchical pooling. In ICCV,
2021.

Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.,
et al. Improving language understanding by generative
pre-training. OpenAI Blog, 2018.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
Sutskever, I., et al. Language models are unsupervised
multitask learners. OpenAI Blog, 2019.

Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,
Song, F., Aslanides, J., Henderson, S., Ring, R., Young,
S., et al. Scaling language models: Methods, analysis &
insights from training gopher. arXiv, 2021.

Raihan, M. A. and Aamodt, T. Sparse weight activation
training. In NeurIPS, 2020.

Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A.,
and Rastegari, M. What's hidden in a randomly weighted
neural network? In CVPR, 2020.

Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.
WINOGRANDE: an adversarial winograd schema chal-
lenge at scale, 2019.

Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and
Chen, L.-C. Mobilenetv2: Inverted residuals and linear
bottlenecks. In CVPR, 2018.

Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou,
I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,
Bolton, A., et al. Mastering the game of go without
human knowledge. Nature, 2017.

Simonyan, K. and Zisserman, A. Very deep convolutional
networks for large-scale image recognition. arXiv, 2014.

Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel,
P., and Vaswani, A. Bottleneck transformers for visual
recognition. In CVPR, 2021.

Stosic, D. and Stosic, D. Search spaces for neural model
training. arXiv, 2021.

Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,
Z. Rethinking the inception architecture for computer
vision. In CVPR, 2016.

Tai, C., Xiao, T., Zhang, Y., Wang, X., and Weinan, E. Con-
volutional neural networks with low-rank regularization.
In ICLR, 2016.

Tai, K. S., Tian, T., and Lim, S.-N. Spartan: Differentiable
Sparsity via Regularized Transportation. In NeurIPS,
2022.

Tan, M. and Le, Q. EfficientNet: Rethinking model scaling
for convolutional neural networks. In ICML, 2019.

Tanaka, H., Kunin, D., Yamins, D. L., and Ganguli, S. Prun-
ing neural networks without any data by iteratively con-
serving synaptic flow. In NeurIPS, 2020.

Thakker, U., Whatmough, P. N., Liu, Z., Mattina, M., and
Beu, J. Doping: A technique for efficient compression of
lstm models using sparse structured additive matrices. In
MLSys, 2021.

Thangarasa, V., Gupta, A., Marshall, W., Li, T., Leong, K.,
DeCoste, D., Lie, S., and Saxena, S. SPDF: Sparse pre-
training and dense fine-tuning for large language models.
In UAI, 2023.

Udell, M. and Townsend, A. Why are big data matrices
approximately low rank? SIAM Journal on Mathematics
of Data Science, 2019.

Vasu, P. K. A., Gabriel, J., Zhu, J., Tuzel, O., and Ranjan, A.
An improved one millisecond mobile backbone. arXiv,
2022.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention
is all you need. In NeurIPS, 2017.

Wang, C., Zhang, G., and Grosse, R. Picking winning
tickets before training by preserving gradient flow. arXiv,
2020a.

Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y.,
Liu, D., Mu, Y., Tan, M., Wang, X., et al. Deep high-
resolution representation learning for visual recognition.
In TPAMI, 2020b.

Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D.,
Lu, T., Luo, P., and Shao, L. Pyramid vision transformer:
A versatile backbone for dense prediction without convo-
lutions. In ICCV, 2021.

Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L.,
and Zhang, L. Cvt: Introducing convolutions to vision
transformers. In ICCV, 2021.

Yuan, G., Ma, X., Niu, W., Li, Z., Kong, Z., Liu, N., Gong,
Y., Zhan, Z., He, C., Jin, Q., et al. Mest: Accurate and
fast memory-economic sparse training framework on the
edge. volume 34, 2021.

Zagoruyko, S. and Komodakis, N. Wide residual networks.
In BMVC, 2016.

Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
Hellaswag: Can a machine really finish your sentence?,
2019.

Zhao, H., Shi, J., Qi, X., Wang, X., and Jia, J. Pyramid
scene parsing network. In CVPR, 2017.

Zhou, H., Lan, J., Liu, R., and Yosinski, J. Deconstruct-
ing lottery tickets: Zeros, signs, and the supermask. In
NeurIPS, 2019.

Zhu, M. and Gupta, S. To prune, or not to prune: exploring
the efficacy of pruning for model compression. arXiv,
2017.

--- TRANG 15 ---
Biến đổi Iso-FLOP Thưa thớt để Tối đa hóa Hiệu quả Huấn luyện

A. Chi tiết Phương pháp Bổ sung

A.1. Sparse-IFT cho Lớp Tích chập

Trong phần này, chúng tôi chi tiết việc mở rộng đơn giản của họ Sparse-IFT cho các lớp tích chập.

Sparse Wide Tương tự như thiết lập cho các lớp được kết nối đầy đủ, trong trường hợp các lớp tích chập, chúng tôi mở rộng số lượng kênh đầu vào và đầu ra.

Sparse Parallel Tương tự như thiết lập cho các lớp được kết nối đầy đủ, trong trường hợp các lớp tích chập, chúng tôi có thể triển khai biến đổi này với việc sử dụng các nhánh tích chập song song.

Sparse Factorized và Sparse Doped Cho θl∈Rcin×cout×kh×kw đại diện cho ma trận trọng số của một lớp tích chập, trong đó cin, cout, kh, kw biểu thị các kênh đầu vào, kênh đầu ra, chiều cao kernel, và chiều rộng kernel tương ứng. Chúng tôi áp dụng phân tích hạng thấp hoặc ma trận cho ma trận trọng số bằng cách đầu tiên chuyển đổi tensor 4D thành ma trận 2D với hình dạng: (cin·kh·kw)×cout. Trong thiết lập này, chúng tôi có thể biểu diễn θl=UVT, trong đó U∈Rcin·kh·kw×d,V∈Rcout×d. Trong phân tích này, U học một tập hợp đặc trưng chiều thấp hơn và được triển khai như một lớp tích chập với d kênh đầu ra và bộ lọc kh×kw. Ma trận V mở rộng tập hợp đặc trưng chiều thấp này và được triển khai như một lớp tích chập với bộ lọc 1×1.

A.1.1. SPARSE-IFT CHO CÁC LỚP TÍCH CHẬP THEO CHIỀU SÂU

Đối với một lớp tích chập bình thường, tất cả đầu vào được tích chập với tất cả đầu ra. Tuy nhiên, đối với các tích chập theo chiều sâu, mỗi kênh đầu vào được tích chập với tập hợp bộ lọc riêng của nó. Cho θl∈Rcin×cout×kh×kw đại diện cho ma trận trọng số của một lớp tích chập bình thường, trong đó cin, cout, kh, kw biểu thị các kênh đầu vào, kênh đầu ra, chiều cao kernel, và chiều rộng kernel tương ứng. Một lớp tích chập theo chiều sâu tương đương sẽ có trọng số θdw,l∈R1×cout×kh×kw.

Sparse Wide Một tích chập theo chiều sâu Sparse Wide sẽ có trọng số θsw dw,l∈R1×ksw·cout×kh×kw. Vì phần của trọng số không thưa thớt được cho bởi 1−s, FLOPs cần thiết bởi biến đổi này là B·(ksw·cout)·kh·kw·(1−s). Đặt những điều này bằng FLOPs của θdw,l dày đặc ban đầu, chúng ta có được hệ số mở rộng ksw=1/(1−s). Trong trường hợp này, chúng tôi không tỷ lệ các kênh đầu vào vì nó chuyển đổi tích chập theo chiều sâu thành một tích chập được nhóm mà không có tỷ lệ tương đương trong số lượng nhóm.

Các Biến đổi Sparse-IFT Khác Sparse Wide IFT nói chung thay đổi các kênh đầu vào và đầu ra của một lớp, sau đó tỷ lệ các lớp tiếp theo trong một CNN. Tuy nhiên, các biến đổi Sparse-IFT khác (Sparse Parallel, Sparse Factorized, và Sparse Doped) không sửa đổi các kênh đầu vào hoặc đầu ra của một lớp tích chập (như được thấy trong Hình 2). Điều này cho phép kiểm soát tinh vi các lớp nào để áp dụng các biến đổi Sparse-IFT. Vì các tích chập theo chiều sâu là một dạng cực đoan của độ thưa thớt có cấu trúc, nơi một số bộ lọc chỉ tương tác với các kênh đầu vào cụ thể, chúng tôi chọn không làm thưa thớt chúng khi sử dụng các biến đổi Sparse-IFT khác và để lại lớp không thay đổi trong khi vẫn duy trì FLOPs tương đương với đường cơ sở dày đặc. Lưu ý rằng các lớp tích chập khác nhau xung quanh tích chập theo chiều sâu vẫn được biến đổi với Sparse-IFT để tăng khả năng biểu diễn của chúng.

A.2. Kiểm soát Iso-FLOP

Như đã đề cập trước đây, trong công việc của chúng tôi, chúng tôi chủ yếu áp dụng một phân phối độ thưa thớt đồng nhất cho mô hình, có nghĩa là mỗi lớp được phân bổ cùng mức độ thưa thớt. Cho N biểu thị một DNN L lớp được tham số hóa bởi ΘN. Cho ΘN∈ {θ1, . . . , θ L} biểu thị các tham số của DNN. Bây giờ, cho Ml là mặt nạ nhị phân cho lớp l∈ {1, . . . , L} với các chiều tương ứng với các tham số của lớp đó. Mặt nạ nhị phân ml có giá trị 1 cho trọng số hoạt động và 0 cho trọng số không hoạt động. Cho θl là tổng số tham số trong l, do đó, mức độ thưa thớt trên mỗi lớp, sl, được định nghĩa là ∑i,jI(ml(i,j)≠0)/θl. Mức độ thưa thớt trung bình trong mạng, s, sau đó được định nghĩa là tỷ lệ của tổng số tham số bằng không với tổng số tham số. Điều này được biểu diễn là s=∑L l=1∑i,jI(ml(i,j)≠0)/ΘN. Dưới đây, chúng tôi đặc trưng cho các tình huống khác nhau khi huấn luyện với các phương pháp huấn luyện thưa thớt khác nhau:

• Độ thưa thớt Tĩnh Ngẫu nhiên: Trong trường hợp này, phân phối độ thưa thớt là đồng nhất, đảm bảo rằng độ thưa thớt trong mỗi lớp khớp với mức độ thưa thớt mục tiêu. Do đó, việc áp dụng Sparse-IFT, được tham số hóa bởi mức độ thưa thớt, duy trì tương đương Iso-FLOP với mô hình dày đặc ban đầu. Tuy nhiên, tuân theo thực hành phổ biến cho các mạng thị giác máy tính (ví dụ: ResNet), chúng tôi giữ lại lớp đầu tiên và cuối cùng (tích chập đầu vào và lớp tuyến tính đầu ra) là dày đặc để ngăn chặn sự suy giảm đáng kể trong chất lượng mô hình trong quá trình tiền huấn luyện. Do đó, mạng Sparse-IFT lệch khỏi Iso-FLOP với mô hình dày đặc, giới thiệu FLOPs bổ sung cần được xem xét.

• Cắt tỉa tại Khởi tạo: Các thuật toán, như SNIP (Lee et al., 2018), GraSP (Wang et al., 2020a), FORCE (de Jorge et al., 2020), v.v., giới thiệu các tiêu chí hoặc phương pháp khác biệt để xác định trọng số nào cần cắt tỉa tại khởi tạo, ảnh hưởng đến phân phối độ thưa thớt. Do đó, các đặc tính vốn có của những thuật toán này dẫn đến thay đổi trong phân phối độ thưa thớt. Trong bối cảnh Sparse-IFT, mặc dù có số lượng tham số thưa thớt tổng cộng giống hệt với mô hình dày đặc ban đầu, mạng Sparse-IFT không còn duy trì tương đương Iso-FLOP.

• Huấn luyện từ Dày đặc đến Thưa thớt: Các phương pháp huấn luyện thưa thớt, như GraNet (Liu et al., 2021a), sử dụng huấn luyện từ dày đặc đến thưa thớt, bắt đầu huấn luyện từ trạng thái hoàn toàn dày đặc hoặc trạng thái ít thưa thớt hơn mức độ thưa thớt mục tiêu. Ví dụ, GraNet sử dụng cắt tỉa độ lớn dần dần (Zhu & Gupta, 2017) ở đầu huấn luyện để hệ thống giảm mật độ của mạng đến mức độ thưa thớt mục tiêu. Do đó, trong bối cảnh các mạng Sparse-IFT, cấu hình này không còn duy trì tương đương Iso-FLOP với mô hình dày đặc, vì FLOPs huấn luyện trung bình vượt quá của mô hình dày đặc ban đầu.

Để giải quyết sự khác biệt FLOPs giữa mạng Sparse-IFT được huấn luyện với các phân phối độ thưa thớt không đồng nhất (ví dụ: các phương pháp PaI hoặc làm dày đặc một số lớp) và huấn luyện từ dày đặc đến thưa thớt (ví dụ: GraNet), chúng tôi sử dụng tìm kiếm nhị phân để tinh chỉnh độ thưa thớt mục tiêu của mạng trước bất kỳ huấn luyện nào. Trong quá trình này, chúng tôi đặt các giá trị tối đa và tối thiểu cho mức độ thưa thớt mục tiêu. Tại mỗi lần lặp, chúng tôi hồ sơ FLOPs được sử dụng bởi mạng Sparse-IFT và so sánh nó với FLOPs mô hình dày đặc ban đầu. Mức độ thưa thớt mục tiêu được điều chỉnh thông qua tìm kiếm nhị phân, đảm bảo rằng tổng FLOPs của mạng Sparse-IFT nằm trong 0.0001% của FLOPs mô hình dày đặc.

B. Phân tích Đồ thị của Sparse-IFT với DST

Trong phân tích của chúng tôi, chúng tôi diễn giải các mô hình Sparse-IFT ResNet-18 như một loạt các đồ thị tính toán hai phía, trong đó mỗi lớp, {θ1, . . . , θ L} trong một DNN thưa thớt L lớp, có dạng của một ma trận kề vuông A. Ramanujan gap được định nghĩa là ∆r= 2∗√d−1−μ̂(A) (Hoang et al., 2023b;a), trong đó d là cạnh trung bình trên mỗi nút, và μ̂(A) là eigenvalue không tầm thường của A. Ngoài ra, chúng tôi phân tích Iterative Mean Difference Bound, ∆rimdb=1/|K|∑|K|i=1(2√di−1−μ̂(AKi)) (Hoang et al., 2023b). Chúng tôi huấn luyện một mô hình ResNet-18 với tất cả các thành viên của họ Sparse-IFT sử dụng một thuật toán huấn luyện thưa thớt động (tức là RigL (Evci et al., 2020)).

Phân tích ∆r: Trong Hình 6, chúng tôi quan sát thấy rằng ∆r giảm trong quá trình huấn luyện và sau đó tối đa hóa ở các giai đoạn sau, điều này gợi ý rằng các tính chất phổ của các ma trận kề đang thay đổi động trong quá trình huấn luyện. Việc ∆r tối đa hóa ở các giai đoạn sau và tương quan với mô hình Sparse-IFT ResNet-18 đạt được độ chính xác kiểm tra cao nhất chỉ ra một kết nối tiềm năng giữa các tính chất phổ của các ma trận kề và hiệu suất của mô hình. Những thay đổi động trong ∆r có thể chỉ ra rằng mạng nơ-ron đang thích ứng cấu trúc của nó trong quá trình huấn luyện. Mạng có thể đang cắt tỉa các kết nối ít quan trọng hơn và củng cố những kết nối quan trọng hơn, dẫn đến một cấu trúc được tối ưu hóa. Hơn nữa, sự tăng trong ∆r có thể liên quan đến các hiệu ứng chính quy hóa ngầm. Các tính chất phổ của các ma trận kề có thể đóng một vai trò trong việc kiểm soát khả năng của mô hình, ngăn chặn overfitting, và tăng cường khái quát hóa. Mối tương quan giữa việc tối đa hóa ∆r ở các giai đoạn sau của huấn luyện và độ chính xác kiểm tra cao nhất gợi ý rằng có một mối quan hệ giữa các tính chất phổ được xác định và hiệu suất của mô hình Sparse-IFT ResNet-18. Việc tối đa hóa ∆r có thể đại diện cho một điểm tối ưu trong sự đánh đổi giữa độ thưa thớt và độ chính xác mô hình cho nhiệm vụ đã cho.

Phân tích ∆rimdb: Xu hướng tăng của ∆rimdb trong quá trình huấn luyện gợi ý rằng ranh giới kết nối tổng thể qua các đồ thị con đang được tăng cường dần dần. Điều này có thể ngụ ý rằng mạng đang học thiết lập các kết nối có ý nghĩa và liên quan hơn trong cấu trúc của nó khi huấn luyện tiến triển. Thuật toán DST có thể đang tạo điều kiện thuận lợi cho một sự tinh chỉnh thích ứng của kết nối trong mạng. Sự tăng quan sát được trong ∆rimdb có thể chỉ ra rằng mô hình đang lặp đi lặp lại điều chỉnh các ranh giới kết nối của nó để cải thiện dòng chảy thông tin. ∆rimdb đánh giá ranh giới kết nối trung bình qua tất cả các đồ thị con, cung cấp một thước đo toàn diện hơn về những thay đổi kết nối tổng thể của mạng. Mối tương quan với các mô hình hoạt động cao nhất ở giai đoạn cuối của huấn luyện gợi ý rằng những cải thiện kết nối trung bình được ghi lại bởi ∆rimdb có lợi cho hiệu suất của mô hình.

--- TRANG 17 ---
Biến đổi Iso-FLOP Thưa thớt để Tối đa hóa Hiệu quả Huấn luyện

0.38 0.40 0.42 0.44 0.46
rimdb
020406080Độ chính xác Kiểm tra (%)
Sparse Wide IFT (s=50%)
050100150200Epochs Huấn luyện

0.36 0.38 0.40
rimdb
020406080Độ chính xác Kiểm tra (%)
Sparse Wide IFT (s=50%)
050100150200Epochs Huấn luyện

0.20 0.22 0.24 0.26 0.28
rimdb
020406080Độ chính xác Kiểm tra (%)
Sparse Factorized IFT (s=50%)
050100150200Epochs Huấn luyện

10 0 10
r
020406080Độ chính xác Kiểm tra (%)
Sparse Wide IFT (s=50%)
050100150200Epochs Huấn luyện

0 5 10
r
020406080Độ chính xác Kiểm tra (%)
Sparse Parallel IFT (s=50%)
050100150200Epochs Huấn luyện

5 0 5 10
r
020406080Độ chính xác Kiểm tra (%)
Sparse Factorized IFT (s=50%)
050100150200Epochs Huấn luyện

Hình 6: Hàng trên minh họa sự tương tác động giữa Iterative Mean Difference Bound, ∆rimdb và độ chính xác kiểm tra, và hàng dưới cho thấy mối tương quan giữa Ramanujan Gap, ∆r và độ chính xác kiểm tra trong suốt quá trình huấn luyện. Điều này minh họa mối quan hệ phát triển giữa các tính chất đồ thị phổ và hiệu suất mạng, làm sáng tỏ động lực kết nối của các mạng Sparse-IFT được huấn luyện với DST.

C. Thị giác Máy tính: Cài đặt Thí nghiệm

C.1. Thị giác Máy tính: Cài đặt Tiền Huấn luyện

CIFAR-100 Việc triển khai CIFAR-100 của chúng tôi tuân theo thiết lập từ (DeVries & Taylor, 2017) cho ResNets. Chúng tôi huấn luyện các mô hình trong 200 epochs với batch 128 sử dụng SGD, momentum Nesterov 0.9, và weight-decay 5×10−4. Tốc độ học ban đầu được đặt thành 0.1 và được lên lịch để giảm bằng cách giảm theo hệ số 5x sau mỗi epoch thứ 60, 120, và 160. Theo các tiến bộ gần đây trong việc cải thiện ResNets, chúng tôi khởi tạo mạng với khởi tạo Kaiming He (He et al., 2016), zero-init residuals (He et al., 2019), và vô hiệu hóa weight-decay trong các lớp biases và BatchNorm (Ioffe & Szegedy, 2015). Đối với các thí nghiệm CIFAR-100 với MobileNetV2, MobileViT-S, và BotNet-50, chúng tôi tuân theo cùng thiết lập huấn luyện được sử dụng cho ResNet, nhưng tốc độ học được lên lịch thông qua cosine annealing.

ImageNet Việc triển khai ImageNet của chúng tôi tuân theo thiết lập tiêu chuẩn từ (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014). Hình ảnh được thay đổi kích thước với cạnh ngắn hơn được lấy mẫu ngẫu nhiên trong [256, 480] để tăng cường tỷ lệ (Simonyan & Zisserman, 2014). Một cắt 224×224 được lấy mẫu ngẫu nhiên từ một hình ảnh hoặc flip ngang của nó, và sau đó được chuẩn hóa. Để đánh giá, hình ảnh đầu tiên được thay đổi kích thước thành 256×256, tiếp theo là một cắt trung tâm 224×224, và sau đó được chuẩn hóa. Theo các tiến bộ gần đây trong việc cải thiện ResNets, chúng tôi khởi tạo mạng với khởi tạo Kaiming He (He et al., 2016) và zero-init residuals (He et al., 2019).

Đối với ResNets, chúng tôi sao chép các cài đặt được khuyến nghị bởi Nvidia (Nvidia, 2019), sử dụng optimizer SGD với momentum 0.875 và weight decay 3.0517578125×10−5. Chúng tôi vô hiệu hóa weight-decay cho các lớp biases và BatchNorm. Mô hình được huấn luyện với label smoothing (Szegedy et al., 2016) 0.1 và mixed precision (Micikevicius et al., 2018) cho 90 epochs tiêu chuẩn sử dụng lịch trình tốc độ học cosine-decay với tốc độ học ban đầu 0.256 cho kích thước batch 256. Srinivas et al. (2021) tuân theo cùng thiết lập như ResNet để huấn luyện BotNet-50 trên ImageNet, do đó chúng tôi duy trì các cài đặt siêu tham số giống như Nvidia (2019) cho các thí nghiệm BotNet-50 ImageNet của chúng tôi.

Thiết lập Độ thưa thớt Để kích hoạt các biến đổi Sparse-IFT, chúng tôi sử dụng thuật toán RigL (Evci et al., 2020) trong các cài đặt siêu tham số mặc định (α= 0.3, ∆T= 100), với drop-fraction (α) được annealed sử dụng một lịch trình cosine decay cho 75% của thời gian huấn luyện. Chúng tôi giữ lớp đầu tiên và cuối cùng (tích chập đầu vào và lớp tuyến tính đầu ra) dày đặc để ngăn chặn sự suy giảm đáng kể trong chất lượng mô hình trong quá trình tiền huấn luyện, đây là thực hành tiêu chuẩn. Chúng tôi tính đến những FLOPs dày đặc bổ sung này bằng cách tăng độ thưa thớt trong các lớp còn lại, tương tự như Gale et al. (2019) và Liu et al. (2022c).

C.2. Tầm quan trọng của Tính phi tuyến

Chúng tôi sử dụng BatchNorm (Ioffe & Szegedy, 2015) tiếp theo là ReLU (Nair & Hinton, 2010) như một tính phi tuyến. Chúng tôi cung cấp một tập hợp kết quả thực nghiệm mở rộng trong Bảng 7 để giúp xác nhận tầm quan trọng của huấn luyện có và không có tính phi tuyến bằng cách huấn luyện các cấu hình của họ Sparse Parallel, Factorized, và Doped IFT ở các mức độ thưa thớt khác nhau. Kết quả không có các hàm kích hoạt phi tuyến thường tệ hơn độ chính xác dày đặc (77%) qua tất cả các biến đổi họ Sparse-IFT. Chúng tôi bỏ qua Sparse Wide trong Bảng 7 vì ở đây chúng tôi tăng số lượng kênh trong các lớp tích chập trong khi duy trì kiến trúc hiện có.

Bảng 7: Đánh giá về tầm quan trọng của việc sử dụng kích hoạt phi tuyến qua các thành viên khác nhau của Sparse-IFT với ResNet-18 trên CIFAR100 qua các giá trị độ thưa thớt khác nhau (các cột). Các kích hoạt phi tuyến tăng cường khả năng biểu diễn của Sparse-IFT, dẫn đến độ chính xác cao hơn. Tất cả kết quả được báo cáo là trung bình qua 3 random seeds.

Biến đổi                 Kích hoạt phi tuyến    0.50    0.75    0.90
Sparse Factorized        ✗                      75.9±0.3 76.6 ±0.4 76.5 ±0.4
                        ✓                      77.8±0.4 78.4 ±0.5 78.9 ±0.5
Sparse Parallel         ✗                      77.1±0.1 77.2 ±0.2 77.6 ±0.1
                        ✓                      77.9±0.2 79.1 ±0.2 78.2 ±0.2
Sparse Doped            ✗                      77.3±0.2 77.1 ±0.1 76.5 ±0.2
                        ✓                      78.2±0.1 77.8 ±0.1 76.9 ±0.2

C.3. Thị giác Máy tính

C.3.1. SPARSE-IFT SO VỚI CÁC LỊCH TRÌNH HUẤN LUYỆN THƯA THỚT MỞ RỘNG

Chúng tôi cung cấp một so sánh trực tiếp với các phương pháp huấn luyện thưa thớt (ví dụ: RigL và SET) trong thiết lập Iso-FLOP (tức là huấn luyện với lịch trình dài hơn) để chứng minh tính quan trọng của kết quả của chúng tôi đối với các đường cơ sở thưa thớt tiêu chuẩn này. Như được hiển thị trong Bảng 8, Sparse-IFTs vượt trội hơn các phương pháp huấn luyện thưa thớt động với một khoảng cách đáng kể qua tất cả các mức độ thưa thớt. Lưu ý, ở các mức độ thưa thớt cao hơn (ví dụ: 90%), các phương pháp huấn luyện thưa thớt có được độ chính xác tệ hơn so với đường cơ sở dày đặc tương đương FLOP. Ngược lại, với Sparse-IFT, chúng tôi quan sát độ chính xác cao hơn qua tất cả các mức độ thưa thớt được đánh giá.

C.3.2. SPARSE-IFT TRÊN CÁC KIẾN TRÚC THISI GIÁC MÁY TÍNH HIỆU QUẢ

Ở đây, chúng tôi cung cấp một tập hợp kết quả mở rộng trên MobileNetV2, MobileViT-S, và BotNet-50 trên CIFAR-100. Cụ thể, chúng tôi kích hoạt Sparse Wide và Sparse Parallel IFT ở các giá trị độ thưa thớt 50% và 75% (xem Bảng 9).

C.3.3. ĐÁNH GIÁ SPARSE-IFT VỚI ĐỘ THƯA THỚT CÓ CẤU TRÚC

Độ thưa thớt Khối Để dẫn xuất các cấu hình Iso-FLOP với độ thưa thớt khối, chúng tôi tái sử dụng phân tích đã thực hiện trước đây với độ thưa thớt không có cấu trúc (xem Phần 2.2) và biểu diễn việc tỷ lệ độ rộng như một hàm của độ thưa thớt. Tuy nhiên, chúng tôi sẽ tìm kiếm một mặt nạ thưa thớt khối trong quá trình huấn luyện thay vì một mặt nạ độ thưa thớt không có cấu trúc. Chúng tôi sử dụng phương pháp được đề xuất bởi Hubara et al. (2021) để tìm kiếm độ thưa thớt N:M có thể chuyển vị, có thể tăng tốc cả pass forward và backward trong quá trình huấn luyện trên GPU NVIDIA với Tensor Cores. Chúng tôi sử dụng các mẫu khối 4:8-T, 2:8-T, và 1:8-T để có được độ thưa thớt 50%, 75%, và 87.5% tương ứng. Lưu ý khối 1:8-T là sự xấp xỉ gần nhất với một mẫu độ thưa thớt 90% có thể đạt được với kích thước khối 8.

Bảng 8: Kết quả với ResNet-18 trên CIFAR-100 qua các giá trị độ thưa thớt khác nhau (các cột). Độ chính xác tốt nhất cho mỗi phương pháp huấn luyện thưa thớt được tô đậm. Mô hình ResNet-18 dày đặc ban đầu có được độ chính xác 77.0 ±0.2. Tất cả kết quả được báo cáo qua 3 random seeds.

Dày đặc    Biến đổi       Phương pháp Huấn luyện Thưa thớt    Epochs    0.50    0.75    0.90
77.0±0.2   Sparse Wide   SET                                  200 ·1/(1−s) 78.7±0.2 78.4±0.1 76.8 ±0.1
           Sparse Wide   RigL                                 200 ·1/(1−s) 78.9±0.1 78.8±0.1 76.4 ±0.2
           Sparse Wide   RigL                                 200          79.1 ±0.2 79.5 ±0.1 80.1±0.2

--- TRANG 19 ---
Biến đổi Iso-FLOP Thưa thớt để Tối đa hóa Hiệu quả Huấn luyện

Bảng 9: Đánh giá Sparse Wide và Sparse Parallel IFT với các kiến trúc hiệu quả tính toán khác nhau trên CIFAR-100 qua các giá trị độ thưa thớt khác nhau (các cột). Sử dụng Sparse Parallel IFT, tất cả các kiến trúc vượt trội hơn đường cơ sở dày đặc với một khoảng cách đáng kể.

Dày đặc      Biến đổi           0.50    0.75
MobileNetV2  72.4±0.2  Sparse Wide        72.9    73.3
                       Sparse Parallel     72.9    73.3
MobileViT-S  73.5±0.1  Sparse Wide        73.7    74.4
                       Sparse Parallel     73.7    74.4
BotNet-50    79.8±0.2  Sparse Wide        79.7    80.5
                       Sparse Parallel     79.7    80.5

Bảng 10: So sánh các phương pháp thưa thớt có cấu trúc và không có cấu trúc trên độ chính xác kiểm tra CIFAR-100 trên ResNet-18.

Hệ số Tỷ lệ Độ rộng
Biến đổi               Kiểu Độ thưa thớt       Độ thưa thớt    1x      1.41x   2x      3.16x
Low Rank, Linear       Có cấu trúc             0%              74.1    74.3    74.3    73.4
Low Rank, Non-Linear   Có cấu trúc             0%              76.8    76.5    76.0    75.3
Sparse Wide           N:M Block Sparse         4:8-T           77.1
                      (Hubara et al., 2021)    2:8-T           78.4
                                               1:8-T           78.1
                      Unstructured Sparse      50%             79.1
                      (Evci et al., 2020)      75%             79.5
                                               90%             80.1

Chúng tôi cũng thiết lập và thí nghiệm sử dụng phương pháp được đề xuất bởi Jiang et al. (2022) để huấn luyện với các cấu trúc khối thưa thớt tinh vi một cách động. Tuy nhiên, thuật toán sử dụng agglomerative clustering dẫn đến runtime chậm hơn nhiều và nhanh chóng hết bộ nhớ ngay cả ở 50% độ thưa thớt sử dụng Sparse Wide IFT trên một Nvidia V100 duy nhất (16 GB).

Low Rank Cho klr là hệ số mà chúng tôi mở rộng tất cả các chiều đầu vào và đầu ra của các lớp cho phân tích hạng thấp. Chúng tôi thay thế tất cả các lớp dày đặc bằng phân tích hạng thấp, tức là θlr l=UlVT l, trong đó Ul∈R(klr.Din)×d và Vl∈R(klr.Dout)×d. Cho một hệ số mở rộng và đặt FLOPs của biến đổi này bằng của một biến đổi dày đặc fθ, chúng ta có được biểu thức sau cho hạng d: Din.Dout.klr/(Din+Dout). Chúng tôi đánh giá phân tích này qua các giá trị khác nhau của width-scaling klr trong Bảng 10.

C.3.4. ĐÁNH GIÁ TRÊN CÁC NHIỆM VỤ DOWNSTREAM

PHÁT HIỆN ĐỐI TƯỢNG COCO
Tập dữ liệu này chứa 118K hình ảnh huấn luyện, 5K hình ảnh xác thực (minival), và 20K hình ảnh test-dev. Chúng tôi áp dụng thiết lập huấn luyện single-scale tiêu chuẩn (Lin et al., 2017a) nơi không có tăng cường dữ liệu bổ sung ngoài flipping ngang tiêu chuẩn. Để huấn luyện và kiểm tra, các hình ảnh đầu vào được thay đổi kích thước sao cho cạnh ngắn hơn là 800 pixel (Lin et al., 2017a). Mô hình được huấn luyện với kích thước batch 16, sử dụng optimizer SGD với momentum 0.9 và weight decay 1×10−4. Chúng tôi tuân theo lịch trình 1x tiêu chuẩn (12 epochs) sử dụng lịch trình tốc độ học step, với giảm 10x tại epoch 8 và 11, một warmup tốc độ học ban đầu 500 bước bắt đầu từ tốc độ học 2×10−5, và tốc độ học đỉnh 0.01.

Bảng 11: Kết quả phát hiện đối tượng trên COCO minival trong framework RetinaNet. Các cấu hình Sparse Wide IFT của RetinaNet vượt trội hơn đường cơ sở dày đặc với một khoảng cách lớn trên tất cả các chỉ số trong khi sử dụng FLOPs tương tự.

Backbone              AP      AP50    AP75    APS     APM     APL
Dày đặc               29.3    46.2    30.9    14.7    31.5    39.6
Sparse Wide (50%)     31.3    49.0    33.0    16.6    34.0    42.0
Sparse Wide (75%)     32.8    51.0    34.8    17.3    35.8    43.3
Sparse Wide (90%)     34.5    53.5    36.5    18.6    37.6    45.3

--- TRANG 20 ---
Biến đổi Iso-FLOP Thưa thớt để Tối đa hóa Hiệu quả Huấn luyện

PHÂN ĐOẠN NGỮ NGHĨA CITYSCAPES
Thiết lập Chúng tôi tuân theo cùng giao thức huấn luyện như (Zhao et al., 2017), nơi dữ liệu được tăng cường bằng cắt ngẫu nhiên (từ 1024×2048 đến 512×1024), tỷ lệ ngẫu nhiên trong phạm vi [0.5, 2], và flipping ngang ngẫu nhiên. Mô hình được huấn luyện với kích thước batch 16, sử dụng optimizer SGD với momentum 0.9 và weight decay 5×10−4. Chúng tôi tuân theo thiết lập 80K lần lặp từ MMSegmentation với tốc độ học ban đầu 0.01 được annealed sử dụng lịch trình tốc độ học poly đến tối thiểu 1×10−4. Tương tự như hầu hết các thiết lập điều chỉnh siêu tham số (Zhao et al., 2017; Liu et al., 2021c; Wang et al., 2020b) để báo cáo kết quả tốt nhất, chúng tôi điều chỉnh tốc độ học cho tất cả các mô hình của chúng tôi. Tất cả kết quả của chúng tôi được báo cáo sử dụng tốc độ học 0.03 cho các backbone thưa thớt và 0.01 cho đường cơ sở dày đặc.

Bảng 12: Kết quả phân đoạn ngữ nghĩa trên tập val Cityscapes sử dụng DeepLabV3+. Các cấu hình backbone ResNet-18 Sparse Wide IFT vượt trội hơn đường cơ sở dày đặc trên tất cả các chỉ số trong khi sử dụng FLOPs tương tự.

Backbone              mIoU    mAcc
Dày đặc               76.72   84.40
Sparse Wide (50%)     77.90   85.12
Sparse Wide (75%)     78.92   85.68
Sparse Wide (90%)     79.10   86.01

D. Xử lý Ngôn ngữ Tự nhiên: Cài đặt Thí nghiệm

D.1. Chi tiết cho Huấn luyện End-to-End GPT

Chúng tôi chứng minh lợi ích của việc sử dụng các biến đổi Sparse-IFT trong lĩnh vực NLP bằng cách tiền huấn luyện các mô hình GPT-3 và thực hiện đánh giá zero-shot trên các nhiệm vụ downstream từ bảng xếp hạng HuggingFace Open LLM. Ở đây, chúng tôi tiền huấn luyện các mô hình trên tập dữ liệu Pile (Gao et al., 2020). Để huấn luyện tất cả các mô hình GPT, chúng tôi sử dụng optimizer AdamW (Loshchilov & Hutter, 2017) với β1= 0.9, β2= 0.95 và ε= 10−8. Norm toàn cục được clip ở 1.0, và weight decay 0.1 được sử dụng. Có một warmup tốc độ học qua 375M token đầu tiên, tiếp theo là một cosine decay đến 10% của tốc độ học đỉnh. Chúng tôi tuân theo các khuyến nghị Chinchilla được công bố gần đây (Hoffmann et al., 2022) để có được các cấu hình đường cơ sở tiền huấn luyện tối ưu về loss của các mô hình. Kích thước cửa sổ ngữ cảnh là 2048 theo (Brown et al., 2020). Bảng 13 cho thấy một phân tích chi tiết về các kiến trúc mô hình, tốc độ học, và cài đặt huấn luyện.

Trong Bảng 13 và 14, chúng tôi phác thảo các cấu hình kiến trúc cho các biến thể Sparse Wide IFT 50% và 75%. Chúng tôi huấn luyện các mô hình Sparse Wide GPT-3 sử dụng thuật toán huấn luyện thưa thớt động, SET (Mocanu et al., 2018) trên Cerebras CS-2 để nhận ra sự tăng tốc từ độ thưa thớt không có cấu trúc. Hiện tại, các kernel chuyên biệt của Cerebras CS-2 hỗ trợ huấn luyện với độ thưa thớt động không có cấu trúc thông qua SET; do đó, kết quả trong phần này được báo cáo với SET. Trong Bảng 13, nparams là tổng số tham số có thể huấn luyện, nlayers là số lượng lớp decoder, và dmodel là kích thước cơ sở của mô hình. Bottleneck feedforward gấp bốn lần kích thước cơ sở, tức là dff= 4×dmodel. Cuối cùng, nheads là số lượng attention heads, và dhead là chiều của mỗi attention head.

Đánh giá Chúng tôi đã tiến hành một đánh giá toàn diện về cả các mô hình GPT-3 Small dày đặc và Sparse Wide IFT, đánh giá hiệu suất của chúng ở mức độ thưa thớt 50% và 75% qua năm nhiệm vụ khác biệt trên bảng xếp hạng Open LLM (Beeching et al., 2023) sử dụng LM-eval-harness (Gao et al., 2021). Các nhiệm vụ bao gồm ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), TruthfulQA (Lin et al., 2022), MMLU (Hendrycks et al., 2021), và Winogrande (Sakaguchi et al., 2019). Trong Bảng 15, kết quả của chúng tôi tiết lộ rằng mô hình Sparse IFT GPT-3 Small ở 75% độ thưa thớt đạt được cải thiện đáng chú ý 0.9% so với đường cơ sở dày đặc, nhấn mạnh hiệu quả của Sparse Wide IFT trong việc tăng cường hiệu suất mô hình qua một phạm vi đa dạng các nhiệm vụ hiểu ngôn ngữ.

Bảng 13: Kích thước, kiến trúc, và siêu tham số học (kích thước batch và tốc độ học) của mô hình GPT-3 Small, được huấn luyện sử dụng các cấu hình tối ưu Chinchilla (≈20 token trên mỗi tham số)

Mô hình       nparams  nlayers  dmodel  nheads  dhead  Kích thước Batch  Tốc độ Học    Token Huấn luyện
GPT-3 Small  125M     12       768     12      64     256               6×10−4        2.5B

--- TRANG 21 ---
Biến đổi Iso-FLOP Thưa thớt để Tối đa hóa Hiệu quả Huấn luyện

Bảng 14: Định nghĩa kích thước và kiến trúc của mô hình GPT-3 Small dày đặc và các biến thể Sparse Wide IFT của nó.

MÔ HÌNH      BIẾN ĐỔI        ĐỘ THƯA THỚT  nlayers  dmodel  dFF   nheads  dhead
GPT-3 SMALL  DÀY ĐẶC         0%            12       768     3072  12      64
GPT-3 SMALL  SPARSE WIDE     50%           12       1092    4344  12      64
GPT-3 SMALL  SPARSE WIDE     75%           12       1536    6144  12      64

Bảng 15: Đánh giá Hiệu suất của Các mô hình GPT-3 Small Dày đặc và Sparse Wide IFT ở mức độ thưa thớt 50% và 75% qua năm nhiệm vụ (tức là ARC, HellaSwag, TruthfulQA, MMLU, và Winogrande) trên Bảng xếp hạng Open LLM

MÔ HÌNH      BIẾN ĐỔI        ĐỘ THƯA THỚT  PHƯƠNG PHÁP THƯA THỚT  BẢNG XẾP HẠNG OPEN LLM
                                                                 ARC  HELLASWAG  TRUTHFULQA  MMLU  WINOGRANDE  TRUNG BÌNH
GPT-3 SMALL  DÀY ĐẶC         0%            -                     20.8  27.2      47.0        24.6  49.4        33.8
             SPARSE WIDE     50%           SET                   20.6  27.4      47.4        25.6  49.6        34.1
             SPARSE WIDE     75%           SET                   22.1  27.8      47.5        25.6  50.4        34.7

E. Đánh giá Hiệu quả so với Wall-clock

Trong phần này chúng tôi cung cấp chi tiết bổ sung về các thiết lập đánh giá cho suy luận trên runtime Neural Magic DeepSparse nhận biết độ thưa thớt (NeuralMagic, 2021; Iofinova et al., 2021; Kurtz et al., 2020) và huấn luyện trên Cerebras CS-2 (Lie, 2023; Cerebras, 2023) để đánh giá hiệu quả của Sparse-IFT đối với thời gian wall-clock.

0.0 0.50.67 0.75 0.80.83 0.86 0.88 0.89 0.9
Mức độ Thưa thớt (%)246810Tăng tốc (hệ số x)
Tăng tốc MatMul so với Mức độ Thưa thớt
trên Lớp GPT-3 (MatMul 12k*12k)
Lý thuyết    Đo được

Hình 7: Tăng tốc đo được so với tăng tốc lý thuyết ở các mức độ thưa thớt khác nhau cho một phép nhân ma trận (MatMul) lớp GPT-3 12k×12k (Lie, 2021).

Thiết lập Suy luận Chúng tôi sử dụng công cụ DeepSparse của Neural Magic để đánh giá các biến thể Sparse-IFT. Việc đánh giá được tiến hành trên Intel Cascade Lake CPUs được tìm thấy trên các instance đám mây AWS G4dn. Những instance này hỗ trợ tập lệnh AVX-512, được sử dụng bởi runtime suy luận DeepSparse để tăng tốc độ thưa thớt không có cấu trúc. Chúng tôi đánh giá các cấu hình khác nhau của mô hình Sparse Wide ResNet-18 với độ thưa thớt ∈ {50%, 75%, 90%} cho suy luận theo batch trên ImageNet. Chúng tôi báo cáo runtime cho batch-inference của 64 hình ảnh ở độ phân giải 224×224.

Thiết lập Huấn luyện Chúng tôi đánh giá hiệu quả huấn luyện của Sparse-IFT trên Cerebras CS-2 hỗ trợ và tăng tốc huấn luyện với độ thưa thớt không có cấu trúc (cả forward và backward pass). Chúng tôi đánh giá tốc độ huấn luyện đo bằng giây/lần lặp. Lưu ý rằng tổng FLOPs của các mô hình trong họ GPT bao gồm FLOPs nhân ma trận và FLOPs attention. FLOPs Attention (tức là được chi tiêu trong multi-head attention) tỷ lệ bậc hai với độ dài chuỗi và không biến đổi với độ thưa thớt trọng số. Để chứng minh hiệu quả của các kernel thưa thớt cho độ thưa thớt trọng số không có cấu trúc, chúng tôi báo cáo kết quả của chúng tôi cho các biến thể dày đặc và Sparse Wide của mô hình GPT-3 20B với độ dài chuỗi 256 và kích thước batch 256. Chúng tôi đánh giá các cấu hình khác nhau của Sparse Wide GPT-3 20B với độ thưa thớt ∈ {50%, 75%, 90%} và báo cáo giây/lần lặp.

Phân tích Đánh giá Hình 5 trong Phần 6 trình bày kết quả đánh giá suy luận và huấn luyện của họ Sparse Wide Sparse-IFT. Trong cả hai thiết lập, chúng tôi đo sự tăng tương đối trong độ trễ hoặc tốc độ huấn luyện cho các biến thể Sparse-IFT so với mô hình dày đặc. Lưu ý rằng các cấu hình Sparse-IFT ở các giá trị độ thưa thớt khác nhau không gây ra thay đổi đáng kể trong FLOPs so với mô hình dày đặc. Trên phần cứng lý tưởng, FLOPs nên dịch trực tiếp thành thời gian wall clock, và do đó, độ trễ suy luận hoặc thời gian huấn luyện cho tất cả các cấu hình Sparse-IFT nên giống như của mô hình dày đặc (đường đen chấm). Ngược lại, khi phần cứng không hỗ trợ độ thưa thớt không có cấu trúc, độ trễ hoặc thời gian huấn luyện của các biến thể Sparse-IFT tăng theo độ thưa thớt (đường xanh lam).

Kết quả trong Hình 5 của Phần 6 cho thấy rằng lên đến 75%, có chi phí tính toán tối thiểu so với huấn luyện mô hình đường cơ sở dày đặc ban đầu. Ở 90% độ thưa thớt, kết quả của chúng tôi nằm giữa hai phổ này (đường xanh lá). Sử dụng runtime suy luận thưa thớt của Neural Magic, chúng tôi quan sát sự giảm đáng kể trong độ trễ suy luận, giảm sự tăng tương đối trong độ trễ từ 19.5x xuống 3.5x. Tương tự, trong trường hợp huấn luyện trên Cerebras CS-2, chúng tôi quan sát sự giảm đáng kể trong thời gian huấn luyện, giảm sự tăng tương đối từ 10.6x xuống 2.8x. Mô hình GPT-3 dày đặc đạt được thông lượng 828.71 lần lặp trên giây trên CS-2, trong khi các biến thể Sparse Wide IFT ghi nhận thông lượng 637.5, 595.3, và 294.3 ở các mức độ thưa thớt tương ứng, dẫn đến chi phí 1.30x, 1.39x, và 2.82x tương ứng.

Trong Hình 7, chúng tôi minh họa các lợi ích có thể đạt được của độ thưa thớt trọng số không có cấu trúc khi sử dụng phần cứng chuyên biệt được thiết kế cho học sâu, chẳng hạn như Cerebras CS-2. Hình này được tái tạo dựa trên biểu đồ trong (Lie, 2021).

F. Đóng góp Tác giả

Chúng tôi cung cấp một tóm tắt về đóng góp của mỗi tác giả:

• Vithursan Thangarasa là một phần không thể thiếu của dự án bằng cách tham gia vào các cuộc thảo luận với Shreyas Saxena và đóng góp vào phương pháp. Anh ấy cũng triển khai tất cả các biến đổi Sparse-IFT trong PyTorch, đề xuất sử dụng tính phi tuyến trong Sparse-IFT, phân tích các phương pháp DST thông qua phân tích phổ, tiến hành các thí nghiệm cho toàn bộ nghiên cứu trên CIFAR-100 và các loại bỏ của nó, có được kết quả ban đầu trên ImageNet, mở rộng Sparse-IFT cho các kiến trúc hiệu quả (ví dụ: BotNet, MobileViT), và đóng góp vào việc viết một số phần của bản thảo.

• Shreyas Saxena nghĩ ra ý tưởng chính về việc khớp FLOPs của biến đổi Sparse Wide với một mô hình dày đặc nhỏ gọn, mở rộng ý tưởng cho các thành viên khác của họ Sparse-IFT, giúp đỡ với việc triển khai, thiết lập tính đa dạng của các thành viên Sparse-IFT để giải thích kết quả, đánh giá Sparse-IFT cho suy luận, và viết một số phần của bản thảo.

• Abhay Gupta xác thực các optimizer thưa thớt trong PyTorch, tiến hành các thí nghiệm với các biến thể Sparse-IFT ResNet trên ImageNet, giúp đỡ với tiền huấn luyện các biến thể Sparse-IFT của GPT trên Cerebras CS-2, tiến hành tất cả các thí nghiệm của Sparse-IFT trên các nhiệm vụ CV downstream, và đóng góp vào việc viết các phần của bản thảo.

• Sean Lie giúp đỡ với việc đưa lên hỗ trợ độ thưa thớt trên Cerebras CS-2 mà rất quan trọng cho việc đánh giá và huấn luyện các biến thể Sparse-IFT của các mô hình GPT, và cung cấp phản hồi để cải thiện cấu trúc và trình bày của bản thảo.
