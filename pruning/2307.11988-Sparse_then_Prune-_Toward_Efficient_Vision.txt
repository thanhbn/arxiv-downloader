# 2307.11988.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2307.11988.pdf
# File size: 669524 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Sparse then Prune: Toward Efficient Vision
Transformers
Yogi Prasetyoa, Novanto Yudistiraa, Agus Wahyu Widodoa
aInformatics Department, Faculty of Computer Science, Universitas Brawijaya, Jalan
Veteran 8, Malang, 65145, Malang, Indonesia
Abstract
The Vision Transformer architecture is a deep learning model inspired
by the success of the Transformer model in Natural Language Processing.
However, the self-attention mechanism, large number of parameters, and
the requirement for a substantial amount of training data still make Vision
Transformers computationally burdensome. In this research, we investigate
the possibility of applying Sparse Regularization to Vision Transformers and
the impact of Pruning, either after Sparse Regularization or without it, on
the trade-off between performance and efficiency. To accomplish this, we ap-
ply Sparse Regularization and Pruning methods to the Vision Transformer
architecture for image classification tasks on the CIFAR-10, CIFAR-100, and
ImageNet-100 datasets. The training process for the Vision Transformer
model consists of two parts: pre-training and fine-tuning. Pre-training uti-
lizes ImageNet21K data, followed by fine-tuning for 20 epochs. The results
show that when testing with CIFAR-100 and ImageNet-100 data, models
with Sparse Regularization can increase accuracy by 0.12%. Furthermore,
applying pruning to models with Sparse Regularization yields even better re-
sults. Specifically, it increases the average accuracy by 0.568% on CIFAR-10
data, 1.764% on CIFAR-100, and 0.256% on ImageNet-100 data compared
to pruning models without Sparse Regularization. Code can be accesed here:
https://github.com/yogiprsty/Sparse-ViT
Keywords: Vision Transformer, Sparse Regularization, Pruning
Preprint submitted to Pattern Recognition Letters July 25, 2023arXiv:2307.11988v1  [cs.CV]  22 Jul 2023

--- PAGE 2 ---
1. Introduction
Modern information technology advancements make it simpler and faster
for people to tackle a variety of difficulties. Humans produce a lot of data
daily due to its broad usage, including search engine activity, online photo
andvideouploads, andmore. Oneeffectiveapproachtoautomatedataclassi-
fication is by utilizing Deep Learning technology. Recently, the most popular
deep learning architecture in digital image processing is the Convolutional
Neural Network (CNN) due to its success in the ILSVRC 2014 GoogleNet
challenge[1]. However, this architecture generally requires a relatively high
computational time, making it possible for new research to develop an archi-
tecture that can outperform CNN.
The self-attention-based architecture, especially the Transformer, has be-
come the preferred choice in natural language processing. In some cases,
the Transformer significantly outperforms convolutional and recurrent layer-
based architectures[2]. Not only in natural language processing, but the use
of Transformers is also starting to expand, including in digital image pro-
cessing. To apply the Vision Transformer, start by dividing the image into
patches or small pieces and provide the sequence of positions of these patches
as input to the Transformer layer. Patches of images are treated like tokens
in natural language processing applications. This method is five times faster
than EfficientNet, which is based on a Convolutional Neural Network. The
accuracy of the Vision Transformer model will increase significantly if it is
pre-trained with large data, such as ImageNet21k, before using it for simpler
cases[3].
In addition to researching better architectures, researchers have proposed
various methods to improve accuracy or speed up computing in Deep Learn-
ing models. The widely used methods include Regularization and Pruning.
Regularization is an additional technique that aims to enhance the model’s
generalization, resulting in better results on test data[4]. On the other hand,
pruning is a popular technique used to obtain a more compact model by re-
moving weights considered less important[5]. This approach allows for faster
computation time while maintaining the model’s accuracy.
Based on the above background, this research applies Sparse Regulariza-
tion and Pruning methods to the Vision Transformer architecture for image
classification tasks utilizing the CIFAR 10 and CIFAR 100 datasets. The
CIFAR 10 dataset comprises 60,000 images with a resolution of 32x32, di-
vided into ten distinct classes, each consisting of 6,000 images. In contrast,
2

--- PAGE 3 ---
the CIFAR 100 dataset includes 60,000 images of the same resolution but
encompasses a broader spectrum of classes, specifically 100 classes. As an
open-source dataset extensively utilized in Deep Learning research, CIFAR
datasets serve as an ideal testbed for evaluating the proposed techniques and
their impact on image classification tasks.
2. Related Work
Research conducted by [3], titled "An Image is Worth 16x16 Words:
TransformersforImageRecognitionatScale,"attemptstoapplypre-training
to the Vision Transformer architecture using varying amounts of data. Con-
sequently, when trained on small data, the Vision Transformer model ex-
hibits poor performance. However, when trained on large data, this model
demonstrates superior performance and surpasses the ResNet model. Other
research also shows the same conclusion when the Vision Transformer is used
to classify mask usage using the MaskedFace-Net dataset. As a result, the
pre-trained ViT-H/14 model performed better than the ViT-H/14 model
without pre-trained, with 1.87% higher on test accuracy[6]. The findings of
this research have inspired the utilization of pre-trained models in the present
study.
Much research has been done to improve Vision Transformer-based archi-
tecture. The Data-efficient image Transformers (DeiT) [7] introduce a CNN
modelasateacherandapplyknowledgedistillation[8]toenhancethestudent
model of ViT in order to lower its dependence on a huge amount of data.
To obtain satisfactory results thus, DeiT only can be trained on ImageNet.
Additionally, the CNN models and distillation types selected may affect the
final performance. The Convolution enhanced Image Transformer (CeiT)[9]
proposed to overcome those limitations. This method works by combining
the advantages of CNN in low-feature extraction, strengthening the locality
of the advantages of Transformers in establishing long-range dependencies.
The Convolutional Vision Transformer (CvT)[10] proposed to improve the
performance and efficiency of ViTs by introducing convolutions into the ViT
architecture. This design introduces convolutions in two core sections of ViT:
a hierarchy of Transformers containing a new convolutional token embedding
and a convolutional Transformer block utilizing a convolutional projection.
Another method called sharpness-aware minimizer (SAM) was proposed
and utilized to explicitly smooths the loss geometry during the training
process[11]. SAM seeks to find solutions that the entire environment has
3

--- PAGE 4 ---
low losses rather than focusing on any single point. The Transformer-iN-
Transformer (TNT)[12] was proposed to enhance the ability of feature repre-
sentation in ViT by dividing the input images into several patches as "visual
sentence" and then divide those patches into sub-patches, as the represen-
tation of "visual word". Apart from using conventional transformer blocks
to extract features and attention of visual sentences, sub-transformers are
also embedded into the architecture to excavate the features and details of
smaller visual words.
Another research conducted by [13], titled "Improvement of Learning for
CNN with ReLU Activation by Sparse Regularization," applies the Sparse
Regularization method to ReLU inputs. The findings demonstrate a sig-
nificant increase in accuracy, ranging from 9.98% to 12.12%, compared to
models without Sparse Regularization. Additionally, compared to the Batch
Normalization method, the accuracy improves by 4.64% to 6.87%. These
compelling results have inspired using the Sparse Regularization method in
the present study.
The research conducted by [14], titled "Vision Transformer Pruning,"
applies the Pruning method to the Vision Transformer architecture. The
findings demonstrate that this method reduces FLOPS (Floating Point Op-
erations per Second) by 55.5% when applied to the Vision Transformer Base
16 architecture. Moreover, the accuracy only decreases by 2% on ImageNet-
1K data. These promising results have inspired using the Pruning method
in the present study.
Previous studies have shown that Vision Transformer active neurons are
highlysparse. Thisimpliesthatonlyasmallportionofneuronswithinalayer
or network become active or "fire" at any given moment. In contrast, most
neurons remain inactive or have low activation values[11]. These findings
indicate that less than 10% of neurons in a Vision Transformer model have
values greater than zero. In simpler terms, this highlights the significant
potential of Vision Transformer models for network pruning.
3. Method
3.1. Datasets
We utilized the widely recognized CIFAR-10 and CIFAR-100 datasets
as standard benchmarks during the experiments. CIFAR-10 comprises RGB
colorimagescategorizedintotenclassesforobjectclassification,whileCIFAR-
100 consists of 100 different classes. Each image in these datasets had been
4

--- PAGE 5 ---
standardized to a resolution of 32x32 pixels. This consistency in size and for-
mat allowed us to focus solely on the intricacies of the data and the impact
of various regularization methods on model performance. The standardized
resolution facilitated seamless comparisons between different models and en-
abled us to discern subtle nuances that might have otherwise gone unno-
ticed. CIFAR datasets is commonly used in research for evaluating perfor-
mance. The research conducted by [15] employs the CIFAR-10 and CIFAR-
100 datasets to evaluate the effectiveness of this regularization method. By
conducting experiments on CIFAR datasets, researchers can gauge the im-
pact of regularization methods on model performance and compare them to
existing approaches.
3.2. Vision Transformer
The Transformer is an attention-based model published in 2017 to replace
the Recurrent layer-based architecture, addressing issues with gradients and
relatively long computation time[2]. The Vision Transformer is an architec-
ture inspired by the success of the Transformer in natural language process-
ing. In 2020, Transformers were employed in image classification problems
and demonstrated superior performance compared to CNN-based architec-
tures, such as EfficientNet[3]. Figure 1 illustrates the architecture of the
Vision Transformer.
Unlike state-of-the-art architecture, The process begins by breaking the
2D images into several parts and converting them into a sequence of flattened
2D patches or 1-dimensional vector, shown in Equation 1, where (H, W )
is the resolution of the original image, Cis the number of channels, Pis
the number of patches and N=HW|P2. During this process, position
embedding is added to the vectors, and these vectors are combined with an
additional vector (class token) that will serve as the output of the Vision
Transformer architecture.
Images that have undergone the Patch and Embedding process will pass
through Transformer Encoder, which includes the Multi-Head Self-Attention
(MSA)layerandtheMulti-LayerPerceptron(MLP).Thenormalizationlayer
isappliedbeforetheMSAandtheMLP.Layernormalizationisusedtoreduce
the computation time of the artificial neural network. This layer operates
by calculating the average and variance of each data input, which are then
used to normalize all inputs [16]. Equation 2 represents the formula for the
normalization layer, where xi,kis vector input.
5

--- PAGE 6 ---
Figure 1: Vision Transformer[3]
x∈RHWC→xp∈RN(P2C)(1)
ˆxi,k=xi,k−µi
p
σ2
i+ϵ(2)
The result will pass through Multi-Head Attention, and this layer works
by dividing the input into several heads, allowing each head to learn a dif-
ferent level of Attention. The heads’ outputs are combined and forwarded
to the Multi-Layer Perceptron. Equation 3 represents the formula for the
Scaled Dot-Product Attention, while Equation 4 represents the formula for
the Multi-Head Attention.
head(Q, K, V ) =softmaxQKt
√dk
V (3)
MultiHead (Q, K, V ) =Concat (head 1... head n) (4)
Thelastlayeroftransformerencodingisthemultilayerperceptron(MLP),
an artificial neural network architecture with one or more hidden layers be-
tween the input and output layers. Training an MLP model involves adjust-
ing the weights and biases between neurons at different layers to minimize
the difference between the model’s output and the desired output[17]. The
6

--- PAGE 7 ---
MLP layer used Gaussian Error Linear Unit (GELU). In certain datasets,
these non-linear activation functions can match and outperform linear func-
tions such as ReLU and ELU[18]. The equation for the GELU activation
function can be seen in Equation 5.
GELU (x) = 0 .5x
1 +tanhhp
2/π 
x+ 0.044715 x3i
(5)
3.3. Sparse Regularization
Sparse regularization is a leading technique in deep learning research that
aims to improve the performance and efficiency of models by promoting spar-
sity in the learned weights. This regularization method encourages a subset
of the weights to become zero or close to zero, resulting in a more compact
and interpretable model[13]. Also, this method has a similar effect to Batch
Normalization and can improve the model’s ability to predict more general
data. Evaluating sparseness can be done in various ways. However, in the
research, sparse regularization is applied to the ReLU input using Equation
6, where hkis the input the k-th ReLU.
S(hk) =log(1 +h2
k) (6)
Once the sparse value is obtained, it is multiplied by λ, a parameter that
controls sparseness, before being added to the loss value L. Further details
can be found in Equation 7.
E=L+λX
kS(hk) (7)
Many explorations have taken place in the field of regularization tech-
niques within deep learning to determine optimal placements within models.
Previous studies have provided insights into the effectiveness of sparse regu-
larization in convolutional neural network (CNN) models, particularly when
applied to the input of ReLU[13]. Figure 2 demonstrates a range of inter-
esting options for strategically incorporating sparse regularization within the
model architecture, each offering unique insights and potential benefits.
Option1suggestsaninterestingplacementforsparseregularization,where
sparsity is evaluated after multiplying queries and keys, followed by scaling,
also known as the similarity score. In option 2, sparsity is evaluated precisely
at the attention weight stage. This value is obtained after the similarity score
7

--- PAGE 8 ---
(a)
 (b)
Figure 2: Choices to put sparse regularization
passes through the softmax activation function. Option 3 applied sparse reg-
ularization after multiplying attention weight with V (value), also known as
weighted value. Option 4, sparsity, is evaluated after the linear or output
layer in Multi-Head Attention. Last option, according to previous research,
we will apply sparse regularization to the input of GELU activation function
in MLP layer.
3.4. Pruning
Pruning is a method used to speed up computation in deep learning
models by removing the least important parameters. Various approaches
for deleting unneeded parameters can be used, such as using the l1-norm or
l2-norm to select the target parameters for deletion[5]. Pruning is broadly
classified into unstructured and structured pruning, each with its strategy
for parameter reduction[19].
Structured pruning operates by systematically removing structured com-
ponents within deep learning models. These components may manifest as
filters[20], channels, or even entire layers, enabling a more holistic approach
toparameterreduction[21]. Byselectivelyexcisingthesestructuredelements,
8

--- PAGE 9 ---
deep learning models can be streamlined, improving efficiency and reducing
computational complexity.
In contrast, unstructured pruning (algorithm 1) focuses on individual
weights within layers or filters. This surgical precision allows for the precise
removal of individual weights without disrupting the overall arrangement
of the deep learning model. Unstructured pruning can still achieve signif-
icant parameter reduction while maintaining the accuracy of the pruned
network[22], resulting in faster computations and optimized model perfor-
mance.
Algorithm 1: Global Pruning with L1 Unstructured Method
Input : model, pruning_ratio
Output: model
parameters ←[];
forparam inmodel.parameters() do
parameters.append(param);
all_parameters ←torch.cat([param.data.view(-1) forparamin
parameters]);
threshold ←torch.kthvalue(torch.abs(all_parameters),
int(pruning_ratio * all_parameters.numel())).values;
forparam inparameters do
param_data ←param.data;
mask←torch.abs(param_data) >threshold;
pruned_param ←param_data * mask.float();
param.data ←pruned_param;
returnmodel;
3.5. Transfer Learning
The Vision Transformer performs remarkably when complemented by a
pre-training process known as transfer learning [3]. Transfer learning in-
volves training the model on a large dataset before applying it to other data,
leveraging the knowledge acquired from the pre-training phase to enhance
performance on new tasks. In this context, the abundance of data and
its diverse nature play pivotal roles in facilitating effective transfer learn-
ing. Large amounts of data and a wide diversity are the most influential
factors for transfer learning [23]. To exploit the full potential of transfer
learning, this research endeavors to capitalize on the Vision Transformer ar-
chitecture, which has undergone extensive training using ImageNet21k data.
9

--- PAGE 10 ---
ImageNet21k boasts an impressive collection of 14,197,122 annotated images,
accurately organized according to the WordNet hierarchy[24]. This vast and
diverse dataset encapsulates numerous objects, scenes, and concepts, provid-
ing rich visual information for the Vision Transformer to learn from.
3.6. Model Training
The ViT architecture can be configured in multiple ways. To ensure un-
biased test results for each configuration, constant hyperparameters will be
set during training for the ViT architecture’s various configurations. For this
research, we will maintain the same setup used in the original ViT research,
as described in reference [3]. During the fine-tuning process, the following
hyperparameters were employed: a batch size of 64, a learning rate of 0.03,
20 epochs, a Cross-Entropy loss function[25], Stochastic Gradient Descent
optimizer, and GeLU activation function[18]. For transfer learning, we uti-
lized pre-trained weights initially trained on the ImageNet21K dataset[26].
The experiments were conducted using hardware specifications that included
an RTX 8000 GPU, an Intel(R) Xeon(R) Gold 6230R processor, and 255 GiB
of RAM.
4. Experiment
To confirm the effectiveness of the implemented method, we have per-
formed experiments on the testing of the Vision Transformer using CIFAR-10
and CIFAR-100 datasets.
4.1. ViT Architecture
According to the original research paper, the authors introduced several
Vision Transformer (ViT) model variants. They explored different model
configurations to investigate the impact of architectural choices on its per-
formance. There are three variants of the vision transformer model. The
first is ViT-Base. This variant serves as the baseline model, featuring a
transformer architecture with a moderate number of layers and attention
heads. It serves as a reference point for comparing the performance of other
ViT models. The Second is ViT-large. This variant extends the ViT model
with more layers and attention heads, increasing its capacity and potential
for capturing complex visual patterns. ViT-large aims to achieve higher ac-
curacy by leveraging a deeper and more parameter-rich architecture. The
last is ViT-Huge; as the name suggests, this variant represents an even more
10

--- PAGE 11 ---
expansive and powerful instantiation of the Vision Transformer. It features
significantly larger layers and attention heads, providing a massive capacity
for learning intricate visual representations. We have done the experiments
using ViT-B/16 architecture. More detailed configuration and hyperparam-
eter for Vision Transformer are shown in the Tabel 1.
Table 1: Configurations and Hyperparameters for ViT-B-16
Configuration Value
image resolution 384 ×384
patch resolution 16 ×16
learning rate 0.001
weight decay 0.0001
batch size 16
hidden size 768
mlp size 3072
#heads 12
encoder length 12
4.2. Sparse Regularization Effect
We have performed experiments to evaluate the effectiveness of sparse
regularization. There are five scenarios for implementing sparse regulariza-
tion: similarity score, attention weight, weighted value, the output layer, and
the input GELU activation function at the MLP layer. The λvalue used is
1
n_feature. Before testing, the vision transformer model will be fine-tuned on
the Cifar 10 and Cifar 100 datasets. The Vision Transformer model without
using the sparse regularization method will also be fine-tuned with the same
data for comparison. The comparison of sparse regularization placement in
the Vision Transformer model is shown in Table 2.
The experiment used the CIFAR-10 dataset and the Vision Transformer
model, which had been fine-tuned for 20 epochs and applied with sparse reg-
ularization. As a result, after calculating the Attention Weight, the model
achieved the highest accuracy of 98.81% when sparse regularization was ap-
plied to the Self-Attention layer. On the other hand, the lowest accuracy of
11

--- PAGE 12 ---
Table 2: Result on CIFAR-10
Layer Sparse Position Acc
- - 98.83
attention similarity score 98.57
attention attention weight 98.81
attention weighted value 98.73
attention output 98.33
MLP input GELU 98.52
the model with sparse regularization was 98.33% when the sparse regular-
ization was applied after the output layer calculation in the Self-Attention
layer. However, despite sparse regularization, the best results from the model
still could not outperform the baseline model, which achieved an accuracy
of 98.81%. The comparison of sparse regularization placement in the Vision
Transformer model is shown in Table 2.
Table 3: Result on CIFAR-100
Layer Sparse Position Acc
- - 92.39
attention similarity score 91.51
attention attention weight 92.52
attention weighted value 92.17
attention output 92.13
MLP input GELU 91.73
The second experiment was conducted using the CIFAR-100 dataset and
the Vision Transformer model, which had been fine-tuned for 20 epochs and
applied with sparse regularization. As a result, the model achieved the high-
est accuracy of 92.52% when sparse regularization was applied to the Self-
Attention layer after calculating the Attention Weight. On the other hand,
thelowestaccuracywas91.73%whenthesparseregularizationwasappliedto
the MLP layer before calculating the GELU activation function. The model
with sparse regularization achieved an accuracy increase of 0.12%, outper-
12

--- PAGE 13 ---
formingthebaselinemodel. Thisindicatesthatdatawithmoreclassesmakes
the vision transformer model have many sparse active neurons. The pruning
method will work better if it is applied to CIFAR-100 datasets.
4.3. Pruning Effect
ToevaluatetheimpactofpruningontheperformanceoftheVisionTrans-
former, we have conducted a comprehensive series of experiments by explor-
ing the effects of varying pruning percentages on both the CIFAR-10 and
CIFAR-100 datasets. Through these experiments, we strive to gauge the in-
fluence of pruning on accuracy and identify optimal pruning thresholds for
the Vision Transformer architecture. We use a range of pruning percentages
to achieve this goal, from 10% to 30% of the model weight. This system-
atic approach allowed us to evaluate the pruned Vision Transformer’s per-
formance under different weight reduction levels. The pruning was applied
globally, ensuring a uniform impact across the entire model. The comparison
of accuracy results using the pruning method on CIFAR-10 and CIFAR-100
datasets is shown in Figure 3.
Figure 3: Pruning effect on CIFAR-10 and CIFAR-100
The experiment was conducted using CIFAR-10 and CIFAR-100 datasets
with the ViT-B-16 model, which had been fine-tuned for 20 epochs. After
training the model, pruning was performed on all layers with weights. The
weights were determined to be pruned using the l1-norm method. As a result,
13

--- PAGE 14 ---
both datasets showed a negative correlation between the decrease in accuracy
and the percentage of pruned parameters: the greater the number of pruned
parameters, the lower the accuracy. These results underscore the need for
careful consideration and optimization during pruning. While pruning offers
potential for model compression and computational efficiency, it must be
done judiciously to reduce the loss of accuracy. Researchers can navigate this
trade-off by leveraging techniques such as the L1-norm method to determine
the weights to trim, seeking the ideal balance between model cohesiveness
and performance.
4.4. Effect of Sparse then Prune
Based on previous experiments, the best accuracy was achieved by ap-
plying sparse regularization after calculating the attention weight on the Self
Attention layer. Therefore, in this test, the model was trained with sparse
(a)
 (b)
Figure 4: (a) Result on CIFAR-10. (b) Result on CIFAR-100
regularization placed after calculating the attention weight on the Self Atten-
tionlayerbeforepruning. Similartotheprevioustest,thepruningpercentage
used ranged from 10% to 30% and was applied globally.
Pruning performed on models with sparse regularization produces higher
accuracy than those without sparse regularization. This indicates that sparse
regularization can effectively distinguish between important and unimpor-
tant parameters or weights. Consequently, when unimportant parameters
are pruned or deleted, the model with sparse regularization demonstrates
better data classification capabilities than the baseline model. Figures 4a
and 4b compare the accuracy of pruning with and without sparse regulariza-
tion on the Vision Transformer model. In the case of CIFAR-10, the average
14

--- PAGE 15 ---
difference in accuracy is 0.568%, while for CIFAR-100, the average difference
in accuracy is 1.764%.
4.5. Result on CIFAR-10 and CIFAR-100
All models are pre-trained on ImageNet before being used in downstream
tasks with smaller datasets. A comparison of the Transformer-based spec-
ification model can be seen in Table 4. The model with ↓10% means the
model is pruned with a 10% pruning ratio. Besides, the model with ↑384 is
fine-tuned on a bigger resolution, which is 384 ×384 pixel.
Table 4: Model Specification
Model #params Image
SizeHidden size #heads
ViT-B/16-Sparse 86M 384 768 12
ViT-B/16-Sparse ↓10% 77M 384 768 12
ViT-B/16-Sparse ↓15% 73M 384 768 12
ViT-B/16-Sparse ↓20% 69M 384 768 12
ViT-B/16-Sparse ↓25% 64M 384 768 12
ViT-B/16-Sparse ↓30% 60M 384 768 12
ViT-B/16-SAM 87M 244 768 12
DeiT-B ↑384 86M 384 768 12
CeiT-S ↑384 24M 384 768 12
TNT-B ↑384 65M 384 40+640 4+10
Table 5 report numerical result on CIFAR-10 and CIFAR-100 as a down-
stream task. Interestingly, with almost the same parameters as defined in
the original paper [3], Vision Transformer with sparse regularization takes
first place on average accuracy.
4.6. Result on ImageNet-100
To provide a broader overview of the sparse regularization effect on prun-
ing, this experiment has also been carried out using ImageNet-100, a subset
of ImageNet1k. The sampling process was carried out randomly by tak-
ing 100 classes from ImageNet1k dataset. As a result, the ViT model with
15

--- PAGE 16 ---
Table 5: Accuracy comparison on CIFAR-10 and CIFAR-100
Model CIFAR-10 CIFAR-100 Average
ViT-B/16-Sparse 98.81 92.52 95.66
ViT-B/16-Sparse ↓10% 98.71 91.66 95.18
ViT-B/16-Sparse ↓15% 98.16 89.62 93.89
ViT-B/16-Sparse ↓20% 96.68 84.88 90.78
ViT-B/16-Sparse ↓25% 93.03 74.67 83.85
ViT-B/16-Sparse ↓30% 85.09 59.47 72.28
ViT-B/16-SAM 98.2 87.6 92.9
DeiT-B ↑384 99.1 90.8 94.95
CeiT-S ↑384 99.1 90.8 94.95
TNT-B ↑384 99.1 91.1 95.1
sparse regularization can overcome the original ViT model with 0.12% higher
accuracy. The detail can be seen in Tabel 6.
Table 6: Accuracy on ImageNet-100
Model Sparse Position Accuracy
ViT-B/16 - 96.8
ViT-B/16-Sparse Attention Weight 96.92
Figure 5 shows the effect of pruning performed on the model that applies
sparse regularization. It can be seen that the result has a similar pattern
to the experiment that has been done on CIFAR-10 and CIFAR-100 data.
This method produces higher accuracy than pruning performed on models
without sparse regularization, with 0.25% higher accuracy on average.
5. Conclusion
The implementation of sparse regularization will produce the best accu-
racy if it is placed after the attention weight calculation on the self-attention
layer. Pruningonmodelswithsparseregularizationproducesbetteraccuracy
than pruning on models without sparse regularization.
16

--- PAGE 17 ---
Figure 5: Pruning effect on ImageNet-100
The Vision Transformer model with sparse regularization can improve ac-
curacy by 0.12% on CIFAR-100 and 0.12% on ImageNet-100. Meanwhile, on
CIFAR-10 data, the model with sparse regularization has yet to outperform
the baseline model. While there is a negative correlation between pruning
and accuracy, with the accuracy decreasing as the percentage of pruned pa-
rameters increases, models with sparse regularization tend to have a slightly
higher average accuracy. In particular. On CIFAR 10 data. Pruning on mod-
els with sparse regularization achieves an average higher accuracy of 0.568%.
Similarly, onCIFAR100data. Theaveragehigheraccuracyachievedthrough
pruning on models with sparse regularization is 1.764% and 0.256% higher
on ImageNet-100.
In summary, our sparse regularization and pruning explorations have re-
vealed a delicate interplay between these techniques and model accuracy.
Sparse regularization, when strategically placed, can unlock the true poten-
tial of deep learning models, enhancing accuracy in certain contexts. Fur-
thermore, the combination of sparse regularization and pruning presents a
compelling approach to mitigating the negative impact of pruning on accu-
racy.
17

--- PAGE 18 ---
References
[1] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, A. Rabinovich, Going deeper with convolu-
tions, CoRR abs/1409.4842 (2014). arXiv:1409.4842 .
URL http://arxiv.org/abs/1409.4842
[2] A.Vaswani, N.Shazeer, N.Parmar, J.Uszkoreit, L.Jones, A.N.Gomez,
L.Kaiser,I.Polosukhin,Attentionisallyouneed,CoRRabs/1706.03762
(2017). arXiv:1706.03762 .
URL http://arxiv.org/abs/1706.03762
[3] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, N. Houlsby, An image is worth 16x16 words: Trans-
formers for image recognition at scale, CoRR abs/2010.11929 (2020).
arXiv:2010.11929 .
URL https://arxiv.org/abs/2010.11929
[4] J. Kukacka, V. Golkov, D. Cremers, Regularization for deep learning: A
taxonomy, CoRR abs/1710.10686 (2017). arXiv:1710.10686 .
URL http://arxiv.org/abs/1710.10686
[5] L.Liebenwein, C.Baykal, B.Carter, D.Gifford, D.Rus, Lostinpruning:
The effects of pruning neural networks beyond test accuracy, CoRR
abs/2103.03014 (2021). arXiv:2103.03014 .
URL https://arxiv.org/abs/2103.03014
[6] H. D. Jahja, N. Yudistira, Sutrisno, Mask usage recognition us-
ing vision transformer with transfer learning and data augmen-
tation, Intelligent Systems with Applications 17 (2023) 200186.
doi:https://doi.org/10.1016/j.iswa.2023.200186 .
URL https://www.sciencedirect.com/science/article/pii/
S266730532300011X
[7] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, H. Jégou,
Training data-efficient image transformers & distillation through atten-
tion, CoRR abs/2012.12877 (2020). arXiv:2012.12877 .
URL https://arxiv.org/abs/2012.12877
18

--- PAGE 19 ---
[8] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural
network (2015). arXiv:1503.02531 .
[9] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, W. Wu, Incorporating convo-
lution designs into visual transformers (2021). arXiv:2103.11816 .
[10] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, L. Zhang, Cvt:
Introducing convolutions to vision transformers, CoRR abs/2103.15808
(2021). arXiv:2103.15808 .
URL https://arxiv.org/abs/2103.15808
[11] X. Chen, C. Hsieh, B. Gong, When vision transformers outper-
form resnets without pretraining or strong data augmentations, CoRR
abs/2106.01548 (2021). arXiv:2106.01548 .
URL https://arxiv.org/abs/2106.01548
[12] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, Y. Wang, Transformer in
transformer, CoRR abs/2103.00112 (2021). arXiv:2103.00112 .
URL https://arxiv.org/abs/2103.00112
[13] H. Ide, T. Kurita, Improvement of learning for cnn with relu activation
by sparse regularization, in: 2017 International Joint Conference on
Neural Networks (IJCNN), 2017, pp. 2684–2691. doi:10.1109/IJCNN.
2017.7966185 .
[14] M. Zhu, K. Han, Y. Tang, Y. Wang, Visual transformer pruning, CoRR
abs/2104.08500 (2021). arXiv:2104.08500 .
URL https://arxiv.org/abs/2104.08500
[15] X. Gastaldi, Shake-shake regularization, CoRR abs/1705.07485 (2017).
arXiv:1705.07485 .
URL http://arxiv.org/abs/1705.07485
[16] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization (2016). arXiv:
1607.06450 .
[17] H. Ramchoun, M. Amine, J. Idrissi, Y. Ghanou, M. Ettaouil, Multilayer
perceptron: Architecture optimization and training, International Jour-
nal of Interactive Multimedia and Artificial Inteligence 4 (2016) 26–30.
doi:10.9781/ijimai.2016.415 .
19

--- PAGE 20 ---
[18] D. Hendrycks, K. Gimpel, Bridging nonlinearities and stochastic regu-
larizers with gaussian error linear units, CoRR abs/1606.08415 (2016).
arXiv:1606.08415 .
URL http://arxiv.org/abs/1606.08415
[19] J. van Amersfoort, M. Alizadeh, S. Farquhar, N. D. Lane, Y. Gal, Single
shot structured pruning before training, CoRR abs/2007.00389 (2020).
arXiv:2007.00389 .
URL https://arxiv.org/abs/2007.00389
[20] Y. He, G. Kang, X. Dong, Y. Fu, Y. Yang, Soft filter pruning for ac-
celerating deep convolutional neural networks, CoRR abs/1808.06866
(2018). arXiv:1808.06866 .
URL http://arxiv.org/abs/1808.06866
[21] Y. He, L. Xiao, Structured pruning for deep convolutional neural net-
works: A survey (2023). arXiv:2303.00566 .
[22] C. Laurent, C. Ballas, T. George, N. Ballas, P. Vincent, Revisiting loss
modelling for unstructured pruning (2020). arXiv:2006.12279 .
[23] K. Weiss, T. M. Khoshgoftaar, D. Wang, A survey of transfer learning,
Journal of Big Data 3 (1) (2016) 9. doi:10.1186/s40537-016-0043-6 .
URL https://doi.org/10.1186/s40537-016-0043-6
[24] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, L. Fei-
Fei, Imagenet large scale visual recognition challenge, International
Journal of Computer Vision 115 (3) (2015) 211–252. doi:10.1007/
s11263-015-0816-y .
URL https://doi.org/10.1007/s11263-015-0816-y
[25] Z. Zhang, M. R. Sabuncu, Generalized cross entropy loss for training
deep neural networks with noisy labels, CoRR abs/1805.07836 (2018).
arXiv:1805.07836 .
URL http://arxiv.org/abs/1805.07836
[26] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A
large-scale hierarchical image database, in: 2009 IEEE Conference on
Computer Vision and Pattern Recognition, 2009, pp. 248–255. doi:
10.1109/CVPR.2009.5206848 .
20
