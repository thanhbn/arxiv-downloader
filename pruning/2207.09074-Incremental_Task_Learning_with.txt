# 2207.09074.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2207.09074.pdf
# File size: 1308449 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Incremental Task Learning with
Incremental Rank Updates
Rakib Hyder1, Ken Shao1, Boyu Hou1, Panos Markopoulos2,
Ashley Prater-Bennette3, and M. Salman Asif1
1University of California Riverside
2Rochester Institute of Technology
3Air Force Research Laboratory
Abstract. Incremental Task learning (ITL) is a category of continual
learning that seeks to train a single network for multiple tasks (one after
another), where training data for each task is only available during the
training of that task. Neural networks tend to forget older tasks when
they are trained for the newer tasks; this property is often known as
catastrophic forgetting. To address this issue, ITL methods use episodic
memory, parameter regularization, masking and pruning, or extensible
network structures. In this paper, we propose a new incremental task
learning framework based on low-rank factorization. In particular, we
represent the network weights for each layer as a linear combination
of several rank-1 matrices. To update the network for a new task, we
learn a rank-1 (or low-rank) matrix and add that to the weights of every
layer. We also introduce an additional selector vector that assigns dif-
ferent weights to the low-rank matrices learned for the previous tasks.
We show that our approach performs better than the current state-of-
the-art methods in terms of accuracy and forgetting. Our method also
offers better memory efficiency compared to episodic memory- and mask-
based approaches. Our code will be available at https://github.com/
CSIPlab/task-increment-rank-update.git
1 Introduction
Deep neural networks have been extremely successful for a variety of learning and
representation tasks (e.g., image classification, object detection/segmentation,
reinforcement learning, generative models). A typical network is trained to learn
a function that maps input to the desired output. The input-output relation is
assumed to be fixed and input-output data samples are drawn from a stationary
distribution [25]. If the input-output relations or data distributions change, the
network can be retrained using a new set of input-output data samples. Since the
storage, computing, and network capacity are limited, we may need to replace
old data samples with new samples. Furthermore, privacy concerns may also
force data samples to be available for a limited time [10,25]. In such a training
process, a network often forgets the previously learned tasks; this effect is termed
catastrophic forgetting [21,26].arXiv:2207.09074v1  [cs.CV]  19 Jul 2022

--- PAGE 2 ---
2 R. Hyder et al.
Fig. 1: An overview of our proposed method for continual learning via low-rank net-
work updates. We first represent (and learn) the weight matrix (or tensor) for each
layer as a product of low-rank matrices. To train a network for new tasks without
forgetting the earlier tasks, we reuse the factors from the earlier tasks and add a new
set of factors for the new task. Our experiments suggest that a rank-1 update is often
sufficient for successful continual learning.
Incremental task learning is a subcategory of continual learning or lifelong
learning approaches aim to address the problem of catastrophic forgetting by
adapting the network or training process to learn new tasks without forgetting
the previously learned ones [23,16,3,2,4,6,28,29,12]. In this paper, we focus on
task-incremental continual learning in which data for every task are provided
in a sequential manner to train/update the network [8]. It has been a popular
continual learning setup even in the very recent literature [7,14,11,36,31,40].
ITL finds its application in setups where task id is available during inference; for
instance, tasks performed under different weather/light/background conditions
and we know the changes, or tasks learned on different data/classes where we
know the task id.
Let us denote the network function that maps input xto output for task tas
f(x;Wt), where Wtdenotes the network weights for task t. We seek to update
theWtfor all tas we sequentially receive dataset for one task at a time. Suppose
the training dataset for task tis given as ( Xt,Yt) drawn from a distribution Pt,
where Xtdenotes the set of input samples and Ytdenotes the corresponding
ground-truth outputs. Our goal is to update network weights from the previous
task (Wt−1) toWtsuch that
y≈f(x;Wt),for all ( x, y)∼ Pt. (1)
ITL setup above assumes that the task identity of test samples is known at the
test time and the corresponding network weights are used for inference. Dynamic
architecture approaches have the potential to achieve zero forgetting , using Wt
for testing data for task t; however, this also requires storing the Wtfor all the

--- PAGE 3 ---
Incremental Task Learning with Incremental Rank Updates 3
tasks. One of the main contributions of this paper is to represent, learn, and
update the Wtusing low-rank factors such that they can be stored and applied
with minimal memory and computation overhead.
We propose a new method for ITL that updates network weights using rank-1
(or low-rank) increments for every new task. Figure 1 provides an illustration of
our proposed method. We represent the network weights for each layer as a linear
combination of several low-rank factors (which can be represented as a product
of two low-rank matrices and a diagonal matrix). To update the network for
tasktwithout forgetting the earlier tasks, we freeze the low-rank factors learned
from the previous tasks, add a new trainable rank-1 (or low-rank) factor for
every layer, and combine that with the older factors using learnable selector
weights (shown as a diagonal matrix). We use a multi-head configuration that
has an independent output layer for each task. As we are learning separate
diagonal matrices for every task, we can achieve zero forgetting during inference.
We present an extensive set of experiments to demonstrate the performance of
our proposed method for different benchmark datasets. We observe that our
proposed method outperforms the current state-of-the-art methods in terms of
accuracy with small memory overhead.
The main contributions of this paper are as follows.
1.Represent layers as low-rank matrices: We represent and learn net-
work weights for each layer as a low-rank structure. We show that low-rank
structure is sufficient to represent all the tasks in continual learning setup.
2.Reuse old factors for better performance with a small memory
overhead: We limit the number of parameters required for network update
by reusing the factors learned from previous tasks. We demonstrate that a
rank-1 increment suffices to outperform the existing techniques.
3.Zero forgetting without replay buffer: Our method has zero forgetting
that is achieved using incremental rank update or network weights. In con-
trast, most of the existing continual learning techniques require replay buffer
or large memory overhead to achieve zero forgetting.
Limitations. Our method shares same inherent limitation of ITL (i.e. the re-
quirement of task-id during inference). In addition, since we use all the previously
learned factors for inference, the later tasks require more memory and compu-
tation for inference. Nevertheless, we show that using low-rank structure, our
total memory requirement is significantly lower than a single network. Further-
more, as we learn separate diagonal matrices for each task, we can maintain high
performance even if the network reaches full rank with a large number of tasks.
2 Background and Related Work
Incremental task learning (ITL) [10,34] aims to train a single model on a sequence
of different tasks and perform well on all the trained tasks once the training is
finished. While training on new tasks, the old data from previous tasks will
not be provided to the model. This scenario mimics the human learning process

--- PAGE 4 ---
4 R. Hyder et al.
where they have the ability to acquire new knowledge and skills throughout their
lifespan. However, this setting is still challenging to neural network models as
a common phenomenon called ”catastrophic forgetting [21]” is observed during
this learning process. Catastrophic forgetting occurs when the data from the
new tasks interfere with the data seen in the previous tasks and thus deterio-
rating model performance on preceding tasks. To overcome this issue, different
approaches have been proposed so far which can be divided into three main cat-
egories: regularization-based approaches, memory and replay-based approaches,
and dynamic network architecture-based approaches. Some of these approaches
are especially designed for ITL whereas others are designed for more general
continual learning setup.
Regularization-based approaches [15,23,16] update the whole model in
each task but a regularization term ℓregis added to the total loss L=ℓcurrent +
λℓregto penalize changes in the parameters important to preceding tasks thus
preserving the performance on previous learned tasks. For example, Elastic
Weight Consolidation (EWC) [15] estimates the importance of parameters using
Fisher Information matrix; Variational Continual Learning (VCL) [23] approx-
imates the posterior distribution of the parameters using variational inference;
Learning without Forgetting (LwF) [16] regularizes the current loss with soft tar-
gets taken from previous tasks using knowledge distillation [13]. GCL [5] mixes
rehearsal with knowledge distillation and regularization to mitigate catastrophic
forgetting. A number of recently proposed methods force weight updates to be-
long to the null space of the feature covariance [37,35].
Memory-based approaches [27,28,8,9,35] usually use memory and re-
play/rehearsal mechanism to recall a small episodic memory of previous tasks
while training new tasks thus reduce the loss in the previous tasks. For example,
iCaRL [27] is the first replay method, which learns in a class-incremental way by
selecting and storing exemplars closest to the feature mean of each class; Meta-
Experience Replay (MER) [28] combines experience replay with optimization-
based meta-learning to optimize the symmetric trade-off between transfer and
interference by enforcing gradient alignment across examples; AGEM [8] projects
the gradient on the current minibatch by using an external episodic memory of
patterns from previous experiences as an optimization constraint; ER-Ring [9]
jointly trains new task data with that of the previous tasks.
Dynamic network architectures [30,19,39,38,33,7,41] try to add new neu-
rons to the model at additional new tasks, thus the performances on previous
tasks are preserved by freezing the old parameters and only updating the newly
added parameters. For example, Progressive neural networks (PNNs) [30] lever-
age prior knowledge via lateral connections to previously learned features; Pack-
Net [19] iteratively assigns parameter subsets to consecutive tasks by constituting
binary masks. SupSup [39] also finds masks in order to assign different subsets
of the weights for different tasks. BatchEnsemble [38] learns on separate rank-1
scaling matrices for each task which are then used to scale weights of the shared
network. HAT [33] incorporates task-specific embeddings for attention mask-
ing. [24] also proposes task-conditioned hypernetworks for continual learning.

--- PAGE 5 ---
Incremental Task Learning with Incremental Rank Updates 5
[20] proposes nonoverlapping sets of units being active for each task. Piggyback
[18] learns binary masks on an existing network to provide good performance
on new tasks. [1] proposes task specific convolutional filter selection for contin-
ual learning. The mask-based methods listed above provide excellent results for
continual learning, but they require a significantly large number of parameters
to represent the masks for each task. A factorization-based approach was pro-
posed in [22] that performs automatic rank selection per task for variational
inference using Indian Buffet process. The method requires significantly large
rank increments per task to achieve high accuracy; in contrast, our method uses
a learning-based approach to find rank-1 increments and reuse old factors with
the learned selector weights. ORTHOG-SUBSPACE [7] learns tasks in different
(low-rank) vector sub-spaces that are kept orthogonal to each other in order to
minimize interference.
Our proposed method falls under the category of dynamic network architec-
ture approaches. Note that we can represent a low-rank weight matrix using two
smaller fully-connected layers and increasing the rank of the weight matrix is
equivalent to adding new nodes in the two smaller fully-connected layers.
3 Incremental Task Learning via Rank Increment
We focus on the incremental task learning setup in which we seek to train a
network for Ttasks. The main difference between incremental task learning
and regular learning is that the training data for every task is only available
while training the network for that task. The main challenge in incremental task
learning is to not forget the previous tasks as we learn new tasks. Learning each
task entails training weights for the network to learn the task-specific input-
output relationship using the task-specific training data.
We seek to develop an ITL framework in which we represent the weights of
any layer using a small number of low-rank factors. We initialize the network
with a base architecture in which weights for each layer can be represented using
a low-rank matrix. We then add new low-rank factors to each layer as we learn
new tasks.
Let us assume the network has Klayers and the weights for the kth layer
and task tcan be represented as Wk,t. Let us further assume that the weights
for the kth layer and task t= 1 can be represented as a low-rank matrix
Wk,1=Uk,1Sk,1,1V⊤
k,1, (2)
where Uk,1, Vk,1represent two low-rank matrices and Sk,1,1represents a diagonal
matrix. To learn the network for task 1, we learn Uk,1, Vk,1, Sk,1,1for all k. For
task 2, we represent the weights for kth layer as
Wk,2=Uk,1Sk,1,2V⊤
k,1+Uk,2Sk,2,2V⊤
k,2.
Uk,1, Vk,1represent the two low-rank matrices learned for task 1 and frozen
afterwards. Uk,2, Vk,2represent two low-rank matrices that are added to update

--- PAGE 6 ---
6 R. Hyder et al.
the weights, and these will be learned for task 2. Sk,1,2, Sk,2,2represent the
diagonal matrices, which will be learned for task 2. We learn Sk,1,2, which is
a diagonal matrix that assigns weights to factors corresponding to task 1, to
include/exclude or favor/suppress frozen factors from previous tasks for the new
tasks. We can represent the weights for the kth layer and task tas
Wlayer,task =Wk,t=X
i≤tUk,iSk,i,tV⊤
k,i
=X
i<tUk,i|{z}
frozenSk,i,tV⊤
k,i|{z}
frozen+Uk,tSk,t,tV⊤
k,t, (3)
where Uk,i, Vk,iare frozen for all i < t andUk,t, Vk,tand all the Sk,i,tare
learned for task t. The entire network for task tcan be represented as Wt=
{Uk,i, Sk,i,t, Vk,i}i≤t. To update the trainable network parameters for task t, we
solve the following optimization problem:
min
Uk,t,Sk,i,t,Vk,tX
(x,y)∈(Xt,Yt)loss(f(x;Wt[Uk,t, Sk,i,t, Vk,t]), y)
for all k≤Kandi≤t, (4)
where we use loss( ·,·) to denote the loss function and Wt[Uk,t, Sk,i,t, Vk,t] to
indicate the trainable parameters in Wt, while the rest are frozen. We sometimes
callSk,i,taselector weight matrix/vector to indicate that its diagonal entries
determine the contribution of each factor toward each task/layer weights.
Our proposed ITL algorithm works as follows. We train the low-rank factors
for the given task using the respective training samples. Then we freeze the
factors corresponding to the older tasks and only update the new factors and
the diagonal matrices. In this manner, the total number of parameters we add
in our model is linearly proportional to the rank of the new factors. To keep the
network complexity small, we seek to achieve good accuracy using small rank for
each task update and layer. We summarize our approach in Algorithms 1 and 2.
Note that we do not need to create the weight matrix Wk,tfor any layer ex-
plicitly since we can compute all the steps in forward and backward propagation
efficiently using the factorized form of each layer. The size of each layer is deter-
mined by the choice of the network architecture. The rank of each layer for every
task is a hyper-parameter that we can select according to the tasks at hand. To
keep the memory overhead small, we need to use small values for rank increment.
Let us denote the rank for Uk,tasrk,t, which represents the increment rank for
kth layer and task t. At the time of test, we can use an appropriate number of
factors depending on the task. For instance, if we want to predict output for
task 1 then we use first rk,1factors and for task 2 we use rk,1+rk,2factors. We
can add new factors in an incremental manner as we add new tasks in the ITL
setup. In the extreme case of rank-1 increments, rk,t= 1. In our experiments,
we observed that rank-1 updates compete or exceed the performance of existing
ITL methods (see Table 1) and the performance of our method improves further
as we increase the rank (see Table 5). Any increase in the rank comes at the
expense of an increased memory overhead.

--- PAGE 7 ---
Incremental Task Learning with Incremental Rank Updates 7
Algorithm 1 ITL with rank-1 increments (Training)
Input: Data ( X1andY1) for the 1sttask.
Set initial rank, r1.
Initialize weight factors Uk,1, Vk,1at random and Sk,1,1as an identity marix.
Learn Uk,1, Vk,1andSk,1,1. ▷Optimization in (4)
fort= 2,3, ..., T do
Input: Training data ( XtandYt) for tthtask.
Initialize low-rank update factors Uk,t, Vk,t.
Freeze the previous factors {Uk,i, Vk,i}i<t.
Initialize the diagonal entries of {Sk,i,t}as 1
fori=tand 0 for i < t.
Learn Uk,t, Vk,tandSk,i,t
fori < t. ▷Optimization in (4)
end for
Algorithm 2 ITL with rank-1 increments (Inference)
Input: Test data xwith task identity t.
Retrieve trained weights: Wt={Uk,i, Vk,i, Sk,i,t}for all kandi≤t.
Output: Calculate the network output as f(x,Wt).
4 Experiments and Results
We used different classification tasks on well known continual learning bench-
marks to show the significance of our proposed approach.
4.1 Datasets and Task Description
Experiments are conducted on four datasets: Split CIFAR100, Permuted MNIST,
Rotated MNIST, and Split MiniImageNet.
P-MNIST creates new tasks by applying a certain random permutation on the
pixels of all images in the original dataset. In our experiment, we generate 20
different tasks, each of which corresponds to a certain but different permutation.
R-MNIST is similar to Permuted MNIST, but instead of applying a certain
random permutation on the pixels, it applies a certain random rotation to the
images in the same tasks. We create 20 different tasks, each corresponds to a
certain but different version of rotation from [0, 180] degree interval.
S-CIFAR100 splits the original CIFAR-100 dataset into 20 disjoint sets, each
of which, containing 5 classes, is considered as a separate task. The 5 classes in
each task is randomly chosen without replacement from the total 100 classes.
S-miniImageNet splits a subset of Imagenet dataset into 20 disjoint sets, each
of which, containing 5 classes, is considered as a separate task. The 5 classes in
each task is randomly chosen without replacement from the total 100 classes.

--- PAGE 8 ---
8 R. Hyder et al.
4.2 Training Details
Network. In the first set of experiments, we used a three layer (fully-connected)
multilayer perceptron (MLP) with 256-node hidden layers, similar to the network
in [7]. We flattened multi-dimensional input image to a 1D vector input. We used
ReLU activation for all the layers except the last one. We used Softmax for the
muticlass classification tasks. We used the same network for all the tasks with
necessary modifications for input and output sizes. Our approach can be used in
convolutional networks as well. We report the results using ResNet18 with our
approach on S-CIFAR100 and S-miniImageNet dataset in Table 6.
Factorization and rank selection. We used the matrix factorization defined
in (3) in all our experiments. We empirically selected the rank for the first
task,rk,1as 11 based on the experiments on a sample Rotated MNIST task
and kept the same value for all the experiments. We then performed rank-1
increment ( rk,t) for each additional task. We would like to point that AGEM
and Orthog Subspace use first 3 tasks for hyperparameter tuning. We did not
tune our hyperparameters on the test data, rather we choose the parameters
which provides better convergence during training. We increment the weight
matrices by rank-1 per task; therefore, learning rate and the number of epochs
are the only hyperparameters in our experiments.
Optimization. We used orthogonal initialization for the low-rank factors, as
described in [32]. We used all one initialization for the additional factors of the
selector matrices Sk,t,t. We used Adam optimization to update the factors. We
used the batch size of 128 for each task.
Performance metrics. We use accuracy and forgetting per task, which are
two commonly used metrics in the continual learning literature [6,7], to evaluate
the performance of the described methods. Let at,jbe the test accuracy of task
j < t after the model has finished learning task t∈ {1, ..., T}in a incremental
manner. The average accuracy Atafter the model has learned task tis defined as
At=1
tPt
j=1at,j.On the other hand, forgetting is the decrease in the accuracy of
a task after its training, and after one or several tasks are learned incrementally.
We define the average forgetting FtasFt=1
t−1Pt−1
j=1(aj,j−at,j).
In Figure 2, we show the evolution of average accuracy Atastincreases. We
also show the evolution of task-wise accuracy at,jin Figure 3, where ( t, j) pixel
intensity reflects at,j. We report the average accuracy AT, the average accuracy
after the model has learnt every tasks incrementally, in Table 1. We report the
forgetting FTafter the model has learnt all the tasks incrementally in Table 2.
Note that our method performs incremental task learning without forgetting.
4.3 Comparing Techniques
We compare our method against different state-of-the-art ITL methods. EWC
[15] is a regularization-based method that uses the Fisher Information ma-
trix to estimate posterior of previous tasks to preserve important parameters.
ICARL [27] is a memory-based method that uses exemplars and knowledge
distillation [13] to retain previous knowledge. AGEM [8] is a memory-based

--- PAGE 9 ---
Incremental Task Learning with Incremental Rank Updates 9
Table 1: Average test accuracy of ITL for P-MNIST, R-MNIST, S-CIFAR100, and
S-miniImageNet with three layer MLP. Standard deviation of test accuracy over five
runs is shown in parenthesis for some of the experiments.
∗Orthog subspace does not use replay buffer for MNIST variations.
MethodReplay
BufferP-MNIST R-MNIST S-CIFAR100 S-miniImageNet
EWC [15] No 67.9 (±0.68) 44.5 ( ±1.09) 52.7 ( ±0.81) 29.3 ( ±1.08)
ICARL [27] Yes 85.4 (±0.01) 51.2( ±2.41) 56.9( ±0.31) 39.9( ±0.27)
AGEM [8] Yes 73.9 (±0.52) 53.4 ( ±1.80) 51.3( ±1.54) 31.3( ±0.89)
HAT [33] No 90.1(±1.60) 89.1( ±2.51) 64.8 ( ±0.32) 47.0 ( ±0.88)
PackNet [19] No 90.0(±0.24) 88.4( ±0.37) 63.7( ±0.41) 45.1( ±1.05)
Orth sub [7] Yes∗86.6 (±0.79) 80.2 ( ±0.41) 57.8 ( ±1.03) 38.1 ( ±0.67)
Adam-NSCL[37] No – – 64.26 47.32
IBP-WF [22] No – – 53.22 40.52
Ours No 85.6 (±0.15) 91.1 (±0.12) 65.9 (±2.16) 54.7 (±2.87)
Parallel 2 (r=2) - 65.3 65.5 62.8 55.4
Parallel 4 (r=4) - 86.3 87.4 65.6 58.6
Parallel fullrank - 95.9 97.3 73.1 63.1
Multitask - 96.8 97.7 16.4 4.21
method built upon [17] that uses episodic memory to solve an constrained opti-
mization problem. ER-Ring [9] is another memory-based method that jointly
trains on new task data with that of the previous tasks. Orth. sub. [7] learn
tasks in different (low-rank) vector subspaces that are kept orthogonal to each
other in order to minimize interference. Other than the above mentioned ap-
proaches, we compared with masked based approaches which, like our approach,
also fall under dynamic architecture category. HAT [33] that incorporates task-
specific embeddings for attention masking. PackNet [19] that iteratively assigns
subsets of a single binary mask to each task. The mask-based approaches uti-
lize the redundancy of the network parameters to represent different tasks with
different masked versions of the same network weights. We also present com-
parisons with some recent methods: IBP-WF [22] and Adam-NSCL [37], in
terms of average accuracy for one experiment on two datasets.
In addition, we report results for two non-continual baseline methods: Paral-
lel learning andMultitask learning .Parallel learning trains independent
(smaller) low-rank networks of same size for each task. We report results for
three such networks. Parallel 2 uses rank-2 layers, Parallel 4 uses rank-4 lay-
ers, and Parallel full uses a full-rank MLP. Parallel 2 requires approximately
the same number of parameters as the rank-1 ITL network that we use in our
experiments; Parallel 4 provides higher network capacity, while requiring fewer
parameters than the full-rank network. We can treat the performance of the Par-
allel full approach as the upper limit that we can achieve using ITL methods.
Finally, Multitask learning has been used as a baseline in [7,8]. In multitask
learning, we have access to all data to optimize a single network.

--- PAGE 10 ---
10 R. Hyder et al.
Table 2: Average forgetting results corresponding to Table 1 for different datasets
using different approaches. We report the forgetting in percentage unit (%). We also
report the standard deviation over 5 experiments for some methods.
EWC AGEMOrthog
subspaceAdam-
NSCLParallel fullrank,
HAT, PackNet
Ours , IBP-WF
P-MNIST 25.8 (±0.70) 19.6 ( ±0.64) 4.49 ( ±0.93) - 0
R-MNIST 52.9 (±1.17) 44.2 ( ±1.85) 14.7 ( ±0.39) - 0
S-CIFAR100 6.96 (±0.80) 21.5 ( ±2.89) 6.30 ( ±0.38) 8.5 0
S-miniImageNet 17.3 (±1.81) 18.8 ( ±1.40) 9.98 ( ±0.31) 11.23 0
4.4 Results with Three-Layer MLP
Classification performance and comparison. We report classification re-
sults for P-MNIST, R-MNIST, S-CIFAR100, and S-miniImageNet tasks in Ta-
ble 1. We also show the results for the comparing techniques. We observe that
our method with rank-1 update perform better than all the comparing meth-
ods (EWC, ICARL, AGEM, HAT, PackNet, Orthog Subspace) on R-MNIST,
S-CIFAR100 and S-miniImageNet tasks using significantly fewer number of pa-
rameters. Our method performs close to Orthog Subspace on P-MNIST tasks.
Fig. 2: Average test accuracy for different datasets (Permuted MNIST, Rotated
MNIST, Split CIFAR100, Split miniImageNet) along different tasks using different
algorithms (AGEM,EWC, Orthog. Subspace, ICARL and our approach). We use three
layer MLP here. Parallel full-rank results corresponds to the case when we train every
task on separate full rank networks independently (serves as an upper limit for ITL
methods). We showed the average of 20 tasks.
We also observe that the proposed rank-1 update outperforms non-continual
Parallel 2 baseline that has similar number of parameters compared to our ap-

--- PAGE 11 ---
Incremental Task Learning with Incremental Rank Updates 11
Fig. 3: Evolution of task-wise test accuracy on P-MNIST (first row) and R-MNIST
(second row) datasets for EWC, Orthogonal Subspace, and Our approach. We can
observe from the decrease in the test accuracy that EWC and Orthogonal Subspace
forget the previous tasks as they learn new tasks. Our approach does not show any
forgetting as we learn new tasks.
proach. We perform similar to Parallel 4 baseline that uses nearly twice the
number of parameters as our approach. Parallel full acts as an upper limit with
the network structure of our choice as it trains independent full rank networks
for every task. Multitask learning is another non-continual baseline that uses
all the data from all the tasks simultaneously. Table 1 suggests that our ITL
method can learn complex tasks such as CIFAR100 and miniImageNet classi-
fication with a three layer MLP, whereas multitask learning (which is solving
100-class classification problem) fails with such a simple network. We also tested
Resnet18 network, which has significantly more parameters than the network
used in Table 1. The results for Resnet18 are presented in Table 6.
We present the task-wise test performance for some of the comparing ap-
proaches on P-MNIST, R-MNIST, S-CIFAR100 and S-miniImageNet datasets
in Figure 2. We observe that as we train new tasks, task-wise performance drops
for the comparing approaches, especially for P-MNIST and R-MNIST.
ICARL and AGEM require replay buffer (episodic memory) for each task.
Although Orthog Subspace did not use replay buffer for MNIST experiments,
it requires replay buffer in their algorithm and used it for S-CIFAR100 and
S-miniImageNet experiments. EWC does not require any replay buffer, but it
suffers from high forgetting as shown in Figure 3. Our proposed approach does
not require a replay buffer, and it outperforms other approaches in Table 1.
Accuracy vs forgetting. We report the average forgetting of different com-
paring approaches in Table 2. Our method, mask-based approaches (HAT and

--- PAGE 12 ---
12 R. Hyder et al.
Table 3: Number of parameters and buffer size in ITL methods with 3-layer MLP.
Ours IBP-WF EWC AGEM Ortho Sub Adam-NSCL Parallel fullrank
# params. 0.17M 0.23M 0.93M 1.76M 2.82M 0.88M 19.7M
buffer size 0 0 1.71M 7.90M 9.01M 0 0
Table 4: Number of parameters used by different zero-forgetting algorithms (HAT,
PackNet, and Ours) using 3-layer MLP.
Method P/R-MNIST S-CIFAR100 S-miniImageNet
HAT 0.33M 0.89M 5.51M
PackNet 0.26M 0.83M 5.50M
Ours 0.11M 0.17M 0.72M
PackNet) and parallel baselines have zero forgetting, whereas all other comparing
methods exhibit some level of forgetting. To better demonstrate the forgetting,
in Figure 3, we show the accuracy for the tasks along the entire training pro-
cedure. ithrow (top-bottom) of the diagram denotes the performance of itasks
on the test sets when we train the ithtask. As expected, we can observe that
the training performance for the previously learned tasks usually drops with the
gradual training of the subsequent tasks specially for the regularization based
approach, EWC. However, our algorithm maintains the same performance for the
past tasks as we do not change any previously learned factors. Even orthogonal
subspace approach observes such forgetting over some tasks.
Memory complexity. Our method increments the rank of each layer for each
task; therefore, we compare the total number of parameters in the incrementally
trained network and the Parallel baselines. Note that if the number of parameters
in two approaches is same, we can train one small network per task indepen-
dently. We report total number of parameters and replay buffer size for different
methods in Table 3. Since we used similar fully connected network structure for
all the tasks, we report results for Split CIFAR100 experiments. Although we
increase the rank for every task, the increment is small enough that even after
20 tasks our total parameter count remains smaller than all other methods.
We also report the number of parameters used by mask-based zero forgetting
algorithms (HAT and PackNet) to learn 20 different tasks on different datasets in
Table 4. We can observe that our approach outperforms HAT and PackNet for R-
MNIST, S-CIAR100 and S-miniImageNet with a significantly smaller number of
parameters. Even though all the approaches use the same network, our approach
uses rank-1 factors that require a significantly smaller number of parameters for
incremental learning of tasks. Note that P-MNIST and R-MNIST experiments
require the same number of parameters.
Effect of rank. In Table 5, we evaluate the effect of different rank selection
for different MNIST datasets using our ITL approach. We tested the initial
rank (rank for the first task) of 1, 6, and 11, keeping the rank increment to 1.
We observed that the accuracy increase as the initial rank increases, and we

--- PAGE 13 ---
Incremental Task Learning with Incremental Rank Updates 13
Table 5: Test accuracy for different rank choices of the proposed ITL approach and
multi-task baseline networks for P-MNIST and R-MNIST. Initial rank is rk,1and rank
increment/task is rk,t.
Setup 1 2 3 4 5
(rk,1, rk,t) (1,1) (6,1) (11,1) (11,2) (11,4)
P-MNIST 74.23 82.21 85.61 90.51 93.84
R-MNIST 81.57 89.39 91.09 92.76 94.12
# parameters 0.09M 0.1M 0.11M 0.14M 0.2M
achieve nearly 90% accuracy with initial rank of 11. We also tested different
values of rank increment per task and observe that the accuracy increases with
larger rank increment. Nevertheless, rank-1 increment provides us comparable
or better performance than the comparing techniques as shown in Table 1.
4.5 Results with ResNet18
The proposed low-rank increments approach can be generalized to other type
of networks and layers as well. For example, convolutional kernels have four-
dimensional weight tensors as opposed to the two-dimensional weight matrices
of fully connected layers. They are usually formulated as a tensor of output
and input channel ( Cout, Cin), and the two dimensions of the convolutional fil-
ters ( H, W ). We reshape the convolutional weight tensors into matrices of size
Cout×CinHW and perform similar low-rank updates per task as we described
for the MLP in the main paper. We report the results for S-CIFAR-100 and S-
miniImageNet datasets with Resnet18 architecture. For each convolutional lay-
ers, we reshaped and decomposed the convolution weight tensors into the same
low-rank factors described in (3) and performed low-rank updates per tasks.
We report the results in Table 6. For most of the comparing techniques, results
from [7] are reported since we use the same architecture and dataset. For missing
comparisons, we trained the models using same procedure as outlined in [7].
Instead of using a fixed value for rank at each layer as we did in the MLP
setup, we used rank size that is proportional to the size of Cout,iatithconvolu-
tional layer because the weights for different layers of ResNet18 are different in
size. We select initial rank = 0 .1Cout,ifor the first task and incremental rank =
0.02Cout,ifor the subsequent incremental tasks.
The results in Table 6 show that the performance of every method improves
with the convolutional ResNet18 structure over the 3-layer MLP. Nevertheless,
our method outperforms the comparing approaches for both datasets. Adam-
NSCL [37] gets better results on CIFAR100, but it requires 11.21M parameters
(compared to 1.33M parameters required by our method).
Effect of updating last few layers. We performed an experiment on S-
CIFAR-100 where we factorize last Llayers of the ResNet18 architecture keeping
the rest of the network fixed at trained weights on Task 1. Updating last L=

--- PAGE 14 ---
14 R. Hyder et al.
Table 6: Comparison of test accuracy and forgetting for split CIFAR-100 and split
miniImageNet datasets using ResNet18 architecture.
Method S-CIFAR-100 S-miniImageNet
Accuracy Forgetting Accuracy Forgetting
EWC [15] 43.2 (±2.77) 26 ( ±2) 34.8 (±2.34) 24 ( ±4)
ICARL [27] 46.4 (±1.21) 16 ( ±1) 44.2 24.64
AGEM [8] 60.34 ( ±2.05) 11.0 ( ±2.88) 42.3 (±1.42) 17 ( ±1))
ER-Ring [9] 59.6 (±11.9) 0.14 ( ±1) 49.8 (±2.92) 12 ( ±1)
Ortho sub [7] 63.42 ( ±1.82) 8.37 ( ±0.71) 51.4 (±1.44) 10 ( ±1)
Adam-NSCL [37] 74.31 9.47 57.92 13.42
IBP-WF [22] 68.25 0 55.84 0
Ours 68.46 ( ±2.52) 0 59.26 (±1.15) 0
Parallel full-rank 92.7 0 94.5 0
Multitask learning 70.2 0 65.1 0
{1,2,3,4,5}layers provide average accuracy of {34.38,34.99,53.41,57.08,65.03},
respectively. This result suggests that updating last few layers may suffice since
the initial layers merely work as a feature extractor.
5 Conclusion
We proposed a new incremental task learning method in which we update the
network weights using low rank increments as we learn new tasks. Network layers
are represented as a linear combination of low-rank factors. To update the net-
work for a new task, we freeze the factors learned for previous tasks, add a new
low-rank (or rank-1) factor, and combine that with the previous factors using
a learned combination. The proposed method offered considerable improvement
in performance compared to the state-of-the-art methods for ITL in image clas-
sification tasks. In addition, the proposed low-rank ITL circumvents the use of
memory buffer or large memory overhead while achieving zero forgetting.
The need for task ID knowledge is a general limitation of our and other ITL
methods. Such methods can be useful for incremental multitask learning where
task ID is available during inference but training data is only available in a short
window. Extending this method to class incremental learning (which does not
require task ID) is an important problem for future work.
Acknowledgments. This material is based upon work supported in part by
by Air Force Office of Scientific Research (AFOSR) awards FA9550-21-1-0330,
FA9550-20-1-0039, Office of Naval Research (ONR) award N00014-19-1-2264,
and National Science Foundation (NSF) award CCF-2046293. Approved for Pub-
lic Release by AFRL; Distribution Unlimited: Case Number AFRL-2021-4063
Corresponding author: M. Salman Asif (sasif@ucr.edu)

--- PAGE 15 ---
Incremental Task Learning with Incremental Rank Updates 15
References
1. Abati, D., Tomczak, J., Blankevoort, T., Calderara, S., Cucchiara, R., Bejnordi,
B.E.: Conditional channel gated networks for task-aware continual learning. In:
Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recog-
nition. pp. 3930–3939 (2020)
2. Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., Tuytelaars, T.: Memory
aware synapses: Learning what (not) to forget. In: Proceedings of the European
Conference on Computer Vision (ECCV). pp. 139–154 (2018)
3. Aljundi, R., Chakravarty, P., Tuytelaars, T.: Expert gate: Lifelong learning with a
network of experts. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. pp. 3366–3375 (2017)
4. Aljundi, R., Lin, M., Goujaud, B., Bengio, Y.: Gradient based sample selection for
online continual learning. Advances in Neural Information Processing Systems 32,
11816–11825 (2019)
5. Buzzega, P., Boschini, M., Porrello, A., Abati, D., Calderara, S.: Dark experience
for general continual learning: a strong, simple baseline. Advances in neural infor-
mation processing systems 33, 15920–15930 (2020)
6. Chaudhry, A., Dokania, P.K., Ajanthan, T., Torr, P.H.: Riemannian walk for in-
cremental learning: Understanding forgetting and intransigence. In: Proceedings of
the European Conference on Computer Vision (ECCV). pp. 532–547 (2018)
7. Chaudhry, A., Khan, N., Dokania, P., Torr, P.: Continual learning in low-rank or-
thogonal subspaces. Advances in Neural Information Processing Systems 33(2020)
8. Chaudhry, A., Marc’Aurelio, R., Rohrbach, M., Elhoseiny, M.: Efficient lifelong
learning with a-gem. In: International Conference on Learning Representations,
ICLR (2019)
9. Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P.K., Torr,
P.H., Ranzato, M.: On tiny episodic memories in continual learning. arXiv preprint
arXiv:1902.10486 (2019)
10. Delange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh,
G., Tuytelaars, T.: A continual learning survey: Defying forgetting in classification
tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021)
11. Deng, D., Chen, G., Hao, J., Wang, Q., Heng, P.A.: Flattening sharpness for dy-
namic gradient projection memory benefits continual learning. Advances in Neural
Information Processing Systems 34(2021)
12. Farajtabar, M., Azizan, N., Mott, A., Li, A.: Orthogonal gradient descent for con-
tinual learning. In: International Conference on Artificial Intelligence and Statis-
tics. pp. 3762–3773. PMLR (2020)
13. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.
NeurIPS (2014)
14. Hurtado, J., Raymond-Saez, A., Soto, A.: Optimizing reusable knowledge for con-
tinual learning via metalearning. Advances in Neural Information Processing Sys-
tems34(2021)
15. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,
A.A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al.: Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of
sciences 114(13), 3521–3526 (2017)
16. Li, Z., Hoiem, D.: Learning without forgetting. IEEE transactions on pattern anal-
ysis and machine intelligence 40(12), 2935–2947 (2017)

--- PAGE 16 ---
16 R. Hyder et al.
17. Lopez-Paz, D., Ranzato, M.: Gradient episodic memory for continual learning.
Advances in neural information processing systems 30, 6467–6476 (2017)
18. Mallya, A., Davis, D., Lazebnik, S.: Piggyback: Adapting a single network to multi-
ple tasks by learning to mask weights. In: Proceedings of the European Conference
on Computer Vision (ECCV). pp. 72–88 (2018)
19. Mallya, A., Lazebnik, S.: Packnet: Adding multiple tasks to a single network by
iterative pruning. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. pp. 7765–7773 (2018)
20. Masse, N.Y., Grant, G.D., Freedman, D.J.: Alleviating catastrophic forgetting us-
ing context-dependent gating and synaptic stabilization. Proceedings of the Na-
tional Academy of Sciences 115(44), E10467–E10475 (2018)
21. McCloskey, M., Cohen, N.J.: Catastrophic interference in connectionist networks:
The sequential learning problem. In: Psychology of learning and motivation, vol. 24,
pp. 109–165. Elsevier (1989)
22. Mehta, N., Liang, K., Verma, V.K., Carin, L.: Continual learning using a bayesian
nonparametric dictionary of weight factors. In: International Conference on Arti-
ficial Intelligence and Statistics. pp. 100–108. PMLR (2021)
23. Nguyen, C.V., Li, Y., Bui, T.D., Turner, R.E.: Variational continual learning. In:
International Conference on Learning Representations (2018)
24. von Oswald, J., Henning, C., Sacramento, J., Grewe, B.F.: Continual learning with
hypernetworks. In: International Conference on Learning Representations (2019)
25. Parisi, G.I., Kemker, R., Part, J.L., Kanan, C., Wermter, S.: Continual lifelong
learning with neural networks: A review. Neural Networks 113, 54–71 (2019)
26. Ratcliff, R.: Connectionist models of recognition memory: constraints imposed by
learning and forgetting functions. Psychological review 97(2), 285 (1990)
27. Rebuffi, S.A., Kolesnikov, A., Sperl, G., Lampert, C.H.: icarl: Incremental classifier
and representation learning. In: Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition. pp. 2001–2010 (2017)
28. Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y., Tesauro, G.: Learning
to learn without forgetting by maximizing transfer and minimizing interference.
In: International Conference on Learning Representations (2019)
29. Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T., Wayne, G.: Experience replay
for continual learning. Advances in Neural Information Processing Systems 32,
350–360 (2019)
30. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
Kavukcuoglu, K., Pascanu, R., Hadsell, R.: Progressive neural networks. arXiv
preprint arXiv:1606.04671 (2016)
31. Saha, G., Garg, I., Roy, K.: Gradient projection memory for continual learning. In:
International Conference on Learning Representations (2021)
32. Saxe, A.M., McClelland, J.L., Ganguli, S.: Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120
(2013)
33. Serra, J., Suris, D., Miron, M., Karatzoglou, A.: Overcoming catastrophic forget-
ting with hard attention to the task. In: International Conference on Machine
Learning. pp. 4548–4557. PMLR (2018)
34. Silver, D.L., Mercer, R.E.: The task rehearsal method of life-long learning: Over-
coming impoverished data. In: Conference of the Canadian Society for Computa-
tional Studies of Intelligence. pp. 90–101. Springer (2002)
35. Tang, S., Chen, D., Zhu, J., Yu, S., Ouyang, W.: Layerwise optimization by gradient
decomposition for continual learning. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. pp. 9634–9643 (2021)

--- PAGE 17 ---
Incremental Task Learning with Incremental Rank Updates 17
36. Veniat, T., Denoyer, L., Ranzato, M.: Efficient continual learning with modular
networks and task-driven priors. In: International Conference on Learning Repre-
sentations (2021)
37. Wang, S., Li, X., Sun, J., Xu, Z.: Training networks in null space of feature co-
variance for continual learning. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 184–193 (2021)
38. Wen, Y., Tran, D., Ba, J.: Batchensemble: an alternative approach to efficient
ensemble and lifelong learning. In: International Conference on Learning Repre-
sentations (2020)
39. Wortsman, M., Ramanujan, V., Liu, R., Kembhavi, A., Rastegari, M., Yosinski, J.,
Farhadi, A.: Supermasks in superposition. Advances in Neural Information Pro-
cessing Systems 33(2020)
40. Yin, H., Li, P., et al.: Mitigating forgetting in online continual learning with neuron
calibration. Advances in Neural Information Processing Systems 34(2021)
41. Yoon, J., Yang, E., Lee, J., Hwang, S.J.: Lifelong learning with dynamically
expandable networks. In: International Conference on Learning Representations
(2018)
