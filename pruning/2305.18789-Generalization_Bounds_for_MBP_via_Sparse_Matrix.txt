# 2305.18789.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2305.18789.pdf
# File size: 556377 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Generalization Bounds for MBP via Sparse Matrix
Sketching
Etash Kumar Guha
College of Computing
Georgia Institute of Technology
Atlanta, GA, 30332
etash@gatech.eduPrasanjit Dubey
School of Industrial and Systems Engineering
Georgia Institute of Technology
Atlanta, GA, 30332
pdubey31@gatech.edu
Xiaoming Huo
School of Industrial and Systems Engineering
Georgia Institute of Technology
Atlanta, GA, 30332
huo@gatech.edu
Abstract
In this paper, we derive a novel bound on the generalization error of overparameter-
ized neural networks when they have undergone Magnitude-Based Pruning (MBP).
Our work builds on the bounds in Arora et al. [2018], where the error depends on
one, the approximation induced by pruning, and two, the number of parameters in
the pruned model, and improves upon standard norm-based generalization bounds.
The pruned estimates obtained using MBP are close to the unpruned functions with
high probability, which improves the first criteria. Using Sparse Matrix Sketching,
the space of the pruned matrices can be efficiently represented in the space of dense
matrices with much smaller dimensions, thereby improving the second criterion.
This leads to stronger generalization bound than many state-of-the-art methods,
thereby breaking new ground in the algorithm development for pruning and bound-
ing generalization error of overparameterized models. Beyond this, we extend our
results to obtain generalization bounds for Iterative Pruning [Frankle and Carbin,
2018]. We empirically verify the success of this new method on ReLU-activated
Feed Forward Networks on the MNIST and CIFAR10 datasets.
1 Introduction
Overparameterized neural networks are often used in practice as they achieve remarkable generaliza-
tion errors [Goodfellow et al., 2016]. However, their immense size makes them slow and expensive to
run during inference [Han et al., 2015]. Machine learning (ML) practitioners often employ Magnitude-
Based pruning (MBP) to amend this computational complexity. After training large neural networks,
the parameters or matrix elements within the model with the smallest magnitude are set to 0. This
reduces the memory requirements of the model and the inference time greatly. However, MBP also
has been shown to induce little generalization error and, in fact, often reduces the generalization error
compared to the original model [Han et al., 2015, Li et al., 2016, Cheng et al., 2017]. Examining
where and why strong generalization happens can help build learning algorithms and models that
generalize better [Foret et al., 2020, Le et al., 2018]. However, theoretical analyses of why MBP
achieves strong generalization errors still need to be made. Providing such analyses is challenging
for several reasons. First, removing the smallest weights is a relatively unstudied operation in linear
Preprint. Under review.arXiv:2305.18789v2  [cs.LG]  24 Jun 2023

--- PAGE 2 ---
algebra, and only few tools are available to analyze the properties of pruned matrices. Second, it is
difficult to characterize the distribution of weights after training and pruning.
However, Arora et al. [2018] provided a tool that more comprehensively analyzes the generalization
error of models with fewer parameters effectively. Specifically, they upper bounded the generalization
error of a large neural network when compressed. We can directly apply this result to pruned models
as pruned models intrinsically have fewer parameters. Their bound is split into two parts: the amount
of error introduced by the model via the compression and the number of different parameters in the
compressed model. We use this primary tool to show that the outputs from MBP generalize well
since they don’t introduce much error and have fewer parameters. We prove both of these phenomena
with a few simple assumptions. Namely, given some justifiable assumptions on the distribution of
the trained weight parameters, we develop an upper bound of the amount of error the pruned neural
network suffers with high probability. We also demonstrate that the number of parameters needed
to fully express the space of pruned models is relatively small. Specifically, we show that our set
of pruned matrices can be efficiently represented in the space of dense matrices of much smaller
dimension via the Sparse Matrix Sketching.
Combining the two parts of the bound, we get a novel generalization error bound that is competitive
with state-of-the-art generalization bounds. Moreover, to our knowledge, this is the firstgeneralization
bound for MBP that uses Compression Bounds. We empirically verify the success of our novel
approach on the MNIST and CIFAR10 datasets where our bound is several orders of magnitude
better (at least 107times better on CIFAR10, refer Figure 1b) than well-known standard bounds of
Neyshabur et al. [2015],Bartlett et al. [2017], and Neyshabur et al. [2017]. We extend our framework
to show that using Iterative Magnitude Pruning (IMP) or Lottery Tickets [Frankle and Carbin, 2018]
also generalizes. Namely, Malach et al. [2020] shows that IMP produces results with small error and
few nonzero parameters. We use matrix sketching to efficiently count the number of parameters in a
usable way for our generalization bound. This results in a strong generalization bound that, to our
knowledge, has only been analyzed empirically [Bartoldson et al., 2020, Jin et al., 2022].
Contributions We formally list our contributions here. We first prove the error induced by MBP
is small relative to the original model. Moreover, we demonstrate that our MBP achieves sufficient
sparsity, i.e., relatively few nonzero parameters are left after pruning. To tighten our generalization
bounds, we show that the pruned matrices from MBP can be sketched into smaller dense matrices.
We combine the results above to prove that the generalization error of the pruned models is small.
We extend the proof framework above to establish a generalization error bound for IMP. To our
knowledge, these are the first results studying the generalization of pruned models through either
MBP or IMP. We empirically verify that our generalization bounds improve upon many standard
generalization error bounds for MLPs on CIFAR10 and MNIST datasets.
2 Related Works
2.1 Norm-based Generalization Bounds
In recent years, many works have studied how to use parameter counting and weight norms to
form tighter generalization bounds as an evolution from classical Rademacher Complexity and VC
dimension. Galanti et al. [2023] uses Rademacher Complexity to develop a generalization bound for
naturally sparse networks such as those from sparse regularization. Neyshabur et al. [2015] studies a
general class of norm-based bounds for neural networks. Moreover, Bartlett and Mendelson [2002]
used Rademacher and Gaussian Complexity to form generalization bounds. Long and Sedghi [2020]
gives generalization error bounds for Convolutional Neural Networks (CNNs) using the distance from
initial weights and the number of parameters that are independent of the dimension of the feature
map and the number of pixels in the input. Daniely and Granot [2019] uses approximate description
length as an intuitive form for parameter counting.
2.2 Pruning Techniques
While MBP is one of the most common forms of pruning in practice, other forms exist. Collins and
Kohli [2014] induce sparsity into their CNNs by using ℓ1regularization in their training. Molchanov
et al. [2017] develops iterative pruning frameworks for compressing deep CNNs using greedy criteria-
based pruning based on the Taylor expansion and fine-tuning by backpropagation. Liu et al. [2017]
2

--- PAGE 3 ---
use Filter Sparsity alongside Network Slimming to enable speedups in their CNNs. Ullrich et al.
[2017] coins soft-weight sharing as a methodology of inducing sparsity into their bounds. Moreover,
Hooker et al. [2019] empirically studied which samples of data-pruned models will significantly
differ from the original models. Many works use less common pruning methods such as coresets
[Mussay et al., 2019] or the phenomenon of IMP [Frankle and Carbin, 2018, Malach et al., 2020].
3 Preliminary
3.1 Notation
We consider a standard multiclass classification problem where for a given sample x, we predict the
class y, which is an integer between 1 and k. We assume that our model uses a learning algorithm
that generates a set of Lmatrices M={A1, . . . ,AL}where Ai∈Rdi
1×di
2. Here, di
1, di
2are the
dimensions of the ith layer. Therefore, given some input x, the output of our model denoted as M(x)
is defined as
M(x) =ALϕL−1(AL−1ϕL−2(. . .A2ϕ1(A1x))),
thereby mapping xtoM(x)∈Rk. Here, ϕiis the activation function for the ith layer of LiLipschitz-
Smoothness. When not vague, we will use the notation x0=xandx1=A1xandx2=A2ϕ1(A1x)
and so on. Given any data distribution Dthe expected margin loss for some margin γ >0is defined
as
Rγ(M) =P(x,y)∼D
M(x)[y]≤γ+ max
j̸=yM(x)[j]
.
The population risk R(M)is obtained as a special case of Rγ(M)by setting γ= 0. The empirical
margin loss for a classifier is defined as
ˆRγ(M) =1
|S|X
(x,y)∈SI
M(x)[y]−max
j̸=y(M(x)[j])≥γ
,
for some margin γ >0where Sis the dataset provided (when γ= 0, this becomes the classification
loss). Intuitively, ˆRγ(M)denotes the number of elements the classifier Mpredicts the correct ywith
a margin greater than or equal to γ. Moreover, we define the size of Sto be|S|=n. We will denote
ˆM={ˆA1, . . . , ˆAL}as the compressed model obtained after pruning M. The generalization error
of the pruned model is then R0(ˆM). Moreover, we will define the difference matrix at layer las
∆l=Al−ˆAl. Now that we have formally defined our notation, we will briefly overview the main
generalization tool throughout this paper.
3.2 Compression Bounds
As compression bounds are one of the main theoretical tools used throughout this paper, we will
briefly overview the bounds presented in Arora et al. [2018]. Given that we feed a model finto a
compression algorithm, the set of possible outputs is a set of models GA,swhereAis a set of possible
parameter configurations and sis some starter information given to the compression algorithm. We
will call gAas one such model corresponding to parameter configuration A∈ A. Moreover, if there
exists a compressed model gA∈GA,ssuch that for any input in a dataset S, the outputs from gA
andfdiffer by at most γ, we say fis(γ,S)compressible. Formally, we make this explicit in the
following definition.
Definition 3.1. Iffis a classifier and GA,s={gA|A∈ A} be a class of classifiers with a set of
trainable parameter configurations Aand fixed string s. We say that fis(γ,S)-compressible via
GA,sif there exists A∈ A such that for any x∈ S, we have for all y,|f(x)[y]−gA(x)[y]| ≤γ.
We now introduce our compression bound. The generalization error of the compressed models in
expectation is, at most, the empirical generalization error of the original model if the original model
has margin γ. Using standard concentration inequalities, we apply this bound over all possible pruned
model outcomes. The resulting generalization bound depends on both the margin and the number of
parameters in the pruned model, as in the following theorem.
3

--- PAGE 4 ---
Theorem 3.1. [Arora et al., 2018] Suppose GA,s={gA,s|A∈ A} where Ais a set of qparameters
each of which can have at most rdiscrete values and sis a helper string. Let Sbe a training set with
nsamples. If the trained classifier fis(γ,S)-compressible via GA,s, with helper string s, then there
exists A∈ A with high probability over the training set ,
R0(gA)≤ˆRγ(f) +O r
qlogr
n!
.
It is to be noted that the above theorem provides a generalization bound for the compressed classifier
gA, not for the trained classifier f. Therefore, the two parts of forming tighter generalization bounds
for a given compression algorithm involve bounding the error introduced by the compression, the γ
inˆRγ(f), and the number of parameters qafter compression. We demonstrate that we can achieve
both with traditional MBP.
3.3 Preliminary Assumptions
Analyzing the effects of pruning is difficult without first understanding from which distribution the
weights of a trained model lie. This is a complex and open question in general. However, Han et al.
[2015] made the empirical observation that weights often lie in a zero-mean Gaussian distribution,
such as in Figure 7 in Han et al. [2015]. We will thus assume this to be true, that the distribution
of weights follows a normal distribution with 0mean and variance Ψ. Here, we state the main
preliminary assumptions that we will use later.
Assumption 3.1. For any l∈[L], i, j∈[dl
1]×[dl
2],Al
i,j∼ N(0,Ψ).
This assumption states that each atom within a matrix of the learned model obeys roughly a Gaussian
distribution centered at 0with variance Ψ. While a strong assumption, this is common. In fact, Qian
and Klabjan [2021] assumes that the weights follow a uniform distribution to analyze the weights of
pruned models. We assume a Gaussian distribution since this is more reasonable than the uniform
distribution assumption. We can now present the MBP algorithm we will analyze throughout this
paper.
4 Magnitude-Based Pruning Algorithms
While many versions of MBP algorithms exist, they are all based on the general framework of
removing weights of small magnitude to reduce the number of parameters while ensuring the pruned
model does not differ vastly from the original. We wish to choose a pruning algorithm based
on this framework often used by practitioners while at the same time being easy to analyze. We
develop our algorithm to mimic the random MBP seen often in works like Han et al. [2015], Qian
and Klabjan [2021]. While the term inside the Bernoulli random variable used as an indicator for
pruning is slightly different as compared to previous literature, this is a small change that allows
us to move away from the uniform distribution assumption from Qian and Klabjan [2021] to a
more favorable Gaussian assumption. We formally present our algorithm in Algorithm 1 below.
Algorithm 1: MBP
Data: {A1, . . . ,AL}, d
Result: {ˆA1, . . . ,ˆAL}
forl∈[L]do
fori, j∈[dl
1]×[dl
2]andi̸=jdo
X:=Bernoulli
exp
−[Al
i,j]2
dΨ
ˆAl
i,j:= 0ifX= 1elseAl
i,j
end
endRemark 4.1. We do not prune the diagonal
elements in Algorithm 1. While not standard,
this enables the use of Matrix Sketching later on
for better generalization bounds. However, in
Dasarathy et al. [2013], they note the necessity
for the diagonal elements being nonzero is for
ease of presentation of the proof, and Matrix
Sketching should still be possible with pruning
the diagonal elements.
The atom’s probability of being compressed is
relatively small for larger atoms. The probability
of getting compressed is larger for smaller atoms
closer to 0. Here, dis a hyperparameter helpful for adjusting the strength of our compression. Using
this compression algorithm, the pruned model will likely maintain the connections between the larger
4

--- PAGE 5 ---
atoms while removing many smaller parameters. To use the generalization bounds from Section 3.2,
we need to show that Algorithm 1 creates a pruned model ˆMthat produces outputs similar to the
original model M. We also need to show that the number of nonzero parameters in the pruned models
is small. We prove this in the sections below.
4.1 Error Proof
We begin by bounding the difference between the outputs of corresponding layers in the pruned and
original models to prove that the expected difference between the pruned and original models is small.
The normality assumption from Assumption 3.1 makes this much more tractable to compute. Indeed,
each atom of the difference matrix ∆l=ˆAl−Alis an independent and identical random variable.
Bounding the ℓ2norm of such a matrix relies only on the rich literature studying the norms of random
matrices. In fact, from Latala [2005], we only need a bounded second and fourth moment of the
distribution of each atom. To utilize this bound, we only need to demonstrate that the difference
matrix ∆land the pruned model obtained using our compression scheme Algorithm 1 have atoms
whose moments are bounded and have zero-mean. Given the compression algorithm chosen and the
distribution of weights after training using Assumption 3.1, the ∆lmatrix does satisfy such properties.
We demonstrate them in the following lemma.
Lemma 4.1. The Expected Value of any entry ∆l
ijof the matrix ∆l=ˆAl−Alis given by E(∆l
ij) = 0
for any i, j∈[dl
1]×[dl
2], l∈[L]. Thus, E(∆l) =0is a matrix full of 0’s. Furthermore, we have
thatE((∆l
ij)2) =d3
2Ψ
(d+2)3
2. Moreover, the fourth moment of any entry (∆l
ij)4of∆lis given by
E((∆l
ij)4) =3d5
2Ψ2
(d+2)5
2.
Given these properties of the ∆lmatrix, we can use simple concentration inequalities to bound the
error accumulated at any layer between the pruned and original models. If we simulate some sample
input xgoing through our pruned model, we can show that the error accumulated through the entire
model is bounded via induction. We formally present such a lemma here.
Lemma 4.2. For any given layer l∈[L], we have with probability at least 1−1
ϵl
∥(ˆAl−Al)∥2≤ϵlΓlwhere Γl=C
 s
d3
2Ψ
(d+ 2)3
2!q
dl
1+q
dl
2
+ 
3dl
1dl
2d5
2Ψ2
(d+ 2)5
2!1
4
.
Here, ˆAlis generated by Algorithm 1 and Cis a universal positive constant.
We now have a formal understanding of how pruning a given layer in the original model affects the
outcome of that layer. We can now present our error bound for our entire sparse network.
Lemma 4.3. The difference between outputs of the pruned model and the original model on any
input xis bounded by, with probability at least 1−PL
lϵ−1
l,
∥ˆxL−xL∥ ≤ed0
1 LY
l=1Ll∥Al∥2!LX
l=1ϵlΓl
∥Al∥2.
This bound states that the error accumulated by the pruned model obtained using Algorithm 1 depends
only on the dimension of the model, the Lipschitz constant of the activation functions, the variance of
our entries, and the error of the compression. Such a bound is intuitive as the error is accumulated
iteratively throughout the layers. We provide a brief proof sketch here.
Proof Sketch. By the Perturbation Bound from Neyshabur et al. [2017], we can bound how much
error accumulates through the model using the norm of the difference matrix from Lemma 4.2.
We can form tighter bounds by considering what the expected maximum of (ˆAl−Al)xis with high
probability. If dl
2< dl
1, we observe that the matrix ˆAl−Alhas at most dl
2nonzero singular values.
For more details, please see Appendix C.
5

--- PAGE 6 ---
However, more than this error bound is needed to prove strong generalization bounds. We require the
number of possible models after training and compression to be finite to use compression bounds.
Therefore, we need to apply discretization to our compressed model to ensure that the number of
models is finite. This, however, is relatively simple given the theoretical background already provided.
4.2 Discretization
We now show that the prediction error between a discretization of the pruned model and the original
model is also bounded. Our discretization method is simply rounding each value in layer lto the
nearest multiple of ρl. We will call the discretized pruned model ˜Mwhere the lth layer will be
denoted as ˜Al. We provide the following lemma bounding the norm of the difference of the layers
between the pruned and the discretized model. Using this intuition, we can prove that the error
induced by the discretization is small.
Lemma 4.4. The norm of the difference between the pruned layer and the discretized layer is
upper-bounded as ∥˜Al−ˆAl∥2≤ρlJlwhere Jlis the number of nonzero parameters in ˆAl(Jlis
used for brevity here and will be analyzed later). With probability at least 1−PL
l=1ϵ−1
l, given
that the parameter ρlfor each layer is chosen such that ρl≤1
L∥Al∥2−ϵlΓl
Jl, we have that the error
induced by both discretization and the pruning is bounded by
∥xL−˜xL∥2≤ed0
1 LY
l=1Ll∥Al∥2!LX
l=1ϵlΓl+ρlJl
∥Al∥2.
Now, we have a sufficient error bound on our MBP algorithm. Thus, as the next step, we focus on
bounding the number of parameters our compressed model will have. To do this, we introduce our
next significant theoretical tool: Matrix Sketching .
5 Sketching Sparse Matrices
As seen in Theorem 3.1, the generalization error depends strongly on the number of parameters.
We try to count the number of possible parameterizations of the pruned model ˆMachievable by
combining the learning algorithm and the compression algorithm. In the appendix, we discuss one
naive approach by counting the number of possible sparse matrices generated by the combination
of a learning algorithm and Algorithm 1. This achieves a less-than-desirable generalization bound,
forming motivation for Matrix Sketching. We now introduce the preliminaries and motivate the need
for matrix sketching.
5.1 Preliminary on Matrix Sketching
Here we introduce the preliminary concepts of matrix sketching. Namely, we can represent a sparse
matrix X∈Rp1×p2asY∈Rm×mwhere p1, p2≥m. The idea of matrix sketching is to create an
embedding for this matrix as Y=AXB⊤. Here, the matrices A∈ {0,1}m×p1,B∈ {0,1}m×p2are
chosen before the sketching is to be done. To recover the original matrix, we solve the minimization
problem
min
˜X∈Rp1×p2∥˜X∥1s.t.Y=A˜XB⊤. (1)
If the problem from Equation (1) enjoys a unique minimum and that unique minimum is the true X,
we can say that this sketching scheme is lossless. In such a case, all the information in Xis encoded
inY. Given such a mapping, we can use this one-to-one mapping between YandXto count the
number of parametrizations of Xusing Y, which is of a smaller dimension. We use properties from
this literature to help develop and prove the improved generalization error bounds.
We claim with matrix sketching that we can represent the space of large sparse matrices of dimension
pwith the set of small dense matrices of dimension√jplogpwhere jis the maximum number of
nonzero elements in any row or column. Counting the number of parameters in small dense matrices
is more efficient in terms of parameters than counting the number of large sparse matrices, thus
providing a way of evasion of the combinatorial explosion. We formalize this in the following section.
6

--- PAGE 7 ---
5.2 Sparse Case
To apply the matrix sketching literature to our sparse matrices, we need to prove several properties of
the matrices obtained using our compression scheme Algorithm 1. We introduce one such structure
called jr, jc-distributed-sparsity , which ensures sketching can be applied to matrices. Intuitively,
such a property ensures that any row or column of our sparse matrix does not contain too many
nonzero elements. We formally define such intuition here.
Definition 5.1. A matrix is jr, jc-distributed sparse if at most jrelements in any column are nonzero,
jcelements in any row are nonzero, and the diagonal elements are all nonzero.
The other main knowledge is how to form A, B for sparse matrix sketching. If the reader is interested,
we discuss how to form AandBalongside some intuition behind matrix sketching in Appendix E.1.
5.3 Bounds for Sparse Matrix Sketching
From Dasarathy et al. [2013], it can be proved that sketching the set of jr, jc-distributed sparse
matrices requires only small m. Given a choice of mand probability term δ, one can show that the
solution to Equation (1) matches the uncompressed value with high probability. This is mainly shown
by first demonstrating that a solution exists and that the best solution to A−1Y B−1=˜Xis the only
solution that minimizes the ℓ1norm with high probability.
Theorem 5.1. (From Theorem 1 of Dasarathy et al. [2013]) Let p= max( dl
1, dl
2). Suppose that
A∈ {0,1}m×dl
1,B∈ {0,1}m×dl
2are drawn independently and uniformly from the δ-random
bipartite ensemble. Then, as long as m=O(p
max( jcdl
1, jrdl
2) log( p))andδ=O(log(p)), there
exists a c≥0such that for any given jr, jc-distributed sparse matrix X, sketches AXB into ˜X
results in a unique sketch for each X. This statement holds with probability 1−p−c.
Remark 5.1. The theorem statement for Theorem 5.1 states that c≥0. However, in the proof, they
demonstrate the stronger claim that c≥2. Therefore, the probability that Theorem 5.1 holds is at
least1−p−2. Aspgrows, the probability that this theorem holds approaches 1.
5.4 Generalization Error from Sketching
To use the above theoretical tools of matrix sketching, we must show that outputs from our com-
pression algorithm Algorithm 1 satisfy the definitions of jr, jc-distributed-sparsity. Such a claim is
intuitive and similar properties have been shown for random matrices following different distributions.
Given that our trained matrices satisfy the Gaussian distribution, one row or column is unlikely to
contain many nonzero elements. Here, we prove in the following lemma that the pruned model using
Algorithm 1 satisfies the condition of distributed sparsity using Assumption 3.1.
Lemma 5.1. With probability at least 1−1
λl−(dl
1)−1
3−(dl
2)−1
3, we have that the outputs from
Algorithm 1 are jr, jc-sparsely distributed where max( jr, jc)≤3λlmax( dl
1, dl
2)χandλl∈R.
Here, χ=√d+2−√
d√d+2.
Given the above quantification of the space of sparse matrices and the bound of the error of our
model, we can apply the compression bound from Arora et al. [2018]. Such a compression bound
intuitively depends mainly on these two values.
Theorem 5.2. For every matrix ˆAl, define jlto be the max( jr, jc)where jrandjcare the distribution-
sparsity coefficients for ˆAl. Moreover, for every matrix ˆAl, define pl= max( dl
1, dl
2). Then we have
that
R0(gA)≤ˆRγ(f) +O
sP
l3λlχdl
2dl
1log2(pl) log(1
ρl)
n
.
This holds when dis chosen such that γ≥ed0
1Qd
l=1Ll∥Al∥2PL
l=1ϵlΓl+ρlJl
∥Al∥2where Jl≤
O 
χdl
2dl
1
. This claim holds with probability at least 1−hPL
l=1λ−1
l+ϵ−1
l+p−c
li
.
7

--- PAGE 8 ---
Here, χdepends on our hyperparameter d. Indeed, this generalization bound vastly improves on
a trivial application of compression bounds as in Lemma D.1. Quite notably, this removes the
combinatorial nature of the naive bound in Lemma D.1 for a small price ofq
log2(pl)
n.
6 Generalization of Lottery Tickets
Such a generalization error proof framework for pruning applies to more than just Magnitude-based
pruning. An exciting pruning approach is simply creating a very large model Gsuch that some smaller
target model Mis hidden inside Gand can be found by pruning. This lottery ticket formulation
for pruning has seen many empirical benefits. Formally, we will call our lottery ticket within Ga
weight-subnetwork eGofG. ThiseGis a pruned version of the original G. In fact, Malach et al. [2020]
shows that for a sufficiently large G, there exists with high probability a pruning ˜Gsuch that ˜Gand
the target function Mdiffer by at most ϵ. Moreover, this ˜Gwill have approximately the same number
of nonzero parameters as the original target network M. This is formally presented in Theorem 6.1.
Theorem 6.1. Fix some ϵ, δ∈(0,1). LetMbe some target network of depth Lsuch that for
every i∈[L]we have ∥Ai∥2≤1,Ai
max≤1√
d1,i. Furthermore, let nMbe the maximum
hidden dimension of M. LetGbe a network where each of the hidden dimensions is upper bounded
bypoly 
d1,0, nM, L,1
ϵ,log1
δ:=DGand depth 2L, where we initialize Aifrom the uniform
distribution U([−1,1]). Then, w.p at least 1−δthere exists a weight-subnetwork eGofGsuch that:
sup
x∈S|eG(x)−M(x)| ≤ϵ.
Furthermore, the number of active (nonzero) weights in eGisO 
d0,1DG+D2
GL
.
Theorem 6.2. With probability at least 1−δ−LD−c
Ghave the generalization error of
R0(˜G)≤ˆRϵ+ϵρ(˜G) +O
vuut[nMd0,1log(DG)2+Ln2
Mlog(DG)2] log
1
ρ
n
.
Here, ϵρis the small error introduced by discretization.
Here, we have a generalization bound for our pruned model. One interesting thing to note is that
this bound is only a small factor of log(DG)worse than if we had applied a compression bound to a
model of the size of the target function M. To our knowledge, this is one of the first generalization
analyses for lottery tickets.
7 Empirical Analysis
Code We have provided our code here for reproducibility. This was based on a fork of LaBonte
[2023].
We study the generalization bound obtained using our pruning Algorithm 1 with some standard
well-known norm-based generalization bounds of Neyshabur et al. [2015],Bartlett et al. [2017],
and Neyshabur et al. [2017] used as a baseline. Our experiments compare the generalization error
obtained by these bounds, the generalization bound of our algorithm (as provided in 5.2), and the true
generalization error of the compressed, trained model. We also provide an experiment demonstrating
how our generalization bound scales when increasing the hidden dimension of our models.
Our models are Multi-Layer Perceptron Models (MLPs) with ReLU activation layers with 5layers.
We train our algorithm with a learning rate of 0.02with the Adam optimizer [Kingma and Ba, 2014]
for300epochs. We conduct our experiments on two different image classification datasets: MNIST
[LeCun and Cortes, 2010] and CIFAR10 [Krizhevsky et al.]. We use an MLP with a hidden dimension
of784to compare our bounds to other generalization bounds. For our experiments on scaling with
model size, we test on hidden dimensions 500,1000,1500,2000,and2500 where the depth is kept
constant.
8

--- PAGE 9 ---
−404812162024
Bartlett 2017
Neyshabur 2015 Neyshabur 2017Ours
True errorGeneralization error (log)Setting
MNIST 784(a) Comparing bounds on MNIST.
−4048121620242832
Bartlett 2017
Neyshabur 2015 Neyshabur 2017Ours
True errorGeneralization error (log)Setting
CIFAR10 (b) Comparing bounds on CIFAR10.
36912
MNIST 500MNIST 1000 MNIST 1500 MNIST 2000 MNIST 2500Generalization error (log)Bartlett 2017
Our bound (c) Dependence on model size.
Figure 1: Comparison of the generalization bounds on logarithmic scale w.r.t. (a) MNIST, and (b)
CIFAR10 datasets. In (c), we see how our bounds depend on the size of the model.
Results Our bounds are several orders of magnitude better than the baseline state-of-the-art gener-
alization bounds, as can be inferred from Figure 1a and Figure 1b above. In both experiments, the
closest bound to ours is that of Bartlett et al. [2017], which is still at least 103and107times greater
than our bounds on the MNIST and the CIFAR10 datasets respectively. Moreover, our generalization
bound is consistently better than Bartlett et al. [2017] as the hidden dimension grows Figure 1c. This
demonstrates that across several datasets, our bounds are tighter than traditional norm-based general-
ization bounds and scale better with hidden dimensions. We provide some additional experiments in
the appendix as well. The results, although remarkable, are not surprising mainly due to the use of
our pruning algorithm, which ensures the error due to compression is low, and making use of Sparse
Matrix Sketching, which significantly reduces the dimension of the pruned model, a key factor while
computing generalization bounds.
8 Conclusion
This paper has made progress on the problem of deriving generalization bounds of overparametrized
neural networks. With our efficient pruning algorithm and Sparse Matrix Sketching, we have obtained
bounds for pruned models which are significantly better than well-known norm-based generalization
bounds and empirically verified the effectiveness of this approach on actual data. We hope these
results will fuel further research in deep learning to understand better how and why models generalize.
It would be interesting to see if matrix sketching can be used to prove the generalization for different
types of pruning, such as coresets. Moreover, it could also be fruitful to see whether matrix sketching
can be used alongside PAC-Bayes bounds to yield generalization bounds as well. In this regard, we
have extended the general framework of this paper to derive generalization bounds for lottery tickets
in Section 6, a result which, to our knowledge, is the first of its kind. Another possibility would be
to explore how different generalization bounds can be formed for different data distributions from
different training algorithms.
Limitations Our magnitude-based pruning algorithm does not prune the diagonal of the matrix,
which is not standard. Moreover, after training, we assume that each atom belongs to an i.i.d Gaussian
distribution, which may not always hold. Also, similar to the standard bounds, our generalization
bounds are still vacuous, not fully explaining the generalization of the models.
References
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. CoRR , abs/1802.05296, 2018. URL http://arxiv.org/
abs/1802.05296 .
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research , 3(Nov):463–482, 2002.
9

--- PAGE 10 ---
Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. CoRR , abs/1706.08498, 2017. URL http://arxiv.org/abs/1706.08498 .
Brian Bartoldson, Ari Morcos, Adrian Barbu, and Gordon Erlebacher. The generalization-stability
tradeoff in neural network pruning. Advances in Neural Information Processing Systems , 33:
20852–20864, 2020.
Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration
for deep neural networks. CoRR , abs/1710.09282, 2017. URL http://arxiv.org/abs/1710.
09282 .
Maxwell D. Collins and Pushmeet Kohli. Memory bounded deep convolutional networks. CoRR ,
abs/1412.1442, 2014. URL http://arxiv.org/abs/1412.1442 .
Amit Daniely and Elad Granot. Generalization bounds for neural networks via approximate descrip-
tion length. Advances in Neural Information Processing Systems , 32, 2019.
Gautam Dasarathy, Parikshit Shah, Badri Narayan Bhaskar, and Robert D. Nowak. Sketching sparse
matrices. CoRR , abs/1303.6544, 2013. URL http://arxiv.org/abs/1303.6544 .
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization
for efficiently improving generalization. CoRR , abs/2010.01412, 2020. URL https://arxiv.
org/abs/2010.01412 .
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Training pruned neural networks.
CoRR , abs/1803.03635, 2018. URL http://arxiv.org/abs/1803.03635 .
Tomer Galanti, Mengjia Xu, Liane Galanti, and Tomaso Poggio. Norm-based generalization bounds
for compositionally sparse neural networks. arXiv preprint arXiv:2301.12033 , 2023.
Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning . MIT Press, Cambridge,
MA, USA, 2016. http://www.deeplearningbook.org .
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. Advances in neural information processing systems , 28, 2015.
Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. What do com-
pressed deep neural networks forget? arXiv preprint arXiv:1911.05248 , 2019.
Tian Jin, Michael Carbin, Daniel M Roy, Jonathan Frankle, and Gintare Karolina Dziugaite. Prun-
ing’s effect on generalization through the lens of training and regularization. arXiv preprint
arXiv:2210.13738 , 2022.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced
research). URL http://www.cs.toronto.edu/~kriz/cifar.html .
Tyler LaBonte. Milkshake: Quick and extendable experimentation with classification models.
http://github.com/tmlabonte/milkshake , 2023.
Rafal Latala. Some estimates of norms of random matrices. Proceedings of the American Mathemati-
cal Society , 133:1273–1282, 05 2005. doi: 10.2307/4097777.
Lei Le, Andrew Patterson, and Martha White. Supervised autoencoders: Improving generalization
performance with unsupervised regularizers. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems ,
volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_
files/paper/2018/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf .
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/ .
10

--- PAGE 11 ---
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for
efficient convnets. CoRR , abs/1608.08710, 2016. URL http://arxiv.org/abs/1608.08710 .
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learn-
ing efficient convolutional networks through network slimming. In Proceedings of the IEEE
international conference on computer vision , pages 2736–2744, 2017.
Philip M. Long and Hanie Sedghi. Generalization bounds for deep convolutional neural networks,
2020.
Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir. Proving the lottery ticket
hypothesis: Pruning is all you need. CoRR , abs/2002.00585, 2020. URL https://arxiv.org/
abs/2002.00585 .
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efficient inference, 2017.
Ben Mussay, Samson Zhou, Vladimir Braverman, and Dan Feldman. On activation function core-
sets for network pruning. CoRR , abs/1907.04018, 2019. URL http://arxiv.org/abs/1907.
04018 .
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on learning theory , pages 1376–1401. PMLR, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. A pac-bayesian
approach to spectrally-normalized margin bounds for neural networks. CoRR , abs/1707.09564,
2017. URL http://arxiv.org/abs/1707.09564 .
Xin Qian and Diego Klabjan. A probabilistic approach to neural network pruning. CoRR ,
abs/2105.10065, 2021. URL https://arxiv.org/abs/2105.10065 .
Andréa Richa, Michael Mitzenmacher, and Ramesh Sitaraman. The power of two random choices: A
survey of techniques and results. 10 2000. doi: 10.1007/978-1-4615-0013-1_9.
Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression,
2017.
Roman Vershynin. High-dimensional probability. 2019. URL https://www.math.uci.edu/
~rvershyn/papers/HDP-book/HDP-book.pdf .
11

--- PAGE 12 ---
A Computation Details
Here, we provide some information about the hardware and setup used for our work. We include
a copy of the code in the Supplementary Material for reproducibility. Our experiments were run
with an Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz and an NVIDIA GeForce RTX 2080 Ti.
Moreover, our experiments are run using Python version 3.10. Also, our experiments were done
using a batch size of 256for our experiments on MNIST and a batch size of 4for CIFAR10. The
Deep Learning software library used was PyTorch Lightning. We have additionally included our code
in the supplementary material alongside with a README on how to reproduce our experiments.
B Proof of Pruning and Discretization Error
We state that much of this error analysis is inspired by the proofs in Qian and Klabjan [2021]. We,
however, use the more reasonable Gaussian distribution assumption over the weights and have a
slightly different Magnitude-based Pruning algorithm.
B.1 Proof of Lemma 4.1
Lemma 4.1. The Expected Value of any entry ∆l
ijof the matrix ∆l=ˆAl−Alis given by E(∆l
ij) = 0
for any i, j∈[dl
1]×[dl
2], l∈[L]. Thus, E(∆l) =0is a matrix full of 0’s. Furthermore, we have
thatE((∆l
ij)2) =d3
2Ψ
(d+2)3
2. Moreover, the fourth moment of any entry (∆l
ij)4of∆lis given by
E((∆l
ij)4) =3d5
2Ψ2
(d+2)5
2.
Proof. By the definition of our Algorithm 1, the random variable ∆l
ijdepends on the random variable
Al
i,j.
∆l
ij=

Al
i,j w.p.exp
−[Al
i,j]2
dΨ
0 w.p.1−exp
−[Al
i,j]2
dΨ.
To calculate E 
∆l
ij
, we will use the Law of Total Expectation. That is, E 
∆l
ij
=E(E(∆l
ij|Al
i,j)).
We have that, E(∆l
ij|Al
i,j) =Al
i,jexp
−[Al
i,j]2
dΨ
. Therefore, to calculate E(E(∆l
ij|Al
i,j)), we use
the definition of expectation for continuous variables as
E(∆l
ij) =E(E(∆l
ij|Al
i,j))
=Z∞
−∞Al
i,jexp 
−
Al
i,j2
dΨ!
1√
2πΨexp
−1
2 
Al
i,j√
Ψ!2
dAl
i,j
= 0.
We now focus on the squared component of our lemma. Similarly, we have
E((∆l
ij)2) =E(E((∆l
ij)2|Al
i,j))
=Z∞
−∞(Al
i,j)2exp 
−
Al
i,j2
dΨ!
1√
2πΨexp
−1
2 
Al
i,j√
Ψ!2
dAl
i,j
=d3
2Ψ
(d+ 2)3
2.
12

--- PAGE 13 ---
We similarly compute the fourth moment as
E((∆l
ij)4) =E(E((∆l
ij)4|Al
i,j))
=Z∞
−∞(Al
i,j)4exp 
−
Al
i,j2
dΨ!
1√
2πΨexp
−1
2 
Al
i,j√
Ψ!2
dAl
i,j
=3d5
2Ψ2
(d+ 2)5
2.
B.2 Proof of Lemma 4.2
Lemma 4.2. For any given layer l∈[L], we have with probability at least 1−1
ϵl
∥(ˆAl−Al)∥2≤ϵlΓlwhere Γl=C
 s
d3
2Ψ
(d+ 2)3
2!q
dl
1+q
dl
2
+ 
3dl
1dl
2d5
2Ψ2
(d+ 2)5
2!1
4
.
Here, ˆAlis generated by Algorithm 1 and Cis a universal positive constant.
We will prove that the error from the compression is bounded. To do so will first use the result that
the expected norm of any zero-mean matrix can be bounded using the second and fourth moments.
We restate this useful technical lemma from Theorem 2 of Latala [2005].
Lemma B.1. LetAbe a random matrix whose entries Ai,jare independent mean zero random
variables with finite fourth moment. Then
E∥A∥2≤C
max
i
X
jEA2
i,j
1
2
+ max
j X
iEA2
i,j!1
2
+
X
jEA4
i,j
1
4
.
Here, C, is a universal positive constant.
We now use this lemma to bound the error due to compression using Algorithm 1.
Proof. LetZbe our mask such that ˆAl
i,j=Z◦Al
i,jwhere◦is the elementwise-matrix product. We
will analyze the difference matrix ∆ =Z◦Al
i,j−Al
i,j. Note that
E((∆l
ij)2) =E((∆l
ij)2|Zi,j= 0)·P(Zi,j= 0) + E((∆l
ij)2|Zi,j= 1)·P(Zi,j= 1) .
Trivially, if the mask for an atom is set to 1, the squared error for that atom is 0. Therefore, we have
that
E(∆l
ij|Zi,j= 1)P(Zi,j= 1) = 0 .
Thus, we only need to analyze the second term. We have
E((∆l
ij)2|Zi,j= 0)·P(Zi,j= 0) = P(Zi,j= 0)Z∞
−∞(Al
i,j)2·P(Al
i,j|Zi,j= 0)dAl
i,j
=Z∞
−∞(Al
i,j)2·P(Zi,j= 0|Al
i,j)·P(Al
i,j)dAl
i,j
=Z∞
−∞(Al
i,j)2·exp 
−[Al
i,j]2
dΨ!
·1√
2πΨexp
−1
2 
Al
i,j√
Ψ!2
dAl
i,j
=d3
2Ψ
(d+ 2)3
2.
We then have that
E((∆l
ij)2) =d3
2Ψ
(d+ 2)3
2.
13

--- PAGE 14 ---
Similarly, we can do the same for the fourth moment.
E((∆l
ij)4|Zi,j= 0)·P(Zi,j= 0) = P(Zi,j= 0)Z∞
−∞(Al
i,j)4·P(Al
i,j|Zi,j= 0)dAl
i,j
=Z∞
−∞(Al
i,j)4·P(Zi,j= 0|Al
i,j)·P(Al
i,j)dAl
i,j
=Z∞
−∞(Al
i,j)4·P(Zi,j= 0|Al
i,j)·P(Al
i,j)dAl
i,j
=Z∞
−∞(Al
i,j)4·exp 
−[Al
i,j]2
dΨ!
·1√
2πΨexp
−1
2 
Al
i,j√
Ψ!2
dAl
i,j
=3d5
2Ψ2
(d+ 2)5
2.
Combining this with Lemma B.1, we have,
E∥∆l∥2≤C
 s
d3
2Ψ
(d+ 2)3
2!q
dl
1+q
dl
2
+ 
3dl
1dl
2d5
2Ψ2
(d+ 2)5
2!1
4
.
We can then apply Markov’s inequality where
P(∥∆l∥2≥t)≤E∥∆l∥2
t.
We set Γl=C"r
d3
2Ψ
(d+2)3
2p
dl
1+p
dl
2
+
3dl
1dl
2d5
2Ψ2
(d+2)5
21
4#
for notational ease. If we set
t=ϵlΓl, then we have with probability at least 1−1
ϵlthat,
∥∆l∥2≤ϵlΓl.
We have proven our statement.
B.3 Proof of True Perturbation Bound
Lemma B.2. For the weights of the model Mand any perturbation Ulforl∈[h]where the
perturbed layer lisUl+Al, given that ∥Ul∥2≤1
L∥Al∥2, we have that for all input x0∈ S,
∥xL−¯xL∥2≤ed0
1 LY
l=1κlLl∥Al∥2!LX
l=1∥Ul∥2
∥Al∥2.
Here, ¯xLdenotes the output of the Lth layer of the perturbed model.
Proof. This proof mainly follows from Neyshabur et al. [2017]. We restate it here with the differing
notation for clarity and completeness. We will prove the induction hypothesis that ∥¯xl−xl∥2≤ 
1 +1
Ll∥x0∥2Ql
i=1Li∥Al∥2Pl
i=1∥Ui∥2
∥Ai∥2. The base case of induction trivially holds, given we
have that ∥¯x0−x0∥2= 0by definition. Now, we prove the induction step. Assume that the induction
14

--- PAGE 15 ---
hypothesis holds for l. We will prove that it holds for l+ 1. We have that
∥xl−¯xl∥2≤ ∥ 
Al+Ul
ϕl(¯xl−1)−Alϕl(xl−1)∥2
≤ ∥ 
Al+Ul
(ϕl(¯xl−1)−ϕl(xl−1)) +Ulϕl(xl−1)∥2
≤ 
∥Al∥2+∥Ul∥2
∥ϕl(¯xl−1)−ϕl(xl−1)∥2+∥Ul∥2∥ϕl(xl−1)∥2 (2)
≤ 
∥Al∥2+∥Ul∥2
∥ϕl(¯xl−1)−ϕl(xl−1)∥2+∥Ul∥2∥ϕl(xl−1)∥2
≤Ll 
∥Al∥2+∥Ul∥2
∥¯xl−1−xl−1∥2+Ll∥Ul∥2∥xl−1∥2 (3)
≤Ll
1 +1
d 
∥Al∥2
∥¯xl−1−xl−1∥2+Ll∥Ul∥2∥xl−1∥2
≤Ll
1 +1
d 
∥Al∥2
1 +1
Ll−1
∥x0∥2 l−1Y
i=1Li∥Ai∥2!l−1X
i=1∥Ui∥2
∥Ai∥2+Ll∥Ul∥2∥xl−1∥2
(4)
≤Ll
1 +1
Ll l−1Y
i=1Li∥Ai∥2!l−1X
i=1∥Ui∥2
∥Ai∥2∥x0∥2+Ll∥x0∥2∥Ul∥2l−1Y
i=1Li∥Ai∥2
≤Ll
1 +1
Ll l−1Y
i=1Li∥Ai∥2!l−1X
i=1∥Ui∥2
∥Ai∥2∥x0∥2+∥x0∥2∥Ul∥2
∥Al∥2lY
i=1Li∥Ai∥2
≤
1 +1
Ll lY
i=1Li∥Ai∥2!lX
i=1∥Ui∥2
∥Ai∥2∥x0∥2
Here, Equation (2) results from applying Lemma C.1. Equation (3) comes from the fact that ϕiis
Li-Lipschitz smooth and that ϕi(0) = 0 . Moreover, Equation (4) comes from applying the induction
hypothesis. Therefore, we have proven the induction hypothesis for all layers. We now only need the
fact that 
1 +1
LL≤e, and we have our final statement.
B.4 Proof of Lemma B.3
Lemma B.3. The difference between outputs of the pruned model and the original model on any
input xis bounded by, with probability at least 1−PL
iϵ−1
i,
∥ˆxL−xL∥ ≤ed0
1 LY
l=1Ll∥Al∥2!LX
l=1ϵlΓl
∥Al∥2.
Proof. We will compare the output of the original model xlwith the output of the compressed
model. We need the fact that1
L∥Al∥2≥ϵlΓl≥ ∥Al−ˆAl∥2. From Vershynin [2019], we have that
E(1
L∥Al∥2)≥1
4Lp
dl
1+p
dl
2
, andϵlΓlis smaller than this in expectation for sufficiently small
ϵl. Therefore, we can use Lemma B.2 and Lemma 4.2. Thus we have the following
∥xl−ˆxl∥2≤ed0
1 LY
l=1Ll∥Al∥2!LX
l=1∥Al−ˆAl∥2
∥Al∥2
≤ed0
1 dY
l=1Ll∥Al∥2!LX
l=1ϵlΓl
∥Al∥2
B.5 Proof of Lemma 4.4
Lemma 4.4. The norm of the difference between the pruned layer and the discretized layer is
upper-bounded as ∥˜Al−ˆAl∥2≤ρlJlwhere Jlis the number of nonzero parameters in ˆAl(Jlis
used for brevity here and will be analyzed later). With probability at least 1−PL
l=1ϵ−1
l, given
15

--- PAGE 16 ---
that the parameter ρlfor each layer is chosen such that ρl≤1
L∥Al∥2−ϵlΓl
Jl, we have that the error
induced by both discretization and the pruning is bounded by
∥xL−˜xL∥2≤ed0
1 LY
l=1Ll∥Al∥2!LX
l=1ϵlΓl+ρlJl
∥Al∥2.
Proof. We will compare the output of the original model xlwith the output of the compressed and
discretized model ˜xl. To use the perturbation bound from Lemma B.2, we need that ∥Al−˜Al∥2≤
1
L∥Al∥2. For each layer, we can choose a discretization parameter to satisfy this. We demonstrate
this in the following:
∥Al−˜Al∥2≤ ∥Al−ˆAl∥2+∥ˆAl−˜Al∥2
≤ϵlΓl+ρlJl
Therefore, as long as we choose
ρl≤1
L∥Al∥2−ϵlΓl
Jl,
we have our desired property. Therefore, using Lemma B.2, we have that
∥xl−˜xl∥2≤ed0
1 LY
l=1Ll∥Al∥2!hX
l=1∥Al−˜Al∥2
∥Al∥2
≤ed0
1 dY
l=1Llκl∥Al∥2!LX
l=1∥Al−ˆAl∥2+∥ˆAl−˜Al∥2
∥Al∥2
≤ed0
1 dY
l=1Ll∥Al∥2!LX
l=1ϵlΓl+ρlJl
∥Al∥2
This happens only if the event from Lemma 4.2 occurs for every layer. Using the union bound, we
know that this happens with probability at least 1−PL
lϵ−1
l
C Error Bound under Subgaussian Conditions
We can form tighter bounds by considering what the expected maximum of (ˆAl−Al)xwould be
with high probability. If dl
2< dl
1, we observe that the matrix ˆAl−Alhas at most dl
2nonzero singular
values. We need a Subgaussian condition assumption on our input to each layer to do this well to
improve our bounds.
Condition C.1. The input to each layer l∈[L], belongs to a distribution D, such that for some unit
magnitude vector vand an arbitrary vector xsampled from Dsatisfy
P(⟨x, v⟩ ≥t)≤ae−bt2dl
1.
Here aandbare universal constants greater than 0.
It should be noted that Condition C.1 is often seen throughout the theory of High Dimensional Statis-
tics. The uniform distribution over the unit sphere satisfies Condition C.1. Given this Condition C.1,
we can bound the approximation error from significantly increasing in any given layer.
We want to do a bound on how much error the compression introduces on the margin of the training
dataset. While traditional bounds assume worst-case blow-up, we can use the fact that vectors are
roughly orthogonal in high-dimensional spaces.
Lemma C.1. Suppose we are given a matrix Bof size dl
1×dl
2where dl
1≥dl
2andSis a collection
of vectors from a distribution satisfying Condition C.1. For any vector x∈ S, there exists constants
a, bsuch that
∥Bx∥2≤q
dl
2tl∥B∥2∥x∥2,
with probability at least 1− |S| ae−bt2
ldl
1. We will call κl=p
dl
2tlifdl
2≤dl
1andκl= 1otherwise.
16

--- PAGE 17 ---
Proof. We first decompose B=UΣVusing Singular Value Decomposition. Therefore, for any x
we have that,
∥Bx∥2=∥UΣV x∥2
=∥ΣV x∥2
=∥Σy∥2.
The second equality comes from the fact that Uis unitary and norm-preserving, and the third equality
comes from setting y=V x. Now, if xis some standard random normal vector, then ytoo is a
standard random normal vector. We also observe that Σis a diagonal matrix where only the first dl
2
values are nonzero. We use the well-known identity that if vis a vector with a magnitude of 1,
P(⟨v, y⟩ ≥t)≤ae−bt2dl
1.
Here, aandbare global constants. Therefore, applying this inequality for the respective nonzero
singular values in Σ, we have
P
∥Σy∥2≥q
dl
2t∥B∥2
≤ae−bt2dl
1,
since∥B∥2is the maximum singular value. Applying the union bound for each element of S, we
have that for every element in S
∥Bx∥2≤q
dl
2t∥B∥2∥x∥2,
with probability at least 1− |S| ae−bt2dl
1.
Lemma C.2. For the weights of the model Mand any perturbation Ulforl∈[h]where the
perturbed layer lisUl+Al, given that ∥Ul∥2≤1
L∥Al∥2, we have that for all input x0∈ S,
∥xL−¯xL∥2≤ed0
1 LY
l=1κlLl∥Al∥2!LX
l=1∥Ul∥2
∥Al∥2.
Here, ¯xLdenotes the output of the Lth layer of the perturbed model. This happens if Condition C.1
occurs.
Proof. This proof mainly follows from Neyshabur et al. [2017]. We restate it here with the differing
notation for clarity and completeness. We will prove the induction hypothesis that ∥¯xl−xl∥2≤ 
1 +1
Ll∥x0∥2Ql
i=1κiLi∥Al∥2Pl
i=1∥Ui∥2
∥Ai∥2. The base case of induction trivially holds, given
we have that ∥¯x0−x0∥2= 0 by definition. Now, we prove the induction step. Assume that the
17

--- PAGE 18 ---
induction hypothesis holds for l. We will prove that it holds for l+ 1. We have that
∥xl−¯xl∥2≤ ∥ 
Al+Ul
ϕl(¯xl−1)−Alϕl(xl−1)∥2
≤ ∥ 
Al+Ul
(ϕl(¯xl−1)−ϕl(xl−1)) +Ulϕl(xl−1)∥2
≤ 
∥Al∥2+∥Ul∥2
∥ϕl(¯xl−1)−ϕl(xl−1)∥2+∥Ul∥2∥ϕl(xl−1)∥2 (5)
≤ 
∥Al∥2+∥Ul∥2
∥ϕl(¯xl−1)−ϕl(xl−1)∥2+∥Ul∥2∥ϕl(xl−1)∥2
≤Ll 
∥Al∥2+∥Ul∥2
∥¯xl−1−xl−1∥2+Ll∥Ul∥2∥xl−1∥2 (6)
≤Ll
1 +1
d 
∥Al∥2
∥¯xl−1−xl−1∥2+Ll∥Ul∥2∥xl−1∥2
≤Ll
1 +1
d 
∥Al∥2
1 +1
Ll−1
∥x0∥2 l−1Y
i=1Liκi∥Ai∥2!l−1X
i=1∥Ui∥2
∥Ai∥2+Ll∥Ul∥2∥xl−1∥2
(7)
≤Ll
1 +1
Ll l−1Y
i=1Liκi∥Ai∥2!l−1X
i=1∥Ui∥2
∥Ai∥2∥x0∥2+Ll∥x0∥2∥Ul∥2l−1Y
i=1Li∥Ai∥2
≤Ll
1 +1
Ll l−1Y
i=1Liκi∥Ai∥2!l−1X
i=1∥Ui∥2
∥Ai∥2∥x0∥2+∥x0∥2∥Ul∥2
∥Al∥2lY
i=1Li∥Ai∥2
≤
1 +1
Ll lY
i=1Liκi∥Ai∥2!lX
i=1∥Ui∥2
∥Ai∥2∥x0∥2
Here, Equation (5) results from applying Lemma C.1. Equation (6) comes from the fact that ϕiis
Li-Lipschitz smooth and that ϕi(0) = 0 . Moreover, Equation (7) comes from applying the induction
hypothesis. Therefore, we have proven the induction hypothesis for all layers. We now only need the
fact that 
1 +1
LL≤e, and we have our final statement. If Condition C.1 is not satisfied, we need
only set κl= 1for all l∈[L]and the analysis will remain valid.
C.1 Proof of Lemma C.3
Lemma C.3. The difference between outputs of the pruned model and the original model on any
input xis bounded by, with probability at least 1−hPL
iϵ−1
i+|S|ae−bt2
ldl
1i
,
∥ˆxL−xL∥ ≤ed0
1 LY
l=1Llκl∥Al∥2!LX
l=1ϵlΓl
∥Al∥2.
Here, a, bare positive constants from the distribution of input data.
Proof. We will compare the output of the original model xlwith the output of the compressed
model. We need the fact that1
L∥Al∥2≥ϵlΓl≥ ∥Al−ˆAl∥2. From Vershynin [2019], we have that
E(1
L∥Al∥2)≥1
4Lp
dl
1+p
dl
2
, andϵlΓlis smaller than this in expectation for sufficiently small
ϵl. Therefore, we can use Lemma C.2. Thus we have the following
∥xl−ˆxl∥2≤ed0
1 LY
l=1Llκl∥Al∥2!LX
l=1∥Al−ˆAl∥2
∥Al∥2
≤ed0
1 dY
l=1Llκl∥Al∥2!LX
l=1ϵlΓl
∥Al∥2
Lemma C.4. The norm of the difference between the pruned layer and the discretized layer is upper-
bounded as ∥˜Al−ˆAl∥2≤ρlJlwhere Jlis the number of nonzero parameters in ˆAl(Jlis used for
18

--- PAGE 19 ---
brevity here and will be analyzed later). With probability at least 1−hPL
l=1ϵ−1
l+|S|ae−bt2
ldl
1i
,
given that the parameter ρlfor each layer is chosen such that ρl≤1
L∥Al∥2−ϵlΓl
Jl, we have that the
error induced by both discretization and the pruning is bounded by
∥xL−˜xL∥2≤ed0
1 LY
l=1Llκl∥Al∥2!LX
l=1ϵlΓl+ρlJl
∥Al∥2.
Proof. We will compare the output of the original model xlwith the output of the compressed and
discretized model ˜xl. To use the perturbation bound from Lemma C.2, we need that ∥Al−˜Al∥2≤
1
L∥Al∥2. For each layer, we can choose a discretization parameter to satisfy this. We demonstrate
this in the following:
∥Al−˜Al∥2≤ ∥Al−ˆAl∥2+∥ˆAl−˜Al∥2
≤ϵlΓl+ρlJl
Therefore, as long as we choose
ρl≤1
L∥Al∥2−ϵlΓl
Jl,
we have our desired property. Therefore, using Lemma B.2, we have that
∥xl−˜xl∥2≤ed0
1 LY
l=1Llκl∥Al∥2!hX
l=1∥Al−˜Al∥2
∥Al∥2
≤ed0
1 dY
l=1Llκl∥Al∥2!LX
l=1∥Al−ˆAl∥2+∥ˆAl−˜Al∥2
∥Al∥2
≤ed0
1 dY
l=1Llκl∥Al∥2!LX
l=1ϵlΓl+ρlJl
∥Al∥2
This happens only if the event from Lemma 4.2 and Lemma C.1 occur for every layer. Using the
union bound, we know that this happens with probability at least 1−hPL
lϵ−1
l− |S| ae−bt2
ldl
1i
.
D Naive Generalization Proofs
Given the Gaussian assumption, it is natural to count the number of possible outcomes of the
compression algorithm by counting the number of possible configurations of nonzero atoms in any
matrix and then counting the possible values each atom could take after quantization. We provide the
generalization bound from this intuition.
Lemma D.1. Using the counting arguments above yields a generalization bound
R0(gA)≤ˆRγ(f) +O
sP
llog( dl
1dl
2
α
) +αlog1
ρl
n
.
This holds when dis chosen such that γ≥ed0
1QL
l=1Ll∥Al∥2PL
l=1ϵlΓl+ρlJl
∥Al∥2.
We now provide the requisite knowledge to prove this bound. We first analyze a naive methodology
for counting the number of possible outcomes from the learning algorithm and compression scheme.
We first provide a slightly altered generalization bound to fit our use case better.
Theorem D.1. If there are Jdifferent parameterizations, the generalization error of a compression
gais, with probability at least 1−δ,
L0(gA)≤ˆLγ(f) +vuutlnq
J
δ
n.
19

--- PAGE 20 ---
Proof. Most of this proof follows from Theorem 2.1 from Arora et al. [2018]. For each A∈ A, the
training loss ˆR0(gA)is the average of ni.i.d. Bernoulli Random variables with expected value equal
toR0(gA). Therefore, by standard Chernoff bounds, we have that,
P(R0(gA)−ˆR0(gA)≥τ)≤exp(−2τ2n).
Given fis(γ,S)-compressible by gA, we know the empirical margin loss of gAfor margin 0is
less than the empirical margin loss of fwith margin γ, i.e. ˆR0(gA)≤ˆRγ(f). Given there are J
different parameterizations, by union bound, with probability at least 1−Jexp(−2τn), we have
R0(gA)≤τ+ˆR0(gA). Setting Jexp(−2τn) =δ, we have τ=r
ln√
J
δ
n. Therefore, with
probability 1−δ, we have
R0(gA)≤ˆRγ(f) +vuutlnq
J
δ
n.
Now, we can state the number of parameterizations achievable by our compression algorithm. If
there are dl
1dl
2elements in the matrix and αstays nonzero after compression, then there are dl
1dl
2
α
total parameterizations for each layer. Moreover, within each parameterization, there are rαways
to choose the values that each nonzero element takes given each of the αatoms can take rvalues
where r=O
1
ρl
. We, therefore, need a bound on the number of elements that stay nonzero after
pruning. We achieve this with the following two lemmas. We will first prove that at least τelements
have probability κof getting compressed. Using such a counting argument yields the following
generalization bound.
Lemma D.2. At least τelements of a given matrix Alwill have a probability of at least κof
getting compressed. This event occurs with probability at least 1−I1−p1(d1d2−τ,1 +τ)where
p1= erfq
−dln(κ)
2
. Here, erfis the Gauss Error Function.
Proof. For any given element to have a probability of at least κof getting compressed,
exp 
−A2
i,j
dΨ!
≥κ.
This means that
|Ai,j| ≤p
−dΨ ln(κ).
Given that |Ai,j|follows a Folded Normal Distribution, the probability of this happening is
p1=P
|Ai,j| ≤p
−dΨ ln(κ)
= erf p
−dΨ ln(κ)√
2Ψ!
= erf r
−dln(κ)
2!
(8)
For notational ease, we denote the set of atoms that satisfy this criterion C=n
(i, j)|exp−A2
i,j
dΨ
≥κo
. Therefore, the number of elements τthat will have the probability
of getting compressed larger than κobeys a binomial distribution. Therefore,
P(|C| ≥ τ) = 1−I1−p1(d1d2−τ,1 +τ).
Here, Iis the Regularized Incomplete Beta Function.
20

--- PAGE 21 ---
Using this probabilistic upper bound from Lemma D.2, we can upper bound the number of nonzero
elements in any matrix.
Lemma D.3. Given the event from Lemma D.2 happens, the probability that at least αelements will
end up being compressed is at least 1−I1−κ(τ−α, α+ 1) .
Proof. There are at least τelements with probability greater than κ. In the worst case, the other
dl
1dl
2−τelements are not compressed. The probability distribution over the remaining elements is a
binomial distribution with probability κ. Therefore, the probability that at least αof the τelements
are compressed is at least 1−I1−κ(τ−α, α+ 1) .
Now, we can finally prove our naive generalization bound.
Proof. From Theorem D.1, we have
L0(gA)≤ˆLγ(f) +vuutlnq
J
δ
n,
where Jis the number of parameter configurations. Each matrix has dl
1dl
2
α
different ways to arrange
the nonzero elements. Within any such configuration, there are rαways to select the values for any
of the nonzero elements, where rl=O
1
ρl
is the number of values any atom could take after
discretization. This yields a generalization bound of
R0(gA)≤ˆRγ(f) +O
s
log( dl
1dl
2
α
) +αlogrl
n
.
Here, we only require that γ≥ed0
1Qd
l=1Llκl∥Al∥2PL
l=1ϵlΓl+ρlJl
∥Al∥2given Lemma C.4.
Regrettably, such a bound is quite poor in its dependence on the size of the matrices, mainly due
to the logarithm term of the factorial, which is a significantly large value. This is, in fact, worse
than many of the previous bounds in the literature. This is due to the combinatorial explosion of the
number of sparse matrices. However, if there exists a way to instead represent the space of sparse
matrices within the space of dense matrices of much smaller dimensions, then we could avoid such a
combinatorial explosion of the number of parameters. This is the exact purpose of matrix sketching.
E Matrix Sketching Proofs
E.1 How to choose A, B
To generate AorB, we can first sample a random bipartite graph where the left partition is of size m,
and the right partition is of size p1or the dimension of the matrix to be sketched. If we say that any
node in the left partition is connected to at most δnodes, we can call this bipartite graph a δ-random
bipartite ensemble. We have the resulting definition below.
Definition E.1. Gis a bipartite graph G= ([x],[y],E)where xandyare the size of the left and
right partitions, respectively, and Eis the set of edges. We call Gaδ-random bipartite ensemble if
every node in [x]is connected to most δnodes in [y]and each possible connection is equally likely.
Given this setup, we can choose the matrices AandBas adjacency matrices from a random δ-random
bipartite ensemble. Intuitively, such AandBare chosen such that any row in AorBhas at most δ1’s.
Any given element of YijisP
klAik˜XklBlj. However, only approximately δ2of the elements in the
sum are nonzero. Therefore, Yijis expressed as the sum of δ2terms from the sumP
klAik˜XklBlj.
We can then express many elements from Yby changing which elements are set or not set to zero in
this sum. This gives a visual explanation of how this sketching works. Furthermore, the power of the
expressiveness of the sketching depends mainly on the parameters mandδ. Here, we can bound the
size required for mandδsuch that the solution to Equation (1) leads to one-to-one mapping with
high probability.
21

--- PAGE 22 ---
E.2 Remaining Proofs
Given that each of the atoms is identically distributed and independent, given Natoms are not pruned,
the problem of characterizing how these atoms are distributed among the rows or columns is similar
to the famous balls-and-bins problem. We provide a helper lemma to prove that our pruning method
generates a distributed sparse matrix. We use the famous lemma from Richa et al. [2000].
Lemma E.1. Consider the problem of throwing Nballs independently a uniformly at random into n
bins. Let Xjbe the random variable that counts the number of balls in the j-th bin. With probability
at least 1−n−1
3, we have that
max
jXj≤3N
n.
We now use this lemma to prove our distributed sparsity.
E.3 Proof of Lemma 5.1
Lemma 5.1. With probability at least 1−1
λl−(dl
1)−1
3−(dl
2)−1
3, we have that the outputs from
Algorithm 1 are jr, jc-sparsely distributed where max( jr, jc)≤3λlmax( dl
1, dl
2)χandλl∈R.
Here, χ=√d+2−√
d√d+2.
Proof. We will first prove a bound on the number of noncompressed atoms, a random variable we
will call N. The probability that any given element gets pruned is
P(Zi,j= 0) =Z∞
−∞P(Zi,j= 0|Al
i,j)·P(Al
i,j)dAl
i,j (9)
=Z∞
−∞exp 
−(Al
i,j)2
dΨ!
1√
2πΨexp 
−1
2(Al
i,j)2
Ψ!
dAl
i,j (10)
=√
d+ 2−√
d√
d+ 2(11)
Therefore, the expected number of nonzero elements after pruning is E(N) =dl
1dl
2(√d+2−√
d)√d+2. Using
Markov’s inequality, we have that
P(N≥t)≤E(N)
t.
Here, we set t=λidl
1dl
2(√d+2−√
d)√d+2. Using this, we have with probability at least 1−1
λi,
N≤λidl
1dl
2(√
d+ 2−√
d)√
d+ 2.
Here, we can use Lemma E.1. For the rows, with probability at least 1−(dl
1)−1
3, we have that the
maximum number of nonpruned atoms in any row is at most
3N
dl
1= 3λidl
2(√
d+ 2−√
d)√
d+ 2.
Similarly, we have that the maximum number of nonpruned atoms in any column is at most
3N
dl
2= 3λidl
1(√
d+ 2−√
d)√
d+ 2.
Therefore, we have that this occurs with probability at least 1−1
λi−(dl
1)−1
3−(dl
2)−1
3.
22

--- PAGE 23 ---
E.4 Proof of Theorem 5.2
Theorem 5.2. For every matrix ˆAl, define jlto be the max( jr, jc)where jrandjcare the distribution-
sparsity coefficients for ˆAl. Moreover, for every matrix ˆAl, define pl= max( dl
1, dl
2). Then we have
that
R0(gA)≤ˆRγ(f) +O
sP
l3λlχdl
2dl
1log2(pl) log(1
ρl)
n
.
This holds when dis chosen such that γ≥ed0
1Qd
l=1Ll∥Al∥2PL
l=1ϵlΓl+ρlJl
∥Al∥2where Jl≤
O 
χdl
2dl
1
. This claim holds with probability at least 1−hPL
l=1λ−1
l+ϵ−1
l+p−c
li
.
Proof. From Lemma 5.1, we know that max( jr, jc)≤3λimax( dl
2,dl
1)(√d+2−√
d)√d+2. Therefore, we
can compress any matrix Alinto a sparse matrix ˆAland then further into a small matrix of size
(√jlpllog(pl))2from Theorem 5.1.Therefore, we have that
(p
jlpllog(pl))2≤3λidl
2dl
1(√
d+ 2−√
d)√
d+ 2log2(pl).
By Theorem 3.1, we have that
L0(gA)≤ˆLγ(f) +O
sP
l3λidl
2dl
1(√d+2−√
d)√d+2log2(pl) log(1
ρl)
n
.
E.5 Proof of Theorem 6.2
Theorem 6.2. With probability at least 1−δ−LD−c
Ghave the generalization error of
R0(˜G)≤ˆRϵ+ϵρ(˜G) +O
vuut[nMd0,1log(DG)2+Ln2
Mlog(DG)2] log
1
ρ
n
.
Here, ϵρis the small error introduced by discretization.
Proof. Proving a generalization bound using our framework usually includes one, proving the error
due to compression is bounded, and two, obtaining a bound on the number of parameters. Malach
et al. [2020] fortunately proves both. We restate the bound from Arora et al. [2018]:
R0(˜G)≤ˆRγ(˜G) +O r
qlogr
n!
.
From Theorem 6.1, we have that
sup
x∈X|F(x)−˜G(x)| ≤ϵ.
Directly setting γ=ϵ+ϵρsatisfies our error requirement, where ϵρis the small error introduced due to
discretization. Now, we must focus on bounding the number of parameters in the model. Fortunately,
Malach et al. [2020] provides a useful bound. They show that the first layer has approximately
O(DFd1
0)nonzero parameters, and the rest of the layers of ˜Ghave approximately O(D2
F)nonzero
parameters. Moreover, from the proof of Theorem 2.1, they show that these nonzero parameters are
evenly distributed across rows and columns. Therefore, we can use our matrix sketching framework to
show that we can compress the set of outputs from Iterative Pruning to a smaller, dense set of matrices.
Namely, the middle layers of ˜Gsuch as W˜G
ican be represented as a smaller matrix of dimension
m=O(DFlog(DG))from Theorem 5.1. For the first layer, we can also use matrix sketching to
23

--- PAGE 24 ---
represent it as a matrix of size O(p
DFd0,1log(DG)). We now have an appropriate bound on the
number of parameters in our sketched models. We apply trivial discretization by rounding the nearest
value of ρ. Therefore, we have from Arora et al. [2018]
R0(˜G)≤ˆRϵ+ϵρ(˜G) +O
vuut[DFd0,1log(DG)2+LD2
Flog(DG)2] log
1
ρ
n
.
We can apply the matrix sketching to each of the Lrows with probability at least 1−D−c
Gaccording
to Theorem 5.1. The error of the pruned model is also bounded by ϵwith at least probability 1−δ.
Union bounding these together show that this bound holds with probability at least 1−δ−LD−c
G.
F Additional Empirical Results
We show the detailed empirical results on the MNIST and CIFAR10 datasets in Table 1 and Table 2
respectively, and are supplemental to the results obtained in Section 7. All bounds are shown on a
logarithmic scale. We compare our bounds with some standard norm-based generalization bounds
of Neyshabur et al. [2015],Bartlett et al. [2017], and Neyshabur et al. [2017]. For comparing our
bound on MNIST, we use an MLP with hidden dimensions 500, 784, 1000, 1500, 2000, and 2500
where the depth is kept constant. The model training details are detailed in Section 7. We see that
across different hidden dimensions, our generalization bounds are consistently better than other
generalization bounds. Over different hidden dimensions, the true generalization error seems to
remain relatively stable. Relative to other bounds, our generalization bound seems more stable
than other bounds, increasing at a lesser rate than other bounds as the hidden dimension increases.
However, we unfortunately do not capture the seeming independence between the hidden dimension
and true generalization error. For our bound, this is due to the fact that the margin of the trained
model is not increasing enough with the increase in model size. Our bound predicts the generalization
error of pruning in terms of the margin of the original model. If the margin of the original model
does not increase while the model’s size increases, our bound will increase. Therefore, this bound
needs more information to capture generalization more accurately.
Additionally, we show the dependence of our bound on the number of training epochs in Figure
2, where we take the original MLP of depth 5and compare how our generalization bound and the
true generalization error change over epochs. It is to be noted that our bound is scaled to be in the
same range as the true generalization error. There are differences between the curves, indicating our
bound needs to include additional information needed to explain generalization fully. Our bound
does decrease over time as the margin increases, mimicking the true generalization error. The main
interesting difference is that the downward curve for our bound occurs in two places. The first drop
in our generalization bound happens only because of the drop of the generalization error, but the
margin is still negative. Once the margin becomes positive and increases, our bound slowly begins
to decrease. At this point, however, the true generalization error seems to have already reached its
minimum.
In Table 2, all the insights noticed on the MNIST dataset seem to extend to CIFAR10. Our gener-
alization bound is tighter than existing state–of–the–art norm-based generalization bounds. Indeed
our error is orders of magnitude tighter than other generalization bounds. We note that while all
generalization bounds here are far worse than the MNIST counterparts, our generalization bound
most accurately reflects the true jump in generalization error between MNIST and CIFAR10. For
both ours and the true generalilzatiion error, the bounds differ by one order of magnitude between
MNIST and CIFAR10. However, the other bounds differ by at least 7orders of magnitude. Our
bound seems to capture more of the behavior of the true generalization error than these other bounds
in this regard.
24

--- PAGE 25 ---
METHOD MNIST
500MNIST
784MNIST
1000MNIST
1500MNIST
2000MNIST
2500
Neyshabur 2015 22.29 23.56 24.42 25.12 27.03 27.72
Neyshabur 2017 17.91 18.34 18.70 18.81 21.50 21.57
Bartlett 2017 11.51 11.68 11.87 11.70 13.96 13.81
Ours 3.36 3.77 4.00 4.40 4.73 4.96
True error -3.76 -3.84 -3.80 -3.85 -3.86 -3.87
Table 1: Generalization bounds on logarithmic scale w.r.t. MNIST using MLP of varying dimensions.
METHOD CIFAR10
Neyshabur 2015 33.19
Neyshabur 2017 30.10
Bartlett 2017 22.40
Ours 4.68
True error -2.41
Table 2: Comparison of different generalization bounds on the CIFAR10 dataset on a logarithmic
scale
0.000.250.500.751.00
0100 200 300
epochGeneralization ErrorSetting
Our bound (diff. scale)
True error
Figure 2: Comparing bounds on MNIST.
25
