# 2304.04947.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2304.04947.pdf
# Kích thước tệp: 3759848 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Conditional Adapters: Học chuyển giao hiệu quả tham số
với suy luận nhanh
Tao Lei*Junwen Bai Siddhartha Brahma Joshua Ainslie Kenton Lee Yanqi Zhou
Nan Du Vincent Y. Zhao Yuexin Wu Bo Li Yu Zhang Ming-Wei Chang
Google
Tóm tắt
Chúng tôi đề xuất Conditional Adapter (CODA), một phương pháp học chuyển giao hiệu quả tham số
cũng cải thiện hiệu quả suy luận. CODA tổng quát hóa vượt xa các phương pháp adapter tiêu chuẩn
để cho phép một cách mới cân bằng tốc độ và độ chính xác bằng cách sử dụng tính toán có điều kiện.
Bắt đầu với một mô hình tiền huấn luyện dày đặc hiện có, CODA thêm kích hoạt thưa thớt cùng với
một số lượng nhỏ tham số mới và một giai đoạn huấn luyện nhẹ. Các thí nghiệm của chúng tôi chứng minh
rằng phương pháp CODA cung cấp một cách hiệu quả bất ngờ để chuyển giao kiến thức. Trên nhiều
nhiệm vụ ngôn ngữ, thị giác và giọng nói khác nhau, CODA đạt được tăng tốc suy luận từ 2x đến 8x
so với các phương pháp Adapter tiên tiến với mức độ mất mát độ chính xác từ vừa phải đến không có
và cùng hiệu quả tham số.

1 Giới thiệu
Các mô hình tiền huấn luyện lớn đã đạt được kết quả đột phá nhưng trở ngại chính để triển khai
chúng là chi phí thích ứng và suy luận. Do kích thước ngày càng tăng của các mô hình tiền huấn luyện,
ví dụ, tinh chỉnh đã trở nên ngày càng đắt đỏ vì nó yêu cầu một bản sao riêng của
toàn bộ mô hình và cập nhật tất cả tham số cho mỗi nhiệm vụ downstream. Học chuyển giao hiệu quả tham số
như Adapter [Houlsby et al., 2019] và Prompt Tuning [Lester et al., 2021] đã được
đề xuất để giải quyết vấn đề này. Các phương pháp này chỉ cập nhật một tập con nhỏ tham số cho mỗi
nhiệm vụ downstream, cho phép mô hình giữ lại kiến thức và tránh quên thảm khốc [Vu et al.,
2022]. Đáng chú ý, các phương pháp này có thể khớp với độ chính xác của một mô hình được tinh chỉnh đầy đủ, trong khi đạt được
độ chính xác tốt hơn trên phân phối dữ liệu ngoài miền [Lester et al., 2021, Awadalla et al., 2022].
Thật không may, các phương pháp học chuyển giao hiệu quả tham số tiêu chuẩn chỉ mang lại hiệu quả tham số,
không phải hiệu quả suy luận. Ví dụ, trong khi chỉ một vài ma trận chiếu nhỏ được thêm vào
mô hình tiền huấn luyện trong phương pháp Adapter, tất cả đầu vào mô hình (như token) vẫn sử dụng tất cả tham số
trong quá trình suy luận. Do đó, tốc độ suy luận giống nhau (hoặc thấp hơn một chút) so với
phương pháp tinh chỉnh đầy đủ. Hơn nữa, các nghiên cứu trước đây đã chỉ ra rằng các phương pháp học hiệu quả tham số này
hiệu quả nhất khi kích thước của mô hình tiền huấn luyện lớn [Lester et al., 2021], làm cho
nhiều ưu điểm của các phương pháp này khó thực hiện trong thực tế.

Trong bài báo này, chúng tôi đề xuất Conditional Adapter (CODA), một phương pháp học chuyển giao hiệu quả tham số
cung cấp cả hiệu quả tham số và suy luận. CODA là một tổng quát hóa của phương pháp
adapter, được xây dựng với trực giác sau - chúng ta có thể coi mô hình tiền huấn luyện như một nguồn
kiến thức phổ quát nhưng chỉ truy vấn đối với nó cho các đầu vào cần thiết. Hình 1 so sánh CODA với
tinh chỉnh và các phương pháp adapter tiêu chuẩn. Tương tự như các phương pháp adapter tiêu chuẩn, mô hình của chúng tôi
thêm và cập nhật một adapter nhỏ trong mỗi lớp, trong khi cố định các khối Transformer tiền huấn luyện cho
thích ứng downstream. Tuy nhiên, khác với các phương pháp trước đây, CODA giả định rằng nhiều đầu vào

*Liên hệ: taole@google.com
Hội nghị lần thứ 37 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2023).arXiv:2304.04947v2 [cs.CL] 26 Nov 2023

--- TRANG 2 ---
Hình 1: So sánh giữa các cách khác nhau để sử dụng các mô hình Transformer tiền huấn luyện, bao gồm (1) tinh chỉnh tiêu chuẩn (trái) trong đó tất cả tham số có thể điều chỉnh và tính toán dày đặc, (2) adapter tiêu chuẩn (giữa) trong đó một tập nhỏ tham số mới có thể điều chỉnh được thêm vào trong khi tính toán vẫn dày đặc, và (3) CODA (phải) trong đó tính toán được kích hoạt thưa thớt.

[THIS IS TABLE: Comparison table showing performance metrics for different models across text, vision, and speech tasks]
Param mới | MNLI (văn bản) | OCR-VQA (thị giác) | Librispeech (giọng nói)
Acc↑ | Tăng tốc | EM↑ | Tăng tốc | WER↓ | Tăng tốc
P-Adapter 0.4% | 91.5 | 1.0x | 67.5 | 1.0x | 1.4/2.7 | 1.0x
CODA 0.4% | 90.7 | 3.2x | 67.6 | 8.0x | 1.4/2.8 | 2.2x

Bảng 1: CODA giảm đáng kể thời gian suy luận so với phương pháp Parallel Adapter [He et al., 2021], trong khi vẫn duy trì hiệu quả tham số.

biểu diễn token (của mỗi lớp) không quan trọng đối với nhiệm vụ dự đoán và do đó không yêu cầu tính toán nặng. Trong những trường hợp như vậy, khối Transformer tiền huấn luyện có thể được bỏ qua. Do nhiều token không được xử lý bởi khối Transformer, CODA chạy nhanh hơn đáng kể so với các phương pháp trước đây.

Trong khi kích hoạt có điều kiện có lợi ích tốc độ rõ ràng, CODA phải học cách chọn các token quan trọng cho tính toán nặng để duy trì độ chính xác mô hình. Để đạt được điều này, chúng tôi giới thiệu một phép toán soft top-k để tính toán quyết định lựa chọn token. Phép toán soft top-k này, có thể được xem như một tổng quát hóa của softmax và một sự nới lỏng của hard top-k, sử dụng các kỹ thuật tối ưu hóa được điều chỉnh entropy tương tự như tối ưu vận chuyển tính toán [Cuturi, 2013]. Kết quả là, đầu ra của nó có thể được tính toán bằng các vòng lặp nhanh và có thể vi phân, cho phép việc lựa chọn token được tối ưu hóa trực tiếp cho hiệu suất mô hình.

Chúng tôi áp dụng CODA trên các nhiệm vụ nặng về encoder và đánh giá hiệu quả của nó trên ba miền khác nhau - xử lý ngôn ngữ tự nhiên, thị giác máy tính và xử lý giọng nói. Nhìn chung, CODA đạt được tăng tốc suy luận từ 2 đến 8 lần so với phương pháp adapter tiêu chuẩn với mức mất mát độ chính xác từ vừa phải đến không có. Bảng 1 thể hiện kết quả của chúng tôi bằng cách chọn một trong những nhiệm vụ hoạt động tốt nhất trong mỗi miền. Chúng tôi cũng tiến hành các nghiên cứu ablation toàn diện để phân tích hiệu quả, hiệu suất và khả năng mở rộng của CODA. Ví dụ, chúng tôi phát hiện rằng chỉ với ít hoặc không có tiền huấn luyện router, các mô hình dày đặc tiền huấn luyện hiện có như T5 [Raffel et al., 2020] có thể được chuyển đổi hiệu quả thành các mô hình CODA để đạt được cả ưu điểm hiệu quả tham số và tốc độ.

2 Công việc liên quan
Các phương pháp học chuyển giao hiệu quả tham số Do số lượng tham số ngày càng tăng trong các mô hình Transformer tiền huấn luyện, nhiều phương pháp khác nhau đã được đề xuất cho học chuyển giao với cập nhật tham số tối thiểu. Prompt tuning [Lester et al., 2021] và prefix tuning [Li and Liang, 2021] giới thiệu các embedding token ảo mới có thể được tinh chỉnh như các tham số mô hình. Các phương pháp Adapter [Houlsby et al., 2019, He et al., 2021] thêm một số lượng nhỏ tham số mới, có thể học được vào mỗi lớp trong khi giữ các tham số tiền huấn luyện cố định. Một phương pháp phổ biến khác, Low-Rank Adaptation [LoRA; Hu et al., 2021], tiêm các ma trận phân tách thứ hạng thấp có thể học được vào các tham số mô hình tiền huấn luyện. Ngoài việc yêu cầu chi phí lưu trữ ít hơn, các phương pháp hiệu quả tham số đã được chỉ ra là hiệu quả mẫu hơn và đạt được khả năng tổng quát hóa ngoài miền tốt hơn so với tinh chỉnh tiêu chuẩn. CODA là một phương pháp adapter nhưng có thể dễ dàng kết hợp với các phương pháp hiệu quả tham số khác như LoRA để tăng tốc suy luận của chúng.

Tính toán có điều kiện Việc phát triển các mô hình được kích hoạt thưa thớt và có điều kiện đã là một lĩnh vực nghiên cứu rất tích cực. Ví dụ, các mô hình Mixture-of-Experts (MoE) [Shazeer et al.,

--- TRANG 3 ---
2017] và nhiều tiến bộ gần đây [Du et al., 2022, Fedus et al., 2021] đã được đề xuất để mở rộng quy mô kích thước của các mô hình ngôn ngữ mà không tăng chi phí tính toán. Nhiều công trình gần đây đã khám phá các phương pháp định tuyến token tốt hơn cho các mô hình MoE, ví dụ sử dụng băm ngẫu nhiên [Roller et al., 2021], phân công cân bằng [Lewis et al., 2021] và router chọn chuyên gia [Zhou et al., 2022]. CODA áp dụng tính toán có điều kiện cho cả khối attention và feed-forward của mô hình, trong khi các mô hình MoE chỉ tập trung vào kích hoạt thưa thớt trong các khối feed-forward.

Tương tự như phương pháp của chúng tôi, nhiều phương pháp gần đây khác nhau đã đạt được hiệu quả tính toán bằng cách bỏ qua tính toán trên một tập con của các token đầu vào. Tuy nhiên, cơ chế lựa chọn có thể rất khác nhau, như sử dụng pooling [Nawrot et al., 2022], token merging [Bolya et al., 2023], token pruning [Rao et al., 2021, Yin et al., 2022], learned sigmoid gates [Bapna et al., 2020] và early exiting [Schuster et al., 2022]. Trong khi hầu hết các phương pháp token merging và pruning đã được đề xuất cho các nhiệm vụ thị giác, chúng tôi chỉ ra rằng CODA có thể áp dụng cho nhiều miền bao gồm văn bản, thị giác và giọng nói. Ngoài ra, token merging và phương pháp lựa chọn token của chúng tôi được xây dựng với các bias quy nạp và trực giác khác nhau. Token merging tận dụng sự dư thừa trong các token thị giác, trong khi lựa chọn token giả định một sự đột biến về mức độ liên quan của token. Đó là, chỉ một vài token là cần thiết cho nhiệm vụ dự đoán. Một khác biệt chính khác là CODA định tuyến động và cập nhật các biểu diễn token trong mỗi lớp, trong khi nếu một token bị pruned (hoặc merged), nó sẽ không bao giờ được sử dụng lại bởi các lớp tiếp theo. Chúng tôi tin rằng cơ chế định tuyến token của chúng tôi phù hợp hơn cho các ứng dụng văn bản và giọng nói, như trả lời câu hỏi, nơi các token khác nhau có thể đóng vai trò quan trọng trong các lớp khác nhau, hoặc đối với các truy vấn đầu vào khác nhau.

Cuối cùng, CODA có liên quan chặt chẽ đến một công trình đồng thời, CoLT5 [Ainslie et al., 2023], cũng sử dụng kích hoạt có điều kiện (lựa chọn token) để đạt hiệu quả suy luận. Trọng tâm của CoLT5 và CODA rất khác nhau. CoLT5 cụ thể điều chỉnh kiến trúc mô hình của nó cho văn bản dài (ví dụ, hơn 16k token), ví dụ, bằng cách kết hợp attention cục bộ với routed attention. Các mô hình CoLT5 được tiền huấn luyện từ đầu và tất cả tham số được tinh chỉnh cho các nhiệm vụ downstream. Ngược lại, CODA được khởi tạo và thích ứng trực tiếp từ một mô hình dày đặc đã được tiền huấn luyện, và chúng tôi tối ưu hóa hiệu suất của nó trên học chuyển giao hiệu quả tham số. Điểm mạnh của CODA và CoLT5 có thể được kết hợp cho các ứng dụng văn bản dài.

Các mô hình Transformer hiệu quả Nhiều biến thể Transformer hiệu quả đã được đề xuất để tăng tốc tính toán mô hình. Ví dụ bao gồm tạo ra các biến thể attention nhanh [Wang et al., 2020a, Beltagy et al., 2020, Guo et al., 2022, Hua et al., 2022], tìm kiếm kiến trúc mạng [Press et al., 2019, So et al., 2021, Su et al., 2021] và sử dụng các mô-đun thần kinh không-attention cho hiệu quả [Gulati et al., 2020, Lei, 2021]. CODA sử dụng tính toán có điều kiện như một phương pháp trực giao cho hiệu quả.

Nén mô hình Ngoài việc xây dựng các kiến trúc mô hình hiệu quả, các phương pháp nén mô hình như pruning [Han et al., 2016, Zhu and Gupta, 2017, Wang et al., 2020b, Xia et al., 2022] và distillation [Hinton et al., 2015, Kim and Rush, 2016, Turc et al., 2019, Lin et al., 2020] có thể được áp dụng để tăng tốc suy luận mô hình. So với các phương pháp này, CODA giữ lại tất cả tham số mô hình của mô hình lớn tiền huấn luyện, và do đó tránh huấn luyện lại một mô hình mới từ đầu hoặc quên kiến thức do loại bỏ tham số gây ra. Ngoài ra, CODA có thể được xem như một phiên bản động của layer pruning vì nó có thể kích hoạt các lớp Transformer khác nhau cho mỗi token, và có thể được kết hợp thêm với distillation để giảm mất mát độ chính xác do tính toán có điều kiện gây ra.

3 Phương pháp
3.1 Kiến trúc
Trong suốt phần này và phần thí nghiệm, chúng tôi xây dựng CODA trên các parallel adapter [He et al., 2021]. Tuy nhiên, lưu ý rằng phương pháp của chúng tôi có thể được tổng quát hóa cho các loại adapter khác như sequential adapter [Houlsby et al., 2019] và LoRA [Hu et al., 2021]. Chúng tôi trình bày các kết quả thí nghiệm bổ sung sử dụng LoRA trong Phụ lục B.3. Hình 2 minh họa kiến trúc của chúng tôi và cho thấy CODA tính toán đầu ra của nó bằng cách chỉ chọn một tập con nhỏ các token đầu vào để truy vấn đối với mô hình tiền huấn luyện. Khi parallel adapter được sử dụng, CODA giới thiệu một số lượng nhỏ tham số có thể học được trong các nhánh song song, trong khi phần lớn tham số mô hình (liên quan đến các lớp Transformer tiền huấn luyện) vẫn cố định. Ngoài ra, CODA chỉ gửi k=⌈n/r⌉ token để xử lý nặng. Chúng tôi định nghĩa r >1 như hệ số giảm, một hằng số (như 4) để kiểm soát việc tiết kiệm tính toán.

--- TRANG 4 ---
Tiếp theo, chúng tôi giới thiệu ngắn gọn các ký hiệu và mô tả tính toán của CODA một cách chi tiết. Chúng tôi sử dụng F() để biểu thị một mạng thần kinh được tham số hóa và hàm tương ứng được định nghĩa bởi mạng. Ví dụ, một lớp Transformer [Vaswani et al., 2017] bao gồm một lớp con attention Fatt() tiếp theo là một lớp con feed forward Fffn(). Mỗi lớp cũng sử dụng layer normalization [Ba et al., 2016], cụ thể là LNatt() và LNffn(), trước khi áp dụng các hàm attention và feed forward. Chúng tôi định nghĩa X∈Rn×d như đầu vào của một lớp encoder Transformer, trong đó n là số token đầu vào và d là kích thước ẩn của mô hình.

[THIS IS FIGURE: Hình 2 cho thấy sơ đồ của một lớp CODA đơn với parallel adapter. k token được chọn và xử lý bởi lớp Transformer tiền huấn luyện đã đóng băng, và tất cả token được xử lý bởi lớp adapter nhanh.]

Với đầu vào lớp X, đầu tiên chúng ta áp dụng layer normalization, cụ thể là Xnorm=LNatt(X). Đầu vào đã chuẩn hóa sẽ được xử lý bởi nhánh adapter và nhánh Transformer có điều kiện. Đầu ra của chúng sau đó được cộng và kết hợp như đầu ra cuối cùng của lớp.

Nhánh Adapter Gọi Fadapter() là hàm biến đổi của nhánh adapter. Đầu ra được định nghĩa là
Zadapter = Fadapter(Xnorm) (1)

Tương tự như các phương pháp trước đây, Fadapter() được thực hiện bằng một mạng feed forward với kích thước ẩn nhỏ như 64. Kết quả là, việc tính toán Zadapter chỉ phát sinh một số lượng nhỏ phép toán dấu phẩy động và chi phí của nó thường không đáng kể so với chi phí của nhánh Transformer nặng. Nhánh adapter không chọn token có điều kiện. Nói cách khác, Fadapter() được áp dụng cho tất cả token đầu vào X∈Rn×d.

Nhánh có điều kiện Việc tính toán của nhánh có điều kiện gồm ba bước. Đầu tiên, mỗi lớp CODA định nghĩa một hàm router Frouter() để chọn k token cho nhánh có điều kiện. Hàm router trong mỗi lớp trả về hai đầu ra

m,P = Frouter(Xnorm) (2)

trong đó P∈{0,1}k×n là ma trận gồm k vector one-hot chỉ ra việc lựa chọn token. Ở đây P[i,j] = 1 khi và chỉ khi token được chọn thứ i là token thứ j từ X̃. m∈[0,1]n là một mask trọng số trong đó m[j] là trọng số lựa chọn cho token thứ j. m[j] = 0 nếu token không được chọn. Chúng tôi sẽ mô tả cách router học việc lựa chọn chi tiết hơn sau trong phần này.

Sau khi quyết định định tuyến được đưa ra, các biểu diễn đầu vào của các token được chọn có thể được thu thập bằng phép nhân ma trận,

Xrouted = PXnorm ∈ Rk×d (3)

trong đó k hàng trong Xnorm được chọn để xây dựng ma trận k-by-d Xrouted. Tương tự như một lớp Transformer tiêu chuẩn, nhánh có điều kiện áp dụng các biến đổi attention và feed forward cho đầu vào được chọn:

Z̄routed = Fatt(Xrouted) (4)
Zrouted = Fffn(LNffn(Xrouted + Z̄routed)) (5)

trong đó Z̄routed, Zrouted ∈ Rk×d biểu thị đầu ra của mạng attention và mạng feed forward tương ứng.

Chúng tôi xem xét hai biến thể attention khác nhau về cách chúng tính toán các vector key-value. Một biến thể áp dụng attention k-to-k sử dụng Xrouted như cả query vector và key-value vector. Biến thể khác áp dụng attention k-to-all sử dụng toàn bộ vector đầu vào Xnorm như attention key và value. Biến thể k-to-all chạy chậm hơn nhưng đạt được chất lượng cao gần với mô hình đầy đủ. Chúng tôi so sánh hiệu suất của hai biến thể trong Phần 5.

Đầu ra attention và feed-forward Z̄routed và Zrouted được kết hợp và chiếu ngược về cùng hình dạng của đầu vào gốc

Zcond = P⊤(Z̄routed + Zrouted) ∈ Rn×d (6)

--- TRANG 5 ---
Cuối cùng Zcond hợp nhất với đầu ra adapter và đầu vào gốc của lớp hiện tại để tạo ra đầu ra của lớp:

Y = X + Zadapter + m ⊙ Zcond (7)

m ⊙ Zcond là một phép nhân theo từng phần tử để chia tỷ lệ các hàng của Zcond bằng trọng số m. Phép toán này có thể được xem như một phép toán cổng, trong đó trạng thái ẩn Zcond[i] của token thứ i được chia tỷ lệ bởi điểm số lựa chọn token m[i] được gán bởi router. Điều này cho phép lan truyền gradient từ m đến các tham số router, sao cho việc lựa chọn token có thể được tối ưu hóa cùng với các thành phần mô hình khác trong quá trình huấn luyện.

Router được học Một thành phần quan trọng của CODA là hàm router Frouter() được học để chọn một tập con token cho hiệu suất mô hình thuận lợi. Với biểu diễn token Xnorm, router của chúng tôi đầu tiên tính toán điểm số dot-product s = wX⊤norm, trong đó w∈Rd là một vector tham số liên quan đến router trong lớp này. Điểm số dot-product s được chuẩn hóa thêm bởi một hàm f(): Rn → [0,1]n, và được cắt để tạo ra điểm số lựa chọn m:

λ = f(s) (8)
m = λ ⊙ Top(λ, k) ∈ Rn (9)

Ở đây Top(λ, k) ∈ {0,1}n là một hàm chỉ báo trả về một mask nhị phân chỉ ra k giá trị cao nhất trong λ. Ma trận one-hot P được định nghĩa trong (2) có thể được tạo ra theo Top(λ, k). Tóm lại, các giá trị cao nhất của λ sẽ được chọn bởi router.

Hàm f() phải giữ được tính khả vi đối với đầu vào của nó (s trong trường hợp này) sao cho chúng ta có thể cập nhật các tham số router w trong quá trình huấn luyện. Một lựa chọn có thể cho f() là hàm kích hoạt sigmoid chuẩn hóa các giá trị trong s một cách độc lập. Tuy nhiên, điều này không mô hình hóa một cách rõ ràng ràng buộc rằng chúng ta cần chọn k token từ n token có sẵn. Xem xét một trường hợp đơn giản trong đó k = 1, một lựa chọn tự nhiên cho f() sẽ là hàm softmax. Vì softmax cung cấp chuẩn hóa toàn cục trên các điểm số đầu vào, một cập nhật gradient để tăng một trong các điểm số cũng sẽ giảm các điểm số khác, một hiệu ứng mong muốn để học lựa chọn top-1.

Chúng tôi giả thuyết rằng một toán tử soft top-k tổng quát hóa softmax nên được sử dụng cho k > 1 tổng quát. Điều này thực sự có thể bằng cách chính thức hóa soft top-k như bài toán tối ưu hóa sau:

f(s) := arg maxλ s⊤λ + εH(λ)
s.t. 1⊤λ = k, λ[i] ∈ [0,1] for all i = 1, ..., n (10)

Ở đây H(λ) = Σni=1 -λ[i] log λ[i] là một hàm entropy tổng quát (áp dụng cho bất kỳ vector dương λ thay vì một phân phối), và ε > 0 là một hệ số nhỏ.

Bài toán tối ưu hóa này liên quan chặt chẽ đến softmax và phép toán top-k. Cụ thể, khi ε = 0, nó trở thành một chương trình tuyến tính trả về Top(s, k) như lời giải. Ngoài ra, khi k = 1, có thể chỉ ra rằng lời giải của nó là softmax(s/ε). Nói chung, (10) sẽ trả về một mask soft top-k và độ mượt được kiểm soát bởi ε (và do đó ε phải dương để hoạt động như nhiệt độ).

Bài toán (10) không có lời giải dạng đóng cho ε > 0 và k > 1 tùy ý, nhưng lời giải của nó có thể được thu được bằng một thuật toán lặp. Cụ thể, gọi a∈Rn và b∈Rn là hai biến phụ (có thể được khởi tạo về không). Lời giải có dạng λ = exp((s+b+a)/ε). Các giá trị của a và b có thể được thu được bằng các cập nhật lặp sau:

a' = ε ln(k) - ε ln(Σni=1 exp((s[i] + b[i])/ε))
b' = min(-s - a', 0) (11)

Trong thực tế, chúng tôi sử dụng T = 20 lần lặp và hàm f(s) trả về exp((s+b+a)/ε) sử dụng a và b từ lần lặp cuối cùng. Hàm f(s) vẫn khả vi đối với s bằng các cập nhật lặp này, vì vậy chúng ta có thể huấn luyện router cùng với các tham số mô hình khác. Chúng tôi cung cấp thảo luận bổ sung và sự dẫn xuất của các cập nhật trong Phụ lục §C.

--- TRANG 6 ---
[THIS IS TABLE: Table 2 showing results of applying CODA to T5 v1.1 models with different configurations and accuracy metrics across various tasks]

Bảng 2: Kết quả áp dụng CODA cho các mô hình T5 v1.1. CODA đạt được tiết kiệm tính toán đáng kể trong khi vẫn giữ độ chính xác gần với baseline dày đặc. Chúng tôi so sánh CODA với phương pháp parallel adapter tương ứng xử lý tất cả token mà không có tính toán có điều kiện. Chúng tôi báo cáo độ chính xác trên tập phát triển trên 3 nhiệm vụ × 3 kích thước mô hình, và đặt số token được chọn k=⌈n/r⌉. Cột cuối cùng cho thấy thay đổi về độ chính xác trung bình so với phương pháp parallel adapter. Chúng tôi chọn phiên bản k-to-all làm mặc định (được hiển thị in đậm).

3.2 Huấn luyện
CODA có thể được khởi tạo trực tiếp từ một mô hình Transformer hiện có. Với một mô hình tiền huấn luyện như T5 [Raffel et al., 2020], các lớp Transformer được sử dụng lại và sao chép trực tiếp trong các nhánh có điều kiện của CODA, và chỉ các tham số adapter và router được khởi tạo ngẫu nhiên. Vì tiền huấn luyện một mô hình dày đặc lớn có thể tốn kém, phương pháp của chúng tôi giảm chi phí huấn luyện tổng thể.

Các router và thành phần mạng thần kinh trong CODA phải hợp tác và được tối ưu hóa để dự đoán mô hình chính xác. Khi dữ liệu tinh chỉnh có sẵn hạn chế, việc khởi tạo ngẫu nhiên cho các tham số router (và adapter) có thể không tối ưu. Chúng tôi chứng minh rằng CODA có thể được tiền huấn luyện thêm bằng cùng mục tiêu tiền huấn luyện như mô hình dày đặc, để nâng cao hiệu suất downstream. Quan trọng là, CODA yêu cầu ít bước huấn luyện hơn đáng kể trong quá trình tiền huấn luyện, vì hầu hết các tham số của nó được lấy từ một mô hình đã được tiền huấn luyện. Chúng tôi chỉ ra rằng chi phí tiền huấn luyện CODA có thể thấp hơn 10-30 lần so với tiền huấn luyện mô hình dày đặc gốc. Chúng tôi trình bày phân tích này trong Phần 5.

Cuối cùng, chúng tôi huấn luyện CODA trên các nhiệm vụ downstream bằng cách chỉ cập nhật các tham số adapter, router và layer normalization. Kích thước của các adapter nhỏ (ví dụ 5M tham số), và mỗi khối router và layer normalization chỉ giới thiệu d tham số, trong đó d là chiều mô hình. Kết quả là, CODA vẫn hiệu quả tham số tương tự như các phương pháp adapter và prompt-tuning trước đây.

4 Thiết lập thí nghiệm
CODA được đánh giá trên ba miền bao gồm xử lý ngôn ngữ tự nhiên (NLP), thị giác máy tính và xử lý giọng nói, và trên một loạt các ứng dụng như phân loại, trả lời câu hỏi, tóm tắt và nhận dạng giọng nói. Các thí nghiệm được tổ chức như sau: Đầu tiên chúng tôi chứng minh hiệu quả của CODA và tiến hành phân tích về các lựa chọn thiết kế của nó bằng các mô hình T5 có sẵn công khai (§5). Trong kết quả cuối cùng của chúng tôi (§6), chúng tôi tiền huấn luyện các mô hình Transformer từ đầu và mở rộng đánh giá của chúng tôi sang các miền thị giác và giọng nói.

Tập dữ liệu Chúng tôi sử dụng corpus C4 [Raffel et al., 2020] để tiền huấn luyện các mô hình văn bản. Đối với các mô hình giọng nói, chúng tôi sử dụng corpus LibriLight [Kahn et al., 2020] để tiền huấn luyện. Các mô hình Transformer thị giác của chúng tôi sử dụng cùng dữ liệu và quy trình huấn luyện trong Pix2Struct [Lee et al., 2022]. Các tập dữ liệu tinh chỉnh của chúng tôi cho các mô hình văn bản bao gồm các tập dữ liệu MNLI [Williams et al., 2018], RTE [Dagan et al., 2005, Haim et al., 2006, Giampiccolo et al., 2007, Bentivogli et al., 2009], BoolQ [Clark et al., 2019], SQuAD [Rajpurkar et al., 2016] và XSum [Narayan et al., 2018]. Các mô hình giọng nói được đánh giá trên nhiệm vụ nhận dạng giọng nói bằng tập dữ liệu LibriSpeech [Panayotov et al., 2015]. Cuối cùng, chúng tôi sử dụng các tập dữ liệu OCR-VQA [Mishra et al., 2019], DocVQA [Mathew et al., 2021], và Screen2Words [Wang et al., 2021] cho các mô hình thị giác.

--- TRANG 7 ---
[THIS IS TABLE: Table 3 showing ablation study on routing methods with different configurations and accuracy scores across tasks]

Bảng 3: Nghiên cứu ablation về các phương pháp định tuyến. Chúng tôi sử dụng biến thể CODA k-to-k để so sánh công bằng với phương pháp truncation. Phương pháp định tuyến tốt hơn mang lại độ chính xác tốt hơn trên các nhiệm vụ và kích thước mô hình khác nhau được thử nghiệm. Chúng tôi sử dụng soft top-k làm phương pháp mặc định.

5 Hiểu và Phân tích CODA

Thiết lập Chúng tôi trình bày một số phân tích để xác thực các lựa chọn thiết kế của CODA trong phần này. Chúng tôi khởi tạo CODA bằng bản phát hành version 1.1 của các checkpoint T5², và thực hiện tiền huấn luyện CODA bằng cùng thiết lập như các mô hình T5. Trong quá trình tiền huấn luyện, chúng tôi đặt capacity định tuyến k = 192 với độ dài chuỗi đầu vào n = 512. Chúng tôi không điều chỉnh giá trị k cho tiền huấn luyện, nhưng sẽ báo cáo kết quả sử dụng các giá trị k khác nhau trong tinh chỉnh. Chúng tôi thực hiện 100K bước gradient, tương đương 10% tổng số bước được sử dụng để huấn luyện các mô hình T5 dày đặc. Chi phí tính toán tổng thể thấp hơn 20 lần so với huấn luyện đầy đủ các mô hình dày đặc, vì CODA chỉ áp dụng tính toán nặng trên ít hơn một nửa số token.

Để đơn giản, chúng tôi đánh giá trên các nhiệm vụ phân loại cho các nghiên cứu ablation khác nhau của CODA. Cụ thể, chúng tôi báo cáo kết quả trên các tập dữ liệu MNLI, RTE và BoolQ, và thử nghiệm ba kích thước mô hình khác nhau bao gồm kích thước Base, Large và XL của T5. Chúng tôi sẽ mở rộng đánh giá của mình sang các nhiệm vụ generation như trả lời câu hỏi trong phần kết quả đầy đủ (§6).

CODA có thể nhanh và chính xác không? Bảng 2 trình bày kết quả tinh chỉnh của CODA. Để so sánh, chúng tôi cũng báo cáo kết quả của Parallel Adapter, tương tự như CODA ngoại trừ việc nó áp dụng các lớp Transformer đắt đỏ cho tất cả token đầu vào. Điều này tạo thành một upper-bound, và là một baseline mạnh đã được báo cáo là tốt nhất trong số một loạt các phương pháp adapter và prompt tuning [He et al., 2021]. Như được hiển thị trong Bảng 2, CODA có thể đạt được giảm tính toán 3-5x (r = 3,5) trong các lớp Transformer với chi phí là giảm ít hơn 1.0 điểm về độ chính xác trung bình. Như mong đợi, biến thể k-to-all attention của chúng tôi đạt được độ chính xác tốt hơn một cách nhất quán so với biến thể k-to-k, vì nó có thể truy cập ngữ cảnh attention đầy đủ. Mặt khác, biến thể k-to-k attention chạy nhanh hơn trong thực tế, có thể có lợi cho các nhiệm vụ với đầu vào rất dài. Chúng tôi chọn phiên bản k-to-all trong phần kết quả cuối cùng (§6).

Cần bao nhiêu bước tiền huấn luyện? Hình 3 vẽ đồ thị độ chính xác tinh chỉnh bằng cách thay đổi số bước tiền huấn luyện cho CODA. Vì CODA có thể được khởi tạo bằng các mô hình dày đặc tiền huấn luyện, nó yêu cầu ít nhất 20K bước để có được kết quả tinh chỉnh cạnh tranh. Tất nhiên, sử dụng nhiều bước tiền huấn luyện hơn có thể cải thiện độ chính xác downstream. Thực tế là CODA có thể được cập nhật nhanh chóng mà không lặp lại việc tiền huấn luyện đắt đỏ sẽ rất có lợi trong các ứng dụng thực tế.

Việc định tuyến được học có quan trọng không? Chúng tôi phân tích tác động của việc định tuyến được học trong Bảng 3 bằng cách so sánh router soft top-k của chúng tôi với các triển khai router khác. Chúng tôi triển khai một biến thể thay thế soft top-k bằng hàm kích hoạt sigmoid, vì vậy trọng số lựa chọn của mỗi token kích hoạt riêng (mà không xem xét ràng buộc capacity). Như được hiển thị trong bảng, biến thể này

²https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511

--- TRANG 8 ---
[THIS IS TABLE: Table 4 comparing CODA and parallel adapter on 6 language tasks, showing parameters, reduction factors, and performance metrics]

Bảng 4: So sánh CODA và parallel adapter trên 6 nhiệm vụ ngôn ngữ. Chúng tôi báo cáo kết quả trên tập test của XSum, và trên tập phát triển của các nhiệm vụ khác. †chỉ ra kết quả được lấy từ He et al. [2021], và các kết quả tham chiếu trong ngoặc tương ứng với việc sử dụng 2M tham số adapter. Lưu ý rằng các số Parallel Adapter của chúng tôi mạnh hơn vì backbone Transformer tiền huấn luyện của chúng tôi sử dụng nhiều tham số hơn so với mô hình được sử dụng trong He et al. [2021].

[THIS IS FIGURE: Figure 4 showing average finetuning scores of Parallel Adapter and CODA at different model sizes]

Hình 4: Điểm số tinh chỉnh trung bình của Parallel Adapter và CODA ở các kích thước mô hình khác nhau.

[THIS IS FIGURE: Figure 5 showing scaling of CODA on XSum and LibriSpeech datasets with speed vs quality trade-offs]

Hình 5: Việc mở rộng quy mô của CODA trên tập dữ liệu XSum và LibriSpeech. Trái: CODA đạt được trade-off tốc độ-chất lượng tốt hơn so với tinh chỉnh adapter với các mô hình nhỏ hơn, trên tập dữ liệu XSum. Giữa: mô hình CODA lớn hơn đạt được tăng tốc cao hơn. Phải: CODA đạt được trade-off tốc độ-chất lượng tốt hơn so với baseline dày đặc trên tập dữ liệu LibriSpeech.

đạt được độ chính xác tệ hơn trên hầu hết tất cả các nhiệm vụ và kích thước mô hình được thử nghiệm, tệ hơn 2.0 điểm trung bình. Chúng tôi cũng triển khai một baseline "không học" chỉ đơn giản chọn k token đầu tiên, tương đương với việc cắt ngắn chuỗi đầu vào.³ Baseline này hoạt động tệ hơn nhiều, dẫn đến giảm hơn 10 điểm độ chính xác cho k nhỏ (và tương đương r lớn). Phân tích này xác nhận tầm quan trọng của việc học một định tuyến tốt để giữ lại hiệu suất mô hình mạnh.

6 Kết quả Đầy đủ

Thiết lập Trong phần này, chúng tôi áp dụng công thức huấn luyện tốt nhất của chúng tôi cho tất cả các nhiệm vụ và miền ứng dụng. Đầu tiên chúng tôi tiền huấn luyện các mô hình Transformer dày đặc, tiếp theo là quy trình huấn luyện CODA trong §3.2. Các mô hình giọng nói của chúng tôi được tiền huấn luyện bằng mục tiêu masked language modeling (MLM) tương tự BERT [Devlin et al., 2019], và không gian nhãn đầu ra được lượng tử hóa ngẫu nhiên [Chiu et al., 2022]. Các mô hình thị giác và văn bản của chúng tôi sử dụng kiến trúc encoder-decoder tương tự T5 nhưng kết hợp một vài thay đổi. Theo PaLM [Chowdhery et al., 2022], chúng tôi sử dụng multi-query attention [Shazeer, 2019] chia sẻ cùng chiếu key và value cho nhiều query head. Chúng tôi chỉ sử dụng 6 lớp decoder và tăng kích thước ẩn feed forward (để bù đắp cho việc giảm số lượng lớp). Những sửa đổi này có hiệu ứng trung tính đối với chất lượng mô hình, nhưng tăng tốc đáng kể việc giải mã auto-regressive. Chúng tôi sẽ chỉ ra CODA tương thích với những thay đổi này và có thể tăng tốc suy luận thêm một hệ số lớn đáng kể. Chúng tôi cung cấp thêm chi tiết về thiết lập thí nghiệm trong Phụ lục A.

Kết quả NLP Ngoài các tập dữ liệu phân loại được sử dụng trong Phần 5, chúng tôi cũng đánh giá các mô hình cuối cùng trên các tập dữ liệu SQuAD, ReCord và XSum yêu cầu tạo ra một câu trả lời hoặc tóm tắt

³Chúng tôi luôn bao gồm văn bản câu hỏi cho BoolQ, để đạt được độ chính xác cao hơn.

--- TRANG 9 ---
[THIS IS TABLE: Table 5 comparing CODA and parallel adapter baselines on Librispeech, showing WER results]

Bảng 5: So sánh CODA và các baseline parallel adapter trên Librispeech. Chúng tôi báo cáo kết quả WER trên test-clean và test-other. Có thể tìm thêm kết quả trong §B.2.

[THIS IS TABLE: Table 6 comparing CODA and parallel adapter applied to Pix2Struct model on 3 vision tasks]

Bảng 6: So sánh CODA và parallel adapter được áp dụng cho mô hình Pix2Struct tiền huấn luyện [Lee et al., 2022] trên 3 nhiệm vụ hiểu ngôn ngữ tình huống thị giác.

cho đầu vào. Bảng 4 chứa kết quả tinh chỉnh của các mô hình XL. So với baseline parallel adapter sử dụng tính toán đầy đủ, CODA đạt được giảm tính toán 3x và 5x chỉ với mất mát 1.0 và 1.7 điểm trong điểm số trung bình.

Hình 4 và 5 làm nổi bật xu hướng mở rộng quy mô của CODA. CODA chạy nhanh hơn nhiều với chất lượng hơi tệ hơn so với baseline parallel adapter. Điều này được mong đợi vì baseline xử lý tất cả token trong mọi lớp, trong khi CODA chỉ chọn 1/r token để xử lý nặng. Quan trọng là, khoảng cách chất lượng này giảm khi kích thước mô hình tăng (như được hiển thị trong Hình 4), làm cho CODA trở thành lựa chọn hiệu quả tính toán cho các mô hình lớn. Thực vậy, CODA có thể đánh đổi chất lượng lấy tốc độ bằng cách thay đổi số lượng token được chọn. Hình 5 (trái) chứng minh rằng CODA đạt được trade-off tốc độ-chất lượng mạnh hơn nhiều so với các mô hình dày đặc không có tính toán có điều kiện. Đường đen chỉ ra kết quả của Parallel Adapter khi kích thước mô hình tăng từ Small đến XL, và mỗi đường xanh biểu diễn trade-off tốc độ-chất lượng của CODA sử dụng r = 1,3,5. Hơn nữa, Hình 5 (giữa) cho thấy các mô hình CODA lớn hơn thể hiện tăng tốc suy luận cao hơn. Những quan sát này nhất quán trên các nhiệm vụ khác. Chúng tôi cung cấp kết quả bổ sung trong Phụ lục §B.

Kết quả nhận dạng giọng nói Chúng tôi xác thực thêm hiệu suất của CODA trong miền giọng nói. Mô hình của chúng tôi sử dụng một Transformer encoder và một LSTM Transducer 2 lớp [Graves, 2012]. Tương tự như thiết lập NLP, chúng tôi thử nghiệm hiệu suất của mô hình giọng nói trên 3 quy mô - Base, Large và XL (xem Phụ lục A để biết chi tiết). Bảng 5 chứng minh rằng với tỷ lệ giảm đáng kể (r = 2,4), thay đổi về tỷ lệ lỗi từ (WER) nhất quán là tối thiểu trên các tập test-clean và test-other của LibriSpeech qua các kích thước mô hình khác nhau (và trên các tập khác trong §B.2). Hơn nữa, kết quả của chúng tôi có thể so sánh với các mô hình hiệu suất cao nhất, như w2v-BERT [Chung et al., 2021] và BEST-RQ [Chiu et al., 2022], được tinh chỉnh đầy đủ bằng cách cập nhật tất cả tham số. Hình 5 (phải) nhấn mạnh một lần nữa rằng việc áp dụng tính toán có điều kiện dẫn đến trade-off tốc độ-chất lượng tốt hơn so với các mô hình dày đặc.

Kết quả thị giác Chúng tôi mở rộng thí nghiệm sang các nhiệm vụ thị giác liên quan đến ngôn ngữ tự nhiên trong hình ảnh, như tài liệu và giao diện người dùng. Thí nghiệm của chúng tôi dựa trên Pix2Struct [Lee et al., 2022], trong đó một image-encoder-text-decoder được tiền huấn luyện bằng cách học dự đoán HTML đơn giản từ ảnh chụp màn hình trang web. Bảng 6 cho thấy kết quả trên ba nhiệm vụ cũng được đánh giá trong bài báo Pix2Struct gốc. Trong OCRVQA và Screen2Words, chúng tôi quan sát việc giảm hiệu suất tương đối nhỏ khi giảm số lượng token được định tuyến (tức các patch). Khi capacity là 1/16 của độ dài chuỗi gốc, dẫn đến tăng tốc khoảng 13×, chúng tôi chỉ mất khoảng 1 điểm. Chúng tôi suy đoán rằng điều này do sự thưa thớt cấp cao trong đầu vào cho hai nhiệm vụ này. Đối với DocVQA, nơi có tương đối nhiều thông tin văn bản hơn, chúng tôi quan sát trade-off hiệu suất-tốc độ dốc hơn nhưng vẫn đạt được tăng tốc 8× với giảm 4 điểm.

Để cung cấp hiểu biết trực quan hơn về lý do CODA hoạt động, chúng tôi hình dung hành vi router cho mô hình OCR-VQA trong Hình 6. Chúng tôi hiển thị các patch mà router ưa thích nhất (màu ấm nhất) và ít nhất (màu mát nhất), cho một số lớp. Quan sát đầu tiên, ngay lập tức rõ ràng, là router tránh các patch tần số thấp, tức là các patch có khả năng là "khoảng trắng", vì chúng có thể được xử lý đầy đủ bởi các lớp adapter rẻ. Quan sát thứ hai, tinh tế hơn, là router dần dần hội tụ trên một số lượng nhỏ các patch chính mà chúng tôi giả thuyết phục vụ như biểu diễn cho các vùng lớn hơn. Việc hình dung xác nhận rằng CODA có thể chọn các patch có ý nghĩa và đại diện hữu ích cho nhiệm vụ dự đoán.

--- TRANG 10 ---
Hình 6: Hình dung sở thích định tuyến cho mô hình CODA được áp dụng cho nhiệm vụ OCR-VQA. Màu ấm và màu mát hơn biểu thị điểm số cao hơn và thấp hơn tương ứng. Router ưa thích độ phủ đa dạng trong các lớp đầu, nhưng hội tụ để chọn các patch thưa thớt và đại diện trong các lớp sau.

7 Kết luận và Hạn chế

Chúng tôi trình bày CODA, một phương pháp adapter hiệu quả tham số cho phép suy luận nhanh. CODA dựa vào tính toán có điều kiện để chọn lọc kích hoạt tính toán mô hình trên các đơn vị đầu vào quan trọng, cung cấp một cách mới để cân bằng tính biểu đạt và hiệu quả của mô hình.

Trong công trình này, chúng tôi tập trung vào các ứng dụng nặng về encoder như tóm tắt, nhận dạng giọng nói và trả lời câu hỏi thị giác, bằng cách áp dụng phương pháp của chúng tôi cho encoder. Một hạn chế của CODA là cơ chế định tuyến hiện tại (tức lựa chọn token trong một chuỗi cho trước) không thể áp dụng trực tiếp cho các mô hình decoder-only để tạo token auto-regressive. Việc cho phép tạo token nhanh bằng kích hoạt có điều kiện trong các lớp decoder là một hướng thú vị mà chúng tôi dự định khám phá trong công việc tương lai.

8 Lời cảm ơn

Chúng tôi muốn cảm ơn Rama Pasumarthi, Hongkun Yu, Kelvin Guu, Zhuyun Dai, Timothy Dozat, Raphael Hoffmann, Tao Wang, Tal Schuster, Ziwei Ji, Frederick Liu và Slav Petrov vì lời khuyên và thảo luận hữu ích.

Tài liệu tham khảo
[Phần tài liệu tham khảo được giữ nguyên như trong bản gốc, bao gồm tất cả các trích dẫn từ trang 10-15]

--- TRANG 11 ---
[Tiếp tục phần tài liệu tham khảo từ trang 11-16]

--- TRANG 16 ---
A Chi tiết thí nghiệm

Triển khai mô hình Đối với các thí nghiệm văn bản và thị giác, chúng tôi triển khai các mô hình bằng JAX [Bradbury et al., 2018]. Cụ thể, các mô-đun huấn luyện và mô hình của chúng tôi được xây dựng trên framework T5X, Flax và Flaxformer [Roberts et al., 2022, Heek et al., 2020]. Theo triển khai T5 v1.1 và PaLM [Chowdhery et al., 2022], các mô hình Transformer của chúng tôi sử dụng biến thể GLU [Shazeer, 2020] như mạng feed forward và multi-query-attention [Shazeer, 2019] như khối attention. Những sửa đổi này được chỉ ra là nâng cao khả năng mô hình hóa và tăng tốc giải mã tương ứng.

Đối với các thí nghiệm giọng nói, chúng tôi sử dụng TensorFlow [Abadi et al., 2015] và framework Lingvo [Shen et al., 2019]. Biến thể Transformer tiên tiến cho nhận dạng giọng nói là kiến trúc Conformer [Gulati et al., 2020] cũng sử dụng thêm convolution theo chiều sâu trong mỗi lớp. Vì phép toán convolution được áp dụng cho các đầu vào liên tiếp và không hỗ trợ ngay lập tức việc định tuyến, chúng tôi sử dụng kiến trúc Transformer tiêu chuẩn [Vaswani et al., 2017] thay thế. Kích hoạt Swish được sử dụng trong các khối feed forward, theo Gulati et al. [2020]. Chúng tôi cung cấp chi tiết cấu hình mô hình trong Bảng 7.

[THIS IS TABLE: Bảng 7 showing configuration details for Transformer models with columns for Model, Num of params, Layers, Num of heads, dmodel, dffn, dhead, dadpt]

[THIS IS TABLE: Bảng 8 showing fine-tuning hyperparameters with columns for Dataset, Input length, Batch size, Steps, Optimizer, Learning rate]

Huấn luyện mô hình Chúng tôi sử dụng cùng dữ liệu và quy trình được mô tả trong T5 [Raffel et al., 2020], BEST-RQ [Chiu et al., 2022] và Pix2struct [Lee et al., 2022] để tiền huấn luyện các mô hình văn bản, giọng nói và thị giác tương ứng. Chúng tôi sử dụng cùng siêu tham số huấn luyện, như kích thước batch, độ dài chuỗi đầu vào, số bước tiền huấn luyện và lựa chọn optimizer và lập lịch tỷ lệ học. Tất cả các mô hình đã được tiền huấn luyện bằng 128 hoặc 256 chip TPUv3/TPUv4.

Chúng tôi chạy tiền huấn luyện CODA cho các mô hình văn bản và thị giác, sử dụng thêm 100K bước và 200K bước tương ứng. Đối với các mô hình văn bản, độ dài chuỗi đầu vào là n = 512 và chúng tôi đặt số token được chọn k = 192. Đối với các mô hình thị giác, chuỗi đầu vào chứa n = 4096 patch hình ảnh và chúng tôi đặt k = 1024. Tiền huấn luyện CODA không được sử dụng cho các mô hình giọng nói của chúng tôi vì có

--- TRANG 17-21 ---
[Phần còn lại bao gồm các chi tiết thí nghiệm bổ sung, kết quả NLP, Speech, và Vision, cùng với các phân tích về thuật toán soft top-k và đóng góp của tác giả]
