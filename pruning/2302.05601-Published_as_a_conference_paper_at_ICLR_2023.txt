# 2302.05601.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2302.05601.pdf
# File size: 13671955 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published as a conference paper at ICLR 2023
PRUNING DEEPNEURAL NETWORKS FROM A SPARSITY
PERSPECTIVE
Enmao Diao∗
Department of Electrical
and Computer Engineering
Duke University
Durham, NC 27705, USA
enmao.diao@duke.eduGanghua Wang∗
School of Statistics
University of Minnesota
Minneapolis, MN 55455, USA
wang9019@umn.eduJiawei Zhang
School of Statistics
University of Minnesota
Minneapolis, MN 55455, USA
zhan4362@umn.edu
Yuhong Yang
School of Statistics
University of Minnesota
Minneapolis,
MN 55455, USA
yangx374@umn.eduJie Ding
School of Statistics
University of Minnesota
Minneapolis,
MN 55455, USA
dingj@umn.eduVahid Tarokh
Department of Electrical
and Computer Engineering
Duke University
Durham, NC 27705, USA
vahid.tarokh@duke.edu
ABSTRACT
In recent years, deep network pruning has attracted significant attention in order to
enable the rapid deployment of AI into small devices with computation and memory
constraints. Pruning is often achieved by dropping redundant weights, neurons, or
layers of a deep network while attempting to retain a comparable test performance.
Many deep pruning algorithms have been proposed with impressive empirical
success. However, existing approaches lack a quantifiable measure to estimate the
compressibility of a sub-network during each pruning iteration and thus may under-
prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure
the potential compressibility of deep neural networks and use this to develop a
Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments
corroborate the hypothesis that for a generic pruning procedure, PQI decreases
first when a large model is being effectively regularized and then increases when
its compressibility reaches a limit that appears to correspond to the beginning
of underfitting. Subsequently, PQI decreases again when the model collapse and
significant deterioration in the performance of the model start to occur. Additionally,
our experiments demonstrate that the proposed adaptive pruning algorithm with
proper choice of hyper-parameters is superior to the iterative pruning algorithms
such as the lottery ticket-based pruning methods, in terms of both compression
efficiency and robustness. Our code is available here.
1 I NTRODUCTION
Over-parameterized deep neural networks have been applied with enormous success in a variety of
fields, including computer vision (Krizhevsky et al., 2012; He et al., 2016b; Redmon et al., 2016),
natural language processing (Devlin et al., 2018; Radford et al., 2018), audio signal processing (Oord
et al., 2016; Schneider et al., 2019; Wang et al., 2020), and distributed learning (Kone ˇcn`y et al., 2016;
Ding et al., 2022; Diao et al., 2022). These deep neural networks have significantly expanded in size.
For example, LeNet-5 (LeCun et al., 1998) (1998; image classification) has 60 thousand parameters
whereas GPT-3 (Brown et al., 2020) (2020; language modeling) has 175 billion parameters. This
rapid growth in size has necessitated the deployment of a vast amount of computation, storage, and
energy resources. Due to hardware constraints, these enormous model sizes may be a barrier to
deployment in some edge devices such as mobile phones and virtual assistants. This has greatly
increased interest in deep neural network compression/pruning. To this end, various researchers have
developed empirical methods of building much simpler networks with similar performance based
∗Equally contributed
1arXiv:2302.05601v3  [cs.LG]  23 Aug 2023

--- PAGE 2 ---
Published as a conference paper at ICLR 2023
Regularization Compression CollapsePerformance
SparsityPerformance
SparsityPerformance
Sparsity
Figure 1: An illustration of our hypothesis on the relationship between sparsity and compressibility
of neural networks. The width of connections denotes the magnitude of model parameters.
on pre-trained networks (Han et al., 2015; Frankle & Carbin, 2018). For example, Han et al. (2015)
demonstrated that AlexNet (Krizhevsky et al., 2017) could be compressed to retain only 3%of the
original parameters on the ImageNet dataset without impacting classification accuracy.
An important topic of interest is the determination of limits of network pruning. An overly pruned
model may not have enough expressivity for the underlying task, which may lead to significant
performance deterioration (Ding et al., 2018). Existing methods generally monitor the prediction
performance on a validation dataset and terminate pruning when the performance falls below a
pre-specified threshold. Nevertheless, a quantifiable measure for estimating the compressibility of a
sub-network during each pruning iteration is desired. Such quantification of compressibility can lead
to the discovery of the most parsimonious sub-networks without performance degradation.
In this work, we connect the compressibility and performance of a neural network to its sparsity. In a
highly over-parameterized network, one popular assumption is that the relatively small weights are
considered redundant or non-influential and may be pruned without impacting the performance. Let
us consider the sparsity of a non-negative vector w= [w1, . . . , w d], since sparsity is related only to
the magnitudes of entries. Suppose S(w)is a sparsity measure, and a larger value indicates higher
sparsity. Hurley & Rickard (2009) summarize six properties that an ideal sparsity measure should
have, originally proposed in economics (Dalton, 1920; Rickard & Fallon, 2004). They are
(D1) Robin Hood. For any wi> w jandα∈(0,(wi−wj)/2), we have S([w1, . . . , w i−
α, . . . , w j+α, . . . , w d])< S(w).
(D2) Scaling. S(αw) =S(w)for any α >0.
(D3) Rising Tide. S(w+α)< S(w)for any α >0andwinot all the same.
(D4) Cloning. S(w) =S([w, w]).
(P1) Bill Gates. For any i= 1, . . . , d , there exists βi>0such that for any α >0we have
S([w1, . . . , w i+βi+α, . . . , w d])> S([w1, . . . , w i+βi, . . . , w d]).
(P2) Babies. S([w1, . . . , w d,0])> S(w)for any non-zero w.
Hurley & Rickard (2009) point out that only Gini index satisfies all six criteria among a comprehensive
list of sparsity measures. In this work, we propose a measure of sparsity named PQ Index (PQI). To
the best of our knowledge, PQI is the first measure related to the norm of a vector that satisfies all the
six properties above. Therefore, PQI is an ideal indicator of vector sparsity and is of its own interest.
We suggest using PQI to infer the compressibility of neural networks. Furthermore, we discover
the relationship between the performance and sparsity of iteratively pruned models as illustrated in
Figure 1. Our hypothesis is that for a generic pruning procedure, the sparsity will first decrease when
a large model is being effectively regularized, then increase when its compressibility reaches a limit
that corresponds to the start of underfitting, and finally decrease when the model collapse occurs, i.e.,
the model performance significantly deteriorates.
Our intuition is that the pruning will first remove redundant parameters. As a result, the sparsity
of model parameters will decrease and the performance may be improved due to regularization.
When the model is further compressed, part of the model parameters will become smaller when the
2

--- PAGE 3 ---
Published as a conference paper at ICLR 2023
model converges. Thus, the sparsity will increase and the performance will moderately decrease.
Finally, when the model collapse starts to occur, all attenuated parameters are removed and the
remaining parameters become crucial to maintain the performance. Therefore, the sparsity will
decrease and performance will significantly deteriorate. Our extensive experiments on pruning
algorithms corroborate the hypothesis. Consequently, PQI can infer whether a model is inherently
compressible. Motivated by this discovery, we also propose the Sparsity-informed Adaptive Pruning
(SAP) algorithm, which can compress more efficiently and robustly compared with iterative pruning
algorithms such as the lottery ticket-based pruning methods. Overall, our work presents a new
understanding of the inherent structures of deep neural networks for model compression. Our main
contributions are summarized below.
1.We propose a new notion of sparsity for vectors named PQ Index (PQI), with a larger value
indicating higher sparsity. We prove that PQI meets all six properties proposed by (Dalton,
1920; Rickard & Fallon, 2004), which capture the principles a sparsity measure should
obey. Among 15 commonly used sparsity measures, the only other measure satisfying
all properties is Gini Index (Hurley & Rickard, 2009). Thus, norm-based PQI is an ideal
sparsity/equity measure and may be of independent interest to many areas, e.g., signal
processing and economics.
2.We develop a new perspective on the compressibility of neural networks. In particular, we
measure the sparsity of pruned models by PQI and postulate the above hypothesis on the
relationship between sparsity and compressibility of neural networks.
3.Motivated by our proposed PQI and hypothesis, we further develop a Sparsity-informed
Adaptive Pruning (SAP) algorithm that uses PQI to choose the pruning ratio adaptively. In
particular, the pruning ratio at each iteration is decided based on a PQI-related inequality. In
contrast, Gini Index does not have such implications for the pruning ratio.
4.We conduct extensive experiments to measure the sparsity of pruned models and corroborate
our hypothesis. Our experimental results also demonstrate that SAP with proper choice
of hyper-parameters can compress more efficiently and robustly compared with iterative
pruning algorithms such as the lottery ticket-based pruning methods.
2 R ELATED WORK
Model compression The goal of model compression is to find a smaller model that has comparable
performance to the original model. A smaller model saves storage and computation resources, boosts
training, and facilitates the deployment of the model to devices with limited capacities, such as
mobile phones and virtual assistants. Therefore, model compression is vital for deploying deep
neural networks with millions or even billions of parameters. Various model compression methods
have been proposed for neural networks. Among them, pruning is one of the most popular and
effective approach (LeCun et al., 1989; Hagiwara, 1993; Han et al., 2015; Hu et al., 2016; Luo et al.,
2017; Frankle & Carbin, 2018; Lee et al., 2018; He et al., 2017). The idea of pruning is the sparsity
assumption that many redundant or non-influential neuron connections exist in an over-parameterized
model. Thus, we can remove those connections (e.g., weights, neurons, or neuron-like structures such
as layers) without sacrificing much test accuracy. Two critical components of pruning algorithms are
a pruning criterion that decides which connection to be pruned and a stop criterion that determines
when to stop pruning and thus prevent underfitting and model collapse. There are many pruning
criteria motivated by different interpretations of redundancy. A widely-used criterion removes the
parameters with the smallest magnitudes, assuming that they are less important (Hagiwara, 1993;
Han et al., 2015). Besides magnitude-based pruning, one may prune the parameters based on their
sensitivity or contribution to the network output (LeCun et al., 1989; Lee et al., 2018; Hu et al., 2016;
Soltani et al., 2021) or restrict different model components to share a large proportion of neural
weights (Diao et al., 2019; 2021). As for the stop criterion, the common choice is validation: to stop
pruning once the test accuracy on a validation dataset falls below a given threshold.
While pruning is a post-processing method that requires a pre-trained model, there are also pre-
processing and in-processing methods based on the sparsity assumption. For example, one can add
explicit sparse constraints on the network, such as forcing the parameters to have a low-rank structure
and sharing weights. Alternatively, one can implicitly force the trained model to be sparse, such
as adding a sparsity penalty (e.g., ℓ1-norm) to the parameters. In contrast to those sparsity-based
compression methods, which find a sub-network of the original one, researchers have also proposed
3

--- PAGE 4 ---
Published as a conference paper at ICLR 2023
compressing the model by finding a smaller model with a different architecture. The efforts include
knowledge distillation (Hinton et al., 2015) and architecture search (Mushtaq et al., 2021). We refer
the reader to (Hoefler et al., 2021) for a comprehensive survey of model compression.
Theory of model compression In practice, the compressibility of a model depends on the network
architecture and learning task. The pruning usually involves a lot of ad hoc hyper-parameter fine-
tuning. Thus, an understanding of model compressibility is urgently needed. There have been some
recent works to show the existence of or find a sub-network with guaranteed performance. Arora et al.
(2018) show that a model is more compressible if it is more stable to the noisy inputs and provides
a generalization error bound of the pruned model. Yang et al. (2022) propose a backward pruning
algorithm inspired by approximating functions using ℓq-norm (Wang et al., 2014), and quantify its
generalization error. Baykal et al. (2018); Mussay et al. (2019) utilize the concept of coreset to
prove the existence of a pruned network with similar performance. The main idea is to sample the
parameters based on their importance, and thus selected parameters could preserve the output of the
original network. Ye et al. (2020) propose a greedy selection algorithm to reconstruct a network and
bound the generalization error for two-layer neural networks. Our work develops a new perspective
on the compressibility of neural networks by directly measuring the sparsity of pruned models to
reveal the relationship between the sparsity and compressibility of neural networks.
Sparsity measure Sparsity is a crucial concept in many fundamental fields such as statistics and
signal processing (Tibshirani, 1996; Donoho, 2006; Akçakaya & Tarokh, 2008). Intuitively, sparsity
means that the most energy is concentrated in a few elements. For example, a widely-used assumption
in high-dimensional machine learning is that the model has an underlying sparse representation.
Various sparsity measures have been proposed in the literature from different angles. One kind
of sparsity measure originates from sociology and economics. For example, the well-known Gini
Index (Gini, 1912) can measure the inequality in a population’s wealth or welfare distribution. A
highly wealth-concentrated population forms a sparse vector if the vector consists of the wealth of
each person. In addition, to measure the diversity in a group, entropy-based measures like Shannon
entropy and Gaussian entropy are often used (Jost, 2006). Another kind of sparsity measure has
been studied in mathematics and engineering for a long time. A classic measure is the hard sparsity,
also known as ℓ0-norm, which is the number of non-zero elements in w. A small hard sparsity
implies that only a few vector elements are active or effective. However, a slight change in the
zero-valued element may cause a significant increase in the hard sparsity, which can be undesirable.
Thus, its relaxations such as ℓp-norm ( 0< p≤1) are also widely used. For example, ℓ1-norm-based
constraints or penalties are used for function approximation (Barron, 1993), model regularization
and variable selection (Tibshirani, 1996; Chen et al., 2001). Our work proposes the first measure of
sparsity related to vector norms that satisfies all the properties shared by the Gini Index (Hurley &
Rickard, 2009) and an adaptive pruning algorithm based on our proposed measure of sparsity.
3 P RUNING WITH PQ I NDEX
3.1 PQ I NDEX
We will prove all the six properties (D1)-(D4) and (P1), (P2), which are mentioned in the introduction,
hold for our proposed PQ Index (PQI). We realized that a similar form was originally proposed
in Bronstein et al. (2005) ( 0< p≤1, and q= 2), and a proof is given by Hurley & Rickard
(2009)( 0< p≤1< q). In our work, we prove it is necessary to have 0< p≤1< q, and
demonstrate its applicability for model compression.
Definition 1 (PQ Index) .For any 0< p≤1< q, the PQ Index of a non-zero vector w∈Rdis
Ip,q(w) = 1−d1
q−1
p∥w∥p
∥w∥q, (1)
where ∥w∥p= (Pd
i=1|wi|p)1/pis the ℓp-norm of wfor any p >0. For simplicity, we will use I(w)
and drop the dependency on pandqwhen the context is clear.
Theorem 1. We have 0≤Ip,q(w)≤1−d1
q−1
p, and a larger Ip,q(w)indicates a sparser vector.
Furthermore, Ip,q(w)satisfies all the six properties (D1)-(D4) and (P1), (P2).
Remark 1 (Sanity check) .For the densest or most equal situation, we have wi=cfori= 1, . . . , d ,
where cis a non-zero constant. It can be verified that Ip,q(w) = 0 . In contrast, the sparsest or most
unequal case is that wi’s are all zeros except one of them, and corresponding Ip,q(w) = 1−d1
q−1
p.
4

--- PAGE 5 ---
Published as a conference paper at ICLR 2023
Note that I(w)for an all-zero vector is not defined. From the perspective of the number of important
elements, an all-zero vector is sparse; however, it is dense from the aspect of energy distribution.
Remark 2 (Insights) .The form of Ip,qis not a random thought but inherently driven by properties
(D1)-(D4). Why do we need the ratio of two norms? It is essentially decided by the requirement of
(D2) Scaling. If S(w)involves only a single norm, then S(w)is not scale-invariant. However, since
ℓr-norm is homogeneous for all r >0, the ratio of two norms is inherently scale-invariant. Why is
there an additional scaling constant d1
q−1
p? This is necessary to satisfy (D4) Cloning. Inspired by
the well-known Root Mean Squared Error (RMSE), we found out that the additional scaling constant
is the correct term to help Ip,qbe independent of the vector length. It is essentially appealing for
comparing the sparsity of neural networks with different model parameters. Why do we require
p < q ? We find it plays a central role in meeting (D1) and (D3). The insight is that ∥w∥pdecreases
faster than ∥w∥qwhen a vector becomes sparser, thus guaranteeing a larger PQ Index.
Theorem 2 (PQI-bound on pruning) .LetMrdenote the set of rindices of wwith the largest
magnitudes, and ηrbe the smallest value such thatP
i/∈Mr|wi|p≤ηrP
i∈Mr|wi|p.Then, we have
r≥d(1 +ηr)−q/(q−p)[1−I(w)]qp
q−p. (2)
Remark 3. The PQI-bound is inspired by Yang et al. (2022) that proposed to use ∥w∥1/∥w∥q, q∈
(0,1)as a measure of sparsity. We use similar techniques to derive the bound based on our proposed
PQ Index. It is worth mentioning that ∥w∥1/∥w∥qdoes not satisfy properties (D2), (D4), (P1), and
(P4), which an ideal sparsity measure should have Hurley & Rickard (2009). Consequently, we
cannot use it to compare the sparsity of models of different sizes. The merit of the PQI-bound is that
it applies to iterative pruning of models that involve different numbers of model parameters.
Recall that the pruning is based on the assumption that parameters with small magnitudes are
removable. Therefore, suppose we know I(w)andηr, then we immediately have a lower bound for
the retaining ratio of the pruning from Theorem 2. Thus, we can adaptively choose the pruning ratio
based on I(w)andηr, which inspires our Sparsity-informed Adaptive Pruning (SAP) algorithm in
the following subsection. In practice, ηris unavailable before we decide the pruning ratio. Therefore,
we treat it as a hyper-parameter in our experiments. Since ηris non-increasing with respect to r, a
larger ηrmeans that we assume the model is more compressible and leads to a higher pruning ratio.
Experiments show that it is safe to choose ηr= 0.
3.2 S PARSITY -INFORMED ADAPTIVE PRUNING
In this section, we introduce the Sparsity-informed Adaptive Pruning (SAP) algorithm as illustrated
in Algorithm 1. Our algorithm is based on the well-known lottery ticket pruning method (Frankle
& Carbin, 2018). The lottery ticket pruning algorithm proposes to prune and retrain the model
iteratively. Compared with one shot pruning algorithm, which does not retrain the model at each
pruning iteration, the lottery ticket pruning algorithm produces pruned models of better performance
with the same percent of remaining model parameters. However, both methods use a fixed pruning
ratioPat each pruning iteration. As a result, they may under-prune or over-prune the model at
earlier or later pruning iterations, respectively. The under-pruned models with spare compressibility
require more pruning iterations and computation resources to obtain the smallest neural networks
with satisfactory performance. The over-pruned model suffers from underfitting due to insufficient
compressibility to maintain the desired performance. Therefore, we propose SAP to adaptively
determine the number of pruned parameters at each iteration based on the PQI-bound derived in
Formula 2. Furthermore, we introduce two additional hyper-parameters, the scaling factor γand the
maximum pruning ratio β, to make our algorithm further flexible and applicable.
Next, we walk through our SAP algorithm. Before the pruning starts, we randomly generate model
parameters winit. We will use winitto initialize retained model parameters at each pruning iterations.
The lottery ticket pruning algorithm shows that the performance of retraining the subnetworks from
winitis better than from scratch. Then, we initialize the mask m0with all ones. Suppose we have
Tnumber of pruning iterations. For each pruning iteration t= 0,1,2, . . . T , we first initialize the
model parameters ˜wtand compute the number of model parameters dtas follows
˜wt=winit⊙mt, d t=|mt|, (3)
where ⊙is the Hadamard product. After training the model parameters ˜wtwithmtby freezing the
gradient for Eepoch, we arrive at trained model parameters wt. Upon this point, our algorithm has
5

--- PAGE 6 ---
Published as a conference paper at ICLR 2023
Algorithm 1 Sparsity-informed Adaptive Pruning (SAP)
Input: model parameters w, mask m, norm 0< p≤1< q, compression hyper-parameter ηr,
scaling factor γ, maximum pruning ratio β, number of epochs E, and number of pruning
iterations T.
Randomly generate model parameters winit
Initialize mask m0with all ones
foreach pruning iteration t= 0,1,2, . . . T do
Initialize model parameters ˜wt=winit⊙mt
Compute the number of model parameters dt=|mt|
Train the model parameters ˜wtwithmtforEepochs and arrive at wt
Compute PQ Index I (wt) = 1−d1
q−1
p
t∥wt∥p
∥wt∥q
Compute the lower bound of the number of retained model parameters
rt=dt(1 +ηr)−q/(q−p)[1−I(wt)]qp
q−p
Compute the number of pruned model parameters
ct=⌊dt·min(γ(1−rt
dt), β)⌋
Prune ctmodel parameters with the smallest magnitude based on wtandmt
Create new mask mt+1
end
Output: The pruned model parameters wTand mask mT.
no difference from the classical lottery ticket pruning algorithm. The lottery ticket pruning algorithm
will then prune d·Pparameters from mtaccording to the magnitude of wtand finally create new
mask mt+1.
After arriving at wt, our proposed SAP will compute the PQ Index, denoted by I(wt), and the lower
bound of the number of retrained model parameters, denoted by rt, as follows
I(wt) = 1−d1
q−1
p
t∥wt∥p
∥wt∥q, r t=dt(1 +ηr)−q/(q−p)[1−I(wt)]qp
q−p. (4)
Then, we compute the number of pruned model parameters ct=⌊dt·min(γ(1−rt
dt), β)⌋.Here, we
introduce γto accelerate or decelerate the pruning at initial pruning iterations. Specifically, γ >1
or<1encourages the pruning ratio to be larger or smaller than the pruning ratio derived from the
PQI-bound, respectively. As the retraining of pruned models is time-consuming, it is appealing to
efficiently obtain the smallest pruned model with satisfactory performance for a small number of
pruning iterations. In addition, we introduce the maximum pruning ratio βto avoid excessive pruning
because we will compress more than the PQI-bound if γ >1and may completely prune all model
parameters. If we set γ= 1,βcan be safely omitted. In our experiments, we set β= 0.9only to
provide minimum protection from excessive pruning at each pruning iteration. Finally, we prune
ctmodel parameters with the smallest magnitude based on wtandmt, and finally create new mask
mt+1for the next pruning iteration.
4 E XPERIMENTAL STUDIES
4.1 E XPERIMENTAL SETUP
We conduct experiments with FashionMNIST (Xiao et al., 2017), CIFAR10, CIFAR100 (Krizhevsky
et al., 2009), and TinyImageNet (Le & Yang, 2015) datasets. Our backbone models are Linear,
Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), ResNet18, ResNet50 (He
et al., 2016a), and Wide ResNet28x8 (WResNet28x8) (Zagoruyko & Komodakis, 2016). We run
experiments for T= 30 pruning iterations with Linear, MLP, and CNN, and T= 15 pruning
iterations with ResNet18. We compare the proposed SAP with two baselines, including ‘One Shot’
and ‘Lottery Ticket’ (Frankle & Carbin, 2018) pruning algorithms. The difference between ‘One
Shot’ and ‘Lottery Ticket’ pruning algorithms is that ‘One Shot’ prunes d·Pmodel parameters at
each pruning iteration from w0instead of wt. We have P= 0.2throughout our experiments. We
compare the proposed PQ Index ( p= 0.5,q= 1.0) with the well-known Gini Index (Gini, 1912) to
validate its effectiveness in evaluating sparsity. Furthermore, we perform pruning on various pruning
scopes, including ‘Neuron-wise Pruning,’ ‘Layer-wise Pruning,’ and ‘Global Pruning.’ In particular,
‘Global Pruning’ gather all model parameters as a vector for pruning, while ‘Neuron-wise Pruning’
6

--- PAGE 7 ---
Published as a conference paper at ICLR 2023
(a) Retrained models
(b) Pruned models
Figure 2: Results of (a) retrained and (b) pruned models at each pruning iteration for ‘Global Pruning’
with FashionMNIST and MLP, where (a) is obtained from the models after retraining and (b) is
directly from those after pruning.
(a) Retrained models
(b) Pruned models
Figure 3: Results of (a) retrained and (b) pruned models at each pruning iteration for ‘Global Pruning’
with CIFAR10 and ResNet18.
and ‘Layer-wise Pruning’ prune each neuron and layer of model parameters separately. The number
of neurons at each layer is equal to the output size of that layer. For example, ‘One Shot’ and ‘Lottery
Ticket methods with ‘Neuron-wise Pruning’ prune di·Pmodel parameters of each neuron, where di
refers to the size of each neuron. Similarly, SAP computes the PQ Index and the number of pruned
model parameters for neuron i. Details of the model architecture and learning hyper-parameters are
included in the Appendix. We conducted four random experiments with different seeds, and the
standard deviation is shown in the error bar of all figures. Further experimental results can be found
in the Appendix.
4.2 E XPERIMENTAL RESULTS
Retrained models We demonstrate the results of retrained models ( wt⊙mt) at each pruning iteration
in Figure 2(a) and 3(a). In particular, we illustrate the performance, percent of remaining weights,
PQ Index, and Gini Index at each pruning iteration. In these experiments, ηrandγare set to 0and
1to prevent interference. The ‘Lottery Ticket’ method outperforms ‘One Shot’ as expected. SAP
(p= 1.0,q= 2.0) compresses more aggressively than SAP ( p= 0.5,q= 1.0). Recall that SAP
adaptively adjusts the pruning ratio based on the sparsity of models, while ‘One Shot’ and ‘Lottery
Ticket’ have a fixed pruning ratio at each pruning iteration. In Figure 2(a), SAP ( p= 1.0,q= 2.0) at
around T= 10 achieves similar performance as ‘Lottery Ticket’ at around T= 25 . Furthermore,
SAP ( p= 0.5,q= 1.0) and ‘Lottery Ticket’ have similar pruning ratio before T= 10 , but SAP
(p= 0.5,q= 1.0) adaptively lowers the pruning ratio and thus prevents the performance from
deteriorating like ‘Lottery Ticket’ does after T= 25 . In Figure 3(a), we observe similar fast pruning
phenomenon of SAP ( p= 1.0,q= 2.0). Meanwhile, SAP ( p= 0.5,q= 1.0) performs similar to
‘Lottery Ticket’ but prunes more than ‘Lottery Ticket’ does. Consequently, by carefully choosing p
7

--- PAGE 8 ---
Published as a conference paper at ICLR 2023
(a) Compression Trade -off
(b) Layer -wise Percent of Remaining Weights
(c) Layer -wise PQ Index
Figure 4: Results of various pruning scopes regarding (a) compression trade-off, (b) layer-wise percent
of remaining weights, and (c) layer-wise PQ Index for CIFAR10 and CNN. (b, c) are performed with
SAP ( p= 0.5,q= 1.0).
andq, SAP can provide more efficient and robust pruning. As for the sparsity of retrained models,
the result of the PQ Index is aligned with the Gini Index, which experimentally demonstrates that
the PQ Index can effectively measure the sparsity of model parameters. Furthermore, the dynamics
of the sparsity also corroborates our hypothesis, as shown by ‘One Shot’ in Figure 2(a) and SAP
(p= 1.0,q= 2.0) in Figure 3(a). The results show that an ideal pruning procedure should avoid a
rapid increase in sparsity.
Pruned models We demonstrate the results of pruned models at each pruning iteration in Figure 2(b)
and 3(b). In particular, we illustrate the performance, performance difference, PQ Index, and PQ
Index difference at each pruning iteration. The performance and PQ Index are computed directly
from the pruned models without retraining. The performance and PQ Index difference is between the
retrained ( w0⊙mtfor ‘One Shot’ and wt⊙mtfor ‘Lottery Ticket’ and SAP) and pruned models
(w0⊙mt+1for ‘One Shot’ and wt⊙mt+1for ‘Lottery Ticket’ and SAP). The results of performance
difference show that the performance of pruned models without retraining from SAP can perform
close to that of retrained models. Furthermore, the sparsity of pruned models shows that iterative
pruning generally decreases the sparsity of pruned models. Meanwhile, the sparsity difference of
pruned models provides a sanity check by showing that pruning at each pruning iteration decreases
the sparsity of retrained models.
Pruning scopes We demonstrate various pruning scopes regarding compression trade-off, layer-wise
percent of remaining weights, and layer-wise PQ Index in Figure 4. The results of the compression
trade-off show that SAP with ‘Global Pruning’ may perform worse than ‘One Shot’ and ‘Lottery
Ticket’ when the percent of remaining weights is small. As illustrated in the ‘Global Pruning’ of
Figure 4(b), the first layer has not been pruned enough. It is because SAP with ‘Global Pruning’
measures the sparsity of all model parameters in a vector, and the magnitude of the parameters of
the first layer and other layers may not be at the same scale. As a result, the parameters of the first
layer will not be pruned until late pruning iterations. However, SAP with ‘Neuron-wise Pruning’
8

--- PAGE 9 ---
Published as a conference paper at ICLR 2023
(a) 𝒒=𝟏.𝟎
(b) 𝒑=𝟏.𝟎
Figure 5: Ablation studies of pandqfor global pruning with CIFAR10 and CNN.
(b) 𝜼𝒓=𝟎.𝟎(a) 𝜸=𝟏.𝟎
Figure 6: Ablation studies of ηrandγfor global pruning with CIFAR10 and CNN.
and ‘Layer-wise Pruning’ perform better than ‘One Shot’ and ‘Lottery Ticket.’ SAP can adaptively
adjust the pruning ratio of each neuron and layer, but ‘One Shot’ and ‘Lottery Ticket’ may over-prune
specific neurons and layers because they adopt a fixed pruning ratio. Interestingly, the parameters of
the first layer are pruned more aggressively by ‘Neuron-wise Pruning’ than ‘Global Pruning’ in the
early pruning iterations. However, they are not pruned by ‘Neuron-wise Pruning’ in the late pruning
iterations, while ‘Global Pruning’ still prunes them aggressively. It aligns with the intuition that
the initial layers of CNN are more important to maintain the performance, e.g., Gale et al. (2019)
observed that the first layer was often more important to model quality and pruned less than other
layers. Furthermore, the PQ Index of ‘Neuron-wise Pruning’ is also more stable than the other two
pruning scopes, which indicates that ‘Neuron-wise Pruning’ is more appropriate for SAP, as the PQ
Index is computed more precisely.
Ablation studies We demonstrate ablation studies of pandqin Figure 5. In Figure 5(a), we fix
q= 1.0and study the effect of p. The results show that SAP prunes more aggressively when q= 1.0
andpis close to q. In Figure 5(b), we fix p= 1.0and study the effect of q. The results show that SAP
prunes more aggressively when p= 1.0andqis distant from p. We demonstrate ablation studies
ofηrandγin Figure 6. In Figure 6(a), we fix γ= 1.0and study the effect of ηr. The results show
that SAP prunes more aggressively when ηr>0. In Figure 6(b), we fix ηr= 0.0and study the
effect of γ. The results show that SAP prunes more aggressively when γ > 1. Interestingly, the
performance of our results roughly follows a logistic decay model due to the adaptive pruning ratio,
and the inflection point corresponds to the peak of the sparsity measure. Moreover, the dynamics of
the sparsity measure of SAP with various ablation studies also corroborate our hypothesis.
5 C ONCLUSION
We proposed a new notion of sparsity for vectors named PQ Index (PQI), which follows the principles
a sparsity measure should obey. We develop a new perspective on the compressibility of neural
networks by measuring the sparsity of pruned models. We postulate a hypothesis on the relationship
9

--- PAGE 10 ---
Published as a conference paper at ICLR 2023
between the sparsity and compressibility of neural networks. Motivated by our proposed PQI and
hypothesis, we further develop a Sparsity-informed Adaptive Pruning (SAP) algorithm that uses PQI
to choose the pruning ratio adaptively. Our experimental results demonstrate that SAP can compress
more efficiently and robustly than state-of-the-art algorithms.
ACKNOWLEDGMENTS
This work was supported in part by the Office of Naval Research (ONR) under grant number
N00014-21-1-2590.
REFERENCES
Mehmet Akçakaya and Vahid Tarokh. A frame construction and a universal distortion bound for
sparse representations. IEEE Transactions on Signal Processing , 56(6):2443–2450, 2008.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In Proc. ICML , pp. 254–263, 2018.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information Theory , 39(3):930–945, 1993.
Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, and Daniela Rus. Data-dependent
coresets for compressing neural networks with applications to generalization bounds. arXiv
preprint arXiv:1804.05345 , 2018.
Alexander M Bronstein, Michael M Bronstein, Michael Zibulevsky, and Yehoshua Y Zeevi. Sparse
ica for blind separation of transmitted and reflected images. International Journal of Imaging
Systems and Technology , 15(1):84–91, 2005.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Proc. NeurIPS , 33:1877–1901, 2020.
Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic decomposition by basis
pursuit. SIAM review , 43(1):129–159, 2001.
Hugh Dalton. The measurement of the inequality of incomes. The Economic Journal , 30(119):
348–361, 1920.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Enmao Diao, Jie Ding, and Vahid Tarokh. Restricted recurrent neural networks. In IEEE International
Conference on Big Data , pp. 56–63. IEEE, 2019.
Enmao Diao, Jie Ding, and Vahid Tarokh. HeteroFL: Computation and communication efficient
federated learning for heterogeneous clients. International Conference on Learning Representations
(ICLR) , 2021.
Enmao Diao, Jie Ding, and Vahid Tarokh. GAL: Gradient assisted learning for decentralized multi-
organization collaborations. Conference on Neural Information Processing Systems (NeurIPS) ,
2022.
Jie Ding, Vahid Tarokh, and Yuhong Yang. Model selection techniques: An overview. IEEE Signal
Processing Magazine , 35(6):16–34, 2018.
Jie Ding, Eric Tramel, Anit Kumar Sahu, Shuang Wu, Salman Avestimehr, and Tao Zhang. Federated
learning challenges and opportunities: An outlook. In IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pp. 8752–8756. IEEE, 2022.
David L Donoho. Compressed sensing. IEEE Transactions on Information Theory , 52(4):1289–1306,
2006.
10

--- PAGE 11 ---
Published as a conference paper at ICLR 2023
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:
Making all tickets winners. In International Conference on Machine Learning , pp. 2943–2952.
PMLR, 2020.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635 , 2018.
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv
preprint arXiv:1902.09574 , 2019.
Corrado Gini. Variabilità e mutabilità: contributo allo studio delle distribuzioni e delle relazioni
statistiche.[Fasc. I.] . Tipogr. di P. Cuppini, 1912.
Masafumi Hagiwara. Removal of hidden units and weights for back propagation networks. In Proc.
IJCNN , volume 1, pp. 351–354, 1993.
Song Han, Huizi Mao, and William Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 , 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pp. 770–778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In Proc. ECCV , pp. 630–645. Springer, 2016b.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks.
InProc. ICCV , pp. 1389–1397, 2017.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 , 2015.
Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep
learning: Pruning and growth for efficient inference and training in neural networks. J. Mach.
Learn. Res. , 22(241):1–124, 2021.
Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven
neuron pruning approach towards efficient deep architectures. arXiv preprint arXiv:1607.03250 ,
2016.
Niall Hurley and Scott Rickard. Comparing measures of sparsity. IEEE Transactions on Information
Theory , 55(10):4723–4741, 2009.
Lou Jost. Entropy and diversity. Oikos , 113(2):363–375, 2006.
Jakub Kone ˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492 , 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. Proc. NeurIPS , 25, 2012.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. Communications of the ACM , 60(6):84–90, 2017.
Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N , 7(7):3, 2015.
Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Proc. NeurIPS , 2, 1989.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proc. IEEE , 86(11):2278–2324, 1998.
11

--- PAGE 12 ---
Published as a conference paper at ICLR 2023
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning
based on connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983 , 2016.
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural
network compression. In Proc. ICCV , pp. 5058–5066, 2017.
Erum Mushtaq, Chaoyang He, Jie Ding, and Salman Avestimehr. Spider: Searching personalized
neural architecture for federated learning. arXiv preprint arXiv:2112.13939 , 2021.
Ben Mussay, Margarita Osadchy, Vladimir Braverman, Samson Zhou, and Dan Feldman. Data-
independent neural pruning via coresets. arXiv preprint arXiv:1907.04018 , 2019.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio. arXiv preprint arXiv:1609.03499 , 2016.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training. 2018.
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified,
real-time object detection. In Proc. CVPR , pp. 779–788, 2016.
Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in neural
network pruning. arXiv preprint arXiv:2003.02389 , 2020.
Scott Rickard and Maurice Fallon. The gini index of speech. In Proceedings of the 38th Conference
on Information Science and Systems (CISS’04) , 2004.
Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised
pre-training for speech recognition. arXiv preprint arXiv:1904.05862 , 2019.
Mohammadreza Soltani, Suya Wu, Jie Ding, Robert Ravier, and Vahid Tarokh. On the information of
feature maps and pruning of deep neural networks. In Proc. ICPR , pp. 6988–6995, 2021.
Robert Tibshirani. Regression shrinkage and selection via the lasso. J. Royal Stat. Soc. B , 58(1):
267–288, 1996.
Jianyou Wang, Michael Xue, Ryan Culhane, Enmao Diao, Jie Ding, and Vahid Tarokh. Speech
emotion recognition with dual-sequence lstm architecture. In IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , pp. 6474–6478. IEEE, 2020.
Zhan Wang, Sandra Paterlini, Fuchang Gao, and Yuhong Yang. Adaptive minimax regression
estimation over sparse lq-hulls. J. Mach. Learn. Res. , 15(1):1675–1711, 2014.
Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
Wenjing Yang, Ganghua Wang, Jie Ding, and Yuhong Yang. A theoretical understanding of neural
network compression from sparse linear approximation. arXiv preprint arXiv:2206.05604 , 2022.
Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good subnetworks
provably exist: Pruning via greedy forward selection. In Proc. ICML , pp. 10820–10830, 2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,
2016.
12

--- PAGE 13 ---
Published as a conference paper at ICLR 2023
Appendix
A L IMITATIONS AND FUTURE WORK
Theory We leverage our proposed PQ Index and derive a PQI-bound to indicate the number of
retained parameters r. Our result is a lower bound for r, which only suggests the maximum number
of model parameters we should prune. One potential future work is to develop an upper for rso
that we can better understand the relationship between sparsity and pruning. Furthermore, our result
introduces an additional term ηr, which is unavailable before determining the pruning ratio. We treat
it as a hyper-parameter in our algorithm and experiments. However, it is desirable to develop a tighter
bound without such approximation. Additionally, how pandqtogether impact the sparsity measure
and model pruning has not been thoroughly analyzed. Finally, the theoretical justification of the
proposed hypothesis is lacking.
Method Our corroborated hypothesis indicates that a dynamic relationship exists between the
model’s sparsity and compressibility. However, our proposed SAP algorithm determines the number
of pruned parameters based on a static PQI-bound at each iteration. Thus, one potential future work is
to further develop the SAP algorithm by considering the dynamics of sparsity. For example, stopping
pruning when the PQI starts to increase or the pruning ratio is below some threshold. In this work,
we demonstrate the relationship between the performance of iterative pruning and the dynamics
of PQI. It is interesting to analyze the critical factors that may determine such dynamics, such as
initialization and model architecture. Recent works also introduce gradual magnitude pruning which
can outperform iterative pruning algorithms Gale et al. (2019); Renda et al. (2020). Therefore, it is
also interesting to study how PQI are related to various pruning methods Evci et al. (2020); Hoefler
et al. (2021). We demonstrate that SAP with proper choice of hyper-parameters can outperform LT.
It is interesting to explore when SAP can outperform LT with respect to the accuracy-compression
trade-off.
Application We apply the proposed PQ Index for model compression because model compression is
one of the most important topics related to sparsity. However, many other interesting topics could
leverage the PQ Index. For example, one may consider directly optimizing the objective function and
PQ Index together for regularization. Furthermore, fairness that advocates similar performances of
various groups may also benefit from our PQ Index. Finally, other fields using Gini Index, such as
sociology and economics, may also find the proposed alternative index interesting.
B T HEORETICAL ANALYSIS
Proof of Theorem 1 :
Range of PQI. Note that for any 0< p < q , Hölder’s inequality gives that
∥w∥q≤ ∥w∥p≤d1
p−1
q∥w∥q.
We immediately obtain I (w)∈[0,1−d1
q−1
p]from the inequality above.
Properties. Next, we prove that Isatisfies six properties, which implies that I(w)is larger if wis
sparser. Hurley & Rickard (2009) prove that (P1) and (P2) are automatically satisfied as long as
(D1)-(D4) are met. Furthermore, we note that f(x) = 1−1/xis monotonous for x >0. Therefore,
we only have to prove (D1)-(D4) hold for S(w) =d1
p−1
q∥w∥q/∥w∥p.
(D2) Scaling. It is automatically satisfied for S(w)since ℓq-norm is homogeneous for any q >0.
(D4) Cloning. It is clear from the definition of S(w).
(D1) Robin Hood. . Without loss of generality, we only need to prove that for w1> w 2>0, the
derivative of f(t)att= 0is negative, where
f(t) =
(w1−t)q+ (w2+t)q+Pd
i>2wq
i	1/q

(w1−t)p+ (w2+t)p+Pd
i>2wp
i	1/p.
13

--- PAGE 14 ---
Published as a conference paper at ICLR 2023
The derivative is given by
f′(0) = f(0)−wq−1
1+wq−1
2Pd
i=1wq
i−−wp−1
1+wp−1
2Pd
i=1wp
i
. (5)
It is obvious that f′(0)<0when 0< p≤1≤qandp̸=q, noting that w1> w 2>0.
Next, we aim to show that f′(0)might be positive when the condition that 0< p≤1< qis violated,
thus (D1) is not satisfied. We prove this claim by contradiction.
We first consider the case that 0< p < q < 1. Iff′(0)<0always holds, by Eq. (5), we have that
g(t) :=−wt−1
1+wt−1
2Pd
i=1wt
i
is a monotonously decreasing function for t∈[0,1]. That is, g′(t)<0fort∈(0,1). Note that
g′(t) =(−wt−1
1lnw1+wt−1
2lnw2)(Pd
i=1wt
i)−(−wt−1
1+wt−1
2)(Pd
i=1wt
ilnwi)
(Pd
i=1wt
i)2,
its numerator can be further written as the sum of two terms:
(S1)wt−1
1wt−1
2(w1+w2)(lnw2−lnw1),
(S2)dX
i>2wt
i{wt−1
2(lnw2−lnwi)−wt−1
1(lnw1−lnwi)}.
The first term (S1) is negative since w1> w 2. As for the second term, we define
h(w) =wt{wt−1
2(lnw2−lnw)−wt−1
1(lnw1−lnw)}.
We note that h(w)→0asw→0andh(w)→ −∞ asw→ ∞ , thus h(w)achieves its maximal
value when w=w∗, where w∗satisfies that h′(w∗) = 0 . Since
h′(w) =twt−1{wt−1
2(lnw2−lnw)−wt−1
1(lnw1−lnw)}+wt−1(−wt−1
2+wt−1
1),
taking h′(w∗) = 0 yields that
w∗= expwt−1
1lnw1−wt−1
2lnw2
−wt−1
2+wt−1
1−1/t
,
h(w∗) =1
twt
∗(wt−1
2−wt−1
1).
Since t <1andw1> w 2, we know h(w∗)is positive, thus (S2) can be arbitrarily large as d→ ∞ ,
meaning that g′(t)is also positive, which is a contradiction.
Similarly, if (D1) holds for any 1< p < q , it implies g′(t)<0for any t >1. However, we have (S2)
goes to infinity when t >1andwi→ ∞ , i > 2. Thus, g′(t)may be positive for t >1, leading to a
contradiction and completing the proof.
(D3) Rising tide. We prove that that f′(t)is negative for any 0< p < q , where
f(t) =(Pd
i=1(wi+t)q)1/q
(Pd
i=1(wi+t)p)1/p.
We can verify that
f′(0) = f(0)Pd
i=1wq−1
iPd
i=1wq
i−Pd
i=1wp−1
iPd
i=1wp
i
.
Thus, we conclude the proof by showing that h(t) = (Pd
i=1wt−1
i)/(Pd
i=1wt
i)is a monotonously
decreasing function for t >0. This is done by showing h′(t)<0for all t >0. Actually, since
wi≥0andwi’s are not all the same, we know
h′(t) =(Pd
i=1wt−1
iln(wi))(Pd
i=1wt
i)−(Pd
i=1wt−1
i)(Pd
i=1wt
iln(wi))
(Pd
i=1wt
i)2
=P
1≤i<j≤d(wj−wi)(ln(wi)−ln(wj))wt−1
iwt−1
j
(Pd
i=1wt
i)2<0.
14

--- PAGE 15 ---
Published as a conference paper at ICLR 2023
Proof of Theorem 2 :
Recall that Mris the largest rcomponents of w, and ηris a constant such thatP
i/∈Mr|wi|p≤
ηrP
i∈Mr|wi|p. Therefore,
∥w∥p=X
1≤i≤d|wi|p1
p
=X
i∈Mr|wi|p+X
i̸∈Mr|wi|p1
p
≤X
i∈Mr|wi|p+ηrX
i∈Mr|wi|p1
p
=X
i∈Mr|wi|p1
p
(1 +ηr)1
p
≤X
i∈Mr|wi|q1
q
r1
p−1
q(1 +ηr)1
p≤ ∥w∥qr1
p−1
q(1 +ηr)1
p.
Rearranging the above inequality gives
r≥d(1 +ηr)−q/(q−p)[1−I(w)]qp
q−p.
15

--- PAGE 16 ---
Published as a conference paper at ICLR 2023
C E XPERIMENTAL SETUP
Table 1 and 2 summarizes the model architecture of MLP and CNN used in our experiments. Table 3
shows the statistics of model architecture and hyper-parameters used in our experiments.
Table 1: The model architecture of Multi-Layer Perceptron (MLP) used in our experiments. The
nc, H, W represent the shape of images, namely the number of image channels, height, and width, re-
spectively. Kis the number of classes in the classification task. The ReLU layers follow Linear(input
channel size, output channel size) layers, apart from the last one.
Image x∈Rnc×H×W
Linear( nc×H×W, 128)
Linear(128, 256)
Linear(256, K)
Table 2: The model architecture of Convolutional Neural Networks (CNN) used in our experiments.
Thenc, H, W represent the shape of images, namely the number of image channels, height, and
width, respectively. Kis the number of classes in the classification task. The BatchNorm and ReLU
layers follow Conv2d(input channel size, output channel size, kernel size, stride, padding) layers.
The MaxPool2d(output channel size, kernel size) layer reduces the height and width by half.
Image x∈Rnc×H×W
Conv2d( nc, 64, 3, 1, 1)
MaxPool2d(64, 2)
Conv2d(64, 128, 3, 1, 1)
MaxPool2d(128, 2)
Conv2d(128, 256, 3, 1, 1)
MaxPool2d(256, 2)
Conv2d(256, 512, 3, 1, 1)
MaxPool2d(512, 2)
Global Average Pooling
Linear(512, K)
Table 3: Statistics of the models and hyper-parameters used in our experiments for training and
pruning.
Dataset FashionMNIST CIFAR10
Model Architecture Linear MLP CNN ResNet18 Linear MLP CNN ResNet18
Model Size 7.9 K 136.1 K 1.6 M 11.2 M 30.7 K 428.9 K 1.6 M 11.2 M
FLOPS 3.9 M 67.8 M 20.1 G 114.4 G 15.4 M 214.2 M 29.4 G 139.4 G
TrainEpoch E 200
Batch size 250
Optimizer SGD
Learning rate 1E-01
Momentum 0.9
Weight decay 5E-04
Nesterov ✓
Scheduler Cosine Annealing (Loshchilov & Hutter, 2016)
PruneT 30 15 30 15
P 0.2
16

--- PAGE 17 ---
Published as a conference paper at ICLR 2023
D E XPERIMENTAL RESULTS
D.1 PQ I NDEX
We visualize the PQ Index of pruned models at the global scale with various combinations of pandq.
We use zero to indicate the numerical overflow may happen when p= 0.1. Note that we use p= 0.5
andq= 1.0to compute PQ Index for other figures. The results show that various combinations of p
andqalso corroborate our hypothesis in different scales, e.g (d) One Shot in Figure 7 and (b) SAP
(p= 1.0,q= 2.0) of Figure 8.
(a) SAP (𝒑=𝟎.𝟓, 𝒒=𝟏.𝟎)
(b) SAP (𝒑=𝟏.𝟎, 𝒒=𝟐.𝟎)
(c) Lottery Ticket
(d) One Shot
Figure 7: Results of PQ index visualized with various combinations of pandqfor FashionMNIST
and MLP.
17

--- PAGE 18 ---
Published as a conference paper at ICLR 2023
(a) SAP (𝒑=𝟎.𝟓, 𝒒=𝟏.𝟎)
(b) SAP (𝒑=𝟏.𝟎, 𝒒=𝟐.𝟎)
(c) Lottery Ticket
(d) One Shot
Figure 8: Results of PQ index visualized with various combinations of pandqfor CIFAR10 and
ResNet18.
18

--- PAGE 19 ---
Published as a conference paper at ICLR 2023
(a) SAP (𝒑=𝟎.𝟓,𝒒=𝟏.𝟎)
(b) SAP (𝒑=𝟏.𝟎,𝒒=𝟐.𝟎)
(c) Lottery Ticket
(d) One Shot
Figure 9: Results of PQ index visualized with various combinations of pandqfor CIFAR100 and
WResNet28x8.
19

--- PAGE 20 ---
Published as a conference paper at ICLR 2023
(a) SAP (𝒑=𝟎.𝟓,𝒒=𝟏.𝟎)
(b) SAP (𝒑=𝟏.𝟎,𝒒=𝟐.𝟎)
(c) Lottery Ticket
(d) One Shot
Figure 10: Results of PQ index visualized with various combinations of pandqfor TinyImageNet
and ResNet50.
20

--- PAGE 21 ---
Published as a conference paper at ICLR 2023
D.2 R ETRAINED AND PRUNED MODELS
(a) Retrained models
(b) Pruned models
Figure 11: Results of (a) retrained and (b) pruned models at each pruning iteration for ‘Global
Pruning’ with FashionMNIST and Linear.
(a) Retrained models
(b) Pruned models
Figure 12: Results of (a) retrained and (b) pruned models at each pruning iteration for ‘Global
Pruning’ with FashionMNIST and CNN.
(a) Retrained models
(b) Pruned models
Figure 13: Results of (a) retrained and (b) pruned models at each pruning iteration for ‘Global
Pruning’ with FashionMNIST and ResNet18.
21

--- PAGE 22 ---
Published as a conference paper at ICLR 2023
(a) Retrained models
(b) Pruned models
Figure 14: Results of (a) retrained and (b) pruned models at each pruning iteration for ‘Global
Pruning’ with CIFAR10 and Linear.
(a) Retrained models
(b) Pruned models
Figure 15: Results of (a) retrained and (b) pruned models at each pruning iteration for ‘Global
Pruning’ with CIFAR10 and MLP.
(a) Retrained models
(b) Pruned models
Figure 16: Results of (a) retrained and (b) pruned models at each pruning iteration for ‘Global
Pruning’ with CIFAR10 and CNN.
22

--- PAGE 23 ---
Published as a conference paper at ICLR 2023
(a) Retrained models
(b)Pruned model s
Figure 17: Results of (a) retrained and (b) pruned models at each pruning iteration for ‘Global
Pruning’ with CIFAR100 and WResNet28x8.
(a) Retrained models
(b) Pruned models
Figure 18: Results of (a) retrained and (b) pruned models at each pruning iteration for ‘Global
Pruning’ with TinyImageNet and ResNet50.
23

--- PAGE 24 ---
Published as a conference paper at ICLR 2023
D.3 P RUNING SCOPES
(a) Compression Trade -off
(b) Layer -wise Percent of Remaining Weights
(c) Layer -wise PQ Index
Figure 19: Results of various pruning scopes regarding (a) compression trade-off, (b) layer-wise
percent of remaining weights, and (c) layer-wise PQ Index for FashionMNIST and MLP. (b, c) are
performed with SAP ( p= 0.5,q= 1.0).
(a) Compression Trade -off
(b) Layer -wise Percent of Remaining Weights
(c) Layer -wise PQ Index
Figure 20: Results of various pruning scopes regarding (a) compression trade-off, (b) layer-wise
percent of remaining weights, and (c) layer-wise PQ Index for FashionMNIST and CNN. (b, c) are
performed with SAP ( p= 0.5,q= 1.0).
24

--- PAGE 25 ---
Published as a conference paper at ICLR 2023
(a) Compression Trade -off
(b) Layer -wise Percent of Remaining Weights
(c) Layer -wise PQ Index
Figure 21: Results of various pruning scopes regarding (a) compression trade-off, (b) layer-wise
percent of remaining weights, and (c) layer-wise PQ Index for FashionMNIST and ResNet18. (b, c)
are performed with SAP ( p= 0.5,q= 1.0).
(a) Compression Trade -off
(b) Layer -wise Percent of Remaining Weights
(c) Layer -wise PQ Index
Figure 22: Results of various pruning scopes regarding (a) compression trade-off, (b) layer-wise
percent of remaining weights, and (c) layer-wise PQ Index for CIFAR10 and MLP. (b, c) are performed
with SAP ( p= 0.5,q= 1.0).
25

--- PAGE 26 ---
Published as a conference paper at ICLR 2023
(a) Compression Trade -off
(b) Layer -wise Percent of Remaining Weights
(c) Layer -wise PQ Index
Figure 23: Results of various pruning scopes regarding (a) compression trade-off, (b) layer-wise
percent of remaining weights, and (c) layer-wise PQ Index for CIFAR10 and CNN. (b, c) are
performed with SAP ( p= 0.5,q= 1.0).
(a) Compression Trade -off
(b) Layer -wise Percent of Remaining Weights
(c) Layer -wise PQ Index
Figure 24: Results of various pruning scopes regarding (a) compression trade-off, (b) layer-wise
percent of remaining weights, and (c) layer-wise PQ Index for CIFAR10 and ResNet18. (b, c) are
performed with SAP ( p= 0.5,q= 1.0).
26

--- PAGE 27 ---
Published as a conference paper at ICLR 2023
(a) Compression Trade -off
(b) Layer -wise Percent of Remaining Weights
(c) Layer -wise PQ Index
Figure 25: Results of various pruning scopes regarding (a) compression trade-off, (b) layer-wise
percent of remaining weights, and (c) layer-wise PQ Index for CIFAR100 and WResNet28x8. (b, c)
are performed with SAP ( p= 0.5,q= 1.0).
(a) Compression Trade -off
(b) Layer -wise Percent of Remaining Weights
(c) Layer -wise PQ Index
Figure 26: Results of various pruning scopes regarding (a) compression trade-off, (b) layer-wise
percent of remaining weights, and (c) layer-wise PQ Index for TinyImageNet and ResNet50. (b, c)
are performed with SAP ( p= 0.5,q= 1.0).
27

--- PAGE 28 ---
Published as a conference paper at ICLR 2023
D.4 E FFECTS OF pANDq
(a) 𝒒=𝟏.𝟎
(b) 𝒑=𝟏.𝟎
Figure 27: Ablation studies of pandqfor global pruning with CIFAR10 and Linear.
(a) 𝒒=𝟏.𝟎
(b) 𝒑=𝟏.𝟎
Figure 28: Ablation studies of pandqfor global pruning with CIFAR10 and MLP.
(a) 𝒒=𝟏.𝟎
(b) 𝒑=𝟏.𝟎
Figure 29: Ablation studies of pandqfor global pruning with CIFAR10 and ResNet18.
28

--- PAGE 29 ---
Published as a conference paper at ICLR 2023
D.5 E FFECTS OF ηrANDγ
(b) 𝜼𝒓=𝟎.𝟎(a) 𝜸=𝟏.𝟎
Figure 30: Ablation studies of ηrandγfor global pruning with CIFAR10 and Linear.
(b) 𝜼𝒓=𝟎.𝟎(a) 𝜸=𝟏.𝟎
Figure 31: Ablation studies of ηrandγfor global pruning with CIFAR10 and MLP.
(b) 𝜼𝒓=𝟎.𝟎(a) 𝜸=𝟏.𝟎
Figure 32: Ablation studies of ηrandγfor global pruning with CIFAR10 and ResNet18.
29
