# Masking as an Efﬁcient Alternative to Finetuning
# for Pretrained Language Models
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2004.12406.pdf
# File size: 807949 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Masking như một Giải pháp Thay thế Hiệu quả cho Finetuning
đối với Mô hình Ngôn ngữ Tiền huấn luyện
Mengjie Zhao†*, Tao Lin‡*, Fei Mi‡, Martin Jaggi‡, Hinrich Schütze†
†LMU Munich, Germany‡EPFL, Switzerland
mzhao@cis.lmu.de, {tao.lin, fei.mi, martin.jaggi}@epfl.ch
Tóm tắt
Chúng tôi trình bày một phương pháp hiệu quả để sử dụng các mô hình ngôn ngữ tiền huấn luyện, trong đó chúng tôi học các mask nhị phân có tính chọn lọc cho các trọng số tiền huấn luyện thay vì sửa đổi chúng thông qua finetuning. Đánh giá mở rộng việc masking BERT, RoBERTa, và DistilBERT trên mười một tác vụ NLP đa dạng cho thấy rằng sơ đồ masking của chúng tôi mang lại hiệu suất có thể so sánh với finetuning, nhưng có dung lượng bộ nhớ nhỏ hơn nhiều khi cần suy luận cho nhiều tác vụ. Đánh giá nội tại cho thấy rằng các biểu diễn được tính toán bởi các mô hình ngôn ngữ được mask nhị phân mã hóa thông tin cần thiết để giải quyết các tác vụ downstream. Phân tích landscape mất mát, chúng tôi cho thấy rằng masking và finetuning tạo ra các mô hình nằm trong các minima có thể được kết nối bằng một đoạn thẳng với độ chính xác kiểm tra gần như không đổi. Điều này xác nhận rằng masking có thể được sử dụng như một giải pháp thay thế hiệu quả cho finetuning.

1 Giới thiệu
Finetuning một mô hình ngôn ngữ tiền huấn luyện lớn như BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), và XLNet (Yang et al., 2019) thường mang lại kết quả cạnh tranh hoặc thậm chí là state-of-the-art trên các benchmark NLP (Wang et al., 2018, 2019). Với một tác vụ NLP, finetuning tiêu chuẩn xếp chồng một lớp tuyến tính lên trên mô hình ngôn ngữ tiền huấn luyện và sau đó cập nhật tất cả các tham số sử dụng mini-batch SGD. Các khía cạnh khác nhau như tính dễ vỡ (Dodge et al., 2020) và khả năng thích ứng (Peters et al., 2019) của paradigm NLP học chuyển giao hai giai đoạn này (Dai và Le, 2015; Howard và Ruder, 2018) đã được nghiên cứu.

Mặc dù tính đơn giản và hiệu suất ấn tượng của finetuning, số lượng tham số cực lớn cần được finetuned, ví dụ, 340 triệu trong BERT-large, là một trở ngại lớn cho việc triển khai rộng rãi hơn của các mô hình này. Dung lượng bộ nhớ lớn của các mô hình đã finetuned trở nên nổi bật hơn khi cần giải quyết nhiều tác vụ – một số bản sao của hàng triệu tham số đã finetuned phải được lưu để suy luận.

Nghiên cứu gần đây (Gaier và Ha, 2019; Zhou et al., 2019) chỉ ra tiềm năng của việc tìm kiếm kiến trúc mạng neural trong một mô hình cố định, như một giải pháp thay thế cho việc tối ưu hóa trọng số mô hình cho các tác vụ downstream. Được truyền cảm hứng từ các kết quả này, chúng tôi trình bày masking, một sơ đồ đơn giản nhưng hiệu quả để sử dụng các mô hình ngôn ngữ tiền huấn luyện. Thay vì cập nhật trực tiếp các tham số tiền huấn luyện, chúng tôi đề xuất chọn lọc các trọng số quan trọng cho các tác vụ NLP downstream trong khi loại bỏ những trọng số không liên quan. Cơ chế chọn lọc bao gồm một tập hợp các mask nhị phân, một mask được học cho mỗi tác vụ downstream thông qua huấn luyện end-to-end.

Chúng tôi cho thấy rằng masking, khi được áp dụng cho các mô hình ngôn ngữ tiền huấn luyện như BERT, RoBERTa, và DistilBERT (Sanh et al., 2019), đạt được hiệu suất có thể so sánh với finetuning trong các tác vụ như gắn thẻ từ loại, nhận dạng thực thể có tên, phân loại chuỗi, và hiểu đọc. Điều này đáng ngạc nhiên ở chỗ một cơ chế lựa chọn con đơn giản không thay đổi bất kỳ trọng số nào lại có thể cạnh tranh với một chế độ huấn luyện – finetuning – có thể thay đổi giá trị của mọi trọng số đơn lẻ.

Chúng tôi tiến hành các phân tích chi tiết tiết lộ các yếu tố quan trọng và lý do có thể cho hiệu suất mong muốn của masking.

Masking có hiệu quả về tham số: chỉ cần lưu một tập hợp các mask nhị phân 1-bit cho mỗi tác vụ sau khi huấn luyện, thay vì tất cả các tham số float 32-bit trong finetuning. Dung lượng bộ nhớ nhỏ này cho phép triển khai các mô hình ngôn ngữ tiền huấn luyện để giải quyết nhiều tác vụ trên các thiết bị edge. Tính compact của masking cũng tự nhiên cho phép các ensemble tiết kiệm tham số của các mô hình ngôn ngữ tiền huấn luyện.

Đóng góp của chúng tôi: (i) Chúng tôi giới thiệu masking, một sơ đồ mới để sử dụng các mô hình ngôn ngữ tiền huấn luyện bằng cách học các mask có tính chọn lọc cho các trọng số tiền huấn luyện, như một giải pháp thay thế hiệu quả cho finetuning.arXiv:2004.12406v2 [cs.CL] 11 Oct 2020

--- TRANG 2 ---
Chúng tôi cho thấy rằng masking có thể áp dụng cho các mô hình như BERT/RoBERTa/DistilBERT, và tạo ra hiệu suất ngang bằng với finetuning. (ii) Chúng tôi thực hiện phân tích thực nghiệm mở rộng về masking, làm sáng tỏ các yếu tố quan trọng để đạt được hiệu suất tốt trên mười một tác vụ NLP đa dạng. (iii) Chúng tôi nghiên cứu landscape mất mát và biểu diễn ngôn ngữ của các mô hình ngôn ngữ được mask nhị phân, tiết lộ những lý do tiềm năng tại sao masking có hiệu suất tác vụ có thể so sánh với finetuning.

2 Nghiên cứu liên quan
Paradigm NLP hai giai đoạn. Các mô hình ngôn ngữ tiền huấn luyện (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019; Radford et al., 2019) thúc đẩy NLP với biểu diễn có ngữ cảnh của từ. Finetuning một mô hình ngôn ngữ tiền huấn luyện (Dai và Le, 2015; Howard và Ruder, 2018) thường mang lại hiệu suất cạnh tranh một phần vì pretraining dẫn đến khởi tạo tốt hơn trên các tác vụ downstream khác nhau so với huấn luyện từ đầu (Hao et al., 2019). Tuy nhiên, finetuning trên các tác vụ NLP riêng lẻ không hiệu quả về tham số. Mỗi mô hình đã finetuned, thường bao gồm hàng trăm triệu tham số floating point, cần được lưu riêng lẻ.

Stickland và Murray (2019) sử dụng các lớp attention được chiếu với học đa tác vụ để cải thiện hiệu quả của finetuning BERT. Houlsby et al. (2019) chèn các module adapter vào BERT để cải thiện hiệu quả bộ nhớ. Các module được chèn làm thay đổi quá trình forward pass của BERT, do đó cần được khởi tạo cẩn thận để gần với đồng nhất.

Chúng tôi đề xuất chọn trực tiếp các tham số phù hợp với một tác vụ downstream, bằng cách học các mask nhị phân có tính chọn lọc thông qua huấn luyện end-to-end. Giữ nguyên các tham số tiền huấn luyện, chúng tôi giải quyết một số tác vụ NLP downstream với overhead tối thiểu.

Mạng nhị phân và pruning mạng. Các mask nhị phân có thể được huấn luyện sử dụng "straight-through estimator" (Bengio et al., 2013; Hinton, 2012). Hubara et al. (2016), Rastegari et al. (2016), Hubara et al. (2017), inter alia, áp dụng kỹ thuật này để huấn luyện các mạng neural nhị phân hiệu quả.

Chúng tôi sử dụng estimator này để huấn luyện các mask có tính chọn lọc cho các tham số mô hình ngôn ngữ tiền huấn luyện.

Nghiên cứu giả thuyết lottery ticket (Frankle và Carbin, 2018) của pruning mạng (Han et al., 2015a; He et al., 2018; Liu et al., 2019c; Lee et al., 2019; Lin et al., 2020), Zhou et al. (2019) thấy rằng việc áp dụng mask nhị phân cho một mạng neural là một dạng huấn luyện mạng. Gaier và Ha (2019) đề xuất tìm kiếm kiến trúc mạng neural cho học tăng cường và các tác vụ phân loại hình ảnh, mà không có bất kỳ huấn luyện trọng số rõ ràng nào. Công trình này truyền cảm hứng cho sơ đồ masking của chúng tôi (có thể được diễn giải như tìm kiếm kiến trúc mạng neural ngầm (Liu et al., 2019c)): áp dụng các mask cho một mô hình ngôn ngữ tiền huấn luyện tương tự như finetuning, nhưng hiệu quả hơn nhiều về tham số.

Có lẽ công trình gần nhất, Mallya et al. (2018) áp dụng mask nhị phân cho CNN và đạt được hiệu suất tốt trong thị giác máy tính. Chúng tôi học các mask nhị phân có tính chọn lọc cho các mô hình ngôn ngữ tiền huấn luyện trong NLP và làm sáng tỏ các yếu tố quan trọng để có được hiệu suất tốt. Mallya et al. (2018) cập nhật rõ ràng các trọng số trong một lớp phân loại đặc thù cho tác vụ. Ngược lại, chúng tôi cho thấy rằng học end-to-end các mask có tính chọn lọc, một cách nhất quán cho cả mô hình ngôn ngữ tiền huấn luyện và một lớp phân loại được khởi tạo ngẫu nhiên, đạt được hiệu suất tốt.

Radiya-Dixit và Wang (2020) nghiên cứu finetuning của BERT bằng cách sử dụng một số kỹ thuật, bao gồm cái mà họ gọi là sparsification, một phương pháp tương tự masking. Trọng tâm của họ là phân tích finetuning BERT trong khi mục tiêu của chúng tôi là cung cấp một giải pháp thay thế hiệu quả cho finetuning.

3 Phương pháp
3.1 Nền tảng về Transformer và finetuning
Encoder của kiến trúc Transformer (Vaswani et al., 2017) được sử dụng phổ biến khi pretraining các mô hình ngôn ngữ lớn. Chúng tôi sơ lược xem xét kiến trúc của nó và sau đó trình bày sơ đồ masking của chúng tôi. Lấy BERT-base làm ví dụ, mỗi một trong 12 block transformer bao gồm (i) bốn lớp tuyến tính¹ WK, WQ, WV, và WAO để tính toán và xuất ra self attention giữa các wordpiece đầu vào (Wu et al., 2016). (ii) hai lớp tuyến tính WI và WO truyền thẳng biểu diễn từ tới block transformer tiếp theo.

Cụ thể hơn, xét một câu đầu vào X ∈ RN×d trong đó N là độ dài câu tối đa và d là kích thước chiều ẩn. WK, WQ, và WV được sử dụng để tính toán các phép biến đổi của X:
K = XWK; Q = XWQ; V = XWV;

¹Chúng tôi bỏ qua các bias term để ngắn gọn.

--- TRANG 3 ---
và self attention của X được tính như sau:
Attention(K; Q; V) = softmax(QKT/√d)V.

Attention sau đó được biến đổi bởi WAO, và tiếp theo được truyền thẳng bởi WI và WO tới block transformer tiếp theo.

Khi finetuning trên một tác vụ downstream như phân loại chuỗi, một lớp phân loại tuyến tính WT, chiếu từ chiều ẩn tới chiều đầu ra, được khởi tạo ngẫu nhiên. Tiếp theo, WT được xếp chồng lên trên một lớp tuyến tính tiền huấn luyện WP (lớp pooler). Tất cả các tham số sau đó được cập nhật để giảm thiểu mất mát tác vụ như cross-entropy.

3.2 Học mask
Với một mô hình ngôn ngữ tiền huấn luyện, chúng tôi không finetuning, tức là, chúng tôi không cập nhật các tham số tiền huấn luyện. Thay vào đó, chúng tôi chọn lọc một tập con của các tham số tiền huấn luyện quan trọng cho một tác vụ downstream trong khi loại bỏ những tham số không liên quan với các mask nhị phân. Chúng tôi liên kết mỗi lớp tuyến tính Wl ∈ {WlK, WlQ, WlV, WlAO, WlI, WlO} của block transformer thứ l với một ma trận giá trị thực Ml được khởi tạo ngẫu nhiên từ phân phối đều và có cùng kích thước với Wl. Sau đó chúng tôi truyền Ml qua một hàm ngưỡng theo từng phần tử (Hubara et al., 2016; Mallya et al., 2018), tức là, một binarizer, để có được một mask nhị phân Mlbin cho Wl:

(mlbin)i,j = {1 nếu mli,j ≥ θ, 0 ngược lại; (1)

trong đó mli,j ∈ Ml, i, j chỉ ra tọa độ của lớp tuyến tính 2-D và θ là một siêu tham số ngưỡng toàn cục.

Trong mỗi forward pass của huấn luyện, mask nhị phân Mlbin (được dẫn xuất từ Ml thông qua Eq. 1) chọn lọc trọng số trong một lớp tuyến tính tiền huấn luyện Wl bằng tích Hadamard:
Ŵl := Wl ⊙ Mlbin.

Trong backward pass tương ứng của huấn luyện, với hàm mất mát liên quan L, chúng ta không thể backpropagate qua binarizer, vì Eq. 1 là một thao tác ngưỡng cứng và gradient đối với Ml bằng không gần như mọi nơi. Tương tự như cách xử lý² trong Bengio et al. (2013); Hubara et al. (2016); Lin et al. (2020), chúng tôi sử dụng ∂L(Ŵl)/∂Mlbin như một estimator nhiễu của ∂L(Ŵl)/∂Ml để cập nhật Ml, tức là:

Ml ← Ml - α∂L(Ŵl)/∂Mlbin; (2)

trong đó α chỉ kích thước bước. Do đó, toàn bộ cấu trúc có thể được huấn luyện end-to-end.

Chúng tôi học một tập hợp mask nhị phân cho một tác vụ NLP như sau. Nhớ lại rằng mỗi lớp tuyến tính Wl được liên kết với một Ml để có được một lớp tuyến tính được mask Ŵl thông qua Eq. 1. Chúng tôi khởi tạo ngẫu nhiên một lớp tuyến tính bổ sung với một Ml liên quan và xếp chồng nó lên trên mô hình ngôn ngữ tiền huấn luyện. Sau đó chúng tôi cập nhật mỗi Ml thông qua Eq. 2 với mục tiêu tác vụ trong quá trình huấn luyện.

Sau khi huấn luyện, chúng tôi truyền mỗi Ml qua binarizer để có được Mlbin, sau đó được lưu cho suy luận trong tương lai. Vì Mlbin là nhị phân, nó chỉ chiếm 3% bộ nhớ so với việc lưu các tham số float 32-bit trong một mô hình đã finetuned. Ngoài ra, chúng tôi sẽ cho thấy rằng nhiều lớp – đặc biệt là lớp embedding – không cần phải được mask. Điều này càng giảm thêm tiêu thụ bộ nhớ của masking.

3.3 Cấu hình masking
Sơ đồ masking của chúng tôi được thúc đẩy bởi quan sát: các trọng số tiền huấn luyện tạo thành một khởi tạo tốt (Hao et al., 2019), nhưng vẫn cần một vài bước thích ứng để tạo ra hiệu suất cạnh tranh cho một tác vụ cụ thể. Tuy nhiên, không phải mọi tham số tiền huấn luyện đều cần thiết để đạt hiệu suất hợp lý, như được gợi ý bởi lĩnh vực pruning mạng neural (LeCun et al., 1990; Hassibi và Stork, 1993; Han et al., 2015b). Bây giờ chúng tôi nghiên cứu hai lựa chọn cấu hình ảnh hưởng đến số lượng tham số "đủ điều kiện" để masking.

Sparsity ban đầu của Mlbin. Khi chúng tôi khởi tạo ngẫu nhiên các mask từ phân phối đều, sparsity của mask nhị phân Mlbin trong giai đoạn khởi tạo mask kiểm soát có bao nhiêu tham số tiền huấn luyện trong một lớp Wl được giả định là không liên quan đến tác vụ downstream. Các tỷ lệ sparsity ban đầu khác nhau dẫn đến các hành vi tối ưu hóa khác nhau.

Việc hiểu rõ hơn cách sparsity ban đầu của một mask tác động đến động lực học huấn luyện và hiệu suất mô hình cuối cùng là rất quan trọng, để có thể tổng quát hóa sơ đồ masking của chúng tôi cho các miền và tác vụ rộng hơn. Trong §5.1, chúng tôi nghiên cứu khía cạnh này một cách chi tiết. Trong thực tế, chúng tôi cố định θ trong Eq. 1 trong khi điều chỉnh phân phối đều để đạt được sparsity ban đầu mục tiêu.

²Bengio et al. (2013); Hubara et al. (2016) mô tả nó như "straight-through estimator", và Lin et al. (2020) cung cấp đảm bảo hội tụ với diễn giải error feedback.

--- TRANG 4 ---
Lớp nào để mask. Các lớp khác nhau của mô hình ngôn ngữ tiền huấn luyện nắm bắt các khía cạnh riêng biệt của ngôn ngữ trong quá trình pretraining, ví dụ, Tenney et al. (2019) thấy rằng thông tin về gắn thẻ từ loại, phân tích cú pháp, nhận dạng thực thể có tên, vai trò ngữ nghĩa, và đồng tham chiếu được mã hóa trên các lớp cao hơn dần của BERT. Khó có thể biết trước loại tác vụ NLP nào phải được giải quyết trong tương lai, khiến việc quyết định các lớp để mask trở nên không tầm thường. Chúng tôi nghiên cứu yếu tố này trong §5.2.

Chúng tôi không học mask cho lớp embedding thấp nhất, tức là, các embedding wordpiece không có ngữ cảnh được "chọn" hoàn toàn, cho tất cả các tác vụ. Động lực có hai mặt. (i) Trọng số lớp embedding chiếm một phần lớn, ví dụ, gần 21% (23m/109m) trong BERT-base-uncased, của tổng số tham số. Không cần học mask có tính chọn lọc cho lớp này giảm tiêu thụ bộ nhớ. (ii) Pretraining đã mã hóa hiệu quả ý nghĩa chung độc lập ngữ cảnh của từ trong lớp embedding (Zhao et al., 2020). Do đó, việc học mask có tính chọn lọc cho lớp này là không cần thiết. Ngoài ra, chúng tôi không học mask cho bias và các tham số layer normalization vì chúng tôi không quan sát được tác động tích cực đến hiệu suất.

4 Datasets và Thiết lập
Datasets. Chúng tôi trình bày kết quả cho masking BERT, RoBERTa, và DistilBERT trong gắn thẻ từ loại, nhận dạng thực thể có tên, phân loại chuỗi, và hiểu đọc.

Chúng tôi thử nghiệm với gắn thẻ từ loại (POS) trên Penn Treebank (Marcus et al., 1993), sử dụng phân chia train/dev/test của Collins (2002). Đối với nhận dạng thực thể có tên (NER), chúng tôi tiến hành thử nghiệm trên tác vụ chia sẻ CoNLL-2003 NER (Tjong Kim Sang và De Meulder, 2003).

Đối với phân loại chuỗi, các tác vụ GLUE sau đây (Wang et al., 2018) được đánh giá: Stanford Sentiment Treebank (SST2) (Socher et al., 2013), Microsoft Research Paraphrase Corpus (MRPC) (Dolan và Brockett, 2005), Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019), Recognizing Textual Entailment (RTE) (Dagan et al., 2005), và Question Natural Language Inference (QNLI) (Rajpurkar et al., 2016).

Ngoài ra, chúng tôi thử nghiệm trên các dataset phân loại chuỗi có test set công khai: dataset phân loại câu hỏi 6-lớp TREC (Voorhees và Tice, 2000), dataset phân loại tin tức 4-lớp AG News (AG) (Zhang et al., 2015), và tác vụ phân loại cảm xúc Twitter nhị phân SemEval-2016 4B (SEM) (Nakov et al., 2016).

Chúng tôi thử nghiệm với hiểu đọc trên SWAG (Zellers et al., 2018) sử dụng các phân chia dữ liệu chính thức. Chúng tôi báo cáo hệ số tương quan Matthew (MCC) cho CoLA, micro-F1 cho NER, và độ chính xác cho các tác vụ khác.

Thiết lập. Do hạn chế tài nguyên và theo tinh thần trách nhiệm môi trường (Strubell et al., 2019; Schwartz et al., 2019), chúng tôi tiến hành thử nghiệm trên các mô hình base: BERT-base-uncased, RoBERTa-base, và DistilBERT-base-uncased. Do đó, các mô hình BERT/RoBERTa chúng tôi sử dụng có 12 block transformer (được đánh số 0–11) tạo ra vector 768-chiều; mô hình DistilBERT chúng tôi sử dụng có cùng chiều nhưng chứa 6 block transformer (được đánh số 0–5). Chúng tôi triển khai các mô hình của chúng tôi trong PyTorch (Paszke et al., 2019) với framework HuggingFace (Wolf et al., 2019).

Trong tất cả các thử nghiệm, chúng tôi giới hạn độ dài tối đa của một câu (cặp) là 128 sau tokenization wordpiece. Theo Devlin et al. (2019), chúng tôi sử dụng optimizer Adam (Kingma và Ba, 2014) mà learning rate là một siêu tham số trong khi các tham số khác giữ mặc định. Chúng tôi điều chỉnh cẩn thận learning rate cho mỗi thiết lập: quy trình điều chỉnh đảm bảo rằng learning rate tốt nhất không nằm ở biên của lưới tìm kiếm, nếu không chúng tôi mở rộng lưới tương ứng. Lưới ban đầu là {1e-5, 3e-5, 5e-5, 7e-5, 9e-5}.

Đối với phân loại chuỗi và hiểu đọc, chúng tôi sử dụng [CLS] làm biểu diễn của câu (cặp). Theo Devlin et al. (2019), chúng tôi công thức hóa NER như một tác vụ gắn thẻ và sử dụng một lớp đầu ra tuyến tính, thay vì một lớp conditional random field. Đối với thử nghiệm POS và NER, biểu diễn của một từ được tokenize là wordpiece cuối cùng của nó (Liu et al., 2019a; He và Choi, 2020). Lưu ý rằng độ dài tối đa 128 của một câu đối với POS và NER có nghĩa là một số chú thích từ-thẻ cần được loại trừ. Phụ lục §A trình bày checklist tái tạo của chúng tôi chứa thêm chi tiết triển khai và tiền xử lý.

5 Thử nghiệm
5.1 Sparsity ban đầu của mask nhị phân
Trước tiên chúng tôi nghiên cứu cách tỷ lệ phần trăm sparsity ban đầu (tức là, phần các số 0) của mask nhị phân Mlbin ảnh hưởng đến hiệu suất của một mô hình ngôn ngữ được mask nhị phân trên các tác vụ downstream. Chúng tôi thử nghiệm trên bốn tác vụ, với sparsity ban đầu trong {1%, 3%, 5%,

--- TRANG 5 ---
10%, 15%, 20%, . . . , 95%}. Tất cả các siêu tham số khác được kiểm soát: learning rate được cố định ở 5e-5; batch size là 32 cho các dataset tương đối nhỏ (RTE, MRPC, và CoLA) và 128 cho SST2. Mỗi thử nghiệm được lặp lại bốn lần với các random seed khác nhau {1, 2, 3, 4}. Trong thử nghiệm này, tất cả các block transformer, lớp pooler, và lớp phân loại đều được mask.

Hình 1 cho thấy rằng masking đạt được hiệu suất khá tốt mà không cần tìm kiếm siêu tham số. Cụ thể, (i) một sparsity ban đầu lớn loại bỏ hầu hết các tham số tiền huấn luyện, ví dụ, 95%, dẫn đến hiệu suất kém cho bốn tác vụ. Điều này là do kiến thức tiền huấn luyện bị loại bỏ phần lớn. (ii) Giảm dần sparsity ban đầu cải thiện hiệu suất tác vụ. Nói chung, một sparsity ban đầu trong 3%-10% mang lại kết quả hợp lý trên các tác vụ. Các dataset lớn như SST2 ít nhạy cảm hơn các dataset nhỏ như RTE. (iii) Chọn gần như tất cả các tham số tiền huấn luyện, ví dụ, 1% sparsity, làm tổn hại hiệu suất tác vụ. Nhớ lại rằng một mô hình tiền huấn luyện cần được thích ứng với một tác vụ downstream; masking đạt được việc thích ứng bằng cách học các mask có tính chọn lọc – bảo tồn quá nhiều tham số tiền huấn luyện trong khởi tạo cản trở tối ưu hóa.

5.2 Hành vi theo lớp
Các lớp mạng neural thể hiện các đặc điểm không đồng nhất (Zhang et al., 2019) khi được áp dụng cho các tác vụ. Ví dụ, thông tin cú pháp được biểu diễn tốt hơn ở các lớp thấp hơn trong khi thông tin ngữ nghĩa được nắm bắt ở các lớp cao hơn trong ELMo (Peters et al., 2018). Kết quả là, việc đơn giản mask tất cả các block transformer (như trong §5.1) có thể không lý tưởng.

Chúng tôi nghiên cứu hiệu suất tác vụ khi áp dụng mask cho các lớp BERT khác nhau. Hình 2 trình bày hiệu suất tác vụ tối ưu khi chỉ mask một tập con của các block transformer của BERT trên MRPC, CoLA, và RTE. Các số lượng và chỉ số khác nhau của các block transformer được mask: "bottom-up" và "top-down" chỉ ra việc mask số lượng block transformer mục tiêu, từ dưới lên hoặc từ trên xuống của BERT.

Chúng ta có thể quan sát rằng (i) trong hầu hết các trường hợp, masking top-down vượt trội hơn masking bottom-up khi sparsity ban đầu và số lượng lớp được mask được cố định. Do đó, việc chọn tất cả các trọng số tiền huấn luyện trong các lớp thấp hơn là hợp lý, vì chúng nắm bắt thông tin chung hữu ích và có thể chuyển giao cho các tác vụ khác nhau (Liu et al., 2019a; Howard và Ruder, 2018). (ii) Đối với masking bottom-up, việc tăng số lượng lớp được mask dần cải thiện hiệu suất. Quan sát này minh họa sự phụ thuộc giữa các lớp BERT và động lực học của masking: được cung cấp các trọng số tiền huấn luyện được chọn trong các lớp thấp hơn, các lớp cao hơn cần được trao sự linh hoạt để chọn các trọng số tiền huấn luyện tương ứng để đạt hiệu suất tác vụ tốt. (iii) Trong masking top-down, hiệu suất CoLA tăng khi mask một số lượng lớp ngày càng tăng trong khi MRPC và RTE không nhạy cảm. Nhớ lại rằng CoLA kiểm tra tính chấp nhận được về mặt ngôn ngữ thường đòi hỏi cả thông tin cú pháp và ngữ nghĩa³. Tất cả các lớp BERT đều tham gia vào việc biểu diễn thông tin này, do đó cho phép nhiều lớp hơn thay đổi sẽ cải thiện hiệu suất.

5.3 So sánh finetuning và masking
Chúng tôi đã nghiên cứu hai yếu tố – sparsity ban đầu (§5.1) và hành vi theo lớp (§5.2) – quan trọng trong masking các mô hình ngôn ngữ tiền huấn luyện. Ở đây, chúng tôi so sánh hiệu suất và tiêu thụ bộ nhớ của masking và finetuning.

Dựa trên các quan sát trong §5.1 và §5.2, chúng tôi sử dụng 5% sparsity ban đầu khi áp dụng masking cho BERT, RoBERTa, và DistilBERT. Chúng tôi mask các block transformer 2–11 trong BERT/RoBERTa và 2–5 trong DistilBERT. WP và WT luôn được mask. Lưu ý rằng thiết lập toàn cục này chắc chắn là không tối ưu cho một số kết hợp mô hình-tác vụ, nhưng mục tiêu của chúng tôi là minh họa hiệu quả và khả năng tổng quát hóa của masking. Do đó, việc tiến hành tìm kiếm siêu tham số mở rộng là không cần thiết.

Đối với AG và QNLI, chúng tôi sử dụng batch size 128. Đối với các tác vụ khác, chúng tôi sử dụng batch size 32. Chúng tôi tìm kiếm learning rate tối ưu cho mỗi tác vụ như mô tả trong §4,

³Ví dụ, để phân biệt các cấu trúc caused-motion chấp nhận được (ví dụ, "the professor talked us into a stupor") khỏi những cấu trúc không chấp nhận được (ví dụ, "water talked it into red"), cả thông tin cú pháp và ngữ nghĩa đều cần được xem xét (Goldberg, 1995).

[HÌNH: Một biểu đồ cho thấy hiệu suất dev set của masking BERT khi chọn các lượng khác nhau của tham số tiền huấn luyện, với Initial Mask Sparsity (%) trên trục x và Task Performance trên trục y, hiển thị kết quả cho RTE, MRPC, CoLA, và SST2]

--- TRANG 6 ---
[HÌNH: Ba biểu đồ cho thấy tác động của masking các block transformer khác nhau của BERT cho MRPC (trái), CoLA (giữa), và RTE (phải). Trục x hiển thị số block được mask; số đó được mask "bottom-up" hoặc "top-down".]

và chúng được hiển thị trong Phụ lục §A.4.

So sánh hiệu suất. Bảng 1 báo cáo hiệu suất của masking và finetuning trên dev set cho mười một tác vụ NLP. Chúng tôi quan sát rằng việc áp dụng masking cho BERT/RoBERTa/DistilBERT mang lại hiệu suất có thể so sánh với finetuning. Chúng tôi quan sát một sự sụt giảm hiệu suất⁴ trên RoBERTa-RTE. RTE có kích thước dataset nhỏ nhất (train: 2.5k; dev: 0.3k) trong tất cả các tác vụ – điều này có thể góp phần vào kết quả không hoàn hảo và độ lệch lớn.

Kết quả BERT-NER của chúng tôi hơi kém hơn Devlin et al. (2019). Điều này có thể do "maximal document context" được sử dụng bởi Devlin et al. (2019) trong khi chúng tôi sử dụng ngữ cảnh cấp câu với độ dài chuỗi tối đa 128⁵.

Các hàng "Single" trong Bảng 2 so sánh hiệu suất của masking và finetuning BERT trên test set của SEM, TREC, AG, POS, và NER. Cùng thiết lập và tìm kiếm siêu tham số như Bảng 1 được sử dụng, các siêu tham số tốt nhất được chọn trên dev set. Kết quả từ Sun et al. (2019); Palogiannidi et al. (2016) được bao gồm làm tham khảo. Sun et al. (2019) sử dụng các tối ưu hóa như learning rate theo lớp, tạo ra hiệu suất hơi tốt hơn kết quả của chúng tôi. Palogiannidi et al. (2016) là hệ thống hoạt động tốt nhất trên tác vụ SEM (Nakov et al., 2016). Một lần nữa, masking mang lại kết quả có thể so sánh với finetuning.

So sánh bộ nhớ. Sau khi cho thấy rằng hiệu suất tác vụ của masking và finetuning có thể so sánh được, tiếp theo chúng tôi chứng minh một điểm mạnh chính của masking: hiệu quả bộ nhớ. Chúng tôi lấy BERT-base-uncased làm ví dụ. Hình 3 cho thấy số lượng tham số tích lũy tính bằng triệu và bộ nhớ tính bằng megabyte (MB) cần thiết khi một số lượng tác vụ downstream ngày càng tăng cần được giải quyết bằng finetuning và masking. Masking cần

[BẢNG: Hiển thị hiệu suất tác vụ dev set (%) của masking và finetuning cho BERT, RoBERTa, và DistilBERT trên nhiều tác vụ khác nhau]

[HÌNH: Hai biểu đồ con (a) và (b) cho thấy số lượng tham số tích lũy và tiêu thụ bộ nhớ theo masking và finetuning khi số lượng tác vụ tăng]

⁴Các quan sát tương tự đã được thực hiện: DistilBERT có sụt giảm độ chính xác 10% trên RTE so với BERT-base (Sanh et al., 2019); Sajjad et al. (2020) báo cáo sự bất ổn trên MRPC và RTE khi áp dụng các chiến lược giảm mô hình của họ.

⁵Các quan sát tương tự đã được thực hiện: https://github.com/huggingface/transformers/issues/64

--- TRANG 7 ---
[BẢNG: Bảng 2 cho thấy tỷ lệ lỗi (%) trên test set và so sánh kích thước mô hình, bao gồm Single và Ensem. cho cả Masking và Finetuning]

một overhead nhỏ khi giải quyết một tác vụ đơn lẻ nhưng hiệu quả hơn nhiều so với finetuning khi cần suy luận cho nhiều tác vụ. Masking lưu một bản sao duy nhất của một mô hình ngôn ngữ tiền huấn luyện chứa các tham số float 32-bit cho tất cả mười một tác vụ và một tập hợp mask nhị phân 1-bit cho mỗi tác vụ. Ngược lại, finetuning lưu mọi mô hình đã finetuned nên tiêu thụ bộ nhớ tăng tuyến tính.

Masking tự nhiên cho phép ensemble nhẹ của các mô hình. Các hàng "Ensem." trong Bảng 2 so sánh kết quả ensemble và kích thước mô hình. Chúng tôi xem xét ensemble của (i) nhãn dự đoán; (ii) logit; (iii) xác suất. Phương pháp ensemble tốt nhất được chọn trên dev và sau đó đánh giá trên test. Masking chỉ tiêu thụ 474MB bộ nhớ – nhỏ hơn nhiều so với 1752MB cần thiết cho finetuning – và đạt hiệu suất có thể so sánh. Do đó, masking cũng hiệu quả hơn nhiều về bộ nhớ so với finetuning trong một thiết lập ensemble.

6 Thảo luận
6.1 Đánh giá nội tại
§5 chứng minh rằng masking là một giải pháp thay thế hiệu quả cho finetuning. Bây giờ chúng tôi phân tích các thuộc tính của biểu diễn được tính toán bởi các mô hình ngôn ngữ được mask nhị phân với đánh giá nội tại.

Một thuộc tính hấp dẫn của finetuning, tức là xếp chồng một lớp phân loại lên trên một mô hình ngôn ngữ tiền huấn luyện rồi cập nhật tất cả các tham số, là một lớp phân loại tuyến tính đủ để tiến hành phân loại khá chính xác. Quan sát này ngụ ý rằng cấu hình của các điểm dữ liệu, ví dụ, các câu có cảm xúc tích cực hoặc tiêu cực trong SST2, nên gần với khả năng tách tuyến tính trong không gian ẩn.

Giống như finetuning, masking cũng sử dụng một lớp phân loại tuyến tính. Do đó, chúng tôi giả thuyết rằng các lớp trên của các mô hình ngôn ngữ được mask nhị phân, ngay cả khi không có cập nhật trọng số rõ ràng, cũng tạo ra một không gian ẩn trong đó các điểm dữ liệu gần với khả năng tách tuyến tính.

Hình 4 sử dụng t-SNE (Maaten và Hinton, 2008) để trực quan hóa biểu diễn của [CLS] được tính toán bởi block transformer trên cùng trong BERT/RoBERTa tiền huấn luyện, đã finetuned, và được mask, sử dụng các ví dụ dev set của SST2. Biểu diễn của các mô hình tiền huấn luyện (trái) rõ ràng không thể tách được vì mô hình cần được thích ứng với các tác vụ downstream. Biểu diễn câu được tính toán bởi encoder đã finetuned (trên phải) và được mask nhị phân (dưới phải) gần như có thể tách tuyến tính và nhất quán với các nhãn gold. Do đó, một phân loại tuyến tính được mong đợi sẽ mang lại độ chính xác phân loại khá tốt. Đánh giá nội tại này minh họa rằng các mô hình được mask nhị phân trích xuất biểu diễn tốt từ dữ liệu cho tác vụ NLP downstream.

[HÌNH: Hình 4 cho thấy trực quan hóa t-SNE của biểu diễn [CLS] cho BERT và RoBERTa trong các trạng thái khác nhau]

6.2 Thuộc tính của các mô hình được mask nhị phân
Các mô hình được mask nhị phân có tổng quát hóa không? Hình 4 cho thấy rằng một mô hình ngôn ngữ được mask nhị phân tạo ra biểu diễn phù hợp cho lớp phân loại và do đó hoạt động tốt như một mô hình đã finetuned. Ở đây, chúng tôi quan tâm đến việc xác minh rằng

[BẢNG: Bảng 3 cho thấy khả năng tổng quát hóa trên dev (%) của BERT được mask nhị phân và đã finetuned]

--- TRANG 8 ---
[HÌNH: Hình 5 cho thấy điểm số s của hai tập hợp mask, được huấn luyện với hai tác vụ khác nhau, của lớp WO trong các block transformer 2 (trái) và 11 (phải) trong BERT]

mô hình được mask nhị phân thực sự giải quyết các tác vụ downstream bằng cách học các biểu diễn có ý nghĩa – thay vì khai thác các tương quan giả tạo tổng quát hóa kém (Niven và Kao, 2019; McCoy et al., 2019). Để làm điều này, chúng tôi kiểm tra liệu mô hình được mask nhị phân có thể tổng quát hóa cho các dataset khác cùng loại tác vụ downstream không. Chúng tôi sử dụng hai dataset phân loại cảm xúc: SST2 và SEM. Chúng tôi đơn giản đánh giá mô hình được mask hoặc finetuned trên SST2 đối với dev set của SEM và ngược lại. Bảng 3 báo cáo kết quả so với baseline majority-vote. Các mô hình finetuned và được mask nhị phân của SEM tổng quát hóa tốt trên SST2, cho thấy cải thiện 20% so với baseline majority-vote.

Mặt khác, chúng tôi quan sát rằng kiến thức được học trên SST2 không tổng quát hóa cho SEM, cho cả finetuning và masking. Chúng tôi giả thuyết rằng điều này là do miền Twitter (SEM) cụ thể hơn nhiều so với đánh giá phim (SST2). Ví dụ, một số Emoji hoặc ký hiệu như ":)" phản ánh cảm xúc mạnh không xuất hiện trong SST2, dẫn đến tổng quát hóa không thành công. Để kiểm tra giả thuyết của chúng tôi, chúng tôi lấy một dataset đánh giá phim khác IMDB (Maas et al., 2011), và áp dụng trực tiếp các mô hình SST2-finetuned- và SST2-binary-masked- trên nó. Masking và finetuning đạt độ chính xác 84.79% và 85.25%, có thể so sánh và cả hai đều vượt trội baseline 50%, chứng minh chuyển giao kiến thức thành công.

Do đó, finetuning và masking tạo ra các mô hình với khả năng tổng quát hóa tương tự. Các mô hình được mask nhị phân thực sự tạo ra biểu diễn chứa thông tin hợp lệ cho các tác vụ downstream.

Phân tích mask. Chúng tôi nghiên cứu sự khác biệt giữa các mask được học bởi các lớp BERT và tác vụ downstream khác nhau. Đối với mask nhị phân ban đầu và đã huấn luyện Mt,initbin và Mt,trainedbin của một lớp được huấn luyện trên tác vụ t ∈ {t1,t2}. Chúng tôi tính toán:

s = ||Mt1,trainedbin - Mt2,trainedbin||1 / (||Mt1,trainedbin - Mt1,initbin||1 + ||Mt2,trainedbin - Mt2,initbin||1),

trong đó ||W||1 = Σmi=1Σnj=1|wi,j|. Lưu ý rằng với cùng random seed, Mt1,initbin và Mt2,initbin là giống nhau. Độ khác biệt s đo lường sự khác nhau giữa hai mask như một phần của tất cả thay đổi được tạo ra bởi huấn luyện. Hình 5 cho thấy rằng, sau khi huấn luyện, độ khác biệt của mask của các lớp BERT cao hơn lớn hơn so với các lớp BERT thấp hơn. Các quan sát tương tự được thực hiện cho finetuning: trọng số lớp trên trong BERT đã finetuned đặc thù cho tác vụ hơn (Kovaleva et al., 2019). Hình cũng cho thấy rằng các mask đã học cho các tác vụ downstream có xu hướng khác biệt với nhau, ngay cả đối với các tác vụ tương tự. Đối với một tác vụ nhất định, tồn tại các tập hợp mask khác nhau (được khởi tạo với các random seed khác nhau) mang lại hiệu suất tương tự. Quan sát này tương tự như kết quả đánh giá giả thuyết lottery ticket trên BERT (Prasanna et al., 2020; Chen et al., 2020): một số subnetwork tồn tại trong BERT đạt được hiệu suất tác vụ tương tự.

6.3 Landscape mất mát
Huấn luyện các mạng neural phức tạp có thể được xem như tìm kiếm các minima tốt trong landscape rất phi lồi được định nghĩa bởi hàm mất mát (Li et al., 2018). Các minima tốt thường được mô tả như các điểm ở đáy của các thung lũng lồi cục bộ khác nhau (Keskar et al., 2016; Draxler et al., 2018), đạt được hiệu suất tương tự. Trong phần này, chúng tôi nghiên cứu mối quan hệ giữa hai minima thu được bởi masking và finetuning.

Nghiên cứu gần đây phân tích landscape mất mát gợi ý rằng các minima cục bộ trong landscape mất mát đạt được bởi các thuật toán huấn luyện tiêu chuẩn có thể được kết nối bằng một đường dẫn đơn giản (Garipov et al., 2018; Gotmare et al., 2018), ví dụ, một đường cong Bézier, với mất mát tác vụ thấp (hoặc độ chính xác tác vụ cao) dọc theo đường dẫn. Chúng tôi quan tâm đến việc kiểm tra liệu hai minima được tìm thấy bởi finetuning và masking có thể được kết nối dễ dàng trong landscape mất mát không. Để bắt đầu, chúng tôi xác minh hiệu suất tác vụ của một mô hình nội suy W(α) trên đoạn thẳng giữa một mô hình đã finetuned W0 và một mô hình được mask nhị phân W1:

W(α) = W0 + α(W1 - W0); 0 ≤ α ≤ 1.

Chúng tôi tiến hành thử nghiệm trên MRPC và SST2 với các mô hình BERT và RoBERTa hoạt động tốt nhất

--- TRANG 9 ---
[HÌNH: Kết quả kết nối mode trên MRPC (trái) và SST2 (phải). Hình trên: độ chính xác dev set của một mô hình nội suy giữa hai minima được tìm thấy bởi finetuning (α=0) và masking (α=1). Hình dưới: độ chính xác của một mô hình nội suy giữa BERT tiền huấn luyện (α=0) và finetuned/masked (α=1).]

thu được trong Bảng 1 (cùng seed và epoch huấn luyện); Hình 6 (trên) cho thấy kết quả của kết nối mode, tức là sự tiến triển của độ chính xác tác vụ dọc theo một đường thẳng kết nối hai minima ứng viên.

Đáng ngạc nhiên, các mô hình nội suy trên đoạn thẳng kết nối một mô hình đã finetuned và một mô hình được mask nhị phân tạo thành một đường dẫn độ chính xác cao, chỉ ra landscape mất mát được kết nối cực kỳ tốt. Do đó, masking tìm thấy minima trên cùng manifold mất mát thấp được kết nối như finetuning, xác nhận hiệu quả của phương pháp chúng tôi. Ngoài ra, chúng tôi cho thấy trong Hình 6 (dưới) cho đoạn thẳng giữa BERT tiền huấn luyện và BERT đã finetuned/masked, rằng kết nối mode không chỉ đơn thuần do một mô hình ngôn ngữ tiền huấn luyện có quá nhiều tham số. Thử nghiệm đường cong Bézier cho thấy kết quả tương tự, xem Phụ lục §B.

7 Kết luận
Chúng tôi đã trình bày masking, một giải pháp thay thế hiệu quả cho finetuning để sử dụng các mô hình ngôn ngữ tiền huấn luyện như BERT/RoBERTa/DistilBERT. Thay vì cập nhật các tham số tiền huấn luyện, chúng tôi chỉ huấn luyện một tập hợp mask nhị phân cho mỗi tác vụ để chọn lọc các tham số quan trọng. Các thử nghiệm mở rộng cho thấy rằng masking mang lại hiệu suất có thể so sánh với finetuning trên một loạt tác vụ NLP. Giữ nguyên các tham số tiền huấn luyện, masking hiệu quả hơn nhiều về bộ nhớ khi cần giải quyết nhiều tác vụ. Đánh giá nội tại cho thấy rằng các mô hình được mask nhị phân trích xuất biểu diễn hợp lệ và có thể tổng quát hóa cho các tác vụ downstream. Hơn nữa, chúng tôi chứng minh rằng các minima thu được bởi finetuning và masking có thể được kết nối dễ dàng bằng một đoạn thẳng, xác nhận hiệu quả của việc áp dụng masking cho các mô hình ngôn ngữ tiền huấn luyện.

Mã của chúng tôi có sẵn tại: https://github.com/ptlmasking/maskbert.

Nghiên cứu tương lai có thể khám phá khả năng áp dụng masking cho các encoder đa ngôn ngữ tiền huấn luyện như mBERT (Devlin et al., 2019) và XLM (Conneau và Lample, 2019). Ngoài ra, các mask nhị phân được học bởi phương pháp của chúng tôi có sparsity thấp sao cho tốc độ suy luận không được cải thiện. Phát triển các phương pháp cải thiện cả hiệu quả bộ nhớ và suy luận mà không hy sinh hiệu suất tác vụ có thể mở ra khả năng triển khai rộng rãi các mô hình ngôn ngữ tiền huấn luyện mạnh mẽ cho nhiều ứng dụng NLP hơn.

Lời cảm ơn
Chúng tôi cảm ơn các reviewer ẩn danh cho những nhận xét và gợi ý sâu sắc. Công trình này được tài trợ bởi European Research Council (ERC #740516), grant SNSF 200021_175796, cũng như Google Focused Research Award.

Tài liệu tham khảo
Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation.

Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. 2020. The lottery ticket hypothesis for pre-trained bert networks. arXiv preprint arXiv:2007.12223.

Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 1–8. Association for Computational Linguistics.

Alexis Conneau and Guillaume Lample. 2019. Cross-lingual language model pretraining. In Advances in Neural Information Processing Systems, pages 7059–7069.

Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177–190. Springer.

Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,

--- TRANG 10 ---
Advances in Neural Information Processing Systems 28, pages 3079–3087. Curran Associates, Inc.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping.

William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).

Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred A Hamprecht. 2018. Essentially no barriers in neural network energy landscape. arXiv preprint arXiv:1803.00885.

Jonathan Frankle and Michael Carbin. 2018. The lottery ticket hypothesis: Finding sparse, trainable neural networks.

Adam Gaier and David Ha. 2019. Weight agnostic neural networks. In Advances in Neural Information Processing Systems, pages 5365–5379.

Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. 2018. Loss surfaces, mode connectivity, and fast ensembling of dnns. In Advances in Neural Information Processing Systems, pages 8789–8798.

Adele E Goldberg. 1995. Construction grammar. Wiley.

Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation. arXiv preprint arXiv:1810.13243.

Song Han, Jeff Pool, John Tran, and William Dally. 2015a. Learning both weights and connections for efficient neural network. In NeurIPS - Advances in Neural Information Processing Systems, pages 1135–1143.

Song Han, Jeff Pool, John Tran, and William Dally. 2015b. Learning both weights and connections for efficient neural network. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1135–1143. Curran Associates, Inc.

Yaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visualizing and understanding the effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4143–4152, Hong Kong, China. Association for Computational Linguistics.

Babak Hassibi and David G. Stork. 1993. Second order derivatives for network pruning: Optimal brain surgeon. In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, Advances in Neural Information Processing Systems 5, pages 164–171. Morgan-Kaufmann.

Han He and Jinho D. Choi. 2020. Establishing Strong Baselines for the New Decade: Sequence Tagging, Syntactic and Semantic Parsing with BERT. In Proceedings of the 33rd International Florida Artificial Intelligence Research Society Conference, FLAIRS'20. Best Paper Candidate.

Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. 2018. Soft filter pruning for accelerating deep convolutional neural networks. In International Joint Conference on Artificial Intelligence (IJCAI), pages 2234–2240.

Geoffrey Hinton. 2012. Neural networks for machine learning.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790–2799, Long Beach, California, USA. PMLR.

Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328–339, Melbourne, Australia. Association for Computational Linguistics.

Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. Binarized neural networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 4107–4115. Curran Associates, Inc.

Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2017. Quantized neural networks: Training neural networks with low precision weights and activations. The Journal of Machine Learning Research, 18(1):6869–6898.

Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. 2016. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836.

--- TRANG 11 ---
Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.

Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4356–4365, Hong Kong, China. Association for Computational Linguistics.

Yann LeCun, John S. Denker, and Sara A. Solla. 1990. Optimal brain damage. In D. S. Touretzky, editor, Advances in Neural Information Processing Systems 2, pages 598–605. Morgan-Kaufmann.

Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. 2019. SNIP: Single-shot network pruning based on connection sensitivity. In ICLR - International Conference on Learning Representations.

Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018. Visualizing the loss landscape of neural nets. In Advances in Neural Information Processing Systems, pages 6389–6399.

Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. 2020. Dynamic model pruning with feedback. In International Conference on Learning Representations.

Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019a. Linguistic knowledge and transferability of contextual representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1073–1094, Minneapolis, Minnesota. Association for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. Roberta: A robustly optimized bert pretraining approach.

Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. 2019c. Rethinking the value of network pruning. In ICLR - International Conference on Learning Representations.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA. Association for Computational Linguistics.

Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579–2605.

Arun Mallya, Dillon Davis, and Svetlana Lazebnik. 2018. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In The European Conference on Computer Vision (ECCV).

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.

Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428–3448, Florence, Italy. Association for Computational Linguistics.

Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Sebastiani, and Veselin Stoyanov. 2016. SemEval-2016 task 4: Sentiment analysis in twitter. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1–18, San Diego, California. Association for Computational Linguistics.

Timothy Niven and Hung-Yu Kao. 2019. Probing neural network comprehension of natural language arguments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658–4664, Florence, Italy. Association for Computational Linguistics.

Elisavet Palogiannidi, Athanasia Kolovou, Fenia Christopoulou, Filippos Kokkinos, Elias Iosif, Nikolaos Malandrakis, Haris Papageorgiou, Shrikanth Narayanan, and Alexandros Potamianos. 2016. Tweester at SemEval-2016 task 4: Sentiment analysis in twitter using semantic-affective model adaptation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 155–163, San Diego, California. Association for Computational Linguistics.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke

--- TRANG 12 ---
Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227–2237, New Orleans, Louisiana. Association for Computational Linguistics.

Matthew E. Peters, Sebastian Ruder, and Noah A. Smith. 2019. To tune or not to tune? adapting pretrained representations to diverse tasks. In Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 7–14, Florence, Italy. Association for Computational Linguistics.

Sai Prasanna, Anna Rogers, and Anna Rumshisky. 2020. When BERT plays the lottery, all tickets are winning.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.

Evani Radiya-Dixit and Xin Wang. 2020. How fine can fine-tuning be? learning efficient language models. volume 108 of Proceedings of Machine Learning Research, pages 2435–2443, Online. PMLR.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas. Association for Computational Linguistics.

Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016. Xnor-net: Imagenet classification using binary convolutional neural networks. In European conference on computer vision, pages 525–542. Springer.

Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. 2020. Poor man's bert: Smaller and faster transformer models. arXiv preprint arXiv:2004.03844.

Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.

Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 2019. Green ai.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA. Association for Computational Linguistics.

Asa Cooper Stickland and Iain Murray. 2019. BERT and PALs: Projected attention layers for efficient adaptation in multi-task learning. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5986–5995, Long Beach, California, USA. PMLR.

Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3645–3650, Florence, Italy. Association for Computational Linguistics.

Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to fine-tune bert for text classification? In China National Conference on Chinese Computational Linguistics, pages 194–206. Springer.

Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593–4601, Florence, Italy. Association for Computational Linguistics.

Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142–147.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008.

Ellen Voorhees and Dawn Tice. 2000. The trec-8 question answering track evaluation. Proceedings of the 8th Text Retrieval Conference.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, pages 3261–3275.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium. Association for Computational Linguistics.

Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625–641.

--- TRANG 13 ---
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding.

Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93–104, Brussels, Belgium. Association for Computational Linguistics.

Chiyuan Zhang, Samy Bengio, and Yoram Singer. 2019. Are all layers created equal? arXiv preprint arXiv:1902.01996.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 649–657. Curran Associates, Inc.

Mengjie Zhao, Philipp Dufter, Yadollah Yaghoobzadeh, and Hinrich Schütze. 2020. Quantifying the contextualization of word representations with semantic class probing. In Findings of EMNLP.

Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. 2019. Deconstructing lottery tickets: Zeros, signs, and the supermask. In Advances in Neural Information Processing Systems, pages 3592–3602.

--- TRANG 14 ---
A Checklist Tái tạo
A.1 Cơ sở hạ tầng tính toán
Tất cả các thử nghiệm được tiến hành trên các mô hình GPU sau: Tesla V100, GeForce GTX 1080 Ti, và GeForce GTX 1080. Chúng tôi sử dụng kích thước batch per-GPU là 32. Do đó, các thử nghiệm so sánh masking và finetuning trên QNLI và AG sử dụng 4 GPU và tất cả các tác vụ khác sử dụng một GPU duy nhất.

A.2 Số lượng tham số
Trong §5.3 chúng tôi so sánh kỹ lưỡng số lượng tham số và tiêu thụ bộ nhớ của finetuning và masking. Các giá trị số được trong Bảng 8.

A.3 Hiệu suất validation
Hiệu suất dev set của Bảng 2 được đề cập trong Bảng 1. Chúng tôi báo cáo hệ số tương quan Matthew (MCC) cho CoLA, micro-F1 cho NER, và độ chính xác cho các tác vụ khác. Chúng tôi sử dụng các hàm đánh giá trong scikit-learn (Pedregosa et al., 2011) và seqeval (https://github.com/chakki-works/seqeval).

A.4 Tìm kiếm siêu tham số
Siêu tham số duy nhất chúng tôi tìm kiếm là learning rate, cho cả masking và finetuning, theo thảo luận thiết lập trong §4. Các giá trị tối ưu có trong Bảng 4.

A.5 Datasets
Đối với các tác vụ GLUE, chúng tôi sử dụng các dataset chính thức từ benchmark https://gluebenchmark.com/. Đối với TREC và AG, chúng tôi tải xuống các dataset được phát triển bởi Zhang et al. (2015), có sẵn tại đây. Lưu ý rằng liên kết này được cung cấp bởi Zhang et al. (2015) và cũng được sử dụng bởi Sun et al. (2019). Đối với SEM, chúng tôi có được dataset từ trang web SemEval chính thức: http://alt.qcri.org/semeval2016/task4/. Đối với NER, chúng tôi sử dụng dataset chính thức: https://www.clips.uantwerpen.be/conll2003/ner/. Chúng tôi có được dataset POS của chúng tôi từ linguistic data consortium (LDC). Chúng tôi sử dụng dataset chính thức của SWAG (Zellers et al., 2018): https://github.com/rowanz/swagaf/tree/master/data.

Đối với POS, các phần 0-18 của WSJ là train, các phần 19-21 là dev, và các phần 22-24 là test (Collins, 2002). Chúng tôi sử dụng các phân chia train/dev/test chính thức của tất cả các dataset khác.

Để tiền xử lý các dataset, chúng tôi sử dụng các tokenizer được cung cấp bởi gói Transformers (Wolf et al., 2019) để chuyển đổi dataset thô sang các định dạng yêu cầu bởi BERT/RoBERTa/DistilBERT. Vì tokenization wordpiece được sử dụng, không có từ ngoài từ vựng.

Vì chúng tôi sử dụng độ dài chuỗi tối đa là 128, các bước tiền xử lý của chúng tôi loại trừ một số chú thích từ-thẻ trong POS và NER. Đối với POS, sau tokenization wordpiece, chúng tôi thấy 1 câu trong dev và 2 câu trong test có nhiều hơn 126 (cần xem xét [CLS] và [SEP]) wordpiece. Kết quả là, chúng tôi loại trừ 5 từ được chú thích trong dev và 87 từ được chú thích trong test. Tương tự, đối với NER (cũng được công thức hóa như một tác vụ gắn thẻ theo Devlin et al. (2019)), chúng tôi thấy 3 câu trong dev và 1 câu trong test có nhiều hơn 126 wordpiece. Kết quả là, chúng tôi loại trừ 27 từ được chú thích trong dev và 8 từ được chú thích trong test.

Số lượng ví dụ trong dev và test cho mỗi tác vụ được hiển thị trong Bảng 5 sau đây.

B Thêm về Kết nối Mode
Theo framework kết nối mode được đề xuất trong Garipov et al. (2018), chúng tôi tham số hóa đường dẫn nối hai minima bằng đường cong Bézier. Gọi w0 và wn+1 là các tham số của các mô hình được huấn luyện từ finetuning và masking. Sau đó, một đường cong Bézier n-bend kết nối w0 và wn+1, với n mô hình trung gian có thể huấn luyện = {w1, ..., wn}, có thể được biểu diễn bởi β(t), sao cho β(0) = w0 và β(1) = wn+1, và

β(t) = Σi=0^(n+1) (n+1 choose i)(1-t)^(n+1-i)t^i wi.

Chúng tôi huấn luyện một đường cong Bézier 3-bend bằng cách giảm thiểu mất mát Et~U[0,1]L(β(t)), trong đó U[0,1] là phân phối đều trong khoảng [0,1]. Phương pháp Monte Carlo được sử dụng để ước lượng gradient của hàm dựa trên kỳ vọng này và tối ưu hóa dựa trên gradient được sử dụng cho việc giảm thiểu. Kết quả được minh họa trong Hình 7. Masking thực hiện ngầm gradient descent, tương tự như cập nhật trọng số đạt được bởi finetuning; các quan sát bổ sung cho lập luận của chúng tôi trong văn bản chính.

C Thêm Kết quả Thực nghiệm
Kết quả ensemble của RoBERTa và DistilBERT. Bảng 6 sau đây cho thấy kết quả single và ensemble của RoBERTa và DistilBERT trên test set của SEM, TREC, AG, POS, và NER.

--- TRANG 15 ---
[BẢNG: Bảng 4 cho thấy learning rate tối ưu trên các tác vụ khác nhau cho BERT/RoBERTa/DistilBERT]

[HÌNH: Hình 7 cho thấy độ chính xác trên MRPC dev set, như một hàm của điểm trên các đường cong (α), kết nối hai minima được tìm thấy bởi finetuning và masking]

[BẢNG: Bảng 5 cho thấy số lượng ví dụ trong dev và test cho mỗi tác vụ]

D Giá trị Số của Các Biểu đồ
D.1 Hành vi theo lớp
Bảng 7 chi tiết các giá trị số của Hình 2.

[BẢNG: Bảng 6 cho thấy tỷ lệ lỗi (%) trên test set của các tác vụ bởi RoBERTa và DistilBERT]

D.2 Tiêu thụ bộ nhớ
Bảng 8 chi tiết các giá trị số của Hình 3.

--- TRANG 16 ---
[BẢNG: Bảng 7 cho thấy giá trị số của thử nghiệm hành vi theo lớp với các cấu hình masking khác nhau cho BERT]

[BẢNG: Bảng 8 cho thấy so sánh kích thước mô hình khi áp dụng masking và finetuning, với các số liệu chi tiết về số lượng tham số và sử dụng bộ nhớ]
