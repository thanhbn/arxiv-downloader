# 2203.07259.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2203.07259.pdf
# File size: 634868 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
The Optimal BERT Surgeon: Scalable and Accurate Second-Order
Pruning for Large Language Models
Eldar Kurtic1, Daniel Campos2,3, Tuan Nguyen2, Elias Frantar1, Mark Kurtz2,
Benjamin Fineran2, Michael Goin2, and Dan Alistarh1,2
1Institute of Science and Technology Austria
2Neural Magic Inc.
3Department of Computer Science, University of Illinois Urbana-Champaign
Abstract
In this paper, we consider the problem of
sparsifying BERT models, which are a key
building block for natural language process-
ing, in order to reduce their storage and
computational cost. We introduce the Optimal
BERT Surgeon (oBERT), an efﬁcient and ac-
curate pruning method based on approximate
second-order information, which we show to
yield state-of-the-art results for compression
in both stages of language tasks: pre-training
and ﬁne-tuning. Speciﬁcally, oBERT extends
existing work on second-order pruning by
allowing for pruning blocks of weights, and
is the ﬁrst such method that is applicable at
BERT scale. Second, we investigate com-
pounding compression approaches to obtain
highly compressed but accurate models for
deployment on edge devices. These models
signiﬁcantly push boundaries of the current
state-of-the-art sparse BERT models with
respect to all metrics: model size, inference
speed and task accuracy. For example, rela-
tive to the dense BERT BASE , we obtain 10x
model size compression with < 1% accuracy
drop, 10x CPU-inference speedup with
< 2% accuracy drop, and 29x CPU-inference
speedup with < 7.5% accuracy drop. Our
code, fully integrated with Transformers and
SparseML, is available at https://github.
com/neuralmagic/sparseml/tree/main/
research/optimal_BERT_surgeon_oBERT .
1 Introduction
Pre-trained Transformer models (Vaswani et al.,
2017; Devlin et al., 2019) provide robust language
representations which can be specialized on var-
ious tasks. Given their massive growth (Radford
et al., 2019; Smith et al., 2022), techniques for
reducing their computational overheads have be-
come popular. One classic technique is Knowledge
Corresponding author: eldar.kurtic@ist.ac.at.
60 70 80 90 97
Sparsity (%)70.072.575.077.580.082.585.087.590.0F1 score
BERTBASE
Lottery Ticket
Sparse BERT
Movement Pruning
oBERT (ours)Figure 1: Performance overview relative to state-
of-the-art unstructured downstream pruning methods
Chen et al. (2020), Xu et al. (2021), Sanh et al.
(2020), in this order, of the BERT BASE model on the
SQuADv1.1 task.
Distillation (KD) (Hinton et al., 2015), which trans-
fers knowledge from a larger teacher to a smaller
student model. Other work has leveraged lower-
precision representations to produce quantized
models. An orthogonal approach, which is our
primary focus, has been to apply unstructured and
block pruning, i.e. removing individual weights, to
produce compressed but accurate language mod-
els.Figure 1 provides a comparative overview of
state-of-the-art results for unstructured pruning.
In this paper, we introduce a method for im-
proved unstructured and semi-structured (block)
pruning, by leveraging the second-order approach
pioneered by the Optimal Brain Surgeon frame-
work (LeCun et al., 1989; Hassibi and Stork, 1992),
which we scale for the ﬁrst time to LLMs. Further,
we put our results in the context of a compound
compression approach, which combines several
compression techniques to obtain sparse models
which we execute on a sparsity-aware CPU-based
runtime (NeuralMagic, 2021), showing order-of-
magnitude speedups at low accuracy loss.
In summary, our contributions are as follows:
•We perform a thorough exploration ofarXiv:2203.07259v3  [cs.CL]  17 Oct 2022

--- PAGE 2 ---
weight pruning approaches applied to LLMs,
including lottery-ticket, movement pruning,
magnitude and second-order pruning.
•We introduce a general second-order prun-
ing method called Optimal BERT Surgeon
(oBERT), which supports unstructured and
block pruning, and is the ﬁrst second-order
method to be both highly-accurate and scal-
able to the dimensionality of BERT models.
•We illustrate the beneﬁts of oBERT by
signiﬁcantly improving upon existing state-
of-the-art pruning methods, in both stages
of language tasks: pre-training and ﬁne-
tuning. For illustration, when pruning
BERT BASE , oBERT outperforms Movement
Pruning (MvP), the most accurate prior ap-
proach, by more than 2% absolute F1 score at
the same sparsity, and can match the accuracy
of MvP models with 3x fewer parameters.
•We investigate the applicability of this prun-
ing method in a framework which compounds
popular compression approaches for LLMs,
i.e. applying pruning in combination with
layer dropping and/or quantization. In this
context, we show that our resulting sparse
models provide order-of-magnitude improve-
ments compared to other compound com-
pressed models, and that they can be easily
deployed for CPU inference.
2 Background and Related Work
Transformer LLMs are usually built using multi-
ple transformer layers with self-attention (Vaswani
et al., 2017). Each transformer has a variation of
two sub-components: multi head attention (MHA)
and fully connected feed forward network (FFN).
Given the massive size of well-performing models,
there has been growing interest in LLM compres-
sion. They have been shown to be fragile as minor
perturbations can lead to model collapse (Kovaleva
et al., 2021). Pruning schemes are motivated by
weight saliency metrics which represent the loss
in accuracy due to pruning. It is common to prune
in iterative steps, each of which removes weights
until a desired sparsity level is reached. Now, we
brieﬂy overview existing approaches.
Structured pruning for LLMs focuses on reduc-
ing the number of layers and/or attention heads,
and requires structural understanding of the model.
Michel et al. (2019) and V oita et al. (2019) demon-strated that for some tasks nearly 40% of attention
heads can be removed without major impact on
accuracy. Other work has focused on removing
layers (Sridhar and Sarah, 2020), and on the order
in which they are removed (Sajjad et al., 2020). In
some of our experiments, we apply standard “di-
rect” layer dropping in conjunction with pruning.
Semi-structured pruning is an intermediate ap-
proach, by which smaller groups, e.g. rectangular
sets of weights (Lagunas et al., 2021), are set to
zero. This approach has recently gained in pop-
ularity thanks to efﬁcient computational support.
We extend the second-order pruning formulation
to such groupings, and show results for a speciﬁc
grouping supported by a CPU-inference engine.
Unstructured pruning removes individual
weights by setting them to zero. Gradual Magni-
tude Pruning (GMP) is a classic approach, which
makes use of weight magnitudes as a saliency met-
ric for pruning (Han et al., 2015; Gale et al., 2019).
First-order pruning methods use a gradient
based formulation of the saliency metric. A pop-
ular method is Movement Pruning (MvP) (Sanh
et al., 2020), speciﬁcally designed for pruning
in the ﬁne-tuning stage. Intuitively, it removes
weights that are moving towards zero. The
resulting models were the ﬁrst to achieve high
sparsity with tolerable accuracy loss. Methods
such as PLATON (Zhang et al., 2022) attempt
to capture the uncertainty of weights importance
scores by upper conﬁdence bound estimation.
Prior to our work, MvP and PLATON approaches
set state-of-the-art results for unstructured pruning.
Second-order pruning methods (LeCun et al.,
1989; Hassibi and Stork, 1992; Singh and Alis-
tarh, 2020; Frantar et al., 2021) were developed
in the context of image classiﬁcation, and lever-
age complex approximations of the loss curvature.
However, second-order pruning methods require
an approximation of the inverse Hessian, which
is expensive to store and compute with for LLM
parameter counts. The approach we propose is sim-
ilar to WoodFisher/M-FAC methods (Singh and
Alistarh, 2020; Frantar et al., 2021), but is the
ﬁrst to work accurately at LLM scale. Speciﬁ-
cally, the WoodFisher approach is infeasible at
BERT scale, as it requires storing gradients for
inverse Fisher calculation in memory at the point
of pruning. The M-FAC approach scales, but we
show that its parametrization yields worse prun-
2

--- PAGE 3 ---
ing results (Appendix Figure 3). This is because
M-FAC performs full-matrix (non-blocked) inver-
sion by default, which is inherently noisy. In ad-
dition, we extend the theoretical OBS approach
to semi-structured (block) compression. We also
show that our method can be applied during LLM
pre-training and ﬁne-tuning, yielding state-of-the-
art results in both regimes.
Knowledge Distillation (Hinton et al., 2015)
trains a smaller student model against outputs of a
larger teacher model by adding a loss component
which minimizes the KL-divergence between the
two output distributions, which is the approach we
adopt in our setup too. A hardness parameter is
used to control the mixture of regular and distil-
lation loss, and a temperature parameter to con-
trol softness of the distribution. Contrary to this,
approaches like DistilBERT (Sanh et al., 2019),
TinyBERT (Jiao et al., 2020), MobileBERT (Sun
et al., 2020a), and MiniLM (Wang et al., 2020) uti-
lize more complex distillation schemes, based on
transferring knowledge from intermediate model’s
representations. Our sparse models provide order-
of-magnitude improvements upon some of these
methods.
Quantization represents weights and activations
in lower precision (Courbariaux et al., 2016), and
was used to obtain models such as Q8BERT (Zafrir
et al., 2019) and TernaryBERT (Zhang et al., 2020).
Shen et al. (2020) uses information about
the Hessian spectrum to choose quantization bit-
widths, whereas Yu et al. (2022) uses an approxi-
mation of the Hessian trace for structured pruning.
These Hessian-based approaches are different from
the one we propose, as we use completely different
inverse-Hessian approximations to guide pruning
decisions. The focus of our work is on weight prun-
ing, and on computational speedups achievable on
commodity CPUs. As such, the methods we inves-
tigate are orthogonal to quantization. Moreover, it
is impossible to directly compare to low-bitwidth
quantized models as most inference frameworks
do not support such custom formats. Therefore, we
will only make use of the standard Quantization-
Aware Training (QAT) to 8-bit weights, which is
well-supported on Intel CPUs, and showcase the
resulting speedups in conjunction with layer drop-
ping and weight pruning.
Downstream compression methods attempt to
compress directly while ﬁne-tuning on a speciﬁctask. MvP method is specially designed for this
setup. Upstream compression methods compress
during the pre-training phase, reducing the need
for task-speciﬁc pruning. Chen et al. (2020)
examined the “Lottery Ticket” strategies (Frankle
and Carbin, 2018) which, as we illustrate later,
incur huge accuracy loss even at moderate
sparsities. Recent work “Prune Once for All”
(Prune OFA) by Zafrir et al. (2021) showed that
well-tuned magnitude pruning can be competitive
with downstream methods like MvP.
We ﬁrst examine the performance of prior prun-
ing methods, notably MvP, Prune OFA, and Lottery
Tickets, relative to the new second-order oBERT
method. The approach we propose consistently
improves upon all these prior methods, both in
pre-training (upstream) and ﬁne-tuning (down-
stream) stages, and can be compounded with other
compression techniques to obtain models that are
smaller, faster and more accurate than models like
DistilBERT, TinyBERT, and block MvP.
Additional approaches for efﬁcient inference of
LLMs exist, like token-pruning and early-exiting.
These approaches are orthogonal to ours; therefore
we discuss them in Appendix A.2.
3 The Optimal BERT Surgeon (oBERT)
3.1 Generalized Second-Order Block
Pruning
The pruning problem starts from a well-optimized
dense model w2Rd, and aims to ﬁnd a sparse
version of w, where many of the weights are set
to zero, and the remaining weights may be up-
dated accordingly in order to preserve the loss. It
is common for this process to occur gradually, i.e.
by progressively removing the weights. A classic
approach (LeCun et al., 1989; Hassibi and Stork,
1992) for “optimal” pruning of weights from w
at a step is to expand the loss function Llocally
around wwith respect to a sparse 0/1 weight mask
M. If we denote by wM= (Mw), the model
resulting from the Hadamard (element-wise) prod-
uct between M2f0;1gdandw, we can use the
Taylor expansion at wMto obtain:
L(wM)'L(w) + (wM w)>rL(w)
+1
2(wM w)>HL(w)(wM w):
Given that wis well-optimized, it is reasonable
in practice to assume that rL(w)0. Then,
3

--- PAGE 4 ---
the change in loss incurred by pruning a subset of
weights can be expressed as
L(w)'1
2w>HL(w)w (1)
whereL(w):=L(wM) L(w)andw:=
wM w. A popular way of approximating the
Hessian at wis via a dampened empirical Fisher
information matrix (Hassibi and Stork, 1992):
HL(w)'bF(w)=Id+1
mmX
i=1rLi(w)rL>
i(w);
(2)
where0is a small dampening constant,
Id2Rddidentity matrix and mis the number
of gradient outer products used to approximate the
Hessian. Given the positive-deﬁniteness of (2), the
quadratic form (1)is always nonnegative which
is why we will refer to L(w)as aloss increase
incurred by pruning.
Returning to our pruning problem, assume we
wish to identify a block of weights Qof a given
shape whose removal by zero-masking would in-
cur minimum increase in loss. This leads to the
following constrained optimization problem:
min
w1
2w>bF(w)w
s:t:e>
kw+wk= 0;8k2Q(3)
where ek2Rdstands for the k-th canonical
basis vector. Here, we will provide a general-
ized solution, which applies to general Q. First,
for convenience, we express the system of jQj
equality constraints in matrix-equation form as
EQw+EQw=0;where EQ2RjQjdis
a matrix composed of the corresponding canon-
ical basis vectors ek(8k2Q)arranged in rows.
This optimization problem can be solved with the
method of Lagrange multipliers. Speciﬁcally, we
wish to ﬁnd stationary points of the Lagrangian
L(w;), where 2RjQjdenotes a vector of
Lagrange multipliers. Solving the system of equa-
tions@L(w;)
@w=0and@L(w;)
@=0yields the
following optimal weight update:
w= bF 1(w)E>
Q
EQbF 1(w)E>
Q 1
EQw
which prunes a set of weights Qand updates the
remaining weights to preserve the loss. Now, thecorresponding loss increase incurred by the opti-
mal weight update wcan be expressed as the
saliency score of weights Q, which we denote by:
Q=1
2(EQw)>
EQbF 1(w)E>
Q 1
EQw:
We use this saliency/importance score to rank
groups of weights for pruning. As a sanity check,
if we prune a single weight wjat a time, our deriva-
tions will yield the standard formulas of (Hassibi
and Stork, 1992). The full version of Singh and
Alistarh (2020) provided a slightly less general
derivation for the blocked case, under additional
assumptions.
3.2 An Efﬁcient Implementation
Directly implementing the previously described
approach for LLMs, where number of weights
w2Rdis huge, is infeasible. In particular, this is
due to the dependence on the inverse of the empir-
ical Fisher information matrix bF 1(w)2Rdd,
appearing in formulations of the saliency score and
of the optimal weight update. We now describe
how to circumvent these issues.
3.2.1 Pruning the optimal set of weights
Assume a gradual pruning setup, in which at each
pruning step we wish to prune a model to a tar-
get sparsity s2(0;1], effectively zeroing out
sdweights, in groups of size jQj. Typically
sdjQj, meaning that we want to remove
multiple groups at the same time. Finding the
optimal set ofsd
jQjgroups is an intractable combi-
natorial problem, due to all possible correlations
between them, given by the binomial coefﬁcient n
k
, wheren=d
jQjandk=sd
jQj. This problem
can be alleviated by ignoring correlations between
different groups of weights Q, and solving only for
correlations between the weights within the same
group. In practice, this boils down to evaluating the
saliency score Qfor each group Q, and pruning
thesd
jQjgroups with the lowest score. As pruning
many weights in the same step can make the Taylor
approximation of the loss function less accurate,
one can consider pruning with multiple smaller
sub-steps with recomputations of the Hessian ap-
proximation in between (without intermediate ﬁne-
tuning). While this can further improve the quality
of the pruning step (Frantar et al., 2021), we do not
implement this additional optimization since the
competing methods do not utilize recomputations.
4

--- PAGE 5 ---
3.2.2 Inverse empirical Fisher computation
The key space and time complexity cost of the
above procedure is computing products with the
inverse empirical Fisher. A direct approach would
be to perform a block-wise diagonal approximation
of this matrix (which we detail next), and perform
direct block inversion. However, we found ex-
perimentally that this approach is too expensive
in terms of time, and quite numerically-sensitive.
As an alternative, we rely on the fact that the ma-
trix we wish to invert is a sum of rank-1 matri-
ces, and employ the Woodbury/Sherman-Morrison
(WSM) inversion formula. Speciﬁcally, given a
sum(A+uv>)of an invertible matrix Aand an
outer product of vectors uandvwith compati-
ble dimensions, the inverse (A+uv>) 1can be
exactly calculated as A 1 A 1uv>A 1
1+v>A 1u. Plac-
ing the expression of the empirical Fisher in the
WSM formula, we obtain the following recursive
formulation, where mis the number of gradients
employed in the approximation:
bF 1(w) =bF 1
m(w) =

bFm 1(w) +1
mrLm(w)rL>
m(w) 1
:
Unrolling the recursion with bF 1
0(w) =1
Id, we
can obtain an iterative formula to exactly calculate
the inverse of the empirical Fisher matrix as
bF 1(w) =bF 1
m(w) =
1
Id Pm
i=1(bF 1
i 1(w)rLi(w))(bF 1
i 1(w)rLi(w))>
m+rL>
i(w)bF 1
i 1(w)rLi(w):
The iterative formulation enjoys a number of com-
putational advantages over the direct implementa-
tion. The most notable ones are 1) avoiding explicit
calls to the expensive and dampening-sensitive ma-
trix inversions, and 2) allowing successive updates
of the inverse as new gradients are computed, never
needing to store all mgradients of size dand thus
signiﬁcantly reducing memory requirements.
3.3 Memory and run-time complexity
Computing and storing the inverse empirical Fisher
bF 1(w)2Rddis prohibitively expensive for
modern LLMs, which have hundreds of millions of
parameters, due to the quadratic complexity on the
number of weights d. However, Singh and Alistarh
(2020) have shown that a diagonal block-wise ap-
proximation of the empirical Fisher matrix can bevery accurate for pruning of convolutional neural
networks. We adapt the same approach here, in
the context of LLMs. Thus, for blocks of width
Balong the main diagonal, memory requirements
for the computation of the inverse Fisher matrix
are reduced from the quadratic O(d2)to a linear
O(Bd)dependence on the number of weights d.
At the same time, run-time complexity relaxes
fromO(md2)toO(mBd ). As we will show, this
computation can be efﬁciently and accurately per-
formed for moderate values of mandB.
Another alternative we investigated was the
matrix-free approach of Frantar et al. (2021), which
does not require a block-wise approximation and
has complexity (dm). However, our investiga-
tion showed that this approach required high values
ofmto be accurate (Appendix Figure 3), which
leads to excessive memory cost in the case of
BERT models.
3.4 Efﬁcient and scalable implementation
On the practical side, we have identiﬁed general
hyper-parameters B= 50 for the block size, and
m= 1024 for the number of gradients which pro-
duce state-of-the-art results for all analyzed BERT
models (for more details please see Appendix A.4),
while still being able to ﬁt on the 24GB RTX 3090
GPU. We reﬂect upon the computational costs
in more detail in Appendix A.3. Moreover, for
these parameter values, the block-wise approxi-
mation ofbF 1(w)can be implemented very efﬁ-
ciently on modern accelerators. Speciﬁcally, we
take advantage of the fact that such hardware fa-
vors batched matrix operations, and that the blocks
of sizeBBinbF 1(w)are independent. With
NB=d
Bwe refer to the total number of blocks,
i.e. the batch-dimension. The procedure works as
follows. First, we compute batched matrix-vector
productsbF 1
i 1(w)rLi(w)2RNBBand scalar
denominators m+rL>
i(w)bF 1
i 1(w)rLi(w)2
RNB:Then, we update the inverse Fisher for each
block by computing the scalar-scaled outer prod-
ucts
bF 1
i 1(w)rLi(w)
bF 1
i 1(w)rLi(w)>
of shape RNBBB.
4 Experimental Validation
To ease reproducibility, we conduct our ex-
periments in modiﬁed versions of the popular
open-source libraries: Transformers (Wolf et al.,
5

--- PAGE 6 ---
2020), and SparseML (Kurtz et al., 2020). All
of our experiments are using publicly available
datasets via Lhoest et al. (2021) and focus on the
BERT BASE model (Devlin et al., 2019), one of the
most commonly used LLMs, composed of 12 trans-
former layers with 110M parameters. Following
community standards, we prune encoder’s weights
(85M) and report sparsities relative to this number.
All of our models, compression recipes and the full
implementation will be made public.
4.1 Downstream Unstructured Pruning
We ﬁrst revisit the accuracy-compression trade-off
for pruning on downstream tasks.
Goals and setup. We compare existing ap-
proaches, notably Movement Pruning (MvP) (Sanh
et al., 2020) and Lottery Ticket (LT-BERT) (Chen
et al., 2020), against the gradual unstructured
oBERT method, introduced in Section 3. Our ex-
periments evaluate performance on a variety of
downstream (English) tasks commonly used to
evaluate model compression: question answering
SQuAD v1.1 (Rajpurkar et al., 2016), sentence
classiﬁcation Quora Duplicate Query Dataset QQP
(Shankar et al., 2017), and natural language infer-
ence MNLI (Williams et al., 2018).
Comparison with MvP. For a fair comparison
with MvP, we consider the 10-epoch gradual prun-
ing setup used to obtain the best results by Sanh
et al. (2020). Speciﬁcally, we start from the
BERT BASE model and perform 2 epochs of ﬁne-
tuning, followed by 6 epochs of pruning, and 2 fur-
ther epochs of ﬁne-tuning of the compressed model.
We impose a global sparsity distribution over all
layers, prune with oBERT two times per epoch,
and use KD from the ﬁne-tuned BERT BASE teacher.
For oBERT pruning we use m= 1024 gradients,
block sizeB= 50 , and dampening = 10 7
to approximate the inverse Hessian matrix. In
all of our runs, the ﬁrst pruning step prunes 70%
of weights and then follows the cubic interpola-
tion (Zhu and Gupta, 2018) to the target sparsity.
This large ﬁrst pruning step gives more time to
recover from the later pruning steps, which im-
pose higher sparsities. All hyper-parameters are
described in detail in Appendix A.5, and the results
are given in Table 1 (in the 10 Epochs section).
We observe that Optimal BERT Surgeon outper-
forms Movement Pruning by a signiﬁcant margin,
more than 2 points of F1 score at the same spar-Table 1: Downstream tasks dev-set performance of
pruned BERT BASE models. (approximate results as
the exact numbers are not available.)
TaskBERT
BASESpars.Soft
MvPoBERT
(ours)LT-
BERToBERT
(ours)
Epochs 10 Epochs 30 Epochs
SQuAD
F188.5480%
90%
97%-
84.90
82.30-
87.98
84.6586.54
68.00
-89.04
88.31
85.98
MNLI
m-acc84.5480%
90%
97%-
81.20
79.50-
83.20
81.0082.60
75.00
-84.32
83.79
81.77
QQP
Acc91.0680%
90%
97%-
90.20
89.10-
90.89
90.2390.30
90.00
-91.57
91.35
90.87
sity. Remarkably, the model pruned with oBERT to
97% sparsity has similar accuracy to MvP-pruned
model at 90% sparsity, which has roughly 3x
more weights. This reinforces the effectiveness
of second-order information for pruning.
Extended pruning and ﬁne-tuning. Next, we ex-
amine effects of extending the gradual schedule
to 30 epochs, matching the setup used for LT-
BERT (Chen et al., 2020). The only difference
compared to our 10 epoch setup is that we now
prune with oBERT every four epochs, and rewind
learning rate after each pruning step. The extended
setup leaves more time to recover from pruning,
which reﬂects in the improved results in Table 1
(30 Epochs section). We report the mean over three
runs. For additional evaluation metrics and stan-
dard deviations please see Tables 12 and 15 in the
Appendix. The results show a clear accuracy dif-
ference between oBERT and LT-BERT, especially
at high sparsities. This difference is justiﬁed since
the LT based approach attempts to mainly transfer
network connectivity , whereas the oBERT can also
beneﬁt from the weight values. Finally, we exam-
ined the impact of extended setup with Soft MvP
on SQuAD, targeting 90% sparsity (not shown in
the Table), leading to an (F1, EM) combination
of(87:42;79:83)for MvP. The F1 gap in favor of
oBERT is lower than at 10 epochs, suggesting that
extended ﬁnetuning helps all methods; yet, it is far
from negligible.
4.2 Upstream Unstructured Pruning
An appealing alternative to downstream pruning
is to compress models upstream, on the semi-
supervised pre-training task (Zafrir et al., 2021).
Given the upstream pruned model, computational
6

--- PAGE 7 ---
requirements for obtaining downstream ﬁne-tuned
models are signiﬁcantly reduced, as only ﬁne-
tuning of the remaining weights is necessary.
Goals and setup. To compare with existing ap-
proaches, notably Prune OFA (Zafrir et al., 2021)
and LT-BERT (Chen et al., 2020), we gradually
prune with oBERT directly at upstream datasets,
BookCorpus and English Wikipedia, and then ﬁne-
tune the remaining unpruned weights on the subset
of GLUE tasks.
Teacher preparation. Following Liu et al. (2019),
we start with the HuggingFace BERT BASE uncased
model, and ﬁne-tune it for additional 10 epochs
only on the masked language modeling task.
Pruning at upstream. Once the distillation
teacher is trained, we gradually prune and ﬁne-tune
theBERT BASE model for 3 epochs, using KD from
the dense teacher. We prune four times per epoch,
and rewind learning rate to the initial value after
each pruning step. Hyper-parameters for oBERT
are the same as for downstream pruning in 4.1; a
full description can be found in Appendix A.6.
Sparse-transfer to downstream. To evaluate the
resulting upstream-pruned models, we ﬁnetune the
unpruned weights on downstream tasks with KD
from the ﬁne-tuned BERT BASE model. For a fair
comparison with Prune OFA, we ﬁne-tune for 8
epochs. The results in Table 2 show that sparse
models produced by oBERT outperform state-of-
the-art methods by signiﬁcant margins. We report
the mean over four runs. For additional evalua-
tion metrics and standard deviations please see Ap-
pendix Tables 13 and 16. It is worth emphasizing
that in contrast to Prune OFA, which performed ex-
tensive hyper-parameter tuning for sparse-transfer,
our recipe is simple and general across downstream
tasks: 8 epochs of ﬁne-tuning with linearly de-
caying learning rate. This suggests that sparse
pre-trained models found by oBERT constitute a
strong starting point for sparse transfer learning,
which can be further improved by task-speciﬁc
hyper-parameter tuning.
4.3 Compound Compression for CPUs
To probe the potential practical impact of our ap-
proach, we specialize the technique for deployment
on CPUs, corresponding to “edge” deployments.
Speciﬁcally, we tailor our sparse models to the
DeepSparse (NeuralMagic, 2021) sparsity-aware
runtime, by compounding unstructured pruningTable 2: Sparse-transfer dev-set performance of
upstream-pruned BERT BASE models. (approximate
results as the exact numbers are not available.)
TaskBERT
BASESparsityLT-
BERTPrune
OFAoBERT
(ours)
SQuAD
F188.5490%
97%68.00
-87.25
-88.49
84.92
MNLI
m-acc84.5490%
97%75.00
-81.45
-83.40
80.91
QQP
Acc91.0690%
97%90.00
-90.93
-90.99
90.33
SST-2
Acc93.01 90% 85.0090.88 92.20
QNLI
Acc91.25 90% 80.0089.07 89.97
with additional compression techniques.
Direct layer dropping. The competitive results
obtained at high sparsities in sections 4.1 and 4.2
suggest that BERT BASE may be overparameterized
for downstream tasks. To improve compression
ratio and inference speed, we apply “direct” layer
dropping: we initially drop all but 3 or 6 of the
BERT’s 12 layers. We drop layers from our up-
stream teacher, and, following (Turc et al., 2019),
ﬁne-tune them with KD in the same setup used
to prepare the upstream teacher. These 3 and 6
layer models are used as starting points for down-
stream pruning. More sophisticated layer dropping
techniques (Fan et al., 2019), could bring further
accuracy gains; we leave this for future work.
Block pruning and QAT. High-performance in-
ference usually beneﬁts more from (semi) struc-
tured sparsity patterns than from the unstructured
ones. Hence, we employ the generalized oBERT
formulation introduced in the section 3 and prune
weights in the 4-block pattern, meaning that con-
tiguous blocks of 4 weights are either set to zero
or kept dense. Both pruning types, unstructured
and 4-block, can be leveraged for computational
speedups with the DeepSparse runtime, but 4-
block pruning coupled with INT8 quantization can
provide further performance gains. For quantiza-
tion, we apply standard quantization-aware train-
ing (QAT) (Jacob et al., 2018) on top of the 4-block
models (see Appendix A.7 for a full description).
Compounding for deployment. To determine the
impact of different compression schemes, we inves-
tigate unstructured and 4-block pruning of the 3, 6,
and 12-layer models. For all runs, we use the same
set of hyper-parameters from the extended pruning
7

--- PAGE 8 ---
Table 3: F1 score of the 3, 6, and 12-layer models
compound-compressed on the SQuADv1.1.
Layers Sparsity Unstructured 4-block +QAT
120%
80%
90%89.48
89.04
88.3189.48
88.57
87.5789.06
87.89
86.68
60%
80%
90%88.32
88.20
86.7888.32
87.00
85.3487.94
86.10
84.59
30%
80%
90%84.66
84.08
82.5084.66
82.79
80.6984.25
82.04
79.66
93 94 95 96 97 98 99 100
F1 recall (%)051015202530Magnitude of improvement
Inference speed
Model compression
Figure 2: F1 recall on the SQuADv1.1 task relative to
improvements in CPU-inference speed and model size.
and ﬁne-tuning setup in Section 4.1. The results
are given in Table 3, where we also report accuracy
of the corresponding dense models (0% sparsity)
in the same setup. For additional evaluation met-
rics, please see Table 14. The results indicate that
compression methods can be combined without
model collapse, although the accuracy drops do
compound. The fact that the layer-dropped models
are also highly compressible suggests that struc-
tured and ﬁne-grained (unstructured) compression
are complementary. We ﬁnd it remarkable that
our 6-layer unstructured oBERT-pruned model is
competitive with the 12-layer MvP-pruned model
when both are pruned to 90% sparsity.
Practical trade-offs. We now benchmark these
models in end-to-end fashion, both in terms of
model size and inference speed. For model size,
we report size of the checkpoint in MB after stan-
dard gzip compression. For inference speed, we
report number of items per second (throughput) on
the well-established SQuAD v1.1 CPU-inference
benchmark with a sequence length of 128 and a
batch size of 32. Figure 2 depicts relative accu-
racy versus magnitude of improvement in speedand model size. As baseline for full recovery, we
follow the community-standard e.g. (Sanh et al.,
2020), and adopt the dense BERT BASE model with
88.54 F1 score. The baseline for inference speed
is dense BERT BASE inference with DeepSparse,
which matches the industry-standard ONNX Run-
time inference engine. Results suggest a roughly-
linear trade-off between compression and accuracy
loss, with a compression jump around 1% accuracy
drop, due to quantization being applied. Speciﬁ-
cally, we observe 8.4x higher inference speedup at
< 1% accuracy drop, 10x speedup at < 2% drop,
15x speedup at < 3% drop, and 29x speedup at <
7.5% accuracy drop. This shows how compound
compression can optimize LLMs to various laten-
cies. See Appendix Table 17 for full results.
4.4 Pruning for GPU speedups (N:M
sparsity)
Even though our previous results targeted CPUs
for deployment, we now show that our pruning
approach can also be relevant to GPUs. We ap-
ply the semi-structured variant of oBERT to im-
pose the 2-out-of-4 sparsity pattern, which is sup-
ported on NVIDIA Ampere GPUs (Mishra et al.,
2021). More speciﬁcally, we prune in one-shot ,
and compare against the magnitude pruning base-
line in Table 4. All other methods require full
ﬁne-tuning, and thus don’t support the one-shot
setup. oBERT signiﬁcantly outperforms magni-
tude pruning, and with only 1-epoch of ﬁne-tuning
it is able to fully recover dense accuracy with (F1,
EM) = (88.58, 81.16). With this sparsity pattern,
the pruned model achieves 1.85x speedup on Am-
pere devices.
Table 4: One-shot 2:4 pruning of the ﬁne-tuned
BERT BASE model.
Task BERT BASE Magnitude oBERT (ours)
SQuAD
F1 / EM88.54 / 81.41 49.97 / 35.24 83.17 / 74.18
5 Discussion
Comparison with concurrent work. Concurrent
work introduced PLATON (Zhang et al., 2022),
which addresses unstructured pruning of BERT
models via estimates of conﬁdence bounds. It does
not make use of KD, so for a fair comparison we
8

--- PAGE 9 ---
rerun our experiments without KD as well. Con-
trary to PLATON, which reports best results after
an extensive hyper-parameter search for each task
independently, we apply our sparse-transfer setup
with the upstream pruned model and only sweep
for the number of epochs 2[1;8]. We employ
early stopping to prevent overﬁtting on smaller
GLUE tasks. As can be seen from Table 5, oBERT
outperforms PLATON across all tasks.
Table 5: Compressed BERT BASE models to 90% spar-
sity on GLUE tasks without knowledge distillation.
Task BERT BASE PLATONoBERT
(ours)
MNLI
m / mm84.6 / 83.4 82.0 / 82.2 82.2 / 82.5
QQP
Acc / F191.5 / 88.5 90.2 / 86.8 90.4 / 87.1
QNLI
Acc91.3 88.9 89.3
MRPC
Acc / F186.4 / 90.3 84.3 / 88.8 85.6 / 89.3
SST-2
Acc92.7 90.5 92.0
CoLA
Mcc58.3 44.3 48.47
STS-B
Pear / Spear90.2 / 89.7 87.4 / 87.1 88.0 / 87.6
Broader comparison. We now contrast our
compound-compressed BERT BASE models relative
to alternative compression techniques. We com-
pare against DistilBERT (Sanh et al., 2019), Tiny-
BERT (Jiao et al., 2020), and Block Pruning For
Faster Transformers (Hybrid Filled MvP) (Lagunas
et al., 2021). DistilBERT leverages KD during pre-
training and ﬁne-tuning to obtain a 6-layer model
ﬁne-tuned for a speciﬁc downstream task. Tiny-
BERT makes use of a specialized Transformer-KD
scheme to distill knowledge and intermediate rep-
resentations at both stages, pre-training and ﬁne-
tuning on a speciﬁc task. In contrast, we use a
simpler approach and employ KD from teacher’s
outputs only. Hybrid Filled MvP (Lagunas et al.,
2021) employs semi-structured pruning and weight
reintroduction. The comparison is given in Ta-
ble 6, where we report the number of unpruned
encoder weights as size, compression ratio and in-
ference speedup relative to the dense BERT BASE in
the same inference environment, and F1 score on
the dev-set of the SQuAD v1.1 dataset. The re-sults suggest that our compressed models improve
upon the current state-of-the-art techniques, setting
new very competitive baselines with respect to all
metrics: accuracy, model size, and inference speed.
Table 6: Compressed BERT BASE models on the
SQuADv1.1 task. (oBERT 6;80stands for the 6-layer
model pruned to 80% sparsity.)
Model Size Compr. Speedup F1 Dev.
BERT BASE 85.0M 1.00x 1.00x 88.54
< 6-layers
TinyBERT4 4.5M 18.88x 9.40x 82.10 GPU
oBERT 3;90 2.1M 40.00x 14.80x 82.50 CPU
6-layers
DistilBERT 42.5M 2.00x 2.00x 86.90 GPU
TinyBERT6 42.5M 2.00x 2.00x 87.50 GPU
oBERT 6;80 8.5M 10.00x 6.38x 88.20 CPU
12-layers
Hybrid F. MvP 30.7M 2.76x 1.84x 88.70 GPU
oBERT 12;80 17.0M 5.00x 3.38x 89.04 CPU
BERT LARGE results. Most of our results pre-
sented in Section 4 targeted the widely-adopted
BERT BASE model. This gave us an opportunity for
a fair comparison against many different methods.
To verify that our approach does not pertain only to
theBERT BASE model, in Table 7 we present down-
stream pruning results on the three times larger
BERT LARGE model and the SQuADv1.1 task. As
can be seen from the Table, even the model pruned
with oBERT at double the sparsity (95%) outper-
forms Prune OFA (90%).
Table 7: Compressed BERT LARGE models on the
SQuADv1.1 task.
BERT LARGE
F1 / EMSparsityPrune
OFAoBERT
(ours)
91.22 / 84.45 90% 90.20 / 83.35 91.07 / 84.61
91.22 / 84.45 95% NA 90.29 / 83.58
MLPerf Inference Benchmark. Motivated by
our state-of-the-art results across-the-board, we
apply our full compound compression pipeline
to compress BERT LARGE and MobileBERT (Sun
et al., 2020b) models in the context of the in-
dustrial MLPerf Inference Benchmark1. In brief,
we were able to achieve order-of-magnitude im-
provements in terms of model size and inference
speedups, while maintaining >99% of the dense
1https://mlcommons.org/en/
9

--- PAGE 10 ---
BERT LARGE accuracy. For details please see Ap-
pendix A.1, as well as our open-source submission.
6 Broader Impact
Our work is part of the general trend of produc-
ing inference efﬁcient models which approximate
performance of their larger bases. By and large,
this work should help increase model efﬁciency,
thereby reducing computational and ultimately
monetary cost of executing such models. More-
over, it could allow models to be used by those
who do not have access to expensive specialized
computing clusters: for instance, our main speedup
results are aimed at widely-available CPUs.
7 Limitations
As any academic study, our work is not without
its limitations. We split their discussion into lim-
itations that are inherent to our method , and lim-
itations of our present study ; the latter can be
overcome by extensions of our work. In the ﬁrst
category, we begin by highlighting the fact that
our second-order method relies on approximations,
which are inherent in order to scale such methods
to BERT scale. Prior studies, e.g. (Singh and Alis-
tarh, 2020) have performed careful examinations
of the validity of these approximations in the con-
text of CNN models. The strength of our empirical
results can be seen as indirect evidence that these
approximations apply to BERT models as well. A
second, technical, limitation is the fact that our
method requires non-trivial additional storage cost;
while we have shown that our experiments can be
executed on a single commodity GPU (NVIDIA
RTX 3090), this limits the range of devices on
which the technique may be applied. However, we
provide an efﬁcient and easy way to scale our ap-
proach with more GPUs, which is automatically
utilized in a multi-GPU environment.
Another limitation which we aim to remove in
future work is the focus on relatively ﬁne-grained
sparsity types, such as unstructured and semi-
structured pruning.
References
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia
Liu, Yang Zhang, Zhangyang Wang, and Michael
Carbin. 2020. The lottery ticket hypothesis for pre-trained bert networks. Advances in neural informa-
tion processing systems , 33:15834–15846.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry,
Ran El-Yaniv, and Yoshua Bengio. 2016. Bina-
rized neural networks: Training deep neural net-
works with weights and activations constrained to+
1 or-1. arXiv preprint arXiv:1602.02830 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Angela Fan, Edouard Grave, and Armand Joulin. 2019.
Reducing transformer depth on demand with struc-
tured dropout. In International Conference on
Learning Representations .
Wikimedia Foundation. Wikimedia downloads.
Jonathan Frankle and Michael Carbin. 2018. The lot-
tery ticket hypothesis: Finding sparse, trainable neu-
ral networks. In International Conference on Learn-
ing Representations .
Elias Frantar, Eldar Kurtic, and Dan Alistarh. 2021. M-
fac: Efﬁcient matrix-free approximations of second-
order information. Advances in Neural Information
Processing Systems , 34.
Trevor Gale, Erich Elsen, and Sara Hooker. 2019. The
state of sparsity in deep neural networks. arXiv
preprint arXiv:1902.09574 .
Song Han, Huizi Mao, and William J Dally. 2015. A
deep neural network compression pipeline: Prun-
ing, quantization, huffman encoding. arXiv preprint
arXiv:1510.00149 , 10.
Babak Hassibi and David Stork. 1992. Second order
derivatives for network pruning: Optimal brain sur-
geon. Advances in neural information processing
systems , 5.
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 , 2(7).
Benoit Jacob, Skirmantas Kligys, Bo Chen, Meng-
long Zhu, Matthew Tang, Andrew Howard, Hartwig
Adam, and Dmitry Kalenichenko. 2018. Quanti-
zation and training of neural networks for efﬁcient
integer-arithmetic-only inference. In Proceedings
of the IEEE conference on computer vision and pat-
tern recognition , pages 2704–2713.
10

--- PAGE 11 ---
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
2020. Tinybert: Distilling bert for natural language
understanding. In Findings of the Association for
Computational Linguistics: EMNLP 2020 , pages
4163–4174.
Sehoon Kim, Sheng Shen, David Thorsley, Amir Gho-
lami, Woosuk Kwon, Joseph Hassoun, and Kurt
Keutzer. 2022. Learned token pruning for trans-
formers. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing, KDD ’22, page 784–794. Association for Com-
puting Machinery.
Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers,
and Anna Rumshisky. 2021. BERT busters: Outlier
layernorm dimensions that disrupt BERT. CoRR ,
abs/2105.06990.
Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexan-
der Matveev, John Carr, Michael Goin, William
Leiserson, Sage Moore, Bill Nell, Nir Shavit, and
Dan Alistarh. 2020. Inducing and exploiting activa-
tion sparsity for fast inference on deep neural net-
works. In Proceedings of the 37th International
Conference on Machine Learning , volume 119 of
Proceedings of Machine Learning Research , pages
5533–5543, Virtual. PMLR.
François Lagunas, Ella Charlaix, Victor Sanh, and
Alexander Rush. 2021. Block pruning for faster
transformers. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language
Processing , pages 10619–10629, Online and Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Yann LeCun, John Denker, and Sara Solla. 1989. Opti-
mal brain damage. Advances in neural information
processing systems , 2.
Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien
Plu, Lewis Tunstall, Joe Davison, Mario Šaško,
Gunjan Chhablani, Bhavitvya Malik, Simon Bran-
deis, Teven Le Scao, Victor Sanh, Canwen Xu,
Nicolas Patry, Angelina McMillan-Major, Philipp
Schmid, Sylvain Gugger, Clément Delangue, Théo
Matussière, Lysandre Debut, Stas Bekman, Pier-
ric Cistac, Thibault Goehringer, Victor Mustar,
François Lagunas, Alexander Rush, and Thomas
Wolf. 2021. Datasets: A community library for nat-
ural language processing. In Proceedings of the
2021 Conference on Empirical Methods in Natu-
ral Language Processing: System Demonstrations ,
pages 175–184. Association for Computational Lin-
guistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Paul Michel, Omer Levy, and Graham Neubig. 2019.
Are sixteen heads really better than one? Advances
in neural information processing systems , 32.
Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko
Stosic, Dusan Stosic, Ganesh Venkatesh, Chong
Yu, and Paulius Micikevicius. 2021. Accelerat-
ing sparse deep neural networks. arXiv preprint
arXiv:2104.08378 .
NeuralMagic. 2021. Deep sparse: A fast cpu inference
engine.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Lan-
guage models are unsupervised multitask learners.
OpenAI blog , 1(8):9.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
and Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. In EMNLP .
Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and
Preslav Nakov. 2020. Poor man’s BERT:
smaller and faster transformer models. CoRR ,
abs/2004.03844.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 .
Victor Sanh, Thomas Wolf, and Alexander Rush.
2020. Movement pruning: Adaptive sparsity by
ﬁne-tuning. Advances in Neural Information Pro-
cessing Systems , 33:20378–20389.
Iyer Shankar, Dandekar Nikhil, and Csernai Kornel.
2017. First quora dataset release: Question pairs.
S. Shankar. 2017. Identifying quora question pairs hav-
ing the same intent.
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei
Yao, Amir Gholami, Michael W Mahoney, and Kurt
Keutzer. 2020. Q-bert: Hessian based ultra low
precision quantization of bert. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , vol-
ume 34, pages 8815–8821.
Sidak Pal Singh and Dan Alistarh. 2020. Woodﬁsher:
Efﬁcient second-order approximation for neural net-
work compression. Advances in Neural Information
Processing Systems , 33.
Shaden Smith, Mostofa Patwary, Brandon Norick,
Patrick LeGresley, Samyam Rajbhandari, Jared
Casper, Zhun Liu, Shrimai Prabhumoye, George
Zerveas, Vijay Korthikanti, et al. 2022. Using
11

--- PAGE 12 ---
deepspeed and megatron to train megatron-turing
nlg 530b, a large-scale generative language model.
arXiv preprint arXiv:2201.11990 .
Sharath Nittur Sridhar and Anthony Sarah. 2020. Un-
divided attention: Are intermediate layers necessary
for bert? arXiv preprint arXiv:2012.11881 .
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, and Denny Zhou. 2020a. Mobilebert:
a compact task-agnostic bert for resource-limited de-
vices. In ACL.
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, and Denny Zhou. 2020b. Mobilebert:
a compact task-agnostic bert for resource-limited de-
vices. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics ,
pages 2158–2170.
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. 2019. Well-read students learn better:
The impact of student initialization on knowledge
distillation. ArXiv , abs/1908.08962.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information process-
ing systems , 30.
Elena V oita, David Talbot, F. Moiseev, Rico Sennrich,
and Ivan Titov. 2019. Analyzing multi-head self-
attention: Specialized heads do the heavy lifting, the
rest can be pruned. In ACL.
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, and Ming Zhou. 2020. Minilm: Deep self-
attention distillation for task-agnostic compression
of pre-trained transformers. Advances in Neural In-
formation Processing Systems , 33:5776–5788.
Liu Weijie, Zhou Peng, Zhao Zhe, Wang Zhiruo, Deng
Haotang, and Ju Qi. 2020. Fastbert: a self-distilling
bert with adaptive inference time. In Proceedings of
ACL 2020 .
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers) , pages 1112–1122. Association for
Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing: System Demonstrations , pages 38–45, Online.
Association for Computational Linguistics.
Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and
Jimmy J. Lin. 2020. Deebert: Dynamic early exit-
ing for accelerating bert inference. In ACL.
Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, and Zhibin
Xiao. 2021. Rethinking network pruning – under
the pre-train and ﬁne-tune paradigm. In NAACL .
Shixing Yu, Zhewei Yao, Amir Gholami, Zhen Dong,
Sehoon Kim, Michael W Mahoney, and Kurt
Keutzer. 2022. Hessian-aware pruning and optimal
neural implant. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vi-
sion, pages 3880–3891.
Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe
Wasserblat. 2019. Q8bert: Quantized 8bit bert.
2019 Fifth Workshop on Energy Efﬁcient Machine
Learning and Cognitive Computing - NeurIPS Edi-
tion (EMC2-NIPS) , pages 36–39.
Oﬁr Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen,
and Moshe Wasserblat. 2021. Prune once for all:
Sparse pre-trained language models. arXiv preprint
arXiv:2111.05754 .
Qingru Zhang, Simiao Zuo, Chen Liang, Alexander
Bukharin, Pengcheng He, Weizhu Chen, and Tuo
Zhao. 2022. Platon: Pruning large transformer
models with upper conﬁdence bound of weight im-
portance. In International Conference on Machine
Learning , pages 26809–26823. PMLR.
Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao
Chen, Xin Jiang, and Qun Liu. 2020. Ternarybert:
Distillation-aware ultra-low bit bert. arXiv preprint
arXiv:2009.12812 .
M. Zhu and Suyog Gupta. 2018. To prune, or not to
prune: exploring the efﬁcacy of pruning for model
compression. ArXiv , abs/1710.01878.
Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan
Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Aligning books and movies:
Towards story-like visual explanations by watch-
ing movies and reading books. 2015 IEEE Inter-
national Conference on Computer Vision (ICCV) ,
pages 19–27.
A Appendix
A.1 MLPerf Inference benchmark
Following the MLPerf benchmark guidelines on
producing compressed and fast models while
maintaining >99% of the BERT LARGE F1 score on
12

--- PAGE 13 ---
the SQuADv1.1 task, we explore two directions. In
the ﬁrst one, dubbed oBERT-Large, we compound
compress the BERT LARGE model without any
changes to its architecture. Therefore, we apply
4-block downstream pruning to 95% sparsity
followed by the quantization aware training (QAT).
In the second direction we focus on recovering
the BERT LARGE accuracy by compressing an
already compact MobileBERT model, dubbed
oBERT-MobileBERT. More speciﬁcally, we
apply direct layer dropping, leaving only 14
transformer layers out of the original 24, followed
by the 4-block pruning to 50% sparsity and
quantization aware training. We present results in
Table 8, where models were evaluated with the
DeepSparse inference engine, using a server with
two Intel(R) Xeon(R) Platinum 8380 (IceLake)
CPUs with 40 cores each, batch-size 128 and
sequence length 384. For more details please see
our ofﬁcial submission at https://github.com/
neuralmagic/mlperf_inference_results_v2.
1/tree/master/open/NeuralMagic .
A.2 Additional comparisons
Here we reﬂect upon some other methods focused
on efﬁcient inference for LLMs, which are orthog-
onal to weight pruning. For example, Learned To-
ken Pruning (Kim et al., 2022) tries to adaptively
remove unimportant tokens in input sequences and
provides 2x higher throughput at < 1% accuracy
drop; at the same accuracy drop, our compressed
model is able to achieve 8.4x higher throughput.
DeeBERT (Xin et al., 2020) and FastBERT (Wei-
jie et al., 2020) apply an early-exit technique for
inference speedup. The latter achieves 2-3x faster
inference without performance degradation. How-
ever, the method only applies to batch size one.
Nevertheless, in terms of direct comparison, our
compressed models are able to achieve 4x faster
inference on CPUs without accuracy degradation.
Overall, we emphasize the fact that these methods
are complementary to our compression techniques,
so it would be interesting to investigate computa-
tional gains by combining such methods.
A.3 Computational costs
In practice, for the 12-layer BERT BASE model with
d= 85Mencoder weights and block size B= 50 ,
theO(Bd)memory requirement translates to ap-
proximately 17GB, which can be easily kept onthe 24GB RTX 3090 card. While this amount of
memory is available on high-performance GPUs,
it is also straightforward to split the NBBB
tensor along the batch-dimension NBand utilize
additional GPUs or even memory swapping with
CPU. Our implementation updates the inverse Hes-
sian approximation in negligible time, and can run
asynchronously while the next gradient is being
fetched. Computing saliency scores and optimal
weight updates takes only a few seconds.
A.4 Optimal BERT Surgeon (oBERT)
hyper-parameters
Hyper-parameters. The oBERT pruning method
has three tunable hyper-parameters: number of
gradients (m), block size ( B), and dampening ( ).
These are supposed to be tuned with respect to
the model and available computational resources.
In all of our runs, across all models and datasets,
we use the same set of hyper-parameters which we
found to work best for the BERT BASE model on the
SQuAD v1.1 dataset. We conjecture that further
tuning for smaller models (3 and 6-layer models)
could improve their results, but for simplicity and
fairness to other methods, we apply the same ones
found for the BERT BASE .
Ablation studies. The procedure to ﬁnd the opti-
mal set of hyper-parameters for a model consists
of a grid search over the possible hyper-parameter
combinations and one-shot pruning runs to various
high sparsity targets to evaluate the quality of the
pruning approximation for each combination. We
found thatm= 1024 ,B= 50 , and= 10 7pro-
duce state-of-the-art results for a negligible compu-
tational overhead with the BERT BASE model. Fran-
tar et al. (2021) shows that larger block sizes
require more gradients for better approximation.
Given the massive size of the BERT BASE model,
we picked this setup as it was the best performing
one that could still ﬁt on a single 24GB RTX 3090
GPU card. In Figures 3, 4, and 5 we visualize a
fraction of the one-shot pruning ablations with re-
spect to all three hyper-parameters that motivated
us to pick these speciﬁc values.
A.5 Downstream pruning
Teacher preparation. For all downstream prun-
ing runs we make use of the KD from the ﬁne-
tuned BERT BASE teacher outputs. The teacher
is ﬁne-tuned on the corresponding downstream
13

--- PAGE 14 ---
Table 8: MLPerf inference results for oBERT compressed BERT LARGE and MobileBERT models.
Model PrecisionF1 Score
(R=X% recovery)File SizeCompression
RatioThroughput
(samples/sec)Speedup
BERT-Large
dense baselineFP32 90.87 (R=100%) 1.30 GB 1x 15.49 1x
oBERT-Large INT8 90.21 (R=99.27%) 38.20 MB 34x 230.74 15x
oBERT-MobileBERT INT8 90.32 (R=99.39%) 9.56 MB 136x 928.58 60x
50 60 70
Sparsity (%)102030405060708090F1 score
BERTBASE
B = 50
B = 5k
B = 500k
M-FAC
Figure 3: One-shot pruning ablation study with respect
to the block size ( B), withm= 1024 and= 10 7,
on the BERT BASE model and the question-answering
SQuAD v1.1 dataset. M-FAC stands for the full in-
verse Hessian approximation (Frantar et al., 2021).
50 60 70
Sparsity (%)838485868788F1 score
BERTBASE
m = 128
m = 512
m = 1024
Figure 4: One-shot pruning ablation study with respect
to the number of gradients ( m), withB= 50 and
= 10 7, on the BERT BASE model and the question-
answering SQuAD v1.1 dataset.
50 60 70
Sparsity (%)788082848688F1 score
BERTBASE
=106
=108
=107
Figure 5: One-shot pruning ablation study with respect
to the dampening ( ), withm= 1024 andB= 50 ,
on the BERT BASE model and the question-answering
SQuAD v1.1 dataset.
task following the default hyper-parameters for
SQuAD2and GLUE (QQP and MNLI)3.
Pruning setup. In Table 9 we describe in detail all
hyper-parameters for downstream pruning results
presented in Tables 1 and 3. For easier comprehen-
sion, we also visualize learning rate schedules in
Figures 6 and 8, and sparsity schedules in Figures
7 and 9.
3-, 6-layer models. We prepare our 3 and 6 layer
models for downstream runs in two stages: layer
dropping and retraining phase. We drop layers
from our upstream teacher model (more details on
it in Appendix A.6). After dropping, we retrain the
remaining layers, following insights from (Turc
et al., 2019), in the same setup used to prepare the
upstream teacher with addition of the KD from it.
A.6 Upstream pruning
Teacher preparation. We prepare a teacher for
upstream pruning by following some insights from
(Liu et al., 2019). More concretely we start with the
2https://github.com/huggingface/transformers/tree/main/
examples/pytorch/question-answering
3https://github.com/huggingface/transformers/tree/main/
examples/pytorch/text-classiﬁcation
14

--- PAGE 15 ---
10 Epochs 30 Epochs
Batch size16 for SQuAD,
32 for GLUE
Learning rate (initial, ﬁnal)(8e-5, 3e-5) for SQuAD,
(8e-5, 2e-5) for GLUE(8e-5, 8e-6) for SQuAD,
(5e-5, 5e-6) for GLUE
Learning rate schedule linear decay with rewinds
Learning rate rewinds one at epoch=8periodic every 4 epochs,
start at epoch=2
Knowledge Distillation (hardness, temp.) (1.0, 2.0)
Student model12-layer: bert-base-uncased
6-layer: layer drop + pre-train with KD
3-layer: layer drop + pre-train with KD
Teacher model BERT BASE
Prune start epoch=2
Prune end epoch=8 epoch=26
Pruning frequency 2x per epoch once every 4 epochs
Initial sparsity step12-layer: 70%
6-layer: 30%
3-layer: 30%
Sparsity distribution global over all layers
oBERT parametersNumber of gradients m= 1024
Block sizeB= 50
Dampening= 10 7
Table 9: Downstream pruning hyper-parameters used to obtain results presented in Tables 1 and 3.
0 2 4 6 8 10
Epoch2345678Learning rate1e5
SQuAD
GLUE
Figure 6: Visualized learning rate schedule for 10-
epoch downstream runs.
bert-base-uncased4model, adopt pre-training on
two datasets (BookCorpus5& English Wikipedia6)
with focus on the masked language modeling task
(MLM) for 10-epochs with batch size 256 and
learning rate linearly decaying to zero from the
initial value of 1e-4.
4https://huggingface.co/bert-base-uncased
5https://huggingface.co/datasets/bookcorpus
6https://huggingface.co/datasets/wikipedia
0 2 4 6 8 10
Epoch0708090Sparsity (%)Figure 7: Visualized sparsity schedule for 10-epoch
downstream runs with initial sparsity of 70% and target
sparsity of 90%, following the cubic interpolation (Zhu
and Gupta, 2018).
Pruning setup. In Table 10 we describe in de-
tail our upstream pruning recipe. As can be
noticed, our upstream pruning recipe is just a
downscaled version of our 30-epoch downstream-
pruning recipe to 3-epochs.
15

--- PAGE 16 ---
3 Epochs
Datasets BookCorpus & English Wikipedia
Batch size 256
Initial learning rate 5e-4
Learning rate schedule linear decay with rewinds
Learning rate rewinds periodic every 0.5 epochs
Max sequence length 512
Weight decay 0.01
Knowledge Distillation
(hardness, temperature)(1.0, 5.5)
Student model prepared upstream teacher
Teacher model prepared upstream teacher
Pruning frequency 4x per epoch
Table 10: Upstream pruning hyper-parameters.
8 Epochs
Initial learning rate 1.5e-4
Learning rate schedule linear decay to 1.5e-6
Batch size16 for SQuAD,
32 for GLUE
Knowledge Distillation
(hardness, temperature)(1.0, 5.5)
Teacher model BERT BASE
Table 11: Sparse-transfer learning hyper-parameters
used to ﬁne-tune upstream-pruned models at down-
stream tasks. These hyper-parameters are used to ob-
tain results presented in Table 2.
0 5 10 15 20 25 30
Epoch12345678Learning rate1e5
SQuAD
GLUEFigure 8: Visualized learning rate schedule for 30-
epoch downstream runs.
0 5 10 15 20 25 30
Epoch0708090Sparsity (%)
Figure 9: Visualized sparsity schedule for 30-epoch
downstream runs with initial sparsity of 70% and target
sparsity of 90%, following the cubic interpolation (Zhu
and Gupta, 2018).
A.7 Downstream quantization
We perform QAT on top of dense and 4-block
pruned models on SQuAD v1.1 as shown in Table
3. We quantize to 8 bits the embedding matrices,
linear modules of all encoder units which includes
matrices in their attention and feed forward layers,
and the linear module of the output layer. Weights
that were pruned are kept constant (zero) during
quantization (sparsity mask preserved). Non-linear
operations within the Softmax, LayerNorm and
GeLU are not quantized. For each dense and 4-
block pruned model in Table 3, we perform a total
of ten epochs training where the quantization ob-
servers are active for the ﬁrst ﬁve and the remaining
is ﬁne-tuning. We do hyper-parameter search over
the learning rates of 1e-4, 8e-5, 5e-5, 3e-5 and the
distillation hardness of 0.9 and 1.0. We then pick
the model with the best F1 score.
16

--- PAGE 17 ---
A.8 Additional performance metrics
Due to the space constraints, in the paper we report
F1 score for SQuAD v1.1, matched accuracy for
MNLI, and accuracy for QQP dataset. As all of our
hyper-parameters for MNLI and QQP are exactly
the same, we refer to these two datasets as GLUE.
In Table 12 we report the additional metrics too:
exact match (EM) for SQuAD v1.1, mismatched
accuracy for MNLI, and F1 score for QQP dataset.
Tables 15 and 16 present standard deviations of the
corresponding results in Tables 1, 2 and 12. Finally,
Table 14 presents the exact-match metric for the
corresponding results in Table 3.
TaskBERT
BASESparsitySoft
MvPoBERT
(ours)oBERT
(ours)
Epochs 10 Epochs 30 Epochs
SQuAD
EM81.2280%
90%
97%-
76.60
72.70-
80.76
76.1482.08
81.12
78.11
MNLI
mm-acc85.0680%
90%
97%-
81.80
80.10-
83.58
80.6784.91
84.35
82.01
QQP
F188.0080%
90%
97%-
86.80
85.50-
87.69
87.0588.63
88.30
87.66
Table 12: Additional evaluation metrics for results pre-
sented in Table 1.
TaskBERT
BASESparsityPrune
OFAoBERT
(ours)
SQuAD
EM81.4290%
97%79.83
-81.43
76.90
MNLI
mm-acc85.0690%
97%82.43
-83.78
81.13
QQP
F188.0090%
97%87.72
-87.81
86.97
Table 13: Additional evaluation metrics for results pre-
sented in Table 2.
A.9 Inference speedups and compression
ratios of compressed models
Details on the results shown in Figure 2 are drawn
from Table 17. As shown in the results, not all
compound compressed models yield improvements
in inference or compression relative to retained
model performance but those that do allow for
massive improvements.Layers Sparsity Unstructured 4-block +QAT
120%
80%
90%82.71
82.08
81.1282.71
81.46
80.1481.99
80.57
78.84
60%
80%
90%81.17
81.15
79.1681.17
79.55
77.6580.85
78.27
76.56
30%
80%
90%76.62
75.62
73.6176.62
74.07
71.3676.06
72.70
70.00
Table 14: Additional evaluation metric (exact-match)
for results presented in Table 3.
Task SparsityoBERT
(ours)
Epochs 30 Epochs
SQuAD
F1, EM80%
90%
97%0.11, 0.03
0.13, 0.13
0.11, 0.17
MNLI
m, mm80%
90%
97%0.14, 0.13
0.05, 0.04
0.35, 0.22
QQP
acc, F180%
90%
97%0.08, 0.08
0.04, 0.06
0.05, 0.08
Table 15: Standard deviations for results presented in
Tables 1 and 12.
Task SparsityoBERT
(ours)
SQuAD
F1, EM90%
97%0.13, 0.13
0.03, 0.14
MNLI
m, mm90%
97%0.08, 0.24
0.17, 0.35
QQP
acc, F190%
97%0.06, 0.07
0.09, 0.18
Table 16: Standard deviations for results presented in
Table 2 and 13.
A.10 Responsible NLP Research -
Reproducibility Checklist
In addition to many items from the “Reproducibil-
ity Checklist” which are already carefully ad-
dressed throughout the paper and Appendix sec-
tions, here we provide the remaining details to
facilitate reproducibility of our results.
A.10.1 Scientiﬁc Artifacts
Datasets. Our experiments use existing and well
established benchmarks for pre-training and ﬁne-
17

--- PAGE 18 ---
Layers Sparsity
(%)Compression
MethodF1 score F1 recall
(%)Throughput
(items per sec.)Speedup
DeepSparseModel size
(gzip MB)Compression
Ratio (w.r.t. gzip)
12 0 none 88.54 100.00 65.81 1.00 384.7 1.00
12 80 unstructured 89.04 100.56 222.66 3.38 173.1 2.22
12 90 unstructured 88.31 99.74 292.40 4.44 140.1 2.75
12 80 4-block+QAT 87.89 99.26 552.22 8.39 37.8 10.18
6 80 unstructured 88.20 99.62 419.68 6.38 128.3 3.00
6 90 unstructured 86.78 98.01 663.02 10.07 111.8 3.44
6 80 4-block+QAT 86.10 97.24 989.54 15.04 26.2 14.70
3 80 unstructured 84.08 94.96 737.62 11.21 105.9 3.63
3 90 unstructured 82.50 93.18 974.00 14.80 97.7 3.94
3 80 4-block+QAT 82.04 92.66 1892.27 28.75 20.3 18.92
Table 17: Compression effects on model size and inference speed, evaluated at batch size 32 with sequence length
128 on SQuAD v1.1 dataset. Evaluated at the c5.12xlarge AWS instance.
tuning of LLMs. Each dataset was used without
any additional forms of modiﬁcations. Given that
we did not modify any of the datasets, we did not
inspect for personal, sensitive, or offensive con-
tent, nor did we perform any kind of anonymiza-
tion. For pre-training, we make use of the Toronto
Book Corpus (TBC) (Zhu et al., 2015)7and the
wikipedia.20200501.en (Foundation)8. For ﬁne-
tuning we make use of SQuAD v1.1 (Rajpurkar
et al., 2016)9, Quora Duplicate Question Dataset
(QQP) (Shankar, 2017)10, and Multi-Genre Nat-
ural Language Inference (MNLI) (Williams et al.,
2018)11datasets. All these datasets are pub-
licly available via HuggingFace datasets reposi-
tory (Lhoest et al., 2021). The terms of usage and
further details on each dataset can be found in their
respective repositories.
Models. The model used as a starting point for all
of our experiments is BERT BASE , publicly avail-
able via HuggingFace Hub12. All other models
presented in this paper will be released in openly-
available repositories along with their compression
recipes, training metrics and hyper-parameters.
A.10.2 Dataset Statistics
Dataset statistics are detailed in Table 18.
A.10.3 Computational Experiments
Upstream. All upstream runs are in general com-
putationally expensive due to the large batch sizes
7https://huggingface.co/datasets/bookcorpus
8https://huggingface.co/datasets/wikipedia
9https://huggingface.co/datasets/squad
10https://huggingface.co/datasets/glue
11https://huggingface.co/datasets/glue
12https://huggingface.co/bert-base-uncasedDataset Train Eval
SQuAD (examples) 87599 10570
MNLI (examples) 392702 19628
QQP (examples) 363,846 40,430
Wikipedia (words) 6078422 -
TBC (words) 74004228 -
Table 18: Statistics for training and evaluation datasets
and huge datasets. In our experiments we make
use of 4x A100 40GB NVIDIA GPUs. In this
conﬁguration, a single training epoch takes approx-
imately 6 hours. Since the cost of such a large com-
pute instance is high, these experiments were only
run with a single seed and without major hyper-
parameter exploration.
Downstream. Our downstream experiments make
use of various different GPU cards that were at
out disposal: 16GB V100, 11GB RTX 2080 Ti,
and 24GB RTX 3090. Each training epoch takes
approximately 30 minutes, and as a result the 30
epoch runs take approximately 15 hours. For these
experiments, we report mean results of three runs
with different random seeds.
DeepSparse inference. We pair our compressed
models with DeepSparse (NeuralMagic, 2021) a
publicly-available sparsity-aware CPU inference
engine. This CPU runtime can leverage both
structured and unstructured sparsity, and quanti-
zation to deliver high performance on commodity
CPUs. We ran DeepSparse on a 24-core Intel AWS
c5.12xlarge server with 24 cores, 96 vCPUs, 192
GB of RAM and an A VX-512 compatible instruc-
tion set. All models are exported using the standard
18

--- PAGE 19 ---
ONNX13format.
A.10.4 Computational Packages
Our experiments build on publicly available li-
braries to ensure ease of reproduction and exten-
sibility. All of our implementations, training and
evaluation code are built on top of HuggingFace’s
Transformers14and Datasets15libraries, Neural-
Magic’s SparseML16library for model compres-
sion, and their DeepSparse17engine for efﬁcient
inference on commodity CPUs.
13https://onnx.ai/
14https://github.com/huggingface/transformers
15https://github.com/huggingface/datasets
16https://github.com/neuralmagic/sparseml
17https://github.com/neuralmagic/deepsparse
19
