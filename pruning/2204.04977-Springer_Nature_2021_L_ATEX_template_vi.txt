# 2204.04977.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2204.04977.pdf
# Kích thước tệp: 492111 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Springer Nature 2021 L ATEX template
Tỉa tỉnh dựa trên Chính quy hóa các Trọng số Không liên quan trong Kiến trúc Mạng thần kinh Sâu
Giovanni Bonetta1*, Matteo Ribero1và Rossella Cancelliere1
1Khoa Khoa học Máy tính, Đại học Turin, Via Pessinetto, Turin, 10149, Ý.
*Tác giả liên hệ. E-mail: giovanni.bonetta@unito.it;
Tác giả đóng góp: matteo.ribero@edu.unito.it; rossella.cancelliere@unito.it;
Tóm tắt
Các mạng nơ-ron sâu khai thác hàng triệu tham số hiện đang là chuẩn mực. Đây là một vấn đề tiềm tàng vì số lượng lớn các phép tính cần thiết cho việc huấn luyện, và có thể mất khả năng tổng quát hóa của các mạng có quá nhiều tham số. Chúng tôi đề xuất trong bài báo này một phương pháp để học các cấu trúc thần kinh thưa thớt thông qua một cách tiếp cận chính quy hóa xác định các trọng số không liên quan trong bất kỳ loại lớp nào (tức là, các lớp tích chập, kết nối đầy đủ, chú ý và nhúng) và có chọn lọc thu nhỏ chuẩn của chúng trong khi thực hiện cập nhật lan truyền ngược tiêu chuẩn cho các lớp liên quan. Kỹ thuật này, là một cải tiến của suy giảm trọng số cổ điển, dựa trên định nghĩa của một thuật ngữ chính quy hóa có thể được thêm vào bất kỳ hàm mất mát nào bất kể hình thức của nó, dẫn đến một khung thống nhất có thể khai thác trong nhiều bối cảnh khác nhau. Việc loại bỏ thực tế các tham số được xác định là không liên quan được xử lý bởi một thuật toán tỉa lặp.
Để khám phá khả năng sử dụng liên ngành của kỹ thuật đề xuất, chúng tôi kiểm tra nó trên sáu nhiệm vụ phân loại hình ảnh và tạo ngôn ngữ tự nhiên khác nhau, trong đó bốn nhiệm vụ dựa trên các tập dữ liệu thực. Chúng tôi đạt được hiệu suất tiên tiến trong một trong bốn nhiệm vụ hình ảnh trong khi có được kết quả tốt hơn so với các đối thủ cạnh tranh cho những nhiệm vụ khác và một trong hai nhiệm vụ tạo ngôn ngữ được xem xét, cả về mặt nén và số liệu.
Từ khóa: Thưa thớt, Tỉa tỉnh, Chính quy hóa, NLP, Xử lý Hình ảnh

1 Giới thiệu
Các mô hình học sâu đã liên tục thiết lập trong vài năm qua các hiệu suất tiên tiến mới trong một loạt các lĩnh vực khác nhau, bao gồm xử lý hình ảnh [1{4], tạo chú thích hình ảnh [5, 6], tạo ngôn ngữ [7, 8], và dịch máy [9, 10].
Tuy nhiên, các tài nguyên cần thiết để huấn luyện chúng một cách đúng đắn có thể cấm đoán, vì số lượng trọng số được sử dụng cho các nhiệm vụ này có thể dễ dàng lên tới vài triệu. Do đó, những chi phí hiệu suất ngày càng tăng này đã thúc đẩy các nhà khoa học tìm kiếm các kỹ thuật hạn chế kích thước của các tham số kiến trúc thần kinh.
Một cách tiếp cận hiệu quả để giảm độ phức tạp này là tính thưa thớt, được định nghĩa là tính chất mà một tập con đáng kể của các trọng số mô hình có giá trị bằng không (tức là, các ma trận trọng số của các lớp là thưa thớt). Tính thưa thớt cho phép yêu cầu tính toán và lưu trữ nhỏ hơn, và như được chỉ ra, ví dụ, trong [11] và [12], các kiến trúc sâu chịu đựng nó tốt.
Nó có thể rút ngắn thời gian huấn luyện và giảm dấu chân bộ nhớ của các mạng thông thường để phù hợp với các thiết bị di động, chỉ với chi phí nhỏ về độ chính xác. Các mô hình nhỏ hơn dễ gửi đến các thiết bị biên và cũng ít tham lam năng lượng hơn đáng kể, như được ghi nhận trong [13]: "phần lớn tiêu thụ năng lượng đến từ việc lấy các tham số mô hình từ bộ lưu trữ dài hạn của thiết bị di động vào bộ nhớ tạm thời của nó". Ngoài ra, tính thưa thớt cũng là một giải pháp để cải thiện hiệu suất suy luận thông qua kiểm soát quá khớp và, như được đề xuất bởi [14], có thể dẫn đến hiệu suất cải thiện trong các tình huống học chuyển giao.
Hai cách tiếp cận chính tạo ra tính thưa thớt có thể được tìm thấy trong tài liệu, được gọi là không có cấu trúc hoặc có cấu trúc, tùy thuộc vào việc các trọng số đơn lẻ hoặc toàn bộ các nhóm trọng số và/hoặc nơ-ron có cấu trúc được loại bỏ.
Nhiều phương pháp đã được đề xuất trong vài năm qua để đạt được những mục tiêu này: một đánh giá không đầy đủ về tài liệu liên quan gần đây có thể được tìm thấy trong Phần 2.
Các kỹ thuật dựa trên chính quy hóa L2, được chi tiết trong Phần 3, là một trong những kỹ thuật phổ biến nhất: chúng thêm một thuật ngữ phạt vào hàm chi phí để thu nhỏ các giá trị tham số. Tất cả các tham số giảm xuống dưới một ngưỡng được xác định trước sau đó được đặt thành không, do đó có được các kiến trúc thưa thớt.
Một nhược điểm của những phương pháp này là các chuẩn trọng số thần kinh đều được đẩy gần về không mà không tính đến tính liên quan của trọng số trong kiến trúc thần kinh, như được thảo luận chi tiết trong [15, 16].
Công việc của chúng tôi dựa vào cách tiếp cận này: chúng tôi đề xuất một hàm mất mát mới giữ một thuật ngữ chính quy hóa phù hợp là một chuyên môn hóa của suy giảm trọng số nổi tiếng. Nó cho phép dẫn xuất một quy tắc cập nhật trọng số mới có chọn lọc giảm chuẩn của các trọng số không liên quan, trong khi thực hiện cập nhật lan truyền ngược tiêu chuẩn cho các trọng số liên quan. Cập nhật trọng số trực tiếp theo từ tối ưu hóa mất mát, không yêu cầu định nghĩa các quy tắc cập nhật đặc biệt, như thường được thực hiện (xem [11, 15{17]).
Các trọng số giảm sau đó được tỉa để làm thưa thớt kiến trúc thần kinh.
Kỹ thuật của chúng tôi là tổng quát, vì thuật ngữ chính quy hóa được đề xuất có thể được thêm vào bất kỳ hàm mất mát nào bất kể hình thức của nó và tạo thành một khung thống nhất có khả năng khai thác cho nhiều ứng dụng khác nhau.
Chúng tôi xác minh hiệu quả của phương pháp trong bối cảnh phân loại hình ảnh và tạo ngôn ngữ tự nhiên, làm thưa thớt các kiến trúc thần kinh dựa trên tích chập (LeNet-5, ResNet32/50) và tự chú ý transformer, tương ứng. Trong khi nhiệm vụ trước đã được giải quyết trong tài liệu, vẫn hiếm khi tìm thấy các kỹ thuật thưa thớt được áp dụng cho các kiến trúc tạo ngôn ngữ.
Chúng tôi đạt được kết quả tiên tiến trong một trong bốn nhiệm vụ phân loại hình ảnh, trong khi thiết lập, theo hiểu biết tốt nhất của chúng tôi, hiệu suất tiên tiến mới cho những nhiệm vụ khác và một trong hai nhiệm vụ tạo ngôn ngữ được xem xét.
Phần còn lại của bài báo này được tổ chức như sau: Phần 2 chứa một tổng quan về các công trình liên quan về tỉa có cấu trúc và không có cấu trúc. Trong Phần 3, các nền tảng lý thuyết của mô hình được trình bày, và Phần 4 phác thảo các chi tiết của thuật toán tỉa. Các phần 5 và 6 mô tả các tập dữ liệu, chi tiết triển khai và kết quả có được trong cả hai bối cảnh.

2 Các Công trình Liên quan
Trong phần này, chúng tôi phác thảo một số cách tiếp cận gần đây điều tra các kỹ thuật thưa thớt có cấu trúc và không có cấu trúc. Mặc dù bài báo của chúng tôi đề xuất một phương pháp tỉa không có cấu trúc, để hoàn thiện và để giới thiệu một số công trình được sử dụng để so sánh trong Phần 5, chúng tôi tóm tắt ngắn gọn trong phần sau một vài phương pháp gần đây cho tính thưa thớt có cấu trúc.
Trong bối cảnh này, các cách tiếp cận tỉa khối/lớp, tỉa nhóm/bộ lọc/kênh, và tỉa nhân đóng một vai trò quan trọng, đặc biệt cho các mạng sâu dựa trên tích chập.
Tỉa khối/lớp ([21, 22]) nhằm thu nhỏ một mạng bằng cách loại bỏ toàn bộ các khối hoặc lớp; đặc biệt, các kỹ thuật tỉa nhóm được sử dụng để loại bỏ các nhóm dư thừa. Một phương pháp tinh tế hơn là tỉa bộ lọc/kênh ([18, 19, 25]), được sử dụng để loại bỏ các bộ lọc dư thừa. Do đó, chiều của các bản đồ đặc trưng được giảm, đến mức thậm chí toàn bộ các kênh có thể bị loại bỏ.
Tỉa nhân ([20, 23, 24]) nhằm loại bỏ các đơn vị trích xuất đặc trưng cơ bản, các ma trận k x k (nhân) trong các bộ lọc, tạo ra theo cách này các mạng thưa thớt với độ chi tiết tinh tế. Lin et al. [20] đã thêm một thuật ngữ chính quy hóa trong quá trình huấn luyện để tỉa các kết nối được đặc trưng bởi sức mạnh khớp thần kinh ít hơn. Zhu và Pei [23] đề xuất tỉa tiến bộ với ánh xạ độ nổi bật của các kênh đầu vào-đầu ra để giải quyết vấn đề thiếu kênh trong quá trình tỉa và cải thiện hiệu quả.
Trong bối cảnh tỉa không có cấu trúc, các phương pháp dựa trên việc chỉ loại bỏ các trọng số chuẩn nhỏ là đơn giản nhất [11].
Các phương pháp dựa trên thống kê Bayesian tạo thành một khả năng khác để đạt được tính thưa thớt: cách tiếp cận chia sẻ trọng số mềm (SWS) [12] thành công trong việc giảm số lượng tham số cần được lưu trữ, chia sẻ các tham số dư thừa giữa các lớp. Các trọng số được biểu diễn như một hỗn hợp các phân phối Gaussian, trong đó các thành phần của hỗn hợp được học.
Dropout biến phân thưa thớt [29] xử lý dropout như tiếng ồn được áp dụng cho các đầu vào của tất cả các lớp thần kinh; nó đại diện cho nỗ lực đầu tiên sử dụng kỹ thuật dropout để làm thưa thớt mạng, mở đường cho nghiên cứu mới trong lĩnh vực này (xem, ví dụ, [30]). Dropout có mục tiêu [17] là một cách tiếp cận dựa trên dropout khác tập trung vào việc tách biệt một tập con quan trọng của các trọng số khỏi các trọng số không quan trọng để có được tỉa ổn định. Trước khi tính toán gradient cho mỗi cập nhật trọng số, dropout có mục tiêu ngẫu nhiên chọn một tập hợp các đơn vị hoặc trọng số để bỏ, sử dụng các chuẩn L1 và L2 làm proxy cho tầm quan trọng của trọng số, và sau đó tính toán gradient cho các trọng số còn lại.
Các tác giả của SNIP [31] định nghĩa một tiêu chí dựa trên độ nổi bật trên các kết nối mạng để chọn các kết nối quan trọng nhất cho nhiệm vụ đã cho. Sau đó, họ tỉa mạng tương ứng theo cách một lần và cuối cùng huấn luyện mô hình để có được độ chính xác tốt. Nhược điểm của thuật toán này là bản chất một lần của nó ngăn cản nó đạt được mức độ thưa thớt có thể so sánh với các cách tiếp cận lặp.
Guo et al. đã cố gắng giải quyết vấn đề này bằng cách đề xuất kỹ thuật DNS [32], trong đó các kết nối được tỉa không chính xác có thể được tái lập bởi một quá trình ghép nếu được đánh giá là quan trọng, làm cho nỗ lực tỉa trở nên động.
Như đã nêu trong Phần 1, chính quy hóa có thể được sử dụng như một cách tiếp cận thu nhỏ và tỉa đặc biệt hiệu quả đối với tính thưa thớt [33] nhưng với nhược điểm là tất cả các trọng số đều bị phạt một cách không phân biệt.
Một phương pháp hiệu quả để tránh vấn đề này được trình bày trong [15], nơi ý tưởng về độ nhạy dựa trên đầu ra được giới thiệu: các trọng số được phạt có chọn lọc tùy thuộc vào khả năng của chúng để tạo ra các biến thể trong đầu ra mạng khi thay đổi. Một tinh chỉnh của phương pháp này được trình bày trong [16], nơi kết quả tiên tiến được đạt được trong phân loại hình ảnh nhờ việc giới thiệu độ nhạy dựa trên mất mát, nhằm thu nhỏ các trọng số đóng góp ít nhất cho giá trị mất mát cuối cùng.
Một vấn đề thường được lưu ý với các kỹ thuật không có cấu trúc là mạng thưa thớt thu được có phân phối không đều của các trọng số null, yêu cầu hỗ trợ phần mềm/phần cứng cụ thể để tăng tốc. Vì lý do này, thường không quan sát thấy cải thiện đáng kể trong tốc độ suy luận hiện tại, nhưng chủ đề vẫn thú vị vì các nhà sản xuất phần cứng đang phát triển các chip mới được thiết kế để khai thác mô hình không có cấu trúc1.

3 Phương pháp Tỉa dựa trên Liên quan: Lý thuyết
Các phương pháp chính quy hóa biến một vấn đề ban đầu không ổn định, đặt sai thành một vấn đề đặt đúng; chúng giới hạn khả năng của các mô hình bằng cách thêm một hình phạt chuẩn tham số vào hàm mất mát L, để kiểm soát quá khớp và giảm lỗi kiểm tra (xem [34]).
Một trong những hình phạt chuẩn tham số phổ biến nhất là L2, thường được biết đến như chính quy hóa Tikhonov [35], hồi quy ridge hoặc suy giảm trọng số.
Trong bối cảnh thần kinh, mất mát được chính quy hóa ~L phụ thuộc vào mỗi trọng số wn_i,j thuộc lớp n và kết nối các nơ-ron i và j và có dạng:
~L( w) =L( w) +kwk2=L( w) +X n;i;jjwn i;jj2(1)
trong đó w là vector có các phần tử là wn_i,j và là tham số chính quy hóa. Việc áp dụng lặp lại thuật toán gradient descent ngẫu nhiên (SGD) tại bước thời gian t
wn i;j(t)wn i;j(t1)@~L( w) @wn i;j(2)
dẫn đến quy tắc cập nhật suy giảm trọng số nổi tiếng
wn i;j(t) =wn i;j(t1)@L( w) @wn i;j2wn i;j (3)
trong đó là tốc độ học. Do đó, các trọng số thần kinh được đẩy gần về không mà không tính đến tính liên quan của chúng trong kiến trúc thần kinh.
Đóng góp chính của công việc này là đề xuất một hàm mất mát mới ^L, được sửa đổi so với eq. (1), cho phép chúng tôi có được một quy tắc suy giảm trọng số có chọn lọc mới chỉ thu nhỏ độ lớn của những trọng số không liên quan đến lỗi cuối cùng. Chúng tôi đề xuất sửa đổi thuật ngữ chính quy hóa trong eq. (1) bằng cách nhân nó với một hệ số đo lường mức độ giá trị mất mát cuối cùng bị ảnh hưởng bởi việc sửa đổi wn_i,j.
Đại lượng j@L @wn i;jj có vẻ là một ứng cử viên tốt cho điều này: các giá trị đạo hàm nhỏ, ví dụ, chỉ ra rằng ngay cả một biến thiên lớn trong wn_i,j cũng không gây ra các biến thiên mất mát lớn, tức là, các thay đổi trọng số không liên quan đến giá trị mất mát cuối cùng. Ngoài ra, các giá trị đạo hàm lớn không thú vị vì chúng đặc trưng cho các trọng số liên quan, và chúng tôi nhằm đẩy về không (và tỉa) chỉ các trọng số không liên quan. Đạo hàm này không bị chặn trên, đó là một vấn đề có thể xảy ra trong việc bảo tồn các tính chất hội tụ.
Xem xét những yêu cầu này, do đó chúng tôi định nghĩa hệ số không liên quan In;i;j như:
In;i;jexp(j@L @wn i;jj);0<In;i;j<1:(4)
In;i;j bị chặn và nhận các giá trị gần 1 cho các trọng số không liên quan và gần 0 cho các trọng số liên quan. Hơn nữa, nó có tính chất hữu ích là gần như khả vi ở mọi nơi.
Bây giờ chúng ta có thể định nghĩa hàm mất mát mới ^L, được sửa đổi so với eq. (1) để có chọn lọc giới hạn độ lớn của các trọng số:
^L( w)L( w) +X n;i;j(In;i;jjwn i;jj2) = =L( w) +X n;i;j(exp(j@L @wn i;jj)jwn i;jj2) (5)
Việc áp dụng lặp lại thuật toán gradient descent ngẫu nhiên cho ^L cho phép chúng tôi dẫn xuất quy tắc cập nhật trọng số mới:
wn i;j(t)wn i;j(t1)@^L @wn i;j= =wn i;j(t1)@L @wn i;j2exp(j@L @wn i;jj)wn i;j jwn i;jj2exp(j@L @wn i;jj)(1)@ @wn i;jj@L @wn i;jj= =wn i;j(t1)@L @wn i;j2exp(j@L @wn i;jj)wn i;j +jwn i;jj2exp(j@L @wn i;jj)sgn(j@2L @wn i;j2j) (6)
Như thường được thực hiện trong các phương pháp tối ưu hóa đạo hàm bậc nhất, chúng ta có thể bỏ qua thuật ngữ đạo hàm bậc hai, sao cho eq. (6) trở thành:
wn i;j(t) =wn i;j(t1)@L @wn i;j2exp(j@L @wn i;jj)wn i;j (7)
Các cập nhật trọng số khác nhau được thực hiện trong hai trường hợp được xác định bởi các giá trị cực của I:
•I0: trong trường hợp này, trọng số là liên quan. Thuật ngữ thứ ba trong eq. (7) xấp xỉ bằng không, nên một cập nhật tiêu chuẩn được thực hiện, không có sự giảm có mục tiêu trong chuẩn trọng số.
•I1: trong trường hợp này, trọng số là không liên quan. Bởi vì I1 ngụ ý @L @wn i;j0, thuật ngữ thứ hai trong eq. (7) xấp xỉ bằng không, và quy tắc cập nhật trở thành:
wn i;j(t)'wn i;j(t1)2wn i;j (8)
Chúng ta có thể thấy rằng trong trường hợp này, trọng số thực sự được đẩy về không tại mỗi lần lặp, bởi vì nếu wn_i,j>0 thì wn_i,j<0 (tức là, chuẩn của một trọng số không liên quan dương được giảm); ngược lại, nếu wn_i,j<0 thì wn_i,j>0.
Để hiểu rõ hơn về cách hệ số không liên quan I hoạt động và ảnh hưởng đến chuẩn của trọng số, chúng tôi báo cáo ở đây như một ví dụ các biểu đồ của các phân phối ban đầu và cuối cùng (tức là, sau khi huấn luyện) của hai đại lượng này cho hai lớp của kiến trúc tích chập ResNet50, được phân tích chi tiết trong Phần 5.5.
Hình 1 cho thấy biểu đồ của các giá trị hệ số không liên quan của các trọng số thuộc lớp tích chập đầu tiên (chứa 9,408 trọng số) ở đầu (xanh) và cuối (đỏ) của việc huấn luyện mô hình này trên tập dữ liệu ImageNet. Trong trường hợp này, phần lớn các trọng số có giá trị I rất thấp cho toàn bộ quá trình huấn luyện, có nghĩa là các trọng số tương ứng thực sự liên quan và không nên được tỉa. Hình 2 cho thấy phân phối của chuẩn trọng số còn lại: lưu ý rằng chỉ 2,731 phần tử được tỉa trong số 9,408, tương ứng với 29% trọng số lớp. Con số này thấp hơn nhiều so với 73.85%, tỷ lệ phần trăm trọng số được tỉa trên toàn bộ mô hình (như được báo cáo trong Bảng 5), và cho thấy các trọng số liên quan ít có khả năng bị tỉa hơn so với các trọng số khác.
Hình 3 cho thấy biểu đồ của các giá trị hệ số không liên quan cho lớp tích chập thứ 18 trong mô hình. Trong trường hợp này, phần lớn các giá trị I ở đầu quá trình huấn luyện gần 1, có nghĩa là các trọng số tương ứng không liên quan và có thể được tỉa. Như thấy trong Hình 4, chiều cao của biểu đồ trọng số cuối cùng (đỏ) giảm đáng kể so với biểu đồ ban đầu (xanh) vì việc huấn luyện gây ra sự giảm đáng kể trong số lượng trọng số, lên đến 23.0% số trọng số ban đầu. Ngoài ra, quan sát phân phối cuối cùng của các giá trị hệ số không liên quan, chúng ta có thể lưu ý rằng nó chắc chắn đồng đều hơn, có nghĩa là số lượng trọng số không liên quan giảm rất nhiều theo tỷ lệ phần trăm.

4 Mô tả Thuật toán Tỉa
Thuật toán tỉa tiến hành như sau:
1. Chúng tôi có được một checkpoint để tinh chỉnh. Một lựa chọn phổ biến là tìm nó trong tài liệu hoặc huấn luyện nó tự mình. Một khả năng khác là sử dụng một checkpoint được khởi tạo ngẫu nhiên.
2. Chúng tôi tinh chỉnh checkpoint (hoặc huấn luyện nó từ đầu khi bắt đầu từ một khởi tạo ngẫu nhiên) bằng cách sử dụng thuật ngữ chính quy hóa được đề xuất của chúng tôi, như trong Phương trình (5). Trong bước này, bất kỳ bộ tối ưu hóa nào cũng có thể được sử dụng. Trong quá trình tinh chỉnh, chúng tôi đánh giá hiệu suất mô hình trên tập validation mỗi khoảng thời gian đánh giá bước và:
•tỉa. Nếu hiệu suất validation cao hơn một ngưỡng dưới do người dùng định nghĩa, một tỷ lệ cố định (tỷ lệ phần trăm tỉa) của các tham số mô hình còn lại được tỉa, chọn từ những tham số có độ lớn thấp hơn.
•không tỉa. Nếu hiệu suất validation thấp hơn ngưỡng dưới do người dùng định nghĩa, mô hình không sẵn sàng để được tỉa, nên việc tinh chỉnh tiến hành bình thường.
3. Hai bước cuối này của quá trình tinh chỉnh được lặp lại cho đến khi mô hình đạt đến một plateau hiệu suất validation (nên nó không thể được tỉa thêm).
4. Chúng tôi thực hiện một giai đoạn tinh chỉnh cuối cùng không có chính quy hóa, nhằm có được checkpoint hiệu suất tốt nhất.
Khi tiến xa trong huấn luyện, mô hình thường đạt đến một plateau độ chính xác, làm cho việc thuật toán đạt đến một bước tỉa trở nên khó khăn. Vì điều này, chúng tôi giới thiệu một lịch trình suy giảm mũ trên α giảm thuật ngữ chính quy hóa giữa hai bước validation. Quy trình này ưu tiên độ chính xác so với sparsification và giúp mô hình phá vỡ plateau hiệu suất và vượt qua ngưỡng dưới, dẫn đến việc tỉa thêm.
Một số siêu tham số cần thiết để triển khai quá trình này:
•khoảng thời gian đánh giá: số bước giữa hai đánh giá hiệu suất validation.
•ngưỡng dưới: ngưỡng dưới hiệu suất. Nó được chọn thấp hơn một chút so với hiệu suất tiên tiến. Đối với các nhiệm vụ phân loại hình ảnh, hiệu suất được đo bằng độ chính xác, trong khi đối với các nhiệm vụ tạo ngôn ngữ, chúng tôi sử dụng chỉ số BLEU [36], được giới thiệu ngắn gọn trong Phần 6.
•tỷ lệ phần trăm tỉa: Tỷ lệ phần trăm của các tham số không bằng không còn lại được tỉa tại mỗi bước tỉa.
Nếu không được nêu khác, các siêu tham số được chọn thông qua tìm kiếm lưới trong cả giai đoạn huấn luyện và tinh chỉnh.
Chúng tôi thực hiện tất cả các thí nghiệm của mình bằng bốn GPU Nvidia TITAN RTX 24Gb, và mã nguồn có sẵn tại https://github.com/giobin/Applied Intelligence sparsity.

5 Hình ảnh
Chúng tôi kiểm tra phương pháp của mình trên bốn tập dữ liệu phân loại hình ảnh khác nhau được chọn từ các điểm chuẩn phổ biến nhất trong tài liệu cho nghiên cứu tính thưa thớt. Mỗi tập dữ liệu được xử lý bằng các kiến trúc tạo ra hiệu suất tiên tiến.

5.1 Mô tả Tập dữ liệu
MNIST
MNIST [37], được tạo thành từ 70,000 hình ảnh thang xám 28x28 chứa các số viết tay; tập dữ liệu được chia thành tập huấn luyện (60,000 hình ảnh) và tập kiểm tra (10,000 hình ảnh).

Fashion-MNIST
Fashion-MNIST [38], dựa trên sự lựa chọn của trang web Zalando, là một tập dữ liệu gồm các hình ảnh thang xám 28 ×28 của 70,000 sản phẩm thời trang từ 10 danh mục, với 7,000 hình ảnh mỗi danh mục. Tập huấn luyện có 60,000 mẫu, và tập kiểm tra có 10,000, và, mặc dù tương tự như MNIST, nhưng thách thức hơn.

CIFAR-10
CIFAR-10 [39], từ Viện Nghiên cứu Nâng cao Canada, là một tập con của tập dữ liệu Tiny Images [40] và gồm 60,000 hình ảnh màu 32x32 được gắn nhãn với một trong 10 lớp loại trừ lẫn nhau: máy bay, ô tô, chim, mèo, hươu, chó, ếch, ngựa, tàu và xe tải. Có 6,000 mẫu mỗi lớp, chia thành 5,000 cho huấn luyện và 1,000 cho kiểm tra.

ImageNet
ImageNet [41] được sắp xếp theo phân cấp danh từ WordNet [42] và là một cơ sở dữ liệu hình ảnh thực, trong đó mỗi nút trong phân cấp được đại diện bởi hàng ngàn hình ảnh; nó chứa hơn 20,000 danh mục. Tổng cộng, 14 triệu hình ảnh được chú thích bằng tay, và đối với một triệu trong số đó, các hộp giới hạn cũng được cung cấp. Các hình ảnh RGB có kích thước trung bình 469x387 pixel nhưng thường được tiền xử lý bằng cách lấy mẫu chúng ở 256x256 pixel.

5.2 LeNet-5 trên MNIST
Chi tiết về kiến trúc LeNet-5 được đưa ra trong Bảng 1. Mạng này [43] gồm một lớp tích chập (Conv) với 6 bộ lọc 5x5, một lớp pooling 2x2, một lớp tích chập với 10 bộ lọc 5x5, một lớp pooling 2x2 khác và ba lớp kết nối đầy đủ (FC) (120, 84, 10), tổng cộng 431,080 tham số.
Vì MNIST là một tập dữ liệu dễ, một checkpoint được huấn luyện trước không cần thiết, nên chúng tôi trực tiếp huấn luyện với chính quy hóa từ đầu, sử dụng các giá trị siêu tham số được tóm tắt trong Bảng 1. Cuối cùng, chúng tôi tinh chỉnh không có chính quy hóa trong 5 epoch bổ sung.
Kết quả từ mô hình của chúng tôi và các đối thủ cạnh tranh được hiển thị trong Bảng 2, cùng với hiệu suất của mô hình cơ sở không được sparsified. Cột "Sparsity(%)" đề cập đến tỷ lệ phần trăm trọng số được tỉa của mỗi mô hình so với tổng số trọng số cơ sở. Cột "Tỷ lệ Nén" là tỷ lệ giữa tổng số trọng số trong mô hình cơ sở và số lượng trọng số còn lại sau khi tỉa.
Hiệu suất trên MNIST rất tương tự với nhau vì độ chính xác và sparsification trên nhiệm vụ đơn giản này đạt đến đỉnh có thể, tức là, rất gần với 100%. Với phương pháp của chúng tôi, chúng tôi có được ít hơn 1.5 k trọng số dư thừa không bằng không, một kết quả chủ yếu do sparsification tốt hơn, khi so sánh với các công trình khác, của các lớp kết nối đầy đủ, nơi phần lớn các trọng số ở đó. Chúng tôi có được độ chính xác gần như tốt nhất với ngoại lệ duy nhất của Sparse VD, mặc dù nó có tính thưa thớt cao hơn. Chúng tôi cũng lưu ý rằng chúng tôi có được kết quả tương tự như Han et al. nhưng với ít hơn 8% trọng số.
Bảng 6 cho thấy không gian đĩa được chiếm bởi các mô hình được tỉa và không được tỉa sau khi được nén bằng các thuật toán GZIP2 và BZIP23 với hai tỷ lệ nén khác nhau, được xác định bởi -1 và -9. Đặc biệt, chúng ta có thể thấy rằng khi sử dụng BZIP2, mô hình được tỉa nhỏ hơn hơn 20 lần so với mô hình không được tỉa. Thời gian suy luận CPU của mô hình được tỉa là 0:001s cho một batch.

5.3 LeNet-5 trên Fashion-MNIST
Chúng tôi có được checkpoint ban đầu sau khi huấn luyện trong 21 epoch. Sau đó chúng tôi sử dụng các siêu tham số được hiển thị trong Bảng 1, ngoại trừ lower-bound = 90.5 và epochs = 75, để tinh chỉnh với chính quy hóa; cuối cùng, chúng tôi tinh chỉnh không có chính quy hóa trong 50 epoch bổ sung. Bảng 3 so sánh hiệu suất trên tập dữ liệu này.
Như có thể thấy, phương pháp của chúng tôi đạt được hiệu suất tốt nhất cả về độ chính xác và nén. Phương pháp của chúng tôi tốt hơn khoảng 2 lần về tỷ lệ nén so với [16] với cải thiện độ chính xác 0.2%. Tương tự, kết quả của chúng tôi với LeNet-5 trên MNIST là do sparsification hiệu quả của các lớp kết nối đầy đủ. Không gian đĩa được chiếm bởi mô hình được tỉa và không được tỉa sau khi nén bằng GZIP và BZIP2 được báo cáo trong Bảng 6 và có thể so sánh với những gì được có được cho cùng kiến trúc trên MNIST. Thời gian suy luận CPU của mô hình được tỉa là 0:001s cho một batch.

5.4 ResNet32 trên CIFAR-10
Mô hình này [3] được tạo thành từ một lớp tích chập với 16 bộ lọc 3x3, một lớp chuẩn hóa batch, 3 lớp ResNet và một lớp dense cuối cùng, tổng cộng 464,154 tham số có thể huấn luyện. Tất cả các lớp ResNet được tạo thành từ 5 khối ResNet với các cấu hình khác nhau: chúng đều chia sẻ 2 lớp chuẩn hóa batch nhưng khác nhau về số lượng kernel được tạo bởi 2 lớp tích chập (16 bộ lọc 3x3 cho các khối của lớp ResNet đầu tiên, 32 3x3 cho lớp ResNet thứ hai và 64 3x3 cho lớp ResNet thứ ba).
Checkpoint ban đầu được có được với các siêu tham số được hiển thị trong Bảng 1, huấn luyện trong 200 epoch với = 0:1. Sau khi tinh chỉnh với chính quy hóa, chúng tôi tinh chỉnh không có nó trong 1 epoch cuối với batch size = 64 và = 6e-5.
Bảng 4 cho thấy kỹ thuật của chúng tôi cho nhiệm vụ thách thức hơn này, liên quan đến kiến trúc thần kinh sâu hơn và hình ảnh thực, vượt trội hơn tất cả các đối thủ cạnh tranh về tính thưa thớt. Về độ chính xác, cả phương pháp của chúng tôi và [16] đều đạt các giá trị cơ sở, nhưng phương pháp của chúng tôi cải thiện sparsification thêm 1:45% so với [16]. Ngoài ra, kết quả cho thấy rằng trong trường hợp của một mạng sâu phức tạp với các lớp dư thừa như ResNet-32, có thể tỉa một tỷ lệ lớn các trọng số mà không mất hiệu suất phân loại và không có bất kỳ thay đổi nào trong thuật toán tỉa. Không gian đĩa được chiếm bởi mô hình được tỉa và không được tỉa sau khi nén bằng GZIP và BZIP2 được báo cáo trong Bảng 6; đặc biệt chúng ta có thể thấy rằng sử dụng BZIP2 mô hình được tỉa nhỏ hơn 4 lần trên đĩa so với mô hình không được tỉa. Thời gian suy luận CPU của mô hình được tỉa là 0:011s cho một batch.

5.5 ResNet50 trên ImageNet
Mô hình bao gồm một lớp tích chập với chuẩn hóa batch và max pooling tiếp theo là 4 lớp ResNet và một lớp average pooling cuối cùng với một bộ phân loại kết nối đầy đủ ở trên. Tất cả các lớp ResNet được tạo thành từ một số khối ResNet khác nhau gồm 3 lớp tích chập xen kẽ với 3 lớp chuẩn hóa batch. Hầu hết các lớp tích chập chủ yếu có bộ lọc 3×3 và downsampling được thực hiện ở cuối mỗi khối ResNet đầu tiên dựa vào tích chập với stride = 2. Các mạng chứa 25,557,032 trọng số. Cho các thí nghiệm của chúng tôi, chúng tôi bắt đầu tinh chỉnh được chính quy hóa từ một checkpoint được huấn luyện trước4 sử dụng các siêu tham số được hiển thị trong Bảng 1. Chúng tôi thực hiện tinh chỉnh thêm trong 5 epoch để có được độ chính xác tốt nhất. Một so sánh giữa kết quả của mô hình chúng tôi và những mô hình của các công trình đối thủ cạnh tranh gần đây được hiển thị trong Bảng 5.
Kỹ thuật của chúng tôi được chỉ ra là rất hiệu quả trên ResNet50, đạt được lên đến 3.83x tính thưa thớt so với cơ sở và chỉ với mất mát nhẹ về độ chính xác (1:9%). Hơn nữa, giải pháp của chúng tôi hiệu quả trong cả chế độ nén thấp và cao (tức là, <2:5x và 2:5x), dẫn đến độ chính xác và mức độ thưa thớt tốt nhất khi so sánh với tất cả các kỹ thuật khác. Lượng không gian đĩa được chiếm bởi các mô hình được tỉa và không được tỉa sau khi nén bằng GZIP và BZIP2 được báo cáo trong Bảng 6. Đáng chú ý, khi sử dụng BZIP2, mô hình được tỉa yêu cầu ít hơn 3 lần không gian đĩa so với mô hình không được tỉa. Thời gian suy luận CPU của mô hình được tỉa là 2:09s cho một batch.

6 Tạo Ngôn ngữ
Các nghiên cứu đầu tiên về tính thưa thớt trong các kiến trúc ngôn ngữ xuất hiện gần đây [29, 33], theo sau sự thay thế tiến bộ của các kiến trúc tái phát bằng các mô hình dựa trên transformer, và chủ yếu tập trung vào tỉa đầu chú ý [45, 46]. Tuy nhiên, so với số lượng lớn các kỹ thuật thưa thớt có sẵn cho hình ảnh, hiếm khi tìm thấy nhiều kết quả như vậy trong bối cảnh tạo ngôn ngữ. Chúng tôi cố gắng lấp đầy khoảng trống này bằng cách làm thưa thớt transformer [9] trên hai nhiệm vụ ngôn ngữ khác nhau: học đối thoại và dịch máy.
Chúng tôi triển khai mô hình bằng thư viện HuggingFace5 cung cấp truy cập dễ dàng đến các tập dữ liệu, tokenizer và kỹ thuật tạo đầu ra khác nhau. Vì thuật toán sparsification của chúng tôi không cụ thể cho kiến trúc, không cần sửa đổi nào so với những gì được mô tả trong Sec. 4.
Ngoài ra, như đã đề cập trước đó trong cùng phần, chúng tôi sử dụng điểm BLEU (Bilingual Evaluation Understudy), so sánh một câu được tạo với một câu tham chiếu, như một chỉ số phù hợp để đánh giá trong bối cảnh tạo ngôn ngữ. Nó hoạt động bằng cách đếm các n-gram khớp trong bản dịch ứng cử viên với các n-gram trong văn bản tham chiếu. Một kết quả khớp hoàn hảo dẫn đến điểm 1.0, trong khi một kết quả không khớp hoàn hảo dẫn đến điểm 0.0. BLEU ban đầu được đề xuất bởi [36] để đánh giá các dự đoán được thực hiện bởi các hệ thống dịch máy tự động nhưng bây giờ thường được sử dụng cho nhiều nhiệm vụ tạo ngôn ngữ khác như tạo đối thoại, tạo chú thích hình ảnh, tóm tắt văn bản và nhận dạng giọng nói.

6.1 Mô tả Tập dữ liệu
WMT14
WMT14 [48] là một bộ sưu tập các tập dữ liệu được trình bày trong Workshop thứ Chín về Dịch Máy Thống kê. Nó đến như một corpus song song được tạo bởi các câu được dịch thành các ngôn ngữ khác nhau. Nó được bắt nguồn từ nhiều nguồn khác nhau, trong số đó có corpus Europarl [49] (được tạo từ Thủ tục Nghị viện Châu Âu trong các ngôn ngữ chính thức của EU), corpus News Commentary [50] và corpus Common Crawl [51] (được thu thập từ các nguồn web).
Cho các thí nghiệm của chúng tôi, chúng tôi sử dụng tập dữ liệu dịch Anh sang Đức En-De WMT14, được cung cấp bởi Nhóm Xử lý Ngôn ngữ Tự nhiên Stanford [52], lớn hơn hơn 300 lần so với Taskmaster-1; nó chứa 4,468,840 mẫu huấn luyện và 3000 mẫu kiểm tra. Một số ví dụ nguồn-dịch được hiển thị trong Bảng 7.

Taskmaster-1
Taskmaster-1 [47] là một tập dữ liệu crowdsourced công khai, được phát hành bởi Google vào năm 2019, nơi Amazon Turkers được yêu cầu viết các đối thoại dyadic (xem Bảng 8) theo một tập hợp hướng dẫn nhất định mô tả sáu nhiệm vụ: đặt pizza, tạo cuộc hẹn sửa chữa ô tô, thiết lập chuyến đi thuê, đặt vé phim, đặt đồ uống cà phê và đặt chỗ nhà hàng. Tập dữ liệu được tạo thành từ 13,215 đối thoại dựa trên nhiệm vụ (12,355 cho tập huấn luyện và 770 cho tập kiểm tra), bao gồm 5,507 đối thoại nói và 7,708 đối thoại viết.

6.2 Transformer trên WMT14
Chi tiết về kiến trúc transformer được đưa ra trong Bảng 9 và tuân theo các thiết lập từ [33].
Để có được checkpoint ban đầu, chúng tôi huấn luyện mô hình trong 10 epoch với batch size = 120 và = 5e05, sử dụng bộ tối ưu hóa Adam ( 1= 0:85;2= 0:997;eps = 1e8) và đạt được hiệu suất BLEU có thể so sánh với cơ sở được định nghĩa trong [33].
Bắt đầu từ checkpoint được mô tả ở trên, quá trình tinh chỉnh với chính quy hóa tiếp tục trong 16 epoch với batch size = 100, = 2:5e05 và= 2:22e07. Đánh giá trên tập validation được thực hiện mỗi 6000 bước (24 lần mỗi epoch) với BLEU lower-bound = 27.3 và 10% của các trọng số còn lại được tỉa khi cần thiết. Cuối cùng chúng tôi tinh chỉnh không có chính quy hóa trong 5 epoch nữa. So với [33], chúng tôi dừng tỉa khi hiệu suất validation đạt đến một plateau (hoặc đột ngột giảm) và không bao giờ vượt qua ngưỡng dưới do người dùng định nghĩa, như được mô tả trong Phần 4. Tiêu chí này khiến việc tỉa dừng lại ở 80 % tính thưa thớt.
Như được hiển thị trong Hình 5, kỹ thuật tỉa của chúng tôi hoạt động tốt hơn tất cả các phương pháp khác, bảo tồn các giá trị BLEU lên đến 75% tính thưa thớt trong khi giảm nhiều nhất 0.5 điểm so với cơ sở. Nó cũng có vẻ kiên cường hơn ở các mức nén cao hơn vì các điểm BLEU bắt đầu suy giảm rõ ràng chỉ sau khi đạt được 75 % tính thưa thớt, trong khi những điểm của năm phương pháp tỉa khác suy giảm sớm hơn. Đây là thí nghiệm đầu tiên nơi kỹ thuật của chúng tôi được sử dụng trong bối cảnh tạo ngôn ngữ tự nhiên, cho thấy rằng nó rất có thể tổng quát hóa và có thể được áp dụng hiệu quả cho các mô hình transformer dựa nhiều vào các lớp chú ý và có nhiều trọng số được chia sẻ, như trong lớp nhúng từ và trong chính lớp chú ý.
Bảng 10 cho thấy chi tiết các tỷ lệ phần trăm tỉa theo lớp và toàn cục ở mức nén cao hơn đạt được. Bảng 11 cho thấy lượng không gian đĩa được chiếm bởi các mô hình được tỉa và không được tỉa sau khi được nén bằng GZIP và BZIP2. Đáng chú ý, khi sử dụng BZIP2, mô hình được tỉa yêu cầu ít hơn 4 lần không gian đĩa so với mô hình không được tỉa. Thời gian suy luận CPU của mô hình được tỉa là4:05s cho một batch.

6.3 Transformer trên Taskmaster-1
Theo [47], chúng tôi sử dụng bối cảnh đối thoại cho đến lượt người dùng cuối cùng làm dữ liệu đầu vào, và làm mục tiêu cho phát ngôn trợ lý tiếp theo. Một ví dụ về định dạng này được hiển thị trong Bảng 8.
Chi tiết kiến trúc Transformer được trình bày trong [47], và được hiển thị trong Bảng 9. Chúng tôi huấn luyện nó trong 15 epoch sử dụng bộ tối ưu hóa Adam ( 1= 0:85;2= 0:997;eps = 1e8) với batch size = 150 và dropout = 0.2. Checkpoint cuối cùng chúng tôi có được cho thấy hiệu suất BLEU có thể so sánh với mô hình của tác giả.
Bắt đầu từ checkpoint này, chúng tôi tinh chỉnh với chính quy hóa trong 40 epoch với = 0:0005. Chúng tôi dựa vào một  nhỏ= 1e05 để tránh mất hiệu suất trong các giai đoạn đầu. Chúng tôi thấy rằng kiểm tra mỗi 300 bước huấn luyện (tức là, 4 lần mỗi epoch) là một sự thỏa hiệp tốt để có được các bước tỉa thường xuyên trong khi giữ lại khả năng tạo. BLEU lower-bound được đặt thành 6.03, rất gần với kết quả cơ sở của tác giả là 6.11, và pruning-percentage là 10%. Sau 30 epoch, thuật toán thực hiện việc tỉa cuối cùng, và 10 epoch cuối được sử dụng để khôi phục điểm BLEU.
Theo hiểu biết tốt nhất của chúng tôi, chưa có thành tựu nào trong tài liệu về tính thưa thớt trọng số trong các nhiệm vụ tạo đối thoại. Do đó, chúng tôi thiết lập kết quả đầu tiên trong bối cảnh này, được hiển thị trong Bảng 12, kiểm tra phương pháp của chúng tôi so với các cơ sở chính quy hóa L1 và L2.
Kỹ thuật sparsification của chúng tôi cho phép chúng tôi có được một mô hình được sparsified cao, với mức độ thưa thớt lớn hơn 90%. Hơn nữa, BLEU cuối cùng của chúng tôi thậm chí còn cao hơn kết quả ban đầu, gợi ý rằng trong một số trường hợp, một mô hình được sparsified có thể tổng quát hóa tốt hơn một mô hình không được sparsified. Trong Hình 6, chúng tôi cho thấy các điểm BLEU của kỹ thuật của chúng tôi và các cơ sở L1 và L2 ở các mức độ thưa thớt khác nhau. Trong trường hợp này, khoảng cách giữa phương pháp của chúng tôi và những phương pháp khác ít rõ ràng hơn do tính biến thiên cao của các điểm BLEU. Điều này có lẽ do thực tế là Taskmaster-1 chứa các câu khá ngắn khi so sánh với tập dữ liệu WMT-14, nên ngay cả những khác biệt đầu ra nhỏ với câu mục tiêu cũng có tác động cao đến điểm cuối cùng, dựa trên việc đếm n-gram. Bất kể, hệ thống của chúng tôi gần như luôn có thể hoạt động tốt hơn so với các chính quy hóa L1 và L2 với ngoại lệ của các mức độ thưa thớt giữa 0.8 và 0.9, nơi L1 được ưa thích.
Bảng 13 cho thấy không gian đĩa được chiếm bởi các mô hình được tỉa và không được tỉa sau khi được nén. Ngoài ra, trong trường hợp này, các nén rất tốt có thể được đạt được. Đặc biệt, sử dụng BZIP2, mô hình được tỉa nhỏ hơn khoảng 6,6 lần trên đĩa so với mô hình không được tỉa. Thời gian suy luận CPU của mô hình được tỉa là2:28s cho một batch.

7 Kết luận
Việc xác định các tham số mô hình không liên quan để tỉa là điểm trọng tâm của công việc này. Chúng tôi đề xuất một giải pháp là một cải tiến của suy giảm trọng số cổ điển và do đó phù hợp cho bất kỳ mất mát chức năng nào. Hơn nữa, nó đơn giản để triển khai và dẫn đến một khung có thể sử dụng rộng rãi và tổng quát chứng minh hiệu quả trong việc làm thưa thớt các kiến trúc sâu khác nhau.
Chúng tôi đạt được kết quả tiên tiến trong một trong bốn tập dữ liệu phân loại hình ảnh và cải thiện tiên tiến cho những tập dữ liệu khác về sự kết hợp của tính thưa thớt và độ chính xác, cũng có được một tiên tiến mới trong tập dữ liệu dịch máy WMT14. Vì có rất ít kết quả cho tính thưa thớt trong các nhiệm vụ tạo ngôn ngữ, một đóng góp khác của bài báo này là chúng tôi cung cấp một điểm dữ liệu mới trên Taskmaster-1.
Một đóng góp thú vị trong tương lai có thể là khám phá các ứng dụng của phương pháp chúng tôi cho các thiết bị tài nguyên thấp như điện thoại thông minh và hệ thống IoT.

Lời cảm ơn. Hoạt động đã được thực hiện một phần trong bối cảnh Chương trình Giáo sư Thỉnh giảng của Gruppo Nazionale per il Calcolo Scientifico (GNCS) của Istituto Nazionale di Alta Matematica (INdAM) của Ý.

Tài liệu tham khảo
[1] K. Zhang, L. V. Gool, R. Timofte, Deep unfolding network for image super-resolution, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.
[2] T. He, Z. Zhang, H. Zhang, Z. Zhang, J. Xie, M. Li, Bag of tricks for image classification with convolutional neural networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
[3] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770{778.
[4] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1{9.
[5] L. Guo, J. Liu, X. Zhu, P. Yao, S. Lu, H. Lu, Normalized and geometry-aware self-attention network for image captioning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.
[6] Y. Feng, L. Ma, W. Liu, J. Luo, Unsupervised image captioning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
[7] R. Puduppully, L. Dong, M. Lapata, Data-to-text generation with content selection and planning, in: Proceedings of the Thirty-Third Conference on Artificial Intelligence, AAAI, Honolulu, Hawaii, USA, 2019, pp. 6908{6915.
[8] O. Dusek, J. Novikova, V. Rieser, Evaluating the state-of-the-art of end-to-end natural language generation: The E2E NLG challenge, Comput. Speech Lang. 59, 2020, pp. 123{156.
[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, in: Advances in Neural Information Processing Systems 31, 2017, pp. 6000{6010.
[10] D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly learning to align and translate, in: 3rd International Conference on Learning Representations ICLR, San Diego, CA, USA, 2015.
[11] S. Han, J. Pool, J. Tran, W. J. Dally, Learning both weights and connections for efficient neural network, in: C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, R. Garnett (Eds.), Advances in Neural Information Processing Systems 28, 2015, pp. 1135{1143.
[12] K. Ullrich, E. Meeds, M. Welling, Soft weight-sharing for neural network compression, in: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France.
[13] V. Sanh, T. Wolf, A. M. Rush, Movement Pruning: Adaptive Sparsity by Fine-Tuning, Advances in Neural Information Processing Systems 34, 2020.
[14] Liu, J., Wang, Y., Qiao, Y., Sparse deep transfer learning for convolutional neural network, in: the Thirty-First AAAI Conference on Artificial Intelligence, AAAI 2017.
[15] E. Tartaglione, S. Lepsy, A. Fiandrotti, G. Francini, Learning sparse neural networks via sensitivity-driven regularization, in: S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, R. Garnett (Eds.), Advances in Neural Information Processing Systems 32, NeurIPS 2018.
[16] E. Tartaglione, A. Bragagnolo, A. Fiandrotti, M. Grangetto, LOss-Based SensiTivity rEgulaRization: Towards deep sparse neural networks, Neural Networks, Vol. 146, pp. 230-237, 2022.
[17] A. N. Gomez, I. Zhang, K. Swersky, Y. Gal, G. E. Hinton, Learning sparse networks using targeted dropout, arXiv preprint arXiv:1905.13678, 2019.
[18] S. Lin et al., Accelerating Convolutional Networks via Global & Dynamic Filter Pruning, Proceedings of the 27th International Joint Conference on Artificial Intelligence IJCAI, 2018.
[19] S. Lin et al., Toward Compact ConvNets via Structure-Sparsity Regularized Filter Pruning, IEEE Transactions on Neural Networks and Learning Systems, Vol. 31, N. 2, 2020, pp. 574-588.
[20] C. Lin et al., Synaptic Strength For Convolutional Neural Network, Advances in Neural Information Processing Systems 32, NeurIPS 2018.
[21] Z. Wang, S. Lin, J. Xie and Y. Lin, Pruning Blocks for CNN Compression and Acceleration via Online Ensemble Distillation, in IEEE Access, vol. 7, 2019, pp. 175703-175716.
[22] G. Ding, S. Zhang, Z. Jia, J. Zhong and J. Han, Where to Prune: Using LSTM to Guide Data-Dependent Soft Pruning, in IEEE Transactions on Image Processing, vol. 30, pp. 293-304, 2021.
[23] J. Zhu, J. Pei, Progressive kernel pruning with saliency mapping of input-output channels, Neurocomputing, Vol. 467, N. 7, 2022, pp. 360-378.
[24] J. Zhu, J. Pei, Progressive kernel pruning CNN compression method with an adjustable input channel, Applied Intelligence Vol. 52, N.3, 2022, pp. 1-22.
[25] Z. Huang, N. Wang, Data-Driven Sparse Structure Selection for Deep Neural, Proceedings of the 15th European Conference on Computer Vision ECCV, 2018.
[26] Y. He, X. Dong, G. Kang, Y. Fu, C. Yan and Y. Yang, Asymptotic Soft Filter Pruning for Deep Convolutional Neural Networks, in IEEE Transactions on Cybernetics, vol. 50, no. 8, pp. 3594-3604, Aug. 2020.
[27] M. Lin et al., HRank: Filter Pruning Using High-Rank Feature Map, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 1526-1535.
[28] Z. Zhuang, M. Tan, B. Zhuang, J. Liu, Y. Guo, Q. Wu, J. Huang and J. Zhu. Discrimination-aware channel pruning for deep neural networks. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS'18). Red Hook, NY, USA, 2018, pp. 883{894.
[29] D. Molchanov, A. Ashukha, D. P. Vetrov, Variational dropout sparsifies deep neural networks, in: D. Precup, Y. W. Teh (Eds.), Proceedings of the 34th International Conference on Machine Learning, ICML, 2017, pp. 2498{2507.
[30] H. Salehinejad, S. Valaee, EDropout: Energy-Based Dropout and Pruning of Deep Neural Networks, IEEE Transactions on Neural Networks and Learning, 2021, pp. 1-14.
[31] N. Lee, T. Ajanthan, P. H. S. Torr, Snip: single-shot network pruning based on connection sensitivity, in: Proceedings of the 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, 2019.
[32] Y. Guo, A. Yao, Y. Chen, Dynamic network surgery for efficient dnns, in: D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, R. Garnett (Eds.), Advances in Neural Information Processing Systems 29, Barcelona, Spain, 2016, pp. 1379{1387.
[33] T. Gale, E. Elsen, S. Hooker, The state of sparsity in deep neural networks, arXiv preprint arXiv:1902.09574, 2019.
[34] I. J. Goodfellow, Y. Bengio, A. C. Courville, Deep Learning, Adaptive computation and machine learning series, The MIT Press, Massachusetts Institute of Technology, Cambridge, Massachusetts, 2016.
[35] A. N. Tikhonov, Solution of incorrectly formulated problems and the regularization method, Soviet Math. Dokl. 4, 1963, pp. 1035{1038.
[36] K. Papineni, S. Roukos, T. Ward, W. J. Zhu, BLEU: a Method for Automatic Evaluation of Machine Translation, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 311-318.
[37] Y. LeCun, C. Cortes, MNIST handwritten digit database, 1990.
[38] H. Xiao, K. Rasul, R. Vollgraf, Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, arXiv preprint arXiv:1708.07747, 2017.
[39] V. N. Alex Krizhevsky, G. Hinton, CIFAR RGB image dataset, 2009.
[40] A. Torralba, R. Fergus, W. T. Freeman, 80 million tiny images: A large data set for nonparametric object and scene recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence 30 (11), 2008, pp. 1958{1970.
[41] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A large-scale hierarchical image database, in: Proceedings of the 2009 IEEE conference on computer vision and pattern recognition, 2009, pp. 248{255.
[42] G. A. Miller, Wordnet: A lexical database for english, Commun. ACM 38 (11), 1995, pp 39{41.
[43] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, L. D. Jackel, Backpropagation applied to handwritten zip code recognition, Neural Computation 1 (4), 1989, pp. 541{551.
[44] C. Louizos, M. Welling, D. P. Kingma, Learning sparse neural networks through L 0 regularization, in: 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, OpenReview.net, 2018.
[45] P. Michel, O. Levy, G. Neubig, Are sixteen heads really better than one?, in: H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch e-Buc, E. Fox, R. Garnett (Eds.), Advances in Neural Information Processing Systems, Vol. 33, 2019.
[46] E. Voita, D. Talbot, F. Moiseev, R. Sennrich, I. Titov, Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), Stroudsburg, PA, USA, 2019, pp. 5797{5808.
[47] B. Byrne, K. Krishnamoorthi, C. Sankar, A. Neelakantan, B. Goodrich, D. Duckworth, S. Yavuz, A. Dubey, K. Kim, A. Cedilnik, Taskmaster-1: Toward a realistic and diverse dialog dataset, in: K. Inui, J. Jiang, V. Ng, X. Wan (Eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing EMNLP-IJCNLP, Hong Kong, China, 2019, pp. 4515{4524.
[48] O. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post, H. Saint-Amand, R. Soricut, L. Specia, A. Tamchyna, in: Proceedings of the Ninth Workshop on Statistical Machine Translation, Association for Computational Linguistics, Baltimore, Maryland, USA, 2014, pp. 12{58.
[49] P. Koehn, Europarl: A Parallel Corpus for Statistical Machine Translation, in: Proceedings of the tenth Machine Translation Summit, AAMT, Phuket, Thailand, 2005, pp. 79{86.
[50] J. Tiedemann, Parallel data, tools and interfaces in opus, in: Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12), European Language Resources Association (ELRA), Istanbul, Turkey, 2012.
[51] CommonCrawl, CommonCrawl's dataset, 2012.
[52] T. S. N. L. P. Group, Neural Machine Translation, 2015.
