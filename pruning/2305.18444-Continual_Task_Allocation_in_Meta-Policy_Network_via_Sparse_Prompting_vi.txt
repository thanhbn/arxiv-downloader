# 2305.18444.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2305.18444.pdf
# Kích thước tệp: 4856226 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Phân bổ Nhiệm vụ Liên tục trong Mạng Meta-Policy thông qua Sparse Prompting
Yijun Yang1 2Tianyi Zhou3Jing Jiang2Guodong Long2Yuhui Shi1
Tóm tắt
Làm thế nào để huấn luyện một meta-policy tổng quát bằng cách liên tục học một chuỗi các nhiệm vụ? Đây là một kỹ năng tự nhiên của con người nhưng lại là thử thách đối với học tăng cường hiện tại: tác nhân được kỳ vọng nhanh chóng thích ứng với các nhiệm vụ mới (tính dẻo) đồng thời duy trì kiến thức chung từ các nhiệm vụ trước đó (tính ổn định). Chúng tôi giải quyết vấn đề này bằng "Phân bổ Nhiệm vụ Liên tục thông qua Sparse Prompting (CoTASP)", học các từ điển over-complete để tạo ra các mặt nạ thưa thớt như các prompt trích xuất một mạng con cho mỗi nhiệm vụ từ một mạng meta-policy. CoTASP huấn luyện một policy cho mỗi nhiệm vụ bằng cách tối ưu hóa các prompt và trọng số mạng con một cách xen kẽ. Từ điển sau đó được cập nhật để căn chỉnh các prompt đã tối ưu với embedding của nhiệm vụ, do đó nắm bắt được các mối tương quan ngữ nghĩa của nhiệm vụ. Do đó, các nhiệm vụ liên quan chia sẻ nhiều neuron hơn trong mạng meta-policy do các prompt tương tự trong khi can thiệp chéo giữa các nhiệm vụ gây ra quên lãng được kiềm chế hiệu quả. Với một meta-policy và từ điển được huấn luyện trên các nhiệm vụ trước đó, thích ứng nhiệm vụ mới được rút gọn thành sparse prompting và điều chỉnh mạng con có hiệu quả cao. Trong các thí nghiệm, CoTASP đạt được sự cân bằng tính dẻo-ổn định đầy hứa hẹn mà không cần lưu trữ hoặc phát lại bất kỳ kinh nghiệm nào của các nhiệm vụ trong quá khứ. Nó vượt trội hơn các phương pháp RL liên tục và đa nhiệm vụ hiện có trên tất cả các nhiệm vụ đã thấy, giảm quên lãng và tổng quát hóa cho các nhiệm vụ chưa thấy. Mã của chúng tôi có sẵn tại https://github.com/stevenyangyj/CoTASP

1. Giới thiệu
Mặc dù học tăng cường (RL) đã chứng minh hiệu suất xuất sắc trong việc học một nhiệm vụ đơn lẻ, ví dụ như chơi cờ vây (Silver et al., 2016), điều khiển robot (Schulman et al., 2017; Degrave et al., 2022), và tối ưu hóa policy offline (Yu et al., 2020; Yang et al., 2022), nó vẫn gặp khó khăn với quên lãng thảm khốc và can thiệp chéo giữa các nhiệm vụ khi học một dòng nhiệm vụ trong thời gian thực (McCloskey & Cohen, 1989; Bengio et al., 2020) hoặc một chương trình giảng dạy được sắp xếp (Fang et al., 2019; Ao et al., 2021; 2022). Vì vậy, việc huấn luyện một meta-policy có thể tổng quát hóa cho tất cả các nhiệm vụ đã học hoặc thậm chí là các nhiệm vụ chưa thấy với khả năng thích ứng nhanh là một thử thách, tuy nhiên đây lại là một kỹ năng vốn có của việc học của con người.

Vấn đề này đã được công nhận là RL liên tục hoặc suốt đời (Mendez & Eaton, 2022) và thu hút sự quan tâm ngày càng tăng trong nghiên cứu RL gần đây.

Một thử thách chính và lâu dài trong RL liên tục là sự cân bằng tính dẻo-ổn định (Khetarpal et al., 2022): policy RL một mặt cần duy trì và tái sử dụng kiến thức được chia sẻ giữa các nhiệm vụ khác nhau trong lịch sử (tính ổn định) trong khi mặt khác có thể nhanh chóng thích ứng với các nhiệm vụ mới mà không bị can thiệp từ các nhiệm vụ trước đó (tính dẻo). Giải quyết thử thách này là rất quan trọng để cải thiện hiệu quả của RL liên tục và khả năng tổng quát hóa của policy đã học. Một meta-policy với tính ổn định tốt hơn có thể giảm sự cần thiết của phát lại kinh nghiệm và chi phí bộ nhớ/tính toán của nó. Hơn nữa, kích thước mạng yêu cầu có thể được giảm hiệu quả nếu meta-policy có thể quản lý việc chia sẻ kiến thức giữa các nhiệm vụ theo cách nhỏ gọn và hiệu quả hơn. Do đó, tính ổn định có thể cải thiện đáng kể hiệu quả của RL liên tục khi số lượng nhiệm vụ tăng (Shin et al., 2017; Li & Hoiem, 2018). Hơn nữa, tính dẻo tốt hơn cho thấy thích ứng nhanh hơn và tổng quát hóa cho các nhiệm vụ mới.

Để trực tiếp giải quyết sự cân bằng tính dẻo-ổn định và khắc phục những hạn chế của công trình trước đây, chúng tôi nghiên cứu cách huấn luyện một mạng meta-policy trong RL liên tục. Sau đó, chỉ với một mô tả bằng văn bản của một nhiệm vụ đã học trước đó hoặc chưa thấy, một policy cụ thể có thể được trích xuất tự động và hiệu quả từ meta-policy. Điều này có cùng tinh thần với prompting trong các mô hình ngôn ngữ lớn gần đây (Li & Liang, 2021; Liu et al., 2021) nhưng khác với các phương pháp hiện có lựa chọn ngẫu nhiên các policy nhiệm vụ (Rajasegaran et al., 2019; Mirzadeh et al., 2020) hoặc tối ưu hóa độc lập một policy cho mỗi nhiệm vụ từ đầu (Serrà et al., 2018; Kang et al., 2022). Để đạt được điều này, chúng tôi đề xuất học các từ điển theo lớp cùng với mạng meta-policy để tạo ra các prompt thưa thớt (tức là, mặt nạ nhị phân) cho mỗi nhiệm vụ, trích xuất một mạng con từ meta-policy làm policy cụ thể cho nhiệm vụ. Chúng tôi gọi cách tiếp cận này là "Phân bổ Nhiệm vụ Liên tục thông qua Sparse Prompting (CoTASP)".

Như được minh họa trong Hình 1, với mỗi nhiệm vụ t, prompt αt được tạo bởi sparse coding của embedding nhiệm vụ et dưới từ điển Dt và được sử dụng để phân bổ một mạng con policy θt từ meta-policy. Sau đó αt và θt được tối ưu hóa thông qua RL. Ở cuối mỗi nhiệm vụ, từ điển Dt được tối ưu hóa (Mairal et al., 2009; Arora et al., 2015) cho tất cả các nhiệm vụ đã học để cung cấp một ánh xạ từ embedding nhiệm vụ của chúng đến các prompt/mạng con đã tối ưu, điều này khai thác các mối tương quan nhiệm vụ trong cả không gian embedding và prompt. Điều này dẫn đến việc sử dụng hiệu quả dung lượng của mạng meta-policy và tối ưu hóa tự động sự cân bằng tính dẻo-ổn định, tức là các nhiệm vụ liên quan tái sử dụng kỹ năng bằng cách chia sẻ nhiều neuron hơn (tính dẻo và thích ứng nhanh) trong khi can thiệp có hại giữa các nhiệm vụ không liên quan được tránh bằng cách chia sẻ ít hoặc không có neuron (tính ổn định và ít quên lãng hơn). Hơn nữa, do từ điển, CoTASP không cần lưu trữ hoặc phát lại bất kỳ kinh nghiệm nào của các nhiệm vụ trước đó và do đó tốn ít tính toán và bộ nhớ hơn nhiều so với các phương pháp dựa trên rehearsal (Rolnick et al., 2019; Wolczyk et al., 2022). Hơn nữa, sparse prompting trong CoTASP, như một phương pháp thích ứng nhiệm vụ hiệu quả, có thể trích xuất các mạng con policy cho các nhiệm vụ chưa thấy và do đó dẫn đến một meta-policy có khả năng tổng quát hóa hơn.

Trên các benchmark Continual World (Wolczyk et al., 2021), CoTASP vượt trội hơn hầu hết các baseline trên tất cả các nhiệm vụ đã học, giảm quên lãng và tổng quát hóa cho các nhiệm vụ chưa thấy (Bảng 1). Một nghiên cứu ablation kỹ lưỡng (Bảng 2) chứng minh tầm quan trọng của học từ điển và tối ưu hóa prompt thưa thớt. Hơn nữa, phân tích thực nghiệm của chúng tôi cho thấy từ điển hội tụ nhanh (Hình 5(b)) và có thể được tổng quát hóa cho các nhiệm vụ tương lai, giảm đáng kể chi phí thích ứng của chúng (Hình 5(a)), trong khi các prompt thưa thớt đã học nắm bắt các mối tương quan ngữ nghĩa giữa các nhiệm vụ (Hình 7). So với các phương pháp tiên tiến, CoTASP đạt được sự cân bằng tính dẻo-ổn định tốt nhất (Hình 2) và sử dụng dung lượng mô hình rất hiệu quả (Hình 4).

2. Kiến thức cơ bản và Công trình liên quan
Chúng tôi tuân theo thiết lập task-incremental được áp dụng bởi công trình trước đây (Khetarpal et al., 2022; Wolczyk et al., 2022; 2021; Rolnick et al., 2019; Mendez et al., 2020; Schwarz et al., 2018; Rusu et al., 2016), xem xét một chuỗi các nhiệm vụ, mỗi nhiệm vụ định nghĩa một Quá trình Quyết định Markov (MDP) Mt=⟨St,At, pt, rt, γ⟩ sao cho S là không gian trạng thái, A là không gian hành động, p:S × A → ∆(S) là xác suất chuyển đổi trong đó ∆(S) là simplex xác suất trên S, r:S ×A → R là hàm phần thưởng nên rt(st,h, at,h) là phần thưởng tức thì trong nhiệm vụ t khi thực hiện hành động at,h tại trạng thái st,h, h lập chỉ mục bước môi trường, và γ∈[0,1) là hệ số chiết khấu. RL liên tục nhằm đạt được một policy πθ tại nhiệm vụ T có hiệu suất tốt (với lợi nhuận kỳ vọng cao) trên tất cả các nhiệm vụ đã thấy t≤ T, chỉ với một buffer hạn chế (hoặc không có) các kinh nghiệm của các nhiệm vụ trước đó:

θ∗= arg max θ ∑T t=1 Eπθ [∑∞ h=0 γh rt(st,h, at,h)] (1)

Học liên tục là một kỹ năng tự nhiên của con người có thể tích lũy kiến thức có thể tổng quát hóa cho các nhiệm vụ mới mà không quên những nhiệm vụ đã học. Tuy nhiên, các tác nhân RL thường gặp khó khăn trong việc đạt được mục tiêu trong Phương trình 1 do sự cân bằng tính dẻo-ổn định: policy được kỳ vọng nhanh chóng thích ứng với các nhiệm vụ mới t≥ T (tính dẻo) nhưng đồng thời duy trì hiệu suất của nó trên các nhiệm vụ trước đó t <T (tính ổn định).

Các chiến lược hiện có cho RL liên tục chủ yếu tập trung vào cải thiện tính ổn định và giảm quên lãng thảm khốc. Các phương pháp dựa trên rehearsal như CLEAR (Rolnick et al., 2019) và P&C (Schwarz et al., 2018) liên tục phát lại các kinh nghiệm được buffer từ các nhiệm vụ trước đó nhưng chi phí bộ nhớ buffer và tính toán của chúng tăng tuyến tính với số lượng nhiệm vụ (Kumari et al., 2022). Các phương pháp dựa trên regularization như EWC (Kirkpatrick et al., 2017) và PC (Kaplanis et al., 2019) giảm thiểu quên lãng mà không cần buffer phát lại bằng cách thêm các regularizer bổ sung khi học các nhiệm vụ mới, điều này có thể làm thiên lệch tối ưu hóa policy và dẫn đến các giải pháp dưới tối ưu (Zhao et al., 2023). Cuối cùng, các phương pháp dựa trên structure sử dụng các module khác nhau, tức là các mạng con trong một mạng policy dung lượng cố định, cho mỗi nhiệm vụ (Mendez & Eaton, 2022). Chúng tôi tóm tắt hai danh mục chính của các phương pháp dựa trên structure như sau. Chúng tôi cũng cung cấp một thảo luận chi tiết hơn về công trình liên quan trong Phụ lục B và C.

Các phương pháp connection-level. Danh mục này bao gồm các phương pháp như PackNet (Mallya & Lazebnik, 2018), SupSup (Wortsman et al., 2020), và WSN (Kang et al., 2022). Đối với nhiệm vụ t, hành động at được rút ra từ at∼π(st;θ⊗ϕt) trong đó st là trạng thái và ϕt là một mặt nạ nhị phân được áp dụng cho trọng số mô hình θ theo cách element-wise (tức là, ⊗). PackNet tạo ϕt bằng cách lặp lại pruning θ sau khi học mỗi nhiệm vụ, do đó bảo tồn các trọng số quan trọng cho nhiệm vụ trong khi để lại những trọng số khác cho các nhiệm vụ tương lai. SupSup cố định một mạng được khởi tạo ngẫu nhiên và tìm ϕt tối ưu cho mỗi nhiệm vụ t. WSN học đồng thời θ và ϕt và sử dụng mã hóa Huffman (Huffman, 1952) để nén ϕt cho kích thước tăng dưới tuyến tính của {ϕt}T t=1 với các nhiệm vụ tăng. Tuy nhiên, các phương pháp này thường cần lưu trữ các mặt nạ cụ thể cho nhiệm vụ cho mỗi nhiệm vụ trong lịch sử, dẫn đến chi phí bộ nhớ bổ sung (Lange et al., 2022). Hơn nữa, các mặt nạ của chúng hiếm khi được tối ưu hóa cho việc chia sẻ kiến thức giữa các nhiệm vụ, cản trở policy đã học khỏi việc được tổng quát hóa cho các nhiệm vụ chưa thấy.

Các phương pháp neuron-level. Thay vì trích xuất các mạng con cụ thể cho nhiệm vụ bằng cách áp dụng mặt nạ cho trọng số mô hình, danh mục phương pháp khác (Fernando et al., 2017; Serrà et al., 2018; Ke et al., 2021; Sokar et al., 2021) tạo ra các mạng con bằng cách áp dụng mặt nạ cho neurons/outputs của mỗi lớp của mạng policy. So với các phương pháp connection-level, chúng sử dụng masking theo lớp để đạt được biểu diễn linh hoạt và nhỏ gọn hơn của các mạng con. Nhưng việc tạo mặt nạ phụ thuộc vào các quy tắc heuristic hoặc các phương pháp policy gradient không hiệu quả về mặt tính toán (Gurbuz & Dovrolis, 2022; Serrà et al., 2018). Ngược lại, CoTASP tạo ra mặt nạ bằng sparse coding có hiệu quả cao (giải quyết một bài toán lasso tương đối nhỏ).

3. Phương pháp
Trong phần này, chúng tôi giới thiệu các bước chính và thành phần của CoTASP (xem Hình 1). Cụ thể, Phần 3.1 giới thiệu mạng meta-policy trong thiết lập RL liên tục. Phần 3.2 mô tả sparse prompting được đề xuất của chúng tôi để trích xuất policy nhiệm vụ. Cuối cùng, Phần 3.3 cung cấp thủ tục tối ưu hóa chi tiết cho mỗi thành phần trong CoTASP, bao gồm prompt, policy nhiệm vụ và từ điển.

3.1. RL Liên tục với Mạng Meta-Policy
Như đã thảo luận trong Phần 2, việc fine-tuning tất cả trọng số trong θ thông qua tối ưu hóa trong Phương trình 1 mà không truy cập vào các nhiệm vụ trong quá khứ dẫn đến sự dịch chuyển có hại trên một số trọng số quan trọng cho các nhiệm vụ trước đó và quên lãng thảm khốc của chúng. Các phương pháp dựa trên structure giải quyết vấn đề này bằng cách phân bổ một mạng con cho mỗi nhiệm vụ và đóng băng trọng số của nó một khi hoàn thành việc học nhiệm vụ để chúng miễn nhiễm với quên lãng thảm khốc.

Theo công trình trước đây (Srivastava et al., 2014; Fernando et al., 2017; Serrà et al., 2018; Ke et al., 2021; Sokar et al., 2021), chúng tôi biểu diễn một mạng con như vậy bằng cách áp dụng một mặt nạ nhị phân cho output của mỗi neuron. Cụ thể, với một mạng meta-policy có L lớp, cho l∈ {1, . . . , L −1} lập chỉ mục các lớp ẩn của mạng như một superscript, ví dụ, y(l) là vector output của lớp-l và θ(l) biểu thị trọng số của lớp-l. Output của mạng con trên lớp (l+ 1)-th của nó là

y(l+1)=f(ϕ(l) t⊗y(l);θ(l+1)), (2)

trong đó ϕ(l) t là một mặt nạ nhị phân được tạo cho nhiệm vụ t và được áp dụng cho lớp-l, và f là một stand-in cho phép toán neural, ví dụ, một lớp fully-connected hoặc convolutional. Toán tử tích element-wise ⊗ kích hoạt một phần neurons trong lớp-l của mạng meta-policy theo ϕ(l) t. Những neurons được kích hoạt này trên tất cả các lớp trích xuất một mạng con như một policy cụ thể cho nhiệm vụ, sau đó tương tác với môi trường để thu thập kinh nghiệm huấn luyện. Huấn luyện mạng con tránh can thiệp có hại với các nhiệm vụ trước đó trên các neurons khác và đồng thời khuyến khích việc chia sẻ kiến thức của chúng trên các neurons được chia sẻ. Tuy nhiên, việc phân bổ policies trong mạng meta-policy cho một chuỗi các nhiệm vụ đa dạng đặt ra một số thử thách. Để sử dụng hiệu quả dung lượng mạng, mỗi policy nhiệm vụ nên là một mạng con thưa thớt với chỉ một vài neurons được kích hoạt. Hơn nữa, policy nên tái sử dụng có chọn lọc các neurons từ các policies đã học trước đó, điều này có thể tạo điều kiện thuận lợi cho việc chia sẻ kiến thức giữa các nhiệm vụ liên quan và giảm can thiệp từ các nhiệm vụ không liên quan. Lấy cảm hứng từ prompting và huấn luyện in-context cho NLP (Rebuffi et al., 2017; Houlsby et al., 2019; Li & Liang, 2021; Liu et al., 2021), chúng tôi đề xuất "Sparse Prompting" để giải quyết thử thách nói trên của phân bổ nhiệm vụ liên tục, có thể tự động và hiệu quả trích xuất các policies cụ thể cho nhiệm vụ từ mạng meta-policy.

3.2. Trích xuất Policy Nhiệm vụ thông qua Sparse Prompting
Trong phân bổ nhiệm vụ liên tục, mạng con được trích xuất cho một nhiệm vụ mới được kỳ vọng tái sử dụng kiến thức đã học từ các nhiệm vụ liên quan trong quá khứ và đồng thời tránh can thiệp có hại từ các nhiệm vụ không liên quan. Hơn nữa, mạng con nên càng thưa thớt càng tốt để sử dụng hiệu quả dung lượng mạng. Nhiều phương pháp RL liên tục khác nhau (Mallya & Lazebnik, 2018; Serrà et al., 2018; Sokar et al., 2021; Kessler et al., 2022; Kang et al., 2022; Wolczyk et al., 2022) sử dụng một embedding one-hot để trích xuất mạng con cho mỗi nhiệm vụ đã học. Chúng bỏ qua các mối tương quan ngữ nghĩa giữa các nhiệm vụ và cần các cơ chế tinh tế để giữ cho mạng con thưa thớt (Lange et al., 2022).

Trong CoTASP, thay vào đó chúng tôi trích xuất các mạng con thưa thớt từ một embedding nhỏ gọn của mô tả bằng văn bản của nhiệm vụ được tạo bởi Sentence-BERT (S-BERT) (Reimers & Gurevych, 2019) thông qua sparse coding (Mairal et al., 2009; Arora et al., 2015). Cụ thể, chúng tôi học một từ điển over-complete D(l)∈ Rm×k (m≪k) cho mỗi lớp-l của mạng meta-policy, trong đó mỗi cột là một atom đại diện cho một neuron trong lớp. Với một embedding nhiệm vụ et∈Rm, sparse prompting có thể tạo ra một prompt thưa thớt α(l) t cho mỗi lớp-l để tái tạo et như một tổ hợp tuyến tính của một vài biểu diễn neurons, tức là các atom từ từ điển. Nó bằng với việc giải quyết bài toán lasso sau đây.

α(l) t= arg min α∈Rk 1/2∥et−D(l) t−1α∥2 2+λ∥α∥1, cho lớp l= 1, . . . , L −1 (3)

trong đó λ là một tham số regularization kiểm soát độ thưa thớt của α. Bài toán lasso này có thể được giải quyết bởi nhiều cách tiếp cận hiệu quả có thể chứng minh, ví dụ, coordinate descent (Friedman et al., 2007), fast iterative shrinkage thresholding algorithm (Beck & Teboulle, 2009), và thuật toán LARS (Efron et al., 2004). Trong bài báo này, chúng tôi áp dụng một implementation dựa trên Cholesky của thuật toán LARS (Mairal et al., 2009) vì tính hiệu quả và ổn định của nó. Để biến đổi α∈Rk thành một mặt nạ nhị phân, chúng tôi áp dụng một hàm step σ(·) trên α, trong đó σ(α) = 1 nếu α >0 và 0 nếu ngược lại. Sau đó chúng tôi có thể trích xuất một mạng con policy cụ thể cho nhiệm vụ bằng cách áp dụng mặt nạ cho mạng meta-policy như trong Phương trình 2.

3.3. Học Meta-Policy và Từ điển trong CoTASP
Tối ưu hóa Xen kẽ Policy Nhiệm vụ và Prompt Bằng cách xen kẽ tối ưu hóa policy nhiệm vụ hiện tại và prompts α(l) t cho l= 1, . . . , L −1 sử dụng bất kỳ thuật toán RL off-the-shelf nào, CoTASP cập nhật trọng số mạng con liên quan đến policy nhiệm vụ trong mạng meta-policy và các mặt nạ nhị phân tương ứng. Tuy nhiên, có hai mối quan tâm thực tế: (1) cập nhật các trọng số trong θt đã được chọn bởi các nhiệm vụ trước đó có thể làm giảm hiệu suất của các nhiệm vụ cũ mà không phát lại kinh nghiệm; và (2) hàm step σ(·) có gradient bằng không nên việc tối ưu hóa αt sử dụng gradient như vậy là không khả thi.

Để giải quyết mối quan tâm đầu tiên, chúng tôi cập nhật các trọng số một cách có chọn lọc bằng cách chỉ cho phép cập nhật các trọng số chưa bao giờ được phân bổ cho bất kỳ nhiệm vụ trước đó nào. Với mục đích này, chúng tôi tích lũy các mặt nạ nhị phân cho tất cả các nhiệm vụ đã học bằng ˆϕ(l) t−1=∨t−1 i=1ϕ(l) t−1 và cập nhật θ khi học nhiệm vụ t bằng

θ←θ−ηˆgt

ˆg(l) t= {
1−ˆϕ(l) t−1 ⊙ g(l) t, l = 1
1−ˆϕ(l−1) t−1 ⊙ g(l) t, l =L
[1−min(ˆϕ(l−1) t−1,ˆϕ(l) t−1)] ⊙ g(l) t, l > 1
(4)

trong đó η là learning rate và g(l) t biểu thị gradient âm của expected return w.r.t. θ cho lớp-l trên nhiệm vụ t. Trong Phương trình 4, chúng tôi sửa đổi gradient của mỗi trọng số theo mặt nạ tích lũy liên quan đến lớp input và output của nó. Điều này hiệu quả tránh ghi đè các trọng số được chọn bởi policies của các nhiệm vụ trước đó và do đó giảm thiểu quên lãng.

Để giải quyết mối quan tâm thứ hai, chúng tôi sử dụng straight-through estimator (STE) (Bengio et al., 2013), tức là clip(α,0,1), trong backward pass để α có thể được tối ưu hóa trực tiếp sử dụng cùng thuật toán gradient descent được áp dụng cho trọng số meta-policy.

Học Từ điển Với các prompts đã tối ưu α∗ i của các nhiệm vụ trước đó và embedding của chúng, chúng tôi tiếp tục cập nhật từ điển mỗi lớp để sparse prompting sẽ được cải thiện để tạo ra các prompts đã tối ưu cho mỗi nhiệm vụ trước đó, tức là,

D(l) t= arg min D∈Rm×k 1/2 ∑t i=1∥ei−Dα∗(l) i∥2 2,s.t.,∥D[j]∥2≤c ∀j= 1, . . . , k, cho lớp l= 1, . . . , L −1 (5)

trong đó chúng tôi ràng buộc norm ℓ2 của mỗi atom D[j] để ngăn scale của D tăng lên tùy ý lớn, điều này sẽ dẫn đến các entry tùy ý nhỏ trong α.

Để giải quyết tối ưu hóa với các ràng buộc bất đẳng thức trong Phương trình 5, chúng tôi sử dụng block-coordinate descent với D(l) t−1 như warm restart, như được mô tả trong Thuật toán 1. Cụ thể, chúng tôi cập nhật tuần tự mỗi atom của D(l) t−1 dưới ràng buộc ∥D[j]∥2≤c trong khi cố định các atom còn lại. Vì tối ưu hóa này thừa nhận các ràng buộc có thể tách rời cho các atom được cập nhật, sự hội tụ đến một tối ưu toàn cục được đảm bảo (Bertsekas, 1997; Lee et al., 2006; Mairal et al., 2009). Hơn nữa, D(l) t−1 như một warm start cho D(l) t giảm đáng kể các bước tối ưu hóa yêu cầu: chúng tôi thấy thực nghiệm rằng một bước là đủ. Thủ tục huấn luyện hoàn chỉnh của CoTASP được chi tiết trong Thuật toán 2.

4. Thí nghiệm
4.1. Thiết lập Thí nghiệm
Benchmarks. Để đánh giá CoTASP, chúng tôi tuân theo cùng thiết lập như công trình trước đây (Wolczyk et al., 2022) và thực hiện các thí nghiệm kỹ lưỡng. Cụ thể, chúng tôi chủ yếu sử dụng CW10, một benchmark trong Continual World (CW) (Wolczyk et al., 2021), bao gồm 10 nhiệm vụ thao tác đại diện từ MetaWorld (Yu et al., 2019). Để làm cho benchmark thử thách hơn, chúng tôi xếp hạng những nhiệm vụ này theo một ma trận chuyển giao được tính trước để có sự biến đổi cao của forward transfer cả trong toàn bộ chuỗi và cục bộ. Chúng tôi cũng sử dụng CW20, lặp lại CW10 hai lần, để đo khả năng chuyển giao của policy đã học khi gặp cùng một nhiệm vụ. Để so sánh công bằng giữa các nhiệm vụ khác nhau, số bước tương tác môi trường được giới hạn ở 1M mỗi nhiệm vụ.

Metrics đánh giá. Theo một giao thức đánh giá được sử dụng rộng rãi trong literature học liên tục (Lopez-Paz & Ranzato, 2017; Rolnick et al., 2019; Chaudhry et al., 2019; Wolczyk et al., 2021; 2022), chúng tôi áp dụng ba metrics. (1) Hiệu suất Trung bình (cao hơn là tốt hơn): hiệu suất trung bình tại thời điểm t được định nghĩa là P(t) =1/T ∑T i=1 pi(t) trong đó pi(t)∈ [0,1] biểu thị tỷ lệ thành công của nhiệm vụ i tại thời điểm t. Đây là một metric chính được sử dụng trong cộng đồng học liên tục. (2) Quên lãng (thấp hơn là tốt hơn): nó đo sự suy giảm trung bình trên tất cả các nhiệm vụ ở cuối việc học, được biểu thị bởi F=1/T ∑T i=1 pi(i·δ)−pi(T ·δ), trong đó δ là các bước môi trường được phép cho mỗi nhiệm vụ. (3) Tổng quát hóa (thấp hơn là tốt hơn): nó bằng số bước trung bình cần thiết để đạt ngưỡng thành công trên tất cả các nhiệm vụ. Lưu ý rằng chúng tôi dừng huấn luyện khi tỷ lệ thành công trong hai đánh giá liên tiếp đạt ngưỡng (đặt là 0.9). Hơn nữa, metric được chia cho δ để chuẩn hóa scale của nó về [0,1].

Các phương pháp so sánh. Chúng tôi so sánh CoTASP với một số baseline và các phương pháp RL liên tục tiên tiến (SoTA). Theo (Lange et al., 2022), những phương pháp này có thể được chia thành ba danh mục: các phương pháp dựa trên regularization, structure và rehearsal. Cụ thể, các phương pháp dựa trên regularization bao gồm L2, Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), Memory-Aware Synapses (MAS) (Aljundi et al., 2018), và Variational Continual Learning (VCL) (Nguyen et al., 2018). Các phương pháp dựa trên structure bao gồm PackNet (Mallya & Lazebnik, 2018), Hard Attention to Tasks (HAT) (Serrà et al., 2018), và TaDeLL (Rostami et al., 2020). Các phương pháp dựa trên rehearsal bao gồm Reservoir, Average Gradient Episodic Memory (A-GEM) (Chaudhry et al., 2019), và ClonEx-SAC (Wolczyk et al., 2022). Để đầy đủ, chúng tôi cũng bao gồm một phương pháp huấn luyện tuần tự đơn giản (tức là, Finetuning) và các baseline RL đa nhiệm vụ đại diện (MTL (Yu et al., 2019) và MTL+PopArt (Hessel et al., 2019)), thường được coi là giới hạn trên mềm mà một phương pháp RL liên tục có thể đạt được. Để so sánh công bằng, chúng tôi tham khảo repository Continual World¹ để implementation và lựa chọn siêu tham số. Chúng tôi chạy lại những phương pháp này để đảm bảo hiệu suất tốt nhất có thể. Ngoài ra, chúng tôi áp dụng kết quả do tác giả báo cáo cho ClonEx-SAC do thiếu implementation mã nguồn mở. Một mô tả mở rộng và thảo luận về những phương pháp này được cung cấp trong Phụ lục C.

Chi tiết huấn luyện. Để đảm bảo độ tin cậy và khả năng so sánh của các thí nghiệm, chúng tôi tuân theo các chi tiết huấn luyện được mô tả trong (Wolczyk et al., 2021; 2022) và implement tất cả các phương pháp baseline dựa trên Soft Actor-Critic (SAC) (Haarnoja et al., 2018), một thuật toán actor-critic off-policy SoTA. Actor và critic được implement như hai mạng multi-layer perceptron (MLP) riêng biệt, mỗi mạng có 4 lớp ẩn với 256 neurons. Đối với các phương pháp dựa trên structure (PackNet, HAT) và CoTASP được đề xuất của chúng tôi, một mạng MLP rộng hơn với 1024 neurons mỗi lớp được sử dụng làm actor. Chúng tôi gọi những lớp ẩn này là backbone và lớp output cuối cùng là head. Không giống như các phương pháp RL liên tục khác (Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2019; Serrà et al., 2018; Kessler et al., 2022) dựa vào việc sử dụng một head riêng biệt cho mỗi nhiệm vụ mới, CoTASP sử dụng thiết lập single-head trong đó chỉ một head được sử dụng cho tất cả các nhiệm vụ. Trong trường hợp này, CoTASP không yêu cầu lựa chọn head phù hợp cho mỗi nhiệm vụ và cho phép tái sử dụng tham số giữa các nhiệm vụ tương tự. Theo (Wolczyk et al., 2021), việc regularize critic thường dẫn đến sự suy giảm hiệu suất. Do đó, chúng tôi hoàn toàn bỏ qua vấn đề quên lãng trong mạng critic và huấn luyện lại nó cho mỗi nhiệm vụ mới. Thêm chi tiết về các siêu tham số được sử dụng trong huấn luyện có thể được tìm thấy trong Phụ lục D.

4.2. Kết quả Chính
Phần này trình bày so sánh giữa CoTASP và mười phương pháp RL liên tục đại diện trên các benchmark CW. Chúng tôi tập trung vào tính ổn định (duy trì hiệu suất trên các nhiệm vụ đã thấy) và tính dẻo (nhanh chóng thích ứng với các nhiệm vụ chưa thấy) và giữ các ràng buộc về tính toán, bộ nhớ, số lượng mẫu và kiến trúc mạng neural không đổi.

Bảng 1 tóm tắt kết quả chính của chúng tôi trên chuỗi CW10 và CW20. CoTASP luôn vượt trội hơn tất cả các phương pháp so sánh trên các độ dài khác nhau của chuỗi nhiệm vụ, về cả hiệu suất trung bình (đo tính ổn định) và tổng quát hóa (đo tính dẻo). Chúng tôi quan sát thấy khi kích thước lớp ẩn giống như các phương pháp dựa trên structure khác (PackNet và HAT), CoTASP vượt trội hơn chúng với một khoảng cách lớn, đặc biệt trong metric tổng quát hóa, cho thấy lợi thế của CoTASP trong việc cải thiện khả năng thích ứng với các nhiệm vụ mới. Một phân tích chi tiết hơn về lý do cho tính hiệu quả của CoTASP được trình bày trong Phần 4.3 và 4.4. Hơn nữa, chúng tôi thấy rằng hầu hết các phương pháp RL liên tục không đạt được backward transfer tích cực (tức là F <0) ngoại trừ VCL, cho thấy khả năng cải thiện hiệu suất của các nhiệm vụ trước đó bằng cách học những nhiệm vụ mới vẫn là một thử thách đáng kể. Chúng tôi để lại điều này cho công việc tương lai. Cuối cùng, kết quả trong Hình 3 cho thấy CoTASP là phương pháp duy nhất hoạt động tương đương với các baseline học đa nhiệm vụ trên mười nhiệm vụ đầu tiên của chuỗi CW20, và nó thể hiện hiệu suất vượt trội so với những baseline này sau khi học toàn bộ chuỗi CW20. Một giải thích có thể là kiến thức được tích lũy bởi mạng meta-policy và từ điển của CoTASP dẫn đến khả năng tổng quát hóa được cải thiện.

4.3. Nghiên cứu Ablation
Hiệu quả của các thiết kế cốt lõi. Để cho thấy hiệu quả của mỗi thành phần của chúng tôi, chúng tôi tiến hành một nghiên cứu ablation trên bốn biến thể của CoTASP, mỗi biến thể loại bỏ hoặc thay đổi một lựa chọn thiết kế duy nhất được thực hiện trong CoTASP gốc. Bảng 2 trình bày kết quả của nghiên cứu ablation trên chuỗi CW20, sử dụng hai metrics đánh giá đại diện. Trong số bốn biến thể của CoTASP, "D frozen" thay thế từ điển có thể học được bằng một từ điển cố định, được khởi tạo ngẫu nhiên; "α frozen" loại bỏ tối ưu hóa prompt được đề xuất trong Phần 3.3; "both frozen" không cập nhật từ điển cũng không tối ưu hóa prompt; "lazily update D" dừng học từ điển sau khi hoàn thành mười nhiệm vụ đầu tiên của chuỗi CW20. Theo kết quả trong Bảng 2, chúng tôi đưa ra các kết luận sau: (1) Việc sử dụng một từ điển cố định, được khởi tạo ngẫu nhiên làm suy giảm hiệu suất của CoTASP trên hai metrics đánh giá, làm nổi bật tầm quan trọng của từ điển có thể học được trong việc nắm bắt các mối tương quan ngữ nghĩa giữa các nhiệm vụ. (2) Biến thể "α frozen" hoạt động tương đương với CoTASP của chúng tôi nhưng vượt trội hơn kết quả đạt được bởi EWC và PackNet. Điều này cho thấy tối ưu hóa prompt có thể cải thiện hiệu suất của CoTASP nhưng không quan trọng đối với kết quả hấp dẫn của chúng tôi. (3) Biến thể "both frozen" thể hiện sự suy giảm đáng chú ý trong hiệu suất, hỗ trợ kết luận rằng sự kết hợp của các thiết kế cốt lõi được đề xuất trong CoTASP là cần thiết để đạt được kết quả mạnh mẽ. (4) Biến thể "lazily update D" chỉ suy giảm nhẹ so với CoTASP gốc về hiệu suất nhưng vẫn vượt trội hơn tất cả baseline với một khoảng cách lớn, cho thấy từ điển đã học đã tích lũy đủ kiến thức trong mười nhiệm vụ đầu tiên để CoTASP có thể đạt được kết quả cạnh tranh mà không cần cập nhật từ điển cho các nhiệm vụ lặp lại.

Ảnh hưởng của các siêu tham số chính. CoTASP giới thiệu tham số thưa thớt λ, một siêu tham số kiểm soát sự cân bằng giữa dung lượng mạng được sử dụng và hiệu suất của policy kết quả. Một giá trị λ lớn hơn dẫn đến một mạng con policy thưa thớt hơn, cải thiện hiệu quả sử dụng dung lượng của mạng meta-policy. Nhưng chi phí là hiệu suất giảm trên mỗi nhiệm vụ do mất tính biểu đạt của policy nhiệm vụ quá thưa thớt. Theo kết quả trong Hình 4, CoTASP với λ=1e-3 hoặc 1e-4 đạt được sự cân bằng tốt hơn giữa hiệu suất và hiệu quả sử dụng so với các phương pháp dựa trên structure khác (HAT và PackNet) trên chuỗi CW10.

4.4. Tại sao CoTASP hoạt động? Một Nghiên cứu Thực nghiệm
Trong phần này, chúng tôi trả lời các câu hỏi sau dựa trên các hiện tượng quan sát được từ kết quả thực nghiệm của chúng tôi: (1) Từ điển đã học có thể tổng quát hóa không? (2) Các prompt thưa thớt được tạo bởi CoTASP có nắm bắt các mối tương quan ngữ nghĩa giữa các nhiệm vụ không?

Để trả lời câu hỏi đầu tiên, chúng tôi đo sự thay đổi của từ điển, trực quan hóa động lực của chúng và so sánh chi phí thích ứng của CoTASP với các phương pháp RL liên tục/meta khác trên chuỗi CW20. Kết quả trong Hình 5(b) cho thấy những từ điển này hội tụ nhanh chóng với số lượng nhiệm vụ tăng, dẫn đến một ánh xạ ổn định từ embedding nhiệm vụ đến prompt đã tối ưu. Trong nhiệm vụ mới tiếp theo, CoTASP sẽ tạo ra các prompt "tốt" ban đầu bằng sparse coding của embedding nhiệm vụ của nó. Điều này giảm đáng kể số bước huấn luyện cần thiết để đạt ngưỡng thành công, như được chứng minh bởi Hình 5(a). Hơn nữa, chúng tôi so sánh CoTASP với một thuật toán meta-RL SoTA, CoMPS (Berseth et al., 2022), để cho thấy sự vượt trội của nó. Cụ thể, chúng tôi pre-train CoTASP và CoMPS trên chuỗi CW10 và sau đó sử dụng meta-policy đã học làm policy ban đầu để fine-tune các nhiệm vụ chưa thấy, ví dụ, reach-v1 và button-press-v1. Theo kết quả được hiển thị trong Hình 6, CoTASP hoạt động tương đương với CoMPS nhưng tốt hơn đáng kể so với baseline RL liên tục (HAT) về chi phí thích ứng. Tuy nhiên, do thiếu cơ chế chống quên lãng thảm khốc, CoMPS thích ứng với nhiệm vụ mới trong khi hiệu suất của nó trên các nhiệm vụ trước đó nhanh chóng suy giảm, dẫn đến lợi nhuận trung bình tồi tệ hơn trên tất cả các nhiệm vụ.

Để trả lời câu hỏi thứ hai, chúng tôi trực quan hóa sự tương tự (tức là, sự chồng chéo giữa hai prompt nhị phân) của các prompt được tạo bởi CoTASP giữa mỗi hai nhiệm vụ trên chuỗi CW10 trong Hình 7. Heatmap màu xanh tóm tắt các giá trị tương tự được tính trung bình trên tất cả các lớp ẩn. Cụ thể, phần tử trên hàng i và cột j là giá trị tương tự trung bình được tính giữa nhiệm vụ i và nhiệm vụ j. Đối với nhiệm vụ 2 và nhiệm vụ 7, mô tả nhiệm vụ của chúng chia sẻ cùng một primitive thao tác, tức là, đẩy một puck. Do đó, các prompt được tạo bằng cách giải quyết bài toán lasso trong Phương trình 3 có mối tương quan cao. Ngược lại, đối với nhiệm vụ 2 và nhiệm vụ 7 với mô tả nhiệm vụ không liên quan, CoTASP tạo ra các prompt khác nhau, giảm can thiệp chéo giữa các nhiệm vụ và cải thiện tính dẻo.

5. Kết luận
Chúng tôi đề xuất CoTASP để giải quyết hai thử thách chính trong RL liên tục: (1) huấn luyện một meta-policy có thể tổng quát hóa cho tất cả các nhiệm vụ đã thấy và thậm chí chưa thấy, và (2) trích xuất hiệu quả một policy nhiệm vụ từ meta-policy. CoTASP học một từ điển để tạo ra các mặt nạ thưa thớt (prompts) để trích xuất policy của mỗi nhiệm vụ như một mạng con của meta-policy và tối ưu hóa mạng con thông qua RL. Điều này khuyến khích chia sẻ/tái sử dụng kiến thức giữa các nhiệm vụ liên quan trong khi giảm can thiệp chéo có hại gây ra quên lãng và thích ứng nhiệm vụ mới kém. Mà không cần bất kỳ phát lại kinh nghiệm nào, CoTASP đạt được sự cân bằng tính dẻo-ổn định tốt hơn đáng kể và phân bổ dung lượng mạng hiệu quả hơn so với baseline. Các policy được trích xuất của nó vượt trội hơn tất cả baseline trên cả nhiệm vụ trước đó và mới.
