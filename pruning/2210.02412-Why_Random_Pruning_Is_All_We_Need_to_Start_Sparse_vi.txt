I'll work on translating this academic paper from English to Vietnamese. Given the length and complexity of this document, I'll start with the translation:

# Tại Sao Cắt Tỉa Ngẫu Nhiên Là Tất Cả Những Gì Chúng Ta Cần Để Bắt Đầu Thưa
Advait Gadhikar1 Sohom Mukherjee1 Rebekka Burkholz1

## Tóm tắt
Các mặt nạ ngẫu nhiên xác định các mô hình mạng nơ-ron thưa hiệu quả một cách đáng ngạc nhiên, như đã được chứng minh thực nghiệm. Các mạng thưa kết quả thường có thể cạnh tranh với các kiến trúc dày đặc và các thuật toán cắt tỉa vé số tiên tiến, mặc dù chúng không dựa vào các lần lặp cắt tỉa-huấn luyện tốn kém về mặt tính toán và có thể được vẽ ban đầu mà không có chi phí tính toán đáng kể. Chúng tôi đưa ra một lời giải thích lý thuyết về cách các mặt nạ ngẫu nhiên có thể xấp xỉ các mạng mục tiêu tùy ý nếu chúng rộng hơn một hệ số logarit trong độ thưa nghịch đảo 1/log(1/sparsity). Hệ số quá tham số hóa này là cần thiết ít nhất cho các mạng ngẫu nhiên 3 lớp, điều này làm sáng tỏ hiệu suất suy giảm quan sát được của các mạng ngẫu nhiên ở độ thưa cao hơn. Tuy nhiên, ở mức độ thưa trung bình đến cao, kết quả của chúng tôi ngụ ý rằng các mạng thưa hơn được chứa trong các mạng nguồn ngẫu nhiên để bất kỳ lược đồ huấn luyện dày đặc-đến-thưa nào có thể được chuyển đổi thành một lược đồ thưa-đến-thưa hiệu quả hơn về mặt tính toán bằng cách hạn chế việc tìm kiếm trong một mặt nạ ngẫu nhiên cố định. Chúng tôi chứng minh tính khả thi của phương pháp này trong các thí nghiệm cho các phương pháp cắt tỉa khác nhau và đề xuất các lựa chọn đặc biệt hiệu quả về tỷ lệ thưa ban đầu theo lớp của mạng nguồn ngẫu nhiên. Như một trường hợp đặc biệt, chúng tôi chứng minh về mặt lý thuyết và thực nghiệm rằng các mạng nguồn ngẫu nhiên cũng chứa các vé số mạnh. Mã của chúng tôi có sẵn tại https://github.com/RelationalML/sparse_to_sparse.

## 1. Giới thiệu
Những đột phá ấn tượng được đạt được bởi học sâu phần lớn được quy cho việc quá tham số hóa rộng rãi của các mạng nơ-ron sâu, vì có vẻ như nó có nhiều lợi ích cho khả năng biểu diễn và tối ưu hóa của chúng (Belkin et al., 2019). Tuy nhiên, xu hướng kết quả hướng tới các mô hình và tập dữ liệu ngày càng lớn hơn áp đặt chi phí tính toán và năng lượng ngày càng tăng khó đáp ứng. Điều này đặt ra câu hỏi: Liệu mức độ quá tham số hóa cao này có thực sự cần thiết không?

Huấn luyện các kiến trúc mạng nơ-ron sâu quy mô nhỏ hoặc thưa tổng quát từ đầu vẫn là một thách thức đối với các lược đồ khởi tạo tiêu chuẩn (Li et al., 2016; Han et al., 2015). Tuy nhiên, (Frankle & Carbin, 2019) gần đây đã chứng minh rằng tồn tại các kiến trúc thưa có thể được huấn luyện để giải quyết các vấn đề chuẩn một cách cạnh tranh. Theo Giả thuyết Vé Số Xổ (LTH) của họ, các mạng được khởi tạo ngẫu nhiên dày đặc chứa các mạng con có thể được huấn luyện riêng biệt đến độ chính xác kiểm tra có thể so sánh với mạng dày đặc gốc. Các mạng con như vậy, các vé số (LT), kể từ đó đã được thu được bằng các thuật toán cắt tỉa yêu cầu các lần lặp cắt tỉa-huấn luyện tốn kém về mặt tính toán (Frankle & Carbin, 2019; Tanaka et al., 2020) hoặc các thủ tục học mặt nạ (Savarese et al., 2020; Sreenivasan et al., 2022b). Mặc dù chúng có thể dẫn đến lợi ích tính toán tại thời gian huấn luyện và suy luận và giảm yêu cầu bộ nhớ (Hassibi et al., 1993; Han et al., 2015), mục tiêu thực sự vẫn là xác định các kiến trúc thưa có thể huấn luyện được trước khi huấn luyện, vì điều này có thể dẫn đến tiết kiệm tính toán đáng kể. Tuy nhiên, các phương pháp cắt tỉa tại khởi tạo đương đại (Lee et al., 2018; Wang et al., 2020; Tanaka et al., 2020; Fischer & Burkholz, 2022; Frankle et al., 2021) đạt được hiệu suất ít cạnh tranh hơn. Vì lý do đó, thật đáng chú ý rằng ngay cả các phương pháp lặp tiên tiến cũng phải vật lộn để vượt qua một phương pháp thay thế đơn giản, rẻ về mặt tính toán và độc lập dữ liệu: cắt tỉa ngẫu nhiên tại khởi tạo (Su et al., 2020). Liu et al. (2021) đã cung cấp bằng chứng thực nghiệm có hệ thống cho hiệu quả 'phi lý' của nó trong nhiều thiết lập, bao gồm các kiến trúc và dữ liệu phức tạp, quy mô lớn.

Chúng tôi giải thích về mặt lý thuyết tại sao chúng có thể hiệu quả bằng cách chứng minh rằng một mạng được che mặt ngẫu nhiên có thể xấp xỉ một mạng mục tiêu tùy ý nếu nó rộng hơn một hệ số logarit trong độ thưa của nó 1/log(1/sparsity). Bằng cách rút ra một cận dưới về độ rộng yêu cầu của một mạng ẩn 1 lớp ngẫu nhiên, chúng tôi tiếp tục chỉ ra rằng mức độ quá tham số hóa này là cần thiết nói chung. Điều này ngụ ý rằng các mạng ngẫu nhiên thưa có tính chất xấp xỉ hàm phổ quát như các mạng dày đặc và ít nhất cũng có tính biểu diễn như các mạng mục tiêu tiềm năng. Tuy nhiên, nó cũng làm nổi bật những hạn chế của cắt tỉa ngẫu nhiên trong trường hợp độ thưa cực cao, vì yêu cầu độ rộng sau đó tỷ lệ xấp xỉ như 1/log(1/sparsity) ≈ 1/(1-sparsity) (xem cũng Hình 2 cho một ví dụ). Trong thực tế, chúng tôi quan sát sự suy giảm tương tự về hiệu suất ở mức độ thưa cao.

Ngay cả đối với độ thưa trung bình đến cao, tính ngẫu nhiên của các kết nối dẫn đến một số lượng đáng kể các trọng số dư thừa không cần thiết cho việc biểu diễn mạng mục tiêu. Hiểu biết này gợi ý rằng, một mặt, việc cắt tỉa bổ sung có thể nâng cao thêm độ thưa của cấu trúc mạng nơ-ron kết quả, vì các mặt nạ ngẫu nhiên có khả năng không thưa tối ưu. Mặt khác, bất kỳ phương pháp huấn luyện dày đặc-đến-thưa nào cũng không cần phải bắt đầu từ một mạng dày đặc mà cũng có thể bắt đầu huấn luyện từ một mạng ngẫu nhiên thưa hơn và do đó được chuyển đổi thành một phương pháp học thưa-đến-thưa. Ý tưởng chính được minh họa trong

[Hình 1: Huấn luyện thưa với các mạng được che mặt ngẫu nhiên (ER): Một biểu diễn trực quan của hàm ý chính của lý thuyết chúng tôi - huấn luyện thưa đến thưa có thể hiệu quả bằng cách bắt đầu từ một mạng được che mặt ngẫu nhiên (ER).]

Hình 1 và được xác minh trong các thí nghiệm rộng rãi với các phương pháp cắt tỉa vé số khác nhau và liên tục thưa hóa. Các kết quả chính của chúng tôi cũng có thể được hiểu như là sự biện minh lý thuyết cho Huấn luyện Thưa Động (DST) (Evci et al., 2020; Liu et al., 2021.; Bellec et al., 2018), cắt tỉa các mạng ngẫu nhiên có độ thưa vừa phải. Tuy nhiên, nó còn dựa vào các bước kết nối lại cạnh đôi khi yêu cầu tính toán gradient của mạng dày đặc tương ứng (Evci et al., 2020). Những hạn chế được rút ra của chúng tôi về cắt tỉa ngẫu nhiên cho thấy rằng việc kết nối lại này có thể là cần thiết ở độ thưa cực đoan nhưng có thể không cần thiết cho các điểm khởi đầu ngẫu nhiên thưa vừa phải, như chúng tôi cũng làm nổi bật trong các thí nghiệm bổ sung.

Như một trường hợp đặc biệt của ý tưởng chính để cắt tỉa các mạng ngẫu nhiên, chúng tôi cũng xem xét các vé số mạnh (SLT) (Zhou et al., 2019; Ramanujan et al., 2020). Đây là các mạng con của các mạng nguồn lớn, được khởi tạo ngẫu nhiên, không yêu cầu bất kỳ huấn luyện thêm nào sau khi cắt tỉa. Các bằng chứng tồn tại lý thuyết (Malach et al., 2020; Pensia et al., 2020; Fischer et al., 2021; da Cunha et al., 2022; Burkholz, 2022a;b; Burkholz et al., 2022) cũng như thực nghiệm Ramanujan et al. (2020); Zhou et al. (2019); Diffenderfer & Kailkhura (2021); Sreenivasan et al. (2022a) cho đến nay chỉ tập trung vào việc cắt tỉa các mạng nguồn dày đặc. Chúng tôi làm nổi bật tiềm năng tiết kiệm tài nguyên tính toán trong việc tìm kiếm SLT bằng cách chứng minh sự tồn tại của chúng trong các mạng ngẫu nhiên thưa thay thế. Thành phần chính của kết quả chúng tôi là Bổ đề 2.2, mở rộng các xấp xỉ tổng tập con đến thiết lập đồ thị ngẫu nhiên thưa. Điều này cho phép chuyển đổi trực tiếp hầu hết các kết quả tồn tại SLT cho các kiến trúc và hàm kích hoạt khác nhau sang các mạng nguồn thưa. Hơn nữa, chúng tôi sửa đổi thuật toán edge-popup (EP) (Ramanujan et al., 2020) để tìm SLT tương ứng, dẫn đến phương pháp cắt tỉa thưa-đến-thưa đầu tiên cho SLT, theo hiểu biết của chúng tôi. Chúng tôi chứng minh trong các thí nghiệm rằng việc bắt đầu ngay cả ở độ thưa cao như 0.8 không cản trở hiệu suất tổng thể của EP.

Lưu ý rằng lý thuyết tổng quát của chúng tôi áp dụng cho bất kỳ tỷ lệ thưa theo lớp nào của mạng nguồn ngẫu nhiên và chúng tôi xác nhận thực tế này trong các thí nghiệm khác nhau trên dữ liệu hình ảnh chuẩn và các kiến trúc mạng nơ-ron thường được sử dụng, bổ sung kết quả của Liu et al. (2021) cho các lựa chọn tỷ lệ thưa bổ sung. Hai đề xuất của chúng tôi, tỷ lệ thưa cân bằng và hình kim tự tháp, có vẻ hoạt động cạnh tranh trong nhiều thiết lập, đặc biệt là ở các chế độ thưa cao hơn.

## Đóng góp
1. Chúng tôi chứng minh rằng các mạng ngẫu nhiên được cắt tỉa ngẫu nhiên đủ biểu diễn và có thể xấp xỉ một mạng mục tiêu tùy ý nếu chúng rộng hơn một hệ số 1/log(1/sparsity). Hệ số quá tham số hóa này là cần thiết nói chung, như cận dưới của chúng tôi cho các mạng mục tiêu đơn biến chỉ ra.

2. Được truyền cảm hứng từ các bằng chứng của chúng tôi, chúng tôi chứng minh thực nghiệm rằng, không mất đáng kể về hiệu suất, việc bắt đầu bất kỳ lược đồ huấn luyện dày đặc-đến-thưa nào có thể được dịch thành một lược đồ thưa-đến-thưa bằng cách bắt đầu từ một mạng nguồn ngẫu nhiên thay vì một mạng dày đặc.

3. Như một trường hợp đặc biệt, chúng tôi cũng chứng minh sự tồn tại của Vé Số Mạnh (SLT) trong các mạng nguồn ngẫu nhiên thưa, nếu mạng nguồn rộng hơn mục tiêu một hệ số 1/log(1/sparsity). Việc sửa đổi thuật toán edge-popup (EP) (Ramanujan et al., 2020) của chúng tôi dẫn đến phương pháp cắt tỉa SLT thưa-đến-thưa đầu tiên, xác nhận lý thuyết của chúng tôi và làm nổi bật tiềm năng tiết kiệm tính toán.

4. Để chứng minh rằng lý thuyết của chúng tôi áp dụng cho các lựa chọn tỷ lệ thưa khác nhau, chúng tôi giới thiệu hai đề xuất bổ sung vượt trội hơn các đề xuất tiên tiến trên nhiều chuẩn và do đó là các ứng cử viên hứa hẹn cho điểm khởi đầu của các lược đồ học thưa-đến-thưa.

### 1.1. Công trình liên quan
Các thuật toán cắt tỉa mạng nơ-ron cho độ thưa không có cấu trúc có thể được phân loại rộng rãi thành hai nhóm, cắt tỉa sau huấn luyện và cắt tỉa trước (hoặc trong) huấn luyện. Nhóm thuật toán đầu tiên cắt tỉa sau huấn luyện hiệu quả trong việc tăng tốc suy luận, nhưng chúng vẫn dựa vào một quy trình huấn luyện tốn kém về mặt tính toán (Hassibi et al., 1993; LeCun et al., 1989; Molchanov et al., 2016; Dong et al., 2017; Yu et al., 2022). Nhóm thuật toán thứ hai cắt tỉa tại khởi tạo (Lee et al., 2018; Wang et al., 2020; Tanaka et al., 2020; Sreenivasan et al., 2022b; de Jorge et al., 2020) hoặc theo một chu kỳ cắt tỉa và huấn luyện lại tốn kém về mặt tính toán cho nhiều lần lặp (Gale et al., 2019; Savarese et al., 2020; You et al., 2019; Frankle & Carbin, 2019; Renda et al., 2019). Các phương pháp này tìm thấy các mạng con có thể huấn luyện được cũng được gọi là Vé Số (Frankle & Carbin, 2019). Các phương pháp cắt tỉa một lần rẻ hơn về mặt tính toán nhưng dễ bị các vấn đề như sự sụp đổ lớp khiến mạng được cắt tỉa không thể huấn luyện (Lee et al., 2018; Wang et al., 2020). Tanaka et al. (2020) giải quyết vấn đề này bằng cách bảo tồn luồng trong mạng thông qua cơ chế tính điểm của họ. Các mạng thưa hoạt động tốt nhất vẫn được thu được bằng các phương pháp cắt tỉa lặp đắt đỏ như Cắt tỉa Độ lớn Lặp (IMP), Synflow Lặp (Frankle & Carbin, 2019; Fischer & Burkholz, 2022) hoặc các phương pháp thưa hóa liên tục (Sreenivasan et al., 2022b; Savarese et al., 2020; Kusupati et al., 2020; Louizos et al., 2018).

Tuy nhiên, Su et al. (2020) phát hiện rằng các mặt nạ được cắt tỉa ngẫu nhiên có thể vượt trội hơn các chiến lược cắt tỉa lặp đắt đỏ trong các tình huống khác nhau. Được truyền cảm hứng từ phát hiện này, Golubeva et al. (2021); Chang et al. (2021) đã đưa ra giả thuyết rằng các mạng thưa quá tham số hóa hiệu quả hơn các mạng nhỏ hơn với cùng số lượng tham số. Liu et al. (2021) đã tiếp tục chứng minh tính cạnh tranh của các mặt nạ ngẫu nhiên cho các lựa chọn độc lập dữ liệu khác nhau về tỷ lệ thưa theo lớp trên một loạt rộng các kiến trúc mạng nơ-ron và tập dữ liệu, bao gồm cả những cái phức tạp. Phân tích của chúng tôi xác định các điều kiện mà tính hiệu quả của các mặt nạ ngẫu nhiên là hợp lý. Chúng tôi chỉ ra rằng một mạng nguồn ngẫu nhiên thưa có thể xấp xỉ một mạng mục tiêu nếu nó rộng hơn một hệ số tỷ lệ với log độ thưa nghịch đảo. Bổ sung các thí nghiệm của Liu et al. (2021), chúng tôi làm nổi bật rằng các mặt nạ ngẫu nhiên cạnh tranh cho các lựa chọn tỷ lệ thưa theo lớp khác nhau. Tuy nhiên, chúng tôi cũng chỉ ra rằng tính ngẫu nhiên của chúng cũng có khả năng tạo ra tiềm năng cho việc cắt tỉa thêm.

Chúng tôi xây dựng dựa trên lý thuyết tồn tại vé số (Malach et al., 2020; Pensia et al., 2020; Orseau et al., 2020; Fischer et al., 2021; Burkholz et al., 2022; Burkholz, 2022b; Ferbach et al., 2022) để chứng minh rằng các mạng nguồn ngẫu nhiên thưa thực sự chứa các vé số mạnh (SLT) nếu độ rộng của chúng vượt quá một giá trị tỷ lệ với độ rộng của mạng mục tiêu. Lý thuyết này đã được truyền cảm hứng từ bằng chứng thực nghiệm cho SLT (Ramanujan et al., 2020; Zhou et al., 2019; Diffenderfer & Kailkhura, 2021; Sreenivasan et al., 2022a). Thuật toán cơ bản edge-popup (Ramanujan et al., 2020) tìm SLT bằng cách huấn luyện điểm cho mỗi tham số của mạng nguồn dày đặc và do đó tốn kém về mặt tính toán như huấn luyện dày đặc. Chúng tôi chỉ ra rằng việc huấn luyện các mạng nguồn ngẫu nhiên thưa nhỏ hơn là đủ, do đó, giảm hiệu quả yêu cầu tính toán để tìm SLT.

Tuy nhiên, lý thuyết của chúng tôi gợi ý rằng các mạng ER ngẫu nhiên đối mặt với một hạn chế cơ bản ở độ thưa cực đoan, vì hệ số quá tham số hóa tỷ lệ trong chế độ này như 1/log(1/(sparsity)) ≈ 1/(1-sparsity). Thiếu sót này có thể được giải quyết bằng việc kết nối lại có mục tiêu các cạnh ngẫu nhiên với Huấn luyện Thưa Động (DST) bắt đầu cắt tỉa từ một mạng ER (Liu et al., 2021.; Mocanu et al., 2018; Yuan et al., 2021). Cho đến nay, các phương pháp huấn luyện thưa-đến-thưa như Evci et al. (2020); Dettmers & Zettlemoyer (2019) vẫn yêu cầu gradient dày đặc cho hoạt động kết nối lại cạnh ở đó. Zhou et al. (2021) thu được huấn luyện thưa bằng cách ước tính gradient thưa sử dụng hai lần truyền thuận. Chúng tôi chỉ ra thực nghiệm rằng dưới ánh sáng của sức mạnh biểu diễn của các mạng ngẫu nhiên, chúng ta cũng có thể đạt được huấn luyện thưa-đến-thưa bằng cách đơn giản hạn chế bất kỳ phương pháp cắt tỉa hoặc gradient nào trong một mặt nạ ngẫu nhiên thưa ban đầu cố định.

## 2. Tính biểu diễn của Mạng Ngẫu nhiên
Các điều tra lý thuyết của chúng tôi trong phần tiếp theo có mục đích giải thích tại sao tính hiệu quả của các mạng ngẫu nhiên là hợp lý do sức mạnh biểu diễn cao của chúng. Chúng tôi chỉ ra rằng chúng ta có thể xấp xỉ bất kỳ mạng mục tiêu nào với sự giúp đỡ của một mạng ngẫu nhiên, với điều kiện nó rộng hơn một hệ số logarit trong độ thưa nghịch đảo. Đầu tiên, ràng buộc duy nhất mà chúng ta đối mặt trong việc xây dựng rõ ràng một mạng con đại diện là các cạnh có sẵn hoặc không có sẵn một cách ngẫu nhiên. Nhưng chúng ta có thể chọn các tham số mạng còn lại, tức là các trọng số và độ lệch, theo cách mà chúng ta có thể biểu diễn tối ưu một mạng mục tiêu. Như thông thường trong các kết quả về tính biểu diễn và sức mạnh đại diện, chúng tôi đưa ra các phát biểu về sự tồn tại của các tham số như vậy, không nhất thiết, nếu chúng có thể được tìm thấy theo thuật toán. Trong thực tế, các tham số thường sẽ được xác định bằng huấn luyện mạng nơ-ron tiêu chuẩn hoặc các lần lặp cắt tỉa-huấn luyện. Các thí nghiệm của chúng tôi xác nhận rằng điều này thực sự khả thi ngoài nhiều bằng chứng thực nghiệm khác (Su et al., 2020; Ma et al., 2021; Liu et al., 2021). Thứ hai, chúng tôi chứng minh sự tồn tại của các vé số mạnh (SLT), giả định rằng chúng ta phải xấp xỉ các tham số mục tiêu bằng cách cắt tỉa mạng nguồn ngẫu nhiên thưa. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên cung cấp bằng chứng thực nghiệm và lý thuyết cho tính khả thi của trường hợp này.

**Bối cảnh, Ký hiệu và Thiết lập Chứng minh** Gọi x = (x₁, x₂, .., xₐ) ∈ [a₁, b₁]ᵈ là một vector đầu vào d chiều bị chặn, trong đó a₁, b₁ ∈ ℝ với a₁ < b₁. f: [a₁, b₁]ᵈ → ℝⁿᴸ là một mạng nơ-ron truyền thẳng kết nối đầy đủ với kiến trúc (n₀, n₁, .., nᴸ), tức là độ sâu L và nₗ nơ-ron trong Lớp l. Mỗi lớp l ∈ {1,2, .., L} tính toán các trạng thái nơ-ron x⁽ˡ⁾ = φ(h⁽ˡ⁾), h⁽ˡ⁾ = W⁽ˡ⁻¹⁾x⁽ˡ⁻¹⁾ + b⁽ˡ⁻¹⁾. h⁽ˡ⁾ được gọi là tiền kích hoạt, W⁽ˡ⁾ ∈ ℝⁿˡˣⁿˡ⁻¹ là ma trận trọng số và b⁽ˡ⁾ là vector độ lệch. Chúng tôi cũng viết f(x;θ) để nhấn mạnh sự phụ thuộc của mạng nơ-ron vào các tham số θ = (W⁽ˡ⁾, b⁽ˡ⁾)ᴸₗ₌₁ của nó. Để đơn giản, chúng tôi hạn chế bản thân với hàm kích hoạt ReLU thông thường φ(x) = max{x,0}, nhưng hầu hết các kết quả của chúng tôi có thể dễ dàng mở rộng đến các hàm kích hoạt tổng quát hơn như trong (Burkholz, 2022b;a). Ngoài các lớp kết nối đầy đủ, chúng tôi cũng xem xét các lớp tích chập. Để có ký hiệu thuận tiện, không mất tính tổng quát, chúng tôi làm phẳng các tensor trọng số để W⁽ˡ⁾ᵀ ∈ ℝᶜˡˣᶜˡ⁻¹ˣᵏˡ trong đó cₗ, cₗ₋₁, kₗ lần lượt là các kênh đầu ra, kênh đầu vào và chiều bộ lọc. Ví dụ, một tích chập 2 chiều trên dữ liệu hình ảnh sẽ dẫn đến kₗ = k'₁,ₗk'₂,ₗ, trong đó k'₁,ₗ, k'₂,ₗ định nghĩa kích thước bộ lọc.

Chúng tôi phân biệt ba loại mạng nơ-ron, một mạng mục tiêu fₜ, một mạng nguồn fₛ, và một mạng con fₚ của fₛ. fₜ được xấp xỉ hoặc biểu diễn chính xác bởi fₚ, được thu được bằng cách che các tham số của nguồn fₛ. fₛ được cho là chứa một SLT nếu mạng con này không yêu cầu huấn luyện thêm sau khi thu được mặt nạ (bằng cắt tỉa). Chúng tôi giả định rằng fₜ có độ sâu L và các tham số W⁽ˡ⁾ₜ, b⁽ˡ⁾ₜ, nₜ,ₗ, mₜ,ₗ là trọng số, độ lệch, số nơ-ron và số tham số khác không của ma trận trọng số trong Lớp l ∈ {1,2, .., L}. Lưu ý rằng điều này ngụ ý mₗ ≤ nₗnₗ₋₁. Tương tự, fₛ có độ sâu L + 1 với các tham số W⁽ˡ⁾ₛ, b⁽ˡ⁾ₛ, nₛ,ₗ, mₛ,ₗᴸₗ₌₀. Lưu ý rằng l dao động từ 0 đến L cho mạng nguồn, trong khi nó chỉ dao động từ 1 đến L cho mạng mục tiêu. Lớp mạng nguồn bổ sung l = 0 tính cho một lớp bổ sung mà chúng tôi cần trong việc xây dựng của chúng tôi để chứng minh sự tồn tại.

**Mạng ER** Mặc dù phổ biến, thuật ngữ 'mạng ngẫu nhiên' không chính xác đối với phân phối ngẫu nhiên mà từ đó một đồ thị được vẽ. Phù hợp với lý thuyết đồ thị tổng quát, do đó chúng tôi sử dụng thuật ngữ mạng Erdős-Rényi (ER) (Erdos et al., 1960) trong phần tiếp theo. Một mạng nơ-ron ER fₑᵣ ∈ ER(p) được đặc trưng bởi các tỷ lệ thưa theo lớp pₗ. Một nguồn ER fₑᵣ được định nghĩa là một mạng con của một mạng nguồn hoàn chỉnh sử dụng một mặt nạ nhị phân S⁽ˡ⁾ₑᵣ ∈ {0,1}ⁿˡˣⁿˡ⁻¹ hoặc S⁽ˡ⁾ₑᵣ ∈ {0,1}ⁿˡˣⁿˡ⁻¹ˣᵏˡ cho mỗi lớp. Các mục mặt nạ được rút ra từ các phân phối Bernoulli độc lập với xác suất thành công theo lớp pₗ > 0, tức là s⁽ˡ⁾ᵢⱼ,ₑᵣ ~ Ber(pₗ). Việc cắt tỉa ngẫu nhiên được thực hiện ban đầu với chi phí tính toán không đáng kể và mặt nạ giữ cố định trong suốt quá trình huấn luyện. Lưu ý rằng pₗ cũng là mật độ dự kiến của lớp đó. Mật độ dự kiến tổng thể của mạng được cho như p = Σₗmₗpₗ/Σₖmₖ = 1-sparsity. Trong trường hợp độ thưa đồng nhất, pₗ = p, chúng tôi cũng viết ER(p) thay vì ER(p). Một mạng ER được định nghĩa là fₑᵣ = fₛ(x; W·Sₑᵣ). Khác với các bằng chứng tồn tại SLT thông thường (Ramanujan et al., 2020), chúng tôi gọi fₑᵣ ∈ ER(p) là mạng nguồn, và chỉ ra rằng SLT được chứa trong mạng ER này. SLT sau đó được định nghĩa bởi mặt nạ Sₚ, là một mạng con của Sₑᵣ, tức là một mục không sᵢⱼ,ₑᵣ = 0 cũng ngụ ý một số không trong sᵢⱼ,ₚ = 0, nhưng điều ngược lại không đúng. Chúng tôi bỏ qua các chỉ số con nếu bản chất của mặt nạ rõ ràng từ ngữ cảnh. Trong phân tích tiếp theo về tính biểu diễn trong các mạng ER, chúng tôi tiếp tục sử dụng Sₑᵣ và Sₚ để biểu thị một mạng nguồn ER ngẫu nhiên và một mạng con thưa trong mạng ER tương ứng.

**Tỷ lệ Thưa** Có nhiều lựa chọn hợp lý cho các tỷ lệ thưa theo lớp và do đó các xác suất ER pₗ. Lý thuyết của chúng tôi áp dụng cho tất cả chúng. Lựa chọn tối ưu cho một kiến trúc mạng nguồn nhất định phụ thuộc vào mạng mục tiêu và do đó giải pháp cho một vấn đề học tập, thường không biết trước trong thực tế. Để chứng minh rằng lý thuyết của chúng tôi áp dụng cho các phương pháp khác nhau, chúng tôi điều tra các tỷ lệ thưa theo lớp sau đây trong các thí nghiệm. Đường cơ sở đơn giản nhất là một lựa chọn đồng nhất toàn cục pₗ = p. Liu et al. (2021) đã so sánh lựa chọn này trong các thí nghiệm rộng rãi với đề xuất chính của họ, ERK, gán pₗ ∝ (nᵢₙ + nₒᵤₜ)/(nᵢₙnₒᵤₜ) cho một lớp tuyến tính và pₗ ∝ (cₗ + cₗ₋₁ + kₗ)/(cₗcₗ₋₁kₗ) (Mocanu et al., 2017) cho một lớp tích chập. Ngoài ra, chúng tôi đề xuất một phương pháp hình kim tự tháp và cân bằng, được trực quan hóa trong Phụ lục A.15.

**Hình kim tự tháp**: Phương pháp này mô phỏng một tính chất của các mạng được cắt tỉa thu được bằng IMP (Frankle & Carbin, 2019) tức là mật độ lớp giảm với độ sâu tăng của mạng. Đối với một mạng có độ sâu L, chúng tôi sử dụng pₗ = (p₁)ˡ, pₗ ∈ (0,1) sao cho Σˡ⁼ᴸₗ₌₁pₗmₗ/Σˡ⁼ᴸₗ₌₁mₗ = p. Với kiến trúc đã cho, chúng tôi sử dụng một bộ giải phương trình đa thức (Harris et al., 2020) để có p₁ cho lớp đầu tiên sao cho p₁ ∈ (0,1).

**Cân bằng**: Phương pháp thưa theo lớp thứ hai nhằm duy trì cùng số lượng tham số trong mỗi lớp cho một độ thưa mạng p và kiến trúc mạng nguồn nhất định. Mỗi nơ-ron có cùng bậc vào và ra trung bình. Mỗi lớp có x = p(Σˡ⁼ᴸₗ₌₁mₗ)/L tham số khác không. Một mạng ER như vậy có thể được thực hiện với pₗ = x/mₗ. Trong trường hợp x ≥ mₗ, chúng tôi đặt pₗ = 1.

### 2.1. Tính biểu diễn tổng quát của Mạng ER
Mục tiêu chính của chúng tôi trong phần này là rút ra các phát biểu xác suất về sự tồn tại của các cạnh trong một mạng nguồn ER cho phép chúng ta xấp xỉ một mạng mục tiêu nhất định. Vì mỗi kết nối trong mạng nguồn chỉ tồn tại với xác suất pₗ, đối với mỗi trọng số mục tiêu, chúng ta cần tạo nhiều cạnh ứng cử viên, trong đó ít nhất một cạnh khác không với xác suất đủ cao. Điều này có thể đạt được bằng cách đảm bảo rằng mỗi cạnh mục tiêu có nhiều điểm khởi đầu tiềm năng trong mạng nguồn ER. Việc xây dựng của chúng tôi thực hiện ý tưởng này với nhiều bản sao của mỗi nơ-ron trong một lớp. Số lượng bản sao nơ-ron yêu cầu phụ thuộc vào độ thưa của mạng nguồn ER và giới thiệu một hệ số quá tham số hóa liên quan đến độ rộng của mạng. Để tạo nhiều bản sao của các nơ-ron đầu vào cũng như vậy, việc xây dựng của chúng tôi dựa vào một lớp bổ sung trong mạng nguồn so với mạng mục tiêu, như được trực quan hóa trong Hình 4 trong Phụ lục. Chúng tôi trước tiên giải thích việc xây dựng cho một lớp mục tiêu đơn và mở rộng nó sau đó đến các kiến trúc sâu hơn.

**Mục tiêu Lớp Ẩn Đơn** Chúng tôi bắt đầu với việc xây dựng một mạng mục tiêu kết nối đầy đủ một lớp ẩn với một mạng con của một mạng nguồn ER ngẫu nhiên bao gồm thêm một lớp. Chiến lược chứng minh của chúng tôi được giải thích trực quan bằng Hình 4 trong Phụ lục. Định lý sau đây phát biểu yêu cầu độ rộng chính xác mà việc xây dựng của chúng tôi yêu cầu.

**Định lý 2.1** (Xây dựng Mục tiêu Lớp Ẩn Đơn). Giả sử rằng một mạng mục tiêu một lớp ẩn kết nối đầy đủ fₜ(x) = W⁽²⁾ₜφ(W⁽¹⁾ₜx + b⁽¹⁾ₜ) + b⁽²⁾ₜ, một xác suất thất bại cho phép δ ∈ (0,1), mật độ nguồn p và một mạng nguồn ER 2 lớp fₛ ∈ ER(p) với độ rộng nₛ,₀ = q₀d, nₛ,₁ = q₁nₜ,₁, nₛ,₂ = q₂nₜ,₂ được cho. Nếu

q₀ ≥ 1/(log(1/(1-p₁))) log(2mₜ,₁q₁/δ),
q₁ ≥ 1/(log(1/(1-p₂))) log(2mₜ,₂/δ) và q₂ = 1

thì với xác suất 1-δ, mạng nguồn ngẫu nhiên fₛ chứa một mạng con Sₚ sao cho fₛ(x,W·Sₚ) = fₜ.

**Phác thảo Chứng minh:**
Ý tưởng chính là tạo nhiều bản sao (khối trong Hình 4 (b) trong Phụ lục) trong mạng nguồn cho mỗi nơ-ron mục tiêu sao cho mỗi liên kết mục tiêu được thực hiện bằng cách trỏ đến ít nhất một trong những bản sao này trong nguồn ER. Để tạo nhiều ứng cử viên của các nơ-ron đầu vào, chúng tôi tạo một lớp đầu tiên đơn biến trong mạng nguồn như được giải thích trong Hình 4. Trong phụ lục, chúng tôi rút ra các tham số trọng số và độ lệch tương ứng của mạng nguồn để nó có thể biểu diễn mạng mục tiêu chính xác. Đương nhiên, nhiều liên kết có sẵn sẽ nhận trọng số không nếu chúng không cần thiết trong việc xây dựng cụ thể nhưng được yêu cầu cho xác suất đủ cao rằng ít nhất một trọng số có thể được đặt thành khác không. Nhiệm vụ chính của chúng tôi trong chứng minh là ước tính xác suất mà chúng ta có thể tìm thấy các đại diện của tất cả các liên kết mục tiêu trong mạng nguồn ER, tức là mỗi nơ-ron trong Lớp l = 1 có ít nhất một cạnh đến mỗi khối trong l = 0 có kích thước q₀, như được hiển thị trong Hình 4 (b). Xác suất này được cho bởi (1-(1-p₁)^q₀)^(mₜ,₁q₁). Đối với lớp thứ hai, chúng tôi lặp lại một lập luận tương tự để ràng buộc xác suất (1-(1-p₂)^q₁)^mₜ,₂ với q₂ = 1, vì chúng tôi không yêu cầu nhiều bản sao của các nơ-ron đầu ra. Ràng buộc xác suất này bởi 1-δ hoàn thành chứng minh, như được chi tiết trong Phụ lục A.3.

**Mạng Mục tiêu Sâu** Định lý 2.1 chỉ ra rằng q₀ và q₁ phụ thuộc vào 1/log(1/sparsity). Bây giờ chúng tôi tổng quát hóa ý tưởng tạo nhiều bản sao của các nơ-ron mục tiêu trong mỗi lớp đến một mạng kết nối đầy đủ có độ sâu L (các chứng minh trong Phụ lục A.4) và mạng tích chập có độ sâu L như được phát biểu trong Phụ lục A.5, cho kết quả tương tự như trên. Thách thức bổ sung của việc mở rộng là xử lý các phụ thuộc của các lớp, vì việc xây dựng mỗi lớp cần phải khả thi.

**Định lý 2.2** (Mạng ER có thể biểu diễn mạng mục tiêu L lớp). Cho một mạng mục tiêu kết nối đầy đủ fₜ có độ sâu L, δ ∈ (0,1), mật độ nguồn p và một mạng nguồn ER L+1 lớp fₛ ∈ ER(p) với độ rộng nₛ,₀ = q₀d và nₛ,ₗ = qₗnₜ,ₗ, l ∈ {1,2, .., L}, trong đó

qₗ ≥ 1/(log(1/(1-pₗ₊₁))) log(Lmₜ,ₗ₊₁qₗ₊₁/δ)

cho l ∈ {0,1, .., L-1} và qₗ = 1,

thì với xác suất 1-δ mạng nguồn ngẫu nhiên fₛ chứa một mạng con Sₚ sao cho fₛ(x,W·Sₚ) = fₜ.

**Cận Dưới về Quá tham số hóa** Trong khi các kết quả tồn tại của chúng tôi chứng minh rằng các mạng ER có tính chất xấp xỉ hàm phổ quát như các mạng nơ-ron dày đặc, để đạt được điều đó, việc xây dựng của chúng tôi yêu cầu một lượng đáng kể quá tham số hóa so với một mạng mục tiêu dày đặc. Đặc biệt các mạng ER cực kỳ thưa có vẻ đối mặt với một hạn chế tự nhiên, vì đối với độ thưa 1-p ≥ 0.9, hệ số quá tham số hóa tỷ lệ xấp xỉ như 1/log(1/(1-p)) ≈ 1/p. Hình 2 trực quan hóa cách tỷ lệ này trở nên có vấn đề đối với độ thưa tăng. Định lý tiếp theo thiết lập rằng, thật không may, chúng ta không thể mong đợi thoát khỏi hạn chế 1/log(1/(1-pₗ)) này.

**Định lý 2.3** (Cận dưới về Quá tham số hóa trong Mạng ER). Tồn tại các mạng mục tiêu đơn biến fₜ(x) = φ(wₜᵀx + bₜ) không thể được biểu diễn bởi một mạng nguồn ER 1-lớp ẩn ngẫu nhiên fₛ ∈ ER(p) với xác suất ít nhất 1-δ, nếu độ rộng của nó là nₛ,₁ < 1/(log(1/(1-p))) log(1/(1-(1-δ)^(1/d))).

Xem Hình 6 và Phụ lục A.6 để có chứng minh đầy đủ.

**Hiểu biết Lý thuyết** Chúng tôi đã chỉ ra rằng các mạng ER có thể chứa các mạng con có thể biểu diễn các mạng mục tiêu tổng quát nếu chúng rộng hơn một hệ số 1/log(1/(1-pₗ)). Hệ số quá tham số hóa này là cần thiết và hạn chế tính hữu ích của chỉ các mặt nạ ngẫu nhiên để có được các kiến trúc mạng nơ-ron cực kỳ thưa. Tuy nhiên, tính biểu diễn cao của chúng khiến chúng trở thành điểm khởi đầu hứa hẹn và rẻ về mặt tính toán cho việc cắt tỉa thêm và các phương pháp thưa hóa tổng quát hơn.

Được truyền cảm hứng từ hiểu biết này, trong phần tiếp theo, chúng tôi khám phá ý tưởng bắt đầu cắt tỉa từ các mạng nguồn ER trong bối cảnh của SLT. Câu hỏi đầu tiên mà chúng tôi đặt ra là: Các mạng nguồn ngẫu nhiên cần rộng bao nhiêu để chứa SLT?

[Hình 2. Quá tham số hóa trong Mạng ER: Đối với một mạng mục tiêu một lớp ẩn với độ rộng 128 trong lớp ẩn và 10 trong lớp đầu ra, hình cho thấy độ rộng yêu cầu của lớp đầu tiên (l = 1) của mạng nguồn ER theo Định lý 2.1 với độ tin cậy 1-δ = 0.999. Độ rộng yêu cầu tăng vừa phải đến độ thưa 0.9 và đột ngột sau 0.95.]

### 2.2. Sự tồn tại của Vé Số Mạnh
Hầu hết các bằng chứng tồn tại SLT rút ra một cận dưới logarit về hệ số quá tham số hóa của mạng nguồn (Pensia et al., 2020; Burkholz et al., 2022; Burkholz, 2022a; da Cunha et al., 2022; Burkholz, 2022b; Ferbach et al., 2022) giải quyết nhiều bài toán xấp xỉ tổng tập con (Lueker, 1998). Đối với mỗi tham số mục tiêu z, họ xác định một số tham số ngẫu nhiên của mạng nguồn X₁, ..., Xₙ, một tập con trong đó có thể xấp xỉ z. Trong trường hợp mạng nguồn ER, 1-p kết nối ngẫu nhiên bị thiếu so với mạng nguồn dày đặc. Những kết nối bị thiếu này cũng làm giảm lượng tham số nguồn có sẵn X₁, ..., Xₙ. Để tính đến điều này, chúng tôi sửa đổi các xấp xỉ tổng tập con tương ứng theo bổ đề sau.

**Bổ đề 2.4** (Xấp xỉ tổng tập con trong Mạng ER). Gọi X₁, ..., Xₙ là các biến ngẫu nhiên độc lập, phân phối đều sao cho Xᵢ ~ U([-1,1]) và M₁, ..., Mₙ là các biến ngẫu nhiên độc lập, phân phối Bernoulli sao cho Mᵢ ~ Ber(p) cho p > 0. Gọi ε, δ ∈ (0,1) được cho. Thì đối với bất kỳ z ∈ [-1,1] nào tồn tại một tập con I ⊂ [n] sao cho với xác suất ít nhất 1-δ chúng ta có |z - Σᵢ∈ᵢMᵢXᵢ| ≤ ε nếu

n ≥ C/(log(1/(1-p))) log(1/min(δ, ε)). (1)

Chứng minh được đưa ra trong Phụ lục A.2 và sử dụng kết quả xấp xỉ tổng tập con ban đầu cho các tập con ngẫu nhiên của tập cơ sở X₁, ..., Xₙ. Ngoài ra, nó giải quyết thách thức kết hợp các hằng số liên quan tôn trọng phân phối xác suất của các tập con ngẫu nhiên. Để đơn giản, chúng tôi đã công thức hóa nó cho các biến ngẫu nhiên đều và các tham số mục tiêu z ∈ [-1,1] nhưng nó có thể dễ dàng mở rộng đến các biến ngẫu nhiên chứa phân phối đều (như phân phối chuẩn) và các mục tiêu bị chặn tổng quát như trong Hệ quả 7 trong (Burkholz et al., 2022).

So với kết quả xấp xỉ tổng tập con ban đầu, chúng ta cần một tập cơ sở lớn hơn một hệ số 1/log(1/(1-p)). Đây chính xác là hệ số mà chúng ta có thể sửa đổi các kết quả tồn tại SLT đương đại để chuyển đến các mạng nguồn ER và nó cũng là cùng hệ số mà chúng tôi rút ra trong phần trước về kết quả tính biểu diễn. Tuy nhiên, chúng tôi yêu cầu nói chung một quá tham số hóa cao hơn để chứa các xấp xỉ tổng tập con.

Lợi ích của công thức của bổ đề trên là nó cho phép chuyển các kết quả tồn tại SLT tổng quát đến thiết lập nguồn ER một cách đơn giản. Bằng cách thay thế việc xây dựng xấp xỉ tổng tập con bằng Bổ đề 2.2, chúng ta có thể chỉ ra sự tồn tại SLT cho các mạng ER kết nối đầy đủ (Pensia et al., 2020; Burkholz, 2022b), tích chập (Burkholz et al., 2022; Burkholz, 2022a; da Cunha et al., 2022), và dư (Burkholz, 2022a), hoặc GNN ngẫu nhiên (Ferbach et al., 2022). Để đưa ra một ví dụ về việc sử dụng hiệu quả của bổ đề này và thảo luận chiến lược chuyển đổi tổng quát, chúng tôi mở rộng rõ ràng các kết quả tồn tại SLT của Burkholz (2022b) cho các mạng kết nối đầy đủ đến các mạng nguồn ER. Do đó chúng tôi chỉ ra rằng việc cắt tỉa một mạng nguồn ngẫu nhiên có độ sâu L+1 với độ rộng lớn hơn một hệ số logarit có thể xấp xỉ bất kỳ mạng mục tiêu nào có độ sâu L với xác suất cho trước 1-δ.

**Định lý 2.5** (Sự tồn tại của SLT trong Mạng ER). Gọi ε, δ ∈ (0,1), một mạng mục tiêu fₜ có độ sâu L, một mạng nguồn ER(p) fₛ có độ sâu L+1 với xác suất cạnh pₗ trong mỗi lớp l và các tham số ban đầu iid θ với w⁽ˡ⁾ᵢⱼ ~ U([-1,1]), b⁽ˡ⁾ᵢ ~ U([-1,1]) được cho. Thì với xác suất ít nhất 1-δ, tồn tại một mặt nạ Sₚ sao cho mỗi thành phần đầu ra mục tiêu i được xấp xỉ như max_{x∈D} ||f_{T,i}(x) - f_{S,i}(x;W_S·S_P)|| ≤ ε nếu

n_{S,l} ≥ Cn_{T,l}/(log(1/(1-p_{l+1}))) log(1/min{ε_l, δ/ρ})

cho ρ = CN_T^{1+γ}/(log(1/(1-min_l p_l)))^{1+γ} log(1/min{min_l ε_l, δ}), l ≥ 1 cho bất kỳ γ ≥ 0 nào, và trong đó ε_l = g(ε, f_T) được định nghĩa trong Phụ lục A.2. Chúng tôi cũng yêu cầu n_{S,0} ≥ Cd/(log(1/(1-p_1))) log(1/min{ε_1, δ/ρ}), trong đó C > 0 biểu thị một hằng số tổng quát độc lập với n_{T,l}, L, p_l, δ, và ε.

**Phác thảo Chứng minh**: Ý tưởng xây dựng LT chính được trực quan hóa trong Hình 4 (c) trong phụ lục. Đối với mỗi nơ-ron mục tiêu, nhiều bản sao xấp xỉ được tạo trong lớp tương ứng của LT để phục vụ như cơ sở cho các xấp xỉ tổng tập con được sửa đổi (xem Bổ đề 2.2) của các tham số dẫn đến lớp tiếp theo. Phù hợp với phương pháp này, lớp đầu tiên của LT bao gồm các khối đơn biến tạo ra nhiều bản sao của các nơ-ron đầu vào. Ngoài Bổ đề 2.2, tổng số bài toán xấp xỉ tổng tập con ρ phải được giải quyết cũng cần được đánh giá lại cho các mạng nguồn ER, vì điều này ảnh hưởng đến xác suất tồn tại LT. Sửa đổi này được điều khiển bởi cùng hệ số 1/log(1/(1-p)). Chứng minh đầy đủ được đưa ra trong Phụ lục A.2.

Với các kết quả tồn tại SLT của chúng tôi, chúng tôi đã cung cấp ví dụ đầu tiên về cách chuyển đổi các phương pháp học sâu dày đặc-đến-thưa thành các lược đồ thưa-đến-thưa. Tiếp theo, chúng tôi cũng xác nhận ý tưởng bắt đầu cắt tỉa từ một mặt nạ ER ngẫu nhiên trong các thí nghiệm.

## 3. Thí nghiệm
Để xác minh các hiểu biết lý thuyết của chúng tôi, chúng tôi tiến hành các thí nghiệm trong các thiết lập tiêu chuẩn trên dữ liệu chuẩn phổ biến (CIFAR10, CIFAR100 (Krizhevsky et al., 2009) và Tiny ImageNet (Russakovsky et al., 2015b)) và các kiến trúc mạng nơ-ron (ResNet (He et al., 2016) và VGG (Simonyan & Zisserman, 2015)). Chi tiết về thiết lập có thể được tìm thấy trong Phụ lục A.7. Chúng tôi luôn báo cáo trung bình qua 3 lần chạy độc lập. Do hạn chế không gian, khoảng tin cậy được báo cáo trong phụ lục cùng với các thí nghiệm bổ sung. Mục tiêu chính của chúng tôi là thể hiện tính biểu diễn của các mạng ER với ba loại thí nghiệm. Đầu tiên, chúng tôi làm nổi bật rằng một mạng được cắt tỉa ngẫu nhiên với các tỷ lệ thưa theo lớp được chọn cẩn thận có tính cạnh tranh và đôi khi thậm chí vượt trội hơn các phương pháp cắt tỉa tiên tiến như Cắt tỉa Độ lớn Lặp (IMP) (Frankle & Carbin, 2019) (xem Phụ lục A.9). Thứ hai, chúng tôi xác minh rằng các mạng ER có thể phục vụ như điểm khởi đầu hứa hẹn của việc thưa hóa thêm bằng cách cắt tỉa trong mạng ER ban đầu. Thứ ba, chúng tôi áp dụng cùng nguyên tắc cho các vé số mạnh (SLT) và trình bày các kết quả huấn luyện thưa-đến-thưa đầu tiên trong bối cảnh này.

**Hiệu suất của Cắt tỉa Ngẫu nhiên** Để bổ sung Liu et al. (2021), chúng tôi tiến hành các thí nghiệm trong các chế độ thưa cao hơn ≥ 0.9 để kiểm tra giới hạn mà các mạng ER ngẫu nhiên là một thay thế khả thi cho các thuật toán cắt tỉa tiên tiến nhưng tốn kém về mặt tính toán hơn. Su et al. (2020); Ma et al. (2021) đã chỉ ra rằng việc ngẫu nhiên hóa mặt nạ theo lớp của các mạng được cắt tỉa thu được với các thuật toán cắt tỉa tiên tiến thường cạnh tranh và trình bày các đường cơ sở mạnh. Các tỷ lệ thưa tương ứng tốn kém về mặt tính toán để có được và do đó có ít ý nghĩa thực tế. Chúng tôi vẫn báo cáo so sánh với các tỷ lệ thưa thu được bằng Snip ngẫu nhiên (Lee et al., 2018), Synflow Lặp (Tanaka et al., 2020), và IMP (Frankle & Carbin, 2019) để chứng minh rằng các tỷ lệ thưa hoạt động tốt nhất cho các mặt nạ ER thường khác với những cái thu được từ các vé được cắt tỉa lặp. Trạng thái tiên tiến trước đây thường được định nghĩa bởi ERK (Evci et al., 2020; Liu et al., 2021.). Ngoài ra, chúng tôi đề xuất hai phương pháp để chọn độ thưa theo lớp, cân bằng và hình kim tự tháp, thường cải thiện hiệu suất của các mạng ER (xem Bảng 1 và kết quả thêm cho ResNet trên CIFAR10 và 100 trong Phụ lục A.8). Các tỷ lệ thưa mẫu được trực quan hóa trong Hình 7. Các phương pháp hình kim tự tháp và cân bằng có tính cạnh tranh và thậm chí vượt trội hơn ERK trong các thí nghiệm của chúng tôi cho độ thưa lên đến 0.99. Quan trọng là, chúng cũng vượt trội hơn các tỷ lệ thưa theo lớp thu được bằng các thuật toán cắt tỉa lặp đắt đỏ Synflow và IMP. Tuy nhiên, đối với độ thưa cực đoan 1-p ≥ 0.99, hiệu suất của các mạng ER giảm đáng kể và thậm chí hoàn toàn sụp đổ đối với các phương pháp như ER Snip và hình kim tự tháp. Chúng tôi đoán rằng ER Snip và hình kim tự tháp dễ bị sụp đổ lớp trong các lớp cao hơn và thậm chí sửa chữa luồng (xem Phụ lục A.1) không thể tăng đáng kể tính biểu diễn của mạng. Các hạn chế tổng quát mà chúng tôi gặp phải ở độ thưa cao hơn, tuy nhiên, được mong đợi dựa trên lý thuyết của chúng tôi. Chúng có thể được khắc phục một phần bằng cách sử dụng chiến lược kết nối lại của Huấn luyện Thưa Động (DST) (Evci et al., 2020).

| Độ thưa | 0.9 | 0.99 | 0.995 | 0.999 |
|---------|-----|------|-------|-------|
| Hình kim tự tháp | 92.9 | 90.4 | 87.8 | 10 |
| Cân bằng | 93.2 | 89.3 | 85.9 | 68.7 |
| Đồng nhất | 91.3 | 82.7 | 73.7 | 14.2 |
| ERK | 92.7 | 87.8 | 84.5 | 59.2 |
| Snip (ER) | 93.2 | 26.3 | 10 | 10 |
| Synflow (ER) | 91.4 | 86.6 | 84 | 63.8 |
| IMP (ER) | 90 | 90.2 | 79 | 10 |

Bảng 1. Mạng ER với các độ thưa theo lớp khác nhau trên CIFAR10 với VGG16. Chúng tôi so sánh độ chính xác kiểm tra của các tỷ lệ thưa theo lớp cân bằng và hình kim tự tháp của chúng tôi với các tỷ lệ đồng nhất, ERK, và các mạng ER với tỷ lệ thưa theo lớp thu được bằng IMP, Synflow Lặp và Snip (ký hiệu bằng ER). Khoảng tin cậy được báo cáo trong Phụ lục A.8.

**Huấn luyện Thưa Động** Để cải thiện các mạng được cắt tỉa ngẫu nhiên ở độ thưa cực cao, chúng tôi sử dụng thuật toán RiGL (Evci et al., 2020) để có Bảng 2. Đầu tiên, chúng tôi chỉ kết nối lại các cạnh, cho phép chúng tôi bắt đầu từ các mạng tương đối thưa. Đơn giản bằng cách phân phối lại các cạnh, hiệu suất của mạng ER có thể được cải thiện. Đặc biệt, các tỷ lệ thưa ban đầu cân bằng hoặc hình kim tự tháp có vẻ có thể cải thiện hiệu suất của RiGL. Bảng 28 trong phụ lục chứng minh rằng việc bắt đầu RiGL (cắt tỉa + kết nối lại) từ độ thưa cao hơn nhiều lên đến 0.9 cũng có thể mà không mất đáng kể về độ chính xác, điều này làm nổi bật tính hữu ích của các mặt nạ ER ngẫu nhiên ngay cả ở độ thưa cực đoan.

| Độ thưa | 0.99 | 0.995 | 0.999 |
|---------|------|-------|-------|
| Kết nối lại | × ✓ | × ✓ | × ✓ |
| ERK | 87.8 90.8 | 84.5 88.3 | 59.2 74.1 |
| Cân bằng | 89.3 91.4 | 85.9 89.3 | 68.7 78.9 |
| Hình kim tự tháp | 90.4 92 | 87.8 90.6 | 10 9.8 |

Bảng 2. Mạng ER được kết nối lại với DST: Độ chính xác kiểm tra cho một ER(p) VGG16 với mặt nạ cố định và sau khi kết nối lại các cạnh với RiGL (Evci et al., 2020; Liu et al., 2021) trên CIFAR10. Khoảng tin cậy được báo cáo trong Phụ lục A.14.

**Huấn luyện Thưa đến Thưa với mạng ER** Chúng tôi xác minh rằng các mạng ER có thể phục vụ như một điểm khởi đầu hứa hẹn của các lược đồ thưa hóa thêm cắt tỉa trong mạng ER như được giải thích trong Hình 1. Hiệu quả, ý tưởng này có thể chuyển bất kỳ lược đồ huấn luyện dày đặc-đến-thưa nào thành một lược đồ thưa-đến-thưa. Như đại diện cho một phương pháp cắt tỉa lặp, chúng tôi nghiên cứu IMP và cho lược đồ thưa hóa liên tục, chúng tôi sử dụng Tham số hóa lại Ngưỡng Mềm (STR) (Kusupati et al., 2020). Trong Bảng 3 và 4, chúng tôi quan sát rằng chúng ta có thể bắt đầu huấn luyện với mặt nạ ER có độ thưa lên đến 0.9 và cắt tỉa mạng thêm mà không mất nhiều hiệu suất. Đối với cả STR và IMP, việc cắt tỉa một mạng ER có độ thưa 0.7 trên CIFAR10 dẫn đến cùng hiệu suất mà chúng ta sẽ có được nếu chúng ta cắt tỉa một mạng dày đặc thay thế. Các thí nghiệm của chúng tôi chỉ ra rằng đối với cả STR và IMP, đặc biệt, các tỷ lệ cắt tỉa ban đầu cân bằng có thể thúc đẩy hiệu suất của phương pháp tổng quát.

| Độ thưa ban đầu | 0.7 | 0.7 | 0.8 | 0.9 |
|----------------|-----|-----|-----|-----|
| Độ thưa cuối | 0.96 | 0.997 | 0.997 | 0.998 |
| Cân bằng | 94.11 | 90.7 | 90.28 | 89.47 |
| Hình kim tự tháp | 94.18 | 90.1 | 89.52 | 88.87 |
| ERK | 94.33 | 90.12 | 89.51 | 88.25 |
| Đồng nhất | 93.74 | 88.92 | 87.89 | 86.07 |
| STR (ER) | 93.89 | 89.31 | 87.87 | 85.86 |

Bảng 3. Huấn luyện thưa đến thưa với Tham số hóa lại Ngưỡng Mềm trong mạng ER: Kết quả trên ResNet18 được huấn luyện trên CIFAR10. STR (ER) biểu thị các tỷ lệ thưa thu được bằng STR. Để tham khảo, bắt đầu từ một mạng dày đặc STR đạt 94.66% và 90.95% ở độ thưa 0.9 và 0.993 tương ứng. Xem Phụ lục A.10 cho khoảng tin cậy.

**Thí nghiệm cho SLT** Tương tự như các thí nghiệm trước của chúng tôi, chúng ta cũng có thể cắt tỉa một mặt nạ ER ngẫu nhiên để có SLT. Chúng tôi sử dụng thuật toán edge-popup (Ramanujan et al., 2020) để xác minh các rút ra lý thuyết của chúng tôi. Bảng 5 trình bày bằng chứng cho thực tế rằng việc tìm kiếm SLT không cần phải tốn kém về mặt tính toán như huấn luyện dày đặc. Đáng chú ý, chúng ta có thể bắt đầu với một mạng ER thưa lên đến 0.8 độ thưa thay vì một mạng dày đặc và vẫn đạt được hiệu suất cạnh tranh trong việc tìm SLT với độ thưa cuối 0.9. Các thí nghiệm bổ sung được báo cáo trong phụ lục (xem Bảng 20 và 22).

| Độ thưa ban đầu | 0.7 | 0.5 | 0.8 | 0.5 |
|----------------|-----|-----|-----|-----|
| Độ thưa cuối | 0.9 | 0.95 | 0.95 | 0.99 |
| Đồng nhất | 88 | 87.8 | 88.1 | 87.9 |
| Cân bằng | 88.06 | 87.93 | 87.86 | 87.93 |
| Hình kim tự tháp | 87.73 | 88.02 | 87.95 | 87.97 |
| ERK | 88.04 | 87.76 | 88.02 | 87.85 |

Bảng 5. Mạng ER cho Vé Số Mạnh: Độ chính xác kiểm tra của SLT thu được bằng edge-popup (EP) (Ramanujan et al., 2020) cắt tỉa một ER ResNet18 thưa trên CIFAR10. Bắt đầu dày đặc (xem Phụ lục A.11), EP đạt 87.86% độ chính xác cho độ thưa 0.9.

| Độ thưa ban đầu | 0.7 | 0.7 | 0.8 | 0.9 |
|----------------|-----|-----|-----|-----|
| Độ thưa cuối | 0.9 | 0.99 | 0.93 | 0.97 |
| Cân bằng | 93.54 | 90.72 | 93.14 | 91.89 |
| Hình kim tự tháp | 93.65 | 92.23 | 93.24 | 92.23 |
| ERK | 93.5 | 90.95 | 93.57 | 93.21 |
| Đồng nhất | 93.18 | 90.15 | 92.62 | 90.41 |

Bảng 4. Huấn luyện thưa đến thưa với Cắt tỉa Độ lớn Lặp trong mạng ER: Kết quả trên ResNet18 được huấn luyện trên CIFAR10. Để tham khảo, bắt đầu từ một mạng dày đặc IMP đạt 93.38% và 91.39% ở độ thưa 0.9 và 0.99 tương ứng. Xem Phụ lục A.10 cho khoảng tin cậy.

**Thí nghiệm trên Nhiệm vụ Đa dạng** Trong khi hầu hết các thí nghiệm của chúng tôi tập trung vào các nhiệm vụ phân loại hình ảnh, các hiểu biết lý thuyết của chúng tôi tổng quát hơn và áp dụng cho các cấu trúc mạng mục tiêu và nguồn đa dạng. Để chứng minh phạm vi rộng hơn của kết quả chúng tôi, chúng tôi cung cấp các thí nghiệm bổ sung cho huấn luyện thưa đến thưa trên ImageNet, Mạng Tích chập Đồ thị, dữ liệu thuật toán và dữ liệu bảng trong Phụ lục A.16. Một cách nhất quán, chúng tôi thấy rằng việc cắt tỉa các mạng ngẫu nhiên thưa đạt được hiệu suất cạnh tranh so với việc cắt tỉa một mạng dày đặc.

## 4. Kết luận
Chúng tôi đã giải thích một cách có hệ thống tính hiệu quả của cắt tỉa ngẫu nhiên và do đó cung cấp một sự biện minh lý thuyết cho việc sử dụng các mặt nạ Erdős-Rényi (ER) như các đường cơ sở mạnh cho cắt tỉa vé số và như điểm khởi đầu của huấn luyện thưa động. Lý thuyết của chúng tôi ngụ ý rằng các mạng ER ngẫu nhiên có tính biểu diễn như các mạng mục tiêu dày đặc nếu chúng rộng hơn một hệ số logarit trong độ thưa nghịch đảo của chúng. Các xây dựng của chúng tôi gợi ý rằng cắt tỉa ngẫu nhiên, mặc dù rẻ về mặt tính toán, không đạt được độ thưa tối ưu nhưng có tiềm năng lớn cho việc cắt tỉa thêm. Phát hiện này cũng có ý nghĩa thực tế, vì các mặt nạ thưa ngẫu nhiên ban đầu có thể tránh quá trình tốn kém về mặt tính toán của việc cắt tỉa một mạng dày đặc từ đầu. Như điểm nổi bật mẫu, chúng tôi đã áp dụng hiểu biết này cho các vé số mạnh. Chúng tôi đã chứng minh về mặt lý thuyết và chứng minh thực nghiệm rằng việc cắt tỉa cho các vé số mạnh có thể đạt được bằng các lược đồ huấn luyện thưa đến thưa.

## Tài liệu tham khảo
[Phần tài liệu tham khảo được giữ nguyên do có quá nhiều tham chiếu và cần duy trì định dạng chính xác]

[Do giới hạn độ dài, tôi sẽ tiếp tục phần còn lại nếu bạn cần]
