Cắt tỉa hay không cắt tỉa: khám phá hiệu quả của việc cắt tỉa cho nén mô hình

Tóm tắt
Việc cắt tỉa mô hình tìm cách tạo ra độ thưa thớt trong các ma trận kết nối khác nhau của mạng nơ-ron sâu, từ đó giảm số lượng tham số có giá trị khác không trong mô hình. Các báo cáo gần đây (Han et al., 2015a; Narang et al., 2017) cắt tỉa mạng sâu với chi phí chỉ là sự suy giảm nhỏ về độ chính xác và đạt được sự giảm đáng kể về kích thước mô hình. Điều này gợi ý khả năng rằng các mô hình cơ sở trong những thí nghiệm này có lẽ bị tham số hóa quá mức nghiêm trọng ngay từ đầu và một giải pháp thay thế khả thi cho việc nén mô hình có thể là đơn giản là giảm số lượng đơn vị ẩn trong khi duy trì cấu trúc kết nối dày đặc của mô hình, để lộ sự đánh đổi tương tự về kích thước mô hình và độ chính xác. Chúng tôi điều tra hai con đường riêng biệt này cho việc nén mô hình trong bối cảnh suy luận tiết kiệm năng lượng trong môi trường hạn chế tài nguyên và đề xuất một kỹ thuật cắt tỉa từ từ mới đơn giản và dễ dàng áp dụng trên nhiều loại mô hình/tập dữ liệu với việc điều chỉnh tối thiểu và có thể được tích hợp liền mạch trong quá trình huấn luyện. Chúng tôi so sánh độ chính xác của các mô hình lớn nhưng đã cắt tỉa (lớn-thưa) và các đối tác nhỏ hơn nhưng dày đặc (nhỏ-dày đặc) của chúng với cùng dung lượng bộ nhớ. Trên một phạm vi rộng các kiến trúc mạng nơ-ron (CNN sâu, LSTM xếp chồng, và mô hình LSTM seq2seq), chúng tôi thấy các mô hình lớn-thưa liên tục vượt trội so với các mô hình nhỏ-dày đặc và đạt được sự giảm lên đến 10x về số lượng tham số khác không với sự mất mát tối thiểu về độ chính xác.

1 Giới thiệu
Trong vài năm qua, mạng nơ-ron sâu đã đạt được hiệu suất tối tân trên một số nhiệm vụ thách thức trong các lĩnh vực thị giác máy tính, nhận dạng giọng nói và xử lý ngôn ngữ tự nhiên. Được thúc đẩy bởi lượng dữ liệu và sức mạnh tính toán ngày càng tăng, các mô hình học sâu đã trở nên lớn hơn và sâu hơn để học tốt hơn từ dữ liệu. Trong khi những mô hình này thường được triển khai trong back-end trung tâm dữ liệu, việc bảo vệ quyền riêng tư người dùng và giảm thời gian truy vấn mà người dùng cảm nhận đòi hỏi việc di chuyển trí thông minh được cung cấp bởi những mạng nơ-ron sâu này về phía các thiết bị tính toán biên.

Triển khai các mô hình học sâu lớn và chính xác lên các môi trường tính toán hạn chế tài nguyên như điện thoại di động, camera thông minh v.v. để suy luận trên thiết bị đặt ra một vài thách thức chính. Thứ nhất, các mô hình học sâu tối tân thường có hàng triệu tham số đòi hỏi lưu trữ O(MB), trong khi bộ nhớ trên thiết bị bị hạn chế. Hơn nữa, việc thậm chí chỉ một lần suy luận mô hình duy nhất cũng có thể gọi O(10^9) lần truy cập bộ nhớ và phép toán số học, tất cả đều tiêu thụ năng lượng và tỏa nhiệt có thể làm cạn kiệt dung lượng pin hạn chế và/hoặc thử thách giới hạn nhiệt của thiết bị.

Đối mặt với những thách thức này, một cơ quan nghiên cứu ngày càng lớn đã xuất hiện với ý định khám phá các phương pháp nén mô hình mạng nơ-ron trong khi hạn chế bất kỳ tổn thất tiềm tàng nào về chất lượng mô hình. Các khối lượng công việc nhạy cảm với độ trễ dựa vào suy luận mạng nơ-ron tiết kiệm năng lượng trên thiết bị thường bị ràng buộc băng thông bộ nhớ, và việc nén mô hình mang lại lợi ích kép là giảm tổng số lần truy cập bộ nhớ tiêu thụ năng lượng cũng như cải thiện thời gian suy luận do băng thông bộ nhớ hiệu quả cao hơn để tìm nạp các tham số mô hình được nén. Trong lĩnh vực các kỹ thuật nén mô hình, việc cắt tỉa bỏ (ép về không) các kết nối ít nổi bật hơn (tham số) trong mạng nơ-ron đã được chứng minh là giảm số lượng tham số khác không trong mô hình với ít hoặc không có sự mất mát trong chất lượng mô hình cuối cùng. Việc cắt tỉa mô hình cho phép đánh đổi một sự suy giảm nhỏ về chất lượng mô hình để giảm kích thước mô hình, có khả năng thu được cải thiện về thời gian suy luận và hiệu quả năng lượng. Mô hình được cắt tỉa kết quả thường có các ma trận kết nối thưa thớt, vì vậy việc suy luận hiệu quả sử dụng những mô hình thưa này đòi hỏi phần cứng được xây dựng có mục đích có khả năng tải các ma trận thưa và/hoặc thực hiện các phép toán ma trận-vector thưa (Zhang et al., 2016; Han et al., 2016; Parashar et al., 2017). Ngoài ra, việc biểu diễn các ma trận thưa mang theo một chi phí lưu trữ bổ sung làm tăng dung lượng bộ nhớ ròng của mô hình cũng phải được xem xét.

Trong công trình này, chúng tôi tìm cách thực hiện một khảo sát gần gũi hơn về hiệu quả của việc cắt tỉa mô hình như một phương tiện nén mô hình. Từ góc độ suy luận mạng nơ-ron trên thiết bị, cho một giới hạn về dung lượng bộ nhớ của mô hình, làm thế nào chúng ta có thể đến được mô hình chính xác nhất? Chúng tôi cố gắng trả lời câu hỏi này bằng cách so sánh chất lượng của các mô hình thu được thông qua hai phương pháp riêng biệt: (1) huấn luyện một mô hình lớn, nhưng được cắt tỉa để có được một mô hình thưa với số lượng nhỏ tham số khác không (lớn-thưa); và (2) huấn luyện một mô hình nhỏ-dày đặc với kích thước có thể so sánh với mô hình lớn-thưa. Cả hai phương pháp này đều để lộ sự đánh đổi giữa độ chính xác và kích thước mô hình, nhưng khác nhau đáng kể về mặt ý nghĩa của chúng đối với thiết kế kiến trúc phần cứng cơ bản. Đối với nghiên cứu so sánh này, chúng tôi chọn các mô hình trên một tập hợp đa dạng các lĩnh vực ứng dụng: InceptionV3 (Szegedy et al., 2016) và MobileNets (Howard et al., 2017) cho các nhiệm vụ nhận dạng hình ảnh, LSTM xếp chồng cho mô hình hóa ngôn ngữ, và các mô hình seq2seq được sử dụng trong hệ thống Dịch máy Nơ-ron của Google (Wu et al., 2016). Trong quá trình điều tra này, chúng tôi cũng phát triển một phương pháp cắt tỉa từ từ đơn giản đòi hỏi việc điều chỉnh tối thiểu và có thể được tích hợp liền mạch trong quá trình huấn luyện và chứng minh khả năng ứng dụng và hiệu suất của nó trên một loạt các kiến trúc mạng nơ-ron.

2 Công trình liên quan
Các công trình đầu trong những năm 1990 (LeCun et al., 1990; Hassibi et al., 1993) đã thực hiện cắt tỉa sử dụng phép xấp xỉ Taylor bậc hai của sự gia tăng trong hàm mất mát của mạng khi một trọng số được đặt về không. Trong Optimal Brain Damage (LeCun et al., 1990), độ nổi bật cho mỗi tham số được tính toán sử dụng phép xấp xỉ Hessian đường chéo, và các tham số có độ nổi bật thấp được cắt tỉa khỏi mạng và mạng được huấn luyện lại. Trong Optimal Brain Surgeon (Hassibi et al., 1993), độ nổi bật cho mỗi tham số được tính toán sử dụng ma trận Hessian nghịch đảo, và các trọng số có độ nổi bật thấp được cắt tỉa và tất cả các trọng số khác trong mạng được cập nhật sử dụng thông tin bậc hai.

Gần đây hơn, các phương pháp cắt tỉa trọng số dựa trên độ lớn đã trở thành các kỹ thuật phổ biến cho việc cắt tỉa mạng (Han et al., 2015b,a; See et al., 2016; Narang et al., 2017). Các kỹ thuật cắt tỉa trọng số dựa trên độ lớn có hiệu quả tính toán, mở rộng cho các mạng và tập dữ liệu lớn. Thuật toán cắt tỉa từ từ tự động của chúng tôi cắt tỉa các trọng số có độ lớn nhỏ nhất để đạt được mức độ thưa thớt mạng đã được đặt trước. Ngược lại với các công trình được liệt kê ở trên, bài báo của chúng tôi tập trung vào việc so sánh sự đánh đổi giữa độ chính xác và kích thước mô hình của các mô hình lớn-thưa so với nhỏ-dày đặc.

Một công trình tương tự như của chúng tôi là công trình của Narang et al. (2017) về việc cắt tỉa một mô hình RNN và GRU cho nhận dạng giọng nói và cho thấy rằng một RNN thưa đã được cắt tỉa vượt trội so với một RNN dày đặc được huấn luyện bình thường có kích thước tương đương. Trong khi họ cung cấp một điểm dữ liệu so sánh hiệu suất của mô hình thưa so với dày đặc, công trình của chúng tôi thực hiện một so sánh mở rộng về các mô hình thưa so với dày đặc trên một phạm vi rộng các mô hình trong các lĩnh vực khác nhau (thị giác và NLP). Narang et al. cũng giới thiệu một lược đồ cắt tỉa từ từ dựa trên việc cắt tỉa tất cả các trọng số trong một lớp nhỏ hơn một ngưỡng nào đó (được chọn thủ công) có tính tuyến tính với một độ dốc trong giai đoạn 1 và tuyến tính với một độ dốc trong giai đoạn 2 sau đó là huấn luyện bình thường. So với phương pháp của họ, chúng tôi không có hai giai đoạn và không phải chọn hai độ dốc, và chúng tôi không cần chọn ngưỡng trọng số cho mỗi lớp (chúng tôi dựa vào một lịch trình độ thưa xác định ngưỡng trọng số). Vì vậy, kỹ thuật của chúng tôi đơn giản hơn, không đòi hỏi nhiều việc điều chỉnh siêu tham số, và được chứng minh là hoạt động tốt trên các mô hình khác nhau.

Trong bối cảnh giảm kích thước mô hình bằng cách loại bỏ các kết nối dư thừa, một số công trình gần đây (Anwar et al., 2015; Lebedev and Lempitsky, 2015; Li et al., 2016; Changpinyo et al., 2017) đề xuất các kỹ thuật để cắt tỉa và tạo ra độ thưa thớt theo cách có cấu trúc, được thúc đẩy chủ yếu bởi mong muốn tăng tốc tính toán trên các kiến trúc phần cứng hiện có được tối ưu hóa cho đại số tuyến tính dày đặc. Những kỹ thuật như vậy thực hiện cắt tỉa hạt thô và phụ thuộc nghiêm trọng vào cấu trúc của các lớp tích chập, và có thể không thể mở rộng trực tiếp cho các kiến trúc mạng nơ-ron khác thiếu những tính chất cấu trúc như vậy (ví dụ LSTM). Ngược lại, phương pháp của chúng tôi không đưa ra bất kỳ giả định nào về cấu trúc của mạng hoặc các lớp thành phần của nó và do đó có thể áp dụng tổng quát hơn.

Trong khi cắt tỉa tập trung vào việc giảm số lượng tham số khác không, về nguyên tắc, việc cắt tỉa mô hình có thể được sử dụng kết hợp với các kỹ thuật khác để giảm thêm kích thước mô hình. Các kỹ thuật lượng tử hóa nhằm giảm số bit cần thiết để biểu diễn mỗi tham số từ số thực 32-bit xuống 8 bit hoặc ít hơn. Các kỹ thuật lượng tử hóa khác nhau như lượng tử hóa điểm cố định (Vanhoucke et al., 2011) hoặc lượng tử hóa vector (Gong et al., 2014) đạt được các tỷ lệ nén và độ chính xác khác nhau nhưng cũng đòi hỏi phần mềm hoặc phần cứng khác nhau để hỗ trợ suy luận tại thời gian chạy. Cắt tỉa có thể được kết hợp với lượng tử hóa để đạt được nén tối đa (Han et al., 2015a). Ngoài ra, một lĩnh vực nghiên cứu mới nổi là các mạng độ chính xác thấp nơi các tham số và/hoặc kích hoạt được lượng tử hóa xuống 4 bit hoặc ít hơn (Courbariaux et al., 2015; Lin et al., 2015; Hubara et al., 2016; Rastegari et al., 2016; Zhu et al., 2016). Bên cạnh lượng tử hóa, các phương pháp có khả năng bổ sung khác để giảm kích thước mô hình bao gồm phân tích nhân tử ma trận hạng thấp (Denil et al., 2013; Denton et al., 2014; Jaderberg et al., 2014; Lebedev et al., 2014) và chính quy hóa độ thưa nhóm để đến kích thước lớp tối ưu (Alvarez and Salzmann, 2016).

3 Phương pháp
Chúng tôi mở rộng framework TensorFlow (Abadi et al., 2015) để cắt tỉa các kết nối của mạng trong quá trình huấn luyện. Đối với mỗi lớp được chọn để cắt tỉa, chúng tôi thêm một biến mặt nạ nhị phân có cùng kích thước và hình dạng với tensor trọng số của lớp đó và xác định trọng số nào tham gia vào việc thực thi tiến của đồ thị. Chúng tôi chèn các ops vào đồ thị huấn luyện TensorFlow để sắp xếp các trọng số trong lớp đó theo giá trị tuyệt đối của chúng và che về không các trọng số có độ lớn nhỏ nhất cho đến khi đạt được một mức độ thưa s mong muốn. Các gradient lan truyền ngược chảy qua các mặt nạ nhị phân, và các trọng số đã được che trong việc thực thi tiến không được cập nhật trong bước lan truyền ngược.

Chúng tôi giới thiệu một thuật toán cắt tỉa từ từ tự động mới trong đó độ thưa được tăng từ một giá trị độ thưa ban đầu s_i (thường là 0) đến một giá trị độ thưa cuối cùng s_f trong khoảng n bước cắt tỉa, bắt đầu tại bước huấn luyện t_0 và với tần suất cắt tỉa Δt:

s_t = s_f + (s_i - s_f) * (1 - (t - t_0)/(n * Δt))^3

cho t ∈ {t_0, t_0 + Δt, ..., t_0 + nΔt}  (1)

Các mặt nạ trọng số nhị phân được cập nhật mỗi Δt bước khi mạng được huấn luyện để tăng dần độ thưa của mạng trong khi cho phép các bước huấn luyện mạng phục hồi từ bất kỳ mất mát độ chính xác nào do cắt tỉa gây ra. Trong kinh nghiệm của chúng tôi, việc thay đổi tần suất cắt tỉa Δt giữa 100 và 1000 bước huấn luyện có tác động không đáng kể đến chất lượng mô hình cuối cùng. Một khi mô hình đạt được độ thưa mục tiêu s_f, các mặt nạ trọng số không còn được cập nhật nữa. Trực giác đằng sau hàm độ thưa này trong phương trình (1) là cắt tỉa mạng nhanh chóng trong giai đoạn ban đầu khi các kết nối dư thừa dồi dào và giảm dần số lượng trọng số được cắt tỉa mỗi lần vì có ngày càng ít trọng số còn lại trong mạng, như được minh họa trong Hình 1. Trong các kết quả thí nghiệm được trình bày trong bài báo này, việc cắt tỉa được khởi tạo sau khi mô hình đã được huấn luyện trong vài epoch hoặc từ một mô hình đã được huấn luyện trước. Điều này xác định giá trị cho siêu tham số t_0. Một lựa chọn phù hợp cho n phụ thuộc lớn vào lịch trình tỷ lệ học. Stochastic gradient descent (và nhiều biến thể của nó) thường giảm tỷ lệ học trong quá trình huấn luyện, và chúng tôi đã quan sát thấy rằng việc cắt tỉa khi có tỷ lệ học quá nhỏ làm cho các bước huấn luyện tiếp theo khó phục hồi từ sự mất mát độ chính xác do ép các trọng số về không. Đồng thời, cắt tỉa với tỷ lệ học quá cao có thể có nghĩa là cắt tỉa trọng số khi các trọng số chưa hội tụ đến một giải pháp tốt, vì vậy việc chọn lịch trình cắt tỉa gần gũi với lịch trình tỷ lệ học là quan trọng.

Hình 2a cho thấy tỷ lệ học và lịch trình cắt tỉa được sử dụng để huấn luyện các mô hình sparse-InceptionV3 (Szegedy et al., 2016). Tất cả các lớp tích chập trong mô hình này được cắt tỉa sử dụng cùng hàm độ thưa, và việc cắt tỉa xảy ra trong chế độ nơi tỷ lệ học vẫn còn khá cao để cho phép mạng phục hồi từ thiệt hại do cắt tỉa gây ra. Hình 2b đưa ra thêm hiểu biết về cách lược đồ cắt tỉa này tương tác với quy trình huấn luyện. Đối với mô hình thưa 87.5%, với sự gia tăng dần về độ thưa, có một điểm khi mô hình gặp phải sự suy giảm gần như thảm khốc, nhưng phục hồi gần như nhanh chóng với việc tiếp tục huấn luyện. Hành vi này rõ rệt hơn trong các mô hình được huấn luyện để có độ thưa cao hơn. Bảng 1 so sánh hiệu suất của các mô hình sparse-InceptionV3 được cắt tỉa ở mức độ khác nhau. Như mong đợi, có sự suy giảm dần về chất lượng mô hình khi độ thưa tăng. Tuy nhiên, mô hình thưa 50% hoạt động tốt như mô hình cơ sở (độ thưa 0%), và chỉ có giảm 2% về độ chính xác phân loại top-5 cho mô hình thưa 87.5% mang lại sự giảm 8x về số lượng tham số mô hình khác không (NNZ). Cũng lưu ý rằng vì các trọng số được khởi tạo ngẫu nhiên, độ thưa trong các tensor trọng số không thể hiện bất kỳ cấu trúc cụ thể nào. Hơn nữa, phương pháp cắt tỉa được mô tả ở đây không phụ thuộc vào bất kỳ tính chất cụ thể nào của mạng hoặc các lớp thành phần, và có thể được mở rộng trực tiếp cho một phạm vi rộng các kiến trúc mạng nơ-ron.

4 So sánh các mô hình lớn-thưa và nhỏ-dày đặc

4.1 MobileNets
MobileNets là một lớp mạng nơ-ron tích chập hiệu quả được thiết kế đặc biệt cho các ứng dụng thị giác di động (Howard et al., 2017). Thay vì sử dụng tích chập tiêu chuẩn, MobileNets dựa trên một dạng tích chập được phân tích nhân tử gọi là tích chập phân tách theo chiều sâu. Tích chập phân tách theo chiều sâu bao gồm một tích chập theo chiều sâu theo sau bởi một tích chập 1x1 gọi là tích chập điểm. Việc phân tích nhân tử này làm giảm đáng kể số lượng tham số trong mô hình bằng cách lọc và kết hợp các kênh đầu vào trong hai bước riêng biệt thay vì cùng nhau như trong tích chập tiêu chuẩn. Kiến trúc MobileNet bao gồm một lớp tích chập tiêu chuẩn tác động lên hình ảnh đầu vào, một ngăn xếp các tích chập phân tách theo chiều sâu, và cuối cùng là pooling trung bình và các lớp kết nối đầy đủ. 99% tham số trong MobileNet dày đặc 1.0 nằm trong các lớp tích chập điểm 1x1 (74.6%) và các lớp kết nối đầy đủ (24.3%). Chúng tôi không cắt tỉa các tham số trong một lớp tích chập tiêu chuẩn và trong các lớp tích chập theo chiều sâu vì có rất ít tham số trong những lớp này (1.1% của tổng số tham số).

Hệ số nhân độ rộng là một tham số của mạng MobileNet cho phép đánh đổi độ chính xác của mô hình với số lượng tham số và chi phí tính toán. Hệ số nhân độ rộng của mô hình cơ sở là 1.0. Đối với một hệ số nhân độ rộng α ∈ (0,1], số kênh đầu vào và số kênh đầu ra trong mỗi lớp được chia tỷ lệ theo α so với mô hình cơ sở 1.0. Chúng tôi so sánh hiệu suất của MobileNets dày đặc được huấn luyện với hệ số nhân độ rộng 0.75, 0.5, và 0.25 với hiệu suất của MobileNets thưa được cắt tỉa từ MobileNet dày đặc 1.0 trong Hình 3a và Bảng 2 trên tập dữ liệu ImageNet. Chúng ta thấy rằng đối với một số lượng tham số khác không cho trước, MobileNets thưa có thể vượt trội so với MobileNets dày đặc. Ví dụ, mô hình thưa 75% (có 1.09 triệu tham số và độ chính xác top-1 là 67.7%) vượt trội so với MobileNet dày đặc 0.5 (có 1.32 triệu tham số và độ chính xác top-1 là 63.7%) 4% về độ chính xác top-1 trong khi nhỏ hơn. Tương tự, mô hình thưa 90% (có 0.46 triệu tham số và độ chính xác top-1 là 61.8%) vượt trội so với MobileNet dày đặc 0.25 (có 0.46 triệu tham số và độ chính xác top-1 là 50.6%) 10.2% về độ chính xác top-1 trong khi có cùng số lượng tham số khác không.

Nhìn chung, cắt tỉa là một phương pháp đầy hứa hẹn cho việc nén mô hình ngay cả đối với một kiến trúc được thiết kế để nhỏ gọn và hiệu quả bằng cách sử dụng tích chập phân tách theo chiều sâu thay vì tích chập tiêu chuẩn như một kỹ thuật giống phân tích nhân tử để giảm số lượng tham số. Tham số độ thưa được chứng minh là một cách hiệu quả để đánh đổi độ chính xác của mô hình với việc sử dụng bộ nhớ của nó và so sánh thuận lợi với hệ số nhân độ rộng cho MobileNet. Huấn luyện một MobileNet thưa sử dụng thuật toán cắt tỉa từ từ của chúng tôi cũng dễ dàng. Để cắt tỉa, chúng tôi sử dụng cùng lịch trình tỷ lệ học như huấn luyện MobileNet dày đặc nhưng với tỷ lệ học ban đầu nhỏ hơn 10 lần so với huấn luyện MobileNet dày đặc, và tất cả các siêu tham số khác được giữ nguyên.

4.2 Mô hình ngôn ngữ Penn Tree Bank (PTB)
Chúng tôi huấn luyện một mô hình ngôn ngữ LSTM trên tập dữ liệu Penn Tree Bank sử dụng các mô hình và quy trình huấn luyện được mô tả trong Zaremba et al. (2014). Tại mỗi bước thời gian, mô hình ngôn ngữ LSTM xuất ra xác suất của từ tiếp theo trong câu cho trước lịch sử của các từ trước đó. Hàm mất mát là log âm trung bình của các từ mục tiêu, và perplexity là exponential của hàm mất mát. Mô hình ngôn ngữ bao gồm một lớp embedding, 2 lớp LSTM, và một lớp softmax. Kích thước từ vựng là 10,000, và kích thước lớp ẩn LSTM là 200 cho mô hình nhỏ, 650 cho mô hình trung bình, và 1,500 cho mô hình lớn. Trong trường hợp mô hình lớn, có 15M tham số trong lớp embedding, 18M tham số trong mỗi hai lớp LSTM, và 15M tham số trong lớp softmax với tổng cộng 66M tham số. Các siêu tham số khác nhau được sử dụng để huấn luyện các mô hình có kích thước khác nhau. Khi cắt tỉa một mô hình có kích thước nhất định, chúng tôi sử dụng các siêu tham số giống như đã được sử dụng để huấn luyện mô hình dày đặc có kích thước đó. Chúng tôi so sánh hiệu suất của các mô hình dày đặc với các mô hình thưa được cắt tỉa từ trung bình và lớn đến độ thưa 80%, 85%, 90%, 95%, và 97.5% trong Hình 3b và Bảng 3. Trong trường hợp này, chúng ta thấy rằng các mô hình thưa có thể vượt trội so với các mô hình dày đặc có số lượng tham số nhiều hơn đáng kể (lưu ý thang log cho số lượng tham số). Mô hình lớn thưa 90% (có 6.6 triệu tham số và perplexity 80.24) có thể vượt trội so với mô hình trung bình dày đặc (có 19.8 triệu tham số và perplexity 83.37), một mô hình có gấp 3 lần tham số. So với MobileNet, việc cắt tỉa mô hình PTB có khả năng cho kết quả tốt hơn vì mô hình PTB lớn hơn với số lượng tham số nhiều hơn đáng kể. Kết quả của chúng tôi cho thấy rằng việc cắt tỉa hoạt động rất tốt không chỉ trên các trọng số LSTM dày đặc và lớp softmax dày đặc mà còn cả ma trận embedding dày đặc. Điều này gợi ý rằng trong quá trình tối ưu hóa, mạng nơ-ron có thể tìm thấy một embedding thưa tốt cho các từ trong từ vựng hoạt động tốt cùng với cấu trúc kết nối thưa của trọng số LSTM và lớp softmax.

Từ Hình 3b và Bảng 3, chúng ta cũng thấy rằng mô hình trung bình thưa 85% (có 3 triệu tham số và perplexity 85.17) vượt trội so với mô hình lớn thưa 95% (có 3.3 triệu tham số và perplexity 87.83). Độ chính xác của mô hình lớn thưa 95% có thể so sánh với độ chính xác của mô hình trung bình thưa 90% (có 2 triệu tham số và perplexity 87.86). Cùng nhau, những kết quả này gợi ý rằng có một phạm vi nén tối ưu khi cắt tỉa. Trong trường hợp PTB, việc cắt tỉa đến độ thưa 95% cho tỷ lệ nén 20x làm giảm đáng kể hiệu suất của mô hình thưa so với việc cắt tỉa đến độ thưa 90% cho tỷ lệ nén 10x, như thấy trong Hình 3b từ đường cong perplexity so với số lượng tham số được vẽ bởi một trong hai mô hình thưa. Những kết quả này gợi ý rằng để có được mô hình thưa hoạt động tốt nhất của một kích thước nhất định, chúng ta nên huấn luyện một mô hình dày đặc lớn hơn 5x-10x và sau đó cắt tỉa đến số lượng tham số mong muốn thay vì lấy mô hình dày đặc lớn nhất và hoạt động tốt nhất và cắt tỉa mô hình này 20x hoặc hơn đến số lượng tham số mong muốn, giả sử rằng sự khác biệt về hiệu suất của hai mô hình dày đặc cơ sở không lớn lắm. Chúng tôi lưu ý rằng có thể có được kết quả tốt hơn một chút để cắt tỉa đến độ thưa 95% hoặc cao hơn với việc điều chỉnh siêu tham số nhiều hơn, và kết quả chúng tôi có được để cắt tỉa một mô hình có kích thước nhất định là từ việc sử dụng chính xác cùng cấu hình siêu tham số như huấn luyện mô hình dày đặc có kích thước đó.

4.3 Dịch máy Nơ-ron Google
Chúng tôi huấn luyện một mô hình LSTM sâu cho dịch máy sử dụng implementation TensorFlow mã nguồn mở có sẵn tại Luong et al. (2017). Implementation này dựa trên kiến trúc Dịch máy Nơ-ron Google (Wu et al., 2016). Mô hình là một kiến trúc encoder-decoder. Encoder có một lớp embedding ánh xạ từ vựng nguồn 36,548 từ vào không gian k-chiều, 1 lớp LSTM hai chiều, và 3 lớp LSTM tiêu chuẩn. Decoder có một lớp embedding ánh xạ từ vựng đích 36,548 từ vào không gian k-chiều, 4 lớp LSTM với attention, và cuối cùng là một lớp softmax. Đối với mô hình cơ sở dày đặc với số đơn vị k=1024, có 37.4M tham số trong mỗi encoder embedding, decoder embedding, và lớp softmax và 98.6M tham số trong tất cả các lớp LSTM với tổng cộng 211M tham số. Vì có tương đối ít tham số attention, chúng tôi không cắt tỉa các tham số attention. Chúng tôi sử dụng tập dữ liệu German và English WMT16 với news-test2013 làm tập dev và news-test2015 làm tập test. Điểm BLEU được báo cáo như một thước đo chất lượng dịch thuật. Lịch trình tỷ lệ học được sử dụng để huấn luyện là 170K iteration với tỷ lệ học ban đầu 1.0 và 170K iteration với giảm tỷ lệ học 0.5 mỗi 17K iteration. Để cắt tỉa, lịch trình tỷ lệ học chúng tôi sử dụng là 70K iteration với tỷ lệ học ban đầu 0.5 và 170K iteration với giảm tỷ lệ học 0.5 mỗi 17K iteration, và tất cả các siêu tham số khác được giữ nguyên.

Vì chúng tôi nhận thấy rằng quy trình huấn luyện NMT có phương sai cao, chúng tôi đã thử nghiệm một số lược đồ cắt tỉa được áp dụng cho NMT. Implementation tiêu chuẩn của chúng tôi về cắt tỉa từ từ tăng độ thưa của mỗi lớp đến cùng mức độ thưa tại mỗi bước cắt tỉa. Chúng tôi đã thử nghiệm một biến thể mà chúng tôi gọi là độ thưa "layerwise constant": thay vì đồng thời tăng độ thưa của tất cả các lớp đến một mức độ thưa nào đó tại mỗi bước cắt tỉa, chúng tôi chia nhỏ khoảng cắt tỉa và tăng độ thưa của từng lớp một đến mức độ thưa đó. Điều này có khả năng có hiệu quả giảm tác động của việc cắt tỉa và cho phép mạng phục hồi tốt hơn với việc huấn luyện. Cuối cùng, chúng tôi so sánh với cắt tỉa "global": chúng tôi cắt tỉa các trọng số có độ lớn nhỏ nhất trên toàn bộ mạng, bất kể chúng ở lớp nào. Cắt tỉa global tạo ra một mức độ thưa khác nhau cho mỗi lớp và được chứng minh là hoạt động tốt trên NMT trong công trình của See et al. (2016). Nhìn chung, lược đồ cắt tỉa layerwise constant hoạt động tốt nhất trung bình, vì vậy chúng tôi báo cáo kết quả với lược đồ cắt tỉa layerwise constant trong Hình 4 và Bảng 4. Chúng tôi lưu ý rằng có phương sai cao trong kết quả do tính ngẫu nhiên của quá trình huấn luyện, như được minh họa bởi thanh lỗi trong Hình 4. Thanh lỗi đại diện cho độ lệch chuẩn của điểm BLEU của 10 mô hình NMT được khởi tạo ngẫu nhiên và huấn luyện độc lập.

Kết quả trong Bảng 4 cho thấy rằng đối với độ thưa 80% (nén 5x), mô hình được cắt tỉa thực sự đạt được điểm BLEU cao hơn một chút so với mô hình cơ sở (mặc dù chúng tôi lưu ý thanh lỗi). Đối với độ thưa 85%, điểm BLEU giảm khoảng 0.25, và đối với độ thưa 90%, điểm BLEU giảm khoảng 0.6. Khi chúng tôi so sánh hiệu suất của các mô hình dày đặc và thưa trong Hình 4 và Bảng 4, chúng ta lại thấy rằng các mô hình thưa vượt trội so với các mô hình dày đặc có kích thước lớn hơn. Điểm BLEU của mô hình dày đặc giảm nhanh sau khi giảm 2x kích thước mô hình trong khi điểm BLEU của mô hình thưa bắt đầu giảm chỉ sau khi giảm 5x về số lượng tham số khác không. Ví dụ, mô hình 1024-unit thưa 90% có thể so sánh hoặc vượt trội so với mô hình 512-unit dày đặc (26.19 so với 26.05 cho EN-DE và 28.81 so với 28.88 cho DE-EN) mặc dù có ít hơn 3.5x tham số khác không (23M so với 81M).

5 Thảo luận
Dung lượng bộ nhớ ròng của một mô hình thưa bao gồm lưu trữ cho các tham số khác không và bất kỳ cấu trúc dữ liệu phụ trợ nào cần thiết để đánh chỉ mục những phần tử này. Việc cắt tỉa mô hình giúp giảm số lượng kết nối có giá trị khác không trong mạng; tuy nhiên chi phí trong lưu trữ ma trận thưa không tránh khỏi làm giảm tỷ lệ nén có thể đạt được. Biểu diễn ma trận thưa bit-mask đòi hỏi 1 bit cho mỗi phần tử ma trận chỉ ra liệu phần tử có khác không hay không, và một vector chứa tất cả các phần tử ma trận khác không. Biểu diễn này gây ra chi phí không đổi bất kể độ thưa của mô hình. Trong lưu trữ hàng (cột) thưa nén (CSR(C)) được áp dụng trong Parashar et al. (2017), mỗi tham số khác không trong ma trận thưa được liên kết với một đếm (thường được lưu trữ như một số nguyên 4 hoặc 5 bit) về số lượng số không đứng trước nó. Chi phí trong trường hợp này tỷ lệ với NNZ trong mô hình. Bảng 5 so sánh hai biểu diễn này cho sparse-MobileNets. Tự nhiên, biểu diễn CSR(C) có thể cho phép tỷ lệ nén cao hơn cho các mạng có độ thưa cao. Tuy nhiên, lưu ý rằng biểu diễn bit-mask mang lại chi phí thấp hơn một cách nhỏ ở các mức độ thưa nhỏ hơn.

Dù có chi phí này, các mô hình lớn-thưa xuất hiện để đạt được độ chính xác cao hơn so với các mô hình nhỏ-dày đặc với dung lượng bộ nhớ tương đương. Ví dụ, MobileNet với hệ số nhân độ rộng 1 và độ thưa 50% có dung lượng tương tự như MobileNet với hệ số nhân độ rộng 0.75, nhưng có được độ chính xác cao hơn. Bảng 6 tiếp tục làm nổi bật sự đánh đổi giữa kích thước mô hình và độ chính xác cho các mô hình dày đặc và thưa. Khoảng cách hiệu suất giữa các mô hình lớn-thưa và nhỏ-dày đặc mở rộng cho các mô hình lớn hơn như các mô hình ngôn ngữ PTB và NMT (xem Bảng 3 và Bảng 4). Đáng chú ý rằng các kết quả được trình bày trong công trình này được thu được bằng cách huấn luyện mạng nơ-ron sử dụng biểu diễn số thực dấu phẩy động 32-bit. Đối với các mạng nơ-ron được huấn luyện để thực hiện suy luận sử dụng số học độ chính xác giảm (số nguyên 8-bit, chẳng hạn), chi phí bộ nhớ của lưu trữ ma trận thưa đại diện cho một phần lớn hơn của tổng dung lượng bộ nhớ. Lượng tử hóa các tham số đến biểu diễn số có độ chính xác giảm cũng là một phương pháp hiệu quả cho việc nén mô hình, và sự tương tác giữa lượng tử hóa mô hình và cắt tỉa cùng tác động tập thể của chúng lên độ chính xác mô hình đáng được xem xét gần gũi hơn. Chúng tôi hoãn điều tra đó cho một mở rộng tương lai của công trình này.

6 Kết luận
Công trình này làm sáng tỏ sự đánh đổi giữa kích thước mô hình và độ chính xác gặp phải trong các mạng nơ-ron sâu được cắt tỉa. Chúng tôi chứng minh rằng các mô hình lớn-thưa vượt trội so với các mô hình nhỏ-dày đặc có kích thước tương đương trên một tập hợp đa dạng các kiến trúc mạng nơ-ron. Chúng tôi cũng trình bày một kỹ thuật cắt tỉa từ từ có thể được áp dụng một cách dễ dàng trên những kiến trúc khác nhau này. Chúng tôi tin rằng những kết quả này sẽ khuyến khích việc áp dụng cắt tỉa mô hình như một công cụ để nén mạng nơ-ron để triển khai trong các môi trường hạn chế tài nguyên. Đồng thời, chúng tôi giữ quan điểm rằng kết quả của chúng tôi sẽ cung cấp thêm động lực cho cộng đồng kiến trúc phần cứng để tùy chỉnh thế hệ tiếp theo của kiến trúc accelerator học sâu để xử lý hiệu quả lưu trữ và tính toán ma trận thưa.

Mã nguồn
Chúng tôi đã mở mã nguồn thư viện cắt tỉa TensorFlow được sử dụng để tạo ra các kết quả được báo cáo trong công trình này.
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/model_pruning

Lời cảm ơn
Các tác giả cảm ơn Huizhong Chen, Volodymyr Kysenko, David Chen, SukHwan Lim, Raziel Alvarez, và Thang Luong cho những thảo luận hữu ích.
