# 2405.01943.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2405.01943.pdf
# Kích thước tệp: 988279 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Thưa thớt Bán cấu trúc Nhận thức Phụ thuộc của
Các Biến thể GLU trong Mô hình Ngôn ngữ Lớn
Zhiyu Guo guo.zhiyu.fy1@is.naist.jp
Viện Khoa học và Công nghệ Nara
Hidetaka Kamigaito kamigaito.h@is.naist.jp
Viện Khoa học và Công nghệ Nara
Taro Watanabe taro@is.naist.jp
Viện Khoa học và Công nghệ Nara

Tóm tắt
Sự tiến bộ nhanh chóng trong các Mô hình Ngôn ngữ Lớn (LLMs) đã nâng cao đáng kể khả năng hiểu và tạo sinh ngôn ngữ. Tuy nhiên, kích thước mô hình lớn đặt ra những thách thức về phần cứng, ảnh hưởng đến cả kích thước bộ nhớ để phục vụ và độ trễ suy luận để tạo sinh token. Để giải quyết những thách thức đó, chúng tôi đề xuất Thưa thớt Bán cấu trúc Nhận thức Phụ thuộc (DaSS), một phương pháp mới để cắt tỉa các LLM dựa trên GLU hiện đang phổ biến, kết hợp phụ thuộc cấu trúc vào việc cắt tỉa không cấu trúc dựa trên độ lớn trọng số. Chúng tôi giới thiệu một metric cắt tỉa đặc thù cho MLP đánh giá tầm quan trọng của mỗi trọng số bằng cách xem xét cả độ lớn của nó và các chuẩn kích hoạt trung gian MLP tương ứng. DaSS tạo điều kiện cho sự cân bằng giữa khả năng thích ứng do cắt tỉa không cấu trúc cung cấp và tính nhất quán cấu trúc vốn có trong cắt tỉa có cấu trúc dựa trên phụ thuộc. Đánh giá thực nghiệm trên các họ mô hình LLaMA2, Mistral và Gemma cho thấy DaSS không chỉ vượt trội hơn cả SparseGPT và Wanda trong việc đạt được các mẫu thưa thớt N:M thân thiện với phần cứng mà còn duy trì hiệu quả tính toán của Wanda.

1 Giới thiệu
Những năm gần đây đã chứng kiến thành công lớn của các Mô hình Ngôn ngữ Lớn (LLMs) trên nhiều nhiệm vụ thách thức khác nhau, như lập luận toán học, tạo sinh mã. Tuy nhiên, việc sử dụng thực tế của các mô hình này để suy luận đã gặp phải một trở ngại lớn do tài nguyên tính toán đáng kể mà chúng tiêu thụ. Để giải quyết điều này, nhiều phát triển chính cho đến nay đã xoay quanh việc lượng tử hóa trọng số. Có thể lượng tử hóa LLMs xuống 4 bit mỗi trọng số với ít tác động đến độ chính xác, điều này hỗ trợ giảm bộ nhớ và tăng tốc suy luận (Lin et al., 2024). Tuy nhiên, việc duy trì độ chính xác trở nên có vấn đề khi lượng tử hóa xuống khoảng 3 bit mỗi trọng số với các phương pháp hiện có (Dettmers et al., 2024; Egiazarian et al., 2024).

Một phương pháp bổ sung là cắt tỉa mạng nơ-ron (Han et al., 2015b), có thể được kết hợp với lượng tử hóa để cải thiện thêm hiệu quả suy luận của LLMs (Kurtic et al., 2023; Frantar & Alistarh, 2023). Cắt tỉa có thể được phân loại thành hai cách tiếp cận chính: cắt tỉa không cấu trúc (Sun et al., 2024; Frantar & Alistarh, 2023), bao gồm việc loại bỏ các trọng số cụ thể, và cắt tỉa có cấu trúc (Ma et al., 2023), bao gồm việc loại bỏ các hàng hoặc cột trọng số hoàn chỉnh. Trái ngược với cắt tỉa có cấu trúc, vốn gặp khó khăn với hiệu suất trong LLMs ngay cả ở mức thưa thớt thấp, các phương pháp cắt tỉa không cấu trúc như SparseGPT (Frantar & Alistarh, 2023) và Wanda (Sun et al., 2024) thể hiện kết quả đầy hứa hẹn mà không cần huấn luyện lại bổ sung, và đạt được tăng tốc thực tế trong cả CPU và GPU thông qua các tiến bộ kỹ thuật gần đây (Agarwalla et al., 2024). Chúng cũng có lợi ích trong việc giảm ảo giác của LLMs (Chrysostomou et al., 2024).

Các LLM hiện đại, như LLaMA2 (Touvron et al., 2023) và Mistral (Jiang et al., 2023), đã áp dụng một số thay đổi kiến trúc, bao gồm việc sử dụng các biến thể GLU (Gated Linear Unit) (ví dụ: SwiGLU) (Shazeer, 2020)

--- TRANG 2 ---
trong các mô-đun MLP và attention truy vấn nhóm (GQA) (Ainslie et al., 2023). Vì mô-đun MLP dựa trên GLU chiếm hơn 80% tham số trong các LLM sử dụng GQA¹, việc cắt tỉa của nó nổi lên như một yếu tố then chốt trong việc xác định hiệu quả nén tổng thể của LLMs. Trong cắt tỉa có cấu trúc nhận thức phụ thuộc, việc xem xét rằng các tham số bị cắt tỉa có phụ thuộc với các tham số khác là rất quan trọng, do bản chất liên kết của chúng (Ma et al., 2023; Fang et al., 2023). Trong bối cảnh cắt tỉa MLP, tất cả các trọng số kết nối với mỗi nơ-ron trung gian nên được bảo tồn hoặc cắt tỉa đồng thời. Sự phối hợp chính xác trong việc cắt tỉa là rất quan trọng để duy trì tính toàn vẹn cấu trúc của mô hình và khả năng chức năng của nó. Mặc dù các phương pháp cắt tỉa không cấu trúc hiện tại (Sun et al., 2024; Frantar & Alistarh, 2023) loại bỏ hiệu quả một số lượng đáng kể trọng số dư thừa, chúng hoạt động hoàn toàn cục bộ trong mỗi lớp tuyến tính mà không xem xét các phụ thuộc liên tầng với các lớp khác. Điều này có thể dẫn đến sự không khớp cấu trúc, rõ ràng hơn trong Wanda như được hiển thị trong Hình 1b: Trong các phép chiếu Gate và Up, cùng một lượng tham số được cắt tỉa cho mỗi nơ-ron MLP. Tuy nhiên, các chuẩn kích hoạt trung gian của GLU không được phân bố đồng đều và một số nơ-ron có chuẩn lớn hơn nhiều so với những nơ-ron khác. Dựa trên metric cắt tỉa Wanda, nhiều trọng số kết nối với các nơ-ron có chuẩn kích hoạt lớn được bảo tồn. Ở mức thưa thớt cao, vấn đề này được phóng đại trong Wanda gây ra sự sụt giảm đáng kể về hiệu suất do mạng bị hỏng.

Để vượt qua những hạn chế có trong các phương pháp cắt tỉa hiện tại, chúng tôi giới thiệu một mô hình mới, cụ thể là Thưa thớt Bán cấu trúc Nhận thức Phụ thuộc (DaSS). Cách tiếp cận này được thiết kế đặc biệt để điều hướng vùng trung gian giữa tính linh hoạt của cắt tỉa không cấu trúc và tính nhất quán cấu trúc của cắt tỉa có cấu trúc dựa trên phụ thuộc. Để nhấn mạnh tầm quan trọng của các trọng số tương ứng với các kích hoạt trung gian lớn, chúng tôi trình bày một metric cắt tỉa MLP mới đánh giá tầm quan trọng của mỗi trọng số dựa trên tích của độ lớn của nó và chuẩn của các kích hoạt trung gian MLP tương ứng. Phương pháp DaSS được đề xuất của chúng tôi, được minh họa trong Hình 1c, thể hiện một mẫu bán cấu trúc giữ lại một mức độ khả năng thích ứng vốn có trong cắt tỉa không cấu trúc trong khi kết hợp khía cạnh nhận thức phụ thuộc của cắt tỉa có cấu trúc. Sự cân bằng này cho phép cắt tỉa chính xác hơn. DaSS có thể dễ dàng được mở rộng cho các mẫu thưa thớt N:M thân thiện với phần cứng Mishra et al. (2021).

Chúng tôi thực hiện các thí nghiệm rộng rãi trên LLaMA2 (Touvron et al., 2023), Gemma (Team et al., 2024), và Mistral (Jiang et al., 2023) để đánh giá DaSS trên nhiều nhiệm vụ khác nhau từ mô hình hóa ngôn ngữ, 5 nhiệm vụ lập luận thông thường. Trong việc đạt được các mẫu thưa thớt N:M thân thiện với phần cứng, DaSS liên tục vượt trội so với các phương pháp cắt tỉa LLM hiện có SparseGPT và Wanda, trong khi duy trì hiệu quả tính toán tương tự như Wanda. Hơn nữa, DaSS thể hiện hiệu quả nhất quán trong tất cả các biến thể GLU phổ biến, bao gồm SwiGLU, GeGLU và ReGLU. Ấn tượng là DaSS vượt trội hơn SparseGPT ở mức thưa thớt cao ngay cả khi không cập nhật trọng số. Chúng tôi hy vọng những hiểu biết mới của chúng tôi có thể thúc đẩy các chiến lược nén LLM đặc thù cho GLU tinh tế hơn.

2 Kiến thức cơ bản

2.1 Phương pháp Cắt tỉa Wanda
Trong bối cảnh cắt tỉa LLM, chúng tôi ký hiệu ma trận trọng số lớp tuyến tính W ∈ R^(d_out×d_in) và các kích hoạt đầu vào X ∈ R^(L×d_in), trong đó d_in, d_out và L đại diện cho chiều đầu vào trọng số, chiều đầu ra trọng số và độ dài chuỗi đầu vào, tương ứng. Wanda (Sun et al., 2024) đánh giá tầm quan trọng trọng số thông qua phép nhân độ lớn trọng số với chuẩn của đặc trưng đầu vào. Cụ thể, điểm quan trọng cho một trọng số W_i,j được xác định như sau:

I_i,j = |W_i,j| · ∥X_j∥_2                    (1)

Ở đây, I_i,j là điểm quan trọng trọng số cho một trọng số W_i,j trong ma trận trọng số. ∥X_j∥_2 đại diện cho chuẩn ℓ_2 của đặc trưng thứ j của đầu vào X, được tính trên tất cả L token để tạo ra một vô hướng. Trong cắt tỉa Wanda, điểm quan trọng cho các trọng số được so sánh cho mỗi đầu ra riêng lẻ, tương ứng với mỗi hàng trong ma trận W. Chúng tôi gọi đây là độ chi tiết cân bằng đầu ra.

2.2 Các Biến thể GLU cho Transformer
Các Đơn vị Tuyến tính Có cổng (GLU) (Dauphin et al., 2017) được hình thành bởi phép nhân theo từng phần tử của hai phép chiếu tuyến tính, với hàm sigmoid được áp dụng cho một phép chiếu trước khi nhân. Shazeer (2020) đề xuất một thiết kế thay thế cho lớp MLP của Transformer kết hợp các biến thể GLU, thay thế hiệu quả cho phép biến đổi tuyến tính đầu tiên thông thường và hàm kích hoạt. Chính thức hơn, chúng tôi ký hiệu d_hidden là chiều của trạng thái ẩn của mô hình và d_int là chiều trung gian của mô-đun MLP. Với x ∈ R^d_hidden, W^(1) ∈ R^(d_hidden×d_int), W^(2) ∈ R^(d_hidden×d_int), W^(3) ∈ R^(d_int×d_hidden), và hàm kích hoạt sigma(·), mỗi lớp MLP tạo ra z ∈ R^d_hidden bằng cách đánh giá

y = sigma(xW^(1)) ⊗ xW^(2)                   (2)
z = yW^(3)                                   (3)

Chúng tôi gọi W^(1), W^(2), W^(3) là các phép chiếu tuyến tính Gate-Proj, Up-Proj và Down-Proj, tương ứng. Các biến thể GLU sử dụng hàm kích hoạt Swish (Ramachandran et al., 2017), GeLU (Hendrycks & Gimpel, 2016), và ReLU (Glorot et al., 2011) trong Phương trình 2 được gọi là SwiGLU, GeGLU và ReGLU, tương ứng. SwiGLU được sử dụng rộng rãi nhất trong các LLM gần đây.

2.3 Hạn chế Cắt tỉa Cân bằng Đầu ra
Mẫu thưa thớt N:M cung cấp cải thiện tốc độ đáng chú ý trên các GPU NVIDIA gần đây (Mishra et al., 2021; Kurtic et al., 2023). Nó được chỉ định rằng mỗi nhóm M trọng số liên tiếp phải bao gồm N số không. SparseGPT (Frantar & Alistarh, 2023) không được thiết kế rõ ràng cho độ chi tiết cắt tỉa cân bằng đầu ra. Khi chuyển đổi thành mẫu thưa thớt N:M, SparseGPT buộc mỗi M trọng số liên tiếp trong mỗi hàng phải có chính xác N số không. Trong những trường hợp như vậy, SparseGPT (Frantar & Alistarh, 2023) và Wanda (Sun et al., 2024) đồng thuận loại bỏ cùng một lượng trọng số cho mỗi đầu ra. Những phương pháp như vậy thường nhấn mạnh tầm quan trọng của các thành phần riêng lẻ trong ma trận trọng số và bỏ qua các phụ thuộc liên tầng với các lớp khác trong mạng. Đối với cắt tỉa phép chiếu đầu vào MLP, một lượng bằng nhau của các trọng số tương ứng với mỗi nơ-ron trung gian được loại bỏ. Tuy nhiên, đối với phép chiếu đầu ra dựa trên các metric cắt tỉa SparseGPT và Wanda, các trọng số kết nối với các nơ-ron trung gian có kích hoạt lớn hơn có nhiều khả năng được bảo tồn, dẫn đến sự không khớp cấu trúc.

3 Thưa thớt Bán cấu trúc Nhận thức Phụ thuộc

Trong phần này, chúng tôi giới thiệu Thưa thớt Bán cấu trúc Nhận thức Phụ thuộc (DaSS) để cắt tỉa MLP, kết hợp phụ thuộc cấu trúc vào phương pháp cắt tỉa không cấu trúc dựa trên độ lớn trọng số. Tổng quan về DaSS và so sánh của nó với phương pháp cắt tỉa hiện có được hiển thị trong Hình 1.

Ở đây, chúng tôi ký hiệu các ma trận trọng số chuyển vị W^(1), W^(2), W^(3) trong Phương trình 2 và 3 là W^(1)⊤, W^(2)⊤ và W^(3)⊤, tương ứng. Đối với W^(1)⊤ và W^(2)⊤, các hình dạng là (d_int, d_hidden). Đối với W^(3)⊤, hình dạng là (d_hidden, d_int).

Phụ thuộc Cấu trúc trong MLP. Trong cắt tỉa có cấu trúc dựa trên phụ thuộc (Ma et al., 2023; Fang et al., 2023), bước đầu tiên được dành để nhận biết các nhóm cấu trúc liên kết trong mô hình. Về việc cắt tỉa mô-đun MLP dựa trên GLU, có ba ma trận phép chiếu, tất cả các trọng số kết nối với một nơ-ron trung gian giống hệt nhau được phân loại tập thể vào cùng một nhóm phụ thuộc. Khi cắt tỉa các trọng số trong W^(1)⊤_i,:, việc cắt tỉa tất cả các trọng số trong W^(2)⊤_i,: và W^(3)⊤_:,i là cần thiết để duy trì tính nhất quán cấu trúc của mạng nơ-ron được cắt tỉa. Trong cắt tỉa DaSS, thay vì cắt tỉa tích cực tất cả các trọng số trong các nhóm phụ thuộc ít quan trọng hơn, chúng tôi kết hợp sự phối hợp cấu trúc như vậy vào cắt tỉa không cấu trúc. Nếu tất cả các trọng số trong W^(3)⊤_:,i được nhấn mạnh tầm quan trọng thông qua chỉ báo quan trọng bổ sung, chẳng hạn như tăng cường các kích hoạt đầu vào tương ứng trong Eq. 1, thì các trọng số trong W^(1)⊤_i,: và W^(2)⊤_i,: cũng nên được nhấn mạnh tầm quan trọng tương tự.

Kết hợp Phụ thuộc vào Tầm quan trọng Trọng số. Trong cắt tỉa dựa trên phụ thuộc, chúng tôi đánh giá tầm quan trọng của mỗi trọng số và sau đó tổng hợp điểm quan trọng trong cùng một nhóm thành điểm quan trọng nhóm. Do đó, mỗi trọng số trong cùng nhóm phụ thuộc chia sẻ điểm quan trọng nhất quán, đảm bảo việc giữ lại hoặc loại bỏ đồng thời của chúng. Điểm quan trọng của mỗi trọng số bằng điểm quan trọng nhóm. Trong cắt tỉa DaSS, chúng tôi xem xét cả tầm quan trọng nhóm và độ lớn trọng số để đánh giá tầm quan trọng của mỗi trọng số. LLM-pruner (Ma et al., 2023) đánh giá tầm quan trọng nhóm bằng các phương pháp dựa trên gradient. Tuy nhiên, việc tính toán gradient cho LLMs sẽ đưa vào một lượng chi phí bộ nhớ đáng kể và ít thực tế hơn đối với các mô hình lớn hơn. Các phương pháp cắt tỉa không cấu trúc hiện có (Sun et al., 2024; Frantar & Alistarh, 2023) hiệu quả hơn các phương pháp dựa trên gradient.

Trong cắt tỉa Down-Proj, Wanda ưu tiên các trọng số liên kết với các giá trị ngoại lai trong kích hoạt trung gian. Để giảm thiểu tác động lên các giá trị ngoại lai kích hoạt trung gian quan trọng này, sẽ có lợi khi cũng tăng cường ý nghĩa cho các trọng số dẫn đến tạo ra giá trị ngoại lai trong cả phép chiếu Gate-Proj và Up-Proj. Vì vậy, chúng tôi sử dụng chuẩn của kích hoạt trung gian ∥y∥_2 làm chỉ báo tầm quan trọng nhóm. Việc tính toán chuẩn kích hoạt rất đơn giản mà không đưa vào thêm chi phí tính toán và bộ nhớ hơn so với việc tính toán gradient. Để gán điểm quan trọng lớn hơn cho các trọng số tương ứng với các nhóm quan trọng hơn, chúng tôi đánh giá tầm quan trọng trọng số dựa trên tích của độ lớn trọng số và điểm quan trọng nhóm tương ứng.

Cụ thể hơn, đối với Gate-Proj và Up-Proj, điểm quan trọng được gán cho một trọng số cụ thể W^(k)⊤_i,j được xác định như sau:

I^(k)_i,j = |W^(k)⊤_i,j| · ∥y_i∥^α_2                 (4)

trong đó k = 1,2. Chúng tôi giới thiệu một siêu tham số cường độ tầm quan trọng nhóm α. Chúng tôi thực nghiệm thấy rằng α = 0.5 là một điểm cân bằng tốt cho các mô hình và tập dữ liệu khác nhau trong các nghiên cứu sơ bộ của chúng tôi. Đối với việc cắt tỉa phép chiếu Down-Proj, chúng tôi trực tiếp tăng cường tầm quan trọng nhóm vào độ lớn trọng số mà không có siêu tham số bổ sung. Đối với ma trận Down-Proj, điểm quan trọng của trọng số W^(3)⊤_i,j được xác định như sau:

I^(3)_i,j = |W^(3)⊤_i,j| · ∥y_j∥_2                 (5)

Trong cắt tỉa Down-Proj, metric cắt tỉa giống với Wanda (Sun et al., 2024). Bằng cách tăng cường kích hoạt trung gian vào tất cả ba ma trận tầm quan trọng trọng số, DaSS vốn gán nhấn mạnh lớn hơn cho các trọng số tương ứng với các giá trị ngoại lai kích hoạt trung gian, từ đó tạo điều kiện cho sự phối hợp cấu trúc tinh tế hơn giữa toàn bộ mô-đun MLP.

Độ Chi tiết Cắt tỉa. Cắt tỉa LLMs ở độ chi tiết mịn hơn có thể cải thiện hiệu suất (Sun et al., 2024). Để kết hợp kích hoạt trung gian vào cắt tỉa MLP dựa trên GLU, mỗi trọng số trong cùng nhóm so sánh nên tương ứng với các kích hoạt trung gian khác nhau. Do đó, chúng tôi chọn sử dụng cắt tỉa cân bằng đầu vào cho cắt tỉa Gate-Proj và Up-Proj, trong đó các trọng số được so sánh cho mỗi đầu vào riêng lẻ. Trong độ chi tiết cắt tỉa như vậy, chúng tôi có thể tăng cường kích hoạt trung gian vào metric cắt tỉa Gate-Proj và Up-Proj. Trong cắt tỉa cân bằng đầu vào, chúng tôi loại bỏ s% trọng số liên kết với mỗi đầu vào cho tỷ lệ thưa thớt được xác định trước là s% dựa trên điểm quan trọng trọng số. DaSS sử dụng thưa thớt cân bằng đầu ra cho cắt tỉa Down-Proj, giống như Wanda. Trong mỗi nhóm so sánh, các trọng số được sắp xếp dựa trên điểm quan trọng của chúng, và những trọng số có điểm thấp nhất được cắt tỉa.

Mở rộng cho Thưa thớt N:M. Thiết kế cắt tỉa DaSS cho phép thích ứng dễ dàng với mẫu thưa thớt N:M. Đối với Gate-Proj và Up-Proj, mẫu thưa thớt N:M được hình thành trên cơ sở cân bằng đầu vào. Điều này có nghĩa là đối với các trọng số kết nối với mỗi nơ-ron đầu vào, trong mỗi nhóm M trọng số liên tiếp, có chính xác N số không được bao gồm. Đối với Down-Proj, mẫu thưa thớt N:M được hình thành trên cơ sở cân bằng đầu ra.

Thảo luận. Tóm lại, phương pháp DaSS của chúng tôi cung cấp nhiều khía cạnh hấp dẫn cho việc cắt tỉa LLMs:

1. Nó giữ lại sự đơn giản cơ bản vốn có trong phương pháp cắt tỉa Wanda. Mà không cập nhật trọng số, nó vẫn phù hợp với hiệu suất của SparseGPT ngay cả ở mức thưa thớt cao như được chứng minh trong Phần 4.4. Điều này chứng minh khả năng nhất quán hiệu quả và hiệu quả của phương pháp DaSS trong việc xác định các mạng nơ-ron thưa thớt.

2. Không giống như SparseGPT và Wanda sử dụng kích hoạt đầu vào + trung gian cho cắt tỉa MLP, DaSS chỉ sử dụng kích hoạt trung gian. Bằng cách sử dụng kích hoạt trung gian làm chỉ báo tầm quan trọng nhóm, DaSS cắt tỉa mô-đun MLP trong một cái nhìn toàn diện hơn nắm bắt tầm quan trọng tập thể của tất cả các trọng số kết nối với mỗi nơ-ron trung gian.

3. DaSS khám phá hiệu quả sự cân bằng giữa tính linh hoạt của cắt tỉa không cấu trúc và sự gắn kết cấu trúc trong cắt tỉa có cấu trúc dựa trên phụ thuộc.

4 Thí nghiệm

4.1 Cài đặt

Mô hình. Hiệu suất của DaSS được đánh giá trên các LLM mở sử dụng các biến thể GLU. SwiGLU là MLP dựa trên GLU được sử dụng rộng rãi nhất trong các LLM gần đây, bao gồm họ mô hình LLaMA2 (Touvron et al., 2023), có các mô hình với tham số từ 7 tỷ đến 70 tỷ, và cả mô hình Mistral-7B (Jiang et al., 2023). Trong số đó, LLaMA2-70B và Mistral-7B sử dụng attention truy vấn nhóm (Ainslie et al., 2023), mô-đun MLP chiếm khoảng 80% tổng số tham số mô hình. Chỉ có một vài LLM mở sử dụng các biến thể khác. Đối với GeGLU, chúng tôi sử dụng Gemma-7B. Đáng chú ý là chiều trung gian MLP của Gemma-7B là 8×chiều mô hình, làm cho mô-đun MLP lớn hơn nhiều. Đối với ReGLU, chúng tôi sử dụng ReluLLaMA (Team, 2023), được tinh chỉnh sử dụng biến thể ReGLU (Shazeer, 2020; Mirzadeh et al., 2023) dựa trên LLaMA2 với mất mát độ chính xác nhỏ. Chi tiết cấu hình mô hình có trong Phụ lục A.1. Chúng tôi truy cập các checkpoint công khai của các mô hình liên quan được cung cấp bởi HuggingFace Transformers (Wolf et al., 2019).

--- TRANG 6 ---

Bảng 1: Độ phức tạp WikiText của các LLM được cắt tỉa. Ở đây chúng tôi chỉ cắt tỉa mô-đun MLP. Dấu (*) trong Size chỉ ra một mô hình sử dụng MLP lớn hơn Transformer Vanilla (ví dụ: attention truy vấn nhóm).

[Bảng với kết quả PPL cho các mô hình khác nhau ở các mức thưa thớt khác nhau]

Các Phương pháp Cơ sở. Chúng tôi so sánh hiệu suất với hai phương pháp cắt tỉa một lần đặc thù cho LLM, SparseGPT (Frantar & Alistarh, 2023) và Wanda (Sun et al., 2024). Chúng tôi không xem xét các phương pháp cắt tỉa có cấu trúc như LLM-Pruner (Ma et al., 2023) và Sheared LLaMA (Xia et al., 2023), vì chúng thường yêu cầu huấn luyện lại để phục hồi độ chính xác, và ít thực tế hơn cho các mô hình lớn như LLaMA2-70B. Những phương pháp cơ sở đó sử dụng thưa thớt theo lớp đồng đều có thể dễ dàng chuyển đổi thành mẫu thưa thớt N:M thân thiện với phần cứng. Chúng tôi sử dụng cùng tập dữ liệu hiệu chuẩn như SparseGPT và Wanda trong quá trình cắt tỉa mô hình của họ, bao gồm 128 chuỗi 2048 token mỗi chuỗi, được chọn ngẫu nhiên từ shard đầu tiên của tập dữ liệu C4 (Raffel et al., 2020).

Đánh giá. Để đánh giá toàn diện hiệu quả của phương pháp được đề xuất, hai metric khác nhau được sử dụng để đánh giá hiệu suất của các mô hình được cắt tỉa: (1) độ phức tạp (PPL) của mô hình hóa ngôn ngữ (2) độ chính xác zero-shot trên 5 nhiệm vụ lập luận thông thường. Độ phức tạp đã được coi là một metric nhất quán và đáng tin cậy để đo lường các mô hình nén (Dettmers & Zettlemoyer, 2023; Frantar & Alistarh, 2023), trong khi các nhiệm vụ downstream đôi khi có xu hướng hành vi nhiễu, nhưng dễ diễn giải hơn. Để đánh giá độ phức tạp, chúng tôi sử dụng tập dữ liệu xác thực của WikiText2 (Merity et al., 2017). Đối với các nhiệm vụ lập luận thông thường zero-shot, chúng tôi chọn năm nhiệm vụ được sử dụng rộng rãi để đánh giá độ chính xác: ARC (Easy và Challenge) (Clark et al., 2018), HellaSwag (Zellers et al., 2019), PiQA (Bisk et al., 2020), và WinoGrande (Sakaguchi et al., 2021), được thực hiện trong Lm-Evaluation-Harness (Gao et al., 2021). Chúng tôi đánh giá độ phức tạp của tất cả các mô hình nói trên. Để chứng minh đầy đủ hiệu suất theo nhiệm vụ trong các mẫu thưa thớt khác nhau, chúng tôi báo cáo hiệu suất nhiệm vụ downstream của mô hình LLaMA2-70B lớn nhất. Đáng chú ý, LLaMA2-70B sử dụng attention truy vấn nhóm (Ainslie et al., 2023), và mô-đun MLP chiếm hơn 80% tổng số tham số, đại diện cho thiết kế kiến trúc được áp dụng rộng rãi nhất trong các LLM hiện đại.

Thưa thớt. Trong đánh giá độ phức tạp ít diễn giải, chúng tôi chỉ cắt tỉa các lớp MLP. Trong đánh giá theo nhiệm vụ của mô hình LLaMA2-70B, cả mô-đun attention và MLP đều được cắt tỉa, nhất quán với các công trình trước để đánh giá chính xác khoảng cách hiệu suất giữa các mô hình được cắt tỉa và mô hình gốc. Vì DaSS không áp dụng cho cắt tỉa mô-đun attention, chúng tôi sử dụng phương pháp Wanda để cắt tỉa mô-đun attention, đảm bảo hiệu quả tổng thể vẫn nhất quán với Wanda. Chúng tôi áp dụng tỷ lệ thưa thớt đồng đều trên tất cả các lớp được cắt tỉa và đánh giá ba loại thưa thớt: thưa thớt không cấu trúc, và thưa thớt bán cấu trúc 4:8 và 2:4.

4.2 Mô hình hóa Ngôn ngữ

Chúng tôi kiểm tra tất cả các LLM nói trên về độ phức tạp như được hiển thị trong Bảng 1. Phương pháp của chúng tôi liên tục đạt được hiệu suất tốt hơn SparseGPT và Wanda trong mẫu thưa thớt N:M hạn chế và thực tế hơn. Như được chỉ ra bởi Sun et al. (2024), nơi cập nhật trọng số có thể cải thiện hiệu suất trong mẫu thưa thớt N:M, phương pháp của chúng tôi cho thấy hiệu suất vượt trội ngay cả khi không có cập nhật trọng số tốn kém về mặt tính toán. Đối với các mô hình Mistral-7B, Gamma-7B và LLaMA2-70B với các lớp MLP lớn hơn, phương pháp của chúng tôi cũng vượt trội hơn SparseGPT trong thưa thớt không cấu trúc. Ấn tượng là đối với Gemma-7B với các lớp MLP lớn hơn nhiều, DaSS vượt trội hơn Wanda trong thưa thớt N:M đáng kể. DaSS cho thấy hiệu quả nhất quán trên SwiGLU, GeGLU và ReGLU, chứng minh tính tổng quát của phương pháp DaSS trên các biến thể GLU.

4.3 Các Nhiệm vụ Downstream

Ngoài việc đánh giá độ phức tạp, chúng tôi đánh giá toàn diện hiệu suất của các mô hình LLaMA2-70B được cắt tỉa trong các nhiệm vụ lập luận thông thường được sử dụng rộng rãi.

Trong Bảng 2, chúng tôi trình bày hiệu suất của các mô hình LLaMA2-70B thưa thớt khác nhau trên các nhiệm vụ downstream với prompting. Kết quả cho thấy phương pháp của chúng tôi vượt trội hơn SparseGPT và Wanda trong hầu hết các nhiệm vụ ở mẫu thưa thớt bán cấu trúc N:M. Ngoại lệ duy nhất là SparseGPT vượt trội hơn cả Wanda và DaSS trong nhiệm vụ Winogrande. Mô hình được cắt tỉa SparseGPT không cấu trúc 50% thậm chí vượt trội hơn mô hình dày đặc. Những kết quả như vậy phù hợp với các công trình nén LLM phổ biến (Dettmers & Zettlemoyer, 2023; Frantar & Alistarh, 2023), nơi kết quả nhiệm vụ đơn lẻ có thể nhiễu và độ chính xác trung bình cung cấp kết quả ổn định và đáng tin cậy. Đối với thưa thớt không cấu trúc, phương pháp của chúng tôi vượt trội hơn Wanda chia sẻ cùng mức độ phức tạp.

4.4 Phân tích Hiệu suất

Biến thiên Thưa thớt Bảng 3 minh họa so sánh độ chính xác nhiệm vụ zero-shot trung bình ở các mức thưa thớt khác nhau cho mô-đun MLP của mô hình LLaMA2-70B. Rõ ràng là cắt tỉa DaSS duy trì hiệu suất cạnh tranh phù hợp chặt chẽ với SparseGPT trên toàn bộ phạm vi tỷ lệ thưa thớt được kiểm tra. Đáng chú ý, DaSS đạt được hiệu suất như vậy mà không cần cập nhật trọng số, gợi ý hiệu quả và hiệu suất của nó trong việc định vị các mạng nơ-ron thưa thớt. Mặt khác, cắt tỉa Wanda cân bằng đầu ra cho thấy sự suy giảm đáng kể về độ chính xác khi tỷ lệ thưa thớt tăng. Điều này gợi ý rằng cắt tỉa Wanda có thể gặp phải các vấn đề không khớp cấu trúc trong lớp MLP, trở nên rõ ràng hơn ở các mức thưa thớt cao. Kết quả là, hiệu suất của mạng nơ-ron bị suy giảm, có thể dẫn đến một mô hình không hoạt động ở tỷ lệ thưa thớt cực đoan.

Tính Robust với mẫu hiệu chuẩn. Ashkboos et al. (2023) quan sát thấy rằng các kích hoạt trung gian của các lớp MLP dựa trên SwiGLU thể hiện phương sai cao, chủ yếu do tích Hadamard của hai đầu ra trước đó. Điều này dẫn đến độ chính xác giảm trong lượng tử hóa kích hoạt 4-bit. Trong Hình 2, chúng tôi trình bày cách thay đổi số lượng chuỗi được lấy mẫu để hiệu chuẩn ảnh hưởng đến hiệu suất của các phương pháp cắt tỉa. Mặc dù chỉ sử dụng kích hoạt trung gian, phương pháp của chúng tôi thể hiện độ nhạy cảm tối thiểu với các thay đổi trong số lượng mẫu hiệu chuẩn.

Nghiên cứu Ablation về Siêu tham số α Chúng tôi tiến hành một nghiên cứu ablation bổ sung để điều tra tác động của các giá trị khác nhau của siêu tham số cường độ tầm quan trọng nhóm α đến hiệu suất của DaSS. Cụ thể, chúng tôi thí nghiệm với LLaMA2-7B sử dụng α = {0.25, 0.5, 0.75, 1.0}. Kết quả được trình bày trong Bảng 4.

Kết quả ablation cho thấy α = 0.25 đạt được hiệu suất tốt hơn một chút so với α = 0.5 đối với LLaMA2-7B về độ phức tạp WikiText. Tuy nhiên, chúng tôi lưu ý rằng α = 1.0 vẫn vượt trội hơn Wanda (PPL: 6.50). Chúng tôi sử dụng α = 0.5 cho tất cả các thí nghiệm mà không cố ý điều chỉnh siêu tham số này cho các mô hình và nhiệm vụ khác nhau.

4.5 Phân tích Thời gian Chạy

Tốc độ Cắt tỉa Đối với DaSS và Wanda, độ phức tạp tính toán được định lượng là O(d²), trong khi SparseGPT thể hiện độ phức tạp cao hơn là O(d³). Chúng tôi ghi lại tổng thời gian cần thiết để cắt tỉa các lớp MLP, không bao gồm quá trình forward pass, theo cách tiếp cận được mô tả bởi Sun et al. (2024). Chúng tôi sử dụng một GPU A6000 48GB để cắt tỉa các mô hình 7B và 13B, và sử dụng 8 GPU A100 40GB để cắt tỉa mô hình 70B lớn hơn.

--- TRANG 9 ---

Bảng 6: Hiệu suất throughput giải mã cho các mức thưa thớt khác nhau với LLaMA2-7B sử dụng engine suy luận DeepSparse NeuralMagic (2021).

[Bảng hiển thị tốc độ token và speedup cho các mức thưa thớt khác nhau]

Như được chứng minh trong Bảng 5, chi phí tính toán phát sinh bởi DaSS là tối thiểu, đặc biệt khi so sánh với SparseGPT. Mặc dù tốc độ xử lý của DaSS chậm hơn một chút so với Wanda, sự khác biệt này phát sinh chủ yếu từ các chi tiết triển khai của torch.sort(). Trong việc triển khai torch.sort(), việc sắp xếp theo chiều cuối cùng hiệu quả hơn. Do đó, độ trễ quan sát được trong DaSS được gán cho các thao tác sắp xếp thay vì sự không hiệu quả vốn có trong phương pháp của chúng tôi. Bất chấp điều này, những cải thiện đáng kể về độ chính xác do DaSS đạt được làm cho thời gian tính toán bổ sung được biện minh và có lợi.

Hiệu quả Suy luận Chúng tôi cắt tỉa cả mô-đun attention và MLP để kiểm tra tốc độ suy luận. Tăng tốc đạt được bởi mô hình thưa thớt, như được chứng minh trong Bảng 6, làm nổi bật sự giảm đáng kể trong độ trễ giải mã end-to-end khi sử dụng LLaMA2-7B trên engine suy luận DeepSparse (NeuralMagic, 2021). Tốc độ được kiểm tra trên Intel Xeon Platinum 8160 CPU với 24 lõi. Chúng tôi nhấn mạnh rằng tăng tốc suy luận không độc quyền với phương pháp cắt tỉa của chúng tôi mà là kết quả của sức mạnh vốn có của thưa thớt trong việc tăng tốc tính toán.

5 Công trình Liên quan

Cắt tỉa LLM. Cắt tỉa mạng nơ-ron trong LLM có thể được phân loại rộng rãi thành hai nhóm: cắt tỉa có cấu trúc (Ma et al., 2023; Zhang et al., 2023) và cắt tỉa không cấu trúc (Frantar & Alistarh, 2023; Sun et al., 2024). Ma et al. (2023) đề xuất một thuật toán phát hiện phụ thuộc để phát hiện và cắt tỉa các cấu trúc nhóm không quan trọng theo sau bởi tinh chỉnh LoRA. Mặc dù cắt tỉa có cấu trúc thường có thể đạt được hiệu quả phần cứng tốt hơn, độ chính xác giảm rất nhiều ngay cả ở tỷ lệ nén thấp. Cắt tỉa không cấu trúc có thể mang lại tỷ lệ nén cao hơn và đạt được tăng tốc trên GPU của Nvidia bằng cách sử dụng mẫu thưa thớt N:M thân thiện với phần cứng. SparseGPT (Frantar & Alistarh, 2023) tận dụng nghịch đảo Hessian để cắt tỉa và giảm lỗi tái tạo của các trọng số dày đặc và thưa thớt bằng cập nhật trọng số tiếp theo. Wanda (Sun et al., 2024) sử dụng một phương pháp hiệu quả tăng cường kích hoạt đầu vào vào độ lớn trọng số, và phù hợp với hiệu suất của SparseGPT ở mức thưa thớt trung bình. Công trình của chúng tôi kết hợp thông tin phụ thuộc vào cắt tỉa không cấu trúc, đạt được một mô hình cắt tỉa mới.

Thưa thớt Vốn có của Transformer MLP. Thú vị là thưa thớt trong các kích hoạt MLP của các mô hình dựa trên Transformer được huấn luyện xảy ra một cách tự nhiên ngay cả khi không áp dụng các ràng buộc hoặc chính quy hóa rõ ràng (Zhang et al., 2022; Li et al., 2023; Dong et al., 2023). Hiện tượng như vậy phổ biến trong các Transformer đã học, bao gồm các hàm không bão hòa khác. Liu et al. (2023); Mirzadeh et al. (2023); Zhang et al. (2022) đạt được tăng tốc suy luận LLM thực tế bằng cách chỉ thực hiện tính toán tương ứng với nơ-ron kích hoạt cho một đầu vào nhất định. Họ không thực sự giảm kích thước mô hình vì họ chủ yếu giảm độ trễ I/O và tính toán theo cách tải trọng số có chọn lọc, và do đó, những phương pháp này ít áp dụng hơn trong các cài đặt suy luận batch-size lớn. Công trình của chúng tôi điều tra thưa thớt trọng số trong mô-đun MLP bằng cách xem xét các kích hoạt trung gian tương ứng.

Nén LLM Phụ thuộc Outlier. Các đặc trưng outlier, được định nghĩa là các đặc trưng có độ lớn lớn hơn đáng kể so với những đặc trưng khác, là một đặc điểm đáng chú ý của LLMs (Dettmers et al., 2022). Mặc dù chỉ chiếm một phần nhỏ của tất cả các chiều đặc trưng, những outlier này đóng vai trò quan trọng trong attention và hiệu suất dự đoán. Quan sát như vậy đã thúc đẩy sự phát triển của các phương pháp lượng tử hóa đặc thù cho LLM (Dettmers et al., 2022; Xiao et al., 2023; Lin et al., 2024; Ashkboos et al., 2023) để xử lý outlier hiệu quả hơn. Wanda (Sun et al., 2024) và OWL (Yin et al., 2024) mở rộng những hiểu biết này, tiết lộ rằng các đặc trưng outlier có ý nghĩa trong việc quyết định tầm quan trọng trọng số khi cắt tỉa LLMs. Phương pháp của chúng tôi khác biệt với quan điểm thông thường bằng cách chứng minh rằng, trong bối cảnh các phép chiếu đầu vào MLP dựa trên GLU, tầm quan trọng của các outlier kích hoạt đầu vào không rõ ràng như trước đây giả định, thúc đẩy việc đánh giá lại vai trò của chúng trong các chiến lược cắt tỉa LLM.

6 Kết luận

Chúng tôi đề xuất phương pháp Thưa thớt Bán cấu trúc Nhận thức Phụ thuộc (DaSS) giải quyết hiệu quả những thách thức của việc cắt tỉa các mô-đun MLP dựa trên GLU LLMs. DaSS tạo ra sự cân bằng độc đáo giữa khả năng thích ứng của cắt tỉa không cấu trúc và tính trật tự của cắt tỉa có cấu trúc. Bằng cách tận dụng các chuẩn kích hoạt trung gian MLP làm chỉ báo tầm quan trọng nhóm, chúng tôi phát triển một metric cắt tỉa mới đánh giá tầm quan trọng trọng số theo cách nhất quán về mặt cấu trúc hơn. Đánh giá thực nghiệm trên các họ mô hình Mistral, Gemma và LLaMA2 cho thấy DaSS vượt trội hơn các phương pháp cắt tỉa tiên tiến như SparseGPT và Wanda trong việc đạt được các mẫu thưa thớt N:M thân thiện với phần cứng. Chúng tôi hy vọng công trình của chúng tôi thúc đẩy nghiên cứu và phát triển thêm trong việc nén các LLM dựa trên GLU.

Tài liệu tham khảo

[Danh sách tài liệu tham khảo đầy đủ với các tác giả, tiêu đề, và thông tin xuất bản]

--- TRANG 13 ---

Phụ lục A

A.1 Cấu hình Mô hình

Bảng 7 là cấu hình của các mô hình được sử dụng trong bài báo. Chúng tôi không sử dụng LLaMA2-34B vì nó không được phát hành. ReluLLaMA sử dụng cùng cấu hình với LLaMA2, với sự khác biệt duy nhất là hàm kích hoạt. Các liên kết đến các mô hình ReluLLaMA được cung cấp dưới đây:

• ReluLLaMA-7B: https://huggingface.co/SparseLLM/ReluLLaMA-7B
• ReluLLaMA-13B: https://huggingface.co/SparseLLM/ReluLLaMA-13B

[Bảng 7 với cấu hình chi tiết của các mô hình]

A.2 Kết hợp với Lượng tử hóa

Cắt tỉa và lượng tử hóa, truyền thống được coi là các kỹ thuật nén mô hình riêng biệt, không loại trừ lẫn nhau và có thể được tích hợp hiệu quả để nâng cao hiệu quả tổng thể (Han et al., 2015a; Frantar & Alistarh, 2023; Agarwalla et al., 2024). Ở đây chúng tôi kiểm tra sự kết hợp của DaSS với AWQ 4-bit (Lin et al., 2024) để nén LlaMA2-7B. Như được hiển thị trong Bảng 8, hiệu suất của tất cả các mô hình được cắt tỉa chỉ giảm nhẹ sau lượng tử hóa 4-bit. DaSS vẫn vượt trội hơn wanda sau lượng tử hóa 4-bit.

A.3 Kết quả MMLU

Công trình gần đây (Gromov et al., 2024) chỉ ra hành vi bất thường của LLMs trong việc thực hiện các nhiệm vụ Hiểu Ngôn ngữ Đa nhiệm vụ Lớn (MMLU) (Hendrycks et al., 2021). Chúng tôi sử dụng Chain-of-Thought Hub (Fu et al., 2023) dựa trên việc triển khai chính thức của MMLU (Hendrycks et al., 2021). MMLU bao gồm 57 nhiệm vụ, trải dài từ STEM, Nhân văn, Khoa học Xã hội, trong số những lĩnh vực khác và chúng tôi báo cáo độ chính xác trung bình của 57 nhiệm vụ. Chúng tôi kiểm tra hiệu suất của mô hình LlaMA2-70B được cắt tỉa trong nhiệm vụ MMLU. Từ Bảng 9, chúng tôi quan sát thấy rằng sự cải thiện của DaSS so với Wanda trở nên rõ ràng hơn trong nhiệm vụ MMLU thách thức, nơi nó đạt được sự gia tăng độ chính xác 1.17 và 1.12 cho các mẫu thưa thớt 4:8 và 2:4, tương ứng. Theo các quan sát với Jaiswal et al. (2023), chúng tôi thấy sự suy giảm hiệu suất đáng kể trong nhiệm vụ MMLU chuyên sâu về kiến thức, trong đó chỉ các mô hình thưa thớt không cấu trúc 50% vượt trội hơn mô hình dày đặc LLaMA2-34B. Vì GPU đóng vai trò quan trọng hơn trong suy luận mô hình lớn hơn, việc cải thiện hiệu suất của các mô hình được cắt tỉa trong thưa thớt N:M thân thiện với phần cứng là cần thiết. Việc giảm đồng đều mức thưa thớt tổng thể không khả thi đối với thưa thớt N:M, Frantar & Alistarh (2023) đề xuất một tập con cụ thể của các lớp có thể được chọn để thưa thớt hóa N:M hoàn toàn. Ở đây chúng tôi bỏ qua cắt tỉa 1/4 lớp liên tiếp (20 lớp) và dẫn đến tỷ lệ thưa thớt cuối cùng 37.5%. Chúng tôi sử dụng 10 nhiệm vụ đầu tiên của MMLU để nghiên cứu độ nhạy cắt tỉa. Chúng tôi chia mô hình thành 4 phần liên tiếp và nghiên cứu việc bỏ qua cắt tỉa mỗi phần. Như được hiển thị trong Bảng 10, các lớp trước nhạy cảm hơn các lớp sau trong các nhiệm vụ chuyên sâu về kiến thức, trái ngược với các phát hiện trong Frantar & Alistarh (2023) sử dụng độ phức tạp, và phù hợp với các phát hiện của Gromov et al. (2024).

Chúng tôi tiếp tục tìm kiếm các lớp bỏ qua tốt hơn với phạm vi chỉ số lớp bắt đầu trong [0,20]. Chúng tôi thấy rằng bắt đầu bỏ qua từ lớp 10 có thể đạt được hiệu suất tốt nhất trong tập con. Sau đó chúng tôi kiểm tra kết quả của nó trong các nhiệm vụ MMLU đầy đủ. Như được hiển thị trong Bảng 2, việc bỏ qua 1/4 lớp nhạy cảm có thể cải thiện đáng kể hiệu suất của các mô hình được cắt tỉa, đặc biệt đối với thưa thớt N:M. Chúng tôi có thể đạt được các mô hình thưa thớt hoạt động tốt hơn LLaMA2-34B. Mặc dù các mô hình thưa thớt N:M một phần có nhiều tham số hơn các mô hình dày đặc nhỏ hơn, việc huấn luyện nhiều mô hình dày đặc nhỏ hơn như LLaMA2-34B vẫn tốn kém về mặt tính toán. Cắt tỉa sau huấn luyện hiệu quả cho phép chúng tôi dễ dàng điều chỉnh sự đánh đổi độ chính xác-hiệu quả trong các ứng dụng thế giới thực.

Công trình gần đây (Gromov et al., 2024) đề xuất một phương pháp hiệu quả để loại bỏ khoảng một nửa số lớp trong LLaMA2-70B với mất mát độ chính xác nhỏ cho các nhiệm vụ MMLU, gợi ý hành vi bất thường của LLMs trong việc thực hiện các nhiệm vụ MMLU. Công trình của họ tập trung nhiều hơn vào cắt tỉa đặc thù cho nhiệm vụ, trong khi công trình của chúng tôi nhấn mạnh việc bảo tồn khả năng tổng quát của LLMs để thực hiện các nhiệm vụ khác nhau. Phương pháp cắt tỉa trọng số của chúng tôi về cơ bản trực giao với cắt tỉa lớp về nguyên tắc.

[Bảng 8, 9, 10 với các kết quả tương ứng]
