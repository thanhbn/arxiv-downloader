A Phụ lục

A.1 Cấu hình Mô hình

Bảng 7 là các cấu hình của các mô hình được sử dụng trong bài báo. Chúng tôi không sử dụng LLaMA2-34B vì nó không được phát hành. ReluLLaMA sử dụng cùng cấu hình với LLaMA2, với sự khác biệt duy nhất là hàm kích hoạt. Các liên kết đến các mô hình ReluLLaMA được cung cấp dưới đây:
• ReluLLaMA-7B: https://huggingface.co/SparseLLM/ReluLLaMA-7B
• ReluLLaMA-13B: https://huggingface.co/SparseLLM/ReluLLaMA-13B

Bảng 7: Cấu hình mô hình của các mô hình Llama2, Gemma và Mistral.

Mô hình    Tham số  Lớp  Ẩn   Trung gian  Đầu Query  Đầu KV
LlaMa2-7B   7B     32   4096   11008      32        32
LlaMa2-13B  13B    40   5120   13824      40        40
LlaMa2-70B  70B    80   8192   28672      64        8
Mistral-7B  7B     32   4096   14336      32        8
Gemma-7B    7B     28   3072   24576      16        16

A.2 Kết hợp với Lượng tử hóa

Tỉa và lượng tử hóa, truyền thống được xem là các kỹ thuật nén mô hình riêng biệt, không loại trừ lẫn nhau và có thể được tích hợp hiệu quả để nâng cao hiệu quả tổng thể (Han et al., 2015a; Frantar & Alistarh, 2023; Agarwalla et al., 2024). Ở đây chúng tôi kiểm tra sự kết hợp của DaSS với AWQ 4-bit (Lin et al., 2024) để nén LlaMA2-7B. Như thể hiện trong Bảng 8, hiệu suất của tất cả các mô hình được tỉa chỉ giảm nhẹ sau lượng tử hóa 4-bit. DaSS vẫn vượt trội hơn wanda sau lượng tử hóa 4-bit.

A.3 Kết quả MMLU

Công trình gần đây (Gromov et al., 2024) chỉ ra hành vi bất thường của LLMs trong việc thực hiện các nhiệm vụ Hiểu biết Ngôn ngữ Đa nhiệm vụ Lớn (MMLU) (Hendrycks et al., 2021). Chúng tôi sử dụng Chain-of-Thought Hub (Fu et al., 2023) dựa trên triển khai chính thức của MMLU (Hendrycks et al., 2021). MMLU bao gồm 57 nhiệm vụ, trải dài từ STEM, Nhân văn, Khoa học Xã hội, trong số những lĩnh vực khác và chúng tôi báo cáo độ chính xác trung bình của 57 nhiệm vụ. Chúng tôi kiểm tra hiệu suất của mô hình LlaMA2-70B được tỉa trong nhiệm vụ MMLU. Từ Bảng 9, chúng tôi quan sát thấy rằng sự cải thiện của DaSS so với Wanda trở nên rõ ràng hơn trong nhiệm vụ MMLU thách thức

Bảng 8: Kết quả độ hỗn loạn Wikitext của LLaMA2-7B với độ thưa 50% và lượng tử hóa 4 bit

Phương pháp    SaprseGPT  Wanda  DaSS
Độ thưa 50%    6.38       6.50   6.44
với AWQ 4-bit  6.51       6.63   6.57

Bảng 9: Điểm MMLU cho các phương pháp và mức độ thưa khác nhau

Độ thưa  Phương pháp        MMLU (5 shot)
Dày đặc  -                  69.10
4:8      SparseGPT         60.24
         Wanda             59.69
         DaSS              60.86
         DaSS+bỏ qua 1/4   65.82
2:4      SparseGPT         56.99
         Wanda             56.41
         DaSS              57.53
         DaSS+bỏ qua 1/4   64.36
50%      SparseGPT         64.52
         Wanda             62.72
         DaSS              63.27
         DaSS+bỏ qua 1/4   66.81

MMLU, nơi nó đạt được mức tăng độ chính xác 1.17 và 1.12 cho các mẫu độ thưa 4:8 và 2:4, tương ứng. Phù hợp với các quan sát với Jaiswal et al. (2023), chúng tôi thấy sự suy giảm hiệu suất đáng kể trong nhiệm vụ MMLU chuyên sâu về kiến thức, trong đó chỉ các mô hình độ thưa không cấu trúc 50% vượt trội hơn mô hình LLaMA2-34B dày đặc. Vì GPU đóng vai trò quan trọng hơn trong suy luận mô hình lớn hơn, điều cần thiết là cải thiện hiệu suất của các mô hình được tỉa trong độ thưa N:M thân thiện với phần cứng. Giảm đồng đều mức độ thưa tổng thể không khả thi cho độ thưa N:M, Frantar & Alistarh (2023) đề xuất một tập con cụ thể của các lớp có thể được chọn để thưa hóa N:M đầy đủ. Ở đây chúng tôi bỏ qua tỉa 1/4 lớp liên tiếp (20 lớp) và dẫn đến tỷ lệ độ thưa cuối cùng 37.5%. Chúng tôi sử dụng 10 nhiệm vụ đầu tiên của MMLU để nghiên cứu độ nhạy tỉa. Chúng tôi chia mô hình thành 4 phần liên tiếp và nghiên cứu việc bỏ qua tỉa mỗi phần. Như thể hiện trong Bảng 10, các lớp trước nhạy cảm hơn các lớp sau trong các nhiệm vụ chuyên sâu kiến thức, trái ngược với các phát hiện trong Frantar & Alistarh (2023) sử dụng độ hỗn loạn, và phù hợp với các phát hiện của Gromov et al. (2024).

Chúng tôi tiếp tục tìm kiếm các lớp bỏ qua tốt hơn với phạm vi chỉ số lớp bắt đầu trong [0,20]. Chúng tôi phát hiện rằng bắt đầu bỏ qua từ lớp 10 có thể đạt được hiệu suất tốt nhất trong tập con. Sau đó chúng tôi kiểm tra kết quả của nó trong các nhiệm vụ MMLU đầy đủ. Như thể hiện trong Bảng 2, bỏ qua 1/4 lớp nhạy cảm có thể cải thiện đáng kể hiệu suất của các mô hình được tỉa, đặc biệt cho độ thưa N:M. Chúng tôi có thể đạt được các mô hình thưa hoạt động tốt hơn LLaMA2-34B. Mặc dù các mô hình độ thưa N:M một phần có nhiều tham số hơn các mô hình dày đặc nhỏ hơn, việc huấn luyện nhiều mô hình dày đặc nhỏ hơn như LLaMA2-34B vẫn tốn kém về mặt tính toán. Tỉa sau huấn luyện hiệu quả cho phép chúng tôi dễ dàng điều chỉnh sự đánh đổi độ chính xác-hiệu quả trong các ứng dụng thực tế.

Công trình gần đây (Gromov et al., 2024) đề xuất một phương pháp hiệu quả để loại bỏ khoảng một nửa số lớp trong LLaMA2-70B với mất độ chính xác nhỏ cho các nhiệm vụ MMLU, gợi ý hành vi bất thường của LLMs trong việc thực hiện các nhiệm vụ MMLU. Công trình của họ tập trung nhiều hơn vào tỉa cụ thể nhiệm vụ, trong khi công trình của chúng tôi nhấn mạnh việc bảo tồn

Bảng 10: Độ chính xác tập con MMLU sau khi bỏ qua tỉa 20 lớp tại các chỉ số bắt đầu khác nhau trong độ thưa 4:8.

Chỉ số Bắt đầu  0     10    20    40    60    Dày đặc
Độ chính xác (%) 60.12 61.72 59.75 56.54 56.83 64.86

khả năng tổng quát của LLMs để thực hiện nhiều nhiệm vụ khác nhau. Phương pháp tỉa trọng số của chúng tôi về cơ bản trực giao với tỉa lớp về nguyên tắc.
