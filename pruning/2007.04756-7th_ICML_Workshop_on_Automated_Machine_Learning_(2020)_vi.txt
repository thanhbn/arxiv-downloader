# 2007.04756.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2007.04756.pdf
# Kích thước tệp: 280719 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Hội thảo ICML lần thứ 7 về Học máy Tự động (2020)
Học cách Cắt tỉa Mạng nơ-ron Sâu
thông qua Học Tăng cường
Manas Gupta MANAS GUPTA @I2R.A-STAR .EDU.SG
I2R, ASTAR Singapore
Siddharth Aravindan SIDDHARTH .ARAVINDAN @COMP .NUS.EDU.SG
Đại học Quốc gia Singapore
Aleksandra KaliszyKALISZ .OLA@GMAIL .COM
I2R, ASTAR Singapore
Vijay ChandrasekharyVIJAY @I2R.A-STAR .EDU.SG
I2R, ASTAR Singapore
Lin Jie LIN-J@I2R.A-STAR .EDU.SG
I2R, ASTAR Singapore
Tóm tắt
Bài báo này đề xuất PuRL - một thuật toán dựa trên học tăng cường sâu (RL) để cắt tỉa mạng nơ-ron. Khác với các phương pháp nén mô hình dựa trên RL hiện tại chỉ đưa ra phản hồi vào cuối mỗi episode cho agent, PuRL cung cấp phần thưởng ở mỗi bước cắt tỉa. Điều này cho phép PuRL đạt được độ thưa thớt và độ chính xác tương đương với các phương pháp tiên tiến hiện tại, trong khi có chu kỳ huấn luyện ngắn hơn nhiều. PuRL đạt được hơn 80% độ thưa thớt trên mô hình ResNet-50 trong khi vẫn giữ được độ chính xác Top-1 là 75.37% trên tập dữ liệu ImageNet. Thông qua các thí nghiệm của chúng tôi, chúng tôi cho thấy PuRL cũng có thể làm thưa thớt các kiến trúc đã hiệu quả như MobileNet-V2. Ngoài các thí nghiệm đặc tính hóa hiệu suất, chúng tôi cũng cung cấp thảo luận và phân tích về các lựa chọn thiết kế RL khác nhau đã được sử dụng để tinh chỉnh Quá trình Quyết định Markov cơ bản của PuRL. Cuối cùng, chúng tôi chỉ ra rằng PuRL đơn giản để sử dụng và có thể dễ dàng thích ứng với các kiến trúc khác nhau.

1. Giới thiệu
Hiệu quả của mạng nơ-ron là quan trọng đối với các ứng dụng cụ thể, ví dụ, triển khai trên các thiết bị biên và các cân nhắc về khí hậu Strubell et al. (2019). Cắt tỉa trọng số đã nổi lên như một phương pháp giải pháp khả thi cho nén mô hình Han et al. (2016), nhưng cắt tỉa trọng số hiệu quả vẫn là một nhiệm vụ khó khăn - không gian tìm kiếm của các hành động cắt tỉa là lớn, và việc cắt tỉa quá mức trọng số (hoặc cắt tỉa chúng theo cách sai) dẫn đến các mô hình thiếu sót Frankle et al. (2019); Deng et al. (2009).

Trong công trình này, chúng tôi tiếp cận vấn đề cắt tỉa từ góc độ ra quyết định, và đề xuất tự động hóa quá trình cắt tỉa trọng số thông qua học tăng cường (RL). RL cung cấp một khung làm việc có nguyên tắc và có cấu trúc cho cắt tỉa mạng, nhưng vẫn chưa được khám phá đầy đủ. Dường như chỉ có một phương pháp cắt tỉa dựa trên RL hiện có, đó là AutoML cho Nén Mô hình (AMC) He et al. (2018). Ở đây, chúng tôi xây dựng dựa trên AMC và đóng góp một khung làm việc cải tiến: Cắt tỉa sử dụng Học Tăng cường (PuRL).

So với AMC, PuRL dựa trên một Quá trình Quyết định Markov (MDP) khác cho cắt tỉa. Một khía cạnh chính của mô hình chúng tôi là việc cung cấp "phần thưởng dày đặc" - thay vì dựa vào các

yĐối với các đóng góp được thực hiện vào năm 2019
c
2020 M. Gupta, S. Aravindan, A. Kalisz, V . Chandrasekhar and L. Jie.arXiv:2007.04756v1  [cs.AI]  9 Jul 2020

--- TRANG 2 ---
GUPTA , ARAVINDAN , KALISZ , CHANDRASEKHAR , JIE
phần thưởng "thưa thớt" (chỉ được đưa ra ở cuối mỗi episode), chúng tôi định hình hàm phần thưởng để cung cấp phản hồi phần thưởng ở mỗi bước của quá trình cắt tỉa. Điều này dẫn đến chu kỳ huấn luyện ngắn hơn nhiều và giảm số episode huấn luyện cần thiết tới 85%. Các thay đổi thiết kế còn lại được thông báo bởi các thí nghiệm kiểu ablation; chúng tôi thảo luận chi tiết về những thay đổi này và làm rõ sự cân bằng của các cấu hình MDP khác nhau.

2. Công trình Liên quan
Nhiều kỹ thuật đã được đề xuất để nén mạng nơ-ron Cheng et al. (2017). Cắt tỉa nổi lên như một phương pháp tổng quát không có hạn chế về các nhiệm vụ mà nó có thể áp dụng. Tuy nhiên, cắt tỉa cũng có kích thước không gian tìm kiếm lớn và do đó, truyền thống, người ta đã dựa vào chuyên môn của con người để thực hiện cắt tỉa. Nhưng với sự ra đời của các kỹ thuật tìm kiếm mới như học tăng cường sâu, giờ đây chúng ta có thể tự động hóa quá trình cắt tỉa. Trong Runtime Neural Pruning, Lin et al. (2017) trình bày một phương pháp sớm để sử dụng RL cho cắt tỉa. Họ sử dụng RL để thực hiện lựa chọn mạng con trong quá trình suy luận. Do đó, họ thực sự không cắt tỉa mạng, mà chọn một mạng con để thực hiện suy luận. He et al. (2018) trình bày việc sử dụng RL đầu tiên cho cắt tỉa. Tuy nhiên, họ chỉ thưởng cho agent ở cuối một episode (phần thưởng thưa thớt) và không đưa ra bất kỳ củng cố nào ở mỗi bước trong episode. Điều này làm chậm quá trình học của agent RL. Chúng tôi cải thiện điều này bằng cách tạo ra một thủ tục huấn luyện mới thưởng cho agent ở mỗi bước của episode (phần thưởng dày đặc) và đạt được hội tụ nhanh hơn. Phương pháp của chúng tôi cũng có tính chất tổng quát và có thể dễ dàng thích ứng với các kiến trúc khác nhau. Chúng tôi so sánh và báo cáo hiệu suất của chúng tôi đối với AMC và các thuật toán cắt tỉa tiên tiến khác trên tập dữ liệu ImageNet.

3. Cắt tỉa sử dụng Học Tăng cường (PuRL)
Phần này mô tả chi tiết PuRL, phương pháp học tăng cường của chúng tôi cho cắt tỉa mạng. Chúng tôi chính thức hóa cắt tỉa mạng như một MDP; chúng tôi xác định các thành phần cấu thành, cùng với trực giác cơ bản của thiết kế chúng.

3.1 Cắt tỉa như một Bài toán Quyết định Markov
Chúng tôi mô hình hóa nhiệm vụ cắt tỉa một mạng nơ-ron như một Bài toán Quyết định Markov (MDP). Chúng tôi công thức hóa và xây dựng từng thành phần của bộ tuple MDP tức là hS;A;R;T;πi để cho phép chúng tôi sử dụng RL cho cắt tỉa. Trong các phần phụ dưới đây, chúng tôi diễn giải chi tiết từng thành phần của tuple.

3.1.1 BIỂU DIỄN TRẠNG THÁI
Chúng tôi biểu diễn trạng thái mạng s thông qua một tuple các đặc trưng. Chúng tôi thí nghiệm với hai loại biểu diễn khác nhau. Đầu tiên là một sơ đồ biểu diễn đơn giản bao gồm ba đặc trưng, s=hl;a;pi, trong đó l là chỉ số của lớp đang được cắt tỉa, a là độ chính xác hiện tại đạt được trên tập kiểm tra (sau khi huấn luyện lại) và p tương ứng với tỷ lệ trọng số đã được cắt tỉa cho đến nay. Các thuộc tính phục vụ như các chỉ báo của trạng thái mạng. Biểu diễn thứ hai là một biểu diễn chiều cao hơn nhằm nắm bắt thông tin chi tiết hơn về trạng thái của mạng. Nó được công thức hóa như, s=ha1;p1;a2;p2;::;a n;pni, trong đó ai là độ chính xác kiểm tra sau khi cắt tỉa lớp i và thực hiện huấn luyện lại và pi là tỷ lệ phần trăm thưa thớt của lớp i. s là một tuple các số không ở đầu

--- TRANG 3 ---
HỌC CÁCH CẮT TỈA MẠNG NƠ-RON SÂU THÔNG QUA HỌC TĂNG CƯỜNG
mỗi episode. Mỗi phần tử tuple được cập nhật từng bước khi lớp i được cắt tỉa. Chúng tôi báo cáo kết quả từ cả hai biểu diễn trạng thái trong các thí nghiệm ablation.

3.1.2 KHÔNG GIAN HÀNH ĐỘNG
Không gian hành động bao gồm các hành động trong đó mỗi hành động tương ứng với một giá trị α quyết định lượng cắt tỉa. Chúng tôi sử dụng một ngưỡng độ lớn được dẫn xuất từ độ lệch chuẩn của các trọng số của một lớp làm tiêu chí cắt tỉa. Chúng tôi cắt tỉa tất cả các trọng số nhỏ hơn ngưỡng này về độ lớn tuyệt đối. Tập hợp các trọng số bị cắt tỉa khi một Hành động αi được thực hiện cho lớp i được đưa ra bởi Phương trình 1.

Trọng số Cắt tỉa αi = {w|‖w‖ < αi·σ(wi)} (1)

trong đó σ(wi) là độ lệch chuẩn của các trọng số trong lớp i. Để giảm thêm độ phức tạp tìm kiếm, chúng tôi cũng thí nghiệm với việc tăng kích thước bước của các hành động từ 0.1 lên 0.2, để lấy mẫu ít hành động hơn nhưng vẫn đạt được cùng tỷ lệ cắt tỉa mục tiêu. Chúng tôi báo cáo kết quả trong các thí nghiệm ablation. Chúng tôi sử dụng cùng một không gian hành động cho tất cả các lớp trong mạng tức là α ∈ {0.0; 0.1; 0.2; ...; 2.2}. Điều này trái ngược với các phương pháp hiện tại như AMC và State of Sparsity đặt một phạm vi cắt tỉa khác nhau cho các lớp ban đầu, để cắt tỉa chúng với lượng ít hơn. Do đó phương pháp của chúng tôi tổng quát hơn về khía cạnh này.

3.1.3 HÀM PHẦN THƯỞNG
Vì agent RL học chính sách thưa thớt tối ưu dựa trên mục tiêu tối đa hóa tổng phần thưởng mỗi episode, việc định hình phần thưởng giúp hội tụ nhanh hơn Ng et al. (1999). Chúng tôi công thức hóa tổng phần thưởng là một tích lũy của các phần thưởng phụ phụ thuộc vào độ chính xác kiểm tra và thưa thớt đạt được. Hàm phần thưởng tương ứng với một trạng thái s được đưa ra trong Phương trình 2. Ở đây, A(s) và P(s) biểu thị độ chính xác kiểm tra và thưa thớt tại trạng thái s, TA và TP biểu thị độ chính xác mong muốn và mục tiêu thưa thớt được đặt bởi người dùng, và λ tương ứng với một hệ số tỷ lệ cố định là 5.

R(s) = λ·(max(1-A(s)/TA; 0) + max(1-P(s)/TP; 0)) (2)

Thiết kế phần thưởng đảm bảo rằng agent tối ưu hóa chung cho thưa thớt và độ chính xác mong muốn.

3.2 Thuật toán PuRL
Chúng tôi thiết kế thuật toán PuRL để nhanh và hiệu quả khi giải quyết MDP được đề cập ở trên. Khía cạnh đầu tiên của điều này là lựa chọn một agent RL tốt. Cân nhắc thứ hai và quan trọng hơn là sơ đồ phần thưởng tức là phần thưởng thưa thớt so với dày đặc. Chúng tôi diễn giải chi tiết từng cái trong các phần phụ dưới đây.

3.2.1 LỰA CHỌN AGENT RL
Để giải quyết MDP, chúng tôi chọn trong số các thuật toán RL khác nhau có sẵn. Trọng tâm chính của chúng tôi là hiệu quả mẫu và độ chính xác. Deep Q-Network (DQN) Mnih et al. (2013), một dạng của Q-learning, thực hiện khám phá rất nhanh, tuy nhiên, nó không ổn định lắm. Thông qua thiết kế cẩn thận cấu trúc phần thưởng của chúng tôi, chúng tôi làm cho DQN ổn định và do đó, sử dụng nó để thực hiện cắt tỉa.

--- TRANG 4 ---
GUPTA , ARAVINDAN , KALISZ , CHANDRASEKHAR , JIE

Hình 1: Cái nhìn tổng quan về thuật toán PuRL với phần thưởng dày đặc. PuRL gán một tỷ lệ nén duy nhất α cho mỗi lớp. Sau đó nó nhận phản hồi về độ chính xác kiểm tra và thưa thớt đạt được, sau khi cắt tỉa lớp đó. Điều này trái ngược với các phương pháp hiện tại chỉ đưa phản hồi ở cuối việc cắt tỉa toàn bộ mạng. Kết quả là, PuRL học chính sách thưa thớt tối ưu nhanh hơn 85% so với các phương pháp hiện tại

3.2.2 LÀM CHO RL NHANH: PHẦN THƯỞNG DÀY ĐẶC
Thủ tục cắt tỉa bao gồm việc cắt tỉa các trọng số trong một lớp dựa trên độ lớn của chúng trước. Các trọng số còn lại sau đó được huấn luyện lại để lấy lại độ chính xác. Huấn luyện lại là một khía cạnh quan trọng của quá trình này.

Bằng cách đặt một α khác nhau cho mỗi lớp, chúng tôi cố gắng cắt tỉa bỏ sự dư thừa tối đa cụ thể cho từng lớp. Như đã đề cập trong phần 2, một cách điều này đã được thực hiện là 1) gán alpha cho mỗi lớp 2) cắt tỉa mỗi lớp và 3) huấn luyện lại mạng đã cắt tỉa ở cuối. Mặc dù phương pháp này hoạt động, như được chỉ ra bởi He et al. (2018), nó có thể không nhanh nhất vì nó không trực tiếp quy kết độ chính xác cho α của mỗi lớp. Nói cách khác, vì việc huấn luyện lại chỉ được tiến hành sau khi cắt tỉa tất cả các lớp chứ không phải sau mỗi lớp (phần thưởng thưa thớt), mạng không thể suy luận trực tiếp độ chính xác liên kết với α của mỗi lớp như thế nào. Điều này có thể kéo dài thời gian huấn luyện vì cần nhiều mẫu hơn để suy ra ảnh hưởng của α của mỗi lớp đến độ chính xác cuối cùng của mạng.

Chúng tôi cố gắng giảm thiểu điều này bằng cách đưa phần thưởng sau khi cắt tỉa mỗi lớp thay vì đưa chúng ở cuối episode (phần thưởng dày đặc). Chúng tôi huấn luyện lại mạng sau khi cắt tỉa mỗi lớp để có giá trị độ chính xác kiểm tra. Chúng tôi thực hiện huấn luyện lại bằng cách chỉ sử dụng một tập huấn luyện nhỏ gồm 1000 hình ảnh trong trường hợp thí nghiệm ImageNet, để không thêm chi phí huấn luyện. Chúng tôi đo độ chính xác sau khi mỗi lớp được cắt tỉa và chuyển nó cho agent thông qua phần thưởng và nhúng trạng thái. Như đã đề cập trong phần 4, phương pháp đưa phần thưởng dày đặc này giúp đạt được hội tụ nhanh hơn nhiều so với việc đưa phần thưởng thưa thớt. Cái nhìn tổng quan về thuật toán PuRL được trình bày trong Hình 1.

4. Thí nghiệm & Phân tích
Trong phần này, chúng tôi mô tả các thí nghiệm tính toán so sánh PuRL với các biến thể ablated, cũng như các phương pháp cơ sở và tiên tiến. Mục tiêu chính của chúng tôi là làm rõ ảnh hưởng của các lựa chọn thiết kế khác nhau (được mô tả trong phần 4.1) đến hiệu suất cắt tỉa. Thứ hai, chúng tôi chứng minh rằng PuRL đạt được kết quả tương đương với tiên tiến trong khi sử dụng chu kỳ huấn luyện RL ngắn hơn 85% bằng cách kiểm tra nó trên các tập dữ liệu CIFAR-100 và ImageNet và các kiến trúc khác nhau như ResNet-50, MobileNet-V2 và WideResNet-28-10 (tham khảo phần 4.2 và B.3). Cuối cùng, chúng tôi thể hiện khả năng tổng quát hóa của PuRL bằng cách sử dụng cùng cài đặt để cắt tỉa tất cả các kiến trúc trên ImageNet.

4.1 Hiểu Không gian Thiết kế RL
Chúng tôi tiến hành một loạt thí nghiệm ablation để hiểu những thành phần nào của không gian thiết kế RL giúp tạo ra một agent RL tốt. Các lựa chọn mang lại kết quả vượt trội so với baseline sau đó được sử dụng. Do hạn chế về không gian, chúng tôi diễn giải chi tiết một số lựa chọn trong Phụ lục B.1. Chúng tôi thí nghiệm trên kiến trúc ResNet-50 được huấn luyện trên tập dữ liệu CIFAR-10. Chúng tôi đặt mục tiêu thưa thớt 60% và độ chính xác mục tiêu 95% cho agent của chúng tôi (thông qua hàm phần thưởng), trong tất cả các thí nghiệm.

[Bảng 1 với kết quả thí nghiệm chi tiết về không gian Trạng thái, Hành động và Phần thưởng]

4.1.1 PHẦN THƯỞNG DÀY ĐẶC CÓ TỐT HỢN PHẦN THƯỞNG THƯA THỚT KHÔNG?
Chúng tôi so sánh phần thưởng thưa thớt tức là phần thưởng chỉ được đưa cho agent ở cuối episode và phần thưởng dày đặc tức là phần thưởng ở mỗi bước của episode, và cố gắng trả lời cái nào tốt hơn. Tham khảo Bảng 1, chúng tôi so sánh phần thưởng thưa thớt (hàng 1) với phần thưởng dày đặc (hàng 2). Phương pháp phần thưởng dày đặc của chúng tôi vượt trội hơn phần thưởng thưa thớt với biên độ lớn, 4% về thưa thớt và 24% về độ chính xác. Phần thưởng dày đặc giúp agent học nhanh hơn nhiều bằng cách hướng dẫn agent ở mỗi bước thay vì chỉ ở cuối episode. Sau đó chúng tôi sử dụng phần thưởng dày đặc làm baseline để tiến hành tất cả các ablation tiếp theo.

4.1.2 ÍT HÀNH ĐỘNG HƠN CÓ TỐT HƠN KHÔNG?
Trong thí nghiệm sử dụng Action 2 (hàng 6), chúng tôi sửa đổi không gian hành động để bao phủ cùng độ rộng hành động nhưng có ít hành động hơn. Vậy phạm vi vẫn giữ nguyên nhưng kích thước bước giữa các hành động tăng lên. Vậy thay vì các hành động là (0.0, 0.1, .., 2.2), giờ chúng ta có (0.0, 0.2, .., 2.2). Chúng ta thấy rằng thí nghiệm này Pareto thống trị baseline tức là nó vượt qua baseline ở

--- TRANG 5 ---
GUPTA , ARAVINDAN , KALISZ , CHANDRASEKHAR , JIE

[Bảng 2 so sánh PuRL với các phương pháp cắt tỉa tiên tiến trên ImageNet]

cả thưa thớt và độ chính xác. Điều này có thể do với số lượng hành động ít hơn để thử, agent có thể lấy mẫu mỗi hành động nhiều hơn và có được hiểu biết tốt hơn về mỗi hành động so với các chỉ số hiệu suất kết quả. Do đó, nó chọn ra các hành động tốt hơn tức là học một chính sách cắt tỉa tốt hơn cho một lớp cụ thể trong mạng.

4.2 Tổng quát hóa trên ImageNet
Để đánh giá hiệu suất của agent trên các nhiệm vụ quy mô lớn, chúng tôi thí nghiệm với tập dữ liệu ImageNet. Chúng tôi cắt tỉa một mô hình ResNet-50 đã được huấn luyện trước bằng sơ đồ cắt tỉa lặp như đã đề cập trong Han et al. (2015) để bảo tồn độ chính xác bằng cách cung cấp các mục tiêu cắt tỉa từ từ cho mạng. Chúng tôi so sánh hiệu suất của chúng tôi với các thuật toán cắt tỉa tiên tiến AMC: AutoML for Model Compression He et al. (2018) và State of Sparsity Gale et al. (2019). Chúng tôi cắt tỉa hơn 80% và đạt độ chính xác tương đương với các phương pháp tiên tiến (xem Bảng 2 để biết kết quả đầy đủ).

Hơn nữa, PuRL hoàn thành mỗi chu kỳ huấn luyện RL chỉ trong 55 episode, so với 400 episode cần thiết cho AMC, do thủ tục huấn luyện phần thưởng dày đặc. Chúng tôi cũng tiến hành thí nghiệm trên các kiến trúc hiệu quả tiên tiến khác như MobileNet-V2 Sandler et al. (2018) và EfficientNet-B2 Tan and Le (2019). Tham khảo tài liệu bổ sung, PuRL đạt được hơn 1.5x thưa thớt so với AMC mà không mất nhiều độ chính xác. Đồng thời, PuRL đạt được hiệu suất này trên MobileNet-V2 mà không có bất kỳ thay đổi nào trong các siêu tham số cơ bản so với ResNet-50. Do đó, PuRL có thể dễ dàng được sử dụng trên các kiến trúc mà không cần sửa đổi MDP cơ bản.

5. Kết luận
Trong bài báo này, chúng tôi trình bày PuRL - một thuật toán RL hoàn toàn tự động để thực hiện nén quy mô lớn các mạng nơ-ron. Bằng cách cải thiện cấu trúc phần thưởng so với các phương pháp hiện tại, chúng tôi rút ngắn chu kỳ huấn luyện của agent RL từ 400 xuống 55 episode. Chúng tôi tiếp tục thực hiện một tập chi tiết các thí nghiệm ablation để xác định tác động của mỗi thành phần MDP đến thưa thớt và độ chính xác cuối cùng đạt được bởi agent. Chúng tôi đạt được kết quả tương đương với các thuật toán cắt tỉa tiên tiến hiện tại trên tập dữ liệu ImageNet, làm thưa thớt mô hình ResNet-50 hơn 80% và đạt độ chính xác Top-1 là 75.37%. Chúng tôi cũng đánh giá PuRL trên các kiến trúc khác như WideResNet-28-10 bao gồm các kiến trúc đã hiệu quả như MobileNet-V2 và EfficientNet-B2. Cuối cùng, thuật toán của chúng tôi đơn giản để thích ứng với các kiến trúc mạng nơ-ron khác nhau và có thể được sử dụng để cắt tỉa mà không cần tìm kiếm cho mỗi thành phần MDP.

--- TRANG 6 ---
HỌC CÁCH CẮT TỈA MẠNG NƠ-RON SÂU THÔNG QUA HỌC TĂNG CƯỜNG

Lời cảm ơn
Nghiên cứu này được hỗ trợ bởi Agency for Science, Technology and Research (A*STAR) trong khuôn khổ AME Programmatic Funds (Dự án Số A1892b0026 và Số A19E3b0099).

Tài liệu tham khảo
Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks, 2017.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09 , 2009.

Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Stabilizing the lottery ticket hypothesis, 2019.

Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574 , 2019.

Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in neural information processing systems , pages 1135-1143, 2015.

Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. International Conference on Learning Representations , 2016.

Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 784-800, 2018.

Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.. cifar-100 (canadian institute for advanced research). URL http://www.cs.toronto.edu/ kriz/cifar.html.

Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30 , pages 2181-2191. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/6813-runtime-neural-pruning.pdf.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop , 2013.

Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML , volume 99, pages 278-287, 1999.

Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4510-4520, 2018.

--- TRANG 7 ---
GUPTA , ARAVINDAN , KALISZ , CHANDRASEKHAR , JIE

Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. 57th Annual Meeting of the Association for Computational Linguistics (ACL) , 2019.

Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. Proceedings of the 36th International Conference on Machine Learning , 2019.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Edwin R. Hancock Richard C. Wilson and William A. P. Smith, editors, Proceedings of the British Machine Vision Conference (BMVC) , pages 87.1-87.12. BMVA Press, September 2016. ISBN 1-901725-59-6. doi: 10.5244/C.30.87. URL https://dx.doi.org/10.5244/C.30.87.

--- TRANG 8 ---
HỌC CÁCH CẮT TỈA MẠNG NƠ-RON SÂU THÔNG QUA HỌC TĂNG CƯỜNG

Phụ lục A. Thuật toán PuRL
Thuật toán 1 mô tả một thủ tục DQN học cách chọn ngưỡng thưa thớt cho mỗi lớp trong mô hình. Ở đầu mỗi episode, mô hình gốc với các trọng số đã được huấn luyện trước được tải. Agent sau đó thực hiện một hành động và lớp được cắt tỉa theo lượng đó. Mô hình sau đó được huấn luyện lại trong một epoch trên một tập con nhỏ của dữ liệu huấn luyện (1000 hình ảnh trong số 1.2 triệu hình ảnh trong ImageNet). Sau đó, độ chính xác xác thực được tính toán, để thực hiện chuyển đổi trạng thái và phần thưởng sau đó được tính toán bằng độ chính xác xác thực và tỷ lệ phần trăm cắt tỉa. Việc huấn luyện được thực hiện trong max_episodes episode. Khi việc huấn luyện hoàn thành, mô hình được cắt tỉa bằng agent đã được huấn luyện và sau đó được tinh chỉnh trên toàn bộ tập dữ liệu ImageNet.

Thuật toán 1 Thuật toán PuRL
1:Giai đoạn 1: Huấn luyện agent DQN
2:episodes ← 0
3:while episodes < max_episodes do
4: model ← tải mô hình gốc
5: foreach lớp t trong mô hình do
6:   αt ← Lấy mẫu hành động từ agent DQN
7:   Cắt tỉa lớp t sử dụng αt
8:   Huấn luyện lại mô hình trên tập con nhỏ dữ liệu trong 1 epoch
9:   Tính phần thưởng rt và trạng thái mới st dựa trên thưa thớt và độ chính xác kết quả đạt được
10:  Trả về rt và st cho agent
11: episodes ← episodes + 1
12:
13:Giai đoạn 2: Cắt tỉa và tinh chỉnh mô hình
14:model ← tải mô hình gốc
15:Cắt tỉa mô hình sử dụng agent DQN đã được huấn luyện (trung bình trên 5 episode)
16:Tinh chỉnh mô hình
17:return mô hình

Phụ lục B. Kết quả Thí nghiệm
B.1 Hiểu Không gian Thiết kế RL

B.1.1 ĐỘ LỚN TUYỆT ĐỐI CÓ TỐT HƠN ĐỘ LỆCH CHUẨN KHÔNG?
Tham khảo ablation dựa trên Magnitude Target Bảng 1 (hàng 3), chúng tôi so sánh cắt tỉa dựa trên độ lớn tuyệt đối với cắt tỉa dựa trên độ lệch chuẩn (Phần 3). Đối với độ lớn tuyệt đối, chúng tôi đặt mục tiêu thưa thớt cho một lớp và sau đó loại bỏ tất cả các trọng số nhỏ cho đến khi chúng tôi đạt mức thưa thớt mong muốn. Như chúng tôi quan sát, cả kết quả thưa thớt và độ chính xác đều thấp hơn trong trường hợp này so với thí nghiệm baseline của chúng tôi (phần thưởng dày đặc). Do đó, cắt tỉa dựa trên độ lệch chuẩn tốt hơn.

B.1.2 ĐỊNH HÌNH PHẦN THƯỞNG CÓ GIÚP ÍCH KHÔNG?
Trong các thí nghiệm sử dụng Reward 2 và Reward 3 (Bảng 1), chúng tôi điều tra xem định hình phần thưởng có thể giúp agent đạt độ chính xác cao hơn không. Đối với Reward 2, chúng tôi cho phép agent nhận phần thưởng tích cực nếu nó vượt qua mục tiêu độ chính xác đã cho (Phương trình 3). Điều này trái ngược với hàm phần thưởng baseline trong đó có một giới hạn trên về phần thưởng tối đa mà agent có thể đạt được tức là không.

R2=λ·((A/TA-1) + max(1-P/TP;0)) (3)

Đối với Reward 3, chúng tôi xây dựng dựa trên Reward 2 và đưa phần thưởng khối cho agent. Agent giờ thấy tăng trưởng khối trong củng cố tích cực khi nó tiếp cận và vượt qua mục tiêu độ chính xác (Phương trình 4). Do đó, bằng cách thực hiện cùng kích thước bước hướng tới cải thiện độ chính xác so với Reward 2, agent giờ được thưởng nhiều hơn cho điều đó.

R3=λ·(((A/TA)³-1) + max(1-P/TP;0)) (4)

Hiệu suất của cả hai hàm này gần với baseline (phần thưởng dày đặc), nhưng baseline vẫn vượt trội hơn chúng. Độ phức tạp thêm vào của các hàm này có thể yêu cầu agent lấy mẫu nhiều bước hơn để học chúng tốt. Do đó, với ngân sách huấn luyện chặt chẽ, hàm phần thưởng baseline hoạt động tốt.

B.1.3 THÔNG TIN NHIỀU HƠN CÓ TỐT HƠN CHO AGENT KHÔNG?
Trong thí nghiệm cuối cùng với State 2 (Bảng 1), chúng tôi thay đổi không gian trạng thái và làm cho nó có 108 chiều thay vì 3 chiều. Ý tưởng ở đây là đưa cho agent nhiều thông tin hơn về biểu diễn trạng thái (Xem Phần 3 để biết chi tiết). Chúng tôi thấy rằng trong thí nghiệm này, agent đạt độ chính xác ít hơn baseline tuy nhiên cắt tỉa nhiều hơn. Do đó, không có thí nghiệm nào Pareto thống trị lẫn nhau và không có kết luận để xác định cái nào tốt hơn. Để có thêm bằng chứng về điều này, chúng tôi tiến hành ablation tiếp theo trên tập dữ liệu ImageNet. Tham khảo Bảng 3, chúng ta thấy rằng trạng thái 108 chiều vượt trội hơn trạng thái 3 chiều. Do đó, thông tin nhiều hơn thực sự tốt hơn và chúng tôi sử dụng tính năng này trong cấu hình cuối cùng.

B.2 Mở rộng PuRL lên CIFAR100
Chúng tôi đầu tiên thí nghiệm với PuRL trên kiến trúc WideResNet-28-10 Zagoruyko and Komodakis (2016) trên tập dữ liệu CIFAR-100 Krizhevsky et al.. Chúng tôi so sánh nó với baseline cắt tỉa đồng đều nơi mỗi lớp được cắt tỉa cùng lượng để đạt mục tiêu thưa thớt 93.5%. PuRL vượt trội hơn baseline trong Bảng 4 về cả thưa thớt và độ chính xác cuối cùng.

--- TRANG 9 ---
HỌC CÁCH CẮT TỈA MẠNG NƠ-RON SÂU THÔNG QUA HỌC TĂNG CƯỜNG

[Bảng 3: Thí nghiệm tiếp theo về việc thay đổi Không gian Trạng thái trên tập dữ liệu ImageNet]

[Bảng 4: So sánh thuật toán PuRL với baseline cắt tỉa đồng đều trên kiến trúc WideResNet-28-10 trên tập dữ liệu CIFAR-100]

B.3 Tổng quát hóa trên ImageNet
Chúng tôi cũng tiến hành thí nghiệm trên các kiến trúc hiệu quả tiên tiến khác trên ImageNet để xem liệu thuật toán cắt tỉa của chúng tôi có thể làm cho các kiến trúc này thậm chí còn thưa thớt hơn không. Chúng tôi thí nghiệm trên MobileNet-V2 Sandler et al. (2018) và EfficientNet-B2 Tan and Le (2019). Tham khảo Bảng 5, PuRL đạt được hơn 1.5x thưa thớt so với AMC mà không mất nhiều độ chính xác.

Đồng thời, PuRL đạt được hiệu suất này trên MobileNet-V2 mà không có bất kỳ thay đổi nào trong các siêu tham số cơ bản so với ResNet-50. Do đó, PuRL có thể dễ dàng được sử dụng trên các kiến trúc mà không cần sửa đổi MDP cơ bản. Đối với EfficientNet-B2, Bảng 6, chúng tôi so sánh PuRL với baseline cắt tỉa đồng đều. PuRL vượt trội hơn baseline về cả thưa thớt và độ chính xác cuối cùng, đạt được cải thiện độ chính xác hơn 5%. Trong trường hợp này, chúng tôi cũng đặt chính xác cùng siêu tham số và cài đặt MDP như ResNet-50 và MobileNet-V2. Tuy nhiên, vì Efficient-B2 rất sâu, có 116 lớp so với 54 trong ResNet-50, chúng tôi thực hiện dừng sớm episode RL, để làm cho việc huấn luyện thậm chí nhanh hơn. Chúng tôi dừng episode nếu độ chính xác kiểm tra giảm xuống dưới 0.1% và chuyển sang episode tiếp theo.

[Bảng 5: So sánh PuRL với AMC cho kiến trúc MobileNet-V2]

[Bảng 6: So sánh PuRL với baseline cắt tỉa đồng đều trên kiến trúc tiên tiến EfficientNet-B2 trên tập dữ liệu ImageNet]
