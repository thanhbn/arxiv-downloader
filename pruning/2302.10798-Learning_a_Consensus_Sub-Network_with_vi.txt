# 2302.10798.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2302.10798.pdf
# Kích thước tệp: 1379958 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Học Một Sub-Network Đồng Thuận với
Regularization Phân Cực và Huấn Luyện Một Lần
Xiaoying Zhi, Varun Babbar, Rundong Liu, Pheobe Sun,
Fran Silavong, Ruibo Shi, Sean Moran
JPMorgan Chase, 25 Bank Street, London, E145JP, UK.
Tác giả đóng góp: xiaoying.zhi@jpmchase.com;
varun.babbar@jpmchase.com; eric.liu@jpmchase.com;
pheobe.sun@jpmchase.com; fran.silavong@jpmchase.com;
ruibo.shi@jpmchase.com; sean.j.moran@jpmchase.com;
Tóm tắt
Chủ đề về AI xanh đã nhận được sự chú ý trong cộng đồng học sâu do xu hướng gần đây của các mô hình mạng nơ-ron ngày càng lớn và phức tạp hơn. Các giải pháp hiện tại để giảm tải tính toán trong quá trình huấn luyện tại thời điểm suy luận thường liên quan đến việc cắt tỉa các tham số mạng. Các sơ đồ cắt tỉa thường tạo ra chi phí phụ thêm bằng cách huấn luyện và tinh chỉnh lặp đi lặp lại cho việc cắt tỉa tĩnh hoặc tính toán lặp đi lặp lại của đồ thị cắt tỉa động. Chúng tôi đề xuất một chiến lược cắt tỉa tham số mới để học một sub-network nhẹ hơn nhằm giảm thiểu chi phí năng lượng trong khi duy trì hiệu suất tương đương với mạng được tham số hóa đầy đủ trên các tác vụ downstream nhất định. Sơ đồ cắt tỉa được đề xuất của chúng tôi hướng tới xanh, vì nó chỉ yêu cầu một lần huấn luyện để khám phá các sub-network tĩnh tối ưu bằng các phương pháp cắt tỉa động. Sơ đồ cắt tỉa bao gồm một mô-đun gating nhị phân và một hàm mất mát phân cực để khám phá các sub-network với độ thưa do người dùng định nghĩa. Phương pháp của chúng tôi cho phép cắt tỉa và huấn luyện đồng thời, giúp tiết kiệm năng lượng trong cả giai đoạn huấn luyện và suy luận và tránh chi phí tính toán phụ từ các mô-đun gating tại thời điểm suy luận. Kết quả của chúng tôi trên CIFAR-10, CIFAR-100, và Tiny Imagenet cho thấy rằng sơ đồ của chúng tôi có thể loại bỏ ~=50% kết nối trong các mạng sâu với <=1% giảm độ chính xác phân loại. So với các phương pháp cắt tỉa liên quan khác, phương pháp của chúng tôi thể hiện mức giảm độ chính xác thấp hơn cho việc giảm chi phí tính toán tương đương.
Từ khóa: Cắt Tỉa Kiến Trúc Nơ-ron, Học Máy, Thị Giác Máy Tính.
1arXiv:2302.10798v5  [cs.LG]  10 Jan 2025

--- TRANG 2 ---
1 Giới thiệu
Các mô hình lớn, thưa và được tham số hóa quá mức mang lại hiệu suất tối tân (SOTA) trên nhiều tác vụ nhưng yêu cầu đáng kể nhiều sức mạnh tính toán hơn và do đó năng lượng so với các mô hình học máy thông thường [1]. Ví dụ, mô hình vision transformer (ViT-L16) với 307M tham số có thể đạt 99.42% độ chính xác trên bộ dữ liệu CIFAR-10 và 87.76% trên bộ dữ liệu ImageNet [2]. Việc huấn luyện mô hình ViT-L16 yêu cầu 680 TPUv3-core-days¹ và tiêu thụ năng lượng 3672kWh, tương đương với 32.5% tiêu thụ năng lượng hàng năm của một hộ gia đình Mỹ trung bình [2-4].
Cắt tỉa mạng là một hướng nghiên cứu đầy hứa hẹn giúp đạt được các mô hình AI xanh hơn, dựa trên giả định rằng chúng ta có thể loại bỏ an toàn các tham số khỏi các mạng được tham số hóa quá mức, mà không làm suy giảm đáng kể hiệu suất mạng [5].
Có hai loại phương pháp cắt tỉa mạng phổ biến - tĩnh và động. Cắt tỉa mạng tĩnh tạo ra một sub-network thống nhất cho tất cả dữ liệu, trong khi cắt tỉa động tính toán các sub-network khác nhau cho từng mẫu dữ liệu. Cắt tỉa mạng tĩnh thường yêu cầu một thước đo tầm quan trọng nơ-ron được định nghĩa trước để xác định nơ-ron nào đã được huấn luyện nên được cắt tỉa [6-10]. Việc tinh chỉnh thêm hoặc tái phát triển sub-network được chọn thường được thực hiện sau huấn luyện, có thể dẫn đến cải thiện hiệu suất thêm [11-13]. Mặt khác, cắt tỉa động áp dụng một hàm gating được tham số hóa và có thể học được tính toán tầm quan trọng nơ-ron trong thời gian thực, dẫn đến một đồ thị tính toán khác nhau cho từng mẫu dữ liệu. Giai đoạn huấn luyện tối ưu hóa các hàm gating có thể học được với một mất mát thực nghiệm, và giai đoạn suy luận tính toán sub-network phù hợp thông qua lan truyền tiến qua các mô-đun gating [14-17].
Từ góc độ AI xanh, cả hai cách tiếp cận cắt tỉa động hay tĩnh đều không lý tưởng. Cắt tỉa động không tối ưu cho tính toán song song do các thao tác đánh chỉ mục cần thiết tại thời điểm suy luận. Hơn nữa, cắt tỉa động giới thiệu chi phí phụ từ các tính toán tầm quan trọng kết nối cần thiết. Cắt tỉa tĩnh có thể giảm tài nguyên tính toán tại thời điểm suy luận, nhưng quá trình cắt tỉa và tinh chỉnh lặp đi lặp lại tiêu thụ nhiều tài nguyên tính toán hơn trong giai đoạn huấn luyện. Cắt tỉa một lần sau huấn luyện cũng không tốt hơn thủ tục lặp vì hiệu quả của nó phụ thuộc nhiều vào các tiên nghiệm được giả định, thiếu xác minh về tính hợp lệ trước khi huấn luyện [18].
Phương pháp cắt tỉa được đề xuất của chúng tôi tạo thành một mạng nén mà không có chi phí của tài nguyên huấn luyện bổ sung đáng kể. Nó đạt được điều này bằng cách đồng thời tối ưu hóa cấu trúc mạng và các tham số trong một lần. Chúng tôi cho rằng mô hình mới này mang lại một cải thiện đáng kể cho các yêu cầu hiệu quả trong các phương pháp cắt tỉa, đặc biệt là trong các môi trường hạn chế tài nguyên. Đặc biệt, việc tối ưu hóa đồng thời được thực hiện với một mô-đun gating nhị phân có thể huấn luyện nhẹ cùng với một regularizer phân cực. Regularizer phân cực tạo ra một sub-network ổn định hoạt động tốt cho tất cả các điểm dữ liệu về cuối huấn luyện. Thời gian suy luận được giảm vì sub-network tĩnh nhỏ hơn đã sẵn sàng sử dụng. Chúng tôi xác minh sơ đồ cắt tỉa trên hai loại cắt tỉa (lớp và kênh) trên các ResNet khác nhau [19], áp dụng cho ba bộ dữ liệu với kích thước và số lượng lớp khác nhau (CIFAR-10, CIFAR-100, và Tiny Imagenet[20, 21]). Các so sánh với các baseline cạnh tranh hiện có được trình bày.

¹Phép nhân số lượng lõi TPUv3 được sử dụng và thời gian huấn luyện tính bằng ngày
2

--- TRANG 3 ---
1.1 AI Xanh và AI Đỏ: Nâng Cao Nhận Thức về Chi Phí Năng Lượng
của AI
Schwartz et al. [1] là những tác giả đầu tiên định nghĩa các khái niệm về AI xanh (AI thân thiện với môi trường) và AI đỏ (AI tiêu thụ nhiều năng lượng), đề xuất rằng các mô hình AI nên được đánh giá vượt ra ngoài độ chính xác bằng cách tính đến phát thải carbon và sử dụng điện, thời gian trôi qua, số lượng tham số, và các phép toán dấu phẩy động (FPOs/FLOPs). Patterson et al. [22] và Dodge et al. [23] đề xuất các khung công tác định lượng phát thải carbon từ việc áp dụng các mô hình AI cụ thể trên các thiết bị phổ biến khác nhau. Để giảm phát thải carbon từ việc huấn luyện mô hình, các cách tiếp cận khác nhau đã được sử dụng phổ biến. Ví dụ, lượng tử hóa mô hình có thể được sử dụng để giảm thời gian trôi qua và sử dụng bộ nhớ bộ xử lý [24], trong khi các cách tiếp cận chưng cất mạng và cắt tỉa mạng có thể được sử dụng để giảm số lượng tham số và tổng FLOPs [25].

1.2 Cắt Tỉa Mạng
Cắt tỉa mạng nhằm xếp hạng tầm quan trọng của các cạnh trong một mô hình mạng nơ-ron để tìm một sub-network với các cạnh quan trọng nhất. Thường có hai cách tiếp cận để đạt được mục tiêu này: phương pháp tĩnh hoặc động. Cắt tỉa mạng tĩnh tìm một sub-network thống nhất ở cuối quá trình huấn luyện, và thường được theo sau bởi một thủ tục tinh chỉnh để cải thiện thêm hiệu suất sub-network. Sơ đồ cắt tỉa này dựa vào các điểm số tầm quan trọng được tính toán của các cạnh quan tâm. Tầm quan trọng cạnh có thể được tính toán, ví dụ, bằng độ lớn hoặc ảnh hưởng của một cạnh đến đầu ra cuối cùng.

Trong các mạng nơ-ron tích chập (CNN), cắt tỉa thường được thực hiện trong ba chiều: độ sâu (lớp), độ rộng (kênh), và độ phân giải (bản đồ đặc trưng) [26]. Các thí nghiệm về cắt tỉa bản đồ đặc trưng tĩnh [27] và cắt tỉa kênh [28] đã chứng minh giảm 30% FLOPs hoặc giảm 2.5× thời gian GPU chỉ với suy giảm hiệu suất không đáng kể hoặc thậm chí cải thiện trong một số trường hợp. Cai et al. [29] mở rộng vấn đề thành cắt tỉa đa giai đoạn để làm cho cách tiếp cận cắt tỉa có thể thích ứng với các yêu cầu kích thước khác nhau. Mục tiêu này được đạt được bằng cách huấn luyện và tinh chỉnh các sub-network với việc giảm kích thước tăng dần trong khi đảm bảo độ chính xác mô hình vẫn giữ nguyên mỗi khi yêu cầu kích thước được giảm.

Cắt tỉa động, mặt khác, nhằm tìm các sub-network phụ thuộc đầu vào. Các yếu tố phụ thuộc đầu vào thường được thêm vào mạng gốc để tính toán tầm quan trọng của các cạnh quan tâm. Veit và Belongie [14] đề xuất một đồ thị suy luận thích ứng tính toán tầm quan trọng của các lớp CNN với một mô-đun gating xác suất và có thể học được trước mỗi lớp. Lin et al. [30] đề xuất một khung công tác tương tự để cắt tỉa các kênh CNN bằng cách sử dụng học tăng cường để huấn luyện một cơ chế tính điểm tầm quan trọng kênh tối ưu.

Kết hợp cả hai phương pháp cắt tỉa tĩnh và động có thể đạt được lợi ích hiệp đồng trong hiệu quả tính toán. Một cách tiếp cận kết hợp có thể hưởng lợi từ tính tương thích của cắt tỉa tĩnh với tính toán song song, tiết kiệm năng lượng đặc biệt trên tính toán GPU. Cách tiếp cận này cũng có thể tận dụng khả năng của cắt tỉa động để tạo ra các mạng thích ứng với dữ liệu đầu vào. Ví dụ, Lee [31] đề xuất một phương pháp sparsification có thể vi phân phụ nơi các tham số có thể được làm về không sau tối ưu hóa dưới descent gradient ngẫu nhiên. Tuy nhiên, các sub-network không thống nhất vẫn gây ra tính toán đánh chỉ mục dư thừa trong tính toán song song. Công trình của chúng tôi tập trung vào vấn đề tìm một sub-network thống nhất cho dữ liệu theo một phân phối nhất định, bằng cách sử dụng các sub-network được cắt tỉa động như các trạng thái trung gian. Công trình gần đây về việc thống nhất sub-network đã liên quan đến việc cắt tỉa một biểu diễn đồ thị của mạng nơ-ron [32,33]

Ngoài ra, các phát hiện của chúng tôi xác nhận công trình gần đây về sự tồn tại của "vé số", tức là các sub-network được cắt tỉa có thể đạt độ chính xác tương tự như mạng gốc [34]. Để tạo ra các mạng như vậy, Frankle và Carbin [34] phát triển sơ đồ IMP (Iterative Magnitude Pruning) liên quan đến cắt tỉa và huấn luyện lặp đi lặp lại qua nhiều vòng cho đến hội tụ. Điều này khác với phương pháp được đề xuất của chúng tôi, thực hiện huấn luyện và cắt tỉa đồng thời trong một phiên huấn luyện và do đó rẻ hơn về mặt tính toán để huấn luyện.

1.3 Tính Ngẫu Nhiên Rời Rạc và Ước Lượng Gradient
Để có được một cấu trúc sub-network ổn định thông qua tối ưu hóa dựa trên gradient tức là trạng thái kích hoạt nhị phân cho mỗi kết nối, một mô-đun gating là cần thiết với các biến tiềm ẩn có thể vi phân và rời rạc. Các biến rời rạc thường yêu cầu một sự nới lỏng hoặc ước lượng do sự không tương thích của hàm rời rạc hóa với lan truyền ngược (ví dụ gradient bằng không ở mọi nơi).

Một cách tiếp cận ước lượng nổi tiếng là ước lượng viên Gumbel-Softmax (GS) [35]. Gumbel-Softmax là một phân phối xác suất liên tục có thể được điều chỉnh để xấp xỉ một phân phối phân loại rời rạc. Các gradient w.r.t. phân phối đầu ra phân loại được định nghĩa rõ ràng cho phân phối GS. Kỹ thuật này thường được áp dụng cho các mô hình chuỗi tạo sinh yêu cầu lấy mẫu dưới các phân phối đa thức [36-38].

Một cách tiếp cận được cho là đơn giản hơn, nhưng vẫn hiệu quả là ước lượng viên straight-through (STE) [39], nhị phân hóa đầu ra ngẫu nhiên dựa trên một ngưỡng trong lần truyền tiến, và sao chép gradient của lớp tiếp theo một cách heuristic cho ước lượng viên. Các thí nghiệm cho thấy rằng các mạng nơ-ron được gating bởi STE đưa ra tỷ lệ lỗi thấp nhất trong số các gate có thể vi phân khác (đa thức và không đa thức) [39]. Chúng tôi mô tả STE chi tiết hơn trong Phần 3.2.

1.4 Regularizer Sparsity
Với mục đích cắt tỉa tham số, một regularizer sparsity thường được sử dụng để kiểm soát tỷ lệ cắt tỉa trong quá trình huấn luyện. Các regularizer l1 và l2 là hai loại phổ biến nhất. Tuy nhiên, các hàm regularization tiêu chuẩn có thể dẫn đến cắt tỉa không cần thiết và ước lượng sai về tầm quan trọng kết nối mạng. Các regularizer nhạy cảm hơn với cấu trúc mạng bao gồm regularization sparsity có cấu trúc l2,0 và l2,1 [40], được nhóm trên các mẫu hoặc trên các bản đồ đặc trưng [41] v.v.

Srinivas và Srinivas và Babu [42] đề xuất một regularizer nhị phân hóa khuyến khích mỗi kết nối mạng tiếp cận hoặc 1 hoặc 0 cho tất cả các mẫu. Cơ chế nhị phân hóa cũng có thể được mở rộng để kích hoạt tỷ lệ liên tục. Ví dụ, Zhuang et al. [43] tích hợp một regularizer phân cực vào cắt tỉa mạng để buộc việc hủy kích hoạt các nơ-ron. Các mạng được cắt tỉa dưới thiết lập này đạt độ chính xác cao nhất ngay cả ở tỷ lệ cắt tỉa cao so với các sơ đồ cắt tỉa khác.

4

--- TRANG 4 ---
2 Thiết Lập Vấn Đề: Học Tham Số và Kiến Trúc Kết Hợp
Chúng tôi ký hiệu một mạng nơ-ron với kết nối đầy đủ dưới dạng một đồ thị là Φ := (V, E), trong đó V là một tập hợp các nút, E là tập hợp các cạnh E:={e(x,y), cho tất cả x, y trong V}. Một sub-network với kết nối một phần do đó có thể được biểu diễn là Φ′= (V, E′) trong đó E′⊆E. Chúng tôi cũng ký hiệu sự biến đổi của một mạng là f_theta(·)≡f(·;theta), trong đó theta ký hiệu tất cả các tham số trong một mạng. Mỗi cạnh e trong E được liên kết với một trọng số theta_e. Đối với mạng đầy đủ theta=theta_Φ, và đối với sub-network theta=theta_Φ′. Một sub-network có thể được biểu diễn theo mạng đầy đủ sử dụng một ma trận kích hoạt W_e với các phần tử nhất định được làm về không, tức là:
theta_Φ′=W⊤_e theta_Φ, (1)
trong đó w_e,c trong {0,1} cho mọi mục trong ma trận kích hoạt cạnh W_e là nhị phân, và theta_Φ′ được tính từ một tích Hadamard. Trong cắt tỉa mạng, chúng ta nhằm tìm một sub-network Φ′ và các tham số mạng tối ưu theta*_Φ′ đồng thời. Chúng ta ước lượng giải pháp tối ưu theta*_Φ′ với ˆtheta*_Φ′ bằng cách giảm thiểu mất mát thực nghiệm. Chúng ta tiếp tục sử dụng lại các thiết lập trong Eq.1 để cải tổ mục tiêu dưới đây: tức là
min_theta_Φ′,Φ′ L(f(x;theta_Φ′),y).
min_theta_Φ,W_e L(f(x;W⊤_e theta_Φ),y). (2)

3 Phương Pháp Luận
Trong thực tế, ma trận kích hoạt cạnh W_e không được học như một tổng thể và mỗi mục trong ma trận không độc lập. Khi huấn luyện một mạng tuần tự, việc kích hoạt các kết nối trước đó có thể ảnh hưởng đến đầu ra của các kết nối sau đó, và do đó cũng ảnh hưởng đến lan truyền gradient ngược. Một W_e nhị phân/phân loại ngây thơ sẽ ngăn cản gradient lan truyền ngược, vì một hàm có giá trị không đổi có gradient bằng không. Do đó, một ước lượng viên gradient là cần thiết như yếu tố gating cốt lõi của mỗi kết nối. Chúng tôi chọn ước lượng viên straight-through (STE), như được giới thiệu trong Phần 3.2, làm yếu tố cốt lõi này.

3.1 Kiến Trúc Mạng
Hình 1 minh họa thiết kế cho việc tích hợp mô-đun gating vào một ResNet. Sơ đồ cắt tỉa của chúng tôi có quy trình công việc hơi khác nhau cho giai đoạn huấn luyện và giai đoạn kiểm tra. Trong huấn luyện, các mô-đun gating với các lớp dày đặc có thể học được được huấn luyện như một phần của mạng. Tại suy luận (cho xác thực hoặc kiểm tra), W_e kết quả được tải, điều này quyết định tập con các tham số được chọn - chỉ các kết nối với w_e,c khác không sẽ được tải cho các tham số và được bao gồm trong lần truyền tiến.

Việc lựa chọn ResNet làm mạng cơ sở dựa trên sự cần thiết của các kết nối dư để tránh khả năng chấm dứt đường dẫn tính toán ở giữa mạng do một lớp đầy đủ bị hủy kích hoạt. Đối với ResNet, chúng tôi tập trung vào cắt tỉa lớp và kênh (bản đồ đặc trưng) tập trung vào CNN. Tuy nhiên, chúng tôi cũng lập luận rằng phương pháp luận này có tiềm năng được áp dụng cho bất kỳ loại kết nối nào, ngay cả trong cắt tỉa ít có cấu trúc hơn (ví dụ các kết nối kernel-to-kernel được chọn giữa các lớp tích chập) và để lại bằng chứng thực nghiệm như một hướng cho công việc tương lai. Mặc dù phương pháp của chúng tôi có điểm tương đồng với các phương pháp dựa trên dropout trong ResNet, những phương pháp này liên quan đến cắt tỉa các kết nối cụ thể giữa các nút. Từ góc độ kiến trúc, điều này không nhất thiết giảm số lượng FLOPs vì không có sự giảm trong số lượng phép nhân ma trận cần thiết. Ngược lại, loại bỏ toàn bộ các kênh / lớp có hiệu ứng mong muốn này.

5

--- TRANG 5 ---
Hình 1: Minh họa của một mô-đun gating với quyết định nhị phân như được tích hợp vào mô hình dư gốc. Các mô-đun gating có thể học được được huấn luyện như các phần khác của mạng. Tại suy luận, các quyết định gate được tải sẵn, và chỉ các tham số mạng có quyết định gate mở được tải và tính toán.

3.2 Ước Lượng Viên Straight-through
Chúng tôi chọn ước lượng viên straight-through (STE) làm đầu nhị phân cho mô-đun gating. Đường dẫn tiến của STE là một hàm ngưỡng cứng:
STE(x) = {
1, nếu x > 0
0, nếu x <= 0. (3)

Gradient ngược phản ánh lý do tại sao ước lượng viên này được biết đến như "straight-through":
∂L/∂x = ∂L/∂STE(x) · ∂STE(x)/∂x = {
∂L/∂STE(x), nếu |x| <= 1
0, nếu |x| > 1, (4)

trong đó trạng thái không nhạy cảm được kích hoạt khi |x| > 1. Điều này nhằm tránh một tình huống có thể xảy ra trong đó một gradient lớn làm cho giá trị đầu ra STE ở mức 1 hoặc 0 vĩnh viễn.

Một lợi thế rõ ràng của STE làm đầu gating là nó tạo thành một mô-đun nhẹ cho cả lan truyền tiến và ngược. Trong lần truyền tiến, không cần tính toán nào khác ngoài kiểm tra dấu. Trong lần truyền ngược không cần tính toán. Ước lượng gradient, thường được xem như một xấp xỉ thô của gradient thực dưới nhiễu, đã được chứng minh có tương quan tích cực với gradient quần thể, và do đó descent gradient giúp giảm thiểu mất mát thực nghiệm [44].

6

--- TRANG 6 ---
3.3 Regularization Phân Cực cho Kích Hoạt Thống Nhất
Trong quá trình huấn luyện kiểu cắt tỉa động, ma trận W_e(x) có thể không giống nhau cho tất cả x trong X. Để khuyến khích một ma trận kích hoạt cạnh thống nhất sao cho W_e(x) = W_e(x′), cho tất cả x, x′ trong X, chúng tôi giới thiệu một regularizer phân cực R_polar({W_e(x)|x trong X}). Hàm mất mát hoàn chỉnh là:
L(f(x),y) = L_task(f(x),y) + λR_polar(W_e(x)) (5)

trong đó L_task là mất mát tác vụ, ví dụ mất mát entropy chéo cho các tác vụ phân loại và lỗi bình phương trung bình cho các tác vụ hồi quy, và λ là hệ số tỷ lệ cho regularizer phân cực.

Dạng tổng quát của R_polar(W_e(x)) ở dạng một parabol đảo ngược. Giả sử W_e(x) trong R^|C| được làm phẳng cho tất cả các kết nối được bao phủ c trong C:
R_polar(W_e(x)) := 1/|C|(1−W̄_e(x))⊤W̄_e(x), (6)

trong đó W̄_e(x) = 1/|X| ∑_{x trong X} W_e(x) là ma trận kích hoạt cạnh trung bình trên tất cả các mẫu dữ liệu. Cho trước phạm vi của W̄_e,c trong [0,1], dạng parabol đảo ngược này đảm bảo rằng một tối ưu tương đương có thể đạt được khi W̄_e,c đạt đến một trong hai biên của phạm vi.

Cụ thể, trong tình huống cắt tỉa lớp ResNet của chúng tôi, số hạng regularisation được viết là:
R_polar := 1/|L| ∑_{ly trong L} (1−ḡ_ly)ḡ_ly, (7)

trong đó ḡ_ly = 1/|X| ∑_{x trong X} g_ly(x) trong [0,1] là trung bình của các đầu ra mô-đun gating trên tất cả các mẫu đầu vào của lớp.

Tương tự, trong tình huống cắt tỉa kênh ResNet của chúng tôi, số hạng regularisation được viết là:
R_polar := 1/|L| ∑_{ly trong L} 1/|C| ∑_{ch trong C} (1−ḡ_ly,ch)ḡ_ly,ch (8)

trong đó ḡ_ly,ch = 1/|X| ∑_{x trong X} g_ly,ch(x) trong [0,1] là trung bình của các đầu ra mô-đun gating trên tất cả các mẫu đầu vào của kênh ch trong C trong lớp ly trong L..

4 Thí Nghiệm
4.1 Đặc Tả Bộ Dữ Liệu và Kiến Trúc
Chúng tôi kiểm tra hiệu quả của phương pháp được đề xuất trên kiến trúc ResNet [19] trên các bộ dữ liệu CIFAR-10, CIFAR-100, và Tiny Imagenet, với 10, 100, và 200 lớp tương ứng. Chúng tôi chọn ba bộ dữ liệu để đánh giá công việc của mình vì chúng có kết quả kiểm tra được công nhận rộng rãi trên hầu hết các biến thể của ResNet. Việc kiểm tra trên cả hai bộ dữ liệu cho thấy hiệu quả của phương pháp chúng tôi dưới cả phân phối dữ liệu đơn giản và phức tạp.

Các thí nghiệm của chúng tôi trên các bộ dữ liệu CIFAR được tiến hành trên một GPU NVIDIA T4 với bộ nhớ 16GB. Kích thước batch được đặt thành 256 cho CIFAR-10 và 64 cho CIFAR-100. Việc huấn luyện của chúng tôi được thực hiện dưới tỷ lệ học giảm theo giai đoạn trong 350 epoch (mặc dù sự hội tụ thường có thể đạt được trước 250 epoch). Tỷ lệ học ban đầu cho cả hai bộ dữ liệu là 0.1, và ở mỗi giai đoạn tiếp theo sẽ giảm xuống 10%. Trên CIFAR-10, tỷ lệ học được điều chỉnh tại epoch 60, 120, và 160. Trên CIFAR-100, tỷ lệ học được điều chỉnh tại epoch 125, 190, và 250. Chúng tôi chọn stochastic gradient descent (SGD) làm bộ tối ưu hóa, với momentum 0.9 và weight decay 5×10^4. Các mạng và thủ tục huấn luyện được triển khai trong PyTorch. Khi có tính ngẫu nhiên, chúng tôi đặt seed ngẫu nhiên thành 1.

Các thí nghiệm của chúng tôi trên bộ dữ liệu Tiny Imagenet được tiến hành trên bốn GPU NVIDIA A10 với bộ nhớ 24GB mỗi cái. Việc huấn luyện của chúng tôi được thực hiện với bộ tối ưu hóa SGD dưới tỷ lệ học giảm theo giai đoạn trong 1200 epoch. Tỷ lệ học được đặt thành 0.2 ban đầu, sau đó giảm xuống 10% giá trị trước đó tại 600 và 900 epoch. Kích thước batch của chúng tôi là 100. Ngoài ra, chúng tôi áp dụng Puzzle mix [45] cho cả việc huấn luyện mạng baseline và việc huấn luyện mạng được cắt tỉa để cải thiện độ chính xác phân loại trên Tiny Imagenet.

Chúng tôi áp dụng mạng cho tác vụ phân loại hình ảnh. Các mạng được cắt tỉa được đánh giá bằng độ chính xác top-1 và FLOPs (phép toán dấu phẩy động). Số đếm FLOPs được xấp xỉ bởi gói fvcore².

7

--- TRANG 7 ---
4.2 So Sánh với Các Baseline Cắt Tỉa Hiện Có
Đầu tiên chúng tôi so sánh hiệu suất của phương pháp chúng tôi với các baseline ngây thơ và các phương pháp khác trong văn học có liên quan đến chúng tôi. Chúng tôi tuân theo các đặc tả huấn luyện được đặt ra trong Phần 4.1. Cho các thí nghiệm này, chúng tôi chọn kiến trúc cắt tỉa hiệu suất tốt nhất của chúng tôi dựa trên kết quả thực nghiệm trên tập kiểm tra được hiển thị sau trong phần này, trong đó chúng tôi sử dụng sơ đồ cắt tỉa lớp, Gumbell softmax (phần 4.4) và đặt λ_polar = ↑ (như trong Bảng 6). Hơn nữa, chúng tôi chọn ResNet56 làm kiến trúc cơ sở đặc biệt để cho phép so sánh dễ dàng với các sơ đồ cắt tỉa khác được thấy trong văn học.

Ngoài hàm mất mát gốc của chúng tôi, bây giờ chúng tôi muốn kiểm soát tỷ lệ cắt tỉa của phương pháp chúng tôi. Điều này được đạt được bằng cách thêm một số hạng regularization sparsity bổ sung trong hàm mất mát cung cấp một tín hiệu cho regulariser phân cực để giữ ít gate mở hơn. Hàm mất mát kết quả là:
L(f(x),y) = L_task(f(x),y) + λ_polar R_polar(W_e(x))
+ λ_act R_act(W_e(x)) (9)

trong đó:
R_act(W_e(x)) = 1/|L| ∑_{ly trong L} ḡ_ly (10)

là kích hoạt lớp trung bình tổng thể, trong đó trung bình được tính trên các lớp ly trong L và đầu vào x trong X. Chúng tôi thay đổi λ_act trong [0,1] để thay đổi tỷ lệ cắt tỉa (tức là % tham số được cắt tỉa). Trong bối cảnh công việc của chúng tôi, điều này được định nghĩa là:
E_{x trong X} [∑_l #Tham số trong lớp l: Gate g_l(x) = 0] / (#Tổng Tham Số) × 100 (11)

trong đó kỳ vọng trên các đầu vào tính đến thực tế rằng một số sơ đồ cắt tỉa có thể không đạt được sự thống nhất sub-network hoàn hảo.

Chúng tôi xem xét các baseline ngây thơ sau đây, do hiệu suất thực nghiệm của chúng trong văn học:
• Naïve Dropout ResNet-56: Một bộ phân loại tiêu chuẩn nhưng với k trong {20%, 30%, 50%, 60, 80%} tham số được cắt tỉa ngẫu nhiên trong quá trình kiểm tra.
• Naïve Layer Pruned ResNet-56: Cùng bộ phân loại nhưng với k trong {20%, 30%, 50%, 60, 80%} kích hoạt lớp được đặt ngẫu nhiên thành 0 trong quá trình kiểm tra.

Hình 2: So sánh phương pháp của chúng tôi với một số baseline ngây thơ trên CIFAR-10 với ResNet-56. Trái: Tỷ lệ cắt tỉa trung bình tại suy luận vs Độ chính xác Top-1. Phải: % giảm FLOPs tại suy luận vs Độ chính xác Top-1. Phương pháp dropout ngây thơ không giảm FLOPs vì nó vẫn liên quan đến tính toán thông qua các nút "dropped" - do đó bỏ qua.

Một so sánh trực quan với các baseline ngây thơ được hiển thị trong Hình 2, chứng minh khả năng của phương pháp cắt tỉa chúng tôi để duy trì hiệu suất mạng với tỷ lệ cắt tỉa cao trên tập kiểm tra. Tiếp tục đánh giá phương pháp của chúng tôi, chúng tôi xem xét các phương pháp cũng theo ý tưởng cắt tỉa và học đồng thời, như được liệt kê trong Bảng 1. Hình 3 cho thấy hiệu suất của sơ đồ chúng tôi so với các phương pháp được tìm thấy trong văn học, tất cả đều sử dụng ResNet-56 làm mạng cơ sở. Chúng tôi lưu ý rằng sơ đồ của chúng tôi cung cấp kết quả cạnh tranh không chỉ về độ chính xác tuyệt đối, mà còn về mức giảm độ chính xác từ việc cắt tỉa.

8

--- TRANG 8 ---
Phương pháp | Độ Chính Xác Không Cắt | Độ Chính Xác Đã Cắt | % Giảm FLOPS | Giảm Độ Chính Xác
Của chúng tôi | 93.43 | 92.42±0.14 | 41.81±4.01 | 1.01±0.14
AMC [46] | 92.80 | 91.90 | 50.00 | 1.10
Importance [47] | 93.60 | 91.90 | 39.90 | 1.14
SFP [48] | 93.59 | 92.26 | 52.60 | 1.33
CP [49] | 92.80 | 91.80 | 50.00 | 1.00
PFEC [50] | 93.04 | 91.31 | 27.60 | 1.73
VCP [51] | 93.04 | 92.26 | 20.30 | 0.78

Bảng 1: Hiệu suất của phương pháp chúng tôi qua 5 thử nghiệm so với một số phương pháp đã được thiết lập, liên quan trong văn học cho ~=50% giảm FLOPs (Bộ dữ liệu: CIFAR-10, Mô hình: ResNet-56). Chúng tôi lưu ý rằng phương pháp của chúng tôi cung cấp một sự đánh đổi cạnh tranh giữa độ chính xác và FLOPs trong khi đơn giản để triển khai. Đối với SFP, chúng tôi chỉ xem xét biến thể được huấn luyện trước để so sánh công bằng vì biến thể tinh chỉnh trong bài báo gây ra chi phí tính toán bổ sung không nhất thiết được xem xét.

Hình 3: So sánh giữa sơ đồ của chúng tôi và các phương pháp liên quan trong văn học trên CIFAR-10 với ResNet-56 tại suy luận. Trái: Tỷ lệ cắt tỉa vs Độ chính xác Top-1. Phải: % giảm FLOPs vs Giảm độ chính xác Top-1.

gating module positions, chúng tôi thí nghiệm đặt gating module trước lớp convolutional đầu tiên, giữa hai lớp convolution, và ngay sau lớp convolutional thứ hai. Hình 5 trực quan hóa các quyết định nói trên. Bảng 3 hiển thị kết quả cắt tỉa trên CIFAR-10 dưới các kiến trúc và vị trí gating module khác nhau. Đối với các kiến trúc gating module (nhớ lại Bảng 2), kết quả cho thấy rằng mặc dù tất cả các thiết kế đạt được tỷ lệ cắt tỉa kênh tương tự, thiết kế với hai lớp dày đặc và được đặt ở cuối mỗi lớp dư (2FC-after) đạt được độ chính xác phân loại tốt nhất cao hơn đáng kể so với hầu hết các thiết kế khác. Tuy nhiên, thiết kế với 1 lớp dày đặc được đặt giữa hai lớp convolution (1FC-middle) cũng đạt được độ chính xác tương tự. Từ nay, chúng tôi sử dụng baseline cắt tỉa kênh của chúng tôi là thiết kế 2FC-after, theo Bảng 3, vì nó cho thấy hiệu suất tốt nhất trong độ chính xác phân loại và tỷ lệ cắt tỉa kênh trên tập kiểm tra.

9

--- TRANG 9 ---
4.3 Điều Tra Cắt Tỉa Kênh và Lớp
ResNet chứa các lớp dư bao gồm hai lớp convolutional và một kết nối dư bypass.

Trong cắt tỉa kênh, chúng tôi thí nghiệm trên hai thiết kế lớp và ba vị trí của gating module. Cụ thể, chúng tôi thí nghiệm với các kiến trúc gating module bao gồm một và hai lớp dày đặc, trong đó Bảng 2 hiển thị thiết kế chi tiết. Đối với các vị trí gating module, chúng tôi thí nghiệm đặt gating module trước lớp convolutional đầu tiên, giữa hai lớp convolution, và ngay sau lớp convolutional thứ hai. Hình 5 trực quan hóa các quyết định nói trên. Bảng 3 hiển thị kết quả cắt tỉa trên CIFAR-10 dưới các kiến trúc và vị trí gating module khác nhau. Đối với các kiến trúc gating module (nhớ lại Bảng 2), kết quả cho thấy rằng mặc dù tất cả các thiết kế đạt được tỷ lệ cắt tỉa kênh tương tự, thiết kế với hai lớp dày đặc và được đặt ở cuối mỗi lớp dư (2FC-after) đạt được độ chính xác phân loại tốt nhất cao hơn đáng kể so với hầu hết các thiết kế khác. Tuy nhiên, thiết kế với 1 lớp dày đặc được đặt giữa hai lớp convolution (1FC-middle) cũng đạt được độ chính xác tương tự. Từ nay, chúng tôi sử dụng baseline cắt tỉa kênh của chúng tôi là thiết kế 2FC-after, theo Bảng 3, vì nó cho thấy hiệu suất tốt nhất trong độ chính xác phân loại và tỷ lệ cắt tỉa kênh trên tập kiểm tra.

Gating Module Lớp | Gating Module Kênh (K=2)
avgpool 2d (kích thước đầu ra=kênh vào) | flatten()
dense (chiều vào=kênh vào, chiều ra=16) | dense(chiều vào=kênh ra*kích thước đặc trưng, chiều ra=16)  
batch norm 1d() | batch norm 1d()
ReLU() | ReLU()
dense (chiều vào=16, chiều ra=1) | dense(chiều vào=16, chiều ra=1)
STE() | STE()

Bảng 2: Thiết kế gating module lớp và kênh. Trái: "kênh vào" là số kênh đầu vào cho lớp convolution đầu tiên trong lớp dư. Phải: "kênh giữa" là số kênh đầu ra cho lớp convolution đầu tiên, bằng số kênh đầu vào cho lớp convolution thứ hai. "kích thước đặc trưng" là chiều của bản đồ đặc trưng được làm phẳng. Đối với các giá trị K khác, chúng tôi chỉ đơn giản thay đổi số lượng lớp dày đặc.

Mô hình | độ chính xác top-1 (%) (rel) | tỷ lệ gate mở (%)
baseline | 93.68 (0) | 100.00
1FC-before | 69.71 (-23.97) | 47.82
1FC-middle | 90.60 (-3.08) | 47.77
1FC-after | 83.89 (-9.79) | 41.91
2FC-before | 85.39 (-8.29) | 48.93
2FC-middle | 82.94 (-10.74) | 49.36
2FC-after | 91.01 (-2.67) | 48.76

Bảng 3: Kết quả cắt tỉa kênh dưới các đặc tả gating module khác nhau trên bộ dữ liệu CIFAR-10. Số trong ngoặc cho độ chính xác top-1 hiển thị sự khác biệt tương đối (rel) từ mô hình baseline. "{K}FC" đề cập đến K={1,2} lớp dày đặc trong gating module và "before", "middle", và "after" cho ba vị trí gating module, được minh họa trong Hình 5.

Trong cắt tỉa lớp, các quyết định thiết kế của chúng tôi được đơn giản hóa. Chúng tôi thêm gating module trước hai lớp convolutional, như được thấy trong Hình 4, để quyết định liệu lớp có được tính toán hay không. Bảng 2 hiển thị thiết kế chi tiết của gating module.

Bây giờ chúng tôi tiến hành so sánh các sơ đồ cắt tỉa kênh và lớp trên các bộ dữ liệu CIFAR-10, CIFAR-100, và Tiny Imagenet, như được tóm tắt trong Bảng 4. Chúng tôi quan sát thấy rằng sơ đồ cắt tỉa lớp có thể tiết kiệm ít nhất 14% tính toán (FLOPs) trong khi hy sinh độ chính xác dưới 2.5%. Dưới sơ đồ cắt tỉa kênh, chúng tôi có thể tiết kiệm ít nhất 22% tính toán (FLOPs) trong khi hy sinh độ chính xác dưới 3%.

Nói chung, chúng tôi lưu ý rằng các mô hình được cắt tỉa lớp hoạt động tốt hơn các mô hình được cắt tỉa kênh (tức là có mức giảm độ chính xác thấp hơn) cho tất cả ba bộ dữ liệu, ngay cả khi các khác biệt tương đối trong FLOPs được tính vào. Chúng tôi tin rằng điều này là do dưới thiết kế của ResNet, các bản đồ đặc trưng trung gian trong mỗi lớp dư đủ compact về thông tin, và bất kỳ sự loại bỏ nào trên các bản đồ đặc trưng có thể dẫn đến mất thông tin. Tỷ lệ cắt tỉa của cắt tỉa kênh tuy nhiên là tích cực, với gần 50% giảm FLOPs.

10

--- TRANG 10 ---
Hình 4: Minh họa các gating module cắt tỉa lớp trong ResNet.

(a) Cắt tỉa kênh trước lớp convolution đầu tiên. | (b) Cắt tỉa kênh giữa hai lớp convolution. | (c) Cắt tỉa kênh sau lớp convolution thứ hai.

Hình 5: Minh họa các gating module cắt tỉa kênh trong ResNet: Gating module (a) trước lớp convolution đầu tiên; (b) giữa hai lớp convolutional; (c) sau lớp convolution thứ hai. K=1 hoặc 2 trong các thí nghiệm của chúng tôi.

4.4 Nghiên Cứu Ablation cho Ước Lượng Gradient
Chúng tôi kiểm tra tiện ích riêng lẻ của hai mô-đun chính, STE và regularizer phân cực, thông qua các nghiên cứu ablation. Để kiểm tra tiện ích của STE, chúng tôi thay thế STE bằng lấy mẫu từ phân phối Bernoulli và với Gumbel-softmax. Khi lấy mẫu từ phân phối Bernoulli, chúng tôi thiết lập một ngưỡng bằng trung bình của các đầu ra gating module ngay sau lớp dày đặc cuối cùng (tức là ngay trước STE gốc). Nếu đầu ra lớn hơn trung bình, chúng tôi giữ lớp; nếu không chúng tôi cắt tỉa lớp.

11

--- TRANG 11 ---
Bộ dữ liệu | Mô hình | Độ chính xác Top-1 (%) | Tỷ lệ gate mở (%) | FLOPs (M)(rel)
CIFAR-10 | baseline | 93.68 (0) | 100.00 | 255.3 (1)
         | layer pruned | 92.82 (-0.86) | 53.70 | 137.7 (0.54)
         | channel pruned | 91.01 (-2.67) | 48.76 | 189.9 (0.74)
CIFAR-100 | baseline | 71.85 (0) | 100.00 | 255.3 (1)
          | layer pruned | 70.01 (-1.84) | 66.67 | 171.1 (0.67)
          | channel pruned | 66.91 (-4.94) | 51.14 | 135.41 (0.52)
Tiny Imagenet | baseline | 61.43 (0) | 100.00 | 556.6 (1)
              | layer pruned | 59.14 (-2.29) | 87.50 | 480.9 (0.86)
              | channel pruned | 58.70 (-2.73) | 66.60 | 437.84 (0.78)

Bảng 4: Kết quả của các mạng được cắt tỉa trên các bộ dữ liệu CIFAR-10, CIFAR-100, và Tiny Imagenet. Số trong ngoặc cho độ chính xác top-1 hiển thị sự khác biệt tương đối từ mô hình baseline. FLOPs được tính bằng triệu (M). Số trong ngoặc cho FLOPs hiển thị tỷ lệ tương đối từ mô hình baseline. Mô hình baseline là ResNet110 cho CIFAR-10 và CIFAR-100. Mô hình baseline là PreActResNet18 cho Tiny Imagenet.

Hàm | Độ chính xác Top-1 (%) | R_polar
STE | 92.86 | 0.00
Gumbel-softmax | 94.05 | 0.0039
Bernoulli | 91.96 | 0.2454

Bảng 5: Kết quả của các hàm gating khác nhau trên CIFAR-10. R_polar được lấy ở cuối một phiên huấn luyện, mỗi phiên với cùng số epoch. R_polar lớn hơn tương ứng với các sub-network ít thống nhất hơn sau hội tụ. Các thí nghiệm này được thực hiện trên ResNet110 với λ_polar = 3.

Bảng 5 hiển thị kết quả từ ba hàm gating, được thí nghiệm trên CIFAR-10. Chúng tôi quan sát thấy rằng ngoài STE, không có hàm head gating nào khác có thể tạo ra một sub-network thống nhất và ổn định hoàn hảo. STE có tiện ích ở đây về mặt ổn định hóa sự tiến hóa của các sub-network động và duy trì hiệu suất mong đợi. Tuy nhiên, chúng tôi lưu ý rằng Gumbel-softmax có tiềm năng đạt được hiệu suất tác vụ tốt hơn trong khi giữ một tập hợp các sub-network hơi động được chỉ ra bởi mức độ thấp của R_polar. Chúng tôi để lại cho công việc tương lai một nghiên cứu về cách một sự thống nhất phù hợp của các sub-network động kết quả từ Gumbel-softmax có thể được đạt được để cải thiện thêm hiệu suất so với STE cho tác vụ này.

12

--- TRANG 12 ---
Spec | Độ chính xác Top-1 (%) (rel) | tỷ lệ gate mở (%)
baseline | 93.68 (0) | 100.00
λ_polar = 0 | 90.79 (-2.89) | 18.52 - 53.70
λ_polar = 1 | 92.44 (-1.24) | 53.70
λ_polar = 2 | 91.67 (-2.01) | 40.74
λ_polar = 3 | 91.85 (-1.83) | 57.41
λ_polar↑ | 93.18 (-0.50) | 55.56

Bảng 6: Kết quả của các mạng được cắt tỉa dưới các giá trị λ_polar khác nhau trên CIFAR-10 (ResNet110). "λ_polar↑" sử dụng các thiết lập λ_polar = 0 cho 125 epoch đầu tiên; λ = 2 cho 65 epoch tiếp theo; và λ = 3 cho các epoch còn lại cho đến cuối huấn luyện.

Hình 6: Tỷ lệ mở lớp trong quá trình huấn luyện dưới λ khác nhau. Mỗi hàng đại diện cho một lớp trong số 54 lớp. Mỗi cột đại diện cho một epoch. Đối với λ trong {0,1,2,3}, chúng tôi bao gồm 50 epoch huấn luyện đầu tiên. Đối với λ↑, sự thay đổi trong λ được phân tách bởi đường màu hồng.

Để hiểu sự đánh đổi giữa tính tích cực cắt tỉa, duy trì độ chính xác, và tiết kiệm tính toán, chúng tôi thí nghiệm trên một loạt các mức độ tích cực cắt tỉa, được thực hiện bằng cách điều chỉnh hyperparameter λ_polar. Chúng tôi thí nghiệm trên các giá trị λ_polar thay đổi từ 0 đến 3. Chúng tôi cũng kiểm tra hiệu ứng của việc tăng dần trọng số regularizer λ_polar trong quá trình huấn luyện để xác minh liệu một mạng được huấn luyện một phần có ảnh hưởng đến kết quả cắt tỉa hay không. Hình 6 hiển thị sự tiến hóa cắt tỉa lớp dưới các thiết lập λ_polar khác nhau và Bảng 6 hiển thị hiệu suất của các sub-network kết quả.

13

--- TRANG 13 ---
Các hình tiến hóa cắt tỉa lớp cho thấy rằng một mức độ tích cực cắt tỉa cao hơn (một λ_polar cao hơn) đẩy nhanh sự hội tụ đến một sub-network thống nhất hơn một mức độ thấp hơn. Mặt khác, Bảng 6 cho thấy rằng một tỷ lệ mở cao hơn không nhất thiết mang lại nhiều tiết kiệm tính toán và duy trì độ chính xác hơn. Tăng tính tích cực cắt tỉa (giới thiệu số hạng phạt) ở giai đoạn muộn hơn trong huấn luyện, tuy nhiên, cho thấy độ chính xác cao hơn các số hạng phạt không đổi khác, với tiết kiệm tính toán cao hơn các thí nghiệm khác.

5 Thảo Luận
5.1 Độ Phức Tạp Tính Toán
Tương tự như bất kỳ mô-đun gating bổ sung nào, mô-đun gating được giới thiệu từ phương pháp của chúng tôi thêm một chi phí phụ vào độ phức tạp tính toán tổng thể trong quá trình huấn luyện. Độ phức tạp thời gian của STE của một đầu vào đơn cho đường dẫn tiến là O(1) theo Eq. 3. Đối với đường dẫn ngược, theo Eq. 4, STE trên mỗi chiều chỉ mang lại O(1) tính toán bổ sung so với đường dẫn ngược gốc.

Lấy thiết lập trong Bảng 2 (cột trái) làm ví dụ, một gating module gồm K lớp kết nối đầy đủ, d chiều mỗi lớp, và |C| kênh làm đầu vào giới thiệu một chi phí phụ O(dK|C|).

Hợp nhất gating module nói trên vào mạng trong Hình 4, đối với một mạng có N lớp và |C| kênh mỗi lớp, các gating module được chèn giới thiệu một chi phí phụ O(dNK|C|).

Trong thí nghiệm của chúng tôi với K = 1 hoặc 2, chi phí phụ cuối cùng được thu nhỏ thành O(dN|C|), trong đó chỉ có d phụ thuộc vào thiết kế gating module. Một giá trị d nhỏ thực tế hữu ích trong việc kiểm soát chi phí tính toán tổng thể.

Phân tích chi phí phụ trên chỉ áp dụng cho việc huấn luyện mạng. Sau huấn luyện, các tham số mạng được tối ưu hóa bao gồm đầu ra gating module, và một lớp được kích hoạt hoặc hủy kích hoạt tại suy luận cho các đầu vào tùy ý. Chi phí phụ do đó được loại trừ tại suy luận. Thí nghiệm của chúng tôi (Bảng 6) cho thấy rằng mô hình được cắt tỉa có thể chỉ sử dụng 55.56% nơ-ron để đạt được chỉ 0.5% giảm hiệu suất.

6 Kết Luận
Chúng tôi đề xuất một sơ đồ cắt tỉa mạng duy trì hiệu suất trong khi hiệu quả hơn về mặt tính toán. Thông qua tối ưu hóa tham số và cấu trúc đồng thời, sơ đồ cắt tỉa của chúng tôi tìm thấy một sub-network ổn định với hiệu suất tương tự trên cùng tác vụ downstream như mạng đầy đủ (không được cắt tỉa). Sơ đồ bao gồm một mô-đun gating nhị phân có thể vi phân, nhẹ và các regulariser để thực thi tính bất biến dữ liệu của các sub-network được cắt tỉa. Các thí nghiệm về cắt tỉa lớp và kênh cho thấy kết quả cạnh tranh với (hoặc thực sự tốt hơn) các phương pháp tương tự trong văn học. Với việc tinh chỉnh các sub-network được khám phá của chúng tôi, chúng tôi dự đoán cải thiện hiệu suất thêm - tuy nhiên, điều này nằm ngoài phạm vi công việc của chúng tôi, nhằm nhấn mạnh lợi ích hiệu suất tối đa trong ngân sách tính toán hạn chế. Chúng tôi mong chờ việc kiểm tra khả năng áp dụng của sơ đồ cắt tỉa của chúng tôi cho các mạng cơ sở, bộ dữ liệu và tác vụ khác, hy vọng rằng các kết quả khuyến khích của chúng tôi tạo điều kiện cho việc chuyển đổi từ AI đỏ sang các mô hình học sâu tiết kiệm năng lượng và xanh.

14

--- TRANG 14 ---
7 Tuyên Bố Miễn Trừ
Bài báo này được chuẩn bị cho mục đích thông tin bởi nhóm Applied Innovation of AI của JPMorgan Chase & Co. Bài báo này không phải là sản phẩm của Bộ phận Nghiên cứu của JPMorgan Chase & Co. hoặc các công ty liên kết. Cả JPMorgan Chase & Co. cũng như bất kỳ công ty liên kết nào không đưa ra bất kỳ tuyên bố rõ ràng hoặc ngụ ý nào và không ai trong số họ chấp nhận bất kỳ trách nhiệm nào liên quan đến bài báo này, bao gồm, không giới hạn, đối với tính đầy đủ, chính xác, hoặc độ tin cậy của thông tin chứa trong đó và các tác động pháp lý, tuân thủ, thuế, hoặc kế toán tiềm ẩn. Tài liệu này không nhằm mục đích làm nghiên cứu đầu tư hoặc lời khuyên đầu tư, hoặc như một khuyến nghị, đề nghị, hoặc mời chào mua hoặc bán bất kỳ bảo mật, công cụ tài chính, sản phẩm tài chính hoặc dịch vụ nào, hoặc được sử dụng theo bất kỳ cách nào để đánh giá giá trị của việc tham gia vào bất kỳ giao dịch nào. Công việc được mô tả là một nguyên mẫu và không phải là một hệ thống triển khai sản xuất.

Tài Liệu Tham Khảo
[1]Schwartz, R., Dodge, J., Smith, N.A., Etzioni, O.: Green ai. Commun. ACM 63(12), 54-63 (2020) https://doi.org/10.1145/3381831

[2]Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv (2020). https://doi.org/10.48550/ARXIV.2010.11929 . https://arxiv.org/abs/2010.11929

[3]Jouppi, N.P., Yoon, D.H., Kurian, G., Li, S., Patil, N., Laudon, J., Young, C., Patterson, D.: A domain-specific supercomputer for training deep neural networks. Commun. ACM 63(7), 67-78 (2020) https://doi.org/10.1145/3360307

[4]EIA US: 2020 Average Monthly Bill- Residential. https://www.eia.gov/electricity/sales revenue price/pdf/table5 a.pdf. Truy cập: 2022-07-19 (2021)

[5]Denil, M., Shakibi, B., Dinh, L., Ranzato, M., Freitas, N.: Predicting Parameters in Deep Learning. arXiv (2013). https://doi.org/10.48550/ARXIV.1306.0543 . https://arxiv.org/abs/1306.0543

[6]Shafiee, M.S., Shafiee, M.J., Wong, A.: Dynamic Representations Toward Efficient Inference on Deep Neural Networks by Decision Gates. arXiv (2018). https://doi.org/10.48550/ARXIV.1811.01476 . https://arxiv.org/abs/1811.01476

15

--- TRANG 15 ---
[7]Luo, J., Zhang, H., Zhou, H., Xie, C., Wu, J., Lin, W.: Thinet: Pruning cnn filters for a thinner net. IEEE Transactions on Pattern Analysis 'I&' Machine Intelligence 41(10), 2525-2538 (2019) https://doi.org/10.1109/TPAMI.2018.2858232

[8]Cheong, R.: transformers . zip : Compressing transformers with pruning and quantization. (2019)

[9]Zhang, M.S., Stadie, B.: One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation. arXiv (2019). https://doi.org/10.48550/ARXIV.1912.00120 . https://arxiv.org/abs/1912.00120

[10]Lin, Z., Liu, J., Yang, Z., Hua, N., Roth, D.: Pruning redundant mappings in transformer models via spectral-normalized identity prior. In: Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 719-730. Association for Computational Linguistics, Online (2020). https://doi.org/10.18653/v1/2020.findings-emnlp.64 . https://aclanthology.org/2020.findings-emnlp.64

[11] Han, S., Pool, J., Tran, J., Dally, W.J.: Learning both Weights and Connections for Efficient Neural Networks. arXiv (2015). https://doi.org/10.48550/ARXIV.1506.02626 . https://arxiv.org/abs/1506.02626

[12]Zhu, M., Tang, Y., Han, K.: Vision Transformer Pruning. arXiv (2021). https://doi.org/10.48550/ARXIV.2104.08500 . https://arxiv.org/abs/2104.08500

[13]Hou, Z., Qin, M., Sun, F., Ma, X., Yuan, K., Xu, Y., Chen, Y.-K., Jin, R., Xie, Y., Kung, S.-Y.: CHEX: CHannel EXploration for CNN Model Compression. arXiv (2022). https://doi.org/10.48550/ARXIV.2203.15794 . https://arxiv.org/abs/2203.15794

[14]Veit, A., Belongie, S.: Convolutional Networks with Adaptive Inference Graphs. arXiv (2017). https://doi.org/10.48550/ARXIV.1711.11503 . https://arxiv.org/abs/1711.11503

[15]Gao, X., Zhao, Y., Dudziak, L., Mullins, R., Xu, C.-z.: Dynamic Channel Pruning: Feature Boosting and Suppression. arXiv (2018). https://doi.org/10.48550/ARXIV.1810.05331 . https://arxiv.org/abs/1810.05331

[16]Bejnordi, B.E., Blankevoort, T., Welling, M.: Batch-Shaping for Learning Conditional Channel Gated Networks. arXiv (2019). https://doi.org/10.48550/ARXIV.1907.06627 . https://arxiv.org/abs/1907.06627

[17]Yin, H., Vahdat, A., Alvarez, J., Mallya, A., Kautz, J., Molchanov, P.: A-ViT: Adaptive Tokens for Efficient Vision Transformer. arXiv (2021). https://doi.org/10.48550/ARXIV.2112.07658 . https://arxiv.org/abs/2112.07658

[18]Lee, N., Ajanthan, T., Torr, P.H.S.: SNIP: Single-shot Network Pruning based on Connection Sensitivity. arXiv (2018). https://doi.org/10.48550/ARXIV.1810.02340

16

--- TRANG 16 ---
[19]He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image Recognition. arXiv (2015). https://doi.org/10.48550/ARXIV.1512.03385 . https://arxiv.org/abs/1512.03385

[20]Krizhevsky, A.: Learning multiple layers of features from tiny images. (2009). https://www.cs.toronto.edu/ \texttildelowkriz/learning-features-2009-TR.pdf

[21]Le, Y., Yang, X.S.: Tiny imagenet visual recognition challenge. (2015). https://api.semanticscholar.org/CorpusID:16664790

[22]Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., Dean, J.: Carbon Emissions and Large Neural Network Training. arXiv (2021). https://doi.org/10.48550/ARXIV.2104.10350 . https://arxiv.org/abs/2104.10350

[23] Dodge, J., Prewitt, T., Combes, R.T.D., Odmark, E., Schwartz, R., Strubell, E., Luccioni, A.S., Smith, N.A., DeCario, N., Buchanan, W.: Measuring the Carbon Intensity of AI in Cloud Instances. arXiv (2022). https://doi.org/10.48550/ARXIV.2206.05229 . https://arxiv.org/abs/2206.05229

[24]Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M.W., Keutzer, K.: A Survey of Quantization Methods for Efficient Neural Network Inference. arXiv (2021). https://doi.org/10.48550/ARXIV.2103.13630 . https://arxiv.org/abs/2103.13630

[25]Hinton, G., Vinyals, O., Dean, J.: Distilling the Knowledge in a Neural Network. arXiv (2015). https://doi.org/10.48550/ARXIV.1503.02531 . https://arxiv.org/abs/1503.02531

[26]Wang, W., Chen, M., Zhao, S., Chen, L., Hu, J., Liu, H., Cai, D., He, X., Liu, W.: Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework. arXiv (2020). https://doi.org/10.48550/ARXIV.2010.04879 . https://arxiv.org/abs/2010.04879

[27]Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning Filters for Efficient ConvNets. arXiv (2016). https://doi.org/10.48550/ARXIV.1608.08710 . https://arxiv.org/abs/1608.08710

[28]He, Y., Zhang, X., Sun, J.: Channel Pruning for Accelerating Very Deep Neural Networks. arXiv (2017). https://doi.org/10.48550/ARXIV.1707.06168 . https://arxiv.org/abs/1707.06168

[29]Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Network and Specialize it for Efficient Deployment. arXiv (2019). https://doi.org/10.48550/ARXIV.1908.09791 . https://arxiv.org/abs/1908.09791

17

--- TRANG 17 ---
[30]Lin, J., Rao, Y., Lu, J., Zhou, J.: Runtime neural pruning. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing Systems, vol. 30. Curran Associates, Inc., ??? (2017). https://proceedings.neurips.cc/paper/2017/file/a51fb975227d6640e4fe47854476d133-Paper.pdf

[31]Lee, Y.: Differentiable Sparsification for Deep Neural Networks. arXiv (2019). https://doi.org/10.48550/ARXIV.1910.03201 . https://arxiv.org/abs/1910.03201

[32]Wortsman, M., Farhadi, A., Rastegari, M.: Discovering Neural Wirings. arXiv (2019). https://doi.org/10.48550/ARXIV.1906.00586 . https://arxiv.org/abs/1906.00586

[33]Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., Rastegari, M.: What's Hidden in a Randomly Weighted Neural Network? arXiv (2019). https://doi.org/10.48550/ARXIV.1911.13299 . https://arxiv.org/abs/1911.13299

[34]Frankle, J., Carbin, M.: The lottery ticket hypothesis: Finding sparse, trainable neural networks. In: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, ??? (2019). https://openreview.net/forum?id=rJl-b3RcF7

[35]Jang, E., Gu, S., Poole, B.: Categorical Reparameterization with Gumbel-Softmax. arXiv (2016). https://doi.org/10.48550/ARXIV.1611.01144 . https://arxiv.org/abs/1611.01144

[36]Kusner, M.J., Hernández-Lobato, J.M.: GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution. arXiv (2016). https://doi.org/10.48550/ARXIV.1611.04051 . https://arxiv.org/abs/1611.04051

[37]Shen, J., Zhen, X., Worring, M., Shao, L.: Variational multi-task learning with gumbel-softmax priors. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P.S., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems, vol. 34, pp. 21031-21042. Curran Associates, Inc., ??? (2021). https://proceedings.neurips.cc/paper/2021/file/afd4836712c5e77550897e25711e1d96-Paper.pdf

[38]Chang, J., Zhang, X., Guo, Y., Meng, G., Xiang, S., Pan, C.: Differentiable Architecture Search with Ensemble Gumbel-Softmax. arXiv (2019). https://doi.org/10.48550/ARXIV.1905.01786 . https://arxiv.org/abs/1905.01786

[39]Bengio, Y., Léonard, N., Courville, A.: Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. arXiv (2013). https://doi.org/10.48550/ARXIV.1308.3432 . https://arxiv.org/abs/1308.3432

[40]Lin, S., Ji, R., Li, Y., Deng, C., Li, X.: Towards Compact ConvNets via Structure-Sparsity Regularized Filter Pruning. arXiv (2019). https://doi.org/10.48550/ARXIV.1901.07827 . https://arxiv.org/abs/1901.07827

18

--- TRANG 18 ---
[41]Li, Y., Gu, S., Mayer, C., Van Gool, L., Timofte, R.: Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression. arXiv (2020). https://doi.org/10.48550/ARXIV.2003.08935 . https://arxiv.org/abs/2003.08935

[42]Srinivas, S., Babu, R.V.: Learning Neural Network Architectures using Backpropagation. arXiv (2015). https://doi.org/10.48550/ARXIV.1511.05497 . https://arxiv.org/abs/1511.05497

[43]Zhuang, T., Zhang, Z., Huang, Y., Zeng, X., Shuang, K., Li, X.: Neuron-level structured pruning using polarization regularizer. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in Neural Information Processing Systems, vol. 33, pp. 9865-9877. Curran Associates, Inc., ??? (2020). https://proceedings.neurips.cc/paper/2020/file/703957b6dd9e3a7980e040bee50ded65-Paper.pdf

[44]Yin, P., Lyu, J., Zhang, S., Osher, S., Qi, Y., Xin, J.: Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets. arXiv (2019). https://doi.org/10.48550/ARXIV.1903.05662 . https://arxiv.org/abs/1903.05662

[45]Kim, J.-H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statistics for optimal mixup. In: International Conference on Machine Learning (ICML) (2020)

[46]He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., Han, S.: AMC: AutoML for model compression and acceleration on mobile devices. In: Computer Vision - ECCV 2018, pp. 815-832. Springer, ??? (2018). https://doi.org/10.1007/978-3-030-01234-2 48 . https://doi.org/10.1007/978-3-030-01234-2 48

[47]Dekhovich, A., Tax, D.M.J., Sluiter, M.H.F., Bessa, M.A.: Neural network relief: a pruning algorithm based on neural activity. CoRR abs/2109.10795 (2021) 2109.10795

[48]He, Y., Kang, G., Dong, X., Fu, Y., Yang, Y.: Soft filter pruning for accelerating deep convolutional neural networks. In: International Joint Conference on Artificial Intelligence (IJCAI), pp. 2234-2240 (2018)

[49]He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural networks. In: 2017 IEEE International Conference on Computer Vision (ICCV), pp. 1398-1406 (2017). https://doi.org/10.1109/ICCV.2017.155

[50]Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning filters for efficient convnets. ArXiv abs/1608.08710 (2016)

[51]Zhao, C., Ni, B., Zhang, J., Zhao, Q., Zhang, W., Tian, Q.: Variational convolutional neural network pruning. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2775-2784 (2019). https://doi.org/10.1109/CVPR.2019.00289

19
