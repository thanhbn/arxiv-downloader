# 2301.11063.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2301.11063.pdf
# Kích thước tệp: 1347625 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Cắt tỉa meta có phần thưởng: Học meta sử dụng phần thưởng cho việc cắt tỉa kênh
Athul Shibu, Abhishek Kumar, Heechul Jung, Dong-Gyu Lee
Khoa Trí tuệ nhân tạo, Đại học Quốc gia Kyungpook
fathulshibu, abhishek.ai, heechul, dglee g@knu.ac.kr
Tóm tắt
Mạng nơ-ron tích chập (CNN) có số lượng tham số lớn và cần tài nguyên phần cứng đáng kể để tính toán, vì vậy các thiết bị cạnh gặp khó khăn khi chạy các mạng cấp cao. Bài báo này đề xuất một phương pháp mới để giảm tham số và FLOPs cho hiệu quả tính toán trong các mô hình học sâu. Chúng tôi giới thiệu các hệ số độ chính xác và hiệu quả để kiểm soát sự đánh đổi giữa độ chính xác của mạng và hiệu quả tính toán của nó. Thuật toán cắt tỉa meta có phần thưởng được đề xuất huấn luyện một mạng để tạo ra trọng số cho một mô hình đã cắt tỉa được chọn dựa trên các tham số xấp xỉ của mô hình cuối cùng bằng cách kiểm soát các tương tác sử dụng hàm phần thưởng. Hàm phần thưởng cho phép kiểm soát nhiều hơn các chỉ số của mô hình đã cắt tỉa cuối cùng. Các thí nghiệm mở rộng chứng minh hiệu suất vượt trội của phương pháp đề xuất so với các phương pháp tiên tiến nhất trong việc cắt tỉa các mạng ResNet-50, MobileNetV1 và MobileNetV2.

1. Giới thiệu
Mạng nơ-ron tích chập (CNN) đã được chứng minh đạt được kết quả tiên tiến nhất trong nhiều tác vụ thị giác máy tính [23,26,29-31]. Tuy nhiên, việc huấn luyện các tham số của CNN đòi hỏi một lượng đáng kể dữ liệu được gán nhãn. Hơn nữa, một lượng lớn tài nguyên phần cứng cũng được yêu cầu để huấn luyện một lượng lớn dữ liệu huấn luyện. Gần đây, việc cắt tỉa mạng đã trở thành một chủ đề quan trọng để đơn giản hóa và tăng tốc các CNN lớn [24, 30, 52, 58].

Nhiều vấn đề phát sinh khi cố gắng cắt tỉa mạng, chẳng hạn như cấu trúc [32], tính liên tục [42] hoặc khả năng mở rộng [41]. Có chủ yếu hai cách để nén mạng nơ-ron: cắt tỉa trọng số [3,12,50] và cắt tỉa kênh [8,10,38]. Việc giảm tham số bằng cách cắt tỉa các kết nối là cách trực quan nhất để cắt tỉa một mạng. Cắt tỉa trọng số bao gồm việc xác định các trọng số có hiệu suất thấp để cắt tỉa [14]. Điều này liên quan đến việc đơn giản loại bỏ các trọng số có độ lớn nhỏ, điều này dễ thực hiện [15]. Tuy nhiên, hầu hết các framework không thể tăng tốc các ma trận thưa thớt trong quá trình tính toán. Để có được nén và tăng tốc thực sự, nó yêu cầu phần mềm [24] hoặc phần cứng [13] được định nghĩa cụ thể để xử lý độ thưa thớt. Chi phí thực tế không bị ảnh hưởng bất kể có bao nhiêu trọng số bị cắt tỉa.

Do đó, việc cắt tỉa kênh liên quan đến việc loại bỏ toàn bộ bộ lọc thay vì đơn giản giảm giá trị trọng số về không được ưa chuộng hơn [27, 39]. Sự ưa chuộng này bắt nguồn từ thực tế là việc cắt tỉa kênh có thể loại bỏ toàn bộ bộ lọc, tạo ra một mô hình với độ thưa thớt có cấu trúc [20]. Với độ thưa thớt có cấu trúc, mô hình có thể tận dụng đầy đủ các thư viện Chương trình con Đại số tuyến tính cơ bản (BLAS) hiệu quả cao để đạt được tăng tốc tốt hơn. Điều này làm cho mô hình đã cắt tỉa có cấu trúc hơn và đạt được tăng tốc thực tế [13].

MetaPruning [40] là một trong những phương pháp cắt tỉa kênh như vậy có thể đạt được tăng tốc của CNN. Phương pháp trung tâm của MetaPruning là tạo ra trọng số cho các cấu trúc đã cắt tỉa thay vì cắt tỉa trọng số hoặc bộ lọc của mạng hiện có. Độ chính xác của các mô hình chưa được huấn luyện được tính toán để xếp hạng từng Vector mã hóa mạng (NEV). Các thuật toán tiến hóa, được thúc đẩy bởi các quá trình tiến hóa tự nhiên [28], được sử dụng để tìm NEV tạo ra mô hình có độ chính xác cao nhất. Do đó, tuy nhiên, MetaPruning chỉ có thể chọn độ chính xác tốt nhất được chọn từ một phạm vi FLOPs đã đặt. Vì vậy thuật toán tìm độ chính xác cao nhất trong phạm vi FLOPs được xác định trước thay vì cố gắng tìm sự cân bằng phù hợp giữa việc hy sinh độ chính xác và giảm FLOPs.

Bài báo này cố gắng giải quyết vấn đề này. Trong phương pháp cắt tỉa meta có phần thưởng được đề xuất, thay vì tìm các NEV tạo ra độ chính xác cao nhất, mô hình cố gắng cân bằng độ chính xác với FLOPs của mạng để tìm độ chính xác cao nhất có thể cho FLOPs đã cho. Trong MetaPruning, phần thưởng tỷ lệ thuận trực tiếp với độ chính xác, bởi vì phần thưởng là độ chính xác. Vì vậy việc tăng phần thưởng của các đột biến tiếp theo thấp hơn so với những phần thưởng của cắt tỉa meta có phần thưởng, nơi phần thưởng tỷ lệ thuận trực tiếp với bình phương của độ chính xác. Đồng thời, cắt tỉa meta có phần thưởng có thể kiểm soát FLOPs của mô hình cuối cùng bằng cách tính toán một điểm số có tính đến cả độ chính xác và FLOPs để tìm các mô hình có độ chính xác cao và FLOPs thấp. Điểm số này, chính là phần thưởng, có thể được điều chỉnh thêm để bao gồm các tham số khác nhau và kiểm soát các chỉ số của mô hình đã cắt tỉa, cũng như cách các tham số tương tác với nhau.

arXiv:2301.11063v1 [cs.CV] 26 Jan 2023

--- TRANG 2 ---
Đóng góp của chúng tôi nằm ở ba khía cạnh:
• Chúng tôi đề xuất một phương pháp cắt tỉa kênh, cắt tỉa meta có phần thưởng, có thể học cách gán trọng số cho các mạng đã cắt tỉa.
• Chúng tôi khám phá tầm quan trọng của các hàm phần thưởng và các đặc điểm để định nghĩa một hàm phần thưởng hiệu quả.
• Chúng tôi chứng minh thực nghiệm tính ưu việt của phương pháp cắt tỉa được đề xuất trên các CNN được huấn luyện trước công khai; ResNet-50, MobileNetV1, và MobileNetV2.

2. Công trình liên quan
Giả thuyết vé số may mắn nói rằng một mạng nơ-ron dày đặc được khởi tạo ngẫu nhiên chứa một mạng con mà khi được huấn luyện riêng biệt, có thể mang lại kết quả tốt bằng hoặc thậm chí vượt trội so với mạng gốc [12]. Nói cách khác, một mạng đã cắt tỉa tiêu chuẩn có thể có độ chính xác tương tự, nếu không cao hơn, so với mạng gốc. Có một số phương pháp để tìm ra những vé đúng.

Cắt tỉa mạng không có cấu trúc: Các phương pháp cắt tỉa ngẫu nhiên khác nhau [3, 12, 50] dựa vào việc cắt tỉa các tham số ngẫu nhiên dựa trên các yếu tố khác nhau của trọng số. [60] sử dụng chuẩn L1 và L2 của mỗi trọng số để tính toán tầm quan trọng của chúng. Mô hình đã cắt tỉa cuối cùng được tạo ra bằng cách cắt tỉa các trọng số ít quan trọng hơn. [19] tính toán trung vị hình học của các trọng số trong khi [46] sử dụng khai triển chuỗi Taylor phức tạp để tính toán hàm trọng số. Các phương pháp cắt tỉa trọng số khác như [44] và [35] sử dụng tầm quan trọng KL-divergence và độ nhạy cảm thực nghiệm của các trọng số tương ứng để cắt tỉa chúng.

Cắt tỉa mạng có cấu trúc: Trong các phương pháp khác, AutoPruner [43] tích hợp việc lựa chọn bộ lọc vào việc huấn luyện mô hình để mạng được tinh chỉnh có thể tự động chọn các bộ lọc không quan trọng. Lựa chọn cấu trúc thưa thớt (SSS) [25] đề xuất việc giới thiệu một tham số mới, hệ số tỷ lệ, để tỷ lệ đầu ra của các cấu trúc cụ thể. Việc chính quy hóa thưa thớt trên các hệ số tỷ lệ này đẩy chúng về không trong quá trình huấn luyện. Cắt tỉa kênh nhận biết phân biệt (DCP) [62] có thể tìm các kênh với sức mạnh phân biệt thực sự và cập nhật mô hình bằng cách cắt tỉa theo từng giai đoạn sử dụng các tổn thất nhận biết phân biệt. Adaptive DCP [38] giới thiệu một tổn thất nhận biết phân biệt bổ sung sử dụng tổn thất p-th, và các tổn thất bổ sung như tổn thất lề góc cộng [8]. AutoML cho nén mô hình (AMC) [18] tận dụng học tăng cường để tự động lấy mẫu không gian thiết kế và cải thiện chất lượng nén mô hình. Các phương pháp đơn giản hơn như HRank [36] xác định thứ hạng của các bản đồ đặc trưng được tạo ra bởi các bộ lọc để xếp hạng các bộ lọc và hiệu quả của chúng đối với độ chính xác cuối cùng. Tuy nhiên điều này mất nhiều epoch hơn để huấn luyện sau khi cắt tỉa. [61] tận dụng giả thuyết vé số may mắn để tìm kiếm tham lam qua một mạng, tìm các mạng con với tổn thất thấp hơn các mạng được huấn luyện với gradient descent. Các thuật toán cắt tỉa được lấy cảm hứng từ lý thuyết Hebbian, như Fire Together Wire Together (FTWT) [10], cắt tỉa các bộ lọc dựa trên mặt nạ nhị phân của mỗi lớp và kích hoạt của lớp trước đó.

Học meta: Học meta là việc học các thuật toán từ các thuật toán học khác [11, 40, 53]. Cơ bản có ba mô hình [54] trong học meta; meta-optimizer, meta-representation và meta-objective. Meta-optimizer là optimizer được sử dụng để học tối ưu hóa trong vòng lặp ngoài của học meta [21]. Meta-representation nhằm học và cập nhật meta-knowledge [11]. Cuối cùng, Meta-objective là nhiệm vụ cuối cùng đạt được sau khi hoàn thành huấn luyện [34].

Học cắt tỉa bộ lọc: Các thuật toán học tăng cường (RL) đã được sử dụng để tạo ra các mô tả kiến trúc mạng sử dụng mạng nơ-ron tái phát được huấn luyện qua phương pháp policy gradient [63]. Điều tương tự cũng đã được thực hiện sử dụng Q-learning [49]. Thuật toán Try-and-learn [24] sử dụng RL để tính toán phần thưởng của mỗi bộ lọc và những phần thưởng này sau đó được sử dụng để xếp hạng các bộ lọc. Nó cắt tỉa tích cực các bộ lọc trong mạng cơ sở trong khi duy trì hiệu suất ở mức mong muốn. Mô hình tính toán một phần thưởng như một tích của số hạng độ chính xác và hiệu quả, và sau đó sử dụng REINFORCE [55] để ước tính các gradient. Các gradient sau đó có thể được sử dụng để tính toán tổn thất và huấn luyện mạng, mạng này học cách cắt tỉa các bộ lọc. Hàm phần thưởng làm cho việc kiểm soát sự đánh đổi giữa hiệu suất mạng và quy mô mà không cần can thiệp của con người trở nên khả thi. Thuật toán try-and-learn tự động khám phá các bộ lọc dư thừa và loại bỏ chúng bằng cách lặp lại quá trình cho mỗi lớp.

Tìm kiếm kiến trúc nơ-ron: Nhiều phương pháp đã được đề xuất để tìm kiếm các cấu trúc mạng tối ưu từ các kiến trúc nơ-ron có thể [51, 56, 63]. Có chủ yếu năm phương pháp được sử dụng để tìm kiếm một mạng được tối ưu hóa; học tăng cường [1, 63], thuật toán di truyền [47, 57], các phương pháp dựa trên gradient [56], chia sẻ tham số [5] và dự đoán trọng số [4]. [63] sử dụng RL để tối ưu hóa các mạng được tạo ra từ các mô tả mô hình được cung cấp bởi một mạng tái phát. MetaQNN [1] sử dụng RL với chiến lược khám phá tham lam để tạo ra các kiến trúc CNN có hiệu suất cao. Các thuật toán di truyền được sử dụng để giải quyết các vấn đề tìm kiếm và tối ưu hóa sử dụng các toán tử lấy cảm hứng từ sinh học [57]. [47] sử dụng thuật toán di truyền để khám phá các kiến trúc mạng nơ-ron, giảm thiểu vai trò của con người trong thiết kế. Học dựa trên gradient cho phép một mạng tối ưu hóa hiệu quả các thể hiện mới của một nhiệm vụ [51]. FBNets (Facebook-Berkeley-Nets) [56] sử dụng một phương pháp dựa trên gradient để tối ưu hóa các CNN được tạo ra bởi một framework tìm kiếm kiến trúc nơ-ron có thể vi phân.

--- TRANG 3 ---
3. Cắt tỉa meta có phần thưởng
Thuật toán cắt tỉa meta có phần thưởng đề xuất sử dụng một hệ số phần thưởng để kiểm soát sự đánh đổi giữa độ chính xác và hiệu quả của mỗi mô hình thay vì tìm mô hình có độ chính xác cao nhất trong một phạm vi FLOPs đã đặt trước. Mục tiêu là tối đa hóa phần thưởng, tỷ lệ thuận trực tiếp với độ chính xác và tỷ lệ nghịch với hiệu quả của mô hình. Phương pháp này được thực hiện trong ba giai đoạn: huấn luyện, tìm kiếm và huấn luyện lại như được hiển thị trong Thuật toán 1.

3.1. Huấn luyện
Hầu hết các CNN phổ biến [16, 22, 48] chủ yếu sử dụng ba loại lớp; khối tích chập, bottleneck, và các lớp tuyến tính. Quy mô kênh đại diện cho kích thước của mỗi lớp. Đối với khối tích chập ban đầu và mỗi loại bottleneck, normalization theo batch với các kích thước từ 10% đến 100% kích thước của kiến trúc gốc, được phân bố đều giữa 31 trọng số ban đầu.

Hình 1. Phương pháp huấn luyện ngẫu nhiên

Mỗi mô hình được định nghĩa bởi một NEV, là một danh sách các số ngẫu nhiên định nghĩa quy mô mà mỗi lớp được cắt tỉa từ 31 quy mô và các trọng số Batch Normalisation tương ứng của chúng. Nó cũng tạo ra các lớp tuyến tính ở quy mô được định nghĩa bởi NEV. NEV được truyền vào hai lớp Fully Connected (FC) tạo ra ma trận trọng số như được hiển thị trong Hình 1. Các trọng số của những lớp này được huấn luyện bởi các gradient của trọng số được tạo ra tính toán w.r.t các trọng số của các lớp FC. Các trọng số được tạo ra cho mỗi kết hợp kích thước bộ lọc đầu ra được ánh xạ duy nhất bởi vì hàm chuyển đổi NEV thành trọng số là một hàm đơn ánh. Do đó, cho mỗi epoch, một NEV ngẫu nhiên tạo ra một mô hình mới với các trọng số ánh xạ một-một với không gian mẫu của các NEV. Những trọng số được tạo ra này được định hình lại. Mỗi giá trị Ci trong NEV C tương ứng với các kênh đầu ra của lớp i.

Một khi một mô hình được tạo ra, nó được huấn luyện cho mỗi batch của dữ liệu huấn luyện với những trọng số ban đầu này, và tổn thất cross-entropy được tính toán để cập nhật các trọng số. Mỗi mô hình về cơ bản là một phần của một mô hình hoàn chỉnh, nơi phần được định nghĩa bởi NEV. Dữ liệu xác thực không được sử dụng để xác thực mô hình, thay vào đó chỉ để đo lường tiến độ.

3.2. Tìm kiếm
Các mô hình được tạo ra từ các ứng cử viên NEV sử dụng các trọng số của mô hình đã huấn luyện để tạo ra một mô hình đã cắt tỉa. Mỗi NEV do đó được chuyển đổi thành một mô hình đã cắt tỉa và phần thưởng được tính toán cho chúng. Tìm kiếm tiến hóa [45] sau đó được sử dụng để tìm NEV tốt nhất, do đó tìm ra mô hình đã cắt tỉa tối ưu. Các trọng số ban đầu là các trọng số đã huấn luyện, nhưng mô hình cuối cùng sẽ được huấn luyện từ đầu để loại bỏ bias trong mô hình được huấn luyện trước.

3.2.1 Tạo gen
Các ứng cử viên ngẫu nhiên được tạo ra để khởi tạo tìm kiếm tiến hóa. Mỗi gen được tạo ra như một danh sách các kích thước tương ứng với mô hình với các giá trị đại diện cho trọng số từ từ điển. Tuy nhiên, vì các chỉ số của các mô hình được tạo ra bởi những NEV này cũng ngẫu nhiên, các siêu tham số tùy ý được sử dụng để kiểm soát mô hình cuối cùng. Một gen được coi là hợp lệ nếu FLOPs của mô hình được tạo ra nằm giữa maxFLOPs và minFLOPs. FLOPs của mỗi gen được lưu trữ như phần tử cuối cùng của NEV để giảm thời gian tính toán tổng thể. Điều này sau đó được thay thế bằng phần thưởng.

3.2.2 Phần thưởng và lựa chọn NEV
Các ứng cử viên được xếp hạng sau mỗi epoch theo phần thưởng, được tính toán như một tích của các hệ số độ chính xác và hiệu quả được đưa ra bởi Phương trình (2) và (3) cho mỗi NEV như được hiển thị trong Hình 2. Phần thưởng được tính toán theo công thức sau:

R(Gi) = α(Gi; ba) × β(Gi; bf)                    (1)

Các hệ số độ chính xác và hiệu quả, được ký hiệu bằng α và β, được định nghĩa như:

α(Gi; ba) = ba / (ba - A(Gi))²                  (2)

β(Gi; bf) = log(bf) / F(Gi)                     (3)

trong đó Gi biểu thị một gen có chỉ số i từ tất cả các gen ứng cử viên G, và A và F đại diện cho các hàm trả về độ chính xác và FLOPs của mô hình được tạo ra sử dụng gen được truyền vào chúng. Hệ số độ chính xác tăng theo cấp số nhân

--- TRANG 4 ---
Hình 2. Tính toán phần thưởng cho các vector mã hóa mạng.

với việc tăng độ chính xác của mô hình, nhưng khi nó tiến đến độ chính xác cơ sở, giá trị có xu hướng vô cực. Vì mô hình không được tinh chỉnh, độ chính xác không đủ gần với độ chính xác của mô hình cơ sở, ba, để phần thưởng đạt đến mức cao. Nếu chưng cất kiến thức được sử dụng để tăng độ chính xác của mô hình mới, độ chính xác cơ sở sẽ là độ chính xác của mô hình mới, do đó loại bỏ bất kỳ tác động tiêu cực nào từ bản chất đối xứng của phương trình 2. Mặt khác, hệ số hiệu quả β giảm tuyến tính với việc tăng FLOPs nhưng bị giới hạn bởi FLOPs của mô hình gốc bf. Vì tỷ lệ cắt tỉa tỷ lệ nghịch với FLOPs, một hệ số hiệu quả thấp hơn tương ứng với tỷ lệ cắt tỉa cao hơn cho phần lớn. Hàm phần thưởng tỷ lệ thuận trực tiếp với độ chính xác và tỷ lệ cắt tỉa.

Hệ số độ chính xác tỷ lệ thuận trực tiếp với phần thưởng nhưng được điều tiết bởi hệ số hiệu quả. Điều này tạo ra sự cân bằng giữa chúng để độ chính xác cao không đạt được với chi phí tỷ lệ cắt tỉa thấp trong mô hình cuối cùng. Một khi phần thưởng được tính toán, nó được lưu trữ như phần tử cuối cùng của NEV, sau đó được sử dụng để xếp hạng mỗi NEV. Top-50 NEV từ mỗi epoch được lưu trữ, sau đó 10 cái tốt nhất trong số chúng được đột biến và lai tạo để có được các gen ứng cử viên cho epoch tiếp theo.

3.2.3 Đột biến và lai tạo
Các ứng cử viên tốt nhất từ mỗi epoch được đột biến và lai tạo để tạo ra các ứng cử viên cho epoch tiếp theo. Đột biến liên quan đến việc thay đổi một vài phần tử trong một gen để tạo ra một gen mới. Có 10% cơ hội cho mỗi phần tử trong một gen được thay đổi thành một phần tử hợp lệ ngẫu nhiên. Lai tạo là việc kết hợp hai gen ngẫu nhiên để tạo ra một gen mới. Một phần tử được chọn ngẫu nhiên từ một trong hai gen được chọn cho mỗi chỉ số. Các cấu hình kênh trong một vùng cục bộ của không gian cấu hình có xu hướng có các chỉ số tương tự [33], vì vậy các ứng cử viên mới cũng có độ chính xác và FLOPs tương tự. Điều này làm cho phần thưởng của ít nhất ứng cử viên tốt nhất có xu hướng không giảm. Nếu phần thưởng không tăng trong quá nhiều epoch, tìm kiếm di truyền bị kẹt trong cực tiểu cục bộ. Tuy nhiên, vì tìm kiếm tiến hóa là một tìm kiếm không lồi đa chiều, các điểm tới hạn với lỗi lớn hơn nhiều so với cực tiểu toàn cục có khả năng là các điểm yên ngựa [6]. Nói cách khác, các cực tiểu cục bộ được tìm thấy có khả năng đủ gần với cực tiểu toàn cục, vì vậy tìm kiếm có thể được kết thúc. Không phải là không thể rằng việc đột biến và lai tạo nhiều gen hơn có thể tìm thấy các gen, nhưng việc tăng tỷ lệ đột biến và lai tạo sẽ ảnh hưởng đến tính toàn vẹn của tìm kiếm tiến hóa. Nếu không thể tạo ra đủ ứng cử viên mới từ hai toán tử tiến hóa, phần còn lại được tạo ra sử dụng các gen ngẫu nhiên.

3.3. Huấn luyện lại
Một khi tìm kiếm tiến hóa đã được hoàn thành, gen tốt nhất được chọn như gen đầu tiên trong danh sách các ứng cử viên. Đây là gen có phần thưởng cao nhất như được tìm thấy sau nhiều epoch tìm kiếm di truyền. NEV tốt nhất được chuyển đổi thành một mô hình và được huấn luyện từ đầu. Tất cả các thuật toán cắt tỉa đều huấn luyện một mô hình đã cắt tỉa trong vài epoch để lấy lại độ chính xác bị mất trong một quá trình được gọi là tinh chỉnh [15]. Trong thuật toán cắt tỉa meta có phần thưởng, mô hình được tạo ra từ một NEV thay vì sử dụng NEV để cắt tỉa, và sau đó được huấn luyện từ đầu. Do đó độ chính xác bão hòa ở một epoch cao hơn trong quá trình tinh chỉnh.

4. Kết quả thực nghiệm
Trong phần này, chúng tôi chứng minh tính ưu việt của phương pháp cắt tỉa meta có phần thưởng. Chúng tôi đầu tiên mô tả các cài đặt thực nghiệm để tái tạo các thí nghiệm. Sau đó chúng tôi so sánh các kết quả thu được với các phương pháp khác cắt tỉa ba mạng chính. Cuối cùng, chúng tôi thực hiện một nghiên cứu ablation để hiểu hiệu quả của phương pháp được đề xuất.

4.1. Cài đặt thực nghiệm
Mạng ResNet-50 [16] được huấn luyện trong 32 epoch, trong khi MobileNetV1 [22] và MobileNetV2 [48] được huấn luyện trong 64 epoch. ResNet và MobileNetV2 được huấn luyện lại sau khi tìm kiếm trong 400 epoch, nhưng MobileNetV1 chỉ yêu cầu 320 epoch. Tìm kiếm các NEV mất 20 epoch cho tất cả các mạng. Mỗi epoch tìm kiếm 50 NEV, tìm kiếm qua 1000 NEV duy nhất trong suốt quá trình chạy. MobileNetV1 và MobileNetV2 đều sử dụng bộ lập lịch Lambda để suy giảm các mô hình với γ là 0.1 mỗi epoch từ tỷ lệ học

--- TRANG 5 ---
Thuật toán 1 Thuật toán của cắt tỉa meta có phần thưởng
Siêu tham số: maxtraining: Số epoch huấn luyện, maxiter: Số epoch tìm kiếm, maxtuning: Số epoch tinh chỉnh
Đầu vào: dataset: hình ảnh huấn luyện có thể được chia thành batch, ri: Số nguyên ngẫu nhiên được lập chỉ mục tại i, wi: Trọng số ngẫu nhiên được lập chỉ mục tại i, ∇: Gradient của tổn thất của mô hình đã cho
Các hàm: norm(nev) chuyển đổi nev thành trọng số, FC(weights) tạo mô hình sử dụng trọng số, f(model; data) huấn luyện mô hình sử dụng dữ liệu đã cho, reward(nev) tính toán phần thưởng của mô hình được tạo ra sử dụng nev, mutation và crossover thực hiện các thao tác tiến hóa trên danh sách nev
Đầu ra: x: Mô hình đã cắt tỉa và huấn luyện

for i = 0, 1, ..., maxtraining do
    for each batch in dataset do
        nev = [r1, r2, ..., rn]
        {w1, w2, ..., wn} = norm(nev)
        x = FC({w1, w2, ..., wn})
        L = f(x, batch)
        x += ∇L
    end for
end for

candidate = Danh sách n nev ngẫu nhiên
for i = 0, 1, ..., maxiter do
    rewards = [r1, r2, ..., rn]
    for j = 0, 1, ..., n do
        rj = reward(nevj)
    end for
    sắp xếp candidate theo thứ tự giảm dần của rewards
    mutated = mutation(candidate[:10])
    crossedover = crossover(candidate[:10])
    candidate = mutated + crossedover
end for

{w1, w2, ..., wn} = norm(candidate[0])
x = FC({w1, w2, ..., wn})
for i = 0, 1, ..., maxtuning do
    x += ∇f(x, dataset)
end for

ban đầu là 0.2. Bộ lập lịch cho ResNet, tuy nhiên, giảm với hệ số 0.1 tại epoch 80 và 160.

Các thí nghiệm được tiến hành trên ba mạng thường được sử dụng bao gồm ResNet-50, MobileNetV1, và MobileNetV2. Các mạng được huấn luyện sử dụng ImageNet [7] từ đầu như được mô tả trong phần trước. ImageNet bao gồm 1.2M hình ảnh huấn luyện và 50K hình ảnh xác thực. Nó cũng bao gồm 100K hình ảnh kiểm tra, nhưng vì các nhãn của dữ liệu kiểm tra không được phát hành, dữ liệu xác thực được sử dụng để kiểm tra. Dữ liệu xác thực không được sử dụng trong bất kỳ phần nào của quá trình huấn luyện ngoại trừ tính toán độ chính xác ở mỗi giai đoạn. Như một hệ quả tự nhiên của việc sử dụng tính toán tiến hóa, cắt tỉa meta có phần thưởng tiêu tốn nhiều tài nguyên trong việc tính toán các mạng đã cắt tỉa. Nhưng chi phí này được cân bằng bởi hiệu quả và độ chính xác của các mô hình được tạo ra. Một mạng không cần được tạo ra mỗi lần nó được sử dụng bởi vì kiến thức được chưng cất từ nó có thể được chuyển giao và sử dụng trong các bối cảnh khác nhau.

4.2. Giao thức đánh giá
Bốn chỉ số được sử dụng để đánh giá các thuật toán cắt tỉa: tỷ lệ tham số, lỗi top-1 và top-5 và FLOPs. Tỷ lệ tham số là tỷ lệ của mô hình đã cắt tỉa so với mô hình cơ sở. Nó được tính toán như:

P = (Pm/Pb) × 100%                                    (4)

trong đó Pm và Pb là số lượng trọng số trong mô hình đã cắt tỉa và mô hình cơ sở tương ứng. Độ chính xác là tỷ lệ phần trăm dữ liệu xác thực được xác định chính xác so với toàn bộ tập dữ liệu. Lỗi Top-1 là nghịch đảo của độ chính xác, tức là tỷ lệ hình ảnh mà các nhãn dự đoán có xác suất cao nhất là sai. Lỗi Top-5 là tỷ lệ hình ảnh mà nhãn chính xác không có mặt trong năm xác suất cao nhất của các nhãn dự đoán. FLOPs là thước đo số lượng phép toán dấu phẩy động được tính toán mỗi giây.

4.3. Hiệu suất trên ResNet-50
ResNet-50 là một CNN với độ sâu 50 lớp. Nó được tạo ra để giải quyết sự suy giảm trong mô hình khi các lớp sâu hơn được xếp chồng [16]. ResNet sử dụng các kết nối bỏ qua để xác định ánh xạ. Điều này thêm các đặc trưng với các tham số gốc của chúng trước khi truyền chúng vào lớp tiếp theo. Ánh xạ đồng nhất tiếp theo là phép chiếu tuyến tính được sử dụng để mở rộng các kênh của các đặc trưng để có thể thêm vào với các tham số gốc.

Phương pháp             Lỗi Top-1  Lỗi Top-5  FLOPs
Cơ sở [33]              23.40%     -          4110M
GAL-0.5 [37]            28.05%     9.06%      2341M
SSS [25]                28.18%     9.21%      2341M
HRank [36]              25.02%     7.67%      2311M
Cắt tỉa ngẫu nhiên [33] 24.87%     7.48%      2013M
AutoPruner [43]         25.24%     7.85%      2005M
Adapt-DCP [38]          24.85%     7.70%      1955M
MetaPruning [40]        24.60%     -          2005M
Cắt tỉa meta có phần thưởng 24.24%  7.35%     1950M

Bảng 1. So sánh các phương pháp cắt tỉa kênh tiên tiến nhất với ResNet-50

Bảng 1 hiển thị kết quả của ResNet-50 được huấn luyện sử dụng ImageNet-2012 sau khi cắt tỉa với cắt tỉa meta có phần thưởng và các phương pháp cạnh tranh khác. Có thể suy ra rằng phương pháp này có tỷ lệ lỗi thấp hơn mọi phương pháp, và điều này

--- TRANG 6 ---
đạt được trong khi giữ FLOPs tương đối thấp. FLOPs, so với mạng cơ sở [33], giảm 52.55%, nhưng lỗi chỉ tăng 0.84%. Khi so sánh với cắt tỉa ngẫu nhiên tiêu chuẩn, với việc giảm FLOPs tương tự, có lỗi thấp hơn 0.63%. Phương pháp MetaPruning [40] cho thấy lỗi cao hơn 0.36% trong khi vẫn sử dụng FLOPs cao hơn 1.34% so với mô hình cơ sở. Adapt-DCP [38] có việc giảm FLOPs gần nhất so với cơ sở, nhưng phương pháp này có lỗi thấp hơn 0.61%. Các phương pháp SSS [25] và HRank [36] có tỷ lệ cắt tỉa rất tương tự với cắt tỉa meta có phần thưởng, nhưng có FLOPs cao hơn khoảng 9%, và lỗi cao hơn 3.94% và 0.78% tương ứng.

4.4. Hiệu suất trên MobileNetV2
MobileNetV2 chứa tích chập theo chiều sâu và theo điểm. Nó có một residual đảo ngược với một bottleneck tuyến tính nhận biểu diễn nén chiều thấp làm đầu vào và mở rộng nó thành chiều cao hơn, sau đó lọc chúng với các tích chập theo chiều sâu nhẹ như trong MobileNetV1 [48]. MobileNetV2 là một mạng hiệu quả với lỗi tương đối thấp. Do đó, chứng minh trên MobileNetV2 là một cách hiệu quả để hiển thị hiệu suất của thuật toán cắt tỉa.

Phương pháp                      Lỗi Top-1  Lỗi Top-5  FLOPs
Cơ sở [33]                      28.12%     9.71%      314M
0.75 MobileNetV2 [33]           30.20%     -          220M
Cắt tỉa ngẫu nhiên [10]         29.10%     -          223M
AMC [18]                        29.20%     -          220M
MetaPruning [40]                28.80%     -          227M
Lựa chọn tham lam [61]          28.80%     -          201M
Adapt-DCP [38]                  28.55%     -          216M
Cắt tỉa meta có phần thưởng     28.51%     10.65%     199M

Bảng 2. So sánh các phương pháp cắt tỉa kênh tiên tiến nhất với MobileNetV2

Bảng 2 so sánh hiệu suất của phương pháp cắt tỉa meta có phần thưởng với các phương pháp tiên tiến nhất. Phương pháp cắt tỉa meta có phần thưởng có FLOPs thấp hơn bất kỳ phương pháp nào khác trong khi cho thấy chỉ 0.39% lỗi cao hơn so với cơ sở. 0.75 MobileNetV2, là MobileNetV2 có chiều rộng thấp hơn 25% so với gốc, có lỗi cao hơn 1.69% so với phương pháp này. Khi so sánh với cắt tỉa ngẫu nhiên, là cơ sở cho tất cả các phương pháp cắt tỉa [2], phương pháp này có lỗi thấp hơn 0.59%. MetaPruning [40] có lỗi cao hơn 0.29% mặc dù có FLOPs cao hơn 7.64%. AMC [18] và Adapt-DCP [38] có lỗi cao hơn 0.69% và 0.04% và FLOPs. Cắt tỉa meta có phần thưởng cũng vượt trội hơn Lựa chọn tham lam [61] 0.29% mặc dù có lượng FLOPs gần như tương tự.

4.5. Hiệu suất trên MobileNetV1
MobileNetV1 có kiến trúc được tinh giản xây dựng các mạng sâu nhẹ sử dụng các tích chập tách biệt theo chiều sâu. Tất cả các lớp sử dụng Batch Normalisation và ReLu, ngoại trừ lớp fully connected tiếp theo là lớp softmax cho phân loại [22].

Phương pháp                     Lỗi Top-1  Lỗi Top-5  FLOPs
Cơ sở [22]                     29.40%     -          569M
0.75 MobileNet-224 [22]        31.60%     -          325M
FTWT (r=1.0) [10]              30.34%     -          335M
MetaPruning [40]               29.10%     -          324M
Cắt tỉa meta có phần thưởng    29.60%     9.65%      295M

Bảng 3. So sánh các phương pháp cắt tỉa kênh tiên tiến nhất với MobileNetV1

Trong Bảng 3, chúng tôi so sánh phương pháp cắt tỉa meta có phần thưởng với các kỹ thuật cắt tỉa cạnh tranh khác để cắt tỉa một mô hình MobileNetV1. Nó gần như lấy lại được độ chính xác của mạng cơ sở [22], với độ chính xác thấp hơn 0.2% và FLOPs thấp hơn 48.15%. Phương pháp này rõ ràng đạt được kết quả vượt trội khi so sánh với phương pháp cắt tỉa Fire-Together-Wire-Together [10], với lỗi thấp hơn 0.94% và FLOPs thấp hơn 7.03%. Nó vượt trội hơn 0.75 MobileNet-224 [22], là MobileNetV1 với chiều rộng thấp hơn 25%, 2% trong khi sử dụng FLOPs thấp hơn 5.27%. Trong khi MetaPruning [40] cho thấy lỗi thấp hơn cả mạng cơ sở, phương pháp của chúng tôi có FLOPs thấp hơn. Tuy nhiên, kích thước của mạng đã cắt tỉa vẫn lớn hơn 9.83%, khi so sánh với phương pháp được đề xuất. Điều này có thể do thiếu kết nối tắt trong MobileNetV1 mặc dù là một mạng nhỏ hơn, dẫn đến một số lượng lớn các lớp fully-connected. Về hiệu suất đạt được cho mỗi tài nguyên, cắt tỉa meta có phần thưởng vượt trội hơn MetaPruning. Có thể giả định rằng phương pháp cắt tỉa meta có phần thưởng có thể có kết quả tốt hơn với các hàm phần thưởng tiên tiến hơn. Điều này sẽ được xác thực bởi nghiên cứu tiếp theo về tính mạnh mẽ của các siêu tham số khác nhau.

4.6. Thảo luận
Từ các kết quả, rõ ràng rằng phương pháp được đề xuất hoạt động tốt nhất dưới các hàm phần thưởng phù hợp. Hàm phần thưởng trong trường hợp này tỷ lệ thuận trực tiếp với độ chính xác và tỷ lệ nghịch với FLOPs.

Khi độ chính xác của mô hình đã cắt tỉa tiến đến độ chính xác cơ sở, phần thưởng tăng theo cấp số nhân. Trong MetaPruning [40], phần thưởng tỷ lệ thuận trực tiếp với độ chính xác, trong khi trong phương pháp này, phần thưởng tỷ lệ thuận với bình phương độ chính xác của mô hình. Điều này tự nó sẽ không tăng độ chính xác của mô hình cuối cùng, nhưng việc theo đuổi phần thưởng cao hơn chỉ phụ thuộc vào độ chính xác có thể dẫn đến mô hình cuối cùng có FLOPs cao. Điều này có thể thấy trong MetaPruning, nơi mô hình đã cắt tỉa có xu hướng

--- TRANG 7 ---
cho thấy FLOPs cao như FLOPs tối đa được đặt trước sẽ cho phép. Phần thưởng được kiểm soát bởi hệ số hiệu quả để ngăn chặn điều này.

Phần thưởng giảm với việc tăng FLOPs bởi vì hệ số hiệu quả tỷ lệ nghịch với FLOPs. Tuy nhiên, nếu tỷ lệ quá cao, phần thưởng sẽ bị điều tiết. Do đó nó được kiểm soát bởi hệ số hiệu quả giảm tuyến tính.

Hình 3. Phần thưởng ở các mức Độ chính xác và FLOPs khác nhau.

Từ định nghĩa của hàm phần thưởng trong Phương trình (1), rõ ràng rằng chỉ có độ chính xác cao thôi là không đủ để một mô hình được chọn. Khi độ chính xác tăng, xác suất mô hình được chọn tăng, nhưng chỉ khi FLOPs của mô hình cũng đủ thấp. Điều này có thể suy ra từ cách phân bố Xanh tăng khi chúng ta di chuyển về phía bên phải như được hiển thị trong Hình 3. Phần thưởng tăng khi FLOPs giảm, nhưng nó không được coi là chấp nhận được cho đến khi độ chính xác đủ cao. Điều này có thể suy ra từ cách phân bố Đỏ tăng khi FLOPs giảm như được suy ra từ Hình 3. Do đó theo định nghĩa, phương pháp cắt tỉa meta có phần thưởng nghiêng về phía độ chính xác hơn.

Khi so sánh với MetaPruning, phần thưởng của phương pháp cắt tỉa meta có phần thưởng tăng cao hơn với mỗi lần lặp tìm kiếm. Điều này là do phần thưởng của MetaPruning tỷ lệ thuận trực tiếp với độ chính xác trong khi phần thưởng của cắt tỉa meta có phần thưởng tỷ lệ thuận với bình phương độ chính xác. Điều này có thể quan sát được trong Hình 4 hiển thị phần thưởng, độ chính xác, và FLOPs của mô hình tốt nhất sau mỗi lần lặp tìm kiếm. Độ chính xác và FLOPs, ở đầu, được chọn ngẫu nhiên, nhưng điều đó không thể thay đổi mà không làm xáo trộn tìm kiếm tiến hóa cơ bản. Nhưng có thể quan sát thấy rằng FLOPs của mô hình tốt nhất trong MetaPruning có xu hướng tăng phần lớn trong khi, trong trường hợp phương pháp cắt tỉa meta có phần thưởng, nó có xu hướng giữ thấp. Độ chính xác tăng trong cả hai trường hợp, nhưng MetaPruning bão hòa sớm hơn cắt tỉa meta có phần thưởng. Cũng có thể suy ra rằng nếu hai mô hình bắt đầu với cùng một batch các mô hình được khởi tạo ngẫu nhiên, phương pháp cắt tỉa meta có phần thưởng sẽ có độ chính xác cao hơn và FLOPs thấp hơn bởi vì độ dốc của độ chính xác tốt nhất và FLOPs của các mô hình tốt nhất cao hơn và thấp hơn tương ứng so với MetaPruning.

Tuy nhiên, cách các NEV được chọn, cả ngẫu nhiên và sau đột biến hoặc lai tạo, có nghĩa là FLOPs của mô hình đã cắt tỉa xấp xỉ tạo thành một đường cong hình chuông giữa 1350 và 2100. Điều này có thể thay đổi bằng cách kiểm soát việc tạo ra các kích thước bộ lọc ngẫu nhiên trong các NEV như được hiển thị trong Hình 5.

Tính mạnh mẽ của các siêu tham số được sử dụng bởi cắt tỉa meta có phần thưởng đã được khám phá bởi He et al. [17]. Khi hàm phần thưởng được điều chỉnh để thêm các siêu tham số khác nhau, các chỉ số khác nhau của mô hình cuối cùng có thể được kiểm soát. Có nhiều hệ số khác có thể được sử dụng trong phần thưởng. Tỷ lệ cắt tỉa của mô hình đã cắt tỉa có thể được đặt tỷ lệ nghịch với phần thưởng, vì tỷ lệ cắt tỉa thấp tự động dẫn đến FLOPs thấp hơn. FLOPs cũng tỷ lệ thuận trực tiếp với độ trễ phần cứng, là thời gian chạy của các mạng [9]. Nhưng điều này phụ thuộc vào phần cứng, và các phần cứng khác nhau có độ trễ khác nhau. Các chỉ số khác như tiêu thụ năng lượng cũng đã được sử dụng để cắt tỉa. NetAdapt [59] sử dụng tiêu thụ năng lượng như một chỉ số để đo lường độ phức tạp của mạng ở mọi giai đoạn và cắt tỉa mạng thêm trong khi duy trì độ chính xác.

5. Kết luận
Trong công trình này, chúng tôi đã trình bày những điều sau: 1) Thực hiện hàm phần thưởng tốt hơn để meta-học các tham số cho cắt tỉa và cho phép kiểm soát tốt hơn các tham số khác nhau của mô hình đã cắt tỉa. 2) Phương pháp cắt tỉa meta có phần thưởng đã được chứng minh là vượt trội hơn các phương pháp tiên tiến nhất khác, với độ chính xác cao hơn và FLOPs thấp hơn so với các phương pháp cắt tỉa kênh truyền thống. 3) Hàm phần thưởng có thể được tối ưu hóa sử dụng các chỉ số khác để tối đa hóa phần thưởng. 4) ResNet-50, MobileNetV1 và MobileNetV2 được cắt tỉa hiệu quả.

6. Lời cảm ơn
Công trình này được hỗ trợ bởi quỹ của Quỹ Nghiên cứu Quốc gia Hàn Quốc (NRF) được tài trợ bởi Chính phủ Hàn Quốc (MSIT) (Số.2021R1C1C1012590), (Số. NRF-2022R1A4A1023248) và chương trình hỗ trợ Trung tâm Nghiên cứu Công nghệ Thông tin (ITRC) được giám sát bởi Viện Lập kế hoạch và Đánh giá Công nghệ Truyền thông và Thông tin (IITP) được tài trợ bởi Chính phủ Hàn Quốc (MSIT) (IITP-2022-2020-0-01808).

Tài liệu tham khảo
[1] Bowen Baker, Otkrist Gupta, Nikhil Naik, và Ramesh Raskar. Thiết kế kiến trúc mạng nơ-ron sử dụng học tăng cường. arXiv preprint arXiv:1611.02167, 2016. 2

--- TRANG 8 ---
Hình 4. Phần thưởng, Độ chính xác và FLOPs của các mô hình tốt nhất sau mỗi lần lặp tìm kiếm.

Hình 5. Phân bố FLOPs của 1000 NEV được tạo ngẫu nhiên với các phạm vi FLOPs khác nhau.

tăng cường. arXiv preprint arXiv:1611.02167, 2016. 2
[2] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, và John Guttag. Tình trạng của việc cắt tỉa mạng nơ-ron là gì? Proceedings of machine learning and systems, 2:129-146, 2020. 6
[3] Alexandre Bouchard-Côté, Slav Petrov, và Dan Klein. Cắt tỉa ngẫu nhiên: Tính toán hiệu quả kỳ vọng trong các chương trình động lớn. Advances in Neural Information Processing Systems, 22, 2009. 1, 2
[4] Andrew Brock, Theodore Lim, James M Ritchie, và Nick Weston. Smash: tìm kiếm kiến trúc mô hình một lần qua hypernetworks. arXiv preprint arXiv:1708.05344, 2017. 2
[5] Han Cai, Ligeng Zhu, và Song Han. Proxylessnas: Tìm kiếm kiến trúc nơ-ron trực tiếp trên nhiệm vụ mục tiêu và phần cứng. arXiv preprint arXiv:1812.00332, 2018. 2
[6] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, và Yoshua Bengio. Xác định và tấn công vấn đề điểm yên ngựa trong tối ưu hóa không lồi đa chiều. Advances in neural information processing systems, 27, 2014. 4
[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: Một cơ sở dữ liệu hình ảnh phân cấp quy mô lớn. Trong 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009. 5
[8] Jiankang Deng, Jia Guo, Niannan Xue, và Stefanos Zafeiriou. Arcface: Tổn thất lề góc cộng cho nhận dạng khuôn mặt sâu. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690-4699, 2019. 1, 2
[9] Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, và Min Sun. Dpp-net: Tìm kiếm tiến bộ nhận biết thiết bị cho các kiến trúc nơ-ron tối ưu Pareto. Trong Proceedings of the European Conference on Computer Vision (ECCV), pages 517-531, 2018. 7
[10] Sara Elkerdawy, Mostafa Elhoushi, Hong Zhang, và Nilanjan Ray. Fire together wire together: Một phương pháp cắt tỉa động với dự đoán mặt nạ tự giám sát. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12454-12463, 2022. 1, 2, 6
[11] Chelsea Finn, Pieter Abbeel, và Sergey Levine. Học meta bất khả tri mô hình cho thích ứng nhanh của các mạng sâu. Trong International conference on machine learning, pages 1126-1135. PMLR, 2017. 2
[12] Jonathan Frankle và Michael Carbin. Giả thuyết vé số may mắn: Tìm các mạng nơ-ron thưa thớt, có thể huấn luyện. arXiv preprint arXiv:1803.03635, 2018. 1, 2
[13] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, và William J Dally. Eie: Công cụ suy luận hiệu quả trên mạng nơ-ron sâu nén. ACM SIGARCH Computer Architecture News, 44(3):243-254, 2016. 1
[14] Song Han, Huizi Mao, và William J Dally. Nén sâu: Nén mạng nơ-ron sâu với cắt tỉa, lượng tử hóa huấn luyện và mã hóa huffman. arXiv preprint arXiv:1510.00149, 2015. 1
[15] Song Han, Jeff Pool, John Tran, và William Dally. Học cả trọng số và kết nối cho mạng nơ-ron hiệu quả. Advances in neural information processing systems, 28, 2015. 1, 4

--- TRANG 9 ---
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Học residual sâu cho nhận dạng hình ảnh. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 3, 4, 5
[17] Yang He, Yuhang Ding, Ping Liu, Linchao Zhu, Hanwang Zhang, và Yi Yang. Học tiêu chí cắt tỉa bộ lọc cho tăng tốc mạng nơ-ron tích chập sâu. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2009-2018, 2020. 7
[18] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, và Song Han. Amc: Automl cho nén và tăng tốc mô hình trên thiết bị di động. Trong Proceedings of the European conference on computer vision (ECCV), pages 784-800, 2018. 2, 6
[19] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, và Yi Yang. Cắt tỉa bộ lọc qua trung vị hình học cho tăng tốc mạng nơ-ron tích chập sâu. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4340-4349, 2019. 2
[20] Yang He, Ping Liu, Linchao Zhu, và Yi Yang. Cắt tỉa bộ lọc bằng cách chuyển sang các CNN lân cận với thuộc tính tốt. IEEE Transactions on Neural Networks and Learning Systems, 2022. 1
[21] Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho, và Pieter Abbeel. Gradient chính sách tiến hóa. Advances in Neural Information Processing Systems, 31, 2018. 2
[22] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, và Hartwig Adam. Mobilenets: Mạng nơ-ron tích chập hiệu quả cho ứng dụng thị giác di động. arXiv preprint arXiv:1704.04861, 2017. 3, 4, 6
[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, và Kilian Q Weinberger. Mạng nơ-ron tích chập kết nối dày đặc. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700-4708, 2017. 1
[24] Qiangui Huang, Kevin Zhou, Suya You, và Ulrich Neumann. Học cắt tỉa bộ lọc trong mạng nơ-ron tích chập. Trong 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 709-718. IEEE, 2018. 1, 2
[25] Zehao Huang và Naiyan Wang. Lựa chọn cấu trúc thưa thớt dựa trên dữ liệu cho mạng nơ-ron sâu. Trong Proceedings of the European conference on computer vision (ECCV), pages 304-320, 2018. 2, 5, 6
[26] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, và Li Fei-Fei. Phân loại video quy mô lớn với mạng nơ-ron tích chập. Trong Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725-1732, 2014. 1
[27] John K Kruschke và Javier R Movellan. Lợi ích của gain: Học tăng tốc và các lớp ẩn tối thiểu trong mạng lan truyền ngược. IEEE Transactions on systems, Man, and Cybernetics, 21(1):273-280, 1991. 1
[28] Abhishek Kumar, Rakesh Kumar Misra, Devender Singh, Sujeet Mishra, và Swagatam Das. Thuật toán tìm kiếm cầu cho bài toán tối ưu hóa toàn cục có ràng buộc. Applied Soft Computing, 85:105734, 2019. 1
[29] Dong-Gyu Lee và Yoon-Ki Kim. Hiểu ngữ nghĩa chung với nhánh đa cấp cho nhận thức lái xe. Applied Sciences, 12(6):2877, 2022. 1
[30] Dong-Gyu Lee và Seong-Whan Lee. Dự đoán hoạt động con người dựa trên bộ mô tả mối quan hệ sub-volume. Trong 2016 23rd International Conference on Pattern Recognition (ICPR), pages 2060-2065. IEEE, 2016. 1
[31] Dong-Gyu Lee và Seong-Whan Lee. Dự đoán hoạt động con người được quan sát một phần dựa trên biểu diễn sâu được huấn luyện trước. Pattern Recognition, 85:198-206, 2019. 1
[32] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Cắt tỉa bộ lọc cho convnets hiệu quả. arXiv preprint arXiv:1608.08710, 2016. 1
[33] Yawei Li, Kamil Adamczewski, Wen Li, Shuhang Gu, Radu Timofte, và Luc Van Gool. Xem xét lại cắt tỉa kênh ngẫu nhiên cho nén mạng nơ-ron. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 191-201, 2022. 4, 5, 6
[34] Yiying Li, Yongxin Yang, Wei Zhou, và Timothy Hospedales. Mạng feature-critic cho tổng quát hóa miền không đồng nhất. Trong International Conference on Machine Learning, pages 3915-3924. PMLR, 2019. 2
[35] Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, và Daniela Rus. Cắt tỉa bộ lọc có thể chứng minh cho mạng nơ-ron hiệu quả. arXiv preprint arXiv:1911.07412, 2019. 2
[36] Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, và Ling Shao. Hrank: Cắt tỉa bộ lọc sử dụng bản đồ đặc trưng thứ hạng cao. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1529-1538, 2020. 2, 5, 6
[37] Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao, Qixiang Ye, Feiyue Huang, và David Doermann. Hướng tới cắt tỉa CNN có cấu trúc tối ưu qua học đối kháng sinh. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2790-2799, 2019. 5
[38] Jing Liu, Bohan Zhuang, Zhuangwei Zhuang, Yong Guo, Junzhou Huang, Jinhui Zhu, và Mingkui Tan. Cắt tỉa mạng nhận biết phân biệt cho nén mô hình sâu. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. 1, 2, 5, 6
[39] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. Học mạng tích chập hiệu quả qua làm mảnh mạng. Trong Proceedings of the IEEE international conference on computer vision, pages 2736-2744, 2017. 1
[40] Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, và Jian Sun. Metapruning: Học meta cho cắt tỉa kênh mạng nơ-ron tự động. Trong Proceedings of the IEEE/CVF international conference on computer vision, pages 3296-3305, 2019. 1, 2, 5, 6
[41] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, và Trevor Darrell. Suy nghĩ lại giá trị của cắt tỉa mạng. arXiv preprint arXiv:1810.05270, 2018. 1
[42] Christos Louizos, Max Welling, và Diederik P Kingma. Học mạng nơ-ron thưa thớt qua chính quy hóa l0. arXiv preprint arXiv:1712.01312, 2017. 1

--- TRANG 10 ---
[43] Jian-Hao Luo và Jianxin Wu. Autopruner: Một phương pháp cắt tỉa bộ lọc có thể huấn luyện end-to-end cho suy luận mô hình sâu hiệu quả. Pattern Recognition, 107:107461, 2020. 2, 5
[44] Jian-Hao Luo và Jianxin Wu. Cắt tỉa mạng nơ-ron với kết nối residual và dữ liệu hạn chế. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1458-1467, 2020. 2
[45] Rammohan Mallipeddi, Ponnuthurai N Suganthan, Quan-Ke Pan, và Mehmet Fatih Tasgetiren. Thuật toán tiến hóa vi phân với tập hợp tham số và chiến lược đột biến. Applied soft computing, 11(2):1679-1696, 2011. 3
[46] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, và Jan Kautz. Ước tính tầm quan trọng cho cắt tỉa mạng nơ-ron. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11264-11272, 2019. 2
[47] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, và Alexey Kurakin. Tiến hóa quy mô lớn của các bộ phân loại hình ảnh. Trong International Conference on Machine Learning, pages 2902-2911. PMLR, 2017. 2
[48] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, và Liang-Chieh Chen. Mobilenetv2: Residual đảo ngược và bottleneck tuyến tính. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510-4520, 2018. 3, 4, 6
[49] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Làm chủ trò chơi go với mạng nơ-ron sâu và tìm kiếm cây. nature, 529(7587):484-489, 2016. 2
[50] Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, và Jason D Lee. Kiểm tra tính hợp lý của các phương pháp cắt tỉa: Vé ngẫu nhiên có thể thắng jackpot. Advances in Neural Information Processing Systems, 33:20390-20401, 2020. 1, 2
[51] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T Barron, và Ren Ng. Khởi tạo đã học để tối ưu hóa biểu diễn nơ-ron dựa trên tọa độ. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2846-2855, 2021. 2
[52] Hongduan Tian, Bo Liu, Xiao-Tong Yuan, và Qingshan Liu. Học meta với cắt tỉa mạng để giảm quá khớp. CoRR, 2019. 1
[53] Joaquin Vanschoren. Học meta: Một khảo sát. arXiv preprint arXiv:1810.03548, 2018. 2
[54] Vibashan VS, Domenick Poster, Suya You, Shuowen Hu, và Vishal M. Patel. Meta-uda: Phát hiện đối tượng nhiệt thích ứng miền không giám sát sử dụng học meta. Trong Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1412-1423, January 2022. 2
[55] Ronald J Williams. Các thuật toán gradient-following thống kê đơn giản cho học tăng cường kết nối chủ nghĩa. Machine learning, 8(3):229-256, 1992. 2
[56] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, và Kurt Keutzer. Fbnet: Thiết kế convnet hiệu quả nhận biết phần cứng qua tìm kiếm kiến trúc nơ-ron có thể vi phân. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10734-10742, 2019. 2
[57] Lingxi Xie và Alan Yuille. Genetic cnn. Trong Proceedings of the IEEE international conference on computer vision, pages 1379-1388, 2017. 2
[58] Kohei Yamamoto và Kurato Maeno. Pcas: Cắt tỉa kênh với thống kê attention cho nén mạng sâu. arXiv preprint arXiv:1806.05382, 2018. 1
[59] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, và Hartwig Adam. Netadapt: Thích ứng mạng nơ-ron nhận biết nền tảng cho ứng dụng di động. Trong Proceedings of the European Conference on Computer Vision (ECCV), pages 285-300, 2018. 7
[60] Jianbo Ye, Xin Lu, Zhe Lin, và James Z Wang. Suy nghĩ lại giả định smaller-norm-less-informative trong cắt tỉa kênh của các lớp tích chập. arXiv preprint arXiv:1802.00124, 2018. 2
[61] Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, và Qiang Liu. Mạng con tốt có thể chứng minh tồn tại: Cắt tỉa qua lựa chọn tiến tham lam. Trong International Conference on Machine Learning, pages 10820-10830. PMLR, 2020. 2, 6
[62] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, và Jinhui Zhu. Cắt tỉa kênh nhận biết phân biệt cho mạng nơ-ron sâu. Trong S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, và R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. 2
[63] Barret Zoph và Quoc V Le. Tìm kiếm kiến trúc nơ-ron với học tăng cường. arXiv preprint arXiv:1611.01578, 2016. 2
