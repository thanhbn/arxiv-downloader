# 2310.17157.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2310.17157.pdf
# Kích thước tệp: 1516997 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Deja Vu: Độ thưa thớt ngữ cảnh cho các LLM hiệu quả tại thời điểm suy luận
Zichang Liu1Jue Wang2Tri Dao3Tianyi Zhou4Binhang Yuan5Zhao Song6Anshumali Shrivastava1
Ce Zhang5Yuandong Tian7Christopher Ré3Beidi Chen8 7

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) với hàng trăm tỷ tham số đã tạo ra một làn sóng mới của các ứng dụng AI thú vị. Tuy nhiên, chúng có chi phí tính toán đắt đỏ tại thời điểm suy luận. Độ thưa thớt là một cách tiếp cận tự nhiên để giảm chi phí này, nhưng các phương pháp hiện tại hoặc yêu cầu việc huấn luyện lại tốn kém, phải từ bỏ khả năng học trong ngữ cảnh của LLM, hoặc không mang lại tăng tốc thời gian thực tế trên phần cứng hiện đại. Chúng tôi giả thuyết rằng độ thưa thớt ngữ cảnh, là các tập hợp nhỏ, phụ thuộc đầu vào của các đầu attention và tham số MLP tạo ra đầu ra gần giống với mô hình dày đặc cho một đầu vào cụ thể, có thể giải quyết những vấn đề này. Chúng tôi chỉ ra rằng độ thưa thớt ngữ cảnh tồn tại, có thể được dự đoán chính xác, và chúng ta có thể khai thác nó để tăng tốc suy luận LLM về thời gian thực tế mà không làm giảm chất lượng hoặc khả năng học trong ngữ cảnh của LLM. Dựa trên những hiểu biết này, chúng tôi đề xuất DEJAVU, một hệ thống sử dụng thuật toán chi phí thấp để dự đoán độ thưa thớt ngữ cảnh một cách tức thời cho các đầu vào tới mỗi lớp, cùng với việc triển khai không đồng bộ và nhận biết phần cứng để tăng tốc suy luận LLM. Chúng tôi xác nhận rằng DEJAVU có thể giảm độ trễ suy luận của OPT-175B hơn 2× so với FasterTransformer tiên tiến nhất, và hơn 6× so với việc triển khai Hugging Face được sử dụng rộng rãi, mà không làm giảm chất lượng mô hình. Mã nguồn có sẵn tại https://github.com/FMInference/DejaVu.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM), như GPT-3, PaLM, và OPT đã chứng minh rằng một số lượng khổng lồ các tham số giải phóng hiệu suất ấn tượng và khả năng học trong ngữ cảnh nổi bật—chúng có thể thực hiện một tác vụ bằng cách điều kiện hóa trên các ví dụ đầu vào-đầu ra, mà không cập nhật tham số của chúng (Bommasani et al., 2021; Liang et al., 2022; Brown et al., 2020; Min et al., 2022; Chan et al., 2022). Tuy nhiên, chúng rất đắt đỏ tại thời điểm suy luận, đặc biệt đối với các ứng dụng nhạy cảm với độ trễ (Pope et al., 2022). Một mô hình suy luận lý tưởng nên sử dụng ít tính toán và bộ nhớ hơn trong khi duy trì hiệu suất và khả năng đặc biệt của các LLM được huấn luyện trước. Cách tiếp cận đơn giản và tự nhiên nhất là thưa thớt hóa hoặc cắt tỉa, có lịch sử lâu dài trước thời đại LLM (LeCun et al., 1989).

Thật không may, việc tăng tốc các LLM thưa thớt thời gian suy luận trong thời gian thực tế trong khi duy trì chất lượng và khả năng học trong ngữ cảnh vẫn là một vấn đề thách thức.

Mặc dù độ thưa thớt và cắt tỉa đã được nghiên cứu kỹ lưỡng, chúng không được áp dụng rộng rãi trên các LLM do sự đánh đổi chất lượng và hiệu quả kém trên phần cứng hiện đại như GPU. Đầu tiên, việc huấn luyện lại hoặc cắt tỉa lặp đi lặp lại các mô hình ở quy mô hàng trăm tỷ tham số là không khả thi. Do đó, các phương pháp trong cắt tỉa lặp và giả thuyết vé số (Lee et al., 2018; Frankle & Carbin, 2018) chỉ có thể được áp dụng cho các mô hình quy mô nhỏ hơn. Thứ hai, việc tìm ra độ thưa thớt bảo tồn khả năng học trong ngữ cảnh của LLM là thách thức. Nhiều công trình đã chỉ ra hiệu quả của cắt tỉa phụ thuộc tác vụ (Michel et al., 2019; Bansal et al., 2022), nhưng việc duy trì các mô hình khác nhau cho mỗi tác vụ mâu thuẫn với mục tiêu độc lập tác vụ của LLM. Cuối cùng, khó có thể đạt được tăng tốc thời gian thực tế với độ thưa thớt không có cấu trúc do khó khăn nổi tiếng của nó với phần cứng hiện đại (Hooker, 2021). Ví dụ, phát triển gần đây trong cắt tỉa zero-shot như SparseGPT (Frantar & Alistarh, 2023) tìm thấy 60% độ thưa thớt không có cấu trúc nhưng chưa dẫn đến bất kỳ tăng tốc thời gian thực tế nào.

Một độ thưa thớt lý tưởng cho LLM nên (i) không yêu cầu huấn luyện lại mô hình, (ii) bảo tồn chất lượng và khả năng học trong ngữ cảnh, và (iii) dẫn đến tăng tốc thời gian thực tế trên phần cứng hiện đại. Để đạt được những yêu cầu khắt khe như vậy, chúng tôi vượt ra ngoài độ thưa thớt tĩnh trong các công trình trước đây (ví dụ, cắt tỉa trọng số có cấu trúc/không có cấu trúc). Thay vào đó, chúng tôi hình dung độ thưa thớt ngữ cảnh, là các tập hợp nhỏ, phụ thuộc đầu vào của các đầu attention và tham số MLP dẫn đến đầu ra (gần đúng) giống như mô hình đầy đủ cho một đầu vào. Được trình hầm từ các kết nối giữa LLM, Mô hình Markov Ẩn (Xie et al., 2022; Baum & Petrie, 1966), và thuật toán Viterbi cổ điển (Viterbi, 1967), chúng tôi giả thuyết rằng đối với các LLM được huấn luyện trước,
độ thưa thớt ngữ cảnh tồn tại với bất kỳ đầu vào nào.

Giả thuyết này, nếu đúng, sẽ cho phép chúng ta cắt bỏ các đầu attention và tham số MLP cụ thể (độ thưa thớt có cấu trúc) một cách tức thời cho thời gian suy luận, mà không sửa đổi các mô hình được huấn luyện trước. Tuy nhiên, có ba thách thức.

Sự tồn tại: Việc xác minh xem độ thưa thớt ngữ cảnh như vậy có tồn tại hay không là không tầm thường, và việc xác minh ngây thơ có thể tốn kém một cách cấm đoán.

Dự đoán: Ngay cả khi độ thưa thớt ngữ cảnh tồn tại, việc dự đoán độ thưa thớt cho một đầu vào cụ thể trước đó là thách thức.

Hiệu quả: Ngay cả khi độ thưa thớt có thể được dự đoán, việc đạt được tăng tốc thời gian thực tế từ đầu đến cuối có thể khó khăn.

Lấy OPT-175B làm ví dụ, độ trễ của một khối MLP chỉ là 0,2 ms trên máy 8×A100 80GB. Không có dự đoán nhanh và triển khai tối ưu, chi phí có thể dễ dàng tăng độ trễ LLM thay vì giảm nó.

Trong công trình này, chúng tôi giải quyết những thách thức này như sau:

Sự tồn tại: May mắn thay, chúng tôi xác minh sự tồn tại của độ thưa thớt ngữ cảnh với một cách tiếp cận đơn giản một cách đáng ngạc nhiên. Để đạt được đầu ra về cơ bản giống nhau, độ thưa thớt ngữ cảnh trung bình là 85% thưa thớt có cấu trúc và do đó có thể dẫn đến giảm 7× tham số cho mỗi đầu vào cụ thể trong khi duy trì độ chính xác (Hình 1(a)). Trong quá trình khám phá độ thưa thớt ngữ cảnh, chúng tôi tạo ra các quan sát thực nghiệm quan trọng và xây dựng hiểu biết lý thuyết về các thành phần chính trong LLM giúp giải quyết thách thức dự đoán và hiệu quả.

[Hình 2 với mô tả: DEJAVU sử dụng các bộ dự đoán lookahead để tránh chi phí dự đoán: cho đầu vào tới lớp attention tại khối k, chúng (không đồng bộ) dự đoán độ thưa thớt ngữ cảnh cho MLP tại khối k, và cho đầu vào tới MLP tại khối k, chúng dự đoán độ thưa thớt cho đầu attention tại lớp tiếp theo.]

Dự đoán: Chúng tôi khám phá ra rằng độ thưa thớt ngữ cảnh không chỉ phụ thuộc vào các token đầu vào riêng lẻ (tức là, độ thưa thớt động không ngữ cảnh) mà còn vào tương tác của chúng (độ thưa thớt động ngữ cảnh). Hình 1(b) cho thấy rằng với thông tin động thuần túy, dự đoán độ thưa thớt không chính xác. Chỉ với các embedding token có thông tin ngữ cảnh đầy đủ, chúng ta mới có thể dự đoán độ thưa thớt chính xác. Một phát hiện khác là độ thưa thớt động ngữ cảnh cho mỗi lớp có thể được dự đoán dựa trên "độ tương tự" giữa các tham số lớp (đầu/MLP) và đầu ra từ lớp trước, mang theo hỗn hợp ngữ cảnh tức thời của các embedding token.

Hiệu quả: Bởi vì tại thời điểm suy luận, các tham số mô hình là tĩnh, được truyền cảm hứng từ văn học tìm kiếm láng giềng gần nhất (NNS) cổ điển và các ứng dụng của nó trong học sâu hiệu quả, có thể công thức hóa việc dự đoán dựa trên độ tương tự ở trên như một bài toán NNS (Indyk & Motwani, 1998b; Zhang et al., 2018; Chen et al., 2020a). Tuy nhiên, như đã đề cập, chi phí có thể khó vượt qua vì chúng ta cần thực hiện dự đoán tức thời trước mỗi lớp. May mắn thay, chúng tôi khai thác một hiện tượng của LLM trong đó các embedding token thay đổi chậm qua các lớp do kết nối dư (nổi tiếng trong thị giác máy tính (He et al., 2016)). Vì các đầu vào của một vài lớp liên tiếp rất giống nhau, chúng ta có thể thiết kế một bộ dự đoán lookahead không đồng bộ (Hình 2).

Dựa trên những phát hiện của chúng tôi, chúng tôi trình bày một hệ thống, DEJAVU, khai thác độ thưa thớt ngữ cảnh và hiện thực hóa các LLM hiệu quả cho các ứng dụng nhạy cảm với độ trễ.

• Trong Phần 4.1 và Phần 4.2, chúng tôi trình bày một thuật toán dựa trên học tập chi phí thấp để dự đoán độ thưa thớt tức thời. Cho đầu vào tới một lớp cụ thể, nó dự đoán một tập con liên quan của attention (đầu) hoặc tham số MLP trong lớp tiếp theo và chỉ tải chúng cho tính toán.

• Trong Phần 4.3, chúng tôi đề xuất một bộ dự đoán không đồng bộ (tương tự như bộ dự đoán nhánh cổ điển (Smith, 1998)) để tránh chi phí tuần tự. Một đảm bảo lý thuyết biện minh rằng thiết kế xuyên lớp đủ cho dự đoán độ thưa thớt chính xác.

Sau khi tích hợp triển khai nhận biết phần cứng của phép nhân ma trận thưa thớt (Phần 4.4), DEJAVU (viết chủ yếu bằng Python) có thể giảm độ trễ của các LLM mã nguồn mở như OPT-175B hơn 2× từ đầu đến cuối mà không làm suy giảm chất lượng so với thư viện FasterTransformer tiên tiến nhất từ Nvidia (viết hoàn toàn bằng C++/CUDA), và hơn 2× so với việc triển khai Hugging Face được sử dụng rộng rãi ở kích thước batch nhỏ. Hơn nữa, chúng tôi chỉ ra một số ablation về các thành phần khác nhau của DEJAVU và tính tương thích của nó với các kỹ thuật lượng tử hóa.

2 Công trình liên quan và Công thức hóa vấn đề

Trước tiên chúng tôi thảo luận ngắn gọn về văn học phong phú về suy luận hiệu quả. Sau đó, chúng tôi giới thiệu sự phân tích độ trễ trong thiết lập của chúng tôi. Cuối cùng, chúng tôi cung cấp một công thức hóa vấn đề chính thức.

2.1 Lượng tử hóa, Cắt tỉa, Chưng cất cho Suy luận

Các nới lỏng khác nhau đã được nghiên cứu trong nhiều thập kỷ cho suy luận mô hình trong học máy. Có ba kỹ thuật chính: lượng tử hóa (Han et al., 2015; Jacob et al., 2018; Nagel et al., 2019; Zhao et al., 2019), cắt tỉa hoặc độ thưa thớt (Molchanov et al., 2016; Liu et al., 2018; Hoefler et al., 2021), và chưng cất (Hinton et al., 2015; Tang et al., 2019; Touvron et al., 2021). Chúng là những lĩnh vực trực giao và thường xuất sắc trong các thiết lập khác nhau. Gần đây, có nghiên cứu tích cực cố gắng áp dụng một hoặc kết hợp các kỹ thuật như vậy trong suy luận LLM (Yao et al., 2022; Park et al., 2022; Dettmers et al., 2022; Frantar et al., 2022; Frantar & Alistarh, 2023; Bansal et al., 2022; Xiao et al., 2022). Thảo luận thêm được trình bày trong Phụ lục A.

2.2 Phân tích độ trễ suy luận LLM

Quy trình sinh của LLM bao gồm hai giai đoạn: (i) giai đoạn prompt lấy một chuỗi đầu vào để tạo ra các khóa và giá trị (KV cache) cho mỗi khối transformer của LLM, tương tự như việc chuyển tiếp của huấn luyện LLM; và (ii) giai đoạn sinh token sử dụng và cập nhật KV cache để sinh token từng bước, trong đó việc sinh token hiện tại phụ thuộc vào các token được sinh trước đó.

Bài báo này nghiên cứu thiết lập trong đó giai đoạn sinh token dễ dàng chiếm ưu thế trong thời gian suy luận từ đầu đến cuối. Như được hiển thị trong Bảng 1, việc sinh một chuỗi có độ dài 128 mất thời gian lâu hơn nhiều so với việc xử lý một chuỗi có độ dài 128 như prompt do độ trễ I/O của việc tải tham số mô hình.

Ngoài ra, Bảng 2 cho thấy attention và MLP đều là nút cổ chai trong LLM, ví dụ, trong các mô hình 175B, việc tải tham số MLP chiếm khoảng 2/3 tổng I/O và các đầu attention chiếm 1/3 còn lại. Hơn nữa, trong chế độ song song tensor, có hai giao tiếp giữa các GPU, một sau khối attention, và một khác sau khối MLP.

Như được hiển thị trong Bảng 3, giao tiếp giữa các GPU chiếm khoảng 15% độ trễ sinh token. Bài báo này tập trung vào việc làm cho attention và MLP hiệu quả hơn. Chi phí giao tiếp ngụ ý rằng giới hạn trên của việc tăng tốc như vậy là khoảng 6× khi bỏ qua tất cả các khối transformer.

[Bảng 1: Phân tích lý thuyết cho prompting so với sinh token (song song mô hình tensor trên 8 GPU A100-80G)]

[Bảng 2: Phân tích lý thuyết cho khối Attention so với khối MLP trong một lớp transformer khi sinh một token (song song mô hình tensor trên 8 GPU A100-80G)]

[Bảng 3: Phân tích độ trễ sinh 1 token dưới thiết lập kích thước batch 1 và độ dài prompt 128 trên 8 A100-80GB]

2.3 Công thức hóa vấn đề

Mục tiêu là giảm độ trễ sinh của LLM bằng cách khai thác độ thưa thớt ngữ cảnh. Trong phần sau, chúng tôi định nghĩa chính thức các khối attention và MLP được thưa thớt hóa.

MLP được thưa thớt hóa: Có hai lớp tuyến tính trong một khối MLP, W1,W2∈Rd×4d. Ký hiệu y∈R1×d là đầu vào tới khối MLP trong bước sinh hiện tại. Gọi mỗi cột (trọng số của neuron thứ i) của các lớp tuyến tính là W1i,W2i∈Rd×1. Với độ thưa thớt ngữ cảnh, chỉ một tập nhỏ của chúng được yêu cầu cho tính toán. Gọi SM⊆[4d] ký hiệu tập hợp các neuron như vậy cho đầu vào y. Tính toán MLP được thưa thớt hóa là
MLPSM(y)=σ(yW1SM)(W2SM)⊤, (1)
trong đó σ là hàm kích hoạt, ví dụ, ReLU, GeLU. Lưu ý rằng vì tính toán trong tuyến tính đầu tiên dẫn đến các kích hoạt thưa thớt, lớp tuyến tính thứ hai cũng được thưa thớt hóa.

Attention được thưa thớt hóa: Gọi X∈Rn×d ký hiệu các embedding của tất cả token (ví dụ, prompt và các token được sinh trước đó). Gọi y∈R1×d là đầu vào tới Multi-Head-Attention (MHA) trong bước sinh hiện tại. Giả sử có h đầu. Đối với mỗi i∈[h], chúng ta sử dụng WKi,WQi,WVi∈Rd×dh để ký hiệu các phép chiếu khóa, truy vấn, giá trị cho đầu thứ i, và WOi∈Rdh×d cho các phép chiếu đầu ra. Với độ thưa thớt ngữ cảnh, chúng ta ký hiệu SA là một tập nhỏ các đầu attention dẫn đến đầu ra gần giống như attention đầy đủ cho đầu vào y. Theo hệ thống ký hiệu trong (Alman & Song, 2023), tính toán MHA được thưa thớt hóa có thể được viết chính thức như
MHASA(y)=∑i∈SAHi(y)|{z}1×dhWOi|{z}dh×d,
trong đó Hi(y):Rd→Rdh và Di(y)∈R có thể được viết như
Hi(y):=Di(y)−1exp(yWQi(WKi)⊤X⊤)XWVi,(2)
Di(y):=exp(yWQi(WKi)⊤X⊤)1n.

Đối với cả MLP và Attention, cho một ngân sách tính toán, mục tiêu là tìm SM và SA để giảm thiểu lỗi giữa xấp xỉ thưa thớt và tính toán đầy đủ.

--- TRANG 2 ---

[Hình 1: (1) LLM có tới 85% độ thưa thớt ngữ cảnh cho một đầu vào cụ thể. (2) Độ thưa thớt ngữ cảnh có sự đánh đổi hiệu quả-độ chính xác tốt hơn nhiều (tới 7×) so với độ thưa thớt không ngữ cảnh hoặc độ thưa thớt tĩnh.]

3 Các LLM được huấn luyện trước là thưa thớt ngữ cảnh

Trong phần này, chúng tôi trình bày một số quan sát chính và hiểu biết lý thuyết về độ thưa thớt trong LLM, dựa trên đó thiết kế DEJAVU được xây dựng. Trước tiên chúng tôi kiểm tra giả thuyết độ thưa thớt ngữ cảnh và xác minh rằng độ thưa thớt ngữ cảnh tồn tại trong các LLM được huấn luyện trước trong Phần 3.1. Sau đó, chúng tôi xây dựng hiểu biết về lý do tại sao độ thưa thớt ngữ cảnh xảy ra tự nhiên ngay cả khi LLM được huấn luyện một cách dày đặc trong Phần 3.2. Cuối cùng, chúng tôi trình bày một quan sát về kết nối dư và giải thích mối quan hệ của chúng với độ thưa thớt ngữ cảnh một cách phân tích trong Phần 3.3.

3.1 Giả thuyết độ thưa thớt ngữ cảnh

Được truyền cảm hứng từ văn học cắt tỉa trước đây (Molchanov et al., 2016), chúng tôi tìm thấy một phương pháp đơn giản một cách đáng ngạc nhiên là đủ để nghiên cứu và xác minh giả thuyết của chúng tôi. Trong phần này, chúng tôi mô tả quy trình kiểm tra, chi tiết quan sát, và hiểu biết từ nghiên cứu này.

Xác minh: Kiểm tra của chúng tôi được thực hiện trên các mô hình OPT-175B, 66B, và 30B và các bộ dữ liệu downstream khác nhau như OpenBookQA (Mihaylov et al., 2018) và Wiki-Text (Merity et al., 2016). Chúng tôi tìm độ thưa thớt ngữ cảnh cho mỗi ví dụ đầu vào với hai lần chuyển tiếp của mô hình. Trong lần chuyển tiếp đầu tiên, chúng tôi ghi lại một tập con tham số, cụ thể là các đầu attention và neuron MLP nào tạo ra norm đầu ra lớn cho đầu vào. Trong lần chuyển tiếp thứ hai, mỗi ví dụ đầu vào chỉ sử dụng tập con tham số được ghi lại cho tính toán. Đáng ngạc nhiên, hai lần chuyển tiếp này dẫn đến dự đoán hoặc hiệu suất tương tự trên tất cả các tác vụ học trong ngữ cảnh và mô hình hóa ngôn ngữ.

Quan sát: Hình 3 cho thấy trung bình, chúng ta có thể áp đặt tới 80% độ thưa thớt trên các đầu attention và 95% độ thưa thớt trên các neuron MLP. Như đã đề cập trong Phần 2, mô hình OPT-175B có 2× tham số MLP so với các khối attention. Do đó tổng độ thưa thớt ở đây là khoảng 85%. Vì tất cả đều là độ thưa thớt có cấu trúc (đầu và neuron), việc dự đoán chúng chính xác có thể dẫn đến tăng tốc 7×.

Hiểu biết: Việc chúng ta có thể tìm thấy độ thưa thớt ngữ cảnh trong các khối MLP tại thời điểm suy luận là trực quan vì các hàm kích hoạt của chúng, ví dụ, ReLU hoặc GeLU (Kurtz et al., 2020). Các quan sát tương tự đã được (Li et al., 2022) đưa ra. Tuy nhiên, thật ngạc nhiên khi chúng ta có thể tìm thấy độ thưa thớt ngữ cảnh trong các lớp attention. Lưu ý rằng, việc tìm độ thưa thớt ngữ cảnh trong attention không giống với cắt tỉa đầu. Chúng tôi kiểm tra chéo rằng các ví dụ khác nhau có độ thưa thớt ngữ cảnh khác nhau. Mặc dù 80% tham số không được bao gồm trong các đường dẫn cho một ví dụ cụ thể, chúng có thể được sử dụng bởi các ví dụ khác. Tiếp theo, chúng tôi sẽ cố gắng hiểu lý do cho những hiện tượng như vậy, đặc biệt trong các khối attention.

3.2 Phân cụm token trong các lớp Attention

Trong phần trước, chúng tôi đã xác minh rằng tồn tại độ thưa thớt ngữ cảnh cho một đầu vào cụ thể trong LLM. Trong phần này, chúng tôi cố gắng hiểu lý do cho những hiện tượng như vậy, đặc biệt trong các lớp attention. Trước tiên chúng tôi chỉ ra một quan sát sâu sắc về attention. Sau đó chúng tôi trình bày một giả thuyết rằng self-attention về khái niệm là các thuật toán phân cụm. Cuối cùng chúng tôi chỉ ra bằng chứng phân tích để hỗ trợ giả thuyết này.

Quan sát: Hình 4 cho thấy bản đồ attention của ba đầu khác nhau từ cùng một lớp cho một ví dụ đầu vào. Token tiếp theo nó nên dự đoán là "Truck". Màu tối hơn đại diện cho điểm attention cao hơn. Chúng tôi quan sát thấy rằng đầu giữa là một đầu token-mixing tương đối đồng đều trong khi đầu trên và dưới là các đầu attention "heavy hitter" (với attention cao đối với "like" và "shipping").

Không có gì ngạc nhiên, việc chỉ chọn các đầu heavy hitter mà không chọn các đầu đồng đều không ảnh hưởng đến dự đoán, vì các đầu đồng đều không mô hình hóa hoặc mã hóa các tương tác token quan trọng. Trong phần tiếp theo, chúng tôi cũng sẽ giải thích chi tiết cách các tiêu chí để chọn các đầu attention đồng đều và các đầu có norm đầu ra nhỏ có tương quan cao.

Giả thuyết: Chúng tôi giả thuyết rằng đầu attention đang thực hiện phân cụm mean-shift (Derpanis, 2005).

Nhớ lại ký hiệu được định nghĩa trong Phần 2.3. Đối với đầu thứ i tại lớp hiện tại, X = [x1,...,xn]⊤∈Rn×d là các embedding token trong các bước thời gian trước đó. XWKi và XWVi là phép chiếu của embedding. Đối với một embedding đầu vào y, đầu ra ỹi=Hi(y), trong đó Hi(y) được định nghĩa trong Eq. 2.

Đối với mỗi i∈[h], nếu chúng ta gọi Ki(xj,y):=exp(yWQi(WKi)⊤xj) đo độ tương tự giữa xj và y, và định nghĩa mi(y):=∑jKi(xj,y)xj∑jKi(xj,y), thì chúng ta có ỹi=mi(y)WVi. Hơn nữa, nếu chúng ta đặt WVi=I và xem xét kết nối dư theo sau bởi layer norm, thì trong lớp tiếp theo, embedding ŷi của token hiện tại trở thành ŷi= Normalize(y + ỹi) = Normalize(y+mi(y)), có một điểm cố định y=γmi(y) cho bất kỳ vô hướng γ nào. Phép lặp này mang sự tương đồng với phân cụm mean-shift, đơn giản thực hiện phép lặp y←mi(y) cho đến khi hội tụ. Điều này có một điểm cố định rõ ràng y=mi(y).

Do đó, đầu self-attention có thể được coi là một bước mean-shift để đẩy các embedding đầu vào của các token khác nhau lại với nhau, nếu chúng đã là láng giềng trong không gian chiếu được chỉ định bởi WQi(WKi)⊤. Các đầu khác nhau học các không gian chiếu khác nhau để thực hiện phân cụm. Những động lực này giải thích lý do chính xác tại sao các embedding token có xu hướng phân cụm sau khi đi qua nhiều lớp hơn, dẫn đến điểm attention cao giữa các thành viên cụm, và điểm thấp cho những người không phải thành viên. Hơn nữa, các mẫu cụm khác nhau ở các đầu khác nhau (Chi tiết thêm trong Phụ lục K).

Phân tích trên không chỉ cung cấp hiểu biết về lý do tại sao độ thưa thớt ngữ cảnh tồn tại tự nhiên trong các LLM được huấn luyện trước, mà còn truyền cảm hứng cho thiết kế dự đoán độ thưa thớt dựa trên "độ tương tự" của chúng tôi cho DEJAVU trong Phần 4.

3.3 Embedding thay đổi chậm qua các lớp

Trước tiên chúng tôi trình bày quan sát của chúng tôi rằng các embedding thay đổi chậm qua các lớp liên tiếp. Sau đó chúng tôi cung cấp phân tích chi tiết về hiện tượng này. Cuối cùng, chúng tôi chỉ ra mối liên hệ chặt chẽ của nó với độ thưa thớt ngữ cảnh. Chi tiết trong Phần B.

Embedding tương tự cao trong các lớp liên tiếp: Trong Hình 5(a), chúng tôi chỉ ra rằng đối với cùng một đầu vào cụ thể, độ tương tự cosine giữa các embedding hoặc kích hoạt trong hai lớp liên tiếp là cực kỳ cao trên 7 kích thước khác nhau của các mô hình OPT. Cụ thể, chúng tôi thu thập các kích hoạt từ mỗi lớp trong khi thực hiện suy luận mô hình OPT trên tập validation C4 (Raffel et al., 2019). Lấy OPT-175B làm ví dụ, bắt đầu từ lớp thứ hai, độ tương tự giữa hai lớp liên tiếp bất kỳ là khoảng 0,99, cho thấy rằng khi một đầu vào được truyền qua mô hình, hướng của embedding của nó thay đổi chậm. Thú vị thay, sự thay đổi mạnh mẽ nhất xảy ra ở lớp đầu tiên. Hơn nữa, chúng tôi tăng khoảng cách và điều tra độ tương tự giữa embedding tại lớp l và tại lớp l+n được hiển thị trong Hình 5(b). Khi chúng ta tăng khoảng cách, độ tương tự giảm như mong đợi trong khi sự khác biệt trong độ tương tự cosine giữa các lựa chọn khác nhau của n nhỏ hơn ở lớp nông hơn. Chúng tôi vẽ độ tương tự trung bình, và độ lệch chuẩn được chỉ ra bởi bóng mờ.

Kết nối với dư: Chúng tôi xác minh rằng độ tương tự cao trong các embedding trong suy luận LLM là do kết nối dư. Trước tiên chúng tôi phân tích đồ thị tính toán bên trong mỗi lớp transformer để hiểu nguyên nhân đằng sau hiện tượng này. Có hai kết nối dư bên trong một lớp transformer, một xung quanh khối attention, và một khác xung quanh khối MLP. Kết nối dư có thể được viết như X+F(X), trong đó F là Multi-Head Attention hoặc hai lớp MLP. Trong Hình 5(c) và Hình 5(d), thực sự chúng ta có thể thấy rằng ∥X∥ lớn hơn đáng kể so với ∥F(X)∥, xác nhận rằng các embedding đang thay đổi chậm vì norm dư lớn.

Kết nối với độ thưa thớt ngữ cảnh: Chúng tôi đi sâu hơn một bước cố gắng hiểu lý do đằng sau norm dư lớn với mô hình hóa toán học. Chúng tôi khám phá ra rằng một lý do có thể cho ∥F(X)∥ nhỏ là do độ thưa thớt cao. Đối với khối MLP, độ thưa thớt cao có thể góp phần vào norm nhỏ của F(X) vì một phần lớn đầu ra có norm nhỏ. Lý luận tương tự áp dụng cho khối Attention, và do đó một số lượng lớn các đầu attention tạo ra đầu ra norm nhỏ.

Giới hạn hai phía dư: Bên cạnh lý luận thực nghiệm, chúng tôi định nghĩa chính thức việc tính toán của LLM một cách toán học. Dưới mô hình tính toán của chúng tôi, chúng ta có thể chỉ ra rằng một thuộc tính thu nhỏ được quan sát bởi các thí nghiệm thực tế của chúng tôi. Các chứng minh trong Phụ lục G, H, I.

Bổ đề 3.1 (Không chính thức). Gọi 0<ε1<ε2<1 là giới hạn dưới và trên của hệ số thu nhỏ. Gọi x là y là đầu ra. Chúng ta có kết nối dư y=x+F(x). Đối với khối MLP F(x), chúng ta có ε1≤∥y−x∥2≤ε2. Đối với khối attention F(x), chúng ta có ε1≤∥y−x∥2≤ε2.

4 DEJAVU

Trong phần này, chúng tôi trình bày framework của chúng tôi cho tìm kiếm độ thưa thớt ngữ cảnh thời gian suy luận cho LLM. Chúng tôi giới thiệu bộ dự đoán độ thưa thớt cho MLP trong Phần 4.1 và cho các đầu attention trong Phần 4.2. Quy trình làm việc của DEJAVU được hiển thị trong Hình 2. Phần 4.3 thảo luận về việc khai thác quan sát của chúng tôi trên LLM để tránh chi phí dự đoán thưa thớt với các đảm bảo lý thuyết. Trong Phần 4.4, chúng tôi trình bày triển khai tối ưu của chúng tôi cho phép giảm độ trễ từ đầu đến cuối. Chi tiết thêm được trình bày trong Phần D.

4.1 Dự đoán độ thưa thớt ngữ cảnh trong các khối MLP

Như được giải thích trong Phần 2, các khối MLP là một trong những nút cổ chai chính cho việc sinh LLM (2/3 FLOP và IO). Trong phần này, chúng tôi thảo luận cách chúng tôi đạt được tăng tốc thời gian thực tế với độ thưa thớt ngữ cảnh trong các khối MLP.

Thách thức: Hình 3(b) cho thấy rằng đối với một token cụ thể, độ thưa thớt ngữ cảnh 95% là có thể. Độ thưa thớt ngữ cảnh trong khối MLP có thể được xác định sau khi tính toán kích hoạt. Tuy nhiên, điều này chỉ chứng minh sự tồn tại của độ thưa thớt ngữ cảnh nhưng không mang lại lợi ích gì về mặt hiệu quả. Cần có dự đoán nhanh và chính xác để khai thác độ thưa thớt ngữ cảnh cho hiệu quả từ đầu đến cuối. Cách ngây thơ là chọn một tập con neuron ngẫu nhiên. Không có gì ngạc nhiên, việc chọn ngẫu nhiên không thể xác định độ thưa thớt ngữ cảnh chính xác, dẫn đến suy giảm mô hình mạnh mẽ.

Một bài toán tìm kiếm láng giềng gần: Nhớ lại rằng chúng tôi xác minh sự tồn tại của độ thưa thớt ngữ cảnh bằng cách ghi lại neuron nào tạo ra norm đáng kể. Về cơ bản, cho đầu vào, mục tiêu là tìm kiếm các neuron có tích vô hướng cao với đầu vào, vì hàm kích hoạt "lọc" kích hoạt thấp. Do đó, chúng tôi công thức hóa dự đoán độ thưa thớt ngữ cảnh của một lớp MLP như bài toán tìm kiếm láng giềng gần cổ điển dưới metric tích vô hướng.

Định nghĩa 4.1 (MaxIP xấp xỉ trong MLP). Gọi c∈(0,1) và τ∈(0,1) ký hiệu hai tham số. Cho một bộ dữ liệu n-vector W1⊂Sd−1 trên một cầu đơn vị, mục tiêu của (c,τ)-MaxIP là xây dựng một cấu trúc dữ liệu mà, cho một truy vấn y∈Sd−1 sao cho maxw∈W1⟨y,w⟩≥τ, nó truy xuất một vector z từ W1 thỏa mãn ⟨y,z⟩≥c·maxw∈W1⟨y,w⟩.

Nhận xét 4.2. W1 (lớp tuyến tính đầu tiên) và y (embedding đầu vào) của chúng tôi trong các khối MLP có thể được xem như bộ dữ liệu và truy vấn trong Định nghĩa 4.1 tương ứng.

Thiết kế: Các phương pháp và triển khai tìm kiếm láng giềng gần tiêu chuẩn tiên tiến làm chậm tính toán. Lấy OPT-175B với d là 12288 làm ví dụ. HNSW (Malkov & Yashunin, 2018) yêu cầu hơn 10ms, và FAISS (Johnson et al., 2019) yêu cầu hơn 4ms, trong khi tính toán MLP chỉ là 0,2ms. Chiều cao và các phức tạp của triển khai cấu trúc dữ liệu trên GPU làm cho thời gian tìm kiếm dài hơn tính toán MLP. Do đó, chúng tôi chọn một bộ phân loại mạng neural như phương pháp tìm kiếm láng giềng gần để khai thác phép nhân ma trận nhanh trên GPU. Đối với mỗi khối MLP, chúng tôi huấn luyện một mạng kết nối đầy đủ hai lớp nhỏ để dự đoán độ thưa thớt ngữ cảnh. Việc thu thập dữ liệu huấn luyện rất đơn giản vì chúng ta biết độ thưa thớt ngữ cảnh bằng tính toán dày đặc. Thuật toán huấn luyện được tóm tắt trong Thuật toán 1. Tính toán thưa thớt hóa trong W1 có hai bước: (1) Cho y, bộ dự đoán độ thưa thớt SPM dự đoán một tập SM các neuron quan trọng trong trọng số W1. (2) Tính toán MLP được thưa thớt hóa được định nghĩa trong Eq. 1. Lưu ý ở đây độ thưa thớt trong MLP có cấu trúc cao.

Thuật toán 1 Huấn luyện bộ dự đoán thưa thớt
Đầu vào: Một khối LLM được huấn luyện trước với tập tham số M, tập embedding token tại khối M={xi}i∈[N], ngưỡng t
Bộ dự đoán thưa thớt SP
P+←∅,P−←∅
for i=1→N do
P+←P+∪{(xi,mr)|mr∈M,mr(xi)≥t}
P−←P−∪{(xi,mr)|mr∈M,mr(xi)<t}
end for
SP ← TRAIN(P+,P−,L) ▷L là một hàm mất mát

4.2 Dự đoán độ thưa thớt ngữ cảnh trong các khối Attention

Các khối attention chiếm khoảng 30% IO trong việc sinh. Trong phần này, chúng tôi mô tả cách DEJAVU khai thác độ thưa thớt ngữ cảnh để tăng tốc các khối Attention.

Thách thức: Như đã thảo luận trong Phần 3.1, chỉ một vài đầu thực hiện tính toán quan trọng cho một token đầu vào cụ thể. Tương tự như các khối MLP, cần có việc chọn nhanh các đầu attention mà không có tính toán đầy đủ để giảm độ trễ từ đầu đến cuối. Hơn nữa, một thách thức đặc biệt của dự đoán thưa thớt trong các khối attention là sự phụ thuộc của attention vào các token trước đó. Một mặt, không rõ liệu cache khóa và giá trị của token quá khứ có cần thiết cho dự đoán thưa thớt hay không. Mặt khác, không rõ cách xử lý cache KV bị thiếu của các token quá khứ cho tính toán token hiện tại tại đầu được chọn.

Một bài toán tìm kiếm láng giềng gần: Dự đoán đầu cũng có thể được công thức hóa như một bài toán tìm kiếm láng giềng gần dựa trên hiểu biết của chúng tôi trong Phần 3.2. Vì mỗi đầu đang thực hiện phân cụm mean-shift, sau một vài lớp đầu tiên, embedding token hiện tại một mình là đủ cho dự đoán nhờ vào bản chất token-mixing của transformer. Do đó, dự đoán có thể dựa trên độ tương tự giữa y và tham số đầu.

Cách tiếp cận: Chúng tôi thiết kế bộ dự đoán attention thưa thớt có cùng kiến trúc với bộ dự đoán MLP thưa thớt. Mỗi đầu được coi như một lớp và một quy trình huấn luyện tương tự được sử dụng (Thuật toán 1). Sau đó, tương tự như cách thực hiện dự đoán MLP, bộ dự đoán độ thưa thớt attention SPA chọn một tập SA các đầu Hi (xem Eq. 2). Để giải quyết vấn đề cache KV bị thiếu cho token quá khứ, chúng tôi khai thác thực tế rằng độ trễ sinh bị giới hạn bởi I/O trong khi tính toán về cơ bản là "miễn phí". Cụ thể, đối với đầu attention được dự đoán của đầu vào y, chúng tôi tính toán các khóa và giá trị tương ứng và lưu trữ chúng trong cache KV. Nhưng chúng tôi cũng lưu một bản sao của y cho tất cả các đầu không được chọn khác. Sau đó trong quá trình sinh token tương lai, nếu có cache KV bị thiếu trong các đầu được chọn, chúng ta có thể tải các embedding token được lưu trữ và tính toán các khóa và giá trị cùng nhau. Điều này yêu cầu truy cập bộ nhớ bổ sung tối thiểu (chi phí chính là tải các ma trận trọng số).

4.3 Giảm chi phí với thực thi không đồng bộ

Chi phí dự đoán thưa thớt có thể dễ dàng tăng độ trễ từ đầu đến cuối thay vì giảm nó bất chấp việc giảm FLOP. Do đó, chúng tôi giới thiệu một phương pháp dự đoán thưa thớt look-ahead, được truyền cảm hứng từ các quan sát của chúng tôi trong Phần 3.3.

Thách thức: Ký hiệu yl∈Rd là đầu vào tới lớp transformer l. Chúng ta có thể viết tính toán tại lớp l như ẽyl←MHAl(yl), ḃyl←MLPl(ẽyl). Với các bộ dự đoán SPlA và SPlM, tính toán tại lớp transformer l có thể được viết lại như
SlA←SPlA(yl),ẽyl←MHAlSlA(yl),
SlM←SPlM(ẽyl),ḃyl←MLPlSlM(ẽyl)
trong đó tập SlA là độ thưa thớt ngữ cảnh cho khối Attention, và tập SlM là độ thưa thớt ngữ cảnh cho khối MLP tại lớp l. Lưu ý rằng tính toán tại các khối Attention và MLP phải chờ quyết định bộ dự đoán thưa thớt. Chi phí này có thể lấn át việc tiết kiệm từ các khối Attention và MLP về mặt độ trễ.

Cách tiếp cận: Trong Phần 3.3, chúng tôi trình bày hiện tượng embedding phát triển chậm, cung cấp cơ hội để nới lỏng tính toán tuần tự thành tính toán song song. Cùng với quan sát về cường độ tính toán thấp trong quá trình sinh, chúng tôi song song hóa dự đoán thưa thớt với tính toán của mỗi khối (Xem Hình 2). Tính toán có thể được viết như sau:
ẽyl←MHAlSlA(yl),ḃyl←MLPlSlM(ẽyl),
Sl+1A←SPlA(yl), Sl+1M←SPlM(yl),

Chúng tôi nhận xét Sl+1A và Sl+1M có thể được tính toán song song với ẽyl hoặc ḃyl, trong khi 4 bước trước đó là tuần tự.

Đảm bảo lý thuyết: Bộ dự đoán thưa thớt có thể đưa ra quyết định xuyên lớp hơn nữa vì kết nối dư. Chúng tôi trình bày một phát biểu bổ đề không chính thức về dự đoán xuyên lớp. Việc MaxIP tương đương với tìm kiếm láng giềng gần nhất ℓ2 là nổi tiếng. Để thuận tiện, chúng tôi sử dụng MaxIP ở đây. Chúng tôi bao gồm thảo luận thêm và chứng minh trong Phần J.

Bổ đề 4.3 (Không chính thức). Gọi ε∈(0,1). Gọi yl là đầu vào tại lớp l. Gọi yl−1 là đầu vào tại lớp (l−1). Giả sử rằng ∥yl−yl−1∥2≤ε. Đối với bất kỳ tham số c,τ sao cho ε < O(cτ). Khi đó chúng ta có thể chỉ ra rằng, việc giải MaxIP (c,τ) đủ để giải MaxIP (0.99c,τ).

4.4 Triển khai hiệu quả phần cứng

Chúng tôi mô tả cách DEJAVU được triển khai theo cách hiệu quả phần cứng để hiện thực hóa tăng tốc lý thuyết của độ thưa thớt ngữ cảnh. Việc tính đến các đặc tính phần cứng dẫn đến tăng tốc hơn 2× so với mô hình dày đặc được tối ưu, và nhanh hơn 4× so với triển khai thưa thớt tiêu chuẩn.

Chúng tôi nêu bật một số đặc tính phần cứng của GPU:
• Việc sinh batch nhỏ bị giới hạn bởi I/O bộ nhớ GPU (NVIDIA, 2022; Ivanov et al., 2021; Dao et al., 2022). Điều này là do cường độ số học thấp. Đối với mỗi phần tử được tải từ bộ nhớ GPU, chỉ một số lượng nhỏ các phép toán dấu phẩy động được thực hiện.
• GPU là các thiết bị định hướng khối: việc tải một byte bộ nhớ đơn lẻ mất cùng thời gian như việc tải một khối bộ nhớ xung quanh cùng địa chỉ đó (Harris, 2013). Kích thước khối thường là 128 byte cho GPU NVIDIA (Cook, 2012).

Những đặc tính này đặt ra một số thách thức trong việc triển khai độ thưa thớt ngữ cảnh. Tuy nhiên, chúng có thể được giải quyết bằng các kỹ thuật cổ điển trong lập trình GPU.

Kernel fusion: Một triển khai tiêu chuẩn của phép nhân ma trận-vector thưa thớt (ví dụ, trong PyTorch) riêng biệt lập chỉ mục một tập con của ma trận W1SM trước khi nhân với đầu vào y sẽ phát sinh 3× lượng I/O bộ nhớ. Do đó, để tránh chi phí như vậy, chúng tôi hợp nhất bước lập chỉ mục và nhân. Cụ thể, chúng tôi tải một tập con của W1SM vào bộ nhớ, cùng với y, thực hiện phép nhân, sau đó ghi xuống kết quả. Triển khai hợp nhất này (trong Triton (Tillet et al., 2019)) mang lại tăng tốc tới 4× so với triển khai PyTorch tiêu chuẩn (Phụ lục E).

Memory coalescing: Trong triển khai dày đặc, các ma trận trọng số của hai lớp tuyến tính trong MLP được lưu trữ như (W1)⊤ và W2 để không cần phép toán chuyển vị bổ sung. Chúng thường được lưu trữ ở định dạng row-major. Trong triển khai thưa thớt, nó cho phép chúng ta tải (W1SM)⊤ tối ưu (chiều thứ hai liên tục trong bộ nhớ). Tuy nhiên, đối với các trường hợp cần tải (W2SM), định dạng này làm chậm đáng kể việc tải bộ nhớ, vì các chỉ mục trong SM trỏ đến bộ nhớ không liên tục. Chúng tôi đơn giản lưu trữ các ma trận này ở định dạng column-major (tức là, lưu trữ (W2)⊤ ở định dạng row-major), sau đó sử dụng cùng kernel hợp nhất ở trên. Tương tự, trong các khối attention, chúng tôi lưu trữ phép chiếu đầu ra attention WO ở định dạng column-major.

Hai kỹ thuật này (kernel fusion và memory-coalescing) làm cho DEJAVU hiệu quả phần cứng, mang lại tăng tốc tới 2× từ đầu đến cuối so với FasterTransformer tiên tiến nhất (Phần 5.1).

--- TRANG 3 ---

[Hình 3: Trong Hình (a), chúng tôi vẽ phần trăm các đầu attention không được kích hoạt. Bằng cách chỉ giữ các đầu tạo ra norm đầu ra lớn, chúng ta có thể im lặng hơn 80% đầu attention cho một token cụ thể. Trong Hình (b), chúng tôi vẽ độ thưa thớt trung bình mà chúng tôi áp đặt lên các lớp MLP. Chúng ta có thể zeroing hơn 95% tham số MLP cho một token cụ thể.]

5 Đánh giá thực nghiệm

Trong Phần 5.1, chúng tôi trình bày kết quả từ đầu đến cuối cho thấy DEJAVU đạt được giảm hơn 2× trong độ trễ sinh token so với FasterTransformer tiên tiến nhất và hơn 6× so với Hugging Face mà không mất độ chính xác. Trong Phần 5.2, chúng tôi thực hiện một danh sách các nghiên cứu ablation như đánh giá độc lập về độ thưa thớt ngữ cảnh thời gian suy luận của khối MLP và khối Attention (Chi tiết được trình bày trong Phần C). Cuối cùng, chúng tôi trình bày các kết quả bổ sung để chứng minh khả năng tương lai của việc thưa thớt hóa toàn bộ LLM thông qua bỏ qua lớp trong Phần C.3.

5.1 Kết quả từ đầu đến cuối

Thiết lập thí nghiệm: Chúng tôi so sánh độ chính xác của DEJAVU-OPT với mô hình OPT gốc trên hai bộ dữ liệu mô hình hóa ngôn ngữ Wiki-Text (Merity et al., 2016) và C4 (Raffel et al., 2019) và bảy tác vụ downstream few-shot: CB (de Marneffe et al., 2019), COPA (Gordon et al., 2012), Lambada (Radford et al., 2019), OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), RTE (Giampiccolo et al., 2007), Winogrande (ai2, 2019). Chúng tôi sử dụng lm-eval-harness (Gao et al., 2021) cho các tác vụ zero-shot và five-shot. Chúng tôi thu thập dữ liệu huấn luyện cho bộ dự đoán độ thưa thớt bằng 500 điểm dữ liệu ngẫu nhiên từ bộ dữ liệu huấn luyện C4. Các thí nghiệm của chúng tôi được tiến hành trên các máy chủ GPU NVIDIA A100 80GB.

Không giảm độ chính xác cho đến 75% độ thưa thớt: Trong Hình 6, chúng tôi trình bày xu hướng độ chính xác của DEJAVU-OPT-175B. Trong thiết lập zero-shot, độ chính xác trung bình qua các tác vụ không giảm cho đến 75% độ thưa thớt. Xu hướng tương tự có thể được quan sát cho thiết lập five-shot, xác minh khả năng của mô hình cho học trong ngữ cảnh. Kết quả này đặc biệt khuyến khích cho quan sát của chúng tôi trong Hình 1(a), nơi chúng ta có thể áp đặt 85% độ thưa thớt khi cho phép tính toán đầy đủ.

Giảm độ trễ hơn 2×: Hình 7 trình bày tăng tốc độ trễ cho việc sinh token với OPT-175B ở kích thước batch 1, nơi DEJAVU đạt hiệu suất tốt nhất. Ở khoảng 75% độ thưa thớt, DEJAVU tăng tốc sinh 1,8-2× so với FasterTransformers (FT) tiên tiến nhất và 4,8-6× so với triển khai Hugging Face (HF).

5.2 Kết quả Ablation

Độ thưa thớt ngữ cảnh cho batch lớn hơn: Mặc dù bài báo này tập trung vào các thiết lập nhạy cảm với độ trễ, chúng tôi chứng minh rằng DEJAVU tổng quát hóa cho các batch lớn hơn. Chúng tôi trình bày độ thưa thớt ngữ cảnh Union (phần của neuron/đầu không được sử dụng bởi bất kỳ đầu vào nào trong batch) của các kích thước batch khác nhau cho các khối MLP và Attention, tương ứng, trong Hình 8 và 11. Phép toán union là cần thiết để hiện thực hóa GEMM thưa thớt nhanh. Đáng ngạc nhiên, số lượng neuron MLP và đầu Attention mà DEJAVU kích hoạt không tăng tuyến tính với kích thước batch. Điều này gợi ý một phân phối luật lũy thừa thay vì phân phối đồng đều của truy cập tham số từ tất cả các ví dụ đầu vào. Điều này cung cấp cơ hội có thể mở rộng Dejavu cho thiết lập thông lượng cao. Ví dụ, chúng ta có thể tiền xử lý các đầu vào và nhóm các đầu vào tương tự để tận hưởng mức độ thưa thớt ngữ cảnh union cao hơn.

Độ thưa thớt ngữ cảnh trên các khối MLP: Chúng tôi nghiên cứu việc thưa thớt hóa ngữ cảnh của khối MLP trong OPT-175B. Chúng tôi để khối Attention như tính toán dày đặc. Bảng 4 cho thấy hiệu suất mô hình ở 85% độ thưa thớt. Bộ dự đoán MLP thưa thớt không gây mất độ chính xác trên cả các tác vụ zero-shot và mô hình hóa ngôn ngữ. Trong việc huấn luyện bộ dự đoán MLP thưa thớt, chúng tôi quan sát thấy rằng bộ dự đoán thưa thớt đạt độ chính xác validation cao. Lớp nông dường như dễ mô hình hóa hơn vì bộ dự đoán có độ chính xác validation trên 99% trong các lớp nông và giảm xuống khoảng 93% trong các lớp cuối.

Độ thưa thớt ngữ cảnh trên các khối attention: Trong phần này, chúng tôi nghiên cứu bộ dự đoán thưa thớt cho khối Attention trên OPT-175B và để khối MLP như tính toán dày đặc. Bảng 4 hiển thị độ chính xác thử nghiệm trên các tác vụ zero-shot và perplexity trên các bộ dữ liệu mô hình hóa ngôn ngữ. Tóm lại, bộ dự đoán Attention thưa thớt không gây mất độ chính xác ở khoảng 50% độ thưa thớt. Trong quá trình huấn luyện bộ dự đoán Attention thưa thớt, chúng tôi quan sát các xu hướng khác nhau so với bộ dự đoán MLP thưa thớt. Độ chính xác validation khoảng 93% trong các lớp giữa và gần 99% trong các lớp nông và sâu.

Độ thưa thớt ngữ cảnh trên các mô hình nhỏ hơn: Các thí nghiệm chính của chúng tôi tập trung vào OPT-175B. Ở đây, chúng tôi xác minh hiệu quả của DEJAVU trên một mô hình nhỏ hơn, cụ thể là OPT-66B. Trong Bảng 5, chúng tôi tóm tắt độ chính xác trên tác vụ zero-shot ở 50% độ thưa thớt. Tương tự như DEJAVU-OPT-175B, chúng tôi nhận thấy không mất độ chính xác.

Độ thưa thớt ngữ cảnh trên các mô hình khác: Chúng tôi mở rộng đánh giá cho một family mô hình khác. Trong Bảng 6, chúng tôi tóm tắt độ chính xác ở độ thưa thớt attention 50% và độ thưa thớt MLP 30%. Tương tự như family OPT, chúng tôi nhận thấy không mất độ chính xác. Mức độ thưa thớt thấp hơn trong MLP là do sự khác biệt trong hàm kích hoạt.

Độ thưa thớt không ngữ cảnh: Như chúng tôi đã đề cập trong Phần 1, người ta có thể dự đoán độ thưa thớt mà không có thông tin ngữ cảnh. Đối với độ thưa thớt không ngữ cảnh, chúng tôi dựa vào embedding gốc tại lớp đầu vào. Tại mỗi khối, trước tiên chúng tôi chuyển embedding gốc để ghi lại một tập con tham số tạo ra norm lớn. Trong lần chuyển thứ hai, embedding tại mỗi lớp chỉ sử dụng tập con được ghi lại. Như được hiển thị trong Hình 1, dự đoán không ngữ cảnh không đủ và dẫn đến mất độ chính xác ngay cả ở 50% độ thưa thớt. Kết quả này xác minh các lựa chọn thiết kế của chúng tôi dựa vào kích hoạt tại mỗi lớp như đầu vào để đưa ra dự đoán độ thưa thớt ngữ cảnh.

Tương thích với lượng tử hóa: Lượng tử hóa là một hướng đầy hứa hẹn khác cho các mô hình ngôn ngữ hiệu quả. Chúng tôi điều tra khả năng kết hợp độ thưa thớt ngữ cảnh với các kỹ thuật lượng tử hóa. Đối với DEJAVU-OPT-175B, chúng tôi đặt độ thưa thớt mô hình toàn bộ ở 75%. Đối với lượng tử hóa, chúng tôi áp dụng lượng tử hóa 4-bit trên trọng số mô hình (W4A16). Như được hiển thị trong Bảng 7, sự kết hợp của lượng tử hóa và DEJAVU hầu như luôn đạt độ chính xác tốt hơn so với DEJAVU hoặc lượng tử hóa riêng lẻ. Điều này gợi ý rằng các lỗi xấp xỉ từ hai hướng này không bị ghép lại.

[Bảng 4, 5, 6, 7 với dữ liệu hiệu suất như đã mô tả]

6 Kết luận

Mục tiêu chính của chúng tôi là làm cho suy luận LLM hiệu quả để khả năng học trong ngữ cảnh mạnh mẽ của chúng có thể được sử dụng trong nhiều lĩnh vực ứng dụng hơn. Chúng tôi quan sát thấy rằng độ thưa thớt ngữ cảnh có thể được dự đoán chính xác với các thuật toán dựa trên học tập nhẹ. Điều này thúc đẩy chúng tôi thiết kế DEJAVU sử dụng các bộ dự đoán lookahead không đồng bộ và độ thưa thớt hiệu quả phần cứng để tăng tốc suy luận LLM trong thời gian thực tế. Kết quả thực nghiệm khuyến khích của chúng tôi xác nhận rằng độ thưa thớt ngữ cảnh có thể giảm độ trễ suy luận hơn 2× so với FasterTransformer tiên tiến nhất mà không làm giảm chất lượng mô hình. Phương pháp của chúng tôi là một bước hướng tới việc làm cho LLM dễ tiếp cận hơn với cộng đồng chung, có thể mở khóa các ứng dụng AI mới thú vị.

Lời cảm ơn

Chúng tôi muốn cảm ơn Ryan Spring, Laurel Orr, Guangxuan Xiao, Eric Han, Xun Huang, Daniel Y. Fu, Benjamin Spector, Ruan Silva, Diana Liskovich, và các reviewer ẩn danh cho các thảo luận và phản hồi hữu ích. Chúng tôi ghi nhận sự hỗ trợ hào phóng của Together Computer, cho phép các tính toán một phần cần thiết trong công trình này.

[Phần tài liệu tham khảo và phụ lục tiếp theo với nội dung tương tự được dịch...]
