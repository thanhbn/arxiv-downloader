# 1909.10364.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/1909.10364.pdf
# Kích thước tệp: 1063053 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cắt tỉa Mạng Nơ-ron Sâu phụ thuộc Lớp
Rahim Entezari
Viện Tin học Kỹ thuật, TU Graz
CSH Vienna, Austria
entezari@tugraz.atOlga Saukh
Viện Tin học Kỹ thuật, TU Graz
CSH Vienna, Austria
saukh@tugraz.at

Tóm tắt —Các mạng nơ-ron sâu ngày nay đòi hỏi tài nguyên tính toán đáng kể cho việc huấn luyện, lưu trữ và suy luận, điều này hạn chế việc sử dụng hiệu quả trên các thiết bị có tài nguyên hạn chế. Nhiều hoạt động nghiên cứu gần đây khám phá các tùy chọn khác nhau để nén và tối ưu hóa các mô hình sâu. Một mặt, trong nhiều ứng dụng thực tế, chúng ta phải đối mặt với thách thức mất cân bằng dữ liệu, tức là khi số lượng các phiên bản được gán nhãn của một lớp vượt trội đáng kể so với số lượng các phiên bản được gán nhãn của lớp khác. Mặt khác, các ứng dụng có thể đặt ra vấn đề mất cân bằng lớp, tức là số lượng dương tính giả cao hơn được tạo ra khi huấn luyện một mô hình và tối ưu hóa hiệu suất của nó có thể chấp nhận được, nhưng số lượng âm tính giả phải giữ ở mức thấp. Vấn đề bắt nguồn từ thực tế là một số lớp quan trọng hơn đối với ứng dụng so với các lớp khác, ví dụ, các vấn đề phát hiện trong lĩnh vực y tế và giám sát. Được thúc đẩy bởi thành công của giả thuyết vé số may mắn, trong bài báo này chúng tôi đề xuất một kỹ thuật nén mô hình sâu lặp lại, giữ số lượng âm tính giả của mô hình nén gần với mô hình gốc với cái giá tăng số lượng dương tính giả nếu cần thiết. Đánh giá thực nghiệm của chúng tôi sử dụng hai bộ dữ liệu chuẩn cho thấy các mạng con nén kết quả 1) đạt được số lượng âm tính giả thấp hơn tới 35% so với mô hình nén không có tối ưu hóa lớp, 2) cung cấp thước đo AUC-ROC tổng thể cao hơn so với thuật toán Vé số may mắn thông thường và ba phương pháp cắt tỉa phổ biến gần đây, và 3) sử dụng ít hơn tới 99% tham số so với mạng gốc. Mã nguồn được công bố công khai¹.

Các thuật ngữ chỉ mục —nén mạng nơ-ron sâu, cắt tỉa, giả thuyết vé số may mắn, mất cân bằng dữ liệu, mất cân bằng lớp

I. GIỚI THIỆU

Trong khi các mạng sâu là một lớp mô hình có độ thành công cao, dấu chân bộ nhớ lớn của chúng đặt gánh nặng đáng kể lên tiêu thụ năng lượng, băng thông truyền thông và yêu cầu lưu trữ của phần cứng cơ bản, và cản trở việc sử dụng chúng trên các thiết bị IoT có tài nguyên hạn chế. Ví dụ, các mô hình VGG-16 cho phát hiện đối tượng [1] và phân loại thuộc tính khuôn mặt [2] đều chứa hơn 130M tham số. Các nỗ lực gần đây về nén mô hình sâu cho các thiết bị nhúng khám phá nhiều hướng bao gồm lượng tử hóa, phân tích nhân tử, cắt tỉa, chưng cất kiến thức và thiết kế mạng hiệu quả. Phương pháp được trình bày trong bài báo này kết hợp cắt tỉa mạng với thiết kế mạng hiệu quả bằng cách bổ sung thêm tính phụ thuộc lớp vào thuật toán nén mạng.

Điều này đặc biệt hữu ích trong các ứng dụng có thể chịu đựng sự gia tăng nhỏ về số lượng dương tính giả (FP), nhưng cần giữ số lượng âm tính giả (FN) gần với mô hình gốc khi cắt tỉa các trọng số. Nhiều ứng dụng thực tế, ví dụ, phân loại hình ảnh y tế và phát hiện bất thường trong các quy trình sản xuất, phải đối phó với cả mất cân bằng dữ liệu và mất cân bằng lớp khi huấn luyện một mô hình sâu. Một mặt, dữ liệu thực tế thường tuân theo phân phối dữ liệu đuôi dài trong đó một số ít lớp chiếm phần lớn dữ liệu, trong khi nhiều lớp có ít mẫu hơn đáng kể [3]. Các mô hình được huấn luyện trên các bộ dữ liệu này bị thiên vị về các lớp chiếm ưu thế [4]. Văn hệ liên quan coi mất cân bằng dữ liệu là một vấn đề dẫn đến chất lượng mô hình thấp [5]. Các giải pháp được đề xuất thường áp dụng các chiến lược cân bằng lại lớp như lấy mẫu lại [6] và tái trọng số [7] dựa trên số lượng quan sát trong mỗi lớp. Mặt khác, có nhiều ứng dụng TinyML có tầm quan trọng lớp không cân bằng: ví dụ, bỏ lỡ một sự kiện có thể có hậu quả nghiêm trọng hơn nhiều so với việc kích hoạt báo động giả. Điều này đặc biệt đúng trong nhiều tình huống phát hiện và hệ thống cảnh báo sớm trong lĩnh vực IoT. Phương pháp của chúng tôi tự động thu nhỏ một mạng nơ-ron sâu đã được huấn luyện để tích hợp thiết bị di động. Trong bài báo này, chúng tôi tập trung vào việc giữ số lượng FN thấp khi nén một mạng sâu.

Theo hiểu biết của chúng tôi, đây là bài báo đầu tiên đề xuất nén mô hình phụ thuộc lớp. Chúng tôi cung cấp một phương pháp nén mạng từ đầu đến cuối dựa trên thuật toán vé số may mắn (LT) lặp lại được đề xuất gần đây [8] để tìm các mạng con nhỏ hiệu quả trong mạng gốc. Vì mất cân bằng dữ liệu là một vấn đề phổ biến mà các nhà thiết kế mô hình phải đối phó, trong bước đầu tiên, chúng tôi mở rộng thuật toán LT gốc với một hàm mất mát có tham số để chống lại mất cân bằng dữ liệu. Văn hệ liên quan cho thấy rằng việc bù đắp trực tiếp cho mất cân bằng dữ liệu vào hàm mất mát vượt trội hơn các phương pháp thay thế [4]. Trong bước thứ hai, chúng tôi kiểm soát số lượng âm tính giả và dương tính giả của mô hình bằng cách bao gồm một thước đo AUC-ROC có tham số (xem Mục III) vào nhiệm vụ nén mô hình. Chúng tôi quan sát thấy rằng việc huấn luyện epoch đầu tiên cho các lớp cân bằng để học các ranh giới lớp là có lợi. Tuy nhiên, trong các epoch sau, chúng tôi tập trung vào tối ưu hóa mất cân bằng lớp với mục tiêu giữ số lượng FN ở mức của mô hình gốc. Chúng tôi đánh giá phương pháp mới trên hai bộ dữ liệu công khai và cho thấy nó đạt được số lượng FN thấp hơn tới 35% so với thuật toán LT gốc không có huấn luyện mất cân bằng lớp trong khi chỉ bảo tồn 1% trọng số. Đáng ngạc nhiên, phương pháp của chúng tôi với hàm mất mát mới luôn vượt trội hơn phiên bản gốc của thuật toán LT trong tất cả các trường hợp được thử nghiệm.

--- TRANG 2 ---
Cân bằng Dữ liệu
Cân bằng Lớp
(AUC-ROC) Cân bằng Dữ liệu Mất cân bằng Lớp
(Trọng số FN)

Cắt tỉa
(tiêu chí tăng độ lớn) Huấn luyện:
chỉ epoch đầu tiên Dữ liệu
Huấn luyện Huấn luyện:
Tất cả các epoch khác

Mô hình Hình 1: Cắt tỉa mạng lặp lại với tối ưu hóa FN.

II. NÉN MẠNG PHỤ THUỘC LỚP

Phương pháp đề xuất của chúng tôi bao gồm đường ống nén mạng được trình bày trong Hình 1. Dữ liệu huấn luyện được sử dụng để đầu tiên huấn luyện một mô hình cân bằng lớp, trong khi các epoch tiếp theo ưu tiên giảm thiểu FN hơn FP. Hàm tối ưu hóa của chúng tôi trình bày sự kết hợp của hai hàm mất mát để 1) kiểm soát mất cân bằng dữ liệu, và 2) kiểm soát mất cân bằng lớp. Chúng tôi áp dụng mất mát cross-entropy có tham số [4] để đạt được mục tiêu trước, và sử dụng mất mát xếp hạng hinge [9] để tối đa hóa AUC-ROC và kiểm soát sự đánh đổi giữa FN và FP để giải quyết mục tiêu sau. Quy trình cắt tỉa lặp lại dựa trên thuật toán LT. Các đoạn văn dưới đây mô tả các khối xây dựng cá nhân của giải pháp của chúng tôi.

Thuật toán Vé số may mắn (LT). Phương pháp nén mạng phụ thuộc lớp của chúng tôi tận dụng cắt tỉa lặp lại được giới thiệu gần đây được sử dụng để tìm kiếm các mạng con thưa thớt hiệu quả được gọi là vé số may mắn (LT) [8] trong một mạng sâu gốc. Các mạng LT thường cho thấy hiệu suất vượt trội khi so sánh với mạng gốc. Phân tích được tiến hành gần đây về các mạng LT [10] cho thấy rằng cấu trúc mạng con kết hợp chặt chẽ với khởi tạo mạng là rất quan trọng để đạt được hiệu suất cao của các mạng LT. Hơn nữa, các điều kiện sau chịu trách nhiệm cho kết quả tốt nhất: 1) đặt các giá trị đã cắt tỉa về không thay vì bất kỳ giá trị nào khác, 2) giữ dấu của các trọng số khởi tạo khi tua lại, và 3) giữ các trọng số có giá trị tuyệt đối lớn nhất hoặc áp dụng tiêu chí mặt nạ tăng độ lớn trong quá trình cắt tỉa lặp lại. Chúng tôi tận dụng tất cả những phát hiện này trong công việc này. Chúng tôi sử dụng tiêu chí tăng độ lớn xuyên suốt bài báo, tức là chúng tôi xếp hạng các khác biệt giữa các giá trị cuối cùng và ban đầu của các trọng số mạng trong mỗi vòng và cắt tỉa p phần trăm ít nhất.

Thuật toán 1 Nén mạng phụ thuộc lớp
1: Khởi tạo ngẫu nhiên mạng f(x;m⊙W₀) với mặt nạ cắt tỉa ban đầu tầm thường m = 1|W₀|;
2: Huấn luyện mạng cho n lần lặp với mất mát phụ thuộc lớp L tạo ra mạng f(x;m⊙Wₖ);
3: Cắt tỉa p% trọng số còn lại với chiến lược tăng độ lớn, tức là m[i] := 0 nếu Wₖ[i] bị cắt tỉa;
4: Thay thế các trọng số còn lại Wₖ bằng các giá trị ban đầu W₀;
5: Đi đến bước 2 nếu vòng (k+1) tiếp theo được yêu cầu.

Thuật toán 1 cung cấp mã giả của thuật toán LT với tiêu chí mặt nạ tăng độ lớn và hàm mất mát phụ thuộc lớp L được giải thích bên dưới. Thuật toán khởi tạo mạng với các trọng số ngẫu nhiên W₀ và áp dụng mặt nạ cắt tỉa ban đầu tầm thường m = 1|W₀|. Phép toán ⊙ biểu thị phép nhân theo từng phần tử. Sau khi huấn luyện mạng cho n lần lặp, chúng tôi cắt tỉa p phần trăm trọng số bằng chiến lược tăng độ lớn bằng cách cập nhật mặt nạ m tương ứng. Các trọng số còn lại Wₖ sau đó được đặt lại về các giá trị ban đầu W₀ trước khi vòng thuật toán tiếp theo bắt đầu.

Trong mỗi vòng của thuật toán, chúng tôi giảm thiểu hàm mất mát L có dạng sau

L = L_wCE + L_SHR, (1)

trong đó L_wCE và L_SHR lần lượt là mất mát cross-entropy nhị phân có trọng số và mất mát xếp hạng hinge bình phương được chi tiết bên dưới.

Mất mát cross-entropy nhị phân có trọng số. Được truyền cảm hứng từ [4], chúng tôi mở rộng khái niệm cross-entropy nhị phân cổ điển để bù đắp cho mất cân bằng dữ liệu bằng cách giới thiệu các hệ số trọng số theo lớp như sau

L_wCE = Σ(c=1 to M) γc y_{o,c} log(p_{o,c}), (2)

trong đó γc là các hệ số trọng số cho mỗi lớp; M là số lớp; y_{o,c} ∈ {0,1} là chỉ số nhị phân nếu nhãn lớp c là phân loại đúng của quan sát o; p_{o,c} là xác suất dự đoán rằng quan sát o thuộc lớp c, và nc là số mẫu trong c.

Chúng tôi tận dụng kết quả trong [7], [11] và xử lý các hệ số trọng số γc cho các lớp riêng lẻ như γc = 1/(1-β)^nc, trong đó β ∈ [0,1) là siêu tham số. Trái ngược với công việc của họ, chúng tôi chọn giá trị của siêu tham số để bù đắp mất cân bằng dữ liệu có lợi cho một lớp cụ thể. Cài đặt β = 0 tương ứng với không trọng số và β → 1 tương ứng với trọng số theo tần số nghịch đảo cho một lớp nhất định. Công việc gần đây của [4] cho thấy rằng các hệ số trọng số γc đóng vai trò quan trọng trong cân bằng dữ liệu. Cụ thể, khi huấn luyện CNN trên dữ liệu mất cân bằng, cân bằng dữ liệu cho mỗi lớp, thông qua γc, cung cấp sự tăng cường đáng kể cho hiệu suất của các hàm mất mát thường được sử dụng, bao gồm cross-entropy.

Mất mát xếp hạng hinge bình phương. Văn hệ trước đó cho thấy rằng 1) tối ưu hóa độ chính xác phân loại bằng cách giảm thiểu cross-entropy không thể đảm bảo tối đa hóa AUC-ROC [12], và 2) tối đa hóa AUC-ROC như một nhiệm vụ tối ưu hóa tạo ra một hàm mục tiêu không liên tục không lồi và do đó không thể được tiếp cận bằng các phương pháp dựa trên gradient [13]. Các giải pháp được đề xuất cho tối đa hóa AUC-ROC [14] dựa trên các xấp xỉ. Trong bài báo này, chúng tôi sử dụng mất mát xếp hạng hinge bình phương được đề xuất trong [9], trong khi thêm các tham số λc để kiểm soát mất cân bằng lớp.

L_SHR = Σ(c=1 to M) λc max(1 - y_{o,c} r_{o,c}, 0)^2. (3)

Mất mát xếp hạng hinge bình phương được thu được từ mất mát hinge bằng cách thay thế p_{o,c} bằng đầu ra bộ phân loại r_{o,c} được sắp xếp theo thứ tự tăng dần.

--- TRANG 3 ---
Hình 2: Hình ảnh từ bộ dữ liệu ISIC [15]: mẫu lành tính (trái) và ung thư hắc tố (phải).

Hình 3: Hình ảnh từ bộ dữ liệu CRACK [16]: mẫu âm tính (trái) và dương tính (phải).

Các tác giả của [9] cho thấy rằng AUC-ROC có thể được viết dưới dạng mất mát xếp hạng hinge như sau

AUC-ROC ≈ 1 - L_SHR/C / (n+ · n-), (4)

trong đó n+, n- là số lượng mẫu dương tính và âm tính và C là hằng số độc lập với thứ tự xếp hạng. Giảm thiểu mất mát xếp hạng hinge dẫn đến tối đa hóa AUC-ROC [9]. Chúng tôi sử dụng mất mát xếp hạng hinge bình phương L_SHR để đảm bảo hàm mất mát L của chúng tôi có thể vi phân.

Trong phần tiếp theo, chúng tôi đánh giá hiệu quả của thuật toán nén mạng phụ thuộc lớp được đề xuất trên hai bộ dữ liệu chuẩn.

III. ĐÁNH GIÁ THỰC NGHIỆM

Phần này giới thiệu các bộ dữ liệu chuẩn, liệt kê các chỉ số chúng tôi sử dụng để đánh giá hiệu suất của phương pháp, biện minh cho các lựa chọn tham số và trình bày kết quả.

A. Bộ dữ liệu

Chúng tôi chọn các bộ dữ liệu chuẩn dựa trên độ phức tạp của nhiệm vụ phân loại mà chúng đưa ra. ISIC là một bộ dữ liệu phân loại tổn thương hình ảnh y tế đầy thách thức được giới thiệu trong một cuộc thi khoa học dữ liệu². Bộ dữ liệu CRACK có độ phức tạp phân loại thấp. Cả hai bộ chuẩn đều được giới thiệu bên dưới.

²http://challenge2016.isic-archive.com

Tên | Mô tả & Giá trị
Độ bão hòa | Điều chỉnh độ bão hòa bằng 0.3
Độ tương phản | Điều chỉnh độ tương phản bằng 0.3
Độ sáng | Điều chỉnh độ sáng bằng 0.3
Sắc độ | Dịch chuyển sắc độ bằng 0.1
Lật | Lật ngẫu nhiên theo chiều ngang và dọc
Affine | Quay 90°, cắt 20°, tỷ lệ [0.8, 1.2]
Cắt | Cắt ngẫu nhiên (>40% diện tích) hình ảnh
Đàn hồi | Làm méo hình ảnh với splines tấm mỏng

BẢNG I: Tăng cường dữ liệu ISIC-2016 [17].

Bộ dữ liệu phân loại tổn thương ISIC-2016 [15] bao gồm các hình ảnh y tế gốc được ghép nối với nhãn chẩn đoán ác tính được xác nhận thu được từ sự đồng thuận của chuyên gia và thông tin báo cáo bệnh lý, tức là mỗi hình ảnh được gán một nhãn lành tính hoặc ung thư hắc tố. Bộ dữ liệu huấn luyện chứa 900 hình ảnh tổn thương soi da với 173 ví dụ dương tính và 727 ví dụ âm tính tương ứng, trong khi bộ kiểm tra bao gồm 379 hình ảnh với 76 mẫu dương tính và 303 mẫu âm tính tương ứng. Hình 2 cho thấy các hình ảnh mẫu dương tính và âm tính từ bộ dữ liệu ISIC.

Chúng tôi tận dụng các kỹ thuật tăng cường dữ liệu thực hành tốt nhất được đề xuất trong [17] và được hiển thị trong Bảng I. Điều này bao gồm điều chỉnh độ bão hòa, độ tương phản và độ sáng của hình ảnh bằng 0.3, dịch chuyển sắc độ bằng 0.1, lật ngẫu nhiên hình ảnh, biến đổi Affine (quay 90°, cắt 20°, tỷ lệ [0.8, 1.2]), cắt ngẫu nhiên hình ảnh (>40% diện tích). Các tăng cường độ bão hòa, độ tương phản và độ sáng mô phỏng các thay đổi màu sắc do cài đặt camera và đặc điểm tổn thương. Các biến đổi Affine tái tạo các biến dạng camera và tạo ra các hình dạng tổn thương mới. Làm méo đàn hồi được tạo ra bằng cách xác định các điểm gốc như một lưới 4×4 các điểm cách đều nhau, và các điểm đích như các điểm ngẫu nhiên xung quanh các điểm gốc (lên đến 10% chiều rộng hình ảnh theo mỗi hướng). Những tăng cường này tạo ra sự gia tăng dữ liệu huấn luyện gấp 10 lần so với bộ dữ liệu gốc, trong khi duy trì các thuộc tính y tế [17].

Bộ dữ liệu CRACK [16] chứa 40K hình ảnh 224×224 pixel được cắt từ 500 hình ảnh độ phân giải đầy đủ 4032×3024 pixel được chụp từ tường và sàn của một số tòa nhà bê tông. Các hình ảnh được chụp cách khoảng 1m từ các bề mặt với camera hướng trực tiếp vào mục tiêu. Các bề mặt bê tông có sự biến đổi về mặt hoàn thiện bề mặt (lộ ra, trát và sơn). Nhãn là dương tính nếu hình ảnh chứa vết nứt và âm tính nếu ngược lại. Các nhãn được gán bởi các chuyên gia khoa học vật liệu. Hình 3 cho thấy các mẫu dương tính và âm tính trong bộ dữ liệu này.

B. Chỉ số đánh giá

Chúng tôi áp dụng các chỉ số đánh giá sau: AUC-ROC, độ chính xác, Tỷ lệ Âm tính Giả (FNR) và Tỷ lệ Dương tính Giả (FPR) được tóm tắt ngắn gọn bên dưới.

Thước đo AUC-ROC ước tính xác suất rằng một mẫu được chọn ngẫu nhiên của lớp dương tính có xác suất ước tính thuộc về lớp âm tính nhỏ hơn so với một thành viên được chọn ngẫu nhiên của lớp âm tính [9]. :

AUC-ROC = 1/(n+ · n-) Σ(i=1 to n+) Σ(j=1 to n-) I(r+i > r-j), (5)

--- TRANG 4 ---
trong đó n+, n- là số lượng mẫu dương tính và âm tính và I là hàm chỉ thị. r+i ∈ {1,...,n+} biểu thị thứ hạng của các ví dụ dương tính và r-j ∈ {1,...,n-} biểu thị thứ hạng của các ví dụ âm tính.

Chúng tôi sử dụng thước đo FNR để cho thấy rằng phương pháp đề xuất thực sự giảm số lượng âm tính giả. Chúng tôi cũng báo cáo thước đo FPR để hiểu các đánh đổi đạt được giữa tỷ lệ dương tính giả và âm tính giả. AUC-ROC đo diện tích bên dưới toàn bộ đường cong ROC. Đường cong ROC vẽ TPR so với FPR ở các ngưỡng phân loại khác nhau. Kết quả trong [12] cho thấy rằng giá trị kỳ vọng của AUC-ROC trên tất cả các phân loại là hàm đơn điệu của độ chính xác. Điều này cũng đúng cho dữ liệu mất cân bằng. Chúng tôi báo cáo độ chính xác để đảm bảo hiệu suất mạng nén tổng thể vẫn cao.

C. Thiết lập thực nghiệm

Chúng tôi tạo ra mất cân bằng dữ liệu trong bộ dữ liệu CRACK bằng cách sử dụng 20K hình ảnh trong lớp âm tính (không có vết nứt) và 4K hình ảnh của lớp dương tính, và sử dụng 70%, 15%, 15% mẫu cho huấn luyện, xác thực và kiểm tra tương ứng.

Mạng. Để phân loại trên các bộ dữ liệu ISIC-2016 và CRACK, chúng tôi áp dụng AlexNet [18] được huấn luyện trước trên ImageNet [19] với số lượng lớp kết nối đầy đủ được điều chỉnh để chứa 256, 8 và 2 nơ-ron. Mạng này có 2'471'842 tham số. Vì phương pháp nén của chúng tôi sử dụng cắt tỉa lặp lại dựa trên thuật toán LT, chúng tôi sử dụng các mạng tương đối nông này để giữ tính toán có thể quản lý được trên tài nguyên tính toán có sẵn. Nút thắt cổ chai kỹ thuật ở đây là việc tắt gradient trong lượt truyền ngược để giữ các giá trị đã cắt tỉa được đặt về không. Với cơ sở hạ tầng phần cứng mạnh hơn, phương pháp của chúng tôi có thể được sử dụng trên các mạng sâu hơn như VGG và ResNet.

Siêu tham số. Để đánh giá hiệu suất của phương pháp đề xuất, chúng tôi tuân theo thiết kế hai bước của mình. Đầu tiên chúng tôi tập trung vào cân bằng dữ liệu mất cân bằng bằng tham số β, sau đó chúng tôi sử dụng các nhân λ cho mất mát xếp hạng để tối ưu hóa AUC-ROC. Tận dụng kết quả được báo cáo trong [4], [11] để đạt được cân bằng dữ liệu bằng cách đặt các tham số hàm mất mát theo tần số lớp nghịch đảo, chúng tôi đặt β gần 1 trong các thử nghiệm của mình. Đối với các bộ dữ liệu ISIC và CRACK γc = 727/173 = 4.2 và γc = 14K/2.8K = 5 tương ứng. Để đơn giản, chúng tôi sử dụng γc = 5 trong cả hai trường hợp. Điều này tương ứng với β = 0.99997 cho cả hai bộ dữ liệu. Chúng tôi thử nghiệm λc ∈ {0,1,2,10}, trong đó λc = 0 đại diện cho thuật toán LT tiêu chuẩn, và λc = 5 hoạt động tốt nhất trong tất cả các kịch bản khác với trọng số không đồng nhất cho các lớp dương tính và âm tính.

Đối với mỗi vòng của Thuật toán 1 trong bước 2, chúng tôi huấn luyện mạng cho k = 100 lần lặp. Với phần cứng mạnh hơn, có thể huấn luyện mạng lâu hơn để đạt được kết quả tốt hơn tiềm năng. Vì quy trình của chúng tôi huấn luyện AlexNet đã được huấn luyện trước trên bộ dữ liệu ImageNet, sự khác biệt độ chính xác giữa k = 1000 và k = 100 lần lặp là ít hơn 3%. Bằng cách tuân theo chiến lược cắt tỉa tăng độ lớn, chúng tôi cắt tỉa p = 50% trọng số còn lại trong mỗi vòng. Điều này tạo ra các mạng nén với |Wk| = 100%, 50%, 25%, 12.5%, 6.25%, 3.12% và 1.57% trọng số còn lại. Chúng tôi sử dụng hạ gradient ngẫu nhiên (SGD) với cài đặt động lượng 0.9 để huấn luyện mạng.

Kịch bản. Chúng tôi thử nghiệm phương pháp của mình trong ba kịch bản xanh, đen và xanh lá cây, cùng với thuật toán LT làm cơ sở, và so sánh hiệu suất thu được với ba bộ chuẩn phổ biến gần đây. Các kịch bản này tương ứng với các đường có màu khác nhau trong các biểu đồ:

đỏ Tương ứng với thuật toán LT gốc với tiêu chí cắt tỉa tăng độ lớn. Trọng số cho cả lớp dương tính và âm tính được đặt thành γc = 1 và chúng tôi không sử dụng mất mát xếp hạng với λc = 0. Do đó, sử dụng hiệu suất tốt nhất của thuật toán LT cổ điển làm chuẩn.

xanh Trong kịch bản này, chúng tôi thử nghiệm hiệu ứng của mất mát xếp hạng (cân bằng lớp) so với thuật toán LT tiêu chuẩn (đỏ), không có trọng số mất mát cross entropy (cân bằng dữ liệu). Do đó, chúng tôi có γc = 1 và λc = 5.

đen Trong kịch bản này, chúng tôi thử nghiệm hiệu ứng của trọng số mất mát cross entropy (cân bằng dữ liệu). Do đó, so với kịch bản trước (xanh), chúng tôi có γc = 5 cùng với λc = 5. Chúng tôi thấy rằng bắt đầu vòng đầu tiên với γ1 = 1 giúp mạng ban đầu tìm ra ranh giới giữa hai lớp mà không có tiêu điểm cụ thể nào

xanh lá cây Trong kịch bản này chúng tôi thử nghiệm với các giá trị trọng số cao hơn cho cân bằng dữ liệu, vì vậy chúng tôi đặt γc = 10 ở mọi nơi, trong khi λc = 5.

LOBS [20] đại diện cho thuật toán Layer-Wise Optimal Brain Surgeon. Phương pháp cắt tỉa này xác định tầm quan trọng của các nơ-ron từ các giá trị trong ma trận Hessian tương ứng và cắt tỉa những nơ-ron ít quan trọng nhất trong mỗi lớp.

SNIP là thuật toán cắt tỉa mạng một lần [21]. Nó cắt tỉa các bộ lọc dựa trên tiêu chí độ nhạy kết nối. Ở đây chúng tôi cắt tỉa mạng mà không huấn luyện lại.

SNIP có huấn luyện có nghĩa là một mạng được huấn luyện được cắt tỉa bởi SNIP và sau đó được huấn luyện lại.

MobileNet [22] thay thế tích chập bình thường bằng tích chập theo chiều sâu theo sau bởi tích chập theo điểm, được gọi là tích chập có thể phân tách theo chiều sâu. Điều này giảm tổng số phép nhân dấu phẩy động và do đó giảm đáng kể số lượng tham số.

D. Kết quả

Hình 4 và Hình 5 cho thấy kết quả đánh giá cho các bộ dữ liệu ISIC và CRACK. Chúng tôi so sánh kết quả của mình với hiệu suất tốt nhất thu được với thuật toán LT cổ điển với tiêu chí cắt tỉa tăng độ lớn cùng với ba bộ chuẩn gần đây khác. Mục tiêu của chúng tôi là cắt tỉa mạng trong khi tối đa hóa thước đo AUC-ROC và độ chính xác phân loại, nhưng giữ số lượng âm tính giả cho một lớp cụ thể càng thấp càng tốt.

Như có thể thấy trong Hình 4(a), AUC-ROC tốt nhất cho ISIC (đường đen) đạt được khi λc = 5, γc = 1 cho epoch huấn luyện đầu tiên (γ1 = 1) và γc = 5 cho các epoch sau (γ2,n = 5), trong đó γc = 5 cho tần số lớp nghịch đảo cho bộ dữ liệu này.

--- TRANG 5 ---
[Các biểu đồ được mô tả nhưng không dịch chi tiết vì chúng là hình ảnh]

Hình 4: Kết quả đánh giá trên ISIC-2016. Phương pháp của chúng tôi (đường đen) vượt trội hơn thuật toán LT (đường đỏ) về AUC-ROC, độ chính xác, FNR và FPR cho tới 1% trọng số còn lại trong mạng đã cắt tỉa. Phương pháp của chúng tôi cũng vượt trội hơn LOBS, SNIP và MobileNet về AUC-ROC và FPR.

Hình 5: Kết quả đánh giá trên bộ dữ liệu CRACK. Phương pháp của chúng tôi (đường đen) vượt trội hơn thuật toán LT (đường đỏ) về AUC-ROC, độ chính xác, FNR và FPR cho tới 12% trọng số còn lại. Chúng tôi cũng vượt trội hơn LOBS, SNIP và MobileNet về AUC-ROC và độ chính xác.

Cài đặt γ1 = 1 nhấn mạnh tầm quan trọng của việc học các ranh giới lớp trong lần lặp đầu tiên bằng cách sử dụng huấn luyện cân bằng. Do đó, tiêu điểm vào lớp dương tính bắt đầu từ lần lặp thứ hai nơi chúng tôi sử dụng γ2,n = 5 cho lớp dương tính mong muốn. Cài đặt này không chỉ vượt trội hơn vé số may mắn tiêu chuẩn về AUC-ROC, mà còn vượt trội hơn các bộ chuẩn phổ biến gần đây, tức là LOBS, SNIP và MobileNet.

Một mặt, các kết quả được trình bày trong [12] cho thấy rằng giá trị kỳ vọng của AUC-ROC trên tất cả các phân loại là hàm đơn điệu của độ chính xác, khi chúng ta có bộ dữ liệu mất cân bằng. Mặt khác, các tác giả trong [4] lập luận rằng thuật ngữ cân bằng dữ liệu β cải thiện hiệu suất của mất mát cross-entropy về độ chính xác. Kết quả của chúng tôi trong Hình 4(b) xác nhận những phát hiện này: độ chính xác cho các cài đặt tốt nhất của phương pháp chúng tôi (đường đen) luôn tốt hơn độ chính xác của thuật toán LT (đường đỏ). Mặc dù độ chính xác cho LOBS và SNIP vẫn cao qua nén, điều này là do thực tế rằng chúng chỉ phân loại chính xác tất cả các mẫu trong lớp dương tính (xem FPR cao trong Hình 4(d)).

Hình 4(c) cho thấy rằng thêm mất mát xếp hạng vào thuật toán LT tiêu chuẩn như một proxy cho cân bằng lớp cải thiện FNR (đường xanh). Tuy nhiên, trọng số cross entropy cho cân bằng dữ liệu cải thiện FNR thậm chí mạnh hơn (đường đen). FNR tốt nhất đạt được khi chúng tôi sử dụng trọng số cao hơn cho lớp dương tính bằng cách đặt γc = 10 trong tất cả các vòng cắt tỉa (đường xanh lá cây). Thuật toán LT tiêu chuẩn và phương pháp xanh lá cây tạo ra FNR là 0.13 và 0.09, cho thấy cải thiện 35% so với LT.

So sánh Hình 4(d) với Hình 4(c) cho thấy sự đánh đổi giữa FPR và FNR. Như chúng ta có thể thấy ở đây, chúng ta có FPR thấp nhất khi chúng ta không có cân bằng lớp và không có cân bằng dữ liệu. FPR cho LOBS và SNIP không có huấn luyện (sau cắt tỉa) cũng rất cao (gần 1), có nghĩa là chúng phân loại sai tất cả các mẫu âm tính thành dương tính.

Cài đặt tốt nhất của chúng tôi (đường đen) trong lần lặp đầu tiên (không nén) đánh bại AUC-ROC và độ chính xác tốt nhất cho thử thách ISIC: AUC-ROC và độ chính xác của chúng tôi lần lượt là 0.8069 và 0.8608, vượt trội hơn AUC-ROC = 0.804 và độ chính xác = 0.855 được báo cáo bởi các tác giả trong [23]. Kiến trúc rất sâu được đề xuất của họ phụ thuộc cao vào kết quả phân đoạn, trong khi phương pháp của chúng tôi là thuật toán từ đầu đến cuối.

Đối với bộ dữ liệu CRACK, Hình 5(a) cho thấy kết quả cho AUC-ROC. AUC-ROC tốt nhất đạt được khi sử dụng bộ tham số sau: γ1 = 1, γ2,n = 5, λc = 5 (đường đen), trong đó γc = 5 cho tần số lớp nghịch đảo cho bộ dữ liệu này. Tương tự như ISIC, phương pháp của chúng tôi vượt trội hơn LT tiêu chuẩn và các bộ chuẩn phổ biến LOBS, SNIP và MobileNet về AUC-ROC. Như có thể thấy trong Hình 5(b), độ chính xác cho phương pháp của chúng tôi trong cài đặt tốt nhất (đường đen), không chỉ tốt hơn độ chính xác tốt nhất đạt được bởi phương pháp LT (đường đỏ), mà còn LOBS, SNIP và MobileNet.

Ngoài LOBS và SNIP không có huấn luyện (sau cắt tỉa) có FPR gần 1, FNR tốt nhất đạt được trong Hình 5(c) là khi chúng tôi đặt γc = 10 cho tất cả các vòng cắt tỉa (đường xanh lá cây). FNR cho thuật toán LT và đường xanh lá cây tạo ra FNR là 0.017 và 0.011, cho thấy cải thiện 35%. Tuy nhiên, do sự đánh đổi tự nhiên giữa FNR và FPR, việc tăng trọng số cho lớp dương tính dẫn đến FPR cao hơn.

IV. CÔNG VIỆC LIÊN QUAN

Các mạng sâu được biết là có độ dư thừa cao. Điều này đã thúc đẩy nhiều nhà nghiên cứu tìm kiếm các kỹ thuật nén mạng và các mạng con hiệu quả. Phần này tóm tắt các nỗ lực gần đây.

Lượng tử hóa và nhị phân hóa dựa vào các trọng số có giá trị rời rạc. [24] đề xuất một thuật toán xấp xỉ hậu nghiệm của các trọng số mạng nơ-ron, nhưng các trọng số có thể bị hạn chế có giá trị rời rạc hoặc nhị phân. [25] áp dụng các xấp xỉ cho CNN tiêu chuẩn. Binary-Weight-Network của họ xấp xỉ các bộ lọc với giá trị nhị phân và giảm kích thước của các mạng ví dụ với hệ số 32x.

Phân tách và phân tích nhân tử khám phá cơ sở thấp hạng của các bộ lọc để giảm kích thước mô hình. [26] biểu diễn ngân hàng đầy đủ hạng đã học của CNN như sự kết hợp của các bộ lọc hạng-1 dẫn đến tăng tốc 4.5x. Các phương pháp gần đây hơn [27] dựa vào tích chập có thể phân tách theo chiều sâu và theo điểm để giảm độ phức tạp tính toán. Tích chập theo chiều sâu thực hiện lọc nhẹ bằng cách áp dụng một nhân tích chập duy nhất cho mỗi kênh đầu vào. Tích chập theo điểm mở rộng bản đồ đặc trưng dọc theo các kênh bằng cách học các kết hợp tuyến tính của các kênh đầu vào.

Cắt tỉa bao gồm một tập hợp các phương pháp giảm kích thước mô hình bằng cách loại bỏ các kết nối mạng. Các phương pháp này có từ thời optimal brain damage [28], nơi các tác giả đề xuất cắt tỉa các trọng số dựa trên Hessian của hàm mất mát. [29] đề xuất cắt tỉa các kênh trong CNN dựa trên chuẩn trọng số bộ lọc tương ứng, trong khi [30] sử dụng phần trăm trung bình của các số không trong đầu ra để cắt tỉa các kênh không quan trọng. Giả thuyết LT [8] đề xuất cắt tỉa lặp lại để loại bỏ các trọng số có độ lớn nhỏ. Các phương pháp thường tạo ra các mạng con thưa thớt hiệu quả.

Chưng cất kiến thức bao gồm các phương pháp chuyển giao kiến thức từ một giáo viên lớn hơn sang một học sinh nhỏ hơn. [31] khai thác cài đặt đối kháng để huấn luyện mạng học sinh. Bộ phân biệt cố gắng phân biệt giữa học sinh và giáo viên. Họ sử dụng mất mát L2 để buộc học sinh bắt chước đầu ra của giáo viên. [32] áp dụng chưng cất kiến thức cho GAN để tạo ra một bộ tạo nén mà không mất chất lượng hay khả năng tổng quát hóa. Họ giả định rằng tồn tại giới hạn nén cơ bản của GAN tương tự như lý thuyết nén của Shannon.

Phương pháp nén mạng của chúng tôi kết hợp cắt tỉa mạng với thiết kế mạng hiệu quả. Không giống như các phương pháp nén khác cố gắng giảm thiểu tỷ lệ lỗi tổng thể, phương pháp của chúng tôi bổ sung tối ưu hóa AUC-ROC trong khi tập trung vào một lớp mong muốn có vẻ hữu ích trong một số ứng dụng thực tế trong lĩnh vực IoT.

TÀI LIỆU THAM KHẢO
[1] K. Simonyan và A. Zisserman, "Very deep convolutional networks for large-scale image recognition," International Conference on Learning Representations (ICLR), 2015, 2015.
[2] Y. Lu, A. Kumar, S. Zhai, và et. al., "Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification," trong CVPR, 2017, tr. 5334–5343.
[3] D. Hasenfratz, O. Saukh, C. Walser, C. Hueglin, M. Fierz, và L. Thiele, "Pushing the spatio-temporal resolution limit of urban air pollution maps," trong Proceedings of the IEEE Conference on Pervasive Computing and Communications (PerCom), 2014, tr. 69–77.
[4] Y. Cui, M. Jia, T.-Y. Lin, và et. al., "Class-balanced loss based on effective number of samples," trong CVPR, 2019, tr. 9268–9277.
[5] N. V. Chawla, K. W. Bowyer, và et. al., "SMOTE: synthetic minority over-sampling technique," AI research, tập 16, tr. 321–357, 2002.
[6] M. Buda, A. Maki, và M. A. Mazurowski, "A systematic study of the class imbalance problem in convolutional neural networks," Neural Networks, tập 106, tr. 249–259, 2018.
[7] Y.-X. Wang, D. Ramanan, và M. Hebert, "Learning to model the tail," trong NIPS, 2017, tr. 7029–7039.
[8] J. Frankle và M. Carbin, "The lottery ticket hypothesis: Finding sparse, trainable neural networks," In International Conference on Learning Representations (ICLR), 2019.
[9] H. Steck, "Hinge rank loss and the area under the roc curve," trong ECML, 2007, tr. 347–358.
[10] H. Zhou, J. Lan, R. Liu, và J. Yosinski, "Deconstructing lottery tickets: Zeros, signs, and the supermask," arXiv preprint arXiv:1905.01067, 2019.
[11] C. Huang, Y. Li, C. Change Loy, và X. Tang, "Learning deep representation for imbalanced classification," trong CVPR, 2016, tr. 5375–5384.
[12] C. Cortes và M. Mohri, "AUC optimization vs. error rate minimization," trong NIPS, 2004, tr. 313–320.
[13] L. Yan, R. H. Dodier, M. Mozer, và R. H. Wolniewicz, "Optimizing classifier performance via an approximation to the wilcoxon-mann-whitney statistic," trong ICML, 2003, tr. 848–855.
[14] S. Gultekin, A. Saha, A. Ratnaparkhi, và J. Paisley, "Mba: Mini-batch auc optimization," arXiv preprint arXiv:1805.11221, 2018.
[15] D. Gutman, N. C. Codella, E. Celebi, và et. al., "Skin lesion analysis toward melanoma detection," arXiv preprint arXiv:1605.01397, 2016.
[16] Ç. Özgenel, "Concrete crack images for classification," Mendeley Data, v1, 2017.
[17] F. Perez, C. Vasconcelos, S. Avila, và E. Valle, "Data augmentation for skin lesion analysis," trong OR 2.0, 2018, tr. 303–311.
[18] A. Krizhevsky, I. Sutskever, và G. E. Hinton, "ImageNet classification with deep convolutional neural networks," trong NIPS, 2012, tr. 1097–1105.
[19] J. Deng, W. Dong, R. Socher, và et. al., "ImageNet: A large-scale hierarchical image database," trong CVPR, 2009, tr. 248–255.
[20] X. Dong, S. Chen, và S. Pan, "Learning to prune deep neural networks via layer-wise optimal brain surgeon," trong Advances in Neural Information Processing Systems, 2017, tr. 4857–4867.
[21] N. Lee, T. Ajanthan, và P. H. Torr, "Snip: Single-shot network pruning based on connection sensitivity," In International Conference on Learning Representations (ICLR), 2019, 2018.
[22] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, và et. al., "MobileNets: Efficient convolutional neural networks for mobile vision applications," CoRR, abs/1704.04861, 2018.
[23] L. Yu, H. Chen, Q. Dou, và et. al., "Automated melanoma recognition in dermoscopy images via very deep residual networks," IEEE Trans. on Medical Imaging, tập 36, số 4, tr. 994–1004, 2016.
[24] D. Soudry, I. Hubara, và R. Meir, "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights," trong NIPS, 2014, tr. 963–971.
[25] M. Rastegari, V. Ordonez, J. Redmon, và A. Farhadi, "Xnor-net: Imagenet classification using binary convolutional neural networks," trong ECCV, 2016, tr. 525–542.
[26] M. Jaderberg, A. Vedaldi, và A. Zisserman, "Speeding up convolutional neural networks with low rank expansions," arXiv preprint arXiv:1405.3866, 2014.
[27] S. Mehta, M. Rastegari, L. Shapiro, và H. Hajishirzi, "Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network," trong CVPR, 2019, tr. 9190–9200.
[28] Y. LeCun, J. S. Denker, và S. A. Solla, "Optimal brain damage," trong Advances in neural information processing systems, 1990, tr. 598–605.
[29] H. Li, A. Kadav, I. Durdanovic, H. Samet, và H. P. Graf, "Pruning filters for efficient convnets," arXiv preprint arXiv:1608.08710, 2016.
[30] H. Hu, R. Peng, Y.-W. Tai, và C.-K. Tang, "Network trimming: A data-driven neuron pruning approach towards efficient deep architectures," arXiv preprint arXiv:1607.03250, 2016.
[31] V. Belagiannis, A. Farshad, và F. Galasso, "Adversarial network compression," trong ECCV, 2018, tr. 0–0.
[32] A. Aguinaldo, P.-Y. Chiang, và et. al., "Compressing GANs using knowledge distillation," arXiv preprint arXiv:1902.00159, 2019.
