Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Lý luận về thông thức vật lý trong ngôn ngữ tự nhiên. Trong Proc. AAAI Conf. on Arti. Intel., tập 34, trang 7432–7439.

Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, và John Guttag. 2020. Tình trạng của việc cắt tỉa mạng neural là gì? Proc. Int. Conf. Mach. Learn. and Syst., 2:129–146.

Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, và Ping Luo. 2022. Adaptformer: Thích ứng vision transformer cho nhận dạng thị giác có thể mở rộng. Proc. Adv. Neural Inf. Process. Syst.

Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, và Jiaya Jia. 2023. Longlora: Fine-tuning hiệu quả của các mô hình ngôn ngữ lớn ngữ cảnh dài. arXiv preprint arXiv:2309.12307.

Zhenyi Lu Chenghao Fan và Jie Tian. 2023. Chinese-vicuna: Một mô hình dựa trên llama theo hướng dẫn tiếng Trung.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, và Oyvind Tafjord. 2018. Nghĩ bạn đã giải quyết được việc trả lời câu hỏi? thử arc, thách thức lý luận ai2. arXiv preprint arXiv:1803.05457.

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. 2023. Qlora: Fine-tuning hiệu quả của llm được lượng tử hóa. arXiv preprint arXiv:2305.14314.

Xin Dong, Shangyu Chen, và Sinno Pan. 2017. Học cách cắt tỉa mạng neural sâu qua layer-wise optimal brain surgeon. Proc. Adv. Neural Inf. Process. Syst., 30.

Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, và Jie Tang. 2022. Glm: Pre-training mô hình ngôn ngữ chung với autoregressive blank infilling. Trong Proc. Annual Associa. Comp. Linguis., trang 320–335.

Bryn Elesedy, Varun Kanade, và Yee Whye Teh. 2020. Lottery tickets trong mô hình tuyến tính: Một phân tích về iterative magnitude pruning. arXiv preprint arXiv:2007.08243.

Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, và Xinchao Wang. 2023. Depgraph: Hướng tới bất kỳ cắt tỉa cấu trúc nào. Trong Proc. IEEE Conf. Comp. Vis. Patt. Recogn., trang 16091–16101.

Elias Frantar và Dan Alistarh. 2023. Các mô hình ngôn ngữ khổng lồ có thể được cắt tỉa chính xác trong một lần. arXiv preprint arXiv:2301.00774.

Elias Frantar, Saleh Ashkboos, Torsten Hoefler, và Dan Alistarh. 2022. Gptq: Lượng tử hóa sau huấn luyện chính xác cho transformer tạo sinh được huấn luyện trước. arXiv preprint arXiv:2210.17323.

Song Guo, Jiahang Xu, Li Lyna Zhang, và Mao Yang. 2023a. Compresso: Cắt tỉa có cấu trúc với collaborative prompting học các mô hình ngôn ngữ lớn compact. arXiv preprint arXiv:2310.05015.

Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, và Bo Dai. 2023b. Animatediff: Animate các mô hình diffusion text-to-image cá nhân hóa của bạn mà không cần tuning cụ thể. arXiv preprint arXiv:2307.04725.

Song Han, Jeff Pool, John Tran, và William Dally. 2015. Học cả trọng số và kết nối cho mạng neural hiệu quả. Proc. Adv. Neural Inf. Process. Syst., 28.

Babak Hassibi, David G Stork, và Gregory J Wolff. 1993. Optimal brain surgeon và general network pruning. Trong Proc. IEEE Conf. on Neural Networks, trang 293–299.

Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, và Bohan Zhuang. 2023. Sensitivity-aware visual parameter-efficient tuning. Trong Proc. IEEE Int. Conf. Comp. Vis.

Yang He, Yuhang Ding, Ping Liu, Linchao Zhu, Hanwang Zhang, và Yi Yang. 2020. Học tiêu chí cắt tỉa filter cho tăng tốc mạng neural tích chập sâu. Trong Proc. IEEE Conf. Comp. Vis. Patt. Recogn., trang 2009–2018.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. 2020. Đo lường hiểu biết ngôn ngữ đa tác vụ khổng lồ. arXiv preprint arXiv:2009.03300.

Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. 2022. LoRA: Thích ứng thứ bậc thấp của các mô hình ngôn ngữ lớn. Trong Proc. Int. Conf. Learn. Repren.

Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, và Ser-Nam Lim. 2022. Visual prompt tuning. Trong Proc. Eur. Conf. Comp. Vis.

Yann LeCun, John Denker, và Sara Solla. 1989. Optimal brain damage. Proc. Adv. Neural Inf. Process. Syst., 2.

Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, và Jinwoo Shin. 2020. Layer-adaptive sparsity cho magnitude-based pruning. arXiv preprint arXiv:2010.07611.

Namhoon Lee, Thalaiyasingam Ajanthan, và Philip Torr. 2019. Snip: Single-shot network pruning dựa trên connection sensitivity. Trong Proc. Int. Conf. Learn. Repren.

Guiying Li, Chao Qian, Chunhui Jiang, Xiaofen Lu, và Ke Tang. 2018. Optimization based layer-wise magnitude-based pruning cho nén DNN. Trong Int. Joi. Conf. on Artificial Intelligence, trang 2383–2389.

Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. 2017. Cắt tỉa filter cho convnet hiệu quả. Trong Proc. Int. Conf. Learn. Repren.

Yuchao Li, Fuli Luo, Chuanqi Tan, Mengdi Wang, Songfang Huang, Shen Li, và Junjie Bai. 2022. Parameter-efficient sparsity cho fine-tuning mô hình ngôn ngữ lớn. arXiv preprint arXiv:2205.11005.

Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Zhiyu Wang, và Rongrong Ji. 2023. Hướng tới thích ứng thị giác hiệu quả qua structural re-parameterization. arXiv preprint arXiv:2302.08106.

Xinyin Ma, Gongfan Fang, và Xinchao Wang. 2023. Llm-pruner: Về cắt tỉa cấu trúc của các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2305.11627.

Mitchell Marcus, Beatrice Santorini, và Mary Ann Marcinkiewicz. 1993. Xây dựng một corpus chú thích lớn của tiếng Anh: Penn treebank.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.

Todor Mihaylov, Peter Clark, Tushar Khot, và Ashish Sabharwal. 2018. Một bộ áo giáp có thể dẫn điện không? một tập dữ liệu mới cho trả lời câu hỏi sách mở. arXiv preprint arXiv:1809.02789.

Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, và Jan Kautz. 2019. Ước lượng tầm quan trọng cho cắt tỉa mạng neural. Trong Proc. IEEE Conf. Comp. Vis. Patt. Recogn., trang 11264–11272.

Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, và Jan Kautz. 2017. Cắt tỉa mạng neural tích chập cho suy luận hiệu quả tài nguyên. Trong Proc. Int. Conf. Learn. Repren.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. 2020. Khám phá giới hạn của transfer learning với unified text-to-text transformer. 21(1):5485–5551.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. 2021. Winogrande: Một thách thức winograd schema đối kháng ở quy mô lớn. Communications of the ACM, 64(9):99–106.

Victor Sanh, Thomas Wolf, và Alexander Rush. 2020. Movement pruning: Adaptive sparsity bởi fine-tuning. Proc. Adv. Neural Inf. Process. Syst., 33:20378–20389.

Mingjie Sun, Zhuang Liu, Anna Bair, và J Zico Kolter. 2023. Một phương pháp cắt tỉa đơn giản và hiệu quả cho các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2306.11695.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023. Stanford alpaca: Một mô hình llama theo hướng dẫn. https://github.com/tatsu-lab/stanford_alpaca.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Các mô hình ngôn ngữ nền tảng mở và hiệu quả. arXiv preprint arXiv:2302.13971.

Chaoqi Wang, Guodong Zhang, và Roger Grosse. 2020. Chọn vé trúng thưởng trước khi huấn luyện bằng cách bảo tồn gradient flow. arXiv preprint arXiv:2002.07376.

Chen Henry Wu, Saman Motamed, Shaunak Srivastava, và Fernando D De la Torre. 2022. Generative visual prompt: Thống nhất kiểm soát phân phối của các mô hình tạo sinh được huấn luyện trước. Proc. Adv. Neural Inf. Process. Syst., 35:22422–22437.

Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, và Alham Fikri Aji. 2023. Lamini-lm: Một đàn đa dạng của các mô hình chưng cất từ hướng dẫn quy mô lớn. CoRR, abs/2304.14402.

Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, và Danqi Chen. 2023. Sheared llama: Tăng tốc pre-training mô hình ngôn ngữ qua cắt tỉa có cấu trúc. arXiv preprint arXiv:2310.06694.

Haoran You, Zhanyi Sun, Huihong Shi, Zhongzhi Yu, Yang Zhao, Yongan Zhang, Chaojian Li, Baopu Li, và Yingyan Lin. 2023. Vitcod: Tăng tốc Vision transformer qua thuật toán chuyên dụng và thiết kế đồng tác accelerator. Trong Proc. IEEE Int. Sym. on High-Perf. Comp. Arch., trang 273–286. IEEE.

Fang Yu, Kun Huang, Meng Wang, Yuan Cheng, Wei Chu, và Li Cui. 2022a. Width & depth pruning cho vision transformer. Trong Proc. AAAI Conf. on Arti. Intel., tập 36, trang 3143–3151.

Xin Yu, Thiago Serra, Srikumar Ramalingam, và Shandian Zhe. 2022b. The combinatorial brain surgeon: Cắt tỉa trọng số hủy bỏ lẫn nhau trong mạng neural. Trong Proc. Int. Conf. Mach. Learn., trang 25668–25683.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. 2019. Hellaswag: Một máy có thể thực sự hoàn thành câu của bạn không? arXiv preprint arXiv:1905.07830.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: Một mô hình song ngữ mở được huấn luyện trước. arXiv preprint arXiv:2210.02414.

Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, và Tuo Zhao. 2022. Platon: Cắt tỉa mô hình transformer lớn với upper confidence bound của tầm quan trọng trọng số. Trong Proc. Int. Conf. Mach. Learn., trang 26809–26823.

Minxuan Zhou, Weihong Xu, Jaeyoung Kang, và Tajana Rosing. 2022. Transpim: Một gia tốc dựa trên bộ nhớ qua thiết kế đồng tác phần mềm-phần cứng cho transformer. Trong Proc. IEEE Int. Sym. on High-Perf. Comp. Arch., trang 1071–1085. IEEE.

## Phụ lục

### A Phụ thuộc Trọng số cho LLaMA

Ở đây, chúng tôi sử dụng kiến trúc LLaMA làm ví dụ để giải thích phụ thuộc trọng số. Chi tiết phụ thuộc được thể hiện trong Hình 5. Về mặt module Attention, khi chúng ta quyết định cắt tỉa một head cụ thể của trọng số trong lớp Query, việc các trọng số tương ứng với cùng chỉ số trong các lớp Key, Value và Out cũng được cắt tỉa là bắt buộc. Tương tự, đối với module Feed-Forward Network (FFN), khi cắt tỉa một kênh cụ thể của trọng số trong lớp Up, việc cắt tỉa trọng số với chỉ số phù hợp trong các lớp Gate và Down là cần thiết. Sự phối hợp tỉ mỉ này đảm bảo rằng việc cắt tỉa duy trì tính toàn vẹn cấu trúc và chức năng của mô hình. Theo (Ma et al., 2023) và (Fang et al., 2023), chúng tôi cắt tỉa head cho Attention và kênh cho FFN, tương ứng. Chi tiết phụ thuộc được thể hiện trong Hình 5.

### B Nghiên cứu Loại bỏ Thêm

**Cắt tỉa trên tập dữ liệu C4 được lấy mẫu 20k.** Chúng tôi cũng đánh giá LoRAPrune trên một tập dữ liệu nhỏ lấy mẫu ngẫu nhiên 20k dữ liệu từ tập dữ liệu C4. Như được trình bày trong Bảng 5, LoRAPrune vượt trội hơn cả LLM-Pruner và WANDA trên đa số tập dữ liệu lý luận zero-shot, do đó đảm bảo điểm trung bình cao nhất tổng thể. Cụ thể, LoRAPrune vượt hiệu suất của LLM-Pruner với biên độ 0.82% và 1.02%, tương ứng.

**Hiệu quả của trung bình động.** Chúng tôi xác minh lý do đằng sau trung bình động thông qua việc thiết lập các giá trị khác nhau cho λ. Những thí nghiệm này được tiến hành trên LLaMA-7b với tập dữ liệu C4 được lấy mẫu 20k. Kết quả thí nghiệm, như thể hiện trong Hình 6 (a), tiết lộ rằng khi λ tăng, kết quả cắt tỉa thể hiện sự giảm đáng kể trong perplexity. Hiệu ứng này đặc biệt rõ ràng khi λ = 0 trong đó việc cắt tỉa chỉ được xác định bởi tầm quan trọng của batch hiện tại, xác nhận hiệu quả của trung bình động.

**Tác động của các lần lặp.** Để đánh giá tác động của các lần lặp cắt tỉa đối với kết quả cắt tỉa, chúng tôi tiến hành thí nghiệm trên mô hình LLaMA-7b với các lần lặp khác nhau trên tập dữ liệu C4 được lấy mẫu 20k. Kết quả được thể hiện trong Hình 6 (b), cho thấy rằng các lần lặp quá mức có thể dẫn đến giảm hiệu suất zero-shot của mô hình, có thể do overfitting trên tập dữ liệu hiệu chuẩn. Hơn nữa, chúng tôi quan sát thấy rằng mô hình yêu cầu nhiều lần lặp hơn để lấy lại hiệu suất của nó khi cắt tỉa với nén cao (ví dụ: tỷ lệ=50%).

**LoRAPrune so với LLM-Pruner với gradient off-loading.** Chiến lược gradient off-loading có thể giảm nhẹ nhu cầu bộ nhớ của LLM-Pruner, như chuyển một số gradient sang bộ nhớ CPU. Tuy nhiên, chi phí truy cập bộ nhớ và overhead tính toán là đáng kể. Bảng 6 cho thấy LoRAPrune vượt trội hơn LLM-Pruner về hiệu quả, nhanh hơn 8.19× với CPU offloading và 2.75× nhanh hơn mà không có nó. Tốc độ này cho phép cắt tỉa lặp để chống lại sự sụt giảm hiệu suất do tính thưa thớt có cấu trúc.

**Kết hợp so với tách biệt.** Để chứng minh sự cần thiết của việc tích hợp cắt tỉa và fine-tuning, chúng tôi tiến hành thí nghiệm thực hiện tuần tự cắt tỉa theo sau bởi fine-tuning, cụ thể áp dụng cắt tỉa một lần cho mô hình LLaMA-7b và sau đó sử dụng fine-tuning LoRA để phục hồi hiệu suất của mô hình. Kết quả thí nghiệm được trình bày trong Bảng 7 cho thấy rằng cắt tỉa và fine-tuning kết hợp mang lại hiệu suất tốt hơn nhiều so với đối tác tách biệt, đặc biệt dưới tỷ lệ nén cao.

**Tần suất cắt tỉa.** Chúng tôi khám phá tác động của các tần suất cắt tỉa khác nhau, tức là bao nhiêu lần lặp fine-tuning trước khi cắt tỉa, đối với hiệu suất cuối cùng. Kết quả thí nghiệm, như thể hiện trong Bảng 8, cho thấy rằng tần suất mặc định của chúng tôi (tần suất=10) có được kết quả cắt tỉa tốt nhất. Ngoài ra, chúng tôi quan sát thấy rằng nếu cắt tỉa quá thường xuyên (tần suất=1), mô hình có thể không có đủ lần lặp để phục hồi thông qua fine-tuning, dẫn đến ước lượng tầm quan trọng không chính xác. Hơn nữa, fine-tuning quá mức giữa các lần lặp cắt tỉa (tần suất=20) dẫn đến overfitting trên dữ liệu hiệu chuẩn.

### C Kết quả Tạo sinh

Chúng tôi thể hiện khả năng tổng quát hóa của LoRAPrune bằng một số hướng dẫn bao gồm các tác vụ thông thức, dịch thuật và lập trình trong Bảng 9.

Hình 5: Phụ thuộc trọng số trong (a) lớp Attention, (b) lớp FFN.

Bảng 5: Hiệu suất zero-shot của các mô hình LLaMA được nén fine-tuned trên tập dữ liệu C4 được lấy mẫu 20k. Độ chính xác trung bình được tính toán trong bảy tập dữ liệu phân loại. In đậm / biểu thị hiệu suất tốt nhất ở cùng tỷ lệ nén. ⋆ biểu thị kết quả thu được bởi việc tái tạo của chúng tôi.

| Tỷ lệ Cắt tỉa | Phương pháp | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA | Trung bình ↑ |
|---------------|-------------|-------|------|-----------|------------|-------|-------|------|-------------|
| Tỷ lệ = 0% | LLaMA-7B (Touvron et al., 2023) | 73.18 | 78.35 | 72.99 | 67.01 | 67.45 | 41.38 | 42.40 | 63.25 |
| Tỷ lệ = 20% | Magnitude⋆ | 61.89 | 70.81 | 58.34 | 56.87 | 54.87 | 34.02 | 38.40 | 53.59 |
| | WANDA⋆(Sun et al., 2023) | 65.75 | 74.70 | 64.52 | 59.35 | 60.65 | 36.26 | 39.40 | 57.23 |
| | LLM-Pruner (Ma et al., 2023) | 64.62 | 77.20 | 68.80 | 63.14 | 64.31 | 36.77 | 39.80 | 59.23 |
| | LoRAPrune-8bit (Ours) | 65.37 | 76.65 | 69.41 | 63.78 | 65.45 | 36.12 | 39.50 | 59.46 |
| | LoRAPrune (Ours) | 65.62 | 79.31 | 70.00 | 62.76 | 65.87 | 37.69 | 39.14 | 60.05 |
| Tỷ lệ = 50% | Magnitude⋆ | 47.40 | 54.36 | 33.49 | 53.10 | 37.88 | 26.60 | 30.12 | 40.42 |
| | WANDA⋆(Sun et al., 2023) | 50.90 | 57.38 | 38.12 | 55.98 | 42.68 | 34.20 | 38.78 | 45.43 |
| | LLM-Pruner (Ma et al., 2023) | 60.28 | 69.31 | 47.06 | 53.43 | 45.96 | 29.18 | 35.60 | 48.69 |
| | LoRAPrune-8bit (Ours) | 61.43 | 70.88 | 47.65 | 55.12 | 45.78 | 30.50 | 35.62 | 49.56 |
| | LoRAPrune (Ours) | 61.88 | 71.53 | 47.86 | 55.01 | 45.13 | 31.62 | 34.98 | 49.71 |

Hình 6: Nghiên cứu loại bỏ thêm cho các siêu tham số cắt tỉa: (a) giá trị λ trong trung bình động, (b) lần lặp cắt tỉa.

Bảng 6: So sánh hiệu quả giữa LoRAPrune và LLM-Pruner với CPU off-loading.

| Phương pháp | Throughput (s/iter) | Bộ nhớ GPU (GB) | FLOPs (G) | Tổng thời gian (h) | Thời gian cắt tỉa (h) | Thời gian fine-tuning (h) |
|-------------|-------------------|----------------|-----------|-------------------|---------------------|----------------------|
| LLM-Pruner | 38.87 | 38.6 | 20298 | 5.3 | 3.5 | 1.8 |
| LLM-Pruner + CPU offloading | 115.67 | 19.5 | 20298 | 25.8 | 24 | 1.8 |
| LoRAPrune (Ours) | 14.13 | 18.3 | 12881 | 2.0 | 0.2 | 1.8 |

Bảng 7: Hiệu ứng của cắt tỉa và fine-tuning kết hợp. "Trung bình" đại diện cho hiệu suất trung bình trên bảy tập dữ liệu phân loại.

| Phương pháp | WikiText2 ↓ | PTB ↓ | Trung bình ↑ |
|-------------|-------------|-------|-------------|
| Tỷ lệ=20% | | | |
| Kết hợp | 12.93 | 22.52 | 60.05 |
| Tách biệt | 14.51 | 24.30 | 57.18 |
| Tỷ lệ=50% | | | |
| Kết hợp | 18.37 | 28.68 | 49.71 |
| Tách biệt | 21.78 | 40.39 | 47.56 |

Bảng 8: Kết quả dưới các tần suất cắt tỉa khác nhau. "Trung bình" biểu thị hiệu suất trung bình trên bảy tập dữ liệu phân loại.

| Tần suất | WikiText2 ↓ | PTB ↓ | Trung bình ↑ |
|----------|-------------|-------|-------------|
| 1 | 18.57 | 29.02 | 49.37 |
| 5 | 18.63 | 28.73 | 49.16 |
| 10 | 18.21 | 28.68 | 49.59 |
| 20 | 18.47 | 28.70 | 49.71 |

Bảng 9: Ví dụ được tạo ra từ các mô hình đã cắt tỉa

| Mô hình | Câu được tạo ra |
|---------|----------------|
| LLaMA-7b (tỷ lệ=0%) | Hướng dẫn: Hãy kể cho tôi về alpaca. Phản hồi: Alpaca là thành viên của họ lạc đà và có nguồn gốc từ dãy núi Andes của Nam Mỹ. Chúng được biết đến với bộ lông mềm mại, sang trọng, được sử dụng để làm quần áo, chăn và các vật dụng khác. Alpaca là động vật bầy đàn và sống thành nhóm lên đến 20 cá thể. Chúng thường hiền lành và dễ chăm sóc, khiến chúng trở nên phổ biến như thú cưng và gia súc. Alpaca có thể sống đến 20 năm trong điều kiện nuôi nhốt, và bộ lông của chúng có thể được cắt lên đến ba lần một năm. Hướng dẫn: Dịch câu 'I have no mouth but I must scream' sang tiếng Tây Ban Nha. Phản hồi: I no tengo boca pero tengo que gritar. Hướng dẫn: Viết một chương trình Python in 10 số Fibonacci đầu tiên. Phản hồi: def fibonacci(n): if n == 0: return 1 if n == 1: return 1 else: return fibonacci(n - 1) + fibonacci(n - 2) for n in range(1, 10): print(fibonacci(n)) |
| LLaMA-7b (tỷ lệ=20%) | Hướng dẫn: Hãy kể cho tôi về alpaca. Alpaca là một loài lạc đà có nguồn gốc từ Nam Mỹ. Chúng được biết đến với bộ lông mềm mại, sang trọng, được sử dụng để làm quần áo, chăn và các sản phẩm dệt may khác. Alpaca cũng được sử dụng vì thịt của chúng, được coi là một món ngon ở một số nơi trên thế giới. Alpaca là động vật xã hội sống thành đàn và được biết đến với tính khí hiền lành. Chúng cũng được biết đến với trí thông minh và khả năng được huấn luyện cho nhiều nhiệm vụ khác nhau, như kéo xe hoặc biểu diễn các trò để giải trí. Hướng dẫn: Dịch câu 'I have no mouth but I must scream' sang tiếng Tây Ban Nha. Phản hồi: Yo no tengo la boca pero tengo que gritar. Hướng dẫn: Viết một chương trình Python in 10 số Fibonacci đầu tiên. Phản hồi: def fibonacci(n): if n == 0 or n == 1: return 1 else: return fibonacci(n - 1) + fibonacci(n - 2) for n in range(1, 10): print(fibonacci(n)) |
