# 2210.06384.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2210.06384.pdf
# File size: 333821 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
GMPF: Well-Tuned Gradual Magnitude Pruning Can Outperform Most
BERT-Pruning Methods
Eldar Kurtic1and Dan Alistarh1,2
1Institute of Science and Technology Austria
2Neural Magic Inc.
Abstract
We revisit the performance of the classic grad-
ual magnitude pruning (GMP) baseline for
large language models, focusing on the clas-
sic BERT benchmark on various popular tasks.
Despite existing evidence in the literature that
GMP performs poorly, we show that a simple
and general variant, which we call GMP F, can
match and sometimes outperform more com-
plex state-of-the-art methods. Our results pro-
vide a simple yet strong baseline for future
work, highlight the importance of parameter
tuning for baselines, and even improve the per-
formance of the state-of-the-art second-order
pruning method in this setting.
1 Introduction
The massive recent growth of the computational
cost of accurate deep learning models, in particular
large language models (LLMs), has motivated the
development of several advanced model compres-
sion techniques (Hoeﬂer et al., 2021; Gholami et al.,
2021), encompassing unstructured and structured
pruning, quantization, and knowledge distillation.
In this paper, we focus on the unstructured pruning,
for which we follow the standard pipeline. Such
models are ﬁrst pre-trained on a large upstream cor-
pus of unlabelled text. Then, they are ﬁne-tuned in
a supervised manner on a smaller downstream task,
such as question-answering or text classiﬁcation.
In the context of compression, this pipeline led to
two paradigms: 1) upstream pruning , followed by
ﬁne-tuning of the remaining weights on a down-
stream task, and 2) downstream pruning , pruning
and ﬁne-tuning directly on the downstream task.
A tempting baseline approach in most settings
isgradual magnitude pruning (GMP) (Hagiwara,
1994; Zhu and Gupta, 2017), that is, periodically
removing the smallest fraction of weights dur-
ing training, possibly interspersed with ﬁne-tuning
steps designed to recover accuracy. GMP has been
Corresponding author: eldar.kurtic@ist.ac.at.
90 9780.0
68.0
60.088.5F1 scoreSQuADv1.1
90 97Sparsity (%)81.0
75.0
69.084.5Matched accuracyMNLI
Lottery Ticket
GMPMvPMvP
Prune OFAGMP (ours)
oBERTFigure 1: Performance of state-of-the-art unstructured
pruning methods relative to the dense BERT BASEmodel
at high sparsities and two tasks, SQuADv1.1 and
MNLI.
shown to be an extremely strong baseline in the
context of computer vision (Gale et al., 2019; Hoe-
ﬂer et al., 2021). However, the literature on pruning
LLMs, and in particular BERT models (Sanh et al.,
2020; Chen et al., 2020; Zafrir et al., 2021), clearly
suggests that GMP does not perform well.
Contribution. In this paper, we re-examine this
conclusion and investigate whether GMP can be a
competitive baseline, once carefully tuned. Specif-
ically, we show that a well tuned variant which
we call GMP F, can produce highly accurate and
sparse language models in both upstream and down-
stream pruning regimes, matching or even outper-
forming more complex methods. We explore ef-
fects of the crucial parameters for gradual pruning,
and provide simple and intuitive guidelines on how
to integrate them in a principled manner.
1arXiv:2210.06384v3  [cs.CL]  8 Dec 2022

--- PAGE 2 ---
Our results are summarized in Figure 1, which
presents performance of state-of-the-art unstruc-
tured pruning techniques on two benchmarks.
Speciﬁcally, we compare GMP Fwith the Lot-
tery Ticket approach (Chen et al., 2020), Move-
ment Pruning (MvP) (Sanh et al., 2020) (as well
as its GMP baseline GMPMvP), upstream Prune
OFA (Zafrir et al., 2021), as well as the recently-
proposed second-order pruning oBERT (Kurtic
et al., 2022). We observe that: 1) for both bench-
marks, GMP Fis only second to the more complex
oBERT method; 2) GMP Fin fact outperforms the
highly competitive Prune OFA and MvP methods;
and 3) GMP Foutperforms both Lottery Tickets
and GMPMvPby extremely wide margins.
Prior Work. Following the vast BERT-pruning
literature, we focus on the unstructured pruning of
theBERT BASEmodel (Devlin et al., 2019). As pre-
viously noted, upstream and downstream pruning
paradigms exist, and methods are usually devel-
oped and specialized for only one of the two. For
example, Movement Pruning (MvP) (Sanh et al.,
2020; Lagunas et al., 2021) for downstream prun-
ing and Prune Once for All (Prune OFA) (Zafrir
et al., 2021) for upstream pruning. Simplicity and
generality of the GMP makes it suitable for both
paradigms, without any regime-speciﬁc modiﬁca-
tions. New and more advanced pruning techniques,
which are, contrary to GMP, able to leverage gradi-
ents (Sanh et al., 2020; Lagunas et al., 2021), loss
curvature (Kurtic et al., 2022), compute-intensive
pre-training setup (Zafrir et al., 2021) are built on
the premise that the simple magnitude-based GMP
method falters when applied to BERT-pruning. In
this work, contrary to what is currently available in
the literature, we present empirical evidence that
GMP, when tuned carefully, can produce very accu-
rate sparse models which are competitive or even
better than most state-of-the-art pruning techniques
across both regimes (upstream and downstream).
As can be seen from Figure 1 and our later results,
we massively improve upon existing GMP-based
pruning baselines, in some cases by even more than
20 accuracy points .
2 Competitive Gradual Magnitude
Pruning (GMP F)
Experimental setup. We focus our attention on
the standard BERT BASEmodel, composed of embed-
ding and encoder layers, which has approximately
110M parameters. All methods focus on pruning
0 2 4 6 8 10
Epoch1.00
0.01Learning rate1e4
07090
Sparsity (%)Figure 2: Learning rate and sparsity schedules for the
proposed gradual pruning framework.
among approximately 85M weights of encoder lay-
ers and report sparsities with respect to that number.
We evaluate models on the validation split of the
respective dataset, and to improve conﬁdence in
the obtained results we perform multiple runs with
different seeds and report mean performance.
2.1 Downstream pruning
Following the literature, we consider three pop-
ular tasks: question-answering SQuADv1.1 (Ra-
jpurkar et al., 2016), recognition of textual entail-
ment MNLI (Williams et al., 2017), and duplicate
question detection QQP (Iyer et al., 2017). Now,
we reﬂect upon the most important constituents of
the gradual pruning framework that enabled us to
attain massive improvements.
Sparsity schedule. In all of our gradual runs,
there is no pruning during the ﬁrst two and the
last two epochs. The former ﬁne-tunes the pre-
trained model, and the latter ﬁne-tunes the sparse
model with the ﬁxed mask. In between the two,
GMPFfollows the cubic sparsity scheduler (Zhu
and Gupta, 2017) and prunes weights with the fre-
quency of ten times per epoch. Motivated by the
fact that BERT BASEis heavily overparametrized for
downstream tasks, we deviate from the standard
cubic schedule by introducing a large ﬁrst pruning
step. This showed to be of a crucial importance
when pruning the model to high target sparsities
(e.g. 97%) as it leaves more time to recover from
later pruning steps which are much more difﬁcult.
In Table 8 we report results from an ablation study
with respect to the size of the initial step. For con-
venience, we visualize the sparsity scheduler in
Figure 2. Our preliminary experiments showed
similar performance between uniform and global
sparsity distributions, so we use the former.
Learning rate schedule. Our goal is to provide
a simple baseline setup that works well across
wide range of datasets without any additional task-
dependent tuning. Currently, papers either report
best results following an extensive hyperparameter
2

--- PAGE 3 ---
0 250050T = 1.0
0 250050T = 2.0
0 2500.02.5T = 5.5
0 2500100
0 250050
0 2500.02.5
Teacher's output logitsProbability (%)Figure 3: Teacher’s output distribution at commonly
used temperatures T2f1:0;2:0gand the proposed
T= 5:5.
search for each task, e.g. Zafrir et al. (2021), or they
make use of carefully crafted schedulers for each
setup independently which may include warm-up
phases with and without rewinds (Sanh et al., 2020;
Kurtic et al., 2022). This may lead to high special-
ization to the target task/model, which is undesir-
able in practice and makes it hard to distinguish
beneﬁts from the pruning technique itself. We pro-
pose to simply replicate the standard 2-epoch ﬁne-
tuning schedule (Devlin et al., 2019) by a certain
factor and intertwine it with pruning steps. For a
fair comparison with Sanh et al. (2020) we replicate
it by a factor of 5, reproducing their 10-epoch setup.
And for a fair comparison with Chen et al. (2020)
we replicate it by a factor of 15, reproducing their
30-epoch setup. For convenience, we visualize the
learning rate schedule in Figure 2. In appendix F,
we describe results with other schedulers that didn’t
work.
Knowledge Distillation (KD) Hardness. We
leverage KD (Hinton et al., 2015) of outputs from a
ﬁne-tuned dense teacher. KD is a standard practice
when pruning, e.g. (Sanh et al., 2020; Zafrir et al.,
2021; Xu et al., 2021). The loss function is formu-
lated as a linear combination of the standard loss
associated with the speciﬁc task (e.g. cross-entropy
for classiﬁcationLCE) and the Kullback-Leibler
divergence (LKL) between output distributions of
the dense (teacher) model and the sparse (student)
model in the form: L= (1 h)LCE+hLKL. The
ratio between the two is controlled with the hard-
ness hyperparameter h. To determine its optimal
value at high sparsities we run an ablation study
reported in Table 10, and adopt the hardness h= 1.
Knowledge Distillation Temperature. The
temperature Tis an additional KD-hyperparameter
that requires proper tuning, as it controls the
“softness” of the output distribution. In the pruningTable 1: Downstream pruning comparison of
GMPFwith other GMP-based baselines.
Method Spars. Ep.SQuAD MNLI QQP
F1 m-acc acc
BERT BASE 0% 88.5 84.5 91.1
GMPMvP 90% 1080.1 78.3 79.8
GMPF 86.7 81.9 90.6
GMPMvP 97% 1059.6 69.4 72.4
GMPF 81.3 79.1 89.7
GMPLTH 90% 3068.0 75.0 90.0
GMPF 87.9 82.7 90.8
GMPF 97% 30 85.4 80.9 90.6
literature, it is standard to use the “stronger” T= 1
orT= 2 values (Xu et al., 2021; Zafrir et al.,
2021; Sanh et al., 2020; Lagunas et al., 2021;
Kurtic et al., 2022); we revisit this by visualizing
teacher’s output distributions to get an insight into
what the sparse student is learning. In Figure 3,
we visualize generated distributions for randomly
picked samples from the SQuADv1.1 task softened
with three values of the temperature. As can be
seen, teacher’s high conﬁdence in predicting the
correct class at the commonly used temperatures
T2f1:0;2:0gmakes the knowledge distillation
almost obsolete. Motivated by this observation, we
run an ablation study for many higher temperatures
and report a fraction of results in Table 11. Given
the results, we adopt the temperature T= 5:5.
2.1.1 GMP Fvs. other GMP-based baselines
Due to space constraints, we aggregate all the pre-
viously analyzed improvements in a downstream
pruning recipe and present it in detail in Ap-
pendix B. We compare our optimized GMP Fwith
other GMP results reported in the pruning literature.
For a fair comparison, we consider both setups, 10
and 30-epoch. In the 10-epoch setup, we compare
against the GMP baselines reported in Sanh et al.
(2020) and refer to them as GMP MvP. In the 30-
epoch setup, we compare against the best reported
results in Chen et al. (2020), obtained either via
GMP or via Lottery Ticket (LTH) approach, and
refer to them as GMP LTH. As can be seen from
the Table 1, our GMP Fremarkably outperforms all
other results; in some cases the improvements are
more than 20 points !
2.1.2 GMP Fvs. advanced pruning
techniques
Now, we wish to compare our GMP Fwith methods
that rely on higher-order information to make prun-
3

--- PAGE 4 ---
Table 2: Downstream pruning comparison of
GMPFwith advanced pruning techniques.
Method Spars. Ep.SQuAD MNLI QQP
F1 m-acc acc
BERT BASE 0% 88.5 84.5 91.1
GMPF90% 1086.7 81.9 90.6
MvP 84.9 81.2 90.2
GMPF97% 1081.3 79.1 89.7
MvP 82.3 79.5 89.1
GMPF
90% 3087.9 82.7 90.8
oBERT 88.3 83.8 91.4
oBERT F 88.6
GMPF
97% 3085.4 80.9 90.6
oBERT 86.0 81.8 90.9
oBERT F 86.6
ing decisions, like gradients in MvP (Sanh et al.,
2020) and the loss curvature in oBERT (Kurtic
et al., 2022). Both of these impose higher com-
putational overhead compared to the magnitude-
based pruning, but we still put our results in the
context with respect to theirs to fully grasp the
scope of improvements introduced by careful opti-
mizations of GMP. As can be seen from results in
Table 2, GMP Fis able to improve upon the perfor-
mance of Movement Pruning in 4 out of 6 analyzed
conﬁgurations, but unfortunately can’t match the
performance of the oBERT method. In addition to
these comparisons, we make use of the open-source
implementation of oBERT, current state-of-the-art
BERT-pruning method, and run it with optimized
hyperparameters from GMP Fon the SQuADv1.1
task. We refer to these results as oBERT F. As can
be seen from the Table 2, even the very competi-
tive oBERT results beneﬁt from the GMP Fsetup.
For all GMP Fruns, we report mean performance
across three runs with different seeds, and addi-
tional metrics in Tables 5 and 6.
2.2 Upstream pruning
To validate the optimized GMP Fsetup introduced
in the previous section, we apply it now to the
pre-training phase of LLMs. This is a two-stage
process. In the ﬁrst stage, the BERT BASEmodel is
pruned during pre-training and then, in the sec-
ond stage, the remaining weights are ﬁne-tuned
with the ﬁxed mask on a speciﬁc downstream task
to evaluate performance. Given the high costs of
experimenting in the pre-training phase, we use
the dense teacher open-sourced by Kurtic et al.
(2022). Due to the space constraints, we summa-
rize all hyperparameters in an upstream pruningTable 3: Upstream pruning comparison of GMP Fwith
other GMP-based baselines and more advanced prun-
ing techniques.
Method SparsitySQuAD MNLI QQP
F1 m-acc acc
BERT BASE 0% 88.5 84.5 91.1
GMPPrune OFA85% 86.2 82.5 90.9
Lottery Ticket
90%68.0 75.0 90.0
Prune OFA 87.3 81.5 90.9
GMPF 88.2 83.2 90.8
oBERT 88.5 83.4 91.0
GMPF97%84.7 80.3 89.8
oBERT 84.9 80.9 90.3
recipe and present it in detail in Appendix C. In
Table 3 we present results obtained in this setup
and compare against other methods that are uti-
lizing the same approach. More speciﬁcally, we
compare against the Lottery Ticket (Chen et al.,
2020), Prune OFA (Zafrir et al., 2021), and The
Optimal BERT Surgeon (oBERT) (Kurtic et al.,
2022). In addition to this, we report the GMP base-
lines obtained in the Prune OFA work and refer
to them as GMP Prune OFA . As can be seen from
the Table 3, the GMP Fsigniﬁcantly outperforms
GMP Prune OFA , Lottery Tickets and even the Prune
OFA, and comes really close to the performance
of oBERT. For all GMP Fruns, we report mean
performance across four runs with different seeds.
These results conﬁrm ﬁndings from the previous
section and establish the GMP Fas an extremely
competitive baseline in all regimes.
3 Conclusion
In this work, we presented a set of updates to the
standard gradual pruning setup for BERT models
which enabled us to achieve very competitive re-
sults with the simple magnitude pruner. These
results outperformed, by signiﬁcant margins, all
magnitude-based results currently available in the
pruning literature which have been used as base-
lines for development and benchmarking of the
new and more advanced pruning techniques. We
hope that these new baselines will help the com-
munity to start off from a competitive set of results
when compressing large language models. More-
over, our GMP Fhas even outperformed some re-
sults obtained with more advanced and computa-
tionally heavier pruning techniques. At this point,
we would like to strongly emphasize that these
results should not be interpreted as evidence that
magnitude pruning is better than other more ad-
4

--- PAGE 5 ---
vanced methods. Rather, they should be interpreted
as evidence that their current results could signif-
icantly beneﬁt from updates of the gradual setup
presented on the GMP Fuse-case. To support this
claim, we ran the state-of-the-art oBERT pruner
with the GMP Fsetup and managed to improve its
results by non-trivial margins.
4 Limitations
As any academic study, our work is not without
its limitations. Following the literature, our ex-
tensive empirical studies were conducted only on
the standard BERT BASEmodel, giving us opportu-
nity to compare against a vast amount of different
pruning techniques. Throughout the literature, this
model emerged as a consistent benchmark for un-
structured pruning methods. However, the current
results don’t directly imply that our ﬁndings will
be generally applicable to other language models
as well. To partially ﬁll in this uncertainty gap,
we conduct a few experiments on the three times
larger BERT LARGE model and report results in the
Appendix A. Another limitation which we aim to
remove in future work is the focus on ﬁne-grained
unstructured sparsity type, and explore other vari-
ants such as semi-structured and structured prun-
ing.
References
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia
Liu, Yang Zhang, Zhangyang Wang, and Michael
Carbin. 2020. The lottery ticket hypothesis for pre-
trained bert networks. Advances in neural informa-
tion processing systems , 33:15834–15846.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In North American Chapter of the Associ-
ation for Computational Linguistics (NAACL) .
Trevor Gale, Erich Elsen, and Sara Hooker. 2019. The
state of sparsity in deep neural networks. In Interna-
tional Conference on Machine Learning (ICML) .
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei
Yao, Michael W Mahoney, and Kurt Keutzer.
2021. A survey of quantization methods for ef-
ﬁcient neural network inference. arXiv preprint
arXiv:2103.13630 .
Masafumi Hagiwara. 1994. A simple and effective
method for removal of hidden units and weights.
Neurocomputing , 6(2):207 – 218. Backpropagation,
Part IV .Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 , 2(7).
Torsten Hoeﬂer, Dan Alistarh, Tal Ben-Nun, Nikoli
Dryden, and Alexandra Peste. 2021. Sparsity in
deep learning: Pruning and growth for efﬁcient infer-
ence and training in neural networks. arXiv preprint
arXiv:2102.00554 .
Shankar Iyer, Nikhil Dandekar, Kornél Csernai, et al.
2017. First quora dataset release: Question pairs.
data. quora. com .
Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias
Frantar, Mark Kurtz, Benjamin Fineran, Michael
Goin, and Dan Alistarh. 2022. The optimal bert
surgeon: Scalable and accurate second-order prun-
ing for large language models. arXiv preprint
arXiv:2203.07259 .
Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexan-
der Matveev, John Carr, Michael Goin, William
Leiserson, Sage Moore, Bill Nell, Nir Shavit, and
Dan Alistarh. 2020. Inducing and exploiting ac-
tivation sparsity for fast inference on deep neural
networks. In International Conference on Machine
Learning (ICML) .
François Lagunas, Ella Charlaix, Victor Sanh, and
Alexander Rush. 2021. Block pruning for faster
transformers. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language
Processing , pages 10619–10629. Association for
Computational Linguistics.
Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien Plu,
Lewis Tunstall, Joe Davison, Mario Šaško, Gun-
jan Chhablani, Bhavitvya Malik, Simon Brandeis,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
Patry, Angelina McMillan-Major, Philipp Schmid,
Sylvain Gugger, Clément Delangue, Théo Matus-
sière, Lysandre Debut, Stas Bekman, Pierric Cis-
tac, Thibault Goehringer, Victor Mustar, François
Lagunas, Alexander Rush, and Thomas Wolf. 2021.
Datasets: A community library for natural language
processing. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing: System Demonstrations , pages 175–184, On-
line and Punta Cana, Dominican Republic. Associ-
ation for Computational Linguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP) .
Victor Sanh, Thomas Wolf, and Alexander Rush.
2020. Movement pruning: Adaptive sparsity by ﬁne-
tuning. Advances in Neural Information Processing
Systems , 33:20378–20389.
5

--- PAGE 6 ---
Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv
preprint arXiv:1704.05426 .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing:
System Demonstrations , pages 38–45, Online. Asso-
ciation for Computational Linguistics.
Dongkuan Xu, Ian EH Yen, Jinxi Zhao, and Zhibin
Xiao. 2021. Rethinking network pruning–under the
pre-train and ﬁne-tune paradigm. arXiv preprint
arXiv:2104.08682 .
Oﬁr Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen,
and Moshe Wasserblat. 2021. Prune once for all:
Sparse pre-trained language models. arXiv preprint
arXiv:2111.05754 .
Michael Zhu and Suyog Gupta. 2017. To prune, or not
to prune: exploring the efﬁcacy of pruning for model
compression. arXiv preprint arXiv:1710.01878 .
A Additional models
All of our experiments in the paper focus on the
BERT BASEmodel as it is the standard benchmark
used in the pruning literature and we are able to
compare results with a vast amount of other tech-
niques. To verify that our proposed GMP Fsetup
doesn’t pertain only to the BERT BASEmodel, in the
Table 4 we present results on the three times larger
BERT LARGE model. As a proof of concept, we run
our downstream gradual pruning setup crafted for
theBERT BASEmodel without any hyper-parameter
tuning.
Table 4: Downstream pruning results when pruning the
BERT LARGE model with the GMP Fand gradual setup
crafted for the BERT BASEmodel.
Method SparsitySQuAD
F1 EM
BERT LARGE 0% 91.22 84.45
GMPF 90% 90.12 83.21
GMPF 97% 87.93 80.50B Downstream pruning recipe
All of our implementations are built on top of Hug-
gingFace’s Transformers1(Wolf et al., 2020) and
Datasets2(Lhoest et al., 2021) libraries, and Neu-
ralMagic’s SparseML3(Kurtz et al., 2020) library
for model compression, and will be open-sourced
to community along with our sparse models.
As our goal is to provide a simple and unique
gradual pruning setup, all of our downstream runs
(for all datasets) are using the same set of hyperpa-
rameters. The ones used to obtain results reported
in Tables 1, 2, 5, and 6 are as follows:
•learning-rate : recurring 2-epoch scheduler
(visualized in Figure 2) with the initial value
of1e-4 , and the ﬁnal value of 1e-6 ,
•number-of-epochs : 10 or 30 epochs, de-
pending on the methods we compare against,
•sparsity : cubic scheduler with the initial
pruning step of 70% sparsity (visualized in
Figure 2),
•pruning : prune frequency of ten times per
epoch, except during the ﬁrst and last 2-
epochs when only ﬁne-tuning happens and
masks are ﬁxed,
•student-initialization : standard
BERT BASE(bert-base-uncased4),
•knowledge-distillation (KD) : (hardness,
temperature) = (1.0, 5.5),
•KD-teachers : standard BERT BASEﬁne-tuned
on the corresponding task,
•weight-decay : 0.0,
•all other hyper-parameters are set to the stan-
dard default values, e.g. Sanh et al. (2020):
–SQuADv1.1: batch-size=16 ,
max-sequence-length=384 ,
doc-stride=128 ,
–MNLI and QQP: batch-size=32 ,
max-sequence-length=128 .
1https://github.com/huggingface/transformers
2https://github.com/huggingface/datasets
3https://github.com/neuralmagic/sparseml
4https://huggingface.co/bert-base-uncased
6

--- PAGE 7 ---
C Upstream pruning recipe
For a fair comparison with Zafrir et al. (2021) and
Kurtic et al. (2022), we adopt the same gradual
setup for pruning and ﬁne-tuning, but apply our
speciﬁc GMP Fupdates. The entire process is car-
ried out in two stages. The ﬁrst stage prunes the
BERT BASEmodel at upstream datasets, BookCorpus
and English Wikipedia. Both datasets are available
via Lhoest et al. (2021). The upstream pruning
recipe can be summarized as follows:
•learning-rate : recurring 0.5-epoch sched-
uler with the initial learning rate value of 5e-4 ,
and the ﬁnal value of 5e-6 ,
•number-of-epochs : 3,
•sparsity : cubic scheduler with the initial
pruning step of 70% sparsity,
•pruning : prune frequency of hundred times
per epoch, except during the last epoch when
only ﬁne-tuning happens and masks are ﬁxed,
•knowledge-distillation (KD) : (hardness,
temperature) = (1.0, 5.5),
•KD teacher and student initialization :
BERT BASEprepared and open-sourced by Kur-
tic et al. (2022),
•weight-decay : 0.01,
•batch-size : 256,
•max-sequence-length : 512.
The second stage makes use of this upstream
pruned model and ﬁne-tunes it on a speciﬁc down-
stream task (SQuADv1.1, MNLI, QQP) for 8-
epochs with ﬁxed masks. All task-speciﬁc hy-
perparameters (batch-size, max-sequence-length,
doc-stride, weight-decay) are the same as in Ap-
pendix B, and the remaining ones are as follows:
•learning-rate : linear decay with initial
value of 1.5e-5 ,
•knowledge-distillation : (hardness, tem-
perature) = (1.0, 5.5),
•KD-teachers : standard BERT BASEﬁne-tuned
on the corresponding task.Table 5: Downstream pruning comparison of
GMPFwith other GMP-based baselines. We report
complementary evaluation metrics for results in
Table 1.
Method Spars. Ep.SQuAD MNLI QQP
EM mm-acc F1
BERT BASE 0% 81.4 85.0 88.0
GMPMvP 90% 1070.2 79.3 65.0
GMPF 78.7 82.1 87.4
GMPMvP 97% 1045.5 70.6 57.8
GMPF 71.3 79.6 86.1
GMPLTH 90% 30- - -
GMPF 80.4 83.2 87.7
GMPF 97% 30 77.1 81.2 87.3
Table 6: Downstream pruning comparison of
GMPFwith advanced pruning techniques. We report
complementary evaluation metrics for results in
Table 2.
Method Spars. Ep.SQuAD MNLI QQP
EM mm-acc F1
BERT BASE 0% 81.4 85.0 88.0
GMPF90% 1078.7 82.1 87.4
MvP 76.6 81.8 86.8
GMPF97% 1071.3 79.6 86.1
MvP 72.7 80.1 85.5
GMPF
90% 3080.4 83.2 87.7
oBERT 81.1 84.4 88.3
oBERT F 88.6
GMPF
97% 3077.1 81.2 87.3
oBERT 78.1 82.0 87.7
oBERT F 78.8
Table 7: Upstream pruning comparison of GMP Fwith
other GMP-based baselines and advanced pruning tech-
niques. We report complementary evaluation metrics
for results in Table 3.
Method SparsitySQuAD MNLI QQP
EM mm-acc F1
BERT BASE 0% 81.4 85.0 88.0
GMPPrune OFA85% 78.0 83.1 87.7
Lottery Ticket
90%- - -
Prune OFA 79.8 82.4 87.7
GMPF 81.1 83.8 87.6
oBERT 81.4 83.8 87.8
GMPF97%76.3 81.0 86.5
oBERT 76.9 81.1 87.0
D Additional metrics
Due to space constraints, for corresponding runs
in Tables 1, 2, and 3, we present additional perfor-
7

--- PAGE 8 ---
Table 8: Initial sparsity step ablation study on the
SQuADv1.1 dataset.
Initial sparsity
(%)F1 score at
90% 97%
0 85.2 77.2
30 85.5 77.8
50 85.8 78.5
70 85.8 79.1
Table 9: Initial learning rate ablation study on the
MNLI dataset.
Initial learning rateAccuracy at
90% 97%
3e-5 80.8 76.3
5e-5 81.4 77.8
8e-5 81.9 78.6
1e-4 81.6 79.3
mance metrics in Tables 5, 6, and 7.
E Ablation studies
In Tables 8, 9, 10, 11 we present a subset of results
from ablation studies conducted to ﬁnd the optimal
values of hyperparameters for the GMP Fgradual
pruning setup. These results illustrate the general
trend of effects caused by varying one hyperpa-
rameter. Therefore, they don’t cover all possible
scenarios (i.e. higher-order effects when multiple
hyperparameters are updated together), but such
studies are computationally too expensive and we
don’t conduct them.
F Learning rate schedulers we tried, but
didn’t work
The schedulers we tried but didn’t work: 1) linearly
decaying learning rate, 2) the default ﬁne-tuning
learning rates (3e-5 for SQuADv1.1 and 2e-5 for
Table 10: Knowledge Distillation (KD) hardness abla-
tion study on the SQuADv1.1 dataset.
Knowledge distillation
hardnessF1 score at
90% 97%
0.6 84.6 78.4
0.8 85.9 80.1
0.9 86.2 80.7
1.0 86.7 81.0Table 11: Knowledge Distillation (KD) temperature ab-
lation study on the SQuADv1.1 dataset.
Knowledge distillation
temperatureF1 score at
90% 97%
1.0 84.7 77.3
2.0 85.8 79.0
5.5 86.7 81.0
8.5 86.4 80.9
MNLI and QQP), 3) learning rates with the warm-
up phase. In the preliminary experiments, we have
noticed that 1) and 2) have problems in recovering
from the pruning steps at higher sparsities. The
former one has extremely small learning rate val-
ues during the last few epochs when the model is
pruned to high sparsities. The latter one continu-
ously fails to recover properly even at moderate
sparsity targets, which is why we run a sweep over
a range of initial learning rate values. Given the
results in Table 9, we decided to proceed with the
1e-4 as it helped to recover signiﬁcantly at high
sparsities. We haven’t observed any beneﬁts from
the warmup phase, which is why we have decided
not to use it as it adds an additional hyperparameter
to tune.
8
