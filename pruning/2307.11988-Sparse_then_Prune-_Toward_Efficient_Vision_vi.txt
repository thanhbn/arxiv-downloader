# 2307.11988.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2307.11988.pdf
# File size: 669524 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Sparse then Prune: Hướng tới Vision Transformers
Hiệu quả
Yogi Prasetyoa, Novanto Yudistiraa, Agus Wahyu Widodoa
aKhoa Tin học, Khoa Công nghệ Thông tin, Đại học Brawijaya, Jalan
Veteran 8, Malang, 65145, Malang, Indonesia
Tóm tắt
Kiến trúc Vision Transformer là một mô hình deep learning được lấy cảm hứng
từ sự thành công của mô hình Transformer trong Xử lý Ngôn ngữ Tự nhiên.
Tuy nhiên, cơ chế self-attention, số lượng lớn tham số, và yêu cầu một lượng
lớn dữ liệu huấn luyện vẫn làm cho Vision Transformers trở nên tốn kém về
mặt tính toán. Trong nghiên cứu này, chúng tôi điều tra khả năng áp dụng
Sparse Regularization cho Vision Transformers và tác động của Pruning,
hoặc sau Sparse Regularization hoặc không có nó, đối với sự cân bằng giữa
hiệu suất và hiệu quả. Để hoàn thành điều này, chúng tôi áp dụng các phương
pháp Sparse Regularization và Pruning cho kiến trúc Vision Transformer cho
các tác vụ phân loại hình ảnh trên các bộ dữ liệu CIFAR-10, CIFAR-100, và
ImageNet-100. Quá trình huấn luyện cho mô hình Vision Transformer bao gồm
hai phần: pre-training và fine-tuning. Pre-training sử dụng dữ liệu ImageNet21K,
tiếp theo là fine-tuning trong 20 epochs. Kết quả cho thấy rằng khi kiểm tra
với dữ liệu CIFAR-100 và ImageNet-100, các mô hình có Sparse Regularization
có thể tăng độ chính xác 0.12%. Hơn nữa, việc áp dụng pruning cho các mô
hình có Sparse Regularization mang lại kết quả thậm chí tốt hơn. Cụ thể, nó
tăng độ chính xác trung bình 0.568% trên dữ liệu CIFAR-10, 1.764% trên
CIFAR-100, và 0.256% trên dữ liệu ImageNet-100 so với pruning các mô hình
không có Sparse Regularization. Code có thể truy cập tại đây:
https://github.com/yogiprsty/Sparse-ViT
Từ khóa: Vision Transformer, Sparse Regularization, Pruning
Preprint submitted to Pattern Recognition Letters July 25, 2023arXiv:2307.11988v1  [cs.CV]  22 Jul 2023

--- TRANG 2 ---
1. Giới thiệu
Những tiến bộ công nghệ thông tin hiện đại làm cho việc con người giải quyết
nhiều khó khăn khác nhau trở nên đơn giản và nhanh chóng hơn. Con người tạo
ra rất nhiều dữ liệu hàng ngày do việc sử dụng rộng rãi của nó, bao gồm hoạt
động của các công cụ tìm kiếm, tải lên ảnh và video trực tuyến, và nhiều hơn
nữa. Một cách tiếp cận hiệu quả để phân loại dữ liệu tự động là bằng cách sử
dụng công nghệ Deep Learning. Gần đây, kiến trúc deep learning phổ biến nhất
trong xử lý hình ảnh kỹ thuật số là Mạng Neural Tích chập (CNN) do sự thành
công của nó trong thử thách ILSVRC 2014 GoogleNet[1]. Tuy nhiên, kiến trúc
này thường yêu cầu thời gian tính toán tương đối cao, làm cho việc nghiên cứu
mới để phát triển một kiến trúc có thể vượt trội hơn CNN trở nên khả thi.

Kiến trúc dựa trên self-attention, đặc biệt là Transformer, đã trở thành lựa
chọn ưa thích trong xử lý ngôn ngữ tự nhiên. Trong một số trường hợp,
Transformer vượt trội đáng kể so với các kiến trúc dựa trên lớp tích chập và
lớp recurrent[2]. Không chỉ trong xử lý ngôn ngữ tự nhiên, mà việc sử dụng
Transformers cũng bắt đầu mở rộng, bao gồm trong xử lý hình ảnh kỹ thuật số.
Để áp dụng Vision Transformer, bắt đầu bằng việc chia hình ảnh thành các
patches hoặc các mảnh nhỏ và cung cấp chuỗi các vị trí của những patches này
làm đầu vào cho lớp Transformer. Các patches của hình ảnh được xử lý như
các tokens trong các ứng dụng xử lý ngôn ngữ tự nhiên. Phương pháp này nhanh
hơn EfficientNet năm lần, mà dựa trên Mạng Neural Tích chập. Độ chính xác
của mô hình Vision Transformer sẽ tăng đáng kể nếu nó được pre-trained với
dữ liệu lớn, chẳng hạn như ImageNet21k, trước khi sử dụng nó cho các trường
hợp đơn giản hơn[3].

Ngoài việc nghiên cứu các kiến trúc tốt hơn, các nhà nghiên cứu đã đề xuất
nhiều phương pháp khác nhau để cải thiện độ chính xác hoặc tăng tốc tính toán
trong các mô hình Deep Learning. Các phương pháp được sử dụng rộng rãi bao
gồm Regularization và Pruning. Regularization là một kỹ thuật bổ sung nhằm
tăng cường khả năng tổng quát hóa của mô hình, dẫn đến kết quả tốt hơn trên
dữ liệu kiểm tra[4]. Mặt khác, pruning là một kỹ thuật phổ biến được sử dụng
để có được một mô hình nhỏ gọn hơn bằng cách loại bỏ các trọng số được coi là
ít quan trọng hơn[5]. Cách tiếp cận này cho phép thời gian tính toán nhanh hơn
trong khi duy trì độ chính xác của mô hình.

Dựa trên nền tảng trên, nghiên cứu này áp dụng các phương pháp Sparse
Regularization và Pruning cho kiến trúc Vision Transformer cho các tác vụ
phân loại hình ảnh sử dụng các bộ dữ liệu CIFAR 10 và CIFAR 100. Bộ dữ
liệu CIFAR 10 bao gồm 60,000 hình ảnh có độ phân giải 32x32, được chia
thành mười lớp riêng biệt, mỗi lớp bao gồm 6,000 hình ảnh. Ngược lại,
2

--- TRANG 3 ---
bộ dữ liệu CIFAR 100 bao gồm 60,000 hình ảnh có cùng độ phân giải nhưng
bao gồm một phổ rộng hơn các lớp, cụ thể là 100 lớp. Là một bộ dữ liệu
mã nguồn mở được sử dụng rộng rãi trong nghiên cứu Deep Learning, các
bộ dữ liệu CIFAR phục vụ như một nền tảng kiểm tra lý tưởng để đánh giá
các kỹ thuật được đề xuất và tác động của chúng đối với các tác vụ phân
loại hình ảnh.

2. Nghiên cứu liên quan
Nghiên cứu được thực hiện bởi [3], có tiêu đề "An Image is Worth 16x16 Words:
Transformers for Image Recognition at Scale," cố gắng áp dụng pre-training
cho kiến trúc Vision Transformer sử dụng các lượng dữ liệu khác nhau. Do đó,
khi được huấn luyện trên dữ liệu nhỏ, mô hình Vision Transformer thể hiện
hiệu suất kém. Tuy nhiên, khi được huấn luyện trên dữ liệu lớn, mô hình này
thể hiện hiệu suất vượt trội và vượt qua mô hình ResNet. Nghiên cứu khác
cũng cho thấy cùng kết luận khi Vision Transformer được sử dụng để phân
loại việc sử dụng khẩu trang sử dụng bộ dữ liệu MaskedFace-Net. Kết quả
là, mô hình ViT-H/14 được pre-trained hoạt động tốt hơn mô hình ViT-H/14
không có pre-trained, với 1.87% cao hơn về độ chính xác kiểm tra[6]. Những
phát hiện của nghiên cứu này đã truyền cảm hứng cho việc sử dụng các mô
hình pre-trained trong nghiên cứu hiện tại.

Nhiều nghiên cứu đã được thực hiện để cải thiện kiến trúc dựa trên Vision
Transformer. Data-efficient image Transformers (DeiT) [7] giới thiệu một
mô hình CNN như một giáo viên và áp dụng knowledge distillation[8] để tăng
cường mô hình học sinh của ViT nhằm giảm sự phụ thuộc của nó vào một
lượng lớn dữ liệu. Để có được kết quả thỏa mãn do đó, DeiT chỉ có thể được
huấn luyện trên ImageNet. Ngoài ra, các mô hình CNN và các loại distillation
được chọn có thể ảnh hưởng đến hiệu suất cuối cùng. Convolution enhanced
Image Transformer (CeiT)[9] được đề xuất để khắc phục những hạn chế đó.
Phương pháp này hoạt động bằng cách kết hợp các ưu điểm của CNN trong
việc trích xuất tính năng thấp, tăng cường tính địa phương của các ưu điểm
của Transformers trong việc thiết lập các phụ thuộc tầm xa.

Convolutional Vision Transformer (CvT)[10] được đề xuất để cải thiện hiệu
suất và hiệu quả của ViTs bằng cách giới thiệu convolutions vào kiến trúc
ViT. Thiết kế này giới thiệu convolutions trong hai phần cốt lõi của ViT:
một hệ thống phân cấp của Transformers chứa một convolutional token embedding
mới và một convolutional Transformer block sử dụng một convolutional projection.

Một phương pháp khác được gọi là sharpness-aware minimizer (SAM) được đề
xuất và sử dụng để làm mịn một cách rõ ràng hình học loss trong quá trình
huấn luyện[11]. SAM tìm cách tìm các giải pháp mà toàn bộ môi trường có
3

--- TRANG 4 ---
losses thấp thay vì tập trung vào bất kỳ điểm đơn lẻ nào. Transformer-iN-
Transformer (TNT)[12] được đề xuất để tăng cường khả năng biểu diễn đặc
trưng trong ViT bằng cách chia các hình ảnh đầu vào thành nhiều patches
như "visual sentence" và sau đó chia những patches đó thành sub-patches,
như biểu diễn của "visual word". Ngoài việc sử dụng các conventional transformer
blocks để trích xuất đặc trưng và attention của các visual sentences, sub-transformers
cũng được nhúng vào kiến trúc để khai thác các đặc trưng và chi tiết của
các visual words nhỏ hơn.

Một nghiên cứu khác được thực hiện bởi [13], có tiêu đề "Improvement of
Learning for CNN with ReLU Activation by Sparse Regularization," áp dụng
phương pháp Sparse Regularization cho các đầu vào ReLU. Những phát hiện
cho thấy sự gia tăng độ chính xác đáng kể, từ 9.98% đến 12.12%, so với các
mô hình không có Sparse Regularization. Ngoài ra, so với phương pháp Batch
Normalization, độ chính xác cải thiện từ 4.64% đến 6.87%. Những kết quả
thuyết phục này đã truyền cảm hứng cho việc sử dụng phương pháp Sparse
Regularization trong nghiên cứu hiện tại.

Nghiên cứu được thực hiện bởi [14], có tiêu đề "Vision Transformer Pruning,"
áp dụng phương pháp Pruning cho kiến trúc Vision Transformer. Những phát
hiện cho thấy rằng phương pháp này giảm FLOPS (Floating Point Operations
per Second) 55.5% khi được áp dụng cho kiến trúc Vision Transformer Base
16. Hơn nữa, độ chính xác chỉ giảm 2% trên dữ liệu ImageNet-1K. Những kết
quả hứa hẹn này đã truyền cảm hứng cho việc sử dụng phương pháp Pruning
trong nghiên cứu hiện tại.

Các nghiên cứu trước đây đã cho thấy rằng các neurons hoạt động của Vision
Transformer rất sparse. Điều này ngụ ý rằng chỉ một phần nhỏ neurons trong
một lớp hoặc mạng trở nên hoạt động hoặc "fire" tại bất kỳ thời điểm nào.
Ngược lại, hầu hết neurons vẫn không hoạt động hoặc có giá trị kích hoạt
thấp[11]. Những phát hiện này chỉ ra rằng ít hơn 10% neurons trong một mô
hình Vision Transformer có giá trị lớn hơn zero. Nói một cách đơn giản hơn,
điều này làm nổi bật tiềm năng đáng kể của các mô hình Vision Transformer
cho network pruning.

3. Phương pháp
3.1. Bộ dữ liệu
Chúng tôi đã sử dụng các bộ dữ liệu CIFAR-10 và CIFAR-100 được công
nhận rộng rãi như các chuẩn mực tiêu chuẩn trong các thí nghiệm. CIFAR-10
bao gồm các hình ảnh màu RGB được phân loại thành mười lớp cho phân loại
đối tượng, trong khi CIFAR-100 bao gồm 100 lớp khác nhau. Mỗi hình ảnh
trong các bộ dữ liệu này đã được tiêu chuẩn hóa thành độ phân giải 32x32
4

--- TRANG 5 ---
pixels. Sự nhất quán này về kích thước và định dạng cho phép chúng tôi tập
trung chỉ vào sự phức tạp của dữ liệu và tác động của các phương pháp regularization
khác nhau đối với hiệu suất mô hình. Độ phân giải tiêu chuẩn hóa tạo điều
kiện thuận lợi cho việc so sánh liền mạch giữa các mô hình khác nhau và cho
phép chúng tôi phân biệt những sắc thái tinh tế có thể đã bị bỏ qua. Các bộ
dữ liệu CIFAR thường được sử dụng trong nghiên cứu để đánh giá hiệu suất.
Nghiên cứu được thực hiện bởi [15] sử dụng các bộ dữ liệu CIFAR-10 và CIFAR-100
để đánh giá hiệu quả của phương pháp regularization này. Bằng cách thực
hiện các thí nghiệm trên các bộ dữ liệu CIFAR, các nhà nghiên cứu có thể
đánh giá tác động của các phương pháp regularization đối với hiệu suất mô
hình và so sánh chúng với các cách tiếp cận hiện có.

3.2. Vision Transformer
Transformer là một mô hình dựa trên attention được công bố vào năm 2017
để thay thế kiến trúc dựa trên lớp Recurrent, giải quyết các vấn đề với gradients
và thời gian tính toán tương đối dài[2]. Vision Transformer là một kiến trúc
được lấy cảm hứng từ sự thành công của Transformer trong xử lý ngôn ngữ
tự nhiên. Vào năm 2020, Transformers được sử dụng trong các vấn đề phân
loại hình ảnh và thể hiện hiệu suất vượt trội so với các kiến trúc dựa trên
CNN, chẳng hạn như EfficientNet[3]. Hình 1 minh họa kiến trúc của Vision
Transformer.

Không giống như kiến trúc tối tân, quá trình bắt đầu bằng việc chia các hình
ảnh 2D thành nhiều phần và chuyển đổi chúng thành một chuỗi các 2D patches
phẳng hoặc vector 1 chiều, được hiển thị trong Phương trình 1, trong đó (H, W)
là độ phân giải của hình ảnh gốc, C là số kênh, P là số patches và N=HW|P2.
Trong quá trình này, position embedding được thêm vào các vectors, và các
vectors này được kết hợp với một vector bổ sung (class token) sẽ phục vụ
như đầu ra của kiến trúc Vision Transformer.

Các hình ảnh đã trải qua quá trình Patch và Embedding sẽ đi qua Transformer
Encoder, bao gồm lớp Multi-Head Self-Attention (MSA) và Multi-Layer Perceptron
(MLP). Lớp normalization được áp dụng trước MSA và MLP. Layer normalization
được sử dụng để giảm thời gian tính toán của mạng neural nhân tạo. Lớp này
hoạt động bằng cách tính toán trung bình và phương sai của mỗi đầu vào dữ
liệu, sau đó được sử dụng để normalize tất cả các đầu vào [16]. Phương trình
2 biểu diễn công thức cho lớp normalization, trong đó xi,k là vector đầu vào.
5

--- TRANG 6 ---
Hình 1: Vision Transformer[3]
x∈RHWC→xp∈RN(P2C)(1)
ˆxi,k=xi,k−µi
p
σ2
i+ϵ(2)

Kết quả sẽ đi qua Multi-Head Attention, và lớp này hoạt động bằng cách chia
đầu vào thành nhiều heads, cho phép mỗi head học một mức độ Attention khác
nhau. Các đầu ra của các heads được kết hợp và chuyển tiếp đến Multi-Layer
Perceptron. Phương trình 3 biểu diễn công thức cho Scaled Dot-Product Attention,
trong khi Phương trình 4 biểu diễn công thức cho Multi-Head Attention.

head(Q, K, V ) =softmaxQKt
√dk
V (3)
MultiHead (Q, K, V ) =Concat (head 1... head n) (4)

Lớp cuối cùng của transformer encoding là multilayer perceptron (MLP),
một kiến trúc mạng neural nhân tạo với một hoặc nhiều lớp ẩn giữa các lớp
đầu vào và đầu ra. Việc huấn luyện một mô hình MLP liên quan đến việc điều
chỉnh các trọng số và biases giữa các neurons ở các lớp khác nhau để giảm
thiểu sự khác biệt giữa đầu ra của mô hình và đầu ra mong muốn[17]. Lớp
MLP sử dụng Gaussian Error Linear Unit (GELU). Trong một số bộ dữ liệu,
6

--- TRANG 7 ---
các hàm kích hoạt phi tuyến này có thể khớp và vượt trội hơn các hàm tuyến
tính như ReLU và ELU[18]. Phương trình cho hàm kích hoạt GELU có thể thấy
trong Phương trình 5.

GELU (x) = 0 .5x
1 +tanhhp
2/π
x+ 0.044715 x3i
(5)

3.3. Sparse Regularization
Sparse regularization là một kỹ thuật hàng đầu trong nghiên cứu deep learning
nhằm cải thiện hiệu suất và hiệu quả của các mô hình bằng cách thúc đẩy tính
sparse trong các trọng số đã học. Phương pháp regularization này khuyến khích
một tập con của các trọng số trở thành zero hoặc gần zero, dẫn đến một mô
hình nhỏ gọn và có thể diễn giải hơn[13]. Ngoài ra, phương pháp này có tác
dụng tương tự như Batch Normalization và có thể cải thiện khả năng dự đoán
dữ liệu tổng quát hơn của mô hình. Việc đánh giá tính sparse có thể được thực
hiện theo nhiều cách khác nhau. Tuy nhiên, trong nghiên cứu, sparse regularization
được áp dụng cho đầu vào ReLU sử dụng Phương trình 6, trong đó hk là đầu
vào của ReLU thứ k.

S(hk) =log(1 +h2
k) (6)

Khi giá trị sparse được thu thập, nó được nhân với λ, một tham số kiểm soát
tính sparse, trước khi được thêm vào giá trị loss L. Chi tiết thêm có thể được
tìm thấy trong Phương trình 7.

E=L+λX
kS(hk) (7)

Nhiều khám phá đã diễn ra trong lĩnh vực các kỹ thuật regularization trong
deep learning để xác định vị trí tối ưu trong các mô hình. Các nghiên cứu
trước đây đã cung cấp hiểu biết về hiệu quả của sparse regularization trong
các mô hình convolutional neural network (CNN), đặc biệt khi được áp dụng
cho đầu vào của ReLU[13]. Hình 2 cho thấy một loạt các tùy chọn thú vị để
kết hợp một cách chiến lược sparse regularization trong kiến trúc mô hình,
mỗi tùy chọn đều mang lại những hiểu biết độc đáo và lợi ích tiềm năng.

Option 1 đề xuất một vị trí thú vị cho sparse regularization, trong đó tính
sparse được đánh giá sau khi nhân queries và keys, tiếp theo là scaling, còn
được gọi là similarity score. Trong option 2, tính sparse được đánh giá chính
xác ở giai đoạn attention weight. Giá trị này được thu được sau khi similarity
score
7

--- TRANG 8 ---
(a)
 (b)
Hình 2: Các lựa chọn để đặt sparse regularization

đi qua hàm kích hoạt softmax. Option 3 áp dụng sparse regularization sau
khi nhân attention weight với V (value), còn được gọi là weighted value.
Option 4, tính sparse, được đánh giá sau lớp linear hoặc output trong Multi-Head
Attention. Tùy chọn cuối cùng, theo nghiên cứu trước đây, chúng tôi sẽ áp
dụng sparse regularization cho đầu vào của hàm kích hoạt GELU trong lớp MLP.

3.4. Pruning
Pruning là một phương pháp được sử dụng để tăng tốc tính toán trong các
mô hình deep learning bằng cách loại bỏ các tham số ít quan trọng nhất. Nhiều
cách tiếp cận khác nhau để xóa các tham số không cần thiết có thể được sử
dụng, chẳng hạn như sử dụng l1-norm hoặc l2-norm để chọn các tham số đích
để xóa[5]. Pruning được phân loại rộng rãi thành unstructured và structured
pruning, mỗi loại có chiến lược riêng để giảm tham số[19].

Structured pruning hoạt động bằng cách loại bỏ một cách có hệ thống các
thành phần có cấu trúc trong các mô hình deep learning. Những thành phần
này có thể biểu hiện như filters[20], channels, hoặc thậm chí toàn bộ các lớp,
cho phép một cách tiếp cận toàn diện hơn đối với việc giảm tham số[21]. Bằng
cách loại bỏ một cách có chọn lọc những yếu tố có cấu trúc này, các mô hình
deep learning có thể được tối ưu hóa, cải thiện hiệu quả và giảm độ phức tạp
8

--- TRANG 9 ---
tính toán.

Ngược lại, unstructured pruning (thuật toán 1) tập trung vào các trọng số
riêng lẻ trong các lớp hoặc filters. Độ chính xác phẫu thuật này cho phép
loại bỏ chính xác các trọng số riêng lẻ mà không làm gián đoạn sắp xếp tổng
thể của mô hình deep learning. Unstructured pruning vẫn có thể đạt được
việc giảm tham số đáng kể trong khi duy trì độ chính xác của mạng đã được
prune[22], dẫn đến tính toán nhanh hơn và hiệu suất mô hình được tối ưu hóa.

Thuật toán 1: Global Pruning với L1 Unstructured Method
Input : model, pruning_ratio
Output: model
parameters ←[];
forparam inmodel.parameters() do
parameters.append(param);
all_parameters ←torch.cat([param.data.view(-1) forparamin
parameters]);
threshold ←torch.kthvalue(torch.abs(all_parameters),
int(pruning_ratio * all_parameters.numel())).values;
forparam inparameters do
param_data ←param.data;
mask←torch.abs(param_data) >threshold;
pruned_param ←param_data * mask.float();
param.data ←pruned_param;
returnmodel;

3.5. Transfer Learning
Vision Transformer hoạt động đáng chú ý khi được bổ sung bởi một quá
trình pre-training được gọi là transfer learning [3]. Transfer learning bao
gồm việc huấn luyện mô hình trên một bộ dữ liệu lớn trước khi áp dụng nó
cho dữ liệu khác, tận dụng kiến thức thu được từ giai đoạn pre-training để
tăng cường hiệu suất trên các tác vụ mới. Trong bối cảnh này, sự phong phú
của dữ liệu và bản chất đa dạng của nó đóng vai trò then chốt trong việc
tạo điều kiện cho transfer learning hiệu quả. Lượng lớn dữ liệu và sự đa
dạng rộng rãi là những yếu tố có ảnh hưởng nhất đối với transfer learning
[23]. Để khai thác toàn bộ tiềm năng của transfer learning, nghiên cứu này
nỗ lực tận dụng kiến trúc Vision Transformer, mà đã trải qua việc huấn luyện
9

--- TRANG 10 ---
rộng rãi sử dụng dữ liệu ImageNet21k. ImageNet21k tự hào với một bộ sưu
tập ấn tượng gồm 14,197,122 hình ảnh được chú thích, được tổ chức chính
xác theo hệ thống phân cấp WordNet[24]. Bộ dữ liệu rộng lớn và đa dạng này
bao gồm nhiều đối tượng, cảnh tượng, và khái niệm, cung cấp thông tin hình
ảnh phong phú cho Vision Transformer để học từ đó.

3.6. Model Training
Kiến trúc ViT có thể được cấu hình theo nhiều cách. Để đảm bảo kết quả
kiểm tra không thiên vị cho mỗi cấu hình, các siêu tham số không đổi sẽ
được thiết lập trong quá trình huấn luyện cho các cấu hình khác nhau của
kiến trúc ViT. Đối với nghiên cứu này, chúng tôi sẽ duy trì cùng một thiết
lập được sử dụng trong nghiên cứu ViT gốc, như được mô tả trong tài liệu
tham khảo [3]. Trong quá trình fine-tuning, các siêu tham số sau đã được
sử dụng: batch size là 64, learning rate là 0.03, 20 epochs, hàm loss Cross-Entropy[25],
optimizer Stochastic Gradient Descent, và hàm kích hoạt GeLU[18]. Đối với
transfer learning, chúng tôi đã sử dụng các trọng số pre-trained ban đầu được
huấn luyện trên bộ dữ liệu ImageNet21K[26]. Các thí nghiệm được thực hiện
sử dụng thông số kỹ thuật phần cứng bao gồm GPU RTX 8000, bộ xử lý Intel(R)
Xeon(R) Gold 6230R, và 255 GiB RAM.

4. Thí nghiệm
Để xác nhận hiệu quả của phương pháp được triển khai, chúng tôi đã thực
hiện các thí nghiệm về việc kiểm tra Vision Transformer sử dụng các bộ dữ
liệu CIFAR-10 và CIFAR-100.

4.1. Kiến trúc ViT
Theo bài báo nghiên cứu gốc, các tác giả đã giới thiệu một số biến thể mô
hình Vision Transformer (ViT). Họ đã khám phá các cấu hình mô hình khác
nhau để điều tra tác động của các lựa chọn kiến trúc đối với hiệu suất của
nó. Có ba biến thể của mô hình vision transformer. Đầu tiên là ViT-Base.
Biến thể này phục vụ như mô hình baseline, có kiến trúc transformer với
số lượng lớp và attention heads vừa phải. Nó phục vụ như một điểm tham
chiếu để so sánh hiệu suất của các mô hình ViT khác. Thứ hai là ViT-large.
Biến thể này mở rộng mô hình ViT với nhiều lớp và attention heads hơn, tăng
khả năng và tiềm năng của nó để nắm bắt các mẫu hình ảnh phức tạp. ViT-large
nhằm đạt được độ chính xác cao hơn bằng cách tận dụng một kiến trúc sâu
hơn và giàu tham số hơn. Cuối cùng là ViT-Huge; như tên gọi, biến thể này
đại diện cho một thể hiện thậm chí mở rộng và mạnh mẽ hơn của Vision Transformer.
Nó có các lớp và attention heads lớn hơn đáng kể, cung cấp một khả năng
10

--- TRANG 11 ---
khổng lồ để học các biểu diễn hình ảnh phức tạp. Chúng tôi đã thực hiện các
thí nghiệm sử dụng kiến trúc ViT-B/16. Cấu hình chi tiết hơn và siêu tham
số cho Vision Transformer được hiển thị trong Bảng 1.

Bảng 1: Cấu hình và Siêu tham số cho ViT-B-16
Cấu hình Giá trị
độ phân giải hình ảnh 384 ×384
độ phân giải patch 16 ×16
learning rate 0.001
weight decay 0.0001
batch size 16
hidden size 768
mlp size 3072
#heads 12
encoder length 12

4.2. Tác động của Sparse Regularization
Chúng tôi đã thực hiện các thí nghiệm để đánh giá hiệu quả của sparse
regularization. Có năm kịch bản để triển khai sparse regularization: similarity
score, attention weight, weighted value, output layer, và đầu vào hàm kích
hoạt GELU tại lớp MLP. Giá trị λ được sử dụng là 1/n_feature. Trước khi
kiểm tra, mô hình vision transformer sẽ được fine-tuned trên các bộ dữ liệu
Cifar 10 và Cifar 100. Mô hình Vision Transformer không sử dụng phương
pháp sparse regularization cũng sẽ được fine-tuned với cùng dữ liệu để so
sánh. So sánh vị trí sparse regularization trong mô hình Vision Transformer
được hiển thị trong Bảng 2.

Thí nghiệm sử dụng bộ dữ liệu CIFAR-10 và mô hình Vision Transformer,
mà đã được fine-tuned trong 20 epochs và áp dụng với sparse regularization.
Kết quả là, sau khi tính toán Attention Weight, mô hình đạt được độ chính
xác cao nhất là 98.81% khi sparse regularization được áp dụng cho lớp Self-Attention.
Mặt khác, độ chính xác thấp nhất của
11

--- TRANG 12 ---
Bảng 2: Kết quả trên CIFAR-10
Lớp Vị trí Sparse Acc
- - 98.83
attention similarity score 98.57
attention attention weight 98.81
attention weighted value 98.73
attention output 98.33
MLP input GELU 98.52

mô hình với sparse regularization là 98.33% khi sparse regularization được
áp dụng sau tính toán output layer trong lớp Self-Attention. Tuy nhiên, mặc
dù có sparse regularization, kết quả tốt nhất từ mô hình vẫn không thể vượt
trội hơn mô hình baseline, mà đạt được độ chính xác 98.81%. So sánh vị trí
sparse regularization trong mô hình Vision Transformer được hiển thị trong
Bảng 2.

Bảng 3: Kết quả trên CIFAR-100
Lớp Vị trí Sparse Acc
- - 92.39
attention similarity score 91.51
attention attention weight 92.52
attention weighted value 92.17
attention output 92.13
MLP input GELU 91.73

Thí nghiệm thứ hai được thực hiện sử dụng bộ dữ liệu CIFAR-100 và mô
hình Vision Transformer, mà đã được fine-tuned trong 20 epochs và áp dụng
với sparse regularization. Kết quả là, mô hình đạt được độ chính xác cao
nhất là 92.52% khi sparse regularization được áp dụng cho lớp Self-Attention
sau khi tính toán Attention Weight. Mặt khác, độ chính xác thấp nhất là
91.73% khi sparse regularization được áp dụng cho lớp MLP trước khi tính
toán hàm kích hoạt GELU. Mô hình với sparse regularization đạt được sự
gia tăng độ chính xác 0.12%, vượt trội hơn
12

--- TRANG 13 ---
mô hình baseline. Điều này chỉ ra rằng dữ liệu với nhiều lớp hơn làm cho
mô hình vision transformer có nhiều sparse active neurons. Phương pháp
pruning sẽ hoạt động tốt hơn nếu nó được áp dụng cho các bộ dữ liệu CIFAR-100.

4.3. Tác động của Pruning
Để đánh giá tác động của pruning đối với hiệu suất của Vision Transformer,
chúng tôi đã thực hiện một loạt thí nghiệm toàn diện bằng cách khám phá
các tác động của việc thay đổi tỷ lệ phần trăm pruning trên cả hai bộ dữ
liệu CIFAR-10 và CIFAR-100. Thông qua những thí nghiệm này, chúng tôi
nỗ lực đánh giá ảnh hưởng của pruning đối với độ chính xác và xác định
các ngưỡng pruning tối ưu cho kiến trúc Vision Transformer. Chúng tôi sử
dụng một phạm vi tỷ lệ phần trăm pruning để đạt được mục tiêu này, từ
10% đến 30% trọng số mô hình. Cách tiếp cận có hệ thống này cho phép chúng
tôi đánh giá hiệu suất của Vision Transformer đã được prune dưới các mức
độ giảm trọng số khác nhau. Pruning được áp dụng toàn cục, đảm bảo tác
động đồng nhất trên toàn bộ mô hình. So sánh kết quả độ chính xác sử dụng
phương pháp pruning trên các bộ dữ liệu CIFAR-10 và CIFAR-100 được hiển
thị trong Hình 3.

Hình 3: Tác động pruning trên CIFAR-10 và CIFAR-100

Thí nghiệm được thực hiện sử dụng các bộ dữ liệu CIFAR-10 và CIFAR-100
với mô hình ViT-B-16, mà đã được fine-tuned trong 20 epochs. Sau khi huấn
luyện mô hình, pruning được thực hiện trên tất cả các lớp có trọng số. Các
trọng số được xác định để prune sử dụng phương pháp l1-norm. Kết quả là,
13

--- TRANG 14 ---
cả hai bộ dữ liệu đều cho thấy một tương quan âm giữa sự giảm độ chính xác
và tỷ lệ phần trăm tham số đã được prune: số lượng tham số được prune càng
lớn, độ chính xác càng thấp. Những kết quả này nhấn mạnh sự cần thiết của
việc xem xét cẩn thận và tối ưu hóa trong quá trình pruning. Mặc dù pruning
mang lại tiềm năng cho việc nén mô hình và hiệu quả tính toán, nó phải được
thực hiện một cách thận trọng để giảm thiểu mất mát độ chính xác. Các nhà
nghiên cứu có thể điều hướng sự cân bằng này bằng cách tận dụng các kỹ
thuật như phương pháp L1-norm để xác định các trọng số cần cắt tỉa, tìm
kiếm sự cân bằng lý tưởng giữa tính gắn kết của mô hình và hiệu suất.

4.4. Tác động của Sparse then Prune
Dựa trên các thí nghiệm trước đây, độ chính xác tốt nhất đã được đạt được
bằng cách áp dụng sparse regularization sau khi tính toán attention weight
trên lớp Self Attention. Do đó, trong kiểm tra này, mô hình được huấn luyện
với sparse

(a)
 (b)
Hình 4: (a) Kết quả trên CIFAR-10. (b) Kết quả trên CIFAR-100

regularization được đặt sau khi tính toán attention weight trên lớp Self Attention
trước khi pruning. Tương tự như kiểm tra trước đây, tỷ lệ phần trăm pruning
được sử dụng dao động từ 10% đến 30% và được áp dụng toàn cục.

Pruning được thực hiện trên các mô hình có sparse regularization tạo ra
độ chính xác cao hơn so với những mô hình không có sparse regularization.
Điều này chỉ ra rằng sparse regularization có thể phân biệt hiệu quả giữa
các tham số hoặc trọng số quan trọng và không quan trọng. Do đó, khi các
tham số không quan trọng được prune hoặc xóa, mô hình với sparse regularization
thể hiện khả năng phân loại dữ liệu tốt hơn so với mô hình baseline. Hình
4a và 4b so sánh độ chính xác của pruning có và không có sparse regularization
trên mô hình Vision Transformer. Trong trường hợp CIFAR-10, sự khác biệt
độ chính xác trung bình là 0.568%, trong khi đối với CIFAR-100, sự khác
biệt độ chính xác trung bình là 1.764%.
14

--- TRANG 15 ---
4.5. Kết quả trên CIFAR-10 và CIFAR-100
Tất cả các mô hình được pre-trained trên ImageNet trước khi được sử dụng
trong các tác vụ downstream với các bộ dữ liệu nhỏ hơn. So sánh mô hình
đặc tả dựa trên Transformer có thể thấy trong Bảng 4. Mô hình với ↓10% có
nghĩa là mô hình được prune với tỷ lệ pruning 10%. Bên cạnh đó, mô hình
với ↑384 được fine-tuned trên độ phân giải lớn hơn, là 384 ×384 pixel.

Bảng 4: Đặc tả Mô hình
Mô hình #params Kích thước
HìnhHidden size #heads
ViT-B/16-Sparse 86M 384 768 12
ViT-B/16-Sparse ↓10% 77M 384 768 12
ViT-B/16-Sparse ↓15% 73M 384 768 12
ViT-B/16-Sparse ↓20% 69M 384 768 12
ViT-B/16-Sparse ↓25% 64M 384 768 12
ViT-B/16-Sparse ↓30% 60M 384 768 12
ViT-B/16-SAM 87M 244 768 12
DeiT-B ↑384 86M 384 768 12
CeiT-S ↑384 24M 384 768 12
TNT-B ↑384 65M 384 40+640 4+10

Bảng 5 báo cáo kết quả số trên CIFAR-10 và CIFAR-100 như một tác vụ
downstream. Thú vị là, với gần như cùng tham số như được định nghĩa trong
bài báo gốc [3], Vision Transformer với sparse regularization đứng đầu về
độ chính xác trung bình.

4.6. Kết quả trên ImageNet-100
Để cung cấp cái nhìn tổng quan rộng hơn về tác động của sparse regularization
đối với pruning, thí nghiệm này cũng đã được thực hiện sử dụng ImageNet-100,
một tập con của ImageNet1k. Quá trình lấy mẫu được thực hiện ngẫu nhiên
bằng cách lấy 100 lớp từ bộ dữ liệu ImageNet1k. Kết quả là, mô hình ViT với
15

--- TRANG 16 ---
Bảng 5: So sánh độ chính xác trên CIFAR-10 và CIFAR-100
Mô hình CIFAR-10 CIFAR-100 Trung bình
ViT-B/16-Sparse 98.81 92.52 95.66
ViT-B/16-Sparse ↓10% 98.71 91.66 95.18
ViT-B/16-Sparse ↓15% 98.16 89.62 93.89
ViT-B/16-Sparse ↓20% 96.68 84.88 90.78
ViT-B/16-Sparse ↓25% 93.03 74.67 83.85
ViT-B/16-Sparse ↓30% 85.09 59.47 72.28
ViT-B/16-SAM 98.2 87.6 92.9
DeiT-B ↑384 99.1 90.8 94.95
CeiT-S ↑384 99.1 90.8 94.95
TNT-B ↑384 99.1 91.1 95.1

sparse regularization có thể vượt qua mô hình ViT gốc với độ chính xác cao
hơn 0.12%. Chi tiết có thể thấy trong Bảng 6.

Bảng 6: Độ chính xác trên ImageNet-100
Mô hình Vị trí Sparse Độ chính xác
ViT-B/16 - 96.8
ViT-B/16-Sparse Attention Weight 96.92

Hình 5 hiển thị tác động của pruning được thực hiện trên mô hình áp dụng
sparse regularization. Có thể thấy rằng kết quả có một mẫu tương tự như
thí nghiệm đã được thực hiện trên dữ liệu CIFAR-10 và CIFAR-100. Phương
pháp này tạo ra độ chính xác cao hơn so với pruning được thực hiện trên
các mô hình không có sparse regularization, với độ chính xác cao hơn 0.25%
trung bình.

5. Kết luận
Việc triển khai sparse regularization sẽ tạo ra độ chính xác tốt nhất nếu
nó được đặt sau tính toán attention weight trên lớp self-attention. Pruning
trên các mô hình có sparse regularization tạo ra độ chính xác tốt hơn so với
pruning trên các mô hình không có sparse regularization.
16

--- TRANG 17 ---
Hình 5: Tác động pruning trên ImageNet-100

Mô hình Vision Transformer với sparse regularization có thể cải thiện độ
chính xác 0.12% trên CIFAR-100 và 0.12% trên ImageNet-100. Trong khi đó,
trên dữ liệu CIFAR-10, mô hình với sparse regularization vẫn chưa vượt trội
hơn mô hình baseline. Mặc dù có một tương quan âm giữa pruning và độ chính
xác, với độ chính xác giảm khi tỷ lệ phần trăm tham số được prune tăng,
các mô hình có sparse regularization có xu hướng có độ chính xác trung bình
cao hơn một chút. Đặc biệt. Trên dữ liệu CIFAR 10. Pruning trên các mô
hình có sparse regularization đạt được độ chính xác trung bình cao hơn 0.568%.
Tương tự, trên dữ liệu CIFAR100. Độ chính xác trung bình cao hơn đạt được
thông qua pruning trên các mô hình có sparse regularization là 1.764% và
0.256% cao hơn trên ImageNet-100.

Tóm lại, những khám phá sparse regularization và pruning của chúng tôi
đã tiết lộ một sự tương tác tinh tế giữa những kỹ thuật này và độ chính
xác của mô hình. Sparse regularization, khi được đặt một cách chiến lược,
có thể mở khóa tiềm năng thực sự của các mô hình deep learning, tăng cường
độ chính xác trong những bối cảnh nhất định. Hơn nữa, sự kết hợp của sparse
regularization và pruning trình bày một cách tiếp cận thuyết phục để giảm
thiểu tác động tiêu cực của pruning đối với độ chính xác.
17

--- TRANG 18 ---
Tài liệu tham khảo
[1] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, A. Rabinovich, Going deeper with convolu-
tions, CoRR abs/1409.4842 (2014). arXiv:1409.4842 .
URL http://arxiv.org/abs/1409.4842
[2] A.Vaswani, N.Shazeer, N.Parmar, J.Uszkoreit, L.Jones, A.N.Gomez,
L.Kaiser,I.Polosukhin,Attentionisallyouneed,CoRRabs/1706.03762
(2017). arXiv:1706.03762 .
URL http://arxiv.org/abs/1706.03762
[3] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, N. Houlsby, An image is worth 16x16 words: Trans-
formers for image recognition at scale, CoRR abs/2010.11929 (2020).
arXiv:2010.11929 .
URL https://arxiv.org/abs/2010.11929
[4] J. Kukacka, V. Golkov, D. Cremers, Regularization for deep learning: A
taxonomy, CoRR abs/1710.10686 (2017). arXiv:1710.10686 .
URL http://arxiv.org/abs/1710.10686
[5] L.Liebenwein, C.Baykal, B.Carter, D.Gifford, D.Rus, Lostinpruning:
The effects of pruning neural networks beyond test accuracy, CoRR
abs/2103.03014 (2021). arXiv:2103.03014 .
URL https://arxiv.org/abs/2103.03014
[6] H. D. Jahja, N. Yudistira, Sutrisno, Mask usage recognition us-
ing vision transformer with transfer learning and data augmen-
tation, Intelligent Systems with Applications 17 (2023) 200186.
doi:https://doi.org/10.1016/j.iswa.2023.200186 .
URL https://www.sciencedirect.com/science/article/pii/
S266730532300011X
[7] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, H. Jégou,
Training data-efficient image transformers & distillation through atten-
tion, CoRR abs/2012.12877 (2020). arXiv:2012.12877 .
URL https://arxiv.org/abs/2012.12877
18

--- TRANG 19 ---
[8] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural
network (2015). arXiv:1503.02531 .
[9] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, W. Wu, Incorporating convo-
lution designs into visual transformers (2021). arXiv:2103.11816 .
[10] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, L. Zhang, Cvt:
Introducing convolutions to vision transformers, CoRR abs/2103.15808
(2021). arXiv:2103.15808 .
URL https://arxiv.org/abs/2103.15808
[11] X. Chen, C. Hsieh, B. Gong, When vision transformers outper-
form resnets without pretraining or strong data augmentations, CoRR
abs/2106.01548 (2021). arXiv:2106.01548 .
URL https://arxiv.org/abs/2106.01548
[12] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, Y. Wang, Transformer in
transformer, CoRR abs/2103.00112 (2021). arXiv:2103.00112 .
URL https://arxiv.org/abs/2103.00112
[13] H. Ide, T. Kurita, Improvement of learning for cnn with relu activation
by sparse regularization, in: 2017 International Joint Conference on
Neural Networks (IJCNN), 2017, pp. 2684–2691. doi:10.1109/IJCNN.
2017.7966185 .
[14] M. Zhu, K. Han, Y. Tang, Y. Wang, Visual transformer pruning, CoRR
abs/2104.08500 (2021). arXiv:2104.08500 .
URL https://arxiv.org/abs/2104.08500
[15] X. Gastaldi, Shake-shake regularization, CoRR abs/1705.07485 (2017).
arXiv:1705.07485 .
URL http://arxiv.org/abs/1705.07485
[16] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization (2016). arXiv:
1607.06450 .
[17] H. Ramchoun, M. Amine, J. Idrissi, Y. Ghanou, M. Ettaouil, Multilayer
perceptron: Architecture optimization and training, International Jour-
nal of Interactive Multimedia and Artificial Inteligence 4 (2016) 26–30.
doi:10.9781/ijimai.2016.415 .
19

--- TRANG 20 ---
[18] D. Hendrycks, K. Gimpel, Bridging nonlinearities and stochastic regu-
larizers with gaussian error linear units, CoRR abs/1606.08415 (2016).
arXiv:1606.08415 .
URL http://arxiv.org/abs/1606.08415
[19] J. van Amersfoort, M. Alizadeh, S. Farquhar, N. D. Lane, Y. Gal, Single
shot structured pruning before training, CoRR abs/2007.00389 (2020).
arXiv:2007.00389 .
URL https://arxiv.org/abs/2007.00389
[20] Y. He, G. Kang, X. Dong, Y. Fu, Y. Yang, Soft filter pruning for ac-
celerating deep convolutional neural networks, CoRR abs/1808.06866
(2018). arXiv:1808.06866 .
URL http://arxiv.org/abs/1808.06866
[21] Y. He, L. Xiao, Structured pruning for deep convolutional neural net-
works: A survey (2023). arXiv:2303.00566 .
[22] C. Laurent, C. Ballas, T. George, N. Ballas, P. Vincent, Revisiting loss
modelling for unstructured pruning (2020). arXiv:2006.12279 .
[23] K. Weiss, T. M. Khoshgoftaar, D. Wang, A survey of transfer learning,
Journal of Big Data 3 (1) (2016) 9. doi:10.1186/s40537-016-0043-6 .
URL https://doi.org/10.1186/s40537-016-0043-6
[24] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, L. Fei-
Fei, Imagenet large scale visual recognition challenge, International
Journal of Computer Vision 115 (3) (2015) 211–252. doi:10.1007/
s11263-015-0816-y .
URL https://doi.org/10.1007/s11263-015-0816-y
[25] Z. Zhang, M. R. Sabuncu, Generalized cross entropy loss for training
deep neural networks with noisy labels, CoRR abs/1805.07836 (2018).
arXiv:1805.07836 .
URL http://arxiv.org/abs/1805.07836
[26] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A
large-scale hierarchical image database, in: 2009 IEEE Conference on
Computer Vision and Pattern Recognition, 2009, pp. 248–255. doi:
10.1109/CVPR.2009.5206848 .
20
