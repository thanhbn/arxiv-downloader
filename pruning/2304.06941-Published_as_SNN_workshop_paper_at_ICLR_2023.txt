# 2304.06941.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2304.06941.pdf
# File size: 1046482 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published as SNN workshop paper at ICLR 2023
AUTOSPARSE : TOWARDS AUTOMATED SPARSE TRAIN -
ING OF DEEP NEURAL NETWORKS
Abhisek Kundu
Parallel Computing Lab, Intel Labs
abhisekkundu@gmail.comNaveen K. Mellempudi
Parallel Computing Lab, Intel Labs
naveen.k.mellempudi@intel.com
Dharma Teja Vooturi
Parallel Computing Lab, Intel Labs
dharma.teja.vooturi@intel.comBharat Kaul
Parallel Computing Lab, Intel Labs
bharat.kaul@intel.com
Pradeep Dubey
Parallel Computing Lab, Intel Labs
pradeep.dubey@intel.com
ABSTRACT
Sparse training is emerging as a promising avenue for reducing the computational
cost of training neural networks. Several recent studies have proposed pruning
methods using learnable thresholds to efÔ¨Åciently explore the non-uniform distri-
bution of sparsity inherent within the models. In this paper, we propose Gradient
Annealing (GA), where gradients of masked weights are scaled down in a non-linear
manner. GA provides an elegant trade-off between sparsity and accuracy without
the need for additional sparsity-inducing regularization. We integrated GA with
the latest learnable pruning methods to create an automated sparse training algo-
rithm called AutoSparse , which achieves better accuracy and/or training/inference
FLOPS reduction than existing learnable pruning methods for sparse ResNet50
and MobileNetV1 on ImageNet-1K: AutoSparse achieves ( 2,7) reduction in
(training,inference) FLOPS for ResNet50 on ImageNet at 80% sparsity. Finally,
AutoSparse outperforms sparse-to-sparse SotA method MEST (uniform sparsity)
for80% sparse ResNet50 with similar accuracy, where MEST uses 12% more
training FLOPS and 50% more inference FLOPS.
1 I NTRODUCTION
Deep learning models (DNNs) have emerged as the preferred solution for many important problems
in the domains of computer vision, language modeling, recommendation systems and reinforcement
learning. Models have grown larger and more complex over the years, as they are applied to
increasingly difÔ¨Åcult problems on ever-growing datasets. In addition, DNNs are designed to operate
in overparameterized regime Arora et al. (2018); Belkin et al. (2019); Ardalani et al. (2019) to facilitate
easier optimization using gradient descent methods. Consequently, computational costs (memory
and Ô¨Çoating point operations FLOPS) of performing training and inference tasks on state-of-the-art
(SotA) models has been growing at an exponential rate (Amodei & Hernandez).
Two major techniques to make DNNs more efÔ¨Åcient are (1) reduced-precision Micikevicius et al.
(2017); Wang et al. (2018); Sun et al. (2019); Das et al. (2018); Kalamkar et al. (2019); Mellempudi
et al. (2020), and (2) sparse representation. Today, SotA training hardware consists of signiÔ¨Åcantly
more reduced-precision FLOPS compared to traditional FP32 computations, while support for
structured sparsity is also emerging Mishra et al. (2021). In sparse representation, selected model
parameters are masked/pruned resulting in signiÔ¨Åcant reduction in FLOPS, which has a potential of
more than 10throughput. While overparameterized models help to ease the training, only a fraction
of the model parameters is needed to perform inference accurately Str√∂m (1997); Han et al. (2015);
Narang et al. (2017); Gale et al. (2019); Li et al. (2020). Modern DNNs have large number of layers
with non-uniform distribution of number of model parameters as well as FLOPS per layer. Sparse
1arXiv:2304.06941v1  [cs.LG]  14 Apr 2023

--- PAGE 2 ---
Published as SNN workshop paper at ICLR 2023
training algorithms aim to achieve an overall sparsity budget using layer-wise (a) uniform sparsity (b)
heuristic non-uniform sparsity, or (c) learned non-uniform sparsity. First two techniques can cause
sub-optimal parameter allocation which leads to drop in accuracy and/or lower FLOPS efÔ¨Åciency.
Sparse training methods can be categorized based on when and how to prune. Producing sparse
model from fully-trained dense model (Han et al. (2015)) is useful for efÔ¨Åcient inference and not
for efÔ¨Åcient training. Sparse-to-sparse methods MEST (Yuan et al. (2021)), despite being efÔ¨Åcient
in each iteration, requires longer training epochs (reducing the gain in training FLOPS) or frequent
dense gradient update TopKAST (Jayakumar et al. (2021)) (more FLOPS during backpass) while
keeping Ô¨Årst/last layer dense to achieve high accuracy results. Dense-to-sparse methods monotonically
increase sparsity from a dense, untrained model throughout training GMP (Zhu & Gupta (2017))
using some schedule, and/or use full gradient updates to masked weights OptG (Zhang et al. (2022))
to improve accuracy. These methods typically have higher training FLOPS count.
Learning Sparsity-Accuracy Trade-off: Recent methods that learn sparsity distribution during
training are STR (Kusupati et al. (2020)), DST (Liu et al. (2020)), LTP (Azarian et al. (2021)), CS
(Savarese et al. (2021)) and SCL (Tang et al. (2022)). Learned sparsity methods offer a two-fold
advantage over methods with uniform sparsity and heuristically allocated sparsity: (1) computationally
efÔ¨Åcient, as the overhead of pruning mask computation (e.g. choosing top klargest) is eliminated,
(2) learns a non-uniform sparsity distribution inherent within the model, producing a more FLOPS-
efÔ¨Åcient sparse model for inference. However, the main challenge with these methods is to identify
the sparsity distribution in early epochs, in order to become competitive to sparse-to-sparse methods
in terms of training FLOPS, while achieving sparsity-accuracy trade-off. For this, these methods
typically apply sparsity-inducing regularizer with the objective function (except STR). However,
these methods either produce sub-optimal sparsity/FLOPS (SCL) or suffer from signiÔ¨Åcant accuracy
loss (LTP), or susceptible to ‚Äòrun-away sparsity‚Äô while extracting high sparsity from early iterations
(STR, DST). In order to deal with uncontrolled sparsity growth, DST implements hard upper limit
checks (e.g. 99%) on sparsity to trigger a reset of the offending threshold (falling into dense training
regime and resulting in higher training FLOPS count) to prevent loss of accuracy. Similarly, STR
uses a combination of small initial thresholds with an appropriate weight decay to delay the induction
of sparsity until later in the training cycle (e.g. 30epochs) to control unfettered growth of sparsity.
Motivated by the above challenges, we propose Gradient Annealing (GA) method, to address the
aforementioned issues related to training sparse models. Comparing with existing methods, GA
offers greater Ô¨Çexibility to explore the trade-off between model sparsity and accuracy, and provides
greater stability by preventing divergence due to runaway sparsity. We also propose a uniÔ¨Åed training
algorithm called AutoSparse , combining best of learnable threshold methods STR with GAthat
attempts to pave the path towards full automation of sparse-to-sparse training. Additionally, we also
demonstrated that when coupled with uniform pruning methods TopKAST, GAcan extract better
accuracy at a 80% sparsity budget. The key contributions of this work are as follows:
We present a novel Gradient Annealing (GA) method (with hyper-parameter ) which is a more
generalized gradient approximator than STE (Bengio et al. (2013)) and ReLU. For training end-to-end
sparse models, GA provides greater Ô¨Çexibility for sparsity-accuracy trade-off than other methods.
We propose AutoSparse , an algorithm that combines GA with learnable sparsity method STR
to create an automated framework for end-to-end sparse training. AutoSparse outperforms SotA
learnable sparsity methods in terms of accuracy and training/inference FLOPS by maintaining
consistently high sparsity throughout the training for ResNet50 and MobileNetV1 on Imagenet-1K.
We demonstrate the efÔ¨Åcacy of Gradient Annealing as a general learning technique independent of
AutoSparse by applying it to TopKAST method to improve the Top-1 accuracy of ResNet50 by 0:3%
on ImageNet-1K dataset using uniform sparsity budget of 80%.
Finally, AutoSparse outperforms SotA sparse-to-sparse training method MEST (uniform
sparsity) where MEST uses (12%, 50%) more FLOPS for (training,inference) than AutoSparse for
80% sparse ResNet50 with comparable accuracy.
2

--- PAGE 3 ---
Published as SNN workshop paper at ICLR 2023
2 R ELATED WORK
Sparse training methods can be categorized the following way based on when and how to prune.
Pruning after Training: Here sparse models are created using a three-stage pipeline‚Äì train a dense
model, prune, and re-train (Han et al. (2015); Guo et al. (2016)). These methods are computationally
more expensive than dense training and are useful only for FLOPS and memory reduction in inference.
Pruning before Training: Such methods are inspired by the ‚ÄòLottery Ticket Hypothesis‚Äô (Frankle
& Carbin (2019)), which tries to Ô¨Ånd a sparse weight mask (subnetwork) that can be trained from
scratch to match the quality of dense trained model. However, they use an iterative pruning scheme
that is repeated for several full training passes to Ô¨Ånd the mask. SNIP (Lee et al. (2019)) preserves the
loss after pruning based on connection sensitivity. GraSP (Wang et al. (2020)) prunes connection such
that it preserves the network‚Äôs gradient Ô¨Çow. SynFlow (Tanaka et al. (2020)) uses iterative synaptic
Ô¨Çow pruning to preserve the total Ô¨Çow of synaptic strength (global pruning at initialization with no
data and no backpropagation). 3SP (van Amersfoort et al. (2020)) introduces compute-aware pruning
of channels (structured pruning). The main advantage of these methods is that they are compute and
memory efÔ¨Åcient in every iteration. However, they often suffer from signiÔ¨Åcant loss of accuracy.
Note that, the weight pruning mask is Ô¨Åxed throughout the training.
Pruning during Training: For these methods, the weight pruning mask evolves dynamically with
training. Methods of this category can belong to either sparse-to-sparse or dense-to-sparse training.
Insparse-to-sparse training , we have a sparse model to start with (based on sparsity budget) and the
budget sparsity is maintained throughout the training. SET (Mocanu et al. (2018)) pioneered this
approach where they replaced a fraction of least magnitude weights by random weights for better
exploration. DSR (Mostafa & Wang (2019)) allowed sparsity budget to be non-uniform across layers
heuristically, e.g., higher sparsity for later layers. SNFS (Dettmers & Zettlemoyer (2019)) proposed
to use momentum of each parameter as a criterion to grow the weights leading to increased accuracy.
However, this method requires computing full gradient for all the weights even for masked weights.
RigL (Evci et al. (2021)) activates/revives new weights ranked according to gradient magnitude
(using infrequent full gradient calculation), i.e., masked weights receive gradients only after certain
iterations. They decay this number of revived weights with time. The sparsity distribution can be
uniform or non-uniform (using ERK where sparsity is scaled using number of neurons and kernel
dimension). MEST (Yuan et al. (2021)) always maintains Ô¨Åxed sparsity in forward and backward
pass by computing gradients of survived weights only. For better exploration, they remove some
of the least important weights (ranked proportional to magnitude plus the gradient magnitude) and
introduce same number of random ‚Äòzero‚Äô weights to maintain the sparsity budget. This method is
suitable for sparse training on edge devices with limited memory. Restricting gradient Ô¨Çow causes
accuracy loss in sparse-to-sparse methods despite having computationally efÔ¨Åcient iterations, and
they need a lot longer training epochs (250 for MEST and 500 for RigL) to regain accuracy at the
cost of higher training FLOPS. TopKAST (Jayakumar et al. (2021)) always prunes highest magnitude
weights (uniform sparsity), but updates a superset of active weights based on gradients of this superset
(backward sparsity) so that the pruned out weights can be revived. For high quality results, they
had to use full weight gradients (no backward sparsity) throughout the training. PowerPropagation
(PP) (Schwarz et al. (2021)) transforms the weights as w=vjvj 1, s.t. it creates a heavy-tailed
distribution of trained weights. They observe that weights initialized close to 0 are likely to be pruned
out, and weights are less likely to change sign. PP, when applied on TopKAST, i.e., TopKAST + PP,
improves the accuracy of TopKAST. RigL and MEST keep Ô¨Årst layer dense while TopKAST and its
variants make Ô¨Årst and last layers dense. Gradmax (Evci et al. (2022)) proposed to grow network
by adding more weights gradually with training epochs in order to reduce overall training FLOPS.
Dense-to-Sparse Training : GMP (Zhu & Gupta (2017)) is a simple, magnitude-based weight pruning
applied gradually throughout the training. Gradients do not Ô¨Çow to masked weights. Gale et al.
(2019)) improved the accuracy of GMP by keeping Ô¨Årst layer dense and last layer sparsity at 80%.
DNW (Wortsman et al. (2019)) also uses magnitude pruning while allowing gradients to Ô¨Çow to
masked weight via STE (Bengio et al. (2013)). DPF (Lin et al. (2020)) updates dense weights using
full-gradients of sparse weights while simultaneously maintains dense model. OptG (Zhang et al.
(2022)) learns both weights and a pruning supermask in a gradient driven manner. They argued in
favour of letting gradient Ô¨Çow to pruned weights so that it solves the ‚Äòindependence paradox‚Äô problem
that prevents from achieving high-accuracy sparse results. However, they achieve a given sparsity
budget by increasing sparsity according to some sigmoid schedule (sparsity is extracted only after 40
epochs). This suffers from larger training FLOPs count. The achieved sparsity distribution is uniform
3

--- PAGE 4 ---
Published as SNN workshop paper at ICLR 2023
across layers. SWAT (Raihan & Aamodt (2020)) sparsiÔ¨Åes both weights and activations by keeping
Top magnitude elements in order to further reduce the FLOPS.
Learned Pruning: For a given sparsity budget, the methods discussed above either keeps sparsity
uniform at each layer or heuristically creates non-uniform sparsity distribution. The following
methods learns a non-uniform sparsity distribution via some sparsity-inducing optimization.
DST (Liu et al. (2020)) learns both weights and pruning thresholds (creating weight mask) where
they impose exponential decay of thresholds as regularizer. A hyperparameter controls the amount of
regularization, leading to a balance between sparsity and accuracy. In order to reduce the sensitivity
of the hyperparameter, they manually reset the sparsity if it exceeds some predeÔ¨Åned limit (thus
occasionally falling into dense training regime). Also, approximation of gradient of pruning step
function helps some masked elements to receive loss gradient.
STR (Kusupati et al. (2020)) is a SotA method that learns weights and pruning threshold using ReLU
as a mask function, where STE of ReLU is used to approximate gradients of survived weights and
masked weights do not receive loss gradients. It does not use explicit sparsity-inducing regularizer.
However, extracting high sparsity from early iterations leads to run-away sparsity; this forces STR to
run fully dense training for many epochs before sparsity extraction kicks in.
SCL (Tang et al. (2022)) learns weights and a mask (same size as weights) that is binarized in forward
pass. This mask along with a decaying connectivity hyperparameter are used as a sparsity-inducing
regularizer in the objective function. The learned mask increases the effective model size during
training, which might create overhead moving parameter from memory.
LTP (Azarian et al. (2021)) learns the pruning thresholds using soft pruning and soft L0 regularization
where sigmoid is applied on transformed weights and sparsity is controlled by a hyper-parameter. CS
(Savarese et al. (2021)) uses sigmoidal soft-threshold function as a sparsity-inducing regularization.
For a detailed discussion on related work, see HoeÔ¨Çer et al. (2021).
3 G RADIENT ANNEALING (GA)
A typical pruning step of deep networks involves masking out weights that are below some threshold
T. This sparse representation of weights beneÔ¨Åts from sparse computation in forward pass and in
computation of gradients of inputs. We propose the following pruning step, where wis a weight and
Tis a threshold that can be deterministic (e.g., TopK magnitude) or learnable:
(sparse) ~w=sign(w)h(jwj T)
Forward pass h(x) =x; x> 0
0; x0(Proxy) Gradient@h(x)
@x=1; x> 0
; x0(1)
where 01.~wis0ifjwjis below threshold T. Magnitude-based pruning is a greedy,
temporal view of parameter importance. However, some of the pruned-out weights (in early training
epochs) might turn out to be important in later epochs when a more accurate sparse pattern emerges.
For this,h()in eqn (1) allows the loss gradient to Ô¨Çow to masked weights in order to avoid
permanently pruning out some important weights. The proposed gradient approximation is inspired
by the Straight Through Estimator (STE) Bengio et al. (2013) which replaces zero gradients of
discrete sub-differentiable functions by proxy gradients in back-propagation. Furthermore, we decay
thisas the training progresses. We call this technique the Gradient Annealing .
We decayat the beginning of every epoch and keep this value Ô¨Åxed for all the iterations in that
epoch. We want to decay slowly in early epochs and then decay it steadily. For this, we compare
several choices for decaying : Ô¨Åxed scale (no decay), linear decay, cosine decay (same as learning
rate (2)), sigmoid decay (deÔ¨Åned in (3)) and Sigmoid-Cosine decay (deÔ¨Åned in (4)). For sigmoid
decay in (3), L0= 6andL1= 6. For total epochs T, scale forin epochiis
Cosine-Decay (i;T)ci= (1 + cosine (i=T))=2 (2)
Sigmoid-Decay (i;T)si= 1 sigmoid (L0+ (L1 L0)i=T) (3)
Sigmoid-Cosine-Decay (i;T) = maxfsi;cig (4)
4

--- PAGE 5 ---
Published as SNN workshop paper at ICLR 2023
Figure 1 shows the effect of various linear and non-linear annealing of on dynamic sparsity. Fixed
scale with no decay (STE) does not give us a good control of dynamic sparsity. Linear decay is better
than this but suffers from drop in sparsity towards the end of training. Non-linear decays in eqn (2, 3,
4) provide much superior trade-off between sparsity and accuracy. While eqn (2) and eqn (4) show
very similar behavior, sharp drop of eqn (3) towards the end of training push up the sparsity a bit
(incurring little more drop in accuracy). These plots are consistent with our analysis of convergence
of GA in eqn (5). Annealing schedule of closely follows learning rate decay schedule.
Analysis of Gradient Annealing Here we analyze the effect of the transformation h()on the
convergence of the learning process using a simpliÔ¨Åed example as follows. Let v=jwj T,
u=h(v), optimal weights be wand optimal threshold be T, i.e.,v=jwj T. Let us
consider the loss function as
min
vL(v) = 0:5(h(v) v)2
and let@h(v)denote the gradient@h(v)
@v. We consider the following cases for loss gradient for v.
@L
@v=@h(v)(h(v) v) =8
>>>>><
>>>>>:@h(v)0 = 0 ifh(v) =v
@h(v)(v v) = 1(v v)ifv>0andv>0
@h(v)(v+jvj) = 1(v+jvj)ifv>0andv0
@h(v)( v) =( v)ifv0andv>0
@h(v)(jvj) =(jvj)ifv0andv0(5)
Correct proxy gradients for h()should move vtowardsvduring pruning (e.g., opposite direction of
gradient for gradient descent) and stabilize it at its optima (no more updates). Therefore, @h(v)>0
should be satisÔ¨Åed for better convergence of vtov. Ourh()satisÔ¨Åes this condition for  > 0.
Furthermore, for v>0,vgets updated proportional to v v, i.e., how far vis fromv. As training
progresses and vgets closer to v,vreceives gradually smaller gradients to Ô¨Ånally converge to v.
However, for v0,vreceives gradient proportional to magnitude of v, irrespective of how close
vis tov. Also, note that we beneÔ¨Åt from sparse compute when v0.
We set initial Thigh in order to achieve sparsity from early epochs. However, this likely leads to a
large number of weights following condition 4 in eqn (5). Fixed, large (close to 1) makes large
correction to vand moves it to vquickly. Consequently, vmoves from condition 4 to condition 2,
losing out the beneÔ¨Åt of sparse compute. A lower ‚Äòdelays‚Äô this transition and enjoys the beneÔ¨Åts of
sparsity. This is why we choose <1rather than identity STE as proxy gradient (unlike Tang et al.
(2022)). However, as training progresses, more and more weights move from condition 4 to condition
2 leading to a drop in sparsity. This behavior is undesirable to reach a target sparsity at the end of
training. In order to overcome this, we propose to decay with training epochs such that we enjoy
the beneÔ¨Åts of sparse compute while vbeing close to v. That is, GA provides a more controlled and
stable trade-off between sparsity and accuracy throughout the training.
Note that, GA is applicable when we compute loss gradients for a superset of active (non-zero)
weights that take part in forward sparse computation using gradient descend. For an iteration t, let
the forward sparsity be S. If= 0, then we need to calculate gradient for only those non-zero
weights as other weights would not receive gradients due to ReLU STE. In order to beneÔ¨Åt from such
computational reduction, we can set = 0after several epochs of annealing.
4 A UTOSPARSE : SPARSE TRAINING WITH GRADIENT ANNEALING
AutoSparse is the sparse training algorithm that combines the best of learnable threshold pruning
techniques STR with Gradient Annealing (GA). AutoSparse meets the requirements necessary for
efÔ¨Åcient training of sparse neural networks.
Learnable Pruning Thresholds : Eliminates sorting-based threshold computation to reduce
sparsiÔ¨Åcation overhead compared to uniform pruning methods. Learns the non-uniform distribution
of sparsity across the layers.
5

--- PAGE 6 ---
Published as SNN workshop paper at ICLR 2023
(a) FLOPS (M) for 80% sparse ResNet50 produced
by uniform-MEST and learned sparsity methods
 (b) Sparsity achieved for various gradient annealing
Figure 1: Sparse ResNet50 training on ImageNet
Sparse Model Discovery : Discover an elegant trade-off between model accuracy vs. level of
sparsity by applying Gradient Annealing method (as shown in Figure 1). Produce a sparse model at
the end of the training with desired sparsity level guided by the hyper-parameter .
Accelerate Training/Inference : Reduce training FLOPs by training with sparse weights from
scratch, maintaining high levels of sparsity throughout the training, and using sparsity in both forward
and backward pass. Produce FLOPS-efÔ¨Åcient sparse model for inference.
Previous proposals using learnable threshold methods such as DST and STR address the Ô¨Årst cri-
terion but do not effectively deal with accuracy vs sparsity trade-off. These methods also do not
accelerate training by reducing FLOPS as effectively as our method. Uniform pruning methods such
as TopKAST, RigL, MEST address the third criterion of accelerating training, but incur sparsÔ¨Åca-
tion overheads for computing threshold values and cannot automatically discover the non-uniform
distribution of sparsity. This leads to sub-optimal sparse models for inference (Figure 1a).
Formulation: LetD:=f(xi2Rd;yi2R)gbe the observed data, Wbe the learnable network
parameters,Lbe a loss function. For an L-layer DNN,Wis divided into layer-wise trainable
parameter tensors, [W`]L
`=1. As various layers can have widely different number of parameters and
also unequal sensitivity to parameter alteration, we use one trainable pruning parameter, s`for each
layer`, i.e.,s= [s1;:::;sL]is the vector of trainable pruning parameter. Let g:R!Rbe applied
element-wise. For layer `,T`=g(s`)is the pruning threshold for W`. We seek to optimize:
min
W;sL(Sh;g(W;s);D) (6)
where, functionSh;g, parameterized by h2R!Rthat is applied element-wise.
^W`=Sh;g(W`;s`) =sign(W`)h(jW`j g(s`)) (7)
Gradient annealing is applied via h()as discussed earlier.
Sparse Forward Pass: At iteration t, for layer`, sparse weights are ^W(t)
`=Sh;g(W(t)
`;s(t)
`)as
deÔ¨Åned in eqn (7). Let the set of non-zero (active) weights A(t)
`=fi:^W(t)
`;i>0g. For simplicity, let
us drop the subscript notations. ^Wis used in the forward pass as Y=X
^Wwhere
is tensor MAC
operation, e.g., convolution. Let Adenote the fraction of non-zero elements of Wbelonging toA, i.e,
out forward sparsity is 1 A.^Wis also used in computation of input gradient during backward pass.
Every iteration, we update Wand construct ^Wfrom updated Wand learned threshold.
Sparse Compute of Input Gradient: For an iteration and a layer (we drop the subscripts t,`for
simplicity), output of sparse operations in forward pass need not be sparse, i.e., Y=X
^Wis
typically dense. Consequently, gradient of output rYis also dense. We compute gradient of input
rXas^W
rY. Computation ofrXis sparse due to sparsity in ^W.
Sparse Weight Gradient: Gradient of weights rWis computed as X
rY. Note that, for forward
sparsity S,= 0implies weight gradient sparsity S as no gradient Ô¨Çows to pruned weights. We can
have a hyperparameter that speciÔ¨Åes at which epoch we set = 0, to enjoy beneÔ¨Åts of sparse weight
gradients. However, we need to keep 6= 0 for several epochs to achieve high accuracy results,
6

--- PAGE 7 ---
Published as SNN workshop paper at ICLR 2023
losing the beneÔ¨Åts of sparse weight gradient. In order to overcome this, we propose the following for
the epochs when 6= 0. We can make sparse rWif we compute loss gradient using a subset BofW.
B=fi:Wi2TopK( W,B)g; BA (8)
whereBis a superset ofAand TopK( W,k) picks indices of klargest magnitude elements from
W. This constitutes our weight gradient sparsity 1 B(similar to TopKAST). We apply gradient
annealing on setBnA , i.e., gradients of weights in BnA are decayed using . Note that, = 0
impliesB=A.
FLOPS Computation: For a dense model with FLOPS f`
Dand sparse version with FLOPS f`
Sfor
layer`, total dense training FLOPS for a single sample is 3P
`f`
Dand sparse training FLOPS is
3P
`f`
Swhen only sparse weight gradient is involved (AutoSparse with = 0), otherwise, it is
2P
`f`
S+P
`f`
Dfor full-dense weight gradient (AutoSparse with 6= 0). Also, for AutoSparse,
f`
Svaries with iterations, so we sum it up for all layers, all iterations and all data points to get the
Ô¨Ånal training FLOPS count. For explicitly set sparsity fBfor weight gradients, AutoSparse training
FLOPS for a sample is 2P
`f`
S+P
`fB(for6= 0) and 2P
`f`
S+P
`max(fB;f`
S)(for= 0).
5 E XPERIMENTS
ForVision Models , we show sparse training results on ImageNet-1K (Deng et al. (2009)) for two
popular CNN architectures: ResNet50 He et al. (2016) and and MobileNetV1 Howard et al. (2017),
to demonstrate the generalizability of our method. For AutoSparse training, we use SGD as the
optimizer, momentum 0:875, learning rate (max) 0:256using a cosine annealing with warm up of
5epochs. We run all the experiments for 100epochs using a batch size 256. We use weight decay
= 0:000030517578125 (picked from STR Kusupati et al. (2020)), label smoothing 0:1,s0= 5.
We presented our results only using Sigmoid-Cosine decay of (deÔ¨Åned in eqn (4)).
ForLanguage Models , we choose Transformer models Vaswani et al. (2017) for language translation
on WMT14 English-German data. We have 6 encoder and 6 decoder layers with standard hyper-
parameter setting: optimizer is ADAM with betas (0:9;0:997), token size 10240 , warm up 4000 ,
learning rate 0:000846 that follows inverse square root decay. We apply AutoSparse by introducing a
learnable threshold for each linear layer, and we initialize them as s0= 7:0. Also, initial 0= 0:4
is annealed according to exponential decay as follows. For epoch tand >0(we use= 1):
Exponential-Decay (t;) =e( t)(9)
We keep the Ô¨Årst and last layers of transformer dense and apply AutoSparse to train it for 44epochs.
We repeat the experiments several times with different random seeds and report the average numbers.
Notation: We deÔ¨Åne the following notations that are used in the tables. ‚ÄòBase‚Äô: Dense baseline
accuracy, ‚ÄòTop1(S)‚Äô: Top-1 accuracy for sparse models, ‚ÄòDrop%‚Äô: relative drop in accuracy for
sparse models from Base, ‚Äò S%‚Äô: percentage of model sparsity, ‚ÄòTrain F‚Äô: fraction of training FLOPs
comparing to baseline FLOPs, ‚ÄòTest F‚Äô: fraction of inference FLOPs comparing to bsaeline FLOPs,
‚ÄòBackS%‚Äô: explicitly set sparsity in weight gradients ( Bin eqn (8)), ‚ÄòBLEU(S)‚Äô: BLEU score for
sparse models. Smaller values of ‚ÄòTrain F‚Äô and ‚ÄòTest F‚Äô suggests larger reduction in computation.
5.1 E FFICACY OF GRADIENT ANNEALING
We compare AutoSparse results with STR and DST. Lack of gradient Ô¨Çow to pruned out elements
prevents STR to achieve the optimal sparsity-accuracy trade-off. For example, they need dense
training for many epochs in order to achieve high accuracy results, losing out the beneÔ¨Åts of sparse
training. Our gradient annealing overcomes such problems and achieves much superior sparsity-
accuracy trade-off (Table 1, 3). Similarly, our method achieves higher accuracy than DST for both
80% and90% sparsity budget for ResNet50 (Table 1). However, DST uses separate sparsity-inducing
regularizer, whereas our gradient annealing itself does the trick. SCL reports 0:23% drop in accuracy
at74% sparsity for ResNet50 on ImageNet with inference FLOPs 0:21of baseline). LTP produces
89% sparse ResNet50 that suffers from 3.2% drop in accuracy from baseline (worse than ours).
Continuous SparsiÔ¨Åcation (CS) induces sparse training using soft-thresholding as a regularizer. The
sparse model produced by them is used to test Lottery Ticket Hypothesis (retrained). Lack of FLOPs
number makes it harder to directly compare with our method. GDP (Guo et al. (2021)) prunes
7

--- PAGE 8 ---
Published as SNN workshop paper at ICLR 2023
Method Base Top1(S) Drop% S% Train F Test F comment
RigL 76:8 74:6 2:86 80 0:33 0:22 uniform sparsity
TopKAST?76:8 75:7 0:94 80 0:48 0:22 uniform sparsity
TopKAST?+PP 76:8 76:24 0:73 80 0:48 0:22 uniform sparsity
TopKAST+GA 76:8 76:47 0:43 80 0:48 0:22 uniform sparsity
MEST 1:7+EM 76:9 76:71 0:25 80 0:57 0:21 uniform sparsity
DST 74:95 74:02 1:24 80:4 ‚Äì 0:15 learnable sparsity
STR 77:01 76:19 1:06 79:55 0:54 0:18 learnable sparsity
AutoSparse 77:01 76:77 0:31 79:67 0:51 0:140=.75,=0@epoch90
AutoSparse 77:01 76:59 0:55 80:78 0:46 0:140=.8,=0@epoch70
MEST 1:7+EM 76:9 75:91 1:29 90 0:28 0:11 uniform sparsity
DST 74:95 72:78 2:9 90:13 ‚Äì 0:087learnable sparsity
STR 77:01 74:31 3:51 90:23 0:44 0:083learnable sparsity
AutoSparse 77:01 75:9 1:44 85:1 0:42 0:0960=.9,=0@epoch50
AutoSparse 77:01 75:19 2:36 89:94 0:40 0:0810=.9,=0@epoch45
STR 77:01 70:4 8:58 95:03 0:28 0:039learnable sparsity
AutoSparse 77:01 70:84 8:01 95:09 0:2 0:0360=0.8,=0@epoch20
Table 1: ResNet50 on ImageNet: Comparing accuracy, sparsity and the FLOPS (dense 1) for
training and inference for selected sparse training methods. TopKAST?: TopKAST with 0%
backward sparsity. TopKAST?+GA : TopKAST?with Gradient Annealing boosts the accuracy
despite having same training/inference FLOPS. For AutoSparse 79:67% sparsity,0=0.75 is decayed
till epoch 90 and then set to 0 (implying 80% forward and backward sparsity after epoch 90).
Similarly, for AutoSparse 95:09% sparsity,0=0.8 is decayed till epoch 20 and then set to 0. Standard
deviation of results for AutoSparse is less than 0.03.
Method S% Top1(S) Drop% Train F Test F Back(S) comment
AutoSparse 83:74 75:02 2:58 0:37 0:128500=1.0,s0=-8, w grad
50% sparse
TopKAST 80 75 2:34 0:33 0:22 50 uniform sparse fwd & in
grad 80% w grad 50%
Table 2: ResNet50 on ImageNet: AutoSparse with explicitly set sparsity for weight gradient. For
AutoSparse 83:74% sparsity,0= 1 is decayed till epoch 100 resulting in identical sparsity for
forward and input gradients, along with 50% sparsity for weight gradient throughout the training
(involves invoking TopK method).
channels using FLOPS count as a regularizer to achieve optimal accuracy vs compute trade-off
(0.4% drop in accuracy with 0:49inference FLOPS). This method is not directly comparable with
our results for unstructured sparsity. Finally, MEST (SOTA for uniform sparse training) achieves
comparable accuracy for 80% sparse Resnet50, however, using 12% more training FLOPS and 50%
more inference FLOPS (as their sparse model is not FLOPS-efÔ¨Åcient).
Results for AutoSparse with Predetermined Backward Sparsity
In AutoSparse, the forward sparsity is determined by weights that are below the learned threshold (let
this dynamic sparsity be S). We decay = 0:85throughout sparse training and apply TopK to select
indices of 50% largest magnitude weights (during forward pass) that are used in computation of loss
gradients (S50%). These weights are superset of non-zero weights used in forward computation.
This way, we have sparsity S for forward and input gradient compute, and 50% sparsity for weight
gradient. Gradient annealing is applied on gradients of those weights that appear in these top 50% but
were pruned in forward pass. We compare our results with TopKAST 80% forward sparsity and 50%
backward sparsity in Table 2. We produce 83.74% sparse model that signiÔ¨Åcantly reduces inference
FLOPs ( 0:128of baseline) while achieving similar accuracy and training FLOPs as TopKAST
although TopKAST takes 1:7more inference FLOPS than ours.
8

--- PAGE 9 ---
Published as SNN workshop paper at ICLR 2023
Method Base Top1(S) Drop% S% Train F Test F comment
STR 71.95 68:35 5 75 :28 0:43 0:18 learnable sparsity
AutoSparse 71.95 69:97 2:75 74:98 0:53 0:210=.4,=0 @ epoch 90
STR 71.95 64.83 9.9 85.8 0:37 0:1 learnable sparsity
AutoSparse 71.95 64:87 9.84 86:36 0:30 0:10=.8,=0 @ epoch 20
AutoSparse 71.95 64:18 10:8 87:72 0:25 0:080=.6,=0 @ epoch 20
Table 3: MobileNetV1 on ImageNet: Comparing accuracy, sparsity and FLOPS (dense 1) for
training and inference for selected sparse training methods. AutoSparse achieves signiÔ¨Åcantly higher
accuracy for comparable sparsity. For AutoSparse 74:98% sparsity,= 0:4is decayed till epoch 90
and then set to 0. For AutoSparse ( 87:72%,86:36%) sparsity, (= 0:6,= 0:8) decayed till epoch
20 and then set to 0. Setting = 0at epochtimplies identical forward and backward sparsity after
epocht. Standard deviation for AutoSparse is 0:04.
Method BLEU S% BLEU(S) Drop% comment
STR 27:83 59:25 27:49 1:22 our implementation
AutoSparse 27:83 60:99 27:57 0:930=0.4,= 0at epoch 10
Table 4: Transformer on WMT: AutoSparse achieves better accuracy at higher sparsity than STR.
Standard deviation for AutoSparse BLEU and STR BLEU are 0:08and0:09, respectively.
5.2 A UTOSPARSE FOR LANGUAGE MODELS
ForLanguage models , we applied our AutoSparse on Transformer model using exponential decay for
0= 0:4. This annealing schedule resulted in a better trade-off between accuracy and sparsity. Also,
we set= 0 after 10epochs to take advantage of more sparse compute in backward pass. With
baseline dense BLEU 27:83, AutoSparse achieves a BLEU score of 27:57(0.93% drop) with overall
sparsity 60:99%. For STR (= 0), we used weight decay 0:01with initials=-20, and we achieve
BLEU score 27.49 (drop 1.22%) at sparsity 59.25%.
6 D ISCUSSION
The purpose of gradient annealing is to establish a trade-off between dynamic, learnable sparsity
and (high) accuracy where we can beneÔ¨Åt from sparse compute from early epochs. Our AutoSparse
can learn a sparsity distribution that is signiÔ¨Åcantly more FLOPS-efÔ¨Åcient for inference comparing
to uniform sparsity methods. However, at extreme sparsity (where we sacriÔ¨Åce accuracy) we have
observed that sparse-to-sparse training methods using much longer epochs ( 2:5-5) with uniform
sparsity recovers loss of accuracy signiÔ¨Åcantly (MEST, RigL), and can outperform learnable sparsity
methods in terms training FLOPS. Learnable sparsity methods reach target sparsity in a gradual
manner incurring more FLOPS count at early epochs, whereas uniform sparsity methods have less
FLOPS per iterations (ignoring other overheads for longer training). We like to study our AutoSparse
for extended training epochs, along with (a) infrequent dense gradient update, (b) sparse gradient
update in order to improve accuracy, training FLOPS and memory footprint. Also, appropriate choice
of gradient annealing may depend on the learning hyperparameters, e.g., learning rate decay, choice
of optimizer, amount of gradient Ô¨Çow etc. It would be interesting to automate various choices of
gradient annealing across workloads.
9

--- PAGE 10 ---
Published as SNN workshop paper at ICLR 2023
REFERENCES
D. Amodei and D. Hernandez. AI and Compute. https://openai.com/blog/
ai-and-compute/ .
N. Ardalani, J. Hestness, and G. Diamos. Empirically Characterizing Overparameterization Impact
on Convergence. In https://openreview.net/forum?id=S1lPShAqFm , 2019.
S. Arora, N. Cohen, and E. Hazan. On the optimization of deep networks: Implicit acceleration by
overparameterization. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th Interna-
tional Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research ,
pp. 244‚Äì253. PMLR, 10‚Äì15 Jul 2018. URL https://proceedings.mlr.press/v80/
arora18a.html .
K. Azarian, Y . Bhalgat, J. Lee, and T. Blankevoort. Learned Threshold Pruning. In https:
//arxiv.org/pdf/2003.00075.pdf , 2021.
M. Belkin, D. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning practice and
the classical bias-variance trade-off. Proceedings of the National Academy of Sciences , 116(32):
15849‚Äì15854, 2019. doi: 10.1073/pnas.1903070116. URL https://www.pnas.org/doi/
abs/10.1073/pnas.1903070116 .
Y . Bengio, N. L√©onard, and A. Courville. Estimating or propagating gradients through stochastic
neurons for conditional computation. In arXiv preprint arXiv:1308.3432 , 2013.
Dipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar, Sasikanth Avancha,
Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat Kaul, Evangelos Georganas,
et al. Mixed precision training of convolutional neural networks using integer operations. arXiv
preprint arXiv:1802.00930 , 2018.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. A large-scale hierarchical image
database. In IEEE conference on computer vision and pattern recognition , 2009.
T. Dettmers and L. Zettlemoyer. Sparse Networks from Scratch: Faster Training without Losing
Performance. In https://arxiv.org/pdf/1907.04840.pdf , 2019.
U. Evci, T. Gale, J. Menick, P.S. Castro, and E. Elsen. Rigging the Lottery: Making All Tickets
Winners. In https://arxiv.org/pdf/1911.11134.pdf , 2021.
U. Evci, B. van Merri√´nboer, T. Unterthiner, M. Vladymyrov, and F. Pedregosa. GRADMAX:
Growing Neural Networks Using Gradient Information. In International Conference on Learning
Representations (ICLR) , 2022.
J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In
International Conference on Learning Representations , 2019. URL https://openreview.
net/forum?id=rJl-b3RcF7 .
T. Gale, E. Elsen, and S. Hooker. The State of Sparsity in Deep Neural Networks. In https:
//arxiv.org/pdf/1902.09574.pdf , 2019.
Y . Guo, A. Yao, and Y . Chen. Dynamic Network Surgery for EfÔ¨Åcient DNNs. In Advances in Neural
Information Processing Systems (NeurIPS) , 2016.
Y . Guo, H. Yuan, J. Tan, Z. Wang, S. Yang, and J. Liu. GDP: Stabilized Neural Network Pruning via
Gates with Differentiable Polarization. In https://arxiv.org/pdf/2109.02220.pdf ,
2021.
S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights and connections for efÔ¨Åcient neural
network. In In Advances in Neural Information Processing Systems (NeurIPS) , 2015.
K. He, X. Zhang, S. ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of
the IEEE conference on computer vision and pattern recognition , 2016.
10

--- PAGE 11 ---
Published as SNN workshop paper at ICLR 2023
T. HoeÔ¨Çer, D Alistarh, T. Ben-Nun, N. Dryden, and A. Peste. Sparsity in Deep Learning: Pruning
and growth for efÔ¨Åcient inference and training in neural networks. Journal of Machine Learning
Research , pp. 1‚Äì124, 2021.
A.G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam.
Mobilenets: EfÔ¨Åcient convolutional neural networks for mobile vision applications. In arXiv
preprint arXiv:1704.04861 , 2017.
S.M. Jayakumar, R. Pascanu, J.W. Rae, S. Osindero, and E. Elsen. Top-KAST: Top-K Always Sparse
Training. In , 2021.
Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee,
Sasikanth Avancha, Dharma Teja V ooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen,
Jiyan Yang, Jongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudarshan Srinivasan,
Abhisek Kundu, Misha Smelyanskiy, Bharat Kaul, and Pradeep Dubey. A study of bÔ¨Çoat16 for
deep learning training, 2019. URL https://arxiv.org/abs/1905.12322 .
A. Kusupati, V . Ramanujan, R. Somani, M. Wortsman, P. Jain, S. Kakade, and A. Farhadi. Soft
Threshold Weight Reparameterization for Learnable Sparsity. In https://arxiv.org/abs/
2002.03231 , 2020.
N. Lee, T. Ajanthan, and P.H.S Torr. SNIP: Single-shot Network Pruning based on Connection
Sensitivity. In International Conference on Learning Representations (ICLR) , 2019.
Z. Li, E. Wallace, S. Shen, K. Lin, K. Keutzer, D. Klein, and J. Gonzalez. Train Big, Then
Compress: Rethinking Model Size for EfÔ¨Åcient Training and Inference of Transformers. In
https://arxiv.org/pdf/2002.11794.pdf , 2020.
T. Lin, S. U. Stich, L. Barba, D. Dmitriev, and M. Jaggi. Dynamic model pruning with feedback. In
International Conference on Learning Representations (ICLR) , 2020.
J. Liu, Z. Xu, R. Shi, R.C.C Cheung, and H.K.H So. Dynamic Sparse Training: Find EfÔ¨Åcient Sparse
Network from Scratch with Trainable Masked Layers. In International Conference on Learning
Representations (ICLR) , 2020.
Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul. Mixed precision
training with 8-bit Ô¨Çoating point, 2020. URL https://openreview.net/forum?id=
HJe88xBKPr .
P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston,
O. Kuchaiev, G. Venkatesh, et al. Mixed precision training. In arXivpreprintarXiv:
1710.03740 , 2017.
A. Mishra, J. A. Latorre, J. Pool, D. Stosic, D. Stosic, G. Venkatesh, C. Yu, and P. Micikevicius.
Accelerating Sparse Deep Neural Networks. In https://arxiv.org/abs/2104.08378 ,
2021.
D.C. Mocanu, E. Mocanu, P. Stone, P.H. Nguyen, M. Gibescu, and A. Liotta. Scalable training
of artiÔ¨Åcial neural networks with adaptive sparse connectivity inspired by network science. In
https://www.nature.com/articles/s41467-018-04316-3 , 2018.
H. Mostafa and X. Wang. Parameter EfÔ¨Åcient Training of Deep Convolutional Neural Networks
by Dynamic Sparse Reparameterization. In In International Conference on Machine Learning
(ICML) , 2019.
S. Narang, G. F. Diamos, S. Sengupta, and E. Elsen. Exploring sparsity in recurrent neural networks.
CoRR , abs/1704.05119, 2017. URL http://arxiv.org/abs/1704.05119 .
Md. A. Raihan and T.M. Aamodt. Sparse Weight Activation Training. In Conference on Neural
Information Processing Systems (NeurIPS 2020) , 2020.
P. Savarese, H. Silva, and M. Maire. Winning the Lottery with Continuous SparsiÔ¨Åcation. In
https://arxiv.org/pdf/1912.04427.pdf , 2021.
11

--- PAGE 12 ---
Published as SNN workshop paper at ICLR 2023
J. Schwarz, S. M. Jayakumar, R. Pascanu, P.E. Latham, and Y . W. Teh. Powerpropagation: A sparsity
inducing weight reparameterisation. In https://arxiv.org/pdf/2110.00296.pdf ,
2021.
N. Str√∂m. Sparse Connection And Pruning In Large Dynamic ArtiÔ¨Åcial Neural Networks. In
http://www.nikkostrom.com/publications/euro97/euro97.pdf , 1997.
X. Sun, J. Choi, C-Y Chen, N. Wang, S. Venkataramani, V . Srinivasan, X. Cui, W. Zhang, and
K. Gopalakrishnan. Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural
Networks. In In The Conference and Workshop on Neural Information Processing Systems , 2019.
H. Tanaka, D. Kunin, D.L. Yamins, and S. Ganguli. Pruning neural networks without any data
by iteratively conserving synaptic Ô¨Çow. In Advances in Neural Information Processing Systems
(NeurIPS) , 2020.
Z. Tang, L. Luo, B. Xie, Y . Zhu, R. Zhao, L. Bi, and C. Lu. Automatic Sparse Connectivity Learning
for Neural Networks. In https://arxiv.org/pdf/2201.05020.pdf , 2022.
J. van Amersfoort, M. Alizadeh, S. Farquhar, N. Lane, and Y . Gal. Single shot structured pruning
before training. In https://arxiv.org/abs/2007.00389 , 2020.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, ≈Å. Kaiser, and I. Polosukhin.
Attention Is All You Need. In Neural Information Processing Systems , 2017.
C. Wang, G. Zhang, and R. Grosse. PICKING WINNING TICKETS BEFORE TRAINING BY
PRESERVING GRADIENT FLOW. In International Conference on Learning Representations
(ICLR) , 2020.
N. Wang, J. Choi, D. Brand, C-Y Chen, and K. Gopalakrishnan. Training Deep Neural Networks with
8-bit Floating Point Numbers. In https://arxiv.org/pdf/1812.08011.pdf , 2018.
M. Wortsman, A. Farhadi, and M. Rastegari. Discovering neural wirings. In Conference on Neural
Information Processing Systems (NeurIPS , 2019.
G. Yuan, X. Ma, W. Niu, Z. Li, Z. Kong, N. Liu, Y . Gong, Z. Zhan, C. He, Q. Jin, S. Wang, M. Qin,
B. Ren, Y . Wang, S. Liu, and X. Lin. MEST: Accurate and Fast Memory-Economic Sparse Training
Framework on the Edge. In Neural Information Processing Systems (NeurIPS) , 2021.
Y . Zhang, M. Lin, M. Chen, Z. Xu, F. Chao, Y . Shen, K. Li, Y . Wu, and R. Ji. Optimizing Gradient-
driven Criteria in Network Sparsity: Gradient is All You Need. In https://arxiv.org/
pdf/2201.12826.pdf , 2022.
M.H. Zhu and S. Gupta. To prune, or not to prune: exploring the efÔ¨Åcacy of pruning for model
compression. In https://arxiv.org/pdf/1710.01878.pdf , 2017.
12

--- PAGE 13 ---
Published as SNN workshop paper at ICLR 2023
7 A PPENDIX A
7.1 G RADIENT COMPUTATION
LetI()denote an indicator function, h;idenote inner product, denote elementwise product, and
SdenoteSh;g.gshould be continuous so that we can apply gradient descend for learning. Using the
deÔ¨Ånition in eqn (1), loss gradients for W`ands`are
L(t) L(S(W(t);s(t));D);G(t)
` rS(W`;s`)L(t)
rW(t)
`S(W`;s`) G(t)
I;G(t)
I IfS(W(t)
`;s(t)
`)6= 0g+IfS(W(t)
`;s(t)
`) = 0g
(Loss grad for W)rW(t)
`L(t) G(t)
`G(t)
I (10)
(Loss grad for s)rs(t)
`L(t)  g0(s(t)
`)D
G(t)
`;sign(W(t)
`)G(t)
IE
(11)
Apart from the classiÔ¨Åcation loss, standard L2regularization is added on W`ands`,82[L]with
weight decay hyperparameter . Gradients received by W`ands`for regularization are W`and
s`, respectively. Parameter updates involve adding momentum to gradients, and multiplying by
learning rate . We choose Sigmoid function for g(s)as in Kusupati et al. (2020).
Figure 2: Comparing Dynamic Model Sparsity for AutoSparse, STR and OptG.
7.2 A BLATION STUDIES
We conducted additional studies on STR (Kusupati et al. (2020)) method to test if the average training
sparsity can be improved by selecting different hyper-parameters. We initialized the threshold
parameterssinitwith larger values to introduce sparsity in the early epochs ‚Äì we also appropriately
scaled the value of . Figure 3 shows that when sinitvalue is increased to  5from the original value
of 3200 ), the method introduces sparsity early in the training ‚Äì however the model quickly diverges
after about 5epochs.
7.3 L EARNING SPARSITY DISTRIBUTION
AutoSparse learns a signiÔ¨Åcantly different sparsity distribution than STR despite achieving similar
model sparsity. AutoSparse is able to produce higher sparsity in earlier layers which leads to
signiÔ¨Åcant reduction in FLOPS (Figure 4).
7.4 L EARNING SPARSITY BUDGET BY AUTO-TUNING(AUTOSPARSE -AT)
The main challenges of true sparse training are (1) the inherent sparsity (both the budget and the
distribution) of various (new) models trained on various (new) data might be unknown, (2) baseline
accuracy of dense training is unknown (unless we perform dense training). How can we produce a high
quality sparse solution via sparse training? We seek to answer this by tuning our GA hyperparameter
as shown in Algorithm 1. We have observed that increasing reduces sparsity (improves accuracy)
and vice versa. For auto-tuning of (dynamically balancing sparsity and accuracy) we want the
sparse loss to follow the loss trajectory of dense training. For this, we note the loss of few epochs
13

--- PAGE 14 ---
Published as SNN workshop paper at ICLR 2023
Figure 3: Ablation study on early sparse training on STR and AutoSparse. sinit= 5for all the cases.
STR 80:=:000017 (used for STR 80%), STR:=:000030517578125209 , AutoSparse 80 and
AutoSparse 90 both have = 0:000030517578125209 . Sparsity for both STR and STR 80 rapidly
grow in an uncontrolled manner to prune out all the parameters in few epochs, while AutoSparse
achieves sparsity in a controlled manner.
Figure 4: AutoSparse learns a different sparsity distribution from STR. STR achieves higher sparsity
for later layers (higher parameter density) whereas AutoSparse produces more sparsity for earlier
layers, leading to signiÔ¨Åcant FLOP reduction despite having the same Ô¨Ånal model sparsity.
of dense training (incurring more FLOPS count) and use it as a reference to measure the quality of
our sparse training. For AutoSparse, we initiate the sparse training with = 0:5(unbiased value as
2[0;1]) and note the average sparse training loss at each epoch. If sparse training loss exceeds
the dense loss beyond some tolerance, we increase to reduce the sparsity so that the sparse loss
closely follows the dense loss in the next epoch. On the other hand, when sparse loss is close to dense
loss, we reduce to explore more available sparsity. This tuning happens only in Ô¨Årst few epochs of
AutoSparse + AutoTune training and rest of the sparse training applies gradient annealing with the
tuned value of .
7.4.1 A UTOSPARSE +AUTO-TUNING EXPERIMENTS
We perform auto-tuning of for Resnet50 and MobileNetV1 on ImageNet dataset. We use identical
training hyper-parameters for both ResNet50 and MobileNetV1 for our AutoSparse+Auto-Tuning
along with identical initial unbiased 0= 0:5to understand the efÔ¨Åcacy of auto-tuning the gradient
annealing. This simpliÔ¨Åes the issues of manually-tunung many hyper-parameters.
Starting at an unbiased value 0:5,is tuned for Ô¨Årst 9epochs (10% of intended dense training epochs)
of sparse training using Algorithm 1. Figure 5 shows how is tuned differently for ResNet50 and
MobileNetV1 in a dynamic manner. This is because of the inherent sparsity budget for ResNet50 is
different from that of MobileNetV1 to produce a highly accurate sparse model. Figures 6a and 6b
show how the sparsity-accuracy trade-off is discovered from early epochs for both ResNet50 and
MobileNetV1. Table 5) summarizes various experiments.
14

--- PAGE 15 ---
Published as SNN workshop paper at ICLR 2023
Algorithm 1 AutoSparse + AutoTune
‚Ä¢Input ModelM, dataD, total #epochs T, tuning epochs T0,0, reference loss L2RT0
‚Ä¢ 0/* initialization */
‚Ä¢ Fort= 0;:::;T 1epochs
‚Ä¢ Train model MonDand calculate training loss Ltin (6)
‚Ä¢ Ift<T 0 /* Auto-tuning phase for */
‚Ä¢ IfLt(1 +"0)L[t], (1 +"1) /* reduce sparsity to control loss */
‚Ä¢ Else  (1 "2) /* reduceto explore more sparsity */
‚Ä¢ Else
‚Ä¢ Ift==T0,0 
‚Ä¢ 0Sigmoid-Cosine-Decay (t T0;T T0)in (4)
‚Ä¢ Ift>= 90 , 0/* resetafter certain epoch for compute beneÔ¨Åt*/
‚Ä¢Output Trained sparse Model M
Figure 5: Auto-tuning hyper-parameter that dynamically controls the trade-off between accuracy
and sparsity using average training loss of only 9epochs ( 10% budget of dense epochs) of dense
training. In remaining epochs, is decayed according to equation 4. 0= 0:5irrespective of the
network type.
(a) AutoSparse+AT ResNet50
 (b) AutoSparse+AT MobileNetV1
Figure 6: AutoSparse+AT: Our auto-tuned AutoSparse almost matches dense accuracy while dynami-
cally discovering the right amount of inherent sparsity by tuning using average training loss of only
9epochs ( 10% budget of dense epochs) of dense training run.
Our AutoSparse + AutoTune can discover this in an automated manner in a single experiment. This
saves a lot of training cost of manually tuning and re-running experiments to Ô¨Ånd the right balance of
sparsity-accuracy.
15

--- PAGE 16 ---
Published as SNN workshop paper at ICLR 2023
Network Base ("0;"1;"2) Top-1 (S) Sparsity Train F Test F
ResNet50 77:01 (:01;:05;:005) 76:74 78:6 ‚Äì ‚Äì
77:01 (:01;:05;:01) 76:58 80:1 0:59 0:14
MobileNetV1 71:95 (:01;:05;:005) 70:5 70:5 0:67 0:26
71:95 (:01;:05;:01) 70:43 71:2 ‚Äì ‚Äì
Table 5: AutoSparse +AutoTune (Algorithm 1) accuracy vs sparsity trade-off for a budget of 100
epochs. During auto-tuning of (Ô¨Årst 9epochs), we set a loss tolerance "0= 1% , increase or
decreaseby"1or"2, respectively. We use "1= 5% and"20:5%or1%. Initial0= 0:5is
dynamically tuned (Algorithm 1) and it is set to 0 at epoch 90. AutoSparse standard deviation for
ResNet50 and MobileNetV1 are 0:07and0:08, respectively.
16

--- PAGE 17 ---
Published as SNN workshop paper at ICLR 2023
8 A PPENDIX B
8.1 P YTORCH CODE FOR AUTOSPARSE WITH AUTOTUNE
This code is built using the code base for STR Kusupati et al. (2020). The main changes are: (1)
replace ReLU with non-saturating ReLU (class NSReLU), (2) decay the gradient scaling of negative
inputs of NSReLU (neg_grad which is denoted by in the paper) every epoch, (3) auto-tune this 
based on (dense) ref_loss for initial few epochs (function grad_annealing)
#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
def set_neg_grad(neg_grad_val = 0.5):
NSReLU.neg_grad = neg_grad_val
def get_neg_grad():
return NSReLU.neg_grad
def set_neg_grad_max(neg_grad_val = 0.5):
NSReLU.neg_grad_max = neg_grad_val
def get_neg_grad_max():
return NSReLU.neg_grad_max
#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
class NSReLU(torch.autograd.Function):
neg_grad = 0.5
neg_grad_max = 0.5
#topk = 0.5 # for backward sparsity
@staticmethod
def forward(self,x):
self.neg = x < 0
#k = int(NSReLU.topk * x.numel()) # for backward sparsity
#kth_val, kth_id = torch.kthvalue(x.view(-1), k) # for backward sparsity
#self.exclude = x < kth_val # for backward sparsity
return x.clamp(min=0.0)
@staticmethod
def backward(self,grad_output):
grad_input = grad_output.clone()
grad_input[self.neg] *= NSReLU.neg_grad
#grad_input[self.exclude] *= 0.0 # for backward sparsity
return grad_input
#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
def non_sat_relu(x):
return NSReLU.apply(x)
def sparseFunction(x, s, activation=torch.relu, g=torch.sigmoid):
return torch.sign(x)*activation(torch.abs(x)-g(s))
def initialize_sInit():
if parser_args.sInit_type == "constant":
return parser_args.sInit_value*torch.ones([1, 1])
#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
class STRConv(nn.Conv2d):
def __init__(self, *args, **kwargs):
super().__init__(*args, **kwargs)
self.activation = non_sat_relu
if parser_args.sparse_function == ‚Äòsigmoid‚Äô:
17

--- PAGE 18 ---
Published as SNN workshop paper at ICLR 2023
self.g = torch.sigmoid
self.s = nn.Parameter(initialize_sInit())
else:
self.s = nn.Parameter(initialize_sInit())
def forward(self, x):
sparseWeight = sparseFunction(self.weight, self.s, self.activation, self.g)
x = F.conv2d( x, sparseWeight, self.bias, self.stride, self.padding, self.dilation, self.groups )
return x
#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
from utils.conv_type import get_neg_grad, set_neg_grad, get_neg_grad_max, set_neg_grad_max
#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
def grad_annealing(epoch, loss, ref_loss):
# ref_loss: reference dense loss for auto-tuning
# number of auto-tuning epochs
old_neg_grad = get_neg_grad()
new_neg_grad = 0.0
if epoch < len(ref_loss):
dense_loss = Ô¨Çoat(ref_loss[str(epoch)])
eps_0 = 0.01
eps_1 = 0.05
eps_2 = 0.005
if loss > dense_loss * (1.0 + eps_0):
new_neg_grad = old_neg_grad * (1.0 + eps_1)
else:
new_neg_grad = old_neg_grad * (1.0 - eps_2)
else:
if epoch == len(ref_loss):
set_neg_grad_max(get_neg_grad())
new_neg_grad = _sigmoid_cosine_decay(
args.epochs - len(ref_loss), epoch - len(ref_loss), get_neg_grad_max())
set_neg_grad(new_neg_grad)
#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
def _cosine_decay(total_epochs, epoch, neg_grad_max):
PI = torch.tensor(math.pi)
return 0.5 * neg_grad_max * (1 +torch.cos(PI * epoch / Ô¨Çoat(total_epochs)))
def _sigmoid_decay( rem_total_epochs, rem_epoch, neg_grad_max):
Lmax = 6
Lmin = -6
return neg_grad_max * (1 - torch.sigmoid(torch.tensor(Lmin+(Lmax - Lmin) *
( Ô¨Çoat ( rem_epoch ) / rem_total_epochs ) ) ) )
def _sigmoid_cosine_decay(rem_total_epochs, rem_epoch, neg_grad_max):
cosine_scale = _cosine_decay(rem_total_epochs, rem_epoch, get_neg_grad_max())
sigmoid_scale = _sigmoid_decay(rem_total_epochs,
rem_epoch, get_neg_grad_max())
return max(cosine_scale, sigmoid_scale)
#‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
def train(train_loader, model, criterion, optimizer, epoch, ref_loss, args):
losses = AverageMeter("Loss", ":.3f")
top1 = AverageMeter("Acc@1", ":6.2f")
top5 = AverageMeter("Acc@5", ":6.2f")
18

--- PAGE 19 ---
Published as SNN workshop paper at ICLR 2023
# switch to train mode
model.train()
if epoch == 0:
set_neg_grad(args.init_neg_grad)
batch_size = train_loader.batch_size
num_batches = len(train_loader)
for i, (images, target) in tqdm.tqdm( enumerate(train_loader),
ascii=True, total=len(train_loader) ):
output = model(images)
loss = criterion(output, target.view(-1))
acc1, acc5 = accuracy(output, target, topk=(1,5))
losses.update(loss.item(), images.size(0))
top1.update(acc1.item(), images.size(0))
top5.update(acc5.item(), images.size(0))
optimizer.zero_grad()
loss.backward()
optimizer.step()
# ‚Äî anneal gradient every epoch ‚Äî
grad_annealing(epoch, losses.avg, ref_loss)
return top1.avg, top5.avg
19
