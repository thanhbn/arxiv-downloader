DSEE: Điều chỉnh hiệu quả nhúng độ thưa kép của các mô hình ngôn ngữ được đào tạo trước

Xuxi Chen1, Tianlong Chen1, Weizhu Chen2, Ahmed Hassan Awadallah2
Zhangyang Wang1,Yu Cheng2
1Đại học Texas tại Austin,2Tập đoàn Microsoft
{xxchen,tianlong.chen,atlaswang}@utexas.edu
{wzchen,hassanam,yu.cheng}@microsoft.com

Tóm tắt
Các mô hình được đào tạo trước khổng lồ đã trở thành trung tâm của xử lý ngôn ngữ tự nhiên (NLP), phục vụ như điểm khởi đầu để tinh chỉnh hướng tới một loạt các tác vụ downstream. Tuy nhiên, hai điểm đau đớn vẫn tồn tại đối với mô hình này: (a) khi các mô hình được đào tạo trước trở nên lớn hơn (ví dụ: 175B tham số cho GPT-3), ngay cả quá trình tinh chỉnh cũng có thể tốn thời gian và tốn kém về mặt tính toán; (b) mô hình được tinh chỉnh có cùng kích thước với điểm khởi đầu của nó theo mặc định, điều này không hợp lý do chức năng chuyên biệt hơn của nó, cũng như không thực tế vì nhiều mô hình được tinh chỉnh sẽ được triển khai trong các môi trường hạn chế tài nguyên. Để giải quyết những điểm đau này, chúng tôi đề xuất một khung cho việc tinh chỉnh hiệu quả tài nguyên và tham số bằng cách tận dụng prior độ thưa trong cả cập nhật trọng số và trọng số mô hình cuối cùng. Khung được đề xuất của chúng tôi, được gọi là Điều chỉnh hiệu quả nhúng độ thưa kép (DSEE), nhằm đạt được hai mục tiêu chính: (i) tinh chỉnh hiệu quả tham số - bằng cách thực thi các cập nhật thứ hạng thấp nhận biết độ thưa trên đỉnh các trọng số được đào tạo trước; và (ii) suy luận hiệu quả tài nguyên - bằng cách khuyến khích cấu trúc trọng số thưa hướng tới mô hình được tinh chỉnh cuối cùng. Chúng tôi tận dụng độ thưa theo hai hướng này bằng cách khai thác cả các mẫu thưa không có cấu trúc và có cấu trúc trong các mô hình ngôn ngữ được đào tạo trước thông qua một cách tiếp cận thống nhất. Các thí nghiệm mở rộng và điều tra sâu, với các backbone mạng đa dạng (tức là BERT, RoBERTa và GPT-2) trên hàng chục bộ dữ liệu, liên tục chứng minh hiệu quả tham số/suy luận ấn tượng, trong khi duy trì hiệu suất downstream cạnh tranh. Ví dụ, DSEE tiết kiệm khoảng 25% FLOPs suy luận trong khi đạt được hiệu suất tương đương, với 0,5% tham số có thể huấn luyện trên BERT. Mã nguồn có sẵn tại https://github.com/VITA-Group/DSEE.

1 Giới thiệu

Hầu hết các ứng dụng NLP gần đây đều đã tuân theo mô hình đào tạo trước rồi tinh chỉnh, bắt đầu từ một mô hình được đào tạo trước khổng lồ và tinh chỉnh nó hướng tới các tác vụ downstream. Tinh chỉnh truyền thống hoạt động thông qua việc cập nhật tất cả các tham số trong mô hình được đào tạo trước. Tuy nhiên, khi kích thước của các mô hình được đào tạo trước tăng lên, việc cập nhật tất cả các tham số trở nên ít khả thi hơn trong hầu hết các tình huống thực tế, do yêu cầu bộ nhớ và tính toán đắt đỏ. Ví dụ, BERT BASE (Devlin et al., 2019) có 110M tham số có thể huấn luyện, trong khi GPT-2 (Radford et al., 2019) có tới 1,5B và phiên bản lớn nhất của GPT-3 (Radford et al., 2019) có tới 175B tham số có thể huấn luyện đáng kinh ngạc. Do đó, tinh chỉnh thông thường của các mô hình lớn hơn có thể yêu cầu hàng trăm giờ GPU. Một nhược điểm khác của mô hình này là nó yêu cầu lưu trữ nhiều tham số như trong các mô hình được đào tạo trước quy mô lớn cho mỗi tác vụ downstream, điều này tạo ra những trở ngại cho việc triển khai trong các môi trường hạn chế tài nguyên thực tế.

Một giải pháp để giải quyết yêu cầu tài nguyên rộng rãi của tinh chỉnh thông thường là cắt tỉa mô hình (LeCun et al., 1990; Han et al., 2015; Ren et al., 2018; He et al., 2017; Liu et al., 2017), trong đó các trọng số không cần thiết được loại bỏ để thu nhỏ kích thước mô hình. Ví dụ, Chen et al. (2021b) tận dụng điều chuẩn ℓ1 để loại bỏ các đầu attention không đáng kể và đạt được 35∼45% thời gian huấn luyện với hiệu suất tương đương; Chen et al. (2021a); Dao et al. (2022) tận dụng các ma trận thưa với cấu trúc cố định để giảm kích thước của các mô hình được đào tạo trước. Tất cả các nghiên cứu này cho thấy sự xuất hiện tự nhiên của độ thưa trong quá trình tinh chỉnh một mô hình được đào tạo trước có mục đích chung, thành một số chức năng downstream chuyên biệt. Một giải thích tiềm năng về lý do tại sao độ thưa xuất hiện là các tập con khác nhau của các tham số có thể chịu trách nhiệm cho các tác vụ downstream và miền dữ liệu khác nhau (Sanh et al., 2020). Tuy nhiên, việc xác định các mặt nạ thưa phù hợp có thể gây gánh nặng: tinh chỉnh một mô hình ngôn ngữ được đào tạo trước lớn như GPT-3 chỉ cho một bước tiêu thụ ít nhất 1,2TB VRAM và yêu cầu 96 NVIDIA Tesla (Hu et al., 2021), và các phương pháp này hoặc yêu cầu quyền truy cập vào các trọng số được đào tạo trước hoặc giới thiệu các hệ số có thể học thêm (chẳng hạn như điểm quan trọng của các đầu attention).

Một lựa chọn thay thế song song là thiết kế các thuật toán tinh chỉnh hiệu quả tham số, nhằm tối ưu hóa một phần nhỏ trọng số trong khi cố định hầu hết chúng khi tinh chỉnh trên các tác vụ downstream. Các công trình tiên phong theo hướng này, sử dụng adapters (Houlsby et al., 2019), embeddings có thể học (Li and Liang, 2021; Liu et al., 2021), phân rã thứ hạng thấp (Hu et al., 2021) hoặc sự kết hợp của chúng (He et al., 2021), có thể giảm đáng kể số lượng tham số có thể huấn luyện trong khi bảo toàn hiệu suất tinh chỉnh tốt. Mặc dù các phương pháp này có thể cải thiện đáng kể hiệu quả lưu trữ và triển khai của các mô hình, có hai rào cản chính: (i) chúng không mang lại bất kỳ lợi ích hiệu quả suy luận nào vì các trọng số được đào tạo trước đầy đủ vẫn được yêu cầu để tính toán đầu ra; và (ii) các phương pháp hiện tại giả định rằng các cập nhật trên trọng số được đào tạo trước hoặc là thưa (Guo et al., 2020) hoặc thứ hạng thấp (Hu et al., 2021), tuy nhiên những giả định đó có thể được đơn giản hóa quá mức (Yu et al., 2017) và bị hạn chế quá mức để cho phép các cập nhật hiệu quả. Những quan sát này đã truyền cảm hứng cho chúng tôi khám phá các phương pháp hiệu quả tham số tốt hơn.

Để cải thiện cả hiệu quả tài nguyên và tham số trong quá trình tinh chỉnh mô hình, chúng tôi rõ ràng dựa vào prior của độ thưa cho cả cập nhật trọng số và trọng số cuối cùng, và thiết lập khung điều chỉnh hiệu quả nhúng độ thưa kép (DSEE). Bắt đầu từ một mô hình được đào tạo trước, DSEE đầu tiên áp dụng một cập nhật trọng số thứ hạng thấp nhận biết độ thưa để đạt được hiệu quả tham số của quá trình tinh chỉnh; và sau đó thực thi một cấu trúc trọng số thưa trực tiếp từ các cập nhật trọng số bằng masking để đạt được hiệu quả tài nguyên của mô hình được tinh chỉnh tại thời gian suy luận. Đóng góp của chúng tôi có thể được tóm tắt như sau:

• Chúng tôi đề xuất điều chỉnh hiệu quả nhúng độ thưa kép, thống nhất cập nhật trọng số hiệu quả tham số nhận biết độ thưa và trọng số được đào tạo trước thưa trong việc tinh chỉnh các mô hình được đào tạo trước khổng lồ. Đây là nỗ lực đầu tiên hướng tới việc tối ưu hóa đồng thời cả hiệu quả tham số của quá trình tinh chỉnh và hiệu quả tài nguyên của mô hình được tinh chỉnh.

• Cả priors thưa không có cấu trúc và có cấu trúc đều được điều tra trong thuật toán DSEE được đề xuất của chúng tôi. Đối với các cập nhật trọng số, prior độ thưa được tiêm nâng cao các sơ đồ cập nhật hiệu quả tham số hiện có (ví dụ: phân rã thứ hạng thấp). Đối với các trọng số cuối cùng, chúng tôi rút ra các mặt nạ thưa vượt trội, hoặc không có cấu trúc hoặc có cấu trúc, trực tiếp từ các cập nhật trọng số, không yêu cầu tham số bổ sung hay truy cập vào các trọng số được đào tạo trước và tiết kiệm chi phí sparsification.

• Các thí nghiệm mở rộng chứng minh tính hiệu quả của đề xuất của chúng tôi trên các mô hình ngôn ngữ được đào tạo trước đại diện khác nhau (BERT, GPT-2 và RoBERTa) và trên các benchmark đánh giá đa dạng (E2E, DART, WebNLG và GLUE). Trên GPT-2, các phương pháp của chúng tôi có thể đạt được điểm BLUE {69,5, 54,9, 47,5} với 0,1% tham số có thể huấn luyện trên {E2E, WebNLG, DART} với 20% tham số được loại bỏ trong trọng số được đào tạo trước. Trên BERT, DSEE có thể tinh chỉnh chỉ 0,5% tham số và tiết kiệm khoảng 25% FLOPs suy luận, trong khi mất ít hơn 2% hiệu suất.

2 Công trình liên quan

Cắt tỉa và Sparsification Cắt tỉa là một kỹ thuật nén mô hình cổ điển có thể giảm số lượng tham số, có thể mang lại hiệu quả huấn luyện và suy luận. Các nhà nghiên cứu đã đề xuất một số phương pháp cắt tỉa cho các mô hình ngôn ngữ được đào tạo trước: McCarley et al. (2019); Chen et al. (2021b) cắt tỉa các đầu attention có ít đóng góp hơn trong quá trình tinh chỉnh; Sanh et al. (2020) đề xuất một tiêu chí cắt tỉa nhắm mục tiêu vào sự thay đổi trọng số sau huấn luyện, phù hợp với transfer learning tốt hơn; Wang et al. (2020) kết hợp phân rã thừa số thứ hạng thấp và điều chuẩn ℓ0 để cắt tỉa. Gần đây, có một loạt các công trình sparsification sử dụng các mặt nạ thưa với cấu trúc cụ thể, được gọi là Butterflies, và đạt được hiệu quả cao trong việc đào tạo trước các mô hình (Chen et al., 2021a) hoặc tinh chỉnh trên các tác vụ downstream (Dao et al., 2022). Tuy nhiên, các phương pháp này không cho phép các cập nhật hiệu quả tham số.

Phân rã thứ hạng thấp Xấp xỉ thứ hạng thấp (Ye, 2005) có ứng dụng rộng rãi trong cộng đồng machine learning và được nghiên cứu rộng rãi. Một tình huống cổ điển là phân tích thành phần chính mạnh mẽ (Candès et al., 2011), phân rã một ma trận thành một thành phần thứ hạng thấp cộng với một thành phần thưa. Tài liệu hiện có cho thấy rằng trong deep learning, các mô hình over-parameterized được học thường tự nhiên mang theo các cấu trúc trọng số thứ hạng thấp xấp xỉ (Oymak et al., 2019; Yu et al., 2017).

Một số (Jaderberg et al., 2014; Povey et al., 2018; Sainath et al., 2013; Zhang et al., 2014; Zhao et al., 2016) đã áp đặt rõ ràng ràng buộc thứ hạng thấp trong quá trình huấn luyện. Wang et al. (2020); Hu et al. (2021) sử dụng phân rã thứ hạng thấp để thu nhỏ kích thước mô hình và cắt giảm các tham số có thể huấn luyện trong quá trình tinh chỉnh. Tuy nhiên, theo hiểu biết tốt nhất của chúng tôi, việc tích hợp các cấu trúc thưa và thứ hạng thấp chưa bao giờ được nghiên cứu trước đây cho việc tinh chỉnh hiệu quả các mô hình ngôn ngữ được đào tạo trước.

Thích ứng hiệu quả tham số. Tinh chỉnh hiệu quả tham số nhằm giảm số lượng tham số có thể huấn luyện khi tinh chỉnh các mô hình trên các miền downstream khác nhau. Không giống như cắt tỉa, nó nhằm thích ứng các mô hình với ít tham số hơn thay vì xây dựng các mô hình thưa. Nhiều cách tiếp cận khác nhau được đề xuất để đạt được mục tiêu này: Rebuffi et al. (2017); Houlsby et al. (2019) chèn và chỉ huấn luyện các adapters giữa các lớp hiện có, có tham số ít hơn nhiều so với các mô hình được đào tạo trước. Guo et al. (2020) tận dụng điều chuẩn ℓ0 để hạn chế số lượng phần tử khác không trong các vector cập nhật. Lester et al. (2021); Li and Liang (2021); Liu et al. (2021) giới thiệu điều chỉnh prompt hiệu quả chỉ tối ưu hóa một vector liên tục nhỏ cụ thể cho tác vụ. Zaken et al. (2021) tinh chỉnh chỉ các số hạng bias bên trong các mô hình. Hu et al. (2021) đề xuất một phương pháp dựa trên phân rã thứ hạng thấp, và He et al. (2021) kết hợp các phương pháp dựa trên thứ hạng thấp và adapter để tinh chỉnh hiệu quả. Tuy nhiên, các mô hình được tinh chỉnh được tạo ra bởi các phương pháp này có cùng số lượng trọng số với các mô hình được đào tạo trước; do đó chúng không đóng góp hiệu quả tài nguyên.

3 Phương pháp

Trong phần này, chúng tôi mô tả ký hiệu và định nghĩa của chúng tôi về tạo độ thưa và tinh chỉnh hiệu quả tham số trong Phần 3.1, và sau đó giới thiệu các thuật toán tinh chỉnh hiệu quả nhúng độ thưa kép trong Phần 3.2 và 3.3.

3.1 Kiến thức cơ bản

Sparsification và tinh chỉnh hiệu quả tài nguyên. Chúng tôi áp dụng cả phương pháp cắt tỉa không có cấu trúc và có cấu trúc để tạo ra độ thưa. Chúng có thể dẫn đến hiệu quả tài nguyên bao gồm tiết kiệm bộ nhớ và tính toán.

Cho W ∈ Rm×n là một ma trận trọng số, cắt tỉa nhằm tìm một mặt nạ nhị phân M ∈ {0,1}m×n được áp dụng cho W và tạo ra một trọng số thưa W ⊙ M. Các trọng số tại các vị trí mà M có giá trị "0" được coi là bị cắt tỉa. Các phương pháp cắt tỉa có thể được phân loại thành hai lớp theo cấu trúc của M: Đối với cắt tỉa không có cấu trúc trong đó M không có các cấu trúc thưa như hàng và cột, chi phí bộ nhớ được tiết kiệm do ít tham số khác không hơn; đối với cắt tỉa có cấu trúc, nó cũng giúp tiết kiệm chi phí tính toán vì các trọng số thưa có thể nhỏ hơn về kích thước. Một trong những phương pháp cắt tỉa không có cấu trúc được sử dụng rộng rãi nhất là độ lớn trọng số (Han et al., 2015), tức là loại bỏ các trọng số có giá trị tuyệt đối nhỏ nhất. Một phương pháp cắt tỉa có cấu trúc phổ biến trong lĩnh vực NLP là cắt tỉa đầu (McCarley et al., 2019), cố gắng loại bỏ các đầu attention không quan trọng khỏi mô hình.

Tinh chỉnh hiệu quả tham số. Để tận dụng kiến thức trong các trọng số được đào tạo trước W, các mô hình downstream học cập nhật trọng số cụ thể cho tác vụ ∆W thông qua tinh chỉnh và tạo ra dự đoán với trọng số W + ∆W, trong đó đầu ra của các mô hình được tính toán là (W + ∆W)x với x là đầu vào. Vì ∆W có cùng kích thước với W, việc học các ma trận cập nhật thường yêu cầu tài nguyên lớn khi kích thước của mô hình được đào tạo trước tăng lên. Tinh chỉnh hiệu quả tham số cố gắng giải quyết vấn đề này bằng cách sử dụng ít tham số có thể huấn luyện nhất có thể để biểu diễn ∆W, trong khi duy trì hiệu suất tinh chỉnh downstream cạnh tranh. Tài liệu trước đây đạt được mục tiêu thông qua việc sparsify các ma trận cập nhật trọng số ∆W (Guo et al., 2020) hoặc tận dụng các ma trận phân rã thứ hạng thấp để tính toán ∆W (Hu et al., 2021), trong khi trong công trình của chúng tôi, chúng tôi kết hợp cả hai.

3.2 Tinh chỉnh hiệu quả tham số nhúng độ thưa

Một nghiên cứu gần đây (Hu et al., 2021) thực thi ràng buộc thứ hạng thấp cho các tensor cập nhật trọng số ∆W, và đạt được sự cân bằng thỏa đáng giữa hiệu quả tham số và chất lượng mô hình. Tuy nhiên, như được tiết lộ thực nghiệm bởi (Yu et al., 2017), một phần thông tin quan trọng trong các trọng số được huấn luyện nằm rải rác bên ngoài không gian con thứ hạng thấp, tạo ra "phần dư" thưa. Được truyền cảm hứng bởi quan sát này, chúng tôi điều tra một không gian con thứ hạng thấp nhận biết độ thưa mới của ∆W, và giới thiệu thành phần đầu tiên của đề xuất của chúng tôi trong Hình 1, tức là tinh chỉnh hiệu quả tham số nhúng độ thưa.

Cụ thể, các cập nhật trọng số ∆W bao gồm hai thành phần như được minh họa trong Hình 1: (1) một thành phần thứ hạng thấp ∆Wl được xây dựng bởi phép nhân của hai ma trận U ∈ Rm×r và V ∈ Rr×n; và (2) một phần dư thưa ∆Ws = PΩ(S) trong đó S ∈ Rm×n là một ma trận có thể học, PΩ(S) = (si,j, (i,j) ∈ Ω; 0, (i,j) ∈ ΩC, i = 1,2,...,m, j = 1,2,...,n, wi,j là tham số của S tại vị trí (i,j), và Ω là một tập chỉ số chứa các vị trí của các phần tử khác không trong S. Ma trận cập nhật ∆W được biểu diễn là ∆Wl + ∆Ws, với U, V và S là các tham số có thể học trong khi Ω được cố định một khi đã xác định. So với tinh chỉnh đầy đủ có m×n tham số có thể huấn luyện cho một ma trận có kích thước m×n, phương pháp của chúng tôi chỉ có (m+n)×r + card(Ω) tham số có thể huấn luyện. Nếu r nhỏ hơn (m×n−card(Ω))/(m+n) ≪ 0,5 min{m,n}, phương pháp của chúng tôi có khả năng giảm các tham số có thể huấn luyện cho tinh chỉnh downstream. Trong thực tế, giá trị của r rất nhỏ so với m và n nên việc tiết kiệm là đáng kể.

Một câu hỏi cho pipeline trên là làm thế nào để tìm một tập chỉ số Ω chất lượng cao. Được truyền cảm hứng bởi quan sát rằng thành phần thứ hạng thấp ∆Wl có tương quan cao với cấu trúc thứ hạng thấp của W (Hu et al., 2021), chúng tôi giả thuyết rằng tập chỉ số Ω cũng nên có tương quan cao. Cụ thể hơn, chúng tôi giả thuyết rằng các phần dư thưa không nằm trong không gian con chiều thấp của W cũng có thể nằm ngoài ∆Wl, điều này thúc đẩy thiết kế cập nhật thưa ∆Ws. Chúng tôi hình thức hóa bài toán khám phá các phần dư thưa của W như một Phân tích Thành phần Chính Mạnh mẽ (Candès et al., 2011). Chính thức, chúng tôi nhằm giải quyết bài toán tối ưu hóa sau:

min U,V,S 1/2 ∥W−UV−S∥²F
s.t. rank(U) ≤ r, rank(V) ≤ r,
card(S) ≤ c. (1)

trong đó rank(·) và card(·) biểu thị hạng và số lượng phần tử khác không của một ma trận, tương ứng. S' biểu diễn các phần dư thưa không thể được khớp trong thành phần thứ hạng thấp AB, và chúng tôi thu được các vị trí của các phần tử có độ lớn khác không vào Ω. Để giải quyết Bài toán 1 một cách hiệu quả, chúng tôi áp dụng một thuật toán không cần SVD được gọi là GreBsmo (Zhou and Tao, 2013) (tham khảo Phần A.2). Thuật toán 1 tóm tắt quy trình chi tiết của việc xây dựng các tập chỉ số thưa Ω. Theo kinh nghiệm, chúng tôi đặt kích thước của Ω (tức là c) là 16 vì nó mang lại hiệu suất kiểm tra cao (tham khảo Phần 4.2) trong khi áp đặt ít chi phí phụ trên các tham số. Các giá trị ban đầu của V và S được đặt là 0 để các ma trận này không ảnh hưởng đến đầu ra vào đầu quá trình huấn luyện.

3.3 Điều chỉnh hiệu quả nhúng độ thưa kép (DSEE)

Thích ứng các mô hình được đào tạo trước với ∆Wl và ∆Ws có thể mang lại hiệu quả tham số đáng kể, nhưng không trực tiếp mang lại bất kỳ hiệu quả tài nguyên nào như chi phí bộ nhớ hoặc tính toán. Được thúc đẩy bởi điều đó, chúng tôi đề xuất một khung thống nhất được gọi là DSEE theo đuổi cả hiệu quả tham số và tài nguyên đồng thời. Chúng tôi tận dụng độ thưa trong trọng số của các mô hình được đào tạo trước để nâng cao hiệu quả tài nguyên, như được chứng minh trong Hình 1. Cụ thể hơn, chúng tôi dẫn xuất các mặt nạ thưa M trực tiếp từ các cập nhật hiệu quả tham số ∆W, và áp dụng các mặt nạ thưa bằng cách cắt tỉa các trọng số được đào tạo trước W để tìm kiếm hiệu quả tài nguyên. Nó không yêu cầu tham số bổ sung để sparsify mô hình và không cần truy cập vào các trọng số được đào tạo trước cơ bản, điều này thuận lợi do chi phí sparsification thấp hơn.

Như được thể hiện trong Thuật toán 2, DSEE xử lý cắt tỉa không có cấu trúc và có cấu trúc cùng một lúc: đối với cắt tỉa không có cấu trúc, chúng tôi sắp xếp độ lớn của ∆W, tạo ra một mặt nạ thưa M bằng cách gán "1" cho vị trí mà ∆W có độ lớn lớn nhất và "0" cho phần còn lại; đối với cắt tỉa có cấu trúc, chúng tôi tính tổng độ lớn của ∆W của mỗi đầu và loại bỏ những đầu có điểm số ít nhất. Chúng tôi cũng thu nhỏ ∆W tương ứng bằng cách loại bỏ các cột trọng số tương ứng trong V và ∆Ws để khớp hình dạng trong khi giữ U nguyên vẹn. Một so sánh các tiêu chí cắt tỉa khác nhau được hiển thị trong Phần 4.2.1, chứng minh rằng ∆W là một lựa chọn vượt trội do hiệu suất tác vụ downstream cao và không cần truy cập vào các trọng số được đào tạo trước W.

Cho một ngân sách tham số, số lượng tham số trên mỗi module giảm nếu chúng ta chọn thích ứng nhiều module hơn, điều này áp đặt một sự cân bằng. Chúng tôi nghiên cứu các lựa chọn khác nhau của các module để thích ứng trong Phần 4.2.2, và chúng tôi thấy rằng các module tối ưu để thích ứng là Wq và Wv, trong đó Wq và Wv đại diện cho các trọng số projection cho query và value trong các đầu attention. Vì một số module không được thích ứng trong quá trình tinh chỉnh (tức là ∆W = 0), chúng tôi cắt tỉa chúng riêng biệt theo độ lớn của các trọng số được đào tạo trước tương ứng. Sau khi áp dụng mặt nạ M cho các trọng số được đào tạo trước W, chúng tôi điều chỉnh thông thường ∆Wl (= UV) và ∆Ws (= PΩ(S)) trong vài epoch để khôi phục hiệu suất (Han et al., 2015).

4 Kết quả thí nghiệm

Bộ dữ liệu và mô hình. Chúng tôi sử dụng ba mô hình ngôn ngữ được đào tạo trước cổ điển trong các thí nghiệm của chúng tôi: BERT BASE (Devlin et al., 2019), RoBERTa LARGE (Liu et al., 2019) và GPT-2 (Radford et al., 2019), có 12/24/24 lớp với kích thước ẩn 768/1024/1024 và 110/380/354M tham số có thể huấn luyện, tương ứng. Đối với BERT và RoBERTa, chúng tôi đánh giá trên các benchmark GLUE (Wang et al., 2018), và đối với GPT-2 chúng tôi sử dụng E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017) và DART (Nan et al., 2021).

Chi tiết huấn luyện và đánh giá. Đối với BERT và RoBERTa, chúng tôi tuân theo các thiết lập mặc định trong Wolf et al. (2019); Devlin et al. (2019). Chúng tôi sử dụng trình tối ưu AdamW (Loshchilov and Hutter, 2017) cho tinh chỉnh downstream, và kích thước batch là 32 cho BERT và RoBERTa, và kích thước batch là 2 cho GPT-2. Các siêu tham số còn lại cho huấn luyện được báo cáo trong Bảng 11.

Chỉ số đánh giá. Đối với benchmark GLUE, chúng tôi báo cáo độ chính xác, tương quan Matthew và tương quan Pearson trong đánh giá. Trên GPT-2, chúng tôi sử dụng BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006) và NIST (Doddington, 2002) làm chỉ số đánh giá. Để đánh giá hiệu quả của các mô hình, chúng tôi báo cáo số lượng tham số có thể huấn luyện để đo hiệu quả tham số, số lượng tham số tổng (số lượng tham số khác không trong mô hình) để đo hiệu quả tài nguyên, và FLOPs cho hiệu quả tính toán.

Baselines. Trên BERT và RoBERTa, chúng tôi tiến hành các thí nghiệm toàn diện với các phương pháp baseline sau: ❶Fine-tune: trực tiếp tinh chỉnh mô hình đầy đủ; ❷EarlyBERT (Chen et al., 2021b): học điểm quan trọng cho các đầu và thực hiện cắt tỉa dựa trên chúng sau đó; ❸BERT Tickets (Chen et al., 2020): cắt tỉa không có cấu trúc dựa trên IMP; ❹P-Tuning v2 (Liu et al., 2021); ❺Bitfit (Zaken et al., 2021): chỉ tinh chỉnh các số hạng bias; và ❻LoRA: phân rã thứ hạng thấp, chỉ học ∆Wl (Hu et al., 2021). Trên GPT-2, chúng tôi tiến hành so sánh với nhiều phương pháp baseline: ❶Adapters (Houlsby et al., 2019): chèn adapters sau các lớp tuyến tính; ❷FT-Top2: chỉ tinh chỉnh 2 lớp trên cùng; ❸Prefix: điều chỉnh prefix được giới thiệu bởi Li and Liang (2021); và ❹LoRA.

4.1 Điều chỉnh hiệu quả với DSEE

Hiệu quả tham số với phần dư thưa. Để xác minh rằng việc sử dụng một thành phần thứ hạng thấp đơn giản ∆Wl có hạn chế, chúng tôi so sánh hiệu suất của nó với tinh chỉnh hiệu quả nhúng độ thưa của chúng tôi. Bảng 1 cho thấy rằng trên bốn benchmark (tức là SST-2, RTE, CoLA và MRPC), việc thêm một phần dư thưa trong các cập nhật trọng số có thể mang lại lợi ích hiệu suất: ở mức khoảng 600K tham số có thể huấn luyện, việc thêm phần dư thưa với chỉ 384 phần tử khác không (12×2×16 = 384) có thể tăng hiệu suất xác thực trên tất cả các benchmark ngoại trừ CoLA từ 0,23%∼1,09%; ở mức khoảng 300K tham số có thể huấn luyện, việc thêm phần dư thưa có thể mang lại lợi ích hiệu suất từ 0,34% đến 1,08% trên tất cả bốn benchmark.

Chúng tôi tiếp tục xác minh rằng việc thêm phần dư thưa ∆Ws có thể có lợi cho các tác vụ NLG với GPT-2. Bảng 2 cho thấy rằng dưới các mức độ tham số khác nhau, việc thêm phần dư thưa ∆Ws mang lại hiệu suất cao hơn cho hầu hết các chỉ số trên ba tác vụ. Ở mức 0,39M tham số, việc thêm phần dư thưa có thể cải thiện tất cả các chỉ số trên WebNLG và DART, và tăng nhẹ điểm NIST trên E2E. Ở mức 0,20M tham số, ∆Ws giúp tăng tất cả các chỉ số trên ba tác vụ. Chúng tôi cũng hiển thị độ lệch chuẩn trong Bảng 10.

Hiệu quả tài nguyên và tham số với mặt nạ thưa không có cấu trúc. Chúng tôi xác minh rằng DSEE có khả năng nâng cao cả hiệu quả tham số và tài nguyên, trong khi bảo toàn hiệu suất trên các tác vụ downstream, trên các kiến trúc khác nhau. Bảng 3 tóm tắt kết quả thí nghiệm trên BERT BASE, và chúng tôi quan sát thấy rằng việc giới thiệu các mẫu độ thưa không có cấu trúc bên trong các trọng số được đào tạo trước không chỉ mang lại hiệu quả tài nguyên (thể hiện bởi số lượng tham số tổng ít hơn) mà còn có thể cải thiện hiệu suất trên các tác vụ downstream. Cụ thể, ở 80% và 70% tham số tổng, DSEE có thể duy trì hiệu suất tương đương trên các tác vụ downstream, và thậm chí thể hiện sự tăng hiệu suất trên QQP, RTE và SST-2 so với LoRA. Ở mức 50% tham số, hiệu suất trên các bộ dữ liệu nhỏ hơn như CoLA và RTE giảm với biên độ rộng hơn; nhưng trên các bộ dữ liệu lớn hơn như QQP, DSEE có thể duy trì hiệu suất tương đương (<1,5% khoảng cách) sau sparsification.

Trên GPT-2, chúng tôi quan sát một xu hướng tương tự như được hiển thị trong Bảng 4. DSEE có thể đạt được hiệu suất vượt trội với các mẫu thưa không có cấu trúc với 80% tham số tổng so với tinh chỉnh toàn bộ mô hình, và vẫn có tính cạnh tranh cao với các baseline khác với ít tham số hơn trong mô hình. Chỉ sử dụng 50% tham số trong trọng số được đào tạo trước, DSEE có thể đạt được hiệu suất tương đương với tinh chỉnh đầy đủ trên E2E và DART.

Cuối cùng, chúng tôi xác thực nếu DSEE có thể hoạt động trên mô hình lớn hơn RoBERTa LARGE. Chúng tôi tiến hành thí nghiệm trên bốn bộ dữ liệu (CoLA, SST-2, QNLI và RTE), và trình bày kết quả trong Bảng 5. So với tinh chỉnh đầy đủ, LoRA và Adapter, phương pháp của chúng tôi đạt được hiệu suất tương đương trên bốn tác vụ downstream này và tiết kiệm tài nguyên cùng lúc. Khoảng cách hiệu suất tối đa là 1% nhưng 30% tham số trong các mô hình được loại bỏ.

Hiệu quả tài nguyên và tham số với mặt nạ thưa có cấu trúc. DSEE có thể thực hiện trực tiếp cắt tỉa có cấu trúc trên trọng số mà không cần tham số bổ sung như điểm quan trọng của các đầu. Trong Bảng 6 chúng tôi hiển thị hiệu suất của BERT BASE được cắt tỉa có cấu trúc trên một số tác vụ trong benchmark GLUE, trong đó chúng tôi nghiên cứu độ chính xác kiểm tra sau khi loại bỏ 3, 6 và 9 đầu attention trên SST-2, MNLI, QNLI và QQP, cũng như tỷ lệ FLOPs suy luận của mô hình. Thứ nhất, việc loại bỏ 3 đầu khỏi mô hình đạt được hiệu suất tương đương so với tinh chỉnh đầy đủ (cải thiện trên SST-2, MNLI và QNLI) và LoRA (cải thiện trên SST-2 và QQP), trong khi tận dụng FLOPs suy luận giảm. Thứ hai, việc loại bỏ 6 đầu khỏi mô hình sẽ dẫn đến hiệu suất thấp hơn vì một nửa số tham số trong các ma trận projection bị loại bỏ. Tuy nhiên, hiệu suất của DSEE vẫn cao hơn EarlyBERT. Cuối cùng, DSEE với 9 đầu được loại bỏ khỏi mô hình dẫn đến hiệu suất tương đương với EarlyBERT, nhưng số lượng tham số có thể huấn luyện nhỏ hơn đáng kể (0,6M so với 66M).

4.2 Ablation và Visualization

Chúng tôi nghiên cứu một số lựa chọn tham số và cung cấp visualization trong phần này.

4.2.1 Các tiêu chí khác nhau cho mặt nạ thưa

Chúng tôi thấy rằng độ lớn của các cập nhật trọng số (tức là |∆W|) là một giải pháp hiệu quả để bảo toàn hiệu suất với cả cắt tỉa không có cấu trúc và có cấu trúc. Chúng tôi tiến hành thí nghiệm trên các trọng số được thích ứng (tức là Wq và Wv), và so sánh với hai baseline: ❶Random: thực hiện cắt tỉa ngẫu nhiên trên các module được thích ứng; ❷|W + ∆W|: thực hiện cắt tỉa dựa trên độ lớn của trọng số được thích ứng cuối cùng. Bảng 7 hiển thị kết quả trên RTE và SST-2 với BERT BASE. Chúng ta có thể thấy từ bảng rằng: ❶thực hiện cắt tỉa không có cấu trúc mà không truy cập vào các trọng số được đào tạo trước có thể đạt được hiệu suất tương đương trên RTE và SST-2, chỉ yếu hơn một chút so với cắt tỉa với trọng số được thích ứng cuối cùng; ❷thực hiện cắt tỉa có cấu trúc theo ∆W mang lại hiệu suất cao nhất trên cả hai bộ dữ liệu sau huấn luyện. Những quan sát này xác minh tính hiệu quả của đề xuất của chúng tôi.

4.2.2 Các lựa chọn khác nhau của các module để thích ứng

Chúng tôi nghiên cứu các lựa chọn của các module để thích ứng cho DSEE trên RTE. Chúng tôi chọn các module có thể thích ứng trong Wq, Wk, Wv và Wo, đại diện cho ma trận projection cho query, key, value và output, tương ứng. Chúng tôi giữ số lượng tham số có thể huấn luyện ở cùng một mức và đặt mức độ thưa ở 30%. Bảng 9 tóm tắt hiệu suất với các trọng số được thích ứng khác nhau, chứng minh rằng việc thích ứng Wq và Wv mang lại hiệu suất cao nhất. Mỗi module sẽ được cung cấp ít tham số hơn khi thích ứng nhiều module hơn và mô hình có thể không được tinh chỉnh đầy đủ khi thích ứng ít module hơn và dẫn đến hiệu suất kém hơn.

Các phương pháp khác nhau để xác định Ω. Chúng tôi so sánh đề xuất của chúng tôi với các phương pháp khác nhau để xác định Ω từ các trọng số được đào tạo trước W: ❶Magnitude, chọn vị trí của các phần tử có độ lớn cao nhất vào Ω; ❶Random, ngẫu nhiên lấy mẫu các vị trí vào Ω. Kết quả được hiển thị trong Hình 2. Chúng ta có thể quan sát thấy rằng đề xuất của chúng tôi có thể xác định Ω chất lượng cao cho tinh chỉnh trên các tác vụ downstream, được thể hiện bởi hiệu suất liên tục cao hơn với các kích thước khác nhau của tập chỉ số Ω.

Các kích thước khác nhau của Ω. Chúng tôi tìm kiếm từ 8∼256 để tìm kích thước tối ưu của Ω. Ω với kích thước nhỏ hơn mang lại ít lợi ích hiệu suất hơn, và Ω với kích thước lớn hơn có thể làm hại hiệu quả. Hình 2 hiển thị mối quan hệ giữa kích thước của Ω và hiệu suất trên SST-2. Chúng tôi thấy lựa chọn tối ưu cho tác vụ này là 16 trong đó mô hình đạt được hiệu suất cao nhất. Do đó, chúng tôi mặc định đặt kích thước của Ω là 16 để đơn giản.

5 Kết luận

Bài báo này dựa vào prior của độ thưa và thiết lập khung DSEE. Đây là nỗ lực đầu tiên hướng tới việc tối ưu hóa đồng thời cả hiệu quả tham số của quá trình tinh chỉnh và hiệu quả tài nguyên của mô hình được tinh chỉnh. Trên các mô hình ngôn ngữ quy mô lớn hiện đại (ví dụ: BERT, GPT và RoBERTa) và trên một số bộ dữ liệu, DSEE liên tục chứng minh hiệu quả tham số và suy luận rất ấn tượng, ngoài việc bảo toàn hiệu suất transfer downstream cạnh tranh trên các tác vụ khác nhau. Công việc tương lai của chúng tôi nhắm mục tiêu mở rộng DSEE cho việc tinh chỉnh các mô hình được đào tạo trước về computer vision quy mô lớn và/hoặc đa phương thức.

Hạn chế Các mẫu thưa không có cấu trúc mà chúng tôi giới thiệu không thân thiện với phần cứng như các mẫu có cấu trúc, cho thấy tăng tốc của việc sử dụng các mẫu không có cấu trúc có thể bị hạn chế do việc triển khai. Số lượng tham số của các mô hình mà chúng tôi đang nghiên cứu chỉ ở mức 100∼300M, và các bộ dữ liệu tập trung vào GLUE, E2E, WebNLG và DART. Chúng tôi sẽ khái quát hóa cho các lựa chọn bộ dữ liệu rộng hơn trong các công trình tương lai.

6 Tác động đạo đức và rộng hơn

DSEE nhằm giảm số lượng tham số có thể huấn luyện khi tinh chỉnh các mô hình, có thể giúp tiết kiệm chi phí lưu các trọng số mới. Điều này có thể hữu ích cho các công ty đang tinh chỉnh các mô hình ngôn ngữ quy mô lớn trên các tác vụ downstream khác nhau, cho thấy công trình của chúng tôi có tác động rộng hơn có khả năng tích cực. Mặt khác, công trình của chúng tôi không có tác động đạo đức rõ ràng, vì chúng tôi tập trung vào điều chỉnh mô hình.
