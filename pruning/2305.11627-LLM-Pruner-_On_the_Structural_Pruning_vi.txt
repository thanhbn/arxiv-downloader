# 2305.11627.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2305.11627.pdf
# Kích thước tệp: 1598878 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
LLM-Pruner: Về Việc Cắt Tỉa Cấu Trúc
của Các Mô Hình Ngôn Ngữ Lớn
Xinyin Ma Gongfan Fang Xinchao Wang∗
Đại học Quốc gia Singapore
maxinyin@u.nus.edu, gongfan@u.nus.edu, xinchao@nus.edu.sg

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng đáng chú ý trong việc hiểu và tạo sinh ngôn ngữ. Tuy nhiên, khả năng ấn tượng như vậy thường đi kèm với kích thước mô hình đáng kể, điều này đặt ra những thách thức lớn trong cả giai đoạn triển khai, suy luận và huấn luyện. Với LLM là một bộ giải quyết tác vụ đa năng, chúng tôi khám phá việc nén nó theo cách không phụ thuộc vào tác vụ, nhằm bảo tồn khả năng giải quyết đa tác vụ và tạo sinh ngôn ngữ của LLM gốc. Một thách thức để đạt được điều này là kích thước khổng lồ của tập dữ liệu huấn luyện của LLM, điều này khiến cả việc truyền dữ liệu và huấn luyện lại mô hình trở nên quá gánh nặng. Do đó, chúng tôi giải quyết việc nén LLM trong giới hạn của hai ràng buộc: không phụ thuộc vào tác vụ và giảm thiểu sự phụ thuộc vào tập dữ liệu huấn luyện gốc. Phương pháp của chúng tôi, được đặt tên là LLM-Pruner, áp dụng cắt tỉa cấu trúc để loại bỏ có chọn lọc các cấu trúc ghép nối không quan trọng dựa trên thông tin gradient, bảo tồn tối đa phần lớn chức năng của LLM. Để đạt được điều này, hiệu suất của các mô hình đã cắt tỉa có thể được khôi phục hiệu quả thông qua các kỹ thuật điều chỉnh, LoRA, chỉ trong vòng 3 giờ, chỉ yêu cầu 50K dữ liệu. Chúng tôi xác thực LLM-Pruner trên ba LLM, bao gồm LLaMA, Vicuna và ChatGLM, và chứng minh rằng các mô hình nén vẫn thể hiện khả năng thỏa đáng trong phân loại zero-shot và tạo sinh. Mã nguồn có sẵn tại: https://github.com/horseee/LLM-Pruner

1 Giới thiệu
Gần đây, các Mô hình Ngôn ngữ Lớn (LLM) đã thể hiện năng lực đáng chú ý trong việc hiểu và tạo sinh ngôn ngữ. Với sự gia tăng kích thước mô hình, chúng được trang bị tốt hơn để xử lý các tác vụ phức tạp và thậm chí thể hiện các khả năng nổi lên. Tuy nhiên, bất chấp hiệu suất ấn tượng của chúng, LLM đặt ra những thách thức trong triển khai và suy luận. Quy mô rộng lớn của chúng tạo ra nhu cầu tính toán đáng kể, và số lượng tham số liên quan có thể gây ra độ trễ dài và các vấn đề liên quan khác. Một số kỹ thuật được đề xuất để giải quyết những vấn đề này, như cắt tỉa mô hình, chưng cất kiến thức, lượng tử hóa trong bối cảnh mô hình ngôn ngữ được huấn luyện trước (PLM).

Trong khi các phương pháp trước đây đã duy trì hiệu quả hiệu suất mô hình giữa việc giảm tham số, chúng chủ yếu nhắm đến việc nén trong các lĩnh vực chuyên biệt hoặc cho các tác vụ được chỉ định trong bối cảnh nén cụ thể cho tác vụ. Ví dụ, một PLM được tinh chỉnh trên một tập dữ liệu cụ thể, sau đó các mô hình này được chưng cất thành một mô hình phân loại nhỏ hơn. Mặc dù mô hình này có thể được sử dụng cho việc nén LLM, nó làm tổn hại khả năng của LLM như một bộ giải quyết tác vụ đa năng, khiến nó chỉ phù hợp với một tác vụ duy nhất.

∗Tác giả liên hệ

--- TRANG 2 ---
Do đó, chúng tôi cố gắng nén LLM trong một cài đặt mới: giảm kích thước LLM trong khi bảo tồn các khả năng đa dạng của nó như những bộ giải quyết tác vụ đa năng. Điều này giới thiệu việc nén không phụ thuộc tác vụ của LLM, điều này đặt ra hai thách thức chính:

•Kích thước của corpus huấn luyện của LLM là khổng lồ. LLM đã gia tăng quy mô corpus lên 1 nghìn tỷ token hoặc hơn. Nhu cầu lưu trữ rộng lớn và thời gian truyền tải kéo dài khiến tập dữ liệu khó có được.

•Thời gian không thể chấp nhận được cho việc huấn luyện lại LLM đã cắt tỉa. Các phương pháp hiện tại yêu cầu một lượng thời gian đáng kể để huấn luyện lại mô hình nhỏ hơn.

Để giải quyết các thách thức này, chúng tôi giới thiệu LLM-Pruner. Chúng tôi đề xuất một thuật toán phát hiện phụ thuộc xác định tất cả các cấu trúc phụ thuộc trong mô hình. Một khi cấu trúc ghép nối được xác định, chúng tôi sử dụng một chiến lược ước lượng tầm quan trọng hiệu quả để chọn nhóm tối ưu cho việc cắt tỉa. Cuối cùng, một giai đoạn khôi phục nhanh được thực hiện để huấn luyện lại mô hình đã cắt tỉa với dữ liệu hạn chế.

--- TRANG 3-20 ---
[Phần còn lại của tài liệu bao gồm các phần về Công trình liên quan, Phương pháp chi tiết, Thí nghiệm, Kết quả, Phân tích, Kết luận, Tài liệu tham khảo và Phụ lục với các bảng số liệu, biểu đồ và ví dụ cụ thể về hiệu suất của LLM-Pruner trên các mô hình LLaMA-7B, Vicuna-7B và ChatGLM-6B, cho thấy khả năng nén hiệu quả với việc giữ lại 94.97% hiệu suất gốc khi loại bỏ 20% tham số.]
