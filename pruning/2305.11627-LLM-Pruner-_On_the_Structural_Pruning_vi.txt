# 2305.11627.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2305.11627.pdf
# Kích thước tệp: 1598878 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
LLM-Pruner: Về Việc Cắt Tỉa Cấu Trúc
của Các Mô Hình Ngôn Ngữ Lớn
Xinyin Ma Gongfan Fang Xinchao Wang*
Đại học Quốc gia Singapore
maxinyin@u.nus.edu, gongfan@u.nus.edu, xinchao@nus.edu.sg
Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng đáng kinh ngạc trong việc hiểu và sinh ra ngôn ngữ. Tuy nhiên, khả năng ấn tượng như vậy thường đi kèm với kích thước mô hình đáng kể, điều này tạo ra những thách thức lớn trong cả giai đoạn triển khai, suy luận và huấn luyện. Với LLM là một công cụ giải quyết tác vụ đa năng, chúng tôi khám phá việc nén nó theo cách không phụ thuộc vào tác vụ, nhằm bảo tồn khả năng giải quyết đa tác vụ và sinh ngôn ngữ của LLM gốc.
Một thách thức để đạt được điều này là kích thước khổng lồ của tập dữ liệu huấn luyện của LLM, điều này làm cho cả việc truyền dữ liệu và huấn luyện lại mô hình trở nên quá tải. Do đó, chúng tôi giải quyết việc nén LLM trong phạm vi của hai ràng buộc: không phụ thuộc vào tác vụ và giảm thiểu sự phụ thuộc vào tập dữ liệu huấn luyện gốc. Phương pháp của chúng tôi, có tên LLM-Pruner, áp dụng cắt tỉa cấu trúc có chọn lọc loại bỏ các cấu trúc liên kết không quan trọng dựa trên thông tin gradient, tối đa hóa việc bảo tồn phần lớn chức năng của LLM. Để đạt được mục tiêu này, hiệu suất của các mô hình đã cắt tỉa có thể được khôi phục hiệu quả thông qua các kỹ thuật điều chỉnh, LoRA, chỉ trong 3 giờ, chỉ yêu cầu 50K dữ liệu. Chúng tôi xác thực LLM-Pruner trên ba LLM, bao gồm LLaMA, Vicuna và ChatGLM, và chứng minh rằng các mô hình nén vẫn thể hiện khả năng thỏa đáng trong phân loại zero-shot và sinh ngôn ngữ. Mã nguồn có sẵn tại: https://github.com/horseee/LLM-Pruner

1 Giới thiệu
Gần đây, các Mô hình Ngôn ngữ Lớn (LLM) [37,49,48,42,62,4,69] đã thể hiện khả năng đáng kinh ngạc trong việc hiểu và sinh ngôn ngữ. Với sự gia tăng kích thước mô hình, chúng được trang bị tốt hơn để xử lý các tác vụ phức tạp [3,5,56,58] và thậm chí thể hiện các khả năng nổi lên [55].
Tuy nhiên, bất chấp hiệu suất ấn tượng của chúng, LLM đặt ra những thách thức trong triển khai và suy luận. Quy mô rộng lớn của chúng tạo ra nhu cầu tính toán đáng kể, và vô số tham số liên quan có thể gây ra độ trễ dài và các vấn đề liên quan khác. Một số kỹ thuật được đề xuất để giải quyết những vấn đề này, như cắt tỉa mô hình [54,59,67,21], chưng cất kiến thức [44,39,45], lượng tử hóa [1,13] trong bối cảnh mô hình ngôn ngữ được huấn luyện trước (PLM).

Trong khi các phương pháp trước đã duy trì hiệu quả hiệu suất mô hình giữa việc giảm tham số, chúng chủ yếu nhắm vào nén trong các lĩnh vực chuyên biệt hoặc cho các tác vụ được chỉ định trong bối cảnh nén cụ thể theo tác vụ. Ví dụ, một PLM được tinh chỉnh trên một tập dữ liệu cụ thể, chẳng hạn như một trong các tác vụ phân loại trong benchmark GLUE [51], sau đó những mô hình này được chưng cất thành một mô hình phân loại nhỏ hơn [44,18]. Mặc dù mô hình này có thể được sử dụng cho việc nén LLM, nó làm giảm khả năng của LLM như một công cụ giải quyết tác vụ đa năng, khiến nó chỉ phù hợp với một tác vụ duy nhất.

*Tác giả liên hệarXiv:2305.11627v3 [cs.CL] 28 Sep 2023

--- TRANG 2 ---
(i) Nén Cụ thể Theo Tác Vụ
LLaMA-7B
Mô hình cụ thể theo tác vụTập dữ liệu tác vụ
Tập dữ liệu tác vụ
(ii)TinyBERT~20GB Kho dữ liệuTập dữ liệu tác vụ
(iii) LLM-Pruner~50MB Kho dữ liệu

TheLeaningTowerofPisaisknownforitsunusualtilt,whichisaresultofanumberoffactors.Whenthetowerwasbuiltinthetwelfthcentury,thesoilbeneathitwasextremelysoft,allowingthebuttressestosettleunevenly.Thisresultedinatilttowardsoneside.TheLeaningTowerofPisaisknownforbeingtiltedandunstable.However,itsstoryismuchmorefascinating.Althoughconstructionbeganin1173,thetowerwasnevermeanttobetilted.Itsimplybecamethatwaybecauseitwasbuiltonunstableground.

LLMLLaMA-5.4B bởi LLM-Pruner
3.5 ngày (4 GPU)3 giờ (1 GPU)
Đánh giáQANLIMRC
Tinh chỉnh

Hình 1: Minh họa LLM-Pruner. (i) Nén cụ thể theo tác vụ: mô hình được tinh chỉnh sau đó nén trên một tác vụ cụ thể. (ii) TinyBERT: Đầu tiên chưng cất mô hình trên kho dữ liệu không gán nhãn và sau đó tinh chỉnh nó trên tác vụ cụ thể. (iii) LLM-Pruner: Nén không phụ thuộc tác vụ trong vòng 3 giờ.

Do đó, chúng tôi phấn đấu nén LLM trong một thiết lập mới: giảm kích thước LLM trong khi bảo tồn các khả năng đa dạng của nó như các công cụ giải quyết tác vụ đa năng, như được mô tả trong Hình 1. Điều này giới thiệu việc nén LLM không phụ thuộc tác vụ, điều này đưa ra hai thách thức chính:

•Kích thước của kho dữ liệu huấn luyện của LLM là khổng lồ. Các phương pháp nén trước đây phụ thuộc nhiều vào kho dữ liệu huấn luyện. LLM đã leo thang quy mô kho dữ liệu lên 1 nghìn tỷ token hoặc hơn [17,49]. Nhu cầu lưu trữ rộng lớn và thời gian truyền kéo dài làm cho việc thu thập tập dữ liệu trở nên khó khăn. Hơn nữa, nếu tập dữ liệu là độc quyền, việc thu thập kho dữ liệu huấn luyện gần như không thể, một tình huống gặp phải trong [69,37].

•Thời gian không thể chấp nhận được cho việc huấn luyện sau của LLM đã cắt tỉa. Các phương pháp hiện tại yêu cầu một lượng thời gian đáng kể để huấn luyện sau mô hình nhỏ hơn [53,28]. Ví dụ, chưng cất tổng quát trong TinyBERT mất khoảng 14 ngày GPU [20]. Thậm chí việc huấn luyện sau một mô hình nén cụ thể theo tác vụ của BERT đòi hỏi khoảng 33 giờ [59,22]. Khi kích thước của cả mô hình và kho dữ liệu cho LLM tăng nhanh, bước này sẽ tiêu tốn thậm chí nhiều thời gian hơn nữa.

Để giải quyết các thách thức nói trên liên quan đến việc nén LLM không phụ thuộc tác vụ, chúng tôi giới thiệu một phương pháp mới có tên LLM-Pruner. Vì mục tiêu của chúng tôi là nén LLM với sự phụ thuộc dữ liệu giảm và huấn luyện sau nhanh chóng, cách cắt tỉa mô hình với sự gián đoạn tối thiểu đến bản gốc là rất quan trọng. Để hoàn thành điều này, chúng tôi đề xuất một thuật toán phát hiện phụ thuộc xác định tất cả các cấu trúc phụ thuộc trong mô hình. Một khi cấu trúc liên kết được xác định, chúng tôi sử dụng một chiến lược ước lượng tầm quan trọng hiệu quả để chọn nhóm tối ưu cho việc cắt tỉa dưới thiết lập không phụ thuộc tác vụ, nơi thông tin bậc nhất và thông tin hessian xấp xỉ được tính đến. Cuối cùng, một giai đoạn khôi phục nhanh được thực hiện để huấn luyện sau mô hình đã cắt tỉa với dữ liệu hạn chế.

Đóng góp. Trong bài báo này, chúng tôi đề xuất một khung mới, LLM-Pruner, cho việc nén không phụ thuộc tác vụ của mô hình ngôn ngữ lớn. Theo hiểu biết của chúng tôi, LLM-Pruner là khung đầu tiên được thiết kế cho cắt tỉa có cấu trúc của LLM. Chúng tôi kết luận các ưu điểm của LLM-Pruner như (i) Nén không phụ thuộc tác vụ, nơi mô hình ngôn ngữ nén giữ lại khả năng phục vụ như một công cụ giải quyết đa tác vụ. (ii) Giảm nhu cầu cho kho dữ liệu huấn luyện gốc, nơi chỉ cần 50k mẫu có sẵn công khai để nén, giảm đáng kể ngân sách thu thập dữ liệu huấn luyện (iii) Nén nhanh, nơi quá trình nén kết thúc trong ba giờ. (iv) Một khung cắt tỉa cấu trúc tự động, nơi tất cả các cấu trúc phụ thuộc được nhóm mà không cần thiết kế thủ công nào. Để đánh giá hiệu quả của LLM-Pruner, chúng tôi tiến hành các thí nghiệm rộng rãi trên ba mô hình ngôn ngữ lớn: LLaMA-7B, Vicuna-7B và ChatGLM-6B. Các mô hình nén được đánh giá bằng chín tập dữ liệu để đánh giá cả chất lượng sinh và hiệu suất phân loại zero-shot của các mô hình đã cắt tỉa. Kết quả thí nghiệm chứng minh rằng ngay cả với việc loại bỏ 20% tham số, mô hình đã cắt tỉa vẫn duy trì 94.97% hiệu suất của mô hình gốc.

2

--- TRANG 3 ---
2 Công trình liên quan
Nén Mô hình Ngôn ngữ. Các mô hình ngôn ngữ [9,29,25] đã thu hút nhiều sự chú ý và tăng nhu cầu giảm kích thước tham số và giảm độ trễ [23,46]. Để nén mô hình ngôn ngữ, các công trình trước có thể được chia thành nhiều loại: cắt tỉa mạng [21,61,30,15], chưng cất kiến thức [44,45,38], lượng tử hóa [63,1,66] và các kỹ thuật khác, như thoát sớm [60] hoặc giảm token động [64]. Chúng tôi tập trung vào việc cắt tỉa các mô hình ngôn ngữ, đặc biệt là cắt tỉa cấu trúc [26]. Cắt tỉa cấu trúc loại bỏ toàn bộ bộ lọc khỏi mạng neural, điều này thân thiện hơn với phần cứng. Có nhiều cách để loại bỏ cấu trúc, chẳng hạn như cắt tỉa phụ thuộc l1 [16,67], ước lượng tầm quan trọng bậc nhất [18], ước lượng dựa trên hessian [21,52] hoặc phẫu thuật não tối ưu [24,21]. Đối với đơn vị cắt tỉa trong cắt tỉa cấu trúc, một số công trình áp dụng toàn bộ lớp [10] làm đơn vị tối thiểu, và những công trình khác lấy attention đa đầu [50] hoặc các lớp feed-forward [18,34] làm cấu trúc cơ bản để cắt tỉa. CoFi [59] nghiên cứu đơn vị cắt tỉa ở độ chi tiết khác nhau.

Nén Hiệu quả và Ít Tài nguyên. Với kích thước mô hình ngày càng tăng, có nhu cầu ngày càng tăng về nén LLM hiệu quả và nén độc lập với dữ liệu huấn luyện gốc. Đối với nén hiệu quả, [22] tăng tốc huấn luyện sau bằng cách định nghĩa lỗi tái tạo như một bài toán bình phương tối thiểu tuyến tính. [13,12] đề xuất phẫu thuật não tối ưu theo lớp. Đối với ràng buộc về tính sẵn có của kho dữ liệu huấn luyện, cắt tỉa không có dữ liệu [43,65] đưa ra nhiều chiến lược để cắt tỉa mô hình bằng cách đo độ tương tự của neuron. Bên cạnh đó, [32,31,40] đề xuất các phương pháp chưng cất mô hình mà không dựa vào kho dữ liệu huấn luyện của mô hình. Tuy nhiên, những phương pháp đó quá tốn thời gian, liên quan đến việc tổng hợp mẫu bằng cách lan truyền ngược các mô hình ngôn ngữ được huấn luyện trước.

3 Phương pháp
Trong phần này, chúng tôi cung cấp giải thích chi tiết về LLM-Pruner. Theo pipeline nén mô hình thông thường [22], LLM-Pruner bao gồm ba bước: (1) Giai đoạn Khám phá (Phần 3.1). Bước này tập trung vào xác định các nhóm cấu trúc phụ thuộc lẫn nhau trong LLM. (2) Giai đoạn Ước lượng (Phần 3.2). Một khi các cấu trúc liên kết được nhóm, bước thứ hai đòi hỏi ước lượng đóng góp của mỗi nhóm vào hiệu suất tổng thể của mô hình và quyết định nhóm nào sẽ bị cắt tỉa. (3) Giai đoạn Khôi phục (Phần 3.3). Bước này liên quan đến huấn luyện sau nhanh để giảm thiểu sự suy giảm hiệu suất tiềm ẩn do việc loại bỏ cấu trúc.

3.1 Khám phá Tất cả Cấu trúc Liên kết trong LLM
Với sự sẵn có hạn chế của dữ liệu cho huấn luyện sau, việc ưu tiên loại bỏ các cấu trúc với thiệt hại tối thiểu khi nén mô hình trở nên bắt buộc. Điều này nhấn mạnh cắt tỉa cấu trúc dựa trên phụ thuộc, đảm bảo các cấu trúc liên kết được cắt tỉa cùng nhau. Chúng tôi cung cấp một thí nghiệm trong Phần 4.3 để cho thấy tầm quan trọng của cắt tỉa cấu trúc dựa trên phụ thuộc khi nén mô hình ngôn ngữ lớn.

Phụ thuộc Cấu trúc trong LLM. Tương tự như [11], việc cắt tỉa bắt đầu bằng việc xây dựng phụ thuộc cho LLM. Giả sử Ni và Nj là hai neuron trong mô hình, In(Ni) và Out(Ni) đại diện cho tất cả các neuron hướng về hoặc hướng từ Ni. Phụ thuộc giữa các cấu trúc có thể được định nghĩa như:

Nj trong Out(Ni) ∧ Deg⁻(Nj) = 1 ⇒ Nj phụ thuộc vào Ni (1)

trong đó Deg⁻(Nj) đại diện cho bậc vào của neuron Nj. Lưu ý rằng phụ thuộc này có hướng, do đó chúng ta có thể thu được một phụ thuộc khác tương ứng:

Ni trong In(Nj) ∧ Deg⁺(Ni) = 1 ⇒ Ni phụ thuộc vào Nj (2)

trong đó Deg⁺(Ni) đại diện cho bậc ra của neuron Ni. Nguyên lý phụ thuộc ở đây là, nếu một neuron hiện tại (ví dụ, Ni) chỉ phụ thuộc vào một neuron khác (ví dụ, Nj), và neuron Nj bị cắt tỉa, thì neuron Ni cũng phải trải qua cắt tỉa.

3

--- TRANG 4 ---
Nhóm Loại A: MLP
NormNorm
Attention Đa đầuMLPĐầu LM
EmbeddingL xNhóm Loại B: Attention Đa đầu
Đầu 1
TruyVấnKhóaGiá trịĐầu n...............Nhóm Loại C: Nhóm theo Kênh
MLP:Norm:Norm:
QKVĐầu n...Đầu 1MHA:
Đầu LM:
Nhúng:Cổng ChiếuLên ChiếuXuống Chiếu

Hình 2: Minh họa các cấu trúc liên kết trong LLaMA. Chúng tôi đơn giản hóa các neuron trong mỗi lớp để làm rõ nhóm phụ thuộc. Neuron kích hoạt, được đánh dấu là một vòng tròn có chuông, gây ra trọng số với phụ thuộc bị cắt tỉa (đường nét đứt), có thể lan truyền (đường nét đứt đỏ) đến các neuron liên kết (vòng tròn nét đứt). Một nhóm có thể được kích hoạt bởi nhiều neuron kích hoạt khác nhau. Lấy Nhóm Loại B làm ví dụ, kích hoạt cho nhóm này bao gồm (i) đầu attention, (ii) neuron đầu ra trong Query, Key hoặc Value, và (iii) neuron đầu vào trong phép chiếu đầu ra cuối cùng.

Kích hoạt Đồ thị Phụ thuộc. Bằng cách có định nghĩa về phụ thuộc, các cấu trúc liên kết trong LLM có thể được phân tích tự động. Xem xét bất kỳ neuron nào trong LLM như kích hoạt ban đầu, nó có khả năng kích hoạt các neuron phụ thuộc vào nó. Tiếp theo, những neuron mới được kích hoạt này có thể đóng vai trò là các kích hoạt tiếp theo để xác định phụ thuộc và kích hoạt các neuron phụ thuộc tương ứng của chúng. Quá trình lặp này tiếp tục cho đến khi không có neuron mới nào được phát hiện. Những neuron đó sau đó tạo thành một nhóm để cắt tỉa thêm. Lấy LLaMA làm ví dụ, bằng cách tìm kiếm trên tất cả các neuron làm kích hoạt ban đầu, chúng ta có thể định vị tất cả các cấu trúc liên kết, như được hiển thị trong Hình 2.

Với sự đa dạng trong cấu trúc của các LLM khác nhau, việc phân tích thủ công và loại bỏ các cấu trúc liên kết trong mỗi LLM có thể cực kỳ tốn thời gian. Tuy nhiên, bằng cách sử dụng LLM-Pruner, tất cả các cấu trúc liên kết có thể được xác định và trích xuất tự động.

3.2 Ước lượng Tầm quan trọng Nhóm của Cấu trúc Liên kết
Đến bây giờ, tất cả các cấu trúc liên kết trong mô hình đã được nhóm. Trọng số trong cùng một nhóm nên được cắt tỉa đồng thời, vì cắt tỉa một phần không chỉ làm tăng kích thước tham số mà còn giới thiệu các biểu diễn trung gian không phù hợp. Do đó, chúng tôi ước lượng tầm quan trọng của nhóm như một tổng thể, trái ngược với việc đánh giá tầm quan trọng của các mô-đun. Với quyền truy cập hạn chế vào tập dữ liệu huấn luyện, chúng tôi khám phá việc sử dụng các tập dữ liệu công cộng hoặc mẫu được tạo thủ công như tài nguyên thay thế. Mặc dù các lĩnh vực của những tập dữ liệu này có thể không hoàn toàn phù hợp với tập huấn luyện, chúng vẫn cung cấp thông tin có giá trị để đánh giá tầm quan trọng.

Tầm quan trọng theo Vector. Giả sử rằng cho một tập dữ liệu D={xi, yi}ᴺᵢ₌₁, trong đó N là số lượng mẫu. Trong các thí nghiệm của chúng tôi, chúng tôi đặt N bằng 10 và chúng tôi sử dụng một số tập dữ liệu công cộng làm nguồn của D. Một nhóm (như được định nghĩa trước đây là một tập hợp các cấu trúc liên kết) có thể được định nghĩa là G={Wi}ᴹᵢ₌₁, trong đó M là số lượng cấu trúc liên kết trong một nhóm và Wi là trọng số cho mỗi cấu trúc.

Trong khi cắt tỉa, mục tiêu của chúng tôi là loại bỏ nhóm có tác động ít nhất đến dự đoán của mô hình, điều này có thể được chỉ ra bởi độ lệch trong hàm mất mát. Đặc biệt, để ước lượng tầm quan trọng của Wi, sự thay đổi trong hàm mất mát có thể được công thức hóa như [24]:

IWi=|ΔL(D)|=|LWi(D)− LWi=0(D)|=|∂L^T(D)/∂Wi Wi - 1/2 Wi^T HWi + O(||Wi||³)| (3)

trong đó H là ma trận hessian. Ở đây, L đại diện cho hàm mất mát dự đoán token tiếp theo. Số hạng đầu tiên thường bị bỏ qua trong các công trình trước [24,52,12], vì mô hình đã hội tụ trên tập dữ liệu huấn luyện, nơi ∂L^T/∂Wi ≈ 0. Tuy nhiên, vì D ở đây không được trích xuất từ dữ liệu huấn luyện gốc, có nghĩa là ∂L^T/∂Wi ≠ 0. Điều này trình bày một thuộc tính mong muốn để xác định tầm quan trọng

4

--- TRANG 5 ---
của Wi bằng số hạng gradient dưới LLM, vì việc tính toán số hạng thứ hai, ma trận Hessian, trên LLM là không thực tế với độ phức tạp O(N²).

Tầm quan trọng theo Phần tử. Cái trên có thể được xem như một ước lượng cho trọng số Wi. Chúng ta có thể rút ra một thước đo tầm quan trọng khác ở độ chi tiết tốt hơn, nơi mỗi tham số trong Wi được đánh giá về ý nghĩa của nó:

IWᵢᵏ=|ΔL(D)|=|LWᵢᵏ(D)− LWᵢᵏ=0(D)|=|∂L(D)/∂Wᵢᵏ Wᵢᵏ - 1/2 Wᵢᵏ Hkk Wᵢᵏ + O(||Wᵢᵏ||³)| (4)

Ở đây, k đại diện cho tham số thứ k trong Wi. Đường chéo của hessian Hkk có thể được xấp xỉ bằng ma trận thông tin Fisher, và tầm quan trọng có thể được định nghĩa như:

IWᵢᵏ=|LWᵢᵏ(D)− LWᵢᵏ=0(D)| ≈ |∂L(D)/∂Wᵢᵏ Wᵢᵏ - 1/2N ∑ⱼ₌₁ᴺ (∂L(Dⱼ)/∂Wᵢᵏ Wᵢᵏ)² + O(||Wᵢᵏ||³)| (5)

Tầm quan trọng Nhóm. Bằng cách sử dụng IWᵢᵏ hoặc IWᵢ, chúng tôi ước lượng tầm quan trọng ở độ chi tiết của một tham số hoặc một trọng số. Nhớ rằng mục tiêu của chúng tôi là ước lượng tầm quan trọng của G, chúng tôi tổng hợp điểm tầm quan trọng theo bốn cách: (i) Tổng: IG=∑ᵢ₌₁ᴹ IWᵢ hoặc IG=∑ᵢ₌₁ᴹ ∑k IWᵢᵏ, (ii) Tích: IG=∏ᵢ₌₁ᴹ IWᵢ hoặc IG=∏ᵢ₌₁ᴹ ∑k IWᵢᵏ, (iii) Max: IG= maxᵢ₌₁ᴹ IWᵢ hoặc IG= maxᵢ₌₁ᴹ ∑k IWᵢᵏ; (iv) Chỉ-Cuối: Vì việc xóa cấu trúc thực thi cuối cùng trong một nhóm phụ thuộc tương đương với việc xóa tất cả kết quả tính toán trong nhóm đó, chúng tôi gán tầm quan trọng của cấu trúc thực thi cuối cùng làm tầm quan trọng của nhóm: IG=IWₗ hoặc IG=∑k IWₗᵏ, trong đó l là cấu trúc cuối cùng. Sau khi đánh giá tầm quan trọng của mỗi nhóm, chúng tôi xếp hạng tầm quan trọng của mỗi nhóm và cắt tỉa các nhóm với tầm quan trọng thấp hơn dựa trên tỷ lệ cắt tỉa được xác định trước.

3.3 Khôi phục Nhanh với Xấp xỉ Hạng thấp
Để tăng tốc quá trình khôi phục mô hình và cải thiện hiệu quả của nó dưới dữ liệu hạn chế, việc giảm thiểu số lượng tham số cần tối ưu hóa trong giai đoạn khôi phục là rất quan trọng. Để tạo điều kiện cho điều này, chúng tôi sử dụng xấp xỉ hạng thấp, LoRA [19], để huấn luyện sau mô hình đã cắt tỉa. Mỗi ma trận trọng số có thể học trong mô hình, được ký hiệu là W, bao gồm cả phép chiếu tuyến tính đã cắt tỉa và chưa cắt tỉa trong LLM, có thể được biểu diễn là W. Giá trị cập nhật ΔW cho W có thể được phân tích thành ΔW=PQ trong ℝᵈ⁻×ᵈ⁺, trong đó P trong ℝᵈ⁻×ᵈ và Q trong ℝᵈ×ᵈ⁺. Tính toán tiến có thể được biểu diễn như:

f(x) = (W+ ΔW)X+b= (WX +b) + (PQ)X (6)

trong đó b là bias trong lớp dày đặc. Chỉ huấn luyện P và Q giảm độ phức tạp huấn luyện tổng thể, giảm nhu cầu dữ liệu huấn luyện quy mô lớn. Bên cạnh đó, các tham số bổ sung P và Q có thể được tham số hóa lại thành ΔW, điều này sẽ không gây ra tham số bổ sung trong mô hình nén cuối cùng.

4 Thí nghiệm
4.1 Thiết lập Thí nghiệm
Mô hình Ngôn ngữ Lớn Nền tảng. Để thể hiện hiệu quả và tính đa dạng của LLM-Pruner, chúng tôi thử nghiệm nó trên ba mô hình ngôn ngữ lớn mã nguồn mở với hai loại cấu trúc: LLaMA-7B [49], Vicuna-7B [4]² và ChatGLM-6B [69].

Đánh giá và Tập dữ liệu. Để đánh giá hiệu suất của mô hình trong thiết lập không phụ thuộc tác vụ, chúng tôi theo đánh giá của LLaMa để thực hiện phân loại tác vụ zero-shot trên các tập dữ liệu suy luận thông thường: BoolQ [6], PIQA [2], HellaSwag [68], WinoGrande [41], ARC-easy [7], ARC-challenge [7] và OpenbookQA [36]. Theo [14], mô hình xếp hạng các lựa chọn trong các tác vụ lựa chọn nhiều hoặc sinh ra câu trả lời trong sinh mở³. Ngoài ra, chúng tôi bổ sung đánh giá của mình với phân tích độ bối rối (PPL) zero-shot trên WikiText2 [35] và PTB [33].

²https://huggingface.co/lmsys/vicuna-7b-delta-v0
³https://github.com/EleutherAI/lm-evaluation-harness

5

--- TRANG 6 ---
Bảng 1: Hiệu suất zero-shot của LLaMA-7B nén. Trung bình được tính toán trong bảy tập dữ liệu phân loại. 'Gạch dưới' chỉ hiệu suất tốt nhất chỉ cắt tỉa, trong khi 'đậm' đại diện cho hiệu suất tốt nhất tổng thể với cùng tỷ lệ cắt tỉa, xem xét cả cắt tỉa và huấn luyện sau. Chiến lược 'Channel' chỉ cắt tỉa nhóm phụ thuộc Loại C, trong khi tất cả các phương pháp khác sử dụng chiến lược 'Block' để cắt tỉa nhóm phụ thuộc ở cả Loại A và Loại B. Vì [49] không cung cấp prompt của nó, việc đánh giá kết quả với ⋆ được thực hiện dưới các prompt khác nhau, thấp hơn kết quả chính thức.

[Bảng hiển thị kết quả chi tiết với các tỷ lệ cắt tỉa khác nhau và phương pháp khác nhau]

Bảng 2: Hiệu suất zero-shot của LLaMA-13B nén. Ở đây chúng tôi áp dụng Element1 làm ước lượng tầm quan trọng cho 'Channel' và 'Block'.

[Bảng hiển thị kết quả cho LLaMA-13B]

Chi tiết Triển khai. Trong quá trình cắt tỉa mô hình, chúng tôi sử dụng 10 mẫu được chọn ngẫu nhiên từ Bookcorpus [70], mỗi mẫu được cắt ngắn thành độ dài chuỗi 128, làm mẫu hiệu chuẩn để thiết lập phụ thuộc và tính toán gradient cho cả LLaMA và Vicuna. Đối với ChatGLM, chúng tôi chọn 10 mẫu ngẫu nhiên từ DailyDialog [27]. Trong giai đoạn khôi phục, chúng tôi sử dụng phiên bản đã làm sạch của Alpaca [47], bao gồm khoảng 50k mẫu. Đáng chú ý, việc điều chỉnh những mẫu này chỉ yêu cầu 3 giờ trên một GPU với chỉ 2 epoch. Nhiều siêu tham số của cắt tỉa và huấn luyện có thể được tìm thấy trong Phụ lục B.

Bảng 3: Thống kê của mô hình cơ sở và mô hình nén.
[Bảng hiển thị số liệu thống kê về tham số, MACs, bộ nhớ và độ trễ]

Thống kê của Mô hình Nén. Bảng 3 trình bày thống kê của các mô hình 7B được sử dụng trong thí nghiệm của chúng tôi: số lượng tham số, MACs, yêu cầu bộ nhớ và độ trễ để chạy mỗi mô hình. Đánh giá thống kê được tiến hành bằng chế độ suy luận, nơi mô hình được cung cấp một câu gồm 64 token. Độ trễ được kiểm tra dưới tập kiểm tra của WikiText2 trên một A5000. Ở đây, chiến lược 'Block' có nghĩa là đơn vị cắt tỉa trong mô hình bao gồm Nhóm Loại A và Nhóm Loại B như được minh họa trong Hình 2, trong khi 'Channel' chỉ ra rằng đơn vị cắt tỉa là Nhóm Loại C. Chúng tôi đi sâu vào phân tích hai lựa chọn này trong Phần 4.2 (Chiến lược Channel vs. Chiến lược Block). Tỷ lệ cắt tỉa được nêu ở đây biểu thị tỷ lệ xấp xỉ của các tham số cắt tỉa vì số lượng tham số trong mỗi cấu trúc cắt tỉa không hoàn toàn khớp với tổng số tham số cắt tỉa.

4.2 Hiệu suất Zero-shot
Bảng 1,2,4 và 5 cho thấy hiệu suất zero-shot của mô hình đã cắt tỉa. Dựa trên đánh giá được tiến hành trên LLaMA, sử dụng giảm 20% tham số mà không có huấn luyện sau, mô hình đã cắt tỉa

6

--- TRANG 7 ---
Bảng 4: Hiệu suất zero-shot của Vicuna-7B nén
[Bảng chi tiết với các phương pháp và kết quả khác nhau]

quản lý để giữ lại 89.8% hiệu suất được thể hiện bởi mô hình chưa cắt tỉa. Hơn nữa, thông qua huấn luyện sau hiệu quả, độ chính xác phân loại được cải thiện thêm lên 60.07%, đạt 94.97% độ chính xác của mô hình gốc. Điều này chứng minh tính khả thi của việc sử dụng LLM-Pruner để nén mô hình một cách hiệu quả, thậm chí không dựa vào dữ liệu huấn luyện, và trong thời gian ngắn đáng kể. Đáng ngạc nhiên, chúng tôi phát hiện rằng trên hầu hết các tập dữ liệu, mô hình đã cắt tỉa với LLaMA 5.4B thậm chí vượt trội hơn chatGLM-6B. Điều này làm nổi bật tính ưu việt của LLM-Pruner: nếu cần một mô hình nhỏ hơn với kích thước tùy chỉnh, LLM-Pruner hiệu quả về chi phí hơn so với việc huấn luyện lại một mô hình khác với hiệu suất thỏa đáng. Tuy nhiên, với 50% tham số bị cắt tỉa, một sự suy giảm độ chính xác lớn được quan sát (xem Phụ lục C.5). Nén LLM dưới tỷ lệ nén cao vẫn là một thách thức lớn.

Kết quả nén của Vicuna-7B phù hợp với LLaMA, vì cắt tỉa 20% tham số trên Vicuna-7B duy trì hiệu suất ở 92.03% của mô hình gốc. Chúng tôi thử nghiệm tỷ lệ cắt tỉa nhỏ hơn 10% trên chatGLM-7B, nơi mô hình đã cắt tỉa chỉ trải qua sự giảm hiệu suất nhỏ 0.89%, có thể được khôi phục thông qua huấn luyện sau. Mặc dù mô hình đã cắt tỉa vượt trội hơn mô hình chưa nén, chúng tôi không khẳng định nó tốt hơn mô hình gốc. Điều này chủ yếu vì chatGLM-6B, một mô hình song ngữ, có tiếp xúc hạn chế với huấn luyện trước tiếng Anh. Tuy nhiên, huấn luyện sau giới thiệu nó với kho dữ liệu tiếng Anh nhiều hơn, mặc dù hạn chế, cải thiện khả năng hiểu tiếng Anh của nó.

Phân tích loại bỏ: Tác động của Ước lượng Tầm quan trọng. Chúng tôi tiến hành thử nghiệm trên tất cả các kỹ thuật ước lượng tầm quan trọng được đề xuất trong Phần 3.2. Kết quả có thể được tìm thấy trong Bảng 1 và 4. Ở đây, Elementn đại diện cho đánh giá tầm quan trọng sử dụng số hạng bậc n trong Eq.5. Vector đại diện cho kết quả tương ứng với Eq.3. Dựa trên kết quả thu được từ LLaMA-7B và Vicuna-7B, thuật toán cắt tỉa đạt hiệu suất trung bình tốt nhất chủ yếu bằng cách tận dụng đạo hàm bậc hai cho mỗi tham số. Tuy nhiên, với đạo hàm bậc nhất hiệu quả hơn đáng kể so với đạo hàm bậc hai, mặc dù cho kết quả hơi kém hơn, chúng tôi vẫn bỏ phiếu cho số hạng bậc nhất như một phương pháp cạnh tranh. Bên cạnh đó, kết quả trên chatGLM-7B khác biệt đáng kể so với những phát hiện này. Ước lượng tầm quan trọng trên mỗi tham số thất bại, thậm chí hiệu suất tệ hơn l2, trong khi ước lượng tầm quan trọng trên ma trận trọng số đạt hiệu suất tốt nhất.

[Hình 3: Độ nhạy lớp cho Cắt tỉa]

Chiến lược Channel vs. Chiến lược Block. Từ kết quả được trình bày trong Bảng 2, rõ ràng là cắt tỉa 'Channel' làm giảm hiệu suất đáng kể so với cắt tỉa 'Block'. Sự khác biệt này phát sinh vì các lớp trong transformer xếp chồng không phân bổ đều tầm quan trọng của chúng. Như được hiển thị trong Hình 3, các lớp đầu tiên và cuối cùng có tác động sâu sắc đến hiệu suất của mô hình, và cắt tỉa chúng dẫn đến sự suy giảm hiệu suất đáng kể hơn so với các lớp khác. Tuy nhiên, do việc xử lý đồng nhất nhóm 'Channel' trên tất cả các lớp, việc cắt tỉa các lớp đầu tiên và cuối cùng trở nên không thể tránh khỏi, dẫn đến sự suy giảm đáng kể về hiệu suất.

7

--- TRANG 8 ---
Bảng 5: Hiệu suất Zero-shot của ChatGLM-6B nén
[Bảng chi tiết kết quả]

Hình 4: Kết quả cắt tỉa trên LLaMA-7B (trái) và Vicuna-7B (phải) với các tỷ lệ cắt tỉa khác nhau.

Hình 5: Độ bối rối trên các tập dữ liệu zero-shot qua các bước khác nhau.

4.3 Phân tích Thêm
Tác động của Các Tỷ lệ Cắt tỉa Khác nhau. Chúng tôi điều tra tác động của việc cắt tỉa LLM ở các tỷ lệ cắt tỉa khác nhau trong Hình 4. Chúng tôi so sánh kết quả cắt tỉa của mình với chiến lược L2 vì L2 cũng là một thuật toán cắt tỉa không có dữ liệu. Quan sát thấy trong thí nghiệm của LLaMA rằng khi tỷ lệ cắt tỉa đạt khoảng 20%, thuật toán phụ thuộc độ lớn trải qua sự sụp đổ nhanh chóng, dẫn đến mất thông tin. Ngược lại, bằng cách sử dụng LLM-Pruner, chúng tôi có thể tăng tỷ lệ cắt tỉa lên khoảng 60% trong khi đạt được mức độ bối rối tương đương. Hơn nữa, trong trường hợp Vicuna-7B, việc loại bỏ 10% tham số dẫn đến sự suy giảm hiệu suất tương đương với LLM-Pruner với 60%. Việc sử dụng LLM-Pruner cho phép tăng đáng kể số lượng tham số mô hình có thể được cắt tỉa, từ đó giảm đáng kể chi phí tính toán.

Điều chỉnh trên Tập dữ liệu Ngoài. Để điều chỉnh mô hình đã cắt tỉa, chúng tôi sử dụng tập dữ liệu ngoài Alpaca [47]. Các đường cong đánh giá của mô hình đã cắt tỉa trên hai tập dữ liệu zero-shot trong quá trình huấn luyện sau được mô tả trong Hình 5. Kết quả chứng minh sự giảm nhanh chóng trong độ bối rối của mô hình đã cắt tỉa trong vòng 300 bước, sau đó là sự tăng dần dần. Chúng tôi cung cấp đánh giá toàn diện hơn trong Phụ lục C.4. Quan trọng là lưu ý rằng nếu mô hình được huấn luyện quá nhiều bước, nó có nguy cơ overfitting tập dữ liệu ngoài, có thể làm tổn hại hiệu suất của nó trong các tác vụ đa năng khác.

Tác động của Cắt tỉa Cấu trúc Dựa trên Phụ thuộc. Để nghiên cứu tầm quan trọng của cắt tỉa cấu trúc dựa trên phụ thuộc, chúng tôi tiến hành thí nghiệm để phá vỡ phụ thuộc trong các nhóm, nơi mỗi ma trận trọng số Wi được cắt tỉa chỉ dựa trên điểm tầm quan trọng được ước lượng trên chính nó. Bảng 6 trình bày kết quả chứng minh tác động của phụ thuộc trong cắt tỉa cấu trúc. Trong trường hợp không có phụ thuộc, mô hình gần như thất bại trong sinh zero-shot và các tác vụ phân loại. Thậm chí với điều chỉnh, mô hình không thể khôi phục, cho thấy sự khác biệt đáng kể so với kết quả trong cắt tỉa dựa trên phụ thuộc.

Tác động của Các Chiến lược Tổng hợp Khác nhau. Chúng tôi tiến hành thử nghiệm trên các thuật toán tổng hợp được đề xuất trong Phần 3.2. Kết quả thí nghiệm của chúng tôi tiết lộ những khác biệt đáng chú ý trong hiệu suất mô hình qua các chiến lược tổng hợp khác nhau, với sự nhấn mạnh đặc biệt vào chiến lược 'Last-only'. Trong số các phương pháp được đánh giá, chiến lược 'Max' đạt được kết quả thuận lợi nhất về độ bối rối, biểu thị sự gắn kết và trôi chảy tăng cường trong sinh câu. Tuy nhiên, quan trọng là lưu ý rằng chiến lược 'Max' thể hiện kết quả phân loại zero-shot tệ nhất so với tất cả bốn chiến lược. Ngược lại, chiến lược 'Last-only' thể hiện hiệu suất phân loại vượt trội nhưng

8

--- TRANG 9 ---
Bảng 6: Hiệu ứng của cắt tỉa cấu trúc dựa trên phụ thuộc. Trung bình đại diện cho hiệu suất trung bình trên 7 tập dữ liệu phân loại.

Bảng 7: Tác động của các chiến lược tổng hợp khác nhau về ước lượng tầm quan trọng nhóm. Thí nghiệm được thực hiện trên LLaMA-7B.

chịu đựng chất lượng sinh tệ nhất. Trong các thí nghiệm của chúng tôi, chúng tôi đưa ra sự đánh đổi bằng cách chọn chiến lược 'Sum' vì nó cho thấy cả chất lượng tổng quát hóa tốt và hiệu suất phân loại.

Bảng 8: DistilBert vs. LLM-Pruner. Trung bình ở đây có nghĩa là điểm trung bình trên bảy tập dữ liệu trên.

So sánh với DistilBERT Chúng tôi cho thấy kết quả so sánh của DistilBERT và LLM-Pruner trên LLaMA-7B trong Bảng 8. LLM-Pruner vượt trội hơn DistilBERT 4.24% trung bình với kích thước thậm chí nhỏ hơn. Lý do nằm ở chỗ LLM-Pruner giảm thiểu sự gián đoạn mô hình trong quá trình cắt tỉa, trong khi DistilBERT chỉ đơn giản chọn một lớp trong hai. Kết quả là, mô hình được cắt tỉa bởi LLM-Pruner đòi hỏi ít dữ liệu hơn để khôi phục hiệu suất của nó so với DistilBERT, do đó đạt hiệu suất vượt trội.

Huấn luyện từ Đầu vs. Cắt tỉa. Chúng tôi so sánh LLM-Pruner với StableLM-3B⁴ với kích thước tham số tương tự. Để đảm bảo công bằng, cả hai mô hình đều được tinh chỉnh trên tập dữ liệu Alpaca. Kết quả thí nghiệm của hai mô hình này được hiển thị trong Bảng 9. LLM-Pruner tạo ra LLM nhẹ với tài nguyên thấp, và thậm chí đôi khi có thể đạt hiệu suất tốt hơn LLM từ huấn luyện từ đầu. Tuy nhiên, chúng tôi cũng thừa nhận rằng LLaMA-3B thu được bởi LLM-Pruner sẽ không luôn vượt trội hơn các mô hình 3B khác từ huấn luyện từ đầu, do khoảng cách lớn trong kích thước kho dữ liệu huấn luyện.

Bảng 9: Huấn luyện từ Đầu (StableLM-3B) vs. LLaMA-3B (bởi LLM-Pruner)

Nghiên cứu Trường hợp. Chúng tôi cung cấp một số ví dụ về câu được sinh ra bởi mô hình nén sử dụng LLM-Pruner trong Bảng 10. Chúng tôi đã nỗ lực đảm bảo sự chồng chéo tối thiểu giữa những câu được sinh ra này và thông tin có trong kho dữ liệu điều chỉnh, điều này chứng minh rằng thông tin xuất phát từ mô hình gốc chứ không phải kho dữ liệu điều chỉnh. Chúng tôi cung cấp thêm ví dụ trong Phụ lục, bao gồm các câu được sinh ra của mô hình mà không có huấn luyện sau. Từ các trường hợp trong Bảng 10, rõ ràng là các câu được sinh ra bởi mô hình nén có thể so sánh với những câu được tạo ra bởi mô hình gốc. Chúng thể hiện tính trôi chảy, liên quan và cung cấp thông tin về chủ đề đã cho. Tuy nhiên, trong các thí nghiệm của chúng tôi, chúng tôi quan sát thấy rằng hiệu suất của mô hình đã cắt tỉa lệch khỏi mô hình gốc, đặc biệt khi sinh ra các câu dài. Thỉnh thoảng, nó có thể sinh ra các câu vô nghĩa hoặc chứa token lặp lại.

5 Kết luận
Trong bài báo này, chúng tôi đề xuất LLM-Pruner, một phương pháp cắt tỉa có cấu trúc cho các mô hình ngôn ngữ lớn. LLM-Pruner nhằm nén các mô hình ngôn ngữ lớn theo cách không phụ thuộc tác vụ trong khi giảm thiểu sự phụ thuộc vào kho dữ liệu huấn luyện gốc và bảo tồn khả năng ngôn ngữ của LLM. LLM-Pruner hoàn thành điều này bằng cách lặp đi lặp lại kiểm tra mỗi neuron trong mô hình như một kích hoạt để xác định nhóm phụ thuộc, từ đó xây dựng đồ thị phụ thuộc của LLM. Tiếp theo, LLM-Pruner đánh giá tầm quan trọng của những nhóm này bằng cách sử dụng cả ước lượng theo tham số và theo trọng số. Cuối cùng, chúng tôi sử dụng LoRA để khôi phục và điều chỉnh nhanh mô hình đã cắt tỉa. Chúng tôi đánh giá hiệu quả của LLM-Pruner trên ba mô hình khác biệt-LLaMA, Vicuna và ChatGLM-sử dụng

⁴https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b

9

--- TRANG 10 ---
Bảng 10: Một số ví dụ về sinh được thu được với mô hình gốc và mô hình đã cắt tỉa.

[Bảng chi tiết với các ví dụ sinh văn bản từ các mô hình khác nhau]

nhiều tập dữ liệu zero-shot khác nhau. Kết quả thí nghiệm của chúng tôi chỉ ra rằng LLM-Pruner thành công cắt tỉa mô hình, giảm gánh nặng tính toán trong khi giữ lại khả năng zero-shot của nó. Tuy nhiên, sự suy giảm hiệu suất đáng kể xảy ra khi sử dụng tỷ lệ cắt tỉa cao, chẳng hạn như việc loại bỏ 50% tham số của LLaMA, dẫn đến sự suy giảm đáng kể trong hiệu suất mô hình. Ngoài ra, chúng tôi quan sát các trường hợp mà mô hình sinh ra các câu không mạch lạc. Giải quyết các thách thức liên quan đến việc nén LLM ở tỷ lệ cắt tỉa cao hơn vẫn là một nhiệm vụ thách thức.

10

--- TRANG 11 ---
Tài liệu tham khảo
[1]Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020.

[2]Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.

[3]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

[4]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

[5]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[6]Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300.

[7]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018.

[8]Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.

[9]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[10] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019.

[11] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards any structural pruning, 2023.

[12] Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.

[13] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.

[14] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot language model evaluation. Version v0. 0.1. Sept, 2021.

[15] Fu-Ming Guo, Sijia Liu, Finlay S. Mungall, Xue Lin, and Yanzhi Wang. Reweighted proximal pruning for large-scale language representation. CoRR, abs/1909.12486, 2019. URL http://arxiv.org/abs/1909.12486.

[16] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf.

[17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

11

--- TRANG 12 ---
[18] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic bert with adaptive width and depth. Advances in Neural Information Processing Systems, 33:9782-9793, 2020.

[19] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.

[20] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163-4174, 2020.

[21] Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert surgeon: Scalable and accurate second-order pruning for large language models. arXiv preprint arXiv:2203.07259, 2022.

[22] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers. arXiv preprint arXiv:2204.09656, 2022.

[23] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.

[24] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989.

[25] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.

[26] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.

[27] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A manually labelled multi-turn dialogue dataset. In Proceedings of The 8th International Joint Conference on Natural Language Processing (IJCNLP 2017), 2017.

[28] Chen Liang, Haoming Jiang, Zheng Li, Xianfeng Tang, Bin Yin, and Tuo Zhao. Homodistil: Homotopic task-agnostic distillation of pre-trained transformers. arXiv preprint arXiv:2302.09632, 2023.

[29] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

[30] Zejian Liu, Fanrong Li, Gang Li, and Jian Cheng. Ebert: Efficient bert inference with dynamic structured pruning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4814-4823, 2021.

[31] Xinyin Ma, Yongliang Shen, Gongfan Fang, Chen Chen, Chenghao Jia, and Weiming Lu. Adversarial self-supervised data-free distillation for text classification. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6182-6192, 2020.

[32] Xinyin Ma, Xinchao Wang, Gongfan Fang, Yongliang Shen, and Weiming Lu. Prompting to distill: Boosting data-free knowledge distillation via reinforced prompt. arXiv preprint arXiv:2205.07523, 2022.

[33] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330, 1993. URL https://www.aclweb.org/anthology/J93-2004.

[34] JS McCarley, Rishav Chakravarti, and Avirup Sil. Structured pruning of a bert-based question answering model. arXiv preprint arXiv:1910.06360, 2019.

[35] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.

[36] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP, 2018.

[37] OpenAI. Gpt-4 technical report, 2023.

12

--- TRANG 13 ---
[38] Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang Li, and Jun Huang. Meta-kd: A meta knowledge distillation framework for language model compression across domains. CoRR, abs/2012.01266, 2020. URL https://arxiv.org/abs/2012.01266.

[39] Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang Li, and Jun Huang. Meta-kd: A meta knowledge distillation framework for language model compression across domains. arXiv preprint arXiv:2012.01266, 2020.

[40] Ahmad Rashid, Vasileios Lioutas, Abbas Ghaddar, and Mehdi Rezagholizadeh. Towards zero-shot knowledge distillation for natural language processing, 2020.

[41] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.

[42] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

[43] Suraj Srinivas and R Venkatesh Babu. Data-free parameter pruning for deep neural networks. arXiv preprint arXiv:1507.06149, 2015.

[44] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355, 2019.

[45] Siqi Sun, Zhe Gan, Yuwei Fang, Yu Cheng, Shuohang Wang, and Jingjing Liu. Contrastive distillation on intermediate representations for language model compression. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 498-508, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.36. URL https://aclanthology.org/2020.emnlp-main.36.

[46] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: a compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984, 2020.

[47] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

[48] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.

[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[50] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797-5808, 2019.

[51] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.

[52] Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. Eigendamage: Structured pruning in the kronecker-factored eigenbasis. In International conference on machine learning, pages 6566-6575. PMLR, 2019.

[53] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776-5788, 2020.

[54] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. arXiv preprint arXiv:1910.04732, 2019.

[55] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.

13

--- TRANG 14 ---
[56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.

[57] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. Lamini-lm: A diverse herd of distilled models from large-scale instructions, 2023.

[58] Yiquan Wu, Kun Kuang, Yating Zhang, Xiaozhong Liu, Changlong Sun, Jun Xiao, Yueting Zhuang, Luo Si, and Fei Wu. De-biased court's view generation with causality. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 763-780, 2020.

[59] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and accurate models. arXiv preprint arXiv:2204.00408, 2022.

[60] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. DeeBERT: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2246-2251, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.204. URL https://aclanthology.org/2020.acl-main.204.

[61] Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, and Zhibin Xiao. Rethinking network pruning-under the pre-train and fine-tune paradigm. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2376-2382, 2021.

[62] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020.

[63] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:27168-27183, 2022.

[64] Deming Ye, Yankai Lin, Yufei Huang, and Maosong Sun. TR-BERT: Dynamic token reduction for accelerating BERT inference. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5798-5809, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.463. URL https://aclanthology.org/2021.naacl-main.463.

[65] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Red++: Data-free pruning of deep neural networks via input splitting and output merging. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):3664-3676, 2022.

[66] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 36-39. IEEE, 2019.

[67] Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, and Moshe Wasserblat. Prune once for all: Sparse pre-trained language models. arXiv preprint arXiv:2111.05754, 2021.

[68] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.

[69] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.

[70] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015.

14

--- TRANG 15 ---
A Giải thích Chi tiết cho Các Quy tắc Phụ thuộc

Nhóm B: Attention Đa đầu......

[Hình 6: Minh họa hai quy tắc phụ thuộc với các trường hợp khác nhau]

Chúng tôi cung cấp giải thích chi tiết về hai quy tắc phụ thuộc. Quan trọng là lưu ý rằng những quy tắc phụ thuộc này không chỉ liên quan đến tính toán tiến. Thay vào đó, chúng đại diện cho các mối quan hệ có hướng tồn tại ở cả hai hướng. Ví dụ, việc loại bỏ một nút trong lớp tiếp theo cũng có thể dẫn đến việc cắt tỉa một nút trong lớp trước đó. Nhớ lại hai quy tắc phụ thuộc như sau:

Nj trong Out(Ni) ∧ Deg⁻(Nj) = 1 ⇒ Nj phụ thuộc vào Ni (7)
Ni trong In(Nj) ∧ Deg⁺(Ni) = 1 ⇒ Ni phụ thuộc vào Nj (8)

trong đó Ni và Nj là hai neuron. In(Ni) và Out(Ni) đại diện cho tất cả các neuron hướng về hoặc hướng từ Ni. Deg⁻(Ni) và Deg⁺(Ni) đại diện cho bậc vào và bậc ra của neuron Ni.

Hình 6 đóng vai trò như minh họa cho hai quy tắc phụ thuộc:
• Trong trường hợp 1, Nút I và Nút J thỏa mãn quy tắc được nêu trong Eq.7. Do đó, Nút J phụ thuộc vào Nút I. Khi Nút I bị cắt tỉa, cần thiết phải cắt tỉa Nút J.
• Trong trường hợp 2, Nút I và Nút J thỏa mãn quy tắc Eq.8. Do đó, Nút I phụ thuộc vào Nút J. Nếu Nút J bị cắt tỉa, việc cắt tỉa Nút I trở nên bắt buộc.
• Trong trường hợp 3, Nút J và Nút K không đáp ứng yêu cầu của Eq.7 do sự không khớp trong Deg⁻(Nk) ≠ 1. Do đó, với Nút J bị cắt tỉa, Nút K sẽ không bị ảnh hưởng.

B Chi tiết Triển khai

B.1 Cho Cắt tỉa
Đối với baseline Với sự thiếu hụt công trình trước đây về cắt tỉa cấu trúc của Các Mô hình Ngôn ngữ Lớn trong thiết lập không phụ thuộc tác vụ và ít tài nguyên, hiện tại không có baseline hiện có cho mô hình của chúng tôi. Để cung cấp chứng minh toàn diện về hiệu quả của LLM-Pruner, chúng tôi sử dụng hai phương pháp bổ sung để đánh giá, cùng với phương pháp cắt tỉa không có dữ liệu. Tất cả những phương pháp này được xây dựng dựa trên các nhóm phụ thuộc được xác định trong Phần 3.1:

• L2: Chúng tôi đánh giá tầm quan trọng của mỗi nhóm dựa trên độ lớn của ma trận trọng số của nó.
• Random: Phương pháp này liên quan đến việc chọn ngẫu nhiên các nhóm nhất định để cắt tỉa.

Đối với Nhóm 'Block'. Dựa trên những phát hiện được trình bày trong Bảng 3, tốt hơn là để ba lớp đầu tiên và lớp cuối cùng không thay đổi, vì việc sửa đổi tham số trong những lớp đó ảnh hưởng đáng kể đến mô hình. Trong mỗi mô-đun, chẳng hạn như MLP hoặc Attention Đa đầu, các nhóm được phát hiện được cắt tỉa dựa trên tỷ lệ được đặt trước. Ví dụ, trong lớp MLP của LLaMA-7B, chúng tôi xác định 11,008 nhóm, và với tỷ lệ cắt tỉa 25%, mô-đun sẽ cắt tỉa 2,752 nhóm. Đáng chú ý là tỷ lệ cắt tỉa cho các nhóm được chọn cao hơn tỷ lệ cắt tỉa cho tham số, vì các lớp nhất định (ví dụ, lớp embedding và các lớp được loại trừ đã đề cập) giữ lại tham số của chúng. Khi nhắm đến tỷ lệ cắt tỉa tham số 20%, chúng tôi cắt tỉa 25% từ Lớp 5 đến Lớp 30. Tương tự, để loại bỏ 50% tham số, chúng tôi cắt tỉa 60% nhóm từ Lớp 4 đến Lớp 30.

15

--- TRANG 16 ---
Đối với Nhóm 'Channel'. Nhóm 'Channel' thể hiện sự tương tự với cắt tỉa chiều trong mô hình, nhắm vào việc cắt tỉa các chiều nhất định. Trong trường hợp phép chiếu Query, Key và Value trong MHA, chỉ chiều đầu vào được cắt tỉa, trong khi đối với phép chiếu Đầu ra trong MHA, chỉ chiều đầu ra được cắt tỉa. Quan trọng là lưu ý rằng toàn bộ phụ thuộc được thiết lập tự động, không có thiết kế thủ công nào liên quan. Nhóm 'Channel' hoạt động theo cách bổ sung cho 'Block Group'. Trong Nhóm 'Channel', tỷ lệ cắt tỉa của nhóm bằng tỷ lệ cắt tỉa của tham số, vì tất cả ma trận trọng số, bao gồm ma trận embedding, trải qua cắt tỉa. Do đó, tỷ lệ cắt tỉa 20% tham số có nghĩa là cắt tỉa 20% nhóm, trong khi tỷ lệ cắt tỉa 50% có nghĩa là cắt tỉa 50% nhóm.

B.2 Đối với Giai đoạn Khôi phục

Bảng 11: Phân tích loại bỏ: Điều chỉnh các mô-đun khác nhau trong giai đoạn khôi phục

Chúng tôi theo [19] trong giai đoạn khôi phục của chúng tôi. Chúng tôi đặt hạng d thành 8 trong thí nghiệm của chúng tôi. Tỷ lệ học được đặt thành 1e-4 với 100 bước khởi động. Kích thước batch huấn luyện được chọn từ {64, 128} và bộ tối ưu hóa AdamW được sử dụng trong thí nghiệm của chúng tôi. Epoch huấn luyện tốt nhất chúng tôi tìm thấy là 2 epoch, vì huấn luyện với nhiều epoch hơn thậm chí có tác động tiêu cực đến hiệu suất mô hình. Chúng tôi chạy thí nghiệm của mình trên một GPU với bộ nhớ 24GB, sử dụng khoảng 2.5 giờ nếu RTX4090 được sử dụng. Tất cả mô-đun tuyến tính được tính vào để điều chỉnh hiệu quả. Một thí nghiệm phân tích loại bỏ cho điều này được hiển thị trong Bảng 11.

C Phân tích Thêm

C.1 Thêm Dữ liệu cho Khôi phục
Mặc dù các thí nghiệm chính của chúng tôi được tiến hành sử dụng 50k mẫu, chúng tôi vẫn tin rằng việc bao gồm dữ liệu bổ sung có thể cải thiện đáng kể quá trình khôi phục, mặc dù với chi phí tính toán cao hơn đáng kể. Do đó, chúng tôi tiến hành một thí nghiệm nhằm khôi phục mô hình với nhiều dữ liệu hơn, sử dụng tập dữ liệu bao gồm 2.59 triệu mẫu [57]. Kết quả được chi tiết trong Bảng 12. Từ kết quả, rõ ràng là hiệu suất của mô hình nén xấp xỉ chặt chẽ với mô hình cơ sở, chỉ thể hiện sự giảm hiệu suất nhỏ 0.89%.

Bảng 12: Khôi phục Mô hình: 50k mẫu vs. 2.59M mẫu

C.2 Cắt tỉa vs. Lượng tử hóa
Ở đây, chúng tôi tiến hành phân tích so sánh các kỹ thuật nén khác nhau và minh họa rằng những kỹ thuật này có thể được kết hợp hiệu quả với sự suy giảm hiệu suất ít. Chúng tôi đã chọn LLM.int8() [8] làm ví dụ đại diện của các phương pháp lượng tử hóa. Kết quả của chúng tôi cho thấy LLM.int8() vượt trội hơn LLM-Pruner trong khi LLM-Pruner tăng cường độ trễ, giảm kích thước tham số. Khi hai kỹ thuật này được áp dụng cùng nhau, chúng cùng nhau giảm tiêu thụ bộ nhớ và tăng tốc suy luận, cung cấp một phương pháp cân bằng kết hợp lợi ích của cả hai phương pháp.

Bảng 13: Cắt tỉa và Lượng tử hóa trên LLaMA-7B

C.3 Cắt tỉa Toàn cục vs. Cắt tỉa Địa phương
chúng tôi trình bày phân tích so sánh giữa cắt tỉa toàn cục và cắt tỉa địa phương, nơi tỷ lệ cắt tỉa là 20% và mô hình cơ sở là LLaMA-7B. Cắt tỉa toàn cục đề cập đến việc xếp hạng tất cả các nhóm trong mô hình cùng nhau, trong khi cắt tỉa địa phương chỉ liên quan đến việc xếp hạng các nhóm trong cùng một mô-đun để cắt tỉa. Kết quả của cắt tỉa toàn cục dẫn đến độ rộng khác nhau qua các lớp và mô-đun khác nhau, trong khi cắt tỉa địa phương đảm bảo tính đồng nhất trên tất cả các lớp.

Dựa trên những phát hiện thí nghiệm của chúng tôi, chúng tôi quan sát một lợi thế nhỏ của cắt tỉa địa phương so với cắt tỉa toàn cục. Chúng tôi nghĩ điều này là do độ lớn khác nhau trong các lớp hoặc mô-đun khác nhau, điều này làm cho điểm tầm quan trọng không thể so sánh được giữa các nhóm qua các lớp khác nhau.

Bảng 14: Kết quả của cắt tỉa toàn cục và cắt tỉa địa phương

C.4 Hiện tượng Overfitting trong Huấn luyện Sau
Chúng tôi trình bày phân tích toàn diện về vấn đề overfitting trong giai đoạn khôi phục, như đã đề cập trước đây trong Hình 5. Ở đây kết quả bao gồm tất cả 9 tập dữ liệu qua các bước huấn luyện khác nhau. Dựa trên những phát hiện được trình bày trong Bảng 15, một xu hướng đáng chú ý xuất hiện: độ chính xác hoặc chất lượng sinh ban đầu cho thấy sự cải thiện nhưng sau đó trải qua sự suy giảm nhỏ. Mô hình này cho thấy rằng quá trình khôi phục được hoàn thành trong thời gian ngắn. Và với kho dữ liệu huấn luyện bị hạn chế về lĩnh vực, nhiều epoch huấn luyện hơn có thể dẫn đến overfitting tập dữ liệu cụ thể trong khi có thể làm tổn hại khả năng gốc của mô hình ngôn ngữ.

Bảng 15: PPL và Độ chính xác trên các bước huấn luyện khác nhau

C.5 Cắt tỉa với Tỷ lệ Lớn
Ngoài ra, chúng tôi tiến hành thử nghiệm trên LLaMA-7B và Vicuna-7B với 50% tham số bị cắt tỉa. Chúng tôi quan sát sự giảm đáng kể về hiệu suất so với mô hình cơ sở. Tuy nhiên, giai đoạn khôi phục tỏ ra có lợi, dẫn đến sự cải thiện khoảng 7.39%. Cắt tỉa Mô hình Ngôn ngữ với tỷ lệ cắt tỉa cao như vậy vẫn là một nhiệm vụ thách thức.

Bảng 16: PPL và Độ chính xác trên LLaMA-7B

17

--- TRANG 17 ---
Bảng 17: PPL và Độ chính xác trên Vicuna-7B

[Bảng chi tiết với các kết quả khác nhau cho Vicuna-7B]

mô hình nén mang lại một cách tiếp cận cân bằng kết hợp lợi ích của cả hai phương pháp.

D Sinh từ Mô hình Nén
Bảng 18, 19, 20 và 21 cho thấy thêm ví dụ về các mô hình được cắt tỉa bởi LLM-Pruner. Chúng tôi trình bày kết quả sinh của cả mô hình đã cắt tỉa với huấn luyện sau và không có huấn luyện sau. Việc

18

--- TRANG 18 ---
không có huấn luyện sau cho phép chúng tôi hiểu rõ hơn thông tin được giữ lại trong mô hình. Chúng tôi bao gồm kết quả của ChatGLM-6B bằng hai ngôn ngữ vì nó là một mô hình song ngữ.

Bảng 18: Ví dụ Sinh từ LLaMA-5.4B Nén
[Bảng với các ví dụ văn bản được sinh ra]

Bảng 19: Ví dụ Sinh từ Vicuna-5.4B Nén
[Bảng với các ví dụ văn bản được sinh ra]

Bảng 20: Ví dụ Sinh từ ChatGLM-5.6B Nén (Tiếng Anh)
[Bảng với các ví dụ văn bản tiếng Anh được sinh ra]

19

--- TRANG 19 ---
Bảng 21: Ví dụ Sinh từ ChatGLM-5.6B Nén (Tiếng Trung)

[Bảng với các ví dụ văn bản tiếng Trung được sinh ra, bao gồm các câu hỏi và câu trả lời về địa điểm nổi tiếng ở Bắc Kinh, giải thích về thuyết tương đối, và lời khuyên về lối sống lành mạnh]

20
