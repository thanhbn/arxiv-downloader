# 2306.01385.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2306.01385.pdf
# Kích thước tập tin: 185142 bytes

===============================================
NỘI DUNG TẬP TIN PDF
===============================================

--- TRANG 1 ---
arXiv:2306.01385v2  [eess.AS]  9 Jul 2023Cắt tỉa có cấu trúc bất khả tri nhiệm vụ của các mô hình biểu diễn tiếng nói
Haoyu Wang1, Siyuan Wang1, Wei-Qiang Zhang1∗, Hongbin Suo2, Yulong Wan2
1Khoa Kỹ thuật Điện tử, Đại học Thanh Hoa, Bắc Kinh 100084, Trung Quốc
2Hệ thống Kỹ thuật Dữ liệu & AI, OPPO, Bắc Kinh 100026, Trung Quốc
w-hy21@mails.tsinghua.edu.cn, wq-zhang@tsinghua.edu.cn
Tóm tắt
Các mô hình tự giám sát được tiền huấn luyện như Wav2vec2, Hubert,
và WavLM đã được chứng minh cải thiện đáng kể nhiều tác vụ tiếng nói. Tuy nhiên, yêu cầu bộ nhớ lớn và tính toán mạnh của chúng cản trở khả năng ứng dụng trong công nghiệp. Cắt tỉa có cấu trúc là một kỹ thuật nén mô hình thân thiện với phần cứng nhưng thường dẫn đến mất mát độ chính xác lớn hơn. Trong bài báo này, chúng tôi đề xuất một phương pháp cắt tỉa đầu chú ý mịn để bù đắp cho sự suy giảm hiệu suất. Ngoài ra, chúng tôi cũng giới thiệu bộ ước lượng thẳng vào chính quy hóa L0 để tăng tốc hơn nữa mô hình đã cắt tỉa. Các thí nghiệm trên bộ đánh giá SUPERB cho thấy mô hình của chúng tôi có thể đạt hiệu suất tương đương với mô hình dày đặc trong nhiều tác vụ và vượt trội hơn mô hình Wav2vec 2.0 base trung bình, với ít hơn 72% tham số và tốc độ suy luận nhanh hơn 2 lần.
Từ khóa: Cắt tỉa mô hình, chưng cất kiến thức, nén mô hình, học biểu diễn
1. Giới thiệu
Gần đây, tiền huấn luyện tự giám sát đã trở thành một trong những chủ đề hấp dẫn nhất trong lĩnh vực tiếng nói [1, 2]. Với phương pháp này, một lượng lớn dữ liệu không nhãn có thể được sử dụng để huấn luyện một mô hình sâu để trích xuất các biểu diễn cấp cao từ âm thanh thô, có thể mang lại cải thiện đáng kể cho nhiều tác vụ xuôi dòng.
Trong khi các mô hình tiền huấn luyện cung cấp cải thiện hiệu suất to lớn, chúng cũng yêu cầu lượng lớn bộ nhớ và sức mạnh tính toán. Các mô hình tiếng nói tiền huấn luyện tự giám sát lớn như Wav2vec2 [3], Hubert [4], và WavLM [5] thường có hàng trăm triệu tham số, khiến chúng không phù hợp để sử dụng trên các sản phẩm tiêu dùng như laptop và điện thoại thông minh. Đây là một trở ngại đối với việc ứng dụng các mô hình này trong nhiều tình huống thực tế. Do đó, nén mô hình đã trở thành một mối quan tâm chính đối với các mô hình tự giám sát lớn này.
Chưng cất kiến thức thường sử dụng một mô hình giáo viên để hướng dẫn một mô hình học sinh nhỏ hơn, và cấu trúc của mô hình học sinh phải được thiết kế cẩn thận để đạt hiệu suất tốt hơn. DistilHubert [6] chưng cất một mô hình dựa trên Hubert 12 lớp để có được một mô hình học sinh 2 lớp và giảm đáng kể kích thước mô hình. FitHubert [7], được lấy cảm hứng từ FitNets [8], thiết kế một mạng học sinh mỏng nhưng sâu để cung cấp khả năng biểu diễn tốt hơn.
Cắt tỉa mô hình cố gắng loại bỏ các trọng số không quan trọng và có được một mạng con từ mô hình tiền huấn luyện. Trong cắt tỉa không có cấu trúc, các trọng số bị loại bỏ này được phân bố ngẫu nhiên trong các ma trận; trong cắt tỉa có cấu trúc, các đơn vị mạng như đầu chú ý hoặc các lớp feed-forward được loại bỏ hoàn toàn. Các mô hình được cắt tỉa có cấu trúc không yêu cầu phần cứng được thiết kế đặc biệt để tăng tốc, có thể phù hợp hơn cho các thiết bị tiêu dùng. LightHubert coi cắt tỉa mô hình như một vấn đề tìm kiếm kiến trúc mạng nơ-ron và giảm đáng kể sự suy giảm hiệu suất, nhưng quá trình tìm kiếm vẫn yêu cầu một số lựa chọn thủ công tốn thời gian [9]. Peng et al. đề xuất một phương pháp linh hoạt hơn bằng cách áp dụng phương pháp cắt tỉa dựa trên chính quy hóa L0 [10] cho mô hình Wav2vec 2.0, nhưng phương pháp của họ là đặc thù nhiệm vụ và có chi phí bổ sung khi áp dụng cho các tác vụ xuôi dòng [11].
Chúng tôi cố gắng sử dụng một phương pháp dựa trên chính quy hóa L0 tương tự để có được một mô hình nén bất khả tri nhiệm vụ. Tuy nhiên, học các mặt nạ cắt tỉa bằng chính quy hóa L0 trên các tác vụ tiền huấn luyện không giám sát như mã hóa dự đoán tương phản [12] yêu cầu tài nguyên tính toán lớn. Sự kết hợp của chưng cất và cắt tỉa là một giải pháp đầy hứa hẹn [13, 14]. Biểu diễn được cung cấp bởi mô hình tiền huấn luyện không chỉ giảm nỗ lực huấn luyện của các mô hình xuôi dòng, mà còn cung cấp thông tin độc lập nhiệm vụ cho việc cắt tỉa mô hình.
So với các phương pháp cắt tỉa không có cấu trúc hiện có của các mô hình tiếng nói tiền huấn luyện [15, 16], cắt tỉa có cấu trúc thường phải chịu sự suy giảm hiệu suất lớn hơn [17]. Điểm then chốt của vấn đề này là việc sử dụng cấu trúc thay vì các trọng số riêng lẻ làm đơn vị cơ bản của cắt tỉa làm giảm mức độ tự do, dẫn đến việc loại bỏ một số trọng số quan trọng. Để bù đắp cho sự suy giảm hiệu suất, chúng tôi giới thiệu một phương pháp cắt tỉa đầu chú ý mịn để cắt tỉa từng đầu chú ý riêng biệt. Để thúc đẩy việc cắt tỉa các cấu trúc thô và tăng tốc hơn nữa mô hình đã cắt tỉa, chúng tôi cũng giới thiệu bộ ước lượng thẳng (STE) [18] vào phương pháp cắt tỉa có cấu trúc đa tỷ lệ [13] dựa trên chính quy hóa L0.
Các thí nghiệm trên bộ đánh giá SUPERB cho thấy khả năng tổng quát hóa của mô hình đề xuất trên các tác vụ xuôi dòng khác nhau. Với sự trợ giúp của giáo viên tiền huấn luyện, mô hình đề xuất là bất khả tri nhiệm vụ và có thể được tinh chỉnh trực tiếp cho nhiều tác vụ xuôi dòng. Các thí nghiệm tương phản tiếp theo chứng minh hiệu quả của cắt tỉa đầu chú ý mịn và STE. Mô hình của chúng tôi vượt trội hơn các đường cơ sở được chưng cất, và đạt kết quả tương đương với mô hình giáo viên trên nhiều tác vụ, với ít hơn 72% tham số và nhanh hơn 2 lần về tốc độ.
2. Nền tảng
2.1. Các mô hình biểu diễn tiếng nói tiền huấn luyện
Thí nghiệm của chúng tôi chủ yếu được thực hiện trên WavLM [5], nhưng phương pháp có thể dễ dàng mở rộng cho Wav2vec 2.0 [3], data2vec

--- TRANG 2 ---
[19], Hubert [4], và các mô hình khác với cấu trúc dựa trên transformer tương tự.
WavLM là một tập hợp các mô hình tiền huấn luyện tự giám sát tiên tiến. Trong quá trình tiền huấn luyện, các đơn vị được phân cụm ngoại tuyến được sử dụng làm mục tiêu huấn luyện và các mô hình học cách biểu diễn các đầu vào liên tục bằng một số đơn vị ẩn rời rạc. WavLM cũng giới thiệu khử nhiễu tiếng nói có mặt nạ và độ lệch vị trí tương đối có cổng để cải thiện hiệu suất.
2.2. Cắt tỉa dựa trên chính quy hóa L0
Cắt tỉa dựa trên chính quy hóa L0 là một trong các phương pháp học mặt nạ. Trong một số phương pháp cắt tỉa, các tham số được loại bỏ theo một số tiêu chí được đặt một cách nhân tạo, chẳng hạn như độ lớn của trọng số hoặc gradient. Mặt khác, các phương pháp học mặt nạ có xu hướng coi cắt tỉa như một vấn đề tối ưu hóa [10].
Như tên gọi, cắt tỉa dựa trên chính quy hóa L0 thêm một mặt nạ vào các tham số (hoặc nhóm tham số) và sử dụng chuẩn L0 của các mặt nạ cắt tỉa này như một số hạng chính quy hóa của hàm mất mát. Ví dụ, trong các thí nghiệm của chúng tôi, mục tiêu huấn luyện là:
R(θ,π) =Ez∼q(π)[1
NN/summationdisplay
i=1L(fs(xi,/tildewideθ),ft(xi))+λ||/tildewideθ||0],
(1)
trong đó fs và ft là các mô hình học sinh và giáo viên cho chưng cất kiến thức, xi là dữ liệu đầu vào thứ i, θ là tập hợp tham số của mô hình học sinh, z∈ {0,1} là tập hợp mặt nạ cắt tỉa, /tildewideθ=θ⊙z là tập hợp tham số sau khi đặt mặt nạ. Biến ngẫu nhiên rời rạc z tuân theo phân phối Bernoulli q(π).
Tuy nhiên, hàm mục tiêu này không thể được tối ưu hóa bằng các phương pháp gradient descent vì quá trình lấy mẫu z cho q(π) không khả vi. Louizos et al. giới thiệu một thủ thuật tham số hóa lại để giải quyết vấn đề này [10]. Sau khi tham số hóa lại, z trở thành một biến liên tục, được xác định bởi một tham số có thể học α và một biến ngẫu nhiên bổ sung u "thu thập" tính ngẫu nhiên từ z. Nói một cách chính thức, z được tính bởi:
u∼U(0,1),s=sigmoid(1
βlog(u
1−u)+logα)
¯s=s(ζ−γ)+γ,z=hardtanh(¯s),(2)
trong đó u được lấy mẫu từ phân phối đều U(0,1), ζ= 1.1, γ=−0.1 là 2 hằng số để chia tỷ lệ s thành một khoảng lớn hơn và đảm bảo z có thể chính xác là 0 hoặc 1. β điều khiển nhiệt độ, và α là tham số có thể học.
Hình 1a cho thấy phân phối xác suất của z và ¯s, trong khi hình 1b cho thấy các giá trị của chúng như các hàm của log α. Chúng ta có thể thấy rằng thủ thuật tham số hóa lại biến các mặt nạ rời rạc z thành các biến liên tục trong khi vẫn cho phép chúng chính xác là 0 hoặc 1.
2.3. Cắt tỉa có cấu trúc đa tỷ lệ
Chính quy hóa L0 không giới hạn mức độ mịn của việc cắt tỉa. Nếu z che một số cấu trúc, chính quy hóa L0 có thể được sử dụng cho cắt tỉa có cấu trúc. Mức độ mịn có thể lớn đến một lớp hoàn chỉnh hoặc nhỏ đến một chiều nhất định của ma trận trọng số. Gần đây, Xia et al. giới thiệu một phương pháp cắt tỉa đa tỷ lệ loại bỏ các cấu trúc mịn và thô song song để thúc đẩy việc loại bỏ các cấu trúc lớn và đạt được tăng tốc thêm [13]. Chúng tôi giới thiệu phương pháp này để tăng khả năng loại bỏ các cấu trúc thô để bù đắp cho các tác động tiêu cực tiềm tàng của phương pháp cắt tỉa đầu chú ý mịn đối với tốc độ suy luận của mô hình.
3. Phương pháp
3.1. Cắt tỉa đầu chú ý mịn
Trong các nghiên cứu trước [11, 13], các đầu chú ý được sử dụng làm đơn vị nhỏ nhất cho cắt tỉa. Điều này có thể làm giảm mức độ tự do của cắt tỉa và dẫn đến suy giảm hiệu suất nhiều hơn. Để làm cho cắt tỉa có cấu trúc linh hoạt hơn, chúng tôi đề xuất một phương pháp chú ý mịn để cắt tỉa riêng biệt từng chiều của ma trận trong lớp chú ý dựa trên phương pháp cắt tỉa có cấu trúc đa tỷ lệ của Xia et.al [13]. Nói một cách chính thức, một khối transformer được đặt mặt nạ như sau:
fMHA(X) =zMHA·concat(fATT(X))
fATT(X) =Sc·(XWi
V)·diag(zi
vo)
Sc=softmax((XWi
Q)·diag(zi
qk)·(XWi
K)T)
fFFN(X) =zFFN·gelu(XWU)·diag(zint)·WD,(3)
trong đó X là dữ liệu đầu vào, Wi
Q,Wi
K,Wi
V,WO là các ma trận truy vấn, khóa, giá trị và đầu ra tương ứng. zMHA ,zi
qk,zi
vo, zFFN,zint biểu thị mặt nạ cắt tỉa cho các lớp chú ý đa đầu, ma trận chú ý, lớp feed-forward, và các chiều trung gian. Chúng tôi bỏ qua các yếu tố tỷ lệ trong fATT(X) để rõ ràng, và xin lưu ý rằng WO cũng cần được cắt tỉa theo zi
vo. Đối với WQ,WV∈Rdhidden×dhead, zi
qk và zi
vo sẽ có dhead biến.
3.2. Tối ưu hóa mặt nạ cắt tỉa với STE
Mặc dù thủ thuật tham số hóa lại làm cho z khả vi, việc giới thiệu hardtanh trong Eq. 2 tạo ra một trở ngại mới cho việc tối ưu hóa. Như được hiển thị trong Hình 1b, khi log α nhận một giá trị trong vùng tô bóng, sự hiện diện của hardtanh làm cho ∂z/∂s= 0, và tham số có thể học α không thể được cập nhật. Có nghĩa là mô hình quyết định giữ một cấu trúc khi z là 1, nhưng nó không thể đánh giá quyết định đó.
Vấn đề này trở nên rõ ràng hơn đối với cắt tỉa có cấu trúc đa tỷ lệ. Hình 3a cho thấy giá trị trung bình của zFFN không thay đổi trong quá trình huấn luyện, làm cho cắt tỉa đa tỷ lệ không hiệu quả. Lý do có thể là trong các giai đoạn đầu của huấn luyện, cắt tỉa toàn bộ lớp FFN có thể dẫn đến suy giảm hiệu suất rất lớn, vì vậy α có thể được tối ưu hóa thành một giá trị dương lớn, và khó cập nhật trong các bước huấn luyện còn lại.
Việc không cắt được các cấu trúc tỷ lệ thô sẽ khiến trọng số thưa thớt của mô hình cắt tỉa quá phân tán, dẫn đến tỷ lệ tăng tốc thấp hơn. Để giải quyết vấn đề này, chúng tôi áp dụng bộ ước lượng thẳng [18] để đảm bảo rằng

--- TRANG 3 ---
gradient có thể đi qua hàm hardtanh trong Eq. 2. Vì các gradient từ STE không phải là gradient cho hàm mất mát, việc tối ưu hóa theo hướng này có thể không dẫn đến học sinh chính xác nhất và có thể gây bất ổn gần một số cực tiểu địa phương [20]. Để ổn định huấn luyện, chúng tôi định nghĩa gradient của STE sao cho:
∂L
∂¯s=

1, if∂L
∂z>= 1;
−1, if∂L
∂z<−1;
∂L
∂z, otherwise.(4)
3.3. Mục tiêu huấn luyện
Các trạng thái ẩn của các lớp khác nhau chứa các loại thông tin khác nhau [6, 21]. Do đó, chúng tôi theo Xia et al. [13] để sử dụng chưng cất kiến thức đa nhiệm vụ có thể học để học biểu diễn của các lớp khác nhau. Chúng tôi cũng theo Wang et al. để thay đổi số hạng thứ 2 ở vế phải của eq. 1 thành một số hạng Lagrangian để kiểm soát độ thưa thớt tốt hơn [22]. Mục tiêu huấn luyện của chúng tôi như sau:
L=1
NN/summationdisplay
i=0/summationdisplay
(j,k)∈DLMSE(hj
i,ˆhk
i)+λ1(ˆp−p)+λ2(ˆp−p)2,
(5)
trong đó ˆp là độ thưa thớt mô hình xấp xỉ, p là độ thưa thớt mục tiêu. λ1 và λ2 là các tham số có thể học cho chính quy hóa Lagrangian. D là mối quan hệ ghép nối lớp giáo viên-học sinh được học trong quá trình huấn luyện [13], đối với mẫu i, hj
i và ˆhk
i là đầu ra của lớp j/k của mô hình học sinh và giáo viên tương ứng.
4. Thí nghiệm
4.1. SUPERB
SUPERB (Speech processing Universal PERformance Benchmark) là một bộ đánh giá để đánh giá hiệu suất của các mô hình tiền huấn luyện tiếng nói [23]. SUPERB cung cấp 10 tác vụ tiếng nói được định nghĩa trước từ các góc độ khác nhau nơi các mô hình tiền huấn luyện được sử dụng làm bộ trích xuất đặc trưng ngược dòng. Các tác vụ này bao gồm nhận dạng âm vị (PR), nhận dạng tiếng nói tự động (ASR), phát hiện từ khóa (KS), phát hiện thuật ngữ nói bằng truy vấn ví dụ (QbE), nhận dạng người nói (SID), xác minh người nói tự động (SV), phân đoạn người nói (SD), phân loại ý định (IC), điền khe (SF), và nhận dạng cảm xúc (ER).
4.2. Thiết lập cắt tỉa
Mô hình. Mô hình của chúng tôi được khởi tạo từ mô hình WavLM base, bao gồm một bộ trích xuất đặc trưng CNN 7 lớp và một bộ mã hóa transformer 12 lớp. Đối với các ma trận trong Eq. 3, Wi
Q,Wi
K,Wi
V∈R768×64,WO∈R768×768,WU∈ R768×3072, và WD∈R3072×768. Đối với mỗi khối transformer, chúng tôi có 12 đầu chú ý, dẫn đến 12∗64 = 768 phần tử trong zqk và zvo. Chúng tôi cũng có 3072 phần tử trong zint cho mỗi chiều trong lớp FFN, và 1 phần tử trong zMHA và zFNN để che toàn bộ lớp. Độ thưa thớt cắt tỉa mục tiêu được đặt là 80%. Mô hình giáo viên của chưng cất kiến thức cũng là mô hình WavLM base.
Dữ liệu. Chúng tôi sử dụng corpus Librispeech [24] 960 giờ cho cắt tỉa. Đối với các tác vụ SUPERB, chúng tôi sử dụng tập dữ liệu theo hướng dẫn chính thức1.
1https://github.com/s3prl/s3prl
Cắt tỉa. Cắt tỉa được thực hiện trên GPU RTX 3090 trong 200k bước và mất khoảng 36 giờ. Các siêu tham số huấn luyện của chúng tôi được chọn theo DistilHuBERT [6] và Xia et al. [13]. Tốc độ học tăng tuyến tính đến 2.0e-4 trong 7% bước đầu tiên và giảm tuyến tính về 0 trong các bước còn lại, và độ thưa thớt mục tiêu tăng tuyến tính đến 80% trong 7% bước đầu tiên và giữ nguyên trong phần còn lại.
5. Kết quả
Bảng 1 cho thấy kết quả đánh giá trên các tác vụ xuôi dòng SUPERB. Mô hình của chúng tôi có hiệu suất tương đương với mô hình giáo viên trong các tác vụ KS, IC, ER, SV, và SD, chứng minh hiệu quả của phương pháp của chúng tôi. Suy giảm hiệu suất xảy ra chủ yếu trong các tác vụ PR, ASR, và SF. Các tác vụ này yêu cầu thông tin liên quan đến nội dung phức tạp hơn, có nhiều khả năng bị mất trong quá trình cắt tỉa. Sử dụng cùng mô hình giáo viên WavLM base, phương pháp của chúng tôi vượt trội hơn các mô hình được chưng cất trong hầu hết các tác vụ, đặc biệt là trong các tác vụ liên quan đến nội dung như ASR, cho thấy mô hình của chúng tôi bảo tồn hiệu suất của mô hình giáo viên tốt hơn.
Ngoài các chỉ số đặc thù tác vụ, chúng tôi cũng sử dụng điểm SUPERB (superbs) để cung cấp đánh giá tổng thể. Điểm SUPERB là trung bình của các phép biến đổi tuyến tính của tất cả các chỉ số đặc thù tác vụ, và được xác định bởi mô hình SOTA trên bộ đánh giá và một đường cơ sở FBANK được định nghĩa trước. Tại thời điểm viết, mô hình SOTA là WavLM-Large2. Nói một cách chính thức, điểm SUPERB được định nghĩa là:
superbs=1
T/summationdisplay
t∈T1000
msota
t−mfbank
t(mu
t−mfbank
t), (6)
trong đó mu
t là chỉ số của tác vụ t và mô hình u, superbs(sota)≡ 1000, superbs(fbank)≡0.
Hình 2 cho thấy mối quan hệ giữa điểm SUPERB và số lượng tham số. Mô hình của chúng tôi vượt trội đáng kể so với các mô hình chưng cất với số lượng tham số tương tự, và thậm chí có hiệu suất vượt trội so với mô hình Wav2vec 2.0 base. Những kết quả này cho thấy phương pháp đề xuất đạt được sự cân bằng tốt hơn giữa hiệu suất và số lượng tham số so với phương pháp dựa trên chưng cất.
Chúng tôi cũng so sánh phương pháp của chúng tôi với phương pháp cắt tỉa trước đó trực tiếp loại bỏ các đầu chú ý (w/o FAHP trong Bảng 1). Một lần nữa, cải thiện chủ yếu được phản ánh trong các tác vụ
2Hiệu suất của mô hình WavLM-Large có thể được tìm thấy tại https://superbbenchmark.org/leaderboard.

--- TRANG 4 ---
Bảng 1: Kết quả trên SUPERB của mô hình đề xuất, và các đường cơ sở khác. Hiệu suất được đánh giá bởi Tỷ lệ lỗi âm vị (PER%), Độ chính xác (Acc%), Tỷ lệ lỗi từ (WER%), Giá trị có trọng số tối đa về thuật ngữ(MTWV), Điểm F1 (F1%), Tỷ lệ lỗi khái niệm (CER%), Tỷ lệ lỗi bằng nhau (EER%), và Tỷ lệ lỗi phân đoạn (DER%). DistilWavLM là sự tái tạo của chúng tôi về DistilHubert với giáo viên được thay đổi thành WavLM base; FAHP là viết tắt của phương pháp Cắt tỉa đầu chú ý mịn đề xuất.
Phương pháp KS IC PR ASR ER QbE SF SID SV SD Superbs↑ Acc↑ Acc↑ PER↓ WER↓ Acc↑ MTWV ↑ F1↑/CER↓ Acc↑ EER↓ DER↓
Đường cơ sở
wav2vec [25] 95.59 84.92 31.58 15.86 59.79 4.85 76.37/43.71 56.56 7.99 9.90 491.59
w2v2 Base 96.23 92.35 5.74 6.43 63.43 2.33 88.3/24.77 75.18 6.02 6.08 735.00
HuBERT Base 96.30 98.34 5.41 6.42 64.92 7.36 88.53/25.2 81.42 5.11 5.88 837.63
WavLM Base 96.79 98.63 4.84 6.21 65.94 8.70 89.38/22.86 84.51 4.69 4.55 895.99
Mô hình được chưng cất
DistilHuBERT 95.98 94.99 16.27 13.37 63.02 5.11 82.57/35.39 73.54 8.55 6.19 647.88
DistilWavLM 96.40 96.39 14.18 13.24 63.69 7.07 85.27/31.80 71.00 8.87 7.2 668.39
Của chúng tôi
Đề xuất 96.57 98.08 9.09 10.61 63.61 7.40 87.14/27.13 74.56 6.17 6.11 769.62
w/o FAHP 96.14 98.05 10.51 11.83 63.78 5.19 85.57/30.91 70.03 6.12 7.18 721.79

Hình 3: Hiệu quả của STE
(a) Giá trị trung bình của zFFN và zMHA.
(b) Các tham số còn lại (màu xanh) trong W0 V cho 12 lớp.

như ASR, gợi ý rằng cắt tỉa đầu chú ý mịn có thể giúp bù đắp cho việc mất thông tin phức tạp trong cắt tỉa có cấu trúc.
Hình 3a cho thấy trung bình của các mặt nạ cắt tỉa zFFN và zMHA trong quá trình cắt tỉa. Bằng cách giới thiệu STE, các mặt nạ cắt tỉa của các cấu trúc thô thay đổi thường xuyên hơn và cuối cùng giảm xuống các giá trị thấp hơn, điều này chứng minh hiệu quả của STE. Hình 3b cho thấy phân phối của các trọng số còn lại của mỗi lớp sau khi cắt tỉa. Vì các cấu trúc thô có thể được loại bỏ hoàn toàn, các tham số còn lại có xu hướng tập trung, dẫn đến tăng tốc thêm.
Ngoài ra, trọng số còn lại tập trung ở đỉnh mạng. Vì thông tin liên quan đến nội dung nổi bật hơn trong các đặc trưng của các lớp trên cùng, phân phối trọng số còn lại này có thể là một trong những lý do cho việc cải thiện mạng trong các tác vụ liên quan đến nội dung.
Chúng tôi cũng đo thời gian suy luận của 2 mô hình trên. Bảng 2 cho thấy tác động tốc độ của STE. Có thể thấy rằng phân phối trọng số tập trung do STE mang lại cải thiện đáng kể tốc độ suy luận của mô hình. Với STE, mô hình đã cắt tỉa nhanh hơn 1.4 lần với số lượng tham số tương tự.

Bảng 2: Thời gian suy luận được đo trên GPU RTX3090, bằng cách trích xuất đặc trưng của tập dev-clean librispeech và được tính trung bình trên 5 lần chạy.
Phương pháp #Params Thời gian suy luận
Triệu Giây
WavLM base 94.70 91.87(1.0x)
Đề xuất 26.57 46.08(1.99x)
w/o STE 26.37 67.78(1.35x)

Bảng 3: Ảnh hưởng của STE đến độ chính xác. ASR, IC, ER, SID là đại diện cho các tác vụ nội dung, ngoài ngôn ngữ, người nói, và ngữ nghĩa SUPERB.
Phương pháp ASR IC ER SID
WER↓ Acc↑ Acc↑ Acc↑
đề xuất 10.29 98.08 63.61 74.56
w/o STE 10.61 97.07 64.17 74.65

Hơn nữa, chúng tôi cho thấy tác động của STE đến độ chính xác. Trong số 4 tác vụ này, STE mang lại cải thiện trong ASR và IC, trong khi gây suy giảm trong ER và SID, nhưng cả ảnh hưởng tích cực và tiêu cực đều không đáng kể. Suy giảm trong ER và SID có thể do các tham số bị loại bỏ từ các lớp thấp hơn có liên quan đến thông tin người nói hoặc cảm xúc.

6. Kết luận
Trong bài báo này, chúng tôi trình bày một phương pháp cắt tỉa có cấu trúc bất khả tri nhiệm vụ của các mô hình biểu diễn tiếng nói tiền huấn luyện. Bằng cách sử dụng cắt tỉa đầu chú ý mịn, chúng tôi giữ lại khả năng biểu diễn thông tin cấp nội dung và giảm suy giảm hiệu suất do cắt tỉa có cấu trúc gây ra. Chúng tôi giới thiệu STE vào cắt tỉa có cấu trúc đa tỷ lệ để tăng tốc hơn nữa mô hình. Các thí nghiệm của chúng tôi chứng minh rằng mô hình đề xuất giảm 72% tham số trong khi có hiệu suất tương đương với mô hình dày đặc trong nhiều tác vụ, và vượt trội hơn mô hình Wav2vec2 base về hiệu suất trung bình.

--- TRANG 5 ---
7. Tài liệu tham khảo
[1] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin,
C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maaløe,
T. N. Sainath, và S. Watanabe, "Self-supervised speech
representation learning: A review," IEEE Journal of Selected
Topics in Signal Processing, vol. 16, no. 6, pp. 1179–1210,
Oct. 2022, conference Name: IEEE Journal of Selected
Topics in Signal Processing. [Online]. Available: https:
//ieeexplore.ieee.org/abstract/document/9893562
[2] J. Zhao và W.-Q. Zhang, "Improving Automatic Speech
Recognition Performance for Low-Resource Languages With
Self-Supervised Models," IEEE Journal of Selected Topics
in Signal Processing, vol. 16, no. 6, pp. 1227–1241, Oct.
2022. [Online]. Available: https://ieeexplore.ieee.org/document/
9801640/
[3] A. Baevski, Y. Zhou, A. Mohamed, và M. Auli, "Wav2vec
2.0: A framework for self-supervised learning of speech
representations," Advances in Neural Information Processing
Systems, vol. 33, pp. 12 449–12 460, 2020. [Online]. Available:
https://dl.acm.org/doi/abs/10.5555/3495724.3496768
[4] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhut-
dinov, và A. Mohamed, "HuBERT: Self-supervised speech
representation learning by masked prediction of hidden units,"
IEEE/ACM Transactions on Audio, Speech, and Language
Processing, vol. 29, pp. 3451–3460, 2021. [Online]. Available:
https://dl.acm.org/doi/abs/10.1109/TASLP.2021.3122291
[5] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li,
N. Kanda, T. Yoshioka, X. Xiao et al., "WavLM: Large-scale
self-supervised pre-training for full stack speech processing,"
IEEE Journal of Selected Topics in Signal Processing, vol. 16,
no. 6, pp. 1505–1518, 2022. [Online]. Available: https:
//x-lance.sjtu.edu.cn/en/papers/2022/zyc97-jstsp22.pdf
[6] H.-J. Chang, S.-w. Yang, và H.-y. Lee, "DistilHuBERT:
Speech representation learning by layer-wise distillation of
hidden-unit bert," in ICASSP 2022-2022 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 2022, pp. 7087–7091. [Online]. Available:
https://ieeexplore.ieee.org/document/9747490/
[7] Y. Lee, K. JANG, J. Goo, Y. Jung, và H.-R. Kim,
"FitHuBERT: Going thinner and deeper for knowledge dis-
tillation of speech self-supervised learning," in 23rd Annual
Conference of the International Speech Communication Asso-
ciation, INTERSPEECH 2022. ISCA, 2022, pp. 3588–3592.
[Online]. Available: https://www.isca-speech.org/archive//pdfs/
interspeech_2022/lee22p_interspeech.pdf
[8] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,
và Y. Bengio, "Fitnets: Hints for thin deep nets," arXiv
preprint arXiv:1412.6550, 2014. [Online]. Available: https:
//arxiv.org/abs/1412.6550
[9] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y. Zhang,
T. Ko, và H. Li, "LightHuBERT: Lightweight and Configurable
Speech Representation Learning with Once-for-All Hidden-Unit
BERT," in Interspeech 2022. ISCA, Sep. 2022, pp. 1686–
1690. [Online]. Available: https://www.isca-speech.org/archive/
interspeech_2022/wang22t_interspeech.html
[10] C. Louizos, M. Welling, và D. Kingma, "Learning sparse
neural networks through l0 regularization." in Sixth International
Conference on Learning Representations, 2018, 2018. [Online].
Available: https://openreview.net/pdf?id=H1Y8hhg0b
[11] Y. Peng, K. Kim, F. Wu, P. Sridhar, và S. Watanabe, "Structured
pruning of self-supervised pre-trained models for speech
recognition and understanding," in 2023 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP), Jun. 2023, pp. 1–5. [Online]. Available: https:
//ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095780
[12] A. v. d. Oord, Y. Li, và O. Vinyals, "Representation learning with
contrastive predictive coding," arXiv preprint arXiv:1807.03748,
2018. [Online]. Available: https://arxiv.org/abs/1807.03748
[13] M. Xia, Z. Zhong, và D. Chen, "Structured pruning learns
compact and accurate models," in Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers). Dublin, Ireland: Association for
Computational Linguistics, May 2022, pp. 1513–1528. [Online].
Available: https://aclanthology.org/2022.acl-long.107
[14] V. Sanh, T. Wolf, và A. Rush, "Movement pruning: Adaptive
sparsity by fine-tuning," Advances in Neural Information
Processing Systems, vol. 33, pp. 20 378–20 389, 2020. [Online].
Available: https://proceedings.neurips.cc/paper_files/paper/2020/
file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf
[15] M. Yang, A. Tjandra, C. Liu, D. Zhang, D. Le, và
O. Kalinli, "Learning ASR pathways: A sparse multilingual ASR
model," in 2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), Jun. 2023, pp. 1–5.
[Online]. Available: https://ieeexplore.ieee.org/stamp/stamp.jsp?
arnumber=10094300
[16] C.-I. J. Lai, Y. Zhang, A. H. Liu, S. Chang, Y.-L. Liao,
Y.-S. Chuang, K. Qian, S. Khurana, D. Cox, và J. Glass,
"PARP: Prune, adjust and re-prune for self-supervised speech
recognition," Oct. 2021, arXiv:2106.05933 [cs, eess]. [Online].
Available: http://arxiv.org/abs/2106.05933
[17] Z. Liu, M. Sun, T. Zhou, G. Huang, và T. Darrell,
"Rethinking the value of network pruning," in International
Conference on Learning Representations, 2018. [Online].
Available: https://openreview.net/pdf?id=rJlnB3C5Ym
[18] Y. Bengio, N. Léonard, và A. Courville, "Estimating or
propagating gradients through stochastic neurons for conditional
computation," arXiv preprint arXiv:1308.3432, 2013. [Online].
Available: https://arxiv.org/abs/1308.3432
[19] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, và
M. Auli, "Data2vec: A general framework for self-supervised
learning in speech, vision and language," in International
Conference on Machine Learning. PMLR, 2022, pp. 1298–
1312. [Online]. Available: https://proceedings.mlr.press/v162/
baevski22a/baevski22a.pdf
[20] P. Yin, J. Lyu, S. Zhang, S. J. Osher, Y. Qi, và J. Xin,
"Understanding straight-through estimator in training activation
quantized neural nets," in International Conference on Learning
Representations, 2019. [Online]. Available: https://openreview.
net/forum?id=Skh4jRcKQ
[21] L. Chen, M. Asgari, và H. H. Dodge, "Optimize Wav2vec2s
architecture for small training set through analyzing its pre-
trained models attention pattern," in 2022 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP), May 2022, pp. 7112–7116. [Online]. Available:
https://ieeexplore.ieee.org/document/9747831
[22] Z. Wang, J. Wohlwend, và T. Lei, "Structured pruning of large
language models," in Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP),
2020, pp. 6151–6162. [Online]. Available: https://aclanthology.
org/2020.emnlp-main.496.pdf
[23] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakho-
tia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T.
Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu,
Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed,
và H. yi Lee, "SUPERB: Speech Processing Universal PER-
formance Benchmark," in Proc. Interspeech 2021, 2021, pp.
1194–1198. [Online]. Available: https://www.isca-speech.org/
archive/interspeech_2021/yang21c_interspeech.html
[24] V. Panayotov, G. Chen, D. Povey, và S. Khudanpur,
"Librispeech: an ASR corpus based on public domain audio
books," in 2015 IEEE international conference on acoustics,
speech and signal processing (ICASSP). IEEE, 2015, pp.
5206–5210. [Online]. Available: https://www.danielpovey.com/
files/2015_icassp_librispeech.pdf
[25] S. Schneider, A. Baevski, R. Collobert, và M. Auli, "Wav2vec:
Unsupervised Pre-Training for Speech Recognition," in Proc.
Interspeech 2019, 2019, pp. 3465–3469. [Online]. Available:
http://dx.doi.org/10.21437/Interspeech.2019-1873
