# 2302.06354.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2302.06354.pdf
# Kích thước tệp: 2969469 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Ít hơn là Nhiều hơn:
Tinh chỉnh Lớp Chọn lọc với SubTuning
Gal Kaplun∗
Harvard University & MobileyeAndrey Gurevich
MobileyeTal Swisa
Mobileye
Mazor David
MobileyeShai Shalev-Shwartz
Hebrew University & MobileyeEran Malach
Hebrew University & Mobileye
Tóm tắt
Tinh chỉnh một mô hình đã được huấn luyện trước đã trở thành phương pháp tiêu chuẩn để huấn luyện mạng nơ-ron cho các nhiệm vụ mới, dẫn đến hội tụ nhanh chóng và hiệu suất được cải thiện. Trong công trình này, chúng tôi trình bày một phương pháp tinh chỉnh hiệu quả về tham số, trong đó chúng tôi chọn lọc huấn luyện một tập con được lựa chọn cẩn thận của các lớp trong khi giữ các trọng số còn lại bị đóng băng ở giá trị ban đầu (đã được huấn luyện trước). Chúng tôi quan sát thấy rằng không phải tất cả các lớp đều được tạo ra như nhau: các lớp khác nhau trên mạng đóng góp khác nhau vào hiệu suất tổng thể, và việc lựa chọn tối ưu các lớp phụ thuộc vào nhiệm vụ hạ nguồn và phân phối dữ liệu cơ bản. Chúng tôi chứng minh rằng phương pháp được đề xuất của chúng tôi, được gọi là tinh chỉnh tập con (hoặc SubTuning), mang lại một số lợi thế so với tinh chỉnh thông thường. Chúng tôi cho thấy rằng SubTuning vượt trội hơn cả tinh chỉnh và thăm dò tuyến tính trong các tình huống với dữ liệu khan hiếm hoặc bị hỏng, đạt được kết quả tối tân so với các phương pháp cạnh tranh để tinh chỉnh trên các tập dữ liệu nhỏ. Khi dữ liệu dồi dào, SubTuning thường đạt được hiệu suất tương đương với tinh chỉnh trong khi đồng thời cho phép suy luận hiệu quả trong bối cảnh đa nhiệm vụ khi được triển khai cùng với các mô hình khác. Chúng tôi trình bày hiệu quả của SubTuning trên các nhiệm vụ khác nhau, kiến trúc mạng đa dạng và phương pháp huấn luyện trước.

1 Giới thiệu
Học chuyển giao từ một mô hình lớn đã được huấn luyện trước đã trở thành một phương pháp được sử dụng rộng rãi để đạt được hiệu suất tối ưu trên một loạt các nhiệm vụ học máy đa dạng trong cả Thị giác Máy tính và Xử lý Ngôn ngữ Tự nhiên [2,8,64,66]. Theo truyền thống, mạng nơ-ron được huấn luyện "từ đầu", trong đó ở đầu quá trình huấn luyện, các trọng số của mạng được khởi tạo ngẫu nhiên. Tuy nhiên, trong học chuyển giao, chúng ta sử dụng các trọng số của một mô hình đã được huấn luyện trên một nhiệm vụ khác làm điểm bắt đầu để huấn luyện trên nhiệm vụ mới, thay vì sử dụng khởi tạo ngẫu nhiên. Trong phương pháp này, chúng ta thường thay thế lớp cuối cùng (readout) của mô hình bằng một "đầu" mới được điều chỉnh cho nhiệm vụ mới, và điều chỉnh phần còn lại của mô hình (backbone), bắt đầu từ các trọng số đã được huấn luyện trước. Việc sử dụng backbone đã được huấn luyện trước cho phép tận dụng kiến thức thu được từ một tập dữ liệu lớn, dẫn đến thời gian hội tụ nhanh hơn và hiệu suất được cải thiện, đặc biệt khi dữ liệu huấn luyện cho nhiệm vụ hạ nguồn mới là khan hiếm.

Các phương pháp phổ biến nhất cho học chuyển giao là thăm dó tuyến tính và tinh chỉnh. Trong thăm dò tuyến tính, chỉ có đầu readout tuyến tính được huấn luyện trên nhiệm vụ mới, trong khi các trọng số của tất cả các lớp khác trong mô hình bị đóng băng ở giá trị ban đầu (đã được huấn luyện trước). Phương pháp này rất nhanh và hiệu quả về số lượng tham số được huấn luyện, nhưng có thể không tối ưu do khả năng thấp trong việc khớp mô hình với dữ liệu huấn luyện mới. Thay vào đó, cũng phổ biến là tinh chỉnh tất cả các tham số của

Bản thảo. Đang được xem xét.
∗Tác giả liên hệ. galkaplun@g.harvard.edu .arXiv:2302.06354v3  [cs.LG]  2 Jul 2023

--- TRANG 2 ---
mô hình đã được huấn luyện trước cho nhiệm vụ mới. Phương pháp này thường đạt được hiệu suất tốt hơn thăm dò tuyến tính, nhưng thường tốn kém hơn về dữ liệu huấn luyện và tính toán.

B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B160.9400.9450.9500.955
Hồ sơ Tinh chỉnh: ResNet-50, CIFAR-10
Lớp 1
Lớp 2Lớp 3
Lớp 4
1 Khối 2 Khối 3 Khối Tinh chỉnh Đầy đủ
Các lớp được tinh chỉnh0.700.730.770.800.830.870.90
Tinh chỉnh CIFAR-10 sang CIFAR-10-C so với SubTuning
làm mờ kính
làm mờ zoom
jpeg
Trung bình
100 500 1000 5000 10000 50000
Mẫu huấn luyện0.60.70.80.9
Mẫu so với Độ chính xác (CIFAR-10)
SubTuning
Thăm dò Tuyến tính
Tinh chỉnh      Độ chính xác
Hình 1: Trái. Hồ sơ Tinh chỉnh của ResNet-50 được huấn luyện trước trên ImageNet và được tinh chỉnh trên CIFAR-10. Trên trục x chúng ta có 16 res-block trong đó mỗi Lớp (với chữ L viết hoa) tương ứng với một sự giảm độ phân giải không gian. Giữa. SubTuning trên các dịch chuyển phân phối CIFAR-10-C với ResNet-26. Ngay cả với số ít khối dư được lựa chọn thích hợp, SubTuning có thể tốt hơn Tinh chỉnh. Phải. Ảnh hưởng của kích thước tập dữ liệu lên SubTuning, Tinh chỉnh và Thăm dò Tuyến tính. SubTuning thể hiện hiệu suất tốt trên tất cả kích thước tập dữ liệu, thể hiện tính linh hoạt của nó. Dưới. Minh họa SubTuning. Chúng tôi chỉ tinh chỉnh một tập con được lựa chọn chiến lược của các lớp và lớp readout cuối cùng, trong khi phần còn lại của các lớp bị đóng băng ở giá trị đã được huấn luyện trước.

Trong bài báo này, chúng tôi đề xuất một phương pháp thay thế đơn giản, phục vụ như một điểm giữa giữa thăm dò tuyến tính và tinh chỉnh đầy đủ. Nói một cách đơn giản, chúng tôi đề xuất huấn luyện một tập con nhỏ được lựa chọn cẩn thận của các lớp trong mạng. Phương pháp này, mà chúng tôi gọi là SubTuning (xem Hình 1), cho phép tìm một điểm tối ưu giữa thăm dò tuyến tính và tinh chỉnh đầy đủ. SubTuning tận hưởng những điều tốt nhất của cả hai thế giới: nó hiệu quả về số lượng tham số được huấn luyện, trong khi vẫn tận dụng khả năng tính toán của việc huấn luyện các lớp sâu trong mạng. Chúng tôi cho thấy rằng SubTuning là một phương pháp học chuyển giao ưa thích khi dữ liệu bị hạn chế (xem Hình 1), bị hỏng hoặc trong bối cảnh đa nhiệm vụ với các ràng buộc tính toán. Chúng tôi so sánh phương pháp của chúng tôi trong các thiết lập khác nhau với thăm dò tuyến tính và tinh chỉnh, cũng như các phương pháp gần đây khác cho học chuyển giao hiệu quả về tham số (ví dụ: Head2Toe [13] và LoRA [22]).

Đóng góp chính của công trình này là việc phát triển thuật toán SubTuning, tạo cầu nối khoảng cách giữa thăm dò tuyến tính và tinh chỉnh đầy đủ bằng cách chọn lọc huấn luyện một tập con của các lớp trong mạng nơ-ron. Phương pháp này cung cấp một giải pháp linh hoạt và hiệu quả hơn cho học chuyển giao, đặc biệt trong các tình huống khi dữ liệu khan hiếm hoặc bị tổn hại, và tài nguyên tính toán bị hạn chế. Hơn nữa, các đánh giá thực nghiệm của chúng tôi chứng minh hiệu quả của SubTuning so với các kỹ thuật học chuyển giao hiện có, nổi bật tiềm năng của nó để được áp dụng rộng rãi trong các ứng dụng và thiết lập khác nhau.

Đóng góp của Chúng tôi. Chúng tôi tóm tắt các đóng góp của chúng tôi như sau:
•Chúng tôi nâng cao hiểu biết của chúng ta về tinh chỉnh bằng cách giới thiệu khái niệm hồ sơ tinh chỉnh, một công cụ có giá trị mang lại cái nhìn mới về tầm quan trọng của các lớp riêng lẻ trong quá trình tinh chỉnh. Khái niệm này được mở rộng thêm trong Phần 2.
•Chúng tôi trình bày SubTuning, một thuật toán đơn giản nhưng hiệu quả mà chọn lọc tinh chỉnh các lớp cụ thể dựa trên chiến lược lựa chọn tham lam sử dụng hồ sơ tinh chỉnh. Trong Phần 3, chúng tôi cung cấp bằng chứng rằng SubTuning thường xuyên vượt trội hơn hiệu suất của các phương pháp học chuyển giao cạnh tranh trong các nhiệm vụ khác nhau liên quan đến dữ liệu bị hạn chế hoặc bị hỏng.
•Chúng tôi trình bày hiệu quả và hiệu quả thời gian chạy tính toán của SubTuning trong bối cảnh học đa nhiệm vụ. Phương pháp này cho phép triển khai nhiều mạng được tinh chỉnh cho các nhiệm vụ hạ nguồn riêng biệt với chi phí tính toán tối thiểu, như được thảo luận trong Phần 4.

1.1 Công trình Liên quan

Học Chuyển giao Hiệu quả về Tham số. Trong những năm gần đây, việc tinh chỉnh các mô hình lớn đã được huấn luyện trước ngày càng trở nên phổ biến [11,50,17]. Khi tính phổ biến của việc tinh chỉnh các mô hình này tăng lên, tầm quan trọng của việc triển khai chúng một cách hiệu quả để giải quyết các nhiệm vụ hạ nguồn mới cũng tăng theo. Do đó, đã có một sự quan tâm ngày càng tăng, đặc biệt trong lĩnh vực NLP, về Học Chuyển giao Hiệu quả về Tham số (PETL) [58,63,13,68,51,43,67,19] trong đó chúng ta hoặc sửa đổi một số lượng nhỏ tham số, thêm một vài lớp nhỏ hoặc che dấu [69] hầu hết mạng. Việc chỉ sử dụng một phần nhỏ các tham số cho mỗi nhiệm vụ có thể giúp tránh quên thảm khốc [45] và có thể là một giải pháp hiệu quả cho cả học đa nhiệm vụ và học liên tục. Các phương pháp này bao gồm Prompt Tuning [39,35,24], adapter [21,52,6,53], LoRA [22], sidetuning [68], lựa chọn đặc trưng [13] và masking [51]. Fu et al. [15], và He et al. [16], (xem thêm các tài liệu tham khảo trong đó) cố gắng xây dựng một phương pháp thống nhất của PETL và đề xuất các phương pháp cải tiến.

Trong một nghiên cứu gần đây, Lee et al. [33], đã điều tra tác động của tinh chỉnh lớp chọn lọc trên các tập dữ liệu nhỏ và thấy rằng nó hiệu quả hơn tinh chỉnh truyền thống. Họ quan sát thấy rằng việc huấn luyện các lớp khác nhau mang lại kết quả khác nhau, tùy thuộc vào sự dịch chuyển trong phân phối dữ liệu. Cụ thể, họ thấy rằng khi có sự dịch chuyển nhãn giữa dữ liệu nguồn và đích, các lớp sau hoạt động tốt hơn, nhưng trong trường hợp hỏng hình ảnh, các lớp đầu hiệu quả hơn.

Trong khi công trình của chúng tôi có một số điểm tương đồng với Lee et al., động cơ và thiết lập thực nghiệm về cơ bản khác nhau. Chủ yếu, chúng tôi đi sâu hơn vào sự tương tác phức tạp giữa các lớp thích hợp để tinh chỉnh và nhiệm vụ hạ nguồn, mục tiêu huấn luyện trước và kiến trúc mô hình, và quan sát rằng cần có một quan điểm tinh tế hơn. Như được chứng minh bởi các hồ sơ tinh chỉnh của chúng tôi (ví dụ: xem Hình 2 và Hình 5), một lời giải thích đơn giản về lớp nào cần tinh chỉnh dựa trên loại hỏng hóc là rất không phổ quát và phương pháp chính xác đòi hỏi lựa chọn lớp chiến lược, như được chứng minh trong phương pháp tham lam của chúng tôi.

Hơn nữa, chúng tôi cho thấy rằng tinh chỉnh với lựa chọn lớp không chỉ khả thi cho việc thích ứng với dữ liệu nhỏ bị hỏng mà còn cho các dịch chuyển phân phối chung (trong một số trường hợp chúng tôi đạt được hiệu suất tối tân) và thậm chí cho các tập dữ liệu lớn hơn. Ngoài ra, phương pháp của chúng tôi có thể được tối ưu hóa cho hiệu quả thời gian suy luận trong thiết lập Học Đa Nhiệm vụ (MTL).

Chúng tôi lưu ý rằng các hồ sơ tinh chỉnh của chúng tôi cung cấp một cái nhìn độc đáo về hiểu biết cơ chế của tinh chỉnh, làm cho nghiên cứu của chúng tôi không chỉ thực tế trong các thiết lập MTL và PETL mà còn có ý nghĩa khoa học. Chúng tôi cũng lưu ý rằng SubTuning tương thích với nhiều phương pháp PETL khác, và việc kết hợp SubTuning với các phương pháp như LoRA và Head2Toe là một hướng nghiên cứu đầy hứa hẹn mà chúng tôi để dành cho công việc tương lai.

Học Đa Nhiệm vụ. Mạng nơ-ron thường được sử dụng để giải quyết nhiều nhiệm vụ. Các nhiệm vụ này thường chia sẻ các tính chất tương tự, và việc giải quyết chúng đồng thời cho phép chia sẻ các đặc trưng chung có thể nắm bắt kiến thức có liên quan cho tất cả các nhiệm vụ [5]. Tuy nhiên, MTL cũng đưa ra những thách thức đáng kể, chẳng hạn như chuyển giao tiêu cực [41], cân bằng mất mát [40,46], khó khăn tối ưu hóa [48], cân bằng và trộn dữ liệu [49]. Trong khi những vấn đề này có thể được giảm thiểu bằng cách lấy mẫu dữ liệu cẩn thận và điều chỉnh hàm mất mát, những giải pháp này thường dễ vỡ [65]. Trong một thiết lập liên quan được gọi là Học Liên tục [59,54,29,27], việc thêm nhiệm vụ mới cần phải xảy ra trên các nhiệm vụ đã được triển khai trước đó, trong khi mất quyền truy cập vào dữ liệu cũ hơn do ràng buộc lưu trữ hoặc quyền riêng tư, làm phức tạp thêm vấn đề. Trong bối cảnh này, chúng tôi cho thấy rằng các nhiệm vụ mới có thể được thêm vào một cách hiệu quả bằng SubTuning, mà không ảnh hưởng đến hiệu suất hoặc gây suy giảm các nhiệm vụ đã học trước đó (Phần 4).

2 Không Phải Tất cả Các Lớp Đều Được Tạo Ra Như Nhau

Trong quá trình tinh chỉnh mạng nơ-ron sâu, một khía cạnh quan trọng nhưng thường bị đánh giá thấp là sự đóng góp không bằng nhau của các lớp riêng lẻ vào hiệu suất tổng thể của mô hình. Sự biến đổi này trong tầm quan trọng của lớp đặt câu hỏi về các giả định phổ biến và đòi hỏi một phương pháp tinh vi hơn để cải thiện hiệu quả quá trình tinh chỉnh. Bằng cách chọn lọc huấn luyện các lớp, có thể phân bổ tài nguyên tính toán một cách chiến lược và cải thiện hiệu suất của mô hình. Để xác định các thành phần thiết yếu trong mạng, chúng tôi kiểm tra hai phương pháp liên quan: xây dựng hồ sơ tinh chỉnh bằng cách quét để tìm lớp (hoặc khối lớp) tối ưu với độ phức tạp O(số lớp), và thuật toán SubTuning Tham lam, trong đó chúng tôi lặp đi lặp lại tận dụng hồ sơ tinh chỉnh để chọn k-lớp từng cái một, trong khi sử dụng độ phức tạp cao hơn O(số lớp · k).

Hồ sơ Tinh chỉnh. Chúng tôi bắt đầu bằng cách tiến hành một phân tích toàn diện về tầm quan trọng của việc tinh chỉnh các thành phần khác nhau của mạng. Phân tích này hướng dẫn việc lựa chọn tập con của các lớp được sử dụng cho SubTuning. Để thực hiện điều này, chúng tôi chạy một loạt thí nghiệm trong đó chúng tôi

--- TRANG 3 ---

--- TRANG 4 ---
Lớp 1
Lớp 2
Lớp 3
Lớp 4
Lớp 5
Lớp 6
Lớp 7
Lớp 8
Lớp 9
Lớp 10
Lớp 11
Lớp 120.9620.9640.9660.9680.9700.9720.974Hồ sơ Tinh chỉnh: ViT-B/16 trên CIFAR-10
B1
B2
B3
B4
B5
B6
B7
B80.9000.9050.9100.9150.9200.9250.9300.935
Hồ sơ Tinh chỉnh: ResNet-18 trên CIFAR-10
B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B160.9200.9250.9300.9350.9400.945
Hồ sơ Tinh chỉnh: ResNet-50(DINO) trên CIFAR-10B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B160.550.600.650.700.750.800.850.90
Hồ sơ Tinh chỉnh: ResNet-50 trên Flowers102
B1
B2
B3
B4
B5
B6
B7
B80.500.550.600.650.700.750.800.85
Hồ sơ Tinh chỉnh: ResNet-18 trên Flowers102
B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B160.770.780.790.800.81
Hồ sơ Tinh chỉnh: ResNet-50 trên CIFAR-100Độ chính xác
Lớp để Tinh chỉnhHình 2: Hồ sơ tinh chỉnh cho các kiến trúc, khởi tạo và tập dữ liệu khác nhau.

cố định một tập con cụ thể của các lớp liên tiếp trong mạng và chỉ tinh chỉnh những lớp này, trong khi duy trì các trọng số ban đầu (đã được huấn luyện trước) cho các lớp còn lại.

Ví dụ, chúng tôi lấy một ResNet-50 được huấn luyện trước trên tập dữ liệu ImageNet, và tinh chỉnh nó trên tập dữ liệu CIFAR-10, thay thế lớp readout của ImageNet (có 1000 lớp) bằng một lớp readout được điều chỉnh cho CIFAR-10 (với 10 lớp). Như đã lưu ý, trong các thí nghiệm của chúng tôi, chúng tôi không tinh chỉnh tất cả các trọng số của mạng, mà thay vào đó chỉ tối ưu hóa một vài lớp từ mô hình (cũng như lớp readout). Cụ thể, vì kiến trúc ResNet-50 được cấu thành từ 16 khối (tức là ResBlocks, xem Phụ lục A và [18] để biết thêm chi tiết), chúng tôi chọn chạy 16 thí nghiệm, trong đó ở mỗi thí nghiệm chúng tôi chỉ huấn luyện một khối, cố định các trọng số của tất cả các khối khác ở giá trị ban đầu (đã được huấn luyện trước). Sau đó chúng tôi vẽ đồ thị độ chính xác của mô hình như một hàm của khối mà chúng tôi huấn luyện, như được trình bày trong Hình 1 bên trái. Chúng tôi gọi đồ thị này là hồ sơ tinh chỉnh của mạng. Theo một giao thức tương tự (xem Phụ lục A), chúng tôi tính toán hồ sơ tinh chỉnh cho các kết hợp khác nhau của kiến trúc (ResNet-18, ResNet-50 và ViT-B/16), phương pháp huấn luyện trước (có giám sát và DINO [4]), và nhiệm vụ đích (CIFAR-10, CIFAR-100 và Flower102). Trong Hình 2, chúng tôi trình bày các hồ sơ cho các thiết lập khác nhau.

B1B2B3B4B5B6B7B8B9B10B11B12B13B14B15B16B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B16SubTuning Hai Khối - CIFAR-10
95.0095.2595.5095.7596.0096.2596.5096.75
Hình 3: Hồ sơ tinh chỉnh 2 khối cho ResNet-50 trên CIFAR-10.Kết quả. Thú vị thay, các phát hiện của chúng tôi chỉ ra rằng đối với hầu hết các kiến trúc và tập dữ liệu, tầm quan trọng của một lớp không thể được dự đoán bằng cách chỉ quan sát các tính chất như độ sâu của lớp, số lượng tham số trong lớp hoặc độ phân giải không gian của nó. Thực tế, cùng một kiến trúc có thể có các hồ sơ tinh chỉnh khác biệt rõ rệt khi được huấn luyện trên một nhiệm vụ hạ nguồn khác hoặc từ khởi tạo khác (xem Hình 2). Trong khi chúng tôi thấy rằng các lớp gần đầu vào có xu hướng đóng góp ít hơn vào quá trình tinh chỉnh, hiệu suất của mạng thường không tăng đơn điệu theo độ sâu hoặc theo số lượng tham số¹, và sau một điểm nhất định hiệu suất thường bắt đầu giảm khi huấn luyện các lớp sâu hơn. Ví dụ, trong hồ sơ tinh chỉnh của ResNet-50 được tinh chỉnh trên tập dữ liệu CIFAR-10 (Hình 1 bên trái), chúng ta thấy rằng tinh chỉnh Khối 13 dẫn đến hiệu suất tốt hơn đáng kể so với tối ưu hóa Khối 16, mặc dù Khối 16 sâu hơn và có nhiều tham số hơn. Chúng tôi cũng xem xét hiệu ứng của việc tinh chỉnh nhiều khối liên tiếp hơn. Trong Hình 9 (ở Phụ lục B) chúng tôi trình bày các hồ sơ tinh chỉnh cho việc huấn luyện nhóm 2 và 3 khối liên tiếp. Kết quả chỉ ra rằng tinh chỉnh nhiều lớp hơn cải thiện hiệu suất, và cũng làm cho hồ sơ tinh chỉnh trở nên đơn điệu hơn.

¹Trong các kiến trúc ResNet, các khối sâu hơn có nhiều tham số hơn, trong khi đối với ViT tất cả các lớp có số lượng tham số bằng nhau.

--- TRANG 5 ---
Lựa chọn Tham lam. Cuộc thảo luận cho đến nay dẫn đến một câu hỏi về hậu quả của việc huấn luyện các lớp tùy ý (có thể không liên tiếp). Đầu tiên, chúng tôi quan sát thấy rằng các kết hợp khác nhau của các lớp có những tương tác không tầm thường, và do đó việc chỉ chọn các tập con của các lớp liên tiếp có thể không tối ưu. Ví dụ, trong Hình 3 chúng tôi vẽ đồ thị độ chính xác của việc huấn luyện tất cả các tập con có thể có của hai khối từ ResNet-50, và quan sát thấy rằng hiệu suất tối ưu được đạt bởi Khối 2 và Khối 14. Do đó, cần có một sự lựa chọn cẩn thận các lớp để huấn luyện.

Một phương pháp vũ phu để kiểm tra tất cả các tập con có thể có của k lớp sẽ dẫn đến gánh nặng tính toán O(số lớp^k). Để giải quyết vấn đề này, chúng tôi giới thiệu một thuật toán tham lam hiệu quả với chi phí O(số lớp · k). Thuật toán này lặp đi lặp lại chọn lớp mang lại đóng góp biên lớn nhất cho độ chính xác xác thực, cho trước các lớp hiện được chọn. Quá trình lựa chọn lớp được dừng lại khi lợi ích biên giảm xuống dưới một ngưỡng được xác định trước, ε, sau đó các lớp được chọn được tinh chỉnh. Mã giả cho thuật toán này được mô tả trong Thuật toán 1 ở Phụ lục A. Chúng tôi lưu ý rằng tối ưu hóa tham lam như vậy là một phương pháp phổ biến cho lựa chọn tập con trong các bài toán tổ hợp khác nhau, và được biết là xấp xỉ giải pháp tối ưu dưới một số giả định nhất định. Chúng tôi cho thấy rằng SubTuning dẫn đến hiệu suất tương đương với tinh chỉnh đầy đủ ngay cả đối với các tập dữ liệu đầy đủ (xem Hình 1 bên phải).

2.1 Động cơ Lý thuyết

Bây giờ chúng tôi cung cấp một số biện minh lý thuyết để sử dụng SubTuning Tham lam khi kích thước dữ liệu bị hạn chế. Ký hiệu θ∈R^r là một tập các tham số đã được huấn luyện trước ban đầu, và f_θ là mạng gốc sử dụng các tham số này. Trong tinh chỉnh tiêu chuẩn, chúng ta điều chỉnh θ trên nhiệm vụ mới, dẫn đến một tập tham số mới θ̃, thỏa mãn ||θ̃−θ||_2≤∆. Sử dụng khai triển Taylor bậc nhất, khi ∆ nhỏ, chúng ta có:

f_θ̃(x)≈f_θ(x) + ⟨∇f_θ(x),θ̃−θ⟩ = ⟨ψ_θ(x),w⟩

đối với một ánh xạ nào đó của đầu vào ψ_θ (thường được gọi là Neural Tangent Kernel [23]), và một vector w nào đó có chuẩn ≤∆. Bây giờ, nếu chúng ta tối ưu hóa w trên một tập dữ liệu kích thước m, sử dụng các ràng buộc tổng quát hóa dựa trên chuẩn tiêu chuẩn [61], chúng ta có thể chỉ ra rằng khả năng tổng quát hóa của bộ phân loại thu được là O(√(r∆)/√m), trong đó r là số lượng tham số trong mạng. Điều này có nghĩa là nếu số lượng tham số lớn, chúng ta sẽ cần nhiều mẫu để đạt được hiệu suất tốt.

SubTuning có thể dẫn đến các bảo đảm tổng quát hóa tốt hơn nhiều. Vì trong SubTuning chúng ta chỉ huấn luyện một tập con của các tham số của mạng, chúng ta có thể hy vọng rằng khả năng tổng quát hóa chỉ phụ thuộc vào số lượng tham số trong các lớp được huấn luyện. Điều này không đúng ngay lập tức, vì thuật toán SubTuning Tham lam tái sử dụng cùng một tập dữ liệu trong khi tìm kiếm tập con tối ưu, điều này có thể làm tăng độ phức tạp mẫu (tức là khi tập con tối ưu bị "overfitted" với tập huấn luyện). Tuy nhiên, một phân tích cẩn thận cho thấy rằng SubTuning Tham lam thực sự cho phép cải thiện các bảo đảm tổng quát hóa, và việc tối ưu hóa tập con chỉ thêm các yếu tố logarit vào độ phức tạp mẫu:

Định lý 1. Giả sử chúng ta chạy SubTuning Tham lam trên một mạng với L lớp, điều chỉnh nhiều nhất k lớp với r'≪r tham số. Khi đó lỗi tổng quát hóa của bộ phân loại thu được là O(√(r'∆ log(kL))/√m).

Chúng tôi đưa ra chứng minh của định lý trên trong Phụ lục. Quan sát rằng các thí nghiệm được báo cáo trong Hình 1 (phải) thực sự xác nhận tính ưu việt của SubTuning về độ phức tạp mẫu.

3 SubTuning cho Chế độ Dữ liệu Thấp

Trong phần này, chúng tôi tập trung vào tinh chỉnh trong chế độ dữ liệu thấp. Như đã đề cập, học chuyển giao là một phương pháp phổ biến trong thiết lập này, tận dụng sức mạnh của một mô hình đã được huấn luyện trước trên lượng lớn dữ liệu. Chúng tôi cho thấy rằng trong bối cảnh này, SubTuning có thể vượt trội hơn cả thăm dò tuyến tính và tinh chỉnh đầy đủ, cũng như các phương pháp học chuyển giao hiệu quả tham số khác. Ngoài ra, chúng tôi chứng minh lợi ích của việc sử dụng SubTuning khi dữ liệu bị hỏng.

3.1 Đánh giá SubTuning trong Chế độ Dữ liệu Thấp

Chúng tôi nghiên cứu các lợi thế của việc sử dụng SubTuning khi dữ liệu khan hiếm, so với các phương pháp học chuyển giao khác. Bên cạnh thăm dò tuyến tính và tinh chỉnh, chúng tôi cũng so sánh phương pháp của chúng tôi với

--- TRANG 6 ---
93.4 93.8 93.9 94.2 94.0 94.3 94.3 94.2 94.3 94.0 94.0 93.7 93.7 93.4 92.6 91.5 10k
92.2 92.7 92.7 92.9 92.8 93.0 93.2 92.4 92.7 92.5 92.2 92.3 92.2 92.0 91.0 89.8 5k
90.1 90.9 90.9 90.8 90.8 91.5 91.3 89.6 90.5 90.5 90.0 90.1 90.0 89.9 89.4 88.5 2.5k
85.0 86.8 87.1 85.4 86.3 87.3 88.0 84.6 85.7 86.3 86.1 86.3 86.5 86.5 86.4 86.3 1k
78.0 81.5 81.3 79.0 80.7 82.3 82.9 78.8 80.0 80.4 82.4 81.7 82.9 82.7 83.2 84.3 500
B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B1648.1 61.4 67.1 54.8 60.6 64.5 62.9 60.9 57.9 61.6 62.9 66.0 67.3 65.4 69.3 74.4 100Khối so với Kích thước Tập con CIFAR-10Hình 4: SubTuning khối đơn của ResNet-50 trên CIFAR-10. Trục y là kích thước tập dữ liệu, trục x là khối được chọn. Với kích thước tập dữ liệu tăng, việc huấn luyện các lớp trước đó tỏ ra có lợi hơn.

Hình 5: Hồ sơ lựa chọn khối cho phương pháp SubTuning Tham lam, hiển thị thứ tự lựa chọn khối cho mỗi loại hỏng dữ liệu.

các thuật toán hiệu suất cao trong chế độ dữ liệu thấp: Head2Toe [13] và LoRA [22]. Head2Toe là một phương pháp để tạo cầu nối khoảng cách giữa thăm dò tuyến tính và tinh chỉnh, hoạt động bằng cách huấn luyện một lớp tuyến tính trên các đặc trưng được chọn từ bản đồ kích hoạt trên toàn mạng. LoRA là một phương pháp huấn luyện một nhánh "residual" (chủ yếu bên trong Transformer) sử dụng phân tích cấp thấp của lớp.

Bảng 1: Hiệu suất của ResNet-50 và ViT-b/16 được huấn luyện trước trên ImageNet và được tinh chỉnh trên các tập dữ liệu từ VTAB-1k. FT ký hiệu tinh chỉnh trong khi LP là thăm dò tuyến tính. Độ lệch chuẩn được báo cáo trong Bảng 5 ở phụ lục.

ResNet50 ViT-b/16
CIFAR-100 Flowers102 Caltech101 DMLAB CIFAR-100 Flowers102 Caltech101 DMLab
Ours 54.6 90.5 86.5 51.2 68.0 97.7 86.5 36.4
H2T² [13] 47.1 85.6 88.8 43.9 58.2 85.9 87.3 41.6
FT 33.7 87.3 78.7 48.2 47.8 91.2 80.7 34.3
LP 35.4 64.2 67.1 36.3 29.9 84.7 72.7 31.0
LoRA [22] - - - - 40.4 88.3 79.2 36.4

VTAB-1k. Đầu tiên, chúng tôi đánh giá hiệu suất của SubTuning trên điểm chuẩn VTAB-1k, tập trung vào các tập dữ liệu CIFAR-100, Flowers102, Caltech101 và DMLab sử dụng phân chia 1k ví dụ được chỉ định trong giao thức. Chúng tôi sử dụng phương pháp SubTuning Tham lam được mô tả trong Phần 2 để chọn tập con của các lớp để tinh chỉnh. Đối với lựa chọn lớp, chúng tôi chia tập dữ liệu huấn luyện thành năm phần và thực hiện xác thực chéo năm fold. Chúng tôi sử dụng ResNet-50 PyTorch chính thức được huấn luyện trước trên ImageNet và ViT-b/16 được huấn luyện trước trên ImageNet-22k từ kho lưu trữ chính thức của [56]. Kết quả được trình bày trong Bảng 1. Các phát hiện của chúng tôi chỉ ra rằng SubTuning thường xuyên vượt trội hơn các phương pháp cạnh tranh và vẫn cạnh tranh trong các trường hợp khác.

Ảnh hưởng của Kích thước Tập dữ liệu. Việc lựa chọn lớp tối ưu cho một nhiệm vụ nhất định phụ thuộc vào nhiều yếu tố khác nhau, chẳng hạn như kiến trúc, bản thân nhiệm vụ và kích thước tập dữ liệu. Chúng tôi tiếp tục điều tra tác động của kích thước tập dữ liệu lên hiệu suất của SubTuning với các lớp khác nhau bằng cách so sánh việc tinh chỉnh một khối residual đơn với thăm dò tuyến tính và tinh chỉnh trên CIFAR-10 với kích thước tập dữ liệu khác nhau. Chúng tôi trình bày kết quả trong Hình 4. Các phát hiện của chúng tôi chứng minh rằng các lớp gần đầu ra thể hiện hiệu suất vượt trội khi huấn luyện trên các tập dữ liệu nhỏ hơn.

Ngoài những thí nghiệm này, chúng tôi cũng khám phá việc sử dụng SubTuning trong thiết lập học tích cực dựa trên nhóm (AL), trong đó có sẵn một nhóm lớn dữ liệu không nhãn, và các ví dụ bổ sung có thể được gắn nhãn để cải thiện độ chính xác của mô hình. Kết quả của chúng tôi cho thấy rằng SubTuning vượt trội hơn cả thăm dò tuyến tính và tinh chỉnh đầy đủ trong thiết lập này. Chúng tôi trình bày kết quả của chúng tôi trong thiết lập AL trong Hình 11 ở phụ lục.

3.2 Dịch chuyển Phân phối và Hỏng Dữ liệu

Mạng nơ-ron sâu được biết là nhạy cảm với các dịch chuyển phân phối nhỏ giữa miền nguồn và đích, điều này dẫn đến giảm hiệu suất của chúng [55,20,30]. Một giải pháp hiệu quả về chi phí cho vấn đề này là thu thập một tập dữ liệu nhỏ có nhãn từ miền đích và tinh chỉnh một mô hình đã được huấn luyện trước trên tập dữ liệu này. Trong phần này, chúng tôi tập trung vào một tình huống trong đó có sẵn một tập dữ liệu có nhãn lớn từ miền nguồn, nhưng chỉ có dữ liệu có nhãn hạn chế từ miền đích. Chúng tôi chứng minh rằng SubTuning Tham lam mang lại kết quả tốt hơn so với tinh chỉnh tất cả các lớp, và cũng so với tinh chỉnh Phẫu thuật [34], trong đó một tập con lớn của các khối liên tiếp được huấn luyện.

Bảng 2: Dịch chuyển phân phối CIFAR-10 sang CIFAR-10-C.

Dịch chuyển phân phối SubTuning Finetuning Surgical L1 Surgical L2 Surgical L3 Linear
zoom blur 90.0±0.1 87.8±0.4 89.2±0.1 89.1±0.2 85.5±0.3 68.7±0.04
speckle noise 81.5±0.2 77.8±0.6 78.4±0.1 74.8±0.1 71.1±0.1 51.5±0.01
spatter 89.2±0.2 86.8±0.3 89.4±0.1 87.4±0.2 85.3±0.0 80.4±0.07
snow 86.0±0.2 84.1±0.2 84.8±0.2 84.3±0.1 82.2±0.2 78.7±0.07
shot noise 82.0±0.3 77.6±0.4 77.0±0.9 74.2±0.1 69.9±0.1 46.4±0.01
saturate 92.0±0.1 89.5±0.3 91.7±0.0 91.2±0.0 90.4±0.0 89.8±0.04
pixelate 86.1±0.0 82.8±0.5 85.8±0.1 83.6±0.2 78.5±0.2 54.8±0.02
motion blur 87.3±0.1 85.5±0.3 86.7±0.1 86.9±0.1 83.4±0.1 72.9±0.03
jpeg compression 80.8±0.2 76.5±0.7 80.1±0.5 76.8±0.1 74.9±0.1 72.0±0.04
impulse noise 75.4±0.5 70.8±0.7 69.6±0.3 63.8±0.1 56.7±0.1 35.2±0.01
glass blur 74.3±0.3 72.2±0.2 69.9±0.4 71.5±0.1 67.8±0.1 55.2±0.06
gaussian noise 80.0±0.2 75.1±1.2 72.7±0.1 71.0±0.1 66.6±0.2 41.1±0.01
gaussian blur 89.5±0.2 86.4±0.4 88.1±0.0 87.3±0.1 80.0±0.0 41.7±0.05
frost 84.2±0.2 83.1±0.4 84.2±0.3 83.2±0.1 80.4±0.2 68.5±0.03
Average 84.2±0.2 81.1±0.5 82.0±0.2 80.4±0.1 76.6±0.1 61.2±0.04

Trong suốt phần này chúng tôi tuân theo thiết lập được đề xuất bởi [34], phân tích dịch chuyển phân phối từ CIFAR-10 sang CIFAR-10-C [20] cho ResNet-26. Nhiệm vụ là phân loại hình ảnh trong đó phân phối đích được cấu thành từ các hình ảnh của phân phối gốc với hỏng đầu vào được thêm vào từ một tập được xác định trước gồm 14 loại hỏng. Để biết chi tiết thí nghiệm, tham khảo Phụ lục A.

Kết quả. Trong Bảng 2, chúng tôi hiển thị hiệu suất của thăm dò tuyến tính, tinh chỉnh, tinh chỉnh Phẫu thuật³ cũng như SubTuning. Chúng ta thấy rằng phương pháp của chúng tôi thường vượt trội và luôn cạnh tranh với các phương pháp khác. Trung bình, SubTuning hoạt động tốt hơn 3% so với tinh chỉnh đầy đủ và 2.2% tốt hơn tinh chỉnh Phẫu thuật được tái tạo trong thiết lập của chúng tôi⁴.

Tiếp theo, chúng tôi phân tích số lượng khối residual cần thiết cho SubTuning như được hiển thị trong Hình 1(giữa). Chúng tôi báo cáo độ chính xác trung bình trên 3 dịch chuyển phân phối (glass blur, zoom blur và jpeg compression) và hiệu suất trung bình cho 14 loại hỏng trong CIFAR-10-C. Ngay cả với chỉ 2 khối residual được lựa chọn thích hợp, SubTuning cho thấy hiệu suất tốt hơn tinh chỉnh đầy đủ.

Cuối cùng, chúng tôi phân tích khối nào được sử dụng bởi phương pháp Greedy-SubTuning ở trên. Hình 5 minh họa các khối được chọn và thứ tự tương ứng của chúng cho mỗi tập dữ liệu. Các phát hiện của chúng tôi mâu thuẫn với niềm tin phổ biến rằng chỉ có vài khối cuối cùng cần điều chỉnh. Thực tế, SubTuning sử dụng nhiều khối từ đầu và giữa mạng. Hơn nữa, kết quả của chúng tôi thách thức tuyên bố được đưa ra trong [34] rằng việc điều chỉnh chỉ các lớp đầu tiên của mạng là đủ cho các dịch chuyển mức đầu vào trong CIFAR-10-C. Thú vị thay, chúng tôi thấy rằng khối cuối cùng hoặc khối áp cuối là lớp đầu tiên được chọn cho tất cả các loại hỏng, dẫn đến sự gia tăng hiệu suất lớn nhất.

4 Học Đa Nhiệm vụ Hiệu quả với SubTuning

Cho đến nay, chúng tôi đã chứng minh tác động khác nhau của các lớp khác nhau lên hiệu suất tổng thể của một mô hình được tinh chỉnh, cho thấy rằng có thể đạt được độ chính xác cao mà không cần huấn luyện tất cả các tham số của mạng, miễn là các lớp phù hợp được chọn để huấn luyện. Trong phần này, chúng tôi tập trung vào việc sử dụng SubTuning cho Học Đa Nhiệm vụ (MTL).

Một nhược điểm chính của tinh chỉnh tiêu chuẩn trong bối cảnh học đa nhiệm vụ [5,57] là một khi mô hình được tinh chỉnh trên một nhiệm vụ mới, các trọng số của nó có thể không còn phù hợp cho nhiệm vụ nguồn ban đầu (một vấn đề được gọi là quên thảm khốc [45]). Hãy xem xét ví dụ thiết lập đa nhiệm vụ sau đây, phục vụ như động cơ chính cho phần này. Giả sử chúng ta có một mạng backbone lớn được huấn luyện trên một nhiệm vụ nguồn nào đó, và đã được triển khai và chạy như một phần của hệ thống học máy của chúng ta. Khi được đưa ra một nhiệm vụ mới, chúng ta tinh chỉnh backbone đã triển khai của chúng ta trên nhiệm vụ này, và muốn chạy mạng mới được tinh chỉnh song song với mạng cũ. Điều này đặt ra một vấn đề, vì bây giờ chúng ta phải chạy cùng một kiến trúc hai lần, mỗi lần với một tập trọng số khác nhau. Làm như vậy làm tăng gấp đôi chi phí cả về mặt tính toán (số phép nhân-cộng cần thiết để tính toán cả hai nhiệm vụ), và về mặt bộ nhớ và IO (số bit cần thiết để tải trọng số của cả hai mô hình từ bộ nhớ). Một lựa chọn thay thế sẽ là thực hiện huấn luyện đa nhiệm vụ cho cả nhiệm vụ cũ và mới, nhưng điều này thường dẫn đến suy giảm hiệu suất trên cả hai nhiệm vụ, với các vấn đề như cân bằng dữ liệu, chia sẻ tham số và trọng số mất mát xuất hiện [7, 62, 60].

Chúng tôi cho thấy rằng sử dụng SubTuning, chúng ta có thể triển khai hiệu quả các nhiệm vụ mới tại thời điểm suy luận (xem Hình 6), với chi phí tối thiểu về tính toán, bộ nhớ và IO, trong khi duy trì độ chính xác cao trên các nhiệm vụ hạ nguồn. Thay vì huấn luyện tất cả các nhiệm vụ đồng thời, điều này có thể dẫn đến sự can thiệp nhiệm vụ và tối ưu hóa phức tạp, chúng tôi đề xuất bắt đầu với một mạng được huấn luyện trước trên một nhiệm vụ chính nào đó, và thêm các nhiệm vụ mới với SubTuning trên đó (Hình 6). Khung này cung cấp đảm bảo rằng hiệu suất của các nhiệm vụ đã học trước đó sẽ được bảo tồn trong khi thêm các nhiệm vụ mới.

4.1 Suy luận Hiệu quả về Tính toán

Bây giờ chúng tôi sẽ chứng minh cách SubTuning cải thiện hiệu quả tính toán của mạng tại thời điểm suy luận, đây là động cơ chính cho phần này.

³Tinh chỉnh phẫu thuật tập trung vào huấn luyện toàn bộ các Lớp (bao gồm 4 khối cho ResNet26).
⁴Chúng tôi lưu ý rằng [34] báo cáo hiệu suất cao hơn một chút so với việc tái tạo của chúng tôi, nhưng vẫn đạt được độ chính xác thấp hơn 1.4% so với SubTuning.

--- TRANG 7 ---

--- TRANG 8 ---
Hình 6: SubTuning cho MTL. Mỗi nhiệm vụ mới sử dụng một tập con liên tiếp của các lớp của một mạng và chia sẻ những lớp khác. Ở cuối sự phân tách, các đầu ra của các nhiệm vụ khác nhau được nối và song song hóa dọc theo trục batch để có hiệu quả tính toán.

5% 6% 7% 8% 9% 10% 11%0.9400.9450.9500.9550.960
B14
B15ResNet50, Độ chính xác SubTuning so với Suy luận (1 Khối)
11% 12% 13% 14% 15% 16% 17% 18% 19%
B12-B14ResNet50, Độ chính xác SubTuning so với Suy luận (3 Khối)        Độ chính xác
Thời gian Suy luận Thêm vào %, GPU A100
Lớp1 Lớp1 + Lớp2 Lớp2 Lớp2 + Lớp3 Lớp3 Lớp3 + Lớp4 Lớp4
Hình 7: Độ chính xác trên CIFAR-10 so với độ trễ A100 với kích thước batch là 1 và độ phân giải đầu vào là 224.

Hãy xem xét thiết lập học đa nhiệm vụ sau đây. Chúng ta đã huấn luyện một mạng f_θ trên một nhiệm vụ nào đó. Mạng nhận một đầu vào x và trả về một đầu ra f_θ(x). Bây giờ chúng ta muốn huấn luyện một mạng mới trên một nhiệm vụ khác bằng cách tinh chỉnh các trọng số θ, dẫn đến một tập trọng số mới θ̃. Bây giờ, tại thời điểm suy luận, chúng ta nhận một đầu vào x và muốn tính toán cả f_θ(x) và f_θ̃(x) với ngân sách tính toán tối thiểu.

Vì chúng ta không thể mong đợi tổng tính toán sẽ thấp hơn chỉ chạy f_θ(x)⁵, chúng ta chỉ đo chi phí bổ sung của việc tính toán f_θ̃(x), cho rằng f_θ(x) đã được tính toán.

Vì thời gian suy luận phụ thuộc rất nhiều vào các tham số khác nhau như phần cứng được sử dụng để suy luận (ví dụ: CPU, GPU, FPGA), tải song song của phần cứng, biên dịch mạng (tức là kernel fusion) và kích thước batch, chúng tôi sẽ tiến hành một phân tích thô về yêu cầu tính toán (xem thảo luận chi tiết trong [38]). Hai yếu tố chính góp phần vào thời gian tính toán là: 1) Chi phí tính toán, hoặc số phép nhân-cộng (FLOPs) cần thiết để tính toán mỗi lớp và 2) IO, đề cập đến số bit cần thiết để đọc từ bộ nhớ để tải trọng số của mỗi lớp.

Nếu chúng ta thực hiện tinh chỉnh đầy đủ của tất cả các lớp, để tính toán f_θ̃(x) chúng ta cần tăng gấp đôi cả chi phí tính toán và IO, vì bây giờ chúng ta thực sự đang chạy hai mạng riêng biệt, f_θ và f_θ̃, với hai tập trọng số riêng biệt. Lưu ý rằng điều này không nhất thiết có nghĩa là thời gian tính toán được tăng gấp đôi, vì hầu hết phần cứng được sử dụng để suy luận thực hiện song song hóa đáng kể, và nếu phần cứng không được sử dụng đầy đủ khi chạy f_θ(x), chi phí bổ sung của việc chạy f_θ̃(x) song song có thể nhỏ hơn. Tuy nhiên, về mặt tính toán bổ sung, tinh chỉnh đầy đủ là điều kém tối ưu nhất để làm.

Hãy xem xét bây giờ chi phí tính toán của SubTuning. Để đơn giản chúng tôi phân tích trường hợp các lớp được chọn là liên tiếp, nhưng phân tích tương tự có thể được áp dụng cho trường hợp không liên tiếp. Ký hiệu N là số lượng lớp trong mạng, và giả sử rằng các tham số θ̃ khác với các tham số gốc θ chỉ trong các lớp ℓ_start đến ℓ_end (trong đó 1≤ℓ_start≤ℓ_end≤N). Hãy tách thành hai trường hợp: 1) ℓ_end là lớp cuối cùng của mạng và 2) ℓ_end là một lớp trung gian nào đó.

Trường hợp ℓ_end là lớp cuối cùng là đơn giản nhất: chúng ta chia sẻ toàn bộ tính toán của f_θ(x) và f_θ̃(x) cho đến lớp ℓ_start (vì vậy không có chi phí bổ sung cho các lớp dưới ℓ_start), và sau đó chúng ta "phân nhánh" mạng và chạy các lớp của f_θ và f_θ̃ song song. Trong trường hợp này, cả tính toán và IO đều tăng gấp đôi chỉ cho các lớp giữa ℓ_start và ℓ_end.

Trong trường hợp thứ hai, khi ℓ_end là một lớp trung gian nào đó, các cân nhắc tính toán phức tạp hơn. Như trong trường hợp trước, chúng ta chia sẻ toàn bộ tính toán trước lớp ℓ_start, không có tính toán bổ sung. Sau đó chúng ta "phân nhánh" mạng, trả chi phí tính toán và IO gấp đôi cho các lớp giữa ℓ_start và ℓ_end. Tuy nhiên, đối với các lớp sau ℓ_end, chúng ta có thể "hợp nhất" lại các đầu ra của hai nhánh song song (tức là nối chúng trong "trục batch"), và sử dụng cùng trọng số mạng cho cả hai đầu ra. Điều này có nghĩa là đối với các lớp sau ℓ_end chúng ta tăng gấp đôi tính toán (tức là trong FLOPs), nhưng IO vẫn giữ nguyên (bằng cách tái sử dụng trọng số cho cả hai đầu ra). Cơ chế này được minh họa trong Hình 6.

Chính thức hơn, cho c_i là chi phí tính toán của lớp thứ i, và cho s_i là IO cần thiết cho lớp thứ i. Để có một ước tính thô về cách IO và tính toán ảnh hưởng đến thời gian chạy backbone, hãy xem xét một thiết lập đơn giản trong đó tính toán và IO được song song hóa. Do đó, trong khi bộ xử lý tính toán lớp i, trọng số của lớp i+1 được tải vào bộ nhớ. Tổng thời gian suy luận của mô hình là:

Tính toán = max(2s_ℓ_start, c_ℓ_start-1) + Σ(i=ℓ_start đến ℓ_end) 2max(c_i, s_i+1) + Σ(i=ℓ_end+1 đến N-1) max(2c_i, s_i+1) + 2c_N

Do đó, cả các lớp sâu hơn và nông hơn đều có thể tối ưu cho SubTuning, tùy thuộc vào môi trường triển khai chính xác, khối lượng công việc và liệu chúng ta bị ràng buộc IO hay tính toán. Chúng tôi tiến hành điều tra thực nghiệm về sự đánh đổi hiệu suất so với độ trễ của SubTuning cho MTL. Chúng tôi tiến hành một thí nghiệm sử dụng ResNet-50 trên GPU NVIDIA A100-SXM-80GB với kích thước batch là 1 và độ phân giải 224. Chúng tôi tinh chỉnh 1 và 3 res-block liên tiếp và vẽ đồ thị độ chính xác so với chi phí suy luận thêm vào, như được thấy trong Hình 7. Bằng cách này chúng ta có thể đạt được những cải thiện hiệu suất đáng kể, với chi phí tính toán tối thiểu. Tuy nhiên, điều quan trọng là phải đề cập rằng việc lựa chọn chính xác lớp nào mang lại sự đánh đổi độ chính xác-độ trễ tối ưu có thể phụ thuộc rất nhiều vào môi trường triển khai, vì ước tính thời gian chạy có thể thay đổi tùy thuộc vào các yếu tố như phần cứng, tải công việc và ngăn xếp phần mềm. Để điều tra thêm về độ chính xác-độ trễ trong thiết lập MTL, tham khảo Phụ lục B.

5 Thảo luận

Mạng nơ-ron hiện đang trở thành một phần không thể thiếu của phát triển phần mềm. Trong phát triển thông thường, các nhóm có thể làm việc độc lập và giải quyết xung đột bằng hệ thống kiểm soát phiên bản. Nhưng với mạng nơ-ron, việc duy trì tính độc lập trở nên khó khăn. Các nhóm xây dựng một mạng duy nhất cho các nhiệm vụ khác nhau phải phối hợp chu kỳ huấn luyện, và những thay đổi trong một nhiệm vụ có thể ảnh hưởng đến những nhiệm vụ khác. Chúng tôi tin rằng SubTuning cung cấp một giải pháp khả thi cho vấn đề này. Nó cho phép các nhà phát triển "fork" các mạng đã triển khai và phát triển các nhiệm vụ mới mà không can thiệp vào các nhóm khác. Phương pháp này thúc đẩy phát triển độc lập, chia sẻ kiến thức và triển khai hiệu quả các nhiệm vụ mới. Như chúng tôi đã cho thấy, nó cũng dẫn đến hiệu suất được cải thiện so với các phương pháp học chuyển giao cạnh tranh trong các thiết lập khác nhau. Kết luận, chúng tôi hy vọng rằng SubTuning, cùng với các phương pháp tinh chỉnh hiệu quả khác, có thể đóng một vai trò trong sự phát triển liên tục của phát triển phần mềm trong kỷ nguyên mạng nơ-ron.

⁵Tối ưu hóa ngân sách tính toán của một mạng duy nhất nằm ngoài phạm vi của bài báo này.

--- TRANG 9 ---

--- TRANG 10 ---
Tài liệu Tham khảo
[1]Maria-Florina Balcan, Andrei Broder, và Tong Zhang. Margin based active learning. Trong International Conference on Computational Learning Theory, trang 35–50. Springer, 2007.
[2]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020.
[3]Colin Campbell, Nello Cristianini, Alex Smola, et al. Query learning with large margin classifiers. Trong ICML, tập 20, trang 0, 2000.
[4]Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, và Armand Joulin. Emerging properties in self-supervised vision transformers, 2021.
[5] Rich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.
[6]Hao Chen, Ran Tao, Han Zhang, Yidong Wang, Wei Ye, Jindong Wang, Guosheng Hu, và Marios Savvides. Conv-adapter: Exploring parameter efficient transfer learning for convnets, 2022.
[7]Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, và Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. Trong International conference on machine learning, trang 794–803. PMLR, 2018.
[8]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.
[9]Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, và Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670, 2020.
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong 2009 IEEE Conference on Computer Vision and Pattern Recognition, trang 248–255, 2009.
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs/2010.11929, 2020.
[13] Utku Evci, Vincent Dumoulin, Hugo Larochelle, và Michael C. Mozer. Head2toe: Utilizing intermediate representations for better transfer learning. CoRR, abs/2201.03529, 2022.
[14] Gongfan Fang. Torch-Pruning, 7 2022.
[15] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, và Nigel Collier. On the effectiveness of parameter-efficient fine-tuning. arXiv preprint arXiv:2211.15583, 2022.
[16] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, và Graham Neubig. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.

--- TRANG 11 ---
[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, và Ross B. Girshick. Masked autoencoders are scalable vision learners. CoRR, abs/2111.06377, 2021.
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.
[19] Xuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei Yang, và Xin Eric Wang. Parameter-efficient model adaptation for vision transformers, 2022.
[20] Dan Hendrycks và Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.
[21] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. Parameter-efficient transfer learning for NLP. CoRR, abs/1902.00751, 2019.
[22] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, và Weizhu Chen. Lora: Low-rank adaptation of large language models. CoRR, abs/2106.09685, 2021.
[23] Arthur Jacot, Franck Gabriel, và Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.
[24] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, và Ser-Nam Lim. Visual prompt tuning, 2022.
[25] Ajay J Joshi, Fatih Porikli, và Nikolaos Papanikolopoulos. Multi-class active learning for image classification. Trong 2009 ieee conference on computer vision and pattern recognition, trang 2372–2379. IEEE, 2009.
[26] Ajay J. Joshi, Fatih Porikli, và Nikolaos Papanikolopoulos. Multi-class active learning for image classification. Trong 2009 IEEE Conference on Computer Vision and Pattern Recognition, trang 2372–2379, 2009.
[27] Minsoo Kang, Jaeyoo Park, và Bohyung Han. Class-incremental learning by knowledge distillation with adaptive feature consolidation. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 16071–16080, 2022.
[28] Angelos Katharopoulos và François Fleuret. Not all samples are created equal: Deep learning with importance sampling. Trong International conference on machine learning, trang 2525–2534. PMLR, 2018.
[29] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.
[30] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. Trong International Conference on Machine Learning, trang 5637–5664. PMLR, 2021.
[31] Jonathan Krause, Michael Stark, Jia Deng, và Li Fei-Fei. 3d object representations for fine-grained categorization. Trong 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia, 2013.
[32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009.
[33] Yoonho Lee, Annie S. Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, và Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts, 2022.
[34] Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, và Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts. arXiv preprint arXiv:2210.11466, 2022.
[35] Brian Lester, Rami Al-Rfou, và Noah Constant. The power of scale for parameter-efficient prompt tuning. CoRR, abs/2104.08691, 2021.

--- TRANG 12 ---
[36] David D. Lewis và William A. Gale. A sequential algorithm for training text classifiers. Trong Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '94, trang 3–12, Berlin, Heidelberg, 1994. Springer-Verlag.
[37] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for efficient convnets, 2016.
[38] Sheng Li, Mingxing Tan, Ruoming Pang, Andrew Li, Liqun Cheng, Quoc V Le, và Norman P Jouppi. Searching for fast model families on datacenter accelerators. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 8085–8095, 2021.
[39] Xiang Lisa Li và Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. CoRR, abs/2101.00190, 2021.
[40] Baijiong Lin, Feiyang YE, và Yu Zhang. A closer look at loss weighting in multi-task learning, 2022.
[41] Shengchao Liu, Yingyu Liang, và Anthony Gitter. Loss-balanced task weighting to reduce negative transfer in multi-task learning. Trong Proceedings of the AAAI conference on artificial intelligence, tập 33, trang 9977–9978, 2019.
[42] Ilya Loshchilov và Frank Hutter. Decoupled weight decay regularization, 2017.
[43] Arun Mallya và Svetlana Lazebnik. Piggyback: Adding multiple tasks to a single, fixed network by learning to mask. CoRR, abs/1801.06519, 2018.
[44] Sébastien Marcel và Yann Rodriguez. Torchvision the machine-vision package of torch. Trong Proceedings of the 18th ACM international conference on Multimedia, trang 1485–1488, 2010.
[45] Michael McCloskey và Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Trong Psychology of learning and motivation, tập 24, trang 109–165. Elsevier, 1989.
[46] Paul Michel, Sebastian Ruder, và Dani Yogatama. Balancing average and worst-case accuracy in multitask learning, 2022.
[47] Maria-Elena Nilsback và Andrew Zisserman. Automated flower classification over a large number of classes. Trong Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.
[48] Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, và Maria A. Zuluaga. Optimization strategies in multi-task learning: Averaged or separated losses? CoRR, abs/2109.11678, 2021.
[49] Senthil Purushwalkam, Pedro Morgado, và Abhinav Gupta. The challenges of continuous self-supervised learning. arXiv preprint arXiv:2203.12710, 2022.
[50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, và Ilya Sutskever. Learning transferable visual models from natural language supervision. CoRR, abs/2103.00020, 2021.
[51] Evani Radiya-Dixit và Xin Wang. How fine can fine-tuning be? learning efficient language models. CoRR, abs/2004.14129, 2020.
[52] Sylvestre-Alvise Rebuffi, Hakan Bilen, và Andrea Vedaldi. Learning multiple visual domains with residual adapters. CoRR, abs/1705.08045, 2017.
[53] Sylvestre-Alvise Rebuffi, Hakan Bilen, và Andrea Vedaldi. Efficient parametrization of multi-domain deep neural networks. CoRR, abs/1803.10082, 2018.
[54] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, và Christoph H Lampert. icarl: Incremental classifier and representation learning. Trong Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, trang 2001–2010, 2017.
[55] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, và Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? Trong Kamalika Chaudhuri và Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, tập 97 của Proceedings of Machine Learning Research, trang 5389–5400. PMLR, 09–15 Jun 2019.
[56] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, và Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021.

--- TRANG 13 ---
[57] Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017.
[58] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, và Raia Hadsell. Progressive neural networks. CoRR, abs/1606.04671, 2016.
[59] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, và Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.
[60] Ozan Sener và Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances in neural information processing systems, 31, 2018.
[61] Shai Shalev-Shwartz và Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
[62] Ximeng Sun, Rameswar Panda, Rogerio Feris, và Kate Saenko. Adashare: Learning what to share for efficient deep multi-task learning. Advances in Neural Information Processing Systems, 33:8728–8740, 2020.
[63] Yi-Lin Sung, Jaemin Cho, và Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning, 2022.
[64] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, và Furu Wei. Image as a foreign language: Beit pretraining for all vision and vision-language tasks, 2022.
[65] Joseph Worsham và Jugal Kalita. Multi-task learning for natural language processing in the 2020s: where are we going? Pattern Recognition Letters, 136:120–126, 2020.
[66] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, và Yonghui Wu. Coca: Contrastive captioners are image-text foundation models, 2022.
[67] Elad Ben Zaken, Shauli Ravfogel, và Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. CoRR, abs/2106.10199, 2021.
[68] Jeffrey O. Zhang, Alexander Sax, Amir Zamir, Leonidas J. Guibas, và Jitendra Malik. Side-tuning: Network adaptation via additive side networks. CoRR, abs/1912.13503, 2019.
[69] Mengjie Zhao, Tao Lin, Martin Jaggi, và Hinrich Schütze. Masking as an efficient alternative to finetuning for pretrained language models. CoRR, abs/2004.12406, 2020.

--- TRANG 14 ---
A Thiết lập Thí nghiệm
Trừ khi được nêu khác, đối với các thí nghiệm trong suốt bài báo chúng tôi đã sử dụng một thiết lập thí nghiệm cố định được trình bày trong Bảng 3. Chúng tôi tập trung vào huấn luyện ngắn 10 epoch, sử dụng bộ tối ưu hóa AdamW [42] trên độ phân giải hình ảnh 224 với cắt và thay đổi kích thước ngẫu nhiên để chuyển đổi từ độ phân giải thấp hơn sang cao hơn. Chúng tôi cũng sử dụng lật ngang ngẫu nhiên và đối với Flowers102 chúng tôi sử dụng xoay ngẫu nhiên lên đến xác suất 30%. Chúng tôi có một vài ngoại lệ đối với thiết lập này. Một là trong Phần 3, do dữ liệu khan hiếm, chúng tôi huấn luyện trong 50 epoch. Ngoài ra, trong Tiểu mục 3.2, chúng tôi huấn luyện trong 15 epoch và sử dụng cùng điều chỉnh tỷ lệ học cho lựa chọn lớp như trong [34]. Chúng tôi báo cáo kết quả của chúng tôi trên CIFAR-10, CIFAR-100 [32], Flower102 [47] và Standford Cars [31], ngoài kết quả CIFAR-C được báo cáo trong Tiểu mục 3.2.

Bảng 3: Tham số Huấn luyện. Đối với ViT-B/16 chúng tôi sử dụng hai bộ tham số. Một cho các tập dữ liệu độ dài đầy đủ và bộ khác cho các tập dữ liệu nhỏ với 1k ví dụ huấn luyện (Bảng 1).

ResNet ViT-B/16 tập dữ liệu đầy đủ ViT-B/16 1k ví dụ
Tỷ lệ Học 0.001* 0.0001 0.001
Suy giảm Trọng số 0.01 0.00005 0.01
Kích thước Batch 256 256 256
Bộ tối ưu hóa AdamW AdamW AdamW
Bộ lập lịch Cosine Annealing Cosine Annealing Cosine Annealing
*Đối với thăm dò tuyến tính với ResNet, chúng tôi sử dụng tỷ lệ học 0.01.

Trọng số Đã được Huấn luyện trước Đối với ResNet-50, ResNet-18 [18] và ViT-B/16 [12] chúng tôi sử dụng trọng số đã được huấn luyện trước bằng cách sử dụng triển khai TorchVision mặc định [44] được huấn luyện trước trên ImageNet [10]. Đối với DINO ResNet-50 [4], chúng tôi đã sử dụng trọng số github chính thức của bài báo https://github.com/facebookresearch/dino.

Hồ sơ Tinh chỉnh. Để tạo hồ sơ tinh chỉnh chúng tôi chỉ huấn luyện tập con thích hợp của các khối residual (cho ResNets) và các lớp Self-Attention (Cho ViT) ngoài việc huấn luyện một đầu tuyến tính thích hợp. Ví dụ, đối với ResNet-18, có 8 khối residual (ResBlocks), 2 trong mỗi lớp hoặc độ phân giải không gian (xem triển khai đầy đủ ở đây: link.). Nói chung, mỗi khối như vậy bao gồm một vài lớp Convolution với một kết nối residual liên kết đầu vào của khối và đầu ra của các lớp Conv. Tương tự, ResNet-50 có 16 khối, [3, 4, 6, 3], cho các lớp [1, 2, 3, 4] tương ứng. Đối với ViT-B/16, có 12 lớp attention tự nhiên và chúng tôi huấn luyện một (hoặc vài) lớp tại một thời điểm.

SubTuning Tham lam. Chúng tôi đánh giá mỗi tập con của Khối bằng xác thực chéo 5-fold trong tất cả các thí nghiệm của chúng tôi mà chúng tôi sử dụng thuật toán tham lam. Trong Thuật toán 1 chúng tôi trình bày mã giả cho SubTuning Tham lam. Bảng 4 hiển thị các Khối được chọn bởi thuật toán tham lam cho các tập con CIFAR-10 có kích thước khác nhau, tạo ra kết quả được trình bày trong Hình 1 (phải).

Bảng 4: Các Khối Được Chọn của ResNet-50 cho Các Kích thước Tập Huấn luyện Khác nhau của CIFAR-10

Kích thước Tập con CIFAR-10 Các Khối ResNet-50 Được Chọn
100 6, 11, 13, 16
500 8, 13, 15, 16
1,000 7, 9, 11, 15
5,000 4, 8, 11, 13, 15
10,000 3, 5, 8, 9, 10, 14, 15, 16
50,000 2, 4, 5, 12, 13, 14, 15, 16

--- TRANG 15 ---
Thuật toán 1 Greedy-SubTuning
1:procedure GREEDY SUBSET SELECTION (model, all_layers, ε)
2: S← {} ,n← |all_layers |
3: Abest= 0
4: fori= 1tondo
5: Aiter←0,Lbest←null
6: forL∈(all_layers - S)do
7: S′←S∪ {L}
8: Anew←evaluate(model, S′)
9: ifAnew> A iterthen
10: Lbest←L,Aiter←Anew
11: end if
12: end for
13: ifAiter> A best+εthen
14: Abest←Aiter,S←S∪ {Lbest}# nếu không có lớp nào giúp đủ, chúng ta dừng
15: else
16: Break
17: end if
18: end for
19: return S
20:end procedure

Hỏng dữ liệu. Trong suốt Phần 3.2 chúng tôi tuân theo thiết lập được đề xuất bởi [34], phân tích dịch chuyển phân phối từ CIFAR-10 sang CIFAR-10-C [20] cho ResNet-26. Nhiệm vụ là phân loại hình ảnh trong đó phân phối đích được cấu thành từ các hình ảnh của phân phối gốc với hỏng đầu vào được thêm vào từ một tập được xác định trước gồm 14 loại hỏng.

Tương tự như [34], đối với mỗi loại hỏng chúng tôi sử dụng 1k hình ảnh làm tập huấn luyện và 9k làm tập kiểm tra. Đối với lựa chọn lớp chúng tôi thực hiện xác thực chéo 5 fold chỉ sử dụng 1k ví dụ của tập huấn luyện, và chỉ sau khi tập con lớp được chọn chúng tôi huấn luyện trên toàn bộ 1k dữ liệu huấn luyện, đánh giá trên tập kiểm tra. Chúng tôi sử dụng mô hình ResNet-26 với huấn luyện trước "Tiêu chuẩn" và mã tải dữ liệu từ Croce et al. [9]. Chúng tôi sử dụng mức độ nghiêm trọng hỏng cao nhất là 5. Chúng tôi điều chỉnh trên 5 tỷ lệ học 1e-3,5e-4,1e-4,5e-5,1e-5 và báo cáo trung bình của 5 lần chạy.

Đo Thời gian Suy luận. Chúng tôi đo thời gian suy luận trên một GPU NVIDIA A100-SXM-80GB duy nhất với kích thước batch là 1 và độ phân giải đầu vào 224. Chúng tôi khởi động GPU trong 300 vòng lặp và chạy thêm 300 vòng lặp để đo thời gian chạy. Vì đo thời gian suy luận về bản chất có nhiễu, chúng tôi đảm bảo số lượng tiến trình khác chạy trên GPU ở mức tối thiểu và báo cáo thời gian trung bình từ 10 trung vị của 300. Chúng tôi đính kèm hình ảnh cho thời gian tuyệt đối trong Hình 18.

A.1 Thiết lập Thí nghiệm cho Ablations

Pruning. Chúng tôi sử dụng thư viện Torch-Pruning [14] để áp dụng cả pruning cục bộ và toàn cục, sử dụng các yếu tố quan trọng L1 và L2. Chúng tôi tiến hành một lần lặp pruning duy nhất, thay đổi yếu tố thưa thớt kênh giữa 0.1 và 0.9 với mức tăng 0.1, và chọn giá trị độ chính xác cao nhất cho mỗi phạm vi 5% tổng tham số SubTuning.

Học Tích cực Trong các thí nghiệm của chúng tôi, chúng tôi chọn các ví dụ theo biên phân loại của chúng. Ở mỗi vòng lặp, sau khi SubTuning mô hình của chúng tôi trên tập dữ liệu có nhãn, chúng tôi tính toán biên phân loại cho bất kỳ ví dụ không nhãn nào (tương tự như phương pháp được đề xuất trong [1,25,36]). Tức là, cho một ví dụ x nào đó, cho P(y|x) là xác suất mà mô hình gán cho nhãn y khi được cho đầu vào x⁶. Ký hiệu y1 là nhãn có xác suất tối đa và y2 là nhãn có xác suất cao thứ hai, cụ thể y1= max_y P(y|x) và y2= max_{y≠y1} P(y|x). Chúng tôi định nghĩa biên phân loại của x là P(y1|x)−P(y2|x), điều này nắm bắt mức độ tự tin của mô hình trong dự đoán của nó (biên phân loại càng thấp, mô hình càng ít tự tin). Chúng tôi chọn các ví dụ có biên phân loại nhỏ nhất (các ví dụ có độ bất định cao) làm những ví dụ cần được gắn nhãn.

⁶Chúng tôi tập trung vào các bài toán phân loại, trong đó mô hình tự nhiên đưa ra một xác suất cho mỗi nhãn cho trước đầu vào. Đối với các thiết lập khác, các khái niệm biên khác có thể áp dụng.

Trong các thí nghiệm Học Tích cực của chúng tôi, chúng tôi bắt đầu với 100 ví dụ được chọn ngẫu nhiên từ tập dữ liệu CIFAR-10. Ở mỗi vòng lặp chúng tôi chọn và gắn nhãn các ví dụ bổ sung, huấn luyện với 500, 1000, 2500, 5000 và 10,000 ví dụ có nhãn được chọn lặp đi lặp lại theo biên của chúng. Tức là, sau khi huấn luyện trên 100 ví dụ, chúng tôi chọn 400 ví dụ tiếp theo là những ví dụ gần biên nhất, huấn luyện một mô hình mới trên toàn bộ 500 ví dụ, sử dụng mô hình mới để chọn 500 ví dụ tiếp theo, và cứ thế. Trong Hình 11 chúng tôi so sánh hiệu suất của mô hình khi được huấn luyện trên các ví dụ được chọn bởi quy tắc dựa trên biên của chúng tôi, so với huấn luyện trên các tập con của các ví dụ được chọn ngẫu nhiên. Chúng tôi cũng so sánh phương pháp của chúng tôi với tinh chỉnh đầy đủ có và không có lựa chọn ví dụ dựa trên biên.

B Thí nghiệm Bổ sung

B.1 Hồ sơ Tinh chỉnh Bổ sung

Trong tiểu mục này chúng tôi cung cấp thêm một số hồ sơ SubTuining. Chúng tôi xác nhận rằng huấn luyện dài hơn không ảnh hưởng đến kết quả ViT, báo cáo kết quả trong Hình 8. Trong Hình 9 chúng tôi cung cấp kết quả SubTuning cho 2 và 3 khối liên tiếp. Chúng tôi cho thấy rằng sử dụng nhiều khối hơn cải thiện độ chính xác phân loại, và làm cho việc lựa chọn các khối sau hiệu quả hơn.

Hình 8: Hồ sơ tinh chỉnh được huấn luyện với ViT-B/16 trên Cifar10 được huấn luyện trong 40 epoch.

1,2
2,3
3,4
4,5
5,6
6,7
7,8
8,9
9,10
10,11
11,12
12,13
13,14
14,15
15,160.9450.9500.9550.960
Hồ sơ Tinh chỉnh (2 Khối, ResNet-50, CIFAR-10)
1,2,3
2,3,4
3,4,5
4,5,6
5,6,7
6,7,8
7,8,9
8,9,10
9,10,11
10,11,12
11,12,13
12,13,14
13,14,15
14,15,16
Hồ sơ Tinh chỉnh (3 Khối, ResNet-50, CIFAR-10)                Độ chính xác
Lớp để Tinh chỉnh
Lớp1 Lớp1 + Lớp2 Lớp2 Lớp2 + Lớp3 Lớp3 Lớp3 + Lớp4 Lớp4
Hình 9: Hồ sơ tinh chỉnh của ResNet-50 được huấn luyện trước trên ImageNet trên CIFAR-10 với 2 khối (Trái.) và 3 khối (Phải.)

B.2 Hồ sơ tinh chỉnh theo cặp bổ sung.

Chúng tôi đã thực hiện SubTuning với tất cả các cặp khối residual trên các tập con 1k ví dụ CIFAR-100, Flowers102, Caltech101 và DMLab từ tập dữ liệu VTAB-1k. Ngoài ra, chúng tôi cũng huấn luyện trên toàn bộ tập dữ liệu CIFAR-100. Chúng tôi trình bày kết quả của chúng tôi trong Hình10.

Khi kiểm tra kết quả cho các tập dữ liệu CIFAR-100 và DMLab, rõ ràng là việc sử dụng các Khối sâu hơn mang lại hiệu suất vượt trội khi kích thước tập dữ liệu bị hạn chế. Tuy nhiên, đối với tập dữ liệu DMLab, việc sử dụng các Khối SubTuning ở giữa mạng dường như hiệu quả hơn, mặc dù kích thước tập dữ liệu nhỏ. Sự không nhất quán rõ ràng này có thể được quy cho các đặc tính độc đáo của tập dữ liệu, bắt nguồn từ dữ liệu mô phỏng, và giai đoạn huấn luyện trước ban đầu được tiến hành trên dữ liệu thế giới thực. Những kết quả này nhấn mạnh tầm quan trọng của việc xem xét các tính chất cụ thể của tập dữ liệu và quá trình huấn luyện trước khi thiết kế và tối ưu hóa việc lựa chọn lớp.

--- TRANG 16 ---

--- TRANG 17 ---
B1B2B3B4B5B6B7B8B9B10B11B12B13B14B15B16B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B16SubTuning Hai Khối - CIFAR-100
79.580.080.581.081.582.082.583.083.5
B1B2B3B4B5B6B7B8B9B10B11B12B13B14B15B16B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B16SubTuning Hai Khối - Flowers102 (1k)
7075808590
B1B2B3B4B5B6B7B8B9B10B11B12B13B14B15B16B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B16SubTuning Hai Khối - DMLab (1k)
404244464850
B1B2B3B4B5B6B7B8B9B10B11B12B13B14B15B16B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B16SubTuning Hai Khối - CIFAR-100 (1k)
253035404550
B1B2B3B4B5B6B7B8B9B10B11B12B13B14B15B16B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B16SubTuning Hai Khối - Caltech101 (1k)
6570758085Hình 10: SubTuning Hai Khối của ResNet-50 trên CIFAR-100, và các phiên bản VTAB-1k của các tập dữ liệu CIFAR-100, Flowers102, Caltech101 và DMLab, được ký hiệu với 1k. Chúng tôi chạy một thí nghiệm duy nhất cho bất kỳ cặp Khối nào trong 20 epoch.

B.3 Chi tiết Bổ sung cho Phần 3

Trong bảng 5 chúng tôi báo cáo các độ lệch chuẩn tương ứng cho bảng 1 trong nội dung chính.

Bảng 5: Độ Lệch Chuẩn của ResNet-50 và ViT-b/16 được huấn luyện trước trên ImageNet và được tinh chỉnh trên các tập dữ liệu từ VTAB-1k. FT ký hiệu tinh chỉnh trong khi LP là thăm dò tuyến tính.

ResNet50 ViT-b/16
CIFAR-100 Flowers102 Caltech101 DMLAB CIFAR-100 Flowers102 Caltech101 DMLab
Ours 0.0068 0.0056 0.0071 0.0064 0.029 0.0016 0.0076 0.0132
H2T⁷[13] 0.14 0.08 0.25 0.13 0.29 0.5 0.16 0.14
FT 0.0085 0.0124 0.0206 0.01 0.0681 0.0226 0.0131 0.0132
LP 0.0051 0.0113 0.009 0.0054 0.0171 0.0079 0.0053 0.0028
LoRA [22] - - - - 0.0348 0.0159 0.0147 0.0132

B.4 Mở rộng và Ablations

Trong phần này, chúng tôi báo cáo kết quả bổ sung cho SubTuning, đã được bỏ qua hoặc chỉ được thảo luận một phần trong nội dung chính của bài báo. Cụ thể, chúng tôi nghiên cứu sự tương tác của SubTuning và Học Tích cực (Tiểu mục B.4.1), cách việc tái sử dụng các đặc trưng đông băng ảnh hưởng đến hiệu suất (xem Tiểu mục B.4.2), sự tương tác giữa SubTuning và pruning trọng số (xem Tiểu mục B.4.3. và cuối cùng là liệu việc khởi tạo lại một phần trọng số có thể khôi phục hiệu suất tinh chỉnh hay không (xem Tiểu mục B.4.4).

⁷Kết quả từ bài báo gốc. Các mô hình đã được huấn luyện trước có thể khác nhau do sự khác biệt trong bộ phần mềm (TensorFlow).

--- TRANG 18 ---
1005001k2.5k5k10k
Số lượng Ví dụ0.40.50.60.70.80.90.95Độ chính xác
CIFAR-10: Độ chính xác so với Kích thước Tập dữ liệu
SubTuning Khối7, Học Tích cực
SubTuning Khối7, Ngẫu nhiên
Tinh chỉnh, Học Tích cực
Tinh chỉnh, Ngẫu nhiênHình 11: ResNet-50 được huấn luyện trước trên ImageNet với SubTuning trên CIFAR-10 sử dụng Học Tích cực. Chúng tôi đã sử dụng thang logarit cho trục y để hiển thị sự khác biệt giữa nhiều thang độ chính xác.

B.4.1 Học Tích cực với SubTuning

Chúng ta đã thấy rằng SubTuning là một phương pháp vượt trội so với cả thăm dò tuyến tính và tinh chỉnh khi lượng dữ liệu có nhãn bị hạn chế. Bây giờ chúng tôi khám phá thêm các lợi thế của SubTuning trong thiết lập Học Tích cực dựa trên nhóm (AL), trong đó có sẵn một nhóm lớn dữ liệu không nhãn, và các ví dụ bổ sung có thể được gắn nhãn để cải thiện độ chính xác của mô hình. Điều cần thiết cần lưu ý là trong các tình huống thực tế, việc gắn nhãn là một quá trình tốn kém, đòi hỏi chuyên môn lĩnh vực và một lượng lớn nỗ lực thủ công. Do đó, việc xác định các ví dụ thông tin nhất để tối ưu hóa hiệu suất của mô hình là rất quan trọng [28].

Một phương pháp phổ biến trong thiết lập này là sử dụng sự bất định của mô hình để chọn các ví dụ tốt nhất [36,25,3,26]. Quá trình gắn nhãn các ví dụ trong AL bao gồm việc huấn luyện mô hình lặp đi lặp lại sử dụng tất cả dữ liệu có nhãn, và chọn tập ví dụ tiếp theo để được gắn nhãn bằng mô hình. Quá trình này được lặp lại cho đến khi đạt được hiệu suất mong muốn hoặc ngân sách để gắn nhãn các ví dụ bị cạn kiệt.

Trong các thí nghiệm của chúng tôi (xem chi tiết trong Phụ lục A.1), chúng tôi chọn các ví dụ theo biên phân loại của chúng. Chúng tôi bắt đầu với 100 ví dụ được chọn ngẫu nhiên từ tập dữ liệu CIFAR-10. Ở mỗi vòng lặp chúng tôi chọn và gắn nhãn các ví dụ bổ sung, huấn luyện với 500 đến 10,000 ví dụ có nhãn được chọn lặp đi lặp lại theo biên của chúng. Ví dụ, sau khi huấn luyện trên 100 ví dụ được chọn ngẫu nhiên ban đầu, chúng tôi chọn 400 ví dụ có biên phân loại thấp nhất và tiết lộ nhãn của chúng. Sau đó chúng tôi huấn luyện trên 500 ví dụ có nhãn mà chúng tôi có, trước khi chọn thêm 500 ví dụ khác để gắn nhãn để đạt 1k ví dụ. Trong Hình 11 chúng tôi so sánh hiệu suất của mô hình khi được huấn luyện trên các ví dụ được chọn bởi quy tắc dựa trên biên của chúng tôi, với huấn luyện trên các tập con của các ví dụ được chọn ngẫu nhiên. Chúng tôi cũng so sánh phương pháp của chúng tôi với tinh chỉnh đầy đủ có và không có lựa chọn ví dụ dựa trên biên.

Rõ ràng, chúng ta thấy rằng sử dụng SubTuning cho AL vượt trội hơn tinh chỉnh đầy đủ, và tiêu chí lựa chọn mà chúng tôi sử dụng mang lại sự thúc đẩy đáng kể trong hiệu suất.

B.4.2 SubTuning Siamese

Trong thiết lập đa nhiệm vụ được thảo luận trong Phần 4, chúng ta có một mạng f_θ được huấn luyện trên một nhiệm vụ, và chúng ta muốn huấn luyện một mạng khác bằng cách tinh chỉnh các trọng số θ cho một nhiệm vụ khác, dẫn đến trọng số mới θ̃. Tại thời điểm suy luận, chúng ta cần tính toán cả f_θ(x) và f_{θ̃}(x), giảm thiểu chi phí bổ sung của việc tính toán f_{θ̃}(x), trong khi bảo tồn hiệu suất tốt. Vì f_θ(x) được tính toán dù sao đi nữa, các đặc trưng của nó có sẵn mà không tốn chi phí bổ sung, và chúng ta có thể kết hợp chúng với các đặc trưng mới. Để đạt được điều này, chúng tôi nối các biểu diễn được cung cấp bởi f_θ(x) và f_{θ̃}(x) trước khi đưa chúng vào đầu phân loại. Phương pháp này được gọi là SubTuning Siamese (Xem minh họa trong Hình 12).

Hiệu quả của SubTuning Siamese được đánh giá trên nhiều tập dữ liệu và thấy rằng nó đặc biệt có lợi trong các tình huống dữ liệu bị hạn chế. Ví dụ, khi tinh chỉnh trên 5,000 mẫu huấn luyện được chọn ngẫu nhiên từ các tập dữ liệu CIFAR-10, CIFAR-100 và Stanford Cars, SubTuning Siamese với ResNet-18 vượt trội hơn SubTuning tiêu chuẩn. Cả SubTuning và SubTuning Siamese đều cải thiện hiệu suất đáng kể khi so sánh với thăm dò tuyến tính trong thiết lập này. Ví dụ, thăm dò tuyến tính trên ResNet-18 trên CIFAR-10 đạt được 79% độ chính xác, trong khi SubTuning Siamese đạt được 88% độ chính xác trong cùng thiết lập (Xem Hình 14).

--- TRANG 19 ---
Hình 12: Minh họa SubTuning so với SubTuning Siamese. Lưu ý rằng sự khác biệt là nhiệm vụ mới bây giờ nhận được các đặc trưng gốc làm đầu vào.

So sánh giữa SubTuning và SubTuning Siamese của chúng tôi dựa trên các thí nghiệm được thực hiện trên 5,000 mẫu huấn luyện được chọn ngẫu nhiên từ các tập dữ liệu CIFAR10, CIFAR100 và Stanford Cars. Kết quả cho các resblock của ResNet-50 và ResNet-18 được cung cấp trong Hình 13 và 14 tương ứng. Chúng tôi cũng cung cấp kết quả của SubTuning các lớp ResNet đầy đủ, bao gồm SubTuning một vài khối liên tiếp được áp dụng cho cùng độ phân giải (Xem Hình 15). Như có thể thấy từ các hình, SubTuning Siamese thêm một sự thúc đẩy hiệu suất trong đại đa số các kiến trúc, tập dữ liệu và lựa chọn khối.

B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B16
Các khối để Tinh chỉnh0.880.890.900.910.92Độ chính xác
CIFAR10
SubTuning Siamese
SubTuning
B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B16
Các khối để Tinh chỉnh0.560.580.600.620.640.66Độ chính xác
CIFAR100
SubTuning Siamese
SubTuning
B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B16
Các khối để Tinh chỉnh0.20.30.40.50.60.7Độ chính xác
Stanford Cars
SubTuning Siamese
SubTuning
Hình 13: SubTuning Siamese cho ResNet-50 trên CIFAR-10 (trái.), CIFAR-100 (giữa.) và Standford Cars (phải.). Chúng tôi sử dụng 5,000 mẫu huấn luyện được chọn ngẫu nhiên từ mỗi tập dữ liệu.

B1
B2
B3
B4
B5
B6
B7
B8
Các khối để Tinh chỉnh0.820.840.860.88Độ chính xác
CIFAR10
SubTuning Siamese
SubTuning
B1
B2
B3
B4
B5
B6
B7
B8
Các khối để Tinh chỉnh0.5000.5250.5500.5750.6000.625Độ chính xác
CIFAR100
SubTuning Siamese
SubTuning
B1
B2
B3
B4
B5
B6
B7
B8
Các khối để Tinh chỉnh0.20.30.40.5Độ chính xác
Stanford Cars
SubTuning Siamese
SubTuning
Hình 14: Tác động của SubTuning Siamese trên ResNet-18 khi sử dụng 5,000 mẫu huấn luyện được chọn ngẫu nhiên từ mỗi tập dữ liệu.

B.4.3 Pruning

Trong khám phá SubTuning của chúng tôi, chúng tôi đã chứng minh hiệu quả của nó trong việc giảm chi phí thêm nhiệm vụ mới cho Học Đa Nhiệm vụ (MTL) trong khi duy trì hiệu suất cao trên những nhiệm vụ đó. Để tối ưu hóa thêm hiệu quả tính toán và giảm kích thước mô hình cho các nhiệm vụ mới, chúng tôi giới thiệu khái niệm pruning kênh trên thành phần SubTuned của mô hình. Chúng tôi sử dụng hai loại pruning, cục bộ và toàn cục, để giảm kích thước tham số và thời gian chạy của mô hình trong khi bảo tồn độ chính xác của nó. Pruning cục bộ loại bỏ một phần bằng nhau của các kênh cho mỗi lớp, trong khi pruning toàn cục loại bỏ các kênh trên toàn mạng bất kể có bao nhiêu kênh được loại bỏ mỗi lớp. Theo phương pháp của Li et al. [37], đối với cả hai kỹ thuật pruning chúng tôi prune các trọng số có chuẩn L1 và L2 thấp nhất để đáp ứng tỷ lệ pruning mục tiêu.

--- TRANG 20 ---
L1
L2
L3
L4
Các lớp để Tinh chỉnh0.880.890.900.910.92Độ chính xác
CIFAR10
SubTuning Siamese
SubTuning
L1
L2
L3
L4
Các lớp để Tinh chỉnh0.5500.5750.6000.6250.6500.675Độ chính xác
CIFAR100
SubTuning Siamese
SubTuning
L1
L2
L3
L4
Các lớp để Tinh chỉnh0.20.30.40.50.60.7Độ chính xác
Stanford Cars
SubTuning Siamese
SubTuning
L1
L2
L3
L4
Các lớp để Tinh chỉnh0.800.820.840.860.88Độ chính xác
CIFAR10
SubTuning Siamese
SubTuning
L1
L2
L3
L4
Các lớp để Tinh chỉnh0.450.500.550.60Độ chính xác
CIFAR100
SubTuning Siamese
SubTuning
L1
L2
L3
L4
Các lớp để Tinh chỉnh0.20.30.40.50.6Độ chính xác
Stanford Cars
SubTuning Siamese
SubTuningHình 15: Tác động của SubTuning Siamese của toàn bộ các Lớp ResNet (một nhóm khối được áp dụng cho cùng độ phân giải). Kết quả cho 5,000 mẫu huấn luyện được chọn ngẫu nhiên từ mỗi tập dữ liệu được trình bày cho ResNet50 (Trên) và ResNet18 (Dưới).

Hiệu quả của việc kết hợp pruning kênh với SubTuning trên 3 khối cuối của ResNet-50 được chứng minh trong kết quả của chúng tôi. Thay vì chỉ sao chép các trọng số và sau đó huấn luyện các khối, chúng tôi thêm một bước pruning bổ sung trước khi huấn luyện. Bằng cách này, chúng tôi chỉ prune mạng gốc, đông băng, một lần cho tất cả các nhiệm vụ tương lai. Kết quả của chúng tôi cho thấy rằng pruning có hiệu quả trên các mục tiêu tham số khác nhau, giảm chi phí với chỉ suy giảm hiệu suất nhỏ. Ví dụ, khi sử dụng ít hơn 3% của 3 khối cuối (khoảng 2% của tất cả các tham số của ResNet-50), chúng tôi duy trì 94% độ chính xác trên tập dữ liệu CIFAR-10, so với khoảng 91% độ chính xác đạt được bởi thăm dò tuyến tính trong cùng thiết lập.

Tất cả kết quả pruning cho pruning Cục bộ hoặc Toàn cục với chuẩn L1 hoặc L2 và yếu tố thưa thớt kênh thay đổi giữa 0.1 và 0.9 với mức tăng 0.1 được trình bày trong Hình 16. Vì chúng tôi không có mục tiêu cụ thể về hiệu suất hoặc tỷ lệ tham số, chúng tôi cung cấp kết quả cho nhiều phần của tổng tham số SubTuning và giá trị độ chính xác. Mặc dù có sự khác biệt nhỏ giữa các phương pháp, tất cả chúng đều mang lại kết quả tốt trong việc giảm độ phức tạp của mô hình SubTuning với chỉ suy giảm độ chính xác nhỏ.

0% 20% 40% 60% 80% 100%
% Tham số của SubTuning đầy đủ0.9350.9400.9450.9500.9550.960Độ chính xác
Độ chính xác so với Tham số trong SubTuning
L1 Cục bộ
L1 Toàn cục
L2 Cục bộ
L2 Toàn cục
Không Pruning
Hình 16: Kết quả đầy đủ của SubTuning với pruning theo kênh trên 3 khối cuối của ResNet-50. Chúng tôi vẽ đồ thị độ chính xác so với tỷ lệ pruning của các kỹ thuật pruning khác nhau (Toàn cục so với Cục bộ và chuẩn pruning) cho các tỷ lệ pruning khác nhau.

B.4.4 Ảnh hưởng của Khởi tạo Lại Ngẫu nhiên

Trong khám phá SubTuning của chúng tôi, chúng tôi phát hiện rằng việc khởi tạo các trọng số của khối SubTuned với các trọng số đã được huấn luyện trước từ một nhiệm vụ khác cải thiện đáng kể cả hiệu suất và tốc độ huấn luyện. Cụ thể, chúng tôi chọn một khối của ResNet-50, được huấn luyện trước trên ImageNet, và tinh chỉnh nó trên tập dữ liệu CIFAR-10. Chúng tôi so sánh phương pháp này với một phương pháp thay thế trong đó chúng tôi khởi tạo lại ngẫu nhiên các trọng số của cùng khối trước khi tinh chỉnh nó trên tập dữ liệu CIFAR-10. Kết quả, được trình bày trong Hình 17, cho thấy rằng các trọng số đã được huấn luyện trước dẫn đến hội tụ nhanh hơn và hiệu suất tốt hơn, đặc biệt khi tinh chỉnh các lớp trước đó. Ngược lại, khởi tạo ngẫu nhiên các trọng số của khối dẫn đến hiệu suất kém, ngay cả với thời gian huấn luyện dài hơn là 80 epoch.

--- TRANG 21 ---
B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
B11
B12
B13
B14
B15
B16
Khối SubTuning0.600.700.800.900.95Độ chính xác
Ảnh hưởng của Khởi tạo Lại
ReInit 10e
SubTuning 10e
ReInit 80e
SubTuning 80e
Hình 17: Ảnh hưởng của huấn luyện dài hơn và khởi tạo lại trọng số lên Hồ sơ Tinh chỉnh của ResNet-50 được huấn luyện trước trên ImageNet và được tinh chỉnh trên CIFAR-10. Đối với việc khởi tạo lại ngẫu nhiên các trọng số, chúng tôi gặp một số vấn đề tối ưu hóa khi huấn luyện khối đầu tiên trên mỗi độ phân giải của mô hình ResNet, tức là các khối 1, 4, 8 và 14. Chúng tôi sử dụng thang logarit cho trục y, vì nó cho phép nhìn rõ khoảng cách giữa các thang đo khác nhau.

B.5 Hiệu quả Tính toán

Trong tiểu mục này chúng tôi cung cấp thêm một số kết quả thời gian suy luận. Trong Hình 18 chúng tôi cung cấp kết quả tuyệt đối cho thời gian suy luận SubTuning cho một số khối liên tiếp khác nhau. Trong Hình 19 chúng tôi cung cấp độ chính xác so với thời gian suy luận thêm vào cho 2 khối liên tiếp. Chúng ta có thể thấy rằng sử dụng khối 13 và 14 mang lại kết quả xuất sắc cả về thời gian chạy và độ chính xác.

ResNet-50
B1+B2
B2+B3
B3+B4
B4+B5
B5+B6
B6+B7
B7+B8
B8+B9
B9+B10
B10+B11
B11+B12
B12+B13
B13+B14
B14+B15
B15+B163.43.53.63.73.8Thời gian Suy luận (ms)Tốc độ Suy luận so với Subtune 2 Khối
ResNet-50
B:1-3
B:2-4
B:3-5
B:4-6
B:5-7
B:6-8
B:7-9
B:8-10
B:9-11
B:10-12
B:11-13
B:12-14
B:13-15
B:14-163.43.53.63.73.83.94.0Thời gian Suy luận (ms)Tốc độ Suy luận so với Subtune 3 Khối
Hình 18: Thời gian suy luận tuyệt đối cho GPU A100 SubTuning trên 2 và 3 khối.

--- TRANG 22 ---
9% 10% 11% 12% 13%
Thời gian Suy luận Thêm vào %0.94250.94500.94750.95000.95250.95500.95750.9600Độ chính xác
L3B5+L4B0Thời gian Suy luận so với Độ chính xác (Subtune 2 Lớp)
Lớp1
Lớp1 + Lớp2
Lớp2
Lớp2 + Lớp3Lớp3
Lớp3 + Lớp4
Lớp4Lớp1
Lớp1 + Lớp2
Lớp2
Lớp2 + Lớp3Lớp3
Lớp3 + Lớp4
Lớp4Hình 19: Độ chính xác so với thời gian suy luận của SubTuning hai khối liên tiếp.

C Chứng minh Định lý 1

Chúng tôi phân tích một phiên bản hơi sửa đổi của thuật toán SubTuning tham lam. Đối với một tập các tham số đã được huấn luyện trước θ nào đó, và một tập con của các lớp S nào đó, ký hiệu θ_S là tập các tham số của các lớp trong tập con S, và ψ_{θ_S} là các đặc trưng Neural Tangent Kernel (NTK) được tạo ra bởi các tham số này. Chúng tôi giả sử rằng đối với tất cả x và θ chúng ta có ||ψ_θ(x)||_∞≤1. Đối với một w nào đó, định nghĩa giả thuyết h_{θ,S,w}(x) = ⟨ψ_{θ_S}(x),w⟩.

Cho ℓ là hinge-loss, và ký hiệu mất mát trên phân phối L_D(h) = E_{(x,y)∼D}[ℓ(h(x), y)]. Khi đó, chúng tôi định nghĩa thuật toán chọn bộ tối thiểu hóa của hàm mất mát trên NTK, tuân theo ràng buộc chuẩn Δ:

evaluate(D, θ, S, Δ) = min_{||w||≤Δ} L_D(h_{θ,S,w})

Chúng tôi phân tích thuật toán sau:

Thuật toán 2 Greedy-SubTuning
1:procedure GREEDY SUBSET SELECTION (all_layers, D,θ,ε,Δ,r′)
2: S← {} ,n← |all_layers |
3: A_best←evaluate(D,θ,S,Δ)
4: fori= 1tondo
5: A_iter← ∞ ,L_best←null
6: forL∈(all_layers - S)do
7: S′←S∪ {L}
8: A_new←evaluate(D,θ,S′,Δ)
9: ifA_new< A_iter−εthen
10: L_best←L,A_iter←A_new
11: end if
12: end for
13: ifA_iter< A_best−εandparams(S∪ {L_best})≤r′then
14: A_best←A_iter,S←S∪ {L_best}
15: else
16: Break
17: end if
18: end for
19: return S
20:end procedure

Cố định một phân phối của các ví dụ có nhãn D nào đó. Cho S là một mẫu của m ví dụ i.i.d. từ D, và ký hiệu bằng Ď phân phối thực nghiệm của việc chọn ngẫu nhiên một ví dụ từ S. Cố định một δ′ nào đó. Khi đó, sử dụng Định lý 26.12 từ [61], đối với mọi tập con của các lớp S với nhiều nhất r′ tham số, với xác suất ít nhất 1−δ′, đối với mọi w với ||w|| ≤ Δ:

L_D(h_{θ,S,w})− L_Ď(h_{θ,S,w})≤2√(r′Δ)/√m + (1 +√(r′Δ))√(2 log(4/δ′))/m

Bây giờ, cho S_1, . . . , S_T là tất cả các tập con được đánh giá trong thời gian chạy của GreedySubsetSelection(all_layers, D, θ, ε, Δ, r′), cụ thể là thuật toán chạy trên phân phối thực D. Lưu ý rằng nếu có n lớp trong mô hình, thì có nhiều nhất n² tập con như vậy. Cho

m > 16r′Δ²/ε² + 2(1 +√(r′Δ))² 2 log(4n²/δ)/ε² = O(r′Δ² log(n/δ)/ε²)

sử dụng ràng buộc hợp nhất chúng ta nhận được rằng, với xác suất ít nhất 1−δ, đối với tất cả S_1, . . . , S_T ta có L_D(h_{θ,S_i,w})− L_Ď(h_{θ,S_i,w})≤ε/2. Do đó, chúng ta có evaluate(D, θ, S_i, Δ)−evaluate(Ď, θ, S_i, Δ)≤ε/2 đối với tất cả S_i. Điều này có nghĩa là, với xác suất ít nhất 1−δ, việc chạy GreedySubsetSelection(all_layers, Ď, θ, ε, Δ, r′) phải chọn các tập con S_1, . . . , S_T. Vì chúng ta đã chỉ ra rằng L_D(h_{θ,S_T,w})− L_Ď(h_{θ,S_T,w})≤ε/2 chúng ta nhận được bảo đảm tổng quát hóa yêu cầu trên đầu ra của thuật toán thực nghiệm.
