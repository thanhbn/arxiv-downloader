# 2308.07163.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2308.07163.pdf
# Kích thước file: 966831 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Mạng Nơ-ron HyperSparse: Chuyển từ Khám phá sang Khai thác thông qua
Chính quy hóa Thích ứng
Patrick Glandorf*, Timo Kaiser*, Bodo Rosenhahn
Viện Xử lý Thông tin (tnt)
L3S - Đại học Leibniz Hannover, Đức
{glandorf, kaiser, rosenhahn }@tnt.uni-hannover.de
Tóm tắt
Mạng nơ-ron thưa thớt là yếu tố quan trọng trong việc phát triển
các ứng dụng học máy tiết kiệm tài nguyên. Chúng tôi đề xuất
phương pháp học thưa thớt mới và mạnh mẽ Huấn luyện Chính quy hóa Thích ứng (ART) để nén mạng dày đặc thành
mạng thưa thớt. Thay vì sử dụng mặt nạ nhị phân thông thường
trong quá trình huấn luyện để giảm số lượng trọng số mô hình,
chúng tôi thu nhỏ trọng số gần bằng không một cách lặp lại
với việc tăng chính quy hóa trọng số. Phương pháp của chúng tôi
nén "kiến thức" mô hình được huấn luyện trước vào các
trọng số có độ lớn cao nhất. Do đó, chúng tôi giới thiệu một
hàm mất mát chính quy hóa mới có tên HyperSparse khai thác
các trọng số cao nhất đồng thời bảo tồn khả năng khám phá trọng số. Các thí nghiệm mở rộng trên CIFAR và TinyImageNet cho thấy phương pháp của chúng tôi dẫn đến những cải thiện hiệu suất đáng chú ý so với các phương pháp làm thưa khác, đặc biệt trong chế độ thưa thớt cực cao lên đến 99.8% thưa thớt mô hình. Các điều tra bổ sung cung cấp hiểu biết mới về các mẫu được mã hóa trong trọng số có độ lớn cao.¹

1. Giới thiệu
Những năm gần đây đã chứng kiến tiến bộ to lớn trong
lĩnh vực học máy dựa trên việc sử dụng mạng nơ-ron (NN). Cùng với việc tăng độ chính xác trong gần như tất cả
các nhiệm vụ, độ phức tạp tính toán của NN cũng tăng,
ví dụ, đối với Transformers [5,7] hoặc Mô hình Ngôn ngữ Lớn [2].
Độ phức tạp gây ra chi phí năng lượng cao, hạn chế khả năng áp dụng cho các hệ thống hiệu quả về chi phí [10], và có tác động tiêu cực đến tính công bằng và đáng tin cậy do khả năng diễn giải giảm sút [38].

Đối mặt với những vấn đề này, những năm gần đây cũng dẫn đến một cộng đồng ngày càng lớn trong lĩnh vực NN thưa thớt [12]. Mục tiêu là tìm các đồ thị con nhỏ (còn gọi là NN thưa thớt) trong NN hoạt động tốt có khả năng tương tự hoặc có thể so sánh được liên quan đến các nhiệm vụ chính trong khi ít phức tạp hơn đáng kể

*Những tác giả này đóng góp như nhau cho công trình này
¹Mã nguồn có sẵn tại https://github.com/GreenAutoML4FAS/HyperSparse

và do đó rẻ hơn và có khả năng diễn giải tốt hơn.
Các phương pháp tiêu chuẩn thường tạo NN thưa thớt bằng cách có được mặt nạ nhị phân hạn chế số lượng trọng số được sử dụng trong NN [20, 34, 42]. Phương pháp nổi bật nhất là Cắt tỉa Độ lớn Lặp (IMP) [16] dựa trên Giả thuyết Vé số (LTH) [9]. Giả định rằng các trọng số quan trọng có độ lớn cao sau khi huấn luyện, nó huấn luyện một NN dày đặc và loại bỏ một lượng phần tử khỏi mặt nạ tương ứng với các trọng số thấp nhất. Sau đó, NN thưa thớt được khởi tạo lại và huấn luyện lại từ đầu. Quá trình được lặp lại cho đến khi đạt được mức độ thưa thớt.

Giả định của cắt tỉa độ lớn rằng các trọng số cao nhất trong NN dày đặc mã hóa các quy tắc quyết định quan trọng nhất cho một tập hợp đa dạng các lớp là có vấn đề, vì điều này không được đảm bảo. Các trọng số bị loại bỏ có thể hữu ích

--- TRANG 2 ---
cho dự đoán không còn có thể được kích hoạt lại trong quá trình tinh chỉnh. Trong trường hợp xấu nhất, "sự sụp đổ lớp" có thể ngăn cản việc lan truyền thuận hữu ích [37]. Việc thiếu khả năng khám phá vẫn tồn tại trong phương pháp IMP lặp chính xác hơn nhưng tiêu thụ tài nguyên.

Phục hồi các ý tưởng chính của Han et al. [10] và Narang et al. [27] (có thể so sánh với [26]), chúng tôi giới thiệu một phương pháp nhẹ và mạnh mẽ được gọi là Huấn luyện Chính quy hóa Thích ứng (ART) để có được NN thưa thớt cao, nó ngầm "loại bỏ" trọng số với việc tăng chính quy hóa cho đến khi đạt được mức độ thưa thớt mong muốn. ART chính quy hóa mạnh các trọng số trước khi cắt tỉa độ lớn. Đầu tiên, một NN dày đặc được huấn luyện trước cho đến hội tụ. Trong giai đoạn thứ hai, NN được huấn luyện với chính quy hóa tăng dần và phân rã trọng số cho đến khi NN được cắt tỉa độ lớn giả định hoạt động ngang bằng với đối tác dày đặc. Cuối cùng, chúng tôi áp dụng cắt tỉa độ lớn và tinh chỉnh NN mà không có chính quy hóa. Tránh mặt nạ nhị phân trong giai đoạn thứ hai cho phép khám phá và chính quy hóa buộc khai thác các trọng số còn lại trong NN thưa thớt. Chúng tôi giới thiệu phương pháp chính quy hóa mới HyperSparse cho giai đoạn thứ hai vượt qua chính quy hóa tĩnh như Lasso [39] hoặc Weight Decay [44] và thích ứng với độ lớn trọng số bằng cách phạt các trọng số nhỏ. HyperSparse cân bằng sự đánh đổi khám phá/khai thác và do đó tăng độ chính xác đồng thời dẫn đến hội tụ nhanh hơn trong giai đoạn thứ hai.

Sự kết hợp của lịch trình chính quy hóa và HyperSparse cải thiện độ chính xác phân loại và thời gian tối ưu hóa đáng kể, đặc biệt trong chế độ thưa thớt cao với tối đa 99.8% trọng số bằng không. Chúng tôi đánh giá phương pháp của mình trên CIFAR-10/100 [19] và TinyImageNet [6] với ResNet-32 [11] và VGG-19 [33].

Hơn nữa, chúng tôi phân tích phân bố gradient và trọng số trong quá trình huấn luyện chính quy hóa, cho thấy HyperSparse dẫn đến hội tụ nhanh hơn đến NN thưa thớt. Các thí nghiệm cũng cho thấy rằng tuyên bố của [34], rằng NN thưa thớt tối ưu có thể được thu được thông qua heuristic phân bố trọng số đơn giản, không đúng nói chung. Cuối cùng, chúng tôi phân tích quá trình nén NN dày đặc thành NN thưa thớt và cho thấy rằng các trọng số cao nhất trong NN không mã hóa quy tắc quyết định cho một tập hợp đa dạng các lớp với ưu tiên bằng nhau.

Tóm lại, bài báo này
• giới thiệu HyperSparse, một hàm mất mát chính quy hóa thích ứng vượt trội ngầm thúc đẩy tính thưa thớt mạng có thể cấu hình bằng cách cân bằng sự đánh đổi khám phá và khai thác.
• giới thiệu khung mới ART để có được mạng thưa thớt sử dụng chính quy hóa với đòn bẩy tăng dần, cải thiện thời gian tối ưu hóa và độ chính xác phân loại của mạng nơ-ron thưa thớt, đặc biệt trong chế độ thưa thớt cao.
• phân tích quá trình liên tục nén các mẫu từ mạng nơ-ron dày đặc sang thưa thớt.

2. Công trình liên quan
Các phương pháp Học thưa thớt tìm mặt nạ nhị phân để loại bỏ một lượng trọng số được xác định trước có thể được phân loại là tĩnh hoặc động (ví dụ, trong [4, 12, 14]). Theo [4], trong huấn luyện thưa thớt động "[...] các phần tử bị loại bỏ [khỏi mặt nạ] có cơ hội được phát triển trở lại nếu chúng có thể có lợi cho dự đoán" trong khi huấn luyện tĩnh kết hợp mặt nạ cố định.

Các phương pháp tĩnh thường dựa trên Frankle et al. [9], người giới thiệu LTH và cho thấy rằng NN thưa thớt hoạt động tốt trong NN khởi tạo ngẫu nhiên có thể được tìm thấy sau khi huấn luyện dày đặc thông qua cắt tỉa độ lớn. Phương pháp cắt tỉa độ lớn được cải thiện bởi IMP [16] lặp lại quá trình này. Thay thế quy trình huấn luyện tốn thời gian, các phương pháp như SNIP [20] hoặc GraSP [42] tìm NN thưa thớt trong NN dày đặc khởi tạo ngẫu nhiên sử dụng một dự đoán mạng duy nhất và gradient của nó. Để cũng giải quyết rủi ro sụp đổ lớp trong quá trình cắt tỉa, SynFlow [37] bổ sung bảo tồn tổng luồng trong mạng. Trái ngược với các công trình sau này, Su et al. [34] tuyên bố rằng NN thưa thớt phù hợp không phụ thuộc vào dữ liệu hoặc khởi tạo trọng số và cung cấp một heuristic chung cho phân bố trọng số.

Khác với các phương pháp tĩnh, các phương pháp động cắt tỉa và kích hoạt lại các phần tử bằng không trong mặt nạ nhị phân. Các trọng số được kích hoạt lại có thể được chọn ngẫu nhiên [25] hoặc được xác định bởi gradient [3, 4, 8]. Ví dụ, RigL [8] lặp lại cắt tỉa trọng số có độ lớn thấp và do đó kích hoạt lại trọng số có gradient cao nhất. Ngoài ra, các phương pháp động hiện đại sử dụng mặt nạ liên tục. Ví dụ, Tai et al. [36] nới lỏng khung IMP bằng cách giới thiệu softmask được tham số hóa để có được trung bình có trọng số giữa IMP và Top-KAST [15]. Tương tự, [24, 31] nới lỏng mặt nạ nhị phân và tối ưu hóa chuẩn L0 của nó. Một cách khác là cắt tỉa mô hình một cách vốn có, ví dụ, bằng cách giảm gradient của trọng số có độ lớn nhỏ [32]. So với các phương pháp tĩnh, Liu et al. [22, 23] cho thấy các phương pháp huấn luyện thưa thớt động vượt qua hầu hết các phương pháp tĩnh bằng cách cho phép khám phá trọng số.

Một thuộc tính khác để phân biệt các phương pháp học thưa thớt hiện đại là độ phức tạp trong quá trình tạo mặt nạ, ví dụ, như được thực hiện bởi Schwarz et al. [32]. Các phương pháp thưa thớt→thưa thớt tiết kiệm tài nguyên hơn duy trì NN thưa thớt trong quá trình huấn luyện [4,8,20,32,34,37,42], trong khi các phương pháp dày đặc→thưa thớt sử dụng tất cả tham số trước khi tìm mặt nạ cuối cùng [9, 15, 16, 24, 31, 36].

Tuy nhiên, như sẽ giải thích sau, phương pháp của chúng tôi thuộc về các phương pháp dày đặc→thưa thớt giảm độ phức tạp mô hình một cách vốn có mà không che mặt trước khi cắt tỉa độ lớn để có được mặt nạ thưa thớt tĩnh cho tinh chỉnh. Chúng tôi muốn đề cập đến các công trình chính của Han et al. [10], Narang et al. [27] và Molchanov et al. [26] mà sự kết hợp của chúng là một mô hình vai trò cho chúng tôi. Han et al. sử dụng chính quy hóa L1 và L2 để giảm số lượng phần tử khác không trong

--- TRANG 3 ---
quá trình huấn luyện. Khung làm việc ban đầu của họ sử dụng chính quy hóa mà không có những thứ cầu kỳ và không có khả năng kiểm soát mức độ thưa thớt. Narang et al. và Molchanov et al. loại bỏ trọng số theo từng phần nhỏ với ngưỡng loại bỏ tăng dần, nhưng không kết hợp khám phá trọng số.

Khả năng diễn giải và Hiểu biết về học máy có liên quan chặt chẽ đến học thưa thớt và cũng được đề cập trong bài báo này. Có số lượng công trình ngày càng tăng trong những năm gần đây sử dụng học thưa thớt cho những lợi ích khác, ví dụ, để tìm mối tương quan có thể diễn giải giữa không gian đặc trưng và hình ảnh [38] hoặc để hình dung sự mơ hồ giữa các lớp [18]. Công trình của Paul et al. [28] cung cấp chi tiết về giai đoạn học ban đầu rất quan trọng, ví dụ, để xác định việc ghi nhớ nhiễu nhãn [17]. Họ cho thấy rằng hầu hết dữ liệu không cần thiết để có được các mạng con phù hợp. Mối quan hệ chung giữa LTH và khái quát hóa được điều tra trong [30]. Varma et al. [35] cho thấy rằng NN thưa thớt phù hợp hơn trong các chế độ dữ liệu hạn chế và nhiễu. Mặt khác Hooker et al. [13] cho thấy rằng NN thưa thớt có tác động không tầm thường đến thiên vị đạo đức bằng cách điều tra mẫu nào bị "quên" đầu tiên trong quá trình nén mạng. Câu hỏi nghiên cứu cơ bản của công trình sau này được thay đổi thành "Mẫu nào được nén đầu tiên?" và được thảo luận trong bài báo này.

3. Phương pháp
Làm thưa thớt nhằm giảm số lượng trọng số khác không trong NN. Để giải quyết vấn đề này, chúng tôi sử dụng một lịch trình nhất định cho chính quy hóa sao cho các trọng số nhỏ hội tụ về không và mô hình của chúng tôi ngầm trở nên thưa thớt.

Trong Mục 3.1, chúng tôi định nghĩa chính thức vấn đề làm thưa thớt. Sau đó, chúng tôi trình bày Huấn luyện Chính quy hóa Thích ứng (ART) trong Mục 3.2, tăng đòn bẩy chính quy hóa một cách lặp để tối đa hóa số lượng trọng số gần bằng không. Hơn nữa, chúng tôi giới thiệu hàm mất mát chính quy hóa HyperSparse trong Mục 3.3 được tích hợp trong ART. Nó đồng thời cho phép khám phá các tô-pô mới trong khi khai thác trọng số của mạng con thưa thớt cuối cùng.

3.1. Kiến thức cơ bản
Chúng tôi xem xét một NN f(W, x) với tô-pô f và trọng số W được huấn luyện để phân loại hình ảnh từ một tập dữ liệu S={(xn, yn)}^N_{n=1}, trong đó yn là lớp thực tế cho một mẫu hình ảnh xn. Quá trình huấn luyện được cấu trúc theo các epoch, là những tối ưu hóa lặp của trọng số W={w1, . . . , wD} trên tất cả các mẫu trong S để tối thiểu hóa mục tiêu mất mát L. Các trọng số thu được sau epoch e được ký hiệu là We, với W0 ký hiệu trọng số trước khi tối ưu hóa. Hơn nữa, độ chính xác phân loại của NN được đo bằng hàm đánh giá ψ(W).

Mục tiêu trong làm thưa thớt là giảm cardinality của W bằng cách loại bỏ một tỷ lệ trọng số được xác định trước κ, trong khi tối đa hóa ψ(W). Mạng được cắt tỉa bằng tích Hadamard m⊙W của mặt nạ nhị phân m∈[0,1]^D và trọng số mô hình W. Mặt nạ thường được tạo bằng cách áp dụng cắt tỉa độ lớn m=ν(W)[3, 4, 9, 16], là một kỹ thuật đặt κ-trọng số thấp nhất bằng không.

3.2. Huấn luyện Chính quy hóa Thích ứng (ART)
Các hàm mất mát chính quy hóa như chuẩn L1 (hồi quy Lasso) [39] hoặc chuẩn L2 [44] được sử dụng để ngăn chặn overfitting bằng cách thu nhỏ độ lớn của trọng số. Chúng tôi sử dụng hiệu ứng này trong ART để làm thưa thớt, vì các trọng số có độ lớn thấp có ít ảnh hưởng đến việc thay đổi đầu ra và do đó có thể được loại bỏ chỉ với tác động nhỏ đến ψ(W).

Chính quy hóa trong quá trình huấn luyện có thể được biểu diễn như một hàm mất mát hỗn hợp

L_total = L_class + λ_init · η^e · L_reg, (1)

trong đó L_class là hàm mất mát phân loại và L_reg là hàm mất mát chính quy hóa. Gradient của L_reg thu nhỏ một tập hợp trọng số về xấp xỉ bằng không và tạo ra một mạng thưa thớt vốn có với tỷ lệ cắt tỉa không xác định [39]. Tăng η tạo đòn bẩy cho chính quy hóa L_reg theo cách tăng dần, nhưng các phương pháp hiện tại sử dụng tỷ lệ chính quy hóa cố định η = 1[4, 10, 26].

Sau khi huấn luyện không chính quy hóa của NN dày đặc đến hội tụ, ART sử dụng khung chính quy hóa tiêu chuẩn và sửa đổi nó bằng cách đặt η > 1 và khởi tạo thấp λ_init. Sau đó, hàm mất mát chính quy hóa L_reg hầu như không có ảnh hưởng đến L_total ban đầu, nhưng bắt đầu thu nhỏ trọng số mà không có nhiều tác động đến L_class về không. Tuy nhiên, nó cho phép mọi trọng số wi có khả năng có độ lớn cao sao cho wi được chuyển vào NN thưa thớt của các trọng số cao nhất (khám phá). Với việc tăng chính quy hóa, ảnh hưởng của gradient dL_reg/dwi trên wi tăng và có khả năng vượt qua gradient dL_class/dwi. Chính quy hóa cản trở việc khám phá đúng của các trọng số nhỏ bằng cách kéo độ lớn về không. Mặt khác, các trọng số lớn hơn

Thuật toán 1 Huấn luyện Chính quy hóa Thích ứng (ART)
Tham số: Trọng số huấn luyện trước W_pre, tỷ lệ ban đầu λ_init, hàm đánh giá ψ(W), cắt tỉa độ lớn ν(W), hệ số tăng η > 1, hàm mất mát phân loại L_class, hàm mất mát chính quy hóa L_reg, dữ liệu huấn luyện S, optimizer SGD(W,L, S)
Kết quả: Trọng số tốt nhất cho tinh chỉnh W_best
1: W0, W_best ← W_pre
2: e ← 0
3: while ψ(ν(W_best)⊙W_best) < ψ(We) do
4:    We+1 ← SGD(We, L_class + λ_init · η^e · L_reg, S)
5:    if ψ(ν(We+1)⊙We+1) > ψ(ν(W_best)⊙W_best) then
6:        W_best ← We+1
7:    end if
8:    e ← e + 1
9: end while

--- TRANG 4 ---
cần được khai thác để bảo tồn kết quả phân loại. Do đó, chính quy hóa tăng dần của chúng tôi liên tục chuyển đổi sự đánh đổi khám phá/khai thác từ khám phá sang khai thác. Phương pháp cho phép sắp xếp lại trọng số để tìm các tô-pô tốt hơn, nhưng buộc phải khai thác các trọng số cao nhất liên quan đến nhiệm vụ phân loại. Do số lượng trọng số xấp xỉ bằng không tăng lên, mô hình dày đặc hội tụ thành một mô hình thưa thớt vốn có. Chúng tôi dừng huấn luyện chính quy hóa nếu NN với trọng số cắt tỉa tốt nhất ψ(ν(W_best)⊙W_best) có độ chính xác cao hơn so với trọng số chưa cắt tỉa mới nhất ψ(We) và chọn W_best làm ứng cử viên cho tinh chỉnh.

Quy trình huấn luyện tổng thể được định nghĩa như sau:
Bước 1: Huấn luyện trước mô hình dày đặc đến hội tụ mà không có chính quy hóa.
Bước 2: Loại bỏ trọng số ngầm sử dụng ART như mô tả trong thuật toán 1.
Bước 3: Áp dụng cắt tỉa độ lớn và tinh chỉnh mạng đã cắt tỉa đến hội tụ.

ART nới lỏng phương pháp IMP lặp cắt tỉa các trọng số ít quan trọng nhất qua các lần lặp nhất định. Tương tự như tỷ lệ cắt tỉa tăng dần trong các phương pháp lặp tiêu chuẩn, chúng tôi tăng dần lượng trọng số gần bằng không và do đó xấp xỉ mặt nạ nhị phân một cách ngầm.

3.3. Chính quy hóa HyperSparse
Mục 3.2 trước mô tả quá trình thu nhỏ trọng số trong W bằng cách phạt với chính quy hóa tăng dần. Một nhược điểm của quy trình này là các trọng số còn lại sau khi cắt tỉa cũng bị phạt bởi chính quy hóa. Điều này ảnh hưởng tiêu cực đến việc khai thác liên quan đến nhiệm vụ chính. Do đó, các trọng số còn lại không nên bị phạt. Mặt khác, nếu các trọng số nhỏ bị phạt mạnh, thuộc tính khám phá mong muốn của các phương pháp cắt tỉa động để "phát triển" lại các phần tử này bị hạn chế. Để giải quyết sự đánh đổi này giữa khai thác và khám phá, chúng tôi giới thiệu hàm mất mát chính quy hóa thích ứng tạo thưa thớt HyperSparse.

Kết hợp hàm Hyperbolic Tangent được áp dụng trên độ lớn ký hiệu là t(·) = tanh(|·|) để đơn giản, hàm mất mát HyperSparse được định nghĩa là

L_HS(W) = (1/A) * Σ|wi| * Σt(s·wj) - Σ|wi|
với A := Σt(s·w) và với mọi w trong W: dA/dw = 0, (2)

trong đó A được coi là một hằng số giả trong tính toán gradient và s là một hệ số căn chỉnh được mô tả sau.

Chính quy hóa phạt trọng số tùy thuộc vào gradient và có thể thay đổi cho các trọng số khác nhau. Gradient của HyperSparse đối với trọng số wi xấp xỉ là

dL_HS(W)/dwi = sign(wi) · t'(s·wi) · (Σ|wj|)/(Σt(s·wj)),
với wi, wj trong W, t'(·) trong (0,1]. (3)

Đạo hàm t' = dt/dwi hội tụ về 1 cho độ lớn nhỏ |wi| ≈ 0 và về 0 cho độ lớn lớn |wi| ≫ 0. Do đó, số hạng thứ hai trong Eq. (3) thích ứng với trọng số và phạt mạnh độ lớn nhỏ, nhưng phân rã về không cho những cái lớn. Chi tiết cho tính toán gradient và phân tích có thể tìm thấy trong tài liệu bổ sung, Mục D.

Hệ số căn chỉnh s là bắt buộc để khai thác các thuộc tính đã đề cập cho nhiệm vụ làm thưa thớt với tỷ lệ cắt tỉa cụ thể κ. Vì L_HS phụ thuộc vào độ lớn trọng số, nhưng không có phạm vi giá trị xác định cho trọng số, hàm mất mát L_HS của chúng tôi không được đảm bảo thích ứng hợp lý với W đã cho. Ví dụ, xem xét s = 1 cố định và tất cả trọng số trong W đều gần bằng không, gradient từ Eq. (3) dẫn đến gần như cùng giá trị cho mọi trọng số. Do đó, chúng tôi thích ứng s với trọng số nhỏ nhất |w_κ| sẽ còn lại sau cắt tỉa độ lớn, sao cho t'''(s·w_κ) = 0, là điểm uốn của t'. Theo căn chỉnh này, gradient trong Eq. (3) của các trọng số còn lại |w| ≥ |w_κ| được chuyển gần hơn đến 1 và được tăng cho trọng số |w| ≤ |w_κ|, trong khi tuân thủ gradient mượt từ trọng số còn lại đến bị loại bỏ. Hơn nữa, mẫu số trong Eq. (3) giảm theo thời gian, nếu nhiều trọng số trong W gần bằng không sau chính quy hóa tăng dần. Gradient cho các phân bố trọng số khác nhau của NN dựa trên HyperSparse được hiển thị trong Hình 1 và hình dung hành vi gradient được mô tả của chính quy hóa trọng số thích ứng.

4. Thí nghiệm
Phần này trình bày các thí nghiệm cho thấy phương pháp ART được đề xuất của chúng tôi vượt trội hơn các phương pháp có thể so sánh, đặc biệt trong chế độ thưa thớt cực cao. Thiết lập thí nghiệm của chúng tôi được mô tả trong Mục 4.1. Trong phần tiếp theo, chúng tôi cho thấy HyperSparse có tác động tích cực lớn đến thời gian tối ưu hóa và độ chính xác phân loại. Cải thiện này được giải thích bằng phân tích sự đánh đổi giữa khám phá và khai thác, phân bố gradient và trọng số trong Mục 4.3 và 4.4. Cuối cùng, chúng tôi phân tích và thảo luận về hành vi nén trong quá trình huấn luyện chính quy hóa và rút ra những hiểu biết sâu hơn về trọng số độ lớn cao nhất trong Mục 4.5.

4.1. Thiết lập thí nghiệm
Chúng tôi đánh giá ART trên các tập dữ liệu CIFAR-10/100 [19] và TinyImageNet [6] để bao gồm các độ phức tạp khác nhau, được đưa ra bởi

--- TRANG 5 ---
[Bảng 1 chứa kết quả độ chính xác phân loại cho các mạng NN thưa thớt với các tỷ lệ cắt tỉa κ khác nhau, so sánh phương pháp ART được đề xuất với các phương pháp khác trên CIFAR-10, CIFAR-100 và TinyImageNet với ResNet-32 và VGG-19]

[Bảng 2 cho thấy số epoch với chính quy hóa để có được mặt nạ cuối cùng, đánh giá cho nhiều tập dữ liệu, tô-pô mạng và tỷ lệ cắt tỉa κ]

số lượng nhãn lớp khác nhau. Hơn nữa, chúng tôi sử dụng các độ phức tạp mô hình khác nhau, trong đó ResNet-32 [11] là một mô hình đơn giản với 1.8 M tham số và VGG-19 [33] là một mô hình phức tạp với 20 M tham số. Lưu ý rằng chúng tôi sử dụng implementation được đưa ra trong [34]. Như đã giải thích trong Mục 3.2, chúng tôi nhóm quá trình huấn luyện thành 3 bước. Đầu tiên chúng tôi huấn luyện mô hình của mình trong 60 epoch đến hội tụ (bước 1), sử dụng tỷ lệ học không đổi 0.1. Trong bước chính quy hóa tiếp theo, chúng tôi khởi tạo chính quy hóa với λ_init = 5·10^-6, η = 1.05, và sử dụng cùng tỷ lệ học như được sử dụng trong huấn luyện trước. Bước tinh chỉnh (bước 3) tương tự như [34], vì chúng tôi huấn luyện trong 160 epoch trong CIFAR-10/100 và 300 epoch trên TinyImageNet, sử dụng tỷ lệ học 0.1 và áp dụng phân rã nhân với 0.1 tại 2/4 và 3/4 tổng số epoch. Chúng tôi cũng thích ứng kích thước batch là 64 và weight-decay là 10^-4. Tất cả thí nghiệm được lấy trung bình qua 5 lần chạy.

Chúng tôi so sánh phương pháp ART với SNIP [20], Grasp [42], SRatio [34], và LTH [9] tương tự như được thực hiện trong [34, 41]. Ngoài ra chúng tôi đánh giá IMP [16] và RigL [8] như các phương pháp cắt tỉa động. Để có thể so sánh, tất cả đối thủ cạnh tranh trong thí nghiệm của chúng tôi được huấn luyện với cùng thiết lập như được đưa ra trong bước tinh chỉnh. Để cải thiện hiệu suất của RigL, chúng tôi mở rộng thời gian huấn luyện thêm 360 epoch. Chi tiết thêm được đưa ra trong tài liệu bổ sung, Mục A.

4.2. Mức độ thưa thớt
Trong phần này, chúng tôi so sánh hiệu suất của ART với các phương pháp khác trên các mức độ thưa thớt khác nhau κ∈{90%,95%,98%,99%,99.5%,99.8%}, sử dụng các tập dữ liệu và mô hình khác nhau. Để chứng minh ưu điểm của hàm mất mát chính quy hóa mới của chúng tôi, chúng tôi bổ sung thay thế HyperSparse bằng L1[39] và L2[44]. Bảng 1 cho thấy độ chính xác kết quả với độ lệch chuẩn.

Phương pháp ART của chúng tôi kết hợp với HyperSparse vượt trội

--- TRANG 6 ---
hơn các phương pháp SNIP [20], Grasp [42], SRatio [34], LTH [9] và RigL [8] ở tất cả các mức độ thưa thớt. Xem xét độ thưa thớt cao 99%, 99.5% và 99.8%, tất cả đối thủ cạnh tranh đều giảm mạnh về độ chính xác, thậm chí đến ranh giới phân loại tối thiểu của dự đoán ngẫu nhiên cho SNIP và LTH sử dụng VGG-19. Tuy nhiên, ART có thể duy trì độ chính xác cao ngay cả ở mức độ thưa thớt cực cao. So sánh với các hàm mất mát chính quy hóa L1 và L2, hàm mất mát HyperSparse của chúng tôi đạt được độ chính xác cao hơn trong gần như tất cả các thiết lập và thậm chí giảm thiểu phương sai. Nếu chúng tôi bỏ qua bước Huấn luyện trước (bước 1) của ART, hiệu suất giảm nhẹ. Tuy nhiên, ART mà không có huấn luyện trước vẫn có kết quả tốt.

Hơn nữa, chúng tôi trình bày số epoch được huấn luyện cho giai đoạn chính quy hóa (bước 2) trong Bảng 2. Trong hầu hết tất cả các trường hợp, HyperSparse yêu cầu ít epoch hơn để kết thúc so với L1 và L2 và hội tụ nhanh hơn đến một mô hình thưa thớt hoạt động tốt. Như một khía cạnh thứ hai, ART thay đổi động độ dài huấn luyện theo mức độ thưa thớt, mô hình và độ phức tạp dữ liệu. Do đó, ART huấn luyện lâu hơn nếu yêu cầu độ thưa thớt cao hơn hoặc mô hình có nhiều tham số hơn và phức tạp hơn như VGG-19. So sánh hai tập dữ liệu CIFAR-10 và CIFAR-100, có cùng số lượng mẫu huấn luyện và do đó cùng số bước tối ưu hóa mỗi epoch, ART mở rộng độ dài huấn luyện cho vấn đề phân loại phức tạp hơn trong CIFAR-100.

ART huấn luyện mô hình trong 60 epoch ở huấn luyện trước (bước 1) và 160 epoch ở tinh chỉnh (bước 3). Xem xét độ dài huấn luyện động trong bước 2, các epoch của ART sử dụng L_HS tổng cộng từ 226.2 đến 301.2 epoch trung bình. So sánh, các phương pháp cắt tỉa lặp tốn kém về mặt tính toán hơn nhiều, vì mỗi mô hình được huấn luyện nhiều lần. Ví dụ, IMP [16] yêu cầu 860 epoch trên CIFAR-10/100 trong thí nghiệm của chúng tôi.

[Hình 2 và 3 cho thấy phân bố trọng số và hành vi hội tụ]

4.3. Gradient nhận biết khám phá và khai thác
Lịch trình huấn luyện của ART cho phép khám phá các tô-pô mới của mạng thưa thớt, trong khi nén mạng dày đặc vào các trọng số còn lại được khai thác để tối thiểu hóa hàm mất mát L_class. Để giảm sự đánh đổi giữa khám phá và khai thác, hàm mất mát chính quy hóa HyperSparse của chúng tôi phạt các trọng số nhỏ với chính quy hóa cao hơn và buộc hầu hết trọng số gần bằng không, trong khi bảo tồn độ lớn của trọng số còn lại sau khi cắt tỉa.

Để làm nổi bật hành vi có lợi của HyperSparse, phần này hình dung và phân tích gradient. Hình 1 cho thấy các giá trị và gradient tương ứng của tất cả trọng số, được sắp xếp theo độ lớn trọng số. Lưu ý rằng chúng tôi chỉ tập trung vào bước thứ hai của ART, nơi chính quy hóa được kết hợp. Epoch 0 đại diện cho epoch đầu tiên sử dụng chính quy hóa.

Trong hình phụ dưới, chúng tôi quan sát thấy gradient của HyperSparse đối với trọng số lớn hơn |w_κ| gần hơn với 0 so với trọng số nhỏ hơn. So sánh, L1 duy trì liên tục bằng 1 cho tất cả trọng số. Hiệu ứng tăng chính quy hóa của trọng số nhỏ mạnh hơn đối với mạng có nhiều trọng số gần bằng không và do đó khuếch đại theo thời gian, vì chính quy hóa tăng thu nhỏ độ lớn trọng số. Ví dụ, epoch 40 cho thấy gradient cao hơn cho trọng số nhỏ so với epoch 0, trong khi có nhiều trọng số với độ lớn thấp hơn. L_HS phụ thuộc tỷ lệ cắt tỉa κ tăng gradient cho trọng số nhỏ |w|<|w_κ| theo thời gian nhưng bảo tồn gradient thấp của trọng số lớn hơn |w|>|w_κ| xấp xỉ tại 0 để ưu tiên

--- TRANG 7 ---
khai thác. Trong quá trình tối ưu hóa, gradient vẫn mượt mà và tăng chậm cho trọng số nhỏ hơn, nhưng gần với |w_κ|. Điều này ưu tiên khám phá trong miền trọng số gần w_κ. Do đó, mô hình trở nên thưa thớt vốn có và hành vi chuyển đổi liên tục từ khám phá sang khai thác.

4.4. Sắp xếp lại trọng số
Chúng tôi sử dụng hàm mất mát chính quy hóa với đòn bẩy tăng dần để tìm một tập hợp trọng số hợp lý, còn lại sau khi cắt tỉa. Chúng tôi ngầm làm điều này bằng cách thu nhỏ trọng số nhỏ gần bằng không. Trong quá trình huấn luyện, trọng số được sắp xếp lại và do đó có thể thay đổi tư cách thành viên từ tập hợp bị cắt tỉa sang trọng số còn lại, và ngược lại. Chúng tôi phân tích quy trình sắp xếp lại trong Hình 2, cho thấy giao điểm của mặt nạ trung gian và cuối cùng qua tất cả các epoch, sử dụng các hàm mất mát chính quy hóa khác nhau trong ART. Mô hình được huấn luyện trước đến hội tụ mà không có chính quy hóa trong 60 epoch đầu tiên (bước 1) và với chính quy hóa trong các epoch tiếp theo (bước 2). Tinh chỉnh không được hình dung (bước 3). Sau huấn luyện trước, các trọng số cao nhất chỉ giao với mặt nạ cuối cùng thu được bởi L1 và L2 lên đến 20%, trong khi HyperSparse dẫn đến giao điểm khoảng 35%. Kết quả này cho thấy HyperSparse thay đổi ít tham số hơn trong khi sắp xếp lại trọng số, ngụ ý rằng nhiều cấu trúc từ mô hình dày đặc được khai thác. Nó cũng cho thấy HyperSparse có thời gian học ngắn hơn đáng kể so với L1 và L2. Các thanh ngang chỉ đến giao điểm trước epoch huấn luyện cuối cùng và cho thấy L1 và L2 chỉ giao với 60% và 50%, trong khi HyperSparse tiến rất gần đến mặt nạ cuối cùng với hơn 90% giao điểm. Điều này chỉ ra rằng HyperSparse tìm một tập hợp trọng số có giá trị cao ổn định hơn và giảm khám phá, vì mặt nạ có ít biến thiên trong các epoch cuối cùng. Nhiều kết quả cho các thiết lập huấn luyện khác được hiển thị trong tài liệu bổ sung, Mục B.

Hơn nữa, chúng tôi phân tích phân bố trọng số kết quả của phương pháp chúng tôi và so sánh với IMP [16] và SRatio [34]. Hình 3 cho thấy số lượng trọng số còn lại mỗi lớp cho ResNet-32 bao gồm ba mức độ tỷ lệ, kết thúc với lớp tuyến tính (LL). Mỗi mức độ tỷ lệ bao gồm bốn khối dư (RES), được kết nối bởi khối giảm mẫu (DS). Tô-pô cơ bản của ART và IMP trông tương tự, vì cả hai phương pháp đều cho thấy tỷ lệ giữ không đổi qua các khối dư. Hơn nữa, ART và IMP sử dụng nhiều tham số hơn trong các lớp giảm mẫu và tuyến tính. Chúng tôi kết luận rằng hai loại lớp này yêu cầu nhiều trọng số hơn và do đó quan trọng hơn đối với mô hình. Độ chính xác cao hơn được thảo luận trước đó gợi ý rằng phương pháp của chúng tôi khai thác các trọng số này tốt hơn. Để cho thấy rằng kết quả này cũng được thu được trên các tập dữ liệu, mô hình và mức độ thưa thớt khác, chúng tôi mô tả các phân bố trọng số thêm trong tài liệu bổ sung, Mục C và cho thấy số lượng tham số trong lớp tuyến tính giảm mạnh cho một tập hợp nhỏ các lớp trong CIFAR-10. Hơn nữa, phương pháp được so sánh

--- TRANG 8 ---
SRatio giả định rằng mạng thưa thớt phù hợp có thể được thu được bằng cách sử dụng tỷ lệ giữ được tạo thủ công mỗi lớp. Nó có tỷ lệ giữ giảm bậc hai có thể quan sát trong Hình 3. Như được hiển thị trong Bảng 1, phương pháp ART của chúng tôi hoạt động tốt hơn đáng kể so với SRatio và do đó chúng tôi suy ra rằng tỷ lệ giữ cố định có tác động bất lợi đến hiệu suất. Sắp xếp lại trọng số trong quá trình huấn luyện ưu tiên NN thưa thớt hoạt động tốt, đặc biệt trong chế độ thưa thớt cao.

4.5. Mạng nén cái gì đầu tiên?
Cùng với việc giới thiệu ART, chúng tôi đối mặt với câu hỏi về những mẫu nào được nén đầu tiên vào các trọng số lớn còn lại sau cắt tỉa độ lớn trong quá trình chính quy hóa. Câu hỏi này trái ngược với câu hỏi của Hooker "Mạng Nơ-ron Sâu Nén Quên gì?" [13] và thách thức giả định cơ bản của cắt tỉa độ lớn, cho rằng trọng số lớn là quan trọng nhất. Trong phần này, chúng tôi phân tích thứ tự thời gian về cách các mẫu được nén và giới thiệu chỉ số Vị trí Nén (CP) để xác định nó.

Theo phương pháp của chúng tôi, chính quy hóa bắt đầu tại epoch e_S và kết thúc tại e_E và do đó trọng số W có các trạng thái khác nhau W={W_e}^{e_E}_{e=e_S} trong quá trình huấn luyện. Chúng tôi đo độ chính xác cá nhân theo thời gian ψ^I đạt được bởi mạng thưa thớt cho một mẫu huấn luyện (x, y) trong S, được định nghĩa bởi

ψ^I(x, y, f, W) = |{W_e trong W | f(ν(W_e)⊙W_e, x) = y}| / (e_E - e_S). (4)

Sau khi tính toán độ chính xác cá nhân cho tất cả các mẫu Ψ = {ψ^I(x_n, y_n, f, W)}^N_{n=1} và sắp xếp Ψ theo thứ tự giảm dần, chỉ số CP(x, y, f, W) mô tả vị trí tương đối của ψ^I(x, y, f, W) trong sort(Ψ). Nói cách khác, các mẫu được nén sớm và phân loại đúng thu được CP thấp gần 0, và những cái được nén sau gần 1.

Chúng tôi tính toán chỉ số CP cho tất cả các mẫu trong CIFAR-10 trong quá trình huấn luyện của NN dày đặc, thưa thớt thấp và cao. Hành vi nén cho NN dày đặc được đo trong giai đoạn huấn luyện trước (e_S = 0 và e_E = 60) và cho NN thưa thớt trong giai đoạn chính quy hóa (e_S = 60 và e_E = e_max). Để cho thấy, những mẫu nào được nén đầu tiên vào các trọng số cao nhất còn lại, 5% mẫu với CP thấp nhất được hình dung trong Hình 4 trong không gian tiềm ẩn của khung CLIP nổi tiếng [29] được ánh xạ bởi t-SNE [40]. Như thường biết, mô hình dày đặc nén các mẫu dễ của tất cả các lớp trong giai đoạn đầu [17, 21], trong khi mô hình thưa thớt thấp đã mất một số. Trong chế độ thưa thớt cao không còn quy tắc quyết định phân biệt nào ở đầu huấn luyện, và các lớp còn lại được nén từng bước khi huấn luyện tiếp tục (xem tài liệu bổ sung, Mục E). Trong thí nghiệm của chúng tôi, chúng tôi đã thấy liên tục rằng có một thiên vị hướng đến lớp deer. Chúng tôi gọi hiệu ứng này là "thiên vị deer", phải được giảm với chính quy hóa. Thiên vị deer gợi ý rằng trọng số lớn trong NN dày đặc không mã hóa quy tắc quyết định cho tất cả các lớp.

Để định lượng kết quả trên, Bảng 3 cho thấy CP trung bình cho tất cả các mẫu thuộc về một lớp cụ thể. Bổ sung, chúng tôi chia các tập hợp lớp thành bốn tập con theo độ khó của chúng. Chúng tôi ước tính độ khó của một mẫu bằng cách đếm các lỗi nhãn của con người được thực hiện từ ba người chú thích con người được lấy từ CIFAR-N [43], ví dụ, 2 có nghĩa là hai trong ba người đã gán nhãn sai mẫu. Quan sát đầu tiên là sự tách biệt các lớp được đề cập ở trên được xác nhận, vì các giá trị CP tương tự trong NN dày đặc, nhưng phân kỳ trong NN thưa thớt. Trong chế độ thưa thớt cao, thiên vị deer tồn tại trước khi các mẫu đầu tiên của các lớp khác được nén. Các lớp horse và airplane chỉ được bao gồm ở cuối huấn luyện. Quan sát thứ hai là, trong một tập hợp đóng các mẫu thuộc về một lớp, các mẫu khó được nén sau. Bản chất này tương tự như quá trình huấn luyện của NN dày đặc. Chi tiết triển khai và kết quả chi tiết hơn có sẵn trong tài liệu bổ sung, Mục E.

5. Kết luận
Công trình của chúng tôi trình bày Huấn luyện Chính quy hóa Thích ứng (ART), một phương pháp sử dụng chính quy hóa để có được mạng nơ-ron thưa thớt. Chính quy hóa được khuếch đại liên tục và được sử dụng để thu nhỏ hầu hết độ lớn trọng số gần bằng không. Chúng tôi giới thiệu hàm mất mát chính quy hóa mới HyperSparse tạo thưa thớt vốn có trong khi duy trì sự đánh đổi cân bằng tốt giữa khám phá các tô-pô thưa thớt mới và khai thác trọng số còn lại sau khi cắt tỉa. Các thí nghiệm mở rộng trên CIFAR và TinyImageNet cho thấy khung mới của chúng tôi vượt trội hơn các đối thủ cạnh tranh học thưa thớt. HyperSparse vượt trội hơn các hàm mất mát chính quy hóa tiêu chuẩn và dẫn đến những cải thiện hiệu suất ấn tượng trong chế độ thưa thớt cực cao và nhanh hơn nhiều. Các điều tra bổ sung cung cấp hiểu biết mới về phân bố trọng số trong quá trình nén mạng và về các mẫu được mã hóa trong trọng số có giá trị cao.

Nhìn chung, công trình này cung cấp hiểu biết mới về mạng nơ-ron thưa thớt và giúp phát triển học máy bền vững bằng cách giảm độ phức tạp mạng nơ-ron.

6. Lời cảm ơn
Công trình này được hỗ trợ bởi Bộ Giáo dục và Nghiên cứu Liên bang (BMBF), Đức dưới dự án trung tâm dịch vụ AI KISSKI (số hiệu 01IS22093C), Deutsche Forschungsgemeinschaft (DFG) dưới Chiến lược Xuất sắc của Đức trong Cụm Xuất sắc PhoenixD (EXC 2122), và bởi Bộ Môi trường, Bảo tồn Thiên nhiên, An toàn Hạt nhân và Bảo vệ Người tiêu dùng Liên bang, Đức dưới dự án GreenAutoML4FAS (số hiệu 67KI32007A).

--- TRANG 9 ---
Tài liệu tham khảo
[1] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, và Simon Lacoste-Julien. A closer look at memorization in deep networks. Trong International Conference on Machine Learning (ICML), 2017. 5

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Language models are few-shot learners. Trong Conference on Neural Information Processing Systems (NeurIPS), 2020. 1

[3] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, và Zhangyang Wang. Chasing sparsity in vision transformers: An end-to-end exploration. Trong Conference on Neural Information Processing Systems (NeurIPS), 2021. 2, 3

[4] Tianlong Chen, Zhenyu Zhang, pengjun wang, Santosh Balachandra, Haoyu Ma, Zehao Wang, và Zhangyang Wang. Sparsity winning twice: Better robust generalization from more efficient training. Trong International Conference on Learning Representations (ICLR), 2022. 2, 3

[5] Yuren Cong, Michael Ying Yang, và Rosenhahn Bodo. Reltr: Relation transformer for scene graph generation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2023. 1

[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 2, 4

[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Trong International Conference on Learning Representations (ICLR), 2021. 1, 4

[8] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, và Erich Elsen. Rigging the lottery: Making all tickets winners. Trong International Conference on Machine Learning (ICML), 2020. 2, 5, 6

[9] Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. Trong International Conference on Learning Representations (ICLR), 2018. 1, 2, 3, 5, 6

[10] Song Han, Jeff Pool, John Tran, và William Dally. Learning both weights and connections for efficient neural network. Trong Conference on Neural Information Processing Systems (NeurIPS), 2015. 1, 2, 3

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2, 5

[12] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, và Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. Journal of Machine Learning Research (JMLR), 2021. 1, 2

[13] Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, và Andrea Frome. What do compressed deep neural networks forget? Trong arXiv:1911.05248, 2019. 3, 8

[14] Ajay Kumar Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding, và Zhangyang Wang. Training your sparse neural network better with any mask. Trong International Conference on Machine Learning (ICML), 2022. 2

[15] Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, và Erich Elsen. Top-kast: Top-k always sparse training. Trong Conference on Neural Information Processing Systems (NeurIPS), 2020. 2

[16] Karolina Gintare Jonathan Frankle, Dziugaite, Daniel Roy, và Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. Trong International Conference on Machine Learning (ICML), 2020. 1, 2, 3, 5, 6, 7, 8

[17] Timo Kaiser, Lukas Ehmann, Christoph Reinders, và Bodo Rosenhahn. Blind knowledge distillation for robust image classification. Trong arXiv:2211.11355, 2022. 3, 8, 5

[18] Timo Kaiser, Christoph Reinders, và Bodo Rosenhahn. Compensation learning in semantic segmentation. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2023. 3

[19] Alex Krizhevsky và Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. 2, 4

[20] Namhoon Lee, Thalaiyasingam Ajanthan, và Philip Torr. Snip: Single-shot network pruning based on connection sensitivity. Trong International Conference on Learning Representations (ICLR), 2018. 1, 2, 5, 6

[21] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, và Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. Trong Conference on Neural Information Processing Systems (NeurIPS), 2020. 8, 5

[22] Shiwei Liu, Tim Van der Lee, Anil Yaman, Zahra Atashgahi, Davide Ferraro, Ghada Sokar, Mykola Pechenizkiy, và Decebal Constantin Mocanu. Topological insights into sparse neural networks. Trong European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD), 2021. 2

[23] Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, và Mykola Pechenizkiy. Do we actually need dense over-parameterization? in-time over-parameterization in sparse training. Trong International Conference on Machine Learning (ICML), 2021. 2

[24] Christos Louizos, Max Welling, và Diederik P. Kingma. Learning sparse neural networks through l0 regularization. Trong International Conference on Learning Representations (ICLR), 2018. 2

[25] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, và Antonio Liotta. Scalable training of artificial neural networks with adaptive

--- TRANG 10 ---
sparse connectivity inspired by network science. Nature communications, 2018. 2

[26] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, và Jan Kautz. Pruning convolutional neural networks for resource efficient inference. Trong International Conference on Learning Representations (ICLR), 2017. 2, 3

[27] Sharan Narang, Greg Diamos, Shubho Sengupta, và Erich Elsen. Exploring sparsity in recurrent neural networks. Trong International Conference on Learning Representations (ICLR), 2017. 2

[28] Mansheej Paul, Brett W Larsen, Surya Ganguli, Jonathan Frankle, và Gintare Karolina Dziugaite. Lottery tickets on a data diet: Finding initializations with sparse trainable networks. Trong Conference on Neural Information Processing Systems (NeurIPS), 2022. 3

[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, và Ilya Sutskever. Learning transferable visual models from natural language supervision. Trong International Conference on Machine Learning (ICML), 2021. 8, 4

[30] Keitaro Sakamoto và Issei Sato. Analyzing lottery ticket hypothesis from PAC-bayesian theory perspective. Trong Conference on Neural Information Processing Systems (NeurIPS), 2022. 3

[31] Pedro Savarese, Hugo Silva, và Michael Maire. Winning the lottery with continuous sparsification. Trong Conference on Neural Information Processing Systems (NeurIPS), 2020. 2

[32] Jonathan Schwarz, Siddhant Jayakumar, Razvan Pascanu, Peter E Latham, và Yee Teh. Powerpropagation: A sparsity inducing weight reparameterisation. Trong Conference on Neural Information Processing Systems (NeurIPS), 2021. 2

[33] Karen Simonyan và Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. Trong International Conference on Learning Representations (ICLR), 2015. 2, 5

[34] Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, và Jason D Lee. Sanity-checking pruning methods: Random tickets can win the jackpot. Trong Conference on Neural Information Processing Systems (NeurIPS), 2020. 1, 2, 5, 6, 7, 8

[35] Mukund Varma T, Xuxi Chen, Zhenyu Zhang, Tianlong Chen, Subhashini Venugopalan, và Zhangyang Wang. Sparse winning tickets are data-efficient image recognizers. Trong Conference on Neural Information Processing Systems (NeurIPS), 2022. 3

[36] Kai Sheng Tai, Taipeng Tian, và Ser-Nam Lim. Spartan: Differentiable sparsity via regularized transportation. Trong Conference on Neural Information Processing Systems (NeurIPS), 2022. 2

[37] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, và Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. Trong Conference on Neural Information Processing Systems (NeurIPS), 2020. 2

[38] Bodo Rosenhahn Thomas Norrenbrock, Marco Rudolph. Take 5: Interpretable image classification with a handful of features. Trong Conference on Neural Information Processing Systems, Workshop Progress and Challenges in Building Trustworthy Embodied AI (NeurIPSW), 2022. 1, 3

[39] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the royal statistical society series b-methodological, 1996. 2, 3, 5

[40] Laurens Van der Maaten và Geoffrey Hinton.. Visualizing data using t-sne. Journal of machine learning research (JMLR), 2008. 8, 4

[41] Vinay Kumar Verma, Nikhil Mehta, Shijing Si, Ricardo Henao, và Lawrence Carin. Pushing the efficiency limit using structured sparse convolutions. Trong Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2023. 5

[42] Chaoqi Wang, Guodong Zhang, và Roger Grosse. Picking winning tickets before training by preserving gradient flow. Trong International Conference on Learning Representations (ICLR), 2019. 1, 2, 5, 6

[43] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, và Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. Trong International Conference on Learning Representations (ICLR), 2022. 8, 4

[44] Guodong Zhang, Chaoqi Wang, Bowen Xu, và Roger Grosse. Three mechanisms of weight decay regularization, 2018. 2, 3, 5

--- TRANG 11 ---
Tài liệu Bổ sung
Tài liệu này cung cấp tài liệu bổ sung cho bài báo Mạng Nơ-ron HyperSparse: Chuyển từ Khám phá sang Khai thác thông qua Chính quy hóa Thích ứng. Đầu tiên, Mục A cung cấp thông tin chi tiết về việc triển khai phương pháp của chúng tôi. Tiếp theo, Mục B trình bày kết quả chi tiết hơn về giao điểm của trọng số lớn nhất trong quá trình huấn luyện và mặt nạ cắt tỉa cuối cùng. Phân bố trọng số sau khi huấn luyện với phương pháp được giới thiệu của chúng tôi được hiển thị trong bài báo chính được phân tích cho một tập hợp cấu hình rộng hơn trong Mục C. Hơn nữa, Mục D và Mục E chi tiết hóa gradient và hành vi nén trong quá trình chính quy hóa được trình bày trong bài báo chính.

A. Thiết lập thí nghiệm chi tiết
Như được mô tả trong [34], chúng tôi đánh giá phương pháp của mình trên các tập dữ liệu CIFAR-10/100 [19] và TinyImageNet [6] với các mô hình ResNet-32 [11] và VGG-19 [33]. CIFAR-10 là một tập dữ liệu cho nhiệm vụ phân loại với 50.000 mẫu huấn luyện và 10.000 mẫu xác thực trên hình ảnh màu 32x32 được gán nhãn với 10 lớp. Tương ứng CIFAR-100 có 100 lớp và cùng số lượng mẫu. Tập dữ liệu TinyImageNet bao gồm 100.000 mẫu huấn luyện và 10.000 mẫu xác thực với kích thước hình ảnh 64x64, trong đó các mẫu được gán nhãn với một tập hợp 200 lớp.

Như được thực hiện trong [34], chúng tôi huấn luyện các mô hình của mình trong 160 epoch trên CIFAR-10/100 và 300 epoch trên TinyImageNet sử dụng optimizer SGD, với tỷ lệ học ban đầu 0.1 và kích thước batch 64. Chúng tôi phân rã tỷ lệ học với hệ số 0.1 tại epoch 2/4 và 3/4 tổng số epoch. Weight decay được đặt thành 1·10^-4. Trong thí nghiệm của chúng tôi, tất cả kết quả được lấy trung bình qua 5 lần chạy.

Trong implementation gốc của SmartRatio [34], trọng số trong lớp tuyến tính cuối cùng được cắt tỉa với tỷ lệ cắt tỉa cố định 70%. Do đó, quá nhiều trọng số còn lại khi huấn luyện trên ResNet-32 với tỷ lệ cắt tỉa 99.8% trên tập dữ liệu CIFAR-100 và TinyImageNet. Vì lý do này, chúng tôi thay đổi tỷ lệ cắt tỉa trong lớp tuyến tính thành 90% chỉ cho hai thiết lập huấn luyện này. Các phương pháp SNIP [20], GraSP [42], SmartRatio [34], và LTH [9] đề xuất quy tắc để có được mặt nạ cố định. Mặt nạ này được áp dụng cho trọng số mô hình trước khi huấn luyện. Ngược lại, IMP [16] lặp lại huấn luyện một mô hình đến epoch T và cắt tỉa 20% trọng số còn lại cho đến khi đạt được tỷ lệ cắt tỉa mong muốn. Sau mỗi lần lặp, trọng số và tỷ lệ học được đặt lại về epoch k và huấn luyện lại đến epoch T. Để có thể so sánh, chúng tôi định nghĩa k = 20 và T = 160 cho CIFAR-10/100 cũng như k = 40 và T = 300 cho TinyImageNet. Như được mô tả trong [8], RigL hoạt động tốt hơn với thời gian huấn luyện dài hơn. Vì lý do này, chúng tôi mở rộng thời gian tối ưu hóa của phương pháp RigL phân bố đều bằng cách huấn luyện trong 360 epoch với tỷ lệ học 0.1, theo sau là bước tinh chỉnh 160 epoch trên CIFAR-10/100 và 300 epoch trên TinyImageNet. Bước tinh chỉnh bằng với ART. Tất cả các siêu tham số khác của RigL được áp dụng từ [8].

Phương pháp ART được đề xuất của chúng tôi, được mô tả trong Mục 3.2 trong bài báo chính, bao gồm ba bước. Trong bước đầu tiên, chúng tôi huấn luyện mô hình của mình đến hội tụ trong 60 epoch sử dụng tỷ lệ học cố định 0.1. Tiếp theo chúng tôi kích hoạt số hạng chính quy hóa được sử dụng, với tỷ lệ khởi tạo nhỏ λ_init = 5·10^-6 và hệ số tăng η = 1.05. Để giảm nhiễu trong việc chọn mô hình cắt tỉa tốt nhất, chúng tôi lấy trung bình độ chính xác ψ(ν(W_e)⊙W_e) qua epoch (e-1, e, e+1), trong đó e mô tả epoch hiện tại và ν ký hiệu cắt tỉa độ lớn thu được mặt nạ nhị phân. Hai bước đầu tiên được sử dụng để có được trọng số và mặt nạ cho tinh chỉnh. Trong quá trình tinh chỉnh, chúng tôi sử dụng lịch trình huấn luyện được mô tả ở trên như được thực hiện trong [34].

B. Giao điểm mặt nạ trong Huấn luyện Chính quy hóa
Trong phần này chúng tôi hiển thị kết quả thêm của thí nghiệm đo giao điểm mặt nạ qua epoch e từ Mục 4.4 trong bài báo chính. Chúng tôi đo sự chồng chéo tương đối giữa trọng số có độ lớn cao nhất tại epoch e và mặt nạ cuối cùng trong các thiết lập khác nhau với các mô hình, tập dữ liệu, hàm mất mát chính quy hóa và tỷ lệ cắt tỉa khác nhau. Do đó, Bảng 2 hiển thị các điểm quan trọng của giao điểm ở cuối huấn luyện trước (epoch 60) và một epoch trước khi tìm thấy mặt nạ cuối cùng (e = K-1). Chúng tôi quan sát thấy hàm mất mát chính quy hóa L_HS của chúng tôi có giao điểm cao hơn trong gần như tất cả các thiết lập tại epoch 60 và epoch K-1 so với hàm mất mát L1 và L2. Điều này chỉ ra rằng hàm mất mát HyperSparse của chúng tôi thay đổi ít tham số hơn trong khi sắp xếp lại trọng số từ còn lại sang bị cắt tỉa và ngược lại.

Ngoài ra, Bảng 2 trình bày tổng số epoch huấn luyện để có được mặt nạ cuối cùng (bao gồm bước 1 và bước 2). Nó cho thấy hàm mất mát HyperSparse của chúng tôi cần ít epoch hơn để kết thúc trong gần như tất cả các thiết lập. Vì ART kết thúc, nếu mô hình cắt tỉa tốt nhất vượt trội hơn mô hình chưa cắt tỉa tại epoch e, chúng tôi suy ra rằng L_HS tạo ra một mạng thưa thớt hoạt động tốt nhanh hơn so với hàm mất mát L1 và L2.

C. Phân bố trọng số
Trong phần này, chúng tôi hiển thị thí nghiệm thêm về phân bố trọng số mỗi lớp trong mặt nạ cuối cùng, như được đánh giá trong Mục 4.4 trong bài báo chính. Chúng tôi phân tích các mặt nạ kết quả cho tập dữ liệu CIFAR-10 và CIFAR-100, tỷ lệ cắt tỉa κ∈{90%, 98%, 99.5%} cũng như cho mô hình ResNet-32 và VGG-19. Phân bố trọng số thu được bởi các phương pháp IMP [16], SRatio [34] và ART sử dụng hàm mất mát HyperSparse được phân tích. Tất cả giá trị được lấy trung bình qua 5 lần chạy.

Trong Hình 2, chúng tôi hiển thị phân bố trọng số kết quả cho ResNet-32. Lưu ý rằng mô hình được nhóm thành ba khối dư (RES), hai khối giảm mẫu (DS) và một lớp tuyến tính (LL). Chúng tôi quan sát thấy ART + L_HS và IMP có phân bố trọng số có thể so sánh. Cả hai phương pháp đều hiển thị phân bố tương đối không đổi trong các lớp dư, ngoại trừ lớp cuối cùng. Lớp cuối cùng này có số lượng trọng số giảm, đặc biệt trong nhiệm vụ đơn giản hơn được đưa ra trong CIFAR-10. So sánh, SRatio sử dụng tỷ lệ giữ cố định theo cách giảm bậc hai và do đó phân bố trọng số không phụ thuộc vào dữ liệu. Vì ART và IMP vượt trội hơn SRatio rất nhiều về độ chính xác (Bảng 1 trong bài báo chính), quy tắc được tạo thủ công này có tác động bất lợi đến hiệu suất. Hơn nữa, chúng tôi quan sát một số lượng tương đối cao trọng số trong lớp giảm mẫu cho ART và IMP, cho thấy các lớp này quan trọng hơn.

Thêm vào đó, chúng tôi trình bày phân bố trọng số cho VGG-19 trong Hình 3. Chúng tôi quan sát thấy lớp xung quanh chỉ số 5 có nhiều trọng số hơn cho ART và IMP. Gần như không có trọng số nào còn lại trong lớp có chỉ số cao hơn 10, ngoại trừ lớp tuyến tính cuối cùng. Xem xét độ thưa thớt tăng, phân bố trọng số được chuyển về phía các lớp trước với chỉ số thấp. Chúng tôi suy ra rằng trong chế độ thưa thớt cao hơn, trọng số trong lớp trước quan trọng hơn trong VGG-19. Quy tắc được tạo thủ công của SRatio hiển thị phân bố trọng số tương đối phẳng. Nhìn chung, số lượng trọng số trong lớp tuyến tính tăng cho CIFAR-100, do số lượng lớp tăng so với CIFAR-10 trong ResNet-32 và cho VGG-19.

D. Phân tích gradient HyperSparse
Trong phần này, chúng tôi phân tích gradient của hàm mất mát chính quy hóa HyperSparse của chúng tôi đối với trọng số mô hình w trong W. Giả định rằng trọng số quan trọng có độ lớn lớn, chúng tôi cho thấy HyperSparse giảm xuống không chính quy hóa cho các giá trị quan trọng và phát triển thành một hình phạt mạnh cho các giá trị không quan trọng. Hành vi này cho phép khai thác trong tập hợp các trọng số quan trọng còn lại sau cắt tỉa độ lớn. Hơn nữa, chúng tôi cho thấy hàm mất mát của chúng tôi đảm bảo một chuyển đổi mượt mà trong gradient giữa trọng số không quan trọng và quan trọng, sao cho khám phá trong tập hợp trọng số không quan trọng có thể trong quá trình huấn luyện.

Gradient. Hàm mất mát của chúng tôi phát triển tính thưa thớt và thích ứng với độ lớn trọng số bằng cách sử dụng tính phi tuyến của hàm Hyperbolic Tangent

tanh(x) = (e^x - e^-x)/(e^x + e^-x) trong (-1,1), (2)

đây là lý do cho tên HyperSparse. Cực đại của đạo hàm của tanh là 1 tại x = 0 và mạnh biến mất gần bằng không cho các giá trị lớn:

arg max_x d tanh(x)/dx = 0 và lim_{x→±∞} d tanh(x)/dx = 0. (3)

Trong bài báo này, hàm Hyperbolic Tangent của một độ lớn tanh(|·|) được ký hiệu bởi t(·) để đơn giản.

Vì tính đầy đủ, chúng tôi tóm tắt lại định nghĩa của HyperSparse từ Eq. (2) trong bài báo chính:

L_HS(W) = (1/A) Σ|w_i| Σt(s·w_j) - Σ|w_i|
với A := Σt(s·w) và với mọi w trong W: dA/dw = 0, (4)

và muốn lưu ý lại, A ký hiệu một số hạng hằng số giả được coi là một hằng số trong tính toán gradient, và s là một hệ số tỷ lệ được thảo luận ở cuối phần này. Trong phần này, ký hiệu tổng như Σ|W|_{i=1} sẽ được đơn giản hóa bởi Σw_i hoặc bởi Σw nếu w là duy nhất. Hơn nữa, chúng tôi sẽ bỏ qua các khai báo về tư cách thành viên tập hợp như w_i trong W và nói rằng mọi w đều trong tập hợp trọng số mô hình W. Cũng phạm vi của các công thức được định nghĩa nhất quán là với mọi w trong W.

Với các ký hiệu và đơn giản hóa này, đạo hàm của Eq.(4) w.r.t. đến một trọng số w_i có thể được định nghĩa như sau:

dL_HS/dw_i = d/dw_i [|w_i|Σw_j t(s·w_j)]/A + d/dw_i [Σ_{w_n≠w_i} |w_n|Σw_j t(s·w_j)]/A - sign(w_i)
= sign(w_i)·|w_i|·t'(s·w_i) + Σw_j t(s·w_j)/A + sign(w_i)·Σ_{w_n≠w_i} |w_n|·t'(s·w_i)/A - sign(w_i)
= sign(w_i)·Σw_j t(s·w_j) + Σw_n |w_n|·t'(s·w_i)/A - 1
= sign(w_i)·t'(s·w_i)·Σw_n |w_n|/Σw_j t(s·w_j). (5)

Gradient bao gồm một số hạng phụ thuộc vào phân bố trọng số trong W và đạo hàm t' = dt/dw_i tại độ lớn trọng số được xem xét |w_i| được tỷ lệ với s. Hành vi của HyperSparse có thể được giải thích với gradient cho độ lớn rất nhỏ và rất lớn: Đối với độ lớn lớn |w_i| ≫ 0, đạo hàm trong Eq. (5) sụp đổ thành

dL_HS/dw_i|_{s·w_i≫0} ≈ sign(w_i)·0 = 0 (6)

thực tế là không có chính quy hóa. Đối với các giá trị rất nhỏ w_i ≈ 0, đạo hàm

dL_HS/dw_i|_{s·w_i≈0} ≈ sign(w_i)·Σw_n |w_n|/Σw_j t(s·w_j) (7)

lớn hơn và tăng, nếu trọng số trong W được tách biệt rõ ràng thành hai tập hợp quan trọng (độ lớn lớn) và không quan trọng (độ lớn thấp). Gradient của trọng số không được gán cho một trong những tập hợp đó nằm giữa Eq. (6) và (7) và do đó cho phép khám phá dễ dàng hơn của những trọng số đó trong quá trình huấn luyện.

Căn chỉnh với s. Trong định nghĩa của HyperSparse, hệ số tỷ lệ s căn chỉnh hàm mất mát với phân bố trọng số thực tế. Mục đích là các trọng số |w|>|w_κ| không bị hoặc bị phạt nhẹ và |w|<|w_κ| bị phạt mạnh. Một phân bố trọng số không cần phải được căn chỉnh với đạo hàm của hàm Hyperbolic Tangent sao cho trọng số lớn được ánh xạ gần 0 và trọng số nhỏ gần 1. Để khắc phục điều này, chúng tôi căn chỉnh W bằng cách tỷ lệ w_κ với s sao cho nó nằm trên điểm uốn của gradient. Hệ số tỷ lệ mong muốn có thể được rút ra bởi

t'''(s·|w_κ| ≈ 0.6585) = 0 (8)

và đặt s = 0.6586/|w_κ|. Các hệ số tỷ lệ lớn s dẫn đến phân bố gradient tăng vọt tại trọng số w_κ hướng về trọng số có độ lớn thấp. Ví dụ có thể tìm thấy trong Hình 1 trong bài báo chính.

E. Nén có thể diễn giải
Chương này thảo luận về quá trình nén kiến thức nén các mẫu từ một mạng dày đặc được huấn luyện trước thành một mạng thưa thớt bao gồm tập hợp 1-κ trọng số cao nhất, trong đó κ ký hiệu tỷ lệ cắt tỉa mong muốn. Phần đầu tiên trình bày tập dữ liệu CIFAR-N [43] được sử dụng để phân tích hành vi nén trong Mục 4.5. Chúng tôi hiển thị cách nó liên quan đến CIFAR [19] và cách chúng tôi hình dung phân bố nhãn với khung CLIP hiện đại [29]. Sau đó chúng tôi chi tiết hóa chỉ số Vị trí Nén được giới thiệu. Vì tính đầy đủ, chúng tôi cuối cùng trình bày và thảo luận các hình và bảng hiển thị kết quả cho các thiết lập bổ sung không thể được trình bày trong bài báo chính do thiếu không gian.

CIFAR-N
Để phân tích lỗi nhãn giống con người và cung cấp nhiễu nhãn thế giới thực cho các nhà nghiên cứu, Wei et al. giới thiệu

[Các bảng và hình chi tiết được tiếp tục mô tả trong các trang còn lại của tài liệu bổ sung...]
