# 2212.12770.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2212.12770.pdf
# Kích thước tệp: 2320324 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
TẠP CHÍ IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, TẬP 00, SỐ 0, THÁNG 2020 1
COLT: Vé Số Trùng Lặp Tuần Hoàn cho
Tỉa Nhanh Hơn của Mạng Nơ-ron Tích Chập
Md. Ismail Hossain, Mohammed Rakib, M. M. Lutfe Elahi, Nabeel Mohammed và Shafin Rahman
Tóm tắt —Tỉa đề cập đến việc loại bỏ các trọng số tầm thường
khỏi mạng nơ-ron. Các mạng con trong một mô hình có quá nhiều
tham số được tạo ra sau khi tỉa thường được gọi là vé số. Nghiên cứu này
nhằm tạo ra các vé số thắng cuộc từ một tập hợp các vé số có thể đạt được
độ chính xác tương tự như mạng gốc chưa được tỉa. Chúng tôi giới thiệu
một vé thắng cuộc mới gọi là Vé Số Trùng Lặp Tuần Hoàn (COLT)
bằng cách chia dữ liệu và huấn luyện lại tuần hoàn mạng đã tỉa từ đầu.
Chúng tôi áp dụng một thuật toán tỉa tuần hoàn chỉ giữ lại các trọng số
trùng lặp của các mô hình tỉa khác nhau được huấn luyện trên các phân
đoạn dữ liệu khác nhau. Kết quả của chúng tôi chứng minh rằng COLT
có thể đạt được độ chính xác tương tự (thu được bởi mô hình chưa tỉa)
trong khi duy trì độ thưa thớt cao. Dựa trên các nhiệm vụ nhận dạng và
phát hiện đối tượng, chúng tôi cho thấy rằng độ chính xác của COLT ngang
bằng với các vé thắng cuộc của Giả thuyết Vé Số (LTH) và đôi khi còn
tốt hơn. Hơn nữa, COLT có thể được tạo ra bằng ít vòng lặp hơn so với
các vé được tạo bởi phương pháp Tỉa Độ Lớn Lặp Lại (IMP) phổ biến.
Ngoài ra, chúng tôi cũng nhận thấy rằng COLT được tạo trên các tập dữ
liệu lớn có thể được chuyển sang các tập nhỏ mà không ảnh hưởng đến
hiệu suất, thể hiện khả năng tổng quát hóa của nó. Chúng tôi tiến hành
tất cả các thí nghiệm trên các tập dữ liệu Cifar-10, Cifar-100, TinyImageNet
và ImageNet và báo cáo hiệu suất vượt trội so với các phương pháp tiên
tiến. Mã nguồn có sẵn tại: https://github.
com/ismail31416/COLT
Tuyên bố Tác động — Sự xuất hiện của các mô hình học sâu quy mô lớn
vượt trội hơn các hệ thống truyền thống trong việc giải quyết nhiều vấn đề
thực tế. Thành công của nó đôi khi thậm chí còn vượt qua hiệu suất của
con người trong một số nhiệm vụ. Tuy nhiên, điểm nghẽn chính vẫn là
sự gia tăng chi phí tính toán. Để giảm thiểu điều này, các nhà nghiên cứu
bắt đầu tìm kiếm các giải pháp thay thế quy mô nhỏ cho các mô hình quy
mô lớn. Một hướng quan trọng là tỉa mô hình, nhằm tỉa mô hình học sâu
hiện có mà không ảnh hưởng đến hiệu suất. Các phương pháp hiện có trong
dòng nghiên cứu này thực hiện tỉa lặp lại cùng một mô hình trong nhiều
vòng tỉa. Trong bài báo này, chúng tôi đề xuất một ý tưởng mới để giảm
thiểu số vòng tỉa trong khi vẫn giữ độ chính xác của mô hình chưa tỉa.
Nói cách khác, bằng cách sử dụng thuật toán COLT của chúng tôi, các mô
hình có thể được tỉa nhanh hơn, giảm dấu chân carbon và làm cho thuật toán
của chúng tôi thân thiện hơn với môi trường. Hơn nữa, phù hợp với tài liệu,
chúng tôi chứng minh rằng mạng con tỉa được tính toán bằng một tập dữ
liệu có thể được sử dụng cho các tập dữ liệu khác nhau mà không cần tỉa
đặc thù cho tập dữ liệu. Điều này sẽ giúp xây dựng một mạng con tỉa cho
một miền mới một cách nhanh chóng. Nghiên cứu này sẽ mở ra một triển
vọng nghiên cứu mới trong việc tìm kiếm một chiến lược tỉa mới cho các
mạng nơ-ron tích chập.
Thuật ngữ Chỉ mục —Nén Mô hình, Tỉa, Mạng Thưa Thớt
I. GIỚI THIỆU
Tỉa mạng nơ-ron đề cập đến việc loại bỏ các trọng số không
cần thiết khỏi mạng nơ-ron. Vấn đề này đã được nghiên cứu
rộng rãi từ những năm 1980 để làm cho mạng thưa thớt và hiệu
quả mà không ảnh hưởng xấu đến hiệu suất [1]–[6]. Do đó,
tỉa có thể nén kích thước của mô hình [3], [7] và làm cho nó
ít tiêu tốn điện năng hơn [8]–[10], có thể cho phép suy luận
hiệu quả hơn. Với sự gia tăng của mạng nơ-ron sâu (DNN),
tỉa đã chứng kiến sự quan tâm trở lại. Nhiều nghiên cứu đã
được tiến hành nhằm làm cho DNN thưa thớt trong lĩnh vực
hình ảnh và tính toán thị giác [3], [4], [8], [11]–[16]. Tỉa đã
liên tục cho thấy hiệu quả trong việc giảm nhu cầu không gian
và thời gian cao của các nhiệm vụ phân loại [17], [18] và phát
hiện đối tượng [19], [20]. Trong bài báo này, chúng tôi đề xuất
một chiến lược tỉa mạng nơ-ron mới có thể đạt được nén cao
hơn, duy trì hội tụ nhanh hơn và hiệu suất tốt.

Một kỹ thuật tỉa, cụ thể là giả thuyết vé số (LTH) [21], gần
đây đã thu hút nhiều sự chú ý từ cộng đồng nghiên cứu. Đây
là một phương pháp tỉa dựa trên trọng số, trong đó các trọng
số có độ lớn thấp nhất được tỉa lặp lại sau khi huấn luyện. Tuy
nhiên, khía cạnh phân biệt LTH với các phương pháp khác là
việc đặt lại trọng số về trạng thái ban đầu (còn gọi là tua lại
trọng số) sau khi tỉa. Vì nhiều vòng lặp của chu trình huấn
luyện-tỉa-tua lại này tạo thành cơ sở của thuật toán tỉa này,
quá trình này được gọi là tỉa độ lớn lặp lại (IMP). Theo [21],
một mạng con (còn gọi là vé) được tạo bằng IMP có thể đạt
độ thưa thớt 90% hoặc hơn mà không ảnh hưởng đến hiệu
suất. Vì vậy, một khi vé thưa thớt được tạo ra, nó có thể được
huấn luyện trên tập dữ liệu từ đầu và đạt được độ chính xác
bằng hoặc lớn hơn mạng gốc. Các công trình sau đó như [22]
chứng minh rằng những vé này được tạo từ một tập dữ liệu
thậm chí có thể được chuyển sang tập khác và đạt được độ
chính xác có thể so sánh với mạng gốc. Cụ thể, họ cho thấy
rằng các mạng con được tạo bởi IMP độc lập với cả tập dữ
liệu và bộ tối ưu hóa. Xem xét khả năng chuyển đổi tập dữ
liệu và tính độc lập với bộ tối ưu hóa của các vé LTH như vậy,
chúng tôi đề xuất một loại vé số mới có thể đạt được nén mô
hình tương tự (tức là tỉa mạng) nhưng nhanh hơn về tính toán
(ít vòng tỉa hơn) so với phương pháp dựa trên LTH (xem Hình 1).

Tương tự như IMP của phương pháp tỉa dựa trên LTH, thuật
toán tỉa của chúng tôi cũng sử dụng IMP. Chúng tôi đầu tiên
chia tập dữ liệu huấn luyện thành hai phân vùng theo lớp
(Phân vùng 1 và 2). Mỗi phần chứa các lớp không trùng lặp
và các thể hiện của chúng. Chúng tôi huấn luyện hai mô hình
sử dụng Phân vùng 1 và 2 độc lập nhưng với cùng một khởi
tạo. Trước vòng lặp tiếp theo của chu trình huấn luyện-tỉa-tua
lại của IMP, thuật toán tỉa của chúng tôi lấy giao của các trọng
số của hai mô hình. Nói cách khác, chỉ có các trọng số trùng
lặp tồn tại, và những trọng số không trùng lặp trở thành số
không (tức là được tỉa). Sự trùng lặp này được tính toán bằng
các mặt nạ tìm mạng con chung trong cả hai

--- TRANG 2 ---
2 TẠP CHÍ IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, TẬP 00, SỐ 0, THÁNG 2020

Hình 1: Chúng tôi giải quyết vấn đề tỉa một mạng có quá nhiều tham số mà không ảnh hưởng đến hiệu suất. Ở đây, chúng tôi
hiển thị các trọng số ban đầu của mạng đã học trong khi huấn luyện với TinyImageNet. (a) cho thấy trọng số của mạng có
quá nhiều tham số chưa được tỉa (không tỉa), đạt độ chính xác 72,7%. Phương pháp tỉa phổ biến, (b) LTH có thể tỉa cùng
một mạng đến độ thưa thớt 88,8% (vùng lưới xanh) trong 10 vòng tỉa, đạt độ chính xác 71,42%. (c) Trong bài báo này,
chúng tôi đề xuất một phương pháp tỉa mới tên là COLT có thể tỉa mạng trong (a) đến độ thưa thớt 89,1% trong 7 vòng tỉa
và đạt độ chính xác 72,8%. So với LTH trong (b), COLT có thể tạo ra các mạng con thắng cuộc có độ thưa thớt cao (vé) trong
ít vòng lặp hơn (7 so với 10), duy trì độ chính xác tương tự.

mô hình (được huấn luyện trên các phân vùng riêng lẻ). Chúng
tôi nhận thấy rằng giao điểm của hai vé số này thường là một
vé thắng cuộc. Vì tỉa xảy ra hai lần mỗi vòng lặp (từ hai mô
hình riêng biệt), phương pháp của chúng tôi có thể tạo ra một
vé thắng cuộc thưa thớt trong ít vòng lặp hơn LTH. Nói chung,
IMP của LTH cần chạy trong nhiều vòng lặp trước khi có được
một vé thắng cuộc thưa thớt. Tuy nhiên, sử dụng thuật toán
tỉa lặp lại của chúng tôi, chúng tôi có thể suy ra một vé thắng
cuộc có độ thưa thớt tương tự trong ít vòng lặp hơn LTH. Đối
với các thí nghiệm của chúng tôi, chúng tôi đã sử dụng ResNet-18
[23], MobileNetV2 [24], và một Mạng Nơ-ron Tích chập (CNN)
đơn giản 4 lớp gọi là Conv-3 được huấn luyện trên Cifar-10
[25], Cifar-100 [25], Tiny ImageNet [21] và ImageNet [26].
Chúng tôi đã chia mỗi bộ dữ liệu trong ba bộ thành N phần
(N=2,3,4), mỗi phần có số lượng lớp khác nhau nhưng bằng
nhau. Kết quả thí nghiệm của chúng tôi chứng minh rằng sau
khi hoàn thành huấn luyện-tỉa-tua lại, nếu chúng tôi chỉ giữ
các trọng số trùng lặp giữa hai phần và tỉa phần còn lại, nó
hoạt động tốt như mạng gốc chưa được tỉa cho cả hai tập dữ
liệu. Quá trình này được lặp lại nhiều lần cho đến khi đạt được
độ thưa thớt mong muốn cho mạng. Chúng tôi gọi mạng đã
tỉa là Vé Số Trùng Lặp Tuần Hoàn (COLT). COLT có thể
được tạo trong ít vòng lặp hơn so với các vé số được tạo từ
một tập dữ liệu duy nhất. Hơn nữa, COLT cũng có thể được
sử dụng để khởi tạo trọng số cho tập dữ liệu khác với kết quả
huấn luyện thành công. Chúng tôi đã so sánh hiệu suất của
COLT với LTH thử nghiệm trên các tập dữ liệu Cifar-10,
Cifar-100, Tiny ImageNet và ImageNet. Chúng tôi cũng đã
chuyển COLT được huấn luyện trên Tiny ImageNet sang Cifar-10
và Cifar-100. Ngoài phân loại, chúng tôi tiếp tục đánh giá
phương pháp trên nhiệm vụ Phát hiện Đối tượng. Trong các
thí nghiệm, chúng tôi đạt được hiệu suất tương tự như mạng
gốc (chưa tỉa) nhưng mất ít vòng tỉa hơn so với phương pháp
LTH. Tóm lại, chúng tôi đóng góp những điều sau đây:

• Chúng tôi đề xuất một thuật toán tỉa lặp lại cho nén mô
hình dựa trên các trọng số trùng lặp/giao cắt được huấn
luyện từ N phân vùng không trùng lặp của cùng một tập
dữ liệu. Mạng con kết quả (sau khi tỉa) trở nên có độ
thưa thớt cao và đạt độ chính xác tương tự như mạng
gốc chưa được tỉa. Chúng tôi gọi mạng con/vé được tạo
này là Vé Số Trùng Lặp Tuần Hoàn (COLT).

• Mà không ảnh hưởng đến hiệu suất, COLT có thể đạt
được độ thưa thớt mong muốn bằng cách tỉa một mạng
cho trước với chi phí ít vòng lặp hơn so với phương
pháp tỉa dựa trên LTH phổ biến.

• Chúng tôi chứng minh tính ưu việt của phương pháp
bằng cách thử nghiệm trên năm tập dữ liệu (Cifar-10,
Cifar-100, Tiny ImageNet, ImageNet và Pascal-VOC)
và ba kiến trúc CNN (Conv-3, ResNet-18, và MobileNetV2).

II. CÁC CÔNG TRÌNH LIÊN QUAN

Tỉa truyền thống: Mục tiêu tổng thể của tỉa là giảm kiến trúc
mạng bằng cách giảm thiểu số lượng tham số mạng mà không
làm hại hiệu suất mạng. Han et al. [3] đề xuất phương pháp
tỉa phổ biến nhất, gợi ý huấn luyện mạng cần được tỉa cho
đến khi hội tụ và sau đó tính toán điểm số cho mỗi tham số
đại diện cho tầm quan trọng của tham số đó. Sau đó, các tham
số có điểm số thấp bị tỉa, dẫn đến suy giảm hiệu suất mạng.
[3], [27] gợi ý tinh chỉnh mạng chỉ sử dụng các tham số chưa
được tỉa. Chu trình huấn luyện-tỉa này dần dần làm cho mạng
thưa thớt và là phương pháp tỉa truyền thống. Sau đó, các
công trình liên quan khác đề xuất những biến thể nhỏ cho
phương pháp này trong đó trọng số được tỉa trong khi huấn
luyện theo cách định kỳ [28] hoặc trong quá trình khởi tạo
[29]. Một bài báo khác [13] thêm rõ ràng các tham số bổ sung
vào mạng để thúc đẩy độ thưa thớt và phục vụ như cơ sở cho
việc chấm điểm mạng sau khi huấn luyện. Trong bài báo này,
thay vì tinh chỉnh sau khi tỉa, chúng tôi đã tua lại các trọng
số chưa được tỉa về trạng thái ban đầu của chúng và sau đó
huấn luyện lại chúng đến hội tụ.

Tỉa với giả thuyết vé số: Nói chung, tỉa được thực hiện như
một bước sau huấn luyện, đòi hỏi phải huấn luyện mô hình
đầy đủ trước khi cố gắng tỉa tham số. Tuy nhiên, với sự ra
đời của giả thuyết vé số (LTH) [21], các mạng thưa thớt được
xác định trước có thể được huấn luyện từ đầu. Trái ngược với
các công trình trước đó [30]–[35], LTH [21] gợi ý rằng tham
số hóa quá mức chỉ cần thiết để tìm một khởi tạo "tốt" của
một mạng được tham số hóa đúng cách. Họ chứng minh rằng
các mạng con nhỏ hơn đáng kể (vé số) tồn tại trong các mô
hình có quá nhiều tham số lớn.

--- TRANG 3 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 3

5 Vòng 10 Vòng 17 Vòng
3 Vòng 7 Vòng 13 Vòng
0-70% 0-90% 0-98% 0 500 1000 1500 2000 2500
ResNet18-TinyImageNet
LTH
COLT
Độ Thưa Thớt (%) Độ Trễ (Phút)

5 Vòng 16 Vòng 35 Vòng
3 Vòng 11 Vòng 24 Vòng
0-70% 0-90% 0-98% 0 100 200 300 400 500
ResNet18 - Cifar100
LTH
COLT
Độ Thưa Thớt (%) Độ Trễ (Phút)

Hình 2: Minh họa lợi thế của COLT được đề xuất so với phương pháp tỉa dựa trên LTH trong khi sử dụng kiến trúc ResNet-18
trên tập dữ liệu (Trái) Cifar10 và (Phải) Cifar-100. So với LTH, COLT có thể tạo ra một vé có độ thưa thớt cao trong ít
vòng/lặp hơn. Chúng tôi nhận thấy xu hướng tương tự để đạt được độ thưa thớt của các phạm vi khác nhau, ví dụ: 0-70%,
0-90%, và 0-98%. Trong tất cả các trường hợp, COLT duy trì độ chính xác tương tự như LTH trong khi yêu cầu ít vòng
tỉa hơn. Cụ thể, đối với (Trái) ResNet-18 trên Cifar-10, độ thưa thớt 61,5%, 88,8% và 97,6% đạt được độ chính xác lần
lượt là 60,28%, 59,01% và 51,81%. Đối với (Phải) ResNet-18 trên Cifar100, độ thưa thớt 61,7%, 89,1% và 97,4% đạt được
độ chính xác lần lượt khoảng 73,59%, 72,88% và 68,53%.

Trong khi được huấn luyện riêng biệt, những mạng con này
đạt được hiệu suất tương tự hoặc tốt hơn mạng gốc ngay cả
sau khi tỉa hơn 90% tham số. Nó cho phép các mạng huấn
luyện với ít tài nguyên hơn và chạy suy luận của các mô hình
trên các thiết bị nhỏ hơn như điện thoại di động [36]. Ngoài
ra, nó khuyến khích tổng quát hóa bằng cách hoạt động như
một bộ điều chỉnh cho các mô hình có quá nhiều tham số. Để
tìm và đánh giá các vé thắng cuộc bằng LTH, một mạng có
quá nhiều tham số đầu tiên được khởi tạo và huấn luyện đến
hội tụ. Sau đó, các trọng số có độ lớn thấp nhất được tỉa, và
các trọng số còn lại được đặt lại về trạng thái ban đầu của
chúng khi bắt đầu huấn luyện. Việc đặt lại trọng số này cũng
được gọi là tua lại trọng số. Cuối cùng, mạng con nhỏ hơn
này (vé thắng cuộc) sau đó được huấn luyện lại để so sánh
hiệu suất của nó với mạng con khác có cùng số lượng tham
số và cùng khởi tạo nhưng trọng số được tỉa ngẫu nhiên. Một
vé thắng cuộc tốt vượt trội hơn một vé được tỉa ngẫu nhiên.
[37] cung cấp một định nghĩa chặt chẽ hơn về LTH bằng cách
thực hiện các thí nghiệm rộng rãi và điều chỉnh toàn diện các
siêu tham số như tốc độ học và số epoch huấn luyện. Trong
bài báo này, chúng tôi đề xuất một thuật toán tỉa lặp lại sử
dụng ít vòng lặp hơn để đạt được độ thưa thớt được chỉ định.

Tỉa trong học chuyển đổi: Tỉa mạng có thể tận dụng các khái
niệm học chuyển đổi [38]–[41]. Các phương pháp dựa trên
học chuyển đổi đáng chú ý [8], [15] huấn luyện mô hình trên
một tập dữ liệu, sau đó áp dụng các bước tỉa và sau đó tinh
chỉnh mô hình trên tập dữ liệu khác để đạt được hiệu suất
cao. Tất cả những bài báo này phân tích việc chuyển đổi các
biểu diễn đã học của các mô hình. Tuy nhiên, việc chuyển đổi
khởi tạo trọng số qua các tập dữ liệu đầu tiên được phân tích
bởi [22]. Nó có nghĩa là chuyển đổi các trọng số ban đầu sau
chu trình huấn luyện-tỉa-tua lại của IMP. [22] thảo luận chi
tiết về khả năng tổng quát hóa và chuyển đổi của các vé số.
Họ chứng minh rằng khởi tạo vé thắng cuộc có thể được tạo
ra độc lập với các tập dữ liệu và bộ tối ưu hóa được sử dụng
trong các thí nghiệm. Hơn nữa, họ cũng cho thấy rằng các
vé thắng cuộc được tạo bởi các tập dữ liệu lớn có thể được
chuyển thành công hơn so với những vé được tạo bằng các
tập dữ liệu nhỏ. Người ta có thể phát triển một vé mạnh mẽ
từ một tập dữ liệu rộng lớn và sau đó tái sử dụng những khởi
tạo đó trên các tập dữ liệu khác mà không cần huấn luyện đầy
đủ một mô hình. Bài báo này chứng minh khả năng chuyển
đổi của các vé được tạo bởi COLT qua các tập dữ liệu khác
nhau.

Trong khi tất cả các phương pháp đạt được tỉa và độ thưa
thớt cao, COLT phân biệt bản thân bằng cách cung cấp tỉa
nhanh hơn và khả năng chuyển đổi, các tính năng không có
mặt nhất quán trong các kỹ thuật khác. Ví dụ, LTH [21] nhấn
mạnh độ thưa thớt cao cùng với tỉa hiệu quả nhưng thiếu tốc
độ tỉa mà COLT chứng minh. Tương tự, SNIP [29] và GraSP
[33] đạt được độ thưa thớt cao, nhưng không ai trong số chúng
nhất quán phù hợp với COLT đề xuất của chúng tôi về cả tốc
độ và khả năng chuyển đổi qua các nhiệm vụ.

III. PHƯƠNG PHÁP LUẬN

A. Công thức hóa vấn đề

Hãy xem xét một mạng lan truyền tiến F(x;θ) với các tham
số ban đầu θ=θ₀∼D_θ và đầu vào x. Sau khi huấn luyện cho
đến khi hội tụ, mạng này đạt được độ chính xác a đạt được
mất mát xác thực tối thiểu. F cũng có thể được huấn luyện
với một mặt nạ m∈{0,1}^|θ| được vận hành trên các tham
số bởi m⊙θ đạt được độ thưa thớt s%. Để tạo ra mặt nạ m
này, chúng ta cần thực hiện j vòng chu trình huấn luyện-tỉa-
tua lại. Mục tiêu của chúng ta là thiết kế mặt nạ m sao cho
khi được vận hành trên các tham số, θ (bởi m⊙θ) tạo ra một
mạng, F(x;m⊙θ) đạt được độ chính xác tương đương, a'≥a
sử dụng ít thời gian huấn luyện và vòng tỉa j'≤j trong khi
duy trì cùng độ thưa thớt s%. Huấn luyện với m=1^|θ| có
nghĩa là chúng ta huấn luyện một mạng chưa tỉa F. Trong
khi sử dụng m∈{0,1}^|θ|, chúng ta huấn luyện một mạng
con đã tỉa của F. Trong trường hợp này, các trọng số/tham
số tương ứng bên trong θ, nơi mà 0 được gán bên trong m
đã được tỉa. Nhiều số không hơn bên trong m tạo ra nhiều
độ thưa thớt hơn bên trong các tham số mạng

--- TRANG 4 ---
4 TẠP CHÍ IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, TẬP 00, SỐ 0, THÁNG 2020

BẢNG I: Chuyển đổi các vé LTH được tính từ Phân vùng 1
và 2 sang toàn bộ tập dữ liệu Cifar-100. Hiệu suất của các
vé trùng lặp đạt được độ thưa thớt tốt hơn mà không ảnh hưởng
đến độ chính xác. Điều này xác nhận rằng trùng lặp cũng có
tính chất chuyển đổi ngoài các vé của các phân vùng riêng lẻ.

Sử dụng vé từ        Phân vùng-1  Phân vùng-2  Vé trùng lặp
Độ chính xác (%)     72.4         72.3         69.1
Độ thưa thớt (%)     79.9         79.9         95.0

khi áp dụng m⊙θ. Chúng tôi nhằm tăng độ thưa thớt như vậy
trong F mà không ảnh hưởng đến hiệu suất và thời gian huấn
luyện.

Giả thuyết vé số (LTH): Frankle và Carbin [21] đề xuất giải
pháp phổ biến nhất cho vấn đề này. Sau đó [22], [42] cải
thiện ý tưởng LTH. LTH sử dụng thuật toán tỉa dựa trên độ
lớn. Bằng cách xác định một mặt nạ thưa thớt, m, các trọng
số có độ lớn thấp hơn được loại bỏ trong các vòng tỉa khác
nhau. Các trọng số còn lại sau đó được tua lại về trạng thái
ban đầu của chúng bởi θ=m⊙θ. Tập hợp trọng số được tua
lại này là một vé số. Nếu người ta có thể tìm thấy một vé số
có thể huấn luyện một mô hình và đạt được độ chính xác bằng
hoặc lớn hơn trọng số chưa tỉa ban đầu, vé số đó được gọi
là vé thắng cuộc. Chu trình huấn luyện-tỉa-tua lại được thực
hiện nhiều lần lặp lại để tạo ra các vé thắng cuộc có độ thưa
thớt cao.

Vấn đề với LTH: Việc huấn luyện lặp đi lặp lại từ đầu đến
hội tụ của các mạng lớn trong khi tìm kiếm vé thắng cuộc là
một quá trình tốn kém, tăng thời gian tính toán. Trong Hình
2, chúng ta thấy rằng để đạt được một vé có độ thưa thớt cao,
LTH yêu cầu số lượng vòng tỉa cao (tức là tăng độ trễ). Ví
dụ, phải mất 62 vòng tỉa để tạo ra một vé thưa thớt 98% của
MobileNetV2 trên Cifar-100. Trong bài báo này, chúng tôi
nhằm giảm số lượng vòng tỉa (thời gian huấn luyện) để đạt
được độ thưa thớt cao như vậy.

B. Chiến lược giải pháp

Morcos et al. [22] quan sát một sự thật thú vị về LTH liên
quan đến khả năng chuyển đổi của các vé số. Các mạng con
đã tỉa, tức là các vé thắng cuộc được tính toán bằng một tập
dữ liệu, có thể được chuyển sang các tập dữ liệu khác, với
điều kiện miền dữ liệu tương tự (ví dụ: hình ảnh tự nhiên).
Họ lập luận rằng các mạng con đã tỉa giống nhau có thể tổng
quát hóa qua các điều kiện huấn luyện, bộ tối ưu hóa và tập
dữ liệu khác nhau. Ý nghĩa chính của phát hiện này là chúng
ta có thể tính toán một vé thắng cuộc chung một lần bằng một
tập dữ liệu và sau đó tái sử dụng nó qua nhiều tập dữ liệu
hoặc bộ tối ưu hóa. Nó có thể tiết kiệm chi phí tính toán lặp
đi lặp lại các vé đặc thù cho bộ tối ưu hóa hoặc tập dữ liệu.
Vé thắng cuộc có thể đạt được độ chính xác tương tự và độ
thưa thớt giống hệt với vé thắng cuộc của toàn bộ mô hình
LTH huấn luyện trên tập dữ liệu mới.

Trong bài báo này, chúng tôi điều tra tính chất chuyển đổi
của LTH. Chúng tôi quan sát rằng các vé trung gian (trước
khi thu được các vé thắng cuộc) cũng có tính chất chuyển
đổi nhưng với độ thưa thớt ít hơn. Mỗi vòng tỉa của phương
pháp tỉa chúng tôi tạo ra một số mạng con/vé trung gian,
thực hiện giao của các mạng con/vé khác nhau, và sau đó
chuyển vé mới sang vòng tiếp theo. Khái niệm chuyển vé
qua các vòng lặp/tỉa của chúng tôi giúp chúng tôi đạt được
hiệu suất tương tự của LTH, với chi phí ít vòng tính toán hơn.

0 5 10 15 62 64 66 68 70 72 74
0 20 40 60 80 100
Acc sử dụng m(1)
Acc sử dụng m(2)
Acc sử dụng m
Tỷ lệ Tỉa - m(1)
Tỷ lệ Tỉa - m(2)
Tỷ lệ Tỉa - m
ResNet18: CIFAR-100
Vòng Tỉa
Độ Chính Xác Kiểm Tra (%)
Tỷ Lệ Tỉa (%)

Hình 3: Khả năng chuyển đổi của các vé được tính từ Phân
vùng 1, m(1), phân vùng 2, m(2) và sự trùng lặp của chúng,
m=m(1)∩m(2). Vé trùng lặp, m đạt được tỷ lệ tỉa cao hơn
(độ thưa thớt), duy trì độ chính xác tương tự như những vé
khác. m(1) và m(2) có độ thưa thớt phù hợp vì chúng tôi tỉa
p% trọng số có độ lớn thấp cố định trong mỗi vòng tỉa. Trong
các vòng tỉa sau, hành vi của tất cả các vé tương tự vì mỗi
lần cùng một m được tính từ vòng hiện tại được chuyển sang
cả F1 và F2 cho vòng tiếp theo làm cho θ(1) và θ(2) tương tự.

Động lực của chúng tôi bắt đầu với kết quả thí nghiệm được
hiển thị trong Bảng I. Chúng tôi chia Cifar-100 thành N phân
vùng. Thông thường, N được sử dụng như một siêu tham số.
Trong trường hợp N=2, nếu chúng ta có 100 lớp cho một
tập dữ liệu, thì hình ảnh của 50 lớp đầu tiên vẫn trong một
phân vùng và 50 lớp còn lại trong phân vùng khác. Sau đó
chúng tôi tạo ra các vé LTH (cho đến khoảng 80% độ thưa
thớt) từ mỗi phân vùng. Theo [22], cả hai vé từ hai phân
vùng riêng biệt đều có thể chuyển đổi sang toàn bộ tập dữ
liệu Cifar-100. Do đó, chúng tôi đạt được 72,4/79,9% và
72,3/79,9% độ chính xác/độ thưa thớt dựa trên huấn luyện
từ phân vùng 1 và 2, tương ứng. Chúng tôi biết có tồn tại
một mối tương quan mạnh giữa các vé được tạo bởi cả hai
phân vùng. Nó thúc đẩy chúng tôi kiểm tra hiệu suất của các
vé trùng lặp, có nghĩa là mạng con giao cắt, vẫn chung sau
khi tỉa. Chúng tôi nhận thấy rằng ngay cả khi loại trừ các
trọng số không trùng lặp (tăng độ thưa thớt lên 95%), chúng
tôi vẫn có thể đạt được hiệu suất đáng kính (69,1%) trên
Cifar-100 đầy đủ. Nó cho biết rằng ngoài việc cải thiện độ
thưa thớt, các vé trùng lặp được tính từ nhiều phân vùng của
cùng một tập dữ liệu cũng có thể chuyển đổi sang toàn bộ
tập dữ liệu. Vì các vé trung gian thu được từ các vòng tỉa
LTH trung gian có thể chuyển đổi, chúng tôi tính toán vé
trùng lặp của N phân vùng khác nhau trong mỗi vòng tỉa.
Nó có thể cung cấp cho chúng tôi tỉa thêm, dẫn chúng tôi
đến ít vòng tỉa hơn LTH, duy trì khả năng chuyển đổi sang
các phân vùng riêng lẻ và toàn bộ tập dữ liệu. Trong mỗi
vòng tỉa, chúng tôi huấn luyện N mô hình dựa trên N phân
vùng của cùng một tập dữ liệu để tính toán N vé từ mỗi phân
vùng, sau đó tính toán các vé trùng lặp giữa chúng, và cuối
cùng, chuyển vé trùng lặp sang vòng tỉa tiếp theo để khởi
tạo những N mô hình đó. Chúng tôi tiếp tục điều này cho
đến khi vé trùng lặp đạt được độ chính xác tương tự như mô
hình chưa tỉa.

--- TRANG 5 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 5

BẢNG II: Một ví dụ đơn giản. Trọng số ngẫu nhiên ban đầu
được cập nhật sau khi huấn luyện. Sử dụng trọng số đã cập
nhật, chúng tôi tính toán m(1) và m(2) bằng cách gán số
không cho các vị trí mà (giá trị in đậm) p% trọng số có độ
lớn thấp hơn tồn tại. Sau đó, chúng tôi tính toán một vé chung,
m=m(1)∩m(2), đề cập đến mạng con đã tỉa. Trọng số ban
đầu đã tỉa cho vòng tỉa tiếp theo được tính bởi m⊙θ(1) và
m⊙θ(2).

Trọng số ban đầu  Sau huấn luyện    Mặt nạ          Mặt nạ trùng lặp   Trọng số đã tỉa
θ(1)=            θ(1)=              m(1)=           m=m(1)∩m(2)        m⊙θ(1)=
0.1 -0.2  0.9    0.8 -0.3  0.4     1  1  1        1  1  0           0.1 -0.2  0
-0.4  0.6  0.8   -0.1  0.2  0.7    0  0  1        0  0  1           0   0   0.8
0.3   0.5 -0.7    0.9  0.5 -0.5    1  1  1        1  1  1           0.3 0.5 -0.7

θ(2)=            θ(2)=              m(2)=                              m⊙θ(2)=
0.1 -0.2  0.9    0.9 -0.7  0.2     1  1  0                          0.1 -0.2  0
-0.4  0.6  0.8   -0.1  0.4  0.5    0  1  1                          0   0   0.8
0.3   0.5 -0.7    0.8  0.3 -0.6    1  1  1                          0.3 0.5 -0.7

C. Vé Số Trùng Lặp Tuần Hoàn (COLT)

Gọi Y là tập hợp các lớp trong tập dữ liệu huấn luyện của
chúng ta D={(xi, yi)}^n_{i=1}, trong đó, xi∈R^{h×w} và yi∈Y
đại diện cho hình ảnh đầu vào và nhãn lớp, tương ứng, và n
biểu thị số lượng thể hiện trong tập dữ liệu. F(x;θ) là mô
hình chưa tỉa với tất cả các tham số θ. Chúng tôi nhằm tính
toán một vé thắng cuộc được biểu diễn như một mặt nạ thưa
thớt, m∈{0,1}^{|θ|}, sao cho F(x;m⊙θ) có thể đạt được độ
chính xác tương tự của mô hình chưa tỉa. Chúng tôi đề xuất
một phương pháp mới để tạo ra các vé thắng cuộc có thể
được tính toán bằng ít vòng tỉa hơn LTH.

Gọi khởi tạo một mạng nơ-ron F(x;θ₀), trong đó các tham
số ban đầu θ=θ₀. Tiếp theo, tập dữ liệu D được phân vùng
thành hai tập con rời rạc D(1) và D(2), đảm bảo D=D(1)∪D(2)
và D(1)∩D(2)=∅. Hai mô hình, F1 và F2, sau đó được huấn
luyện đến hội tụ trên D(1) và D(2), tương ứng, dẫn đến các
tham số đã huấn luyện θ1 và θ2 với độ chính xác a1 và a2.
Tính mới cốt lõi của phương pháp này nằm ở tỉa tuần hoàn
với các vé trùng lặp. Trong quá trình tỉa, các tham số θ₀ của
mô hình được che mặt lặp lại bởi N mạng con được biểu
diễn bởi các mặt nạ mi∈{0,1}^{|θ₀|}. Mỗi mặt nạ mi loại bỏ
p% trọng số có độ lớn thấp nhất. Các tham số đã tỉa θi cho
mỗi mặt nạ được thu được bằng phép nhân theo phần tử
θi=mi⊙θ. Những mặt nạ này cùng nhau tạo thành mặt nạ
kết hợp M=∩^N_{i=1}mi, đảm bảo mức độ thưa thớt tổng thể
s trong khi duy trì hoặc cải thiện hiệu suất. Sau khi thu được
mặt nạ kết hợp M, nó được áp dụng cho các tham số của mô
hình F(x;M⊙θ).

Phương pháp bao gồm bốn bước chính được mô tả dưới đây:

(a) Phân vùng dữ liệu: Chúng tôi phân vùng dữ liệu huấn
luyện thành N nửa không trùng lặp dựa trên nhãn lớp. Đối
với điều này, chúng tôi chia ngẫu nhiên tập hợp các lớp Y
thành N nửa bằng nhau không trùng lặp Y(1), Y(2)...Y(N),
trong đó, Y(1)∩Y(2)∩Y(N)=∅. Đối với một ví dụ về phân
vùng đặc thù cho lớp của hai tập, chúng tôi tạo hai tập dữ
liệu riêng biệt: D(1)={(x_i^{(1)}, y_i^{(1)})}^{n(1)}_{i=1} và
D(2)={(x_i^{(2)}, y_i^{(2)})}^{n(2)}_{i=1} trong đó, y_i^{(1)}∈Y(1)
và y_i^{(2)}∈Y(2). Chúng tôi ưu tiên việc chia dựa trên lớp
vì nó có thể cung cấp cho chúng tôi sự đa dạng hơn bên
trong các vé trung gian (so với các lựa chọn thay thế dựa
trên thể hiện), điều này sẽ giúp tỉa nhanh hơn. Đối với thí
nghiệm chính (trong Hình 5), chúng tôi chia tập dữ liệu chỉ
thành hai nửa vì mỗi nửa sẽ chứa đủ thể hiện để tổng quát
hóa. Hơn hai phân vùng (ba/bốn) có thể kết thúc với các tập
con nhỏ hơn của tập dữ liệu. Tác động của nhiều phân vùng
được thảo luận thêm trong Mục IV-D

(b) Huấn luyện mô hình: Sử dụng dữ liệu được phân vùng
D1 và D2, chúng tôi huấn luyện hai mô hình riêng biệt
F1(x(1);m⊙θ(1)) và F2(x(2);m⊙θ(2)), tương ứng. Tại vòng
tỉa đầu tiên, m=1^{|θ|} có nghĩa là huấn luyện mô hình chưa
tỉa. Trong các vòng tỉa sau, m sẽ được xác định bởi quá
trình tạo mặt nạ được thảo luận trong đoạn tiếp theo, trong
đó m sẽ trở nên thưa thớt, chứa số không ở các vị trí đã tỉa.
Sau khi tính toán mặt nạ, m, chúng tôi áp dụng cùng một
mặt nạ trên các tham số được khởi tạo ngẫu nhiên bởi m⊙θ(1)
và m⊙θ(2). Phép toán ⊙ thực sự đang thực hiện tỉa vì nó
mang lại độ thưa thớt trong ma trận trọng số. Khi bắt đầu
huấn luyện, các tham số θ(1) và θ(2) sẽ nhận được chính
xác cùng trọng số ban đầu như θ₀∼D_θ mà sau đó sẽ được
cập nhật trong quá trình lan truyền ngược. Do đó, sau khi
hội tụ của các mô hình, F1 và F2, cả θ(1) và θ(2) đều được
mong đợi chứa các trọng số khác nhau. Một ví dụ đơn giản
về trọng số ban đầu và sau huấn luyện của θ(1) và θ(2) được
hiển thị trong Bảng II. Lưu ý rằng việc huấn luyện F1 và
F2 là độc lập, có nghĩa là việc thực hiện song song là có thể.
Hơn nữa, các tập dữ liệu liên kết D1 và D2 nhỏ hơn so với
những tập gốc. Điều này có nghĩa là chúng ta có thể giảm
thiểu thời gian huấn luyện tổng thể.

(c) Tạo mặt nạ: Dựa trên các trọng số đã huấn luyện θ(1)
và θ(2) từ hai mô hình riêng biệt, chúng tôi tính toán một
mặt nạ, m. Đầu tiên, chúng tôi tạo ra hai mặt nạ trung gian,
m(1) và m(2) từ θ(1) và θ(2), tương ứng. Phân tích các trọng
số của θ(1), chúng tôi tỉa p% trọng số có độ lớn thấp hơn.
Do đó, mặt nạ nhị phân m(1) sẽ nhận được số không và số
một nơi các trọng số cần được tỉa và không được tỉa, tương
ứng. Tương tự, chúng tôi tạo m(2) từ θ(2). Trong thí nghiệm
của chúng tôi, chúng tôi sử dụng p=20 như được sử dụng
trong LTH [21]. Lưu ý chúng tôi sử dụng p nhỏ vì p lớn
tăng độ thưa thớt hơn nhưng, đồng thời, tăng cơ hội sụp đổ
lớp. Bây giờ, chúng tôi tạo ra m=m(1)∩m(2) để xác định
các vị trí trọng số trùng lặp. m mô tả các vé chung nơi cả
hai mô hình đồng ý. Phương pháp của chúng tôi đạt được
tỉa nhanh hơn vì tỉa từ p% trọng số có độ lớn thấp và giao
cắt các mặt nạ (m) được tính từ hai phân vùng khác nhau
của tập dữ liệu. Trong Hình 3, chúng tôi cho thấy hiệu suất
của m(1), m(2) và m trên toàn bộ Cifar-100. Nó cho thấy
bằng chứng thêm rằng vé trùng lặp, m, có thể chuyển đổi
qua các vòng tỉa khác nhau, đảm bảo độ thưa thớt cao hơn.

(d) Tua lại: Trong bước này, chúng tôi đang chuyển các vé
được tính từ cả hai phân vùng sang các phân vùng riêng lẻ
của tập dữ liệu. Sử dụng m được tính từ bước trước, chúng
tôi lặp lại quá trình cho vòng tỉa tiếp theo bằng cách lặp lại
sang bước (b). Cùng một m sẽ sửa đổi θ(1) và θ(2). Việc
huấn luyện mô hình tua lại các trọng số chưa được tỉa cho
đến nay (sau phép toán ⊙

--- TRANG 6 ---
6 TẠP CHÍ IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, TẬP 00, SỐ 0, THÁNG 2020

Hình 4: Minh họa trực quan về việc tạo COLT khi N=2.
(a) Hai mô hình, F1&F2, có kiến trúc giống hệt nhau, đầu
tiên được khởi tạo với cùng trọng số ban đầu (màu xanh).
(b) Tiếp theo, hai mô hình được huấn luyện bằng cùng một
tập hợp các siêu tham số để suy ra các trọng số cuối cùng
(màu đỏ & xanh lá). (c) Sau khi huấn luyện, p% trọng số
cuối cùng có độ lớn thấp nhất được tỉa khỏi mỗi mô hình.
(trọng số màu đỏ là p% trọng số có độ lớn thấp nhất trong
khi những trọng số màu xanh lá vượt ngưỡng) (d) Sau đó
các mô hình được tua lại về trạng thái trọng số ban đầu của
chúng (màu xanh, với các trọng số đã tỉa là số không). (e)
Sau khi tua lại, các trọng số trùng lặp của mô hình (màu
xanh) được giữ lại, và phần còn lại được tỉa. Kết quả là
một vé COLT có thể đạt được độ chính xác tương tự hoặc
tốt hơn mạng gốc.

phép toán) về các giá trị ban đầu của chúng được sử dụng
trong quá trình khởi tạo khi bắt đầu huấn luyện. Sử dụng
m được tính từ vòng trước tạo điều kiện cho việc huấn luyện
sao cho phương pháp của chúng tôi cố gắng tỉa trong các
trọng số chưa được tỉa cho đến nay chỉ trong lần lặp tiếp
theo thay vì xem xét tất cả các tham số.

Quá trình tỉa lặp lại này sẽ tiếp tục cho đến khi độ chính
xác xác thực của toàn bộ tập dữ liệu, D, trở nên bằng hoặc
tốt hơn độ chính xác mô hình chưa tỉa gốc. Do chu trình
huấn luyện-tỉa-tua lại trên các mặt nạ/vé trùng lặp, chúng
tôi đặt tên cho vé được tạo bởi phương pháp của chúng tôi
là Vé Số Trùng Lặp Tuần Hoàn (COLT). Hình 4 cung cấp
một minh họa trực quan để tạo COLT. Lưu ý rằng chúng
tôi tạo COLT dựa trên một kiến trúc có thể gán điểm dự
đoán cho

Thuật toán 1 Vé Số Trùng Lặp Tuần Hoàn (COLT)
Yêu cầu: Tập dữ liệu D, tham số mạng ban đầu θ, số vòng
tỉa R, tỷ lệ tỉa p
Đảm bảo: Vé thắng cuộc (mặt nạ thưa thớt) m
1: Chia ngẫu nhiên tập dữ liệu D thành hai nửa D(1) và D(2)
dựa trên nhãn lớp
2: Khởi tạo mặt nạ m=1|θ| (tất cả số một, cùng kích thước với θ)
3: for r=1 to R do
4:   Huấn luyện mô hình F1(x;m⊙θ(1)) trên D(1)
5:   Huấn luyện mô hình F2(x;m⊙θ(2)) trên D(2)
6:   Tạo mặt nạ trung gian:
7:   m(1)=tỉa độ lớn thấp nhất(θ(1), p%)
8:   m(2)=tỉa độ lớn thấy nhất(θ(2), p%)
9:   if r < R then
10:    Tua lại các trọng số chưa tỉa trong θ(1) và θ(2) về
      giá trị ban đầu của chúng
11:    Cập nhật mặt nạ: m=m(1)∩m(2) (phép toán AND
      theo phần tử)
12:  end if
13: end for
14: return mặt nạ cuối cùng m

một nửa tổng số lớp (Y(1) hoặc Y(2) lớp). Tuy nhiên, chúng
tôi muốn đánh giá COLT trên toàn bộ tập dữ liệu (Y lớp),
có nghĩa là cùng một kiến trúc cần dự đoán điểm cho tất cả
các danh mục. Do đó, để tạo mô hình cuối cùng của chúng
tôi, F, chúng tôi đã loại bỏ lớp đầu ra của các vé thắng cuộc
và thay thế nó bằng một lớp kết nối đầy đủ được khởi tạo
lại ngẫu nhiên để dự đoán điểm cho tất cả các lớp. Sau đó
chúng tôi huấn luyện các vé cho đến khi hội tụ trước khi
báo cáo hiệu suất cuối cùng trên toàn bộ tập dữ liệu.

Thuật toán 1 tóm tắt quá trình tổng thể của phương pháp
đề xuất của chúng tôi.

Chúng tôi phác thảo một số tính năng và lợi thế chính của
phương pháp tỉa của chúng tôi. (a) Sử dụng tính chất chuyển
đổi: Chúng tôi điều tra tính chất 'chuyển đổi' của các vé số
trong các phân vùng khác nhau của cùng một tập dữ liệu.
Nó giúp xác định một cách tự tin các trọng số triển vọng
cần được tỉa trong các vòng tỉa sớm. (b) Tỉa nhanh hơn:
COLT sử dụng một phương pháp mới của việc trùng lặp hai
mặt nạ để tạo ra các mạng nơ-ron có độ thưa thớt cao trong
thời gian ít hơn đáng kể so với LTH. Bằng cách yêu cầu ít
vòng lặp hơn để đạt được các mức độ thưa thớt tương tự,
COLT tăng tốc đáng kể quá trình tỉa. Hơn nữa, bằng cách
bảo tồn các trọng số thiết yếu, COLT duy trì hiệu suất tương
tự hoặc tốt hơn LTH. Ngoài ra, heuristic này hiệu quả hơn
heuristic tỉa phi cấu trúc truyền thống được sử dụng trong
LTH. (c) Tránh sụp đổ lớp: Ngay cả sau khi hoàn thành tỉa
hai lần trong một vòng duy nhất, phương pháp của chúng
tôi có thể tránh sụp đổ lớp [43]. Các vé từ phân vùng thứ
nhất và thứ hai có thể chuyển đổi và có mối tương quan
mạnh với nhau. Do đó, giao cắt m(1) và m(2) không loại
bỏ các trọng số sao cho một lớp hoàn chỉnh bị sụp đổ. (d)
Giới thiệu Điều chuẩn: Bằng cách chỉ giữ lại các trọng số
có độ lớn cao và đặt về không các trọng số có độ lớn thấp
hơn, COLT thực hiện một hình thức điều chuẩn hóa ngầm,
giảm khả năng của mô hình để quá khớp.

--- TRANG 7 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 7

BẢNG III: Các kiến trúc mô hình được kiểm tra trong công
trình này. Dấu ngoặc biểu thị các kết nối dư thừa xung quanh
các lớp.

Mạng          Conv-3      MobileNetV2   ResNet-18     VGG-16
             Conv64, pool  6×[34,34]     3×[16,16]     2×64, pool
             128, pool     5×[17,17]     3×[32,32]     2×128, pool
             256, pool     8×[9,9]       3×[64,64]     3×256, pool
                          31×[5,5]                     3×512, pool
                                                      3×512, pool

Trọng số     Tất cả: 397K  Tất cả: 2.3M  Tất cả: 11M   Tất cả: 136M
             Conv: 371K    Conv: 2.2M                  Conv: 14M

P Conv       15%          15%           15%           15%
FC           20%

D. Heuristics tỉa

Heuristics tỉa của phương pháp đề xuất của chúng tôi được
thúc đẩy bởi công trình trước đó [21], [22]. Chúng tôi mô
tả các khía cạnh khác nhau của heuristics tỉa của chúng tôi
dưới đây:

• Tỉa có thể là có cấu trúc hoặc không có cấu trúc. Tỉa có
cấu trúc loại bỏ trọng số theo cụm, tức là bằng cách loại
bỏ các lớp/nơ-ron (cột trọng số), bộ lọc, hoặc kênh trong
CNN. Tỉa không có cấu trúc không tuân theo bất kỳ quy
tắc cụ thể nào, trong đó các trọng số được tỉa theo cách
phân tán để mang lại độ thưa thớt. Vì COLT được tạo
ra bằng cách chọn các trọng số có độ lớn thấp hơn từ
toàn bộ mạng, chúng có thể tỉa trọng số từ nhiều lớp
ngẫu nhiên trong một vòng tỉa. Do đó, chiến lược của
chúng tôi tuân theo tỉa không có cấu trúc.

• Dựa trên chiến lược được sử dụng, tỉa có thể là cục bộ
hoặc toàn cục. Tỉa cục bộ là về việc tỉa một phần cố
định của trọng số từ mỗi lớp. Ngược lại, tại mỗi vòng
tỉa, tỉa toàn cục liên quan đến việc tỉa một phần cố định
của trọng số từ toàn bộ mạng cho phép mỗi lớp có tỷ
lệ phần trăm khác nhau của trọng số cần được loại bỏ.
Chúng tôi xác định chiến lược tỉa của chúng tôi là toàn
cục vì chúng tôi xử lý toàn bộ mạng cùng một lúc.

• Tỉa có thể được thực hiện một lần (tỉa một lần) hoặc lặp
lại (tỉa lặp lại). Tỉa một phần đáng kể của trọng số trong
một vòng có thể gây nhiễu và loại bỏ các trọng số quan
trọng. Để khắc phục vấn đề này, chúng tôi tuân theo
phương pháp lặp lại, tức là nhiều vòng lặp của các chu
trình huấn luyện-tỉa xen kẽ với một phần trọng số được
tỉa cùng một lúc. Điều này tạo ra các mô hình đã tỉa
tốt hơn đáng kể và các vé thắng cuộc [21], [23].

• Phương pháp của chúng tôi sử dụng một heuristic mới
để tính toán các vé thắng cuộc từ các trọng số trùng
lặp giữa hai mô hình được huấn luyện trên các phân
đoạn khác nhau của một tập dữ liệu. Nó làm cho các
vé thắng cuộc mạnh mẽ hơn đối với sự biến đổi lớp.
Ngoài ra, nó tăng khả năng tổng quát hóa của các vé
khi chuyển sang tập dữ liệu khác.

IV. THÍ NGHIỆM

A. Thiết lập

Tập dữ liệu: Chúng tôi thực hiện các thí nghiệm trên ba tập
dữ liệu hình ảnh.

(a) Cifar-10: Tập dữ liệu Cifar-10 chứa 60000 hình ảnh màu
(độ phân giải 32×32) thuộc mười lớp có 6000 hình ảnh mỗi
lớp [25].

(b) Cifar-100: Tập dữ liệu Cifar-100 [25] tương tự như Cifar-10
về độ phân giải hình ảnh và tổng số hình ảnh. Tuy nhiên, nó
có 100 lớp với 600 hình ảnh mỗi lớp.

(c) Tiny ImageNet: Tập dữ liệu Tiny ImageNet [44] là một
phiên bản thu nhỏ của ImageNet được giới thiệu trong thử
thách phân loại MicroImagenet. Nó có 100000 hình ảnh màu
(độ phân giải 64×64) với 200 lớp và 500 hình ảnh mỗi lớp.

(d) ImageNet: ImageNet [26] là một tập dữ liệu quy mô lớn
được thiết kế để sử dụng trong nghiên cứu phần mềm nhận
dạng đối tượng thị giác. Ban đầu được giới thiệu trong Thử
thách Nhận dạng Thị giác Quy mô Lớn (ILSVRC), nó chứa
hơn 14 triệu hình ảnh và bao gồm 1000 lớp. Mỗi lớp trong
ImageNet thường chứa từ vài trăm đến hơn một nghìn hình
ảnh, cung cấp một mẫu đa dạng và rộng rãi của mỗi danh
mục.

(e) Pascal VOC: Tập dữ liệu Pascal VOC (Visual Object Classes)
[45] là một tập dữ liệu nổi tiếng để đánh giá các nhiệm vụ
phát hiện và phân đoạn đối tượng. Nó chứa các hình ảnh có
kích thước khác nhau, được chụp từ các cảnh tự nhiên, với
mỗi hình ảnh được gắn nhãn với các đối tượng của một hoặc
nhiều lớp trong 20 lớp khác nhau (như người, xe, chim, v.v.).
Tập dữ liệu chứa 9.963 hình ảnh và được chia thành ba tập:
huấn luyện, xác thực và kiểm tra. Tập huấn luyện được sử
dụng để huấn luyện các mô hình, tập xác thực được sử dụng
để điều chỉnh các siêu tham số, và tập kiểm tra được sử dụng
để đánh giá hiệu suất cuối cùng của các mô hình. Mỗi hình
ảnh trong tập dữ liệu được chú thích với các hộp giới hạn
thực tế cho các đối tượng trong hình ảnh và nhãn lớp cho
mỗi đối tượng. Tất cả các tập dữ liệu đã được phân vùng
một cách có hệ thống thành N tập con riêng biệt để tạo điều
kiện cho một phân tích chuyên biệt hơn, đảm bảo rằng mỗi
tập con chứa số lượng lớp bằng nhau. Chiến lược phân vùng
này khác với [22], đã phân phối tập dữ liệu thành các phần
trong đó tất cả các lớp đều có mặt bằng nhau trong mỗi phân
vùng.

Quá trình Đánh giá: Để đánh giá các mô hình của chúng
tôi, chúng tôi đã sử dụng độ chính xác, Eq. 1 (theo phần
trăm), đây là một thước đo đánh giá tiêu chuẩn cho phân
loại hình ảnh. Ngoài ra, để xác định định lượng độ thưa thớt
của các mô hình của chúng tôi, chúng tôi đã sử dụng tỷ lệ
tỉa, Eq. 2 (theo phần trăm) cung cấp tỷ lệ phần trăm của
các tham số là số không trong tổng số tham số của một mô
hình. Thuật toán tỉa lặp lại của chúng tôi chạy cho đến khi
đạt được TỷLệTỉa mong muốn. Mỗi vòng lặp của thuật toán
tỉa chạy trong năm mươi epoch, và mô hình tốt nhất (với
mất mát xác thực thấp nhất) được chọn.

Độ chính xác = (Số dự đoán đúng)/(Tổng số dự đoán) × 100    (1)

TỷLệTỉa = (Số tham số bằng không)/(Tổng số tham số) × 100    (2)

Đối với các nhiệm vụ phát hiện đối tượng, chúng tôi huấn
luyện 50 epoch tương tự như các nhiệm vụ phân loại hình
ảnh. Chúng tôi tính toán độ chính xác trung bình (mAP) được
sử dụng phổ biến để đánh giá khung phát hiện.

Kiến trúc Mô hình: Chúng tôi đã sử dụng ba mô hình để
tiến hành các thí nghiệm: Conv-3, ResNet-18, và MobileNetV2
(xem Bảng III). Chúng tôi đã sử dụng các kiến trúc Conv và
ResNet vì chúng đã được kiểm tra trong LTH [21]. Chúng
tôi đã sử dụng kiến trúc MobileNet vì nó là một mô hình
học sâu nhẹ với ít tham số và hiệu suất phân loại cao. Kiến
trúc mô hình Conv-3 là một phiên bản thu nhỏ của VGG [46]
bao gồm khoảng 0,37M tham số.

--- TRANG 8 ---
8 TẠP CHÍ IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, TẬP 00, SỐ 0, THÁNG 2020

80 90 100 50 55 60 65 70
5 10 15 20 25 30 35
Độ chính xác-LTH
Độ chính xác-COLT
Vòng Tỉa-LTH
Vòng Tỉa-COLT
(a) Conv3: CIFAR-10
Độ Thưa Thớt(%) Độ chính xác kiểm tra(%)
Vòng Tỉa

70 80 90 100 20 40 60 80
10 20 30 40 50
(b) MobilNetV2: CIFAR-10
Độ Thưa Thớt(%) Độ chính xác kiểm tra(%)
Vòng Tỉa

70 80 90 100 88 90 91 92 93
5 10 15 20
(c) ResNet18: CIFAR-10
Độ Thưa Thớt(%) Độ chính xác kiểm tra(%)
Vòng Tỉa

70 80 90 0 10 20 30 40
10 20 30 40 50 60 70
(d) Conv3: CIFAR-100
Độ Thưa Thớt(%) Độ chính xác kiểm tra(%)
Vòng Tỉa

70 80 90 0 20 40 60
20 40 60
(e) MobilNetV2: CIFAR-100
Độ Thưa Thớt(%) Độ chính xác kiểm tra(%)
Vòng Tỉa

80 90 100 55 60 65 70
5 10 15 20 25
(f) ResNet-18: CIFAR-100
Độ Thưa Thớt(%) Độ chính xác kiểm tra(%)
Vòng Tỉa

65 70 75 80 85 90 0 10 20 30 40 50
5 10 15 20 25 30 35
(g) MobilNetV2: TinyImageNet
Độ Thưa Thớt(%) Độ chính xác kiểm tra(%)
Vòng Tỉa

80 90 100 45 50 55 60
5 10 15 20
(h) ResNet18: TinyImageNet
Độ Thưa Thớt(%) Độ chính xác kiểm tra(%)
Vòng Tỉa

0 20 40 60 66 67 68 69
1 2 3 4 5 6 7
(i) ResNet18: ImageNet
Độ Thưa Thớt(%) Độ chính xác kiểm tra(%)
Vòng Tỉa

Hình 5: So sánh hiệu suất của LTH và COLT đề xuất của chúng tôi về độ chính xác (trục y bên trái) và vòng tỉa (trục y bên
phải) trong khi độ thưa thớt tăng (trong trục x). Chúng tôi đã sử dụng ba tập dữ liệu khác nhau (Cifar-10, Cifar-100, TinyImageNet)
và ba kiến trúc mô hình khác nhau (Conv3, MobileNetV2, ResNet18). Các đường màu đỏ biểu thị LTH, và các đường màu
xanh lá biểu thị COLT. Các đường cong chấm (- - -) đại diện cho số vòng tỉa cần thiết để đạt độ thưa thớt mong muốn,
trong khi các đường cong liền (—) đại diện cho độ chính xác khi độ thưa thớt tăng. Quan sát các đường cong liền, chúng
ta có thể thấy rằng hiệu suất của cả LTH và COLT bắt đầu giảm khi đạt được độ thưa thớt cao hơn. Tuy nhiên, từ các đường
cong chấm, chúng ta nhận thấy rằng số vòng tỉa cần thiết bởi COLT và LTH để đạt một độ thưa thớt cụ thể không giống
nhau. COLT yêu cầu ít vòng tỉa hơn LTH để đạt được cùng độ thưa thớt trong khi duy trì độ chính xác tương tự. Cụ thể,
đối với bất kỳ mức độ thưa thớt nào, tỉa dựa trên COLT tốn ít vòng tỉa hơn so với LTH. Do đó, COLT có thể tỉa nhanh
hơn LTH trong khi duy trì độ chính xác tốt.

Nó có ba lớp tích chập, tiếp theo là một lớp đầu ra tuyến tính.
Bên cạnh đó, chúng tôi bao gồm một lớp chuẩn hóa lô tiếp
theo là max-pooling sau mỗi lớp tích chập. Kiến trúc ResNet-18
bao gồm 72 lớp với 18 lớp kết nối dư thừa sâu [23] và khoảng
11,1M tham số. Kiến trúc MobileNetV2 [24] bao gồm 53 lớp
với 19 lớp cổ chai dư thừa và khoảng 2,2M tham số. Chúng
tôi sử dụng kỹ thuật khởi tạo trọng số Xavier hoặc Glorot
cho tất cả các mô hình [47]. Đối với tất cả ba mô hình, lớp
cuối cùng là một lớp kết nối đầy đủ đầu ra từ pooling trung
bình toàn cục của lớp tích chập cuối cùng đến số lớp đầu ra
như được thực hiện trong [21], [23]. Lớp tích chập cuối cùng
được pooling trung bình toàn cục để đảm bảo rằng chúng tôi
có thể chuyển các trọng số của một mô hình được huấn luyện
trên một tập dữ liệu sang tập dữ liệu khác có hình dạng đầu
vào khác nhau mà không mang lại bất kỳ sửa đổi nào cho
mô hình. Chúng tôi đã huấn luyện tất cả ba mô hình trên
Cifar-10 và Cifar-100, trong khi Conv-3 được loại trừ khỏi
Tiny ImageNet vì nó quá nông để huấn luyện trên Tiny ImageNet.

Chúng tôi cũng thực hiện tỉa dựa trên COLT cho các nhiệm
vụ phát hiện đối tượng dựa trên Faster-RCNN [48] và tập
dữ liệu Pascal VOC [45]. Mô hình Faster-RCNN bao gồm
hai thành phần chính: một Mạng Đề xuất Vùng (RPN) và
một mạng Fast R-CNN. RPN tạo ra các đề xuất vùng bằng
cách trượt một mạng nhỏ trên bản đồ đặc trưng tích chập
của hình ảnh đầu vào. Mạng Faster R-CNN sau đó phân loại
các vùng được đề xuất và tinh chỉnh các hộp giới hạn của
chúng. Chúng tôi sử dụng VGG16 [49] làm backbone, một
mạng trích xuất đặc trưng được chia sẻ giữa RPN và mạng
phát hiện chứa 13 lớp tích chập. Mạng Faster-RCNN bao
gồm một lớp pooling ROI và hai lớp kết nối đầy đủ để phân
loại các vùng được đề xuất và tinh chỉnh các hộp giới hạn
của chúng. Mô hình này có 136 triệu tham số, làm cho nó
trở thành một

--- TRANG 9 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 9

60 80 100 40 50 60 70
Nguồn Vé
Độ chính xác- Morcos et al.
Độ chính xác - COLT
a) Conv3: Chuyển CIFAR-100 sang CIFAR-10
Tỷ Lệ Tỉa (%) Độ Chính Xác Kiểm Tra (%)

60 80 100 20 40 60 80
Nguồn Vé
Độ chính xác- Morcos et al.
Độ chính xác - COLT
b) MobileNetV2 : Chuyển CIFAR-100 sang CIFAR-10
Tỷ Lệ Tỉa (%) Độ Chính Xác Kiểm Tra (%)

50 60 70 80 90 100 66 68 70 72
Nguồn Vé
Độ chính xác- Morcos et al.
Độ chính xác - COLT
c) ResNet18 : Chuyển TinyImageNet sang CIFAR-100
Tỷ Lệ Tỉa (%) Độ Chính Xác Kiểm Tra (%)

50 60 70 80 90 75 80 85 90
Nguồn Vé
Độ chính xác- Morcos et al.
Độ chính xác - COLT
d) MobileNetV2 : Chuyển TinyImageNet sang CIFAR-10
Tỷ Lệ Tỉa (%) Độ Chính Xác Kiểm Tra (%)

50 60 70 80 90 100 90 91 92 93
Nguồn Vé
Độ chính xác- Morcos et al.
Độ chính xác - COLT
e) ResNet18 : Chuyển TinyImageNet sang CIFAR-10
Tỷ Lệ Tỉa (%) Độ Chính Xác Kiểm Tra (%)

50 60 70 80 90 50 55 60 65
Nguồn Vé
Độ chính xác- Morcos et al.
Độ chính xác - COLT
f) MobileNetV2 : Chuyển TinyImageNet sang CIFAR-100
Tỷ Lệ Tỉa (%) Độ Chính Xác Kiểm Tra (%)

Hình 6: So sánh hiệu suất của COLT với Morocos et al. [22] khi chuyển các vé được tính từ một tập dữ liệu sang tập khác.
Hiệu suất của cả hai phương pháp giảm nhẹ khi các vé đạt độ thưa thớt cao. Tuy nhiên, khi các vé được chuyển từ một tập
dữ liệu lớn (TinyImageNet) sang các tập dữ liệu nhỏ (Cifar-10, Cifar-100), COLT (đường cong xanh lá) liên tục vượt trội
hơn Morcos et al. (đường cong đỏ) ở độ thưa thớt cao.

mô hình lớn và tốn kém về mặt tính toán.

Chi tiết Triển khai: Sau khi huấn luyện và tỉa, trọng số phải
được đặt lại về giá trị ban đầu của chúng khi bắt đầu huấn
luyện (vòng lặp huấn luyện 0) để tạo ra các vé thắng cuộc
[21]. Tuy nhiên, Frankle et al. [21] đã phát hiện rằng điều
này chỉ hoạt động đối với các mô hình nông. Đối với các mô
hình sâu hơn, cần có học tỷ lệ học khởi động, cùng với việc
đặt lại trọng số để tạo ra các vé thắng cuộc. Vì vậy, các thí
nghiệm của chúng tôi sử dụng khởi động tỷ lệ học cho epoch
đầu tiên. Chúng tôi tỉa toàn cục chỉ các lớp tích chập với tỷ
lệ 0,2 (20%) cho LTH và 0,15 (15%) cho COLT mỗi vòng
lặp. Tương tự, chúng tôi không tỉa các lớp đầu ra tuyến tính
vì chúng chỉ chiếm một phần rất nhỏ của toàn bộ mạng. Mỗi
mô hình trên các tập dữ liệu cifar10, cifar100, và Tiny Imagenet
được huấn luyện trong 50 epoch với kích thước lô 64 và tỷ
lệ học 0,1. Tỷ lệ học được làm nguội bằng một hệ số 5 ở
epoch 25, 35, và 45. Trong khi huấn luyện Imagenet với mô
hình ResNet-18, chúng tôi huấn luyện mô hình trong 90 epoch
với kích thước lô 512 và tỷ lệ học 0,2. Chúng tôi đã sử dụng
một bộ lập lịch tỷ lệ học để làm nguội tỷ lệ học ở epoch 30,
60 và 80 bằng một hệ số 5. Tất cả các mô hình, ngoại trừ
Conv-3, được huấn luyện bằng Stochastic Gradient Descent
(SGD) với momentum 0,9 và weight decay 0,0005. Conv3
được huấn luyện bằng bộ tối ưu hóa Adam với beta 0,9 và
0,999 và weight decay 0,0001. Đối với nhiệm vụ phát hiện
đối tượng, chúng tôi triển khai mô hình Faster-RCNN. Để
huấn luyện mô hình, chúng tôi sử dụng bộ tối ưu hóa Adam
với tỷ lệ học 0,0001 và một bộ lập lịch tỷ lệ học cosine annealing
với tỷ lệ học tối thiểu 1,02e-06. Chúng tôi sử dụng kích thước
lô 4 và huấn luyện mô hình trong 50 epoch. Hình dạng đầu
vào là [600,600], và chúng tôi tỉa 15% trọng số của các lớp
tích chập và 20% trọng số của các lớp kết nối đầy đủ mỗi
vòng tỉa. Chúng tôi triển khai công trình này với framework
PyTorch sử dụng một GPU RTX 3090 duy nhất.

B. Các phương pháp so sánh

Chúng tôi so sánh công trình của chúng tôi với ba phương
pháp được thảo luận như sau: (1) Mạng chưa tỉa: Mạng chưa
tỉa là CNN gốc (Conv-3/Mobilenetv2/Resnet-18) được huấn
luyện cho đến hội tụ, trong đó không có chiến lược tỉa nào
được xem xét trong quá trình học. Chúng tôi khởi tạo trọng
số của chúng bằng kỹ thuật khởi tạo trọng số Xavier/Glorot
[47]. Chúng tôi giả định rằng một mạng chưa tỉa thường có
quá nhiều tham số và do đó cần một phương pháp tỉa để loại
bỏ các trọng số không cần thiết. (2) Giả thuyết Vé Số (LTH)
[21]: Để so sánh với LTH, chúng tôi cũng tạo ra các vé LTH.
Chúng tôi tỉa 20% trọng số sau mỗi vòng lặp huấn luyện và
sau đó tua chúng lại về trạng thái ban đầu. Sau đó chúng
tôi so sánh hiệu suất của chúng với các vé COLT trong Mục
IV-C. [21] và [22] đã chỉ ra rằng các vé ngẫu nhiên có và
không có mặt nạ bảo tồn dẫn đến hiệu suất thấp hơn so với
các vé thắng cuộc với các tham số bằng nhau. Do đó, chúng
tôi đã bỏ qua việc sử dụng các vé ngẫu nhiên trong khi đánh
giá các vé COLT và trực tiếp so sánh các vé COLT với các
vé LTH. (3) Morcos et al. [22]: Các vé thắng cuộc tốn kém
về mặt tính toán để tạo ra do chu trình huấn luyện-tỉa-tua
lại lặp đi lặp lại. Morcos et al. đã chỉ ra rằng một khi được
tạo từ một tập dữ liệu, các vé thắng cuộc có thể được chuyển
sang tập dữ liệu khác, bỏ qua nhu cầu tìm các vé thắng cuộc
riêng biệt cho mỗi tập dữ liệu. Cụ thể, họ quan sát rằng việc
chuyển các vé thắng cuộc từ một tập dữ liệu sang tập khác
có thể đạt được hiệu suất gần như tương tự với các vé thắng
cuộc được tạo trên cùng một tập dữ liệu. Họ đã chỉ ra rằng
các vé thắng cuộc tổng quát hóa tốt trong miền hình ảnh tự
nhiên. Hơn nữa, họ cũng chứng minh rằng các vé thắng cuộc
được tạo từ các tập dữ liệu lớn liên tục chuyển tốt hơn so
với những vé được tạo bằng các tập dữ liệu nhỏ.

COLT so với các phương pháp nén mạng khác: Mặc dù có
nhiều kỹ thuật nén mạng khả dụng cho CNN, chẳng hạn như
chưng cất kiến thức [7], lượng tử hóa [50], và tỉa có cấu trúc
[17], những chiến lược này thường đòi hỏi tiền huấn luyện
hoàn chỉnh của mạng hoặc tạo ra các mô hình phải trải qua
tinh chỉnh. Điều này khác với mục tiêu của chúng tôi là tìm
các mạng con thưa thớt có thể huấn luyện từ đầu. Do đó,
phù hợp với công trình trước đó [51], [52], chúng tôi không
so sánh công trình của chúng tôi với các phương pháp nén
mạng khác. Đóng góp chính của COLT nằm ở việc cải thiện
quá trình tìm vé chính nó trong khi duy trì lợi thế chính của
LTH. Trọng tâm này vào độ thưa thớt thời gian khởi tạo và
khả năng huấn luyện đặt các phương pháp vé số ra ngoài
trong bối cảnh rộng hơn của nén mạng.

C. Kết quả chính

Chúng tôi tạo ra các vé COLT và LTH cho ResNet-18 và
MobileNetV2 trên Cifar-10, Cifar-100, và Tiny ImageNet.
Đối với Conv-3, chúng tôi thu được các vé chỉ cho Cifar-10
và Cifar-100 theo [21]. Đối với thí nghiệm chuyển trọng số,
chúng tôi chuyển các vé được tính từ Tiny ImageNet sang
Cifar-10 và Cifar-100. Một lần nữa, chúng tôi chuyển các
vé từ Cifar-100 sang Cifar-10. Tất cả các thí nghiệm tạo vé
COLT và LTH được lặp lại nhiều lần với khởi tạo ngẫu nhiên
khác nhau, và chúng tôi báo cáo điểm số tốt nhất.

Kết quả trên Cifar-10: Chúng tôi quan sát rằng độ chính
xác của COLT và LTH ngang bằng nhau đối với ResNet-18.
Đối với các vé Conv-3 (Hình 5(a)), chúng tôi nhận thấy một
bức tranh tương tự với cả COLT và LTH đều hoạt động ngang
bằng nhau. Đối với MobileNetV2 (Hình 5(b)), chúng ta thấy
rằng hiệu suất của COLT và LTH tương tự cho đến tỷ lệ tỉa
70%, và sau đó hiệu suất của các vé COLT bắt đầu giảm so
với các vé LTH. Đối với cả hai phương pháp, hiệu suất của
các vé giảm mạnh khi tỷ lệ tỉa cao hơn 95% đối với Conv-3,
90% đối với MobileNetV2, và 96% đối với ResNet-18. Đối
với tất cả các vé, COLT liên tục yêu cầu ít vòng tỉa hơn LTH
để tạo ra một vé thưa thớt cụ thể. Ví dụ, để đạt độ thưa thớt
70%, COLT cần 3, 1, và 2 vòng ít hơn so với LTH trên Conv-3,
MobileNetV2, và ResNet-18, tương ứng. Tương tự, để đạt độ
thưa thớt 90%, COLT yêu cầu 4, 2, và 3 vòng ít hơn so với
LTH trên Conv-3, MobileNetV2, và ResNet-18, tương ứng.
Cuối cùng, để đạt độ thưa thớt 98%, COLT cần 9, 2, và 5
vòng ít hơn so với LTH trên Conv-3, MobileNetV2, và ResNet-18,
tương ứng. Do việc tỉa các trọng số không trùng lặp trong
COLT, nó liên tục yêu cầu ít vòng hơn LTH để tạo ra các
vé thắng cuộc thưa thớt. COLT cũng giữ được hiệu suất
tương tự như LTH vì những trọng số không trùng lặp này
không quan trọng, và việc loại trừ những trọng số đó không
ảnh hưởng nhiều đến hiệu suất tổng thể.

Kết quả trên Cifar-100: Quan sát Hình 5(f), hiệu suất của
các vé COLT và LTH tương tự cho đến tỷ lệ tỉa 80% đối với
ResNet-18. Tuy nhiên, sau khi tỉa 80%, các vé dựa trên COLT
hoạt động tốt hơn so với các vé LTH. Đối với MobileNetV2
(Hình 5(e)) và Conv-3 Hình 5(d), hiệu suất của các vé COLT
và LTH gần như tương tự, với LTH hơi vượt trội hơn COLT
ở một số tỷ lệ tỉa. Ngoài ra, độ chính xác của các vé LTH
và COLT bắt đầu giảm mạnh khi tỷ lệ tỉa lớn hơn 95% đối
với Resnet-18, 85% đối với MobileNetV2, và 90% đối với
Conv-3. Bên cạnh đó, COLT thường xuyên tạo ra các vé
thắng cuộc trong ít vòng tỉa hơn LTH. Để đạt độ thưa thớt
70%, COLT yêu cầu 3, 2, và 2 vòng ít hơn so với LTH trên
Conv-3, MobileNetV2, và ResNet-18, tương ứng. Để đạt độ
thưa thớt 90%, COLT yêu cầu 6, 6, và 3 vòng ít hơn so với
LTH trên Conv-3, MobileNetV2, và ResNet-18, tương ứng.
Cuối cùng, để đạt độ thưa thớt 98%, COLT cần 9, 8, và 6
vòng ít hơn so với LTH trên Conv-3, MobileNetV2, và ResNet-18,
tương ứng. Điều này trở nên khả thi do chiến lược tỉa tích
cực các trọng số không khớp của COLT tại mỗi vòng. Hơn
nữa, COLT bảo tồn độ chính xác vì các trọng số không khớp
là không đáng kể và không cản trở độ chính xác.

Kết quả trên Tiny ImageNet: Phân tích Hình 5(g) và 5(h),
chúng ta có thể quan sát rằng hiệu suất của các vé COLT
và LTH tương tự đối với hầu hết các vé, với các vé COLT
hơi hoạt động tốt hơn so với các Vé LTH ở một số độ thưa
thớt. Ngoài ra, độ chính xác của cả hai vé giảm đáng kể từ
85% đối với ResNet-18 và từ 75% đối với MobileNetV2.
Nhìn vào các vòng tỉa, chúng ta có thể thấy rằng COLT luôn
tạo ra các vé thưa thớt ở ít vòng lặp hơn LTH. Để đạt độ
thưa thớt 70%, COLT yêu cầu hai vòng ít hơn so với LTH
trên MobileNetV2 và ResNet-18. Để đạt độ thưa thớt 90%,
COLT yêu cầu 9 và 3 vòng ít hơn so với LTH trên MobileNetV2
và ResNet-18, tương ứng. Cuối cùng, để đạt độ thưa thớt
98%, COLT yêu cầu 4 và 6 vòng ít hơn so với LTH trên
MobileNetV2 và ResNet-18, tương ứng. Vì COLT tỉa tỷ lệ
cao hơn các trọng số dư thừa so với LTH tại mỗi vòng lặp,
nó có thể tạo ra các vé thắng cuộc trong ít vòng hơn LTH
mà không ảnh hưởng đến độ chính xác.

Kết quả trên ImageNet: Trong một phân tích mở rộng trên
ImageNet sử dụng ResNet-18, cả hai phương pháp COLT
và LTH đều duy trì độ chính xác gần với baseline sau khi
giảm mô hình đáng kể. Ban đầu, không có mất mát độ chính
xác (69,7%) được quan sát mà không có tỉa, như mong đợi.
Với độ thưa thớt tăng, độ chính xác của COLT giảm nhẹ
xuống 69,3% (ở tỷ lệ tỉa 26,5%) và tiếp tục xuống 66,1%
(ở tỷ lệ tỉa 70,11%). LTH cho thấy sự suy giảm tương tự,
đạt độ chính xác 66,8% (ở tỷ lệ tỉa 70,52% với một biến
động nhỏ). Hiệu quả của COLT rõ ràng khi nó đạt được tỉa
70% chỉ trong bốn vòng, ít hơn hai vòng so với LTH. Điều
này nhấn mạnh việc tỉa tích cực nhưng hiệu quả của COLT,
cung cấp lợi thế về độ trễ và hiệu quả mô hình. Kết quả ngụ
ý rằng trên các tập dữ liệu quy mô lớn, việc loại bỏ các trọng
số (không nhất quán qua các mô hình) được huấn luyện trên
các tập con khác nhau của các lớp và những trọng số có độ
lớn nhỏ hơn có tác động tối thiểu đến hiệu suất mô hình.
Điều này gợi ý về bản chất dư thừa của những trọng số này
trong việc duy trì mức độ chính xác cao.

Tác động của Phân vùng Dữ liệu Không đồng nhất (Non-IID):
Để tạo phân vùng dữ liệu non-IID, chúng tôi triển khai COLT_niid,
loại bỏ ngẫu nhiên 0-30% mẫu từ mỗi lớp. Phương pháp này
giới thiệu sự mất cân bằng dữ liệu qua các lớp, vì một số lớp
giữ lại tất cả các mẫu của chúng trong khi những lớp khác
mất đến 30%. Tập dữ liệu kết quả duy trì tất cả các lớp gốc
nhưng với

--- TRANG 10 ---
10 TẠP CHÍ IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, TẬP 00, SỐ 0, THÁNG 2020

BẢNG IV: Nghiên cứu ablation về nhiều phân vùng (COLT-2,
COLT-3 và COLT-4) sử dụng kiến trúc ResNet18.

Phương pháp  Độ chính xác ↑  Tỷ lệ Tỉa ↑  Độ trễ ↓      Số vòng tỉa ↓
              (%)            (%)           (phút)       
CIFAR10
Chưa tỉa      94.4           0             -            -
LTH           92.3           97.7          320          17
COLT-2        92.4           97.7          276          12
COLT-3        92.2           97.3          225          9
COLT-4        89.2           97            148          6

CIFAR100
Chưa tỉa      73.3           0             -            -
LTH           66.2           97.3          479          17
COLT-2        68.4           97.4          355          12
COLT-3        68.0           97.3          236          8
COLT-4        67.7           97.2          188          6

TinyImageNet
Chưa tỉa      59.9           0             -            -
LTH           53.2           97.3          2625         18
COLT-2        53.9           97.4          1756         12
COLT-3        53.4           97.6          1266         8
COLT-4        53.7           97            930          6

ImageNet
Chưa tỉa      69.7           0             -            -
LTH           66.8           70.5          10320        6
COLT-2        66.1           70.1          6960         4
COLT-4        55.7           74.1          5760         3

BẢNG V: So sánh hiệu suất của LTH và COLT dưới phân
phối dữ liệu IID và non-IID trên CIFAR-100 sử dụng ResNet-18.
Các giá trị trong ngoặc cho thấy sự khác biệt độ chính xác
giữa các kịch bản IID và non-IID.

Tỷ lệ tỉa (%)  LTH                    COLT
               IID      non-IID       IID      non-IID
85.2           71.55    70.39 (-1.16) 72.84    71.33 (-1.51)
89.1           71.42    69.88 (-1.54) 72.88    70.64 (-2.24)
92.1           70.59    69.28 (-1.31) 71.91    70.30 (-1.61)

kích thước mẫu khác nhau, tạo ra một phân phối không đồng
nhất thách thức khả năng của mô hình học từ dữ liệu được
phân phối không đều. Chúng tôi kiểm tra cả COLT và LTH
bằng ResNet-18 trên CIFAR-100, so sánh hiệu suất của chúng
ở các tỷ lệ tỉa khác nhau dưới cả thiết lập IID và non-IID.
Kết quả, được hiển thị trong Bảng V, cho thấy rằng cả hai
phương pháp đều trải qua một số suy giảm hiệu suất dưới
điều kiện non-IID. Ở tỷ lệ tỉa 89,1%, LTH cho thấy sự giảm
1,54 điểm phần trăm (từ 71,42% xuống 69,88%), trong khi
COLT trải qua sự giảm 2,24 điểm phần trăm (từ 72,88% xuống
70,64%). Mặc dù có sự suy giảm này, cả hai phương pháp
đều duy trì mức hiệu suất hợp lý, với COLT cho thấy khả
năng chống chịu tương đương với LTH trong việc xử lý tính
không đồng nhất của dữ liệu.

D. Nhiều phân vùng

Trong phần này, chúng tôi đánh giá hiệu suất của COLT với
nhiều phân vùng tập dữ liệu, so sánh COLT-2, COLT-3, và
COLT-4, đại diện cho phương pháp cho 2, 3, và 4 phân vùng,
tương ứng, với phương pháp LTH được sử dụng rộng rãi bằng
các mô hình ResNet18 trên nhiều tập dữ liệu. Kết quả chi
tiết được hiển thị trong Bảng IV. Kết quả cho thấy rằng phương
pháp tỉa dựa trên phân vùng hoạt động khác nhau cho các
tập dữ liệu có kích thước và độ phức tạp khác nhau. COLT-3
(phân vùng ba phần của tập dữ liệu) đã cho thấy hiệu suất
cân bằng với mất mát độ chính xác nhỏ và tỷ lệ tỉa tốt qua
CIFAR-10 (độ chính xác 92,2%, tỉa 97,3%), CIFAR-100
(68,0%, 97,3%), và Tiny ImageNet (53,4%, 97,6%). Tuy nhiên,
COLT-4 cho thấy độ chính xác giảm trên CIFAR-10 (88,7%)
so với LTH (91,3%) do kích thước nhỏ của CIFAR-10, làm
cho việc duy trì các trọng số thông tin với việc phân vùng
tăng trở nên khó khăn. Ngược lại, đối với CIFAR-100, độ
chính xác của COLT-4 (67,7%) thấp hơn một chút so với
COLT-3 (68,0%) nhưng vẫn cao hơn LTH (66,2%). Đối với
Tiny ImageNet, COLT-4 (53,7%) hơi vượt trội hơn LTH
(53,2%), cho thấy các tập dữ liệu phức tạp hơn có thể bảo
tồn các trọng số thiết yếu ngay cả với nhiều phân vùng hơn.
Gợi ý các tập dữ liệu lớn hơn có thể xử lý tốt hơn nhiều
phân vùng hơn. Trên ImageNet, tập dữ liệu thách thức nhất,
việc tỉa tích cực với COLT-4 dẫn đến sự giảm độ chính xác
đáng kể (55,7%) mặc dù có tỷ lệ tỉa cao (74,01%) và độ trễ
thấp nhất (5760 phút), ngụ ý mất mát các trọng số quan
trọng. Có khả năng do mất mát các trọng số quan trọng cho
các đặc trưng được đại diện ít trong tập dữ liệu [53]. Mặt
khác, COLT-2 đạt được độ chính xác gần hơn với LTH (66,1%
so với 66,8%) với tỷ lệ tỉa tốt hơn (70,11%) và độ trễ thấp
hơn (6960 phút), cho thấy rằng các chiến lược tỉa vừa phải
như COLT-2 có thể giữ lại các đặc trưng quan trọng trên
các tập dữ liệu phức tạp, tạo ra sự cân bằng giữa kích thước
mô hình, hiệu quả và hiệu suất.

E. Chuyển đổi các vé LTH & COLT qua các tập dữ liệu

Quá trình lặp lại của việc tạo vé có thể tốn kém. Để giảm
thiểu vấn đề này, Morcos et al. [22] cho thấy rằng các vé
thắng cuộc (mạng con đã tỉa) được tạo trên các tập dữ liệu
lớn hơn, phức tạp hơn có thể tổng quát hóa tốt hơn đáng kể
so với những vé được tạo trên các tập dữ liệu nhỏ. Nói cách
khác, chúng ta không cần tạo vé cho mỗi tập dữ liệu. Các
vé được tạo từ một tập dữ liệu lớn hơn có thể được chuyển
đổi/sử dụng lại để huấn luyện cùng một mạng cho một tập
dữ liệu nhỏ hơn. Vì lý do này, chúng tôi thực hiện các thí
nghiệm để chuyển các vé COLT và LTH của Tiny ImageNet
sang Cifar-10 và Cifar-100. Vì chúng tôi đang sử dụng các
tập dữ liệu với hình dạng đầu vào khác nhau, sẽ có sự không
khớp trọng số khi chuyển trọng số của lớp tuyến tính. Để
khắc phục vấn đề này, chúng tôi đã pooling trung bình toàn
cục đầu ra từ lớp tích chập cuối cùng trước khi truyền nó
qua lớp đầu ra dày đặc. [22] tiếp tục cho thấy rằng chỉ việc
tăng số lượng lớp trong khi giữ kích thước tập dữ liệu cố
định có thể cải thiện đáng kể khả năng tổng quát hóa của
các vé thắng cuộc. Để xác minh điểm này, trong công trình
của chúng tôi, chúng tôi đã chuyển các vé COLT và LTH
của Cifar-100 sang Cifar-10 vì chúng có cùng kích thước,
nhưng Cifar-100 có nhiều lớp hơn Cifar-10.

Quan sát Hình 6, chúng ta có thể thấy rằng độ chính xác
từ từ xấu đi khi COLT và Morcos et al. đạt độ thưa thớt cao.
Tuy nhiên, COLT liên tục hoạt động tốt hơn Morcos et al.
ở độ thưa thớt cao khi các vé TinyImageNet được chuyển
sang Cifar-10 & Cifar-100. Ngoài ra, cả hai phương pháp
đều ngang bằng và hoạt động tốt khi các vé Cifar-100 được
chuyển sang Cifar-10 cho tất cả độ thưa thớt. Do đó, chúng
tôi khẳng định rằng các vé COLT có khả năng tổng quát hóa
cao và khả năng chuyển đổi sang các tập dữ liệu khác của
miền tương tự.

Người ta có thể so sánh hiệu suất của các vé thường xuyên
(COLT) với các vé được chuyển bằng cách phân tích Hình
5 và 6. Chúng tôi quan sát rằng xu hướng hiệu suất cho các
vé thường xuyên và được chuyển là tương tự qua các kiến
trúc và tập dữ liệu. Nếu một vé thường xuyên hoạt động tốt
trên một tập dữ liệu, vé được chuyển hoạt động

--- TRANG 11 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 11

BẢNG VI: Kết quả tiên tiến mô tả so sánh hiệu suất của các vé được tạo bởi mạng chưa tỉa gốc, LTH, và COLT. Các vé
được tạo và đánh giá trên TinyImageNet và sau đó chuyển sang Cifar-10 và Cifar-100. Chúng tôi so sánh các vé thưa thớt
từ ba phương pháp trên các kiến trúc ResNet-18 và MobileNetV2. ↑(↓) có nghĩa là cao hơn (thấp hơn) là tốt hơn. "-" biểu
thị kết quả không áp dụng ở đó.

Mô hình: ResNet18 và Tập dữ liệu: Tiny ImageNet
Phương pháp   Độ chính xác↑  Tỷ lệ Tỉa ↑  Độ trễ ↓      Số vòng Tỉa ↓
              (%)           (%)          (phút)        
Chưa tỉa      59.9          0            -             -
LTH           53.2          97.3         2625          18
Của chúng tôi (COLT) 53.9   97.2         1756          12

Vé chuyển từ TinyImageNet sang Cifar10
Chưa tỉa      93.4          0            -             -
Morcos et al. [22] 89.9     97.1         -             -
Của chúng tôi (COLT) 90.5   97.1         -             -

Vé chuyển từ TinyImageNet sang Cifar100
Chưa tỉa      73.2          0            -             -
Morcos et al. [22] 64.8     97.3         -             -
Của chúng tôi (COLT) 65.2   97.4         -             -

Mô hình: MobileNetV2 và Tập dữ liệu: Tiny ImageNet
              Độ chính xác↑  Tỷ lệ Tỉa ↑  Độ trễ ↓      Số vòng Tỉa ↓
              (%)           (%)          (phút)        
              56.1          0            -             -
              54.7          70.3         856           7
              55.2          71.0         354           5

Vé chuyển từ TinyImageNet sang Cifar10
              90.6          0            -             -
              88.9          78.0         -             -
              89.2          78.8         -             -

Vé chuyển từ TinyImageNet sang Cifar100
              65.8          0            -             -
              64.1          74.1         -             -
              65.5          74.9         -             -

tốt, và ngược lại. Điểm mấu chốt là chúng ta có thể tạo ra
các vé COLT từ một tập dữ liệu và sau đó sử dụng cùng vé
đó để huấn luyện tập dữ liệu khác của miền tương tự mà
không ảnh hưởng đến hiệu suất mạng.

F. So sánh tiên tiến

Bảng VI so sánh kết quả của mạng chưa tỉa, các phương
pháp COLT và LTH về số vòng tỉa, độ chính xác và độ trễ.
Đối với thí nghiệm khả năng chuyển đổi, các vé được tạo
bằng TinyImageNet và chuyển sang Cifar10 và Cifar100.
Lặp lại [22], việc chuyển vé từ tập dữ liệu lớn hơn (TinyImageNet)
sang tập dữ liệu nhỏ hơn (Cifar10) cải thiện hiệu suất. Các
nghiên cứu của chúng tôi trên ResNet-18 và MobileNetV2
hỗ trợ điều này. Đáng chú ý, ResNet-18 duy trì độ chính
xác với 97,2% tham số được tỉa, cho thấy rằng chỉ 2,8%
trọng số là cần thiết cho độ chính xác thỏa đáng. COLT vượt
trội hơn LTH về độ chính xác ở độ thưa thớt cao với ít vòng
tỉa hơn và ít thời gian huấn luyện hơn. Tuy nhiên, nó kém
hơn độ chính xác của mạng chưa tỉa 5-6%, có khả năng vì
việc tỉa 97% trọng số có thể loại bỏ một số trọng số quan
trọng, dẫn đến giảm độ chính xác. Ngoài ra, có nguy cơ
cao sụp đổ lớp đối với không có cấu trúc ở độ thưa thớt
cao. Tanaka et al. [43] tuyên bố rằng tỉa độ lớn lặp lại có
thể tránh điều này bằng cách thực thi một định luật bảo tồn
thông qua gradient descent, làm tăng điểm số độ lớn cho
các lớp lớn hơn trong suốt quá trình tỉa. Trong khía cạnh
này, COLT hoạt động tương đối tốt hơn LTH, ngay cả khi
nó nhận được ít vòng lặp hơn để tỉa ở độ thưa thớt cao. Do
đó, chúng tôi khẳng định rằng COLT có thể giữ lại "trọng
số tốt" ngay cả sau khi tỉa ở độ thưa thớt cao. Mặt khác,
MobileNet có chín triệu trọng số ít hơn so với ResNet18.
Do đó, không giống như mạng chưa tỉa, cả LTH và COLT
đều không đạt được độ chính xác tương đương ở độ thưa
thớt cao. Tuy nhiên, mạng chưa tỉa và COLT (với tỉa 71%)
giữ độ chính xác tương tự. Tương tự, trong ResNet18, chúng
tôi tạo ra các vé nhanh hơn với ít vòng tỉa hơn so với LTH.
Trong trường hợp này, LTH yêu cầu tám vòng tỉa và 856
phút cho một vé thưa thớt 70,3% với độ chính xác 54,7%
trên TinyImageNet. Ngược lại, COLT chỉ cần năm vòng và
354 phút cho một vé thưa thớt 71% với độ chính xác 55,2%.
Đối với MobileNetV2 và ResNet18, COLT nhanh hơn đáng
kể so với LTH, với ít vòng hơn và tính toán gradient nhanh
hơn. Các vé của COLT thưa thớt hơn. Do đó, cần ít gradient
hơn, cho phép tạo vé nhanh hơn nhiều so với LTH.

Một xu hướng tương tự được quan sát khi các vé được chuyển
sang Cifar-10 và Cifar-100 cho các vé độ thưa thớt trung
bình (tỷ lệ tỉa khoảng 80%). Đặc biệt đối với MobileNetV2,
cả COLT và Morcos et al. [22] đều có độ chính xác gần với
mạng chưa tỉa gốc. Điều này ngụ ý rằng chúng ta có thể
áp dụng một mạng đã được tỉa trên một tập dữ liệu và chuyển
nó sang tập khác với độ chính xác có thể so sánh với mạng
chưa tỉa. Hơn nữa, chúng ta không cần tài nguyên bổ sung
để tỉa mạng cho mỗi tập dữ liệu riêng lẻ. Ngược lại, đối
với các vé độ thưa thớt cao, độ chính xác giảm khoảng 3%
khi chuyển sang Cifar-10 và khoảng 8% khi chuyển sang
Cifar-100. Tuy nhiên, COLT dường như trở nên chung chung
hơn so với Morcos et al. [22]. Điều này là do COLT giữ lại
các trọng số trùng lặp của hai mạng được huấn luyện trên
hai phân chia một nửa của một tập dữ liệu, làm cho các vé
COLT mạnh mẽ hơn đối với các biến thể hình ảnh.

G. Ngoài phân loại

Ngoài các nhiệm vụ phân loại hình ảnh, chúng tôi cũng đánh
giá hiệu quả của COLT trên nhiệm vụ phát hiện đối tượng
Pascal VOC bằng mô hình Faster-RCNN với backbone VGG.
Nhiệm vụ này phức tạp hơn so với phân loại hình ảnh, vì
nó liên quan đến việc xác định đối tượng và vị trí của nó
trong hình ảnh. Mô hình tương đối lớn, với tổng cộng 113
triệu tham số. Trong số đó, 13 triệu tham số được gán cho
các lớp CNN chịu tỉa để giảm tài nguyên tính toán. Trong
Bảng VIII, chúng tôi so sánh hiệu suất của COLT với LTH
trên tập dữ liệu Pascal VOC và thấy rằng COLT vượt trội
hơn LTH về vòng tỉa, độ trễ và độ chính xác. Chúng tôi cũng
báo cáo mAP theo lớp trong Bảng VII để cho thấy rằng sau
khi tỉa 68%, mAP của từng lớp tương tự như mạng chưa tỉa.
Kết quả của chúng tôi chứng minh rằng COLT và LTH có
thể đạt được độ chính xác tương tự như mạng Chưa tỉa, với
mAP tối đa lần lượt là 63,8% và 63,7%, trong khi tỉa lên
đến 79% mạng.

--- TRANG 12 ---
12 TẠP CHÍ IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, TẬP 00, SỐ 0, THÁNG 2020

BẢNG VII: mAP theo lớp của tập dữ liệu PASCAL VOC sau khi tỉa 68%.

aero. bicycle bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv
Chưa tỉa 64.2 77.4 53.2 56.5 33.7 77.8 82.1 69.4 46.4 56.8 55.3 67.3 82.4 82.1 76.3 36.9 49.8 54.7 79.7 65.3
LTH 63.4 79.1 51.4 56.7 32.4 77.7 82.1 71.3 48.1 60.4 55.7 68.6 81.4 81.5 77.3 31.3 46.5 54.5 78.7 68.4
COLT 63.5 77.3 54.1 60.5 34.9 77.4 81.5 69.9 44.7 61.4 55.3 67.1 82.2 83.1 77.4 35.9 52.7 52.4 77.7 67.8

BẢNG VIII: Tỉa mô hình Faster-RCNN trong khi thực hiện
các nhiệm vụ phát hiện đối tượng trên tập dữ liệu PASCAL VOC.

Phương pháp mAP↑  Tỷ lệ Tỉa ↑  Độ trễ ↓      Số vòng tỉa ↓
           (%)    (%)          (giờ)        

Chưa tỉa   63.4   0            -            -
LTH        60.8   36.0         26.0         2
COLT       60.6   32.2         14.3         1
LTH        61.6   48.8         39.3         3
COLT       62.5   53.5         28.8         2
LTH        63.4   67.0         65.5         5
COLT       63.8   68.6         43.1         3
LTH        63.7   79.0         91.7         7
COLT       63.3   79.1         58.0         4

0 1 1 2 2 3 3 4 4 5 5 6 6 7 8 9
23.7 48.0 60.3 72.9 79.3 85.8 89.1 92.4 94.1 95.9 96.8 97.7 98.1 98.6 98.9 99.1 0 20 40 60 80 100
0 5 10 15 20 25
ResNet18 - Cifar100
Tỷ lệ Tương đồng
Vòng Tỉa-COLT
Vòng Tỉa-LTH
Độ Thưa Thớt (%) Tỷ lệ Tương đồng Trọng số Đã Tỉa (%)
Vòng Tỉa

Hình 7: Sự tương đồng (thanh màu xanh) của các trọng số
đã tỉa giữa các vé thưa thớt được tính bởi phương pháp COLT
và LTH. Với sự gia tăng của độ thưa thớt, cả hai loại phương
pháp đều tỉa các trọng số tương tự. Để đạt được điều này,
COLT yêu cầu ít vòng huấn luyện hơn LTH (đường cong đỏ
so với xanh lá).

Điều này cho thấy rằng phương pháp đề xuất của chúng tôi
có thể đạt được mức nén đáng kể với mất mát độ chính xác
không đáng kể. Khi so sánh COLT với LTH, chúng tôi quan
sát rằng COLT đạt được độ chính xác tương tự hoặc tốt hơn
LTH, yêu cầu ít vòng tỉa hơn đáng kể và ít độ trễ hơn.

H. Thảo luận

Phân tích Tương đồng Trọng số: Hình 7 mô tả so sánh giữa
LTH và COLT về tương đồng trọng số đã tỉa và vòng tỉa
cho mô hình ResNet-18 trên tập dữ liệu Cifar-100. Trục X
biểu thị độ thưa thớt theo phần trăm. Trục Y bên trái biểu
thị tương đồng tỷ lệ tỉa của các trọng số chưa tỉa, và trục
Y bên phải biểu thị các vòng tỉa được thực hiện để tạo ra
một vé có cùng độ thưa thớt. Các thanh màu xanh biểu thị
sự tương đồng trong các trọng số đã tỉa giữa các vé LTH
và COLT. Sự tương đồng được tính toán ở đầu mỗi vòng
tỉa. Các vé COLT và LTH được khởi tạo với cùng trọng số.
Tỷ lệ tương đồng là số tham số đã tỉa chung giữa COLT
và LTH chia cho tổng số tham số ban đầu. Ở đây, 'chung'
ngụ ý rằng LTH và COLT đã tỉa cùng trọng số. Đường màu
đỏ và xanh lá nối các thanh biểu thị các vòng tỉa cần thiết
bởi COLT và LTH, tương ứng. Các số trên đỉnh của các
thanh mô tả các vòng tỉa bổ sung cần thiết bởi LTH so với
COLT. Chúng tôi thấy ít trọng số còn lại giống hệt nhau
hơn đáng kể giữa COLT và LTH trong giai đoạn đầu. Tuy
nhiên, khi tỷ lệ tỉa tăng, sự tương đồng giữa các trọng số
còn lại của LTH và COLT cũng tăng. Ví dụ, ở tỷ lệ tỉa
28%, sự tương đồng của các trọng số còn lại là 20%. Tiếp
theo, sự tương đồng của các trọng số còn lại nhảy lên 50%
khi độ thưa thớt của các vé đạt khoảng 48%. Điều này ngụ
ý rằng các trọng số giống hệt nhau giữa COLT và LTH bắt
đầu bị tỉa trong khi ma trận trọng số trở nên thưa thớt. Nói
cách khác, LTH và COLT về cơ bản tỉa các trọng số tương
tự để tạo ra một vé thắng cuộc thưa thớt. Tuy nhiên, COLT
(đường liền màu đỏ) yêu cầu ít vòng tỉa hơn LTH (đường
liền màu xanh lá) để tạo ra một vé có cùng độ thưa thớt.
Ví dụ, để đạt độ thưa thớt khoảng 89%, LTH yêu cầu ba
vòng tỉa hơn so với COLT, làm cho quá trình LTH tốn thời
gian.

Phân tích Hiệu suất: Phân tích tất cả kết quả, chúng ta có
thể thấy rằng các vé COLT từ các mô hình lớn như ResNet-18
có thể so sánh hoặc đôi khi vượt trội hơn các vé LTH qua
các kích thước tập dữ liệu và duy trì độ chính xác cao khi
chuyển sang các tập dữ liệu khác nhau, ngay cả ở độ thưa
thớt cao. Mặt khác, các vé MobileNetV2 COLT có xu hướng
hoạt động kém trên các tập dữ liệu nhỏ như Cifar-10 và
Cifar-100, được hiển thị trong Hình 5 (b), (c), và (e) so với
các vé LTH. Lý do có thể là do sự đa dạng đặc trưng giảm
sau khi chia các tập dữ liệu nhỏ thành các phân vùng nhỏ
hơn, có thể dẫn đến khó khăn trong việc xác định các đặc
trưng thiết yếu trong quá trình tỉa. Tuy nhiên, độ chính xác
của chúng có thể so sánh khi được tạo trên các tập dữ liệu
lớn hơn như Tiny ImageNet. Khi chuyển từ Tiny ImageNet
sang các tập dữ liệu nhỏ hơn, các vé MobileNetV2 COLT
phù hợp hoặc vượt hiệu suất LTH. Các vé Conv-3 COLT
cũng duy trì hiệu suất cấp độ LTH qua các tập dữ liệu, bao
gồm khi các vé Cifar-100 được áp dụng cho Cifar-10. Điều
này theo sau rằng việc chia một tập dữ liệu thành N phần
dẫn đến một tập dữ liệu nhỏ hơn, sau đó được huấn luyện
bằng một mô hình có cùng số lượng trọng số như tập dữ
liệu đầy đủ. Do đó, mô hình có thể sử dụng toàn bộ tiềm
năng của nó cho tập dữ liệu nhỏ hơn để xác định các đặc
trưng và trọng số tối ưu cho các mẫu huấn luyện. Các kỹ
thuật tỉa tích cực như COLT-2, COLT-3, hoặc COLT-4 do
đó có thể bảo tồn các trọng số tốt nhất để giữ độ chính xác
như, hoặc thỉnh thoảng tốt hơn, LTH qua các mô hình và
tập dữ liệu.

Trong phát hiện đối tượng, các mô hình COLT hoạt động
cạnh tranh

--- TRANG 13 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 13

với các mô hình LTH ở các mức độ thưa thớt khác nhau,
với điểm mAP tương tự, nhấn mạnh hiệu quả tỉa của COLT.
Điều này cho thấy tiềm năng của COLT như một giải pháp
thay thế khả thi cho LTH trong việc tỉa, có khả năng tạo ra
các mô hình thưa thớt hơn mà không mất hiệu suất và có
thể cung cấp việc tỉa đơn giản hơn và ít tốn kém hơn.

Giải quyết Quá khớp: Trong khi các mạng nơ-ron vốn dĩ
đối mặt với thách thức quá khớp, phương pháp tỉa lặp lại
của COLT thực sự phục vụ như một cơ chế điều chuẩn tự
nhiên giúp ngăn ngừa quá khớp hơn là gây ra nó [54]. Bằng
cách loại bỏ dần các kết nối dư thừa trong khi duy trì chỉ
những con đường thiết yếu nhất, COLT hiệu quả giảm khả
năng của mô hình ghi nhớ dữ liệu huấn luyện. Cụ thể, COLT
sử dụng ba cơ chế chính góp phần vào việc ngăn ngừa quá
khớp: (1) việc giữ lại chỉ các trọng số có độ lớn cao trong
khi đặt về không các trọng số có độ lớn thấp hơn phục vụ
như điều chuẩn ngầm [55], (2) quá trình chọn trọng số trùng
lặp qua nhiều phân vùng dữ liệu đảm bảo rằng chỉ những
trọng số quan trọng nhất quán được giữ lại, và (3) kiến trúc
thưa thớt kết quả vốn dĩ giới hạn khả năng của mô hình quá
khớp [56]. Kết quả thực nghiệm của chúng tôi hỗ trợ khẳng
định này. Khoảng cách hiệu suất giữa độ chính xác huấn
luyện và xác thực vẫn ổn định hoặc thậm chí giảm khi việc
tỉa tiến triển, đặc biệt ở các mức độ thưa thớt cao hơn (85-92%).
Hiệu suất mạnh mẽ dưới các điều kiện non-IID (xem Bảng
V) tiếp tục chứng minh rằng các mạng được tỉa COLT học
các đặc trưng có thể tổng quát hóa thay vì quá khớp với các
phân phối dữ liệu cụ thể.

V. KẾT LUẬN

Trong bài báo này, chúng tôi đề xuất một tập hợp vé số mới
được gọi là COLT để tỉa mạng nơ-ron sâu. Giống như các
vé từ LTH, COLT có thể đạt được độ thưa thớt cao và khả
năng chuyển trọng số qua các tập dữ liệu mà không ảnh
hưởng đến độ chính xác. Chúng tôi tính toán COLT bằng
cách tận dụng các vé thu được từ hai/bốn phân vùng theo
lớp của một tập dữ liệu. Chúng tôi nhận thấy rằng hiệu suất
của những vé này ngang bằng với các vé số được tạo bởi
LTH về độ chính xác. Hơn nữa, COLT được tạo ra ở ít vòng
lặp hơn so với các vé LTH. Chúng tôi xác thực khẳng định
của mình bằng cách sử dụng cả nhiệm vụ nhận dạng và
phát hiện đối tượng. Trong các thí nghiệm, chúng tôi cũng
đã chứng minh rằng COLT được tạo trên một tập dữ liệu
lớn (Tiny ImageNet) có thể được chuyển sang một tập dữ
liệu nhỏ (Cifar-10, Cifar-100) mà không ảnh hưởng đến hiệu
suất, cho thấy khả năng tổng quát hóa của những vé này.
Các công trình tương lai trong dòng nghiên cứu này có thể
xem xét độ thưa thớt và khả năng chuyển đổi của COLT
trên các kiến trúc sâu hơn (DenseNet/ShuffleNet) và các
tập dữ liệu rộng lớn (YouTube-BoundingBoxes/MSCOCO).
Hơn nữa, người ta có thể khám phá các thiết lập vấn đề khác
nhau (như phân đoạn đối tượng hoặc chú thích hình ảnh)
trong bối cảnh tỉa mạng.

TÀI LIỆU THAM KHẢO
[1] M. C. Mozer và P. Smolensky, "Using Relevance to Reduce Network
Size Automatically," Connection Science, vol. 1, pp. 3–16, 1989.
[2] E. Karnin, "A Simple Procedure for Pruning Back-propagation Trained
Neural Networks," IEEE Transactions on Neural Networks, vol. 1, no. 2,
pp. 239–242, 1990.
[3] S. Han, J. Pool, J. Tran, và W. J. Dally, "Learning both weights and
connections for efficient neural networks," trong Proceedings of the 28th
International Conference on Neural Information Processing Systems -
Volume 1, 2015, p. 1135–1143.
[4] H. Li, A. Kadav, I. Durdanovic, H. Samet, và H. P. Graf, "Pruning
filters for efficient convnets," trong International Conference on Learning
Representations, 2017.
[5] Y. LeCun, J. Denker, và S. Solla, "Optimal brain damage," trong Advances
in Neural Information Processing Systems, vol. 2, 1989, pp. 598–605.
[6] B. Hassibi và D. G.Stork, "Second Order Derivatives for Network
Pruning: Optimal Brain Surgeon," Adv Neural Inform Proc Syst, vol. 5,
10 1992.
[7] G. Hinton, O. Vinyals, J. Dean et al., "Distilling the knowledge in a
neural network," arXiv preprint arXiv:1503.02531, vol. 2, no. 7, 2015.
[8] P. Molchanov, S. Tyree, T. Karras, T. Aila, và J. Kautz, "Pruning
convolutional neural networks for resource efficient inference," trong 5th
International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.
[9] T.-J. Yang, Y. hsin Chen, và V. Sze, "Designing energy-efficient
convolutional neural networks using energy-aware pruning," 2017 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp.
6071–6079, 2017.
[10] J.-H. Luo, J. Wu, và W. Lin, "Thinet: A filter level pruning method
for deep neural network compression," trong 2017 IEEE International
Conference on Computer Vision (ICCV), 2017, pp. 5068–5076.
[11] B. O. Ayinde, T. Inanc, và J. M. Zurada, "Redundant feature pruning
for accelerated inference in deep neural networks," Neural Netw., vol.
118, p. 148–158, 2019.
[12] Y. Guo, A. Yao, và Y. Chen, "Dynamic network surgery for efficient
dnns," Advances in neural information processing systems, vol. 29, 2016.
[13] D. Molchanov, A. Ashukha, và D. P. Vetrov, "Variational dropout
sparsifies deep neural networks," trong ICML, 2017.
[14] K. Yao, F. Cao, Y. Leung, và J. Liang, "Deep neural network compression through interpretability-based filter pruning," Pattern Recognition,
vol. 119, p. 108056, 2021.
[15] M. Zhu và S. Gupta, "To prune, or not to prune: Exploring the efficacy
of pruning for model compression," trong 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Workshop Track Proceedings, 2018.
[16] M. Mondal, B. Das, S. D. Roy, P. Singh, B. Lall, và S. D. Joshi,
"Adaptive cnn filter pruning using global importance metric," Computer
Vision and Image Understanding, vol. 222, p. 103511, 2022.
[17] Q. Tian, T. Arbel, và J. J. Clark, "Structured deep fisher pruning for
efficient facial trait classification," Image and Vision Computing, vol. 77,
pp. 45–59, 2018.
[18] X. Lin, S. Kim, và J. Joo, "Fairgrape: Fairness-aware gradient pruning
method for face attribute classification," trong European Conference on
Computer Vision. Springer, 2022, pp. 414–432.
[19] F. Jia, X. Wang, J. Guan, H. Li, C. Qiu, và S. Qi, "Arank: Toward
specific model pruning via advantage rank for multiple salient objects
detection," Image and Vision Computing, vol. 111, p. 104192, 2021.
[20] M. Bonnaerens, M. Freiberger, và J. Dambre, "Anchor pruning for object detection," Computer Vision and Image Understanding, p. 103445,
2022.
[21] J. Frankle và M. Carbin, "The lottery ticket hypothesis: Finding sparse,
trainable neural networks," trong 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019,
2019.
[22] A. S. Morcos, H. Yu, M. Paganini, và Y. Tian, One Ticket to Win
Them All: Generalizing Lottery Ticket Initializations across Datasets
and Optimizers, 2019.
[23] K. He, X. Zhang, S. Ren, và J. Sun, "Deep residual learning for image
recognition," 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 770–778, 2016.
[24] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, và L.-C. Chen, "Mobilenetv2: Inverted residuals and linear bottlenecks," trong 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2018, pp.
4510–4520.
[25] A. Krizhevsky và G. Hinton, "Learning Multiple Layers of Features
from Tiny Images," University of Toronto, Toronto, Ontario, Tech.
Rep. 0, 2009.
[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., "Imagenet large
scale visual recognition challenge," International journal of computer
vision, vol. 115, pp. 211–252, 2015.
[27] N. Liu, G. Yuan, Z. Che, X. Shen, X. Ma, Q. Jin, J. Ren, J. Tang, S. Liu,
và Y. Wang, "Lottery ticket preserves weight correlation: Is it desirable
or not?" trong International Conference on Machine Learning, 2021.
[28] T. Gale, E. Elsen, và S. Hooker, "The state of sparsity in deep neural
networks," ArXiv, vol. abs/1902.09574, 2019.

--- TRANG 14 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 14

[29] N. Lee, T. Ajanthan, và P. Torr, "SNIP: SINGLE-SHOT NETWORK
PRUNING BASED ON CONNECTION SENSITIVITY," trong International Conference on Learning Representations, 2019.
[30] Z. Allen-Zhu, Y. Li, và Y. Liang, Learning and Generalization in
Overparameterized Neural Networks, Going beyond Two Layers, 2019.
[31] Z. Allen-Zhu, Y. Li, và Z. Song, "A convergence theory for deep learning via over-parameterization," trong Proceedings of the 36th International
Conference on Machine Learning, ser. Proceedings of Machine Learning
Research, vol. 97, 2019, pp. 242–252.
[32] S. Du và J. Lee, "On the power of over-parametrization in neural
networks with quadratic activation," trong Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine
Learning Research, vol. 80, 10–15 Jul 2018, pp. 1329–1338.
[33] S. S. Du, X. Zhai, B. Pócz'os, và A. Singh, "Gradient descent provably
optimizes over-parameterized neural networks," trong 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019, 2019.
[34] B. Neyshabur, R. Tomioka, và N. Srebro, "In search of the real
inductive bias: On the role of implicit regularization in deep learning."
trong ICLR (Workshop), 2015.
[35] B. Neyshabur, Z. Li, S. Bhojanapalli, Y. LeCun, và N. Srebro, "The
role of over-parametrization in generalization of neural networks," trong
International Conference on Learning Representations, 2019.
[36] R. Van Soelen và J. W. Sheppard, "Using Winning Lottery Tickets
in Transfer Learning for Convolutional Neural Networks," trong 2019
International Joint Conference on Neural Networks (IJCNN). IEEE,
2019, pp. 1–8.
[37] X. Ma, G. Yuan, X. Shen, T. Chen, X. Chen, X. Chen, N. Liu, M. Qin,
S. Liu, Z. Wang, và Y. Wang, "Sanity checks for lottery tickets: Does
your winning ticket really win the jackpot?" trong Advances in Neural
Information Processing Systems, vol. 34. Curran Associates, Inc., 2021,
pp. 12 749–12 760.
[38] S. Kornblith, J. Shlens, và Q. V. Le, "Do better imagenet models
transfer better?" trong 2019 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), Los Alamitos, CA, USA, jun 2019,
pp. 2656–2666.
[39] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, và C. Liu, "A survey
on deep transfer learning," trong Artificial Neural Networks and Machine
Learning – ICANN 2018, 2018, pp. 270–279.
[40] J. Yosinski, J. Clune, Y. Bengio, và H. Lipson, "How transferable are
features in deep neural networks?" trong Advances in Neural Information
Processing Systems, vol. 27, 2014.
[41] B. Zoph, V. Vasudevan, J. Shlens, và Q. V. Le, "Learning transferable
architectures for scalable image recognition," 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8697–8710, 2018.
[42] H. Zhou, J. Lan, R. Liu, và J. Yosinski, "Deconstructing lottery tickets:
Zeros, signs, and the supermask," Advances in neural information
processing systems, vol. 32, 2019.
[43] H. Tanaka, D. Kunin, D. L. K. Yamins, và S. Ganguli, "Pruning neural
networks without any data by iteratively conserving synaptic flow," trong
Proceedings of the 34th International Conference on Neural Information
Processing Systems, ser. NIPS'20, Red Hook, NY, USA, 2020.
[44] Y. Le và X. S. Yang, "Tiny ImageNet Visual Recognition Challenge,"
2015.
[45] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, và A. Zisserman, "The pascal visual object classes (voc) challenge," International
Journal of Computer Vision, vol. 88, no. 2, pp. 303–338, Jun. 2010.
[46] K. Simonyan và A. Zisserman, "Very deep convolutional networks for
large-scale image recognition," CoRR, vol. abs/1409.1556, 2015.
[47] X. Glorot và Y. Bengio, "Understanding the difficulty of training deep
feedforward neural networks," trong AISTATS, 2010.
[48] S. Ren, K. He, R. Girshick, và J. Sun, "Faster r-cnn: Towards real-time
object detection with region proposal networks," IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 39, no. 6, pp. 1137–
1149, 2017.
[49] K. Simonyan và A. Zisserman, "Very deep convolutional networks for
large-scale image recognition," 2015.
[50] J. Yang, X. Shen, J. Xing, X. Tian, H. Li, B. Deng, J. Huang, và
X.-s. Hua, "Quantization networks," trong Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, 2019, pp. 7308–
7316.
[51] J. Frankle, G. K. Dziugaite, D. Roy, và M. Carbin, "Linear mode connectivity and the lottery ticket hypothesis," trong International Conference
on Machine Learning. PMLR, 2020, pp. 3259–3269.
[52] B. Bartoldson, A. Morcos, A. Barbu, và G. Erlebacher, "The
generalization-stability tradeoff in neural network pruning," Advances
in Neural Information Processing Systems, vol. 33, pp. 20 852–20 864,
2020.
[53] S. Hooker, A. Courville, G. Clark, Y. Dauphin, và A. Frome,
"What do compressed deep neural networks forget?" arXiv preprint
arXiv:1911.05248, 2019.
[54] X. Ying, "An overview of overfitting and its solutions," trong Journal of
physics: Conference series, vol. 1168. IOP Publishing, 2019, p. 022022.
[55] T. Vaskevicius, V. Kanade, và P. Rebeschini, "Implicit regularization
for optimal sparse recovery," Advances in Neural Information Processing
Systems, vol. 32, 2019.
[56] T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, và A. Peste, "Sparsity
in deep learning: Pruning and growth for efficient inference and training
in neural networks," Journal of Machine Learning Research, vol. 22, no.
241, pp. 1–124, 2021.
