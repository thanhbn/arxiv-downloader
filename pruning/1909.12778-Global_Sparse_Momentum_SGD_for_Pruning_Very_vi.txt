# Global Sparse Momentum SGD để Cắt tỉa các Mạng Nơ-ron Sâu Rất Lớn

Xiaohan Ding1Guiguang Ding1Xiangxin Zhou2
Yuchen Guo1, 3Jungong Han4Ji Liu5

1Trung tâm Nghiên cứu Quốc gia Bắc Kinh về Khoa học và Công nghệ Thông tin (BNRist);
Trường Phần mềm, Đại học Thanh Hoa, Bắc Kinh, Trung Quốc
2Khoa Kỹ thuật Điện tử, Đại học Thanh Hoa, Bắc Kinh, Trung Quốc
3Khoa Tự động hóa, Đại học Thanh Hoa;
Viện Khoa học Não bộ và Nhận thức, Đại học Thanh Hoa, Bắc Kinh, Trung Quốc
4WMG Data Science, Đại học Warwick, Coventry, Vương quốc Anh
5Phòng thí nghiệm AI Seattle Kwai, Phòng thí nghiệm FeDA Kwai, Nền tảng AI Kwai

dxh17@mails.tsinghua.edu.cn dinggg@tsinghua.edu.cn
xx-zhou16@mails.tsinghua.edu.cn yuchen.w.guo@gmail.com
jungonghan77@gmail.com ji.liu.uwisc@gmail.com

## Tóm tắt

Mạng Nơ-ron Sâu (DNN) mạnh mẽ nhưng tốn kém về mặt tính toán và tiêu tốn nhiều bộ nhớ, do đó cản trở việc sử dụng thực tế trên các thiết bị đầu cuối có tài nguyên hạn chế. Cắt tỉa DNN là một phương pháp nén mô hình sâu, nhằm loại bỏ một số tham số với độ suy giảm hiệu năng có thể chấp nhận được. Trong bài báo này, chúng tôi đề xuất một phương pháp tối ưu hóa dựa trên momentum-SGD mới để giảm độ phức tạp mạng bằng cách cắt tỉa khi đang bay. Cụ thể, cho một tỷ lệ nén toàn cục, chúng tôi phân loại tất cả các tham số thành hai phần tại mỗi vòng lặp huấn luyện được cập nhật bằng các quy tắc khác nhau. Bằng cách này, chúng tôi dần dần làm cho các tham số dư thừa bằng không, khi chúng tôi cập nhật chúng chỉ sử dụng suy giảm trọng số thông thường mà không có gradient dẫn xuất từ hàm mục tiêu. Khác với các phương pháp trước đó yêu cầu công việc thủ công nặng để điều chỉnh tỷ lệ thưa theo từng lớp, cắt tỉa bằng cách giải quyết các bài toán không khả vi phức tạp hoặc tinh chỉnh mô hình sau khi cắt tỉa, phương pháp của chúng tôi có đặc điểm: 1) nén toàn cục tự động tìm ra tỷ lệ thưa thích hợp cho mỗi lớp; 2) huấn luyện từ đầu đến cuối; 3) không cần quá trình tái huấn luyện tốn thời gian sau khi cắt tỉa; và 4) khả năng vượt trội để tìm ra những vé số trúng tương thích hơn đã thắng xổ số khởi tạo.

## 1 Giới thiệu

Những năm gần đây đã chứng kiến thành công lớn của Mạng Nơ-ron Sâu (DNN) trong nhiều ứng dụng thực tế. Tuy nhiên, các mô hình rất sâu ngày nay đi kèm với hàng triệu tham số, do đó khiến chúng khó triển khai trên các thiết bị có hạn chế về tính toán. Trong bối cảnh này, các phương pháp cắt tỉa DNN đã thu hút nhiều sự chú ý, nơi chúng ta loại bỏ một số kết nối (tức là, các tham số riêng lẻ) [21,22,31], hoặc các kênh [32], do đó không gian lưu trữ yêu cầu và tính toán có thể được giảm. Bài báo này tập trung vào cắt tỉa kết nối, nhưng phương pháp đề xuất có thể được khái quát hóa dễ dàng cho cắt tỉa có cấu trúc (ví dụ, mức nơ-ron, nhân hoặc bộ lọc). Để đạt được sự cân bằng tốt giữa độ chính xác và kích thước mô hình, nhiều phương pháp cắt tỉa đã được đề xuất, có thể được phân loại thành hai mô hình điển hình. 1) Một số nhà nghiên cứu [13,18,21,22,26,31,32,39,41] đề xuất cắt tỉa mô hình bằng một số phương tiện để đạt đến một mức độ nén nhất định, sau đó tinh chỉnh nó bằng SGD thông thường để khôi phục độ chính xác. 2) Các phương pháp khác tìm cách tạo ra tính thưa trong mô hình thông qua một quy trình học tùy chỉnh [1, 12, 33, 34, 51, 54, 56].

Mặc dù các phương pháp hiện tại đã đạt được thành công lớn trong cắt tỉa, có một số hạn chế điển hình. Cụ thể, khi chúng ta tìm cách cắt tỉa một mô hình trước và tinh chỉnh nó, chúng ta đối mặt với hai vấn đề:

**Tỷ lệ thưa theo từng lớp về bản chất khó đặt làm siêu tham số.** Nhiều công trình trước đây [21,24,26,32] đã chỉ ra rằng một số lớp trong DNN nhạy cảm với việc cắt tỉa, nhưng một số có thể được cắt tỉa đáng kể mà không làm giảm độ chính xác của mô hình. Do đó, cần có kiến thức trước để điều chỉnh các siêu tham số theo từng lớp nhằm tối đa hóa tỷ lệ nén toàn cục mà không có sự sụt giảm độ chính xác không thể chấp nhận được.

**Các mô hình đã cắt tỉa khó huấn luyện, và chúng ta không thể dự đoán độ chính xác cuối cùng sau khi tinh chỉnh.** Ví dụ, các mô hình đã cắt tỉa ở mức bộ lọc có thể dễ dàng rơi vào cực tiểu địa phương xấu, và đôi khi thậm chí không thể đạt đến mức độ chính xác tương tự với một đối tác được huấn luyện từ đầu [10,38]. Và trong bối cảnh cắt tỉa kết nối, mạng càng thưa thì việc học càng chậm và độ chính xác kiểm tra cuối cùng càng thấp [15].

Mặt khác, cắt tỉa bằng cách học không dễ dàng hơn do:

**Trong một số trường hợp, chúng ta giới thiệu một siêu tham số để kiểm soát sự cân bằng, không phản ánh trực tiếp tỷ lệ nén kết quả.** Ví dụ, MorphNet [17] sử dụng group Lasso [44] để làm không một số bộ lọc cho cắt tỉa có cấu trúc, trong đó một siêu tham số chính là hệ số Lasso. Tuy nhiên, cho một giá trị cụ thể của hệ số, chúng ta không thể dự đoán tỷ lệ nén cuối cùng trước khi quá trình huấn luyện kết thúc. Do đó, khi chúng ta nhắm đến một tỷ lệ nén cuối cùng cụ thể, chúng ta phải thử nhiều giá trị hệ số trước và chọn cái mang lại kết quả gần nhất với kỳ vọng của chúng ta.

**Một số phương pháp cắt tỉa bằng cách giải quyết một bài toán tối ưu hóa trực tiếp liên quan đến tính thưa.** Vì bài toán không khả vi, nó không thể được giải quyết bằng các phương pháp dựa trên SGD theo cách từ đầu đến cuối. Một cuộc thảo luận chi tiết hơn sẽ được cung cấp trong Phần 3.2.

Trong bài báo này, chúng tôi tìm cách khắc phục các hạn chế đã thảo luận ở trên bằng cách trực tiếp thay đổi dòng gradient dựa trên momentum SGD, một cách rõ ràng liên quan đến tỷ lệ nén cuối cùng và có thể được thực hiện thông qua huấn luyện từ đầu đến cuối. Cụ thể, chúng tôi sử dụng chuỗi Taylor bậc nhất để đo lường tầm quan trọng của một tham số bằng cách ước tính giá trị hàm mục tiêu sẽ thay đổi bao nhiều khi loại bỏ nó [41,49]. Dựa trên đó, cho một tỷ lệ nén toàn cục, chúng tôi phân loại tất cả các tham số thành hai phần sẽ được cập nhật bằng các quy tắc khác nhau, được gọi là **lựa chọn kích hoạt**. Đối với các tham số không quan trọng, chúng tôi thực hiện **cập nhật thụ động** không có gradient dẫn xuất từ hàm mục tiêu mà chỉ có suy giảm trọng số thông thường (tức là, điều chuẩn ℓ-2) để phạt các giá trị của chúng. Mặt khác, thông qua **cập nhật chủ động**, các tham số quan trọng được cập nhật bằng cả gradient liên quan đến hàm mục tiêu và suy giảm trọng số để duy trì độ chính xác của mô hình. Việc lựa chọn như vậy được thực hiện tại mỗi vòng lặp huấn luyện, để một kết nối bị vô hiệu hóa có cơ hội được kích hoạt lại tại vòng lặp tiếp theo. Thông qua các cập nhật thụ động được tăng tốc bằng momentum liên tục, chúng tôi có thể làm cho hầu hết các tham số tiến vô cùng gần đến không, sao cho việc cắt tỉa chúng không gây tổn hại đến độ chính xác của mô hình. Nhờ điều này, không cần quá trình tinh chỉnh. Ngược lại, một số điều khoản điều chuẩn được đề xuất trước đây chỉ có thể giảm các tham số ở một mức độ nào đó, do đó việc cắt tỉa vẫn làm giảm mô hình. Những đóng góp của chúng tôi được tóm tắt như sau.

**Để cắt tỉa không mất mát và huấn luyện từ đầu đến cuối, chúng tôi đề xuất trực tiếp thay đổi dòng gradient**, điều này được phân biệt rõ ràng với các phương pháp hiện tại hoặc thêm một điều khoản điều chuẩn hoặc tìm cách giải quyết một số bài toán tối ưu hóa không khả vi.

**Chúng tôi đề xuất Global Sparse Momentum SGD (GSM), một phương pháp tối ưu hóa SGD mới**, chia quy tắc cập nhật của momentum SGD thành hai phần. Cắt tỉa DNN dựa trên GSM yêu cầu một tỷ lệ nén cuối cùng toàn cục duy nhất làm siêu tham số và có thể tự động khám phá tỷ lệ thưa thích hợp cho mỗi lớp để đạt được nó.

**Từ các thí nghiệm, chúng tôi đã xác nhận khả năng của GSM để đạt được tỷ lệ nén cao trên MNIST, CIFAR-10 [29] và ImageNet [9] cũng như tìm ra những vé số trúng tương thích hơn [15].** Các mã có sẵn tại https://github.com/DingXiaoH/GSM-SGD.

## 2 Công việc liên quan

### 2.1 Momentum SGD

Gradient descent ngẫu nhiên chỉ tính đến đạo hàm bậc nhất của hàm mục tiêu chứ không phải các bậc cao hơn [28]. Momentum là một kỹ thuật phổ biến được sử dụng cùng với SGD, tích lũy các gradient của các bước trước để xác định hướng đi, thay vì chỉ sử dụng gradient của bước hiện tại. Tức là, momentum cho SGD một bộ nhớ ngắn hạn [16]. Chính thức, gọi L là hàm mục tiêu, w là một tham số đơn, η là tốc độ học, γ là hệ số momentum kiểm soát tỷ lệ phần trăm gradient được giữ lại mỗi vòng lặp, λ là hệ số suy giảm trọng số thông thường (ví dụ, 1×10^-4 cho ResNets [23]), quy tắc cập nhật là

z^(k+1) ← γz^(k) + λw^(k) + ∂L/∂w^(k);
w^(k+1) ← w^(k) - ηz^(k+1).                    (1)

Có một câu chuyện phổ biến về momentum [16,42,45,48]: gradient descent là một người đàn ông đi xuống một ngọn đồi. Anh ta đi theo con đường dốc nhất xuống dưới; tiến độ của anh ta chậm, nhưng ổn định. Momentum là một quả bóng nặng lăn xuống cùng ngọn đồi đó. Quán tính được thêm vào hoạt động như một bộ làm mượt và một bộ tăng tốc, làm giảm dao động và khiến chúng ta lao qua các thung lũng hẹp, những gò nhỏ và cực tiểu địa phương. Trong bài báo này, chúng tôi sử dụng momentum như một bộ tăng tốc để thúc đẩy các cập nhật thụ động.

### 2.2 Cắt tỉa DNN và các kỹ thuật khác để nén và tăng tốc

Cắt tỉa DNN tìm cách loại bỏ một số tham số mà không có sự sụt giảm độ chính xác đáng kể, có thể được phân loại thành các kỹ thuật không có cấu trúc và có cấu trúc dựa trên độ chi tiết cắt tỉa. Cắt tỉa không có cấu trúc (hay còn gọi là cắt tỉa kết nối) [7,21,22,31] nhắm đến việc giảm đáng kể số lượng tham số khác không, tạo ra một mô hình thưa, có thể được lưu trữ bằng cách sử dụng ít không gian hơn nhiều, nhưng không thể giảm hiệu quả gánh nặng tính toán trên các nền tảng phần cứng và phần mềm có sẵn. Mặt khác, cắt tỉa có cấu trúc loại bỏ các cấu trúc (ví dụ, nơ-ron, nhân hoặc toàn bộ bộ lọc) từ DNN để có được tăng tốc thực tế. Ví dụ, cắt tỉa kênh [10,11,32,35,37,38] không thể đạt được tỷ lệ nén cực cao về kích thước mô hình, nhưng có thể chuyển đổi một CNN rộng thành một CNN hẹp hơn (nhưng vẫn dày đặc) để giảm chi phí bộ nhớ và tính toán. Trong các ứng dụng thực tế, cắt tỉa không có cấu trúc và có cấu trúc thường được sử dụng cùng nhau để đạt được sự cân bằng mong muốn.

Bài báo này tập trung vào cắt tỉa kết nối (nhưng phương pháp đề xuất có thể được khái quát hóa dễ dàng cho cắt tỉa có cấu trúc), đã thu hút nhiều sự chú ý kể từ khi Han và cộng sự [21] cắt tỉa các kết nối DNN dựa trên độ lớn của các tham số và khôi phục độ chính xác thông qua SGD thông thường. Một số công trình truyền cảm hứng đã cải thiện mô hình cắt tỉa và tinh chỉnh bằng cách nối các kết nối khi chúng trở nên quan trọng trở lại [18], nhắm mục tiêu trực tiếp vào tiêu thụ năng lượng [55], sử dụng đạo hàm bậc hai theo từng lớp [13], v.v. Các phương pháp cắt tỉa dựa trên học tập khác sẽ được thảo luận trong Phần 3.2.

Ngoài cắt tỉa, chúng ta cũng có thể nén và tăng tốc DNN theo những cách khác. Một số công trình [2,46,57] phân rã hoặc xấp xỉ các tensor tham số; các kỹ thuật lượng tử hóa và nhị phân hóa [8,19,20,36] xấp xỉ một mô hình bằng cách sử dụng ít bit hơn cho mỗi tham số; chưng cất kiến thức [3,25,43] chuyển kiến thức từ một mạng lớn sang một mạng nhỏ hơn; một số nhà nghiên cứu tìm cách tăng tốc tích chập với sự giúp đỡ của thủng [14], FFT [40,50] hoặc DCT [53]; Wang và cộng sự [52] nén bản đồ đặc trưng bằng cách trích xuất thông tin thông qua ma trận Circulant.

## 3 GSM: Global Sparse Momentum SGD

### 3.1 Công thức hóa

Trước tiên chúng tôi làm rõ các ký hiệu trong bài báo này. Đối với một lớp kết nối đầy đủ với đầu vào p chiều và đầu ra q chiều, chúng tôi sử dụng W ∈ ℝ^(p×q) để biểu thị ma trận nhân. Đối với một lớp tích chập với tensor nhân K ∈ ℝ^(h×w×r×s), trong đó h và w là chiều cao và chiều rộng của nhân tích chập, r và s là số lượng kênh đầu vào và đầu ra, tương ứng, chúng tôi mở tensor K thành W ∈ ℝ^(hwr×s). Gọi N là số lượng tất cả các lớp như vậy, chúng tôi sử dụng Θ = [Wi] (∀1≤i≤N) để biểu thị tập hợp tất cả các ma trận nhân như vậy, và tỷ lệ nén toàn cục C được cho bởi

C = |Θ|/||Θ||₀,                    (2)

trong đó |Θ| là kích thước của Θ và ||Θ||₀ là chuẩn ℓ-0, tức là, số lượng phần tử khác không. Gọi L, X, Y là hàm mất mát liên quan đến độ chính xác (ví dụ, entropy chéo cho các tác vụ phân loại), các ví dụ kiểm tra và nhãn, tương ứng, chúng tôi tìm cách có được một sự cân bằng tốt giữa độ chính xác và kích thước mô hình bằng cách đạt được tỷ lệ nén cao C mà không có sự gia tăng không thể chấp nhận được trong mất mát L(X; Y; Θ).

### 3.2 Xem xét lại cắt tỉa dựa trên học tập

Mục tiêu hoặc hướng tối ưu hóa của huấn luyện DNN thông thường chỉ là tối thiểu hóa hàm mục tiêu, nhưng khi chúng ta tìm cách tạo ra một mô hình thưa thông qua một quy trình học tùy chỉnh, điều quan trọng là làm lệch hướng huấn luyện ban đầu bằng cách tính đến tính thưa của các tham số. Thông qua huấn luyện, tính thưa xuất hiện dần dần, và cuối cùng chúng ta đạt đến sự cân bằng mong đợi giữa độ chính xác và kích thước mô hình, thường được kiểm soát bởi một hoặc một loạt siêu tham số.

#### 3.2.1 Cân bằng rõ ràng như tối ưu hóa có ràng buộc

Sự cân bằng có thể được mô hình hóa rõ ràng như một bài toán tối ưu hóa có ràng buộc [56], ví dụ,

minimize L(X; Y; Θ) + ∑ᵢ₌₁ᴺ gᵢ(Wᵢ),                    (3)

trong đó gᵢ là một hàm chỉ thị,

gᵢ(W) = {0 nếu ||W||₀ ≤ lᵢ; +∞ ngược lại,                    (4)

và lᵢ là số lượng tham số khác không yêu cầu tại lớp i. Vì điều khoản thứ hai của hàm mục tiêu không khả vi, bài toán không thể được giải quyết một cách phân tích hoặc bằng gradient descent ngẫu nhiên, nhưng có thể được giải quyết bằng cách áp dụng SGD và giải quyết bài toán không khả vi một cách luân phiên, ví dụ, sử dụng ADMM [6]. Bằng cách này, hướng huấn luyện bị lệch, và sự cân bằng được đạt được.

#### 3.2.2 Cân bằng ngầm sử dụng điều chuẩn

Đó là một thực tiễn phổ biến để áp dụng một số điều chuẩn khả vi bổ sung trong quá trình huấn luyện để giảm độ lớn của một số tham số, sao cho việc loại bỏ chúng gây ra ít tổn hại hơn [1,21,54]. Gọi R(Θ) là điều khoản điều chuẉ liên quan đến độ lớn, λ là một siêu tham số cân bằng, bài toán là

minimize L(X; Y; Θ) + λR(Θ).                    (5)

Tuy nhiên, các điểm yếu là hai mặt. 1) Một số điều chuẩn phổ biến, ví dụ, ℓ-1, ℓ-2 và Lasso [44], không thể làm cho các phần tử trong Θ thực sự bằng không, mà chỉ có thể giảm độ lớn ở một mức độ nào đó, sao cho việc loại bỏ chúng vẫn làm giảm hiệu năng. Chúng tôi gọi hiện tượng này là **cao nguyên độ lớn**. Nguyên nhân đằng sau là đơn giản: đối với một tham số có thể huấn luyện cụ thể w, khi độ lớn |w| của nó lớn ở đầu, gradient dẫn xuất từ R, tức là, ∂R/∂w, áp đảo ∂L/∂w, do đó |w| dần được giảm. Tuy nhiên, khi |w| co lại, ∂R/∂w cũng giảm, sao cho xu hướng giảm của |w| bằng phẳng khi ∂R/∂w tiến đến ∂L/∂w, và w duy trì một độ lớn tương đối nhỏ. 2) Siêu tham số λ không phản ánh trực tiếp tỷ lệ nén kết quả, do đó chúng ta có thể cần thực hiện một số nỗ lực để có được một số kiến thức thực nghiệm trước khi chúng ta có được mô hình với tỷ lệ nén mong đợi của chúng ta.

### 3.3 Dòng gradient thưa toàn cục thông qua momentum SGD

Để khắc phục các nhược điểm của hai mô hình đã thảo luận ở trên, chúng tôi dự định kiểm soát rõ ràng tỷ lệ nén cuối cùng thông qua huấn luyện từ đầu đến cuối bằng cách trực tiếp thay đổi dòng gradient của momentum SGD để làm lệch hướng huấn luyện nhằm đạt được tỷ lệ nén cao cũng như duy trì độ chính xác. Một cách trực quan, chúng tôi tìm cách sử dụng các gradient để hướng dẫn một số ít tham số hoạt động nhằm tối thiểu hóa hàm mục tiêu, và phạt hầu hết các tham số để đẩy chúng vô cùng gần với không. Do đó, điều đầu tiên là tìm một metric thích hợp để phân biệt phần hoạt động. Cho một tỷ lệ nén toàn cục C, chúng tôi sử dụng Q = |Θ|/C để biểu thị số lượng phần tử khác không trong Θ. Tại mỗi vòng lặp huấn luyện, chúng tôi đưa một mini-batch dữ liệu vào mô hình, tính các gradient sử dụng quy tắc chuỗi thông thường, tính các giá trị metric cho mỗi tham số, thực hiện cập nhật chủ động trên Q tham số có giá trị metric lớn nhất và cập nhật thụ động trên những tham số khác. Để làm cho GSM khả thi trên các mô hình rất sâu, các metric nên được tính toán chỉ sử dụng các kết quả tính toán trung gian ban đầu, tức là, các tham số và gradient, nhưng không có đạo hàm bậc hai. Lấy cảm hứng từ hai phương pháp trước đó đã sử dụng chuỗi Taylor bậc nhất để cắt tỉa kênh tham lam [41,49], chúng tôi định nghĩa metric theo cách tương tự. Chính thức, tại mỗi vòng lặp huấn luyện với một mini-batch các ví dụ x và nhãn y, gọi T(x; y; w) là giá trị metric của một tham số cụ thể w, chúng tôi có

T(x; y; w) = |∂L(x; y; Θ)/∂w · w|.                    (6)

Lý thuyết là đối với mini-batch hiện tại, chúng tôi mong đợi giảm những tham số có thể được loại bỏ với ít tác động hơn đến L(x; y; Θ). Sử dụng chuỗi Taylor, nếu chúng tôi đặt một tham số cụ thể w về 0, giá trị mất mát trở thành

L(x; y; Θ|w → 0) = L(x; y; Θ) - ∂L(x; y; Θ)/∂w · (0 - w) + o(w²).                    (7)

Bỏ qua điều khoản bậc cao hơn, chúng tôi có

|L(x; y; Θ|w → 0) - L(x; y; Θ)| = |∂L(x; y; Θ)/∂w · w| = T(x; y; w),                    (8)

đây là một xấp xỉ của sự thay đổi trong giá trị mất mát nếu w được đặt về không.

Chúng tôi viết lại quy tắc cập nhật của momentum SGD (Công thức 1). Tại vòng lặp huấn luyện thứ k với một mini-batch các ví dụ x và nhãn y trên một lớp cụ thể với nhân W, quy tắc cập nhật là

Z^(k+1) ← γZ^(k) + λW^(k) + B^(k) ⊙ ∂L(x; y; Θ)/∂W^(k);
W^(k+1) ← W^(k) - ηZ^(k+1),                    (9)

trong đó ⊙ là phép nhân theo từng phần tử (hay còn gọi là tích Hadamard), và B^(k) là ma trận mặt nạ,

B^(k)_{m,n} = {1 nếu T(x; y; W^(k)_{m,n}) ≥ giá trị lớn thứ Q trong T(x; y; Θ^(k)); 0 ngược lại.                    (10)

Chúng tôi gọi việc tính toán B cho mỗi nhân là **lựa chọn kích hoạt**. Rõ ràng, có chính xác Q số một trong tất cả các ma trận mặt nạ, và GSM thoái hóa thành momentum SGD thông thường khi Q = |Θ|.

Cần lưu ý rằng GSM không phụ thuộc vào mô hình vì nó không đưa ra giả định về cấu trúc mô hình hoặc dạng hàm mất mát. Tức là, việc tính toán gradient thông qua lan truyền ngược là liên quan đến mô hình, tất nhiên, nhưng nó không phụ thuộc vào mô hình để sử dụng chúng cho cắt tỉa GSM.

### 3.4 GSM cho phép kích hoạt lại ngầm và giảm liên tục nhanh

Vì GSM thực hiện lựa chọn kích hoạt tại mỗi vòng lặp huấn luyện, nó cho phép các kết nối bị phạt được kích hoạt lại, nếu chúng được phát hiện là quan trọng đối với mô hình một lần nữa. So với hai công trình trước đây đã chèn một giai đoạn nối [18] hoặc khôi phục [55] một cách rõ ràng vào toàn bộ quy trình để nối lại các kết nối bị cắt tỉa nhầm, GSM có đặc điểm thực hiện đơn giản hơn và huấn luyện từ đầu đến cuối.

Tuy nhiên, như sẽ được chỉ ra trong Phần 4.4, việc kích hoạt lại chỉ xảy ra trên một thiểu số các tham số, nhưng hầu hết chúng trải qua một loạt các cập nhật thụ động, do đó tiếp tục di chuyển về phía không. Vì chúng tôi muốn biết cần bao nhiều vòng lặp huấn luyện để làm cho các tham số đủ nhỏ để thực hiện cắt tỉa không mất mát, chúng tôi cần dự đoán giá trị cuối cùng của một tham số w sau k cập nhật thụ động, cho η, γ và λ. Chúng tôi có thể sử dụng Công thức 1 để dự đoán w^(k), điều này thực tế nhưng phức tạp. Trong các trường hợp sử dụng phổ biến của chúng tôi trong đó z^(0) = 0 (từ đầu của quá trình huấn luyện), k lớn (ít nhất hàng chục nghìn), và λ nhỏ (ví dụ, λ = 5×10^-3, η = 5×10^-4), chúng tôi đã quan sát một công thức thực nghiệm đủ chính xác (Hình 1) để xấp xỉ giá trị kết quả,

w^(k) ≈ w^(0)(1 - η/(1-γ))^k.                    (11)

Trong thực tế, chúng tôi cố định λ (ví dụ, 1×10^-4 cho ResNets [23] và DenseNets [27]) và điều chỉnh η giống như chúng tôi làm cho huấn luyện DNN thông thường, và sử dụng γ = 0.98 hoặc γ = 0.99 để làm không nhanh hơn 50× hoặc 100×.

Khi quá trình huấn luyện hoàn thành, chúng tôi cắt tỉa mô hình một cách toàn cục bằng cách chỉ bảo tồn Q tham số trong Θ có độ lớn lớn nhất. Chúng tôi quyết định số lượng vòng lặp huấn luyện k sử dụng Eq. 11 dựa trên một quan sát thực nghiệm rằng với (1 - η/(1-γ))^k < 1×10^-4, một thao tác cắt tỉa như vậy không gây ra sụt giảm độ chính xác trên các mô hình rất sâu như ResNet-56 và DenseNet-40.

Momentum là quan trọng đối với cắt tỉa dựa trên GSM để hoàn thành với chi phí thời gian có thể chấp nhận được. Vì hầu hết các tham số liên tục tăng trưởng theo cùng một hướng được xác định bởi suy giảm trọng số (tức là, về phía không), xu hướng như vậy tích lũy trong momentum, do đó quá trình làm không được tăng tốc đáng kể. Mặt khác, nếu một tham số không luôn thay đổi theo cùng một hướng, việc tăng γ ít ảnh hưởng đến động lực học huấn luyện của nó. Ngược lại, nếu chúng tôi tăng tốc độ học để làm không nhanh hơn, các tham số quan trọng đang lơ lửng xung quanh cực tiểu toàn cục sẽ lệch đáng kể khỏi các giá trị hiện tại của chúng đã đạt được với tốc độ học thấp hơn nhiều trước đó.

## 4 Thí nghiệm

### 4.1 Kết quả cắt tỉa và so sánh

Chúng tôi đánh giá GSM bằng cách cắt tỉa một số mô hình benchmark phổ biến trên MNIST, CIFAR-10 [29] và ImageNet [9], và so sánh với các kết quả báo cáo từ một số đối thủ cạnh tranh gần đây. Đối với mỗi thử nghiệm, chúng tôi bắt đầu từ một mô hình cơ sở được huấn luyện tốt và áp dụng huấn luyện GSM trên tất cả các lớp **đồng thời**.

**MNIST.** Trước tiên chúng tôi thí nghiệm trên MNIST với LeNet-300-100 và LeNet-5 [30]. LeNet-300-100 là một mạng kết nối đầy đủ ba lớp với 267K tham số, đạt được độ chính xác Top1 98.19%. LeNet-5 là một mạng tích chập bao gồm hai lớp tích chập và hai lớp kết nối đầy đủ, chứa 431K tham số và cho độ chính xác Top1 99.21%. Để đạt được nén 60× và 125×, chúng tôi đặt Q = 267K/60 = 4.4K cho LeNet-300-100 và Q = 431K/125 = 3.4K cho LeNet-5, tương ứng. Chúng tôi sử dụng hệ số momentum γ = 0.99 và kích thước batch 256. Lịch trình tốc độ học là η = 3×10^-2; 3×10^-3; 3×10^-4 cho 160, 40 và 40 epoch, tương ứng. Sau khi huấn luyện GSM, chúng tôi thực hiện cắt tỉa không mất mát và kiểm tra trên tập dữ liệu xác thực. Như được chỉ ra trong Bảng 1, GSM có thể tạo ra các mô hình có độ thưa cao vẫn duy trì độ chính xác. Bằng cách tăng thêm tỷ lệ nén trên LeNet-5 lên 300×, chúng tôi chỉ quan sát một sự sụt giảm độ chính xác nhỏ (0.15%), điều này cho thấy rằng GSM có thể mang lại hiệu năng hợp lý với tỷ lệ nén cực cao.

**CIFAR-10.** Chúng tôi trình bày kết quả của một bộ thí nghiệm khác trên CIFAR-10 trong Bảng 2 sử dụng ResNet-56 [23] và DenseNet-40 [27]. Chúng tôi sử dụng γ = 0.98, kích thước batch 64 và tốc độ học η = 5×10^-3; 5×10^-4; 5×10^-5 cho 400, 100 và 100 epoch, tương ứng. Chúng tôi áp dụng tăng cường dữ liệu tiêu chuẩn bao gồm padding đến 40×40, cắt ngẫu nhiên và lật trái-phải. Mặc dù ResNet-56 và DenseNet-40 sâu hơn và phức tạp hơn đáng kể, GSM cũng có thể giảm các tham số 10× và vẫn duy trì độ chính xác.

**ImageNet.** Chúng tôi cắt tỉa ResNet-50 để xác minh GSM trên các ứng dụng nhận dạng hình ảnh quy mô lớn. Chúng tôi sử dụng kích thước batch 64 và huấn luyện mô hình với η = 1×10^-3; 1×10^-4; 1×10^-5 cho 40, 10 và 10 epoch, tương ứng. Chúng tôi so sánh kết quả với L-OBS [13], đó là phương pháp trước đây duy nhất có thể so sánh đã báo cáo kết quả thí nghiệm trên ResNet-50, theo hiểu biết tốt nhất của chúng tôi. Rõ ràng, GSM vượt trội hơn L-OBS với một khoảng cách rõ ràng (Bảng 3). Chúng tôi giả định rằng hiệu quả của GSM trên một mạng rất sâu như vậy là do khả năng khám phá tỷ lệ thưa thích hợp theo từng lớp, cho một tỷ lệ nén toàn cục mong muốn. Ngược lại, L-OBS thực hiện cắt tỉa lớp theo lớp sử dụng cùng tỷ lệ nén. Giả định này được xác minh thêm trong Phần 4.2.

### 4.2 GSM để quyết định tỷ lệ thưa theo từng lớp tự động

Các DNN hiện đại thường chứa hàng chục hoặc thậm chí hàng trăm lớp. Khi kiến trúc sâu hơn, việc đặt tỷ lệ thưa theo từng lớp thủ công để đạt được tỷ lệ nén toàn cục mong muốn trở nên ngày càng không thực tế. Do đó, cộng đồng nghiên cứu đang tìm kiếm các kỹ thuật có thể tự động khám phá tỷ lệ thưa thích hợp trên các mô hình rất sâu. Trong thực tế, chúng tôi nhận thấy rằng nếu cắt tỉa trực tiếp một lớp đơn của mô hình gốc bằng một tỷ lệ cố định dẫn đến giảm độ chính xác đáng kể, GSM tự động chọn cắt tỉa nó ít hơn, và ngược lại.

Trong phần này, chúng tôi trình bày một phân tích định lượng về độ nhạy cảm với cắt tỉa, đây là một thuộc tính cơ bản của một lớp được định nghĩa thông qua một proxy tự nhiên: giảm độ chính xác gây ra bởi việc cắt tỉa một tỷ lệ nhất định các tham số từ nó. Trước tiên chúng tôi đánh giá độ nhạy cảm như vậy thông qua các nỗ lực cắt tỉa lớp đơn với các tỷ lệ cắt tỉa khác nhau (Hình 2). Ví dụ, đối với đường cong được gắn nhãn là "cắt tỉa 90%" của LeNet-5, trước tiên chúng tôi thí nghiệm trên lớp đầu tiên bằng cách đặt 90% các tham số có độ lớn nhỏ hơn về không sau đó kiểm tra trên tập xác thực. Sau đó chúng tôi khôi phục lớp đầu tiên, cắt tỉa lớp thứ hai và kiểm tra. Cùng một quy trình được áp dụng cho lớp thứ ba và thứ tư. Sau đó, chúng tôi sử dụng các tỷ lệ cắt tỉa khác nhau là 99%, 99.5%, 99.7%, và có được ba đường cong theo cách tương tự. Từ các thí nghiệm như vậy, chúng tôi biết rằng lớp đầu tiên nhạy cảm hơn nhiều so với lớp thứ ba, vì việc cắt tỉa 99% các tham số từ lớp đầu tiên làm giảm độ chính xác Top1 khoảng 85% (tức là, xuống gần trên 10%), nhưng làm như vậy trên lớp thứ ba chỉ làm giảm nhẹ độ chính xác 3%.

Sau đó chúng tôi chỉ ra tỷ lệ khác không theo từng lớp kết quả của các mô hình đã cắt tỉa GSM (LeNet-5 cắt tỉa 125× và DenseNet-40 cắt tỉa 6.6×, như được báo cáo trong Bảng 1, 2) như một proxy khác cho độ nhạy cảm, trong đó các đường cong được gắn nhãn là "GSM discovered" trong Hình 2. Vì hai đường cong thay đổi theo cùng xu hướng trên các lớp như những đường cong khác, chúng tôi phát hiện ra rằng các độ nhạy cảm được đo trong hai proxy có mối quan hệ chặt chẽ, điều này cho thấy rằng GSM tự động quyết định cắt tỉa các lớp nhạy cảm ít hơn (ví dụ, lớp thứ 14, 27 và 40 trong DenseNet-40, thực hiện các chuyển đổi giai đoạn [27]) và các lớp không nhạy cảm nhiều hơn để đạt được tỷ lệ nén toàn cục mong muốn, loại bỏ nhu cầu công việc thủ công nặng để điều chỉnh tỷ lệ thưa làm siêu tham số.

### 4.3 Momentum để tăng tốc làm không tham số

Chúng tôi điều tra vai trò momentum đóng trong GSM bằng cách chỉ thay đổi hệ số momentum và giữ tất cả các cấu hình huấn luyện khác giống như DenseNet-40 cắt tỉa 8× trong Phần 4.1. Trong quá trình huấn luyện, chúng tôi đánh giá mô hình cả trước và sau cắt tỉa mỗi 8000 vòng lặp (tức là, 10.24 epoch). Chúng tôi cũng trình bày trong Hình 3 tỷ lệ toàn cục của các tham số có độ lớn dưới 1×10^-3 và 1×10^-4, tương ứng. Như có thể quan sát, một hệ số momentum lớn có thể tăng đáng kể tỷ lệ các tham số có độ lớn nhỏ. Ví dụ, với tỷ lệ nén mục tiêu là 8× và γ = 0.98, GSM có thể làm cho 87.5% các tham số gần với không (dưới 1×10^-4) trong khoảng 150 epoch, do đó việc cắt tỉa mô hình không gây tổn hại. Và với γ = 0.90, 400 epoch không đủ để làm không các tham số một cách hiệu quả, do đó việc cắt tỉa làm giảm độ chính xác xuống khoảng 65%. Mặt khác, vì giá trị γ lớn hơn mang lại sự thay đổi cấu trúc nhanh hơn trong mô hình, độ chính xác ban đầu giảm ở đầu nhưng tăng khi sự thay đổi như vậy trở nên ổn định và quá trình huấn luyện hội tụ.

### 4.4 GSM để kích hoạt lại kết nối ngầm

GSM thực hiện kết nối lại một cách ngầm bằng cách thực hiện lựa chọn kích hoạt tại mỗi vòng lặp để khôi phục các tham số đã bị phạt sai (tức là, đã trải qua ít nhất một cập nhật thụ động). Chúng tôi điều tra tầm quan trọng của việc làm như vậy bằng cách cắt tỉa DenseNet-40 8× một lần nữa sử dụng γ = 0.98 và các cấu hình huấn luyện tương tự như trước nhưng không có tái lựa chọn (Hình 4). Cụ thể, chúng tôi sử dụng các ma trận mặt nạ được tính toán tại vòng lặp đầu tiên để hướng dẫn các cập nhật cho đến cuối quá trình huấn luyện. Được quan sát rằng nếu tái lựa chọn bị hủy, mất mát huấn luyện trở nên cao hơn, và độ chính xác bị giảm. Điều này là do lựa chọn đầu tiên quyết định loại bỏ một số kết nối không quan trọng đối với vòng lặp đầu tiên nhưng có thể quan trọng đối với các ví dụ đầu vào tiếp theo. Không có tái lựa chọn, GSM khăng khăng làm không các tham số như vậy, dẫn đến độ chính xác thấp hơn. Và bằng cách mô tả tỷ lệ kích hoạt lại (tức là, tỷ lệ số lượng tham số chuyển từ thụ động sang chủ động so với tổng số tham số) tại việc tái lựa chọn của mỗi vòng lặp huấn luyện, chúng tôi biết rằng việc kích hoạt lại xảy ra trên một thiểu số các kết nối, và tỷ lệ giảm dần, sao cho quá trình huấn luyện hội tụ và tỷ lệ thưa mong muốn được đạt được.

### 4.5 GSM cho những vé số trúng mạnh mẽ hơn

Frankle và Carbin [15] báo cáo rằng các tham số được phát hiện là quan trọng sau khi huấn luyện thực sự quan trọng ngay từ đầu (sau khởi tạo ngẫu nhiên nhưng trước khi huấn luyện), được gọi là các vé số trúng, bởi vì chúng đã thắng xổ số khởi tạo. Được phát hiện rằng nếu chúng ta 1) khởi tạo ngẫu nhiên một mạng được tham số hóa bởi Θ₀, 2) huấn luyện và có được Θ, 3) cắt tỉa một số tham số từ Θ tạo ra một mạng con được tham số hóa bởi Θ', 4) đặt lại các tham số còn lại trong Θ' về các giá trị khởi tạo của chúng trong Θ₀, được gọi là các vé số trúng Θ̂, 5) cố định các tham số khác về không và chỉ huấn luyện Θ̂, chúng ta có thể đạt được một mức độ chính xác có thể so sánh với mô hình đã huấn luyện rồi cắt tỉa Θ'. Trong công trình đó, bước thứ ba được hoàn thành bằng cách đơn giản bảo tồn các tham số có độ lớn lớn nhất trong Θ. Chúng tôi phát hiện ra rằng GSM có thể tìm ra một bộ vé số trúng tốt hơn, vì việc huấn luyện các vé được khám phá bởi GSM mang lại độ chính xác cuối cùng cao hơn so với những vé được tìm thấy bằng độ lớn (Bảng 4). Cụ thể, chúng tôi chỉ thay thế bước 3 bằng một quá trình cắt tỉa thông qua GSM trên Θ, và sử dụng các tham số khác không kết quả làm Θ', và tất cả các cài đặt thí nghiệm khác được giữ nguyên để có thể so sánh. Thú vị là, 100% tham số trong lớp kết nối đầy đủ đầu tiên của LeNet-5 được cắt tỉa bởi cắt tỉa độ lớn 300×, sao cho các vé số trúng được tìm thấy hoàn toàn không thể huấn luyện được. Nhưng GSM vẫn có thể tìm ra các vé số trúng hợp lý. Thêm chi tiết thí nghiệm có thể được tìm thấy trong các mã.

Hai giải thích có thể cho sự vượt trội của GSM là: 1) GSM phân biệt các tham số không quan trọng bằng lựa chọn kích hoạt sớm hơn nhiều (tại mỗi vòng lặp) so với tiêu chí dựa trên độ lớn (sau khi hoàn thành huấn luyện), và 2) GSM quyết định các vé số trúng cuối cùng theo cách mạnh mẽ trước sai lầm (tức là, thông qua tái lựa chọn kích hoạt). Trực giác là vì chúng ta mong đợi tìm ra các tham số đã "thắng xổ số khởi tạo", thời điểm khi chúng ta đưa ra quyết định nên gần hơn với khi việc khởi tạo diễn ra, và chúng ta mong muốn sửa chữa các sai lầm ngay lập tức khi chúng ta nhận thức được các quyết định sai. Frankle và Carbin cũng lưu ý rằng có thể mang lại lợi ích khi cắt tỉa sớm nhất có thể [15], điều này chính xác là những gì GSM làm, vì GSM tiếp tục đẩy các tham số không quan trọng liên tục về không từ ngay đầu.

## 5 Kết luận

Chúng tôi đề xuất Global Sparse Momentum SGD (GSM) để trực tiếp thay đổi dòng gradient cho cắt tỉa DNN, chia việc cập nhật dựa trên momentum-SGD thông thường thành hai phần: cập nhật chủ động sử dụng các gradient dẫn xuất từ hàm mục tiêu để duy trì độ chính xác của mô hình, và cập nhật thụ động chỉ thực hiện suy giảm trọng số được tăng tốc bằng momentum để đẩy các tham số dư thừa vô cùng gần với không. GSM có đặc điểm huấn luyện từ đầu đến cuối, thực hiện dễ dàng, cắt tỉa không mất mát, kết nối lại ngầm, khả năng tự động khám phá tỷ lệ thưa thích hợp cho mỗi lớp trong các mạng nơ-ron rất sâu hiện đại và khả năng tìm ra các vé số trúng mạnh mẽ.
