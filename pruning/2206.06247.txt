# 2206.06247.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2206.06247.pdf
# File size: 155607 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2206.06247v1  [cs.NE]  13 Jun 2022Leveraging Structured Pruning of
Convolutional Neural Networks
Hugo Tessier∗†, Vincent Gripon†, Mathieu L´ eonardon†, Matthieu Arzel†, David Bertrand∗, Thomas Hannagan∗
∗Stellantis , V´ elizy-Villacoublay, France
{1, 5, 6}@stellantis.com
†IMT Atlantique , Lab-STICC, UMR CNRS 6285, F-29238 Brest, France
{1, 2, 3, 4 }@imt-atlantique.fr
Abstract —Structured pruning is a popular method to reduce
the cost of convolutional neural networks, that are the stat e of
the art in many computer vision tasks. However, depending on
the architecture, pruning introduces dimensional discrep ancies
which prevent the actual reduction of pruned networks. To ta ckle
this problem, we propose a method that is able to take any
structured pruning mask and generate a network that does not
encounter any of these problems and can be leveraged efﬁcien tly.
We provide an accurate description of our solution and show
results of gains, in energy consumption and inference time o n
embedded hardware, of pruned convolutional neural network s.
Index Terms —Deep Learning, Compression, Pruning, Energy,
Inference, GPU
I. I NTRODUCTION
Deep neural networks are at the state of the art in many
domains, such as computer vision. For instance, convolutio nal
neural networks are used to tackle different tasks such as
classiﬁcation [17] or semantic segmentation [16]. However ,
their cost in energy, memory and latency is prohibitive on
embedded hardware, and this is why many works focus on
reducing their cost to ﬁt targets with limited resources [1] .
The ﬁeld of deep neural networks compression counts multi-
ple types of method, such as quantization [3] or distillatio n [9].
The one we focus on in this article is pruning [5], that involv es
removing unnecessary weights from a network. Pruning is a
popular technique that presents many challenges, includin g
that of ﬁnding the most adequate type of sparsity to be
leveraged on hardware [13].
To focus on the theoretical approach of studying the impact
of removing weights from the network’s function on its accu-
racy, many papers only remove weights by putting their value
to zero. However, this does not reduce the cost of networks
and only provides a rough estimate of network compression in
terms of memory. Leveraging pruning to get gains on hardware
is actually not a trivial task. Pruning isolated weights [5]
(“non-structured pruning”) produces sparse matrices, tha t are
difﬁcult to accelerate [13]. Pruning entire convolution ﬁl ters
(a.k.a. “structured pruning”) is more easily exploitable, but the
input and output dimensions of layers are altered, which can
induce many problems in networks, especially those includi ng
long-range dependencies between layers [6]. The solution t o
this problem is, almost always, either not mentioned, or cir -
cumvented by constraining pruning into targeting only laye rsthat do not induce problems [11]. However, these constraint s
are expected to reduce the efﬁciency of pruning.
In this paper we propose a solution to reduce effectively the
size of networks using structured pruning, that were applie d
a mask using structured pruning. Our method is generic,
automatic and reliably produces an effectively pruned netw ork.
We demonstrate its ability to operate on networks of any
complexity by applying it on both a standard classiﬁcation
network [6] on the ImageNet ILSVRC2012 dataset [17] and on
a more complex semantic segmentation network [18] trained
on CityScapes [2]. We show that our solution allows gains
in energy consumption and inference time on embedded hard-
ware such as the NVIDIA Jetson AGX Xavier embedded GPU,
providing an actual estimate of how structured pruning can b e
leveraged to reduce energy and latency footprints on a real
hardware target.
II. R ELATED WORKS
Originally designed to improve generalization of neural
networks [10], pruning is now a popular method to reduce thei r
memory or computational footprints. The most basic form of
pruning involves masking out weights of least magnitude in
a non-structured way [5]. This method does not reduce the
size of the parameters’ tensors, but instead the introduced
zeroes help compressing the network weights through encode d
schemes [4]. However, getting any type of speed-up out of thi s
method is difﬁcult on most hardware [13].
To better leverage pruning on hardware, many methods
instead apply “structured pruning”, that usually involves prun-
ing whole neurons, i.e.ﬁlters in the case of convolution
layers [11]. Other types of structured pruning exist, such a s
“ﬁlter shape pruning” [21] and this is why we will favor
the “ﬁlter pruning” denomination to avoid ambiguity. Weigh t
pruning and ﬁlter pruning are the two most popular types of
pruning structures.
When pruning any type of structure, two aspects have to be
tackled: 1) how to identify elements to prune and 2) how to
prune them. The ﬁrst issue can be solved using various types
of pruning criteria. In the case of non-structured pruning, the
magnitude of weights [5] or their gradient [15] are two popul ar
criteria. When pruning ﬁlters, these criteria can be extend ed to
either their norm over a ﬁlter [11] or a proxy that accounts fo r
the whole ﬁlter’s importance, for example the multiplicati ve

--- PAGE 2 ---
C
CC
+
C R
Indexation-Addition Indexation-AdditionConvolution Layer Convolution Layer Convolution Layer
Fig. 1: Illustration of the difﬁculties when pruning ﬁlters in convolutional neural networks. Convolution layers are m ade of
ﬁlters, each one outputing a channel (or “feature map”). Gre yed out elements symbolise pruned ﬁlters and the kernels to
remove to ﬁt the dimensions of inputs (Problem 1). At the end o f every residual block, the output of the last layer is summed
with the input of the block. If the two tensors are pruned diff erently (Problem 3), what was an addition is now a mixture of
additions (+), concatenations (C) or bypasses (dashed circ les) that we call the indexation-addition operator (Section III-D).
The consequence is that the ﬁnal number of channels cannot be predicted solely from a particular layer in the network, but
must be deduced by taking into account all the dependencies ( Problem 2).
learned weight included in batch-normalization layers [12 ].
These criteria can be applied in two different ways: either t hey
are used to identify the same (or a pre-determined) amount of
weights/ﬁlters to remove in all layers (local pruning) or th e
target is set globally and the criterion is applied to all lay ers
at the same time (global pruning).
Concerning the second issue, many popular methods apply
a simple framework [4]: training the network, pruning a give n
proportion of weights by masking them away, ﬁne-tuning the
network and repeating the last two steps multiple times unti l
a target pruning rate is reached. Other methods can involve a
more progressive approach [7] that can include a regrowing
mechanism [14]. Some techniques propose a more continuous
way to prune weights, for example by applying them a penalty
during training [20].
III. M ETHOD
A. Consequences of Structured Pruning
In Section II, we explained what is structured pruning. In
order to present the problems it can induce, as well as the
solutions we propose, we need to introduce some notations.
LetNbe a convolutional neural network. For the sake
of convenience, we will consider that it is only made of
convolutional layers li, whose input and output dimensions are
fi
inandfi
out. Each convolution contains fi
out×fi
in×ki
h×ki
w
weightswi(withki
h×ki
wthe size of the layer’s kernel) and
fi
outbiasesbi. A ﬁlter corresponds to the fi
in×ki
h×ki
w
weights and one bias that produce one of the fi
outchannels
in the output feature maps. Each of these layers operates on
feature maps of size fi
in×hi×wiwithhi×withe resolution of
the feature maps. In the case of networks such as ResNet [6]
or HRNet [18], different layers can take the same feature
maps as an input and multiple feature maps can be summed
together. This simpliﬁed presentation is sufﬁcient to expo se
the problems induced by global pruning.a) Problem 1: Pruning ﬁlters reduces the output dimen-
sionfoutof a layer. Therefore, the dimension of its output
is different and the input dimension finof the following
layers must be adapted. This problem is well-known in the
literature [11] and easy to solve in simple networks.
b) Problem 2: Residual connections [6] can introduce
long-range dependencies and, therefore, identifying all t he lay-
ers impacted by the change in dimension can be difﬁcult. This
problem is usually solved by avoiding pruning layers involv ed
in such dependencies [11], but this solution is suboptimal.
c) Problem 3: Residual connections [6] usually involve
summing together feature maps, that must therefore be of
same dimensions, which is not the case anymore after global
pruning. In the case of local pruning, feature maps are of
the same dimensions, but the same mask may not have
been applied on both feature maps, and summing together
channels that are meant to be summed together produces a
tensor of higher dimensions. This problem is less discussed
in the literature and mostly solved using custom operations to
manually adapt dimensions of feature maps [8].
These three problems, illustrated in Figure 1, are either
eluded or not solved in the literature, even though most
papers deal with ResNet-based architectures that are causi ng
all of the three. Some expertise allows manually ﬁguring out
dependencies in such networks, but the complexity can get ou t
of hand in the case of networks such as HRNets [18]. Indeed,
missing any of these problems makes the networks impossible
either to run efﬁciently or to run at all on hardware. This is t he
reason why we propose a method that can automatically and
reliably produce pruned networks that can be ran efﬁciently
on hardware.
B. Generalizing Operators to Handle a Subset of Channels

--- PAGE 3 ---
+ - +
0 + 0
- 0 ++ 0 +
-
0- - +
-
+
0
0
0+
-
+-
+
-
0 0 0
0 0 0
0 0 0- + -
0
+0 0 +
+
-
X
BN Normalization
Batch-Normalization Weights
Biases
+ + +
+ + +
+ + ++ - +
+
-+ + 0
-
-
ReLU
+ + +
+ + +
+ + ++ 0 +
+
0+ + 0
0
0Activation FunctionConvolutionInputs + + +
+ + +
+ + ++ + +
+
++ + +
+
+
0
0
01
1
11
1
1
0 0 0
0 0 0
0 0 0+ + +
+
++ + +
+
+
X
0 0 0
0 0 0
0 0 0+ + +
+
++ + +
+
+A
B
C
D
E
F
Fig. 2: Illustration of the proposed method to identify disc on-
nected weights, with the original network on the left and the
modiﬁed version on the right. (A) Input tensors are uniform t o
avoid unwanted null values, (B) weights of layers are replac ed
with their mask, therefore (C) the output only contain null
values if a ﬁlter is pruned. (D) Normalization and biases are
removed to keep null values null and (E) activation function s
are removed not to add extra ones. The ﬁnal output (F) allows
deducing which ﬁlters are pruned.
The ﬁrst step of our method is to make sure a given network
is robust to pruning. Indeed, networks such as ResNets or
HRNets contain operations that are applied to outputs of
multiple layers. In such cases, the involved tensors must be of
the same dimension, which may not be the case anymore after
pruning. In the case of ResNets and HRNets, all operations
of these types are additions of two tensors, such as those at
the end of every residual connection. This means that we can
tackle this problem by replacing additions with a generaliz ed
operator able to handle missing ﬁlters in any of its inputs.
To this mean, we replace additions with a new indexation-
addition operation, with aandbthe tensors to sum, that
contain respectively naandnbchannels, iaandibtwo listsof indices and the output tensor c, that contains ncchannels,
deﬁned in Equation (1):
∀k∈/llbracket1;nc/rrbracket,ck=/braceleftBigg
aia
k,ifia
k∈/llbracket1;na/rrbracket
∅,otherwise(1)
+/braceleftBigg
bib
k,ifib
k∈/llbracket1;nb/rrbracket
∅, otherwise
Ifna=nb,ia= [1,2,...,na]andib=/bracketleftbig
1,2,...,nb/bracketrightbig
,
this indexation-addition operation is purely equivalent to an
element-wise addition. Properly parameterized by adequat e
iaandib, this operation allows leveraging any type of ﬁlter
pruning. It is however necessary to ﬁnd the right iaandiband
we provide a solution in Section III-D. Figure 1 illustrates how
our solution relates to the problems mentioned in Section II I-A
and provides a simple way to view how it can behave like a
mix of additions and concatenations.
C. Automatic Adaptation of Networks
Once the network is prepared for pruning by the intro-
duction of this new indexation-addition operation to ﬁt any
distribution of the sparsity induced by pruning, the next st ep
of the method is to identify automatically all dependencies
between ﬁlters, kernels, biases or any sort of weights in the
network. In summary, it is necessary to search for all the par ts
of the network that are disconnected when removing ﬁlters.
To identify all parameters whose contribution in a network’ s
function is null, one can use its gradient over, for example, a
mini-batch from the training data. Indeed, provided this mi ni-
batch is a satisfying approximation of the network’s domain
of deﬁnition, a null gradient means that the network’s funct ion
is null relatively to the involved weights, or at least const ant
in the case of biases. However, for our use-case, this is insu fﬁ-
cient: not only does it not allow removing disconnected bias es
that still produce constant outputs that somehow contribut e to
the function, but it may also identify some isolated weights
as pruned in a non-structured way, while it is not possible to
leverage them.
This is why we instead operate on an architectural abstrac-
tion of the network, which is a copy of it that received three
modiﬁcations that are illustrated in Figure 2:
•Its biases are removed to prevent them from adding a
constant output that makes some disconnected/useless
weights downstream have a non-null gradient.
•Its activation functions, and other non-linear operations
such as normalization, are removed, so that a non-null
input of a layer cannot produce a null output and gradient.
•The value of its weights are replaced by the value of the
mask, made either of zeros or ones, so that, when fed with
an input ﬁlled with non-null values of the same sign, the
output cannot contain null values if it is not because of
null, masked out weights.
Because of these modiﬁcations, a single input ﬁlled with
non-null values of the same sign is enough to identify all

--- PAGE 4 ---
disconnected weights. Indeed, this network behaves like a
purely linear and positive function and any null gradient in
its parameters can only be due to a null function that can
be removed. Weights, identiﬁed as disconnected in this copy
network, are then removed from the original network.
D. Automatic Indexation
To deduce automatically the right iaandibdeﬁned in
Section III-B, we add another modiﬁcation to the copy networ k
described in Section III-C: we apply an identity convolution
to the two tensors before summing them together. This identity
convolution has weights of shape n×n×1×1(withnthe
number of channels in the input tensor) whose values equates
that of an identity matrix.
The gradient of the weights of this identity convolution
allows deducing the corresponding list of indices. Indeed, once
the null rows and columns of its weights are removed, the
output dimensions are the same for both tensors to be summed
while the input dimension matches that of the input tensors
after pruning. The zero and non-zero remaining coefﬁcient
allows deducing how to map the input and output channels.
E. Summary of the Method
Here are all the steps to follow to apply our method:
Algorithm 1 Summary of the Method
1:train the network N
2:generate the pruning mask mthat masks out ﬁlters
3:create a copy N′of the network
4:remove all biases bfromN′
5:remove all activation functions and normalization from N′
6:replace the weights wofN′bym
7:insert the identity convolutions where needed in N′
8:generate an input tensor x, of adequate size, ﬁlled with
ones and run N′(x)
9:computedN′
dw(x)
10:generate the new pruning mask m′that masks away all
weights whose gradient is null in N′
11:applym′toNand mask away biases whose weights are
pruned
12:deduce from the mask of the identity convolutions the right
iaandibto replace additions with indexation-addition
operations where needed
The method, summed up in Algorithm 1, solves all problems
presented in Section III-A. It allows pruning a network and
then generating its nearest equivalent whose dimensions ar e
consistent and that can be leveraged on hardware. Since our
method not only removes weights of null contribution but als o
biases whose gradient is constant, the function of the netwo rk
is not preserved. However, the impact on accuracy is negligi ble
and detailed in our experiments in Section IV-B.
IV. E XPERIMENTS
In our experiments, we will ﬁrst detail the impact of
our method on both the accuracy of the network and theevaluation of its compression rate. Then we will demon-
strate how the networks, whose type of sparsity usually
prevents running inference, can be leveraged efﬁciently
on resource-limited hardware. Our source code is avail-
able at: https://github.com/HugoTessier-lab/Neural-Ne twork-
Shrinking.git
A. Training conditions
a) ImageNet: We trained ResNet-50 [6] on the ImageNet
ILSVRC2012 image classiﬁcation dataset [17] for 90 epochs
with a batch-size of 170 and a learning rate of 0.01 reduced by
10 every 30 epochs. We used the SGD optimizer with weight
decay set to 1·10−4and momentum set to 0.9.
b) Cityscapes: We trained the HRNet-48 network [18]
on the Cityscapes semantic segmentation dataset [2] for 200
epochs with a batch size of 10 and a learning rate of 0.01
reduced by (1−current epoch
epochs)2at each epoch. We used the
RMI loss [22] and the SGD optimizer with weight decay set
to5·10−4and momentum set to 0.9. During training, images
are randomly cropped and resized, with a scale of [0.5,2],
to3×512×1024 . Data augmentation involves random ﬂips,
random Gaussian blur and color jittering.
c) Pruning: We prune networks following the method
of Liu et al. [12]: pruning is divided in three iterations, wi th
a linearly growing proportion of removed ﬁlters until the
ﬁnal pruning rate is matched. At each iteration, ﬁlters are
masked out depending on the magnitude of the weight of
their batch-normalization layer. After each iteration, Re sNet-
50 ﬁne-tuned during 10 epochs and HRNet-48 during 20
epochs. The method of Liu et al. [12] also implies penalizing
weights of batch-normalization layers with a smooth- L1norm,
with an importance factor of λ= 10−5for ResNet-50 and
λ= 10−6for HRNet-48.
B. Impact on Accuracy and Compression Rate
0 50 100020406080
Pruning Rate (%)Top-1 accuracy (%)
0 50 100020406080
Pruning Rate (%)mIoU (%)
Before After
Fig. 3: For ResNet-50 on ImageNet (left) or HRNet-48 on
Cityscapes (right): accuracy depending on pruning rate, ei ther
in terms of proportion of pruned ﬁlters (blue) or remaining
parameters after application of our method (red).
In our experiments, we reported mostly no difference in
accuracy before and after applying our method, as it can be
seen in Figure 3. This implies that the parameters removed by

--- PAGE 5 ---
our method, that did not have a null contribution to the func-
tion, such as the remaining biases mentioned in Section III- C,
might have had a negligible impact on the network’s accuracy .
The only outliers are points where accuracy is already sever ely
decreased, for example the accuracy of ResNet-50 pruned at
60% that goes from 66.05% to 63.478%, while the baseline
is at 75.7%.
In Figure 3 we also show the trade-off between accuracy
and two types of pruning rate: one deﬁned as the proportion
of removed ﬁlters, which is a widespread target criterion in
the literature, and one deﬁned as the exact count of remainin g
parameters in the network once our method has been applied.
We see that using the percentage of removed ﬁlter is not
faithful to the actual compression rate of the network. The
actual trade-off is more advantageous once our method has
been applied to both purge the network from useless weights
and get a faithful estimation of all eliminated weights.
100101100101
Filters Comp.Param. Comp.
100101100101
Filters Comp.Param. Comp.
Fig. 4: For ResNet-50 on ImageNet (left) or HRNet-48 on
Cityscapes (right): relation between the estimated compre ssion
rate in terms of pruned ﬁlters (x-axis) and remaining param-
eters after reducing the network using our method (y-axis).
In Figure 4, we compare the compression rate ( i.e.
100%
100%−pruning rate%) in terms of removed ﬁlters or removed
parameters, i.e.before and after our method. The relationship
between the two measures seem to depend on the involved
architecture and we expect it to depend on the pruning
criterion too.
C. Impact on Hardware
To measure the inference time and energetic consumption
of pruned networks on NVIDIA Jetson AGX Xavier in the
“30W All” mode, we ﬁrst converted our networks to ONNX,
that is a format that can be handled by many frameworks
on most hardware. The indexation-addition operations were
implemented using ScatterND andtranspose operators. Scat-
terND allows operating on slices in tensors and transpositions
allow operating speciﬁcally on channels, while Scatter is
element-wise and requires storing a cumbersome array of
indices. Before summation, both tensors need to be scattere d
into a temporary tensor, that is instantiated dynamically. We
used the JetPack SDK 5.0, with CUDA 11.4.14, cuDNN 8.3.2,
TensorRT 8.4.0 EA and ONNX Runtime 1.12.0. Energetic
consumption was given using the tegrastats utility. Infere nce
on ResNet-50 is run with an input of size (1×3×224×224)0 50 100050100
Pruning Rate (%)Energy (mJ)
0 50 100051015
Pruning Rate (%)Inference Time (ms)
(a) ResNet-50
0 50 1000123
Pruning Rate (%)Energy (J)
0 50 1000100200300400
Pruning Rate (%)Inference Time (ms)
(b) HRNet-48
Fig. 5: Energetic consumption and inference time of ResNet-
50 and HRNet-48, depending on the pruning rate in terms
of parameters, on NVIDIA Jetson AGX Xavier in the “30W
All” mode, using JetPack SDK 5.0 and ONNX Runtime
1.12.0 running with the TensorRT execution provider. Re-
sults are averaged over 10k inferences with inputs of size
(1×3×224×224) after 1k runs of warm-up for ResNet-
50 and 1k inferences with inputs of size (1×3×512×1024)
after 100 runs of warm-up for HRNet-48.
and HRNet-48 with one of size (1×3×512×1024) . ONNX
Runtime was used with the TensorRT execution provider, that
turned out to be the one that gave the best inference time.
Figure 5 provides results for ResNet-50 on ImageNet and
HRNet-48 on Cityscapes. Both show similar tendencies: at
ﬁrst, the extra cost of indexation-addition operations takes a
toll on the efﬁciency of pruning, but after that initial jump ,
the cost of networks, either in terms of energy consumption
or inference time, decreases signiﬁcantly. This shows that ,
although a better implementation of the indexation-addition
operations would be beneﬁcial, our current solution is enou gh
for free and unconstrained structured pruning to be cost
effective. Therefore, we can say that it is possible to lever age
efﬁciently any type of ﬁlter pruning in even complex deep
convolutional neural networks.
V. D ISCUSSION
Three observations can be drawn from our experiments: 1)
our method allows a more reliable measurement of the count
of remaining parameters in the network, as can be seen in
Figure 3, 2) the relation between this accurate pruning rate and
inference time or energy consumption is non-linear and 3) th e
cost introduced by our custom operators is not negligible an d

--- PAGE 6 ---
makes the least pruned networks cost more than non-pruned
ones, as can be seen in Figure 5.
The ﬁrst two observations show that our method is a useful
tool to better study the efﬁciency of unconstrained ﬁlter pr un-
ing. Indeed, it produces a network in which the vast majority
of remaining parameters are guaranteed to contribute to the
function, with the marginal exception of some isolated weig hts
that may be inactive by accident. Therefore, it is now possib le
to directly measure the accuracy-to-energy or accuracy-to -
latency trade-off, which provide a more relevant insight in to
the impact of pruning on hardware than a more theoretical
accuracy-to-parameters trade-off. Since this is not the fo cus of
this article, we did not provide such an analysis and did not
choose the pruning method that gave the absolute best possib le
performance. This will be the focus of future contributions .
This ability to provide a more faithful compression rate tha n
the naive rate of removed ﬁlters also allows better controll ing
the growth of pruning rate between pruning iterations. This is
likely to help improving performance and avoiding to remove
entire layers by accident, which is called layer collapse [19].
Concerning the last observation, ﬁnding the best implemen-
tation of the custom operators, necessary to run pruned net-
works, obviously requires further investigation. Using tr texec,
we did the proﬁling of the operators of the HRNet-48, with
10% of the ﬁlters pruned and 6.26% of removed parameters,
which is the HRNet-48 with the highest inference time. It
turned out that the “Foreign Nodes” generated by TensorRT,
that contain the ScatterND we used for our indexation-addition
operations, are responsible for 14.8% of the total inferenc e
time. When substracting the cost of these nodes from the
network’s total average time of 369.8ms according to trtexe c,
the remaining inference time is of 314.3ms, which is actuall y
lower than that of the non-pruned network, which is of
318.7ms. This means that if an optimized implementation of
the operators allowed their cost to be negligible, it would m ake
pruning a lot more beneﬁcial, even at low pruning rates.
VI. C ONCLUSION
We have proposed an efﬁcient and generic way to lever-
age any type of ﬁlter pruning in deep convolutional neural
networks. Indeed, even though removing ﬁlters in a network
can trigger a certain array of problems that can even prevent
running its inference, our solution is able to tackle them
and generates functional pruned networks that can be run
efﬁciently on hardware. Our experiments, even though they
show that our current ONNX implementation has a non-
negligible cost, demonstrate that unconstrained ﬁlter pru ning
can be cost-effective.
REFERENCES
[1] Chun-Fu Chen, Gwo Giun Lee, Vincent Sritapan, and Ching- Yung Lin.
Deep convolutional neural network on ios mobile devices. In 2016 IEEE
International Workshop on Signal Processing Systems (SiPS ), pages
130–135. IEEE, 2016.
[2] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Reh feld,
Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Rot h, and
Bernt Schiele. The cityscapes dataset for semantic urban sc ene under-
standing. In Proceedings of the IEEE conference on computer vision
and pattern recognition , pages 3213–3223, 2016.[3] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre D avid. Bina-
ryconnect: Training deep neural networks with binary weigh ts during
propagations. Advances in neural information processing systems , 28,
2015.
[4] Song Han, Huizi Mao, and William J Dally. Deep compressio n:
Compressing deep neural networks with pruning, trained qua ntization
and huffman coding. arXiv preprint arXiv:1510.00149 , 2015.
[5] Song Han, Jeff Pool, John Tran, and William Dally. Learni ng both
weights and connections for efﬁcient neural network. Advances in neural
information processing systems , 28, 2015.
[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. De ep
residual learning for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pages 770–778,
2016.
[7] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yan g. Soft
ﬁlter pruning for accelerating deep convolutional neural n etworks. arXiv
preprint arXiv:1808.06866 , 2018.
[8] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning fo r accelerat-
ing very deep neural networks. In Proceedings of the IEEE international
conference on computer vision , pages 1389–1397, 2017.
[9] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distil ling the knowl-
edge in a neural network. arXiv preprint arXiv:1503.02531 , 2(7), 2015.
[10] Yann LeCun, John Denker, and Sara Solla. Optimal brain d amage.
Advances in neural information processing systems , 2, 1989.
[11] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Ha ns Pe-
ter Graf. Pruning ﬁlters for efﬁcient convnets. arXiv preprint
arXiv:1608.08710 , 2016.
[12] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoum eng Yan,
and Changshui Zhang. Learning efﬁcient convolutional netw orks
through network slimming. In Proceedings of the IEEE international
conference on computer vision , pages 2736–2744, 2017.
[13] Xiaolong Ma, Sheng Lin, Shaokai Ye, Zhezhi He, Linfeng Z hang, Geng
Yuan, Sia Huat Tan, Zhengang Li, Deliang Fan, Xuehai Qian, et al. Non-
structured dnn weight pruning–is it beneﬁcial in any platfo rm? IEEE
Transactions on Neural Networks and Learning Systems , 2021.
[14] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H
Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable tr aining of
artiﬁcial neural networks with adaptive sparse connectivi ty inspired by
network science. Nature communications , 9(1):1–12, 2018.
[15] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila , and Jan
Kautz. Pruning convolutional neural networks for resource efﬁcient
inference. arXiv preprint arXiv:1611.06440 , 2016.
[16] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convo-
lutional networks for biomedical image segmentation. In International
Conference on Medical image computing and computer-assist ed inter-
vention , pages 234–241. Springer, 2015.
[17] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sa njeev
Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya K hosla,
Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Image Net Large
Scale Visual Recognition Challenge. International Journal of Computer
Vision (IJCV) , 115(3):211–252, 2015.
[18] Ke Sun, Yang Zhao, Borui Jiang, Tianheng Cheng, Bin Xiao , Dong Liu,
Yadong Mu, Xinggang Wang, Wenyu Liu, and Jingdong Wang. High -
resolution representations for labeling pixels and region s.arXiv preprint
arXiv:1904.04514 , 2019.
[19] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Sur ya Ganguli.
Pruning neural networks without any data by iteratively con serving
synaptic ﬂow. Advances in Neural Information Processing Systems ,
33:6377–6389, 2020.
[20] Hugo Tessier, Vincent Gripon, Mathieu L´ eonardon, Mat thieu Arzel,
Thomas Hannagan, and David Bertrand. Rethinking weight dec ay for
efﬁcient neural network pruning. Journal of Imaging , 8(3):64, 2022.
[21] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li .
Learning structured sparsity in deep neural networks. Advances in neural
information processing systems , 29, 2016.
[22] Shuai Zhao, Yang Wang, Zheng Yang, and Deng Cai. Region m utual
information loss for semantic segmentation. Advances in Neural Infor-
mation Processing Systems , 32, 2019.
