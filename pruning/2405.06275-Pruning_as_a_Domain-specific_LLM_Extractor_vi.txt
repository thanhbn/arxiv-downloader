# Cắt tỉa như một Bộ trích xuất LLM chuyên biệt theo Lĩnh vực

Nan Zhang♣†Yanchi Liu♢Xujiang Zhao♢Wei Cheng♢
Runxue Bao♢Rui Zhang♣Prasenjit Mitra♣Haifeng Chen♢
♣Đại học Bang Pennsylvania ♢NEC Labs America
{njz5124,rmz5227,pmitra}@psu.edu
{yanchi,xuzhao,weicheng,rbao,haifeng}@nec-labs.com

Tóm tắt

Các Mô hình Ngôn ngữ Lớn (LLMs) đã thể hiện khả năng xuất sắc trên một loạt rộng lớn các nhiệm vụ NLP. Tuy nhiên, việc gia tăng kích thước mô hình cũng tạo ra chi phí triển khai đáng kể. Trong khi ít nỗ lực đã khám phá các kỹ thuật cắt tỉa mô hình để giảm kích thước của LLMs, chúng chủ yếu tập trung vào các trọng số chung hoặc chuyên biệt theo nhiệm vụ. Điều này dẫn đến hiệu suất dưới mức tối ưu do thiếu tính chuyên biệt trên lĩnh vực mục tiêu hoặc tính tổng quát trên các nhiệm vụ khác nhau khi áp dụng cho các thách thức chuyên biệt theo lĩnh vực. Nghiên cứu này giới thiệu một phương pháp cắt tỉa kép không có cấu trúc đổi mới, D-PRUNER, để nén chuyên biệt theo lĩnh vực trên LLM. Nó trích xuất một LLM nén, chuyên biệt theo lĩnh vực, và không phụ thuộc nhiệm vụ bằng cách xác định các trọng số LLM quan trọng cho khả năng chung, như khả năng ngôn ngữ và giải quyết đa nhiệm vụ, và kiến thức chuyên biệt theo lĩnh vực. Cụ thể hơn, chúng tôi đầu tiên đánh giá tầm quan trọng của trọng số chung bằng cách lượng hóa lỗi phát sinh khi loại bỏ chúng với sự trợ giúp của một bộ dữ liệu hiệu chuẩn miền mở. Sau đó, chúng tôi sử dụng tầm quan trọng trọng số chung này để tinh chỉnh hàm mất mát huấn luyện, để nó bảo tồn tính tổng quát khi phù hợp với một lĩnh vực cụ thể. Hơn nữa, bằng cách ước lượng hiệu quả tầm quan trọng trọng số với hàm mất mát huấn luyện được tinh chỉnh trên một bộ dữ liệu hiệu chuẩn chuyên biệt theo lĩnh vực, chúng tôi thu được một mô hình đã cắt tỉa nhấn mạnh tính tổng quát và tính chuyên biệt. Các thí nghiệm toàn diện của chúng tôi trên các nhiệm vụ khác nhau trong lĩnh vực y tế và pháp lý cho thấy hiệu quả của D-PRUNER trong nén chuyên biệt theo lĩnh vực. Mã nguồn của chúng tôi có sẵn tại https://github.com/psunlpgroup/D-Pruner.

1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LLMs) như họ GPT (Brown et al., 2020) và họ LLaMA (Touvron et al., 2023) đã thể hiện những tiến bộ đáng chú ý trên một phổ rộng các nhiệm vụ NLP. Tuy nhiên, kích thước đáng kể của LLMs tạo ra việc triển khai tốn kém chi phí trong các ứng dụng thực tế và khiến chúng không phù hợp cho các tình huống đòi hỏi suy luận hiệu quả và độ trễ thấp (Bai et al., 2024). Gần đây, các kỹ thuật cắt tỉa mô hình đã được áp dụng thành công cho các mô hình ngôn ngữ (Han et al., 2015; Xia et al., 2022; Frantar and Alistarh, 2023). Các phương pháp này nhằm tạo ra một mô hình ngôn ngữ gọn nhẹ được đặc trưng bởi số lượng tham số giảm đáng kể, có hiệu quả chi phí để triển khai. Tuy nhiên, hầu hết chúng nhắm mục tiêu các mô hình ngôn ngữ tương đối nhỏ, và chỉ một số ít tập trung vào LLMs (Frantar and Alistarh, 2023; Ma et al., 2023; Sun et al., 2023; Xia et al., 2023). Hơn nữa, các chiến lược hiện tại chủ yếu tập trung vào các trọng số chung hoặc chuyên biệt theo nhiệm vụ, dẫn đến hiệu suất dưới mức tối ưu do thiếu tính chuyên biệt trên lĩnh vực mục tiêu hoặc tính tổng quát trên các nhiệm vụ khác nhau khi áp dụng cho các thách thức chuyên biệt theo lĩnh vực. Ở đây tính tổng quát đề cập đến các khả năng chung của một LLM như hiểu biết và tạo sinh ngôn ngữ, và giải quyết đa nhiệm vụ, và tính chuyên biệt đề cập đến khả năng của một LLM để hiểu kiến thức chuyên biệt theo lĩnh vực.

Như thể hiện trong Hình 1, các trọng số trong một LLM làm việc cùng nhau để hỗ trợ các khả năng chung của nó và để lưu trữ kiến thức lĩnh vực khác nhau. Các trọng số chia sẻ lĩnh vực (hoặc trọng số chung) trao quyền cho LLM với khả năng ngôn ngữ và giải quyết đa nhiệm vụ tương tự như việc sử dụng ngôn ngữ và tư duy của con người. Các trọng số chuyên biệt theo lĩnh vực (hoặc trọng số lĩnh vực) là then chốt để trang bị cho LLM chuyên môn chuyên biệt theo lĩnh vực phản ánh của các chuyên gia lĩnh vực. Tuy nhiên, các phương pháp cắt tỉa hiện tại chủ yếu tập trung vào việc bảo tồn các trọng số chung hoặc chuyên biệt theo nhiệm vụ, có thể không đủ để xử lý các vấn đề chuyên biệt theo lĩnh vực. Ví dụ, các phương pháp cắt tỉa sau huấn luyện (Frantar and Alistarh, 2023) giả định mô hình được tối ưu hóa và cắt tỉa các trọng số không quan trọng dựa trên một bộ dữ liệu hiệu chuẩn miền mở. Điều này dẫn đến một mô hình đã cắt tỉa tập trung vào tính tổng quát của mô hình với các trọng số chuyên biệt theo lĩnh vực không được xem xét. Mặt khác, các phương pháp cắt tỉa với tinh chỉnh (Ma et al., 2023) sử dụng gradient trong quá trình tinh chỉnh trên một nhiệm vụ cụ thể để ước lượng tầm quan trọng của các tham số. Kết quả là, mô hình đã cắt tỉa tập trung vào tính chuyên biệt của mô hình trong khi giảm khả năng ngôn ngữ và giải quyết đa nhiệm vụ, làm tổn hại khả năng của LLM như một bộ giải quyết đa năng không phụ thuộc nhiệm vụ.

Để giải quyết vấn đề này, nghiên cứu này giới thiệu một phương pháp cắt tỉa kép mới, D-PRUNER, để cắt tỉa không có cấu trúc chuyên biệt theo lĩnh vực trên LLMs, nhằm trích xuất một LLM chuyên biệt theo lĩnh vực từ LLM nền tảng. Mô hình được trích xuất này có thể giải quyết các nhiệm vụ khác nhau trong lĩnh vực mục tiêu và tạo điều kiện cho việc tinh chỉnh chuyên biệt theo lĩnh vực tiếp theo. D-PRUNER được thiết kế để khai thác dữ liệu hiệu chuẩn để hướng dẫn các quá trình cắt tỉa LLM trong khi bảo tồn tính tổng quát và tính chuyên biệt cho việc giải quyết đa nhiệm vụ và các thách thức lĩnh vực. LLM nén kết quả có thể được điều chỉnh một cách liền mạch cho lĩnh vực mục tiêu, cho phép triển khai với tài nguyên tính toán hạn chế. Cụ thể, D-PRUNER khéo léo nắm bắt và giữ lại cả tham số chung và tham số lĩnh vực trong khi loại bỏ có chọn lọc các tham số mô hình không quan trọng. Cơ chế này bao gồm các bước sau: đầu tiên, một mô-đun tầm quan trọng trọng số chung hoạt động để đánh giá tầm quan trọng của các tham số mô hình cho các khả năng chung. Tiếp theo, chúng tôi đề xuất một hàm mất mát huấn luyện được cập nhật dựa trên mục tiêu huấn luyện tự hồi quy cho dự đoán token tiếp theo bằng cách tích hợp tầm quan trọng chung như một số hạng chính quy hóa. Bằng cách này, chúng tôi xác định các trọng số đóng góp cho cả tính tổng quát và tính chuyên biệt lĩnh vực khi huấn luyện trên một bộ dữ liệu hiệu chuẩn lĩnh vực. Sau đó, với hàm mất mát được cập nhật, chúng tôi tính toán tầm quan trọng trọng số bằng cách tận dụng gradient mà không cập nhật mô hình. Hơn nữa, một thuật toán xấp xỉ, Fisher thực nghiệm (Martens, 2020; Sung et al., 2021), được sử dụng để tính toán tầm quan trọng trọng số một cách hiệu quả cho việc cắt tỉa.

Chúng tôi đánh giá hiệu suất của D-PRUNER trên LLaMA2 (Touvron et al., 2023), một LLM mã nguồn mở được sử dụng rộng rãi. Các phát hiện thí nghiệm của chúng tôi chứng minh rằng D-PRUNER thể hiện hiệu quả đáng chú ý trong việc trích xuất các mạng lĩnh vực thưa thớt từ các LLMs được huấn luyện trước, với một lượng hạn chế dữ liệu hiệu chuẩn được cung cấp. Đáng chú ý, D-PRUNER đạt được kết quả tương đương với mô hình dày đặc đầy đủ trong khi đạt được 50% độ thưa thớt, vượt trội hơn hiệu suất của các kỹ thuật cắt tỉa thay thế trên các bộ dữ liệu chuyên biệt theo lĩnh vực đa dạng trong lĩnh vực y tế và pháp lý bao gồm các nhiệm vụ hiểu ngôn ngữ, trả lời câu hỏi, và tóm tắt.

2 Nghiên cứu Liên quan

Nén mô hình liên quan đến việc chuyển đổi một mô hình lớn, tốn nhiều tài nguyên thành một phiên bản nhỏ gọn phù hợp cho triển khai tài nguyên thấp (Deng et al., 2020; Zhu et al., 2023). Có chủ yếu ba kỹ thuật cho nén mô hình, đó là cắt tỉa, chưng cất kiến thức, và lượng tử hóa.

Cắt tỉa. Các kỹ thuật cắt tỉa trong mạng nơ-ron có thể được phân loại rộng rãi thành cắt tỉa có cấu trúc và cắt tỉa không có cấu trúc (Xia et al., 2022; Sanh et al., 2020; Du et al., 2021). Cắt tỉa có cấu trúc đòi hỏi việc loại bỏ toàn bộ các thành phần mạng, như các kênh hoặc lớp, được hướng dẫn bởi các tiêu chí cụ thể, trong khi duy trì kiến trúc mạng tổng thể. Ngược lại, cắt tỉa không có cấu trúc nhắm mục tiêu các trọng số riêng lẻ, dẫn đến một cấu trúc thưa thớt không đều.

Trong khi nhiều nỗ lực đã được thực hiện để cắt tỉa các mô hình ngôn ngữ có quy mô tương đối nhỏ, như BERT (Kenton and Toutanova, 2019), ít sự chú ý đã được dành cho việc cắt tỉa LLMs chứa hàng tỷ tham số. Những mô hình lớn hơn này sở hữu gấp 100-1000 lần nhiều trọng số hơn, khiến nhiệm vụ cắt tỉa trở nên khó khăn hơn đáng kể. SparseGPT (Frantar and Alistarh, 2023), một phương pháp sau huấn luyện cho Các Mô hình Ngôn ngữ Lớn (LLMs), thiếu khả năng xác định các trọng số quan trọng được điều chỉnh cho các lĩnh vực hoặc nhiệm vụ cụ thể vì nó tránh tinh chỉnh. Mặt khác, LLM-Pruner (Ma et al., 2023) sử dụng các kỹ thuật dựa trên gradient để cắt tỉa. Tuy nhiên, nó thiếu sót trong việc xác định các trọng số then chốt cần thiết cho kiến thức chia sẻ lĩnh vực, dẫn đến các mô hình đã cắt tỉa thiếu mức độ tổng quát mong muốn.

Các phương pháp cắt tỉa hiện tại hoặc tập trung vào trọng số chung hoặc chuyên biệt theo lĩnh vực, tuy nhiên không có phương pháp nào trong số chúng xem xét việc bảo tồn cả hai cùng một lúc. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên làm việc về cắt tỉa LLMs trong khi bảo tồn các trọng số quan trọng cho cả tính tổng quát và tính chuyên biệt.

Chưng cất Kiến thức. Chưng cất Kiến thức (KD) đã nổi lên như một kỹ thuật mạnh mẽ, thu hút sự quan tâm đáng kể vì khả năng tăng cường hiệu suất mô hình và nâng cao khả năng tổng quát hóa (Hinton et al., 2015; Zhu et al., 2023). Về cốt lõi, KD xoay quanh việc chuyển giao chuyên môn từ một mô hình phức tạp, được gọi là "mô hình giáo viên", sang một đối tác đơn giản hóa được biết đến như "mô hình học sinh". Quá trình phức tạp này của việc chuyển giao kiến thức nhằm chưng cất những hiểu biết sâu sắc được đóng gói trong các mô hình giáo viên, cô đọng chúng thành một biểu diễn ngắn gọn và hiệu quả hơn trong các mô hình học sinh.

Trong khi KD đã được chứng minh là một công cụ mạnh mẽ cho nén mô hình, nó cần các nhiệm vụ hạ nguồn cụ thể và một lượng lớn dữ liệu để các mô hình học sinh học từ các mô hình giáo viên. Do đó, đầu ra mà các mô hình học sinh tạo ra chủ yếu tập trung vào một nhiệm vụ cụ thể và mất khả năng tổng quát. KD nói chung đặt ra các yêu cầu cao hơn về tính khả dụng của dữ liệu và ngân sách tính toán (ví dụ, bộ nhớ GPU) so với cắt tỉa.

Lượng tử hóa. Trong lĩnh vực nén mô hình, lượng tử hóa đã nổi lên như một kỹ thuật được áp dụng rộng rãi để giảm bớt các thách thức lưu trữ và tính toán vốn có trong các mô hình học sâu (Guo et al., 2020; Dettmers et al., 2021, 2022, 2023). Các biểu diễn mô hình thông thường dựa vào các số dấu phẩy động, nhưng lượng tử hóa chuyển đổi chúng thành các số nguyên hoặc dạng rời rạc. Sự chuyển đổi này dẫn đến việc giảm đáng kể trong các yêu cầu lưu trữ và độ phức tạp tính toán. Trong khi một mức độ mất mát độ chính xác nhất định là không thể tránh khỏi, các phương pháp lượng tử hóa được thiết kế cẩn thận có thể đạt được nén mô hình đáng kể với sự suy giảm độ chính xác tối thiểu. Mặc dù các thách thức vẫn tồn tại, chẳng hạn như duy trì khả năng diễn giải mô hình và giải quyết các phức tạp chuyên biệt theo nhiệm vụ, các nghiên cứu hiện tại thiết lập một nền tảng vững chắc cho những tiến bộ liên tục trong lượng tử hóa LLM, có thể bổ sung cho việc cắt tỉa LLM.

3 Phương pháp luận

Để bảo tồn cả tính tổng quát và tính chuyên biệt trên mô hình đã cắt tỉa, phương pháp cắt tỉa kép của chúng tôi D-PRUNER xem xét các trọng số quan trọng cho cả tính tổng quát và tính chuyên biệt trong quá trình huấn luyện trên một bộ dữ liệu hiệu chuẩn. Lưu ý chúng tôi chỉ sử dụng gradient trọng số được tạo từ quá trình huấn luyện nhưng không cập nhật các trọng số mô hình. Mô hình của chúng tôi được cắt tỉa theo cách không phụ thuộc nhiệm vụ (ví dụ, chúng tôi áp dụng một mục tiêu huấn luyện trước, dự đoán token tiếp theo, như một phần của hàm mất mát huấn luyện) để mô hình đã cắt tỉa có thể giải quyết các nhiệm vụ khác nhau trong lĩnh vực mục tiêu.

D-PRUNER bao gồm các bước sau: đầu tiên, một mô-đun định vị trọng số chung hoạt động để đánh giá tầm quan trọng của các tham số mô hình cho hiểu biết chung (Phần 3.1). Tiếp theo, một hàm mất mát được cập nhật cho quá trình huấn luyện được đề xuất bằng cách tích hợp tầm quan trọng trọng số chung như một số hạng chính quy hóa. Bằng cách này, chúng tôi xác định các trọng số đóng góp cho cả kiến thức chung và kiến thức lĩnh vực (Phần 3.2). Cuối cùng, với hàm mất mát được cập nhật, chúng tôi tính toán các gradient trọng số trên một bộ dữ liệu hiệu chuẩn lĩnh vực nhỏ mà không cập nhật mô hình và xấp xỉ tầm quan trọng trọng số cắt tỉa kép của chúng tôi bằng cách sử dụng chỉ số Fisher thực nghiệm (Sung et al., 2021) để cắt tỉa (Phần 3.3).

Phương pháp của chúng tôi tập trung vào cắt tỉa không có cấu trúc theo cách từng lớp cho mô hình Transformers. Chúng tôi xem xét các phép chiếu truy vấn, khóa, giá trị, và đầu ra của tất cả các lớp tự chú ý và các phép chiếu cổng (Liu et al., 2021), xuống, và lên của tất cả các lớp MLP (perceptron đa lớp) để cắt tỉa.

3.1 Tầm quan trọng Trọng số Chung

Bước đầu tiên của phương pháp chúng tôi liên quan đến việc định vị các trọng số quan trọng về mặt kiến thức chung. Theo cùng một giả thuyết như Frantar và Alistarh (2023), chúng tôi giả định rằng một trọng số quan trọng sẽ gây ra sự gia tăng lớn hơn trong giá trị mất mát so với những trọng số ít quan trọng hơn nếu nó được cắt tỉa (đặt thành 0) trong quá trình huấn luyện. Chính thức, nếu một bộ dữ liệu của hiệu chuẩn miền mở Dg={xj, yj}N j=1 với kích thước N được sử dụng để huấn luyện và W đại diện cho các ma trận trọng số của một mô hình, tầm quan trọng của mỗi trọng số tại chỉ số m, được ký hiệu là IWm, có thể được xấp xỉ sử dụng chuỗi Taylor như được thể hiện bởi LeCun et al. (1989):

IWm=|L(Dg)− LWm=0(Dg)|
=|∂L(Dg)/∂WmWm+1/2WmHmmWm+O(||Wm||3)| (1)

trong đó H ký hiệu ma trận Hessian, và L là mất mát entropy chéo. Đối với một mô hình được huấn luyện đủ tới một cực tiểu địa phương trên độ cong mất mát của nó (ví dụ, các mô hình ngôn ngữ nền tảng được huấn luyện trước như LLaMA), Optimal Brain Surgeon cổ điển (Hassibi et al., 1993) tiếp tục xấp xỉ tầm quan trọng của Wm như:

εm=1/2(Wm)2/[H−1]mm (2)

εm cũng có thể được xem như lỗi gây ra bởi việc loại bỏ trọng số Wm. Chúng tôi tính toán εm cho tất cả các trọng số phải chịu cắt tỉa và xây dựng một ma trận điểm số tầm quan trọng G đối với các lĩnh vực chung có cùng chiều với W.

3.2 Mất mát Được cập nhật với Chính quy hóa

Để xác định các trọng số quan trọng trong cả kiến thức chung và chuyên biệt theo lĩnh vực, chúng tôi sửa đổi hàm mất mát gốc của huấn luyện LLM. Trong huấn luyện LLM, mất mát entropy chéo được sử dụng trong nhiệm vụ dự đoán token tiếp theo (Radford et al., 2018). Tương tự như Thompson et al. (2019), chúng tôi thêm một số hạng chính quy hóa để ràng buộc sự thay đổi của các trọng số chung quan trọng được tìm thấy trong bước đầu tiên. Giả sử có M số trọng số có thể cắt tỉa trong tổng số. Để huấn luyện trên một bộ dữ liệu hiệu chuẩn chuyên biệt theo lĩnh vực Ds= {xj, yj}P j=1, chúng tôi thêm số hạng chính quy hóa được đề xuất lên trên mất mát dự đoán token tiếp theo Lnext để có được mục tiêu huấn luyện cuối cùng của chúng tôi:

Lours=Lnext+λΣm=1M Gm(Wm′−Wm)2 (3)

trong đó Gm là tầm quan trọng trọng số chung, Wm′ ký hiệu giá trị trọng số được cập nhật của Wm, λ là một siêu tham số, và số hạng thứ hai bên phải là Lregular.

Trong thực tế, việc tính toán trực tiếp số hạng chính quy hóa này trong lượt truyền tiến tốn kém về mặt tính toán vì hai lý do: (1) nó liên quan đến cả Wm và Gm rất lớn, và (2) việc thu thập các tham số mô hình được cập nhật (Wm′) trong một hệ thống được phân vùng (Rasley et al., 2020) hoặc được chia sẻ (Zhao et al., 2023) là không hiệu quả. Dựa trên thành công gần đây của việc áp dụng gradient descent trên tinh chỉnh đầy đủ của LLMs (Lv et al., 2023), chúng tôi chọn sử dụng gradient descent để tối ưu hóa tham số. Do đó, với tốc độ học α, ký hiệu gradient của mỗi tham số đối với Lnext là gm next, chúng tôi giảm số hạng chính quy hóa thành:

Lregular =Σm=1M Gm(Wm′−Wm)2
=λΣm=1M Gm(Wm−αgm next−Wm)2
=λΣm=1M α2Gm(gm next)2 (4)

Trong lượt truyền ngược, việc tối ưu hóa số hạng chính quy hóa này đòi hỏi các đạo hàm bậc hai, điều này chỉ ra rằng các ma trận Hessian (H) là cần thiết. Việc tính toán trực tiếp các ma trận Hessian là không khả thi đối với một số lượng lớn như vậy của các tham số. Do đó, chúng tôi sử dụng ma trận thông tin Fisher để xấp xỉ đường chéo của Hessian (Sung et al., 2021). Và ma trận thông tin Fisher có thể được xấp xỉ tiếp theo bởi trung bình của gradient bình phương của dự đoán mô hình trên P. Chúng tôi viết gradient của chính quy hóa đối với mỗi ma trận tham số trong độ chi tiết mịn hơn:

∂Lregular/∂Wm ≈ 2λα2Gmgm nextHmm (5)

Hmm ≈ 1/P ΣPj=1(gm next(xj, yj))2 (6)

Chúng tôi tính toán trực tiếp ∂Lregular/∂W qua Phương trình 5 ở trên thay vì dựa vào lượt truyền ngược PyTorch để tối đa hóa hiệu quả tính toán. Tính toán gradient cuối cùng của hàm mất mát được chính quy hóa của chúng tôi được thể hiện dưới đây:

∂Lours/∂Wm=∂Lnext/∂Wm+∂Lregular/∂Wm (7)

3.3 Điểm số Tầm quan trọng Cắt tỉa Kép

Cuối cùng, chúng tôi tính toán điểm số tầm quan trọng cắt tỉa kép của mỗi trọng số, và các trọng số không quan trọng có thể được cắt tỉa theo tầm quan trọng của chúng. Chúng tôi sử dụng Phương trình 1 để ước lượng tầm quan trọng thay vì Phương trình 2, vì mô hình của chúng tôi chưa hội tụ đến một tối ưu trên lĩnh vực mục tiêu. Tuy nhiên, việc tính toán trực tiếp ma trận Hessian trong Phương trình 2 là không khả thi vì nó liên quan đến độ phức tạp O(M2) cho mỗi cập nhật trọng số. Do đó, chúng tôi cũng tận dụng Sung et al. (2021) để xấp xỉ đường chéo của Hessian, và điểm số tầm quan trọng cuối cùng Sm có thể được định nghĩa như:

Sm ≈ |∂Lours(Ds)/∂WmWm+1/2[∂Lours(Ds)/∂WmWm]2+O(||Wm||3)| (8)

Ở đây O(||Wm||3) có thể được bỏ qua theo xấp xỉ bậc hai (LeCun et al., 1989). Lưu ý việc tính toán Sm xem xét cả kiến thức chung và chuyên biệt theo lĩnh vực qua mục tiêu huấn luyện được chính quy hóa của chúng tôi. Kết hợp cả chính quy hóa và ước lượng tầm quan trọng qua xấp xỉ Fisher thực nghiệm, phương pháp của chúng tôi dự kiến sẽ tiến hành cắt tỉa duy trì các trọng số quan trọng cho cả kiến thức chung và chuyên biệt theo lĩnh vực, do đó bảo tồn tính tổng quát và tính chuyên biệt. Và những điểm số tầm quan trọng này được sử dụng để hướng dẫn các quyết định cắt tỉa của chúng tôi. Ví dụ, nếu chúng tôi đặt mức độ thưa thớt là 50%, các trọng số có 50% điểm số tầm quan trọng nhỏ nhất trong mỗi lớp sẽ được cắt tỉa.

4 Thiết lập Thí nghiệm

Chúng tôi đánh giá D-PRUNER trên hai lĩnh vực tập trung kiến thức, đó là y tế và pháp lý. Đối với tính tổng quát mô hình dưới các thách thức chuyên biệt theo lĩnh vực, chúng tôi đánh giá khả năng ngôn ngữ sử dụng tạo sinh văn bản lĩnh vực, và đánh giá khả năng giải quyết đa nhiệm vụ trên các nhiệm vụ lĩnh vực khác nhau, tức là suy luận ngôn ngữ tự nhiên (NLI), trả lời câu hỏi (QA), và tóm tắt. Vì chúng tôi sử dụng các bộ dữ liệu lĩnh vực, tính chuyên biệt mô hình trên các lĩnh vực cũng có thể được đánh giá. Ngoài ra, chúng tôi tinh chỉnh mô hình đã cắt tỉa trên các bộ dữ liệu lĩnh vực để đánh giá thêm tính tổng quát và tính chuyên biệt.

Chúng tôi đánh giá D-PRUNER trên họ mô hình LLaMA2, là LLM mã nguồn mở được sử dụng nhiều nhất. Chúng tôi chủ yếu áp dụng phương pháp cắt tỉa và các phương pháp cơ sở của chúng tôi cho LLaMA2-7B và LLaMA2-13B để thể hiện kết quả của chúng tôi. Phương pháp của chúng tôi cũng có thể được áp dụng dễ dàng cho các LLM khác với kích thước và kiến trúc khác nhau. Ví dụ, Phụ lục B thể hiện thí nghiệm bổ sung trên mô hình BLOOM (Le Scao et al., 2022).

4.1 Chặn lặp

Được thúc đẩy bởi Frantar và Alistarh (2023), chúng tôi thực hiện các thí nghiệm (trong Bảng 2) trên D-PRUNER có và không có chặn lặp. Chặn lặp có nghĩa là đưa ra quyết định cắt tỉa cho mỗi số cố định (Bs) cột trong một ma trận trọng số. Nói cách khác, thay vì chọn một mặt nạ cắt tỉa duy nhất cho toàn bộ ma trận trọng số, một mặt nạ con cắt tỉa được chọn cho mỗi Bs cột để đạt mức độ thưa thớt tổng thể. Chúng tôi đặt Bs = 128 cho các ma trận trọng số với số cột nhỏ nhất và tăng Bs cho những ma trận có nhiều cột hơn. Ngoại trừ Bảng 2, D-PRUNER trong các bảng khác không áp dụng chặn lặp.

4.2 Bộ dữ liệu và Đánh giá

Bộ dữ liệu. Bảng 1 thể hiện chi tiết của mỗi bộ dữ liệu mà chúng tôi đã sử dụng. Cụ thể, đối với y tế, chúng tôi chọn một sách giáo khoa y học InternalMed_Harrison (Bigby, 1988), MedNLI (Romanov và Shivade, 2018), PubMedQA (Jin et al., 2019), và Health Question Summarization (HQS) từ nhiệm vụ chia sẻ MEDIQA 2021 số 1 (Ben Abacha et al., 2021; Ben Abacha và Demner-Fushman, 2019) như các bộ dữ liệu lĩnh vực. Đối với lĩnh vực pháp lý, chúng tôi chọn MultiLegalPile (Niklaus et al., 2023), CaseHOLD (Zheng et al., 2021), và BillSum (Kornilova và Eidelman, 2019). Đối với dữ liệu hiệu chuẩn miền mở, chúng tôi trích xuất văn bản từ bộ dữ liệu C4 (Raffel et al., 2019).

Để xây dựng dữ liệu hiệu chuẩn chuyên biệt theo lĩnh vực của chúng tôi, chúng tôi chọn các trường hợp huấn luyện từ MedNLI, PubMedQA, và HQS với tỷ lệ 20%/60%/20% và từ CaseHOLD và BillSum với tỷ lệ 50%/50%. Những tỷ lệ này được xác định dựa trên độ khó và kích thước huấn luyện của những điểm chuẩn này. Cả nhiệm vụ NLI và QA mà chúng tôi áp dụng đều yêu cầu các mô hình thực hiện phân loại. Chúng tôi thí nghiệm với các kích thước khác nhau của bộ dữ liệu hiệu chuẩn chuyên biệt theo lĩnh vực và thấy rằng kích thước 1000 đạt được sự cân bằng tốt nhất về mặt hiệu quả và hiệu suất cắt tỉa cho cả hai lĩnh vực. Đối với đánh giá mô hình, ngoài việc sử dụng các trường hợp kiểm tra của những điểm chuẩn đó, chúng tôi tận dụng InternalMed_Harrison và MultiLegalPile để đánh giá perplexity. 300 đoạn văn được chọn từ mỗi nguồn dữ liệu để tạo thành bộ kiểm tra của perplexity. Lưu ý rằng chúng tôi sử dụng một tập con của tất cả các ví dụ kiểm tra của CaseHOLD và BillSum, vì hai điểm chuẩn này lớn hơn đáng kể về kích thước và trường hợp riêng lẻ của chúng có xu hướng dài hơn.

Chỉ số Đánh giá. Chúng tôi đầu tiên đánh giá khả năng ngôn ngữ của các mô hình đã cắt tỉa trên InternalMed_Harrison và MultiLegalPile sử dụng perplexity. Sau đó chúng tôi đánh giá khả năng giải quyết đa nhiệm vụ và tính chuyên biệt lĩnh vực trên các nhiệm vụ lĩnh vực khác nhau. Cụ thể, chúng tôi chọn chỉ số độ chính xác cho nhiệm vụ NLI (MedNLI), macro-F1 cho các nhiệm vụ QA (PubMedQA và CaseHOLD), và điểm số ROUGE (Lin, 2004) cho các nhiệm vụ tóm tắt (HQS và BillSum).

4.3 Các phương pháp Cơ sở

Chúng tôi so sánh phương pháp của chúng tôi với nhiều phương pháp cơ sở cắt tỉa LLM. Tất cả các phương pháp được áp dụng cho cùng một mô hình nền tảng (7B hoặc 13B của LLaMA2) để so sánh công bằng. Như một nghiên cứu loại bỏ, chúng tôi cũng đánh giá một phương pháp cắt tỉa không có cấu trúc sử dụng gradient trọng số bằng cách loại bỏ số hạng chính quy hóa trong hàm mất mát huấn luyện của D-PRUNER.

• Cắt tỉa theo độ lớn cắt tỉa các trọng số dựa trên độ lớn của chúng (Han et al., 2015). Chúng tôi theo thực hành tiêu chuẩn của cắt tỉa theo độ lớn trên các mô hình ngôn ngữ, nơi các trọng số được so sánh theo từng lớp. Cắt tỉa theo độ lớn là một phương pháp cơ sở đơn giản và mạnh mẽ đã được chứng minh vượt trội hơn nhiều phương pháp cắt tỉa khác.

• LLM-Pruner là một phương pháp cắt tỉa có cấu trúc sử dụng gradient trọng số để đánh giá tầm quan trọng trọng số (Ma et al., 2023). Một bộ dữ liệu hiệu chuẩn được sử dụng cho tính toán gradient của nó, vì vậy chúng tôi kết hợp cả dữ liệu hiệu chuẩn miền mở (C4) và chuyên biệt theo lĩnh vực khi chúng tôi sử dụng LLM-Pruner.

• SparseGPT là một phương pháp cắt tỉa không có cấu trúc sau huấn luyện (Frantar và Alistarh, 2023). Nó sử dụng một thủ tục cập nhật trọng số hiệu quả lặp lại giữa loại bỏ trọng số và cập nhật trọng số tại mỗi lớp. Nó cũng sử dụng một bộ dữ liệu hiệu chuẩn để xấp xỉ. Do đó, tương tự như D-PRUNER và LLM-Pruner, chúng tôi sử dụng dữ liệu hiệu chuẩn miền mở và chuyên biệt theo lĩnh vực để so sánh công bằng.

Hơn nữa, đối với tất cả các phương pháp cơ sở, chúng tôi tiếp tục tinh chỉnh các mô hình đã cắt tỉa của chúng sử dụng LoRA (Hu et al., 2021) trên tất cả các bộ dữ liệu cùng nhau (dữ liệu NLI, QA, và tóm tắt kết hợp) trong mỗi lĩnh vực và sau đó kiểm tra mô hình đã tinh chỉnh trên các bộ dữ liệu trong Bảng 1. Chúng tôi chỉ sử dụng bộ dữ liệu hiệu chuẩn miền mở mặc định cho các mô hình đã cắt tỉa của LLM-Pruner và SparseGPT ở bước này, vì những mô hình này cuối cùng sẽ trải qua tinh chỉnh LoRA. Các trường hợp dữ liệu của bộ dữ liệu tinh chỉnh của chúng tôi tuân theo mẫu Alpaca (Taori et al., 2023) để các mô hình được huấn luyện để dự đoán các phản hồi. Cụ thể, đối với y tế, chúng tôi có 7000, 7000, và 1000 trường hợp huấn luyện từ MedNLI, PubMedQA, và HQS, tương ứng. Đối với lĩnh vực pháp lý, chúng tôi có 13000 trường hợp huấn luyện từ CaseHOLD và 2000 từ BillSum.

4.4 Chi tiết Triển khai

Chúng tôi thực hiện kỹ thuật prompt trong một thiết lập zero-shot trước khi nhắc nhở một loạt mô hình. Prompt cuối cùng được giữ nguyên trên tất cả các mô hình ứng viên trên một nhiệm vụ để đảm bảo công bằng. Các siêu tham số được sử dụng bởi các mô hình khác nhau có trong Phụ lục C.

5 Kết quả và Phân tích

Kết quả và phân tích của chúng tôi nhằm trả lời các câu hỏi nghiên cứu sau:

• RQ 1: D-PRUNER so sánh như thế nào với các phương pháp cơ sở cắt tỉa khác (5.1)?

• RQ 2: Hiệu suất của tất cả các mô hình ứng viên sau tinh chỉnh LoRA là gì (5.2)?

• RQ 3: Như một đóng góp quan trọng của D-PRUNER, liệu cắt tỉa kép có phải là một phương pháp hiệu quả để nén LLM (5.1, 5.3, và 5.5)?

• RQ 4: D-PRUNER hoạt động như thế nào dưới các mức độ thưa thớt khác nhau hoặc các kích thước khác nhau của dữ liệu hiệu chuẩn chuyên biệt theo lĩnh vực (5.4)?

5.1 Kết quả Tổng thể

Kết quả tổng thể của chúng tôi cho hai lĩnh vực được trình bày trong Bảng 2. Tất cả các mô hình được cắt tỉa đến mức độ thưa thớt 50% ngoại trừ mô hình dày đặc.

Cải thiện trên NLI và QA D-PRUNER mang lại cải thiện điểm số nhất quán trên các nhiệm vụ NLI và QA khi nó được so sánh với các phương pháp cơ sở dựa trên LLaMA2-7B và LLaMA2-13B. Với hai ngoại lệ, các biến thể của D-PRUNER dựa trên việc bao gồm và loại trừ chặn lặp vượt trội hơn các phương pháp cơ sở trên 4 trong số 6 trường hợp khi phân loại được thực hiện (MedNLI, PubMedQA, và CaseHOLD trên cả 7B và 13B LLaMA2) trong Bảng 2. Rõ ràng có thể thấy rằng cắt tỉa theo độ lớn và SparseGPT nói chung là những mô hình mạnh hơn LLM-Pruner. Mô hình dày đặc đôi khi có điểm số tệ hơn những mô hình khác trên 7B và 13B LLaMA2, điều này chỉ ra rằng việc mở rộng tham số của một mô hình ngôn ngữ được huấn luyện trước không nhất thiết tăng hiệu suất trên một điểm chuẩn duy nhất trên NLI và QA. Chúng tôi có thể thấy rằng chặn lặp nói chung mang lại điểm số tốt hơn trên những nhiệm vụ phân loại này như đạt điểm F1 30.56 trên CaseHOLD dựa trên LLaMA2-7B, đây là một cải thiện đáng kể so với các phương pháp cơ sở và D-PRUNER không có nó. Do đó, chúng tôi khuyến nghị áp dụng chặn lặp trên các nhiệm vụ phân loại khi kiến thức lĩnh vực mạnh mẽ được yêu cầu.

Cải thiện trên Tóm tắt D-PRUNER thể hiện hiệu suất tóm tắt mạnh nhất. Điều thú vị nhất là điểm số ROUGE của nó hầu hết cao hơn những mô hình dày đặc. Chúng tôi nhận thấy hiệu suất tóm tắt hàng đầu của các mô hình dựa trên LLaMA2-13B trên HQS thấp hơn so với các mô hình dựa trên LLaMA2-7B, điều này phản trực giác. Theo hiện trạng nghệ thuật của HQS (Zhang et al., 2023; He et al., 2021), chúng tôi thấy rằng D-PRUNER gần với điểm số ROUGE tốt nhất được tạo ra bởi các hệ thống đơn, vì vậy chúng tôi coi rằng bộ dữ liệu này tương đối đơn giản. Do đó, các mô hình dựa trên LLaMA2-7B của chúng tôi dường như tìm thấy một giới hạn trên của ROUGE cho các tóm tắt tham chiếu hiện có, vì vậy việc chuyển từ 7B lên 13B gây ra sự suy giảm hiệu suất nhỏ trên mô hình dày đặc, SparseGPT, và D-PRUNER. Hiệu suất tóm tắt mạnh mẽ của D-PRUNER trên cả hai lĩnh vực chứng minh khả năng sử dụng của nó như một mô hình ngôn ngữ hiệu quả và chuyên biệt theo lĩnh vực. Đối với chặn lặp, D-PRUNER không có nó nói chung có hiệu suất perplexity và tóm tắt tốt hơn. Tuy nhiên, xem xét ngoại lệ trong lĩnh vực pháp lý dựa trên LLaMA2-7B, chúng tôi khuyến nghị kiểm tra điểm số perplexity trên dữ liệu xác thực khi quyết định có nên sử dụng chặn lặp để đánh giá perplexity và tóm tắt.

Cải thiện trên Perplexity D-PRUNER có điểm số perplexity tốt thứ hai trên lĩnh vực y tế và pháp lý trên 7B và 13B LLaMA2. Những điểm số này phản ánh khả năng ngôn ngữ mạnh mẽ của SparseGPT và D-PRUNER khi chúng gặp các lĩnh vực tập trung kiến thức. D-PRUNER không vượt qua SparseGPT trên chỉ số perplexity, và lý do có thể đến từ pipeline tinh chỉnh (Lv et al., 2023) mà chúng tôi sử dụng. Lv et al. (2023) là một pipeline tinh chỉnh tham số đầy đủ nhằm hướng tới hiệu quả bộ nhớ GPU, vì vậy hiệu quả của nó trên một chỉ số cụ thể có thể bị ảnh hưởng. Hơn nữa, chúng tôi nghi ngờ rằng dữ liệu chúng tôi sử dụng từ InternalMed_Harrison và MultiLegalPile có thể gần với lĩnh vực chung hơn cả về mặt ngữ nghĩa và cú pháp. Vì SparseGPT cắt tỉa LLM chủ yếu dựa trên tính tổng quát, nó có điểm số perplexity tốt hơn của chúng tôi.

5.2 Hiệu suất Sau Tinh chỉnh

Bảng 3 thể hiện kết quả của các mô hình ứng viên đã tinh chỉnh ở mức độ thưa thớt 50%. Tương tự như hiệu suất được thảo luận ở trên, D-PRUNER luôn mang lại điểm số tóm tắt tốt nhất và hầu hết thể hiện kết quả phân loại tốt nhất sau tinh chỉnh, điều này chứng minh rằng tinh chỉnh có thể cải thiện thêm hiệu suất cắt tỉa của phương pháp chúng tôi. Đối với hầu hết các mô hình, macro-F1 trên PubMedQA giảm sau tinh chỉnh, vì bộ kiểm tra này không cân bằng và các mô hình hầu hết học để dự đoán các nhãn lớp đa số. Thực tế, độ chính xác của hầu hết các mô hình trên PubMedQA tăng sau tinh chỉnh như thể hiện trong Phụ lục A, vì vậy phương pháp tinh chỉnh này vẫn tạo ra sự khác biệt. Chúng tôi cũng không thấy quá nhiều cải thiện điểm số cho nhiều mô hình trên CaseHOLD, vì đây là một nhiệm vụ khá thách thức cho thiết lập thí nghiệm của chúng tôi (ví dụ, chúng tôi chỉ kết hợp một tập con nhỏ của dữ liệu huấn luyện gốc cho mỗi nhiệm vụ và thực hiện tinh chỉnh đa nhiệm vụ như được thảo luận trong Phần 4).

5.3 Nghiên cứu Loại bỏ

Trong Bảng 4, chúng tôi thể hiện rằng cắt tỉa mà không tích hợp tầm quan trọng lĩnh vực chung như một số hạng chính quy hóa mang lại hiệu suất dưới mức tối ưu. Nói cách khác, điều này có nghĩa là loại bỏ việc xem xét tính tổng quát. Chúng tôi thấy perplexity trong cả hai lĩnh vực cao hơn so với cắt tỉa với chính quy hóa. Điều này chứng minh rằng cơ chế cắt tỉa kép của chúng tôi xem xét cả tính tổng quát và tính chuyên biệt có thể cải thiện hiệu suất mô hình.

5.4 Ảnh hưởng của Độ thưa thớt và Dữ liệu Hiệu chuẩn Lĩnh vực

Trong Bảng 5, rõ ràng rằng perplexity tiếp tục tăng khi D-PRUNER trở nên thưa thớt hơn, điều này được mong đợi. Vì độ thưa thớt 50% là sự cân bằng tốt giữa độ thưa thớt và hiệu suất, chúng tôi chọn nó để báo cáo hiệu suất của chúng tôi trong Bảng 2 và 3.

Dựa trên Bảng 6, chúng tôi tin rằng việc đặt kích thước dữ liệu hiệu chuẩn chuyên biệt theo lĩnh vực thành 1000 là hợp lý. Như hàng cuối cùng thể hiện, việc tăng kích thước của nó không phải lúc nào cũng đảm bảo cải thiện hiệu suất.

5.5 Độ tương đồng Mặt nạ

Để hiểu rõ hơn về mô hình đã cắt tỉa trên các lĩnh vực khác nhau, chúng tôi so sánh độ tương đồng của các mặt nạ cắt tỉa. Trong nghiên cứu của chúng tôi trên LLaMA2-7B, mỗi mặt nạ được tạo ra chứa 7*32 ma trận cho 32 lớp và 7 ma trận phép chiếu trong mô-đun tự chú ý (q, k, v, o) và mô-đun MLP (xuống, lên, cổng) trong mỗi lớp. Đối với mỗi ma trận, chúng tôi tính toán độ tương đồng như số phần tử "1" được chia sẻ ("1" có nghĩa là trọng số không được cắt tỉa) trong hai mặt nạ chia cho kích thước ma trận. Lưu ý tất cả các mặt nạ được tạo ra trong độ thưa thớt 50%.

Hình 2 (a) thể hiện độ tương đồng mặt nạ giữa lĩnh vực miền mở và lĩnh vực y tế, và 2 (b) thể hiện độ tương đồng mặt nạ giữa lĩnh vực y tế và lĩnh vực pháp lý. Kết quả cho thấy rằng các mặt nạ khá khác nhau, với các phần tử được chia sẻ thấp tới 35%. Nói chung, các mô-đun tự chú ý chia sẻ ít phần tử hơn so với các mô-đun MLP. Điều này có nghĩa là các mô-đun tự chú ý đóng góp nhiều hơn cho tính chuyên biệt, và các mô-đun MLP lưu trữ kiến thức được chia sẻ bởi các lĩnh vực khác nhau.

6 Kết luận

Chúng tôi giới thiệu D-PRUNER, một phương pháp cắt tỉa kép không có cấu trúc đổi mới để nén chuyên biệt theo lĩnh vực trên LLM. Nó có thể trích xuất một LLM nén, chuyên biệt theo lĩnh vực, và không phụ thuộc nhiệm vụ bằng cách xác định các trọng số then chốt cho cả tính tổng quát và tính chuyên biệt. Cụ thể hơn, tầm quan trọng trọng số chung được đánh giá đầu tiên bằng cách lượng hóa lỗi phát sinh khi loại bỏ chúng với sự trợ giúp của dữ liệu hiệu chuẩn miền mở. Sau đó, chúng tôi sử dụng tầm quan trọng trọng số chung này để tinh chỉnh hàm mất mát huấn luyện của chúng tôi, để nó xem xét tính tổng quát khi phù hợp với một lĩnh vực cụ thể. Hơn nữa, bằng cách ước lượng hiệu quả tầm quan trọng trọng số với hàm mất mát huấn luyện được tinh chỉnh trên một bộ dữ liệu hiệu chuẩn chuyên biệt theo lĩnh vực, chúng tôi thu được một mô hình đã cắt tỉa nhấn mạnh các khả năng chung và kiến thức chuyên biệt theo lĩnh vực. Các thí nghiệm toàn diện của chúng tôi trên các nhiệm vụ khác nhau trong các lĩnh vực khác nhau cho thấy hiệu quả của D-PRUNER trong cắt tỉa chuyên biệt theo lĩnh vực.

Hạn chế

Mặc dù D-PRUNER thể hiện hiệu suất mạnh mẽ trong Phần 5, nhiều điểm số perplexity của nó đạt vị trí thứ hai trong lĩnh vực y tế và pháp lý (mô hình dày đặc không được tính ở đây). Việc cải thiện thêm perplexity này là một phần mở rộng có giá trị của bài báo này.

Một hạn chế khác của công trình này là D-PRUNER tốn nhiều bộ nhớ hơn SparseGPT trong quá trình cắt tỉa, vì D-PRUNER dựa trên tinh chỉnh tham số đầy đủ và SparseGPT không tận dụng thông tin gradient toàn cục. D-PRUNER đặt ra yêu cầu bộ nhớ tương tự như LLM-Pruner. Như một sự đánh đổi, D-PRUNER đạt hiệu suất tốt hơn trên hầu hết các chỉ số. Nó cũng linh hoạt hơn, vì nó tính toán các ma trận điểm số tầm quan trọng mà không thực sự làm thưa thớt LLMs. Do đó, các nhà nghiên cứu có thể đưa ra quyết định thời gian thực về mức độ thưa thớt mong muốn, và việc thay đổi độ thưa thớt rất hiệu quả.

Lời cảm ơn

Chúng tôi cảm ơn Yusen Zhang, Sarkar Snigdha Sarathi Das, Ranran Haoran Zhang, Xiaoxin Lu, và Ryo Kamoi cho những thảo luận và nhận xét có giá trị. Chúng tôi cũng muốn cảm ơn các nhà phản biện ẩn danh cho những nhận xét hữu ích của họ.
