PockEngine: Tinh chỉnh thưa thớt và hiệu quả trong túi
Ligeng Zhu
MIT
Cambridge, MA, USA
ligeng@mit.eduLanxiang Hu
UCSD
San Diego, CA, USA
hlxde2@gmail.comJi Lin
MIT
Cambridge, MA, USA
jilin@mit.edu
Wei-Chen Wang
MIT
Cambridge, MA, USA
wweichen@mit.eduWei-Ming Chen
MIT
Cambridge, MA, USA
wmchen@mit.eduChuang Gan
MIT-IBM Watson AI Lab
Cambridge, MA, USA
chuangg@mit.edu
Song Han
MIT, NVIDIA
Cambridge, MA, USA
songhan@mit.edu

TÓM TẮT
Học tập trên thiết bị và tinh chỉnh hiệu quả cho phép tùy chỉnh liên tục và bảo vệ quyền riêng tư (ví dụ: tinh chỉnh cục bộ các mô hình ngôn ngữ lớn trên dữ liệu cá nhân hóa). Tuy nhiên, các framework huấn luyện hiện tại được thiết kế cho các máy chủ đám mây với bộ tăng tốc mạnh mẽ (ví dụ: GPU, TPU) và thiếu các tối ưu hóa cho việc học tập trên biên, gặp phải thách thức về giới hạn tài nguyên và đa dạng phần cứng biên. Chúng tôi giới thiệu PockEngine: một công cụ nhỏ, thưa thớt và hiệu quả để cho phép tinh chỉnh trên các thiết bị biên khác nhau. PockEngine hỗ trợ lan truyền ngược thưa thớt: nó cắt tỉa đồ thị ngược và cập nhật mô hình một cách thưa thớt với việc tiết kiệm bộ nhớ và giảm độ trễ được đo lường trong khi duy trì chất lượng mô hình. Thứ hai, PockEngine là biên dịch trước: toàn bộ đồ thị huấn luyện (bao gồm các bước tiến, lùi và tối ưu hóa) được suy ra tại thời điểm biên dịch, giảm chi phí runtime và mang lại cơ hội cho các biến đổi đồ thị. PockEngine cũng tích hợp một tập hợp phong phú các tối ưu hóa đồ thị huấn luyện, do đó có thể tăng tốc hơn nữa chi phí huấn luyện, bao gồm sắp xếp lại toán tử và chuyển đổi backend. PockEngine hỗ trợ các ứng dụng, frontend và backend phần cứng đa dạng: nó linh hoạt biên dịch và điều chỉnh các mô hình được định nghĩa trong PyTorch/TensorFlow/Jax và triển khai tệp nhị phân đến CPU/GPU/DSP di động. Chúng tôi đánh giá PockEngine trên cả mô hình thị giác và mô hình ngôn ngữ lớn. PockEngine đạt tăng tốc lên đến 15× so với TensorFlow có sẵn (Raspberry Pi), tiết kiệm bộ nhớ 5.6× trong lan truyền ngược (Jetson AGX Orin). Đáng chú ý, PockEngine cho phép tinh chỉnh LLaMav2-7B trên NVIDIA Jetson AGX Orin ở tốc độ 550 token/s, nhanh hơn 7.9× so với PyTorch.

KHÁI NIỆM CCS
•Tổ chức hệ thống máy tính →Mạng nơ-ron.

TỪ KHÓA
mạng nơ-ron, cập nhật thưa thớt, huấn luyện trên thiết bị, tinh chỉnh hiệu quả

Định dạng tham chiếu ACM:
Ligeng Zhu, Lanxiang Hu, Ji Lin, Wei-Chen Wang, Wei-Ming Chen, Chuang Gan, và Song Han. 2023. PockEngine: Tinh chỉnh thưa thớt và hiệu quả trong túi. Trong Hội nghị Quốc tế lần thứ 56 về Kiến trúc Vi mô IEEE/ACM (MICRO '23), 28 tháng 10-1 tháng 11, 2023, Toronto, ON, Canada. ACM, New York, NY, USA, 14 trang. https://doi.org/10.1145/3613424.3614307

1 GIỚI THIỆU
Các thiết bị biên có mặt khắp nơi và tạo ra lượng dữ liệu ngày càng tăng trong cuộc sống hàng ngày của chúng ta. Nhu cầu về AI thông minh, cá nhân hóa và riêng tư đang tăng nhanh, vì một mô hình duy nhất không thể phù hợp với nhu cầu của các người dùng khác nhau. Tuy nhiên, trong khi suy luận học sâu được thực hiện rộng rãi trên các thiết bị biên, việc huấn luyện mạng nơ-ron sâu thường được chạy trên các máy chủ GPU đám mây. Huấn luyện dựa trên đám mây yêu cầu người dùng tải dữ liệu cá nhân lên đám mây, điều này không chỉ phát sinh chi phí truyền dữ liệu bổ sung mà còn mang lại rủi ro quyền riêng tư về dữ liệu nhạy cảm (ví dụ: dữ liệu chăm sóc sức khỏe, lịch sử nhập bàn phím, vị trí GPS, v.v.).

Huấn luyện trên thiết bị là một giải pháp đầy hứa hẹn cho việc tùy chỉnh mô hình mà không hy sinh quyền riêng tư (Hình 1). Nó cho phép một mô hình được huấn luyện trước liên tục thích ứng với dữ liệu cảm biến mà không cần gửi nó lên đám mây. Ví dụ, mô hình bàn phím thông minh có thể tự cập nhật để dự đoán tốt hơn từ tiếp theo từ lịch sử gõ của người dùng; trợ lý email có thể học từ các bản dự thảo trước đây của người dùng và huấn luyện mô hình ngôn ngữ cá nhân hóa; mô hình thị giác có thể tự động thích ứng với môi trường có dịch chuyển miền [53]). Mô hình huấn luyện gần cảm biến cũng mang lại những lợi ích quan trọng về năng lượng và kết nối: nó tiết kiệm năng lượng từ truyền dữ liệu (đắt hơn nhiều so với tính toán [35]); nó cũng giúp ích cho các ứng dụng như cảm biến đại dương [25] và nông nghiệp thông minh [56] không có quyền truy cập vật lý vào Internet.

Mặc dù có tất cả những lợi ích, huấn luyện trên thiết bị là khó khăn do các thách thức sau:

(1) Giới hạn tài nguyên. Khả năng của các thiết bị biên nhỏ hơn các máy chủ đám mây hàng bậc độ lớn. Mọi người đã cố gắng rất nhiều để nén các mô hình học sâu chỉ để suy luận biên, trong khi huấn luyện và tinh chỉnh mô hình tốn nhiều năng lượng, tính toán và bộ nhớ hơn. Chúng ta cần thêm bộ nhớ để lưu trữ tất cả các bản đồ đặc trưng trung gian cho lan truyền ngược, và thêm tính toán cho lượt truyền ngược (khoảng 3× so với suy luận). Đôi khi việc huấn luyện cần kích thước lô lớn hơn để đảm bảo hội tụ ổn định, làm cho quá trình thậm chí còn tốn kém hơn. Đối với MobilenetV2 [50], bộ nhớ huấn luyện lớn hơn 14× và 7.3× so với suy luận (kích thước lô 8) và đối với BERT [18], việc sử dụng bộ nhớ đỉnh lớn hơn 7.3× so với suy luận. Hơn nữa, các bộ tối ưu hóa cũng yêu cầu thêm bộ nhớ (2x cho Momentum và 3x cho Adam [30]). Với framework huấn luyện hiện tại, chi phí huấn luyện có thể sớm vượt quá giới hạn tài nguyên của phần cứng biên.

(2) Đa dạng phần cứng Trong khi các bộ tăng tốc trên máy chủ đám mây được thống trị bởi GPU, phần cứng của nền tảng biên có nhiều lựa chọn trên thị trường. Bộ xử lý từ vi điều khiển ARM đến chip Apple M1 mạnh mẽ, và bộ tăng tốc khác nhau giữa GPU Qualcomm Adreno, DSP Hexagon và TPU biên. Mỗi phần cứng đi kèm với thư viện suy luận khác nhau. PockEngine có thể trực tiếp sử dụng các thư viện suy luận này để huấn luyện bằng cách biên dịch đồ thị huấn luyện thành định dạng ONNX tiêu chuẩn. Mặt khác, các framework huấn luyện học sâu phổ biến như TensorFlow [4], PyTorch [46] và Jax [9] được phát triển cho GPU/TPU đám mây cao cấp. Hiệu suất kém khi áp dụng trực tiếp cho nền tảng biên¹.

Để giải quyết các thách thức trên, chúng tôi giới thiệu PockEngine, một công cụ huấn luyện nhỏ và hiệu quả được thiết kế cho huấn luyện trên thiết bị. Chúng tôi nhấn mạnh các tính chất sau:

• PockEngine cung cấp hỗ trợ cấp hệ thống cho cả lan truyền ngược dày đặc và thưa thớt. Ngoài việc cập nhật toàn bộ mô hình, PockEngine hỗ trợ các lược đồ cập nhật thưa thớt linh hoạt bằng cách tính gradient chỉ cho một phần của các trọng số, điều này được chứng minh là một lựa chọn hiệu quả hơn cho tinh chỉnh/học chuyển giao mà không làm hại độ chính xác [10,20,23,24,37,41,42]. Các framework huấn luyện hiện tại chỉ có thể mô phỏng lan truyền ngược thưa thớt bằng cách tính toán ngược và che gradient, nhưng không thể nhận ra tăng tốc và tiết kiệm bộ nhớ được đo lường. PockEngine hỗ trợ lan truyền ngược thưa thớt thông qua cắt tỉa đồ thị và loại bỏ mã chết với bản chất biên dịch, dẫn đến tính toán và sử dụng bộ nhớ nhỏ hơn.

• PockEngine là một công cụ huấn luyện hiệu quả dựa trên biên dịch và cho phép nhiều framework chỉ suy luận thực hiện huấn luyện. Quy trình biên dịch của chúng tôi giúp kết nối các kiến trúc mô hình đa dạng và các lựa chọn frontend (ví dụ: mô hình thị giác/NLP, định nghĩa PyTorch/TensorFlow/ONNX) với các thư viện backend khác nhau (ví dụ: SNPE cho Qualcomm, Metal cho Apple Silicon, TVM), tiết lộ một biểu diễn trung gian thống nhất (IR). Bằng cách chia sẻ cùng một tập hợp toán tử cho cả hoạt động tiến và lùi, chúng tôi không chỉ cho phép các framework suy luận huấn luyện mạng nơ-ron mà còn cho phép các tối ưu hóa đồ thị khác nhau để cải thiện hiệu quả (xem Hình 4).

• PockEngine triển khai một tập hợp phong phú các tối ưu hóa đồ thị để cải thiện hiệu quả trên các thiết bị biên, bao gồm fusion toán tử, sắp xếp lại toán tử, chuyển đổi layout và chuyển đổi backend thường chỉ được sử dụng cho suy luận. Chúng tôi thấy rằng các đồ thị huấn luyện thực sự có nhiều cơ hội tối ưu hóa hơn do độ phức tạp của chúng. Bằng cách chia sẻ cùng một tập hợp toán tử với đồ thị suy luận, PockEngine có thể sử dụng tốt các kỹ thuật tối ưu hóa từ các công cụ suy luận (ví dụ: PockEngine sử dụng convolution winograd trước đây chỉ dành cho suy luận để tăng tốc huấn luyện).

Chúng tôi đánh giá toàn diện PockEngine trên sáu nền tảng biên và sáu tác vụ học sâu từ thị giác đến NLP. PockEngine đạt tăng tốc lên đến 11× so với TensorFlow cho cùng khối lượng công việc huấn luyện. Với lan truyền ngược thưa thớt, chúng ta có thể cải thiện thêm tăng tốc lên đến 21× mà không mất độ chính xác học chuyển giao trên vi điều khiển nhỏ. Chúng tôi hy vọng công việc của mình có thể đóng góp vào sự phát triển mạnh mẽ của huấn luyện trên thiết bị bằng cách cung cấp một framework huấn luyện đa năng, hiệu quả cao, thân thiện với người dùng cho các thiết bị biên.

2 CÔNG VIỆC LIÊN QUAN

2.1 Hệ thống học sâu đám mây
Thành công của học sâu được xây dựng trên các framework huấn luyện phổ biến như PyTorch [46], TensorFlow [5], MXNet [12], JAX [9], v.v. Các hệ thống này được thiết kế cho tính linh hoạt phát triển và phụ thuộc vào ngôn ngữ chủ (ví dụ: Python) để thực thi. Điều này mang lại chi phí bộ nhớ đáng kể (>300MB) và làm cho runtime đặc biệt chậm trên CPU tần số thấp (ví dụ: ARM Cortex). Hơn nữa, các kernel toán tử được tối ưu hóa cho các thiết bị GPU cao cấp và thiếu điều chỉnh hiệu suất cho các thiết bị biên và một số chi phí như bộ đệm gradient bổ sung cho bước tối ưu hóa không được coi là nút thắt cổ chai đối với phần cứng máy chủ mạnh mẽ. PockEngine là một framework dựa trên biên dịch do đó runtime không phụ thuộc vào ngôn ngữ chủ như được so sánh trong Bảng 1. Điều này chuyển hầu hết khối lượng công việc từ runtime sang thời gian biên dịch để giảm thiểu chi phí runtime và cho phép các tối ưu hóa sau này để cải thiện thông lượng huấn luyện.

2.2 Hệ thống học sâu biên
Khi triển khai mô hình trên các thiết bị biên nhỏ, các thư viện suy luận như TVM [13], TF-Lite, NCNN [1], TensorRT [2] và OpenVINO [57] cung cấp các kernel được tối ưu hóa cho nền tảng di động và cung cấp runtime nhẹ mà không có ngôn ngữ chủ. Tuy nhiên, chúng chủ yếu tập trung vào suy luận và không hỗ trợ huấn luyện trên thiết bị. MNN [29] có hỗ trợ sơ bộ cho CNN nhưng tính linh hoạt khá hạn chế và nó không tối ưu hóa việc sử dụng bộ nhớ huấn luyện. POET [47] áp dụng tái vật chất hóa và phân trang để xử lý kích thước bộ nhớ hạn chế, nhưng nó giới thiệu tính toán bổ sung, dựa vào Flash bên ngoài lớn (ví dụ: thẻ SD 32GB) và không hỗ trợ định nghĩa mô hình và khối lượng công việc chung. PockEngine cung cấp hỗ trợ huấn luyện hoàn chỉnh cho các mô hình phổ biến ở nhiều quy mô khác nhau bao gồm MCUNet [40], MobilenetV2 [50], ResNet [22], DistilBERT [51] và BERT [18]. PockEngine tối ưu hóa cả hiệu quả tính toán và bộ nhớ để làm cho huấn luyện trên thiết bị trở nên dễ dàng và thực tế.

2.3 Thuật toán học hiệu quả trên thiết bị
Các thiết bị biên có khả năng tính toán hạn chế. Do đó, huấn luyện trên thiết bị cho các thiết bị biên thường tập trung vào học chuyển giao [10,33]. Nó trước tiên huấn luyện trước mô hình trên các tập dữ liệu quy mô lớn để học các đặc trưng chung và phong phú, như ImageNet [17] cho ConvNets hoặc BooksCorpus [64] cho BERT. Sau đó mô hình được chuyển giao đến các tác vụ downstream, như Visual Wake Words [16] cho thị giác hoặc benchmark GLUE [58] cho ngôn ngữ. Sau đó, mô hình có thể được tùy chỉnh với một lượng nhỏ dữ liệu cá nhân (ví dụ: học giọng của người dùng) để thực hiện tốt hơn trong cùng tác vụ.

Do quy mô nhỏ hơn và đa dạng của dữ liệu downstream, mọi người thấy rằng không phải lúc nào cũng cần thiết phải cập nhật toàn bộ mô hình để đạt được hiệu suất tốt. Cập nhật thưa thớt một phần của mô hình được chứng minh là một giải pháp tốt đạt được hiệu suất tương tự hoặc tốt hơn với chi phí huấn luyện nhỏ hơn [10,20,23,24,37,41,42]. Phương pháp đơn giản nhất là chỉ tinh chỉnh lớp phân loại [11,19,21,52], nhưng khả năng bị hạn chế khi dịch chuyển miền lớn. Đối với mô hình CNN, mọi người đã điều tra tinh chỉnh chỉ bias [10,61], các lớp chuẩn hóa lô [20,43], thêm các nhánh song song [10], v.v. Lược đồ lan truyền ngược thưa thớt thậm chí còn phổ biến hơn để thích ứng các mô hình ngôn ngữ được huấn luyện trước (ví dụ: BERT [18], GPT [49]) với các tác vụ downstream khác nhau, giảm đáng kể các tham số có thể huấn luyện [23,24,37]. Tuy nhiên, lan truyền ngược thưa thớt thiếu hỗ trợ hệ thống. Mặc dù có tiết kiệm lý thuyết tuyệt vời, các framework huấn luyện hiện tại không thể nhận ra tăng tốc được đo lường hoặc tiết kiệm bộ nhớ từ lan truyền ngược thưa thớt. PockEngine cung cấp hỗ trợ cấp hệ thống cho các khối lượng công việc linh hoạt như vậy để cung cấp chương trình nhanh hơn và runtime hiệu quả.

2.4 Biến đổi đồ thị tính toán và tối ưu hóa
Có rất nhiều biến đổi đồ thị cho các tình huống suy luận. Ví dụ, một biến đổi phổ biến được sử dụng trong triển khai biên là chuyển đổi layout dữ liệu, vì 'NCHW' được GPU huấn luyện ưa thích không hiệu quả trên biên. Một kỹ thuật tối ưu hóa phổ biến khác là fusion lớp. Các lớp tốn nhiều IO (ví dụ: ReLU) thường có thể được fusion vào các lớp tốn nhiều tính toán trước đó (ví dụ: CONV, LINEAR). Ngoài ra, MetaFlow [27] đề xuất các biến đổi đồ thị bảo toàn chức năng để tối ưu hóa kiến trúc DNN. TASO [26] tiếp tục giới thiệu việc tạo tự động các quy tắc biến đổi sử dụng xác minh chính thức. Các kỹ thuật này đã được chứng minh hiệu quả trong suy luận, nhưng ít nghiên cứu khám phá hiệu suất của chúng trong huấn luyện, mặc dù đồ thị huấn luyện phức tạp hơn nhiều. Đứng trên vai của trí tuệ thông thường, PockEngine là khám phá sớm để áp dụng các kỹ thuật tối ưu hóa đồ thị này cho huấn luyện trên thiết bị và khám phá thêm các tối ưu hóa tiềm năng. PockEngine cho thấy rằng các tối ưu hóa này mang lại tăng tốc lên đến 1.2x.

2.5 Quy trình dựa trên biên dịch
Các framework huấn luyện hiện tại (ví dụ: PyTorch, TensorFlow) dựa trên phân tích tự động runtime cho tính linh hoạt. Tuy nhiên, thiết kế không phù hợp cho các thiết bị biên với tài nguyên bộ nhớ và tính toán hạn chế. Thay vào đó, PockEngine dựa trên quy trình dựa trên biên dịch, chia sẻ các lợi ích sau:

Giảm tải khối lượng công việc từ Runtime sang thời gian biên dịch. Với thiết kế tập trung vào biên dịch, chúng ta có thể giảm tải một phần khối lượng công việc từ runtime sang thời gian biên dịch, như suy ra đồ thị ngược với autodiff, lập lịch bộ nhớ, lập kế hoạch thực thi, v.v. Mạng nơ-ron hiện đại thường bao gồm hàng ngàn toán tử, chi phí có thể nhỏ đối với máy chủ đám mây nhưng không thể bỏ qua đối với các thiết bị biên (Hình 7).

Bằng cách giảm tải tính toán cho trình biên dịch, có thể thực hiện các tối ưu hóa tích cực hơn sẽ không khả thi hoặc hiệu quả để thực hiện tại runtime. Ví dụ, PockEngine thực hiện cắt tỉa đồ thị, fusion và chuyển đổi backend, có thể dẫn đến tăng hiệu suất đáng kể và tiết kiệm bộ nhớ.

Một lợi thế khác của quy trình dựa trên biên dịch là nó cho phép chúng ta tối ưu hóa mã trên toàn bộ chương trình, thay vì chỉ tập trung vào tối ưu hóa các hoạt động riêng lẻ tại runtime. Điều này không chỉ cho phép chúng ta biên dịch chỉ các toán tử được sử dụng để vận chuyển tệp nhị phân mỏng mà còn tiết lộ sự dư thừa bộ nhớ trong vòng lặp huấn luyện (chi tiết trong Phần 3.2).

Hỗ trợ Frontend/Backend đa dạng. Không giống như đám mây, các nền tảng biên rất đa dạng, với các tập lệnh khác nhau, mức độ song song, v.v. Quy trình dựa trên biên dịch của chúng tôi cung cấp hỗ trợ chung cho các frontend/backend khác nhau. Nó có thể dễ dàng hỗ trợ huấn luyện trên phần cứng và thư viện nhà cung cấp được thiết kế đặc biệt cho suy luận (ví dụ: PockEngine có thể cho phép huấn luyện trên DSP Qualcomm Hexagon với thư viện SNPE).

Frontend PockEngine nhận vào một mạng nơ-ron được biểu diễn trong các biểu diễn khác nhau (ví dụ: ONNX, torchscript, tf.graph) và phân tích cấu trúc DAG. Sau đó nó sẽ thực hiện phân tích tự động (autodiff) để suy ra đồ thị ngược tính gradient w.r.t. hàm mất mát (Hình 7). Với đồ thị tiến và lùi tĩnh, PockEngine sẽ chuyển đổi nó thành biểu diễn trung gian thống nhất (IR), thực hiện tối ưu hóa đồ thị (sẽ được giới thiệu sau) và tạo mã cho các backend khác nhau. Chỉ các toán tử được sử dụng sẽ được biên dịch và PockEngine liên kết các OP này để xây dựng tệp nhị phân thực thi nhẹ. Backend PockEngine hỗ trợ cả thư viện nhà cung cấp (ví dụ: SNPE cho GPU và DSP Snapdragon, TensorRT cho GPU NVIDIA) và kernel tùy chỉnh (ví dụ: điều chỉnh TVM [13] cho CPU ARM).

Đáng chú ý, thay vì ràng buộc mỗi toán tử với một triển khai ngược (ví dụ: matmul, matmul_backward), PockEngine sử dụng cùng một tập hợp các hoạt động nguyên thủy như suy luận để xây dựng đồ thị huấn luyện, cho phép chúng ta sử dụng các backend chỉ suy luận (ví dụ: SNPE, TensorRT, TVM) để huấn luyện, đạt được hiệu quả cao với nỗ lực kỹ thuật tối thiểu.

2.6 Lan truyền ngược thưa thớt và cắt tỉa đồ thị tính toán
Các thiết bị biên có khả năng tính toán hạn chế so với đám mây. Do đó, huấn luyện trên thiết bị trên biên thường nhắm đến tình huống học chuyển giao/tinh chỉnh. Do quy mô nhỏ hơn và đa dạng của dữ liệu downstream, mọi người thấy rằng cập nhật toàn bộ mô hình có thể không phải lúc nào cũng dẫn đến hiệu suất tốt nhất do over-fitting và biến dạng đặc trưng [10,33]. Cập nhật chỉ một tập con của các mô hình được chứng minh là một giải pháp tốt đạt được hiệu suất tương tự hoặc tốt hơn với chi phí huấn luyện nhỏ hơn nhiều, bao gồm cập nhật các thuật ngữ bias [10] và các lớp chuẩn hóa [20] cho huấn luyện mô hình thị giác các phần rank thấp [24] và prompt đầu vào cho mô hình ngôn ngữ [37], và cập nhật thưa thớt các mô-đun quan trọng [41]. PockEngine nhằm hỗ trợ chung huấn luyện trên thiết bị cho các khối lượng công việc khác nhau và chúng tôi tập trung vào cập nhật thưa thớt để giảm chi phí huấn luyện.

Trong quá trình biên dịch, PockEngine nhận vào một lược đồ lan truyền ngược thưa thớt do người dùng định nghĩa và sẽ cắt tỉa các đồ thị con tương ứng của tính toán lan truyền ngược. PockEngine hỗ trợ linh hoạt các mẫu lan truyền ngược thưa thớt sau:

Cập nhật chỉ Bias. Cập nhật chỉ bias không yêu cầu lưu activation trung gian [10], giảm đáng kể việc sử dụng bộ nhớ (xem xét một lớp tuyến tính y=Wx, dW=f1(dy,x), db=f2(dy), chỉ gradient trọng số yêu cầu lưu đầu vào). Nó cũng tiết kiệm tính toán bằng 1/3 bằng cách bỏ qua tính toán dW.

Lan truyền ngược thưa thớt theo lớp. Không phải tất cả các lớp/tensor trọng số đều quan trọng như nhau đối với học chuyển giao [41]. Đối với học chuyển giao đến tác vụ downstream, chúng tôi thấy rằng một phần của các lớp có thể được giữ đóng băng mà không ảnh hưởng đến hiệu suất học chuyển giao (chúng ta có thể tìm các lớp để đóng băng bằng phân tích độ nhạy [41]; chi tiết trong Phần 4.1). Do đó, chúng ta có thể bỏ qua tính toán của một phần các lớp để cải thiện thêm thông lượng huấn luyện.

Lan truyền ngược thưa thớt dưới lớp. Đối với các thiết bị biên với khả năng hạn chế (ví dụ: vi điều khiển), chúng tôi hỗ trợ thêm BP thưa thớt cấp dưới lớp, trong đó chỉ một phần của các kênh của một lớp (các lớp tích chập và các lớp tuyến tính) được cập nhật². Nó giảm thêm chi phí bộ nhớ để lưu trữ activation trung gian (chúng ta không cần lưu trữ activation cho các kênh đóng băng) và chi phí tính toán để tính gradient.

3 POCKENGINE
So với các framework huấn luyện thông thường, lan truyền ngược thưa thớt có những lợi thế độc đáo sau:

• Các activation trung gian đắt đỏ có thể được giải phóng ngay lập tức sau khi tiến Khi học chỉ bias (dy/db và dy/dx) hoặc hoàn toàn bỏ qua lớp (chỉ dy/dx để giữ quy tắc chuỗi). Do đó lan truyền ngược thưa thớt giảm đáng kể nút thắt cổ chai bộ nhớ chính của huấn luyện (đường kết nối đỏ trong Hình 3.a).

• Lan truyền ngược thưa thớt không lan truyền ngược đến các lớp đầu tiên trong mô hình DNN vì không cần tính gradient đến các lớp phía trước nếu chúng không yêu cầu gradient (dấu X đỏ trong Hình 5).

Không có công việc trước đó nào có thể chuyển đổi tiết kiệm lý thuyết thành tăng tốc được đo lường và tiết kiệm bộ nhớ. PockEngine cung cấp hỗ trợ hệ thống cho BP thưa thớt và có thể thực sự giảm chi phí huấn luyện trên thiết bị và chúng tôi mở rộng như sau:

3.1 Tìm kiếm lược đồ lan truyền ngược thưa thớt
Không phải tất cả các trọng số đều quan trọng như nhau đối với học chuyển giao [20,34,41]. Chúng tôi nhằm tinh chỉnh chỉ các trọng số quan trọng để giảm chi phí huấn luyện trong khi bảo toàn độ chính xác của mô hình.

Mô hình chi phí và tiêu chí tìm kiếm. Để tìm lược đồ huấn luyện, chúng tôi xây dựng các mô hình chi phí cho chất lượng mô hình và chi phí huấn luyện. Theo [41], đầu tiên chúng tôi tinh chỉnh chỉ một lớp tuyến tính (conv, fc) cho đến khi hội tụ, và sau đó lặp lại quá trình này cho tất cả các lớp. Đây là phân tích ngoại tuyến và chúng tôi sử dụng cải thiện/suy thoái độ chính xác làm "đóng góp" của các trọng số của lớp thứ i (ΔaccWi). Tương tự, chúng tôi có được kết quả cho các thuật ngữ bias của lớp thứ k (Δaccbk) và sau đó lặp lại các hoạt động tương tự cho tất cả trọng số và bias để ước tính hiệu suất của chúng.

Đối với chi phí huấn luyện, chúng tôi tập trung vào bộ nhớ vì các thiết bị biên thường có bộ nhớ hạn chế và sẽ dễ dàng gặp OOM. Do đó chúng tôi profile kích thước bản đồ đặc trưng và ghi nó là Memoryk,i,r. Sau đó chúng tôi giải quyết tối ưu hóa sau:

k*,i*,r* = max k,i,r (∑k∈iΔaccbk + ∑i∈i,r∈rΔaccWi,r)
s.t. Memory(k,i,r) ≤ constraint, (1)

trong đó i là chỉ số lớp của trọng số, k là chỉ số lớp của bias và r là tỷ lệ trọng số có thể học. Tối ưu hóa các mục tiêu tìm config cập nhật tối ưu trong đó tổng đóng góp được tối đa hóa và dấu chân bộ nhớ không vượt quá ràng buộc. Chúng tôi giả định rằng đóng góp độ chính xác của mỗi tensor (Δacc) có thể được tổng hợp do đó vấn đề có thể được giải quyết hiệu quả với tìm kiếm tiến hóa.

Tổng quát hóa và tăng tốc. Đáng chú ý rằng lược đồ cập nhật thưa thớt là tổng quát và phổ quát trên các tập dữ liệu khác nhau. Chúng tôi chỉ thực hiện MỘT tìm kiếm lược đồ trên CIFAR (cho mô hình thị giác) và CoLA (cho mô hình ngôn ngữ) và sparse-BP thể hiện khả năng tổng quát hóa tốt. Các lược đồ đạt được độ chính xác huấn luyện cạnh tranh so với tinh chỉnh đầy đủ (Bảng 2 và Bảng 3). Cụ thể, chúng tôi thấy rằng đối với CNN: hiệu quả nhất là cập nhật trọng số của convolution đầu tiên trong mỗi khối, trong khi đối với các khối transformer, các trọng số trong mô-đun attention và lớp tuyến tính đầu tiên trong Feed-Forward Network (FFN) quan trọng hơn (Hình 6). Các lược đồ như vậy cũng hiệu quả về bộ nhớ: depthwise conv và pointwise conv thứ hai trong khối bottleneck đảo ngược (Hình 6.a) và lớp tuyến tính thứ hai trong FFN (Hình 6.b) có activation đầu vào lớn nhất, trong khi lược đồ cập nhật của chúng tôi không yêu cầu lưu các đặc trưng lớn này.

Sau khi tìm và chỉ định các gradient cần thiết cho huấn luyện trên thiết bị, PockEngine tự động theo dõi phụ thuộc và phân tích topology được cập nhật, sau đó cắt tỉa đồ thị huấn luyện bằng cách sử dụng loại bỏ mã chết (DCE) để cắt tỉa đồ thị tính toán và loại bỏ các nút trung gian và bộ đệm không còn cần thiết cho huấn luyện. Bởi vì việc cắt tỉa được thực hiện ở cấp độ đồ thị tại thời gian biên dịch, nó có thể mang lại tiết kiệm bộ nhớ được đo lường và cải thiện thông lượng.

3.2 Tối ưu hóa đồ thị huấn luyện
Sau khi chúng ta có đồ thị huấn luyện tĩnh, được cắt tỉa, PockEngine áp dụng các kỹ thuật tối ưu hóa đồ thị khác nhau trên IR thống nhất trước khi dịch sang các backend khác nhau, điều này cải thiện thêm hiệu quả huấn luyện.

Sắp xếp lại toán tử và cập nhật tại chỗ. Các thứ tự thực thi khác nhau dẫn đến các chu kỳ sống khác nhau của tensor và dấu chân bộ nhớ tổng thể/đỉnh cũng sẽ bị ảnh hưởng ngay cả đối với cùng một đồ thị tính toán. Điều này đã được nghiên cứu kỹ cho suy luận [6,38] nhưng ít được thảo luận cho huấn luyện vì đồ thị ngược thường được suy ra trong runtime và trình biên dịch/lập lịch không có thông tin toàn cục về quá trình huấn luyện.

Một ví dụ cụ thể là bộ tối ưu hóa, trong đó các gradient được áp dụng để cập nhật các tham số mô hình. Trong huấn luyện thông thường, các framework tính tất cả gradient và sau đó áp dụng cập nhật. Điều này phổ biến trong các framework như PyTorch và TensorFlow vì bộ tối ưu hóa và tiến-lùi là các thành phần riêng biệt trong thiết kế hệ thống. Tuy nhiên, thực hành như vậy dẫn đến lãng phí bộ nhớ đáng kể để lưu trữ gradient. Trong huấn luyện lô nhỏ với lan truyền ngược thưa thớt, chi phí lưu trữ gradient tham số gần với việc sử dụng bộ nhớ đỉnh trong tiến và lùi như được hiển thị trong Bảng 4:

Để giải quyết chi phí này, PockEngine có được tất cả thông tin tensor và lập kế hoạch cho một lịch trình thực thi tốt hơn. Bằng cách sắp xếp lại các toán tử, các gradient có thể được áp dụng ngay lập tức cho các tham số tương ứng trước khi lan truyền ngược đến các lớp trước đó. Chúng tôi tiếp tục theo dõi chu kỳ sống của tất cả tensor (trọng số, activation, gradient) và sắp xếp lại các lịch trình để giảm việc sử dụng bộ nhớ, dẫn đến tiết kiệm lên đến 21x trên vi điều khiển cho MCUNet.

Fusion toán tử. Trong hầu hết các framework học sâu, một hoạt động đơn giản thường yêu cầu một số kernel tinh vi để triển khai. Ví dụ, một hoạt động chuẩn hóa lớp đơn yêu cầu ba lời gọi kernel và hai lần đọc và ghi bộ nhớ cho tiến, và sáu lời gọi kernel và năm lần đọc và ghi bộ nhớ cho lùi. Hơn nữa, các biến đổi như fusion các hoạt động rẻ vào các hoạt động đắt (ví dụ: CONV-BN-ReLU) và các hoạt động tuyến tính song song (ví dụ: batch matmul) đã được chứng minh hiệu quả trong việc cải thiện suy luận. Trong quá trình biên dịch và tạo mã, PockEngine fusion các kernel này thành một kernel duy nhất và dẫn đến ít IO bộ nhớ và lời gọi kernel hơn.

Biến đổi đồ thị bảo toàn chức năng. Các framework DNN hiện tại tối ưu hóa đồ thị tính toán bằng cách áp dụng các quy tắc được thiết kế bởi các chuyên gia miền [2,4] hoặc được khám phá tự động bởi chương trình [26,28]. Có nhiều cơ hội tối ưu hóa hơn nhưng nghiên cứu trước đây không thể sử dụng chúng vì đồ thị ngược được suy ra tại runtime trong các framework trước đó. Điều tra rộng rãi các tối ưu hóa đồ thị tiềm năng sẽ dẫn đến huấn luyện chậm và phát sinh chi phí runtime không mong muốn.

Công cụ của chúng tôi tích hợp các kỹ thuật tối ưu hóa này và là một thử nghiệm sớm để áp dụng cho đồ thị huấn luyện. PockEngine biến đổi layout dữ liệu cho phần cứng khác nhau. Đối với các tác vụ thị giác, NCHW là layout được sử dụng rộng rãi nhất. Nhưng định dạng này chỉ hiệu quả trên các bộ tăng tốc như GPU. Khi huấn luyện trên CPU di động / DSP, định dạng như vậy không còn tối ưu và PockEngine sẽ biến đổi layout tại thời gian biên dịch để tạo điều kiện cho hiệu quả huấn luyện runtime.

Hơn nữa, PockEngine khám phá các triển khai khác nhau của kernel. Ví dụ, Winograd đã được sử dụng rộng rãi trong suy luận vì tính toán nhanh hơn. Tuy nhiên, tiết kiệm không miễn phí: nó yêu cầu tiền xử lý bổ sung của trọng số. Nếu trọng số không tĩnh, thì biến đổi cần được áp dụng mỗi epoch và tổng FLOP thậm chí có thể cao hơn convolution bình thường. Do đó nó được sử dụng trong suy luận và không được tích hợp vào các framework huấn luyện. Đối với các tình huống huấn luyện trên thiết bị, có nhiều lớp đóng băng trong đó trọng số không được thay đổi trong quá trình huấn luyện [10,61]. Các lớp này thực tế có thể sử dụng Winograd để tăng tốc nhưng các cơ hội như vậy bị bỏ qua trong các framework hiện tại ngay cả khi thuộc tính requires_grad được đặt thành False. PockEngine có được đồ thị huấn luyện hoàn chỉnh trong thời gian biên dịch do đó biết thông tin cập nhật của mỗi tham số. Do đó, chúng ta có thể phân tích thông tin tensor và đồ thị, biết trọng số nào tĩnh và trọng số nào động. PockEngine có thể ràng buộc hoạt động với triển khai nhanh nhất và cho phép cơ hội sử dụng Winograd ngay cả trong huấn luyện.

4 KẾT QUẢ
Trong phần này, chúng tôi đánh giá toàn diện hiệu suất của PockEngine. Đầu tiên chúng tôi nghiên cứu hiệu quả của lan truyền ngược thưa thớt, sau đó trình bày kết quả thực nghiệm trên các phần cứng và nền tảng khác nhau, so sánh với các framework huấn luyện khác. Cuối cùng, chúng tôi thảo luận kết quả tối ưu hóa đồ thị.

4.1 Thiết lập
Mô hình. Chúng tôi đánh giá PockEngine trên các mô hình thị giác và ngôn ngữ phổ biến. Đối với các tác vụ thị giác, chúng tôi chọn MCUNet [40] (mô hình 5FPS), MobilenetV2 [50] (hệ số nhân chiều rộng 0.35 và 1.0) và ResNet-50 [22]. Tất cả các lớp chuẩn hóa (ví dụ: BatchNorm) được fusion vào các hoạt động tuyến tính (ví dụ: Conv, Linear). Đối với các mô hình ngôn ngữ che mặt, chúng tôi chọn phiên bản base-uncased của BERT [18] và DistilBERT [51] để benchmark hiệu suất.

Tập dữ liệu. Đối với các mô hình thị giác, đầu tiên chúng tôi huấn luyện trước chúng trên ImageNet [17] với độ phân giải 224×224 (trừ 128×128 cho MCUNet), và sau đó tinh chỉnh trên một tập hợp các tác vụ downstream để đánh giá độ chính xác học chuyển giao (bao gồm Cars [31], CIFAR-10 [32], CUB [60], Flowers [44], Foods [8], Pets [45] và VWW [16] thiết lập TinyML thông thường được sử dụng trong [10,41]). Các mô hình NLP (BERT và DistilBERT) được huấn luyện trước trên Wikipedia và BookCorpus [65]. Chúng tôi đánh giá hiệu suất học chuyển giao của chúng trên benchmark GLUE [58] (bao gồm CoLA, MNLI, MRPC, QNLI, QQP, RTE, SST-2). Đối với các mô hình chatbot, chúng tôi sử dụng Llamav2 [55] sau đó tinh chỉnh với hướng dẫn từ tập dữ liệu Stanford Alpaca [54]. Chúng tôi theo alpaca-eval [36] và MT-Bench [15,62] để đánh giá chất lượng phản hồi.

4.2 Hiệu quả của lan truyền ngược thưa thớt
Lan truyền ngược thưa thớt đạt độ chính xác đầy đủ. Huấn luyện trên thiết bị nhằm liên tục cải thiện trải nghiệm người dùng bằng dữ liệu cục bộ do đó số lượng mẫu huấn luyện thường nhỏ so với huấn luyện trước quy mô lớn. Do đó, không cần thiết phải thực hiện lan truyền ngược đầy đủ để cập nhật toàn bộ mô hình như được hiển thị trong Bảng 2. Đáng chú ý, độ chính xác downstream của lan truyền ngược thưa thớt có thể phù hợp với các baseline lan truyền đầy đủ trên cả mô hình thị giác và ngôn ngữ (<1% suy giảm hiệu suất trung bình). Trên một số tập dữ liệu downstream, hiệu suất của lan truyền ngược thưa thớt thậm chí còn cao hơn vượt qua các baseline đầy đủ như Flower trong thị giác và mrpc-acc trong ngôn ngữ. Hiệu suất cao hơn nhiều so với các yêu cầu thông thường cho TinyML [7] (độ chính xác 80% trên VWW), cho thấy lan truyền thưa thớt là một chiến lược tốt cho huấn luyện trên thiết bị.

Hơn nữa, khi đánh giá các mô hình ngôn ngữ, lan truyền ngược thưa thớt cũng duy trì độ chính xác tinh chỉnh với chi phí huấn luyện giảm. Suy giảm hiệu suất trung bình nằm trong 1%. Điều này có nghĩa là việc sử dụng lan truyền ngược thưa thớt có thể hiệu quả giảm thời gian và chi phí cần thiết để huấn luyện mô hình ngôn ngữ mà không hy sinh độ chính xác. Thực tế, kết quả cho thấy lan truyền ngược thưa thớt thậm chí có thể cải thiện hiệu suất của mô hình trên một số tác vụ phụ nhất định (ví dụ: MRPC và RTE). Bằng cách làm cho huấn luyện hiệu quả hơn, lan truyền ngược thưa thớt có thể giúp tăng tốc tiến bộ trong các lĩnh vực này và cho phép phát triển các mô hình ngôn ngữ tiên tiến hơn.

Lan truyền ngược thưa thớt giảm thời gian huấn luyện và bộ nhớ. Bên cạnh hiệu suất tương đương khi chuyển giao đến các tác vụ downstream, lan truyền ngược thưa thớt giảm đáng kể bộ nhớ đỉnh huấn luyện và cải thiện tốc độ huấn luyện.

Như được hiển thị trong Bảng 4, bộ nhớ huấn luyện tăng nhanh w.r.t kích thước lô và sớm vượt quá giới hạn cho các thiết bị biên (ví dụ: 1GB cho Raspberry Pi), việc sử dụng kỹ thuật swap hoặc tái vật chất hóa [47] sẽ giới thiệu tính toán và chi phí năng lượng bổ sung. Lan truyền ngược thưa thớt cắt giảm việc sử dụng bộ nhớ đỉnh (2.2× đến 21.3×) và tiết kiệm là chung trên các mô hình và ứng dụng. Ngay cả khi kích thước lô tăng, bộ nhớ cần thiết vẫn nhỏ và chi phí bộ nhớ của huấn luyện MCUNet-5FPS sparse-BP với kích thước lô 8 vẫn nhỏ hơn lô 1. Huấn luyện theo lô giúp cải thiện việc sử dụng thiết bị cũng như tính ổn định huấn luyện.

Khi áp dụng lan truyền ngược thưa thớt, các hoạt động và tensor liên quan đến các lớp đóng băng được tự động cắt tỉa từ đồ thị huấn luyện thông qua loại bỏ mã chết, dẫn đến ít tính toán hơn và thông lượng huấn luyện cao hơn. Hình 9 cho thấy lan truyền ngược thưa thớt có thể tăng tốc thêm 1.3x đến 1.6x trên Raspberry Pi. Các thuật toán huấn luyện hiệu quả trước đó chỉ thảo luận hiệu suất lý thuyết và PockEngine cung cấp hỗ trợ cấp hệ thống và dịch thành giảm được đo lường.

4.3 PockEngine tăng tốc huấn luyện trên thiết bị
Chúng tôi so sánh PockEngine với các framework huấn luyện khác trong Hình 9. PockEngine cho phép huấn luyện trên các nền tảng phần cứng khác nhau, bao gồm Raspberry Pi 4, CPU và DSP Snapdragon, Apple M1, Jetson Nano và vi điều khiển. Nó cũng hỗ trợ nhiều mô hình, như MCUNet, MobilenetV2, ResNet-50, BERT và DistilBERT. PockEngine dễ dàng hỗ trợ các mô hình đa dạng thông qua frontend của nó, chuyển đổi mạng nơ-ron được biểu diễn trong các định dạng khác nhau thành biểu diễn trung gian thống nhất.

Hơn nữa, quy trình dựa trên biên dịch cho phép chúng ta chọn backend runtime tốt nhất cho các tình huống huấn luyện khác nhau, bao gồm cả thư viện nhà cung cấp (ví dụ: SNPE cho GPU và DSP Snapdragon, TensorRT cho GPU NVIDIA) và kernel tùy chỉnh (ví dụ: kernel được điều chỉnh TVM cho CPU ARM và Apple M1). Chúng tôi trình bày so sánh các quy trình huấn luyện trong Hình 9 và thảo luận dưới đây:

CPU biên. Đối với các nền tảng như Raspberry Pi, PockEngine cung cấp hiệu suất tốt hơn 13 đến 21× so với các framework huấn luyện DNN phổ biến. Tăng tốc này là do điều chỉnh kernel, mà các framework hiện tại hoặc bỏ qua để ưu tiên triển khai kernel GPU (PyTorch, TensorFlow, Jax) hoặc chỉ tối ưu hóa cho pipeline suy luận và toán tử (MNN). Các kernel ARM tương ứng không cung cấp hiệu suất lý tưởng, chưa kể chi phí mà framework mang lại.

GPU biên. Chúng tôi benchmark các nền tảng GPU biên bằng NVIDIA Jetson Nano và Jetson AGX Orin do việc sử dụng rộng rãi của chúng trong các ứng dụng biên. GPU có mức độ song song cao hơn nhiều và thông lượng huấn luyện tốt hơn CPU. Tốc độ huấn luyện nhanh hơn của PockEngine (tăng tốc 2.2x đến 2.6×) chủ yếu là do quá trình biên dịch: Ngôn ngữ chủ Python thường chậm trên CPU tần số thấp, trong khi đồ thị được biên dịch của PockEngine có thể chạy mà không có ngôn ngữ chủ. Trong khi các framework khác như TensorRT [2] cũng có thể đạt được điều này, chúng bị giới hạn chỉ suy luận và không cung cấp hỗ trợ huấn luyện.

Chip Apple M. Chip Apple M1 là một nền tảng tương đối mới để huấn luyện. Trong khi PyTorch và Tensorflow có hỗ trợ GPU sơ bộ, tính tương thích không lý tưởng³. Ngay cả với bản build mới nhất (commit ID: c9913cf), PyTorch ném lỗi khi khởi chạy huấn luyện cho BERT và DistilBERT. Mặt khác, PockEngine biên dịch đồ thị huấn luyện sang Metal, cung cấp tính tương thích tốt hơn và tốc độ huấn luyện nhanh hơn.

DSP di động. Đối với DSP Qualcomm, chúng tôi tích hợp SNPE [48] để cung cấp các tệp nhị phân cuối cùng. Đáng chú ý rằng SNPE là thư viện thông thường chỉ suy luận cho các mô hình nguyên và PockEngine của chúng tôi dễ dàng mở rộng nó với khả năng huấn luyện. Như được hiển thị trong Hình 9 (g), hiệu suất đỉnh của DSP ấn tượng và thậm chí ngang bằng với GPU biên.

Vi điều khiển. Đối với nền tảng vi điều khiển, chúng tôi tích hợp TinyEngine [40] để thực hiện codegen và cho phép huấn luyện dưới ràng buộc bộ nhớ cực kỳ hạn chế. Các framework trước đó như TF-Lite-Micro [3] chỉ suy luận và chúng tôi báo cáo độ trễ dự kiến. Hiển thị trong Hình 7 (c), tốc độ thấp hơn nhiều so với PockEngine.

PockEngine cho phép huấn luyện hiệu quả trên thiết bị bằng biên dịch và thích ứng với các runtime khác nhau. Nó hỗ trợ thêm các lược đồ lan truyền ngược tiên tiến và tối ưu hóa đồ thị tiên tiến, mà chúng tôi sẽ mở rộng thêm trong phần sau.

5 TINH CHỈNH CHATBOT VỚI POCKENGINE
Với sự chú ý ngày càng tăng mà ChatGPT đã nhận được, nhu cầu tinh chỉnh mô hình Chatbot riêng của mình cũng đang tăng lên. Điều này cho phép người dùng điều chỉnh mô hình theo nhu cầu cụ thể của miền (ví dụ: luật, y sinh, chăm sóc sức khỏe) và đảm bảo quyền riêng tư (ví dụ: email riêng tư, trợ lý cá nhân) bằng cách không tải thông tin lên đám mây. Bằng cách tinh chỉnh mô hình ngôn ngữ của riêng mình, chúng ta có thể giải quyết những lo ngại này và có được mô hình ngôn ngữ chất lượng cao đáp ứng nhu cầu của chúng ta. Trong phần này, chúng tôi thể hiện cách PockEngine có thể tinh chỉnh hiệu quả chatbot trên nền tảng biên (Jetson AGX Orin).

Mô hình. Chúng tôi chọn LlamaV2 [55] của Meta và chọn mô hình 7B làm backbone cho các thí nghiệm của chúng tôi. Quyết định này dựa trên sự đánh đổi giữa chất lượng mô hình và tài nguyên thiết bị. Các thiết lập tinh chỉnh chi tiết được thảo luận dưới đây.

Đánh giá. Để đánh giá, chúng tôi theo Alpaca-Eval [36] và MT-Bench [62] để sử dụng LLM làm bộ đánh giá tự động cho việc tạo benchmark và đánh giá hiệu suất. Chất lượng của các câu trả lời được đánh giá dựa trên tính hữu ích, liên quan, chính xác và chi tiết từ 805 câu hỏi⁴ và 80 câu hỏi từ dự án Vicuna⁵. Đây là so sánh cặp đôi và chúng tôi chọn text-davinci-003 cho tỷ lệ thắng Alpaca-Eval (%) và ChatGPT-3.5-Turbo cho MT-Bench Score.

Tập dữ liệu. Để điều chỉnh các mô hình ngôn ngữ được huấn luyện trước với hướng dẫn, chúng tôi theo self-instruct [59] và thích ứng dữ liệu từ Stanford Alpaca [54]. Tập huấn luyện tổng cộng có 52K ví dụ chứa hướng dẫn và phản hồi đa dạng⁶.

Tinh chỉnh. Chúng tôi tinh chỉnh các mô hình trong 3 epoch sử dụng learning rate 10⁻⁴ và không có weight decay. Bộ tối ưu hóa chúng tôi sử dụng là Lion [14] tiết kiệm bộ nhớ, và độ dài câu tối đa được giới hạn ở 512. Kích thước lô tinh chỉnh hướng dẫn là 1, và gradient được tích lũy qua 16 bước. Chúng tôi cập nhật thưa thớt các bias của 5 khối cuối (trong tổng số 24) và trọng số của mô-đun attention và lớp tuyến tính đầu tiên trong FFN cho 5 khối cuối. Chúng tôi tiếp tục đóng băng các lớp layer-norm để giảm chi phí huấn luyện và tăng tốc huấn luyện.

5.1 So sánh định lượng
PockEngine tăng tốc huấn luyện. Như được hiển thị trong Bảng 5, PyTorch có thể huấn luyện trên Jetson AGX Orin, nhưng một iteration mất hơn 7 giây cho LlamaV2-7B. Tinh chỉnh trên 1000 bản ghi sẽ cần 2 giờ trong khi PockEngine tăng tốc huấn luyện 4.4x và có thể hoàn thành trong chưa đến nửa giờ.

Lan truyền ngược thưa thớt tăng tốc huấn luyện. Đối với các phương pháp tinh chỉnh hiệu quả tham số phổ biến như LoRA [24], mặc dù chúng có thể hiệu quả giảm dấu chân bộ nhớ (từ 45.1GB xuống 30.9GB), chi phí huấn luyện không được cải thiện đáng kể vì chúng vẫn cần lan truyền ngược đến lớp đầu tiên. Ngược lại, lan truyền ngược thưa thớt giảm độ sâu lan truyền ngược và cải thiện đáng kể tốc độ huấn luyện (từ 1768ms xuống 914ms, nhanh hơn 1.9×).

Sparse-BP đạt độ chính xác tương đương. Bên cạnh cải thiện thông lượng huấn luyện, lan truyền ngược thưa thớt cũng duy trì độ chính xác tinh chỉnh. Khi so sánh với full-BP, sparse-BP thể hiện hiệu suất tương tự, đạt điểm Alpaca-Eval ấn tượng là 43.7. Điểm này gần khớp với hiệu suất của full-BP, có điểm Alpaca-Eval là 44.1. Sparse-BP cũng hoạt động tốt khi so sánh với LoRA (điểm Alpaca-Eval 43.1).

5.2 So sánh định tính giữa Full-BP và Sparse-BP
Ngoài phân tích định lượng về độ chính xác và thông lượng, chúng tôi cũng tiến hành phân tích định tính để so sánh chất lượng của sparse-BP và full-BP khi thực hiện tinh chỉnh hướng dẫn trên LlamaV2-7B. Chúng tôi đưa ra giả định rằng kiến thức và khả năng của mô hình được học chủ yếu trong quá trình huấn luyện trước, trong khi điều chỉnh dạy LLM cách tương tác với người dùng [63]. Do đó, chúng tôi kiểm tra chất lượng phản hồi từ nhiều góc độ:

Nhớ lại sự thật. Để đánh giá kỹ lưỡng hiệu suất giữa lan truyền ngược đầy đủ và thưa thớt, chúng tôi tiến hành phân tích chi tiết về khả năng thành thạo của các mô hình tương ứng để trả lời các câu hỏi liên quan đến kiến thức chung. Đầu tiên chúng tôi kiểm tra khả năng xác định chính xác thủ đô của một quốc gia nhất định. Tất cả các mô hình nhất quán cung cấp phản hồi chính xác trong khi các mô hình được điều chỉnh không chỉ cung cấp câu trả lời chính xác mà còn cung cấp thông tin toàn diện hơn, làm phong phú trải nghiệm người dùng tổng thể.

Đầu vào: Thủ đô của Hoa Kỳ là gì?
LlamaV2-7B-gốc: Washington, D.C.
LlamaV2-7B-điều chỉnh (Full-BP): Thủ đô của Hoa Kỳ là Washington D.C. (District of Columbia).
LlamaV2-7B-điều chỉnh (Sparse-BP): Thủ đô của Hoa Kỳ là Washington, D.C.

Ví dụ 1. Thủ đô của Hoa Kỳ.

Tương tự, khi chúng tôi hỏi về ngày của các sự kiện đáng chú ý, tất cả các mô hình thể hiện mức độ tự tin và chính xác đáng kể trong phản hồi của chúng. Điều này cho thấy độ tin cậy và tính chính xác của cả lan truyền ngược đầy đủ và thưa thớt trong quá trình tinh chỉnh.

Đầu vào: Thế vận hội Bắc Kinh được tổ chức vào năm nào?
LlamaV2-7B-gốc: Thế vận hội Bắc Kinh được tổ chức vào năm 2008.
LlamaV2-7B-điều chỉnh (Full-BP): Thế vận hội Bắc Kinh, còn được gọi là Thế vận hội XXIX, được tổ chức vào năm 2008. Lễ khai mạc diễn ra vào ngày 8 tháng 8 năm 2008, và lễ bế mạc được tổ chức vào ngày 24 tháng 8 năm 2008.
LlamaV2-7B-điều chỉnh (Sparse-BP): Thế vận hội Bắc Kinh, còn được gọi là Thế vận hội mùa hè 2008, được tổ chức tại Bắc Kinh, Trung Quốc vào năm 2008.

Ví dụ 2. Ngày của Thế vận hội Bắc Kinh.

Tuy nhiên, khi câu hỏi trở nên mơ hồ, mô hình Llama gốc không còn cung cấp câu trả lời chính xác. Ví dụ, mô hình Llama thô (không tinh chỉnh) sai khi nói Michael Jackson chết năm 2020 (năm thực tế là 2009), và nó cũng cung cấp tên album sai (tiêu đề đúng là "Invincible"). Trong khi đó, mô hình Full-BP và mô hình Sparse-BP đều giải thích đúng câu hỏi và cung cấp phản hồi chính xác.

Đầu vào: Album cuối cùng của Michael Jackson là gì?
LlamaV2-7B-gốc: Michael Jackson qua đời vào ngày 25 tháng 6 năm 2020, vì vậy anh ấy không phát hành album nào sau đó. Tuy nhiên, trước khi qua đời, anh ấy đã phát hành album "Michael Jackson: The Ultimate Collection," là một tuyển tập các album phòng thu trước đó và các track bonus.
LlamaV2-7B-điều chỉnh (Full-BP): Album cuối cùng của Michael Jackson là 'Invincible', phát hành năm 2001.
LlamaV2-7B-điều chỉnh (Sparse-BP): Album cuối cùng của Michael Jackson, được phát hành trong cuộc đời của anh ấy, là "Invincible" được phát hành năm 2001.

Ví dụ 3. Tiêu đề album cuối cùng của Michael Jackson

Hoạt động và suy luận logic. Tiếp theo, chúng tôi đánh giá một tác vụ đơn giản nhưng thú vị: đảo ngược thứ tự của từ. Tác vụ này yêu cầu LLM hiểu khái niệm đảo ngược và phân đoạn câu một cách đúng đắn. Mô hình gốc hiểu sai tác vụ và đảo ngược ý nghĩa của từ, dẫn đến câu trả lời sai. Trong khi đó, các mô hình Full-BP và sparse-BP, được tinh chỉnh, đều hiểu đúng thuật ngữ "đảo ngược" và thực hiện tác vụ đã cho, mặc dù mô hình sparse-BP đã mắc một lỗi nhỏ và vẫn đặt sai chỗ từ "the".

Đầu vào: Vui lòng đảo ngược từ trong câu "I love the Micro conference"
LlamaV2-7B-gốc: I hate the conference Micro.
LlamaV2-7B-điều chỉnh (Full-BP): Kết quả đảo ngược là "Conference Micro the love I"
LlamaV2-7B-điều chỉnh (Sparse-BP): Câu đảo ngược sẽ là "Conference the Micro love I".

Ví dụ 4. Đảo ngược một câu.

Sau khi so sánh chất lượng phản hồi của việc thực hiện các tác vụ đã cho, chúng tôi sau đó đánh giá khả năng suy luận logic của các mô hình được tinh chỉnh. Đầu tiên chúng tôi giới thiệu ngắn gọn về địa điểm tổ chức hội nghị Micro trong ba năm qua và hỏi hội nghị được tổ chức ở đâu năm ngoái. Không có tinh chỉnh, mô hình Llama gốc không hiểu được câu hỏi và chỉ đơn giản lặp lại câu đầu tiên. Tuy nhiên, đối với các mô hình được tinh chỉnh, cả Full-BP và Sparse-BP đều có thể đặt tên đúng địa điểm, với phản hồi từ Full-BP tự tin hơn.

Đầu vào: Micro chạy ảo trực tuyến năm 2021, và được tổ chức thực tế tại Chicago năm 2022 và sẽ sớm được tổ chức tại Toronto năm 2023. Bây giờ là năm 2023, Micro được tổ chức ở đâu năm ngoái?
LlamaV2-7B-gốc: Micro được tổ chức ảo năm 2021
LlamaV2-7B-điều chỉnh (Full-BP): Micro được tổ chức thực tế tại Chicago năm 2022.
LlamaV2-7B-điều chỉnh (Sparse-BP): Dựa trên thông tin được cung cấp, có vẻ như Micro được tổ chức thực tế tại Chicago năm 2022.

Ví dụ 5. Suy luận thông tin dựa trên ngữ cảnh.

Toán học. Sau đó chúng tôi so sánh khả năng toán học của các mô hình khác nhau. Toán học thường được coi là một thách thức đáng kể đối với nhiều mô hình ngôn ngữ, và rõ ràng, mô hình LlamaV2 gốc gặp khó khăn với các câu hỏi liên quan đến toán học, trong khi các mô hình Full-BP và Sparse-BP thể hiện khả năng nhất định để lý luận và tính toán đúng kết quả.

Đầu vào: Lee cắt một bãi cỏ và tính phí $33. Tuần trước anh ấy cắt 16 bãi cỏ và ba khách hàng mỗi người đã cho anh ấy tiền tip $10. Lee kiếm được bao nhiều đô la khi cắt cỏ tuần trước?
LlamaV2-7B-gốc: Lee kiếm được $660 từ việc cắt cỏ tuần trước.
LlamaV2-7B-điều chỉnh (Full-BP): Lee kiếm được $33 x 16 = $528 từ việc cắt cỏ. Tổng thu nhập của anh ấy từ 16 khách hàng đã cho anh ấy tiền tip $10 là $10 x 3 = $30. Do đó, Lee kiếm được tổng cộng $528 + $30 = $558 tuần trước khi cắt cỏ.
LlamaV2-7B-điều chỉnh (Sparse-BP): Để xác định Lee kiếm được bao nhiều tiền khi cắt cỏ tuần trước, chúng ta cần tính tổng số tiền anh ấy kiếm được từ việc cắt cỏ và tiền tip. Lee cắt một bãi cỏ và tính phí $33, vì vậy anh ấy kiếm được $33 từ bãi cỏ đó. Tuần trước, Lee cắt 16 bãi cỏ, và ba khách hàng mỗi người đã cho anh ấy tiền tip $10, vì vậy anh ấy kiếm được $10 x 3 = $30 tiền tip từ những bãi cỏ đó và kiếm được $33 từ việc cắt một bãi cỏ và tổng cộng 16 bãi cỏ = $528. Do đó, Lee kiếm được $558 tuần trước từ việc cắt cỏ và nhận tiền tip.

Ví dụ 6. Giải quyết bài toán

Lưu ý rằng đây là một nghiên cứu định tính ngắn gọn so sánh LLM gốc, được tinh chỉnh Full-BP và được tinh chỉnh Sparse-BP. Chúng tôi đã cẩn thận chọn các mẫu đại diện cho nghiên cứu này, mặc dù quan trọng cần lưu ý rằng nó không toàn diện cho phạm vi rộng lớn của phản hồi mà mô hình có thể cung cấp. Mục tiêu của phân tích này là trình bày bằng chứng thuyết phục hỗ trợ hai phát hiện: (1) tinh chỉnh là một quá trình thiết yếu để cá nhân hóa Chatbot của riêng bạn, và (2) Sparse-BP có khả năng tinh chỉnh mô hình với chất lượng tương đương với chi phí giảm đáng kể.

6 KẾT LUẬN
Chúng tôi trình bày PockEngine, một framework huấn luyện hiệu quả cho việc học trên biên. PockEngine có hỗ trợ chung cho các frontend/backend khác nhau để xử lý tính không đồng nhất của phần cứng trên biên. Nó cải thiện hiệu quả của huấn luyện trên thiết bị thông qua (1) phân tích tự động dựa trên biên dịch để giảm tải chi phí từ runtime sang thời gian biên dịch; (2) hỗ trợ lan truyền ngược thưa thớt với cắt tỉa đồ thị ngược; (3) tối ưu hóa đồ thị huấn luyện bao gồm sắp xếp lại/fusion toán tử và các biến đổi bảo toàn chức năng khác nhau.

Thí nghiệm trên các thiết bị biên khác nhau cho thấy PockEngine có thể tăng tốc đáng kể huấn luyện trên thiết bị: 11.2× trên CPU ARM, 2× trên Apple M1, và 2.7× trên GPU NVIDIA biên, và 9.6× trên vi điều khiển so với TensorFlow. PockEngine hỗ trợ lan truyền ngược thưa thớt, tăng tốc thêm 1.5 - 3.5× trong khi khớp với độ chính xác của lan truyền ngược đầy đủ. Hơn nữa, PockEngine cho phép tinh chỉnh mô hình ngôn ngữ LLamaV2-7B trên Jetson AGX Orin ở 914ms, nhanh hơn 7.9× so với baseline PyTorch. Chúng tôi hy vọng thiết kế công cụ của chúng tôi có thể tạo điều kiện cho các ứng dụng AI với khả năng cá nhân hóa và học tập suốt đời bằng cách dân chủ hóa việc học trên biên.

LỜI CẢM ơn
Công việc này được hỗ trợ bởi MIT-IBM Watson AI Lab, MIT AI Hardware Program, MIT-Amazon Science Hub và NSF. Ligeng Zhu và Ji Lin được hỗ trợ một phần bởi Qualcomm Innovation Fellowship.

TÀI LIỆU THAM KHẢO
[1][n. d.]. NCNN: Một framework tính toán suy luận mạng nơ-ron hiệu suất cao được tối ưu hóa cho nền tảng di động. https://github.com/Tencent/ncnn.
[2][n. d.]. NVIDIA TensorRT, một SDK cho suy luận học sâu hiệu suất cao. https://developer.nvidia.com/tensorrt.
[3][n. d.]. TensorFlow Lite Micro.
[4]Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, và Xiaoqiang Zheng. 2015. TensorFlow: Học máy quy mô lớn trên hệ thống không đồng nhất. https://www.tensorflow.org/ Phần mềm có sẵn từ tensorflow.org.
[5]Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. Tensorflow: Một hệ thống cho học máy quy mô lớn. Trong OSDI.
[6]Byung Hoon Ahn, Jinwon Lee, Jamie Menjay Lin, Hsin-Pai Cheng, Jilei Hou, và Hadi Esmaeilzadeh. 2020. Sắp xếp hỗn loạn: Lập lịch nhận thức bộ nhớ của mạng nơ-ron được kết nối bất thường cho thiết bị biên. arXiv preprint arXiv:2003.02369 (2020).
[7]Colby R Banbury, Vijay Janapa Reddi, Max Lam, William Fu, Amin Fazel, Jeremy Holleman, Xinyuan Huang, Robert Hurtado, David Kanter, Anton Lokhmotov, et al. 2020. Benchmarking hệ thống TinyML: Thách thức và hướng đi. arXiv preprint arXiv:2003.04821 (2020).
[8]Lukas Bossard, Matthieu Guillaumin, và Luc Van Gool. 2014. Food-101–khai thác các thành phần phân biệt với rừng ngẫu nhiên. Trong Hội nghị châu Âu về thị giác máy tính. Springer, 446–461.
[9]James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, và Qiao Zhang. 2018. JAX: các biến đổi có thể kết hợp của chương trình Python+NumPy. http://github.com/google/jax
[10]Han Cai, Chuang Gan, Ligeng Zhu, và Song Han. 2020. TinyTL: Giảm Activation, không phải tham số có thể huấn luyện cho học hiệu quả trên thiết bị. arXiv preprint arXiv:2007.11622 (2020).
[11]Ken Chatfield, Karen Simonyan, Andrea Vedaldi, và Andrew Zisserman. 2014. Sự trở lại của quỷ trong chi tiết: Đào sâu vào mạng tích chập. Trong BMVC.
[12]Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, và Zheng Zhang. 2015. Mxnet: Một thư viện học máy linh hoạt và hiệu quả cho hệ thống phân tán không đồng nhất. arXiv preprint arXiv:1512.01274 (2015).
[13]Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: Một trình biên dịch tối ưu hóa end-to-end tự động cho học sâu. Trong OSDI.
[14]Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, và Quoc V. Le. 2023. Khám phá tượng trưng của thuật toán tối ưu hóa. arXiv:2302.06675 [cs.LG]
[15]Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. 2023. Vicuna: Một chatbot mã nguồn mở gây ấn tượng GPT-4 với 90%* chất lượng ChatGPT. https://vicuna.lmsys.org
[16]Aakanksha Chowdhery, Pete Warden, Jonathon Shlens, Andrew Howard, và Rocky Rhodes. 2019. Tập dữ liệu từ thức tỉnh thị giác. arXiv preprint arXiv:1906.05721 (2019).
[17]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. 2009. ImageNet: Một cơ sở dữ liệu hình ảnh phân cấp quy mô lớn. Trong CVPR.
[18]Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2018. Bert: Huấn luyện trước transformer hai chiều sâu cho hiểu ngôn ngữ. arXiv preprint arXiv:1810.04805 (2018).
[19]Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, và Trevor Darrell. 2014. Decaf: Một đặc trưng kích hoạt tích chập sâu cho nhận dạng thị giác chung. Trong ICML.
[20]Jonathan Frankle, David J Schwab, và Ari S Morcos. 2020. Huấn luyện BatchNorm và chỉ BatchNorm: Về sức mạnh biểu đạt của đặc trưng ngẫu nhiên trong CNN. arXiv preprint arXiv:2003.00152 (2020).
[21]Chuang Gan, Naiyan Wang, Yi Yang, Dit-Yan Yeung, và Alex G Hauptmann. 2015. Devnet: Một mạng sự kiện sâu để phát hiện sự kiện đa phương tiện và kể lại bằng chứng. Trong CVPR. 2568–2577.
[22]Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. 2016. Học residual sâu cho nhận dạng hình ảnh. Trong CVPR.
[23]Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. 2019. Học chuyển giao hiệu quả tham số cho NLP. Trong Hội nghị quốc tế về học máy. PMLR, 2790–2799.
[24]Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, và Weizhu Chen. 2021. LoRA: Thích ứng rank thấp của mô hình ngôn ngữ lớn. arXiv:2106.09685 [cs.CL]
[25]Junsu Jang và Fadel Adib. 2019. Mạng backscatter dưới nước. Trong Kỷ yếu nhóm quan tâm đặc biệt ACM về truyền thông dữ liệu. 187–199.
[26]Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, và Alex Aiken. 2019. TASO: tối ưu hóa tính toán học sâu với tạo tự động thay thế đồ thị. Trong Kỷ yếu Hội nghị chuyên đề ACM lần thứ 27 về nguyên lý hệ điều hành. 47–62.
[27]Zhihao Jia, James Thomas, Todd Warszawski, Mingyu Gao, Matei Zaharia, và Alex Aiken. 2019. Tối ưu hóa tính toán DNN với thay thế đồ thị nới lỏng. Kỷ yếu hệ thống học máy 1 (2019), 27–39.
[28]Zhihao Jia, James Thomas, Todd Warszawski, Mingyu Gao, Matei Zaharia, và Alex Aiken. 2019. Tối ưu hóa tính toán DNN với thay thế đồ thị nới lỏng. Kỷ yếu hệ thống học máy 1 (2019), 27–39.
[29]Xiaotang Jiang, Huan Wang, Yiliu Chen, Ziqi Wu, Lichuan Wang, Bin Zou, Yafeng Yang, Zongyang Cui, Yu Cai, Tianhang Yu, et al. 2020. MNN: Một công cụ suy luận phổ quát và hiệu quả. arXiv preprint arXiv:2002.12418 (2020).
[30]Diederik P Kingma và Jimmy Ba. 2014. Adam: Một phương pháp cho tối ưu hóa ngẫu nhiên. arXiv preprint arXiv:1412.6980 (2014).
[31]Jonathan Krause, Michael Stark, Jia Deng, và Li Fei-Fei. 2013. Biểu diễn đối tượng 3d cho phân loại tinh. Trong Kỷ yếu hội thảo hội nghị quốc tế IEEE về thị giác máy tính. 554–561.
[32]Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Học nhiều lớp đặc trưng từ hình ảnh nhỏ. (2009).
[33]Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, và Percy Liang. 2022. Tinh chỉnh có thể làm biến dạng đặc trưng được huấn luyện trước và hoạt động kém ngoài phân phối. arXiv preprint arXiv:2202.10054 (2022).
[34]Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, và Chelsea Finn. 2022. Tinh chỉnh phẫu thuật cải thiện thích ứng với dịch chuyển phân phối. arXiv preprint arXiv:2210.11466 (2022).
[35]Philip Levis, Neil Patel, David Culler, và Scott Shenker. 2004. Trickle: Một thuật toán tự điều chỉnh cho lan truyền và bảo trì mã trong mạng cảm biến không dây. Trong Proc. của Hội nghị chuyên đề USENIX/ACM lần thứ 1 về thiết kế và triển khai hệ thống mạng, Tập. 25. 37–52.
[36]Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023. AlpacaEval: Một bộ đánh giá tự động của mô hình theo hướng dẫn. https://github.com/tatsu-lab/alpaca_eval.
[37]Xiang Lisa Li và Percy Liang. 2021. Prefix-tuning: Tối ưu hóa prompt liên tục cho tạo. arXiv preprint arXiv:2101.00190 (2021).
[38]Edgar Liberis và Nicholas D Lane. 2019. Mạng nơ-ron trên vi điều khiển: tiết kiệm bộ nhớ tại suy luận thông qua sắp xếp lại toán tử. arXiv preprint arXiv:1910.05110 (2019).
[39]Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, và Song Han. 2021. Mcunetv2: Suy luận dựa trên patch tiết kiệm bộ nhớ cho học sâu nhỏ. arXiv preprint arXiv:2110.15352 (2021).
[40]Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, và Song Han. 2020. Mcunet: Học sâu nhỏ trên thiết bị iot. Trong NeurIPS.
[41]Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, và Song Han. 2022. Huấn luyện trên thiết bị dưới 256KB bộ nhớ. Trong NeurIPS.
[42]Pramod Kaushik Mudrakarta, Mark Sandler, Andrey Zhmoginov, và Andrew Howard. 2018. K với giá của 1: Học đa tác vụ và chuyển giao hiệu quả tham số. arXiv preprint arXiv:1810.10703 (2018).
[43]Pramod Kaushik Mudrakarta, Mark Sandler, Andrey Zhmoginov, và Andrew Howard. 2019. K với giá của 1: Học đa tác vụ và chuyển giao hiệu quả tham số. Trong ICLR.
[44]Maria-Elena Nilsback và Andrew Zisserman. 2008. Phân loại hoa tự động trên một số lượng lớn lớp. Trong Hội nghị Ấn Độ lần thứ sáu 2008 về thị giác máy tính, đồ họa và xử lý hình ảnh. IEEE, 722–729.
[45]Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, và CV Jawahar. 2012. Mèo và chó. Trong Hội nghị IEEE 2012 về thị giác máy tính và nhận dạng mẫu. IEEE, 3498–3505.
[46]Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: Một thư viện học sâu hiệu suất cao, phong cách mệnh lệnh. Tiến bộ trong hệ thống xử lý thông tin nơ-ron 32 (2019).
[47]Shishir G Patil, Paras Jain, Prabal Dutta, Ion Stoica, và Joseph Gonzalez. 2022. POET: Huấn luyện mạng nơ-ron trên thiết bị nhỏ với tái vật chất hóa tích hợp và phân trang. Trong Hội nghị quốc tế về học máy. PMLR, 17573–17583.
[48]Qualcomm. [n. d.]. Snapdragon Neural Processing Engine SDK. https://developer.qualcomm.com/sites/default/files/docs/snpe/overview.html.
[49]Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Cải thiện hiểu ngôn ngữ bằng huấn luyện trước tạo. (2018).
[50]Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, và Liang-Chieh Chen. 2018. MobileNetV2: Residual đảo ngược và nút thắt tuyến tính. Trong CVPR.
[51]Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. 2019. DistilBERT, một phiên bản chưng cất của BERT: nhỏ hơn, nhanh hơn, rẻ hơn và nhẹ hơn. arXiv preprint arXiv:1910.01108 (2019).
[52]Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, và Stefan Carlsson. 2014. Đặc trưng CNN off-the-shelf: một baseline đáng kinh ngạc cho nhận dạng. Trong CVPR Workshops.
[53]Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, và Moritz Hardt. 2020. Huấn luyện thời gian thử nghiệm với tự giám sát cho tổng quát hóa dưới dịch chuyển phân phối. Trong Hội nghị quốc tế về học máy. PMLR, 9229–9248.
[54]Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023. Stanford Alpaca: Một mô hình LLaMA theo hướng dẫn. https://github.com/tatsu-lab/stanford_alpaca.
[55]Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Mô hình nền tảng mở và chat được tinh chỉnh. arXiv preprint arXiv:2307.09288 (2023).
[56]Deepak Vasisht, Zerina Kapetanovic, Jongho Won, Xinxin Jin, Ranveer Chandra, Sudipta Sinha, Ashish Kapoor, Madhusudhan Sudarshan, và Sean Stratman. 2017. {FarmBeats}: Một nền tảng {IoT} cho nông nghiệp {dựa trên dữ liệu}. Trong Hội nghị chuyên đề USENIX lần thứ 14 về thiết kế và triển khai hệ thống mạng (NSDI 17). 515–529.
[57]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, và Illia Polosukhin. 2017. Attention là tất cả những gì bạn cần. arXiv preprint arXiv:1706.03762 (2017).
[58]Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R Bowman. 2018. GLUE: Một benchmark đa tác vụ và nền tảng phân tích cho hiểu ngôn ngữ tự nhiên. arXiv preprint arXiv:1804.07461 (2018).
[59]Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, và Hannaneh Hajishirzi. 2022. Self-Instruct: Điều chỉnh mô hình ngôn ngữ với hướng dẫn tự tạo. arXiv:2212.10560 [cs.CL]
[60]Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, và Pietro Perona. 2010. Caltech-UCSD Birds 200. Báo cáo kỹ thuật CNS-TR-201. Caltech. /se3/wp-content/uploads/2014/09/WelinderEtal10_CUB-200.pdf,http://www.vision.caltech.edu/visipedia/CUB-200.html
[61]Elad Ben Zaken, Shauli Ravfogel, và Yoav Goldberg. 2021. BitFit: Tinh chỉnh hiệu quả tham số đơn giản cho mô hình ngôn ngữ che mặt dựa trên transformer. CoRR abs/2106.10199 (2021). arXiv:2106.10199 https://arxiv.org/abs/2106.10199
[62]Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica. 2023. Đánh giá LLM-as-a-judge với MT-Bench và Chatbot Arena. arXiv:2306.05685 [cs.CL]
[63]Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, và Omer Levy. 2023. LIMA: Ít hơn là nhiều hơn cho điều chỉnh. arXiv:2305.11206 [cs.CL]
[64]Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, và Sanja Fidler. 2015. Điều chỉnh sách và phim: Hướng tới giải thích thị giác giống như câu chuyện bằng cách xem phim và đọc sách. Trong Kỷ yếu hội nghị quốc tế IEEE về thị giác máy tính. 19–27.
[65]Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, và Sanja Fidler. 2015. Điều chỉnh sách và phim: Hướng tới giải thích thị giác giống như câu chuyện bằng cách xem phim và đọc sách. Trong Hội nghị quốc tế IEEE về thị giác máy tính (ICCV).
