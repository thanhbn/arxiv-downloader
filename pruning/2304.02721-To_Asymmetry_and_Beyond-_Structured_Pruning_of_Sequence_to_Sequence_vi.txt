# 2304.02721.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2304.02721.pdf
# Kích thước tệp: 339736 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Đến Bất đối xứng và Xa hơn: Cắt tỉa có cấu trúc của các mô hình Sequence to Sequence
để cải thiện hiệu quả suy luận∗
Daniel Campos1,2và ChengXiang Zhai1
1Khoa Khoa học Máy tính, Đại học Illinois Urbana-Champaign
2Neeva Inc.
Tóm tắt
Các mô hình ngôn ngữ sequence-to-sequence có thể
được sử dụng để tạo ra các bản tóm tắt trừu tượng
mạch lạc, liên quan và súc tích. Tuy nhiên, kích thước
mô hình có thể khiến việc triển khai trong các ứng dụng
nhạy cảm với độ trễ hoặc quy mô web trở nên khó khăn.
Bài báo này nghiên cứu mối quan hệ giữa kích thước
mô hình, cắt tỉa có cấu trúc, hiệu quả suy luận và độ
chính xác tóm tắt trên các bộ dữ liệu tóm tắt được sử
dụng rộng rãi. Chúng tôi chỉ ra rằng độ chính xác của
mô hình gắn liền với kích thước encoder trong khi hiệu
quả suy luận được kết nối với decoder. Sử dụng cắt tỉa
bất đối xứng có thể dẫn đến cải thiện gần 3 lần trong
độ trễ suy luận với mất 1 điểm trong Rouge-2. Hơn nữa,
chúng tôi thấy rằng cả sự suy giảm trung bình và vai trò
của bất đối xứng đều nhất quán qua các kích thước mô
hình và biến thể trong bộ dữ liệu. Chúng tôi công bố
mã nguồn1, chế độ huấn luyện và mô hình liên quan2
để khuyến khích việc sử dụng và thử nghiệm rộng rãi.

1 Giới thiệu
Việc ứng dụng các mô hình ngôn ngữ sequence-to-
sequence đã trở thành một công cụ quan trọng cho
các nhiệm vụ xử lý ngôn ngữ tự nhiên như dịch máy
(Sutskever et al., 2014), phiên âm âm thanh (Radford
et al., 2022), và tóm tắt trừu tượng (Raffel et al., 2020).
Các mô hình sequence-to-sequence hiệu quả biến đổi
mỗi nhiệm vụ được đề cập ở trên thành các vấn đề hai
bước: trích xuất và sinh ra, và điều kiện hóa mạnh mẽ
việc sinh ra dựa trên đầu vào.

Bên cạnh việc đảm bảo các phản hồi đúng chủ đề, các
mô hình sequence to sequence có lợi ích bổ sung là có
thể ánh xạ đầu vào thành mục tiêu với độ dài khác nhau

∗Tác giả liên hệ: dcampos3@illinois.edu
1https://github.com/spacemanidol/Efficient-Web-Scale-
Absractive-Summarization
2https://huggingface.co/spacemanidol

[Hình 1: Tác động của Cắt tỉa Bất đối xứng lên tốc độ tăng suy luận và suy giảm ROUGE-2 trên Tóm tắt Web Độc lập với Truy vấn. Thời gian Suy luận là thời gian suy luận trung bình cho kích thước batch 1 trên GPU A10 qua bảy lần lặp.]

và các phương thức theo cách mà các hệ thống chỉ có
encoder hoặc decoder không thể.

Khi được sử dụng cho tóm tắt trừu tượng, mô hình
sequence-to-sequence có hai bước, trích xuất sử dụng
encoder và sinh ra sử dụng decoder, thường liên quan
đến việc thực thi lặp lại cho đến khi một token kết thúc
chuỗi được phát ra. Vì encoder chạy một lần trên đầu
vào (Sutskever et al., 2014) nên chi phí thực thi của nó
tỷ lệ với kích thước batch. Chi phí thực thi decoder có
thể rất thay đổi dựa trên độ dài sinh ra (Tay et al., 2021).
Mặc dù có nghiên cứu rộng rãi về các mô hình sequence-
to-sequence (Raffel et al., 2020) và cách chúng nén
(Li et al., 2022), vai trò của sự đối xứng mô hình áp
dụng cho hiệu quả suy luận và độ chính xác mô hình
vẫn còn thiếu.

--- TRANG 2 ---
Những tiến bộ gần đây trong việc mở rộng quy mô các
mô hình ngôn ngữ đã dẫn đến một nghiên cứu rộng rãi
về các định luật mở rộng quy mô áp dụng cho hiệu suất
mô hình ngôn ngữ (Kaplan et al., 2020), kích thước dữ
liệu huấn luyện (Hoffmann et al., 2022), dịch máy
(Henighan et al., 2020), và thậm chí học tăng cường
(Neumann and Gros, 2022).

Chúng tôi xây dựng trên công trình này và nghiên cứu
tác động của việc mở rộng quy mô đối với tóm tắt trừu
tượng và vai trò của sự bất đối xứng mô hình trong đó.
Sự bất đối xứng này có thể biểu hiện theo nhiều cách
khác nhau, chẳng hạn như số lượng lớp và đơn vị ẩn
trong encoder và decoder và loại cơ chế attention được
sử dụng.

Trong bài báo này, chúng tôi khám phá vai trò của sự
bất đối xứng trong số lượng lớp trong mô hình ngôn
ngữ encoder-decoder cho tóm tắt và tác động của nó
đối với hiệu suất của các mô hình này. Như được hiển
thị trong Hình 1, tính đối xứng của việc cắt tỉa thúc đẩy
tác động đến độ chính xác và tốc độ tăng suy luận cho
các mô hình sequence-to-sequence.

Các câu hỏi nghiên cứu sau thúc đẩy công việc của
chúng tôi:
• Những định luật mở rộng quy mô nào có thể được
quan sát trong tóm tắt trừu tượng?
• Sự bất đối xứng encoder-decoder có tác động gì
đến độ chính xác tóm tắt trừu tượng?
• Sự bất đối xứng encoder-decoder có tác động gì
đến hiệu quả suy luận tóm tắt trừu tượng?
• Tác động của sự bất đối xứng đối với độ chính xác
và hiệu quả suy luận có quy mô nào trong các mô
hình encoder-decoder cho tóm tắt trừu tượng?

Chính trong việc trả lời những câu hỏi này mà chúng
tôi đưa ra những đóng góp sau:
• Chúng tôi trình bày nghiên cứu mạnh mẽ đầu tiên
về các định luật mở rộng quy mô áp dụng cho việc
nén mô hình sequence-to-sequence.
• Chúng tôi chứng minh rằng chi phí suy luận bất
đối xứng của các mô hình sequence-to-sequence
dẫn đến cắt tỉa bất đối xứng để nén hiệu quả suy
luận tối ưu.
• Chúng tôi chứng minh thực nghiệm trên nhiều
benchmark đa dạng cách Nén Bất đối xứng có thể
dẫn đến tăng tốc suy luận 2.7 lần mà không mất
độ chính xác trên bộ dữ liệu XSUM.

2 Công trình liên quan
Các Mô hình Ngôn ngữ dựa trên Transformer như
BERT (Devlin et al., 2019) và T5 (Raffel et al., 2020)
cung cấp các biểu diễn ngôn ngữ theo ngữ cảnh được
xây dựng trên kiến trúc Transformer (Vaswani et al.,
2017) có thể được chuyên môn hóa và thích ứng cho
các nhiệm vụ và lĩnh vực cụ thể (Lee et al., 2020).
Sử dụng các mô hình này, việc xuất sắc trong một
loạt rộng các nhiệm vụ xử lý ngôn ngữ tự nhiên như
trả lời câu hỏi, phân loại văn bản và phân tích cảm
xúc trở nên tương đối dễ dàng.

Các Định luật Mở rộng Quy mô đã trở thành một lĩnh
vực nghiên cứu ngày càng quan trọng khi kích thước
mô hình và dữ liệu huấn luyện tăng lên. Hiệu suất của
mô hình ngôn ngữ dựa trên transformer cải thiện theo
mối quan hệ với kích thước mô hình (Radford, 2018)
và các mô hình lớn hơn vượt trội hơn các mô hình nhỏ
hơn (Brown et al., 2020) trong hầu hết các nhiệm vụ
NLP. Tăng kích thước tập dữ liệu huấn luyện có thể
dẫn đến cải thiện lớn về hiệu suất, và kích thước mô
hình có thể có kích thước dữ liệu huấn luyện tối ưu
(Hoffmann et al., 2022). Li et al. (2020) khám phá mối
quan hệ giữa kích thước mô hình và hiệu quả huấn
luyện, phát hiện ra rằng các mô hình lớn hơn huấn
luyện nhanh hơn và mạnh mẽ hơn đối với cắt tỉa và
lượng tử hóa (Na et al., 2022).

Bất đối xứng trong các mô hình sequence-to-sequence
nói chung đề cập đến sự không đồng nhất giữa hình
dạng hoặc thuộc tính mô hình encoder và decoder. Các
quy trình huấn luyện và suy luận nên khớp càng gần
càng tốt (Ranzato et al., 2015) (Mihaylova and Martins,
2019) vì những cải thiện trong loss huấn luyện trong
quá trình tối ưu hóa dẫn đến cải thiện trong hiệu suất
mô hình trong Suy luận. Mặc dù điều này có thể dẫn
đến hiệu suất mô hình tốt nhất, nó bỏ qua chi phí suy
luận thay đổi của các mô hình sequence-to-sequence.

Trong Suy luận, độ trễ bị chi phối bởi việc thực thi bất
đối xứng của mô hình ngôn ngữ. Encoder tự mã hóa
thực thi một lần trên toàn bộ chuỗi đầu vào, trong khi
decoder tự hồi quy thực thi lặp đi lặp lại cho đến khi
một token kết thúc chuỗi được tạo ra.

Kasai et al. đã chứng minh cách hiệu suất mô hình
ngôn ngữ sequence-to-sequence cho dịch máy bị chi
phối bởi độ sâu encoder (Kasai et al., 2020). Tay et al.
2021 mở rộng công việc này bằng cách tìm ra
DeepNarrow cho thấy rằng đối với mô hình ngôn ngữ
rộng, có thể có ít hơn 50% tham số và suy luận nhanh
hơn 40% mà không mất độ chính xác (Tay et al., 2021).

Suy luận Hiệu quả cho mô hình ngôn ngữ là một lĩnh
vực nghiên cứu đang phát triển tập trung rộng rãi vào
việc giảm chi phí suy luận mà không mất độ chính xác.

Cắt tỉa Không có cấu trúc đã được nghiên cứu rộng rãi
(Han et al., 2015) (Sanh et al., 2020) (Kurti ´c et al.,
2022) (Zafrir et al., 2021) (Campos et al., 2022) nhưng
việc thực hiện tăng tốc có thể khó khăn.

Cắt tỉa Có cấu trúc loại bỏ các thành phần cấu trúc cơ
bản trong mô hình ngôn ngữ như các đầu attention
riêng lẻ (V oita et al., 2019) hoặc toàn bộ các lớp mô
hình như các encoder transformer (Sanh et al., 2019).
Rosenfeld et al. 2020 chứng minh rằng tác động cắt
tỉa không có cấu trúc tuân theo các định luật mở rộng
quy mô (Rosenfeld et al., 2020) nơi các mô hình lớn
hơn có thể được cắt tỉa dễ dàng hơn.

Nén Sequence-to-sequence là một lĩnh vực nghiên cứu
đang phát triển nơi các phương pháp từ Suy luận hiệu
quả thông thường đã cho thấy một số khả năng chuyển
giao. Shleifer et al. cho thấy rằng có thể đạt được tăng
tốc 1.93 lần trên mô hình tóm tắt BART bằng cách áp
dụng cắt tỉa cấu trúc (Shleifer and Rush, 2020) nhưng
thấy rằng các phương pháp nén khác nhau trong thành
công của chúng tùy thuộc vào bộ dữ liệu. Tận dụng cắt
tỉa bán cấu trúc, Lagunas et al. có thể đạt được tăng
tốc 1.19 (Lagunas et al., 2021) với những mất mát nhỏ
về độ chính xác. Mặc dù họ thấy rằng encoder dễ cắt
tỉa hơn decoder, họ không sử dụng bằng chứng về sự
bất đối xứng này để tăng tốc hiệu suất hơn nữa.

Li et al. điều tra cách kích hoạt lượng tử hóa, phát hiện
ra rằng nếu không có chưng cất chuyên biệt trong quá
trình lượng tử hóa, hiệu suất sẽ sụp đổ (Li et al., 2022).
Tận dụng việc sinh ra xảy ra lặp đi lặp lại, và một số
token dễ sinh ra hơn những token khác, CALM (Schuster
et al., 2022) áp dụng thoát sớm để cải thiện tốc độ suy
luận 1.4 lần. Mặc dù công việc hiện tại đã tìm thấy sự
quan tâm đến bất đối xứng, nó chưa được nghiên cứu
trực tiếp, cũng như các mối quan hệ trong quy mô mô
hình chưa được khám phá.

Mặc dù có những phương pháp khác như chưng cất
kiến thức (Hinton et al., 2015) (Sanh et al., 2019) (Jiao
et al., 2020), lượng tử hóa (Zafrir et al., 2019), thoát
sớm (Xin et al., 2020) và cắt tỉa token (Kim et al., 2021),
những phương pháp này không phải là trọng tâm của
công việc chúng tôi vì việc hiểu tác động của nhiều
biến cùng nhau hạn chế độ sâu khám phá của chúng
tôi. Chúng tôi để dành nghiên cứu sâu hơn về sự tương
tác giữa tóm tắt và lượng tử hóa, cắt tỉa không có cấu
trúc, cắt tỉa có cấu trúc và chưng cất kiến thức cho
công việc tương lai.

3 Quy mô và Tóm tắt Trừu tượng
3.1 Nền tảng
Các mô hình ngôn ngữ sequence-to-sequence như BART
(Lewis et al., 2021), T5 (Raffel et al., 2020), và PEGASUS
(Zhang et al., 2020) kết hợp các encoder và decoder
transformer để tạo ra các mô hình có thể thích ứng
với các nhiệm vụ mới và đạt hiệu suất hàng đầu trong
các nhiệm vụ từ truy xuất thông tin (Nogueira et al.,
2020) đến tóm tắt (Raffel et al., 2020).

Chúng tôi tập trung vào các mô hình FLAN-T5 được
điều chỉnh theo hướng dẫn (Wei et al., 2021) vì hiệu
suất của chúng có tính cạnh tranh và chúng có những
biến đổi rộng về kích thước mô hình từ 60 triệu đến
11 tỷ tham số và do chi phí huấn luyện các biến thể
lớn hơn, tập trung vào các biến thể small, base và large.
Chi tiết về kích thước mô hình và kiến trúc có thể được
tìm thấy trong bảng 1.

Tóm tắt trừu tượng là một phương pháp nén chuỗi
trong đó một tài liệu nguồn D được chuyển đổi thành
một tài liệu đích dsum, ngắn hơn nhưng trung thành
với đầu vào.

Các bộ dữ liệu sử dụng là sự kết hợp của các benchmark
công cộng và học thuật và một bộ dữ liệu tìm kiếm
web độc quyền. Các bộ dữ liệu CNN/DailyMail (CNNDM)
(See et al., 2017) và XSUM (Narayan et al., 2018) dựa
trên việc tóm tắt các mô hình ngôn ngữ tin tức tiếng
Anh. Query Independent Web Summary (QIWS) là một
tập dữ liệu độc quyền gồm các bản tóm tắt trừu tượng
của các trang web được sử dụng để tạo ra các đoạn
trích ngữ cảnh thông tin cho người dùng công cụ tìm
kiếm. Điều quan trọng cần lưu ý là sự khác biệt trong
hệ số nén trong mỗi bộ dữ liệu vì mỗi bộ tác động đến
cách độ trễ suy luận được điều khiển bởi decoder.
Thông tin thêm về thành phần của mỗi bộ dữ liệu có
thể được tìm thấy trong bảng 11.

Các Chỉ số Đối với mỗi bộ dữ liệu, chúng tôi đánh giá
hiệu suất mô hình bằng cách đo ROUGE-1 (R-1),
ROUGE-2 (R-2), ROUGE-L (R-L), RougeSum-L (RSL)6
(Lin, 2004), và Generation Length (GenL) trên phần
test của bộ dữ liệu. Để hỗ trợ tính tái tạo và mở rộng
công việc của chúng tôi, chúng tôi thử nghiệm sử dụng
Transformers7 của HuggingFace, công bố các script
huấn luyện và cắt tỉa8 và các biến thể mô hình cho
các bộ dữ liệu công khai9.

3.2 Các Định luật Mở rộng Quy mô cho Tóm tắt Trừu tượng
Để nghiên cứu vai trò của quy mô trong tóm tắt trừu
tượng, chúng tôi huấn luyện các mô hình small, base
và large trên ba bộ dữ liệu được đề cập ở trên. Chúng
tôi không nghiên cứu XL (3B) và XXL (11B) vì chúng
đắt đỏ và chậm để huấn luyện.

Đối với tất cả các thử nghiệm của chúng tôi, chúng tôi
tận dụng các tham số được hiển thị trong 12 trên các
bộ dữ liệu được hiển thị trong 11

Như được hiển thị trong 2, 13, 14, và 15, có một vai
trò đáng kể giữa quy mô và hiệu suất, nhưng có sự
biến đổi đáng kể qua các bộ dữ liệu.

Các bộ dữ liệu với bản tóm tắt ứng viên ngắn, như
XSUM, thấy gần ba lần tác động so với các bản tóm
tắt dài của QIWS và CNNDM. Trong các đánh giá định
tính, vai trò của quy mô có thể dễ dàng quan sát được
vì các mô hình nhỏ hơn tạo ra nhiều bản tóm tắt từ
khóa ngắn hơn trong khi việc đưa vào quy mô làm cho
các phản hồi trở nên tự nhiên hơn.

3.3 Benchmark Suy luận
Để đánh giá tác động của bất đối xứng đối với suy luận,
chúng tôi chạy các thử nghiệm về thông lượng của
mỗi mô hình. Sử dụng GPU A10 và các mô hình từ bộ
dữ liệu QIWS của chúng tôi, chúng tôi đánh giá hiệu
suất với độ dài chuỗi tối đa là 1024, bản tóm tắt tối
đa là 256, và kích thước batch 1, 8, và 16 sử dụng suy
luận gốc trong PyTorch. Chúng tôi báo cáo giá trị
trung bình và độ lệch chuẩn của thời gian đo trên bảy
lần chạy.

Khi so sánh tác động của quy mô đối với R-2 so với
các hiệu ứng đối với độ trễ qua các kích thước batch
trong 2, 4, 3, rõ ràng là các mô hình lớn hơn đắt đỏ
hơn để thực thi đáng kể khi kích thước batch tăng.
Điều này là do sự khác biệt tiềm ẩn trong độ dài đầu
ra trong một batch vì batch hoàn thành khi tất cả các
chuỗi đã tạo ra một token EOS. Để giảm bớt vấn đề
nghẽn cổ chai này, các phương pháp streaming cải
tiến cho việc batching cải tiến đã được đề xuất (Yang
et al., 2020) nhưng có thể khó quản lý.

4 Đến Bất đối xứng và Xa hơn
Mặc dù công việc trước đây đã nghiên cứu cách cải
thiện suy luận và gián tiếp khám phá sự bất đối xứng
giữa encoder và decoder, chúng tôi nghiên cứu điều
đó một cách rõ ràng và qua các quy mô mô hình.
Chúng tôi tập trung nghiên cứu của mình vào cắt tỉa
cấu trúc vì lợi ích suy luận dễ thực hiện, và phương
pháp này có tính tương thích cao với các phương pháp
khác như lượng tử hóa và cắt tỉa không có cấu trúc.
Chúng tôi không nghiên cứu cách sự bất đối xứng bị
ảnh hưởng bởi cắt tỉa không có cấu trúc hoặc lượng
tử hóa vì những phương pháp này khó kết hợp với
các thư viện tối ưu hóa như FasterTransformers10.

Theo Shleifer et al., chúng tôi áp dụng phương pháp
"Shrink and then fine" tune (STF) cho nén. Đầu tiên,
một mô hình được huấn luyện cho đến khi hội tụ trên
một nhiệm vụ fine-tuning tóm tắt. Sau đó, toàn bộ các
lớp được loại bỏ khỏi encoder, decoder, hoặc cả hai,
và mô hình được fine-tune thêm cho đến khi nó hội
tụ lại. Chúng tôi không nghiên cứu việc sử dụng chưng
cất kiến thức để tránh chi phí huấn luyện bổ sung mà
không có cải thiện được đảm bảo theo kết quả của
Shleifer et al.

Mỗi mô hình chúng tôi nghiên cứu có số lượng lớp
encoder và decoder đồng nhất, vì vậy chúng tôi chỉ
cắt tỉa encoder, decoder, và sự kết hợp đối xứng của
hai thành phần này. Chúng tôi sử dụng ba quy mô
mô hình không nén (small, base, large), và chúng tôi
cắt tỉa mô hình theo bội số của 1 trên encoder, decoder,
và cả hai. Sau khi cắt tỉa, các mô hình được fine-tune
lại và đánh giá. Điều này có nghĩa là đối với mỗi bộ
dữ liệu, chúng tôi có 16 biến thể cho mỗi kích thước
mô hình dẫn đến 48 mô hình mỗi bộ dữ liệu và 144
mô hình tổng cộng.

Với số lượng lớn các mô hình và chi phí của nhiều
seeds hoặc tối ưu hóa dành riêng cho mô hình, chúng
tôi huấn luyện mỗi mô hình một lần và không tối ưu
hóa các tham số cho mỗi mô hình. Mặc dù điều này
dẫn đến hiệu suất kém hơn lý tưởng, mục tiêu của
chúng tôi không phải là siêu tối ưu hóa các mô hình
mà khám phá nơi có độ nhạy cao. Để tiết kiệm không
gian, chúng tôi sử dụng tên viết tắt lenc và ldec để
chỉ phần số của các lớp transformer encoder và decoder
(trong số 6), và R đề cập đến tỷ lệ phần trăm nhớ lại
hiệu suất so với baseline không nén. Kết quả chi tiết
đã được chuyển đến A.3 để tiết kiệm không gian.

--- TRANG 5 ---
[Tiếp tục với phần còn lại của bản dịch...]

4.1 Quy mô và Cắt tỉa
Nhìn vào kết quả rút gọn trong 5, 6, và 7, có một định luật mở rộng quy mô rõ ràng vì các mô hình nhỏ hơn thấy những giảm sút hiệu suất lớn hơn nhiều khi được nén ở cùng mức độ. Ví dụ, trên bộ dữ liệu QIWS, nén xuống 1/6 số lớp trên encoder và decoder gây ra giảm 80% R-2 trên mô hình small nhưng chỉ 40% trên mô hình lớn hơn. So sánh quy mô này là 65% xuống 26% trên bộ dữ liệu CNNDM và 64% xuống 45% trên bộ dữ liệu XSUM.

Kết quả mở rộng quy mô tương tự đúng với cắt tỉa encoder hoặc decoder, nơi nén các mô hình lớn dẫn đến mất mát hiệu suất thấp hơn 5 lần so với các mô hình nhỏ. Khi kích thước mô hình tăng lên, tác động của cắt tỉa chỉ decoder so với chỉ encoder trở nên mờ nhạt hơn. Trên bộ dữ liệu CNNDM, khoảng cách giữa chỉ decoder và chỉ encoder được cắt tỉa xuống 1/6 là 10% với FLAN-T5 small nhưng chỉ 4% với biến thể large. Khi so sánh bất đối xứng và đối xứng, khoảng cách thậm chí còn rõ rệt hơn nữa nơi khoảng cách small là 30% trong khi large là 20%.

Như được hiển thị trong Hình 3, tác động của nén trở nên mờ nhạt hơn khi kích thước mô hình tăng lên. Nói cách khác, các mô hình lớn hơn có thể nén được hơn và dễ chịu sự bất đối xứng trong việc nén này. Tác động của bất đối xứng dễ hiểu nhất vì không đáng ngạc nhiên khi việc cắt tỉa hoàn toàn một mô hình dẫn đến mất mát cao hơn việc cắt tỉa một phần qua các bộ dữ liệu và kích thước mô hình. Mặc dù phát hiện này không ngay lập tức đáng ngạc nhiên, việc đánh giá chi phí suy luận trở nên quan trọng.

4.2 Benchmark Suy luận
Chúng tôi đánh giá tác động của bất đối xứng theo phương pháp tương tự như các thử nghiệm quy mô của chúng tôi. Sử dụng GPU A10, chúng tôi đánh giá hiệu suất cho tóm tắt trên một phần của bộ dữ liệu đánh giá tương ứng của mỗi mô hình với độ dài chuỗi tối đa là 1024, độ dài tóm tắt tối đa là 256, và kích thước batch 1, 8, và 16. Chúng tôi chọn những kích thước batch này để đại diện cho các workload streaming (kích thước batch 1), kết quả thời gian thực cho các kết quả hàng đầu từ một truy vấn tìm kiếm (kích thước batch 8), và thông lượng tối đa được cung cấp ngân sách bộ nhớ của A10 (kích thước batch 16)

[Bảng 8 hiển thị mối quan hệ giữa độ chính xác và tăng tốc của cắt tỉa chỉ encoder, chỉ decoder, encoder và decoder trên các mô hình FLAN-T5 Large trên CNN/DM, XSUM, và QIWS]

Nhìn vào tập kết quả tập trung cho các mô hình large qua các bộ dữ liệu trong bảng 8 về tác động của R-2 so với tăng tốc suy luận, chúng ta có thể thấy một mối quan hệ rõ ràng giữa bất đối xứng và hiệu quả suy luận. Mặc dù kết quả suy luận chi tiết có thể được tìm thấy trong phụ lục A.4 trên tập kết quả tập trung này, chúng ta có thể thấy rằng cắt tỉa chỉ encoder dẫn đến không quá 30% cải thiện trong hiệu quả suy luận với mất mát đáng kể về độ chính xác. Cắt tỉa mô hình đối xứng dẫn đến cải thiện suy luận có thể thực hiện được lên đến 5 lần với chi phí là độ chính xác tóm tắt.

Ngược lại, khi chỉ decoder được cắt tỉa, có thể thấy hầu hết các tăng tốc suy luận được thấy trong quá trình cắt tỉa liên tục với tác động thấp hơn đáng kể đối với độ chính xác. Trên bộ dữ liệu CNN/DM, cắt tỉa liên tục dẫn đến suy luận tốt hơn 8% nhưng mất gần bốn lần hiệu suất của nén không đồng nhất.

[Bảng 9 và 10 hiển thị các mối quan hệ tương tự cho các mô hình khác nhau và kích thước batch khác nhau]

5 Thảo luận
5.1 Quy mô, Suy luận, và Cắt tỉa
Như được hiển thị trong bảng 9, các lợi ích tìm thấy bằng cắt tỉa cực kỳ nhất quán độc lập với việc mở rộng quy mô. Cắt tỉa chỉ encoder dẫn đến cải thiện 4-6% về độ trễ, và cắt tỉa chỉ decoder dẫn đến 400%, cũng như nén đồng nhất. Điều này được mong đợi vì cắt tỉa cấu trúc loại bỏ một phần không đổi của mạng, dẫn đến lợi ích độ trễ nhất quán bất kể quy mô mô hình.

5.2 Quy mô, Cắt tỉa và Độ dài được sinh ra
Mặc dù mong đợi một xu hướng đáng kể trong vai trò của quy mô và cắt tỉa trong việc sinh ra, chúng tôi không thấy bất kỳ xu hướng đáng chú ý nào. Như được hiển thị trong hình 6 và 4, không có xu hướng rõ ràng nào về Vai trò của quy mô và cắt tỉa trong độ dài sinh ra. Có một bước nhảy nhỏ trong độ dài sinh ra từ FLAN-T5 small đến FLAN-T5 base qua tất cả các bộ dữ liệu nhưng không có bước nhảy như vậy từ FLAN-T5 base đến FLAN-T5 large. Chúng tôi tin rằng điều này là do các mô hình nhỏ hơn ít lưu loát hơn và cần nhiều token hơn để đảm bảo độ bao phủ chính xác. Khi các mô hình mở rộng quy mô, điều này không còn cần thiết nữa, và các mô hình hội tụ về độ dài tóm tắt đồng nhất.

5.3 Bất đối xứng với các batch lớn
Mặc dù có sức hấp dẫn của cắt tỉa bất đối xứng, nó không phải không có lỗi. Như được hiển thị trong bảng 10 và Hình 5, các cải thiện trong hiệu quả suy luận bị ảnh hưởng mạnh bởi kích thước batch. Khi kích thước batch là tối thiểu, sự khác biệt trong loại bất đối xứng có tác động đáng kể đến hiệu quả suy luận. Khi các batch mở rộng quy mô, tăng tốc từ chỉ encoder hoặc chỉ decoder trở nên gần gũi hơn nhiều và trở nên nhỏ khi so sánh với các phương pháp đồng nhất. Điều này chỉ ra tại sao công việc tiếp theo về cải thiện các phương pháp suy luận sinh ra rất có liên quan, vì vấn đề này ảnh hưởng đến các quy trình hướng hiệu quả khác như CALM (Schuster et al., 2022).

6 Kết luận và Công việc Tương lai
Trong công việc này, chúng tôi khám phá vai trò của sự đối xứng trong việc cắt tỉa các mô hình sequence-to-sequence cho tóm tắt trừu tượng, phát hiện ra rằng cắt tỉa bất đối xứng có thể dẫn đến tăng tốc suy luận với mất mát thấp về độ chính xác. Công việc của chúng tôi cũng khám phá mối quan hệ giữa quy mô mô hình và độ nhạy cảm với cắt tỉa, phát hiện ra rằng các mô hình lớn hơn thấy mất mát thấp hơn khi được cắt tỉa. Điều này nén các mô hình FLAN-T5 để mang lại lợi ích suy luận 3 lần với mất 1 điểm Rouge-2.

Trong công việc tương lai, chúng tôi tìm cách nghiên cứu cách gắn nhãn giả, thoát sớm, và lượng tử hóa có thể được kết hợp để cải thiện thêm hiệu quả suy luận của các mô hình sequence-to-sequence.

[Phần Tài liệu tham khảo và Phụ lục tiếp theo với các bảng và chi tiết kỹ thuật...]
