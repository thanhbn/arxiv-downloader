# 2212.12770.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2212.12770.pdf
# File size: 2320324 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020 1
COLT: Cyclic Overlapping Lottery Tickets for
Faster Pruning of Convolutional Neural Networks
Md. Ismail Hossain, Mohammed Rakib, M. M. Lutfe Elahi, Nabeel Mohammed and Shafin Rahman
Abstract —Pruning refers to the elimination of trivial weights
from neural networks. The sub-networks within an overparam-
eterized model produced after pruning are often called Lottery
tickets. This research aims to generate winning lottery tickets
from a set of lottery tickets that can achieve accuracy similar
to that of the original unpruned network. We introduce a novel
winning ticket called Cyclic Overlapping Lottery Ticket (COLT)
by data splitting and cyclic retraining of the pruned network
from scratch. We apply a cyclic pruning algorithm that keeps
only the overlapping weights of different pruned models trained
on different data segments. Our results demonstrate that COLT
can achieve similar accuracies (obtained by the unpruned model)
while maintaining high sparsities. Based on object recognition
and detection tasks, we show that the accuracy of COLT is on
par with the winning tickets of the Lottery Ticket Hypothesis
(LTH) and, at times, is better. Moreover, COLTs can be generated
using fewer iterations than tickets generated by the popular
Iterative Magnitude Pruning (IMP) method. In addition, we
also notice that COLTs generated on large datasets can be
transferred to small ones without compromising performance,
demonstrating its generalizing capability. We conduct all our
experiments on Cifar-10, Cifar-100, TinyImageNet, and ImageNet
datasets and report superior performance than the state-of-the-
art methods. The codes are available at: https://github.
com/ismail31416/COLT
Impact Statement — The emergence of large-scale deep learning
models outperforms traditional systems in solving many real-
life problems. Its success sometimes even beats human-level
performance in several tasks. However, the main bottleneck
is still the rise in computational cost. To minimize this, the
researcher started looking for small-scale alternatives for large-
scale models. One important direction is model pruning, which
aims to prune the existing deep model without compromising
performance. Existing approaches in this line of investigation
perform iterative pruning of the same model in several pruning
rounds. In this paper, we propose a novel idea to minimize the
number of pruning rounds while keeping the unpruned model’s
accuracy. In other words, by utilizing our COLT algorithm,
models can be pruned quicker, reducing the carbon footprint
and making our algorithm more environment-friendly. Moreover,
aligning with the literature, we demonstrate that the pruned
subnetwork computed using one dataset can be used for different
datasets without any dataset-specific pruning. It will help to build
a pruned sub-network for a new domain quickly. This study
will open up a new research prospect in finding a new pruning
strategy for convolutional neural networks.
Index Terms —Model Compression, Pruning, Sparse Networks
I. I NTRODUCTION
Neural network pruning refers to removing unnecessary
weights from neural networks. This problem has been widely
Md. Ismail Hossain, Mohammed Rakib, M. M. Lutfe Elahi, Nabeel Mo-
hammed, and Shafin Rahman are with the Department of Electrical and Com-
puter Engineering at North South University, Dhaka 1229, Bangladesh. Cor-
responding author: Shafin Rahman (e-mail: shafin.rahman@northsouth.edu)studied since the 1980s to make networks sparse and effi-
cient without adversely affecting performance [1]–[6]. Con-
sequently, pruning can compress the size of the model [3],
[7] and make it less power-hungry [8]–[10], possibly allowing
inference to be more efficient. With the rise of deep neural
networks (DNNs), pruning has witnessed a resurgence of
interest. Many studies have been conducted towards making
DNNs sparse in the image and vision computing field [3], [4],
[8], [11]–[16]. Pruning has consistently shown to be effective
in reducing the high space and time demands of classification
[17], [18] and object detection tasks [19], [20]. In this paper,
we propose a novel neural network pruning strategy that can
achieve higher compression, maintaining faster convergence
and decent performance.
A pruning technique, namely the lottery ticket hypothesis
(LTH) [21], has recently gained much attention from the
research community. It is a weight-based pruning method,
where the lowest magnitude weights are pruned iteratively
after training. However, the aspect that differentiates LTH
from other methods is resetting weights to their initial state
(also called weight-rewinding) after pruning. Since multiple
iterations of this train-prune-rewind cycle form the basis of this
pruning algorithm, this process is called iterative magnitude
pruning (IMP). According to [21], a subnetwork (also called
a ticket) generated using IMP can reach a sparsity of 90% or
more without compromising performance. So, once the sparse
ticket is generated, it can be trained on the dataset from scratch
and achieve accuracies equal to or greater than the original
network. Later works like [22] demonstrate that these tickets
generated from one dataset can even be transferred to another
and achieve accuracies comparable to the original network.
Specifically, they show that the subnetworks generated by IMP
are independent of both datasets and optimizers. Considering
such dataset transferability and optimizer independence of
LTH tickets, we propose a new kind of lottery ticket that
can achieve similar model compression (i.e., network pruning)
but faster in computation (fewer pruning rounds) compared to
LHT based method (see Fig. 1).
Similar to IMP of the LTH-based pruning method, our
pruning algorithm also utilizes IMP. We first split the training
dataset into two class-wise partitions (Partitions 1 and 2).
Each part contains non-overlapping classes and their instances.
We train two models using Partition 1 and 2 independently
but with the same initialization. Before the next iteration of
the train-prune-rewind cycle of IMP, our pruning algorithm
takes the intersection of the weights of the two models. In
other words, only the overlapping weights survive, and non-
overlapping ones become zero (i.e., pruned). This overlap is
calculated by masks that find the subnetwork common in botharXiv:2212.12770v2  [cs.CV]  24 Jan 2025

--- PAGE 2 ---
2 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
Fig. 1: We deal with the problem of pruning an overparameterized network without compromising performance. Here, we
visualize the initial weights of learned networks while training with TinyImageNet. ( a) shows weights of the unpruned
overparameterized network (no pruning), achieving an accuracy of 72.7%. The popular pruning method, ( b) LTH can prune
the same network to a sparsity of 88.8% (green grid regions) in 10 pruning rounds, achieving an accuracy of 71.42%. ( c) In
this paper, we propose a novel pruning method named COLT that can prune the network in (a)to a sparsity of 89.1% in 7
pruning rounds and achieve an accuracy of 72.8%. In comparison to LTH in (b), COLT can generate highly sparse winning
subnetworks (tickets) in fewer iterations (7 vs. 10), maintaining similar accuracy.
models (trained on individual partitions). We notice that this
intersection of the two lottery tickets is often a winning ticket.
Since pruning occurs twice per iteration (from two separate
models), our method can generate a sparse winning ticket in
fewer iterations than LTH. Generally, IMP of LTH needs to
run for several iterations before getting a sparse winning ticket.
However, utilizing our iterative pruning algorithm, we can
deduce a winning ticket of similar sparsity in fewer iterations
than LTH. For our experiments, we have used ResNet-18 [23],
MobileNetV2 [24], and a 4-layer plain Convolutional Neural
Network (CNN) called Conv-3 trained on Cifar-10 [25], Cifar-
100 [25], Tiny ImageNet [21] and ImageNet [26]. We have
divided each of the three datasets into Nparts ( N=2,3,4),
each having a different but equ‘al number of classes. Our
experimental results demonstrate that after finishing training-
pruning-rewinding, if we only keep the overlapping weights
between the two parts and prune the rest, it works as well as
the original unpruned network for both datasets. This process
is repeated multiple times until the desired sparsity for the
network is reached. We call the pruned network a Cyclic
Overlapping Lottery Ticket (COLT). COLT can be generated
in fewer iterations than lottery tickets, which are generated
from a single dataset. Moreover, COLT can also be used to
initialize weights for another dataset with successful training
outcomes. We have compared COLT’s performance with LTH
experimenting on Cifar-10, Cifar-100, Tiny ImageNet, and
ImageNet datasets. We also have transferred COLT trained
on Tiny ImageNet on Cifar-10 and Cifar-100. Beyond classi-
fication, we further assess the method on the Object Detection
task. In experiments, we achieve performances similar to the
original (unpruned network) network but take fewer pruning
rounds than the LTH method. In summary, we make the
following contributions:
•We propose an iterative pruning algorithm for model
compression based on overlapping/intersecting weights
trained from Nnon-overlapping partitions of the same
dataset. The resulting subnetwork (after pruning) be-
comes high sparse and reaches the accuracy similar
to that of the original unpruned network. We call this
generated subnetwork/ticket Cyclic Overlapping LotteryTicket (COLT).
•Without compromising performance, COLT can achieve
the desired sparsity by pruning a given network costing
fewer iterations than the popular LTH-based pruning
method.
•We demonstrate the superiority of our method by ex-
perimenting on five datasets (Cifar-10, Cifar-100, Tiny
ImageNet, ImageNet and Pascal-VOC) and three CNN
architectures (Conv-3, ResNet-18, and MobileNetV2).
II. R ELATED WORKS
Traditional pruning: The overall goal of pruning is to reduce
the network architecture by minimizing the number of network
parameters without hampering the network performance. Han
et al. [3] proposed the most popular pruning method, which
suggests training the network to be pruned until convergence
and then calculating a score for each parameter representing
the importance of the given parameter. Later, parameters
with low scores get pruned, resulting in network performance
degradation. [3], [27] suggested finetuning the network using
only the unpruned parameters. This train-prune cycle gradually
makes the network sparse and is the traditional pruning
method. Later, other related works proposed slight variations
to this approach where weights are pruned while training in
a periodic manner [28] or during initialization [29]. Another
paper [13] explicitly adds extra parameters to the network to
promote sparsity and serve as a basis for scoring the network
after training. In this paper, instead of finetuning after pruning,
we have rewound the unpruned weights to their initial state and
then retrained them to convergence.
Pruning with lottery ticket hypothesis: In general, pruning
is performed as a post-training step, necessitating training
the full model before attempting parameter pruning. How-
ever, with the advent of the lottery ticket hypothesis (LTH)
[21], predetermined sparse networks can be trained from
scratch. In contrary to previous works [30]–[35], LTH [21]
suggested that over-parameterization is only required to find
a “good” initialization of a properly parameterized network.
They demonstrated that substantially smaller sub-networks
(lottery tickets) exist within large over-parameterized models.

--- PAGE 3 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 3
5 Rounds10 Rounds17 Rounds
3 Rounds7 Rounds13 Rounds
0-70% 0-90% 0-98%05001000150020002500
ResNet18-T inyImageNet
L TH
COL T
Sparsity (%)Latency (Mins)
5 Rounds16 Rounds35 Rounds
3 Rounds11 Rounds24 Rounds
0-70% 0-90% 0-98%0100200300400500
ResNet18 - Cifar100
LTH
COLT
Sparsity (%)Latency (Mins)
Fig. 2: Illustration of the advantage of our proposed COLT over the LTH-based pruning method while using architecture
ResNet-18 on ( Left) Cifar10 and ( Right ) Cifar-100 datasets. Compared to LTH, COLT can generate a highly sparse ticket in
fewer rounds/iterations. We notice the same trend to achieve sparsity of different ranges, e.g., 0-70%, 0-90%, and 0-98%. In
all cases, COLT maintains a similar accuracy to LTH while requiring fewer pruning rounds. Specifically, for ( Left) ResNet-18
on Cifar-10, the sparsity 61.5%, 88.8% and 97.6% achieved an accuracy of 60.28%, 59.01% and 51.81% respectively. For
(Right ) ResNet-18 on Cifar100, the sparsity 61.7%, 89.1% and 97.4% achieved an accuracy of about 73.59%, 72.88% and
68.53%, respectively.
While trained in isolation, these sub-networks achieve similar
or better performance than the original network even after
pruning more than 90% of the parameters. It allows networks
to train with fewer resources and run inference of models
on smaller devices like cellphones [36]. Also, it encourages
generalization by acting as a regularizer for overparameterized
models. To find and evaluate winning tickets using LTH, an
overparameterized network is first initialized and trained to
converge. Then the lowest magnitude weights are pruned, and
the remaining weights are reset to their initial state at the start
of training. This resetting of weights is also called weight
rewinding. Finally, this smaller subnetwork (winning ticket)
is then trained again to compare its performance with another
subnetwork with the same number of parameters and the same
initialization but randomly pruned weights. A good winning
ticket outperforms a randomly pruned ticket. [37] provides
a more rigorous definition of LTH by performing extensive
experiments and exhaustively tuning hyperparameters like
learning rate, and training epochs. In this paper, we propose an
iterative pruning algorithm that uses fewer iterations to reach
a specified sparsity.
Pruning in transfer learning: Network pruning can take
advantage of transfer learning concepts [38]–[41]. Notable
transfer learning-based methods [8], [15] train the model on
one dataset, then apply pruning steps and later fine-tune the
model on another dataset to achieve high performance. All of
these papers analyze the transfer of learned representations of
models. However, the transfer of weight initializations across
datasets was first analyzed by [22]. It means transferring
the initial weights after the train-prune-rewind cycle of IMP.
[22] elaborately discussed the generalizing and transferring
capability of lottery tickets. They demonstrate that winning
ticket initializations can be generated independently of datasets
and optimizers used in experiments. Furthermore, they alsoshow that winning tickets generated by large datasets can
successfully be transferred to small datasets. One could de-
velop a robust ticket from a vast dataset and then reuse those
initializations on other datasets without fully training a model.
This paper demonstrates the transferable capability of tickets
generated by COLT across various datasets.
While all methods achieve pruning and high sparsity, COLT
distinguishes itself by offering faster pruning and transfer-
ability, features not consistently present in other techniques.
For instance, LTH [21] emphasizes high sparsity alongside
effective pruning but lacks the speed of pruning that COLT
demonstrates. Similarly, SNIP [29] and GraSP [33] achieve
high sparsity, yet neither consistently matches our proposed
COLT in terms of both speed and transferability across tasks.
III. M ETHODOLOGY
A. Problem formulation
Let us consider a feed-forward network F(x;θ)with initial
parameters θ=θ0∼ D θand input x. After training until
convergence, this network reaches an accuracy of aachieving
a minimum validation loss. Fcan also be trained with a mask
m∈ {0,1}|θ|operated on parameters by m⊙θachieving a
sparsity s%. To generate this mask m, we need to perform j
rounds of train-prune-rewind cycle. Our goal is to design such
a mask mthat when operated on parameters, θ(bym⊙θ)
generates a network, F(x;m⊙θ)achieving a commensurate
accuracy, a′≥ausing less training time and pruning rounds
j′≤jwhile maintaining the same sparsity s%. Training with
m= 1|θ|means we train an unpruned network F. While
using m∈ {0,1}|θ|, we train a pruned subnetwork of F.
In this case, the corresponding weights/parameters inside θ,
where 0s are assigned inside mhave been pruned. More zeros
inside mcreates more sparsity inside network parameters

--- PAGE 4 ---
4 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
TABLE I: Transferring LTH tickets calculated from Partition 1
and 2 to the entire Cifar-100 dataset. The performance of over-
lapping tickets achieves better sparsity without compromising
accuracy. It confirms that overlapping also has transferability
property in addition to individual partitions’ tickets.
Using tickets from Partition-1 Partition-2 Overlapping ticket
Accuracy (%) 72.4 72.3 69.1
Sparsity (%) 79.9 79.9 95.0
when applying m⊙θ. We aim to increase such sparsity in
Fwithout compromising performance and training time.
Lottery ticket hypothesis (LTH): Frankle and Carbin [21]
proposed the most popular solution for this problem. Later
[22], [42] improved the LTH idea. LTH used a magnitude-
based pruning algorithm. By identifying a sparse mask, m,
lower magnitude weights are removed in different pruning
rounds. The remaining weights are then rewound to their
initial state by θ=m⊙θ. This rewound set of weights is
a lottery ticket. If one can find a lottery ticket that can train a
model and achieve accuracy equal to or greater than the initial
unpruned weights, that lottery ticket is called the winning
ticket. The train-prune-rewind cycle is performed multiple
times iteratively to generate highly sparse winning tickets.
Issues with LTH: Repetitively training from scratch to the
convergence of large networks while finding a winning ticket
is a costly process, increasing computational time. In Fig. 2,
we see that to reach a highly sparse ticket, LTH requires a high
no. of pruning rounds (i.e., increasing latency). For example,
it takes 62 pruning rounds to generate a 98% sparse ticket of
MobileNetV2 on Cifar-100. In this paper, we aim to reduce
the number of pruning rounds (training time) to reach such a
high sparsity.
B. Solution strategy
Morcos et al. [22] observed an intriguing fact about LTH
regarding the transferability of lottery tickets. Pruned subnet-
works, i.e., winning tickets computed using one dataset, can
be transferred to other datasets, given the domain of data is
similar (e.g., natural images). They argue that the same pruned
subnetworks can generalize across different training condi-
tions, optimizers, and datasets. The main implication of this
finding is that we can compute a generic winning ticket once
using one dataset and then reuse it across multiple datasets
or optimizers. It can save the cost of repeatedly computing
optimizers or dataset-specific tickets. The winning ticket can
achieve similar accuracy and sparsity identical to the winning
ticket of the entire model LTH training on the novel dataset.
In this paper, we investigate the transferability property of
LTH. We observe that intermediate tickets (before obtaining
the winning tickets) also have the transferability property but
with less sparsity. Each pruning round of our pruning method
produces some intermediate subnetworks/tickets, perform the
intersection of different subnetworks/tickets, and then transfer
the new ticket to the next round. Our concept of transferring
tickets across iteration/pruning rounds helps us achieve a simi-
lar performance of LTH, costing fewer rounds of computation.
0 5 10 1562646668707274
020406080100
Acc using m (1) 
Acc using m (2) 
Acc using m
Pruning Rate - m (1) 
Pruning Rate - m (2) 
Pruning Rate - mResNet18: CIF AR-100
Pruning RoundsTest Accuracy (%)
Pruning Rates(%)Fig. 3: Transferability of tickets calculated from Partition 1,
m(1), partition 2, m(2)and their overlap, m=m(1)∩m(2).
Overlapping ticket, machieves a higher pruning rate (spar-
sity), maintaining similar accuracy to others. m(1)andm(2)
get matching sparsity because we prune fixed p%low magni-
tude weight in every pruning round. In later pruning rounds,
the behavior of all tickets is similar because every time the
same mcalculated from the current round is transferred to both
F1andF2for the next round making θ(1)andθ(2)similar.
Our motivation begins with experiment results shown in
Table I. We divide Cifar-100 into Npartitions. Usually, N
is used as a hyperparameter. In the case of N=2, if we have
100 classes for a dataset, then images of the first 50 classes
remain in one partition and the remaining 50 in another.
Then we generate LTH tickets (until around 80% sparsity)
from each partition. According to [22], both tickets from
two separate partitions are transferable to the entire Cifar-
100 dataset. Because of this, we achieve 72.4/79.9% and
72.3/79.9% accuracy/sparsity based on training from parti-
tions 1 and 2, respectively. We know there exists a strong
correlation between tickets generated by both partitions. It
motivates us to check the performance of overlapping tickets,
meaning the intersecting sub-network, which remains common
after pruning. We notice that even excluding non-overlapping
weights (increasing sparsity to 95%), we can still achieve
respectable performance (69.1%) on full Cifar-100. It tells
that in addition to improving sparsity, overlapping tickets
calculated from multiple partitions of the same dataset are
also transferrable to the entire dataset. Since intermediate
tickets obtained from the intermediate LTH pruning rounds are
transferable, we calculate the overlapping ticket of N different
partitions in each pruning round. It can provide us with further
pruning, which leads us to fewer pruning rounds than LTH,
maintaining transferability to individual partitions and the full
dataset. In each pruning round, we train Nmodels based on N
partitions of the same dataset to calculate Ntickets from each
partition, then calculate the overlapping tickets between them,
and finally, transfer the overlapping ticket to the next pruning
round for initialization of those Nmodels. We continue this
until the overlapping ticket achieves similar accuracy to the
unpruned model.

--- PAGE 5 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 5
TABLE II: A toy example. Initial random
weights are updated after training. Using up-
dated weights, we calculate m(1)andm(2)
by assigning zero to the locations where
(bold values) p%lower magnitude weights
exist. Then, we calculate a generic ticket,
m=m(1)∩m(2), that refers to the pruned
subnetwork. Pruned initial weights for the
subsequent pruning round are calculated by
m⊙θ(1)andm⊙θ(2).Initial weights After training Mask Overlapping mask Pruned weights
θ(1)= θ(1)= m(1)= m=m(1)∩m(2)m⊙θ(1)=
0.1−0.2 0 .9
−0.4 0 .6 0 .8
0.3 0 .5−0.7

0.8−0.3 0 .4
-0.1 0.2 0.7
0.9 0 .5−0.5

1 1 1
0 0 1
1 1 1

1 1 0
0 0 1
1 1 1

0.1−0.2 0
0 0 0 .8
0.3 0 .5−0.7

θ(2)= θ(2)= m(2)= m⊙θ(2)=
0.1−0.2 0 .9
−0.4 0 .6 0 .8
0.3 0 .5−0.7

0.9−0.7 0.2
-0.1 0.4 0 .5
0.8 0 .3−0.6

1 1 0
0 1 1
1 1 1

0.1−0.2 0
0 0 0 .8
0.3 0 .5−0.7

C. Cyclic Overlapping Lottery Ticket (COLT)
LetYbe the set of classes in our training dataset D=
{xi, yi}n
i=1, where, xi∈Rh×wandyi∈ Y represent the
input image and class label, respectively, and ndenotes the
number of instances in the dataset. F(x;θ)is the unpruned
model with all parameters θ. We aim to calculate a winning
ticket represented as a sparse mask, m∈ {0,1}|θ|, such that
F(x;m⊙θ)can achieve similar accuracy of the unpruned
model. We propose a novel approach for generating winning
tickets that can be calculated using fewer pruning rounds than
LTH.
Let initialize a neural network F(x;θ0), where the initial
parameters θ=θ0. Subsequently, the dataset Dis par-
titioned into two disjoint subsets D(1)andD(2), ensuring
D=D(1)∪D(2)andD(1)∩D(2)=∅. Two models, F1
andF2, are then trained to convergence on D(1)andD(2),
respectively, resulting in trained parameters θ1andθ2with
accuracies a1anda2. The core novelty of this method lies
in cyclic pruning with overlapping tickets. During the pruning
process, the model’s parameters θ0are iteratively masked by
Nsubnetworks represented by masks mi∈0,1|θ0|. Each
mask miremoves the lowest p%magnitude weights. Pruned
parameters θifor each mask are obtained using element-wise
multiplication θi=mi⊙θ. These masks collectively form the
combined mask M=∩N
i=1mi, ensuring an overall sparsity
level swhile maintaining or improving performance. After
obtaining the combined mask M, it is applied to the model’s
parameters F(x;M⊙θ).
The method consists of four key steps described below:
(a) Data partitioning: We partition the training data into N
non-overlapping halves based on class labels. For this, we ran-
domly split the set of classes YintoNequal non-overlapping
halves Y(1),Y(2)...Y(N), where, Y(1)∩Y(2)∩Y(N)=∅. For an
example of the class-specific partition of two sets , we create
two separate datasets: D(1)=n
x(1)
i, y(1)
ion(1)
i=1andD(2)=
n
x(2)
i, y(2)
ion(2)
i=1where, y(1)
i∈ Y(1)andy(2)
i∈ Y(2). We
prefer the class-based split because it can provide us with more
diversity inside intermediate tickets (compared to instance-
based alternatives), which will help in faster pruning. For the
main experiment (in Fig. 5), we split the dataset into only two
halves because each half will contain enough instances for
generalization. More than two partitions (three/fours) may end
up with smaller subsets of the dataset. The effect of multiple
partitions is further discussed in Sec. IV-D(b) Model training: Using the partitioned data D1and
D2, we train two separate models F1(x(1);m⊙θ(1))and
F2(x(2);m⊙θ(2)), respectively. At the first pruning round,
m= 1|θ|meaning training of the unpruned model. In later
pruning rounds, mwill be determined by the mask generation
process discussed in the next paragraph, where mwill become
sparse, containing zeros in the pruned locations. After calculat-
ing mask, m, we apply the same mask on randomly initialized
parameters by m⊙θ(1)andm⊙θ(2). The⊙operation is
actually performing the pruning because it brings sparsity in
weight matrices. At the beginning of the training, parameters
θ(1)andθ(2)will get exactly the same initial weights as
θ0∼ D θthat will later be updated during the course of
backpropagation. Therefore, after convergence of models, F1
andF2, both θ(1)andθ(2)are expected to contain different
weights. A toy example of initial and after training weight of
θ(1)andθ(2)are shown in Table II. Note that training of F1
andF2are independent, meaning a parallel implementation is
possible. Moreover, associated datasets D1andD2are smaller
than the original ones. It means that we can minimize the
overall training time.
(c) Mask generation: Based on the trained weights θ(1)and
θ(2)from two separate model, we calculate a mask, m. First,
we generate two intermediate masks, m(1)andm(2)from
θ(1)andθ(2), respectively. Analyzing the weights of θ(1),
we prune p%of lower magnitude weights. Thus, the binary
mask m(1)will get zeros and ones where weights need to be
pruned and not pruned, respectively. Similarly, we generate
m(2)from θ(2). In our experiment, we use p= 20 as used
in LTH [21]. Note we use a small pbecause a large pgains
more sparsity but, at the same time, increases the chance of
layer collapse. Now, we generate m=m(1)∩m(2)to identify
overlapping weight locations. mdescribes the common tickets
where both models agree. Our method achieves faster pruning
because of the pruning from p%low-magnitude weights and
intersecting masks ( m) calculated from two different partitions
of the dataset. In Fig. 3, we show the performance of m(1),
m(2)andmon entire Cifar-100. It shows further evidence
that the overlapping ticket, m, is transferable across different
pruning rounds, ensuring higher sparsity.
(d) Rewinding: In this step, we are transferring the tickets
calculated from both partitions to individual partitions of
the dataset. Using the calculated mfrom the previous step,
we repeat the process for the subsequent pruning round by
iterating to step (b). The same mwill modify θ(1)andθ(2).
Models’ training rewinds the so far unpruned weights (after ⊙

--- PAGE 6 ---
6 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
Fig. 4: Visual illustration of COLT generation while N=2.
(a)Two models, F1&F2, of identical architectures, are first
initialized with the same initial weights (blue). (b)Next, the
two models are trained using the same set of hyperparameters
to deduce the final weights (red & green). (c)After training,
p% lowest magnitude final weights are pruned from each
model. (red weights are the p% lowest magnitude weights
while the green ones are above the threshold) (d)Then the
models are rewound to their initial state of weights (blue,
with the pruned weights being zero. (e)After rewinding, the
models’ overlapping weights (blue) are kept, and the rest are
pruned. The result is a COLT ticket that can gain accuracies
similar to or better than the original network.
operation) to their initial values used during the initialization
at the beginning of the training. Using the mcalculated from
the previous round facilitates training such that our method
tries to prune within so far unpruned weights only in the next
iteration rather than considering all parameters.
This iterative pruning process will continue until a valida-
tion accuracy of the entire dataset, D, becomes equal to or
better than the accuracy original unpruned model. Because of
the train-prune-rewind cycle on the overlapping masks/tickets,
we name the ticket generated by our method as Cyclic
Overlapping Lottery Ticket (COLT). Fig. 4 provides a visual
illustration for generating COLT. Note that we generate COLT
based on an architecture that can assign prediction scores forAlgorithm 1 Cyclic Overlapping Lottery Ticket (COLT)
Require: Dataset D, initial network parameters θ, number of
pruning rounds R, pruning rate p
Ensure: Winning ticket (sparse mask) m
1:Randomly split dataset Dinto two halves D(1)andD(2)
based on class labels
2:Initialize mask m=1|θ|(all ones, same size as θ)
3:forr= 1 toRdo
4: Train model F1(x;m⊙θ(1))onD(1)
5: Train model F2(x;m⊙θ(2))onD(2)
6: Generate intermediate masks:
7: m(1)=prune lowest magnitude (θ(1), p%)
8: m(2)=prune lowest magnitude (θ(2), p%)
9: ifr < R then
10: Rewind unpruned weights in θ(1)andθ(2)to their
initial values
11: Update mask: m=m(1)∩m(2)(element-wise
AND operation)
12: end if
13:end for
14:return final mask m
half of the total classes (either Y(1)orY(2)classes). However,
we want to evaluate COLT on the entire dataset ( Yclasses),
which means the same architecture needs to predict scores
for all categories. Therefore, to create our final model, F, we
removed the winning tickets’ output layer and replaced it with
a randomly reinitialized fully connected layer to predict scores
for all classes. We then train the tickets until convergence
before reporting the final performance on the full dataset.
Algorithm 1 summarises the overall process of our proposed
method.
We outline some key features and advantages of our pruning
approach. (a)Utilizing transfer property: We investigate the
‘transfer’ property of lottery tickets within different partitions
of the same dataset. It helps to confidently identify prospective
weights to be pruned in earlier pruning rounds. (b)Faster
pruning: COLT utilizes a novel approach of overlapping two
masks to generate highly sparse neural networks in signif-
icantly less time than LTH. By requiring fewer iterations
to achieve similar sparsity levels, COLT considerably speeds
up the pruning process. Furthermore, by preserving essential
weights, COLT maintains a similar or better performance
than LTH. Additionally, This heuristic is more efficient than
the traditional unstructured pruning heuristic used in LTH.
(c)Avoiding layer collapse: Even after completing pruning
two times in a single round, our method can avoid layer
collapse [43]. Tickets from the first and second partitions
are transferrable and strongly correlate with each other. Thus,
intersecting m(1)andm(2)do not discard weights such that a
complete layer gets collapsed. (d)Introducing Regulirization:
By retaining only high-magnitude weights and zeroing out
lower-magnitude ones, COLT implements a form of implicit
regularization, reducing the model’s capacity to overfit.

--- PAGE 7 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 7
TABLE III: Model architectures examined in this work. Brack-
ets denote residual connections around layers.
Network Conv-3 MobileNetV2 ResNet-18 VGG-16
Conv64, pool
128, pool
256, pool6×[34,34]
5×[17,17]
8×[9,9]
31×[5,5]16
3×[16,16]
3×[32,32]
3×[64,64]2×64, pool
2×128, pool
3×256, pool
3×512, pool
3×512, pool
WeightsAll: 397K
Conv: 371KAll: 2.3M
Conv: 2.2MAll: 11MAll: 136M
Conv: 14
P Conv 15% Conv 15% Conv 15%Conv 15%
FC 20%
D. Pruning heuristics
Pruning heuristics of our proposed method is motivated by
previous work [21], [22]. We describe different aspects of our
pruning heuristics below:
•Pruning can either be structured or unstructured. Struc-
tured pruning discards weights in clusters, i.e., by remov-
ing layers/neurons (weight columns), filters, or channels
in CNNs. Unstructured pruning does not follow any
specific rule, where weights are pruned in a scattered
manner to bring sparsity. Since COLTs are generated by
selecting the lower magnitude weights from the entire
network, they can prune weights from multiple random
layers in one pruning round. Therefore, our strategy
follows unstructured pruning.
•Based on the strategy used, pruning can be local or
global. Local pruning is about pruning a fixed fraction
of weights from each layer. In contrast, at each pruning
round, global pruning involves pruning a fixed fraction
of weights from the entire network allowing each layer
to have a different percentage of weights to be discarded.
We identify our pruning strategy as global since we treat
the whole network at once.
•Pruning can be performed either once (one-shot pruning)
or iteratively (iterative pruning). Pruning a significant
fraction of the weights in one round can be noisy and
eliminate important weights. To overcome this problem,
we follow an iterative approach, i.e., several iterations
of alternating train-prune cycles with a fraction of the
weights pruned at once. This generates significantly
better-pruned models and winning tickets [21], [23].
•Our approach uses a novel heuristic to calculate winning
tickets from overlapping weights between two models
trained on different segments of a dataset. It makes the
winning tickets more robust to class variation. Also, it
increases the generalizing capability of the tickets while
transferring to another dataset.
IV. E XPERIMENTS
A. Setup
Datasets: We perform experiments on three image datasets.
(a)Cifar-10: Cifar-10 dataset contains 60000 color images
(32×32 resolution) belonging to ten classes having 6000
images per class [25].(b)Cifar-100: Cifar-100 [25] dataset is similar to Cifar-10 in
terms of image resolution and the total number of images.
However, it has 100 classes with 600 images per class.
(c)Tiny ImageNet: Tiny ImageNet dataset [44] is a scaled-
down version of ImageNet introduced in the MicroImagenet
classification challenge. It has 100000 color images (64 ×64
resolution) with 200 classes and 500 images per class. (d)Ima-
geNet: The ImageNet [26] is a large-scale dataset designed for
use in visual object recognition software research. Originally
introduced in the Large Scale Visual Recognition Challenge
(ILSVRC), it contains over 14 million images and spans 1000
classes. Each class in ImageNet typically contains several
hundred to over a thousand images, providing a diverse and
extensive sample of each category. (e)Pascal VOC: The Pascal
VOC (Visual Object Classes) [45] dataset is a well-known
dataset for benchmarking object detection and segmentation
tasks. It contains images of various sizes, taken from natural
scenes, with each image labeled with objects of one or more of
20 different classes (such as person, car, bird, etc.). The dataset
contains 9,963 images and is divided into three sets: training,
validation, and testing. The training set is used to train the
models, the validation set is used to tune the hyperparameters,
and the testing set is used to evaluate the final performance
of the models. Each image in the dataset is annotated with
ground-truth bounding boxes for the objects in the image and
the class label for each object. All the datasets have been
systematically partitioned into Ndistinct subsets to facilitate
a more specialized analysis, ensuring that each subset contains
an equal number of classes. This partitioning strategy deviates
from [22], which distributed the dataset into parts where all
classes were present equally in each partition.
Evaluation Process: To evaluate our models, we have used
accuracy, Eq. 1 (in percent), which is a standard evaluation
metric for image classification. Additionally, to quantitatively
determine the sparsity of our models, we have used prune rate,
Eq. 2 (in percent) that gives the percentage of parameters that
are zero out of all the parameters of a model. Our iterative
pruning algorithm runs until a desired PruneRate is reached.
Each iteration of the pruning algorithm runs for fifty epochs,
and the best model (with the lowest validation loss) is selected.
Accuracy =Number of correct predictions
Total number of predictions×100 (1)
PruneRate =Number of zero params
Total number of params×100 (2)
For object detection tasks, we train 50 epochs similar to
the image classification tasks. We calculate the commonly
used mean Average Precision (mAP) to evaluate the detection
framework.
Model Architecture: We have used three models for con-
ducting experiments: Conv-3, ResNet-18, and MobileNetV2
(see Table III). We have used the Conv and ResNet archi-
tectures since they were tested in LTH [21]. We have used
the MobileNet architecture because it is a lightweight deep-
learning model with few parameters and high classification
performance. Conv-3 model architecture is a scaled-down
version of VGG [46] consisting of about 0.37M parameters.

--- PAGE 8 ---
8 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
80 90 1005055606570
5101520253035
Accuracy-L TH
Accuracy-COL T
Pruning Rounds-L TH
Pruning Rounds-COL T(a) Conv3: CIF AR-10
Sparsity(%) Test accuracy(%)
Pruning Rounds
70 80 90 10020406080
1020304050(b) MobilNetV2: CIF AR-10
Sparsity(%) Test accuracy(%)
Pruning Rounds
70 80 90 1008990919293
5101520(c) ResNet18: CIF AR-10
Sparsity(%) Test accuracy(%)
Pruning Rounds
70 80 90010203040
10203040506070(d) Conv3: CIF AR-100
Sparsity(%) Test accuracy(%)
Pruning Rounds
70 80 900204060
204060(e) MobilNetV2: CIF AR-100
Sparsity(%) Test accuracy(%)
Pruning Rounds
80 90 10055606570
510152025(f) ResNet-18: CIF AR-100
Sparsity(%) Test accuracy(%)
Pruning Rounds
65 70 75 80 85 9001020304050
5101520253035(g) MobilNetV2: T inyImageNet
Sparsity(%) Test accuracy(%)
Pruning Rounds
80 90 10045505560
5101520(h) ResNet18: T inyImageNet
Sparsity(%) Test accuracy(%)
Pruning Rounds
0 20 40 6066676869
1234567(i) ResNet18: ImageNet
Sparsity(%) Test accuracy(%)
Pruning Rounds
Fig. 5: Performance comparison of LTH and our proposed COLT in terms of accuracy (left y-axis) and pruning rounds (right
y-axis) while the sparsity increases (in x-axis). We have used three different datasets (Cifar-10, Cifar-100, TinyImageNet) and
three different model architectures (Conv3, MobileNetV2, ResNet18). The red lines denote LTH, and the green lines denote
COLT. The dotted curves (- - -) represent the pruning rounds taken to reach a desired sparsity, while the solid curves (—)
represent the accuracy as the sparsity increases. Observing the solid curves, we can see that the performance of both LTH and
COLT starts decreasing when higher sparsities are achieved. However, from dotted curves, we notice that the pruning rounds
required by COLT and LTH to reach a particular sparsity are not the same. COLT requires fewer pruning rounds than LTH
to achieve the same sparsity while maintaining similar accuracy. Concretely, for any sparsity level, COLT-based pruning costs
fewer pruning rounds in comparison to LTH. Therefore, COLT can prune faster than LTH while maintaining decent accuracy.
It has three convolutional layers, followed by one linear
output layer. Besides, we include a batch normalization layer
followed by max-pooling after each convolution layer. The
ResNet-18 architecture consists of 72 layers with 18 deep
residual connection layers [23] and about 11.1M parameters.
MobileNetV2 [24] architecture consists of 53 layers with 19
residual bottleneck layers and about 2.2M parameters. We use
Xavier or Glorot weight initialization technique for all the
models [47]. For all three models, the last layer is a fully-
connected output layer from the global average pool of the
final convolution layer to the number of output classes as done
in [21], [23]. The last convolutional layer was globally average
pooled to ensure that we could transfer the weights of a model
trained on one dataset to another dataset of a different input
shape without bringing any modifications to the model. We
trained all three models on Cifar-10 and Cifar-100, whereasConv-3 is excluded from Tiny ImageNet because it is too
shallow to train on Tiny ImageNet.
We also perform COLT-based pruning for object detection
tasks based on Faster-RCNN [48] and Pascal VOC dataset
[45]. The Faster-RCNN model comprises two main com-
ponents: a Region Proposal Network (RPN) and a Fast R-
CNN network. The RPN generates region proposals by sliding
a small network over the convolutional feature map of the
input image. The Faster R-CNN network then classifies the
proposed regions and refines their bounding boxes. We use
VGG16 [49] as the backbone, a shared feature extraction
network between RPN and the detection network containing
13 convolutional layers. The Faster-RCNN network consists
of an ROI pooling layer and two fully connected layers for
classifying the proposed regions and refining their bounding
boxes. This model had 136 million parameters, making it a

--- PAGE 9 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 9
60 80 10040506070
Ticket Source
Accuracy- Morcos et al.
Accuracy - COL Ta) Conv3: T ransfering CIF AR-100 to CIF AR-10
Pruning Rate (%)Test Accuracy (%)
60 80 10020406080
Ticket Source
Accuracy- Morcos et al.
Accuracy - COL Tb) MobileNetV2 : T ransfering CIF AR-100 to CIF AR-10
Pruning Rate (%)Test Accuracy (%)
50 60 70 80 90 10066687072
Ticket Source
Accuracy- Morcos et al.
Accuracy - COL Tc) ResNet18 : T ransfering T inyImageNet to CIF AR-100
Pruning Rate (%)Test Accuracy (%)
50 60 70 80 9075808590
Ticket Source
Accuracy- Morcos et al.
Accuracy - COL Td) MobileNetV2 : T ransfering T inyImageNet to CIF AR-10
Pruning Rate (%)Test Accuracy (%)
50 60 70 80 90 10090919293
Ticket Source
Accuracy- Morcos et al.
Accuracy - COL Te) ResNet18 : T ransfering T inyImageNet to CIF AR-10
Pruning Rate (%)Test Accuracy (%)
50 60 70 80 9050556065
Ticket Source
Accuracy- Morcos et al.
Accuracy - COL Tf) MobileNetV2 : T ransfering T inyImageNet to CIF AR-100
Pruning Rate (%)Test Accuracy (%)
Fig. 6: Performance comparison of COLT with Morocos et al. [22] when transferring tickets calculated from one dataset
to another. Both methods’ performance declines slightly when the tickets reach high sparsities. However, when tickets are
transferred from a large dataset (TinyImageNet) to small datasets (Cifar-10, Cifar-100), COLT (green curve) consistently
outperforms Morcos et al. (red curve) at high sparsities.
large and computationally expensive model.
Implementation details: After training and pruning, weights
must be reset to their initial values at the beginning of
training (training iteration 0) to generate winning tickets [21].
However, Frankle et al. [21] have found that this only works
for shallow models. For deeper models, a learning rate warmup
is necessary, along with weight resetting to generate winning
tickets. So, our experiments use a learning rate warmup for the
first epoch. We globally prune only the convolutional layers
at a rate of 0.2 (20%) for LTH and 0.15 (15%) for COLT per
iteration. Similarly, we do not prune the linear output layers
as they comprise only a tiny fraction of the whole network.
Each model on cifar10,cifar100, and Tiny Imagenet datasets
is trained for 50 epochs with a batch size of 64 and a learning
rate of 0.1. The learning rate is annealed by a factor of 5
at 25, 35, and 45 epochs. While training Imagenet with the
ResNet-18 model, we trained the model for 90 epochs with a
batch size of 512 and a learning rate of 0.2. We have used a
learning rate schedular to anneale the learning rate at 30,60
and 80 epochs by a factor of 5. All the models, except Conv-
3, are trained using Stochastic Gradient Descent (SGD) with
a momentum of 0.9 and weight decay of 0.0005. Conv3 is
trained using an Adam optimizer with betas of 0.9 and 0.999
and weight decay of 0.0001. For the object detection task, we
implement the Faster-RCNN model. To train the model, we
use the Adam optimizer with a learning rate of 0.0001 and
a cosine annealing learning rate scheduler with a minimum
learning rate of 1.02e-06. We use a batch size of 4 and train
the model for 50 epochs. The input shape is [600,600], and we
prune 15% of the weights of convolutional layers and 20% of
the weights of fully connected layers per pruning round. Weimplement this work with PyTorch framework using a single
RTX 3090 GPU.
B. Compared methods
We compare our work with the three methods discussed as
follows: (1) Unpruned network: The unpruned network is the
original CNN (Conv-3/Mobilenetv2/Resnet-18) trained until
convergence, where no pruning strategy is considered during
the learning process. We initialize their weights using the
Xavier/Glorot weight initialization technique [47]. We assume
that an unpruned network is generally overparameterized and,
therefore, a pruning method is needed to discard unnecessary
weights. (2) Lottery Ticket Hypothesis (LTH) [21]: To
compare with LTH, we also generated LTH tickets. We pruned
20% weights after each training iteration and then rewound
them to their initial state. We then compare their performance
with COLT tickets in Sec. IV-C. [21] and [22] have shown that
random tickets with and without masking preserved results in
lower performance than winning tickets with equal parameters.
As a result, we have omitted using random tickets while
evaluating COLT tickets and directly compared COLT tickets
with LTH tickets. (3) Morcos et al. [22]: Winning tickets
are computationally expensive to generate because of the
repetitive train-prune-rewind cycle. Morcos et al. have shown
that once generated from a dataset, winning tickets can be
transferred to another dataset, bypassing the need to find
separate winning tickets for each dataset. Concretely, they
observed that transferring the winning tickets from one dataset
to another can achieve almost similar performance to the
winning tickets generated on the same dataset. They have
shown that winning tickets generalize well in the natural im-

--- PAGE 10 ---
10 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
age domain. Moreover, they have also demonstrated winning
tickets generated from large datasets consistently transferred
better than those generated using small datasets.
COLT vs. other network compression methods: Although
numerous techniques for network compression are available
for CNNs, such as knowledge distillation [7], quantization
[50], and structured pruning [17], these strategies often neces-
sitate complete pre-training of the network or yield models
that must undergo fine-tuning . This is different from our
objective of finding trainable sparse subnetworks from scratch .
Therefore, in line with previous work [51], [52], we do not
compare our work with other network compression methods.
COLT’s primary contribution lies in improving the ticket-
finding process itself while maintaining the key advantage of
LTH. This focus on initialization-time sparsity and trainability
sets lottery ticket methods apart in the broader landscape of
network compression.
C. Main Results
We generate COLT and LTH tickets for ResNet-18 and
MobileNetV2 on Cifar-10, Cifar-100, and Tiny ImageNet. For
Conv-3, we obtain tickets for Cifar-10 and Cifar-100 only
following [21]. For the weight transferability experiment, we
transfer the tickets calculated from Tiny ImageNet to Cifar-10
and Cifar-100. Again, we transfer the tickets from Cifar-100
to Cifar-10. All the experiments generating COLT and LTH
tickets are repeated multiple times with a different random
initialization, and we reported the best scores.
Results on Cifar-10: We observe that the accuracy of COLT
and LTH are level pegging for ResNet-18. For Conv-3 tickets
(Fig. 5(a)), we notice a similar picture with both COLT
and LTH performing equally. For MobileNetV2 (Fig. 5(b)),
we see that the performance of COLT and LTH are similar
till the pruning rate of 70%, and then the performance of
COLT tickets starts decreasing compared to LTH tickets. For
both methods, the performance of the tickets drops steeply
when the pruning rate is higher than 95% for Conv-3, 90%
for MobileNetV2, and 96% for ResNet-18. For all tickets,
COLT consistently requires fewer pruning rounds than LTH to
generate a particular sparse ticket. For example, to reach 70%
sparsity, COLT needs 3, 1, and 2 fewer rounds than LTH on
Conv-3, MobileNetV2, and ResNet-18, respectively. Similarly,
to achieve 90% sparsity, COLT requires 4, 2, and 3 fewer
rounds than LTH on Conv-3, MobileNetV2, and ResNet-18,
respectively. Lastly, to reach 98% sparsity, COLT needs 9,
2, and 5 fewer rounds than LTH on Conv-3, MobileNetV2,
and ResNet-18, respectively. Because of the pruning of non-
overlapping weights in COLT, it consistently requires fewer
rounds than LTH to generate sparse winning tickets. COLT
also retains a similar performance to LTH as these non-
overlapping weights are unimportant, and excluding those
weights does not affect the overall performance much.
Results on Cifar-100: Observing Fig.. 5(f), the performance
of COLT and LTH tickets is similar until a prune rate of 80%
for ResNet-18. However, after 80% pruning, the COLT-based
tickets perform better than LTH tickets. For MobileNetV2
(Fig. 5(e)) and Conv-3 Fig. 5(d), the performance of COLTand LTH tickets are almost similar, with LTH slightly outper-
forming COLT at some prune rates. In addition, the accuracy
of the LTH and COLT tickets starts to drop drastically when
the pruning rate is greater than 95% for Resnet-18, 85% for
MobileNetV2, and 90% for Conv-3. Besides, COLT regularly
generates winning tickets in fewer pruning rounds than LTH.
To reach 70% sparsity, COLT requires 3, 2, and 2 fewer
rounds than LTH on Conv-3, MobileNetV2, and ResNet-18,
respectively. To achieve 90% sparsity, COLT requires 6, 6,
and 3 fewer rounds than LTH on Conv-3, MobileNetV2,
and ResNet-18, respectively. Lastly, to reach 98% sparsity,
COLT needs 9, 8, and 6 fewer rounds than LTH on Conv-
3, MobileNetV2, and ResNet-18, respectively. This becomes
possible due to COLT’s aggressive non-matching weights
pruning strategy at each round. Moreover, COLT preserves
accuracy as the non-matching weights are insignificant and do
not hamper the accuracy.
Results on Tiny ImageNet: Analyzing Fig. 5(g) and 5(h), we
can observe that the performance of COLT and LTH tickets
are similar for most of the tickets, with COLT tickets slightly
performing better than LTH Tickets at some sparsities. Apart
from that, both tickets’ accuracy drops significantly from 85%
for ResNet-18 and from 75% for MobileNetV2. Looking at the
pruning rounds, we can see that COLT always generates sparse
tickets at fewer iterations than LTH. To reach 70% sparsity,
COLT requires two fewer rounds than LTH on MobileNetV2
and ResNet-18. To reach 90% sparsity, COLT requires 9 and
3 fewer rounds than LTH on MobileNetV2 and ResNet-18,
respectively. Finally, to reach 98% sparsity, COLT requires 4
and 6 fewer rounds than LTH on MobileNetV2 and ResNet-
18, respectively. As COLT prunes a higher rate of redundant
weights than LTH at each iteration, it can generate winning
tickets in fewer rounds than LTH without compromising
accuracy.
Results on ImageNet: In an extended analysis on ImageNet
using ResNet-18, both COLT and LTH methods maintained
near-baseline accuracy after significant model reductions. Ini-
tially, no loss in accuracy (69.7%) was observed without prun-
ing, as expected. With increased sparsity, COLT’s accuracy
decreased slightly to 69.3% (at a 26.5% pruning rate) and
further to 66.1% (at a 70.11% pruning rate). LTH showed a
similar decline, reaching 66.8% accuracy (at a 70.52% pruning
rate with a slight fluctuation). The efficiency of COLT is
evident as it achieves 70% pruning in just four rounds, two
rounds fewer than LTH. This underscores COLT’s aggressive
yet effective pruning, offering latency and model efficiency
advantages. The results imply that on large-scale datasets,
eliminating weights (inconsistent across models) trained on
different subsets of classes and those with lesser magnitudes
has a minimal impact on model performance. This hints at
the redundant nature of these weights in maintaining high
accuracy levels.
Impact of Non-uniform (Non-IID) Data Partition: To
create a non-IID data partition, we implemented COLT niid,
randomly removing 0-30% samples from each class. This
approach introduces data imbalance across classes, as some
classes retain all their samples while others lose up to 30%.
The resulting dataset maintains all original classes but with

--- PAGE 11 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 11
TABLE IV: Ablation study on multiple partitions (COLT-2,
COLT-3 and COLT-4) using ResNet18 architectures.
Method Acc. ↑ Pruning ↑ Latency ↓ # of Pruning ↓
(%) Rate (%) (minutes) RoundsCIFAR10Unpruned 94.4 0 - -
LTH 92.3 97.7 320 17
COLT-2 92.4 97.7 276 12
COLT-3 92.2 97.3 225 9
COLT-4 89.2 97 148 6CIFAR100Unpruned 73.3 0 - -
LTH 66.2 97.3 479 17
COLT-2 68.4 97.4 355 12
COLT-3 68.0 97.3 236 8
COLT-4 67.7 97.2 188 6TinyImageNetUnpruned 59.9 0 - -
LTH 53.2 97.3 2625 18
COLT-2 53.9 97.4 1756 12
COLT-3 53.4 97.6 1266 8
COLT-4 53.7 97 930 6ImageNetUnpruned 69.7 0 - -
LTH 66.8 70.5 10320 6
COLT-2 66.1 70.1 6960 4
COLT-4 55.7 74.1 5760 3
TABLE V: Performance comparison of LTH and COLT un-
der IID and non-IID data distributions on CIFAR-100 using
ResNet-18. Values in parentheses show the accuracy difference
between IID and non-IID scenarios.
Pruning Rate (%)LTH COLT
IID non-IID IID non-IID
85.2 71.55 70.39 (-1.16) 72.84 71.33 (-1.51)
89.1 71.42 69.88 (-1.54) 72.88 70.64 (-2.24)
92.1 70.59 69.28 (-1.31) 71.91 70.30 (-1.61)
varying sample sizes, creating a non-uniform distribution
that challenges the model’s ability to learn from irregularly
distributed data. We tested both COLT and LTH using ResNet-
18 on CIFAR-100, comparing their performance at various
pruning rates under both IID and non-IID settings. The results,
shown in Table V, indicate that both methods experience some
performance degradation under non-IID conditions. At the 89.
1% pruning rate, LTH shows a decrease of 1.54 percentage
points (from 71.42% to 69.88%), while COLT experiences a
reduction of 2.24 percentage points (from 72.88% to 70.64%).
Despite this degradation, both methods maintain reasonable
performance levels, with COLT showing comparable resilience
to LTH in handling data heterogeneity.
D. Multiple Partitions
In this section, we evaluate COLT’s performance with multi-
ple dataset partitions, comparing COLT-2, COLT-3, and COLT-
4, representing the method for 2, 3, and 4 partitions, respec-
tively, against the widely used LTH method using ResNet18
models on multiple datasets. Detailed results are shown in
Table-IV. The results indicate that the partition-based pruning
approach works differently for datasets of different sizes and
complexities. COLT-3 (partitioning three parts of the dataset)
has shown a balanced performance with minor accuracy losses
and good pruning rates across CIFAR-10 (92.2% accuracy,
97.3% pruning), CIFAR-100 (68.0%, 97.3%), and Tiny Im-
ageNet (53.4%, 97.6%). COLT-4, however, shows reducedaccuracy on CIFAR-10 (88.7%) compared to LTH (91.3%)
due to the small size of CIFAR-10, which makes it hard to
maintain informative weights with increased partitioning. Con-
versely, for CIFAR-100, COLT-4’s accuracy (67.7%) is slightly
lower than COLT-3’s(68.0) but still above LTH’s (66.2%).
For Tiny ImageNet, COLT-4 (53.7%) slightly outperforms
LTH (53.2%), indicating more complex datasets can preserve
essential weights even with more partitions. Suggesting larger
datasets can better handle more partitions. On ImageNet, the
most challenging dataset, aggressive pruning with COLT-4
leads to a significant accuracy drop (55.7%) despite a high
pruning rate (74.01%) and the lowest latency (5760 minutes),
implying a loss of critical weights. It is likely due to the loss
of weights critical for underrepresented features within the
dataset [53]. COLT-2, on the other hand, achieves a closer
accuracy to LTH (66.1% vs. 66.8%) with a better pruning
rate (70.11%) and lower latency (6960 minutes), showing that
moderate pruning strategies like COLT-2 can retain important
features on complex datasets, striking a balance between
model size, efficiency, and performance.
E. Transferring LTH & COLT Tickets Across Datasets
The iterative process of ticket generation can be costly.
To mitigate this problem, Morcos et al. [22] showed that
winning tickets (pruned subnetwork) generated on larger, more
complex datasets could generalize substantially better than
those generated on small datasets. In other words, we do not
need to create tickets for each dataset. Tickets generated from
one larger dataset can be transferred/employed again to train
the same network for a smaller dataset. For this reason, we
perform experiments to transfer the COLT and LTH tickets
of Tiny ImageNet to Cifar-10 and Cifar-100. Since we are
using datasets with different input shapes, there will be a
weight mismatch when transferring weights of the linear layer.
To overcome this problem, we have globally average pooled
the output from the last convolutional layer before passing it
through the dense output layer. [22] further showed that just
increasing the number of classes while keeping the dataset size
fixed can significantly improve the generalization capability
of winning tickets. To verify this point, in our work, we have
transferred the COLT and LTH tickets of Cifar-100 to Cifar-10
since they are the same size, but Cifar-100 has more classes
than Cifar-10.
Observing Fig. 6, we can see that the accuracy slowly
deteriorates as COLT and Morcos et al. reach high sparsities.
However, COLT consistently performs better than Morcos et
al. at high sparsities when TinyImageNet tickets are transferred
to Cifar-10 & Cifar-100. Additionally, both methods are on par
and work well when Cifar-100 tickets are transferred to Cifar-
10 for all sparsities. Therefore, we claim that COLT tickets
have a high generalization capacity and transferability to other
datasets of a similar domain.
One can compare the performance of regular tickets (COLT)
with transferred tickets by analyzing Fig. 5 and 6. We observe
that the performance trend for regular and transferred tickets
is similar across architectures and datasets. If a regular ticket
performs well on a dataset, the transferred ticket performs

--- PAGE 12 ---
12 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
TABLE VI: State-of-the-art results describing the performance comparison of tickets generated by the original unpruned
network, LTH, and COLT. The tickets were generated and evaluated on TinyImageNet and then transferred to Cifar-10 and
Cifar-100. We compare sparse tickets from three methods on ResNet-18 and MobileNetV2 architectures. ↑(↓) means higher
(lower) is better. “-” denotes results that are not applicable there.
Model: ResNet18 and Dataset: Tiny ImageNet
MethodAcc.↑ Pruning ↑ Latency ↓ No of Pruning ↓
(%) Rate (%) (minutes) Rounds
Unpruned 59.9 0 - -
LTH 53.2 97.3 2625 18
Ours (COLT) 53.9 97.2 1756 12
Tickets transformed from TinyImageNet to Cifar10
Unpruned 93.4 0 - -
Morcos et al. [22] 89.9 97.1 - -
Ours (COLT) 90.5 97.1 - -
Tickets transformed from TinyImageNet to Cifar100
Unpruned 73.2 0 - -
Morcos et al. [22] 64.8 97.3 - -
Ours (COLT) 65.2 97.4 - -Model: MobileNetV2 and Dataset: Tiny ImageNet
Acc.↑ Pruning ↑ Latency ↓ # of Pruning ↓
(%) Rate (%) (minutes) Rounds
56.1 0 - -
54.7 70.3 856 7
55.2 71.0 354 5
Tickets transformed from TinyImageNet to Cifar10
90.6 0 - -
88.9 78.0 - -
89.2 78.8 - -
Tickets transformed from TinyImageNet to Cifar100
65.8 0 - -
64.1 74.1 - -
65.5 74.9 - -
well, and vice versa. The bottom line is that we can generate
COLT tickets from one dataset and then use the same ticket
to train another dataset of a similar domain without compro-
mising network performance.
F . State-of-the-art comparison
Table VI compares the results of unpruned network, COLT,
and LTH methods regarding no. of pruning rounds, accuracy,
and latency. For the transferability experiment, tickets were
generated using TinyImageNet and transferred to Cifar10
and Cifar100. Echoing [22], transferring tickets from larger
(TinyImageNet) to smaller datasets (Cifar10) improves perfor-
mance. Our studies on ResNet-18 and MobileNetV2 support
this. Notably, ResNet-18 maintains accuracy with 97.2% of
parameters pruned, showing that just 2.8% of weights are
needed for satisfactory accuracy. COLT outperforms LTH in
accuracy at high sparsity with fewer pruning rounds and less
training time. Yet, it falls short of the unpruned network’s
accuracy by 5-6%, likely because pruning 97% of weights
might remove some crucial ones, leading to accuracy drops. In
addition, there is a high risk of layer collapse for unstructured
at high sparsity. Tanaka et al. [43] state that iterative magnitude
pruning can avert this by enforcing a conservation law through
gradient descent, which raises the magnitude scores for larger
layers throughout the pruning process. In this regard, COLT
performs comparatively better than LTH, even if it receives
fewer iterations to prune at high sparsity. Therefore, we claim
that COLT can retain “good weights” even after pruning at
high sparsities. On the other hand, MobileNet has nine million
fewer weights than ResNet18. As a result, unlike an unpruned
network, neither LTH nor COLT achieves par accuracy at high
sparsity. However, the unpruned network and COLT (with 71%
pruning) hold similar accuracy. Likewise, in ResNet18, we
generate tickets faster with fewer pruning rounds than LTH. In
this case, LTH requires eight pruning rounds and 856 minutes
for a 70.3% sparse ticket with 54.7% accuracy on TinyIma-
geNet. In contrast, COLT needs a mere five rounds and 354
minutes for a 71% sparse ticket with 55.2% accuracy. For
MobileNetV2 and ResNet18, COLT is significantly faster thanLTH, with fewer rounds and quicker gradient computation.
COLT’s tickets are sparser. Hence, fewer gradients are needed,
allowing for much faster ticket generation than LTH.
A similar trend is observed when the tickets are transferred
to Cifar-10 and Cifar-100 for medium sparsity tickets (prune
rate around 80%). Particularly for MobileNetV2, both COLT
and Morcos et al. [22] have accuracies close to the original
unpruned network. It implies that we can adopt a network
that has already been pruned on one dataset and transfer it to
another with an accuracy comparable to an unpruned network.
Moreover, we do not need additional resources to prune the
network for each individual dataset. In contrast, for high-
sparsity tickets, the accuracy decreases by about 3% when
transferred to Cifar-10 and by about 8% when transferred to
Cifar-100. However, COLT appears to become more generic
than Morcos et al. [22]. This is because COLT retains the
overlapping weights of two networks trained on two one-half
splits of a dataset, making the COLT tickets more robust to
image variations.
G. Beyound classification
In addition to image classification tasks, we also evaluated
the effectiveness of COLT on the Pascal VOC object detection
task using the Faster-RCNN model with the VGG backbone.
This task is more complex than image classification, as it
involves identifying the object and its location in the image.
The model is relatively large, with a total of 113 million
parameters. Among them, 13 million parameters are attributed
to the CNN layers subject to pruning to reduce computational
resources. In Table VIII, we compare the performance of
COLT with LTH on the Pascal VOC dataset and find that
COLT outperforms LTH in terms of pruning rounds, latency,
and accuracy. We also report per-class mAP in Table VII
to show that after 68% pruning, individual class mAPs are
similar to the unpruned network. Our results demonstrate
that COLT and LTH can achieve similar accuracy as the
Unpruned network, with a maximum mAP of 63.8% and
63.7%, respectively, while pruning up to 79% of the network.

--- PAGE 13 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 13
TABLE VII: Per-class mAP of PASCAL VOC dataset after 68% of pruning.aero.
bicycle
bird
boat
bottle
bus
car
cat
chair
cow
table
dog
horse
mbike
person
plant
sheep
sofa
train
tv
Unpruned 64.2 77.4 53.2 56.5 33.7 77.8 82.1 69.4 46.4 56.8 55.3 67.3 82.4 82.1 76.3 36.9 49.8 54.7 79.7 65.3
LTH 63.4 79.1 51.4 56.7 32.4 77.7 82.1 71.3 48.1 60.4 55.7 68.6 81.4 81.5 77.3 31.3 46.5 54.5 78.7 68.4
COLT 63.5 77.3 54.1 60.5 34.9 77.4 81.5 69.9 44.7 61.4 55.3 67.1 82.2 83.1 77.4 35.9 52.7 52.4 77.7 67.8
TABLE VIII: Pruning the Faster-RCNN model while perform-
ing object detection tasks on the PASCAL VOC dataset.
MethodmAP↑ Pruning ↑ Latency ↓ No of pruning ↓
(%) Rate (%) (hours) Rounds
Unpruned 63.4 0 - -
LTH 60.8 36.0 26.0 2
COLT 60.6 32.2 14.3 1
LTH 61.6 48.8 39.3 3
COLT 62.5 53.5 28.8 2
LTH 63.4 67.0 65.5 5
COLT 63.8 68.6 43.1 3
LTH 63.7 79.0 91.7 7
COLT 63.3 79.1 58.0 4
01122334 45 5 6 6 7 8 9
23.7 48.0 60.3 72.9 79.3 85.8 89.1 92.4 94.1 95.9 96.8 97.7 98.1 98.6 98.9 99.1020406080100
0510152025ResNet18 - Cifar100
Similarity Rate
Pruning Rounds-COL T
Pruning Rounds-L TH
Sparsity ( % )Pruned W eights Similarity Rate( % )
Pruning Rounds
Fig. 7: The similarity (blue bars) of pruned weights between
sparse tickets calculated by COLT and LTH method. With
the increase of sparsity, both kinds of methods prune similar
weights. To achieve this, COLT requires fewer training rounds
than LTH (red vs. green curve).
This indicates that our proposed method can achieve a signif-
icant compression level with negligible accuracy loss. When
comparing COLT with LTH, we observe that COLT achieves
similar or better accuracy than LTH, requiring significantly
fewer pruning rounds and less latency.
H. Discussion
Weight Similarity Analysis: Fig. 7 depicts a comparison
between LTH and COLT in terms of pruned weights similarity
and pruning rounds for the ResNet-18 model on the Cifar-
100 dataset. The X-axis represents the sparsities in percentage.
The Y-axis on the left represents the pruning rate similarity
of unpruned weights, and the Y-axis on the right represents
the pruning rounds taken to generate a ticket of the same
sparsity. The blue bars represent the similarity in the pruned
weights between LTH and COLT tickets. The similarity is
calculated at the beginning of each pruning round. COLTand LTH tickets are initialized with the same weights. The
similarity rate is the number of common pruned parameters
between COLT and LTH divided by the total no. of initial
parameters. Here, ‘common’ implies that LTH and COLT
pruned the same weights. The red and green line joining the
bars represents the pruning rounds required by COLT and
LTH, respectively. The numbers on the top of the bars depict
the additional pruning rounds required by LTH compared to
COLT. We find substantially fewer identical remaining weights
between COLT and LTH during the early stages. However, as
the prune rate increases, the similarity between the remaining
weights of LTH and COLT also increases. For example, at
the 28% pruning rate, the similarity of remaining weights is
20%. Subsequently, the similarity of remaining weights jumps
to 50% when the sparsity of tickets reaches around 48%. It
implies that identical weights between COLT and LTH start
getting pruned while the weight matrix becomes sparse. In
other words, LTH and COLT essentially prune similar weights
to generate a sparse winning ticket. However, COLT (solid red
line) requires fewer pruning rounds than LTH (solid green line)
to create a ticket of the same sparsity. For example, to reach
a sparsity of about 89%, LTH requires three more pruning
rounds than COLT, making the LTH process time-consuming.
Performance Analysis: Analyzing all results, we can see
that COLT tickets from large models like ResNet-18 are
comparable or sometimes superior to LTH tickets across
dataset sizes and maintain high accuracy when transferred
to different datasets, even at high sparsities. On the other
hand, MobileNetV2 COLT tickets tend to underperform on
small datasets such as Cifar-10 and Cifar-100, shown in Fig.
5 (b), (c), and (e) relative to LTH tickets. The reason is
possibly due to reduced feature diversity after splitting small
datasets into smaller partitions, which may lead to difficulty
in identifying essential features during pruning. However, their
accuracy is comparable when created on larger datasets like
Tiny ImageNet. When transferred from Tiny ImageNet to
smaller datasets, MobileNetV2 COLT tickets match or exceed
LTH performance. Conv-3 COLT tickets also maintain LTH-
level performance across datasets, including when Cifar-100
tickets are applied to Cifar-10. It follows that dividing a
dataset into Nparts results in a smaller dataset, which is
subsequently trained using a model with the same number
of weights as the full dataset. Consequently, the model may
utilize its full potential for the smaller dataset to determine
the optimal features and weights for the training samples.
Aggressive pruning techniques such as COLT-2, COLT-3, or
COLT-4 thus can preserve the best weights to hold accuracy
as, or occasionally better than, LTH across the models and
datasets.
In object detection, COLT models perform competitively

--- PAGE 14 ---
14 JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020
with LTH models at various sparsity levels, with similar
mAP scores, underscoring COLT’s pruning effectiveness. This
indicates COLT’s potential as a viable alternative to LTH
for pruning, capable of generating sparser models without
performance loss and possibly offering simpler and less costly
pruning.
Addressing Overfitting: While neural networks inherently
face overfitting challenges, COLT’s iterative pruning approach
actually serves as a natural regularization mechanism that
helps prevent overfitting rather than causing it [54]. By pro-
gressively removing redundant connections while maintaining
only the most essential pathways, COLT effectively reduces
the model’s capacity to memorize training data. Specifically,
COLT employs three key mechanisms that contribute to the
prevention of overfitting: (1)the retention of only high-
magnitude weights while zeroing out lower-magnitude ones
serves as implicit regularization [55], (2)the overlapping
weight selection process across multiple data partitions ensures
that only consistently important weights are retained, and (3)
the resulting sparse architecture inherently limits the model’s
capacity to overfit [56]. Our empirical results support this
claim. The performance gap between training and validation
accuracy remains stable or even decreases as pruning pro-
gresses, particularly at higher sparsity levels (85-92%). Robust
performance under non-IID conditions (see Table V) further
demonstrates that COLT-pruned networks learn generalizable
features rather than overfitting to specific data distributions.
V. C ONCLUSION
In this paper, we proposed a new set of lottery tickets
termed COLT to prune deep neural networks. Like tickets
from LTH, COLTs can achieve high sparsity and weights’
transferability across datasets without compromising accuracy.
We compute COLTs by leveraging tickets obtained from
two/four class-wise partitions of a dataset. We notice that
the performance of these tickets is on par with the LTH-
generated lottery tickets in terms of accuracy. Moreover,
COLTs are generated at fewer iterations than LTH tickets.
We validate our claim using both object recognition and
detection tasks. In experiments, we have also demonstrated
that COLTs generated on a large dataset (Tiny ImageNet)
can be transferred to a small dataset (Cifar-10, Cifar-100)
without compromising performance, showing these tickets’
generalizing capability. Future works in this line of investi-
gation may consider COLT’s sparsity and transferability on
deeper architectures (DenseNet/ShuffleNet) and vast datasets
(YouTube-BoundingBoxes/MSCOCO). Moreover, one can ex-
plore different problem setups (like object segmentation or
image captioning) in the context of network pruning.
REFERENCES
[1] M. C. Mozer and P. Smolensky, “Using Relevance to Reduce Network
Size Automatically,” Connection Science , vol. 1, pp. 3–16, 1989.
[2] E. Karnin, “A Simple Procedure for Pruning Back-propagation Trained
Neural Networks,” IEEE Transactions on Neural Networks , vol. 1, no. 2,
pp. 239–242, 1990.
[3] S. Han, J. Pool, J. Tran, and W. J. Dally, “Learning both weights and
connections for efficient neural networks,” in Proceedings of the 28th
International Conference on Neural Information Processing Systems -
Volume 1 , 2015, p. 1135–1143.[4] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning
filters for efficient convnets,” in International Conference on Learning
Representations , 2017.
[5] Y . LeCun, J. Denker, and S. Solla, “Optimal brain damage,” in Advances
in Neural Information Processing Systems , vol. 2, 1989, pp. 598–605.
[6] B. Hassibi and D. G.Stork, “Second Order Derivatives for Network
Pruning: Optimal Brain Surgeon,” Adv Neural Inform Proc Syst , vol. 5,
10 1992.
[7] G. Hinton, O. Vinyals, J. Dean et al. , “Distilling the knowledge in a
neural network,” arXiv preprint arXiv:1503.02531 , vol. 2, no. 7, 2015.
[8] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, “Pruning
convolutional neural networks for resource efficient inference,” in 5th
International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings , 2017.
[9] T.-J. Yang, Y . hsin Chen, and V . Sze, “Designing energy-efficient
convolutional neural networks using energy-aware pruning,” 2017 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) , pp.
6071–6079, 2017.
[10] J.-H. Luo, J. Wu, and W. Lin, “Thinet: A filter level pruning method
for deep neural network compression,” in 2017 IEEE International
Conference on Computer Vision (ICCV) , 2017, pp. 5068–5076.
[11] B. O. Ayinde, T. Inanc, and J. M. Zurada, “Redundant feature pruning
for accelerated inference in deep neural networks,” Neural Netw. , vol.
118, p. 148–158, 2019.
[12] Y . Guo, A. Yao, and Y . Chen, “Dynamic network surgery for efficient
dnns,” Advances in neural information processing systems , vol. 29, 2016.
[13] D. Molchanov, A. Ashukha, and D. P. Vetrov, “Variational dropout
sparsifies deep neural networks,” in ICML , 2017.
[14] K. Yao, F. Cao, Y . Leung, and J. Liang, “Deep neural network compres-
sion through interpretability-based filter pruning,” Pattern Recognition ,
vol. 119, p. 108056, 2021.
[15] M. Zhu and S. Gupta, “To prune, or not to prune: Exploring the efficacy
of pruning for model compression,” in 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April
30 - May 3, 2018, Workshop Track Proceedings , 2018.
[16] M. Mondal, B. Das, S. D. Roy, P. Singh, B. Lall, and S. D. Joshi,
“Adaptive cnn filter pruning using global importance metric,” Computer
Vision and Image Understanding , vol. 222, p. 103511, 2022.
[17] Q. Tian, T. Arbel, and J. J. Clark, “Structured deep fisher pruning for
efficient facial trait classification,” Image and Vision Computing , vol. 77,
pp. 45–59, 2018.
[18] X. Lin, S. Kim, and J. Joo, “Fairgrape: Fairness-aware gradient pruning
method for face attribute classification,” in European Conference on
Computer Vision . Springer, 2022, pp. 414–432.
[19] F. Jia, X. Wang, J. Guan, H. Li, C. Qiu, and S. Qi, “Arank: Toward
specific model pruning via advantage rank for multiple salient objects
detection,” Image and Vision Computing , vol. 111, p. 104192, 2021.
[20] M. Bonnaerens, M. Freiberger, and J. Dambre, “Anchor pruning for ob-
ject detection,” Computer Vision and Image Understanding , p. 103445,
2022.
[21] J. Frankle and M. Carbin, “The lottery ticket hypothesis: Finding sparse,
trainable neural networks,” in 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 ,
2019.
[22] A. S. Morcos, H. Yu, M. Paganini, and Y . Tian, One Ticket to Win
Them All: Generalizing Lottery Ticket Initializations across Datasets
and Optimizers , 2019.
[23] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , pp. 770–778, 2016.
[24] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mo-
bilenetv2: Inverted residuals and linear bottlenecks,” in 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2018, pp.
4510–4520.
[25] A. Krizhevsky and G. Hinton, “Learning Multiple Layers of Features
from Tiny Images,” University of Toronto, Toronto, Ontario, Tech.
Rep. 0, 2009.
[26] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al. , “Imagenet large
scale visual recognition challenge,” International journal of computer
vision , vol. 115, pp. 211–252, 2015.
[27] N. Liu, G. Yuan, Z. Che, X. Shen, X. Ma, Q. Jin, J. Ren, J. Tang, S. Liu,
and Y . Wang, “Lottery ticket preserves weight correlation: Is it desirable
or not?” in International Conference on Machine Learning , 2021.
[28] T. Gale, E. Elsen, and S. Hooker, “The state of sparsity in deep neural
networks,” ArXiv , vol. abs/1902.09574, 2019.

--- PAGE 15 ---
M. HOSSAIN et al. : IEEE JOURNALS OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE 15
[29] N. Lee, T. Ajanthan, and P. Torr, “SNIP: SINGLE-SHOT NETWORK
PRUNING BASED ON CONNECTION SENSITIVITY ,” in Interna-
tional Conference on Learning Representations , 2019.
[30] Z. Allen-Zhu, Y . Li, and Y . Liang, Learning and Generalization in
Overparameterized Neural Networks, Going beyond Two Layers , 2019.
[31] Z. Allen-Zhu, Y . Li, and Z. Song, “A convergence theory for deep learn-
ing via over-parameterization,” in Proceedings of the 36th International
Conference on Machine Learning , ser. Proceedings of Machine Learning
Research, vol. 97, 2019, pp. 242–252.
[32] S. Du and J. Lee, “On the power of over-parametrization in neural
networks with quadratic activation,” in Proceedings of the 35th Interna-
tional Conference on Machine Learning , ser. Proceedings of Machine
Learning Research, vol. 80, 10–15 Jul 2018, pp. 1329–1338.
[33] S. S. Du, X. Zhai, B. P ´oczos, and A. Singh, “Gradient descent provably
optimizes over-parameterized neural networks,” in 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019 , 2019.
[34] B. Neyshabur, R. Tomioka, and N. Srebro, “In search of the real
inductive bias: On the role of implicit regularization in deep learning.”
inICLR (Workshop) , 2015.
[35] B. Neyshabur, Z. Li, S. Bhojanapalli, Y . LeCun, and N. Srebro, “The
role of over-parametrization in generalization of neural networks,” in
International Conference on Learning Representations , 2019.
[36] R. Van Soelen and J. W. Sheppard, “Using Winning Lottery Tickets
in Transfer Learning for Convolutional Neural Networks,” in 2019
International Joint Conference on Neural Networks (IJCNN) . IEEE,
2019, pp. 1–8.
[37] X. Ma, G. Yuan, X. Shen, T. Chen, X. Chen, X. Chen, N. Liu, M. Qin,
S. Liu, Z. Wang, and Y . Wang, “Sanity checks for lottery tickets: Does
your winning ticket really win the jackpot?” in Advances in Neural
Information Processing Systems , vol. 34. Curran Associates, Inc., 2021,
pp. 12 749–12 760.
[38] S. Kornblith, J. Shlens, and Q. V . Le, “Do better imagenet models
transfer better?” in 2019 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , Los Alamitos, CA, USA, jun 2019,
pp. 2656–2666.
[39] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, “A survey
on deep transfer learning,” in Artificial Neural Networks and Machine
Learning – ICANN 2018 , 2018, pp. 270–279.
[40] J. Yosinski, J. Clune, Y . Bengio, and H. Lipson, “How transferable are
features in deep neural networks?” in Advances in Neural Information
Processing Systems , vol. 27, 2014.
[41] B. Zoph, V . Vasudevan, J. Shlens, and Q. V . Le, “Learning transferable
architectures for scalable image recognition,” 2018 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pp. 8697–8710, 2018.
[42] H. Zhou, J. Lan, R. Liu, and J. Yosinski, “Deconstructing lottery tickets:
Zeros, signs, and the supermask,” Advances in neural information
processing systems , vol. 32, 2019.
[43] H. Tanaka, D. Kunin, D. L. K. Yamins, and S. Ganguli, “Pruning neural
networks without any data by iteratively conserving synaptic flow,” in
Proceedings of the 34th International Conference on Neural Information
Processing Systems , ser. NIPS’20, Red Hook, NY , USA, 2020.
[44] Y . Le and X. S. Yang, “Tiny ImageNet Visual Recognition Challenge,”
2015.
[45] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisser-
man, “The pascal visual object classes (voc) challenge,” International
Journal of Computer Vision , vol. 88, no. 2, pp. 303–338, Jun. 2010.
[46] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” CoRR , vol. abs/1409.1556, 2015.
[47] X. Glorot and Y . Bengio, “Understanding the difficulty of training deep
feedforward neural networks,” in AISTATS , 2010.
[48] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” IEEE Transactions on
Pattern Analysis and Machine Intelligence , vol. 39, no. 6, pp. 1137–
1149, 2017.
[49] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” 2015.
[50] J. Yang, X. Shen, J. Xing, X. Tian, H. Li, B. Deng, J. Huang, and
X.-s. Hua, “Quantization networks,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , 2019, pp. 7308–
7316.
[51] J. Frankle, G. K. Dziugaite, D. Roy, and M. Carbin, “Linear mode con-
nectivity and the lottery ticket hypothesis,” in International Conference
on Machine Learning . PMLR, 2020, pp. 3259–3269.
[52] B. Bartoldson, A. Morcos, A. Barbu, and G. Erlebacher, “The
generalization-stability tradeoff in neural network pruning,” Advancesin Neural Information Processing Systems , vol. 33, pp. 20 852–20 864,
2020.
[53] S. Hooker, A. Courville, G. Clark, Y . Dauphin, and A. Frome,
“What do compressed deep neural networks forget?” arXiv preprint
arXiv:1911.05248 , 2019.
[54] X. Ying, “An overview of overfitting and its solutions,” in Journal of
physics: Conference series , vol. 1168. IOP Publishing, 2019, p. 022022.
[55] T. Vaskevicius, V . Kanade, and P. Rebeschini, “Implicit regularization
for optimal sparse recovery,” Advances in Neural Information Processing
Systems , vol. 32, 2019.
[56] T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste, “Sparsity
in deep learning: Pruning and growth for efficient inference and training
in neural networks,” Journal of Machine Learning Research , vol. 22, no.
241, pp. 1–124, 2021.
