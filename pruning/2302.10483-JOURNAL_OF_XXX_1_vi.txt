# 2302.10483.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2302.10483.pdf
# Kích thước tệp: 2502734 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TẠP CHÍ XXX 1
Nén Bayesian có Cấu trúc cho Mạng Thần kinh Sâu
Dựa trên Phương pháp Turbo-VBI
Chengyu Xia, Danny H. K. Tsang, Fellow, IEEE, và Vincent K. N. Lau, Fellow, IEEE
Tóm tắt - Với sự phát triển của kích thước mạng thần kinh, nén mô hình đã thu hút ngày càng nhiều sự quan tâm trong nghiên cứu gần đây. Là một trong những kỹ thuật phổ biến nhất, cắt tỉa đã được nghiên cứu trong thời gian dài. Bằng cách khai thác tính thưa thớt có cấu trúc của mạng thần kinh, các phương pháp hiện có có thể cắt tỉa các nơ-ron thay vì các trọng số riêng lẻ. Tuy nhiên, trong hầu hết các phương pháp cắt tỉa hiện có, các nơ-ron còn sót lại được kết nối ngẫu nhiên trong mạng thần kinh mà không có cấu trúc nào, và các trọng số khác không trong mỗi nơ-ron cũng được phân bố ngẫu nhiên. Cấu trúc thưa thớt không đều như vậy có thể gây ra chi phí kiểm soát rất cao và truy cập bộ nhớ không đều cho phần cứng và thậm chí làm tăng độ phức tạp tính toán của mạng thần kinh. Trong bài báo này, chúng tôi đề xuất một tiên nghiệm phân cấp ba lớp để thúc đẩy cấu trúc thưa thớt đều hơn trong quá trình cắt tỉa. Tiên nghiệm phân cấp ba lớp được đề xuất có thể đạt được tính thưa thớt có cấu trúc ở cấp độ trọng số từng nơ-ron và tính thưa thớt có cấu trúc ở cấp độ nơ-ron. Chúng tôi suy ra một thuật toán suy luận Bayesian biến phân Turbo (Turbo-VBI) hiệu quả để giải quyết bài toán nén mô hình kết quả với tiên nghiệm được đề xuất. Thuật toán Turbo-VBI được đề xuất có độ phức tạp thấp và có thể hỗ trợ các tiên nghiệm tổng quát hơn so với các thuật toán nén mô hình hiện có. Kết quả mô phỏng cho thấy thuật toán được đề xuất có thể thúc đẩy cấu trúc đều hơn trong các mạng thần kinh được cắt tỉa trong khi đạt được hiệu suất thậm chí tốt hơn về tỷ lệ nén và độ chính xác suy luận so với các phương pháp cơ sở.

Từ khóa chỉ mục - Mạng thần kinh sâu, Tính thưa thớt nhóm, Nén mô hình, Cắt tỉa.

I. GIỚI THIỆU

MẠNG thần kinh sâu (DNN) đã cực kỳ thành công trong nhiều ứng dụng. Tuy nhiên, để đạt được hiệu suất tốt, các DNN tiên tiến có xu hướng có số lượng tham số khổng lồ. Việc triển khai và truyền tải các mô hình lớn như vậy đặt ra những thách thức đáng kể về tính toán, bộ nhớ và truyền thông. Điều này đặc biệt quan trọng đối với các thiết bị biên, có tài nguyên rất hạn chế. Vì những lý do trên, nén mô hình đã trở thành một chủ đề nóng trong học sâu.

Có nhiều nghiên cứu đa dạng tập trung vào nén mô hình. Trong [1]-[3], các phương pháp cắt tỉa trọng số được áp dụng để giảm độ phức tạp của suy luận DNN. Trong [1], ngưỡng được đặt làm tầm quan trọng của trọng số, được đo bằng cách giới thiệu một biến tầm quan trọng phụ trợ cho mỗi trọng số. Với các biến phụ trợ, việc cắt tỉa có thể được thực hiện trong một lần và trước khi huấn luyện chính thức. Trong [2] và [3], ma trận Hessian của các trọng số được sử dụng làm tiêu chí để thực hiện cắt tỉa. Tuy nhiên, các phương pháp nói trên không thể chủ động buộc một số trọng số ít quan trọng hơn có giá trị nhỏ và do đó các phương pháp chỉ thụ động cắt tỉa các trọng số nhỏ. Trong [4]-[13], các phương pháp hệ thống nén mô hình sử dụng tối ưu hóa các hàm mất mát được chính quy hóa được áp dụng. Trong [11], một regularizer dựa trên chuẩn `2 được áp dụng cho các trọng số của DNN và các trọng số gần với không được cắt tỉa. Trong [4], một regularizer chuẩn `0, buộc các trọng số bằng chính xác bằng không, được đề xuất. Chính quy hóa chuẩn `0 được chứng minh là hiệu quả hơn để thực thi tính thưa thớt trong các trọng số nhưng bài toán huấn luyện khó giải do bản chất không liên tục của chuẩn `0. Trong các công trình này, việc chính quy hóa chỉ thúc đẩy tính thưa thớt trong các trọng số. Tuy nhiên, tính thưa thớt trong các trọng số không tương đương với tính thưa thớt trong các nơ-ron. Trong [9], chính quy hóa phân cực được đề xuất, có thể đẩy một tỷ lệ trọng số về không và những trọng số khác về các giá trị lớn hơn không. Sau đó, tất cả các trọng số kết nối với cùng một nơ-ron được gán với một regularizer phân cực chung, sao cho một số nơ-ron có thể được cắt tỉa hoàn toàn và một số có thể được đẩy đến các giá trị lớn hơn không. Bằng cách này, các trọng số còn lại không phải được đẩy đến các giá trị nhỏ và do đó có thể cải thiện khả năng biểu đạt của mạng. Trong [5], các tác giả sử dụng một regularizer group Lasso để loại bỏ toàn bộ các bộ lọc trong CNN và cho thấy rằng chính quy hóa thưa thớt nhóm được đề xuất của họ thậm chí có thể tăng độ chính xác của ResNet. Tuy nhiên, các nơ-ron kết quả được kết nối ngẫu nhiên trong mạng thần kinh và đường dẫn dữ liệu kết quả của mạng thần kinh được cắt tỉa khá không đều, gây khó khăn cho việc triển khai trong phần cứng [5], [6].

Ngoài các phương pháp chính quy hóa xác định nói trên, chúng ta cũng có thể áp dụng tính thưa thớt trong các trọng số của mạng thần kinh bằng các phương pháp Bayesian. Trong [14], cắt tỉa trọng số và lượng tử hóa được thực hiện cùng lúc bằng cách gán một tập hợp các cổng lượng tử hóa cho giá trị trọng số. Một tiên nghiệm được thiết kế thêm để buộc các cổng lượng tử hóa "đóng", sao cho các trọng số bị buộc về không và các trọng số khác không bị buộc về độ chính xác bit thấp. Bằng cách thiết kế một phân phối tiên nghiệm phù hợp cho các trọng số, người ta có thể thực thi các cấu trúc tinh tế hơn trong vector trọng số. Trong [15], một tiên nghiệm thưa thớt đơn giản được đề xuất để thúc đẩy tính thưa thớt ở cấp độ trọng số, và suy luận Bayesian biến phân (VBI) được áp dụng để giải quyết bài toán nén mô hình. Trong [16], tính thưa thớt nhóm được nghiên cứu từ góc độ Bayesian. Một tiên nghiệm thưa thớt phân cấp hai lớp được đề xuất trong đó các trọng số tuân theo phân phối Gaussian và tất cả các trọng số đầu ra của một nơ-ron chia sẻ cùng một phương sai tiên nghiệm. Do đó, các trọng số đầu ra của một nơ-ron có thể được cắt tỉa cùng lúc và đạt được tính thưa thớt ở cấp độ nơ-ron.

--- TRANG 2 ---
TẠP CHÍ XXX 2

Trong bài báo này, chúng tôi xem xét nén mô hình của DNN từ góc độ Bayesian. Mặc dù có nhiều công trình hiện có về nén mô hình, vẫn còn một số vấn đề kỹ thuật cần được giải quyết.

Đường dẫn dữ liệu có cấu trúc trong mạng thần kinh được cắt tỉa: Trong các phương pháp chính quy hóa xác định, chúng ta có thể đạt được tính thưa thớt nhóm trong ma trận trọng số. Ví dụ, dưới chính quy hóa chuẩn `2;1 [5], [7], một số hàng có thể được đặt về không trong ma trận trọng số sau khi cắt tỉa và điều này dẫn đến tính thưa thớt ở cấp độ nơ-ron. Tuy nhiên, các nơ-ron còn sót lại được kết nối ngẫu nhiên trong mạng thần kinh mà không có cấu trúc nào. Hơn nữa, các trọng số khác không trong mỗi nơ-ron cũng được phân bố ngẫu nhiên. Trong các phương pháp Bayesian hiện có, cả các tiên nghiệm một lớp trong [15] và các tiên nghiệm phân cấp hai lớp trong [16] cũng không thể thúc đẩy kết nối nơ-ron có cấu trúc trong mạng thần kinh được cắt tỉa. Do đó, kết nối nơ-ron ngẫu nhiên và không đều trong đường dẫn dữ liệu của DNN đặt ra thách thức trong việc triển khai phần cứng.

Hiệu quả của cắt tỉa trọng số: Với các phương pháp nén mô hình hiện có, mô hình nén vẫn có thể quá lớn để triển khai hiệu quả trên các thiết bị di động. Hơn nữa, các phương pháp nén mô hình Bayesian hiện có có xu hướng có độ phức tạp cao và độ mạnh mẽ thấp đối với các tập dữ liệu khác nhau, vì chúng thường áp dụng lấy mẫu Monte Carlo trong việc giải quyết bài toán tối ưu hóa [15], [16]. Do đó, cần có một giải pháp nén mô hình hiệu quả hơn có thể đạt được tỷ lệ nén cao hơn và độ phức tạp thấp hơn.

Để vượt qua những thách thức trên, chúng tôi đề xuất một tiên nghiệm thưa thớt phân cấp ba lớp có thể khai thác tính thưa thớt có cấu trúc ở cấp độ trọng số và cấp độ nơ-ron trong quá trình huấn luyện. Để xử lý tiên nghiệm thưa thớt phân cấp, chúng tôi đề xuất một thuật toán Turbo-VBI [17] để giải quyết bài toán tối ưu hóa Bayesian trong nén mô hình. Các đóng góp chính được tóm tắt dưới đây.

Tiên nghiệm Phân cấp Ba lớp cho Tính thưa thớt Có cấu trúc ở Cấp độ Trọng số và Cấp độ Nơ-ron: Việc thiết kế một phân phối tiên nghiệm phù hợp trong ma trận trọng số là quan trọng để đạt được các cấu trúc tinh tế hơn trong các trọng số và nơ-ron. Để khai thác tính thưa thớt có cấu trúc ở cấp độ trọng số và cấp độ nơ-ron, chúng tôi đề xuất một tiên nghiệm phân cấp ba lớp bao gồm cả tiên nghiệm phân cấp hai lớp truyền thống [16] và tiên nghiệm dựa trên hỗ trợ [18]. Tiên nghiệm phân cấp ba lớp được đề xuất có những ưu điểm sau so với các tiên nghiệm thưa thớt hiện có trong nén mô hình Bayesian: 1) Nó thúc đẩy tính thưa thớt có cấu trúc nâng cao ở cấp độ trọng số cũng như tính thưa thớt có cấu trúc ở cấp độ nơ-ron, như được minh họa trong Hình 1c. Khác với các tiên nghiệm thưa thớt hiện có, tiên nghiệm được đề xuất của chúng tôi không chỉ cắt tỉa các nơ-ron, mà còn thúc đẩy các cấu trúc đều trong các nơ-ron không bị cắt tỉa cũng như các trọng số của một nơ-ron. Do đó, nó có thể đạt được cấu trúc đều hơn trong toàn bộ mạng thần kinh. 2) Nó linh hoạt để thúc đẩy các cấu trúc thưa thớt khác nhau. Kích thước nhóm cũng như khoảng cách trung bình giữa hai nhóm có thể được điều chỉnh thông qua việc tinh chỉnh các siêu tham số. 3) Nó thân thiện với tối ưu hóa. Nó cho phép áp dụng các thuật toán huấn luyện có độ phức tạp thấp.

Thuật toán Turbo-VBI cho Nén Mô hình Bayesian: Dựa trên tiên nghiệm phân cấp 3 lớp, việc huấn luyện và nén mô hình được công thức hóa như một bài toán suy luận Bayesian. Chúng tôi đề xuất một thuật toán Turbo-VBI có độ phức tạp thấp với một số mở rộng mới so với các phương pháp hiện có: 1) Nó có thể hỗ trợ các tiên nghiệm tổng quát hơn. Các phương pháp nén Bayesian hiện có [15], [16] không thể xử lý tiên nghiệm phân cấp ba lớp do lớp bổ sung trong tiên nghiệm. 2) Nó có độ mạnh mẽ cao hơn và độ phức tạp thấp hơn so với các phương pháp nén Bayesian hiện có. Các phương pháp nén Bayesian hiện có sử dụng lấy mẫu Monte Carlo để xấp xỉ số hạng log-likelihood trong hàm mục tiêu, có độ mạnh mẽ thấp và độ phức tạp cao [19]. Để khắc phục nhược điểm này, chúng tôi đề xuất một xấp xỉ xác định cho số hạng log-likelihood.

Hiệu suất Nén Mô hình Vượt trội: Giải pháp được đề xuất có thể đạt được một mạng thần kinh được nén cao so với các phương pháp cơ sở tiên tiến và đạt được hiệu suất suy luận tương tự. Do đó, giải pháp được đề xuất có thể giảm đáng kể độ phức tạp tính toán trong suy luận của mạng thần kinh và các mạng thần kinh được cắt tỉa kết quả thân thiện với phần cứng. Kết quả là, nó có thể hoàn toàn giải phóng tiềm năng của các ứng dụng AI trên các thiết bị di động.

Phần còn lại của bài báo được tổ chức như sau. Trong Phần II, các cấu trúc hiện có và cấu trúc mong muốn trong ma trận trọng số được trình bày chi tiết. Tiên nghiệm phân cấp ba lớp cho cấu trúc mong muốn cũng được giới thiệu. Trong Phần III, bài toán nén mô hình được công thức hóa. Trong Phần IV, thuật toán Turbo-VBI cho nén mô hình dưới tiên nghiệm phân cấp ba lớp được trình bày. Trong Phần V và Phần VI, các thí nghiệm số và kết luận, tương ứng, được cung cấp.

II. CÁC CẤU TRÚC TRONG MA TRẬN TRỌNG SỐ

Nén mô hình được thực hiện bằng cách thực thi tính thưa thớt trong ma trận trọng số. Trước tiên chúng tôi xem xét các cấu trúc thưa thớt có cấu trúc chính của ma trận trọng số trong tài liệu. Dựa trên điều này, chúng tôi trình bày chi tiết các cấu trúc mong muốn trong ma trận trọng số chưa được xem xét. Cuối cùng, chúng tôi đề xuất một mô hình tiên nghiệm phân cấp 3 lớp để thực thi cấu trúc mong muốn như vậy trong ma trận trọng số.

A. Xem xét các Ma trận Trọng số Có cấu trúc Thưa thớt Hiện có

Có hai cấu trúc thưa thớt chính hiện có trong ma trận trọng số, đó là tính thưa thớt ngẫu nhiên và tính thưa thớt nhóm. Chúng tôi tập trung vào một lớp kết nối đầy đủ để trình bày chi tiết hai cấu trúc thưa thớt này.

1) Tính thưa thớt Ngẫu nhiên trong Ma trận Trọng số: Trong tính thưa thớt ngẫu nhiên, các trọng số được cắt tỉa riêng lẻ. Chính quy hóa chuẩn `1

--- TRANG 3 ---
TẠP CHÍ XXX 3

[THIS IS FIGURE: Three diagrams showing different structures in weight matrices labeled (a), (b), and (c)]

(a) Cấu trúc tính thưa thớt ngẫu nhiên.
(b) Cấu trúc tính thưa thớt nhóm.  
(c) Cấu trúc đa cấp được đề xuất.

Hình 1: So sánh các cấu trúc khác nhau trong ma trận trọng số của một lớp kết nối đầy đủ với 6 nơ-ron đầu vào và 6 nơ-ron đầu ra. Mỗi hàng biểu thị các trọng số kết nối với một nơ-ron đầu vào.

và tiên nghiệm một lớp [15] thường được áp dụng để chính quy hóa các trọng số riêng lẻ. Chính quy hóa như vậy đơn giản và thân thiện với tối ưu hóa; tuy nhiên, cả trọng số và nơ-ron kết quả đều được kết nối hoàn toàn ngẫu nhiên vì không có ràng buộc bổ sung nào đối với các trọng số được cắt tỉa. Trong ma trận trọng số, điều này có nghĩa là các phần tử được đặt về không mà không có cấu trúc nào, dẫn đến các phần tử còn lại cũng không có cấu trúc cụ thể. Khi chuyển đổi thành cấu trúc liên kết mạng thần kinh, điều này có nghĩa là các trọng số và nơ-ron còn sót lại được kết nối ngẫu nhiên, như được minh họa trong Hình 1a. Do các kết nối ngẫu nhiên trong mạng được cắt tỉa, cấu trúc như vậy không thân thiện với các ứng dụng thực tế như lưu trữ, tính toán và truyền tải [5], [20].

2) Tính thưa thớt Nhóm trong Ma trận Trọng số: Trong tính thưa thớt nhóm, các trọng số được cắt tỉa theo nhóm. Đặc biệt, trong các lớp kết nối đầy đủ, một nhóm thường được định nghĩa là các trọng số đi ra của một nơ-ron để thúc đẩy tính thưa thớt ở cấp độ nơ-ron. Để thúc đẩy tính thưa thớt nhóm, chính quy hóa sparse group Lasso [7], [21] và các tiên nghiệm hai lớp được đề xuất. Nói chung, chính quy hóa như vậy thường có hai khía cạnh: một regularizer cho cắt tỉa nhóm và một regularizer cho cắt tỉa trọng số riêng lẻ. Do đó, các trọng số không chỉ có thể được cắt tỉa theo nhóm, mà còn có thể được cắt tỉa riêng lẻ trong trường hợp nhóm không thể được loại bỏ hoàn toàn. Tuy nhiên, khi một nơ-ron không thể được cắt tỉa hoàn toàn, các trọng số kết nối với nó được cắt tỉa ngẫu nhiên, và các nơ-ron cũng được cắt tỉa ngẫu nhiên. Trong Hình 1b, chúng tôi minh họa một ví dụ về tính thưa thớt nhóm. Trong ma trận trọng số, các phần tử khác không trong một hàng được phân bố ngẫu nhiên và các hàng khác không cũng được phân bố ngẫu nhiên. Khi chuyển đổi thành cấu trúc liên kết mạng thần kinh, điều này có nghĩa là các trọng số không bị cắt tỉa kết nối với một nơ-ron được phân bố ngẫu nhiên, và trong mỗi lớp, các nơ-ron còn sót lại cũng được phân bố ngẫu nhiên. Do đó, tính thưa thớt nhóm vẫn không hoàn toàn có cấu trúc.

Mặt khác, nghiên cứu gần đây đã chỉ ra rằng các kết nối không đều trong mạng thần kinh có thể mang lại nhiều bất lợi khác nhau trong việc giảm độ phức tạp tính toán hoặc thời gian suy luận [20], [22]-[25]. Như đã thảo luận ở trên, các cấu trúc thưa thớt hiện có không đủ đều, và các nhược điểm của các cấu trúc không đều như vậy được liệt kê như sau.

1) Tính song song giải mã và tính toán thấp: Ma trận trọng số thưa thớt thường được lưu trữ ở định dạng thưa thớt trên phần cứng, ví dụ: compressed sparse row (CSR), compressed sparse column (CSC), coordinate format (COO). Tuy nhiên, như được minh họa trong Hình 2 (a), khi giải mã từ định dạng thưa thớt, số bước giải mã cho mỗi hàng (cột) có thể khác nhau rất nhiều do cấu trúc không đều. Điều này có thể làm hại nghiêm trọng tính song song giải mã [23]. Hơn nữa, cấu trúc không đều cũng cản trở việc tính toán song song như matrix tiling [24], [20], như được minh họa trong Hình 2 (b).

2) Cắt tỉa nơ-ron không hiệu quả: Như được minh họa trong Hình 2 (c), các trọng số còn sót lại không có cấu trúc từ lớp trước có xu hướng kích hoạt các nơ-ron ngẫu nhiên trong lớp tiếp theo, điều này không hiệu quả trong việc cắt tỉa các nơ-ron trong lớp tiếp theo. Điều này có thể dẫn đến sự không hiệu quả trong việc giảm các phép toán dấu phẩy động (FLOP) của mạng thần kinh được cắt tỉa.

3) Gánh nặng lưu trữ lớn: Vì các trọng số còn sót lại của các phương pháp nén mô hình hiện có không có cấu trúc, vị trí chính xác cho mỗi trọng số còn sót lại phải được ghi lại. Điều này dẫn đến chi phí lưu trữ khổng lồ, thậm chí có thể lớn hơn việc lưu trữ ma trận dày đặc [24], [20]. Chi phí lưu trữ khổng lồ cũng có thể ảnh hưởng đến thời gian suy luận bằng cách tăng thời gian truy cập bộ nhớ và giải mã.

B. Cấu trúc Đa cấp trong Ma trận Trọng số

Dựa trên thảo luận trên, một mạng thần kinh thưa thớt có cấu trúc đa cấp nên đáp ứng ba tiêu chí sau:
1) Tính thưa thớt có cấu trúc ở cấp độ trọng số từng nơ-ron: Các trọng số còn sót lại kết nối với một nơ-ron nên thể hiện cấu trúc đều thay vì được phân bố ngẫu nhiên như trong tính thưa thớt nhóm truyền thống.

--- TRANG 4 ---
TẠP CHÍ XXX 4

Trong ma trận trọng số, điều này có nghĩa là các phần tử khác không của mỗi hàng nên tuân theo một cấu trúc đều nào đó.
2) Tính thưa thớt có cấu trúc ở cấp độ nơ-ron: Các nơ-ron còn sót lại của mỗi lớp cũng nên có một số cấu trúc đều. Trong ma trận trọng số, điều này có nghĩa là các hàng khác không cũng nên tuân theo một cấu trúc nào đó. 3) Tính linh hoạt: Cấu trúc của ma trận trọng số kết quả nên đủ linh hoạt để chúng ta có thể đạt được sự cân bằng giữa độ phức tạp mô hình và độ chính xác dự đoán trong các tình huống khác nhau. Hình 1c minh họa một ví dụ về cấu trúc mong muốn trong ma trận trọng số. Trong mỗi hàng, các giá trị quan trọng có xu hướng tập trung thành nhóm. Khi chuyển đổi thành cấu trúc liên kết mạng thần kinh, điều này cho phép cấu trúc nâng cao ở cấp độ trọng số: Các trọng số còn sót lại của mỗi nơ-ron có xu hướng tập trung thành nhóm. Trong mỗi cột, các giá trị quan trọng cũng có xu hướng tập trung thành nhóm. Khi chuyển đổi thành cấu trúc liên kết mạng thần kinh, điều này cho phép cấu trúc ở cấp độ nơ-ron: Các nơ-ron còn sót lại trong mỗi lớp có xu hướng tập trung thành nhóm. Hơn nữa, kích thước nhóm trong mỗi hàng và cột có thể được điều chỉnh để đạt được sự cân bằng giữa độ phức tạp mô hình và độ chính xác dự đoán. Các cấu trúc đa cấp được đề xuất như vậy cho phép đường dẫn dữ liệu khai thác việc nén để đơn giản hóa việc tính toán với chi phí kiểm soát rất nhỏ. Cụ thể, các ưu điểm của cấu trúc đa cấp được đề xuất của chúng tôi được liệt kê như sau.

1) Tính song song giải mã và tính toán cao: Như được minh họa trong Hình 2 (a), với cấu trúc khối, việc giải mã có thể được thực hiện theo từng khối sao cho tất cả các trọng số trong cùng một khối có thể được giải mã đồng thời. Hơn nữa, cấu trúc đều có thể mang lại nhiều tính song song hơn trong tính toán. Ví dụ, matrix tiling hiệu quả hơn có thể được áp dụng, trong đó các khối không có thể được bỏ qua trong quá trình nhân ma trận, như được minh họa trong Hình 2 (b). Ngoài ra, các kernel có thể được nén thành kích thước nhỏ hơn, sao cho các đầu vào tương ứng với các trọng số bằng không có thể được bỏ qua [20].

2) Cắt tỉa nơ-ron hiệu quả: Như được minh họa trong Hình 2 (c), các trọng số còn sót lại có cấu trúc từ lớp trước có thể dẫn đến một kích hoạt nơ-ron có cấu trúc và gọn gàng trong lớp tiếp theo, điều này có lợi cho việc cắt tỉa nơ-ron hiệu quả hơn.

3) Lưu trữ hiệu quả hơn: Với cấu trúc khối, chúng ta có thể đơn giản ghi lại vị trí và kích thước cho mỗi khối thay vì ghi lại vị trí chính xác cho mỗi trọng số. Đối với một ma trận trọng số có kích thước cụm trung bình là m×m và số cụm P, mức tăng mã hóa so với định dạng COO truyền thống là O(Pm²).

Trong bài báo này, chúng tôi tập trung vào việc thúc đẩy cấu trúc nhóm đa cấp trong nén mô hình. Trong cấu trúc được đề xuất của chúng tôi, các trọng số còn sót lại kết nối với một nơ-ron có xu hướng tập trung thành nhóm và các nơ-ron còn sót lại cũng có xu hướng tập trung thành nhóm. Ngoài ra, kích thước nhóm có thể điều chỉnh được để cấu trúc được đề xuất của chúng tôi linh hoạt.

C. Mô hình Tiên nghiệm Phân cấp Ba lớp

Mô hình xác suất của tiên nghiệm thưa thớt cung cấp nền tảng cho các cấu trúc thưa thớt cụ thể. Như đã đề cập trước đây, các tiên nghiệm thưa thớt hiện có cho nén mô hình không thể thúc đẩy cấu trúc đa cấp trong ma trận trọng số. Để nắm bắt tính thưa thớt có cấu trúc đa cấp, chúng tôi đề xuất một tiên nghiệm phân cấp ba lớp bằng cách kết hợp tiên nghiệm hai lớp và tiên nghiệm dựa trên hỗ trợ [18].

Gọi w biểu thị một trọng số trong mạng thần kinh. Đối với mỗi trọng số w, chúng tôi giới thiệu một hỗ trợ s ∈ {0,1} để chỉ ra liệu trọng số w có hoạt động (s = 1) hay không hoạt động (s = 0). Cụ thể, gọi ρ biểu thị độ chính xác cho trọng số w. Tức là, 1/ρ là phương sai của w. Khi s = 0, phân phối của ρ được chọn để thỏa mãn E[ρ] ≫ 1, sao cho phương sai của trọng số w tương ứng rất nhỏ. Khi s = 1, phân phối của ρ được chọn để thỏa mãn E[ρ] = O(1), sao cho w có một số xác suất để nhận các giá trị quan trọng. Khi đó, tiên nghiệm phân cấp ba lớp (phân phối chung của w, ρ, s) được cho bởi

p(w,ρ,s) = p(s)p(ρ|s)p(w|ρ), (1)

trong đó w biểu thị tất cả các trọng số trong mạng thần kinh, ρ biểu thị các độ chính xác tương ứng, và s biểu thị các hỗ trợ tương ứng. Phân phối của mỗi lớp được trình bày chi tiết dưới đây.

Mô hình Xác suất cho p(s): p(s) có thể được phân tách thành từng lớp bằng p(s) = ∏ᴸₗ₌₁p(sₗ), trong đó L là tổng số lớp của mạng thần kinh và sₗ biểu thị các hỗ trợ của các trọng số trong lớp thứ l. Bây giờ chúng tôi tập trung vào lớp thứ l. Giả sử kích thước của ma trận trọng số là K×M, tức là lớp có K nơ-ron đầu vào và M nơ-ron đầu ra. Phân phối p(sₗ) của các hỗ trợ được sử dụng để nắm bắt cấu trúc đa cấp trong lớp này. Vì chúng tôi tập trung vào một lớp cụ thể bây giờ, chúng tôi sử dụng p(s) để thay thế p(sₗ) trong thảo luận sau đây cho đơn giản ký hiệu. Để thúc đẩy cấu trúc đa cấp nói trên, các phần tử hoạt động trong mỗi hàng của ma trận trọng số nên tập trung thành cụm và các phần tử hoạt động trong mỗi cột của ma trận trọng số cũng nên tập trung thành cụm. Cấu trúc khối như vậy có thể được đạt được bằng cách mô hình hóa ma trận hỗ trợ như một trường ngẫu nhiên Markov (MRF). Cụ thể, mỗi hàng của ma trận hỗ trợ được mô hình hóa bởi một chuỗi Markov như

p(srow) = p(srow,1)∏ᴹₘ₌₁p(srow,m+1|srow,m), (2)

trong đó srow biểu thị vector hàng của ma trận hỗ trợ, và srow,m biểu thị phần tử thứ m trong srow. Xác suất chuyển tiếp được cho bởi p(srow,m+1 = 1|srow,m = 0) = p^row₀₁ và p(srow,m+1 = 0|srow,m = 1) = p^row₁₀. Nói chung, p^row₀₁ nhỏ hơn dẫn đến khoảng cách trung bình lớn hơn giữa hai cụm, và p^row₁₀ nhỏ hơn dẫn đến kích thước cụm trung bình lớn hơn. Tương tự, mỗi cột của ma trận hỗ trợ cũng được mô hình hóa bởi một chuỗi Markov như

p(scol) = p(scol,1)∏ᴷₖ₌₁p(scol,k+1|scol,k), (3)

trong đó scol biểu thị vector cột của ma trận hỗ trợ và scol,k biểu thị phần tử thứ n trong scol. Xác suất chuyển tiếp được cho bởi p(scol,k+1 = 1|scol,k = 0) = p^col₀₁ và p(scol,k+1 = 0|scol,k = 1) = p^col₁₀. Mô hình MRF như vậy cũng

--- TRANG 5 ---
TẠP CHÍ XXX 5

[Hình 2: (a) Minh họa tính song song giải mã. (b) Minh họa tính song song tính toán. (c) Minh họa cắt tỉa nơ-ron hiệu quả.]

[Hình 3: Minh họa tiên nghiệm hỗ trợ cho ma trận trọng số 6×6.]

được biết đến như một mô hình Ising 2D. Một minh họa về tiên nghiệm MRF được cho trong Hình 3. Lưu ý rằng p(s) cũng có thể được mô hình hóa với các loại tiên nghiệm khác, như tiên nghiệm cây Markov và tiên nghiệm Markov ẩn, để thúc đẩy các cấu trúc khác.

Mô hình Xác suất cho p(ρ|s): Xác suất có điều kiện p(ρ|s) được cho bởi

p(ρ|s) = ∏ᴺₙ₌₁(Γ(ρₙ; aₙ, bₙ))^sₙ × (Γ(ρₙ; ãₙ, b̃ₙ))^(1-sₙ), (4)

trong đó N là tổng số trọng số trong mạng thần kinh. ρₙ nhận hai phân phối Gamma khác nhau tùy theo giá trị của sₙ. Khi sₙ = 1, trọng số tương ứng wₙ hoạt động. Trong trường hợp này, tham số hình dạng aₙ và tham số tỷ lệ bₙ nên thỏa mãn aₙ/bₙ = E[ρₙ] = O(1) sao cho phương sai của wₙ là O(1). Khi sₙ = 0, trọng số tương ứng wₙ không hoạt động. Trong trường hợp này, tham số hình dạng ãₙ và tham số tỷ lệ b̃ₙ nên thỏa mãn ãₙ/b̃ₙ = E[ρₙ] ≫ 1 sao cho phương sai của wₙ rất gần với không. Động lực để chọn phân phối Gamma là phân phối Gamma là liên hợp với phân phối Gaussian. Do đó, nó có thể dẫn đến một giải pháp dạng đóng trong suy luận Bayesian [17].

Mô hình Xác suất cho p(w|ρ): Xác suất có điều kiện p(w|ρ) được cho bởi

p(w|ρ) = ∏ᴺₙ₌₁p(wₙ|ρₙ), (5)

trong đó p(wₙ|ρₙ) được giả định là một phân phối Gaussian:

p(wₙ|ρₙ) = N(wₙ|0, 1/ρₙ). (6)

Mặc dù việc chọn p(wₙ|ρₙ) không bị hạn chế đối với phân phối Gaussian [16], động lực để chọn Gaussian vẫn hợp lý. Đầu tiên, theo các mô phỏng hiện có, hiệu suất nén không quá nhạy cảm với loại phân phối của p(wₙ|ρₙ) [16], [15]. Hơn nữa, giả định p(wₙ|ρₙ) là phân phối Gaussian cũng góp phần vào việc tối ưu hóa dễ dàng hơn trong suy luận Bayesian, như được hiển thị trong Phần IV.

Nhận xét 1. (So sánh với các tiên nghiệm thưa thớt hiện có) Một trong những khác biệt chính giữa công trình của chúng tôi và các công trình hiện có [15], [16] là thiết kế của tiên nghiệm. Do lớp hỗ trợ MRF, tiên nghiệm ba lớp được đề xuất của chúng tôi tổng quát hơn và có thể nắm bắt các cấu trúc đều hơn như cấu trúc đa cấp mong muốn. Cấu trúc đa cấp như vậy trong ma trận trọng số không thể được mô hình hóa bởi các tiên nghiệm hiện có trong [15] và [16] nhưng tiên nghiệm được đề xuất của chúng tôi có thể dễ dàng hỗ trợ các cấu trúc thưa thớt trong chúng. Trong công trình của chúng tôi, nếu tiên nghiệm p(w,ρ,s) rút gọn thành một lớp p(w) và nhận phân phối Laplace, nó tương đương với chính quy hóa Lasso. Bằng cách này, tính thưa thớt ngẫu nhiên có thể được thực hiện. Nếu tiên nghiệm p(w,ρ,s) rút gọn thành một lớp p(w) và nhận phân phối đồng nhất log-scale không đúng, nó chính xác là bài toán của variational dropout [15]. Bằng cách này, tính thưa thớt ngẫu nhiên có thể được thực hiện. Nếu tiên nghiệm p(w,ρ,s) rút gọn thành hai lớp p(w|ρ), và nhận phân phối log-uniform nhóm không đúng hoặc phân phối horseshoe nhóm, nó chính xác là bài toán trong [16]. Bằng cách này, tính thưa thớt nhóm có thể được thực hiện. Hơn nữa, vì tiên nghiệm được đề xuất của chúng tôi phức tạp hơn, nó cũng dẫn đến một thiết kế thuật toán khác trong Phần IV.

III. CÔNG THỨC HOÁ NÉN MÔ HÌNH BAYESIAN

Nén mô hình Bayesian xử lý bài toán nén mô hình từ góc độ Bayesian. Trong nén mô hình Bayesian, các trọng số tuân theo một phân phối tiên nghiệm, và mục tiêu chính của chúng ta là tìm phân phối hậu nghiệm có điều kiện trên tập dữ liệu. Trong phần sau, chúng tôi trình bày chi tiết về mô hình mạng thần kinh và huấn luyện Bayesian của DNN.

A. Mô hình Mạng Thần kinh

Trước tiên chúng tôi giới thiệu mô hình mạng thần kinh. Gọi D = {(x₁,y₁), (x₂,y₂), ..., (x_D,y_D)} biểu thị tập dữ liệu, chứa D cặp đầu vào huấn luyện (x_d) và đầu ra huấn luyện (y_d). Gọi NN(x,w) biểu thị đầu ra của mạng thần kinh với đầu vào x và trọng số w, trong đó NN(·) là hàm mạng thần kinh. Lưu ý rằng NN(x,w) là một mô hình mạng thần kinh tổng quát, có thể bao gồm nhiều cấu trúc thường được sử dụng, một số ví dụ được liệt kê như sau.

Multi-Layer Perceptron (MLP): MLP là một lớp đại diện của mạng thần kinh feedforward, bao gồm một lớp đầu vào, lớp đầu ra và các lớp ẩn. Như được minh họa trong Hình 4a, chúng ta có thể sử dụng NN(x,w) để biểu diễn tính toán feedforward trong một MLP.

Convolutional Neural Network (CNN): Một CNN bao gồm các kernel tích chập và các lớp dày đặc. Như được minh họa trong Hình 4b, chúng ta có thể sử dụng NN(x,w) để biểu diễn tính toán phức tạp trong một CNN.

Vì các điểm dữ liệu từ tập huấn luyện thường được giả định là độc lập, chúng ta có thể sử dụng mô hình ngẫu nhiên sau để mô tả các quan sát từ một mạng thần kinh tổng quát:

y ~ p(D|w) = ∏ᴰ_d=1 p(y_d|x_d,w), (7)

trong đó likelihood p(y_d|x_d,w) có thể khác nhau cho các nhiệm vụ hồi quy và phân loại [26]. Trong các nhiệm vụ hồi quy, nó có thể được mô hình hóa như một phân phối Gaussian:

p(y_d|x_d,w) = N(NN(x_d,w), σ²_d), (8)

--- TRANG 6 ---
TẠP CHÍ XXX 6

trong đó σ²_d là phương sai nhiễu trong dữ liệu huấn luyện, trong khi trong các nhiệm vụ phân loại, nó có thể được mô hình hóa như

p(y_d|x_d,w) = exp(G(y_d|x_d,w)), (9)

trong đó G(y_d|x_d,w) là hàm lỗi cross-entropy.

(a) Minh họa NN(x,w) trong MLP.
(b) Minh họa NN(x,w) trong CNN.

Hình 4: Minh họa NN(x,w) trong các mạng thần kinh khác nhau.

B. Huấn luyện Bayesian của DNN

Huấn luyện Bayesian của một DNN thực tế là tính toán phân phối hậu nghiệm p(w,ρ,s|D) dựa trên phân phối tiên nghiệm p(w,ρ,s) và mô hình ngẫu nhiên mạng thần kinh (7). Trong nghiên cứu của chúng tôi, sau khi chúng tôi thu được hậu nghiệm p(w,ρ,s|D), chúng tôi sử dụng ước lượng maximum a posteriori (MAP) của w, ρ và s như một ước lượng xác định của trọng số, độ chính xác và hỗ trợ, tương ứng:

(ŵ,ρ̂,ŝ) = arg max_{w,ρ,s} p(w,ρ,s|D). (10)

Theo quy tắc Bayesian, phân phối hậu nghiệm của w, ρ và s có thể được suy ra như

p(w,ρ,s|D) = p(D|w,ρ,s)p(w,ρ,s)/p(D), (11)

trong đó p(D|w,ρ,s) là số hạng likelihood. Dựa trên (7) và (8), nó được cho bởi:

p(D|w,ρ,s) = ∏ᴰ_d=1 p(y_d|x_d,w,ρ,s), (12)

p(y_d|x_d,w,ρ,s) = N(NN(x_d,w,ρ,s), σ²_d). (13)

Phương trình (11) chủ yếu chứa hai số hạng: 1) Likelihood p(D|w,ρ,s), liên quan đến biểu hiện của tập dữ liệu. 2) Prior p(w,ρ,s), liên quan đến cấu trúc thưa thớt chúng ta muốn thúc đẩy. Theo trực giác, bằng huấn luyện Bayesian của DNN, chúng ta thực tế đồng thời 1) điều chỉnh các trọng số để biểu hiện tập dữ liệu và 2) điều chỉnh các trọng số để thúc đẩy cấu trúc được nắm bắt trong tiên nghiệm.

Tuy nhiên, việc tính toán trực tiếp hậu nghiệm theo (11) bao gồm một số thách thức, được tóm tắt như sau.

--- TRANG 7 ---
TẠP CHÍ XXX 7

Tích phân đa chiều không thể tính toán: Việc tính toán hậu nghiệm chính xác p(w,ρ,s|D) bao gồm việc tính toán số hạng evidence p(D). Việc tính toán p(D) = ∑_s ∫∫ p(D|w,ρ,s)p(w,ρ,s)dwdρ thường không thể tính toán vì số hạng likelihood có thể rất phức tạp [26].

Số hạng tiên nghiệm phức tạp: Vì số hạng tiên nghiệm p(w,ρ,s) chứa một lớp hỗ trợ bổ sung p(s), divergence KL giữa phân phối xấp xỉ và phân phối hậu nghiệm khó tính toán hoặc xấp xỉ. Phương pháp VBI truyền thống không thể được áp dụng trực tiếp để đạt được một phân phối hậu nghiệm xấp xỉ.

Trong phần tiếp theo, chúng tôi đề xuất một phương pháp nén mô hình dựa trên Turbo-VBI, tính toán xấp xỉ phân phối hậu nghiệm biên của w, ρ và s.

IV. THUẬT TOÁN TURBO-VBI CHO NÉN MÔ HÌNH

Để xử lý hai thách thức nói trên, chúng tôi đề xuất một thuật toán nén mô hình dựa trên Turbo-VBI [17]. Ý tưởng cơ bản của thuật toán Turbo-VBI được đề xuất là xấp xỉ hậu nghiệm không thể tính toán p(w,ρ,s|D) bằng một phân phối biến phân q(w,ρ,s). Đồ thị nhân tử của phân phối chung p(w,ρ,s,D) được minh họa trong Hình 5, trong đó các nút biến được biểu thị bằng các vòng tròn trắng và các nút nhân tử được biểu thị bằng các hình vuông đen. Cụ thể, đồ thị nhân tử được suy ra dựa trên phân tích nhân tử

p(w,ρ,s,D) = p(D|w)p(w|ρ)p(ρ|s)p(s). (14)

Như được minh họa trong Hình 5, các nút biến là w, ρ và s, g biểu thị hàm likelihood, f biểu thị phân phối tiên nghiệm p(w_n|ρ_n) cho trọng số w, η biểu thị phân phối tiên nghiệm p(ρ_n|s_n) cho độ chính xác ρ, và h biểu thị phân phối tiên nghiệm chung p(s) cho hỗ trợ s. Biểu thức chi tiết của mỗi nút nhân tử được liệt kê trong Bảng I.

A. Các Module Cấp cao của Thuật toán Nén Mô hình Dựa trên Turbo-VBI

Vì đồ thị nhân tử trong Hình 5 có vòng lặp, việc áp dụng trực tiếp thuật toán truyền tin thường không thể đạt được hiệu suất tốt. Ngoài ra, vì mô hình xác suất phức tạp hơn so với các mô hình hiện có trong [15] và [16], khó áp dụng trực tiếp thuật toán VBI. Do đó, chúng tôi xem xét việc tách đồ thị nhân tử phức tạp trong Hình 5 thành hai phần, và thực hiện suy luận Bayesian tương ứng, như được minh họa trong Hình 6. Cụ thể, chúng tôi theo khung turbo và chia đồ thị nhân tử thành hai phần. Một là phần hỗ trợ (Phần B), chứa thông tin tiên nghiệm. Phần còn lại là phần còn lại (Phần A). Tương ứng, thuật toán Turbo-VBI được đề xuất cũng được chia thành hai module sao cho mỗi module thực hiện suy luận Bayesian trên phần tương ứng của nó. Module A và Module B cũng cần trao đổi tin nhắn. Cụ thể, tin nhắn đầu ra v_{h→s_n} của Module B đề cập đến tin nhắn từ nút nhân tử h_{A,n} đến nút biến s_n trong Phần A, và tương đương với tin nhắn từ nút biến s_n đến nút nhân tử h_{B,n} trong Phần B, đề cập đến xác suất biên có điều kiện p(s_n|v_{→s}). Nó được tính toán bằng sum-product message passing (SPMP) trên phần B, và hoạt động như đầu vào của Module A để hỗ trợ VBI trên Phần A. Tin nhắn đầu ra v_{n→s_n} của Module A đề cập đến xác suất hậu nghiệm trừ đi đầu ra của Module B: q(s_n)/v_{h→s_n}, và hoạt động như đầu vào của Module B để hỗ trợ SPMP trên Phần B.

Cụ thể, mô hình xác suất cho phân phối tiên nghiệm trong Phần A được giả định là

p̂(w,ρ,s) = p̂(s)p(ρ|s)p(w|ρ), (15)

trong đó

p̂(s) = ∏ᴺₙ₌₁ πₙˢⁿ(1-πₙ)¹⁻ˢⁿ

là một tiên nghiệm mới cho hỗ trợ s. πₙ là xác suất sₙ = 1, được định nghĩa là:

πₙ = p(sₙ = 1) = v_{h→s_n}(1)/(v_{h→s_n}(1) + v_{h→s_n}(0)). (16)

Lưu ý rằng sự khác biệt duy nhất giữa tiên nghiệm mới trong (15) và tiên nghiệm trong (1) là p(s) phức tạp được thay thế bằng một phân phối đơn giản hơn p̂(s) với các mục độc lập. Các tương quan giữa các hỗ trợ được tách thành Phần B. Sau đó, dựa trên tiên nghiệm mới p̂(w,ρ,s), Module A thực hiện một thuật toán VBI.

--- TRANG 8 ---
TẠP CHÍ XXX 8

BẢNG I: Biểu thức chi tiết của mỗi nút nhân tử.

| Nút nhân tử | Hàm phân phối |
|-------------|---------------|
| g_d(x_d,y_d,w) | p(y_d\|x_d,w) | N(NN(x_d,w),σ²_d) |
| f_n(w_n,ρ_n) | p(w_n\|ρ_n) | N(w_n\|0,1/ρ_n) |
| η_n(ρ_n,s_n) | p(ρ_n\|s_n) | (Γ(ρ_n;a_n,b_n))^{s_n} × (Γ(ρ_n;ã_n,b̃_n))^{1-s_n} |
| h | p(s) | MRF prior. p(s_{row,m+1}\|s_{row,m}); p(s_{col,k+1}\|s_{col,k}) |

Trong thuật toán VBI, các phân phối biến phân q(w), q(ρ) và q(s) được sử dụng để xấp xỉ hậu nghiệm. Sau đó, hậu nghiệm xấp xỉ q(s) được chuyển từ Module A trở lại Module B. Tin nhắn được chuyển v_{n→s_n} được định nghĩa là

v_{n→s_n} = q(s_n)/v_{h→s_n}, (17)

theo quy tắc sumproduct.

Với tin nhắn đầu vào v_{n→s_n}, Module B tiếp tục thực hiện SPMP trên Phần B để khai thác tính thưa thớt có cấu trúc, được chứa trong phân phối tiên nghiệm p(s): Cụ thể, trong Phần B, các nút nhân tử

h_{B,n} = v_{n→s_n}, n = 1,...,N (18)

mang thông tin của phân phối hậu nghiệm biến phân q(s), và nút nhân tử h mang thông tin tiên nghiệm có cấu trúc của p(s). Bằng cách thực hiện SPMP trên Phần B, tin nhắn v_{h→s_n} có thể được tính toán và sau đó được chuyển đến Module A. Hai module này trao đổi tin nhắn lặp đi lặp lại cho đến khi hội tụ hoặc vượt quá số lần lặp tối đa.

Sau đó, các đầu ra cuối cùng q(w), q(ρ) và q(s) của Module A là phân phối hậu nghiệm xấp xỉ cho w, ρ và s, tương ứng. Lưu ý rằng mặc dù SPMP không được đảm bảo hội tụ trên Phần B, vì p(s) của chúng ta là một tiên nghiệm MRF, cấu trúc mong muốn thường được thúc đẩy tốt trong đầu ra cuối cùng của Module A, như được minh họa trong kết quả mô phỏng. Điều này là do SPMP có thể đạt được kết quả tốt trên mô hình Ising 2-D và đã có nhiều công trình vững chắc liên quan đến SPMP trên các đồ thị có vòng lặp [27]. Và thuật toán nén VBI được đề xuất của chúng tôi trong Module A có thể đạt được hiệu suất đủ tốt để nắm bắt cấu trúc. Ngoài ra, vì thiết kế của p(s) linh hoạt, người ta có thể mô hình hóa nó với các tiên nghiệm chuỗi Markov hoặc tiên nghiệm cây Markov. Trong trường hợp này, sự hội tụ được đảm bảo. Trong phần sau, chúng tôi trình bày chi tiết hai module.

B. Bộ Ước lượng VBI Thưa thớt (Module A)

Trong Module A, chúng tôi muốn sử dụng một phân phối biến phân có thể tính toán q(w,ρ,s) để xấp xỉ phân phối hậu nghiệm không thể tính toán p̂(w,ρ,s|D) dưới tiên nghiệm p̂(w,ρ,s). Chất lượng của xấp xỉ này được đo bằng divergence Kullback-Leibler (divergence KL):

D_{KL}(q(w,ρ,s)||p̂(w,ρ,s|D))
= ∑_s ∫∫ q(w,ρ,s) ln[q(w,ρ,s)/p̂(w,ρ,s|D)]dwdρ. (19)

Do đó, mục tiêu là tìm phân phối biến phân tối ưu q(w,ρ,s) giảm thiểu divergence KL này. Tuy nhiên, divergence KL vẫn bao gồm việc tính toán hậu nghiệm không thể tính toán p̂(w,ρ,s|D). Để vượt qua thách thức này, việc giảm thiểu divergence KL thường được chuyển đổi thành một việc giảm thiểu cận dưới evidence âm (ELBO), tuân theo ràng buộc dạng phân tích nhân tử [28].

Bài toán VBI Thưa thớt:
min_{q(w,ρ,s)} −ELBO,
s.t. q(w,ρ,s) = ∏ᴺₙ₌₁ q(w_n)q(ρ_n)q(s_n), (20)

trong đó −ELBO = ∑_s ∫∫ q(w,ρ,s) ln p(D|w,ρ,s)dwdρ − D_{KL}(q(w,ρ,s)||p̂(w,ρ,s)). Ràng buộc có nghĩa là tất cả các biến riêng lẻ w, ρ và s được giả định là độc lập. Giả định như vậy được biết đến như giả định trường trung bình và được sử dụng rộng rãi trong các phương pháp VBI [16], [28].

Bài toán trong (20) không lồi và nói chung chúng ta chỉ có thể đạt được một giải pháp dừng q(w,ρ,s). Một giải pháp dừng có thể được đạt được bằng cách áp dụng thuật toán descent tọa độ khối (BCD) cho bài toán trong (20), như sẽ được chứng minh trong Lemma 1. Theo ý tưởng của thuật toán BCD, để giải quyết (20) tương đương với việc giải quyết lặp đi lặp lại ba bài toán con sau.

Bài toán con 1 (cập nhật ρ):
min_{q(ρ)} D_{KL}(q(ρ)||e^{p_ρ}),
s.t. q(ρ) = ∏ᴺₙ₌₁ q(ρ_n), (21)

trong đó ln e^{p_ρ} = ⟨ln p̂(w,ρ,s,D)⟩_{q(s)q(w)}.

Bài toán con 2 (cập nhật s):
min_{q(s)} D_{KL}(q(s)||e^{p_s}),
s.t. q(s) = ∏ᴺₙ₌₁ q(s_n), (22)

trong đó ln e^{p_s} = ⟨ln p̂(w,ρ,s,D)⟩_{q(ρ)q(w)}.

Bài toán con 3 (cập nhật w):
min_{q(w)} D_{KL}(q(w)||e^{p_w}),
s.t. q(w) = ∏ᴺₙ₌₁ q(w_n), (23)

trong đó ln e^{p_w} = ⟨ln p̂(w,ρ,s,D)⟩_{q(ρ)q(s)} và ⟨f(x)⟩_{q(x)} = ∫ f(x)q(x)dx. Các suy dẫn chi tiết của ba bài toán con được cung cấp trong Phụ lục A. Trong phần sau, chúng tôi trình bày chi tiết việc giải quyết ba bài toán con tương ứng.

--- TRANG 9 ---
TẠP CHÍ XXX 9

1) Cập nhật q(ρ): Trong bài toán con 1, rõ ràng, giải pháp tối ưu q*(ρ) được đạt được khi q*(ρ) = e^{p_ρ}. Trong trường hợp này, q*(ρ) có thể được suy ra như

q*(ρ) = ∏ᴺₙ₌₁ Γ(ρₙ; ãₙ, b̃ₙ), (24)

trong đó ãₙ và b̃ₙ được cho bởi:

ãₙ = ⟨sₙ⟩aₙ + ⟨1-sₙ⟩ãₙ + 1
= πₙaₙ + (1-πₙ)ãₙ + 1, (25)

b̃ₙ = |wₙ|²/2 + ⟨sₙ⟩bₙ + ⟨1-sₙ⟩b̃ₙ
= |μₙ|²+σ²ₙ/2 + πₙbₙ + (1-πₙ)b̃ₙ, (26)

trong đó μₙ và σ²ₙ là trung bình và phương sai hậu nghiệm của trọng số wₙ tương ứng, và πₙ là kỳ vọng hậu nghiệm của sₙ.

2) Cập nhật q(s): Trong bài toán con 2, chúng ta có thể đạt được giải pháp tối ưu q*(s) bằng cách đặt q*(s) = e^{p_s}. Trong trường hợp này, q*(s) có thể được suy ra như

q*(s) = ∏ᴺₙ₌₁ (πₙ)^{sₙ}(1-πₙ)^{1-sₙ}, (27)

trong đó πₙ = C₁/(C₁+C₂). C₁ và C₂ được cho bởi:

C₁ = πₙb^{aₙ}ₙ/Γ(aₙ) e^{(aₙ-1)⟨ln ρₙ⟩-bₙ⟨ρₙ⟩}, (28)

C₂ = (1-πₙ)b̃^{ãₙ}ₙ/Γ(ãₙ) e^{(ãₙ-1)⟨ln ρₙ⟩-b̃ₙ⟨ρₙ⟩}, (29)

trong đó ⟨ln ρₙ⟩ = ψ(ãₙ) - ln b̃ₙ, ψ(x) = d/dx ln Γ(x) là hàm digamma. Suy dẫn chi tiết của q*(ρ) và q*(s) được cung cấp trong Phụ lục B.

3) Cập nhật q(w): Mở rộng hàm mục tiêu trong bài toán con 3, chúng ta có

D_{KL}(q(w)||e^{p_w})
= E_{q(w)}[ln q(w)] - E_{q(w)}[⟨ln p(w,ρ,s,D)⟩_{q(ρ)q(s)}]
= E_{q(w)}[ln q(w) - ⟨ln p(w|ρ)⟩_{q(ρ)}]
- E_{q(w)}[ln p(D|w)] + const. (30)

Tương tự như nén mô hình Bayesian biến phân truyền thống [16], [15], hàm mục tiêu (30) chứa hai phần: "phần tiên nghiệm" E_{q(w)}[ln q(w) - ⟨ln p(w|ρ)⟩_{q(ρ)}] và "phần dữ liệu" E_{q(w)}[ln p(D|w)]. Phần tiên nghiệm buộc các trọng số tuân theo phân phối tiên nghiệm trong khi phần dữ liệu buộc các trọng số biểu hiện tập dữ liệu. Sự khác biệt duy nhất so với nén mô hình Bayesian biến phân truyền thống [16], [15] là số hạng tiên nghiệm ⟨ln p(w|ρ)⟩_{q(ρ)} được tham số hóa bởi ρ và mang cấu trúc được nắm bắt bởi tiên nghiệm phân cấp ba lớp. Mục tiêu của chúng ta là tìm q(w) tối ưu giảm thiểu D_{KL}(q(w)||e^{p_w}), tuân theo ràng buộc phân tích nhân tử q(w) = ∏ᴺₙ₌₁ q(wₙ). Có thể quan sát thấy rằng hàm mục tiêu chứa số hạng likelihood ln p(D|w), có thể rất phức tạp đối với các mô hình phức tạp. Do đó, bài toán con 3 không lồi và khó thu được giải pháp tối ưu. Trong phần sau, chúng tôi nhắm đến việc tìm một giải pháp dừng q(w) cho bài toán con 3. Có một số thách thức trong việc giải quyết bài toán con 3, được tóm tắt như sau.

Thách thức 1: Dạng đóng cho phần tiên nghiệm E_{q(w)}[ln q(w) - ⟨ln p(w|ρ)⟩_{q(ρ)}]. Trong nén mô hình Bayesian biến phân truyền thống, kỳ vọng này thường được tính toán bằng xấp xỉ [15], bao gồm lỗi xấp xỉ. Để tính toán kỳ vọng một cách chính xác, cần có dạng đóng cho kỳ vọng này.

Thách thức 2: Xấp xỉ xác định có độ phức tạp thấp cho phần dữ liệu E_{q(w)}[ln p(D|w)]. Trong nén mô hình Bayesian biến phân truyền thống [16], [15], kỳ vọng không thể tính toán này thường được xấp xỉ bằng lấy mẫu Monte Carlo. Tuy nhiên, xấp xỉ Monte Carlo đã được chỉ ra có độ phức tạp cao và độ mạnh mẽ thấp. Ngoài ra, phương sai của các ước lượng gradient Monte Carlo khó kiểm soát [19]. Do đó, cần có một xấp xỉ xác định có độ phức tạp thấp cho số hạng likelihood.

Dạng đóng cho E_{q(w)}[ln q(w) - ⟨ln p(w|ρ)⟩_{q(ρ)}]. Để vượt qua Thách thức 1, chúng tôi đề xuất sử dụng phân phối Gaussian làm hậu nghiệm xấp xỉ. Tức là, q(wₙ) = N(μₙ, σ²ₙ), q(w) = ∏ᴺₙ₌₁ q(wₙ), μₙ và σ²ₙ là các tham số biến phân mà chúng ta muốn tối ưu hóa. Sau đó, vì tiên nghiệm p(w|ρ) cũng được chọn làm phân phối Gaussian, E_{q(w)}[ln q(w) - ⟨ln p(w|ρ)⟩_{q(ρ)}] có thể được viết như divergence KL giữa hai phân phối Gaussian. Do đó, kỳ vọng có dạng đóng và có thể được tính toán đến một hằng số. Cụ thể, bằng cách mở rộng p(w|ρ), chúng ta có

⟨ln p(w|ρ)⟩ = ∑ᴺₙ₌₁ [1/2 ln ρₙ - ρₙw²ₙ/2]
= ∑ᴺₙ₌₁ [1/2⟨ln ρₙ⟩ - w²ₙ/2⟨ρₙ⟩ + const.]
= ∑ᴺₙ₌₁ (ln p(wₙ|⟨ρₙ⟩) + const.)
= ln p(w|⟨ρ⟩) + const., (31)

trong đó chúng tôi sử dụng ⟨·⟩ để thay thế ⟨·⟩_{q(ρ)} hoặc ⟨·⟩_{q(ρ)} cho đơn giản. Do đó, chúng ta có

E_{q(w)}[ln q(w) - ⟨ln p(w|ρ)⟩_{q(ρ)}]
= E_{q(w)}[ln q(w) - ln p(w|E[ρ])] + const.
= D_{KL}(q(w)||p(w|E[ρ])) + const. (32)

Vì q(w) và p(w|E[ρ]) đều là phân phối Gaussian, divergence KL giữa chúng có dạng đóng:

D_{KL}(q(w)||p(w|E[ρ])) = ∑ᴺₙ₌₁ [ln σ̃ₙ/σₙ + (σ²ₙ + μ²ₙ)/(2σ̃²ₙ) - 1/2], (33)

--- TRANG 10 ---
TẠP CHÍ XXX 10

trong đó σ̃²ₙ = 1/E[ρₙ] là phương sai của tiên nghiệm p(w|E[ρ]). Vì mục tiêu của chúng ta là giảm thiểu (30), số hạng hằng số trong (32) có thể được bỏ qua.

Xấp xỉ xác định có độ phức tạp thấp cho E_{q(w)}[ln p(D|w)]. Để vượt qua Thách thức 2, chúng tôi đề xuất một xấp xỉ xác định có độ phức tạp thấp cho E_{q(w)}[ln p(D|w)] dựa trên khai triển Taylor.

Để đơn giản, gọi g(w) biểu thị hàm phức tạp ln p(D|w). Cụ thể, chúng tôi muốn xây dựng một xấp xỉ xác định cho E_w[q(w)]g(w) với các tham số biến phân μₙ và σ²ₙ, n = 1,...,N. Theo thủ thuật tham số hóa lại trong [15] và [29], chúng tôi biểu diễn wₙ bằng μₙ + σₙε̃ₙ, ε̃ₙ ~ N(0,1). Bằng cách này, wₙ được chuyển đổi thành một hàm xác định có thể vi phân của một nhiễu không tham số ε̃ₙ, và E_w[q(w)]g(w) được chuyển đổi thành E_{N(0,1)}[g(μ + σ⊙ε̃)].

Lưu ý rằng trong nén mô hình Bayesian biến phân truyền thống, kỳ vọng này được xấp xỉ bằng lấy mẫu ε̃. Tuy nhiên, để xây dựng một xấp xỉ xác định, chúng tôi lấy xấp xỉ Taylor bậc nhất của E_{N(0,1)}[g(μ + σ⊙ε̃)] tại điểm ε̃ = E[ε̃] = 0, và chúng ta có

E[g(μ + σ⊙ε̃)] ≈ g(μ + σ⊙E[ε̃]) = g(μ). (34)

Tức là, chúng tôi sử dụng xấp xỉ Taylor bậc nhất để thay thế E[g(μ + σ⊙ε̃)]. Kết quả mô phỏng cho thấy xấp xỉ được đề xuất của chúng tôi có thể đạt được hiệu suất tốt. Ngoài ra, vì xấp xỉ được đề xuất của chúng tôi là xác định, phương sai gradient có thể được loại bỏ.

Dựa trên thảo luận nói trên, bài toán con 3 có thể được giải quyết bằng cách giải quyết một bài toán xấp xỉ như sau.

Bài toán con 3 xấp xỉ:
min_{μ,σ²} ∑ᴺₙ₌₁ [ln σ̃ₙ/σₙ + (σ²ₙ + μ²ₙ)/(2σ̃²ₙ) - 1/2] - ln p(D|μ). (35)

Bài toán này tương đương với việc huấn luyện một mạng thần kinh Bayesian với (35) làm hàm mất mát.

4) Hội tụ của VBI Thưa thớt: VBI thưa thớt trong Module A thực tế là một thuật toán BCD để giải quyết bài toán trong (20). Bằng ý nghĩa vật lý của ELBO và divergence KL, hàm mục tiêu liên tục trên một tập mức compact. Ngoài ra, hàm mục tiêu có một cực tiểu duy nhất cho hai khối tọa độ (q(ρ) và q(s)) [17], [28]. Vì chúng ta có thể đạt được một điểm dừng cho khối tọa độ thứ ba q(w), hàm mục tiêu đều tại các điểm cluster được tạo ra bởi thuật toán BCD. Dựa trên thảo luận trên, thuật toán BCD được đề xuất của chúng tôi thỏa mãn các yêu cầu hội tụ thuật toán BCD trong [30]. Do đó, chúng ta có lemma hội tụ sau.

Lemma 1. (Hội tụ của VBI Thưa thớt): Mọi điểm cluster q(w,ρ,s) = q*(ρ)q*(s)q̃(w) được tạo ra bởi thuật toán VBI thưa thớt là một giải pháp dừng của bài toán (20).

C. Truyền Tin (Module B)

Trong Module B, SPMP được thực hiện trên đồ thị nhân tử hỗ trợ Gₛ và tin nhắn chuẩn hóa vₛₙ→h_B được phản hồi đến Module A như xác suất tiên nghiệm p̂(sₙ).

Chúng tôi tập trung vào một lớp kết nối đầy đủ với K nơ-ron đầu vào và M nơ-ron đầu ra để trình bày dễ dàng. Tiên nghiệm hỗ trợ p(s) cho lớp K×M này được mô hình hóa như một MRF. Đồ thị nhân tử Gₛ được minh họa trong Hình 7. Ở đây chúng tôi sử dụng sᵢ,ⱼ để biểu thị biến hỗ trợ ở hàng thứ i và cột thứ j. Nút nhân tử đơn hB,i,j mang xác suất được cho bởi (18). Các nút nhân tử cặp mang xác suất chuyển tiếp hàng và cột p(srow,m+1|srow,m) và p(scol,k+1|scol,k), tương ứng. Các tham số p^row₀₁, p^row₁₀ và p^col₀₁, p^col₁₀ cho mô hình MRF được cho trong Phần II-C. Gọi vₛ→f(s) biểu thị tin nhắn từ nút biến s đến nút nhân tử f, và vf→s(s) biểu thị tin nhắn từ nút nhân tử f đến nút biến s. Theo luật sum-product, các tin nhắn được cập nhật như sau:

vₛ→f(s) = ∏_{h∈n(s)\{f}} vh→s(s), (36)

vf→s(s) = {
f(s) nếu f là nút nhân tử đơn;
∑ₜ f(s,t)vₜ→f(t) nếu f là nút nhân tử cặp;
} (37)

trong đó n(s)\{f} biểu thị các nút nhân tử láng giềng của s ngoại trừ nút f, và t biểu thị nút biến láng giềng khác của f ngoại trừ nút s. Sau khi thuật toán SPMP được thực hiện, tin nhắn cuối cùng vₛ→h_B từ mỗi nút biến sᵢ,ⱼ đến nút nhân tử đơn kết nối hB,i,j được gửi trở lại Module A.

Thuật toán tổng thể được tóm tắt trong Thuật toán 1.

Nhận xét 2. (So sánh với suy luận biến phân truyền thống) Suy luận biến phân truyền thống không thể xử lý tiên nghiệm thưa thớt ba lớp được đề xuất p(w,ρ,s) trong khi Turbo-VBI được đề xuất của chúng tôi được thiết kế đặc biệt để xử lý nó. Trong các công trình nén Bayesian hiện có [14]-[16], divergence KL giữa hậu nghiệm thực p(w|D) và hậu nghiệm biến phân q(w) dễ tính toán, do đó divergence KL có thể được tối ưu hóa trực tiếp. Do đó, suy luận biến phân đơn giản có thể được áp dụng trực tiếp. Lý do cơ bản đằng sau điều này là tiên nghiệm p(w) trong các công trình này tương đối đơn giản, và chúng thường chỉ có một lớp [14], [15] hoặc hai lớp [16]. Do đó, divergence KL giữa tiên nghiệm p(w) và hậu nghiệm biến phân q(w) dễ tính toán, điều này là bắt buộc trong việc tính toán divergence KL giữa hậu nghiệm thực p(w|D) và hậu nghiệm biến phân q(w). Tuy nhiên, các tiên nghiệm đơn giản này không thể thúc đẩy cấu trúc đều mong muốn trong ma trận trọng số và thiếu tính linh hoạt để cấu hình cấu trúc được cắt tỉa. Do đó, chúng tôi giới thiệu lớp hỗ trợ bổ sung s, dẫn đến tiên nghiệm phân cấp ba lớp p(w,ρ,s). Với tiên nghiệm ba lớp này, suy luận biến phân truyền thống không thể được áp dụng vì divergence KL giữa tiên nghiệm p(w,ρ,s) và hậu nghiệm biến phân q(w,ρ,s) khó tính toán. Để suy luận hậu nghiệm, chúng tôi đề xuất tách đồ thị nhân tử thành hai phần và thực hiện suy luận Bayesian lặp đi lặp lại, dẫn đến thuật toán Turbo-VBI được đề xuất của chúng tôi.

V. PHÂN TÍCH HIỆU SUẤT

Trong phần này, chúng tôi đánh giá hiệu suất của phương pháp nén mô hình dựa trên Turbo-VBI được đề xuất trên một số mô hình mạng thần kinh và tập dữ liệu tiêu chuẩn. Chúng tôi cũng so sánh với một số phương pháp cơ sở, bao gồm các phương pháp nén mô hình cổ điển và tiên tiến. Các phương pháp cơ sở được xem xét được liệt kê như sau:

1) Variational dropout (VD): Đây là một phương pháp rất cổ điển trong lĩnh vực nén mô hình Bayesian, được đề xuất trong [15]. Một tiên nghiệm log-uniform không đúng một lớp được gán cho mỗi trọng số để tạo ra tính thưa thớt trong quá trình huấn luyện.

2) Group variational dropout (GVD): Điều này được đề xuất trong [16], có thể được coi là một phần mở rộng của VD thành cắt tỉa nhóm. Trong thuật toán này, mỗi trọng số tuân theo một tiên nghiệm Gaussian có trung bình bằng không. Các tỷ lệ tiên nghiệm của các trọng số trong cùng một nhóm cùng tuân theo một phân phối log-uniform không đúng hoặc một phân phối half-Cauchy đúng để thực hiện cắt tỉa nhóm. Trong bài báo này, chúng tôi chọn tiên nghiệm log-uniform không đúng để so sánh. Lưu ý rằng một số phương pháp nén Bayesian gần đây hơn chủ yếu tập trung vào góc độ lượng tử hóa [14], [31], đây là một phạm vi khác với công trình của chúng tôi. Mặc dù công trình của chúng tôi có thể được mở rộng thêm vào lĩnh vực lượng tử hóa, chúng tôi chỉ tập trung vào cắt tỉa có cấu trúc ở đây. Theo hiểu biết tốt nhất của chúng tôi, GVD vẫn là một trong những phương pháp cắt tỉa Bayesian tiên tiến mà không xem xét lượng tử hóa.

3) Cắt tỉa dựa trên regularizer phân cực (polarization): Điều này được đề xuất trong [9]. Thuật toán có thể đẩy một số nhóm trọng số về không trong khi những nhóm khác về các giá trị lớn hơn bằng cách gán một regularizer phân cực cho hệ số tỷ lệ batch norm của mỗi nhóm trọng số

min_w 1/D ∑^D_{d=1} L(NN(x_d,w),y_d) + R(w) + λ(t|γ_k|_1^β - |γ_k|_∞^β/n_k^{1/β}),

trong đó λ(t|γ_k|_1^β - |γ_k|_∞^β/n_k^{1/β}) là regularizer phân cực được đề xuất và γ là hệ số tỷ lệ batch norm của mỗi trọng số.

4) Single-shot network pruning (SNIP): Điều này được đề xuất trong [1]. Thuật toán đo tầm quan trọng của trọng số bằng cách giới thiệu một biến phụ trợ cho mỗi trọng số. Tầm quan trọng của mỗi trọng số được đạt được trước khi huấn luyện bằng cách tính toán gradient của "biến tầm quan trọng" của nó. Sau đó, các trọng số có tầm quan trọng ít hơn được cắt tỉa trước khi huấn luyện bình thường.

Chúng tôi thực hiện thí nghiệm trên các kiến trúc LeNet-5, AlexNet và VGG-11, và sử dụng các tập dữ liệu benchmark sau đây để so sánh hiệu suất.

1) Fashion-MNIST [32]: Nó bao gồm một tập huấn luyện gồm 60000 ví dụ và một tập kiểm tra gồm 10000 ví dụ. Mỗi ví dụ là một hình ảnh grayscale 28×28 của đồ thời trang thực tế, được liên kết với một nhãn từ 10 lớp.

2) CIFAR-10 [33]: Đây là tập dữ liệu được sử dụng rộng rãi nhất để so sánh các phương pháp cắt tỉa mạng thần kinh. Nó bao gồm một tập huấn luyện gồm 50000 ví dụ và một tập kiểm tra gồm 10000 ví dụ. Mỗi ví dụ là một hình ảnh RGB ba kênh 32×32 của đối tượng thực tế, được liên kết với một nhãn từ 10 lớp.

3) CIFAR-100 [33]: Nó được coi là phiên bản khó hơn của tập dữ liệu CIFAR-10. Thay vì 10 lớp trong CIFAR-10, nó bao gồm 100 lớp với mỗi lớp chứa 600 hình ảnh. Có 500 hình ảnh huấn luyện và 100 hình ảnh kiểm tra trong mỗi lớp.

Chúng tôi sẽ so sánh phương pháp nén mô hình dựa trên Turbo-VBI được đề xuất với các phương pháp cơ sở từ các khía cạnh khác nhau, bao gồm 1) so sánh hiệu suất tổng thể; 2) trực quan hóa cấu trúc ma trận trọng số; 3) tăng tốc CPU và GPU và 4) độ mạnh mẽ của mạng thần kinh được cắt tỉa.

Chúng tôi chạy LeNet-5 trên Fashion-MNIST, AlexNet trên CIFAR-10 và VGG trên CIFAR-100. Trong suốt các thí nghiệm, chúng tôi đặt tỷ lệ học tập là 0.01, và a=b=ã=1; b̃=1×10^{-3}. Đối với LeNet-5+Fashion-MNIST, chúng tôi đặt kích thước batch là 64, và đối với AlexNet+CIFAR-10 và VGG+CIFAR-100, chúng tôi đặt kích thước batch là 128. Tất cả các phương pháp ngoại trừ SNIP đều trải qua fine tuning sau khi cắt tỉa. Đối với LeNet, fine tuning là 15 epoch trong khi đối với AlexNet và VGG, fine tuning là 30 epoch. Lưu ý rằng các mô hình và cài đặt tham số có thể không đạt được độ chính xác tiên tiến nhưng đủ để so sánh.

A. So sánh Hiệu suất Tổng thể

Trước tiên chúng tôi đưa ra so sánh hiệu suất tổng thể của các phương pháp khác nhau về độ chính xác, tỷ lệ thưa thớt, cấu trúc được cắt tỉa và giảm FLOP. Kết quả được tóm tắt trong Bảng II. Cấu trúc được đo bằng kênh đầu ra cho các lớp tích chập và nơ-ron cho các lớp kết nối đầy đủ. Có thể quan sát thấy rằng phương pháp được đề xuất có thể đạt được tỷ lệ thưa thớt cực kỳ thấp, ở cùng mức (hoặc thậm chí thấp hơn) so với các phương pháp cắt tỉa không có cấu trúc (VD và SNIP). Hơn nữa, nó cũng có thể đạt được cấu trúc được cắt tỉa gọn gàng hơn so với các phương pháp cắt tỉa có cấu trúc (polarization và GVD). Đồng thời, phương pháp được đề xuất có thể duy trì độ chính xác cạnh tranh. Chúng tôi tóm tắt rằng hiệu suất vượt trội của phương pháp được đề xuất đến từ hai khía cạnh: 1) khả năng cắt tỉa trọng số riêng lẻ vượt trội, dẫn đến tỷ lệ thưa thớt thấp, và 2) khả năng cắt tỉa nơ-ron vượt trội, dẫn đến FLOP thấp. So với các phương pháp cắt tỉa ngẫu nhiên như VD và SNIP thường có thể đạt được tỷ lệ thưa thớt thấp hơn so với các phương pháp cắt tỉa nhóm, phương pháp được đề xuất có thể đạt được tỷ lệ thưa thớt thậm chí thấp hơn. Điều này cho thấy tiên nghiệm thưa thớt được đề xuất có hiệu quả cao trong việc cắt tỉa các trọng số riêng lẻ. Điều này là do khả năng chính quy hóa của p(w|ρ) có thể được cấu hình bằng cách đặt b̃ nhỏ. So với các phương pháp cắt tỉa nhóm như GVD và polarization, phương pháp được đề xuất có thể đạt được cấu trúc gọn gàng hơn. Điều này cho thấy phương pháp được đề xuất cũng có hiệu quả cao trong việc cắt tỉa nơ-ron. Lý do có hai mặt: Đầu tiên, mỗi hàng trong tiên nghiệm p(s) có thể được xem như một chuỗi Markov. Khi một số trọng số của một nơ-ron bị cắt tỉa, các trọng số khác cũng sẽ có xác suất rất cao bị cắt tỉa. Thứ hai, các trọng số còn sót lại trong một ma trận trọng số có xu hướng tập trung thành cụm, có nghĩa là các trọng số còn sót lại của các nơ-ron khác nhau trong một lớp có xu hướng kích hoạt các nơ-ron giống nhau trong lớp tiếp theo. Điều này cải thiện đáng kể tính đều từ phía đầu vào, trong khi các phương pháp cắt tỉa nhóm hiện có như GVD và polarization chỉ có thể chính quy hóa phía đầu ra của một nơ-ron.

Trực quan hóa Cấu trúc Được cắt tỉa

Trong phần này, chúng tôi trực quan hóa ma trận trọng số được cắt tỉa của phương pháp được đề xuất và so sánh với các phương pháp cơ sở để trình bày thêm về lợi ích mang lại bởi cấu trúc đều được đề xuất. Hình 8 minh họa 6 kernel trong lớp đầu tiên của LeNet. Chúng tôi chọn GVD và polarization để so sánh ở đây vì hai phương pháp này cũng là các phương pháp cắt tỉa có cấu trúc. Có thể quan sát thấy rằng GVD và polarization đều có thể cắt tỉa toàn bộ kernel nhưng các trọng số trong các kernel còn lại không thể bị cắt tỉa và hoàn toàn không có cấu trúc. Tuy nhiên, trong phương pháp được đề xuất, các trọng số trong các kernel còn lại được cắt tỉa rõ ràng theo cách có cấu trúc. Đặc biệt, chúng tôi thấy rằng ba kernel còn lại 5×5 đều vừa với các kernel nhỏ hơn

--- TRANG 11 ---
TẠP CHÍ XXX 11

[THIS IS FIGURE: Thuật toán 1 - Thuật toán Turbo-VBI cho Nén Mô hình]

Thuật toán 1 Thuật toán Turbo-VBI cho Nén Mô hình
Đầu vào: tập huấn luyện D, tiên nghiệm p(w), p(ρ), p(s), số lần lặp tối đa I_max.
Đầu ra: w, ρ, s.
1: Khởi tạo π_n.
2: for k = 1, ..., I_m do
3:    Module A:
4:    Khởi tạo phân phối biến phân q(ρ) và q(s).
5:    while chưa hội tụ do
6:        Cập nhật q(ρ), q(s) và q(w).
7:    end while
8:    Tính toán v_{n→s_n} dựa trên (17) và gửi v_{n→s_n} đến Module B.
9:    Module B:
10:   Thực hiện SPMP trên G_s, gửi v_{h→s_n} đến Module A.
11:   if hội tụ then
12:       break
13:   end if
14:   k = k + 1.
15: end for
16: Đầu ra ŵ = arg max_w q(w), ρ̂ = arg max_ρ q*(ρ), ŝ = arg max_s q*(s).

[THIS IS FIGURE: Hình 7 - Minh họa đồ thị nhân tử hỗ trợ trong Module B với K nơ-ron đầu vào và M nơ-ron đầu ra]

Hình 8: Cấu trúc kernel của CONV_1 trong LeNet.

(a) Thời gian suy luận trung bình trên CPU.
(b) Thời gian suy luận trung bình trên GPU.

Hình 9: Thời gian trung bình được tiêu tốn bởi một lần forward pass của một batch trên CPU và GPU. Đối với LeNet và AlexNet, kích thước batch là 10000 ví dụ và mỗi kết quả được tính trung bình trên 1000 thí nghiệm. Đối với VGG, kích thước batch là 1000 ví dụ và mỗi kết quả được tính trung bình trên 100 thí nghiệm.

có kích thước 3×3. Điều này minh họa rõ ràng cấu trúc cụm được thúc đẩy bởi tiên nghiệm MRF. Với việc cắt tỉa cụm, chúng ta có thể trực tiếp thay thế các kernel lớn ban đầu bằng các kernel nhỏ hơn, và tận hưởng lợi ích lưu trữ và tính toán của kích thước kernel nhỏ hơn.

B. Hiệu suất Tăng tốc

Trong phần này, chúng tôi so sánh hiệu suất tăng tốc của phương pháp được đề xuất với các phương pháp cơ sở. Lưu ý rằng mục tiêu chính của cắt tỉa có cấu trúc là giảm độ phức tạp tính toán và tăng tốc suy luận của các mạng thần kinh. Trong ma trận trọng số của phương pháp được đề xuất, chúng tôi ghi lại các cụm lớn hơn 3×3 như một toàn thể. Tức là, chúng tôi ghi lại vị trí và kích thước của các cụm thay vì ghi lại vị trí chính xác cho mỗi phần tử. Trong Hình 9, chúng tôi vẽ thời gian trung bình được tiêu tốn bởi một lần forward pass của một batch trên CPU và GPU tương ứng. Có thể quan sát thấy rằng phương pháp được đề xuất có thể đạt được hiệu suất tăng tốc tốt nhất trên các mô hình khác nhau. Đối với LeNet, phương pháp được đề xuất có thể đạt được mức tăng 1.50× trên CPU và 2.85× trên GPU. Đối với AlexNet, kết quả là 1.63× và 1.64×. Đối với VGG, kết quả là 2.19× và 1.88×. Lưu ý rằng các phương pháp cắt tỉa không có cấu trúc hiển thị ít mức tăng tốc vì cấu trúc thưa thớt của chúng hoàn toàn ngẫu nhiên. Chúng tôi tóm tắt lý do cho mức tăng tốc của chúng tôi từ hai khía cạnh: 1) Cắt tỉa nơ-ron hiệu quả hơn. Như được hiển thị trong phần V-A, phương pháp được đề xuất có thể dẫn đến mô hình gọn gàng hơn và do đó dẫn đến giảm FLOP lớn hơn. 2) Giải mã ma trận trọng số hiệu quả hơn. Các phương pháp hiện có yêu cầu lưu trữ vị trí chính xác cho mỗi trọng số không bị cắt tỉa, bất kể ở định dạng CSR, CSC hay COO vì cấu trúc thưa thớt không đều. Tuy nhiên, trong phương pháp được đề xuất, chúng tôi chỉ cần vị trí và kích thước cho mỗi cụm để giải mã tất cả các trọng số không bị cắt tỉa trong cụm. Do đó, việc giải mã nhanh hơn nhiều. Lưu ý rằng hiệu suất của phương pháp được đề xuất có thể được cải thiện thêm nếu cấu trúc được khám phá trong quá trình nhân ma trận-ma trận. Ví dụ, các khối không có thể được bỏ qua trong matrix tiling, như đã thảo luận trong phần V-A. Tuy nhiên, điều này bao gồm việc sửa đổi trong mã CUDA cấp thấp hơn nên chúng tôi không nghiên cứu ở đây.

C. Hiểu biết về Thuật toán Được đề xuất

Trong phần này, chúng tôi cung cấp một số hiểu biết về phương pháp được đề xuất. Đầu tiên, Hình 10 minh họa hành vi hội tụ của phương pháp được đề xuất trên các nhiệm vụ khác nhau. Có thể quan sát thấy rằng nói chung phương pháp được đề xuất có thể hội tụ trong 15 lần lặp trong tất cả các nhiệm vụ. Thứ hai, chúng tôi minh họa cách cấu trúc được dần dần nắm bắt bởi thông tin tiên nghiệm v_{h→s_n} từ Module B và cách hậu nghiệm trong Module A được chính quy hóa theo cấu trúc này. Vì cấu trúc cụm trong kernel đã được minh họa trong Hình 8, ở đây trong Hình 11, chúng tôi trực quan hóa tin nhắn v_{h→s_n} và v_{n→s_n} của lớp FC_3 trong AlexNet. Có thể quan sát thấy rằng trong lần lặp 1, tin nhắn v_{n→s_n} không có cấu trúc vì tiên nghiệm p̂(s) trong Module A được khởi tạo ngẫu nhiên. Trong các lần lặp của thuật toán, v_{n→s_n} từ Module B dần dần nắm bắt cấu trúc từ tiên nghiệm MRF và hoạt động như một chính quy hóa trong Module A, sao cho cấu trúc được dần dần thúc đẩy trong v_{h→s_n}. Cuối cùng, ma trận hỗ trợ

--- TRANG 12 ---
TẠP CHÍ XXX 12

[THIS IS TABLE: Bảng so sánh hiệu suất tổng thể với các phương pháp cơ sở trên các mô hình mạng và tập dữ liệu khác nhau. Độ chính xác Top-1 được báo cáo. Kết quả tốt nhất được in đậm.]

Network | Method | Accuracy (%) | |w≠0|/|w|(%) | Pruned structure | FLOPs reduction (%)
---|---|---|---|---|---
No pruning | 89.01 | 100 | 6-16-120-84 | 0
VD | 88.91 | 6.16 | 6-15-49-51 | 11.82
LeNet-5 | GVD | 88.13 | 4.20 | 5-7-21-23 | 53.89
6-16-120-84 | Polarization | 87.27 | 17.96 | 4-8-46-27 | 59.04
SNIP | 82.83 | 9.68 | 5-16-73-57 | 19.82
Proposed | 88.77 | 1.14 | 3-5-16-17 | 76.03

(a) Kết quả trên LeNet+Fashion-MNIST.

Network | Method | Accuracy (%) | |w≠0|/|w|(%) | Pruned structure | FLOPs reduction (%)
---|---|---|---|---|---
No pruning | 66.18 | 100 | 6-16-32-64-128-120-84 | 0
VD | 64.91 | 3.17 | 6-16-31-61-117-42-74 | 6.12
AlexNet | GVD | 64.82 | 9.30 | 6-16-27-31-21-17-14 | 39.29
6-16-32-64-128-120-84 | Polarization | 63.95 | 25.95 | 4-15-16-20-19-58-53 | 65.27
SNIP | 63.19 | 11.75 | 6-16-30-57-102-56-68 | 12.51
Proposed | 65.57 | 3.42 | 6-16-25-20-7-7-7 | 45.94

(b) Kết quả trên AlexNet+CIFAR-10.

Network | Method | Accuracy (%) | |w≠0|/|w|(%) | Pruned structure | FLOPs reduction (%)
---|---|---|---|---|---
No pruning | 68.28 | 100 | 64-128-256-256-512-512-512-512-1000 | 0
VD | 67.24 | 5.39 | 64-128-256-211-447-411-293-279-441 | 6.63
VGG-11 | GVD | 66.52 | 9.05 | 63-112-204-134-271-174-91-77-336 | 60.90
64-128-256-256-512-512-512-512-100 | Polarization | 64.41 | 26.74 | 62-122-195-148-291-133-76-74-68 | 59.54
SNIP | 65.20 | 17.79 | 64-128-254-246-441-479-371-274-69 | 7.92
Proposed | 67.12 | 4.68 | 60-119-191-148-181-176-44-64-59 | 63.14

(c) Kết quả trên VGG11+CIFAR-100.

có cấu trúc cụm và ma trận trọng số cũng có cấu trúc tương tự.

D. Độ mạnh mẽ của Mô hình Được cắt tỉa

Trong phần này, chúng tôi đánh giá độ mạnh mẽ của mô hình được cắt tỉa. Độ mạnh mẽ của mô hình được cắt tỉa thường bị bỏ qua trong các phương pháp cắt tỉa mô hình trong khi nó có tầm quan trọng cao [34], [35], [36]. Chúng tôi so sánh độ mạnh mẽ mô hình của phương pháp được đề xuất với các phương pháp cơ sở trên mô hình LeNet và tập dữ liệu Fashion-MNIST. Chúng tôi tạo ra 10000 ví dụ đối kháng bằng phương pháp fast gradient sign method (FGSM) và truyền chúng qua các mô hình được cắt tỉa. Trong Hình 12, chúng tôi vẽ độ chính xác mô hình dưới các ε khác nhau. Cả độ chính xác có và không có fine tuning đối kháng được báo cáo. Có thể quan sát thấy rằng tất cả các phương pháp đều cho thấy sự sụt giảm nhanh chóng mà không có fine tuning. Cụ thể, phương pháp được đề xuất cho thấy sự sụt giảm độ chính xác 72.1% từ 89.01% khi ε = 0 xuống 24.86% khi ε = 0.05, trong khi SNIP báo cáo sự sụt giảm lớn nhất là 73.4%. Tuy nhiên, chúng tôi cũng quan sát thấy rằng các phương pháp cắt tỉa có cấu trúc (được đề xuất và polarization) cho thấy độ mạnh mẽ mạnh hơn so với các phương pháp không có cấu trúc (VD và SNIP). Chúng tôi nghĩ lý do tại sao phương pháp được đề xuất và polarization hoạt động tốt hơn có thể khác nhau. Đối với polarization, các trọng số không bị cắt tỉa nhiều hơn đáng kể so với các phương pháp cơ sở khác, có nghĩa là các bộ lọc quan trọng còn lại có nhiều tham số hơn đáng kể. Điều này cho phép mô hình có khả năng mạnh mẽ để chống lại nhiễu. Đối với phương pháp được đề xuất, lý do có thể là nó có thể ép buộc các trọng số tập trung thành nhóm. Do đó, mô hình có thể "tập trung" vào một số đặc trưng cụ thể ngay cả khi chúng bị thêm nhiễu. Có thể quan sát thấy rằng sau một fine tuning đối kháng, tất cả các phương pháp đều cho thấy cải thiện. VD cho thấy cải thiện lớn nhất là 77.31% tại ε = 0.1 trong khi phương pháp được đề xuất chỉ cho thấy cải thiện vừa phải là 56.26%. Lý do có thể đằng sau điều này là phương pháp được đề xuất "quá" thưa thớt và các trọng số còn lại không thể học các đặc trưng mới. Lưu ý rằng SNIP cho thấy cải thiện nhỏ nhất. Lý do có thể là trong SNIP, các trọng số được cắt tỉa trước khi huấn luyện và các biến tầm quan trọng phụ trợ có thể không đo tầm quan trọng trọng số một cách chính xác, dẫn đến một số trọng số quan trọng bị cắt tỉa. Do đó, khả năng biểu đạt của mô hình có thể bị tổn hại.

VI. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Kết luận

Chúng tôi đã xem xét bài toán nén mô hình từ góc độ Bayesian. Đầu tiên chúng tôi đề xuất một tiên nghiệm thưa thớt phân cấp ba lớp trong đó một lớp hỗ trợ bổ sung được sử dụng để nắm bắt cấu trúc mong muốn cho các trọng số. Do đó, tiên nghiệm thưa thớt được đề xuất có thể thúc đẩy cả tính thưa thớt có cấu trúc ở cấp độ trọng số từng nơ-ron và tính thưa thớt có cấu trúc ở cấp độ nơ-ron. Sau đó, chúng tôi suy ra một thuật toán nén Bayesian dựa trên Turbo-VBI cho bài toán nén mô hình kết quả. Thuật toán được đề xuất có độ phức tạp thấp và độ mạnh mẽ cao. Chúng tôi đã thiết lập thêm sự hội tụ của phần VBI thưa thớt trong thuật toán Turbo-VBI. Kết quả mô phỏng cho thấy thuật toán nén mô hình dựa trên Turbo-VBI được đề xuất có thể thúc đẩy cấu trúc đều hơn trong các mạng thần kinh được cắt tỉa trong khi đạt được hiệu suất thậm chí tốt hơn về tỷ lệ nén và độ chính xác suy luận so với các phương pháp cơ sở.

Công việc Tương lai

Với phương pháp nén mô hình có cấu trúc dựa trên Turbo-VBI được đề xuất, chúng ta có thể thúc đẩy các cấu trúc đều hơn và linh hoạt hơn và đạt được hiệu suất nén vượt trội trong quá trình cắt tỉa mạng thần kinh. Điều này cũng mở ra nhiều khả năng hơn cho nghiên cứu tương lai, như được liệt kê dưới đây:

Học Liên kết Hiệu quả Truyền thông: Phương pháp được đề xuất cho thấy tiềm năng lớn trong tình huống học liên kết. Đầu tiên, nó có thể thúc đẩy cấu trúc đều hơn trong ma trận trọng số sao cho ma trận trọng số có entropy thấp hơn. Do đó, cấu trúc đều có thể được tận dụng thêm để giảm chi phí truyền thông trong học liên kết. Thứ hai, thiết kế hai module của thuật toán Turbo-VBI tự nhiên phù hợp với môi trường học liên kết. Máy chủ có thể thúc đẩy cấu trúc thưa thớt toàn cục bằng cách chạy Module B trong khi các máy khách có thể thực hiện huấn luyện Bayesian cục bộ bằng cách chạy Module A.

Thiết kế Kết hợp Lượng tử hóa và Cắt tỉa: Các công trình hiện có [16], [14] đã chỉ ra rằng lượng tử hóa và cắt tỉa có thể được đạt được đồng thời bằng nén mô hình Bayesian. Do đó, cũng có thể nén thêm mô hình bằng cách kết hợp lượng tử hóa với phương pháp được đề xuất.

PHỤ LỤC

A. Suy dẫn Ba Bài toán con

Ở đây, chúng tôi cung cấp suy dẫn từ bài toán gốc (20) đến ba bài toán con (21), (22) và (23). Để đơn giản biểu thức, gọi z = [z₁, z₂, z₃] biểu thị tất cả các biến w, ρ và s, trong đó z₁ = w, z₂ = ρ và z₃ = s. Sau đó, bài toán gốc (20) có thể được viết tương đương như

Bài toán VBI Thưa thớt:
min_{q(z)} −ELBO,
s.t. q(z) = ∏³ₙ₌₁ q(zₙ). (38)

Sau đó, bằng (15) trong [28], chúng ta có

−ELBO
= ∫ q(z) ln p̂(D|z)dz − D_{KL}(q(z)||p̂(z))
= ∫ q(z) ln [p̂(D,z)/q(z)]dz
= ∫ ∏³ᵢ₌₁ q(zᵢ) [ln p̂(D,z) − ∑³ᵢ₌₁ ln q(zᵢ)] ∏³ᵢ₌₁ dzᵢ
= ∫ ∏³ᵢ₌₁ q(zᵢ) ln p̂(D,z) ∏³ᵢ₌₁ dzᵢ − ∑³ᵢ₌₁ ∫ ∏³ⱼ₌₁ q(zⱼ) ln q(zᵢ)dzᵢ
= ∫ q(zⱼ) [ln p̂(D,z) ∏ᵢ≠ⱼ q(zᵢ)dzᵢ] dzⱼ − ∫ q(zⱼ) ln q(zⱼ)dzⱼ − ∑ᵢ≠ⱼ ∫ q(zᵢ) ln q(zᵢ)dzᵢ
= ∫ q(zⱼ) ln e^{p̂ⱼ}dzⱼ − ∫ q(zⱼ) ln q(zⱼ)dzⱼ − ∑ᵢ≠ⱼ ∫ q(zᵢ) ln q(zᵢ)dzᵢ
= D_{KL}(q(zⱼ)||e^{p̂ⱼ}) − ∑ᵢ≠ⱼ ∫ q(zᵢ) ln q(zᵢ)dzᵢ,

trong đó ln e^{p̂ⱼ} = ∫ ln p̂(D,z) ∏ᵢ≠ⱼ q(zᵢ)dzᵢ = ⟨p̂(D,z)⟩ᵢ≠ⱼ. Rõ ràng là −ELBO được giảm thiểu khi D_{KL}(q(zⱼ)||e^{p̂ⱼ}) được giảm thiểu. Do đó, Bài toán VBI Thưa thớt (38) có thể được giải quyết bằng cách giảm thiểu lặp đi lặp lại D_{KL}(q(zⱼ)||e^{p̂ⱼ}) cho j = 1, 2, 3.

B. Suy dẫn của (25) - (30)

Gọi q*(ρ) = e^{p̂ᵨ}, chúng ta có

ln q*(ρ)
∝ ln e^{p̂ᵨ}
∝ ⟨ln p̂(w,ρ,s,D)⟩_{q(s)q(w)}
∝ ⟨ln p(w|ρ)⟩_{q(w)} + ⟨ln p(ρ|s)⟩_{q(s)}
∝ ∑ᴺₙ₌₁ (⟨sₙ⟩aₙ + ⟨1-sₙ⟩ãₙ) ln ρₙ − [|wₙ|²/2 + ⟨sₙ⟩bₙ + ⟨1-sₙ⟩b̃ₙ] ρₙ.

Do đó, chúng ta có q*(ρ) tuân theo phân phối Gamma trong (25), với các tham số trong (26) và (27).

Gọi q*(s) = e^{p̂ₛ}, chúng ta có

ln q*(s)
∝ ln e^{p̂ₛ}
∝ ⟨ln p̂(w,ρ,s,D)⟩_{q(ρ)q(w)}
∝ ⟨ln p(ρ|s)⟩_{q(ρ)} + ln p̂(s)
∝ ∑ᴺₙ₌₁ sₙ(ln b^{aₙ}ₙ/Γ(aₙ) + (aₙ-1)⟨ln ρₙ⟩ - bₙ⟨ρₙ⟩ - ln Γ(aₙ))

--- TRANG 13 ---
TẠP CHÍ XXX 13

+ (1-sₙ)[ln (b̃ₙ^ãₙ)/Γ(ãₙ) + (ãₙ-1)⟨ln ρₙ⟩ - b̃ₙ⟨ρₙ⟩ - ln Γ(ãₙ)]

+ ∑ₙ₌₁ᴺ [sₙ ln πₙ + (1-sₙ) ln (1-πₙ)]

∝ ln ∏ₙ₌₁ᴺ (π̃ₙ)^sₙ(1-π̃ₙ)^(1-sₙ).

Do đó, chúng ta có q*(s) tuân theo phân phối Bernoulli trong (28), với các tham số trong (29) và (30).

TÀI LIỆU THAM KHẢO

[1] N. Lee, T. Ajanthan, và P. H. Torr, "Snip: Single-shot network pruning based on connection sensitivity," International Conference on Learning Representations (ICLR), 2019.

[2] Y. LeCun, J. S. Denker, và S. A. Solla, "Optimal brain damage," trong Advances in Neural Information Processing Systems (NeurIPS), 1990, tr. 598-605.

[3] B. Hassibi và D. G. Stork, Second Order Derivatives for Network Pruning: Optimal Brain Surgeon. Morgan Kaufmann, 1993.

[4] C. Louizos, M. Welling, và D. P. Kingma, "Learning sparse neural networks through l_0 regularization," arXiv preprint arXiv:1712.01312, 2017.

[5] W. Wen, C. Wu, Y. Wang, Y. Chen, và H. Li, "Learning structured sparsity in deep neural networks," arXiv preprint arXiv:1608.03665, 2016.

[6] J. O. Neill, "An overview of neural network compression," arXiv preprint arXiv:2006.03669, 2020.

[7] S. Scardapane, D. Comminiello, A. Hussain, và A. Uncini, "Group sparse regularization for deep neural networks," Neurocomputing, vol. 241, tr. 81-89, 2017.

[8] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan et al., "Searching for mobilenetv3," trong Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019, tr. 1314-1324.

[9] T. Zhuang, Z. Zhang, Y. Huang, X. Zeng, K. Shuang, và X. Li, "Neuron-level structured pruning using polarization regularizer," Advances in Neural Information Processing Systems (NeurIPS), vol. 33, tr. 9865-9877, 2020.

[10] L. Yang, Z. He, và D. Fan, "Harmonious coexistence of structured weight pruning and ternarization for deep neural networks," trong Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, 2020, tr. 6623-6630.

[11] A. S. Weigend, D. E. Rumelhart, và B. A. Huberman, "Generalization by weight-elimination with application to forecasting," trong Advances in Neural Information Processing Systems (NeurIPS), 1991, tr. 875-882.

[12] J. Frankle và M. Carbin, "The lottery ticket hypothesis: Finding sparse, trainable neural networks," arXiv preprint arXiv:1803.03635, 2018.

[13] J. Frankle, G. K. Dziugaite, D. Roy, và M. Carbin, "Linear mode connectivity and the lottery ticket hypothesis," trong International Conference on Machine Learning (ICML). PMLR, 2020, tr. 3259-3269.

[14] M. Van Baalen, C. Louizos, M. Nagel, R. A. Amjad, Y. Wang, T. Blankevoort, và M. Welling, "Bayesian bits: Unifying quantization and pruning," Advances in Neural Information Processing Systems (NeurIPS), vol. 33, tr. 5741-5752, 2020.

[15] D. Molchanov, A. Ashukha, và D. Vetrov, "Variational dropout sparsifies deep neural networks," trong International Conference on Machine Learning (ICML). PMLR, 2017, tr. 2498-2507.

[16] C. Louizos, K. Ullrich, và M. Welling, "Bayesian compression for deep learning," arXiv preprint arXiv:1705.08665, 2017.

[17] A. Liu, G. Liu, L. Lian, V. K. Lau, và M.-J. Zhao, "Robust recovery of structured sparse signals with uncertain sensing matrix: A turbo-vbi approach," IEEE Trans. Wireless Commun., vol. 19, no. 5, tr. 3185-3198, 2020.

[18] P. Schniter, "Turbo reconstruction of structured sparse signals," trong 2010 44th Annual Conference on Information Sciences and Systems (CISS). IEEE, 2010, tr. 1-6.

[19] A. Wu, S. Nowozin, E. Meeds, R. E. Turner, J. M. Hernandez-Lobato, và A. L. Gaunt, "Deterministic variational inference for robust Bayesian neural networks," arXiv preprint arXiv:1810.03958, 2018.

[20] J. Zhang, X. Chen, M. Song, và T. Li, "Eager pruning: Algorithm and architecture support for fast training of deep neural networks," trong 2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2019, tr. 292-303.

[21] J. Friedman, T. Hastie, và R. Tibshirani, "A note on the group lasso and a sparse group lasso," arXiv preprint arXiv:1001.0736, 2010.

[22] T.-J. Yang, Y.-H. Chen, và V. Sze, "Designing energy-efficient convolutional neural networks using energy-aware pruning," trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, tr. 5687-5695.

[23] S. J. Kwon, D. Lee, B. Kim, P. Kapoor, B. Park, và G.-Y. Wei, "Structured compression by weight encryption for unstructured pruning and quantization," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, tr. 1909-1918.

[24] J. Yu, A. Lukefahr, D. Palframan, G. Dasika, R. Das, và S. Mahlke, "Scalpel: Customizing dnn pruning to the underlying hardware parallelism," trong 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA), 2017, tr. 548-560.

[25] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, và J. Guttag, "What is the state of neural network pruning?" Proceedings of Machine Learning and Systems, vol. 2, tr. 129-146, 2020.

[26] L. V. Jospin, W. Buntine, F. Boussaid, H. Laga, và M. Bennamoun, "Hands-on Bayesian neural networks-a tutorial for deep learning users," arXiv preprint arXiv:2007.06823, 2020.

[27] K. Murphy, Y. Weiss, và M. I. Jordan, "Loopy belief propagation for approximate inference: An empirical study," arXiv preprint arXiv:1301.6725, 2013.

[28] D. G. Tzikas, A. C. Likas, và N. P. Galatsanos, "The variational approximation for Bayesian inference," IEEE Signal Processing Mag., vol. 25, no. 6, tr. 131-146, 2008.

[29] D. P. Kingma, T. Salimans, và M. Welling, "Variational dropout and the local reparameterization trick," Advances in Neural Information Processing Systems (NeurIPS), vol. 28, tr. 2575-2583, 2015.

[30] P. Tseng, "Convergence of a block coordinate descent method for nondifferentiable minimization," J. Optim. Theory Appl., vol. 109, no. 3, tr. 475-494, 2001.

[31] X. Yuan, L. Ren, J. Lu, và J. Zhou, "Enhanced bayesian compression via deep reinforcement learning," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, tr. 6946-6955.

[32] H. Xiao, K. Rasul, và R. Vollgraf, "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms," arXiv preprint arXiv:1708.07747, 2017.

[33] A. Krizhevsky, V. Nair, và G. Hinton, "The cifar-10 and cifar-100 dataset." cs.toronto.edu. http://www.cs.toronto.edu/~kriz/cifar.html, (truy cập ngày 21 tháng 8 năm 2022).

[34] V. Sehwag, S. Wang, P. Mittal, và S. Jana, "Hydra: Pruning adversarially robust neural networks," Advances in Neural Information Processing Systems (NeurIPS), vol. 33, tr. 19655-19666, 2020.

[35] J. Li, R. Drummond, và S. R. Duncan, "Robust error bounds for quantised and pruned neural networks," trong Learning for Dynamics and Control. PMLR, 2021, tr. 361-372.

[36] L. Wang, G. W. Ding, R. Huang, Y. Cao, và Y. C. Lui, "Adversarial robustness of pruned neural networks," 2018.
