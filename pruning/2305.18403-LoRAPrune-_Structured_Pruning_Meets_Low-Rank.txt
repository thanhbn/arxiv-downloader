# 2305.18403.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2305.18403.pdf
# File size: 796215 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
LoRAPrune: Structured Pruning Meets Low-Rank
Parameter-Efficient Fine-Tuning
Mingyang Zhangâ€ â€¡, Hao Chenâ€ , Chunhua Shenâ€ Â§, Zhen Yangâ€ , Linlin Ouâ€¡,
Xinyi Yuâ€¡âˆ—, Bohan Zhuangâ€ 
â€ Zhejiang Universityâ€¡Zhejiang University of TechnologyÂ§Ant Group
Abstract
Large Language Models (LLMs), such as
LLaMA and T5, have shown exceptional per-
formance across various tasks through fine-
tuning. Although low-rank adaption (LoRA)
has emerged to cheaply fine-tune these LLMs
on downstream tasks, their deployment is still
hindered by the vast model scale and computa-
tional costs. Post-training model pruning offers
a way to compress LLMs. However, the cur-
rent pruning methods designed for LLMs are
not compatible with LoRA. This is due to their
utilization of unstructured pruning on LLMs,
impeding the merging of LoRA weights, or
their dependence on the gradients of pre-trained
weights to guide pruning, which can impose
significant memory overhead. To this end, we
propose LoRAPrune, a new framework that
delivers an accurate structured pruned model
in a highly memory-efficient manner. Specif-
ically, we first design a LoRA-guided prun-
ing criterion, which uses the weights and gra-
dients of LoRA, rather than the gradients of
pre-trained weights for importance estimation.
We subsequently integrate this criterion into
an iterative pruning process, effectively remov-
ing redundant channels and heads. Extensive
experimental results demonstrate the superior
performance of our LoRAPrune over existing
approaches on the LLaMA series models. At
a 50% compression rate, LoRAPrune demon-
strates superior performance over LLM-Pruner,
achieving a reduction in perplexity by 4.81 on
WikiText2 and 3.46 on PTB, while also de-
creasing memory usage by 52.6%. Besides,
LoRAPrune also matches semi-structural prun-
ing across multiple LLMs, proving its wide
applicability. The code is available at https:
//github.com/aim-uofa/LoRAPrune .
1 Introduction
Large Language Models (LLMs) (Touvron et al.,
2023; Du et al., 2022; Frantar et al., 2022) have
âˆ—X. Yu is the corresponding author.Table 1: The memory costs for pruning LLaMA-65B.
â€œIter.â€ indicates whether the method supports iterative
pruning and â€œ#GPU" indicates the number of NVIDIA
A100 (80G) GPUs required.
Method Iter. #GPU Mem.(G)
PST (Li et al., 2022) âœ“ 3 234
LLM-Pruner (Ma et al., 2023) Ã— 2 154
LoRAPrune âœ“ 1 72
showcased remarkable prowess, exhibiting out-
standing performance across numerous tasks. To
enable LLMs to perform specific tasks, such as
chat-bots (Du et al., 2022; Zeng et al., 2022),
they are often efficiently fine-tuned on downstream
datasets (Taori et al., 2023; Chenghao Fan and Tian,
2023) by parameter-efficient fine-tuning (PEFT)
methods (Jia et al., 2022; Hu et al., 2022; Chen
et al., 2022), among which LoRA-based fine-tuning
methods (Hu et al., 2022; Luo et al., 2023; He et al.,
2023) have gained widespread use. However, the
remarkable success of LLMs is accompanied by
obstacles from their vast scale and substantial com-
putational costs, making deployment exceedingly
arduous (Frantar and Alistarh, 2023).
Neural network pruning (Li et al., 2017; Molchanov
et al., 2017), a prevailing technique for model com-
pression, can significantly reduce the model size
and complexity. Recently, the post-training prun-
ing literature, such as SparseGPT (Frantar and Al-
istarh, 2023) and WANDA (Sun et al., 2023), have
achieved high-performance unstructured sparse
LLMs. However, unstructured sparse models face
two critical issues: 1) Unstructured sparse mod-
els are hard to obtain direct inference speedup .
They often require specialized hardware support
to achieve satisfying acceleration benefits, which
leads to unstructured pruning not benefiting legacy
off-the-shelf platforms, e.g., CPUs, DSPs, and
GPUs (Fang et al., 2023; You et al., 2023; Zhou
et al., 2022). 2) Unstructured sparse models are
not compatible with LoRA. As shown in Figure 1arXiv:2305.18403v5  [cs.LG]  7 Aug 2024

--- PAGE 2 ---
W0
BAW0
GradBGradA
InputGrad Grad
Inputâœ‚Too expensive !
Lightweight !ðŸ˜ŠðŸ˜¡
ï¼ˆcï¼‰ LoRAPrune (b) Gradient guided pruningGrad W 0 â„ðŸ”¥
ðŸ”¥W0
Input
ï¼ˆaï¼‰ Â Unstructured pruning Â â„
BABA
âŒ Cannot merge !â˜‘ Can merge !ðŸ”¥
âœ‚Figure 1: Comparing LoRAPrune with other pruning methods: (a) Unstructured sparse model cannot directly
merge LoRA weights, which is computationally inefficient. (b) Gradient-guided pruning requires the gradients of
the pre-trained weights, which is memory-intensive. (c) LoRAPrune only needs the gradients of LoRA weights and
can seamlessly merge LoRA weights into pre-trained weights, which is efficient in both memory and computation.
(a), since the weights BA produced by LoRA are
dense, it poses challenges when trying to merge
BA into the unstructured sparse weights. For in-
stance, LoRA without merging increases inference
time by nearly 54% (see Table 3), diminishing the
benefits of pruning. One potential solution is to
perform fine-tuning using LoRA on downstream
tasks first and then carry out post-training pruning.
However, separating tuning and pruning can lead to
sub-optimal results (Molchanov et al., 2019; Sanh
et al., 2020). To tackle this challenge, PST (Li et al.,
2022) combines unstructured pruning with efficient
fine-tuning, which simultaneously prunes LoRA
and pre-trained weights. This method ensures a
seamless merge of LoRA weights and avoids ad-
ditional computational overhead that comes from
LoRA. However, unstructured pruning of LoRA
necessitates computing BA first and then doing
Hadamard product with a binary mask M, which
results in significant memory overhead (see Table
1) since BA andMshare the same shape with
pre-trained weights. For instance, when pruning
LLaMA-65b, the intermediate variables necessitate
the storage capacity of three NVIDIA A100 (80G)
GPUs. This poses a significant memory challenge
when adapting PST to LLMs. Instead, structured
pruning can mitigate this issue since we can di-
rectly prune the structured weights of Ain LoRA
without storing BA. Therefore, it is significant to
combine LoRA with structured pruning to achieve
simultaneous PEFT and direct acceleration on gen-
eral hardware platforms with high performance.
To this end, we propose a unified framework for
LoRA and structured pruning, named LoRAPrune.As shown in Figure 1 (c), LoRAPrune not only
prunes the structured weights ( e.g., heads, chan-
nels) from the pre-trained model weights W0but
also trims the corresponding weights in LoRA
weight Awithout computing BA first. Conse-
quently, after pruning and fine-tuning, the weights
of LoRA can be seamlessly merged with the pre-
trained weights, ensuring that no additional com-
putations are needed during inference. To identify
weight connections of structural importance, the
criterion used in the structured pruning methods
(Ma et al., 2023; Molchanov et al., 2019, 2017)
is often estimated by gradients or its variants, as
shown in Figure 1 (b). However, LoRA typically
requires frozen pre-trained weights without com-
puting their gradients, thus pruning approaches
that rely on gradients of the pre-trained weights
cannot be directly applied. To efficiently esti-
mate the importance of pre-trained weights, Lo-
RAPrune introduces a novel criterion that exclu-
sively utilizes the gradients of LoRA. In contrast
to the vanilla gradient-guided pruning method, Lo-
RAPrune leverages LoRAâ€™s gradients as the ap-
proximation for the gradients of the pre-trained
weights. Based on the presented criterion, we can
iteratively perform pruning while simultaneously
conducting efficient fine-tuning to restore the per-
formance of the pruned LLMs, requiring only a
small calibration set. Specifically, we compute the
importance of every batch of data and update the
importance using a moving average. Every few it-
erations, we remove a portion of unimportant struc-
tured weights until the desired sparsity is achieved.
Through extensive experiments on diverse bench-

--- PAGE 3 ---
W0
BA
Input0.3 1.41.2 1.5 1.6 1.41.7 1.8 1.31.1 1.5 1.2 1.4
0.2
0.1 0.1
âœ‚5.2
5.0
5.7
1.9â„ðŸ”¥I=â–½WÂ  Â  Â  W
GradBGradA
BA
+ -
GradBGradAâ–½Wâ‰ˆBâ€¢â–½A+â–½Bâ€¢AÂ -â–½Bâ€¢â–½A
A
B+W=BA+W0
âœ‚.Figure 2: The pruning process for the LoRA-guided criterion involves utilizing the LoRA matrices A,Band their
respective gradients âˆ‡A,âˆ‡Bto compute the importance I. Subsequently, weight importance (gray number) with
the same group are aggregated to the group importance (black number) and the groups with low scores are removed.
mark datasets and various scales of LLMs, we
demonstrate that LoRAPrune consistently outper-
forms other structured pruning techniques tailored
for LLMs. Furthermore, compared to the vanilla
gradient-guided pruning, LoRAPrune significantly
diminishes memory and computational overhead,
facilitating efficient pruning and fine-tuning of
LLaMA-65b on a single GPU concurrently. This
paper has the following key contributions:
â€¢We introduce a novel memory-efficient pruning
criterion tailored for LLMs, termed the LoRA-
guided criterion, which seamlessly integrates
with LoRA. Leveraging the gradients of LoRA,
we can efficiently approximate the importance of
pre-trained weights without needing to compute
their gradients.
â€¢As we can efficiently approximate gradients and
update weights using LoRA, LoRAPrune facil-
itates iterative structured pruning, resulting in
precise small models. Our framework ensures
both high memory efficiency during pruning and
incurs efficient inference.
â€¢Pruning experiments conducted on the LLaMA
models demonstrate that LoRAPrune can effi-
ciently perform structured pruning with up to 65
billion weights on a single GPU. Furthermore,
the pruning results achieved by LoRAPrune sig-
nificantly surpass other pruning methods. For
example, against LLM-Pruner, LoRAPrune uses
only 52.6% of the memory yet scores lower per-
plexities by 4.81 on WikiText2 and 3.46 on PTB.
LoRAPrune also matches semi-structural prun-ing performance across various LLMs, proving
its broad applicability.
2 Related Work
Parameter-efficient fine-tuning. PEFT methods
(Jia et al., 2022; Wu et al., 2022; Chen et al.,
2022; Hu et al., 2022; Luo et al., 2023; He et al.,
2023) have received increasing attention from both
academia and industry. Among them, LoRA (Hu
et al., 2022) proposes injecting trainable low-rank
decomposition matrices into each layer which can
be merged into the pre-trained weights, avoiding
extra computation in inference. Since inference
efficiency, many methods based on LoRA have
emerged. For instance, LongLoRA (Chen et al.,
2023) improves upon LoRA, enabling efficient fine-
tuning of LLMs on long contexts. AnimateDiff
(Guo et al., 2023b) obtains a personalized genera-
tor by inserting LoRA into the frozen text-to-image
model. Quantizing the pre-trained weights into 4-
bit, QLoRA (Dettmers et al., 2023) employs LoRA
for fine-tuning LLMs in downstream tasks while
maintaining efficient memory usage. Therefore,
LoRA is indispensable for fine-tuning LLMs. Our
method seamlessly integrates LoRA and pruning,
making it easily extensible to other PEFT methods
based on LoRA.
Neural network pruning. Removing unimpor-
tant weights from LLMs to reduce memory and
the computational cost of deployment has become
a common approach for model compression. Un-
structured pruning (Dong et al., 2017; Lee et al.,
2019; Wang et al., 2020; Sun et al., 2023; Fran-
tar and Alistarh, 2023; Li et al., 2022) can ob-

--- PAGE 4 ---
tain highly compressed models by directly pruning
neurons, which also causes unstructured sparsity
and hard deployment. In contrast, structured prun-
ing (Ma et al., 2023; Xia et al., 2023; Guo et al.,
2023a) directly discards the whole grouped param-
eters ( e.g.heads, channels) and leaves a model with
deploy-friendly structures. However, structured
pruning models require extensive finetuning to re-
gain their performance levels. For example, Xia
et al. (2023) utilized 50B tokens sampled for con-
tinued pretraining of their pruned model, a process
that proves to be prohibitively expensive in terms of
hardware resources. In contrast, our approach lever-
ages structured pruning, enabling direct inference
acceleration while maintaining training expenses
at an acceptable level.
Pruning criterion. Determining the importance
of weights in a network is still an open question
(Blalock et al., 2020). A common approach to
model pruning is to use parameter magnitude (Li
et al., 2018; Lee et al., 2020; Elesedy et al., 2020;
Han et al., 2015; Li et al., 2017) as a criterion.
However, the small weights can still have a sig-
nificant impact on the model output due to the
complex structure of neural networks, while large
weights may not be as important. Many methods
(Sanh et al., 2020; Yu et al., 2022a; Zhang et al.,
2022; Lee et al., 2019; Yu et al., 2022b; Wang et al.,
2020; LeCun et al., 1989; Hassibi et al., 1993) em-
ploy Taylor expansion to approximate the errors
introduced by pruning and use this as the criterion
for importance estimation. To avoid computing
the Hessian matrix (Hassibi et al., 1993) or Hes-
sian inverse (LeCun et al., 1989) in Taylor expan-
sion, (Molchanov et al., 2017, 2019) only use the
first-order term in Taylor expansion. Furthermore,
LLM-Pruner (Ma et al., 2023) similarly utilizes the
first-order expansion for pruning and extends the
pruning technique to LLMs. However, the first-
order term in Taylor expansion still requires gra-
dients of the pre-trained weights. As shown in
Table 1, computing and storing the gradients of pre-
trained weights significantly increases the pruning
cost. To avoid computing gradients of pre-trained
weights, PST (Li et al., 2022) learns the gradients
of pre-trained weights by an extra low-rank matrix,
which is motivated by LoRA. Nevertheless, PST
conducts unstructured pruning and needs to com-
pute a substantial mask with the equivalent shape of
pre-trained weights in each forward pass, which is
memory-intensive and hard to be adapted to LLMs.Different from LLM-Pruner (Ma et al., 2023) and
PST (Li et al., 2022), our criterion only relies on
LoRAâ€™s gradients and does not require expensive
mask computation, making it memory-efficient.
3 Method
3.1 Preliminary
We define the notation used in this paper. Bold
letters represent matrices and vectors. Lower-case
letters indicate scalars. â€œSubscriptsâ€ identify the in-
dex of elements within a matrix, and â€œsuperscriptsâ€
indicate the layer index in a network.
Low-rank adaptation. To efficiently fine-tune
LLMs, low-rank adapter LoRA (Hu et al., 2022)
constrains the update of model parameters to main-
tain a low intrinsic rank. During fine-tuning, the
pre-trained weights remain frozen, abstaining from
gradient computation, while the inserted LoRA
is kept trainable. Given two low-rank matrices
AâˆˆRrÃ—kandBâˆˆRdÃ—r(râ‰ªmin(d, k)), the
update of a linear module can be written as
z=xW 0+xBA , (1)
where W0âˆˆRdÃ—k,zâˆˆRnÃ—kandxâˆˆRnÃ—d
denote the pre-trained weights, outputs and inputs,
respectively. After adaption, the new weights W
can be re-parameterized as W=W0+BA.
Pruning with Taylor expansion. In vanilla prun-
ing approaches (Molchanov et al., 2017, 2019), the
importance of a weight Wi,jâˆˆW0can be quanti-
fied by measuring the impact of its removal on the
loss. For an input xand the ground-truth prediction
y, the induced error of Wi,jcan be given as:
Ii,j= [L(x,y,W0)âˆ’L(x,y,W0|Wi,j= 0)]2.
(2)
Computing Ii,jfor each weight is computationally
expensive. Following (Molchanov et al., 2019), we
can use first-order Taylor expansion to approximate
the importance Ë†Ii,jby:
Ë†Ii,j= (âˆ‚L
âˆ‚Wi,jWi,j)2. (3)
Dependency-aware structured pruning. In struc-
tured pruning, it is crucial to consider that pruned
neurons can exhibit dependencies with other neu-
rons due to their interconnected nature. The de-
pendencies of weights are illustrated in Figure 5.
We organize the connected weights as a group and
estimate the group importance by accumulating the

--- PAGE 5 ---
Algorithm 1: Progressive pruning with
LoRA-guided criterion
Require : Calibration data D; Pre-trained
weights W0; Randomly
initialized low-rank matrices A
andB; Loss function L; Target
sparsity level S; Fine-tuning
iterations T.
Output : Trained low-rank adaption Aand
B; Binary mask M.
Â¯Gl
gâ†0,Ml
gâ†1 forâˆ€l,âˆ€g;
// Initialization for masks and
group importance
sâ†0;// Initialize sparsity level
fortâˆˆ[1, . . . , T ]do
Clear gradient;
Forward and backward via Eq. (13);
Update AandBvia AdamW;
Calculate Ë†I|tvia Eq. (10);
Calculate Ë†G|tvia Eq. (4);
Calculate Â¯G|tvia Eq. (11);
forlâˆˆ[1, . . . , L ]do
pâ†SortDescending( Â¯G)s;// Set
threshold
Ml
gâ†0 where Â¯Gl
gâ‰¤p, and
gâˆˆ {1, . . . , G }
end
// Remove unimportant groups
Progressively increase suntil
||M||0> S;
end
weight importance within the same group. For-
mally, the importance for the g-th group can be
expressed as
Ë†Gg=X
Wi,jâˆˆGË†Ii,j, (4)
where Ë†GâˆˆR1Ã—Grepresents the importance of
groups, Gdenotes a set of weights within a group
andGis the candidate group number in a layer.
3.2 Pruning with Low-rank Adaption
Motivation. To achieve highly-compressed LLMs,
it is essential to accurately evaluate the importance
of pre-trained weights. A key approach is to utilize
the criteria in Eq. (3) for this evaluation. How-
ever, obtaining the gradient of W0in a LLM is
difficult since it requires a lot of computing power
and storage space. Fine-tuning LLMs with LoRAis becoming prevalent (Taori et al., 2023; Cheng-
hao Fan and Tian, 2023). During LoRA fine-tuning,
only the gradients of LoRAâ€™s weights are com-
puted, yielding remarkable computation and mem-
ory efficiency. Therefore, can we rely solely on
the weights and gradients of LoRA to accurately
estimate the importance of pre-trained weights?
LoRA-guided criterion. In this work, we discuss
how to estimate the importance of W0by inserting
the learnable matrices AandBin the downstream
task adaption.
The core idea lies in setting the element (BA)ij=
âˆ’Wijif the element WijâˆˆW0is removed. The
importance of each parameter in Eq. (2) can be
reformulated as follows
Ii,j= [L(x,y,W)âˆ’L(x,y,W|(BA)i,j=âˆ’Wi,j]2.
(5)
Exploiting the first-order Taylor expansion with
(BA)i,j=âˆ’Wi,jto approximate Eq. (5), the es-
timated importance Ë†Ii,jof parameter Wi,jcan be
represented by
Ë†Ii,j= [âˆ‚L
âˆ‚(BA)i,j((BA)i,j+Wi,j)]2.(6)
However, as shown in Eq. (1), the LoRA compu-
tation sequence involves first multiplying by B
and then by A, which means that BA cannot be
obtained during the forward and backward pass.
Besides, preservingâˆ‚L
âˆ‚(BA)i,jstill entails the same
level of complexity asâˆ‚L
âˆ‚Wi,jsinceBA shares the
same shape of W0.
Here, we only save and use the gradients of
two low-rank matrices AandBto approximate
âˆ‚L
âˆ‚(BA). We can rely on the gradient update that
(BA)i,j|t= (BA)i,j|tâˆ’1âˆ’Î·âˆ‚L
âˆ‚(BA)i,jto estimate
the gradient, where (BA)i,j|tand(BA)i,j|tâˆ’1rep-
resents the (BA)i,jint-th and (tâˆ’1)-th step, re-
spectively. Apparently, Î·âˆ‚L
âˆ‚(BA)i,jis equal to the
change of BA, which can be written as
Î·âˆ‚L
âˆ‚(BA)i,j= [(BA)i,j|tâˆ’1âˆ’(BA)i,j|t].(7)
Here, (BA)i,j|t=Bi,:|tA:,j|tis generated by the
multiplication of the i-th row of B|twith the j-th
column of A|t. Using the above assumption, we
can also estimate Î·âˆ‚L
âˆ‚A:,j=A:,j|tâˆ’1âˆ’A:,j|tand
Î·âˆ‚L
âˆ‚Bi,:=Bi,:|tâˆ’1âˆ’Bi,:|t, respectively. Further,
we can obtain that A:,j|t=A:,j|tâˆ’1âˆ’Î·âˆ‚L
âˆ‚A:,jand

--- PAGE 6 ---
Table 2: Zero-shot performance of the compressed LLaMA models fine-tuned on the LaMini dataset. We evaluate
WikiText2 and PTB on perplexity with 2048-token segments. The average accuracy is calculated among seven
classification datasets. Bold denotes the best performance at the same compression rate.â‹†denotes the results
obtained by our reproduction.
Pruning Ratio Method WikiText2 â†“PTBâ†“MMLU (5-shot) OBQA ARC-e WinoGrande ARC-c PIQA HellaSwag Average â†‘
Ratio = 0% LLaMA-7B (Touvron et al., 2023) 5.69 8.93 37.10 42.40 67.45 67.01 67.45 78.35 72.99 65.34
Ratio = 20%Magnitudeâ‹†9.06 13.80 27.84 35.80 65.36 61.33 38.74 74.87 63.90 56.67
WANDAâ‹†(Sun et al., 2023) 8.64 12.66 28.35 35.26 68.96 64.01 38.46 74.80 52.63 58.68
LLM-Prunerâ‹†(Ma et al., 2023) 8.14 12.38 33.67 38.8 70.62 65.82 40.7 77.37 66.6 62.36
Compresso (Guo et al., 2023a) - - 31.90 36.4 68.64 67.80 37.97 75.46 53.44 59.82
LoRAPrune-8bit (Ours) 7.70 11.91 36.45 38.1 70.25 65.93 41.43 77.10 68.90 60.29
LoRAPrune (Ours) 7.63 11.87 36.81 38.6 70.20 66.77 41.89 77.48 68.64 62.70
Ratio = 30%Magnitudeâ‹†11.38 16.90 26.38 33.67 65.58 60.79 37.47 73.15 60.35 55.16
WANDAâ‹†(Sun et al., 2023) 10.10 15.83 27.90 34.90 65.06 61.16 39.44 74.38 60.84 55.96
LLM-Prunerâ‹†(Ma et al., 2023) 9.36 13.82 30.67 34.86 66.2 63.85 40.55 75.60 65.12 57.70
Compresso (Guo et al., 2023a) - - 27.68 29.8 66.23 64.80 37.2 75.63 49.16 53.79
LoRAPrune-8bit (Ours) 8.83 13.30 33.36 36.40 69.48 62.31 41.93 77.40 65.91 58.90
LoRAPrune (Ours) 8.79 13.33 33.60 36.20 69.61 62.75 41.21 77.48 66.68 58.98
Ratio = 50%Magnitudeâ‹†18.36 23.88 21.84 30.26 53.61 55.86 36.98 67.10 53.10 49.48
WANDAâ‹†(Sun et al., 2023) 17.38 21.34 24.15 28.78 52.68 55.98 34.20 70.38 54.12 49.35
LLM-Prunerâ‹†(Ma et al., 2023) 16.41 20.85 25.60 33.12 55.36 56.12 34.98 73.25 58.60 51.90
LoRAPrune-8bit (Ours) 11.65 17.41 27.71 35.30 60.54 56.13 40.58 74.89 59.86 54.55
LoRAPrune (Ours) 11.60 17.39 27.84 35.80 60.38 56.97 40.12 75.39 60.21 54.81
Table 3: Runtime results of the structured pruned LLMs.
Model Unmerged time (s) â†“Merged time (s) â†“Perplexity â†“Ratio (%)
LLaMA-
7B0.184(+0.0%) 0.105(+0.0%) 5.69 0
0.120(-34.8%) 0.079(-24.7%) 7.63 20
0.089(-51.6%) 0.053(-49.5%) 11.60 50
Bi,:|t=Bi,:|tâˆ’1âˆ’Î·âˆ‚L
âˆ‚Bi,:. Subsequently, we can
calculate
(BA)i,j|t=Bi,:|tA:,j|t
= (Bi,:|tâˆ’1âˆ’Î·âˆ‚L
âˆ‚Bi,:)(A:,j|tâˆ’1âˆ’Î·âˆ‚L
âˆ‚A:,j)
(8)
Substitute (BA)i,j|tin Eq. (8) to Eq. (7) and obtain
âˆ‚L
âˆ‚(BA)i,j=[âˆ‚L
âˆ‚Bi,:A:,j|tâˆ’1+Bi,:|tâˆ’1âˆ‚L
âˆ‚A:,j
âˆ’Î·âˆ‚L
âˆ‚Bi,:âˆ‚L
âˆ‚A:,j].
(9)
For simplicity, we set the learning rate Î·=
1. Substitute Eq. (9) to Eq. (6), we can esti-
mate the importance in a gradient-based manner
Ë†Ii,j= [(âˆ‚L
âˆ‚Bi,:A:,j+Bi,:âˆ‚L
âˆ‚A:,jâˆ’âˆ‚L
âˆ‚Bi,:âˆ‚L
âˆ‚A:,j)(Wi,j+
(BA)i,j)]2.
Ë†Ii,j=[(âˆ‚L
âˆ‚Bi,:A:,j+Bi,:âˆ‚L
âˆ‚A:,jâˆ’âˆ‚L
âˆ‚Bi,:âˆ‚L
âˆ‚A:,j)
(Wi,j+ (BA)i,j)]2.
(10)
As shown in Figure 2, the LoRA-guided criterion
only needs to compute the gradients of AandBwith the approximation in Eq. (10), which saves
memory and computation compared with the gradi-
ents of pre-trained weights W0.
Progressive pruning. To efficiently obtain group
importance for structured pruning, we can substi-
tute Eq. (10) into Eq. (4). However, estimating im-
portance and pruning weights with a single batch
of data can lead to significant bias and performance
loss. To mitigate this, we apply moving average
to evaluate group importance Gand incrementally
prune less critical groups. Specifically, the group
importance at t-th iteration is computed as follows:
Â¯G|t=Î»Â¯G|tâˆ’1+ (1âˆ’Î»)Ë†G|t. (11)
Here, Â¯G|tdenotes the group importance scores cal-
culated by Eq. (10) and Eq. (4) at the t-th iteration,
andÎ»âˆˆ[0,1]balances the importance between
historical and current statistics.
To this end, we can efficiently and accurately esti-
mate the importance of each group. We then prune
the unimportant groups by setting a binary mask
Mâˆˆ {0,1}1Ã—Gfor each pruned layer. The binary
maskMis obtained by
Mg=
1Â¯Gg> p
0Â¯Ggâ‰¤p, (12)
where the index gâˆˆ {1, . . . , G }denotes the g-th
group in the layer, and prepresents the threshold

--- PAGE 7 ---
0 20 40
Ratio (%)51015202530Perplexity
(a) LLaMA-13B
LoRAPrune
Magnitude
WANDA(2:4)
SparseGPT(2:4)
LLM-Pruner
0 20 40
Ratio (%)5101520
(b) LLaMA-30B
LoRAPrune
Magnitude
WANDA(2:4)
SparseGPT(2:4)
LLM-Pruner
0 20 40
Ratio (%)5.07.510.012.515.017.5
(c) LLaMA-65B
LoRAPrune
Magnitude
WANDA(2:4)
SparseGPT(2:4)Figure 3: Pruning results on large-scale LLMs: (a) LLaMA-13B, (b) LLaMA-30B, (c) LLaMA-65B.
5 10 15 20 25 30
Layer0.50.60.70.80.91.0Similarity (%)(a) Attention Layer
Ratio=10%
Ratio=30%
Ratio=50%
5 10 15 20 25 30
Layer0.550.600.650.700.750.800.85Similarity (%)(b) MLP Layer
Ratio=10%
Ratio=30%
Ratio=50%
Figure 4: Similarity between LoRA gradient and vanilla criterion on (a) Attention, (b) MLP layers.
of importance. Groups falling below this threshold
will be pruned. After setting the mask, the forward
process of each pruned layer can be written as
z= (xW 0+xBA )âŠ™M, (13)
where âŠ™denotes Hardamard product and can be
calculated by broadcast. The complete algorithm
of LoRAPrune is given in Algorithm 1.
4 Experiments
4.1 Experimental Setup
Models and metrics. Our method is applied
to the LLaMA-1 model family (Touvron et al.,
2023), which comprises LLaMA-7B, LLaMA-13B,
LLaMA-30B and LLaMA-65B. Following (Frantar
and Alistarh, 2023), we evaluate models on the per-
plexity metric with WikiText (Merity et al., 2016)
and PTB (Marcus et al., 1993) dataset. To assess
the zero-shot ability of LLMs, we follow LLaMA
to perform zero-shot task classification on common
sense reasoning datasets: PIQA (Bisk et al., 2020),
HellaSwag (Zellers et al., 2019), WinoGrande (Sak-
aguchi et al., 2021), ARC-easy (Clark et al., 2018),
ARC-challenge (Clark et al., 2018), OpenbookQA
(Mihaylov et al., 2018). We evaluate the in-context
learning ability under a 5-shot setting on MMLU
(Hendrycks et al., 2020).Implementation details. We provide results for
LoRAPrune as a single-shot method and with post-
training recovery fine-tuning. We iteratively prune
models on LaMini instruction dataset (Wu et al.,
2023) for LLaMA-7b and 20k sampled C4 dataset
(Raffel et al., 2020) for LLaMA-13b, LLaMA-30B
and LLaMA-65B. Our training configuration in-
cludes a batch size of 128, a learning rate set to
1e-4, and a total of 2 training epochs. As the pre-
trained weights remain frozen, there is the option
to quantize them into 8-bit values to save memory.
All models are optimized by AdamW optimizer
(He et al., 2020) with a cosine learning rate decay.
Contenders. We compare LoRAPrune with the
following pruning methods in both fine-tuning and
without fine-tuning settings: 1)Magnitude Prun-
ing: pruning based on the absolute values of model
weights. 2)LLM-Pruner (Ma et al., 2023): prun-
ing using criterion in Eq. (3). 3)WANDA (Sun
et al., 2023): pruning based on the magnitude of
input features and pre-trained weights. 4)Com-
presso (Guo et al., 2023a): pruning based on a set
of learnable masks.
4.2 Main Results
Zero-shot performance. Table 2 demonstrates
the effectiveness of our proposed method. Our
LoRAPrune far surpasses other large model prun-

--- PAGE 8 ---
Table 4: Pruning resource required by different pruning criteria.
Model Pruning criteria Fine-tuning Throughput â†“GPU Memory â†“ Total time â†“ Perplexity â†“
LLaMA-7B
(Ratio=50%)Vanilla 38.87s/iter (+0.0%) 38.6G (+0.0%) 5.3 h (+0.0%) 11.48 (+0.0%)
Magnitude 13.08s/iter (-66.3%) 16.8G (-56.7%) 1.8 h (-66.04%) 17.38 (+52.9%)
LoRA-guided 14.13s/iter (-63.6%) 18.3G (-52.6%) 2.0 h (-62.26%) 11.60 (+1.0%)
LoRA-guided (8-bit) 15.63s/iter (-59.8%) 13.8G (-64.2%) 2.0 h (-62.26%) 12.38 (+9.0%)
ing methods under structured sparsity. For in-
stance, at a 50% compression rate, LoRAPrune
achieves a perplexity of 11.60 on WikiText2, signif-
icantly outperforming LLM-Prunerâ€™s perplexity of
16.41. We also replicate the experimental results of
WANDA under structured pruning scenarios. Our
findings reveal that the pruning outcomes achieved
by WANDA fell short in comparison to gradient-
based pruning methods such as LLM-Pruner and
LoRAPrune. This observation underscores the su-
perior performance and effectiveness of gradient-
based pruning approaches in our experiments.
Itâ€™s worth noting that LoRAPruneâ€™s efficient ap-
proximation for the gradients of the pre-trained
weights allows for 8-bit quantization of those
weights, greatly reducing the memory requirements
for pruning. Moreover, LoRAPrune demonstrates
superior pruning results even when models are
quantized to 8 bits. These findings underscore
the effectiveness and versatility of LoRAPrune in
achieving impressive pruning results across various
scenarios and compression rates.
Few-shot performance. To verify whether the
pruned LLMs retain the in context learning ca-
pability, we evaluate on the MMLU with 5-shot.
As shown in Table 2, LoRAPrune consistently
achieves a higher score than other pruning meth-
ods across all sparsity ratios. Notably, LoRAPrune
achieves performance on par with the unpruned
LLaMA-7B model at a 20% sparsity ratio.
Acceleration for pruned LLMs. Models with
structured pruning can be directly sped up in gen-
eral GPU devices. We conducted tests with 2048
tokens, averaging the results over 100 trials. We
specifically examined the inference time with and
without merging LoRA weights into the pre-trained
weights. As shown in Table 3, we observed that
when pruning 20% weights, LLM without merging
LoRA has an even slower inference speed than
LLM with LoRA merged without pruning. In
addition, through structured pruning, the model
achieves reductions in inference time of 24.7% and
49.5% at compression rates of 20% and 50%.
Pruning on large-scale LLMs. Due to the ef-ficient approximation of the pre-trained weightsâ€™
gradients, LoRAPrune enables iterative pruning on
larger-scale LLMs. To ensure that all experiments
can be conducted on one GPU, we quantize the
pre-trained weights of LLaMA-30b and LLaMA-
65b to 8 bits. The experimental results are shown
in Figure 3. We observe that, in comparison to
the magnitude-based method, LoRAPrune exhibits
significant superiority across various scales. Fur-
thermore, in comparison to the 2:4 sparsity model,
LoRAPrune achieves comparable pruning results
at a 50% sparsity rate. However, it is worth noting
that the 2:4 sparsity model also faces challenges in
direct weight merging with LoRA, resulting in ad-
ditional computational overhead during inference.
Besides, accelerating 2:4 sparsity models requires
specialized hardware support, such as NVIDIA
GPUs based on the Ampere architecture, which
significantly increases the deployment constraints
for 2:4 sparsity models.
4.3 Ablation Study
Efficiency of LoRA-guided criterion vs. vanilla
criterion. We conduct a comparative analysis of
different pruning criteria with respect to their re-
source requirements and computational efficiency,
including GPU memory and throughput. We adopt
the vanilla criterion, as outlined in Eq. (3), as our
baseline. For each forward pass, we set the batch
size to 1, and we accumulate this process itera-
tively until we reach a total of 128 accumulations.
To ensure robustness and reliability, we compute
and subsequently average the results obtained over
a span of 100 steps. The comparison results can
be found in Table 4. Compared to the vanilla cri-
terion, LoRA-guided and LoRA-guided (8bit) cri-
teria demonstrate a significant reduction in GPU
memory usage, saving 52.6% and 64.2% of the
memory, respectively. Moreover, as the LoRA-
guided criterion does not require the computation
of original gradients, it achieves a 64.6% increase
in throughput compared to the vanilla criterion with
comparable performance, greatly enhancing the
speed of the pruning process.
Efficacy of LoRA-guided criterion vs. vanilla

--- PAGE 9 ---
criterion. Since the LoRA-guided criterion in
Eq. (10) is an efficient approximation of the vanilla
criterion in Eq. (3), we evaluate the effectiveness
of the proposed LoRA-guided criterion by compar-
ing mask similarity with the vanilla criterion. We
randomly sample 128 data and then perform one-
shot pruning with both LoRA gradient and vanilla
criterion. Figure 4 illustrates that in the case of low
compression rates (Ratio=10%), the masks gener-
ated by these two criteria exhibit a high degree of
consistency. As the compression rates increase, the
mask similarity may decrease. However, it is cru-
cial to emphasize that LoRAPrune follows an itera-
tive pruning approach. In each pruning iteration, it
only needs to precisely identify the least important
weights (about top-5%), thus ensuring the accurate
approximation. Hence, the LoRA-guided criterion
can attain results that are on par with those of the
vanilla criterion while incurring reduced costs.
5 Conclusion
In this paper, we have proposed a method to effec-
tively prune and fine-tune LLMs simultaneously,
achieving state-of-the-art efficiency-accuracy trade-
offs. Specifically, we have proposed a novel LoRA-
guided criterion, for evaluating the parameter im-
portance by only computing the LoRA gradients,
which greatly reduces the computational resources
required for pruning LLMs. Building upon the pro-
posed criterion, we have presented LoRAPrune, a
technique that performs efficient joint pruning and
fine-tuning without the need for computing gradi-
ents of the pre-trained weights. Finally, comprehen-
sive experiments on various LLMs and benchmarks
have demonstrated the superiority of LoRAPrune
over other pruning methods. In terms of compar-
ison with the vanilla criterion, the LoRA-guided
criterion shows its efficiency and effectiveness. In
the future, we aim to further enhance the pruning
results of LoRAPrune at higher compression rates.
Limitation. LoRAPrune requires fine-tuning to
restore model performance. This limitation can
restrict the application of LoRAPrune in scenarios
where fine-tuning is unavailable.
Acknowledgements: This work was supported by
National Key R&D Program of China (No. 2022-
ZD0118700), National Natural Science Foundation
of China (No. 62373329) and Baima Lake Labora-
tory Joint Funds of the Zhejiang Provincial Natural
Science Foundation of China (No. LBMHD24F03-
0002).References
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,
et al. 2020. Piqa: Reasoning about physical common-
sense in natural language. In Proc. AAAI Conf. on
Arti. Intel. , volume 34, pages 7432â€“7439.
Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan
Frankle, and John Guttag. 2020. What is the state
of neural network pruning? Proc. Int. Conf. Mach.
Learn. and Syst. , 2:129â€“146.
Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
Yibing Song, Jue Wang, and Ping Luo. 2022. Adapt-
former: Adapting vision transformers for scalable
visual recognition. Proc. Adv. Neural Inf. Process.
Syst.
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
Zhijian Liu, Song Han, and Jiaya Jia. 2023. Longlora:
Efficient fine-tuning of long-context large language
models. arXiv preprint arXiv:2309.12307 .
Zhenyi Lu Chenghao Fan and Jie Tian. 2023. Chinese-
vicuna: A chinese instruction-following llama-based
model.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. arXiv
preprint arXiv:1803.05457 .
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. 2023. Qlora: Efficient finetuning
of quantized llms. arXiv preprint arXiv:2305.14314 .
Xin Dong, Shangyu Chen, and Sinno Pan. 2017. Learn-
ing to prune deep neural networks via layer-wise op-
timal brain surgeon. Proc. Adv. Neural Inf. Process.
Syst., 30.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Proc. Annual Associa. Comp.
Linguis. , pages 320â€“335.
Bryn Elesedy, Varun Kanade, and Yee Whye Teh.
2020. Lottery tickets in linear models: An analy-
sis of iterative magnitude pruning. arXiv preprint
arXiv:2007.08243 .
Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi
Mi, and Xinchao Wang. 2023. Depgraph: Towards
any structural pruning. In Proc. IEEE Conf. Comp.
Vis. Patt. Recogn. , pages 16091â€“16101.
Elias Frantar and Dan Alistarh. 2023. Massive language
models can be accurately pruned in one-shot. arXiv
preprint arXiv:2301.00774 .
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
Dan Alistarh. 2022. Gptq: Accurate post-training
quantization for generative pre-trained transformers.
arXiv preprint arXiv:2210.17323 .

--- PAGE 10 ---
Song Guo, Jiahang Xu, Li Lyna Zhang, and Mao Yang.
2023a. Compresso: Structured pruning with col-
laborative prompting learns compact large language
models. arXiv preprint arXiv:2310.05015 .
Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang,
Yu Qiao, Dahua Lin, and Bo Dai. 2023b. Animated-
iff: Animate your personalized text-to-image diffu-
sion models without specific tuning. arXiv preprint
arXiv:2307.04725 .
Song Han, Jeff Pool, John Tran, and William Dally.
2015. Learning both weights and connections for
efficient neural network. Proc. Adv. Neural Inf. Pro-
cess. Syst. , 28.
Babak Hassibi, David G Stork, and Gregory J Wolff.
1993. Optimal brain surgeon and general network
pruning. In Proc. IEEE Conf. on Neural Networks ,
pages 293â€“299.
Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao,
and Bohan Zhuang. 2023. Sensitivity-aware visual
parameter-efficient tuning. In Proc. IEEE Int. Conf.
Comp. Vis.
Yang He, Yuhang Ding, Ping Liu, Linchao Zhu, Han-
wang Zhang, and Yi Yang. 2020. Learning filter
pruning criteria for deep convolutional neural net-
works acceleration. In Proc. IEEE Conf. Comp. Vis.
Patt. Recogn. , pages 2009â€“2018.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2020. Measuring massive multitask language under-
standing. arXiv preprint arXiv:2009.03300 .
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. 2022. LoRA: Low-rank adaptation of large
language models. In Proc. Int. Conf. Learn. Repren.
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire
Cardie, Serge Belongie, Bharath Hariharan, and Ser-
Nam Lim. 2022. Visual prompt tuning. In Proc. Eur.
Conf. Comp. Vis.
Yann LeCun, John Denker, and Sara Solla. 1989. Op-
timal brain damage. Proc. Adv. Neural Inf. Process.
Syst., 2.
Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn,
and Jinwoo Shin. 2020. Layer-adaptive sparsity
for the magnitude-based pruning. arXiv preprint
arXiv:2010.07611 .
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip
Torr. 2019. Snip: Single-shot network pruning based
on connection sensitivity. In Proc. Int. Conf. Learn.
Repren.
Guiying Li, Chao Qian, Chunhui Jiang, Xiaofen Lu,
and Ke Tang. 2018. Optimization based layer-wise
magnitude-based pruning for dnn compression. In
Int. Joi. Conf. on Artificial Intelligence , pages 2383â€“
2389.Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet,
and Hans Peter Graf. 2017. Pruning filters for effi-
cient convnets. In Proc. Int. Conf. Learn. Repren.
Yuchao Li, Fuli Luo, Chuanqi Tan, Mengdi Wang,
Songfang Huang, Shen Li, and Junjie Bai. 2022.
Parameter-efficient sparsity for large language mod-
els fine-tuning. arXiv preprint arXiv:2205.11005 .
Gen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun,
Guannan Jiang, Zhiyu Wang, and Rongrong Ji. 2023.
Towards efficient visual adaption via structural re-
parameterization. arXiv preprint arXiv:2302.08106 .
Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023.
Llm-pruner: On the structural pruning of large lan-
guage models. arXiv preprint arXiv:2305.11627 .
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els.arXiv preprint arXiv:1609.07843 .
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question answer-
ing. arXiv preprint arXiv:1809.02789 .
Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri
Frosio, and Jan Kautz. 2019. Importance estimation
for neural network pruning. In Proc. IEEE Conf.
Comp. Vis. Patt. Recogn. , pages 11264â€“11272.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo
Aila, and Jan Kautz. 2017. Pruning convolutional
neural networks for resource efficient inference. In
Proc. Int. Conf. Learn. Repren.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. 21(1):5485â€“5551.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-
ula, and Yejin Choi. 2021. Winogrande: An adver-
sarial winograd schema challenge at scale. Commu-
nications of the ACM , 64(9):99â€“106.
Victor Sanh, Thomas Wolf, and Alexander Rush. 2020.
Movement pruning: Adaptive sparsity by fine-tuning.
Proc. Adv. Neural Inf. Process. Syst. , 33:20378â€“
20389.
Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico
Kolter. 2023. A simple and effective pruning ap-
proach for large language models. arXiv preprint
arXiv:2306.11695 .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .

--- PAGE 11 ---
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix,
Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Chaoqi Wang, Guodong Zhang, and Roger Grosse. 2020.
Picking winning tickets before training by preserving
gradient flow. arXiv preprint arXiv:2002.07376 .
Chen Henry Wu, Saman Motamed, Shaunak Srivastava,
and Fernando D De la Torre. 2022. Generative visual
prompt: Unifying distributional control of pre-trained
generative models. Proc. Adv. Neural Inf. Process.
Syst., 35:22422â€“22437.
Minghao Wu, Abdul Waheed, Chiyu Zhang, Muham-
mad Abdul-Mageed, and Alham Fikri Aji. 2023.
Lamini-lm: A diverse herd of distilled models from
large-scale instructions. CoRR , abs/2304.14402.
Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi
Chen. 2023. Sheared llama: Accelerating language
model pre-training via structured pruning. arXiv
preprint arXiv:2310.06694 .
Haoran You, Zhanyi Sun, Huihong Shi, Zhongzhi Yu,
Yang Zhao, Yongan Zhang, Chaojian Li, Baopu Li,
and Yingyan Lin. 2023. Vitcod: Vision transformer
acceleration via dedicated algorithm and accelerator
co-design. In Proc. IEEE Int. Sym. on High-Perf.
Comp. Arch. , pages 273â€“286. IEEE.
Fang Yu, Kun Huang, Meng Wang, Yuan Cheng, Wei
Chu, and Li Cui. 2022a. Width & depth pruning for
vision transformers. In Proc. AAAI Conf. on Arti.
Intel. , volume 36, pages 3143â€“3151.
Xin Yu, Thiago Serra, Srikumar Ramalingam, and Shan-
dian Zhe. 2022b. The combinatorial brain surgeon:
Pruning weights that cancel one another in neural
networks. In Proc. Int. Conf. Mach. Learn. , pages
25668â€“25683.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machine really finish your sentence? arXiv preprint
arXiv:1905.07830 .
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 .
Qingru Zhang, Simiao Zuo, Chen Liang, Alexander
Bukharin, Pengcheng He, Weizhu Chen, and Tuo
Zhao. 2022. Platon: Pruning large transformer mod-
els with upper confidence bound of weight impor-
tance. In Proc. Int. Conf. Mach. Learn. , pages 26809â€“
26823.Minxuan Zhou, Weihong Xu, Jaeyoung Kang, and Ta-
jana Rosing. 2022. Transpim: A memory-based ac-
celeration via software-hardware co-design for trans-
former. In Proc. IEEE Int. Sym. on High-Perf. Comp.
Arch. , pages 1071â€“1085. IEEE.

--- PAGE 12 ---
Appendix
A Weight Dependency for LLaMA
Here, we use LLaMA architecture as an example
to explain the weight dependency. The dependency
details are shown in Figure 5. In terms of the At-
tention module, when we decide to prune a specific
head of weights in the Query layer, it is impera-
tive that the corresponding weights with the same
index in the Key, Value and Out layers are also
pruned. Similarly, for the Feed-Forward Network
(FFN) module, when pruning a particular channel
of weights in the Up layer, it is essential to prune
the weights with matching indices in the Gate and
Down layers. This meticulous coordination ensures
that pruning maintains the structural integrity and
functionality of the model. Following (Ma et al.,
2023) and (Fang et al., 2023), we prune heads for
Attention and channels for FFN, respectively. The
dependency details are shown in Figure 5.
B More Ablation Studies
Pruning on 20k sampled C4 dataset. We also
evaluate LoRAPrune on a tiny dataset that ran-
domly samples 20k data from C4 dataset. As pre-
sented in Table 5, LoRAPrune outperforms both
LLM-Pruner and WANDA across the majority of
zero-shot reasoning datasets, thereby securing the
highest average score overall. Specifically, Lo-
RAPrune exceeds the performance of LLM-Pruner
by margins of 0.82% and 1.02%, respectively.
Effectiveness of the moving average. We verify
the rationale behind the moving average through
the setting of different values for Î». These exper-
iments were conducted on LLaMA-7b with 20k
sampled C4 dataset. The experimental results, as
shown in Figure 6 (a), reveal that as Î»increases,
the pruning results exhibit a significant reduction
in perplexity. This effect is especially pronounced
when Î»= 0where pruning is solely determined by
the importance of the current batch, confirming the
effectiveness of the moving average.
Impact of iterations. To assess the impact of
the pruning iterations on pruning results, we con-
ducted experiments on the LLaMA-7b model with
different iterations on 20k sampled C4 dataset. The
results are shown in Figure 6 (b), which indicates
that excessive iterations can lead to a decrease in
the modelâ€™s zero-shot performance, potentially due
to overfitting on the calibration dataset. Further-
more, we observe that the model requires moreiterations to regain its performance when pruning
with high compression ( e.g., ratio=50%).
LoRAPrune vs. LLM-Pruner with gradients
off-loading. The gradient off-loading strategy can
partially mitigate LLM-Prunerâ€™s memory demands,
such as transferring certain gradients to CPU mem-
ory. However, the memory access cost and com-
putational overhead are substantial. Table 6 shows
LoRAPrune outperforms LLM-Pruner in efficiency,
being 8.19 Ã—faster with CPU offloading and 2.75 Ã—
faster without it. This speed allows iterative prun-
ing to counteract the performance drop due to struc-
tured sparsity.
Joint vs. separate. To demonstrate the neces-
sity of integrating pruning and fine-tuning, we con-
ducted experiments that sequentially performed
pruning followed by fine-tuning, specifically ap-
plying one-shot pruning to the LLaMA-7b model
and then employing LoRA fine-tuning to recover
the modelâ€™s performance. The experimental results
presented in Table 7 indicate that joint pruning and
fine-tuning yields much better performance than
the separate counterpart, especially under the high
compression ratio.
Pruning frequency. We explore the impact of
different pruning frequencies, i.e., how many iter-
ations of fine-tuning before pruning, on the final
performance. The experimental results, as shown
in Table 8, indicate that our default frequency (fre-
quency=10) obtains the best pruning result. Addi-
tionally, we observe that if pruning is too frequent
(frequency=1), the model may not have enough
iterations to recover through fine-tuning, leading
to inaccurate importance estimation. Furthermore,
excessive fine-tuning between pruning iterations
(frequency=20) leads to overfitting on the calibra-
tion data.
C Generative Results
We show the generalization capability of the Lo-
RAPrune by some instructions encompass common
sense, translation, and coding tasks in Table 9.

--- PAGE 13 ---
Group 1
...Query Head 1 Key Head 1 Value Head 1
Out Head 1
Group N
Query Head N Key Head N Value Head N
Out Head N
(a) Dependency graph of Attention layer...Gate Output
Channel 1Up Output
Channel 1
Down Input
Channel 1
Gate Output
Channel NUp Output
Channel N
Down Input
Channel NGroup 1
Group N
(b) Dependency graph of FFN layerFigure 5: Weight dependency in (a) Attention layer, (b) FFN layer.
Table 5: Zero-shot performance of the compressed LLaMA models fine-tuned on the 20k sampled C4 dataset. The
average accuracy is calculated among seven classification datasets. Bold / denotes the best performance at the same
compression rate.â‹†denotes the results obtained by our reproduction.
Pruning Ratio Method BoolQ PIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Average â†‘
Ratio = 0% LLaMA-7B (Touvron et al., 2023) 73.18 78.35 72.99 67.01 67.45 41.38 42.40 63.25
Ratio = 20%Magnitudeâ‹†61.89 70.81 58.34 56.87 54.87 34.02 38.40 53.59
WANDAâ‹†(Sun et al., 2023) 65.75 74.70 64.52 59.35 60.65 36.26 39.40 57.23
LLM-Pruner (Ma et al., 2023) 64.62 77.20 68.80 63.14 64.31 36.77 39.80 59.23
LoRAPrune-8bit (Ours) 65.37 76.65 69.41 63.78 65.45 36.12 39.50 59.46
LoRAPrune (Ours) 65.62 79.31 70.00 62.76 65.87 37.69 39.14 60.05
Ratio = 50%Magnitudeâ‹†47.40 54.36 33.49 53.10 37.88 26.60 30.12 40.42
WANDAâ‹†(Sun et al., 2023) 50.90 57.38 38.12 55.98 42.68 34.20 38.78 45.43
LLM-Pruner (Ma et al., 2023) 60.28 69.31 47.06 53.43 45.96 29.18 35.60 48.69
LoRAPrune-8bit (Ours) 61.43 70.88 47.65 55.12 45.78 30.50 35.62 49.56
LoRAPrune (Ours) 61.88 71.53 47.86 55.01 45.13 31.62 34.98 49.71
0.0 0.2 0.4 0.6 0.8
12131415161718Perplexity
(a) Moving average
ratio=20%
ratio=50%
200 400 600 800 1000
Iterations12141618202224
(b) Pruning iterations
ratio=20%
ratio=50%
Figure 6: More ablation studies for pruning hyper-parameters: (a) Î»value in moving average, (b) fine-tuning
iterations.
Table 6: Efficiency comparison between LoRAPrune and LLM-Pruner with CPU off-loading.
Method Throughput (s/iter) GPU Memory (GB) FLOPs (G) Total time (h) Pruning time (h) Fine-tuning time (h)
LLM-Pruner 38.87 38.6 20298 5.3 3.5 1.8
LLM-Pruner + CPU offloading 115.67 19.5 20298 25.8 24 1.8
LoRAPrune (Ours) 14.13 18.3 12881 2.0 0.2 1.8
Table 7: Effect of the joint pruning and fine-tuning.
â€œAverageâ€ represents the average performance on seven
classification datasets.
Method WikiText2 â†“PTBâ†“Average â†‘
Ratio=20%Joint 12.93 22.52 60.05
Separate 14.51 24.30 57.18
Ratio=50%Joint 18.37 28.68 49.71
Separate 21.78 40.39 47.56Table 8: Results under different pruning frequen-
cies. â€œAverageâ€ denotes the average performance
on seven classification datasets.
Frequency WikiText2 â†“PTBâ†“Average â†‘
1 18.57 29.02 49.37
5 18.63 28.73 49.16
10 18.21 28.68 49.59
20 18.47 28.70 49.71

--- PAGE 14 ---
Table 9: Generated examples from the Pruned models
Model Generated Sentences
LLaMA-7b
(ratio=0%)Instruction: Tell me about alpacas. Response: Alpacas are members of the camelid family and are native to the Andes
Mountains of South America. They are known for their soft, luxurious fleece, which is used to make clothing, blankets,
and other items. Alpacas are herd animals and live in groups of up to 20 individuals. They are generally docile and
easy to care for, making them popular as pets and livestock. Alpacas can live up to 20 years in captivity, and their
fleece can be sheared up to three times a year.
Instruction: Translate the sentence â€™I have no mouth but I must screamâ€™ into Spanish. Response: I no tengo boca pero
tengo que gritar.
Instruction: Write a Python program that prints the first 10 Fibonacci numbers. Response:
def fibonacci(n):
if n == 0:
return 1
if n == 1:
return 1
else:
return fibonacci(n - 1) + fibonacci(n - 2)
for n in range(1, 10):
print(fibonacci(n))
LLaMA-7b
(ratio=20%)Instruction: Tell me about alpacas. Alpacas are a species of camelid native to South America. They are known for
their soft, luxurious wool, which is used to make clothing, blankets, and other textiles. Alpacas are also used for their
meat, which is considered a delicacy in some parts of the world. Alpacas are social animals that live in herds and are
known for their gentle demeanor. They are also known for their intelligence and ability to be trained for various tasks,
such as pulling carts or performing tricks for entertainment.
Instruction: Translate the sentence â€™I have no mouth but I must screamâ€™ into Spanish. Response: Yo no tengo la boca
pero tengo que gritar.
Instruction: Write a Python program that prints the first 10 Fibonacci numbers. Response:
def fibonacci(n):
if n == 0 or n == 1:
return 1
else:
return fibonacci(n - 1) + fibonacci(n - 2)
for n in range(1, 10):
print(fibonacci(n))
