# 2004.14765.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2004.14765.pdf
# Kích thước tệp: 313841 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2004.14765v1  [cs.LG]  30 Apr 2020Cắt tỉa mạng nơ-ron nhân tạo:
một cách để tìm ra các cực tiểu sắc bén 
có entropy cao và khả năng tổng quát hóa tốt
Enzo Tartaglione[0000−0003−4274−8298], Andrea Bragagnolo, và
Marco Grangetto[0000−0002−2709−7864]
Đại học Torino, Khoa Khoa học Máy tính, Torino TO 101 49, Ý
{enzo.tartaglione, andrea.bragagnolo }@unito.it
Tóm tắt. Gần đây, một cuộc đua hướng tới việc đơn giản hóa các mạng sâu
đã bắt đầu, cho thấy rằng có thể hiệu quả giảm kích thước của
những mô hình này với việc mất hiệu suất tối thiểu hoặc không có. Tuy nhiên, có một
sự thiếu hiểu biết chung về lý do tại sao những chiến lược cắt tỉa này hiệu quả.
Trong công trình này, chúng tôi sẽ so sánh và phân tích các giải pháp đã cắt tỉa
với hai phương pháp cắt tỉa khác nhau, một lần và dần dần, cho thấy
hiệu quả cao hơn của phương pháp sau. Đặc biệt, chúng tôi thấy rằng cắt tỉa
dần dần cho phép truy cập vào các cực tiểu hẹp, có khả năng tổng quát hóa tốt, thường
bị bỏ qua khi sử dụng các phương pháp một lần. Trong công trình này chúng tôi cũng
đề xuất PSP-entropy, một thước đo để hiểu cách một nơ-ron nhất định tương
quan với một số lớp đã học cụ thể. Thú vị là, chúng tôi quan sát thấy rằng
các đặc trưng được trích xuất bởi các mô hình cắt tỉa lặp đi lặp lại ít tương
quan với các lớp cụ thể, có khả năng làm cho những mô hình này phù hợp hơn trong
các phương pháp học chuyển giao.
Từ khóa: Cắt tỉa ·Cực tiểu sắc bén ·Entropy ·Tiềm năng hậu synap
·Học sâu.
1 Giới thiệu
Mạng nơ-ron nhân tạo (ANNs) ngày nay là một trong những thuật toán được nghiên cứu nhiều nhất
được sử dụng để giải quyết một loạt lớn các tác vụ. Thành công của chúng đến từ khả năng
học từ các ví dụ, không đòi hỏi bất kỳ chuyên môn cụ thể nào và sử dụng các
chiến lược học rất tổng quát. Việc sử dụng GPU (và gần đây là TPU) để huấn luyện
ANNs đã mang lại một động lực quyết định cho việc triển khai quy mô lớn của chúng.
Tuy nhiên, nhiều mô hình sâu chia sẻ một trở ngại chung: số lượng lớn các tham
số, điều này cho phép huấn luyện thành công của chúng [1,4], lại tạo ra một số lượng lớn
các phép toán tại thời điểm suy luận, ngăn cản việc triển khai hiệu quả trên
các thiết bị di động và nhúng giá rẻ.
Để giải quyết vấn đề này, một số phương pháp đã được đề xuất,
như định nghĩa các mô hình mới, hiệu quả hơn [9]. Gần đây, một cuộc đua để thu nhỏ kích thước của
những mô hình ANN này đã bắt đầu: các chiến lược cắt tỉa được gọi là thực sự có thể
loại bỏ (hoặc cắt tỉa) các tham số không liên quan từ các mô hình đã được huấn luyện trước, giảm

--- TRANG 2 ---
2 E. Tartaglione et al.
kích thước của mô hình ANN, nhưng vẫn giữ khả năng tổng quát hóa cao. Về chủ đề này,
một lượng rất lớn các chiến lược đã được đề xuất [6,13,19,21] từ đó chúng ta có thể xác định hai lớp chính:
–các chiến lược một lần: có thể cắt tỉa tham số bằng cách sử dụng các
phương pháp tham lam rất nhanh;
–các chiến lược dần dần: chậm hơn nhiều so với các phương pháp một lần, có khả năng
chúng có thể đạt được tỷ lệ nén cao hơn (hoặc nói cách khác, chúng hứa hẹn
cắt tỉa nhiều tham số hơn với chi phí độ phức tạp tính toán cao hơn).
Trong cuộc đua như vậy, tuy nhiên, một nỗ lực để hiểu sâu hơn về các tính chất tiềm năng
của những kiến trúc thưa thớt như vậy đã phần lớn bị đặt sang một bên: có một lý do cụ thể
nào mà chúng ta có thể cắt tỉa nhiều tham số với việc mất tổng quát hóa tối thiểu hoặc không có không?
Các chiến lược một lần có đủ để sánh với các phương pháp cắt tỉa dần dần không?
Có tính chất ẩn nào đằng sau những kiến trúc thưa thớt này không?
Trong công trình này, đầu tiên chúng tôi so sánh các chiến lược cắt tỉa một lần với các
đối tác dần dần của chúng, điều tra các lợi ích có thể có của việc có một chiến lược
thưa thớt hóa chuyên sâu về mặt tính toán hơn nhiều. Sau đó, chúng tôi làm sáng tỏ một số tính chất cục bộ
của các cực tiểu đạt được bằng cách sử dụng hai chiến lược cắt tỉa khác nhau và cuối cùng, chúng tôi
đề xuất PSP-entropy, một thước đo về trạng thái của các nơ-ron được kích hoạt ReLU, để
được sử dụng như một công cụ phân tích để hiểu rõ hơn về các mô hình ANN thưa thớt thu được.
Phần còn lại của bài báo này được tổ chức như sau. Mục 2 xem xét tầm quan trọng của
việc cắt tỉa mạng và tài liệu liên quan nhất. Tiếp theo, trong Mục 3 chúng tôi thảo luận về
tài liệu liên quan xung quanh các tính chất cục bộ của cực tiểu cho các mô hình ANN. Sau đó, trong
Mục 4 chúng tôi đề xuất PSP-entropy, một thước đo để đo lường mức độ một nơ-ron chuyên
môn hóa trong việc xác định các đặc trưng thuộc về một tập con các lớp được học tại thời điểm huấn luyện.
Mục 5 cung cấp các phát hiện của chúng tôi về các tính chất cho các kiến trúc thưa thớt và
cuối cùng, trong Mục 6, chúng tôi rút ra các kết luận và xác định các hướng tiếp theo cho
nghiên cứu tương lai.
2 Các kỹ thuật cắt tỉa hiện đại
Trong tài liệu có thể tìm thấy một số lượng lớn các phương pháp cắt tỉa, một số
cũ [11] và những cái khác gần đây hơn [8,12,16]. Trong số những cái sau, nhiều tiểu
loại có thể được xác định. Ullrich et al. giới thiệu cái mà họ gọi là chia sẻ trọng số mềm,
qua đó có thể giới thiệu sự dư thừa trong mạng và
giảm lượng tham số được lưu trữ [23]. Các phương pháp khác
dựa trên điều hòa tham số và cắt tỉa: ví dụ, Louizos et al. sử dụng một
điều hòa proxy L0; Tartaglione et al., thay vào đó, định nghĩa tầm quan trọng của một tham
số thông qua một thước đo độ nhạy được sử dụng như điều hòa [21]. Các phương pháp khác
dựa trên dropout, như dropout biến thiên thưa thớt, được đề xuất bởi Molchanov et al.,
tận dụng giải thích Bayesian của dropout Gaussian và thúc đẩy độ thưa
thớt trong mô hình ANN [16].
Nhìn chung, hầu hết các kỹ thuật cắt tỉa được đề xuất có thể được chia thành hai lớp
vĩ mô. Lớp đầu tiên được định nghĩa bởi các phương pháp dựa trên cắt tỉa dần dần [14,19,25],

--- TRANG 3 ---
Cắt tỉa ANNs: một cách để tìm các cực tiểu sắc bén có entropy cao và khả năng tổng quát hóa tốt 3
trong đó mạng được, cùng lúc, huấn luyện và cắt tỉa theo một số
phương pháp heuristic, trải qua một số lượng lớn các lần lặp cắt tỉa. Một trong số này, cho thấy hiệu suất tốt nhất, là LOBSTER, nơi các tham số được cắt tỉa
dần dần theo đóng góp cục bộ của chúng vào loss [19]. Lớp thứ hai, thay vào đó, bao gồm tất cả các kỹ thuật dựa trên cắt tỉa một lần [6,8,15]:
ở đây quy trình cắt tỉa bao gồm ba giai đoạn:
1. một mạng lớn, có quá nhiều tham số được huấn luyện bình thường đến hoàn thành;
2. mạng sau đó được cắt tỉa bằng cách sử dụng một loại heuristic nào đó (ví dụ: ngưỡng độ lớn) để thỏa mãn (tỷ lệ phần trăm các tham số còn lại thường
là một siêu tham số);
3. mô hình đã cắt tỉa được tinh chỉnh thêm để khôi phục độ chính xác bị mất do
giai đoạn cắt tỉa.
Một công trình gần đây của Frankle và Carbin [6] được gọi là giả thuyết vé số,
đang có tác động lớn đến cộng đồng nghiên cứu. Họ tuyên bố rằng từ
một ANN, sớm trong quá trình huấn luyện, có thể trích xuất một mạng con thưa thớt theo
cách một lần: mạng thưa thớt như vậy, khi được huấn luyện, có thể đạt độ chính xác
của mô hình gốc. Trong một nghiên cứu tiếp theo, Renda et al. đề xuất một phương pháp huấn luyện lại
thay thế bước tinh chỉnh bằng việc tua lại trọng số: sau khi cắt tỉa, các
tham số còn lại được đặt lại về giá trị ban đầu của chúng và mạng đã cắt tỉa được
huấn luyện lại. Họ cũng lập luận rằng việc sử dụng các giá trị trọng số ban đầu là cơ
bản để đạt được hiệu suất cạnh tranh, điều này bị suy giảm khi bắt đầu từ một
khởi tạo ngẫu nhiên [18].
Mặt khác, Liu et al. cho thấy rằng, ngay cả khi huấn luyện lại một mạng con đã cắt tỉa
bằng cách sử dụng một khởi tạo ngẫu nhiên mới, họ có thể đạt được một mức độ chính xác
có thể so sánh với đối tác dày đặc của nó; thách thức một trong những phỏng đoán
được đề xuất cùng với giả thuyết vé số [13].
Trong công trình của chúng tôi, chúng tôi cố gắng làm sáng tỏ cuộc thảo luận này, so sánh cắt tỉa một lần
hiện đại với cắt tỉa dần dần.
3 Tính chất cục bộ của cực tiểu
Trong phần trước chúng ta đã khám phá một số chiến lược cắt tỉa liên quan nhất.
Tất cả chúng đều dựa vào các chiến lược tối ưu hóa hiện đại: áp dụng
các heuristic tối ưu hóa rất đơn giản để tối thiểu hóa hàm loss, như ví
dụ SGD [2,26], ngày nay có thể thành công trong việc huấn luyện ANNs trên
các tập dữ liệu khổng lồ. Về mặt lý thuyết, đây là "phép màu" của học sâu, vì
tính chiều của bài toán là rất lớn (thực sự, những bài toán này thường có quá
nhiều tham số, và tính chiều có thể được giảm hiệu quả [21]). Hơn
nữa, việc tối thiểu hóa các hàm mục tiêu không lồi thường được cho là làm cho
kiến trúc được huấn luyện bị kẹt vào các cực tiểu cục bộ. Tuy nhiên, bằng chứng thực nghiệm
cho thấy rằng có điều gì đó khác đang xảy ra bên dưới: hiểu nó
nói chung là rất quan trọng.
Goodfellow et al. tiên phong trong vấn đề hiểu tại sao học sâu
hoạt động. Đặc biệt, họ quan sát thấy về cơ bản không có rào cản loss giữa

--- TRANG 4 ---
4 E. Tartaglione et al.
một khởi tạo ngẫu nhiên chung cho mô hình ANN và cấu hình cuối
cùng [7]. Hiện tượng này cũng đã được quan sát trên các kiến trúc lớn hơn bởi
Draxler et al. [5]. Những công trình này đặt nền móng cho các bài báo
"giả thuyết vé số". Tuy nhiên, một quan sát thứ cấp nhưng liên quan trong [7] tuyên bố rằng có
một rào cản loss giữa các cấu hình ANN khác nhau cho thấy khả năng tổng
quát hóa tương tự. Sau đó, đã được chỉ ra rằng thường có thể tìm thấy một đường dẫn loss thấp giữa
các giải pháp tổng quát hóa tốt cho cùng một bài toán học [20]. Từ
cuộc thảo luận ngắn gọn này, rõ ràng là một phương pháp chung về cách đặc trưng tốt hơn
những cực tiểu như vậy vẫn chưa được tìm thấy.
Keskar et al. đã chỉ ra tại sao chúng ta nên ưu tiên các phương pháp batch nhỏ hơn các phương pháp batch lớn:
họ tương quan tính ngẫu nhiên được giới thiệu bởi các phương pháp batch nhỏ với
độ sắc bén của cực tiểu đạt được [10]. Nói chung, họ quan sát thấy rằng batch
càng lớn, cực tiểu đạt được càng sắc bén. Thậm chí thú vị hơn, họ quan
sát thấy rằng cực tiểu càng sắc bén, khả năng tổng quát hóa của mô hình ANN
càng tệ. Nói chung, có nhiều công trình hỗ trợ giả thuyết rằng các cực tiểu phẳng
tổng quát hóa tốt, và điều này cũng đã là sức mạnh cho một phần đáng kể
của nghiên cứu hiện tại [3,10]. Tuy nhiên, nói chung điều này không nhất thiết có nghĩa là
không có cực tiểu sắc bén nào tổng quát hóa tốt, như chúng ta sẽ thấy trong Mục 5.2.
4 Hướng tới hiểu biết sâu hơn: một phương pháp dựa trên entropy
Trong phần này chúng tôi đề xuất PSP-entropy, một thước đo để đánh giá sự phụ thuộc
của đầu ra cho một nơ-ron nhất định trong mô hình ANN vào tác vụ phân loại mục tiêu.
Thước đo được đề xuất sẽ cho phép chúng ta hiểu rõ hơn về hiệu ứng của việc cắt tỉa.
4.1 Tiềm năng hậu synap
Hãy định nghĩa đầu ra của nơ-ron thứ i cho trước ở lớp thứ l là
yl,i=ϕ[f(yl−1,θl,i]) (1)
trong đó yl−1 là đầu vào của nơ-ron đó, θl,i là các tham số liên kết với nó,
f(·) là một hàm affine nào đó và ϕ(·) là hàm kích hoạt, chúng ta có thể định nghĩa
tiềm năng hậu synap (PSP) [22] của nó là
zl,i=f(yl−1,θl,i) (2)
Thường thì, các mô hình sâu được kích hoạt ReLU: từ đây trở đi, hãy xem xét hàm kích
hoạt cho tất cả các nơ-ron trong các lớp ẩn là ϕ(·) = ReLU(·). Dưới giả định như vậy,
dễ dàng xác định hai vùng riêng biệt cho việc kích hoạt nơ-ron:
–zl,i≤0: đầu ra của nơ-ron sẽ chính xác bằng không ∀zl,i≤0;
–zl,i>0: có sự phụ thuộc tuyến tính của đầu ra vào zl,i.

--- TRANG 5 ---
Cắt tỉa ANNs: một cách để tìm các cực tiểu sắc bén có entropy cao và khả năng tổng quát hóa tốt 5
Do đó, hãy định nghĩa
ϕ′(z) =/braceleftbigg
0z≤0
1z >0(3)
Theo trực giác, chúng ta hiểu rằng nếu hai nơ-ron thuộc cùng một lớp, cho
cùng một đầu vào, chia sẻ cùng ϕ′(z), thì chúng có thể ánh xạ tuyến tính thành một
nơ-ron tương đương:
–zl,i≤0,zl,j≤0: một trong số chúng có thể đơn giản bị loại bỏ;
–zl,i>0,zl,j>0: chúng tương đương với một tổ hợp tuyến tính của chúng.
Trong công trình này chúng tôi không quan tâm đến việc sử dụng phương pháp này hướng tới cắt tỉa
có cấu trúc: có nhiều công trình trong tài liệu giải quyết vấn đề này bằng cách sử dụng
các proxy hiệu quả. Trong phần tiếp theo chúng ta sẽ công thức hóa một thước đo để đánh giá
mức độ rối loạn trong các tiềm năng hậu synap. Mục đích của thước đo như vậy
sẽ là có một công cụ phân tích để cho chúng ta hiểu rộng hơn về
hành vi của các nơ-ron trong các kiến trúc thưa thớt.
4.2 PSP-entropy cho các nơ-ron được kích hoạt ReLU
Trong phần trước chúng ta đã nhắc lại khái niệm tiềm năng hậu synap.
Một số khái niệm thú vị cũng đã được giới thiệu cho các mạng được kích
hoạt ReLU: chúng ta có thể sử dụng giá trị của nó để tiếp cận vấn đề phân bin trạng thái của một
nơ-ron, theo ϕ′(zl,i). Do đó, chúng ta có thể xây dựng một quá trình ngẫu nhiên nhị phân
mà chúng ta có thể xếp hạng theo entropy của nó. Để làm điều này, hãy giả sử chúng ta đặt làm
đầu vào của mô hình ANN hai mẫu khác nhau, µc,1 và µc,2, thuộc về
cùng lớp c (cho những đầu vào đó, chúng ta muốn có cùng mục tiêu ở đầu ra
của mô hình ANN). Hãy xem xét PSP zl,i (trong đó l là một lớp ẩn):
–nếu ϕ′(zl,i|µc,1) =ϕ′(zl,i|µc,2) chúng ta có thể nói có entropy PSP thấp;
–nếu ϕ′(zl,i|µc,1)/ne}ationslash=ϕ′(zl,i|µc,2) chúng ta có thể nói có entropy PSP cao.
Chúng ta có thể mô hình hóa một thước đo entropy cho PSP:
H(zl,i|c) =−/summationdisplay
t={0,1}p[ϕ′(zl,i) =t|c]·log2{p[ϕ′(zl,i) =t|c]}(4)
trong đó p[ϕ′(zl,i) =t|c] là xác suất ϕ′(zl,i) =t khi được trình bày một đầu vào thuộc
lớp thứ c. Vì chúng ta thường muốn giải quyết một bài toán đa lớp,
chúng ta có thể mô hình hóa một entropy tổng thể cho nơ-ron là
H(zl,i) =/summationdisplay
cH(zl,i|c) (5)
Rất quan trọng là phải tách các đóng góp của entropy theo
lớp mục tiêu thứ c vì chúng ta mong đợi các nơ-ron sẽ nắm bắt các đặc trưng liên quan
có tương quan cao với các lớp mục tiêu. Phương trình (5) cung cấp cho chúng ta thông tin rất quan trọng
hướng tới mục đích này: giá trị càng thấp thì nó càng chuyên môn hóa cho một số
lớp cụ thể.

--- TRANG 6 ---
6 E. Tartaglione et al.
Công thức trong (5) rất tổng quát và có thể dễ dàng mở rộng cho entropy bậc cao hơn,
tức là entropy của các tập hợp nơ-ron có trạng thái tương quan cho cùng
các lớp. Bây giờ chúng ta đã sẵn sàng sử dụng thước đo này để làm sáng tỏ thêm các phát hiện
trong Mục 5.
5 Thí nghiệm
Cho thử nghiệm của chúng tôi, chúng tôi đã quyết định so sánh cắt tỉa một lần hiện đại
được đề xuất bởi Frankle và Carbin [6] với một trong những chiến lược cắt tỉa dần dần
có hiệu suất cao nhất, LOBSTER [19]. Để làm điều này, đầu tiên chúng tôi thu được một mô hình mạng thưa thớt
bằng cách sử dụng LOBSTER; các tham số không bị cắt tỉa sau đó được khởi tạo lại về
giá trị gốc của chúng, theo giả thuyết vé số [6]. Mục đích của chúng tôi
ở đây là xác định liệu giả thuyết vé số có áp dụng cũng cho các
mô hình thưa thớt thu được bằng cách sử dụng các chiến lược cắt tỉa dần dần có hiệu suất cao.
Như một thí nghiệm thứ hai, chúng tôi muốn kiểm tra các hiệu ứng của việc khởi tạo
ngẫu nhiên khác nhau trong khi giữ kiến trúc thưa thớt đã đạt được. Theo Liu et al.,
điều này sẽ dẫn đến kết quả tương tự với những kết quả thu được với việc khởi tạo gốc [13].
Để làm điều này, chúng tôi đã thử 10 cấu hình bắt đầu khác nhau mới.
Như một thí nghiệm cuối cùng, chúng tôi muốn đánh giá mức độ quan trọng của cấu trúc bắt
nguồn từ thuật toán cắt tỉa trong việc đạt được hiệu suất cạnh tranh sau
khởi tạo lại: cho mục đích này, chúng tôi ngẫu nhiên định nghĩa một kiến trúc đã cắt tỉa mới
với cùng số lượng tham số bị cắt tỉa như những tham số được tìm thấy thông qua LOBSTER. Cũng
trong trường hợp này, 10 cấu trúc khác nhau đã được kiểm tra.
Chúng tôi quyết định thí nghiệm với các kiến trúc và tập dữ liệu khác nhau thường
được sử dụng trong tài liệu liên quan: LeNet-300 và LeNet-5-caffe được huấn luyện trên
MNIST, LeNet-5-caffe được huấn luyện trên Fashion-MNIST [24] và ResNet-32 được huấn luyện
trên CIFAR-10.1 Cho tất cả các lần huấn luyện của chúng tôi, chúng tôi đã sử dụng phương pháp tối ưu hóa SGD với
các siêu tham số tiêu chuẩn và tăng cường dữ liệu, như được định nghĩa trong các bài báo của
các kỹ thuật so sánh khác nhau [6,13,19].
5.1 Cắt tỉa một lần so với cắt tỉa dần dần
Trong Hình 1 chúng tôi cho thấy, đối với các tỷ lệ phần trăm khác nhau của các tham số bị cắt tỉa,
một so sánh giữa độ chính xác kiểm tra của các mô hình được cắt tỉa bằng kỹ thuật LOBSTER và
các mô hình được huấn luyện lại theo các phương pháp chúng tôi đã định nghĩa trước đó.
Chúng ta có thể xác định rõ ràng một chế độ tỷ lệ nén thấp trong đó mô hình được khởi tạo lại
có thể khôi phục độ chính xác gốc, xác nhận giả thuyết vé số. Mặt khác, khi tỷ lệ nén tăng (ví
dụ khi chúng ta loại bỏ hơn 95% tham số của mô hình LeNet-300, như quan sát trong
Hình 1a), phương pháp huấn luyện lại cố gắng đạt được các lỗi phân loại thấp.
Như người ta có thể mong đợi, các kết hợp khác của tập dữ liệu và mô hình có thể phản ứng
khác nhau. Ví dụ, LeNet-300 không còn có thể tái tạo hiệu suất gốc
khi được cấu tạo từ ít hơn 5% của các tham số gốc. Mặt
1https://github.com/akamaster/pytorch resnetcifar10

--- TRANG 7 ---
Cắt tỉa ANNs: một cách để tìm các cực tiểu sắc bén có entropy cao và khả năng tổng quát hóa tốt 7
0 40.15 60.64 80.06 90.07 95.0 98.0
Tham số bị cắt tỉa (%)1.61.92.53.03.55.0Lỗi phân loạiLOBSTER
Frankle và Carbin
Liu et al.
Cắt tỉa ngẫu nhiên0 0.98 2.44 11.7 33.66 48.54 64.14Nơ-ron bị cắt tỉa (%)
(a)
0 63.05 80.14 90.01 95.06 98.0 99.02 99.57
Tham số bị cắt tỉa (%)0.70.81.01.52.03.0Lỗi phân loạiLOBSTER
Frankle và Carbin
Liu et al.
Cắt tỉa ngẫu nhiên0 1.5513.62 37.75 60.17 74.31 81.03 83.45Nơ-ron bị cắt tỉa (%)
(b)
0 60.89 80.53 90.08 95.02
Tham số bị cắt tỉa (%)8.208.458.809.5010.0010.50Lỗi phân loạiLOBSTER
Frankle và Carbin
Liu et al.
Cắt tỉa ngẫu nhiên0 0 7.59 30.17 46.72Nơ-ron bị cắt tỉa (%)
(c)
0 40.24 60.72 80.11
Tham số bị cắt tỉa (%)7.27.47.67.88.59.0Lỗi phân loạiLOBSTER
Frankle và Carbin
Liu et al.
Cắt tỉa ngẫu nhiên0 0.43 0.52 0.69Nơ-ron bị cắt tỉa (%)
(d)
Hình 1: Lỗi tập kiểm tra cho các tỷ lệ nén khác nhau: LeNet-300 (a) được huấn luyện trên
MNIST, LeNet-5 được huấn luyện trên MNIST (b), LeNet-5 được huấn luyện trên Fashion-MNIST (c)
và ResNet-32 được huấn luyện trên CIFAR-10 (d).

--- TRANG 8 ---
8 E. Tartaglione et al.
khác, LeNet-5, khi áp dụng trên MNIST, có thể đạt được độ chính xác
khoảng 99.20% ngay cả khi 98% tham số của nó bị cắt tỉa (Hình 1b). Điều này
không xảy ra khi áp dụng trên một tập dữ liệu phức tạp hơn như Fashion-MNIST,
nơi loại bỏ 80% tham số đã dẫn đến suy giảm hiệu suất (Hình 1c).
Khoảng cách như vậy trở nên cực kỳ rõ ràng khi chúng ta khởi tạo lại
một kiến trúc phức tạp hơn như ResNet-32 được huấn luyện trên CIFAR-10 (Hình 1d).
Từ các kết quả được báo cáo, chúng tôi quan sát thấy rằng việc khởi tạo gốc không phải lúc nào cũng
quan trọng: khoảng cách lỗi giữa một mô hình được khởi tạo ngẫu nhiên và một mô hình
sử dụng giá trị trọng số gốc là nhỏ, với cái sau hơi tốt hơn.
Hơn nữa, cả hai đều thất bại trong việc khôi phục hiệu suất cho tỷ lệ nén cao.
5.2 Các cực tiểu sắc bén cũng có thể tổng quát hóa tốt
10−510−4
(a)
102103
(b)
Hình 2: Kết quả của LeNet-5 được huấn luyện trên MNIST với nén cao nhất
(99.57%): (a) vẽ loss trong tập huấn luyện và (b) vẽ 5 giá trị riêng hessian lớn nhất.
G là giải pháp được tìm thấy với học dần dần trong khi 1-S là giải pháp một lần tốt nhất (Frankle và Carbin).
Để nghiên cứu độ sắc bén của các cực tiểu cục bộ, hãy tập trung, ví dụ, vào
các kết quả thu được trên LeNet-5 được huấn luyện trên MNIST. Chúng tôi chọn tập trung
sự chú ý của mình vào mô hình ANN cụ thể này vì, theo hiện trạng và
phù hợp với các phát hiện của chúng tôi, chúng tôi quan sát thấy khoảng cách hiệu suất thấp nhất
giữa cắt tỉa dần dần và một lần (như được mô tả trong Hình 1b); do đó, đây là một
tình huống thách thức hơn để quan sát các khác biệt định tính giữa hai phương pháp.
Tuy nhiên, chúng tôi nhấn mạnh rằng tất cả các quan sát cho trường hợp như vậy cũng áp dụng cho
các kiến trúc/tập dữ liệu khác được khám phá trong Mục 5.1.
Để thu được các bản đồ trong Hình 2, chúng tôi theo phương pháp được đề xuất bởi [7]

--- TRANG 9 ---
Cắt tỉa ANNs: một cách để tìm các cực tiểu sắc bén có entropy cao và khả năng tổng quát hóa tốt 9
và chúng tôi vẽ loss cho các cấu hình ANN giữa hai cấu hình tham chiếu:
trong trường hợp của chúng tôi, chúng tôi so sánh một giải pháp được tìm thấy với cắt tỉa dần dần (G) và một
lần (1-S). Sau đó, chúng tôi lấy một hướng trực giao ngẫu nhiên để tạo ra một bản đồ 2D.
Hình 2a cho thấy loss trên tập huấn luyện giữa cắt tỉa lặp và một lần
cho tỷ lệ nén cao nhất (99.57% tham số bị cắt tỉa như được hiển thị trong
Hình 1b). Theo các phát hiện trước đó của chúng tôi, chúng ta thấy rằng cắt tỉa lặp nằm
trong một vùng loss thấp hơn. Ở đây, chúng tôi cũng cho thấy đồ thị của 5 giá trị riêng Hessian
hàng đầu (tất cả đều dương), trong Hình 2b, sử dụng phương pháp hiệu quả được đề xuất trong [17].
Rất thú vị, chúng tôi quan sát thấy rằng giải pháp được đề xuất bởi cắt tỉa lặp
nằm trong một cực tiểu hẹp hơn so với cái được tìm thấy bằng chiến lược một lần,
mặc dù tổng quát hóa hơi tốt hơn. Với điều này, chúng tôi không tuyên bố rằng các cực tiểu hẹp hơn
tổng quát hóa tốt: các chiến lược cắt tỉa dần dần cho phép truy cập vào một tập con của
các cực tiểu hẹp tổng quát hóa tốt, cho thấy rằng không phải tất cả các cực tiểu hẹp đều tổng
quát hóa tệ hơn so với các cực tiểu rộng. Phát hiện này đưa ra cảnh báo chống lại tối ưu hóa bậc hai,
có thể ưu tiên việc tìm kiếm các cực tiểu phẳng hơn, rộng hơn, bỏ qua
các cực tiểu hẹp tổng quát hóa tốt. Những giải pháp không tầm thường này được tự nhiên tìm thấy
bằng cách sử dụng cắt tỉa dần dần không thể được tìm thấy bằng các phương pháp một lần, mà
ngược lại tập trung nỗ lực của chúng vào các cực tiểu lớn hơn. Nói chung, độ sắc bén của
những cực tiểu này giải thích tại sao, đối với tỷ lệ nén cao, các chiến lược huấn luyện lại thất bại
trong việc khôi phục hiệu suất, xem xét rằng nói chung khó tiếp cận hơn
lớp cực tiểu này.
5.3 Nghiên cứu về tiềm năng hậu synap
103104
Hình 3: Chuẩn L2 của các giá trị PSP cho LeNet-5 được huấn luyện trên MNIST với 99.57%
tham số bị cắt tỉa.
Trong Mục 5.2 chúng ta đã quan sát thấy rằng, như một kết quả, các chiến lược lặp tập trung vào
các cực tiểu sắc bén tổng quát hóa tốt. Có điều gì khác còn phải nói về những cái đó không?
Hãy kiểm tra các giá trị độ lớn trung bình của PSP cho các
giải pháp được tìm thấy khác nhau: để làm điều này, chúng ta có thể vẽ trung bình của các giá trị chuẩn L2
của chúng (z2). Như một phát hiện đầu tiên, các kiến trúc được cắt tỉa dần dần tự nhiên có
giá trị chuẩn L2 PSP thấp hơn, như chúng ta quan sát trong Hình 3. Không có chiến lược cắt tỉa nào được sử dụng

--- TRANG 10 ---
10 E. Tartaglione et al.
tối thiểu hóa một cách rõ ràng thành phần trong z2: chúng tự nhiên điều khiển việc học hướng tới
những vùng như vậy. Tuy nhiên, giải pháp cho thấy khả năng tổng quát hóa tốt hơn
cho thấy giá trị z2 thấp hơn. Tất nhiên, có những vùng với giá trị z2 thậm chí thấp hơn;
tuy nhiên, theo Hình 2a, chúng nên được loại trừ vì chúng tương ứng
với các giá trị loss cao (không phải tất cả các vùng z2 thấp đều là loss thấp). Nếu chúng ta nhìn vào
1.9
1.88
1.86
1.84
1.82
(a)
4.50
4.45
4.40
4.35
(b)
Hình 4: Kết quả trên LeNet-5 được huấn luyện trên MNIST với 99.57% tham số bị cắt tỉa.
(a) vẽ PSP-entropy bậc một, trong khi (b) hiển thị PSP entropy bậc hai.
PSP-entropy được công thức hóa trong (5), chúng ta quan sát thấy điều gì đó thú vị: cắt tỉa dần dần và
một lần cho thấy entropy bậc một có thể so sánh, như được hiển thị trong Hình 4a.2
Thú vị là thấy rằng cũng có những vùng entropy thấp hơn mà
tuy nhiên tương ứng với các giá trị loss cao hơn, theo Hình 2a. Khi chúng ta chuyển sang
entropy bậc cao hơn, điều gì đó thậm chí thú vị hơn xảy ra: cắt tỉa dần dần
cho thấy entropy cao hơn so với một lần, như được mô tả trong Hình 4b (hiển thị
entropy bậc hai). Trong trường hợp như vậy, có entropy thấp hơn có nghĩa là có nhiều
nhóm nơ-ron chuyên môn hóa cho các mẫu cụ thể tương quan với lớp
mục tiêu; ngược lại, có entropy cao hơn nhưng vẫn cho thấy hiệu suất tổng quát hóa tốt hơn
dẫn đến có các đặc trưng tổng quát hơn, bất khả tri hơn đối với một
lớp cụ thể, vẫn cho phép phân loại chính xác được thực hiện bởi lớp
đầu ra. Phát hiện phản trực giác này có các ứng dụng tiềm năng khổng lồ trong học
chuyển giao và thích ứng miền, nơi việc trích xuất các đặc trưng tổng quát hơn là quan trọng,
không rất cụ thể cho bài toán được huấn luyện ban đầu.
2mã nguồn cho PSP-entropy có sẵn tại
https://github.com/EIDOSlab/PSP-entropy.git

--- TRANG 11 ---
Cắt tỉa ANNs: một cách để tìm các cực tiểu sắc bén có entropy cao và khả năng tổng quát hóa tốt 11
6 Kết luận
Trong công trình này chúng tôi đã so sánh cắt tỉa một lần và dần dần trên các
kiến trúc và tập dữ liệu hiện đại khác nhau. Đặc biệt, chúng tôi đã tập trung sự chú ý của mình
vào việc hiểu các khác biệt và giới hạn tiềm năng của cả hai phương pháp hướng tới
việc đạt được độ thưa thớt trong các mô hình ANN.
Chúng tôi đã quan sát thấy rằng các chiến lược một lần rất hiệu quả để đạt được độ
thưa thớt vừa phải với chi phí tính toán thấp hơn. Tuy nhiên, có một giới hạn đối với
độ thưa thớt tối đa có thể đạt được, có thể được vượt qua bằng cách sử dụng cắt tỉa dần dần.
Kiến trúc thưa thớt cao, thú vị là, tập trung vào một tập con của các cực tiểu sắc bén
có thể tổng quát hóa tốt, điều này đặt ra một số câu hỏi về tính tối ưu phụ tiềm năng
của tối ưu hóa bậc hai trong những tình huống như vậy. Điều này giải thích tại sao chúng ta
quan sát thấy rằng các chiến lược một lần thất bại trong việc khôi phục hiệu suất cho tỷ lệ nén cao.
Quan trọng hơn, chúng tôi đã quan sát, trái với những gì có thể
được mong đợi, rằng các kiến trúc được cắt tỉa dần dần có độ thưa thớt cao có thể trích xuất
các đặc trưng tổng quát không tương quan chặt chẽ với các lớp được huấn luyện,
làm cho chúng một cách bất ngờ, có khả năng, phù hợp tốt cho các tình huống học chuyển giao.
Các công trình tương lai bao gồm một nghiên cứu định lượng về học chuyển giao cho các kiến trúc thưa thớt
và học dựa trên tối đa hóa PSP-entropy.
Tài liệu tham khảo
1. Ba, J., Caruana, R.: Các mạng sâu có thực sự cần phải sâu không? Trong: Advances in neural
information processing systems. trang 2654–2662 (2014)
2. Bottou, L.: Học máy quy mô lớn với descent gradient ngẫu nhiên. Trong: Pro-
ceedings of COMPSTAT'2010, trang 177–186. Springer (2010)
3. Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
Chayes, J., Sagun, L., Zecchina, R.: Entropy-sgd: Thiên vị gradient descent vào
thung lũng rộng. arXiv preprint arXiv:1611.01838 (2016)
4. Denton, E.L., Zaremba, W., Bruna, J., LeCun, Y., Fergus, R.: Khai thác cấu trúc tuyến tính
trong mạng convolutional để đánh giá hiệu quả. Trong: Advances in
neural information processing systems. trang 1269–1277 (2014)
5. Draxler, F., Veschgini, K., Salmhofer, M., Hamprecht, F.A.: Về cơ bản không có rào cản
trong cảnh quan năng lượng mạng nơ-ron. arXiv preprint arXiv:1803.00885 (2018)
6. Frankle, J., Carbin, M.: Giả thuyết vé số: Tìm các mạng nơ-ron thưa thớt, có thể huấn luyện (2019),
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069453436&partnerID=40&md5=fd1a2b2384d79f66a49cc838a76343d3 ,
được trích dẫn Bởi 8
7. Goodfellow, I.J., Vinyals, O., Saxe, A.M.: Đặc trưng định tính các vấn đề tối ưu hóa mạng nơ-ron. arXiv preprint arXiv:1412.6544 (2014)
8. Han, S., Pool, J., Tran, J., Dally, W.: Học cả trọng số và kết nối cho
mạng nơ-ron hiệu quả. Trong: Advances in neural information processing systems.
trang 1135–1143 (2015)
9. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-
dreetto, M., Adam, H.: Mobilenets: Mạng nơ-ron convolutional hiệu quả cho
ứng dụng thị giác di động. arXiv preprint arXiv:1704.04861 (2017)

--- TRANG 12 ---
12 E. Tartaglione et al.
10. Keskar, N.S., Mudigere, D., Nocedal, J., Smelyanskiy, M., Tang, P.T.P.: Về huấn luyện
batch lớn cho học sâu: Khoảng cách tổng quát hóa và cực tiểu sắc bén. arXiv
preprint arXiv:1609.04836 (2016)
11. LeCun, Y., Denker, J.S., Solla, S.A.: Tổn thương não tối ưu. Trong: Advances trong neural
information processing systems. trang 598–605 (1990)
12. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Cắt tỉa bộ lọc cho
convnets hiệu quả. arXiv preprint arXiv:1608.08710 (2016)
13. Liu, Z., Sun, M., Zhou, T., Huang, G., Darrell, T.: Suy nghĩ lại về giá trị của
cắt tỉa mạng. arXiv preprint arXiv:1810.05270 (2018)
14. Louizos, C., Welling, M., Kingma, D.P.: Học mạng nơ-ron thưa thớt thông qua
điều hòa l0. arXiv preprint arXiv:1712.01312 (2017)
15. Luo, J.H., Wu, J., Lin, W.: Thinet: Một phương pháp cắt tỉa cấp bộ lọc cho nén
mạng nơ-ron sâu. Trong: Proceedings of the IEEE international conference on
computer vision. trang 5058–5066 (2017)
16. Molchanov, D., Ashukha, A., Vetrov, D.: Dropout biến thiên làm
thưa thớt mạng nơ-ron sâu. Trong: 34th International Conference on
Machine Learning, ICML 2017. vol. 5, trang 3854–3863 (2017),
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048506601&partnerID=40&md5=c352a4786ef977ccea7e397bd7469f14 ,
được trích dẫn Bởi 29
17. Noah Golmant, Zhewei Yao, A.G.M.M.J.G.: pytorch-hessian-
eigentings: phân tích eigenvalue hessian pytorch hiệu quả (Oct 2018),
https://github.com/noahgolmant/pytorch-hessian-eigenthings
18. Renda, A., Frankle, J., Carbin, M.: So sánh rewinding và fine-tuning trong
cắt tỉa mạng nơ-ron. arXiv preprint arXiv:2003.02389 (2020)
19. Tartaglione, E., B.A.G.M.L.S.: Điều hòa độ nhạy dựa trên loss: hướng tới mạng nơ-ron sâu thưa thớt.
https://iris.unito.it/retrieve/handle/2318/1737767/608158/ICML20.pdf (2020)
20. Tartaglione, E., Grangetto, M.: Đi dạo vào không gian giải pháp cho các vấn đề phân loại
trong mạng nơ-ron. Trong: International Conference on Image Analysis and
Processing. trang 345–355. Springer (2019)
21. Tartaglione, E., Lepsøy, S., Fiandrotti, A., Francini, G.: Học mạng nơ-ron thưa thớt
thông qua điều hòa hướng độ nhạy. Trong: Advances in Neural Information
Processing Systems. trang 3878–3888 (2018)
22. Tartaglione, E., Perlo, D., Grangetto, M.: Điều hòa tiềm năng hậu synap
có tiềm năng. Trong: International Conference on Artificial Neural Networks. trang 187–
200. Springer (2019)
23. Ullrich, K., Welling, M., Meeds, E.: Chia sẻ trọng số mềm cho nén mạng nơ-ron. Trong: 5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings (2019),
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071003624&partnerID=40&md5=dc00c36113f775ff4a6978b86543814d ,
được trích dẫn Bởi 2
24. Xiao, H., Rasul, K., Vollgraf, R.: Fashion-mnist: một tập dữ liệu hình ảnh mới cho
đo chuẩn các thuật toán học máy. CoRR abs/1708.07747 (2017),
http://arxiv.org/abs/1708.07747
25. Zhu, M., Gupta, S.: Cắt tỉa, hay không cắt tỉa: khám phá hiệu quả của cắt tỉa
cho nén mô hình. arXiv preprint arXiv:1710.01878 (2017)
26. Zinkevich, M., Weimer, M., Li, L., Smola, A.J.: Descent gradient ngẫu nhiên song song hóa. Trong: Advances in neural information processing systems. trang 2595–2603
(2010)
