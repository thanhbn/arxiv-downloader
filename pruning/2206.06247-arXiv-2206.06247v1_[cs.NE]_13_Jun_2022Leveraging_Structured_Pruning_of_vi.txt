# 2206.06247.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2206.06247.pdf
# Kích thước tệp: 155607 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2206.06247v1  [cs.NE]  13 Jun 2022Tận dụng Cắt tỉa có Cấu trúc của
Mạng Nơ-ron Tích chập
Hugo Tessier∗†, Vincent Gripon†, Mathieu L´ eonardon†, Matthieu Arzel†, David Bertrand∗, Thomas Hannagan∗
∗Stellantis , V´ elizy-Villacoublay, France
{1, 5, 6}@stellantis.com
†IMT Atlantique , Lab-STICC, UMR CNRS 6285, F-29238 Brest, France
{1, 2, 3, 4 }@imt-atlantique.fr
Tóm tắt —Cắt tỉa có cấu trúc là một phương pháp phổ biến để giảm
chi phí của mạng nơ-ron tích chập, vốn là công nghệ tiên tiến
trong nhiều tác vụ thị giác máy tính. Tuy nhiên, tùy thuộc vào
kiến trúc, việc cắt tỉa gây ra sự không nhất quán về chiều
dữ liệu ngăn cản việc giảm thực tế các mạng đã cắt tỉa. Để giải quyết
vấn đề này, chúng tôi đề xuất một phương pháp có thể lấy bất kỳ
mặt nạ cắt tỉa có cấu trúc nào và tạo ra một mạng không
gặp phải bất kỳ vấn đề nào trong số này và có thể được tận dụng hiệu quả.
Chúng tôi cung cấp mô tả chính xác về giải pháp của mình và cho thấy
kết quả về lợi ích, trong việc tiêu thụ năng lượng và thời gian suy luận trên
phần cứng nhúng, của mạng nơ-ron tích chập đã cắt tỉa.
Từ khóa chỉ mục —Học Sâu, Nén, Cắt tỉa, Năng lượng,
Suy luận, GPU
I. GIỚI THIỆU
Mạng nơ-ron sâu đang ở mức tiên tiến trong nhiều
lĩnh vực, chẳng hạn như thị giác máy tính. Ví dụ, mạng nơ-ron tích chập
được sử dụng để giải quyết các tác vụ khác nhau như
phân loại [17] hoặc phân đoạn ngữ nghĩa [16]. Tuy nhiên,
chi phí về năng lượng, bộ nhớ và độ trễ của chúng là cấm đoán trên
phần cứng nhúng, và đây là lý do tại sao nhiều công trình tập trung vào
việc giảm chi phí của chúng để phù hợp với các mục tiêu có tài nguyên hạn chế [1].
Lĩnh vực nén mạng nơ-ron sâu có nhiều
loại phương pháp, chẳng hạn như lượng tử hóa [3] hoặc chưng cất [9].
Phương pháp mà chúng tôi tập trung trong bài viết này là cắt tỉa [5], bao gồm
việc loại bỏ các trọng số không cần thiết khỏi mạng. Cắt tỉa là một
kỹ thuật phổ biến đưa ra nhiều thách thức, bao gồm
việc tìm ra loại độ thưa thớt phù hợp nhất để được
tận dụng trên phần cứng [13].
Để tập trung vào cách tiếp cận lý thuyết nghiên cứu tác động
của việc loại bỏ trọng số khỏi chức năng của mạng đối với độ chính xác của nó, nhiều bài báo chỉ loại bỏ trọng số bằng cách đặt giá trị
của chúng về không. Tuy nhiên, điều này không làm giảm chi phí của mạng
và chỉ cung cấp ước tính thô về nén mạng về
mặt bộ nhớ. Tận dụng cắt tỉa để có được lợi ích trên phần cứng
thực tế không phải là một nhiệm vụ tầm thường. Cắt tỉa các trọng số riêng lẻ [5]
("cắt tỉa không có cấu trúc") tạo ra các ma trận thưa, khó
tăng tốc [13]. Cắt tỉa toàn bộ bộ lọc tích chập
(còn gọi là "cắt tỉa có cấu trúc") dễ khai thác hơn, nhưng các
chiều đầu vào và đầu ra của các lớp bị thay đổi, có thể
gây ra nhiều vấn đề trong mạng, đặc biệt là những mạng bao gồm
các phụ thuộc tầm xa giữa các lớp [6]. Giải pháp cho
vấn đề này, hầu như luôn luôn, hoặc không được đề cập, hoặc được tránh bằng cách hạn chế việc cắt tỉa chỉ nhắm vào các lớp không gây ra vấn đề [11]. Tuy nhiên, những hạn chế này
được dự kiến sẽ làm giảm hiệu quả của việc cắt tỉa.
Trong bài báo này, chúng tôi đề xuất một giải pháp để giảm hiệu quả
kích thước của mạng bằng cách sử dụng cắt tỉa có cấu trúc, đã được áp dụng
một mặt nạ sử dụng cắt tỉa có cấu trúc. Phương pháp của chúng tôi là tổng quát,
tự động và đáng tin cậy tạo ra một mạng được cắt tỉa hiệu quả.
Chúng tôi chứng minh khả năng hoạt động trên mạng với độ phức tạp bất kỳ
bằng cách áp dụng nó trên cả mạng phân loại tiêu chuẩn [6] trên tập dữ liệu ImageNet ILSVRC2012 [17] và trên
mạng phân đoạn ngữ nghĩa phức tạp hơn [18] được huấn luyện
trên CityScapes [2]. Chúng tôi cho thấy rằng giải pháp của chúng tôi cho phép có được lợi ích
trong việc tiêu thụ năng lượng và thời gian suy luận trên phần cứng nhúng như NVIDIA Jetson AGX Xavier embedded GPU,
cung cấp ước tính thực tế về cách cắt tỉa có cấu trúc có thể được
tận dụng để giảm dấu chân năng lượng và độ trễ trên một
mục tiêu phần cứng thực.
II. CÁC CÔNG TRÌNH LIÊN QUAN
Ban đầu được thiết kế để cải thiện khả năng tổng quát của mạng nơ-ron [10], cắt tỉa hiện là một phương pháp phổ biến để giảm dấu chân
bộ nhớ hoặc tính toán của chúng. Hình thức cắt tỉa cơ bản nhất liên quan đến việc che đi các trọng số có độ lớn nhỏ nhất theo
cách không có cấu trúc [5]. Phương pháp này không làm giảm
kích thước của các tensor tham số, mà thay vào đó các số không được giới thiệu
giúp nén trọng số mạng thông qua các
sơ đồ mã hóa [4]. Tuy nhiên, việc có được bất kỳ loại tăng tốc nào từ
phương pháp này là khó khăn trên hầu hết phần cứng [13].
Để tận dụng tốt hơn việc cắt tỉa trên phần cứng, nhiều phương pháp
thay vào đó áp dụng "cắt tỉa có cấu trúc", thường liên quan đến việc cắt tỉa
toàn bộ nơ-ron, tức là bộ lọc trong trường hợp các
lớp tích chập [11]. Các loại cắt tỉa có cấu trúc khác tồn tại, chẳng hạn như
"cắt tỉa hình dạng bộ lọc" [21] và đây là lý do tại sao chúng tôi sẽ ưu tiên
tên gọi "cắt tỉa bộ lọc" để tránh sự mơ hồ. Cắt tỉa
trọng số và cắt tỉa bộ lọc là hai loại cấu trúc cắt tỉa phổ biến nhất.
Khi cắt tỉa bất kỳ loại cấu trúc nào, hai khía cạnh phải được
giải quyết: 1) cách xác định các phần tử cần cắt tỉa và 2) cách
cắt tỉa chúng. Vấn đề đầu tiên có thể được giải quyết bằng cách sử dụng các loại
tiêu chí cắt tỉa khác nhau. Trong trường hợp cắt tỉa không có cấu trúc, độ lớn của trọng số [5] hoặc gradient của chúng [15] là hai tiêu chí phổ biến. Khi cắt tỉa bộ lọc, những tiêu chí này có thể được mở rộng để
hoặc chuẩn của chúng trên một bộ lọc [11] hoặc một proxy tính đến
tầm quan trọng của toàn bộ bộ lọc, ví dụ như hệ số nhân

--- TRANG 2 ---
C
CC
+
C R
Thao tác Chỉ mục-Cộng Thao tác Chỉ mục-CộngLớp Tích chập Lớp Tích chập Lớp Tích chập
Hình 1: Minh họa các khó khăn khi cắt tỉa bộ lọc trong mạng nơ-ron tích chập. Các lớp tích chập được tạo từ
bộ lọc, mỗi bộ lọc xuất ra một kênh (hoặc "bản đồ đặc trưng"). Các phần tử màu xám tượng trưng cho các bộ lọc đã cắt tỉa và các kernel cần
loại bỏ để phù hợp với chiều của đầu vào (Vấn đề 1). Ở cuối mỗi khối dư, đầu ra của lớp cuối được cộng
với đầu vào của khối. Nếu hai tensor được cắt tỉa khác nhau (Vấn đề 3), việc từng là phép cộng bây giờ là hỗn hợp của
phép cộng (+), nối (C) hoặc bypass (vòng tròn gạch ngang) mà chúng tôi gọi là toán tử chỉ mục-cộng (Phần III-D).
Hậu quả là số kênh cuối cùng không thể được dự đoán chỉ từ một lớp cụ thể trong mạng, mà
phải được suy ra bằng cách tính đến tất cả các phụ thuộc (Vấn đề 2).
trọng số đã học được bao gồm trong các lớp batch-normalization [12].
Những tiêu chí này có thể được áp dụng theo hai cách khác nhau: hoặc chúng
được sử dụng để xác định cùng một lượng (hoặc một lượng được xác định trước) trọng số/bộ lọc để loại bỏ trong tất cả các lớp (cắt tỉa cục bộ) hoặc mục tiêu được đặt toàn cục và tiêu chí được áp dụng cho tất cả các lớp
cùng một lúc (cắt tỉa toàn cục).
Về vấn đề thứ hai, nhiều phương pháp phổ biến áp dụng
một khung đơn giản [4]: huấn luyện mạng, cắt tỉa một tỷ lệ nhất định
trọng số bằng cách che đi chúng, tinh chỉnh
mạng và lặp lại hai bước cuối nhiều lần cho đến khi
đạt được tỷ lệ cắt tỉa mục tiêu. Các phương pháp khác có thể liên quan đến một
cách tiếp cận tiến bộ hơn [7] có thể bao gồm cơ chế tái sinh [14]. Một số kỹ thuật đề xuất cách liên tục hơn
để cắt tỉa trọng số, ví dụ bằng cách áp dụng cho chúng một hình phạt
trong quá trình huấn luyện [20].
III. PHƯƠNG PHÁP
A. Hậu quả của Cắt tỉa có Cấu trúc
Trong Phần II, chúng tôi đã giải thích cắt tỉa có cấu trúc là gì. Để
trình bày các vấn đề nó có thể gây ra, cũng như các
giải pháp chúng tôi đề xuất, chúng ta cần giới thiệu một số ký hiệu.
Gọi N là một mạng nơ-ron tích chập. Để thuận tiện, chúng ta sẽ xem xét rằng nó chỉ được tạo từ
các lớp tích chập li, có chiều đầu vào và đầu ra là
fi
in và fi
out. Mỗi tích chập chứa fi
out×fi
in×ki
h×ki
w
trọng số wi (với ki
h×ki
w là kích thước của kernel của lớp) và
fi
out bias bi. Một bộ lọc tương ứng với fi
in×ki
h×ki
w
trọng số và một bias tạo ra một trong fi
out kênh
trong các bản đồ đặc trưng đầu ra. Mỗi lớp này hoạt động trên
các bản đồ đặc trưng có kích thước fi
in×hi×wi với hi×wi là độ phân giải của
các bản đồ đặc trưng. Trong trường hợp các mạng như ResNet [6]
hoặc HRNet [18], các lớp khác nhau có thể lấy cùng các bản đồ đặc trưng
làm đầu vào và nhiều bản đồ đặc trưng có thể được cộng
lại với nhau. Cách trình bày đơn giản hóa này đủ để trình bày
các vấn đề gây ra bởi cắt tỉa toàn cục.a) Vấn đề 1: Cắt tỉa bộ lọc làm giảm chiều đầu ra
fout của một lớp. Do đó, chiều của đầu ra
của nó khác và chiều đầu vào fin của các lớp tiếp theo
phải được điều chỉnh. Vấn đề này được biết đến rộng rãi trong
tài liệu [11] và dễ giải quyết trong các mạng đơn giản.
b) Vấn đề 2: Các kết nối dư [6] có thể giới thiệu
các phụ thuộc tầm xa và, do đó, việc xác định tất cả các lớp
bị ảnh hưởng bởi thay đổi chiều có thể khó khăn. Vấn đề này thường được giải quyết bằng cách tránh cắt tỉa các lớp liên quan
đến những phụ thuộc như vậy [11], nhưng giải pháp này không tối ưu.
c) Vấn đề 3: Các kết nối dư [6] thường liên quan đến
việc cộng các bản đồ đặc trưng lại với nhau, do đó phải có
cùng chiều, điều này không còn đúng nữa sau khi cắt tỉa toàn cục.
Trong trường hợp cắt tỉa cục bộ, các bản đồ đặc trưng có
cùng chiều, nhưng cùng một mặt nạ có thể không được
áp dụng trên cả hai bản đồ đặc trưng, và việc cộng các
kênh được dự định cộng lại với nhau tạo ra một
tensor có chiều cao hơn. Vấn đề này ít được thảo luận trong
tài liệu và chủ yếu được giải quyết bằng cách sử dụng các phép toán tùy chỉnh để
điều chỉnh thủ công chiều của các bản đồ đặc trưng [8].
Ba vấn đề này, được minh họa trong Hình 1, hoặc được
lảng tránh hoặc không được giải quyết trong tài liệu, mặc dù hầu hết
các bài báo đều đối phó với các kiến trúc dựa trên ResNet gây ra
cả ba vấn đề. Một số chuyên môn cho phép thủ công tìm ra
các phụ thuộc trong những mạng như vậy, nhưng độ phức tạp có thể vượt quá
tầm kiểm soát trong trường hợp các mạng như HRNets [18]. Thật vậy,
việc bỏ lỡ bất kỳ vấn đề nào trong số này làm cho các mạng không thể
chạy hiệu quả hoặc chạy hoàn toàn trên phần cứng. Đây là
lý do tại sao chúng tôi đề xuất một phương pháp có thể tự động và
đáng tin cậy tạo ra các mạng đã cắt tỉa có thể chạy hiệu quả
trên phần cứng.
B. Tổng quát hóa Toán tử để Xử lý một Tập con Kênh

--- TRANG 3 ---
+ - +
0 + 0
- 0 ++ 0 +
-
0- - +
-
+
0
0
0+
-
+-
+
-
0 0 0
0 0 0
0 0 0- + -
0
+0 0 +
+
-
X
BN Chuẩn hóa
Trọng số Batch-Normalization
Bias
+ + +
+ + +
+ + ++ - +
+
-+ + 0
-
-
ReLU
+ + +
+ + +
+ + ++ 0 +
+
0+ + 0
0
0Hàm KíchhoạtTích chậpĐầu vào + + +
+ + +
+ + ++ + +
+
++ + +
+
+
0
0
01
1
11
1
1
0 0 0
0 0 0
0 0 0+ + +
+
++ + +
+
+
X
0 0 0
0 0 0
0 0 0+ + +
+
++ + +
+
+A
B
C
D
E
F
Hình 2: Minh họa phương pháp được đề xuất để xác định các trọng số
ngắt kết nối, với mạng gốc ở bên trái và phiên bản
đã sửa đổi ở bên phải. (A) Các tensor đầu vào đồng nhất để
tránh các giá trị null không mong muốn, (B) trọng số của các lớp được thay thế
bằng mặt nạ của chúng, do đó (C) đầu ra chỉ chứa giá trị null
nếu một bộ lọc bị cắt tỉa. (D) Chuẩn hóa và bias được
loại bỏ để giữ giá trị null là null và (E) các hàm kích hoạt
được loại bỏ để không thêm thêm giá trị. Đầu ra cuối cùng (F) cho phép
suy ra bộ lọc nào bị cắt tỉa.
Bước đầu tiên của phương pháp của chúng tôi là đảm bảo một mạng nhất định
mạnh mẽ với việc cắt tỉa. Thật vậy, các mạng như ResNets hoặc
HRNets chứa các phép toán được áp dụng cho đầu ra của
nhiều lớp. Trong những trường hợp như vậy, các tensor liên quan phải có
cùng chiều, điều này có thể không còn đúng nữa sau
việc cắt tỉa. Trong trường hợp ResNets và HRNets, tất cả các phép toán
thuộc loại này đều là phép cộng hai tensor, chẳng hạn như những phép toán ở
cuối mỗi kết nối dư. Điều này có nghĩa là chúng ta có thể
giải quyết vấn đề này bằng cách thay thế phép cộng bằng một toán tử
tổng quát có thể xử lý các bộ lọc bị thiếu trong bất kỳ đầu vào nào.
Vì mục đích này, chúng tôi thay thế phép cộng bằng một phép toán chỉ mục-
cộng mới, với a và b là các tensor cần cộng, chứa
tương ứng na và nb kênh, ia và ib là hai danh sách
chỉ số và tensor đầu ra c, chứa nc kênh,
được định nghĩa trong Phương trình (1):
∀k∈[1;nc],ck={
aia
k,nếu ia
k∈[1;na]
∅,ngược lại(1)
+{
bib
k,nếu ib
k∈[1;nb]
∅, ngược lại
Nếu na=nb, ia= [1,2,...,na] và ib=[1,2,...,nb],
phép toán chỉ mục-cộng này hoàn toàn tương đương với phép
cộng theo từng phần tử. Được tham số hóa đúng cách bằng ia và ib
thích hợp, phép toán này cho phép tận dụng bất kỳ loại cắt tỉa bộ lọc nào. Tuy nhiên, cần thiết phải tìm ia và ib đúng và
chúng tôi cung cấp một giải pháp trong Phần III-D. Hình 1 minh họa cách
giải pháp của chúng tôi liên quan đến các vấn đề được đề cập trong Phần III-A
và cung cấp một cách đơn giản để xem cách nó có thể hoạt động như một
hỗn hợp của phép cộng và nối.
C. Điều chỉnh Tự động Mạng
Khi mạng đã được chuẩn bị cho việc cắt tỉa bằng việc giới thiệu phép toán chỉ mục-cộng mới này để phù hợp với bất kỳ
phân phối độ thưa nào được gây ra bởi việc cắt tỉa, bước tiếp theo
của phương pháp là tự động xác định tất cả các phụ thuộc
giữa bộ lọc, kernel, bias hoặc bất kỳ loại trọng số nào trong
mạng. Tóm lại, cần thiết phải tìm kiếm tất cả các phần
của mạng bị ngắt kết nối khi loại bỏ bộ lọc.
Để xác định tất cả các tham số có đóng góp trong chức năng của mạng
là null, người ta có thể sử dụng gradient của nó trên, ví dụ, một
mini-batch từ dữ liệu huấn luyện. Thật vậy, với điều kiện mini-
batch này là một xấp xỉ thỏa đáng của miền định nghĩa của mạng, một gradient null có nghĩa là chức năng của mạng
là null tương đối với các trọng số liên quan, hoặc ít nhất là hằng số
trong trường hợp bias. Tuy nhiên, đối với trường hợp sử dụng của chúng ta, điều này không đủ: nó không chỉ không cho phép loại bỏ các bias
ngắt kết nối vẫn tạo ra đầu ra hằng số bằng cách nào đó đóng góp vào
chức năng, mà còn có thể xác định một số trọng số riêng lẻ
như được cắt tỉa theo cách không có cấu trúc, trong khi không thể
tận dụng chúng.
Đây là lý do tại sao chúng tôi thay vào đó hoạt động trên một bản trừu tượng kiến trúc
của mạng, đó là một bản sao của nó đã nhận được ba
sửa đổi được minh họa trong Hình 2:
•Bias của nó được loại bỏ để ngăn chúng thêm một
đầu ra hằng số làm cho một số trọng số ngắt kết nối/vô dụng
ở phía sau có gradient khác null.
•Các hàm kích hoạt và các phép toán phi tuyến khác
như chuẩn hóa được loại bỏ, để một đầu vào khác null
của một lớp không thể tạo ra đầu ra và gradient null.
•Giá trị trọng số của nó được thay thế bằng giá trị của
mặt nạ, được tạo từ số không hoặc số một, để khi được cung cấp với
một đầu vào chứa đầy các giá trị khác null cùng dấu, đầu ra không thể chứa giá trị null nếu không phải vì
trọng số null, bị che đi.
Do những sửa đổi này, một đầu vào duy nhất chứa đầy
các giá trị khác null cùng dấu là đủ để xác định tất cả

--- TRANG 4 ---
trọng số ngắt kết nối. Thật vậy, mạng này hoạt động như một
hàm hoàn toàn tuyến tính và dương và bất kỳ gradient null nào trong
các tham số của nó chỉ có thể do một hàm null có thể
được loại bỏ. Các trọng số, được xác định là ngắt kết nối trong mạng sao chép này, sau đó được loại bỏ khỏi mạng gốc.
D. Chỉ mục Tự động
Để suy ra tự động ia và ib đúng được định nghĩa trong
Phần III-B, chúng tôi thêm một sửa đổi khác vào mạng sao chép
được mô tả trong Phần III-C: chúng tôi áp dụng một tích chập đồng nhất
cho hai tensor trước khi cộng chúng lại với nhau. Tích chập đồng nhất
này có trọng số có hình dạng n×n×1×1 (với n là
số kênh trong tensor đầu vào) có giá trị bằng
của một ma trận đồng nhất.
Gradient của trọng số của tích chập đồng nhất này
cho phép suy ra danh sách chỉ số tương ứng. Thật vậy, khi
các hàng và cột null của trọng số của nó được loại bỏ, chiều đầu ra là như nhau đối với cả hai tensor cần cộng
trong khi chiều đầu vào khớp với chiều của tensor đầu vào
sau khi cắt tỉa. Hệ số null và khác null còn lại
cho phép suy ra cách ánh xạ các kênh đầu vào và đầu ra.
E. Tóm tắt Phương pháp
Đây là tất cả các bước cần theo để áp dụng phương pháp của chúng tôi:
Thuật toán 1 Tóm tắt Phương pháp
1:huấn luyện mạng N
2:tạo mặt nạ cắt tỉa m che đi bộ lọc
3:tạo bản sao N′ của mạng
4:loại bỏ tất cả bias b khỏi N′
5:loại bỏ tất cả hàm kích hoạt và chuẩn hóa khỏi N′
6:thay thế trọng số w của N′ bằng m
7:chèn các tích chập đồng nhất khi cần thiết trong N′
8:tạo tensor đầu vào x, có kích thước thích hợp, chứa đầy
số một và chạy N′(x)
9:tính dN′
dw(x)
10:tạo mặt nạ cắt tỉa mới m′ che đi tất cả
trọng số có gradient null trong N′
11:áp dụng m′ cho N và che đi bias có trọng số bị
cắt tỉa
12:suy ra từ mặt nạ của các tích chập đồng nhất ia và ib đúng để thay thế phép cộng bằng phép toán chỉ mục-cộng
khi cần thiết
Phương pháp, được tóm tắt trong Thuật toán 1, giải quyết tất cả các vấn đề
được trình bày trong Phần III-A. Nó cho phép cắt tỉa một mạng và
sau đó tạo ra tương đương gần nhất có chiều
nhất quán và có thể được tận dụng trên phần cứng. Vì phương pháp của chúng tôi không chỉ loại bỏ trọng số có đóng góp null mà còn
bias có gradient hằng số, chức năng của mạng
không được bảo toàn. Tuy nhiên, tác động đến độ chính xác là không đáng kể
và được chi tiết trong các thí nghiệm của chúng tôi trong Phần IV-B.
IV. THÍ NGHIỆM
Trong các thí nghiệm của chúng tôi, trước tiên chúng tôi sẽ chi tiết tác động của
phương pháp của chúng tôi đối với cả độ chính xác của mạng và đánh giá tỷ lệ nén của nó. Sau đó chúng tôi sẽ chứng minh cách các mạng, có loại độ thưa thường
ngăn chặn việc chạy suy luận, có thể được tận dụng hiệu quả
trên phần cứng có tài nguyên hạn chế. Mã nguồn của chúng tôi có sẵn
tại: https://github.com/HugoTessier-lab/Neural-Network-
Shrinking.git
A. Điều kiện Huấn luyện
a) ImageNet: Chúng tôi huấn luyện ResNet-50 [6] trên tập dữ liệu phân loại ảnh ImageNet
ILSVRC2012 [17] trong 90 epoch
với batch-size 170 và tỷ lệ học 0.01 giảm 10 lần mỗi 30 epoch. Chúng tôi sử dụng trình tối ưu SGD với weight
decay được đặt ở 1·10−4 và momentum được đặt ở 0.9.
b) Cityscapes: Chúng tôi huấn luyện mạng HRNet-48 [18]
trên tập dữ liệu phân đoạn ngữ nghĩa Cityscapes [2] trong 200
epoch với batch size 10 và tỷ lệ học 0.01
giảm bằng (1−epoch hiện tại
epoch)2 tại mỗi epoch. Chúng tôi sử dụng
loss RMI [22] và trình tối ưu SGD với weight decay được đặt ở
5·10−4 và momentum được đặt ở 0.9. Trong quá trình huấn luyện, ảnh
được cắt và thay đổi kích thước ngẫu nhiên, với tỷ lệ [0.5,2],
thành 3×512×1024. Tăng cường dữ liệu bao gồm lật ngẫu nhiên,
làm mờ Gaussian ngẫu nhiên và jittering màu.
c) Cắt tỉa: Chúng tôi cắt tỉa mạng theo phương pháp
của Liu et al. [12]: việc cắt tỉa được chia thành ba lần lặp, với
tỷ lệ bộ lọc được loại bỏ tăng tuyến tính cho đến khi
đạt được tỷ lệ cắt tỉa cuối cùng. Tại mỗi lần lặp, bộ lọc được
che đi tùy thuộc vào độ lớn của trọng số của
lớp batch-normalization của chúng. Sau mỗi lần lặp, ResNet-
50 được tinh chỉnh trong 10 epoch và HRNet-48 trong 20
epoch. Phương pháp của Liu et al. [12] cũng ngụ ý việc phạt
trọng số của các lớp batch-normalization với chuẩn smooth-L1,
với hệ số quan trọng λ= 10−5 cho ResNet-50 và
λ= 10−6 cho HRNet-48.
B. Tác động đến Độ chính xác và Tỷ lệ Nén
0 50 100020406080
Tỷ lệ Cắt tỉa (%)Độ chính xác Top-1 (%)
0 50 100020406080
Tỷ lệ Cắt tỉa (%)mIoU (%)
Trước Sau
Hình 3: Đối với ResNet-50 trên ImageNet (trái) hoặc HRNet-48 trên
Cityscapes (phải): độ chính xác tùy thuộc vào tỷ lệ cắt tỉa, hoặc
theo tỷ lệ bộ lọc đã cắt tỉa (xanh dương) hoặc tham số còn lại
sau khi áp dụng phương pháp của chúng tôi (đỏ).
Trong các thí nghiệm của chúng tôi, chúng tôi báo cáo hầu như không có sự khác biệt về
độ chính xác trước và sau khi áp dụng phương pháp của chúng tôi, như có thể
thấy trong Hình 3. Điều này ngụ ý rằng các tham số được loại bỏ bởi

--- TRANG 5 ---
phương pháp của chúng tôi, không có đóng góp null cho chức năng, chẳng hạn như các bias còn lại được đề cập trong Phần III-C,
có thể đã có tác động không đáng kể đến độ chính xác của mạng.
Các điểm ngoại lệ duy nhất là các điểm mà độ chính xác đã bị
giảm nghiêm trọng, ví dụ độ chính xác của ResNet-50 được cắt tỉa ở
60% giảm từ 66.05% xuống 63.478%, trong khi đường cơ sở
ở 75.7%.
Trong Hình 3, chúng tôi cũng cho thấy sự đánh đổi giữa độ chính xác
và hai loại tỷ lệ cắt tỉa: một được định nghĩa là tỷ lệ
bộ lọc được loại bỏ, đây là tiêu chí mục tiêu phổ biến trong
tài liệu, và một được định nghĩa là số lượng chính xác các tham số còn lại trong mạng khi phương pháp của chúng tôi đã được áp dụng.
Chúng ta thấy rằng việc sử dụng phần trăm bộ lọc được loại bỏ không
trung thực với tỷ lệ nén thực tế của mạng. Sự đánh đổi thực tế có lợi hơn khi phương pháp của chúng tôi đã được áp dụng để cả làm sạch mạng khỏi trọng số vô dụng
và có được ước tính trung thực về tất cả trọng số đã loại bỏ.
100101100101
Nén Bộ lọc. Nén Tham số.
100101100101
Nén Bộ lọc. Nén Tham số.
Hình 4: Đối với ResNet-50 trên ImageNet (trái) hoặc HRNet-48 trên
Cityscapes (phải): mối quan hệ giữa tỷ lệ nén ước tính
theo bộ lọc đã cắt tỉa (trục x) và tham số còn lại
sau khi giảm mạng bằng phương pháp của chúng tôi (trục y).
Trong Hình 4, chúng tôi so sánh tỷ lệ nén (tức là
100%
100%−tỷ lệ cắt tỉa%) theo bộ lọc được loại bỏ hoặc tham số được loại bỏ, tức là trước và sau phương pháp của chúng tôi. Mối quan hệ
giữa hai đo lường dường như phụ thuộc vào kiến trúc
liên quan và chúng tôi mong đợi nó phụ thuộc vào tiêu chí cắt tỉa cũng.
C. Tác động trên Phần cứng
Để đo thời gian suy luận và tiêu thụ năng lượng
của các mạng đã cắt tỉa trên NVIDIA Jetson AGX Xavier ở
chế độ "30W All", trước tiên chúng tôi chuyển đổi mạng của chúng tôi sang ONNX,
đây là định dạng có thể được xử lý bởi nhiều framework
trên hầu hết phần cứng. Các phép toán chỉ mục-cộng được
triển khai bằng các toán tử ScatterND và transpose. ScatterND cho phép hoạt động trên các lát trong tensor và hoán vị
cho phép hoạt động cụ thể trên kênh, trong khi Scatter là
từng phần tử và yêu cầu lưu trữ một mảng chỉ số cồng kềnh.
Trước phép cộng, cả hai tensor cần được phân tán
vào một tensor tạm thời, được khởi tạo động. Chúng tôi
sử dụng JetPack SDK 5.0, với CUDA 11.4.14, cuDNN 8.3.2,
TensorRT 8.4.0 EA và ONNX Runtime 1.12.0. Tiêu thụ năng lượng
được cung cấp bằng tiện ích tegrastats. Suy luận
trên ResNet-50 được chạy với đầu vào có kích thước (1×3×224×224)0 50 100050100
Tỷ lệ Cắt tỉa (%)Năng lượng (mJ)
0 50 100051015
Tỷ lệ Cắt tỉa (%)Thời gian Suy luận (ms)
(a) ResNet-50
0 50 1000123
Tỷ lệ Cắt tỉa (%)Năng lượng (J)
0 50 1000100200300400
Tỷ lệ Cắt tỉa (%)Thời gian Suy luận (ms)
(b) HRNet-48
Hình 5: Tiêu thụ năng lượng và thời gian suy luận của ResNet-
50 và HRNet-48, tùy thuộc vào tỷ lệ cắt tỉa theo
tham số, trên NVIDIA Jetson AGX Xavier ở chế độ "30W
All", sử dụng JetPack SDK 5.0 và ONNX Runtime
1.12.0 chạy với provider thực thi TensorRT. Kết quả được tính trung bình trên 10k lần suy luận với đầu vào có kích thước
(1×3×224×224) sau 1k lần chạy khởi động cho ResNet-
50 và 1k lần suy luận với đầu vào có kích thước (1×3×512×1024)
sau 100 lần chạy khởi động cho HRNet-48.
và HRNet-48 với một có kích thước (1×3×512×1024). ONNX
Runtime được sử dụng với provider thực thi TensorRT, hóa ra
là provider cho thời gian suy luận tốt nhất.
Hình 5 cung cấp kết quả cho ResNet-50 trên ImageNet và
HRNet-48 trên Cityscapes. Cả hai đều cho thấy xu hướng tương tự: lúc đầu, chi phí thêm của các phép toán chỉ mục-cộng gây tổn hại
đến hiệu quả của việc cắt tỉa, nhưng sau bước nhảy ban đầu đó,
chi phí của mạng, dù về tiêu thụ năng lượng
hay thời gian suy luận, đều giảm đáng kể. Điều này cho thấy rằng,
mặc dù việc triển khai tốt hơn các phép toán chỉ mục-cộng
sẽ có lợi, giải pháp hiện tại của chúng tôi đủ
để việc cắt tỉa có cấu trúc tự do và không bị ràng buộc có hiệu quả về chi phí. Do đó, chúng ta có thể nói rằng có thể tận dụng
hiệu quả bất kỳ loại cắt tỉa bộ lọc nào trong ngay cả các mạng nơ-ron tích chập sâu phức tạp.
V. THẢO LUẬN
Ba quan sát có thể rút ra từ các thí nghiệm của chúng tôi: 1)
phương pháp của chúng tôi cho phép đo lường đáng tin cậy hơn số lượng
tham số còn lại trong mạng, như có thể thấy trong
Hình 3, 2) mối quan hệ giữa tỷ lệ cắt tỉa chính xác này và
thời gian suy luận hoặc tiêu thụ năng lượng là phi tuyến và 3) chi phí được giới thiệu bởi các toán tử tùy chỉnh của chúng tôi không đáng kể và

--- TRANG 6 ---
làm cho các mạng ít được cắt tỉa nhất tốn kém hơn so với các mạng không
được cắt tỉa, như có thể thấy trong Hình 5.
Hai quan sát đầu tiên cho thấy rằng phương pháp của chúng tôi là một công cụ hữu ích
để nghiên cứu tốt hơn hiệu quả của việc cắt tỉa bộ lọc không bị ràng buộc. Thật vậy, nó tạo ra một mạng trong đó phần lớn
các tham số còn lại được đảm bảo đóng góp vào
chức năng, với ngoại lệ nhỏ của một số trọng số riêng lẻ
có thể không hoạt động do tình cờ. Do đó, giờ đây có thể
đo trực tiếp sự đánh đổi độ chính xác-năng lượng hoặc độ chính xác-
độ trễ, cung cấp cái nhìn sâu sắc có liên quan hơn về
tác động của việc cắt tỉa trên phần cứng so với sự đánh đổi độ chính xác-tham số lý thuyết hơn. Vì đây không phải là trọng tâm của
bài viết này, chúng tôi đã không cung cấp phân tích như vậy và đã không
chọn phương pháp cắt tỉa cho hiệu suất tuyệt đối tốt nhất có thể.
Điều này sẽ là trọng tâm của các đóng góp tương lai.
Khả năng cung cấp tỷ lệ nén trung thực hơn so với
tỷ lệ ngây thơ của các bộ lọc được loại bỏ cũng cho phép kiểm soát tốt hơn
sự tăng trưởng của tỷ lệ cắt tỉa giữa các lần lặp cắt tỉa. Điều này có khả năng giúp cải thiện hiệu suất và tránh loại bỏ
toàn bộ các lớp do tình cờ, được gọi là sự sụp đổ lớp [19].
Về quan sát cuối cùng, việc tìm ra triển khai tốt nhất của các toán tử tùy chỉnh, cần thiết để chạy các mạng
đã cắt tỉa, rõ ràng đòi hỏi điều tra thêm. Sử dụng trtexec,
chúng tôi đã làm profiling các toán tử của HRNet-48, với
10% bộ lọc được cắt tỉa và 6.26% tham số được loại bỏ,
đây là HRNet-48 có thời gian suy luận cao nhất. Hóa ra
"Foreign Nodes" được tạo bởi TensorRT,
chứa ScatterND chúng tôi sử dụng cho các phép toán chỉ mục-cộng
của mình, chịu trách nhiệm cho 14.8% tổng thời gian suy luận. Khi trừ chi phí của các node này khỏi
tổng thời gian trung bình của mạng là 369.8ms theo trtexec,
thời gian suy luận còn lại là 314.3ms, thực tế
thấp hơn so với mạng không được cắt tỉa, là
318.7ms. Điều này có nghĩa là nếu việc triển khai tối ưu các
toán tử cho phép chi phí của chúng không đáng kể, nó sẽ làm cho
việc cắt tỉa có lợi hơn nhiều, ngay cả ở tỷ lệ cắt tỉa thấp.
VI. KẾT LUẬN
Chúng tôi đã đề xuất một cách hiệu quả và tổng quát để tận dụng bất kỳ loại cắt tỉa bộ lọc nào trong mạng nơ-ron tích chập sâu. Thật vậy, mặc dù việc loại bỏ bộ lọc trong một mạng
có thể kích hoạt một loạt vấn đề nhất định thậm chí có thể ngăn
chạy suy luận của nó, giải pháp của chúng tôi có thể giải quyết chúng
và tạo ra các mạng đã cắt tỉa chức năng có thể chạy
hiệu quả trên phần cứng. Các thí nghiệm của chúng tôi, mặc dù chúng
cho thấy rằng việc triển khai ONNX hiện tại của chúng tôi có chi phí
không đáng kể, chứng minh rằng việc cắt tỉa bộ lọc không bị ràng buộc
có thể hiệu quả về chi phí.
TÀI LIỆU THAM KHẢO
[1] Chun-Fu Chen, Gwo Giun Lee, Vincent Sritapan, và Ching-Yung Lin.
Deep convolutional neural network on ios mobile devices. Trong 2016 IEEE
International Workshop on Signal Processing Systems (SiPS), trang
130–135. IEEE, 2016.
[2] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld,
Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, và
Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. Trong Proceedings of the IEEE conference on computer vision
and pattern recognition, trang 3213–3223, 2016.
[3] Matthieu Courbariaux, Yoshua Bengio, và Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during
propagations. Advances in neural information processing systems, 28,
2015.
[4] Song Han, Huizi Mao, và William J Dally. Deep compression:
Compressing deep neural networks with pruning, trained quantization
and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
[5] Song Han, Jeff Pool, John Tran, và William Dally. Learning both
weights and connections for efficient neural network. Advances in neural
information processing systems, 28, 2015.
[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep
residual learning for image recognition. Trong Proceedings of the IEEE
conference on computer vision and pattern recognition, trang 770–778,
2016.
[7] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, và Yi Yang. Soft
filter pruning for accelerating deep convolutional neural networks. arXiv
preprint arXiv:1808.06866, 2018.
[8] Yihui He, Xiangyu Zhang, và Jian Sun. Channel pruning for accelerating very deep neural networks. Trong Proceedings of the IEEE international
conference on computer vision, trang 1389–1397, 2017.
[9] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, và cộng sự. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.
[10] Yann LeCun, John Denker, và Sara Solla. Optimal brain damage.
Advances in neural information processing systems, 2, 1989.
[11] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint
arXiv:1608.08710, 2016.
[12] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan,
và Changshui Zhang. Learning efficient convolutional networks
through network slimming. Trong Proceedings of the IEEE international
conference on computer vision, trang 2736–2744, 2017.
[13] Xiaolong Ma, Sheng Lin, Shaokai Ye, Zhezhi He, Linfeng Zhang, Geng
Yuan, Sia Huat Tan, Zhengang Li, Deliang Fan, Xuehai Qian, và cộng sự. Nonstructured dnn weight pruning–is it beneficial in any platform? IEEE
Transactions on Neural Networks and Learning Systems, 2021.
[14] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H
Nguyen, Madeleine Gibescu, và Antonio Liotta. Scalable training of
artificial neural networks with adaptive sparse connectivity inspired by
network science. Nature communications, 9(1):1–12, 2018.
[15] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, và Jan
Kautz. Pruning convolutional neural networks for resource efficient
inference. arXiv preprint arXiv:1611.06440, 2016.
[16] Olaf Ronneberger, Philipp Fischer, và Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. Trong International
Conference on Medical image computing and computer-assisted intervention, trang 234–241. Springer, 2015.
[17] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev
Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,
Michael Bernstein, Alexander C. Berg, và Li Fei-Fei. ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer
Vision (IJCV), 115(3):211–252, 2015.
[18] Ke Sun, Yang Zhao, Borui Jiang, Tianheng Cheng, Bin Xiao, Dong Liu,
Yadong Mu, Xinggang Wang, Wenyu Liu, và Jingdong Wang. Highresolution representations for labeling pixels and regions.arXiv preprint
arXiv:1904.04514, 2019.
[19] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, và Surya Ganguli.
Pruning neural networks without any data by iteratively conserving
synaptic flow. Advances in Neural Information Processing Systems,
33:6377–6389, 2020.
[20] Hugo Tessier, Vincent Gripon, Mathieu Léonardon, Matthieu Arzel,
Thomas Hannagan, và David Bertrand. Rethinking weight decay for
efficient neural network pruning. Journal of Imaging, 8(3):64, 2022.
[21] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, và Hai Li.
Learning structured sparsity in deep neural networks. Advances in neural
information processing systems, 29, 2016.
[22] Shuai Zhao, Yang Wang, Zheng Yang, và Deng Cai. Region mutual
information loss for semantic segmentation. Advances in Neural Information Processing Systems, 32, 2019.
