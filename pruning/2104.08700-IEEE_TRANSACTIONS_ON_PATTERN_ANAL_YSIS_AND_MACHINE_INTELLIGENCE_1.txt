# 2104.08700.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2104.08700.pdf
# File size: 2017851 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 1
Lottery Jackpots Exist in Pre-trained Models
Yuxin Zhang, Mingbao Lin, Yunshan Zhong, Fei Chao, Member, IEEE , Rongrong Ji, Senior Member, IEEE
Abstract —Network pruning is an effective approach to reduce network complexity with acceptable performance compromise. Existing
studies achieve the sparsity of neural networks via time-consuming weight training or complex searching on networks with expanded
width, which greatly limits the applications of network pruning. In this paper, we show that high-performing and sparse sub-networks
without the involvement of weight training, termed “lottery jackpots”, exist in pre-trained models with unexpanded width. Our presented
lottery jackpots are traceable through empirical and theoretical outcomes. For example, we obtain a lottery jackpot that has only 10%
parameters and still reaches the performance of the original dense VGGNet-19 without any modifications on the pre-trained weights
on CIFAR-10. Furthermore, we improve the efficiency for searching lottery jackpots from two perspectives. Firstly, we observe that the
sparse masks derived from many existing pruning criteria have a high overlap with the searched mask of our lottery jackpot, among
which, the magnitude-based pruning results in the most similar mask with ours. In compliance with this insight, we initialize our sparse
mask using the magnitude-based pruning, resulting in at least 3 ×cost reduction on the lottery jackpot searching while achieving
comparable or even better performance. Secondly, we conduct an in-depth analysis of the searching process for lottery jackpots. Our
theoretical result suggests that the decrease in training loss during weight searching can be disturbed by the dependency between
weights in modern networks. To mitigate this, we propose a novel short restriction method to restrict change of masks that may have
potential negative impacts on the training loss, which leads to a faster convergence and reduced oscillation for searching lottery jackpots.
Consequently, our searched lottery jackpot removes 90% weights in ResNet-50, while it easily obtains more than 70% top-1 accuracy
using only 5 searching epochs on ImageNet. Our code is available at https://github.com/zyxxmu/lottery-jackpots.
Index Terms —Convolutional neural networks, Network pruning, Lottery ticket hypothesis.
✦
1 I NTRODUCTION
EVER-INCREASING model complexity has greatly limited
the real-world applications of deep neural networks
(DNNs) on edge devices. Various methods have been pro-
posed to mitigate this obstacle by the deep learning com-
munity. Generally, existing research can be divided into
network pruning [1], [2], parameter quantization [3], [4],
[5], low-rank decomposition [6], [7] and knowledge distil-
lation [8], [9]. Among these techniques, network pruning
has been known as one of the leading approaches with no-
table reductions on the network complexity and acceptable
performance degradation [10], [11], [12].
Given a large-scale neural network, network pruning
removes a portion of network connections to obtain a
sparse sub-network. Extensive pruning algorithms have
been proposed over the past few years [13], [14], [15].
Typical approaches devise various importance criteria to
prune weights on the basis of pre-trained models, which
is reasonable since the pre-trained models are mostly visible
on the Internet, or available from the client [1], [16], [17].
Other studies conduct pruning from scratch by imposing a
sparsity regularization upon the network loss, or directly
pruning from randomly initialized weights [18], [19], [20].
Although progress has been made to reduce the size of
•Y. Zhang, Y. Zhong, F. Chao, and R. Ji (Corresponding Author) are
with the Key Laboratory of Multimedia Trusted Perception and Efficient
Computing, Ministry of Education of China, Xiamen University, Xiamen
361005, China, and also with School of Informatics, Xiamen University,
Xiamen 361005, China (e-mail: rrji@xmu.edu.cn).
•M. Lin is with Youtu Laboratory, Tencent, Shanghai 200233, China.
•R. Ji is also with Institute of Artificial Intelligence, Xiamen University,
Xiamen 361005, China.
Manuscript received April 19, 2005; revised August 26, 2015.network parameters with little degradation in accuracy, ex-
isting methods still require a time-consuming weight train-
ing process to recover the performance of pruned models
as shown in Fig. 1. For instance, when pruning the well-
known ResNet-50 [21] on ImageNet [22], most methods
require over 100 epochs to train the pruned model [18],
[19], [23]. Thus, the compressed models come at the cost of
expensive weight training, which greatly restricts practical
applications of existing researches.
The lottery ticket hypothesis [26] reveals that a randomly
initialized network contains lottery ticket sub-networks that
can reach good performance after appropriate weight train-
ing. In light of this, more recent studies further discover
that these lottery tickets emerge even without the necessity
of weight training [27], [28], [29]. However, a complex
searching algorithm has to be conducted upon randomly
initialized weights, the cost of which is even higher than
training the weights since the width of the original network
is usually exponentially expanded to ensure the finding of
high-performing lottery tickets. Moreover, the performance
of such sub-networks still falls far behind existing weight-
training pruning methods [18], [19], [23].
In this paper, we innovatively reveal that high-
performing sub-networks can be located in pre-trained
models without the involvement of weight training from
empirical and theoretical perspectives. We term these sub-
networks as lottery jackpots in this paper. First, different
from existing works [27], [28], [29] which search for the
lottery tickets in a randomly initialized network with width
expansion, our lottery jackpots are built on top of an un-
expanded pre-trained model. For example, a lottery jackpot
can be found in the pre-trained VGGNet-19 [30] on CIFAR-
10 [31], which has only 10% parameters of the originalarXiv:2104.08700v7  [cs.CV]  2 Sep 2023

--- PAGE 2 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2
0 10 20 30 40 50 60 70 80 90 100
Training/Search Cost (Epochs)30354045505560657075T op-1 Accuracy (%)
Ours
RigL
SET
SNIP
Fig. 1: Training/Search cost v.s.top-1 accuracy of ResNet-
50 [21] with a sparse rate of 90% on ImageNet [22]. Search
epoch differs from training epoch in that it only trains the
mask for indicating the removal or preserve of weights,
without modifying the weight value. Our method can
quickly find the high-performing sparse networks (lottery
jackpots) without modifications on the trained weights,
while existing methods such as SET [24], SNIP [25] and
RigL [19] conduct pruning via a time-consuming weight
training process to recover the performance. Each dot in the
figure indicates a complete training with cosine annealing.
network while still reaching the performance of the full
model without any modifications on the trained weights.
Nevertheless, off-the-shelf edge-popup algorithm [27] that
we leverage to find lottery jackpots is time-consuming. It
takes almost the same computation cost compared with
existing weight training methods [25], [32], which heavily
barricades the application value of lottery jackpots.
To alleviate the above problem, we further propose to
improve the searching efficiency of lottery jackpots from
two perspectives, unfolded as mask initialization and mask
search. Toward the first goal, we look into existing weight-
training based pruning criteria [1], [25], [32], and then ob-
serve a high overlap between the sparsity masks from these
existing pruning criteria and the searched mask from our
lottery jackpot. Among them, the magnitude-based pruning
criterion results in the most similar sparsity pattern. This
inspires us to initialize our mask with magnitude pruning
as a warm-up for searching our lottery jackpots. As a result,
at least 3×reductions on the searching cost are observed
when compared to the existing weight searching methods
on the randomly initialized networks [27], [28] or weight
training methods [18], [19], [24].
Next, we study how to boost the efficiency of the search-
ing process for lottery jackpots. As the pre-trained weights
are fixed during the searching phase, the only opportunity
to decrease training loss falls into pruning weights that are
preserved in the previous searching iterations and reviving
back the same number of pruned weights, which we call
weight swapping in this paper. Then, we mathematically
prove that the loss drop carried by such weight swapping
in the edge-popup algorithm is bounded by a distortion
error item with regard to the dependency between weightsin modern DNNs, limiting the searching efficiency of lot-
tery jackpots. To cope with this drawback, we present
an intuitive yet effective short restriction popup, which
greedily prevents weight swapping that receives little or
even negative influence for minimizing the training loss.
Consequently, faster convergence and less oscillation can be
reached for searching lottery jackpots.
Extensive experiments have demonstrated that our pro-
posed short restriction popup, termed as SR-popup, can
efficiently locate high-performing lottery jackpots in many
representative DNNs including ResNet [21], MobileNet-
V1 [33], etc. For instance, SR-popup successfully searches a
lottery jackpot that removes 90% weights of ResNet-50 [21]
while reaching the top-1 accuracy of 70% using only 5
searching epochs on ImageNet and achieves comparable
performance with state-of-the-art methods using only 30
searching epochs, as shown in Fig. 1.
Overall, our contributions are summarized as:
•We find that pre-trained models contain high-
performing sub-networks without the necessity of
weight training. Moreover, our searching for these
sub-networks is built on the top of no expansion of
the network width.
•We discover that existing weight-training based
pruning criteria often generate a similar sparsity
mask with our searched mask. Inspired by this dis-
covery, we propose to use magnitude-based sparsity
mask as a warm-up for the weight searching, leading
to a more efficient searching for our lottery jackpots.
•With theoretical guarantee, we propose a novel short
restriction popup, which adaptively preserves mask
changes that earn largest expected loss drops, such
that the searching instability of a previous method
edge-popup is effectively relieved.
•Extensive experiments demonstrate the effectiveness
and efficiency of our proposed approach for network
pruning. The high-performing sub-networks can be
expeditiously found in pre-trained models with at
least 3×reductions on the computation cost in com-
parison with the existing methods.
2 R ELATED WORK
Neural Network Pruning. Pruning neural networks has
been demonstrated to be an effective approach for com-
pressing large models in the past decades [16], [34], [35].
Earlier techniques usually implement pruning by designing
various importance criteria upon pre-trained models [1],
[13], [16]. For instance, Han et al. [1] considered the mag-
nitude of weights and Molchanov et al. [36] treated Taylor
expansion that approximates changes in the loss function as
the pruning principle.
Recently, a plurality of studies has questioned the role
of pre-trained models by devising well-performing pruning
algorithms without the dependency on the pre-trained mod-
els. For example, Sparse Evolutionary Training (SET) [24]
cultivates sparse weights throughout training in a prune-
redistribute-regrowth manner while training the models
from scratch. Kusupati et al. [23] designed Soft Threshold
Reparameterization (STR) to learn non-uniform sparsity
budget across layers. Evci et al. [19] further proposed RigL,

--- PAGE 3 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 3
which uses weight magnitude and gradient information to
improve the sparse networks optimization. Other works
directly prune a randomly initialized model by considering
the connection sensitivity as an importance measure [25] or
maximizing the gradient of the pruned sub-network [32].
Network pruning can also be viewed as finding binary
masks that indicate the removal of preserve of weights. For
instance, Guo et al. [37] proposed to iteratively update the
binary masks and train the network parameters to avoid
incorrect pruning. Such binary mask can also be learned
during training with additional gate variables [38] or aux-
iliary parameters [39]. Savarese et al. [40] further proposed
Continuous Sparsification to learn the binary masks using
a differentiable approximation to ℓ0-regularization penalty
for the parameter count of sparse networks.
There are also multiple studies that remove the entire
neurons or convolution filters to achieve a high-level accel-
eration through parallelization [41], [42], [43], [44], [45], [46],
[47], [48]. However, models compressed by these methods
often suffer severe performance degradation, i.e., classifi-
cation accuracy drops at a high pruning rate, and thus
the complexity reduction is often very limited. In addition,
some recent studies [49], [50] presented hardware-friendly
weight pruning methods to enable practical complexity re-
duction on off-the-shelf platforms. In this paper, we focus on
weight pruning that removes individual weights to achieve
a high sparsity level.
Lottery Ticket Hypothesis. The lottery ticket hypothesis
was originally proposed in [26] which reveals the existence
of sub-networks ( a.k.a. ,“winning tickets”) in a randomly-
initialized neural network that can match the test accuracy
of the original network when trained in isolation. Zhou et
al. [28] further proved the potential existence of lottery
tickets that can achieve good performance without train-
ing the weights. Inspired by this progress, Ramanujan et
al.[27] designed an edge-popup algorithm to search for
these sub-networks within the randomly initialized weights.
They found a sub-network of the Wide ResNet-50 [51] that
is lightly smaller than, but matches the performance of
ResNet-34 [21] on ImageNet. Later, such sub-networks are
further confirmed by [29] and [52].
Unfortunately, finding the sub-networks without weight
training strongly relies on the original networks to be
sufficiently over-parameterized. To this end, the width of
randomly initialized networks is usually exponentially ex-
panded, which however increases the searching complex-
ity [14], [29]. As a result, the cost of searching these sub-
networks is even higher than weight training. Moreover,
the searched sub-networks are still redundant with unsat-
isfying accuracy, far away from the purpose of network
pruning [27]. Thus, it is of great urgency to find out the high-
performing lottery tickets with a small searching complexity.
3 M ETHODOLOGY
3.1 Preliminary
Let the weight vector of a full network be w∈Rkwhere
kis the weight size. Technically, network pruning can be
viewed as applying a mask m∈ {0,1}konwto indicate
whether to preserve or remove some of the weights. Givena desired global sparsity p, the conventional objective of
network pruning can be formulated as:
min
w,mL(m⊙w;D),s.t.1−∥m∥0
k≥p, (1)
where Dis the observed dataset, Lrepresents the loss
function, ∥·∥0means the standard ℓ0-norm, and ⊙denotes
the element-wise multiplication.
As can be seen from Eq. (1), most previous methods [18],
[19], [24] pursue sparse DNNs by training the given weight
vector wwhile learning the mask m. Though training
the weights increases the performance of pursued sparse
networks, its heavy time-consumption becomes a severe
bottleneck for practical deployments as discussed before.
Inspired by the lottery ticket hypothesis [26] which indi-
cates a randomly initialized network contains sub-networks
(lottery tickets) that can reach considerable accuracy, recent
studies [27], [28], [29] proposed to search for these lottery
tickets upon the initialized weights without the necessity
of weight training. Basically, their learning objective can be
given in the following:
min
mL(m⊙w;D),s.t.1−∥m∥0
k≥p. (2)
The main difference between Eq. (1) and Eq. (2) is that
thewis regarded as a constant vector to get rid of the
dependency on weight training. Nevertheless, Eq. (2) can-
not break the limitation of time consumption: The high-
performing ticket without weight training rarely exists in
a randomly initialized network, thus the network width is
usually exponentially expanded to ensure the existence of
lottery tickets [27], [28], [29]. This increases the searching
space, and thus more searching cycles are required. As a
result, the searching cost is even higher than training the
weights in Eq. (1). This paper attempts to solve such a time-
consuming problem. In what follows, we first reveal that
the lottery tickets exist in the pre-trained models without
the need for network width expansion, and then provide a
fast manner for searching these lottery tickets.
3.2 Lottery Jackpots in Pre-trained Models
We aim to verify the existence of lottery tickets without
weight training in the pre-trained models, termed lottery
jackpots. We concentrate on pre-trained models since these
models are widely visible on the Internet, or available from
the client, which should be fully utilized and also eliminate
the necessity of pre-training a model from scratch.
Built upon a pre-trained model, our target for finding
lottery jackpots can be re-formulated as:
min
mL(m⊙˜w;D),s.t.1−∥m∥0
k≥p, (3)
where ˜wrepresents the pre-trained weight vector, which
differs our method from previous work [27], [28] that search
for the lottery tickets in a randomly initialized network with
the requirement of expanded network width.
To prove the existence of lottery jackpots in an un-
widened pre-trained model, we firstly adopt the edge-
popup [27] to search for the lottery jackpots. Specifically, the
mask m∈ {0,1}kis firstly relaxed to a randomly initialized

--- PAGE 4 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 4
10 20 30 40 50 60 70 80 90
Sparsity (%)75777981838587899193Top-1 Accuracy (%)
VGG-19 (CIFAR-10)
Dense Network
Lottery Jackpots
Random Initialize
10 20 30 40 50 60 70 80 90
Sparsity (%)354045505560657075Top-1 Accuracy (%)
ResNet-32 (CIFAR-100)
Dense Network
Lottery Jackpots
Random Initialize
10 20 30 40 50 60 70 80 90
Sparsity (%)354045505560657075Top-1 Accuracy (%)
ResNet-50 (ImageNet)
Dense Network
Lottery Jackpots
Random Initialize
Fig. 2: Performance of our lottery jackpots searched from pre-trained models without modifying any convergent weights
and subnets searched from random initialized networks at different sparsity levels. Our lottery jackpots outperform sub-
networks searched from randomly initialized networks by a large margin.
¯ m∈[0,1]k. Denote the input of the network as X, and then
the output Yis obtained as
Y=F(h(¯ m)⊙˜w,X), (4)
where F()represents the neural network function, and h(·)
is defined as:
h(¯ mi) =0,if¯ miin the top- p%smallest of ¯ m,
1,otherwise,(5)
where i∈ {1,2, ..., k}.
During the network back-propagation, the straight-
through-estimator (STE) [53] is used to calculate the gra-
dient of the loss Lw.r.t. the relaxed mask ¯ mas:
∂L
∂¯ m=∂L
∂h(¯ m)∂h(¯ m)
∂¯ m≈∂L
∂h(¯ m)·1. (6)
In this manner, we conduct mask training, i.e., weight
searching on various pre-trained models by optimizing ¯ m
with the Stochastic gradient descent (SGD) optimizer to
verify the existence of lottery jackpots. As can be surpris-
ingly observed in Fig. 2, given any sparsity level, the lottery
jackpots can be found in all pre-trained networks. For ex-
ample, a lottery jackpot can be found in ResNet-32 [21] pre-
trained on CIFAR-10 [31], which has only 10% parameters
while retaining over 94% top-1 accuracy. Lottery jackpots
also outperform sub-networks searched from randomly ini-
tialized networks by a large margin, which require the
original networks to be exponentially expanded for better
performance [27], [28], [29]. Moreover, the found lottery
jackpots can achieve comparable or even better performance
than state-of-the-art methods in network pruning [19], [25],
[32], which will be quantitatively shown in the next section.
Discussion. Some of the recent studies hold a different
view that the pre-trained models are not necessary. For
example, the well-known lottery ticket hypothesis [26] finds
that sparse networks can be trained in isolation to achieve
considerable accuracy even without a pre-trained model.
Liuet al. [54] stated that inheriting weights from pre-trained
models is not necessarily optimal. However, their claims are
built on premise of a time-consuming weight training which
helps to recover the accuracy performance. In contrast, our
lottery jackpots in this paper re-justify the importance of the
pre-trained models where high-performing sub-networksalready exist without the necessity of weight training and
network width expansion.
Traceability of Lottery Jackpots. We further delve into
the principles behind lottery jackpots by initiating an error
analysis in network pruning. For simplicity, we consider
one-layer full-connected network, followed by a ReLU ac-
tivation function. The analysis can be easily extended to
multiple-layer networks and other types of networks. Let
the input be zl−1∈Rnl−1and the weights be wl∈
Rnl×nl−1, where nl−1andnlare the neuron number of layer
l−1and layer l. Then, the output zl∈Rnlis:
zl=σ(wl·zl−1), (7)
where σis the ReLU function. For a compressed layer with
the binary mask ml, the new output ˆzlis:
ˆzl=σ((ml⊙wl)·zl−1). (8)
Proposition 1. Denote ξ=||wi,:
l−(ml⊙wl)i,:||2and
zMax
l−1=max|zl−1|. For the i-th neuron, we have:
0≤ |zi
l−ˆzi
l| ≤ξ√nl−1zMax
l−1. (9)
Proof. |zi
l−ˆzi
l|can be derived as:
|σ(wi,:
l·zl−1)−σ((mi,:
l⊙wi,:
l)·zl−1)|
(a)
≤ |wi,:
l·zl−1−(ml⊙wl)i,:·zl−1|
=|((1−ml)⊙wl)i,:·zl−1|
(b)=|Cos(((1−ml)⊙wl)i,:,zl−1)|
||((1−ml)⊙wl)i,:||2||zl−1||2
(c)
≤ξ√nl−1zMax
l−1|Cos(((1−ml)⊙wl)i,:,zl−1)|,(10)
where (a) follows |σ(x)−σ(y)| ≤ |x−y|for the ReLU func-
ton, (b) follows |a·b|=|Cos(a,b)||a||2||b||2where Cos(a,b)
returns the cosine similarity, (c) follows ||a||2≤√ckfor any
a∈[−k, k]c. Considering 0≤ |Cos(a,b)| ≤1, we therefore
complete the proof. ■
We can observe from Eq. (10) that the bound of output
discrepancy is up to the magnitude difference ξand the
cosine similarity |Cos(((1−ml)⊙wl)i,:,zl−1)|. Taking into
consideration both factors would well lower the output
discrepancy. As can be referred to Fig. 3, Magnitude-based
pruning [1] fails to preserve better performance since it

--- PAGE 5 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 5
10 20 30 40 50 60 70 80 90
Sparsity (%)01020304050607080Top-1 Accuracy (%)
Dense Network
Lottery Jackpots
Magnitude
SNIP
GraSP
Random
Fig. 3: The comparison of accuracy performance between
our searched lottery jackpots and existing pruning criteria
without weight training. Experiments are performed using
ResNet-50 [21] on ImageNet [22].
solely emphasizes the magnitude difference while ignoring
the cosine similarity. Nevertheless, we highlight that in
situations where cosine similarity approaches zero, there
even occurs no output discrepancy in network pruning,
regardless of the difference in magnitude between the dense
and sparse weights.
Here we give a toy example for a better understand-
ing. Let wi,:
l= [1 ,−1,0.2,0.3],zl−1= [1 ,1,1,1], it is
clear that magnitude-based pruning generates a binary
mask of [1,1,0,0]at50% pruning rate to removes weights
of smallest magnitude ( 0.2and 0.3). This produces the
smallest magnitude difference ξ= 0.3606 , cosine similar-
ityCos([0,0,0.2,0.3],[1,1,1,1]) = 0 .6918 , and an output
derivation |zi
l−ˆzi
l|= 0.5. Nevertheless, opportunity exists
in a binary mask of [0,0,1,1]that leads to cosine similarity
Cos([1,−1,0,0],[1,1,1,1]) = 0 . This produces an output
derivation |zi
l−ˆzi
l|= 0, even with lager magnitude deriva-
tionξ= 1.4142 .
The above example well elucidates the rationale behind
the existence of lottery jackpots in pre-trained networks.
Without any re-training on the weights, a significantly lower
output discrepancy can be attained through the simultane-
ous consideration of both cosine similarity and magnitude
difference. Though the existence of lottery jackpots greatly
highlights the values of the pre-trained models in network
pruning, the weight searching still leads to a significant time
consumption even though the network width is not neces-
sary to be expanded in our lottery jackpots. For example,
it takes around 100 searching epochs on ImageNet [22] to
successfully find the lottery jackpots. Thus, it is of great
need to shorten the weight searching process to find the
lottery jackpots quickly. The community might focus more
on how to locate these lottery jackpots efficiently within pre-
trained network models, which is also our important study
in Sec. 3.3, Sec. 3.4 and Sec. 3.5.
3.3 Mask Overlap
Han et al. [1] observed an interesting phenomenon of “free
lunch” that using the magnitude to prune the pre-trainedAlexNet [55] on ImageNet without weight training leads to
no performance drops when the sparsity is less than 50%.
Despite its simplicity, the model performance degenerates
sharply in a higher sparsity. In contrast, our lottery jackpots
can retain good performance at most sparsity levels as
shown in Fig. 2. This inspires us to explore the potential
linkage to accelerate the searching of our high-performing
lottery jackpots. To this end, we first define the following
ratio of sparse mask overlap to measure the similarity of
two masks m1andm2as:
Overlap (m1,m2) = 1−∥h(¯ m1)−h(¯ m2)∥1
k. (11)
It is easy to know that a larger overlap rate indicates
two more similar masks. Then, we consider various pruning
criteria in existing works based on weight-training and
compare their masks with the searched mask of our lottery
jackpot. We briefly revisit these typical criteria as follows:
Magnitude [1]. This method uses weight magnitude as
importance score sand derives a 0−1mask vector ˆ mby
removing these weights with small magnitudes.
SNIP [25]. The connection sensitivity to the loss is con-
sidered as weight importance score sand then the mask ˆ m
is derived by removing unimportant connections. Specifi-
cally, the importance score is defined as ∥w⊙g∥1, where g
denotes the gradient of w.
GraSP [32]. It first computes the Hessian-gradient prod-
ucthof the l-th layer using the sampled training data. Then,
the importance score sis defined as −w⊙hto preserve
the gradient flow, and the mask vector ˆmis obtained by
removing low-scored weights.
Random . We first randomly initialize the relaxed ¯ m∈
[0,1], and then remove weights in compliance with these
low-scored relaxed mask values.
In Fig. 3, we show the performance of the pruned models
by the above pruning criteria without weight training. As
can be observed, our lottery tickets outperform the pruned
models by a large margin as the network sparsity goes
up. Further, Fig. 4 indicates a large overlap ratio between
the masks of existing methods ˆmand our searched result
h(¯m). Among them, the magnitude-based pruning takes the
top position. For example, the overlap between magnitude-
based mask and our lottery jackpot is more than 95% when
removing around 90% parameters of ResNet-50 [21]. This
discovery indicates that a very small portion of the pruned
mask from existing pruning criteria needs to be corrected
to fit our searched mask without the necessity of a time-
consuming weight training process in existing work.
Thus, we propose to leverage the importance score s
from existing pruning criteria as a warm-up initialization
of our relaxed mask ¯ mfor searching the lottery jackpots.
The relaxed mask ¯ mis initialized as:
¯ mi=η,ifsiin the top- p%smallest of s,
1,otherwise,(12)
where η= 0.99in this paper. The rationale behind this is
that closer initial mask values of pruned weights to pre-
served counterparts indicate smaller distances to overcome
the remaining non-overlaps between the initialized mask
and lottery jackpot, which leads to a fast convergence as we
quantitatively demonstrate in Sec. 4.4.

--- PAGE 6 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 6
60 70 80 90
Sparsity (%)2030405060708090100Sparse Architecture Overlap (%)GraSP
SNIP
Magnitude
Fig. 4: The comparison of sparse architecture overlap be-
tween our searched lottery jackpots and existing pruning
criteria without weight training. Experiments are performed
using ResNet-50 [21] on ImageNet [22].
Consequently, the initialized binary mask h(¯m)gener-
ated by Eq. (5) is the same as the mask ˆ mfrom existing
pruning criteria. Overall, our target of finding the lottery
jackpots can be reformulated as:
min
¯ mL h(¯ m)⊙˜w;D,s.t.1−∥h(¯ m)∥0
k≥p. (13)
Then, we conduct experiments using different initialized
masks from the pruning criteria mentioned above. As can
be seen from Fig. 5, different warm-up initializations offer a
faster convergence of weight searching in finding our lottery
jackpots. Particularly, with magnitude-based mask as the
initialization, a high-performing lottery jackpot removing
around 90% parameters of VGGNet-19, is located quickly
using only 30searching epochs. In contrast, it is usually 300
searching epochs that are used to find out a comparable
lottery jackpot with randomly initialized weights. Thus, a
significant searching complexity can be reduced when using
the magnitude-based mask as the initialization. It is easy
to understand this phenomenon since the magnitude-based
pruning mask results in the highest rate of overlap with our
searched mask as shown in Fig. 4.
We further study how to improve the searching ef-
ficiency of lottery jackpots after our mask initialization.
Before that, we take an in-depth analysis on the searching
instability of the edge-popup algorithm [27], which, as we
demonstrate, primarily stems from the weight interdepen-
dence in modern neural networks [27].
3.4 Weight Interdependence
Intuitively, Eq. (13) minimizes the network loss Lby op-
timizing the relaxed mask ¯ mat each training iteration.
Let the relaxed mask at the t-th training iteration be ¯ mt,
which generates binary masks to prune or preserve weights
through Eq. (5). Since we adopt magnitude-based mask,
mask values for preserved weights are larger than those of
pruned weights. Denoting the mask indexes correspondingto the preserved and pruned weights as ΦtandΨt, we can
have the following relationship:
min(¯ mΦt
t)> max (¯ mΨt
t), (14)
where min(·)andmax(·)return the lowest value and
highest value, respectively.
Nevertheless, the above inequality might be broken after
mask updating at the t+ 1iteration since some elements in
¯ mΨt
t+1become larger than some elements in ¯ mΦt
t+1. Conse-
quently, these elements in ¯ mΨt
t+1are revived while these in
¯ mΦt
t+1are re-pruned at the t+ 1 iteration, which we name
as weight swapping in this paper. Let the indexes of these
re-pruned and revived weights be Φ∗
t⊆ΦtandΨ∗
t⊆Ψt,
Proposition 1 reveals that weight swapping decreases the
training loss Lunder Assumption 1.
Assumption 1 : Let ∆˜widenote some weight pertur-
bation on a specific weight ˜wiandL(˜wi+ ∆˜wi)1denote
the new loss after adding the perturbation. Weights ˜wiand
˜wjindependently contribute to the loss function Land the
following constraint satisfies:
L(˜wi,˜wj)− L(˜wi+ ∆˜wi,˜wj+ ∆˜wj)
=L(˜wi,˜wj)− L(˜wi+ ∆˜wi,˜wj)
+L(˜wi,˜wj)− L(˜wj+ ∆˜wj,˜wi).(15)
Proposition 2 :For∀i∈Φ∗
t,∀j∈Ψ∗
t, the following
inequality can always be satisfied:
L h(¯ mi
t+1)˜wi, h(¯ mj
t+1)˜wj<L h(¯ mi
t)˜wi, h(¯ mj
t)˜wj.
(16)
Proof .L h(¯ mi
t+1)˜wi)indicates the loss after removing
the˜wiand it can be approximated according to the first-
order Taylor series in the following:
L h(¯ mi
t+1)˜wi
≈L h(¯ mi
t)˜wi−∂L
∂ h(¯ mi
t)˜wi h(¯ mi
t)˜wi−h(¯ mi
t+1)˜wi
=L h(¯ mi
t)˜wi−∂L
∂ h(¯ mi
t)˜wi˜wi.
(17)
AndL h(¯ mj
t+1)˜wj)denotes the loss after reviving the
˜wj. Similarly, it can be approximated as:
L h(¯ mj
t+1)˜wj
≈ L h(¯ mj
t)˜wj−∂L
∂ h(¯ mj
t)˜wj h(¯ mj
t)˜wj−h(¯ mj
t+1)˜wj
=L h(¯ mj
t)˜wj+∂L
∂ h(¯ mj
t)˜wj˜wj.
(18)
Combining the above two equalities yields:
L h(¯ mj
t+1)˜wj)− L h(¯ mj
t)˜wj
+L h(¯ mi
t+1)˜wi)− L h(¯ mi
t)˜wi
=∂L
∂ h(¯ mj
t)˜wj˜wj−∂L
∂ h(¯ mi
t)˜wi˜wi.(19)
1. The notations of other weights and the observed training set are
neglected for simplicity.

--- PAGE 7 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 7
5 10 20 30 50 100 150 200 300
Searching Cost (Epochs)75777981838587899193Top-1 Accuracy (%)
VGGNet-19 (CIFAR-10)
Dense Network
Magnitude
SNIP
GraSP
Random
5 10 20 30 50 100 150 200 300
Searching Cost (Epochs)45505560657075Top-1 Accuracy (%)
ResNet-32 (CIFAR-100)
Dense Network
1-norm
SNIP
GraSP
Random
5 10 15 20 30 45 60 100
Searching Cost (Epochs)45505560657075Top-1 Accuracy (%)
ResNet-50 (ImageNet)
Dense Network
Magnitude
SNIP
GraSP
Random
Fig. 5: Convergence ability of our lottery jackpots using the masks of existing pruning criteria as a warm-up initialization
for the weight searching in different networks and benchmarks (90% sparsity). As can be seen, with the mask from the
magnitude pruning as the initialization, our weight searching can easily get convergence with less computation cost.
Considering Assumption 3 we have:
L h(¯ mi
t+1)˜wi, h(¯ mj
t+1)˜wj− L h(¯ mi
t)˜wi, h(¯ mj
t)˜wj
=∂L
∂ h(¯ mj
t)˜wj˜wj−∂L
∂ h(¯ mi
t)˜wi˜wi.
(20)
By Eq. (14) we have ¯ mi
t>¯ mj
t, and given i∈Φ∗
t,j∈Ψ∗
t
we obtain ¯ mj
t+1>¯ mi
t+1. Combining these two inequalities
therefore results in:
¯ mi
t−¯ mi
t+1>¯ mj
t−¯ mj
t+1. (21)
Looking back to Eq. (6), the updated state of ¯ mfrom the
t-th iteration to the ( t+ 1)-th can derived as:
¯ mt+1=¯ mt−∂L
∂¯ mt
=¯ mt−∂L
∂ h(¯ mt)⊙˜w∂ h(¯ mt)⊙˜w
∂¯ mt
=¯ mt−∂L
∂ h(¯ mt)⊙˜w∂h(¯ mt)
∂¯ mt˜w
≈¯ mt−∂L
∂ h(¯ mt)⊙˜w˜w,(22)
where learning rate and optimizer items like weight decay
are neglected for ease of representation. By taking this
derivation into Eq. (21), we obtain:
∂L
∂ h(¯ mi
t)˜wi˜wi>∂L
∂ h(¯ mj
t)˜wj˜wj. (23)
This inequality and Eq. (20) lead to:
L h(¯ mi
t+1)˜wi, h(¯ mj
t+1)˜wj− L h(¯ mi
t)˜wi, h(¯ mj
t)˜wj
=∂L
∂ h(¯ mj
t)˜wj˜wj−∂L
∂ h(¯ mi
t)˜wi˜wi<0.
(24)
Consequently, we have:
L h(¯ mi
t+1)˜wi, h(¯ mj
t+1)˜wj<L h(¯ mi
t)˜wi, h(¯ mj
t)˜wj,
(25)
which finally complete our proof of Proposition 1. ■
So far, we have proved that Proposition 1 guarantees
the loss drop for weight swapping between ˜wiand ˜wj
𝜖
Fig. 6: The distortion error ϵcaused by the interdependence
among weights v.s.quantities of weight swapping q. Exper-
iments are performed using ResNet-32 [21].
under Assumption 1, and the expected loss drop denoted
by∆L(˜wi,˜wj)is:
∆L(˜wi,˜wj) =∂L
∂ h(¯ mj
t)˜wj˜wj−∂L
∂ h(¯ mi
t)˜wi˜wi.(26)
Unfortunately, Assumption 1 does not hold always in
modern neural networks due to the existence of Batch
normalization layers, Softmax function, etc. In contrast, the
interdependence frequently stems among different weights
as discussed in [32], [56], [57]. Under this case, Eq. (15) is
indeed presented as:
L(˜wi,˜wj)− L(˜wi+ ∆˜wi,˜wj+ ∆˜wj)
=L(˜wi,˜wj)− L(˜wi+ ∆˜wi,˜wj)
+L(˜wi,˜wj)− L(˜wj+ ∆˜wj,˜wi) +ϵ,(27)
where ϵ∈Ris the distortion error caused by the depen-
dency of weights. Repeating the deduction procedures for
Proposition 1, we obtain:
L h(¯ mi
t+1)˜wi, h(¯ mj
t+1)˜wj− L h(¯ mi
t)˜wi, h(¯ mj
t)˜wj
=∂L
∂ h(¯ mj
t)˜wj˜wj−∂L
∂ h(¯ mi
t)˜wi˜wi
| {z }
∆L(˜wi,˜wj)in Eq. (26)+ϵ.
(28)

--- PAGE 8 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 8
0 5 10 15 20 25 30
Epoch0.50.60.70.80.9Training LossVGGNet-19 (CIFAR-10)
SR-popup (Ours)
Edge-popup
0 5 10 15 20 25 30
Epoch1.01.52.02.53.03.54.0Training LossResNet-32 (CIFAR-100)
SR-popup (Ours)
Edge-popup
0 5 10 15 20 25 30
Epoch2.252.502.753.003.253.503.754.00Training LossResNet-50 (ImageNet)
SR-popup (Ours)
Edge-popup
Fig. 7: Loss curve of our proposed SR-popup and the Edge-popup algorithms for searching lottery jackpots in different
networks and benchmarks (90% sparsity). As can be seen, SR-popup exhibits higher stability and convergence speed.
Hence, the actual loss drop for the weight swapping falls
in∆L(˜wi,˜wj) +ϵ. It is easy to know that Eq. (24) always
holds and the loss still decreases from weight swapping if
the following constraint satisfies:
ϵ <−∆L(˜wi,˜wj). (29)
Otherwise, Eq. (24) breaks and the loss even becomes
larger after updating, in particular to frequent weight swap-
ping. Fig. 6 displays quantitative distributions of ϵby ran-
domly performing weight swapping in different quantities
of swapped weights for ResNet-32 [21]. We can see that the
variance of the distortion error ϵincreases proportionally
to the number of swapped weights, destroying the above
constraint. As a result, the searching stability of the edge-
popup algorithm is bounded by such a distortion error
stemming from the weight interdependence.
3.5 Short Restriction Popup
To solve the above issue, on the premise of mask initial-
ization introduced in Sec. 3.3, we further propose a short
restriction popup to improve the searching efficiency for
lottery jackpots. Our key motivation is to encourage weight
swapping if we observe a small value of expected loss drop
∆L(˜wi,˜wj)which is robust to the distortion error accord-
ing to Eq. (29); otherwise, the weight swapping is going to be
prevented. Consequently, the continuously decreasing loss
results in a stable training convergence.
Concretely, we first obtain the indexes of preserved and
pruned weights from the initialized mask ¯ m0in Eq. (12):
Φ0={i|¯ mi= 1, i= 1,2, ..., k},
Ψ0={i|¯ mi=η, i= 1,2, ..., k},(30)
where kis the weight size as recalled in Sec. 3.1. In the t-th
forward propagation, we obtain the binary mask by:
h∗(¯ mi) =1,ifi∈Φt,
0,otherwise,(31)
The backward propagation is the same as Eq. (6). After
thet-th updating, the indexes of re-pruned and re-revived
weights Φ∗
tandΨ∗
tare obtained by:
Φ∗
t={i|¯ mi
t+1≤TopK (¯ mt+1,∥Ψt∥0), i∈Φt},
Ψ∗
t={i|¯ mi
t+1> TopK (¯ mt+1,∥Ψt∥0), i∈Ψt},(32)Algorithm 1: Short Restriction Popup for Locating
Lottery Jackpots
Input: Pre-trained weights ˜w, pruning rate p,
searching iteration tf.
1Initialize the relaxed mask ¯ m0via Eq. (12).
2Obtain preserved and pruned weight indexes Φ0
andΨ0via Eq. (30).
3fort = 1→tfdo
4 Forward propagation via Eq. (31).
5 Backward propagation via Eq. (6).
6 Update ¯ musing SGD optimizer.
7 Conduct weight swapping via Eq. (34).
8end
9Return the compressed model h∗(¯ m)⊙˜w.
where TopK (v, K)returns the K-thlargest value within
vector v. Then, we gradually decrease the weight swapping
number qtbetween ¯ mΨ∗
t
tand¯ mΦ∗
t
tas:
qt=⌈∥Ψ∗
t∥0(1−t
tf)4⌉, (33)
where ⌈·⌉is the ceiling operation and tfis the total training
iterations. To explain, we encourage more weight swapping
to achieve enough exploration at the early stage of training,
and then gradually limit the number of weight swapping to
achieve efficient convergence by more loss drops. We choose
to swap the weights of top- qtsmallest ¯ mΦ∗
tand weights of
top-qtlargest ¯ mΨ∗
t. Consequently, the mask indexes corre-
sponding to the preserved weights and the pruned weights
at iteration t+ 1are obtained as:
Φt+1= Φt\ArgBotK (¯ mΦ∗
t
t+1, qt)∪ArgTopK (¯ mΨ∗
t
t+1, qt),
Ψt+1= Ψt\ArgTopK (¯ mΨ∗
t
t+1, qt)∪ArgBotK (¯ mΦ∗
t
t+1, qt),
(34)
where ArgTopK (v, K),ArgBotK (v, K)returns the top- K
largest and smallest values within vector v.
Our proposed short restriction pop-up, termed SR-
popup for searching lottery jackpots, is listed in Alg. 1. Fig. 7
shows the loss curves of the SR-popup and Edge-popup
when searching lottery jackpots across various datasets. By
escaping weight swapping that is not robust to the distor-
tion error, SR-popup effectively reduces the loss oscillation
and leads to a more stable searching process.

--- PAGE 9 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 9
TABLE 1: Comparison with existing weight training meth-
ods for pruning VGGNet-19 [30] and ResNet-32 [21] on
CIFAR-10 [31]. We report top-1 accuracy (%), training cost
(T) and searching cost (S).
Pruning Rate 90% 95% Epoch
VGGNet-19 94.23 - -
SET [24] 92.46 91.73 160 (T)
Deep-R [60] 90.81 89.59 160 (T)
SNIP [25] 93.65 93.43 160(T)
GraSP [32] 93.01 92.82 160 (T)
LT [26] 93.66 93.30 160 (T)
OBD [16] 93.74 93.58 160 (T)
Jackpot (Ours) 93.70±0.12 93.54 ±0.17 10 (S)
Jackpot (Ours) 93.86±0.09 93.67 ±0.12 30 (S)
ResNet-32 94.62 - -
SET [24] 92.30 90.76 160 (T)
Deep-R [60] 91.62 89.84 160 (T)
SNIP [25] 92.59 91.01 160 (T)
GraSP [32] 92.79 91.80 160 (T)
LT [26] 92.61 91.37 160 (T)
OBD [16] 94.17 93.29 160 (T)
Jackpot (Ours) 94.03±0.12 92.54 ±0.11 10 (S)
Jackpot (Ours) 94.30±0.07 93.51 ±0.07 30 (S)
4 E XPERIMENTS
4.1 Settings
Datasets . We consider representative benchmarks with
different scales for image classification. For the small-
scale dataset, we choose the CIFAR-10 and CIFAR-100
datasets [31]. CIFAR-10 contains 60,000 32 ×32 color im-
ages from 10 different classes, with 6,000 images in each
class. The CIFAR-100 dataset is similar to the CIFAR-10,
except that it has 100 classes, each of which contains 600
images. For the large-scale dataset, we choose the challeng-
ing ImageNet-1K [22] that has over 1.2 million images for
training and 50,000 validation images with 1,000 categories.
Networks . For CIFAR-10 and CIFAR-100, we find lottery
jackpots in the classic VGGNet-19 [30] and ResNet-32 [21].
Following previous studies [32], [58], we double the filter
numbers of each convolution layer of ResNet-32 to make
it suitable to fit the dataset. For the ImageNet dataset, we
demonstrate the efficacy of lottery jackpots for pruning
ResNet-50 [21], which serves as a typical backbone for
network pruning community. Besides, we conduct experi-
ments to find lottery jackpots for pruning very light-weight
networks including MobileNet-V1 [33], EfficientNet-B4 [59].
Implementation Details . We adopt the proposed SR-
popup to searching for lottery jackpots using the SGD opti-
mizer with an initial learning rate of 0.1 for all experiments.
With different total epochs for searching, i.e., 10 and 30, we
adjust the learning rate with the cosine scheduler [61]. The
momentum is set to 0.9 and the batch size is set to 256.
The weight decay is set to 5 ×10−4on CIFAR-10 and 1
×10−4on ImageNet. For fair comparison, we conduct data
augmentation for image pre-processing including cropping
and horizontal flipping, which is the same as the official
implementation in Pytorch [62]. We repeat all of our ex-
periments three times on CIFAR-10 with different seeds,TABLE 2: Comparison with existing weight training meth-
ods for pruning VGGNet-19 [30] and ResNet-32 [21] on
CIFAR-100 [31]. We report top-1 accuracy (%), training cost
(T) and searching cost (S).
Pruning Rate 90% 95% Epoch
VGGNet-19 [30] 74.16 - -
SET [24] 72.36 69.81 160 (T)
Deep-R [60] 66.83 63.46 160 (T)
SNIP [25] 72.83 71.83 160 (T)
GraSP [32] 71.07 70.1 160 (T)
LT [26] 72.58 70.47 160 (T)
OBD [16] 73.83 71.98 160 (T)
Jackpot (Ours) 72.41±0.14 72.27 ±0.21 10 (S)
Jackpot (Ours) 74.63±0.18 72.92 ±0.31 30 (S)
ResNet-32 [21] 74.64 - -
SET [24] 69.66 67.41 160 (T)
Deep-R [60] 66.78 63.90 160 (T)
SNIP [25] 69.97 64.81 160 (T)
GraSP [32] 70.12 67.05 160 (T)
LT [26] 69.63 66.48 160 (T)
OBD [16] 71.96 68.73 160 (T)
Jackpot (Ours) 71.48±0.16 68.33 ±0.22 10 (S)
Jackpot (Ours) 72.68±0.13 70.01 ±0.14 30 (S)
and report the mean and standard deviation of top-1 clas-
sification accuracy. On ImageNet, we run all experiments
one time considering the heavy resource consumption and
stable performance on the large dataset, and report both top-
1 and top-5 classification accuracy. All experiments are run
on NVIDIA Tesla V100 GPUs.
4.2 Comparison with Weight Training Methods
CIFAR-10/100. On CIFAR-10/100, we compare lottery jack-
pots with many weight training competitors including
OBD [16], SET [24], Deep-R [60], Lottery Tickets (LT) [26],
SNIP [25], and GraSP [32]. We quantitatively report top-1
classification accuracy for pruning VGGNet-19 and ResNet-
32 under two sparsity levels {90%,95%}in Table 1 and Ta-
ble 2. The weight training or searching cost, which refers to
epochs expenditure for obtaining the final pruned model, is
also listed for comparison of the pruning efficiency. Besides,
we plot the performance of different approaches using the
same training/search epoch at 95% sparsity in Fig. 8.
As can be observed in Table 1, lottery jackpots provide a
state-of-the-art performance while leveraging rare pruning
computation cost on CIFAR-10. For example, only 10 epochs
are required for finding a lottery jackpot that contains only
10% parameters of VGGNet-19, while attaining a superior
top-1 accuracy of 93.70%. In contrast, SNIP takes 160 epochs
in total to reach a lower accuracy of 93.65%. Lottery jack-
pot also serves as a front-runner for pruning ResNet-32.
Different from the lottery tickets [26] that need an entire
weight training process for recovering performance, we
directly search for lottery jackpots without modifying the
pre-trained weights, resulting in even better pruned models
(94.30% v.s.92.61% for top-1 accuracy) with more than 5 ×
reductions on the pruning cost (30 epochs v.s.160 epochs).
The results of CFIAR-100 in Table 2 also show that our
lottery jackpots retain better top-1 accuracy than the com-

--- PAGE 10 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 10
5 10 20 30 90 160
Searching/Training Cost (Epochs)83.084.586.087.589.090.592.093.595.0Top-1 Accuracy (%)
VGGNet-19 (CIFAR-10, 95% SPARSITY)
Dense Network
Jackpot(Ours)
SNIP
OBD
LT
5 10 20 30 90 160
Training/Searching Cost (Epochs)60.062.565.067.570.072.575.0Top-1 Accuracy (%)
VGGNet-19 (CIFAR-100, 90% Sparsity)
Dense Network
Jackpot(Ours)
SNIP
OBD
LT
5 10 20 30 90 160
Training/Searching Cost (Epochs)60.062.565.067.570.072.575.0Top-1 Accuracy (%)
VGGNet-19 (CIFAR-100, 95% Sparsity)
Dense Network
Jackpot(Ours)
SNIP
OBD
LT
5 10 20 30 90 160
Training/Searching Cost (Epochs)84.586.087.589.090.592.093.595.0Top-1 Accuracy (%)
ResNet-32 (CIFAR-10, 95% Sparsity)
Dense Network
Jackpot(Ours)
SNIP
OBD
LT
5 10 20 30 90 160
Training/Searching Cost (Epochs)60.062.565.067.570.072.575.0Top-1 Accuracy (%)
ResNet-32 (CIFAR-100, 90% Sparsity)
Dense Network
Jackpot(Ours)
SNIP
OBD
LT
5 10 20 30 90 160
Training/Searching Cost (Epochs)57606366697275Top-1 Accuracy (%)
ResNet-32 (CIFAR-100, 95% Sparsity)
Dense Network
Jackpot(Ours)
SNIP
OBD
LT
Fig. 8: Training/Search cost v.s.top-1 accuracy of different methods including SNIP [25], OBD [16] and LT [26] to prune
VGGNet-19 [30] and ResNet-32 [21] on CIFAR-10/100.
petitors under different pruning rates with less cost. For
instance, our method can achieve a top-1 accuracy of 74.63%
when pruning VGGNet-19 at 90% sparsity, which surpasses
other state-of-the-arts by a large margin of 2.27%, 2.05%, and
1.80% higher than SET [24], LT [26], and SNIP [25], respec-
tively. Note that it even exceeds the origin dense network by
0.47%. Compared with OBD [16] that conducts fine-training
after pruning the pre-trained ResNet-32, lottery jackpots can
achieve better accuracy under all sparsity levels.
From Fig. 8, we further observe supreme performance of
lottery jackpots when maintaining similar pruning cost. De-
tailedly, previous methods [19], [25], [32] suffer significant
performance degradation with limited epochs, mostly due
to the insufficient training of weights. On the contrary, we
directly search lottery jackpots on the pre-trained weights,
leading to a satisfying performance with far less cost.
ImageNet . We further demonstrate the efficacy of our
lottery jackpots for pruning ResNet-50 on the challeng-
ing ImageNet dataset. Two pruning rates {80%,90%}are
considered for fair comparison with other state-of-the-arts
including SET [24], Deep-R [60], Dynamic Sparse [18],
SNIP [25], GraSP [32], OBD [16] and RigL [19]. In Ta-
ble 3, lottery jackpots beat all the competitors in both top-
1 and top-5 accuracy under the same sparsity ratio. We
can search for a lottery jackpot with only 1/5 computation
cost compared with GraSP [32] (30 epochs for us and 150
epochs for GraSP), while obtaining pruned model of 80%
pruning rate with higher top-1 and top-5 accuracy (75.19%
and 92.62% for lottery jackpot v.s.72.06% and 90.82% for
GraSP). When increasing the sparsity level to 90%, a lottery
jackpot that achieves 73.04% in top-1 accuracy with 5.80%
and 1.04% improvements over GraSP [32] and RigL [19], can
be successfully searched with fewer epochs (60, 100, and 150TABLE 3: Comparison with off-the-shelf weight training
methods for pruning ResNet-50 [21] on ImageNet [22]. We
report top-1 and top-5 accuracy (%), training cost (T) and
searching cost (S).
Top-1 acc. Top-5 acc. Epoch
Pruning Rate 80%
ResNet-50 [21] 76.15 92.95 -
SET [24] 72.60 91.20 100 (T)
Deep-R [60] 71.70 90.60 100 (T)
Dynamic Sparse [18] 73.30 92.40 100 (T)
SNIP [25] 69.67 89.24 100 (T)
GraSP [32] 72.06 90.82 150 (T)
RigL [19] 74.60 - 100 (T)
OBD [16] 75.12 68.73 100(T)
Jackpot (Ours) 75.19 92.52 30 (S)
Jackpot (Ours) 75.71 92.90 60 (S)
Pruning Rate 90%
ResNet-50 [21] 76.15 92.95 -
SET [24] 70.40 90.10 100( T)
Deep-R [60] 70.20 90.00 100 (T)
Dynamic Sparse [18] 71.60 90.50 100 (T)
SNIP [25] 67.21 87.19 100 (T)
GraSP [32] 68.14 88.67 150 (T)
RigL [19] 72.00 - 100 (T)
OBD [16] 72.51 90.78 100(T)
Jackpot (Ours) 72.61 91.09 30 (S)
Jackpot (Ours) 73.04 91.34 60 (S)
epochs for lottery jackpots, RigL and GraSP , respectively).
These results demonstrate the feasibility of directly finding
good-performing subnets without the requirement of time-
consuming weight training process.
We also conduct experiments for pruning light-designed

--- PAGE 11 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 11
TABLE 4: Comparison with existing weight training meth-
ods for pruning MobileNet-V1 [33] and EfficientNet-B4 [59]
on ImageNet [22]. We report top-1 accuracy (%), training
cost (T) and searching cost (S).
Top-1 acc. Top-5 acc. Epoch
Pruning Rate 80%
MobileNet-V1 [33] 71.93 90.37 -
SNIP [25] 63.95 85.12 180(T)
GraSP [32] 63.71 84.99 180(T)
RigL [19] 65.01 86.98 180 (T)
Jackpot (Ours) 67.21 87.83 30 (S)
Jackpot (Ours) 67.50 88.02 60 (S)
EfficientNet-B4 [59] 82.61 96.41 -
SNIP [25] 74.81 92.12 180(T)
GraSP [32] 74.70 92.01 180(T)
RigL [19] 75.46 92.57 180 (T)
Jackpot (Ours) 76.26 93.14 30 (S)
Jackpot (Ours) 76.67 93.39 60 (S)
TABLE 5: Comparison with existing weight searching meth-
ods for pruning ResNet-50 [21] on ImageNet [22]. The prun-
ing rate is set to 90%. We report top-1 and top-5 accuracy
(%), and searching cost (S).
Top-1 acc. Top-5 acc. Epoch
ResNet-50 [21] 76.15 92.95 -
Zhou et al. [24] 65.41 86.11 30(S)
Edge-popup [60] 70.20 88.91 30(S)
SR-popup (Ours) 72.61 91.09 30(S)
ResNet-50 [21] 76.15 92.95 -
Zhou et al. [24] 68.40 88.03 60(S)
Edge-popup [60] 70.89 90.12 60(S)
SR-popup (Ours) 73.04 91.34 60(S)
networks, including MobileNet-V1 [33] and EfficientNet-
B4 [59] at a pruning rate of 80%. Table 4 lists the perfor-
mance of lottery jackpots in comparison with SNIP [25],
GraSP [32], and RigL [19] based on our re-implementation.
The results again suggest the superiority of lottery jackpots
for the higher top-1 accuracy even without modifying any
pre-trained weights. Upon MobileNet-V1, a lottery jackpot
that significantly outperforms the RigL by 2.21% can be
found with 6 ×reduction on the pruning cost. For the more
compact EfficientNet-B4, we observe a considerable perfor-
mance drop for all methods. Nevertheless, lottery jackpots
still performs the best. Thus, it well demonstrates the advan-
tage of finding lottery jackpots in pre-trained light-weight
networks empowered with the ability to effectively reduce
the network complexity.
4.3 Comparison with Weight Searching Methods
In this section, we compare the performance of lottery
jackpots found by our proposed SR-popup and weight-
searching methods of Zhou et al. [28] and Edge-popup [27]
that are originally designed for searching good-performing
subnets in randomly initialized networks. Results for prun-
ing pre-trained ResNet-50 on ImageNet are listed in Table 5.
Our proposed SR-popup significantly outperforms other
10 30 50 70 90
Sparsity (%)90919293949596Top-1 Accuracy (%)Lottery Jackpots Fine-tuneFig. 9: Comparison between the searched lottery jackpots
and fine-tuned results based on the searched mask un-
der different sparsity levels for pruning ResNet-32 [21] on
CIFAR-10 [31].
methods when consuming the same searching epochs. Par-
ticularly, serious performance drops are observed for all
methods except our SR-popup with a limited epoch of 30,
which demonstrates the efficacy of leveraging magnitude-
based pruning as a warm-up for the relaxed masks. Al-
though adding the searching epochs brings clear improve-
ment for Edge-popup, its performance still falls behind SR-
popup by a noticeable margin. For example, when con-
suming 60 searching epochs, SR-popup successfully locates
a lottery jackpot with 73.04% top-1 accuracy, surpassing
edge-popup by 2.15% whose searching process is heavily
stumbled by the distortion error of weight interdependence
as discussed in Sec. 3.4.
4.4 Performance Analysis
In this section, we conduct multiple experiments to investi-
gate the performance of lottery jackpots. We first study the
effect of weight training process for lottery jackpots. After
searching the lottery jackpots, we freeze the found mask
and fine-tune the weights to investigate whether such a
weight training process can further benefit the performance
of the pruned model or not. The weight training objective is
formulated as:
min
˜wL(h(ˆ m)⊙˜w;D). (35)
We conduct experiments for ResNet-32 on CIFAR-10
with different pruning rates. The weights are fine-tuned
with 150 epochs and the other settings are described in
Sec. 4.1. Fig. 9 shows that the improvement of weight train-
ing process is negligible. It can not further improve the
performance of lottery jackpots, and even lead to a little
accuracy degradation under high sparsity. Thus, we can
conclude that the time-consuming weight training process
is unnecessary for getting good-performing sparse models
to some extent. In contrast, lottery jackpots already exist in
pre-trained models.
Then, we investigate the influence of searching epochs
for lottery jackpots. Detailedly, while comparing the per-
formance, we also observe the sparse architecture overlap

--- PAGE 12 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 12
10 20 30 60 100
Searching Cost (Epochs) (%)86878889909192939495Top-1 Accuracy (%)
Dense Network
=0
=0.5
=0.8
=0.9
=0.95
=0.99
Fig. 10: Ablation study for the initialization of relaxed
masks corresponding to pruned weights.
TABLE 6: Top-1 accuracy (%) of lottery jackpots with dif-
ferent epochs and sparse architecture overlap with the
magnitude-based pruning for pruning ResNet-32 [21] on
CIFAR-10 [31] at 90% sparse level.
Search epoch Top-1 acc. Architecture
overlap
0 10.00±0.01 100.0%
10 94.03 ±0.01 96.8%
30 94.30 ±0.08 96.1%
60 94.43±0.13 95.4%
100 94.41 ±0.11 94.2%
300 94.42 ±0.11 94.3%
1000 94.43 ±0.14 94.1%
between lottery jackpots and the initial mask ˆmobtained
by magnitude pruning. We show the quantitative results in
Table 6. As the searching epoch begins to increase, the per-
formance of found lottery jackpots can be boosted and the
sparse architecture also changes to some extent. However,
such a phenomenon is not consistent with more searching
epochs. The performance of lottery jackpots searched with
100 and 1000 epochs are at the same level. Further, the
sparse structure of lottery jackpots gradually tends to be
stable, and will not change significantly with the increase
of searching cost. Thus, only a relatively small amount of
computation cost is needed to find lottery jackpots, which
well demonstrates the efficiency of our approach.
Lastly, we perform four ablation studies for the pro-
posed SR-popup method. The experiments are conducted
for pruning ResNet-32 and VGGNet-19 on CIFAR-10 at
90% sparse level using 30 searching epochs. 1) Table 7
shows the performance comparison between without using
magnitude-based initialization (w/o W) or without using
short-restriction (w/o S), and using both of them (SR)
to search lottery jackpots. The results suggest that both
components in the proposed SR-popup are essential to
the performance of lottery jackpots. 2) For the magnitude-
based initialization in Eq. (12), we investigate how the initial
value ηof the relaxed masks affects the pruned weights.TABLE 7: Ablation study for the components in our pro-
posed SR-popup.
Method w/o M w/o S SR
Top-1 acc. 94.01±0.11 93.69 ±0.07 94.30±0.07
TABLE 8: Ablation study for the choice of weight swapping
pairs.
Method Inverse Random Ours
Top-1 acc. 93.13±0.12 93.84 ±0.08 94.30±0.07
TABLE 9: Ablation study for the schedule of restriction
number for weight swapping.
Schedule Constant Inverse Ours
Top-1 acc. 93.13±0.12 93.44 ±0.08 93.51±0.14
Detailedly, we plot the top-1 accuracy v.s.searching epochs
using different ηto search lottery jackpots. Fig. 10 shows
that smaller initial values result in a slower decrease in the
training loss. To explain, the distance between the masks
corresponding to the pruned and preserved weights is too
large, resulting in slow convergence. 3) We excavate the
choice for the elected weight swapping pairs. Two variants
include selecting these with the smallest loss drops random
selection are considered for comparison with our proposed
method. Results in Table 8 suggest that our proposed re-
striction method surpasses other variants by a large margin,
which demonstrates our point for only preserving weight
swapping that contributes the most significant expected
loss drops for searching lottery jackpots. 4) The declining
schedule for the restriction number of weight swapping, i.e.,
Eq. (33), is compared with its inverse version:
qt=⌈∥Ψ∗
t∥0(t
tf)4⌉, (36)
and a constant schedule is also considered, w.r.t. qt= 1 .
This circumstance is equivalent to adopting the edge-popup
algorithm on the foundation of our proposed magnitude-
based initialization. The results listed in Table 9 suggest that
our proposed schedule leads to the best performance thanks
to a sufficient exploration for weight swapping during the
early searching iterations and incremental restriction of un-
stable weight swapping for efficient convergence.
5 L IMITATIONS
As stressed throughout the paper, our lottery jackpots are
proposed on the premise of pre-trained models, which,
we believe, are mostly available from the Internet or the
client. However, the opportunity exists to be inaccessible
to the pre-trained models in some situation, indicating the
inapplicability of our lottery jackpots. Besides, our limited
hardware resources disable us to explore the existence of
lottery jackpots beyond the convolutional neural networks.
We expect to show more results on other tasks such as
natural language processing in our future work.

--- PAGE 13 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 13
6 D ISCUSSION AND CONCLUSION
In the field of network pruning, the state of pre-trained
models has been increasingly overlooked. Instead, many
researchers devised complex and time-consuming mecha-
nisms for training sparse networks from scratch. On the
contrary, in this paper, we re-justify the importance of pre-
trained models by revealing the existence of lottery jackpots
that, high-performing sub-networks emerge in pre-trained
models without the necessity of weight training.
To mitigate the inefficiency problem of existing searching
methods, we further present a novel SR-popup method that
enhances both the initialization and searching process of lot-
tery jackpots. For the mask initialization, we experimentally
observe that directly leveraging existing pruning criteria
leads to sparse masks that overlap with our lottery jack-
pot to a notable extent. Among those pruning criteria, the
magnitude-based pruning results in the most similar masks
with lottery jackpots. Based on this insight, we initialize the
sparse mask using magnitude pruning. On the other hand,
a short restriction mechanism is proposed to restrict change
of masks that may have potential negative impacts on the
training loss during searching under theoretical guarantee.
Extensive experiments demonstrate that our lottery jackpots
can achieve comparable or even better performance with
many state-of-the-arts without complex expert knowledge
for training sparse networks, while greatly reducing the
pruning cost.
ACKNOWLEDGEMENT
This work was supported by National Key R&D Program of
China (No.2022ZD0118202), the National Science Fund for
Distinguished Young Scholars (No.62025603), the National
Natural Science Foundation of China (No. U21B2037, No.
U22B2051, No. 62176222, No. 62176223, No. 62176226, No.
62072386, No. 62072387, No. 62072389, No. 62002305 and
No. 62272401), and the Natural Science Foundation of Fujian
Province of China (No.2021J01002, No.2022J06001).
REFERENCES
[1] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efficient neural network,” in Advances in Neural
Information Processing Systems (NeurIPS) , 2015, pp. 1135–1143.
[2] Y. He, P . Liu, Z. Wang, Z. Hu, and Y. Yang, “Filter pruning
via geometric median for deep convolutional neural networks
acceleration,” in IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2019, pp. 4340–4349.
[3] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio,
“Binarized neural networks,” in Advances in Neural Information
Processing Systems (NeurIPS) , 2016, pp. 4107–4115.
[4] Y. Zhong, M. Lin, M. Chen, K. Li, Y. Shen, F. Chao, Y. Wu, and
R. Ji, “Fine-grained data distribution alignment for post-training
quantization,” in European Conference on Computer Vision (ECCV) .
Springer, 2022, pp. 70–86.
[5] Y. Zhong, M. Lin, G. Nan, J. Liu, B. Zhang, Y. Tian, and R. Ji, “In-
traq: Learning synthetic images with intra-class heterogeneity for
zero-shot network quantization,” in IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , 2022, pp. 12 339–12 348.
[6] B. Peng, W. Tan, Z. Li, S. Zhang, D. Xie, and S. Pu, “Extreme
network compression via filter group approximation,” in European
Conference on Computer Vision (ECCV) , 2018, pp. 300–316.
[7] K. Hayashi, T. Yamaguchi, Y. Sugawara, and S.-i. Maeda, “Explor-
ing unexplored tensor network decompositions for convolutional
neural networks,” in Advances in Neural Information Processing
Systems (NeurIPS) , 2019, pp. 5552–5562.[8] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and
Y. Bengio, “Fitnets: Hints for thin deep nets,” arXiv preprint
arXiv:1412.6550 , 2014.
[9] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a
neural network,” arXiv preprint arXiv:1503.02531 , 2015.
[10] M. Ashby, C. Baaij, P . Baldwin, M. Bastiaan, O. Bunting, A. Cairn-
cross, C. Chalmers, L. Corrigan, S. Davis, N. van Doorn et al. ,
“Exploiting unstructured sparsity on next-generation datacenter
hardware,” 2019.
[11] E. Elsen, M. Dukhan, T. Gale, and K. Simonyan, “Fast sparse
convnets,” in IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) , 2020, pp. 14 629–14 638.
[12] J. Park, S. R. Li, W. Wen, H. Li, Y. Chen, and P . Dubey, “Holistic
sparsecnn: Forging the trident of accuracy, speed, and size,” arXiv
preprint arXiv:1608.01409 , 2016.
[13] D. Molchanov, A. Ashukha, and D. Vetrov, “Variational dropout
sparsifies deep neural networks,” in International Conference on
Machine Learning (ICML) , 2017, pp. 2498–2507.
[14] X. Chang, Y. Li, S. Oymak, and C. Thrampoulidis, “Provable
benefits of overparameterization in model compression: From
double descent to pruning neural networks,” in AAAI Conference
on Artificial Intelligence (AAAI) , 2020, pp. 6974–6983.
[15] D. Joo, E. Yi, S. Baek, and J. Kim, “Linearly replaceable filters for
deep network channel pruning,” in AAAI Conference on Artificial
Intelligence (AAAI) , 2021, pp. 8021–8029.
[16] Y. LeCun, J. Denker, and S. Solla, “Optimal brain damage,” in
Advances in Neural Information Processing Systems (NeurIPS) , 1989,
pp. 598–605.
[17] Y. He, Y. Ding, P . Liu, L. Zhu, H. Zhang, and Y. Yang, “Learning
filter pruning criteria for deep convolutional neural networks
acceleration,” in IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2020, pp. 2009–2018.
[18] H. Mostafa and X. Wang, “Parameter efficient training of deep
convolutional neural networks by dynamic sparse reparameter-
ization,” in International Conference on Machine Learning (ICML) ,
2019, pp. 4646–4655.
[19] U. Evci, T. Gale, J. Menick, P . S. Castro, and E. Elsen, “Rigging the
lottery: Making all tickets winners,” in International Conference on
Machine Learning (ICML) , 2020, pp. 2943–2952.
[20] Y. Wang, X. Zhang, L. Xie, J. Zhou, H. Su, B. Zhang, and X. Hu,
“Pruning from scratch,” in AAAI Conference on Artificial Intelligence
(AAAI) , 2020, pp. 12 273–12 280.
[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
for image recognition,” in IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) , 2016, pp. 770–778.
[22] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
“Imagenet: A large-scale hierarchical image database,” in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) , 2009,
pp. 248–255.
[23] A. Kusupati, V . Ramanujan, R. Somani, M. Wortsman, P . Jain,
S. Kakade, and A. Farhadi, “Soft threshold weight reparameteriza-
tion for learnable sparsity,” in International Conference on Machine
Learning (ICML) , 2020, pp. 5544–5555.
[24] D. C. Mocanu, E. Mocanu, P . Stone, P . H. Nguyen, M. Gibescu,
and A. Liotta, “Scalable training of artificial neural networks with
adaptive sparse connectivity inspired by network science,” Nature
Communications , vol. 9, pp. 1–12, 2018.
[25] N. Lee, T. Ajanthan, and P . Torr, “Snip: Single-shot network prun-
ing based on connection sensitivity,” in International Conference on
Learning Representations (ICLR) , 2019.
[26] J. Frankle and M. Carbin, “The lottery ticket hypothesis: Finding
sparse, trainable neural networks,” in International Conference on
Learning Representations (ICLR) , 2019.
[27] V . Ramanujan, M. Wortsman, A. Kembhavi, A. Farhadi, and
M. Rastegari, “What’s hidden in a randomly weighted neural
network?” in IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) , 2020, pp. 11 893–11 902.
[28] H. Zhou, J. Lan, R. Liu, and J. Yosinski, “Deconstructing lottery
tickets: Zeros, signs, and the supermask,” in Advances in Neural
Information Processing Systems (NeurIPS) , 2019, pp. 3597–3607.
[29] L. Orseau, M. Hutter, and O. Rivasplata, “Logarithmic pruning is
all you need,” in Advances in Neural Information Processing Systems
(NeurIPS) , 2020, pp. 2925–2934.
[30] K. Simonyan and A. Zisserman, “Very deep convolutional net-
works for large-scale image recognition,” in International Confer-
ence on Learning Representations (ICLR) , 2015.

--- PAGE 14 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 14
[31] A. Krizhevsky, G. Hinton et al. , “Learning multiple layers of
features from tiny images,” 2009.
[32] C. Wang, G. Zhang, and R. Grosse, “Picking winning tickets before
training by preserving gradient flow,” in International Conference on
Learning Representations (ICLR) , 2020.
[33] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient
convolutional neural networks for mobile vision applications,”
arXiv preprint arXiv:1704.04861 , 2017.
[34] B. Hassibi and D. Stork, “Second order derivatives for network
pruning: Optimal brain surgeon,” in Advances in Neural Information
Processing Systems (NeurIPS) , 1992, pp. 164–171.
[35] G. Thimm and E. Fiesler, “Evaluating pruning methods,” in Inter-
national Symposium on Artificial Neural Networks (ISANN) , 1995, pp.
20–25.
[36] P . Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, “Pruning
convolutional neural networks for resource efficient inference,” in
International Conference on Learning Representations (ICLR) , 2017.
[37] Y. Guo, A. Yao, and Y. Chen, “Dynamic network surgery for
efficient dnns,” arXiv preprint arXiv:1608.04493 , 2016.
[38] S. Srinivas, A. Subramanya, and R. Venkatesh Babu, “Training
sparse neural networks,” in IEEE Conference on Computer Vision
and Pattern Recognition Workshops (CVPRW) , 2017, pp. 138–145.
[39] X. Xiao, Z. Wang, and S. Rajasekaran, “Autoprune: Automatic
network pruning by regularizing auxiliary parameters,” Advances
in Neural Information Processing Systems (NeurIPS) , vol. 32, 2019.
[40] P . Savarese, H. Silva, and M. Maire, “Winning the lottery with
continuous sparsification,” Advances in Neural Information Process-
ing Systems (NeurIPS) , vol. 33, pp. 11 380–11 390, 2020.
[41] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P . Graf, “Pruning
filters for efficient convnets,” in International Conference on Learning
Representations (ICLR) , 2017.
[42] X. Ding, G. Ding, Y. Guo, and J. Han, “Centripetal sgd for pruning
very deep convolutional networks with complicated structure,” in
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,
2019, pp. 4943–4953.
[43] Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, T. K.-T. Cheng, and
J. Sun, “Metapruning: Meta learning for automatic neural network
channel pruning,” in International Conference on Computer Vision
(ICCV) , 2019, pp. 3296–3305.
[44] M. Lin, R. Ji, Y. Wang, Y. Zhang, B. Zhang, Y. Tian, and L. Shao,
“Hrank: Filter pruning using high-rank feature map,” in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) , 2020,
pp. 1529–1538.
[45] X. Ruan, Y. Liu, B. Li, C. Yuan, and W. Hu, “Dpfps: Dynamic and
progressive filter pruning for compressing convolutional neural
networks from scratch,” in AAAI Conference on Artificial Intelligence
(AAAI) , 2021, pp. 2495–2503.
[46] X. Ding, T. Hao, J. Tan, J. Liu, J. Han, Y. Guo, and G. Ding,
“Resrep: Lossless cnn pruning via decoupling remembering and
forgetting,” in International Conference on Computer Vision (ICCV) ,
2021, pp. 4510–4520.
[47] S. Guo, Y. Wang, Q. Li, and J. Yan, “Dmcp: Differentiable markov
channel pruning for neural networks,” in IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , 2020, pp. 1539–
1547.
[48] B. Li, B. Wu, J. Su, and G. Wang, “Eagleeye: Fast sub-net evaluation
for efficient neural network pruning,” in European Conference on
Computer Vision (ECCV) . Springer, 2020, pp. 639–654.
[49] J. Yu, A. Lukefahr, D. Palframan, G. Dasika, R. Das, and S. Mahlke,
“Scalpel: Customizing dnn pruning to the underlying hardware
parallelism,” ACM SIGARCH Computer Architecture News , vol. 45,
no. 2, pp. 548–560, 2017.
[50] H. Mao, S. Han, J. Pool, W. Li, X. Liu, Y. Wang, and W. J. Dally,
“Exploring the regularity of sparse structure in convolutional
neural networks,” arXiv preprint arXiv:1705.08922 , 2017.
[51] S. Zagoruyko and N. Komodakis, “Wide residual networks,” arXiv
preprint arXiv:1605.07146 , 2016.
[52] M. Ye, L. Wu, and Q. Liu, “Greedy optimization provably wins
the lottery: Logarithmic number of winning tickets is enough,” in
Advances in Neural Information Processing Systems (NeurIPS) , 2020,
pp. 16 409–16 420.
[53] Y. Bengio, N. L ´eonard, and A. Courville, “Estimating or propagat-
ing gradients through stochastic neurons for conditional compu-
tation,” arXiv preprint arXiv:1308.3432 , 2013.[54] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the
value of network pruning,” in International Conference on Learning
Representations (ICLR) , 2019.
[55] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classi-
fication with deep convolutional neural networks,” in Advances
in Neural Information Processing Systems (NeurIPS) , 2012, pp. 1106–
1114.
[56] M. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and
T. Blankevoort, “Up or down? adaptive rounding for post-training
quantization,” in International Conference on Machine Learning
(ICML) . PMLR, 2020, pp. 7197–7206.
[57] Y. Zhang, M. Lin, M. Chen, F. Chao, and R. Ji, “Optimizing
gradient-driven criteria in network sparsity: Gradient is all you
need,” arXiv preprint arXiv:2201.12826 , 2022.
[58] S. Hayou, J.-F. Ton, A. Doucet, and Y. W. Teh, “Pruning un-
trained neural networks: Principles and analysis,” arXiv preprint
arXiv:2002.08797 , 2020.
[59] M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for con-
volutional neural networks,” in International conference on machine
learning (ICML) . PMLR, 2019, pp. 6105–6114.
[60] G. Bellec, D. Kappel, W. Maass, and R. Legenstein, “Deep rewiring:
Training very sparse deep networks,” in International Conference on
Learning Representations (ICLR) , 2018.
[61] I. Loshchilov and F. Hutter, “Sgdr: Stochastic gradient descent
with warm restarts,” in International Conference on Learning Rep-
resentations (ICLR) , 2017.
[62] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch:
An imperative style, high-performance deep learning library,” in
Advances in Neural Information Processing Systems (NeurIPS) , 2019,
pp. 8026–8037.
Yuxin Zhang received the B.E. degree in Com-
puter Science, School of Informatics, Xiamen
University, Xiamen, China, in 2020. He is cur-
rently pursuing the P .H.D degree with Xiamen
University, China. His publications on top-tier
conferences/journals include IEEE TPAMI, IEEE
TNNLS, NeurIPS, ICLR, ICML, ICCV, IJCAI and
so on. His research interests include computer
vision and neural network compression & accel-
eration.
Mingbao Lin finished his M.S.-Ph.D. study and
obtained the Ph.D. degree in intelligence science
and technology from Xiamen University, Xiamen,
China, in 2022. Earlier, he received the B.S.
degree from Fuzhou University, Fuzhou, China,
in 2016.
He is currently a senior researcher with the
Tencent Y outu Lab, Shanghai, China. His publi-
cations on top-tier conferences/journals include
IEEE TPAMI, IJCV, IEEE TIP , IEEE TNNLS,
CVPR, NeurIPS, AAAI, IJCAI, ACM MM and so
on. His current research interest is to develop efficient vision model, as
well as information retrieval.

--- PAGE 15 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 15
Yunshan Zhong received the B.Sc degree in
Software Engineering from the Beijing Insti-
tute of Technology, Beijing, China in 2017, and
the M.S. degree in Software Engineering from
Peking University, Beijing, China in 2020. He
is currently a second-year Ph.D. student in the
MAC lab, the Institute of Artificial Intelligence,
Xiamen University, China, under the supervision
of Prof. Rongrong Ji. He has published multiple
peer-reviewed papers on top-tier conferences
including CVPR, ICCV, and ECCV. His current
research interest is model compression.
Fei Chao received the B.Sc. degree in mechan-
ical engineering from the Fuzhou University, P .
R. China, and the M.Sc. Degree with distinc-
tion in computer science from the University of
Wales, UK, in 2004 and 2005, respectively, and
the Ph.D. degree in robotics from the Aberys-
twyty University, Wales, UK in 2009. He was
a Research Associate under the supervision of
Professor Mark H. Lee at the Aberystwyth Uni-
versity from 2009 to 2010. He is currently an
Assistant Professor with the Cognitive Science
Department, at the Xiamen University, P . R. China. He has published
about 20 peer-reviewed journal and conference papers. His research in-
terests include developmental robotics, machine learning, and optimiza-
tion algorithms. He is the Vice Chair of the IEEE Computer Intelligence
Society Xiamen Chapter. Also, he is a member of ACM and CCF .
Rongrong Ji (Senior Member, IEEE) is a Nan-
qiang Distinguished Professor at Xiamen Uni-
versity, the Deputy Director of the Office of Sci-
ence and Technology at Xiamen University, and
the Director of Media Analytics and Computing
Lab. He was awarded as the National Science
Foundation for Excellent Y oung Scholars (2014),
the National Ten Thousand Plan for Y oung Top
Talents (2017), and the National Science Foun-
dation for Distinguished Y oung Scholars (2020).
His research falls in the field of computer vision,
multimedia analysis, and machine learning. He has published 50+ pa-
pers in ACM/IEEE Transactions, including TPAMI and IJCV, and 100+
full papers on top-tier conferences, such as CVPR and NeurIPS. His
publications have got over 10K citations in Google Scholar. He was the
recipient of the Best Paper Award of ACM Multimedia 2011. He has
served as Area Chairs in top-tier conferences such as CVPR and ACM
Multimedia. He is also an Advisory Member for Artificial Intelligence
Construction in the Electronic Information Education Committee of the
National Ministry of Education.
