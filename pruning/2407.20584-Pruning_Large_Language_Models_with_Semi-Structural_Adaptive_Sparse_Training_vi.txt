# 2407.20584.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2407.20584.pdf
# Kích thước tệp: 871854 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Cắt tỉa Mô hình Ngôn ngữ Lớn với Huấn luyện Thưa thớt Thích ứng Bán-cấu trúc
Weiyu Huang, Hu Yuezhou, Guohao Jian, Jun Zhu, Jianfei Chen†
Khoa Khoa học và Công nghệ Máy tính, Viện AI, Trung tâm BNRist, Phòng thí nghiệm THBI,
Trung tâm ML Tsinghua-Bosch, Đại học Tsinghua
{hwy23, huyz21, jgh22 }@mails.tsinghua.edu.cn; {dcszj, jianfeic }@tsinghua.edu.cn

Tóm tắt
Thành công đáng kể của các Mô hình Ngôn ngữ Lớn (LLM) phụ thuộc rất nhiều vào quy mô đáng kể của chúng, điều này đặt ra những thách thức đáng kể trong quá trình triển khai mô hình về mặt độ trễ và tiêu thụ bộ nhớ. Gần đây, nhiều nghiên cứu đã cố gắng nén LLM bằng các phương pháp cắt tỉa một lần. Tuy nhiên, các phương pháp này thường gặp phải sự suy giảm hiệu suất đáng kể trong các tác vụ hiểu ngôn ngữ phức tạp, làm dấy lên lo ngại về tính khả thi của việc cắt tỉa trong LLM. Để giải quyết vấn đề này, chúng tôi đề xuất Adaptive Sparse Trainer (AST), một khung huấn luyện lại mới và hiệu quả được thiết kế riêng cho các mô hình thưa thớt bán-cấu trúc. AST cho phép các mô hình học được mặt nạ tối ưu trong quá trình cập nhật trọng số mà không phát sinh chi phí tính toán bổ sung. Hơn nữa, chúng tôi chứng minh rằng việc kết hợp chưng cất kiến thức cải thiện đáng kể hiệu quả huấn luyện lại và nâng cao hiệu suất mô hình dưới các ràng buộc tính toán cố định. Ngoài ra, một bộ tham số được khởi tạo tốt bổ sung được tích hợp để tăng cường thêm hiệu quả của mô hình. AST đạt được hiệu suất tối tân với chi phí huấn luyện tối thiểu. Khi áp dụng cho mô hình LLaMA2-7B, AST giảm khoảng cách perplexity và độ chính xác zero-shot giữa các mô hình dày đặc và thưa thớt bán-cấu trúc 2:4 xuống 0.6 và 1.16%, tương ứng, sử dụng ít hơn 0.4% token tiền huấn luyện và giờ GPU. Công trình của chúng tôi chứng minh tính khả thi của việc triển khai LLM thưa thớt bán-cấu trúc và đưa ra một giải pháp thay thế đầy hứa hẹn để đạt được các mô hình được nén cao khi kết hợp với các kỹ thuật lượng tử hóa hiện có.
Mã nguồn — https://github.com/thu-ml/Adaptive-Sparse-Trainer

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) dựa trên Transformer được trang bị để xử lý các tác vụ phức tạp (Devlin et al. 2018; Brown et al. 2020; Achiam et al. 2023) và thể hiện các khả năng nổi lên (Wei et al. 2022) do số lượng tham số ngày càng tăng của chúng. Tuy nhiên, sự tăng trưởng liên tục này về kích thước mô hình đặt ra những thách thức đáng kể cho việc triển khai thực tế. Cụ thể, tốc độ suy luận bị ảnh hưởng do yêu cầu tính toán và bộ nhớ ngày càng tăng. Điều này đã thúc đẩy một loạt nỗ lực phát triển các kỹ thuật nén mô hình hiệu quả nhằm giảm dấu chân bộ nhớ và giảm bớt các ràng buộc liên quan đến việc triển khai các mô hình quy mô lớn này.

Cắt tỉa mô hình (Frantar và Alistarh 2023; Han, Mao, và Dally 2016; Sun et al. 2023) là một kỹ thuật hiệu quả để nén LLM bằng cách đặt một tỷ lệ trọng số về zero, từ đó tuân thủ một mô hình thưa thớt cụ thể. Gần đây, độ thưa thớt N:M đã nổi lên như một loại mô hình thưa thớt bán-cấu trúc cung cấp sự cân bằng tối ưu giữa độ chính xác và hiệu quả phần cứng. Cụ thể, độ thưa thớt N:M chỉ giữ lại N phần tử khác zero trong mỗi nhóm M phần tử. Mô hình thưa thớt này có thể tăng tốc cả phép nhân ma trận và truy cập bộ nhớ, có khả năng nâng cao hiệu suất của cả quá trình điền trước và giải mã trên GPU thương mại. Mặc dù có tiềm năng của độ thưa thớt N:M, các phương pháp tối tân hiện tại để cắt tỉa LLM, như SparseGPT (Frantar và Alistarh 2023) và Wanda (Sun et al. 2023), sử dụng cách tiếp cận sau huấn luyện xác định mô hình thưa thớt theo từng lớp mà không có lan truyền ngược. Mặc dù các phương pháp này cải thiện hiệu quả, chúng có thể dẫn đến suy giảm hiệu suất đáng kể, đặc biệt trong các tác vụ thâm dụng kiến thức (Nowak et al. 2024), làm dấy lên lo ngại về tính khả thi của việc cắt tỉa LLM. Ngoài ra, trong khi huấn luyện lại các mô hình đã cắt tỉa đã thành công trong thời đại trước LLM (Wang, Wohlwend, và Lei 2019; Lee, Ajanthan, và Torr 2019; Evci et al. 2020), việc áp dụng nó cho các mô hình có hàng tỷ tham số vẫn còn ít được khám phá.

Mặc dù huấn luyện lại LLM thưa thớt có tiềm năng đáng kể, nó đưa ra một số thách thức độc đáo: (1) Huấn luyện lại tốn kém về mặt tính toán, đòi hỏi các kỹ thuật đảm bảo hội tụ nhanh chóng. (2) Mô hình đầu ra phải tuân thủ mô hình thưa thớt nghiêm ngặt, điều này thêm các ràng buộc bổ sung vào quá trình huấn luyện lại. (3) Để đạt được hiệu suất tối ưu, cả mặt nạ và trọng số đều phải có thể học được trong quá trình huấn luyện. (4) Cắt tỉa LLM có thể làm suy giảm các khả năng hiểu và lý luận ngôn ngữ thiết yếu, điều này khó khôi phục hơn đáng kể so với các chỉ số đơn giản hơn như perplexity. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp huấn luyện mới và hiệu quả, Adaptive Sparse Trainer (AST), được thiết kế để tạo ra LLM thưa thớt hiệu suất cao. Lần đầu tiên, chúng tôi chứng minh rằng LLM thưa thớt 2:4 được tiền huấn luyện có thể đạt được hiệu suất cạnh tranh không chỉ về mặt perplexity mà còn trong các tác vụ thâm dụng kiến thức đòi hỏi khắt khe hơn, khiến chúng trở nên khả thi cho việc triển khai thực tế. AST tích hợp quá trình chuyển đổi từ mô hình dày đặc sang thưa thớt trực tiếp vào quá trình huấn luyện, dần dần phân rã các trọng số không quan trọng về zero thông qua một cơ chế phân rã được thiết kế cẩn thận. Cách tiếp cận này cho phép hồi sinh các trọng số đã cắt tỉa trong quá trình huấn luyện, tạo điều kiện cho việc điều chỉnh mượt mà và động của mô hình kết nối trong khi tuân thủ cấu trúc thưa thớt N:M. Hơn nữa, AST áp dụng chưng cất kiến thức sử dụng mô hình dày đặc làm giáo viên, có thể tăng tốc đáng kể sự hội tụ của mô hình và ngăn các mô hình thưa thớt rơi vào các tối ưu cục bộ. Nó cũng giúp mô hình thưa thớt giữ lại kiến thức thế giới có giá trị và các đặc tính hiệu suất của mô hình dày đặc gốc, từ đó nâng cao khả năng tổng quát hóa và bù đắp cho việc sử dụng các bộ dữ liệu huấn luyện yếu hơn. Để nâng cao hiệu suất hơn nữa, một bộ tham số bổ sung được tích hợp bằng cách sử dụng thông tin từ các trọng số đã cắt tỉa. Khi áp dụng cho mô hình LLaMA2-7B, AST đạt được cấu hình thưa thớt 2:4 với mất mát hiệu suất tối thiểu, chứng minh rằng các mô hình nén vẫn có thể hoạt động hiệu quả trong thực tế. Mô hình của chúng tôi còn được hưởng lợi từ lượng tử hóa AWQ, đạt được tỷ lệ nén cạnh tranh với hiệu suất tối tân.

Các đóng góp chính của công trình này như sau:
• Chúng tôi đề xuất Adaptive Sparse Trainer (AST), một khung cắt tỉa bán-cấu trúc mới được thiết kế để nén các mô hình ngôn ngữ lớn một cách hiệu quả trong khi duy trì hiệu suất cao.
• AST giới thiệu một bộ lập lịch phân rã dần (Annealing SR-STE) kết hợp với chưng cất kiến thức để tăng tốc sự hội tụ của mô hình và cải thiện hiệu suất trong các tác vụ phức tạp.
• Chúng tôi giới thiệu SLoRB, một kỹ thuật tăng cường hiệu suất mô hình bằng cách thêm một bộ tham số được khởi tạo tốt bổ sung.
• Khi áp dụng cho mô hình LLaMA2-7B, mô hình thưa thớt 2:4 của chúng tôi chỉ trải qua sự tăng perplexity 0.6 và giảm độ chính xác 1.16% trong các tác vụ zero-shot, với ít hơn 0.4% chi phí tiền huấn luyện.

2 Công trình liên quan
Cắt tỉa Mạng Cắt tỉa là một kỹ thuật phổ biến nhằm giảm kích thước mô hình và chi phí tính toán. Nó bắt nguồn từ các phương pháp như OBD (LeCun, Denker, và Solla 1990) và OBS (Hassibi, Stork, và Wolff 1993). Dựa trên các mô hình thưa thớt, các phương pháp cắt tỉa có thể được phân loại rộng rãi thành cắt tỉa không cấu trúc, có cấu trúc và bán-cấu trúc. Cắt tỉa không cấu trúc loại bỏ các trọng số riêng lẻ (Han, Mao, và Dally 2016; Paul et al. 2023), có thể duy trì hiệu suất ngay cả với độ thưa thớt cao. Tuy nhiên, do mô hình ngẫu nhiên của nó, các mô hình không cấu trúc khó tăng tốc. Mặt khác, cắt tỉa có cấu trúc (Liu et al. 2017; Molchanov et al. 2019; Nova, Dai, và Schuurmans 2023; Shen et al. 2022) loại bỏ toàn bộ neuron, bộ lọc hoặc đầu attention, dẫn đến các mô hình dễ tăng tốc hơn trên phần cứng tiêu chuẩn nhưng thường gặp phải mất mát hiệu suất nghiêm trọng. Độ thưa thớt bán-cấu trúc (ví dụ: độ thưa thớt N:M) (Hubara et al. 2021) đã được áp dụng như một sự đánh đổi giữa hiệu suất và đạt được tăng tốc thực tế. Gần đây, một loạt công trình (Frantar và Alistarh 2023; Sun et al. 2023; Zhang et al. 2024a,b) đã đạt được tiến bộ trong việc cắt tỉa LLM với hàng tỷ tham số. Tuy nhiên, các mô hình đã cắt tỉa từ các phương pháp không cần huấn luyện này vẫn còn thiếu sót trong hiệu suất zero-shot phức tạp.

Huấn luyện lại Mô hình Đã cắt tỉa Một hướng nghiên cứu khác (Han et al. 2015; Singh và Alistarh 2020; Renda, Frankle, và Carbin 2020; Zhou et al. 2023) đã tập trung vào huấn luyện lại các mô hình đã cắt tỉa để nâng cao hiệu suất. Trong khi huấn luyện lại đã được chứng minh là hiệu quả với các mô hình nhỏ hơn và các tác vụ đơn giản hơn (Kurtic và Alistarh 2022; Zhu và Gupta 2018), nó thường liên quan đến các bước huấn luyện bổ sung (Frankle và Carbin 2019) hoặc giới thiệu các tham số bổ sung cho quá trình cắt tỉa (Shi et al. 2023), điều này hạn chế ứng dụng của nó đối với các mô hình quy mô lớn do yêu cầu tài nguyên tính toán đáng kể. Các phương pháp huấn luyện lại khác tập trung vào độ thưa thớt không cấu trúc, có thể gặp khó khăn khi được đưa ra các mô hình thưa thớt nghiêm ngặt hơn (Lee, Ajanthan, và Torr 2019; Evci et al. 2020). Gần đây, Sheared LLaMA (Xia et al. 2024) đã sử dụng quy trình cắt tỉa có cấu trúc hai giai đoạn để huấn luyện các mô hình vượt trội hơn những mô hình có kích thước tương tự. Tuy nhiên, họ sử dụng giai đoạn cắt tỉa tốn kém về mặt tính toán, không phù hợp cho độ thưa thớt chi tiết hơn. Công trình của chúng tôi đề xuất một quy trình cắt tỉa nhẹ cho phép huấn luyện lại các mô hình ngôn ngữ lớn thưa thớt bán-cấu trúc với chi phí huấn luyện tối thiểu.

Kết hợp Cắt tỉa với Lượng tử hóa Các mô hình đã cắt tỉa có thể được nén thêm bằng lượng tử hóa. Các phương pháp trước đây như Deep Compression (Han, Mao, và Dally 2016) đã kết hợp cắt tỉa và lượng tử hóa để giảm đáng kể kích thước của mạng neural. Công trình gần đây hơn (Frantar và Alistarh 2023; Sun et al. 2023; Guo et al. 2024) đã kết hợp độ thưa thớt với lượng tử hóa trong các mô hình ngôn ngữ lớn. Trong công trình của chúng tôi, chúng tôi báo cáo kết quả sử dụng lượng tử hóa AWQ (Lin et al. 2024) với mô hình thưa thớt bán-cấu trúc của chúng tôi.

3 Phương pháp
Chúng tôi bắt đầu bằng việc xem xét lại một cách tiếp cận ngây thơ để huấn luyện các mô hình thưa thớt trước khi giới thiệu phương pháp của chúng tôi. Toàn bộ quy trình huấn luyện được minh họa trong Hình 1.

3.1 Huấn luyện Thưa thớt
Cắt tỉa trọng số mô hình tương đương với việc nhân chúng với một mặt nạ theo từng phần tử. Ví dụ, hãy xem xét phép nhân ma trận trong một lớp tuyến tính:
Z=XW⊤, Z∈RN×D, X∈RN×C, W ∈RD×C,
trong đó X, W và Z biểu diễn đầu vào mô hình, ma trận trọng số và activation đầu ra, tương ứng. Ma trận trọng số đã cắt tỉa có thể được biểu diễn như:
˜W=m(W)⊙W, m (W)∈ {0,1}D×C,
trong đó m(·) là một ánh xạ chọn mặt nạ dựa trên ma trận trọng số W. Trong công trình này, chúng tôi tập trung vào độ thưa thớt N:M (Hubara et al. 2021), trong đó có N phần tử khác zero trong mỗi M trọng số liên tiếp trong cùng một hàng. Tuy nhiên, khi thực hiện bước lan truyền ngược cho một mô hình thưa thớt với phân biệt tự động, gradient không thể truyền qua mặt nạ rời rạc m(W). Một cách tiếp cận đơn giản để giải quyết vấn đề này là straight-through estimator (STE) (Bengio, Léonard, và Courville 2013) cập nhật các tham số bằng gradient đối với trọng số có mặt nạ ˜Wt, trong đó Wt là tham số tại lần lặp t:
Wt+1←Wt−γtg(˜Wt), (1)
ở đây g(˜Wt) biểu diễn gradient đối với ˜Wt và γt chỉ tốc độ học tại lần lặp t.

Một phương pháp trước đây (Sun et al. 2023) sử dụng mặt nạ cố định có nguồn gốc từ các kỹ thuật cắt tỉa một lần và chỉ cập nhật các tham số còn lại bằng cách sử dụng loss mô hình ngôn ngữ. Tuy nhiên, chiến lược này có hai hạn chế đáng kể. Thứ nhất, nó loại bỏ thông tin có giá trị được nhúng trong các trọng số đã cắt tỉa và ngăn cản một quá trình chuyển đổi mượt mà từ mô hình dày đặc sang thưa thớt, dẫn đến sự hội tụ chậm hơn và hiệu suất không tối ưu. Thứ hai, việc chỉ dựa vào loss mô hình ngôn ngữ trong quá trình huấn luyện lại làm tăng nguy cơ mô hình thưa thớt bị mắc kẹt trong một tối ưu cục bộ, dẫn đến kết quả kém.

Để giải quyết những vấn đề này, chúng tôi duy trì tất cả trọng số mô hình và chọn mặt nạ một cách linh hoạt trong khi dần dần phân rã các trọng số không thiết yếu về zero, từ đó cho phép một quá trình chuyển đổi mượt mà (§3.2). Ngoài ra, chúng tôi tận dụng chưng cất kiến thức để hướng dẫn mô hình hướng tới một tối ưu toàn cục, tăng tốc đáng kể sự hội tụ dưới ngân sách tính toán cố định (§3.3). Cuối cùng, chúng tôi cung cấp một phương pháp tùy chọn để nâng cao hiệu suất mô hình hơn nữa bằng cách kết hợp một bộ tham số được khởi tạo tốt bổ sung, như chi tiết trong (§3.4).

3.2 Học Mặt nạ Thích ứng với Annealing SR-STE
Như được nhấn mạnh trong công trình trước đây (Frankle và Carbin 2019; Evci et al. 2020), mô hình kết nối cũng quan trọng như chính các trọng số trong việc xác định hiệu suất của các mô hình thưa thớt. Do đó, chúng tôi cho phép mô hình học một cách thích ứng mặt nạ tối ưu trong quá trình huấn luyện thay vì cố định nó vĩnh viễn. Cụ thể, chúng tôi tính toán lại m(Wt) dựa trên tiêu chí độ lớn mỗi ∆t lần lặp và thêm một hạng phân rã vào các trọng số có mặt nạ. Trực giác đằng sau cách tiếp cận này rất đơn giản: các trọng số quan trọng thường nhận được gradient mạnh có cùng dấu trong quá trình huấn luyện, khiến độ lớn của chúng tăng lên bất chấp sự phân rã, và cuối cùng chúng sẽ được hồi sinh. Ngược lại, các trọng số ít quan trọng hơn có xu hướng nhận được tín hiệu gradient yếu hơn, hỗn hợp, dẫn đến việc chúng phân rã về zero và vẫn bị che mặt nạ dưới cài đặt của chúng tôi. Chúng tôi xây dựng dựa trên SR-STE (Zhou et al. 2021) bằng cách áp dụng phân rã L2 cho các trọng số có mặt nạ, vì cách tiếp cận này giúp bảo tồn hiệu suất tổng thể của mô hình.

Tuy nhiên, việc chọn cường độ phân rã phù hợp là thách thức. Nếu tín hiệu phân rã quá mạnh, các trọng số có mặt nạ sẽ khó tái tạo, dẫn đến một mặt nạ gần như cố định. Mặt khác, nếu tín hiệu phân rã quá yếu, mô hình sẽ không thể lọc ra các trọng số quan trọng, dẫn đến dao động mạnh cản trở sự hội tụ của mô hình. Để giải quyết tình thế tiến thoái này giữa sự ổn định và khám phá, chúng tôi đề xuất thêm một lịch phân rã di động, với cường độ phân rã yếu hơn ở đầu để khuyến khích mô hình khám phá các mô hình kết nối khác nhau và tín hiệu phân rã mạnh hơn về cuối để tạo điều kiện cho sự hội tụ của mô hình.

Một cách chính thức, chúng tôi cập nhật trọng số mô hình với phương trình sau:
λW(t) =αt, nếu 0≤t≤T0,
αT0, nếu T0≤t≤T1, (2)
Wt+1←Wt−γt[g(˜Wt) +λW(t)m(Wt)⊙Wt], (3)
trong đó λW(t) biểu thị hệ số phân rã, m(Wt) biểu thị mặt nạ cho các trọng số đã cắt tỉa tại lần lặp t, T1 biểu thị tổng số batch huấn luyện, và T0 biểu diễn batch mà tại đó hệ số phân rã ngừng tăng. So với STE trong Phương trình 1, các trọng số có mặt nạ nhận được tín hiệu phân rã tỷ lệ với hệ số phân rã λW(t), bắt đầu nhỏ ở đầu và tăng lên sau đó. Phương pháp này, được gọi là Annealing SR-STE (ARS-STE), đã được chứng minh là cải thiện hiệu suất mô hình so với SR-STE ngây thơ. Chúng tôi cũng cung cấp phân tích chi tiết về hành vi dao động mặt nạ trong quá trình huấn luyện trong Phần 7 của Phụ lục.

Chúng tôi đã thử nghiệm với các tiêu chí cắt tỉa một lần khác nhau kết hợp thông tin activation (Frantar và Alistarh 2023; Zhang et al. 2024a) để chọn mặt nạ. Tuy nhiên, tiêu chí độ lớn cổ điển liên tục vượt trội hơn các phương pháp này về cả hiệu quả tính toán và hiệu suất. Vì các cách tiếp cận dựa trên activation phát sinh chi phí tính toán cao hơn và ít đáng tin cậy hơn do dao động activation do cập nhật trọng số gây ra, dẫn đến hiệu suất suy giảm trong các giai đoạn huấn luyện sau.

Vì SR-STE ngây thơ chỉ tương thích với bộ tối ưu SGD, chúng tôi giới thiệu các điều chỉnh thêm để hỗ trợ các bộ tối ưu tiên tiến hơn như AdamW. Cụ thể, vì AdamW duy trì cả moment bậc nhất và bậc hai của gradient, chúng tôi tách hạng phân rã trọng số khỏi moment bậc nhất thay vì trực tiếp thêm phân rã vào gradient, như đã làm trong (Hu et al. 2024). Sự tách biệt này đảm bảo rằng hạng phân rã trọng số chỉ phụ thuộc vào trọng số hiện tại, tránh can thiệp vào tính toán momentum. Sau đó chúng tôi sử dụng tín hiệu bậc nhất đã tách để cập nhật moment bậc hai. Các biểu thức chi tiết và giải thích được cung cấp trong Phần 6 của Phụ lục.

3.3 Giảm thiểu Tình thế Tiến thoái Huấn luyện lại thông qua Chưng cất Kiến thức
Một sự khác biệt chính giữa tiền huấn luyện và huấn luyện lại các mô hình đã cắt tỉa nằm ở cách các tham số của chúng được khởi tạo. Trong cài đặt tiền huấn luyện, các tham số thường được lấy mẫu ngẫu nhiên, trong khi trong huấn luyện lại, chúng được kế thừa từ một mô hình được huấn luyện tốt. Do đó, các mô hình đã cắt tỉa giữ lại một phần các mô hình thu được trong quá trình tiền huấn luyện.

Chúng tôi phát hiện ra rằng tính năng này có xu hướng đẩy các mô hình đã cắt tỉa vào các tối ưu cục bộ. Cụ thể, trong khi huấn luyện lại có thể đạt được sự hội tụ ban đầu nhanh hơn nhiều, nó thường không thể đạt được trạng thái tối ưu sau đó; chúng tôi gọi hiện tượng này là Tình thế Tiến thoái Huấn luyện lại. Như minh họa trong Hình 2, mặc dù sử dụng loss cross-entropy trong quá trình huấn luyện lại ban đầu dẫn đến giảm loss huấn luyện nhanh hơn so với tiền huấn luyện, perplexity tập kiểm tra vẫn không ổn định và cao. Chúng tôi cho rằng điều này là do, không giống như các mô hình được khởi tạo ngẫu nhiên, mô hình đã cắt tỉa dễ bị overfitting hơn trên dữ liệu hạn chế có sẵn ban đầu do kiến thức trước của nó. Việc overfitting này cản trở khả năng học các tính năng toàn cục và đạt được sự hội tụ tối ưu của mô hình. Đáng chú ý, vấn đề này vẫn tồn tại ngay cả khi sử dụng tốc độ học nhỏ hơn đáng kể so với tiền huấn luyện, cho thấy rằng việc chỉ giảm tốc độ học là không đủ để giải quyết vấn đề.

Như một giải pháp, chúng tôi thấy rằng việc áp dụng loss KL-divergence (Kullback và Leibler 1951) có thể giải quyết vấn đề này. Không giống như loss mô hình ngôn ngữ, loss KL-divergence đo lường sự khác biệt giữa hai phân phối xác suất, cung cấp tín hiệu phong phú hơn giúp giảm thiểu overfitting trong các giai đoạn đầu của huấn luyện. Do đó, chúng tôi sử dụng hàm loss sau:
Llogit=DKL(pθt||pθs) =1/(B×S) ∑B×seqi=1 pθt(xi) log[pθt(xi)/pθs(xi)],
L=αLlogit+ (1−α)Ltask, (4)
trong đó Ltask là loss cross-entropy, B là kích thước batch, S là độ dài chuỗi, và pθt và pθs là các phân phối xác suất của mô hình giáo viên và học sinh, tương ứng.

Chúng tôi quan sát thấy rằng Tình thế Tiến thoái Huấn luyện lại rõ rệt hơn trong các mô hình nhỏ hơn; do đó, chúng tôi thường áp dụng α lớn hơn cho các mô hình này. Chúng tôi cũng thử nghiệm với các phương pháp chưng cất khác nhau sử dụng thông tin trung gian để chưng cất. Tuy nhiên, chúng tôi thấy rằng việc thêm ràng buộc vào các đầu ra trung gian cản trở khả năng tổng quát hóa của mô hình và dẫn đến kết quả không mong muốn.

3.4 Sparse Low-Rank Boosting
Mô hình đã cắt tỉa với độ thưa thớt 2:4, chỉ giữ lại một nửa tham số dưới các ràng buộc cấu trúc nghiêm ngặt, trải qua sự giảm năng lực biểu đạt. Để giảm thiểu điều này, chúng tôi kết hợp một tham số hình dạng LoRA (Low-Rank Adaptation) (Hu et al. 2022) được huấn luyện cùng với các tham số gốc, giúp thu hẹp khoảng cách hiệu suất giữa các mô hình dày đặc và thưa thớt với chi phí bộ nhớ tối thiểu. Không giống như fine-tuning LoRA truyền thống, trong đó các tham số mô hình gốc được đóng băng và chỉ các adapter được huấn luyện, cách tiếp cận của chúng tôi huấn luyện đồng thời cả tham số thưa thớt và trọng số adapter. Cách tiếp cận này nhận ra rằng trong khi phương pháp LoRA cổ điển đóng băng mô hình gốc để ngăn overfitting và quên thảm khốc trong các tác vụ downstream, phương pháp của chúng tôi nâng cao khả năng tổng quát hóa bằng cách huấn luyện đồng thời cả trọng số bổ sung và gốc trên bộ dữ liệu tiền huấn luyện, từ đó cho phép huấn luyện đồng thời.

Một khía cạnh đáng chú ý khác của phương pháp chúng tôi là chiến lược khởi tạo. LoRA cổ điển thường sử dụng các kỹ thuật khởi tạo ngẫu nhiên, như khởi tạo Xavier (Glorot và Bengio 2010) hoặc khởi tạo Kaiming (He et al. 2015). Tuy nhiên, trong cách tiếp cận của chúng tôi, chúng tôi tận dụng các trọng số đã cắt tỉa làm thông tin bổ sung để khởi tạo trọng số adapter, từ đó tăng tốc quá trình huấn luyện. Cho rằng trong độ thưa thớt 2:4, mỗi neuron chỉ có thể truy cập một nửa số đầu vào, chúng tôi thấy rằng việc kết hợp thông tin bổ sung để giữ lại thông tin bậc nhất của trọng số giúp bảo tồn năng lực của mô hình. Cụ thể, đối với ma trận trọng số W có kích thước N×d và ma trận mặt nạ tương ứng M, chúng tôi chọn rank r là d/k, trong đó k có thể là 64, 32, 16, v.v. k nhỏ hơn cải thiện hiệu suất nhưng cũng tăng việc sử dụng bộ nhớ. Chúng tôi chọn ma trận chiếu X có kích thước d/k×d và ma trận trọng số SLoRB S có kích thước N×d/k. Các ma trận X và S được định nghĩa như sau:
xij=1, nếu i·k≤j≤(i+ 1)·k−1, 0, ngược lại, (5)
Sij=1/k ∑(j+1)·k−1p=j·k Wip· ¬Mip. (6)

Chúng tôi định nghĩa Nhóm Gij là các phần tử từ j·k đến (i+ 1)·k−1 trong hàng i. Như minh họa trong Hình 3, mỗi trọng số SLoRB Sij được broadcast trong Nhóm Gij. Bằng cách đặt Sij là trung bình của tất cả các trọng số đã cắt tỉa trong Nhóm Gij, thiết kế này đảm bảo rằng tổng trung bình của các nhóm Gij vẫn nhất quán sau khi cắt tỉa. Mặc dù các phương pháp khởi tạo khác nhau cuối cùng có thể hội tụ đến kết quả tương tự với đủ bước huấn luyện, phương pháp của chúng tôi hội tụ nhanh hơn, làm cho nó đặc biệt có lợi khi ngân sách tính toán bị hạn chế, như được chứng minh trong Phần 4 của Phụ lục. Chúng tôi gọi phương pháp này là Sparse Low-Rank Boosting (SLoRB), là một kỹ thuật tùy chọn đánh đổi chi phí bộ nhớ để có hiệu suất tăng cường. Mã giả hoàn chỉnh cho phương pháp của chúng tôi được cung cấp trong Thuật toán 1.

4 Thí nghiệm
4.1 Thiết lập Thí nghiệm
Cấu hình Mô hình. Chúng tôi báo cáo hiệu suất của Adaptive Sparse Trainer (AST) trên ba họ mô hình LLM khác nhau: LLaMA2-7B (Touvron et al. 2023), OPT (Zhang et al. 2022), và GPT2 (Brown et al. 2020). Chúng tôi trình bày kết quả cho hai mô hình thưa thớt khác nhau: AST-Naive, tuân thủ mô hình thưa thớt 2:4 nghiêm ngặt mà không có trọng số bổ sung, và AST-Boosted, kết hợp các trọng số SLoRB bổ sung. Đối với các mô hình AST-Boosted, chúng tôi đã chọn k= 16, giới thiệu thêm 12,5% tham số. Các siêu tham số tối ưu được xác định thông qua tìm kiếm lưới, với các siêu tham số cụ thể và chi tiết huấn luyện được cung cấp trong Phần 3 của Phụ lục.

Dữ liệu. Để huấn luyện các mô hình nhỏ hơn như họ mô hình OPT và GPT2, chúng tôi sử dụng bộ dữ liệu C4 (Raffel et al. 2020). Đối với mô hình LLaMA2-7B, chúng tôi sử dụng bộ dữ liệu toàn diện hơn, RedPajama-v1, bao gồm dữ liệu từ bảy miền: CommonCrawl, C4, GitHub, Wikipedia, Books, ArXiv, và StackExchange. Ngoài ra, chúng tôi tận dụng tính năng batching động được cung cấp trong codebase ShearedLLaMA (Xia et al. 2024).

Baseline. Đối với các mô hình GPT2 và OPT, chúng tôi so sánh các phương pháp của mình với cả các cách tiếp cận không cần huấn luyện và dựa trên huấn luyện. Trong số các phương pháp không cần huấn luyện, chúng tôi bao gồm so sánh với SparseGPT (Frantar và Alistarh 2023) và Wanda (Sun et al. 2023). Đối với các phương pháp dựa trên huấn luyện, do thiếu kết quả hiện có cho việc huấn luyện lại các mô hình ngôn ngữ sinh với độ thưa thớt N:M, chúng tôi đã triển khai cắt tỉa độ lớn lặp (Frankle và Carbin 2019) và cắt tỉa độ lớn dần (Kurtic và Alistarh 2022), cả hai đều có thể được điều chỉnh để đạt được độ thưa thớt mong muốn. Để đảm bảo so sánh công bằng về mặt chi phí tính toán, chúng tôi báo cáo kết quả cho các phương pháp dựa trên huấn luyện bao gồm chưng cất, vì chúng tôi đã quan sát thấy rằng việc kết hợp chưng cất cải thiện đáng kể hiệu suất. Do các ràng buộc tính toán, chúng tôi chỉ báo cáo kết quả của các baseline dựa trên huấn luyện cho các mô hình GPT2 và OPT.

Đối với mô hình LLaMA2-7B, chúng tôi so sánh cách tiếp cận của mình với các LLM dày đặc tiền huấn luyện mạnh có số lượng tham số khác zero tương tự. Ngoài ra, chúng tôi bao gồm so sánh với mô hình LLaMA-7B thưa thớt 2:4 được cung cấp trong bài báo Wanda (Sun et al. 2023).

Đánh giá. Chúng tôi đánh giá perplexity WikiText-2 cho tất cả các mô hình và đánh giá hiệu suất zero-shot và few-shot trên LLaMA2-7B bằng LM Harness của EleutherAI (Gao et al. 2021). Các tác vụ đánh giá của chúng tôi bao gồm zero-shot ARC Easy (Clark et al. 2018), OpenBookQA (Mihaylov et al. 2018), WinoGrande (Sakaguchi et al. 2021), RTE từ benchmark GLUE (Wang et al. 2018), HellaSwag (Zellers et al. 2019), ARC Challenge (Clark et al. 2018), BoolQ (Clark et al. 2019), cũng như các tác vụ phức tạp hơn như benchmark MMLU (Hendrycks et al. 2021a), GSM8K (Cobbe et al. 2021), và MATH (Hendrycks et al. 2021b).

4.2 Kết quả Chính
Mô hình Ngôn ngữ Trong Bảng 1, chúng tôi so sánh kết quả perplexity của các mô hình thưa thớt với các baseline. Các phương pháp của chúng tôi vượt trội đáng kể so với cả các cách tiếp cận không cần huấn luyện và dựa trên huấn luyện trên các mô hình khác nhau, đạt được kết quả gần với baseline dày đặc. Một lý do hợp lý tại sao các phương pháp dựa trên huấn luyện như IMP thất bại là trong khi vé số có thể tồn tại cho một mô hình được khởi tạo ngẫu nhiên, điều này có thể không đúng cho một mô hình được huấn luyện tốt. Đối với baseline dần dần, việc áp dụng một tỷ lệ bước huấn luyện ở độ thưa thớt thấp có thể không cung cấp cho mô hình đủ thời gian để hội tụ.

Kết quả Zero-shot Trong Bảng 2, chúng tôi trình bày kết quả độ chính xác của bảy tác vụ zero-shot cho AST và các mô hình baseline. Phương pháp của chúng tôi hoạt động tốt nhất trong hầu hết các tác vụ. Cần lưu ý rằng các mô hình thưa thớt của chúng tôi liên tục vượt trội hơn các mô hình dày đặc nhỏ hơn có số lượng tham số tương tự, cho thấy một cách tiếp cận đầy hứa hẹn để có được các mô hình hiệu quả tham số. Hơn nữa, phương pháp của chúng tôi chỉ yêu cầu một lượng token huấn luyện tối thiểu để hội tụ. Ví dụ, khi huấn luyện mô hình LLaMA2-7B, chúng tôi chỉ sử dụng 7B token, ít hơn 0,4% so với những token được sử dụng trong tiền huấn luyện, làm cho AST có thể áp dụng cho các mô hình mã nguồn mở.

Phương pháp của chúng tôi cũng có thể được điều chỉnh một cách liền mạch với các kỹ thuật lượng tử hóa hiện tại để đạt được các mô hình được nén cực kỳ với mất mát hiệu suất tối thiểu. Chúng tôi cung cấp kết quả sử dụng các phương pháp lượng tử hóa AWQ trong phần 8 của Phụ lục.

4.3 Kết quả Bổ sung cho LLaMA2-7B
Các phát hiện gần đây (Nowak et al. 2024) đã cho thấy rằng các phương pháp cắt tỉa trước đây gặp phải suy giảm hiệu suất đáng kể trong các tác vụ thâm dụng kiến thức. Để giải quyết điều này, chúng tôi đã kiểm tra mô hình LLaMA2-7B của mình trên các tác vụ phức tạp hơn. Như được hiển thị trong Bảng 3, mô hình của chúng tôi bảo tồn hầu hết kiến thức từ mô hình dày đặc và tiếp tục hoạt động tốt trong các tác vụ thâm dụng kiến thức so với các phương pháp cắt tỉa trước đây. Điều này cung cấp bằng chứng mạnh mẽ rằng các mô hình đã cắt tỉa 2:4 có tiềm năng đáng kể, trái ngược với các quan sát trong các nghiên cứu trước đây.

4.4 Nghiên cứu Loại bỏ
Huấn luyện Mô hình Thưa thớt. Phương pháp AST-Naive của chúng tôi sử dụng chưng cất, Annealing SR-STE, và masking thích ứng trong quá trình huấn luyện. Trong Bảng 4, chúng tôi trình bày một nghiên cứu loại bỏ để đánh giá tác động của từng thành phần này. Để so sánh công bằng, chúng tôi áp dụng các bước huấn luyện bổ sung cho các phương pháp không chưng cất. Cụ thể, chúng tôi phân tích tác động của huấn luyện mà không có chưng cất, sử dụng hệ số phân rã SR-STE ngây thơ, và cố định mặt nạ trong quá trình huấn luyện, từng thành phần một cách riêng biệt. Ngoài ra, chúng tôi cung cấp kết quả cho baseline huấn luyện ngây thơ trong Hình 1 được đề cập ở trên. Chúng tôi chứng minh rằng cả ba cách tiếp cận đều đóng góp vào việc tăng hiệu suất so với huấn luyện ngây thơ trên các mô hình khác nhau.

Các Hàm Chưng cất Chúng tôi cũng thực hiện các nghiên cứu loại bỏ về các hàm chưng cất khác nhau được sử dụng trong công trình trước đây, bao gồm TinyBERT (Jiao et al. 2020), MobileBERT (Sun et al. 2020), và Sparse-Finetuning (Kurtic et al. 2023), sử dụng attention và hidden state để chưng cất. Ngoài ra, chúng tôi kiểm tra MiniLLM (Gu et al. 2024), sử dụng divergence KL ngược. Chúng tôi thấy rằng việc sử dụng thông tin trung gian trong quá trình chưng cất các mô hình ngôn ngữ sinh là có hại, và loss KL là đủ để có hiệu suất tối ưu. KL ngược mang lại hiệu suất tương tự như KL xuôi. Các mô tả chi tiết về các hàm loss chưng cất cho từng phương pháp này được cung cấp trong Phần 5 của Phụ lục.

4.5 Tăng tốc
Bảng 6 trình bày kết quả tăng tốc thu được bằng TensorRT-LLM. Chúng tôi đánh giá tăng tốc giải mã end-to-end thực tế trên hai kiến trúc GPU với mô hình LLaMA2-7B thưa thớt 2:4. Chúng tôi sử dụng throughput, được đo bằng số token được xử lý mỗi giây, làm chỉ số đánh giá chính. Trên một phạm vi độ dài đầu vào và đầu ra, mô hình thưa thớt 2:4 chứng minh sự tăng tốc tổng thể từ 1.33× đến 1.83× so với đối tác dày đặc của nó, làm nổi bật tiềm năng của nó cho việc triển khai thực tế.

5 Kết luận
Trong bài báo này, chúng tôi giới thiệu Adaptive Sparse Trainer (AST), một quy trình huấn luyện mới và hiệu quả cho các mô hình thưa thớt bán-cấu trúc. AST hiệu quả thu hẹp khoảng cách độ chính xác giữa LLM dày đặc và thưa thớt về mặt perplexity và độ chính xác trong các tác vụ zero-shot, trong khi giữ chi phí huấn luyện ở mức tối thiểu. Kết quả của chúng tôi chứng minh rằng việc cắt tỉa LLM là khả thi với mất mát hiệu suất tối thiểu trong các tác vụ thâm dụng kiến thức, và các mô hình thưa thớt bán-cấu trúc lớn có thể vượt trội hơn các mô hình dày đặc có tốc độ giải mã tương tự khi được hỗ trợ bởi phần mềm và phần cứng phù hợp. Mặc dù các phát hiện của chúng tôi đóng góp vào việc thúc đẩy huấn luyện lại các mô hình đã cắt tỉa với hàng tỷ tham số, chúng tôi thừa nhận rằng số lượng token huấn luyện hạn chế, do các ràng buộc tính toán, vẫn là một lĩnh vực để khám phá trong tương lai. Mở rộng công trình này sang các mô hình lớn hơn hoặc tăng số lượng token huấn luyện có thể cung cấp những hiểu biết có giá trị và nâng cao thêm hiệu quả của các phương pháp của chúng tôi.

--- TRANG 8 ---
Lời cảm ơn
Các tác giả muốn cảm ơn Ziteng Wang, Bingrui Li và Pengle Zhang vì những thảo luận hữu ích và gợi ý về triển khai mã. Công trình này được hỗ trợ bởi Dự án NSFC (Số 62376131).

6 Phụ lục
6.1 Nén Mô hình với Mô hình Thưa thớt Bán-cấu trúc Khác nhau
Trong các phần trước, chúng tôi chỉ kiểm tra các mô hình 2:4, nhưng về mặt lý thuyết, bất kỳ mô hình N:M nào cũng có thể nâng cao phép nhân ma trận với sự hỗ trợ phần cứng phù hợp. Điều đó nói rằng, các mô hình với các ràng buộc lỏng hơn (tức là M lớn hơn) thường hoạt động tốt hơn nhưng khó nén hiệu quả hơn. Chúng tôi nhằm xác định sự đánh đổi tối ưu và. Để duy trì tính công bằng trong so sánh, chúng tôi chỉ tập trung vào các mô hình đạt được tỷ lệ thưa thớt 50%, như 2:4, 4:8, v.v.

Cần lưu ý rằng việc triển khai các mô hình thưa thớt bán-cấu trúc có thể dẫn đến chi phí bộ nhớ tiềm ẩn. Lấy mô hình thưa thớt 2:4 làm ví dụ: mặc dù chỉ có hai phần tử khác zero trong số bốn trọng số cần được lưu trữ, thông tin vị trí bổ sung là cần thiết để khôi phục chính xác các trọng số nén này về ma trận gốc để tính toán. Việc lưu trữ các chỉ số này sẽ tiêu thụ một tỷ lệ bộ nhớ bổ sung. Một phương pháp ngây thơ sẽ là lưu trữ một bit làm mặt nạ cho mỗi phần tử, dẫn đến chi phí bộ nhớ 50% với một mô hình lượng tử hóa 4-bit ở độ thưa thớt 50%. Tuy nhiên, một cách tiếp cận hiệu quả hơn tồn tại: vì có 2 phần tử khác zero, có tổng cộng C(4,2) = 6 vị trí có thể, do đó chỉ cần thêm 3 bit là đủ.

Chúng tôi định nghĩa tỷ lệ nén là tỷ lệ phần trăm độ trễ bộ nhớ của mô hình nén so với mô hình gốc. Trong cài đặt của chúng tôi, chúng tôi nén các mô hình dày đặc với độ chính xác FP32 thành mô hình 4-bit với độ thưa thớt bán-cấu trúc n:2n; do đó, tỷ lệ nén có thể được công thức hóa như:
Cn=n∗4 +⌈log2C(2n,n)⌉(bit)/(2n∗32(bit))=1/16+⌈log2C(2n,n)⌉/(64n).

Thông qua công thức toán học (được cung cấp trong Phần 2 của Phụ lục), chúng tôi có thể cung cấp một cận trên gần đúng cho tỷ lệ nén cũng như tỷ lệ nén thực tế được dự đoán khi mở rộng sang các mô hình thưa thớt với N và M lớn hơn

log2C(2n,n) = log2(2n)!/(n!)2 ≤ log2(4n/√πn) = 2n−log2√πn.

Nếu chúng ta loại bỏ hàm ceiling để xấp xỉ:
Cn≈3/32−log2√πn/(64n).

Do đó, tỷ lệ nén xấp xỉ là một hàm tăng với cận trên C∗=3/32≈9.375%. Trong thực tế, để nâng cao nén hơn nữa, chúng tôi có thể sử dụng Huffman Encoding, tương tự như những gì đã được thực hiện trong Deep Compression (Han, Mao, và Dally 2016). Chúng tôi xây dựng một Cây Huffman với tần số phân bố đều, vì kết quả thực nghiệm hỗ trợ khẳng định của chúng tôi. Các tỷ lệ nén thực tế được hiển thị trong Bảng 7. Chúng tôi quan sát thấy rằng khi n tăng, hiệu suất cải thiện; tuy nhiên, mặc dù tỷ lệ nén tăng, nó vẫn dưới cận trên.

6.2 Chứng minh Cận Trên của Số Tổ hợp
Các chứng minh có nguồn gốc từ Stack Exchange. Chúng ta có phương trình sau:
1/(4n)C(2n,n)=(2n−1)!!/(2n)!!=∏k=1^n(1−1/(2k)).

Chúng ta bình phương hai vế của phương trình:
(1/(4n)C(2n,n))2=1/4∏k=2^n(1−1/(2k))2=1/4∏k=1^(n-1)(1−1/((2k+ 1)2))−1.

Sử dụng tích Weierstrass cho hàm cosin, chúng ta thu được:
∏k=1^∞(1−1/((2k+ 1)2))−1=4/π.

Do đó, nó theo sau rằng:
(1/(4n)C(2n,n))2=1/(πn)∏k≥n(1−1/((2k+ 1)2)) =1/(πn)∏k≥n(1 +1/((2k+ 2)2 k))−1.

Do đó chúng ta có
1/√πn≥1/(4n)C(2n,n).

6.3 Siêu tham số
Chúng tôi trình bày các siêu tham số và số lượng token được sử dụng để huấn luyện mô hình của chúng tôi trong Bảng 8. Số lần lặp tăng T0 được chọn là một phần tư của tổng số lần lặp huấn luyện T1.

6.4 Khởi tạo SLoRB
Chúng tôi thực hiện một nghiên cứu loại bỏ về các phương pháp khởi tạo khác nhau cho SLoRB. Chúng tôi báo cáo kết quả trên mô hình GPT2 dưới giới hạn tính toán được chỉ định là 1B, 2.5B và 5B token, như được hiển thị trong Bảng 9. 'Mean' đề cập đến phương pháp khởi tạo được thảo luận trong bài báo, trong khi 'Zero' chỉ việc khởi tạo cả hai ma trận từ zero. Phương pháp khởi tạo của chúng tôi hỗ trợ duy trì hiệu suất mô hình ngay từ đầu, do đó tăng tốc sự hội tụ.

6.5 Hàm Chưng cất Khác nhau
Trong phần này, chúng tôi chính thức trình bày các hàm loss được áp dụng trong nghiên cứu loại bỏ của chúng tôi, tập trung vào các phương pháp chưng cất khác nhau. Chúng tôi bao gồm kết quả từ các công trình trước đây như TinyBERT (Jiao et al. 2020), MobileBERT (Sun et al. 2020), Sparse-Finetuning (Kurtic et al. 2023), và MiniLLM (Gu et al. 2024).

TinyBERT Trong TinyBERT, chúng tôi sử dụng thông tin từ attention đa đầu trung gian và hidden state bằng loss mean square error (MSE). Cụ thể, loss attention được định nghĩa là:
Lattn=1/h∑i=1^h MSE(AS_i, AT_i), (7)
trong đó AS_i và AT_i biểu diễn các đầu ra attention theo lớp của đầu i từ mô hình học sinh và mô hình giáo viên, tương ứng.

Loss hidden state được định nghĩa là:
Lhidn=MSE(HSWh, HT), (8)
trong đó HS và HT biểu diễn các hidden state theo lớp của mô hình học sinh và mô hình giáo viên, tương ứng. Ở đây, Wh là một ma trận chiếu có thể học được. Tuy nhiên, vì các hình dạng của hidden state giống hệt nhau trong cài đặt của chúng tôi, chúng tôi cố định Wh= I.

TinyBERT cũng sử dụng loss cross-entropy:
Lpred=CE(zT/τ, zS/τ), (9)
trong đó zT và zS là các logit đầu ra của mô hình giáo viên và học sinh, tương ứng. Chúng tôi không sử dụng chưng cất embedding, vì các embedding được huấn luyện tốt và không bị cắt tỉa. Hàm chưng cất cho mỗi lớp được định nghĩa là:
Llayer =Lpred nếu đó là lớp cuối cùng,
Lhidn+Lattn ngược lại. (10)

Hàm loss cuối cùng là tổng của các hàm loss theo lớp.

MobileBERT MobileBERT được thiết kế để chưng cất BERT bằng cách sử dụng thông tin từ feature map và attention. Chưng cất feature map trong MobileBERT giống hệt với được sử dụng trong TinyBERT, như được hiển thị trong Phương trình 8. Đối với truyền kiến thức attention tại lớp l, hàm loss được định nghĩa là:
Ll_AT=1/(A*T)∑t=1^T∑a=1^A DKL(atr_t,l,a∥ast_t,l,a), (11)
trong đó A là số lượng đầu attention, T là độ dài chuỗi, và at,l,a biểu diễn đầu ra attention cho lớp l, đầu a, và token t. Tổng loss chưng cất là tổng của loss feature map và truyền attention.

Sparse-Finetuning Sparse-Finetuning sử dụng cả SquareHead Loss và loss KL-divergence. Nó đo lường biểu diễn trung gian bằng mean squared error (MSE):
Lfeat=MSE(fl_t, fl_s)/MSE(fl_t,0), (12)
trong đó fl_t và fl_s là các hidden state của lớp l-th tại bước t cho mô hình giáo viên và học sinh, tương ứng.

Loss KL-divergence (Kullback và Leibler 1951) được sử dụng để đo lường sự khác biệt trong các phân phối xác suất đầu ra giữa mô hình học sinh và giáo viên:
Llogit=DKL(pθt∥pθs), (13)
trong đó pθt và pθs là các phân phối xác suất của mô hình giáo viên và học sinh, tương ứng.

Tổng loss chưng cất là tổng có trọng số:
L=α1Llogit+α2Lfeat. (14)

MiniLLM MiniLLM sử dụng divergence KL ngược thay vì KL xuôi. Cụ thể, hàm loss được định nghĩa là:
Llogit=DKL(pθs∥pθt), (15)
trong đó pθt và pθs là các phân phối xác suất của mô hình giáo viên và học sinh, tương ứng.

6.6 Thêm Phân rã cho Bộ tối ưu AdamW
Công trình gần đây (Hu et al. 2024) đã cho thấy rằng việc áp dụng SR-STE với các bộ tối ưu dựa trên momentum có thể làm suy yếu hiệu quả của hạng phân rã, dẫn đến hiệu suất không tối ưu. Trong các triển khai trước đây, SR-STE đã thêm hạng phân rã trực tiếp vào trọng số. Cụ thể, chúng tôi ký hiệu gradient đối với trọng số Wt tại lần lặp t là gt được điều chỉnh thêm bằng bộ tối ưu AdamW, theo sau bởi cập nhật trọng số bằng:
AdamW (gt) =(utβ1+ (1−β1)gt)/(1−βt1)(√vt+ϵ) (16)
Wt+1←Wt−γt[AdamW (gt) +λW(m(Wt)⊙Wt)]. (17)

trong đó ut và vt là các moment bậc nhất và bậc hai của gt, tương ứng. γt là tốc độ học hiện tại. Tuy nhiên, kết quả chỉ ra rằng cách tiếp cận này gây ra dao động mặt nạ thường xuyên chi tiết có thể được tìm thấy trong công trình trước đây (Hu et al. 2024). Để giảm thiểu vấn đề này, hạng phân rã thay vào đó được áp dụng trực tiếp vào trọng số bằng quy tắc cập nhật sau:
˜gt←gt+λW(m(Wt)⊙Wt), (18)
Wt+1←Wt−γtAdamW ( ˜gt), (19)

Chúng tôi cải tiến thêm phương pháp này, vì hạng phân rã chỉ phụ thuộc vào giá trị hiện tại của tham số nó không nên bị vướng víu với moment bậc nhất. Do đó chúng tôi sử dụng quy tắc cập nhật sau:
ut=ut−1β1+ (1−β1)gt−1 (20)
˜ut=ut+λW(m(Wt)⊙Wt) (21)

Hơn nữa chúng tôi sử dụng gradient đã phân rã để tính toán moment bậc hai và cập nhật trọng số tương ứng.
vt=vt−1β2+ (1−β2) ˜ut−12 (22)
Wt+1←Wt−γt˜ut/(1−βt1)(√vt+ϵ). (23)

6.7 Hành vi Dao động Mặt nạ Trong Quá trình Huấn luyện
Một số chỉ số đã được giới thiệu để đo lường sự ổn định của mặt nạ trong quá trình huấn luyện, như tỷ lệ lật (Hu et al. 2024) và SAD (Zhou et al. 2021). Trong bài báo này, chúng tôi chủ yếu tập trung vào việc đo lường tỷ lệ lật và tỷ lệ lật ban đầu, định lượng các thay đổi mặt nạ giữa bước hiện tại và trước đó, cũng như giữa bước hiện tại và ban đầu, tương ứng.

rt=||m(Wt)−m(Wt−1)||1/D
it=||m(Wt)−m(W0)||1/D

trong đó D biểu diễn tổng số tham số trong lớp tuyến tính. Tỷ lệ lật phản ánh sự ổn định của mặt nạ mô hình, trong khi tỷ lệ lật ban đầu chỉ ra mức độ tổng thể của việc loại bỏ và hồi sinh trọng số.

Chúng tôi so sánh tỷ lệ lật và tỷ lệ lật ban đầu của static SR-STE và Annealing SR-STE. Trong các thí nghiệm của chúng tôi, mặt nạ được cập nhật, và tỷ lệ lật và tỷ lệ lật ban đầu được tính toán mỗi 10 batch trong quá trình huấn luyện, vì tính toán lại thường xuyên hơn không cải thiện độ chính xác và chỉ tăng chi phí tính toán. Hình 4 minh họa tỷ lệ lật và tỷ lệ lật ban đầu trong quá trình huấn luyện lại mô hình GPT2. Không giống như các hệ số phân rã tĩnh truyền thống, Annealing SR-STE sửa đổi tỷ lệ phần trăm cao hơn của mặt nạ ở đầu, cho phép mô hình khám phá các mô hình mặt nạ khác nhau. Ngoài ra, Annealing SR-STE cho phép một mặt nạ ổn định hơn về cuối quá trình huấn luyện, thúc đẩy sự hội tụ tốt hơn. Kết quả là, Annealing SR-STE hỗ trợ tỷ lệ thay đổi mặt nạ cao hơn (ví dụ: tỷ lệ lật ban đầu) trong khi duy trì sự ổn định tổng thể.

6.8 Nén Mô hình Sử dụng AWQ
Trong phần này, chúng tôi trình bày kết quả perplexity cho các mô hình đã lượng tử hóa bằng AWQ. Như được hiển thị trong Bảng 10, mô hình của chúng tôi duy trì hiệu suất của nó ngay cả dưới nén cực đoan. Đối với mô hình AST-Boosted, chúng tôi cũng lượng tử hóa các tham số SLoRB bổ sung.

6.9 Câu hỏi Thường gặp
Phần này kiểm tra và giải quyết các mối quan tâm chính được đề ra trong các thảo luận trước đây.

Biện minh cho Chưng cất Kiến thức Một mối quan tâm là việc sử dụng chưng cất có thể đưa ra chi phí tính toán bổ sung. Tuy nhiên, chúng tôi đã thấy rằng, dưới cùng tổng số FLOP, các phương pháp chưng cất vẫn vượt trội hơn các phương pháp không chưng cất. Chúng tôi trình bày so sánh hiệu suất dưới chi phí tính toán giống hệt nhau, và kết quả trong Bảng 11 chứng minh rằng mô hình đã chưng cất vượt trội đáng kể so với mô hình không chưng cất.

Biện minh cho SLoRB Một mối quan tâm là thời gian huấn luyện thành phần SLoRB bổ sung. Thông qua thử nghiệm, chúng tôi quan sát thấy rằng fine-tuning thành phần LoRA sau huấn luyện ban đầu không mang lại cải thiện hiệu suất thêm. Chúng tôi đưa ra giả thuyết rằng điều này là do mô hình đã hội tụ trên bộ dữ liệu tiền huấn luyện, làm cho việc nâng cao hiệu suất thêm thông qua fine-tuning module LoRA trở nên khó khăn. Ngược lại, việc giữ cả module mô hình và LoRA không đóng băng trong quá trình huấn luyện lại cung cấp tính linh hoạt và tiềm năng cải thiện lớn hơn. Để hỗ trợ giả thuyết này, chúng tôi trình bày kết quả perplexity cho mô hình GPT2 trong Bảng 12.

Lựa chọn Mặt nạ Chúng tôi trình bày độ phức tạp tính toán và kết quả perplexity cho các tiêu chí lựa chọn mặt nạ khác nhau, như các chỉ số Wanda và SparseGPT, trong Bảng 13. Kết quả chứng minh rằng chỉ số độ lớn vượt trội hơn các chỉ số khác về cả hiệu suất và tốc độ.

Tác động của α Chúng tôi cung cấp kết quả sử dụng các phương pháp khác nhau trên huấn luyện lại GPT2 đầy đủ và huấn luyện lại LLaMA2 với token huấn luyện hạn chế trong Bảng 14. Khi α= 1, có nghĩa là chúng tôi chỉ sử dụng loss KD, chúng tôi quan sát hiệu suất tốt hơn một chút trên các mô hình nhỏ hơn. Tuy nhiên, nó hoạt động kém hiệu quả hơn trên các mô hình lớn hơn như LLaMA2-7B, nơi việc điều chỉnh α dẫn đến cải thiện hiệu suất. Điều này cho thấy rằng α đóng vai trò quan trọng hơn trong việc tối ưu hóa kết quả cho các mô hình quy mô lớn hơn.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được dịch sẽ rất dài và chứa nhiều thông tin chi tiết về các công trình nghiên cứu được trích dẫn]

--- TRANG 13 ---
[Tiếp tục danh sách tài liệu tham khảo và các phần cuối của bài báo]
