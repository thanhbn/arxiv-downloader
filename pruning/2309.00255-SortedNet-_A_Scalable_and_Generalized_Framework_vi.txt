# 2309.00255.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2309.00255.pdf
# Kích thước tệp: 2129896 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
SortedNet: Một Framework Có Thể Mở Rộng và Tổng Quát
để Huấn Luyện Mạng Nơ-ron Sâu Modular
Mojtaba Valipour1,2, Mehdi Rezagholizadeh2, Hossein Rajabzadeh1,2,
Parsa Kavehzadeh2,Marzieh Tahaei2,Boxing Chen2,Ali Ghodsi1
1University of Waterloo,2Huawei Noah's Ark Lab
{mojtaba.valipour, hossein.rajabzadeh, ali.ghodsi}@uwaterloo.ca,
{mehdi.rezagholizadeh, marzieh.tahaei, boxing.chen}@huawei.com
Tóm tắt
Mạng nơ-ron sâu (DNN) phải phục vụ nhiều người dùng khác nhau với các nhu cầu hiệu suất và ngân sách khác nhau, dẫn đến thực hành tốn kém là huấn luyện, lưu trữ và duy trì nhiều mô hình cụ thể cho người dùng/tác vụ. Có các giải pháp trong tài liệu để giải quyết các mô hình động đơn lẻ hoặc nhiều-trong-một thay vì nhiều mạng riêng lẻ; tuy nhiên, chúng gặp phải sự sụt giảm hiệu suất đáng kể, thiếu khả năng tổng quát hóa qua các kiến trúc mô hình khác nhau hoặc các chiều khác nhau (ví dụ: độ sâu, độ rộng, khối attention), yêu cầu tìm kiếm mô hình nặng trong quá trình huấn luyện, và huấn luyện số lượng hạn chế các mô hình con. Để giải quyết những hạn chế này, chúng tôi đề xuất SortedNet, một giải pháp huấn luyện tổng quát và có thể mở rộng để khai thác tính modular vốn có của DNN. Nhờ kiến trúc lồng nhau tổng quát (mà chúng tôi gọi là kiến trúc sắp xếp trong bài báo này) với các tham số chia sẻ và sơ đồ cập nhật mới kết hợp việc lấy mẫu mô hình con ngẫu nhiên và cơ chế tích lũy gradient mới, SortedNet cho phép huấn luyện các mô hình con đồng thời cùng với việc huấn luyện mô hình chính (không có bất kỳ chi phí huấn luyện hoặc suy luận bổ sung đáng kể nào), đơn giản hóa việc lựa chọn mô hình động, tùy chỉnh triển khai trong quá trình suy luận, và giảm đáng kể yêu cầu lưu trữ mô hình. Tính đa dạng và khả năng mở rộng của SortedNet được xác thực thông qua các kiến trúc và tác vụ khác nhau bao gồm LLaMA, BERT, RoBERTa (tác vụ NLP), ResNet và MobileNet (phân loại hình ảnh) thể hiện sự vượt trội của nó so với các phương pháp huấn luyện động hiện có. Ví dụ, chúng tôi giới thiệu một phương pháp tự đoán thích ứng mới dựa trên sorted-training để tăng tốc giải mã mô hình ngôn ngữ lớn. Hơn nữa, SortedNet có thể huấn luyện tới 160 mô hình con cùng một lúc, đạt được ít nhất 96% hiệu suất của mô hình gốc.

1 Giới thiệu
Mạng nơ-ron sâu (DNN) ngày càng thu hút sự quan tâm và trở nên phổ biến hơn Sarker [2021]. Sự phổ biến này chuyển thành nhu cầu và yêu cầu ngày càng tăng từ người dùng mà các mô hình này phải đáp ứng. Người dùng pre-train hoặc fine-tune nhiều mô hình với các kích thước khác nhau để giải quyết nhu cầu hiệu suất và tính toán của các tác vụ và thiết bị mục tiêu của họ (với bộ nhớ và sức mạnh tính toán khác nhau cho dù được triển khai trên cloud hay thiết bị edge). Tuy nhiên, việc phát triển, lưu trữ, duy trì và triển khai nhiều mô hình riêng lẻ cho tập hợp người dùng đa dạng có thể rất khó khăn và tốn kém Devvrit et al. [2023]. Hơn nữa, trong kỷ nguyên của các mô hình pre-trained khổng lồ Devlin et al. [2018], Liu et al. [2019] và mô hình ngôn ngữ lớn Brown et al. [2020], Chowdhery et al. [2022], nhu cầu tính toán có thể thay đổi đáng kể từ tác vụ này sang tác vụ khác. Do đó, có nhu cầu ngày càng tăng đối với các mô hình có thể tự thích ứng với các điều kiện động, trong khi mạng nơ-ron thông thường sẽ không thể giải quyết những trường hợp như vậy Xin et al. [2020], Yu et al. [2018a].

Preprint. Under review.arXiv:2309.00255v3  [cs.LG]  1 Jun 2024

--- TRANG 2 ---
Hình 1: (a) Sơ đồ tổng thể của phương pháp huấn luyện SortedNet của chúng tôi. Đầu tiên, chúng ta cần xác định pool các mô hình con quan tâm bao gồm cả mô hình chính. Trong quá trình huấn luyện, tại mỗi iteration, chúng ta lấy mẫu từ pool các mô hình con (theo phân phối ngẫu nhiên được xác định trước) để được huấn luyện cho hàm loss mục tiêu (cho một bước). (b) Khả năng tổng quát hóa của cấu hình Sorted qua các chiều phức tạp hơn, hỗ trợ các block và function bên cạnh width và depth. (c) Minh họa sự khác biệt giữa các mô hình con nested và sorted. Trong kiến trúc nested, các mô hình con nhỏ hơn được đóng gói bởi các mô hình con lớn hơn, điều này không nhất thiết phải xảy ra đối với những gì chúng tôi gọi là mô hình sorted. Hơn nữa, các mô hình sorted được gắn với gốc (tức là chỉ số bắt đầu) của mỗi chiều mà có thể không phải là trường hợp trong các mô hình nested.

Mặt khác, DNN thể hiện kiến trúc modular theo các chiều khác nhau, như các layer và block qua depth, và các neuron, channel và attention head theo width. Tính modular vốn có này cho phép trích xuất các mô hình con có hình dạng tương tự với mô hình gốc. Tuy nhiên, tính modular này chưa được triển khai trong các phương pháp huấn luyện thông thường, và do đó, hiệu suất của các mô hình con kém hơn so với mô hình chính. Do đó, thách thức nằm ở việc khai thác toàn bộ tiềm năng của tính modular trong mạng nơ-ron sâu, cho phép sử dụng hiệu quả các mô hình con để nâng cao hiệu suất của chúng và cho phép triển khai thực tế trong các tình huống thực tế.

Thay vì huấn luyện các mô hình riêng lẻ, chúng ta có thể tận dụng các mô hình con của DNN và huấn luyện chúng cùng với các mô hình chính để có được mạng nhiều-trong-một với các mô hình con có thể được sử dụng cho các tác vụ khác nhau. Có nhiều phương pháp trong tài liệu để huấn luyện các mô hình con Cai et al. [2020], Xin et al. [2020], Hou et al. [2020]. Những kỹ thuật này tuy hiệu quả nhưng có một số khuyết điểm: thường sử dụng quy trình huấn luyện phức tạp kết hợp với knowledge distillation (cần huấn luyện một mô hình teacher riêng biệt) Hou et al. [2020], yêu cầu sửa đổi kiến trúc Nunez et al. [2023], chỉ hoạt động cho các kiến trúc cụ thể Cai et al. [2019], không thể xử lý hơn một số lượng rất nhỏ các mô hình con, cần tìm kiếm mô hình nặng (ví dụ: neural architecture search) trong quá trình huấn luyện hoặc suy luận Cai et al. [2020], liên quan đến tối ưu hóa mô hình con dư thừa Fan et al. [2019], hoặc cho thấy hiệu suất kém cho mô hình chính hoặc mô hình con Xin et al. [2020].

Để giải quyết những vấn đề này, chúng tôi đề xuất SortedNet, một giải pháp huấn luyện tổng quát và có thể mở rộng để khai thác tính modular vốn có của DNN qua các chiều khác nhau. Như tên của phương pháp chúng tôi ngụ ý, nó chọn các mô hình con theo cách sắp xếp (một phiên bản tổng quát của kiến trúc nested) trong mô hình chính để tránh tìm kiếm nặng trong hoặc sau quá trình huấn luyện. Trái ngược với các mô hình nested trong đó các mô hình con nhỏ hơn luôn được đóng gói hoàn toàn bởi các mô hình con lớn hơn, phiên bản sorted tổng quát của chúng tôi nới lỏng ràng buộc nested nhưng gắn gốc của các mô hình con với gốc của mô hình chính qua bất kỳ chiều mục tiêu nào (để biết thêm chi tiết, xem phần 3.1).

2

--- TRANG 3 ---
Để huấn luyện các mô hình con sorted, chúng tôi đề xuất một sơ đồ cập nhật mới kết hợp việc lấy mẫu ngẫu nhiên các mô hình con với tích lũy gradient. Chúng tôi đã thử giải pháp SortedNet thành công trên các kiến trúc và tác vụ khác nhau như mô hình ngôn ngữ lớn decoder-based LLaMA (13B) Touvron et al. [2023] trên tác vụ lý luận toán học GSM8K Cobbe et al. [2021], encoder-based BERT Devlin et al. [2018] và RoBERTa Liu et al. [2019] trên tập các tác vụ hiểu ngôn ngữ GLUE Wang et al. [2018], ResNet He et al. [2015] và MobileNet Sandler et al. [2018] trên tác vụ phân loại hình ảnh CIFAR-10. Các nghiên cứu thực nghiệm toàn diện của chúng tôi qua các kiến trúc, tác vụ và tính động khác nhau theo các chiều khác nhau từ width và depth đến attention head và embedding layer cho thấy sự vượt trội và khả năng tổng quát hóa của phương pháp đề xuất so với các phương pháp huấn luyện động tiên tiến. Hơn nữa, SortedNet mang lại một số lợi ích, bao gồm yêu cầu lưu trữ tối thiểu và khả năng suy luận động (tức là chuyển đổi giữa các ngân sách tính toán khác nhau) trong quá trình suy luận.

Để tóm tắt, những đóng góp chính của bài báo này là:
• Giới thiệu một giải pháp nhiều-trong-một để cấu hình các mô hình con theo cách sắp xếp và huấn luyện chúng đồng thời với một số khía cạnh độc đáo như khả năng mở rộng (huấn luyện nhiều mô hình con), tính tổng quát (CNN, Transformer, depth, width), và không cần tìm kiếm (không cần tìm kiếm trong quá trình huấn luyện hoặc suy luận giữa các mô hình con) và duy trì hiệu suất cạnh tranh cho mô hình chính.
• Theo hiểu biết của chúng tôi, công trình này là nỗ lực đầu tiên hướng tới huấn luyện các mô hình nhiều-trong-một theo các chiều khác nhau đồng thời (và thậm chí không phải ở các giai đoạn khác nhau).
• Chúng tôi triển khai sorted training trong việc tăng tốc giải mã mô hình ngôn ngữ lớn sử dụng phương pháp self speculative có thể dẫn đến tăng tốc suy luận khoảng 2x cho LLaMA 13B.
• Chứng minh các lý thuyết biện minh và bằng chứng thực nghiệm cho hiệu quả của phương pháp đề xuất. Vượt trội hơn các phương pháp tiên tiến trong huấn luyện động trên CIFAR10 Krizhevsky et al. [2009] với việc mở rộng số lượng mô hình con lên 160 và đạt được ít nhất 96% hiệu suất của mô hình gốc. Hơn nữa, cho thấy kết quả thành công trong huấn luyện động của các mô hình BERT, RoBERTa và LLaMA.

2 Nghiên cứu liên quan

Bảng 1: So sánh toàn diện các nghiên cứu liên quan hiện có khác nhau và phân biệt giải pháp của chúng tôi

[Bảng được giữ nguyên format]

Trong phần này, chúng tôi xem xét ngắn gọn các nghiên cứu liên quan nhất với ý tưởng SortedNet của chúng tôi. Tóm tắt về các giải pháp này và cách chúng khác nhau có thể được tìm thấy trong Bảng 1. Để biết thêm chi tiết, vui lòng tham khảo phụ lục A.

Slimmable Networks Yu et al. [2018b] Slimmable networks là phương pháp huấn luyện có thể điều chỉnh width. Nó được đề xuất đặc biệt cho kiến trúc CNN và do đó, cần phải xem xét cẩn thận mô-đun batch normalization cho các kích thước width khác nhau. Trái ngược với slimmable networks, SortedNet của chúng tôi bao phủ nhiều kiến trúc hơn và hoạt động trong cả chiều depth và width.

Early Exit Xin et al. [2020] là một trong những kỹ thuật baseline phổ biến nhất, thêm một classifier vào các layer trung gian của một mô hình đã được huấn luyện. Các tham số của mô hình chính được đóng băng và các classifier được cập nhật trong quy trình fine-tuning riêng biệt. Trong khi giải pháp này tương đối đơn giản, hiệu suất của các mô hình con tụt hậu đáng kể so với mô hình chính.

3

--- TRANG 4 ---
Dayna-BERT Hou et al. [2020] trình bày phương pháp nén động cho các mô hình BERT pre-trained, cho phép điều chỉnh linh hoạt kích thước mô hình, cả về depth và width, trong quá trình suy luận. DynaBERT khác với chúng tôi ở các khía cạnh sau: đầu tiên, trong DynaBERT, chỉ có rất ít mô hình con hoạt động; thứ hai, DynaBERT yêu cầu một mô hình teacher đã được huấn luyện và sử dụng knowledge distillation (KD); thứ ba, DynaBERT cần tìm kiếm để tìm mô hình con tối ưu; cuối cùng, DynaBERT phụ thuộc vào kiến trúc.

Layer-drop Fan et al. [2019] là huấn luyện structured dropout cho phép layer pruning tại thời điểm suy luận. Tương tự như DynaBERT, nó được áp dụng cho các mô hình ngôn ngữ pre-trained; tuy nhiên, trái ngược với DynaBERT, Layer-drop chỉ nhắm mục tiêu depth của mạng nơ-ron chứ không phải width của chúng.

Once-for-All (OFA) Cai et al. [2020] nhắm mục tiêu suy luận hiệu quả qua các thiết bị khác nhau. Nó đầu tiên huấn luyện một mạng hỗ trợ nhiều mô hình con với các đặc điểm latency/accuracy khác nhau; sau đó nó tìm kiếm giữa các mô hình con khả thi theo yêu cầu accuracy và latency của thiết bị mục tiêu. OFA khác với giải pháp của chúng tôi ở: đầu tiên, nó có bản chất huấn luyện progressive trái ngược với stochastic hoặc summation loss của chúng tôi; thứ hai, nó cần teacher và KD; thứ ba, nó yêu cầu neural architecture search (NAS) riêng biệt tại thời điểm suy luận; thứ tư, OFA dành cho các mô hình dựa trên CNN; cuối cùng, nó không có bất kỳ giả định cụ thể nào để cấu hình mô hình con (xem Hình 4 để biết thêm chi tiết).

Learning Compressible Subspace (LCS) Nunez et al. [2023] là kỹ thuật nén thích ứng dựa trên huấn luyện không gian con có thể nén của mạng nơ-ron. Trong khi LCS không yêu cầu bất kỳ huấn luyện lại nào tại thời điểm suy luận, giải pháp này có một số hạn chế khác bao gồm: đầu tiên, nó cần gấp đôi bộ nhớ tại thời điểm huấn luyện; thứ hai, việc lựa chọn trọng số ban đầu và hàm nén không rõ ràng và tùy ý (để lại như một siêu tham số); thứ ba, nó chỉ được thử trên CNN; thứ tư, tương tự như Layer-drop, không gian tìm kiếm của các mô hình con là rất lớn khiến việc huấn luyện trở nên không tối ưu.

MatFormer Devvrit et al. [2023] là giải pháp pre-training only nhiều-trong-một dựa trên summation loss cho các mô hình dựa trên Transformer. MatFormer chỉ hoạt động theo chiều width của block FFN trong Transformer và không thể xử lý hơn một số lượng rất ít mô hình con.

3 Phương pháp đề xuất

3.1 Quan điểm tổng quát và có thể mở rộng

Trong phần nghiên cứu liên quan, chúng tôi đã thảo luận về một số phương pháp liên quan đến việc huấn luyện mạng nhiều-trong-một. Những phương pháp này khác nhau về kiến trúc mục tiêu, loss huấn luyện, số lượng tham số huấn luyện, cấu hình của các mô hình con (ngẫu nhiên, nested, hoặc sorted), số lượng mô hình con được huấn luyện, và sự phụ thuộc vào tìm kiếm hoặc huấn luyện lại trước khi triển khai. Phương pháp SortedNet của chúng tôi có thể được xem như một phiên bản đơn giản, tổng quát và có thể mở rộng của các giải pháp hiện có này. Những lợi ích này chủ yếu là kết quả từ cấu hình sorted của các mô hình con với các tham số chia sẻ và huấn luyện stochastic của chúng tôi. Để huấn luyện mạng nhiều-trong-một, chúng ta cần chỉ định một vài lựa chọn thiết kế: đầu tiên, cách hình thành các mô hình con và cấu hình của chúng; thứ hai, các kiến trúc mục tiêu là gì; và thứ ba, cách huấn luyện các mô hình con cùng với mô hình chính.

Thiết kế các mô hình con SortedNet áp dụng một bias quy nạp vào huấn luyện dựa trên giả định rằng các tham số của mô hình con có kiến trúc đồng tâm gắn với gốc dọc theo mỗi chiều (mà chúng tôi gọi là kiến trúc sorted). Cấu hình sorted này với các tham số chia sẻ tạo ra trật tự thường xuyên và tính nhất quán trong kiến thức được học bởi các mô hình con (xem Hình 1).

Kiến trúc Sorted vs. Nested Trong công trình này, chúng tôi giới thiệu thuật ngữ kiến trúc sorted để mở rộng và tổng quát hóa khái niệm kiến trúc nested. Trái ngược với các mô hình nested trong đó các mô hình con nhỏ hơn luôn được đóng gói hoàn toàn bởi các mô hình con lớn hơn, các mô hình con sorted của chúng tôi sẽ được gắn với gốc (chỉ số bắt đầu) của mỗi chiều độc lập.

Hãy xem xét một mạng nơ-ron nhiều-trong-một f(x;θ(n)) với các tham số θ(n) và input x được tạo thành từ n mô hình con f(x;θ(i))|n−1i=0, trong đó θ(i) đại diện cho trọng số của mô hình con thứ i. Chúng tôi định nghĩa một tập hợp vũ trụ chứa tất cả các mô hình con duy nhất: Θ ={θ(0), θ(1), ..., θ(n)}.

4

--- TRANG 5 ---
Thiết lập một thứ tự Giả sử rằng chúng ta muốn nhắm mục tiêu D={Dim 1, Dim 2, ..., Dim K} nhiều chiều nhiều-trong-một trong mô hình của chúng ta. Sau đó, hãy bắt đầu với Θ = ∅ và xây dựng các mô hình con một cách lặp đi lặp lại. Trong vấn đề này, tại mỗi iteration t trong quá trình huấn luyện, chúng ta có các thủ tục lấy mẫu và cắt cụt dọc theo bất kỳ chiều mục tiêu nào:

θ*t = ∩|D|j=1 θDim j↓btj(n) where btj ∼ PBj
If θ*t ∉ Θ: Θ ← Θ ∪ {θ*t}                (1)

trong đó Dim j↓btj chỉ ra rằng chúng ta đã cắt cụt θ(n) dọc theo chiều Dim j từ chỉ số 1 đến chỉ số btj tại iteration t. btj được lấy mẫu từ phân phối PBj với tập hỗ trợ Bj={1,2, ..., dj} để tạo thành mô hình con thứ i. dj đề cập đến chỉ số tối đa của chiều thứ j.

Quá trình lặp này sẽ được thực hiện trong quá trình huấn luyện và tập n mô hình con duy nhất Θ sẽ được xây dựng.

Để minh họa quá trình tốt hơn, hãy xem một trường hợp đơn giản như BERT base nơi chúng ta muốn tạo mạng nhiều-trong-một qua các chiều width và depth, D={Depth, Width}. Trong trường hợp này, chúng ta có 12 layer và kích thước hidden dimension là 768. Giả sử rằng Depth tương ứng với j=1 và Width tương ứng với j=2 trong Eq. 1. Để đơn giản, hãy sử dụng phân phối uniform rời rạc để lấy mẫu các chỉ số qua hai chiều này. Để tạo mô hình con đầu tiên (i=1), chúng ta cần lấy mẫu b11 đồng đều từ tập các số tự nhiên trong khoảng từ 1 đến 12: B1={1,2, ...,12}; và chúng ta cần lấy mẫu b12 từ khoảng từ 1 đến 768: B2={1,2,3, ...,768}. Lưu ý rằng chúng ta thậm chí có thể chọn một tập con của B1 và B2 làm tập hỗ trợ cho phân phối xác suất lấy mẫu. Sau hai lần lấy mẫu này, chúng ta sẽ có hai tập tham số bị cắt cụt: θDepth↓b11 và θWidth↓b12. Giao của hai tham số bị cắt cụt này sẽ cho chúng ta mô hình con đầu tiên: θ1 = θDepth↓b11 ∩ θWidth↓b12.

Paradigm huấn luyện Huấn luyện thông thường của mạng nơ-ron liên quan đến việc cải thiện hiệu suất của toàn bộ mô hình và thường huấn luyện này không nhận biết về hiệu suất của các mô hình con. Thực tế, trong tình huống này, nếu chúng ta trích xuất và triển khai các mô hình con của mô hình lớn đã được huấn luyện trên một tác vụ mục tiêu, chúng ta sẽ trải nghiệm sự sụt giảm đáng kể trong hiệu suất của những mô hình con này so với mô hình chính. Tuy nhiên trong SortedNet, chúng tôi đề xuất một phương pháp huấn luyện cho phép huấn luyện các mô hình con cùng với mô hình chính theo cách stochastic. Paradigm SortedNet dẫn đến những lợi ích sau:

• Trích xuất mô hình con không cần tìm kiếm: sau khi huấn luyện, bằng cách sắp xếp tầm quan trọng của các mô hình con, mô hình con tốt nhất cho một ngân sách nhất định có thể được chọn mà không cần tìm kiếm.
• Anytime: Mỗi mô hình con nhỏ hơn là một tập con của một mô hình lớn hơn làm cho việc chuyển đổi giữa các mô hình con khác nhau hiệu quả. Điều này dẫn đến một tính năng quan trọng của SortedNet là cái gọi là anytime đó là một mạng có thể tạo ra output của nó tại bất kỳ giai đoạn nào của tính toán.
• Tiết kiệm bộ nhớ: chúng ta huấn luyện một mạng nhiều-trong-một trong đó các mô hình con đều là một phần của một checkpoint duy nhất, điều này giảm thiểu yêu cầu lưu trữ.

Vì mục đích hiệu quả, trong huấn luyện của chúng tôi, layer cuối cùng, ví dụ như layer phân loại, được chia sẻ giữa tất cả các mô hình con; hoặc chúng ta có thể thêm một layer phân loại riêng biệt cho mỗi mô hình con. Để đơn giản và hiệu quả, chúng tôi đã chọn cái trước tức là sử dụng layer phân loại chia sẻ.

3.2 Thuật toán SortedNet

Trong phần này, chúng tôi mô tả thuật toán huấn luyện đề xuất. Để huấn luyện một SortedNet với n mô hình con, tại mỗi iteration trong quá trình huấn luyện, một chỉ số ngẫu nhiên cần được lấy mẫu từ phân phối được xác định trước: bij ∼ PBj. Sau khi tìm mô hình con mục tiêu θ*t tại mỗi iteration, chúng ta có thể sử dụng một trong các mục tiêu sau để cập nhật các tham số của mô hình con được chọn:

• (Stochastic Loss) Chỉ huấn luyện mô hình con được chọn f(x, θ*t):
minθ*t L ≜ L(y, f(x, θ*t)) trong đó L là hàm loss để huấn luyện mô hình trên một tác vụ nhất định (ví dụ L có thể là cross entropy loss thông thường) và y đề cập đến nhãn ground-truth.

• (Stochastic Summation) Huấn luyện mô hình con f(x, θ*t) và tất cả các mô hình con mục tiêu của nó dọc theo mỗi chiều. Hãy giả sử rằng Θ⊥(θ*t) là tập hợp vũ trụ cho tất cả các mô hình con mục tiêu của

5

--- TRANG 6 ---
Hình 2: (a) Độ chính xác phân loại CIFAR10 (và tỷ lệ phục hồi) cho Sorted-Net (160 Models) và baseline. Trong mỗi ô, chúng tôi báo cáo hiệu suất của mô hình con (trên) và hiệu suất tương đối của mô hình (tính bằng phần trăm) so với hiệu suất của mô hình lớn nhất baseline (dưới). W. Only: Chỉ sắp xếp width, D. Only: Chỉ sắp xếp depth. Càng đen càng tốt. (b) Hiệu suất phân loại CIFAR10 cho tập con các mô hình con có hiệu suất tốt nhất được huấn luyện bởi SortedNet từ đầu. Càng đen càng tốt. (c) Tìm các mô hình con tốt nhất tự động bằng cách sử dụng ngưỡng mong muốn để loại bỏ các mô hình có hiệu suất tệ nhất.

θ*t. Khi đó hàm loss có thể được định nghĩa là:
minΘ⊥(θ*t) L ≜ Σθ∈Θ⊥(θ*t) L(y, f(x, θ))

Theo cách này, một mô hình con hoặc một tập con các mô hình con được cập nhật trong mỗi iteration. Hoặc, người ta có thể chọn huấn luyện tất cả các mô hình con tại mỗi iteration điều này tốn kém ở quy mô lớn.

3.3 Tại sao SortedNet hoạt động?

Trong Phụ lục B, chúng tôi cung cấp lý thuyết biện minh cho sự hội tụ tham số của các mô hình con trong các tình huống huấn luyện giống hệt nhau và cũng cung cấp ranh giới hiệu suất giữa các mô hình con được huấn luyện và mạng tương ứng tương tự được huấn luyện độc lập.

Hội tụ Giả sử f̂ là một mô hình con của một mạng lớn hơn, và f là một kiến trúc mô hình giống hệt nhau được huấn luyện độc lập. Chúng tôi muốn hiểu mối quan hệ giữa các tham số của hai mạng này, θ cho f̂ và φ cho f, khi chúng được huấn luyện trong các điều kiện giống hệt nhau. Giả sử rằng gradient của các hàm loss cho f̂ và f, là L-Lipschitz liên tục, và learning rate là η, chúng tôi

6

--- TRANG 7 ---
cho thấy
∥θt+1 − φt+1∥ ≤ (1 + ηL)∥θt − φt∥.                    (2)

Điều này chỉ ra rằng sự khác biệt trong các tham số của f̂ và f được chi phối bởi hằng số Lipschitz L và learning rate η, gợi ý rằng các tham số nên giữ gần nhau trong suốt quá trình huấn luyện, đặc biệt khi sự khác biệt giữa gradient của các hàm loss của hai mạng là không đáng kể.

Ranh giới hiệu suất Hơn nữa, chúng tôi muốn tìm ranh giới hiệu suất giữa một mô hình con được huấn luyện (với các tham số tối ưu θ*) và mô hình riêng lẻ tương ứng (với các tham số tối ưu φ*). Hãy giả sử rằng φ* = θ* + Δθ. Chúng tôi cho thấy trong Phụ lục B rằng độ lệch Δf = f(x;φ*) − f̂(x;θ*) trong giá trị hàm so với giá trị tối ưu do nhiễu loạn tham số Δθ được giới hạn bởi (1/2)L∥Δθ∥² dưới giả định về tính liên tục L-Lipschitz của gradient.

Δf ≈ (1/2)ΔθᵀH(x;θ*)Δθ ≤ (1/2)L∥Δθ∥²                (3)

Kết quả này ngụ ý rằng độ lệch của giá trị hàm tăng nhiều nhất theo bậc hai với kích thước của nhiễu loạn tham số.

4 Thí nghiệm

Trong phần này, chúng tôi tiến hành một tập hợp các thí nghiệm để cho thấy hiệu quả và tầm quan trọng của giải pháp của chúng tôi. Chi tiết về các siêu tham số cho mỗi thí nghiệm có thể được tìm thấy trong Phụ lục C.3.

4.1 SortedNet có thể mở rộng không?

Để cho thấy rằng phương pháp đề xuất của chúng tôi có thể mở rộng, chúng tôi thiết kế một thí nghiệm cố gắng huấn luyện 160 mô hình khác nhau qua nhiều chiều (width và depth) tất cả cùng một lúc. Làm baseline, chúng tôi huấn luyện mạng lớn nhất (một MobileNetV2), và báo cáo hiệu suất tốt nhất của mô hình. Vì hiệu suất của mô hình kém cho tất cả các mô hình con khác (ít hơn 12%), chúng tôi huấn luyện layer classifier thêm 5 epoch trước khi đánh giá mỗi mô hình con cho baseline và báo cáo hiệu suất tốt nhất.

Như kết quả gợi ý trong Hình 2-a, phương pháp của chúng tôi có thể nắm bắt hiệu suất tối đa cho nhiều mô hình con này theo cách zero-shot. Trong mỗi ô, chúng tôi báo cáo hiệu suất của mô hình con ở trên và tỷ lệ phục hồi của mô hình so với mô hình lớn nhất (trong ví dụ này là 95,45). Mặc dù chia sẻ trọng số qua tất cả các mô hình, chia sẻ classifier và đánh giá zero-shot, phương pháp đề xuất bảo tồn tới 96% hiệu suất của mô hình lớn nhất điều này rất đáng khích lệ. Việc huấn luyện thêm classifier cho phương pháp đề xuất của chúng tôi sẽ dẫn đến hiệu suất thậm chí tốt hơn như được hiển thị trong Phụ lục C.5 (cải thiện từ ∼2 đến 15% cho các mô hình con khác nhau). Ngoài ra, chúng tôi cũng thử sắp xếp depth và width bằng phương pháp đề xuất riêng lẻ, được báo cáo trong Hình 2-a là D. Only, và W. Only, tương ứng. Qua width, SortedNet thành công bảo tồn tới 99% hiệu suất của mạng lớn nhất.

4.2 Chúng ta có thể tìm các mô hình con tốt nhất bằng SortedNet không?

Như được hiển thị trong Hình 2-b, dựa trên hiệu suất của các mô hình trong thí nghiệm trước được hiển thị trong Hình 2-a, chúng tôi chọn một tập con các mạng có hiệu suất tốt nhất (width >60% và depth >13 block), và huấn luyện lại mạng từ đầu bằng SortedNet để cho thấy tỷ lệ thành công của phương pháp đề xuất. Như được hiển thị, SortedNet thành công bảo tồn tới 99% hiệu suất của huấn luyện thông thường của mạng lớn nhất. Chúng ta cũng có thể làm cho quá trình lựa chọn này hoàn toàn tự động bằng cách sắp xếp hiệu suất của tất cả các mô hình con sau khi đánh giá và lọc ra một tập con các mô hình có hiệu suất tốt nhất thực hiện tốt hơn ngưỡng mong muốn. Như có thể thấy trong Hình 2-c, có một tập các mô hình con thực hiện tốt hơn 80%. Để hiểu rõ hơn về mẫu, chúng tôi chú thích một số điểm bằng "D/W" như template cho thấy cho mỗi mô hình width và depth tương ứng.

4.3 Chúng ta có thể tổng quát hóa SortedNet không?

Trong một thí nghiệm khác, như được hiển thị trong Bảng 2, chúng tôi chứng minh sự vượt trội của phương pháp stochastic so với các phương pháp tiên tiến như LCS (được hiển thị là LCS p trong bảng) Nunez et al.

7

--- TRANG 8 ---
Bảng 2: So sánh hiệu suất của các phương pháp tiên tiến với Sorted-Net trên CIFAR10 về độ chính xác test.

[Bảng được giữ nguyên format]

[2023], Slimmable Neural Network (NS) Yu et al. [2018a], và Universally Slimmable Networks (US) Yu và Huang [2019]. Để làm cho các so sánh công bằng, chúng tôi cân bằng số lượng gradient update cho tất cả các mô hình. Chúng tôi cũng cố gắng loại bỏ tác động của thiết kế kiến trúc như việc lựa chọn các layer normalization. Do đó, chúng tôi cố gắng so sánh các phương pháp bằng các kỹ thuật layer normalization khác nhau như BatchNorm Ioffe và Szegedy [2015] và InstanceNorm Ulyanov et al. [2016]. Ngoài ra, chúng tôi đảm bảo rằng các phương pháp bổ sung như Knowledge Distillation không có tác động đến kết quả vì những phương pháp này có thể được áp dụng và cải thiện kết quả độc lập với phương pháp. Như được hiển thị trong bảng, SortedNet thể hiện hiệu suất trung bình vượt trội so với các phương pháp khác, cho thấy khả năng tổng quát hóa của nó qua các cài đặt khác nhau như các norm khác nhau. Đáng chú ý là chúng tôi nhận ra bản chất bất ngờ của kết quả LCS-p-BN trong Bảng 2. Tuy nhiên, những kết quả này phù hợp với các quan sát của bài báo LCS gốc Nunez et al. [2023] (xem Hình 3 của bài báo LCS). Các tác giả LCS Nunez et al. [2023] cũng đưa ra giả thuyết rằng sự sụt giảm này do thống kê batch norm không chính xác. Để giải quyết điều này, họ đề xuất điều chỉnh kiến trúc thành GroupNorm. Phương pháp SortedNet của chúng tôi, mặt khác, không bị ảnh hưởng bởi vấn đề này, do đó không cần những sửa đổi như vậy.

4.4 Mở rộng Sorted Net cho các mô hình ngôn ngữ pre-trained

Bảng 3: So sánh hiệu suất của các mô hình con khác nhau có và không có SortedNet. Hiệu suất của mô hình sẽ cải thiện nếu chúng ta có nhiều ngân sách hơn và tính toán representation của các layer sâu hơn.

[Bảng được giữ nguyên format]

Trong thí nghiệm này, mục tiêu là áp dụng SortedNet cho một mô hình transformer pre-trained và đánh giá hiệu suất trên benchmark GLUE Wang et al. [2018]. Làm baseline, chúng tôi chọn RoBERTa Liu et al. [2019] để chứng minh tính linh hoạt của thuật toán. Trong Bảng 3, chúng tôi sắp xếp tất cả các layer của RoBERTa-base. Như kết quả chứng minh, phương pháp đề xuất của chúng tôi trung bình thực hiện tốt hơn baseline một cách đáng kể (∼23%). Tuy nhiên, mô hình lớn nhất có sự sụt giảm nhỏ về hiệu suất (ít hơn 2%). Thú vị là kiến trúc transformer có thể bảo tồn hiệu suất của các mô hình con đến một mức độ nhất định mà không cần huấn luyện bổ sung. Tuy nhiên, thuật toán của chúng tôi cải thiện hiệu suất của những mô hình con này từ 10 đến 40% khoảng. Một cài đặt phức tạp hơn (sắp xếp qua các mô hình Bert) đã được nghiên cứu trong Phụ lục C.6.

4.5 Tăng tốc suy luận của các mô hình ngôn ngữ lớn dựa trên Decoder bằng SortedNet

Để tiếp tục cho thấy khả năng mở rộng và tổng quát hóa của SortedNet trong các tình huống thực tế hơn, chúng tôi fine-tune một LLaMA-13b Touvron et al. [2023] trên GSM8K Cobbe et al. [2021], một trong những tác vụ lý luận toán học thách thức. Chúng tôi chọn 12, 16, 20, 24, 28, 32, 36, và 40 layer đầu tiên của LLaMA để xây dựng các mô hình con. Để cân bằng số lượng update, chúng tôi huấn luyện mô hình dựa trên stochastic loss 8 lần nhiều hơn summation loss, vì chúng ta có 8 mô hình và mỗi forward pass trong stochastic loss là 1/8 của summation loss. Trong bảng 4, chúng tôi báo cáo hiệu suất của một tập con các mô hình con và tăng tốc có thể đạt được bằng cách sử dụng các kỹ thuật lấy mẫu khác nhau như

8

--- TRANG 9 ---
Bảng 4: Tăng tốc thời gian suy luận trên benchmark GSM8K bằng cách sử dụng Speculative Decoding và Adaptive Early-Exit Techniques trên các mô hình SortedNet.

[Bảng được giữ nguyên format]

Auto-regressive decoding, speculative decoding Leviathan et al. [2023], Confidence-based Early-Exit là phương pháp early-exiting dựa trên độ tin cậy của các mô hình con trung gian của Sorted Models Varshney et al. [2023], và Sorted Self-Speculative Decoding, sử dụng thích ứng các mô hình con trung gian để tạo draft token trong thuật toán Speculative Decoding (Hình 3). Như được hiển thị, việc kết hợp SortedNet và speculative decoding có thể cải thiện hiệu quả thời gian mỗi token lên đến 2,09 lần nhanh hơn so với sử dụng auto-regressive cho mô hình kích thước đầy đủ. Ngoài ra, chúng tôi nêu bật chi tiết về các siêu tham số của mỗi thí nghiệm trong Phụ lục C.3 và phân tích thêm đã được cung cấp trong Phụ lục C.7 để hiểu rõ hơn hành vi của phương pháp sortedNet.

Hình 3: (trái) Confidence-based Early-Exit, thoát từ các mô hình con trung gian bất cứ khi nào độ tin cậy vượt qua ngưỡng được xác định (phải) Sorted Self-Speculative Decoding, xác minh các draft token thoát thích ứng từ các mô hình con trung gian bằng mô hình đầy đủ.

Kết luận

Tóm lại, bài báo này đề xuất một phương pháp mới để huấn luyện mạng nơ-ron động tận dụng tính modular của mạng nơ-ron sâu để chuyển đổi hiệu quả giữa các mô hình con trong quá trình suy luận. Phương pháp của chúng tôi sắp xếp các mô hình con dựa trên tính toán/độ chính xác của chúng và huấn luyện chúng bằng sơ đồ cập nhật hiệu quả lấy mẫu ngẫu nhiên các mô hình con trong khi tích lũy gradient. Bản chất stochastic của phương pháp đề xuất giúp thuật toán tổng quát hóa tốt hơn và tránh các lựa chọn tham lam để tối ưu hóa mạnh mẽ nhiều mạng cùng một lúc. Chúng tôi chứng minh thông qua các thí nghiệm rộng rãi rằng phương pháp của chúng tôi vượt trội hơn các phương pháp huấn luyện động trước đây và mang lại các mô hình con chính xác hơn qua các kiến trúc và tác vụ khác nhau. Kiến trúc sorted của mô hình động được đề xuất trong công trình này phù hợp với suy luận hiệu quả mẫu bằng cách cho phép các mẫu dễ hơn thoát khỏi quá trình suy luận tại các layer trung gian. Khám phá hướng này có thể là một lĩnh vực thú vị cho công việc tương lai.

9

--- TRANG 10 ---
Tài liệu tham khảo

[Phần tài liệu tham khảo được giữ nguyên format với tên tiếng Anh]

--- TRANG 11 ---
[Tiếp tục phần tài liệu tham khảo]

--- TRANG 12 ---
Phụ lục

A Nghiên cứu liên quan

Slimmable Networks Yu et al. [2018b] Slimmable networks là phương pháp huấn luyện một mạng nơ-ron đơn lẻ theo cách có thể triển khai với width có thể điều chỉnh tại thời điểm suy luận. Giải pháp này được đề xuất đặc biệt cho kiến trúc CNN và do đó, cần phải xem xét cẩn thận mô-đun batch normalization cho các kích thước width khác nhau. Trong vấn đề này, trong slimmable networks, switchable batch normalization được sử dụng dẫn đến các tham số có thể huấn luyện bổ sung. Trái ngược với slimmable networks, SortedNet của chúng tôi không phụ thuộc vào kiến trúc và hoạt động trong cả chiều depth và width.

Early Exit Xin et al. [2020] Early exit đề cập đến kỹ thuật thêm classifier vào các layer trung gian của mạng nơ-ron đã được huấn luyện. Trong khi các tham số của mô hình chính được đóng băng, các tham số của classifier được cập nhật trong quy trình fine-tuning riêng biệt. Trong phương pháp này, mỗi classifier và mạng tiếp theo của chúng có thể được coi như một mô hình con độc lập. Trong khi giải pháp này tương đối đơn giản, hiệu suất của các mô hình con tụt hậu đáng kể so với mô hình chính. Cũng việc dành một classification head riêng biệt cho mỗi mô hình con có thể tăng đáng kể nhu cầu bộ nhớ tại suy luận.

Dayna-BERT Hou et al. [2020] Dyna-BERT trình bày phương pháp nén động cho các mô hình BERT pre-trained, cho phép điều chỉnh linh hoạt kích thước mô hình, cả về depth và width, trong quá trình suy luận. Trong khi mục tiêu được giới thiệu trong bài báo DynaBERT chia sẻ một số điểm tương đồng với phương pháp của chúng tôi, có một số khác biệt chính. Đầu tiên, trong DynaBERT, chỉ có một vài tập con của mô hình hoạt động, trong khi SortedNet của chúng tôi không dựa trên giả định như vậy. Thứ hai, DynaBERT yêu cầu một mô hình teacher đã được huấn luyện và sử dụng knowledge distillation, trong khi kỹ thuật của chúng tôi hoạt động độc lập với knowledge distillation. Thứ ba, DynaBERT cần tìm kiếm mô hình con tối ưu, trong khi giải pháp của chúng tôi vốn dĩ không cần tìm kiếm. Cuối cùng, khả năng áp dụng của DynaBERT phụ thuộc vào kiến trúc, trong khi phương pháp của chúng tôi không phụ thuộc vào kiến trúc.

Layer-drop Fan et al. [2019] Layer-drop là giải pháp structured dropout tại thời điểm huấn luyện cho phép layer pruning tại thời điểm suy luận. Tương tự như DynaBERT, giải pháp này được áp dụng cho các mô hình ngôn ngữ pre-trained; tuy nhiên, trái ngược với DynaBERT, Layer-drop chỉ nhắm mục tiêu depth của mạng nơ-ron chứ không phải width của chúng. Trong Layer-drop, không có mẫu huấn luyện cố định và bất kỳ layer nào có thể bị bỏ với một xác suất nhất định, được gọi là drop rate. Tại thời điểm suy luận, số lượng layer hoạt động có thể được điều chỉnh bởi drop-rate được thấy trong thời gian huấn luyện của mạng đó (tức là để đạt được hiệu suất tốt nhất trên bất kỳ giá trị drop-rate nào khác, mạng cần được huấn luyện lại.). Layer-drop chỉ hoạt động ở depth trong khi giải pháp của chúng tôi hoạt động cho cả depth và width. Hơn nữa, Layer-Drop yêu cầu các mẫu tìm kiếm cụ thể để bỏ các layer tại thời điểm suy luận và thời gian huấn luyện, trong khi giải pháp của chúng tôi không cần tìm kiếm.

Once-for-All Cai et al. [2020] Once-for-all(OFA) nhắm mục tiêu suy luận hiệu quả qua các thiết bị khác nhau bằng cách đầu tiên huấn luyện một mạng OFA hỗ trợ nhiều mô hình con với các đặc điểm latency/accuracy khác nhau; sau đó nó tìm kiếm giữa các mô hình con khả thi theo yêu cầu accuracy và latency của thiết bị mục tiêu. OFA có bản chất huấn luyện progressive tức là nó đi từ mô hình lớn nhất đến các mô hình con nhỏ hơn. OFA khác với giải pháp của chúng tôi từ các khía cạnh sau: đầu tiên, nó cần teacher và knowledge distillation; thứ hai, OFA yêu cầu Neural Architecture Search (NAS) riêng biệt tại thời điểm suy luận; thứ ba, OFA không phụ thuộc vào kiến trúc (giải pháp của họ dành cho mạng nơ-ron dựa trên CNN trong khi SortedNet của chúng tôi hoạt động cho cả CNN và Transformer). Hơn nữa, OFA khác với giải pháp của chúng tôi về chiến lược lựa chọn mô hình con. Trong khi SortedNet của chúng tôi chọn các mô hình con theo cách sắp xếp, OFA không có bất kỳ giả định cụ thể nào để sắp xếp các mô hình con (xem Hình 4 để biết thêm chi tiết).

Learning Compressible Subspace Nunez et al. [2023] Learning Compressible Subspace (LCS) là kỹ thuật nén thích ứng dựa trên huấn luyện không gian con có thể nén của mạng nơ-ron (sử dụng tổ hợp tuyến tính lồi của hai tập trọng số cho mạng). Trong khi LCS không yêu cầu bất kỳ huấn luyện lại nào tại thời điểm suy luận, giải pháp này có một số hạn chế khác bao gồm: đầu tiên, nó cần gấp đôi bộ nhớ tại thời điểm huấn luyện; thứ hai, việc lựa chọn trọng số ban đầu và

12

--- TRANG 13 ---
Hình 4: So sánh SortedNet và Once For All: trên một mạng giả định 5 layer, chúng tôi cho thấy chiến lược lựa chọn mô hình con của SortedNet khác với phương pháp Once-for-All Cai et al. [2020] như thế nào.

hàm nén không rõ ràng và tùy ý (để lại như một siêu tham số); thứ ba, nó chỉ được thử trên CNN; thứ tư, tương tự như Layer-drop các mô hình con trung gian được huấn luyện ngẫu nhiên điều này sẽ làm cho hiệu suất của mô hình mục tiêu không tối ưu.

B Phân tích lý thuyết

B.1 Hội tụ tham số trong các mạng con được huấn luyện giống hệt nhau

Giả sử f̂ là một mạng con trong một kiến trúc mạng nơ-ron lớn hơn, và f đại diện cho một kiến trúc mạng giống hệt nhau được huấn luyện độc lập. Chúng tôi muốn hiểu mối quan hệ giữa các tham số của hai mạng này, θ cho f̂ và φ cho f, khi chúng được huấn luyện trong các điều kiện giống hệt nhau.

B.2 Giả định về tính liên tục Lipschitz của gradient

Chúng tôi giả sử rằng gradient của các hàm loss cho f̂ và f, được ký hiệu là Lf̂ và Lf tương ứng, là L-Lipschitz liên tục. Điều này ngụ ý rằng:

∥∇Lf̂(θ) − ∇Lf̂(θ′)∥ ≤ L∥θ − θ′∥
∥∇Lf(φ) − ∇Lf(φ′)∥ ≤ L∥φ − φ′∥

cho tất cả θ, θ′ và φ, φ′ trong không gian tham số.

B.3 Quy tắc cập nhật tham số

Các tham số của mạng được cập nhật thông qua gradient descent như sau:
• Cho f̂:
  θt+1 = θt − η∇Lf̂(θt)
• Cho f:
  φt+1 = φt − η∇Lf(φt)

B.4 Suy dẫn ranh giới

Chúng tôi suy dẫn ranh giới về sự khác biệt trong các tham số giữa f̂ và f sau mỗi iteration huấn luyện:

∥θt+1 − φt+1∥ = ∥θt − φt − η(∇Lf̂(θt) − ∇Lf(φt))∥

Áp dụng bất đẳng thức tam giác và tính liên tục Lipschitz của gradient, chúng ta có:

∥θt+1 − φt+1∥ ≤ (1 + ηL)∥θt − φt∥ + ηC

13

--- TRANG 14 ---
trong đó C là một hằng số giới hạn sự khác biệt giữa gradient của các hàm loss của các mạng.

Ranh giới này định lượng sự tiến hóa của sự khác biệt trong các tham số giữa f̂ và f qua các iteration huấn luyện, kết hợp tác động của hằng số Lipschitz L, learning rate η, và hằng số C giới hạn sự khác biệt vốn có trong gradient.

B.5 C không đáng kể dưới điều kiện huấn luyện giống hệt nhau

Cho rằng f̂ và f được huấn luyện dưới các điều kiện hoàn toàn giống hệt nhau (cùng dữ liệu, khởi tạo, và siêu tham số), sự khác biệt trong gradient của chúng có thể được coi là không đáng kể, dẫn chúng ta kết luận rằng C thực tế là zero. Dưới giả định này, ranh giới đơn giản hóa đáng kể:

∥θt+1 − φt+1∥ ≤ (1 + ηL)∥θt − φt∥

Điều này chỉ ra rằng sự khác biệt trong các tham số của f̂ và f được chi phối bởi hằng số Lipschitz L và learning rate η, gợi ý rằng các tham số nên giữ gần nhau trong suốt quá trình huấn luyện, đặc biệt khi C không đáng kể.

B.6 Ranh giới hiệu suất

Chúng tôi muốn tìm ranh giới hiệu suất giữa một mô hình con được huấn luyện (với các tham số tối ưu θ*) và mô hình riêng lẻ tương ứng (với các tham số tối ưu φ*). Hãy giả sử rằng φ* = θ* + Δθ. Khi đó, ranh giới hiệu suất có thể được tính như Δf = f(x;φ*) − f̂(x;θ*) trong giá trị hàm so với giá trị tối ưu do nhiễu loạn tham số

Bước 1: Khai triển Taylor bậc hai Áp dụng khai triển taylor bậc hai cho hàm f(x;φ) quanh (φ = φ*), chúng ta có:

f(x;φ*) = f(x;θ* + Δθ) ≈ 
f(x;θ*) + ∇θf(x;θ*)ᵀΔθ + ½ΔθᵀH(x;θ*)Δθ =
f̂(x;θ*) + ∇θf̂(x;θ*)ᵀΔθ + ½ΔθᵀĤ(x;θ*)Δθ.

Lưu ý rằng f(x;θ) = f̂(x;θ) và H(x;θ*) đề cập đến ma trận Hessian của f tại (θ = θ*).

Bước 2: Điều kiện tối ưu
∇θf̂(x;θ*) = 0
⇒f̂(x;φ*) ≈ f̂(x;θ*) + ½ΔθᵀH(x;θ*)Δθ

Bước 3: Tính liên tục Lipschitz của Gradient
∥∇θf̂(x;θ) − ∇θf̂(x;θ′)∥ ≤ L∥θ − θ′∥

Bước 4: Giới hạn Hessian
∥ΔθᵀĤ(x;θ)Δθ∥ ≤ L∥Δθ∥²

Bước 5: Ước tính độ lệch trong giá trị hàm
Δf = f(x;φ*) − f̂(x;θ*)
Δf ≈ ½ΔθᵀĤ(x;θ)Δθ ≤ ½L∥Δθ∥²

Độ lệch Δf trong giá trị hàm so với giá trị tối ưu do nhiễu loạn tham số Δθ được giới hạn bởi ½L∥Δθ∥² dưới giả định về tính liên tục L-Lipschitz của gradient. Kết quả này ngụ ý rằng độ lệch của giá trị hàm tăng nhiều nhất theo bậc hai với kích thước của nhiễu loạn tham số.

14

--- TRANG 15 ---
C Chi tiết thí nghiệm thêm

C.1 Nghiên cứu Ablation

[Hình 5 và nội dung phân tích được dịch]

Phân tích hội tụ (thời gian huấn luyện) Việc được sắp xếp và chọn ngẫu nhiên một mô hình con tại một thời điểm từ một tập các mô hình con được xác định trước trao quyền cho SortedNet với tỷ lệ hội tụ cao hơn và thời gian huấn luyện nhanh hơn. Hình 5 chứng minh thực nghiệm cho khẳng định này và so sánh sự hội tụ huấn luyện của SortedNet với LCP_p, theo hiểu biết của chúng tôi, LCP_p đứng như phương pháp tiên tiến gần đây nhất. Vì LCS_p sử dụng summation loss trên bốn mô hình con trong mỗi bước huấn luyện và để có so sánh công bằng, chúng tôi do đó báo cáo hiệu suất của SortedNet trong các giá trị gradient accumulation (gacc) khác nhau, trong đó gacc = 4 cung cấp so sánh công bằng với LCS_p. Như được hiển thị trong hình, SortedNet với gacc = 4 hội tụ nhanh hơn hoặc cạnh tranh qua các mô hình con khác nhau. Hơn nữa, SortedNet không yêu cầu bất kỳ for-loop nào trong việc triển khai; do đó điều chỉnh tính toán song song và dẫn đến thời gian chạy nhanh hơn. Chúng tôi nghiên cứu thực nghiệm tính năng này và phát hiện rằng trong cùng một cài đặt.

Tác động của gradient accumulation Mục tiêu của thí nghiệm này là kiểm tra tác động của gradient accumulation (gacc) đối với hiệu suất của SortedNet trong số lượng cập nhật tham số bằng nhau. Bảng 5 trình bày kết quả thu được về độ chính xác cho 4 giá trị gradient accumulation khác nhau. Để đảm bảo số lượng cập nhật bằng nhau, số epoch tối đa được điều chỉnh cho mỗi tình huống, ví dụ gacc = k nhận k lần nhiều epoch hơn gacc = 1. Như kết quả giải thích, việc tăng giá trị gradient accumulation dẫn đến hiệu suất cao hơn cho SortedNet. Quan sát này có thể được quy cho việc tăng tính stochastic của huấn luyện khi gradient accumulation được tăng lên. Do đó, mỗi mô hình con trong SortedNet đóng góp bình đẳng hơn vào việc cập nhật các tham số trọng số, dẫn đến tỷ lệ hội tụ nhanh hơn. Chi tiết thêm được cung cấp trong Phụ lục C.2.

C.2 Ảnh hưởng của gradient accumulation đến hiệu suất SortedNet

[Bảng 5 và 6 với phân tích được dịch]

Thật thú vị khi khám phá liệu việc giới hạn số lượng cập nhật tham số có phải là phương pháp phù hợp để nghiên cứu ảnh hưởng của gradient accumulation đối với SortedNet hay không. Một cách có thể để chứng minh yếu tố này là chạy SortedNet với các giá trị gradient accumulation khác nhau trong khi giữ số lượng cập nhật cố định. Để đạt được điều đó, chúng tôi xem xét cùng cài đặt như Bảng 5 và lặp lại thí nghiệm trong khi cố định số epoch huấn luyện tối đa. Bằng cách cố định giá trị này và tăng giá trị gradient accumulation, chúng tôi ngầm giảm số lượng cập nhật tham số. Bảng 6 báo cáo kết quả. So sánh kết quả của hai bảng này, rõ ràng là số lượng cập nhật đóng vai trò quan trọng trong hiệu suất của mô hình. Ví dụ, khi xem xét gacc = 2, sự sụt giảm hiệu suất trung bình khoảng 2% được quan sát qua tất cả các mô hình con. Sự giảm này chỉ ra rằng mô hình cơ bản cần nhiều thời gian huấn luyện hơn cho các giá trị gacc cao hơn.

15

--- TRANG 16 ---
[Bảng 6 và phần C.3 về siêu tham số được dịch]

C.4 Chi tiết so sánh thời gian huấn luyện

Để so sánh thực nghiệm thời gian huấn luyện giữa SortedNet và LCS_p, thời gian trôi qua mỗi epoch cho năm epoch được ghi lại độc lập cho mỗi phương pháp. Sau đó chúng tôi bỏ qua epoch đầu tiên để giảm tác động của việc tải và khởi tạo lần đầu. Tiếp theo, đối với mỗi phương pháp chúng tôi lấy trung bình của các thời gian trôi qua còn lại. Chúng tôi gọi những thời gian trung bình này (tính bằng giây) là T̄SortedNet = 49.7±2.06 và T̄LCS_p = 292.7±3.17 để đơn giản. Như đã đề cập trong Phần C.1, SortedNet với gacc = 4 có thể được coi là so sánh công bằng với LCS_p. Kết quả là, mỗi epoch trong LCS_p có tầm quan trọng gấp bốn lần SortedNet về tổng số cập nhật tham số. Do đó, chúng ta có thể đơn giản nhân T̄SortedNet với hệ số bốn để cân bằng tác động của chúng về tổng số cập nhật tham số. Bằng cách đó, chúng ta có T̄SortedNet = 198.8, ít hơn gần một phần ba so với T̄LCS_p.

C.5 Chúng ta có thể cải thiện hiệu suất của SortedNet bằng cách điều chỉnh layer classifier không?

Trong Hình 2-a, như đã đề cập trước đó, chúng tôi điều chỉnh hiệu suất của các classifier cho baseline trong thí nghiệm nhưng không cho SortedNet. Do đó, như một thí nghiệm bổ sung, chúng tôi muốn phân tích tác động của việc điều chỉnh classifier đối với hiệu suất của phương pháp đề xuất. Tương tự như thí nghiệm trước, chúng tôi huấn luyện layer classifier thêm 5 epoch cho mỗi mô hình con và báo cáo hiệu suất. Như được hiển thị trong Hình 6, lợi ích cao hơn nhiều đối với các mạng rất nhỏ hơn so với các mạng lớn. Classifier chia sẻ SortedNet đã thực hiện tốt mà không có chi phí tính toán bổ sung cho tất cả các mô hình con nhưng việc điều chỉnh thêm có thể có lợi như đã hiển thị.

C.6 Chúng ta có thể mở rộng SortedNet cho các chiều phức tạp không?

[Bảng 8 với phân tích được dịch]

Trong phần này, chúng tôi quan tâm đến việc nghiên cứu liệu SortedNet có thể áp dụng cho các chiều phức tạp hơn ngoài width và depth hay không. Ví dụ, chúng ta có thể sử dụng SortedNet để sắp xếp Attention Head Vaswani et al. [2017] không. Để đạt được điều này, chúng tôi tiến hành thí nghiệm trên BERT-large Devlin et al. [2019] mà chúng tôi cố gắng sắp xếp thông tin qua nhiều chiều cùng một lúc bao gồm, số lượng layer, hidden dimension, và số lượng attention head. Nói cách khác, chúng tôi cố gắng sắp xếp thông tin trên Bert-large và Bert-base vì Bert-base có thể được xem như một tập con của Bert-large và do đó tôn trọng thuộc tính nested. Như được báo cáo trong Bảng 8, ngoài hiệu suất được báo cáo của Bert-base và Bert-large theo bài báo gốc Devlin et al. [2019], chúng tôi báo cáo hiệu suất của những mô hình này trong cài đặt thí nghiệm của bài báo. Hiệu suất của Bert-base được khởi tạo ngẫu nhiên cũng được báo cáo. Chúng tôi cũng trích xuất một Bert-base từ mô hình Bert-large, và chúng tôi báo cáo hiệu suất của mô hình như vậy trong cùng bảng. Ngoài ra, chúng tôi nêu bật số lượng cập nhật huấn luyện đối với mỗi hàm mục tiêu trước mỗi mô hình. Ví dụ, trong hàng cuối cùng (Sorted BERT LARGE), chúng tôi huấn luyện mô hình Sorted khoảng một nửa số lần (∼3Epochs) trên hàm mục tiêu của Bert-base (LB) và nửa còn lại trên hàm mục tiêu của Bert-large (LL) theo cách lặp ngẫu nhiên như được giới thiệu trong phần 3. Hiệu suất Bert-base đã học với những phương pháp này vẫn khoảng 10% sau một base pre-trained nhưng chúng tôi lập luận rằng đây là giá trị của pre-training. Để nghiên cứu tác động, người ta nên áp dụng SortedNet trong quá trình pre-training mà chúng tôi sẽ để lại cho nghiên cứu tương lai. Tuy nhiên, hiệu suất của Bert-large đã học ngang bằng với một Bert-large riêng lẻ điều này gợi ý rằng việc chia sẻ trọng số không nhất thiết có tác động tiêu cực đối với việc học. Tuy nhiên, bí quyết để đạt được hiệu suất tương tự dường như là chúng ta nên giữ số lượng cập nhật cho mỗi mục tiêu giống như huấn luyện riêng lẻ của Bert-large và Bert-base.

C.7 Tác động của Sorting là gì?

Để hiểu rõ hơn tác động của việc sắp xếp thông tin, chúng tôi thiết kế một thí nghiệm so sánh thứ tự phụ thuộc của tất cả các neuron trong một mạng được sắp xếp. Để giữ thí nghiệm đơn giản, chúng tôi thiết kế một mạng nơ-ron một layer với 10 (hidden dimension) × 2 (input dimension) neuron làm hidden layer và một layer classifier trên đó ánh xạ hidden dimension để dự đoán

17

--- TRANG 18 ---
Hình 6: Độ chính xác phân loại CIFAR10 đã điều chỉnh cho SortedNet (160 Models) và baseline. Lợi ích hiệu suất tương đối của mỗi mô hình con đã được báo cáo ở dưới cùng của mỗi ô so với hiệu suất của cùng mạng mà không có điều chỉnh. Càng trắng càng tốt.

xác suất của một vấn đề 4 lớp. Nhiệm vụ là dự đoán liệu một điểm 2d thuộc về một lớp cụ thể trên tập dữ liệu được tạo tổng hợp. Chúng tôi huấn luyện cả Sorted Network và mạng thông thường trong 10 epoch và tối ưu hóa các mạng bằng optimizer Adam Kingma và Ba [2017] với learning rate 0.01 và batch size 16.

Như có thể thấy trong Bảng 9, hiệu suất của các thứ tự khác nhau trong paradigm huấn luyện mạng nơ-ron gốc có thể khác nhau và không may không có mẫu cụ thể nào trong đó. Do đó, nếu một người tìm kiếm toàn bộ không gian của các thứ tự khác nhau (từ neuron 1 đến neuron n, từ neuron n đến neuron 1, hoặc thậm chí chọn một tập con các neuron bằng các chiến lược khác nhau tức là đối với mạng nửa kích thước kích hoạt mỗi neuron khác như XOXOXOXOXO.) có thể tìm thấy các lựa chọn thay thế tốt hơn hoạt động thậm chí tốt hơn so với

18

--- TRANG 19 ---
Hình 7: Tập dữ liệu được tạo tổng hợp với bốn lớp và với các trung tâm của [[-2, 0], [0, 2], [2, 0], [0, -2]] và độ lệch chuẩn cụm của [0.5, 1, 0.5, 1]. Seed đã được cố định thành 42, và 1000 mẫu đã được tạo.

thứ tự mục tiêu mong muốn. Trong ví dụ này, thứ tự ngược lại trung bình thực hiện tốt hơn thứ tự mục tiêu (86.67% so với 82.22%). Tuy nhiên, với phương pháp đề xuất, chúng ta có thể thấy rõ ràng rằng hiệu suất thứ tự mục tiêu nhất quán tốt hơn nhiều so với thứ tự ngược lại (89.25% so với 59.38%). Điều này có nghĩa là, chúng tôi đã có thể thực thi thứ tự mục tiêu mong muốn như chúng tôi muốn bằng phương pháp đề xuất. Ví dụ, neuron 2 phụ thuộc vào neuron 1 nhiều hơn trong SortedNet so với huấn luyện thông thường. Trong một ví dụ khác, 5 neuron cuối cùng phụ thuộc vào 5 neuron đầu tiên nhiều hơn theo cách khác. Như được hiển thị, hiệu suất của năm neuron đầu tiên là 93.86% trong khi hiệu suất của năm neuron cuối cùng chỉ là 66.06% trong SortedNet. Nói cách khác, lợi ích của việc thêm năm neuron cuối cùng khá nhỏ và rất có thể có thể cắt tỉa, trong khi 5 neuron đầu tiên chứa hầu hết thông tin có giá trị. Thật thú vị khi nghiên cứu thêm về sự phụ thuộc của các neuron với nhau và với các số liệu khác mà chúng tôi sẽ để lại cho nghiên cứu tương lai.

19

--- TRANG 20 ---
Bảng 9: Sự phụ thuộc thứ tự của tất cả các neuron trong mạng sử dụng phương pháp đề xuất (SortedNet) và huấn luyện thông thường qua 5 lần chạy ngẫu nhiên. X có nghĩa là chúng tôi sử dụng neuron như hiện có, và O có nghĩa là chúng tôi loại bỏ tác động của neuron đó bằng cách làm cho nó bằng 0. ↑cao hơn thì tốt hơn, ↓thấp hơn thì tốt hơn.

[Bảng 9 được giữ nguyên format với các giá trị số và ký hiệu X, O]

20
