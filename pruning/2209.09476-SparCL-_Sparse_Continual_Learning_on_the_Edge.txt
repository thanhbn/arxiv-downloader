# 2209.09476.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2209.09476.pdf
# File size: 2094958 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SparCL: Sparse Continual Learning on the Edge
Zifeng Wang1;y, Zheng Zhan1;y, Yifan Gong1, Geng Yuan1, Wei Niu2, Tong Jian1,
Bin Ren2, Stratis Ioannidis1, Yanzhi Wang1, Jennifer Dy1
1Northeastern University,2College of William and Mary
{zhan.zhe, gong.yifa, geng.yuan, yanz.wang}@northeastern.edu ,
{zifengwang, jian, ioannidis, jdy}@ece.neu.edu ,
wniu@email.wm.edu, bren@cs.wm.edu
Abstract
Existing work in continual learning (CL) focuses on mitigating catastrophic for-
getting, i.e., model performance deterioration on past tasks when learning a new
task. However, the training efﬁciency of a CL system is under-investigated, which
limits the real-world application of CL systems under resource-limited scenarios.
In this work, we propose a novel framework called Sparse Continual Learning
(SparCL), which is the ﬁrst study that leverages sparsity to enable cost-effective
continual learning on edge devices. SparCL achieves both training acceleration
and accuracy preservation through the synergy of three aspects: weight sparsity ,
data efﬁciency , and gradient sparsity . Speciﬁcally, we propose task-aware dynamic
masking (TDM) to learn a sparse network throughout the entire CL process, dy-
namic data removal (DDR) to remove less informative training data, and dynamic
gradient masking (DGM) to sparsify the gradient updates. Each of them not only
improves efﬁciency, but also further mitigates catastrophic forgetting. SparCL
consistently improves the training efﬁciency of existing state-of-the-art (SOTA) CL
methods by at most 23less training FLOPs, and, surprisingly, further improves
the SOTA accuracy by at most 1:7%. SparCL also outperforms competitive base-
lines obtained from adapting SOTA sparse training methods to the CL setting in
both efﬁciency and accuracy. We also evaluate the effectiveness of SparCL on a
real mobile phone, further indicating the practical potential of our method. Source
code will be released.
1 Introduction
The objective of Continual Learning (CL) is to enable an intelligent system to accumulate knowledge
from a sequence of tasks, such that it exhibits satisfying performance on both old and new tasks ( 31).
Recent methods mostly focus on addressing the catastrophic forgetting (42) problem – learning
model tends to suffer performance deterioration on previously seen tasks. However, in the real world,
when the CL applications are deployed in resource-limited platforms ( 47) such as edge devices,
the learning efﬁciency, w.r.t. both training speed and memory footprint, are also crucial metrics of
interest, yet they are rarely explored in prior CL works.
Existing CL methods can be categorized into regularization-based ( 2;31;36;67), rehearsal-based ( 8;
11;49;60), and architecture-based ( 30;41;51;57;58;69). Both regularization- and rehearsal-based
methods directly train a dense model, which might even be over-parametrized for the union of all
tasks ( 18;38); Though several architecture-based methods ( 50;56;63) start with a sparse sub-network
from the dense model, they still grow the model size progressively to learn emerging tasks. The
aforementioned methods, although striving for greater performance with less forgetting, still introduce
signiﬁcant memory and computation overhead during the whole CL process.
yBoth authors contributed equally to this work
36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2209.09476v1  [cs.LG]  20 Sep 2022

--- PAGE 2 ---
Figure 1: Left: Overview of SparCL. SparCL consists of three complementary components: task-aware dynamic
masking (TDM) for weight sparsity, dynamic data removal (DDR) for data efﬁciency, and dynamic gradient
masking (DGM) for gradient sparsity. Right: SparCL successfully preserves the accuracy and signiﬁcantly
improves efﬁciency over DER++ ( 8), one of the SOTA CL methods, with different sparsity ratios on the Split
Tiny-ImageNet (15) dataset.
Recently, another stream of work, sparse training (4; 19; 34) has emerged as a new training trend to
achieve training acceleration, which embraces the promising training-on-the-edge paradigm. With
sparse training, each iteration takes less time with the reduction in computation achieved by sparsity,
under the traditional i.i.d. learning setting. Inspired by these sparse training methods, we naturally
think about introducing sparse training to the ﬁeld of CL. A straightforward idea is to directly combine
existing sparse training methods, such as SNIP ( 34), RigL ( 19), with a rehearsal buffer under the
CL setting. However, these methods fail to consider key challenges in CL to mitigate catastrophic
forgetting, for example, properly handling transition between tasks. As a result, these sparse training
methods, though enhancing training efﬁciency, cause signiﬁcant accuracy drop (see Section 5.2).
Thus, we would like to explore a general strategy, which is orthogonal to existing CL methods, that
not only leverages the idea of sparse training for efﬁciency, but also addresses key challenges in CL
to preserve (or even improve) accuracy.
In this work, we propose Sparse Continual Learning (SparCL), a general framework for cost-effective
continual learning, aiming at enabling practical CL on edge devices. As shown in Figure 1 (left),
SparCL achieves both learning acceleration and accuracy preservation through the synergy of three
aspects: weight sparsity ,data efﬁciency , and gradient sparsity . Speciﬁcally, to maintain a small
dynamic sparse network during the whole CL process, we develop a novel task-aware dynamic
masking (TDM) strategy to keep only important weights for both the current and past tasks, with
special consideration during task transitions. Moreover, we propose a dynamic data removal (DDR)
scheme, which progressively removes “easy-to-learn” examples from training iterations, which
further accelerates the training process and also improves accuracy of CL by balancing current and
past data and keeping more informative samples in the buffer. Finally, we provide an additional
dynamic gradient masking (DGM) strategy to leverage gradient sparsity for even better efﬁciency
and knowledge preservation of learned tasks, such that only a subset of sparse weights are updated.
Figure 1 (right) demonstrates that SparCL successfully preserves the accuracy and signiﬁcantly
improves efﬁciency over DER++ (8), one of the SOTA CL methods, under different sparsity ratios.
SparCL is simple in concept, compatible with various existing rehearsal-based CL methods, and
efﬁcient under practical scenarios. We conduct comprehensive experiments on multiple CL bench-
marks to evaluate the effectiveness of our method. We show that SparCL works collaboratively with
existing CL methods, greatly accelerates the learning process under different sparsity ratios, and
even sometimes improves upon the state-of-the-art accuracy. We also establish competing baselines
by combining representative sparse training methods with advanced rehearsal-based CL methods.
SparCL again outperforms these baselines in terms of both efﬁciency and accuracy. Most importantly,
we evaluate our SparCL framework on real edge devices to demonstrate the practical potential of
our method. We are not aware of any prior CL works that explored this area and considered the
constraints of limited resources during training.
In summary, our work makes the following contributions:
•We propose Sparse Continual Learning (SparCL), a general framework for cost-effective continual
learning, which achieves learning acceleration through the synergy of weight sparsity ,data efﬁ-
ciency , and gradient sparsity . To the best of our knowledge, our work is the ﬁrst to introduce the
idea of sparse training to enable efﬁcient CL on edge devices.
2

--- PAGE 3 ---
•SparCL shows superior performance compared to both conventional CL methods and CL-adapted
sparse training methods on all benchmark datasets, leading to at most 23less training FLOPs
and, surprisingly, 1:7%improvement over SOTA accuracy.
•We evaluate SparCL on a real mobile edge device, demonstrating the practical potential of our
method and also encouraging future research on CL on-the-edge. The results indicate that our
framework can achieve at most 3:1training acceleration.
2 Related work
2.1 Continual Learning
The main focus in continual learning (CL) has been mitigating catastrophic forgetting. Existing
methods can be classiﬁed into three major categories. Regularization-based methods ( 2;31;36;67)
limit updates of important parameters for the prior tasks by adding corresponding regularization terms.
While these methods reduce catastrophic forgetting to some extent, their performance deteriorates
under challenging settings ( 39), and on more complex benchmarks ( 49;60).Rehearsal-based
methods (12; 13; 24) save examples from previous tasks into a small-sized buffer to train the model
jointly with the current task. Though simple in concept, the idea of rehearsal is very effective in
practice and has been adopted by many state-of-the-art methods ( 8;10;48).Architecture-based
methods ( 41;50;56;58;62) isolate existing model parameters or assign additional parameters for
each task to reduce interference among tasks. As mentioned in Section 1, most of these methods use
a dense model without consideration of efﬁciency and memory footprint, thus are not applicable to
resource-limited settings. Our work, orthogonal to these methods, serves as a general framework for
making these existing methods efﬁcient and enabling a broader deployment, e.g., CL on edge devices.
A limited number of works explore sparsity in CL, however, for different purposes. Several methods
(40;41;52;56) incorporate the idea of weight pruning ( 23) to allocate a sparse sub-network for
each task to reduce inter-task interference. Nevertheless, these methods still reduce the full model
sparsity progressively for every task and ﬁnally end up with a much denser model. On the contrary,
SparCL maintains a sparse network throughout the whole CL process, introducing great efﬁciency
and memory beneﬁts both during training and at the output model. A recent work ( 14) aims at
discovering lottery tickets ( 20) under CL, but still does not address efﬁciency. However, the existence
of lottery tickets in CL serves as a strong justiﬁcation for the outstanding performance of our SparCL.
2.2 Sparse Training
There are two main approaches for sparse training: ﬁxed-mask sparse training and dynamic sparse
training. Fixed-mask sparse training methods ( 34;53;55;59) ﬁrst apply pruning, then execute
traditional training on the sparse model with the obtained ﬁxed mask. The pre-ﬁxed structure limits
the accuracy performance, and the ﬁrst stage still causes huge computation and memory consumption.
To overcome these drawbacks, dynamic mask methods ( 4;16;19;44;45) adjust the sparsity topology
during training while maintaining low memory footprint. These methods start with a sparse model
structure from an untrained dense model, then combine sparse topology exploration at the given
sparsity ratio with the sparse model training. Recent work ( 66) further considers to incorporate data
efﬁciency into sparse training for better training accelerations. However, all prior sparse training
works are focused on the traditional training setting, while CL is a more complicated and difﬁcult
scenario with inherent characteristics not explored by these works. In contrast to prior sparse training
methods, our work explores a new learning paradigm that introduces sparse training into CL for
efﬁciency and also addresses key challenges in CL, mitigating catastrophic forgetting.
3 Continual Learning Problem Setup
In supervised CL, a model flearns from a sequence of tasks D=fD1;:::;DTg, where each task
Dt=f(xt;i;yt;i)gnt
i=1consists of input-label pairs, and each task has a disjoint set of classes. Tasks
arrive sequentially, and the model must adapt to them. At the t-th step, the model gains access to
data from the t-th task. However, a small ﬁx-sized rehearsal buffer Mis allowed to save data from
prior tasks. At test time, the easiest setting is to assume task identity is known for each coming test
example, named task-incremental learning (Task-IL). If this assumption does not hold, we have the
3

--- PAGE 4 ---
......
T ask-aware Dynamic Masking 
(TDM) (a) -and-ExpandShrinkDynamic Data Removal 
(DDR)Dynamic Gradient Masking 
(DGM)
Remove 
“easier” samplesUpdate 
important gradients 
(b) -and-ShrinkExpand
T ask 1T ask t(a)TDMDDRDGM
(b)T ask T   EpochsFigure 2: Illustration of the SparCL workﬂow. Three components work synergistically to improve training
efﬁciency and further mitigate catastrophic forgetting for preserving accuracy.
more difﬁcult class-incremental learning (Class-IL) setting. In this work, we mainly focus on the
more challenging Class-IL setting, and only report Task-IL performance for reference.
The goal of conventional CL is to train a model sequentially that performs well on all tasks at test
time. The main evaluation metric is average test accuracy on all tasks. In real-world resource-
limited scenarios, we should further consider training efﬁciency of the model. Thus, we measure the
performance of the model more comprehensively by including training FLOPs and memory footprint.
4 Sparse Continual Learning (SparCL)
Our method, Sparse Continual Learning, is a uniﬁed framework composed of three complementary
components: task-aware dynamic masking for weight sparsity, dynamic data removal for data
efﬁciency, and dynamic gradient masking for gradient sparsity. The entire framework is shown in
Figure 2. We will illustrate each component in detail in this section.
4.1 Task-aware Dynamic Masking
To enable cost-effective CL in resource limited scenarios, SparCL is designed to maintain a dynamic
structure when learning a sequence of tasks, such that it not only achieves high efﬁciency, but also
intelligently adapts to the data stream for better performance. Speciﬁcally, we propose a strategy
named task-aware dynamic masking (TDM), which dynamically removes less important weights
and grows back unused weights for stronger representation power periodically by maintaining a
single binary weight mask throughout the CL process. Different from typical sparse training work,
which only leverages the weight magnitude ( 44) or the gradient w.r.t. data from a single training
task ( 19;66), TDM considers also the importance of weights w.r.t. data saved in the rehearsal buffer,
as well as the switch between CL tasks.
Speciﬁcally, TDM strategy starts from a randomly initialized binary mask M=M0, with a given
sparsity constraint kMk0=kk0= 1 s, wheres2[0;1]is the sparsity ratio. Moreover, it makes
different intra- and inter-task adjustments to keep a dynamic sparse set of weights based on their
continual weight importance (CWI). We summarize the process of task-aware dynamic masking in
Algorithm 1 and elaborate its key components in detail below.
Continual weight importance (CWI). For a model fparameterized by , the CWI of weight
wis deﬁned as follows:
CWI(w) =kwk1+k@~L(Dt;)
@wk1+k@L(M;)
@wk1; (1)
whereDtdenotes the training data from the t-th task,Mis the current rehearsal buffer, and ,are
coefﬁcients to control the inﬂuence of current and buffered data, respectively. Moreover, Lrepresents
the cross-entropy loss for classiﬁcation, while ~Lis the single-head (1) version of the cross-entropy
loss, which only considers classes from the current task by masking out the logits of other classes.
Intuitively, CWI ensures we keep (1) weights of larger magnitude for output stability, (2) weights
important for the current task for learning capacity, and (3) weights important for past data to mitigate
catastrophic forgetting. Moreover, inspired by the classiﬁcation bias in CL ( 1), we use the single-head
cross-entropy loss when calculating importance score w.r.t. the current task to make the importance
estimation more accurate.
4

--- PAGE 5 ---
Algorithm 1: Task-aware Dynamic Masking (TDM)
Input : Model weight , number of tasks T, training epochs of the t-th taskKt, binary sparse
maskM, sparsity ratio s, intra-task adjustment ratio pintra , inter-task adjustment ratio
pinter , update interval k
Initialize:,M, s.t.kMk0=kk0= 1 s
fort= 1;:::;T do
fore= 1;:::;KTdo
ift>1then
/* Inter-task adjustment */
ExpandMby randomly adding unused weights,
s.t.kMk0=kk0= 1 (s pinter)
ife=kthen
ShrinkMby removing the least important weights according to equation 1,
s.t.kMk0=kk0= 1 s
end
end
ifemodk= 0then
/* Intra-task adjustment */
ShrinkMby removing the least important weights according to equation 1,
s.t.kMk0=kk0= 1 (s+pintra)
ExpandMby randomly adding unused weights,
s.t.kMk0=kk0= 1 s
end
UpdateMvia backpropagation
end
end
Intra-task adjustment. When training the t-th task, a natural assumption is that the data distribution
is consistent inside the task, thus we would like to update the sparse model in a relatively stable way
while keeping its ﬂexibility. Thus, in Algorithm 1, we choose to update the sparsity mask Min a
shrink-and-expand way everykepochs. We ﬁrst remove pintra of the weights of least CWI to retain
learned knowledge so far. Then we randomly select unused weights to recover the learning capacity
for the model and keep the sparsity ratio sunchanged.
Inter-task adjustment. When tasks switches, on the contrary, we assume data distribution shifts
immediately. Ideally, we would like the model to keep the knowledge learned from old tasks as much
as possible, and to have enough learning capacity to accommodate the new task. Thus, instead of
the shrink-and-expand strategy for intra-task adjustment, we follow an expand-and-shrink scheme.
Speciﬁcally, at the beginning of the (t+ 1) -th task, we expand the sparse model by randomly adding
a proportion of pinter unused weights. Intuitively, the additional learning capacity facilitates fast
adoption of new knowledge and reduces interference with learned knowledge. We allow our model to
have smaller sparsity ( i.e., larger learning capacity) temporarily for the ﬁrst kepochs as a warm-
up period, and then remove the pinter weights with least CWI, following the same process in the
intra-task case, to satisfy the sparsity constraint.
4.2 Dynamic Data Removal
In addition to weight sparsity, decreasing the amount of training data can be directly translated into
the saving of training time without any requirements for hardware support. Thus, we would also like
to explore data efﬁciency to reduce the training workload. Some prior CL works select informative
examples to construct the rehearsal buffer ( 3;6;64). However, the main purpose of them is not
training acceleration, thus they either introduce excessive computational cost or consider different
problem settings. By considering the features of CL, we present a simple yet effective strategy,
dynamic data removal (DDR), to reduce training data for further acceleration.
We measure the importance of each training example by the occurrence of misclassiﬁcation ( 54;66)
during CL. In TDM, the sparse structure of our model updates periodically every kepochs, so we
align our data removal process with the update of weight mask for further efﬁciency and training
5

--- PAGE 6 ---
stability. In Section 4.1, we have partitioned the training process for the t-th task into Nt=Kt=k
stages based on the dynamic mask update. Therefore, we gradually remove training data at the end of
i-th stage, based on the following policy: 1) Calculate the total number of misclassiﬁcations fi(xj)
for each training example during the i-th stage. 2) Remove a proportion of itraining samples with
the least number of misclassiﬁcations. Although our main purpose is to keep the “harder” examples
to learn to consolidate the sparse model, we can get further beneﬁts for better CL result. First, the
removal of “easier” examples increases the probability that “harder” examples to be saved to the
rehearsal buffer, given the common strategy, e.g.reservoir sampling ( 13), to buffer examples. Thus,
we construct a more informative buffer in a implicit way without heavy computation. Moreover, since
the buffer size is much smaller than the training set size of each task, the data from the buffer and the
new task is highly imbalanced, dynamic data removal also relieves the data imbalance issue.
Formally, we set the data removal proportion for each task as 2[0;1], and a cutoff stage, such that:
cutoffX
i=1i=;NkX
i=cutoff +1i= 0 (2)
The cutoff stage controls the trade-off between efﬁciency and accuracy: When we set the cutoff stage
earlier, we reduce the training time for all the following stages; however, when the cutoff stage is set
too early, the model might underﬁt the removed training data. Note that when we set i= 0for all
i= 1;2;:::;Ntandcutoff =Nt, we simply recover the vanilla setting without any data efﬁciency
considerations. In our experiments, we assume i==cutoff , i.e., removing equal proportion of
data at the end of every stage, for simplicity. We also conduct comprehensive exploration study for 
and the selection of the cutoff stage in Section 5.3 and Appendix D.3.
4.3 Dynamic Gradient Masking
With TDM and DDR, we can already achieve bi-level efﬁciency during training. To further boost
training efﬁciency, we explore sparsity in gradient and propose dynamic gradient masking (DGM)
for CL. Our method focuses on reducing computational cost by only applying the most important
gradients onto the corresponding unpruned model parameters via a gradient mask. The gradient mask
is also dynamically updated along with the weight mask deﬁned in Section 4.1. Intuitively, while
targeting for better training efﬁciency, DGM also promotes the preservation of past knowledge by
preventing a fraction of weights from update.
Formally, our goal here is to ﬁnd a subset of unpruned parameters (or, equivalently, a gradient mask
MG) to update over multiple training iterations. For a model fparameterized by , we have the
corresponding gradient matrix Gcalculated during each iteration. To prevent the pruned weights
from updating, the weight mask Mwill be applied onto the gradient matrix GasGMduring
backpropagation. Besides the gradients of pruned weights, we in addition consider to remove less
important gradients for faster training. To achieve this, we introduce the continual gradient importance
(CGI) based on the CWI to measure the importance of weight gradients.
CGI(w) =k@~L(Dt;)
@wk1+k@L(M;)
@wk1: (3)
We remove a proportion qof non-zero gradients from Gwith less importance measured by CGI
and we havekMGk0=kk0= 1 (s+q). The gradient mask MGis then applied onto the gradient
matrixG. During the entire training process, the gradient mask MGis updated with a ﬁxed interval.
5 Experiment
5.1 Experiment Setting
Datasets. We evaluate our SparCL on two representative CL benchmarks, Split CIFAR-10 ( 32)
and Split Tiny-ImageNet ( 15) to verify the efﬁcacy of SparCL. In particular, we follow ( 8;67) by
splitting CIFAR-10 and Tiny-ImageNet into 5 and 10 tasks, each of which consists of 2 and 20 classes
respectively. Dataset licensing information can be found in Appendix C.
Comparing methods. In particular, we select CL methods of all kinds including regularization-
based (EWC ( 31), LwF ( 36)), architecture-based (PackNet ( 41), LPS ( 56)), and rehearsal-based
6

--- PAGE 7 ---
Table 1: Comparison with CL methods. SparCL consistently improves training efﬁciency of the corresponding
CL methods while preserves (or even improves) accuracy on both class- and task-incremental settings.
Method Sparsity Buffer sizeSplit CIFAR-10 Split Tiny-ImageNet
Class-IL (") Task-IL (")FLOPs Train
1015(#)Class-IL (") Task-IL (")FLOPs Train
1016(#)
EWC (31)0.00 –19.490.12 68.293.92 8.3 7.580.10 19.200.31 13.3
LwF (36) 19.610.05 63.292.35 8.3 8.460.22 15.850.58 13.3
PackNet (41)0.50y–- 93.73 0.55 5.0 – 61.88 1.01 7.3
LPS (56) - 94.50 0.47 5.0 – 63.37 0.83 7.3
A-GEM (12)
0.00 20020.040.34 83.881.49 11.1 8.070.08 22.770.03 17.8
iCaRL (49) 49.023.20 88.992.13 11.1 7.530.79 28.191.47 17.8
FDR (5) 30.912.74 91.010.68 13.9 8.700.19 40.360.68 22.2
ER (13) 44.791.86 91.190.94 11.1 8.490.16 38.172.00 17.8
DER++ (8) 64.881.17 91.920.60 13.9 10.961.17 40.871.16 22.2
SparCL-ER 75 46.890.68 92.020.72 2.0 8.980.38 39.140.85 3.2
SparCL-DER++ 750.7566.300.98 94.060.45 2.5 12.730.40 42.060.73 4.0
SparCL-ER 90 45.811.05 91.490.47 0.9 8.670.41 38.790.39 1.4
SparCL-DER++ 900.90 20065.791.33 93.730.24 1.1 12.271.06 41.171.31 1.8
SparCL-ER 95 44.590.23 91.070.64 0.5 8.430.09 38.200.46 0.8
SparCL-DER++ 950.9565.181.25 92.970.37 0.6 10.760.62 40.540.98 1.0
A-GEM (12)
0.00 50022.670.57 89.481.45 11.1 8.060.04 25.330.49 17.8
iCaRL (49) 47.553.95 88.222.62 11.1 9.381.53 31.553.27 17.8
FDR (5) 28.713.23 93.290.59 13.9 10.540.21 49.880.71 22.2
ER (13) 57.740.27 93.610.27 11.1 9.990.29 48.640.46 17.8
DER++ (8) 72.701.36 93.880.50 13.9 19.381.41 51.910.68 22.2
SparCL-ER 75 60.800.22 93.820.32 2.0 10.480.29 50.830.69 3.2
SparCL-DER++ 750.7574.090.84 95.190.34 2.5 20.750.88 52.190.43 4.0
SparCL-ER 90 59.340.97 93.330.10 0.9 10.120.53 49.461.22 1.4
SparCL-DER++ 900.90 50073.420.95 94.820.23 1.1 19.620.67 51.930.36 1.8
SparCL-ER 95 57.750.45 92.730.34 0.5 9.910.17 48.570.50 0.8
SparCL-DER++ 950.9572.140.78 94.390.15 0.6 19.011.32 51.260.78 1.0
yPackNet and LPS actually have a decreased sparsity after learning every task, we use 0.50 to roughly represent the average sparsity.
(A-GEM ( 12), iCaRL ( 43), FDR ( 5), ER ( 13), DER++ ( 8)) methods. Note that PackNet and LPS
are only compatible with task-incremental learning. We also adapt representative sparse training
methods (SNIP ( 34), RigL ( 19)) to the CL setting by combining them with DER++ (SNIP-DER++,
RigL-DER++).
Variants of our method. To show the generality of SparCL, we combine it with DER++ (one of
the SOTA CL methods), and ER (simple and widely-used) as SparCL-DER++ andSparCL-ER ,
respectively. We also vary the weight sparsity ratio ( 0:75;0:90;0:95) of SparCL for a comprehensive
evaluation.
Evaluation metrics. We use the average accuracy on all tasks to evaluate the performance of the ﬁnal
model. Moreover, we evaluate the training FLOPs ( 19), and memory footprint ( 66) (including feature
map pixels and model parameters during training) to demonstrate the efﬁciency of each method.
Please see Appendix D.1 for detailed deﬁnitions of these metrics.
Experiment details. For fair comparison, we strictly follow the settings in prior CL work ( 8;28). We
sets the per task training epochs to 50and100for Split CIFAR-10 and Tiny-ImageNet, respectively,
with a batch size of 32. For the model architecture, We follow ( 8;49) and adopt the ResNet-18 ( 25)
without any pre-training. We also use the best hyperparameter setting reported in ( 8;56) for CL
methods, and in ( 19;34) for CL-adapted sparse training methods. For SparCL and its competing
CL-adapted sparse training methods, we adopt a uniform sparsity ratio for all convolutional layers.
Please see Appendix D for other details.
5.2 Main Results
Comparison with CL methods. Table 1 summarizes the results on Split CIFAR-10 and Tiny-
ImageNet, under both class-incremental (Class-IL) and task-incremental (Task-IL) settings. From
Table 1, we can clearly tell that SparCL signiﬁcantly improves ER and DER++, while also outperforms
other CL baselines, in terms of training efﬁciency (measured in FLOPs). With higher sparsity ratio,
SparCL leads to less training FLOPs. Notably, SparCL achieves 23training efﬁciency improvement
upon DER++ with a sparsity ratio of 0.95. On the other hand, our framework also improves the
average accuracy of ER and DER++ consistently under all cases with a sparsity ratio of 0.75 and 0.90,
and only slight performance drop when sparsity gets larger as 0.95. In particular, SparCL-DER++
7

--- PAGE 8 ---
Table 2: Comparison with CL-adapted sparse training methods. All methods are combined with DER++ with a
500 buffer size. SparCL outperforms all methods in both accuracy and training efﬁciency, under all sparsity
ratios. All three methods here can save 20%51% memory footprint, please see Appendix D.2 for details.
Method SpasitySplit CIFAR-10 Split Tiny-ImageNet
Class-IL (")FLOPs Train
1015(#)Class-IL (")FLOPs Train
1016(#)
DER++ (8) 0.00 72.701.36 13.9 19.381.41 22.2
SNIP-DER++ (34) 69.820.72 1.6 16.130.61 2.5
RigL-DER++ (19) 0.90 69.860.59 1.6 18.360.49 2.5
SparCL-DER++ 90 73.420.95 1.1 19.620.67 1.8
SNIP-DER++ (34) 66.070.91 0.9 14.760.52 1.5
RigL-DER++ (19) 0.95 66.531.13 0.9 15.880.63 1.5
SparCL-DER++ 95 72.140.78 0.6 19.011.32 1.0
Table 3: Ablation study on Split-CIFAR10 with 0.75 sparsity ratio. All components contributes to the overall
performance, in terms of both accuracy and efﬁciency (training FLOPs and memory footprint).
TDM DDR DGM Class-IL (")FLOPs Train
1015(#)Memory
Footprint (#)
7 7 7 72.70 13.9 247MB
3 7 7 73.37 3.6 180MB
3 3 7 73.80 2.8 180MB
3 7 3 73.97 3.3 177MB
3 3 3 74.09 2.5 177MB
with 0.75 sparsity ratio sets new SOTA accuracy, with all buffer sizes under both benchmarks. The
outstanding performance of SparCL indicates that our proposed strategies successfully preserve
accuracy by further mitigating catastrophic forgetting with a much sparser model. Moreover, the
improvement that SparCL brings to two different existing CL methods shows the generalizability of
SparCL as a uniﬁed framework, i.e., it has the potential to be combined with a wide array of existing
methods.
We would also like to take a closer look at PackNet and LPS, which also leverage the idea of sparsity
to split the model by different tasks, a different motivation from training efﬁciency. Firstly, they are
only compatible with the Task-IL setting, since they leverage task identity at both training and test
time. Moreover, the model sparsity of these methods reduces with the increasing number of tasks,
which still leads to much larger overall training FLOPs than that of SparCL. This further demonstrates
the importance of keeping a sparse model without permanent expansion throughout the CL process.
Comparison with CL-adapted sparse training methods. Table 2 shows the result under the more
difﬁcult Class-IL setting. SparCL outperforms all CL-adapted sparse training methods in both
accuracy and training FLOPs. The performance gap between SparCL-DER++ and other methods
gets larger with a higher sparsity. SNIP- and RigL-DER++ achieve training acceleration at the cost of
compromised accuracy, which suggests that keeping accuracy is a non-trivial challenge for existing
sparse training methods under the CL setting. SNIP generates the static initial mask after network
initialization which does not consider the structure suitability among tasks. Though RigL adopts a
dynamic mask, the lack of task-aware strategy prevents it from generalizing well to the CL setting.
5.3 Effectiveness of Key Components
Ablation study. We provide a comprehensive ablation study in Table 3 using SparCL-DER++ with
0.75 sparsity on Split CIFAR10. Table 3 demonstrates that all components of our method contribute
to both efﬁciency and accuracy improvements. Comparing row 1 and 2, we can see that the majority
of FLOPs decrease results from TDM. Interestingly, TDM leads to an increase in accuracy, indicating
TDM generates a sparse model that is even more suitable for learning all tasks than then full dense
model. Comparing row 2 and 3, we can see that DDR indeed further accelerates training by removing
less informative examples. As discussed in Section 4.2, when we remove a certain number of samples
(30% here), we achieve a point where we keep as much informative samples as we need, and also
balance the current and buffered data. Comparing row 2 and 4, DGM reduce both training FLOPs and
memory footprint while improve the performance of the network. Finally, the last row demonstrates
the collaborative performance of all components. We also show the same ablation study with 0.90
sparsity in Appendix D.4 for reference. Detail can be found in Appendix D.1.
8

--- PAGE 9 ---
Figure 3: Comparison between DDR and One-
shot ( 66) data removal strategy w.r.t. different data
removal proportion . DDR outperforms One-shot
and also achieves better accuracy when 30% .
Figure 4: Comparison with CL-adapted sparse
training methods in training acceleration rate
and accuracy results. The radius of circles are
measured by memory footprint.
Exploration on DDR. To understand the inﬂuence of the data removal proportion , and the cutoff
stage for each task, we show corresponding experiment results in Figure 3 and Appendix D.3,
respectively. In Figure 3, we ﬁx cutoff = 4,i.e., gradually removing equal number of examples
every 5 epochs until epoch 20, and vary from 10% to90%. We also compare DDR with One-shot
removal strategy ( 66), which removes all examples at once at cutoff . DDR outperforms One-shot
consistently with different in average accuracy. Also note that since DDR removes the examples
gradually before the cutoff stage, DDR is more efﬁcient than One-shot. When 30%, we also
observe increased accuracy of DDR compared with the baseline without removing any data. When
40%, the accuracy gets increasingly lower for both strategies. The intuition is that when DDR
removes a proper amount of data, it removes redundant information while keeps the most informative
examples. Moreover, as discussed in Section 4.2, it balances the current and buffered data, while
also leave informative samples in the buffer. When DDR removes too much data, it will also lose
informative examples, thus the model has not learned these examples well before removal.
Exploration on DGM. We test the efﬁcacy of DGM at different sparsity levels. Detailed exploratory
experiments are shown in Appendix D.5 for reference. The results indicate that by setting the
proportionqwithin an appropriate range, DGM can consistently improve the accuracy performance
regardless of the change of weight sparsity.
5.4 Mobile Device Results
The training acceleration results are measured on the CPU of an off-the-shelf Samsung Galaxy S20
smartphone, which has the Qualcomm Snapdragon 865 mobile platform with a Qualcomm Kryo 585
Octa-core CPU. We run each test on a batch of 32 images to denote the training speed. The detail of
on-mobile compiler-level optimizations for training acceleration can be found in Appendix E.1.
The acceleration results are shown in Figure 4. SparCL can achieve approximately 3.1 and 2.3
training acceleration with 0.95 sparsity and 0.90 sparsity, respectively. Besides, our framework can
also save 51% and 48% memory footprint when the sparsity is 0.95 and 0.90. Furthermore, the
obtained sparse models save the storage consumption by using compressed sparse row (CSR) storage
and can be accelerated to speed up the inference on-the-edge. We provide on-mobile inference
acceleration results in Appendix E.2.
6 Conclusion
This paper presents a uniﬁed framework named SparCL for efﬁcient CL that achieves both learning
acceleration and accuracy preservation. It comprises three complementary strategies: task-aware
dynamic masking for weight sparsity, dynamic data removal for data efﬁciency, and dynamic gradient
masking for gradient sparsity. Extensive experiments on standard CL benchmarks and real-world edge
device evaluations demonstrate that our method signiﬁcantly improves upon existing CL methods and
outperforms CL-adapted sparse training methods. We discuss the limitations and potential negative
social impacts of our method in Appendix A and B, respectively.
9

--- PAGE 10 ---
References
[1]Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon.
Ss-il: Separated softmax for incremental learning. In CVPR , 2021. 4
[2]Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte-
laars. Memory aware synapses: Learning what (not) to forget. In ECCV , 2018. 1, 3
[3]Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection
for online continual learning. NeurIPS , 2019. 5
[4]Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring:
Training very sparse deep networks. In ICLR , 2018. 2, 3
[5]Ari S Benjamin, David Rolnick, and Konrad Kording. Measuring and regularizing networks in
function space. arXiv preprint arXiv:1805.08289 , 2018. 7
[6]Zalán Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for
continual learning and streaming. NeurIPS , 2020. 5
[7]Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garﬁnkel, Allan
Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, et al. The malicious use of artiﬁcial
intelligence: Forecasting, prevention, and mitigation. arXiv preprint arXiv:1802.07228 , 2018.
15
[8]Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark
experience for general continual learning: a strong, simple baseline. In NeurIPS , 2020. 1, 2, 3,
6, 7, 8
[9]Alfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network
models for practical applications. arXiv preprint arXiv:1605.07678 , 2016. 16
[10] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrastive continual learning. In ICCV ,
2021. 3
[11] Arslan Chaudhry, Albert Gordo, Puneet Kumar Dokania, Philip Torr, and David Lopez-Paz. Us-
ing hindsight to anchor past knowledge in continual learning. arXiv preprint arXiv:2002.08165 ,
2(7), 2020. 1
[12] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient
lifelong learning with a-gem. arXiv preprint arXiv:1812.00420 , 2018. 3, 7
[13] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K
Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual
learning. arXiv preprint arXiv:1902.10486 , 2019. 3, 6, 7
[14] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Long live the
lottery: The existence of winning tickets in lifelong learning. In ICLR , 2020. 3
[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In CVPR . Ieee, 2009. 2, 6
[16] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without
losing performance. arXiv preprint arXiv:1907.04840 , 2019. 3
[17] Peiyan Dong, Siyue Wang, Wei Niu, Chengming Zhang, Sheng Lin, Zhengang Li, Yifan Gong,
Bin Ren, Xue Lin, and Dingwen Tao. Rtmobile: Beyond real-time mobile acceleration of rnns
for speech recognition. In DAC , pages 1–6. IEEE, 2020. 17
[18] Xuanyi Dong and Yi Yang. Network pruning via transformable architecture search. In NeurIPS ,
pages 759–770, 2019. 1
[19] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the
lottery: Making all tickets winners. In ICML , pages 2943–2952. PMLR, 2020. 2, 3, 4, 7, 8, 16
10

--- PAGE 11 ---
[20] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. ICLR , 2019. 3
[21] Yifan Gong, Geng Yuan, Zheng Zhan, Wei Niu, Zhengang Li, Pu Zhao, Yuxuan Cai, Sijia
Liu, Bin Ren, Xue Lin, et al. Automatic mapping of the best-suited dnn pruning schemes for
real-time mobile acceleration. ACM Transactions on Design Automation of Electronic Systems
(TODAES) , 27(5):1–26, 2022. 18
[22] Yifan Gong, Zheng Zhan, Zhengang Li, Wei Niu, Xiaolong Ma, Wenhao Wang, Bin Ren,
Caiwen Ding, Xue Lin, Xiaolin Xu, et al. A privacy-preserving-oriented dnn pruning and
mobile acceleration framework. In GLSVLSI , pages 119–124, 2020. 17
[23] Song Han, Jeff Pool, et al. Learning both weights and connections for efﬁcient neural network.
InNeurIPS , pages 1135–1143, 2015. 3
[24] Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory efﬁcient experience replay
for streaming learning. In ICRA , 2019. 3
[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR , 2016. 7
[26] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft ﬁlter pruning for
accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866 , 2018. 17
[27] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median
for deep convolutional neural networks acceleration. In CVPR , pages 4340–4349, 2019. 17
[28] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating con-
tinual learning scenarios: A categorization and case for strong baselines. arXiv preprint
arXiv:1810.12488 , 2018. 7
[29] Tong Jian, Yifan Gong, Zheng Zhan, Runbin Shi, Nasim Soltani, Zifeng Wang, Jennifer G Dy,
Kaushik Roy Chowdhury, Yanzhi Wang, and Stratis Ioannidis. Radio frequency ﬁngerprinting
on the edge. IEEE Transactions on Mobile Computing , 2021. 17
[30] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence of similar
and dissimilar tasks. NeurIPS , 2020. 1
[31] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,
Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,
et al. Overcoming catastrophic forgetting in neural networks. PNAS , 114(13):3521–3526, 2017.
1, 3, 6, 7
[32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
https://www.cs.toronto.edu/ kriz/learning-features-2009-TR.pdf , 2009. 6, 15
[33] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/ ,
1998. 15
[34] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network
pruning based on connection sensitivity. ICLR , 2019. 2, 3, 7, 8
[35] Tuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, and Wei Liu. Compressing
convolutional neural networks via factorized convolutional ﬁlters. In CVPR , pages 3977–3986,
2019. 17
[36] Zhizhong Li and Derek Hoiem. Learning without forgetting. TPAMI , 40(12):2935–2947, 2017.
1, 3, 6, 7
[37] Xiaolong Ma, Fu-Ming Guo, Wei Niu, Xue Lin, Jian Tang, Kaisheng Ma, Bin Ren, and Yanzhi
Wang. Pconv: The missing but desirable sparsity in dnn weight pruning for real-time execution
on mobile devices. In AAAI , pages 5117–5124, 2020. 17
11

--- PAGE 12 ---
[38] Xiaolong Ma, Wei Niu, Tianyun Zhang, Sijia Liu, Sheng Lin, Hongjia Li, Wujie Wen, Xiang
Chen, Jian Tang, Kaisheng Ma, et al. An image enhancing pattern-based sparsity for real-time
inference on mobile devices. In ECCV , pages 629–645. Springer, 2020. 1
[39] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner.
Online continual learning in image classiﬁcation: An empirical survey. arXiv preprint
arXiv:2101.10423 , 2021. 3
[40] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to
multiple tasks by learning to mask weights. In ECCV , pages 67–82, 2018. 3
[41] Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by
iterative pruning. In CVPR , 2018. 1, 3, 6, 7
[42] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks:
The sequential learning problem. In Psychology of learning and motivation , volume 24, pages
109–165. Elsevier, 1989. 1
[43] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. An empirical
investigation of the role of pre-training in lifelong learning. ICML Workshop , 2021. 7
[44] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine
Gibescu, and Antonio Liotta. Scalable training of artiﬁcial neural networks with adaptive sparse
connectivity inspired by network science. Nature communications , 9(1):1–12, 2018. 3, 4
[45] Hesham Mostafa and Xin Wang. Parameter efﬁcient training of deep convolutional neural
networks by dynamic sparse reparameterization. In ICML , pages 4646–4655. PMLR, 2019. 3
[46] Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin, Yanzhi Wang, and Bin
Ren. Patdnn: Achieving real-time dnn execution on mobile devices with pattern-based weight
pruning. arXiv preprint arXiv:2001.00138 , 2020. 18
[47] Lorenzo Pellegrini, Vincenzo Lomonaco, Gabriele Grafﬁeti, and Davide Maltoni. Con-
tinual learning at the edge: Real-time training on smartphone devices. arXiv preprint
arXiv:2105.13127 , 2021. 1
[48] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Continual learning, fast and slow.
NeurIPS , 2021. 3
[49] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:
Incremental classiﬁer and representation learning. In CVPR , pages 2001–2010, 2017. 1, 3, 7
[50] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv
preprint arXiv:1606.04671 , 2016. 1, 3
[51] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. In ICML , 2018. 1
[52] Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Spacenet: Make free
space for continual learning. Neurocomputing , 439:1–11, 2021. 3
[53] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic ﬂow. NeurIPS , 33:6377–6389, 2020. 3
[54] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio,
and Geoffrey J Gordon. An empirical study of example forgetting during deep neural network
learning. arXiv preprint arXiv:1812.05159 , 2018. 5
[55] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient ﬂow. In ICLR , 2019. 3
[56] Zifeng Wang, Tong Jian, Kaushik Chowdhury, Yanzhi Wang, Jennifer Dy, and Stratis Ioannidis.
Learn-prune-share for lifelong learning. In ICDM , 2020. 1, 3, 6, 7
12

--- PAGE 13 ---
[57] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi
Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting
for rehearsal-free continual learning. ECCV , 2022. 1
[58] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su,
Vincent Perot, Jennifer Dy, and Tomas Pﬁster. Learning to prompt for continual learning. CVPR ,
2022. 1, 3
[59] Paul Wimmer, Jens Mehnert, and Alexandru Condurache. Freezenet: Full performance by
reduced storage costs. In ACCV , 2020. 3
[60] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu.
Large scale incremental learning. In CVPR , pages 374–382, 2019. 1, 3
[61] Yushu Wu, Yifan Gong, Pu Zhao, Yanyu Li, Zheng Zhan, Wei Niu, Hao Tang, Minghai Qin,
Bin Ren, and Yanzhi Wang. Compiler-aware neural architecture search for on-mobile real-time
super-resolution. ECCV , 2022. 18
[62] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for
class incremental learning. In CVPR , pages 3014–3023, 2021. 3
[63] Li Yang, Sen Lin, Junshan Zhang, and Deliang Fan. Grown: Grow only when necessary for
continual learning. arXiv preprint arXiv:2110.00908 , 2021. 1
[64] Jaehong Yoon, Divyam Madaan, Eunho Yang, and Sung Ju Hwang. Online coreset selection for
rehearsal-based continual learning. arXiv preprint arXiv:2106.01085 , 2021. 5
[65] Kun-Hsing Yu, Andrew L Beam, and Isaac S Kohane. Artiﬁcial intelligence in healthcare.
Nature biomedical engineering , 2(10):719–731, 2018. 15
[66] Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan Gong,
Zheng Zhan, Chaoyang He, Qing Jin, et al. Mest: Accurate and fast memory-economic sparse
training framework on the edge. NeurIPS , 34, 2021. 3, 4, 5, 7, 9, 16, 17
[67] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic
intelligence. In ICML , 2017. 1, 3, 6
[68] Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu, Yushu Wu, Tianyun Zhang, Malith
Jayaweera, David Kaeli, Bin Ren, et al. Achieving on-mobile real-time super-resolution with
neural architecture and pruning search. In ICCV , pages 4821–4831, 2021. 17
[69] Tingting Zhao, Zifeng Wang, Aria Masoomi, and Jennifer Dy. Deep bayesian unsupervised
lifelong learning. Neural Networks , 149:95–106, 2022. 1
Checklist
1. For all authors...
(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
contributions and scope? [Yes] The claims match the experimental results and it is
expected to generalize according to the diverse experiments stated in our paper. We
include all of our code, data, and models in the supplementary materials, which can
reproduce our experimental results.
(b) Did you describe the limitations of your work? [Yes] See Section 6 and Appendix A.
(c)Did you discuss any potential negative societal impacts of your work? [Yes] See
Section 6 and Appendix B.
(d)Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes] We have read the ethics review guidelines and ensured that our paper
conforms to them.
2. If you are including theoretical results...
(a)Did you state the full set of assumptions of all theoretical results? [N/A] Our paper is
based on the experimental results and we do not have any theoretical results.
13

--- PAGE 14 ---
(b)Did you include complete proofs of all theoretical results? [N/A] Our paper is based
on the experimental results and we do not have any theoretical results.
3. If you ran experiments...
(a)Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] See Section 5.1,
Section 5.4 and we provide code to reproduce the main experimental results.
(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] See Section 5.1 and Section 5.4.
(c)Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes] See Table 1, Table 2, ﬁg 1, ﬁg 3.
(d)Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] See Section 5.1, Section 5.4.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a)If your work uses existing assets, did you cite the creators? [Yes] We mentioned and
cited the datasets (Split CIFAR-10 and Tiny-ImageNet), and all comparing methods
with their paper and github in it.
(b)Did you mention the license of the assets? [Yes] The licences of used datasets/models
are provided in the cited references and we state them explicitly in Appendix C.
(c)Did you include any new assets either in the supplemental material or as a URL? [Yes]
We provide code for our proposed method in the supplement.
(d)Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e)Did you discuss whether the data you are using/curating contains personally identiﬁable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects...
(a)Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b)Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c)Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]
14

--- PAGE 15 ---
A Limitations
One limitation of our method is that we assume a rehearsal buffer is available throughout the CL
process. Although the assumption is widely-accepted, there are still situations that a rehearsal buffer
is not allowed. However, as a framework targeting for efﬁciency, our work has the potential to
accelerate all types of CL methods. For example, simply removing the terms related to rehearsal
buffer in equation 1 and equation 3 could serve as a naive variation of our method that is compatible
with other non-rehearsal methods. It is interesting to further improve SparCL to be more generic for
all kinds of CL methods. Moreover, the benchmarks we use are limited to vision domain. Although
using vision-based benchmarks has been a common practice in the CL community, we believe
evaluating our method, as well as other CL methods, on datasets from other domains such as NLP will
lead to a more comprehensive and reliable conclusion. We will keep track of newer CL benchmarks
from different domains and further improve our work correspondingly.
B Potential Negative Societal Impact
Although SparCL is a general framework to enhance efﬁciency for various CL methods, we still
need to be aware of its potential negative societal impact. For example, we need to be very careful
about the trade-off between accuracy and efﬁciency when using SparCL. If one would like to pursue
efﬁciency by setting the sparsity ratio too high, then even SparCL will result in signiﬁcant accuracy
drop, since the over-sparsiﬁed model does not have enough representation power. Thus, we should
pay much attention when applying SparCL on accuracy-sensitive applications such as healthcare ( 65).
Another example is that, SparCL as a powerful tool to make CL methods efﬁcient, can also strengthen
models for malicious applications ( 7). Therefore, we encourage the community to come up with
more strategies and regulations to prevent malicious use of artiﬁcial intelligence.
C Dataset Licensing Information
• CIFAR-10 (32) is licensed under the MIT license.
•The licensing information of Tiny-ImageNet ( 33) is not available. However, the data is
available for free to researchers for non-commercial use.
D Additional Experiment Details and Results
We set= 0:5;= 1 in equation 1 and equation 3. We also set k= 5,pinter = 0:01,
pintra = 0:005. We also match different weight sparsity with gradient sparsity for best performance.
We sample 20% data from Split CIFAR-10 training set for validation, and we use grid-search on
this validation set to help us select the mentioned best hyperparameters. We use the same set of
hyperparameters for both datasets. For accurate evaluation, we repeat each experiments 3 times
using different random seeds and report the average performance. During our experiments, we adopt
unstructured sparsity type and uniform sparsity ratio ( 0:75;0:90;0:95) for all convolutional layers in
the models.
D.1 Evaluation Metrics Explanation
Training FLOPs The FLOPs of a single forward pass is calculated by taking the sum of the number
of multiplications and additions in each layer lfor a given layer sparsity sl. Each iteration in the
training process is composed of two phases, i.e., the forward propagation and backward propagation.
The goal of the forward pass is to calculate the loss of the current set of parameters on a given batch
of data. It can be formulated as al=(zl) =(wlal 1+bl)for each layer lin the model. Here,
w,b, andzrepresent the weights, biases, and output before activation, respectively; (:)denotes the
activation function; ais the activations;means convolution operation. The formulation indicates
that the layer activations are calculated in sequence using the previous activations and the parameters
of the layer. Activation of layers are stored in memory for the backward pass.
15

--- PAGE 16 ---
As for the backward propogation, the objective is to back-propagate the error signal while calculating
the gradients of the parameters. The two main calculation steps can be represented as:
l=l+1rotate 180°(wl)0(zl); (4)
Gl=al 1l; (5)
wherelis the error associated with the layer l,Gldenotes the gradients, represents Hadamard
product,0(:)denotes the derivative of activation, and rotate 180°(:)means rotating the matrix by 180°
is the matrix transpose operation. During the backward pass, each layer lcalculates two quantities,
i.e., the gradient of the activations of the previous layer and the gradient of its parameters. Thus, the
backward passes are counted as twice the computation expenses of the forward pass ( 19). We omit the
FLOPs needed for batch normalization and cross entropy. In our work, the total FLOPs introduced by
TDM, DDR, and DGM on split CIFAR-10 is approximately 4:5109which is less than 0:0001% of
total training FLOPs. For split Tiny-ImageNet, the total FLOPs of them is approximately 1:81010,
which is also less than 0:0001% of total training FLOPs. Therefore, the computation introduced by
TDM, DDR, and DGM is negligible.
Memory Footprints Following works ( 9;66), the deﬁnition of memory footprints contain two parts:
1) activations (feature map pixels) during training phase, and 2) model parameters during training
phase. For experiments, activations, model weights, and gradients are stored in 32-bit ﬂoating-point
format for training. The memory footprint results are calculated with an approximate summation of
them.
D.2 Details of Memory Footprint
The memory footprint is composed of three parts: activations, model weights, and gradients. They
are all represented as bw-bit numbers for training.
The number of activations in the model is the sum of the activations in each layer. Suppose that the
output feature of the l-th layer with a batch size of Bis represented as al2RBOlHlWl, where
Olis the number of channels and HlWlis the feature size. The total number of activations of the
model is thus BP
lOlHlWl.
As for the model weights, our SparCL training a sparse model with a sparsity ratio s2[0;1]from
scratch. The sparse model is obtained from a dense model with a total of Nweights. A higher value
ofsindicates fewer non-zero weights in the sparse model. Compressed sparse row (CSR) format is
commonly used for sparse storage, which greatly reduces the number of indices need to be stored for
sparse matrices. As our SparCL adopt only one sparsity type and we use a low-bit format to store the
indices, we omit the indices storage here. Therefore, the memory footprint for model representation
is(1 s)Nbw.
Similar calculations can be applied for the gradient matrix. Besides the sparsity ratio s, additionalq
gradients are masked out from the gradient matrix, resulting a sparsity ratio s+q. Therefore, the
storage of gradients can be approximated as (1 (s+q))Nbw.
Combining the activations, model representation, and gradients, the total memory footprint in SparCL
can be represented as (2BP
lOlHlWl+ (1 s)N+ (1 (s+q))N)bw.
DDR requires store indices for the easier examples during the training process. The number of
training examples for Split CIFAR-10 and Split Tiny-ImageNet on each task is 10000. In our work,
we only need about 3KB (remove 30% training data) for indices storage (in the int8 format) and the
memory cost is negligible compared with the total memory footprint.
D.3 Effect of Cutoff Stage
Table A1: Effect of cutoff .
cutoff 1 2 3 4 5 6 7 8 9
Class-IL (")71.54 72.38 72.74 73.20 73.10 73.32 73.27 73.08 73.23
To evaluate the effect of the cutoff stage, we use the same setting as in Figure 3 by setting the
sparsity ratio to 0:90. We keep the data removal proportion = 30% , and only change cutoff .
16

--- PAGE 17 ---
Table A1 shows the relationship between cutoff and the Class-IL average accuracy. Note that
from the perspective of efﬁciency, we would like the cutoff stage as early as possible, so that the
remaining epochs will have less examples. However, from Table A1, we can see that if we set it too
early, i.e.,cutoff3, the accuracy drop is signiﬁcant. This indicate that even for the “easy-to-learn”
examples, removing them too early results in underﬁtting. As a balance point between accuracy and
efﬁciency, we choose cutoff = 4in our ﬁnal version.
D.4 Supplementary Ablation Study
Table A2: Ablation study on Split-CIFAR10 with 0.90 sparsity.
TDM DDR DGM Class-IL (")FLOPs Train
1015(#)Memory Footprint ( #)
7 7 7 72.70 13.9 247MB
3 7 7 72.98 1.6 166MB
3 3 7 73.20 1.2 166MB
3 7 3 73.30 1.5 165MB
3 3 3 73.42 1.1 165MB
Similar to Table 3, we show ablation study with 0:90sparsity ratio in Table A2. Under a larger
sparsity ratio, the conclusion that all components contribute to the ﬁnal performance still holds.
However, we can observe that the accuracy increase that comes from DDR and DGM is less than
what we show in Table 3. We assume that larger sparsity ratio makes it more difﬁcult for the model
to retain good accuracy in CL. Similar results has also been observed in ( 66) under the usual i.i.d.
learning setting.
D.5 Exploration on DGM
Table A3: Ablation study of the gradient sparsity ratio on Split-CIFAR10.
weight sparsity gradient sparsity Class-IL (")FLOPs Train
1015(#)Memory Footprint ( #)
0.75 0.78 74.08 3.4 178MB
0.75 0.80 73.97 3.3 177MB
0.75 0.82 73.79 3.3 177MB
0.75 0.84 73.26 3.2 176MB
0.90 0.91 73.33 1.6 166MB
0.90 0.92 73.30 1.5 165MB
0.90 0.93 72.64 1.5 165MB
We conduct further experiments to demonstrate the inﬂuence of gradient sparsity, and the results are
shown in Table A3. There are two sets of the experiments with different weight sparsity settings:
0.75 and 0.90. Within each set of the experiments (the weight sparsity is ﬁxed), we vary the gradient
sparsity values. From the results we can see that increasing the gradient sparsity can decrease the
FLOPs and memory footprint. However, the accuracy performance degrades more obvious when the
gradient sparsity is too much for the weight sparsity. The results indicate that suitable gradient sparsity
setting can bring further efﬁciency to the training process while boosting the accuracy performance.
In the main results, the gradient sparsity is set as 0.80 for 0.75 weight sparsity, and set as 0.92 for
0.90 weight sparsity.
E On-Mobile Compiler Optimizations and Inference Results
E.1 Compiler Optimizations
Each iteration in the training process is composed of two phases, i.e., the forward propagation and
backward propagation. Prior works ( 17;22;26;27;29;35;37;68) have proved that sparse weight
matrices (tensors) can provide inference acceleration via reducing the number of multiplications in
convolution operation. Therefore, the forward propagation phase, which is the same as inference,
17

--- PAGE 18 ---
can be accelerated by the sparsity inherently. As for backward pass, both of the calculation steps are
based on convolution, i.e., matrix multiplication. Equation 4 uses sparse weight matrix (tensor) as
the operand, thus can be accelerated in the same way as the forward propagation. Equation 5 allows
a sparse output result since the gradient matrix is also sparse. Thus, both two steps have reduced
computations, which are roughly proportional to the sparsity ratio, providing the acceleration for the
backward propagation phase.
Compiler optimizations are used to accelerate the inference in prior works ( 21;46;61). In this work,
we extend the compiler optimization techniques for accelerating the forward and backward pass
during training on the edge devices. Our compiler optimizations are general, support both sparse
model training and inference accelerations on mobile platforms. The optimizations include 1) the
supports for sparse models; 2) an auto-tuning process to determine the best-suited conﬁgurations of
parameters for different mobile CPUs. The details of our compiler optimizations are presented as
follows.
E.1.1 Supports for Sparse Models
Our framework supports sparse model training and inference accelerations with unstructured pruning.
For the sparse (pruned) model, the framework ﬁrst compacts the model storage with a compression
format called Compressed Sparse Row (CSR) format, and then performs computation reordering to
reduce the branches within each thread and eliminates the load imbalance among threads.
A row reordering optimization is also included to further improve the regularity of the weight matrix.
After this reordering, the continuous rows with identical or similar numbers of non-zero weights
are processed by multi-threads simultaneously, thus eliminating thread divergence and achieving
load balance. Each thread processes more than one rows, thus eliminating branches and improving
instruction-level parallelism. Moreover, a similar optimization ﬂow (i.e., model compaction and
computation reorder and other optimizations) is employed to support all compiler optimizations for
sparsity as PatDNN (46).
E.1.2 Auto-Tuning for Different Mobile CPUs
During DNN sparse training and inference execution, there are many tuning parameters, e.g., matrix
tiling sizes, loop unrolling factors, and data placement on memory, that inﬂuence the performance.
It is hard to determine the best-suited conﬁguration of these parameters manually. To alleviate
this problem, our compiler incorporates an auto-tuning approach for sparse (pruned) models. The
Genetic Algorithm is leveraged to explore the best-suited conﬁgurations automatically. It starts the
parameter search process with an arbitrary number of chromosomes and explores the parallelism
better. Acceleration codes for different DNN models and different mobile CPUs can be generated
efﬁciently and quickly through this auto-tuning process.
E.2 Inference Acceleration Results On Mobile
6012182 4A ccelation R esult s of R esNet -18 on  Split CIF AR-100 . 000 . 7 50 . 900 . 95100203040A ccelation R esult s of R esNet -18 on Split Tin y-ImageNet0 . 000 . 7 50 . 900 . 95
Figure 5: Inference results of sparse models obtained from SparCL under different sparsity ratio compared with
dense models obtained from traditional CL methods (sparsity ratio 0.00).
Besides accelerating the training process, SparCL also possesses the advantages of providing a sparse
model as the output for faster inference. To demonstrate this, we show the inference acceleration
results of SparCL with different sparsity ratio settings on mobile in Figure 5. The inference time is
measured on the CPU of an off-the-shelf Samsung Galaxy S20 smartphone. Each test takes 50 runs
on different inputs with 8 threads on CPU. As different runs do not vary greatly, only the average
time is reported. From the results we can see that the obtained sparse model from SparCL can
signiﬁcantly accelerate the inference on both Split-CIFAR-10 and Tiny-ImageNet dataset compared
to the model obtained by traditional CL training. For ResNet-18 on Split-CIFAR-10, the model
obtained by traditional CL training, which is a dense model, takes 18.53ms for inference. The model
provided by SparCL can achieve an inference time of 14.01ms, 8.30ms, and 5.85ms with sparsity
18

--- PAGE 19 ---
ratio of 0.75, 0.90, and 0.95, respectively. The inference latency of the dense ResNet-18 obtained by
traditional CL training on Tiny-ImageNet is 39.64 ms. While the sparse models provided by SparCL
with sparsity ratio settings as 0.75, 0.90, and 0.95 reach inference speed of 33.06ms, 20.37ms, and
15.49ms, respectively, on Tiny-ImageNet.
19
