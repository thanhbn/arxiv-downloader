# 2306.01385.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2306.01385.pdf
# File size: 185142 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2306.01385v2  [eess.AS]  9 Jul 2023Task-Agnostic Structured Pruning of Speech Representatio n Models
Haoyu Wang1, Siyuan Wang1, Wei-Qiang Zhang1∗, Hongbin Suo2, Yulong Wan2
1Department of Electronic Engineering, Tsinghua Universit y, Beijing 100084, China
2Data & AI Engineering System, OPPO, Beijing 100026, China
w-hy21@mails.tsinghua.edu.cn, wq-zhang@tsinghua.edu. cn
Abstract
Self-supervised pre-trained models such as Wav2vec2, Hube rt,
and WavLM have been shown to signiﬁcantly improve many
speech tasks. However, their large memory and strong compu-
tational requirements hinder their industrial applicabil ity. Struc-
tured pruning is a hardware-friendly model compression tec h-
nique but usually results in a larger loss of accuracy. In thi s pa-
per, we propose a ﬁne-grained attention head pruning method
to compensate for the performance degradation. In addition , we
also introduce the straight through estimator into the L0regu-
larization to further accelerate the pruned model. Experim ents
on the SUPERB benchmark show that our model can achieve
comparable performance to the dense model in multiple tasks
and outperforms the Wav2vec 2.0 base model on average, with
72% fewer parameters and 2 times faster inference speed.
Index Terms : Model pruning, knowledge distillation, model
compression, representation learning
1. Introduction
Recently, self-supervised pre-training has become one of t he
most attractive topics in the speech domain [1, 2]. With this
method, a large amount of unlabeled data can be used to train
a deep model to extract high-level representations from raw au-
dio, which can bring signiﬁcant improvement to many down-
stream tasks.
While pre-trained models provide a tremendous perfor-
mance improvement, they also require large amount of mem-
ory and computing power. Large self-supervised pre-traine d
speech models such as Wav2vec2 [3], Hubert [4], and WavLM
[5] typically have hundreds of millions of parameters, mak-
ing them unsuitable for use on consumer products such as lap-
tops and smartphones. This is an obstacle to the application of
these models in many real-world scenarios. As a result, mode l
compression has become a major concern for these large self-
supervised models.
Knowledge distillation usually uses a teacher model to
guide a smaller student model, and the structure of the stu-
dent model must be carefully designed to achieve better perf or-
mance. DistilHubert [6] distills a 12-layer Hubert-based m odel
to obtain a 2-layer student model and signiﬁcantly reduces t he
model size. FitHubert [7], which is inspired by FitNets [8],
designs a thin but deep student network to provide better rep re-
sentation ability.
Model pruning attempts to discard the unimportant weights
and obtain a subnetwork from the pre-trained model. In un-
* Corresponding author
This work was supported by the National Natural Science Foun da-
tion of China under Grant No. 62276153.structured pruning, these discarded weights are randomly d is-
tributed in the matrices; in structured pruning, network un its
such as attention heads or feed-forward layers are removed e n-
tirely. Structurally pruned models do not require speciall y de-
signed hardware for acceleration, which may be more appropr i-
ate for consumer devices. LightHubert treats model pruning as a
neural architecture search problem and signiﬁcantly reduc es the
performance degradation, but the search process still requ ires
some time-consuming manual selections [9]. Peng et al. pro-
pose a more ﬂexible method by applying the L0-regularization-
based pruning method [10] to the Wav2vec 2.0 model, but their
method is task-speciﬁc and comes at some additional cost whe n
applied to downstream tasks [11].
We attempt to use a similar L0-regularization-based method
to obtain a task-agnostic compressed model. However, learn -
ing the pruning masks using L0regularization on unsupervised
pre-training tasks such as contrastive predictive coding [ 12] re-
quires large computational resources. The combination of d is-
tillation and pruning is a promising solution [13, 14]. The r ep-
resentation provided by the pre-trained model not only redu ces
the training effort of the downstream models, but also provi des
task-independent information for model pruning.
Compared to existing unstructured pruning methods of the
pre-trained speech models [15, 16], structure pruning usua lly
suffers from a larger performance degradation [17]. The cru x
of this problem is that using structure rather than individu al
weights as the basic unit of pruning reduces the degree of fre e-
dom, resulting in the removal of some important weights. To
compensate for the performance degradation, we introduce a
ﬁne-grained attention head pruning method that prunes each
attention head separately. To promote the pruning of coarse -
grained structures and further speed up the pruned model, we
also introduce the straight through estimator (STE) [18] in to the
mutil-scale structured pruning method [13] based on L0regu-
larization.
Experiments on the SUPERB benchmark show the gen-
eralization ability of the proposed model on different down -
stream tasks. With the help of the pre-trained teacher, the p ro-
posed model is task-agnostic and can be directly ﬁne-tuned t o
many downstream tasks. Further contrast experiments demon -
strate the effectiveness of ﬁne-grained attention head pru ning
and STE. Our model outperforms the distilled baselines, and
achieves comparable results to the teacher model on multipl e
tasks, with 72% fewer parameters and 2 times faster in speed.
2. Backgrounds
2.1. Pre-trained Speech Representation Models
Our experiment is mainly performed on WavLM [5], but the
method can be easily extended to Wav2vec 2.0 [3], data2vec

--- PAGE 2 ---
/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000017
p ( /uni0304 s ̄
/uni00000053/uni0000000b/uni0000005d/uni0000000c
(a)/uni000000ed/uni00000015/uni00000013 /uni000000ed/uni00000014/uni00000013 /uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013/uni000000ed/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015
/uni0000005d
̄ s
(b)
Figure 1: (a)the possibility distribution of zand¯s.(b)zand
¯sas a function of log α, averaged on 500 samples. zcan be
exactly 0 or 1 or any value in between. In the shadow region,
∂z/∂¯s= 0.
[19], Hubert [4], and other models with similar transformer -
based structures.
WavLM is a set of state-of-the-art self-supervised pre-
trained models. During pre-training, ofﬂine clustered uni ts are
used as the training target and the models learn to represent the
continuous inputs by some discrete hidden units. WavLM also
introduces masked speech denoising and gated relative posi tion
bias to improve the performance.
2.2. Pruning Based on the L0Regularization
Pruning based on L0regularization is one of the mask learning
methods. In some pruning methods, parameters are discarded
according to some artiﬁcially set criteria, such as the magn itude
of weights or gradients. On the other hand, mask learning met h-
ods tend to consider pruning as an optimization problem [10] .
As the name implies, L0-regularization-based pruning adds a
mask to the parameters (or parameter groups) and uses the L0
norm of these pruning masks as a regularization term of the lo ss
function. For example, in our experiments, the training obj ec-
tive is:
R(θ,π) =Ez∼q(π)[1
NN/summationdisplay
i=1L(fs(xi,/tildewideθ),ft(xi))+λ||/tildewideθ||0],
(1)
wherefsandftare the student and teacher models for knowl-
edge distillation, xiis theith input data, θis the parameter set of
the student model, z∈ {0,1}is the pruning mask set, /tildewideθ=θ⊙z
is the parameter set after masking. The discrete random vari able
zfollows a Bernoulli distribution q(π).
However, this objective function cannot be optimized by
gradient descent methods because the process of sampling zfor
q(π)is not differentiable. Louizos et al. introduce a reparam-
eterization trick to deal with this problem [10]. After the r epa-
rameterization, z becomes a continuous variable, determin ed by
a learnable parameter αand an additional random variable u
that “collects” the randomness from z. Formally speaking, zis
computed by:
u∼U(0,1),s=sigmoid(1
βlog(u
1−u)+logα)
¯s=s(ζ−γ)+γ,z=hardtanh(¯s),(2)
whereuis sampled from a uniform distribution U(0,1),ζ=
1.1,γ=−0.1are 2 constants to scale sto a larger interval and
make sure zcan be exactly 0 or 1. βcontrols the temperature,
andαis the learnable parameter.
Figure 1a shows the probability distribution of zand¯s,
while ﬁgure 1b shows their values as functions of log α. We cansee that the reparameterization trick turns the discrete ma sksz
into continuous variables while still allowing them to be ex actly
0 or 1.
2.3. Multi-scale Structured Pruning
TheL0regularization does not limit the grain of the pruning.
Ifzmasks some structure, L0regularization can be used for
structured pruning. The grain can be as large as an entire lay er
or as small as a certain dimension of a weight matrix. Re-
cently, Xia et al. introduce a multi-scale pruning method th at
removes ﬁne-grained and coarse-grained structures in para llel
to promote the removal of large structures and achieve furth er
speedup [13]. We introduced this method to increase the poss i-
bility of removing coarse-grained structures to compensat e for
the potential negative effects of our ﬁne-grained attentio n head
pruning method on the inference speed of the model.
3. Methods
3.1. Fine-grained Attention Head Pruning
In previous works [11, 13], the attention heads are used as th e
smallest units for pruning. This may reduce the degree of fre e-
dom of pruning and lead to more performance degradation. To
make structure pruning more ﬂexible, we propose a ﬁne-grain ed
attention method that separately prunes each dimension of m a-
trices in the attention layer based on the multi-scale struc tured
pruning method of Xia et.al [13]. Formally speaking, a trans -
former block is masked as follows:
fMHA(X) =zMHA·concat(fATT(X))
fATT(X) =Sc·(XWi
V)·diag(zi
vo)
Sc=softmax((XWi
Q)·diag(zi
qk)·(XWi
K)T)
fFFN(X) =zFFN·gelu(XWU)·diag(zint)·WD,(3)
whereXis the input data, Wi
Q,Wi
K,Wi
V,WOis the query,
key, value and output matrices, respectively. zMHA ,zi
qk,zi
vo,
zFFN,zintdenote the pruning mask for multi-head attention
layers, attention matrices, feed-forward layers, and inte rmediate
dimensions. We omit the scale factors in fATT(X)for clarity,
and please note that WOshould also be pruned according to
zi
vo. ForWQ,WV∈Rdhidden×dhead,zi
qkandzi
vowill have
dhead variables.
3.2. Optimizing Pruning Masks with STE
Although the reparameterization trick makes zdifferentiable,
the introduction of hardtanh in Eq. 2 creates a new obstacle t o
optimization. As shown in Figure 1b, when log αtakes a value
in the shaded region, the presence of hardtanh makes ∂z/∂s=
0, and the learnable parameter αcannot be updated. That is to
say, the model decides to keep a structure when zis 1, but it
cannot evaluate that decision.
This problem becomes more obvious for multi-scale struc-
tured pruning. Figure 3a shows that the mean value of zFFN
does not change during training, which makes multi-scale pr un-
ing ineffective. The reason may be that in the early stages of
training, pruning the entire FFN layer can lead to a huge perf or-
mance degradation, so αmay be optimized to a large positive
value, and difﬁcult to update in the remaining training step s.
The failure to cut the coarse-scale structures will cause th e
sparse weight of the pruning model to be too dispersed, resul t-
ing in lower acceleration ratio. To address this problem, We
apply the straight through estimator [18] to make sure that t he

--- PAGE 3 ---
gradient can pass through the hardtanh function in Eq. 2. Sin ce
the gradients from STE are not the gradients for the loss func -
tion, optimizing in this direction may not lead to the most ac cu-
rate student and may cause instability near some local minim a
[20]. For the stability of training, we deﬁne the gradient of STE
such that:
∂L
∂¯s=

1, if∂L
∂z>= 1;
−1, if∂L
∂z<−1;
∂L
∂z, otherwise.(4)
3.3. Training Objective
Hidden states of different layers contain different types o f in-
formation [6, 21]. Therefore, we follow Xia et al. [13] to use
learnable multi-task knowledge distillation to learn the r epre-
sentation of different layers. We also follow Wang et al. to
change the 2nd term on the r.h.s of eq. 1 into a Lagrangian term
to better control the sparsity [22]. Our training objective is as
follows:
L=1
NN/summationdisplay
i=0/summationdisplay
(j,k)∈DLMSE(hj
i,ˆhk
i)+λ1(ˆp−p)+λ2(ˆp−p)2,
(5)
whereˆpis the approximate model sparsity, pis the target spar-
sity.λ1andλ2are learnable parameters for the Lagrangian
regularization. Dis the teacher-student layer pairing relation
learned during training [13], for sample i,hj
iandˆhk
iare the
output of layer j/kof the student and teacher models, respec-
tively.
4. Experiments
4.1. SUPERB
SUPERB (Speech processing Universal PERformance Bench-
mark) is a benchmark for evaluating the performance of speec h
pre-training models [23]. SUPERB provides 10 predeﬁned
speech tasks from different perspectives where the pre-tra ined
models are used as upstream feature extractors. These tasks
include phoneme recognition (PR), automatic speech recogn i-
tion (ASR), keyword spotting (KS), query-by-example spoke n
term detection (QbE), speaker identiﬁcation (SID), automa tic
speaker veriﬁcation (SV), speaker diarization (SD), inten t clas-
siﬁcation (IC), slot ﬁlling (SF), and emotion recognition ( ER).
4.2. Pruning setup
Model . Our model is initialized from the WavLM base
model, which consists of a 7-layer CNN feature extractor
and a 12-layer transformer encoder. For the matrices in Eq.
3,Wi
Q,Wi
K,Wi
V∈R768×64,WO∈R768×768,WU∈
R768×3072, andWD∈R3072×768. For each transformer block,
we have 12 attention heads, leading to 12∗64 = 768 elements
inzqkandzvo. We also have 3072 elements in zintfor each di-
mension in the FFN layer, and 1 element in zMHA andzFNN to
mask the entire layer. The target pruning sparsity is set to 8 0%.
The teacher model of knowledge distillation is also the WavL M
base model.
Data . We use the 960 hours Librispeech [24] corpus for
pruning. For SUPERB tasks, we use the dataset according to
the ofﬁcial guidelines1.
1https://github.com/s3prl/s3prl//uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013
/uni00000006/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000011/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001a/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni0000001c/uni00000013/uni00000013/uni00000036/uni00000038/uni00000033/uni00000028/uni00000035/uni00000025/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000003/uni0000000b/uni00000139/uni0000000c
/uni0000005a/uni00000044/uni00000059/uni00000015/uni00000059/uni00000048/uni00000046/uni0000005a/uni00000044/uni00000059/uni00000015/uni00000059/uni00000048/uni00000046/uni00000003/uni00000015/uni00000011/uni00000013/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000002b/uni00000058/uni00000025/uni00000028/uni00000035/uni00000037/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000003a/uni00000044/uni00000059/uni0000002f/uni00000030/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048
/uni00000027/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni0000002b/uni00000058/uni00000025/uni00000028/uni00000035/uni00000037/uni00000027/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni0000003a/uni00000044/uni00000059/uni0000002f/uni00000030/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000056/uni00000048/uni00000047
/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000029/uni00000024/uni00000033
/uni00000045/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000056
/uni00000053/uni00000055/uni00000052/uni00000053/uni00000052/uni00000056/uni00000048/uni00000047
/uni00000047/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni0000004f/uni00000048/uni00000047
Figure 2: The relationship between the SUPERB score and the
number of parameters.
Pruning . Pruning is performed on an RTX 3090 GPU for
200k steps and takes about 36 hours. Our training hyperparam -
eters are chosen according to DistilHuBERT [6] and Xia et al.
[13]. The learning rate increases linearly to 2.0e-4 in the ﬁ rst
7% steps and decreases linearly to 0 in the remaining steps, a nd
the target sparsity increases linearly to 80% in the ﬁrst 7% s teps
and remains constant for the rest.
5. Results
Table 1 shows the evaluation results on the SUPERB down-
stream tasks. Our model has comparable performance to the
teacher model in KS, IC, ER, SV , and SD tasks, demonstrating
the effectiveness of our approach. The performance degrada tion
occurred mainly in PR, ASR, and SF tasks. These tasks require
more complex content-related information, which is more li kely
to be lost during pruning. Using the same WavLM base teacher
model, our method outperforms the distilled models in most
tasks, especially in content-related tasks such as ASR, sho wing
that our model better preserves the performance of the teach er
model.
In addition to the task-speciﬁc metrics, we also use the SU-
PERB score (superbs) to provide an overall evaluation. The
SUPERB score is an average of the linear transformations of a ll
the task-speciﬁc metrics, and is determined by the SOTA mode l
on the benchmark and a predeﬁned FBANK baseline. At the
time of writing, the SOTA model is WavLM-Large2. Formally
speaking, the SUPERB score is deﬁned as:
superbs=1
T/summationdisplay
t∈T1000
msota
t−mfbank
t(mu
t−mfbank
t), (6)
wheremu
tis the metric of task tand model u, superbs(sota)≡
1000 , superbs(fbank)≡0.
Figure 2 shows the relationship between the SUPERB score
and the number of parameters. Our model signiﬁcantly outper -
forms the distillation models with similar number of parame -
ters, and even has superior performance to the Wav2vec 2.0 ba se
model. These results show that the proposed method achieves a
better balance between performance and the number of param-
eters compared to the distillation-based method.
We also compare our method with the previous pruning
method which directly removes the attention heads (w/o FAHP
in Table 1). Again, the improvement is mainly reﬂected in tas ks
2The performance of the WavLM-Large model can be found at
https://superbbenchmark.org/leaderboard.

--- PAGE 4 ---
Table 1: Results on SUPERB of the proposed model, and other baselines . The performances are evaluated by Phoneme Error Rate
(PER%), Accuracy (Acc%), Word Error Rate (WER%), Maximum Te rm Weighted Value(MTWV), F1 Score (F1%), Concept Error Rate
(CER%), Equal Error Rate (EER%), and Diarization Error Rate (DER%). DistilWavLM is our reproduction of DistilHubert wi th the
teacher changed to WavLM base; FAHP is the abbreviation for t he proposed Fine-grained Attention Head Pruning method.
MethodKS IC PR ASR ER QbE SF SID SV SDSuperbs↑Acc↑ Acc↑ PER↓WER↓ Acc↑ MTWV ↑ F1↑/CER↓ Acc↑ EER↓DER↓
Baselines
wav2vec [25] 95.59 84.92 31.58 15.86 59.79 4.85 76.37/43.71 56.56 7.99 9.90 491.59
w2v2 Base 96.23 92.35 5.74 6.43 63.43 2.33 88.3/24.77 75.18 6 .02 6.08 735.00
HuBERT Base 96.30 98.34 5.41 6.42 64.92 7.36 88.53/25.2 81.4 2 5.11 5.88 837.63
WavLM Base 96.79 98.63 4.84 6.21 65.94 8.70 89.38/22.86 84.5 1 4.69 4.55 895.99
Distilled Models
DistilHuBERT 95.98 94.99 16.27 13.37 63.02 5.11 82.57/35.3 9 73.54 8.55 6.19 647.88
DistilWavLM 96.40 96.39 14.18 13.24 63.69 7.07 85.27/31.80 71.00 8.87 7.2 668.39
Ours
Proposed 96.57 98.08 9.09 10.61 63.61 7.40 87.14/27.13 74.5 6 6.17 6.11 769.62
w/o FAHP 96.14 98.05 10.51 11.83 63.78 5.19 85.57/30.91 70.0 3 6.12 7.18 721.79
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000029/uni00000029/uni00000031
/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000056/uni00000048/uni00000047
/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000029 /uni00000024/uni0000002b/uni00000033
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000030/uni0000002b/uni00000024
/uni00000036/uni00000057/uni00000048/uni00000053/uni00000056/uni00000012/uni0000004e/uni00000024/uni00000059/uni00000048/uni00000011/uni00000003/uni0000005d
(a)The average value of zFFN andzMHA ./uni00000013 /uni00000017 /uni0000001b /uni00000013 /uni00000017 /uni0000001b
(b)Remaining (blue) parameters in W0
Vfor 12 layers.
Figure 3: The effectiveness of STE
such as ASR, suggesting that ﬁne-grained attention head pru n-
ing can help compensate for the loss of complex information i n
structured pruning.
Figure 3a shows the average of the pruning masks zFFNand
zMHA during pruning. By introducing STE, the pruning masks
of coarse-grained structures change more frequently and ev en-
tually drop to lower values, which proves the effectiveness of
STE. Figure 3b shows the distribution of the remaining weigh ts
of each layer after pruning. Since the coarse-grained struc tures
can be entirely removed, the remaining parameters tend to be
concentrated, leading to further acceleration.
In addition, the remaining weight is concentrated at the
top of the network. Since content-related information is mo re
prominent in the features of the top layers, this distributi on of
remaining weights may be one of the reasons for the network’s
improvement in content-related tasks.
We also measure the inference time of the 2 models above.
Table 2 shows the speed effect of STE. It can be seen that theTable 2: Inference time measured on a RTX3090 GPU, by ex-
tracting features of librispeech dev-clean set and are aver aged
on 5 runs.
Method#Params Infer. time
Millions Seconds
WavLM base 94.70 91.87(1.0x)
Proposed 26.57 46.08(1.99x)
w/o STE 26.37 67.78(1.35x)
Table 3: Inﬂuence of STE on accuracy. ASR, IC, ER, SID are
representative of SUPERB content, paralinguistic, speake r, and
semantic tasks.
MethodsASR IC ER SID
WER↓ Acc↑ Acc↑ Acc↑
proposed 10.29 98.08 63.61 74.56
w/o STE 10.61 97.07 64.17 74.65
concentrated weight distribution brought by STE signiﬁcan tly
improves the inference speed of the model. With STE, the
pruned model is 1.4 times faster with a similar number of pa-
rameters.
Furthermore, we show the effect of STE on accuracy.
Among these 4 tasks, STE brings improvement in ASR and IC,
while causing degradation in ER and SID, but both the positiv e
and negative inﬂuence are not signiﬁcant. The degradation i n
ER and SID may be due to the parameters removed from the
lower layers that are related to speaker or emotion informat ion.
6. Conclusion
In this paper, we present a task-agnostic structured prunin g
method of pre-trained speech representation models. By us-
ing ﬁne-grained attention head pruning, we retain the abili ty to
represent content-level information and reduce the perfor mance
degradation caused by structured pruning. We introduce STE to
multi-scale structured pruning to further accelerate the m odel.
Our experiments prove that the proposed model reduces 72%
of the parameters while having comparable performance to th e
dense model in multiple tasks, and outperforms the Wav2vec2
base model in average performance.

--- PAGE 5 ---
7. References
[1] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin ,
C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maaløe,
T. N. Sainath, and S. Watanabe, “Self-supervised speech
representation learning: A review,” IEEE Journal of Selected
Topics in Signal Processing , vol. 16, no. 6, pp. 1179–1210,
Oct. 2022, conference Name: IEEE Journal of Selected
Topics in Signal Processing. [Online]. Available: https:
//ieeexplore.ieee.org/abstract/document/9893562
[2] J. Zhao and W.-Q. Zhang, “Improving Automatic Speech
Recognition Performance for Low-Resource Languages With
Self-Supervised Models,” IEEE Journal of Selected Topics
in Signal Processing , vol. 16, no. 6, pp. 1227–1241, Oct.
2022. [Online]. Available: https://ieeexplore.ieee.org /document/
9801640/
[3] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, “Wav2vec
2.0: A framework for self-supervised learning of speech
representations,” Advances in Neural Information Processing
Systems , vol. 33, pp. 12 449–12 460, 2020. [Online]. Available:
https://dl.acm.org/doi/abs/10.5555/3495724.3496768
[4] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, K. Lakhotia, R. Salakh ut-
dinov, and A. Mohamed, “HuBERT: Self-supervised speech
representation learning by masked prediction of hidden uni ts,”
IEEE/ACM Transactions on Audio, Speech, and Language
Processing , vol. 29, pp. 3451–3460, 2021. [Online]. Available:
https://dl.acm.org/doi/abs/10.1109/TASLP.2021.31222 91
[5] S. Chen, C. Wang, Z. Chen, Y . Wu, S. Liu, Z. Chen, J. Li,
N. Kanda, T. Yoshioka, X. Xiao et al. , “WavLM: Large-scale
self-supervised pre-training for full stack speech proces sing,”
IEEE Journal of Selected Topics in Signal Processing , vol. 16,
no. 6, pp. 1505–1518, 2022. [Online]. Available: https:
//x-lance.sjtu.edu.cn/en/papers/2022/zyc97-jstsp22. pdf
[6] H.-J. Chang, S.-w. Yang, and H.-y. Lee, “DistilHuBERT:
Speech representation learning by layer-wise distillatio n of
hidden-unit bert,” in ICASSP 2022-2022 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2022, pp. 7087–7091. [Online]. Available:
https://ieeexplore.ieee.org/document/9747490/
[7] Y . Lee, K. JANG, J. Goo, Y . Jung, and H.-R. Kim,
“FitHuBERT: Going thinner and deeper for knowledge dis-
tillation of speech self-supervised learning,” in 23rd Annual
Conference of the International Speech Communication Asso -
ciation, INTERSPEECH 2022 . ISCA, 2022, pp. 3588–3592.
[Online]. Available: https://www.isca-speech.org/arch ive//pdfs/
interspeech 2022/lee22p interspeech.pdf
[8] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta,
and Y . Bengio, “Fitnets: Hints for thin deep nets,” arXiv
preprint arXiv:1412.6550 , 2014. [Online]. Available: https:
//arxiv.org/abs/1412.6550
[9] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y . Zhang,
T. Ko, and H. Li, “LightHuBERT: Lightweight and Conﬁgurable
Speech Representation Learning with Once-for-All Hidden- Unit
BERT,” in Interspeech 2022 . ISCA, Sep. 2022, pp. 1686–
1690. [Online]. Available: https://www.isca-speech.org /archive/
interspeech 2022/wang22t interspeech.html
[10] C. Louizos, M. Welling, and D. Kingma, “Learning sparse
neural networks through l0 regularization.” in Sith International
Conference on Learning Representations, 2018 , 2018. [Online].
Available: https://openreview.net/pdf?id=H1Y8hhg0b
[11] Y . Peng, K. Kim, F. Wu, P. Sridhar, and S. Watanabe, “Stru ctured
pruning of self-supervised pre-trained models for speech
recognition and understanding,” in 2023 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP) , Jun. 2023, pp. 1–5. [Online]. Available: https:
//ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1009 5780
[12] A. v. d. Oord, Y . Li, and O. Vinyals, “Representation lea rning with
contrastive predictive coding,” arXiv preprint arXiv:1807.03748 ,
2018. [Online]. Available: https://arxiv.org/abs/1807. 03748[13] M. Xia, Z. Zhong, and D. Chen, “Structured pruning learn s
compact and accurate models,” in Proceedings of the 60th
Annual Meeting of the Association for Computational Lingui stics
(Volume 1: Long Papers) . Dublin, Ireland: Association for
Computational Linguistics, May 2022, pp. 1513–1528. [Onli ne].
Available: https://aclanthology.org/2022.acl-long.10 7
[14] V . Sanh, T. Wolf, and A. Rush, “Movement pruning: Adapti ve
sparsity by ﬁne-tuning,” Advances in Neural Information
Processing Systems , vol. 33, pp. 20 378–20 389, 2020. [Online].
Available: https://proceedings.neurips.cc/paper ﬁles/paper/2020/
ﬁle/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf
[15] M. Yang, A. Tjandra, C. Liu, D. Zhang, D. Le, and
O. Kalinli, “Learning ASR pathways: A sparse multilingual A SR
model,” in 2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , Jun. 2023, pp. 1–5.
[Online]. Available: https://ieeexplore.ieee.org/stam p/stamp.jsp?
arnumber=10094300
[16] C.-I. J. Lai, Y . Zhang, A. H. Liu, S. Chang, Y .-L. Liao,
Y .-S. Chuang, K. Qian, S. Khurana, D. Cox, and J. Glass,
“PARP: Prune, adjust and re-prune for self-supervised spee ch
recognition,” Oct. 2021, arXiv:2106.05933 [cs, eess]. [On line].
Available: http://arxiv.org/abs/2106.05933
[17] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell,
“Rethinking the value of network pruning,” in International
Conference on Learning Representations , 2018. [Online].
Available: https://openreview.net/pdf?id=rJlnB3C5Ym
[18] Y . Bengio, N. L´ eonard, and A. Courville, “Estimating o r
propagating gradients through stochastic neurons for cond itional
computation,” arXiv preprint arXiv:1308.3432 , 2013. [Online].
Available: https://arxiv.org/abs/1308.3432
[19] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and
M. Auli, “Data2vec: A general framework for self-supervise d
learning in speech, vision and language,” in International
Conference on Machine Learning . PMLR, 2022, pp. 1298–
1312. [Online]. Available: https://proceedings.mlr.pre ss/v162/
baevski22a/baevski22a.pdf
[20] P. Yin, J. Lyu, S. Zhang, S. J. Osher, Y . Qi, and J. Xin,
“Understanding straight-through estimator in training ac tivation
quantized neural nets,” in International Conference on Learning
Representations , 2019. [Online]. Available: https://openreview.
net/forum?id=Skh4jRcKQ
[21] L. Chen, M. Asgari, and H. H. Dodge, “Optimize Wav2vec2s
architecture for small training set through analyzing its p re-
trained models attention pattern,” in 2022 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP) , May 2022, pp. 7112–7116. [Online]. Available:
https://ieeexplore.ieee.org/document/9747831
[22] Z. Wang, J. Wohlwend, and T. Lei, “Structured pruning of large
language models,” in Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP) ,
2020, pp. 6151–6162. [Online]. Available: https://aclant hology.
org/2020.emnlp-main.496.pdf
[23] S. wen Yang, P.-H. Chi, Y .-S. Chuang, C.-I. J. Lai, K. Lak ho-
tia, Y . Y . Lin, A. T. Liu, J. Shi, X. Chang, G.-T.
Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu,
Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed,
and H. yi Lee, “SUPERB: Speech Processing Universal PER-
formance Benchmark,” in Proc. Interspeech 2021 , 2021, pp.
1194–1198. [Online]. Available: https://www.isca-speec h.org/
archive/interspeech 2021/yang21c interspeech.html
[24] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur,
“Librispeech: an ASR corpus based on public domain audio
books,” in 2015 IEEE international conference on acoustics,
speech and signal processing (ICASSP) . IEEE, 2015, pp.
5206–5210. [Online]. Available: https://www.danielpove y.com/
ﬁles/2015 icassp librispeech.pdf
[25] S. Schneider, A. Baevski, R. Collobert, and M. Auli, “Wa v2vec:
Unsupervised Pre-Training for Speech Recognition,” in Proc.
Interspeech 2019 , 2019, pp. 3465–3469. [Online]. Available:
http://dx.doi.org/10.21437/Interspeech.2019-1873
