# 2004.14765.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2004.14765.pdf
# File size: 313841 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2004.14765v1  [cs.LG]  30 Apr 2020Pruning artiﬁcial neural networks:
a way to ﬁnd well-generalizing,
high-entropy sharp minima
Enzo Tartaglione[0000−0003−4274−8298], Andrea Bragagnolo, and
Marco Grangetto[0000−0002−2709−7864]
University of Torino, Computer Science dept., Torino TO 101 49, Italy
{enzo.tartaglione, andrea.bragagnolo }@unito.it
Abstract. Recently, a race towards the simpliﬁcation of deep networks
has begun, showing that it is eﬀectively possible to reduce t he size of
these models with minimal or no performance loss. However, t here is a
general lack in understanding why these pruning strategies are eﬀective.
In this work, we are going to compare and analyze pruned solut ions
with two diﬀerent pruning approaches, one-shot and gradual , showing
the higher eﬀectiveness of the latter. In particular, we ﬁnd that grad-
ual pruning allows access to narrow, well-generalizing min ima, which are
typically ignored when using one-shot approaches. In this w ork we also
propose PSP-entropy, a measure to understand how a given neu ron cor-
relates to some speciﬁc learned classes. Interestingly, we observe that
the features extracted by iteratively-pruned models are le ss correlated to
speciﬁc classes, potentially making these models a better ﬁ t in transfer
learning approaches.
Keywords: Pruning ·Sharp minima ·Entropy ·Post synaptic potential
·Deep learning.
1 Introduction
Artiﬁcial neural networks (ANNs) are nowadays one of the most s tudied algo-
rithm used to solvea huge varietyoftasks.Their successcomesfr om their ability
to learn from examples, not requiring any speciﬁc expertise and usin g very gen-
eral learning strategies. The use of GPUs (and, recently, TPUs) f or training
ANNs gave a decisive kick to their large-scale deploy.
However, many deep models share a common obstacle: the large num ber of pa-
rameters, which allows their successful training [1,4], determines in turn a large
number of operations at inference time, preventing eﬃcient deploy ment to mo-
bile and cheap embedded devices.
In order to address this problem, a number of approaches have be en proposed,
like deﬁning new, more eﬃcient models [9]. Recently, a race to shrink th e size of
these ANN models has begun: the so-called pruning strategies are indeed able to
remove (or prune) non-relevant parameters from pre-trained models, reducing

--- PAGE 2 ---
2 E. Tartaglione et al.
the size of the ANN model, yet keeping a high generalization capability. On this
topic, a very large amount of strategies have been proposed [6,13 ,19,21] from
which we can identify two main classes:
–one-shot strategies: which are able to prune parameters using ve ry fast,
greedy approaches;
–gradual strategies: much slower than one-shot approaches, po tentially they
can achieve higher compression rates (or in other words, they promise to
prune more parameters at the cost of higher computational comp lexity).
In such a rush, however,an eﬀort into a deeper understanding on potential prop-
erties of such sparse architectures has been mostly set aside: is t here a speciﬁc
reason for which we are able to prune many parameters with minimal o r no
generalization loss? Are one-shot strategies enough to match gra dual pruning
approaches? Is there any hidden property behind these sparse a rchitectures?
In this work, we ﬁrst compare one-shot pruning strategies to the ir gradual coun-
terparts,investigatingtheeventualbeneﬁtsofhavingamuchmo recomputationally-
intensive sparsifying strategy. Then, we shine a light on some local p roperties
of minima achieved using the two diﬀerent pruning strategies and ﬁna lly, we
proposePSP-entropy , a measure on the state of ReLU-activated neurons, to be
used as an analysis tool to get a better understanding for the obt ained sparse
ANN models.
The rest of this paper is organized as follows. Sec. 2 reviews the impo rtance of
network pruning and the most relevant literature. Next, in Sec. 3 w e discuss the
relevant literature around local properties of minima for ANN models . Then, in
Sec. 4 we propose PSP-entropy, a metric to measure how much a ne uron special-
izes in identifying features belonging to a sub-set of classes learned at training
time. Sec. 5 provides our ﬁndings on the properties for sparse arc hitectures and
ﬁnally, in Sec. 6, we draw the conclusions and identify further direct ions for
future research.
2 State of the art pruning techniques
In the literatureit is possible to ﬁnd a largenumber ofpruning approa ches,some
old-fashioned [11] and others more recent [8,12,16]. Among the latte r, many sub-
categories can be identiﬁed. Ullrich et al.introduce what they call soft weight
sharing, through which is possible to introduce redundancy in the network a nd
reduce the amount of stored parameters [23]. Other approache s are based on
parameters regularization and pruning: for example, Louizos et al.use anL0
proxy regularization; Tartaglione et al., instead, deﬁne the importance of a pa-
rameter via a sensitivity measure used as regularization [21]. Other a pproaches
aredropout-based,like sparse variational dropout , proposedbyMolchanov et al.,
leveragingon abayesianinterpretationofGaussiandropoutand pr omotingspar-
sity in the ANN model [16].
Overall, most of the proposed pruning techniques can be divided in tw o macro
classes. The ﬁrst is deﬁned by approaches based on gradual pruning [14,19,25],

--- PAGE 3 ---
Pruning ANNs: a way to ﬁnd well-generalizing, high-entropy sharp minima 3
in which the network is, at the same time, trained and pruned following some
heuristic approach, spanning a large number of pruning iterations. One among
these, showing the best performances, is LOBSTER, where param etersare grad-
ually pruned according to their local contribution to the loss [19]. The second
class, instead, includes all the techniques based on one-shot pruning [6,8,15]:
here the pruning procedure consists of three stages:
1. a large, over-parametrized network is normally trained to comple tion;
2. the network is then pruned using some kind of heuristic (e.g. magn itude
thresholding) to satisfaction (the percentage of remaining param eters is typ-
ically an hyper-parameter);
3. the pruned model is further ﬁne-tuned to recover the accura cy lost due to
the pruning stage.
A recent work by Frankle and Carbin [6] termed the lottery ticket hypothesis ,
which is having a large impact on the researchcommunity. They claim th at from
an ANN, early in the training, it is possible to extract a sparse sub-ne twork on
a one-shot fashion: such sparse network, when trained, can mat ch the accuracy
of the original model. In a follow-up, Renda et al.propose a retraining approach
that replaces the ﬁne-tuning step with weight rewinding : after pruning, the re-
maining parameters are reset to their initial values and the pruned n etwork is
trained again. They also argue that using the initial weights values is f undamen-
tal to achieve competitive performance, which is degraded when st arting from a
random initialization [18].
On the other hand, Liu et al.show that, even when retraining a pruned sub-
network using a new random initialization, they are able to reach an ac curacy
level comparable to its dense counterpart; challenging one of the c onjectures
proposed alongside the lottery ticket hypothesis [13].
In our work we try to shed some light on this discussion, comparing st ate-of-
the-art one-shot pruning to gradual pruning.
3 Local properties of minima
In the previous section we have explored some of the most relevant pruning
strategies. All of them rely on state-of-the-art optimization str ategies: applying
very simple optimizing heuristics to minimize the loss function, like for ex am-
ple SGD [2,26], it is nowadays possible to succeed in training ANNs on huge
datasets. Theoretically speaking, this is the “miracle” of deep learn ing, as the
dimensionality of the problem is huge (indeed, these problems are typ ically over-
parametrized, and the dimensionality can be eﬃciently reduced [21]). Further-
more, minimizing non-convex objective functions is typically suppose d to make
the trained architecture stuck into local minima. However, the emp irical evi-
dence shows that something else is happening under the hood: unde rstanding it
is in general critical.
Goodfellow et al.pioneered the problem of understanding why deep learning
works. In particular, they observed there is essentially no loss bar rier between

--- PAGE 4 ---
4 E. Tartaglione et al.
a generic random initialization for the ANN model and the ﬁnal conﬁgu ra-
tion [7]. This phenomena has also been observed on larger architectu res by
Draxleret al.[5]. These works lay as basis for the “lottery ticket hypothesis”
papers. However, a secondary yet relevant observation in [7] sta ted that there
is a loss barrier between diﬀerent ANN conﬁgurations showing similar g eneral-
ization capabilities. Later, it was shown that typically a low loss path be tween
well-generalizingsolutions to the same learning problem can be found [ 20]. From
this brief discussion it is evident that a general approach on how to b etter char-
acterize such minima has yet to be found.
Keskaret al.showed why we should prefer small batch methods to large batch
ones: they correlate the stochasticity introduced by small-batch methods to the
sharpness of the reached minimum [10]. In general, they observe th at the larger
the batch, the sharper the reached minimum. Even more interestin gly, they ob-
serve that the sharper the minimum, the worse the generalization o f the ANN
model. In general, there are many works supporting the hypothes is that ﬂat
minima generalize well, and this has been also the strength for a signiﬁc ant part
of the current research [3,10]. However, in general this does not n ecessarily mean
that no sharp minimum generalizes well, as we will see in Sec. 5.2.
4 Towards a deeper understanding: an entropy-based
approach
In this section we propose PSP-entropy, a metric to evaluate the d ependence of
the output for a given neuron in the ANN model to the target classiﬁ cation task.
The proposed measure will allow us to better understand the eﬀect of pruning.
4.1 Post-synaptic potential
Let us deﬁne the output of the given i-th neuron at the l-th layer as
yl,i=ϕ[f(yl−1,θl,i]) (1)
whereyl−1is the input of such neuron, θl,iare the parameters associated to it,
f(·) is some aﬃne function and ϕ(·) is the activation function, we can deﬁne its
post-synaptic potential (PSP) [22] as
zl,i=f(yl−1,θl,i) (2)
Typically, deep models are ReLU-activated: here on, let us consider the activa-
tion function for all the neurons in hidden layers as ϕ(·) = ReLU( ·). Under such
assumption it is straightforward to identify two distinct regions for the neuron
activation:
–zl,i≤0: the output of the neuron will be exactly zero ∀zl,i≤0;
–zl,i>0: there is a linear dependence of the output to zl,i.

--- PAGE 5 ---
Pruning ANNs: a way to ﬁnd well-generalizing, high-entropy sharp minima 5
Hence, let us deﬁne
ϕ′(z) =/braceleftbigg
0z≤0
1z >0(3)
Intuitively, we understand that if two neurons belonging to the sam e layer, for
the same input, share the same ϕ′(z), then they are linearly-mappable to one
equivalent neuron:
–zl,i≤0,zl,j≤0: one of them can be simply removed;
–zl,i>0,zl,j>0: they are equivalent to a linear combination of them.
In this work we are not interested in using this approach towards st ructured
pruning: there are many works in the literature which tackle this issu e using
eﬃcientproxies.Inthenextsectionwearegoingtoformulateamet rictoevaluate
the degree of disorder in the post synaptic potentials. The aim of su ch measure
will be to have an analytical tool to give us a broader understanding on the
behavior of the neurons in sparse architectures.
4.2 PSP-entropy for ReLU-activated neurons
In the previous section we have recalled the concept of post-syna ptic potential.
Some interesting concepts have been also introduced for ReLU-ac tivated net-
works: we can use its value to approach the problem of binningthe state of a
neuron, according to ϕ′(zl,i). Hence, we can construct a binary random process
that we can rank according to its entropy. To this end, let us assum e we set as
input of our ANN model two diﬀerent patterns, µc,1andµc,2, belonging to the
same class c(for those inputs, we aim at having the same target at the output
of the ANN model). Let us consider the PSP zl,i(wherelis an hidden layer):
–ifϕ′(zl,i|µc,1) =ϕ′(zl,i|µc,2) we can say there is low PSP entropy ;
–ifϕ′(zl,i|µc,1)/ne}ationslash=ϕ′(zl,i|µc,2) we can say there is high PSP entropy .
We can model an entropy measure for PSP:
H(zl,i|c) =−/summationdisplay
t={0,1}p[ϕ′(zl,i) =t|c]·log2{p[ϕ′(zl,i) =t|c]}(4)
wherep[ϕ′(zl,i) =t|c] is the probability ϕ′(zl,i) =twhen presented an input be-
longing to the c-th class. Since we typically aim at solving a multi-class problem,
we can model an overall entropy for the neuron as
H(zl,i) =/summationdisplay
cH(zl,i|c) (5)
It is very important to separate the contributions of the entropy according to
thec-th target class since we expect the neurons to catch relevant fe atures be-
ing highly-correlated to the target classes. Eq. (5) provides us ve ry important
information towards this end: the lower its value the more it specialize s for some
speciﬁc classes.

--- PAGE 6 ---
6 E. Tartaglione et al.
The formulation in (5) is very general and it can be easily extended to higher-
order entropy, i.e. entropy of sets of neurons whose state corr elates for the same
classes. Now we are ready to use this metrics to shed further light t o the ﬁndings
in Sec. 5.
5 Experiments
For our test, we have decided to compare the state-of-the-art one-shot pruning
proposedbyFrankleandCarbin[6] toone ofthe top-performingg radualpruning
strategies, LOBSTER [19]. Towards this end, we ﬁrst obtain a spars e network
model using LOBSTER; the non-pruned parameters are then re-in itialized to
their original values, according to the lottery ticket hypothesis [6]. Our purpose
here is to determine whether the lottery ticket hypothesis applies a lso to the
sparse models obtained using high-performing gradual pruning str ategies.
As a second experiment, we want to test the eﬀects of diﬀerent, r andom initial-
ization while keeping the achieved sparse architecture. According t o Liuet al.,
this should lead to similar results to those obtained with the original init ializa-
tion [13]. Towards this end, we tried 10 diﬀerent new starting conﬁgu rations.
As a last experiment, we want to assess how important is the struct ure origi-
nating from the pruning algorithm in reaching competitive performan ces after
re-initialization: for this purpose, we randomly deﬁne a new pruned a rchitecture
with the same number of pruned parametersas those found via LOB STER. Also
in this case, 10 diﬀerent structures have been tested.
We decided to experiment with diﬀerent architectures and dataset s commonly
employed in the relevant literature: LeNet-300 and LeNet-5-caﬀe trained on
MNIST, LeNet-5-caﬀe trained on Fashion-MNIST [24] and ResNet- 32 trained
on CIFAR-10.1For all our trainings we used the SGD optimization method with
standard hyper-parameters and data augmentation, as deﬁned in the papers of
the diﬀerent compared techniques [6,13,19].
5.1 One-shot vs gradual pruning
In Fig. 1 we show, for diﬀerent percentages of pruned parameter s, a comparison
between the test accuracy of models pruned using the LOBSTER te chnique and
the models retrained following the approaches we previously deﬁned .
We can clearly identify a low compression rate regime in which the re-init ialized
model is able to recover the original accuracy, validating the lotter y ticket hy-
pothesis. On the other hand, when the compression rate rises (fo r example when
we remove more than 95% of the LeNet-300 model’s parameters, as observed in
Fig. 1a), the re-training approach strives in achieving low classiﬁcat ion errors.
As one might expect, other combinations of dataset and models migh t react
diﬀerently. For example, LeNet-300 is no longer able to reproduce t he original
performance when composed of less then 5% of the original parame ters. On the
1https://github.com/akamaster/pytorch resnetcifar10

--- PAGE 7 ---
Pruning ANNs: a way to ﬁnd well-generalizing, high-entropy sharp minima 7
0 40.15 60.64 80.06 90.07 95.0 98.0
Pruned Parameters (%)1.61.92.53.03.55.0Classification ErrorLOBSTER
Frankle and Carbin
Liu et al.
Random pruning0 0.98 2.44 11.7 33.66 48.54 64.14Pruned Neurons (%)
(a)
0 63.05 80.14 90.01 95.06 98.0 99.02 99.57
Pruned Parameters (%)0.70.81.01.52.03.0Classification ErrorLOBSTER
Frankle and Carbin
Liu et al.
Random pruning0 1.5513.62 37.75 60.17 74.31 81.03 83.45Pruned Neurons (%)
(b)
0 60.89 80.53 90.08 95.02
Pruned Parameters (%)8.208.458.809.5010.0010.50Classification ErrorLOBSTER
Frankle and Carbin
Liu et al.
Random pruning0 0 7.59 30.17 46.72Pruned Neurons (%)
(c)
0 40.24 60.72 80.11
Pruned Parameters (%)7.27.47.67.88.59.0Classification ErrorLOBSTER
Frankle and Carbin
Liu et al.
Random pruning0 0.43 0.52 0.69Pruned Neurons (%)
(d)
Fig.1: Test set error for diﬀerent compression rates: LeNet-30 0 (a) trained on
MNIST, LeNet-5 trained on MNIST (b), LeNet-5 trained on Fashion -MNIST (c)
and ResNet-32 trained on CIFAR-10 (d).

--- PAGE 8 ---
8 E. Tartaglione et al.
other hand, LeNet-5, when applied on MNIST, is able to achieve an ac curacy of
around 99.20%even when 98% of its parametersare pruned away(F ig. 1b). This
does not happen when applied on a more complex dataset like Fashion- MNIST,
where removing 80% of the parameters already leads to performan ce degrada-
tion (Fig. 1c). Such a gap becomes extremely evident when we re-init ialize an
even more complex architecture like ResNet-32 trained on CIFAR-1 0 (Fig. 1d).
From the reported results, we observe that the original initializatio n is not al-
waysimportant:the errorgapbetween a randomlyinitialized model a nd a model
using the original weights’ values is minor, with the latter being slightly better.
Furthermore, they both fail in recovering the performance for h igh compression
rates.
5.2 Sharp minima can also generalize well
10−510−4
(a)
102103
(b)
Fig.2: Results of LeNet-5 trained on MNIST with the highest compre ssion
(99.57%): (a) plots loss in the training set and (b) plots the top-5 lar gest hessian
eigenvalues. G is the solution found with gradual learning while 1-S is th e best
one-shot solution (Frankle and Carbin).
In order to study the sharpness of local minima, let us focus, for e xample, on
the results obtained on LeNet-5 trained on MNIST. We choose to fo cus our at-
tention on this particular ANN model since, according to the state- of-the-art
and coherently to our ﬁndings, we observe the lowest performanc e gap between
gradual and one-shot pruning (as depicted in Fig. 1b); hence, it is a more chal-
lenging scenario to observe qualitative diﬀerences between the two approaches.
However, we remark that all the observations for such a case app ly also to the
other architectures/datasets explored in Sec. 5.1.
In order to obtain the maps in Fig. 2, we follow the approach propose d by [7]

--- PAGE 9 ---
Pruning ANNs: a way to ﬁnd well-generalizing, high-entropy sharp minima 9
and we plot the loss for the ANN conﬁgurations between two refere nce ones:
in our, case, we compare a solution found with gradual pruning (G) a nd one-
shot (1-S). Then, we take a random orthogonal direction to gene rate a 2D map.
Fig. 2a shows the loss on the training set between iterative and one- shot prun-
ing for the highest compression rate (99.57% of pruned parameter s as shown in
Fig. 1b). According to our previous ﬁndings, we see that iterative p runing lies
in a lower loss region. Here, we show also the plot of the top-5 Hessian eigen-
values (all positive), in Fig. 2b, using the eﬃcient approach propose d in [17].
Very interestingly, we observe that the solution proposed by itera tive pruning
lies in a narrower minimum than the one found using the one-shot stra tegy,
despite generalizing slightly better. With this, we do not claim that nar rower
minima generalize well: gradual pruning strategies enable access to a subset of
well-generalizing narrow minima , showing that not all the narrowminima gener-
alize worse than the wide ones. This ﬁnding raises warnings against se cond order
optimization, which might favor the research of ﬂatter, wider minima , ignoring
well-generalizingnarrowminima. These non-trivial solutions are natu rally found
using gradual pruning which cannot be found using one-shot appro aches, which
on the contrary focus their eﬀort on larger minima. In general, the sharpness of
these minima explains why, for high compression rates, re-training s trategies fail
in recovering the performance, considering that it is in general har der to access
this class of minima.
5.3 Study on the post synaptic potential
103104
Fig.3: L2 norm of PSP values for LeNet-5 trained on MNIST with 99.57 % of
pruned parameters.
In Sec. 5.2 we have observed that, as a result, iterative strategie s focus on well-
generalizing sharp minima. Is there something else yet to say about t hose?
Let us inspect the average magnitude values of the PSPs for the diﬀ erent found
solutions: towards this end, we could plot the average of their L2 no rm values
(z2). As a ﬁrst ﬁnding, gradually-pruned architectures naturally hav e lower PSP
L2-norm values, as we observe in Fig. 3. None of the used pruning st rategies

--- PAGE 10 ---
10 E. Tartaglione et al.
explicitly minimize the term in z2: they naturally drive the learning towards
such regions. However, the solution showing better generalization capabilities
shows lower z2values. Of course, there are regions with even lower z2values;
however, according to Fig. 2a, they should be excluded since they c orrespond
to high-loss values (not all the low z2regions are low-loss). If we look at the
1.9
1.88
1.86
1.84
1.82
(a)
4.50
4.45
4.40
4.35
(b)
Fig.4:Results on LeNet-5trained on MNIST with 99.57%of pruned pa rameters.
(a) plots the ﬁrst order PSP-entropy, while (b) shows the second -order PSP
entropy.
PSP-entropy formulated in (5), we observe something interesting : gradual and
one-shot pruning show comparable ﬁrst-order entropies, as sho wn in Fig. 4a.2
It is interesting to see that there are also lower entropy regions wh ich how-
ever correspond to higher loss values, according to Fig. 2a. When w e move to
higher-order entropies, something even more interesting happen s: gradual prun-
ing shows higher entropy than one-shot, as depicted in Fig. 4b (disp laying the
second orderentropy). In such a case, having a lowerentropyme ans having more
groups of neurons specializing to speciﬁc patterns which correlate to the target
class; on the contrary, having higher entropy yet showing better generalization
performance results in having more general features, more agno stic towards a
speciﬁc class, which still allow a correct classiﬁcation performed by t he output
layer. This counter-intuitive ﬁnding has potentially-huge application s in trans-
fer learning and domain adaptation, where it is critical to extract mo re general
features, not very speciﬁc to the originally-trained problem.
2the source code for PSP-entropy is available at
https://github.com/EIDOSlab/PSP-entropy.git

--- PAGE 11 ---
Pruning ANNs: a way to ﬁnd well-generalizing, high-entropy sharp minima 11
6 Conclusion
In this work we have compared one-shot and gradual pruning on diﬀ erent state-
of-the-art architectures and datasets. In particular, we have focused our atten-
tioninunderstandingpotentialdiﬀerencesandlimitsofbothapproa chestowards
achieving sparsity in ANN models.
We have observed that one-shot strategies are very eﬃcient to a chieve mod-
erate sparsity at a lower computational cost. However, there is a limit to the
maximum achievable sparsity, which can be overcome using gradual p runing.
The highly-sparse architecture, interestingly, focus on a subset of sharp minima
which areable to generalizewell,which posesome questionstothe pot entialsub-
optimality of second-order optimization in such scenarios. This expla ins why we
observe that one-shot strategies fail in recovering the perform ance for high com-
pression rates. More importantly, we have observed, contrarily t o what it could
be expected, that highly-sparse gradually-pruned architecture s are able to ex-
tract general features non-strictly correlated to the trained c lasses, making them
unexpectedly, potentially, a good match for transfer-learning sc enarios.
Future works include a quantitative study on transfer-learning fo r sparse archi-
tectures and PSP-entropy maximization-based learning.
References
1. Ba, J., Caruana, R.: Do deep nets really need to be deep? In: Advances in neural
information processing systems. pp. 2654–2662 (2014)
2. Bottou, L.: Large-scale machine learning with stochasti c gradient descent. In: Pro-
ceedings of COMPSTAT’2010, pp. 177–186. Springer (2010)
3. Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Ba ldassi, C., Borgs, C.,
Chayes, J., Sagun, L., Zecchina, R.: Entropy-sgd: Biasing g radient descent into
wide valleys. arXiv preprint arXiv:1611.01838 (2016)
4. Denton, E.L., Zaremba, W., Bruna, J., LeCun, Y., Fergus, R .: Exploiting linear
structure within convolutional networks for eﬃcient evalu ation. In: Advances in
neural information processing systems. pp. 1269–1277 (201 4)
5. Draxler, F., Veschgini, K., Salmhofer, M., Hamprecht, F. A.: Essentially no barriers
in neural network energy landscape. arXiv preprint arXiv:1 803.00885 (2018)
6. Frankle, J., Carbin, M.: The lottery ticket hypothe-
sis: Finding sparse, trainable neural networks (2019),
https://www.scopus.com/inward/record.uri?eid=2-s2.0 -85069453436&partnerID=40&md5=fd1a2b2384d79f66a49cc 838a76343d3 ,
cited By 8
7. Goodfellow, I.J., Vinyals, O., Saxe, A.M.: Qualitativel y characterizing neural net-
work optimization problems. arXiv preprint arXiv:1412.65 44 (2014)
8. Han, S., Pool, J., Tran, J., Dally, W.: Learning both weigh ts and connections for
eﬃcient neural network. In: Advances in neural information processing systems.
pp. 1135–1143 (2015)
9. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W ., Weyand, T., An-
dreetto, M., Adam, H.: Mobilenets: Eﬃcient convolutional n eural networks for
mobile vision applications. arXiv preprint arXiv:1704.04 861 (2017)

--- PAGE 12 ---
12 E. Tartaglione et al.
10. Keskar, N.S., Mudigere, D., Nocedal, J., Smelyanskiy, M ., Tang, P.T.P.: On large-
batch training for deep learning: Generalization gap and sh arp minima. arXiv
preprint arXiv:1609.04836 (2016)
11. LeCun, Y., Denker, J.S., Solla, S.A.: Optimal brain dama ge. In: Advancesin neural
information processing systems. pp. 598–605 (1990)
12. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning ﬁlters for eﬃcient
convnets. arXiv preprint arXiv:1608.08710 (2016)
13. Liu, Z., Sun, M., Zhou, T., Huang, G., Darrell, T.: Rethin king the value of network
pruning. arXiv preprint arXiv:1810.05270 (2018)
14. Louizos, C., Welling, M., Kingma, D.P.: Learning sparse neural networks through
l0 regularization. arXiv preprint arXiv:1712.01312 (2017)
15. Luo, J.H., Wu, J., Lin, W.: Thinet: A ﬁlter level pruning m ethod for deep neural
network compression. In: Proceedings of the IEEE internati onal conference on
computer vision. pp. 5058–5066 (2017)
16. Molchanov, D., Ashukha, A., Vetrov, D.: Variational dro pout spar-
siﬁes deep neural networks. In: 34th International Confere nce on
Machine Learning, ICML 2017. vol. 5, pp. 3854–3863 (2017),
https://www.scopus.com/inward/record.uri?eid=2-s2.0 -85048506601&partnerID=40&md5=c352a4786ef977ccea7e3 97bd7469f14 ,
cited By 29
17. Noah Golmant, Zhewei Yao, A.G.M.M.J.G.: pytorch-hessi an-
eigentings: eﬃcient pytorch hessian eigendecomposition ( Oct 2018),
https://github.com/noahgolmant/pytorch-hessian-eige nthings
18. Renda, A., Frankle, J., Carbin, M.: Comparing rewinding and ﬁne-tuningin neural
network pruning. arXiv preprint arXiv:2003.02389 (2020)
19. Tartaglione, E., B.A.G.M.L.S.: Loss-based sensitiv-
ity regularization:towards deep sparse neural networks.
https://iris.unito.it/retrieve/handle/2318/1737767/ 608158/ICML20.pdf (2020)
20. Tartaglione, E., Grangetto, M.: Take a ramble into solut ion spaces for classiﬁcation
problems in neural networks. In: International Conference on Image Analysis and
Processing. pp. 345–355. Springer (2019)
21. Tartaglione, E., Lepsøy, S., Fiandrotti, A., Francini, G.: Learning sparse neural
networks via sensitivity-driven regularization. In: Adva nces in Neural Information
Processing Systems. pp. 3878–3888 (2018)
22. Tartaglione, E., Perlo, D., Grangetto, M.: Post-synapt ic potential regularization
has potential. In: International Conference on Artiﬁcial N eural Networks. pp. 187–
200. Springer (2019)
23. Ullrich, K., Welling, M., Meeds, E.: Soft weight-sharin g for neu-
ral network compression. In: 5th International Conference on Learn-
ing Representations, ICLR 2017 - Conference Track Proceedi ngs (2019),
https://www.scopus.com/inward/record.uri?eid=2-s2.0 -85071003624&partnerID=40&md5=dc00c36113f775ﬀ4a6978 b86543814d ,
cited By 2
24. Xiao, H., Rasul, K., Vollgraf, R.: Fashion-mnist: a nove l image dataset for
benchmarking machine learning algorithms. CoRR abs/1708.07747 (2017),
http://arxiv.org/abs/1708.07747
25. Zhu, M., Gupta, S.: To prune, or not to prune: exploring th e eﬃcacy of pruning
for model compression. arXiv preprint arXiv:1710.01878 (2 017)
26. Zinkevich, M., Weimer, M., Li, L., Smola, A.J.: Parallel ized stochastic gradient
descent. In: Advances in neural information processing sys tems. pp. 2595–2603
(2010)
