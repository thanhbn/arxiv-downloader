# 2211.12551.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2211.12551.pdf
# Kích thước tệp: 1553622 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Mạch Xác Suất Thưa thông qua
Cắt tỉa và Phát triển
Meihua Dang
Khoa CS
UCLA
mhdang@cs.ucla.eduAnji Liu
Khoa CS
UCLA
liuanji@cs.ucla.eduGuy Van den Broeck
Khoa CS
UCLA
guyvdb@cs.ucla.edu
Tóm tắt
Mạch xác suất (PCs) là một biểu diễn dễ xử lý của phân phối xác suất
cho phép tính toán chính xác và hiệu quả các likelihood và marginal. Đã có
tiến bộ đáng kể gần đây trong việc cải thiện quy mô và khả năng biểu đạt của
PCs. Tuy nhiên, hiệu suất huấn luyện PC đạt mức cực hạn khi kích thước mô hình tăng. Chúng tôi khám
phá ra rằng hầu hết dung lượng trong các cấu trúc PC lớn hiện tại bị lãng phí: các lớp tham số
kết nối đầy đủ chỉ được sử dụng một cách thưa thớt. Chúng tôi đề xuất hai phép toán: cắt tỉa
và phát triển, nhằm khai thác tính thưa thớt của cấu trúc PC. Cụ thể, phép toán cắt tỉa
loại bỏ các mạng con không quan trọng của PC để nén mô hình và đi kèm với các đảm bảo lý thuyết. Phép toán phát triển tăng dung lượng mô hình bằng cách tăng kích thước của không gian tiềm ẩn. Bằng cách áp dụng luân phiên cắt tỉa
và phát triển, chúng tôi tăng dung lượng được sử dụng một cách có ý nghĩa, cho phép chúng tôi
mở rộng đáng kể việc học PC. Về mặt thực nghiệm, bộ học của chúng tôi đạt được likelihood tối ưu
trên các bộ dữ liệu hình ảnh họ MNIST và dữ liệu ngôn ngữ Penn Tree Bank so với các bộ học PC khác và các mô hình tạo sinh sâu ít xử lý được hơn như
mô hình dựa trên flow và variational autoencoders (VAEs).

1 Giới thiệu
0.00 0.01 0.02 0.03 0.04 0.05
Giá trị Tham số0.00.10.20.30.40.5Tỷ lệ phần trăm< 0.001 (54%)
< 0.01 (76%)
≥ 0.05 (5%)
Hình 1: Biểu đồ tần suất của giá trị tham số cho một PC tối ưu với 2.18M tham số trên MNIST.
95% tham số có giá trị gần bằng không.Mạch xác suất (PCs) [44,3] là một khung
thống nhất để trừu tượng hóa từ nhiều
mô hình xác suất dễ xử lý. Thuộc tính
chính phân biệt PCs với các mô hình tạo
sinh sâu khác như mô hình dựa trên flow
[31] và VAEs [19] là tính dễ xử lý của chúng. Nó
cho phép chúng tính toán các truy vấn khác nhau, bao
gồm xác suất marginal, một cách chính xác và hiệu
quả [45]. Do đó, PCs ngày càng được
sử dụng trong các ứng dụng đòi hỏi suy luận như
thực thi công bằng thuật toán [2,4], đưa ra
dự đoán dưới dữ liệu thiếu [6,18,23], nén
dữ liệu [26], và phát hiện bất thường [13].
Những tiến bộ gần đây trong việc học PC và điều
chuẩn [40,25], và các triển khai hiệu quả [33,30,8] đã đẩy giới hạn
của khả năng biểu đạt và khả năng mở rộng của PC sao cho
chúng thậm chí có thể phù hợp với hiệu suất của các mô hình tạo sinh sâu ít dễ xử lý hơn, bao gồm mô hình dựa trên flow và VAEs. Tuy nhiên, hiệu suất
Hội nghị lần thứ 36 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2022).arXiv:2211.12551v1 [cs.LG] 22 Nov 2022

--- TRANG 2 ---
của PCs đạt mức cực hạn khi kích thước mô hình tăng. Điều này cho thấy rằng để tiếp tục tăng hiệu suất của PCs,
việc chỉ mở rộng kích thước mô hình là không đủ và chúng ta cần sử dụng tốt hơn dung lượng có sẵn.

Chúng tôi khám phá ra rằng điều này có thể do dung lượng của các PC lớn bị lãng phí. Như được thể hiện trong
Hình 1, hầu hết các tham số trong một PC với 2.18M tham số có giá trị gần bằng không, có ít
ảnh hưởng đến phân phối PC. Vì các cấu trúc PC hiện tại thường có các lớp tham số
kết nối đầy đủ [25, 36], điều này cho thấy rằng các giá trị tham số chỉ được sử dụng một cách thưa thớt.

Trong công trình này, chúng tôi đề xuất khai thác tốt hơn tính thưa thớt của các mô hình PC lớn bằng hai nguyên tố học cấu trúc - cắt tỉa và phát triển. Cụ thể, mục tiêu của phép toán cắt tỉa là xác định
và loại bỏ các mạng con không quan trọng của một PC. Điều này được thực hiện bằng cách định lượng tầm quan trọng của các
tham số PC w.r.t. một bộ dữ liệu sử dụng circuit flows, một thước đo có cơ sở lý thuyết vững chắc giới hạn trên cho sự
giảm log-likelihood do cắt tỉa gây ra. So với điều chuẩn L1, toán tử cắt tỉa đề xuất được thông tin hóa tốt hơn bởi ngữ nghĩa PC, và do đó định lượng
các hiệu ứng toàn cục của cắt tỉa hiệu quả hơn nhiều. Về mặt thực nghiệm, phương pháp cắt tỉa đề xuất đạt được tỷ lệ nén
80-98% với mức giảm likelihood tối đa 1% trên các PC khác nhau.

Phép toán phát triển đề xuất tăng kích thước mô hình bằng cách sao chép các thành phần hiện có và
tiêm nhiễu. Cụ thể, khi áp dụng cho các PC được nén bởi phép toán cắt tỉa, phát triển
tạo ra các PC lớn hơn có thể được tối ưu hóa để đạt hiệu suất tốt hơn. Áp dụng cắt tỉa và
phát triển một cách lặp đi lặp lại có thể tinh chỉnh đáng kể cấu trúc và tham số của một PC. Về mặt thực nghiệm, thước đo log-
likelihoods có thể cải thiện 2% đến 10% sau vài lần lặp. So với các bộ học PC hiện có cũng như các mô hình tạo sinh sâu ít dễ xử lý hơn như VAEs và mô hình dựa trên flow, phương pháp đề xuất của chúng tôi đạt được kết quả ước lượng mật độ tối ưu trên các bộ dữ liệu hình ảnh bao gồm
MNIST, EMNIST, FashionMNIST, và tác vụ mô hình hóa ngôn ngữ Penn Tree Bank.¹

2 Mạch Xác Suất

Mạch xác suất (PCs) [44,3] mô hình hóa phân phối xác suất với đồ thị tính toán có cấu trúc. Chúng là một thuật ngữ bao quát cho một họ lớn các mô hình xác suất dễ xử lý bao gồm
mạch số học [9,10], mạng tổng-tích (SPNs) [35], mạng cutset [36], không gian tìm kiếm and-or [28], và sơ đồ quyết định câu xác suất [21]. Cú pháp và ngữ nghĩa của PCs được
định nghĩa như sau.

Định nghĩa 1 (Mạch Xác Suất). Một PC C := (G; θ) biểu diễn một phân phối xác suất chung p(X)
trên các biến ngẫu nhiên X thông qua một đồ thị có hướng phi chu trình (tính toán) (DAG) G được tham số hóa bởi
θ. Tương tự như mạng neural, mỗi nút trong DAG định nghĩa một đơn vị tính toán. Cụ thể, DAG G bao gồm ba loại đơn vị - đầu vào, tổng, và tích. Mỗi nút lá trong G là một đơn vị đầu vào; mỗi đơn vị trong n (tức là, tổng hoặc tích) nhận đầu vào từ các con của nó in(n), và tính toán
đầu ra, mã hóa một phân phối xác suất pn được định nghĩa đệ quy như sau:

pn(x) := {
fn(x) nếu n là một đơn vị đầu vào,
∏c∈in(n) pc(x) nếu n là một đơn vị tích,
∑c∈in(n) θc|n pc(x) nếu n là một đơn vị tổng,                (1)

trong đó fn(x) là một phân phối đầu vào đơn biến (ví dụ, Gaussian, Categorical), và θc|n biểu thị tham số tương ứng với cạnh (n,c) trong DAG. Đối với mỗi đơn vị tổng n, các tham số đầu vào của nó tổng bằng một, tức là, ∑c∈in(n) θc|n = 1. Trực quan, một đơn vị tích định nghĩa một phân phối nhân tử hóa trên các
đầu vào của nó, và một đơn vị tổng biểu diễn một hỗn hợp trên các phân phối đầu vào của nó với trọng số {θc|n : c ∈ in(n)}.
Cuối cùng, phân phối xác suất của một PC (tức là, pC) được định nghĩa là phân phối được biểu diễn bởi đơn vị gốc r của nó (tức là, pr(x)), nghĩa là, nơ-ron đầu ra của nó. Kích thước của một PC, ký hiệu |C| = |θ|, là số
tham số trong C. Chúng tôi giả định w.l.o.g. rằng một PC luân phiên giữa các lớp đơn vị tổng và đơn vị tích
trước khi đến các đầu vào của nó. Hình 2 cho thấy một ví dụ về PC.

Tính toán (log)likelihood của một PC C cho một mẫu x tương đương với việc đánh giá các đơn vị tính toán của nó
trong G theo cách feedforward tuân theo Phương trình 1. Thuộc tính chính phân biệt PCs với
các mô hình xác suất sâu khác như flows [14] và VAEs [19] là tính dễ xử lý của chúng, đó là khả năng trả lời chính xác và hiệu quả các truy vấn xác suất khác nhau. Bài báo này tập trung vào các PC hỗ trợ tính toán xác suất marginal trong thời gian tuyến tính (w.r.t. kích thước mô hình), vì chúng ngày càng được sử dụng

¹ Mã và thí nghiệm có sẵn tại https://github.com/UCLA-StarAI/SparsePC .

--- TRANG 3 ---
Z1X1
Z2X3
X4X2
(a):12:279
:0140:4
0:6:388
:066X1B(:1) X3B(:2)
:9:8
X1B(:7) X3B(:3):3:7:48
:020:8
0:20:1
0:9X2B(:6)
X4B(:8):6
:8
X2B(:1):1
X4B(:2):2
(b)

Hình 2: Một PC trơn và phân rã được (b) và một mạng Bayes tương đương (a). Mạng Bayes
có 4 biến X = {X1, X2, X3, X4} và 2 biến ẩn Z = {Z1, Z2} với
h = 2 trạng thái ẩn. Thứ tự tính toán feedforward là từ trái sang phải; ⬟ là các phân phối đầu vào Bernoulli,
▲ là các đơn vị tích, và ● là các đơn vị tổng; các giá trị tham số được chú thích trong hộp.
Xác suất của mỗi đơn vị cho gán đầu vào {X1 = 0, X2 = 1, X3 = 0, X4 = 1} được ghi màu đỏ.

(a) PC với các lớp kết nối đầy đủ (b) PC sau phép toán cắt tỉa (c) PC sau phép toán phát triển

Hình 3: Minh họa phép toán cắt tỉa và phát triển. Từ 3a đến 3b, các cạnh màu đỏ được cắt tỉa. Từ 3b đến 3c, các nút được nhân đôi, và mỗi tham số được sao chép 3 lần.

trong các ứng dụng downstream như nén dữ liệu [26] và đưa ra dự đoán dưới dữ liệu thiếu [18], và cũng đạt được khả năng biểu đạt ngang bằng [26,25,24]. Để hỗ trợ suy luận marginal hiệu quả,
PCs cần phải trơn và phân rã được.

Định nghĩa 2 (Tính Trơn và Phân Rã Được [11]). Phạm vi (n) của một đơn vị PC n là tập hợp các
biến đầu vào mà nó phụ thuộc vào; sau đó, (1) một đơn vị tích là phân rã được nếu các con của nó có
phạm vi rời rạc; (2) một đơn vị tổng là trơn nếu các con của nó có phạm vi giống nhau. Một PC là phân rã được nếu tất cả các
đơn vị tích của nó đều phân rã được; một PC là trơn nếu tất cả các đơn vị tổng của nó đều trơn.

Tính phân rã được đảm bảo rằng mỗi đơn vị tích mã hóa một phân phối nhân tử hóa được định nghĩa rõ ràng trên
các tập biến rời rạc; tính trơn đảm bảo rằng các thành phần hỗn hợp của mỗi đơn vị tổng được
định nghĩa rõ ràng trên cùng một tập biến. Cả hai thuộc tính cấu trúc sẽ là chìa khóa để đảm bảo
hiệu quả của các thuật toán học cấu trúc được đề xuất trong các phần sau.

3 Nén Mô hình Mạch Xác Suất thông qua Cắt tỉa

Hình 1 cho thấy rằng hầu hết các tham số trong một PC lớn đều rất gần bằng không. Cho rằng các tham số này
là trọng số liên quan đến các thành phần hỗn hợp (đơn vị tổng), các cạnh và mạch con tương ứng
có ít tác động đến đầu ra của đơn vị tổng. Do đó, bằng cách cắt tỉa những thành phần không quan trọng này,
có thể giảm đáng kể kích thước mô hình trong khi vẫn giữ được khả năng biểu đạt của mô hình. Hình 3b
minh họa kết quả của việc cắt tỉa năm cạnh (màu đỏ) từ PC trong Hình 3a. Cho một PC và một bộ dữ liệu,
mục tiêu của chúng tôi là xác định hiệu quả một tập hợp các cạnh cần cắt tỉa, sao cho khoảng cách log-likelihood giữa
PC đã cắt tỉa và PC ban đầu trên bộ dữ liệu cho trước được tối thiểu hóa.

Cắt tỉa theo tham số. Thống kê giá trị tham số trong Hình 1 cho thấy rằng một tiêu chí tự nhiên
là cắt tỉa các cạnh theo độ lớn của tham số tương ứng. Điều này dẫn đến heuristic EPARAM (tham số cạnh), lựa chọn tập hợp các cạnh có tham số nhỏ nhất. Tuy nhiên, bản thân các tham số cạnh
không đủ để định lượng tầm quan trọng của các đầu vào đối với một đơn vị tổng trong
phân phối của toàn bộ PC. Các tham số của một đơn vị tổng được chuẩn hóa bằng 1 nên chúng chỉ chứa thông tin cục bộ

--- TRANG 4 ---
:114:279
:00420:4
0:6:388
:02X1B(:1) X3B(:2)
:9 :8
X1B(:7) X3B(:3):3 :7:48
:020:8
0:20:1
1:0X2B(:6)
X4B(:8):6
:8
X2B(:1):1
X4B(:2):2
(a)EPARAM loại bỏ cạnh với θ=0:1:147:346
:0140:4
0:6:48
:066X1B(:1) X3B(:2)
:9 :8
X1B(:7) X3B(:3):3 :7:48
:021:0
0:20:1
0:9X2B(:6)
X4B(:8):6
:8
X2B(:1):1
X4B(:2):2
(b)EFLOW loại bỏ cạnh với θ=0:2

Hình 4: Nghiên cứu trường hợp so sánh các heuristic cắt tỉa (EPARAM và EFLOW) trên PC trong Hình 2
cho mẫu {X1 = 0, X2 = 1, X3 = 0, X4 = 1}. Các cạnh bị cắt tỉa được biểu thị bằng đường đứt nét và các tham số được
chuẩn hóa lại. So với likelihood được tính trong Hình 2, các likelihood thay đổi được hiển thị màu đỏ,
cho thấy rằng cắt tỉa theo flows dẫn đến ít giảm likelihood hơn.

Thuật toán 1: Lấy mẫu PC
Đầu vào: một PC biểu diễn xác suất chung pC(X)
Đầu ra: một thể hiện x được lấy mẫu từ pC
1Function SAMPLE (n)
2 if n là một đơn vị đầu vào then
3fn(X) phân phối đơn biến của n; return lấy mẫu x ∼ fn(X)
4 else if n là một đơn vị tích then
5 xc ← SAMPLE (c) for each c ∈ in(n); return Concatenate ({xc}c∈in(n))
6 else n là một đơn vị tổng
7 lấy mẫu một đầu vào c tỷ lệ với {θc|n}c∈in(n); return SAMPLE (c)
8return SAMPLE (r) where r là gốc của PC C

về các thành phần hỗn hợp. Cụ thể, θc|n chỉ định nghĩa tầm quan trọng tương đối
của cạnh (n,c) trong phân phối có điều kiện được biểu diễn bởi đơn vị tổng tương ứng n, không phải
phân phối chung của toàn bộ PC. Hình 4a minh họa điều gì xảy ra khi cạnh có
tham số nhỏ nhất bị cắt tỉa từ PC trong Hình 2.

Tuy nhiên, như được thể hiện trong Hình 4b, việc cắt tỉa một cạnh khác mang lại likelihood tốt hơn vì nó tính đến nhiều hơn cho "ảnh hưởng toàn cục" của các cạnh đối với đầu ra của PC. Ảnh hưởng toàn cục này có liên quan mật thiết đến ngữ nghĩa "circuit flow" xác suất của PCs. Chúng tôi sẽ giới thiệu circuit flows sau trong phần này,
cùng với heuristic tương ứng EFLOW. Trước đó, chúng tôi trước tiên giới thiệu một khái niệm trung gian dựa trên khái niệm về tầm quan trọng tạo sinh của PCs.

Cắt tỉa theo tầm quan trọng tạo sinh. Một chiến lược cắt tỉa thông tin hơn cần xem xét tác động toàn cục của các cạnh đối với phân phối được biểu diễn bởi đầu ra của PC. Để đạt được điều này,
thay vì xem phân phối pC theo cách feedforward tuân theo Phương trình 1, chúng tôi định lượng
tầm quan trọng của một đơn vị hoặc cạnh bằng xác suất mà nó sẽ được "kích hoạt" khi rút mẫu
từ PC. Thật vậy, nếu sự hiện diện của một cạnh hầu như không bao giờ liên quan đến quá trình lấy mẫu tạo sinh, việc loại bỏ nó sẽ không
ảnh hưởng đáng kể đến phân phối của PC.

Thuật toán 1 cho thấy cách rút mẫu từ phân phối PC thông qua triển khai đệ quy:
(1) đối với một đơn vị đầu vào n được định nghĩa trên biến X (dòng 3), thuật toán ngẫu nhiên lấy mẫu giá trị x
theo phân phối đơn biến đầu vào của nó; (2) đối với một đơn vị tích (dòng 5), do tính phân rã được, các con của nó có phạm vi rời rạc, do đó chúng ta rút mẫu từ tất cả các đơn vị đầu vào và sau đó nối các
mẫu lại với nhau; (3) đối với một đơn vị tổng n (dòng 7), do tính trơn, các con của nó có phạm vi giống nhau, do đó
chúng ta đầu tiên ngẫu nhiên lấy mẫu một trong các đơn vị đầu vào của nó theo phân phối categorical được định nghĩa bởi
các tham số tổng {θc|n : c ∈ in(n)}, và sau đó lấy mẫu từ đơn vị đầu vào này một cách đệ quy. Ngoài việc thực sự
rút mẫu từ PC, chúng ta cũng có thể tính xác suất mà n sẽ được thăm trong
quá trình lấy mẫu. Điều này cung cấp một thước đo tốt về tầm quan trọng của đơn vị n đối với phân phối PC như
một tổng thể, mà chúng tôi định nghĩa là xác suất từ trên xuống.

--- TRANG 5 ---
Định nghĩa 3 (Xác Suất Từ Trên Xuống). Xác suất từ trên xuống của mỗi đơn vị n trong một PC với tham số
θ được định nghĩa đệ quy như sau, giả định các lớp tổng và tích luân phiên:

q(n; θ) := {
1 nếu n là đơn vị gốc,
∑m∈out(n) q(m; θ) nếu n là một đơn vị tổng,
∑m∈out(n) θn|m q(m; θ) nếu n là một đơn vị tích,

trong đó out(n) là các đơn vị nhận n làm đầu vào trong tính toán feedforward. Hơn nữa, xác suất từ trên xuống của một cạnh tổng (n,c) được định nghĩa là q(n,c; θ) = θc|n q(n; θ).

Xác suất từ trên xuống của gốc luôn bằng 1; một đơn vị tích chuyển xác suất từ trên xuống của nó cho
tất cả các đầu vào của nó, và một đơn vị tổng phân phối xác suất từ trên xuống của nó cho các đầu vào của nó tỷ lệ với các trọng số cạnh tương ứng. Do đó, xác suất từ trên xuống của một đơn vị không phải gốc là tổng trên
tất cả các xác suất mà nó nhận được từ các đầu ra của nó.

Xác suất từ trên xuống của tất cả các đơn vị PC và cạnh tổng có thể được tính trong một lần truyền ngược duy nhất
trên đồ thị tính toán của PC. Theo trực giác rằng xác suất từ trên xuống định nghĩa
xác suất mà các đơn vị sẽ được thăm trong quá trình lấy mẫu, việc cắt tỉa các cạnh có xác suất từ trên xuống nhỏ nhất tạo thành một chiến lược cắt tỉa hợp lý.

Cắt tỉa theo circuit flows. Xác suất từ trên xuống q(n; θ) biểu diễn xác suất đạt đến đơn vị n trong một quá trình lấy mẫu ngẫu nhiên vô điều kiện. Mặc dù có khả năng nắm bắt thông tin toàn cục
của các tham số PC, xác suất từ trên xuống không được điều chỉnh cho một bộ dữ liệu cụ thể. Do đó, để tiếp tục
sử dụng thông tin bộ dữ liệu, chúng ta có thể đo xác suất đạt đến các đơn vị/cạnh nhất định trong
quá trình lấy mẫu có điều kiện trên một thể hiện x được lấy mẫu. Để lấp đầy khoảng cách này, chúng tôi định nghĩa
circuit flow như một phiên bản phụ thuộc mẫu của xác suất từ trên xuống.

Định nghĩa 4 (Circuit Flow²). Đối với một PC cho trước với tham số θ và ví dụ x, circuit flow
của đơn vị n trên ví dụ x là xác suất mà n sẽ được thăm trong thủ tục lấy mẫu
có điều kiện trên x được lấy mẫu. Điều này có thể được tính đệ quy như sau, giả định các lớp tổng và tích luân phiên:

Fn(x) = {
1 nếu n là đơn vị gốc,
∑m∈out(n) Fm(x) nếu n là một đơn vị tổng,
∑m∈out(n) θn|m pn(x)/pm(x) Fm(x) nếu n là một đơn vị tích.

Tương tự, edge flow Fn,c(x) trên mẫu x được định nghĩa bởi Fn,c(x) = θc|n pc(x)/pn(x) Fn(x).
Chúng tôi tiếp tục định nghĩa Fn,c(D) = ∑x∈D Fn,c(x) là tổng hợp edge flow trên bộ dữ liệu D.

Hiệu quả, chúng ta có thể nghĩ về θ^x n|m := θn|m pn(x)/pm(x) như xác suất hậu nghiệm của thành phần n
trong hỗn hợp của đơn vị tổng m có điều kiện trên việc quan sát mẫu x. Sau đó, circuit flow là xác suất từ trên xuống dưới tham số hóa lại x này của mạch: Fn(x) = q(n; θ^x) và Fn,c(x) = q(n,c; θ^x).

Circuit flow Fn(x) định nghĩa xác suất đạt đến đơn vị n trong thủ tục lấy mẫu từ trên xuống của
Thuật toán 1, cho rằng thể hiện được lấy mẫu là x. Do đó, edge flow Fn,c(x) là một thước đo tự nhiên
về tầm quan trọng của cạnh (n,c) cho x. Trực quan, tổng hợp circuit flow đo lường có bao nhiêu mẫu dự kiến "chảy" qua các cạnh nhất định. Chúng tôi viết EFLOW để chỉ heuristic cắt tỉa
các cạnh có tổng hợp circuit flow nhỏ nhất.

Phân tích Thực nghiệm. Hình 5a so sánh hiệu ứng của các heuristic cắt tỉa EPARAM, EFLOW, cũng như một chiến lược không thông tin, cắt tỉa ngẫu nhiên, mà chúng tôi ký hiệu là ERAND. Nó cho thấy rằng cả EPARAM và EFLOW đều là chiến lược cắt tỉa hợp lý, tuy nhiên, khi chúng ta tăng tỷ lệ phần trăm tham số bị cắt tỉa, EFLOW có ít giảm log-likelihoods hơn so với EPARAM. Sử dụng heuristic EFLOW chúng ta có thể cắt tỉa lên đến 80% tham số mà không có nhiều giảm log-likelihoods. Như
được thể hiện trong Hình 5b, phân phối tham số cân bằng hơn sau khi cắt tỉa so với Hình 1,
cho thấy tầm quan trọng cao hơn của mỗi cạnh. Phần 6 sẽ cung cấp thêm kết quả thực nghiệm. Trước
đó, chúng tôi đầu tiên xác minh lý thuyết hiệu quả của heuristic EFLOW trong phần tiếp theo.

² Công trình trước đây đã định nghĩa "circuit flow" hoặc "expected circuit flow" trong bối cảnh học tham số [4,25,7],
mà không quan sát kết nối với lấy mẫu. Chúng tôi đóng góp ngữ nghĩa lấy mẫu trực quan hơn ở đây.

--- TRANG 6 ---
0 40 80
Pruning %120
110
100
Log-likelihoods
eRand
eFlow
eParams(a) So sánh các heuristic ERAND, EPARAM, và
EFLOW. Heuristic EFLOW có thể cắt tỉa lên đến 80%
tham số mà không có nhiều giảm loglikelihoods.
0.0 0.5 1.0
Parameter Values0.000.050.100.15PercentageInit PC
After Prune(b) Biểu đồ tần suất của các tham số trước (giống như trong
Hình 1) và sau khi cắt tỉa. Các giá trị tham số
có tầm quan trọng cao hơn sau khi cắt tỉa.

Hình 5: Đánh giá thực nghiệm của phép toán cắt tỉa.

4 Giới hạn và Xấp xỉ Mất mát Likelihood

Trong phần này, chúng tôi định lượng lý thuyết tác động của việc cắt tỉa cạnh đối với hiệu suất mô hình. Cụ thể, chúng tôi thiết lập một giới hạn trên cho sự giảm log-likelihood ΔLL trên một bộ dữ liệu D cho trước bằng cách
so sánh (i) PC ban đầu C và (ii) PC đã cắt tỉa C \ E gây ra bởi việc cắt tỉa các cạnh E:

ΔLL(D; C; E) = LL(D; C) − LL(D; C \ E).                    (2)

Chúng tôi bắt đầu từ trường hợp cắt tỉa một cạnh (tức là, |E| = 1 trong Phương trình 2). Trong trường hợp này, mất mát
likelihood có thể được định lượng chính xác sử dụng flows và tham số cạnh:

Định lý 1 (Giảm log-likelihood của việc cắt tỉa một cạnh). Đối với một PC C và một bộ dữ liệu D, mất mát
log-likelihood bằng cách cắt tỉa cạnh (n,c) là

ΔLL(D; C; {(n,c)}) = (1/|D|) ∑x∈D log(1 − θc|n/(1 − θc|n + θc|n Fn(x)/Fn,c(x)))
≥ (1/|D|) ∑x∈D log(1 − Fn,c(x)).

Xem chứng minh trong Phụ lục B.1. Bằng cách tính toán số hạng thứ hai trong Định lý 1, chúng ta có thể chọn cạnh có
giảm log-likelihood nhỏ nhất. Ngoài ra, số hạng thứ ba đặc trưng cho sự giảm log-likelihood mà không chuẩn hóa lại các tham số của jn. Nó gợi ý cắt tỉa cạnh có edge flow nhỏ nhất.

Một insight chính từ Định lý 1 là sự giảm log-likelihood phụ thuộc rõ ràng vào edge flow
Fn,c(x) và unit flow Fn(x). Điều này phù hợp với trực giác từ Phần 3 và gợi ý rằng heuristic circuit
flow được đề xuất trong phần trước là một xấp xỉ tốt của giới hạn trên được rút ra.

Tiếp theo, chúng tôi giới hạn sự giảm log-likelihood của việc cắt tỉa nhiều cạnh.

Định lý 2 (Giảm log-likelihood của việc cắt tỉa nhiều cạnh). Cho C là một PC và D là một bộ dữ liệu. Đối với
bất kỳ tập hợp cạnh E nào trong C, nếu ∀x ∈ D, ∑(n,c)∈E Fn,c(x) < 1, sự giảm log-likelihood bằng cách cắt tỉa E
được giới hạn và xấp xỉ bởi

ΔLL(D; C; E) ≤ (1/|D|) ∑x log(1 − ∑(n,c)∈E Fn,c(x)) ≈ (1/|D|) ∑(n,c)∈E Fn,c(D).    (3)

0 50 100
Pruning %104
102
100102 Log-likelihoods
ΔLL
eFlow
Hình 6: So sánh sự giảm loglikeli-
hood thực tế (ΔLL) và heuristic EFLOW
(giới hạn trên xấp xỉ trong
Phương trình 3). Giới hạn xấp xỉ khớp
gần với sự giảm loglikelihood thực tế.Chứng minh của định lý này được cung cấp trong Phụ lục B.2. Chúng tôi
đầu tiên xem xét số hạng thứ hai của Phương trình 3. Mặc dù
nó cung cấp một giới hạn trên cho sự giảm hiệu suất, nó
không thể được sử dụng như một heuristic cắt tỉa vì giới hạn
không phân rã trên các cạnh. Và do đó việc tìm
tập hợp các cạnh với điểm số thấp nhất đòi hỏi đánh giá
giới hạn một số lần theo cấp số mũ đối với
số lượng cạnh bị cắt tỉa. Do đó, chúng tôi thực hiện một bước xấp xỉ bổ sung
của giới hạn thông qua khai triển Taylor,
dẫn đến số hạng thứ ba của Phương trình 3. Xấp xỉ này
khớp với heuristic EFLOW bằng một hằng số
factor 1/|D|, điều này chứng minh lý thuyết cho hiệu quả của heuristic. Hình 6 so sánh thực nghiệm

--- TRANG 7 ---
sự giảm log-likelihood thực tế và đại lượng được tính
từ heuristic circuit flow (nghĩa là, giới hạn trên xấp xỉ) cho các tỷ lệ phần trăm khác nhau của tham số bị cắt tỉa. Chúng ta thấy rằng giới hạn xấp xỉ khớp gần
với sự giảm log-likelihood thực tế.

5 Học Cấu trúc Có thể Mở rộng

Toán tử cắt tỉa cải thiện hai khía cạnh của PCs. Đầu tiên, như được thể hiện trong Hình 5b, các tham số mô hình
cân bằng hơn sau khi cắt tỉa. Thứ hai, cắt tỉa loại bỏ các mạch con có đóng góp không đáng kể
cho phân phối của mô hình. Nếu chúng ta coi PCs như các hỗn hợp phân cấp của các thành phần, cắt tỉa có thể
được coi như một bước học cấu trúc ngầm loại bỏ các thành phần "không quan trọng" cho
mỗi hỗn hợp. Tuy nhiên, vì cắt tỉa chỉ giảm dung lượng mô hình, không thể có được một PC biểu đạt hơn PC ban đầu. Để giảm thiểu vấn đề này, chúng tôi đề xuất một phép toán phát triển để
tăng dung lượng của một PC bằng cách giới thiệu thêm các thành phần cho mỗi hỗn hợp. Cắt tỉa và phát triển
cùng nhau định nghĩa một thuật toán học cấu trúc có thể mở rộng cho PCs.

n_new c_new cn
Hình 7: Phép toán phát triển. Mỗi đơn vị
được nhân đôi, và mỗi cạnh được tham số hóa
được sao chép 3 lần: (n_new, c_new) (cam),
(n_new, c) (tím), và (n, c_new) (xanh lá).Phát triển. Phát triển là một toán tử tăng kích thước mô hình
bằng cách sao chép các thành phần hiện có và tiêm
nhiễu. Như được thể hiện trong Hình 3, sau khi áp dụng
phép toán phát triển trên PC ban đầu trong Hình 3b, chúng ta có thể có được
một PC phát triển mới như trong Hình 7. Cụ thể, phép toán phát triển
được áp dụng cho các đơn vị, cạnh, và tham số tương
ứng: (1) đối với các đơn vị, phát triển hoạt động trên mỗi đơn vị PC n
và tạo ra một bản sao khác n_new; (2) đối với các cạnh, cạnh tổng
(n,c) từ PC ban đầu (Hình 3b) được sao chép ba
lần đến PC phát triển (Hình 7): từ cha mới đến con mới (n_new, c_new), từ cha cũ đến con mới (n, c_new),
và từ cha mới đến con cũ (n_new, c); các cạnh tích
được thêm để kết nối phiên bản sao chép của một đơn vị tích
và các đầu vào sao chép của nó; (3) một tham số mới θ^new c|n là một
bản sao có nhiễu của một tham số cũ θc|n, nghĩa là θ^new c|n ← θc|n × ε trong đó ε ∼ N(1, σ²) và σ² kiểm soát
phương sai nhiễu Gaussian. Nhiễu Gaussian được thêm vào các tham số sao chép để đảm bảo rằng sau khi chúng ta
áp dụng phép toán phát triển, các thuật toán học tham số có thể
tìm ra các tham số đa dạng cho các bản sao khác nhau. Sau một phép toán phát triển, kích thước PC gấp 4 lần kích thước PC ban đầu. Thuật toán 3 trong phụ lục cho thấy triển khai feedforward của phép toán phát triển.

Học Cấu trúc thông qua Cắt tỉa và Phát triển. Các thuật toán cắt tỉa và phát triển được đề xuất có thể được áp dụng lặp đi lặp lại để tinh chỉnh cấu trúc và tham số của một PC ban đầu. Cụ thể,
vì toán tử phát triển tăng số lượng tham số PC lên một hệ số 4, việc áp dụng phát triển sau khi cắt tỉa 75% các cạnh từ một PC ban đầu giữ nguyên số lượng tham số. Chúng tôi
đề xuất một thuật toán học cấu trúc và tham số chung cho PCs sử dụng hai phép toán này.
Cụ thể, bắt đầu từ một PC ban đầu, chúng tôi áp dụng cắt tỉa 75%, phát triển, và học tham số
lặp đi lặp lại cho đến khi hội tụ. Chúng tôi sử dụng HCLTs [25] làm cấu trúc PC ban đầu vì nó có hiệu suất likelihood tối ưu. Lưu ý rằng pipeline học cấu trúc này có thể được áp dụng cho bất kỳ cấu trúc PC nào.

Ước lượng Tham số. Chúng tôi sử dụng phiên bản stochastic mini-batch của tối ưu hóa Expectation-Maximization [2]. Cụ thể, tại mỗi lần lặp, chúng tôi rút một mini-batch mẫu DB, tính toán
các circuit flows tổng hợp Fn,c(DB) và Fn(DB) của các mẫu này (bước E), và sau đó tính toán tham số mới θ^new c|n = Fn,c(DB)/Fn(DB). Các tham số sau đó được cập nhật với tỷ lệ học α:
θ^{t+1} ← αθ^new + (1−α)θ^t (bước M). Về mặt thực nghiệm, cách tiếp cận này hội tụ nhanh hơn và được điều chuẩn tốt hơn so với EM full-batch.

Tính toán Song song. Các cách tiếp cận hiện tại để mở rộng việc học và suy luận với PCs, như
Einsum networks [33], sử dụng các lớp được tham số hóa kết nối đầy đủ (Hình 3a) của các cấu trúc PC như
HCLT [25] và RatSPN [34]. Các cấu trúc này có thể dễ dàng được vector hóa để sử dụng các gói học sâu như PyTorch. Tuy nhiên, cấu trúc thưa được học bởi cắt tỉa và phát triển không

--- TRANG 8 ---
dễ dàng được vector hóa như một phép toán ma trận dày đặc. Do đó chúng tôi triển khai các kernel GPU tùy chỉnh để
song song hóa việc tính toán học tham số và suy luận dựa trên Juice.jl [8], một
gói Julia mã nguồn mở để học PCs. Các kernel phân đoạn các đơn vị PC thành các lớp sao cho các đơn vị trong mỗi
lớp độc lập. Do đó, việc tính toán có thể được song song hóa hoàn toàn trên GPU. Kết quả là, chúng tôi
có thể huấn luyện PCs với hàng triệu tham số trong vòng chưa đầy nửa giờ.

6 Thí nghiệm

Bây giờ chúng tôi đánh giá phương pháp cắt tỉa và phát triển đề xuất trên hai tập hợp các benchmark ước lượng mật độ khác nhau: (1) các bộ dữ liệu tạo sinh hình ảnh họ MNIST bao gồm MNIST [22], EMNIST [5],
và FashionMNIST [46]; (2) tác vụ mô hình hóa ngôn ngữ Penn Tree Bank ở cấp độ ký tự [27].

Phần 6.1 đầu tiên báo cáo các kết quả tốt nhất chúng tôi có được trên các bộ dữ liệu hình ảnh và các tác vụ mô hình hóa ngôn ngữ thông qua
thủ tục học cấu trúc được đề xuất trong Phần 5. Phần 6.2 sau đó cho thấy hiệu ứng của các phép toán cắt tỉa
và phát triển thông qua hai thiết lập thí nghiệm chi tiết. Nó nghiên cứu hai bài toán tối ưu hóa có ràng buộc khác nhau: tìm PC nhỏ nhất cho một likelihood cho trước thông qua nén mô hình và
tìm PC tốt nhất của một kích thước cho trước thông qua học cấu trúc.

Thiết lập. Đối với tất cả các thí nghiệm, chúng tôi sử dụng hidden Chow-Liu Trees (HCLTs) [25] với số lượng
trạng thái tiềm ẩn trong {16, 32, 64, 128} làm cấu trúc PC ban đầu. Chúng tôi huấn luyện các tham số của PCs với
stochastic mini-batch EM (cf. Phần 5). Chúng tôi thực hiện early stopping và tìm kiếm siêu tham số
sử dụng tập validation và báo cáo kết quả trên tập test. Vui lòng tham khảo Phụ lục C để biết thêm
chi tiết. Chúng tôi sử dụng mean test set bits-per-dimension (bpd) làm tiêu chí đánh giá, trong đó bpd(D; C) =
−LL(D; C)/(log(2) × m) và m là số lượng đặc trưng trong bộ dữ liệu D.

6.1 Benchmark Ước lượng Mật độ

Bộ dữ liệu Hình ảnh. Các bộ dữ liệu họ MNIST chứa các hình ảnh pixel xám kích thước 28×28 trong đó
mỗi pixel nhận giá trị trong [0, 255]. Chúng tôi tách 5% dữ liệu huấn luyện làm tập validation. Chúng tôi so sánh
với hai thuật toán học PC cạnh tranh: HCLT [25] và RatSPN [34], một mô hình dựa trên flow:
IDF [17], và ba phương pháp dựa trên VAE: BitSwap [20], BB-ANS [41], và McBits [38]. Để so sánh công bằng, chúng tôi tự triển khai các cấu trúc RatSPN và sử dụng cùng pipeline huấn luyện và
bộ tối ưu EM như phương pháp đề xuất của chúng tôi. Lưu ý rằng EinsumNet [33] cũng sử dụng cấu trúc RatSPN nhưng
với triển khai PyTorch nên việc so sánh của nó được bao gồm bởi so sánh với RatSPN. Tất cả 7
phương pháp được kiểm tra trên MNIST, 4 phần của EMNIST và FashionMNIST. Như được thể hiện trong Bảng 1, các kết quả tốt nhất được in đậm. Chúng ta thấy rằng phương pháp đề xuất của chúng tôi vượt trội đáng kể so với tất cả các baseline khác
trên tất cả các bộ dữ liệu, và thiết lập kết quả tối ưu mới giữa các mô hình PCs, flows, và VAE. Chi tiết thí nghiệm thêm có trong Phụ lục C.

Bảng 1: Hiệu suất ước lượng mật độ trên các bộ dữ liệu họ MNIST trong test set bpd.
Bộ dữ liệu Sparse PC (của chúng tôi) HCLT RatSPN IDF BitSwap BB-ANS McBits
MNIST 1.14 1.20 1.67 1.90 1.27 1.39 1.98
EMNIST(MNIST) 1.52 1.77 2.56 2.07 1.88 2.04 2.19
EMNIST(Letters) 1.58 1.80 2.73 1.95 1.84 2.26 3.12
EMNIST(Balanced) 1.60 1.82 2.78 2.15 1.96 2.23 2.88
EMNIST(ByClass) 1.54 1.85 2.72 1.98 1.87 2.23 3.14
FashionMNIST 3.27 3.34 4.29 3.47 3.28 3.66 3.72

Tác vụ Mô hình hóa Ngôn ngữ. Chúng tôi sử dụng bộ dữ liệu Penn Tree Bank với xử lý chuẩn
từ Mikolov et al. [29], chứa khoảng 5M ký tự và kích thước từ vựng cấp độ ký tự là
50. Dữ liệu được chia thành các câu với độ dài chuỗi tối đa là 288. Chúng tôi so sánh với
ba mô hình dựa trên normalizing-flow cạnh tranh: Bipartite flow [42] và latent flows [48] bao gồm
AF/SCF và IAF/SCF, vì chúng là công trình so sánh duy nhất với mô hình hóa ngôn ngữ không tự hồi quy. Như được thể hiện trong Bảng 2, phương pháp đề xuất vượt trội so với cả ba baseline.

--- TRANG 9 ---
Bảng 2: Kết quả mô hình hóa ngôn ngữ cấp độ ký tự trên Penn Tree Bank trong test set bpd.
Bộ dữ liệu Sparse PC (của chúng tôi) Bipartite flow [42] AF/SCF [48] IAF/SCF [48]
Penn Tree Bank 1.35 1.38 1.46 1.63

6.2 Đánh giá Cắt tỉa và Phát triển

PC Nhỏ nhất cho cùng Likelihood là gì? Chúng tôi đánh giá khả năng của việc cắt tỉa dựa trên
circuit flows để thực hiện nén mô hình hiệu quả bằng cách lặp đi lặp lại cắt tỉa một phần k của các tham số PC và sau đó
tinh chỉnh chúng cho đến khi log-likelihood huấn luyện cuối cùng không giảm quá 1%. Cụ thể, chúng tôi lấy tỷ lệ phần trăm cắt tỉa k từ {0.05, 0.1, 0.3}. Như được thể hiện trong Hình 8,
chúng tôi có thể đạt được tỷ lệ nén 80-98% với mất mát hiệu suất không đáng kể trên PCs. Ngoài ra, bằng cách
cố định số lượng tham số tiềm ẩn (trục x) và so sánh bpp qua các số lượng khác nhau của trạng thái tiềm ẩn (chú thích), chúng tôi khám phá ra rằng việc nén một PC lớn để có được PC nhỏ hơn mang lại likelihood tốt hơn
so với việc trực tiếp huấn luyện một HCLT với cùng số lượng tham số từ đầu. Điều này có thể
được giải thích bởi tính thưa thớt của các cấu trúc PC được nén, cũng như một cách thông minh hơn để
tìm ra các tham số tốt: học một PC tốt hơn với kích thước lớn hơn và nén nó xuống một PC nhỏ hơn.

104105106
# Parameters1.11.2Train bpd0.88
0.91
0.95
0.98mnist
105106
# Parameters1.61.80.81
0.86
0.93
0.95emnist_mnist
105106
# Parameters1.61.80.81
0.91
0.93
0.97emnist_letters
105106
# Parameters1.61.80.81
0.91
0.93
0.97emnist_balanced
104105106
# Parameters1.61.80.88
0.91
0.93
0.3emnist_byclass
104105106107
# Parameters3.23.4
0.88
0.91
0.96
0.97fashionmnist
latents
16
32
64
128

Hình 8: Nén mô hình thông qua cắt tỉa và tinh chỉnh. Chúng tôi báo cáo training set bpd (trục y) theo
số lượng tham số (trục x) cho các số lượng khác nhau của trạng thái tiềm ẩn. Đối với mỗi đường cong,
nén bắt đầu từ bên phải (PC ban đầu #Params |Cinit|) và kết thúc ở bên trái (PC nén #Params |Ccom|); tỷ lệ nén (1 - |Ccom|/|Cinit|) được chú thích bên cạnh mỗi đường cong.

PC Tốt nhất cho cùng Kích thước là gì? Chúng tôi đánh giá học cấu trúc kết hợp cắt tỉa
và phát triển như được đề xuất trong Phần 5. Bắt đầu từ một HCLT ban đầu, chúng tôi lặp đi lặp lại cắt tỉa 75% các tham số, phát triển lại, và tinh chỉnh cho đến khi đáp ứng tiêu chí dừng. Như được thể hiện trong Hình 9,
phương pháp của chúng tôi liên tục cải thiện likelihood của các PC ban đầu cho các số lượng khác nhau của trạng thái tiềm ẩn
giữa tất cả các bộ dữ liệu.

8 16 32 64 128
Latents1.11.21.3Bpd
mnist
8 16 32 64 128
Latents1.501.752.00
emnist_mnist
8 16 32 64 128
Latents1.61.82.0
emnist_letters
8 16 32 64 128
Latents1.752.00
emnist_balanced
8 16 32 64
Latents1.61.82.0
emnist_byclass
8 16 32 64 128
Latents3.23.4
fashionmnist
train
test

Hình 9: Học cấu trúc thông qua cắt tỉa 75%, phát triển và tinh chỉnh. Chúng tôi báo cáo bpd (trục y) trên cả train (đỏ) và test set (xanh lá) theo số lượng trạng thái tiềm ẩn (trục x). Đối với mỗi đường cong,
huấn luyện bắt đầu từ trên (bpd lớn) và kết thúc ở dưới (bpd nhỏ).

7 Công trình Liên quan

Cải thiện khả năng biểu đạt của PCs đã là một chủ đề trung tâm trong tài liệu. Các công trình chủ đạo
tập trung vào các thuật toán học cấu trúc lặp đi lặp lại sửa đổi cấu trúc PC để tiến bộ phù hợp với
dữ liệu [7,24,15]. Ngoài ra, một xu hướng gần đây là xây dựng PCs với cấu trúc ban đầu tốt và chỉ
thực hiện học tham số sau đó [36,1,47]. Ví dụ, EiNets [33], RAT-SPNs [34], và
XPCs [12] sử dụng các cấu trúc được tạo ngẫu nhiên, HCLTs [25] và ID-SPNs [37] định nghĩa PCs phụ thuộc
vào tương quan cặp trên các biến, và học tương quan biến trực tiếp và gián tiếp giữa
các biến. Cũng có một vài công trình tăng cường hiệu suất PC với sức mạnh biểu đạt của mạng
neural. CSPNs [39] khai thác mạng neural để học các bộ ước lượng mật độ có điều kiện biểu đạt,
và HyperSPN [40] sử dụng mạng neural để điều chuẩn PC tốt hơn.

--- TRANG 10 ---
Cắt tỉa và phát triển đã được giới thiệu trong mạng neural sâu để khai thác tính thưa thớt [16]. Các
chiến lược tương tự có thể được áp dụng trong mạch xác suất. Patil et al. [32] cắt tỉa cây quyết định theo
việc giảm độ chính xác validation. ResSPNs sử dụng giả thuyết vé số để cắt tỉa trọng số để
có được PCs compact hơn [43]. Tuy nhiên, các phương pháp cắt tỉa mạng neural và các phương pháp cắt tỉa PC ở trên
chủ yếu tập trung vào cắt tỉa theo tham số. Trái ngược với những điều này, công trình của chúng tôi phát triển các chiến lược cắt tỉa tốt hơn dựa trên các thuộc tính ngữ nghĩa của PCs.

8 Kết luận

Chúng tôi đề xuất học cấu trúc của mạch xác suất bằng cách kết hợp các phép toán cắt tỉa và phát triển để
khai thác tính thưa thớt của cấu trúc PC. Chúng tôi cho thấy các cải thiện thực nghiệm đáng kể trong các tác vụ ước lượng mật độ của PCs so với các bộ học PC hiện có và các mô hình dựa trên flow và VAEs cạnh tranh.
Tất cả các thuật toán học và suy luận Sparse-PC của chúng tôi đều có sẵn dưới dạng triển khai song song GPU.

Lời cảm ơn Công trình này được tài trợ một phần bởi Chương trình DARPA Perceptually-enabled Task
Guidance (PTG) theo hợp đồng số HR00112220005, các grant NSF #IIS-1943641, #IIS-
1956441, #CCF-1837129, Samsung, CISCO, một Sloan Fellowship, và một UCLA Samueli Fellowship.
Chúng tôi cảm ơn Honghua Zhang vì đã đọc lại và các bình luận sâu sắc về phiên bản cuối cùng của bài báo này.

Tài liệu tham khảo

[1]Tameem Adel, David Balduzzi, và Ali Ghodsi. Learning the structure of sum-product networks
via an svd-based algorithm. In Proceedings of the 31st Conference on Uncertainty in Artificial
Intelligence (UAI), 2015.

[2]YooJung Choi, Golnoosh Farnadi, Behrouz Babaki, và Guy Van den Broeck. Learning fair
naive bayes classifiers by discovering and eliminating discrimination patterns. In Proceedings
of the 34th AAAI Conference on Artificial Intelligence, 2020.

[3]YooJung Choi, Antonio Vergari, và Guy Van den Broeck. Probabilistic circuits: A unifying
framework for tractable probabilistic models. Technical report, 2020.

[4]YooJung Choi, Meihua Dang, và Guy Van den Broeck. Group fairness by probabilistic
modeling with latent fair decisions. In Proceedings of the 35th AAAI Conference on Artificial
Intelligence, 2021.

[5]Gregory Cohen, Saeed Afshar, Jonathan Tapson, và Andre Van Schaik. Emnist: Extending
mnist to handwritten letters. In 2017 International Joint Conference on Neural Networks
(IJCNN), 2017.

[6]Alvaro Correia, Robert Peharz, và Cassio P de Campos. Joints in random forests. In Advances
in Neural Information Processing Systems 33 (NeurIPS), 2020.

[7]Meihua Dang, Antonio Vergari, và Guy Van den Broeck. Strudel: Learning structured-
decomposable probabilistic circuits. In Proceedings of the 10th International Conference on
Probabilistic Graphical Models (PGM), 2020.

[8]Meihua Dang, Pasha Khosravi, Yitao Liang, Antonio Vergari, và Guy Van den Broeck. Juice:
A julia package for logic and probabilistic circuits. In Proceedings of the 35th AAAI Conference
on Artificial Intelligence (Demo Track), 2021.

[9]Adnan Darwiche. A logical approach to factoring belief networks. In Proceedings of the 8th
International Conference on Principles of Knowledge Representation and Reasoning (KR),
2002.

[10] Adnan Darwiche. A differential approach to inference in bayesian networks. Journal of the
ACM, 2003.

[11] Adnan Darwiche và Pierre Marquis. A knowledge compilation map. Journal of Artificial
Intelligence Research, 2002.

--- TRANG 11 ---
[12] Nicola Di Mauro, Gennaro Gala, Marco Iannotta, và Teresa M.A. Basile. Random probabilistic
circuits. In Proceedings of the 37th Conference on Uncertainty in Artificial Intelligence (UAI),
2021.

[13] Marc Dietrichstein, David Major, Martin Trapp, Maria Wimmer, Dimitrios Lenis, Philip Winter,
Astrid Berg, Theresa Neubauer, và Katja Bühler. Anomaly detection using generative models
and sum-product networks in mammography scans. In MICCAI Workshop on Deep Generative
Models, 2022.

[14] Laurent Dinh, David Krueger, và Yoshua Bengio. Nice: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.

[15] Robert Gens và Domingos Pedro. Learning the structure of sum-product networks. In
Proceedings of the 30th International Conference on Machine Learning (ICML), 2013.

[16] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, và Alexandra Peste. Sparsity
in deep learning: Pruning and growth for efficient inference and training in neural networks.
Journal of Machine Learning Research, 2021.

[17] Emiel Hoogeboom, Jorn Peters, Rianne Van Den Berg, và Max Welling. Integer discrete flows
and lossless compression. In Advances in Neural Information Processing Systems 32 (NeurIPS),
2019.

[18] Pasha Khosravi, YooJung Choi, Yitao Liang, Antonio Vergari, và Guy Van den Broeck. On
tractable computation of expected predictions. In Advances in Neural Information Processing
Systems 32 (NeurIPS), 2019.

[19] Diederik P Kingma và Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.

[20] Friso Kingma, Pieter Abbeel, và Jonathan Ho. Bit-swap: Recursive bits-back coding for
lossless compression with hierarchical latent variables. In Proceedings of the 36th International
Conference on Machine Learning (ICML), 2019.

[21] Doga Kisa, Guy Van den Broeck, Arthur Choi, và Adnan Darwiche. Probabilistic sentential
decision diagrams. In Proceedings of the 14th International Conference on Principles of
Knowledge Representation and Reasoning (KR), 2014.

[22] Yann LeCun, Corinna Cortes, và CJ Burges. Mnist handwritten digit database. ATT Labs
[Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.

[23] Wenzhe Li, Zhe Zeng, Antonio Vergari, và Guy Van den Broeck. Tractable computation of
expected kernels. In Proceedings of the 37th Conference on Uncertainty in Artificial Intelligence
(UAI), 2021.

[24] Yitao Liang, Jessa Bekker, và Guy Van den Broeck. Learning the structure of probabilistic
sentential decision diagrams. In Proceedings of the 33rd Conference on Uncertainty in Artificial
Intelligence (UAI), 2017.

[25] Anji Liu và Guy Van den Broeck. Tractable regularization of probabilistic circuits. In Advances
in Neural Information Processing Systems 34 (NeurIPS), 2021.

[26] Anji Liu, Stephan Mandt, và Guy Van den Broeck. Lossless compression with probabilistic
circuits. In Proceedings of the 10th International Conference on Learning Representations
(ICLR), 2022.

[27] Mitchell P. Marcus, Mary Ann Marcinkiewicz, và Beatrice Santorini. Building a large
annotated corpus of english: The penn treebank. Computational Linguistics, 1993.

[28] Radu Marinescu và Rina Dechter. And/or branch-and-bound for graphical models. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), 2005.

[29] Tomáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, và Stefan Kombrink. Subword language modeling with neural networks. preprint (http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf),
2012.

--- TRANG 12 ---
[30] Alejandro Molina, Antonio Vergari, Karl Stelzner, Robert Peharz, Pranav Subramani, Nicola
Di Mauro, Pascal Poupart, và Kristian Kersting. Spflow: An easy and extensible library for
deep probabilistic learning using sum-product networks. arXiv preprint arXiv:1901.03704,
2019.

[31] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, và Balaji
Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of
Machine Learning Research, 2021.

[32] Dipti D Patil, VM Wadhai, và JA Gokhale. Evaluation of decision tree pruning algorithms for
complexity and classification accuracy. International Journal of Computer Applications, 2010.

[33] Robert Peharz, Steven Lang, Antonio Vergari, Karl Stelzner, Alejandro Molina, Martin Trapp,
Guy Van den Broeck, Kristian Kersting, và Zoubin Ghahramani. Einsum networks: Fast and
scalable learning of tractable probabilistic circuits. In Proceedings of the 37th International
Conference on Machine Learning (ICML), 2020.

[34] Robert Peharz, Antonio Vergari, Karl Stelzner, Alejandro Molina, Xiaoting Shao, Martin Trapp,
Kristian Kersting, và Zoubin Ghahramani. Random sum-product networks: A simple and
effective approach to probabilistic deep learning. In Proceedings of the 36th Conference on
Uncertainty in Artificial Intelligence (UAI), 2020.

[35] Hoifung Poon và Pedro Domingos. Sum-product networks: A new deep architecture. In 2011
IEEE International Conference on Computer Vision Workshops (ICCV Workshops), 2011.

[36] Tahrima Rahman, Prasanna Kothalkar, và Vibhav Gogate. Cutset networks: A simple, tractable,
and scalable approach for improving the accuracy of chow-liu trees. In Joint European conference on machine learning and knowledge discovery in databases, 2014.

[37] Amirmohammad Rooshenas và Daniel Lowd. Learning sum-product networks with direct and
indirect variable interactions. In Proceedings of the 31st International Conference on Machine
Learning (ICML), 2014.

[38] Yangjun Ruan, Karen Ullrich, Daniel S Severo, James Townsend, Ashish Khisti, Arnaud Doucet,
Alireza Makhzani, và Chris Maddison. Improving lossless compression rates via monte carlo
bits-back coding. In Proceedings of the 38th International Conference on Machine Learning
(ICML), 2021.

[39] Xiaoting Shao, Alejandro Molina, Antonio Vergari, Karl Stelzner, Robert Peharz, Thomas
Liebig, và Kristian Kersting. Conditional sum-product networks: Modular probabilistic
circuits via gate functions. International Journal of Approximate Reasoning, 2022.

[40] Andy Shih, Dorsa Sadigh, và Stefano Ermon. Hyperspns: Compact and expressive probabilistic
circuits. In Advances in Neural Information Processing Systems 34 (NeurIPS), 2021.

[41] James Townsend, Thomas Bird, và David Barber. Practical lossless compression with latent
variables using bits back coding. In Proceedings of the 6th International Conference on Learning
Representations (ICLR), 2018.

[42] Dustin Tran, Keyon Vafa, Kumar Agrawal, Laurent Dinh, và Ben Poole. Discrete flows:
Invertible generative models of discrete data. In Advances in Neural Information Processing
Systems 32 (NeurIPS), 2019.

[43] Fabrizio Ventola, Karl Stelzner, Alejandro Molina, và Kristian Kersting. Residual sum-product
networks. In Proceedings of the 10th International Conference on Probabilistic Graphical
Models (PGM), 2020.

[44] Antonio Vergari, YooJung Choi, Robert Peharz, và Guy Van den Broeck. Probabilistic circuits:
Representations, inference, learning and applications. AAAI Tutorial, 2020.

[45] Antonio Vergari, YooJung Choi, Anji Liu, Stefano Teso, và Guy Van den Broeck. A compositional atlas of tractable circuit operations for probabilistic inference. In Advances in Neural
Information Processing Systems 34 (NeurIPS), 2021.

--- TRANG 13 ---
[46] Han Xiao, Kashif Rasul, và Roland Vollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.

[47] Honghua Zhang, Brendan Juba, và Guy Van den Broeck. Probabilistic generating circuits. In
Proceedings of the 38th International Conference on Machine Learning (ICML), 2021.

[48] Zachary Ziegler và Alexander Rush. Latent normalizing flows for discrete sequences. In
Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.

--- TRANG 14 ---
Danh sách kiểm tra
1. Đối với tất cả tác giả...
(a)Các tuyên bố chính được đưa ra trong tóm tắt và phần giới thiệu có phản ánh chính xác các
đóng góp và phạm vi của bài báo không? [Có] Các đóng góp được trình bày rõ ràng trong đoạn thứ ba và thứ tư
của phần giới thiệu.
(b)Bạn có mô tả các hạn chế của công trình của mình không? [Có] Các thuật toán học được đề xuất
được thiết kế cho các PC trơn và phân rã được.
(c) Bạn có thảo luận về bất kỳ tác động xã hội tiêu cực tiềm ẩn nào của công trình của mình không? [N/A]
(d)Bạn có đọc các hướng dẫn xem xét đạo đức và đảm bảo rằng bài báo của bạn tuân thủ
chúng không? [Có]

2. Nếu bạn bao gồm kết quả lý thuyết...
(a)Bạn có nêu tập hợp đầy đủ các giả định của tất cả kết quả lý thuyết không? [Có] Tất cả các định lý
trong văn bản chính chính thức nêu tất cả các giả định.
(b)Bạn có bao gồm chứng minh đầy đủ của tất cả kết quả lý thuyết không? [Có] Chứng minh được bao gồm
trong phụ lục.

3. Nếu bạn chạy thí nghiệm...
(a)Bạn có bao gồm mã, dữ liệu và hướng dẫn cần thiết để tái tạo các kết quả thí nghiệm
chính không (hoặc trong tài liệu bổ sung hoặc dưới dạng URL)? [Có] Thiết lập thí nghiệm
được chi tiết trong văn bản chính và phụ lục.
(b)Bạn có chỉ định tất cả các chi tiết huấn luyện không (ví dụ, phân chia dữ liệu, siêu tham số, cách chúng
được chọn)? [Có] Chi tiết huấn luyện được cung cấp trong phụ lục.
(c)Bạn có báo cáo thanh lỗi không (ví dụ, đối với seed ngẫu nhiên sau khi chạy thí nghiệm
nhiều lần)? [N/A]
(d)Bạn có bao gồm tổng lượng tính toán và loại tài nguyên được sử dụng không (ví dụ,
loại GPU, cluster nội bộ, hoặc nhà cung cấp cloud)? [Có] Chi tiết về tài nguyên tính toán
có thể được tìm thấy trong phụ lục.

4. Nếu bạn sử dụng tài sản hiện có (ví dụ, mã, dữ liệu, mô hình) hoặc tuyển chọn/phát hành tài sản mới...
(a)Nếu công trình của bạn sử dụng tài sản hiện có, bạn có trích dẫn người tạo ra chúng không? [Có] Chúng tôi đã trích dẫn tất cả các thuật toán học PC
cũng như các bộ dữ liệu được áp dụng.
(b)Bạn có đề cập đến giấy phép của các tài sản không? [Có] Chúng tôi đã chỉ định rằng các thuật toán được sử dụng
có sẵn công khai.
(c)Bạn có bao gồm bất kỳ tài sản mới nào trong tài liệu bổ sung hoặc dưới dạng URL không? [Có]
Chúng tôi đã bao gồm mã của chúng tôi trong tài liệu bổ sung.
(d)Bạn có thảo luận xem liệu và cách thức đồng ý được thu thập từ những người có dữ liệu bạn đang
sử dụng/tuyển chọn không? [N/A]
(e)Bạn có thảo luận xem liệu dữ liệu bạn đang sử dụng/tuyển chọn có chứa thông tin nhận dạng cá nhân
hoặc nội dung xúc phạm không? [N/A]

5. Nếu bạn sử dụng crowdsourcing hoặc tiến hành nghiên cứu với con người...
(a)Bạn có bao gồm văn bản đầy đủ của hướng dẫn đưa cho người tham gia và ảnh chụp màn hình,
nếu có không? [N/A]
(b)Bạn có mô tả bất kỳ rủi ro tiềm ẩn nào cho người tham gia, với các liên kết đến phê duyệt của Hội đồng Xem xét Tổ chức (IRB),
nếu có không? [N/A]
(c)Bạn có bao gồm mức lương theo giờ ước tính được trả cho người tham gia và tổng số tiền
chi cho việc bồi thường người tham gia không? [N/A]

--- TRANG 15 ---
A Mã giả

Trong phần này, chúng tôi liệt kê các thuật toán chi tiết cho cắt tỉa (Phần 3), phát triển (Phần 5), tính toán circuit
flows (Định nghĩa 4), và mini-batch Expectation Maximization (Phần 5).

Thuật toán 2 cho thấy cách cắt tỉa k phần trăm cạnh từ PC C theo heuristic h.

Thuật toán 2: Prune (C,h,k)
Đầu vào: một PC không xác định C, heuristic h quyết định cạnh nào cần cắt tỉa, h có thể là EFLOW,
ERAND, hoặc EPARAM, phần trăm cạnh cần cắt tỉa k
Đầu ra: một PC C' sau khi cắt tỉa
1old2new ← ánh xạ từ input PC n ∈ C đến pruned PC
2s(n,c) ← tính điểm số cho mỗi cạnh (n,c) dựa trên heuristic h
3f(n,c) ← false
4f(n,c) ← true nếu s(n,c) xếp hạng k cuối cùng
5// thăm con trước cha
6foreach n ∈ C do
7 if n là một lá then
8 old2new[n] ← n
9 else if n là một tổng then
10 old2new[n] ← ⊕([old2new(c) for c ∈ in(n) and not f(n,c)])
11 else n là một tích
12 old2new[n] ← ⊗([old2new(c) for c ∈ in(n)])
13return old2new[nr] where nr là gốc của C

Thuật toán 3 cho thấy triển khai feedforward của phép toán phát triển.

Thuật toán 3: Grow (C,σ²)
Đầu vào: một PC C, phương sai nhiễu Gaussian σ²
Đầu ra: một PC C' sau phép toán phát triển
1old2new ← từ điển ánh xạ các đơn vị input PC n ∈ C đến các đơn vị của PC đã phát triển
2foreach n ∈ C do// thăm con trước cha
3 if n là một đơn vị đầu vào then old2new[n] ← (n, deepcopy(n))
4 else
5 chs_1, chs_2 ← [old2new[c][0] for c in in(n)], [old2new[c][1] for c in in(n)]
6 if n là một đơn vị tích then old2new[n] ← (⊗(chs_1), ⊗(chs_2))
7 else if n là một đơn vị tổng then
8 n1, n2 ← ⊕([chs_1, chs_2]), ⊕([chs_1, chs_2])
9θi|ni ← normalize([θi|n, θi|n]) × N(1, σ²) for i in [1,2]
10 old2new[n] ← (n1, n2)
11return old2new[r][0] // r là đơn vị gốc của C

--- TRANG 16 ---
Thuật toán 4 tính circuit flows của một mẫu x cho PC C với tham số θ thông qua một lần truyền tiến
(dòng 1) và một lần truyền lùi (dòng 2-8).

Thuật toán 4: CircuitFlow (C,θ,x)
Đầu vào: một PC C với tham số θ; mẫu x
Đầu ra: circuit flow flow[n,c] cho mỗi cạnh (n,c) và flow[n] cho mỗi nút n
1∀n ∈ C, p[n] ← pn(x) được tính như trong Phương trình 1
2For root nr, flow[n] ← 1
3for n ∈ C in backward order do
4 flow[n] ← ∑g∈out(n) flow[g]
5 if n là một nút tổng then
6∀c ∈ in(n), flow[n,c] ← θc|n p[c]/p[n] flow[n]
7 else
8∀c ∈ in(n), flow[n,c] ← flow[n]

Thuật toán 5 cho thấy pipeline của thuật toán mini-batches Expectation Maximization cho PC C,
bộ dữ liệu D, kích thước batch B và tỷ lệ học α.

Thuật toán 5: StochasticEM (C,D;B,α)
Đầu vào: một PC C; bộ dữ liệu D; kích thước batch B; tỷ lệ học α
Đầu ra: tham số θ ước lượng từ D
1θ ← khởi tạo ngẫu nhiên
2For root nr, flow[n] ← 1
3while not converged or early stopped do
4D' ← B mẫu ngẫu nhiên từ D
5 flow ← ∑x∈D' CircuitFlow (C,θ,x)
6 for sum unit n and its child c do
7θnew c|n ← flow[n,c]/flow[n]
8θc|n ← α(θnew)c|n + (1−α)θc|n

B Chứng minh

Trong phần này, chúng tôi cung cấp chứng minh chi tiết của Định lý 1 (Phần B.1) và Định lý 2 (Phần B.2).

B.1 Cắt tỉa Một Cạnh trên Một Ví dụ

Bổ đề 1 (Giới hạn Dưới Log-Likelihood Cắt tỉa Một Cạnh). Đối với một PC C và một mẫu x, mất mát
log-likelihood bằng cách cắt tỉa cạnh (n,c) là

ΔLL({x}; C; {(n,c)}) = log(1 − θc|n/(1 − θc|n + θc|n Fn(x)/Fn,c(x)))
≥ log(1 − Fn,c(x)).

Chứng minh. Để ký hiệu đơn giản, ký hiệu xác suất của các đơn vị m (tương ứng n) trong PC ban đầu (tương ứng đã cắt tỉa) cho mẫu x là pm(x) (tương ứng p'n(x)). Như một mở rộng nhỏ của Định nghĩa 4, chúng tôi định nghĩa Fn(x; m)
là flow của đơn vị n w.r.t. PC có gốc tại m.

Chứng minh tiến hành bằng quy nạp trên đơn vị gốc của PC. Nghĩa là, chúng tôi đầu tiên xem xét việc cắt tỉa (n,c) w.r.t.
PC có gốc tại n. Sau đó, trong bước quy nạp, chúng tôi chứng minh rằng nếu bổ đề đúng cho PC có gốc tại m,
thì nó cũng đúng cho PC có gốc tại bất kỳ đơn vị cha nào của m. Thay vì chứng minh trực tiếp câu lệnh trong
Bổ đề 1, chúng tôi đầu tiên chứng minh rằng đối với bất kỳ nút gốc m nào, điều sau đây đúng:

pm(x)/p'm(x) = Fn(x;m)pm(x)/(1 − Fn,c(x;m)/Fn(x;m)) − 1     (4)

--- TRANG 17 ---
Trường hợp cơ sở: cắt tỉa một cạnh của đơn vị gốc. Nghĩa là, đơn vị gốc của PC là n. Trong trường hợp này, chúng ta có
pn(x)/p'n(x) = ∑c'∈in(n) θc'|n pc'(x) / ∑c'∈in(n)\c θ'c'|n p'c'(x)
= θc|n pc(x) + ∑c'∈in(n)\c θc'|n pc'(x) / ∑c'∈in(n)\c θ'c'|n pc'(x),     (5)

trong đó θ'c|n ký hiệu tham số chuẩn hóa tương ứng với cạnh (n,c) trong PC đã cắt tỉa. Cụ thể, chúng ta có
∀m ∈ in(n)\c, θ'm|n = θm|n / ∑c'∈in(n)\c θc'|n = θm|n/(1−θc|n).

Để ký hiệu đơn giản, ký hiệu β := θc|n. Thế định nghĩa trên vào Phương trình 5, chúng ta có
pn(x)/p'n(x) = θc|n pc(x) + ∑c'∈in(n)\c θc'|n pc'(x) / (1/(1−β)) ∑c'∈in(n)\c θc'|n pc'(x)
= θc|n pc(x) / (1−β) − ∑c'∈in(n)\c θc'|n pc'(x)
= θc|n pc(x) / (1−β) − (pn(x) − θc|n pc(x))
= 1 / (1−β) − θc|n pc(x) / (1−β) − pn(x)
(a)= 1 / (1−β) − pn(x)Fn,c(x;n) / (1−β) − Fn(x;n) pn(x)
= Fn(x;n)pn(x) / (1 − Fn,c(x;n)/Fn(x;n)) − 1,     (6)

trong đó (a) theo từ thực tế rằng Fn(x;n) = 1 và Fn,c(x;n) = θc|n pc(x)/pn(x).

Trường hợp quy nạp #1: giả sử Phương trình 4 đúng cho m. Nếu đơn vị tích d là cha của m, chúng tôi cho thấy rằng
Phương trình 4 cũng đúng cho d:
pd(x)/p'd(x) = ∏n'∈in(d) pn'(x) / ∏n'∈in(d) p'n'(x)
= (pm(x)/p'm(x)) ∏n'∈in(d)\m pn'(x)
(a)= Fn(x;m)pm(x) / (1 − Fn,c(x;m)/Fn(x;m)) − 1 ∏n'∈in(d)\m pn'(x)
(b)= Fn(x;d)pd(x) / (1 − Fn,c(x;d)/Fn(x;d)) − 1,

trong đó (a) là bước quy nạp áp dụng Phương trình 6; (b) theo từ thực tế rằng (lưu ý rằng d là một đơn vị tích) Fn(x;m) = Fn(x;d) và Fn,c(x;m) = Fn,c(x;d).

Trường hợp quy nạp #2: đối với đơn vị tổng d, giả sử Phương trình 4 đúng cho m, trong đó m ∈ A iff m ∈ in(d) và
m là tổ tiên của n và c. Giả sử tất cả các con khác của d không phải là tổ tiên của n, chúng tôi cho thấy rằng
Phương trình 4 cũng đúng cho d:
pd(x)/p'd(x) = θm|d(pm(x)/p'm(x))
= θm|d Fn(x;m)pm(x) / (1 − Fn,c(x;m)/Fn(x;m)) − 1
= θm|d Fn(x;m)pm(x) / (1 − Fn,c(x;d)/Fn(x;d)) − 1
= θm|d Fn(x;d) ∑m'∈in(d) θm'|d pm'(x) / (θm|d pm(x)) pm(x) / (1 − Fn,c(x;d)/Fn(x;d)) − 1
= Fn(x;d) ∑m'∈in(d) θm'|d pm'(x) / (1 − Fn,c(x;d)/Fn(x;d)) − 1
= Fn(x;d)pd(x) / (1 − Fn,c(x;d)/Fn(x;d)) − 1.

Do đó, theo Phương trình 4 cho gốc r, chúng ta có
pr(x)/p'r(x) = 1 / (1 − Fn,c(x;r)/(1 − Fn(x;r)))
⟹ p'r(x)/pr(x) = 1 + (1 − Fn(x;r) − 1) / (1 − Fn,c(x;r))

Do đó, chúng ta có
ΔLL({x}; C; {(n,c)}) = log pr(x) − log p'r(x)
= 1/|D| ∑x∈D log(1 − θc|n/(1 − θc|n + θc|n Fn(x;r)/Fn,c(x;r)))
(a)≥ log(1 − Fn,c(x)),

trong đó (a) theo từ thực tế rằng Fn,c(x) ≤ Fn(x).

Định lý 1 theo trực tiếp từ Bổ đề 1 bằng cách lưu ý rằng đối với bất kỳ bộ dữ liệu D nào, ΔLL(D; C; {(n,c)}) =
1/|D| ΔLL({x}; C; {(n,c)}).

B.2 Cắt tỉa Nhiều Cạnh

Chứng minh. Tương tự như chứng minh của Bổ đề 1, chúng tôi chứng minh Định lý 2 bằng quy nạp. Khác với Bổ đề 1,
chúng tôi quy nạp một mục tiêu hơi khác:
pm(x)/p'm(x) ≤ ∑(n,c)∈E∩des(m) Fn(x;m)pm(x) / (1 − θc|n Fn,c(x;m)/(Fn(x;m)θc|n/(1−θc|n))),     (7)

trong đó des(n) là tập hợp các đơn vị con cháu của n.

Trường hợp cơ sở: trường hợp cơ sở theo trực tiếp từ chứng minh của Bổ đề 1, và dẫn đến kết luận trong
Phương trình 6.

--- TRANG 18 ---
Trường hợp quy nạp #1: giả sử đối với tất cả các con của một đơn vị tích d, Phương trình 7 đúng, chúng tôi cho thấy rằng
Phương trình 7 cũng đúng cho d:
pd(x)/p'd(x) = ∏m∈in(d) pm(x) / ∏m∈in(d) p'm(x)
= ∏m∈in(d) pm(x) / ∏m∈in(d) [pm(x) − (pm(x) − p'm(x))]
≤ ∑m∈in(d) (pm(x) − p'm(x)) / ∏m'∈in(d)\m pm'(x)
(a)≤ ∑m∈in(d) ∑(n,c)∈E∩des(m) Fn(x;d)pd(x) / (1 − θc|n Fn,c(x;m)/(Fn(x;m)θc|n/(1−θc|n)))
≤ ∑(n,c)∈E∩des(d) Fn(x;d)pd(x) / (1 − θc|n Fn,c(x;d)/(Fn(x;d)θc|n/(1−θc|n))),

trong đó (a) sử dụng định nghĩa rằng pd(x) = ∏m∈in(d) pm(x).

Trường hợp quy nạp #2: giả sử đối với tất cả các con của một đơn vị tổng d, Phương trình 7 đúng, chúng tôi cho thấy rằng Phương trình 7
cũng đúng cho d:
pd(x)/p'd(x) = ∑m∈in(d)∧(d,m)∉E θm|d pm(x)/p'm(x)
+ ∑m∈in(d)∧(d,m)∈E θm|d pm(x)/p'm(x)
(a)= ∑m∈in(d)∧(d,m)∉E θm|d pm(x)/p'm(x)
+ ∑m∈in(d)∧(d,m)∈E θm|d Fn(x;m)pm(x) / (1 − θc|n Fn,c(x;m)/(Fn(x;m)θc|n/(1−θc|n))),

trong đó (a) theo từ trường hợp cơ sở của quy nạp. Tiếp theo, chúng tôi tập trung vào số hạng đầu tiên của phương trình trên:
∑m∈in(d)∧(d,m)∉E θm|d pm(x)/p'm(x)
≤ ∑m∈in(d)∧(d,m)∉E ∑(n,c)∈E∩des(m) θm|d pm(x)/p'm(x)
≤ ∑m∈in(d)∧(d,m)∉E ∑(n,c)∈E∩des(m) θm|d Fn(x;m)pm(x) / (1 − θc|n Fn,c(x;m)/(Fn(x;m)θc|n/(1−θc|n)))
≤ ∑(n,c)∈E∩des(d) Fn(x;d)pd(x) / (1 − θc|n Fn,c(x;d)/(Fn(x;d)θc|n/(1−θc|n))),

trong đó việc rút ra bất đẳng thức cuối cùng theo từ các bước tương ứng trong chứng minh của
Bổ đề 1.

Do đó, từ Phương trình 7, chúng ta có thể kết luận rằng
ΔLL(D; C; E) ≤ 1/|D| ∑x log(1 − ∑(n,c)∈E Fn,c(x)).

Cuối cùng, chúng tôi chứng minh bước xấp xỉ trong Phương trình 3. Cho β(·) = ∑(n,c)∈E Fn,c(·) ∈ [0,1). Chúng ta có,
RHS = ∑x∈D log(1 − β(x)) = ∑x∈D ∑∞k=1 β(x)k/k (khai triển Taylor)
≈ ∑x∈D ∑∞k=1 β(x)k = ∑x∈D β(x)/(1−β(x)) ≈ 1/(1−1) ∑x∈D β(x) = 1/(1−1) ∑(n,c)∈E ∑x∈D Fn,c(x) = 1/(1−1) ∑(n,c)∈E Fn,c(D).

--- TRANG 19 ---
C Chi tiết Thí nghiệm

Đặc tả phần cứng Tất cả các thí nghiệm được thực hiện trên một máy chủ với 32 CPU, 126G Bộ nhớ,
và GPU NVIDIA RTX A5000 với 26G Bộ nhớ. Trong tất cả các thí nghiệm, chúng tôi chỉ sử dụng một GPU duy nhất trên
máy chủ.

C.1 Bộ dữ liệu

Đối với các bộ dữ liệu họ MNIST, chúng tôi chia 5% của tập huấn luyện làm tập validation để early stopping. Đối với bộ dữ liệu Penn
Tree Bank, chúng tôi tuân theo thiết lập trong Mikolov et al. [29] để chia tập huấn luyện, validation và test
set. Bảng 3 liệt kê tất cả thống kê bộ dữ liệu.

Bảng 3: Thống kê bộ dữ liệu bao gồm số lượng biến (#vars), số lượng danh mục cho mỗi
biến (#cat), và số lượng mẫu cho tập huấn luyện, validation và test (#train, #valid, #test).

Bộ dữ liệu n(#vars) k(#cat) #train #valid #test
MNIST 28×28 256 57000 3000 10000
EMNIST(MNIST) 28×28 256 57000 3000 10000
EMNIST(Letters) 28×28 256 118560 6240 20800
EMNIST(Balanced) 28×28 256 107160 5640 18800
EMNIST(ByClass) 28×28 256 663035 34897 116323
FashionMNIST 28×28 256 57000 3000 10000
Penn Tree Bank 288 50 42068 3370 3761

C.2 Học Hidden Chow-Liu Trees

Cấu trúc HCLT. Áp dụng kiến trúc PC hidden chow liu tree (HCLT) như trong Liu và Van den
Broeck [25], chúng tôi tái triển khai quá trình học để tăng tốc và sử dụng pipeline huấn luyện và điều chỉnh siêu tham số khác.

Học tham số EM Chúng tôi áp dụng thuật toán học tham số EM được giới thiệu trong Choi et al.
[4], tính toán các tham số mục tiêu cập nhật EM sử dụng circuit flows. Chúng tôi sử dụng thuật toán EM
stochastic mini-batches. Ký hiệu θnew là mục tiêu cập nhật EM được tính từ một mini-batch
mẫu, và chúng tôi cập nhật tham số mục tiêu với tỷ lệ học α: θt+1 ← αθnew + (1−α)θt.
α được annealed từng đoạn tuyến tính từ [1.0, 0.1], [0.1, 0.01], [0.01, 0.001], và mỗi đoạn được huấn luyện T
epochs.

Tìm kiếm siêu tham số. Đối với tất cả các thí nghiệm, các siêu tham số được tìm kiếm từ
•h ∈ {8, 16, 32, 64, 128, 256}, kích thước ẩn của cấu trúc HCLT;
•ε ∈ {0.0001, 0.001, 0.01, 0.1, 1.0}, hệ số làm mượn Laplace;
•B ∈ {128, 256, 512, 1024}, kích thước batch trong thuật toán EM mini-batches;
•α được annealed từng đoạn tuyến tính từ [1.0, 0.1], [0.1, 0.01], [0.01, 0.001], trong đó mỗi đoạn
được gọi là một pha EM mini-batch. Thường thuật toán sẽ bắt đầu overfit ở tập validation
và dừng ở pha thứ ba;
•T = 100, số epochs cho mỗi pha EM mini-batch.

Kích thước PC tăng theo bậc hai với kích thước ẩn h, do đó không hiệu quả khi thực hiện tìm kiếm lưới
trong toàn bộ không gian siêu tham số. Việc chúng tôi làm là đầu tiên thực hiện tìm kiếm lưới khi h = 8 hoặc
h = 16 để tìm hệ số làm mượn Laplace ε và kích thước batch B tốt nhất cho mỗi bộ dữ liệu, và sau đó
cố định ε và B để huấn luyện PC với kích thước ẩn lớn hơn h ∈ {32, 64, 128, 256}. B được điều chỉnh tốt nhất trong
{256, 512}, khác nhau cho kích thước ẩn h khác nhau, và ε được điều chỉnh tốt nhất là 0.01.

--- TRANG 20 ---
C.3 Chi tiết của Phần 6.1

Sparse PC (của chúng tôi). Cho một HCLT được học trong Phần C.2 làm PC ban đầu, chúng tôi sử dụng quá trình học cấu trúc được đề xuất trong Phần 5. Cụ thể, bắt đầu từ HCLT ban đầu, cho mỗi lần lặp, chúng tôi
(1) cắt tỉa 75% tham số PC, và (2) phát triển kích thước PC với phương sai Gaussian σ, (3) tinh chỉnh PC
sử dụng học tham số EM mini-batches với tỷ lệ học α. Chúng tôi cắt tỉa và phát triển PC lặp đi lặp lại
cho đến khi likelihood tập validation bị overfit. Các siêu tham số được tìm kiếm từ
•σ ∈ {0.1, 0.3, 0.5}, phương sai Gaussian trong phép toán phát triển;
•α, annealed từng đoạn tuyến tính từ [0.1, 0.01], [0.01, 0.001];
•T = 50, số epochs cho mỗi pha EM mini-batch;
• đối với ε và B, chúng tôi sử dụng số được điều chỉnh tốt nhất từ Phần C.2.

HCLT. Các thí nghiệm HLCT trong Bảng 1 được thực hiện theo bài báo gốc (Mã https:
//github.com/UCLA-StarAI/Tractable-PC-Regularization), khác với
pipeline học mà chúng tôi sử dụng làm PC ban đầu (Phần C.2).

SPN. Chúng tôi tái triển khai kiến trúc SPN theo Peharz et al. [34] và huấn luyện nó với
cùng pipeline mini-batch như HCLT.

IDF. Chúng tôi chạy tất cả các thí nghiệm với mã trong repo GitHub được cung cấp bởi các tác giả. Chúng tôi áp dụng
mô hình IDF với các siêu tham số sau: 8 lớp flow per level; 2 levels; densenets với
depth 6 và 512 channels; base learning rate 0.001; learning rate decay 0.999. Thuật toán áp dụng
entropy coder rANS dựa trên CPU.

BitSwap. Chúng tôi huấn luyện tất cả các mô hình sử dụng script được cung cấp bởi tác giả sau: https://github.com/
fhkingma/bitswap/blob/master/model/mnist_train.

BB-ANS. Tất cả các thí nghiệm được thực hiện sử dụng mã chính thức sau https://github.com/
bits-back/bits-back.

McBits. Tất cả các thí nghiệm được thực hiện sử dụng mã chính thức sau https://github.com/
ryoungj/mcbits.

C.4 Chi tiết của Phần 6.2

Đối với tất cả các thí nghiệm trong Phần 6.2, chúng tôi sử dụng ε và B được điều chỉnh tốt nhất từ Phần C.2 và kích thước ẩn h
trong khoảng {16, 32, 64, 128}. Đối với thí nghiệm "PC Nhỏ nhất cho cùng Likelihood là gì?",
các siêu tham số được tìm kiếm từ
•k ∈ {0.05, 0.1, 0.3}, phần trăm tham số cần cắt tỉa mỗi lần lặp;
•α, annealed từng đoạn tuyến tính từ [0.3, 0.1], [0.1, 0.01], [0.01, 0.001];
•T = 50, số epochs cho mỗi pha EM mini-batch;

Đối với thí nghiệm "PC Tốt nhất cho cùng Kích thước là gì?", chúng tôi sử dụng cùng thiết lập như trong
Phần C.3.
