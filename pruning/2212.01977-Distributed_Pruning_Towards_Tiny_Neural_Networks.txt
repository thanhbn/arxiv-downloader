# 2212.01977.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2212.01977.pdf
# File size: 863880 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Distributed Pruning Towards Tiny Neural Networks
in Federated Learning
Hong Huang1, Lan Zhang2, Chaoyue Sun3, Ruogu Fang3, Xiaoyong Yuan2, Dapeng Wu1
1City University of Hong Kong,2Michigan Technological University,3University of Florida
honghuang2000@outlook.com, lanzhang@mtu.edu, chaoyue.sun@ufl.edu
ruogu.fang@bme.ufl.edu, xyyuan@mtu.edu, dpwu@ieee.org
Abstract ‚ÄîNeural network pruning is an essential technique
for reducing the size and complexity of deep neural networks,
enabling large-scale models on devices with limited resources.
However, existing pruning approaches heavily rely on training
data for guiding the pruning strategies, making them ineffective
for federated learning over distributed and confidential datasets.
Additionally, the memory- and computation-intensive pruning
process becomes infeasible for recourse-constrained devices in
federated learning. To address these challenges, we propose
FedTiny, a distributed pruning framework for federated learn-
ing that generates specialized tiny models for memory- and
computing-constrained devices. We introduce two key modules in
FedTiny to adaptively search coarse- and finer-pruned specialized
models to fit deployment scenarios with sparse and cheap local
computation. First, an adaptive batch normalization selection
module is designed to mitigate biases in pruning caused by the
heterogeneity of local data. Second, a lightweight progressive
pruning module aims to finer prune the models under strict
memory and computational budgets, allowing the pruning policy
for each layer to be gradually determined rather than eval-
uating the overall model structure. The experimental results
demonstrate the effectiveness of FedTiny, which outperforms
state-of-the-art approaches, particularly when compressing deep
models to extremely sparse tiny models. FedTiny achieves an
accuracy improvement of 2.61% while significantly reducing the
computational cost by 95.91% and the memory footprint by
94.01% compared to state-of-the-art methods.
Index Terms ‚Äîfederated learning, neural network pruning, tiny
neural networks
I. I NTRODUCTION
Deep neural networks (DNNs) have achieved great success
in the past decade. However, the huge computational cost
and storage overhead limit the usage of DNNs on resource-
constrained devices. Neural network pruning has been a well-
known solution to improve hardware efficiency [1], [2]. The
core of neural network pruning is to remove insignificant pa-
rameters from a DNN and determine specialized subnetworks
for different hardware platforms and training tasks (defined as
deployment scenarios). To achieve better accuracy, most prun-
ing approaches rely heavily on training data to trade off model
size, efficiency, and accuracy [2]‚Äì[6], which, unfortunately,
becomes ineffective when dealing with confidential training
datasets distributed over resource-constrained devices.
Recent success in federated learning enables collaborative
training across distributed devices with confidential local
datasets [7]. Instead of uploading local data, federated learning
aggregates on-device knowledge by iteratively updating localmodel parameters at the server. While successful, federated
learning cannot determine the specialized pruned model for
participating devices without training data. To address this
issue, [8] proposed to decouple the pruning process under
federated environments, where a large-size model is first
pruned on the server and then fine-tuned on devices. However,
since most pruning algorithms require a guide from the data
distribution, without access to device-side training data, the
server-side pruning leads to significant bias in the pruned
subnetwork, especially under heterogeneous (non-iid) local
data distributions. To mitigate such bias issues, recent research
pushes pruning operations to devices [9]‚Äì[13]. As shown in
Fig. 1 left, either a full-size model or a coarse-pruned model
will be finer-pruned based on the updated importance scores
from devices. The importance scores for all parameters need to
store in memory, which is infeasible for resource-constrained
devices with limited memory budgets. Moreover, without any
interaction with the device side, the initial model through
server-side coarse pruning still suffers from the bias issue,
requiring extra efforts in later finer pruning to find the optimal
subnetwork. Such negative impact becomes more challenging
when pruning towards an extremely tiny subnetwork, as the
biased initial subnetwork can deviate significantly from the
optimal structure, resulting in poor accuracy [14].
To address the above challenges, in this paper, we develop
a novel distributed pruning framework for federated learning
named FedTiny. Depending on the deployment scenarios, i.e.,
participating hardware platforms, and training tasks, FedTiny
can obtain specialized tiny models using distributed and con-
fidential datasets on participating devices. Besides, FedTiny
allows devices with tight memory and computational budgets
to participate in the resource-intensive pruning process by
reconfiguring interactions between the server and devices. As
shown in Fig. 1 right, FedTiny introduces two key mod-
ules: the adaptive batch normalization (BN) selection module
and the progressive pruning module. To avoid the negative
impact of biased initial pruning, we introduce the adaptive
BN module to identify a specialized coarse-pruned model by
indirectly pruning at devices, where devices only evaluate the
server-side pruning. It should be mentioned that evaluating
a pruned model is much cheaper than training and pruning.
The local evaluation is feedback to the server through batch
normalization parameters. Since batch normalization layers
can effectively measure local data distribution with very fewarXiv:2212.01977v2  [cs.LG]  11 Jul 2023

--- PAGE 2 ---
‚Ä¶‚ë¢aggregate BN‚ë§select candidate‚ë°update candidates‚Äô BN‚ë£evaluate candidates‚ë•update top-Kimportance scores for parameters initial poolBN-updated poolServer‚ë¶finer pruneModule 1: Adaptive BN Selection
Module 2: Progressive Pruningcoarse-pruned modelServerDevices‚ë†coarse prunefull-sizemodel‚ë°update full-size importance scores for parameters‚ë¢finer prune
üëébiased pruned model
üëçadaptive specialized modelpruned model‚ë†coarse prunefull-sizemodel‚Ä¶coarse-pruned modelpruned model
üëédense, intensive computation
üëçsparse, cheap computationDevicesFig. 1. Overview of FedTiny for the specialized tiny model in federated learning. Left: Existing federated pruning approaches push pruning operations to
devices. Either a full-size model (solid arrow) or a coarse-pruned model (dash arrow) is finer-pruned under dense and intensive local computation, suffering
biased pruning. Right : FedTiny introduces two key modules, the adaptive batch normalization module and the progressive pruning module, to adaptively search
coarse- and finer-pruned specialized models to fit deployment scenarios with sparse and cheap local computation.
parameters [15], this module guides the initial pruning with
little computation and communication cost. Besides, contrary
to prior research using importance scores of all parameters in a
full-size model for finer pruning, the progressive pruning mod-
uleis developed to iteratively adjust the model structure with
sparse and cheap local computation. Inspired by RigL [14],
devices only rate partial model parameters ( e.g., a single layer)
at a time, where the top-K importance scores are stored locally
and uploaded to the server, significantly reducing memory,
computation, and communication cost.
To demonstrate the effectiveness of FedTiny, we evaluate
FedTiny on ResNet18 [16] and VGG11 [17] with four im-
age classification datasets (CIFAR-10, CIFAR-100, CINIC-
10, and SVHN). Extensive experimental results suggest that
FedTiny achieves much higher accuracy with a lower level of
memory and computational cost than state-of-the-art baseline
approaches. Especially in a low-density regime [18] from 10‚àí2
to10‚àí3, FedTiny gets a slight loss of accuracy, while other
baselines suffer from the sharp drop in accuracy. Moreover,
FedTiny achieves top-one accuracy of 85.23% with the 0.014√ó
FLOPs and 0.03√ómemory footprint of ResNet18 [16], which
outperforms the best baseline, which gets 82.62% accuracy
with0.34√óFLOPs and 0.51√ómemory footprint.
II. R ELATED WORK
A. Neural Network Pruning
Neural network pruning has been a well-known technique
to remove redundant parameters of a DNN for model com-
pression, which can trace back to the late 1980s [1], [19],
[20]. Most existing pruning approaches focus on the trade-
off between accuracy and sparsity in the inference stage. A
typical pruning process first calculates the importance scores
of all parameters in a well-trained DNN and then removes
parameters with lower scores. The importance scores can be
derived based on the weight magnitudes [1], [2], the first-order
Taylor expansion of the loss function [19], [21], the second-
order Taylor expansion of the loss function [5], [20], and other
variants [3], [4], [6].Another line of recent research on neural network pruning
focuses on improving the efficiency of the training stage,
which can be divided into two categories. One is pruning at
initialization, i.e., pruning the original full-size model before
training. The pruning policy can be determined by evaluating
the connection sensitivity [22], Hessian-gradient product [23],
and synaptic flow [24] of the original model. Since such
pruning does not involve the training data, the pruned model
is not specialized for the training task, resulting in biased
performance. The other category is dynamic sparse train-
ing [14], [25], [26]. The pruned model structure is iteratively
adjusted throughout the training process while maintaining the
pruned model size at the desired sparsity. However, the pruning
process is to adjust the model structure in a large search space,
requiring memory-intensive operations, which is infeasible
for resource-constrained devices. Although RigL [14] tries to
reduce memory consumption, it needs to compute gradients
for all parameters, which is computationally expensive and
may lead to straggling issues in federated learning.
B. Neural Network Pruning in Federated Learning
Federated Learning has recently gained attention as a
promising approach to address data privacy concerns in col-
laborative machine learning. FedAvg [27], one of the most
widely used methods in federated learning, utilizes locally
updated on-device models instead of raw data to achieve
private knowledge transferring. Since data is locally stored
and cannot be shared, the aforementioned pruning approaches
that rely on training data cannot be used in federated learning.
Enlighten by pruning at initialization, Xu et al. proposed
to prune the original full-size model at the server and fine-
tune at devices with their local data [8]. Existing pruning at
initialization approaches, such as SNIP [22], GraSP [23], and
SynFlow [24], can be directly converted to server-side pruning.
However, server-side pruning usually results in significantly
biased pruned models, especially for heterogeneous (non-iid)
local data distributions.

--- PAGE 3 ---
To mitigate such bias, recent research pushes pruning oper-
ations under federated settings to devices. By locally training a
full-size model, SCBF [9] dynamically discards the unimpor-
tant channels on devices. Such local training with a full-size
model is assigned to a part of devices in FedPrune to guide
pruning based on the updated activations [11]. Besides, Lot-
teryFL [10] iteratively prunes a full-size model on devices with
a fixed pruning rate to find a personalized local subnetwork.
However, the above research suffers from large memory and
computational cost on the device side because devices need
to locally compute the importance scores of all parameters.
Although PruneFL [13] reduces the local computational cost
by finer pruning a coarse-pruned model rather than a full-
size model, it still requires a large local memory footprint
to record the updated importance scores of all parameters
in the full-size model. ZeroFL [28] partitions weights into
active weights and non-active weights in the inference and
sparsified weights and activations for backward propagation.
However, this approach still needs a large memory space
because the non-active weights and the gradients generated
through the training process are still stored in a dense fashion.
FedDST [29] deploys the mask adjustment on the devices, and
the server generates a new global model via sparse aggregation
and magnitude pruning. It needs much more computation cost
because it needs extra training epochs to recover the growing
weights before uploading, which may lead to straggling issues
in federated learning. The coarse-pruned model still suffers
from bias issues in the server-side pruning. Existing federated
neural network pruning fails to obtain a specialized tiny
model without bias and memory-/compute-budget concerns.
Therefore, we develop FedTiny to achieve this.
C. Federated Learning With Non-iid Data
Federated learning suffers from divergence when the data
distributions across devices are heterogeneous (non-iid) [30].
Several works have been proposed to address non-iid chal-
lenges, e.g., MATCHA [31], FedProx [7], and FedNova [32].
These works provide the convergence guarantees of federated
learning under strong assumptions, which becomes impractical
in real-world scenarios.
Data augmentation ( e.g., Astraea [33], FedGS [34], and
CSFedAvg [35]) and personalization methods ( e.g., meta learn-
ing [36], multi-task learning [37], and knowledge distilla-
tion [38]) are two promising approaches to address non-iid
issues. However, these methods are computationally intensive
and become infeasible in resource-constrained scenarios. In
our work, we develop a novel distributed pruning approach
with adaptive batch normalization selection to find an unbiased
coarse-pruned model for addressing the non-iid challenges in
resource-constrained devices.
III. P ROPOSED FEDTINY
This section introduces the proposed FedTiny. We first
describe the problem statement, followed by our design prin-
ciples. Accordingly, we present two key modules in FedTiny:the adaptive BN selection module and the progressive pruning
module.
A. Problem Statement
We consider a typical federated learning setting, where
Kdevices collaboratively train a neural network with their
corresponding local datasets Dk, k‚àà {1,2, . . . , K }. All de-
vices have limited memory and computing resources. Given
a large neural network with dense parameters Œò, we aim
to find a specialized subnetwork with sparse parameters Œ∏
and mask mon dense parameters to achieve the optimal
prediction performance for federated learning. The sparse
parameters are derived by applying a mask to the dense
parameters: Œ∏=Œò‚äôm(m‚àà {0,1}|Œò|). During training,
density dof sparse mask mcannot exceed target density
dtarget .dtarget is determined by the limitation of devices‚Äô
memory resources. We formulate the problem as a constrained
optimization problem:
min
Œ∏,mKX
k=1L(Œ∏,m,Dk),
s.t.d‚â§dtarget(1)
where L(Œ∏,m,Dk)denotes the loss function for local dataset
Dkon the k-th device.
B. Design Principles
As shown in Fig. 1 left, existing federated neural network
pruning faces two main challenges, bias in coarse pruning and
intensive memory consumption in finer pruning. To address
these challenges, we propose a FedTiny. The overview of
FedTiny is illustrated in Fig. 1 right, which consists of two
key modules: the adaptive BN selection module and the
progressive pruning module.
The adaptive batch normalization selection module (Steps
2-5 in Fig. 1 right) aims to derive an adaptive coarse-pruned
structure on the server and alleviate bias in the coarse pruning
due to unseen heterogeneous data over devices. In this mod-
ule, devices first collaboratively update batch normalization
measurements for all candidate models from coarse pruning.
Then the server selects one less biased candidate model as the
initial coarse-pruned model based on device evaluations.
The progressive pruning module (Steps 6-7 in Fig. 1 right)
further improves the coarse-pruned model by finer pruning
at resource-constrained devices, significantly reducing the on-
device memory footprint and computational cost. In this
module, the devices only maintain the top-K importance scores
of the pruned parameters. Based on the average importance
scores, the server grows and prunes parameters to produce a
new model structure. After iterative growing and pruning, the
model structure progressively approaches the optimal struc-
ture.
In the following, we provide detailed descriptions of the
adaptive batch normalization selection module and the pro-
gressive pruning module, respectively.

--- PAGE 4 ---
C. Adaptive Batch Normalization Selection
It is critical to address the bias issue in the coarse-pruned
model, as the highly biased pruned structure requires more
resources and time to adjust to the optimal structure, especially
in the low-density regime. One possible approach is to send
a set of pruned structure candidates to the devices and let
devices select the least biased model from the candidate pool.
We call this approach vanilla selection [39]. However, recent
research [40] shows that pruned model performance varies
before and after fine-tuning, which makes the pruned structure
candidate selected before fine-tuning not necessarily the best
one after fine-tuning. Such an issue could be exaggerated in
the federated settings as the heterogeneous data distribution
over devices may further increase the discrepancy of pruned
model performance in fine-tuning.
To address this issue, we introduce adaptive batch normal-
ization selection in FedTiny. Adaptive batch normalization
selection updates batch normalization measurements for can-
didate models before evaluation, aiming to derive a less biased
coarse-pruned structure. The algorithm of the adaptive batch
normalization selection module is illustrated in Algorithm 1.
We introduce batch normalization (BN) [15] to provide mea-
surements for data distribution across devices. Such measure-
ments provide representations of on-device data and thus guide
the pruning process. The batch normalization transformation
is calculated upon the following transformation on i-th input
xiin each batch,
ÀÜxi‚Üêxi‚àí¬µ‚àö
œÉ2+œµ, (2)
where œµis a small constant. During training, ¬µandœÉare
updated based on moving mean ¬µiand standard deviation œÉi
of the batch xi,
¬µt=Œ≥¬µt‚àí1+ (1‚àíŒ≥)¬µi, œÉ2
t=Œ≥œÉ2
t‚àí1+ (1‚àíŒ≥)œÉ2
i,(3)
where Œ≥denotes the momentum coeffcient and tis the number
of training iterations. During testing, the mean ¬µand standard
deviation œÉkeep fixed.
In the adaptive batch normalization selection module, batch
normalization measurements are updated in the forward pass
on devices before evaluation to select a less biased-coarse
pruned candidate. Specifically, after coarse pruning on full-
size parameters Œòwith different strategies, the server obtains
an initial pool consisting of Ccandidate models with their
sparse parameters Œ∏(c)and the corresponding masks m(c),
where Œ∏(c)=Œò‚äôm(c), for c‚àà {1,2, . . . , C }. For each
candidate model, we set different pruning ratios for each layer
while keeping overall density d‚â§dtarget . Devices first fetch
all candidate models. Note that the communication cost is
low due to the ultra-low network density. Then, each device
(say the k-th) samples a development dataset from local data,
ÀÜDk‚äÇ D k, freezes all parameters and updates the means ¬µ(c)
k
and standard deviations œÉ(c)
kof batch normalization layers in
thec-th candidate model. Next, the server aggregates all local
batch normalization measurements from devices to obtain newglobal batch normalization measurements for each candidate
model, i.e., forc‚àà {1,2, . . . , C },
¬µ(c)=KX
k=1|ÀÜDk|PK
k=1|ÀÜDk|¬µ(c)
k, œÉ(c)=KX
k=1|ÀÜDk|PK
k=1|ÀÜDk|œÉ(c)
k,
(4)
where |ÀÜDk|denotes the number of samples in the dataset ÀÜDk.
Algorithm 1 Adaptive batch normalization selection
Input :Ccoarse-pruned candidate models with sparse
parameters Œ∏(1), . . . ,Œ∏(C)and their corresponding masks
m(1), . . . ,m(C)on the server, Kdevices with local
development dataset ÀÜD1, . . . , ÀÜDK.
Output : the less biased coarse-pruned model with parameters
Œ∏0and its corresponding mask m0.
1:// Device-side
2:fork= 1 toKdo
3: Fetch sparse parameters Œ∏(1),Œ∏(2), . . . ,Œ∏(C)and their
corresponding masks m(1),m(2), . . . ,m(C)from the
server
4: forc= 1 toCdo
5: Calculate local batch normalization measurements
¬µ(c)
k,œÉ(c)
kby forward pass on Œ∏(c)with dataset ÀÜDk
6: end for
7: Upload ¬µ(1)
k, ¬µ(2)
k, . . . , ¬µ(C)
kandœÉ(1)
k, œÉ(2)
k, . . . , œÉ(C)
kto
the server
8:end for
9:// Server-side
10:forc= 1 toCdo
11: ¬µ(c)=PK
k=1|ÀÜDk|PK
k=1|ÀÜDk|¬µ(c)
k
12: œÉ(c)=PK
k=1|ÀÜDk|PK
k=1|ÀÜDk|œÉ(c)
k
13:end for
14:// Device-size
15:fork= 1 toKdo
16: Fetch ¬µ(1), ¬µ(2), . . . , ¬µ(C)andœÉ(1), œÉ(2), . . . , œÉ(C)from
the server
17: forc= 1 toCdo
18: ¬µ(c)
k, œÉ(c)
k‚Üê¬µ(c), œÉ(c)// Each candidate model
installs global batch normalization measurements
19: s(c)
k‚ÜêL(Œ∏(c),m(c),ÀÜDk)// Calculate the loss as
evaluation metrics
20: end for
21: Upload s(1)
k, s(2)
k, . . . , s(C)
kto the server
22:end for
23:// Server-side
24:forc= 1 toCdo
25: s(c)‚ÜêPK
k=1|ÀÜDk|PK
k=1|ÀÜDk|s(c)
k
26:end for
27:c‚àó‚Üêargminc(s(c))// Select the candidate model with
the lowest loss
28:Œ∏0,m0=Œ∏(c‚àó),m(c‚àó)
29:return Œ∏0,m0
After that, each device updates global batch normalization

--- PAGE 5 ---
measurements ¬µ(c), œÉ(c)forc-th candidate model. Considering
Eq. 1, we let devices calculate evaluation loss for each updated
candidate model with their on-device data and let the server
select the candidate model with the lowest average loss as the
coarse-pruned model.
Note that although the adaptive batch normalization selec-
tion module requires transferring the parameters, the com-
munication cost remains minimal, as only the parameters in
pruned models with ultra-low density need to be transferred.
The detailed analysis of communication costs is discussed in
Section IV-D. Additionally, batch normalization transforma-
tion is calculated as part of the forward pass at the device
without gradient calculation or updates. Therefore, the adap-
tive batch normalization selection effectively addresses the
bias of model structure without incurring significant memory
or computational overhead.
D. Progressive Pruning
Given a coarse-pruned model from the above module, we
introduce progressive pruning to further fine-prune the model
for better performance. We propose the progressive pruning
module with two improvements: 1) only the top-K importance
scores are calculated, while the remaining importance scores
are discarded to save memory space; 2) partial model parame-
ters ( e.g., a single layer) are adjusted per rounds instead of the
entire model to avoid intensive computation. FedTiny utilizes
a growing-pruning adjustment on the model structure while
maintaining the sparsity. Specifically, the server grows the
pruned parameters and prunes the same number of unpruned
parameters to adjust the model structure. Denote al
tas the
number of parameters that will be grown and pruned on
layer lat the t-th iteration. To guide growing and pruning
on the server, each device only trains the sparse model and
computes the Top- al
tgradients for pruned parameters, which
keeps the low memory footprint and computational cost in the
resource-constrained device. Furthermore, to reduce intensive
computation, FedTiny divides the model structure into several
blocks and prunes a block in one round. The progressive
pruning module is detailed in Algorithm 2.
In detail, each device (say the k-th) first downloads global
sparse model parameter Œ∏twith mask mtas their local
parameters Œ∏k
tin the t-th iteration, and applies SGD with
sparse gradients:
Œ∏k
t+1=Œ∏k
t‚àíŒ∑t‚àáL(Œ∏k
t,mt,Bk
t)‚äômt, (5)
where Œ∑tis the learning rate, Bk
tis a batch of sample from the
local dataset Dk, and‚àáL‚äômtdenotes the sparse gradients for
the sparse parameter Œ∏k
t. After Eiterations of local SGD, each
device calculates the top- al
tgradients for pruned parameters on
each layer lwith a batch of samples. We denote Àúgk,l
tas the top-
al
tgradients of pruned parameter with the largest magnitude
onk-th device:
Àúgk,l
t= TopK
gk,l
t, al
t
, (6)
where TopK( v, k)is threshold function, the elements of v
whose absolute value is less than the k-th largest absolutevalue are replaced with 0, and gk,l
tis the gradients of pruned
parameters on layer l.
Algorithm 2 Progressive pruning
Input : initial coarse-pruned parameters Œ∏0with mask m0,
Kdevices with local dataset D1, . . .DK, iteration number
t, learning rate Œ∑t, pruning number al
tfor each layer l, the
number of local iterations per round E, the number of rounds
between two pruning operation ‚àÜR, and the rounds at which
to stop pruning Rstop.
Output : a well-trained model with sparse Œ∏tand adjusted
mask mt
1:t‚Üê0
2:while do
3: // Device-side
4: fork= 1 toKdo
5: Fetch sparse parameters Œ∏tand mask mtfrom the
server
6: fori= 0 toE‚àí1do
7: Œ∏k
t+i+1‚ÜêŒ∏k
t+i‚àíŒ∑t+i‚àáL(Œ∏t+i,mt,Bk
t+i)‚äômt
8: end for
9: Upload Œ∏t+Eto the server
10: iftmod ‚àÜ RE= 0 andt‚â§ERstop then
11: foreach layer lin model do
12: Compute top- al
tgradients Àúgk,l
tusing Eq. 6 with
a memory space of O(al
t)
13: Upload Àúgk,l
tto the server
14: end for
15: end if
16: end for
17: // Server-side
18: Compute the global parameters Œ∏t+Eby averaging the
parameters from the devices
19: iftmod ‚àÜ RE= 0 andt‚â§ERstop then
20: foreach layer lin model do
21: Àúgl
t‚ÜêPK
k=1|Dk|PK
k=1|Dk|Àúgk,l
t
22: Il
grow‚Üêtheal
tpruned indices with the largest
absolute value in Àúgl
t
23: Il
drop‚Üêtheal
tunpruned indices with smallest
weight magnitude in Œ∏t+E
24: Compute the new mask ml
t+Eby adjusting ml
t
based on Il
grow andIl
drop
25: end for
26: Œ∏t+E‚ÜêŒ∏t+E‚äômt+E// Prune the model using the
updated mask
27: else
28: mt+E‚Üêmt
29: end if
30: t‚Üêt+E
31:end while
To calculate Àúgk,l
t, devices create a buffer in the memory to
storeal
tgradients. When a gradient is calculated, and the buffer
is full, if its magnitude is larger than the smallest magnitude

--- PAGE 6 ---
in the buffer, this gradient will be pushed into the buffer, and
the gradient with the smallest magnitude will be discarded.
Otherwise, this gradient will be discarded. In this manner,
devices only need O(al
t)memory space to store gradients.
Next, the server aggregates sparse parameters and gradients
to get average parameters and average gradients Àúgl
tfor each
layer l,
Àúgl
t=KX
k=1|Dk|PK
k=1|Dk|Àúgk,l
t, (7)
where |Dk|denotes the number of samples in dataset Dk.
Then, the server grows al
tpruned parameters with the largest
averaged gradients magnitude on each layer l. After that,
the server prunes al
tunpruned parameters (excluding the
parameters just grown) with the smallest magnitude on each
layer l.
According to growing and pruning, the server generates a
global model with a new model structure, and FedTiny starts
fine-tuning the new global model. FedTiny performs pruning
and fine-tuning iteratively to achieve an optimal tiny neural
network for all devices.
IV. E XPERIMENTS
In this section, we conduct comprehensive experiments on
FedTiny. Firstly, we introduce the experiment setting and
compare FedTiny with other baselines. Secondly, we conduct
the ablation study to demonstrate the effectiveness of the
adaptive batch normalization selection module and the pro-
gressive pruning module. Thirdly, we investigate the overhead
in the adaptive batch normalization module and the impact of
the pruning scheduling strategy. Fourthly, we demonstrate the
effectiveness of FedTiny on heterogeneous data distributions.
Finally, we compare the performance between FedTiny and
small model training.
A. Experimental Setup
1) Federated Learning Setting: We evaluate FedTiny on
image classification tasks with four datasets, CIFAR-10,
CIFAR-100 [41], CINIC-10 [42], and SVHN [43] datasets on
ResNet18 [16] and VGG11 [17] models. We consider K= 10
devices in total. For all datasets, we first generate various
non-iid partitions on devices from Dirichlet distribution with
Œ±= 0.5and then change Œ±in the Section IV-F, following
the setting in [44]. We train the models for 300 FL rounds on
the CIFAR-10, CIFAR-100, and CINIC-10 datasets and 200
rounds on the SVHN dataset. Each round includes 5local
epochs. The mini-batch size is set as 64.
2) FedTiny Setting: We use the following settings in
FedTiny. In the adaptive batch normalization selection module,
the server generates a candidate pool by magnitude pruning
with various layer-wise pruning rate settings. Given target
density, dtarget , the server outputs candidates in the form of
layer-wise pruning rate vectors (d1, d2, . . . , dL)forL-layer
model based on Uniform Noise strategy. We derive the density
dlfor the l-th layer by adding the target density target with
random noise el,i.e.,dl=dtarget +el. A candidate canbe added to the candidate pool only if its total density d
satisfies d‚â§dtarget . After that, server can get a candidate pool
{Œ∏(1),Œ∏(2), . . . ,Œ∏(C)}with mask {m(1),m(2), . . . ,m(C)}.
We first set the size of the candidate pool Cto 50 and
then change the candidate pool size in Section IV-D. We
set the ratio of the development dataset as 0.1, which is
used to update local batch normalization measurements on
the devices. In the progressive pruning module, we divide
ResNet18 and VGG11 into five blocks and prune one block
in each round, as shown in Fig. 2. The order in which the
server selects a block is backward, i.e., from the output layer
to the input layer. We also evaluate pruning a single layer
and pruning the entire model per round in Section IV-E. The
pruning number is set as al
t= 0.15(1 + costœÄ
RstopE)nlfor
layer lthat will be pruned at the t-th iteration, where nlis
the number of unpruned parameters in l-th layer. For layer l
that will not be pruned in the t-th iteration, al
t= 0. We do
not prune the batch normalization layer, bias, input layer, and
output layer because they affect model output directly. FedTiny
does ‚àÜR= 10 rounds of fine-tuning between two finer
pruning. When FedTiny reaches Rstop= 100 rounds, it stops
pruning and continues fine-tuning. FedTiny is implemented
upon FedML [45], an open-source machine learning platform
that enables lightweight, cross-platform, and provably secure
federated learning.
3) Baseline Setting: We involve the following baseline
approaches in the study. We include SNIP, SynFlow, and
FL-PQSU to confirm that pruning at initialization is not the
optimal design choice when the local data are invisible.1
We exclude the FL pruning approaches that are infeasible
for memory-constrained FL. For example, FedPrune [11] and
ZeroFL [28] require powerful devices to continuously process
the dense models.
‚Ä¢SNIP [22] prunes model by connection sensitivity at
initialization with a small public dataset on the server.
‚Ä¢SynFlow [24] prunes model by iteratively conserving
synaptic flow on the server before training.
‚Ä¢FL-PQSU [8] prunes model in a one-shot manner based
onl1-norm on the server before training. FL-PQSU also
includes quantization and selective update parts, but we
only use the pruning part in FL-PQSU.
‚Ä¢PruneFL [13] uses a powerful device to initially prune
the model and applies finer pruning (adaptive pruning) on
the sparse model based on full-size averaged gradients.
But all devices are resource-constrained in our setting.
Therefore, we let PruneFL get the initial pruned model
on the server with a small public dataset.
‚Ä¢LotteryFL [10] iteratively prunes dense model with a
fixed pruning rate on devices and re-initializes the pruned
model with the initial values.
‚Ä¢FedDST [29] first random prunes an initial pruned model
on the server, then it deploys the mask adjustment on
1We implement these frameworks based on their open-source implemen-
tations https://github.com/ganguli-lab/Synaptic-Flow.We choose well-known
baselines, PruneFL [13], FedDST [29] and LotteryFL [10], to compare the
performance of the FedTiny.

--- PAGE 7 ---
Fig. 2. Partition of blocks of VGG11 (top) and ResNet18 (bottom) models.
103
102
101
1000.20.40.60.8T op-1 accuracy
CIFAR-10
103
102
101
1000.20.40.60.8
SVHN
103
102
101
100
Density0.00.20.40.6T op-1 accuracy
CIFAR-100
103
102
101
100
Density0.20.40.6
CINIC-10
FL-PQSU SNIP SynFlow PruneFL FedDST FedTiny
Fig. 3. Top-1 accuracy of different pruning approaches in federated learning. We compare the proposed FedTiny with baselines on the four datasets with
different densities. FedTiny outperforms the baselines, especially in the extremely low-density regimes ( <10‚àí2).
the devices, and the server uses sparse aggregation and
magnitude pruning to obtain a new global model.
Since SNIP [22] and PruneFL [13] require some data for
coarse pruning, we assume that the server provides a public
one-shot dataset Dsfor pretraining. All baselines start with a
model pre-trained with the one-shot dataset Dson the server.
For SNIP, we apply iterative pruning instead of one-shot prun-
ing as [24] shown. Similarly, we let SynFlow prune the model
at initialization to the target density in an iterative manner. For
SNIP and SynFlow, we set 100 pruning epochs on the server at
initialization; refer to [24]. For FL-PQSU, which is originally
structured pruning, we change it to unstructured pruning since
all the other baselines are unstructured pruning frameworks.
LotteryFL [10] is designed for personalized federated learning,
so the model structures are different among devices. Since we
attempt to find an optimal structure for all devices as in Eq. 1,
we let LotteryFL iteratively prune the global model instead ofon-device models to ensure the same model structure for each
device. FedDST [29] deploys mask adjustment on devices and
fine-tunes the parameters before uploading. We let FedDST
adjust the masks after 3 epochs of local training, followed by
2 epochs of fine-tuning. Since LotteryFL, PruneFL, FedDST,
and our FedTiny are iteratively pruning during training, we
use the same pruning schedule for these frameworks, where
the framework does ‚àÜR= 10 rounds of fine-tuning between
two finer pruning. And framework stops pruning and continues
fine-tuning after Rstop = 100 rounds. For PruneFL and
FedDST, we set the pruning number al
tto be the same as in
FedTiny. All baselines will apply uniform sparsity distribution
for layer-wise pruning rate setting.
B. Comparison Between FedTiny and Baseline Approaches
In order to show the performance of FedTiny under dif-
ferent densities, we compare baselines and FedTiny on four
datasets (CIFAR-10, CIFAR-100, CINIC-10, and SVHN) with

--- PAGE 8 ---
TABLE I
TOP-1ACCURACY AND TRAINING COST OF PROPOSED FEDTINY AND OTHER BASELINES WITH VARIOUS DENSITIES AND MODELS
Density MethodResNet18 VGG11
Top-1
AccuracyMax Training
FLOPsMemory
FootprintTop-1
AccuracyMax Training
FLOPsMemory
Footprint
1 FedAvg 0.9048 1x(8.33E13) 90.91MB 0.8696 1x(4.09E13) 1033.33MB
0.01FL-PQSU 0.7038 0.014x 2.75MB 0.475 0.017x 20.96MB
SNIP 0.7245 0.014x 2.76MB 0.3481 0.017x 20.98MB
SynFlow 0.8034 0.014x 2.75MB 0.5803 0.017x 20.92MB
PruneFL 0.8262 0.34x 46.58MB 0.6204 0.34x 526.87MB
FedDST 0.7495 0.015x 2.91MB 0.6067 0.017x 21.03MB
LotteryFL 0.8083 1x 90.91MB 0.6183 1x 1033.33MB
FedTiny 0.8523 0.014x 2.79MB 0.7883 0.017x 20.95MB
0.005FL-PQSU 0.5961 0.008x 2.01MB 0.232 0.012x 11.99MB
SNIP 0.2711 0.009x 1.98MB 0.2409 0.012x 12.00MB
SynFlow 0.7206 0.008x 2.00MB 0.4376 0.012x 11.95MB
PruneFL 0.736 0.34x 46.19MB 0.4956 0.34x 522.31MB
FedDST 0.6245 0.009x 2.04MB 0.4292 0.013x 12.02MB
LotteryFL 0.7586 1x 90.91MB 0.4376 1x 1033.33MB
FedTiny 0.7972 0.009x 2.03MB 0.7534 0.012x 11.98MB
0.001FL-PQSU 0.1352 0.004x 1.22MB 0.1 0.008x 4.71MB
SNIP 0.1377 0.004x 1.19MB 0.1 0.008x 4.72MB
SynFlow 0.2862 0.004x 1.19MB 0.2531 0.008x 4.72MB
PruneFL 0.2955 0.336x 45.72MB 0.2692 0.339x 518.71MB
FedDST 0.1868 0.004x 1.20MB 0.2535 0.008x 4.73MB
LotteryFL 0.3070 1x 90.91MB 0.2634 1x 1033.33MB
FedTiny 0.6311 0.004x 1.17MB 0.5944 0.008x 4.71MB
ResNet18. As shown in Fig. 3, FedTiny outperforms the other
baselines in the low-density regime ( dtarget <10‚àí2),e.g.,
FedTiny achieves an accuracy improvement of 18.91% in
SVHN dataset compared to state-of-the-art methods with 10‚àí3
density. This benefits from the adaptive batch normalization
selection module used in FedTiny, which deduces an adaptive
coarse pruning structure on the server. This initial pruning
structure has less bias, reducing the size of the search space
and improving convergence. Besides, FedTiny is competitive
with a high density ( dtarget >10‚àí2),e.g., FedTiny outper-
forms state-of-the-art methods with 10‚àí1density by 1.3% in
CIFAR-10 dataset. Although PruneFL can partially outperform
FedTiny under high density, it requires over 20√ócomputation
cost and 15√ómemory footprint to process dense importance
scores on devices. SNIP performs badly in low density because
SNIP tends to remove nearly all parameters in some layers.
Moreover, the pruned model in SNIP highly depends on the
samples on the server, which increases bias due to non-iid.
We do not include LotteryFL in Fig. 3 as the utilization of
LotteryFL necessitates the training of a large model, which
incurs a substantial computational cost and memory footprint.
However, the results of LotteryFL are included in Table I for
the purpose of providing a comprehensive comparison with
other baselines.
To show the efficiency of FedTiny, we measure the cost
of training ResNet18 and VGG11 with various densities on
the CIFAR-10 dataset. We use the number of floating pointoperations (FLOPs) to measure the computational cost for each
device. The pruning operation requires a variable amount of
computation per round, resulting in variable training FLOPs
per round. Therefore, we report the maximum training FLOPs
per round (Max Training FLOPs).The maximum training
FLOPs per round is used to evaluate whether devices suffer
from intensive computation in a single round. We also report
the memory footprint in devices, which is related to memory
cost in deployment.
Table I shows the accuracy and training cost of proposed
FedTiny and other baselines with various densities and models.
We mark the best metric in red and the second-best metric
in blue. All cost measurements are for one device in one
pruning round. We also report the performance of FedAvg
to show the upper bound of pruning approaches. As shown in
Table I, FedTiny aims to improve both accuracy and memory
efficiency. The existing works cannot achieve satisfactory
accuracy in ultra-low density. Our proposed FedTiny signifi-
cantly improves the accuracy with the lowest levels of FLOPs
and memory footprint.
C. Ablation Study
This section discusses the effectiveness of each module in
FedTiny via ablation studies. We evaluate vanilla selection,
adaptive batch normalization selection, progressive pruning
after vanilla selection, and FedTiny on the CIFAR-10 dataset
with the VGG11 model. Fig. 4 shows the results of each
module working individually. We have the following three

--- PAGE 9 ---
103
102
101
100
Density0.30.40.50.60.70.80.9T op-1 accuracy
vanilla
adaptive BN selection
vanilla + progressive pruning
FedTinyFig. 4. Ablation studies the two key modules in FedTiny: the adaptive
batch normalization selection module and the progressive pruning module.
We compare vanilla selection, adaptive batch normalization (BN) selection,
progressive pruning with vanilla selection, and FedTiny. We test the ResNet18
model on the CIFAR-10 dataset with various densities.
findings. First, both the adaptive batch normalization selec-
tion module and progressive pruning module improve the
performance in vanilla selection, indicating the effectiveness
of these two modules. Second, a coarse-pruned model from
adaptive batch normalization selection faces a drop in accuracy
compared to FedTiny, indicating that there are still some
biases in the selected coarse-pruned model, and the progressive
pruning module can remove them. Last, the progressive prun-
ing module with vanilla selection reaches the same level of
accuracy compared to FedTiny with the high density ( <10‚àí2).
However, it suffers from severe degradation of accuracy in
the low-density regime ( >10‚àí2), which suggests that the
progressive pruning module only removes the bias to a certain
extent, and it must be combined with the adaptive batch
normalization selection module in the low-density regime.
Therefore, independently using the adaptive batch normaliza-
tion selection module and progressive pruning module can
improve performance, but the improvement is limited. The
combination of the two modules, i.e., FedTiny, achieves the
best prediction performance with the tiny model.
D. Overhead in Adaptive BN Selection Module
Although a larger candidate pool provides more choices for
selection, it brings more communication costs in the adaptive
batch normalization selection module. So, we want to find
an optimal pool size that can trade off the accuracy and
overhead in the adaptive batch normalization selection module.
Therefore, We evaluate FedTiny on VGG11 with different pool
sizes to find an optimal pool size. We do the experiments
on CIFAR-10 datasets with the VGG11 model with different
pool sizes and densities. The experiment results are shown in
Fig. 5. The result shows increasing the pool size beyond the
green line may only yield a marginal increase in accuracy,
while significantly increasing computational costs. Therefore,
the green line serves as a practical threshold for selecting the
optimal candidate pool size. Therefore, the optimal pool size is
selected as C‚àó=0.1
dtargetfor specific density dtarget , where the
communication cost in adaptive batch normalization selection
module is as low as 20% to a full-size VGG11 model, andTABLE II
EXTRA FLOP S IN THE ADAPTIVE BN SELECTION MODEL
Density Pool SizeExtra FLOPs
in selectionTraining FLOPs
in one round
0.01 10 9.15E+10 6.86E+11
0.005 20 1.3E+11 4.92E+11
0.001 100 3.42E+11 3.56E+11
TABLE III
TOP-1ACCURACY FOR FEDTINY WITH DIFFERENT PRUNING SCHEDULING
STRATEGIES
Granularity ‚àÜR/RstopDensity
0.01Density
0.005Density
0.001
Layer 5/100 0.7623 0.7034 0.447
Layer ( b) 5/100 0.7894 0.7343 0.5871
Block 10/100 0.7697 0.7179 0.5721
Block ( b) 10/100 0.7883 0.7534 0.6311
Block ( b) 5/50 0.7675 0.7263 0.6113
Entire 50/100 0.772 0.7395 0.6244
Entire 25/50 0.7583 0.7043 0.5944
FedTiny can receive a relatively good accuracy. A larger pool
size> C‚àóslightly improves accuracy but incurs much higher
communication costs.
We also calculate the extra FLOPs for the adaptive batch
normalization selection module with optimal pool size, as
shown in Table II. The extra FLOPs in adaptive batch nor-
malization selection are less than one round of sparse training.
Since federated learning usually involves more than one hun-
dred rounds of training, the extra computational overhead is
neglectable. Therefore, we argue that the overhead introduced
by adaptive batch normalization selection is marginal.
E. Impact of Pruning Scheduling Strategy
Although layer-wise adjustment in progressive pruning re-
duces the computation cost in one round, it may slow down the
convergence speed. To determine the best pruning granularity
and pruning frequency, we evaluate FedTiny on VGG11 with
different pruning granularities (one layer per round, one block
per round, and the entire model per round) and different
pruning frequencies.
Table III shows the top-1 accuracy of various pruning sched-
ules under different densities on VGG11 with the CIFAR-10
dataset, where the best performance of Top-1 accuracy with the
same density is represented in red, and the second-best metric
is marked in blue. bdenotes sequentially choosing layers or
blocks to prune in backward order, i.e.,from the output layer to
the input layer. We control the pruning frequencies by setting
different interval rounds ‚àÜRbetween two pruning. If the
pruning granularity is too small ( e.g., layer-wise pruning), the
model structure will converge slowly, and the optimal structure
cannot be achieved with limited training resources. But high
updating granularity leads to more intensive computation in
one round. We find that pruning a block per round is an
optimal choice for the progressive pruning module. Moreover,
sequentially choosing blocks to prune in backward order (from
the output layer to the input layer) gets better results than

--- PAGE 10 ---
0.0 0.1 0.2 0.3 0.4 0.5
Density * Pool size0.450.500.550.600.650.700.750.80T op-1 accuracy
dtarget=0.01
dtarget=0.005
dtarget=0.001
0.0 0.1 0.2 0.3 0.4 0.5
Density * Pool size0100200300400500Communication cost(MB)
dtarget=0.01
dtarget=0.005
dtarget=0.001Fig. 5. The performance and cost for sparse VGG11 models with different densities and pool sizes. Left: The effect of pool size on top-1 accuracy under
different densities. Right : The effect of pool size on communication cost in the adaptive batch normalization selection module under different densities. The
gray dash line is the size for a full-size VGG11 model. The green dash line is the optimal candidate pool size for specific density.
forwarding order since the gradient propagation is backward,
and we use gradients to adjust the model structure.
F . Effectiveness of FedTiny Over Heterogeneous Data Distri-
butions
Neural network pruning requires training data to determine
the proper model structure. Due to resource-constrained de-
vices, the server cannot push the dense model to devices.
Therefore, the server needs to coarsely prune to produce the
initial pruned model. Due to privacy concerns in federated
learning, the server cannot know the data distributions for
all devices. In the existing methods, the server only coarsely
prunes the model based on the pre-trained dataset or the data
from some trusted devices. It makes the dataset used for
pruning different from the dataset used for fine-tuning, which
causes bias in coarse pruning. Therefore, our strategy is to use
adaptive batch normalization selection to select one pruned
model with less bias.
To demonstrate the effectiveness of FedTiny over hetero-
geneous data distributions, we set different non-iid degrees
by using different Œ±in the Dirichlet distribution. Lower Œ±
indicates a higher non-iid degree. We do experiments on
the CIFAR-10 dataset with ResNet18 with 1% density. The
experiments are shown in Fig. 6. Our experiments show that
1) the performance of the existing pruning methods ( e.g.,
SynFlow, PruneFL) in Federated Learning will be significantly
degraded given a higher non-iid degree; 2) Our proposed
FedTiny mitigates the bias in pruning and achieves the best
performance compared with the existing pruning methods.
G. Comparison Between FedTiny And Small Model Training
In the previous experiments, we compare the FedTiny with
existing pruning methods. To further investigate the effec-
tiveness of FedTiny, we compare FedTiny with the dense
small models without pruning. FedTiny outperforms other
baselines in the very sparse model, like 1% density. In
this case, training a dense small model without pruning can
also be considered as a baseline. Therefore, we design the
experiments on small models. We train a small model with
three convolutional layers. First, we evaluate the small model
with a similar number of parameters to ResNet18 with 1%
100 3√ó101
4√ó101
6√ó101
0.780.800.820.840.86T op-1 accuracy
SynFlow
PruneFL
FedTinyFig. 6. Top-1 accuracy of different pruning approaches on various non-iid
degrees. Lower Œ±indicates a higher non-iid degree.
TABLE IV
TOP-1ACCURACY FOR RESNET18WITH 1% DENSITY AND A SMALL
MODEL
Method CIFAR-10 CINIC-10 SVHN CIFAR-100
SynFlow 0.8034 0.6057 0.8683 0.4413
PruneFL 0.8262 0.6379 0.8927 0.4373
Small Model 0.8019 0.5578 0.8395 0.4277
FedTiny 0.8523 0.6712 0.8826 0.4865
density on different datasets. Second, we evaluate the small
network with a similar number of parameters to ResNet18 with
different densities on CIFAR-10. We also choose SynFlow
and PruneFL as references. The experiment result is shown in
Table IV and Table V, where the best performance of Top-1
accuracy with the same dataset and same density is represented
in red, and the second-best metric is marked in blue. The
experimental results show that the small model is competitive
compared to other baselines. However, our proposed FedTiny
achieves much better performance than the small model, which
demonstrates the advantage of FedTiny.
TABLE V
TOP-1ACCURACY FOR RESNET18WITH VARIOUS DENSITIES AND SMALL
MODELS ON THE CIFAR-10 DATASET
Method 0.01 0.005 0.003 0.001
SynFlow 0.8034 0.7206 0.6279 0.2862
PruneFL 0.8262 0.7360 0.6453 0.2955
Small Model 0.8019 0.7201 0.6921 0.6158
FedTiny 0.8523 0.7972 0.7572 0.6311

--- PAGE 11 ---
V. C ONCLUSION
This paper develops a novel distributed pruning framework
called FedTiny. FedTiny enables memory-efficient local train-
ing and determines specialized tiny models in federated learn-
ing for different deployment scenarios (participating hardware
platforms and training tasks). FedTiny addresses the challenges
of bias, intensive computation, and memory usage that existing
federated pruning research suffers. FedTiny introduces two
critical modules: an adaptive batch normalization selection
module and a lightweight progressive pruning module. The
batch normalization selection module is designed to mitigate
the bias in pruning caused by the heterogeneity of local data,
while the progressive pruning module enables fine-grained
pruning under strict computational and memory budgets.
Specifically, it gradually determines the pruning policy for
each layer rather than evaluating the overall model structure.
Experimental results demonstrate the effectiveness of FedTiny
when compared to state-of-the-art approaches. In particular,
FedTiny achieves significant improvements in terms of accu-
racy, FLOPs, and memory footprint when compressing deep
models to extremely sparse tiny models. The results on the
CIFAR-10 dataset show that FedTiny outperforms state-of-the-
art methods by achieving an accuracy improvement of 2.61%
while simultaneously reducing FLOPs by 95.9% and memory
footprint by 94.0%. The experimental results demonstrate the
effectiveness and efficiency of FedTiny in federated learning
settings.
VI. A CKNOWLEDGEMENT
This work is supported in part by the National Science
Foundation (CCF-2221741, CCF-2106754, CNS-2151238,
CNS-2153381), the ORAU Ralph E. Powe Junior Faculty En-
hancement Award, and the Hong Kong Research Grants Coun-
cil, General Research Fund (GRF) under Grant 11203523.
REFERENCES
[1] S. A. Janowsky, ‚ÄúPruning versus clipping in neural networks,‚Äù Physical
Review A , vol. 39, no. 12, p. 6600, 1989.
[2] S. Han, H. Mao, and W. J. Dally, ‚ÄúDeep compression: Compressing
deep neural networks with pruning, trained quantization and huffman
coding,‚Äù arXiv preprint arXiv:1510.00149 , 2015.
[3] C. Louizos, M. Welling, and D. P. Kingma, ‚ÄúLearning sparse neural
networks through l0regularization,‚Äù in International Conference on
Learning Representations , 2018.
[4] R. Yu, A. Li, C.-F. Chen, J.-H. Lai, V . I. Morariu, X. Han, M. Gao, C.-Y .
Lin, and L. S. Davis, ‚ÄúNisp: Pruning networks using neuron importance
score propagation,‚Äù in Proceedings of the IEEE conference on computer
vision and pattern recognition , pp. 9194‚Äì9203, 2018.
[5] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, ‚ÄúImportance
estimation for neural network pruning,‚Äù in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 11264‚Äì
11272, 2019.
[6] S. P. Singh and D. Alistarh, ‚ÄúWoodfisher: Efficient second-order ap-
proximation for neural network compression,‚Äù Advances in Neural
Information Processing Systems , vol. 33, pp. 18098‚Äì18109, 2020.
[7] T. Li, A. K. Sahu, A. Talwalkar, and V . Smith, ‚ÄúFederated learning:
Challenges, methods, and future directions,‚Äù IEEE Signal Processing
Magazine , vol. 37, no. 3, pp. 50‚Äì60, 2020.
[8] W. Xu, W. Fang, Y . Ding, M. Zou, and N. Xiong, ‚ÄúAccelerating federated
learning for iot in big data analytics with pruning, quantization and
selective updating,‚Äù IEEE Access , vol. 9, pp. 38457‚Äì38466, 2021.[9] R. Shao, H. Liu, and D. Liu, ‚ÄúPrivacy preserving stochastic channel-
based federated learning with neural network pruning,‚Äù arXiv preprint
arXiv:1910.02115 , 2019.
[10] A. Li, J. Sun, B. Wang, L. Duan, S. Li, Y . Chen, and H. Li, ‚ÄúLotteryfl:
Empower edge intelligence with personalized and communication-
efficient federated learning,‚Äù in 2021 IEEE/ACM Symposium on Edge
Computing (SEC) , pp. 68‚Äì79, IEEE, 2021.
[11] M. T. Munir, M. M. Saeed, M. Ali, Z. A. Qazi, and I. A.
Qazi, ‚ÄúFedprune: Towards inclusive federated learning,‚Äù arXiv preprint
arXiv:2110.14205 , 2021.
[12] S. Liu, G. Yu, R. Yin, and J. Yuan, ‚ÄúAdaptive network pruning for
wireless federated learning,‚Äù IEEE Wireless Communications Letters ,
vol. 10, no. 7, pp. 1572‚Äì1576, 2021.
[13] Y . Jiang, S. Wang, V . Valls, B. J. Ko, W.-H. Lee, K. K. Leung, and
L. Tassiulas, ‚ÄúModel pruning enables efficient federated learning on edge
devices,‚Äù IEEE Transactions on Neural Networks and Learning Systems ,
2022.
[14] U. Evci, T. Gale, J. Menick, P. S. Castro, and E. Elsen, ‚ÄúRigging the
lottery: Making all tickets winners,‚Äù in International Conference on
Machine Learning , pp. 2943‚Äì2952, PMLR, 2020.
[15] S. Ioffe and C. Szegedy, ‚ÄúBatch normalization: Accelerating deep
network training by reducing internal covariate shift,‚Äù in International
conference on machine learning , pp. 448‚Äì456, PMLR, 2015.
[16] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in Proceedings of the IEEE conference on computer vision
and pattern recognition , pp. 770‚Äì778, 2016.
[17] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for
large-scale image recognition,‚Äù arXiv preprint arXiv:1409.1556 , 2014.
[18] T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste, ‚ÄúSparsity
in deep learning: Pruning and growth for efficient inference and training
in neural networks,‚Äù Journal of Machine Learning Research , vol. 22,
no. 241, pp. 1‚Äì124, 2021.
[19] M. C. Mozer and P. Smolensky, ‚ÄúSkeletonization: A technique for
trimming the fat from a network via relevance assessment,‚Äù Advances
in neural information processing systems , vol. 1, 1988.
[20] Y . LeCun, J. Denker, and S. Solla, ‚ÄúOptimal brain damage,‚Äù Advances
in neural information processing systems , vol. 2, 1989.
[21] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, ‚ÄúPruning
convolutional neural networks for resource efficient inference,‚Äù in 5th
International Conference on Learning Representations, ICLR 2017-
Conference Track Proceedings , 2019.
[22] N. Lee, T. Ajanthan, and P. Torr, ‚ÄúSnip: Single-shot network pruning
based on connection sensitivity,‚Äù in International Conference on Learn-
ing Representations , 2018.
[23] C. Wang, G. Zhang, and R. Grosse, ‚ÄúPicking winning tickets before
training by preserving gradient flow,‚Äù in International Conference on
Learning Representations , 2019.
[24] H. Tanaka, D. Kunin, D. L. Yamins, and S. Ganguli, ‚ÄúPruning neural
networks without any data by iteratively conserving synaptic flow,‚Äù
Advances in Neural Information Processing Systems , vol. 33, pp. 6377‚Äì
6389, 2020.
[25] D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu, and
A. Liotta, ‚ÄúScalable training of artificial neural networks with adaptive
sparse connectivity inspired by network science,‚Äù Nature communica-
tions , vol. 9, no. 1, pp. 1‚Äì12, 2018.
[26] T. Dettmers and L. Zettlemoyer, ‚ÄúSparse networks from scratch: Faster
training without losing performance,‚Äù arXiv preprint arXiv:1907.04840 ,
2019.
[27] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
‚ÄúCommunication-efficient learning of deep networks from decentralized
data,‚Äù in Artificial intelligence and statistics , pp. 1273‚Äì1282, PMLR,
2017.
[28] X. Qiu, J. Fernandez-Marques, P. P. Gusmao, Y . Gao, T. Parcollet, and
N. D. Lane, ‚ÄúZerofl: Efficient on-device training for federated learning
with local sparsity,‚Äù arXiv preprint arXiv:2208.02507 , 2022.
[29] S. Bibikar, H. Vikalo, Z. Wang, and X. Chen, ‚ÄúFederated dynamic sparse
training: Computing less, communicating less, yet learning better,‚Äù in
Proceedings of the AAAI Conference on Artificial Intelligence , vol. 36,
pp. 6080‚Äì6088, 2022.
[30] Y . Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V . Chandra, ‚ÄúFederated
learning with non-iid data,‚Äù arXiv preprint arXiv:1806.00582 , 2018.
[31] O. Marfoq, C. Xu, G. Neglia, and R. Vidal, ‚ÄúThroughput-optimal
topology design for cross-silo federated learning,‚Äù Advances in Neural
Information Processing Systems , vol. 33, pp. 19478‚Äì19487, 2020.

--- PAGE 12 ---
[32] J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V . Poor, ‚ÄúTackling the ob-
jective inconsistency problem in heterogeneous federated optimization,‚Äù
Advances in neural information processing systems , vol. 33, pp. 7611‚Äì
7623, 2020.
[33] M. Duan, D. Liu, X. Chen, Y . Tan, J. Ren, L. Qiao, and L. Liang,
‚ÄúAstraea: Self-balancing federated learning for improving classification
accuracy of mobile deep learning applications,‚Äù in 2019 IEEE 37th
international conference on computer design (ICCD) , pp. 246‚Äì254,
IEEE, 2019.
[34] Z. Li, Y . He, H. Yu, J. Kang, X. Li, Z. Xu, and D. Niyato, ‚ÄúData
heterogeneity-robust federated learning via group client selection in
industrial iot,‚Äù IEEE Internet of Things Journal , 2022.
[35] W. Zhang, X. Wang, P. Zhou, W. Wu, and X. Zhang, ‚ÄúClient selection for
federated learning with non-iid data in mobile edge computing,‚Äù IEEE
Access , vol. 9, pp. 24462‚Äì24474, 2021.
[36] F. Chen, M. Luo, Z. Dong, Z. Li, and X. He, ‚ÄúFederated meta-learning
with fast convergence and efficient communication,‚Äù arXiv preprint
arXiv:1802.07876 , 2018.
[37] V . Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, ‚ÄúFederated
multi-task learning,‚Äù Advances in neural information processing systems ,
vol. 30, 2017.
[38] T. Lin, L. Kong, S. U. Stich, and M. Jaggi, ‚ÄúEnsemble distillation
for robust model fusion in federated learning,‚Äù Advances in Neural
Information Processing Systems , vol. 33, pp. 2351‚Äì2363, 2020.
[39] Y . He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han, ‚ÄúAmc: Automl for
model compression and acceleration on mobile devices,‚Äù in Proceedings
of the European conference on computer vision (ECCV) , pp. 784‚Äì800,
2018.
[40] B. Li, B. Wu, J. Su, and G. Wang, ‚ÄúEagleeye: Fast sub-net evaluation for
efficient neural network pruning,‚Äù in European conference on computer
vision , pp. 639‚Äì654, Springer, 2020.
[41] A. Krizhevsky, G. Hinton, et al. , ‚ÄúLearning multiple layers of features
from tiny images,‚Äù 2009.
[42] L. N. Darlow, E. J. Crowley, A. Antoniou, and A. J. Storkey, ‚ÄúCinic-10
is not imagenet or cifar-10,‚Äù arXiv preprint arXiv:1810.03505 , 2018.
[43] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y . Ng,
‚ÄúReading digits in natural images with unsupervised feature learning,‚Äù
2011.
[44] M. Luo, F. Chen, D. Hu, Y . Zhang, J. Liang, and J. Feng, ‚ÄúNo fear
of heterogeneity: Classifier calibration for federated learning with non-
iid data,‚Äù Advances in Neural Information Processing Systems , vol. 34,
pp. 5972‚Äì5984, 2021.
[45] C. He, S. Li, J. So, X. Zeng, M. Zhang, H. Wang, X. Wang,
P. Vepakomma, A. Singh, H. Qiu, et al. , ‚ÄúFedml: A research li-
brary and benchmark for federated machine learning,‚Äù arXiv preprint
arXiv:2007.13518 , 2020.
