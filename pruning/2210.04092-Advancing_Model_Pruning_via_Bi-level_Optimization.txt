# 2210.04092.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2210.04092.pdf
# File size: 2222725 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Advancing Model Pruning via Bi-level Optimization
Yihua Zhang1,*Yuguang Yao1,*Parikshit Ram2Pu Zhao3Tianlong Chen4
Mingyi Hong5Yanzhi Wang3Sijia Liu1,2
1Michigan State University,2IBM Research,3Northeastern University,
4University of Texas at Austin,5University of Minnesota, Twin Cities
*Equal contribution
Abstract
The deployment constraints in practical applications necessitate the pruning of
large-scale deep learning models, i.e., promoting their weight sparsity. As illus-
trated by the Lottery Ticket Hypothesis (LTH), pruning also has the potential of
improving their generalization ability. At the core of LTH, iterative magnitude
pruning (IMP) is the predominant pruning method to successfully Ô¨Ånd ‚Äòwinning
tickets‚Äô. Yet, the computation cost of IMP grows prohibitively as the targeted
pruning ratio increases. To reduce the computation overhead, various efÔ¨Åcient
‚Äòone-shot‚Äô pruning methods have been developed but these schemes are usually
unable to Ô¨Ånd winning tickets as good as IMP. This raises the question of how to
close the gap between pruning accuracy and pruning efÔ¨Åciency? To tackle it, we
pursue the algorithmic advancement of model pruning. SpeciÔ¨Åcally, we formulate
the pruning problem from a fresh and novel viewpoint, bi-level optimization (BLO).
We show that the BLO interpretation provides a technically-grounded optimization
base for an efÔ¨Åcient implementation of the pruning-retraining learning paradigm
used in IMP. We also show that the proposed bi-level optimization-oriented pruning
method (termed BIP) is a special class of BLO problems with a bi-linear problem
structure. By leveraging such bi-linearity, we theoretically show that BIPcan
be solved as easily as Ô¨Årst-order optimization, thus inheriting the computation
efÔ¨Åciency. Through extensive experiments on both structured and unstructured
pruning with 5 model architectures and 4 data sets, we demonstrate that BIP
can Ô¨Ånd better winning tickets than IMP in most cases, and is computationally
as efÔ¨Åcient as the one-shot pruning schemes, demonstrating 2-7 speedup over
IMP for the same level of model accuracy and sparsity. Codes are available at
https://github.com/OPTML-Group/BiP .
1 Introduction
While over-parameterized structures are key to the improved generalization of deep neural networks
(DNNs) [ 1‚Äì3], they create new problems ‚Äì the millions or even billions of parameters not only
increase computational costs during inference, but also pose serious deployment challenges on
resource-limited devices [ 4]. As a result, model pruning has seen a lot of research interest in recent
years, focusing on reducing model sizes by removing (or pruning) redundant parameters [ 4‚Äì8].
Model sparsity (achieved by pruning) also beneÔ¨Åts adversarial robustness [ 9], out-of-distribution
generalization [ 10], and transfer learning [ 11]. Some pruning methods (towards structured sparsity)
facilitate model deployment on hardware [12, 13].
36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.04092v4  [cs.LG]  21 Apr 2023

--- PAGE 2 ---
Among various proposed model pruning algorithms [ 5,9,11,14‚Äì27], the heuristics-based Iterative
Magnitude Pruning (IMP) is the current dominant approach to achieving model sparsity without
suffering performance loss, as suggested and empirically justiÔ¨Åed by the Lottery Ticket Hypothesis
(LTH) [ 17]. The LTH hypothesizes the existence of a subnetwork (the so-called ‚Äòwinning ticket‚Äô)
when trained in isolation ( e.g., from either a random initialization [ 17] or an early-rewinding point of
dense model training [ 19]), can match the performance of the original dense model [ 5,11,17‚Äì20].
The core idea of IMP is to iteratively prune and retrain the model while progressively pruning
a small ratio of the remaining weights in each iteration, and continuing till the desired pruning
ratio has been reached. While IMP often Ô¨Ånds the winning ticket, it incurs the cost of repeated
model retraining from scratch, making it prohibitively expensive for large datasets or large model
architectures [ 28,29]. To improve the efÔ¨Åciency of model pruning, numerous heuristics-based
one-shot pruning methods [ 17,21‚Äì25],e.g., one-shot magnitude pruning (OMP), have been proposed.
These schemes directly prune the model to the target sparsity and are signiÔ¨Åcantly more efÔ¨Åcient than
IMP. Yet, the promise of current one-shot pruning methods is dataset/model-speciÔ¨Åc [ 22,30] and
mostly lies in the low pruning ratio regime [ 25]. As systematically studied in [ 22,31], there exists
a clear performance gap between one-shot pruning and IMP. As an alternative to heuristics-based
schemes, optimization-based pruning methods [ 9,15,16,26,27] still follow the pruning-retraining
paradigm and adopt sparsity regularization [ 32,33] or parameterized masking [ 9,16,27] to prune
models efÔ¨Åciently. However, these methods do not always match the accuracy of IMP and thus have
not been widely used to Ô¨Ånd winning tickets [ 17,21,23‚Äì25,29]. The empirical results showing that
optimization underperforms heuristics motivate us to revisit the algorithmic fundamentals of pruning.
0 100 200 300 400 500
Time (min)91.592.092.593.093.594.094.595.095.596.0Test Accuracy (%)ResNet-18
ResNet-56
ResNet-20Dense Model IMP BiP (ours)
Figure 1: A performance snapshot of the pro-
posed BIPmethod vs. the IMP baseline and the
original dense model across three pruned ResNet
models (ResNet-20, ResNet-56, and ResNet-18)
with 74% sparsity on CIFAR-10. The marker
size indicates the relative model size. The uni-
color region corresponds to the same model type
used by different pruning methods.To this end, we put forth a novel perspective of model
pruning as a bi-level optimization (BLO) problem. In
this new formulation, we show that BLO provides a
technically-grounded optimization basis for an efÔ¨Åcient
implementation of the pruning-retraining paradigm, the
key algorithmic component used in IMP. To the best
of our knowledge, we make the Ô¨Årst rigorous connec-
tion between model pruning and BLO. Technically, we
propose a novel bi-level optimization-enabled pruning
method (termed BIP). We further show how BIPtakes
advantage of the bi-linearity of the pruning problem to
avoid the computational challenges of common BLO
methods, and is as efÔ¨Åcient as any Ô¨Årst-order alternating
optimization scheme. Practically, we demonstrate the
superiority of the proposed BIPin terms of accuracy,
sparsity, and computation efÔ¨Åciency through extensive
experiments. BIPÔ¨Ånds the best winning ticket nearly in all settings while taking time comparable
to the one-shot OMP. In Fig. 1, we present a snapshot of our empirical results for CIFAR-10 with
3 ResNet architectures at 74% pruning ratio. In all cases, BIP(?) Ô¨Ånds winning tickets, improving
accuracy over the dense model ( ) and matching IMP ( N), while being upto 5 faster than IMP.
Ourcontributions can be summarized as follows:
(Formulation) We rethink the algorithmic foundation of model pruning through the lens of BLO
(bi-level optimization). The new BLO-oriented formulation disentangles pruning and retraining
variables, providing the Ô¨Çexibility to design the interface between pruning and retraining.
(Algorithm) We propose the new bi-level pruning ( BIP) algorithm, which is built upon the aforemen-
tioned BLO formulation and the implicit gradient-based optimization theory. Unlike computationally
intensive standard BLO solvers, we theoretically show that BIPis as efÔ¨Åcient as any Ô¨Årst-order
optimization by taking advantage of the bi-linear nature of the pruning variables.
(Experiments) We conduct extensive experiments across 4 datasets (CIFAR-10, CIFAR-100, Tiny-
ImageNet and ImageNet), 5 model architectures, and 3 pruning settings (unstructured pruning,
Ô¨Ålter-wise structured pruning, and channel-wise structured pruning). We show that (i) B IP achieves
higher test accuracy than IMP and Ô¨Ånds the best winning tickets nearly in all settings, (ii) BIPis
highly efÔ¨Åcient (comparable to one-shot pruning schemes), that is able to achieve 2-7 speedup over
IMP for the same level of model accuracy and sparsity, and (iii) BIPis able to Ô¨Ånd subnetworks that
achieve better performance than the dense model regardless of initialization rewinding.
2

--- PAGE 3 ---
2 Related Work and Open Question
Neural network pruning. As neural networks become deeper and more sophisticated, model
pruning technology has gained increasing attention over the last decade since pruned models are
necessary for the deployment of deep networks in practical applications [ 4,34,35]. With the goal
of Ô¨Ånding highly-sparse andhighly-accurate subnetworks from original dense models, a variety
of pruning methods have been developed such as heuristics-based pruning [ 17,21,23‚Äì25,29,36]
and optimization-based pruning [ 9,16,26,27]. The former identiÔ¨Åes redundant model weights by
leveraging heuristics-based metrics such as weight magnitudes [ 6,17,19,11,22,37,31,36,38],
gradient magnitudes [ 21,23,24,39,40], and Hessian statistics [ 41‚Äì46].The latter is typically built on:
1) sparsity-promoting optimization [ 15,33,47‚Äì50], where model weights are trained by penalizing
their sparsity-inducing norms, such as `0and`1norms for irregular weight pruning, and `2norm
for structured pruning; 2) parameterized masking [ 16,9,51‚Äì55], where model weight scores are
optimized to Ô¨Ålter the most important weights and achieve better performance.
Iterative vs. one-shot pruning, and motivation. Existing schemes can be further categorized into
one-shot or iterative pruning based on the pruning schedule employed for achieving the targeted model
sparsity. Among the iterative schemes, the IMP (Iterative Magnitude Pruning scheme) [ 17,20,56‚Äì
65,36] has played a signiÔ¨Åcant role in identifying high-quality ‚Äòwinning tickets‚Äô, as postulated
by LTH (Lottery Ticket Hypothesis) [ 18,19]. To enable consistent comparisons among different
methods, we extend the original deÔ¨Ånition of winning tickets in [ 17] to ‚Äòmatching subnetworks‚Äô [ 20]
so as to cover different implementations of winning tickets, e.g., the use of early-epoch rewinding for
model re-initialization [ 18] and the no-rewinding ( i.e., Ô¨Åne-tuning) variant [ 66]. BrieÔ¨Çy, the matching
subnetworks should match or surpass the performance of the original dense model [ 20]. In this work,
if a matching subnetwork is found better than the winning ticket obtained by the same method that
follows the original LTH setup [ 18,19], we will also call such a matching subnetwork a winning
ticket throughout the paper.
0 20 40 60 80 100
Pruning Ratio (%)80828486889092Test Accuracy (%)
74% Sparsity
Dense Model
IMP
OMP
Grasp
Snip
SynFlow
20 40 60 80 100
Pruning Ratio (%)102103Time Consumption (min)
3√ó more time
Figure 2: Illustration of the pros and cons of different pruning
methods executed over (ResNet-20, CIFAR-10). Left: test ac-
curacy vs. different pruning ratios of IMP and one-shot pruning
methods ( OMP [17],GRASP [23],SNIP[21],SYNFLOW [24]).
Right : comparison of the efÔ¨Åciency with different sparsity.For example, the current state-of-the-
art (SOTA) implementation of IMP in
[22] can lead to a pruned ResNet-20
on CIFAR-10 with 74% sparsity and
92:12% test accuracy, matching the per-
formance of the original dense model
(see red?in Fig. 2- Left). The IMP algo-
rithm typically contains two key ingre-
dients: (i)atemporally-evolving prun-
ing schedule to progressively increase
model sparsity over pruning iterations,
and(ii)thepruning-retraining learning
mechanism applied at each pruning iter-
ation. With a target pruning ratio of p%
withTpruning iterations, an example pruning schedule in (i) could be as follows ‚Äì each iteration
prunes (p%)1=Tof the currently unpruned model weights, progressively pruning fewer weights in
each iteration. For (ii), the unpruned weights in each pruning iteration are re-set to the weights
at initialization or at an early-training epoch [ 18], and re-trained till convergence. In brief, IMP
repeatedly prunes, resets, and trains the network over multiple iterations.
However, winning tickets found by IMP incur signiÔ¨Åcant computational costs. The sparsest winning
ticket found by IMP in Fig. 2- Left (red?) utilizesT= 6pruning iterations. As shown in Fig. 2- Right ,
this takes 3more time than the original training of the dense model. To avoid the computational
cost of IMP, different kinds of ‚Äòaccelerated‚Äô pruning methods were developed [ 17,21,23‚Äì25,29],
and many fall into the one-shot pruning category: The network is directly pruned to the target sparsity
and retrained once. In particular, OMP (one-shot magnitude pruning) is an important baseline that
simpliÔ¨Åes IMP [ 17]. It follows the pruning-retraining paradigm, but prunes the model to the target
ratio with a single pruning iteration. Although one-shot pruning schemes are computationally cheap
(Fig. 2- Right ), they incur a signiÔ¨Åcant accuracy drop compared to IMP (Fig. 2- Left). Even if IMP is
customized with a reduced number of training epochs per pruning round, the pruning accuracy also
drops largely (see Fig. A2). Hence, there is a need for advanced model pruning techniques to Ô¨Ånd
winning tickets like IMP, while being efÔ¨Åcient like one-shot pruning.
3

--- PAGE 4 ---
Different from unstructured weight pruning described above, structured pruning takes into consid-
eration the sparse patterns of the model, such as Ô¨Ålter and channel-wise pruning [ 13,48,51,67‚Äì
71]. Structured pruning is desirable for deep model deployment in the presence of hardware con-
straints [ 4,34]. However, compared to unstructured pruning, it is usually more challenging to
maintain the performance and Ô¨Ånd structure-aware winning tickets [28, 29].
Open question. As discussed above, one-shot pruning methods are unable to match the predic-
tive performance of IMP, and structure-aware winning tickets are hard to Ô¨Ånd. Clearly, the best
optimization foundation of model pruning is underdeveloped. Thus, we ask:
Is there an optimization basis for a successful pruning algorithm that can attain high pruned
model accuracy (like IMP) andhigh computational efÔ¨Åciency (like one-shot pruning)?
The model pruning problem has a natural hierarchical structure ‚Äì we need to Ô¨Ånd the best mask to
prune model parameters, and then , given the mask, Ô¨Ånd the best model weights for the unpruned
model parameters. Given this hierarchical structure, we believe that the bi-level optimization (BLO)
framework is one promising optimization basis for a successful pruning algorithm.
Bi-level optimization (BLO). BLO is a general hierarchical optimization framework, where the
upper-level problem depends on the solution to the lower-level problem. Such a nested structure
makes the BLO in its most generic form very difÔ¨Åcult to solve, since it is hard to characterize the
inÔ¨Çuence of the lower-level optimization on the upper-level problem. Various existing literature
focuses on the design and analysis of algorithms for various special cases of BLO. Applications range
from the classical approximate descent methods [ 72‚Äì74], penalty-based methods [ 75,76], to recent
BigSAM [ 77] and its extensions [ 78,79]. It is also studied in the area of stochastic algorithms [ 80‚Äì82]
and back-propagation based algorithms [ 83‚Äì85]. BLO has also advanced adversarial training [ 86],
meta-learning [ 87], data poisoning attack generation [ 88], neural architecture search [ 89] as well as
reinforcement learning [ 90]. Although BLO was referred in [ 50] for model pruning, it actually called
an ordinary alternating optimization procedure, without taking the hierarchical learning structure of
BLO into consideration. To the best of our knowledge, the BLO framework has not been considered
for model pruning in-depth and systematically. We will show that model pruning yields a special
class of BLO problems with bi-linear optimization variables. We will also theoretically show that this
specialized BLO problem for model pruning can be solved as efÔ¨Åciently as Ô¨Årst-order optimization.
This is in a sharp contrast to existing BLO applications that rely on heuristics-based BLO solvers
(e.g., gradient unrolling in meta learning [87] and neural architecture search [89, 91]).
3 B IP: Model Pruning via Bi-level Optimization
In this section, we re-investigate the problem of model pruning through the lens of BLO and develop
the bi-level pruning ( BIP) algorithm. We can theoretically show that BIPcan be solved as easily as
the Ô¨Årst-order alternating optimization by taking advantage of the bi-linearity of pruning variables.
A BLO viewpoint on model pruning As described in the previous section, the pruning-retraining
learning paradigm covers two kinds of tasks: ¬∂pruning that determines the sparse pattern of model
weights, and ¬∑training remaining non-zero weights to recover the model accuracy. In existing
optimization-based pruning methods [ 92‚Äì95], the tasks ¬∂-¬∑are typically achieved by optimizing
model weights, together with penalizing their sparsity-inducing norms, e.g., the`1and`2norms [ 96].
Different from the above formulation, we propose to separate optimization variables involved in the
pruning tasks ¬∂and¬∑. This leads to the (binary) pruning mask variable m2f0;1gnand the model
weight variable 2Rn. Herendenotes the total number of model parameters. Accordingly, the
pruned model is given by (m), wheredenotes the element-wise multiplication. As will be
evident later, this form of variable disentanglement enables us to explicitly depict how the pruning
and retraining process co-evolve, and helps customize the pruning task with high Ô¨Çexibility.
We next elaborate on how BLO can be established to co-optimize the pruning mask mand the
retrained sparse model weights . Given the pruning ratio p%, the sparsity constraint is given by
m2S, whereS=fmjm2f0;1gn;1Tmkgandk= (1 p%)n. Our goal is to prune
the original model directly to the targeted pruning ratio p%(i.e., without calling for the IMP-like
sparsity schedule as described in Sec. 2) and obtain the optimized sparse model (m). To this
4

--- PAGE 5 ---
end, we interpret the pruning task ( i.e.,¬∂) and the model retraining task ( i.e.,¬∑) astwo optimization
levels , where the former is formulated as an upper-level optimization problem, and it relies on the
optimization of the lower-level retraining task. We thus cast the model pruning problem as the
following BLO problem (with ¬∑being nested inside ¬∂):
minimize
m2S`(m(m))|{z}
¬∂: Pruning task; subject to (m) = arg min
2Rn:=g(m;)z}|{
`(m) +
2kk2
2
| {z }
¬∑: Sparsity-Ô¨Åxed model retraining; (1)
where`denotes the training loss ( e.g., cross-entropy), mandare the upper-level and lower-level
optimization variables respectively, (m)signiÔ¨Åes the lower-level solution obtained by minimizing
the objective function ggiven the pruning mask m, and >0is a regularization parameter introduced
to convexify the lower-level optimization so as to stabilize the gradient Ô¨Çow from (m)tomand
thus the convergence of BLO [ 82,97]. In a sharp contrast to existing single-level optimization-based
model pruning methods [92‚Äì95], the BLO formulation (1) brings in two advantages .
First, BLO has the Ô¨Çexibility to use mismatched pruning and retraining objectives at the upper and
lower optimization levels, respectively. This Ô¨Çexibility allows us to regularize the lower-level training
objective function in (1) and customize the implemented optimization methods at both levels. To be
more speciÔ¨Åc, one can update the upper-level pruning mask musing a data batch (called B2) distinct
from the one (called B1) used for obtaining the lower-level solution (m). The resulting BLO
procedure can then mimic the idea of meta learning to improve model generalization [ 98], where the
lower-level problem Ô¨Åne-tunes usingB1, and the upper-level problem validates the generalization
of the sparsity-aware Ô¨Ånetuned model (m(m))usingB2.
Second , BLO enables us to explicitly model and optimize the coupling between the retrained model
weights (m)and the pruning mask mthrough the implicit gradient (IG)-based optimization
routine. Here IG refers to the gradient of the lower-level solution (m)with respect to (w.r.t.)
the upper-level variable m, and its derivation calls the implicit function theory [ 76]. The use of IG
makes our proposed BLO-oriented pruning (1) signiÔ¨Åcantly different from the greedy alternating
minimization [ 99] that learns the upper-level and lower-level variables independently ( i.e., minimizes
one variable by Ô¨Åxing the other). We refer readers to the following section for the detailed IG theory.
We will also show in Sec. 4 that the pruning strategy from (1) can outperform IMP in many pruning
scenarios but is much more efÔ¨Åcient as it does not call for the scheduler of iterative pruning ratios.
Optimization foundation of B IP. The key optimization challenge of solving the BIPproblem (1)
lies in the computation of IG (implicit gradient). Prior to developing an effective solution, we Ô¨Årst
elaborate on the IG challenge , the unique characteristic of BLO. In the context of gradient descent,
the gradient of the objective function in (1) yields
d`(m(m))
dm|{z}
Gradient of objective=rm`(m(m)) +d((m)>)
dm|{z}
IGr`(m(m)); (2)
wherermandrdenote the partial derivatives of the bi-variate function `(m)w.r.t. the
variable mandrespectively, d>=dm2Rnndenotes the vector-wise full derivative , and for ease
of notation, we will omit the transpose >when the context is clear. In (2), the IG challenge refers to
the demand for computing the full gradient of the implicit function (m) = arg ming(m;)w.r.t.
m, where recall from (1) that g(m;):=`(m) +
2kk2
2.
Next, we derive the IG formula following the rigorous implicit function theory [ 76,82,87]. Based
on the fact that (m)satisÔ¨Åes the stationarity condition for the lower-level objective function in (2),
it is not difÔ¨Åcult to obtain that (see derivation in Appendix A)
d(m)
dm= r2
m`(m)[r2
`(m) +I] 1; (3)
wherer2
m`andr2
`denote the second-order partial derivatives of `respectively, and () 1denotes
the matrix inversion operation.
Yet, the exact IG formula (3) remains difÔ¨Åcult to calculate due to the presence of matrix inversion and
second-order partial derivatives. To simplify it, we impose the Hessian-free assumption, r2
`=0,
which is mild in general; For example, the decision boundaries of neural networks with ReLU
5

--- PAGE 6 ---
activations are piece-wise linear in a tropical hyper-surface [ 100], and this assumption has been
widely used in BLO-involved applications such as meta learning [ 101] and adversarial learning [ 86].
Givenr2
`=0, the matrix inversion in (3) can be then mitigated, leading to the IG formula
d(m)
dm= 1
r2
m`(m): (4)
At the Ô¨Årst glance, the computation of the simpliÔ¨Åed IG (4) still requires the mixed (second-order)
partial derivativer2
m`. However, BIPis a special class of BLO problems with bi-linear variables
(m). Based on this bi-linearity, we can prove that IG in (4) can be analytically expressed using
only Ô¨Årst-order derivatives; see the following theorem.
Proposition 1 Assumingr2
`= 0and deÔ¨Åningrz`(z):=rz`(z)jz=m, the implicit gradient
(4) is then given by
d(m)
dm= 1
diag(rz`(z)); (5)
Further, the gradient of the objective function given by (2) becomes
d`(m)
dm= ( 1
mr z`(z))r z`(z); (6)
wheredenotes the element-wise multiplication.
Proof : Using chain-rule, we can obtain that
r`(m) = diag( m)rz`(z) =mr z`(z); (7)
similarly,rm`(m) = diag( )rz`(z) =r z`(z) (8)
where diag()represents a diagonal matrix with being the main diagonal vector. Further, we can
convert (4) to
r2
m`(m)(7)=rm[mr z`(z)]chain rule= diag(rz`(z)) + diag( m)[rm(rz`(z))]
(8)= diag(rz`(z)) + diag( m)[diag( )r2
z`(z)] = diag(rz`(z)); (9)
where the last equality holds due to the Hessian-free assumption. With (9) and (4) we can prove (5).
Next, substituting the IG (5) to the upper-level gradient (2), we obtain that
d`(m)
dm=rm`(m) 1
rz`(z)r `(m)
(7);(8)=r z`(z) 1
rz`(z)(mr z`(z)) = (  1
mr z`(z))r z`(z);
which leads to (6). The proof is now complete. 
The key insight drawn from Prop. 1 is that the bi-linearity of pruning variables ( i.e.,m) makes
the IG-involved gradient (2) easily solvable, and the computational complexity is almost the same as
that of computing the Ô¨Årst-order gradient rz`(z)just once, as supported by (6)
BIP algorithm and implementation. We next formalize the BIPalgorithm based on Prop. 1 and
the alternating gradient descent based BLO solver [82]. At iteration t, there are two main steps.
HLower-level SGD for model retraining : Given m(t 1),(t 1), andz(t 1):=m(t 1)(t 1), we
update (t)by randomly selecting a data batch with the learning rate and applying SGD (stochastic
gradient descent) to the lower-level problem of (1),
(t)=(t 1) rg(m(t 1);(t 1))(7)=(t 1) [m(t 1)r z`(z)jz=z(t 1)+(t 1)];(-step)
HUpper-level SPGD for pruning : Given m(t 1),(t), andz(t+1=2):=m(t 1)(t), we update
musing SPGD (stochastic projected gradient descent) along the IG-enhanced descent direction (2),
m(t)=PS
m(t 1) d`(m(t))
dmjm=m(t 1)
(6)=PS
m(t 1) 
(t) 1
m(t 1)r z`(z)jz=z(t+1=2)
r z`(z)jz=z(t+1=2)
;(m-step)
6

--- PAGE 7 ---
Our 
proposal
: 
Bi-level 
Pruning 
(BiP) 
directly 
to 
pruning 
ratio 
p%
Original
Pruned
Pretrained 
Model
Original
Prior 
art: 
Iterative 
Magnitude 
Pruning 
(IMP) 
over 
T 
Rounds
Pruned
Train 
N 
epochs
Random 
& 
dense 
init
Train 
N 
epochs
Prune 
p
1/T
%
Prune 
p
1/T
%
Rewind 
to 
init
Train 
N 
epochs
Rewind 
to 
init
Prune 
p
1/T
%
Rewind 
to 
init
Train 
N 
epochs
Prune 
to 
p% 
1 
epoch 
SPGD
Round 
1
Round 
2
......
Round 
T
Prune 
to 
p% 
1 
epoch 
SPGD
Train 
weights
1 
epoch 
SGD
Sparse 
Model
Upper 
level
Upper 
level
Upper 
level
Lower 
level
Lower 
level
Lower 
level
Prune 
to 
p% 
1 
epoch 
SPGD
Train 
weights
1 
epoch 
SGD
Train 
weights 
1 
epoch 
SGD
Sparse 
ModelFigure 3: Visualization of pruning pipeline comparison between IMP and BIP. Edge refers to the mask update
and color refers to the weight update.
where >0is the upper-level learning rate, and PS()denotes the Euclidean projection onto the
constraint setSgiven byS=fmjm2f0;1gn;1Tmkgin (1) and is achieved by the top- k
hard-thresholding operation as will be detailed later.
InBIP, the ( -step) and ( m-step) steps execute iteratively. For clarity, Fig. 3 shows the difference
between the pruning pipelines of BIPand IMP. In contrast to IMP that progressively prunes and
retrains a model with a growing pruning ratio, BIPdirectly prunes the model to the targeted sparsity
level without involving costly re-training process. In practice, we Ô¨Ånd that both the upper- and lower-
level optimization routines of BIPconverge very well (see Fig. A12 and Fig. A13). It is also worth
noting that both ( -step) and ( m-step) only require the Ô¨Årst-order information rz`(z), demonstrating
thatBIPcan be conducted as efÔ¨Åciently as Ô¨Årst-order optimization. In Fig. A1, we highlight the
algorithmic details on the BIPpipeline. We present more implementation details of BIPbelow and
refer readers to Appendix B for a detailed algorithm description.
FDiscrete optimization over m:We follow the ‚Äòconvex relaxation + hard thresholding‚Äô mechanism
used in [ 9,16]. SpeciÔ¨Åcally, we relax the binary masking variables to continuous masking scores
m2[0;1]. We then acquire loss gradients at the backward pass based on the relaxed m. At the
forward pass, we project it onto the discrete constraint set Susing the hard thresholding operator,
where the top kelements are set to 1s and the others to 0s. See Appendix B for more discussion.
FData batch selection for lower-level and upper-level optimization: We adopt different data batches
(with the same batch size) when implementing ( -step) and ( m-step). This is one of the advantages
of the BLO formulation, which enables the Ô¨Çexibility to customize the lower-level and upper-level
problems. The use of diverse data batches is beneÔ¨Åcial to generalization as shown in [98].
FHyperparameter tuning: As described in ( -step)-( m-step), BIPneeds to set two learning rates 
andfor lower-level and upper-level optimization, respectively. We choose = 0:01and= 0:1in
all experiments, where we adopt the mask learning rate from Hydra [ 9] and set a smaller lower-level
learning rate , asis initialized by a pre-trained dense model. We show ablation study on 
in Fig. A8(c). BLO also brings in the low-level convexiÔ¨Åcation parameter . We set= 1:0in
experiments and refer readers to Fig. A8(b) for a sanity check.
FOne-step vs. multi-step SGD: In (-step), the one-step SGD is used and helps reduce the computa-
tion overhead. In practice, we also Ô¨Ånd that the one-step SGD is sufÔ¨Åcient: The use of multi-step
SGD in B IP does not yield much signiÔ¨Åcant improvement over the one-step version; see Fig. A8(a).
FExtension to structured pruning: We formulate and solve the BIPproblem in the context of
unstructured (element-wise) weight pruning. However, if deÔ¨Åne the pruning mask mw.r.t. model‚Äôs
structural units ( e.g., Ô¨Ålters), B IP is easily applied to structured pruning (see Fig. 6 and Fig. A10).
7

--- PAGE 8 ---
0 20 40 60 80 100
Pruning Ratio (%)93.093.594.094.595.095.5 Test Accuracy (%)
CIFAR-10, ResNet-18
Dense Model
Hydra
IMP
OMP
Grasp
BiP
Best Winning Ticket
0 20 40 60 80 100
Pruning Ratio (%)89.089.590.090.591.091.592.092.5 Test Accuracy (%)
CIFAR-10, ResNet-20
0 20 40 60 80 100
Pruning Ratio (%)90.090.591.091.592.092.593.093.5 Test Accuracy (%)
CIFAR-10, ResNet-56
0 20 40 60 80 100
Pruning Ratio (%)92.092.593.093.594.094.5 Test Accuracy (%)
CIFAR-10, VGG-16
0 20 40 60 80 100
Pruning Ratio (%)72.073.074.075.076.077.078.0 Test Accuracy (%)
CIFAR-100, ResNet-18
Dense Model
Hydra
IMP
OMP
Grasp
BiP
Best Winning Ticket
0 20 40 60 80 100
Pruning Ratio (%)60.062.064.066.068.070.0 Test Accuracy (%)
CIFAR-100, ResNet-20
0 20 40 60 80 100
Pruning Ratio (%)57.058.059.060.061.062.063.064.0 Test Accuracy (%)
Tiny-ImageNet, ResNet-18
0 20 40 60 80 100
Pruning Ratio (%)72.073.074.075.076.077.0 Test Accuracy (%)
ImageNet, ResNet-50
Dense Model
IMP
BiP
Best Winning TicketFigure 4: Unstructured pruning trajectory given by test accuracy (%) vs. sparsity (%) on various dataset-model
pairs. The proposed BIPis compared with HYDRA [9], IMP [ 22], OMP [ 22],GRASP [23]. And the performance
of dense model and that of the best winning ticket are marked using dashed lines in each plot. The solid line and
shaded area of each pruning method represent the mean and variance of test accuracies over 3 independent trials.
We observe that BIPconsistently outperforms the other baselines. Note in the (ImageNet, ResNet-50) setting,
we only compare B IP with our strongest baseline IMP due to computational resource constraints.
4 Experiments
In this section, we present extensive experimental results to show the effectiveness of BIPacross
multiple model architectures, various datasets, and different pruning setups. Compared to IMP,
one-shot pruning, and optimization-based pruning baselines, we Ô¨Ånd that BIPcan Ô¨Ånd better winning
tickets in most cases and is computationally efÔ¨Åcient.
4.1 Experiment Setup
Datasets and models. Following the pruning benchmark in [ 22], we consider 4 datasets including
CIFAR-10 [ 102], CIFAR-100 [ 102], Tiny-ImageNet [ 103], ImageNet [ 104], and 5 architecture types
including ResNet-20/56/18/50 and VGG-16 [ 105,106]. Tab. A1 summarizes these datasets and
model conÔ¨Ågurations and setups.
Baselines, training, and evaluation. As baselines, we mainly focus on 4 SOTA pruning methods,
¬¨IMP [ 17],¬≠OMP [ 17],¬ÆGRASP [23] (a one-shot pruning method by analyzing gradient
Ô¨Çow at initialization), and ¬ØHYDRA [9] (an optimization-based pruning method that optimizes
masking scores). It is worth noting that there exist various implementations of IMP, e.g., speciÔ¨Åed
by different learning rates and model initialization or ‚Äòrewinding‚Äô strategies [ 18]. To make a fair
comparison, we follow the recent IMP benchmark in [ 22], which can Ô¨Ånd the best winning tickets over
current heuristics-based pruning baselines. We also remark that HYDRA is originally proposed for
improving the adversarial robustness of a pruned model, but it can be easily customized for standard
pruning when setting the adversary‚Äôs strength as 0[9]. We choose HYDRA as a baseline because
it can be regarded as a single-level variant of BIPwith post-optimization weight retraining. When
implementing BIP, unless speciÔ¨Åed otherwise, we use the 1-step SGD in ( -step), and set the learning
rates (;)and the lower-level regularization parameter as described in the previous section. When
implementing baselines, we follow their ofÔ¨Åcial repository setups. We evaluate the performance
of all methods mainly from two perspectives: (1) the test accuracy of the sub-network, and (2) the
runtime of pruning to reach the desired sparsity. We refer readers to Tab. A3 and Appendix C.2 for
more training and evaluation details, such as training epochs and learning rate schedules.
4.2 Experiment Results
BIP identiÔ¨Åes high-accuracy subnetworks. In what follows, we look at the quality of winning
tickets identiÔ¨Åed by BIP.Two key observations can be drawn from our results: (1) BIPÔ¨Ånds winning
tickets of higher accuracy and/or higher sparsity than the baselines in most cases (as shown in Fig. 4
8

--- PAGE 9 ---
Table 1: The sparsest winning tickets found by different methods at different data-model setups. Winning
tickets refer to the sparse models with an average test accuracy no less than the dense model [ 20]. In each cell,
p%(accstd%) represents the sparsity as well as the test accuracy. The test accuracy of dense models can be
found in the header. 7signiÔ¨Åes that no winning ticket is found by a pruning method. Given the data-model setup
(i.e., per column), the sparsest winning ticket is highlighted in bold .
MethodCIFAR-10 CIFAR-100
ResNet-18 ResNet-20 ResNet-56 VGG-16 ResNet-18 ResNet-20
(94.77%) (92.12%) (92.95%) (93.65%) (76.67%) (68.69%)
IMP 87% (94.770.10%) 74% (92.150.15%) 74% (92.990.12%) 89% (93.680.05%) 87% (76.910.19%) 7
OMP 49% (94.800.10%) 7 7 20% (93.790.06%) 74% (76.990.07%) 7
GRASP 7 7 36% (93.070.34%) 7 7 7
HYDRA 7 7 7 87% (93.730.03%) 7 20% (68.940.17%)
BIP 89% (94.790.15%) 67% (92.140.15%) 74% (93.130.04%) 93% (93.750.15%) 89% (76.690.18%) 49% (68.780.10%)
0 20 40 60 80 100
Pruning Ratio (%)91.091.592.092.593.093.594.094.595.095.5 Test Accuracy (%)
CIFAR-10, ResNet-18
Dense Model
Hydra
IMP
OMP
Grasp
BiP
Best Winning Ticket
0 20 40 60 80 100
Pruning Ratio (%)82.084.086.088.090.092.0 Test Accuracy (%)
CIFAR-10, ResNet-20
0 20 40 60 80 100
Pruning Ratio (%)70.071.072.073.074.075.076.077.078.0 Test Accuracy (%)
CIFAR-100, ResNet-18
0 20 40 60 80 100
Pruning Ratio (%)45.050.055.060.065.070.0 Test Accuracy (%)
CIFAR-100, ResNet-20
Figure 6: Filter pruning given by test accuracy (%) vs. pruning ratio (%). The visual presentation setting is the
same as Fig. 4. We observe that BIPidentiÔ¨Åes winning tickets of structured pruning in certain sparsity regimes.
and Tab. 1); (2) The superiority of BIPholds for both unstructured pruning and structured pruning (as
shown in Fig. 6 and Fig. A10). We refer to more experiment results in Appendix C.3.
Fig. 4 shows the unstructured pruning trajectory (given by test accuracy vs. pruning ratio) of BIP
and baseline methods in 8model-dataset setups. For comparison, we also present the performance of
the original dense model. As we can see, the proposed BIPapproach Ô¨Ånds the best winning tickets
(in terms of the highest accuracy) compared to the baselines across all the pruning setups. Among the
baseline methods, IMP is the most competitive method to ours. However, the improvement brought
byBIPis signiÔ¨Åcant with respect to the variance of IMP, except for the 60%-80% sparsity regime
in (CIFAR-10, ResNet-20). In the case of (CIFAR-100, ResNet-20), where IMP can not Ô¨Ånd any
winning tickets (as conÔ¨Årmed by [ 22]),BIPstill manages to Ô¨Ånd winning tickets with around 0:6%
improvement over the dense model. In Tab. 1 , we summarize the sparsest winning tickets along the
pruning trajectory identiÔ¨Åed by different pruning methods. BIPcan identify the winning tickets with
higher sparsity levels than the other methods, except in the case of (CIFAR-10, ResNet-20).
20 40 60 80 100
Pruning Ratio (%)102103Time Consumption (min)
Dense Model
Hydra
IMP
OMP
Grasp
BiP
Figure 5: Time consumption comparison on
(CIFAR-10, ResNet-18) with different prun-
ing ratiop.Fig. 6 demonstrates the structured pruning trajectory on
the CIFAR-10/100 datasets. Here we focus on Ô¨Ålter prun-
ing, where the Ô¨Ålter is regarded as a masking unit in (1).
We refer readers to Fig. A10 for channel-wise pruning re-
sults. Due to the page limit, we only report the results
of the Ô¨Ålter-wise pruning in the main paper and please
refer to Appendix C.3 for channel-wise pruning. Com-
pared to Fig. 4, Fig. 6 shows that it becomes more difÔ¨Åcult
to Ô¨Ånd winning tickets of high accuracy and sparsity in
the structured pruning, and the gap among different meth-
ods decreases. This is not surprising, since Ô¨Ålter pruning
imposes much stricter pruning structure constraints than
irregular pruning. However, BIPstill outperforms all the
baselines. Most importantly, it identiÔ¨Åes clear winning
tickets in the low sparse regime even when IMP fails.
BIP is computationally efÔ¨Åcient. In our experiments, another key observation is that BIPyields
sparsity-agnostic runtime complexity while IMP leads to runtime exponential to the target sparsity.
Fig. 5 shows the computation cost of different methods versus pruning ratios on (CIFAR-10, ResNet-
18). For example, BIPtakes 86 mins to Ô¨Ånd the sparsest winning ticket (with 89% sparsity in Tab. 1).
This yields 7less runtime than IMP, which consumes 620mins to Ô¨Ånd a comparable winning ticket
with87% sparsity. Compared to the optimization-based baseline HYDRA ,BIPis more efÔ¨Åcient as it
9

--- PAGE 10 ---
N/A 5% 10% 50% 75% 100%
Rewinding Epoch Number89.089.590.090.591.091.592.092.593.0 Test Accuracy of BiP (%)
CIFAR10, ResNet20s
Dense Model
p=20%p=67.2%
p=86.58%
N/A 5% 10% 50% 75% 100%
Rewinding Epoch Number94.094.294.494.694.895.095.295.4 Test Accuracy of BiP (%)
CIFAR10, ResNet18
Dense Model
p=20%p=67.2%
p=86.58%
N/A 5% 10% 50% 75% 100%
Rewinding Epoch Number75.7576.0076.2576.5076.7577.0077.2577.5077.75 Test Accuracy of BiP (%)
CIFAR100, ResNet18
Dense Model
p=20%p=67.2%
p=86.58%Figure 7: The sensitivity of BIPto rewinding epoch numbers on different datasets and model architectures.
"N/A" in the x-axis indicates B IP without retraining.
does not rely on the extra post-optimization retraining; see Tab. A2 for a detailed summary of runtime
and number of training epochs required by different pruning methods. Further, BIPtakes about 1.25
more computation time than GRASP and OMP. However, the latter methods lead to worse pruned
model accuracy, as demonstrated by their failure to Ô¨Ånd winning tickets in Tab. 1, Fig. 4, and Fig. 6.
BIP requires no rewinding. Another advantage of BIPis that it insensitive to model rewinding to
Ô¨Ånd matching subnetworks. Recall that rewinding is a strategy used in LTH [ 19] to determine what
model initialization should be used for retraining a pruned model. As shown in [ 22], an IMP-identiÔ¨Åed
winning ticket could be sensitive to the choice of a rewinding point. Fig. 7 shows the test accuracy
of the BIP-pruned model when it is retrained at different rewinding epochs under various datasets
and model architectures, where ‚ÄòN/A‚Äô in the x-axis represents the case of no retraining (and thus
no rewinding). As we can see, a carefully-tuned rewinding scheme does not lead to a signiÔ¨Åcant
improvement over BIPwithout retraining. This suggests that the subnetworks found by BIPare
already of high quality and does not require any rewinding operation.
Additional results. We include more experiment results in Appendix C.3. In particular, we show
more results in both unstructured and structured pruning settings in Fig. A4, Fig. A5, Fig. A6 and
Fig. A7, where we compare BIPwith more baselines and cover more model architectures. We also
study the sensitivity of BIPto the lower-level step number, lower-level regularization coefÔ¨Åcient,
the signiÔ¨Åcance of the implicit gradient term (2), learning rate, and batch size, as shown in Fig. A8
and Fig. A9. To demonstrate the convergence of the upper-level and lower-level optimization in BIP,
we show the training trajectory of BIPfor accuracy (Fig.A12) and mask score (Fig.A13), and show
how the lower-level step number affects the convergence speed (Fig. A14)). Further, we show the
performance of BIPvs. the growth of training epochs (Fig. A15), and its performance vs. different
data batch schedulers (see Fig. A16).
5 Conclusion
We proposed the BIPmethod to Ô¨Ånd sparse networks through the lens of BLO. Our work advanced
the algorithmic foundation of model pruning by characterizing its pruning-retraining hierarchy
using BLO. We theoretically showed that BIPcan be solved as easily as Ô¨Årst-order optimization
by exploiting the bi-linearity of pruning variables. We also empirically showed that BIPcan Ô¨Ånd
high-quality winning tickets very efÔ¨Åciently compared to the predominant iterative pruning method.
In the future, we will seek the optimal curriculum of training data at different optimization levels of
BIP, and will investigate the performance of our proposal for actual hardware acceleration.
Acknowledgement
The work of Y . Zhang, Y . Yao, and S. Liu was partially supported by National Science Foundation
(NSF) Grant IIS-2207052 and Cisco Research Award. The work of M. Hong was supported by
NSF grants CIF-1910385 and CMMI-1727757. The work of Y . Wang was supported NSF grant
CCF-1919117. The computing resources used in this work were also supported by the MIT-IBM
Watson AI Lab, IBM Research and the Institute for Cyber-Enabled Research (ICER) at Michigan
State University.
10

--- PAGE 11 ---
References
[1]Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, ‚ÄúImagenet classiÔ¨Åcation with deep
convolutional neural networks,‚Äù in Advances in neural information processing systems , 2012,
pp. 1097‚Äì1105.
[2]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., ‚ÄúLanguage
models are few-shot learners,‚Äù Advances in neural information processing systems , vol. 33, pp.
1877‚Äì1901, 2020.
[3]Peter L Bartlett, Andrea Montanari, and Alexander Rakhlin, ‚ÄúDeep learning: a statistical
viewpoint,‚Äù Acta numerica , vol. 30, pp. 87‚Äì201, 2021.
[4]Song Han, Huizi Mao, and William J Dally, ‚ÄúDeep compression: Compressing deep
neural networks with pruning, trained quantization and huffman coding,‚Äù arXiv preprint
arXiv:1510.00149 , 2015.
[5]Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag, ‚ÄúWhat is the
state of neural network pruning?,‚Äù arXiv preprint arXiv:2003.03033 , 2020.
[6]Song Han, Jeff Pool, John Tran, and William Dally, ‚ÄúLearning both weights and connections
for efÔ¨Åcient neural network,‚Äù Advances in neural information processing systems , vol. 28,
2015.
[7]Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han, ‚ÄúAmc: Automl for
model compression and acceleration on mobile devices,‚Äù in Proceedings of the European
conference on computer vision (ECCV) , 2018, pp. 784‚Äì800.
[8]Huizi Mao, Song Han, Jeff Pool, Wenshuo Li, Xingyu Liu, Yu Wang, and William J Dally,
‚ÄúExploring the granularity of sparsity in convolutional neural networks,‚Äù in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition Workshops , 2017, pp. 13‚Äì20.
[9]Vikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana, ‚ÄúHydra: Pruning adversarially
robust neural networks,‚Äù Advances in Neural Information Processing Systems , vol. 33, pp.
19655‚Äì19666, 2020.
[10] James Diffenderfer, Brian Bartoldson, Shreya Chaganti, Jize Zhang, and Bhavya Kailkhura,
‚ÄúA winning hand: Compressing deep networks can improve out-of-distribution robustness,‚Äù
Advances in Neural Information Processing Systems , vol. 34, pp. 664‚Äì676, 2021.
[11] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin,
and Zhangyang Wang, ‚ÄúThe lottery tickets hypothesis for supervised and self-supervised
pre-training in computer vision models,‚Äù in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2021, pp. 16306‚Äì16316.
[12] Xiaolong Ma, Fu-Ming Guo, Wei Niu, Xue Lin, Jian Tang, Kaisheng Ma, Bin Ren, and
Yanzhi Wang, ‚ÄúPconv: The missing but desirable sparsity in dnn weight pruning for real-time
execution on mobile devices,‚Äù in Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence ,
2020, vol. 34, pp. 5117‚Äì5124.
[13] Tianlong Chen, Xuxi Chen, Xiaolong Ma, Yanzhi Wang, and Zhangyang Wang, ‚ÄúCoarsening
the granularity: Towards structurally sparse lottery tickets,‚Äù arXiv preprint arXiv:2202.04736 ,
2022.
[14] Yann LeCun, John Denker, and Sara Solla, ‚ÄúOptimal brain damage,‚Äù Advances in neural
information processing systems , vol. 2, 1989.
[15] Ao Ren, Tianyun Zhang, Shaokai Ye, Jiayu Li, Wenyao Xu, Xuehai Qian, Xue Lin, and Yanzhi
Wang, ‚ÄúAdmm-nn: An algorithm-hardware co-design framework of dnns using alternating
direction method of multipliers,‚Äù 2018.
[16] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad
Rastegari, ‚ÄúWhat‚Äôs hidden in a randomly weighted neural network?,‚Äù in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020, pp. 11893‚Äì11902.
11

--- PAGE 12 ---
[17] Jonathan Frankle and Michael Carbin, ‚ÄúThe lottery ticket hypothesis: Finding sparse, trainable
neural networks,‚Äù arXiv preprint arXiv:1803.03635 , 2018.
[18] Alex Renda, Jonathan Frankle, and Michael Carbin, ‚ÄúComparing rewinding and Ô¨Åne-tuning in
neural network pruning,‚Äù in 8th International Conference on Learning Representations , 2020.
[19] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin, ‚ÄúLinear
mode connectivity and the lottery ticket hypothesis,‚Äù in International Conference on Machine
Learning . PMLR, 2020, pp. 3259‚Äì3269.
[20] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and
Michael Carbin, ‚ÄúThe lottery ticket hypothesis for pre-trained bert networks,‚Äù Advances in
neural information processing systems , vol. 33, pp. 15834‚Äì15846, 2020.
[21] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr, ‚ÄúSnip: Single-shot network
pruning based on connection sensitivity,‚Äù arXiv preprint arXiv:1810.02340 , 2018.
[22] Xiaolong Ma, Geng Yuan, Xuan Shen, Tianlong Chen, Xuxi Chen, Xiaohan Chen, Ning Liu,
Minghai Qin, Sijia Liu, Zhangyang Wang, et al., ‚ÄúSanity checks for lottery tickets: Does your
winning ticket really win the jackpot?,‚Äù arXiv preprint arXiv:2107.00166 , 2021.
[23] Chaoqi Wang, Guodong Zhang, and Roger Grosse, ‚ÄúPicking winning tickets before training
by preserving gradient Ô¨Çow,‚Äù arXiv preprint arXiv:2002.07376 , 2020.
[24] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli, ‚ÄúPruning neural
networks without any data by iteratively conserving synaptic Ô¨Çow,‚Äù Advances in Neural
Information Processing Systems , vol. 33, pp. 6377‚Äì6389, 2020.
[25] Milad Alizadeh, Shyam A. Tailor, Luisa M Zintgraf, Joost van Amersfoort, Sebastian Farquhar,
Nicholas Donald Lane, and Yarin Gal, ‚ÄúProspect pruning: Finding trainable weights at
initialization using meta-gradients,‚Äù in International Conference on Learning Representations ,
2022.
[26] Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin, ‚ÄúLayer-adaptive sparsity
for the magnitude-based pruning,‚Äù arXiv preprint arXiv:2010.07611 , 2020.
[27] R√≥bert Csord√°s, Sjoerd van Steenkiste, and J√ºrgen Schmidhuber, ‚ÄúAre neural nets modu-
lar? inspecting functional modularity through differentiable weight masks,‚Äù arXiv preprint
arXiv:2010.02066 , 2020.
[28] Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G
Baraniuk, Zhangyang Wang, and Yingyan Lin, ‚ÄúDrawing early-bird tickets: Towards more
efÔ¨Åcient training of deep networks,‚Äù arXiv preprint arXiv:1909.11957 , 2019.
[29] Zhenyu Zhang, Xuxi Chen, Tianlong Chen, and Zhangyang Wang, ‚ÄúEfÔ¨Åcient lottery ticket
Ô¨Ånding: Less data is more,‚Äù in International Conference on Machine Learning . PMLR, 2021,
pp. 12380‚Äì12390.
[30] Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, and Jason D Lee,
‚ÄúSanity-checking pruning methods: Random tickets can win the jackpot,‚Äù Advances in Neural
Information Processing Systems , vol. 33, pp. 20390‚Äì20401, 2020.
[31] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin, ‚ÄúPrun-
ing neural networks at initialization: Why are we missing the mark?,‚Äù arXiv preprint
arXiv:2009.08576 , 2020.
[32] Sijia Liu, Parikshit Ram, Deepak Vijaykeerthy, Djallel Bouneffouf, Gregory Bramble, Horst
Samulowitz, Dakuo Wang, Andrew Conn, and Alexander Gray, ‚ÄúAn ADMM based framework
for automl pipeline conÔ¨Åguration,‚Äù 2019.
[33] Huan Wang, Can Qin, Yulun Zhang, and Yun Fu, ‚ÄúNeural pruning via growing regularization,‚Äù
arXiv preprint arXiv:2012.09243 , 2020.
12

--- PAGE 13 ---
[34] Misha Denil, Babak Shakibi, Laurent Dinh, Marc‚ÄôAurelio Ranzato, and Nando De Freitas,
‚ÄúPredicting parameters in deep learning,‚Äù Advances in neural information processing systems ,
vol. 26, 2013.
[35] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O Hero III, and Pramod K
Varshney, ‚ÄúA primer on zeroth-order optimization in signal processing and machine learning:
Principals, recent advances, and applications,‚Äù IEEE Signal Processing Magazine , vol. 37, no.
5, pp. 43‚Äì54, 2020.
[36] Michael Zhu and Suyog Gupta, ‚ÄúTo prune, or not to prune: exploring the efÔ¨Åcacy of pruning
for model compression,‚Äù arXiv preprint arXiv:1710.01878 , 2017.
[37] Steven A Janowsky, ‚ÄúPruning versus clipping in neural networks,‚Äù Physical Review A , vol. 39,
no. 12, pp. 6600, 1989.
[38] Alexandra Peste, Eugenia IoÔ¨Ånova, Adrian Vladu, and Dan Alistarh, ‚ÄúAc/dc: Alternating
compressed/decompressed training of deep neural networks,‚Äù Advances in Neural Information
Processing Systems , vol. 34, pp. 8557‚Äì8570, 2021.
[39] Michael C Mozer and Paul Smolensky, ‚ÄúSkeletonization: A technique for trimming the fat
from a network via relevance assessment,‚Äù in Advances in neural information processing
systems , 1989, pp. 107‚Äì115.
[40] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen, ‚ÄúRigging
the lottery: Making all tickets winners,‚Äù in International Conference on Machine Learning .
PMLR, 2020, pp. 2943‚Äì2952.
[41] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz, ‚ÄúImportance
estimation for neural network pruning,‚Äù in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , 2019, pp. 11264‚Äì11272.
[42] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney, ‚ÄúPyhessian: Neural
networks through the lens of the hessian,‚Äù in 2020 IEEE International Conference on Big Data
(Big Data) . IEEE, 2020, pp. 581‚Äì590.
[43] Yann LeCun, John S Denker, and Sara A Solla, ‚ÄúOptimal brain damage,‚Äù in Advances in
neural information processing systems , 1990, pp. 598‚Äì605.
[44] Babak Hassibi and David G Stork, Second order derivatives for network pruning: Optimal
brain surgeon , Morgan Kaufmann, 1993.
[45] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz, ‚ÄúPruning convo-
lutional neural networks for resource efÔ¨Åcient inference,‚Äù arXiv preprint arXiv:1611.06440 ,
2016.
[46] Sidak Pal Singh and Dan Alistarh, ‚ÄúWoodÔ¨Åsher: EfÔ¨Åcient second-order approximation for
neural network compression,‚Äù Advances in Neural Information Processing Systems , vol. 33,
pp. 18098‚Äì18109, 2020.
[47] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang,
‚ÄúLearning efÔ¨Åcient convolutional networks through network slimming,‚Äù in Proceedings of the
IEEE International Conference on Computer Vision , 2017, pp. 2736‚Äì2744.
[48] Yihui He, Xiangyu Zhang, and Jian Sun, ‚ÄúChannel pruning for accelerating very deep neural
networks,‚Äù in Proceedings of the IEEE international conference on computer vision , 2017, pp.
1389‚Äì1397.
[49] Hao Zhou, Jose M Alvarez, and Fatih Porikli, ‚ÄúLess is more: Towards compact cnns,‚Äù in
European Conference on Computer Vision . Springer, 2016, pp. 662‚Äì677.
[50] Christos Louizos, Max Welling, and Diederik P Kingma, ‚ÄúLearning sparse neural networks
throughl_0regularization,‚Äù arXiv preprint arXiv:1712.01312 , 2017.
13

--- PAGE 14 ---
[51] Yi Guo, Huan Yuan, Jianchao Tan, Zhangyang Wang, Sen Yang, and Ji Liu, ‚ÄúGdp: Stabilized
neural network pruning via gates with differentiable polarization,‚Äù in Proceedings of the
IEEE/CVF International Conference on Computer Vision , 2021, pp. 5239‚Äì5250.
[52] Edgar Liberis and Nicholas D Lane, ‚ÄúDifferentiable network pruning for microcontrollers,‚Äù
arXiv preprint arXiv:2110.08350 , 2021.
[53] Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham
Kakade, and Ali Farhadi, ‚ÄúSoft threshold weight reparameterization for learnable sparsity,‚Äù in
International Conference on Machine Learning . PMLR, 2020, pp. 5544‚Äì5555.
[54] Chao Xue, Xiaoxing Wang, Junchi Yan, Yonggang Hu, Xiaokang Yang, and Kewei Sun,
‚ÄúRethinking bi-level optimization in neural architecture search: A gibbs sampling perspective,‚Äù
inProceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , 2021, vol. 35, pp. 10551‚Äì
10559.
[55] Xiao Zhou, Weizhong Zhang, Hang Xu, and Tong Zhang, ‚ÄúEffective sparsiÔ¨Åcation of neural
networks with global sparsity constraint,‚Äù in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2021, pp. 3599‚Äì3608.
[56] Trevor Gale, Erich Elsen, and Sara Hooker, ‚ÄúThe state of sparsity in deep neural networks,‚Äù
arXiv , vol. abs/1902.09574, 2019.
[57] Zhenyu Zhang, Xuxi Chen, Tianlong Chen, and Zhangyang Wang, ‚ÄúEfÔ¨Åcient lottery ticket
Ô¨Ånding: Less data is more,‚Äù in Proceedings of the 38th International Conference on Machine
Learning , Marina Meila and Tong Zhang, Eds. 18‚Äì24 Jul 2021, vol. 139 of Proceedings of
Machine Learning Research , pp. 12380‚Äì12390, PMLR.
[58] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin,
and Zhangyang Wang, ‚ÄúThe lottery tickets hypothesis for supervised and self-supervised
pre-training in computer vision models,‚Äù arXiv preprint arXiv:2012.06908 , 2020.
[59] Haonan Yu, Sergey Edunov, Yuandong Tian, and Ari S. Morcos, ‚ÄúPlaying the lottery with
rewards and multiple languages: lottery tickets in rl and nlp,‚Äù in 8th International Conference
on Learning Representations , 2020.
[60] Xuxi Chen, Zhenyu Zhang, Yongduo Sui, and Tianlong Chen, ‚Äú{GAN}s can play lottery
tickets too,‚Äù in International Conference on Learning Representations , 2021.
[61] Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie, and Zhangyang Wang,
‚ÄúGood students play big lottery better,‚Äù arXiv preprint arXiv:2101.03255 , 2021.
[62] Zhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu Cheng, Shuohang Wang, and Jingjing
Liu, ‚ÄúPlaying lottery tickets with vision and language,‚Äù arXiv preprint arXiv:2104.11832 ,
2021.
[63] Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang, ‚ÄúA uniÔ¨Åed
lottery ticket hypothesis for graph neural networks,‚Äù arXiv preprint arXiv:2102.06790 , 2021.
[64] Neha Mukund Kalibhat, Yogesh Balaji, and Soheil Feizi, ‚ÄúWinning lottery tickets in deep
generative models,‚Äù 2021.
[65] Tianlong Chen, Yu Cheng, Zhe Gan, Jingjing Liu, and Zhangyang Wang, ‚ÄúUltra-data-
efÔ¨Åcient gan training: Drawing a lottery ticket Ô¨Årst, then training it toughly,‚Äù arXiv preprint
arXiv:2103.00397 , 2021.
[66] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang, ‚ÄúLong live the
lottery: The existence of winning tickets in lifelong learning,‚Äù in International Conference on
Learning Representations , 2020.
[67] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf, ‚ÄúPruning Ô¨Ålters
for efÔ¨Åcient convnets,‚Äù arXiv preprint arXiv:1608.08710 , 2016.
14

--- PAGE 15 ---
[68] Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin, Yanzhi Wang, and
Bin Ren, ‚ÄúPatdnn: Achieving real-time dnn execution on mobile devices with pattern-based
weight pruning,‚Äù in Proceedings of the Twenty-Fifth International Conference on Architectural
Support for Programming Languages and Operating Systems , 2020, pp. 907‚Äì922.
[69] Xiaolong Ma, Wei Niu, Tianyun Zhang, Sijia Liu, Sheng Lin, Hongjia Li, Wujie Wen, Xiang
Chen, Jian Tang, Kaisheng Ma, et al., ‚ÄúAn image enhancing pattern-based sparsity for real-time
inference on mobile devices,‚Äù in European Conference on Computer Vision . Springer, 2020,
pp. 629‚Äì645.
[70] Jingyu Wang, Songming Yu, Zhuqing Yuan, Jinshan Yue, Zhe Yuan, Ruoyang Liu, Yanzhi
Wang, Huazhong Yang, Xueqing Li, and Yongpan Liu, ‚ÄúPaca: A pattern pruning algorithm and
channel-fused high pe utilization accelerator for cnns,‚Äù IEEE Transactions on Computer-Aided
Design of Integrated Circuits and Systems , 2022.
[71] Joost van Amersfoort, Milad Alizadeh, Sebastian Farquhar, Nicholas Lane, and Yarin Gal,
‚ÄúSingle shot structured pruning before training,‚Äù arXiv preprint arXiv:2007.00389 , 2020.
[72] James E Falk and Jiming Liu, ‚ÄúOn bilevel programming, part i: general nonlinear cases,‚Äù
Mathematical Programming , vol. 70, no. 1, pp. 47‚Äì72, 1995.
[73] Luis Vicente, Gilles Savard, and Joaquim J√∫dice, ‚ÄúDescent approaches for quadratic bilevel
programming,‚Äù Journal of Optimization Theory and Applications , vol. 81, no. 2, pp. 379‚Äì399,
1994.
[74] Can Chen, Xi Chen, Chen Ma, Zixuan Liu, and Xue Liu, ‚ÄúGradient-based bi-level optimization
for deep learning: A survey,‚Äù arXiv preprint arXiv:2207.11719 , 2022.
[75] Douglas J White and G Anandalingam, ‚ÄúA penalty function approach for solving bi-level
linear programs,‚Äù Journal of Global Optimization , vol. 3, no. 4, pp. 397‚Äì419, 1993.
[76] Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and
Edison Guo, ‚ÄúOn differentiating parameterized argmin and argmax problems with application
to bi-level optimization,‚Äù arXiv preprint arXiv:1607.05447 , 2016.
[77] Shoham Sabach and Shimrit Shtern, ‚ÄúA Ô¨Årst order method for solving convex bilevel optimiza-
tion problems,‚Äù SIAM Journal on Optimization , vol. 27, no. 2, pp. 640‚Äì660, 2017.
[78] Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang, ‚ÄúA generic Ô¨Årst-
order algorithmic framework for bi-level programming beyond lower-level singleton,‚Äù in
International Conference on Machine Learning . PMLR, 2020, pp. 6305‚Äì6315.
[79] Junyi Li, Bin Gu, and Heng Huang, ‚ÄúImproved bilevel model: Fast and optimal algorithm
with theoretical guarantee,‚Äù arXiv preprint arXiv:2009.00690 , 2020.
[80] Saeed Ghadimi and Mengdi Wang, ‚ÄúApproximation methods for bilevel programming,‚Äù arXiv
preprint arXiv:1802.02246 , 2018.
[81] Kaiyi Ji, Junjie Yang, and Yingbin Liang, ‚ÄúBilevel optimization: Nonasymptotic analysis and
faster algorithms,‚Äù arXiv preprint arXiv:2010.07962 , 2020.
[82] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang, ‚ÄúA two-timescale framework
for bilevel optimization: Complexity analysis and application to actor-critic,‚Äù arXiv preprint
arXiv:2007.05170 , 2020.
[83] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil, ‚ÄúForward and
reverse gradient-based hyperparameter optimization,‚Äù in International Conference on Machine
Learning . PMLR, 2017, pp. 1165‚Äì1173.
[84] Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo, ‚ÄúOn the iteration
complexity of hypergradient computation,‚Äù in International Conference on Machine Learning .
PMLR, 2020, pp. 3748‚Äì3758.
15

--- PAGE 16 ---
[85] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots, ‚ÄúTruncated back-
propagation for bilevel optimization,‚Äù in The 22nd International Conference on ArtiÔ¨Åcial
Intelligence and Statistics . PMLR, 2019, pp. 1723‚Äì1732.
[86] Yihua Zhang, Guanhuan Zhang, Prashant Khanduri, Mingyi Hong, Shiyu Chang, and Sijia Liu,
‚ÄúRevisiting and advancing fast adversarial training through the lens of bi-level optimization,‚Äù
arXiv preprint arXiv:2112.12376 , 2021.
[87] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine, ‚ÄúMeta-learning
with implicit gradients,‚Äù in Advances in Neural Information Processing Systems , 2019, pp.
113‚Äì124.
[88] W Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein, ‚ÄúMetapoison:
Practical general-purpose clean-label data poisoning,‚Äù arXiv preprint arXiv:2004.00225 , 2020.
[89] Hanxiao Liu, Karen Simonyan, and Yiming Yang, ‚ÄúDarts: Differentiable architecture search,‚Äù
arXiv preprint arXiv:1806.09055 , 2018.
[90] Zhangyu Chen, Dong Liu, Xiaofei Wu, and Xiaochun Xu, ‚ÄúResearch on distributed renewable
energy transaction decision-making based on multi-agent bilevel cooperative reinforcement
learning,‚Äù 2019.
[91] Xuefei Ning, Tianchen Zhao, Wenshuo Li, Peng Lei, Yu Wang, and Huazhong Yang, ‚ÄúDsa:
More efÔ¨Åcient budgeted pruning via differentiable sparsity allocation,‚Äù in European Conference
on Computer Vision . Springer, 2020, pp. 592‚Äì607.
[92] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li, ‚ÄúLearning structured sparsity
in deep neural networks,‚Äù Advances in neural information processing systems , vol. 29, 2016.
[93] Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad, and Yanzhi
Wang, ‚ÄúA systematic dnn weight pruning framework using alternating direction method of
multipliers,‚Äù in Proceedings of the European Conference on Computer Vision (ECCV) , 2018,
pp. 184‚Äì199.
[94] Torsten HoeÔ¨Çer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste, ‚ÄúSparsity
in deep learning: Pruning and growth for efÔ¨Åcient inference and training in neural networks,‚Äù
Journal of Machine Learning Research , vol. 22, no. 241, pp. 1‚Äì124, 2021.
[95] Hao Wang, Xiangyu Yang, Yuanming Shi, and Jun Lin, ‚ÄúA proximal iteratively reweighted
approach for efÔ¨Åcient network sparsiÔ¨Åcation,‚Äù IEEE Transactions on Computers , vol. 71, no.
1, pp. 185‚Äì196, 2020.
[96] Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et al., ‚ÄúOptimization
with sparsity-inducing penalties,‚Äù Foundations and Trends ¬Æin Machine Learning , vol. 4, no.
1, pp. 1‚Äì106, 2012.
[97] Uri Shaham, Yutaro Yamada, and Sahand Negahban, ‚ÄúUnderstanding adversarial train-
ing: Increasing local stability of neural nets through robust optimization,‚Äù arXiv preprint
arXiv:1511.05432 , 2015.
[98] Xiang Deng and Zhongfei Mark Zhang, ‚ÄúIs the meta-learning idea able to improve the
generalization of deep neural networks on the standard supervised learning?,‚Äù in 2020 25th
International Conference on Pattern Recognition (ICPR) . IEEE, 2021, pp. 150‚Äì157.
[99] Imre Csisz√°r, ‚ÄúInformation geonetry and alternating minimization procedures,‚Äù Statistics and
decisions , vol. 1, pp. 205‚Äì237, 1984.
[100] Motasem Alfarra, Adel Bibi, Hasan Hammoud, Mohamed Gaafar, and Bernard Ghanem, ‚ÄúOn
the decision boundaries of neural networks: A tropical geometry perspective,‚Äù arXiv preprint
arXiv:2002.08838 , 2020.
[101] Chelsea Finn, Pieter Abbeel, and Sergey Levine, ‚ÄúModel-agnostic meta-learning for fast
adaptation of deep networks,‚Äù arXiv preprint arXiv:1703.03400 , 2017.
16

--- PAGE 17 ---
[102] A. Krizhevsky and G. Hinton, ‚ÄúLearning multiple layers of features from tiny images,‚Äù
Master‚Äôs thesis, Department of Computer Science, University of Toronto , 2009.
[103] Ya Le and Xuan Yang, ‚ÄúTiny imagenet visual recognition challenge,‚Äù CS 231N , vol. 7, no. 7,
pp. 3, 2015.
[104] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, ‚ÄúImagenet: A large-
scale hierarchical image database,‚Äù in Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on . IEEE, 2009, pp. 248‚Äì255.
[105] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, ‚ÄúDeep residual learning for
image recognition,‚Äù in Proceedings of the IEEE conference on computer vision and pattern
recognition , 2016, pp. 770‚Äì778.
[106] Karen Simonyan and Andrew Zisserman, ‚ÄúVery deep convolutional networks for large-scale
image recognition,‚Äù arXiv preprint arXiv:1409.1556 , 2014.
17

--- PAGE 18 ---
Appendix
A Derivation of (3)
Based on the fact that the (m)is satisÔ¨Åed with the stationary condition of the lower-level objective
function in (2), we obtain
rg(m;) =r`(m) +=0; (A1)
where for ease of notation, we omit the dependence of (m)w.r.t.m. We then take derivative of
the second equality of (A1) w.r.t. mby using the implicit function theory. This leads to
r2
m`(m) +d(m)
dmr2
`(m) +d(m)
dm=0;
=)d(m)
dm= r2
m`(m)[r2
`(m) +I] 1: (A2)
B B IP Algorithm Details
Upper 
Level: 
Sparsification
Lower 
Level: 
Retraining
SPGD
SGD
Mask 
0
Initiate
SPGD
Dense 
Model
SGD
...
...
Weight 
1
Mask 
T
Original
Sparse 
Model
Pruned
Mask 
1
Weight 
2
...
Figure A1: Overview of the BIPpruning algorithm. The BIPalgorithm iteratively carry out model retraining
in the lower level and pruning in the upper level. In the plots, SGD refers to the lower-level stochastic gradient
descent update and SPGD refers to the upper-level stochastic projected gradient descent. Masks may vary
between each iteration, and the pruned weights are indicated using the light gray color. Different colors of
the edges in the neural networks refer to the weight update. The arcs in this Ô¨Ågure represent the data Ô¨Çow of
weights/connections.
At iterationtof B IP, there are two main steps:
HLower-level SGD for model retraining : Given m(t 1),(t 1), andz(t 1):=m(t 1)(t 1),
we update (t)by applying SGD (stochastic gradient descent) to the lower-level problem of (1),
(t)=(t 1) rg(m(t 1);(t 1))(7)=(t 1) [m(t 1)r z`(z)jz=z(t 1)+(t 1)];(-step)
where>0is the lower-level learning rate.
HUpper-level SPGD for pruning : Given m(t 1),(t), andz(t+1=2):=m(t 1)(t), we update
musing PGD (projected gradient descent) along the IG-enhanced descent direction (2),
m(t)=PS
m(t 1) d`(m(t))
dmjm=m(t 1)
(6)=PS
m(t 1) 
(t) 1
m(t 1)r z`(z)jz=z(t+1=2)
r z`(z)jz=z(t+1=2)
;(m-step)
where >0is the upper-level learning rate, and PS()denotes the Euclidean projection onto the
constraint setSgiven byS=fmjm2f0;1gn;1Tmkgin (1) and is achieved by the top- k
hard-thresholding operation as will be detailed below.
Implementation of discrete optimization. In the actual implementation, we use em(t)2[0;1]d
and obtain m(t)2f0;1gdwhere m(t) PSem(t)
. The ( m-step) is then implemented as the
following:
18

--- PAGE 19 ---
Table A1: Dataset and model setups. The following parameters are shared across all the methods.
SettingsCIFAR-10 CIFAR-100 Tiny-ImageNet ImageNet
RN-18 RN-20 RN-56 VGG-16 RN-18 RN-20 RN-18 RN-50
Batch Size 64 64 64 64 64 64 32 1024
Model Size 11:22M 0:27M 0:85M 14:72M 11:22M 0:27M 11:22M 25:56M
em(t)=em(t 1) 
(t) 1
em(t 1)r z`(z)jz=z(t+1=2)
r z`(z)jz=z(t+1=2) (em-step)
and then m(t) PSem(t)
, with z(t+1=2):=m(t 1)(t)as deÔ¨Åned above.
Algorithm A1 BIP
1:Initialize: Model 0, pruning mask score m0, binary mask z, sparse ratio p%, regularization
parameter, upper- and lower-level learning rate and.
2:forIterationt= 0;1;:::; do
3: Pick different random data batches BandBfor different levels of tasks.
4: Lower-level : Update model parameters using data batch Bvia SGD calling:
t+1=t d`tr(m)
d
m=z;=t(A3)
5: Upper-level : Update pruning mask score using data batch Bvia SGD calling:
mt+1=mt 
rm`tr(m) 1
rz`tr(z)jz=mr `tr(m)
m=mt;=t+1(A4)
6: Update the binary mask z: Hard-threshold the mask score mwith the give sparse ratio p:
z=Tf0;1gd(mt+1;s): (A5)
7:end for
C Additional Experimental Details and Results
C.1 Datasets and Models
Our dataset and model choices follow the pruning benchmark in [ 22]. We summarize the datasets
and model conÔ¨Ågurations in Tab. A1. In particular, we would like to stress that we adopt the
ResNet-18 with convolutional kernels of 33in the Ô¨Årst layer for Tiny-ImageNet, aligned with
CIFAR-10 and CIFAR-100, compared to ImageNet ( 77). See https://github.com/kuangliu/
pytorch-cifar/blob/master/models/resnet.py for more details.
C.2 Detailed Training Settings
Baselines. For both unstructured and structured pruning settings, we consider four baseline methods
across various pruning categories, including IMP [ 17], OMP [ 17],HYDRA [9] and GRASP [23].
ForHYDRA andGRASP , we adopt the original setting as well as hyper-parameter choices on
their ofÔ¨Åcial code repositories. For IMP and OMP, we adopt the settings from the current SOTA
implementations [ 22]. Details on the pruning schedules can be found in Tab. A2. In particular,
HYDRA prunes the dense model to the desired sparsity with 100 epoch for pruning and 100 epoch
for retraining. GRASP conducts one-shot pruning to the target sparsity, followed by the 200-epoch
retraining. In each pruning iteration, IMP prunes 20% of the remaining parameters before 160-epoch
retraining. HYDRA adopts the cosine learning rate scheduler for both pruning and retraining stage.
The learning rate scheduler for IMP, OMP, and GRASP is the step learning rate with a learning rate
decay rate of 0.1 at 50% and 75% epochs. The initial learning rate for all the methods are 0.1.
19

--- PAGE 20 ---
Table A2: Computation complexities of different pruning methods on (CIFAR-10, ResNet-18) in unstructured
pruning setting. The training epoch numbers of pruning/retraining baselines are consistent with their ofÔ¨Åcial
settings or the latest benchmark implementations. All the evaluations are based on a single Tesla-V100 GPU.
Runtime v.s. targeted sparsityTraining epoch #MethodnSparsity 20% 59% 83:2% 95:6%
IMP 69 min 276 min 621 min 966 min 160 epoch retrain
GRASP 89 min 200 epoch retrain
OMP 69 min 160 epoch retrain
HYDRA 115 min100 epoch prune
100 epoch retrain
BIP 86 min 100 epoch
Table A3: Detailed training details for each method. All the baselines adopt the recommended settings either
from the ofÔ¨Åcial or their latest benchmark ( e.g., LTH[ 22]) for a fair comparison. Note by default setting, only
our method B IP do not require additional epochs for retraining.
Method Epoch NumberInitial
Learning RateLearning Rate
SchedulerLearning Rate
Decay FactorLearning Rate
Decay EpochMementumWeight
DecayRewind
EpochWarm-up
IMP 160 for Retrain 0.1 Step LR 10 80/120 0.9 5.00E-04 8 75 for VGG16
OMP 160 for Retrain 0.1 Step LR 10 80/120 0.9 5.00E-04 8 75 for VGG16
HYDRA100 for Prune
100 for Retrain0.1 Cosine LR N/A N/A 0.9 5.00E-04 N/A 75 for VGG16
GRASP 200 for Retrain 0.1 Step LR 10 100/150 0.9 5.00E-04 N/A 75 for VGG16
BIP 100 0.1 for m; 0.01 for  Cosine LR N/A N/A 0.9 5.00E-04 N/A 75 for VGG16
Hyper-parameters for B IP. In both structured and unstructured pruning settings, cosine learning
rate schedulers are adopted, and BIPtakes an initial learning rate of 0.1 for the upper-level problem
(pruning) and 0.01 for the lower-level problem (retraining). The lower-level regularization coefÔ¨Åcient
is set to 1:0throughout the experiments. By default, we only take one SGD step for lower-level
optimization in all settings. Ablation studies on different SGD steps for lower-level optimization can
be found in Fig. A8(b) and Fig. A14. We use 100 training epochs for BIP, and ablation studies on
different training epochs for larger pruning ratios can be found in Fig. A15.
0 20 40 60 80 100
Pruning Ratio (%)89.089.590.090.591.091.592.092.5 Test Accuracy (%)
Dense Model
IMP
OMP
IMP w/ 20 epochs
IMP w/ 40 epochs
IMP w/ 80 epochs
20 40 60 80 100
Pruning Ratio (%)102103Time Consumption (min)
Figure A2: Performance comparisons among OMP, IMP, and IMP with less retraining epochs on CIFAR-10
with ResNet-20.
Structured pruning. To differentiate of the Ô¨Ålter-wise and channel-wise structured pruning setting,
we illustrate the details of these settings in Fig. A3. Note, the Ô¨Ålter-wise pruning setting prunes the
output dimension (output channel) of the parameters in one layer, while the channel-wise prunes the
input dimension (input channel).
C.3 Additional Experiment Results
Comparison with IMP using reduced retraining epochs. As IMP is signiÔ¨Åcantly more time-
consuming than one-shot pruning methods, a natural way to improve the efÔ¨Åciency is to decrease the
retraining epoch numbers at each pruning cycle. In Fig. A2, the performance and time consumption of
20

--- PAGE 21 ---
(a) Filter-wise Pruning (b) Channel-wise Pruning
Figure A3: Illustration of Ô¨Ålter-wise pruning and channel-wise pruning. The blocks in the middle column in (a)
and (b) represent the parameters (Ô¨Ålters) of the ith convolutional layer, where the red ones represent the pruning
unit in each setting. The left blocks in gray denote the input feature maps and the right columns denote the
output feature maps generated by the corresponding Ô¨Ålters marked in the same color.
IMP using 20, 40, and 80 epochs at each retraining cycle are presented. The results and conclusions
are in general aligned with Fig. 2. First, with fewer epoch numbers, the time consumption decreases
at the cost of evident performance degradation. Second , IMP with fewer epoch numbers are unable
to obtain winning tickets. Thus, the direct simpliÔ¨Åcation of IMP would hamper the pruning accuracy.
This experiment shows the difÔ¨Åculty of achieving efÔ¨Åcient and effective pruning under the scope of
heuristics-based pruning, and thus justiÔ¨Åes the necessity in developing a more powerful optimization-
based pruning method.
0 20 40 60 80 100
Pruning Ratio (%)888990919293 Test Accuracy (%)
Dense Model
Hydra
IMP
OMP
Grasp
SynFlow
Snip
BiP
Best Winning Ticket
Figure A4: Unstructured pruning performance of BIPvs. prune-at-initialization baselines on (CIFAR-10,
ResNet-20).
Comparison with more prune-at-initialization baselines. In Fig. A4, we include more heuristics-
based one-shot pruning baselines ( SYNFLOW [24],SNIP[21]) for comparison. Together with GRASP ,
these methods belong to the category of pruning at initialization, which determines the sparse sub-
networks prior to training. As we can see, the advantage of our method over the newly added methods
are clear, and the beneÔ¨Åt becomes more signiÔ¨Åcant as the sparsity increases. This further demonstrates
the superiority of the optimization-basis of B IP over the heuristics-based one-shot methods.
Experiments on unstructured pruning with more baselines. We compare our proposed method
BIPto more baselines on different datasets and architectures in Fig. A5. We add two more baselines,
including EARLY BIRD[28] and PROSPR[25]. The results show that PROSPRis indeed better
than GRASP but is still not as good as IMP and our method BIPin different architecture-dataset
combinations. Meanwhile, except for the unstructured pruning settings of ResNet18 pruning over
CIFAR10 and CIFAR100, PROSPR, as a pruning before training, can achieve comparable performance
to the state-of-the-art implementation of OMP. However, the gap between this SOTA pruning-at-
initialization method and our method still exists. Besides, the result shows that EARLY BIRDcan
effectively achieve comparable or even better testing performance than OMP in most different
21

--- PAGE 22 ---
0 20 40 60 80 100
Pruning Ratio (%)93.093.594.094.595.095.5 Test Accuracy (%)
CIFAR-10, ResNet-18
Dense Model
Hydra
IMP
OMP
Grasp
Early Bird
ProsPr
BiP
Best Winning Ticket
0 20 40 60 80 100
Pruning Ratio (%)89.089.590.090.591.091.592.092.5 Test Accuracy (%)
CIFAR-10, ResNet-20
0 20 40 60 80 100
Pruning Ratio (%)92.092.593.093.594.094.5 Test Accuracy (%)
CIFAR-10, VGG-16
0 20 40 60 80 100
Pruning Ratio (%)72.073.074.075.076.077.078.0 Test Accuracy (%)
CIFAR-100, ResNet-18
Dense Model
Hydra
IMP
OMP
Grasp
Early Bird
ProsPr
BiP
Best Winning Ticket
0 20 40 60 80 100
Pruning Ratio (%)60.062.064.066.068.070.0 Test Accuracy (%)
CIFAR-100, ResNet-20
0 20 40 60 80 100
Pruning Ratio (%)60.061.062.063.064.0 Test Accuracy (%)
Tiny-ImageNet, ResNet-18Figure A5: Unstructured pruning trajectory given by test accuracy (%) vs. pruning ratio (%). The visual
presentation setting is consistent with Fig. 4. We consider two more baseline methods: EARLY BIRD[28] and
PROSPR[25].
architecture-dataset combinations, which is also the main contribution of [ 28]. However, EARLY BIRD
is still not as strong as IMP in testing performance.
0 20 40 60 80 100
Pruning Ratio (%)91.091.592.092.593.093.594.094.595.095.5 Test Accuracy (%)
CIFAR-10, ResNet-18
Dense Model
Hydra
IMP
OMP
Grasp
ProsPr
BiP
Best Winning Ticket
0 20 40 60 80 100
Pruning Ratio (%)82.084.086.088.090.092.0 Test Accuracy (%)
CIFAR-10, ResNet-20
0 20 40 60 80 100
Pruning Ratio (%)70.071.072.073.074.075.076.077.078.0 Test Accuracy (%)
CIFAR-100, ResNet-18
0 20 40 60 80 100
Pruning Ratio (%)45.050.055.060.065.070.0 Test Accuracy (%)
CIFAR-100, ResNet-20
Figure A6: Structured pruning trajectory given by test accuracy (%) vs. pruning ratio (%). The visual
presentation setting is consistent with Fig. 6. We add P ROSPR[25] as our new baseline.
Experiments on structured pruning with more baselines. We compare our proposed method
BIPto the new baseline PROSPRon different datasets and architectures in Fig. A6. on the structured
pruning setting. As we can see, BIPconsistently outperforms PROSPRand still stands top among all
the baselines.
More results on ImageNet. In Fig. A7, we provide additional results on the dataset ImageNet with
ResNet-18 in the unstructured pruning setting, in addition to the results of (ImageNet, ResNet-50)
shown in Fig. 4. As we can see, the performance of BIPstill outperforms the strongest baseline IMP
and the same conclusion can be drawn as Fig. 4.
Sanity check of B IP on specialized hyperparameters. InFig.A8 , we show the sensitivity of BIP
to its specialized hyperparameters at lower-level optimization, including the number of SGD iterations
(N) in (-step), and the regularization parameter in (1). Fig. A8(a) shows the test accuracy of
BIP-pruned models versus the choice of N. As we can see, more SGD iterations for the lower-level
optimization do not improve the performance of BIP. This is because in BIP, the-step is initialized
by a pre-trained model which does not ask for aggressive weight updating. The best performance of
BIPis achieved at N3. We choose N= 1throughout the experiments as it is computationally
lightest. Fig. A8(b) shows the performance of BIPby varying. As we can see, the choice of
2f0:5;1gyields the best pruning accuracy across all pruning ratios. If is too small, the lack
22

--- PAGE 23 ---
0 20 40 60 80 100
Pruning Ratio (%)69.069.570.070.571.071.572.0 Test Accuracy (%)
ImageNet, ResNet-18
Dense Model
IMP
BiP
Best Winning TicketFigure A7: Unstructured pruning trajectory on ImageNet with ResNet-18. The experiment setting is consistent
with Fig. 4. We only compare B IP with our strongest baseline IMP due to limited computational resource.
1 2 3 4 5
Lower-level Step Number90919293 Test Accuracy (%)
0.1 0.5 1 10
 (Lower-level Regularization)
87888990919293 Test Accuracy (%)
20% 67% 87% 96%
Pruning Ratio (%)8990919293 Test Accuracy (%)92.48
92.14
91.28
89.8991.87
91.14
90.39
89.12Dense Model
w/o IG
w/ IG
(a) (b) (c)
Figure A8: The sensitivity of BIPto (a) the lower-level step number N, (b) the lower-level regularizer ,
and (c) the contribution of the IG-term at various pruning ratios on (CIFAR-10, ResNet-20). Each curve
or column represents a certain sparsity. In (c), we Ô¨Åx the to 1.0 and compare the performance of the
IG-involved/excluded (2) B IP.
of strong convexity of lower-level optimization would hamper the convergence. If is too large,
the lower-level optimization would depart from the true model objective and causes a performance
drop. Fig. A8(c) demonstrates the necessity of the IG enhancement in BIP. We compare BIPwith its
IG-free version by dropping the IG term in (2). We observe that BIPwithout IG (marked in blue)
leads to a signiÔ¨Åcant performance drop ( >1%) at various sparsities. This highlights that the IG in
the (m-step) plays a critical role in the performance improvements obtained by BIP, justifying our
novel BLO formulation for the pruning problem. In Fig. A9, we further demonstrate the inÔ¨Çuence of
different choices of lower-level learning rate as well as the batch size on the performance of BIP.
Fig. A9 (a) shows that the test accuracy of BIP-pruned models is not quite sensitive to the choice of
2f0:01;0:008g. A largevalue ( e.g.,>0:05) will slightly decrease the performance of BIP.
By contrast, a small is preferred due to the fact that the model parameters are updated based on the
pre-trained values. Fig. A9 (b) shows how the batch size inÔ¨Çuences the performance. As we can see,
0.005 0.008 0.01 0.03 0.05
Learning Rate 
90919293 Test Accuracy (%)
Dense Model
p=20%p=67.2%
p=86.58%
32 64 128 256 512
Batch Size90.591.091.592.092.593.0 Test Accuracy (%)
(a) (b)
Figure A9: Ablation studies of BIPon different hyper-parameters. All the experiments are based on CIFAR-10
with ResNet-20. We select three sparsity values of a wide range (from not sparse to extreme sparse) to make the
results more general. We study the inÔ¨Çuence of different (a) lower-level learning rate and (b) batch size.
23

--- PAGE 24 ---
a large batch size might hurt the stochasticity of the algorithm and thus degrades the performance.
We list our detailed batch size choices for different datasets in Tab. A1.
0 20 40 60 80 100
Pruning Ratio (%)84.086.088.090.092.094.0 Test Accuracy (%)
CIFAR-10, VGG-16
Dense Model
Hydra
IMP
OMP
Grasp
BiP
Best Winning Ticket
0 20 40 60 80 100
Pruning Ratio (%)88.089.090.091.092.093.094.0 Test Accuracy (%)
CIFAR-10, ResNet-56
Figure A10: Filter-wise pruning test accuracy (%) v.s. sparsity (%) on CIFAR-10 with VGG-16 and ResNet-56.
Additional structured pruning experiments. In addition to Ô¨Ålter pruning in Fig. 6, we provide
more results in the structured pruning setting, including both Ô¨Ålter-wise and channel-wise pruning (as
illustrated in Fig. A3). In Fig. A10, results on CIFAR-10 with VGG-16 and ResNet-56 are added as
new experiments compared to Fig. 6. Fig. A11 shows the results of the channel-wise pruning. As we
can see, consistent with Fig. 6, BIPis able to Ô¨Ånd the best winning tickets throughout the experiments
while it is difÔ¨Åcult for IMP to Ô¨Ånd winning tickets in most cases. We also notice that HYDRA , as the
optimization-based baseline, serves as a strong baseline in Ô¨Ålter-wise pruning. It also indicates the
superiority of the optimization-based methods over the heuristics-based ones in dealing with more
challenging pruning settings.
0 20 40 60 80 100
Pruning Ratio (%)89.090.091.092.093.094.095.0 Test Accuracy (%)
CIFAR-10, ResNet-18
Dense Model
Hydra
IMP
OMP
Grasp
BiP
Best Winning Ticket
0 20 40 60 80 100
Pruning Ratio (%)82.084.086.088.090.092.0 Test Accuracy (%)
CIFAR-10, ResNet-20
0 20 40 60 80 100
Pruning Ratio (%)88.089.090.091.092.093.094.0 Test Accuracy (%)
CIFAR-10, ResNet-56
0 20 40 60 80 100
Pruning Ratio (%)70.071.072.073.074.075.076.077.078.0 Test Accuracy (%)
CIFAR-100, ResNet-18
0 20 40 60 80 100
Pruning Ratio (%)56.058.060.062.064.066.068.070.0 Test Accuracy (%)
CIFAR-100, ResNet-20
0 20 40 60 80 100
Pruning Ratio (%)86.087.088.089.090.091.092.093.094.0 Test Accuracy (%)
CIFAR-10, VGG-16
Figure A11: Channel-wise pruning test accuracy (%) v.s. sparsity (%). Settings are consistent with Fig. A10.
Training trajectory of B IP.We show in Fig. A12 that the BIPalgorithm converges quite well
within 100 training epochs using a cosine learning rate scheduler.
The training trajectory of the mask IoU score. To verify the argument that the mask also con-
verges at the end of the training, we show the training trajectory of the mask similarity between two
adjacent-epoch models in Fig. A13 at different pruning ratios. Here the mask similarity is represented
through the intersection of the union (IoU) score of the two masks found by two adjacent epochs. The
IoU score ranges from 0.0 to 1.0, and a higher IoU implies a larger similarity between the two masks.
24

--- PAGE 25 ---
0 20 40 60 80 100
Epoch Number80859095100 Accuracy (%)
Training Acc
Test AccFigure A12: The training trajectory of BIPfor unstructured pruning on (CIFAR-10, ResNet-18) with a pruning
ratio ofp= 80% .
0 20 40 60 80 100
Epoch Number0.700.750.800.850.900.951.00 IoU score
p=20%
p=59%
p=86.58%
Figure A13: Training trajectory of the IoU (intersection over union) score between the masks of two adjacent
epochs. We show the trajectory of different pruning ratios.
As we can see, the IoU score converges to 1.0 in the end, which denotes that the mask also converges
at the end of the training phase. Also, with a smaller pruning ratio, the mask turns to converge more
quickly.
0 20 40 60 80 100
Epoch Number86889092949698100 Accuracy (%)Training Accuracy
Step=1
Step=3
Step=5
0 20 40 60 80 100
Epoch Number82848688909294 Accuracy (%)Test Accuracy
Step=1
Step=3
Step=5
Figure A14: Training dynamics of BIPwith different lower-level (SGD) steps on (CIFAR-10, ResNet-18) with
the pruning ratio of p=80%.
The effect of different lower-level steps in B IP on the training dynamics. We conduct additional
experiments to demonstrate the effectiveness of using one-step SGD in BIP. In our new experiments,
we consider the number of SGD steps, 1, 3, and 5. We report the training trajectories of BIPin
Fig. A14. As we can see, the use of multi-step SGD accelerates model pruning convergence at its
early phase. Yet, if we run BIPfor a sufÔ¨Åcient number of epochs (we used 100 by default in other
experiments), the Ô¨Ånal test accuracy of using different SGD settings shows little difference. Although
25

--- PAGE 26 ---
the use of multiple SGD steps could improve the convergence speed, it introduces extra computation
complexity per BLO step. Thus, from the overall computation complexity perspective, using 1 SGD
step but running more epochs is advantageous in practice.
50 100 200 300 400 500
Epoch Number767880828486889092 Test Accuracy of BiP (%)
CIFAR10, ResNet20
Dense Model
p=86.58%p=94.50%
p=97.75%
50 100 200 300 400 500
Epoch Number86878889909192939495 Test Accuracy of BiP (%)
CIFAR10, ResNet18
Dense Model
p=86.58%p=94.50%
p=97.75%
50 100 200 300 400 500
Epoch Number64666870727476 Test Accuracy of BiP (%)
CIFAR100, ResNet18
Dense Model
p=86.58%p=94.50%
p=97.75%
Figure A15: The effect of total training epoch number on the test accuracy with large pruning ratios. The epoch
number is by default set to 100 in this paper. In each sub-Ô¨Ågure, we report the performance of BIPwith three
different sparse ratios p.
The effect of larger training epoch numbers with extreme sparsities. We allow more time
(training epochs) for BIPwhen a higher pruning rate is considered and the results are shown in
Fig. A15. SpeciÔ¨Åcally, we test three datasets and consider three pruning ratios (p=86.58%, 94.50%,
97.75%). For each pruning ratio, we examine the test accuracy of BIPversus the training epoch
number from 50 to 500. Note that the number of training epochs in our original experiment setup was
set to 100. As we can see, the performance of BIPgets saturated when the epoch number is over 100.
Thus, even for a higher pruning ratio, the increase of training epoch number over 100 does not gain
much improvement in accuracy.
The effect of different training batch schemes. We conducted ablation studies on three different
schemes of BIP‚Äôs training batches for the upper and lower level. In addition to two different random
batches for the two levels, we also consider the same batch and the reverse batch scheme. BIP(same
batch) always uses the same data batches for the two levels in each iteration while BIP(reverse batch)
uses the data batches in a reversed order for the two level. Fig. A16 shows that both the random batch
scheme (i.e., BIP) and the reverse batch scheme can bring a better testing accuracy performance than
the same batch scheme throughout different pruning ratio settings. Fig. A17 shows that BIPthe same
batch scheme converges slower compared to the other two. Both of the results indicate BIPbeneÔ¨Åts
from the diverse batch selection.
0 20 40 60 80 100
Pruning Ratio (%)94.094.294.494.694.895.095.295.4 Test Accuracy (%)
CIFAR-10, ResNet-18
Dense Model
IMP
BiP
BiP (reverse batch)
BiP (same batch)
Best Winning Ticket
0 20 40 60 80 100
Pruning Ratio (%)75.075.576.076.577.077.578.0 Test Accuracy (%)
CIFAR-100, ResNet-18
Dense Model
IMP
BiP
BiP (reverse batch)
BiP (same batch)
Best Winning Ticket
Figure A16: The effect of different training batch schemes on the performance of BIP. We consider two
different variants of BIPdenoted as BIP(reverse batch) and BIP(same batch). For BIP(reverse batch), the data
batches are fed into the upper- and lower-step in a reversed order within each epoch, while for BIP(same batch),
the data batches for upper- and lower-level are always the same. Experiment settings are consistent with Fig. 4.
For better readability, we only plot the strongest baseline IMP for comparison.
26

--- PAGE 27 ---
0 20 40 60 80 100
Epoch Number86889092949698100 Accuracy (%)Training Accuracy
BiP
BiP (reverse batch)
BiP (same batch)
0 20 40 60 80 100
Epoch Number77.580.082.585.087.590.092.595.0 Accuracy (%)Test Accuracy
BiP
BiP (reverse batch)
BiP (same batch)Figure A17: The effect of different training batch schemes on the training dynamics of BIP. We plot the
training dynamics of different variants of B IP on (CIFAR-10, ResNet-18) with the pruning ratio of 80%.
Table A4: The sparsest winning tickets found by different methods on Tiny-ImageNet and ImageNet datasets.
Winning tickets refer to the sparse models with an average test accuracy no less than the dense model [ 20].
In each cell, p%(accstd%) represents the sparsity as well as the test accuracy. The test accuracy of dense
models can be found in the header. 7signiÔ¨Åes that no winning ticket is found by a pruning method. Given the
data-model setup ( i.e., per column), the sparsest winning ticket is highlighted in bold .
MethodTiny-ImageNet ImageNet
ResNet-18 ResNet-18 ResNet-50
(63.83%) (70.89%) (75.85%)
IMP 20% (64.170.11%) 74% (71.150.19%) 80% (76.050.13%)
OMP 20% (64.170.11%) 7 7
GRASP 7 7 7
HYDRA 7 7 7
BIP 36% (64.290.13%) 83% (70.950.12%) 74% (76.090.11%)
C.4 Broader Impact
We do not recognize any potential negative social impacts of our work. Instead, we believe our work
can inspire many techniques for model compression. The Ô¨Ånding of structure-aware winning ticket
also beneÔ¨Åts the design of embedded solutions to deploying large-scale models on resource-limited
edge devices ( e.g., FPGAs), providing broader impact on both scientiÔ¨Åc research and practical
applications ( e.g., autonomous vehicles).
27
