# 2305.18513.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2305.18513.pdf
# Kích thước tệp: 314386 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2305.18513v1  [cs.CL]  29 Tháng 5 2023SLIMFIT: Tinh chỉnh hiệu quả bộ nhớ của
các mô hình dựa trên Transformer sử dụng động lực học huấn luyện
Arash Ardakani1Altan Haan1Shangyin Tan1Doru Thom Popovici2
Alvin Cheung1Costin Iancu2Koushik Sen1
Đại học California, Berkeley1Phòng thí nghiệm Quốc gia Lawrence Berkeley2
{arash.ardakani,altanh,shangyin,akcheung,ksen}@berkeley.edu
{dtpopovici,cciancu}@lbl.gov
Tóm tắt
Các mô hình dựa trên Transformer, như BERT và ViT, đã đạt được kết quả tiên tiến nhất trên các tác vụ xử lý ngôn ngữ tự nhiên (NLP) và thị giác máy tính (CV) khác nhau. Tuy nhiên, các mô hình này cực kỳ tốn bộ nhớ trong quá trình tinh chỉnh, khiến chúng khó triển khai trên các GPU có tài nguyên bộ nhớ hạn chế. Để giải quyết vấn đề này, chúng tôi giới thiệu một công cụ mới có tên SLIMFIT giúp giảm yêu cầu bộ nhớ của các mô hình này bằng cách phân tích động lực học huấn luyện của chúng một cách động và đóng băng các lớp ít đóng góp trong quá trình tinh chỉnh. Các lớp cần đóng băng được chọn bằng thuật toán lập lịch liên lớp thời gian thực. SLIMFIT áp dụng lượng tử hóa và cắt tỉa cho các lớp cụ thể để cân bằng tải của các kích hoạt động và giảm thiểu dấu chân bộ nhớ của các kích hoạt tĩnh, trong đó kích hoạt tĩnh đề cập đến những kích hoạt không thể loại bỏ bất kể việc đóng băng. Điều này cho phép SLIMFIT đóng băng lên đến 95% các lớp và giảm tổng mức sử dụng bộ nhớ GPU trên thiết bị của các mô hình dựa trên transformer như ViT và BERT trung bình 2.2×, trên các benchmark/dataset NLP và CV khác nhau như GLUE, SQuAD 2.0, CIFAR-10, CIFAR-100 và ImageNet với độ suy giảm độ chính xác trung bình 0.2%. Đối với các tác vụ NLP và CV như vậy, SLIMFIT có thể giảm lên đến 3.1× tổng mức sử dụng bộ nhớ trên thiết bị với độ suy giảm độ chính xác chỉ lên đến 0.4%. Kết quả là, trong khi việc tinh chỉnh ViT trên ImageNet và BERT trên SQuAD 2.0 với kích thước batch 128 cần 3 và 2 GPU 32GB tương ứng, SLIMFIT cho phép tinh chỉnh chúng trên một GPU 32GB duy nhất mà không có bất kỳ suy giảm độ chính xác đáng kể nào. Mã nguồn của bài báo này có sẵn tại https://github.com/arashardakani/SlimFit.

1 Giới thiệu
Trong vài năm qua, nhiều mô hình dựa trên transformer đã được phát triển với việc áp dụng cơ chế attention để đánh trọng số tầm quan trọng của từng phần dữ liệu đầu vào khác nhau. Việc tiền huấn luyện các mô hình dựa trên transformer như vậy trên dữ liệu lớn đã dẫn đến sự cải thiện đáng kể về độ chính xác khi được tinh chỉnh trên các tác vụ downstream xử lý ngôn ngữ tự nhiên (NLP) và thị giác máy tính (CV) khác nhau [1, 2]. Mặc dù có hiệu suất tuyệt vời trong việc đạt được độ chính xác tiên tiến nhất (SOTA), các mô hình này tốn nhiều bộ nhớ và yêu cầu một lượng lớn bộ nhớ GPU trên thiết bị trong giai đoạn tinh chỉnh so với các mạng nơ-ron tích chập và hồi quy thông thường [3]. Yêu cầu bộ nhớ của các mô hình dựa trên transformer hiện tại đã khiến chúng khó tinh chỉnh ngay cả trên các GPU mạnh mẽ. Với việc giới thiệu các mô hình dựa trên transformer lớn hơn trong vài năm qua, bộ nhớ GPU trên thiết bị đã trở thành nút thắt cổ chai chính cho quá trình tinh chỉnh của chúng [3, 4, 5].

Tổng mức sử dụng bộ nhớ trên thiết bị của GPU chủ yếu bao gồm các kích hoạt, tham số, gradient, trạng thái tối ưu hóa và ngữ cảnh CUDA. Trong số các yếu tố này, kích hoạt chiếm phần lớn mức sử dụng bộ nhớ do batching, khiến chúng lớn hơn các yếu tố khác vài bậc độ lớn (xem Hình 1). Do đó, huấn luyện nén kích hoạt (ACT) đã nổi lên như giải pháp chính cho việc tinh chỉnh hiệu quả bộ nhớ [6, 4]. Phương pháp này đầu tiên nén kích hoạt trong quá trình forward pass và sau đó giải nén chúng trong quá trình backward pass. Theo cách này, dấu chân bộ nhớ có thể

--- TRANG 2 ---
Kích thước Batch: 32 Kích thước Batch: 64 Kích thước Batch: 128051015
6.1
9.1
15.43.2
6.4
12.84·10−1
4·10−1
4·10−12.4
2.3
2.2Bộ nhớ (GByte)
Tổng Kích hoạt Tham số Khác
Hình 1: Phân tích mức sử dụng bộ nhớ của BERT khi được tinh chỉnh trên các kích thước batch khác nhau bao gồm 32, 64, và 128.được giảm đáng kể bằng cách cache các kích hoạt đã nén. Trong ACT, lượng tử hóa [7, 8, 6, 4] đã là lựa chọn phổ biến để nén kích hoạt so với các bộ nén khác như JPEG [9] hoặc cắt tỉa [5]. ACT SOTA hiện tại gán một cách thích ứng số bit lượng tử hóa cho từng lớp đối với một kiến trúc nhất định [4].

Mặc dù ACT SOTA thành công trong việc giảm dấu chân bộ nhớ của kích hoạt, việc giảm tổng bộ nhớ GPU trên thiết bị của nó không đáng kể. Ví dụ, việc giảm tổng bộ nhớ GPU trên thiết bị của ACT SOTA chỉ giới hạn ở 0.1GB mặc dù giảm 6.4× bộ nhớ kích hoạt khi tinh chỉnh BERT trên dataset CoLA với kích thước batch 32. Đáng chú ý rằng chúng tôi đề cập đến mức sử dụng bộ nhớ được báo cáo bởi "nvidia-smi" như bộ nhớ tổng thể trên thiết bị trong bài báo này (xem Phụ lục A để biết thêm thông tin về quản lý bộ nhớ).

Tensor rematerialization [3, 10, 11, 12], còn được gọi là gradient checkpointing, là một phương pháp nổi bật khác để giảm bộ nhớ kích hoạt bằng cách đánh đổi tính toán lấy bộ nhớ. Trong tensor rematerialization, chỉ những kích hoạt cụ thể được lưu trữ trong forward pass, trong khi phần còn lại được tính toán lại trong backward pass. Tất nhiên, việc tính toán lại kích hoạt yêu cầu nhiều phép toán hơn và kéo dài đáng kể quá trình tinh chỉnh [4]. Huấn luyện độ chính xác giảm, như một phương pháp khác, thực hiện các tính toán của cả forward và backward pass ở độ chính xác thấp [13, 14, 15, 16]. Mặc dù các nghiên cứu này có thể huấn luyện thành công các mô hình thông thường, việc tinh chỉnh mô hình few-bit không đơn giản. Ví dụ, lượng tử hóa 8-bit của BERT cho suy luận dẫn đến mất độ chính xác đáng kể [17], khiến việc tinh chỉnh trên few bits trở thành nhiệm vụ thách thức.

Low-rank adaptation (LoRA) [18] là một phương pháp chính khác để giảm tổng bộ nhớ GPU trên thiết bị nơi các mô hình dựa trên transformer được tinh chỉnh bằng cách chèn một số lượng nhỏ tham số có thể huấn luyện vào mỗi lớp trong khi giữ các tham số mô hình đã tiền huấn luyện bị đóng băng. Phương pháp như vậy cho phép tinh chỉnh các mô hình dựa trên transformer với số lượng tham số có thể huấn luyện ít hơn đáng kể, dẫn đến giảm dấu chân bộ nhớ của trạng thái tối ưu hóa và gradient. Việc giảm bộ nhớ như vậy trở nên đáng kể đối với các mô hình transformer cực lớn như GPT [19] với hơn một trăm tỷ tham số.

Khác với các phương pháp này, chúng tôi đưa ra một phương pháp mới để giảm tổng mức sử dụng bộ nhớ trên thiết bị bằng cách phân tích động lực học huấn luyện. Cụ thể hơn, chúng tôi phân tích động các đóng góp gradient của các lớp trong các mô hình dựa trên transformer và thực hiện cập nhật tham số chỉ cho các lớp cụ thể trong khi các lớp còn lại được giữ đóng băng. Động lực học huấn luyện đã được sử dụng để phân tích hành vi của một mô hình trong quá trình huấn luyện/tinh chỉnh [20, 21, 22]. Tuy nhiên, công việc của chúng tôi sử dụng động lực học huấn luyện để phát hiện và loại bỏ các kích hoạt không quan trọng trong quá trình tinh chỉnh bằng cách đóng băng các lớp liên quan của chúng, dẫn đến giảm dấu chân bộ nhớ. Phương pháp của chúng tôi trực giao với các phương pháp hiện có bao gồm rematerialization và LoRA, có thể được kết hợp để giảm thêm.

Việc đóng băng các lớp hoặc tham số đã được nghiên cứu trong các lĩnh vực khác nhau, bao gồm các mô hình dựa trên transformer để bảo tồn thông tin đã học trước đó trong quá trình tinh chỉnh [23]. Việc đóng băng tham số cũng đã được sử dụng để điều chỉnh việc tinh chỉnh (ví dụ: giảm over-fitting) trong các mô hình đã tiền huấn luyện [24]. Gần đây, việc đóng băng đã được sử dụng để tăng tốc tinh chỉnh bằng cách đóng băng dần các khối mô hình [25, 26, 27]. Tuy nhiên, vì phương pháp như vậy bắt đầu quá trình tinh chỉnh mà không đóng băng ít nhất trong vài lần lặp huấn luyện, yêu cầu bộ nhớ tổng thể trên thiết bị của nó vẫn tương tự như huấn luyện không đóng băng. Ví dụ, việc tinh chỉnh ViT trên ImageNet với kích thước batch 128 sử dụng phương pháp đóng băng như vậy trên một GPU 32GB duy nhất dẫn đến lỗi hết bộ nhớ (xem Phụ lục B để biết thêm chi tiết).

Để điều phối các quyết định đóng băng lớp hiệu quả, chúng tôi giới thiệu thuật toán lập lịch liên lớp thời gian thực (ILS). Phương pháp của chúng tôi tìm và đóng băng một tập hợp các lớp trong các mô hình dựa trên transformer có ít đóng góp, tức là các lớp có ít cập nhật hơn trong tham số của chúng, cho quá trình tinh chỉnh tại mỗi lần lặp. Mặc dù thuật toán ILS thành công trong việc phát hiện và đóng băng các lớp không quan trọng, việc giảm bộ nhớ của nó không tỷ lệ với tỷ lệ đóng băng. Lý do đằng sau sự không tỷ lệ này có hai mặt: số lượng kích hoạt không cân bằng giữa các lớp và sự tồn tại của kích hoạt tĩnh. Kích hoạt tĩnh đề cập đến những kích hoạt không thể loại bỏ bất kể việc đóng băng (ví dụ: kích hoạt của các hàm phi tuyến như GELU). Chúng tôi giải quyết hai vấn đề này bằng cách sử dụng lượng tử hóa và cắt tỉa để cân bằng số lượng kích hoạt trên tất cả các lớp và giảm chi phí bộ nhớ của kích hoạt tĩnh. Chúng tôi sử dụng lượng tử hóa và cắt tỉa cho một vài lớp cụ thể của các mô hình dựa trên transformer trái ngược với các phương pháp huấn luyện độ chính xác giảm nơi tất cả các lớp đều được lượng tử hóa. Kết quả là, tác động của lượng tử hóa và cắt tỉa lên độ chính xác là không đáng kể trong công việc của chúng tôi. Ví dụ, độ suy giảm độ chính xác do lượng tử hóa và cắt tỉa chỉ là 0.1% trên dataset MRPC.

--- TRANG 3 ---
Bằng cách kết hợp ILS với lượng tử hóa và cắt tỉa, chúng tôi giới thiệu một công cụ hiệu suất có tên SLIMFIT để giảm mức sử dụng bộ nhớ GPU trên thiết bị của các mô hình dựa trên transformer trong quá trình tinh chỉnh. Chúng tôi chứng minh hiệu quả của SLIMFIT trong việc giảm dấu chân bộ nhớ trên các mô hình phổ biến BERT và ViT. Chúng tôi cho thấy SLIMFIT có thể đóng băng lên đến 95% các lớp và giảm tổng mức sử dụng bộ nhớ trên thiết bị trung bình 2.2× khi tinh chỉnh các mô hình BERT và ViT trên các benchmark và dataset khác nhau, như GLUE, SQuAD 2.0, CIFAR-10, CIFAR-100 và ImageNet với độ suy giảm độ chính xác trung bình 0.2%. Cụ thể hơn, SLIMFIT giảm tổng mức sử dụng bộ nhớ trên thiết bị của quá trình tinh chỉnh trên GLUE từ 6.1GB xuống 4.0GB (giảm 1.5×) với kích thước batch 32, trên SQuAD 2.0 từ 58.5GB xuống 19.1GB (giảm 3.1×) với kích thước batch 128, trên CIFAR-10 từ 7.2GB xuống 4.3GB (giảm 1.7×) với kích thước batch 32, trên CIFAR-100 từ 7.2GB xuống 4.5GB (giảm 1.6×) với kích thước batch 32, và trên ImageNet từ 77.4GB xuống 26.1GB (3.0×) với kích thước batch 128 với chi phí là độ suy giảm độ chính xác lên đến 0.4%. Kết quả là, SLIMFIT cho phép thực hiện các quá trình tinh chỉnh tốn nhiều bộ nhớ trên một GPU 32GB duy nhất như tinh chỉnh ViT trên ImageNet với kích thước batch 128 trong khi điều này thường yêu cầu ba GPU 32GB.

2 Kiến thức cơ bản
Trong vài năm qua, việc tiền huấn luyện các mô hình dựa trên attention đã dẫn đến những tiến bộ đáng kể trên nhiều tác vụ NLP và CV với các mô hình BERT [1] và ViT [2] phổ biến. Quá trình tiền huấn luyện cung cấp một điểm khởi tạo tốt để các mô hình này có thể tổng quát hóa tốt hơn trên dữ liệu chưa thấy của các tác vụ downstream. Do đó, các mô hình này có thể đạt được kết quả tiên tiến nhất bằng cách tinh chỉnh thông qua các điều chỉnh nhỏ đối với tham số của chúng. Về mặt kiến trúc, các mô hình này bao gồm một lớp embedding ban đầu, theo sau là các khối lặp lại của multi-head attention (MHA) được đưa vào mô-đun feed-forward network (FFN) (xem Phụ lục C để biết thêm chi tiết). Các kiến trúc cơ sở của BERT và ViT chứa hơn một trăm lớp được xây dựng theo cách này.

Mặc dù có số lượng lớp lớn, không phải tất cả đều cần được cập nhật trong quá trình tinh chỉnh để đạt được hiệu suất tốt trên các tác vụ downstream, như được chỉ ra trong [28]. Đáng chú ý, các tác giả phát hiện rằng việc đóng băng khoảng 60% các lớp attention sớm trong BERT dẫn đến suy giảm hiệu suất không đáng kể. Điều này cho thấy mô hình được tinh chỉnh có xu hướng bảo tồn các đặc trưng chung đã học trong quá trình tiền huấn luyện. Được thúc đẩy bởi nghiên cứu này, chúng tôi tìm cách phân tích động lực học huấn luyện của các mô hình đã tiền huấn luyện và tự động phát hiện các lớp có ít đóng góp cho quá trình tinh chỉnh.

3 Học tầm quan trọng của các lớp
Động lực học huấn luyện là một lĩnh vực nghiên cứu tích cực cung cấp thông tin chi tiết về hành vi của các mô hình đã tiền huấn luyện khi tinh chỉnh trên các tác vụ downstream. Chứng minh hội tụ của các thuật toán tối ưu hóa như stochastic gradient descent [29] cho thấy khoảng cách giữa các tham số và nghiệm tối ưu được giảm trong các lần lặp huấn luyện và theo đó, khoảng cách trọng số (hoặc lượng cập nhật trọng số) giữa các lần lặp liên tiếp giảm. Do đó, có thể một số lớp chỉ có thể nhận được thay đổi tối thiểu đối với tham số của chúng khi chúng ta tiếp cận cuối quá trình huấn luyện. Tất nhiên, việc phát hiện và đóng băng các lớp như vậy, khi chúng cho thấy cập nhật tối thiểu, sẽ không ảnh hưởng đến độ chính xác. Vì các mô hình dựa trên transformer đã được tiền huấn luyện, chúng đã cho thấy các cập nhật nhỏ trong quá trình tinh chỉnh so với tiền huấn luyện. Do đó, việc phát hiện và đóng băng các lớp có cập nhật tối thiểu (tức là giá trị khoảng cách trọng số) sẽ không ảnh hưởng đáng kể đến quá trình tinh chỉnh và theo đó là độ chính xác cuối cùng. Dựa trên các quan sát trên, chúng tôi xem xét chuẩn ℓ1 của cập nhật nhận được bởi tham số của mỗi lớp thông qua tất cả các lần lặp tinh chỉnh như động lực học huấn luyện trong bài báo này. Cũng đáng chú ý rằng việc đóng băng các lớp không ảnh hưởng đến sự hội tụ huấn luyện vì nó gây ra một sự tạm dừng trong quy trình huấn luyện của các lớp bị đóng băng như được chỉ ra bởi phân tích lý thuyết của chúng tôi trong Phụ lục D.1.

3.1 Động lực học huấn luyện
Hãy xem xét một mô hình đã tiền huấn luyện với một tập hợp tham số W trong đó các tham số liên quan đến lớp thứ i tại lần lặp t được ký hiệu là Wt_i ∈ R^(M×I). Động lực học huấn luyện cho lớp thứ i tại lần lặp t được định nghĩa là chuẩn ℓ1 của khoảng cách giữa Wt-1_i và Wt_i, tức là,

--- TRANG 4 ---
0 100 200 300 400 500 600 700 8000246
Lần lặpGiá trị khoảng cáchTrọng số Query (lớp #1)
Trọng số Query (lớp #5)
Trọng số Query (lớp #11)
(a) CoLA0 50 100 150 200 250 3000510
Lần lặpGiá trị khoảng cáchTrọng số Query (lớp #1)
Trọng số Query (lớp #5)
Trọng số Query (lớp #11)
(b) MRPC

Hình 2: Giá trị khoảng cách của ma trận trọng số query cho các lớp attention thứ nhất, thứ năm và thứ mười một của BERT-base được tinh chỉnh trên (a) CoLA và (b) MRPC dataset cho 3 epoch.

dt_i = 1/(M×I) ||Wt_i - Wt-1_i||/||Wt-1_i||_ℓ1, (1)

trong đó dt ∈ R^n_+ chứa tất cả di tại lần lặp t được gọi là vector khoảng cách, và n biểu thị tổng số lớp. Trên thực tế, Phương trình (1) tính toán sự thay đổi được chuẩn hóa trong tham số của lớp thứ i.

3.2 Thuật toán lập lịch liên lớp
Chúng tôi sử dụng các giá trị khoảng cách như động lực học huấn luyện để phân tích hành vi tinh chỉnh của các mô hình đã tiền huấn luyện. Ví dụ, hãy xem xét các giá trị khoảng cách trên tất cả các lần lặp tinh chỉnh cho các dataset CoLA [30] và MRPC [31]. Hình 2a cho thấy các giá trị khoảng cách của ma trận trọng số query cho các lớp attention thứ nhất, thứ năm và thứ mười một của BERT-base được tinh chỉnh trên dataset CoLA trong khi Hình 2b mô tả những của các lớp tương tự cho BERT-base được tinh chỉnh trên dataset MRPC.

Chúng tôi quan sát những điều sau dựa trên kết quả thực nghiệm của hai dataset này. Đầu tiên, lượng cập nhật cho mỗi lớp trở nên nhỏ hơn qua các lần lặp tinh chỉnh. Thứ hai, lượng cập nhật của mỗi lớp là cụ thể theo tác vụ và độc lập với vị trí của nó. Thứ ba, có một số lớp cho thấy giá trị khoảng cách nhỏ hơn so với các lớp khác trên hầu hết tất cả các lần lặp. Cuối cùng, các lớp có giá trị khoảng cách cao hơn ở đầu có thể trở nên nhỏ hơn qua các lần lặp tinh chỉnh so với các lớp bắt đầu với giá trị khoảng cách thấp hơn.

Dựa trên các quan sát trên, chúng tôi giới thiệu thuật toán ILS để quyết định ưu tiên cập nhật các lớp bằng cách sử dụng giá trị khoảng cách của chúng. Hình 3 cho thấy tổng quan về thuật toán ILS. Tại mỗi lần lặp từ lần lặp đầu tiên đến lần lặp cuối cùng, thuật toán ILS của chúng tôi chọn những lớp có giá trị khoảng cách lớn để được cập nhật và những lớp có giá trị khoảng cách nhỏ để bị đóng băng. Cụ thể hơn, các lớp đầu tiên được xếp hạng dựa trên giá trị khoảng cách của chúng tại mỗi lần lặp huấn luyện và sau đó những lớp có giá trị khoảng cách nhỏ được giữ đóng băng theo tỷ lệ đóng băng như một siêu tham số. Trực giác là các lớp có giá trị khoảng cách nhỏ ít đóng góp hơn cho quá trình tinh chỉnh vì tham số của chúng không được cập nhật nhiều. Mặt khác, các lớp có giá trị khoảng cách lớn đang học các mẫu cụ thể theo tác vụ bằng cách thực hiện các điều chỉnh đáng kể hơn đối với tham số của chúng. Lưu ý rằng việc đóng băng các lớp giữa không làm gián đoạn việc lan truyền gradient đến các lớp đầu của mạng như được chỉ ra thông qua một ví dụ trong Phụ lục D.2.

Tỷ lệ đóng băng của thuật toán ILS có thể được quyết định dựa trên ngân sách bộ nhớ GPU trên thiết bị. Tất nhiên, việc sử dụng tỷ lệ đóng băng cực kỳ cao có thể dẫn đến suy giảm hiệu suất tùy thuộc

ILS ILS ILS ILS
Mô hình Transformer Mô hình Transformer Mô hình Transformer Mô hình TransformerLần lặp huấn luyện 0 Lần lặp huấn luyện 1 Lần lặp huấn luyện 2 Lần lặp huấn luyện n-1

Đóng băng Hoạt động Quyết định đóng băng Động lực học huấn luyện

Hình 3: Tổng quan về thuật toán ILS. ILS đóng băng một số lớp nhất định tùy thuộc vào tỷ lệ đóng băng tại mỗi lần lặp duy nhất trong suốt quá trình tinh chỉnh cho tổng cộng n lần lặp huấn luyện.

--- TRANG 5 ---
Lớp #1
Lớp #2
Lớp #3
Lớp #4
Lớp #5
Lớp #6
Lớp #7
Lớp #810.5
12.3
8.5
9.4
11.4
3.5
6.7
13Mô hình Transformer
Vector khoảng cách
ILS
Lần lặp t−1Lớp #1
Lớp #2
Lớp #3
Lớp #4
Lớp #5
Lớp #6
Lớp #7
Lớp #88.1
5.3
8.5
9.4
7.1
3.5
6.7
11Mô hình Transformer
Vector khoảng cách
ILS
Lần lặp tLớp #1
Lớp #2
Lớp #3
Lớp #4
Lớp #5
Lớp #6
Lớp #7
Lớp #84.2
5.3
4.9
6.4
7.1
3.5
6.7
10Mô hình Transformer
Vector khoảng cách
Lần lặp t+1

Đóng băng Các lớp đóng băng không được cập nhật. Hoạt động Các lớp hoạt động đang được cập nhật.

Hình 4: Một ví dụ về quá trình đóng băng lặp đi lặp lại sử dụng thuật toán ILS của chúng tôi.

Thuật toán 1 Mã giả của thuật toán ILS thực hiện đóng băng lặp đi lặp lại.
Input: model, số lần lặp như itr, số lớp như L, tỷ lệ đóng băng F
d = rand(L)
for i in itr do
    idx = argsort(d)[:int(L*F)]
    for j in idx do
        model.layer[j].requires_grad = False
    end for
    model.train()
    Update d
end for

vào tác vụ downstream, cung cấp sự đánh đổi đáng giá giữa độ chính xác và bộ nhớ GPU trên thiết bị. Mặt khác, trong khi suy giảm hiệu suất không có khả năng với tỷ lệ đóng băng rất nhỏ, việc giảm bộ nhớ cũng không đáng kể.

Vì không có kiến thức trước về giá trị khoảng cách của mỗi lớp ở đầu quá trình tinh chỉnh, thuật toán ILS của chúng tôi khởi tạo vector khoảng cách với các giá trị ngẫu nhiên lớn. Tùy thuộc vào tỷ lệ đóng băng, mỗi lớp cùng với giá trị khoảng cách của nó được cập nhật trong vài lần lặp đầu tiên một lần cho đến khi tất cả các số ngẫu nhiên trong vector khoảng cách được thay thế bằng giá trị khoảng cách thực tế. Sau đó, các lớp được giữ đóng băng theo giá trị khoảng cách thực tế của chúng. Giá trị khoảng cách của các lớp hoạt động chỉ được cập nhật tại mỗi lần lặp trong khi của các lớp đóng băng vẫn không thay đổi. Mã giả của thuật toán ILS thực hiện đóng băng lặp đi lặp lại của chúng tôi được hiển thị trong Thuật toán 1.

Để hiểu rõ hơn thuật toán ILS, chúng tôi minh họa quá trình đóng băng lặp đi lặp lại bằng một ví dụ như được hiển thị trong Hình 4. Giả sử chúng ta có một mô hình dựa trên transformer 8 lớp và tương ứng một vector khoảng cách 8 phần tử tại lần lặp t. Xem xét tỷ lệ đóng băng 50% cho ví dụ này, 4 lớp có giá trị khoảng cách thấp nhất được giữ đóng băng và phần còn lại được cập nhật tại mỗi lần lặp.

4 Cân bằng tải liên lớp
Cho đến nay, chúng tôi đã giới thiệu thuật toán ILS ưu tiên cập nhật các lớp cụ thể trong khi giữ các lớp còn lại đóng băng theo giá trị khoảng cách của chúng. Đối với tỷ lệ đóng băng 50% như một ví dụ, chúng ta mong đợi thấy giảm 2× trong dấu chân bộ nhớ của kích hoạt. Tuy nhiên, đây không phải là trường hợp trong các mô hình dựa trên transformer do số lượng kích hoạt không cân bằng trên tất cả các lớp. Trên thực tế, sự mất cân bằng trong số lượng kích hoạt làm suy yếu khả năng của thuật toán ILS trong việc giảm dấu chân bộ nhớ trong quá trình tinh chỉnh như được hiển thị trong Hình 5.

Vì trọng tâm của bài báo này là các mô hình dựa trên transformer như BERT và ViT, chúng tôi phân tích kiến trúc của chúng cho các lớp mất cân bằng. Bảng 1 tóm tắt số lượng kích hoạt liên quan đến đầu vào của các lớp có tham số có thể huấn luyện trong BERT hoặc ViT. Trong số tất cả các lớp có thể huấn luyện, chỉ có một lớp mất cân bằng trong khối attention chứa kích hoạt nhiều gấp 4× so với các lớp khác.

Để giải quyết vấn đề cân bằng tải trong số lượng kích hoạt cho lớp nói trên, chúng tôi sử dụng lượng tử hóa. Vì hệ số mất cân bằng giữa các lớp là 4×, chúng tôi áp dụng lượng tử hóa 8-bit cho kích hoạt của lớp mất cân bằng trong đó 4 bit được sử dụng cho cả phần nguyên và phần thập phân. Theo cách này, chi phí bộ nhớ của kích hoạt được cân bằng bằng cách sử dụng lượng tử hóa. Trong lược đồ lượng tử hóa của chúng tôi, chúng tôi cache kích hoạt của lớp mất cân bằng sử dụng 8 bit trong forward pass. Trong backward pass, chúng tôi chuyển đổi kích hoạt 8-bit sang định dạng floating-point 32-bit. Do đó, tất cả các tính toán forward và backward vẫn được thực hiện bằng định dạng floating-point độ chính xác đơn. Quá trình chuyển đổi giữa định dạng fixed-point 8-bit và floating-point 32-bit được cung cấp trong Phụ lục E.

5 Kích hoạt động và tĩnh
Loại kích hoạt trong các mô hình dựa trên transformer có thể được chia thành hai loại: động và tĩnh. Chúng tôi đề cập đến các kích hoạt có thể được loại bỏ bằng cách đóng băng lớp của chúng như kích hoạt động. Mặt khác, kích hoạt tĩnh không thể được loại bỏ bất kể việc đóng băng. Trong số các

--- TRANG 6 ---
Lớp #1 (Kích thước: B * 128 * 768)
Lớp #2 (Kích thước: B * 128 * 3072)
Lớp #3 (Kích thước: B * 128 * 768)
Lớp #4 (Kích thước: B * 128 * 3072)Mô hình dựa trên Transformer
Đóng băng
Hoạt động
B: Kích thước Batch
Giảm bộ nhớ = 1.25×

Hình 5: Một ví dụ về mô hình có số lượng kích hoạt mất cân bằng và tác động của nó lên việc giảm bộ nhớ.

Loại lớp Mô tả # Kích hoạt Trạng thái
Dense attention.self.query B*T*H Cân bằng
Dense attention.self.key B*T*H Cân bằng
Dense attention.self.value B*T*H Cân bằng
Dense attention.output B*T*H Cân bằng
LayerNorm attention.output B*T*H Cân bằng
Dense intermediate B*T*H Cân bằng
Dense output B*T*4*H Mất cân bằng
LayerNorm output B*T*H Cân bằng

Bảng 1: Số lượng kích hoạt liên quan đến đầu vào của các lớp có tham số có thể huấn luyện trong BERT trong đó B, T, H biểu thị kích thước batch, độ dài chuỗi, kích thước ẩn, tương ứng. ViT có cùng cấu trúc với các mô tả khác nhau.

loại lớp khác nhau, GELU, MatMul, Softmax và LayerNorm chứa kích hoạt tĩnh như được hiển thị trong Bảng 2. Lưu ý rằng MatMul và Softmax chia sẻ cùng kích hoạt. Đối với các tính toán ngược của Softmax, đầu ra của nó trong forward pass được lưu như kích hoạt của nó. Mặt khác, đầu vào của MatMul được yêu cầu cho các tính toán ngược của nó như kích hoạt. Vì đầu ra của Softmax là đầu vào của MatMul trong forward pass, chúng chia sẻ cùng kích hoạt.

GELU và MatMul/Softmax không có bất kỳ tham số có thể huấn luyện nào và do đó không thể bị đóng băng. Do đó, hai lớp này giữ kích hoạt của chúng trong suốt quá trình tinh chỉnh. Phương pháp tốt nhất để giảm chi phí bộ nhớ của chúng là lượng tử hóa. Chúng tôi sử dụng 4 và 8 bit cho lượng tử hóa kích hoạt trong GELU và MatMul/Softmax, tương ứng. Vì không có hỗ trợ tensor 4-bit trong PyTorch, chúng tôi lưu trữ mỗi hai kích hoạt 4-bit như một kích hoạt 8-bit duy nhất bằng cách sử dụng các phép toán shift. Lưu ý rằng việc sử dụng các mức bit như vậy dẫn đến suy giảm độ chính xác không đáng kể trong khi lượng tử hóa thêm các kích hoạt đó gây ra mất độ chính xác đáng kể.

Trái ngược với GELU và MatMul/Softmax, LayerNorm chứa các tham số có thể huấn luyện và có thể bị đóng băng bởi thuật toán ILS. Tuy nhiên, kích hoạt của nó vẫn là tĩnh. Forward pass của LayerNorm được tính bằng:

x̃ = (x - E(x))/√(Var(x) + epsilon), (2)
y = x̃ * gamma + beta, (3)

trong đó gamma và beta là các tham số có thể huấn luyện. Đầu vào và đầu ra của LayerNorm được ký hiệu bởi x ∈ R^H và y ∈ R^H, tương ứng. E(·) và Var(·) tính toán trung bình và phương sai, tương ứng. Đạo hàm của loss đối với gamma (tức là ∂gamma) được tính bằng

∂gamma = x̃ * ∂y, (4)

và đối với beta (tức là ∂beta) bằng:

∂beta = ∂y, (5)

trong đó ∂y biểu thị đạo hàm của loss đối với y. Chúng ta cũng cần tính đạo hàm của loss đối với x (tức là ∂x) như:

g = gamma * ∂y / (H * √(Var(x) + epsilon)), (6)
∂x = H * g - ΣH g - x̃ * ΣH(g * x̃). (7)

Khi LayerNorm bị đóng băng, không cần tính Phương trình (4). Tuy nhiên, kích hoạt của lớp này không thể được loại bỏ vì chúng vẫn là một phần của các tính toán trong Phương trình (7). Cụ thể hơn, phiên bản chuẩn hóa của x (tức là x̃) được yêu cầu ngay cả khi lớp này bị đóng băng.

Đóng góp của số hạng cuối cùng trong Phương trình (7) (tức là ΣH(g * x̃)) chỉ đáng kể đối với các giá trị lớn của x̃. Do đó, các giá trị nhỏ của x̃ có thể được loại bỏ. Lý tưởng, chúng ta muốn có tất cả kích hoạt của lớp này được loại bỏ khi lớp này bị đóng băng. Tuy nhiên, điều này sẽ dẫn đến suy giảm độ chính xác. Do đó

Bảng 2: Loại kích hoạt của các lớp trong MHA và FFN của BERT và ViT.
Loại lớp # Kích hoạt Loại kích hoạt Loại lớp # Kích hoạt Loại kích hoạt
Dense B*T*H Động LayerNorm B*T*H Tĩnh
MatMul B*T*H(2×) Tĩnh Dense B*T*H Động
Softmax B*T*T Tĩnh GELU B*T*4*H Tĩnh
MatMul B*T*H&B*T*T Tĩnh Dense B*T*4*H Động
Dense B*T*H Động LayerNorm B*T*H Tĩnh

--- TRANG 7 ---
Bảng 3: Hiệu suất độ chính xác và bộ nhớ của SLIMFIT trên GLUE benchmark và SQuAD 2.0. Kích thước batch 32 và 128 được sử dụng cho GLUE benchmark và SQuAD 2.0, tương ứng.

Phương pháp Metric MNLI mQQP QNLI SST-2 CoLA STS-B MRPC RTE SQuAD 2.0
BERT (Baseline) Độ chính xác 83.4 90.8 90.5 92.1 58.9 89.5 86.4 70.2 74.0
Bộ nhớ kích hoạt (GB) 3.2 3.2 3.2 3.2 3.2 3.2 3.2 3.2 55.1
Tổng bộ nhớ GPU trên chip (GB) 6.1 6.1 6.1 6.1 6.1 6.1 6.1 6.1 58.5 (2 GPU)
SLIMFIT Độ chính xác 83.3 90.4 90.4 92.3 59.6 89.4 86.3 70.4 74.0
Tỷ lệ đóng băng (%) 80 80 95 95 90 85 91 90 80
Bộ nhớ kích hoạt (GB) 0.7 0.7 0.5 0.5 0.6 0.7 0.6 0.6 10
Tổng bộ nhớ GPU trên chip (GB) 4.4 4.4 4.0 4.0 4.3 4.3 4.3 4.3 19.1

vậy, chúng tôi cắt tỉa các giá trị nhỏ trong x̃ và giữ 10% giá trị lớn nhất. Theo cách này, tải bộ nhớ của kích hoạt được giảm đáng kể. Tất nhiên, khi lớp này không bị đóng băng, việc lan truyền ngược được thực hiện mà không có bất kỳ xấp xỉ nào. Thủ thuật như vậy chuyển đổi LayerNorm từ lớp tĩnh sang lớp bán tĩnh. Đáng chú ý rằng các chỉ số của kích hoạt đã cắt tỉa cũng được lưu trữ cùng với kích hoạt. Chi tiết của quy trình cắt tỉa được cung cấp trong Phụ lục F.

6 SLIMFIT
SLIMFIT là một công cụ hiệu suất khai thác thuật toán ILS cùng với lượng tử hóa và cắt tỉa để giảm dấu chân bộ nhớ của kích hoạt thông qua quá trình đóng băng lặp đi lặp lại. Việc giảm tổng bộ nhớ GPU trên thiết bị của SLIMFIT là kết quả của việc giảm bộ nhớ trong cả kích hoạt động và tĩnh. Kích hoạt tĩnh đóng góp một lượng bộ nhớ cố định trong khi mức sử dụng bộ nhớ của kích hoạt động phụ thuộc vào tỷ lệ đóng băng. Với tỷ lệ đóng băng cao, dấu chân bộ nhớ của kích hoạt và do đó tổng mức sử dụng bộ nhớ GPU trên thiết bị có thể được giảm đáng kể. Lựa chọn tỷ lệ đóng băng phụ thuộc vào ngân sách bộ nhớ của người dùng. Bằng cách tăng tỷ lệ đóng băng lên đến một điểm nhất định, sẽ không có suy giảm hiệu suất. Tuy nhiên, việc sử dụng tỷ lệ đóng băng cực kỳ cao sẽ đánh đổi bộ nhớ lấy độ chính xác. Việc tìm điểm phá vỡ của phương pháp phụ thuộc vào tác vụ và thay đổi từ dataset này sang dataset khác.

7 Kết quả thực nghiệm
Chúng tôi sử dụng phiên bản cơ sở của BERT và ViT cho các thí nghiệm. Chúng tôi tinh chỉnh hai mô hình này bằng SLIMFIT được triển khai trên PyTorch. Chúng tôi đánh giá BERT [1] sử dụng GLUE benchmark [31] và SQuAD 2.0 [32]. Đối với ViT [2], chúng tôi sử dụng các dataset CIFAR-10, CIFAR-100 và ImageNet [33, 34] cho mục đích đánh giá. Chúng tôi thảo luận về mức sử dụng bộ nhớ của kích hoạt và tổng bộ nhớ GPU trên thiết bị trên GPU NVIDIA V100 32GB. Chúng tôi báo cáo tổng mức sử dụng bộ nhớ GPU trên thiết bị bằng "nvidia-smi". Đối với tất cả các thí nghiệm trong phần này, chúng tôi sử dụng 3 epoch cho tinh chỉnh. Chi tiết về các tác vụ CV/NLP, phép đo và cài đặt siêu tham số được cung cấp trong Phụ lục G.

7.1 Đánh giá độ chính xác trên GLUE và SQuAD 2.0
Để đánh giá khả năng hiểu ngôn ngữ của các mô hình BERT, GLUE benchmark được hình thành bởi một loạt các tác vụ downstream bao gồm phân loại cảm xúc (SST-2), suy luận ngôn ngữ tự nhiên (RTE, QNLI, và MNLI), phát hiện paraphrase (MRPC, QQP, và STS-B), và khả năng chấp nhận ngôn ngữ học (CoLA). Chúng tôi sử dụng tương quan Spearman cho STS-B, tương quan Matthew cho CoLA, độ chính xác phần trăm cho RTE, MRPC, SST-2, QQP, QNLI và MNLI m, và điểm F1 cho SQuAD 2.0. Trong công việc này, chúng tôi tinh chỉnh mô hình BERT-base bằng SLIMFIT trên các tác vụ downstream của GLUE benchmark cũng như tác vụ trả lời câu hỏi trên SQuAD 2.0. Bảng 3 cho thấy độ chính xác trên tập validation của các tác vụ nói trên và mức sử dụng bộ nhớ của SLIMFIT so với baseline. Kết quả của baseline được thu thập mà không có đóng băng. Chúng tôi báo cáo kết quả liên quan đến tỷ lệ đóng băng cao nhất có thể đạt được độ chính xác tương tự như baseline bằng cách thay đổi tỷ lệ học. Kết quả thực nghiệm trên GLUE benchmark cho thấy lên đến 95% kích hoạt động có thể được loại bỏ với độ suy giảm độ chính xác lên đến 0.4%, dẫn đến trung bình giảm 1.9GB trong tổng mức sử dụng bộ nhớ GPU trên thiết bị. Mặt khác, trong khi tinh chỉnh SQuAD 2.0 mà không đóng băng yêu cầu tối thiểu 2 GPU NVIDIA V100 32GB trên kích thước batch 128, SLIMFIT cho phép tinh chỉnh nó trên một GPU NVIDIA V100 32GB duy nhất, giảm tổng yêu cầu bộ nhớ trên thiết bị của tác vụ như vậy từ 58.5GB xuống 19.1GB (giảm 3.1×).

Hình 6 cho thấy tổng mức sử dụng bộ nhớ GPU trên thiết bị của BERT khi được tinh chỉnh bằng SLIMFIT cho các kích thước batch khác nhau ở tỷ lệ đóng băng 95% trên GLUE benchmark và 80% trên SQuAD 2.0. Theo kết quả thực nghiệm, SLIMFIT cho phép giảm từ 1.5× đến 3.1× trong tổng bộ nhớ GPU trên thiết bị trên các tác vụ NLP. Việc giảm tổng mức sử dụng bộ nhớ trên thiết bị đáng kể hơn đối với các kích thước batch lớn hơn vì kích hoạt chiếm ưu thế trong dấu chân bộ nhớ.

--- TRANG 8 ---
GLUE SQuAD 2.0 CIFAR-10 CIFAR-100 ImageNet020406080
6.1
16.4
7.2
7.2
20.54
7.3
4.3
4.5
8.99.1
29.6
11.4
11.4
40.15.1
11.1
5.8
6.3
14.915.4
58.5
20.3
20.3
77.46.8
19.1
8.7
9.6
26.1Tổng bộ nhớ
GPU trên thiết bị (GB)
Baseline-32 SLIMFIT-32 Baseline-64 SLIMFIT-64 Baseline-128 SLIMFIT-128

Hình 6: Tổng mức sử dụng bộ nhớ GPU trên thiết bị của SLIMFIT so với baseline trên các kích thước batch khác nhau bao gồm 32, 64 và 128 trên các dataset NLP và CV.

Bảng 4: Độ chính xác top-1 và hiệu suất bộ nhớ của SLIMFIT trên các benchmark CV sử dụng kích thước batch 32 cho các dataset CIFAR và 128 cho dataset ImageNet.

Baseline SLIMFIT
Mô hình Metric CIFAR-10 CIFAR-100 ImageNet CIFAR-10 CIFAR-100 ImageNet
ViT Độ chính xác (%) 98.8 91.2 83.3 98.5 91.0 83.3
Tỷ lệ đóng băng (%) NA NA NA 90 75 95
Bộ nhớ kích hoạt (GB) 4.5 4.5 69.5 0.8 1.0 11.9
Tổng bộ nhớ (GB) 7.2 7.2 77.4 (3 GPU) 4.3 4.5 26.1

7.2 Đánh giá độ chính xác trên CIFAR và ImageNet
Để đánh giá hiệu quả của phương pháp chúng tôi trên các tác vụ CV, chúng tôi tinh chỉnh mô hình ViT-base trên các dataset CIFAR-10, CIFAR-100 và ImageNet. Chúng tôi sử dụng tập test của CIFAR-10/CIFAR-100 và tập validation của ImageNet để đánh giá độ chính xác của chúng trên ViT. Bảng 4 cho thấy SLIMFIT có thể tinh chỉnh mô hình ViT-base với tỷ lệ đóng băng lên đến 95% với mất độ chính xác lên đến 0.3% trong khi giảm đáng kể tổng mức sử dụng bộ nhớ GPU trên thiết bị. Cụ thể hơn, SLIMFIT giảm tổng mức sử dụng bộ nhớ của quá trình tinh chỉnh trên CIFAR-10 từ 7.2GB xuống 4.3GB (giảm 1.7×) với kích thước batch 32, trên CIFAR-100 từ 7.2GB xuống 4.5GB (giảm 1.6×) với kích thước batch 32, và trên ImageNet từ 77.4GB xuống 26.1GB (giảm 3×) với kích thước batch 128. Hình 6 cũng cho thấy tổng mức sử dụng bộ nhớ GPU trên thiết bị của SLIMFIT trên các kích thước batch khác nhau trên các tác vụ CV.

8 Nghiên cứu loại bỏ
Trong phần này, chúng tôi nghiên cứu các khía cạnh khác nhau của SLIMFIT trong việc tinh chỉnh các mô hình dựa trên transformer thông qua một loạt nghiên cứu loại bỏ. Do không gian hạn chế, chúng tôi thảo luận về tác động của lượng tử hóa/cắt tỉa và tổng thời gian wall-clock trong Phụ lục H và Phụ lục I, tương ứng. Đối với tất cả các thí nghiệm trong phần này, chúng tôi sử dụng kích thước batch 32 và 3 epoch cho tinh chỉnh.

8.1 Độ chính xác so với tỷ lệ đóng băng
Trong Phần (3.2), chúng tôi đã thảo luận rằng thuật toán ILS của chúng tôi điều phối lịch trình đóng băng dựa trên một quy tắc đơn giản: các lớp có giá trị khoảng cách lớn nhất được cập nhật trong khi những lớp có giá trị khoảng cách thấp nhất được giữ đóng băng cho tỷ lệ đóng băng nhất định. Tất nhiên, phương pháp đóng băng lặp đi lặp lại như vậy đánh đổi giữa độ chính xác và tỷ lệ đóng băng. Để thể hiện rõ hơn sự đánh đổi này, chúng tôi đo và minh họa độ chính xác của các dataset CoLA và MRPC trên các tỷ lệ đóng băng khác nhau trong Hình 7. Đường cong đánh đổi cho thấy thuật toán ILS của chúng tôi có thể duy trì độ chính xác ở cùng mức với baseline bằng cách đóng băng lên đến 95% các lớp.

Ngoài thuật toán ILS của chúng tôi, lịch trình đóng băng có thể được quyết định bằng các phương pháp đóng băng ngẫu nhiên hoặc dần dần. Trong phương pháp lập lịch ngẫu nhiên, các lớp đóng băng được chọn ngẫu nhiên tại mỗi lần lặp. Trong phương pháp dần dần, mặt khác, các lớp đầu được đóng băng dần dần trong khi các lớp sau được cập nhật trong suốt quá trình tinh chỉnh. Trong số các phương pháp này, thuật toán ILS của chúng tôi nổi bật đáng kể về cả độ chính xác và tỷ lệ đóng băng như được hiển thị trong Hình 7. Lý do

0 20 40 60 80 100405060
Tỷ lệ đóng băng (%)Tương quan MatthewILS
Ngẫu nhiên
Dần dần
(a) CoLA0 20 40 60 80 10060708090100
Tỷ lệ đóng băng (%)Độ chính xác (%)ILS
Ngẫu nhiên
Dần dần
(b) MRPC

Hình 7: Đường cong đánh đổi giữa độ chính xác và tỷ lệ đóng băng cho ba phương pháp đóng băng lặp đi lặp lại khác nhau (tức là ILS, phương pháp ngẫu nhiên và dần dần) trên (a) CoLA và (b) MRPC dataset.

--- TRANG 9 ---
Bảng 5: So sánh với các kỹ thuật tiên tiến nhất khi tinh chỉnh BERT trên dataset CoLA.

Mô hình Metric Baseline 4-bit GACT (ICML'22 [4]) DropIT (ICLR '23 [5]) SLIMFIT
BERT Độ chính xác (Tương quan Matthew) 58.9 59.0 57.5 59.6
Tỷ lệ đóng băng (%) NA NA NA 90%
Bộ nhớ kích hoạt (GB) 3.2 0.5 2.4 0.6
Tổng bộ nhớ (GB) 6.1 6.0 5.7 4.3
Độ trễ (Giây) 251 455 367 281

0 20 40 60 80
Tỷ lệ đóng băng (%)0
20
40
60
80
100Các lớp
(a) CoLA
0 20 40 60 80
Tỷ lệ đóng băng (%)0
20
40
60
80
100Các lớp
(b) MRPC

Hình 8: Tần suất xuất hiện cập nhật cho mỗi lớp như một heatmap trên (a) CoLA và (b) MRPC dataset. Mô tả các lớp tương ứng với các chỉ số được cung cấp trong Phụ lục J.

đằng sau hiệu suất vượt trội của nó là ILS cho phép nhiều cập nhật hơn cho các lớp có giá trị khoảng cách lớn bằng cách giữ các lớp có giá trị khoảng cách tối thiểu đóng băng trong một số lần lặp cụ thể. Mặt khác, trong phương pháp ngẫu nhiên, các lớp được chọn ngẫu nhiên để được cập nhật. Do đó, các lớp có giá trị khoảng cách lớn nhận được ít cập nhật hơn trong phương pháp ngẫu nhiên so với ILS. Tất nhiên, cơ hội các lớp có giá trị khoảng cách lớn được chọn ngẫu nhiên như các lớp hoạt động giảm khi tỷ lệ đóng băng tăng, điều này giải thích khoảng cách độ chính xác giữa ILS và phương pháp ngẫu nhiên với tỷ lệ đóng băng cao hơn 70%. Trong phương pháp đóng băng dần dần, các lớp đầu không nhận được cập nhật nào trong quá trình tinh chỉnh, dẫn đến suy giảm độ chính xác đáng kể cho các tỷ lệ đóng băng lớn.

8.2 Tần suất xuất hiện cập nhật
Để hình dung tần suất xuất hiện cập nhật cho mỗi lớp, chúng tôi sử dụng heatmap như được hiển thị trong Hình 8 cho cả dataset CoLA và MRPC trong đó các số đếm lớn hơn được liên kết với màu sắc tối hơn. Như được hiển thị trong heatmap, các lớp dense bên trong mô-đun MHA nhận được nhiều cập nhật hơn các lớp khác cho cả hai dataset. Hơn nữa, các mẫu cập nhật của các dataset này tương tự cho các tỷ lệ đóng băng nhỏ trong khi chúng trở nên cụ thể hơn theo tác vụ cho các tỷ lệ đóng băng cao. Trên thực tế, thuật toán ILS ưu tiên cập nhật một số lớp cụ thể hơn các lớp khác cho các tỷ lệ đóng băng cao.

9 So sánh với các kỹ thuật tiên tiến nhất và hạn chế
Tiếp theo, chúng tôi so sánh SLIMFIT với các phương pháp nén tiên tiến nhất nhắm vào việc giảm bộ nhớ, tức là GACT [4] và DropIT [5]. Bảng 5 tóm tắt kết quả so sánh về độ chính xác, bộ nhớ và độ trễ. Để so sánh công bằng, chúng tôi đo hiệu suất của chúng dưới cùng framework và siêu tham số (tức là kích thước batch và số epoch huấn luyện) trong quá trình tinh chỉnh BERT trên CoLA. Kết quả thực nghiệm của GACT và DropIT được thu thập bằng thư viện PyTorch chính thức của chúng. Theo kết quả thực nghiệm, GACT cho thấy lượng bộ nhớ thấp nhất cho kích hoạt. Tuy nhiên, về mức sử dụng bộ nhớ GPU trên thiết bị, SLIMFIT vượt trội hơn GACT. Về độ chính xác, tất cả các mô hình cho thấy độ chính xác tương đương trên CoLA so với baseline. Cuối cùng, về tốc độ, SLIMFIT cho thấy tốc độ tinh chỉnh nhanh nhất trong số các công trình hiện có trong khi nó vẫn kém hơn so với baseline (xem Phụ lục I để biết thêm chi tiết về tốc độ tính toán của SLIMFIT). Mặc dù có độ chính xác tốt hơn của SLIMFIT trên CoLA, nó cho thấy độ suy giảm độ chính xác lên đến 0.4% trên các tác vụ CV/NLP khác nhau, đây là một hạn chế khác của SLIMFIT bên cạnh tốc độ tinh chỉnh so với baseline.

10 Kết luận
Trong bài báo này, chúng tôi trình bày một công cụ hiệu suất có tên SLIMFIT giảm mức sử dụng bộ nhớ của kích hoạt và do đó tổng mức sử dụng bộ nhớ GPU trên thiết bị của các mô hình dựa trên transformer thông qua việc đóng băng lặp đi lặp lại các lớp trong quá trình tinh chỉnh. SLIMFIT áp dụng phương pháp lập lịch liên lớp để điều phối lịch trình đóng băng tại mỗi lần lặp. Để cân bằng số lượng kích hoạt trên tất cả các lớp và giảm mức sử dụng bộ nhớ của kích hoạt tĩnh, SLIMFIT sử dụng lượng tử hóa và cắt tỉa cho một vài lớp cụ thể. Chúng tôi đánh giá hiệu suất của SLIMFIT trên các tác vụ NLP và CV khác nhau. Chúng tôi cho thấy SLIMFIT giảm đáng kể mức sử dụng bộ nhớ GPU trên thiết bị của quá trình tinh chỉnh lên đến 3.1× khi sử dụng kích thước batch 128.

--- TRANG 10 ---
Tài liệu tham khảo
[1] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova, "Bert: Tiền huấn luyện các transformer hai chiều sâu để hiểu ngôn ngữ," 2018, trích dẫn arxiv:1810.04805Comment: 13 trang.
[Online]. Có sẵn: http://arxiv.org/abs/1810.04805
[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, và N. Houlsby, "Một hình ảnh có giá trị 16x16
từ: Transformer cho nhận dạng hình ảnh ở quy mô lớn," trong International Conference on Learning
Representations, 2021. [Online]. Có sẵn: https://openreview.net/forum?id=YicbFdNTTy
[3] P. Jain, A. Jain, A. Nrusimha, A. Gholami, P. Abbeel, J. Gonzalez, K. Keutzer,
và I. Stoica, "Checkmate: Phá vỡ rào cản bộ nhớ với rematerialization tensor tối ưu," trong Proceedings of Machine Learning and Systems, I. Dhillon, D. Papailiopoulos, và V. Sze, Eds., vol. 2, 2020, tr. 497-511. [Online]. Có sẵn:
https://proceedings.mlsys.org/paper/2020/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper.pdf
[4] X. Liu, L. Zheng, D. Wang, Y. Cen, W. Chen, X. Han, J. Chen, Z. Liu, J. Tang, J. Gonzalez,
M. Mahoney, và A. Cheung, "GACT: Huấn luyện nén kích hoạt cho kiến trúc mạng tổng quát," trong Proceedings of the 39th International Conference on Machine Learning, ser.
Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,
G. Niu, và S. Sabato, Eds., vol. 162. PMLR, 17-23 Jul 2022, tr. 14 139-14 152. [Online].
Có sẵn: https://proceedings.mlr.press/v162/liu22v.html
[5] J. Chen, K. Xu, Y. Wang, Y. Cheng, và A. Yao, "DropIT: Loại bỏ tensor trung gian
để huấn luyện DNN hiệu quả bộ nhớ," trong The Eleventh International Conference on Learning
Representations, 2023. [Online]. Có sẵn: https://openreview.net/forum?id=Kn6i2BZW69w
[6] J. Chen, L. Zheng, Z. Yao, D. Wang, I. Stoica, M. Mahoney, và J. Gonzalez, "Actnn: Giảm
dấu chân bộ nhớ huấn luyện thông qua huấn luyện nén kích hoạt 2-bit," trong International Conference
on Machine Learning. PMLR, 2021, tr. 1803-1813.
[7] A. Chakrabarti và B. Moseley, "Backprop với kích hoạt xấp xỉ để huấn luyện mạng hiệu quả bộ nhớ," 2019. [Online]. Có sẵn: https://openreview.net/forum?id=rJgfjjC9Ym
[8] F. Fu, Y. Hu, Y. He, J. Jiang, Y. Shao, C. Zhang, và B. Cui, "Đừng lãng phí bit! Nén
kích hoạt và gradient cho mạng nơ-ron sâu thông qua TinyScript," trong Proceedings of the 37th
International Conference on Machine Learning, ser. Proceedings of Machine Learning Research,
H. D. III và A. Singh, Eds., vol. 119. PMLR, 13-18 Jul 2020, tr. 3304-3314. [Online].
Có sẵn: https://proceedings.mlr.press/v119/fu20c.html
[9] R. D. Evans, L. Liu, và T. M. Aamodt, "Jpeg-act: Tăng tốc học sâu thông qua nén mất mát dựa trên biến đổi," trong 2020 ACM/IEEE 47th Annual International Symposium on Computer
Architecture (ISCA), 2020, tr. 860-873.
[10] T. Chen, B. Xu, C. Zhang, và C. Guestrin, "Huấn luyện mạng sâu với chi phí bộ nhớ sublinear,"
arXiv preprint arXiv:1604.06174, 2016.
[11] O. Beaumont, L. Eyraud-Dubois, và A. Shilova, "Kết hợp hiệu quả rematerialization và offloading để huấn luyện DNN," trong Advances in Neural Information Processing
Systems, M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, và J. W. Vaughan,
Eds., vol. 34. Curran Associates, Inc., 2021, tr. 23 844-23 857. [Online]. Có sẵn:
https://proceedings.neurips.cc/paper/2021/file/c8461bf13fca8a2b9912ab2eb1668e4b-Paper.pdf
[12] M. Kirisame, S. Lyubomirsky, A. Haan, J. Brennan, M. He, J. Roesch, T. Chen, và Z. Tatlock,
"Rematerialization tensor động," trong International Conference on Learning Representations,
2021. [Online]. Có sẵn: https://openreview.net/forum?id=Vfs_2RnOD0H
[13] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg,
M. Houston, O. Kuchaiev, G. Venkatesh, và H. Wu, "Huấn luyện độ chính xác hỗn hợp," 2017, trích dẫn
arxiv:1710.03740Comment: Được xuất bản như một bài báo hội nghị tại ICLR 2018. [Online]. Có sẵn:
http://arxiv.org/abs/1710.03740
[14] S. Wu, G. Li, F. Chen, và L. Shi, "Huấn luyện và suy luận với số nguyên trong mạng nơ-ron sâu," trong International Conference on Learning Representations, 2018. [Online]. Có sẵn:
https://openreview.net/forum?id=HJGXzmspb

--- TRANG 11 ---
[15] N. Wang, J. Choi, D. Brand, C.-Y. Chen, và K. Gopalakrishnan, "Huấn luyện mạng nơ-ron sâu với số floating point 8-bit," trong Advances in Neural Information
Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
và R. Garnett, Eds., vol. 31. Curran Associates, Inc., 2018. [Online]. Có sẵn:
https://proceedings.neurips.cc/paper/2018/file/335d3d1cd7ef05ec77714a215134914c-Paper.pdf
[16] R. Banner, I. Hubara, E. Hoffer, và D. Soudry, "Các phương pháp có thể mở rộng cho huấn luyện 8-bit của mạng nơ-ron," ser. NIPS'18. Red Hook, NY, USA: Curran Associates Inc., 2018, tr. 5151-5159.
[17] O. Zafrir, G. Boudoukh, P. Izsak, và M. Wasserblat, "Q8bert: Quantized 8bit bert," trong 2019 Fifth
Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition
(EMC2-NIPS), 2019, tr. 36-39.
[18] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, và W. Chen,
"LoRA: Thích ứng low-rank của các mô hình ngôn ngữ lớn," trong International Conference on Learning
Representations, 2022. [Online]. Có sẵn: https://openreview.net/forum?id=nZeVKeeFYf9
[19] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,
R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,
M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,
I. Sutskever, và D. Amodei, "Các mô hình ngôn ngữ là người học few-shot," trong Advances in
Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Bal-
can, và H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, tr. 1877-1901. [Online]. Có sẵn:
https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
[20] S. Swayamdipta, R. Schwartz, N. Lourie, Y. Wang, H. Hajishirzi, N. A. Smith, và Y. Choi,
"Dataset cartography: Lập bản đồ và chẩn đoán dataset với động lực học huấn luyện," trong Proceedings
of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).
Online: Association for Computational Linguistics, Nov. 2020, tr. 9275-9293. [Online].
Có sẵn: https://aclanthology.org/2020.emnlp-main.746
[21] R. Teehan, M. Clinciu, O. Serikov, E. Szczechla, N. Seelam, S. Mirkin, và A. Gokaslan,
"Cấu trúc nổi lên và động lực học huấn luyện trong các mô hình ngôn ngữ lớn," trong Proceedings of
BigScience Episode #5 - Workshop on Challenges & Perspectives in Creating Large Language
Models. virtual+Dublin: Association for Computational Linguistics, May 2022, tr. 146-159.
[Online]. Có sẵn: https://aclanthology.org/2022.bigscience-1.11
[22] Z. Fang, M. Shahbazi, T. Probst, D. P. Paudel, và L. Van Gool, "Tối ưu hóa mạng nơ-ron nhận biết động lực học huấn luyện với ổn định," trong Proceedings of the Asian Conference on Computer
Vision (ACCV), December 2022, tr. 4276-4292.
[23] Y. Lee, A. S. Chen, F. Tajwar, A. Kumar, H. Yao, P. Liang, và C. Finn, "Tinh chỉnh phẫu thuật
cải thiện thích ứng với các thay đổi phân phối," arXiv preprint arXiv:2210.11466, 2022.
[24] V. V. Ramasesh, E. Dyer, và M. Raghu, "Giải phẫu quên thảm khốc: Biểu diễn ẩn và ngữ nghĩa tác vụ," trong International Conference on Learning Representations,
2021. [Online]. Có sẵn: https://openreview.net/forum?id=LhY8QdUGSuw
[25] Y. Liu, S. Agarwal, và S. Venkataraman, "Autofreeze: Tự động đóng băng các khối mô hình để tăng tốc tinh chỉnh," CoRR, vol. abs/2102.01386, 2021. [Online]. Có sẵn:
https://arxiv.org/abs/2102.01386
[26] S. Li, G. Yuan, Y. Dai, Y. Zhang, Y. Wang, và X. Tang, "SmartFRZ: Một framework huấn luyện hiệu quả sử dụng đóng băng lớp dựa trên attention," trong The Eleventh International Conference on Learning
Representations, 2023. [Online]. Có sẵn: https://openreview.net/forum?id=i9UlAr1T_xl
[27] C. He, S. Li, M. Soltanolkotabi, và S. Avestimehr, "Pipetransformer: Pipelining đàn hồi tự động cho huấn luyện phân tán các mô hình quy mô lớn," trong Proceedings of the 38th International
Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. Meila
và T. Zhang, Eds., vol. 139. PMLR, 18-24 Jul 2021, tr. 4150-4159. [Online]. Có sẵn:
https://proceedings.mlr.press/v139/he21a.html
[28] A. Merchant, E. Rahimtoroghi, E. Pavlick, và I. Tenney, "Điều gì xảy ra với embedding BERT trong quá trình tinh chỉnh?" trong Proceedings of the Third BlackboxNLP Workshop on Analyzing and
Interpreting Neural Networks for NLP. Online: Association for Computational Linguistics, Nov.
2020, tr. 33-44. [Online]. Có sẵn: https://aclanthology.org/2020.blackboxnlp-1.4

--- TRANG 12 ---
[29] S. Shalev-Shwartz và S. Ben-David, Understanding Machine Learning - From Theory to Algorithms. Cambridge University Press, 2014.
[30] A. Warstadt, A. Singh, và S. R. Bowman, "Các phán đoán về tính chấp nhận của mạng nơ-ron," arXiv
preprint arXiv:1805.12471, 2018.
[31] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, và S. Bowman, "GLUE: Một benchmark
đa tác vụ và nền tảng phân tích để hiểu ngôn ngữ tự nhiên," trong Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP.
Brussels, Belgium: Association for Computational Linguistics, Nov. 2018, tr. 353-355. [Online].
Có sẵn: https://aclanthology.org/W18-5446
[32] P. Rajpurkar, J. Zhang, K. Lopyrev, và P. Liang, "SQuAD: 100,000+ Câu hỏi để Hiểu
Văn bản bằng Máy," arXiv e-prints, tr. arXiv:1606.05250, 2016.
[33] A. Krizhevsky, "Học nhiều lớp đặc trưng từ hình ảnh nhỏ," tr. 32-33, 2009.
[Online]. Có sẵn: https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf
[34] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, và L. Fei-Fei, "Imagenet: Một cơ sở dữ liệu hình ảnh phân cấp quy mô lớn," trong 2009 IEEE conference on computer vision and pattern recognition. Ieee,
2009, tr. 248-255.
[35] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison,
L. Antiga, và A. Lerer, "Tự động vi phân trong pytorch," trong CUDA semantics - PyTorch 2.0
documentation, 2017. [Online]. Có sẵn: https://pytorch.org/docs/stable/notes/cuda.html

--- TRANG 13 ---
Phụ lục

A Quản lý bộ nhớ
Bộ nhớ trên thiết bị của các GPU hiện đại bị giới hạn ở vài chục gigabyte tùy thuộc vào mô hình của chúng (ví dụ: NVIDIA V100 32GB). Nếu yêu cầu bộ nhớ của việc huấn luyện/tinh chỉnh mạng nơ-ron vượt quá bộ nhớ có sẵn trên GPU, lỗi hết bộ nhớ sẽ xảy ra. Yêu cầu bộ nhớ của mã đang chạy trên GPU có thể được xem bằng "nvidia-smi". Đối với quá trình huấn luyện/tinh chỉnh, yêu cầu bộ nhớ này được xác định bởi kích thước của mô hình, kích hoạt được cache, gradient, moment gradient từ optimizer và nội dung CUDA sau các lần lặp huấn luyện đầu tiên. Đáng chú ý rằng mức sử dụng bộ nhớ của quá trình huấn luyện/tinh chỉnh vẫn không đổi sau lần lặp đầu tiên nếu các lần lặp tiếp theo thực hiện cùng các tính toán (xem Hình 9). Nếu yêu cầu bộ nhớ của mỗi lần lặp khác với các lần lặp khác, PyTorch báo cáo yêu cầu bộ nhớ của lần lặp sử dụng bộ nhớ tối đa trong số các lần lặp khác như tổng mức sử dụng bộ nhớ GPU trên thiết bị của chương trình trong "nvidia-smi" [35]. Do đó, bộ nhớ không sử dụng của tensor trong tối ưu hóa bộ nhớ dần dần qua các lần lặp huấn luyện vẫn sẽ hiển thị như đã sử dụng trong "nvidia-smi". Để giảm tổng mức sử dụng bộ nhớ của quá trình huấn luyện/tinh chỉnh với các giải thích trên, chúng ta cần cân bằng mức sử dụng bộ nhớ của tất cả các lần lặp từ lần lặp đầu tiên đến lần cuối cùng (xem Hình 9). SLIMFIT nhằm giảm tổng mức sử dụng bộ nhớ của các mô hình transformer lớn trong quá trình tinh chỉnh.

0 5 10 156668707274
Lần lặp huấn luyệnBộ nhớ của
Kích hoạt (GB)Baseline
0 5 10 151111.51212.513
Lần lặp huấn luyệnBộ nhớ của
Kích hoạt (GB)SlimFit
(a) Bộ nhớ kích hoạt
0 5 10 15020406080
Lần lặp huấn luyệnTổng bộ nhớ
trên thiết bị (GB)Baseline
0 5 10 150102030
Lần lặp huấn luyệnTổng bộ nhớ
trên thiết bị (GB)SlimFit
(b) Tổng bộ nhớ trên thiết bị

Hình 9: Tổng bộ nhớ GPU trên thiết bị và bộ nhớ kích hoạt trong các lần lặp huấn luyện khác nhau khi tinh chỉnh ViT trên ImageNet với kích thước batch 128 với tỷ lệ đóng băng 95% so với baseline. SLIMFIT cân bằng mức sử dụng bộ nhớ của kích hoạt bằng cách đóng băng để giảm tổng mức sử dụng bộ nhớ trên thiết bị của quá trình tinh chỉnh. Trong khi mức sử dụng bộ nhớ của kích hoạt thay đổi tại mỗi lần lặp khi sử dụng SLIMFIT, các thay đổi tương đối nhỏ nhờ kỹ thuật cân bằng tải được mô tả trong Phần 4.

B So sánh với các phương pháp đóng băng hiện có
Ở đây, chúng tôi mô tả những khác biệt chính giữa SLIMFIT và các phương pháp đóng băng khác bao gồm SmartFRZ [26], PipeTransformer [27] và AutoFreeze [25]. Khác biệt chính là các công trình nói trên chủ yếu tập trung vào khai thác đóng băng để tăng tốc quá trình huấn luyện/tinh chỉnh. Về mặt khái niệm, SmartFRZ, PipeTransformer và AutoFreeze đóng băng dần dần các lớp khi quá trình huấn luyện tiến triển. Trong các phương pháp này, lần lặp huấn luyện đầu tiên bắt đầu mà không đóng băng nơi tất cả các lớp được cập nhật. Trong các lần lặp tiếp theo, các phương pháp này sau đó bắt đầu đóng băng dần dần từ các lớp đầu xuống các lớp cuối cùng trong mô hình theo thứ tự. Ví dụ, AutoFreeze thực hiện epoch đầu tiên mà không đóng băng, epoch thứ hai trong khi đóng băng 5 lớp đầu tiên, epoch thứ ba trong khi đóng băng 8 lớp đầu tiên và epoch thứ tư trong khi đóng băng 11 lớp đầu tiên khi tinh chỉnh BERT. Trong ví dụ này, yêu cầu bộ nhớ và tính toán của mỗi epoch khác với các epoch khác vì mỗi epoch thể hiện một mức độ đóng băng khác nhau. Điều này cho phép khai thác các tài nguyên tính toán và bộ nhớ không sử dụng để tăng tốc thêm quá trình bằng cách tăng kích thước batch khi bộ nhớ giảm trong các lần lặp huấn luyện [25] hoặc tăng độ rộng data-parallel thông qua pipelining [27]. Vì lần lặp huấn luyện đầu tiên (hoặc thậm chí epoch) của các phương pháp này thực hiện quá trình huấn luyện mà không đóng băng, yêu cầu bộ nhớ tổng thể của chúng được báo cáo bởi "nvidia-smi" tương tự như huấn luyện không đóng băng như được thảo luận trong Phụ lục A. Nói cách khác, GPU dưới sử dụng vẫn phải có thể đáp ứng yêu cầu bộ nhớ của việc huấn luyện mà không đóng băng. Ví dụ, việc tinh chỉnh ViT với kích thước batch 128 trên một GPU NVIDIA V100 32GB duy nhất sử dụng các phương pháp như vậy dẫn đến lỗi hết bộ nhớ.

SLIMFIT, mặt khác, tập trung vào việc giảm yêu cầu bộ nhớ tổng thể của quá trình tinh chỉnh bằng cách đóng băng. Trái ngược với các phương pháp nói trên (tức là SmartFRZ, PipeTransformer và AutoFreeze), SLIMFIT đóng băng các lớp tại mỗi lần lặp huấn luyện duy nhất từ lần lặp đầu tiên đến lần cuối cùng với tỷ lệ đóng băng cố định. Với cân bằng tải sử dụng lượng tử hóa, SLIMFIT đảm bảo rằng yêu cầu bộ nhớ của mỗi lần lặp duy nhất vẫn gần như giống nhau trong suốt quá trình tinh chỉnh. Điều này cho phép SLIMFIT thực hiện các quá trình tinh chỉnh tốn nhiều bộ nhớ với kích thước batch lớn trên một GPU 32GB duy nhất như ViT trên ImageNet với kích thước batch 128 trong khi điều này thường yêu cầu ba GPU 32GB.

C Kiến trúc của các mô hình dựa trên Transformer
Hình 10 cho thấy kiến trúc tổng thể của các mô hình dựa trên transformer bao gồm một lớp embedding ban đầu, theo sau là các khối lặp lại của multi-head attention (MHA) và feed-forward network (FFN).
Chi tiết của mỗi lớp bên trong các mô-đun MHA và FFN được cung cấp trong Bảng 6.

EmbeddingMulti-head attentionAdd & LayernormFeed-forward networkAdd & Layernorm
MHAFFN
×L

Hình 10: Kiến trúc chính của BERT. Lưu ý rằng ViT có kiến trúc tương tự với LayerNorm nằm trước khối MHA. L biểu thị số lớp attention.

Mô-đun Loại lớp Mô tả # Kích hoạt
MHA Dense attention.query B*T*H
Dense attention.key B*T*H
Dense attention.value B*T*H
MatMul NA B*T*H(2×)
Softmax NA B*T*T
MatMul NA B*T*H&B*T*T
Dense attention.output B*T*H
LayerNorm attention.output B*T*H
FFN Dense intermediate B*T*H
GELU NA B*T*4*H
Dense output B*T*4*H
LayerNorm output B*T*H

Bảng 6: Chi tiết các lớp trong mô-đun MHA và FFN của BERT trong đó B, T, H biểu thị kích thước batch, độ dài chuỗi, kích thước ẩn, tương ứng. ViT có cùng cấu trúc với các mô tả khác nhau.

D Phân tích lý thuyết
D.1 Phân tích hội tụ
Trong phần này, chúng tôi cung cấp phân tích hội tụ cho chiến lược đóng băng của chúng tôi. Cụ thể hơn, chúng tôi chứng minh sự hội tụ của stochastic gradient descent (SGD) khi xem xét đóng băng trong các lần lặp cập nhật. Cho hàm loss f, chúng tôi giả định rằng các tham số được khởi tạo với một số giá trị và được ký hiệu là vector w0 ∈ R^d. Cho ví dụ huấn luyện, các tham số được cập nhật bằng

wt+1 = wt - γt∇f(wt), (8)

trong đó wt biểu thị vector tham số tại thời điểm t, γt là tỷ lệ học, và ∇f đại diện cho gradient của hàm loss. Chúng tôi giả định rằng độ lớn của các mẫu gradient được giới hạn bởi một hằng số G > 0 cho tất cả x trong không gian sao cho

||∇f(x)|| ≤ G. (9)

Ngoài ra, chúng tôi giả định rằng tồn tại một hằng số L > 0 cho bất kỳ vector u ∈ R^d nào trong đó chúng ta có

|u^T∇²f(x)u| ≤ L||u||². (10)

--- TRANG 14 ---
Cho Phương trình (9) và Phương trình (10), thực hiện khai triển Taylor trên Phương trình (8) tương tự như [29] dẫn đến

E[f(wt+1)] ≤ E[f(wt)] - γtE[||∇f(wt)||²] + γt²G²L/2, (11)

trong đó E biểu thị giá trị kỳ vọng.

Bây giờ, hãy giả sử rằng lớp chứa vector tham số bị đóng băng tại lần lặp huấn luyện t. Trong trường hợp này, ∇f(wt) bằng 0 và do đó wt+1 bằng wt. Trong kịch bản đóng băng này, Phương trình (11) vẫn đúng vì γt²G²L/2 lớn hơn 0.

Bằng cách sắp xếp lại các số hạng trong Phương trình (11), tổng qua T lần lặp và telescoping tổng, chúng ta có

∑(t=0 to T-1) γtE[||∇f(wt)||²] ≤ ∑(t=0 to T-1) (E[f(wt)] - E[f(wt+1)]) + ∑(t=0 to T-1) γt²G²L/2, (12)

= f(w0) - f(wT) + G²L/2 ∑(t=0 to T-1) γt², (13)

≤ f(w0) - f(w*) + G²L/2 ∑(t=0 to T-1) γt², (14)

trong đó w* biểu thị một nghiệm tối ưu. Cho bất đẳng thức trên, chúng tôi đã chỉ ra rằng chứng minh hội tụ của SGD vẫn nguyên vẹn trong khi giới thiệu đóng băng cho các lần lặp huấn luyện cụ thể.

D.2 Lan truyền ngược với một lớp đóng băng
Ở đây, chúng tôi cung cấp một ví dụ đơn giản chứng minh cách gradient được lan truyền ngược đến lớp đầu tiên của mạng nơ-ron trong khi lớp giữa của nó bị đóng băng. Để làm điều này, hãy thực hiện lan truyền ngược sử dụng mạng 3 lớp như một ví dụ. Về mặt toán học, kiến trúc của mạng này có thể được mô tả như sau:

y1 = xW1 + b1, (15)
y2 = y1W2 + b2, (16)
y3 = y2W3 + b3, (17)

trong đó W1, W2, W3, b1, b2 và b3 là trọng số và bias của mạng. Trong ví dụ này, x, y1 và y2 là đầu vào của lớp đầu tiên, lớp thứ hai và lớp thứ ba, tương ứng. Bây giờ, hãy suy ra các phương trình lan truyền ngược với loss L sử dụng quy tắc chuỗi như sau (xin lưu ý rằng chúng ta có ∂L/∂y3 bằng cách tính toán loss nơi ∂ biểu thị đạo hàm riêng):

∂L/∂W3 = ∂L/∂y3 × ∂y3/∂W3 = ∂L/∂y3 × y2, (18)
∂L/∂b3 = ∂L/∂y3 × ∂y3/∂b3 = ∂L/∂y3 × 1 = ∂L/∂y3, (19)
∂L/∂W2 = ∂L/∂y3 × ∂y3/∂y2 × ∂y2/∂W2 = ∂L/∂y3 × W3^T × y1, (20)
∂L/∂b2 = ∂L/∂y3 × ∂y3/∂y2 × ∂y2/∂b2 = ∂L/∂y3 × W3^T × 1 = ∂L/∂y3 × W3^T, (21)
∂L/∂W1 = ∂L/∂y3 × ∂y3/∂y2 × ∂y2/∂y1 × ∂y1/∂W1 = ∂L/∂y3 × W3^T × W2^T × x, (22)
∂L/∂b1 = ∂L/∂y3 × ∂y3/∂y2 × ∂y2/∂y1 × ∂y1/∂b1 = ∂L/∂y3 × W3^T × W2^T × 1 = ∂L/∂y3 × W3^T × W2^T. (23)

Cho các phương trình trên, để cập nhật trọng số mạng (tức là W1, W2, và W3), chúng ta cần lưu trữ x, y1 và y2 trong các tính toán forward vì chúng được yêu cầu trong Phương trình (18), Phương trình (20) và Phương trình (22) trong các tính toán backward.

Bây giờ, giả sử lớp giữa bị đóng băng. Trong trường hợp này, không cần tính Phương trình (20) và do đó không cần lưu trữ y1 trong các tính toán forward. Tất nhiên, việc loại bỏ y1 không ảnh hưởng đến các tính toán backward của lớp đầu tiên vì Phương trình (22) và Phương trình (23) độc lập với y1.

--- TRANG 15 ---
E Chuyển đổi giữa 8-bit integer và 32-bit floating-point
Thuật toán 2 cho thấy quá trình chuyển đổi giữa định dạng fixed-point 8-bit và floating-point 32-bit. Đáng chú ý rằng cùng quy trình có thể được sử dụng cho việc chuyển đổi giữa định dạng fixed-point 4-bit và floating-point 32-bit. Hơn nữa, hàm lượng tử hóa được sử dụng để nén các tensor được cache chỉ trong quá trình lan truyền tiến. Tất nhiên, cả tính toán tiến và lùi vẫn được thực hiện bằng tính toán floating-point 32-bit như được hiển thị trong Thuật toán 4 nơi hàm "compress" trong trường hợp này là hàm lượng tử hóa (tức là chuyển đổi từ floating-point 32-bit sang integer 8-bit) và hàm "decompress" thực hiện các tính toán ngược (tức là chuyển đổi từ integer 8-bit sang floating-point 32-bit).

Thuật toán 2 Chuyển đổi giữa integer 8-bit và floating-point 32-bit.
Mô tả: số bit nguyên như ib, số bit thập phân như fb, đầu vào x, đầu ra y
Chuyển đổi 32 bit sang 8 bit:
y = clamp(round(x*2^fb), -2^(fb+ib-1), 2^(fb+ib-1)-1)
Chuyển đổi 8 bit sang 32 bit:
x = y/2^fb

F Thuật toán cắt tỉa
Thuật toán cắt tỉa được thực hiện trong vài bước. Trong bước đầu tiên, vector đầu vào được sắp xếp từ giá trị lớn nhất đến nhỏ nhất cùng với chỉ số và kích thước của vector dày đặc. Sau đó chúng ta chỉ giữ và cache 10% giá trị lớn nhất của vector đầu vào cho các tính toán backward như bước thứ hai. Đáng chú ý rằng việc cắt tỉa vượt quá 90% dẫn đến suy giảm độ chính xác đáng kể. Trong quá trình lan truyền ngược, chúng ta tạo một tensor có giá trị zero sử dụng kích thước của vector dày đặc và sau đó thay thế các giá trị zero bằng 10% giá trị lớn nhất sử dụng chỉ số tương ứng của chúng. Thuật toán 3 cho thấy quá trình cắt tỉa trong các tính toán tiến và quá trình khôi phục trong các tính toán lùi. Đáng chú ý rằng hàm cắt tỉa được sử dụng để nén các tensor được cache chỉ trong quá trình lan truyền tiến. Tất nhiên, cả tính toán tiến và lùi vẫn được thực hiện bằng tính toán floating-point 32-bit như được hiển thị trong Thuật toán 4 nơi hàm "compress" trong trường hợp này là hàm cắt tỉa và hàm "decompress" thực hiện các tính toán khôi phục.

G Chi tiết về các tác vụ CV/NLP, Phép đo và Cài đặt siêu tham số
Đối với các tác vụ hiểu ngôn ngữ và tác vụ CV, chúng tôi sử dụng BERT-base-cased và ViT-base trong suốt bài báo này, tương ứng. BERT-base và ViT-base được tiền huấn luyện trên ImageNet-21k được cấu hình theo [1] và [2], tương ứng. Chúng tôi sử dụng AdamW (beta1=0.9, beta2=0.999 và L2 weight decay

Thuật toán 3 Mô tả quá trình cắt tỉa trong các tính toán tiến và quá trình khôi phục trong các tính toán lùi.
Mô tả: x: vector đầu vào, xs: vector đầu vào đã sắp xếp, xidx: chỉ số của vector đầu vào đã sắp xếp, ys: 10% giá trị lớn nhất, yidx: chỉ số của 10% giá trị lớn nhất, y: vector đầu ra, sort: hàm sắp xếp, zeros: hàm tạo tensor có giá trị zero, và scatter: hàm thay thế giá trị zero bằng giá trị tensor từ ys theo chỉ số.
Quá trình cắt tỉa:
xs, xidx = sort(x)
ys, yidx = xs[0:int(x.numel()*0.1)], xidx[0:int(x.numel()*0.1)]
Quá trình khôi phục:
y = zeros(x.numel())
y = scatter(y, ys, yidx)

--- TRANG 16 ---
của 0.01) như optimizer và giảm tuyến tính của tỷ lệ học với warmup từ 0 đến 0.1 cho cả hai mô hình. Chúng tôi đánh giá BERT-base trên một số tác vụ downstream từ GLUE benchmark và SQuAD 2.0. Chúng tôi sử dụng tương quan Spearman cho STS-B, tương quan Matthews cho CoLA, độ chính xác cho RTE, MRPC, SST-2 QQP, QNLI và MNLI m(matched), và điểm F1 cho SQuAD 2.0. Đối với các tác vụ downstream từ GLUE benchmark, chúng tôi sử dụng độ dài chuỗi 128 trong khi chúng tôi áp dụng độ dài chuỗi 384 cho tác vụ trả lời câu hỏi trên SQuAD 2.0. Đối với CIFAR-10, CIFAR-100 và ImageNet, chúng tôi sử dụng độ chính xác top-1 như metric đánh giá. Đối với các tác vụ phân loại hình ảnh, chúng tôi sử dụng kích thước patch 16 với độ phân giải 224 cho CIFAR-10/CIFAR-100 và độ phân giải 384 cho ImageNet. Tùy thuộc vào tác vụ, tỷ lệ học thay đổi từ 4e-5 đến 1.8e-4. Đối với tất cả các thí nghiệm trong bài báo này, chúng tôi sử dụng 3 epoch cho tinh chỉnh. Cài đặt siêu tham số của mỗi tác vụ được tóm tắt trong Bảng 7. Đáng chú ý rằng các mô hình ViT cũng có thể được tinh chỉnh bằng SGD. Tuy nhiên, việc tinh chỉnh các mô hình ViT bằng SGD yêu cầu nhiều epoch hơn so với AdamW để có được độ chính xác tương tự.

Trong bài báo này, chúng tôi đo kết quả thực nghiệm trực tiếp từ (các) GPU NVIDIA V100 32GB mà không có bất kỳ trao đổi bộ nhớ nào giữa CPU và (các) GPU. Tổng mức sử dụng bộ nhớ GPU trên thiết bị của quá trình tinh chỉnh được đo bằng "nvidia-smi". Chúng tôi đo thời gian wall-clock (tức là độ trễ) của quá trình tinh chỉnh bằng API sự kiện CUDA trong PyTorch (tức là "torch.cuda.Event"). Dấu chân bộ nhớ của kích hoạt trên (các) GPU được đo bằng API quản lý bộ nhớ của PyTorch (tức là "torch.cuda.memory_allocated").

Bảng 7: Cài đặt siêu tham số của mỗi tác vụ NLP/CV.
Dataset Mô hình Optimizer Tỷ lệ học Warmup Metric đánh giá
MNLI m bert-base-cased AdamW 4e-5 0 độ chính xác phần trăm
QQP bert-base-cased AdamW 5e-5 0 độ chính xác phần trăm
QNLI bert-base-cased AdamW 5e-5 0 độ chính xác phần trăm
SST-2 bert-base-cased AdamW 8e-5 0 độ chính xác phần trăm
CoLA bert-base-cased AdamW 8e-5 0.1 tương quan Matthew
STS-B bert-base-cased AdamW 8e-5 0 tương quan Spearman
MRPC bert-base-cased AdamW 1.25e-4 0 độ chính xác phần trăm
RTE bert-base-cased AdamW 1.2e-4 0 độ chính xác phần trăm
SQuAD 2.0 bert-base-uncased AdamW 1.8e-4 0.1 điểm F1
CIFAR-10 vit-base-patch16-224-in21k AdamW 7.5e-5 0 độ chính xác phần trăm
CIFAR-100 vit-base-patch16-224-in21k AdamW 5.5e-5 0 độ chính xác phần trăm
ImageNet vit-base-patch16-384 AdamW 5e-5 0 độ chính xác phần trăm

H Tác động của lượng tử hóa và cắt tỉa
Trong công việc này, chúng tôi sử dụng lượng tử hóa và cắt tỉa cho một vài lớp cụ thể để cân bằng số lượng kích hoạt trên tất cả các lớp và giảm dấu chân bộ nhớ của kích hoạt tĩnh. Chúng tôi sử dụng lượng tử hóa 8-bit cho kích hoạt của lớp linear mất cân bằng và MatMul. Chúng tôi cũng lượng tử hóa kích hoạt của GELU bằng 4 bit. Việc cắt tỉa LayerNorm được thực hiện khi lớp này được giữ đóng băng. Đáng chú ý rằng cả lượng tử hóa và cắt tỉa đều không ảnh hưởng đến các tính toán tiến. Chúng chỉ được sử dụng để nén kích hoạt để cache. Để hiển thị tác động của các phương pháp nén như vậy, chúng tôi báo cáo đánh giá độ chính xác của BERT trên các dataset CoLA và MRPC có và không có lượng tử hóa hoặc cắt tỉa trong Bảng 8. Kết quả thực nghiệm cho thấy không có mất hiệu suất đáng kể do các kỹ thuật nén.

Bảng 8: Tác động của lượng tử hóa và cắt tỉa lên đánh giá độ chính xác.
Dataset Baseline Lượng tử hóa của Cắt tỉa của Tất cả
Linear MatMul GELU LayerNorm cùng nhau
CoLA 58.9 58.9 60.6 60.0 59.7 59.7
MRPC 86.4 86.4 86.3 86.3 86.3 86.3

I Thảo luận về thời gian Wall-Clock
So với huấn luyện không đóng băng, SLIMFIT giới thiệu các tính toán thêm và cũng bỏ qua các tính toán gradient trọng số cho các lớp đóng băng cùng lúc. Nguồn chính của chi phí tính toán trong SLIMFIT là lượng tử hóa và cắt tỉa kích hoạt. Chi phí lượng tử hóa là do

--- TRANG 17 ---
việc chuyển đổi giữa các mức độ chính xác khác nhau (tức là giữa 8 bit và định dạng floating-point 32-bit) như được thảo luận trong Phụ lục E. Cắt tỉa cũng yêu cầu sắp xếp các giá trị để giữ 10% giá trị lớn nhất của chúng, gây ra chi phí tính toán bổ sung. Tính toán metric khoảng cách trọng số là một nguồn chi phí tính toán khác.

Thuật toán 4 Mô tả việc bỏ qua tính toán gradient trọng số khi lớp bị đóng băng. Trong ví dụ này, chúng tôi giả định kích hoạt của lớp đóng băng yêu cầu nén (ví dụ: một lớp linear mất cân bằng hoặc LayerNorm). Kích hoạt được ký hiệu là "input" và được cache bằng cách sử dụng lượng tử hóa hoặc cắt tỉa tùy thuộc vào loại lớp như một phương pháp nén. Các hàm nén và giải nén được ký hiệu là "compress" và "decompress". Vì trọng số được định nghĩa là "Parameter" trong PyTorch, việc cache trọng số không giới thiệu bất kỳ bộ nhớ thêm nào.

class ILSFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input: torch.Tensor, weight: torch.nn.Parameter, requires_grad):
        # Tính toán các tính toán tiến để có được out
        if requires_grad:
            ctx.save_for_backward(compress(input), weight)
        else:
            ctx.save_for_backward(weight)
        ctx.requires_grad = requires_grad
        return out
    
    @staticmethod
    def backward(ctx, grad_output: torch.Tensor):
        if ctx.requires_grad:
            input, weight = decompress(ctx.saved_tensors[0]), ctx.saved_tensors[1]
            # Tính toán các tính toán lùi để có được grad_input và grad_weight
        else:
            weight = ctx.saved_tensors[1]
            grad_weight = None
            # Tính toán các tính toán lùi để có được grad_input
        return grad_input, grad_weight, None

Mặt khác, SLIMFIT bỏ qua các tính toán gradient trọng số của các lớp đóng băng bằng cách sử dụng "requires_grad" của PyTorch như được hiển thị trong Thuật toán 4. Khi một lớp hoạt động bị đóng băng, không cần tính gradient trọng số của nó như được thảo luận trong Phụ lục D.2, điều này giảm thời gian wall-clock. Lượng tăng tốc do các tính toán bị bỏ qua phụ thuộc rất nhiều vào các siêu tham số của mạng như tỷ lệ đóng băng. Do đó, thời gian wall-clock của mỗi mạng thay đổi từ mạng này sang mạng khác tùy thuộc vào các siêu tham số. Ví dụ, Hình 11 cho thấy thời gian wall-clock của việc tinh chỉnh ViT trên ImageNet sử dụng kích thước batch 32 trên các tỷ lệ đóng băng khác nhau. Theo kết quả thực nghiệm, chi phí tính toán của SLIMFIT chiếm ưu thế đối với các tỷ lệ đóng băng nhỏ. Tuy nhiên, khi tỷ lệ đóng băng tăng, tăng tốc của các tính toán gradient bị bỏ qua vượt qua chi phí tính toán của SLIMFIT nơi SLIMFIT với tỷ lệ đóng băng 95% dẫn đến thời gian wall-clock tương tự như

0 20 40 60 80 10018202224
Tỷ lệ đóng băng (%)Giờ mỗi epochSlimFit

Hình 11: Thời gian wall-clock của việc tinh chỉnh ViT trên ImageNet với kích thước batch 32 trên các tỷ lệ đóng băng khác nhau.

--- TRANG 18 ---
baseline. Đáng chú ý rằng baseline là điểm ở tỷ lệ đóng băng 0 nơi không có đóng băng nào được sử dụng trong quá trình tinh chỉnh.

J Mô tả các lớp trong Heatmap
Mô tả các lớp liên quan đến các chỉ số trong Hình 8 được cung cấp trong Thuật toán 5. Đáng chú ý rằng các lớp được ký hiệu bởi "bert.encoder.layer[i].attention" thuộc về mô-đun MHA trong khi các lớp còn lại bên trong vòng lặp thuộc về mô-đun FFN.

Thuật toán 5 Mô tả các lớp liên quan đến các chỉ số trong Hình 8.
bert.embeddings.word_embeddings.weight
bert.embeddings.position_embeddings.weight
bert.embeddings.token_type_embeddings.weight
bert.embeddings.LayerNorm.weight
for i=0 to 11: do
    bert.encoder.layer[i].attention.self.query.weight
    bert.encoder.layer[i].attention.self.key.weight
    bert.encoder.layer[i].attention.self.value.weight
    bert.encoder.layer[i].attention.output.dense.weight
    bert.encoder.layer[i].attention.output.LayerNorm.weight
    bert.encoder.layer[i].intermediate.dense.weight
    bert.encoder.layer[i].output.dense.weight
    bert.encoder.layer[i].output.LayerNorm.weight
end for
bert.pooler.dense.weight
classifier.weight
