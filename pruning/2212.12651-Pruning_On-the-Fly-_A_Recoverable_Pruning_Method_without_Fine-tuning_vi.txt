# Cắt Tỉa Ngay Lập Tức: Phương Pháp Cắt Tỉa Có Thể Phục Hồi Mà Không Cần Tinh Chỉnh
Dan Liu, Xue Liu
Đại học McGill
daniel.liu@mail.mcgill.ca, xue.liu@mcgill.ca

Tóm tắt
Hầu hết các công trình cắt tỉa hiện tại đều tốn nhiều tài nguyên, vì chúng yêu cầu đào tạo lại hoặc tinh chỉnh các mô hình đã cắt tỉa nhằm mục đích độ chính xác. Chúng tôi đề xuất một phương pháp cắt tỉa không cần đào tạo lại dựa trên học siêu cầu và các thành phần phạt loss. Thành phần phạt loss được đề xuất đẩy một số trọng số mô hình xa khỏi số không, trong khi các giá trị trọng số còn lại được đẩy gần số không và có thể được cắt tỉa an toàn mà không cần đào tạo lại và với sự sụt giảm độ chính xác không đáng kể. Ngoài ra, phương pháp đề xuất của chúng tôi có thể ngay lập tức khôi phục độ chính xác của mô hình đã cắt tỉa bằng cách thay thế các giá trị đã cắt bằng giá trị trung bình của chúng. Phương pháp của chúng tôi đạt được kết quả tiên tiến về cắt tỉa không cần đào tạo lại và được đánh giá trên ResNet-18/50 và MobileNetV2 với bộ dữ liệu ImageNet. Có thể dễ dàng có được mô hình ResNet18 đã cắt tỉa 50% với sự sụt giảm độ chính xác 0.47%. Nếu có tinh chỉnh, kết quả thí nghiệm cho thấy phương pháp của chúng tôi có thể tăng đáng kể độ chính xác của các mô hình đã cắt tỉa so với các công trình hiện có. Ví dụ, độ chính xác của mô hình MobileNetV2 đã cắt tỉa 70% (ngoại trừ lớp tích chập đầu tiên) chỉ giảm 3.5%, ít hơn nhiều so với mức giảm độ chính xác 7-10% với các phương pháp truyền thống.

Giới thiệu
Các mô hình mạng nơ-ron sâu (DNN) chứa hàng triệu tham số, khiến chúng không thể triển khai trên các thiết bị edge. Kích thước mô hình và hiệu quả suy luận là những mối quan tâm chính khi triển khai dưới các ràng buộc tài nguyên. Những nỗ lực nghiên cứu đáng kể đã được thực hiện để nén các mô hình DNN. Lượng tử hóa và cắt tỉa phổ biến vì chúng có thể giảm kích thước mô hình và chi phí tính toán.

Có hai chủ đề nghiên cứu thú vị trong cắt tỉa: làm thế nào để giảm thời gian tinh chỉnh và làm thế nào để nhanh chóng khôi phục độ chính xác của mạng từ việc cắt tỉa. Mục đích của việc cắt tỉa mô hình là để có được một mô hình DNN với độ chính xác tối đa và tỷ lệ nén. Tìm ra chiến lược cắt tỉa phù hợp là thách thức chính. Hầu hết các công trình hiện có cần tinh chỉnh. Các bước cắt tỉa và tinh chỉnh được lặp lại nhiều lần để giảm dần kích thước mô hình và duy trì độ chính xác cao hơn. Quá trình tinh chỉnh tốn thời gian và yêu cầu toàn bộ bộ dữ liệu đào tạo. Do đó, các nghiên cứu đã được thực hiện để khám phá các cách cải thiện hiệu quả tinh chỉnh và khả năng phục hồi của mạng nơ-ron chỉ với một vài dữ liệu đào tạo. Tuy nhiên, một số giá trị trọng số được cố định thành số không vĩnh viễn trong quá trình cắt tỉa. Mạng nơ-ron đang thay đổi trong quá trình đào tạo, việc cố định một số giá trị trọng số thành số không có thể hạn chế khả năng học của nó. Các giá trị trọng số bị cắt tỉa sai là không thể tránh khỏi và khó có thể được phục hồi hoặc sửa chữa vì thông tin trọng số ban đầu bị mất. Do đó, một số nhà nghiên cứu đề xuất cắt tỉa trước hoặc trong quá trình đào tạo để mạng có thể thích ứng với việc cắt tỉa.

Trong công trình này, chúng tôi nhằm loại bỏ việc tinh chỉnh sau khi cắt tỉa, tức là tận dụng việc tinh chỉnh trước khi cắt tỉa để xây dựng các ứng cử viên cắt tỉa tiềm năng đáng tin cậy và chính xác. Cụ thể hơn, so với các công trình khác cắt tỉa mô hình dày đặc trực tiếp, phương pháp của chúng tôi giảm khoảng cách cosine giữa các giá trị trọng số dày đặc và mặt nạ cắt tỉa của nó trước hành động cắt tỉa. Phương pháp của chúng tôi ít dễ bị cắt tỉa sai vì mặt nạ cắt tỉa liên tục thích ứng với việc đào tạo và các trọng số tiềm năng bị cắt được đẩy gần số không. Khi khoảng cách cosine đủ nhỏ, mô hình có thể được cắt tỉa đến nhiều mức thưa khác nhau mà không cần tinh chỉnh gì. Với phương pháp của chúng tôi, mô hình ResNet-18 đã cắt tỉa có thể đạt đến 50% độ thưa với ít hơn 0.5% sụt giảm độ chính xác. Kết hợp với phương pháp phục hồi tức thì được đề xuất, độ thưa này có thể được đẩy lên đến 70% với 0.3% sụt giảm độ chính xác. Những đóng góp chính của chúng tôi như sau:

• Chúng tôi đề xuất một phương pháp cắt tỉa ngay lập tức sử dụng các thành phần chính quy hóa để tối thiểu hóa khoảng cách cosine giữa các giá trị trọng số và mặt nạ cắt tỉa của nó trong quá trình đào tạo. Khi việc đào tạo hoàn tất, các giá trị trọng số sẽ được tách thành hai nhóm, một nhóm gần số không và nhóm kia xa số không. Mô hình đã xử lý có thể được cắt tỉa ngay lập tức mà không cần tinh chỉnh gì.

• Chúng tôi đề xuất một phương pháp để tăng khả năng phục hồi của mô hình. Chúng tôi chỉ ra rằng việc thay thế một phần các trọng số đã cắt bằng giá trị trung bình của chúng có thể khôi phục một phần hiệu suất của mô hình ngay lập tức. Phương pháp cắt tỉa của chúng tôi có thể cải thiện đáng kể khả năng phục hồi tức thì này. Ngoài ra, phương pháp của chúng tôi có thể cải thiện đáng kể tiềm năng cắt tỉa dưới các thiết lập độ thưa cao với tinh chỉnh. Ví dụ, đối với cấu trúc MobileNetV2 với độ thưa 70% (ngoại trừ lớp tích chập đầu tiên), độ chính xác đã tinh chỉnh của các phương pháp khác giảm 7-10%, trong khi của chúng tôi chỉ giảm 3.5%.

Công trình liên quan
Các kỹ thuật nén mô hình, như lượng tử hóa (Wu et al. 2016; Li, Zhang, và Liu 2016) và cắt tỉa (Han, Mao, và Dally 2015; Li et al. 2016), đã trở thành một chủ đề nghiên cứu xu hướng vì chúng đóng góp vào kích thước mô hình nhỏ hơn và suy luận nhanh hơn. Một cái nhìn tổng quan toàn diện về cắt tỉa mô hình có thể được tìm thấy trong (Liang et al. 2021; Blalock et al. 2020).

Các phương pháp cắt tỉa có thể được phân loại theo cấu trúc và lịch trình. Cấu trúc cắt tỉa xác định liệu có cắt tỉa lớp (Chin, Zhang, và Marculescu 2018; Dong, Chen, và Pan 2017), toàn bộ kernel (Li et al. 2016; Hu et al. 2016; Alvarez và Salzmann 2017), hoặc các giá trị trọng số cụ thể (Han, Mao, và Dally 2015). Lịch trình cắt tỉa xác định tỷ lệ phần trăm các giá trị trọng số cần loại bỏ qua mỗi giai đoạn. Một số kỹ thuật thực hiện cắt tỉa một bước đến trọng số mục tiêu (Liu et al. 2018). Các kỹ thuật khác thay đổi tỷ lệ cắt tỉa trong quá trình đào tạo (Han et al. 2016; Gale, Elsen, và Hooker 2019) hoặc lặp đi lặp lại cắt tỉa một phần cố định của các giá trị trọng số qua một số vòng lặp (Han et al. 2015).

Cắt tỉa không cần tinh chỉnh làm giảm độ chính xác của mô hình. Tinh chỉnh, mặc dù tốn thời gian, thường được thực hiện để khôi phục độ chính xác. Nhiều công trình khám phá khả năng cắt tỉa trong hoặc thậm chí trước khi đào tạo mà không cần tinh chỉnh (Guo, Yao, và Chen 2016; Molchanov, Ashukha, và Vetrov 2017; Lee, Ajanthan, và Torr 2018; Gale, Elsen, và Hooker 2019). Guo, Yao, và Chen (2016) đề xuất một phương pháp cắt tỉa có thể phục hồi sử dụng ma trận mặt nạ nhị phân với hàm điểm để xác định liệu một giá trị trọng số đơn lẻ có bị cắt tỉa hay không. Cắt tỉa bộ lọc mềm (SFP) (He et al. 2018) mở rộng việc cắt tỉa có thể phục hồi hơn nữa bằng cách cho phép cập nhật các bộ lọc đã cắt tỉa. Dropout biến phân thưa (Molchanov, Ashukha, và Vetrov 2017) sử dụng một siêu tham số dropout khuyến khích độ thưa và hoạt động như một cơ sở để chấm điểm nhằm xác định trọng số nào cần cắt tỉa. Cắt tỉa động (Lin et al. 2017; Wu et al. 2018), chọn các giá trị trọng số đã cắt tỉa bằng cách sử dụng các thành phần quyết định, là một lĩnh vực nghiên cứu khác trong cắt tỉa không cần tinh chỉnh. Tuy nhiên, một số công trình cắt tỉa động tốn nhiều tài nguyên vì quyết định cắt tỉa được thực hiện theo thời gian thực (Leroux et al. 2017; Li et al. 2019; Gao et al. 2018).

Trong công trình này, chúng tôi nghiên cứu khả năng cắt tỉa và phục hồi tức thì của các mạng nơ-ron. So với các phương pháp nêu trên, phương pháp của chúng tôi nhằm sử dụng phạt loss để giảm khoảng cách cosine giữa các giá trị trọng số dày đặc và mặt nạ nhị phân của nó. Do đó, quá trình tối ưu hóa có thể thay đổi phân phối trọng số (Hình 1) và có lợi cho việc cắt tỉa. Phương pháp của chúng tôi có lợi thế so với các phương pháp cắt tỉa hiện có ở chỗ nó ngăn chặn việc cắt tỉa sai và khôi phục độ chính xác của mô hình. Hơn nữa, chiến lược của chúng tôi không yêu cầu lịch trình đào tạo phức tạp hoặc hàm điểm tinh vi. Việc cắt tỉa một lần dựa trên độ lớn đơn giản có thể tạo ra kết quả đáng chú ý.

Cắt tỉa, Phục hồi và Tinh chỉnh

Trong phần này, chúng tôi chỉ ra cách thực hiện cắt tỉa ngay lập tức và cách khôi phục độ chính xác của mô hình tức thì hoặc từ từ. Trước khi cắt tỉa, chúng tôi áp dụng một thành phần chính quy hóa vào hàm mục tiêu để thao tác phân phối trọng số (Hình 1). Sau đó, việc cắt tỉa không có cấu trúc dựa trên độ lớn có thể được thực hiện với sự sụt giảm độ chính xác không đáng kể. Mô hình đã cắt tỉa có thể được phục hồi bằng cách thay thế các giá trị đã cắt bằng giá trị trung bình của chúng ngay lập tức hoặc được phục hồi bằng tinh chỉnh truyền thống với độ thưa cao hơn nhiều.

Học siêu cầu được chính quy hóa

Học siêu cầu (Liu et al. 2017) hạn chế độ lớn của các vectơ đầu vào và trọng số bằng một. Một biểu diễn tổng quát của một lớp siêu cầu được định nghĩa là:

y = σ(W^T x);  (1)

trong đó σ biểu thị một hàm kích hoạt phi tuyến, W ∈ R^(m×n) là ma trận trọng số, x ∈ R^m là vectơ đầu vào cho lớp, và y ∈ R^n là vectơ đầu ra. Mỗi vectơ trọng số cột w_j ∈ R^m của W tuân theo ||w_j||_2 = 1 cho tất cả j = 1, ..., n, và vectơ đầu vào x thỏa mãn ||x||_2 = 1.

Cho một hàm mục tiêu thông thường L, chúng tôi xây dựng quá trình tối ưu hóa như:

min_W J(W) = L(W) + βL_tr(W; tr)  (2)
s.t. W ∈ R, 0 < tr < 1.

Thành phần chính quy hóa L_tr được định nghĩa là:

L_tr(W; r) = (1/n) trace((W^T M - I)^2)  (3)
s.t. M = HyperSign(Prune(W; tr));

trong đó Prune() biểu thị việc cắt tỉa không có cấu trúc dựa trên độ lớn với độ thưa tr, M là một mặt nạ, và trace() trả về vết của ma trận. Thành phần bình phương và β được áp dụng để giữ L(W) và L_tr(W; r) ở cùng quy mô.

Với học siêu cầu, HyperSign() trả về Sign() được chuẩn hóa, tức là một mặt nạ cắt tỉa M trên siêu cầu. Cụ thể hơn, m_ij ∈ {0, 1/√||m_j||_1}, trong đó ||m_j||_1 biểu thị số lượng phần tử khác không trong vectơ cột thứ j m_j của M, và ||m_j||_2 = 1 cho tất cả j = 1, ..., n. Ví dụ, tr = 0.9 chỉ ra rằng 90% của M bằng không.

Các phần tử đường chéo (vết) của W^T M trong Eq. (3) biểu thị độ tương tự cosine giữa w_j và m_j. Tối thiểu hóa L_tr (Eq. (3)) tương đương với việc đẩy w_j gần với m_j, nghĩa là làm cho một phần độ lớn của các giá trị trọng số gần 1/√||m_j||_1 trong khi phần còn lại gần số không. Điều chỉnh tr sẽ thay đổi hình dạng của phân phối trọng số (Hình 1). Thành phần chính quy hóa L_tr có thể được áp dụng cho các mô hình đã được đào tạo trước và đào tạo từ đầu. Trong thực tế, việc giảm dần tr từ một giá trị cao hơn, ví dụ 0.9, xuống giá trị mục tiêu, ví dụ 0.7, hoạt động tốt hơn so với việc sử dụng tr = 0.7 cố định. Chúng tôi so sánh các thiết lập đã đề cập trong phần thí nghiệm.

Cắt tỉa và Phục hồi

Cắt tỉa ngay lập tức Sau khi điều chỉnh phân phối trọng số thông qua Eq. (3), chúng ta có thể cắt tỉa mô hình bằng việc cắt tỉa không có cấu trúc dựa trên độ lớn Prune(), và:

W' = W_0 + Prune(W; r)  (4)

trong đó 0 < r < 1 là tỷ lệ cắt tỉa, W_0 biểu thị các giá trị trọng số đã cắt tỉa, và Prune() trả về các trọng số còn lại.

Liên quan đến học siêu cầu, cho W, việc loại bỏ các giá trị gần số không W_0 của nó hoặc thay đổi tỷ lệ độ lớn của w_j sẽ không ảnh hưởng đến hiệu suất của mô hình miễn là hướng của W vẫn không thay đổi. Điều này cho phép cắt tỉa ngay lập tức mà không cần tinh chỉnh thêm sau đó, điều này không khả thi với các phương pháp cắt tỉa truyền thống do tác động của việc thay đổi độ lớn trọng số. Tuy nhiên, dựa trên kết quả thí nghiệm của chúng tôi và các công trình khác (Lazarevich, Kozlov, và Malinin 2021), một trong những hạn chế của việc cắt tỉa mà không cần tinh chỉnh là độ chính xác vẫn giảm đáng kể ngay khi tỷ lệ cắt tỉa vượt quá 50%. Chúng tôi khám phá hai cách để giải quyết vấn đề này.

Phục hồi tức thì với giá trị trung bình Để phục hồi một mô hình đã cắt tỉa với độ thưa cao hơn, chúng tôi thay thế các trọng số đã cắt tỉa W_0 bằng một dấu hiệu thay thế α sgn(W_0) và tối thiểu hóa khoảng cách Euclidean giữa chúng cùng với một yếu tố tỷ lệ theo lớp α:

min ||W_0 - α W_0^sgn||_2^2;  (5)
s.t. W_0^sgn ∈ {-1, 1}; α > 0.

Và α có một giải pháp dạng đóng (Li, Zhang, và Liu 2016):

α = (1/||W^sgn||_1) Σ |w_0|  (6)

đây là giá trị độ lớn trung bình của các trọng số đã cắt tỉa. W_0^sgn rất quan trọng vì nó giữ thông tin hướng. Công trình của (Zhou et al. 2019) cũng hỗ trợ trực giác này bằng cách chỉ ra rằng dấu hiệu là rất quan trọng để phục hồi một mô hình đã cắt tỉa. So với việc điền số không, việc điền các trọng số đã cắt tỉa bằng α W_0^sgn cung cấp một xấp xỉ chính xác hơn về thông tin hướng của W. Đáng chú ý là phương pháp phục hồi này chỉ hoạt động tốt khi độ lệch của W_0 là nhỏ, vì độ lệch cao hơn chỉ ra sự thay đổi hướng. Do đó, tốt hơn là thay thế một phần thay vì tất cả các giá trị trọng số đã cắt tỉa. Tính chất phục hồi này liên quan đến tỷ lệ cắt tỉa r và thành phần chính quy hóa L_tr. Chúng tôi thực hiện nghiên cứu loại bỏ trên chúng trong phần thí nghiệm.

Kết quả thí nghiệm của chúng tôi tiết lộ thêm rằng tính chất phục hồi tức thì tồn tại trong cả thiết lập siêu cầu và không siêu cầu. Thành phần chính quy hóa L_tr với học siêu cầu có thể ổn định khả năng phục hồi này vì nó tạo ra nhiều giá trị trọng số gần số không hơn và ít nhạy cảm với việc thay đổi độ lớn trọng số.

Khả năng phục hồi tức thì rất hứa hẹn vì chúng ta chỉ cần lưu trữ một mặt nạ nhị phân W_0^sgn và một giá trị trung bình cho mỗi lớp để nén mô hình. Với phương pháp phục hồi tức thì được đề xuất, một mô hình siêu cầu ResNet-18 có thể được cắt tỉa lên đến độ thưa 70% với 0.3% sụt giảm độ chính xác mà không cần tinh chỉnh gì, vượt trội hơn hầu hết các công trình cắt tỉa dựa trên tinh chỉnh hiện có. Tuy nhiên, trong thực tế, chúng tôi quan sát thấy rằng phục hồi tức thì chỉ hoạt động tốt trên MobileNetV2 với độ thưa ít hơn 30%.

Phục hồi chậm với tinh chỉnh Phương pháp phục hồi tức thì chỉ có lợi cho việc nén kích thước mô hình, trong khi nó không giảm FLOP vì các giá trị không được thay thế bằng giá trị trung bình. Do đó, chúng tôi tiếp tục khám phá tác động của L_tr đối với việc cắt tỉa dựa trên tinh chỉnh. Chúng tôi tuân theo cách thông thường để tinh chỉnh các mô hình đã xử lý (Blalock et al. 2020).

Chi tiết đào tạo

Phương pháp đề xuất của chúng tôi có thể được áp dụng cho việc đào tạo từ đầu, mặc dù với nhiều nỗ lực đào tạo hơn và độ chính xác thấp hơn một chút, vì thành phần L_tr dựa vào các trọng số mô hình đã hội tụ như các phương pháp cắt tỉa truyền thống. Tất cả các mô hình trong thí nghiệm của chúng tôi được khởi tạo từ PyTorch Zoo ngoại trừ những mô hình đào tạo từ đầu trong nghiên cứu loại bỏ. Các thiết lập ban đầu được khuyến nghị cho L_tr là tr = 0.9, và 0.5 < β < 2. β ban đầu phụ thuộc vào cấu trúc mạng khác nhau. Ví dụ, β = 2 cho ResNet18/MobileNetV2, và β = 1 cho ResNet50. Quá trình tổng thể có thể được tóm tắt là: i) Tinh chỉnh với học siêu cầu (Liu et al. 2017) và L_tr từ các mô hình PyTorch Zoo đã được đào tạo trước với tr cụ thể (Eq. (3)); ii) Áp dụng phục hồi tức thì hoặc tinh chỉnh sau khi cắt tỉa không có cấu trúc.

Chúng tôi sử dụng độ chính xác hỗn hợp PyTorch với 8 GPU Nvidia A100 để đào tạo. Chúng tôi sử dụng lịch trình cosine annealing với khởi động lại (mỗi 10 epochs) (Loshchilov và Hutter 2016) để điều chỉnh tỷ lệ học. Tỷ lệ học ban đầu là 0.01 và kích thước batch là 256. Việc giảm dần tr từ 0.9 xuống 0.7 trong 90 epochs có thể đạt được kết quả tốt.

Thí nghiệm

Trong phần này, chúng tôi nghiên cứu tác động của tr đối với độ chính xác của cắt tỉa ngay lập tức, phục hồi tức thì và tinh chỉnh. Chúng tôi thực hiện nhiệm vụ phân loại hình ảnh để đánh giá phương pháp đề xuất của chúng tôi trên bộ dữ liệu ImageNet (Russakovsky et al. 2015) với các kiến trúc ResNet-18/50 (He et al. 2016) và MobileNetV2 (Sandler et al. 2018). Chúng tôi sử dụng "+" để biểu thị các mô hình đào tạo từ đầu. Các mô hình khác được khởi tạo bằng trọng số đã đào tạo trước được cung cấp bởi PyTorch zoo. "↓" biểu thị giảm dần. "tr=0" biểu thị mô hình cơ sở thu được từ PyTorch.

Thiết lập thí nghiệm

Kích thước batch là 256. Suy giảm trọng số là 0.0001, và momentum của stochastic gradient descent (SGD) là 0.9. Trong quá trình tinh chỉnh, chúng tôi sử dụng lịch trình cosine annealing với khởi động lại (Loshchilov và Hutter 2016) để điều chỉnh tỷ lệ học. Tỷ lệ học ban đầu là 0.01. Khi đào tạo từ đầu, chúng tôi tuân theo công thức từ PyTorch. Chúng tôi cắt tỉa tất cả các lớp tuyến tính và tích chập ngoại trừ lớp tích chập đầu tiên.

Đào tạo từ đầu L_tr được đề xuất của chúng tôi có thể được áp dụng trực tiếp cho việc đào tạo từ đầu. Bảng 1 cho thấy kết quả so sánh của phương pháp đã đào tạo trước và đào tạo từ đầu. Như chúng tôi đã nêu ở trên, độ chính xác cắt tỉa ngay lập tức của mô hình đào tạo từ đầu hơi kém hơn so với những mô hình đã đào tạo trước. Trong Bảng 4, chúng tôi cũng so sánh khả năng phục hồi giữa chúng. Mặc dù các mô hình đào tạo từ đầu không tốt bằng những mô hình đã đào tạo trước, chúng vẫn có thể vượt trội hơn mô hình cơ sở. Ngoài ra, khi đào tạo từ đầu, tr = 0.95 cố định hoạt động tốt hơn so với những cái giảm dần, tức là tr = 0.9↓0.7.

Cắt tỉa ngay lập tức

Cắt tỉa ngay lập tức có nghĩa là cắt tỉa trực tiếp bằng cách sử dụng phương pháp không có cấu trúc (Han, Mao, và Dally 2015). Chúng tôi so sánh các thiết lập khác nhau của độ thưa và tr để nghiên cứu tác động của chúng đối với hiệu suất. Kết quả được hiển thị trong Bảng 1, 2 và 3.

Tác động của tr và độ thưa khác nhau được liệt kê trong Bảng 1. Các mô hình đã đào tạo trước với tr giảm dần vượt trội đáng kể so với các mô hình cơ sở. Chúng tôi cũng so sánh tác động trên ResNet-50 (Bảng 2) và MobileNetV2 (Bảng 3).

[Bảng 1-3 tiếp tục với dữ liệu số...]

Phục hồi tức thì

Trong phần này, chúng tôi so sánh khả năng phục hồi tức thì. Chúng tôi điền một phần các giá trị trọng số đã cắt tỉa. "Sp1" biểu thị điểm bắt đầu và "Sp2" biểu thị độ thưa mục tiêu. Ví dụ, "Sp1=0.3" và "Sp2=0.5" có nghĩa là các giá trị trọng số đã cắt tỉa trong phạm vi cắt tỉa từ 30% đến 50% được thay thế bằng Eq. (6), tức là W_0 = Prune(W; 0.3) - Prune(W; 0.5); độ thưa là 50% và với một mặt nạ bổ sung W_0.

Chúng tôi so sánh với các mô hình cơ sở từ PyTorch, DPF (Lin et al. 2020) và DSR (Mostafa và Wang 2019). Không giống như các phương pháp không cần tinh chỉnh khác, phương pháp của chúng tôi có thể tạo ra các mô hình với độ thưa và độ chính xác khác nhau. Chúng tôi cũng quan sát thấy rằng khi độ thưa gần với tr, khả năng phục hồi tức thì giảm nhanh (Bảng 5). MobileNetV2 rất nhạy cảm với việc cắt tỉa tức thì (Bảng 6) và hiệu suất không tốt bằng các mô hình cơ sở.

[Bảng 4-7 tiếp tục với dữ liệu số...]

Tinh chỉnh

Chúng tôi so sánh kết quả của chúng tôi với cắt tỉa một lần (Han, Mao, và Dally 2015), cắt tỉa dần dần (Zhu và Gupta 2017) và cắt tỉa tuần hoàn (Srinivas et al. 2022). Độ chính xác tinh chỉnh được cải thiện xuất sắc bởi phương pháp đề xuất của chúng tôi. Ví dụ, MobileNetV2 với độ thưa 50% thậm chí còn vượt trội hơn độ chính xác của mô hình dày đặc ban đầu (Bảng 10); với độ thưa 70%, phương pháp của chúng tôi chỉ mang lại 3.54% sụt giảm độ chính xác, trong khi các phương pháp cắt tỉa truyền thống giảm 7-10% độ chính xác.

[Bảng 8-10 tiếp tục với dữ liệu số...]

Kết luận

Trong bài báo này, chúng tôi chỉ ra rằng học siêu cầu với thành phần chính quy hóa loss có thể cải thiện đáng kể hiệu suất của việc cắt tỉa mô hình. Phương pháp của chúng tôi sử dụng thành phần chính quy hóa để kiểm soát phân phối trọng số, không cần tinh chỉnh sau khi cắt tỉa nếu độ thưa ít hơn 50%. So với các phương pháp dựa trên tinh chỉnh hiện có, phương pháp của chúng tôi có thể cải thiện đáng kể độ chính xác đã tinh chỉnh. Chúng tôi cũng khám phá khả năng phục hồi của các mô hình đã cắt tỉa. Kết quả của chúng tôi cho thấy rằng các mô hình đã cắt tỉa có thể được phục hồi bằng cách thay thế các giá trị đã cắt tỉa bằng giá trị trung bình của chúng. Kết hợp với việc cắt tỉa ngay lập tức và phục hồi tức thì được đề xuất, phương pháp của chúng tôi có thể tạo ra các mô hình với độ thưa và độ chính xác khác nhau ngay lập tức.
