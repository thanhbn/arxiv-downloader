# 1812.02402.pdf
# Đã chuyển đổi từ PDF sang TXT  
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/1812.02402.pdf
# Kích thước tệp: 576259 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cắt Tỉa Thứ Hạng Được Huấn Luyện cho Mạng Nơ-ron Sâu 
Hiệu Quả
Yuhui Xu1, Yuxi Li1, Shuai Zhang2, Wei Wen3, Botao Wang2, Wenrui Dai1, Yingyong Qi2, Yiran
Chen3, Weiyao Lin1và Hongkai Xiong1
1Đại học Giao thông Thượng Hải, Email: {yuhuixu, lyxok1, daiwenrui, wylin,
xionghongkai}@sjtu.edu.cn
2Nghiên cứu AI Qualcomm, Email: {shuazhan, botaow, yingyong}@qti.qualcomm.com
3Đại học Duke, Email: {wei.wen, yiran.chen}@duke.edu
Tóm tắt
Để tăng tốc suy luận DNN, xấp xỉ thứ hạng thấp đã được áp dụng rộng rãi
vì cơ sở lý thuyết vững chắc và các triển khai hiệu quả của nó. Một số
công trình trước đây đã cố gắng trực tiếp xấp xỉ một mô hình đã được huấn luyện trước bằng
phân rã thứ hạng thấp; tuy nhiên, các lỗi xấp xỉ nhỏ trong tham số có thể lan rộng
thành một mất mát dự đoán lớn. Rõ ràng, việc tách riêng xấp xỉ thứ hạng thấp
khỏi huấn luyện là không tối ưu. Không giống như các công trình trước đây, bài báo này tích hợp xấp xỉ thứ hạng thấp
và chính quy hóa vào quá trình huấn luyện. Chúng tôi đề xuất Cắt Tỉa Thứ Hạng Được Huấn Luyện (TRP), 
luân phiên giữa xấp xỉ thứ hạng thấp và huấn
luyện. TRP duy trì khả năng của mạng gốc trong khi áp đặt các ràng buộc thứ hạng thấp
trong quá trình huấn luyện. Một chính quy hóa hạt nhân được tối ưu hóa bằng
gradient con ngẫu nhiên được sử dụng để thúc đẩy thêm thứ hạng thấp trong TRP. Các mạng được huấn luyện
với TRP có cấu trúc thứ hạng thấp theo bản chất, và được xấp xỉ với mất mát hiệu suất
không đáng kể, do đó loại bỏ việc tinh chỉnh sau xấp xỉ thứ hạng thấp. Phương pháp
đề xuất được đánh giá toàn diện trên CIFAR-10 và ImageNet, vượt trội hơn
các đối tác nén trước đây sử dụng xấp xỉ thứ hạng thấp. Mã của chúng tôi có sẵn tại: https://github.com/yuhuixu1993/Trained-Rank-Pruning.
1 Giới thiệu
Mạng Nơ-ron Sâu (DNN) đã cho thấy thành công đáng chú ý trong nhiều nhiệm vụ thị giác máy tính
như phân loại hình ảnh [8], phát hiện đối tượng [15] và phân đoạn ngữ nghĩa [3]. Mặc dù có
hiệu suất cao trong các DNN lớn được hỗ trợ bởi phần cứng tính toán song song tiên tiến, hầu hết
các kiến trúc mạng hiện đại không phù hợp cho việc sử dụng có hạn chế tài nguyên như sử dụng
trên các thiết bị luôn bật, thiết bị cấp thấp chạy bằng pin, do những hạn chế về năng lực
tính toán, bộ nhớ và điện năng.
Để giải quyết vấn đề này, các phương pháp phân rã thứ hạng thấp [6,10,7,17,1] đã được đề xuất
để giảm thiểu sự dư thừa theo kênh và không gian bằng cách phân rã mạng gốc thành một
mạng nhỏ gọn với các lớp thứ hạng thấp. Khác với các công trình trước đây, bài báo này đề xuất một phương pháp
mới để thiết kế các mạng thứ hạng thấp.
Các mạng thứ hạng thấp có thể được huấn luyện trực tiếp từ đầu. Tuy nhiên, rất khó để có được
kết quả thỏa đáng vì một số lý do. (1) Khả năng thấp: So với mạng đầy đủ thứ hạng gốc, 
khả năng của mạng thứ hạng thấp bị hạn chế, gây ra khó khăn trong việc tối ưu hóa hiệu suất của nó.
(2)Cấu trúc sâu: Phân rã thứ hạng thấp thường làm tăng gấp đôi số lượng lớp trong mạng.
Các lớp bổ sung làm cho tối ưu hóa số trở nên khó khăn hơn nhiều do gradient bùng nổ
và/hoặc biến mất. (3) Lựa chọn thứ hạng: Thứ hạng của mạng được phân rã thường được chọn như một
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.arXiv:1812.02402v3  [cs.CV]  23 Jan 2020

--- TRANG 2 ---
siêu tham số dựa trên các mạng đã được huấn luyện trước; có thể không phải là thứ hạng tối ưu cho mạng
được huấn luyện từ đầu.
Thay vào đó, một số công trình trước đây [18,7,10] đã cố gắng phân rã các mô hình đã được huấn luyện trước để
có được các mạng thứ hạng thấp ban đầu. Tuy nhiên, thứ hạng thấp được áp đặt theo kinh nghiệm có thể gây ra
mất mát độ chính xác lớn và cần huấn luyện lại mạng để khôi phục hiệu suất của mạng gốc
nhiều nhất có thể. Một số nỗ lực đã được thực hiện để sử dụng chính quy hóa thưa thớt [17,4] để ràng buộc
mạng vào không gian thứ hạng thấp. Mặc dù chính quy hóa thưa thớt giảm lỗi gây ra bởi
phân rã ở một mức độ nào đó, hiệu suất vẫn giảm nhanh chóng khi tỷ lệ nén tăng.
Trong bài báo này, chúng tôi đề xuất một phương pháp mới, được gọi là Cắt Tỉa Thứ Hạng Được Huấn Luyện (TRP), để huấn luyện các mạng thứ hạng thấp. Chúng tôi nhúng phân rã thứ hạng thấp vào quá trình huấn luyện bằng cách dần dần đẩy
phân phối trọng số của một mạng hoạt động tốt vào dạng thứ hạng thấp, trong đó tất cả các tham số của
mạng gốc được giữ và tối ưu hóa để duy trì khả năng của nó. Chúng tôi cũng đề xuất một
chính quy hóa hạt nhân được tối ưu hóa bằng gradient con ngẫu nhiên để ràng buộc thêm các trọng số trong không gian thứ hạng thấp nhằm tăng cường TRP. Giải pháp đề xuất được minh họa trong Hình 1.
Nhìn chung, các đóng góp của chúng tôi được tóm tắt dưới đây.
1.Một phương pháp huấn luyện mới được gọi là TRP được trình bày bằng cách nhúng rõ ràng
phân rã thứ hạng thấp vào quá trình huấn luyện mạng;
2.Một chính quy hóa hạt nhân được tối ưu hóa bằng gradient con ngẫu nhiên để tăng cường hiệu suất
của TRP;
3.Cải thiện tăng tốc suy luận và giảm mất mát độ chính xác xấp xỉ trong cả phương pháp phân rã
theo kênh và theo không gian.
𝑘𝑤𝑘ℎ𝐶𝑘𝑤𝑘ℎ𝐶
𝑘𝑤𝑘ℎ𝐶Luồng đặc trưng
Luồng gradient
Xấp xỉ thứ hạng thấp
Thay thế
……
…𝑊𝑡
𝑇𝑧
𝑊𝑡
（a） （b）
Hình 1: Quá trình huấn luyện TRP bao gồm hai phần như được minh họa trong (a) và (b). (a) Một lần lặp bình thường với
truyền tiến-lùi và cập nhật trọng số. (b) Một lần lặp huấn luyện được chèn bởi cắt tỉa thứ hạng, trong đó
xấp xỉ thứ hạng thấp được áp dụng trước trên các bộ lọc hiện tại trước khi tích chập. Trong quá trình lan truyền ngược, các
gradient được cộng trực tiếp trên các bộ lọc thứ hạng thấp và các trọng số gốc được thay thế bằng các bộ lọc thứ hạng thấp
đã cập nhật. (b) được áp dụng một lần mỗi mlần lặp (tức là khi lần lặp cập nhật gradient t=zm; z = 0;1;2;  ),
ngược lại (a) được áp dụng.
2 Phương pháp
2.1 Kiến thức cơ bản
Chính thức, các bộ lọc tích chập trong một lớp có thể được ký hiệu bằng tensor W2Rnckwkh, trong đó n
và c là số lượng bộ lọc và kênh đầu vào, kh và kw là chiều cao và chiều rộng của các bộ lọc.
Một đầu vào của lớp tích chập Fi2Rcxy tạo ra một đầu ra là Fo=WFi. Tương quan
theo kênh [18] và tương quan theo không gian [10] được khám phá để xấp xỉ các bộ lọc tích chập trong
không gian thứ hạng thấp. Trong bài báo này, chúng tôi tập trung vào hai sơ đồ phân rã này. Tuy nhiên, không giống như
các công trình trước đây, chúng tôi đề xuất một sơ đồ huấn luyện mới TRP để có được mạng thứ hạng thấp mà không cần
huấn luyện lại sau phân rã.
2

--- TRANG 3 ---
2.2 Cắt Tỉa Thứ Hạng Được Huấn Luyện
Chúng tôi đề xuất một sơ đồ huấn luyện đơn giản nhưng hiệu quả được gọi là Cắt Tỉa Thứ Hạng Được Huấn Luyện (TRP) theo
cách tuần hoàn:
Wt+1=
WtOf(Wt)t%m6= 0
TzOf(Tz)t%m= 0
Tz=D(Wt); z =t=m(1)
trong đó D() là toán tử xấp xỉ tensor thứ hạng thấp, là tốc độ học, t chỉ mục lần lặp
và z là lần lặp của toán tử D, với m là chu kỳ cho xấp xỉ thứ hạng thấp.
Chúng tôi áp dụng xấp xỉ thứ hạng thấp mỗi m lần lặp SGD. Điều này tiết kiệm thời gian huấn luyện rất nhiều.
Như được minh họa trong Hình 1, mỗi m lần lặp, chúng tôi thực hiện xấp xỉ thứ hạng thấp trên các
bộ lọc gốc, trong khi các gradient được cập nhật trên dạng thứ hạng thấp kết quả. Ngược lại, mạng
được cập nhật thông qua SGD bình thường. Sơ đồ huấn luyện của chúng tôi có thể được kết hợp với các toán tử thứ hạng thấp
tùy ý. Trong công trình đề xuất, chúng tôi chọn các kỹ thuật thứ hạng thấp được đề xuất trong [10] và [18], cả hai
đều biến đổi các bộ lọc 4 chiều thành ma trận 2D và sau đó áp dụng phân rã giá trị đơn cắt ngắn
(TSVD). SVD của ma trận Wt có thể được viết như:
Wt=rank (Wt)X
i=1iUi(Vi)T; (2)
trong đó i là giá trị đơn của Wt với 12rank (Wt), và Ui và Vi là các
vector đơn. TSVD được tham số hóa ( Wt;e) là để tìm số nguyên nhỏ nhất k sao cho
rank (Wt)X
j=k+1(j)2erank (Wt)X
i=1(i)2; (3)
trong đó e2(0;1) là siêu tham số được định trước của tỷ lệ bảo toàn năng lượng. Sau khi cắt ngắn
n-k giá trị đơn cuối cùng, chúng tôi biến đổi ma trận 2D thứ hạng thấp trở lại tensor 4D.
2.3 Chính quy hóa Chuẩn Hạt nhân
Chuẩn hạt nhân được sử dụng rộng rãi trong các bài toán hoàn thành ma trận. Gần đây, nó được giới thiệu để ràng buộc
mạng vào không gian thứ hạng thấp trong quá trình huấn luyện [1].
min(
f(x;w) +LX
l=1jjWljj)
(4)
trong đó f() là hàm mất mát mục tiêu, chuẩn hạt nhân jjWljj được định nghĩa là jjWljj=Prank (Wl)
i=1i
l,
với i
l là các giá trị đơn của Wl. là siêu tham số thiết lập ảnh hưởng của chuẩn hạt nhân.
Trong bài báo này, chúng tôi sử dụng gradient con ngẫu nhiên [2] để tối ưu hóa chính quy hóa chuẩn hạt nhân
trong quá trình huấn luyện. Gọi W=UVT là SVD của W và gọi Utru;Vtru là U;V được cắt ngắn
thành rank (W) cột hoặc hàng đầu tiên, thì UtruVT
tru là gradient con của jjWjj[16]. Do đó,
gradient con của Eq. (4) trong một lớp là
Of+UtruVT
tru: (5)
Chuẩn hạt nhân và hàm mất mát được tối ưu hóa đồng thời trong quá trình huấn luyện của các mạng
và có thể được kết hợp thêm với TRP đề xuất.
3 Thí nghiệm
3.1 Chi tiết Triển khai
Chúng tôi đánh giá hiệu suất của sơ đồ TRP trên hai bộ dữ liệu phổ biến, CIFAR-10 [11] và ImageNet
[5]. Chúng tôi triển khai sơ đồ TRP với GPU NVIDIA 1080 Ti. Để huấn luyện trên CIFAR-10, chúng tôi
3

--- TRANG 4 ---
Mô hình ("R-" biểu thị ResNet-.) Top 1 ( %)Tăng tốc
R-20 (cơ sở) 91.74 1.00
R-20 (TRP1) 90.12 1.97
R-20 (TRP1+Nu) 90.50 2.17
R-20 ([18]) 88.13 1.41
R-20 (TRP2) 90.13 2.66
R-20 (TRP2+Nu) 90.62 2.84
R-20 ([10]) 89.49 1.66
R-56 (cơ sở) 93.14 1.00
R-56 (TRP1) 92.77 2.31
R-56 (TRP1+Nu) 91.85 4.48
R-56 ([18]) 91.56 2.10
R-56 (TRP2) 92.63 2.43
R-56 (TRP2+Nu) 91.62 4.51
R-56 ([10]) 91.59 2.10
R-56 [9] 91.80 2.00
R-56 [12] 91.60 2.00
Bảng (2) Kết quả thí nghiệm trên CIFAR-10.Phương pháp Top1( %)Tăng tốc
Cơ sở 69.10 1.00
TRP1 65.46 1.81
TRP1+Nu 65.39 2.23
[18] 63.1 1.41
TRP2 65.51 2:60
TRP2+Nu 65.34 3.18
[10] 62.80 2.00
Bảng (3) Kết quả của ResNet-18 trên ImageNet.
Phương pháp Top1( %)Tăng tốc
Cơ sở 75.90 1.00
TRP1+Nu 72.69 2.30
TRP1+Nu 74.06 1.80
[18] 71.80 1.50
[13] 72.04 1.58
[14] 72.03 2.26
Bảng (4) Kết quả của ResNet-50 trên ImageNet.
bắt đầu với tốc độ học cơ sở 0:1 để huấn luyện 164 epoch và giảm giá trị đi 10 lần tại
epoch thứ 82 và thứ 122. Đối với ImageNet, chúng tôi trực tiếp tinh chỉnh mô hình với sơ đồ TRP từ
cơ sở đã được huấn luyện trước với tốc độ học 0:0001 trong 10 epoch. Đối với cả hai bộ dữ liệu, chúng tôi áp dụng
bộ giải SGD để cập nhật trọng số và đặt giá trị suy giảm trọng số là 104 và giá trị momentum là 0:9.
3.2 Kết quả trên CIFAR-10
Như được hiển thị trong Bảng 2, đối với cả phân rã theo không gian (TRP1) và theo kênh (TRP2), TRP
đề xuất vượt trội hơn các phương pháp cơ bản [18,10] trên ResNet-20 và ResNet-56. Kết quả trở nên
tốt hơn khi sử dụng chính quy hóa hạt nhân. Ví dụ, trong phân rã theo kênh
(TRP2) của ResNet-56, kết quả của TRP kết hợp với chính quy hóa hạt nhân thậm chí có thể đạt được tốc độ
tăng tốc gấp 2 lần so với [18] với cùng mức giảm độ chính xác. Phương pháp của chúng tôi cũng vượt trội hơn việc cắt tỉa bộ lọc [12]
và cắt tỉa kênh [9]. Ví dụ, TRP phân rã kênh được huấn luyện ResNet-56 có thể đạt được
độ chính xác 92:77% với tăng tốc 2:31, trong khi [9] là 91:80% và [12] là 91:60%. Với sự trợ giúp của
chính quy hóa hạt nhân, các phương pháp của chúng tôi có thể đạt được tốc độ tăng tốc gấp 2 lần so với [9] và [12] với
độ chính xác cao hơn.
3.3 Kết quả trên ImageNet
Kết quả trên ImageNet được hiển thị trong Bảng 3 và Bảng 4. Đối với ResNet-18, phương pháp của chúng tôi vượt trội hơn
các phương pháp cơ bản [18,10]. Ví dụ, trong phân rã theo kênh, TRP đạt được tốc độ
tăng tốc 1.81 với độ chính xác Top5 86.48% trên ImageNet, vượt trội hơn cả phương pháp dựa trên dữ liệu [18]1
và phương pháp độc lập dữ liệu [18] với biên độ lớn. Chính quy hóa hạt nhân có thể tăng tốc độ
tăng tốc với cùng độ chính xác.
Đối với ResNet-50, để xác thực tốt hơn hiệu quả của phương pháp, chúng tôi cũng so sánh TRP
đề xuất với [9] và [13]. Với tăng tốc 1:80, ResNet-50 được phân rã của chúng tôi có thể đạt được Top1 73:97% và
độ chính xác Top5 91:98% cao hơn nhiều so với [13]. TRP đạt được tăng tốc 2:23 cao hơn
[9] với cùng mức giảm Top5.
4 Kết luận
Trong bài báo này, chúng tôi đề xuất một sơ đồ mới Cắt Tỉa Thứ Hạng Được Huấn Luyện (TRP) để huấn luyện các mạng thứ hạng thấp.
Nó tận dụng khả năng và cấu trúc của mạng gốc bằng cách nhúng xấp xỉ thứ hạng thấp
vào quá trình huấn luyện. Hơn nữa, chúng tôi đề xuất chính quy hóa chuẩn hạt nhân được tối ưu hóa bằng gradient con ngẫu nhiên
để tăng cường TRP. TRP đề xuất có thể được kết hợp với bất kỳ phương pháp phân rã thứ hạng thấp nào.
Trên các bộ dữ liệu CIFAR-10 và ImageNet, chúng tôi đã chỉ ra rằng các phương pháp của chúng tôi có thể
vượt trội hơn các phương pháp cơ bản trong cả phân rã theo kênh và phân rã theo không gian.
1triển khai của [7]
4

--- TRANG 5 ---
Tài liệu tham khảo
[1]J. M. Alvarez và M. Salzmann. Huấn luyện nhận thức nén của mạng sâu. Trong NIPS , 2017.
[2]H. Avron, S. Kale, S. P. Kasiviswanathan, và V . Sindhwani. Giảm gradient con ngẫu nhiên
hiệu quả và thực tế cho chính quy hóa chuẩn hạt nhân. Trong ICML , 2012.
[3]L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, và A. L. Yuille. Deeplab: Phân đoạn
hình ảnh ngữ nghĩa với mạng tích chập sâu, tích chập atrous, và crf được kết nối đầy đủ.
TPAMI , 40:834–848, 2018.
[4]W. Chen, J. Wilson, S. Tyree, K. Weinberger, và Y . Chen. Nén mạng nơ-ron với
thủ thuật băm. Trong ICML , 2015.
[5]J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, và L. Fei-Fei. Imagenet: Cơ sở dữ liệu hình ảnh
phân cấp quy mô lớn. CVPR , 2009.
[6]E. Denton, W. Zaremba, J. Bruna, Y . Lecun, và R. Fergus. Khai thác cấu trúc tuyến tính trong
mạng tích chập để đánh giá hiệu quả. Trong NIPS , 2014.
[7]J. Guo, Y . Li, W. Lin, Y . Chen, và J. Li. Tách rời mạng: Từ tích chập thông thường đến
tích chập tách theo chiều sâu. Trong BMVC , 2018.
[8] K. He, X. Zhang, S. Ren, và J. Sun. Học tàn dư sâu để nhận dạng hình ảnh. 2016.
[9]Y . He, X. Zhang, và J. Sun. Cắt tỉa kênh để tăng tốc mạng nơ-ron rất sâu. Trong
ICCV , 2017.
[10] M. Jaderberg, A. Vedaldi, và A. Zisserman. Tăng tốc mạng nơ-ron tích chập với
mở rộng thứ hạng thấp. arXiv preprint arXiv:1405.3866 , 2014.
[11] A. Krizhevsky và G. Hinton. Học nhiều lớp đặc trưng từ hình ảnh nhỏ. Khoa học Máy tính , 2009.
[12] H. Li, A. Kadav, I. Durdanovic, H. Samet, và H. P. Graf. Cắt tỉa bộ lọc cho convnet
hiệu quả. arXiv preprint arXiv:1608.08710 , 2016.
[13] J.-H. Luo, J. Wu, và W. Lin. Thinet: Một phương pháp cắt tỉa cấp bộ lọc để nén mạng nơ-ron sâu.
ICCV , 2017.
[14] J.-H. Luo, H. Zhang, H.-Y . Zhou, C.-W. Xie, J. Wu, và W. Lin. Thinet: cắt tỉa bộ lọc cnn cho
mạng mỏng hơn. TPAMI , 2018.
[15] S. Ren, K. He, R. B. Girshick, và J. Sun. Faster r-cnn: Hướng tới phát hiện đối tượng thời gian thực với
mạng đề xuất vùng. TPAMI , 39:1137–1149, 2015.
[16] G. A. Watson. Đặc tính của vi phân con của một số chuẩn ma trận. Đại số tuyến tính
và các ứng dụng của nó , 170:33–45, 1992.
[17] W. Wen, C. Xu, C. Wu, Y . Wang, Y . Chen, và H. Li. Phối hợp bộ lọc cho mạng nơ-ron sâu
nhanh hơn. Trong ICCV , 2017.
[18] X. Zhang, J. Zou, K. He, và J. Sun. Tăng tốc mạng tích chập rất sâu cho
phân loại và phát hiện. TPAMI , 38(10):1943–1955, 2016.
5