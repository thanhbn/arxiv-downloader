# 2203.07259.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2203.07259.pdf
# Kích thước tệp: 634868 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Bác Sĩ Phẫu Thuật BERT Tối Ưu: Tỉa Cắt Bậc Hai Có Thể Mở Rộng và Chính Xác cho Các Mô Hình Ngôn Ngữ Lớn
Eldar Kurtic1, Daniel Campos2,3, Tuan Nguyen2, Elias Frantar1, Mark Kurtz2,
Benjamin Fineran2, Michael Goin2, và Dan Alistarh1,2
1Viện Khoa học và Công nghệ Áo
2Neural Magic Inc.
3Khoa Khoa học Máy tính, Đại học Illinois Urbana-Champaign
Tóm tắt
Trong bài báo này, chúng tôi xem xét vấn đề
làm thưa thớt các mô hình BERT, đây là khối
xây dựng chính cho xử lý ngôn ngữ tự nhiên,
nhằm giảm chi phí lưu trữ và tính toán của
chúng. Chúng tôi giới thiệu Bác Sĩ Phẫu Thuật
BERT Tối Ưu (oBERT), một phương pháp tỉa
cắt hiệu quả và chính xác dựa trên thông tin
bậc hai xấp xỉ, mà chúng tôi chỉ ra mang lại
kết quả tối tân cho nén trong cả hai giai đoạn
của nhiệm vụ ngôn ngữ: tiền huấn luyện và
tinh chỉnh. Cụ thể, oBERT mở rộng công trình
hiện có về tỉa cắt bậc hai bằng cách cho phép
tỉa cắt khối trọng số, và là phương pháp đầu
tiên như vậy áp dụng được ở quy mô BERT.
Thứ hai, chúng tôi khảo sát các phương pháp
nén phức hợp để thu được các mô hình rất
nén nhưng chính xác để triển khai trên thiết
bị biên. Những mô hình này đẩy mạnh đáng
kể ranh giới của các mô hình BERT thưa thớt
tối tân hiện tại về tất cả các thước đo: kích
thước mô hình, tốc độ suy luận và độ chính
xác nhiệm vụ. Ví dụ, so với BERT BASE dày
đặc, chúng tôi thu được nén kích thước mô
hình 10x với giảm độ chính xác < 1%, tăng
tốc suy luận CPU 10x với giảm độ chính xác
< 2%, và tăng tốc suy luận CPU 29x với giảm
độ chính xác < 7.5%. Mã của chúng tôi, tích
hợp đầy đủ với Transformers và SparseML,
có tại https://github.com/neuralmagic/
sparseml/tree/main/research/optimal_
BERT_surgeon_oBERT .
1 Giới thiệu
Các mô hình Transformer được tiền huấn luyện
(Vaswani et al., 2017; Devlin et al., 2019) cung cấp
biểu diễn ngôn ngữ mạnh mẽ có thể được chuyên
môn hóa cho các nhiệm vụ khác nhau. Do sự tăng
trường khổng lồ của chúng (Radford et al., 2019;
Smith et al., 2022), các kỹ thuật giảm chi phí tính
toán đã trở nên phổ biến. Một kỹ thuật cổ điển là
Tác giả liên hệ: eldar.kurtic@ist.ac.at.
60 70 80 90 97
Độ thưa thớt (%)70.072.575.077.580.082.585.087.590.0Điểm F1
BERTBASE
Vé Số Trúng Thưởng
BERT Thưa Thớt
Tỉa Cắt Chuyển Động
oBERT (của chúng tôi)Hình 1: Tổng quan hiệu suất so với các phương pháp
tỉa cắt hạ nguồn không có cấu trúc tối tân Chen et al.
(2020), Xu et al. (2021), Sanh et al. (2020), theo thứ
tự này, của mô hình BERT BASE trên nhiệm vụ
SQuADv1.1.
Chưng Cất Tri Thức (KD) (Hinton et al., 2015), chuyển
giao tri thức từ một mô hình giáo viên lớn hơn sang
một mô hình học sinh nhỏ hơn. Công trình khác đã
tận dụng biểu diễn độ chính xác thấp hơn để sản
xuất các mô hình lượng tử hóa. Một phương pháp
trực giao, đây là trọng tâm chính của chúng tôi, đã
áp dụng tỉa cắt không có cấu trúc và khối, tức loại
bỏ từng trọng số, để sản xuất các mô hình ngôn ngữ
nén nhưng chính xác.Hình 1 cung cấp tổng quan so
sánh về kết quả tối tân cho tỉa cắt không có cấu trúc.
Trong bài báo này, chúng tôi giới thiệu một phương
pháp cải thiện tỉa cắt không có cấu trúc và bán có
cấu trúc (khối), bằng cách tận dụng phương pháp
bậc hai được khởi xướng bởi khung Bác Sĩ Phẫu
Thuật Não Tối Ưu (LeCun et al., 1989; Hassibi and
Stork, 1992), mà chúng tôi mở rộng lần đầu tiên cho
LLMs. Hơn nữa, chúng tôi đặt kết quả của mình
trong bối cảnh của phương pháp nén phức hợp, kết
hợp nhiều kỹ thuật nén để thu được các mô hình
thưa thớt mà chúng tôi thực thi trên runtime dựa
trên CPU nhận biết độ thưa thớt (NeuralMagic,
2021), cho thấy tăng tốc bậc độ lớn với mất mát
độ chính xác thấp.
Tóm lại, các đóng góp của chúng tôi như sau:
•Chúng tôi thực hiện khảo sát kỹ lưỡng vềarXiv:2203.07259v3  [cs.CL]  17 Oct 2022

--- TRANG 2 ---
các phương pháp tỉa cắt trọng số áp dụng cho
LLMs, bao gồm vé số trúng thưởng, tỉa cắt
chuyển động, tỉa cắt độ lớn và bậc hai.
•Chúng tôi giới thiệu một phương pháp tỉa
cắt bậc hai tổng quát gọi là Bác Sĩ Phẫu Thuật
BERT Tối Ưu (oBERT), hỗ trợ tỉa cắt không
có cấu trúc và khối, và là phương pháp bậc
hai đầu tiên vừa có độ chính xác cao vừa có
thể mở rộng tới chiều của các mô hình BERT.
•Chúng tôi minh họa lợi ích của oBERT bằng
cách cải thiện đáng kể so với các phương
pháp tỉa cắt tối tân hiện có, trong cả hai giai
đoạn của nhiệm vụ ngôn ngữ: tiền huấn luyện
và tinh chỉnh. Để minh họa, khi tỉa cắt
BERT BASE, oBERT vượt trội hơn Tỉa Cắt
Chuyển Động (MvP), phương pháp chính xác
nhất trước đây, hơn 2% điểm F1 tuyệt đối
ở cùng độ thưa thớt, và có thể đạt độ chính
xác của các mô hình MvP với ít tham số hơn
3 lần.
•Chúng tôi khảo sát khả năng áp dụng của
phương pháp tỉa cắt này trong khung phối
hợp các phương pháp nén phổ biến cho
LLMs, tức áp dụng tỉa cắt kết hợp với thả
lớp và/hoặc lượng tử hóa. Trong bối cảnh
này, chúng tôi chỉ ra rằng các mô hình thưa
thớt kết quả cung cấp cải thiện bậc độ lớn
so với các mô hình nén phức hợp khác, và
chúng có thể dễ dàng triển khai cho suy luận
CPU.
2 Nền tảng và Công trình Liên quan
LLMs Transformer thường được xây dựng sử dụng
nhiều lớp transformer với tự chú ý (Vaswani et al.,
2017). Mỗi transformer có biến thể của hai thành
phần con: chú ý đa đầu (MHA) và mạng nơ-ron
kết nối đầy đủ (FFN). Do kích thước khổng lồ của
các mô hình hoạt động tốt, đã có sự quan tâm tăng
trưởng về nén LLM. Chúng đã được chỉ ra là dễ
vỡ vì nhiễu nhỏ có thể dẫn đến sụp đổ mô hình
(Kovaleva et al., 2021). Các sơ đồ tỉa cắt được thúc
đẩy bởi các thước đo tầm quan trọng trọng số biểu
diễn mất mát độ chính xác do tỉa cắt. Thông thường
tỉa cắt theo các bước lặp lại, mỗi bước loại bỏ
trọng số cho đến khi đạt mức độ thưa thớt mong
muốn. Bây giờ, chúng tôi tóm tắt ngắn gọn các
phương pháp hiện có.
Tỉa cắt có cấu trúc cho LLMs tập trung vào giảm
số lượng lớp và/hoặc đầu chú ý, và yêu cầu hiểu
biết cấu trúc của mô hình. Michel et al. (2019) và
Voita et al. (2019) đã chứng minh rằng đối với một
số nhiệm vụ, gần 40% đầu chú ý có thể bị loại bỏ
mà không ảnh hưởng lớn đến độ chính xác. Công
trình khác đã tập trung vào loại bỏ lớp (Sridhar and
Sarah, 2020), và thứ tự loại bỏ chúng (Sajjad et al.,
2020). Trong một số thí nghiệm, chúng tôi áp dụng
thả lớp "trực tiếp" tiêu chuẩn kết hợp với tỉa cắt.
Tỉa cắt bán có cấu trúc là phương pháp trung gian,
bằng cách đó các nhóm nhỏ hơn, ví dụ tập hợp
trọng số hình chữ nhật (Lagunas et al., 2021), được
đặt về không. Phương pháp này gần đây đã trở nên
phổ biến nhờ hỗ trợ tính toán hiệu quả. Chúng tôi
mở rộng công thức tỉa cắt bậc hai cho các nhóm
như vậy, và chỉ ra kết quả cho một nhóm cụ thể
được hỗ trợ bởi công cụ suy luận CPU.
Tỉa cắt không có cấu trúc loại bỏ từng trọng số
bằng cách đặt chúng về không. Tỉa Cắt Độ Lớn
Dần Dần (GMP) là phương pháp cổ điển, sử dụng
độ lớn trọng số làm thước đo tầm quan trọng cho
tỉa cắt (Han et al., 2015; Gale et al., 2019).
Các phương pháp tỉa cắt bậc nhất sử dụng công
thức dựa trên gradient của thước đo tầm quan trọng.
Một phương pháp phổ biến là Tỉa Cắt Chuyển Động
(MvP) (Sanh et al., 2020), được thiết kế đặc biệt
cho tỉa cắt trong giai đoạn tinh chỉnh. Trực quan,
nó loại bỏ các trọng số đang chuyển về không. Các
mô hình kết quả là những mô hình đầu tiên đạt độ
thưa thớt cao với mất mát độ chính xác có thể chấp
nhận. Các phương pháp như PLATON (Zhang et
al., 2022) cố gắng nắm bắt sự không chắc chắn
của điểm tầm quan trọng trọng số bằng ước lượng
cận trên tin cậy. Trước công trình của chúng tôi,
các phương pháp MvP và PLATON đặt kết quả
tối tân cho tỉa cắt không có cấu trúc.
Các phương pháp tỉa cắt bậc hai (LeCun et al.,
1989; Hassibi and Stork, 1992; Singh and Alistarh,
2020; Frantar et al., 2021) được phát triển trong
bối cảnh phân loại hình ảnh, và tận dụng xấp xỉ
phức tạp của độ cong mất mát. Tuy nhiên, các
phương pháp tỉa cắt bậc hai yêu cầu xấp xỉ nghịch
đảo Hessian, đắt đỏ để lưu trữ và tính toán với số
lượng tham số LLM. Phương pháp chúng tôi đề
xuất tương tự như các phương pháp WoodFisher/M-
FAC (Singh and Alistarh, 2020; Frantar et al.,
2021), nhưng là phương pháp đầu tiên hoạt động
chính xác ở quy mô LLM. Cụ thể, phương pháp
WoodFisher không khả thi ở quy mô BERT, vì nó
yêu cầu lưu trữ gradient cho tính toán Fisher nghịch
đảo trong bộ nhớ tại thời điểm tỉa cắt. Phương
pháp M-FAC có thể mở rộng, nhưng chúng tôi chỉ
ra rằng tham số hóa của nó mang lại kết quả tỉa
cắt tệ hơn (Hình 3 Phụ lục). Điều này là do M-
FAC thực hiện nghịch đảo ma trận đầy đủ (không
khối) theo mặc định, vốn có nhiễu. Ngoài ra, chúng
tôi mở rộng phương pháp OBS lý thuyết cho nén
bán có cấu trúc (khối). Chúng tôi cũng chỉ ra rằng
phương pháp của chúng tôi có thể được áp dụng
trong quá trình tiền huấn luyện và tinh chỉnh LLM,
mang lại kết quả tối tân trong cả hai chế độ.
Chưng Cất Tri Thức (Hinton et al., 2015) huấn
luyện một mô hình học sinh nhỏ hơn dựa trên đầu
ra của mô hình giáo viên lớn hơn bằng cách thêm
thành phần mất mát tối thiểu hóa phân kỳ KL giữa
hai phân phối đầu ra, đây là phương pháp chúng
tôi cũng áp dụng trong thiết lập của mình. Tham
số độ cứng được sử dụng để kiểm soát hỗn hợp
mất mát thường xuyên và chưng cất, và tham số
nhiệt độ để kiểm soát độ mềm của phân phối. Trái
ngược với điều này, các phương pháp như DistilBERT
(Sanh et al., 2019), TinyBERT (Jiao et al., 2020),
MobileBERT (Sun et al., 2020a), và MiniLM (Wang
et al., 2020) sử dụng các sơ đồ chưng cất phức tạp
hơn, dựa trên chuyển giao tri thức từ biểu diễn
trung gian của mô hình. Các mô hình thưa thớt của
chúng tôi cung cấp cải thiện bậc độ lớn so với một
số phương pháp này.
Lượng tử hóa biểu diễn trọng số và kích hoạt ở
độ chính xác thấp hơn (Courbariaux et al., 2016),
và được sử dụng để thu được các mô hình như
Q8BERT (Zafrir et al., 2019) và TernaryBERT
(Zhang et al., 2020). Shen et al. (2020) sử dụng
thông tin về phổ Hessian để chọn độ rộng bit lượng
tử hóa, trong khi Yu et al. (2022) sử dụng xấp xỉ
vết Hessian cho tỉa cắt có cấu trúc. Các phương
pháp dựa trên Hessian này khác với phương pháp
chúng tôi đề xuất, vì chúng tôi sử dụng các xấp xỉ
nghịch đảo Hessian hoàn toàn khác để hướng dẫn
quyết định tỉa cắt. Trọng tâm công trình của chúng
tôi là tỉa cắt trọng số, và tăng tốc tính toán có thể
đạt được trên CPU thông thường. Do đó, các phương
pháp chúng tôi khảo sát trực giao với lượng tử hóa.
Hơn nữa, không thể so sánh trực tiếp với các mô
hình lượng tử hóa độ rộng bit thấp vì hầu hết khung
suy luận không hỗ trợ các định dạng tùy chỉnh
như vậy. Do đó, chúng tôi sẽ chỉ sử dụng Huấn
Luyện Nhận Biết Lượng Tử Hóa (QAT) tiêu chuẩn
cho trọng số 8-bit, được hỗ trợ tốt trên CPU Intel,
và trình bày tăng tốc kết quả kết hợp với thả lớp
và tỉa cắt trọng số.
Các phương pháp nén hạ nguồn cố gắng nén trực
tiếp trong khi tinh chỉnh trên một nhiệm vụ cụ thể.
Phương pháp MvP được thiết kế đặc biệt cho thiết
lập này. Các phương pháp nén thượng nguồn nén
trong giai đoạn tiền huấn luyện, giảm nhu cầu tỉa
cắt theo nhiệm vụ cụ thể. Chen et al. (2020) đã
kiểm tra các chiến lược "Vé Số Trúng Thưởng"
(Frankle and Carbin, 2018) mà, như chúng tôi minh
họa sau, gây mất mát độ chính xác khổng lồ ngay
cả ở độ thưa thớt vừa phải. Công trình gần đây
"Tỉa Cắt Một Lần Cho Tất Cả" (Prune OFA) của
Zafrir et al. (2021) chỉ ra rằng tỉa cắt độ lớn được
điều chỉnh tốt có thể cạnh tranh với các phương
pháp hạ nguồn như MvP.
Chúng tôi đầu tiên kiểm tra hiệu suất của các
phương pháp tỉa cắt trước đây, đặc biệt là MvP,
Prune OFA, và Vé Số Trúng Thưởng, so với phương
pháp oBERT bậc hai mới. Phương pháp chúng tôi
đề xuất liên tục cải thiện so với tất cả các phương
pháp trước đây này, cả trong giai đoạn tiền huấn
luyện (thượng nguồn) và tinh chỉnh (hạ nguồn),
và có thể được phối hợp với các kỹ thuật nén khác
để thu được các mô hình nhỏ hơn, nhanh hơn và
chính xác hơn các mô hình như DistilBERT, TinyBERT,
và MvP khối.
Các phương pháp bổ sung cho suy luận hiệu quả
của LLMs tồn tại, như tỉa cắt token và thoát sớm.
Các phương pháp này trực giao với phương pháp
của chúng tôi; do đó chúng tôi thảo luận chúng
trong Phụ lục A.2.
3 Bác Sĩ Phẫu Thuật BERT Tối Ưu (oBERT)
3.1 Tỉa Cắt Khối Bậc Hai Tổng Quát
Vấn đề tỉa cắt bắt đầu từ một mô hình dày đặc được
tối ưu hóa tốt w ∈ Rd, và nhằm tìm phiên bản
thưa thớt của w, trong đó nhiều trọng số được đặt
về không, và các trọng số còn lại có thể được cập
nhật tương ứng để bảo toàn mất mát. Thông thường
quá trình này xảy ra dần dần, tức bằng cách loại
bỏ trọng số một cách tiến bộ. Một phương pháp
cổ điển (LeCun et al., 1989; Hassibi and Stork,
1992) cho tỉa cắt "tối ưu" trọng số từ w tại một
bước là mở rộng hàm mất mát L cục bộ xung quanh
w với w liên quan đến mặt nạ trọng số thưa thớt
0/1 M. Nếu chúng ta ký hiệu bằng wM = ⊙(M ⊙ w),
mô hình kết quả từ tích Hadamard (theo phần tử)
giữa M ∈ {0,1}d và w, chúng ta có thể sử dụng
khai triển Taylor tại wM để thu được:
L(wM) ≈ L(w) + (wM − w)⊤∇L(w)
+ 1/2(wM − w)⊤HL(w)(wM − w).
Cho rằng w được tối ưu hóa tốt, hợp lý trong thực
tế để giả định rằng ∇L(w) ≈ 0. Khi đó,
3

--- TRANG 3 ---
thay đổi mất mát gây ra bởi tỉa cắt một tập con
trọng số có thể được biểu diễn là
ΔL(w) ≈ 1/2δw⊤HL(w)δw (1)
trong đó ΔL(w) := L(wM) − L(w) và δw :=
wM − w. Một cách phổ biến xấp xỉ Hessian tại w
là thông qua ma trận thông tin Fisher thực nghiệm
có giảm chấn (Hassibi and Stork, 1992):
HL(w) ≈ F̂(w) = λId + 1/m ∑ᵢ₌₁ᵐ ∇Li(w)∇Li⊤(w);
(2)
trong đó λ > 0 là hằng số giảm chấn nhỏ,
Id ∈ Rd×d ma trận đơn vị và m là số lượng tích
ngoài gradient được sử dụng để xấp xỉ Hessian.
Do tính định dương của (2), dạng bậc hai (1) luôn
không âm, đó là lý do chúng tôi sẽ gọi ΔL(w) là
tăng mất mát gây ra bởi tỉa cắt.
Quay lại vấn đề tỉa cắt của chúng tôi, giả sử chúng
ta muốn xác định khối trọng số Q có hình dạng
cho trước mà việc loại bỏ bằng mặt nạ không sẽ
gây ra tăng mất mát tối thiểu. Điều này dẫn đến
bài toán tối ưu hóa có ràng buộc sau:
min δw 1/2δw⊤F̂(w)δw
s.t. eₖ⊤δw + wₖ = 0, ∀k ∈ Q (3)
trong đó eₖ ∈ Rd là vector cơ sở chính tắc thứ k.
Ở đây, chúng tôi sẽ cung cấp nghiệm tổng quát,
áp dụng cho Q tổng quát. Đầu tiên, để tiện lợi,
chúng tôi biểu diễn hệ thống |Q| ràng buộc đẳng
thức dưới dạng phương trình ma trận là EQδw +
EQw = 0; trong đó EQ ∈ R|Q|×d là ma trận gồm
các vector cơ sở chính tắc tương ứng eₖ (∀k ∈ Q)
được sắp xếp theo hàng. Bài toán tối ưu hóa này
có thể được giải bằng phương pháp nhân tử Lagrange.
Cụ thể, chúng ta muốn tìm điểm dừng của Lagrangian
L(δw, λ), trong đó λ ∈ R|Q| ký hiệu vector nhân
tử Lagrange. Giải hệ phương trình ∂L(δw,λ)/∂δw = 0
và ∂L(δw,λ)/∂λ = 0 cho cập nhật trọng số tối ưu sau:
δw = −F̂⁻¹(w)EQ⊤[EQF̂⁻¹(w)EQ⊤]⁻¹EQw
tỉa cắt một tập trọng số Q và cập nhật các trọng
số còn lại để bảo toàn mất mát. Bây giờ, tăng mất
mát tương ứng gây ra bởi cập nhật trọng số tối ưu
δw có thể được biểu diễn là điểm tầm quan trọng
của trọng số Q, mà chúng tôi ký hiệu bằng:
SQ = 1/2(EQw)⊤[EQF̂⁻¹(w)EQ⊤]⁻¹EQw.
Chúng tôi sử dụng điểm tầm quan trọng/mức độ
quan trọng này để xếp hạng các nhóm trọng số cho
tỉa cắt. Như kiểm tra tỉnh táo, nếu chúng ta tỉa
cắt một trọng số đơn lẻ wⱼ tại một thời điểm, các
suy dẫn của chúng tôi sẽ mang lại các công thức
tiêu chuẩn của (Hassibi and Stork, 1992). Phiên
bản đầy đủ của Singh and Alistarh (2020) đã cung
cấp một suy dẫn ít tổng quát hơn một chút cho
trường hợp khối, dưới các giả định bổ sung.
3.2 Triển Khai Hiệu Quả
Triển khai trực tiếp phương pháp được mô tả trước
đây cho LLMs, trong đó số lượng trọng số w ∈ Rd
rất lớn, là không khả thi. Đặc biệt, điều này do
sự phụ thuộc vào nghịch đảo của ma trận thông
tin Fisher thực nghiệm F̂⁻¹(w) ∈ Rd×d, xuất hiện
trong công thức của điểm tầm quan trọng và cập
nhật trọng số tối ưu. Bây giờ chúng tôi mô tả cách
vượt qua những vấn đề này.
3.2.1 Tỉa cắt tập trọng số tối ưu
Giả sử thiết lập tỉa cắt dần dần, trong đó tại mỗi
bước tỉa cắt chúng ta muốn tỉa cắt một mô hình
đến độ thưa thớt mục tiêu s ∈ (0,1], thực sự làm
không sd trọng số, theo nhóm kích thước |Q|. Thông
thường sd ≫ |Q|, có nghĩa là chúng ta muốn loại
bỏ nhiều nhóm cùng một lúc. Tìm tập tối ưu của
sd/|Q| nhóm là bài toán tổ hợp không giải được,
do tất cả các tương quan có thể giữa chúng, được
cho bởi hệ số nhị thức (n choose k), trong đó n = d/|Q|
và k = sd/|Q|. Vấn đề này có thể được giảm nhẹ
bằng cách bỏ qua tương quan giữa các nhóm trọng
số Q khác nhau, và chỉ giải cho tương quan giữa
các trọng số trong cùng nhóm. Trong thực tế, điều
này quy về việc đánh giá điểm tầm quan trọng SQ
cho mỗi nhóm Q, và tỉa cắt sd/|Q| nhóm có điểm
thấp nhất. Vì tỉa cắt nhiều trọng số trong cùng
bước có thể làm xấp xỉ Taylor của hàm mất mát
kém chính xác hơn, người ta có thể xem xét tỉa
cắt với nhiều bước con nhỏ hơn với tính toán lại
xấp xỉ Hessian ở giữa (không có tinh chỉnh trung
gian). Mặc dù điều này có thể cải thiện thêm chất
lượng bước tỉa cắt (Frantar et al., 2021), chúng tôi
không triển khai tối ưu hóa bổ sung này vì các
phương pháp cạnh tranh không sử dụng tính toán
lại.
4

--- TRANG 4 ---
3.2.2 Tính toán Fisher thực nghiệm nghịch đảo
Chi phí phức tạp không gian và thời gian chính
của quy trình trên là tính toán tích với Fisher thực
nghiệm nghịch đảo. Phương pháp trực tiếp sẽ là
thực hiện xấp xỉ đường chéo khối của ma trận này
(mà chúng tôi sẽ chi tiết tiếp theo), và thực hiện
nghịch đảo khối trực tiếp. Tuy nhiên, chúng tôi
thấy thực nghiệm rằng phương pháp này quá đắt
về thời gian, và khá nhạy cảm về mặt số học. Như
một giải pháp thay thế, chúng tôi dựa vào thực tế
rằng ma trận chúng ta muốn nghịch đảo là tổng
của các ma trận hạng-1, và sử dụng công thức
nghịch đảo Woodbury/Sherman-Morrison (WSM).
Cụ thể, cho tổng (A + uv⊤) của ma trận khả nghịch
A và tích ngoài của vector u và v với chiều tương
thích, nghịch đảo (A + uv⊤)⁻¹ có thể được tính
chính xác là A⁻¹ − A⁻¹uv⊤A⁻¹/(1 + v⊤A⁻¹u). Đặt
biểu thức của Fisher thực nghiệm vào công thức
WSM, chúng ta thu được công thức đệ quy sau,
trong đó m là số gradient được sử dụng trong xấp
xỉ:
F̂⁻¹(w) = F̂ₘ⁻¹(w) = 
[F̂ₘ₋₁(w) + 1/m∇Lₘ(w)∇Lₘ⊤(w)]⁻¹.
Mở rộng đệ quy với F̂₀⁻¹(w) = 1/λId, chúng ta có
thể thu được công thức lặp để tính chính xác nghịch
đảo của ma trận Fisher thực nghiệm là
F̂⁻¹(w) = F̂ₘ⁻¹(w) = 
1/λId − Σᵢ₌₁ᵐ[(F̂ᵢ₋₁⁻¹(w)∇Lᵢ(w))(F̂ᵢ₋₁⁻¹(w)∇Lᵢ(w))⊤]/(λm + ∇Lᵢ⊤(w)F̂ᵢ₋₁⁻¹(w)∇Lᵢ(w)).
Công thức lặp có một số ưu điểm tính toán so với
triển khai trực tiếp. Đáng chú ý nhất là 1) tránh
các lời gọi rõ ràng đến nghịch đảo ma trận đắt đỏ
và nhạy cảm giảm chấn, và 2) cho phép cập nhật
liên tiếp nghịch đảo khi gradient mới được tính,
không bao giờ cần lưu trữ tất cả m gradient kích
thước d và do đó giảm đáng kể yêu cầu bộ nhớ.
3.3 Phức tạp bộ nhớ và thời gian chạy
Tính toán và lưu trữ Fisher thực nghiệm nghịch
đảo F̂⁻¹(w) ∈ Rd×d là cực kỳ đắt đỏ cho LLMs
hiện đại, có hàng trăm triệu tham số, do phức tạp
bậc hai về số lượng trọng số d. Tuy nhiên, Singh
and Alistarh (2020) đã chỉ ra rằng xấp xỉ khối đường
chéo của ma trận Fisher thực nghiệm có thể rất
chính xác cho tỉa cắt mạng nơ-ron tích chập. Chúng
tôi thích ứng cùng phương pháp ở đây, trong bối
cảnh LLMs. Do đó, cho các khối có độ rộng B dọc
theo đường chéo chính, yêu cầu bộ nhớ cho tính
toán ma trận Fisher nghịch đảo được giảm từ phụ
thuộc bậc hai O(d²) xuống phụ thuộc tuyến tính
O(Bd) vào số lượng trọng số d. Đồng thời, phức
tạp thời gian chạy thư giãn từ O(md²) xuống O(mBd).
Như chúng tôi sẽ chỉ ra, tính toán này có thể được
thực hiện hiệu quả và chính xác cho các giá trị
vừa phải của m và B.
Một giải pháp thay thế khác mà chúng tôi khảo sát
là phương pháp không ma trận của Frantar et al.
(2021), không yêu cầu xấp xỉ khối và có phức tạp
O(dm). Tuy nhiên, khảo sát của chúng tôi chỉ ra
rằng phương pháp này yêu cầu giá trị m cao để
chính xác (Hình 3 Phụ lục), dẫn đến chi phí bộ
nhớ quá mức trong trường hợp mô hình BERT.
3.4 Triển khai hiệu quả và có thể mở rộng
Về mặt thực tế, chúng tôi đã xác định các siêu tham
số tổng quát B = 50 cho kích thước khối, và m =
1024 cho số lượng gradient tạo ra kết quả tối tân
cho tất cả mô hình BERT được phân tích (chi tiết
hơn xem Phụ lục A.4), trong khi vẫn có thể vừa
trên GPU RTX 3090 24GB. Chúng tôi phản ánh
về chi phí tính toán chi tiết hơn trong Phụ lục A.3.
Hơn nữa, cho các giá trị tham số này, xấp xỉ khối
của F̂⁻¹(w) có thể được triển khai rất hiệu quả trên
các bộ tăng tốc hiện đại. Cụ thể, chúng tôi tận dụng
thực tế rằng phần cứng như vậy ưu tiên các phép
toán ma trận theo lô, và các khối kích thước B×B
trong F̂⁻¹(w) độc lập. Với NB = d/B chúng tôi
tham chiếu tổng số khối, tức chiều lô. Quy trình
hoạt động như sau. Đầu tiên, chúng tôi tính tích
ma trận-vector theo lô F̂ᵢ₋₁⁻¹(w)∇Lᵢ(w) ∈ RNB×B
và mẫu số vô hướng λm + ∇Lᵢ⊤(w)F̂ᵢ₋₁⁻¹(w)∇Lᵢ(w) ∈
RNB. Sau đó, chúng tôi cập nhật Fisher nghịch đảo
cho mỗi khối bằng cách tính tích ngoài có tỷ lệ
vô hướng
[F̂ᵢ₋₁⁻¹(w)∇Lᵢ(w)][F̂ᵢ₋₁⁻¹(w)∇Lᵢ(w)]⊤
có hình dạng RNB×B×B.
4 Xác Thực Thực Nghiệm
Để dễ dàng tái tạo, chúng tôi tiến hành thí nghiệm
trong các phiên bản sửa đổi của các thư viện mã
nguồn mở phổ biến: Transformers (Wolf et al.,
5

--- TRANG 5 ---
2020), và SparseML (Kurtz et al., 2020). Tất cả
thí nghiệm của chúng tôi sử dụng tập dữ liệu có
sẵn công khai qua Lhoest et al. (2021) và tập trung
vào mô hình BERT BASE (Devlin et al., 2019), một
trong những LLMs được sử dụng phổ biến nhất,
gồm 12 lớp transformer với 110M tham số. Theo
tiêu chuẩn cộng đồng, chúng tôi tỉa cắt trọng số
của encoder (85M) và báo cáo độ thưa thớt so với
con số này. Tất cả mô hình, công thức nén và triển
khai đầy đủ sẽ được công khai.
4.1 Tỉa Cắt Không Có Cấu Trúc Hạ Nguồn
Chúng tôi đầu tiên xem xét lại sự đánh đổi độ chính
xác-nén cho tỉa cắt trên nhiệm vụ hạ nguồn.
Mục tiêu và thiết lập. Chúng tôi so sánh các phương
pháp hiện có, đặc biệt là Tỉa Cắt Chuyển Động
(MvP) (Sanh et al., 2020) và Vé Số Trúng Thưởng
(LT-BERT) (Chen et al., 2020), với phương pháp
oBERT không có cấu trúc dần dần, được giới thiệu
trong Mục 3. Thí nghiệm của chúng tôi đánh giá
hiệu suất trên nhiều nhiệm vụ hạ nguồn (tiếng Anh)
thường được sử dụng để đánh giá nén mô hình:
trả lời câu hỏi SQuAD v1.1 (Rajpurkar et al.,
2016), phân loại câu Tập Dữ Liệu Truy Vấn Trùng
Lặp Quora QQP (Shankar et al., 2017), và suy luận
ngôn ngữ tự nhiên MNLI (Williams et al., 2018).
So sánh với MvP. Để so sánh công bằng với MvP,
chúng tôi xem xét thiết lập tỉa cắt dần dần 10 epoch
được sử dụng để thu được kết quả tốt nhất bởi
Sanh et al. (2020). Cụ thể, chúng tôi bắt đầu từ
mô hình BERT BASE và thực hiện 2 epoch tinh
chỉnh, theo sau bởi 6 epoch tỉa cắt, và 2 epoch
tinh chỉnh thêm của mô hình nén. Chúng tôi áp
dụng phân phối độ thưa thớt toàn cục trên tất cả
lớp, tỉa cắt với oBERT hai lần mỗi epoch, và sử
dụng KD từ giáo viên BERT BASE được tinh chỉnh.
Cho tỉa cắt oBERT chúng tôi sử dụng m = 1024
gradient, kích thước khối B = 50, và giảm chấn
λ = 10⁻⁷ để xấp xỉ ma trận Hessian nghịch đảo.
Trong tất cả lần chạy, bước tỉa cắt đầu tiên tỉa
cắt 70% trọng số và sau đó theo nội suy bậc ba
(Zhu and Gupta, 2018) đến độ thưa thớt mục tiêu.
Bước tỉa cắt lớn đầu tiên này cho nhiều thời gian
hơn để phục hồi từ các bước tỉa cắt sau, áp dụng
độ thưa thớt cao hơn. Tất cả siêu tham số được
mô tả chi tiết trong Phụ lục A.5, và kết quả được
cho trong Bảng 1 (trong phần 10 Epochs).
Chúng tôi quan sát rằng Bác Sĩ Phẫu Thuật BERT
Tối Ưu vượt trội hơn Tỉa Cắt Chuyển Động với
biên độ đáng kể, hơn 2 điểm F1 ở cùng độ thưa
thớt. Đáng chú ý, mô hình được tỉa cắt với oBERT
đến 97% độ thưa thớt có độ chính xác tương tự
mô hình tỉa cắt MvP ở 90% độ thưa thớt, có khoảng
3x trọng số hơn. Điều này củng cố hiệu quả của
thông tin bậc hai cho tỉa cắt.
Bảng 1: Hiệu suất tập dev nhiệm vụ hạ nguồn của
mô hình BERT BASE được tỉa cắt. (kết quả xấp xỉ
vì số chính xác không có sẵn.)
Nhiệm
vụBERT
BASEĐộ
thưa
thớtSoft
MvPoBERT
(của
chúng
tôi)LT-
BERToBERT
(của
chúng
tôi)
Epochs 10 Epochs 30 Epochs
SQuAD
F188.5480%
90%
97%-
84.90
82.30-
87.98
84.6586.54
68.00
-89.04
88.31
85.98
MNLI
m-acc84.5480%
90%
97%-
81.20
79.50-
83.20
81.0082.60
75.00
-84.32
83.79
81.77
QQP
Acc91.0680%
90%
97%-
90.20
89.10-
90.89
90.2390.30
90.00
-91.57
91.35
90.87
Tỉa cắt và tinh chỉnh mở rộng. Tiếp theo, chúng
tôi kiểm tra tác động của mở rộng lịch trình dần
dần đến 30 epoch, phù hợp với thiết lập được sử
dụng cho LT-BERT (Chen et al., 2020). Sự khác
biệt duy nhất so với thiết lập 10 epoch của chúng
tôi là bây giờ chúng tôi tỉa cắt với oBERT mỗi bốn
epoch, và tua lại tỷ lệ học sau mỗi bước tỉa cắt.
Thiết lập mở rộng để lại nhiều thời gian hơn để
phục hồi từ tỉa cắt, phản ánh trong kết quả cải
thiện trong Bảng 1 (phần 30 Epochs). Chúng tôi
báo cáo trung bình trên ba lần chạy. Để có thêm
thước đo đánh giá và độ lệch chuẩn xem Bảng 12
và 15 trong Phụ lục. Kết quả chỉ ra sự khác biệt
độ chính xác rõ ràng giữa oBERT và LT-BERT,
đặc biệt ở độ thưa thớt cao. Sự khác biệt này được
biện minh vì phương pháp dựa trên LT chủ yếu cố
gắng chuyển kết nối mạng, trong khi oBERT cũng
có thể hưởng lợi từ giá trị trọng số. Cuối cùng,
chúng tôi kiểm tra tác động của thiết lập mở rộng
với Soft MvP trên SQuAD, nhắm mục tiêu 90%
độ thưa thớt (không hiện trong Bảng), dẫn đến
tổ hợp (F1, EM) là (87.42, 79.83) cho MvP. Khoảng
cách F1 ủng hộ oBERT thấp hơn so với 10 epoch,
gợi ý rằng tinh chỉnh mở rộng giúp tất cả phương
pháp; tuy nhiên, nó vẫn không thể bỏ qua.
4.2 Tỉa Cắt Không Có Cấu Trúc Thượng Nguồn
Một giải pháp thay thế hấp dẫn cho tỉa cắt hạ nguồn
là nén mô hình thượng nguồn, trên nhiệm vụ tiền
huấn luyện bán giám sát (Zafrir et al., 2021). Cho
mô hình tỉa cắt thượng nguồn, yêu cầu tính toán
cho việc thu được mô hình tinh chỉnh hạ nguồn
được giảm đáng kể, vì chỉ cần tinh chỉnh các trọng
số còn lại.
6

--- TRANG 6 ---
Mục tiêu và thiết lập. Để so sánh với các phương
pháp hiện có, đặc biệt là Prune OFA (Zafrir et al.,
2021) và LT-BERT (Chen et al., 2020), chúng tôi
tỉa cắt dần dần với oBERT trực tiếp trên tập dữ
liệu thượng nguồn, BookCorpus và English Wikipedia,
và sau đó tinh chỉnh các trọng số không tỉa cắt
còn lại trên tập con của nhiệm vụ GLUE.
Chuẩn bị giáo viên. Theo Liu et al. (2019), chúng
tôi bắt đầu với mô hình HuggingFace BERT BASE
uncased, và tinh chỉnh thêm 10 epoch chỉ trên
nhiệm vụ mô hình hóa ngôn ngữ có mặt nạ.
Tỉa cắt tại thượng nguồn. Khi giáo viên chưng
cất được huấn luyện, chúng tôi tỉa cắt dần dần và
tinh chỉnh mô hình BERT BASE trong 3 epoch,
sử dụng KD từ giáo viên dày đặc. Chúng tôi tỉa
cắt bốn lần mỗi epoch, và tua lại tỷ lệ học về giá
trị ban đầu sau mỗi bước tỉa cắt. Siêu tham số cho
oBERT giống như cho tỉa cắt hạ nguồn trong 4.1;
mô tả đầy đủ có thể tìm thấy trong Phụ lục A.6.
Chuyển thưa thớt đến hạ nguồn. Để đánh giá các
mô hình tỉa cắt thượng nguồn kết quả, chúng tôi
tinh chỉnh các trọng số không tỉa cắt trên nhiệm
vụ hạ nguồn với KD từ mô hình BERT BASE được
tinh chỉnh. Để so sánh công bằng với Prune OFA,
chúng tôi tinh chỉnh trong 8 epoch. Kết quả trong
Bảng 2 chỉ ra rằng các mô hình thưa thớt được
sản xuất bởi oBERT vượt trội hơn các phương pháp
tối tân với biên độ đáng kể. Chúng tôi báo cáo
trung bình trên bốn lần chạy. Để có thêm thước
đo đánh giá và độ lệch chuẩn xem Bảng 13 và 16
trong Phụ lục. Đáng nhấn mạnh rằng trái ngược
với Prune OFA, đã thực hiện điều chỉnh siêu tham
số rộng rãi cho chuyển thưa thớt, công thức của
chúng tôi đơn giản và tổng quát trên các nhiệm
vụ hạ nguồn: 8 epoch tinh chỉnh với tỷ lệ học giảm
tuyến tính. Điều này gợi ý rằng các mô hình tiền
huấn luyện thưa thớt được tìm thấy bởi oBERT
tạo thành điểm khởi đầu mạnh mẽ cho học chuyển
thưa thớt, có thể được cải thiện thêm bằng điều
chỉnh siêu tham số theo nhiệm vụ cụ thể.
Bảng 2: Hiệu suất chuyển thưa thớt tập dev của mô
hình BERT BASE tỉa cắt thượng nguồn. (kết quả xấp
xỉ vì số chính xác không có sẵn.)
Nhiệm
vụBERT
BASEĐộ
thưa
thớtLT-
BERTPrune
OFAoBERT
(của
chúng
tôi)
SQuAD
F188.5490%
97%68.00
-87.25
-88.49
84.92
MNLI
m-acc84.5490%
97%75.00
-81.45
-83.40
80.91
QQP
Acc91.0690%
97%90.00
-90.93
-90.99
90.33
SST-2
Acc93.01 90% 85.0090.88 92.20
QNLI
Acc91.25 90% 80.0089.07 89.97
4.3 Nén Phức Hợp cho CPUs
Để thăm dò tác động thực tế tiềm năng của phương
pháp chúng tôi, chúng tôi chuyên môn hóa kỹ thuật
cho triển khai trên CPUs, tương ứng với triển khai
"biên". Cụ thể, chúng tôi điều chỉnh các mô hình
thưa thớt cho runtime nhận biết độ thưa thớt DeepSparse
(NeuralMagic, 2021), bằng cách phối hợp tỉa cắt
không có cấu trúc với các kỹ thuật nén bổ sung.
Thả lớp trực tiếp. Kết quả cạnh tranh thu được ở
độ thưa thớt cao trong mục 4.1 và 4.2 gợi ý rằng
BERT BASE có thể bị tham số hóa quá mức cho
nhiệm vụ hạ nguồn. Để cải thiện tỷ lệ nén và tốc
độ suy luận, chúng tôi áp dụng thả lớp "trực tiếp":
chúng tôi ban đầu thả tất cả trừ 3 hoặc 6 trong 12
lớp của BERT. Chúng tôi thả lớp từ giáo viên thượng
nguồn, và, theo (Turc et al., 2019), tinh chỉnh chúng
với KD trong cùng thiết lập được sử dụng để chuẩn
bị giáo viên thượng nguồn. Các mô hình 3 và 6
lớp này được sử dụng làm điểm khởi đầu cho tỉa
cắt hạ nguồn. Kỹ thuật thả lớp tinh vi hơn (Fan
et al., 2019), có thể mang lại thêm lợi ích độ chính
xác; chúng tôi để lại cho công việc tương lai.
Tỉa cắt khối và QAT. Suy luận hiệu suất cao thường
hưởng lợi nhiều hơn từ các mẫu độ thưa thớt (bán)
có cấu trúc hơn là từ những mẫu không có cấu
trúc. Do đó, chúng tôi sử dụng công thức oBERT
tổng quát được giới thiệu trong mục 3 và tỉa cắt
trọng số theo mẫu khối-4, có nghĩa là các khối
liền kề gồm 4 trọng số được đặt về không hoặc
giữ dày đặc. Cả hai loại tỉa cắt, không có cấu trúc
và khối-4, có thể được tận dụng cho tăng tốc tính
toán với runtime DeepSparse, nhưng tỉa cắt khối-4
kết hợp với lượng tử hóa INT8 có thể cung cấp
thêm lợi ích hiệu suất. Để lượng tử hóa, chúng tôi
áp dụng huấn luyện nhận biết lượng tử hóa (QAT)
tiêu chuẩn (Jacob et al., 2018) trên các mô hình
khối-4 (xem Phụ lục A.7 để mô tả đầy đủ).
Phối hợp cho triển khai. Để xác định tác động của
các sơ đồ nén khác nhau, chúng tôi khảo sát tỉa
cắt không có cấu trúc và khối-4 của các mô hình
3, 6, và 12-lớp. Cho tất cả lần chạy, chúng tôi sử
dụng cùng tập siêu tham số từ thiết lập tỉa cắt và
tinh chỉnh mở rộng trong Mục 4.1. Kết quả được
7

--- TRANG 7 ---
cho trong Bảng 3, trong đó chúng tôi cũng báo
cáo độ chính xác của các mô hình dày đặc tương
ứng (0% độ thưa thớt) trong cùng thiết lập. Để
có thêm thước đo đánh giá, xem Bảng 14. Kết quả
chỉ ra rằng các phương pháp nén có thể được kết
hợp mà không sụp đổ mô hình, mặc dù giảm độ
chính xác có phối hợp. Thực tế rằng các mô hình
thả lớp cũng có thể nén cao gợi ý rằng nén có cấu
trúc và tinh vi (không có cấu trúc) bổ sung cho
nhau. Chúng tôi thấy đáng chú ý rằng mô hình
oBERT 6-lớp tỉa cắt không có cấu trúc có thể cạnh
tranh với mô hình MvP 12-lớp tỉa cắt khi cả hai
được tỉa cắt đến 90% độ thưa thớt.
Bảng 3: Điểm F1 của các mô hình 3, 6, và 12-lớp
nén phức hợp trên SQuADv1.1.
Lớp Độ thưa thớt Không có cấu trúc Khối-4 +QAT
120%
80%
90%89.48
89.04
88.3189.48
88.57
87.5789.06
87.89
86.68
60%
80%
90%88.32
88.20
86.7888.32
87.00
85.3487.94
86.10
84.59
30%
80%
90%84.66
84.08
82.5084.66
82.79
80.6984.25
82.04
79.66
93 94 95 96 97 98 99 100
Hồi tưởng F1 (%)051015202530Độ lớn cải thiện
Tốc độ suy luận
Nén mô hình
Hình 2: Hồi tưởng F1 trên nhiệm vụ SQuADv1.1 so
với cải thiện tốc độ suy luận CPU và kích thước mô
hình.
Đánh đổi thực tế. Bây giờ chúng tôi đánh giá các
mô hình này theo cách đầu cuối đến cuối, cả về
kích thước mô hình và tốc độ suy luận. Để kích
thước mô hình, chúng tôi báo cáo kích thước của
checkpoint tính bằng MB sau nén gzip tiêu chuẩn.
Để tốc độ suy luận, chúng tôi báo cáo số mục mỗi
giây (thông lượng) trên benchmark suy luận CPU
SQuAD v1.1 được thiết lập tốt với độ dài chuỗi
128 và kích thước lô 32. Hình 2 mô tả độ chính
xác tương đối so với độ lớn cải thiện tốc độ và
kích thước mô hình. Làm đường cơ sở cho phục
hồi đầy đủ, chúng tôi theo tiêu chuẩn cộng đồng
ví dụ (Sanh et al., 2020), và áp dụng mô hình
BERT BASE dày đặc với điểm F1 88.54. Đường
cơ sở cho tốc độ suy luận là suy luận BERT BASE
dày đặc với DeepSparse, phù hợp với công cụ suy
luận ONNX Runtime tiêu chuẩn công nghiệp. Kết
quả gợi ý sự đánh đổi khoảng tuyến tính giữa nén
và mất mát độ chính xác, với một bước nhảy nén
quanh mất mát độ chính xác 1%, do lượng tử hóa
được áp dụng. Cụ thể, chúng tôi quan sát tăng tốc
suy luận cao hơn 8.4x ở giảm độ chính xác < 1%,
tăng tốc 10x ở giảm < 2%, tăng tốc 15x ở giảm
< 3%, và tăng tốc 29x ở giảm độ chính xác < 7.5%.
Điều này chỉ ra cách nén phức hợp có thể tối ưu
hóa LLMs đến các độ trễ khác nhau. Xem Bảng
17 Phụ lục cho kết quả đầy đủ.
4.4 Tỉa Cắt cho Tăng Tốc GPU (độ thưa thớt
N:M)
Mặc dù kết quả trước đây của chúng tôi nhắm mục
tiêu CPUs cho triển khai, bây giờ chúng tôi chỉ ra
rằng phương pháp tỉa cắt của chúng tôi cũng có
thể liên quan đến GPUs. Chúng tôi áp dụng biến
thể bán có cấu trúc của oBERT để áp dụng mẫu
độ thưa thớt 2-trong-4, được hỗ trợ trên GPU
NVIDIA Ampere (Mishra et al., 2021). Cụ thể
hơn, chúng tôi tỉa cắt một lần, và so sánh với đường
cơ sở tỉa cắt độ lớn trong Bảng 4. Tất cả phương
pháp khác yêu cầu tinh chỉnh đầy đủ, và do đó
không hỗ trợ thiết lập một lần. oBERT vượt trội
đáng kể so với tỉa cắt độ lớn, và chỉ với 1-epoch
tinh chỉnh nó có thể phục hồi hoàn toàn độ chính
xác dày đặc với (F1, EM) = (88.58, 81.16). Với
mẫu độ thưa thớt này, mô hình tỉa cắt đạt tăng
tốc 1.85x trên thiết bị Ampere.
Bảng 4: Tỉa cắt 2:4 một lần của mô hình BERT BASE
được tinh chỉnh.
Nhiệm vụ BERT BASE Độ lớn oBERT (của chúng tôi)
SQuAD
F1 / EM88.54 / 81.41 49.97 / 35.24 83.17 / 74.18
5 Thảo Luận
So sánh với công trình đồng thời. Công trình đồng
thời giới thiệu PLATON (Zhang et al., 2022), giải
quyết tỉa cắt không có cấu trúc của mô hình BERT
qua ước lượng cận tin cậy. Nó không sử dụng KD,
nên để so sánh công bằng chúng tôi chạy lại thí
nghiệm mà không có KD. Trái ngược với PLATON,
8

--- TRANG 8 ---
báo cáo kết quả tốt nhất sau tìm kiếm siêu tham
số rộng rãi cho mỗi nhiệm vụ độc lập, chúng tôi
áp dụng thiết lập chuyển thưa thớt với mô hình
tỉa cắt thượng nguồn và chỉ quét số epoch ∈ [1,8].
Chúng tôi sử dụng dừng sớm để ngăn overfitting
trên các nhiệm vụ GLUE nhỏ hơn. Như có thể
thấy từ Bảng 5, oBERT vượt trội hơn PLATON
trên tất cả nhiệm vụ.
Bảng 5: Mô hình BERT BASE nén đến 90% độ thưa
thớt trên nhiệm vụ GLUE không có chưng cất tri thức.
Nhiệm vụ BERT BASE PLATONoBERT
(của chúng tôi)
MNLI
m / mm84.6 / 83.4 82.0 / 82.2 82.2 / 82.5
QQP
Acc / F191.5 / 88.5 90.2 / 86.8 90.4 / 87.1
QNLI
Acc91.3 88.9 89.3
MRPC
Acc / F186.4 / 90.3 84.3 / 88.8 85.6 / 89.3
SST-2
Acc92.7 90.5 92.0
CoLA
Mcc58.3 44.3 48.47
STS-B
Pear / Spear90.2 / 89.7 87.4 / 87.1 88.0 / 87.6
So sánh rộng hơn. Bây giờ chúng tôi đối chiếu
các mô hình BERT BASE nén phức hợp so với các
kỹ thuật nén thay thế. Chúng tôi so sánh với DistilBERT
(Sanh et al., 2019), TinyBERT (Jiao et al., 2020),
và Tỉa Cắt Khối Cho Transformers Nhanh Hơn
(Hybrid Filled MvP) (Lagunas et al., 2021). DistilBERT
tận dụng KD trong tiền huấn luyện và tinh chỉnh
để thu được mô hình 6-lớp được tinh chỉnh cho
nhiệm vụ hạ nguồn cụ thể. TinyBERT sử dụng sơ
đồ Transformer-KD chuyên môn để chưng cất tri
thức và biểu diễn trung gian ở cả hai giai đoạn,
tiền huấn luyện và tinh chỉnh trên nhiệm vụ cụ
thể. Ngược lại, chúng tôi sử dụng phương pháp
đơn giản hơn và chỉ sử dụng KD từ đầu ra của
giáo viên. Hybrid Filled MvP (Lagunas et al., 2021)
sử dụng tỉa cắt bán có cấu trúc và tái giới thiệu
trọng số. So sánh được cho trong Bảng 6, trong
đó chúng tôi báo cáo số trọng số encoder không
tỉa cắt làm kích thước, tỷ lệ nén và tăng tốc suy
luận so với BERT BASE dày đặc trong cùng môi
trường suy luận, và điểm F1 trên tập dev của tập
dữ liệu SQuAD v1.1. Kết quả gợi ý rằng các mô
hình nén của chúng tôi cải thiện so với các kỹ
thuật tối tân hiện tại, đặt đường cơ sở mới rất
cạnh tranh về tất cả thước đo: độ chính xác, kích
thước mô hình, và tốc độ suy luận.
Bảng 6: Mô hình BERT BASE nén trên nhiệm vụ
SQuADv1.1. (oBERT 6,80 là mô hình 6-lớp tỉa cắt
đến 80% độ thưa thớt.)
Mô hình Kích thước Nén Tăng tốc F1 Dev
BERT BASE 85.0M 1.00x 1.00x 88.54
< 6-lớp
TinyBERT4 4.5M 18.88x 9.40x 82.10 GPU
oBERT 3,90 2.1M 40.00x 14.80x 82.50 CPU
6-lớp
DistilBERT 42.5M 2.00x 2.00x 86.90 GPU
TinyBERT6 42.5M 2.00x 2.00x 87.50 GPU
oBERT 6,80 8.5M 10.00x 6.38x 88.20 CPU
12-lớp
Hybrid F. MvP 30.7M 2.76x 1.84x 88.70 GPU
oBERT 12,80 17.0M 5.00x 3.38x 89.04 CPU
Kết quả BERT LARGE. Hầu hết kết quả chúng tôi
trình bày trong Mục 4 nhắm mục tiêu mô hình
BERT BASE được áp dụng rộng rãi. Điều này cho
chúng tôi cơ hội so sánh công bằng với nhiều phương
pháp khác nhau. Để xác minh rằng phương pháp
của chúng tôi không chỉ liên quan đến mô hình
BERT BASE, trong Bảng 7 chúng tôi trình bày kết
quả tỉa cắt hạ nguồn trên mô hình BERT LARGE
lớn gấp ba lần và nhiệm vụ SQuADv1.1. Như có
thể thấy từ Bảng, ngay cả mô hình được tỉa cắt
với oBERT ở độ thưa thớt gấp đôi (95%) vẫn vượt
trội hơn Prune OFA (90%).
Bảng 7: Mô hình BERT LARGE nén trên nhiệm vụ
SQuADv1.1.
BERT LARGE
F1 / EMĐộ thưa thớtPrune
OFAoBERT
(của chúng tôi)
91.22 / 84.45 90% 90.20 / 83.35 91.07 / 84.61
91.22 / 84.45 95% NA 90.29 / 83.58
Benchmark Suy Luận MLPerf. Được thúc đẩy bởi
kết quả tối tân toàn diện của chúng tôi, chúng tôi
áp dụng pipeline nén phức hợp đầy đủ để nén mô
hình BERT LARGE và MobileBERT (Sun et al.,
2020b) trong bối cảnh Benchmark Suy Luận MLPerf
công nghiệp¹. Tóm tắt, chúng tôi có thể đạt cải
thiện bậc độ lớn về kích thước mô hình và tăng
tốc suy luận, trong khi duy trì >99% độ chính xác
BERT LARGE dày đặc. Để biết chi tiết xem Phụ
lục A.1, cũng như bài nộp mã nguồn mở của
chúng tôi.
6 Tác Động Rộng Hơn
Công trình của chúng tôi là một phần của xu hướng
chung sản xuất các mô hình hiệu quả suy luận xấp
xỉ hiệu suất của các cơ sở lớn hơn. Nói chung,
công trình này sẽ giúp tăng hiệu quả mô hình, do
đó giảm chi phí tính toán và cuối cùng là chi phí
tiền tệ để thực thi các mô hình như vậy. Hơn nữa,
nó có thể cho phép các mô hình được sử dụng bởi
những người không có quyền truy cập vào các cụm
tính toán chuyên môn đắt tiền: ví dụ, kết quả tăng
tốc chính của chúng tôi nhắm mục tiêu CPUs có
sẵn rộng rãi.
7 Hạn Chế
Như bất kỳ nghiên cứu học thuật nào, công trình
của chúng tôi không phải không có hạn chế. Chúng
tôi chia thảo luận về chúng thành hạn chế vốn có
của phương pháp chúng tôi, và hạn chế của nghiên
cứu hiện tại; phần sau có thể được khắc phục bằng
mở rộng công trình của chúng tôi. Trong danh mục
đầu tiên, chúng tôi bắt đầu bằng cách nhấn mạnh
thực tế rằng phương pháp bậc hai của chúng tôi
dựa vào xấp xỉ, vốn cần thiết để mở rộng các phương
pháp như vậy đến quy mô BERT. Các nghiên cứu
trước, ví dụ (Singh and Alistarh, 2020) đã thực
hiện kiểm tra cẩn thận về tính hợp lệ của các xấp
xỉ này trong bối cảnh mô hình CNN. Sức mạnh
của kết quả thực nghiệm chúng tôi có thể được
xem như bằng chứng gián tiếp rằng các xấp xỉ này
áp dụng cho mô hình BERT. Hạn chế kỹ thuật thứ
hai là thực tế rằng phương pháp của chúng tôi yêu
cầu chi phí lưu trữ bổ sung không tầm thường;
mặc dù chúng tôi đã chỉ ra rằng thí nghiệm có thể
được thực thi trên một GPU thông thường duy nhất
(NVIDIA RTX 3090), điều này hạn chế phạm vi
thiết bị mà kỹ thuật có thể được áp dụng. Tuy nhiên,
chúng tôi cung cấp cách hiệu quả và dễ dàng để
mở rộng phương pháp với nhiều GPU hơn, được
sử dụng tự động trong môi trường đa GPU.
Một hạn chế khác mà chúng tôi nhằm loại bỏ trong
công việc tương lai là tập trung vào các loại độ
thưa thớt tương đối tinh vi, như tỉa cắt không có
cấu trúc và bán có cấu trúc.
Tài Liệu Tham Khảo
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia
Liu, Yang Zhang, Zhangyang Wang, và Michael
Carbin. 2020. Giả thuyết vé số trúng thưởng cho
mạng bert tiền huấn luyện. Advances in neural
information processing systems, 33:15834–15846.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry,
Ran El-Yaniv, và Yoshua Bengio. 2016. Mạng
nơ-ron nhị phân hóa: Huấn luyện mạng nơ-ron
sâu với trọng số và kích hoạt bị ràng buộc về +1
hoặc -1. arXiv preprint arXiv:1602.02830.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, và
Kristina Toutanova. 2019. BERT: Tiền huấn luyện
transformers hai chiều sâu cho hiểu biết ngôn ngữ.
Trong Proceedings of the 2019 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
trang 4171–4186, Minneapolis, Minnesota. Association
for Computational Linguistics.
Angela Fan, Edouard Grave, và Armand Joulin. 2019.
Giảm độ sâu transformer theo yêu cầu với dropout
có cấu trúc. Trong International Conference on
Learning Representations.
Wikimedia Foundation. Wikimedia downloads.
Jonathan Frankle và Michael Carbin. 2018. Giả thuyết
vé số trúng thưởng: Tìm mạng nơ-ron thưa thớt,
có thể huấn luyện. Trong International Conference
on Learning Representations.
Elias Frantar, Eldar Kurtic, và Dan Alistarh. 2021.
M-fac: Xấp xỉ hiệu quả không ma trận của thông
tin bậc hai. Advances in Neural Information Processing
Systems, 34.
Trevor Gale, Erich Elsen, và Sara Hooker. 2019. Trạng
thái độ thưa thớt trong mạng nơ-ron sâu. arXiv
preprint arXiv:1902.09574.
Song Han, Huizi Mao, và William J Dally. 2015.
Pipeline nén mạng nơ-ron sâu: Tỉa cắt, lượng tử
hóa, mã hóa huffman. arXiv preprint arXiv:1510.00149,
10.
Babak Hassibi và David Stork. 1992. Đạo hàm bậc
hai cho tỉa cắt mạng: Bác sĩ phẫu thuật não tối ưu.
Advances in neural information processing systems,
5.
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.
Chưng cất tri thức trong mạng nơ-ron. arXiv preprint
arXiv:1503.02531, 2(7).
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong
Zhu, Matthew Tang, Andrew Howard, Hartwig Adam,
và Dmitry Kalenichenko. 2018. Lượng tử hóa và
huấn luyện mạng nơ-ron cho suy luận hiệu quả
chỉ với số nguyên. Trong Proceedings of the IEEE
conference on computer vision and pattern recognition,
trang 2704–2713.
10

--- TRANG 9 ---
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li, Fang Wang, và Qun Liu.
2020. Tinybert: Chưng cất bert cho hiểu biết ngôn
ngữ tự nhiên. Trong Findings of the Association for
Computational Linguistics: EMNLP 2020, trang
4163–4174.
Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami,
Woosuk Kwon, Joseph Hassoun, và Kurt Keutzer.
2022. Tỉa cắt token học được cho transformers.
Trong Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining, KDD
'22, trang 784–794. Association for Computing
Machinery.
Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers,
và Anna Rumshisky. 2021. Kẻ phá hoại BERT:
Chiều layernorm ngoại lai làm gián đoạn BERT.
CoRR, abs/2105.06990.
Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander
Matveev, John Carr, Michael Goin, William Leiserson,
Sage Moore, Bill Nell, Nir Shavit, và Dan Alistarh.
2020. Tạo và khai thác độ thưa thớt kích hoạt cho
suy luận nhanh trên mạng nơ-ron sâu. Trong Proceedings
of the 37th International Conference on Machine
Learning, volume 119 của Proceedings of Machine
Learning Research, trang 5533–5543, Virtual. PMLR.
François Lagunas, Ella Charlaix, Victor Sanh, và
Alexander Rush. 2021. Tỉa cắt khối cho transformers
nhanh hơn. Trong Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing,
trang 10619–10629, Online và Punta Cana, Dominican
Republic. Association for Computational Linguistics.
Yann LeCun, John Denker, và Sara Solla. 1989. Tổn
thương não tối ưu. Advances in neural information
processing systems, 2.
Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien
Plu, Lewis Tunstall, Joe Davison, Mario Šaško,
Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
Patry, Angelina McMillan-Major, Philipp Schmid,
Sylvain Gugger, Clément Delangue, Théo Matussière,
Lysandre Debut, Stas Bekman, Pierric Cistac,
Thibault Goehringer, Victor Mustar, François Lagunas,
Alexander Rush, và Thomas Wolf. 2021. Datasets:
Thư viện cộng đồng cho xử lý ngôn ngữ tự nhiên.
Trong Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, trang 175–184. Association for
Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, và Veselin Stoyanov. 2019.
Roberta: Phương pháp tiền huấn luyện bert được
tối ưu hóa mạnh mẽ. arXiv preprint arXiv:1907.11692.
Paul Michel, Omer Levy, và Graham Neubig. 2019.
Mười sáu đầu có thực sự tốt hơn một? Advances
in neural information processing systems, 32.
Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko
Stosic, Dusan Stosic, Ganesh Venkatesh, Chong
Yu, và Paulius Micikevicius. 2021. Tăng tốc mạng
nơ-ron sâu thưa thớt. arXiv preprint arXiv:2104.08378.
NeuralMagic. 2021. Deep sparse: Công cụ suy luận
cpu nhanh.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Mô hình
ngôn ngữ là người học đa nhiệm không giám sát.
OpenAI blog, 1(8):9.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
và Percy Liang. 2016. Squad: 100,000+ câu hỏi
cho hiểu văn bản máy. Trong EMNLP.
Hassan Sajjad, Fahim Dalvi, Nadir Durrani, và Preslav
Nakov. 2020. BERT của người nghèo: Mô hình
transformer nhỏ hơn và nhanh hơn. CoRR, abs/2004.03844.
Victor Sanh, Lysandre Debut, Julien Chaumond, và
Thomas Wolf. 2019. Distilbert, phiên bản chưng
cất của bert: nhỏ hơn, nhanh hơn, rẻ hơn và nhẹ
hơn. arXiv preprint arXiv:1910.01108.
Victor Sanh, Thomas Wolf, và Alexander Rush. 2020.
Tỉa cắt chuyển động: Độ thưa thớt thích ứng bằng
tinh chỉnh. Advances in Neural Information Processing
Systems, 33:20378–20389.
Iyer Shankar, Dandekar Nikhil, và Csernai Kornel.
2017. Phát hành tập dữ liệu quora đầu tiên: Cặp
câu hỏi.
S. Shankar. 2017. Xác định cặp câu hỏi quora có
cùng ý định.
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei
Yao, Amir Gholami, Michael W Mahoney, và Kurt
Keutzer. 2020. Q-bert: Lượng tử hóa độ chính xác
cực thấp dựa trên hessian của bert. Trong Proceedings
of the AAAI Conference on Artificial Intelligence,
volume 34, trang 8815–8821.
Sidak Pal Singh và Dan Alistarh. 2020. Woodfisher:
Xấp xỉ bậc hai hiệu quả cho nén mạng nơ-ron.
Advances in Neural Information Processing Systems,
33.
Shaden Smith, Mostofa Patwary, Brandon Norick,
Patrick LeGresley, Samyam Rajbhandari, Jared
Casper, Zhun Liu, Shrimai Prabhumoye, George
Zerveas, Vijay Korthikanti, et al. 2022. Sử dụng
deepspeed và megatron để huấn luyện megatron-
turing nlg 530b, mô hình ngôn ngữ sinh lớn.
arXiv preprint arXiv:2201.11990.
11

--- TRANG 10 ---
Sharath Nittur Sridhar và Anthony Sarah. 2020. Sự
chú ý không phân chia: Các lớp trung gian có cần
thiết cho bert không? arXiv preprint arXiv:2012.11881.
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, và Denny Zhou. 2020a. Mobilebert:
bert nhỏ gọn bất khả tri nhiệm vụ cho thiết bị hạn
chế tài nguyên. Trong ACL.
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, và Denny Zhou. 2020b. Mobilebert:
bert nhỏ gọn bất khả tri nhiệm vụ cho thiết bị hạn
chế tài nguyên. Trong Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics,
trang 2158–2170.
Iulia Turc, Ming-Wei Chang, Kenton Lee, và Kristina
Toutanova. 2019. Học sinh đọc nhiều học tốt hơn:
Tác động của khởi tạo học sinh lên chưng cất tri
thức. ArXiv, abs/1908.08962.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, và Illia Polosukhin. 2017. Sự chú ý là tất
cả những gì bạn cần. Advances in neural information
processing systems, 30.
Elena Voita, David Talbot, F. Moiseev, Rico Sennrich,
và Ivan Titov. 2019. Phân tích tự chú ý đa đầu:
Các đầu chuyên môn làm công việc nặng, phần còn
lại có thể được tỉa cắt. Trong ACL.
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, và Ming Zhou. 2020. Minilm: Chưng cất
tự chú ý sâu cho nén transformer tiền huấn luyện
bất khả tri nhiệm vụ. Advances in Neural Information
Processing Systems, 33:5776–5788.
Liu Weijie, Zhou Peng, Zhao Zhe, Wang Zhiruo,
Deng Haotang, và Ju Qi. 2020. Fastbert: bert tự
chưng cất với thời gian suy luận thích ứng. Trong
Proceedings of ACL 2020.
Adina Williams, Nikita Nangia, và Samuel Bowman.
2018. Corpus thử thách bao phủ rộng cho hiểu câu
thông qua suy luận. Trong Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers),
trang 1112–1122. Association for Computational
Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, và Alexander M. Rush. 2020.
Transformers: Xử lý ngôn ngữ tự nhiên tối tân.
Trong Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations, trang 38–45, Online. Association
for Computational Linguistics.
Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, và
Jimmy J. Lin. 2020. Deebert: Thoát sớm động để
tăng tốc suy luận bert. Trong ACL.
Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, và Zhibin
Xiao. 2021. Suy nghĩ lại tỉa cắt mạng – dưới mô
hình tiền huấn luyện và tinh chỉnh. Trong NAACL.
Shixing Yu, Zhewei Yao, Amir Gholami, Zhen Dong,
Sehoon Kim, Michael W Mahoney, và Kurt Keutzer.
2022. Tỉa cắt nhận biết hessian và cấy ghép nơ-ron
tối ưu. Trong Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision,
trang 3880–3891.
Ofir Zafrir, Guy Boudoukh, Peter Izsak, và Moshe
Wasserblat. 2019. Q8bert: Bert lượng tử 8bit. 2019
Fifth Workshop on Energy Efficient Machine Learning
and Cognitive Computing - NeurIPS Edition (EMC2-
NIPS), trang 36–39.
Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen,
và Moshe Wasserblat. 2021. Tỉa cắt một lần cho
tất cả: Mô hình ngôn ngữ tiền huấn luyện thưa
thớt. arXiv preprint arXiv:2111.05754.
Qingru Zhang, Simiao Zuo, Chen Liang, Alexander
Bukharin, Pengcheng He, Weizhu Chen, và Tuo
Zhao. 2022. Platon: Tỉa cắt mô hình transformer
lớn với cận trên tin cậy của tầm quan trọng trọng
số. Trong International Conference on Machine
Learning, trang 26809–26823. PMLR.
Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao
Chen, Xin Jiang, và Qun Liu. 2020. Ternarybert:
bert cực thấp bit nhận biết chưng cất. arXiv preprint
arXiv:2009.12812.
M. Zhu và Suyog Gupta. 2018. Tỉa cắt, hay không
tỉa cắt: khám phá hiệu quả của tỉa cắt cho nén mô
hình. ArXiv, abs/1710.01878.
Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan
Salakhutdinov, Raquel Urtasun, Antonio Torralba,
và Sanja Fidler. 2015. Căn chỉnh sách và phim:
Hướng tới giải thích trực quan giống câu chuyện
bằng cách xem phim và đọc sách. 2015 IEEE International
Conference on Computer Vision (ICCV), trang
19–27.
A Phụ Lục
A.1 Benchmark suy luận MLPerf
Theo hướng dẫn benchmark MLPerf về sản xuất
các mô hình nén và nhanh trong khi duy trì >99%
điểm F1 BERT LARGE trên nhiệm vụ SQuADv1.1,
chúng tôi khám phá hai hướng. Trong hướng đầu
tiên, được gọi là oBERT-Large, chúng tôi nén phức
hợp mô hình BERT LARGE mà không thay đổi
kiến trúc. Do đó, chúng tôi áp dụng tỉa cắt hạ
nguồn khối-4 đến 95% độ thưa thớt theo sau bởi
huấn luyện nhận biết lượng tử (QAT). Trong hướng
thứ hai chúng tôi tập trung vào phục hồi độ chính
xác BERT LARGE bằng cách nén mô hình MobileBERT
đã nhỏ gọn, được gọi là oBERT-MobileBERT. Cụ
thể hơn, chúng tôi áp dụng thả lớp trực tiếp, chỉ
để lại 14 lớp transformer trong số 24 lớp ban đầu,
theo sau bởi tỉa cắt khối-4 đến 50% độ thưa thớt
và huấn luyện nhận biết lượng tử. Chúng tôi trình
bày kết quả trong Bảng 8, trong đó các mô hình
được đánh giá với công cụ suy luận DeepSparse,
sử dụng máy chủ với hai CPU Intel(R) Xeon(R)
Platinum 8380 (IceLake) với 40 lõi mỗi cái, kích
thước lô 128 và độ dài chuỗi 384. Để biết thêm
chi tiết xem bài nộp chính thức của chúng tôi tại
https://github.com/neuralmagic/mlperf_inference_
results_v2.1/tree/master/open/NeuralMagic.
A.2 So sánh bổ sung
Ở đây chúng tôi phản ánh về một số phương pháp
khác tập trung vào suy luận hiệu quả cho LLMs,
trực giao với tỉa cắt trọng số. Ví dụ, Tỉa Cắt Token
Học Được (Kim et al., 2022) cố gắng loại bỏ thích
ứng các token không quan trọng trong chuỗi đầu
vào và cung cấp thông lượng cao hơn 2x ở giảm
độ chính xác < 1%; ở cùng mức giảm độ chính xác,
mô hình nén của chúng tôi có thể đạt thông lượng
cao hơn 8.4x. DeeBERT (Xin et al., 2020) và FastBERT
(Weijie et al., 2020) áp dụng kỹ thuật thoát sớm
cho tăng tốc suy luận. Phương pháp sau đạt suy
luận nhanh hơn 2-3x mà không suy giảm hiệu suất.
Tuy nhiên, phương pháp chỉ áp dụng cho kích
thước lô một. Tuy nhiên, về so sánh trực tiếp, các
mô hình nén của chúng tôi có thể đạt suy luận
nhanh hơn 4x trên CPUs mà không suy giảm độ
chính xác. Tổng thể, chúng tôi nhấn mạnh thực
tế rằng các phương pháp này bổ sung cho kỹ thuật
nén của chúng tôi, nên sẽ thú vị khi khảo sát lợi
ích tính toán bằng cách kết hợp các phương pháp
như vậy.
A.3 Chi phí tính toán
Trong thực tế, cho mô hình BERT BASE 12-lớp
với d = 85M trọng số encoder và kích thước khối
B = 50, yêu cầu bộ nhớ O(Bd) chuyển thành khoảng
17GB, có thể dễ dàng giữ trên thẻ RTX 3090 24GB.
Mặc dù lượng bộ nhớ này có sẵn trên GPUs hiệu
suất cao, cũng đơn giản để chia tensor NB×B×B
dọc theo chiều lô NB và sử dụng GPUs bổ sung
hoặc thậm chí hoán đổi bộ nhớ với CPU. Triển
khai của chúng tôi cập nhật xấp xỉ Hessian nghịch
đảo trong thời gian không đáng kể, và có thể chạy
bất đồng bộ trong khi gradient tiếp theo đang được
lấy. Tính toán điểm tầm quan trọng và cập nhật
trọng số tối ưu chỉ mất vài giây.
A.4 Siêu tham số Bác Sĩ Phẫu Thuật BERT Tối
Ưu (oBERT)
Siêu tham số. Phương pháp tỉa cắt oBERT có ba
siêu tham số có thể điều chỉnh: số gradient (m),
kích thước khối (B), và giảm chấn (λ). Chúng được
điều chỉnh theo mô hình và tài nguyên tính toán
có sẵn. Trong tất cả lần chạy, trên tất cả mô hình
và tập dữ liệu, chúng tôi sử dụng cùng tập siêu
tham số mà chúng tôi thấy hoạt động tốt nhất cho
mô hình BERT BASE trên tập dữ liệu SQuAD v1.1.
Chúng tôi phỏng đoán rằng điều chỉnh thêm cho
các mô hình nhỏ hơn (mô hình 3 và 6-lớp) có thể
cải thiện kết quả của chúng, nhưng để đơn giản
và công bằng với các phương pháp khác, chúng
tôi áp dụng cùng những cái được tìm thấy cho
BERT BASE.
Nghiên cứu loại bỏ. Quy trình tìm tập siêu tham
số tối ưu cho một mô hình gồm tìm kiếm lưới
trên các tổ hợp siêu tham số có thể và các lần
chạy tỉa cắt một lần đến các mục tiêu độ thưa thớt
cao khác nhau để đánh giá chất lượng xấp xỉ tỉa
cắt cho mỗi tổ hợp. Chúng tôi thấy rằng m = 1024,
B = 50, và λ = 10^-7 tạo ra kết quả tối tân với
chi phí tính toán không đáng kể với mô hình BERT
BASE. Frantar et al. (2021) chỉ ra rằng kích thước
khối lớn hơn yêu cầu nhiều gradient hơn cho xấp
xỉ tốt hơn. Do kích thước khổng lồ của mô hình
BERT BASE, chúng tôi chọn thiết lập này vì nó
là thiết lập hoạt động tốt nhất vẫn có thể vừa trên
một thẻ GPU RTX 3090 24GB. Trong Hình 3, 4,
và 5 chúng tôi trực quan hóa một phần của các
nghiên cứu loại bỏ tỉa cắt một lần đối với tất cả
ba siêu tham số thúc đẩy chúng tôi chọn các giá
trị cụ thể này.
A.5 Tỉa cắt hạ nguồn
Chuẩn bị giáo viên. Cho tất cả lần chạy tỉa cắt
hạ nguồn chúng tôi sử dụng KD từ đầu ra giáo
viên BERT BASE được tinh chỉnh. Giáo viên được
tinh chỉnh trên nhiệm vụ hạ nguồn tương ứng theo
siêu tham số mặc định cho SQuAD² và GLUE
(QQP và MNLI)³.
Thiết lập tỉa cắt. Trong Bảng 9 chúng tôi mô tả
chi tiết tất cả siêu tham số cho kết quả tỉa cắt hạ
nguồn được trình bày trong Bảng 1 và 3. Để hiểu
dễ hơn, chúng tôi cũng trực quan hóa lịch trình
tỷ lệ học trong Hình 6 và 8, và lịch trình độ thưa
thớt trong Hình 7 và 9.
Mô hình 3-, 6-lớp. Chúng tôi chuẩn bị các mô
hình 3 và 6 lớp cho các lần chạy hạ nguồn trong
hai giai đoạn: thả lớp và giai đoạn huấn luyện lại.
Chúng tôi thả lớp từ mô hình giáo viên thượng
nguồn (chi tiết hơn trong Phụ lục A.6). Sau khi
thả, chúng tôi huấn luyện lại các lớp còn lại, theo
hiểu biết từ (Turc et al., 2019), trong cùng thiết
lập được sử dụng để chuẩn bị giáo viên thượng
nguồn với bổ sung KD từ nó.
A.6 Tỉa cắt thượng nguồn
Chuẩn bị giáo viên. Chúng tôi chuẩn bị giáo viên
cho tỉa cắt thượng nguồn bằng cách theo một số
hiểu biết từ (Liu et al., 2019). Cụ thể hơn chúng
tôi bắt đầu với mô hình

²https://github.com/huggingface/transformers/tree/main/
examples/pytorch/question-answering
³https://github.com/huggingface/transformers/tree/main/
examples/pytorch/text-classification
14

--- TRANG 11 ---
10 Epochs 30 Epochs
Kích thước lô16 cho SQuAD,
32 cho GLUE
Tỷ lệ học (ban đầu, cuối)(8e-5, 3e-5) cho SQuAD,
(8e-5, 2e-5) cho GLUE(8e-5, 8e-6) cho SQuAD,
(5e-5, 5e-6) cho GLUE
Lịch trình tỷ lệ học giảm tuyến tính với tua lại
Tua lại tỷ lệ học một lần tại epoch=8định kỳ mỗi 4 epochs,
bắt đầu tại epoch=2
Chưng Cất Tri Thức (độ cứng, nhiệt độ) (1.0, 2.0)
Mô hình học sinh12-lớp: bert-base-uncased
6-lớp: thả lớp + tiền huấn luyện với KD
3-lớp: thả lớp + tiền huấn luyện với KD
Mô hình giáo viên BERT BASE
Epoch bắt đầu tỉa cắt epoch=2
Epoch kết thúc tỉa cắt epoch=8 epoch=26
Tần suất tỉa cắt 2x mỗi epoch một lần mỗi 4 epochs
Bước độ thưa thớt ban đầu12-lớp: 70%
6-lớp: 30%
3-lớp: 30%
Phân phối độ thưa thớt toàn cục trên tất cả lớp
Tham số oBERTSố gradient m = 1024
Kích thước khối B = 50
Giảm chấn λ = 10^-7
Bảng 9: Siêu tham số tỉa cắt hạ nguồn được sử dụng để thu được kết quả trình bày trong Bảng 1 và 3.
0 2 4 6 8 10
Epoch2345678Tỷ lệ học1e5
SQuAD
GLUE
Hình 6: Lịch trình tỷ lệ học được trực quan hóa cho
các lần chạy hạ nguồn 10-epoch.
bert-base-uncased⁴ mô hình, áp dụng tiền huấn
luyện trên hai tập dữ liệu (BookCorpus⁵ & English
Wikipedia⁶) với tập trung vào nhiệm vụ mô hình
hóa ngôn ngữ có mặt nạ (MLM) trong 10-epochs
với kích thước lô 256 và tỷ lệ học giảm tuyến tính
về không từ giá trị ban đầu 1e-4.
⁴https://huggingface.co/bert-base-uncased
⁵https://huggingface.co/datasets/bookcorpus
⁶https://huggingface.co/datasets/wikipedia
0 2 4 6 8 10
Epoch0708090Độ thưa thớt (%)
Hình 7: Lịch trình độ thưa thớt được trực quan hóa
cho các lần chạy hạ nguồn 10-epoch với độ thưa thớt
ban đầu 70% và độ thưa thớt mục tiêu 90%, theo nội
suy bậc ba (Zhu and Gupta, 2018).
Thiết lập tỉa cắt. Trong Bảng 10 chúng tôi mô tả
chi tiết công thức tỉa cắt thượng nguồn của chúng
tôi. Như có thể nhận thấy, công thức tỉa cắt thượng
nguồn của chúng tôi chỉ là phiên bản thu nhỏ của
công thức tỉa cắt hạ nguồn 30-epoch xuống 3-epochs.
15

--- TRANG 12 ---
3 Epochs
Tập dữ liệu BookCorpus & English Wikipedia
Kích thước lô 256
Tỷ lệ học ban đầu 5e-4
Lịch trình tỷ lệ học giảm tuyến tính với tua lại
Tua lại tỷ lệ học định kỳ mỗi 0.5 epochs
Độ dài chuỗi tối đa 512
Phân rã trọng số 0.01
Chưng Cất Tri Thức
(độ cứng, nhiệt độ)(1.0, 5.5)
Mô hình học sinh giáo viên thượng nguồn đã chuẩn bị
Mô hình giáo viên giáo viên thượng nguồn đã chuẩn bị
Tần suất tỉa cắt 4x mỗi epoch
Bảng 10: Siêu tham số tỉa cắt thượng nguồn.
8 Epochs
Tỷ lệ học ban đầu 1.5e-4
Lịch trình tỷ lệ học giảm tuyến tính về 1.5e-6
Kích thước lô16 cho SQuAD,
32 cho GLUE
Chưng Cất Tri Thức
(độ cứng, nhiệt độ)(1.0, 5.5)
Mô hình giáo viên BERT BASE
Bảng 11: Siêu tham số học chuyển thưa thớt được
sử dụng để tinh chỉnh các mô hình tỉa cắt thượng
nguồn tại các nhiệm vụ hạ nguồn. Các siêu tham số
này được sử dụng để thu được kết quả trình bày
trong Bảng 2.
0 5 10 15 20 25 30
Epoch12345678Tỷ lệ học1e5
SQuAD
GLUE
Hình 8: Lịch trình tỷ lệ học được trực quan hóa cho
các lần chạy hạ nguồn 30-epoch.
0 5 10 15 20 25 30
Epoch0708090Độ thưa thớt (%)
Hình 9: Lịch trình độ thưa thớt được trực quan hóa
cho các lần chạy hạ nguồn 30-epoch với độ thưa thớt
ban đầu 70% và độ thưa thớt mục tiêu 90%, theo nội
suy bậc ba (Zhu and Gupta, 2018).
A.7 Lượng tử hóa hạ nguồn
Chúng tôi thực hiện QAT trên các mô hình dày
đặc và tỉa cắt khối-4 trên SQuAD v1.1 như hiển
thị trong Bảng 3. Chúng tôi lượng tử hóa về 8
bits các ma trận embedding, mô-đun tuyến tính
của tất cả đơn vị encoder bao gồm các ma trận
trong lớp chú ý và feed forward, và mô-đun tuyến
tính của lớp đầu ra. Trọng số đã được tỉa cắt được
giữ không đổi (zero) trong quá trình lượng tử hóa
(mặt nạ độ thưa thớt được bảo toàn). Các phép
toán phi tuyến trong Softmax, LayerNorm và GeLU
không được lượng tử hóa. Cho mỗi mô hình dày
đặc và tỉa cắt khối-4 trong Bảng 3, chúng tôi thực
hiện tổng cộng mười epochs huấn luyện trong đó
các quan sát viên lượng tử hóa hoạt động trong
năm đầu và phần còn lại là tinh chỉnh. Chúng tôi
tìm kiếm siêu tham số trên tỷ lệ học 1e-4, 8e-5,
5e-5, 3e-5 và độ cứng chưng cất 0.9 và 1.0. Sau
đó chúng tôi chọn mô hình có điểm F1 tốt nhất.
16

--- TRANG 13 ---
A.8 Thước đo hiệu suất bổ sung
Do hạn chế không gian, trong bài báo chúng tôi
báo cáo điểm F1 cho SQuAD v1.1, độ chính xác
khớp cho MNLI, và độ chính xác cho tập dữ liệu
QQP. Vì tất cả siêu tham số cho MNLI và QQP
hoàn toàn giống nhau, chúng tôi tham chiếu hai
tập dữ liệu này là GLUE. Trong Bảng 12 chúng
tôi báo cáo thêm các thước đo: khớp chính xác
(EM) cho SQuAD v1.1, độ chính xác không khớp
cho MNLI, và điểm F1 cho tập dữ liệu QQP. Bảng
15 và 16 trình bày độ lệch chuẩn của các kết quả
tương ứng trong Bảng 1, 2 và 12. Cuối cùng, Bảng
14 trình bày thước đo khớp chính xác cho các kết
quả tương ứng trong Bảng 3.
Nhiệm
vụBERT
BASEĐộ
thưa
thớtSoft
MvPoBERT
(của
chúng
tôi)oBERT
(của
chúng
tôi)
Epochs 10 Epochs 30 Epochs
SQuAD
EM81.2280%
90%
97%-
76.60
72.70-
80.76
76.1482.08
81.12
78.11
MNLI
mm-acc85.0680%
90%
97%-
81.80
80.10-
83.58
80.6784.91
84.35
82.01
QQP
F188.0080%
90%
97%-
86.80
85.50-
87.69
87.0588.63
88.30
87.66
Bảng 12: Thước đo đánh giá bổ sung cho kết quả
trình bày trong Bảng 1.
Nhiệm
vụBERT
BASEĐộ
thưa
thớtPrune
OFAoBERT
(của
chúng
tôi)
SQuAD
EM81.4290%
97%79.83
-81.43
76.90
MNLI
mm-acc85.0690%
97%82.43
-83.78
81.13
QQP
F188.0090%
97%87.72
-87.81
86.97
Bảng 13: Thước đo đánh giá bổ sung cho kết quả
trình bày trong Bảng 2.
A.9 Tăng tốc suy luận và tỷ lệ nén của các
mô hình nén
Chi tiết về kết quả hiển thị trong Hình 2 được
rút từ Bảng 17. Như hiển thị trong kết quả, không
phải tất cả mô hình nén phức hợp đều mang lại
cải thiện về suy luận hoặc nén so với hiệu suất
mô hình được giữ lại nhưng những mô hình làm
được cho phép cải thiện lớn.
Lớp Độ thưa thớt Không có cấu trúc Khối-4 +QAT
120%
80%
90%82.71
82.08
81.1282.71
81.46
80.1481.99
80.57
78.84
60%
80%
90%81.17
81.15
79.1681.17
79.55
77.6580.85
78.27
76.56
30%
80%
90%76.62
75.62
73.6176.62
74.07
71.3676.06
72.70
70.00
Bảng 14: Thước đo đánh giá bổ sung (khớp chính
xác) cho kết quả trình bày trong Bảng 3.
Nhiệm vụ Độ thưa thớtoBERT
(của chúng tôi)
Epochs 30 Epochs
SQuAD
F1, EM80%
90%
97%0.11, 0.03
0.13, 0.13
0.11, 0.17
MNLI
m, mm80%
90%
97%0.14, 0.13
0.05, 0.04
0.35, 0.22
QQP
acc, F180%
90%
97%0.08, 0.08
0.04, 0.06
0.05, 0.08
Bảng 15: Độ lệch chuẩn cho kết quả trình bày trong
Bảng 1 và 12.
Nhiệm vụ Độ thưa thớtoBERT
(của chúng tôi)
SQuAD
F1, EM90%
97%0.13, 0.13
0.03, 0.14
MNLI
m, mm90%
97%0.08, 0.24
0.17, 0.35
QQP
acc, F190%
97%0.06, 0.07
0.09, 0.18
Bảng 16: Độ lệch chuẩn cho kết quả trình bày trong
Bảng 2 và 13.
A.10 Nghiên Cứu NLP Có Trách Nhiệm -
Danh Sách Kiểm Tra Tái Tạo
Ngoài nhiều mục từ "Danh Sách Kiểm Tra Tái
Tạo" đã được giải quyết cẩn thận trong các phần
bài báo và Phụ lục, ở đây chúng tôi cung cấp các
chi tiết còn lại để tạo điều kiện tái tạo kết quả.
A.10.1 Hiện Vật Khoa Học
Tập dữ liệu. Thí nghiệm của chúng tôi sử dụng
các benchmark hiện có và được thiết lập tốt cho
tiền huấn luyện và tinh chỉnh của LLMs. Mỗi tập
dữ liệu được sử dụng mà không có bất kỳ hình
thức sửa đổi bổ sung nào. Do chúng tôi không
sửa đổi bất kỳ tập dữ liệu nào, chúng tôi không
kiểm tra nội dung cá nhân, nhạy cảm, hoặc xúc
phạm, cũng không thực hiện bất kỳ loại ẩn danh
nào. Để tiền huấn luyện, chúng tôi sử dụng Toronto
Book Corpus (TBC) (Zhu et al., 2015)⁷ và wikipedia.
20200501.en (Foundation)⁸. Để tinh chỉnh chúng
tôi sử dụng SQuAD v1.1 (Rajpurkar et al., 2016)⁹,
Tập Dữ Liệu Câu Hỏi Trùng Lặp Quora (QQP)
(Shankar, 2017)¹⁰, và tập dữ liệu Suy Luận Ngôn
Ngữ Tự Nhiên Đa Thể Loại (MNLI) (Williams et
al., 2018)¹¹. Tất cả các tập dữ liệu này có sẵn
công khai qua kho lưu trữ datasets HuggingFace
(Lhoest et al., 2021). Điều khoản sử dụng và chi
tiết thêm về mỗi tập dữ liệu có thể tìm thấy trong
các kho lưu trữ tương ứng.
Mô hình. Mô hình được sử dụng làm điểm khởi
đầu cho tất cả thí nghiệm là BERT BASE, có sẵn
công khai qua HuggingFace Hub¹². Tất cả mô hình
khác được trình bày trong bài báo này sẽ được
phát hành trong các kho lưu trữ có sẵn công khai
cùng với công thức nén, thước đo huấn luyện và
siêu tham số.
A.10.2 Thống Kê Tập Dữ Liệu
Thống kê tập dữ liệu được chi tiết trong Bảng 18.
A.10.3 Thí Nghiệm Tính Toán
Thượng nguồn. Tất cả lần chạy thượng nguồn
nói chung đắt đỏ về mặt tính toán do kích thước
lô lớn và tập dữ liệu khổng lồ. Trong thí nghiệm
chúng tôi sử dụng 4x A100 40GB NVIDIA GPUs.
Trong cấu hình này, một epoch huấn luyện mất
khoảng 6 giờ. Do chi phí của instance tính toán
lớn như vậy cao, các thí nghiệm này chỉ được chạy
với một seed duy nhất và không có khám phá
siêu tham số lớn.
Hạ nguồn. Thí nghiệm hạ nguồn của chúng tôi
sử dụng nhiều thẻ GPU khác nhau có sẵn: 16GB
V100, 11GB RTX 2080 Ti, và 24GB RTX 3090.
Mỗi epoch huấn luyện mất khoảng 30 phút, và
kết quả là các lần chạy 30 epoch mất khoảng 15
giờ. Cho các thí nghiệm này, chúng tôi báo cáo
kết quả trung bình của ba lần chạy với các seed
ngẫu nhiên khác nhau.
Suy luận DeepSparse. Chúng tôi ghép các mô
hình nén với DeepSparse (NeuralMagic, 2021)
một công cụ suy luận CPU nhận biết độ thưa thớt
có sẵn công khai. Runtime CPU này có thể tận
dụng cả độ thưa thớt có cấu trúc và không có cấu
trúc, và lượng tử hóa để cung cấp hiệu suất cao
trên CPUs thông thường. Chúng tôi chạy DeepSparse
trên máy chủ AWS c5.12xlarge Intel 24-core với
24 lõi, 96 vCPUs, 192 GB RAM và tập lệnh tương
thích AVX-512. Tất cả mô hình được xuất sử dụng
định dạng ONNX¹³ tiêu chuẩn.
A.10.4 Gói Tính Toán
Thí nghiệm của chúng tôi xây dựng trên các thư
viện có sẵn công khai để đảm bảo dễ dàng tái tạo
và khả năng mở rộng. Tất cả triển khai, mã huấn
luyện và đánh giá được xây dựng trên thư viện
Transformers¹⁴ và Datasets¹⁵ của HuggingFace,
thư viện SparseML¹⁶ của NeuralMagic cho nén
mô hình, và công cụ DeepSparse¹⁷ cho suy luận
hiệu quả trên CPUs thông thường.
⁷https://huggingface.co/datasets/bookcorpus
⁸https://huggingface.co/datasets/wikipedia
⁹https://huggingface.co/datasets/squad
¹⁰https://huggingface.co/datasets/glue
¹¹https://huggingface.co/datasets/glue
¹²https://huggingface.co/bert-base-uncased
¹³https://onnx.ai/
¹⁴https://github.com/huggingface/transformers
¹⁵https://github.com/huggingface/datasets
¹⁶https://github.com/neuralmagic/sparseml
¹⁷https://github.com/neuralmagic/deepsparse
19

--- TRANG 14 ---
Bảng 8: Kết quả suy luận MLPerf cho mô hình BERT LARGE và MobileBERT nén oBERT.
Mô hình Độ chính xácĐiểm F1
(R=X% phục hồi)Kích thước tệpTỷ lệ
nénThông lượng
(mẫu/giây)Tăng tốc
BERT-Large
đường cơ sở dày đặcFP32 90.87 (R=100%) 1.30 GB 1x 15.49 1x
oBERT-Large INT8 90.21 (R=99.27%) 38.20 MB 34x 230.74 15x
oBERT-MobileBERT INT8 90.32 (R=99.39%) 9.56 MB 136x 928.58 60x
50 60 70
Độ thưa thớt (%)102030405060708090Điểm F1
BERTBASE
B = 50
B = 5k
B = 500k
M-FAC
Hình 3: Nghiên cứu loại bỏ tỉa cắt một lần đối với
kích thước khối (B), với m = 1024 và λ = 10^-7,
trên mô hình BERT BASE và tập dữ liệu trả lời câu
hỏi SQuAD v1.1. M-FAC là xấp xỉ Hessian nghịch
đảo đầy đủ (Frantar et al., 2021).
50 60 70
Độ thưa thớt (%)838485868788Điểm F1
BERTBASE
m = 128
m = 512
m = 1024
Hình 4: Nghiên cứu loại bỏ tỉa cắt một lần đối với
số gradient (m), với B = 50 và λ = 10^-7, trên mô
hình BERT BASE và tập dữ liệu trả lời câu hỏi SQuAD
v1.1.
50 60 70
Độ thưa thớt (%)788082848688Điểm F1
BERTBASE
λ=10^-6
λ=10^-8
λ=10^-7
Hình 5: Nghiên cứu loại bỏ tỉa cắt một lần đối với
giảm chấn (λ), với m = 1024 và B = 50, trên mô
hình BERT BASE và tập dữ liệu trả lời câu hỏi SQuAD
v1.1.
task theo siêu tham số mặc định cho SQuAD² và
GLUE (QQP và MNLI)³.
Thiết lập tỉa cắt. Trong Bảng 9 chúng tôi mô tả
chi tiết tất cả siêu tham số cho kết quả tỉa cắt hạ
nguồn được trình bày trong Bảng 1 và 3. Để hiểu
dễ hơn, chúng tôi cũng trực quan hóa lịch trình
tỷ lệ học trong Hình 6 và 8, và lịch trình độ thưa
thớt trong Hình 7 và 9.
Mô hình 3-, 6-lớp. Chúng tôi chuẩn bị các mô
hình 3 và 6 lớp cho các lần chạy hạ nguồn trong
hai giai đoạn: giai đoạn thả lớp và huấn luyện lại.
Chúng tôi thả lớp từ mô hình giáo viên thượng
nguồn (chi tiết hơn trong Phụ lục A.6). Sau khi
thả, chúng tôi huấn luyện lại các lớp còn lại, theo
hiểu biết từ (Turc et al., 2019), trong cùng thiết
lập được sử dụng để chuẩn bị giáo viên thượng
nguồn với bổ sung KD từ nó.
²https://github.com/huggingface/transformers/tree/main/
examples/pytorch/question-answering
³https://github.com/huggingface/transformers/tree/main/
examples/pytorch/text-classification
14

--- TRANG 15 ---
Tập dữ liệu Huấn luyện Đánh giá
SQuAD (ví dụ) 87599 10570
MNLI (ví dụ) 392702 19628
QQP (ví dụ) 363,846 40,430
Wikipedia (từ) 6078422 -
TBC (từ) 74004228 -
Bảng 18: Thống kê cho tập dữ liệu huấn luyện và
đánh giá
và tập dữ liệu khổng lồ. Trong thí nghiệm chúng
tôi sử dụng 4x A100 40GB NVIDIA GPUs. Trong
cấu hình này, một epoch huấn luyện mất khoảng
6 giờ. Do chi phí của instance tính toán lớn như
vậy cao, các thí nghiệm này chỉ được chạy với
một seed duy nhất và không có khám phá siêu
tham số lớn.
Hạ nguồn. Thí nghiệm hạ nguồn của chúng tôi
sử dụng nhiều thẻ GPU khác nhau có sẵn: 16GB
V100, 11GB RTX 2080 Ti, và 24GB RTX 3090.
Mỗi epoch huấn luyện mất khoảng 30 phút, và
kết quả là các lần chạy 30 epoch mất khoảng 15
giờ. Cho các thí nghiệm này, chúng tôi báo cáo
kết quả trung bình của ba lần chạy với các seed
ngẫu nhiên khác nhau.
Suy luận DeepSparse. Chúng tôi ghép các mô
hình nén với DeepSparse (NeuralMagic, 2021)
một công cụ suy luận CPU nhận biết độ thưa thớt
có sẵn công khai. Runtime CPU này có thể tận
dụng cả độ thưa thớt có cấu trúc và không có cấu
trúc, và lượng tử hóa để cung cấp hiệu suất cao
trên CPUs thông thường. Chúng tôi chạy DeepSparse
trên máy chủ Intel AWS c5.12xlarge 24-core với
24 lõi, 96 vCPUs, 192 GB RAM và tập lệnh tương
thích AVX-512. Tất cả mô hình được xuất sử dụng
định dạng ONNX¹³ tiêu chuẩn.
A.10.4 Gói Tính Toán
Thí nghiệm của chúng tôi xây dựng trên các thư
viện có sẵn công khai để đảm bảo dễ dàng tái tạo
và khả năng mở rộng. Tất cả triển khai, mã huấn
luyện và đánh giá được xây dựng trên thư viện
Transformers¹⁴ và Datasets¹⁵ của HuggingFace,
thư viện SparseML¹⁶ của NeuralMagic cho nén
mô hình, và công cụ DeepSparse¹⁷ cho suy luận
hiệu quả trên CPUs thông thường.
¹³https://onnx.ai/
¹⁴https://github.com/huggingface/transformers
¹⁵https://github.com/huggingface/datasets
¹⁶https://github.com/neuralmagic/sparseml
¹⁷https://github.com/neuralmagic/deepsparse
19

--- TRANG 16 ---
Lớp Độ thưa thớt
(%)Phương pháp
nénĐiểm F1 Hồi tưởng F1
(%)Thông lượng
(mục per giây)Tăng tốc
DeepSparseKích thước mô hình
(gzip MB)Tỷ lệ nén (w.r.t. gzip)
12 0 không 88.54 100.00 65.81 1.00 384.7 1.00
12 80 không có cấu trúc 89.04 100.56 222.66 3.38 173.1 2.22
12 90 không có cấu trúc 88.31 99.74 292.40 4.44 140.1 2.75
12 80 khối-4+QAT 87.89 99.26 552.22 8.39 37.8 10.18
6 80 không có cấu trúc 88.20 99.62 419.68 6.38 128.3 3.00
6 90 không có cấu trúc 86.78 98.01 663.02 10.07 111.8 3.44
6 80 khối-4+QAT 86.10 97.24 989.54 15.04 26.2 14.70
3 80 không có cấu trúc 84.08 94.96 737.62 11.21 105.9 3.63
3 90 không có cấu trúc 82.50 93.18 974.00 14.80 97.7 3.94
3 80 khối-4+QAT 82.04 92.66 1892.27 28.75 20.3 18.92
Bảng 17: Tác động nén lên kích thước mô hình và tốc độ suy luận, được đánh giá với kích thước lô 32 và độ
dài chuỗi 128 trên tập dữ liệu SQuAD v1.1. Được đánh giá tại instance AWS c5.12xlarge.
18

--- TRANG 17 ---
19
