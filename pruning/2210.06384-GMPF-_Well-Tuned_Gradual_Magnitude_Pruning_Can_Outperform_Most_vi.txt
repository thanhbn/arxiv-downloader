# 2210.06384.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2210.06384.pdf
# Kích thước tệp: 333821 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
GMPF: Phương pháp Tỉa Dần Theo Độ Lớn Được Điều Chỉnh Tốt Có Thể Vượt Trội Hầu Hết
Các Phương Pháp Tỉa BERT
Eldar Kurtic1và Dan Alistarh1,2
1Viện Khoa học và Công nghệ Áo
2Neural Magic Inc.
Tóm tắt
Chúng tôi xem xét lại hiệu suất của phương pháp cơ bản tỉa dần theo độ lớn (GMP) cổ điển cho các mô hình ngôn ngữ lớn, tập trung vào benchmark BERT cổ điển trên các tác vụ phổ biến khác nhau. Mặc dù có bằng chứng trong tài liệu cho thấy GMP hoạt động kém, chúng tôi chỉ ra rằng một biến thể đơn giản và tổng quát, mà chúng tôi gọi là GMPF, có thể bằng và đôi khi vượt trội hơn các phương pháp phức tạp hiện đại nhất. Kết quả của chúng tôi cung cấp một baseline đơn giản nhưng mạnh mẽ cho các công trình tương lai, nêu bật tầm quan trọng của việc điều chỉnh tham số cho các baseline, và thậm chí cải thiện hiệu suất của phương pháp tỉa bậc hai hiện đại nhất trong thiết lập này.

1 Giới thiệu
Sự tăng trưởng gần đây rất lớn về chi phí tính toán của các mô hình học sâu chính xác, đặc biệt là các mô hình ngôn ngữ lớn (LLM), đã thúc đẩy việc phát triển một số kỹ thuật nén mô hình tiên tiến (Hoeﬂer et al., 2021; Gholami et al., 2021), bao gồm tỉa có cấu trúc và không có cấu trúc, lượng tử hóa, và chưng cất kiến thức. Trong bài báo này, chúng tôi tập trung vào tỉa không có cấu trúc, mà chúng tôi tuân theo quy trình tiêu chuẩn. Các mô hình như vậy đầu tiên được tiền huấn luyện trên một corpus lớn không có nhãn ở phía trước. Sau đó, chúng được tinh chỉnh theo cách có giám sát trên một tác vụ hạ nguồn nhỏ hơn, chẳng hạn như hỏi đáp hoặc phân loại văn bản. Trong bối cảnh nén, quy trình này dẫn đến hai mô hình: 1) tỉa phía trên, theo sau bởi tinh chỉnh các trọng số còn lại trên một tác vụ hạ nguồn, và 2) tỉa phía dưới, tỉa và tinh chỉnh trực tiếp trên tác vụ hạ nguồn.

Một phương pháp baseline hấp dẫn trong hầu hết các thiết lập là tỉa dần theo độ lớn (GMP) (Hagiwara, 1994; Zhu và Gupta, 2017), tức là định kỳ loại bỏ phần nhỏ nhất của các trọng số trong quá trình huấn luyện, có thể xen kẽ với các bước tinh chỉnh được thiết kế để phục hồi độ chính xác. GMP đã được chỉ ra là một baseline cực kỳ mạnh trong bối cảnh thị giác máy tính (Gale et al., 2019; Hoeﬂer et al., 2021). Tuy nhiên, tài liệu về tỉa LLM, và đặc biệt là các mô hình BERT (Sanh et al., 2020; Chen et al., 2020; Zafrir et al., 2021), rõ ràng cho thấy rằng GMP không hoạt động tốt.

Đóng góp. Trong bài báo này, chúng tôi xem xét lại kết luận này và điều tra xem liệu GMP có thể là một baseline cạnh tranh, một khi được điều chỉnh cẩn thận. Cụ thể, chúng tôi chỉ ra rằng một biến thể được điều chỉnh tốt mà chúng tôi gọi là GMPF, có thể tạo ra các mô hình ngôn ngữ có độ chính xác cao và thưa thớt trong cả hai chế độ tỉa phía trên và phía dưới, bằng hoặc thậm chí vượt trội hơn các phương pháp phức tạp hơn. Chúng tôi khám phá tác động của các tham số quan trọng cho tỉa dần, và cung cấp các hướng dẫn đơn giản và trực quan về cách tích hợp chúng một cách có nguyên tắc.

--- TRANG 2 ---
Kết quả của chúng tôi được tóm tắt trong Hình 1, trình bày hiệu suất của các kỹ thuật tỉa không có cấu trúc hiện đại nhất trên hai benchmark. Cụ thể, chúng tôi so sánh GMPF với phương pháp Lottery Ticket (Chen et al., 2020), Movement Pruning (MvP) (Sanh et al., 2020) (cũng như baseline GMP của nó GMPMvP), upstream Prune OFA (Zafrir et al., 2021), cũng như phương pháp tỉa bậc hai được đề xuất gần đây oBERT (Kurtic et al., 2022). Chúng tôi quan sát thấy rằng: 1) đối với cả hai benchmark, GMPF chỉ đứng thứ hai sau phương pháp oBERT phức tạp hơn; 2) GMPF thực tế vượt trội hơn các phương pháp cạnh tranh cao Prune OFA và MvP; và 3) GMPF vượt trội hơn cả Lottery Tickets và GMPMvP với biên độ cực kỳ lớn.

Công trình trước đây. Theo tài liệu tỉa BERT rộng lớn, chúng tôi tập trung vào tỉa không có cấu trúc của mô hình BERT BASE (Devlin et al., 2019). Như đã lưu ý trước đây, các mô hình tỉa upstream và downstream tồn tại, và các phương pháp thường được phát triển và chuyên môn hóa chỉ cho một trong hai. Ví dụ, Movement Pruning (MvP) (Sanh et al., 2020; Lagunas et al., 2021) cho tỉa downstream và Prune Once for All (Prune OFA) (Zafrir et al., 2021) cho tỉa upstream. Tính đơn giản và tổng quát của GMP làm cho nó phù hợp cho cả hai mô hình, mà không cần bất kỳ sửa đổi cụ thể nào theo chế độ. Các kỹ thuật tỉa mới và tiên tiến hơn, trái ngược với GMP, có thể tận dụng gradient (Sanh et al., 2020; Lagunas et al., 2021), độ cong mất mát (Kurtic et al., 2022), thiết lập tiền huấn luyện tốn nhiều tính toán (Zafrir et al., 2021) được xây dựng trên tiền đề rằng phương pháp GMP đơn giản dựa trên độ lớn sẽ thất bại khi áp dụng cho tỉa BERT. Trong công trình này, trái ngược với những gì hiện có trong tài liệu, chúng tôi trình bày bằng chứng thực nghiệm rằng GMP, khi được điều chỉnh cẩn thận, có thể tạo ra các mô hình thưa thớt rất chính xác có tính cạnh tranh hoặc thậm chí tốt hơn hầu hết các kỹ thuật tỉa hiện đại nhất trên cả hai chế độ (upstream và downstream). Như có thể thấy từ Hình 1 và các kết quả sau này của chúng tôi, chúng tôi cải thiện rất lớn so với các baseline tỉa dựa trên GMP hiện có, trong một số trường hợp thậm chí hơn 20 điểm độ chính xác.

2 Tỉa Dần Theo Độ Lớn Cạnh Tranh (GMPF)
Thiết lập thí nghiệm. Chúng tôi tập trung sự chú ý vào mô hình BERT BASE tiêu chuẩn, bao gồm các lớp nhúng và mã hóa, có khoảng 110M tham số. Tất cả các phương pháp tập trung vào tỉa trong khoảng 85M trọng số của các lớp mã hóa và báo cáo độ thưa thớt đối với số đó. Chúng tôi đánh giá các mô hình trên phần validation của tập dữ liệu tương ứng, và để cải thiện độ tin cậy trong các kết quả thu được, chúng tôi thực hiện nhiều lần chạy với các seed khác nhau và báo cáo hiệu suất trung bình.

2.1 Tỉa downstream
Theo tài liệu, chúng tôi xem xét ba tác vụ phổ biến: hỏi đáp SQuADv1.1 (Rajpurkar et al., 2016), nhận diện suy luận văn bản MNLI (Williams et al., 2017), và phát hiện câu hỏi trùng lặp QQP (Iyer et al., 2017). Bây giờ, chúng tôi suy ngẫm về các thành phần quan trọng nhất của framework tỉa dần đã cho phép chúng tôi đạt được những cải thiện lớn.

Lịch trình độ thưa thớt. Trong tất cả các lần chạy dần của chúng tôi, không có tỉa trong hai epoch đầu và hai epoch cuối. Phần trước tinh chỉnh mô hình được tiền huấn luyện, và phần sau tinh chỉnh mô hình thưa thớt với mask cố định. Ở giữa hai phần, GMPF tuân theo bộ lập lịch độ thưa thớt dạng khối (Zhu và Gupta, 2017) và tỉa trọng số với tần suất mười lần mỗi epoch. Được thúc đẩy bởi thực tế rằng BERT BASE được tham số hóa quá mức cho các tác vụ downstream, chúng tôi chệch khỏi lịch trình khối tiêu chuẩn bằng cách giới thiệu một bước tỉa đầu tiên lớn. Điều này được chỉ ra là có tầm quan trọng quyết định khi tỉa mô hình đến độ thưa thớt mục tiêu cao (ví dụ 97%) vì nó để lại nhiều thời gian hơn để phục hồi từ các bước tỉa sau này khó khăn hơn nhiều. Trong Bảng 8 chúng tôi báo cáo kết quả từ một nghiên cứu ablation đối với kích thước của bước ban đầu. Để thuận tiện, chúng tôi hình dung bộ lập lịch độ thưa thớt trong Hình 2. Các thí nghiệm sơ bộ của chúng tôi cho thấy hiệu suất tương tự giữa phân phối độ thưa thớt đồng nhất và toàn cầu, vì vậy chúng tôi sử dụng phần trước.

Lịch trình tỷ lệ học. Mục tiêu của chúng tôi là cung cấp một thiết lập baseline đơn giản hoạt động tốt trên phạm vi rộng các tập dữ liệu mà không cần bất kỳ điều chỉnh phụ thuộc tác vụ bổ sung nào. Hiện tại, các bài báo hoặc báo cáo kết quả tốt nhất sau một tìm kiếm siêu tham số mở rộng cho mỗi tác vụ, ví dụ Zafrir et al. (2021), hoặc họ sử dụng các bộ lập lịch được tạo ra cẩn thận cho mỗi thiết lập độc lập có thể bao gồm các giai đoạn khởi động với và không có rewind (Sanh et al., 2020; Kurtic et al., 2022). Điều này có thể dẫn đến chuyên môn hóa cao đối với tác vụ/mô hình mục tiêu, điều không mong muốn trong thực tế và làm cho việc phân biệt lợi ích từ chính kỹ thuật tỉa trở nên khó khăn. Chúng tôi đề xuất đơn giản sao chép lịch trình tinh chỉnh 2-epoch tiêu chuẩn (Devlin et al., 2019) bằng một hệ số nhất định và xen kẽ nó với các bước tỉa. Để so sánh công bằng với Sanh et al. (2020) chúng tôi sao chép nó bằng hệ số 5, tái tạo thiết lập 10-epoch của họ. Và để so sánh công bằng với Chen et al. (2020) chúng tôi sao chép nó bằng hệ số 15, tái tạo thiết lập 30-epoch của họ. Để thuận tiện, chúng tôi hình dung lịch trình tỷ lệ học trong Hình 2. Trong phụ lục F, chúng tôi mô tả kết quả với các bộ lập lịch khác không hoạt động.

Độ cứng Chưng cất Kiến thức (KD). Chúng tôi tận dụng KD (Hinton et al., 2015) của các đầu ra từ một giáo viên dày đặc được tinh chỉnh. KD là một thực hành tiêu chuẩn khi tỉa, ví dụ (Sanh et al., 2020; Zafrir et al., 2021; Xu et al., 2021). Hàm mất mát được công thức hóa như một tổ hợp tuyến tính của mất mát tiêu chuẩn liên quan đến tác vụ cụ thể (ví dụ cross-entropy cho phân loại LCE) và độ phân kỳ Kullback-Leibler (LKL) giữa phân phối đầu ra của mô hình dày đặc (giáo viên) và mô hình thưa thớt (học sinh) dưới dạng: L = (1-h)LCE + hLKL. Tỷ lệ giữa hai được kiểm soát với siêu tham số độ cứng h. Để xác định giá trị tối ưu của nó ở độ thưa thớt cao, chúng tôi chạy một nghiên cứu ablation được báo cáo trong Bảng 10, và chấp nhận độ cứng h = 1.

Nhiệt độ Chưng cất Kiến thức. Nhiệt độ T là một siêu tham số KD bổ sung cần điều chỉnh phù hợp, vì nó kiểm soát "độ mềm" của phân phối đầu ra. Trong tài liệu tỉa

--- TRANG 3 ---
[Hình 3: Phân phối đầu ra của giáo viên ở nhiệt độ thường sử dụng T∈{1.0;2.0} và nhiệt độ được đề xuất T=5.5.]

nó là tiêu chuẩn để sử dụng các giá trị T=1 hoặc T=2 "mạnh hơn" (Xu et al., 2021; Zafrir et al., 2021; Sanh et al., 2020; Lagunas et al., 2021; Kurtic et al., 2022); chúng tôi xem xét lại điều này bằng cách hình dung phân phối đầu ra của giáo viên để có cái nhìn sâu sắc về những gì học sinh thưa thớt đang học. Trong Hình 3, chúng tôi hình dung các phân phối được tạo ra cho các mẫu được chọn ngẫu nhiên từ tác vụ SQuADv1.1 được làm mềm với ba giá trị của nhiệt độ. Như có thể thấy, sự tự tin cao của giáo viên trong việc dự đoán lớp chính xác ở nhiệt độ thường sử dụng T∈{1.0;2.0} làm cho chưng cất kiến thức gần như lỗi thời. Được thúc đẩy bởi quan sát này, chúng tôi chạy một nghiên cứu ablation cho nhiều nhiệt độ cao hơn và báo cáo một phần kết quả trong Bảng 11. Dựa trên kết quả, chúng tôi chấp nhận nhiệt độ T=5.5.

2.1.1 GMPF so với các baseline dựa trên GMP khác
Do hạn chế về không gian, chúng tôi tổng hợp tất cả các cải tiến đã phân tích trước đây trong một công thức tỉa downstream và trình bày nó chi tiết trong Phụ lục B. Chúng tôi so sánh GMPF được tối ưu hóa của chúng tôi với các kết quả GMP khác được báo cáo trong tài liệu tỉa. Để so sánh công bằng, chúng tôi xem xét cả hai thiết lập, 10 và 30-epoch. Trong thiết lập 10-epoch, chúng tôi so sánh với các baseline GMP được báo cáo trong Sanh et al. (2020) và gọi chúng là GMPMvP. Trong thiết lập 30-epoch, chúng tôi so sánh với các kết quả tốt nhất được báo cáo trong Chen et al. (2020), thu được hoặc thông qua GMP hoặc thông qua phương pháp Lottery Ticket (LTH), và gọi chúng là GMPLTH. Như có thể thấy từ Bảng 1, GMPF của chúng tôi vượt trội đáng kể so với tất cả các kết quả khác; trong một số trường hợp những cải thiện là hơn 20 điểm!

2.1.2 GMPF so với các kỹ thuật tỉa tiên tiến
Bây giờ, chúng tôi muốn so sánh GMPF của chúng tôi với các phương pháp dựa trên thông tin bậc cao để đưa ra quyết định tỉa

--- TRANG 4 ---
[Bảng 2: So sánh tỉa downstream của GMPF với các kỹ thuật tỉa tiên tiến.]

như gradient trong MvP (Sanh et al., 2020) và độ cong mất mát trong oBERT (Kurtic et al., 2022). Cả hai đều áp đặt chi phí tính toán cao hơn so với tỉa dựa trên độ lớn, nhưng chúng tôi vẫn đặt kết quả của chúng tôi trong bối cảnh so với của họ để nắm bắt hoàn toàn phạm vi cải thiện được giới thiệu bởi các tối ưu hóa cẩn thận của GMP. Như có thể thấy từ kết quả trong Bảng 2, GMPF có thể cải thiện hiệu suất của Movement Pruning trong 4 trên 6 cấu hình được phân tích, nhưng không may không thể bằng hiệu suất của phương pháp oBERT. Ngoài những so sánh này, chúng tôi sử dụng implementation mã nguồn mở của oBERT, phương pháp tỉa BERT hiện đại nhất hiện tại, và chạy nó với các siêu tham số được tối ưu hóa từ GMPF trên tác vụ SQuADv1.1. Chúng tôi gọi những kết quả này là oBERTF. Như có thể thấy từ Bảng 2, thậm chí kết quả oBERT rất cạnh tranh cũng được hưởng lợi từ thiết lập GMPF. Đối với tất cả các lần chạy GMPF, chúng tôi báo cáo hiệu suất trung bình trên ba lần chạy với seed khác nhau, và các metrics bổ sung trong Bảng 5 và 6.

2.2 Tỉa upstream
Để xác thực thiết lập GMPF được tối ưu hóa được giới thiệu trong phần trước, chúng tôi áp dụng nó bây giờ vào giai đoạn tiền huấn luyện của LLM. Đây là một quá trình hai giai đoạn. Trong giai đoạn đầu tiên, mô hình BERT BASE được tỉa trong quá trình tiền huấn luyện và sau đó, trong giai đoạn thứ hai, các trọng số còn lại được tinh chỉnh với mask cố định trên một tác vụ downstream cụ thể để đánh giá hiệu suất. Do chi phí cao của việc thí nghiệm trong giai đoạn tiền huấn luyện, chúng tôi sử dụng giáo viên dày đặc được mã nguồn mở bởi Kurtic et al. (2022). Do hạn chế về không gian, chúng tôi tóm tắt tất cả siêu tham số trong một công thức tỉa upstream

[Bảng 3: So sánh tỉa upstream của GMPF với các baseline dựa trên GMP khác và các kỹ thuật tỉa tiên tiến hơn.]

và trình bày nó chi tiết trong Phụ lục C. Trong Bảng 3 chúng tôi trình bày kết quả thu được trong thiết lập này và so sánh với các phương pháp khác đang sử dụng cùng phương pháp. Cụ thể hơn, chúng tôi so sánh với Lottery Ticket (Chen et al., 2020), Prune OFA (Zafrir et al., 2021), và The Optimal BERT Surgeon (oBERT) (Kurtic et al., 2022). Ngoài ra, chúng tôi báo cáo các baseline GMP thu được trong công trình Prune OFA và gọi chúng là GMPPrune OFA. Như có thể thấy từ Bảng 3, GMPF vượt trội đáng kể so với GMPPrune OFA, Lottery Tickets và thậm chí Prune OFA, và đến rất gần hiệu suất của oBERT. Đối với tất cả các lần chạy GMPF, chúng tôi báo cáo hiệu suất trung bình trên bốn lần chạy với seed khác nhau. Những kết quả này xác nhận các phát hiện từ phần trước và thiết lập GMPF như một baseline cực kỳ cạnh tranh trong tất cả các chế độ.

3 Kết luận
Trong công trình này, chúng tôi đã trình bày một tập hợp các cập nhật cho thiết lập tỉa dần tiêu chuẩn cho các mô hình BERT đã cho phép chúng tôi đạt được kết quả rất cạnh tranh với bộ tỉa độ lớn đơn giản. Những kết quả này vượt trội, với biên độ đáng kể, tất cả kết quả dựa trên độ lớn hiện có trong tài liệu tỉa đã được sử dụng làm baseline cho việc phát triển và đánh giá các kỹ thuật tỉa mới và tiên tiến hơn. Chúng tôi hy vọng rằng những baseline mới này sẽ giúp cộng đồng bắt đầu từ một tập hợp kết quả cạnh tranh khi nén các mô hình ngôn ngữ lớn. Hơn nữa, GMPF của chúng tôi thậm chí đã vượt trội một số kết quả thu được với các kỹ thuật tỉa tiên tiến và nặng về tính toán hơn. Tại thời điểm này, chúng tôi muốn nhấn mạnh mạnh mẽ rằng những kết quả này không nên được hiểu là bằng chứng rằng tỉa độ lớn tốt hơn các phương pháp tiên tiến khác. Thay vào đó, chúng nên được hiểu là bằng chứng rằng kết quả hiện tại của họ có thể được hưởng lợi đáng kể từ các cập nhật của thiết lập dần được trình bày trong trường hợp sử dụng GMPF. Để hỗ trợ tuyên bố này, chúng tôi đã chạy bộ tỉa oBERT hiện đại nhất với thiết lập GMPF và đã cải thiện kết quả của nó với biên độ không tầm thường.

4 Hạn chế
Như bất kỳ nghiên cứu học thuật nào, công trình của chúng tôi không phải là không có hạn chế. Theo tài liệu, các nghiên cứu thực nghiệm mở rộng của chúng tôi chỉ được tiến hành trên mô hình BERT BASE tiêu chuẩn, cho chúng tôi cơ hội so sánh với một lượng lớn các kỹ thuật tỉa khác nhau. Trong suốt tài liệu, mô hình này nổi lên như một benchmark nhất quán cho các phương pháp tỉa không có cấu trúc. Tuy nhiên, kết quả hiện tại không trực tiếp ngụ ý rằng các phát hiện của chúng tôi sẽ có thể áp dụng tổng quát cho các mô hình ngôn ngữ khác. Để một phần lấp đầy khoảng trống không chắc chắn này, chúng tôi tiến hành một vài thí nghiệm trên mô hình BERT LARGE lớn gấp ba lần và báo cáo kết quả trong Phụ lục A. Một hạn chế khác mà chúng tôi nhắm đến loại bỏ trong công việc tương lai là tập trung vào loại độ thưa thớt không có cấu trúc chi tiết, và khám phá các biến thể khác như tỉa bán cấu trúc và có cấu trúc.

[Phần Tài liệu tham khảo và các Phụ lục tiếp theo...]

--- TRANG 5 ---
[Tiếp tục với nội dung các phụ lục và bảng kết quả chi tiết...]

--- TRANG 6 ---
[Tiếp tục với các bảng kết quả và phân tích bổ sung...]

--- TRANG 7 ---
[Các nghiên cứu ablation và kết quả thực nghiệm chi tiết...]

--- TRANG 8 ---
[Các bảng kết quả cuối cùng và phân tích lịch trình học...]
