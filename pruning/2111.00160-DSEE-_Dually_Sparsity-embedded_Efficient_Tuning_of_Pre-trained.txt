# 2111.00160.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2111.00160.pdf
# File size: 1018725 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained
Language Models
Xuxi Chen1, Tianlong Chen1, Weizhu Chen2, Ahmed Hassan Awadallah2
Zhangyang Wang1,Yu Cheng2
1University of Texas at Austin,2Microsoft Corporation
{xxchen,tianlong.chen,atlaswang}@utexas.edu
{wzchen,hassanam,yu.cheng}@microsoft.com
Abstract
Gigantic pre-trained models have become cen-
tral to natural language processing (NLP), serv-
ing as the starting point for fine-tuning to-
wards a range of downstream tasks. How-
ever, two pain points persist for this paradigm:
(a) as the pre-trained models grow bigger
(e.g., 175B parameters for GPT-3), even the
fine-tuning process can be time-consuming
and computationally expensive; (b) the fine-
tuned model has the same size as its start-
ing point by default, which is neither sensible
due to its more specialized functionality, nor
practical since many fine-tuned models will
be deployed in resource-constrained environ-
ments. To address these pain points, we pro-
pose a framework for resource- and parameter-
efficient fine-tuning by leveraging the sparsity
prior in both weight updates and the final model
weights. Our proposed framework, dubbed
Dually Sparsity- Embedded Efficient Tuning
(DSEE), aims to achieve two key objectives:
(i)parameter efficient fine-tuning - by enforc-
ing sparsity-aware low-rank updates on top
of the pre-trained weights; and (ii) resource-
efficient inference - by encouraging a sparse
weight structure towards the final fine-tuned
model. We leverage sparsity in these two di-
rections by exploiting both unstructured and
structured sparse patterns in pre-trained lan-
guage models via a unified approach. Exten-
sive experiments and in-depth investigations,
with diverse network backbones (i.e., BERT,
RoBERTa, and GPT-2) on dozens of datasets,
consistently demonstrate impressive parameter-
/inference-efficiency, while maintaining com-
petitive downstream performance. For instance,
DSEE saves about 25% inference FLOPs while
achieving comparable performance, with 0.5%
trainable parameters on BERT. Codes are avail-
able at https://github.com/VITA-Group/
DSEE .
1 Introduction
Most recent NLP applications have been following
the pre-train then fine-tune paradigm, starting froma gigantic pre-trained model and fine-tuning it to-
wards downstream tasks. Conventional fine-tuning
works through updating all of the parameters in
the pre-trained model. However, as the size of
pre-trained models grows, updating all parameters
becomes less feasible in most practical scenarios,
due to the expensive memory and computational
requirements. For example, BERT BASE (Devlin
et al., 2019) has 110M trainable parameters, while
GPT-2 (Radford et al., 2019) has up to 1.5B and
the largest version of GPT-3 (Radford et al., 2019)
has an astonishing 175B trainable parameters. As
such, conventional fine-tuning of the larger models
could require hundreds of GPU hours. Another
downside of this paradigm is that it requires stor-
ing as many parameters as in the large-scale pre-
trained models for each downstream task, which
poses impediments to the deployment in real-world
resource-constrained environments.
One solution to address the extensive resource
requirement of conventional fine-tuning is model
pruning (LeCun et al., 1990; Han et al., 2015; Ren
et al., 2018; He et al., 2017; Liu et al., 2017), where
unnecessary weights are eliminated to shrink the
model size. For example, Chen et al. (2021b) lever-
agesℓ1regularization to remove insignificant atten-
tion heads and gains 35∼45% training time with
comparable performance; Chen et al. (2021a); Dao
et al. (2022) leverage sparse matrices with fixed
structures to reduce pretrained models’ sizes. All
these studies indicate the rise of sparsity naturally
during fine-tuning a general-purpose pre-trained
model, to some specialized downstream function-
ality. One potential interpretation, of why sparsity
arises, is that different subsets of the parameters
may be responsible for different downstream tasks
and data domains (Sanh et al., 2020). However,
identifying appropriate sparse masks can be bur-
densome: fine-tuning a large pre-trained language
model like GPT-3 for just one step consumes at
least 1.2TB of VRAM and requires 96pieces ofarXiv:2111.00160v3  [cs.LG]  24 May 2023

--- PAGE 2 ---
NVIDIA Tesla (Hu et al., 2021), and these meth-
ods either require access to pre-trained weights or
introduce additional learnable coefficients (such as
importance scores of attention heads).
A parallel alternative is to design parameter-
efficient fine-tuning algorithms, which aim at op-
timizing a small portion of weights while fixing
most of them when fine-tuning on downstream
tasks. Pioneering works along this line, which
utilize adapters (Houlsby et al., 2019), learnable
embeddings (Li and Liang, 2021; Liu et al., 2021),
low-rank decomposition (Hu et al., 2021) or their
combination (He et al., 2021), can significantly re-
duce the number of trainable parameters while pre-
serving good fine-tuning performance. Although
these methods can substantially improve the stor-
age and deployment efficiency of models, there
are two major hurdles: ( i) they does not yield
any inference efficiency gains since the full pre-
trained weights are still required to calculate out-
puts; and ( ii) current methods assume the updates
on pretrained weights to be either sparse (Guo et al.,
2020) or low-rank (Hu et al., 2021), yet those as-
sumptions might be oversimplified (Yu et al., 2017)
and overly restricted to allow for effective updates.
These observations have inspired us to explore bet-
ter parameter-efficiency methods.
To improve both resource- and parameter-
efficiency during model fine-tuning, we explicitly
draw on the prior of sparsity for both weight up-
dates and the final weights , and establish a dually
sparsity-embedding efficient tuning (DSEE ) frame-
work. Starting from a pre-trained model, DSEE
first adopts a sparsity-aware low-rank weight up-
date to achieve parameter-efficiency of the fine-
tuning process; and then enforces a sparse weight
structure directly from weight updates by mask-
ing to achieve resource-efficiency of the fine-tuned
model at inference time. Our contributions can be
summarized as follows:
•We propose the dually sparsity-embedding ef-
ficient tuning, which unifies sparsity-aware
parameter-efficient weight update and sparse pre-
trained weight in fine-tuning gigantic pre-trained
models. It is the first attempt toward jointly op-
timizing both parameter-efficiency of the fine-
tuning process and the resource-efficiency of the
fine-tuned model.
•Both unstructured and structured sparse priors
are investigated in our proposed DSEE algo-
rithm. For weight updates , the injected sparsityprior enhances existing parameter-efficient up-
date schemes (e.g., low-rank decomposition).
As for the final weights , we draw superior sparse
masks, either unstructured or structured, directly
from the weight updates, which requires nei-
ther additional parameters nor access to the pre-
trained weights and saves the sparsification cost.
•Extensive experiments demonstrate the effective-
ness of our proposal across various representa-
tive pre-trained language models (BERT, GPT-2,
and RoBERTa) and on diverse evaluation bench-
marks (E2E, DART, WebNLG, and GLUE). On
GPT-2, our methods can achieve a BLUE score
of{69.5,54.9,47.5}with0.1%of trainable pa-
rameters on {E2E, WebNLG, DART} with 20%
parameter removed in pre-trained weights. On
BERT, DSEE can fine-tune only 0.5%parame-
ters and save about 25% inference FLOPs, while
losing less than 2%performance.
2 Related Work
Pruning and Sparsification Pruning is a classi-
cal model compression technique that can reduce
the number of parameters, which can bring train-
ing and inference efficiency. Researchers have
proposed several pruning methods for pre-trained
language models: McCarley et al. (2019); Chen
et al. (2021b) pruned attention heads that had less
contribution during finetuning; Sanh et al. (2020)
proposed a pruning criterion targeting the weight
change after training, which suits the transfer learn-
ing better; Wang et al. (2020) incorporated low-
rank factorization and ℓ0regularization for pruning.
Recently, there is a series of sparsification works
that utilize sparse masks with specific structures,
called Butterflies, and achieve high efficiency in
pretraining models (Chen et al., 2021a) or fine-
tuning on downstream tasks (Dao et al., 2022).
However, these methods do not allow for parameter-
efficient updates.
Low-rank decomposition Low-rank approxima-
tion (Ye, 2005) has broad applications in the ma-
chine learning community and is vastly studied.
One classical scenario is the robust principal com-
ponent analysis (Candès et al., 2011), which de-
composes a matrix into a low-rank plus a sparse
component. Existing literature shows that in deep
learning, the learned over-parameterized models
often naturally bear approximate low-rank weight
structures (Oymak et al., 2019; Yu et al., 2017).

--- PAGE 3 ---
Some (Jaderberg et al., 2014; Povey et al., 2018;
Sainath et al., 2013; Zhang et al., 2014; Zhao et al.,
2016) have explicitly imposed the low-rank con-
straint during training. Wang et al. (2020); Hu
et al. (2021) utilized low-rank decomposition to
shrink the model size and trim down the trainable
parameters during fine-tuning. However, to our
best knowledge, integrating sparsity and low-rank
structures has never been studied before for effi-
cient fine-tuning of pre-trained language models.
Parameter-efficient adaptation. Parameter-
efficient fine-tuning aims to reduce the number of
trainable parameters when fine-tuning the models
across different downstream domains. Unlike
pruning, it aims at adapting models with fewer
parameters instead of building sparse models.
Various approaches are proposed to achieve this
goal: Rebuffi et al. (2017); Houlsby et al. (2019)
inserted and only trained adapters between existing
layers, whose parameters are much less compared
to the pretrained models. Guo et al. (2020)
leveraged ℓ0regularization to limit the number of
non-zero elements in the update vectors. Lester
et al. (2021); Li and Liang (2021); Liu et al. (2021)
introduced efficient prompt tuning which optimizes
only a small continuous task-specific vector. Zaken
et al. (2021) fine-tunes only the bias terms inside
models. Hu et al. (2021) proposed a low-rank
decomposition-based method, and He et al. (2021)
combined low-rank and adapter-based methods for
efficient finetuning. However, fine-tuned models
yielded by these methods have the same amount
of weights as the pre-trained models; hence they
contribute no resource efficiency.
3 Methodology
In this section, we describe our notations and defini-
tions of sparsity generation and parameter-efficient
fine-tuning in Section 3.1, and then introduce the
dually sparsity-embedded efficient fine-tuning al-
gorithms in Sections 3.2 and 3.3.
3.1 Preliminaries
Sparsification and resource-efficient fine-tuning.
We adopt both unstructured and structured prun-
ing methods to produce sparsity. They can lead to
resource-efficiency including memory and compu-
tation savings.
Given W ∈ Rm×na weight matrix, pruning
aims at finding a binary mask M ∈ { 0,1}m×n
which is applied to Wand generating a sparse
 
 
  
 Sparse-Embedded Decomposition  
 
Input 
Embeddings  
Output  
Point-wise Pr oduct  
Point-wise Addition  
Trainable W eights  
Pruned W eights  
Sparse-Embedded Pr e-trained W eights  
Pre-trained W eights  
......  
 Sparse Matrix  Figure 1: The overview of our proposal. The sparse masks
can have unstructured or structured patterns, which leads to
resources efficiency. During the fine-tuning, we only train
decomposed matrices U,Vand non-zero elements in S2.
weight W ⊙ M . The weights at the positions
where Mhave “0” value are considered as pruned.
Pruning methods can be classified into two classes
by the structure of M: For unstructured pruning
where Mdoes not have sparse structures such as
rows and columns, the memory cost is saved due
to fewer number of nonzero parameters; for struc-
tured pruning, it also helps save computational cost
since the sparse weights can be smaller in size. One
of the most widely used unstructured pruning meth-
ods is the weight magnitude (Han et al., 2015), i.e.,
remove the weights with the smallest absolute val-
ues. One common structured pruning method in
the NLP field is the head pruning (McCarley et al.,
2019), which tries to remove unimportant attention
heads from the model.
Parameter-efficient fine-tuning. To leverage the
knowledge in pre-trained weights W, downstream
models learn task-specific weight update ∆Wvia
fine-tuning and generate predictions with weights
W+ ∆W, where the output of models is calcu-
lated as (W+ ∆W)xwithxas the input. Since
∆Whas the same size as W, learning the update
matrices usually requires massive resources as the
size of the pre-trained model increases. Parameter-
efficient fine-tuning tries to solve this problem by
using as few trainable parameters as possible to rep-
resent ∆W, while maintaining competitive down-
stream fine-tuning performance. Previous litera-
ture reaches the goal via either sparsifying weight
update matrices ∆W(Guo et al., 2020) or lever-
aging low-rank decomposed matrices to compute
∆W(Hu et al., 2021), while in our work we com-
bine both of them.
3.2 Sparsity-Embedded Parameter-Efficient
Fine-tuning
A recent study (Hu et al., 2021) enforces low-rank
constraint to weight update tensors ∆W, and ob-
tains a satisfactory trade-off between parameter-
efficiency and model quality. However, as re-

--- PAGE 4 ---
Algorithm 1: Sparsity-Embedded Low-
Rank Decomposition
Input: Pretrained weights W, number of
non-zero elements N, number of
weights to decompose n.
Output: Indices sets Ωi, i= 1,2, . . . , n .
1Initialize each Ωito be an empty set.
2foreach weight matrix WiinWdo
/*Decomposition */
3 Perform matrix decomposition
Wi≈ AB +S′by solving the
optimization problem 1.
/*Extract important elements from S′
intoΩi.*/
4 Perform thresholding on S′: Keep N
elements in S′with top magnitudes,
and append their locations into Ωi.
5end
vealed experimentally by (Yu et al., 2017), a part
of the important information in the trained weights
scatters outside the low-rank subspace, creating
sparse “residuals". Inspired by this observation,
we investigate a new sparsity-aware low-rank sub-
space of ∆W, and introduce the first component of
our proposal in Figure 1, i.e., sparsity-embedded
parameter-efficient fine-tuning.
Specifically, the weight updates ∆Ware con-
sisted of two components as illustrated in Figure 1,:
(1) a low-rank component ∆Wlbuilt by the mul-
tiplication of two matrices U ∈Rm×randV ∈
Rr×n; and (2) a sparse residual ∆Ws=PΩ(S)
where S ∈Rm×nis a learnable matrix, PΩ(S) =(
si,j,(i, j)∈Ω
0,(i, j)∈ΩC, i = 1,2, . . . , m, j =
1,2, . . . , n ,wi,jis the parameter of Sat location
(i, j), and Ωis a indices set containing the posi-
tions of non-zero elements in S. The update ma-
trix∆Wis expressed as ∆Wl+ ∆Ws, with U,
VandSas the learnable parameters while Ωis
fixed once determined. Compared to the full fine-
tuning which has m×ntrainable parameters for
a matrix with size m×n, our method only has
(m+n)×r+ card(Ω) trainable parameters. If
ris smaller thanm×n−card(Ω)
m+n⪅0.5 min{m, n},
our method is capable of reducing trainable param-
eters for downstream fine-tuning. In practice, the
value of ris very small compared to mandnso
the savings are significant.
One question for the above pipeline is how tofind a high-quality indices set Ω. Inspired by the
observation that the low-rank component ∆Wlis
highly correlated with the low-rank structure of
W(Hu et al., 2021), we hypothesize that the in-
dices set Ωshould be highly correlated as well.
More concretely, we hypothesize that the sparse
residuals that are not in the low-dimensional sub-
space of Wmay also lay outside ∆Wl, which
motivates the design of sparse update ∆Ws. We
formulate the problem of discovering the sparse
residuals of Was a Robust Principal Component
Analysis (Candès et al., 2011). Formally, we aim
at solving the following optimization problem:
min
U,V,S1
2∥W−UV−S∥2
F
s.t.rank( U)≤r,rank( V)≤r,
card( S)≤c.(1)
where rank(·)andcard(·)indicate the rank and
the number of non-zero elements of a matrix, re-
spectively. S′represents the sparse residuals that
cannot be fit in the low-rank component AB, and
we acquire the locations of elements with non-zero
magnitude into Ω. To solve Problem 1 efficiently,
we adopt an SVD-free algorithm called GreB-
smo (Zhou and Tao, 2013) (refer to Section A.2).
Algorithm 1 summarizes the detailed procedure of
constructing sparse indices sets Ω. Empirically, we
set the size of Ω(i.e.,c) to be 16since it yields
high test performance (refer to Section 4.2) while
imposing little overhead on parameters. The initial
values of VandSare set as 0so these matrices do
not affect outputs at the beginning of training.
3.3 Dually Sparsity-Embedded Efficient
Tuning (DSEE)
Adapting pre-trained models with ∆Wland∆Ws
can bring significant parameter-efficiency, but does
not directly bring any resource-efficiency such as
memory or computational cost. Motivated by such,
we propose a unified framework called DSEE pur-
suing both parameter- and resource-efficiency si-
multaneously. We leverage the sparsity in pre-
trained models’ weights to enhance the resource ef-
ficiency, as demonstrated in Figure 1. More specif-
ically, we derive sparse masks Mdirectly from
the parameter-efficient updates ∆W, and apply the
sparse masks by pruning the pre-trained weights
Wto seek resource-efficiency. It requires no addi-
tional parameter for sparsifying the model and no
access to the underlying pretrained weights, which
is favorable due to the lower sparsification cost.

--- PAGE 5 ---
As shown in Algorithm 2, DSEE handles un-
structured and structured pruning at the same time:
forunstructured pruning , we sort the magnitude
of∆W, generate a sparse mask Mby assigning
“1” to the position where ∆Whave largest mag-
nitude and “0” to the rest; for structured pruning ,
we sum the magnitude of ∆Wof each head and
remove those with least scores. We also shrink
∆Waccordingly by removing the corresponding
weight columns in Vand∆Wsto match the shape
while keeping Uintact. A comparison of different
pruning criteria is shown in Section 4.2.1, which
demonstrates that ∆Wis a superior choice due
to the high downstream task performance and no
access to the pretrained weights W.
Given a parameter budget, the number of pa-
rameters per module decreases if we choose to
adapt more modules, which imposes a trade-off.
We study different choices of modules to adapt in
Section 4.2.2, and we find the optimal modules to
adapt are WqandWv, where WqandWvstand for
the projection weights for query and value in atten-
tion heads. Since some modules are not adapted
during fine-tuning ( i.e.,∆W= 0), we prune them
separately according to the magnitude of the corre-
sponding pre-trained weights. After applying the
maskMto the pretrained weights W, we conven-
tionally tune ∆Wl(=UV)and∆Ws(=PΩ(S))
for several epochs to recover the performance (Han
et al., 2015).
4 Experiment Results
Datasets and models. We use three classi-
cal pre-trained language models in our ex-
periments: BERT BASE (Devlin et al., 2019),
RoBERTa LARGE (Liu et al., 2019) and GPT-
2 (Radford et al., 2019), which have 12/24/24
layers with hidden size of 768/1024/1024 and
110/380/354M trainable parameters, respectively.
For BERT and RoBERTa, we evaluate on the
GLUE benchmarks (Wang et al., 2018), and
for GPT-2 we use E2E (Novikova et al., 2017),
WebNLG (Gardent et al., 2017) and DART (Nan
et al., 2021).
Training and evaluation details. For BERT and
RoBERTa, we follow the default settings in Wolf
et al. (2019); Devlin et al. (2019). We use the
AdamW (Loshchilov and Hutter, 2017) optimizer
for downstream fine-tuning, and a batch size of 32
for BERT and RoBERTa, and a batch size of 2 for
GPT-2. The rest hyper-parameters for training are
reported in Table 11.Algorithm 2: DSEE
Input: Pretrained weights W, number of
non-zero elements N, desired
sparsity s, loss function L.
Output: Sparse mask M, matrices U,V,S.
Derive Ωfrom pretrained weights W.
Initialization: U= 0,V ∼ N (0,0.02), and
S= 0.
/*I: train before pruning */
TrainU,V,Swith respect to Lunder the
constraint of PΩC(S) = 0 .
/*II: pruning the model */
ifusing unstructured pruning then
Prune (1−s%)weights in Wby
sorting the magnitude of ∆W.
else
Prune (1−s%)heads by sorting the
aggregated magnitude of ∆Wof heads.
Shrink VandSaccordingly to match the
shape.
end if
/*III: tuning after pruning */
Fine-tune U,V,Sto recover the performance.
Evaluation Metrics. For the GLUE benchmark,
we report the accuracy, Matthew’s correlation, and
Pearson’s correlation in the evaluation. On GPT-
2, we use BLEU (Papineni et al., 2002), ME-
TEOR (Denkowski and Lavie, 2014), TER (Snover
et al., 2006) and NIST (Doddington, 2002) as the
evaluation metrics. To evaluate the efficiency of
models, we report the number of trainable parame-
ters to measure the parameter-efficiency, the num-
ber of total parameters (the number of non-zero
parameters in the model) to measure the resource-
efficiency, and FLOPs for the computational effi-
ciency.
Baselines. On BERT and RoBERTa, we con-
duct comprehensive experiments with the follow-
ing baseline methods: ❶Fine-tune: directly fine-
tuning the full model; ❷EarlyBERT (Chen et al.,
2021b): learn importance scores for heads and per-
form pruning based on them afterwards; ❸BERT
Tickets (Chen et al., 2020): IMP-based unstruc-
tured pruning; ❹P-Tuning v2 (Liu et al., 2021);
❺Bitfit (Zaken et al., 2021): fine-tuning bias
terms only; and ❻LoRA: low-rank decomposition,
which learns ∆Wlonly (Hu et al., 2021). On GPT-
2, we conduct comparisons with multiple baseline
methods: ❶Adapters (Houlsby et al., 2019): insert

--- PAGE 6 ---
·
Table 1: Performance comparison with BERT BASE on
SST-2, RTE, CoLA, and MRPC. We report both the
median and the standard deviation from five runs.
∆W=# TrainableSST-2 RTE CoLA MRPCParameters
∆Wl 589.8K 92.55 (0.35) 68.95 (2.02) 60.34 (1.69) 86.27 (0.88)
∆Wl+ ∆Ws 590.2K 92.78 (0.34) 70.04 (1.35) 60.31 (1.04) 86.52 (0.57)
∆Wl 294.9K 92.32 (0.36) 68.23 (1.43) 58.48 (1.61) 86.52 (0.72)
∆Wl+ ∆Ws 295.3K 92.66 (0.06) 69.31 (2.08) 58.85 (0.92) 87.01 (0.79)
adapters after linear layers; ❷FT-Top2: fine-tune
the top 2 layers only; ❸: Prefix: prefix tuning in-
troduced by Li and Liang (2021); and ❹LoRA.
4.1 Efficient Tuning with DSEE
Parameter-efficiency with sparse residuals.
To verify that using a simple low-rank component
∆Wlhas limitations, we compare its performance
with our sparsity-embedded efficient fine-tuning.
Table 1 shows that on four benchmarks ( i.e., SST-2,
RTE, CoLA, and MRPC), adding a sparse residual
in weight updates can bring performance gain: at
the level of approximately 600K trainable parame-
ters, adding sparse residuals with only 384 nonzero
elements ( 12×2×16 = 384 ) can increase the
validation performance on all benchmarks except
CoLA by 0.23%∼1.09%; at the level of approxi-
mately 300K trainable parameters, adding sparse
residuals can bring performance gain ranged from
0.34% to1.08% on all four benchmarks.
We further verify that adding sparse residuals
∆Wscould benefit NLG tasks with GPT-2. Ta-
ble 2 shows that under different levels of param-
eters, adding sparse residuals ∆Wsyields higher
performance for most metrics on three tasks. At the
level of 0.39M parameters, adding sparse residuals
can improve all metrics on WebNLG and DART,
and sightly boost the NIST score on E2E. At the
level of 0.20M parameters, ∆Wshelps increase
all metrics across three tasks. We also show the
standard deviation in Table 10.
Resource- and parameter-efficiency with un-
structured sparse masks. We verify that DSEE
is capable of enhancing both parameter- and
resource-efficiency, while preserving performance
on downstream tasks, on various architectures.
Table 3 summarizes the experiment results on
BERT BASE , and we observe that introducing
unstructured sparsity patterns inside pretrained
weights not only brings resource-efficiency (man-
ifested by the fewer number of total parameters)
but also potentially improves the performance on
downstream tasks. Specifically, at 80% and70%
of total parameters, DSEE can remain compara-ble performance on downstream tasks, and even
present a performance boost on QQP, RTE, and
SST-2 compared to LoRA. At the level of 50%
parameters, performance on smaller datasets such
as CoLA and RTE drops by a wider margin; but
on larger datasets such as QQP, DSEE can main-
tain comparable performance ( <1.5%gap) after
sparsification.
On GPT-2, we observe a similar trend as shown
in Table 4. DSEE can achieve superior performance
with unstructured sparse patterns with 80% total pa-
rameters compared to finetuning the entire model,
and remain highly competitive with other baselines
with fewer parameters in the model. Using only
50% of parameters in pre-trained weights, DSEE
can achieve comparable performance with the full
fine-tuning on E2E and DART.
Finally, we validate if DSEE can work on the
larger model RoBERTa LARGE . We conduct experi-
ments on four datasets (CoLA, SST-2, QNLI, and
RTE), and present the results in Table 5. Com-
pared to full-finetuning, LoRA, and Adapter, our
method reaches comparable performance on these
four downstream tasks and saves resources at the
same time. The performance gap is maximal 1%
but30% parameters in the models are removed.
Resource- and parameter-efficiency with struc-
tured sparse masks. DSEE can directly perform
structured pruning on weights without additional
parameters such as importance scores of heads. In
Table 6 we show the performance of structured
pruned BERT BASE on several tasks in the GLUE
benchmark, where we study the testing accuracy
after removing 3,6and9attention heads on SST-2,
MNLI, QNLI and QQP, as well as the inference
FLOPs ratios of the model. Firstly, removing 3
heads from the model reaches comparable perfor-
mance against full fine-tuning (improved on SST-2,
MNLI, and QNLI) and LoRA (improved on SST-2
and QQP), while taking advantage of reduced infer-
ence FLOPs. Secondly, removing 6heads from the
model will lead to lower performance since half
of the parameters in the projection matrices are
eliminated. However, the performance of DSEE is
still higher than EarlyBERT. Lastly, DSEE with 9
heads removed from the model leads to comparable
performance with EarlyBERT, but the number of
trainable parameters is substantially smaller ( 0.6M
versus 66M).

--- PAGE 7 ---
Table 2: Performance comparison of different decomposition on GPT-2 with different weight update terms. We
report the median value of BLEU, MET, NIST and TER from five runs.
Forms# Trainable E2E WebNLG DART
Parameters BLEU MET NIST BLEU MET TER BLEU MET TER
∆W= ∆Wl 0.39M 70.38 46.89 8.844 55.29 0.414 0.394 48.23 0.392 0.469
∆W= ∆Wl+ ∆Ws 0.39M 70.29 46.65 8.858 55.50 0.416 0.392 48.17 0.397 0.467
∆W= ∆Wl 0.20M 69.17 45.90 8.741 55.23 0.413 0.396 46.49 0.387 0.477
∆W= ∆Wl+ ∆Ws 0.20M 69.70 46.85 8.824 55.56 0.413 0.392 47.47 0.393 0.475
Table 3: Performance comparison of different methods on GLUE benchmarks with BERT BASE . We use the
unstructured pruning and report the median value from five runs. †: results taken from Chen et al. (2020).
Methods# Trainable # Total Dataset
Parameters Parameters CoLA STS-B MNLI QQP QNLI MRPC RTE SST-2
Fine-tune†110M 100% 54.5 88.4 82.4 90.2 89.1 85.2 66.2 92.1
BERT Tickets†33∼55M 30∼50% 53.8 88.2 82.6 90.0 88.9 84.9 66.0 91.9
P-Tuning v2 0.3M 100% 59.37 89.36 82.15 88.50 90.59 84.80 67.51 92.20
Bitfit 0.1M 100% 58.61 88.74 78.80 85.93 89.22 87.55 72.20 92.07
LoRA 0.6M 100% 59.99 89.09 83.32 89.48 90.72 86.27 68.95 92.32
DSEE 0.6M 80% 59.94 89.22 83.29 90.00 90.46 86.27 70.76 92.66
DSEE 0.6M 70% 58.69 89.08 83.09 89.97 90.68 86.27 71.48 91.97
DSEE 0.6M 50% 48.49 87.72 81.84 89.55 90.12 81.13 63.90 91.17
4.2 Ablation and Visualization
We study several choices of parameters and provide
visualization in this section.
4.2.1 Different criteria for sparse masks
We find the magnitude of weight updates ( i.e.,
|∆W|) is an effective solution for preserving per-
formance with both unstructured and structured
pruning. We conduct experiments on the adapted
weights ( i.e.,WqandWv), and compare against
two baselines: ❶Random: perform random prun-
ing on the adapted modules; ❷|W+ ∆W|: per-
form pruning based on the magnitude of final
adapted weights. Table 7 shows the results on RTE
and SST-2 with BERT BASE . We can see from the
table that: ❶performing unstructured pruning with-
out accessing the pretrained weights can achieve
comparable performance on RTE and SST-2, only
slightly weaker than pruning with final adapted
weights; ❷performing structured pruning accord-
ing to ∆Wyields the highest performance on both
datasets after training. These observations verify
the effectiveness of our proposal.
4.2.2 Different choices of modules to adapt
We study the choices of modules to adapt for DSEE
on RTE. We choose possible modules to adapt
within Wq,Wk,Wv, and Wo, representing the
projection matrix for query, key, value, and out-
put, respectively. We hold the number of trainableparameters at the same level and set the sparsity
level at 30%. Table 9 summarizes the performance
with different adapted weights, which demonstrates
that adapting WqandWvyields the highest perfor-
mance. Each module will be given fewer parame-
ters when adapting more modules and the model
may not be sufficiently fine-tuned when adapting
fewer modules and leading to inferior performance.
Different methods for identifying Ω.We com-
pare our proposal against various methods to iden-
tifyΩfrom pretrained weights W:❶Magnitude ,
which selects the position of elements with high-
est magnitude into Ω;❷Random , which randomly
samples positions into Ω. The results are shown
in Figure 2. We can observe that our proposal
can identify high-quality Ωfor finetuning on down-
stream tasks, shown by the consistently higher per-
formance with different sizes of the indices set Ω.
Different sizes of Ω.We search over 8∼256
to find the optimal size of Ω.Ωwith a smaller
size brings fewer performance gains, and Ωwith a
larger size may harm the efficiency. Figure 2 shows
the relationship between the size of Ωand the per-
formance on SST-2. We find the optimal choice for
this task is 16where the model achieves the highest
performance. Consequently, we by default set the
size of Ωto 16 for simplicity.

--- PAGE 8 ---
Table 4: Performance comparison of different methods on GPT-2 on E2E, WebNLG and DART. ‡: Results taken
from Hu et al. (2021).
Methods# Trainable # Total E2E WebNLG DART
Parameters Parameters BLEU MET NIST BLEU MET TER BLEU MET TER
Fine-tune‡354.92M 100% 68.2 0.462 8.62 47.6 0.39 0.50 46.0 0.39 0.46
Adapters‡11.48M 100% 68.9 0.461 8.71 55.2 0.41 0.39 45.4 0.38 0.46
FT-Top2‡25.19M 100% 68.1 0.460 8.59 33.5 0.26 0.75 38.1 0.34 0.56
Prefix‡0.35M 100% 69.7 0.461 8.81 54.4 0.41 0.41 45.7 0.38 0.46
LoRA‡0.39M 100% 70.4 0.468 8.85 55.3 0.41 0.39 47.5 0.39 0.45
DSEE 0.39M 80% 69.4 0.465 8.78 54.9 0.44 0.39 47.5 0.39 0.46
DSEE 0.39M 50% 69.5 0.466 8.74 42.0 0.33 0.53 43.4 0.37 0.51
Table 5: Performance comparison of different methods
on RoBERTa LARGE on CoLA, SST-2, MRPC and RTE.
‡: Results taken from Hu et al. (2021).
Methods# Trainable # Total in Dataset
Parameters Parameters CoLA SST-2 QNLI RTE
Fine-tune‡355.0M 100% 68.0 95.1 94.7 86.6
Adapter‡0.8M 100% 66.3 96.3 94.7 72.9
LoRA‡0.8M 100% 68.2 96.2 94.8 85.2
DSEE 0.8M 70% 67.2 96.1 94.4 84.9
Table 6: Performance comparison of different methods
on GLUE benchmarks with BERT BASE . We perform
the structured pruning and report the median value from
five runs. †: results taken from Chen et al. (2020).
Methods FLOPs # Trainable SST-2 MNLI QNLI QQP
Fine-tune†1.0× 110M 92.1 82.4 89.1 90.2
LoRA 1.01× 0.6M 92.32 83.32 90.72 89.48
EarlyBERT 0.63× ∼ 66M 90.71 81.81 89.18 90.06
DSEE (3 heads) 0.92× 0.6M 92.55 83.25 90.65 89.84
DSEE (6 heads) 0.84× 0.6M 92.32 82.32 90.01 89.11
DSEE (9 heads) 0.75× 0.6M 91.63 80.02 88.39 88.56
92.0092.2592.5092.7593.00
816324864 128 256
Size of ΩTest AccuracyMagnitude Ours Random
Figure 2: Testing performance on SST-2 with different
sizes of Ω. We report the average accuracy and the 90%
confidence interval of five runs.
5 Conclusion
This paper draws on the prior of sparsity and es-
tablishes the DSEE framework. It is the first at-
tempt toward jointly optimizing both parameter-
efficiency of the fine-tuning process, and the
resource-efficiency of the fine-tuned model. OnTable 7: Performance of using different pruning criteria
to generate unstructured masks. We only perform prun-
ing on WqandWv. The first part applies unstructured
pruning and the latter applies structured pruning.
Criterion RTE SST-2
|∆W| 69.68 (1.37) 91.97 (0.26)
|W+ ∆W| 70.76 (2.09) 92.78 (0.39)
Random 64.62 (2.28) 91.63 (0.25)
|∆W| 70.40 (1.05) 92.55 (0.43)
|W+ ∆W| 68.59 (1.60) 92.20 (0.60)
Random 68.23 (1.29) 91.97 (0.14)
state-of-the-art large-scale language models (e.g.,
BERT, GPT, and RoBERTa) and across several
datasets, DSEE consistently demonstrates highly
impressive parameter and inference efficiency, in
addition to preserving a competitive downstream
transfer performance on various tasks. Our future
work targets extending DSEE to the finetuning of
large-scale computer vision and/or multi-modal
pre-trained models.
Limitation The unstructured sparse patterns we
introduce are not as hardware-friendly as the struc-
tured patterns, suggesting the speedup of using
unstructured patterns maybe limited due to the
implementation. The number of parameters of
models we are studying are only at the level of
100∼300M, and the datasets are focus on GLUE,
E2E, WebNLG, and DART. We will generalize to
wider choices of datasets in future works.
6 Ethical and Broader Impacts
DSEE aims at reducing the number of trainable pa-
rameters when fine-tuning the models, which can
help save the cost of saving new weights. This can
be helpful to companies who are fine-tuning large-

--- PAGE 9 ---
scale language models on various downstream
tasks, suggesting our work has potentially positive
broader impact. On the other hand, our work does
not have obvious ethical impacts, as we focusing
on model tuning.

--- PAGE 10 ---
References
Emmanuel J Candès, Xiaodong Li, Yi Ma, and John
Wright. 2011. Robust principal component analysis?
Journal of the ACM (JACM) , 58(3):1–37.
Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang,
Zhao Song, Atri Rudra, and Christopher Re. 2021a.
Pixelated butterfly: Simple and efficient sparse train-
ing for neural network models. arXiv preprint
arXiv:2112.00029 .
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Si-
jia Liu, Yang Zhang, Zhangyang Wang, and
Michael Carbin. 2020. The lottery ticket hypoth-
esis for pre-trained bert networks. arXiv preprint
arXiv:2007.12223 .
Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan,
Zhangyang Wang, and Jingjing Liu. 2021b. Early-
bert: Efficient bert training via early-bird lottery tick-
ets. In Proceedings of the Joint Conference of the
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing .
Tri Dao, Beidi Chen, Nimit Sohoni, Arjun De-
sai, Michael Poli, Jessica Grogan, Alexander Liu,
Aniruddh Rao, Atri Rudra, and Christopher Ré.
2022. Monarch: Expressive structured matrices
for efficient and accurate training. arXiv preprint
arXiv:2204.00595 .
Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evaluation
for any target language. In Proceedings of the ninth
workshop on statistical machine translation , pages
376–380.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers) , pages 4171–
4186.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research , pages 138–145.
Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017. The webnlg chal-
lenge: Generating text from rdf data. In Proceedings
of the 10th International Conference on Natural Lan-
guage Generation , pages 124–133.
Demi Guo, Alexander M Rush, and Yoon Kim. 2020.
Parameter-efficient transfer learning with diff prun-
ing. arXiv preprint arXiv:2012.07463 .
Song Han, Huizi Mao, and William J Dally. 2015. Deep
compression: Compressing deep neural networkswith pruning, trained quantization and huffman cod-
ing. arXiv preprint arXiv:1510.00149 .
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. 2021. Towards a
unified view of parameter-efficient transfer learning.
arXiv preprint arXiv:2110.04366 .
Yihui He, Xiangyu Zhang, and Jian Sun. 2017. Channel
pruning for accelerating very deep neural networks.
InProceedings of the IEEE International Conference
on Computer Vision (ICCV) , pages 1389–1397.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for nlp. In In-
ternational Conference on Machine Learning , pages
2790–2799. PMLR.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu
Chen. 2021. Lora: Low-rank adaptation of large
language models. arXiv preprint arXiv:2106.09685 .
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.
2014. Speeding up convolutional neural networks
with low rank expansions. In Proceedings of the
British Machine Vision Conference. BMVA Press .
Yann LeCun, John S Denker, and Sara A Solla. 1990.
Optimal brain damage. In Advances in neural infor-
mation processing systems , pages 598–605.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. arXiv preprint arXiv:2104.08691 .
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. arXiv
preprint arXiv:2101.00190 .
Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du,
Zhilin Yang, and Jie Tang. 2021. P-tuning v2:
Prompt tuning can be comparable to fine-tuning
universally across scales and tasks. arXiv preprint
arXiv:2110.07602 .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,
Shoumeng Yan, and Changshui Zhang. 2017. Learn-
ing efficient convolutional networks through network
slimming. In Proceedings of the IEEE international
conference on computer vision , pages 2736–2744.
Ilya Loshchilov and Frank Hutter. 2017. Decou-
pled weight decay regularization. arXiv preprint
arXiv:1711.05101 .
JS McCarley, Rishav Chakravarti, and Avirup Sil. 2019.
Structured pruning of a bert-based question answer-
ing model. arXiv preprint arXiv:1910.06360 .

--- PAGE 11 ---
Linyong Nan, Dragomir Radev, Rui Zhang, Amrit
Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xi-
angru Tang, Aadit Vyas, Neha Verma, Pranav Kr-
ishna, Yangxiaokang Liu, Nadia Irwanto, Jessica
Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma,
Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan,
Xi Victoria Lin, Caiming Xiong, Richard Socher, and
Nazneen Fatema Rajani. 2021. Dart: Open-domain
structured data record to text generation.
Jekaterina Novikova, Ond ˇrej Dušek, and Verena Rieser.
2017. The e2e dataset: New challenges for end-to-
end generation. In Proceedings of the 18th Annual
SIGdial Meeting on Discourse and Dialogue , pages
201–206.
Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi
Soltanolkotabi. 2019. Generalization guarantees for
neural networks via harnessing the low-rank structure
of the jacobian. arXiv preprint arXiv:1906.05392 .
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics , pages 311–318.
Daniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li,
Hainan Xu, Mahsa Yarmohammadi, and Sanjeev
Khudanpur. 2018. Semi-orthogonal low-rank ma-
trix factorization for deep neural networks. In Inter-
speech , pages 3743–3747.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea
Vedaldi. 2017. Learning multiple visual domains
with residual adapters. In Proceedings of the 31st
International Conference on Neural Information Pro-
cessing Systems , pages 506–516.
Ao Ren, Tianyun Zhang, Shaokai Ye, Jiayu Li, Wenyao
Xu, Xuehai Qian, Xue Lin, and Yanzhi Wang. 2018.
Admm-nn: An algorithm-hardware co-design frame-
work of dnns using alternating direction method of
multipliers.
Tara N Sainath, Brian Kingsbury, Vikas Sindhwani,
Ebru Arisoy, and Bhuvana Ramabhadran. 2013. Low-
rank matrix factorization for deep neural network
training with high-dimensional output targets. In
2013 IEEE international conference on acoustics,
speech and signal processing , pages 6655–6659.
IEEE.
Victor Sanh, Thomas Wolf, and Alexander M Rush.
2020. Movement pruning: Adaptive sparsity by fine-
tuning. In NeurIPS .
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of the 7th Conference of theAssociation for Machine Translation in the Americas:
Technical Papers , pages 223–231.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. In International
Conference on Learning Representations .
Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2020.
Structured pruning of large language models. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 6151–6162.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2019. Huggingface’s transformers: State-of-
the-art natural language processing. arXiv preprint
arXiv:1910.03771 .
Jieping Ye. 2005. Generalized low rank approximations
of matrices. Machine Learning , 61(1-3):167–191.
Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng
Tao. 2017. On compressing deep models by low rank
and sparse decomposition. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition , pages 7370–7379.
Elad Ben Zaken, Shauli Ravfogel, and Yoav Gold-
berg. 2021. Bitfit: Simple parameter-efficient
fine-tuning for transformer-based masked language-
models. arXiv preprint arXiv:2106.10199 .
Yu Zhang, Ekapol Chuangsuwanich, and James Glass.
2014. Extracting deep neural network bottleneck fea-
tures using low-rank matrix factorization. In 2014
IEEE international conference on acoustics, speech
and signal processing (ICASSP) , pages 185–189.
IEEE.
Yong Zhao, Jinyu Li, and Yifan Gong. 2016. Low-rank
plus diagonal adaptation for deep neural networks.
In2016 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages 5005–
5009. IEEE.
Tianyi Zhou and Dacheng Tao. 2013. Greedy bilat-
eral sketch, completion & smoothing. In Artificial
Intelligence and Statistics , pages 650–658. PMLR.

--- PAGE 12 ---
A More Implementation Details
A.1 Hyper-parameters
We report the learning rates, the batch sizes, and the
max sequence length for DSEE in Table 11. The
device we used for experiments are various, includ-
ing NVIDIA GeForce GTX 1080 Ti, GeForce RTX
2080 Ti, Titan RTX, and A6000. We follow (Hu
et al., 2021) to set the evaluation protocols on E2E,
WebNLG, and DART.
A.2 Decomposition Method
GreBsmo (Zhou and Tao, 2013) is an algorithm
for solving the Robust PCA-like methods. The
optimization of U,VandSfollows the following
iterative rules:


Uk=Q,QR 
(X−Sk−1)VT
k−1
=QR
Vk=QT(X−Sk−1)
Sk=Sλ(X−UkVk),
(2)
where Xis the original dense matrix, QR(·)means
the QR decomposition, Sλ(·)indicates the soft-
threshold function ( i.e.,Sλ(x) =x1|x|≥λ) , and
the subscripts kindicates the optimization step.
A.3 Statistics and Usage of Datasets
We report the statistics of datasets in Table 8. For
GLUE tasks we report the sizes of the train, the
dev and the test set, and for non-GLUE tasks we
report the sizes of the train, validation (dev), and
test set. We follow the conventional use of these
datasets (Hu et al., 2021) and do not modify the
conventional splits.
B More Experiments Results
B.1 Ablation Studies
Table 9 summarizes the performance with different
adapted weights, which demonstrates that adapting
WqandWvleads to the highest performance.
Performance of different Ω.We conduct ad-
ditional ablation study experiments (three runs for
each experiment) on the sizes of Ωon three datasets
in GLUE ( i.e., STSB, QNLI and MRPC). The
results shown in Table 12 below verify that our
method are generalizable to other datasets. On
STSB and QNLI, using a size of 16 can achieve the
best performance, while on MRPC it can achieve a
comparable test accuracy.
Compare with recent methods. We have con-
ducted a set of experiment to compare our methodsTable 8: The statistics of datasets we used for experi-
ments.
Name Train Dev Test
GLUE
CoLA 8,551 1,043 -
SST-2 67,349 872 -
MNLI 392,702 9,815 -
QNLI 104,743 5,463 -
QQP 363,846 40,430 -
STS-B 5,749 1,500 -
RTE 2,490 277 -
MRPC 3,668 408 -
non-GLUE
E2E 42,061 4,672 4,693
WebNLG 18,025 2,258 4,928
DART 30,526 2,768 6,959
Table 9: Testing performance of BERT BASE on RTE
with different adapted modules. We report the median
values and the standard deviation from three runs.
Weights Test Acc. Weights Test Acc.
Wq 68.59 (0.21) Wk 67.87 (0.21)
Wv 68.23 (1.82) Wo 68.23 (1.05)
Wq,Wk68.95 (1.11) Wq,Wv 71.48 (2.16)
Wk,Wv70.04 (0.75) Wq,Wk,Wv69.31 (2.56)
with MAM Adapter (He et al., 2021). We train a
RoBERTa-large with their method on SST-2, QNLI,
RTE, by following the same hyperparameters used
in the original work. The results are shown in Ta-
ble 13. We observe that our method, even with
sparse models, achieves same-level performance
with LoRA and MAM Adapter.
Other Pruning Methods. We apply the iterative
magnitude pruning method on RTE. Specifically,
we train the model for 10epochs, prune 10% of
the remaining weights, and fine-tune for 10epochs
before the next pruning. Table 14 shows that di-
rectly applying iterative magnitude pruning does
not bring performance improvements over the one-
shot pruning baseline.

--- PAGE 13 ---
Table 10: Performance comparison of different decomposition on GPT-2 with different weight update terms. We
report the standard deviation of BLEU, MET, NIST and TER from five runs.
Forms# Trainable E2E WebNLG DART
Parameters BLEU MET NIST BLEU MET TER BLEU MET TER
∆W= ∆Wl 0.39M 0.43 0.13 0.037 0.37 0.005 0.003 0.23 0.001 0.001
∆W= ∆Wl+ ∆Ws 0.39M 0.07 0.26 0.047 0.48 0.005 0.004 0.40 0.003 0.002
∆W= ∆Wl 0.20M 0.23 0.03 0.043 0.26 0.005 0.007 0.06 0.002 0.001
∆W= ∆Wl+ ∆Ws 0.20M 0.61 0.19 0.029 0.52 0.006 0.004 0.15 0.001 0.001
Table 11: Hyper-parameters we used on different datasets and architectures.
Architecture Method ParametersDataset
MNLI QNLI QQP SST-2 CoLA MRPC RTE STS-B
BERT BASE DSEE (before pruning) Learning Rate 2e-4 2e-4 2e-4 2e-4 1e-3 8e-4 6e-4 8e-4
BERT BASE DSEE (after pruning) Learning Rate 2e-4 2e-4 2e-4 2e-4 1e-3 8e-4 6e-4 8e-4
BERT BASE DSEE Batch Size 32
BERT BASE DSEE Max Sequence Length 128
RoBERTa LARGE DSEE (before pruning) Learning Rate - 2e-4 - 4e-4 3e-4 - 4e-4 -
RoBERTa LARGE DSEE (after pruning) Learning Rate - 2e-4 - 4e-4 3e-4 - 4e-4 -
RoBERTa LARGE DSEE Batch Size - 32 - 32 16 - 32 -
RoBERTa LARGE DSEE Max Sequence Length - 512 - 512 128 - 512 -
Table 12: Performance on the three datasets using dif-
ferent sizes of Ω.
Dataset Ω = 8 Ω = 16 Ω = 32 Ω = 48
STSB 89.26 89.28 89.10 89.16
MRPC 85.38 86.27 86.36 86.27
QNLI 91.01 91.06 91.00 90.87
Table 13: Compare with more methods.
Method Total Parameters SST-2 QNLI RTE
LoRA 100% 96.2 94.8 85.2
MAM Adapter 100% 96.1 94.7 80.4
Ours 70% 96.1 94.4 84.9
Table 14: Applying iterative magnitude pruning (IMP)
to prune models.
Remaining Weights Accuracy
90% 70.02%
81% 70.76%
72.9% 63.90%
65.6% 61.01%
