# Tỉa cành phân tán hướng tới mạng nơ-ron nhỏ trong học liên kết

Hong Huang1, Lan Zhang2, Chaoyue Sun3, Ruogu Fang3, Xiaoyong Yuan2, Dapeng Wu1
1Đại học Thành phố Hồng Kông, 2Đại học Công nghệ Michigan, 3Đại học Florida
honghuang2000@outlook.com, lanzhang@mtu.edu, chaoyue.sun@ufl.edu
ruogu.fang@bme.ufl.edu, xyyuan@mtu.edu, dpwu@ieee.org

Tóm tắt — Tỉa cành mạng nơ-ron là một kỹ thuật thiết yếu để giảm kích thước và độ phức tạp của mạng nơ-ron sâu, cho phép các mô hình quy mô lớn hoạt động trên các thiết bị có tài nguyên hạn chế. Tuy nhiên, các phương pháp tỉa cành hiện tại phụ thuộc rất nhiều vào dữ liệu huấn luyện để hướng dẫn các chiến lược tỉa cành, khiến chúng không hiệu quả cho học liên kết trên các tập dữ liệu phân tán và bảo mật. Thêm vào đó, quá trình tỉa cành tốn nhiều bộ nhớ và tính toán trở nên không khả thi đối với các thiết bị hạn chế tài nguyên trong học liên kết. Để giải quyết những thách thức này, chúng tôi đề xuất FedTiny, một khung tỉa cành phân tán cho học liên kết tạo ra các mô hình nhỏ chuyên biệt cho các thiết bị hạn chế về bộ nhớ và tính toán. Chúng tôi giới thiệu hai mô-đun chính trong FedTiny để tìm kiếm thích ứng các mô hình chuyên biệt được tỉa cành thô và tinh để phù hợp với các tình huống triển khai với tính toán cục bộ thưa thớt và rẻ. Đầu tiên, một mô-đun lựa chọn chuẩn hóa theo lô thích ứng được thiết kế để giảm thiểu độ lệch trong tỉa cành gây ra bởi tính không đồng nhất của dữ liệu cục bộ. Thứ hai, một mô-đun tỉa cành tiến bộ nhẹ nhằm tỉa cành tinh hơn các mô hình dưới ngân sách bộ nhớ và tính toán nghiêm ngặt, cho phép chính sách tỉa cành cho mỗi lớp được xác định dần dần thay vì đánh giá toàn bộ cấu trúc mô hình. Kết quả thực nghiệm chứng minh hiệu quả của FedTiny, vượt trội hơn các phương pháp hiện đại, đặc biệt khi nén các mô hình sâu thành các mô hình nhỏ cực kỳ thưa thớt. FedTiny đạt được cải thiện độ chính xác 2,61% trong khi giảm đáng kể chi phí tính toán 95,91% và dung lượng bộ nhớ 94,01% so với các phương pháp hiện đại.

Từ khóa chỉ mục — học liên kết, tỉa cành mạng nơ-ron, mạng nơ-ron nhỏ

I. GIỚI THIỆU

Mạng nơ-ron sâu (DNN) đã đạt được thành công lớn trong thập kỷ qua. Tuy nhiên, chi phí tính toán khổng lồ và chi phí lưu trữ hạn chế việc sử dụng DNN trên các thiết bị hạn chế tài nguyên. Tỉa cành mạng nơ-ron đã là một giải pháp nổi tiếng để cải thiện hiệu quả phần cứng [1], [2]. Cốt lõi của tỉa cành mạng nơ-ron là loại bỏ các tham số không quan trọng khỏi DNN và xác định các mạng con chuyên biệt cho các nền tảng phần cứng và nhiệm vụ huấn luyện khác nhau (được định nghĩa là các tình huống triển khai). Để đạt được độ chính xác tốt hơn, hầu hết các phương pháp tỉa cành phụ thuộc rất nhiều vào dữ liệu huấn luyện để đánh đổi giữa kích thước mô hình, hiệu quả và độ chính xác [2]–[6], điều này không may trở nên không hiệu quả khi xử lý các tập dữ liệu huấn luyện bảo mật được phân tán trên các thiết bị hạn chế tài nguyên.

Thành công gần đây trong học liên kết cho phép huấn luyện cộng tác trên các thiết bị phân tán với các tập dữ liệu cục bộ bảo mật [7]. Thay vì tải lên dữ liệu cục bộ, học liên kết tổng hợp kiến thức trên thiết bị bằng cách cập nhật lặp đi lặp lại các tham số mô hình cục bộ tại máy chủ. Mặc dù thành công, học liên kết không thể xác định mô hình tỉa cành chuyên biệt cho các thiết bị tham gia mà không có dữ liệu huấn luyện. Để giải quyết vấn đề này, [8] đề xuất tách biệt quá trình tỉa cành trong môi trường liên kết, nơi một mô hình kích thước lớn được tỉa cành trước tại máy chủ và sau đó được tinh chỉnh trên các thiết bị. Tuy nhiên, vì hầu hết các thuật toán tỉa cành cần hướng dẫn từ phân phối dữ liệu, mà không có quyền truy cập vào dữ liệu huấn luyện phía thiết bị, việc tỉa cành phía máy chủ dẫn đến độ lệch đáng kể trong mạng con được tỉa cành, đặc biệt dưới các phân phối dữ liệu cục bộ không đồng nhất (non-iid). Để giảm thiểu các vấn đề độ lệch như vậy, nghiên cứu gần đây đẩy các hoạt động tỉa cành đến các thiết bị [9]–[13]. Như được hiển thị trong Hình 1 bên trái, một mô hình kích thước đầy đủ hoặc một mô hình tỉa cành thô sẽ được tỉa cành tinh hơn dựa trên điểm quan trọng được cập nhật từ các thiết bị. Điểm quan trọng cho tất cả các tham số cần được lưu trữ trong bộ nhớ, điều này không khả thi đối với các thiết bị hạn chế tài nguyên với ngân sách bộ nhớ hạn chế. Hơn nữa, không có bất kỳ tương tác nào với phía thiết bị, mô hình ban đầu thông qua tỉa cành thô phía máy chủ vẫn gặp phải vấn đề độ lệch, đòi hỏi nỗ lực thêm trong tỉa cành tinh sau đó để tìm mạng con tối ưu. Tác động tiêu cực như vậy trở nên thách thức hơn khi tỉa cành hướng tới một mạng con cực kỳ nhỏ, vì mạng con ban đầu bị lệch có thể lệch đáng kể so với cấu trúc tối ưu, dẫn đến độ chính xác kém [14].

Để giải quyết các thách thức trên, trong bài báo này, chúng tôi phát triển một khung tỉa cành phân tán mới cho học liên kết có tên FedTiny. Tùy thuộc vào các tình huống triển khai, tức là các nền tảng phần cứng tham gia và nhiệm vụ huấn luyện, FedTiny có thể thu được các mô hình nhỏ chuyên biệt sử dụng các tập dữ liệu phân tán và bảo mật trên các thiết bị tham gia. Bên cạnh đó, FedTiny cho phép các thiết bị có ngân sách bộ nhớ và tính toán eo hẹp tham gia vào quá trình tỉa cành tốn nhiều tài nguyên bằng cách cấu hình lại các tương tác giữa máy chủ và thiết bị. Như được hiển thị trong Hình 1 bên phải, FedTiny giới thiệu hai mô-đun chính: mô-đun lựa chọn chuẩn hóa theo lô (BN) thích ứng và mô-đun tỉa cành tiến bộ. Để tránh tác động tiêu cực của tỉa cành ban đầu bị lệch, chúng tôi giới thiệu mô-đun BN thích ứng để xác định một mô hình tỉa cành thô chuyên biệt bằng cách tỉa cành gián tiếp tại các thiết bị, nơi các thiết bị chỉ đánh giá việc tỉa cành phía máy chủ. Cần đề cập rằng việc đánh giá một mô hình được tỉa cành rẻ hơn nhiều so với huấn luyện và tỉa cành. Đánh giá cục bộ được phản hồi cho máy chủ thông qua các tham số chuẩn hóa theo lô. Vì các lớp chuẩn hóa theo lô có thể đo lường hiệu quả phân phối dữ liệu cục bộ với rất ít tham số [15], mô-đun này hướng dẫn việc tỉa cành ban đầu với chi phí tính toán và truyền thông thấp. Bên cạnh đó, trái ngược với nghiên cứu trước đây sử dụng điểm quan trọng của tất cả các tham số trong một mô hình kích thước đầy đủ để tỉa cành tinh, mô-đun tỉa cành tiến bộ được phát triển để điều chỉnh lặp đi lặp lại cấu trúc mô hình với tính toán cục bộ thưa thớt và rẻ. Lấy cảm hứng từ RigL [14], các thiết bị chỉ đánh giá các tham số mô hình một phần (ví dụ, một lớp đơn) tại một thời điểm, nơi các điểm quan trọng top-K được lưu trữ cục bộ và tải lên máy chủ, giảm đáng kể chi phí bộ nhớ, tính toán và truyền thông.

Để chứng minh hiệu quả của FedTiny, chúng tôi đánh giá FedTiny trên ResNet18 [16] và VGG11 [17] với bốn tập dữ liệu phân loại hình ảnh (CIFAR-10, CIFAR-100, CINIC-10 và SVHN). Kết quả thực nghiệm toàn diện cho thấy FedTiny đạt được độ chính xác cao hơn nhiều với mức độ bộ nhớ và chi phí tính toán thấp hơn so với các phương pháp cơ sở hiện đại. Đặc biệt trong chế độ mật độ thấp [18] từ 10^-2 đến 10^-3, FedTiny chỉ mất một chút độ chính xác, trong khi các phương pháp cơ sở khác gặp phải sự giảm mạnh về độ chính xác. Hơn nữa, FedTiny đạt được độ chính xác top-một 85,23% với 0,014× FLOPs và 0,03× dung lượng bộ nhớ của ResNet18 [16], vượt trội hơn phương pháp cơ sở tốt nhất, đạt 82,62% độ chính xác với 0,34× FLOPs và 0,51× dung lượng bộ nhớ.

II. NGHIÊN CỨU LIÊN QUAN

A. Tỉa cành mạng nơ-ron

Tỉa cành mạng nơ-ron đã là một kỹ thuật nổi tiếng để loại bỏ các tham số dư thừa của DNN để nén mô hình, có thể truy nguyên về cuối những năm 1980 [1], [19], [20]. Hầu hết các phương pháp tỉa cành hiện tại tập trung vào sự đánh đổi giữa độ chính xác và độ thưa thớt trong giai đoạn suy luận. Một quá trình tỉa cành điển hình đầu tiên tính điểm quan trọng của tất cả các tham số trong một DNN được huấn luyện tốt và sau đó loại bỏ các tham số có điểm thấp hơn. Điểm quan trọng có thể được suy ra dựa trên độ lớn trọng số [1], [2], khai triển Taylor bậc nhất của hàm mất mát [19], [21], khai triển Taylor bậc hai của hàm mất mát [5], [20], và các biến thể khác [3], [4], [6].

Một hướng nghiên cứu gần đây khác về tỉa cành mạng nơ-ron tập trung vào việc cải thiện hiệu quả của giai đoạn huấn luyện, có thể được chia thành hai loại. Một là tỉa cành tại khởi tạo, tức là tỉa cành mô hình kích thước đầy đủ ban đầu trước khi huấn luyện. Chính sách tỉa cành có thể được xác định bằng cách đánh giá độ nhạy cảm kết nối [22], tích Hessian-gradient [23], và dòng chảy synaptic [24] của mô hình ban đầu. Vì việc tỉa cành như vậy không liên quan đến dữ liệu huấn luyện, mô hình được tỉa cành không được chuyên biệt hóa cho nhiệm vụ huấn luyện, dẫn đến hiệu suất bị lệch. Loại khác là huấn luyện thưa thớt động [14], [25], [26]. Cấu trúc mô hình được tỉa cành được điều chỉnh lặp đi lặp lại trong suốt quá trình huấn luyện trong khi duy trì kích thước mô hình được tỉa cành ở độ thưa thớt mong muốn. Tuy nhiên, quá trình tỉa cành là để điều chỉnh cấu trúc mô hình trong một không gian tìm kiếm lớn, đòi hỏi các hoạt động tốn nhiều bộ nhớ, điều này không khả thi đối với các thiết bị hạn chế tài nguyên. Mặc dù RigL [14] cố gắng giảm tiêu thụ bộ nhớ, nó cần tính gradient cho tất cả các tham số, điều này tốn kém về mặt tính toán và có thể dẫn đến các vấn đề chậm trễ trong học liên kết.

B. Tỉa cành mạng nơ-ron trong học liên kết

Học liên kết gần đây đã thu hút sự chú ý như một phương pháp đầy hứa hẹn để giải quyết các mối quan tâm về quyền riêng tư dữ liệu trong học máy cộng tác. FedAvg [27], một trong những phương pháp được sử dụng rộng rãi nhất trong học liên kết, sử dụng các mô hình được cập nhật cục bộ trên thiết bị thay vì dữ liệu thô để đạt được việc chuyển giao kiến thức riêng tư. Vì dữ liệu được lưu trữ cục bộ và không thể chia sẻ, các phương pháp tỉa cành nêu trên dựa vào dữ liệu huấn luyện không thể được sử dụng trong học liên kết.

Được truyền cảm hứng bởi tỉa cành tại khởi tạo, Xu et al. đề xuất tỉa cành mô hình kích thước đầy đủ ban đầu tại máy chủ và tinh chỉnh tại các thiết bị với dữ liệu cục bộ của họ [8]. Các phương pháp tỉa cành tại khởi tạo hiện tại, như SNIP [22], GraSP [23], và SynFlow [24], có thể được chuyển đổi trực tiếp thành tỉa cành phía máy chủ. Tuy nhiên, tỉa cành phía máy chủ thường dẫn đến các mô hình được tỉa cành bị lệch đáng kể, đặc biệt đối với các phân phối dữ liệu cục bộ không đồng nhất (non-iid).

Để giảm thiểu độ lệch như vậy, nghiên cứu gần đây đẩy các hoạt động tỉa cành trong thiết lập liên kết đến các thiết bị. Bằng cách huấn luyện cục bộ một mô hình kích thước đầy đủ, SCBF [9] loại bỏ động các kênh không quan trọng trên các thiết bị. Việc huấn luyện cục bộ như vậy với một mô hình kích thước đầy đủ được gán cho một phần thiết bị trong FedPrune để hướng dẫn tỉa cành dựa trên các activation được cập nhật [11]. Bên cạnh đó, LotteryFL [10] tỉa cành lặp đi lặp lại một mô hình kích thước đầy đủ trên các thiết bị với tỷ lệ tỉa cành cố định để tìm một mạng con cục bộ được cá nhân hóa. Tuy nhiên, nghiên cứu trên gặp phải chi phí bộ nhớ và tính toán lớn ở phía thiết bị vì các thiết bị cần tính toán cục bộ điểm quan trọng của tất cả các tham số. Mặc dù PruneFL [13] giảm chi phí tính toán cục bộ bằng cách tỉa cành tinh một mô hình tỉa cành thô thay vì một mô hình kích thước đầy đủ, nó vẫn đòi hỏi một dung lượng bộ nhớ cục bộ lớn để ghi lại điểm quan trọng được cập nhật của tất cả các tham số trong mô hình kích thước đầy đủ. ZeroFL [28] phân chia trọng số thành trọng số hoạt động và trọng số không hoạt động trong suy luận và các trọng số và activation thưa thớt cho lan truyền ngược. Tuy nhiên, phương pháp này vẫn cần một không gian bộ nhớ lớn vì các trọng số không hoạt động và gradient được tạo ra thông qua quá trình huấn luyện vẫn được lưu trữ dưới dạng dày đặc. FedDST [29] triển khai điều chỉnh mặt nạ trên các thiết bị, và máy chủ tạo ra một mô hình toàn cục mới thông qua tổng hợp thưa thớt và tỉa cành theo độ lớn. Nó cần nhiều chi phí tính toán hơn vì nó cần các epoch huấn luyện thêm để khôi phục các trọng số tăng trưởng trước khi tải lên, điều này có thể dẫn đến các vấn đề chậm trễ trong học liên kết. Mô hình tỉa cành thô vẫn gặp phải các vấn đề độ lệch trong tỉa cành phía máy chủ. Tỉa cành mạng nơ-ron liên kết hiện tại không thu được một mô hình nhỏ chuyên biệt mà không có độ lệch và các mối quan tâm về ngân sách bộ nhớ/tính toán. Do đó, chúng tôi phát triển FedTiny để đạt được điều này.

C. Học liên kết với dữ liệu không đồng nhất

Học liên kết gặp phải sự phân kỳ khi các phân phối dữ liệu trên các thiết bị là không đồng nhất (non-iid) [30]. Một số công việc đã được đề xuất để giải quyết các thách thức non-iid, ví dụ, MATCHA [31], FedProx [7], và FedNova [32]. Những công việc này cung cấp các đảm bảo hội tụ của học liên kết dưới các giả định mạnh, điều này trở nên không thực tế trong các tình huống thế giới thực.

Tăng cường dữ liệu (ví dụ, Astraea [33], FedGS [34], và CSFedAvg [35]) và các phương pháp cá nhân hóa (ví dụ, meta learning [36], multi-task learning [37], và knowledge distillation [38]) là hai phương pháp đầy hứa hẹn để giải quyết các vấn đề non-iid. Tuy nhiên, những phương pháp này tốn kém về mặt tính toán và trở nên không khả thi trong các tình huống hạn chế tài nguyên. Trong công việc của chúng tôi, chúng tôi phát triển một phương pháp tỉa cành phân tán mới với lựa chọn chuẩn hóa theo lô thích ứng để tìm một mô hình tỉa cành thô không bị lệch để giải quyết các thách thức non-iid trong các thiết bị hạn chế tài nguyên.

III. FEDTINY ĐỀ XUẤT

Phần này giới thiệu FedTiny được đề xuất. Chúng tôi đầu tiên mô tả bài toán, sau đó là các nguyên tắc thiết kế của chúng tôi. Theo đó, chúng tôi trình bày hai mô-đun chính trong FedTiny: mô-đun lựa chọn BN thích ứng và mô-đun tỉa cành tiến bộ.

A. Phát biểu bài toán

Chúng tôi xem xét một thiết lập học liên kết điển hình, nơi K thiết bị cộng tác huấn luyện một mạng nơ-ron với các tập dữ liệu cục bộ tương ứng Dk, k ∈ {1,2,...,K}. Tất cả các thiết bị có tài nguyên bộ nhớ và tính toán hạn chế. Cho một mạng nơ-ron lớn với các tham số dày đặc Θ, chúng tôi nhằm tìm một mạng con chuyên biệt với các tham số thưa thớt θ và mặt nạ m trên các tham số dày đặc để đạt được hiệu suất dự đoán tối ưu cho học liên kết. Các tham số thưa thớt được suy ra bằng cách áp dụng một mặt nạ cho các tham số dày đặc: θ = Θ ⊙ m (m ∈ {0,1}^|Θ|). Trong quá trình huấn luyện, mật độ d của mặt nạ thưa thớt m không thể vượt quá mật độ mục tiêu dtarget. dtarget được xác định bởi giới hạn tài nguyên bộ nhớ của các thiết bị. Chúng tôi xây dựng bài toán như một bài toán tối ưu hóa có ràng buộc:

min θ,m Σ(k=1 to K) L(θ,m,Dk),
s.t. d ≤ dtarget (1)

trong đó L(θ,m,Dk) biểu thị hàm mất mát cho tập dữ liệu cục bộ Dk trên thiết bị thứ k.

B. Nguyên tắc thiết kế

Như được hiển thị trong Hình 1 bên trái, tỉa cành mạng nơ-ron liên kết hiện tại đối mặt với hai thách thức chính, độ lệch trong tỉa cành thô và tiêu thụ bộ nhớ tốn kém trong tỉa cành tinh. Để giải quyết những thách thức này, chúng tôi đề xuất FedTiny. Tổng quan về FedTiny được minh họa trong Hình 1 bên phải, bao gồm hai mô-đun chính: mô-đun lựa chọn BN thích ứng và mô-đun tỉa cành tiến bộ.

Mô-đun lựa chọn chuẩn hóa theo lô thích ứng (Bước 2-5 trong Hình 1 bên phải) nhằm suy ra một cấu trúc tỉa cành thô thích ứng trên máy chủ và giảm bớt độ lệch trong tỉa cành thô do dữ liệu không đồng nhất không được nhìn thấy trên các thiết bị. Trong mô-đun này, các thiết bị đầu tiên cộng tác cập nhật các phép đo chuẩn hóa theo lô cho tất cả các mô hình ứng viên từ tỉa cành thô. Sau đó máy chủ chọn một mô hình ứng viên ít bị lệch hơn làm mô hình tỉa cành thô ban đầu dựa trên đánh giá của thiết bị.

Mô-đun tỉa cành tiến bộ (Bước 6-7 trong Hình 1 bên phải) tiếp tục cải thiện mô hình tỉa cành thô bằng tỉa cành tinh tại các thiết bị hạn chế tài nguyên, giảm đáng kể dung lượng bộ nhớ trên thiết bị và chi phí tính toán. Trong mô-đun này, các thiết bị chỉ duy trì điểm quan trọng top-K của các tham số được tỉa cành. Dựa trên điểm quan trọng trung bình, máy chủ tăng trưởng và tỉa cành các tham số để tạo ra một cấu trúc mô hình mới. Sau khi tăng trưởng và tỉa cành lặp đi lặp lại, cấu trúc mô hình tiến bộ tiếp cận cấu trúc tối ưu.

Trong phần sau, chúng tôi cung cấp mô tả chi tiết về mô-đun lựa chọn chuẩn hóa theo lô thích ứng và mô-đun tỉa cành tiến bộ, tương ứng.

C. Lựa chọn chuẩn hóa theo lô thích ứng

Điều quan trọng là phải giải quyết vấn đề độ lệch trong mô hình tỉa cành thô, vì cấu trúc được tỉa cành có độ lệch cao đòi hỏi nhiều tài nguyên và thời gian hơn để điều chỉnh về cấu trúc tối ưu, đặc biệt trong chế độ mật độ thấp. Một phương pháp có thể là gửi một tập hợp các ứng viên cấu trúc được tỉa cành đến các thiết bị và để các thiết bị chọn mô hình ít bị lệch nhất từ nhóm ứng viên. Chúng tôi gọi phương pháp này là lựa chọn vanilla [39]. Tuy nhiên, nghiên cứu gần đây [40] cho thấy hiệu suất mô hình được tỉa cành khác nhau trước và sau tinh chỉnh, điều này khiến ứng viên cấu trúc được tỉa cành được chọn trước tinh chỉnh không nhất thiết là tốt nhất sau tinh chỉnh. Vấn đề như vậy có thể được phóng đại trong thiết lập liên kết vì phân phối dữ liệu không đồng nhất trên các thiết bị có thể tăng thêm sự khác biệt về hiệu suất mô hình được tỉa cành trong tinh chỉnh.

Để giải quyết vấn đề này, chúng tôi giới thiệu lựa chọn chuẩn hóa theo lô thích ứng trong FedTiny. Lựa chọn chuẩn hóa theo lô thích ứng cập nhật các phép đo chuẩn hóa theo lô cho các mô hình ứng viên trước khi đánh giá, nhằm suy ra một cấu trúc tỉa cành thô ít bị lệch hơn. Thuật toán của mô-đun lựa chọn chuẩn hóa theo lô thích ứng được minh họa trong Thuật toán 1.

Chúng tôi giới thiệu chuẩn hóa theo lô (BN) [15] để cung cấp các phép đo cho phân phối dữ liệu trên các thiết bị. Các phép đo như vậy cung cấp biểu diễn của dữ liệu trên thiết bị và do đó hướng dẫn quá trình tỉa cành. Phép biến đổi chuẩn hóa theo lô được tính toán theo phép biến đổi sau trên đầu vào thứ i xi trong mỗi lô,

x̂i ← (xi - μ) / √(σ² + ε), (2)

trong đó ε là một hằng số nhỏ. Trong quá trình huấn luyện, μ và σ được cập nhật dựa trên giá trị trung bình động μi và độ lệch chuẩn σi của lô xi,

μt = γμt-1 + (1-γ)μi, σ²t = γσ²t-1 + (1-γ)σ²i, (3)

trong đó γ biểu thị hệ số momentum và t là số lượng lần lặp huấn luyện. Trong quá trình kiểm tra, giá trị trung bình μ và độ lệch chuẩn σ được giữ cố định.

Trong mô-đun lựa chọn chuẩn hóa theo lô thích ứng, các phép đo chuẩn hóa theo lô được cập nhật trong lượt truyền xuôi trên các thiết bị trước khi đánh giá để chọn một ứng viên tỉa cành thô ít bị lệch hơn. Cụ thể, sau khi tỉa cành thô trên các tham số kích thước đầy đủ Θ với các chiến lược khác nhau, máy chủ thu được một nhóm ban đầu bao gồm C mô hình ứng viên với các tham số thưa thớt θ(c) và các mặt nạ tương ứng m(c), trong đó θ(c) = Θ ⊙ m(c), cho c ∈ {1,2,...,C}. Đối với mỗi mô hình ứng viên, chúng tôi đặt các tỷ lệ tỉa cành khác nhau cho mỗi lớp trong khi giữ mật độ tổng thể d ≤ dtarget. Các thiết bị đầu tiên tải về tất cả các mô hình ứng viên. Lưu ý rằng chi phí truyền thông thấp do mật độ mạng cực thấp. Sau đó, mỗi thiết bị (ví dụ thiết bị thứ k) lấy mẫu một tập dữ liệu phát triển từ dữ liệu cục bộ, D̂k ⊂ Dk, đóng băng tất cả các tham số và cập nhật giá trị trung bình μ(c)k và độ lệch chuẩn σ(c)k của các lớp chuẩn hóa theo lô trong mô hình ứng viên thứ c. Tiếp theo, máy chủ tổng hợp tất cả các phép đo chuẩn hóa theo lô cục bộ từ các thiết bị để thu được các phép đo chuẩn hóa theo lô toàn cục mới cho mỗi mô hình ứng viên, tức là cho c ∈ {1,2,...,C},

μ(c) = Σ(k=1 to K) |D̂k|/Σ(k=1 to K)|D̂k| μ(c)k, σ(c) = Σ(k=1 to K) |D̂k|/Σ(k=1 to K)|D̂k| σ(c)k, (4)

trong đó |D̂k| biểu thị số lượng mẫu trong tập dữ liệu D̂k.

Thuật toán 1 Lựa chọn chuẩn hóa theo lô thích ứng
Đầu vào: C mô hình ứng viên tỉa cành thô với các tham số thưa thớt θ(1),...,θ(C) và các mặt nạ tương ứng m(1),...,m(C) trên máy chủ, K thiết bị với tập dữ liệu phát triển cục bộ D̂1,...,D̂K.
Đầu ra: mô hình tỉa cành thô ít bị lệch hơn với các tham số θ0 và mặt nạ tương ứng m0.

1: // Phía thiết bị
2: for k = 1 to K do
3:   Tải các tham số thưa thớt θ(1),θ(2),...,θ(C) và các mặt nạ tương ứng m(1),m(2),...,m(C) từ máy chủ
4:   for c = 1 to C do
5:     Tính các phép đo chuẩn hóa theo lô cục bộ μ(c)k,σ(c)k bằng lượt truyền xuôi trên θ(c) với tập dữ liệu D̂k
6:   end for
7:   Tải lên μ(1)k,μ(2)k,...,μ(C)k và σ(1)k,σ(2)k,...,σ(C)k cho máy chủ
8: end for

9: // Phía máy chủ
10: for c = 1 to C do
11:   μ(c) = Σ(k=1 to K)|D̂k|μ(c)k / Σ(k=1 to K)|D̂k|
12:   σ(c) = Σ(k=1 to K)|D̂k|σ(c)k / Σ(k=1 to K)|D̂k|
13: end for

14: // Phía thiết bị
15: for k = 1 to K do
16:   Tải μ(1),μ(2),...,μ(C) và σ(1),σ(2),...,σ(C) từ máy chủ
17:   for c = 1 to C do
18:     μ(c)k,σ(c)k ← μ(c),σ(c) // Mỗi mô hình ứng viên cài đặt các phép đo chuẩn hóa theo lô toàn cục
19:     s(c)k ← L(θ(c),m(c),D̂k) // Tính mất mát làm metric đánh giá
20:   end for
21:   Tải lên s(1)k,s(2)k,...,s(C)k cho máy chủ
22: end for

23: // Phía máy chủ
24: for c = 1 to C do
25:   s(c) ← Σ(k=1 to K)|D̂k|s(c)k / Σ(k=1 to K)|D̂k|
26: end for
27: c* ← argminc(s(c)) // Chọn mô hình ứng viên với mất mát thấp nhất
28: θ0,m0 = θ(c*),m(c*)
29: return θ0,m0

Sau đó, mỗi thiết bị cập nhật các phép đo chuẩn hóa theo lô toàn cục μ(c), σ(c) cho mô hình ứng viên thứ c. Xem xét Eq. 1, chúng tôi để các thiết bị tính mất mát đánh giá cho mỗi mô hình ứng viên được cập nhật với dữ liệu trên thiết bị của họ và để máy chủ chọn mô hình ứng viên với mất mát trung bình thấp nhất làm mô hình tỉa cành thô.

Lưu ý rằng mặc dù mô-đun lựa chọn chuẩn hóa theo lô thích ứng đòi hỏi việc chuyển giao các tham số, chi phí truyền thông vẫn tối thiểu, vì chỉ các tham số trong các mô hình được tỉa cành với mật độ cực thấp cần được chuyển giao. Phân tích chi tiết về chi phí truyền thông được thảo luận trong Phần IV-D. Thêm vào đó, phép biến đổi chuẩn hóa theo lô được tính toán như một phần của lượt truyền xuôi tại thiết bị mà không có tính toán gradient hoặc cập nhật. Do đó, lựa chọn chuẩn hóa theo lô thích ứng hiệu quả giải quyết độ lệch của cấu trúc mô hình mà không gây ra chi phí bộ nhớ hoặc tính toán đáng kể.

D. Tỉa cành tiến bộ

Cho một mô hình tỉa cành thô từ mô-đun trên, chúng tôi giới thiệu tỉa cành tiến bộ để tiếp tục tỉa cành tinh mô hình để có hiệu suất tốt hơn. Chúng tôi đề xuất mô-đun tỉa cành tiến bộ với hai cải tiến: 1) chỉ các điểm quan trọng top-K được tính toán, trong khi các điểm quan trọng còn lại bị loại bỏ để tiết kiệm không gian bộ nhớ; 2) các tham số mô hình một phần (ví dụ, một lớp đơn) được điều chỉnh mỗi vòng thay vì toàn bộ mô hình để tránh tính toán tốn kém. FedTiny sử dụng điều chỉnh tăng trưởng-tỉa cành trên cấu trúc mô hình trong khi duy trì độ thưa thớt. Cụ thể, máy chủ tăng trưởng các tham số được tỉa cành và tỉa cành cùng số lượng tham số không được tỉa cành để điều chỉnh cấu trúc mô hình. Ký hiệu alt là số lượng tham số sẽ được tăng trưởng và tỉa cành trên lớp l tại lần lặp thứ t. Để hướng dẫn tăng trưởng và tỉa cành trên máy chủ, mỗi thiết bị chỉ huấn luyện mô hình thưa thớt và tính toán gradient Top-alt cho các tham số được tỉa cành, điều này giữ cho dung lượng bộ nhớ thấp và chi phí tính toán trong thiết bị hạn chế tài nguyên. Hơn nữa, để giảm tính toán tốn kém, FedTiny chia cấu trúc mô hình thành nhiều khối và tỉa cành một khối trong một vòng. Mô-đun tỉa cành tiến bộ được chi tiết trong Thuật toán 2.

Chi tiết, mỗi thiết bị (ví dụ thiết bị thứ k) đầu tiên tải xuống tham số mô hình thưa thớt toàn cục θt với mặt nạ mt làm tham số cục bộ θkt trong lần lặp thứ t, và áp dụng SGD với gradient thưa thớt:

θkt+1 = θkt - ηt∇L(θkt,mt,Bkt) ⊙ mt, (5)

trong đó ηt là tỷ lệ học, Bkt là một lô mẫu từ tập dữ liệu cục bộ Dk, và ∇L ⊙ mt biểu thị gradient thưa thớt cho tham số thưa thớt θkt. Sau E lần lặp của SGD cục bộ, mỗi thiết bị tính toán gradient top-alt cho các tham số được tỉa cành trên mỗi lớp l với một lô mẫu. Chúng tôi ký hiệu g̃k,lt là gradient top-alt của tham số được tỉa cành với độ lớn lớn nhất trên thiết bị thứ k:

g̃k,lt = TopK(gk,lt, alt), (6)

trong đó TopK(v, k) là hàm ngưỡng, các phần tử của v có giá trị tuyệt đối nhỏ hơn giá trị tuyệt đối lớn thứ k được thay thế bằng 0, và gk,lt là gradient của các tham số được tỉa cành trên lớp l.

Thuật toán 2 Tỉa cành tiến bộ
Đầu vào: các tham số tỉa cành thô ban đầu θ0 với mặt nạ m0, K thiết bị với tập dữ liệu cục bộ D1,...DK, số lần lặp t, tỷ lệ học ηt, số lượng tỉa cành alt cho mỗi lớp l, số lần lặp cục bộ mỗi vòng E, số vòng giữa hai hoạt động tỉa cành ΔR, và số vòng để dừng tỉa cành Rstop.
Đầu ra: một mô hình được huấn luyện tốt với θt thưa thớt và mặt nạ được điều chỉnh mt

1: t ← 0
2: while do
3:   // Phía thiết bị
4:   for k = 1 to K do
5:     Tải các tham số thưa thớt θt và mặt nạ mt từ máy chủ
6:     for i = 0 to E-1 do
7:       θkt+i+1 ← θkt+i - ηt+i∇L(θt+i,mt,Bkt+i) ⊙ mt
8:     end for
9:     Tải lên θt+E cho máy chủ
10:    if t mod ΔRE = 0 and t ≤ ERstop then
11:      for each layer l in model do
12:        Tính gradient top-alt g̃k,lt sử dụng Eq. 6 với không gian bộ nhớ O(alt)
13:        Tải lên g̃k,lt cho máy chủ
14:      end for
15:    end if
16:  end for
17:  // Phía máy chủ
18:  Tính các tham số toàn cục θt+E bằng cách lấy trung bình các tham số từ các thiết bị
19:  if t mod ΔRE = 0 and t ≤ ERstop then
20:    for each layer l in model do
21:      g̃lt ← Σ(k=1 to K)|Dk|g̃k,lt / Σ(k=1 to K)|Dk|
22:      Ilgrow ← các chỉ số được tỉa cành alt có giá trị tuyệt đối lớn nhất trong g̃lt
23:      Ildrop ← các chỉ số không được tỉa cành alt có độ lớn trọng số nhỏ nhất trong θt+E
24:      Tính mặt nạ mới mlt+E bằng cách điều chỉnh mlt dựa trên Ilgrow và Ildrop
25:    end for
26:    θt+E ← θt+E ⊙ mt+E // Tỉa cành mô hình sử dụng mặt nạ được cập nhật
27:  else
28:    mt+E ← mt
29:  end if
30:  t ← t + E
31: end while

Để tính toán g̃k,lt, các thiết bị tạo một bộ đệm trong bộ nhớ để lưu trữ alt gradient. Khi một gradient được tính toán và bộ đệm đầy, nếu độ lớn của nó lớn hơn độ lớn nhỏ nhất trong bộ đệm, gradient này sẽ được đẩy vào bộ đệm, và gradient có độ lớn nhỏ nhất sẽ bị loại bỏ. Ngược lại, gradient này sẽ bị loại bỏ. Theo cách này, các thiết bị chỉ cần không gian bộ nhớ O(alt) để lưu trữ gradient.

Tiếp theo, máy chủ tổng hợp các tham số thưa thớt và gradient để có được các tham số trung bình và gradient trung bình g̃lt cho mỗi lớp l,

g̃lt = Σ(k=1 to K) |Dk|/Σ(k=1 to K)|Dk| g̃k,lt, (7)

trong đó |Dk| biểu thị số lượng mẫu trong tập dữ liệu Dk.

Sau đó, máy chủ tăng trưởng alt tham số được tỉa cành với độ lớn gradient trung bình lớn nhất trên mỗi lớp l. Sau đó, máy chủ tỉa cành alt tham số không được tỉa cành (loại trừ các tham số vừa được tăng trưởng) với độ lớn nhỏ nhất trên mỗi lớp l.

Theo tăng trưởng và tỉa cành, máy chủ tạo ra một mô hình toàn cục với cấu trúc mô hình mới, và FedTiny bắt đầu tinh chỉnh mô hình toàn cục mới. FedTiny thực hiện tỉa cành và tinh chỉnh lặp đi lặp lại để đạt được một mạng nơ-ron nhỏ tối ưu cho tất cả các thiết bị.

IV. THỰC NGHIỆM

Trong phần này, chúng tôi tiến hành các thực nghiệm toàn diện về FedTiny. Đầu tiên, chúng tôi giới thiệu thiết lập thực nghiệm và so sánh FedTiny với các phương pháp cơ sở khác. Thứ hai, chúng tôi tiến hành nghiên cứu ablation để chứng minh hiệu quả của mô-đun lựa chọn chuẩn hóa theo lô thích ứng và mô-đun tỉa cành tiến bộ. Thứ ba, chúng tôi điều tra chi phí phụ trong mô-đun chuẩn hóa theo lô thích ứng và tác động của chiến lược lập lịch tỉa cành. Thứ tư, chúng tôi chứng minh hiệu quả của FedTiny trên các phân phối dữ liệu không đồng nhất. Cuối cùng, chúng tôi so sánh hiệu suất giữa FedTiny và huấn luyện mô hình nhỏ.

A. Thiết lập thực nghiệm

1) Thiết lập học liên kết: Chúng tôi đánh giá FedTiny trên các nhiệm vụ phân loại hình ảnh với bốn tập dữ liệu, CIFAR-10, CIFAR-100 [41], CINIC-10 [42], và SVHN [43] trên các mô hình ResNet18 [16] và VGG11 [17]. Chúng tôi xem xét tổng cộng K = 10 thiết bị. Đối với tất cả các tập dữ liệu, chúng tôi đầu tiên tạo ra các phân vùng non-iid khác nhau trên các thiết bị từ phân phối Dirichlet với α = 0,5 và sau đó thay đổi α trong Phần IV-F, theo thiết lập trong [44]. Chúng tôi huấn luyện các mô hình trong 300 vòng FL trên các tập dữ liệu CIFAR-10, CIFAR-100, và CINIC-10 và 200 vòng trên tập dữ liệu SVHN. Mỗi vòng bao gồm 5 epoch cục bộ. Kích thước mini-batch được đặt là 64.

2) Thiết lập FedTiny: Chúng tôi sử dụng các thiết lập sau trong FedTiny. Trong mô-đun lựa chọn chuẩn hóa theo lô thích ứng, máy chủ tạo ra một nhóm ứng viên bằng tỉa cành theo độ lớn với các thiết lập tỷ lệ tỉa cành theo lớp khác nhau. Cho mật độ mục tiêu, dtarget, máy chủ xuất ra các ứng viên dưới dạng vector tỷ lệ tỉa cành theo lớp (d1, d2,..., dL) cho mô hình L lớp dựa trên chiến lược Uniform Noise. Chúng tôi suy ra mật độ dl cho lớp thứ l bằng cách cộng mật độ mục tiêu target với nhiễu ngẫu nhiên el, tức là dl = dtarget + el. Một ứng viên có thể được thêm vào nhóm ứng viên chỉ khi mật độ tổng thể d của nó thỏa mãn d ≤ dtarget. Sau đó, máy chủ có thể có được một nhóm ứng viên {θ(1),θ(2),...,θ(C)} với mặt nạ {m(1),m(2),...,m(C)}.

Chúng tôi đầu tiên đặt kích thước của nhóm ứng viên C là 50 và sau đó thay đổi kích thước nhóm ứng viên trong Phần IV-D. Chúng tôi đặt tỷ lệ của tập dữ liệu phát triển là 0,1, được sử dụng để cập nhật các phép đo chuẩn hóa theo lô cục bộ trên các thiết bị. Trong mô-đun tỉa cành tiến bộ, chúng tôi chia ResNet18 và VGG11 thành năm khối và tỉa cành một khối trong mỗi vòng, như được hiển thị trong Hình 2. Thứ tự mà máy chủ chọn một khối là ngược, tức là từ lớp đầu ra đến lớp đầu vào. Chúng tôi cũng đánh giá tỉa cành một lớp đơn và tỉa cành toàn bộ mô hình mỗi vòng trong Phần IV-E. Số lượng tỉa cành được đặt là alt = 0,15(1 + cosπt/RstopE)nl cho lớp l sẽ được tỉa cành tại lần lặp thứ t, trong đó nl là số lượng tham số không được tỉa cành trong lớp thứ l. Đối với lớp l sẽ không được tỉa cành trong lần lặp thứ t, alt = 0. Chúng tôi không tỉa cành lớp chuẩn hóa theo lô, bias, lớp đầu vào, và lớp đầu ra vì chúng ảnh hưởng trực tiếp đến đầu ra mô hình. FedTiny thực hiện ΔR = 10 vòng tinh chỉnh giữa hai lần tỉa cành tinh. Khi FedTiny đạt Rstop = 100 vòng, nó dừng tỉa cành và tiếp tục tinh chỉnh. FedTiny được triển khai trên FedML [45], một nền tảng học máy mã nguồn mở cho phép học liên kết nhẹ, đa nền tảng và bảo mật có thể chứng minh.

3) Thiết lập phương pháp cơ sở: Chúng tôi bao gồm các phương pháp cơ sở sau trong nghiên cứu. Chúng tôi bao gồm SNIP, SynFlow, và FL-PQSU để xác nhận rằng tỉa cành tại khởi tạo không phải là lựa chọn thiết kế tối ưu khi dữ liệu cục bộ không nhìn thấy được. Chúng tôi loại trừ các phương pháp tỉa cành FL không khả thi cho FL hạn chế bộ nhớ. Ví dụ, FedPrune [11] và ZeroFL [28] đòi hỏi các thiết bị mạnh để liên tục xử lý các mô hình dày đặc.

• SNIP [22] tỉa cành mô hình bằng độ nhạy cảm kết nối tại khởi tạo với một tập dữ liệu công khai nhỏ trên máy chủ.
• SynFlow [24] tỉa cành mô hình bằng cách bảo tồn lặp đi lặp lại dòng chảy synaptic trên máy chủ trước khi huấn luyện.
• FL-PQSU [8] tỉa cành mô hình theo cách một lần dựa trên chuẩn l1 trên máy chủ trước khi huấn luyện. FL-PQSU cũng bao gồm các phần lượng tử hóa và cập nhật có chọn lọc, nhưng chúng tôi chỉ sử dụng phần tỉa cành trong FL-PQSU.
• PruneFL [13] sử dụng một thiết bị mạnh để tỉa cành ban đầu mô hình và áp dụng tỉa cành tinh (tỉa cành thích ứng) trên mô hình thưa thớt dựa trên gradient trung bình kích thước đầy đủ. Nhưng tất cả các thiết bị đều hạn chế tài nguyên trong thiết lập của chúng tôi. Do đó, chúng tôi để PruneFL có được mô hình được tỉa cành ban đầu trên máy chủ với một tập dữ liệu công khai nhỏ.
• LotteryFL [10] tỉa cành lặp đi lặp lại mô hình dày đặc với tỷ lệ tỉa cành cố định trên các thiết bị và khởi tạo lại mô hình được tỉa cành với các giá trị ban đầu.
• FedDST [29] đầu tiên tỉa cành ngẫu nhiên một mô hình được tỉa cành ban đầu trên máy chủ, sau đó nó triển khai điều chỉnh mặt nạ trên các thiết bị, và máy chủ sử dụng tổng hợp thưa thớt và tỉa cành theo độ lớn để có được một mô hình toàn cục mới.

Vì SNIP [22] và PruneFL [13] đòi hỏi một số dữ liệu để tỉa cành thô, chúng tôi giả định rằng máy chủ cung cấp một tập dữ liệu công khai một lần Ds để tiền huấn luyện. Tất cả các phương pháp cơ sở bắt đầu với một mô hình được tiền huấn luyện với tập dữ liệu một lần Ds trên máy chủ. Đối với SNIP, chúng tôi áp dụng tỉa cành lặp đi lặp lại thay vì tỉa cành một lần như [24] đã chỉ ra. Tương tự, chúng tôi để SynFlow tỉa cành mô hình tại khởi tạo đến mật độ mục tiêu theo cách lặp đi lặp lại. Đối với SNIP và SynFlow, chúng tôi đặt 100 epoch tỉa cành trên máy chủ tại khởi tạo; tham khảo [24]. Đối với FL-PQSU, ban đầu là tỉa cành có cấu trúc, chúng tôi thay đổi nó thành tỉa cành không có cấu trúc vì tất cả các phương pháp cơ sở khác là các khung tỉa cành không có cấu trúc. LotteryFL [10] được thiết kế cho học liên kết được cá nhân hóa, vì vậy các cấu trúc mô hình khác nhau giữa các thiết bị. Vì chúng tôi cố gắng tìm một cấu trúc tối ưu cho tất cả các thiết bị như trong Eq. 1, chúng tôi để LotteryFL tỉa cành lặp đi lặp lại mô hình toàn cục thay vì các mô hình trên thiết bị để đảm bảo cùng một cấu trúc mô hình cho mỗi thiết bị. FedDST [29] triển khai điều chỉnh mặt nạ trên các thiết bị và tinh chỉnh các tham số trước khi tải lên. Chúng tôi để FedDST điều chỉnh các mặt nạ sau 3 epoch huấn luyện cục bộ, theo sau bởi 2 epoch tinh chỉnh. Vì LotteryFL, PruneFL, FedDST, và FedTiny của chúng tôi đều tỉa cành lặp đi lặp lại trong quá trình huấn luyện, chúng tôi sử dụng cùng một lịch trình tỉa cành cho các khung này, nơi khung thực hiện ΔR = 10 vòng tinh chỉnh giữa hai lần tỉa cành tinh. Và khung dừng tỉa cành và tiếp tục tinh chỉnh sau Rstop = 100 vòng. Đối với PruneFL và FedDST, chúng tôi đặt số lượng tỉa cành alt giống như trong FedTiny. Tất cả các phương pháp cơ sở sẽ áp dụng phân phối độ thưa thớt đồng nhất cho thiết lập tỷ lệ tỉa cành theo lớp.

B. So sánh giữa FedTiny và các phương pháp cơ sở

Để hiển thị hiệu suất của FedTiny dưới các mật độ khác nhau, chúng tôi so sánh các phương pháp cơ sở và FedTiny trên bốn tập dữ liệu (CIFAR-10, CIFAR-100, CINIC-10, và SVHN) với ResNet18. Như được hiển thị trong Hình 3, FedTiny vượt trội hơn các phương pháp cơ sở khác trong chế độ mật độ thấp (dtarget < 10^-2), ví dụ, FedTiny đạt được cải thiện độ chính xác 18,91% trong tập dữ liệu SVHN so với các phương pháp hiện đại với mật độ 10^-3. Điều này có lợi từ mô-đun lựa chọn chuẩn hóa theo lô thích ứng được sử dụng trong FedTiny, điều này suy ra một cấu trúc tỉa cành thô thích ứng trên máy chủ. Cấu trúc tỉa cành ban đầu này có ít độ lệch hơn, giảm kích thước của không gian tìm kiếm và cải thiện hội tụ. Bên cạnh đó, FedTiny có thể cạnh tranh với mật độ cao (dtarget > 10^-2), ví dụ, FedTiny vượt trội hơn các phương pháp hiện đại với mật độ 10^-1 bởi 1,3% trong tập dữ liệu CIFAR-10. Mặc dù PruneFL có thể vượt trội một phần so với FedTiny dưới mật độ cao, nó đòi hỏi hơn 20× chi phí tính toán và 15× dung lượng bộ nhớ để xử lý điểm quan trọng dày đặc trên các thiết bị. SNIP hoạt động kém ở mật độ thấp vì SNIP có xu hướng loại bỏ gần như tất cả các tham số trong một số lớp. Hơn nữa, mô hình được tỉa cành trong SNIP phụ thuộc rất nhiều vào các mẫu trên máy chủ, điều này tăng độ lệch do non-iid. Chúng tôi không bao gồm LotteryFL trong Hình 3 vì việc sử dụng LotteryFL đòi hỏi việc huấn luyện một mô hình lớn, điều này gây ra chi phí tính toán và dung lượng bộ nhớ đáng kể. Tuy nhiên, kết quả của LotteryFL được bao gồm trong Bảng I để cung cấp so sánh toàn diện với các phương pháp cơ sở khác.

Để hiển thị hiệu quả của FedTiny, chúng tôi đo chi phí huấn luyện ResNet18 và VGG11 với các mật độ khác nhau trên tập dữ liệu CIFAR-10. Chúng tôi sử dụng số lượng phép toán điểm thả nổi (FLOPs) để đo chi phí tính toán cho mỗi thiết bị. Hoạt động tỉa cành đòi hỏi một lượng tính toán biến đổi mỗi vòng, dẫn đến FLOPs huấn luyện biến đổi mỗi vòng. Do đó, chúng tôi báo cáo FLOPs huấn luyện tối đa mỗi vòng (Max Training FLOPs). FLOPs huấn luyện tối đa mỗi vòng được sử dụng để đánh giá liệu các thiết bị có gặp phải tính toán tốn kém trong một vòng đơn lẻ hay không. Chúng tôi cũng báo cáo dung lượng bộ nhớ trong các thiết bị, liên quan đến chi phí bộ nhớ trong triển khai.

Bảng I hiển thị độ chính xác và chi phí huấn luyện của FedTiny được đề xuất và các phương pháp cơ sở khác với các mật độ và mô hình khác nhau. Chúng tôi đánh dấu metric tốt nhất bằng màu đỏ và metric tốt thứ hai bằng màu xanh dương. Tất cả các phép đo chi phí đều cho một thiết bị trong một vòng tỉa cành. Chúng tôi cũng báo cáo hiệu suất của FedAvg để hiển thị giới hạn trên của các phương pháp tỉa cành. Như được hiển thị trong Bảng I, FedTiny nhằm cải thiện cả độ chính xác và hiệu quả bộ nhớ. Các công việc hiện tại không thể đạt được độ chính xác thỏa đáng ở mật độ cực thấp. FedTiny được đề xuất của chúng tôi cải thiện đáng kể độ chính xác với mức độ FLOPs và dung lượng bộ nhớ thấp nhất.

C. Nghiên cứu Ablation

Phần này thảo luận về hiệu quả của mỗi mô-đun trong FedTiny thông qua các nghiên cứu ablation. Chúng tôi đánh giá lựa chọn vanilla, lựa chọn chuẩn hóa theo lô thích ứng, tỉa cành tiến bộ sau lựa chọn vanilla, và FedTiny trên tập dữ liệu CIFAR-10 với mô hình VGG11. Hình 4 hiển thị kết quả của mỗi mô-đun hoạt động riêng lẻ. Chúng tôi có ba phát hiện sau. Đầu tiên, cả mô-đun lựa chọn chuẩn hóa theo lô thích ứng và mô-đun tỉa cành tiến bộ đều cải thiện hiệu suất trong lựa chọn vanilla, cho thấy hiệu quả của hai mô-đun này. Thứ hai, một mô hình tỉa cành thô từ lựa chọn chuẩn hóa theo lô thích ứng đối mặt với sự giảm độ chính xác so với FedTiny, cho thấy vẫn còn một số độ lệch trong mô hình tỉa cành thô được chọn, và mô-đun tỉa cành tiến bộ có thể loại bỏ chúng. Cuối cùng, mô-đun tỉa cành tiến bộ với lựa chọn vanilla đạt được cùng mức độ chính xác so với FedTiny với mật độ cao (< 10^-2). Tuy nhiên, nó gặp phải sự suy giảm nghiêm trọng về độ chính xác trong chế độ mật độ thấp (> 10^-2), điều này cho thấy mô-đun tỉa cành tiến bộ chỉ loại bỏ độ lệch đến một mức độ nhất định, và nó phải được kết hợp với mô-đun lựa chọn chuẩn hóa theo lô thích ứng trong chế độ mật độ thấp. Do đó, việc sử dụng độc lập mô-đun lựa chọn chuẩn hóa theo lô thích ứng và mô-đun tỉa cành tiến bộ có thể cải thiện hiệu suất, nhưng sự cải thiện là hạn chế. Sự kết hợp của hai mô-đun, tức là FedTiny, đạt được hiệu suất dự đoán tốt nhất với mô hình nhỏ.

D. Chi phí phụ trong mô-đun lựa chọn BN thích ứng

Mặc dù một nhóm ứng viên lớn hơn cung cấp nhiều lựa chọn hơn để lựa chọn, nó mang lại nhiều chi phí truyền thông hơn trong mô-đun lựa chọn chuẩn hóa theo lô thích ứng. Vì vậy, chúng tôi muốn tìm một kích thước nhóm tối ưu có thể đánh đổi giữa độ chính xác và chi phí phụ trong mô-đun lựa chọn chuẩn hóa theo lô thích ứng. Do đó, chúng tôi đánh giá FedTiny trên VGG11 với các kích thước nhóm khác nhau để tìm kích thước nhóm tối ưu. Chúng tôi thực hiện các thí nghiệm trên tập dữ liệu CIFAR-10 với mô hình VGG11 với các kích thước nhóm và mật độ khác nhau. Kết quả thí nghiệm được hiển thị trong Hình 5. Kết quả cho thấy việc tăng kích thước nhóm vượt quá đường xanh lá cây có thể chỉ mang lại sự tăng nhẹ về độ chính xác, trong khi tăng đáng kể chi phí tính toán. Do đó, đường xanh lá cây phục vụ như một ngưỡng thực tế để chọn kích thước nhóm ứng viên tối ưu. Do đó, kích thước nhóm tối ưu được chọn là C* = 0,1/dtarget cho mật độ cụ thể dtarget, trong đó chi phí truyền thông trong mô-đun lựa chọn chuẩn hóa theo lô thích ứng thấp đến 20% so với một mô hình VGG11 kích thước đầy đủ, và FedTiny có thể nhận được độ chính xác tương đối tốt. Kích thước nhóm lớn hơn > C* cải thiện độ chính xác một chút nhưng gây ra chi phí truyền thông cao hơn nhiều.

Chúng tôi cũng tính toán FLOPs phụ cho mô-đun lựa chọn chuẩn hóa theo lô thích ứng với kích thước nhóm tối ưu, như được hiển thị trong Bảng II. FLOPs phụ trong lựa chọn chuẩn hóa theo lô thích ứng ít hơn một vòng huấn luyện thưa thớt. Vì học liên kết thường liên quan đến hơn một trăm vòng huấn luyện, chi phí tính toán phụ có thể bỏ qua được. Do đó, chúng tôi cho rằng chi phí phụ do lựa chọn chuẩn hóa theo lô thích ứng gây ra là nhỏ.

E. Tác động của chiến lược lập lịch tỉa cành

Mặc dù điều chỉnh theo lớp trong tỉa cành tiến bộ giảm chi phí tính toán trong một vòng, nó có thể làm chậm tốc độ hội tụ. Để xác định độ chi tiết tỉa cành và tần suất tỉa cành tốt nhất, chúng tôi đánh giá FedTiny trên VGG11 với các độ chi tiết tỉa cành khác nhau (một lớp mỗi vòng, một khối mỗi vòng, và toàn bộ mô hình mỗi vòng) và các tần suất tỉa cành khác nhau.

Bảng III hiển thị độ chính xác top-1 của các lịch trình tỉa cành khác nhau dưới các mật độ khác nhau trên VGG11 với tập dữ liệu CIFAR-10, trong đó hiệu suất tốt nhất của độ chính xác Top-1 với cùng mật độ được biểu thị bằng màu đỏ, và metric tốt thứ hai được đánh dấu bằng màu xanh dương. b biểu thị việc chọn tuần tự các lớp hoặc khối để tỉa cành theo thứ tự ngược, tức là từ lớp đầu ra đến lớp đầu vào. Chúng tôi kiểm soát tần suất tỉa cành bằng cách đặt các vòng khoảng cách khác nhau ΔR giữa hai lần tỉa cành. Nếu độ chi tiết tỉa cành quá nhỏ (ví dụ, tỉa cành theo lớp), cấu trúc mô hình sẽ hội tụ chậm, và cấu trúc tối ưu không thể đạt được với tài nguyên huấn luyện hạn chế. Nhưng độ chi tiết cập nhật cao dẫn đến tính toán tốn kém hơn trong một vòng. Chúng tôi thấy rằng tỉa cành một khối mỗi vòng là lựa chọn tối ưu cho mô-đun tỉa cành tiến bộ. Hơn nữa, việc chọn tuần tự các khối để tỉa cành theo thứ tự ngược (từ lớp đầu ra đến lớp đầu vào) có kết quả tốt hơn so với thứ tự thuận vì lan truyền gradient là ngược, và chúng tôi sử dụng gradient để điều chỉnh cấu trúc mô hình.

F. Hiệu quả của FedTiny trên các phân phối dữ liệu không đồng nhất

Tỉa cành mạng nơ-ron đòi hỏi dữ liệu huấn luyện để xác định cấu trúc mô hình phù hợp. Do các thiết bị hạn chế tài nguyên, máy chủ không thể đẩy mô hình dày đặc đến các thiết bị. Do đó, máy chủ cần tỉa cành thô để tạo ra mô hình được tỉa cành ban đầu. Do mối quan tâm về quyền riêng tư trong học liên kết, máy chủ không thể biết các phân phối dữ liệu cho tất cả các thiết bị. Trong các phương pháp hiện tại, máy chủ chỉ tỉa cành thô mô hình dựa trên tập dữ liệu được tiền huấn luyện hoặc dữ liệu từ một số thiết bị đáng tin cậy. Điều này khiến tập dữ liệu được sử dụng để tỉa cành khác với tập dữ liệu được sử dụng để tinh chỉnh, gây ra độ lệch trong tỉa cành thô. Do đó, chiến lược của chúng tôi là sử dụng lựa chọn chuẩn hóa theo lô thích ứng để chọn một mô hình được tỉa cành với ít độ lệch hơn.

Để chứng minh hiệu quả của FedTiny trên các phân phối dữ liệu không đồng nhất, chúng tôi đặt các mức độ non-iid khác nhau bằng cách sử dụng α khác nhau trong phân phối Dirichlet. α thấp hơn cho thấy mức độ non-iid cao hơn. Chúng tôi thực hiện thí nghiệm trên tập dữ liệu CIFAR-10 với ResNet18 với mật độ 1%. Các thí nghiệm được hiển thị trong Hình 6. Các thí nghiệm của chúng tôi cho thấy rằng 1) hiệu suất của các phương pháp tỉa cành hiện tại (ví dụ, SynFlow, PruneFL) trong Học liên kết sẽ bị suy giảm đáng kể khi cho mức độ non-iid cao hơn; 2) FedTiny được đề xuất của chúng tôi giảm thiểu độ lệch trong tỉa cành và đạt được hiệu suất tốt nhất so với các phương pháp tỉa cành hiện tại.

G. So sánh giữa FedTiny và huấn luyện mô hình nhỏ

Trong các thí nghiệm trước, chúng tôi so sánh FedTiny với các phương pháp tỉa cành hiện tại. Để điều tra thêm hiệu quả của FedTiny, chúng tôi so sánh FedTiny với các mô hình nhỏ dày đặc mà không có tỉa cành. FedTiny vượt trội hơn các phương pháp cơ sở khác trong mô hình rất thưa thớt, như mật độ 1%. Trong trường hợp này, huấn luyện một mô hình nhỏ dày đặc mà không có tỉa cành cũng có thể được coi là một phương pháp cơ sở. Do đó, chúng tôi thiết kế các thí nghiệm trên các mô hình nhỏ. Chúng tôi huấn luyện một mô hình nhỏ với ba lớp tích chập. Đầu tiên, chúng tôi đánh giá mô hình nhỏ với số lượng tham số tương tự như ResNet18 với mật độ 1% trên các tập dữ liệu khác nhau. Thứ hai, chúng tôi đánh giá mạng nhỏ với số lượng tham số tương tự như ResNet18 với các mật độ khác nhau trên CIFAR-10. Chúng tôi cũng chọn SynFlow và PruneFL làm tham chiếu. Kết quả thí nghiệm được hiển thị trong Bảng IV và Bảng V, trong đó hiệu suất tốt nhất của độ chính xác Top-1 với cùng tập dữ liệu và cùng mật độ được biểu thị bằng màu đỏ, và metric tốt thứ hai được đánh dấu bằng màu xanh dương. Kết quả thí nghiệm cho thấy mô hình nhỏ có thể cạnh tranh so với các phương pháp cơ sở khác. Tuy nhiên, FedTiny được đề xuất của chúng tôi đạt được hiệu suất tốt hơn nhiều so với mô hình nhỏ, điều này chứng minh lợi thế của FedTiny.

V. KẾT LUẬN

Bài báo này phát triển một khung tỉa cành phân tán mới có tên FedTiny. FedTiny cho phép huấn luyện cục bộ tiết kiệm bộ nhớ và xác định các mô hình nhỏ chuyên biệt trong học liên kết cho các tình huống triển khai khác nhau (các nền tảng phần cứng tham gia và nhiệm vụ huấn luyện). FedTiny giải quyết các thách thức về độ lệch, tính toán tốn kém và sử dụng bộ nhớ mà nghiên cứu tỉa cành liên kết hiện tại gặp phải. FedTiny giới thiệu hai mô-đun quan trọng: một mô-đun lựa chọn chuẩn hóa theo lô thích ứng và một mô-đun tỉa cành tiến bộ nhẹ. Mô-đun lựa chọn chuẩn hóa theo lô được thiết kế để giảm thiểu độ lệch trong tỉa cành gây ra bởi tính không đồng nhất của dữ liệu cục bộ, trong khi mô-đun tỉa cành tiến bộ cho phép tỉa cành tinh dưới ngân sách tính toán và bộ nhớ nghiêm ngặt. Cụ thể, nó xác định dần dần chính sách tỉa cành cho mỗi lớp thay vì đánh giá toàn bộ cấu trúc mô hình. Kết quả thí nghiệm chứng minh hiệu quả của FedTiny khi so sánh với các phương pháp hiện đại. Đặc biệt, FedTiny đạt được những cải thiện đáng kể về độ chính xác, FLOPs và dung lượng bộ nhớ khi nén các mô hình sâu thành các mô hình nhỏ cực kỳ thưa thớt. Kết quả trên tập dữ liệu CIFAR-10 cho thấy FedTiny vượt trội hơn các phương pháp hiện đại bằng cách đạt được cải thiện độ chính xác 2,61% trong khi đồng thời giảm FLOPs 95,9% và dung lượng bộ nhớ 94,0%. Kết quả thí nghiệm chứng minh hiệu quả và hiệu suất của FedTiny trong thiết lập học liên kết.

VI. LỜI CẢM ƠN

Công việc này được hỗ trợ một phần bởi Quỹ Khoa học Quốc gia (CCF-2221741, CCF-2106754, CNS-2151238, CNS-2153381), Giải thưởng Nâng cao Năng lực Giảng viên Trẻ Ralph E. Powe của ORAU, và Hội đồng Tài trợ Nghiên cứu Hồng Kông, Quỹ Nghiên cứu Tổng quát (GRF) dưới Grant 11203523.

TÀI LIỆU THAM KHẢO

[1] S. A. Janowsky, "Pruning versus clipping in neural networks," Physical Review A, vol. 39, no. 12, p. 6600, 1989.
[2] S. Han, H. Mao, và W. J. Dally, "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding," arXiv preprint arXiv:1510.00149, 2015.
[3] C. Louizos, M. Welling, và D. P. Kingma, "Learning sparse neural networks through l0 regularization," trong International Conference on Learning Representations, 2018.
[4] R. Yu, A. Li, C.-F. Chen, J.-H. Lai, V. I. Morariu, X. Han, M. Gao, C.-Y. Lin, và L. S. Davis, "Nisp: Pruning networks using neuron importance score propagation," trong Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9194–9203, 2018.
[5] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, và J. Kautz, "Importance estimation for neural network pruning," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11264–11272, 2019.
[6] S. P. Singh và D. Alistarh, "Woodfisher: Efficient second-order approximation for neural network compression," Advances in Neural Information Processing Systems, vol. 33, pp. 18098–18109, 2020.
[7] T. Li, A. K. Sahu, A. Talwalkar, và V. Smith, "Federated learning: Challenges, methods, and future directions," IEEE Signal Processing Magazine, vol. 37, no. 3, pp. 50–60, 2020.
[8] W. Xu, W. Fang, Y. Ding, M. Zou, và N. Xiong, "Accelerating federated learning for iot in big data analytics with pruning, quantization and selective updating," IEEE Access, vol. 9, pp. 38457–38466, 2021.
[9] R. Shao, H. Liu, và D. Liu, "Privacy preserving stochastic channel-based federated learning with neural network pruning," arXiv preprint arXiv:1910.02115, 2019.
[10] A. Li, J. Sun, B. Wang, L. Duan, S. Li, Y. Chen, và H. Li, "Lotteryfl: Empower edge intelligence with personalized and communication-efficient federated learning," trong 2021 IEEE/ACM Symposium on Edge Computing (SEC), pp. 68–79, IEEE, 2021.
[11] M. T. Munir, M. M. Saeed, M. Ali, Z. A. Qazi, và I. A. Qazi, "Fedprune: Towards inclusive federated learning," arXiv preprint arXiv:2110.14205, 2021.
[12] S. Liu, G. Yu, R. Yin, và J. Yuan, "Adaptive network pruning for wireless federated learning," IEEE Wireless Communications Letters, vol. 10, no. 7, pp. 1572–1576, 2021.
[13] Y. Jiang, S. Wang, V. Valls, B. J. Ko, W.-H. Lee, K. K. Leung, và L. Tassiulas, "Model pruning enables efficient federated learning on edge devices," IEEE Transactions on Neural Networks and Learning Systems, 2022.
[14] U. Evci, T. Gale, J. Menick, P. S. Castro, và E. Elsen, "Rigging the lottery: Making all tickets winners," trong International Conference on Machine Learning, pp. 2943–2952, PMLR, 2020.
[15] S. Ioffe và C. Szegedy, "Batch normalization: Accelerating deep network training by reducing internal covariate shift," trong International conference on machine learning, pp. 448–456, PMLR, 2015.
[16] K. He, X. Zhang, S. Ren, và J. Sun, "Deep residual learning for image recognition," trong Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.
[17] K. Simonyan và A. Zisserman, "Very deep convolutional networks for large-scale image recognition," arXiv preprint arXiv:1409.1556, 2014.
[18] T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, và A. Peste, "Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks," Journal of Machine Learning Research, vol. 22, no. 241, pp. 1–124, 2021.
[19] M. C. Mozer và P. Smolensky, "Skeletonization: A technique for trimming the fat from a network via relevance assessment," Advances in neural information processing systems, vol. 1, 1988.
[20] Y. LeCun, J. Denker, và S. Solla, "Optimal brain damage," Advances in neural information processing systems, vol. 2, 1989.
[21] P. Molchanov, S. Tyree, T. Karras, T. Aila, và J. Kautz, "Pruning convolutional neural networks for resource efficient inference," trong 5th International Conference on Learning Representations, ICLR 2017-Conference Track Proceedings, 2019.
[22] N. Lee, T. Ajanthan, và P. Torr, "Snip: Single-shot network pruning based on connection sensitivity," trong International Conference on Learning Representations, 2018.
[23] C. Wang, G. Zhang, và R. Grosse, "Picking winning tickets before training by preserving gradient flow," trong International Conference on Learning Representations, 2019.
[24] H. Tanaka, D. Kunin, D. L. Yamins, và S. Ganguli, "Pruning neural networks without any data by iteratively conserving synaptic flow," Advances in Neural Information Processing Systems, vol. 33, pp. 6377–6389, 2020.
[25] D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu, và A. Liotta, "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science," Nature communications, vol. 9, no. 1, pp. 1–12, 2018.
[26] T. Dettmers và L. Zettlemoyer, "Sparse networks from scratch: Faster training without losing performance," arXiv preprint arXiv:1907.04840, 2019.
[27] B. McMahan, E. Moore, D. Ramage, S. Hampson, và B. A. y Arcas, "Communication-efficient learning of deep networks from decentralized data," trong Artificial intelligence and statistics, pp. 1273–1282, PMLR, 2017.
[28] X. Qiu, J. Fernandez-Marques, P. P. Gusmao, Y. Gao, T. Parcollet, và N. D. Lane, "Zerofl: Efficient on-device training for federated learning with local sparsity," arXiv preprint arXiv:2208.02507, 2022.
[29] S. Bibikar, H. Vikalo, Z. Wang, và X. Chen, "Federated dynamic sparse training: Computing less, communicating less, yet learning better," trong Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, pp. 6080–6088, 2022.
[30] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, và V. Chandra, "Federated learning with non-iid data," arXiv preprint arXiv:1806.00582, 2018.
[31] O. Marfoq, C. Xu, G. Neglia, và R. Vidal, "Throughput-optimal topology design for cross-silo federated learning," Advances in Neural Information Processing Systems, vol. 33, pp. 19478–19487, 2020.
[32] J. Wang, Q. Liu, H. Liang, G. Joshi, và H. V. Poor, "Tackling the objective inconsistency problem in heterogeneous federated optimization," Advances in neural information processing systems, vol. 33, pp. 7611–7623, 2020.
[33] M. Duan, D. Liu, X. Chen, Y. Tan, J. Ren, L. Qiao, và L. Liang, "Astraea: Self-balancing federated learning for improving classification accuracy of mobile deep learning applications," trong 2019 IEEE 37th international conference on computer design (ICCD), pp. 246–254, IEEE, 2019.
[34] Z. Li, Y. He, H. Yu, J. Kang, X. Li, Z. Xu, và D. Niyato, "Data heterogeneity-robust federated learning via group client selection in industrial iot," IEEE Internet of Things Journal, 2022.
[35] W. Zhang, X. Wang, P. Zhou, W. Wu, và X. Zhang, "Client selection for federated learning with non-iid data in mobile edge computing," IEEE Access, vol. 9, pp. 24462–24474, 2021.
[36] F. Chen, M. Luo, Z. Dong, Z. Li, và X. He, "Federated meta-learning with fast convergence and efficient communication," arXiv preprint arXiv:1802.07876, 2018.
[37] V. Smith, C.-K. Chiang, M. Sanjabi, và A. S. Talwalkar, "Federated multi-task learning," Advances in neural information processing systems, vol. 30, 2017.
[38] T. Lin, L. Kong, S. U. Stich, và M. Jaggi, "Ensemble distillation for robust model fusion in federated learning," Advances in Neural Information Processing Systems, vol. 33, pp. 2351–2363, 2020.
[39] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, và S. Han, "Amc: Automl for model compression and acceleration on mobile devices," trong Proceedings of the European conference on computer vision (ECCV), pp. 784–800, 2018.
[40] B. Li, B. Wu, J. Su, và G. Wang, "Eagleeye: Fast sub-net evaluation for efficient neural network pruning," trong European conference on computer vision, pp. 639–654, Springer, 2020.
[41] A. Krizhevsky, G. Hinton, et al., "Learning multiple layers of features from tiny images," 2009.
[42] L. N. Darlow, E. J. Crowley, A. Antoniou, và A. J. Storkey, "Cinic-10 is not imagenet or cifar-10," arXiv preprint arXiv:1810.03505, 2018.
[43] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, và A. Y. Ng, "Reading digits in natural images with unsupervised feature learning," 2011.
[44] M. Luo, F. Chen, D. Hu, Y. Zhang, J. Liang, và J. Feng, "No fear of heterogeneity: Classifier calibration for federated learning with non-iid data," Advances in Neural Information Processing Systems, vol. 34, pp. 5972–5984, 2021.
[45] C. He, S. Li, J. So, X. Zeng, M. Zhang, H. Wang, X. Wang, P. Vepakomma, A. Singh, H. Qiu, et al., "Fedml: A research library and benchmark for federated machine learning," arXiv preprint arXiv:2007.13518, 2020.
