# 2111.13445.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2111.13445.pdf
# File size: 652960 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
How Well Do Sparse ImageNet Models Transfer?
Eugenia Ioﬁnova*
IST AustriaAlexandra Peste*
IST AustriaMark Kurtz
Neural MagicDan Alistarh
IST Austria & Neural Magic
Abstract
Transfer learning is a classic paradigm by which mod-
els pretrained on large “upstream” datasets are adapted to
yield good results on “downstream” specialized datasets.
Generally, more accurate models on the “upstream”
dataset tend to provide better transfer accuracy “down-
stream”. In this work, we perform an in-depth investi-
gation of this phenomenon in the context of convolutional
neural networks (CNNs) trained on the ImageNet dataset,
which have been pruned—that is, compressed by sparsiﬁy-
ing their connections. We consider transfer using unstruc-
tured pruned models obtained by applying several state-
of-the-art pruning methods, including magnitude-based,
second-order, re-growth, lottery-ticket, and regularization
approaches, in the context of twelve standard transfer tasks.
In a nutshell, our study shows that sparse models can match
or even outperform the transfer performance of dense mod-
els, even at high sparsities, and, while doing so, can lead
to signiﬁcant inference and even training speedups. At the
same time, we observe and analyze signiﬁcant differences
in the behaviour of different pruning methods.
1. Introduction
The large computational costs of deep learning have
led to signiﬁcant academic and industrial interest in model
compression , deﬁned roughly as obtaining smaller-footprint
models matching the accuracy of larger models. Model
compression is a rapidly-developing area, and several gen-
eral approaches have been investigated, of which pruning
and quantization are among the most popular [18, 28].
We focus our present study on weight pruning , whose
objective is to remove, by setting to zero, as many weights
as possible without losing model accuracy. Weight pruning
is, arguably, the compression method with the richest his-
tory [42] and is currently a very active research topic [28].
Thanks to this trend, a set of fairly consistent accuracy
benchmarks has emerged for pruning, along with increas-
ingly efﬁcient computational support [11, 20, 40, 52].
*These authors contributed equally. Correspondence to:
feugenia.iofinova, alexandra.peste g@ist.ac.atOne major goal of model compression is to enable de-
ployment on edge devices. Such devices may naturally en-
counter different data distributions, so it is tempting to ask
how compressed models would perform for transfer learn-
ing, broadly deﬁned as leveraging information from some
baseline “upstream” (“pretrained”) task in order to per-
form better on a “downstream” (“ﬁnetuning”) task. Specif-
ically, we mainly focus on a prototypical transfer learning
setup [36]: starting from models trained and compressed on
the ImageNet-1K dataset [60], we reﬁne the resulting mod-
els onto several different target tasks. In this context, we ex-
amine the question of how well the resulting sparse models
transfer. Our motivation is both practical—sparse transfer
can provide speedups for both inference and training on the
downstream model—and analytical, as we aim to shed light
on the impact of sparsity on the resulting features.
Our study will consider two common transfer learning
variants: full ﬁnetuning , where all unpruned weights can be
optimized during transfer, and linear ﬁnetuning , where only
the ﬁnal linear layer of the model is ﬁnetuned downstream.
While both are popular, we will see that they can lead to
different results. We additionally explore inference-time
speedups using a sparsity-aware inference engine [9], and
for the ﬁrst time examine training-time speed-up achievable
for linear ﬁnetuning via sparse models. Furthermore, we
analyze the impact of different pruning methods and task
characteristics on transfer performance.
We consider the top-performing pruning methods in
terms of ImageNet accuracy, roughly split into three cat-
egories. The ﬁrst is given by progressive sparsiﬁcation
methods, which start from an accurate dense baseline and
proceed to gradually remove weights, followed by ﬁnetun-
ing. The prototypical example is gradual magnitude prun-
ing (GMP) [17,22,23,69], which uses absolute weight mag-
nitude as the pruning criterion. In addition, we examine
WoodFisher pruning [63], which leverages second-order in-
formation for highly-accurate pruning.
The second rough category is given by sparse regu-
larized training methods, which perform network com-
pression, and possibly network re-growth, during the
training process itself. The top-performing methods
1arXiv:2111.13445v5  [cs.CV]  21 Apr 2022

--- PAGE 2 ---
we consider here are Soft Threshold Reparametrization
(STR) [41], Alternating Compressed/DeCompressed Train-
ing (AC/DC) [57] and “The Rigged Lottery” (RigL) [12].
The ﬁnal category comprises Lottery Ticket Hypothesis
(LTH)-style methods [5, 6, 14, 15]. These methods empha-
size the discovery of sparse sub-networks, which can yield
good accuracy when re-trained from scratch. Speciﬁcally,
we consider LTH for transfer (LTH-T) [5], which provides
state-of-the-art results among such methods.
We measure the transfer accuracy of sparse ImageNet
models obtained via these pruning methods. Our main tar-
get application is given by twelve classic transfer datasets,
described in Table 2, ranging from general datasets, to
more specialized ones. We mainly focus on the clas-
sic ResNet50 [26] model, but we extend our analysis to
ResNet18, ResNet34 and MobileNet-V1 [31], and we also
examine transfer performance for object detection tasks.
Contribution. We present the ﬁrst systematic study of how
different pruning and transfer approaches impact transfer
performance. Our main ﬁnding is that sparse models can
consistently match the accuracy of the corresponding dense
models on transfer tasks. However, this behaviour is im-
pacted by the following factors: pruning method (e.g. regu-
larization vs. progressive pruning), transfer approach (full
vs. linear), model sparsity (e.g. moderate 80% vs. high
98% sparsity), and task type (e.g. degree of specialization).
We brieﬂy outline our main conclusions, summarized in
Figure 1 and Table 1. For linear ﬁnetuning , sparse models
usually match and can slightly outperform dense models.
Yet, this is not true for all pruning methods: regularization-
based methods perform particularly well, even at high
sparsities (e.g. 95%). For full ﬁnetuning , which gen-
erally provides higher accuracies [36], sparse models are
also competitive with dense ones, but transfer accuracy is
more tightly correlated with accuracy on the ImageNet pre-
training task: consequently, less sparse models (e.g. 80%-
90% sparsity) tend to be more accurate than sparser ones.
Moreover, in this setting we ﬁnd that progressive sparsi-
ﬁcation methods consistently produce models with higher
transfer accuracy, relative to regularization methods. We
provide a ﬁrst analysis of this effect, linking it to structural
properties of the pruned models. In addition, we observe the
markedly lower accuracy of lottery-ticket approaches, espe-
cially at the higher levels of sparsity, e.g. 90%, required
for computational speedups.
Given the difference in behaviour between linear and
full ﬁnetuning, we ﬁnd that there is currently no single
“best” pruning method for transfer. However, using existing
methods, one can consistently achieve order-of-magnitude
(90%) compression without loss of accuracy. In turn,
these compression levels can lead to speedups of more than
3on sparsity-enabled runtimes. This suggests that sparse
transfer may have signiﬁcant practical potential.
                                                     Transfer learning task 
   Hardware or training   
    time constraints    No hardware or training 
          time constraints 
  Specialized task 
  (fine-grained) 
Sparse 
regularization 
AC/DC, STR, RigL Progressive 
sparsification 
GMP, WoodFisher Linear finetuning Full finetuning 
General task 
     (coarse-grained) 
Any pruning 
strategy Figure 1. Overview of a suggested decision process when selecting
the ﬁnetuning and pruning methods to maximize performance and
accuracy when doing transfer learning on pruned models.
2. Background and Related Work
2.1. Sparsiﬁcation Techniques
Recently, there has been signiﬁcant research interest in
pruning techniques, and hundreds of different sparsiﬁcation
approaches have been proposed; please see the recent sur-
veys of [17] and [28] for a comprehensive exposition. We
roughly categorize existing pruning methods as follows.
Progressive Sparsiﬁcation Methods start from an accu-
rate dense baseline model, and remove weights progres-
sively in several steps, separated by ﬁnetuning periods,
which are designed to recover accuracy. A classic instance
is gradual magnitude pruning (GMP) [17,22,23,69], which
progressively removes weights by their absolute magnitude,
measured either globally or per layer. Second-order pruning
methods, e.g. [16, 24, 42, 63, 65] augment this basic metric
with second-order information, which can lead to higher ac-
curacy of the resulting pruned models, relative to GMP.
Regularization Methods are usually applied during
model training, via sparsity-promoting mechanisms. These
mechanisms are very diverse, from surrogates of `0and`1-
regularization [41,67], to variational methods [53], to meth-
ods inspired by compressive sensing mechanisms such as
Iterative Hard Thresholding (IHT) [32, 33, 45, 57]. We also
consider the “The Rigged Lottery” (RigL) method [12],
which achieves close to state-of-the-art ImageNet results by
allowing for dynamic weight pruning and re-introduction
with long ﬁnetuning periods, to be a regularization method.
Lottery Ticket Hypothesis (LTH) Methods [14] start from
a fully-trained model, and often apply pruning in a single or
in multiple incremental steps to obtain a sparse mask over
the weights. They then restart training, but restricted to the
given mask. Training restarts either from intialization [14],
or by “rewinding” to an earlier point during training of the
dense model [6, 7, 15]. (The random mask–initialization
combination is thought of as the “lottery ticket”.) Rewind-
2

--- PAGE 3 ---
Finetuning Sparsity Aircraft Birds Caltech-101 Caltech-256 Cars CIFAR-10 CIFAR-100 DTD Flowers Food-101 Pets SUN397
Linear 0% 49.2 0.1 57.7 0.1 91.9 0.1 84.80.1 53.40.1 91.2 0. 74.60.1 73.50.2 91.6 0.1 73.2 0. 92.60.1 60.10.
80% 55.2 0.2 58.4 0. 92.40.2 84.6 0.1 58.60.1 91.40. 74.7 0.1 74.4 0.1 93.0 0. 73.9 0. 92.5 0.1 60.4 0.
90% 56.60.1 58.7 0. 92.5 0.1 84.50.1 60.50.1 91.00. 74.3 0. 73.8 0.1 93.00.1 73.8 0. 92.00.1 59.8 0.1
Full 0% 83.6 0.4 72.4 0.3 93.50.1 86.1 0.1 90.3 0.2 97.4 0. 85.6 0.2 76.2 0.3 95.00.1 87.30.1 93.4 0.1 64.8 0.
80% 84.80.2 73.4 0.1 93.7 0.1 85.40.2 90.50.2 97.20.1 85.1 0.1 75.70.5 96.1 0.1 87.4 0.1 93.4 0.1 64.00.
90% 84.90.3 72.90.2 93.90.3 84.80.1 90.0 0.2 97.1 0. 84.4 0.2 75.50.4 96.1 0.1 87.3 0.2 92.70.3 63.0 0.
Table 1. Best transfer accuracies at 80% and 90% sparsity for linear and full ﬁnetuning, relative to dense transfer. For each downstream
task, we present the maximum test accuracy across all sparse methods, highlighting the top accuracy. (We highlight multiple methods when
conﬁdence intervals overlap. Results are averaged across ﬁve and three trials for linear and full ﬁnetuning, respectively.) Note that in all
but three cases (all full ﬁnetuning), there is at least one sparse model that is competitive with or better than the dense baseline.
ing appears to be required for stable results on large datasets
such as ImageNet [5, 6, 15].
The above categorization is clearly approximate: for in-
stance, LTH methods could be viewed as a special case
of progressive sparsiﬁcation, where a speciﬁc ﬁnetuning
approach is applied. Moreover, it is not uncommon to
combine approaches, such as regularization and progressive
sparsiﬁcation [28]. We provide a comparison of the efﬁcacy
of these different approaches in the context of transfer learn-
ing, by considering multiple methods from each category.
To our knowledge, this is the ﬁrst such detailed study.
Top-1 test accuracy is the standard metric for comparing
pruning methods. We also adopt this metric for examining
accuracy in the context of transfer, as no such study exists.
Yet, we wish to highlight recent work [29, 30, 44] which
examines the robustness of pruned models to input pertur-
bations, as well as the impact of pruning on accuracy of
speciﬁc segments of the data.
2.2. Transfer Learning and Sparsity
Dense Transfer Learning. A large body of literature
has established that, in general, deep learning architectures
transfer well to smaller “downstream” tasks, and that full
ﬁnetuning typically achieves higher accuracy than linear
ﬁnetuning [36, 61]. (A very recent study [39] suggests that
this may be inverted on out-of-distribution tasks.) These
ﬁndings extend to related tasks, such as object detection and
segmentation [51]. Kolesnikov et al. [35] have focused on
factors determining the success of transfer learning, and on
developing reliable ﬁne-tuning recipes. This has been fur-
ther extended by Djolonga et al. [10], who concluded that
increasing the scale of the original model and dataset sig-
niﬁcantly improves out-of-distribution and transfer perfor-
mance, despite having marginal impact on the original ac-
curacy. Salman et al. [61] considered whether adversarially
robust ImageNet classiﬁers can outperform standard ones
for transfer learning, and ﬁnd that this can indeed be the
case. We complement these studies by examining sparse
models and pruning methods.
Sparse Transfer Learning. One of the earliest works to
consider transfer performance for pruned models was [54],whose goal was to design algorithms which allow the prun-
ing of a (dense) convolutional model when transferring on a
target task. (A similar study was performed by [62] for lan-
guage models.) By contrast, we focus on the different set-
ting where models have already been sparsiﬁed on the up-
stream dataset, and observe higher sparsities than the early
study of [54].
Recent work on sparse transfer learning has focused
speciﬁcally on models obtained via the “Lottery Ticket
Hypothesis” (LTH) approach [14], which roughly states
that there exist sparsity masks and initializations which al-
low accurate sparse networks to be trained from scratch .
There are several works investigating the “transferrability”
of models obtained via this procedure for different tasks: for
instance, [50] shows that lottery tickets obtained on the CI-
FAR dataset can transfer well on smaller downstream tasks,
while [6, 19] investigate the applicability of lottery tickets
for pre-trained language models (BERT), and object recog-
nition tasks, respectively. Mallya et al. [49] considered the
related but different problem of adapting a ﬁxed network to
multiple downstream tasks, by learning task-speciﬁc masks.
The recent work of [5] considers the transfer perfor-
mance of LTH for transfer, proposing LTH-T, and ﬁnding
that this method ensures good downstream accuracy at mod-
erate sparsities (e.g., up to 80%). We consider a similar
setting, but investigate a wider array of pruning methods
(including LTH-T) and additional transfer datasets. Specif-
ically, we are the ﬁrst to compare LTH-T to competitive up-
stream pruning methods. We observe that, on full ﬁnetun-
ing, most pruning methods consistently outperform LTH-T
in terms of downstream accuracy across sparsity levels, by
large margins at high sparsities.
3. Sparse Transfer on ImageNet
3.1. Experimental Choices
Transfer Learning Variants. We consider both full ﬁne-
tuning , where the entire set of features is optimized over
the downstream dataset, and linear ﬁnetuning , where only
the last layer classiﬁer is ﬁnetuned, over sparse models. In
the former case, with the exception of the ﬁnal classiﬁcation
layer and the batch normalization (BN) parameters, only the
3

--- PAGE 4 ---
Dataset Number of Classes Train/Test Examples Accuracy Metric
SUN397 [66] 397 19 850 / 19 850 Top-1
FGVC Aircraft [48] 100 6 667 / 3 333 Mean Per-Class
Birdsnap [1] 500 32 677 / 8 171 Top-1
Caltech-101 [43] 101 3 030 / 5 647 Mean Per-Class
Caltech-256 [21] 257 15 420 / 15 187 Mean Per-Class
Stanford Cars [37] 196 8 144 / 8 041 Top-1
CIFAR-10 [38] 10 50 000 / 10 000 Top-1
CIFAR-100 [38] 100 50 000 / 10 000 Top-1
Describable Textures (DTD) [8] 47 3 760 / 1 880 Top-1
Oxford 102 Flowers [55] 102 2 040 / 6 149 Mean Per-Class
Food-101 [4] 101 75 750 / 25 250 Top-1
Oxford-IIIT Pets [56] 37 3 680 / 3 669 Mean Per-Class
Table 2. Datasets used as downstream tasks for transfer learning.
nonzero weights of the original model are optimized, and
the mask is kept ﬁxed.
We do not consider from-scratch training and pruning on
the downstream task, for two reasons. First, from-scratch
training is often less accurate than (dense) transfer learn-
ing in the same setting [36, 51]. As our experiments will
show, transfer from sparse models can often match or even
slightly outperform transfer from dense models . Second,
since training from scratch is typically less accurate than
transfer [36], it seems unlikely that training and pruning
from scratch will outperform sparse transfer. We give ev-
idence for this claim in Appendix A. One practical advan-
tage of this approach is not needing hyper-parameter tuning
with respect to compression on the downstream dataset.
Network Architectures. Our study is based on an in-
depth analysis of sparse transfer using the ResNet50 ar-
chitecture [26]. This architecture has widespread practical
adoption, and has been extensively studied in the context
of transfer learning [36, 61]. Importantly, its compressibil-
ity has also emerged as a consistent benchmark for CNN
pruning methods [28]. We further validate some of our
ﬁndings on ResNet18, ResNet34 and MobileNet [31] archi-
tectures. In addition, we investigate transfer between two
classical object detection tasks, MS COCO [46] and Pascal
VOC [13], using variants of the YOLOv3 architecture [59].
Sparsiﬁcation Methods. For our study, we chose the
pruning methods providing top validation accuracy for each
method type in Section 2.1. For progressive sparsiﬁcation
methods , we use the leading WoodFisher [63] and Gradual
Magnitude Pruning (GMP) [17,22,23,69] methods. For reg-
ularization methods , we consider the leading Soft Thresh-
old Weight Reparametrization (STR) [41], and Alternat-
ing Compression/Decompression (AC/DC) [57] methods.
Additionally, we include the “The Rigged Lottery” (RigL)
method [12] with Erd ˝os-R ´enyi-Kernel (ERK) weight den-
sity. Compared to STR and AC/DC, RigL extends the train-
ing schedules on ImageNet by up to 5x, and does sparse
training for most of the optimization steps. We consider
both the standard version or RigL (RigL ERK 1x), and the
variant with 5x training iterations (RigL ERK 5x). Finally,
forLTH Methods , we consider the LTH-for-Transfer (LTH-Sparsity MethodOriginal
ValidationReassessed
LabelsImageNetV2
(Average)
0% Dense 76.8% 83.1% 72%
80% AC/DC 76.2% 82.9% 71.8%
STR 75.5% 81.9% 70.3%
WoodFisher 76.7% 83.2% 72.3%
GMP 76.4% 82.9% 71.6%
RigL ERK 1x 74.8% 81.3% 70.2%
RigL ERK 5x 75.8% 81.6% 70.6%
90% AC/DC 75.2% 82.2% 70.6%
STR 74.0% 80.9% 69.1%
WoodFisher 75.1% 82.4% 71.1%
GMP 74.7% 81.6% 70.1%
RigL ERK 1x 73.2% 80.0% 67.9%
RigL ERK 5x 75.7% 81.9% 70.6%
95% AC/DC 73.1% 80.4% 68.6%
STR 70.4% 77.9% 66.0%
WoodFisher 72.0% 79.8% 67.6%
RigL ERK 1x 70.1% 77.5% 65.5%
RigL ERK 5x 74.0% 80.8% 69.0%
Table 3. Accuracy of the pruning methods we use, at different
sparsity levels, evaluated on different ImageNet validation sets.
T) method of [5], which precisely matches our setting. In
this version, the authors apply the masks obtained through
progressive sparsiﬁcation methods directly to the original
trained ImageNet dense model, and evaluate the transfer
accuracy of this masked model through full ﬁnetuning on
different downstream tasks.
We focus on unstructured pruning, as these methods are
the most studied in the pruning literature, have well es-
tablished benchmarks, and achieve the best trade-off be-
tween accuracy and compression. We include results for
full ﬁnetuning from models with structured sparsity in
Appendix I, showing that, given a ﬁxed accuracy level
upstream, structured-sparse models tend to underperform
unstructured-sparse models for transfer.
When available, we use original sparse PyTorch check-
points, and the exact architectures used by the upstream
models. However, since the STR and RigL models were
trained using label smoothing, which has been shown in
[36] to decrease transfer accuracy, we used retrained ver-
sions of these models on ImageNet, without label smooth-
ing. The results we discuss in the following sections are
for these versions, which indeed perform better, particularly
on linear ﬁnetuning (see Appendix H). We manually ported
RigL checkpoints from TensorFlow to PyTorch (see Table
3 for all ImageNet results).
Downstream tasks and training. We follow [61] in using
the twelve standard transfer benchmark datasets described
in Table 2, which span several domains and sizes. We trans-
fer all parameters of the upstream model except for the last
(fully connected) layer, which is adjusted to the number of
classes in the downstream task, using Kaiming uniform ini-
tialization [25], and kept dense. This may slightly change
the sparsity of the model, as in some cases the ﬁnal layer
4

--- PAGE 5 ---
was sparse. As a convention, when discussing sparsity lev-
els, we refer to the upstream checkpoint sparsity. We pro-
vide full training hyperparameters in Appendix B.
Performance metrics. The main quantity of interest is
the top-1 validation accuracy on each transfer task, mea-
sured for all the pruned models, as well as for the dense
baselines. In some cases, we use the mean per-class val-
idation accuracy following the convention for that dataset
(see Table 2). To determine the overall “transfer potential”
for each pruning method, we further present the results ag-
gregated over the downstream tasks. Since datasets we use
for transfer learning have varying levels of difﬁculty, as re-
ﬂected by the wide range of transfer accuracies, we compute
for each downstream task and model the relative increase in
error over the dense baseline. Speciﬁcally, if Bis the base-
line dense model, then for every downstream task Dand
sparse model Swe deﬁne the relative increase in error as
D;S =errD;S errD;B
errD;B, whereerrD;Sis the error corre-
sponding to the top validation accuracy for model Strained
on datasetD. For each pruning method and sparsity level,
we report the mean and standard error of D;S, computed
over all downstream tasks.
We also examine the computational speedup potential
of each method, along with its accuracy. For inference-
time speedups, our ﬁndings are in line with previous work,
e.g. [11,57,63]. We will therefore focus on the training-time
speedup potential in the case of linear ﬁnetuning , which are
usually close to inference-time speedups, as the only differ-
ence is the training time of the classiﬁer layer.
3.2. Validation Accuracy on ImageNet Variants
To set a baseline, we ﬁrst examine accuracies on the
original ImageNet validation set, and on different versions
of this validation set. Namely, we use the ImageNet “re-
assessed labels” [2], where the original ImageNet valida-
tion images are re-assessed by human annotators. We also
use three different ImageNetV2 validation sets [58], where
the new images with a similar data distribution are gath-
ered based on different criteria. We report the average Ima-
geNetV2 accuracy across these three variants in Table 3.
Discussion. We observe that RigL ERK 5x outperforms all
methods on the original validation set at 90% and 95% spar-
sity, followed by AC/DC, GMP and WoodFisher. At 80%
sparsity, WoodFisher has the best original validation accu-
racy, followed closely by GMP and AC/DC. However, de-
spite the gap in original validation accuracy between RigL
ERK 5x and other methods, the results on new variants of
the validation set still reveal some interesting patterns. For
example, WoodFisher outperforms all methods at 80% and
90% sparsity on the reassessed labels, followed closely by
AC/DC. This is true also for ImageNetV2, where Wood-
Fisher outperforms all methods at 80% and 90% sparsity.
At 95% sparsity, however, RigL ERK 5x outperforms allmethods considered, including on the reassessed labels and
ImageNetV2, and is followed by AC/DC. Generally, the ac-
curacies on the reassessed labels and ImageNetV2 correlate
well with those on the original images, which suggests that
top performing methods can “extrapolate” well.
3.3. Linear Finetuning
Next, we study the transfer performance of different
types of pruning methods in the scenario where only the
linear classiﬁer “on top” of a ﬁxed representation is trained
on the downstream task. Speciﬁcally, we study the sim-
ple setup where the features prior to the ﬁnal classiﬁcation
layer of the pre-trained model are extracted for all samples
in the transfer dataset and stored into memory for use when
training the downstream linear classiﬁer. Although this ap-
proach typically results in lower accuracy relative to full
ﬁnetuning [36, 61], it has signiﬁcant practical advantages.
Speciﬁcally, the features can be precomputed, which elimi-
nates the forward passes through the pretrained network. In
this setup, we do not apply any data augmentation on the
transfer samples and we use the Batch Normalization statis-
tics of the pretrained network on ImageNet.
We optimize the linear classiﬁer using SGD with mo-
mentum, weight decay and learning rate annealing, follow-
ing [61]. (The results are typically well-correlated with
those obtained when using data augmentation during train-
ing, or using different optimizers [36]). In Section 3.6,
we show that training speed-ups can also be obtained in
an online learning setup, where new samples are executed
through the backbone network, by taking advantage of the
backbone sparsity.
The results for linear ﬁnetuning are shown in Figure 2,
and Appendix Table C.1. We exclude the LTH-T method
from this analysis, as it is designed for full ﬁnetuning, and
its transfer accuracy in the linear scenario is indeed very low
(see Appendix Table C.1).
Overall, the results clearly show that the choice of prun-
ing strategy on the upstream task can result in signiﬁcant
differences in performance on downstream tasks. These
differences are more apparent for specialized downstream
tasks, with ﬁne-grained classes. For example, consider Air-
craft, where for 80% sparse models we see a 15% gap
in top-1 test accuracy between the best-performing sparse
models (AC/DC and RigL, 55%) and the worst-performing
one (WoodFisher, 40%).
Following this observation, we study the correlation be-
tween the downstream task difﬁculty and relative increase
in error for different pruning strategies. For this purpose,
we use the difference in top-1 validation accuracy between
full and linear ﬁnetuning on the dense backbone as a proxy
for the difﬁculty of a downstream task. Intuitively, a small
gap between full and linear ﬁnetuning would suggest that
the upstream features are directly transferable, and thus the
5

--- PAGE 6 ---
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Transfer Validation Accuracy at 80% Sparsity
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Average Relative Increase in Error across Sparsities
Aircraft Birds CIFAR
10CIFAR
100Caltech
101Caltech
256Cars DTD Flowers Food
101Pets SUN
397
Downstream Task020406080Validation Accuracy (%)ResNet50 - Linear Finetuning
Aircraft Birds CIFAR
10CIFAR
100Caltech
101Caltech
256Cars DTD Flowers Food
101Pets SUN
397
Downstream Task020406080100ResNet50 - Full Finetuning
80 90 95 98
Sparsity (%)010203040Relative Error Increase (%)ResNet50 - Linear Finetuning
80 90 95 98
Sparsity (%)050100150ResNet50 - Full Finetuning
Dense AC/DC STR WoodFisher GMP RigL ERK 1x RigL ERK 5x LTH-TFigure 2. (top row) Validation accuracy for selected pruning strategies at 80% sparsity. (bottom row) Average increase in validation error
relative to the dense baseline; lower values are better. Best viewed in color.
downstream task can be considered “easy”. Conversely, a
large gap would indicate that the pre-trained features are
not enough to capture the internal representation of the data,
making the downstream task more “difﬁcult”. Additionally,
we categorize the downstream tasks into general (Caltech-
101/256, CIFAR-10/100, DTD, SUN397) vs. specialized
(Aircraft, Birds, Cars, Flowers, Food-101, Pets); this is sim-
ilar to previous work [36]. Figure 3 suggests that special-
ized datasets tend to have higher difﬁculty scores.
Following this deﬁnition and categorization, we mea-
sure, for each pruning strategy, the relative error increase
over the dense model against the task difﬁculty. Figure 3
shows the behavior for all pruning methods considered at
80% and 90% sparsity. Interestingly, we observe a trend
forregularization methods (AC/DC, STR, RigL) to improve
over the dense baseline with increased task difﬁculty, which
is more apparent at higher sparsity (90%). In contrast,
progressive sparsiﬁcation methods (GMP, WoodFisher) do
not show a similar behavior. This suggests that regulariza-
tion pruning methods are a better choice for linear transfer
(sometimes even surpassing the dense performance) when
the downstream task is more specialized or more difﬁcult.
Another particularity of linear ﬁnetuning from sparse
models is that the sparsity level is not highly correlated with
the performance on the downstream tasks. This is apparent,
for example, for AC/DC and RigL, where, despite the 1-2%
gap in ImageNet accuracy between the 80% and 90% sparse
0 10 20 30 40
Dataset Difficulty0.4
0.2
0.00.20.4Relative Error Increase (%)80% Sparsity
General
SpecializedAC/DC
STRWoodFisher
GMPRigL ERK 5x0 10 20 30 40
Dataset Difficulty90% SparsityEffect of Task Difficulty on Relative Increase in ErrorFigure 3. Effect of task difﬁculty on various pruning strategies for
transfer with linear ﬁnetuning. Best viewed in color.
models, the relative error with respect to the dense baseline
stays quite ﬂat. A similar trend can be observed for other
pruning methods as well. However, extremely sparse mod-
els (98%) tend to perform worse, probably due to feature
removal and degradation.
In summary, we observe that 1) some sparsiﬁcation
methods can consistently match or even sometimes out-
perform dense models; 2) there is a correlation between
transfer performance for regularization-based methods and
downstream task difﬁculty; and 3) higher sparsity is not
necessarily a disadvantage for transfer performance.
6

--- PAGE 7 ---
3.4. Full Finetuning
We now consider the full ﬁnetuning scenario. Here, we
re-initialize the ﬁnal classiﬁcation layer and ﬁx it as dense,
then ﬁnetune the unpruned weights so that the network is
sparse throughout training. The results are summarized in
Figure 2, and detailed in Appendix Table C.2.
Similar to linear ﬁnetuning, we see substantial perfor-
mance variations among pruning strategies when trans-
ferred to the downstream tasks. Typically, progressive spar-
siﬁcation methods (WoodFisher, GMP) tend to transfer bet-
ter than regularization and lottery ticket methods. The dif-
ferences in test accuracy, measured at the same sparsity
level, are typically small, on the order of 1–3%; the excep-
tion is LTH-T, which is competitive at low sparsity (80%)
but incurs severe accuracy drops at sparsities 90%.
In contrast to linear ﬁnetuning, we see a consistent trend
of decreasing quality with increased sparsity. This is not
surprising, since full ﬁnetuning can take advantage of the
additional parameters available in denser models to better ﬁt
the downstream data. Nevertheless, progressive sparsiﬁca-
tionmethods (GMP and WoodFisher) result in downstream
performance nearly on par with dense models at 80% and
90% sparsity. These methods show better performance than
regularization-based methods (AC/DC, STR, and RigL), a
direct reversal of the results of linear ﬁnetuning.
For speciﬁc downstream tasks, however, there is consid-
erable variability—while WoodFisher and GMP are con-
sistently the top or near-top performing models across all
tasks, other methods show considerable task dependence.
For instance, while AC/DC is the top performing method
across different sparsities for three of the twelve tasks
(SUN397, Caltech-256, and DTD), it shows a considerable
gap compared to the best-performing methods on Aircraft,
Cars, and CIFAR-10. Generally, STR performs worse on
full ﬁnetuning, compared to other regularization methods.
Furthermore, RigL ERK 1x performs roughly on par with
AC/DC, despite having a lower validation accuracy on Im-
ageNet; however, the extended training of RigL ERK 5x
gives the tansfer accuracies a considerable boost, putting
RigL ERK 5x almost on par with WoodFisher, especially at
higher sparsities. This ﬁnding opens the intriguing possi-
bility that extended training may be beneﬁcial for pruning
methods in the full ﬁnetuning regime. We present further
evidence in favor of this hypothesis in Appendix D; namely,
we additionally show that extending the training time of up-
stream AC/DC models (3x or 5x) signiﬁcantly improves
their transfer performance on all downstream tasks. Fi-
nally, LTH-T shows fairly competitive performance at 80%
sparsity, but its transfer accuracy declines dramatically on
six of the twelve datasets (SUN397, Caltech-101, Caltech-
256, DTD, Flowers, and Pets) as sparsity increases. Since
the LTH-T model relies mainly on transferring the sparsity
mask across tasks, this suggests that the additional infor-mation present in the weights , leveraged by other methods,
may be beneﬁcial.
In sum, if the goal is to perform full ﬁnetuning on down-
stream tasks, then progressive sparsiﬁcation methods are a
good choice. They consistently outperform regularization
methods across a wide range of tasks, and offer comparable
performance to the dense backbone at 80% and 90% spar-
sity.
3.5. Discussion
The results of the last two sections show an intriguing
performance gap between pruning methods, depending on
the transfer approach. Investigating further, we examine
the sparse structure of the resulting pruned models, by mea-
suring the percentage of convolutional ﬁlters that are com-
pletely pruned away during the training phase of the sparse
ResNet50 backbones on the original ImageNet dataset. The
results in Table 4 show the differences in the number of
zeroed-out channels among pruning methods. We observe
that AC/DC has a large number of channels that are fully
removed during ImageNet training and pruning, on average
2-4 more at 80% and 90% sparsity, compared to other mod-
els; this results in fewer features that can be trained during
full ﬁnetuning. By contrast, the sparsity in GMP and Wood-
Fisher is less structured and thus can express additional fea-
tures, which can be leveraged during ﬁnetuning. We further
illustrate this effect in Appendix I, where we fully ﬁnetune
from models with structured sparsity.
Pruned
Filters (%)AC/DC WoodFisher GMP STRRigL ERK
1xRigL ERK
5x
80% 2.9% 0.9% 1.6% 0.5% 0.2% 0.6%
90% 8.5% 2.0% 2.8% 2.0% 1.2% 2.7%
95% 18% 3.0% - 6.0% 4.3% 9.1%
Table 4. Percentage of convolutional ﬁlters that are completely
masked out, for different pruning methods on ResNet50, at differ-
ent sparsity levels. AC/DC has signiﬁcantly more pruned ﬁlters.
In the case of linear ﬁnetuning, we hypothesize that the
accuracy reversal in favor of AC/DC can be attributed to
a regularizing effect, which produces more “robust” fea-
tures. The same effect appears to be present in RigL ERK
5x at 95% sparsity, which also has signiﬁcantly many fully-
pruned ﬁlters.
3.6. Training Speed-Up using Linear Finetuning
One of the main beneﬁts of sparse models is that they
can provide inference speed-ups when executed on sparsity-
aware runtimes [11, 40, 57, 63]. For linear ﬁnetuning, this
can also imply training time speed-ups, since the sparse
backbone is ﬁxed, and only used for inference. We illustrate
this in an “online learning” setting, where training samples
arrive dynamically at the device. We ﬁrst compute the cor-
responding features using the sparse backbone. Then, we
7

--- PAGE 8 ---
0.30 0.35 0.40 0.45 0.50
Average training time / epoch (fraction of dense)2.5
2.0
1.5
1.0
0.5
0.00.51.01.5Change in test accuracy (%)Linear finetuning training time for ResNet50 at 90% sparsity
Caltech-101
DTDPets
FlowersAC/DC
STRWoodFisher
GMPRigL ERK 5xFigure 4. Average epoch time vs. gap in validation accuracy, com-
pared to the dense baseline. Results are shown for four differ-
ent downstream tasks, using linear ﬁnetuning from ResNet50 90%
sparse models. Lower is better; best viewed in color.
use these features to train the linear classiﬁer. Thus, the
forward pass can beneﬁt from speed-ups due to sparsity.
To measure these effects, we integrated the freely-
available sparsity-aware DeepSparse CPU inference en-
gine [9, 40] into our PyTorch pipeline. Speciﬁcally, we use
sparse inference for online feature extraction. We report
overall training speedup, in terms of average training time
per epoch on the downstream task, divided by the average
training time using the dense baseline. We use batch size
64 and data augmentation; otherwise, hyperparameters are
identical to the linear ﬁnetuning experiment in Section 3.3.
We execute on an Intel E5-1650 CPU with 12 cores, which
would be similar in performance to a recent laptop CPU.
The speed-ups we report are proportional to the inference
speed-ups of the respective sparse backbone models. The
only difference is the cost of optimizing the last layer, which
varies in size with the number of classes.
Figure 4 shows results on four downstream tasks,
Pets, Flowers, DTD and Caltech-101, where the backbone
ResNet50 models have 90% sparsity. We report the training
speed-up vs. the difference in validation accuracy, com-
pared to the dense baseline. Furthermore, we show speed-
up numbers for additional sparsity levels (80% and 95%),
in Table 5; these numbers representing the average training
time per epoch are computed on the Caltech-101 dataset,
but the speed-up factor should be similar for other datasets
as well, since the training time per batch is almost propor-
tional to the inference through the backbone network. Re-
sults show that using sparse backbones can reduce training
time by 2-4x for linear transfer, without negative impact on
validation accuracy .
4. Extensions
ResNet18/34 and MobileNet Experiments. We have
also executed a subset of the experiments for ResNet18,
ResNet34 and MobileNetV1 [31] models trained on Ima-Sparsity STR GMP WoodFisher AC/DC RigL 5x
80% 0:44 0:50 0:53 0:60 0:71
90% 0:28 0:36 0:37 0:43 0:50
95% 0:22 N /A 0:28 0:32 0:36
Table 5. Average training time per epoch for linear ﬁnetuning us-
ing sparse models, as a fraction of the time per epoch required
for the dense backbone. The numbers shown are computed on the
Caltech-101 dataset.
Architecture YOLOv3 YOLOv5S YOLOv5L
Pruning 90% Sparsity 75% Sparsity 85% Sparsity
COCO Dense 64.2 55.6 65.4
COCO Pruned 62.4 53.4 64.3
VOC Dense Transfer 86.0 83.73 90.0
VOC Pruned Transfer 84.0 81.72 89.35
Table 6. Accuracies for Sparse Transfer from COCO to VOC.
geNet. These results largely validate our analysis above,
and are therefore deferred to Appendix E and F. Speciﬁ-
cally, regularization-based methods also match or slightly
outperform dense ones on linear transfer. However, for Mo-
bileNetV1, we observe that sparse models can match the
dense baseline transfer performance only at lower sparsities
(up to 75%), probably due to the lower parameter count.
Structured Sparsity Experiments. We also performed
full ﬁnetuning using models with structured sparsity , for
both ResNet50 and MobileNet. Our ﬁndings, presented
in Appendix I, show that structured-sparse models tend to
transfer worse compared to unstructured methods.
Sparse Transfer using YOLO. We also examined transfer
performance between YOLO V3 [59] and YOLO “V5” [64]
models for object detection, trained and pruned on the
COCO dataset [46], which are then transferred to the VOC
dataset [13] using full ﬁnetuning. Table 6 presents results
in terms of mean Average Precision (mAP@0.5). Results
show a strong correlation between accuracy on the original
COCO dataset and that on VOC, conﬁrming our claims. We
observed similar trends in a segmentation setup, which we
cover in Appendix J.
5. Conclusions and Future Work
We performed an in-depth study of the transfer perfor-
mance of sparse models, and showed that pruning meth-
ods with similar accuracy on ImageNet can have surpris-
ingly disparate Top-1 accuracy when used for transfer learn-
ing. In particular, regularization-based methods perform
best for linear ﬁnetuning; conversely, progressive sparsiﬁ-
cation methods such as GMP and WoodFisher tend to work
best when full ﬁnetuning is used. One limitation of our
study is that it only investigated accuracy as a measure of
performance for transfer learning tasks. Additional research
8

--- PAGE 9 ---
is needed towards designing pruning strategies with good
performance across both linear and full ﬁnetuning, and to-
wards considering metrics past Top-1 accuracy, such as bias
and robustness. Another limitation is that we considered a
(standard) ﬁxed set of transfer datasets; our study should be
extended to other, more complex transfer learning scenar-
ios, such as distributional shift [34]. Further investigation
could also systematically examine other types of compres-
sion, such as quantization and structured pruning, poten-
tially in conjunction with unstructured pruning, which was
the focus of our current study. Other interesting areas for
future work would be understanding the performance gap
between full ﬁnetuning and linear ﬁnetuning, and realizing
training speedups for sparse full ﬁnetuning, by taking ad-
vantage of the ﬁxed sparsity in the trained model.
Acknowledgments
The authors would like to sincerely thank Christoph
Lampert and Nir Shavit for fruitful discussions during the
development of this work, Eldar Kurtic for experimental
support, Utku Evci for providing RigL models, and the au-
thors of [5] for sharing the LTH-T masks. EI was supported
in part by the FWF DK VGSCO, grant agreement number
W1260-N35, while AP and DA acknowledge generous sup-
port by the ERC, via Starting Grant 805223 ScaleML.
References
[1] Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L.
Alexander, David W. Jacobs, and Peter N. Belhumeur. Bird-
snap: Large-scale ﬁne-grained visual categorization of birds.
InIEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 2019–2026, 2014. 4
[2] Lucas Beyer, Olivier J H ´enaff, Alexander Kolesnikov, Xi-
aohua Zhai, and A ¨aron van den Oord. Are we done with
ImageNet? arXiv preprint arXiv:2006.07159 , 2020. 5
[3] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee.
Yolact: Real-time instance segmentation. In ICCV , 2019. 24
[4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
Food-101 – mining discriminative components with ran-
dom forests. In European Conference on Computer Vision
(ECCV) , 2014. 4
[5] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
Yang Zhang, Michael Carbin, and Zhangyang Wang. The
lottery tickets hypothesis for supervised and self-supervised
pre-training in computer vision models. IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2021. 2, 3, 4, 9
[6] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
Yang Zhang, Zhangyang Wang, and Michael Carbin. The
lottery ticket hypothesis for pre-trained BERT networks.
arXiv preprint arXiv:2007.12223 , 2020. 2, 3
[7] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong.
When vision transformers outperform resnets without pre-
training or strong data augmentations. arXiv preprint
arXiv:2106.01548 , 2021. 2[8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy
Mohamed, and Andrea Vedaldi. Describing textures in the
wild. In IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , 2014. 4
[9] DeepSparse. NeuralMagic DeepSparse Inference Engine,
2021. 1, 8
[10] Josip Djolonga, Jessica Yung, Michael Tschannen, Rob
Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan
Puigcerver, Matthias Minderer, Alexander D’Amour, Dan
Moldovan, et al. On robustness and transferability of convo-
lutional neural networks. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2021. 3
[11] Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Si-
monyan. Fast sparse convnets. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
14629–14638, 2020. 1, 5, 7
[12] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Cas-
tro, and Erich Elsen. Rigging the lottery: Making all tickets
winners. In International Conference on Machine Learning
(ICML) , 2020. 2, 4
[13] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The PASCAL Visual
Object Classes (VOC) challenge. International Journal of
Computer Vision (IJCV) , 88(2):303–338, 2010. 4, 8
[14] Jonathan Frankle and Michael Carbin. The lottery ticket hy-
pothesis: Finding sparse, trainable neural networks. In In-
ternational Conference on Learning Representations (ICLR) ,
2018. 2, 3
[15] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M
Roy, and Michael Carbin. Stabilizing the lottery ticket hy-
pothesis. arXiv preprint arXiv:1903.01611 , 2019. 2, 3
[16] Elias Frantar, Eldar Kurtic, and Dan Alistarh. M-FAC: Ef-
ﬁcient matrix-free approximations of second-order informa-
tion. Conference on Neural Information Processing Systems
(NeurIPS) , 2021. 2, 19, 20
[17] Trevor Gale, Erich Elsen, and Sara Hooker. The state
of sparsity in deep neural networks. arXiv preprint
arXiv:1902.09574 , 2019. 1, 2, 4
[18] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao,
Michael W Mahoney, and Kurt Keutzer. A survey of quanti-
zation methods for efﬁcient neural network inference. arXiv
preprint arXiv:2103.13630 , 2021. 1
[19] Sharath Girish, Shishira R Maiya, Kamal Gupta, Hao Chen,
Larry S Davis, and Abhinav Shrivastava. The lottery ticket
hypothesis for object recognition. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
762–771, 2021. 3
[20] Graphcore. Graphcore Poplar SDK 2.0, 2021. 1
[21] Gregory Grifﬁn, Alexander D. Holub, and Pietro Perona.
The Caltech 256. Caltech Technical Report , 2006. 4
[22] Masafumi Hagiwara. A simple and effective method for
removal of hidden units and weights. Neurocomputing ,
6(2):207 – 218, 1994. Backpropagation, Part IV . 1, 2, 4
[23] Song Han, Jeff Pool, John Tran, and William J Dally. Learn-
ing both weights and connections for efﬁcient neural net-
works. In Conference on Neural Information Processing Sys-
tems (NeurIPS) , 2015. 1, 2, 4
9

--- PAGE 10 ---
[24] Babak Hassibi, David G Stork, and Gregory J Wolff. Op-
timal brain surgeon and general network pruning. In IEEE
International Conference on Neural Networks , pages 293–
299. IEEE, 1993. 2
[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation. In International Confer-
ence on Computer Vision (ICCV) , 2015. 4
[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 770–778, 2016. 2, 4
[27] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
Song Han. AMC: AutoML for model compression and ac-
celeration on mobile devices. In European Conference on
Computer Vision (ECCV) , 2018. 24
[28] Torsten Hoeﬂer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden,
and Alexandra Peste. Sparsity in deep learning: Pruning
and growth for efﬁcient inference and training in neural net-
works. arXiv preprint arXiv:2102.00554 , 2021. 1, 2, 3, 4
[29] Sara Hooker, Aaron Courville, Gregory Clark, Yann
Dauphin, and Andrea Frome. What do compressed deep
neural networks forget? arXiv preprint arXiv:1911.05248 ,
2019. 3
[30] Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Ben-
gio, and Emily Denton. Characterising bias in compressed
models. arXiv preprint arXiv:2010.03058 , 2020. 3
[31] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 , 2017. 2, 4, 8, 19
[32] Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon
Osindero, and Erich Elsen. Top-KAST: Top-K always sparse
training. Conference on Neural Information Processing Sys-
tems (NeurIPS) , 2020. 2
[33] Xiaojie Jin, Xiaotong Yuan, Jiashi Feng, and Shuicheng Yan.
Training skinny deep neural networks with iterative hard
thresholding methods. arXiv preprint arXiv:1607.05423 ,
2016. 2
[34] Pang Wei Koh, Shiori Sagawa, Henrik Marklund,
Sang Michael Xie, Marvin Zhang, Akshay Balsubra-
mani, Weihua Hu, Michihiro Yasunaga, Richard Lanas
Phillips, Irena Gao, et al. WILDS: A benchmark of in-
the-wild distribution shifts. In International Conference on
Machine Learning (ICML) , 2021. 9
[35] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan
Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.
Big transfer (BiT): General visual representation learning. In
European Conference on Computer Vision (ECCV) , 2020. 3
[36] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do bet-
ter ImageNet models transfer better? In IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 2661–2671, 2019. 1, 2, 3, 4, 5, 6, 23
[37] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3D Object Representations for Fine-Grained Categorization.
In4th International IEEE Workshop on 3D Representation
and Recognition (3dRR-13) , Sydney, Australia, 2013. 4[38] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 4, 12
[39] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu
Ma, and Percy Liang. Fine-tuning can distort pretrained fea-
tures and underperform out-of-distribution. arXiv preprint
arXiv:2202.10054 , 2022. 3
[40] Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander
Matveev, John Carr, Michael Goin, William Leiserson, Sage
Moore, Bill Nell, Nir Shavit, and Dan Alistarh. Inducing
and exploiting activation sparsity for fast inference on deep
neural networks. In International Conference on Machine
Learning (ICML) , 2020. 1, 7, 8
[41] Aditya Kusupati, Vivek Ramanujan, Raghav Somani,
Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali
Farhadi. Soft threshold weight reparameterization for learn-
able sparsity. In International Conference on Machine
Learning (ICML) , 2020. 2, 4
[42] Yann LeCun, John S Denker, and Sara A Solla. Optimal
brain damage. In Conference on Neural Information Pro-
cessing Systems (NeurIPS) , 1990. 1, 2
[43] Fei-Fei Li, R. Fergus, and Pietro Perona. Learning gener-
ative visual models from few training examples: an incre-
mental Bayesian approach tested on 101 object categories.
InIEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2004. 4
[44] Lucas Liebenwein, Cenk Baykal, Brandon Carter, David
Gifford, and Daniela Rus. Lost in Pruning: The Effects of
Pruning Neural Networks beyond Test Accuracy. Conference
on Machine Learning and Systems (MLSys) , 2021. 3
[45] Tao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev, and
Martin Jaggi. Dynamic model pruning with feedback. In In-
ternational Conference on Learning Representations (ICLR) ,
2019. 2
[46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
European Conference on Computer Vision (ECCV) , 2014. 4,
8
[47] Dong C Liu and Jorge Nocedal. On the limited memory bfgs
method for large scale optimization. Mathematical program-
ming , 45(1):503–528, 1989. 12
[48] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew
Blaschko, and Andrea Vedaldi. Fine-grained visual classi-
ﬁcation of aircraft. Technical report, 2013. 4
[49] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy-
back: Adapting a single network to multiple tasks by learn-
ing to mask weights. In European Conference on Computer
Vision (ECCV) , 2018. 3
[50] Rahul Mehta. Sparse transfer learning via winning lottery
tickets. arXiv preprint arXiv:1905.07785 , 2019. 3
[51] Thomas Mensink, Jasper Uijlings, Alina Kuznetsova,
Michael Gygli, and Vittorio Ferrari. Factors of inﬂuence for
transfer learning across diverse appearance domains and task
types. arXiv preprint arXiv:2103.13318 , 2021. 3, 4
[52] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko
Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and
Paulius Micikevicius. Accelerating sparse deep neural net-
works. arXiv preprint arXiv:2104.08378 , 2021. 1
10

--- PAGE 11 ---
[53] Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
Variational dropout sparsiﬁes deep neural networks. In In-
ternational Conference on Machine Learning (ICML) , 2017.
2
[54] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
and Jan Kautz. Pruning convolutional neural networks for re-
source efﬁcient inference. arXiv preprint arXiv:1611.06440 ,
2016. 3
[55] Maria-Elena Nilsback and Andrew Zisserman. A visual vo-
cabulary for ﬂower classiﬁcation. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , vol-
ume 2, pages 1447–1454, 2006. 4
[56] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and
C. V . Jawahar. Cats and dogs. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2012. 4
[57] Alexandra Peste, Eugenia Ioﬁnova, Adrian Vladu, and Dan
Alistarh. AC/DC: Alternating Compressed/DeCompressed
Training of Deep Neural Networks. Conference on Neural
Information Processing Systems (NeurIPS) , 2021. 2, 4, 5, 7,
12
[58] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
Vaishaal Shankar. Do ImageNet classiﬁers generalize to im-
agenet? In International Conference on Machine Learning
(ICML) , 2019. 5
[59] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental
improvement. arXiv preprint arXiv:1804.02767 , 2018. 4, 8
[60] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. ImageNet large
scale visual recognition challenge. International Journal of
Computer Vision (IJCV) , 115(3):211–252, 2015. 1
[61] Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish
Kapoor, and Aleksander Madry. Do adversarially robust Im-
ageNet models transfer better? Conference on Neural Infor-
mation Processing Systems (NeurIPS) , 2020. 3, 4, 5, 12
[62] Victor Sanh, Thomas Wolf, and Alexander M Rush. Move-
ment pruning: Adaptive sparsity by ﬁne-tuning. arXiv
preprint arXiv:2005.07683 , 2020. 3
[63] Sidak Pal Singh and Dan Alistarh. WoodFisher: Efﬁcient
second-order approximation for neural network compres-
sion. Conference on Neural Information Processing Systems
(NeurIPS) , 2020. 1, 2, 4, 5, 7
[64] Ultralytics. Implementation of the YOLO V5 model ar-
chitecture. Available at https : / / github . com /
ultralytics/yolov5 ., 2021. 8
[65] Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong
Zhang. Eigendamage: Structured pruning in the Kronecker-
factored eigenbasis. In International Conference on Machine
Learning (ICML) , 2019. 2
[66] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva,
and Antonio Torralba. Sun database: Large-scale scene
recognition from abbey to zoo. IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2010. 4
[67] Huanrui Yang, Wei Wen, and Hai Li. Deephoyer: Learn-
ing sparser neural network with differentiable scale-invariant
sparsity measures. arXiv preprint arXiv:1908.09979 , 2019.
2[68] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. In British Machine Vision Conference (BMVC) , 2016.
12
[69] Michael Zhu and Suyog Gupta. To prune, or not to prune: ex-
ploring the efﬁcacy of pruning for model compression. arXiv
preprint arXiv:1710.01878 , 2017. 1, 2, 4
11

--- PAGE 12 ---
Appendix
A. Comparison between From-Scratch Pruning and Transfer
We now present an experiment which supports our claim that from-scratch pruning and ﬁnetuning is inferior to transfer
from ImageNet sparse models. For instance, image classiﬁcation on the CIFAR-100 [38] dataset using a WideResNet [68]
architecture, following [57], the AC/DC and GMP pruning methods at 90% sparsity reach 79.1% and 77.7% Top-1 validation
accuracies, respectively. In contrast, ﬁnetuning from a ResNet50 backbone pruned on ImageNet using AC/DC and GMP at
90% sparsity reaches a validation accuracy of 83.9% and 84.4%, respectively (please see Table C.2 for the results). This
example serves to illustrate the signiﬁcant accuracy gains from using transfer learning with sparse models, as opposed to
training sparse models from scratch.
B. Hyperparameters and Training Setup
Here we discuss the general hyperparameters and experimental setup used for the full and linear ﬁnetuning experiments.
Regarding data loading image augmentation settings, we are careful to match them to the ones used in the original upstream
training protocol. Speciﬁcally, this affects the choice of whether to use Bicubic or Bilinear image interpolation for image
resizing; for example, RigL models were trained using Bicubic interpolation, whereas the other pruning methods considered
used the Biliniar interpolation. All ResNet and MobileNet models considered were trained using standard ImageNet-speciﬁc
values for the normalization mean and standard deviation. In the case of full ﬁnetuning, we used dataset-speciﬁc normal-
ization values for the downstream tasks; these were obtained by loading the dataset once with standard data augmentations
and computing the means and variances of the resulting data. For linear ﬁnetuning, we use center cropping of the images,
followed by normalization using standard ImageNet values. For both full and linear ﬁnetuning, we use the same training
hyperparameters as [61]; speciﬁcally, we train for 150 epochs, decreasing the initial learning rate by a factor of 10 every 50
epochs. We use 0.01 as the initial learning rate for all linear ﬁnetuning experiments; for full ﬁnetuning, we empirically found
0.001 to be the initial learning rate which gives comparable results for most datasets except Aircraft and Cars, for which we
use 0.01. Our experiments were conducted using PyTorch 1.8.1 and NVIDIA GPUs. All full ﬁnetuning experiments on the
ResNet50 backbone were repeated three times and all linear ﬁnetuning experiments ﬁve times.
C. Extended ResNet50 Results
In this section, we provide additional details, together with the complete results for our experiments for linear and full
ﬁnetuning from ResNet50, presented in Sections 3.3 and 3.4. For each pruning method, we used a range of sparsity levels,
and trained linear and full ﬁnetuning for each model and sparsity level, on all 12 downstream tasks; each experiment was
repeated 5 times for linear and 3 times for full ﬁnetuning. Note that checkpoints for some pruning methods were not available
for some of the higher sparsities.
C.1. Linear Finetuning Results
We provide the complete results for our linear ﬁnetuning experiments on each downstream task, for all pruning methods
and sparsity levels considered. The results for the transfer accuracies for each pruning strategy, sparsity level, and downstream
task are presented in Figure C.1 and Table C.1. We discussed in Section 3.3 that regularization methods match and even
sometimes outperform the dense baseline transfer performance. Note that this fact is valid not only in aggregate, but also at
the level of each individual dataset.
In table C.1, we also include the linear transfer results for LTH-T. We note that the generally poor performance of the
method, especially for more specialized tasks and higher sparsity levels, should not be taken as a criticism of the method
itself: this use case is clearly contrary to the method’s design, and the spirit of the original Lottery Ticket Hypothesis (which
aims to discover masks with the intent to retrain, rather than ﬁnal weights). Rather, we include these results to provide
quantitative justiﬁcation for the omission of LTH-T from any further analyses, and supporting the original authors’ point that
additional ﬁnetuning is necessary in order to obtain a competitive lottery ticket for transfer learning.
Additionally, we also validate our linear ﬁnetuning results by training with a different optimizer than SGD with momen-
tum; namely, we use L-BFGS [47] and L2regularization. We tested multiple values for the L2regularization strength and
we report for each dataset and method the highest value for the test accuracy. The results of this experiment are presented
in Table C.3. Despite the differences in test accuracy between models trained with SGD or L-BFGS, we can observe a very
similar trend related to the performance of sparse models over the dense baseline: namely, regularization pruning methods,
12

--- PAGE 13 ---
80 90 95 9840455055Top-1 Acc. (%)Aircraft
80 90 95 98455055Birds
80 90 95 988890CIFAR-10
80 90 95 9868707274CIFAR-100
80 90 95 9886889092Caltech-101
80 90 95 9875.077.580.082.585.0Caltech-256
80 90 95 98
Sparsity (%)45505560Top-1 Acc. (%)Cars
80 90 95 98
Sparsity (%)68707274DTD
80 90 95 98
Sparsity (%)889092Flowers
80 90 95 98
Sparsity (%)6668707274Food-101
80 90 95 98
Sparsity (%)86889092Pets
80 90 95 98
Sparsity (%)52.555.057.560.0SUN397ResNet50 Linear Finetuning - Validation Accuracy
Dense AC/DC STR WoodFisher GMP RigL ERK 1x RigL ERK 5xFigure C.1. (ResNet50) Per-dataset downstream validation accuracy for transfer learining with linear ﬁnetuning .
80 90 95 98758085Top-1 Acc. (%)Aircraft
80 90 95 9855606570Birds
80 90 95 9894959697CIFAR-10
80 90 95 9877.580.082.585.0CIFAR-100
80 90 95 98708090Caltech-101
80 90 95 98758085Caltech-256
80 90 95 98
Sparsity (%)80.082.585.087.590.0Top-1 Acc. (%)Cars
80 90 95 98
Sparsity (%)67.570.072.575.0DTD
80 90 95 98
Sparsity (%)7580859095Flowers
80 90 95 98
Sparsity (%)77.580.082.585.087.5Food-101
80 90 95 98
Sparsity (%)85.087.590.092.5Pets
80 90 95 98
Sparsity (%)556065SUN397ResNet50 Full Finetuning - Validation Accuracy
Dense AC/DC STR WoodFisher GMP RigL ERK 1x RigL ERK 5x LTH-T
Figure C.2. (ResNet50) Per-dataset downstream validation accuracy for transfer learining with full ﬁnetuning .
such as AC/DC, STR or RigL, tend to be close to—or even outperform—the dense baseline, especially on ﬁne-grained tasks
(Aircraft, Cars).
C.2. Full Finetuning Results
Similarly to linear ﬁnetuning, we further provide complete results for full ﬁnetuning from sparse models. We present
individual results per downstream task and pruning method, at different sparsity levels, in Figure C.2 and Table C.2; we
report for each the average and standard deviation across 3 different trials. The results further support our conclusions
from Section 3.4; namely, downstream task accuracy is correlated with the backbone sparsity, and progressive sparsiﬁcation
methods (GMP, WoodFisher) generally perform better than regularization methods.
13

--- PAGE 14 ---
Pruning Strategy Dense AC/DC GMP LTH-T RigL ERK 1x RigL ERK 5x STR WoodFisher
80% Sparsity
Aircraft 49.2 0.1 55.10.1 45.80.1 36.9 0.1 54.6 0.1 55.20.2 53.70.0 40.0 0.2
Birds 57.7 0.1 58.40.0 56.20.0 29.6 0.1 55.2 0.0 56.7 0.1 56.2 0.1 51.9 0.1
CIFAR-10 91.2 0.0 90.9 0.0 89.7 0.0 83.4 0.1 89.7 0.1 90.0 0.1 91.40.0 89.60.0
CIFAR-100 74.60.1 74.7 0.1 72.00.1 62.0 0.1 73.1 0.1 73.7 0.0 74.70.0 71.30.0
Caltech-101 91.9 0.1 92.40.2 91.50.2 75.4 0.1 91.1 0.1 90.8 0.3 91.2 0.1 91.2 0.1
Caltech-256 84.80.1 84.6 0.1 83.90.1 66.1 0.1 83.3 0.1 84.60.1 83.60.0 83.7 0.1
Cars 53.4 0.1 56.6 0.0 49.1 0.1 32.7 0.1 57.4 0.1 58.60.1 57.00.1 44.9 0.1
DTD 73.5 0.2 74.40.1 71.20.1 64.9 0.2 73.5 0.2 72.9 0.3 74.30.2 70.80.2
Flowers 91.6 0.1 92.7 0.1 90.9 0.1 85.6 0.1 92.2 0.1 92.3 0.1 93.00.0 87.60.1
Food-101 73.2 0.0 73.8 0.0 70.5 0.0 61.9 0.0 73.3 0.0 72.5 0.1 73.90.0 68.50.0
Pets 92.60.1 92.30.1 92.50.1 79.40.1 91.9 0.1 92.50.2 91.70.0 92.2 0.1
SUN397 60.1 0.0 60.40.0 58.10.0 47.4 0.0 59.1 0.1 59.9 0.0 60.3 0.0 57.8 0.1
90% Sparsity
Aircraft 49.2 0.1 55.5 0.1 48.7 0.1 16.5 0.2 54.1 0.1 56.60.1 52.90.1 44.0 0.2
Birds 57.7 0.1 58.70.0 55.40.1 11.4 0.1 53.3 0.0 57.2 0.1 55.2 0.1 52.7 0.1
CIFAR-10 91.20.0 91.00.0 89.4 0.0 67.0 0.1 90.0 0.1 90.2 0.1 90.6 0.0 88.9 0.0
CIFAR-100 74.60.1 74.30.0 71.5 0.0 42.2 0.1 72.8 0.1 73.4 0.1 73.7 0.1 70.5 0.0
Caltech-101 91.9 0.1 92.50.1 91.60.1 49.0 0.6 90.6 0.3 91.4 0.4 90.9 0.1 91.3 0.1
Caltech-256 84.80.1 84.50.0 82.9 0.0 42.0 0.1 81.9 0.0 84.5 0.1 82.6 0.0 83.0 0.1
Cars 53.4 0.1 56.0 0.1 50.2 0.0 15.4 0.1 55.5 0.1 60.50.1 54.80.1 46.7 0.0
DTD 73.50.2 73.7 0.2 72.40.2 54.7 0.1 72.6 0.3 72.7 0.2 73.80.1 71.00.2
Flowers 91.6 0.1 92.4 0.0 91.4 0.1 67.7 0.1 91.6 0.1 92.4 0.1 93.00.1 89.00.1
Food-101 73.2 0.0 73.80.0 71.10.0 46.9 0.0 71.7 0.0 72.6 0.0 72.6 0.0 69.2 0.0
Pets 92.60.1 91.90.1 92.0 0.1 43.8 0.2 91.1 0.1 91.9 0.2 91.1 0.1 92.0 0.1
SUN397 60.10.0 59.80.1 58.1 0.0 31.7 0.1 57.7 0.0 59.8 0.1 58.2 0.0 56.8 0.0
95% Sparsity
Aircraft 49.2 0.1 56.6 0.1 N /A 4.5 0.3 53.5 0.1 56.90.1 50.30.1 45.6 0.3
Birds 57.70.1 57.7 0.0 N /A 2.3 0.1 51.9 0.1 55.9 0.0 52.1 0.1 51.8 0.1
CIFAR-10 91.20.0 90.50.0 N /A 39.9 0.2 89.4 0.0 89.8 0.1 89.1 0.0 88.6 0.0
CIFAR-100 74.60.1 73.40.0 N /A 13.5 0.2 71.5 0.1 72.4 0.1 71.7 0.0 69.7 0.0
Caltech-101 91.90.1 91.60.1 N /A 20.1 0.5 89.0 0.1 91.4 0.1 90.0 0.2 91.0 0.2
Caltech-256 84.80.1 82.80.1 N /A 12.4 0.3 80.1 0.1 83.5 0.1 80.2 0.1 81.2 0.1
Cars 53.4 0.1 56.90.1 N /A 3.9 0.1 52.9 0.0 57.00.1 50.50.1 45.5 0.0
DTD 73.50.2 72.70.1 N /A 27.4 0.2 71.9 0.1 72.9 0.2 72.1 0.2 70.4 0.1
Flowers 91.6 0.1 93.00.1 N /A 27.8 0.6 91.0 0.1 92.4 0.1 91.9 0.1 89.6 0.0
Food-101 73.20.0 73.2 0.0 N /A 15.0 0.1 70.6 0.1 71.9 0.0 70.7 0.0 68.2 0.0
Pets 92.60.1 91.00.2 N /A 15.9 0.2 90.1 0.1 91.1 0.1 89.8 0.1 91.4 0.0
SUN397 60.10.0 58.20.0 N /A 8.4 0.2 55.9 0.1 58.3 0.1 56.3 0.0 55.1 0.1
98% Sparsity
Aircraft 49.2 0.1 54.80.1 N /A N /A N /A N /A 48.0 0.1 45.0 0.1
Birds 57.70.1 54.50.0 N /A N /A N /A N /A 43.7 0.0 48.1 0.1
CIFAR-10 91.20.0 89.20.0 N /A N /A N /A N /A 86.5 0.0 86.6 0.0
CIFAR-100 74.60.1 71.60.0 N /A N /A N /A N /A 67.4 0.0 67.8 0.0
Caltech-101 91.90.1 89.00.1 N /A N /A N /A N /A 86.3 0.1 88.5 0.1
Caltech-256 84.80.1 79.80.0 N /A N /A N /A N /A 73.4 0.1 77.1 0.0
Cars 53.40.1 52.10.0 N /A N /A N /A N /A 44.4 0.1 42.2 0.0
DTD 73.50.2 71.60.1 N /A N /A N /A N /A 68.4 0.2 68.3 0.1
Flowers 91.6 0.1 92.30.1 N /A N /A N /A N /A 90.8 0.1 89.5 0.1
Food-101 73.20.0 70.80.0 N /A N /A N /A N /A 65.3 0.0 66.5 0.0
Pets 92.60.1 89.20.1 N /A N /A N /A N /A 85.5 0.1 88.7 0.1
SUN397 60.10.0 55.10.0 N /A N /A N /A N /A 50.9 0.0 52.4 0.0
Table C.1. Transfer accuracy for sparse ResNet50 transfer with linear ﬁnetuning .
14

--- PAGE 15 ---
Pruning Strategy Dense AC/DC GMP LTH-T RigL ERK 1x RigL ERK 5x STR WoodFisher
80% Sparsity
Aircraft 83.6 0.4 83.3 0.1 84.40.2 84.7 0.5 82.60.3 82.4 0.2 79.8 0.3 84.80.2
Birds 72.4 0.3 69.9 0.2 72.5 0.2 71.4 0.1 72.3 0.3 73.40.1 68.10.1 72.4 0.4
CIFAR-10 97.40.0 96.90.1 97.2 0.0 97.0 0.0 96.9 0.0 97.1 0.0 96.5 0.1 97.2 0.1
CIFAR-100 85.60.2 84.90.2 85.1 0.0 84.4 0.2 83.6 0.2 84.1 0.4 83.6 0.2 85.1 0.1
Caltech-101 93.50.1 92.50.2 93.70.5 92.10.5 92.5 0.1 92.0 0.3 90.7 0.6 93.70.1
Caltech-256 86.10.1 85.40.2 85.1 0.2 83.1 0.1 83.8 0.1 84.2 0.2 84.0 0.1 85.1 0.1
Cars 90.30.2 89.20.1 90.30.1 89.90.0 89.4 0.1 89.6 0.1 87.8 0.1 90.50.2
DTD 76.20.3 75.70.5 75.4 0.1 75.2 0.4 74.5 0.2 74.2 0.2 73.7 0.6 75.4 0.3
Flowers 95.0 0.1 94.7 0.2 95.9 0.2 93.9 0.2 95.7 0.2 96.10.1 93.70.2 95.5 0.2
Food-101 87.30.1 86.90.1 87.40.1 86.90.1 86.9 0.1 87.20.1 85.90.1 87.40.1
Pets 93.40.1 92.50.0 93.40.1 92.90.1 92.2 0.1 92.4 0.1 92.1 0.1 93.30.3
SUN397 64.80.0 64.00.0 63.1 0.1 61.7 0.2 62.2 0.2 62.0 0.3 62.6 0.1 62.8 0.1
90% Sparsity
Aircraft 83.6 0.4 82.8 1.0 83.90.7 84.9 0.3 81.60.5 83.0 0.4 78.7 0.4 84.50.4
Birds 72.40.3 68.50.1 70.5 0.1 67.8 0.2 70.3 0.0 72.90.2 66.00.2 71.6 0.2
CIFAR-10 97.40.0 96.60.1 97.1 0.0 96.6 0.2 96.4 0.1 97.0 0.1 96.1 0.1 97.0 0.1
CIFAR-100 85.60.2 83.90.1 84.4 0.0 83.0 0.1 83.0 0.2 83.7 0.3 82.9 0.2 84.4 0.2
Caltech-101 93.50.1 92.60.2 92.9 0.2 84.5 0.3 91.7 0.3 92.3 0.4 90.9 0.3 93.90.3
Caltech-256 86.10.1 84.80.1 83.7 0.3 78.6 0.1 82.7 0.2 84.0 0.1 83.1 0.2 84.0 0.1
Cars 90.30.2 88.50.2 89.5 0.0 89.5 0.1 88.4 0.1 89.2 0.1 86.7 0.2 90.00.2
DTD 76.20.3 75.20.1 74.2 0.1 71.9 0.1 73.4 0.4 75.2 0.8 73.2 0.4 75.5 0.4
Flowers 95.0 0.1 94.6 0.1 95.3 0.1 89.8 0.2 95.5 0.1 96.10.1 93.40.4 95.5 0.3
Food-101 87.30.1 86.60.1 86.8 0.1 86.4 0.1 85.9 0.1 87.30.2 84.80.0 87.00.1
Pets 93.40.1 92.10.1 92.2 0.1 91.1 0.2 91.4 0.2 92.3 0.1 91.7 0.2 92.7 0.3
SUN397 64.80.0 63.00.0 62.5 0.2 58.3 0.2 61.3 0.1 62.0 0.2 61.2 0.0 62.3 0.1
95% Sparsity
Aircraft 83.60.4 81.20.4 N /A 82.6 0.8 80.7 0.1 82.5 0.4 76.7 0.8 83.60.6
Birds 72.40.3 66.90.1 N /A 62.2 0.1 68.3 0.2 71.6 0.1 62.3 0.1 69.9 0.1
CIFAR-10 97.40.0 96.20.1 N /A 95.5 0.1 96.0 0.1 96.6 0.1 95.4 0.1 96.7 0.1
CIFAR-100 85.60.2 82.90.1 N /A 80.0 0.1 82.0 0.2 82.8 0.0 80.9 0.3 83.1 0.1
Caltech-101 93.50.1 91.90.2 N /A 65.3 0.8 90.7 0.4 92.2 0.3 89.8 0.1 92.0 0.3
Caltech-256 86.10.1 83.10.0 N /A 71.8 0.3 81.1 0.2 83.1 0.2 80.3 0.0 82.4 0.1
Cars 90.30.2 87.60.1 N /A 87.5 0.4 87.9 0.3 88.9 0.2 84.9 0.2 88.9 0.2
DTD 76.20.3 74.10.4 N /A 67.1 0.8 73.3 0.2 73.5 0.2 72.6 0.4 73.7 0.3
Flowers 95.0 0.1 94.1 0.3 N /A 76.0 1.3 94.9 0.3 96.00.0 93.00.3 95.0 0.3
Food-101 87.30.1 85.50.0 N /A 85.4 0.1 85.1 0.2 86.6 0.0 83.0 0.1 86.3 0.1
Pets 93.40.1 91.00.1 N /A 84.5 0.5 90.1 0.2 91.6 0.3 89.9 0.3 92.3 0.3
SUN397 64.80.0 61.40.2 N /A 51.4 0.3 60.0 0.3 61.1 0.2 59.0 0.1 60.9 0.1
98% Sparsity
Aircraft 83.60.4 79.10.2 N /A N /A N /A N /A 72.0 0.2 81.4 0.3
Birds 72.40.3 63.40.1 N /A N /A N /A N /A 54.1 0.1 65.4 0.3
CIFAR-10 97.40.0 95.00.1 N /A N /A N /A N /A 93.8 0.1 96.0 0.0
CIFAR-100 85.60.2 79.80.1 N /A N /A N /A N /A 75.9 0.2 80.7 0.2
Caltech-101 93.50.1 88.90.1 N /A N /A N /A N /A 85.2 0.6 89.8 0.3
Caltech-256 86.10.1 80.30.1 N /A N /A N /A N /A 74.2 0.0 78.9 0.1
Cars 90.30.2 85.50.2 N /A N /A N /A N /A 79.9 0.5 86.8 0.1
DTD 76.20.3 72.60.1 N /A N /A N /A N /A 69.4 0.3 71.8 0.1
Flowers 95.00.1 92.90.1 N /A N /A N /A N /A 91.8 0.3 94.0 0.2
Food-101 87.30.1 83.20.0 N /A N /A N /A N /A 77.9 0.1 84.2 0.1
Pets 93.40.1 88.80.2 N /A N /A N /A N /A 85.5 0.1 89.8 0.1
SUN397 64.80.0 58.40.1 N /A N /A N /A N /A 53.8 0.2 58.5 0.1
Table C.2. Transfer accuracy for sparse ResNet50 transfer with full ﬁnetuning .
15

--- PAGE 16 ---
Pruning Strategy Dense AC/DC GMP RigL ERK 1x RigL ERK 5x STR WoodFisher
80% Sparsity
Aircraft 50.3 56.7 46.9 55.4 55.6 54.6 43.1
Birds 56.7 57.7 54.6 55.1 56.2 55.8 50.7
Caltech-101 91.8 92.0 91.2 91.5 91.2 91.4 91.3
Caltech-256 84.3 84.6 83.2 83.3 84.6 83.3 83.0
Cars 56.2 59.5 50.1 58.9 60.4 60.0 46.5
CIFAR-10 88.5 88.3 87.5 86.9 88.1 88.9 86.3
CIFAR-100 72.3 72.4 69.1 70.7 71.8 72.9 68.1
DTD 73.2 72.8 69.9 72.9 73.1 73.3 70.0
Flowers 92.9 93.9 92.0 93.3 93.3 93.9 89.0
Food-101 67.7 68.6 65.3 67.2 68.1 68.1 62.8
Pets 92.5 91.9 92.2 91.3 92.2 91.5 91.4
SUN397 58.5 59.3 56.4 58.0 59.4 59.4 55.8
90% Sparsity
Aircraft 50.3 56.7 49.6 55.3 57.4 54.6 45.0
Birds 56.7 57.7 54.3 52.8 56.9 54.7 51.5
Caltech-101 91.8 92.3 91.0 90.5 91.5 90.4 91.2
Caltech-256 84.3 84.1 82.6 81.7 84.7 82.3 82.5
Cars 56.2 59.0 52.4 57.3 62.0 57.8 48.4
CIFAR-10 88.5 88.5 86.7 87.1 87.5 87.4 86.2
CIFAR-100 72.3 71.6 68.9 70.1 72.0 72.0 67.6
DTD 73.2 72.8 71.8 71.5 71.6 72.2 69.3
Flowers 92.9 93.4 92.7 92.6 93.3 94.1 90.2
Food-101 67.7 67.7 65.9 65.0 67.5 67.3 63.6
Pets 92.5 91.6 91.8 91.3 91.5 90.5 91.1
SUN397 58.5 58.2 56.3 56.9 59.0 57.2 54.6
95% Sparsity
Aircraft 50.3 57.2 N /A 54.3 57.4 51.5 45.7
Birds 56.7 56.4 N /A 50.8 55.5 51.1 49.9
Caltech-101 91.8 91.6 N /A 89.4 91.7 89.9 90.6
Caltech-256 84.3 82.4 N /A 80.1 83.5 80.0 80.8
Cars 56.2 59.4 N /A 55.1 58.8 53.0 46.8
CIFAR-10 88.5 87.9 N /A 86.7 86.9 86.4 86.3
CIFAR-100 72.3 69.6 N /A 68.8 70.0 69.6 66.4
DTD 73.2 71.3 N /A 71.1 72.8 70.3 70.1
Flowers 92.9 94.2 N /A 92.3 93.5 93.1 90.8
Food-101 67.7 66.6 N /A 63.6 66.1 64.8 63.0
Pets 92.5 90.4 N /A 89.7 90.9 89.2 90.5
SUN397 58.5 56.8 N /A 54.9 57.7 54.8 52.7
Table C.3. Validation accuracy for sparse ResNet50 transfer with linear ﬁnetuning using the L-BFGS optimizer
16

--- PAGE 17 ---
D. Results for Extended Training Schedule
Sparsity Method Imagenet Validation Accuracy
0% Dense 76.8%
Dense 2x 77.1%
80% AC/DC 76.2%
AC/DC 3x 77.5%
RigL ERK 1x 74.8%
RigL ERK 5x 75.8%
90% AC/DC 75.2%
AC/DC 3x 76.8%
AC/DC 5x 77.2%
RigL ERK 1x 73.2%
RigL ERK 5x 75.7%
95% AC/DC 73.1%
AC/DC 3x 75.3%
RigL ERK 1x 70.1%
RigL ERK 5x 74.0%
Table D.4. Accuracy of models with extended training time, evaluated on different ImageNet validation sets.
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Average Relative Increase in Error across Sparsities
80 90 95
Sparsity (%)5
051015Relative Error Increase (%)ResNet50 - Linear Finetuning
80 90 95
Sparsity (%)051015202530ResNet50 - Full Finetuning
Dense Dense 2x AC/DC AC/DC 3x AC/DC 5x RigL ERK 1x RigL ERK 5x
Figure D.3. (ResNet50) Overall performance for extended-time runs of AC/DC and RigL. The AC/DC 5x was only done for 90% sparsity;
its performance is almost the same as AC/DC 3x for linear ﬁnetuning and as dense for full ﬁnetuning.
Noting the improved performance of models compressed RigL ERK when running the sparse upstream training algorithm
for 5x the number of epochs, we repeated the experiment with AC/DC, compressing ResNet50 models to 80%, 90%, and
95% for 3x the training time (300 epochs) and to 90% for 5x the training time (500 epochs). The latter experiment was only
done at one compression level due to the high computational costs of upstream training. We also trained a dense model with
2x training time (200 epochs). We observe that extended training increases performance on ImageNet in all cases (see Table
D.4), especially for sparse models. In particular, extending training 3x raises the ImageNet validation accuracy by 1.3% at
80% sparsity (from 76.2% to 77.5%), by 1.5% at 90% (from 75.2% to 76.7%), and by 2.2% at 95% sparsity (from 73.1%
to 75.3%). Training for 5x epochs at 90% sparsity recovers the dense 2x validation accuracy. We see a similar trend with
RigL ERK 1x/5x results. Thus, for regularization-based methods, extended training can substantially improve validation
performance.
The same behavior extends to transfer learning. The complete results of the extended-training transfer experiments are
presented in tables D.5 for linear downstream ﬁnetuning and D.6 for full downstream ﬁnetuning. We also summarize the
results in ﬁgure D.3, relying on the same average relative increase in error metric that we deﬁne in Section 2.1. We observe
that, while extended dense training overall gives an additional beneﬁt for full ﬁnetuning balanced by a commensurate loss for
linear ﬁnetuning, extended training of compressed models gives a performance improvement in both scenarios, for both RigL
and AC/DC. Notably, 3x upstream training with AC/DC allows 80% sparse models to perform as well as dense 1x models
when doing full downstream ﬁnetuning, and 5x upstream training with AC/DC allows even 90% sparse models to do so.
These results suggest that for regularization-based methods, the extra investment in upstream training can result in upstream
models that transfer very well under both full and linear ﬁnetuning.
17

--- PAGE 18 ---
Dense Dense 2x AC/DC AC/DC 3x AC/DC 5x RigL ERK 1x RigL ERK 5x
80% Sparsity
Aircraft 49.2 0.1 50.0 0.1 55.10.1 52.50.2 N /A 54.60.1 55.20.2
Birds 57.7 0.1 57.1 0.0 58.40.0 59.20.0 N /A 55.20.0 56.7 0.1
CIFAR-10 91.2 0.0 90.2 0.0 90.90.0 91.40.0 N /A 89.70.1 90.0 0.1
CIFAR-100 74.6 0.1 73.7 0.0 74.70.1 75.30.0 N /A 73.10.1 73.7 0.0
Caltech-101 91.9 0.1 92.10.2 92.40.2 92.3 0.2 N /A 91.10.1 90.8 0.3
Caltech-256 84.8 0.1 84.6 0.1 84.60.1 85.40.1 N /A 83.30.1 84.6 0.1
Cars 53.4 0.1 51.4 0.1 56.60.0 56.2 0.1 N /A 57.40.1 58.60.1
DTD 73.5 0.2 73.1 0.2 74.40.1 74.1 0.2 N /A 73.50.2 72.9 0.3
Flowers 91.6 0.1 91.1 0.1 92.70.1 92.6 0.1 N /A 92.20.1 92.3 0.1
Food-101 73.2 0.0 72.2 0.0 73.80.0 74.80.0 N /A 73.30.0 72.5 0.1
Pets 92.60.1 91.90.1 92.30.1 92.80.1 N /A 91.90.1 92.50.2
SUN397 60.1 0.0 59.9 0.0 60.40.0 60.50.0 N /A 59.10.1 59.9 0.0
90% Sparsity
Aircraft 49.2 0.1 50.0 0.1 55.50.1 54.6 0.1 55.5 0.0 54.10.1 56.60.1
Birds 57.7 0.1 57.1 0.0 58.70.0 59.7 0.1 60.40.1 53.30.0 57.2 0.1
CIFAR-10 91.20.0 90.20.0 91.00.0 90.9 0.0 90.3 0.0 90.00.1 90.2 0.1
CIFAR-100 74.60.1 73.70.0 74.30.0 74.70.0 74.20.0 72.80.1 73.4 0.1
Caltech-101 91.9 0.1 92.1 0.2 92.50.1 92.90.2 92.8 0.2 90.60.3 91.4 0.4
Caltech-256 84.8 0.1 84.6 0.1 84.50.0 85.30.1 85.3 0.1 81.90.0 84.5 0.1
Cars 53.4 0.1 51.4 0.1 56.00.1 58.7 0.1 58.4 0.1 55.50.1 60.50.1
DTD 73.5 0.2 73.1 0.2 73.70.2 74.20.3 73.9 0.2 72.60.3 72.7 0.2
Flowers 91.6 0.1 91.1 0.1 92.40.0 92.6 0.0 92.80.0 91.60.1 92.4 0.1
Food-101 73.2 0.0 72.2 0.0 73.80.0 75.1 0.0 75.20.0 71.70.0 72.6 0.0
Pets 92.6 0.1 91.9 0.1 91.90.1 92.50.1 92.6 0.2 91.10.1 91.9 0.2
SUN397 60.1 0.0 59.9 0.0 59.80.1 60.3 0.0 61.20.0 57.70.0 59.8 0.1
95% Sparsity
Aircraft 49.2 0.1 50.0 0.1 56.60.1 55.6 0.0 N /A 53.50.1 56.90.1
Birds 57.7 0.1 57.1 0.0 57.70.0 59.20.1 N /A 51.90.1 55.9 0.0
CIFAR-10 91.20.0 90.20.0 90.50.0 90.2 0.0 N /A 89.40.0 89.8 0.1
CIFAR-100 74.60.1 73.70.0 73.40.0 74.3 0.1 N /A 71.50.1 72.4 0.1
Caltech-101 91.90.1 92.1 0.2 91.60.1 92.30.3 N /A 89.00.1 91.4 0.1
Caltech-256 84.80.1 84.6 0.1 82.80.1 84.0 0.1 N /A 80.10.1 83.5 0.1
Cars 53.4 0.1 51.4 0.1 56.90.1 57.40.0 N /A 52.90.0 57.0 0.1
DTD 73.5 0.2 73.1 0.2 72.70.1 74.70.2 N /A 71.90.1 72.9 0.2
Flowers 91.6 0.1 91.1 0.1 93.00.1 92.50.1 N /A 91.00.1 92.4 0.1
Food-101 73.2 0.0 72.2 0.0 73.20.0 74.70.0 N /A 70.60.1 71.9 0.0
Pets 92.60.1 91.90.1 91.00.2 91.5 0.1 N /A 90.10.1 91.1 0.1
SUN397 60.10.0 59.90.0 58.20.0 59.7 0.0 N /A 55.90.1 58.3 0.1
Table D.5. Transfer accuracy for extended training time for ResNet50 with linear ﬁnetuning .
E. Experiments on ResNet18 and ResNet34
In this section, we further validate our ﬁndings for linear ﬁnetuning from ResNet50 on two additional smaller architectures,
namely ResNet18 and ResNet34. Speciﬁcally, we test whether regularization pruning methods generally have better transfer
potential than progressive sparsiﬁcation methods, and whether regularization pruning methods improve over dense models for
ﬁne-grained classiﬁcation tasks. For this purpose, we trained AC/DC and GMP on ImageNet using ResNet18 and ResNet34
models, for 80% and 90% sparsity, using the same hyperparameters as for ResNet50. For both ResNet18 and ResNet34,
there was a fairly large gap in ImageNet validation accuracy between GMP and AC/DC for both 80% and 90% sparsity, in
favor of GMP, which almost recovered the baseline accuracy at 80% sparsity.
We show the results for linear ﬁnetuning using AC/DC and GMP in Table E.7 for ResNet18, respectively Table E.8 for
ResNet34. Interestingly, despite the larger gap in ImageNet validation accuracy between GMP and AC/DC (with GMP being
18

--- PAGE 19 ---
Dense Dense 2x AC/DC AC/DC 3x AC/DC 5x RigL ERK 1x RigL ERK 5x
80% Sparsity
Aircraft 83.6 0.4 84.30.2 83.30.1 83.2 0.3 N /A 82.60.3 82.4 0.2
Birds 72.4 0.3 73.50.1 69.90.2 72.5 0.2 N /A 72.30.3 73.40.1
CIFAR-10 97.40.0 97.4 0.0 96.90.1 97.50.1 N /A 96.90.0 97.1 0.0
CIFAR-100 85.60.2 85.8 0.2 84.90.2 85.3 0.1 N /A 83.60.2 84.1 0.4
Caltech-101 93.5 0.1 93.90.1 92.50.2 93.4 0.2 N /A 92.50.1 92.0 0.3
Caltech-256 86.1 0.1 86.50.2 85.40.2 86.70.1 N /A 83.80.1 84.2 0.2
Cars 90.30.2 90.5 0.2 89.20.1 89.8 0.3 N /A 89.40.1 89.6 0.1
DTD 76.2 0.3 76.90.3 75.70.5 76.6 0.0 N /A 74.50.2 74.2 0.2
Flowers 95.0 0.1 95.5 0.2 94.70.2 95.1 0.2 N /A 95.70.2 96.10.1
Food-101 87.3 0.1 87.50.1 86.90.1 87.70.1 N /A 86.90.1 87.20.1
Pets 93.40.1 93.4 0.2 92.50.0 93.40.2 N /A 92.20.1 92.4 0.1
SUN397 64.8 0.0 65.10.0 64.00.0 64.8 0.1 N /A 62.20.2 62.0 0.3
90% Sparsity
Aircraft 83.6 0.4 84.30.2 82.81.0 82.6 0.2 83.50.4 81.60.5 83.0 0.4
Birds 72.4 0.3 73.50.1 68.50.1 71.6 0.2 72.8 0.2 70.30.0 72.9 0.2
CIFAR-10 97.40.0 97.4 0.0 96.60.1 97.0 0.1 97.1 0.1 96.40.1 97.0 0.1
CIFAR-100 85.60.2 85.8 0.2 83.90.1 84.7 0.1 85.3 0.1 83.00.2 83.7 0.3
Caltech-101 93.5 0.1 93.90.1 92.60.2 93.0 0.0 93.2 0.1 91.70.3 92.3 0.4
Caltech-256 86.1 0.1 86.50.2 84.80.1 86.1 0.1 86.50.1 82.70.2 84.0 0.1
Cars 90.30.2 90.5 0.2 88.50.2 89.3 0.1 89.8 0.1 88.40.1 89.2 0.1
DTD 76.2 0.3 76.90.3 75.20.1 75.3 0.2 76.4 0.2 73.40.4 75.2 0.8
Flowers 95.0 0.1 95.5 0.2 94.60.1 95.4 0.1 95.90.1 95.50.1 96.10.1
Food-101 87.3 0.1 87.50.1 86.60.1 87.4 0.1 87.70.1 85.90.1 87.3 0.2
Pets 93.40.1 93.4 0.2 92.10.1 92.6 0.1 93.0 0.1 91.40.2 92.3 0.1
SUN397 64.8 0.0 65.10.0 63.00.0 64.3 0.0 64.8 0.1 61.30.1 62.0 0.2
95% Sparsity
Aircraft 83.6 0.4 84.30.2 81.20.4 82.2 0.3 N /A 80.70.1 82.5 0.4
Birds 72.4 0.3 73.50.1 66.90.1 70.1 0.1 N /A 68.30.2 71.6 0.1
CIFAR-10 97.40.0 97.4 0.0 96.20.1 96.6 0.1 N /A 96.00.1 96.6 0.1
CIFAR-100 85.60.2 85.8 0.2 82.90.1 84.0 0.1 N /A 82.00.2 82.8 0.0
Caltech-101 93.5 0.1 93.90.1 91.90.2 92.9 0.1 N /A 90.70.4 92.2 0.3
Caltech-256 86.1 0.1 86.50.2 83.10.0 85.3 0.0 N /A 81.10.2 83.1 0.2
Cars 90.30.2 90.5 0.2 87.60.1 89.0 0.1 N /A 87.90.3 88.9 0.2
DTD 76.2 0.3 76.90.3 74.10.4 75.0 0.4 N /A 73.30.2 73.5 0.2
Flowers 95.0 0.1 95.5 0.2 94.10.3 95.0 0.2 N /A 94.90.3 96.00.0
Food-101 87.30.1 87.5 0.1 85.50.0 86.7 0.1 N /A 85.10.2 86.6 0.0
Pets 93.40.1 93.4 0.2 91.00.1 91.6 0.1 N /A 90.10.2 91.6 0.3
SUN397 64.8 0.0 65.10.0 61.40.2 63.0 0.0 N /A 60.00.3 61.1 0.2
Table D.6. Transfer accuracy for extended training time for ResNet50 with full ﬁnetuning .
closer to the dense baseline), AC/DC tends to outperform GMP in terms of transfer performance, on most of the downstream
tasks. Furthermore, we observe that AC/DC tends to transfer better than the dense baseline, especially for specialized or
ﬁne-grained downstream tasks. These observations conﬁrm our ﬁndings for linear ﬁnetuning on ResNet50.
F. Experiments on MobileNetV1
The MobileNet [31] architecture is a natural choice for devices with limited computational resources. We measure the
results of sparse transfer with full and linear ﬁnetuning on the same downstream tasks starting from dense ImageNet models
pruned using regularization-based and progressive sparsiﬁcation methods. Speciﬁcally, we use AC/DC, STR for regulariza-
tion methods and M-FAC [16] for the progressive sparsiﬁcation category.
M-FAC is a framework for efﬁciently computing high-dimensional inverse-Hessian vector products, which can be applied
to different scenarios that use second-order information. In particular, one such instance is pruning, where M-FAC aims to
19

--- PAGE 20 ---
Pruning Strategy Dense GMP 80% GMP 90% AC/DC 80% AC/DC 90%
Task
Aircraft 47.7 0.1 45.5 0.1 45.6 0.1 48.0 0.1 48.10.1
Birds 49.4 0.1 49.3 0.1 48.1 0.0 50.20.0 48.70.1
CIFAR-10 87.2 0.0 87.40.0 87.20.0 87.40.0 87.20.1
CIFAR-100 68.9 0.0 68.1 0.0 69.1 0.0 69.60.1 68.90.0
Caltech-101 89.4 0.3 89.80.3 88.60.2 89.0 0.2 88.2 0.4
Caltech-256 79.40.1 78.30.1 77.3 0.1 78.8 0.1 77.3 0.1
Cars 45.6 0.1 45.0 0.1 44.4 0.1 46.2 0.1 46.70.1
DTD 68.1 0.1 68.2 0.3 66.9 0.2 68.60.2 68.40.2
Flowers 89.0 0.1 89.3 0.1 89.3 0.1 89.9 0.1 90.20.1
Food-101 64.9 0.0 65.0 0.0 64.6 0.0 65.60.0 65.30.0
Pets 90.10.1 89.80.1 89.4 0.2 89.7 0.1 89.4 0.1
SUN397 54.80.1 53.80.1 52.9 0.1 54.80.1 53.50.1
Table E.7. Transfer accuracy for different pruning methods for linear ﬁnetuning on ResNet18
Pruning Strategy Dense GMP 80% GMP 90% AC/DC 80% AC/DC 90%
Task
Aircraft 45.8 0.2 43.5 0.2 44.9 0.1 48.7 0.1 50.70.2
Birds 52.9 0.0 53.0 0.1 53.0 0.1 54.50.1 54.20.1
CIFAR-10 89.5 0.0 89.1 0.0 88.5 0.0 89.60.0 89.00.0
CIFAR-100 71.0 0.0 70.4 0.1 70.2 0.1 72.00.0 72.0 0.0
Caltech-101 92.50.2 91.80.3 90.9 0.2 92.0 0.3 91.8 0.4
Caltech-256 82.2 0.1 81.8 0.0 81.4 0.1 82.30.1 81.20.1
Cars 47.3 0.1 46.0 0.1 45.6 0.1 48.5 0.1 49.00.1
DTD 69.5 0.1 68.6 0.5 68.6 0.2 70.40.3 69.60.2
Flowers 88.1 0.1 88.5 0.1 89.0 0.1 90.0 0.1 91.10.1
Food-101 66.8 0.0 66.7 0.0 67.4 0.0 68.2 0.0 68.80.0
Pets 92.0 0.1 92.50.1 91.40.1 91.7 0.1 91.1 0.2
SUN397 55.9 0.1 55.4 0.1 55.0 0.1 56.80.1 55.60.1
Table E.8. Transfer accuracy for different pruning methods for linear ﬁnetuning on ResNet34
solve the same optimization problem as WoodFisher, and thus from this point of view these methods are very similar. In
particular, it has been shown [16] that M-FAC outperforms WoodFisher on ImageNet models, in terms of accuracy at a given
sparsity level. Speciﬁcally, for MobileNet, M-FAC surpasses all existing methods at 90% sparsity, reaching 67.2% validation
accuracy. For this reason, we included M-FAC, in favor of WoodFisher, to our list of progressive sparsiﬁcation methods for
MobileNetV1.
Due to the smaller size of the MobileNetV1 architecture, we additionally test the effect that lower sparsity levels have on
the transfer performance, by training on ImageNet AC/DC models at 30%, 40% and 50% sparsity; these models fully recover
the dense baseline accuracy on ImageNet.
The results on MobileNet are presented in Figure F.4 and Table F.9 for linear ﬁnetuning and Figure F.5 and Table F.10 for
full ﬁnetuning. The results for linear ﬁnetuning are obtained after running from ﬁve different random seeds, and the mean
and standard deviation are reported. However, the experiments for full ﬁnetuning were each run once. For both linear and full
ﬁnetuning, we observe that generally the performance decays faster with increased sparsity, compared to ResNet50; this is
expected, given the lower parameter count for MobileNet and the larger gap in ImageNet validation accuracy between dense
and sparse models.
For linear ﬁnetuning, we observe AC/DC outperforms STR at both 75% and 90% sparsity. Furthermore, AC/DC tends to
be close to M-FAC at 75% sparsity, while at 90% sparsity M-FAC performs better on almost half of the tasks. Differently from
ResNet50, for MobileNet neither regularization based nor progressive sparsiﬁcation models outperform the dense baseline,
20

--- PAGE 21 ---
30 40 50 75 9048505254Top-1 Acc. (%)Aircraft
30 40 50 75 9045.047.550.052.5Birds
30 40 50 75 90868890Caltech-101
30 40 50 75 9074767880Caltech-256
30 40 50 75 9047.550.052.555.0Cars
30 40 50 75 90868788CIFAR-10
30 40 50 75 90
Sparsity (%)66687072Top-1 Acc. (%)CIFAR-100
30 40 50 75 90
Sparsity (%)687072DTD
30 40 50 75 90
Sparsity (%)91.091.592.092.5Flowers
30 40 50 75 90
Sparsity (%)666870Food-101
30 40 50 75 90
Sparsity (%)87888990Pets
30 40 50 75 90
Sparsity (%)5456SUN397MobileNet Linear Finetuning - Validation Accuracy
Dense AC/DC STR M-FACFigure F.4. (MobileNetV1) Per-dataset downstream validation accuracy for transfer learining with linear ﬁnetuning.
30 40 50 75 9074767880Top-1 Acc. (%)Aircraft
30 40 50 75 90556065Birds
30 40 50 75 9084868890Caltech-101
30 40 50 75 9074767880Caltech-256
30 40 50 75 90828486Cars
30 40 50 75 9093.594.094.595.095.5CIFAR-10
30 40 50 75 90
Sparsity (%)767880Top-1 Acc. (%)CIFAR-100
30 40 50 75 90
Sparsity (%)6970717273DTD
30 40 50 75 90
Sparsity (%)9091929394Flowers
30 40 50 75 90
Sparsity (%)808284Food-101
30 40 50 75 90
Sparsity (%)868890Pets
30 40 50 75 90
Sparsity (%)54565860SUN397MobileNet Full Finetuning - Validation Accuracy
Dense AC/DC STR M-FAC
Figure F.5. (MobileNetV1) Per-dataset downstream validation accuracy for transfer learining with full ﬁnetuning.
at higher sparsity (75% and 90%). We observe at lower sparsity (30% and 50%) a few instances where sparse models slightly
outperform the dense baseline (Birds, Cars, DTD), but generally the differences are not signiﬁcant.
In the case of full ﬁnetuning, we observe that the performance of sparse models decays more quickly than for ResNet50,
and even at lower sparsity (30-50%) there is a gap in transfer performance compared to the dense baseline. Furthermore,
AC/DC outperforms STR and M-FAC at both 75% and 90% sparsity on all downstream tasks. Overall, the results for
MobileNet indicate that the transfer performance is signiﬁcantly affected by the sparsity of the backbone model, for both
linear and full ﬁnetuning. Moreover, the experiments on MobileNet seem to suggest that although some of the conclusions
derived from the ResNet experiments are conﬁrmed (e.g. sparse models usually have similar or slightly better performance to
the dense baseline for linear ﬁnetuning), the guidelines for the preferred sparsity method in a given scenario might be speciﬁc
to the choice of the backbone architecture.
Finally, we consider the accuracy tradeoff of using a smaller network such as MobileNet (4.2M trainable weights) versus a
larger model, ResNet50 (25.5M trainable weights), but pruned to 90% sparsity. We present linear and full ﬁnetuning accuracy
results for these two scenarios for an easier comparison in Tables F.11 and F.12. We use the overall best pruning strategy for
each type of transfer on ResNet50: AC/DC for linear ﬁnetuning and WoodFisher for full ﬁnetuning. Note that these same
results are also presented in Tables C.2 and C.1, F.10, and F.9.
We observe that generally, pruning ResNet50 to 80 or even 90% sparsity results in higher accuracy than MobileNet, for
both linear and full ﬁnetuning. However, in almost all cases, the gap is below 5%. This ﬁnding conﬁrms conventional wisdom
that training and pruning large networks generally results in higher accuracy than training dense small networks from scratch.
21

--- PAGE 22 ---
Pruning Strategy DenseAC/DC
30%AC/DC
40%AC/DC
50%AC/DC
75%AC/DC
90%M-FAC
75%M-FAC
89%STR
75%STR
90%
Task
Aircraft 54.1 0.2 53.5 0.1 54.20.1 54.10.1 53.5 0.2 52.6 0.3 53.4 0.1 53.7 0.1 49.7 0.1 47.2 0.2
Birds 52.7 0.1 53.0 0.1 53.60.1 52.80.0 52.6 0.1 50.3 0.1 52.1 0.1 49.2 0.1 49.0 0.0 44.2 0.0
CIFAR-10 88.3 0.1 88.4 0.0 88.1 0.0 87.8 0.0 88.50.0 87.40.1 87.5 0.0 87.0 0.0 86.9 0.1 85.4 0.0
CIFAR-100 71.90.0 70.90.1 71.1 0.0 70.2 0.0 70.8 0.0 68.0 0.0 69.3 0.0 68.6 0.0 69.3 0.0 66.3 0.0
Caltech-101 90.30.1 89.70.1 89.8 0.2 89.7 0.2 89.5 0.1 88.4 0.3 89.2 0.1 87.2 0.1 88.0 0.2 85.2 0.2
Caltech-256 80.2 0.1 80.4 0.0 80.2 0.1 80.50.0 79.20.0 77.3 0.1 79.0 0.0 76.9 0.0 77.8 0.1 73.8 0.1
Cars 55.5 0.0 55.90.1 55.10.1 54.9 0.1 54.5 0.1 52.9 0.0 55.90.1 54.50.1 49.9 0.2 47.1 0.1
DTD 70.8 0.2 70.4 0.2 70.3 0.0 71.40.4 70.90.2 68.8 0.1 70.7 0.2 69.6 0.2 70.6 0.2 67.2 0.1
Flowers 92.80.1 92.60.1 92.2 0.1 92.6 0.1 92.6 0.1 91.9 0.1 92.6 0.1 92.0 0.1 91.4 0.1 90.8 0.1
Food-101 70.6 0.0 70.70.0 70.60.0 70.3 0.0 70.2 0.0 68.6 0.0 69.8 0.0 69.1 0.0 67.7 0.0 65.3 0.0
Pets 90.70.1 90.60.1 90.5 0.1 90.4 0.1 90.1 0.1 89.0 0.1 89.9 0.1 88.5 0.1 89.3 0.1 86.9 0.2
SUN397 57.1 0.0 57.20.1 57.20.0 56.8 0.0 56.0 0.0 53.8 0.0 56.3 0.1 54.6 0.0 55.1 0.0 52.5 0.0
Table F.9. Transfer accuracy for linear ﬁnetuning using sparse MobileNet models
Pruning Strategy DenseAC/DC
30%AC/DC
40%AC/DC
50%AC/DC
75%AC/DC
90%M-FAC
75%M-FAC
89%STR
75%STR
90%
Aircraft 80.9 79.7 80.1 79.9 79.0 76.9 76.6 74.5 74.4 73.0
Birds 66.6 66.3 66.5 65.5 63.4 59.5 62.9 58.5 56.1 53.1
CIFAR-10 95.7 95.6 95.5 95.4 95.0 94.2 94.5 93.4 93.8 93.3
CIFAR-100 81.6 81.0 81.3 81.0 80.0 77.7 78.8 76.1 77.2 75.1
Caltech-101 91.0 89.6 89.5 90.0 88.0 87.9 86.8 84.6 85.1 83.4
Caltech-256 80.9 81.2 80.9 81.1 80.0 78.0 79.4 77.2 76.7 73.2
Cars 87.5 87.6 87.6 87.3 86.5 84.0 84.9 82.4 84.1 81.5
DTD 73.6 71.4 72.3 72.3 71.8 71.1 71.2 69.1 70.4 69.6
Flowers 93.9 94.1 94.0 93.9 93.4 92.6 91.9 90.9 89.8 90.6
Food-101 85.2 85.0 85.1 84.7 83.6 81.8 83.4 80.8 81.2 79.7
Pets 91.3 90.8 90.7 90.2 89.9 88.2 88.9 86.7 86.9 85.3
SUN397 60.7 60.7 60.3 60.2 59.2 57.1 58.2 55.9 55.1 53.8
Table F.10. Transfer accuracy for full ﬁnetuning using sparse MobileNet models
ModelMobileNet
DenseResNet50
AC/DC 80%ResNet50
AC/DC 90%
Aircraft 54.1 0.2 55.1 0.1 55.5 0.1
Birds 52.7 0.1 58.4 0.0 58.7 0.0
CIFAR-10 88.3 0.1 90.9 0.0 91.0 0.0
CIFAR-100 71.9 0.0 74.7 0.1 74.3 0.0
Caltech-101 90.3 0.1 92.4 0.2 92.5 0.1
Caltech-256 80.2 0.1 84.6 0.1 84.5 0.0
Cars 55.5 0.0 56.6 0.0 56.0 0.1
DTD 70.8 0.2 74.4 0.1 73:70.2
Flowers 92.8 0.1 92.7 0.1 92.4 0.0
Food-101 70.6 0.0 73.8 0.0 73.8 0.0
Pets 90.7 0.1 92.3 0.1 91.9 0.1
SUN397 57.1 0.0 60.4 0.0 59.8 0.1
Table F.11. Comparison of MobileNet dense versus
ResNet50 sparse models when transferring with linear
ﬁnetuningModelMobileNet
DenseResNet50
WoodFisher 80%ResNet50
WoodFisher 90%
Aircraft 80.9 84.8 0.2 84.5 0.4
Birds 66.6 72.4 0.4 71.6 0.2
CIFAR-10 95.7 97.2 0.1 97.0 0.1
CIFAR-100 81.6 85.1 0.1 84.4 0.2
Caltech-101 91.0 93.7 0.1 93.9 0.3
Caltech-256 80.9 85.1 0.1 84.0 0.1
Cars 87.5 90.5 0.2 90.0 0.2
DTD 73.6 75.4 0.3 75.5 0.4
Flowers 93.9 95.5 0.2 95.5 0.3
Food-101 85.2 87.4 0.1 87.0 0.1
Pets 91.3 93.3 0.3 92.7 0.3
SUN397 60.7 62.8 0.1 62.3 0.1
Table F.12. Comparison of MobileNet dense versus ResNet50
sparse models when transferring with full ﬁnetuning
22

--- PAGE 23 ---
G. Impact of fully connected layer bias on full ﬁnetuning transfer accuracy
In our experiments, we used the original architectures used to train the upstream ImageNet models when performing
transfer with full-ﬁnetuning, only resizing the ﬁnal layer to match the number of output classes in the downstream task. This
choice was necessitated partially by ensuring that the weights were applied correctly. For example, the RigL models were
trained using TensorFlow, which uses slightly different Convolution and MaxPooling padding conventions than PyTorch.
Likewise, STR models were trained using a slightly nonstandard PyTorch implementation of ResNet50, which did not use a
bias term in the ﬁnal Fully-Connected (FC) layer. We investigate the possibility that the latter difference could have an effect
on downstream transfer accuracy. To do so, we transferred a set of 80% sparse ResNet50 STR models to all downstream
tasks, using a bias term in the FC layer. The results are shown in Table G.13. Additionally, we perform a similar comparison
on MobileNetV1, for STR models at 75% sparsity. As in the case of ResNet50, the version of MobileNet used by the
STR models does not use bias in the ﬁnal classiﬁcation layer. The results illustrating the bias effect on full ﬁnetuning for
MobileNet are presented in Table G.14. We observe that the presence of a bias term in the ﬁnal layer can, in some cases, have
a small positive effect on the resulting model, and so we caution that these effects be considered when choosing a transfer
architecture.
Dataset With FC Bias Without FC Bias
Aircraft 79.8 0.6 79.8 0.3
Birds 67.9 0.2 68.1 0.1
CIFAR-10 96.5 0 96.5 0.1
CIFAR-100 83.7 0.2 83.6 0.2
Caltech-101 91.2 0.2 90.7 0.6
Caltech-256 84.4 0.1 84.0 0.1
Cars 87.7 0.1 87.8 0.1
DTD 74.4 0.2 73.7 0.6
Flowers 94 0.1 93.7 0.2
Food-101 86 0.1 85.9 0.1
Pets 92.1 0.1 92.1 0.1
SUN397 63.2 0.1 62.6 0.1
Table G.13. Top-1 validation accuracy on ResNet50 trained using
STR on ImageNet, with using bias in the FC layer versus without.
The original model architecture does not use bias in the FC layer.Dataset With FC Bias Without FC Bias
Aircraft 74.2 74.4
Birds 56.4 56.1
CIFAR-10 93.9 93.8
CIFAR-100 77.5 77.2
Caltech-101 86.3 85.1
Caltech-256 76.9 76.7
Cars 83.8 84.1
DTD 71.8 70.4
Flowers 90.4 89.8
Food-101 80.9 81.2
Pets 87.9 86.9
SUN397 56.1 55.1
Table G.14. Top-1 validation accuracy on MobileNetV1 trained
using STR on ImageNet, with bias in the FC layer versus without.
The original model architecture does not use bias in the FC layer.
H. Impact of label smoothing on transfer accuracy
We take advantage of the fact that we have STR checkpoints trained with and without label smoothing (LS) to investigate
the effect of LS on dense and sparse transfer accuracy in the context of linear transfer. As Table H.15 shows, label smoothing
tends to have a negative effect on transfer accuracy (conﬁrming the results in [36]). However, our experiments suggest that
this effect is more pronounced on the Aircraft and Cars datasets in the case of sparse STR models, and generally for most
specialized datasets for the dense models. Furthermore, we observe that the performance gap tends to narrow with increased
sparsity. We also note that even with label smoothing, at 80% sparsity STR matches or outperforms GMP on all datasets,
although the effect largely reverses at 90% sparsity.
Overall, these data can be taken as a preliminary conﬁrmation of the importance of controlling for variation in hyperpa-
rameters when comparing the transfer performance of various training and pruning methods.
I. Finetuning with structured sparsity
In this section, we examine the transfer properties of models that were sparsiﬁed using structured pruning methods, which
remove entire convolutional ﬁlters. Speciﬁcally, we use both ResNet50 and MobileNetV1 models trained on ImageNet and
we do full ﬁnetuning on all twelve downstream tasks.
I.1. ResNet50 with structured sparsity
We consider a ResNet50 model that was pruned with progressive sparsiﬁcation, using the L1magnitude of the convolu-
tional ﬁlters as a pruning criterion. The resulting model has an ImageNet validation accuracy of 75.7% and results in 2.2x
23

--- PAGE 24 ---
Dense Dense LS STR 80% STR LS 80% STR 90% STR LS 90% STR 95% STR LS 95% STR 98% STR LS 98%
Dataset
Aircraft 49.20.1 38.2 0.1 53.70.0 47.0 0.0 52.90.1 46.4 0.1 50.30.1 46.6 0.1 48.00.1 45.2 0.1
Birds 57.70.1 52.4 0.0 56.20.1 56.4 0.0 55.20.1 56.0 0.0 52.10.1 51.7 0.1 43.70.0 45.6 0.0
CIFAR-10 91.20.0 89.6 0.0 91.40.0 90.1 0.0 90.60.0 89.4 0.0 89.10.0 88.6 0.0 86.50.0 86.0 0.0
CIFAR-100 74.60.1 71.6 0.0 74.70.0 73.3 0.0 73.70.1 72.2 0.1 71.70.0 70.1 0.0 67.40.0 66.3 0.0
Caltech-101 91.90.1 91.6 0.1 91.20.1 92.6 0.1 90.90.1 91.1 0.2 90.00.2 89.8 0.1 86.30.1 85.4 0.1
Caltech-256 84.80.1 84.6 0.1 83.60.0 84.3 0.0 82.60.0 82.6 0.1 80.20.1 79.7 0.0 73.40.1 73.8 0.0
Cars 53.40.1 44.9 0.1 57.00.1 50.9 0.0 54.80.1 49.8 0.1 50.50.1 46.9 0.1 44.40.1 42.5 0.1
DTD 73.50.2 72.3 0.1 74.30.2 73.9 0.3 73.80.1 73.7 0.2 72.10.2 71.9 0.1 68.40.2 68.3 0.1
Flowers 91.60.1 86.7 0.1 93.00.0 91.2 0.0 93.00.1 92.1 0.1 91.90.1 91.0 0.1 90.80.1 90.4 0.1
Food-101 73.20.0 69.5 0.0 73.90.0 72.2 0.0 72.60.0 71.1 0.0 70.70.0 68.8 0.0 65.30.0 64.3 0.0
Pets 92.60.1 92.9 0.1 91.70.0 92.4 0.1 91.10.1 91.7 0.1 89.80.1 90.1 0.1 85.50.1 86.6 0.1
SUN397 60.10.0 59.3 0.1 60.30.0 60.0 0.1 58.20.0 58.5 0.1 56.30.0 55.8 0.0 50.90.0 51.0 0.0
Table H.15. Linear Finetuning Validation Accuracy of STR-pruned and dense models with and without label smoothing.
inference speed-up compared to the dense baseline, when evaluated on a single sample; this makes it comparable to unstruc-
tured 90% sparse models that achieve a similar inference speed-up (please see Table 5). The results for full ﬁnetuning with
the structured sparse model, together with the best results for dense and unstructured 80% and 90% models are presented
in Table I.16. We observe that models with structured sparsity transfer similarly to or worse than unstructured 90% sparse
models. Note that the unstructured ResNet50 model has higher ImageNet accuracy compared to 90% sparse models, at a
similar inference speed-up. These results align with the observations made in Section 3.5, that having fewer ﬁlters in the
structured sparse models limits their capability of expressing features.
Dataset Dense Structured Best 80% Best 90%
Aircraft 83:60:4 81:80:584.80.2 84.9 0.3
Birds 72.4 0.3 70:70:173.40.1 72:90:2
Caltech101 93.50.1 92:80:193.70.1 93.9 0.3
Caltech256 86.10.1 84:60:1 85:40:2 84:80:1
Cars 90:30:2 89:40:090.50.2 90:00:2
CIFAR-10 97.40. 97:10:1 97:20:1 97:10:
CIFAR-100 85.60.2 84:70:2 85:10:1 84:40:2
DTD 76.20.3 75:20:275.70.5 75.50.4
Flowers 95:00:1 95:20:096.10.1 96.1 0.1
Food-101 87.30.1 86:30:187.40.1 87.3 0.2
Pets 93.40.1 92:50:193.40.2 92:70:3
SUN397 64.80. 63:40:1 64:00: 63:00:
Table I.16. (ResNet50) Comparison on full ﬁnetuning between
dense baseline, models with structured sparsity, and best results
for unstructured 80% and 90% sparsity.Dataset Dense 50% Time 50% FLOPs
Aircraft 80.9 82.9 83.0
Birds 66.6 66.1 66.1
Caltech101 91.0 88.6 88.9
Caltech256 80.9 78.6 78.4
Cars 87.5 88.4 88.3
CIFAR-10 95.7 95.2 95.3
CIFAR-100 81.6 79.9 80.2
DTD 73.6 71.1 72.2
Flowers 93.9 94.1 94.1
Food-101 85.2 84.6 84.5
Pets 91.3 91.0 91.0
SUN397 60.7 59.4 59.1
Table I.17. (MobileNet) Full ﬁnetuning validation accuracy for
MobileNet models with structured sparsity, at 50% inference time
or 50% inference FLOPs.
I.2. MobileNet with structured sparsity
We additionally perform full ﬁnetuning using MobileNet models pruned for structured sparsity. For these experiments,
we use the upstream models provided in [27]; speciﬁcally, we use the MobileNet models that achieve 50% of the inference
time or have 50% of the dense FLOPs. These models achieve 70:2%and70:5%ImageNet validation accuracy, respectively.
The results presented in Table I.17 show that in general models with structured sparsity perform similar to or worse than their
dense counterparts, with the exception of Aircraft and Cars where these models signiﬁcantly outperform the dense baseline.
J. Sparse Transfer Learning for Segmentation
To complement the experiments for object detection, we executed transfer learning for a YOLACT model [3] using a
ResNet-101 backbone, that has been trained and sparsiﬁed on the segmentation version of the COCO dataset. The average
sparsity of the model is 87%, obtained via gradual magnitude pruning (GMP). The model has mAP@0.5 values 49.36
(bounding box), and 46.37 (mask), versus 50.16 (bounding box), 46.57 (mask) for the dense model on COCO. We transfer
24

--- PAGE 25 ---
Type all 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95
box 32.62 54.05 51.96 48.72 44.81 40.84 34.72 26.89 17.33 6.33 0.55
mask 30.74 50.28 47.66 44.57 41.02 36.39 31.47 25.53 18.55 10.03 1.91
Table J.18. Mean average precision for dense transfer on Pascal, at various thresholds.
Type all 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95
box 33.55 54.15 51.79 49.2 45.57 41.51 35.95 29.2 19.66 7.78 0.74
mask 31.5 50.66 47.89 45.04 41.67 37.32 32.39 26.35 19.98 11.22 2.5
Table J.19. Mean average precision for sparse transfer on Pascal, at various thresholds. Notice the similar or slightly improved accuracy.
the pruned trained weights onto the Pascal dataset. The prediction heads get initialized as dense, and kept dense for transfer.
The results are presented in Tables J.18 and J.19, and show that indeed sparse transfer is competitive against the dense variant
in this case as well.
K. Distillation from Sparse Teachers
Our linear ﬁnetuning experiments suggest that sparse models may provide superior representations relative to dense ones.
To further test this hypothesis, we employ sparse models as teachers in a standard knowledge distillation (KD) setting, i.e.
training a ResNet34 student model with distillation from a ResNet50 teacher, which may be dense or sparse. The accuracy
of the resulting models is provided in Table K.20.
Results suggest that differences in accuracy between the sparse and dense teachers do not affect distillation. Sparse
teachers will also reduce distillation overhead due to faster inference.
Baseline Dense KDAC/DC
80%WoodFisher
80%AC/DC
90%WoodFisher
90%
73.83% 74.42% 74.64% 74.63% 74.19% 74.44%
Table K.20. Top-1 validation accuracy on ResNet34 trained on ImageNet, when distilling from dense or sparse teachers.
25
