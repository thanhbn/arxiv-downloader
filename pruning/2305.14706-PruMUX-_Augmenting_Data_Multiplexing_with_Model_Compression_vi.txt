PruMUX: Tăng cường ghép kênh dữ liệu với nén mô hình

Yushan Su Vishvak Murahari Karthik Narasimhan Kai Li
Đại học Princeton
Princeton, NJ, Hoa Kỳ
{yushans, murahari, karthikn, li}@princeton.edu

Tóm tắt
Khi các mô hình ngôn ngữ ngày càng tăng kích thước, các phương pháp suy diễn hiệu quả là quan trọng để tận dụng khả năng của chúng cho các ứng dụng khác nhau. Nghiên cứu trước đã khảo sát các kỹ thuật như cắt tỉa mô hình, chưng cất kiến thức và ghép kênh dữ liệu để tăng thông lượng mô hình mà không hy sinh độ chính xác. Trong bài báo này, chúng tôi kết hợp hai phương pháp đó - cắt tỉa có cấu trúc và ghép kênh dữ liệu - để tạo ra hiệu quả tăng tốc từ cả hai phương pháp. Phương pháp của chúng tôi, PruMUX, đạt được cải thiện thông lượng lên đến 7.5-29.5X so với mô hình BERT-base với ngưỡng độ chính xác từ 80% đến 74%. Chúng tôi tiếp tục nghiên cứu các kết hợp khác nhau của tham số (như độ thưa và hệ số ghép kênh) trong hai kỹ thuật để cung cấp phân tích toàn diện về sự đánh đổi giữa độ chính xác và thông lượng trong các mô hình kết quả. Sau đó chúng tôi đề xuất Auto-PruMUX, một mô hình meta-level có thể dự đoán các tham số hiệu suất cao cho cắt tỉa và ghép kênh dựa trên ngân sách tổn thất độ chính xác mong muốn, cung cấp phương pháp thực tế để tận dụng hiệu quả sự kết hợp này.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs) đã đạt được hiệu suất tiên tiến nhất trên nhiều tác vụ NLP khác nhau và tạo ra các cuộc trình diễn ấn tượng hướng đến người dùng như ChatGPT. Tuy nhiên, kích thước lớn của chúng đòi hỏi sử dụng lượng tính toán và bộ nhớ khổng lồ vào thời gian suy diễn, điều này hạn chế việc sử dụng rộng rãi của chúng.

Hai loại kỹ thuật đã được khám phá để giảm chi phí suy diễn mô hình. Thứ nhất là nén mô hình bao gồm cắt tỉa mạng (LeCun et al., 1989; Han et al., 2015b; Frankle và Carbin, 2019), lượng tử hóa (Han et al., 2016), chưng cất kiến thức (Hinton et al., 2015), và sự kết hợp của nhiều phương pháp (Xia et al., 2022). Thứ hai là ghép kênh dữ liệu được đề xuất gần đây (Murahari et al., 2023), ghép nhiều đầu vào thành một đầu vào duy nhất cho suy diễn mô hình.

Trong khi cả hai loại phương pháp đều tận dụng hiệu ứng tham số hóa quá mức (Allen-Zhu et al., 2019; Radhakrishnan et al., 2020) trong các mạng nơ-ron sâu hiện đại để cải thiện tỷ lệ thông lượng trên chi phí tính toán, cách thức chúng thực hiện điều này là khác nhau. Nén mô hình nhằm giảm số lượng tham số trong mô hình, do đó giảm tổng chi phí tính toán (mẫu số) để cải thiện tỷ lệ. Ghép kênh dữ liệu, mặt khác, nén nhiều đầu vào thành một để cải thiện thông lượng (tử số) trong khi giữ kích thước mô hình cố định. Quan sát này tự nhiên dẫn chúng tôi đến giả thuyết rằng hai loại phương pháp có thể bổ sung cho nhau và có thể được kết hợp để đạt được lợi ích tối đa trong tỷ lệ thông lượng trên chi phí tính toán.

Có hai thách thức đối với giả thuyết này. Thứ nhất là cả nén mô hình và ghép kênh dữ liệu đều nhằm đánh đổi một tổn thất độ chính xác nhỏ để có cải thiện thông lượng lớn. Một cách trực quan, sự kết hợp có thể gây ra tổn thất độ chính xác lớn hơn bất kỳ phương pháp nào và không rõ chúng tương tác với nhau như thế nào khi kết hợp lại. Một câu hỏi nghiên cứu là làm thế nào để kết hợp hai phương pháp sao cho sự kết hợp đạt được thông lượng tốt hơn mỗi loại phương pháp riêng lẻ, với bất kỳ ngân sách tổn thất độ chính xác hoặc ngưỡng độ chính xác nào.

Thách thức thứ hai là tìm hiệu quả cặp tham số tốt nhất (N, s) trong đó N là chiều rộng của ghép kênh dữ liệu và s là độ thưa của phương pháp nén mô hình. Đào tạo và kiểm tra với mỗi kết hợp tham số là tốn kém và mất thời gian. Một câu hỏi nghiên cứu là làm thế nào để tự động dự đoán và tìm ra các tham số hàng đầu dựa trên hiệu suất của mô hình trên một tập tham số.

Để giải quyết câu hỏi nghiên cứu đầu tiên, chúng tôi trình bày PruMUX, một sự kết hợp của nén mô hình và ghép kênh dữ liệu. Phương pháp của chúng tôi đơn giản và bao gồm ba giai đoạn - tiền huấn luyện mô hình đã ghép kênh, tinh chỉnh theo tác vụ cụ thể và nén mô hình theo tác vụ cụ thể. Trong việc triển khai của chúng tôi, chúng tôi sử dụng CoFi (Xia et al., 2022), một phương pháp nén mô hình tiên tiến bao gồm các bước chưng cất kiến thức trung gian giúp giảm thiểu tác động đến độ chính xác, và DataMUX (Murahari et al., 2023), thực hiện ghép kênh đầu vào dựa trên vector trên các thể hiện.

Kết quả của chúng tôi trên bốn tập dữ liệu (MNLI, QNLI, QQP và SST-2) chứng minh rằng PruMUX đạt được thông lượng cao hơn đáng kể so với CoFi và DataMUX riêng lẻ trong một phạm vi lớn các ngưỡng độ chính xác. Ví dụ, Hình 1 cho thấy cải thiện thông lượng so với mô hình BERT-base trên tác vụ MNLI, cung cấp một biên giới Pareto tối ưu hơn trong sự đánh đổi giữa độ chính xác và thông lượng.

Để giải quyết câu hỏi nghiên cứu thứ hai, chúng tôi đề xuất Auto-PruMUX, một meta-model để tự động dự đoán và tìm ra các kết hợp tham số hiệu suất cao cho ngân sách tổn thất độ chính xác mong muốn trên một tác vụ dựa trên hiệu suất của mô hình trên một tập tham số mà không cần chạy thêm thí nghiệm. Chúng tôi sử dụng các mô hình nội suy và ước lượng trên một tập điểm dữ liệu để dự đoán độ chính xác và thông lượng của mô hình PruMUX dựa trên độ thưa và hệ số ghép kênh. Chúng tôi cho thấy triển vọng trong việc mô hình hóa chính xác các sự đánh đổi và Auto-PruMUX có thể tìm ra các kết hợp hiệu suất cao của các tham số đã biết cũng như các tham số chưa biết, cung cấp phương pháp thực tế để chọn mô hình PruMUX hiệu suất cao cho tác vụ hạ lưu.

Hiểu biết chính của chúng tôi về tại sao PruMUX có thể đạt được thông lượng tốt hơn so với nén mô hình và ghép kênh dữ liệu riêng lẻ là chúng cải thiện thông lượng của mô hình trong hai chiều khác nhau: giảm độ trễ của một suy diễn và nén nhiều suy diễn. Ngoài ra, cả hai phương pháp đều dẫn đến sự sụt giảm phi tuyến tính trong độ chính xác mô hình tại một số điểm. PruMUX có thể đạt được thông lượng cao trong khi tránh được hạn chế của mỗi phương pháp.

2 Kiến thức cơ bản

2.1 Cắt tỉa CoFi
CoFi là một phương pháp nén mô hình tiên tiến (Xia et al., 2022) sử dụng chưng cất và cắt tỉa có cấu trúc để cắt tỉa chung mạng Transformer (Devlin et al., 2018). Ý tưởng chính của nó là chưng cất kiến thức từ mô hình cơ sở vào mô hình đã cắt tỉa trong quá trình huấn luyện. Một phương pháp chưng cất theo lớp được sử dụng để hướng dẫn việc cắt tỉa từ mô hình giáo viên, tức mô hình dày đặc, đến mô hình học sinh, tức mô hình đã cắt tỉa, với hàm mất mát được định nghĩa là:

Llayer=∑i∈τMSE (WlayerHm(i)s,Hit)

trong đó Hm(i)s và Hit là các biểu diễn ẩn của lớp feed-forward thứ m(i) của mô hình học sinh và lớp feed-forward thứ i của mô hình giáo viên. i là lớp gần nhất của mô hình giáo viên với lớp m(i) của mô hình học sinh. Wlayer là ma trận biến đổi tuyến tính, được khởi tạo như một ma trận đơn vị.

CoFi cắt tỉa cả các đơn vị thô và tinh của mạng đã chưng cất. Các đơn vị thô bao gồm các lớp attention đa đầu, các lớp kết nối đầy đủ và các đầu attention. Các đơn vị tinh bao gồm các chiều ẩn và các chiều trung gian của mô hình Transformer. Các mặt nạ khác nhau được sử dụng cho các đơn vị cắt tỉa khác nhau và được học thông qua điều chuẩn ℓ0 trong quá trình huấn luyện. Các đơn vị có biến mặt nạ nhỏ hơn ngưỡng sẽ được cắt bỏ trước khi suy diễn.

2.2 DataMUX
Ghép kênh dữ liệu (DataMUX) là một phương pháp được đề xuất gần đây (Murahari et al., 2022, 2023) để nén nhiều đầu vào thành một biểu diễn "trộn" duy nhất có cùng kích thước như một đầu vào duy nhất cho mạng, nhằm cải thiện thông lượng suy diễn. DataMUX giới thiệu các lớp ghép kênh, ghép các chuỗi khác nhau thành một chuỗi biểu diễn duy nhất, tức các biểu diễn đã ghép kênh, và các lớp tách kênh, tách/giải nén các biểu diễn đã ghép kênh. Lớp đã ghép kênh đầu tiên nén nhiều chuỗi đầu vào thành một chuỗi biểu diễn duy nhất. Các biểu diễn này sau đó được xử lý bởi mô hình Transformer và các biểu diễn kết quả sau đó được tách thành các biểu diễn độc lập bởi lớp tách kênh. Các biểu diễn này sau đó được sử dụng để đưa ra dự đoán. Do đó, DataMUX dẫn đến sự gia tăng nhiều lần trong thông lượng suy diễn vì chỉ cần một lần đi qua mô hình Transformer lớn.

Lớp ghép kênh được định nghĩa là
x1:N= Φ( x1, ...xN) =1/N∑Ni=1ϕi(xi)

trong đó x là chuỗi đầu vào, ϕi, i∈[1, ...N ], là tích Hadamard với một vector ngẫu nhiên Gaussian cố định và N là số lượng chuỗi đầu vào được ghép kênh. Các biểu diễn đã ghép kênh, x1:N, sau đó được xử lý bởi mô hình Transformer để tạo ra các biểu diễn ẩn đã ghép kênh, h1:N.

Lớp tách kênh, để tách biểu diễn ẩn đã ghép kênh, h1:N, thành các biểu diễn độc lập, học N hàm tách kênh có tham số, ψi. Các biểu diễn độc lập, hi, sau đó được sử dụng để đưa ra dự đoán.

hi=ψi(h1:N)∀i∈1,2, ...N

2.3 Quan sát
Cả nén mô hình và ghép kênh dữ liệu đều nhằm đánh đổi các tổn thất độ chính xác nhỏ để có cải thiện thông lượng suy diễn lớn. Khi CoFi cắt tỉa một Transformer ở các độ thưa tương đối thấp, tổn thất độ chính xác của nó là tối thiểu và cải thiện thông lượng là đáng kể, nhưng ở độ thưa 95%, tổn thất độ chính xác của nó trở nên tương đối đáng kể (Xia et al., 2022). DataMUX cũng có đặc tính phi tuyến này, như được thể hiện trong Hình 1. Nói cách khác, sự đánh đổi của mỗi phương pháp chỉ tốt đến một điểm nhất định.

Hai phương pháp cải thiện thông lượng của mô hình trong hai chiều. CoFi giảm độ trễ của một suy diễn, trong khi DataMUX nén nhiều suy diễn thành một. Một câu hỏi tự nhiên là liệu việc kết hợp hai phương pháp có thể đạt được thông lượng cao hơn với tổn thất độ chính xác nhỏ hơn so với mỗi phương pháp riêng lẻ hay không.

3 PruMUX

Câu hỏi động lực chính của chúng tôi như sau: với một ngân sách tổn thất độ chính xác, liệu sự kết hợp của nén mô hình và ghép kênh dữ liệu có thể đạt được thông lượng tốt hơn so với mỗi phương pháp riêng lẻ không? Trong phần này, chúng tôi đầu tiên trình bày PruMUX, một phương pháp để kết hợp hai phương pháp, và sau đó cho thấy rằng PruMUX đạt được thông lượng tốt hơn đáng kể so với mỗi phương pháp riêng lẻ cho các ngưỡng độ chính xác khác nhau trong kết quả thực nghiệm của chúng tôi.

3.1 Phương pháp
PruMUX là một phương pháp để chuyển đổi bất kỳ Transformer nào thành một mô hình thông lượng cao, có khả năng nén nhiều đầu vào suy diễn thành một đầu vào duy nhất và thực thi nó với độ trễ thấp.

Để ghép kênh, PruMUX sử dụng DataMUX được đề xuất gần đây (Murahari et al., 2023), thêm một bộ ghép kênh và tách kênh như được mô tả trong Phần 2.2. Với chiều rộng N, thông lượng suy diễn của Transformer có thể được cải thiện bởi một hệ số lên đến N, vì mỗi đầu vào đã ghép kênh cần cùng lượng tài nguyên tính toán như thực hiện suy diễn trên một đầu vào duy nhất.

Để nén mô hình, PruMUX có thể sử dụng bất kỳ phương pháp nào như cắt tỉa mạng, chưng cất hoặc sự kết hợp của cả hai (như CoFi). Mục tiêu là giảm đáng kể độ trễ của việc xử lý một suy diễn. Đối với các thí nghiệm của chúng tôi, PruMUX sử dụng CoFi làm phương pháp nén mô hình.

Huấn luyện một mô hình với PruMUX bao gồm ba giai đoạn như được thể hiện trong Hình 2:

Giai đoạn 1: Chuẩn bị mô hình đã ghép kênh với mục tiêu truy xuất token
Chúng tôi đầu tiên chuẩn bị mô hình transformer đã ghép kênh với một tác vụ truy xuất token. Murahari et al. (2022) đã giới thiệu mục tiêu tự giám sát "khởi động truy xuất" này (được thể hiện bên dưới) và thấy rằng nó rất quan trọng để cải thiện hiệu suất của các mô hình đã ghép kênh. L là độ dài của mỗi câu đầu vào. I là chỉ số của câu được chọn ngẫu nhiên từ lô đầu vào.

Lretrieval (x1:N) =∑Lj=1−logP(wIj|hIj)

Giai đoạn 2: Tiền huấn luyện và tinh chỉnh các mô hình đã ghép kênh
Các mô hình đã ghép kênh từ giai đoạn trước sau đó được tiền huấn luyện trên các corpus văn bản quy mô lớn với mục tiêu mô hình hóa ngôn ngữ có mặt nạ (MLM). Các mô hình đã ghép kênh đã được tiền huấn luyện sau đó được tinh chỉnh trên các tác vụ hạ lưu để tạo ra các mô hình đã ghép kênh đặc thù cho tác vụ.

Giai đoạn 3: Nén mô hình
Cuối cùng, chúng tôi sử dụng CoFi để cắt tỉa chung các đơn vị thô và tinh trong mô hình Transformer đã ghép kênh. Các đơn vị thô bao gồm toàn bộ các đầu attention, các lớp attention và các lớp kết nối đầy đủ. Các đơn vị tinh bao gồm các chiều ẩn và các chiều trung gian của mô hình Transformer. Chiều đầu vào của bộ tách kênh được cắt tỉa để phù hợp với chiều ẩn đã cắt tỉa của mô hình Transformer. Trong quá trình cắt tỉa, CoFi sử dụng chưng cất kiến thức để chuyển giao kiến thức từ mô hình giáo viên, tức mô hình đã ghép kênh đặc thù cho tác vụ, đến mô hình đã cắt tỉa.

3.2 Chi tiết triển khai
Chúng tôi sử dụng các mô hình BERT-base đã ghép kênh được tiền huấn luyện (Murahari et al., 2023) với công thức tiền huấn luyện BERT tiêu chuẩn với mục tiêu mô hình hóa ngôn ngữ có mặt nạ cho N= 2,5,10 trên các tập dữ liệu Wikipedia (Foundation) và BooksCorpus (Zhu et al., 2015). Chúng tôi chuẩn bị mô hình đã ghép kênh trước khi tiền huấn luyện với tác vụ truy xuất token trong Phần 2.2 trên các tập dữ liệu Wikipedia và BooksCorpus. Sau đó chúng tôi huấn luyện các mô hình đã ghép kênh được tiền huấn luyện trên bốn Tác vụ GLUE lớn nhất (Wang et al., 2018) – MNLI (Williams et al., 2018), QNLI (Wang et al., 2018), QQP (qqp), và SST-2 (Socher et al., 2013). Sau đó chúng tôi sử dụng mục tiêu cắt tỉa có cấu trúc CoFi để có được mô hình đã ghép kênh đã cắt tỉa trên mỗi tập dữ liệu tác vụ. Các siêu tham số chúng tôi sử dụng cho quá trình huấn luyện được thể hiện trong Phụ lục A.1. Chúng tôi thực hiện một lần chạy duy nhất để huấn luyện mô hình cho mỗi thiết lập, tức tác vụ, chiều rộng ghép kênh N, độ thưa mô hình s, theo quy trình huấn luyện.

3.3 Thí nghiệm

Thiết lập
Chúng tôi muốn trả lời câu hỏi rằng với một ngưỡng độ chính xác, liệu phương pháp PruMUX có thể đạt được thông lượng cao hơn so với CoFi hoặc DataMUX riêng lẻ hay không.

Chúng tôi so sánh mô hình BERT-base đã PruMUX với ba baseline:
•BERT-base : mô hình BERT-base được huấn luyện mà không có ghép kênh dữ liệu và nén mô hình.
•CoFi : mô hình BERT-base được cắt tỉa bởi CoFi (Xia et al., 2022) với độ thưa s=0.50, 0.60, 0.70, 0.80, 0.90, và 0.95.
•DataMUX : mô hình BERT-base được tiền huấn luyện bởi DataMUX (Murahari et al., 2023) với chiều rộng ghép kênh N=2, 5, và 10.

Chúng tôi đã áp dụng PruMUX cho mô hình BERT-base với tất cả các kết hợp của (N, s) cho tất cả 4 tác vụ. Chúng tôi làm theo quy trình trong Xia et al. (2022) để tính toán cải thiện thông lượng cho các Transformer đã PruMUX và tất cả ba baseline, tức BERT-base, DataMUX, và CoFi. Kích thước lô đánh giá là 128* N, trong đó N là chiều rộng ghép kênh.

Kết quả
Hình 3 cho thấy cải thiện thông lượng và độ chính xác của các Transformer đã PruMUX, DataMUX, và CoFi-Pruned so với mô hình Transformer base trên các tác vụ MNLI, QNLI, QQP, và SST-2 với tất cả các tham số có sẵn.

Điểm chính là PruMUX đạt được thông lượng cao hơn so với CoFi hoặc DataMUX riêng lẻ trong tất cả các trường hợp bắt đầu từ các ngưỡng độ chính xác khác nhau:

•Đối với MNLI, với các ngưỡng độ chính xác từ 80% đến 74%, PruMUX đạt được cải thiện thông lượng 7.5-29.5X so với mô hình BERT-base, trong khi CoFi cải thiện 4.0-10.6X và DataMUX 2.0-4.9X.

•Đối với QNLI, với các ngưỡng độ chính xác từ 87% đến 82%, PruMUX đạt được cải thiện 4.1-26.6X, trong khi CoFi cải thiện 3.8-11.2X và DataMUX 2.0-9.6X.

•Đối với QQP, với các ngưỡng độ chính xác từ 89% đến 86%, PruMUX đạt được cải thiện thông lượng so với BERT-base 7.6-29.7X, trong khi CoFi cải thiện 10.6X và DataMUX 2.0-9.8X.

•Đối với SST-2, với các ngưỡng độ chính xác từ 86.5% đến 83%, PruMUX cải thiện thông lượng 10.1-27.8X, trong khi CoFi cải thiện 10.6X và DataMUX 4.8-9.7X.

Kết quả cũng xác nhận trực giác rằng PruMUX với (N, s) gây ra tổn thất độ chính xác, nói một cách lỏng lẻo, gần với tổng tổn thất độ chính xác của DataMUX với N và của CoFi với s. Nói chung, PruMUX có thể đạt được cải thiện thông lượng đáng kể khi có ngân sách tổn thất độ chính xác phù hợp.

3.4 Thảo luận
Kết quả trên tìm ra hiệu suất PruMUX hàng đầu với tất cả các cặp tham số (N, s), trong đó N=2, 5, 10 và s=0.60, 0.70, 0.80, 0.90, và 0.95, cho mỗi ngân sách tổn thất độ chính xác. Tìm kiếm các tham số PruMUX hàng đầu ở độ chi tiết tham số tinh hơn sẽ yêu cầu huấn luyện và kiểm tra trên tất cả các cặp tham số bổ sung.

Các kiểm tra toàn diện là không thực tế. Thứ nhất, đối với mỗi N, việc tiền huấn luyện một mô hình DataMUX với chiều rộng ghép kênh N tốn thời gian. Thứ hai, với mỗi mô hình được tiền huấn luyện với chiều rộng ghép kênh N, các độ thưa s khác nhau cung cấp các sự đánh đổi thông lượng và độ chính xác khác nhau. Để tìm ra các độ thưa s có thông lượng cao nhất với ngân sách độ chính xác, người ta phải huấn luyện mô hình cho tất cả các độ thưa có thể. Tổng thời gian huấn luyện cho các độ thưa từ 0.60 đến 0.95 ở độ chi tiết 0.05 cho mỗi N mất hơn sáu nghìn giờ GPU trên GPU thông thường, cho một mô hình BERT-base gốc nhỏ.

Một câu hỏi chính là liệu người ta có thể tự động tìm ra (N, s) thông lượng cao với một số lượng nhỏ các thí nghiệm PruMUX hay không.

4 Auto-PruMUX

Để giải quyết câu hỏi trên, chúng tôi đề xuất Auto-PruMUX, một phương pháp để tìm kiếm các tham số (N, s) hàng đầu, để giúp các nhà thực hành cân bằng sự đánh đổi hiệu suất so với thông lượng.

Câu hỏi nghiên cứu của chúng tôi là: Giả sử chúng ta có một số dữ liệu thực nghiệm của PruMUX và dữ liệu thực nghiệm của DataMUX và CoFi, làm thế nào chúng ta có thể tìm và dự đoán các tham số hàng đầu (N, s) với một ngân sách tổn thất độ chính xác?

Phương pháp của chúng tôi là phát triển các mô hình hiệu suất cho độ chính xác và thông lượng của PruMUX. Chúng tôi đầu tiên huấn luyện các mô hình PruMUX cho một tập các kết hợp (N, s) và đo cả độ chính xác và cải thiện thông lượng. Sau đó chúng tôi sử dụng dữ liệu này để khớp một mô hình thông lượng và một mô hình độ chính xác để dự đoán thông lượng và độ chính xác tương ứng với các tham số (N, s).

Chúng tôi đầu tiên thảo luận về cách chúng tôi khớp các mô hình độ chính xác và thông lượng với một tập các điểm dữ liệu thưa. Với việc chúng tôi đang làm việc với một tập điểm dữ liệu hạn chế, chúng tôi chọn sử dụng một lớp đơn giản các mô hình nội suy để mô hình hóa độ chính xác PruMUX và sử dụng một mô hình ước lượng để mô hình hóa thông lượng. Sau đó chúng tôi phác thảo cách chúng tôi tận dụng các mô hình này để dự đoán các tham số (N, s) hàng đầu, với một ngân sách tổn thất độ chính xác. Sau đó chúng tôi chứng minh hiệu quả của Auto-PruMUX trong việc dự đoán các tham số hàng đầu qua một phạm vi rộng các ngân sách tổn thất độ chính xác.

4.1 Mô hình độ chính xác tác vụ
Chúng tôi sử dụng nội suy tuyến tính cho mô hình độ chính xác tác vụ của chúng tôi.

fA(N, s) = {
A1,1(N, s) N0≤N≤N1,s0≤s≤s1,
...
Ai,j(N, s) Ni−1≤N≤Ni,sj−1≤s≤sj,
...
Ap,q(N, s) Np−1≤N≤Np,sq−1≤s≤sq
}

Mỗi số hạng là một kết hợp tuyến tính của chiều rộng ghép kênh dữ liệu và độ thưa mô hình.

Ai,j(N, s) =∑1a=0∑1b=0k(i,j)abNasb

Mô hình được khớp trên dữ liệu thu thập về độ chính xác tác vụ mô hình ở các chiều rộng ghép kênh và độ thưa khác nhau.

Ai,j(Ni,sj) =Acc(Ni,sj)
i= 1, ..., p, j = 1, ..., q

trong đó N và s là phạm vi các giá trị N và s được sử dụng để khớp mô hình.

4.2 Mô hình thông lượng
Chúng tôi thu thập các giá trị thông lượng cho tất cả N và s trên một tác vụ (tác vụ 0) và sử dụng các giá trị thông lượng làm ước lượng thông lượng cho tất cả các tác vụ.

fT(N, s) =Throu task 0(N, s)

4.3 Dự đoán (N, s)
Chúng tôi sử dụng các mô hình của chúng tôi, fA(N, s) và fT(N, s), để mô hình hóa độ chính xác và thông lượng của PruMUX với N > 1 và s > 0%. Acc(1, s) và Throu (1, s) là độ chính xác và thông lượng đo được của các mô hình CoFi-pruned. Acc(N,0) và Throu (N,0) là độ chính xác và thông lượng đo được của các mô hình DataMUX. Acc(1,0) và Throu (1,0) là hiệu suất của mô hình BERT-base. Chúng tôi tìm kiếm các tham số (N, s) tối đa hóa ζf được định nghĩa bên dưới.

ζf(N, s) =Throu (N, s)·g(Acc(N, s)) (1)

g(x) ={1 nếu x≥ξ
0 nếu x < ξ}

Trực quan, ζf cố gắng đánh đổi hiệu suất tác vụ và thông lượng, với ngân sách tổn thất độ chính xác ξ với mục tiêu tối đa hóa thông lượng. g(x) cung cấp cơ chế cho ngưỡng độ chính xác nghiêm ngặt - tức một mô hình không đáp ứng độ chính xác tối thiểu yêu cầu sẽ có ζf= 0.

4.4 Kết quả thực nghiệm

Thiết lập thực nghiệm
Trong phần này, chúng tôi cho thấy kết quả dự đoán của Auto-PruMUX bằng cách khớp các mô hình hiệu suất sử dụng một tập không gian tham số và dự đoán các tham số hàng đầu trên một tập không gian tham số lớn hơn.

Chúng tôi định nghĩa tập không gian tham số (N, s) (tập kiểm tra) như sau.

• Mô hình BERT-base - (N, s) N=1,s=0.00
•Các mô hình CoFi - (N, s) N=1,∀s∈0.60, 0.70, 0.80, 0.90, 0.95
•Các mô hình DataMUX - (N, s) ∀N∈2,5,10, s= 0.00
•Các mô hình PruMUX - (N, s) ∀N∈2,5,10, ∀s∈ 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95

Chúng tôi khớp mô hình độ chính xác với độ chính xác mô hình trên (N, s) ∀N∈2,5,10, ∀s∈0.60, 0.70, 0.80, 0.90, 0.95 (tập huấn luyện). Chúng tôi khớp mô hình thông lượng với thông lượng của một tác vụ trên tất cả các cặp tham số.

Mục tiêu của chúng tôi là đánh giá mô hình độ chính xác tác vụ, mô hình thông lượng và hiệu suất dự đoán tham số.

Độ chính xác mô hình hiệu suất
Để đánh giá độ chính xác của các mô hình hiệu suất tác vụ trên tập huấn luyện, chúng tôi thực hiện xác thực chéo leave-one-out cho mỗi tác vụ. Chúng tôi thể hiện phần MA của các dự đoán độ chính xác có lỗi nằm trong Δξ= 1.5% từ độ chính xác thực tế trong Bảng 1. Để đánh giá độ chính xác của mô hình thông lượng trên tập huấn luyện, chúng tôi khớp mô hình sử dụng hiệu suất PruMUX của tác vụ QQP. Chúng tôi thể hiện phần MT của các dự đoán thông lượng có lỗi trong 20% cải thiện thông lượng thực tế trong Bảng 1. Qua các tác vụ khác nhau, các mô hình độ chính xác và thông lượng của chúng tôi chính xác qua một tập rộng các kết hợp tham số.

Dự đoán tham số hàng đầu
Chúng tôi thể hiện kết quả dự đoán của Auto-PruMUX bằng cách khớp mô hình độ chính xác trên tập huấn luyện và khớp mô hình thông lượng sử dụng thông lượng của tác vụ QQP, và dự đoán các tham số hàng đầu trên tập kiểm tra. Chúng tôi thể hiện các dự đoán tham số hàng đầu của Auto-PruMUX cho ngân sách tổn thất độ chính xác 3% trong Bảng 2. Auto-PruMUX dự đoán các cặp tham số thực tế tốt nhất trong top 3 dự đoán của nó. Trong Bảng 3, chúng tôi sử dụng Auto-PruMUX để dự đoán tham số cho các ngân sách tổn thất độ chính xác trong 0%, 0.5%, ..., 10% và thể hiện phần trăm các ngân sách tổn thất độ chính xác mà Auto-PruMUX dự đoán tham số (N, s) thực tế tốt nhất trong top 3 dự đoán của nó. Auto-PruMUX có thể dự đoán các tham số hàng đầu trong hầu hết các trường hợp.

5 Nghiên cứu liên quan

Nén mô hình
Nén mô hình giảm số lượng tham số mô hình với tổn thất tối thiểu trong hiệu suất tác vụ. Một phương pháp được nghiên cứu kỹ là cắt tỉa mạng, loại bỏ các kết nối không quan trọng khỏi mạng với tổn thất độ chính xác tối thiểu hoặc không có (LeCun et al., 1989; Hanson và Pratt, 1989; Hassibi et al., 1993). Cắt tỉa không có cấu trúc (Han et al., 2015b,a; Zhu và Gupta, 2017; Frankle và Carbin, 2019; Chen et al., 2020a; Huang et al., 2021; Sanh et al., 2020) không áp đặt bất kỳ ràng buộc nào về vị trí của các trọng số khác không. Mạng kết quả có thể đạt được độ thưa cao nhưng có thể không chạy hiệu quả trên phần cứng thông thường như GPU.

Cắt tỉa có cấu trúc tạo ra các ma trận thưa có cấu trúc có thể tận dụng tốt hơn tính song song trong phần cứng hiện có, nhưng độ thưa của nó tương đối thấp hơn so với phương pháp cắt tỉa không có cấu trúc cho cùng ngân sách tổn thất độ chính xác (Yu et al., 2017; Narang et al., 2017; Wen et al., 2017; Mao et al., 2017; Wang et al., 2019; McDanel et al., 2022). Cắt tỉa có cấu trúc đã được áp dụng cho transformer để cải thiện thông lượng suy diễn (Fan et al., 2019; Sajjad et al., 2023; Voita et al., 2019; Michel et al., 2019; Prasanna et al., 2020; Chen et al., 2020b; McCarley et al., 2019; Hou et al., 2020; Yao et al., 2021).

Chưng cất nén một mô hình bằng cách chuyển giao kiến thức từ một mô hình giáo viên lớn đến một mô hình học sinh nhỏ (Hinton et al., 2015). Chưng cất tổng quát cho các mô hình Transformer học từ corpus không có nhãn (Sanh et al., 2019; Sun et al., 2020; Wang et al., 2020; Turc et al., 2019; Jiao et al., 2019). Chưng cất đặc thù cho tác vụ cho các mô hình Transformer học trên dữ liệu đặc thù cho tác vụ (Sun et al., 2019). (Jiao et al., 2019) kết hợp hai phương pháp chưng cất để cải thiện hiệu suất.

Cắt tỉa với mục tiêu chưng cất đã được khám phá (Sanh et al., 2020; Lagunas et al., 2021). (Xia et al., 2022) đề xuất cắt tỉa có cấu trúc với mục tiêu chưng cất để giảm các tham số Transformer lên đến 95% và đạt được tăng tốc hơn 10x với sự sụt giảm độ chính xác nhỏ.

Các mô hình đa đầu vào đa đầu ra
Các mô hình đa đầu vào đa đầu ra xử lý đồng thời nhiều đầu vào trong một mạng nơ-ron để giảm tham số hóa quá mức của mạng. (Havasi et al., 2021) và (Ramé et al., 2021) huấn luyện các mạng con độc lập và tổ hợp chúng thành một mô hình đa đầu vào đa đầu ra để có được độ chính xác tốt hơn và ước lượng độ không chắc chắn với chi phí suy diễn tương tự như một mạng đơn. (Murahari et al., 2022) đề xuất kỹ thuật ghép kênh dữ liệu để ghép nhiều chuỗi đầu vào thành một chuỗi đầu vào cho mô hình Transformer, dẫn đến tăng tốc suy diễn lên đến 18x. (Murahari et al., 2023) phát triển các mô hình ngôn ngữ đã ghép kênh được tiền huấn luyện để cải thiện thông lượng mô hình.

Mô hình hóa hiệu suất
Các phương pháp khác nhau đã được đề xuất để ước lượng hiệu suất của các mô hình machine learning. (Justus et al., 2018) đề xuất một phương pháp để dự đoán thời gian thực thi CNN cho huấn luyện. Họ phân tách huấn luyện CNN thành nhiều thành phần, ước lượng thời gian cho mỗi thành phần, và dự đoán thời gian thực thi mô hình như sự kết hợp của các thành phần khác nhau. (Qi et al., 2017; Cai et al., 2017) dự đoán hiệu suất của các mạng nơ-ron sâu dựa trên kiến trúc của các mô hình mạng nơ-ron. (Stamoulis et al., 2018) đề xuất các mô hình dự đoán cho công suất và bộ nhớ của các mạng nơ-ron thực thi trên GPU. Các mô hình chi phí dựa trên machine-learning (Chen et al., 2018; Bouzidi et al., 2020) đã được khám phá để dự đoán thời gian chạy chương trình.

Nội suy (Davis, 1975) được sử dụng rộng rãi trong kỹ thuật và khoa học (Oliver và Webster, 1990; Keys, 1981; Lehmann et al., 1999), trong đó các giá trị hàm tại các điểm dữ liệu rời rạc được thu thập trong các thí nghiệm và các giá trị hàm tại các khoảng giữa các điểm dữ liệu rời rạc được ước lượng sử dụng các phương pháp nội suy.

6 Kết luận

Chúng tôi đề xuất PruMUX, một phương pháp để kết hợp nén mô hình và ghép kênh dữ liệu để xây dựng các transformer thông lượng cao. Việc triển khai PruMUX của chúng tôi sử dụng CoFi và DataMUX và chúng tôi cho thấy rằng nó đạt được cải thiện thông lượng đáng kể so với CoFi hoặc DataMUX trong một phạm vi lớn các ngưỡng độ chính xác.

Chúng tôi kết luận rằng lý do PruMUX hoạt động tốt trong phạm vi nhất định của các ngân sách tổn thất độ chính xác là CoFi và DataMUX cải thiện thông lượng của mô hình trong hai chiều khác nhau: giảm độ trễ của một suy diễn và nén nhiều suy diễn. Khi ngân sách tổn thất độ chính xác lớn, cả hai phương pháp đều dẫn đến sự sụt giảm phi tuyến trong độ chính xác mô hình, PruMUX có thể đạt được hiệu suất tốt hơn nhiều so với cả hai phương pháp vì nó sử dụng các tham số bảo thủ hơn cho CoFi và DataMUX trước khi mỗi cái đạt đến điểm đánh đổi xấu của nó.

Chúng tôi cũng trình bày Auto-PruMUX, một meta-model để tự động dự đoán các kết hợp tham số hiệu suất cao cho độ chính xác mong muốn trên một tác vụ. Chúng tôi cho thấy nó có triển vọng trong việc dự đoán tham số mà không cần các điểm dữ liệu riêng lẻ và huấn luyện bổ sung.

7 Hạn chế

Các thí nghiệm của chúng tôi bị hạn chế đến 3 mô hình được tiền huấn luyện DataMUX (N=2, 5, và 10) do ràng buộc tính toán. Nhiều mô hình được tiền huấn luyện hơn với các N khác nhau sẽ cung cấp cho PruMUX nhiều lựa chọn hơn để cải thiện thông lượng và sẽ cho phép chúng tôi tiến hành đánh giá Auto-PruMUX chi tiết hơn.

PruMUX sử dụng CoFi làm phương pháp nén mô hình của nó. Các thí nghiệm với các phương pháp khác có thể cải thiện hiểu biết của chúng tôi về các tương tác giữa nén mô hình và ghép kênh dữ liệu.

8 Lời cảm ơn

Karthik Narasimhan và Vishvak Murahari biết ơn sự hỗ trợ từ chương trình Samsung GRO.

Tài liệu tham khảo

[Phần tài liệu tham khảo được dịch nguyên văn như trong bản gốc]

A Phụ lục

A.1 Siêu tham số cho huấn luyện mô hình

Bảng 4 và Bảng 5 thể hiện các siêu tham số được sử dụng trong huấn luyện CoFi và huấn luyện PruMUX tương ứng. Các siêu tham số được lấy từ mã nguồn mở của CoFi.

A.2 Thống kê tập dữ liệu

Bảng 6 thể hiện kích thước và các chỉ số của các tập dữ liệu trong các thí nghiệm của chúng tôi.

A.3 Rủi ro tiềm ẩn

Ghép kênh với nén mô hình có thể dẫn đến rò rỉ thông tin giữa các thể hiện khác nhau, điều này có thể gây ra lo ngại về quyền riêng tư nếu được sử dụng trong API công khai phục vụ các mô hình này.
