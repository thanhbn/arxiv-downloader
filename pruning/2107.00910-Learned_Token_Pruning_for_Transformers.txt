# 2107.00910.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2107.00910.pdf
# File size: 1166538 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Learned Token Pruning for Transformers
Sehoon Kimâˆ—
sehoonkim@berkeley.edu
University of California, Berkeley
Berkeley, CA, USASheng Shenâˆ—
sheng.s@berkeley.edu
University of California, Berkeley
Berkeley, CA, USADavid Thorsleyâˆ—
d.thorsley@samsung.com
Samsung Semiconductor, Inc.
San Jose, CA, USA
Amir Gholamiâˆ—
amirgh@berkeley.edu
University of California, Berkeley
Berkeley, CA, USAWoosuk Kwon
woosuk.kwon@berkeley.edu
University of California, Berkeley
Berkeley, CA, USAJoseph Hassoun
j.hassoun@samsung.com
Samsung Semiconductor, Inc.
San Jose, CA, USA
Kurt Keutzer
keutzer@berkeley.edu
University of California, Berkeley
Berkeley, CA, USA
ABSTRACT
Efficient deployment of transformer models in practice is challeng-
ing due to their inference cost including memory footprint, latency,
and power consumption, which scales quadratically with input se-
quence length. To address this, we present a novel token reduction
method dubbed Learned Token Pruning ( LTP) which adaptively
removes unimportant tokens as an input sequence passes through
transformer layers. In particular, LTP prunes tokens with an atten-
tion score below a threshold, whose value is learned for each layer
during training. Our threshold-based method allows the length
of the pruned sequence to vary adaptively based on the input se-
quence, and avoids algorithmically expensive operations such as
top-ğ‘˜token selection. We extensively test the performance of LTP
on GLUE and SQuAD tasks and show that our method outperforms
the prior state-of-the-art token pruning methods by up to âˆ¼2.5%
higher accuracy with the same amount of FLOPs. In particular, LTP
achieves up to 2.1Ã—FLOPs reduction with less than 1% accuracy
drop, which results in up to 1.9 Ã—and 2.0Ã—throughput improvement
on Intel Haswell CPUs and NVIDIA V100 GPUs. Furthermore, we
demonstrate that LTP is more robust than prior methods to varia-
tions in input sequence lengths. Our code has been developed in
PyTorch and open-sourced1.
CCS CONCEPTS
â€¢Computer systems organization â†’Neural networks ;Natu-
ral language processing .
âˆ—Equal contribution.
1https://github.com/kssteven418/LTP
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.
Â©2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9385-0/22/08. . . $15.00
https://doi.org/10.1145/3534678.3539260KEYWORDS
Deep Learning, Network Pruning, Natural Language Processing
ACM Reference Format:
Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon,
Joseph Hassoun, and Kurt Keutzer. 2022. Learned Token Pruning for Trans-
formers. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (KDD â€™22), August 14â€“18, 2022, Washington, DC,
USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3534678.
3539260
1 INTRODUCTION
Transformer-based deep neural network architectures [ 45], such
as BERT [ 7] and RoBERTa [ 28], achieve state-of-the-art results in
Natural Language Processing (NLP) tasks such as sentence classi-
fication and question answering. However, efficiently deploying
these models is increasingly challenging due to their large size,
the need for real-time inference, and the limited energy, compute,
and memory resources available. The heart of a transformer layer
is the multi-head self-attention mechanism, where each token in
the input sequence attends to every other token to compute a new
representation of the sequence. Because all tokens attend to each
others, the computation complexity is quadratic with respect to the
input sequence length; thus the ability to apply transformer models
to long input sequences becomes limited.
Pruning is a popular technique to reduce the size of neural net-
works and the amount of computation required. Unstructured prun-
ing allows arbitrary patterns of sparsification for parameters and
feature maps and can, in theory, produce significant computational
savings while preserving accuracy. However, commodity DNN ac-
celerators cannot efficiently exploit unstructured sparsity patterns.
Thus, structured pruning methods are typically preferred in practice
due to their relative ease of deployment to hardware.
Multi-head self-attention provides several possibilities for struc-
tured pruning; for example, head pruning [ 30,46] decreases the
size of the model by removing unneeded heads in each transformer
layer. Another orthogonal approach that we consider in this paper
istoken pruning , which reduces computation by progressively re-
moving unimportant tokens in the sequence during inference. ForarXiv:2107.00910v3  [cs.CL]  2 Jun 2022

--- PAGE 2 ---
KDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, Kurt Keutzer
0 20 40 60
QQP Sequence Length0.000.020.04Density
(a) QQP
0 20 40
SST2 Sequence Length0.0000.0250.0500.075Density (b) SST-2
0 20 40 60
STSB Sequence Length0.000.020.040.06Density (c) STS-B
Figure 1: Distributions of processed input sequence lengths from datasets for representative tasks in the GLUE benchmark:
(a) QQP (b) SST-2; (c) STS-B. The training set is in orange and the validation set is in blue. The dashed and solid vertical lines
indicate the 99th percentile value for the training and validation sets, respectively.
NLP tasks such as sentence classification, token pruning is an at-
tractive approach to consider as it exploits the intuitive observation
that not all tokens (i.e., words) in an input sentence are necessarily
required to make a successful inference.
There are two main classes of token pruning methods. In the
first class, methods like PoWER-BERT [ 13] and Length-Adaptive
Transformer (LAT) [ 21] search for a single token pruning config-
uration (i.e., sequence length for each layer) for an entire dataset.
In other words, they prune all input sequences to the same length.
However, input sequence lengths can vary greatly within tasks
and between training and validation sets as in Figure 1, and thus
applying a single pruning configuration to all input sequences can
potentially under-prune shorter sequences or over-prune longer
sequences.
In the other class, the token pruning method adjusts the config-
uration based on the input sequence. SpAtten [ 49] uses a pruning
configuration proportional to input sentence length; however, it
does not adjust the proportion of pruned tokens based on the con-
tent of the input sequence. The recently published TR-BERT [ 54]
uses reinforcement learning (RL) to find a policy network that dy-
namically reduces the number of tokens based on the length and
content of the input sequence; however, it requires additional costly
training for convergence of the RL-based method. Additionally, all
of these prior methods rely in part on selecting the ğ‘˜most signif-
icant tokens during inference or training. This selection can be
computationally expensive without the development of specialized
hardware, such as the top- ğ‘˜engine introduced in SpAtten [49].
To this end, we propose a learned threshold -based token prun-
ing method which adapts to the length and content of individual
examples and avoids the use of top- ğ‘˜operations. In particular, our
contributions are as follows:
â€¢We propose Learned Token Pruning ( LTP), a threshold-based
token pruning method, which only needs a simple threshold
operation to detect unimportant tokens. In addition, LTP fully
automates the search for optimal pruning configurations by in-
troducing a differentiable soft binarized mask that allows training
the correct thresholds for different layers and tasks. (Section 3.3)â€¢We apply LTPto RoBERTa and evaluate its performance on GLUE
and SQuAD tasks. We show LTP achieves up to 2.10Ã—FLOPs re-
duction with less than 1% accuracy degradation, which results
in up to 1.93Ã—and 1.97Ã—throughput improvement on NVIDIA
V100 GPU and Intel Haswell CPU, respectively, as compared to
the unpruned FP16 baseline. We also show that LTP outperforms
SpAtten and LAT in most cases, achieving additional FLOPs re-
duction for the same drop in accuracy. (Section 4.2 and 4.5)
â€¢We show that LTP is highly robust against sentence length vari-
ations. LTP exhibits consistently better accuracy over different
sentence length distributions, achieving up to 16.4% accuracy
gap from LAT. (Section 4.3)
2 RELATED WORK
2.1 Efficient Transformers
Multiple different approaches have been proposed to improve speed
and diminish memory footprint of transformers. These can be
broadly categorized as follows: (i) efficient architecture design [ 5,16,
19,23,25,35,44,47,50,57]; (ii) knowledge distillation [ 18,37,41â€“
43]; (iii) quantization [ 1,2,10,22,39,55,56,58]; and (iv) pruning.
Here, we focus only on pruning and briefly discuss the related work.
2.2 Transformer Pruning
Pruning methods can be categorized into unstructured pruning and
structured pruning. For unstructured pruning, the lottery-ticket
hypothesis [ 11] has been explored for transformers in [ 4,31]. Re-
cently, [ 59] leverages pruning as an effective way to fine-tune trans-
formers on downstream tasks. [ 38] proposes movement pruning,
which achieves significant performance improvements versus prior
magnitude-based methods by considering the weights modification
during fine-tuning. However, it is often quite difficult to efficiently
deploy unstructured sparsity on commodity neural accelerators for
meaningful speedup.
For this reason, a number of structured pruning methods have
been introduced to remove structured sets of parameters. [ 30,46]
drop attention heads in multi-head attention layers, and [ 9,36]
prunes entire transformer layers. [ 51] structurally prunes weight
matrices via low-rank factorization, and [ 20,27] attempt to jointly

--- PAGE 3 ---
Learned Token Pruning for Transformers KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.
This is thebest restaurant, and I will be returning for another meal.This is the best restaurant, and I will be returning for another meal.  This is thebest restaurant, and I will be returning for anothermeal.This is thebest restaurant, and I will be returning for another meal.Layer 1   Layer 4Layer 8Layer 12Positive SentimentClassification15 tokens11 tokens4 tokens2 tokens
Figure 2: (Left) Schematic of token pruning for a sentiment analysis task. Unimportant tokens are pruned as the input sequence
passes through the layers. (Right) An example of attention probability in a single head where a more important token receives
more attention from other tokens. Thus each tokenâ€™s importance score is computed by taking the average attention probability
it receives, which is computed by taking the column mean of the attention probability.
prune attention heads and filters of weight matrices. [ 15,29] dy-
namically determines structured pruning ratios during inference.
Recent block pruning schemes chunk weight matrices into multiple
blocks and prune them based on group Lasso optimization [ 26],
adaptive regularization [ 53], and movement pruning [ 24]. All of
these methods correspond to weight pruning , where model parame-
ters (i.e., weights) are pruned.
Recently, there has been work on pruning input sentences to
transformers, rather than model parameters. This is referred to
astoken pruning , where less important tokens are progressively
removed during inference. PoWER-BERT [ 13], one of the earliest
works, proposes to directly learn token pruning configurations.
LAT [ 21] extends this idea by introducing LengthDrop, a procedure
in which a model is trained with different token pruning configu-
rations, followed by an evolutionary search. This method has an
advantage that the former training procedure need not be repeated
for different pruning ratios of the same model. While these meth-
ods have shown a large computation reduction on various NLP
downstream tasks, they fix a single token pruning configuration
for the entire dataset. That is, they prune all input sequences to
the same length. However, as shown in Figure 1, input sequence
lengths vary greatly within a task. As a consequence, fixing a sin-
gle pruning configuration can under-prune shorter sequences so
as to retain sufficient tokens for processing longer sequences or,
conversely, over-prune longer sequences to remove sufficient to-
kens to efficiently process shorter sequences. More importantly,
a single pruning configuration lacks robustness against input se-
quence length variations, where input sentences at inference time
are longer than those in the training dataset [32].
In contrast, SpAtten [ 49] circumvents this issue by assigning a
pruning configuration proportional to the input sequence length.
While this allows pruning more tokens from longer sequences and
fewer tokens from shorter ones, it is not adaptive to individual in-
put sequences as it assigns the same configuration to all sequenceswith the same length regardless of their contents. In addition, the
pruning configurations are determined heuristically and thus can
result in a suboptimal solution. Recently, TR-BERT [ 54] proposes to
learn a RL policy network to apply adaptive pruning configurations
for each input sequence. However, as noted by the authors, the
problem has a large search spaces which can be hard for RL to solve.
This issue is mitigated by heuristics involving imitation learning
and sampling of action sequences, which significantly increases
the cost of training. Importantly, all of the aforementioned token
pruning methods depend partially or entirely on top- ğ‘˜operation for
selecting the ğ‘˜most important tokens during inference or training.
This operation can be costly without specialized hardware support,
as discussed in [ 49].LTP, on the other hand, is based on a fully
learnable threshold-based pruning strategy. Therefore, it is (i) adap-
tive to both input length and content, (ii) robust to sentence length
variations, (iii) computationally efficient, and (iv) easy to deploy.
3 METHODOLOGY
3.1 Background
BERT [ 7] consists of multiple transformer encoder layers [ 45]
stacked up together. A basic transformer encoder layer consists
of a multi-head attention (MHA) block followed by a point-wise
feed-forward (FFN) block, with residual connections around each.
Specifically, an MHA consists of ğ‘â„independently parameter-
ized heads. An attention head â„in layerğ‘™is parameterized by
W(â„,ğ‘™)
ğ‘˜,W(â„,ğ‘™)
ğ‘,W(â„,ğ‘™)
ğ‘£âˆˆRğ‘‘â„Ã—ğ‘‘,W(â„,ğ‘™)
ğ‘œâˆˆRğ‘‘Ã—ğ‘‘â„, whereğ‘‘â„is typ-
ically set to ğ‘‘/ğ‘â„andğ‘‘is the feature dimension. We drop the
superscript ğ‘™for simplicity in the following formula. The MHA
measures the pairwise importance of each token on every other
token in the input:
MHA(x)=ğ‘â„âˆ‘ï¸
â„=1AttW(â„)
ğ‘˜,ğ‘,ğ‘£,ğ‘œ(x), (1)

--- PAGE 4 ---
KDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, Kurt Keutzer
Layer	ğ‘™Importance	scoreğ‘ (")Hard	PruningLayer	ğ‘™+1(Pruned)0.30.10.50.2ğ‘¥!ğ‘¥"ğ‘¥#Threshold	ğœƒ(")ğ‘¥!ğ‘¥#Layer	ğ‘™Importance	score	ğ‘ (")Ã—Layer	ğ‘™+10.30.10.50.2ğ‘¥!ğ‘¥"ğ‘¥#Thresholdğœƒ(")ğ‘¥5!ğ‘¥5#ğ‘¥5"
Figure 3: Different pruning strategies for threshold-based token pruning methods. (Left) Hard pruning uses a binary hard
mask to select tokens to be pruned. (Right) Soft pruning replaces the binary mask with a differentiable soft mask.
where xâˆˆRğ‘‘Ã—ğ‘›is the input sequence with the sequence length ğ‘›,
and Att Wğ‘˜,ğ‘,ğ‘£,ğ‘œis:
AttWğ‘˜,ğ‘,ğ‘£,ğ‘œ(x)=Wğ‘œğ‘›âˆ‘ï¸
ğ‘–=1Wğ‘£xğ‘–softmax(xğ‘‡Wğ‘‡ğ‘Wğ‘˜xğ‘–âˆš
ğ‘‘),(2)
xMHA=LN AttWğ‘˜,ğ‘,ğ‘£,ğ‘œ(x)+x, (3)
where Eq. 3 is the residual connection and the follow up LayerNorm
(LN). The output of the MHA is then fed into the FFN block which
applies two feed-forward layers to this input:
FFN(xMHA)=ğœ W2(W1xMHA+ğ‘1)+ğ‘2, (4)
xout=LN FFN(xMHA)+xMHA, (5)
where W1,W2,ğ‘1andğ‘2are the FFN parameters, and ğœis the
activation function (typically GELU for BERT).
3.2 Threshold Token Pruning
Let us denote the attention probability of headâ„between token x ğ‘–
and xğ‘—asA(â„,ğ‘™):
A(â„,ğ‘™)(xğ‘–,xğ‘—)=softmax(xğ‘‡Wğ‘‡ğ‘Wğ‘˜x
âˆš
ğ‘‘)(ğ‘–,ğ‘—)âˆˆR. (6)
The cost of computational complexity for computing the attention
matrix isO(ğ‘‘2ğ‘›+ğ‘›2ğ‘‘), which quadratically scales with sequence
length. As such, the attention operation becomes a bottleneck when
applied to long sequences. To address this, we apply token pruning
which removes unimportant tokens as the input passes through the
transformer layers to reduce the sequence length ğ‘›for later blocks.
This is schematically shown in Figure 2 (Left).
For token pruning, we must define a metric to determine unim-
portant tokens. Following [ 13,21,49], we define the importance
score of token xğ‘–in layerğ‘™as:
ğ‘ (ğ‘™)(xğ‘–)=1
ğ‘â„1
ğ‘›ğ‘â„âˆ‘ï¸
â„=1ğ‘›âˆ‘ï¸
ğ‘—=1A(â„,ğ‘™)(xğ‘–,xğ‘—). (7)
Intuitively, the attention probability A(â„,ğ‘™)(xğ‘–,xğ‘—)is interpreted
as the normalized amount that all the other tokens x ğ‘—attend to
token xğ‘–. Token xğ‘–is thus considered important if it receives more
attention from all tokens across all heads, which directly leads us
to equation 7. The procedure for computing importance scores from
attention probabilities is illustrated in Figure 2 (Right).In [13,21,49], tokens are ranked by importance score and pruned
using a top-ğ‘˜selection strategy. Specially, token x ğ‘–is pruned at layer
ğ‘™if its important score ğ‘ (ğ‘™)(xğ‘–)is smaller than the ğ‘˜-largest values
of the important score from all the tokens. However, finding the ğ‘˜-
largest values of the importance score is computationally inefficient
without specialized hardware [ 49]; we provide empirical results
showing this in Section A.2. Instead, we introduce a new threshold-
based token pruning approach in which a token is pruned only
if its importance score is below a threshold denoted by ğœƒ(ğ‘™)âˆˆR.
Specifically, we define a pruning strategy by imposing a binary
maskğ‘€(ğ‘™)(Â·):{1,...,ğ‘›} â†’ { 0,1}which indicates whether a
token should be kept or pruned:
ğ‘€(ğ‘™)(xğ‘–)=(
1ifğ‘ (ğ‘™)(xğ‘–)>ğœƒ(ğ‘™),
0otherwise.(8)
Note that this operation only requires a simple comparison op-
erator without any expensive top- ğ‘˜calculation. Once a token is
pruned, it is excluded from calculations in all succeeding layers,
thereby gradually reducing computation complexity towards the
output layers.
3.3 Learnable Threshold for Token Pruning
A key concern with the method above is how to determine the
threshold values for each layer. Not only do threshold values change
for different layers, they also vary between different tasks. We
address this by making the thresholds (i.e., ğœƒin Eq. 8) learnable.
However, there are several challenges to consider. First, due to the
binary nature of ğ‘€there is no gradient flow for pruned tokens. Sec-
ond, theğ‘€operator is non-differentiable which prevents gradient
flow into the thresholds. To address these challenges, we use a soft
pruning scheme that simulates the original hard pruning while still
propagating gradients to the thresholds as shown in Figure 3.
Soft Pruning Scheme. In the soft pruning scheme, we replace
the non-differentiable mask ğ‘€(ğ‘™)with a differentiable soft mask
using the sigmoid operation ğœ:
Ëœğ‘€(ğ‘™)(xğ‘–)=ğœ 
ğ‘ (ğ‘™)(xğ‘–)âˆ’ğœƒ(ğ‘™)
ğ‘‡!
, (9)
whereğ‘‡is temperature, and ğœƒ(ğ‘™)is the learnable threshold value for
layerğ‘™. With sufficiently small temperature ğ‘‡,Ëœğ‘€(ğ‘™)(xğ‘–)will closely
approximate the hard masking ğ‘€(ğ‘™)(xğ‘–)in Eq. 8. In addition, instead

--- PAGE 5 ---
Learned Token Pruning for Transformers KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.
Algorithm 1 Three-step Training Procedure for Learnable Thresh-
old Token Pruning
Input:M: model finetuned on target downstream task
Step 1: Apply soft mask to Mand train both the thresholds and
model parameters âŠ²Soft Pruning
Step 2: Binarize the mask and fix the thresholds
Step 3: Finetune the model parameters âŠ²Hard Pruning
of selecting tokens to be pruned or kept based on the hard mask
of Eq. 8, we multiply the soft mask to the output activation of layer
ğ‘™. That is,
Ëœx(ğ‘™)
out=Ëœğ‘€(ğ‘™)(x(ğ‘™))Â·x(ğ‘™)
out(10)
=Ëœğ‘€(ğ‘™)(x(ğ‘™))Â·LN(FFN(x(ğ‘™)
MHA)+x(ğ‘™)
MHA), (11)
where x(ğ‘™)
MHAis the output activation of MHA in layer ğ‘™. If the
importance score of token x ğ‘–is below the threshold by a large
margin, its layer output activation nears zero and thus it has little
impact on the succeeding layer. Also, because the token gets a zero
importance score in the succeeding layer, i.e., ğ‘ (ğ‘™+1)(xğ‘–)=0, it is
likely to be pruned again. Therefore, the soft pruning scheme is
nearly identical in behavior to hard pruning, yet its differentiable
form allows for backpropagation and gradient-based optimizations
to makeğœƒlearnable. After (i) jointly training model parameters and
thresholds on downstream tasks with the soft pruning scheme, (ii)
we fix the thresholds and binarize the soft mask, and (iii) perform a
follow-up fine-tuning of the model parameters. The pseudo-code
for this three-step algorithm is given in Algorithm 1. Intuitively,
the magnitude of gradient ğ‘‘Ëœğ‘€(ğ‘™)(xğ‘–)/ğ‘‘ğœƒ(ğ‘™)is maximized when the
importance score ğ‘ (ğ‘™)(xğ‘–)is close enough to the threshold ğœƒ(ğ‘™)
and becomes near zero elsewhere. Therefore, the threshold can be
trained only based on the tokens that are about to be pruned or
retained.
Regularization. It is not possible to learn ğœƒwithout regulariza-
tion, as the optimizer generally gets a better loss value if all tokens
are present. As such, we add a regularization term to penalize the
network if tokens are left unpruned. This is achieved by imposing
an L1 loss on the masking operator Ëœğ‘€:
Lnew=L+ğœ†LregwhereLreg=1
ğ¿ğ¿âˆ‘ï¸
ğ‘™=1||Ëœğ‘€(ğ‘™)(x)||1. (12)
Here,Lis the original loss function (e.g., cross-entropy loss), and ğœ†
is the regularization parameter. Larger values of ğœ†result in higher
pruning ratios. This regularization operator induces an additional
gradient to the threshold:
ğ‘‘Lreg
ğ‘‘ğœƒ(ğ‘™)=1
ğ‘‘ğœƒ(ğ‘™)||Ëœğ‘€(ğ‘™)(x)||1=ğ‘›âˆ‘ï¸
ğ‘–=1ğ‘‘Ëœğ‘€(ğ‘™)(xğ‘–)
ğ‘‘ğœƒ(ğ‘™)(13)
If there are more tokens near the threshold, then the gradient
ğ‘‘Lreg/ğ‘‘ğœƒ(ğ‘™)is larger. As a result, the threshold is pushed to a larger
value, which prunes more tokens near the threshold boundary.Table 1: Detailed performance and efficiency comparison of
LTP applied to RoBERTa base.
TaskAccuracy Metric GFLOPs Speedup
RoBERTa LTP RoBERTa LTP LTP
MNLI-m 87.53 86.53 6.83 3.64 1.88Ã—
MNLI-mm 87.36 86.37 7.15 3.63 1.97Ã—
QQP 90.39 89.69 5.31 2.53 2.10Ã—
QNLI 92.86 91.98 8.94 4.77 1.87Ã—
SST-2 94.27 93.46 4.45 2.13 2.09Ã—
STS-B 90.89 90.03 5.53 2.84 1.95Ã—
MRPC 92.14 91.59 9.33 4.44 2.10Ã—
RTE 77.98 77.98 11.38 6.30 1.81Ã—
SQuAD 2.0 83.04 82.25 32.12 16.99 1.89Ã—
4 EXPERIMENTS
4.1 Experiment Setup
We implemented LTP on RoBERTa base[28] using HuggingFaceâ€™s
repo2and tested on (English) GLUE tasks [ 48] and SQuAD 2.0 [ 33].
For GLUE tasks [ 48], we use 6 tasks for evaluation including sen-
tence similarity (QQP [ 17], MRPC [ 8], STS-B [ 3]), sentiment clas-
sification (SST-2 [ 40]), textual entailment (RTE [ 6]) and natural
language inference (MNLI [ 52], QNLI [ 34]). For evaluating the re-
sults, we measure classification accuracy and F1 score for MRPC
and QQP, Pearson Correlation and Spearman Correlation for STS-B,
and classification accuracy for the remaining tasks on validation
sets. For the tasks with multiple metrics (i.e., MRPC, QQP, STS-B),
we report their average. For SQuAD 2.0 [ 33], which is a question
and answering task, we measure F1 score for evaluating the results.
As mentioned in Section 3.3, the training procedure of LTP
consists of two stages: soft pruning that trains both the model pa-
rameters and thresholds on downstream tasks, followed by hard
pruning that fine-tunes the model parameters with fixed thresh-
olds. We also compare LTP with the current state-of-the-art token
pruning methods of SpAtten [ 49] and LAT [ 21] following the im-
plementation details in their papers. See A.1 for the details of the
training process. We use PyTorch 1.8 throughout all experiments.
For CPU inference speed experiments, we use an Intel Haswell CPU
with 3.75GB memory of Google Cloud Platform. For GPU inference
speed experiments, we use an AWS p3.2xlarge instance that has a
NVIDIA V100 GPU with CUDA 11.1.
An important issue in previous work [ 13,21] is that allinput
sequences for a specific task are padded to the nearest power of
2 from the 99th percentile of the sequence lengths, and then the
pruned performance is compared with the padded baseline. This
results in exaggerated performance gain over the baseline. For
instance, in [ 13], inputs from the SST-2 dataset are padded to 64,
while its average sentence length is 26 (cf. Figure 1). With this
approach, one can achieve roughly 2.5Ã—speedup by just removing
padding. As such, we avoid any extra padding of input sequences
and all speedups and throughputs we report are compared with the
unpadded baselines.
2https://github.com/huggingface/transformers/

--- PAGE 6 ---
KDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, Kurt Keutzer
50 60 70 80
Relative FLOPs (%)85.085.586.086.587.087.5Accuracy (%)MNLI-m
50 60 70 80
Relative FLOPs (%)85.586.086.587.087.5Accuracy (%)MNLI-mm
50 60 70
Relative FLOPs (%)89.2589.5089.7590.0090.2590.50Accuracy (%)QQP
50 60 70 80
Relative FLOPs (%)90.591.091.592.092.5Accuracy (%)QNLI
40 50 60 70
Relative FLOPs (%)92.092.593.093.594.094.5Accuracy (%)SST-2
50 60 70 80
Relative FLOPs (%)88.088.589.089.590.090.591.0Accuracy (%)STS-B
40 50 60 70 80
Relative FLOPs (%)90.090.591.091.592.092.5Accuracy (%)MRPC
60 70 80
Relative FLOPs (%)75767778Accuracy (%)RTE
RoBERTabase RoBERTabase - 1% LTP (ours) Manual Threshold SpAtten LAT
Figure 4: Performance of different pruning methods on GLUE tasks for different token pruning methods across different rela-
tive FLOPs, i.e., normalized FLOPs with respect to the the baseline model. Manual threshold assigns linearly raising threshold
values for each layer instead of learning them. The performance of the baseline model without token pruning (RoBERTa base)
and the model with 1% performance drop (RoBERTa base- 1%) are dotted in horizontal lines for comparison.
Table 2: Quantiles (Q1/Q2/Q3) and KL divergence of sen-
tence lengths of training and evaluation datasets for GLUE
tasks. KL divergence are measured after binning the sen-
tence lengths into 20 bins for RTE, MRPC, and STS-B and
50 bins for the others.
Task Quantiles (train) Quantiles (eval) KL Div.
MNLI-m 27/38/50 26/37/50 0.0055
MNLI-mm 27/38/50 29/39/51 0.0042
QQP 23/28/36 23/28/36 0.0006
QNLI 39/48/59 39/49/61 0.0092
SST-2 7/11/19 18/25/33 1.2250
STS-B 20/24/32 21/29/41 0.0925
MRPC 45/54/63 45/54/64 0.0033
RTE 44/57/86 42/54/78 0.0261
4.2 Performance Evaluation
Table 1 lists the accuracy and GFLOPs for LTP. We select a model for
each downstream task that achieves the smallest GFLOPs while con-
straining the accuracy degradation from the baseline (RoBERTa base)
to be at most 1%. Using our method, sequence lengths in each layer
can vary across different input sentences. Therefore, we report the
averaged GFLOPs of processing all input sentences in the develop-
ment set. As shown in the table, our method achieves speedup of
1.96Ã—on average and up to 2.10 Ã—within 1% accuracy degradation.
Figure 4 plots the accuracy of LTP(blue lines) as well as the prior
pruning methods (red lines for SpAtten and orange lines for LAT)
with different FLOPs on GLUE tasks. LTP consistently outperforms
SpAtten for all tasks with up to ~2% higher accuracy under the same
amount of FLOPs. Compared with LAT, LTP outperforms for alltasks except for QQP with up to ~2.5% higher accuracy for the same
target FLOPs. For QQP alone, LTP achieves at most ~0.2% lower
accuracy than LTP.
An important observation is that for SST-2 and STS-B where LTP
(ours) outperforms LAT with large margins, the sequence length
varies greatly from the training dataset to the evaluation dataset as
can be seen from the large KL-divergence in Table 2 and Figure 1 (b,
c). On the other hand, for QQP, the only dataset that LAT slightly
outperforms LTP (ours), the sequence length distribution of the
training dataset is almost identical to that of the evaluation dataset
as can be seen from the small KL-divergence in Table 2 and Figure 2
(a). This observation supports our claim in Section 1 and 2: LTP is
robust to sequence length variations as it does not fix the pruning
configuration unlike other methods using a single pruning configu-
ration regardless of the input sequence length. This is important in
practice because the sequence lengths during inference do not al-
ways follow the sequence length distribution of the training dataset
as in SST-2 and STS-B. We make a further discussion in Section 4.3.
For SQuAD 2.0, we have similar results to GLUE. As can be seen
in Table 1 and Figure 5 (Left), we obtain nearly identical F1 score to
baseline at 0.58 relative FLOPs, and 1.89 Ã—speedup with less than 1%
drop of F1 score. The SQuAD 2.0 dataset is divided into two subsets:
the subset of examples where the answer to the question is included
in the context text, and the subset that has no answer. In Figure 5
(Right), we further plot the results on each subset of the dataset
(black and red for the one with and without answers, respectively).
We see that the F1 score decreases for the subset with answers and
increases for the subset without answers as we decrease the relative
FLOPs. This is to be expected as the question answering head will
predict no answer if the start and end points of the answer within
the context cannot be determined due to high token pruning ratios.

--- PAGE 7 ---
Learned Token Pruning for Transformers KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.
55 60 65 70
Relative FLOPs (%)82.0082.2582.5082.7583.0083.2583.5083.75F1 Score (%)SQuAD 2.0
RoBERTabase
RoBERTabase - 1%
LTP
55 60 65 70
Relative FLOPs (%)8081828384F1 Score (%)SQuAD 2.0 - Has/No Answer
RoBERTabase (Has Ans)
RoBERTabase (No ANs)
LTP (Has Ans)
LTP (No Ans)
Figure 5: (Left) Performance of LTP on SQuAD 2.0 across dif-
ferent relative FLOPs with respect to the the baseline model
on the full validation set. (Right) Performance of LTP on
the subsets of the validation set, one with answers (Has Ans,
black) and the other without (No Ans, red). The performance
of the baseline model without token pruning (RoBERTa base)
and the model with 1% performance drop (RoBERTa base- 1%)
are dotted in horizontal lines for comparison.
Thus, a careful setting of ğœ†in Eq. 12 is necessary to balance the
accuracy between the two subsets.
At last, we also highlight that LTP has an additional gain over
the prior top- ğ‘˜based approaches by avoiding computationally in-
efficient top- ğ‘˜operations as further discussed in Section A.2.
4.3 Robustness to Sequence Length Variation
In Section 4.2, we claim that LTP is more robust against sequence
length variations from training time to evaluation time. Here, we
make a more systematic analysis on this. Ideally, performance
should be independent of sequence length. To quantitatively test
the robustness of pruning methods against sequence length varia-
tions, we train LTP and LAT on QNLI and QQP, but only using the
training examples whose sequence lengths are below the median
length of the evaluation dataset. We then evaluate the resulting
models using the evaluation examples with sequence lengths (i)
below the median (~Q2), (ii) between the median and the third
quantile (Q2~Q3), and (iii) above the third quantile (Q3~) of the
evaluation dataset. To make a fair comparison, we choose models
fromLTP and LAT that require similar FLOPs on ~Q2.
The results are listed in Table 3. LTP consistently achieves better
accuracy and FLOPs over different sequence lengths, even with
those that are significantly longer than the training sequences. On
the contrary, LAT shows significant accuracy degradation as longer
sequences are over-pruned, which can be seen from the significant
FLOPs reduction. In particular, LTP outperforms LAT by up to
16.44% and 9.20% on QNLI and QQP for the Q3 ~evaluation dataset.
4.4 Ablation Studies
Instead of learning thresholds, we can set them manually. Because
manually searching over the exponential search space is intractable,
we add a constraint to the search space by assigning linearly rising
threshold values for each layer, similar to how SpAtten [ 49] assigns
the token retain ratios: given the threshold of the final layer ğœƒ(ğ¿),
the threshold for layer ğ‘™is set asğœƒ(ğ¿)ğ‘™/ğ¿. We plot the accuracy
and FLOPs of the manual threshold approach in Figure 4 as black
lines. While this approach exhibits decent results on all downstreamTable 3: LTP and LAT trained with the sequences shorter
than the median length, and evaluated with the sequences
shorther than the median (~Q2), between the median and the
third quantile (Q2~Q3), and longer than the third quantile
(Q3~) of the evaluation dataset. FLOPs are relative FLOPs (%)
with respect to RoBERTa ğ‘ğ‘ğ‘ ğ‘’.
TaskQNLI QQP
~Q2 Q2~Q3 Q3~ ~Q2 Q2~Q3 Q3~
LTP Acc. 91.21 90.02 91.81 89.42 89.51 91.37
(ours) FLOPs 55.89 55.60 56.02 55.18 56.29 58.01
LAT Acc. 90.87 86.12 75.37 89.20 87.27 82.17
FLOPs 56.21 46.55 35.89 55.17 46.61 34.14
Diff. Acc. +0.34 +3.90 +16.44 +0.22 +2.24 +9.20
tasks, the learned thresholds consistently outperform the manual
thresholds under the same FLOPs. This provides empirical evidence
for the effectiveness of our threshold learning method.
4.5 Direct Throughput Measurement on
Hardware
We directly measure throughputs on real hardware by deploying
LTP on a NVIDIA V100 GPU and a Intel Haswell CPU. For infer-
ence, we completely remove the pruned tokens and rearrange the
retained tokens into a blank-free sequence to have a latency gain.
One consequence of adaptive pruning, however, is that each se-
quence will end up with a different pruning pattern and sequence
length. As such, a naive hardware implementation of batched in-
ference may require padding all the sequences in a batch to ensure
that they all have the same length (i.e., the maximum sequence
length in the batch), which results in a significant portion of com-
putation being wasted to process padding tokens. To avoid this, we
use NVIDIAâ€™s Faster Transformer3for GPU implementation that
requires large batch sizes. This framework dynamically removes
and inserts padding tokens during inference so that most of the
transformer operations effectively skip processing padding tokens.
This enables fast inference even with irregular pruning lengths of
individual sequences. For the CPU implementation, we find naive
batching (i.e., padding sequences to the maximum sentence length)
enough for good throughput.
The measured throughput results are shown in Figure 6 for
different batch sizes. For all experiments, relative throughput is
evaluated 3 times on the randomly shuffled datasets. LTP achieves
up toâˆ¼1.9Ã—andâˆ¼2.0Ã—thoughput improvement for QNLI and QQP
on both CPU and GPU, as compared to the baseline. This is similar to
the theoretical speedup inferred from the FLOPs reduction reported
in Table 1. Importantly, the speedup of LTP increases with larger
batch sizes on both CPU and GPU, proving effectiveness of LTP on
batched cases.
4.6LTP with Quantization and Knowledge
Distillation
Here, we show that our token-level pruning method is compat-
ible with other compression methods. In particular, we perform
3https://github.com/NVIDIA/FasterTransformer

--- PAGE 8 ---
KDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, Kurt Keutzer
Figure 6: Relative throughput of LTP with respect to the baseline without token pruning (RoBERTa base) with different batch
sizes on Intel Haswell CPU and NVIDIA V100 GPU. The performance of RoBERTa baseare dotted in horizontal lines.
10 20 30 40 50 60 70
Relative BOPs (%)87.588.088.589.089.590.090.5Accuracy (%)QQP
10 20 30 40 50 60 70
Relative BOPs (%)91929394Accuracy (%)SST-2
RoBERTabase RoBERTabase INT8 RoBERTabase - 2% LTP (ours) LTP INT8 (ours)
Figure 7: Accuracy and relative BOPs of the FP16 baselines
and INT8 LTP models on QQP and SST-2 datasets. Note that
FP16 unpruned RoBERTa baseis used as the baseline. Thus,
INT8 quantization of the models translates to 4Ã—reduction
in relative BOPs.
compression experiments by combining LTP with quantization and
knowledge distillation [ 14] together. For quantization, we use the
static uniform symmetric integer quantization method [ 12], which
is easy to deploy in commodity hardware with minimal run-time
overhead. All the model parameters are quantized to 8-bit integers,
except for those of the embedding layer whose bit-width does not
affect the inference speed. Afterwards, we apply knowledge distil-
lation that helps recover accuracy for high compression ratios. We
set the baseline RoBERTa basemodel as the teacher and the quan-
tizedLTP model as the student. We then distill knowledge from
the teacher model into the student model through a knowledge
distillation loss that matches the output logits of the classification
layer and the output representations of the embedding layer in
the teacher model to the counterparts in the student model. The
training objective is a convex combination of the original loss and
the knowledge distillation loss. As shown in Figure 7, we achieve
up to 10Ã—reduction in bit operations (BOPs) with less than 2%accu-
racy degradation as compared to FP16 RoBERTa baseby combining
quantization and knowledge distillation. The results empirically
show the effectiveness of LTP with other compression methods.
5 CONCLUSIONS
In this work, we present Learned Token Pruning ( LTP), a fully
automated token pruning framework for transformers. LTP only
requires comparison of token importance scores with threshold
values to determine unimportant tokens, thus adding minimal com-
plexity over the original transformer inference. Importantly, thethreshold values are learned for each layer during training through
a differentiable soft binarized mask that enables backpropagation of
gradients to the threshold values. Compared to the state-of-the-art
token pruning methods, LTP outperforms by up to ~2.5% accuracy
with the same amount of FLOPs. Extensive experiments on GLUE
and SQuAD show the effectiveness of LTP, as it achieves up to 2.10 Ã—
FLOPs reduction over the baseline model within only 1% of accuracy
degradation. Our preliminary (and not highly optimized) imple-
mentation shows up to 1.9 Ã—and 2.0Ã—throughput improvement
on an Intel Haswell CPU and a NVIDIA V100 GPU. Furthermore,
LTP exhibits significantly better robustness and consistency over
different input sequence lengths.
REFERENCES
[1]Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael
Lyu, and Irwin King. 2020. BinaryBERT: Pushing the Limit of BERT Quantization.
arXiv preprint arXiv:2012.15701 (2020).
[2]Aishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada, Vivek Menon, Sun
Choi, Kushal Datta, and Vikram Saletore. 2019. Efficient 8-bit quantization
of transformer neural machine language translation model. arXiv preprint
arXiv:1906.00532 (2019).
[3]Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. 2017.
Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual
focused evaluation. arXiv preprint arXiv:1708.00055 (2017).
[4]Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang
Wang, and Michael Carbin. 2020. The lottery ticket hypothesis for pre-trained
BERT networks. arXiv preprint arXiv:2007.12223 (2020).
[5]Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating
long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019).
[6]Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recog-
nising textual entailment challenge. In Machine Learning Challenges Workshop .
Springer, 177â€“190.
[7]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[8]William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus
of sentential paraphrases. In Proceedings of the Third International Workshop on
Paraphrasing (IWP2005) .
[9]Angela Fan, Edouard Grave, and Armand Joulin. 2019. Reducing transformer
depth on demand with structured dropout. arXiv preprint arXiv:1909.11556 (2019).
[10] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, RÃ©mi Gribonval,
HervÃ© JÃ©gou, and Armand Joulin. 2020. Training with quantization noise for
extreme model compression. arXiv e-prints (2020), arXivâ€“2004.
[11] Jonathan Frankle and Michael Carbin. 2018. The lottery ticket hypothesis: Finding
sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 (2018).
[12] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and
Kurt Keutzer. 2021. A survey of quantization methods for efficient neural network
inference. arXiv preprint arXiv:2103.13630 (2021).
[13] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakar-
avarthy, Yogish Sabharwal, and Ashish Verma. 2020. Power-bert: Accelerating
bert inference via progressive word-vector elimination. In International Confer-
ence on Machine Learning . PMLR, 3690â€“3699.

--- PAGE 9 ---
Learned Token Pruning for Transformers KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.
[14] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in
a neural network. arXiv preprint arXiv:1503.02531 (2015).
[15] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu.
2020. Dynabert: Dynamic bert with adaptive width and depth. arXiv preprint
arXiv:2004.04037 (2020).
[16] Forrest N Iandola, Albert E Shaw, Ravi Krishna, and Kurt W Keutzer. 2020.
SqueezeBERT: What can computer vision teach NLP about efficient neural net-
works? arXiv preprint arXiv:2006.11316 (2020).
[17] Shankar Iyer, Nikhil Dandekar, and Kornl Csernai. 2017. First Quora Dataset
Release: Question Pairs.(2017). URL https://data. quora. com/First-Quora-Dataset-
Release-Question-Pairs (2017).
[18] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,
and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.
arXiv preprint arXiv:1909.10351 (2019).
[19] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret.
2020. Transformers are rnns: Fast autoregressive transformers with linear atten-
tion. In International Conference on Machine Learning . PMLR, 5156â€“5165.
[20] Ashish Khetan and Zohar Karnin. 2020. schubert: Optimizing elements of bert.
arXiv preprint arXiv:2005.06628 (2020).
[21] Gyuwan Kim and Kyunghyun Cho. 2020. Length-Adaptive Transformer: Train
Once with Length Drop, Use Anytime with Search. arXiv preprint arXiv:2010.07003
(2020).
[22] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.
2021. I-BERT: Integer-only BERT Quantization. International conference on
machine learning (2021).
[23] Nikita Kitaev, Åukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient
transformer. arXiv preprint arXiv:2001.04451 (2020).
[24] FranÃ§ois Lagunas, Ella Charlaix, Victor Sanh, and Alexander M Rush. 2021. Block
pruning for faster transformers. arXiv preprint arXiv:2109.04838 (2021).
[25] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning
of language representations. arXiv preprint arXiv:1909.11942 (2019).
[26] Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji Li, Zhengang Li, Hang Liu,
and Caiwen Ding. 2020. Efficient transformer-based large scale language rep-
resentations using hardware-friendly block structured pruning. arXiv preprint
arXiv:2009.08065 (2020).
[27] Zi Lin, Jeremiah Zhe Liu, Zi Yang, Nan Hua, and Dan Roth. 2020. Pruning
Redundant Mappings in Transformer Models via Spectral-Normalized Identity
Prior. arXiv preprint arXiv:2010.01791 (2020).
[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[29] Zejian Liu, Fanrong Li, Gang Li, and Jian Cheng. 2021. EBERT: Efficient BERT
Inference with Dynamic Structured Pruning. In Findings of the Association for
Computational Linguistics: ACL-IJCNLP 2021 . 4814â€“4823.
[30] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really
better than one? arXiv preprint arXiv:1905.10650 (2019).
[31] Sai Prasanna, Anna Rogers, and Anna Rumshisky. 2020. When BERT plays the
lottery, all tickets are winning. arXiv preprint arXiv:2005.00561 (2020).
[32] Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train Short, Test Long: At-
tention with Linear Biases Enables Input Length Extrapolation. arXiv preprint
arXiv:2108.12409 (2021).
[33] Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you donâ€™t know:
Unanswerable questions for SQuAD. arXiv preprint arXiv:1806.03822 (2018).
[34] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.
SQuAD: 100,000+ questions for machine comprehension of text. arXiv preprint
arXiv:1606.05250 (2016).
[35] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Effi-
cient content-based sparse attention with routing transformers. Transactions of
the Association for Computational Linguistics 9 (2021), 53â€“68.
[36] Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. 2020. On the
Effect of Dropping Layers of Pre-trained Transformer Models. arXiv preprint
arXiv:2004.03844 (2020).
[37] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-
tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 (2019).
[38] Victor Sanh, Thomas Wolf, and Alexander M Rush. 2020. Movement pruning:
Adaptive sparsity by fine-tuning. arXiv preprint arXiv:2005.07683 (2020).
[39] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
Michael W Mahoney, and Kurt Keutzer. 2020. Q-BERT: Hessian Based Ultra
Low Precision Quantization of BERT.. In AAAI . 8815â€“8821.
[40] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Proceedings of the 2013 conference
on empirical methods in natural language processing . 1631â€“1642.
[41] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distilla-
tion for bert model compression. arXiv preprint arXiv:1908.09355 (2019).[42] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny
Zhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited devices.
arXiv preprint arXiv:2004.02984 (2020).
[43] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.
2019. Distilling task-specific knowledge from BERT into simple neural networks.
arXiv preprint arXiv:1903.12136 (2019).
[44] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse
sinkhorn attention. In International Conference on Machine Learning . PMLR, 9438â€“
9447.
[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems . 5998â€“6008.
[46] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019.
Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the
rest can be pruned. arXiv preprint arXiv:1905.09418 (2019).
[47] Apoorv Vyas, Angelos Katharopoulos, and FranÃ§ois Fleuret. 2020. Fast transform-
ers with clustered attention. Advances in Neural Information Processing Systems
33 (2020).
[48] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R
Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural
language understanding. arXiv preprint arXiv:1804.07461 (2018).
[49] Hanrui Wang, Zhekai Zhang, and Song Han. 2020. SpAtten: Efficient Sparse
Attention Architecture with Cascade Token and Head Pruning. arXiv preprint
arXiv:2012.09852 (2020).
[50] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer:
Self-Attention with Linear Complexity. arXiv preprint arXiv:2006.04768 (2020).
[51] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2019. Structured pruning of large
language models. arXiv preprint arXiv:1910.04732 (2019).
[52] Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage
challenge corpus for sentence understanding through inference. arXiv preprint
arXiv:1704.05426 (2017).
[53] Zhewei Yao, Linjian Ma, Sheng Shen, Kurt Keutzer, and Michael W Mahoney. 2021.
MLPruning: A Multilevel Structured Pruning Framework for Transformer-based
Models. arXiv preprint arXiv:2105.14636 (2021).
[54] Deming Ye, Yankai Lin, Yufei Huang, and Maosong Sun. 2021. TR-BERT: Dynamic
Token Reduction for Accelerating BERT Inference. arXiv preprint arXiv:2105.11618
(2021).
[55] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. 2020.
Gobo: Quantizing attention-based nlp models for low latency and energy ef-
ficient inference. In 2020 53rd Annual IEEE/ACM International Symposium on
Microarchitecture (MICRO) . IEEE, 811â€“824.
[56] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8BERT:
Quantized 8bit bert. arXiv preprint arXiv:1910.06188 (2019).
[57] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti,
Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al .2020.
Big bird: Transformers for longer sequences. arXiv preprint arXiv:2007.14062
(2020).
[58] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun
Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert. arXiv preprint
arXiv:2009.12812 (2020).
[59] Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich SchÃ¼tze. 2020. Masking
as an efficient alternative to finetuning for pretrained language models. arXiv
preprint arXiv:2004.12406 (2020).

--- PAGE 10 ---
KDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, Kurt Keutzer
A APPENDIX
A.1 Training Details
The training procedure of LTP consists of two separate stages: soft
pruning followed by hard pruning. For soft pruning, we train both
the model parameters and the thresholds on downstream tasks for
1 to 10 epochs, depending on the dataset size. We find it effective
to initialize the thresholds with linearly rising values as described
in 4.4 with a fixed threshold of the final layer. We search the optimal
temperature ğ‘‡in a search space of {1, 2, 5, 10, 20}e-4 and vary ğœ†
from 0.001 to 0.4 to control the number of tokens to be pruned (and
thus the FLOPs) for all experiments. We then fix the thresholds and
perform an additional training with the hard pruning to fine-tune
the model parameters only. More detailed hyperparameter settings
are listed in Table A.1 for GLUE and SQuAD 2.0.
SpAtten is trained based on the implementation details in the
paper: the first three layers retain all tokens and the remaining
layers are assigned with linearly decaying token retain ratio until
it reaches the final token retain ratio at the last layer. We vary the
final token retain ratio from 1.0 to -1.0 (prune all tokens for non-
positive retain ratios) to control the FLOPs of SpAtten. For both
LTP and SpAtten, we use learning rate of {0.5, 1, 2}e-5, except for
the soft pruning stage of LTP where we use 2e-5. We follow the
optimizer setting in RoBERTa [ 28] and use batch size of 64 for all
experiments.
LAT is trained using the same hyperparameter and optimizer
setting in the paper except for the length drop probabilities: for
more extensive search on more aggressive pruning configurations,
we used 0.25, 0.3, 0.35, and 0.4 for the length drop probability instead
of 0.2 in the original setting.
A.2 Computation Efficiency Comparison
Here we compare the efficiency of top- ğ‘˜versus threshold operation.
To do this, we use a batch size of 32 and average the latency over
1000 independent runs. For each sequence length, we test over five
different token retain ratios from 10% to 50% (e.g., 10% token retain
ratio is the case where we select top- ğ‘˜10% of tokens from the input
sequence).
With the above setting, we directly measure the latency of these
two operations on an Intel Haswell CPU, and report the results
in Figure A.1. For top- ğ‘˜operation, there is a noticeable increase
in latency when token retain ratios and sequence lengths become
larger whereas this is not an issue for our threshold pruning method
as it only requires a comparison operation. More importantly, top- ğ‘˜
operation incurs a huge latency overhead that is up to 7.4 Ã—and
33.4Ã—slower than threshold operation for sequence length of 128
and 1024, respectively.4
A.3 Discussion
A.3.1 Example Sequence Length Trajectories. Figure A.2 shows
how the pruned sequence length decreases for input sequences
of varying lengths. For LAT, the token pruning configuration is
fixed for all sequences in the dataset. In LTP, token pruning can
be more or less aggressive depending on the sequence content and
4The inefficiency of top- ğ‘˜is also further confirmed by [ 49], where they report only
1.1Ã—speedup for GPT-2 without the top- ğ‘˜hardware engine that they developed.Table A.1: Detailed hyperparameters for LTP training.
Stage Hyperparam GLUE SQuAD 2.0
Soft
pruningepochs 1 - 10 1
learning rate 2e-5 2e-5
ğ‘‡{1, 2, 5, 10, 20}e-4{1, 10}e-4
ğœ† 0.001 - 0.2 0.001 - 0.4
init. final thres. 0.01 0.003
Hard epochs 10 5
pruning lr {0.5, 1, 2}e-5 {0.5, 1, 2}e-5
the number of important tokens in the sequence. On average, LTP
calculates 25.86% fewer tokens per layer than LAT for MNLI-m and
12.08% fewer tokens for SST-2. For both LTP and LAT, the model
has been trained to produce a 1% drop in accuracy compared to
baseline.
A.3.2 Unbiased Token Pruning for Various Sequence Length. Figure
A.3 shows the distributions of initial sequence lengths for sequences
that are correctly classified and for sequences that are not. We see
that for multiple tasks, there is no significant correlation between
the length of the sequence and the accuracy of the pruned models.
Importantly, this suggests that our method is not biased towards
being more accurate on longer or shorter sequences.
A.4 Comparison with TR-BERT on GLUE
Unlike LAT and SpAtten, TR-BERT [ 54] does not report results on
the GLUE benchmark tasks described in the paper. We attempted to
run TR-BERT on the GLUE tasks using the TR-BERT repo5, but were
unable to get the algorithm to converge to a high accuracy, despite
varying the learning rate between 1e-6 and 1e-3 and the value of ğ›¼,
the parameter that defines the length penalty, over the search space
of{0.01,0.05,0.1,0.5,1,2,5}. We also varied the number of training
epochs based on the number of examples in each taskâ€™s training
set. The authors of TR-BERT note the convergence difficulties of
RL learning while describing the algorithm in their paper.
5https://github.com/thunlp/TR-BERT

--- PAGE 11 ---
Learned Token Pruning for Transformers KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.
Figure A.1: Wall-clock latency comparison between top- ğ‘˜operation and threshold operation on an Intel Haswell CPU for
different sequence length across various token retain ratios. Note that the latency of a threshold operation is independent of
sequence length.
2 4 6 8 10 12
Layer02040Pruned sequence
length
(a) SST-2
2 4 6 8 10 12
Layer0255075100Pruned sequence
length (b) MNLI-m
Figure A.2: Sample trajectories of pruned sequence length as the sequences are passed through model layers. For LTP, 20
samples were evenly selected from the sets after sorting by initial sequence length. (a) SST-2. (b) MNLI-m. The mean sequence
length for LTP is shown by a black dotted line, and the LAT baseline is shown by a black dashed line. Parameters were selected
so as to provide a 1% drop in accuracy from baseline for both methods.
0 20 40 60
Layer 10.000.020.04Density
0 20 40 60
Layer 40.000.020.04
0 20 40 60
Layer 70.000.020.04
0 20 40 60
Layer 100.000.050.10
(a) SST2
0 50 100
Layer 10.000.010.02Density
0 50 100
Layer 40.000.010.020.03
0 50 100
Layer 70.000.010.020.03
0 50 100
Layer 100.00.10.20.3
(b) MNLI-m
Figure A.3: Histogram of pruned sequence length (x-axis) as the input sequence is processed through different transformer
blocks. y-axis shows the relative count of sentences with the particular sequence length in x-axis. Green denotes input se-
quences that are correctly classified, and red denotes incorrect classifications.
