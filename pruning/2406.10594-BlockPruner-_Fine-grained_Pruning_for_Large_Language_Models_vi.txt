Thuật toán 1 Cắt tỉa khối lặp
Đầu vào: Mô hình M có L lớp, bộ dữ liệu hiệu chuẩn C, số khối cần loại bỏ K
Đầu ra: Mô hình đã cắt tỉa M*

1: M0 ← M
2: Chia mô hình M0 thành 2L khối
3: for j = 1 to K do
4:   for i = 1 to 2L - j + 1 do
5:     Tạo mô hình M̂ bằng cách che khối Bi;
6:     Tính toán độ bối rối Pi cho M̂ trên bộ dữ liệu hiệu chuẩn C;
7:   end for
8:   Sắp xếp các khối dựa trên độ bối rối của chúng;
9:   Loại bỏ khối có độ bối rối thấp nhất từ Mj−1 và có được Mj;
10: end for
11: M* ← MK
12: return Mô hình đã cắt tỉa M*

4 Thí nghiệm
Trong phần này, trước tiên chúng tôi giới thiệu các thiết lập thực nghiệm và sau đó trình bày kết quả chính.

4.1 Thiết lập thí nghiệm
Mô hình. Để xác thực hiệu quả rộng rãi của phương pháp cắt tỉa của chúng tôi, chúng tôi thí nghiệm với ba loạt mô hình: Llama2 (Touvron et al., 2023b), Baichuan2 (Yang et al., 2023), và Qwen1.5 (Bai et al., 2023). Các mô hình này có kiến trúc tương tự như được mô tả trong phương trình (1) và (2). Do hạn chế về tính toán, chúng tôi sử dụng các mô hình 7B và 13B cho Llama2 và Baichuan2, tương ứng, và các mô hình 7B và 14B cho Qwen1.5.

Baseline. Chúng tôi so sánh phương pháp của mình với một số phương pháp cắt tỉa có cấu trúc hiện đại. Các phương pháp baseline cụ thể bao gồm SliceGPT (Ashkboos et al., 2024), LaCo (Yang et al., 2024), ShortGPT (Men et al., 2024), và Relative Magnitude (Samragh et al., 2023; Men et al., 2024). SliceGPT đạt được cắt tỉa bằng cách loại bỏ các hàng hoặc cột tương ứng với các thành phần chính nhỏ hơn trong ma trận trọng số. LaCo hợp nhất các lớp mô hình từ sâu đến nông, sử dụng các biểu diễn đầu ra của mô hình để tính toán ngưỡng nhằm tránh hợp nhất quá mức. ShortGPT loại bỏ các lớp dư thừa bằng cách tính toán Block Influence. Relative Magnitude (RM) sử dụng ||f(x)/(x+f(x))|| như một chỉ số quan trọng cho các lớp, trong đó f(.) đại diện cho phần không dư thừa của lớp Transformer, và sử dụng cùng phương pháp cắt tỉa như ShortGPT. Đối với SliceGPT, chúng tôi đã sử dụng implementation chính thức. Đối với LaCo, chúng tôi đã triển khai nó dựa trên mã của họ và kiểm soát số lượng lớp đã cắt tỉa bằng cách điều chỉnh ngưỡng hợp nhất. Đối với ShortGPT và RM, chúng tôi đã tái tạo kết quả dựa trên bản thảo của họ. Thông tin triển khai chi tiết hơn được cung cấp trong Phụ lục A.

Dữ liệu và GPU. Trong thí nghiệm chính của chúng tôi, chúng tôi sử dụng bộ dữ liệu Alpaca (Taori et al., 2023) để tính toán điểm quan trọng. Đối với phương pháp của chúng tôi, chúng tôi chỉ sử dụng 256 mẫu để tính toán độ bối rối, và chúng tôi thảo luận về ảnh hưởng của việc thay đổi kích thước mẫu trong Phần 5.4. Để đảm bảo tính nhất quán, chúng tôi sử dụng cùng số lượng mẫu cho các phương pháp ShortGPT và Relative Magnitude như được hiển thị trong Phụ lục A. Hơn nữa, tác động của kích thước mẫu đối với ShortGPT và Relative Magnitude được chi tiết trong Phụ lục I. Tất cả các thí nghiệm được tiến hành trên hai GPU RTX 4090, và thời gian thực thi cho các phương pháp khác nhau được báo cáo trong Phụ lục G.

Đánh giá. Theo SliceGPT, chúng tôi sử dụng LM Evaluation Harness (Gao et al., 2023) để đánh giá và xác thực trên năm benchmark nổi tiếng: PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2021), HellaSwag (Zellers et al., 2019), ARC-e và ARC-c (Clark et al., 2018). Chúng tôi cũng sử dụng bộ dữ liệu Wikitext2 (Merity et al., 2016) để đánh giá độ bối rối sau khi cắt tỉa. Các chi tiết toàn diện hơn có thể được tìm thấy trong Phụ lục C.

4.2 Kết quả chính
Các nghiên cứu trước đây (Yang et al., 2024; Ashkboos et al., 2024) thường hạn chế tỷ lệ cắt tỉa ở khoảng 25%. Phù hợp với các nghiên cứu này, chúng tôi cũng hạn chế tỷ lệ cắt tỉa trong phạm vi này trong các thí nghiệm chính của chúng tôi. Vì việc đạt được tỷ lệ cắt tỉa giống hệt nhau trên các phương pháp và mô hình khác nhau là thách thức, chúng tôi chọn tỷ lệ cắt tỉa gần nhất có sẵn để so sánh.

Như được thể hiện trong Bảng 1, phương pháp BlockPruner của chúng tôi vượt trội đáng kể so với các baseline cắt tỉa có cấu trúc trước đây về hiệu suất trung bình và đạt được kết quả tốt nhất trên hầu hết các benchmark, mặc dù tỷ lệ cắt tỉa trong phương pháp của chúng tôi cao hơn một chút so với các baseline. Chúng tôi cũng quan sát thấy rằng Llama2-13B duy trì hiệu suất tốt hơn ở tỷ lệ cắt tỉa cao hơn so với Llama2-7B, với Baichuan2 và Qwen1.5 thể hiện hành vi tương tự. Điều này cho thấy rằng khi quy mô mô hình tăng lên, số lượng khối dư thừa cũng tăng theo, cho phép không gian cắt tỉa nhiều hơn.

Hơn nữa, đáng chú ý rằng các mô hình có độ bối rối thấp hơn trên bộ dữ liệu Wikitext2 có xu hướng hoạt động tốt hơn, điều này làm nổi bật mối tương quan giữa độ bối rối và hiệu quả mô hình. Điều này hỗ trợ thêm cho tính hợp lệ của độ bối rối như một chỉ số đáng tin cậy để đánh giá hiệu suất mô hình. Đáng chú ý, mặc dù phương pháp của chúng tôi thực hiện tìm kiếm cắt tỉa trên bộ dữ liệu Alpaca, nó đạt được độ bối rối thấp hơn trên bộ dữ liệu Wikitext2.

Cuối cùng, chúng tôi quan sát thấy rằng trong khi các phương pháp như ShortGPT và Relative Magnitude dẫn đến sự suy giảm đáng kể trong hiệu suất mô hình trên các nhiệm vụ khác nhau, BlockPruner nổi bật bằng cách tránh những sự giảm sút mạnh mẽ như vậy. Điều này cho thấy rằng phương pháp cắt tỉa khối được đề xuất của chúng tôi hiệu quả giảm thiểu suy giảm hiệu suất trong quá trình cắt tỉa. Do hạn chế về không gian, chúng tôi đã chuyển các chi tiết về baseline cắt tỉa và so sánh trên các tỷ lệ cắt tỉa khác nhau sang Phụ lục L.

Bảng 1: Hiệu suất nhiệm vụ hạ nguồn zero-shot của các mô hình khác nhau sử dụng các phương pháp cắt tỉa khác nhau. "Dense" đại diện cho các mô hình gốc, chưa cắt tỉa. "PPL" có nghĩa là độ bối rối trên Wikitext2.

[Bảng 1 với tất cả dữ liệu hiệu suất cho các mô hình Llama2-7B, Llama2-13B, Baichuan2-7B, Baichuan2-13B, Qwen1.5-7B, và Qwen1.5-14B]

5 Phân tích
5.1 Nghiên cứu loại bỏ
Để đánh giá ảnh hưởng của các hoạt động chính khác nhau trong thuật toán cắt tỉa được đề xuất đối với hiệu suất của nó, chúng tôi thực hiện một nghiên cứu loại bỏ toàn diện trên sáu mô hình. Cụ thể, trước tiên chúng tôi loại bỏ tất cả các khối có điểm quan trọng thấp nhất cùng một lúc, không có quy trình tìm kiếm lặp. Sau đó, chúng tôi thay thế việc cắt tỉa khối tinh vi bằng phương pháp cắt tỉa lớp thô hơn. Kết quả của các thí nghiệm này được hiển thị trong Bảng 2.

Bảng 2: Điểm trung bình của nghiên cứu loại bỏ BlockPruner trên các nhiệm vụ hạ nguồn. "- search" biểu thị việc bỏ quy trình tìm kiếm lặp và trực tiếp loại bỏ các khối có điểm quan trọng thấp nhất. "- block" có nghĩa là chúng tôi thay thế việc cắt tỉa khối tinh vi bằng phương pháp cắt tỉa lớp thô hơn.

Các phát hiện thực nghiệm làm nổi bật rằng việc chỉ dựa vào chỉ số độ bối rối mà không kết hợp thành phần tìm kiếm có thể dẫn đến kết quả cắt tỉa kém và thậm chí suy giảm hiệu suất. Hiện tượng này có thể bắt nguồn từ bản chất nội tại của độ bối rối, không giống như các chỉ số quan trọng khác chỉ tập trung vào ảnh hưởng khối cục bộ, được ảnh hưởng nội tại bởi sự tương tác giữa nhiều khối do sự phát sinh của nó từ tính toán đầu ra của mô hình. Trong khi độ bối rối hỗ trợ việc xác định các khối dư thừa trong mô hình, nó không trực tiếp tạo ra một chuỗi cắt tỉa tối ưu.

Hơn nữa, việc cắt tỉa ở cấp độ lớp thay vì cấp độ khối tạo ra hiệu suất kém mạnh mẽ hơn. Quan sát này cho thấy rằng mô hình chứa các dư thừa tinh vi, và việc phân đoạn các lớp thành các khối nhỏ hơn để cắt tỉa cho phép loại bỏ hiệu quả hơn sự dư thừa này, từ đó bảo tồn tốt hơn khả năng của mô hình. Ngoài ra, chúng tôi cung cấp các thí nghiệm loại bỏ ở mức độ thưa cao hơn, với kết quả được trình bày trong Phụ lục E.

5.2 Dư thừa giữa MHA và MLP
Để điều tra ý nghĩa và vai trò của các module MHA và MLP trong các LLM hiện đại, chúng tôi tiến hành các thí nghiệm cắt tỉa tập trung độc quyền vào các khối MHA hoặc MLP. Chúng tôi áp dụng chiến lược cắt tỉa này cho hai mô hình có kích thước khác nhau, Llama2-7B và Llama2-13B, trong khi giữ tỷ lệ cắt tỉa dưới 33%. Kết quả được minh họa trong Hình 4 cho thấy một số quan sát đáng chú ý.

Trước khi đạt tỷ lệ cắt tỉa 17%, việc cắt tỉa chỉ các khối MHA dẫn đến mất hiệu suất ít hơn so với việc cắt tỉa các khối MLP và thậm chí phù hợp với hiệu suất của việc cắt tỉa hỗn hợp. Điều này cho thấy rằng các module MHA trong LLM có thể sở hữu sự dư thừa lớn hơn so với dự kiến ban đầu, trong khi các module MLP tương đối ít dư thừa hơn.

Tuy nhiên, khi tỷ lệ cắt tỉa vượt quá 17%, việc cắt tỉa thêm các khối MHA dẫn đến sự suy giảm mạnh về hiệu suất. Xu hướng này cho thấy rằng khi việc cắt tỉa tiến triển, các khối MHA dư thừa được loại bỏ dần, chỉ để lại các khối MHA quan trọng. Hơn nữa, trong mô hình lớn hơn, sự suy giảm mạnh về hiệu suất xảy ra ở tỷ lệ cắt tỉa cao hơn, điều này phù hợp với phát hiện rằng các mô hình lớn hơn chứa nhiều khối dư thừa hơn. Sự dư thừa như vậy có thể bắt nguồn từ các yếu tố như huấn luyện không đủ, dẫn đến sự dư thừa ban đầu cao hơn.

Chúng tôi cũng kiểm tra tỷ lệ các khối MHA được loại bỏ trong quá trình cắt tỉa. Cụ thể, chúng tôi trình bày số lượng khối MHA và MLP được loại bỏ ở các giai đoạn cắt tỉa khác nhau. Trong Hình 5 (trái), chúng tôi đặt số lượng khối được loại bỏ thành 60. Trong Hình 5 (phải), các mô hình có 22 và 28 khối được loại bỏ, tương ứng, duy trì tỷ lệ cắt tỉa 30%.

Kết quả trong Hình 5 (trái) cho cả hai mô hình cho thấy xu hướng nhất quán ban đầu chỉ loại bỏ các khối MHA. Khi quá trình cắt tỉa tiến triển và nhiều khối hơn được loại bỏ, tỷ lệ các khối MHA được cắt tỉa theo xu hướng zigzag giảm dần. Đáng chú ý, đường cong cho Llama2-13B dịch chuyển sang phải so với Llama2-7B, cho thấy rằng mô hình lớn hơn chứa nhiều khối MHA dư thừa hơn. Điều này được nhấn mạnh thêm trong Hình 5 (phải), nơi, với cùng tỷ lệ cắt tỉa, Llama2-13B cắt tỉa nhiều khối MHA hơn Llama2-7B.

Ngoài ra, cho rằng phương pháp cắt tỉa của chúng tôi có xu hướng loại bỏ nhiều khối MHA hơn ở tỷ lệ cắt tỉa tương đương, nó có thể giảm đáng kể việc sử dụng bộ đệm key-value (KV) (Pope et al., 2023) trong MHA, có thể tăng tốc quá trình suy luận. Để xác thực điều này, chúng tôi cũng đã tiến hành so sánh tốc độ suy luận giữa các mô hình khác nhau thu được thông qua các phương pháp cắt tỉa khác nhau, với kết quả được chi tiết trong Phụ lục F.

5.3 Độ bối rối cho dư thừa khối
Trong phần này, chúng tôi khám phá tác động của các chỉ số quan trọng khối khác nhau. Nói chung, Block Influence (BI) và Relative Magnitude (RM) đo tầm quan trọng của một khối chỉ dựa trên các trạng thái ẩn đầu vào và đầu ra của nó, từ đó phản ánh ảnh hưởng cục bộ của khối. Ngược lại, độ bối rối được tạo ra từ các biểu diễn đầu ra của mô hình và do đó có thể đo lường tốt hơn ảnh hưởng tổng thể của một khối.

Tuy nhiên, như được chỉ ra trong nghiên cứu loại bỏ, việc sử dụng độ bối rối mà không có quy trình tìm kiếm lặp dẫn đến sự suy giảm đáng kể về hiệu suất. Điều này cho thấy rằng trong khi độ bối rối một mình có thể không phải là một chỉ số quan trọng khối mạnh mẽ, phương pháp tìm kiếm lặp của chúng tôi cho phép sử dụng hiệu quả hơn nó.

Như được minh họa trong Hình 6, khi BI và RM được áp dụng trong các thuật toán cắt tỉa động, đôi khi chúng đạt được hiệu suất có thể so sánh với độ bối rối ở tỷ lệ cắt tỉa thấp hơn. Tuy nhiên, khi tỷ lệ cắt tỉa tăng lên, những hạn chế của chúng trở nên rõ ràng, dẫn đến sự suy giảm mạnh trong hiệu suất mô hình. Điều này cho thấy rằng các chỉ số cục bộ này không nắm bắt đầy đủ tác động của các khối khác nhau đối với hiệu suất tổng thể của mô hình.

Tóm lại, độ bối rối tận dụng thông tin toàn cục để đo lường hiệu quả dư thừa khối, đặc biệt khi được sử dụng với chiến lược cắt tỉa động. Sự kết hợp này nắm bắt các tương tác phức tạp giữa các khối. Ngược lại, các chỉ số cục bộ như BI và RM hữu ích trong các tình huống cụ thể nhưng không phản ánh sự đóng góp tổng thể của các khối vào mô hình, đặc biệt ở tỷ lệ cắt tỉa cao hơn.

5.4 Tác động của dữ liệu đối với việc cắt tỉa
Trong công trình về SliceGPT (Ashkboos et al., 2024), các tác giả cũng sử dụng bộ dữ liệu Wikitext2 (Merity et al., 2016) và Alpaca (Taori et al., 2023) cho các thí nghiệm cắt tỉa. Họ quan sát thấy rằng bộ dữ liệu Alpaca thường mang lại kết quả cắt tỉa tốt hơn. Trong nghiên cứu của chúng tôi, chúng tôi có được những phát hiện tương tự. Như được thể hiện trong Hình 7 (trái), khi cắt tỉa Llama2-7B, hiệu suất trên các tỷ lệ cắt tỉa khác nhau cao hơn đáng kể khi sử dụng bộ dữ liệu Alpaca so với Wikitext2. Chúng tôi giả định rằng điều này có thể do bộ dữ liệu Alpaca là một bộ dữ liệu tuân theo hướng dẫn, phù hợp hơn với các nhiệm vụ hạ nguồn. Điều này cho thấy rằng việc lựa chọn bộ dữ liệu có tác động đáng kể đến hiệu suất cắt tỉa cuối cùng của mô hình.

Để xác định kích thước mẫu phù hợp và phân tích tác động của nó đối với hiệu suất cắt tỉa của BlockPruner, chúng tôi trích xuất số lượng thể hiện khác nhau từ bộ dữ liệu Alpaca và tiến hành các thí nghiệm cắt tỉa bằng Llama2-7B. Kết quả được trình bày trong Hình 7 (phải) cho thấy rằng việc tăng kích thước mẫu vượt quá 256 không mang lại cải thiện đáng kể nào trong hiệu quả cắt tỉa của BlockPruner. Do đó, chúng tôi đặt số lượng mẫu thành 256.

6 Kết luận
Trong công trình này, chúng tôi giới thiệu BlockPruner, một phương pháp cắt tỉa có cấu trúc mới để cắt tỉa hiệu quả các LLM. BlockPruner phân tách các lớp Transformer thành hai khối dư thừa tối thiểu và tận dụng một chỉ số quan trọng khối dựa trên độ bối rối kết hợp với thuật toán tìm kiếm cắt tỉa lặp, trong đó hai thành phần làm việc cùng nhau để loại bỏ dần các khối dư thừa. Các thí nghiệm rộng rãi trên các mô hình khác nhau cho thấy rằng phương pháp của chúng tôi vượt trội hơn các baseline khác về hiệu suất sau cắt tỉa. Những phát hiện của chúng tôi tiết lộ sự dư thừa khối tinh vi trong LLM, làm nổi bật sự khác biệt đáng kể về mức độ dư thừa giữa các loại khối khác nhau. Chúng tôi hy vọng công trình của chúng tôi đóng góp vào hiểu biết sâu sắc hơn về tầm quan trọng của các khối khác nhau trong LLM.

Hạn chế
Công trình hiện tại của chúng tôi có ba hạn chế tiềm năng. Thứ nhất, trong khi độ bối rối phục vụ như một chỉ báo hữu ích về tầm quan trọng của khối, nó có thể không phải là chỉ số tối ưu. Thứ hai, trong khi thuật toán tìm kiếm cắt tỉa được đề xuất của chúng tôi hiệu quả, các thuật toán tối ưu hóa tổ hợp khác có thể xác định các chuỗi cắt tỉa vượt trội. Cuối cùng, do hạn chế về tài nguyên tính toán, chúng tôi không áp dụng phương pháp của mình để cắt tỉa các mô hình lớn hơn. Tuy nhiên, phương pháp của chúng tôi có khả năng mở rộng cao và dễ dàng thích nghi để cắt tỉa các mô hình lớn hơn trong nghiên cứu tương lai.

Tuyên bố đạo đức
Mục đích của nghiên cứu này là cung cấp một phương pháp cắt tỉa tổng quát cho các mô hình ngôn ngữ lớn. Tất cả các mô hình và bộ dữ liệu được sử dụng trong các thí nghiệm của chúng tôi đều có thể truy cập công khai và không chứa bất kỳ thông tin riêng tư nào. Chúng tôi tuân thủ nghiêm ngặt các chính sách sử dụng của các tài nguyên này và sử dụng chúng chỉ cho mục đích nghiên cứu.

Lời cảm ơn
Công trình này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62176270) và Quỹ Nghiên cứu Cơ bản và Ứng dụng Cơ bản Quảng Đông (Số 2023A1515012832).

Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ từ trang 9-11]

A Chi tiết về triển khai
Trong phần này, chúng tôi chi tiết thiết lập thực nghiệm của mình. Chúng tôi đã lấy mẫu từ bộ dữ liệu Alpaca với seed ngẫu nhiên cố định là 42. Đối với SliceGPT, chúng tôi đã tuân theo cấu hình của bài báo gốc, sử dụng 1024 mẫu, tỷ lệ thưa được đặt ở 30%, và độ dài chuỗi tối đa là 2048. Đối với ShortGPT, RM, và BlockPruner, chúng tôi đã lấy mẫu 256 mẫu từ bộ dữ liệu, với cùng độ dài chuỗi tối đa là 2048. Đối với LaCo, chúng tôi đã điều chỉnh ngưỡng hợp nhất bằng cách sử dụng mã và dữ liệu được cung cấp để đạt được tỷ lệ cắt tỉa tương ứng.

B Chi tiết về bộ dữ liệu
B.1 Bộ dữ liệu cắt tỉa
Alpaca (Taori et al., 2023) là một bộ dữ liệu tuân theo hướng dẫn tổng quát chứa 52,000 câu hỏi. Mỗi mẫu bao gồm ba trường: instruction, input, và response. Chúng tôi đã chọn 10% của bộ dữ liệu và sử dụng 256 mẫu cho các thí nghiệm chính. Tính toán độ bối rối được thực hiện đồng nhất trên tất cả văn bản trong các mẫu mà không phân biệt giữa các trường.

B.2 Bộ dữ liệu đánh giá
Tất cả các bộ dữ liệu nhiệm vụ hạ nguồn được phân vùng và đánh giá bằng cách sử dụng cấu hình mặc định của LM Evaluation Harness.

Wikitext-2 (Merity et al., 2016) là một tập hợp hơn 100 triệu token được trích xuất từ các bài viết Good và Featured đã được xác minh trên Wikipedia. Bộ dữ liệu này thường được sử dụng để đo lường chất lượng tạo văn bản của mô hình. Chúng tôi đã sử dụng các mẫu từ tập test đã được chia sẵn để tính toán độ bối rối.

PIQA (Bisk et al., 2020) là một bộ dữ liệu được thiết kế để đánh giá hiểu biết của các mô hình ngôn ngữ tự nhiên về thông thức vật lý. Nó sử dụng định dạng lựa chọn đa phương án nơi mô hình chọn giải pháp phù hợp nhất từ hai tùy chọn cho một mục tiêu.

WinoGrande (Sakaguchi et al., 2021) là một bộ dữ liệu rộng lớn để đánh giá khả năng lý luận thông thức của mô hình. Nó bao gồm 44,000 câu hỏi. Bộ dữ liệu có các nhiệm vụ điền vào chỗ trống với các tùy chọn nhị phân, nhằm chọn tùy chọn đúng cho một câu nhất định đòi hỏi lý luận thông thức.

HellaSwag (Zellers et al., 2019) cũng là một bộ dữ liệu được thiết kế để đánh giá khả năng lý luận thông thức của mô hình, cụ thể là để làm nổi bật những hạn chế của các mô hình hiện tại trong việc xử lý các nhiệm vụ lý luận ngôn ngữ tự nhiên thông thức. Mặc dù tầm thường đối với con người (với độ chính xác >95%), bộ dữ liệu trình bày những khó khăn đáng kể cho các mô hình. Việc đánh giá được tiến hành bằng cách sử dụng các câu hỏi lựa chọn bốn phương án.

ARC (Clark et al., 2018) bộ dữ liệu bao gồm 7,787 câu hỏi khoa học lựa chọn đa phương án được lấy từ các nguồn khác nhau. Mỗi câu hỏi thường cung cấp bốn tùy chọn câu trả lời. Các câu hỏi này được phân loại thành hai tập khó khăn riêng biệt: 2,590 câu hỏi cho Challenge Set và 5,197 cho Easy Set.

C Chi tiết về đánh giá
Đảm bảo so sánh công bằng và toàn diện, chúng tôi đã sử dụng cùng phiên bản LM Evaluation Harness như được sử dụng trong các thí nghiệm SliceGPT, thu được điểm đánh giá dưới các cấu hình thí nghiệm giống hệt nhau. Những điểm này khớp chặt chẽ với những điểm được báo cáo trong bài báo SliceGPT, như được chi tiết trong Bảng 3. Để nhất quán, chúng tôi trình bày kết quả tái tạo của mình trong các thí nghiệm chính.

Bảng 3: So sánh hiệu suất trung bình trên các nhiệm vụ hạ nguồn giữa kết quả SliceGPT chính thức và kết quả tái tạo của chúng tôi (được chỉ ra bằng "*" cho kết quả của chúng tôi).

Để đánh giá hiệu suất của các mô hình đã cắt tỉa trên các nhiệm vụ hạ nguồn, chúng tôi đã sử dụng năm bộ dữ liệu QA lựa chọn đa phương án: PIQA, WinoGrande, HellaSwag, ARC-e, và ARC-c. Ngoài ra, để đánh giá chất lượng tạo văn bản, chúng tôi đã tính toán độ bối rối bằng cách sử dụng tập test của bộ dữ liệu Wikitext2. Đối với các đánh giá nhiệm vụ hạ nguồn, chúng tôi đã tuân thủ các tham số đánh giá mặc định và thiết lập zero-shot, với kích thước batch được đặt thành 1. Đối với tính toán độ bối rối, độ dài văn bản tối đa được đặt thành 2048, duy trì kích thước batch là 1.

D Độ bối rối và JS Divergence trong đánh giá khối
Công trình gần đây như FINERCUT (Zhang et al., 2024a) đề xuất một thuật toán cắt tỉa tinh vi đánh giá tầm quan trọng của khối bằng cách sử dụng JS divergence giữa các phân phối đầu ra của mô hình gốc và đã cắt tỉa. Trong khi chỉ số này nắm bắt những thay đổi phân phối, nó bỏ qua tính trôi chảy và mạch lạc ngữ nghĩa của văn bản được tạo ra - những khía cạnh chính để duy trì tính hữu dụng thực tế của LLM.

Ngược lại, chúng tôi áp dụng độ bối rối (PPL) như một chỉ số quan trọng toàn cục được rút ra từ log-likelihood cấp độ chuỗi, phản ánh trực tiếp hơn cách việc cắt tỉa ảnh hưởng đến chất lượng đầu ra và tính trôi chảy trong các mô hình.

Để xác thực quan điểm này, chúng tôi đã tiến hành các thí nghiệm sử dụng cả hai chỉ số trên các quy mô mô hình và tỷ lệ cắt tỉa khác nhau. Kết quả, được tóm tắt trong Bảng 4, cho thấy rằng PPL nhất quán vượt trội hơn JS divergence dưới các cấu hình khác nhau. Những phát hiện này chứng minh rằng PPL phản ánh tốt hơn tính trôi chảy và chất lượng của các đầu ra của mô hình đã cắt tỉa, củng cố sự phù hợp của nó như một chỉ số quan trọng khối cho việc cắt tỉa LLM.

Bảng 4: So sánh PPL và JS divergence trên các tỷ lệ cắt tỉa và quy mô mô hình khác nhau.

E Nghiên cứu loại bỏ ở độ thưa cao hơn
BlockPruner được thúc đẩy bởi mục tiêu bảo tồn hiệu suất mô hình hiệu quả hơn thông qua việc cắt tỉa khối tinh vi. Việc đánh giá cách cắt tỉa khối hoạt động ở các mức độ chi tiết khác nhau, đặc biệt dưới độ thưa cao hơn, là quan trọng để hỗ trợ động cơ và tuyên bố của chúng tôi. Để làm rõ điều này, chúng tôi đã tiến hành các thí nghiệm loại bỏ với tỷ lệ thưa cao hơn trên các mô hình Llama2-7B và Llama2-13B. Kết quả, được hiển thị trong Bảng 5, xác nhận rằng phương pháp của chúng tôi vẫn hiệu quả, xác thực thêm động cơ đằng sau BlockPruner.

Bảng 5: Điểm trung bình của BlockPruner ở các độ chi tiết cắt tỉa khác nhau dưới độ thưa cao hơn.

F Tốc độ suy luận sau khi cắt tỉa
Trong phần này, chúng tôi đánh giá tốc độ suy luận bằng cách đo thời gian cần thiết để tạo ra 128 token bằng cách sử dụng các mô hình thu được từ các phương pháp cắt tỉa khác nhau, tất cả đều sử dụng KV cache để giải mã hiệu quả. Mỗi cấu hình được lặp lại 20 lần để đảm bảo kết quả mạnh mẽ về mặt thống kê, và chúng tôi báo cáo thời gian suy luận trung bình qua các lần chạy.

Như được hiển thị trong Bảng 6, phương pháp của chúng tôi nhất quán đạt được tăng tốc lớn nhất ở tỷ lệ cắt tỉa có thể so sánh. Cải thiện này bắt nguồn từ thực tế là phương pháp của chúng tôi cắt tỉa tỷ lệ lớn hơn các khối MHA ở cùng tỷ lệ cắt tỉa tổng thể so với các phương pháp khác, dẫn đến giảm đáng kể việc sử dụng KV cache, trực tiếp tăng tốc quá trình suy luận của LLM.

Bảng 6: Sự khác biệt về tốc độ suy luận giữa các mô hình thu được bằng các phương pháp cắt tỉa khác nhau, trong đó "Original" biểu thị mô hình chưa cắt tỉa.

G Chi phí thời gian của các phương pháp cắt tỉa
Phương pháp của chúng tôi dựa vào PPL để xác định tầm quan trọng của khối, đòi hỏi tính toán PPL trước khi cắt tỉa, khiến việc thiết kế chiến lược cắt tỉa hiệu quả hơn trở nên thách thức. Chúng tôi đã so sánh các chỉ số quan trọng khối khác (trong Phần 5.3) nhưng thấy rằng PPL vẫn bảo tồn hiệu suất mô hình tốt nhất. Hơn nữa, vì phương pháp của chúng tôi duy trì tốt hơn hiệu suất mô hình và cắt tỉa là một lần mà không tăng chi phí suy luận tiếp theo, nên chúng tôi tin rằng sự đánh đổi là đáng giá. Kết quả so sánh thời gian cắt tỉa giữa BlockPruner và các phương pháp khác được trình bày trong Bảng 7.

Bảng 7: Thời gian thực thi của BlockPruner và các phương pháp cắt tỉa khác trong thí nghiệm chính.

H Huấn luyện sau cắt tỉa
Chúng tôi đã lấy mẫu 8,000 thể hiện từ bộ dữ liệu Alpaca và tiến hành huấn luyện sau trên các mô hình Llama2-7B và Llama2-13B đã cắt tỉa thu được qua BlockPruner bằng LoRA. Tất cả các lớp tuyến tính, loại trừ lớp embedding và đầu mô hình ngôn ngữ, được huấn luyện. Các tham số LoRA rank và LoRA α được đặt thành 32 và 10, tương ứng, với tốc độ học 2e-4 và kích thước batch là 1. Ngoài ra, chúng tôi đã cấu hình các bước tích lũy gradient thành 4 và sử dụng bộ lập lịch tốc độ học tuyến tính. Chúng tôi kiểm soát tỷ lệ cắt tỉa trong phạm vi từ 24% đến 33%. Kết quả được hiển thị trong Hình 8. Có thể thấy rằng sau khi huấn luyện, các mô hình của chúng tôi cho thấy cải thiện thêm ở các tỷ lệ cắt tỉa khác nhau. Các mô hình Llama2-7B và Llama2-13B đã khôi phục đến 89% và 92% hiệu suất của các mô hình chưa cắt tỉa, tương ứng, khi được cắt tỉa khoảng 1/4.

I Độ nhạy với kích thước mẫu
ShortGPT sử dụng Block Influence như chỉ số quan trọng cho các lớp, trong khi RM sử dụng Relative Magnitude. Cái trước tính toán sự tương tự giữa các trạng thái ẩn đầu vào và đầu ra của một lớp, trong khi cái sau sử dụng đầu vào và phần không dư thừa của đầu ra. Trong các thí nghiệm sơ bộ của chúng tôi, chúng tôi thấy rằng hai chỉ số này không nhạy cảm với kích thước mẫu. Chúng tôi đã lấy mẫu số lượng thể hiện khác nhau từ tập test của bộ dữ liệu Alpaca để quan sát tác động của chúng đối với các chỉ số này, và kết quả được hiển thị trong Hình 9. Chúng ta có thể thấy rằng tất cả các đường gần như trùng nhau, cho thấy rằng Block Influence và Relative Magnitude không nhạy cảm với kích thước mẫu. Chúng tôi suy đoán rằng điều này có thể do thông tin hạn chế được cung cấp bởi những thay đổi trong đầu vào và đầu ra của một lớp duy nhất.

J Giữ lại khả năng lý luận sau khi cắt tỉa
Để đánh giá thêm hiệu quả của BlockPruner trong các tình huống đòi hỏi lý luận cao, chúng tôi đánh giá hiệu suất zero-shot của nó trên bộ dữ liệu AQuA-RAT (Ling et al., 2017), một benchmark được thiết kế cho việc giải quyết bài toán từ đại số với lý luận. Chúng tôi so sánh BlockPruner với một số baseline cắt tỉa mạnh bằng cách sử dụng các mô hình LLaMA2-7B và LLaMA2-13B.

Như được tóm tắt trong Bảng 8, BlockPruner nhất quán duy trì hiệu suất cạnh tranh, đạt được độ chính xác gần với các mô hình chưa cắt tỉa và vượt trội hơn các chiến lược cắt tỉa thay thế như ShortGPT, Relative Magnitude (RM), và LaCo. Những kết quả này chứng minh rằng phương pháp của chúng tôi bảo tồn khả năng lý luận của LLM ngay cả dưới nén cấu trúc đáng kể.

Bảng 8: Độ chính xác zero-shot (%) trên AQuA-RAT cho các mô hình được cắt tỉa bằng các phương pháp khác nhau. Tỷ lệ cắt tỉa cho tất cả các phương pháp phù hợp với những tỷ lệ được sử dụng trong các thí nghiệm chính.

K Tổng quát hóa cho các loạt mô hình mới
Để đánh giá khả năng tổng quát hóa của BlockPruner cho các loạt mô hình mới được phát hành, chúng tôi tiến hành các thí nghiệm bổ sung trên hai kiến trúc gần đây: Mistral-7B-v0.3 (Jiang et al., 2023) và LLaMA3-8B (Grattafiori et al., 2024). Chúng tôi so sánh phương pháp của mình với ShortGPT và Relative Magnitude (RM) trên năm tỷ lệ cắt tỉa, báo cáo điểm nhiệm vụ hạ nguồn trung bình trên mỗi loại.

Như được hiển thị trong Bảng 9, BlockPruner nhất quán đạt được hiệu suất vượt trội trên cả hai họ mô hình trên tất cả các mức độ thưa. Khoảng cách hiệu suất trở nên rõ rệt hơn khi tỷ lệ cắt tỉa tăng lên, cho thấy rằng phương pháp cắt tỉa tinh vi của chúng tôi đặc biệt mạnh mẽ dưới các chế độ nén cao. Những kết quả này xác nhận hiệu quả và khả năng thích ứng của BlockPruner ngoài các mô hình được đánh giá trong bài báo chính.

Bảng 9: Hiệu suất nhiệm vụ hạ nguồn trên các họ mô hình mới được phát hành. BlockPruner nhất quán duy trì hiệu suất mạnh hơn trên các tỷ lệ cắt tỉa.

L Các tỷ lệ cắt tỉa khác nhau
Để chứng minh rộng rãi sự vượt trội của phương pháp của chúng tôi, chúng tôi trình bày hiệu quả cắt tỉa của BlockPruner, ShortGPT, và Relative Magnitude trên sáu mô hình lớn đại diện ở các tỷ lệ cắt tỉa khác nhau. Như được mô tả trong Hình 10, phương pháp của chúng tôi hiệu quả giảm thiểu mất hiệu suất trong suốt quá trình cắt tỉa, tránh bất kỳ sự sụt giảm đột ngột nào về hiệu suất. Ngược lại, RM thể hiện sự bất ổn đáng kể và dễ bị sụp đổ hiệu suất. ShortGPT hoạt động tương đối tốt, nhưng trong các thí nghiệm cắt tỉa trên Qwen1.5-14B, nó cũng dẫn đến suy giảm hiệu suất nghiêm trọng ở tỷ lệ cắt tỉa cao hơn. Nhìn chung, lợi thế của phương pháp của chúng tôi trở nên rõ rệt hơn khi cả kích thước mô hình và tỷ lệ cắt tỉa tăng lên.

M Đánh giá trên các bộ dữ liệu bổ sung
Chúng tôi đã mở rộng thí nghiệm chính bằng cách kết hợp bốn bộ dữ liệu đánh giá đã được thiết lập thêm: SWAG (Zellers et al., 2018), TruthfulQA (Lin et al., 2022), OpenBookQA (Mihaylov et al., 2018), và RACE (Lai et al., 2017). Như được minh họa trong Bảng 10, phương pháp được đề xuất của chúng tôi nhất quán vượt trội hơn các baseline cắt tỉa trước đây trên phạm vi rộng hơn các bộ dữ liệu này, chứng minh thêm hiệu quả và khả năng tổng quát hóa của nó.

Bảng 10: Hiệu suất nhiệm vụ hạ nguồn zero-shot của các mô hình khác nhau sử dụng các phương pháp cắt tỉa khác nhau. "Dense" biểu thị các mô hình gốc, chưa cắt tỉa. Tất cả các đánh giá được tiến hành với cùng cấu hình để đảm bảo tính so sánh.
