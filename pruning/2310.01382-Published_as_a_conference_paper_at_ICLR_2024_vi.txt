NÉN LLM: SỰ THẬT HIẾM KHI THUẦN KHIẾT
VÀ KHÔNG BAO GIỜ ĐƠN GIẢN

Ajay Jaiswal1, Zhe Gan2, Xianzhi Du2, Bowen Zhang2, Zhangyang Wang1, Yinfei Yang2
1Đại học Texas tại Austin, 2Apple

TÓM TẮT

Mặc dù đạt được những thành tựu đáng chú ý, các Mô hình Ngôn ngữ Lớn (LLM) hiện đại đối mặt với chi phí tính toán và bộ nhớ cực kỳ lớn. Gần đây, một số nghiên cứu đã cho thấy thành công đáng kể trong việc nén không cần huấn luyện và không cần dữ liệu (tỉa và lượng tử hóa) của LLM đạt được độ thưa 50-60% và giảm độ rộng bit xuống 3 hoặc 4 bit trên mỗi trọng số, với sự suy giảm độ phức tạp không đáng kể so với đường cơ sở không nén. Khi các nỗ lực nghiên cứu gần đây tập trung vào việc phát triển các phương pháp nén ngày càng tinh vi, công trình của chúng tôi lùi lại một bước và đánh giá lại hiệu quả của các phương pháp nén SoTA hiện có, dựa trên một chỉ số khá đơn giản và bị nghi ngờ rộng rãi là độ phức tạp (ngay cả đối với LLM dày đặc). Chúng tôi giới thiệu Bộ Đánh giá LLM Nén Chuyên sâu Kiến thức (LLM-KICK), một bộ sưu tập các nhiệm vụ được tuyển chọn cẩn thận để định nghĩa lại giao thức đánh giá cho LLM nén, có sự tương đồng đáng kể với các đối tác dày đặc của chúng và độ phức tạp không thể nắm bắt được những thay đổi tinh tế trong khả năng thực sự của chúng. LLM-KICK tiết lộ nhiều ưu điểm thuận lợi và khó khăn đáng tiếc của các phương pháp nén SoTA hiện tại: tất cả các phương pháp tỉa đều bị suy giảm hiệu suất đáng kể, đôi khi ở tỷ lệ thưa tầm thường (ví dụ: 25-30%), và thất bại đối với độ thưa N:M trong các nhiệm vụ chuyên sâu kiến thức; các phương pháp lượng tử hóa hiện tại thành công hơn so với tỉa; tuy nhiên, LLM được tỉa ngay cả ở độ thưa >=50% vẫn là hệ thống truy xuất và tóm tắt trong ngữ cảnh mạnh mẽ; cùng nhiều điều khác. LLM-KICK được thiết kế để đánh giá toàn diện khả năng hiểu ngôn ngữ, lý luận, tạo sinh, truy xuất trong ngữ cảnh, tóm tắt trong ngữ cảnh, v.v. của LLM nén. Chúng tôi hy vọng nghiên cứu này có thể thúc đẩy việc phát triển các phương pháp nén LLM tốt hơn. Mã được tái tạo có sẵn tại https://github.com/VITA-Group/llm-kick.

1 GIỚI THIỆU

Các Mô hình Ngôn ngữ Lớn (LLM) có mặt khắp nơi, không chỉ ảnh hưởng sâu sắc đến cảnh quan NLP (Ram et al., 2023; Liu et al., 2023a; Sawada et al., 2023; Qin et al., 2023; Zhuo, 2023; Lee et al., 2023), mà gần đây còn hỗ trợ nhiều thuật toán thị giác máy tính (Lian et al., 2023; Wang et al., 2023; Lai et al., 2023; Lu et al., 2023) và mạng nơ-ron đồ thị (Ye et al., 2023; Chen et al., 2023; Qian et al., 2023; Duan et al., 2023); đạt được hiệu suất xuất sắc trên nhiều bảng xếp hạng nhiệm vụ khác nhau. Mặc dù có nhiều khả năng chưa từng có, việc dân chủ hóa chúng chủ yếu bị hạn chế bởi sự hiện diện của hàng tỷ tham số, phụ thuộc vào yêu cầu tính toán và bộ nhớ cực kỳ cao. Ví dụ, GPT-175B yêu cầu 325 GB bộ nhớ GPU chỉ để tải trọng số mô hình và ít nhất năm GPU A100 (80GB) với các kỹ thuật song song hóa tinh vi (Sheng et al., 2023).

Để dân chủ hóa LLM, những nỗ lực đáng kể đã được thực hiện để giảm thiểu chi phí tính toán cao của chúng, chủ yếu chia thành hai hướng nghiên cứu: tỉa mạng và lượng tử hóa trọng số. Phương pháp đầu tiên thu nhỏ kích thước mạng bằng cách loại bỏ các trọng số cụ thể khỏi mô hình - về cơ bản đặt chúng bằng không, trong khi phương pháp thứ hai nhằm lượng tử hóa các tham số thành các biểu diễn bit thấp hơn. Một số thành công gần đây trong tỉa mạng (Sun et al., 2023; Frantar & Alistarh, 2023; Jaiswal et al., 2023a; Ma et al., 2023; Ji et al., 2023) và lượng tử hóa (Liu et al., 2023c; Kim et al., 2023; Dettmers et al., 2023a; Frantar et al., 2022; Lin et al., 2023a; Dettmers et al., 2023c) (thảo luận chi tiết về công trình liên quan trong Phụ lục A.1) tuyên bố giữ lại hiệu suất của LLM không nén trong khi đạt được độ thưa 50-60% hoặc lượng tử hóa cực độ 2-3 bit. Mặc dù những tiến bộ này trông hấp dẫn, trong hầu hết (nếu không phải tất cả) các trường hợp, chúng dựa rất nhiều vào độ phức tạp như chỉ số chính để đánh giá các tuyên bố hiệu suất. Những đánh giá tương đối hạn chế như vậy giới hạn phạm vi phát triển các phương pháp nén mới và có thể không phù hợp để xác định các khả năng/hạn chế mới và bất ngờ của LLM nén.

Độ phức tạp, ngay cả trong trường hợp LLM dày đặc, đã bị nghi ngờ là một thước đo không thỏa đáng để so sánh tiềm năng thực sự của LLM, mặc dù có sự biến đổi đáng kể về quy mô mô hình, chiến lược huấn luyện và lựa chọn kiến trúc (Muhlgay et al., 2023). Điều quan trọng cần lưu ý là tất cả các mô hình nén đều được dẫn xuất từ cùng một đối tác dày đặc với độ tương đồng cao, và những khác biệt nêu trên không tồn tại, khiến việc đánh giá chúng trở nên thách thức hơn. Trong công trình này, chúng tôi xem xét lại một câu hỏi được biết đến rộng rãi nhưng chưa được khám phá đầy đủ: Độ phức tạp nắm bắt tốt như thế nào sự thay đổi trong khả năng của LLM nén có sự tương đồng đáng kể với đối tác dày đặc của chúng? Chúng tôi tập trung vào trường hợp LLM nén, vì chúng tôi quan sát thấy sự thất bại nghiêm trọng hơn tương đối của độ phức tạp trong việc nắm bắt các biến đổi hiệu suất tinh tế phát sinh qua các giai đoạn nén khác nhau của LLM, đòi hỏi một cuộc điều tra chi tiết hơn.

Trong công trình này, chúng tôi cố gắng điều tra những hứa hẹn thực sự và những hạn chế của các thuật toán nén tiên tiến cho LLM. Chúng tôi tập hợp bộ sưu tập đầu tiên toàn diện và đa dạng của các nhiệm vụ với các mức độ khó khác nhau để nghiên cứu kỹ lưỡng LLM nén dưới lượng tử hóa và tỉa mạng (các mẫu thưa có cấu trúc và không có cấu trúc). Cụ thể hơn, chúng tôi xem xét một loạt rộng các nhiệm vụ để đánh giá những thay đổi tinh tế trong khả năng hiểu ngôn ngữ, lý luận, tạo sinh, truy xuất trong ngữ cảnh, tóm tắt ngữ cảnh dài, v.v. của LLM được tỉa và lượng tử hóa. Lưu ý rằng không có bộ dữ liệu nào trong nghiên cứu đa chiều của chúng tôi về LLM nén được tạo từ đầu, mà chúng tôi dựa vào các bộ dữ liệu hiện có vì chúng đã được các nhà nghiên cứu chấp nhận rộng rãi, nhưng thật không may chưa được áp dụng để nghiên cứu tác động của nén. Chúng tôi đo lường chặt chẽ hiệu suất của các phương pháp lượng tử hóa và tỉa SoTA (trong các cài đặt phổ biến, mặc định nhất của chúng), để hiểu tiềm năng của chúng cho các nhiệm vụ thách thức và thú vị với giá trị thực tế cao.

Các quan sát và đóng góp chính của chúng tôi có thể được triển khai như sau:

• Chúng tôi trình bày Bộ Đánh giá LLM Nén Chuyên sâu Kiến thức (LLM-KICK), để định nghĩa lại các giao thức đánh giá cho LLM nén và tạo điều kiện cho việc đánh giá toàn diện các thuật toán nén SoTA. Tiền đề của công trình chúng tôi là phát triển một bộ các nhiệm vụ thách thức, thực tế và đa dạng có tầm quan trọng thực tế cao và các bộ dữ liệu có thể trao quyền cho việc hiểu có hệ thống về cách các chiến lược nén LLM hiện có thực sự hoạt động trong việc duy trì hiệu suất mặc dù có độ phức tạp tương tự, chúng khác nhau như thế nào với nhau, và chúng so sánh như thế nào với các LLM nhỏ hơn có số lượng tham số tương đương.

• LLM-KICK tiết lộ nhiều quan sát thú vị và quan trọng mà các đánh giá dựa trên độ phức tạp bỏ qua. 1Hầu hết các phương pháp tỉa SoTA bị suy giảm hiệu suất đáng kể, đôi khi ở tỷ lệ thưa tầm thường (ví dụ: 25-30%), mặc dù có những thay đổi không đáng kể trong độ phức tạp. 2Tất cả các phương pháp tỉa SoTA đều không hoạt động thỏa đáng cho các mẫu thưa có cấu trúc N:M trên LLM-KICK. 3Các phương pháp lượng tử hóa LLM SoTA hiện tại thành công hơn trong việc duy trì hiệu suất so với các phương pháp tỉa LLM SoTA. 4LLM nén thất bại trong việc tạo ra câu trả lời giàu kiến thức và đúng về mặt sự thật, mặc dù văn bản được tạo ra là trôi chảy, nhất quán và mạch lạc. 5LLM nén với kiến trúc lớn hơn nhưng cùng số lượng tham số hoạt động kém hơn, điều này ủng hộ các mô hình dày đặc nhỏ hơn.

• Chúng tôi tiếp tục điều tra khả năng của LLM nén cho các thiết lập trong ngữ cảnh, thông qua việc áp dụng hỏi đáp tăng cường truy xuất trong ngữ cảnh (ICRA-QA) (Ram et al., 2023), và tóm tắt văn bản với học tập trong ngữ cảnh (IC-Sum) (Jain et al., 2023). Thật ngạc nhiên, LLM được tỉa, ngay cả ở tỷ lệ thưa không tầm thường (ví dụ: >=50%), là hệ thống truy xuất mạnh mẽ và có thể thực hiện tóm tắt văn bản trong khi duy trì hiệu suất tương tự như đối tác dày đặc của chúng. Tuy nhiên, với mức độ nén tăng, khả năng tiêu hóa ngữ cảnh dài hơn của chúng bị ảnh hưởng nhiều hơn so với ngữ cảnh nhỏ hơn.

2 NÉN LLM SOTA: ĐỘ PHỨC TẠP, HAY CÒN GÌ KHÁC?

Mở rộng quy mô mạng nơ-ron, hiện nay là LLM, đã đạt được lợi ích hiệu suất đáng kinh ngạc trên một loạt rộng các nhiệm vụ, nhưng với cái giá là dấu chân tính toán và bộ nhớ khổng lồ. Tỉa mạng và lượng tử hóa trọng số là hai biện pháp khắc phục phổ biến để giảm thiểu những chi phí này do số lượng tham số hàng tỷ trong LLM hiện tại. Mặc dù có nhiều thuật toán hiện có cho tỉa (Singh & Alistarh, 2020; Zhu & Gupta, 2017; Gale et al., 2019; Jaiswal et al., 2022; Lin et al., 2020; Liu et al., 2021a; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Evci et al., 2020) và lượng tử hóa (Dong et al., 2022; Cardinaux et al., 2020; Kim et al., 2021; Liu et al., 2021b; Martinez et al., 2020), việc áp dụng đặc biệt của chúng cho LLM bị hạn chế, do thiếu sự xa xỉ để thực hiện huấn luyện lại lặp đi lặp lại để khôi phục bất kỳ sự sụt giảm hiệu suất nào trong quá trình nén. Gần đây, một số công trình đã cho thấy thành công đáng kể trong nén LLM không cần huấn luyện và không cần dữ liệu đạt được độ thưa 50-60% và giảm độ rộng bit xuống 3 hoặc 4 bit trên mỗi trọng số, với sự suy giảm độ phức tạp không đáng kể so với đường cơ sở không nén.

Độ phức tạp là một thước đo thống kê về mức độ tin tưởng của mô hình ngôn ngữ dự đoán một mẫu văn bản và lượng hóa "sự bất ngờ" được mã hóa trong các mô hình ngôn ngữ (độ phức tạp càng thấp, mô hình càng tốt). Mặc dù phổ biến, độ phức tạp đã bị nghi ngờ rộng rãi là một thước đo không thỏa đáng để so sánh các ưu điểm thực sự của hai LLM khác nhau (Muhlgay et al., 2023), ngay cả đối với các mô hình dày đặc mặc dù chúng khác nhau đáng kể về quy mô mô hình, chiến lược huấn luyện và lựa chọn thiết kế (chỉ bộ mã hóa, chỉ bộ giải mã, v.v.). Để giải quyết vấn đề này, một số công trình (Li et al., 2023; Kaddour et al., 2023; Muhlgay et al., 2023; Zhang et al., 2023; Valmeekam et al., 2022; Liu et al., 2023a; Sawada et al., 2023; Qin et al., 2023; Zhuo, 2023; Lee et al., 2023) cố gắng vượt ra ngoài độ phức tạp và đánh giá khả năng của LLM dày đặc trên lý luận thông thường, hiểu ngôn ngữ, đọc hiểu, lập trình, v.v. Tuy nhiên, điều quan trọng cần lưu ý là tất cả các mô hình nén đều được dẫn xuất từ cùng một đối tác dày đặc với độ tương đồng cao chia sẻ chính xác cùng quy mô, chiến lược huấn luyện, lựa chọn thiết kế, v.v. Thật ngạc nhiên, không giống như LLM dày đặc, không có nỗ lực nào như vậy đã được thực hiện để hiểu những thay đổi tinh tế trong khả năng của LLM nén với độ mạnh nén khác nhau. Trực giao với xu hướng gần đây là phát triển các thuật toán nén mới, công trình của chúng tôi cung cấp nỗ lực đầu tiên để đánh giá các ưu điểm và hạn chế thực sự của các thuật toán nén LLM SoTA hiện có, để cung cấp một sân chơi công bằng và chi tiết để phát triển các thuật toán nén tốt hơn. Chúng tôi tập trung vào trường hợp LLM nén vì chúng tôi quan sát thấy sự thất bại sâu sắc của độ phức tạp trong việc nắm bắt các biến đổi hiệu suất tinh tế qua các nén LLM khác nhau.

Hình 1(Trên) minh họa sự thay đổi độ phức tạp của các phương pháp nén SoTA (tỉa và lượng tử hóa), như SparseGPT, Wanda, GPTQ và tỉa dựa trên độ lớn một lần cơ sở trên Vicuna-7B, 13B và 33B (Chiang et al., 2023). Rõ ràng, độ phức tạp (↓) của tất cả các mô hình không cho thấy bất kỳ biến đổi đáng kể nào lên đến 45-60%, với sự thất bại hoàn toàn trong việc nắm bắt những thay đổi tinh tế trong khả năng của LLM khi được nén. Cũng thú vị khi quan sát rằng đến một mức độ thưa nhất định (~30%), tất cả các phương pháp tỉa SoTA có hiệu suất gần như tương tự với đường cơ sở đơn giản của tỉa dựa trên độ lớn một lần, điều này đặt ra câu hỏi về ưu điểm thực sự của chúng trong phạm vi thưa này. Hình 1(Dưới) cho thấy phản hồi của mô hình Vicuna-7B khi được nén với Magnitude, SparseGPT và Wanda 50% và lượng tử hóa lên đến 4-bit. Vicuna-7B không nén đã có thể tạo ra câu trả lời đúng thành công, nhưng tất cả các phiên bản nén đều thất bại trong việc phản hồi chính xác, ảo giác với những sự thật sai hoặc phản hồi không liên quan.

3 LLM-KICK: TIẾT LỘ ƯU ĐIỂM THỰC SỰ CỦA NÉN LLM

LLM-KICK, viết tắt của Bộ Đánh giá LLM Nén Chuyên sâu Kiến thức, được tạo ra để mang sự chú ý của cộng đồng nén LLM hướng đến sự bất lực của độ phức tạp trong việc phản ánh chính xác những thay đổi tinh tế trong khả năng của LLM được dẫn xuất từ các đối tác dày đặc với độ mạnh nén khác nhau. LLM-KICK bao gồm một bộ các thiết lập nhiệm vụ thách thức, thực tế và đa dạng có tầm quan trọng thực tế cao và các bộ dữ liệu có thể trao quyền cho việc hiểu có hệ thống về cách các chiến lược nén LLM hiện có thực sự hoạt động trong việc duy trì hiệu suất mặc dù có độ phức tạp tương tự. Công trình của chúng tôi điều tra kỹ lưỡng các ưu điểm/hạn chế được tuyên bố của LLM được tỉa và lượng tử hóa cho hiểu ngôn ngữ, lý luận, tạo sinh, truy xuất trong ngữ cảnh, tóm tắt trong ngữ cảnh, v.v.

Cụ thể, LLM-KICK bao gồm 3 thiết lập nhiệm vụ rộng để nghiên cứu cách nén ảnh hưởng đến kiến thức được mã hóa trong quá trình tiền huấn luyện, cách LLM nén thực hiện các nhiệm vụ khi kiến thức cần thiết được tăng cường trong ngữ cảnh, và LLM nén thực hiện tuân theo hướng dẫn tốt như thế nào. Để phân loại độ khó và tính đa dạng của nhiệm vụ, chúng tôi bao gồm QA dựa trên sự thật, QA dựa trên lý luận đa lựa chọn, QA tăng cường truy xuất trong ngữ cảnh, tóm tắt văn bản trong ngữ cảnh và tạo văn bản tự do dựa trên hướng dẫn. Thay vì tạo ra các bộ dữ liệu mới, chúng tôi tuyển chọn cẩn thận LLM-KICK từ các công trình trước đây và kho lưu trữ GitHub mã nguồn mở đã được các nhà nghiên cứu chấp nhận rộng rãi, nhưng chưa được các nhà nghiên cứu nén LLM khám phá. Các chiến lược thiết kế prompt chi tiết của chúng tôi cho các thiết lập nhiệm vụ khác nhau có thể được tìm thấy trong Phụ lục A.2.

Để giảm chi phí của các thí nghiệm dư thừa và sự lộn xộn trong kết quả, công trình của chúng tôi chủ yếu tập trung vào 2 kỹ thuật tỉa LLM không cần huấn luyện và không cần dữ liệu hàng đầu hiện có (tức là SparseGPT (Frantar & Alistarh, 2023) và Wanda (Sun et al., 2023)), cùng với đường cơ sở Tỉa Dựa trên Độ lớn Một lần (Han et al., 2016), cộng với một kỹ thuật lượng tử hóa phổ biến (GPTQ) trong số các lựa chọn có sẵn gần đây (Lin et al., 2023a; Frantar et al., 2022; Dettmers et al., 2023c). Chúng tôi xem xét hai loại thưa: (i)Thưa Không có Cấu trúc: các trọng số mô hình riêng lẻ được đặt về không độc lập, dẫn đến các mẫu không đều (LeCun et al., 1990; Han et al., 2016); và (ii)Thưa Có Cấu trúc N:M: một mẫu thưa chi tiết trong đó chỉ có N trọng số khác không cho mỗi M trọng số liên tiếp (Nvidia, 2020; Zhou et al., 2021). Chúng tôi sử dụng các mô hình Vicuna cho thí nghiệm, là các mô hình chatbot mã nguồn mở được huấn luyện bằng cách tinh chỉnh LLaMA (Chiang et al., 2023) trên các cuộc hội thoại được chia sẻ bởi người dùng được thu thập từ ShareGPT, và đã chứng minh chất lượng ấn tượng 90% của OpenAI ChatGPT và Google Bard. Lưu ý rằng mục đích của công trình này không giới hạn ở việc xác định các trường hợp thất bại của các phương pháp tỉa SoTA, mà thay vào đó cung cấp một cái nhìn sâu sắc về khả năng của LLM dưới nén, và mang lại những hiểu biết mới bao gồm việc làm nổi bật các quan sát có lợi cho các phương pháp nén SoTA hiện tại.

Chính thức, chúng tôi nghiên cứu sự sụt giảm hiệu suất của LLM sau nén (không tinh chỉnh) so với các đối tác dày đặc của chúng bằng thuật toán nén C. Đối với LLM được tiền huấn luyện f(x;θ), LLM nén là một mạng fcomp(x;θC), là một bản sao của f(x;θ) với một số trọng số được cố định thành 0 được chỉ ra bởi mặt nạ tỉa mC trong trường hợp tỉa, hoặc được lượng tử hóa thành kC-bit bằng thuật toán lượng tử hóa. Tiếp theo, chúng tôi định nghĩa LLM nén phù hợp.

LLM Nén Phù hợp: Một LLM nén fcomp(x;θC) là phù hợp cho thuật toán nén C trên nhiệm vụ T, nếu nó dẫn đến hiệu suất không nhỏ hơn ε0(chế độ dung sai nén) so với f(x;θ). Trong công trình này, chúng tôi xem xét ε0 là <=5% hiệu suất của f(x;θ,T).

Lưu ý rằng ε0 là một chỉ số đơn giản về mức dung sai của sự sụt giảm hiệu suất khi chúng ta bắt đầu nén bất kỳ LLM nào. Nhiều công trình trước đây (Chen et al., 2020b; Jaiswal et al., 2023a) xem xét ngưỡng phù hợp giống như hiệu suất mạng con dày đặc hoặc trong phạm vi 1%. Tuy nhiên, trong công trình của chúng tôi, chúng tôi đã cẩn thận nới lỏng nó thành 5% sụt giảm hiệu suất như một mức dung sai có thể chấp nhận (trước khi gọi mô hình nén là vô dụng) có tính đến rằng hiệu suất của LLM nén trên bất kỳ danh mục/lĩnh vực nhiệm vụ nào của chúng tôi vẫn trên mức đoán ngẫu nhiên.

3.1 THIẾT LẬP 1: LLM NÉN TRUY CẬP KIẾN THỨC CÒN LẠI TỐT NHƯ THẾ NÀO?

1Hỏi Đáp Dựa trên Sự thật

Định nghĩa Nhiệm vụ và Lý do. Hỏi Đáp Dựa trên Sự thật (Factoid-QA) (Iyyer et al., 2014), hỏi các sự thật chính xác về các thực thể, là một vấn đề lâu đời trong NLP. Một nhiệm vụ Factoid-QA điển hình nhằm tìm kiếm các thực thể hoặc thuộc tính thực thể từ đồ thị kiến thức, và nó được sử dụng rộng rãi như một công cụ trong học thuật, công cụ tìm kiếm thương mại và trợ lý hội thoại. LLM hiện đại được huấn luyện trên kho ngữ liệu văn bản khổng lồ tiếp thu một lượng lớn kiến thức thế giới về các thực thể và mối quan hệ của chúng trong quá trình tiền huấn luyện, và có khả năng độc đáo để tạo ra phản hồi đúng về mặt sự thật cho các truy vấn của người dùng. Trong thiết lập nhiệm vụ này, chúng tôi nhằm điều tra cách nén ảnh hưởng đến khả năng của LLM trả lời các câu hỏi ngôn ngữ tự nhiên bằng các sự thật, tức là kiến thức về thực thể hoặc thuộc tính được tiếp thu trong chúng trong quá trình tiền huấn luyện.

Chi tiết Bộ dữ liệu. Chúng tôi sử dụng FreebaseQA (Jiang et al., 2019) là một bộ dữ liệu cho QA miền mở trên đồ thị kiến thức Freebase. Các cặp QA được thu thập từ nhiều nguồn khác nhau, bao gồm bộ dữ liệu TriviaQA (Joshi et al., 2017) và các trang web trivia khác (QuizBalls, QuizZone, KnowQuiz), và được khớp với Freebase để tạo ra các bộ ba chủ thể-vị từ-đối tượng liên quan được xác minh thêm bởi các chú thích viên con người. Bộ dữ liệu TriviaQA cho thấy sự biến đổi và phức tạp ngôn ngữ phong phú, khiến nó trở thành một testbed tốt để đánh giá kiến thức được tiếp thu trong LLM.

Kết quả và Phân tích. Kết quả của các phương pháp nén LLM khác nhau được thể hiện trong Hình 2. Các quan sát chính của chúng tôi bao gồm: 1Tất cả các phương pháp tỉa LLM SoTA dường như thất bại trong việc tìm ra LLM thưa phù hợp, ngay cả ở độ thưa tầm thường như 30-35%. Trong khi một số phương pháp duy trì hiệu suất phù hợp ở độ thưa 20-25%, hiệu suất của chúng bắt đầu sụt giảm đáng kể sau đó trải qua một thất bại thảm khốc khi tỷ lệ thưa tăng lên. Điều này trái ngược với tuyên bố được đưa ra bởi các phương pháp tỉa SoTA rằng tỉa lên đến 50-60% LLM không có bất kỳ sự suy giảm đáng kể nào về hiệu suất. 2Tất cả các phương pháp tỉa đều không hoạt động cho các mẫu thưa có cấu trúc chi tiết N:M với sự sụt giảm hiệu suất nghiêm trọng như >=50%. 3~8-10% sụt giảm hiệu suất cho lượng tử hóa 8-bit không tích cực chỉ ra rằng cùng với việc theo đuổi các mức lượng tử hóa tích cực (1-2 bit), cũng quan trọng khi tập trung vào lượng tử hóa 8-bit chưa được giải quyết.

2Hỏi Đáp Dựa trên Lý luận Đa lựa chọn

Công thức Nhiệm vụ và Lý do. QA Dựa trên Lý luận Đa lựa chọn (MCR-QA) sử dụng một cách tiếp cận prompting tự nhiên để trình bày câu hỏi và các tùy chọn trả lời cho LLM cùng nhau, và yêu cầu nó xuất ra ký hiệu (ví dụ: "A") liên kết với tùy chọn trả lời được chọn của nó. Nó cho phép mô hình so sánh rõ ràng các tùy chọn trả lời. Trong thiết lập này, chúng tôi nhằm điều tra khả năng của LLM nén hiểu các câu hỏi ngôn ngữ tự nhiên, lý luận hiệu quả bằng kiến thức còn lại trong chúng, và liên kết thành công câu trả lời đúng trong số các tùy chọn trả lời đã cho với các ký hiệu đại diện cho chúng; có khả năng giảm thiểu tác động của tokenization và tạo trả lời chính xác.

Chi tiết Bộ dữ liệu. Chúng tôi sử dụng benchmark MMLU (Massive Multitask Language Understanding) phổ biến bao gồm hơn 50 chủ đề trên STEM, Nhân văn, Khoa học Xã hội và nhiều hơn nữa (Hendrycks et al., 2020). Nó có độ khó từ cấp độ cơ bản đến cấp độ chuyên nghiệp tiên tiến, và nó kiểm tra cả kiến thức thế giới và khả năng giải quyết vấn đề của LLM. Tính chi tiết và rộng rãi của các chủ đề khiến nó trở nên lý tưởng cho việc đánh giá chi tiết các điểm mù của LLM nén.

Kết quả và Phân tích. Kết quả của các phương pháp nén LLM khác nhau được thể hiện trong Hình 3. Các quan sát chính của chúng tôi bao gồm: 1Mặc dù có chế độ nén phù hợp tương tự (~20-40%) với Factoid-QA, sự sụt giảm hiệu suất đột ngột của tất cả các phương pháp tỉa SoTA cho MMLU tương đối tinh tế hơn do nới lỏng thiết lập nhiệm vụ từ tạo trả lời chính xác sang lựa chọn trả lời đúng. 2Không tìm thấy LLM nén phù hợp nào cho độ thưa có cấu trúc N:M. 3Lượng tử hóa LLM SoTA dường như thành công hơn so với các phương pháp tỉa SoTA: chúng tôi thấy LLM nén 8-bit và 4-bit phù hợp cho Vicuna-7B và Vicuna-13B, tương ứng. 4Thú vị là, cả lượng tử hóa và tỉa đều có sự sụt giảm hiệu suất tương đối cao hơn cho Nhân văn và Khoa học Xã hội so với STEM, điều này chỉ ra rằng nén ảnh hưởng đến một số lĩnh vực nhiều hơn các lĩnh vực khác. 5Thật ngạc nhiên, trong chế độ dung sai nén, tỉa độ lớn một lần đơn giản dường như hoạt động khá tốt so với phương pháp tỉa SoTA, minh họa hiệu quả cao của nó.

3.2 THIẾT LẬP 2: LLM NÉN TỔNG HỢP KIẾN THỨC TĂNG CƯỜNG TỐT NHƯ THẾ NÀO?

1Hỏi Đáp Tăng cường Truy xuất trong Ngữ cảnh

Công thức Nhiệm vụ và Lý do. Hỏi Đáp Tăng cường Truy xuất trong Ngữ cảnh (ICRA-QA) (Ram et al., 2023) làm nền tảng cho việc tạo trả lời LLM bằng cách điều kiện hóa trên các tài liệu liên quan được truy xuất từ một nguồn kiến thức bên ngoài bằng các thuật toán truy xuất như BM25. Hệ thống đánh giá ICRA-QA của chúng tôi bao gồm hai thành phần cấp cao: alựa chọn tài liệu, chọn tập hợp các tài liệu để điều kiện hóa; và bđọc tài liệu, xác định cách kết hợp các tài liệu đã chọn vào quá trình trả lời LLM, đòi hỏi việc trích xuất các cụm từ trả lời đúng từ các tài liệu được điều kiện hóa. Để giảm tác động của kiến thức được mã hóa bị mất trong quá trình nén, ICRA-QA tăng cường kiến thức liên quan cần thiết cho nhiệm vụ QA trực tiếp trong ngữ cảnh prompt. Trong thiết lập nhiệm vụ này, chúng tôi nhằm đánh giá khả năng của LLM nén tổng hợp kiến thức dài trong ngữ cảnh được cung cấp trong các prompt đầu vào, và định vị và truy xuất các câu trả lời đúng trong đó. Chúng tôi cũng trình bày so sánh trực tiếp về cách kiến thức tăng cường có thể hoạt động như một biện pháp khắc phục để bổ sung kiến thức bị mất dưới nén.

Chi tiết Bộ dữ liệu. Chúng tôi sử dụng TriviaQA (Joshi et al., 2017) để đánh giá, một bộ dữ liệu đọc hiểu phổ biến bao gồm 95K cặp câu hỏi-trả lời được tác giả bởi các người đam mê trivia và các tài liệu bằng chứng được thu thập độc lập, trung bình sáu cho mỗi câu hỏi, cung cấp giám sát từ xa chất lượng cao để trả lời các câu hỏi.

Kết quả và Phân tích. Kết quả của các phương pháp nén LLM khác nhau được thể hiện trong Hình 17. Thiết lập sách đóng khác với ICRA-QA (tức là sử dụng thiết lập sách mở) chỉ về việc có điều kiện hóa trên các tài liệu liên quan được truy xuất từ nguồn kiến thức bên ngoài hay không. Các phát hiện chính của chúng tôi là: 1Khi LLM nén được điều kiện hóa trên kiến thức bên ngoài (sách mở) và được giao nhiệm vụ làm bộ truy xuất trong ngữ cảnh, tức là trích xuất các cụm từ trả lời đúng từ kiến thức trong ngữ cảnh, chúng hoạt động đáng kể tốt ngay cả trong chế độ nén cực kỳ cao. Vicuna-7B có thể duy trì phù hợp đến ~40% độ thưa và lượng tử hóa 8-bit, trong khi Vicuna-13B có thể duy trì phù hợp lên đến ~50% độ thưa và lượng tử hóa 4-bit. Kết quả thí nghiệm của chúng tôi gửi một tín hiệu tích cực rằng ngay cả khi nén cao dẫn đến mất kiến thức đáng kể, nó không khiến LLM trở nên hoàn toàn vô dụng, và chúng vẫn hoạt động như các bộ truy xuất trong ngữ cảnh mạnh mẽ. 2Mặc dù chúng tôi quan sát thấy lợi ích đáng kể khi điều kiện hóa kiến thức bên ngoài, không có LLM nén phù hợp nào có thể được xác định cho độ thưa N:M. 3Một lần nữa, chúng tôi quan sát hiệu suất đáng ngạc nhiên tốt của tỉa độ lớn không có cấu trúc một lần đơn giản so với SparseGPT (tỉa bậc hai) và Wanda (tỉa dựa trên kích hoạt) dựa vào dữ liệu hiệu chuẩn.

2Tóm tắt Văn bản trong Ngữ cảnh

Công thức và Chi tiết Nhiệm vụ. LLM hiện đại đã cho thấy thành công đáng kinh ngạc trong việc tóm tắt các tài liệu ngữ cảnh dài trong cả thiết lập trừu tượng và trích xuất. Tuy nhiên, chưa được khám phá cách nén ảnh hưởng đến khả năng tóm tắt của LLM. Trong thiết lập nhiệm vụ này, chúng tôi nhằm điều tra khả năng của LLM nén giữ vững tính nhất quán, mạch lạc, trôi chảy và liên quan khi được yêu cầu tóm tắt thông tin văn bản có độ dài khác nhau (nhỏ, trung bình và lớn) trong thiết lập trừu tượng (Jain et al., 2023). Để đánh giá, tương tự như Zheng et al. (2023), chúng tôi đề xuất sử dụng GPT-4 làm thẩm phán, so sánh các bản tóm tắt được tạo bởi LLM nén với các bản tóm tắt được tạo bởi GPT-3.5 (text-davinci-003). Các thiết lập đánh giá chi tiết có thể được tìm thấy trong Phụ lục A.3.

Chi tiết Bộ dữ liệu. Chúng tôi sử dụng bộ dữ liệu tóm tắt phổ biến CNN/DailyMail (Chen et al., 2016) để đánh giá, là một bộ dữ liệu tiếng Anh chứa hơn 300k bài báo tin tức độc đáo được viết bởi các nhà báo tại CNN và DailyMail. Chúng tôi tạo ra 3 danh mục tập con {nhỏ (<=470 từ), trung bình (>=470 và <=790 từ), và lớn (>=790 từ)} câu chuyện, mỗi danh mục có 100 bài báo phản ánh phân phối từ của CNN/DailyMail để giảm thiểu chi phí OpenAI API.

Kết quả và Phân tích. Kết quả được tóm tắt trong Hình 5. Chúng tôi tóm tắt các quan sát chính như sau: 1Tất cả các phương pháp tỉa và lượng tử hóa có xu hướng hoạt động đáng ngạc nhiên tốt cho tóm tắt trong ngữ cảnh, duy trì tính nhất quán, mạch lạc, trôi chảy và liên quan cao trong các bản tóm tắt được tạo ra, đây là một quan sát khích lệ có lợi cho nén. 2Với độ dài ngữ cảnh tăng lên (tức là câu chuyện dài), chúng tôi quan sát sự sụt giảm hiệu suất mạnh hơn cho LLM nén, điều này làm nổi bật rằng nén ảnh hưởng đến khả năng tổng hợp và tóm tắt độ dài ngữ cảnh dài hơn của LLM. 3Lượng tử hóa một lần nữa dường như hoạt động tốt hơn so với các phương pháp tỉa SoTA, và đáng ngạc nhiên có lợi tích cực so với hiệu suất mô hình dày đặc. 4Không thể xác định LLM nén phù hợp nào cho độ thưa có cấu trúc 2:4.

3.3 THIẾT LẬP 3: LLM NÉN THỰC HIỆN TUÂN THEO HƯỚNG DẪN TỐT NHƯ THẾ NÀO?

Công thức Nhiệm vụ và Lý do. Trong thiết lập nhiệm vụ này, chúng tôi điều tra khả năng của LLM nén trả lời các câu hỏi mở và đánh giá khả năng hội thoại đa lượt và tuân theo hướng dẫn của chúng - hai yếu tố quan trọng cho sự ưa thích của con người. Đánh giá chatbot AI là một nhiệm vụ thách thức, vì nó đòi hỏi việc kiểm tra hiểu biết ngôn ngữ, lý luận và nhận thức ngữ cảnh. Để so sánh hiệu suất của các phản hồi LLM nén, chúng tôi tuân theo chặt chẽ thiết lập thiết kế prompt trong MT-Bench (Zheng et al., 2023) sử dụng GPT-4 làm thẩm phán. Chúng tôi yêu cầu GPT-4 đánh giá các câu trả lời được tạo bởi LLM nén so với mô hình GPT-3.5 (text-davinci-003) dựa trên các chỉ số khác nhau (ví dụ: tính đúng đắn, hữu ích, logic, chính xác, v.v.) trên thang điểm [0-10] với các giải thích chi tiết.

Chi tiết Bộ dữ liệu. Chúng tôi dựa vào 80 câu hỏi đa lượt chất lượng cao được xác định trong MT-Bench (Zheng et al., 2023). Thiết lập này bao gồm tương tác thông thường tập trung vào con người với LLM, và tập trung vào các câu hỏi thách thức để phân biệt các mô hình. Chúng tôi sử dụng 8 danh mục phổ biến của prompt người dùng để hướng dẫn việc xây dựng prompt để tương tác với LLM nén: viết, nhập vai, trích xuất, lý luận, toán học, lập trình, v.v. Đối với mỗi danh mục, chúng tôi đã áp dụng 10 câu hỏi đa lượt được thiết kế thủ công từ MT-Bench để đánh giá các mô hình nén của chúng tôi. Chi tiết có thể được tìm thấy trong Phụ lục A.4.

Kết quả và Phân tích. Kết quả được tóm tắt trong Hình 6. Các quan sát chính của chúng tôi là: 1Không giống như tóm tắt văn bản trong ngữ cảnh, trong thiết lập nhiệm vụ này, LLM nén phải truy cập kiến thức để phản hồi các cuộc hội thoại duy trì tính hữu ích, liên quan, chính xác và chi tiết cao. Chúng tôi một lần nữa quan sát rằng LLM nén với các phương pháp tỉa khác nhau chỉ phù hợp đến tỷ lệ thưa ~25%. 2Thật ngạc nhiên, trong chế độ phù hợp, đường cơ sở đơn giản của tỉa độ lớn một lần hoạt động tương đương hoặc hơi tốt hơn so với các phương pháp tỉa SoTA. 3Không thể xác định mạng con phù hợp nào cho độ thưa N:M. 4Thú vị là, phân tích token độc đáo trung bình được tạo ra của chúng tôi trong Hình 6(c) minh họa rằng LLM nén mất khả năng tạo ra nội dung độc đáo riêng biệt, thay vào đó, chúng chỉ có thể tạo ra các văn bản lặp lại hơn.

4 KẾT QUẢ BỔ SUNG VÀ THẢO LUẬN

Dày đặc-Nhỏ so với Thưa-Lớn: cái nào thuận lợi hơn? Chúng tôi cố gắng hiểu một câu hỏi thú vị: liệu LLM được tỉa với kiến trúc lớn hơn (Thưa-Lớn) có tốt hơn các mô hình dày đặc nhỏ hơn với số lượng tham số tương tự (Dày đặc-Nhỏ)? Tỉa LLM lớn không miễn phí, và điều quan trọng là điều tra xem chi phí tỉa có thể được phản ánh trong lợi ích hiệu suất của các mô hình Thưa-Lớn hay không. Thật ngạc nhiên, so với Vicuna-7B dày đặc (độ chính xác MMLU 46,7%), chúng tôi thấy Vicuna-13B nén với số lượng tham số chính xác tương tự (độ thưa 46,16%) là 7 tỷ bằng tỉa độ lớn một lần, Wanda, SparseGPT chỉ có thể đạt được độ chính xác MMLU là 31,7%, 45,3% và 46,3%, tương ứng. Đây là một dấu hiệu rõ ràng rằng các thuật toán thưa hiện tại chưa đến giai đoạn mà chi phí tỉa có thể được biện minh bởi các lợi ích hiệu suất thu được từ các mô hình nén thưa-lớn.

Cần bao nhiêu mẫu dữ liệu hiệu chuẩn? Chúng tôi cố gắng phân tích cách các phương pháp tỉa phụ thuộc vào hiệu chuẩn (Wanda và SparseGPT) hoạt động với lượng mẫu hiệu chuẩn khác nhau. Hình 7 minh họa hiệu suất zero-shot của Vicuna-7B được tỉa 50% & 70% bằng Wanda và SparseGPT trên benchmark MMLU chuyên sâu kiến thức. Thật thú vị khi quan sát rằng số lượng mẫu hiệu chuẩn đóng vai trò quan trọng trong việc duy trì hiệu suất của SparseGPT không giống như Wanda. Lưu ý rằng ở tỷ lệ thưa cao (70%), Wanda không thể khôi phục bất kỳ hiệu suất nào; SparseGPT đáng ngạc nhiên có lợi đáng chú ý từ hiệu chuẩn. Điều này gợi ý rằng các mẫu hiệu chuẩn được chọn cẩn thận có thể đóng vai trò quan trọng trong việc thiết kế các thuật toán tỉa tốt hơn để nén LLM ngay cả lên đến độ thưa cao đáng kể.

5 KẾT LUẬN VÀ HẠN CHẾ

Trong bài báo này, chúng tôi đề xuất khám phá hiệu quả của các phương pháp nén SoTA vượt ra ngoài độ phức tạp để giải quyết sự bất lực của độ phức tạp trong việc nắm bắt các biến đổi tinh tế phát sinh trong quá trình dẫn xuất LLM nén từ các đối tác dày đặc của chúng. Công trình của chúng tôi giới thiệu Bộ Đánh giá LLM Nén Chuyên sâu Kiến thức (LLM-KICK) để tạo điều kiện cho việc đánh giá công bằng và toàn diện bằng cách tiết lộ nhiều ưu điểm và khó khăn của các phương pháp nén SoTA. Nghiên cứu của chúng tôi tiết lộ rằng nén ảnh hưởng đáng kể đến kiến thức được mã hóa trong LLM trong quá trình tiền huấn luyện, LLM nén hoạt động khá tốt với kiến thức được tăng cường trong các thiết lập ngữ cảnh. Chúng tôi chủ yếu hạn chế đánh giá của mình đối với Vicuna (kiến trúc chỉ giải mã) do giấy phép mã nguồn mở, hiệu suất cao và khả năng tuân theo hướng dẫn của nó. Đối với công việc tương lai, chúng tôi nhằm điều tra cách kiến thức bị mất do nén có thể được khôi phục bằng các phương pháp tinh chỉnh hiệu quả tham số, ví dụ: LoRA (Hu et al., 2021) và QLoRA (Dettmers et al., 2023b).
