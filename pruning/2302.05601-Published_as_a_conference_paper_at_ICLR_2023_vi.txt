# 2302.05601.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2302.05601.pdf
# Kích thước tệp: 13671955 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023
CẮT TỈA MẠNG NƠRON SÂU TỪ QUAN ĐIỂM ĐỘ THƯA
Enmao Diao∗
Khoa Kỹ thuật Điện
và Máy tính
Đại học Duke
Durham, NC 27705, USA
enmao.diao@duke.eduGanghua Wang∗
Trường Thống kê
Đại học Minnesota
Minneapolis, MN 55455, USA
wang9019@umn.eduJiawei Zhang
Trường Thống kê
Đại học Minnesota
Minneapolis, MN 55455, USA
zhan4362@umn.edu
Yuhong Yang
Trường Thống kê
Đại học Minnesota
Minneapolis,
MN 55455, USA
yangx374@umn.eduJie Ding
Trường Thống kê
Đại học Minnesota
Minneapolis,
MN 55455, USA
dingj@umn.eduVahid Tarokh
Khoa Kỹ thuật Điện
và Máy tính
Đại học Duke
Durham, NC 27705, USA
vahid.tarokh@duke.edu

TÓM TẮT
Trong những năm gần đây, việc cắt tỉa mạng sâu đã thu hút sự chú ý đáng kể nhằm cho phép triển khai nhanh chóng AI vào các thiết bị nhỏ với các ràng buộc về tính toán và bộ nhớ. Cắt tỉa thường được thực hiện bằng cách loại bỏ các trọng số, nơron, hoặc lớp dư thừa của một mạng sâu trong khi cố gắng duy trì hiệu suất kiểm tra tương đương. Nhiều thuật toán cắt tỉa sâu đã được đề xuất với thành công thực nghiệm ấn tượng. Tuy nhiên, các phương pháp hiện có thiếu một thước đo có thể định lượng để ước tính khả năng nén của một mạng con trong mỗi lần lặp cắt tỉa và do đó có thể cắt tỉa thiếu hoặc cắt tỉa quá mức mô hình. Trong nghiên cứu này, chúng tôi đề xuất Chỉ số PQ (PQI) để đo lường khả năng nén tiềm năng của mạng nơron sâu và sử dụng điều này để phát triển thuật toán Cắt tỉa Thích ứng dựa trên Độ thưa (SAP). Các thí nghiệm mở rộng của chúng tôi xác nhận giả thuyết rằng đối với một quy trình cắt tỉa chung, PQI giảm đầu tiên khi một mô hình lớn đang được điều chỉnh hiệu quả và sau đó tăng khi khả năng nén của nó đạt đến giới hạn có vẻ tương ứng với sự bắt đầu của hiện tượng underfitting. Sau đó, PQI giảm lại khi mô hình sụp đổ và sự suy giảm đáng kể trong hiệu suất của mô hình bắt đầu xảy ra. Ngoài ra, các thí nghiệm của chúng tôi chứng minh rằng thuật toán cắt tỉa thích ứng được đề xuất với sự lựa chọn thích hợp về siêu tham số vượt trội hơn các thuật toán cắt tỉa lặp như các phương pháp cắt tỉa dựa trên vé số, về cả hiệu quả nén và tính mạnh mẽ. Mã của chúng tôi có sẵn tại đây.

1 GIỚI THIỆU
Mạng nơron sâu có quá nhiều tham số đã được áp dụng với thành công to lớn trong nhiều lĩnh vực khác nhau, bao gồm thị giác máy tính (Krizhevsky et al., 2012; He et al., 2016b; Redmon et al., 2016), xử lý ngôn ngữ tự nhiên (Devlin et al., 2018; Radford et al., 2018), xử lý tín hiệu âm thanh (Oord et al., 2016; Schneider et al., 2019; Wang et al., 2020), và học phân tán (Kone ˇcn`y et al., 2016; Ding et al., 2022; Diao et al., 2022). Những mạng nơron sâu này đã mở rộng đáng kể về kích thước. Ví dụ, LeNet-5 (LeCun et al., 1998) (1998; phân loại hình ảnh) có 60 nghìn tham số trong khi GPT-3 (Brown et al., 2020) (2020; mô hình hóa ngôn ngữ) có 175 tỷ tham số. Sự tăng trưởng nhanh chóng về kích thước này đã đòi hỏi việc triển khai một lượng lớn tài nguyên tính toán, lưu trữ và năng lượng. Do các ràng buộc phần cứng, những kích thước mô hình khổng lồ này có thể là rào cản cho việc triển khai trong một số thiết bị biên như điện thoại di động và trợ lý ảo. Điều này đã làm tăng đáng kể sự quan tâm đến việc nén/cắt tỉa mạng nơron sâu. Để đạt được mục đích này, nhiều nhà nghiên cứu đã phát triển các phương pháp thực nghiệm để xây dựng các mạng đơn giản hơn nhiều với hiệu suất tương tự dựa
∗Đóng góp như nhau
1arXiv:2302.05601v3  [cs.LG]  23 Aug 2023

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023
Điều chỉnh Nén Sụp đổHiệu suất
Độ thưaHiệu suất
Độ thưaHiệu suất
Độ thưa
Hình 1: Minh họa giả thuyết của chúng tôi về mối quan hệ giữa độ thưa và khả năng nén của mạng nơron. Độ rộng của các kết nối biểu thị độ lớn của các tham số mô hình.

trên các mạng được huấn luyện trước (Han et al., 2015; Frankle & Carbin, 2018). Ví dụ, Han et al. (2015) đã chứng minh rằng AlexNet (Krizhevsky et al., 2017) có thể được nén để chỉ giữ lại 3% tham số gốc trên bộ dữ liệu ImageNet mà không ảnh hưởng đến độ chính xác phân loại.

Một chủ đề quan trọng được quan tâm là xác định giới hạn của việc cắt tỉa mạng. Một mô hình được cắt tỉa quá mức có thể không có đủ khả năng biểu đạt cho nhiệm vụ cơ bản, điều này có thể dẫn đến sự suy giảm hiệu suất đáng kể (Ding et al., 2018). Các phương pháp hiện có thường theo dõi hiệu suất dự đoán trên một bộ dữ liệu xác thực và dừng cắt tỉa khi hiệu suất giảm xuống dưới một ngưỡng được chỉ định trước. Tuy nhiên, một thước đo có thể định lượng để ước tính khả năng nén của một mạng con trong mỗi lần lặp cắt tỉa là mong muốn. Việc định lượng khả năng nén như vậy có thể dẫn đến việc khám phá các mạng con tiết kiệm nhất mà không làm suy giảm hiệu suất.

Trong nghiên cứu này, chúng tôi kết nối khả năng nén và hiệu suất của một mạng nơron với độ thưa của nó. Trong một mạng có quá nhiều tham số, một giả định phổ biến là các trọng số tương đối nhỏ được coi là dư thừa hoặc không có ảnh hưởng và có thể được cắt tỉa mà không ảnh hưởng đến hiệu suất. Hãy xem xét độ thưa của một vector không âm w= [w1, . . . , w d], vì độ thưa chỉ liên quan đến độ lớn của các phần tử. Giả sử S(w) là một thước đo độ thưa, và giá trị lớn hơn cho biết độ thưa cao hơn. Hurley & Rickard (2009) tổng hợp sáu thuộc tính mà một thước đo độ thưa lý tưởng nên có, ban đầu được đề xuất trong kinh tế học (Dalton, 1920; Rickard & Fallon, 2004). Chúng là

(D1) Robin Hood. Với bất kỳ wi> w jvàα∈(0,(wi−wj)/2), chúng ta có S([w1, . . . , w i−
α, . . . , w j+α, . . . , w d])< S(w).

(D2) Scaling. S(αw) =S(w)với bất kỳ α >0.

(D3) Rising Tide. S(w+α)< S(w)với bất kỳ α >0vàwkhông phải tất cả đều giống nhau.

(D4) Cloning. S(w) =S([w, w]).

(P1) Bill Gates. Với bất kỳ i= 1, . . . , d , tồn tại βi>0sao cho với bất kỳ α >0chúng ta có
S([w1, . . . , w i+βi+α, . . . , w d])> S([w1, . . . , w i+βi, . . . , w d]).

(P2) Babies. S([w1, . . . , w d,0])> S(w)với bất kỳ wnon-zero.

Hurley & Rickard (2009) chỉ ra rằng chỉ có chỉ số Gini thỏa mãn tất cả sáu tiêu chí trong danh sách toàn diện các thước đo độ thưa. Trong nghiên cứu này, chúng tôi đề xuất một thước đo độ thưa có tên là Chỉ số PQ (PQI). Theo hiểu biết tốt nhất của chúng tôi, PQI là thước đo đầu tiên liên quan đến chuẩn của một vector thỏa mãn tất cả sáu thuộc tính trên. Do đó, PQI là một chỉ báo lý tưởng của độ thưa vector và có tính độc lập thú vị.

Chúng tôi đề xuất sử dụng PQI để suy ra khả năng nén của mạng nơron. Hơn nữa, chúng tôi khám phá mối quan hệ giữa hiệu suất và độ thưa của các mô hình được cắt tỉa lặp như được minh họa trong Hình 1. Giả thuyết của chúng tôi là đối với một quy trình cắt tỉa chung, độ thưa sẽ giảm đầu tiên khi một mô hình lớn đang được điều chỉnh hiệu quả, sau đó tăng khi khả năng nén của nó đạt đến giới hạn tương ứng với sự bắt đầu của underfitting, và cuối cùng giảm khi sự sụp đổ mô hình xảy ra, tức là hiệu suất mô hình suy giảm đáng kể.

Trực giác của chúng tôi là việc cắt tỉa trước tiên sẽ loại bỏ các tham số dư thừa. Kết quả là, độ thưa của các tham số mô hình sẽ giảm và hiệu suất có thể được cải thiện do điều chỉnh. Khi mô hình được nén thêm, một phần của các tham số mô hình sẽ trở nên nhỏ hơn khi mô hình hội tụ. Do đó, độ thưa sẽ tăng và hiệu suất sẽ giảm vừa phải. Cuối cùng, khi sự sụp đổ mô hình bắt đầu xảy ra, tất cả các tham số bị suy giảm đều bị loại bỏ và các tham số còn lại trở nên quan trọng để duy trì hiệu suất. Do đó, độ thưa sẽ giảm và hiệu suất sẽ suy giảm đáng kể. Các thí nghiệm mở rộng của chúng tôi về các thuật toán cắt tỉa xác nhận giả thuyết này. Vì vậy, PQI có thể suy ra liệu một mô hình có thể nén được bản chất hay không. Được thúc đẩy bởi khám phá này, chúng tôi cũng đề xuất thuật toán Cắt tỉa Thích ứng dựa trên Độ thưa (SAP), có thể nén hiệu quả và mạnh mẽ hơn so với các thuật toán cắt tỉa lặp như các phương pháp cắt tỉa dựa trên vé số. Nhìn chung, nghiên cứu của chúng tôi trình bày một hiểu biết mới về cấu trúc bản chất của mạng nơron sâu cho việc nén mô hình. Các đóng góp chính của chúng tôi được tóm tắt dưới đây.

1.Chúng tôi đề xuất một khái niệm mới về độ thưa cho các vector được gọi là Chỉ số PQ (PQI), với giá trị lớn hơn cho biết độ thưa cao hơn. Chúng tôi chứng minh rằng PQI đáp ứng tất cả sáu thuộc tính được đề xuất bởi (Dalton, 1920; Rickard & Fallon, 2004), nắm bắt các nguyên tắc mà một thước đo độ thưa nên tuân theo. Trong số 15 thước đo độ thưa thường được sử dụng, thước đo duy nhất khác thỏa mãn tất cả các thuộc tính là Chỉ số Gini (Hurley & Rickard, 2009). Do đó, PQI dựa trên chuẩn là một thước đo độ thưa/công bằng lý tưởng và có thể có lợi ích độc lập cho nhiều lĩnh vực, ví dụ, xử lý tín hiệu và kinh tế học.

2.Chúng tôi phát triển một quan điểm mới về khả năng nén của mạng nơron. Cụ thể, chúng tôi đo độ thưa của các mô hình được cắt tỉa bằng PQI và đưa ra giả thuyết trên về mối quan hệ giữa độ thưa và khả năng nén của mạng nơron.

3.Được thúc đẩy bởi PQI được đề xuất và giả thuyết của chúng tôi, chúng tôi tiếp tục phát triển thuật toán Cắt tỉa Thích ứng dựa trên Độ thưa (SAP) sử dụng PQI để chọn tỷ lệ cắt tỉa một cách thích ứng. Cụ thể, tỷ lệ cắt tỉa tại mỗi lần lặp được quyết định dựa trên một bất đẳng thức liên quan đến PQI. Ngược lại, Chỉ số Gini không có những ý nghĩa như vậy đối với tỷ lệ cắt tỉa.

4.Chúng tôi tiến hành các thí nghiệm mở rộng để đo độ thưa của các mô hình được cắt tỉa và xác nhận giả thuyết của chúng tôi. Kết quả thí nghiệm của chúng tôi cũng chứng minh rằng SAP với sự lựa chọn thích hợp về siêu tham số có thể nén hiệu quả và mạnh mẽ hơn so với các thuật toán cắt tỉa lặp như các phương pháp cắt tỉa dựa trên vé số.

2 NGHIÊN CỨU LIÊN QUAN

Nén mô hình Mục tiêu của nén mô hình là tìm một mô hình nhỏ hơn có hiệu suất tương đương với mô hình gốc. Một mô hình nhỏ hơn tiết kiệm tài nguyên lưu trữ và tính toán, tăng tốc huấn luyện, và tạo điều kiện triển khai mô hình lên các thiết bị có khả năng hạn chế, như điện thoại di động và trợ lý ảo. Do đó, nén mô hình là quan trọng để triển khai mạng nơron sâu với hàng triệu hoặc thậm chí hàng tỷ tham số. Nhiều phương pháp nén mô hình khác nhau đã được đề xuất cho mạng nơron. Trong số đó, cắt tỉa là một trong những phương pháp phổ biến và hiệu quả nhất (LeCun et al., 1989; Hagiwara, 1993; Han et al., 2015; Hu et al., 2016; Luo et al., 2017; Frankle & Carbin, 2018; Lee et al., 2018; He et al., 2017). Ý tưởng của cắt tỉa là giả định độ thưa rằng nhiều kết nối nơron dư thừa hoặc không có ảnh hưởng tồn tại trong một mô hình có quá nhiều tham số. Do đó, chúng ta có thể loại bỏ những kết nối đó (ví dụ, trọng số, nơron, hoặc các cấu trúc giống nơron như lớp) mà không làm giảm nhiều độ chính xác kiểm tra. Hai thành phần quan trọng của thuật toán cắt tỉa là tiêu chí cắt tỉa quyết định kết nối nào được cắt tỉa và tiêu chí dừng xác định khi nào dừng cắt tỉa và do đó ngăn ngừa underfitting và sụp đổ mô hình. Có nhiều tiêu chí cắt tỉa được thúc đẩy bởi các cách diễn giải khác nhau về sự dư thừa. Một tiêu chí được sử dụng rộng rãi loại bỏ các tham số có độ lớn nhỏ nhất, giả định rằng chúng ít quan trọng hơn (Hagiwara, 1993; Han et al., 2015). Bên cạnh cắt tỉa dựa trên độ lớn, người ta có thể cắt tỉa các tham số dựa trên độ nhạy hoặc đóng góp của chúng đối với đầu ra mạng (LeCun et al., 1989; Lee et al., 2018; Hu et al., 2016; Soltani et al., 2021) hoặc hạn chế các thành phần mô hình khác nhau chia sẻ một tỷ lệ lớn trọng số nơron (Diao et al., 2019; 2021). Đối với tiêu chí dừng, lựa chọn phổ biến là xác thực: dừng cắt tỉa một khi độ chính xác kiểm tra trên bộ dữ liệu xác thực giảm xuống dưới một ngưỡng nhất định.

Trong khi cắt tỉa là một phương pháp hậu xử lý đòi hỏi một mô hình được huấn luyện trước, cũng có các phương pháp tiền xử lý và trong xử lý dựa trên giả định độ thưa. Ví dụ, người ta có thể thêm các ràng buộc thưa rõ ràng trên mạng, như buộc các tham số có cấu trúc hạng thấp và chia sẻ trọng số. Hoặc, người ta có thể buộc mô hình được huấn luyện trở nên thưa một cách ngầm định, như thêm một hình phạt độ thưa (ví dụ, ℓ1-norm) vào các tham số. Trái ngược với những phương pháp nén dựa trên độ thưa, tìm một mạng con của mạng gốc, các nhà nghiên cứu cũng đã đề xuất nén mô hình bằng cách tìm một mô hình nhỏ hơn với kiến trúc khác. Các nỗ lực bao gồm chưng cất kiến thức (Hinton et al., 2015) và tìm kiếm kiến trúc (Mushtaq et al., 2021). Chúng tôi giới thiệu người đọc đến (Hoefler et al., 2021) để có một khảo sát toàn diện về nén mô hình.

Lý thuyết nén mô hình Trong thực tế, khả năng nén của một mô hình phụ thuộc vào kiến trúc mạng và nhiệm vụ học. Việc cắt tỉa thường liên quan đến rất nhiều điều chỉnh siêu tham số ad hoc. Do đó, một hiểu biết về khả năng nén mô hình được cần thiết một cách cấp bách. Đã có một số nghiên cứu gần đây để chỉ ra sự tồn tại của hoặc tìm một mạng con có hiệu suất được đảm bảo. Arora et al. (2018) chỉ ra rằng một mô hình có thể nén được nhiều hơn nếu nó ổn định hơn với các đầu vào nhiễu và cung cấp một ràng buộc lỗi tổng quát của mô hình được cắt tỉa. Yang et al. (2022) đề xuất một thuật toán cắt tỉa ngược được lấy cảm hứng từ việc xấp xỉ hàm sử dụng ℓq-norm (Wang et al., 2014), và định lượng lỗi tổng quát của nó. Baykal et al. (2018); Mussay et al. (2019) sử dụng khái niệm coreset để chứng minh sự tồn tại của một mạng được cắt tỉa với hiệu suất tương tự. Ý tưởng chính là lấy mẫu các tham số dựa trên tầm quan trọng của chúng, và do đó các tham số được chọn có thể bảo tồn đầu ra của mạng gốc. Ye et al. (2020) đề xuất một thuật toán lựa chọn tham lam để tái tạo một mạng và ràng buộc lỗi tổng quát cho mạng nơron hai lớp. Nghiên cứu của chúng tôi phát triển một quan điểm mới về khả năng nén của mạng nơron bằng cách đo trực tiếp độ thưa của các mô hình được cắt tỉa để tiết lộ mối quan hệ giữa độ thưa và khả năng nén của mạng nơron.

Thước đo độ thưa Độ thưa là một khái niệm quan trọng trong nhiều lĩnh vực cơ bản như thống kê và xử lý tín hiệu (Tibshirani, 1996; Donoho, 2006; Akçakaya & Tarokh, 2008). Một cách trực quan, độ thưa có nghĩa là hầu hết năng lượng được tập trung trong một vài phần tử. Ví dụ, một giả định được sử dụng rộng rãi trong học máy nhiều chiều là mô hình có một biểu diễn thưa cơ bản. Nhiều thước đo độ thưa khác nhau đã được đề xuất trong tài liệu từ các góc độ khác nhau. Một loại thước đo độ thưa xuất phát từ xã hội học và kinh tế học. Ví dụ, Chỉ số Gini nổi tiếng (Gini, 1912) có thể đo bất bình đẳng trong phân phối tài sản hoặc phúc lợi của dân số. Một dân số có tài sản tập trung cao tạo thành một vector thưa nếu vector bao gồm tài sản của mỗi người. Ngoài ra, để đo đa dạng trong một nhóm, các thước đo dựa trên entropy như entropy Shannon và entropy Gaussian thường được sử dụng (Jost, 2006). Một loại thước đo độ thưa khác đã được nghiên cứu trong toán học và kỹ thuật từ lâu. Một thước đo cổ điển là độ thưa cứng, còn được gọi là ℓ0-norm, là số phần tử khác không trong w. Một độ thưa cứng nhỏ có nghĩa là chỉ một vài phần tử vector là hoạt động hoặc hiệu quả. Tuy nhiên, một thay đổi nhỏ trong phần tử có giá trị không có thể gây ra sự tăng đáng kể trong độ thưa cứng, điều này có thể không mong muốn. Do đó, các phép thư giãn của nó như ℓp-norm (0< p≤1) cũng được sử dụng rộng rãi. Ví dụ, các ràng buộc hoặc hình phạt dựa trên ℓ1-norm được sử dụng để xấp xỉ hàm (Barron, 1993), điều chỉnh mô hình và lựa chọn biến (Tibshirani, 1996; Chen et al., 2001). Nghiên cứu của chúng tôi đề xuất thước đo đầu tiên về độ thưa liên quan đến chuẩn vector thỏa mãn tất cả các thuộc tính được chia sẻ bởi Chỉ số Gini (Hurley & Rickard, 2009) và một thuật toán cắt tỉa thích ứng dựa trên thước đo độ thưa được đề xuất của chúng tôi.

3 CẮT TỈA VỚI CHỈ SỐ PQ

3.1 CHỈ SỐ PQ

Chúng tôi sẽ chứng minh tất cả sáu thuộc tính (D1)-(D4) và (P1), (P2), được đề cập trong phần giới thiệu, đều đúng cho Chỉ số PQ (PQI) được đề xuất của chúng tôi. Chúng tôi nhận ra rằng một dạng tương tự ban đầu được đề xuất trong Bronstein et al. (2005) (0< p≤1, và q= 2), và một chứng minh được đưa ra bởi Hurley & Rickard (2009) (0< p≤1< q). Trong nghiên cứu của chúng tôi, chúng tôi chứng minh rằng cần phải có 0< p≤1< q, và chứng minh khả năng áp dụng của nó cho việc nén mô hình.

Định nghĩa 1 (Chỉ số PQ). Với bất kỳ 0< p≤1< q, Chỉ số PQ của một vector khác không w∈Rdlà
Ip,q(w) = 1−d1
q−1
p∥w∥p
∥w∥q, (1)
trong đó ∥w∥p= (Pd
i=1|wi|p)1/plà ℓp-norm của wvới bất kỳ p >0. Để đơn giản, chúng tôi sẽ sử dụng I(w)
và bỏ qua phụ thuộc vào pvàqkhi ngữ cảnh rõ ràng.

Định lý 1. Chúng ta có 0≤Ip,q(w)≤1−d1
q−1
p, và Ip,q(w)lớn hơn cho biết vector thưa hơn.
Hơn nữa, Ip,q(w)thỏa mãn tất cả sáu thuộc tính (D1)-(D4) và (P1), (P2).

Nhận xét 1 (Kiểm tra tính hợp lý). Đối với tình huống dày đặc nhất hoặc bằng nhau nhất, chúng ta có wi=cvới i= 1, . . . , d ,
trong đó clà một hằng số khác không. Có thể xác minh rằng Ip,q(w) = 0 . Ngược lại, trường hợp thưa nhất hoặc không bằng nhau nhất là wi's đều bằng không trừ một trong số chúng, và Ip,q(w) = 1−d1
q−1
ptương ứng.

4

--- TRANG 5 ---
Lưu ý rằng I(w)cho một vector toàn số không không được định nghĩa. Từ quan điểm số lượng phần tử quan trọng, một vector toàn số không là thưa; tuy nhiên, nó dày đặc từ khía cạnh phân phối năng lượng.

Nhận xét 2 (Thông hiểu). Dạng của Ip,qkhông phải là một suy nghĩ ngẫu nhiên mà được thúc đẩy bản chất bởi các thuộc tính (D1)-(D4). Tại sao chúng ta cần tỷ lệ của hai chuẩn? Nó về cơ bản được quyết định bởi yêu cầu của (D2) Scaling. Nếu S(w)chỉ liên quan đến một chuẩn duy nhất, thì S(w)không bất biến tỷ lệ. Tuy nhiên, vì ℓr-norm là đồng nhất với tất cả r >0, tỷ lệ của hai chuẩn về bản chất là bất biến tỷ lệ. Tại sao có một hằng số tỷ lệ bổ sung d1
q−1
p? Điều này là cần thiết để thỏa mãn (D4) Cloning. Được lấy cảm hứng từ Root Mean Squared Error (RMSE) nổi tiếng, chúng tôi phát hiện ra rằng hằng số tỷ lệ bổ sung là thuật ngữ chính xác để giúp Ip,qđộc lập với chiều dài vector. Nó về cơ bản hấp dẫn để so sánh độ thưa của mạng nơron với các tham số mô hình khác nhau. Tại sao chúng ta yêu cầu p < q ? Chúng tôi thấy nó đóng vai trò trung tâm trong việc đáp ứng (D1) và (D3). Thông hiểu là ∥w∥pgiảm nhanh hơn ∥w∥qkhi một vector trở nên thưa hơn, do đó đảm bảo Chỉ số PQ lớn hơn.

Định lý 2 (Ràng buộc PQI về cắt tỉa). Cho Mrbiểu thị tập hợp rchỉ số của wvới độ lớn lớn nhất, và ηrlà giá trị nhỏ nhất sao choP
i/∈Mr|wi|p≤ηrP
i∈Mr|wi|p.Khi đó, chúng ta có
r≥d(1 +ηr)−q/(q−p)[1−I(w)]qp
q−p. (2)

Nhận xét 3. Ràng buộc PQI được lấy cảm hứng từ Yang et al. (2022) đã đề xuất sử dụng ∥w∥1/∥w∥q, q∈
(0,1)như một thước đo độ thưa. Chúng tôi sử dụng các kỹ thuật tương tự để dẫn xuất ràng buộc dựa trên Chỉ số PQ được đề xuất của chúng tôi. Đáng chú ý rằng ∥w∥1/∥w∥qkhông thỏa mãn các thuộc tính (D2), (D4), (P1), và (P4), mà một thước đo độ thưa lý tưởng nên có Hurley & Rickard (2009). Do đó, chúng ta không thể sử dụng nó để so sánh độ thưa của các mô hình có kích thước khác nhau. Ưu điểm của ràng buộc PQI là nó áp dụng cho việc cắt tỉa lặp các mô hình liên quan đến số lượng tham số mô hình khác nhau.

Nhớ lại rằng việc cắt tỉa dựa trên giả định rằng các tham số có độ lớn nhỏ có thể loại bỏ được. Do đó, giả sử chúng ta biết I(w)vàηr, thì chúng ta ngay lập tức có một ràng buộc dưới cho tỷ lệ giữ lại của việc cắt tỉa từ Định lý 2. Do đó, chúng ta có thể chọn tỷ lệ cắt tỉa một cách thích ứng dựa trên I(w)vàηr, điều này thúc đẩy thuật toán Cắt tỉa Thích ứng dựa trên Độ thưa (SAP) của chúng tôi trong phần tiếp theo. Trong thực tế, ηrkhông có sẵn trước khi chúng ta quyết định tỷ lệ cắt tỉa. Do đó, chúng tôi coi nó như một siêu tham số trong các thí nghiệm của chúng tôi. Vì ηrkhông tăng theo r, một ηrlớn hơn có nghĩa là chúng ta giả định mô hình có thể nén được nhiều hơn và dẫn đến tỷ lệ cắt tỉa cao hơn. Các thí nghiệm cho thấy rằng việc chọn ηr= 0là an toàn.

3.2 CẮT TỈA THÍCH ỨNG DỰA TRÊN ĐỘ THƯA

Trong phần này, chúng tôi giới thiệu thuật toán Cắt tỉa Thích ứng dựa trên Độ thưa (SAP) như được minh họa trong Thuật toán 1. Thuật toán của chúng tôi dựa trên phương pháp cắt tỉa vé số nổi tiếng (Frankle & Carbin, 2018). Thuật toán cắt tỉa vé số đề xuất cắt tỉa và huấn luyện lại mô hình một cách lặp đi lặp lại. So với thuật toán cắt tỉa một lần, không huấn luyện lại mô hình tại mỗi lần lặp cắt tỉa, thuật toán cắt tỉa vé số tạo ra các mô hình được cắt tỉa có hiệu suất tốt hơn với cùng tỷ lệ phần trăm tham số mô hình còn lại. Tuy nhiên, cả hai phương pháp đều sử dụng tỷ lệ cắt tỉa cố định Ptại mỗi lần lặp cắt tỉa. Kết quả là, chúng có thể cắt tỉa thiếu hoặc cắt tỉa quá mức mô hình tại các lần lặp cắt tỉa sớm hơn hoặc muộn hơn, tương ứng. Các mô hình được cắt tỉa thiếu với khả năng nén dự phòng đòi hỏi nhiều lần lặp cắt tỉa và tài nguyên tính toán hơn để có được các mạng nơron nhỏ nhất với hiệu suất thỏa đáng. Mô hình được cắt tỉa quá mức gặp phải underfitting do khả năng nén không đủ để duy trì hiệu suất mong muốn. Do đó, chúng tôi đề xuất SAP để xác định một cách thích ứng số lượng tham số được cắt tỉa tại mỗi lần lặp dựa trên ràng buộc PQI được dẫn xuất trong Công thức 2. Hơn nữa, chúng tôi giới thiệu hai siêu tham số bổ sung, hệ số tỷ lệ γvà tỷ lệ cắt tỉa tối đa β, để làm cho thuật toán của chúng tôi linh hoạt và có thể áp dụng hơn nữa.

Tiếp theo, chúng tôi đi qua thuật toán SAP của chúng tôi. Trước khi việc cắt tỉa bắt đầu, chúng tôi tạo ngẫu nhiên các tham số mô hình winit. Chúng tôi sẽ sử dụng winitđể khởi tạo các tham số mô hình được giữ lại tại mỗi lần lặp cắt tỉa. Thuật toán cắt tỉa vé số cho thấy rằng hiệu suất của việc huấn luyện lại các mạng con từ winittốt hơn từ đầu. Sau đó, chúng tôi khởi tạo mặt nạ m0với tất cả số một. Giả sử chúng ta có Tsố lần lặp cắt tỉa. Đối với mỗi lần lặp cắt tỉa t= 0,1,2, . . . T , trước tiên chúng tôi khởi tạo các tham số mô hình ˜wtvà tính số lượng tham số mô hình dtnhư sau
˜wt=winit⊙mt, d t=|mt|, (3)
trong đó ⊙là tích Hadamard. Sau khi huấn luyện các tham số mô hình ˜wtvớimtbằng cách đóng băng gradient trong Eepoch, chúng tôi đến các tham số mô hình được huấn luyện wt. Tại thời điểm này, thuật toán của chúng tôi không

5

--- TRANG 6 ---
khác gì thuật toán cắt tỉa vé số cổ điển. Thuật toán cắt tỉa vé số sau đó sẽ cắt tỉa d·Ptham số từ mttheo độ lớn của wtvà cuối cùng tạo mặt nạ mới mt+1.

Sau khi đến wt, SAP được đề xuất của chúng tôi sẽ tính Chỉ số PQ, được ký hiệu bởi I(wt), và ràng buộc dưới của số lượng tham số mô hình được huấn luyện lại, được ký hiệu bởi rt, như sau
I(wt) = 1−d1
q−1
p
t∥wt∥p
∥wt∥q, r t=dt(1 +ηr)−q/(q−p)[1−I(wt)]qp
q−p. (4)

Sau đó, chúng tôi tính số lượng tham số mô hình được cắt tỉa ct=⌊dt·min(γ(1−rt
dt), β)⌋.Ở đây, chúng tôi giới thiệu γđể tăng tốc hoặc giảm tốc việc cắt tỉa tại các lần lặp cắt tỉa ban đầu. Cụ thể, γ >1
hoặc<1khuyến khích tỷ lệ cắt tỉa lớn hơn hoặc nhỏ hơn tỷ lệ cắt tỉa được dẫn xuất từ ràng buộc PQI, tương ứng. Vì việc huấn luyện lại các mô hình được cắt tỉa tốn thời gian, việc có được hiệu quả mô hình được cắt tỉa nhỏ nhất với hiệu suất thỏa đáng cho một số lượng nhỏ lần lặp cắt tỉa là hấp dẫn. Ngoài ra, chúng tôi giới thiệu tỷ lệ cắt tỉa tối đa βđể tránh việc cắt tỉa quá mức vì chúng tôi sẽ nén nhiều hơn ràng buộc PQI nếu γ >1và có thể cắt tỉa hoàn toàn tất cả tham số mô hình. Nếu chúng ta đặt γ= 1,βcó thể được bỏ qua một cách an toàn. Trong các thí nghiệm của chúng tôi, chúng tôi đặt β= 0.9chỉ để cung cấp bảo vệ tối thiểu khỏi việc cắt tỉa quá mức tại mỗi lần lặp cắt tỉa. Cuối cùng, chúng tôi cắt tỉa cttham số mô hình với độ lớn nhỏ nhất dựa trên wtvàmt, và cuối cùng tạo mặt nạ mới mt+1cho lần lặp cắt tỉa tiếp theo.

4 NGHIÊN CỨU THỰC NGHIỆM

4.1 THIẾT LẬP THỰC NGHIỆM

Chúng tôi tiến hành thí nghiệm với các bộ dữ liệu FashionMNIST (Xiao et al., 2017), CIFAR10, CIFAR100 (Krizhevsky et al., 2009), và TinyImageNet (Le & Yang, 2015). Các mô hình backbone của chúng tôi là Linear, Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), ResNet18, ResNet50 (He et al., 2016a), và Wide ResNet28x8 (WResNet28x8) (Zagoruyko & Komodakis, 2016). Chúng tôi chạy thí nghiệm cho T= 30 lần lặp cắt tỉa với Linear, MLP, và CNN, và T= 15 lần lặp cắt tỉa với ResNet18. Chúng tôi so sánh SAP được đề xuất với hai baseline, bao gồm thuật toán cắt tỉa 'One Shot' và 'Lottery Ticket' (Frankle & Carbin, 2018). Sự khác biệt giữa thuật toán cắt tỉa 'One Shot' và 'Lottery Ticket' là 'One Shot' cắt tỉa d·Ptham số mô hình tại mỗi lần lặp cắt tỉa từ w0thay vì wt. Chúng tôi có P= 0.2trong suốt các thí nghiệm của chúng tôi. Chúng tôi so sánh Chỉ số PQ được đề xuất (p= 0.5,q= 1.0) với Chỉ số Gini nổi tiếng (Gini, 1912) để xác thực hiệu quả của nó trong việc đánh giá độ thưa. Hơn nữa, chúng tôi thực hiện cắt tỉa trên nhiều phạm vi cắt tỉa khác nhau, bao gồm 'Neuron-wise Pruning,' 'Layer-wise Pruning,' và 'Global Pruning.' Cụ thể, 'Global Pruning' thu thập tất cả tham số mô hình như một vector để cắt tỉa, trong khi 'Neuron-wise Pruning'

6

--- TRANG 7 ---
(a) Mô hình được huấn luyện lại
(b) Mô hình được cắt tỉa
Hình 2: Kết quả của (a) mô hình được huấn luyện lại và (b) mô hình được cắt tỉa tại mỗi lần lặp cắt tỉa cho 'Global Pruning' với FashionMNIST và MLP, trong đó (a) được lấy từ các mô hình sau khi huấn luyện lại và (b) trực tiếp từ những mô hình sau khi cắt tỉa.

(a) Mô hình được huấn luyện lại
(b) Mô hình được cắt tỉa
Hình 3: Kết quả của (a) mô hình được huấn luyện lại và (b) mô hình được cắt tỉa tại mỗi lần lặp cắt tỉa cho 'Global Pruning' với CIFAR10 và ResNet18.

và 'Layer-wise Pruning' cắt tỉa từng nơron và lớp tham số mô hình riêng biệt. Số lượng nơron tại mỗi lớp bằng kích thước đầu ra của lớp đó. Ví dụ, phương pháp 'One Shot' và 'Lottery Ticket với 'Neuron-wise Pruning' cắt tỉa di·Ptham số mô hình của mỗi nơron, trong đó dịđề cập đến kích thước của mỗi nơron. Tương tự, SAP tính Chỉ số PQ và số lượng tham số mô hình được cắt tỉa cho nơron i. Chi tiết về kiến trúc mô hình và siêu tham số học được bao gồm trong Phụ lục. Chúng tôi tiến hành bốn thí nghiệm ngẫu nhiên với các seed khác nhau, và độ lệch chuẩn được hiển thị trong thanh lỗi của tất cả các hình. Kết quả thí nghiệm thêm có thể được tìm thấy trong Phụ lục.

4.2 KẾT QUẢ THỰC NGHIỆM

Mô hình được huấn luyện lại Chúng tôi chứng minh kết quả của các mô hình được huấn luyện lại (wt⊙mt) tại mỗi lần lặp cắt tỉa trong Hình 2(a) và 3(a). Cụ thể, chúng tôi minh họa hiệu suất, phần trăm trọng số còn lại, Chỉ số PQ, và Chỉ số Gini tại mỗi lần lặp cắt tỉa. Trong những thí nghiệm này, ηrvàγđược đặt thành 0và 1để ngăn ngừa sự can thiệp. Phương pháp 'Lottery Ticket' vượt trội hơn 'One Shot' như mong đợi. SAP (p= 1.0,q= 2.0) nén mạnh mẽ hơn SAP (p= 0.5,q= 1.0). Nhớ lại rằng SAP điều chỉnh tỷ lệ cắt tỉa một cách thích ứng dựa trên độ thưa của các mô hình, trong khi 'One Shot' và 'Lottery Ticket' có tỷ lệ cắt tỉa cố định tại mỗi lần lặp cắt tỉa. Trong Hình 2(a), SAP (p= 1.0,q= 2.0) tại khoảng T= 10 đạt được hiệu suất tương tự như 'Lottery Ticket' tại khoảng T= 25 . Hơn nữa, SAP (p= 0.5,q= 1.0) và 'Lottery Ticket' có tỷ lệ cắt tỉa tương tự trước T= 10 , nhưng SAP (p= 0.5,q= 1.0) thích ứng giảm tỷ lệ cắt tỉa và do đó ngăn ngừa hiệu suất khỏi suy giảm như 'Lottery Ticket' làm sau T= 25 . Trong Hình 3(a), chúng tôi quan sát hiện tượng cắt tỉa nhanh tương tự của SAP (p= 1.0,q= 2.0). Trong khi đó, SAP (p= 0.5,q= 1.0) hoạt động tương tự như 'Lottery Ticket' nhưng cắt tỉa nhiều hơn 'Lottery Ticket' làm. Do đó, bằng cách lựa chọn cẩn thận p

7

--- TRANG 8 ---
vàq, SAP có thể cung cấp việc cắt tỉa hiệu quả và mạnh mẽ hơn. Đối với độ thưa của các mô hình được huấn luyện lại, kết quả của Chỉ số PQ phù hợp với Chỉ số Gini, điều này chứng minh thực nghiệm rằng Chỉ số PQ có thể đo hiệu quả độ thưa của các tham số mô hình. Hơn nữa, động lực của độ thưa cũng xác nhận giả thuyết của chúng tôi, như được hiển thị bởi 'One Shot' trong Hình 2(a) và SAP (p= 1.0,q= 2.0) trong Hình 3(a). Kết quả cho thấy rằng một quy trình cắt tỉa lý tưởng nên tránh sự gia tăng nhanh chóng trong độ thưa.

Mô hình được cắt tỉa Chúng tôi chứng minh kết quả của các mô hình được cắt tỉa tại mỗi lần lặp cắt tỉa trong Hình 2(b) và 3(b). Cụ thể, chúng tôi minh họa hiệu suất, chênh lệch hiệu suất, Chỉ số PQ, và chênh lệch Chỉ số PQ tại mỗi lần lặp cắt tỉa. Hiệu suất và Chỉ số PQ được tính trực tiếp từ các mô hình được cắt tỉa mà không cần huấn luyện lại. Chênh lệch hiệu suất và Chỉ số PQ là giữa các mô hình được huấn luyện lại (w0⊙mtcho 'One Shot' và wt⊙mtcho 'Lottery Ticket' và SAP) và các mô hình được cắt tỉa (w0⊙mt+1cho 'One Shot' và wt⊙mt+1cho 'Lottery Ticket' và SAP). Kết quả của chênh lệch hiệu suất cho thấy rằng hiệu suất của các mô hình được cắt tỉa mà không cần huấn luyện lại từ SAP có thể hoạt động gần với các mô hình được huấn luyện lại. Hơn nữa, độ thưa của các mô hình được cắt tỉa cho thấy rằng việc cắt tỉa lặp thường giảm độ thưa của các mô hình được cắt tỉa. Trong khi đó, chênh lệch độ thưa của các mô hình được cắt tỉa cung cấp một kiểm tra tính hợp lý bằng cách cho thấy việc cắt tỉa tại mỗi lần lặp cắt tỉa giảm độ thưa của các mô hình được huấn luyện lại.

Phạm vi cắt tỉa Chúng tôi chứng minh nhiều phạm vi cắt tỉa khác nhau liên quan đến sự đánh đổi nén, phần trăm trọng số còn lại theo lớp, và Chỉ số PQ theo lớp trong Hình 4. Kết quả của sự đánh đổi nén cho thấy rằng SAP với 'Global Pruning' có thể hoạt động tệ hơn 'One Shot' và 'Lottery Ticket' khi phần trăm trọng số còn lại nhỏ. Như được minh họa trong 'Global Pruning' của Hình 4(b), lớp đầu tiên chưa được cắt tỉa đủ. Đó là vì SAP với 'Global Pruning' đo độ thưa của tất cả tham số mô hình trong một vector, và độ lớn của các tham số của lớp đầu tiên và các lớp khác có thể không ở cùng quy mô. Kết quả là, các tham số của lớp đầu tiên sẽ không được cắt tỉa cho đến các lần lặp cắt tỉa muộn. Tuy nhiên, SAP với 'Neuron-wise Pruning'

8

--- TRANG 9 ---
(a) Sự đánh đổi nén
(b) Phần trăm trọng số còn lại theo lớp
(c) Chỉ số PQ theo lớp
Hình 4: Kết quả của nhiều phạm vi cắt tỉa khác nhau liên quan đến (a) sự đánh đổi nén, (b) phần trăm trọng số còn lại theo lớp, và (c) Chỉ số PQ theo lớp cho CIFAR10 và CNN. (b, c) được thực hiện với SAP (p= 0.5,q= 1.0).

và 'Layer-wise Pruning' hoạt động tốt hơn 'One Shot' và 'Lottery Ticket.' SAP có thể điều chỉnh tỷ lệ cắt tỉa của mỗi nơron và lớp một cách thích ứng, nhưng 'One Shot' và 'Lottery Ticket' có thể cắt tỉa quá mức các nơron và lớp cụ thể vì họ áp dụng tỷ lệ cắt tỉa cố định. Thú vị thay, các tham số của lớp đầu tiên được cắt tỉa mạnh mẽ hơn bởi 'Neuron-wise Pruning' so với 'Global Pruning' trong các lần lặp cắt tỉa sớm. Tuy nhiên, chúng không được cắt tỉa bởi 'Neuron-wise Pruning' trong các lần lặp cắt tỉa muộn, trong khi 'Global Pruning' vẫn cắt tỉa chúng mạnh mẽ. Điều này phù hợp với trực giác rằng các lớp ban đầu của CNN quan trọng hơn để duy trì hiệu suất, ví dụ, Gale et al. (2019) quan sát rằng lớp đầu tiên thường quan trọng hơn đối với chất lượng mô hình và được cắt tỉa ít hơn các lớp khác. Hơn nữa, Chỉ số PQ của 'Neuron-wise Pruning' cũng ổn định hơn hai phạm vi cắt tỉa khác, cho thấy 'Neuron-wise Pruning' phù hợp hơn cho SAP, vì Chỉ số PQ được tính chính xác hơn.

Nghiên cứu ablation Chúng tôi chứng minh các nghiên cứu ablation của pvàqtrong Hình 5. Trong Hình 5(a), chúng tôi cố định q= 1.0và nghiên cứu ảnh hưởng của p. Kết quả cho thấy rằng SAP cắt tỉa mạnh mẽ hơn khi q= 1.0vàpgần với q. Trong Hình 5(b), chúng tôi cố định p= 1.0và nghiên cứu ảnh hưởng của q. Kết quả cho thấy rằng SAP cắt tỉa mạnh mẽ hơn khi p= 1.0vàqcách xa p. Chúng tôi chứng minh các nghiên cứu ablation của ηrvàγtrong Hình 6. Trong Hình 6(a), chúng tôi cố định γ= 1.0và nghiên cứu ảnh hưởng của ηr. Kết quả cho thấy rằng SAP cắt tỉa mạnh mẽ hơn khi ηr>0. Trong Hình 6(b), chúng tôi cố định ηr= 0.0và nghiên cứu ảnh hưởng của γ. Kết quả cho thấy rằng SAP cắt tỉa mạnh mẽ hơn khi γ > 1. Thú vị thay, hiệu suất của kết quả chúng tôi đại khái tuân theo mô hình suy giảm logistic do tỷ lệ cắt tỉa thích ứng, và điểm uốn tương ứng với đỉnh của thước đo độ thưa. Hơn nữa, động lực của thước đo độ thưa của SAP với nhiều nghiên cứu ablation khác nhau cũng xác nhận giả thuyết của chúng tôi.

5 KẾT LUẬN

Chúng tôi đã đề xuất một khái niệm mới về độ thưa cho các vector được gọi là Chỉ số PQ (PQI), tuân theo các nguyên tắc mà một thước đo độ thưa nên tuân theo. Chúng tôi phát triển một quan điểm mới về khả năng nén của mạng nơron bằng cách đo độ thưa của các mô hình được cắt tỉa. Chúng tôi đưa ra một giả thuyết về mối quan hệ

9

--- TRANG 10 ---
giữa độ thưa và khả năng nén của mạng nơron. Được thúc đẩy bởi PQI được đề xuất và giả thuyết của chúng tôi, chúng tôi tiếp tục phát triển thuật toán Cắt tỉa Thích ứng dựa trên Độ thưa (SAP) sử dụng PQI để chọn tỷ lệ cắt tỉa một cách thích ứng. Kết quả thí nghiệm của chúng tôi chứng minh rằng SAP có thể nén hiệu quả và mạnh mẽ hơn các thuật toán tiên tiến.

LỜI CẢM ƠN

Công trình này được hỗ trợ một phần bởi Văn phòng Nghiên cứu Hải quân (ONR) theo số hiệu tài trợ N00014-21-1-2590.

TÀI LIỆU THAM KHẢO

Mehmet Akçakaya and Vahid Tarokh. A frame construction and a universal distortion bound for sparse representations. IEEE Transactions on Signal Processing , 56(6):2443–2450, 2008.

Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In Proc. ICML , pp. 254–263, 2018.

Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory , 39(3):930–945, 1993.

Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, and Daniela Rus. Data-dependent coresets for compressing neural networks with applications to generalization bounds. arXiv preprint arXiv:1804.05345 , 2018.

Alexander M Bronstein, Michael M Bronstein, Michael Zibulevsky, and Yehoshua Y Zeevi. Sparse ica for blind separation of transmitted and reflected images. International Journal of Imaging Systems and Technology , 15(1):84–91, 2005.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Proc. NeurIPS , 33:1877–1901, 2020.

Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic decomposition by basis pursuit. SIAM review , 43(1):129–159, 2001.

Hugh Dalton. The measurement of the inequality of incomes. The Economic Journal , 30(119): 348–361, 1920.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.

Enmao Diao, Jie Ding, and Vahid Tarokh. Restricted recurrent neural networks. In IEEE International Conference on Big Data , pp. 56–63. IEEE, 2019.

Enmao Diao, Jie Ding, and Vahid Tarokh. HeteroFL: Computation and communication efficient federated learning for heterogeneous clients. International Conference on Learning Representations (ICLR) , 2021.

Enmao Diao, Jie Ding, and Vahid Tarokh. GAL: Gradient assisted learning for decentralized multi-organization collaborations. Conference on Neural Information Processing Systems (NeurIPS) , 2022.

Jie Ding, Vahid Tarokh, and Yuhong Yang. Model selection techniques: An overview. IEEE Signal Processing Magazine , 35(6):16–34, 2018.

Jie Ding, Eric Tramel, Anit Kumar Sahu, Shuang Wu, Salman Avestimehr, and Tao Zhang. Federated learning challenges and opportunities: An outlook. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 8752–8756. IEEE, 2022.

David L Donoho. Compressed sensing. IEEE Transactions on Information Theory , 52(4):1289–1306, 2006.

10

--- TRANG 11 ---
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning , pp. 2943–2952. PMLR, 2020.

Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 , 2018.

Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574 , 2019.

Corrado Gini. Variabilità e mutabilità: contributo allo studio delle distribuzioni e delle relazioni statistiche.[Fasc. I.] . Tipogr. di P. Cuppini, 1912.

Masafumi Hagiwara. Removal of hidden units and weights for back propagation networks. In Proc. IJCNN , volume 1, pp. 351–354, 1993.

Song Han, Huizi Mao, and William Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 , 2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770–778, 2016a.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Proc. ECCV , pp. 630–645. Springer, 2016b.

Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. InProc. ICCV , pp. 1389–1397, 2017.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 , 2015.

Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. J. Mach. Learn. Res. , 22(241):1–124, 2021.

Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven neuron pruning approach towards efficient deep architectures. arXiv preprint arXiv:1607.03250 , 2016.

Niall Hurley and Scott Rickard. Comparing measures of sparsity. IEEE Transactions on Information Theory , 55(10):4723–4741, 2009.

Lou Jost. Entropy and diversity. Oikos , 113(2):363–375, 2006.

Jakub Kone ˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492 , 2016.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Proc. NeurIPS , 25, 2012.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.. imagenet classification with deep convolutional neural networks. Communications of the ACM , 60(6):84–90, 2017.

Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N , 7(7):3, 2015.

Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Proc. NeurIPS , 2, 1989.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proc. IEEE , 86(11):2278–2324, 1998.

11

--- TRANG 12 ---
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.

Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983 , 2016.

Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In Proc. ICCV , pp. 5058–5066, 2017.

Erum Mushtaq, Chaoyang He, Jie Ding, and Salman Avestimehr. Spider: Searching personalized neural architecture for federated learning. arXiv preprint arXiv:2112.13939 , 2021.

Ben Mussay, Margarita Osadchy, Vladimir Braverman, Samson Zhou, and Dan Feldman. Data-independent neural pruning via coresets. arXiv preprint arXiv:1907.04018 , 2019.

Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499 , 2016.

Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.

Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proc. CVPR , pp. 779–788, 2016.

Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in neural network pruning. arXiv preprint arXiv:2003.02389 , 2020.

Scott Rickard and Maurice Fallon. The gini index of speech. In Proceedings of the 38th Conference on Information Science and Systems (CISS'04) , 2004.

Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training for speech recognition. arXiv preprint arXiv:1904.05862 , 2019.

Mohammadreza Soltani, Suya Wu, Jie Ding, Robert Ravier, and Vahid Tarokh. On the information of feature maps and pruning of deep neural networks. In Proc. ICPR , pp. 6988–6995, 2021.

Robert Tibshirani. Regression shrinkage and selection via the lasso. J. Royal Stat. Soc. B , 58(1): 267–288, 1996.

Jianyou Wang, Michael Xue, Ryan Culhane, Enmao Diao, Jie Ding, and Vahid Tarokh. Speech emotion recognition with dual-sequence lstm architecture. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 6474–6478. IEEE, 2020.

Zhan Wang, Sandra Paterlini, Fuchang Gao, and Yuhong Yang. Adaptive minimax regression estimation over sparse lq-hulls. J. Mach. Learn. Res. , 15(1):1675–1711, 2014.

Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.

Wenjing Yang, Ganghua Wang, Jie Ding, and Yuhong Yang. A theoretical understanding of neural network compression from sparse linear approximation. arXiv preprint arXiv:2206.05604 , 2022.

Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good subnetworks provably exist: Pruning via greedy forward selection. In Proc. ICML , pp. 10820–10830, 2020.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 , 2016.

12

--- TRANG 13 ---
Phụ lục

A HẠN CHẾ VÀ CÔNG VIỆC TƯƠNG LAI

Lý thuyết Chúng tôi tận dụng Chỉ số PQ được đề xuất và dẫn xuất ràng buộc PQI để chỉ ra số lượng tham số được giữ lại r. Kết quả của chúng tôi là ràng buộc dưới cho r, chỉ gợi ý số lượng tham số mô hình tối đa mà chúng ta nên cắt tỉa. Một công việc tương lai tiềm năng là phát triển ràng buộc trên cho rnhằm chúng ta có thể hiểu rõ hơn mối quan hệ giữa độ thưa và cắt tỉa. Hơn nữa, kết quả của chúng tôi giới thiệu một thuật ngữ bổ sung ηr, không có sẵn trước khi xác định tỷ lệ cắt tỉa. Chúng tôi coi nó như một siêu tham số trong thuật toán và thí nghiệm của chúng tôi. Tuy nhiên, việc phát triển một ràng buộc chặt chẽ hơn mà không có xấp xỉ như vậy là mong muốn. Ngoài ra, cách pvàqcùng nhau ảnh hưởng đến thước đo độ thưa và cắt tỉa mô hình chưa được phân tích kỹ lưỡng. Cuối cùng, sự biện minh lý thuyết của giả thuyết được đề xuất còn thiếu.

Phương pháp Giả thuyết được xác nhận của chúng tôi chỉ ra rằng một mối quan hệ động tồn tại giữa độ thưa và khả năng nén của mô hình. Tuy nhiên, thuật toán SAP được đề xuất của chúng tôi xác định số lượng tham số được cắt tỉa dựa trên ràng buộc PQI tĩnh tại mỗi lần lặp. Do đó, một công việc tương lai tiềm năng là tiếp tục phát triển thuật toán SAP bằng cách xem xét động lực của độ thưa. Ví dụ, dừng cắt tỉa khi PQI bắt đầu tăng hoặc tỷ lệ cắt tỉa dưới một ngưỡng nào đó. Trong công việc này, chúng tôi chứng minh mối quan hệ giữa hiệu suất của việc cắt tỉa lặp và động lực của PQI. Thật thú vị khi phân tích các yếu tố quan trọng có thể xác định động lực như vậy, chẳng hạn như khởi tạo và kiến trúc mô hình. Các công trình gần đây cũng giới thiệu cắt tỉa độ lớn dần dần có thể vượt trội hơn thuật toán cắt tỉa lặp Gale et al. (2019); Renda et al. (2020). Do đó, cũng thú vị khi nghiên cứu cách PQI liên quan đến các phương pháp cắt tỉa khác nhau Evci et al. (2020); Hoefler et al. (2021). Chúng tôi chứng minh rằng SAP với sự lựa chọn thích hợp về siêu tham số có thể vượt trội hơn LT. Thật thú vị khi khám phá khi nào SAP có thể vượt trội hơn LT đối với sự đánh đổi độ chính xác-nén.

Ứng dụng Chúng tôi áp dụng Chỉ số PQ được đề xuất cho việc nén mô hình vì nén mô hình là một trong những chủ đề quan trọng nhất liên quan đến độ thưa. Tuy nhiên, nhiều chủ đề thú vị khác có thể tận dụng Chỉ số PQ. Ví dụ, người ta có thể xem xét tối ưu hóa trực tiếp hàm mục tiêu và Chỉ số PQ cùng nhau để điều chỉnh. Hơn nữa, công bằng ủng hộ hiệu suất tương tự của các nhóm khác nhau cũng có thể hưởng lợi từ Chỉ số PQ của chúng tôi. Cuối cùng, các lĩnh vực khác sử dụng Chỉ số Gini, như xã hội học và kinh tế học, cũng có thể thấy chỉ số thay thế được đề xuất thú vị.

B PHÂN TÍCH LÝ THUYẾT

Chứng minh Định lý 1 :

Phạm vi của PQI. Lưu ý rằng với bất kỳ 0< p < q , bất đẳng thức Hölder cho rằng
∥w∥q≤ ∥w∥p≤d1
p−1
q∥w∥q.

Chúng ta ngay lập tức có được I (w)∈[0,1−d1
q−1
p]từ bất đẳng thức trên.

Thuộc tính. Tiếp theo, chúng tôi chứng minh rằng Ithỏa mãn sáu thuộc tính, có nghĩa là I(w)lớn hơn nếu wthưa hơn. Hurley & Rickard (2009) chứng minh rằng (P1) và (P2) được thỏa mãn tự động miễn là (D1)-(D4) được đáp ứng. Hơn nữa, chúng tôi lưu ý rằng f(x) = 1−1/xlà đơn điệu với x >0. Do đó, chúng tôi chỉ phải chứng minh (D1)-(D4) đúng cho S(w) =d1
p−1
q∥w∥q/∥w∥p.

(D2) Scaling. Nó được thỏa mãn tự động cho S(w)vì ℓq-norm đồng nhất với bất kỳ q >0.

(D4) Cloning. Rõ ràng từ định nghĩa của S(w).

(D1) Robin Hood. . Không mất tính tổng quát, chúng tôi chỉ cần chứng minh rằng với w1> w 2>0, đạo hàm của f(t)tại t= 0là âm, trong đó
f(t) =
(w1−t)q+ (w2+t)q+Pd
i>2wq
i	1/q

(w1−t)p+ (w2+t)p+Pd
i>2wp
i	1/p.

13

--- TRANG 14 ---
Đạo hàm được cho bởi
f′(0) = f(0)−wq−1
1+wq−1
2Pd
i=1wq
i−−wp−1
1+wp−1
2Pd
i=1wp
i
. (5)

Rõ ràng rằng f′(0)<0khi 0< p≤1≤qvàp̸=q, lưu ý rằng w1> w 2>0.

Tiếp theo, chúng tôi muốn chỉ ra rằng f′(0)có thể dương khi điều kiện 0< p≤1< qbị vi phạm, do đó (D1) không được thỏa mãn. Chúng tôi chứng minh tuyên bố này bằng phản chứng.

Trước tiên chúng tôi xem xét trường hợp 0< p < q < 1. Nếuf′(0)<0luôn đúng, bởi Eq. (5), chúng ta có rằng
g(t) :=−wt−1
1+wt−1
2Pd
i=1wt
i

là một hàm giảm đơn điệu cho t∈[0,1]. Tức là, g′(t)<0cho t∈(0,1). Lưu ý rằng
g′(t) =(−wt−1
1lnw1+wt−1
2lnw2)(Pd
i=1wt
i)−(−wt−1
1+wt−1
2)(Pd
i=1wt
ilnwi)
(Pd
i=1wt
i)2,

tử số của nó có thể được viết thêm như tổng của hai thuật ngữ:
(S1)wt−1
1wt−1
2(w1+w2)(lnw2−lnw1),
(S2)dX
i>2wt
i{wt−1
2(lnw2−lnwi)−wt−1
1(lnw1−lnwi)}.

Thuật ngữ đầu tiên (S1) là âm vì w1> w 2. Đối với thuật ngữ thứ hai, chúng tôi định nghĩa
h(w) =wt{wt−1
2(lnw2−lnw)−wt−1
1(lnw1−lnw)}.

Chúng tôi lưu ý rằng h(w)→0khi w→0vàh(w)→ −∞ khi w→ ∞ , do đó h(w)đạt giá trị tối đa khi w=w∗, trong đó w∗thỏa mãn h′(w∗) = 0 . Vì
h′(w) =twt−1{wt−1
2(lnw2−lnw)−wt−1
1(lnw1−lnw)}+wt−1(−wt−1
2+wt−1
1),

lấy h′(w∗) = 0 cho rằng
w∗= expwt−1
1lnw1−wt−1
2lnw2
−wt−1
2+wt−1
1−1/t
,
h(w∗) =1
twt
∗(wt−1
2−wt−1
1).

Vì t <1vàw1> w 2, chúng ta biết h(w∗)là dương, do đó (S2) có thể lớn tùy ý khi d→ ∞ , có nghĩa là g′(t)cũng dương, đây là một mâu thuẫn.

Tương tự, nếu (D1) đúng cho bất kỳ 1< p < q , nó có nghĩa là g′(t)<0cho bất kỳ t >1. Tuy nhiên, chúng ta có (S2) đi đến vô cùng khi t >1vàwi→ ∞ , i > 2. Do đó, g′(t)có thể dương cho t >1, dẫn đến mâu thuẫn và hoàn thành chứng minh.

(D3) Rising tide. Chúng tôi chứng minh rằng f′(t)là âm cho bất kỳ 0< p < q , trong đó
f(t) =(Pd
i=1(wi+t)q)1/q
(Pd
i=1(wi+t)p)1/p.

Chúng ta có thể xác minh rằng
f′(0) = f(0)Pd
i=1wq−1
iPd
i=1wq
i−Pd
i=1wp−1
iPd
i=1wp
i
.

Do đó, chúng tôi kết thúc chứng minh bằng cách chỉ ra rằng h(t) = (Pd
i=1wt−1
i)/(Pd
i=1wt
i)là một hàm giảm đơn điệu cho t >0. Điều này được thực hiện bằng cách chỉ ra h′(t)<0cho tất cả t >0. Thực tế, vì wi≥0vàwi's không hoàn toàn giống nhau, chúng ta biết
h′(t) =(Pd
i=1wt−1
iln(wi))(Pd
i=1wt
i)−(Pd
i=1wt−1
i)(Pd
i=1wt
iln(wi))
(Pd
i=1wt
i)2
=P
1≤i<j≤d(wj−wi)(ln(wi)−ln(wj))wt−1
iwt−1
j
(Pd
i=1wt
i)2<0.

14

--- TRANG 15 ---
Chứng minh Định lý 2 :

Nhớ lại rằng Mrlà rthành phần lớn nhất của w, và ηrlà một hằng số sao choP
i/∈Mr|wi|p≤
ηrP
i∈Mr|wi|p. Do đó,
∥w∥p=X
1≤i≤d|wi|p1
p
=X
i∈Mr|wi|p+X
i̸∈Mr|wi|p1
p
≤X
i∈Mr|wi|p+ηrX
i∈Mr|wi|p1
p
=X
i∈Mr|wi|p1
p
(1 +ηr)1
p
≤X
i∈Mr|wi|q1
q
r1
p−1
q(1 +ηr)1
p≤ ∥w∥qr1
p−1
q(1 +ηr)1
p.

Sắp xếp lại bất đẳng thức trên cho
r≥d(1 +ηr)−q/(q−p)[1−I(w)]qp
q−p.

15

--- TRANG 16 ---
C THIẾT LẬP THỰC NGHIỆM

Bảng 1 và 2 tóm tắt kiến trúc mô hình của MLP và CNN được sử dụng trong các thí nghiệm của chúng tôi. Bảng 3 hiển thị thống kê kiến trúc mô hình và siêu tham số được sử dụng trong các thí nghiệm của chúng tôi.

Bảng 1: Kiến trúc mô hình của Multi-Layer Perceptron (MLP) được sử dụng trong các thí nghiệm của chúng tôi. nc, H, Wđại diện cho hình dạng của hình ảnh, cụ thể là số kênh hình ảnh, chiều cao và chiều rộng. Klà số lớp trong tác vụ phân loại. Các lớp ReLU theo sau các lớp Linear(kích thước kênh đầu vào, kích thước kênh đầu ra), ngoại trừ lớp cuối cùng.

Hình ảnh x∈Rnc×H×W
Linear( nc×H×W, 128)
Linear(128, 256)
Linear(256, K)

Bảng 2: Kiến trúc mô hình của Convolutional Neural Networks (CNN) được sử dụng trong các thí nghiệm của chúng tôi. nc, H, Wđại diện cho hình dạng của hình ảnh, cụ thể là số kênh hình ảnh, chiều cao và chiều rộng. Klà số lớp trong tác vụ phân loại. Các lớp BatchNorm và ReLU theo sau các lớp Conv2d(kích thước kênh đầu vào, kích thước kênh đầu ra, kích thước kernel, stride, padding). Lớp MaxPool2d(kích thước kênh đầu ra, kích thước kernel) giảm chiều cao và chiều rộng đi một nửa.

Hình ảnh x∈Rnc×H×W
Conv2d( nc, 64, 3, 1, 1)
MaxPool2d(64, 2)
Conv2d(64, 128, 3, 1, 1)
MaxPool2d(128, 2)
Conv2d(128, 256, 3, 1, 1)
MaxPool2d(256, 2)
Conv2d(256, 512, 3, 1, 1)
MaxPool2d(512, 2)
Global Average Pooling
Linear(512, K)

Bảng 3: Thống kê các mô hình và siêu tham số được sử dụng trong các thí nghiệm của chúng tôi để huấn luyện và cắt tỉa.

Bộ dữ liệu FashionMNIST CIFAR10
Kiến trúc mô hình Linear MLP CNN ResNet18 Linear MLP CNN ResNet18
Kích thước mô hình 7.9 K 136.1 K 1.6 M 11.2 M 30.7 K 428.9 K 1.6 M 11.2 M
FLOPS 3.9 M 67.8 M 20.1 G 114.4 G 15.4 M 214.2 M 29.4 G 139.4 G
TrainEpoch E 200
Kích thước batch 250
Optimizer SGD
Tốc độ học 1E-01
Momentum 0.9
Weight decay 5E-04
Nesterov ✓
Scheduler Cosine Annealing (Loshchilov & Hutter, 2016)
PruneT 30 15 30 15
P 0.2

16

--- TRANG 17 ---
D KẾT QUẢ THỰC NGHIỆM

D.1 CHỈ SỐ PQ

Chúng tôi trực quan hóa Chỉ số PQ của các mô hình được cắt tỉa ở quy mô toàn cục với nhiều kết hợp khác nhau của pvàq. Chúng tôi sử dụng số không để chỉ ra rằng tràn số có thể xảy ra khi p= 0.1. Lưu ý rằng chúng tôi sử dụng p= 0.5vàq= 1.0để tính Chỉ số PQ cho các hình khác. Kết quả cho thấy rằng nhiều kết hợp khác nhau của pvàqcũng xác nhận giả thuyết của chúng tôi ở các quy mô khác nhau, ví dụ (d) One Shot trong Hình 7 và (b) SAP (p= 1.0,q= 2.0) của Hình 8.

(a) SAP (𝒑=𝟎.𝟓, 𝒒=𝟏.𝟎)
(b) SAP (𝒑=𝟏.𝟎, 𝒒=𝟐.𝟎)
(c) Lottery Ticket
(d) One Shot
Hình 7: Kết quả chỉ số PQ được trực quan hóa với nhiều kết hợp khác nhau của pvàqcho FashionMNIST và MLP.

17

--- TRANG 18 ---
(a) SAP (𝒑=𝟎.𝟓, 𝒒=𝟏.𝟎)
(b) SAP (𝒑=𝟏.𝟎, 𝒒=𝟐.𝟎)
(c) Lottery Ticket
(d) One Shot
Hình 8: Kết quả chỉ số PQ được trực quan hóa với nhiều kết hợp khác nhau của pvàqcho CIFAR10 và ResNet18.

18

--- TRANG 19 ---
(a) SAP (𝒑=𝟎.𝟓,𝒒=𝟏.𝟎)
(b) SAP (𝒑=𝟏.𝟎,𝒒=𝟐.𝟎)
(c) Lottery Ticket
(d) One Shot
Hình 9: Kết quả chỉ số PQ được trực quan hóa với nhiều kết hợp khác nhau của pvàqcho CIFAR100 và WResNet28x8.

19

--- TRANG 20 ---
(a) SAP (𝒑=𝟎.𝟓,𝒒=𝟏.𝟎)
(b) SAP (𝒑=𝟏.𝟎,𝒒=𝟐.𝟎)
(c) Lottery Ticket
(d) One Shot
Hình 10: Kết quả chỉ số PQ được trực quan hóa với nhiều kết hợp khác nhau của pvàqcho TinyImageNet và ResNet50.

20

--- TRANG 21 ---
D.2 MÔ HÌNH ĐƯỢC HUẤN LUYỆN LẠI VÀ ĐƯỢC CẮT TỈA

(a) Mô hình được huấn luyện lại
(b) Mô hình được cắt tỉa
Hình 11: Kết quả của (a) mô hình được huấn luyện lại và (b) mô hình được cắt tỉa tại mỗi lần lặp cắt tỉa cho 'Global Pruning' với FashionMNIST và Linear.

(a) Mô hình được huấn luyện lại
(b) Mô hình được cắt tỉa
Hình 12: Kết quả của (a) mô hình được huấn luyện lại và (b) mô hình được cắt tỉa tại mỗi lần lặp cắt tỉa cho 'Global Pruning' với FashionMNIST và CNN.

(a) Mô hình được huấn luyện lại
(b) Mô hình được cắt tỉa
Hình 13: Kết quả của (a) mô hình được huấn luyện lại và (b) mô hình được cắt tỉa tại mỗi lần lặp cắt tỉa cho 'Global Pruning' với FashionMNIST và ResNet18.

21

--- TRANG 22 ---
(a) Mô hình được huấn luyện lại
(b) Mô hình được cắt tỉa
Hình 14: Kết quả của (a) mô hình được huấn luyện lại và (b) mô hình được cắt tỉa tại mỗi lần lặp cắt tỉa cho 'Global Pruning' với CIFAR10 và Linear.

(a) Mô hình được huấn luyện lại
(b) Mô hình được cắt tỉa
Hình 15: Kết quả của (a) mô hình được huấn luyện lại và (b) mô hình được cắt tỉa tại mỗi lần lặp cắt tỉa cho 'Global Pruning' với CIFAR10 và MLP.

(a) Mô hình được huấn luyện lại
(b) Mô hình được cắt tỉa
Hình 16: Kết quả của (a) mô hình được huấn luyện lại và (b) mô hình được cắt tỉa tại mỗi lần lặp cắt tỉa cho 'Global Pruning' với CIFAR10 và CNN.

22

--- TRANG 23 ---
(a) Mô hình được huấn luyện lại
(b)Mô hình được cắt tỉa
Hình 17: Kết quả của (a) mô hình được huấn luyện lại và (b) mô hình được cắt tỉa tại mỗi lần lặp cắt tỉa cho 'Global Pruning' với CIFAR100 và WResNet28x8.

(a) Mô hình được huấn luyện lại
(b) Mô hình được cắt tỉa
Hình 18: Kết quả của (a) mô hình được huấn luyện lại và (b) mô hình được cắt tỉa tại mỗi lần lặp cắt tỉa cho 'Global Pruning' với TinyImageNet và ResNet50.

23

--- TRANG 24 ---
D.3 PHẠM VI CẮT TỈA

(a) Sự đánh đổi nén
(b) Phần trăm trọng số còn lại theo lớp
(c) Chỉ số PQ theo lớp
Hình 19: Kết quả của nhiều phạm vi cắt tỉa khác nhau liên quan đến (a) sự đánh đổi nén, (b) phần trăm trọng số còn lại theo lớp, và (c) Chỉ số PQ theo lớp cho FashionMNIST và MLP. (b, c) được thực hiện với SAP (p= 0.5,q= 1.0).

(a) Sự đánh đổi nén
(b) Phần trăm trọng số còn lại theo lớp
(c) Chỉ số PQ theo lớp
Hình 20: Kết quả của nhiều phạm vi cắt tỉa khác nhau liên quan đến (a) sự đánh đổi nén, (b) phần trăm trọng số còn lại theo lớp, và (c) Chỉ số PQ theo lớp cho FashionMNIST và CNN. (b, c) được thực hiện với SAP (p= 0.5,q= 1.0).

24

--- TRANG 25 ---
(a) Sự đánh đổi nén
(b) Phần trăm trọng số còn lại theo lớp
(c) Chỉ số PQ theo lớp
Hình 21: Kết quả của nhiều phạm vi cắt tỉa khác nhau liên quan đến (a) sự đánh đổi nén, (b) phần trăm trọng số còn lại theo lớp, và (c) Chỉ số PQ theo lớp cho FashionMNIST và ResNet18. (b, c) được thực hiện với SAP (p= 0.5,q= 1.0).

(a) Sự đánh đổi nén
(b) Phần trăm trọng số còn lại theo lớp
(c) Chỉ số PQ theo lớp
Hình 22: Kết quả của nhiều phạm vi cắt tỉa khác nhau liên quan đến (a) sự đánh đổi nén, (b) phần trăm trọng số còn lại theo lớp, và (c) Chỉ số PQ theo lớp cho CIFAR10 và MLP. (b, c) được thực hiện với SAP (p= 0.5,q= 1.0).

25

--- TRANG 26 ---
(a) Sự đánh đổi nén
(b) Phần trăm trọng số còn lại theo lớp
(c) Chỉ số PQ theo lớp
Hình 23: Kết quả của nhiều phạm vi cắt tỉa khác nhau liên quan đến (a) sự đánh đổi nén, (b) phần trăm trọng số còn lại theo lớp, và (c) Chỉ số PQ theo lớp cho CIFAR10 và CNN. (b, c) được thực hiện với SAP (p= 0.5,q= 1.0).

(a) Sự đánh đổi nén
(b) Phần trăm trọng số còn lại theo lớp
(c) Chỉ số PQ theo lớp
Hình 24: Kết quả của nhiều phạm vi cắt tỉa khác nhau liên quan đến (a) sự đánh đổi nén, (b) phần trăm trọng số còn lại theo lớp, và (c) Chỉ số PQ theo lớp cho CIFAR10 và ResNet18. (b, c) được thực hiện với SAP (p= 0.5,q= 1.0).

26

--- TRANG 27 ---
(a) Sự đánh đổi nén
(b) Phần trăm trọng số còn lại theo lớp
(c) Chỉ số PQ theo lớp
Hình 25: Kết quả của nhiều phạm vi cắt tỉa khác nhau liên quan đến (a) sự đánh đổi nén, (b) phần trăm trọng số còn lại theo lớp, và (c) Chỉ số PQ theo lớp cho CIFAR100 và WResNet28x8. (b, c) được thực hiện với SAP (p= 0.5,q= 1.0).

(a) Sự đánh đổi nén
(b) Phần trăm trọng số còn lại theo lớp
(c) Chỉ số PQ theo lớp
Hình 26: Kết quả của nhiều phạm vi cắt tỉa khác nhau liên quan đến (a) sự đánh đổi nén, (b) phần trăm trọng số còn lại theo lớp, và (c) Chỉ số PQ theo lớp cho TinyImageNet và ResNet50. (b, c) được thực hiện với SAP (p= 0.5,q= 1.0).

27

--- TRANG 28 ---
D.4 ẢNH HƯỞNG CỦA pVÀq

(a) 𝒒=𝟏.𝟎
(b) 𝒑=𝟏.𝟎
Hình 27: Nghiên cứu ablation của pvàqcho cắt tỉa toàn cục với CIFAR10 và Linear.

(a) 𝒒=𝟏.𝟎
(b) 𝒑=𝟏.𝟎
Hình 28: Nghiên cứu ablation của pvàqcho cắt tỉa toàn cục với CIFAR10 và MLP.

(a) 𝒒=𝟏.𝟎
(b) 𝒑=𝟏.𝟎
Hình 29: Nghiên cứu ablation của pvàqcho cắt tỉa toàn cục với CIFAR10 và ResNet18.

28

--- TRANG 29 ---
D.5 ẢNH HƯỞNG CỦA ηrVÀγ

(b) 𝜼𝒓=𝟎.𝟎(a) 𝜸=𝟏.𝟎
Hình 30: Nghiên cứu ablation của ηrvàγcho cắt tỉa toàn cục với CIFAR10 và Linear.

(b) 𝜼𝒓=𝟎.𝟎(a) 𝜸=𝟏.𝟎
Hình 31: Nghiên cứu ablation của ηrvàγcho cắt tỉa toàn cục với CIFAR10 và MLP.

(b) 𝜼𝒓=𝟎.𝟎(a) 𝜸=𝟏.𝟎
Hình 32: Nghiên cứu ablation của ηrvàγcho cắt tỉa toàn cục với CIFAR10 và ResNet18.

29
