# 2405.02267.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2405.02267.pdf
# Kích thước tệp: 2666494 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)
Cắt tỉa Cấu trúc của Mô hình Ngôn ngữ Tiền huấn luyện thông qua Tìm kiếm Kiến trúc Mạng nơ-ron

Aaron Klein kleiaaro@amazon.com
Amazon Web Services
Jacek Golebiowski jacekgo@amazon.com
Amazon Web Services
Xingchen Ma xgchenma@amazon.com
Amazon Web Services
Valerio Perrone vperrone@amazon.com
Amazon Web Services
Cedric Archambeau cedric.archambeau@helsing.ai
Helsing

Đánh giá trên OpenReview: https: // openreview. net/ forum? id= XiK8tHDQNX

Tóm tắt
Các mô hình ngôn ngữ tiền huấn luyện (PLM), ví dụ như BERT hoặc RoBERTa, đánh dấu mức độ tiên tiến nhất cho các tác vụ hiểu ngôn ngữ tự nhiên khi được tinh chỉnh trên dữ liệu có nhãn. Tuy nhiên, kích thước lớn của chúng đặt ra thách thức trong việc triển khai để suy luận trong các ứng dụng thực tế, do yêu cầu bộ nhớ GPU đáng kể và độ trễ suy luận cao. Bài báo này khám phá tìm kiếm kiến trúc mạng nơ-ron (NAS) cho cắt tỉa cấu trúc để tìm các phần con của mạng đã tinh chỉnh mà tối ưu hóa sự đánh đổi hiệu quả, ví dụ về mặt kích thước mô hình hoặc độ trễ, và hiệu năng tổng quát hóa. Chúng tôi cũng chỉ ra cách chúng ta có thể sử dụng các phương pháp NAS chia sẻ trọng số hai giai đoạn được phát triển gần đây hơn trong bối cảnh này để tăng tốc quá trình tìm kiếm. Không giống như các phương pháp cắt tỉa truyền thống với ngưỡng cố định, chúng tôi đề xuất áp dụng phương pháp đa mục tiêu xác định tập hợp tối ưu Pareto của các mạng con, cho phép quá trình nén linh hoạt và tự động hóa hơn.

1 Giới thiệu
Các mô hình ngôn ngữ tiền huấn luyện (PLM) như BERT (Devlin et al., 2019) hoặc RoBERTa (Liu et al., 2019b) được sử dụng rộng rãi cho các tác vụ hiểu ngôn ngữ tự nhiên (NLU) khi có lượng lớn dữ liệu có nhãn để tinh chỉnh. Tuy nhiên, triển khai PLM để suy luận có thể gặp thách thức do số lượng tham số lớn. Chúng đòi hỏi bộ nhớ GPU đáng kể và thể hiện độ trễ suy luận cao, khiến chúng không thực tế cho nhiều ứng dụng thực tế, ví dụ khi được sử dụng trong điểm cuối cho dịch vụ web hoặc triển khai trên hệ thống nhúng. Nghiên cứu gần đây (Blalock et al., 2020; Kwon et al., 2022; Michel et al., 2019; Sajjad et al., 2022) đã chứng minh rằng trong nhiều trường hợp chỉ một tập con của các tham số mô hình tiền huấn luyện đóng góp đáng kể vào hiệu năng tác vụ downstream. Điều này cho phép nén mô hình bằng cách cắt tỉa các phần của mạng trong khi giảm thiểu sự suy giảm hiệu năng.

Cắt tỉa không cấu trúc (Blalock et al., 2020) tính toán điểm số cho mỗi trọng số trong mạng, chẳng hạn như độ lớn của trọng số, và loại bỏ các trọng số có điểm số dưới ngưỡng xác định trước. Phương pháp này thường đạt tỷ lệ cắt tỉa cao với suy giảm hiệu năng tối thiểu, nhưng nó cũng dẫn đến ma trận trọng số thưa thớt, không được hỗ trợ tốt bởi các framework học máy thường dùng. Cắt tỉa có cấu trúc (Michel et al., 2019; Sajjad et al., 2022) loại bỏ các thành phần lớn hơn của mạng, chẳng hạn như các lớp hoặc

--- TRANG 2 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

Lớp FFN
Lớp MHA
Lớp FFN
Lớp MHA
Lớp FFN
Lớp MHA
Lớp FFN
Lớp MHA

Mặt nạ FFN
Mặt nạ MHA
x
x

Mạng Tiền huấn luyện        Mạng con

(a) Trích xuất mạng con từ mạng tiền huấn luyện.

(b) Mặt trước Pareto của các mạng con.

Hình 1: Minh họa phương pháp của chúng tôi. a) Chúng tôi tinh chỉnh kiến trúc tiền huấn luyện bằng cách cập nhật chỉ các mạng con, mà chúng tôi chọn bằng cách đặt mặt nạ nhị phân trên các head và unit trong mỗi lớp MHA và FFN. b) Sau đó, chúng tôi chạy tìm kiếm đa mục tiêu để chọn tập hợp tối ưu của các mạng con cân bằng số lượng tham số và lỗi validation.

head. Mặc dù thường không đạt được tỷ lệ cắt tỉa như cắt tỉa không cấu trúc, nó chỉ cắt tỉa toàn bộ cột/hàng của ma trận trọng số, làm cho nó tương thích với các framework deep learning và phần cứng phổ biến.

Tìm kiếm kiến trúc mạng nơ-ron (Zoph & Le, 2017; Real et al., 2017; Bergstra et al., 2013) (NAS) tìm kiếm các kiến trúc mạng nơ-ron hiệu quả tài nguyên hơn theo cách hướng dữ liệu bằng cách đưa nó vào bài toán tối ưu hóa. Để giảm gánh nặng tính toán của NAS vanilla, cần phải huấn luyện và validation nhiều kiến trúc, tìm kiếm kiến trúc mạng nơ-ron dựa trên chia sẻ trọng số (Pham et al., 2018; Liu et al., 2019b; Elsken et al., 2018) đầu tiên huấn luyện một mạng lớn duy nhất, được gọi là super-network, và sau đó tìm kiếm các mạng con trong super-network.

Chúng tôi đề xuất sử dụng NAS cho cắt tỉa cấu trúc của các mạng tiền huấn luyện, để tìm các mạng con duy trì hiệu năng của mạng tiền huấn luyện sau khi tinh chỉnh (xem Hình 1 để minh họa). Hầu hết các phương pháp cắt tỉa cấu trúc cắt tỉa mạng dựa trên ngưỡng xác định trước về tỷ lệ cắt tỉa. Trong các tình huống không có ràng buộc nghiêm ngặt về kích thước mô hình, có thể khó khăn để xác định ngưỡng cố định như vậy trước. NAS mang lại lợi thế riêng biệt so với các chiến lược cắt tỉa khác bằng cách cho phép phương pháp đa mục tiêu xác định tập hợp tối ưu Pareto của các mạng con, nắm bắt mối quan hệ phi tuyến giữa kích thước mô hình và hiệu năng thay vì chỉ thu được một giải pháp duy nhất. Điều này cho phép chúng ta tự động hóa quá trình nén và chọn mô hình tốt nhất đáp ứng yêu cầu của chúng ta sau khi quan sát mặt trước Pareto phi tuyến, thay vì chạy quá trình cắt tỉa nhiều vòng để tìm tham số ngưỡng phù hợp.

Mặc dù có một lượng lớn tài liệu về cải thiện hiệu quả của PLM, theo hiểu biết tốt nhất của chúng tôi, chưa có nghiên cứu nào khám phá tiềm năng của NAS cho việc cắt tỉa PLM đã tinh chỉnh. Đóng góp của chúng tôi như sau:

• Chúng tôi thảo luận về mối quan hệ phức tạp giữa NAS và cắt tỉa cấu trúc và trình bày một phương pháp NAS nén PLM để suy luận sau khi tinh chỉnh trên các tác vụ downstream, trong khi giảm thiểu sự suy giảm hiệu năng. Trọng tâm của chúng tôi không nằm ở việc đề xuất một phương pháp NAS mới, mà là cung cấp một trường hợp sử dụng thực tế cho NAS trong bối cảnh PLM hoạt động cạnh tranh với các phương pháp cắt tỉa cấu trúc từ tài liệu.

• Chúng tôi đề xuất bốn không gian tìm kiếm khác nhau với độ phức tạp khác nhau để cắt tỉa các thành phần của PLM dựa trên transformer và thảo luận về cách định nghĩa của chúng ảnh hưởng đến cấu trúc của các mạng con. Hai trong số các không gian tìm kiếm này thường được sử dụng bởi các phương pháp cắt tỉa cấu trúc hiện có (xem Phần 4.2). Trong khi một trong những không gian tìm kiếm thường dùng này thể hiện mức độ tự do cao nhất, chúng tôi chỉ ra trong Phần 4.1.1 rằng một không gian tìm kiếm với độ phức tạp thấp hơn có thể hiệu quả hơn để khám phá và cuối cùng dẫn đến hiệu năng tốt hơn trong ngân sách hợp lý.

--- TRANG 3 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

• Chúng tôi đóng góp một bộ benchmarking cho NAS đa mục tiêu. Chúng tôi cũng chỉ ra cách chúng ta có thể áp dụng các phương pháp NAS dựa trên chia sẻ trọng số được đề xuất gần đây trong bối cảnh này. Dựa trên bộ benchmarking của chúng tôi, chúng tôi thực hiện nghiên cứu ablation kỹ lưỡng về NAS tiêu chuẩn và dựa trên chia sẻ trọng số. Về lâu dài, chúng tôi dự đoán rằng công việc của chúng tôi sẽ thúc đẩy việc phát triển các phương pháp NAS trong tương lai.

Chúng tôi trình bày tổng quan về các nghiên cứu liên quan trong Phần 2 và mô tả phương pháp luận của chúng tôi trong Phần 3. Phần 4 cung cấp so sánh thực nghiệm của phương pháp đề xuất với các phương pháp cắt tỉa cấu trúc khác từ tài liệu, cùng với nghiên cứu ablation sâu. Mã nguồn có sẵn tại https://github.com/whittle-org/plm_pruning.

2 Nghiên cứu Liên quan

Tìm kiếm Kiến trúc Mạng nơ-ron (NAS) (xem Elsken et al. (2018) để có tổng quan) tự động hóa việc thiết kế kiến trúc mạng nơ-ron để tối đa hóa hiệu năng tổng quát hóa và hiệu quả (ví dụ, về mặt độ trễ, kích thước mô hình hoặc tiêu thụ bộ nhớ). Yếu tố hạn chế của NAS thông thường là gánh nặng tính toán của việc tìm kiếm, đòi hỏi nhiều vòng huấn luyện và validation các kiến trúc mạng nơ-ron (Zoph & Le, 2017; Real et al., 2017). Để giảm thiểu chi phí này, nhiều phương pháp đã được đề xuất để tăng tốc quá trình tìm kiếm. Ví dụ, một số phương pháp này sớm kết thúc quá trình huấn luyện cho các cấu hình hoạt động kém (Li et al., 2018) hoặc ngoại suy các đường cong học tập (White et al., 2021b).

NAS chia sẻ trọng số (Pham et al., 2018; Liu et al., 2019a) giải quyết vấn đề chi phí bằng cách huấn luyện một super-network duy nhất bao gồm tất cả các kiến trúc trong không gian tìm kiếm, sao cho mỗi đường dẫn đại diện cho một kiến trúc duy nhất. Ban đầu, Liu et al. (2019a) đã đóng khung điều này như một bài toán tối ưu hóa hai cấp, trong đó mục tiêu bên trong liên quan đến việc tối ưu hóa trọng số mạng, và mục tiêu bên ngoài là việc lựa chọn kiến trúc. Sau khi huấn luyện super-network, kiến trúc tốt nhất được chọn dựa trên trọng số chia sẻ và sau đó được huấn luyện lại từ đầu. Tuy nhiên, nhiều bài báo (Li & Talwalkar, 2020; Yang et al., 2020) báo cáo rằng công thức này phụ thuộc mạnh vào không gian tìm kiếm và không mang lại kết quả tốt hơn so với việc chỉ lấy mẫu ngẫu nhiên các kiến trúc. Để giải quyết hạn chế này, Yu et al. (2020) đề xuất quy trình NAS hai giai đoạn. Trong giai đoạn đầu, super-network được huấn luyện bằng cách cập nhật các mạng con riêng lẻ trong mỗi lần lặp, thay vì cập nhật toàn bộ super-network. Sau khi huấn luyện, mô hình cuối cùng được chọn bằng cách thực hiện tối ưu hóa không gradient dựa trên trọng số chia sẻ của super-network, mà không cần huấn luyện thêm. Đồng thời, Cai et al. (2020) áp dụng phương pháp tương tự cho mạng nơ-ron tích chập trong bối cảnh đa mục tiêu bằng cách đầu tiên huấn luyện một super-network duy nhất và sau đó tìm kiếm các mạng con để giảm thiểu độ trễ trên một số thiết bị đích. Liên quan đến công việc của chúng tôi cũng là nghiên cứu của Xu et al. (2021), tìm kiếm các kiến trúc BERT hiệu quả hơn trong giai đoạn tiền huấn luyện.

Cắt tỉa Cấu trúc bao gồm việc loại bỏ các phần của mạng nơ-ron đã huấn luyện, chẳng hạn như head (Michel et al., 2019), hoặc toàn bộ các lớp (Sajjad et al., 2022), để giảm tổng số tham số trong khi bảo toàn hiệu năng. Các thành phần riêng lẻ được cắt tỉa dựa trên hàm tính điểm cụ thể, sử dụng ngưỡng được xác định thủ công. Đối với kiến trúc dựa trên transformer, Michel et al. (2019) quan sát thấy rằng một số lượng đáng kể các head, lên đến một head duy nhất trong lớp multi-head attention, có thể bị xóa sau khi tinh chỉnh mà không gây ra mất mát hiệu năng đáng kể. Voita et al. (2019) đề xuất regularization L0 như một phương tiện để cắt tỉa các head riêng lẻ trong lớp multi-head attention. Kwon et al. (2022) cắt tỉa các head riêng lẻ và các unit trong các lớp fully-connected sau khi tinh chỉnh theo ma trận thông tin Fisher. Sajjad et al. (2022) chứng minh rằng thậm chí có thể loại bỏ toàn bộ các lớp của mạng tiền huấn luyện trước khi tinh chỉnh, với tác động tối thiểu đến hiệu năng. So với phương pháp hướng dữ liệu của chúng tôi, Sajjad et al. (2022) đề xuất sử dụng các heuristic xác định trước (ví dụ, xóa các lớp trên cùng / lẻ / chẵn) để xác định các lớp cần cắt tỉa. Tuy nhiên, như được thể hiện trong các thí nghiệm của chúng tôi, kiến trúc phù hợp phụ thuộc vào tác vụ cụ thể, và các phương pháp hướng dữ liệu hơn là cần thiết để xác định chính xác các lớp tốt nhất để cắt tỉa.

Distillation (Hinton et al., 2015) huấn luyện một mô hình học sinh nhỏ hơn để bắt chước dự đoán của mô hình giáo viên tiền huấn luyện. Ví dụ, Sanh et al. (2020) sử dụng phương pháp này để chưng cất một mô hình BERT tiền huấn luyện (Devlin et al., 2019) thành một mô hình nhỏ hơn để tinh chỉnh. Jiao et al. (2019) đề xuất phương pháp knowledge distillation đặc biệt cho các mô hình dựa trên transformer, đầu tiên chưng cất từ giáo viên tiền huấn luyện thành mô hình nhỏ hơn và sau đó thực hiện chưng cất cụ thể cho tác vụ trong bước thứ hai dựa trên tập dữ liệu được tăng cường cho tác vụ. Liên quan đến phương pháp của chúng tôi cũng là AdaBERT (Chen et al., 2020) huấn luyện mạng nơ-ron tích chập cụ thể cho tác vụ dựa trên NAS có thể vi phân (Liu et al., 2019a) bằng cách chưng cất kiến thức của một PML như BERT.

--- TRANG 4 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

Không giống như các phương pháp dựa trên cắt tỉa, distillation cho phép thay đổi kiến trúc hoàn toàn ngoài việc chỉ loại bỏ các thành phần riêng lẻ. Tuy nhiên, từ góc độ thực tế, việc xác định cấu trúc và dung lượng tối ưu của mạng học sinh cần thiết để phù hợp với hiệu năng của mạng giáo viên cũng tương đương với một vấn đề tìm kiếm siêu tham số và kiến trúc mạng nơ-ron. Ngoài ra, huấn luyện mạng học sinh đòi hỏi một lượng đáng kể tài nguyên tính toán. Ví dụ, mô hình của Sanh et al. (2020) được huấn luyện khoảng 90 giờ trên 8 GPU V100 16GB. Chi phí này có thể được amortize bằng cách tinh chỉnh mô hình học sinh để giải quyết nhiều tác vụ khác nhau, nhưng, tùy thuộc vào các tác vụ downstream, nó có thể đòi hỏi một lượng đáng kể lần lặp mà không phải lúc nào cũng mong muốn đối với các nhà thực hành nhằm giải quyết một tác vụ cụ thể duy nhất. Điều này đặc biệt quan trọng trong bối cảnh đa mục tiêu nơi nhiều mạng cần được chưng cất để bao phủ toàn bộ mặt trước Pareto kích thước/độ chính xác.

Quantization (Dettmers et al., 2022; Dettmers & Zettlemoyer, 2023) giảm độ chính xác của các tham số mô hình từ số dấu phẩy động thành biểu diễn bit thấp hơn (ví dụ, số nguyên 8-bit). Lợi thế chính của quantization là giảm dung lượng bộ nhớ. Tuy nhiên, như chúng tôi chỉ ra trong Phụ lục F, điều này không nhất thiết dẫn đến độ trễ nhanh hơn. Quantization độc lập với phương pháp NAS của chúng tôi và có thể được sử dụng trên mạng đã cắt tỉa để giảm thêm việc sử dụng bộ nhớ.

3 Cắt tỉa Cấu trúc thông qua Tìm kiếm Kiến trúc Mạng nơ-ron

Đầu tiên chúng tôi cung cấp định nghĩa bài toán đa mục tiêu cho cắt tỉa cấu trúc của PLM đã tinh chỉnh thông qua tìm kiếm kiến trúc mạng nơ-ron. Sau đó, chúng tôi mô tả cách chúng ta có thể áp dụng NAS dựa trên chia sẻ trọng số. Cuối cùng, chúng tôi trình bày bốn không gian tìm kiếm để cắt tỉa kiến trúc dựa trên transformer, thể hiện mức độ cắt tỉa khác nhau.

3.1 Lựa chọn Mạng con Đa mục tiêu

Chúng tôi xem xét một mô hình transformer tiền huấn luyện dựa trên kiến trúc chỉ có encoder, chẳng hạn như BERT (Vaswani et al., 2017), với L lớp không phải embedding, mỗi lớp bao gồm một lớp multi-head attention (MHA) theo sau bởi một lớp feedforward fully connected (FFN). Tuy nhiên, tất cả các phương pháp được trình bày ở đây cũng có thể được áp dụng cho kiến trúc dựa trên decoder hoặc encoder-decoder. Cho một chuỗi đầu vào X∈Rn×dmodel, trong đó n đại diện cho độ dài chuỗi và dmodel là kích thước của token embedding, lớp MHA được định nghĩa bởi: MHA(X) = ∑H i Att(W(i) Q,W(i) K,W(i) V,W(i) O,X) trong đó W(i) Q,W(i) K,W(i) V∈Rdmodel×d và W(i) O∈ RHd×dmodel là các ma trận trọng số. Att(·) là một attention head tích vô hướng (Bahdanau et al., 2015) và H là số lượng head. Đầu ra sau đó được tính bởi XMHA = LN(X + MHA(X)), trong đó LN biểu thị layer normalization (Ba et al., 2016). Lớp FFN được định nghĩa bởi FFN(X) = W1σ(W0X), với W0∈RU×dmodel và W1∈Rdmodel×U, trong đó U biểu thị kích thước trung gian và σ(·) là hàm kích hoạt phi tuyến. Ở đây chúng ta cũng sử dụng kết nối tàn dư để tính đầu ra cuối cùng: xFFN = LN(XMHA + FFN(XMHA)).

Chúng tôi định nghĩa một mặt nạ nhị phân Mhead∈{0,1}L×H cho mỗi head trong lớp multi-head attention và một mặt nạ nhị phân Mneuron∈{0,1}L×U cho mỗi neuron trong các lớp fully-connected. Đầu ra của lớp MHA thứ l và lớp FFN được tính bởi MHAl(X) = ∑H i Mhead[i,l]Att(·) và FFNl(X) = W1 ◦ Mneuron[l,:] σ(W0X), tương ứng, trong đó ◦ biểu thị phép nhân từng phần tử.

Bây giờ, hãy định nghĩa một không gian tìm kiếm θ∈Θ chứa một tập hợp hữu hạn các cấu hình để định nghĩa các mạng con có thể được cắt ra từ mạng tiền huấn luyện. Chúng tôi định nghĩa một hàm CREATEMASK ánh xạ từ một cấu hình θ → Mhead,Mneuron thành các mặt nạ nhị phân. Hãy ký hiệu hàm f0: Θ → R là lỗi validation của mạng con được định nghĩa bởi cấu hình θ sau khi tinh chỉnh trên một tác vụ downstream nào đó. Để tính điểm validation được tạo bởi θ, chúng ta đặt các mặt nạ tương ứng Mhead, Mneuron lên mạng. Ngoài ra, chúng tôi định nghĩa tổng số tham số có thể huấn luyện f1: Θ → N của mạng con. Mục tiêu của chúng tôi là giải quyết bài toán tối ưu hóa đa mục tiêu sau:

min θ∈Θ (f0(θ), f1(θ)). (1)

--- TRANG 5 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

Trong bối cảnh đa mục tiêu, không có θ⋆∈Θ duy nhất đồng thời tối ưu hóa tất cả M mục tiêu. Hãy định nghĩa θ≻θ′ khi và chỉ khi fi(θ)≤fi(θ′), ∀i∈[M] và ∃i∈[k] : fi(θ)<fi(θ′). Chúng tôi nhằm tìm Tập Pareto: Pf={θ∈Θ|∄θ′∈Θ : θ′≻θ} của các điểm thống trị tất cả các điểm khác trong không gian tìm kiếm ít nhất trong một mục tiêu.

Để giải quyết bài toán tối ưu hóa này, chúng ta có thể sử dụng các phương pháp tìm kiếm đa mục tiêu tiêu chuẩn thường được sử dụng cho NAS, chẳng hạn như tìm kiếm ngẫu nhiên. Ở đây mỗi lần đánh giá hàm bao gồm việc tinh chỉnh một mạng con θ được khởi tạo với trọng số tiền huấn luyện thay vì trọng số ngẫu nhiên. Chúng ta cũng có thể trực tiếp áp dụng các chiến lược tiên tiến hơn, chẳng hạn như NAS đa fidelity, ví dụ MO-ASHA (Schmucker et al., 2021) để tăng tốc quá trình tìm kiếm.

3.2 Tìm kiếm Kiến trúc Mạng nơ-ron dựa trên Chia sẻ Trọng số

Theo nghiên cứu trước đây (Yu et al., 2020; Wang et al., 2021), phương pháp NAS dựa trên chia sẻ trọng số của chúng tôi bao gồm hai giai đoạn: giai đoạn đầu tiên là coi mô hình tiền huấn luyện như super-network và tinh chỉnh nó trên tác vụ downstream. Chúng tôi khám phá các chiến lược huấn luyện super-network khác nhau từ tài liệu chỉ cập nhật các phần của mạng trong mỗi bước, để tránh đồng thích ứng của các mạng con. Giai đoạn thứ hai, sử dụng các chiến lược tìm kiếm đa mục tiêu để xấp xỉ tập hợp tối ưu Pareto của các mạng con.

3.2.1 Huấn luyện Super-Network

Trong bối cảnh NAS tiêu chuẩn, chúng ta sẽ đánh giá f0(θ) bằng cách đầu tiên tinh chỉnh mạng con được định nghĩa bởi θ trên dữ liệu huấn luyện trước khi đánh giá trên dữ liệu validation. Trọng số của mạng con được khởi tạo dựa trên trọng số tiền huấn luyện. Trong khi các phương pháp NAS gần đây hơn (Li & Talwalkar, 2020; Klein et al., 2020) tăng tốc quá trình tìm kiếm bằng cách dừng sớm các mạng con hoạt động kém, điều này vẫn tương đương với một quá trình tối ưu hóa đòi hỏi tính toán của nhiều lần chạy tinh chỉnh độc lập.

Ý tưởng của NAS dựa trên chia sẻ trọng số hai giai đoạn (Yu et al., 2020) là huấn luyện một tập trọng số chia sẻ duy nhất, được gọi là super-network, chứa tất cả các mạng có thể trong không gian tìm kiếm. Sau khi huấn luyện super-network, việc đánh giá f0(θ) chỉ yêu cầu một lần chuyển qua dữ liệu validation.

Chúng tôi xem xét mạng tiền huấn luyện như super-network với trọng số chia sẻ chứa tất cả các mạng con có thể θ∈Θ. Để tránh các mạng con đồng thích ứng và vẫn hoạt động ngoài super-network, nghiên cứu trước đây (Yu et al., 2020; Wang et al., 2021) đề xuất chỉ cập nhật một tập con của các mạng con trong mỗi bước gradient descent ngẫu nhiên, thay vì cập nhật tất cả trọng số cùng nhau. Chúng tôi thích ứng chiến lược này và lấy mẫu các mạng con theo quy tắc sandwich (Yu et al., 2020; Wang et al., 2021) trong mỗi bước cập nhật, luôn cập nhật mạng con nhỏ nhất, lớn nhất và k mạng con ngẫu nhiên. Mạng con nhỏ nhất và lớn nhất tương ứng với cận dưới và cận trên của Θ. Đối với tất cả không gian tìm kiếm Θ được định nghĩa dưới đây, cận trên bằng kiến trúc mạng đầy đủ, tức là super-network và cận dưới loại bỏ tất cả các lớp trừ lớp embedding và classification.

Ngoài ra, chúng tôi sử dụng knowledge distillation tại chỗ (Yu et al., 2019) để tăng tốc quá trình huấn luyện của các mạng con. Cho logits πsupernet(x) của super-network, mà chúng ta thu được miễn phí với quy tắc sandwich, và logits của mạng con πθ(x), hàm loss để thu được gradient cho các mạng con theo ý tưởng của knowledge distillation:

LKD = LCE + DKL(σ(πsupernet/T), σ(πθ/T)), (2)

trong đó DKL(·) biểu thị divergence Kullback-Leibler giữa logits của super-network và mạng con, T là tham số nhiệt độ, σ(·) là hàm softmax và LCE là cross-entropy loss của dữ liệu huấn luyện.

3.2.2 Lựa chọn mạng con

Sau khi huấn luyện super-network, chúng tôi tính lỗi validation f0(θ) bằng cách áp dụng Mhead và Mneuron lên trọng số chia sẻ và thực hiện một lần chuyển qua dữ liệu validation. Điều này giảm đáng kể chi phí tính toán liên quan đến bài toán đa mục tiêu được nêu trong Phương trình 1. Để giải quyết bài toán này,

--- TRANG 6 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

về cơ bản chúng ta có thể sử dụng các phương pháp đa mục tiêu giống như cho NAS tiêu chuẩn (xem Phụ lục D), ngoại trừ các phương pháp đa fidelity như MO-ASHA. Các phương pháp đa fidelity dừng sớm quá trình huấn luyện bằng cách đánh giá mô hình tại các bước thời gian trung gian, được gọi là các cấp rung. Vì chúng ta không thực hiện thêm bước huấn luyện nào nữa và chỉ đánh giá mạng con cuối cùng, các cấp rung này không được định nghĩa.

3.3 Không gian Tìm kiếm

Không gian tìm kiếm Θ định nghĩa các mạng con của kiến trúc mạng tiền huấn luyện. Một Θ biểu cảm cho phép cắt tỉa tinh vi nhưng cũng có thể trở nên không khả thi để khám phá. Chúng tôi đề xuất các không gian tìm kiếm sau thể hiện các mức độ phức tạp khác nhau. Đối với mỗi không gian tìm kiếm, chúng tôi cung cấp mã giả để định nghĩa hàm CREATEMASK trong Phụ lục B.

• SMALL: Chúng tôi định nghĩa số lượng head H = [0,H], số lượng unit U = [0,U] và tổng số lớp L = [0,L], sao cho Θ = H × U × L. So với các không gian tìm kiếm khác, chiều của không gian tìm kiếm này vẫn cố định với các kích thước mô hình khác nhau, và chỉ cận trên của nó tức là (H,U,L) tăng lên. Đối với mỗi lớp, chúng ta luôn giữ h∈H head đầu tiên và u∈U unit đầu tiên, tương ứng, để đảm bảo rằng CREATEMASK là một ánh xạ song ánh (xem Phụ lục B).

• LAYER: Lấy cảm hứng từ Sajjad et al. (2022), chúng tôi cắt tỉa các lớp attention và fully-connected riêng lẻ thay vì các head và neuron đơn lẻ. Chúng tôi định nghĩa một không gian tìm kiếm Θ = {0,1}L chứa một siêu tham số nhị phân cho mỗi lớp xác định liệu lớp tương ứng có bị loại bỏ hay không.

• MEDIUM: Dựa trên không gian tìm kiếm trước đó, chúng tôi cho phép số lượng head/unit linh hoạt mỗi lớp. Đối với mỗi lớp l∈[0,L], chúng tôi định nghĩa Hl = [0,H] và Ul = [0,U], sao cho không gian tìm kiếm cuối cùng là Θ = H0 × U0...HL × UL. Như đối với không gian tìm kiếm SMALL, chúng ta cũng giữ các head và unit đầu tiên trong mỗi lớp.

• LARGE: Đối với mỗi head và neuron trong lớp fully-connected, chúng tôi định nghĩa một Θi = {0,1} nhị phân duy nhất được kết hợp để tạo thành không gian tìm kiếm Θ = Θ0×...×ΘL(H+I). Đây là không gian tìm kiếm biểu cảm nhất, nhưng cũng tăng nhanh với kích thước mô hình. Không gian tìm kiếm này cũng thường được sử dụng bởi các phương pháp cắt tỉa cấu trúc khác (Kwon et al., 2022). Nó có thể không hữu ích lắm trong thực tế, vì chúng ta không thể dễ dàng loại bỏ các hàng/cột đơn lẻ của ma trận trọng số với hầu hết các triển khai transformer và do đó nó sẽ không nhất thiết giảm độ trễ suy luận. Tuy nhiên, nó cung cấp cho chúng ta một tham chiếu về mặt hiệu năng dự đoán có thể được giữ lại dưới một tỷ lệ cắt tỉa nhất định.

Mỗi không gian tìm kiếm tạo ra một mẫu khác nhau cho Mhead và Mneuron mà chúng ta đặt lên super-network để chọn các mạng con (xem Hình 2 để có một số ví dụ). Để xem điều này ảnh hưởng như thế nào đến phân phối số lượng tham số và do đó việc lấy mẫu trong quá trình huấn luyện super-network, chúng tôi lấy mẫu N = 500 cấu hình {θ0,...,θN} một cách đồng nhất ngẫu nhiên và tính số lượng tham số có thể huấn luyện {f1(θi),...,f1(θN)} cho tất cả bốn không gian tìm kiếm (xem Hình 3). Không gian tìm kiếm SMALL hơi thiên về các mạng nhỏ hơn. Không gian tìm kiếm MEDIUM, mặc dù biểu cảm hơn, nhưng lại thiên nhiều về các mạng cỡ trung bình, vì trung bình một nửa số head/neuron bị che. Đối với hai không gian tìm kiếm nhị phân LAYER và LARGE, chúng ta có thể đạt được phân phối đồng nhất trên số lượng tham số, bằng cách sử dụng quá trình lấy mẫu sau. Đầu tiên chúng ta lấy mẫu một số nguyên k∼U(0,K), trong đó k=L cho không gian tìm kiếm LAYER, và k=L(H+I) cho không gian tìm kiếm LARGE. Sau đó, chúng ta chọn ngẫu nhiên k mục của vector nhị phân θ∈Θ và đặt chúng thành 1.

4 Thí nghiệm

Chúng tôi đánh giá các loại NAS khác nhau cho cắt tỉa cấu trúc trên tám tác vụ phân loại văn bản, bao gồm suy luận văn bản, phân tích tình cảm và câu hỏi trắc nghiệm/trả lời. Chúng tôi cung cấp mô tả chi tiết về từng tác vụ trong Phụ lục C. Tất cả các tác vụ đều có tập huấn luyện và đánh giá được định nghĩa trước với nhãn và tập test giữ lại không có nhãn. Chúng tôi chia tập huấn luyện thành tập huấn luyện và validation (chia 70%/30%) và sử dụng tập đánh giá làm tập test. Chúng tôi tinh chỉnh mọi mạng, mạng con hoặc super-network, trong

--- TRANG 7 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

[Hình 2: Ví dụ về mặt nạ head Mhead được lấy mẫu đồng nhất ngẫu nhiên từ các không gian tìm kiếm khác nhau. Màu tối cho biết head tương ứng bị che. Cùng một mẫu có thể được quan sát thấy cho Mneuron]

[Hình 3: Phân phối số lượng tham số f1(θ) cho θ∼Θ được lấy mẫu đồng nhất.]

5 epoch trên một GPU duy nhất. Đối với tất cả các phương pháp tìm kiếm đa mục tiêu, chúng tôi sử dụng Syne Tune (Salinas et al., 2022) trên một instance GPU duy nhất. Chúng tôi sử dụng BERT-base (Devlin et al., 2019) (cased) và RoBERTa-base (Liu et al., 2019b) làm mạng tiền huấn luyện, bao gồm L = 12 lớp, I = 3072 unit và H = 12 head (các siêu tham số khác được mô tả trong Phụ lục A). Mặc dù có thể nhỏ theo tiêu chuẩn ngày nay, chúng vẫn đạt hiệu năng cạnh tranh trên các benchmark này và cho phép đánh giá kỹ lưỡng hơn. Chúng tôi cũng trình bày so sánh với quantization trong Phụ lục F.

4.1 Benchmarking Tìm kiếm Kiến trúc Mạng nơ-ron

[Hình 4: Ví dụ tính Hypervolume HV(Pf|r), tương ứng với tổng các hình chữ nhật, qua một điểm tham chiếu r và một tập điểm Pf={y0,y1,y2,y3}]

Bây giờ chúng tôi trình bày đánh giá các phương pháp NAS đa mục tiêu khác nhau trên bộ benchmarking của chúng tôi. Để định lượng hiệu năng của một tập Pareto Pf, chúng tôi tính cho mỗi tập Pareto Hypervolume (Zitzler et al., 2003) và báo cáo regret, tức là sự khác biệt với Hypervolume tốt nhất có thể được tính trung bình qua tất cả các lần lặp lại. Cho một điểm tham chiếu r∈RM, Hypervolume HV(Pf|r) = λ(∪y∈Pf[y,r]) được định nghĩa là thước đo Lebesgue thứ M chiều λ giữa tập Pareto Pf và điểm tham chiếu r, trong đó [y,r] đại diện cho siêu hình chữ nhật giữa y và r (xem Hình 4 để có ví dụ).

Để tính Hypervolume, đầu tiên chúng tôi chuẩn hóa mỗi mục tiêu dựa trên tất cả các giá trị quan sát được qua tất cả các phương pháp và lần lặp lại thông qua chuẩn hóa Quantile. Điều này dẫn đến phân phối đồng nhất giữa [0,1], và chúng tôi sử dụng r = (2,2) làm điểm tham chiếu, có nghĩa là Hypervolume cao nhất có thể sẽ là 4. Chúng tôi đánh giá mỗi phương pháp với 10 seed khác nhau cho việc tạo số ngẫu nhiên.

--- TRANG 8 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

4.1.1 Không gian Tìm kiếm

Đầu tiên, chúng tôi so sánh các định nghĩa không gian tìm kiếm từ Phần 3.3 sử dụng NAS dựa trên chia sẻ trọng số. Chúng tôi tinh chỉnh super-network như được mô tả trong Phần 3.2 và lấy mẫu 100 mạng con một cách đồng nhất ngẫu nhiên để tính hypervolume.

Kết luận: Trong ngân sách này (xem Hình 5), không gian tìm kiếm SMALL đạt hiệu năng tốt nhất trên tất cả các tập dữ liệu, ngoại trừ COLA. Thú vị là, mặc dù không gian tìm kiếm MEDIUM cho phép cắt tỉa tinh vi hơn theo từng lớp, nó lại dẫn đến kết quả tồi tệ hơn. Chúng tôi cho rằng điều này do phân phối không đồng nhất của số lượng tham số như được mô tả trong Phần 3.3. Không gian tìm kiếm LAYER thường vượt trội hơn không gian tìm kiếm MEDIUM và LARGE, nhưng, ngoại trừ COLA, dẫn đến các tập Pareto kém hiệu năng hơn so với không gian tìm kiếm SMALL. Không gian tìm kiếm LARGE, là tập con của các không gian tìm kiếm khác, dường như không khả thi để khám phá với việc lấy mẫu ngẫu nhiên qua quá ít quan sát. Chúng tôi sử dụng không gian tìm kiếm SMALL cho các thí nghiệm còn lại.

4.1.2 Tìm kiếm Kiến trúc Mạng nơ-ron Tiêu chuẩn

Chúng tôi so sánh các phương pháp tìm kiếm đa mục tiêu sau để giải quyết bài toán NAS được mô tả trong Phần 3 trong đó mỗi mạng con được tinh chỉnh riêng biệt. Chúng tôi cung cấp mô tả chi tiết hơn về mỗi phương pháp trong Phụ lục D. Một tìm kiếm cục bộ đa mục tiêu đơn giản (LS) được mô tả trong Phụ lục D. Điều này được lấy cảm hứng từ công trình của White et al. (2021a), cho thấy tìm kiếm cục bộ thường hoạt động cạnh tranh trên các bài toán NAS. Tìm kiếm ngẫu nhiên (RS) (Bergstra & Bengio, 2012) lấy mẫu các kiến trúc một cách đồng nhất ngẫu nhiên từ không gian tìm kiếm. Một phiên bản đa mục tiêu của thuật toán evolution được điều chỉnh (REA) (Real et al., 2019), thường được sử dụng trong tài liệu NAS. So với thuật toán mục tiêu đơn ban đầu, chúng tôi sắp xếp các phần tử trong quần thể thông qua sắp xếp không bị thống trị. Expected Hypervolume Improvement (EHVI) (Daulton et al., 2020) là một chiến lược tối ưu hóa Bayesian đa mục tiêu lấy mẫu các điểm ứng viên sử dụng mô hình Gaussian process của hàm mục tiêu. Cuối cùng, chúng tôi bao gồm MO-ASHA (Schmucker et al., 2021), một phiên bản đa mục tiêu của asynchronous successive halving (Li & Talwalkar, 2020; Jamieson & Talwalkar, 2016) kết thúc sớm quá trình huấn luyện của các ứng viên hoạt động kém để tăng tốc quá trình tối ưu hóa tổng thể. Mặc dù MO-ASHA có thể được kết hợp với phương pháp dựa trên mô hình, như thường được thực hiện cho tối ưu hóa mục tiêu đơn (Falkner et al., 2018; Klein et al., 2020), ở đây chúng tôi theo thuật toán ban đầu và lấy mẫu ứng viên một cách đồng nhất ngẫu nhiên từ không gian tìm kiếm.

Theo thực hành thí nghiệm thông thường từ tài liệu HPO, chúng tôi tổng hợp kết quả bằng cách tính thứ hạng trung bình của mỗi phương pháp qua các lần lặp lại, tập dữ liệu và bước thời gian. Theo Feurer et al. (2015), chúng tôi lấy mẫu 1000 mẫu bootstrap qua tất cả các lần lặp lại và tác vụ, để tính thứ hạng của mỗi phương pháp và tính trung bình qua tất cả các mẫu. Kết quả được hiển thị trong Hình 6a. Chúng tôi hiển thị kết quả cho mỗi tác vụ riêng lẻ trong Phụ lục E.

Kết luận: Hơi bất ngờ, RS là một baseline mạnh trên các benchmark này, vượt trội hơn các phương pháp tinh vi hơn như EHVI hoặc MO-REA. Tinh chỉnh các mô hình này thường không ổn định (Mosbach et al., 2021) đặc biệt trên các tập dữ liệu nhỏ hơn, dẫn đến nhiễu quan sát cao. Đối với mô hình RoBERTa-base, LS thường hoạt động cạnh tranh với RS với ngân sách đủ lớn. MO-ASHA nhanh chóng dừng đánh giá các mạng con hoạt động kém và do đó vượt trội hơn RS trung bình. Tuy nhiên, trên các tập dữ liệu nhỏ như RTE, tinh chỉnh nhanh hơn so với sắp xếp không bị thống trị của MO-ASHA, sao cho nó hội tụ chậm hơn RS (xem Phụ lục E).

4.1.3 Tìm kiếm Kiến trúc Mạng nơ-ron dựa trên Chia sẻ Trọng số

Tiếp theo, chúng tôi đánh giá các kỹ thuật khác nhau cho NAS hai giai đoạn cho bài toán này. Chúng tôi phân biệt giữa tinh chỉnh super-network và tìm kiếm đa mục tiêu.

Tinh chỉnh super-network: Đầu tiên, chúng tôi so sánh các chiến lược sau để tinh chỉnh super-network từ tài liệu. Để so sánh các phương pháp này, sau khi tinh chỉnh super-network, chúng tôi lấy mẫu 100 mạng con một cách đồng nhất ngẫu nhiên từ không gian tìm kiếm SMALL để ước tính tập Pareto và báo cáo ở đây

--- TRANG 9 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

[Hình 5: So sánh bốn không gian tìm kiếm khác nhau sử dụng NAS dựa trên chia sẻ trọng số. Chúng tôi lấy mẫu 100 mạng con ngẫu nhiên một cách đồng nhất sử dụng trọng số đã tinh chỉnh của super-network. Không gian tìm kiếm SMALL thống trị các không gian tìm kiếm khác ngoại trừ tập dữ liệu COLA. Mặc dù SMALL là tập con của MEDIUM và LARGE, các không gian này quá nhiều chiều để được khám phá với ngân sách tính toán hợp lý. Hai hàng đầu hiển thị kết quả cho BERT-base-cased và hai hàng cuối cho RoBERTa-base.]

Hypervolume. Chúng tôi lặp lại quá trình này 10 lần với seed khác nhau cho việc tạo số ngẫu nhiên. Đối với mỗi lần lặp lại, chúng tôi sử dụng chính xác cùng một tập các mạng con ngẫu nhiên cho tất cả các chiến lược huấn luyện super-network.

• standard: Huấn luyện tất cả trọng số của super-network trong bối cảnh tinh chỉnh tiêu chuẩn
• random: Lấy mẫu một mạng con ngẫu nhiên duy nhất trong mỗi bước cập nhật
• random-linear: Lấy cảm hứng từ Bender et al. (2018), chúng tôi lấy mẫu một mạng con ngẫu nhiên với xác suất p hoặc mạng đầy đủ với xác suất 1−p trong mỗi bước cập nhật. Trong đó, p được tăng tuyến tính từ 0 đến 1 sau mỗi bước cập nhật trong suốt quá trình huấn luyện.
• sandwich: Super-network được cập nhật theo quy tắc sandwich (Yu et al., 2020; Wang et al., 2021) được mô tả trong Phần 3.2. Chúng tôi đặt số lượng mạng con ngẫu nhiên trong mỗi bước cập nhật là k = 2.

--- TRANG 10 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

[Hình 6: Thứ hạng trung bình của các phương pháp tìm kiếm đa mục tiêu khác nhau qua tất cả tập dữ liệu cho a) NAS tiêu chuẩn và b) NAS dựa trên chia sẻ trọng số cho cả mô hình BERT-based cased và RoBERTa-base. Các phương pháp dựa trên tìm kiếm ngẫu nhiên (RS, MO-ASHA) hoạt động cạnh tranh trong bối cảnh NAS tiêu chuẩn. EHVI vượt trội hơn các phương pháp khác ở các bước thời gian sớm hơn mà chúng tôi cho là do mô hình xác suất của nó. Với đủ thời gian thường tìm thấy các mặt trước Pareto hoạt động tốt.]

• kd: Cập nhật k = 2 mạng con ngẫu nhiên sử dụng knowledge distillation tại chỗ (Yu et al., 2020; Wang et al., 2021) theo Phương trình 2.

• full: Triển khai giao thức huấn luyện được mô tả trong Phần 3.2, tức là kết hợp quy tắc sandwich với knowledge distillation tại chỗ để cập nhật mạng con.

Kết luận: Hình 7 hiển thị Hypervolume qua tất cả tác vụ cho BERT-base và RoBERTa-base, tương ứng. Tinh chỉnh tiêu chuẩn và chỉ lấy mẫu ngẫu nhiên một mạng con dẫn đến các tập Pareto hoạt động kém so với các phương pháp khác. Ngoại lệ duy nhất là tập dữ liệu COLA, nơi tinh chỉnh tiêu chuẩn đôi khi hoạt động tốt nhất. Tuy nhiên, chúng tôi cũng quan sát thấy sự biến động cao qua các lần chạy trên tập dữ liệu này. Tăng tuyến tính xác suất lấy mẫu một mạng con ngẫu nhiên cải thiện so với chỉ lấy mẫu ngẫu nhiên. Kết quả tốt hơn đạt được bằng cách sử dụng quy tắc sandwich hoặc knowledge distillation. Trong đó, kết hợp cả hai cải thiện kết quả thêm một chút.

Tìm kiếm Đa mục tiêu: Cuối cùng, chúng tôi so sánh trong Hình 6b thứ hạng trung bình của các phương pháp tìm kiếm đa mục tiêu giống như cho NAS tiêu chuẩn. Chúng tôi theo cùng quy trình như được mô tả trong Phần 4.1.2 để tính thứ hạng trung bình. Chúng tôi không bao gồm MO-ASHA trong bối cảnh này, vì mỗi lần đánh giá hàm chỉ bao gồm việc validation một mạng con dựa trên trọng số chia sẻ của super-network sau khi tinh chỉnh và do đó không cho phép phương pháp đa fidelity. Quỹ đạo tối ưu hóa cho tất cả tập dữ liệu trong Phụ lục E.

Kết luận: Như đối với NAS tiêu chuẩn, RS là một baseline mạnh đáng ngạc nhiên. EHVI hoạt động tốt hơn ở các bước thời gian sớm. Chúng tôi thấy rằng việc sử dụng trọng số chia sẻ của super-network để đánh giá dẫn đến nhiễu quan sát nhỏ hơn nhiều so với tinh chỉnh mỗi mạng con riêng biệt, điều này ít đánh lừa hơn cho mô hình xác suất của EHVI. Với đủ thời gian, LS bắt đầu vượt trội hơn RS và EHVI trên mô hình RoBERTa-base và hoạt động cạnh tranh với EHVI trên mô hình BERT-base.

4.2 So sánh với các Phương pháp Cắt tỉa Cấu trúc khác

Bây giờ chúng tôi trình bày so sánh với các phương pháp cắt tỉa cấu trúc khác. Đối với NAS, chúng tôi sử dụng không gian tìm kiếm SMALL được định nghĩa trong Phần 3.3 dựa trên nghiên cứu ablation của chúng tôi trong Phần 4.1. Mỗi phương pháp có cùng tổng thời gian wall-clock và tính toán, bao gồm cả tinh chỉnh và tìm kiếm. Chúng tôi so sánh các phương pháp sau:

• Head-Pruning (HP) (Michel et al., 2019) cắt tỉa head một cách tham lam sử dụng điểm proxy cho tầm quan trọng của mỗi head đối với hiệu năng cuối cùng dựa trên gradient của hàm loss.

• Retraining Free Pruning (RFP) (Kwon et al., 2022) sử dụng chiến lược cắt tỉa ba giai đoạn mà, dựa trên ngưỡng α, cắt tỉa các head riêng lẻ trong lớp MHA và các unit trong lớp FFN.

--- TRANG 11 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

[Hình 7: So sánh các chiến lược khác nhau để tinh chỉnh super-network. Hai hàng đầu hiển thị BERT-base và hai hàng cuối hiển thị RoBERTa-base.]

Giai đoạn đầu tiên tính một mặt nạ nhị phân cho các head và unit bằng cách tính ma trận thông tin Fisher đường chéo. Ma trận sau đó được sắp xếp lại bởi ma trận thông tin Fisher xấp xỉ khối. Trong bước cuối cùng, mặt nạ được tinh chỉnh thêm bằng cách giảm thiểu lỗi tái tạo theo từng lớp. Phương pháp này hoạt động trong không gian tìm kiếm LARGE được mô tả trong Phần 3.3. Chúng tôi chạy RFP với các giá trị khác nhau cho α∈{0.1,0.2,...,0.9} để thu được tập Pareto của các kiến trúc.

• Layer Dropping (LD): Theo Sajjad et al. (2022), chúng tôi đầu tiên loại bỏ n∈1,...,L−1 lớp trên cùng và tinh chỉnh các lớp còn lại trực tiếp trên tác vụ downstream. Để thu được tập Pareto của N điểm, chúng tôi tinh chỉnh N mô hình với số lượng lớp được loại bỏ khác nhau. Phương pháp này phục vụ như một heuristic đơn giản để khám phá không gian tìm kiếm LAYER.

• Standard NAS (S-NAS): Như đối với các thí nghiệm trước đây, chúng tôi sử dụng NAS tiêu chuẩn sử dụng tìm kiếm ngẫu nhiên trong đó mỗi mạng con được khởi tạo với trọng số tiền huấn luyện và sau đó được tinh chỉnh độc lập.

--- TRANG 12 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

[Hình 8: Mất mát trong hiệu năng test (càng cao càng tốt) so với số lượng tham số tương đối với mô hình chưa cắt tỉa trên tất cả 8 tập dữ liệu phân loại văn bản. Hai hàng đầu hiển thị BERT-base và hai hàng cuối hiển thị RoBERTa-base.]

• Weight-sharing NAS (WS-NAS): Theo phương pháp NAS dựa trên chia sẻ trọng số hai giai đoạn được nêu trong Phần 3.2. Chúng tôi sử dụng quy tắc sandwich và knowledge distillation tại chỗ để huấn luyện super-network và EHVI để tìm kiếm tập hợp tối ưu Pareto của các mạng con.

Để so sánh kết quả, chúng tôi chuẩn hóa số lượng tham số thành [0,1] và phân nhóm kết quả dựa trên các ngưỡng β∈{0.2,...0.9} khác nhau. Lưu ý rằng khoảng 20% tham số của BERT-base/RoBERTa-base được bao gồm trong embedding và classification head, và do đó không thể được cắt tỉa một cách tầm thường mà không thay đổi chiều embedding hoặc số lượng lớp. Đối với mỗi nhóm, chúng tôi báo cáo hiệu năng tốt nhất của giải pháp với ≤β tham số. Chúng tôi thảo luận mối quan hệ giữa số lượng tham số và thời gian suy luận mô hình trong F.

Hình 8 hiển thị số lượng tham số (trục ngang) và lỗi test (trục dọc) tương đối với mạng chưa cắt tỉa cho tất cả tập dữ liệu. Để tham khảo, chúng tôi chỉ ra 95% và 90% hiệu năng tương đối với mạng chưa cắt tỉa cũng như hiệu năng ban đầu bằng các đường đứt nét. Chúng tôi sắp xếp tập dữ liệu theo kích thước từ trái sang phải.

Kết luận: Đầu tiên, cả S-NAS và WS-NAS đều đạt hiệu năng cạnh tranh với các phương pháp cắt tỉa cấu trúc. Đặc biệt đối với tỷ lệ cắt tỉa cao hơn, cả S-NAS và WS-NAS đều vượt trội hơn RFP và HP mặc dù chúng hoạt động trong không gian tìm kiếm LARGE, có nhiều chiều hơn nhiều so với không gian tìm kiếm SMALL được sử dụng cho NAS. Điều này cho thấy rằng các phương pháp này không thể xử lý nghiêm ngặt các không gian có chiều cao như vậy.

Thứ hai, các phương pháp NAS dường như hoạt động tốt hơn đối với các tập dữ liệu lớn hơn. Với đủ ngân sách, LD đơn giản, hoạt động trong không gian tìm kiếm LAYER, cũng đạt kết quả cạnh tranh. Điều này phù hợp với kết quả của chúng tôi trong Phần 4.1.1, cho thấy rằng không gian tìm kiếm LAYER vẫn có thể cung cấp kết quả hợp lý so với các không gian tìm kiếm biểu cảm hơn.

WS-NAS giảm đáng kể thời gian chạy tổng thể bằng cách chỉ tinh chỉnh một super-network duy nhất, cho phép nó đạt hiệu năng tốt hơn nhiều với lượng tính toán hạn chế so với S-NAS. Tuy nhiên, chúng tôi kỳ vọng S-NAS sẽ vượt trội hơn WS-NAS nếu chúng ta tăng tổng tính toán, vì mỗi mạng con được tinh chỉnh riêng biệt và do đó có thể thích ứng tối ưu với dữ liệu huấn luyện.

[Hình 9: Mặt trước Pareto của các lần chạy đơn lẻ cho mỗi phương pháp cho tập dữ liệu SST2, IMDB và QNLI để cân bằng số lượng tham số và test-error (càng thấp càng tốt).]

Để so sánh định tính, chúng tôi hiển thị kết quả cho một lần chạy đơn lẻ trên ba tập dữ liệu trong Hình 9 bên phải. Các ví dụ chạy khác được hiển thị trong Phụ lục E.

4.3 Kết luận

Chúng tôi đề xuất NAS cho cắt tỉa cấu trúc của PLM đã tinh chỉnh. Bằng cách sử dụng phương pháp đa mục tiêu, chúng ta có thể tìm tập hợp tối ưu Pareto của các mạng con cân bằng giữa kích thước mô hình và lỗi validation. Trả về tập Pareto của các mạng con cho phép các nhà thực hành chọn mạng tối ưu mà không cần chạy quá trình cắt tỉa nhiều lần với các ngưỡng khác nhau. Chúng tôi cũng cung cấp phân tích sâu về các phương pháp chia sẻ trọng số hai giai đoạn được phát triển gần đây trong bối cảnh này, chỉ yêu cầu một lần chạy tinh chỉnh duy nhất của PLM.

Nghiên cứu tương lai có thể khám phá bối cảnh instruction tuning (Wei et al., 2022), nơi mô hình cuối cùng được đánh giá trong bối cảnh few-shot. Phương pháp của chúng tôi lấy mẫu các mạng con một cách đồng nhất ngẫu nhiên, phân bổ cùng lượng bước cập nhật cho tất cả các mạng con trung bình. Nghiên cứu tương lai có thể khám phá phân phối lấy mẫu phức tạp hơn thiên về các mạng con gần với tập Pareto hơn.

Tài liệu Tham khảo

J. L. Ba, J. R. Kiros, và G. E. Hinton. Layer normalization. arXiv:1607.06450 [stat.ML], 2016.

D. Bahdanau, K. Cho, và Y. Bengio. Neural machine translation by jointly learning to align and translate. Trong International Conference on Learning Representations (ICLR'15), 2015.

G. Bender, P. Kindermans, B. Zoph, V. Vasudevan, và Q. Le. Understanding and simplifying one-shot architecture search. Trong Proceedings of the 35th International Conference on Machine Learning (ICML'18), 2018.

J. Bergstra và Y. Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research (JMLR-12), 2012.

J. Bergstra, D. Yamins, và D. Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. Trong Proceedings of the 30th International Conference on Machine Learning (ICML'13), 2013.

--- TRANG 14 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

D. Blalock, J. J. G. Ortiz, J. Frankle, và J. Guttag. What is the state of neural network pruning? arXiv:2003.03033 [cs.LG], 2020.

H. Cai, C. Gan, T. Wang, Z. Zhang, và S. Han. Once-for-all: Train one network and specialize it for efficient deployment. Trong International Conference on Learning Representations (ICLR'20), 2020.

D. Chen, Y. Li, M. Qiu, Z. Wang, B. Li, B. Ding, H. Deng, J. Huang, W. Lin, và J. Zhou. Adabert: Task-adaptive bert compression with differentiable neural architecture search. Trong Proceedings of the 29th International Joint Conference on Artificial Intelligence (IJCAI'20), 2020.

S. Daulton, M. Balandat, và E. Bakshy. Differentiable expected hypervolume improvement for parallel multi-objective bayesian optimization. Trong Proceedings of the 34th International Conference on Advances in Neural Information Processing Systems (NeuRIPS'20), 2020.

T. Dettmers và L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv:2212.09720 [cs.LG], 2023.

T. Dettmers, M. Lewis, Y. Belkada, và L. Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. Trong Proceedings of the 36th International Conference on Advances in Neural Information Processing Systems (NeuRIPS'22), 2022.

J. Devlin, M. Chang, K. Lee, và K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019.

T. Elsken, J. H. Metzen, và F. Hutter. Neural architecture search: A survey. arXiv:1808.05377 [stat.ML], 2018.

S. Falkner, A. Klein, và F. Hutter. BOHB: Robust and efficient hyperparameter optimization at scale. Trong Proceedings of the 35th International Conference on Machine Learning (ICML 2018), 2018.

M. Feurer, T. Springenberg, và F. Hutter. Initializing bayesian hyperparameter optimization via meta-learning. Trong Proceedings of the 29th National Conference on Artificial Intelligence (AAAI'15), 2015.

G. Hinton, O. Vinyals, và J. Dean. Distilling the knowledge in a neural network. arXiv:1503.02531 [stat.ML], 2015.

K. Jamieson và A. Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. Trong Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AISTATS'16), 2016.

X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, và Q. Liu. Tinybert: Distilling bert for natural language understanding. arXiv:1909.10351 [cs.CL], 2019.

A. Klein, L. C. Tiao, T. Lienart, C. Archambeau, và M. Seeger. Model-based asynchronous hyperparameter optimization. arXiv:2003.10865 [cs.LG], 2020.

W. Kwon, S. Kim, M. W. Mahoney, J. Hassoun, K. Keutzer, và A. Gholami. A fast post-training pruning framework for transformers. arXiv:2204.09656 [cs.CL], 2022.

L. Li và A. Talwalkar. Random search and reproducibility for neural architecture search. Trong Proceedings of The 35th Uncertainty in Artificial Intelligence Conference, 2020.

L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, và A. Talwalkar. Hyperband: Bandit-based configuration evaluation for hyperparameter optimization. Trong International Conference on Learning Representations (ICLR'17), 2017.

L. Li, K. Jamieson, A. Rostamizadeh, K. Gonina, M. Hardt, B. Recht, và A. Talwalkar. Massively parallel hyperparameter tuning. arXiv:1810.05934 [cs.LG], 2018.

--- TRANG 15 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

H. Liu, K. Simonyan, và Y. Yang. DARTS: Differentiable architecture search. Trong International Conference on Learning Representations (ICLR'19), 2019a.

Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, và V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692 [cs.CL], 2019b.

P. Michel, O. Levy, và G. Neubig. Are sixteen heads really better than one? Trong Proceedings of the 32th International Conference on Advances in Neural Information Processing Systems (NeurIPS'19), 2019.

M. Mosbach, M. Andriushchenko, và D. Klakow. On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines. Trong International Conference on Learning Representations (ICLR'21), 2021.

H. Pham, M. Guan, B. Zoph, Q. Le, và J. Dean. Efficient neural architecture search via parameters sharing. Trong Proceedings of the 35th International Conference on Machine Learning (ICML'18), 2018.

E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, J. Tan, Q. V. Le, và A. Kurakin. Large-scale evolution of image classifiers. Trong Proceedings of the 34th International Conference on Machine Learning (ICML'17), 2017.

E. Real, A. Aggarwal, Y. Huang, và Q. V. Le. Regularized Evolution for Image Classifier Architecture Search. Trong Proceedings of the Conference on Artificial Intelligence (AAAI'19), 2019.

H. Sajjad, F. Dalvi, N. Durrani, và P. Nakov. On the effect of dropping layers of pre-trained transformer models. arXiv:2004.03844 [cs.CL], 2022.

D. Salinas, M. Seeger, A. Klein, V. Perrone, M. Wistuba, và C. Archambeau. Syne tune: A library for large scale hyperparameter tuning and reproducible research. Trong First Conference on Automated Machine Learning (Main Track), 2022.

V. Sanh, L. Debut, J. Chaumond, và T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv:1910.01108 [cs.CL], 2020.

R. Schmucker, M. Donini, M. B. Zafar, D. Salinas, và C. Archambeau. Multi-objective asynchronous successive halving. arXiv:2106.12639 [stat.ML], 2021.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser, và I. Polosukhin. Attention is all you need. Trong Advances in Neural Information Processing Systems (NeurIPS'17), 2017.

E. Voita, D. Talbot, F. Moiseev, R. Sennrich, và I. Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.

D. Wang, M. Li, C. Gong, và V. Chandra. AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling. arXiv:2011.09011 [cs.CV], 2021.

J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, và Q. V Le. Finetuned language models are zero-shot learners. Trong International Conference on Learning Representations (ICLR'22), 2022.

C. White, S. Nolen, và Y. Savani. Exploring the loss landscape in neural architecture search. Trong Proceedings of the 37th conference on Uncertainty in Artificial Intelligence (UAI'21), 2021a.

C. White, A. Zela, R. Ru, Y. Liu, và F. Hutter. How powerful are performance predictors in neural architecture search? Trong Proceedings of the 35th International Conference on Advances in Neural Information Processing Systems (NeuRIPS'21), 2021b.

J. Xu, X. Tan, R. Luo, K. Song, J. Li, T. Qin, và T. Liu. NAS-BERT: task-agnostic and adaptive-size bert compression with neural architecture search. Trong Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD'21), 2021.

--- TRANG 16 ---
Xuất bản trong Transactions on Machine Learning Research (09/2024)

A. Yang, P. M. Esperança, và F. M. Carlucci. NAS valuation is frustratingly hard. Trong International Conference on Learning Representations (ICLR'20), 2020.

J. Yu, L. Yang, N. Xu, J. Yang, và T. Huang. Slimmable neural networks. Trong International Conference on Learning Representations (ICLR'19), 2019.

J. Yu, P. Jin, H. Liu, G. Bender, P. J. Kindermans, M. Tan, T. Huang, X. Song, R. Pang, và Q. Le. BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models. Trong The European Conference on Computer Vision (ECCV'20), 2020.

E. Zitzler, L. Thiele, M. Laumanns, C. M. Fonseca, và V. G. Da Fonseca. Performance assessment of multiobjective optimizers: An analysis and review. IEEE Transactions on evolutionary computation, 2003.

B. Zoph và Q. V. Le. Neural architecture search with reinforcement learning. Trong International Conference on Learning Representations (ICLR'17), 2017.

A Siêu tham số

Bảng A hiển thị các siêu tham số để tinh chỉnh super-network. Chúng tôi chủ yếu theo các siêu tham số mặc định được khuyến nghị bởi thư viện HuggingFace transformers. Đối với tất cả các phương pháp tìm kiếm đa mục tiêu, chúng tôi theo siêu tham số mặc định của Syne Tune.

Siêu tham số | Giá trị
Tỷ lệ Học | 0.00002
Số lượng mạng con ngẫu nhiên k | 2
Nhiệt độ T | 10
Kích thước Batch | 4

B Masking

Thuật toán 1, 2, 3 và 4 hiển thị mã giả cho không gian tìm kiếm LAYER, SMALL, MEDIUM và LARGE, tương ứng. Lưu ý rằng, 1 chỉ ra một vector gồm các số một. Đối với ma trận M, chúng ta viết M[:,:N] để biểu thị N cột đầu tiên cho tất cả các hàng và, ngược lại, M[:N,:] cho N hàng đầu tiên.

[Thuật toán 1-4 được trình bày với mã giả cho các hàm CREATEMASK khác nhau]

C Tập dữ liệu

Chúng tôi sử dụng 10 tập dữ liệu phân loại văn bản sau. Tất cả tập dữ liệu đều là các tác vụ phân loại, ngoại trừ STSB, là tập dữ liệu hồi quy.

• Tập dữ liệu Recognizing Textual Entailment (RTE) nhằm xác định sự kéo theo văn bản của hai câu.

• Tập dữ liệu Microsoft Research Paraphrase Corpus (MRPC) bao gồm các cặp câu được trích xuất từ các nguồn tin tức trực tuyến. Tác vụ là dự đoán liệu các cặp này có tương đương về mặt ngữ nghĩa với nhau hay không.

• Semantic Textual Similarity Benchmark (STSB) bao gồm các cặp câu được chấm điểm từ 1 đến 5 dựa trên sự tương tự của chúng.

• Tập dữ liệu Corpus of Linguistics Acceptability (COLA) chứa các câu tiếng Anh được gắn nhãn là đúng ngữ pháp hay không.

• Tập dữ liệu IMDB cho phân loại tình cảm (tích cực/tiêu cực) của các đánh giá phim.

• Tập dữ liệu Stanford Sentiment Treebank (SST2) phân loại tình cảm tích cực/tiêu cực của các câu được trích xuất từ đánh giá phim.

• Tập dữ liệu Situations With Adversarial Generations (SWAG) cho câu hỏi trắc nghiệm/trả lời.

• QNLI là phiên bản sửa đổi của Stanford Question Answering Dataset là tập hợp các cặp câu hỏi/câu trả lời trong đó các câu hỏi được viết bởi người chú thích và câu trả lời được trích xuất từ Wikipedia. Tác vụ là dự đoán liệu câu trả lời có đúng hay không.

--- TRANG 17 ---
[Tiếp tục với các phần D, E và F như trong bản gốc]
