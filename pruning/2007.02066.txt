# 2007.02066.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2007.02066.pdf
# File size: 937022 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1
Weight-dependent Gates for Network Pruning
Yun Li, Zechun Liu, Weiqun Wu, Haotian Yao, Xiangyu Zhang, Chi Zhang, and Baoqun Yin
Abstract —In this paper, a simple yet effective network pruning
framework is proposed to simultaneously address the problems of
pruning indicator, pruning ratio, and efﬁciency constraint. This
paper argues that the pruning decision should depend on the
convolutional weights, and thus proposes novel weight-dependent
gates (W-Gates) to learn the information from ﬁlter weights and
obtain binary gates to prune or keep the ﬁlters automatically.
To prune the network under efﬁciency constraints, a switchable
Efﬁciency Module is constructed to predict the hardware latency
or FLOPs of candidate pruned networks. Combined with the
proposed Efﬁciency Module, W-Gates can perform ﬁlter pruning
in an efﬁciency-aware manner and achieve a compact network
with a better accuracy-efﬁciency trade-off. We have demonstrated
the effectiveness of the proposed method on ResNet34, ResNet50,
and MobileNet V2, respectively achieving up to 1.33/1.28/1.1
higher Top-1 accuracy with lower hardware latency on ImageNet.
Compared with state-of-the-art methods, W-Gates also achieves
superior performance.
Index Terms —Weight-dependent gates, switchable Efﬁciency
Module, Accuracy-efﬁciency trade-off, Network pruning
I. I NTRODUCTION
IN recent years, convolutional neural networks (CNNs)
have achieved state-of-the-art performance in many tasks,
including but not limited to image classiﬁcation [1], [2],
semantic segmentation [3], and object detection [4], [5], etc.
Despite their great success, billions of ﬂoat-point-operations
(FLOPs) cost and long inference latency are still prohibitive
for CNNs to deploy on many resource-constraint hardware.
As a result, a signiﬁcant amount of effort has been invested in
CNNs compression and acceleration, in which ﬁlter pruning
[6], [7] is seen as an intuitive and effective network compres-
sion method.
However, ﬁlter pruning is non-trivial and faces three major
challenges. 1) Pruning indicator: CNNs are usually seen as a
black box, and individual ﬁlters may play different roles within
and across different layers in the network. Thus, it is difﬁcult
to manually design indicators that can fully quantify the
importance of their internal convolutional ﬁlters and feature
maps. 2) Pruning ratio: How many ﬁlters should be pruned
in each layer? The redundancy varies from different layers,
making it a challenging problem to set appropriate pruning
ratios for different layers. 3) Efﬁciency constraint: Most pre-
vious works only adopt hardware-agnostic metrics such as
Yun Li and Baoqun Yin are with University of Science and Technology of
China, Hefei, China (e-mail: yli001@mail.ustc.edu.cn, bqyin@ustc.edu.cn).
(Corresponding author: Baoqun Yin.)
Zechun Liu is with Hong Kong University of Science and Technology,
Hong Kong, China (e-mail: zliubq@connect.ust.hk).
Weiqun Wu is with Chongqing University, Chongqing, China (e-mail:
wuwq@cqu.edu.cn)
Haotian Yao, Xiangyu Zhang, and Chi Zhang are with Megvii Inc.,
Beijing, China (email: yaohaotian@megvii.com, zhangxiangyu@megvii.com,
zhangchi@megvii.com).parameters or FLOPs to evaluate the efﬁciency of a CNN.
But the inconsistency between hardware-agnostic metrics and
actual efﬁciency [8] lead to an increasing industrial demand
on directly optimizing the hardware latency.
Previous works have tried to address these issues from dif-
ferent perspectives. Conventional ﬁlter pruning works mainly
rely on manual-designed indicators [7], [9], [10] or data-driven
indicators [11], [12]. However, manual-designed indicators
usually involve human participation, and data-driven pruning
indicators may be affected by the feature maps. Besides, in
previous works, the pruning ratio of each layer or a global
pruning threshold is usually human-speciﬁed, making the
results prone to be trapped in sub-optimal solutions [13].
In this paper, we propose a simple yet effective ﬁlter pruning
method, which can automatically obtain the pruning decision
and the pruning ratio of each layer while considering the
overall efﬁciency of the network, as shown in Fig. 1.
To address the issue of the pruning indicator, we propose
weight-dependent gates (W-Gates). Instead of designing a
manual indicator or data-driven scale factors, we argue that
the pruning decision should depend on the ﬁlter itself, in
other words, it should be a learnable function of ﬁlter weights.
Thus, we propose a type of novel weight-dependent gates to
directly learn a mapping from ﬁlter weights to ﬁlter gates. W-
Gates takes the weights of a convolutional layer as input to
learn information from them, and outputs binary gates to open
or close the corresponding ﬁlters automatically (0: close, 1:
open) during the training process. Each W-Gates consists of a
fully connected layer and binary activation, which is simple to
implement and train given the pre-trained CNN model. More
importantly, the output ﬁlter gates here are weights-dependent,
which is the essential difference between the proposed W-
Gates and conventional data-driven methods [11], [12], [14].
To address the issue of the efﬁciency constraint, we propose
a switchable Efﬁciency Module, which provides two options
to cope with different scenarios, a Latency Prediction Net-
work (LPNet) and FLOPs Estimation. To prune the network
under hardware constraint, we propose LPNet to predict
the hardware latency of candidate pruned networks. LPNet
takes the network encoding vector as input and outputs a
predicted latency. After trained ofﬂine based on the latency
data collected from the target hardware, the LPNet is able to
provide latency guidance for network pruning. Considering
that hardware information is not always available, we add
the FLOPs Estimation as an auxiliary unit in the Efﬁciency
Module. For scenarios without hardware information, we can
switch the Efﬁciency Module to the FLOPs Estimation and
provide FLOPs guidance in a gradient manner for the pruning.
With the proposed W-Gates and the switchable Efﬁcient
Module, we construct a differential pruning framework to
perform ﬁlter pruning. We ﬁnd that the output of W-GatesarXiv:2007.02066v4  [cs.CV]  14 May 2022

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2
Reshape×Filter GatesFilter GatesWeight-dependent Gates
Weight-dependent GatesWeight-dependent GatesBinary ActivationInput…𝑪𝒊𝒏𝑪𝒐𝒖𝒕…Weights Tensor 𝑾∗FC layer𝒇𝑾∗𝒔𝟏𝒔𝟐…𝒔𝑪𝒐𝒖𝒕Scores…𝑪𝒊𝒏…
[3   20   16 …64  ]Network Encoding…𝑪𝒊𝒏…Accuracy Loss𝑮𝒂𝒕𝒆𝒔
Sumi-thconv-layer(i+1)-thconv-layer(i+2)-thconv-layer
Layer EncodingLayer EncodingLayer Encoding𝑮𝒂𝒕𝒆𝒔𝑮𝒂𝒕𝒆𝒔𝑪𝒐𝒖𝒕𝑪𝒐𝒖𝒕𝑪𝒐𝒖𝒕××
Efficiency LossAccuracy GradientEfficiency LossLatency Prediction NetworkFLOPs EstimationEfficiency Module
[10011]
Fig. 1. Pruning framework overview. There are two main parts in our framework, Weight-dependent Gates (W-Gates) and a switchable Efﬁciency Module.
The W-Gates learns the information from the ﬁlter weights of each layer and generates binary ﬁlter gates to open or close corresponding ﬁlters automatically
(0: close, 1: open). The ﬁlter gates of each layer are then summed to obtain a layer encoding and all the layer encodings constitute a network encoding. Next,
the network encoding of the candidate pruned network is fed into Efﬁciency Module to get the predicted latency or FLOPs of the candidate pruned network.
During the pruning process, accuracy loss and efﬁciency loss compete against each other to obtain a compact model with a better accuracy-efﬁciency trade-off.
can be summed as a layer encoding to determine the pruning
ratio of each layer, and all the layer encoding can constitute the
network encoding vector of the candidate pruned architecture.
Therefore, the network encoding is fed to the Efﬁciency
Module to obtain the latency or FLOPs guidance. Then, Given
the pre-trained CNN model and the LPNet, we can carry out
network pruning in an end-to-end manner. To perform ﬁlter
pruning during training, we deﬁne an efﬁciency-aware loss
function that consists of the accuracy loss and the efﬁciency
loss. The whole pruning framework is fully differentiable,
allowing us to simultaneously impose the gradients of accuracy
loss and efﬁciency loss to optimize the W-Gates of each
layer. Retaining more ﬁlters will keep more information in
the network, which is beneﬁcial to the ﬁnal accuracy. On the
contrary, pruning more ﬁlters will improve the efﬁciency of the
network inference. To this end, during training, the accuracy
loss will pull the binary gates to more ‘1’s (retaining more
ﬁlters), while the efﬁciency loss pulls the binary gates to more
‘0’s (pruning more ﬁlters). They compete against each other
and ﬁnally obtain a compact network with the better accuracy-
efﬁciency trade-off.
It is worth mentioning that the pruning decision and the
pruning ratio of each layer can all be obtained automatically,
which involves little human partipication. As the input of
the Efﬁciency Module is the network encoding, the pruning
framework can carry out ﬁlter pruning with consideration of
the overall network architecture, which is beneﬁcial for ﬁnding
optimal pruning ratios for different layers. The proposed W-
Gates can be applied in various CNN-based computer vision
tasks to compress and accelerate their backbones or the entire
networks.
We evaluate our method on ResNets [1], MobileNet V2[8], and VGG [15]. Comparing with uniform baselines, we
consistently deliver much higher accuracy and lower latency.
With lower latency, we achieve 1.09%-1.33% higher accuracy
than ResNet34, 0.67%-1.28% higher accuracy than ResNet50,
and 1.1% higher accuracy than MobileNet V2 on ImageNet
dataset. Compared to other state-of-the-art pruning methods
[16]–[24], our method also produces superior results.
The main contributions of our paper are three-fold:
We propose a kind of novel weight-dependent gates (W-
Gates) to directly learn a mapping from ﬁlter weights to
ﬁlter gates. W-Gates takes the convolutional weights as
input and generates binary ﬁlter gates to prune or keep
the ﬁlters automatically during training.
A switchable Efﬁciency Module is constructed to predict
the hardware latency or FLOPs of candidate pruned
networks. Efﬁciency Module is fully differentiable with
respect to W-Gates, allowing us to impose efﬁciency
constraints based on gradients and obtain a compact
network with a better accuracy-efﬁciency trade-off.
The proposed method simultaneously addresses three
challenges in ﬁlter pruning (pruning indicator, pruning
ratio, efﬁciency constraint). Compared with state-of-the-
art ﬁlter pruning methods, the proposed method achieves
superior performance.
This paper extends the preliminary workshop paper [25] in
the following aspects. 1) We propose a switchable Efﬁciency
Module to cope with various scenarios, so that the pruning
framework can not only perform ﬁlter pruning under the
latency constraint, but also under FLOPs constraints. 2) We
generalize our idea of weight-dependent gates and latency
predict network, enabling the application of our W-Gates from
a single ResNet architecture to more types of CNN archi-

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3
tectures, such as MobileNetv2, VGGNet, etc. 3) We conduct
four important ablation studies to illustrate the rationality of
the W-Gates design in our method. We show that W-Gates
can indeed learn information from ﬁlter weights and make a
good ﬁlter selection automatically, which helps the network
training and obtains higher accuracy. 4) We investigate the
effects of the key coefﬁcient in the proposed loss function
on the ﬁnal accuracy-latency trade-off. 5) We generalize our
idea from a single dataset to more dataset scenarios. 6) We
conduct visualization analysis on the pruned architectures. We
show that W-Gates trends not to prune the layers with the
downsample operations, and prunes more 33convolutional
layers than 11layers.
The rest of this paper is structured as follows. In Section
II, we brieﬂy review the related work. Section III details our
proposed method. Experimental settings, ablation study, and
results analysis are presented in Section IV followed by the
conclusion in Section V .
II. R ELATED WORK
A signiﬁcant amount of effort has been devoted to deep
model compression, such as matrix decomposition [26], [27],
quantization [28], [29], knowledge distillation [30], [31], com-
pact architecture learning [8], [32], neural network search [33],
[34], inference acceleration [35], [36], and network pruning
[37]–[39]. Pruning is an intuitive and effective network com-
pression method. Prior works devote to weights pruning. [37]
proposes to prune unimportant connections whose absolute
weights are smaller than a given threshold, which achieves
good performance on parameter compression. However, it is
not implementation friendly and can not obtain faster inference
without dedicated sparse matrix operation hardware. To tackle
this problem, some ﬁlter pruning methods [7], [40], [41] have
been explored recently. These methods prune or sparse parts
of the network structures (e.g., neurons, channels) instead of
individual weights, so they usually require less specialized
libraries to achieve inference speedup. In this paper, our work
also falls into ﬁlter pruning. Next, we mainly discuss the works
which are most related to our work.
A. Manual-designed indicator
Many excellent ﬁlter pruning methods based on manual-
designed indicators have been proposed to compress large
CNNs. [7] present an acceleration method for CNNs, where we
prune ﬁlters from CNNs that are identiﬁed as having a small
effect on the output accuracy. [9] proposes an iterative two-
step algorithm to effectively prune each layer, by a LASSO
regression based channel selection and least square reconstruc-
tion. [41] formally establishes ﬁlter pruning as an optimization
problem, and reveals that we need to prune ﬁlters based on
statistics information computed from its next layer, not the
current layer. The above two methods all prune ﬁlters based on
feature maps. Different from the above data-driven methods,
[10] proposes a feature-agnostic method, which prunes ﬁlter
based on a kernel sparsity and entropy indicator. [42] proposes
a soft ﬁlter pruning method, which prunes ﬁlters based on
L2norm and updates the pruned model when training themodel after pruning. [43] introduces a binary global mask
after each ﬁlter to dynamically and iteratively prune and tune
the network, with the mechanism to recall ﬁlters that are
mistakenly pruned in the previous iterations. [17] proposes a
Geometric Median based ﬁlter pruning method, which prunes
ﬁlters with relatively less contribution and chooses the ﬁlters
with the most replaceable contribution. These methods above
need to manually set a global pruning ratio [10], [43] or
layer pruning ratios [7], [9], [17], [41], which is difﬁcult to
fully consider the redundancy of different layers. To solve
this problem, [44] establishes a graph for each convolutional
layer of a CNN and uses two quantities associated with the
graph to obtain the redundancy and the pruning ratio of each
layer. However, a common problem of these manual-designed
pruning indicators is that they usually involve human partici-
pation, which makes them prone to be trapped in sub-optimal
solutions. To tackle this issue, our proposed method introduces
Weight-dependent Gates to learn the pruning decision from
ﬁlter weights automatically and cooperate with Efﬁciency
Module to determine the pruning ratio of each layer, which
involves little human participation.
B. Data-driven Pruning Methods
There are some other ﬁlter pruning methods that propose
data-driven indicators. [12] imposes L1 regularization on the
scaling factors in batch normalization (BN) layers to identify
insigniﬁcant ﬁlters. Similarly, [11] introduces scaling factors
to scale the outputs of speciﬁc structures, such as neurons,
groups, or residual blocks, and then do network pruning in an
end-to-end manner. [45] proposes to prune both channels in-
side and outside the residual connections via a KL-divergence
based criterion to compensate for the weakness of limited data.
[46] proposes gates with differentiable polarization to control
the on-and-off of each channel or whole layer block, which is
essentially a kind of data-driven gates. ManiDP [47] explores
a data-driven paradigm for dynamic pruning, which explores
the manifold information of all samples in the training process
and derives the corresponding sub-networks to preserve the
relationship between different instances. ResRep [48] proposes
to re-parameterize a CNN into the remembering parts and
forgetting parts, where the former learn to maintain the per-
formance and the latter learn to prune. [49] proposes a data-
dependent soft pruning method, dubbed Squeeze-Excitation-
Pruning (SEP), which does not physically prune any ﬁlters
but selects different ﬁlters for different inputs. [50] proposes
a data-driven pruning indicator, in which the task-irrelevant
channels are removed in a task-driven manner. AutoPruner
[14] proposed a data-driven channel selection layer, which
takes the feature map of the previous layer as input and
generates a binary index code for pruning. AutoPrunner [14]
are most similar to our W-Gates. The major difference is
that our W-Gates is weight-dependent, which directly learns
information from ﬁlter weights instead of feature maps. The
second difference is that AutoPrunner adopts a scaled sigmoid
function as soft binarization to generate an approximate binary
vector, while we adopt hard binarization (a variant of sign
function) to obtain true binary gates during training.

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4
C. Quantization and Binary Activation.
Quantization [29] proposes to quantize the real value
weights into binary/ternary weights to yield a large amount of
model size saving. [51] proved that good performance could
be achieved even if all neurons and weights are binarized to
1. [28] proposes to use the derivative of the clip function to
approximate the derivative of the sign function in the binarized
networks. [52] relaxes the discrete mask variables to be con-
tinuous random variables computed by the Gumbel Softmax
[53] to make them differentiable. [54] uses an identity shortcut
to add the real-valued outputs to the next most adjacent real-
valued outputs, and then adopt a tight approximation to the
derivative of the non-differentiable sign function with respect
to real-valued activation, which inspired our binary activation
and gradient estimation in W-Gates.
D. Resource-Constrained Compression
Recently, real hardware performance has attracted more
attention compared to FLOPs. AutoML methods [55], [56]
propose to prune ﬁlters iteratively in different layers of a
CNN via reinforcement learning or an automatic feedback
loop, which take real-time latency as a constraint. Some recent
works [52], [57] introduce a look-up table to record the latency
of each operation or each layer and sum them to obtain the
latency of the whole network. This method is valid for many
CPUs and DSPs, but may not for parallel computing devices
such as GPUs. [58] treats the hardware platform as a black
box and creates an energy estimation model to predict the
latency of speciﬁc hardware as an optimization constraint. [19]
proposes DMCP to searching optimal sub-structure from un-
pruned networks under FLOPs constraint. [59] trains a stand-
alone neural network to predict sub-networks’ performance
and then maximize the output of the network as a proxy of
accuracy to guide pruning. [13] proposes PruningNet, which
takes the network encoding vector as input and output weight
parameters of the pruned network. These above works inspired
our Efﬁciency Module and the LPNet training. We train the
LPNet to predict the latency of the target hardware platform,
which takes the network encoding vector as input and outputs
the predicted latency.
III. P ROPOSED METHOD
In this section, we introduce the proposed method which
adopt an efﬁciency-aware approach to prune the CNN archi-
tecture under multiple constraints. Following the work [13], we
formulate the network pruning as a constrained optimization
problem:
arg min
wc`(c;wc)
s:t: Inference (c)Const(1)
where`is the loss function speciﬁc to a given learning task,
andcis the network encoding vector, which is a set of the
pruned network channel width (c1;c2;;cl).Inference (c)
denotes the real latency or FLOPs of the pruned network,
which depends on the network channel width set c.wcmeans
×input…𝑪𝒊𝒏𝑪𝒐𝒖𝒕…𝑮𝒂𝒕𝒆𝒔i-thconv-layer
Binary ActivationReshape𝒇𝑾∗Weight-dependent GatesWeights tensor 𝑾∗𝑪𝒐𝒖𝒕FC layer𝒔𝟏𝒔𝟐…𝒔𝑪𝒐𝒖𝒕output
Scores[10011]Fig. 2. The proposed Weight-dependent Gates. It introduces a fully connected
layer to learn the information from the reshaped ﬁlter weights Wand
generates a score for each ﬁlter. After binary activation, we obtain the ﬁlter
gates (0 or 1) to open or close the corresponding ﬁlters automatically (0:
close, 1: open). The gates are placed after the BN transform and ReLU.
the weights of the remained channels and Const is the given
latency or FLOPs constraint.
To solve this problem, we propose an efﬁciency-aware net-
work pruning framework, in which the weight-dependent gates
is the key part. Firstly, we propose the Weight-dependent Gates
Module (W-Gates), which is adopted to learn the information
from ﬁlter weights and generate binary gates to determine
which ﬁlters to prune automatically. The W-Gates works
as adaptive pruning indicator. Then, a Efﬁciency Module is
constructed to provide the latency or FLOPs constraint to the
pruning indicator and guide the pruning ratios of each layer.
As key parts of the efﬁcient module, a Latency Prediction
Network (LPNet) is trained ofﬂine to predict the real latency
of a given architecture in speciﬁc hardware, and FLOPs
Estimation is set to predict FLOPs for candidate architectures.
The two modules above complement each other to generate
the pruning strategy and obtain the best CNN model under
efﬁciency constraint.
A. Weight-dependent Gates
The convolutional layer has always been adopted as a black
box, and we could only judge from the output what it has done.
Conventional ﬁlter pruning methods mainly rely on hand-craft
indicators or optimization-based indicators. They share one
common motivation: they are essentially looking for a pruning
function that can map ﬁlter weights to ﬁlter gates. However,
their pruning function usually involves human participation.
We argue that the gates should depend on the ﬁlters them-
selves, in other words, it is a learnable function of ﬁlter
weights. Thus, instead of designing a manual indicator, we

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5
xy
xy
xy-1012
0 1𝜆(𝑥)
0 1 0 1 0 1𝜎(𝑥)𝜕𝜎(𝑥)
𝜕𝑥𝜕𝜆(𝑥)
𝜕𝑥
(a) (b)
xy
Fig. 3. (a) The proposed binary activation function and its derivative. (b) The designed differentiable piecewise polynomial function and its derivative, and
this derivative is used to approximate the derivative of binary activation function in gradients computation.
directly learn the pruning function from the ﬁlter weights,
which is a direct reﬂection of the characteristics of ﬁlters.
To achieve the above goal, we propose the Weight-dependent
Gates (W-Gates). W-Gates takes the weights of a convolutional
layer as input to learn information, and output binary ﬁlter
gates as the pruning function to remove ﬁlters adaptively.
Filter Gates Learning. LetW2RClCl 1KlKldenotes
the weights of a convolutional layer, which can usually be
modeled as Clﬁlters and each ﬁlter Wi2RCi 1KlKl;i=
1;2;;Cl.Klis the ﬁlter size of the l-th layer. To ex-
tract the information in each ﬁlter, a fully-connected layer,
whose weights are denoted as_W2R(Cl 1KlKl)1, is
introduced here. Then, reshaped to two-dimensional tensor
W2RCl(Cl 1KlKl), the ﬁlter weights are input to the
fully-connected layer to generate the score of each ﬁlter:
sr=f(W) =W_W; (2)
wheresr=
sr
1;sr
2;:::;sr
Cl
denotes the score set of the ﬁlters
in this convolutional layer.
To suppress the expression of ﬁlters with lower scores and
obtain binary ﬁlter gates, we introduce the following activation
function:
(x) =Sign (x) + 1
2: (3)
The curve of Eq. (3) is shown in Fig. 3(a). We can see that
after processing by the activation function, the negative scores
will be converted to 0, and positive scores will be converted
to 1. Then, we get the binary ﬁlter gates of this layer:
gatesb=(sr) =(f(W)): (4)
Different from the path binarization [60] in neural architec-
ture search, in ﬁlter pruning tasks, the pruning decision should
depend on the ﬁlter weights, in other words, our proposed
binary ﬁlter gates are weights-dependent, as shown in Eq.
(4). Different from the time stepping controller [61] which
is weight-dependent, the weight-dependent gates are proposed
as pruning indicators.
Next, we sum the binary ﬁlter gates of each layer to obtain
the layer encoding, and all the layer encodings form the
network encoding vector c. The layer encoding here denotes
the number of ﬁlters kept, which can also determine the
pruning ratio of each layer.
Gradient Estimation. As can be seen from Fig. 3, the
derivative of function ()is an impulse function, whichcannot be used directly during the training process. Inspired by
the recent Quantized Model works [28], [54], speciﬁcally Bi-
Real Net [54], we introduce a differentiable approximation of
the non-differentiable function (x). The gradient estimation
process is as follows:
@L
@Xr=@L
@Xb@Xb
@Xr=@L
@Xb@(Xr)
@Xr@L
@Xb@(Xr)
@Xr;(5)
whereXrdenotes the real value output sr
i,Xbmeans the
binary output. (Xr)is the approximation function we de-
signed, which is a piecewise polynomial function:
(Xr) =8
>><
>>:0; if X r< 1
2
2Xr+ 2X2
r+1
2; if 1
2Xr<0
2Xr 2X2
r+1
2; if 0Xr<1
2
1; otherwise;(6)
and the gradient of above approximation function is:
@(Xr)
@Xr=8
<
:2 + 4Xr; if 1
2Xr<0
2 4Xr; if 0Xr<1
2
0; otherwise:(7)
As discussed above, we can adopt the binary activation
function Eq. (3) to obtain the binary ﬁlter gates in the
forward propagation, and then update the weights of the fully-
connected layer with an approximate gradient Eq. (7) in the
backward propagation.
B. Efﬁciency Module
With the proposed W-Gates, we can carry out ﬁlter scor-
ing and selection based on the information in convolutional
weights. However, to determine which structure is more ef-
ﬁcient for inference, the hardware or FLOPs information is
also necessary. For the motivation above, in this section, we
introduce an Efﬁciency Module, which contains a Latency
Prediction Network and a FLOPs Estimation unit to provide
efﬁciency guidance for the proposed W-Gates. As shown in
Fig. 1, the Efﬁciency Module is a switchable module, in which
the Latency Prediction Network is the main unit to select
hardware-friendly architectures and the FLOPs Estimation is
the substitute unit to provide FLOPs constraint.
Latency Prediction Network. Previous works on model
compression aim primarily to reduce the parameters and
calculations, but they does not always reﬂect the actual latency
on hardware. Therefore, some recent NAS-based methods [13],
[52] pay more attention to adopt the hardware latency as a

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6
Predicted LatencyFCLatency Prediction NetworkNetwork EncodingHardwareDevicesDecodingReal  LatencyMSE LossFCFC[…64  ]320[…96  ]1816[…22  ]48
Fig. 4. The ofﬂine training process of LPNet. ReLU is placed after the ﬁrst two FC layers. LPNet takes network encoding vectors as the input data and the
measured hardware latency as the ground truth. Mean Squared Error (MSE) loss function is adopted here.
direct evaluation indicator than parameters and FLOPs. [52]
proposes to build a latency lookup table to estimate the overall
latency of a network, as they assume that the runtime of
each operator is independent of other operators, which works
well on many mobile serial devices, such as CPUs and DSPs.
Previous works [13], [52] have demonstrated the effectiveness
of this method. However, the latency generated by the lookup
table is not differentiable with respect to the ﬁlter selection
and the pruning ratio of each layer.
To address the above problem, we construct LPNet to
predict the real latency of the whole network or building
blocks. The proposed LPNet is fully differentiable with respect
to ﬁlter gates and the pruning ratio of each layer. As shown
in Fig. 4, the LPNet consists of three fully-connected layers,
which takes a network encoding vector c= (c1;c2;:::;c L)as
input and output the latency for speciﬁed hardware platform:
lat(c) = LPNet ( c1;c2;:::;c L); (8)
where
cl=ClX
i=1gatesb
i (9)
in the pruning framework.
To pretrain the LPNet, we sample network encoding vectors
cfrom the search space and decode the corresponding network
to test their real latency on speciﬁc hardware platforms. As
shown in Fig. 4, during the training process, the network
encoding vector is adopted as input and the latency on speciﬁc
hardware is used as the label. As there is no need to train the
decoding network, it takes only a few milliseconds to get a
latency label and the training of LPNet is also very efﬁcient.
For deeper building block architecture, such as ResNet50,
the network encoding sampling space is huge. We choose to
predict the latency of building blocks and then sum them up
to get the predicted latency of the overall network, and this
will greatly reduce the encoding sampling space. Besides, the
LPNet of building blocks can also be reused across models
of different depths and different tasks on the same type of
hardware.
As a result, training such a LPNet makes the latency
constraint differentiable with respect to the network encoding
and binary ﬁlter gates shown in Fig. 1. Thus we can use
gradient-based optimization to adjust the ﬁlter pruning ratioof each convolutional layer and obtain the best pruning ratio
automatically.
FLOPs Estimation. Although the hardware latency is the
most direct reﬂection of inference efﬁciency for the pruned
models, it is not always available. For scenarios with unknown
hardware information, we add the FLOPs Estimation in the
Efﬁcient Module as an alternative unit to optimize FLOPs by
gradient descent in the backward propagation.
Letcl 1denotes the number of input channels of layer l,
that is, the output channels of layer l 1.cldenotes the number
of the output channels of layer l. Inspired by [19], [62], the
FLOPs of convolutional layer lin the pruned network can bu
formulated as:
Fl=Ml
hMl
wKl
hKl
wcl 1cl; (10)
in which,Ml
handMl
ware the height and the width of the
input feature map in layer l, respectively. Kl
handKl
wdenote
the height and the width of the ﬁlter size, respectively.
Among all layer types, convolutional layers are the primary
performance hotspots [35]. Therefore, we focus on optimizing
the FLOPs of convolutional layers when carrying out pruning.
The FLOPs of a CNN with Lconvolutional layers can be
estimated as follows:
Flops (c) =LX
l=1Fi=LX
l=1Ml
hMl
wKl
hKl
wcl 1cl;(11)
wherec0= 3 (the number of channels of the input image)
andcl=PCl
i=1gatesb
iin the pruning framework.
The FLOPs Estimation above is based on ﬁlter gates and
training-free, which can be adopted to optimize the pruning
process by gradient descent in the scenario with unknown
hardware information.
C. Efﬁciency-aware Filter Pruning
If hardware information is available, the proposed method
consists of three main stages. First, training the LPNet ofﬂine,
as described in Section III-B. With the pretrained LPNet, we
can obtain the latency by inputting the encoding vector of a
candidate pruned network. Second, pruning the network under
latency constraint. We add W-Gates and the Efﬁciency Module
(the LPNet) to a pretrained network to do ﬁlter pruning, in
which the weights of LPNet are ﬁxed. As shown in Fig. 1, W-
Gates learns the information from convolutional weights and

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7
generates binary ﬁlter gates to determine which ﬁlters to prune.
Next, LPNet takes the network encoding of candidate pruned
net as input and output a predicted latency to optimize the
pruning ratio and ﬁlter gates of each layer. Then, the accuracy
loss and the efﬁciency loss compete against each other during
training and ﬁnally obtain a compact network with the best
accuracy while meeting the latency constraint. Third, ﬁne-
tuning the network. After getting the pruned network, a ﬁne-
tuning process with only a few epochs follows to regain
accuracy and obtain a better performance, which is less time-
consuming than training from scratch.
Furthermore, to make a better accuracy-efﬁciency trade-off,
we deﬁne the following efﬁciency-aware loss function:
`(c;wc) =Acc(c;wc) +log (1 + Eff(c)); (12)
where Acc (c;wc)denotes the accuracy loss of an architecture
with a network encoding cand parameters wc. Eff (c)is
the latency prediction lat(c)or FLOPs estimation Flops (c)
of the pruned architecture with network encoding vector c.
The coefﬁcient can modulate the magnitude of the latency
term. Such a loss function can carry out ﬁlter pruning tasks
with consideration of the overall network structure, which is
beneﬁcial for ﬁnding optimal solutions for network pruning.
In addition, this function is differentiable with respect to layer-
wise ﬁlter choices cand the number of ﬁlters, which allows us
to use a gradient-based method to optimize them and obtain
a better trade-off between accuracy and efﬁciency.
IV. E XPERIMENTS
In this section, we demonstrate the effectiveness of our
method. First, we give a detailed description of our experiment
settings. Next, we carry out four ablation studies on the Ima-
geNet dataset to illustrate the effect of the key part W-Gates in
our method. Then, we prune ResNet34, ResNet50, MobileNet
V2, and VGG16 under latency constraint, and compare our
method with several state-of-the-art ﬁlter pruning methods.
Afterward, we prune VGG16 and ResNet56 under FLOPs
constraint. Finally, we visualize the pruned architectures to
explore what our method has learned from the network and
what kind of architectures have a better trade-off between
accuracy and efﬁciency.
A. Experiment Settings
We carry out experiments on the ImageNet ILSVRC 2012
dataset [63] and Cifar-10 dataset [64]. ImageNet contains 1.28
million training images and 50,000 validation images, which
are categorized into 1000 classes. The resolution of the input
images is set to 224224. Cifar-10 consisting of 50,000
training images, 10,000 test images, which are categorized
into 10 classes. All images of Cifar-10 are cropped randomly
into3232with four paddings and horizontal ﬂip is also
applied. All the experiments are implemented with PyTorch
framework and networks are trained using stochastic gradient
descent (SGD) with momentum set to 0.9. For ResNet34 and
ResNet50, we adopt the same training scheme in [1].
To train the LPNet ofﬂine, we sample network encoding
vectorscfrom the search space and decode the correspondingnetwork to test their real latency on one NVIDIA RTX 2080 Ti
GPU as latency labels. For deeper building block architecture,
such as ResNet50, the network encoding sampling space is
huge. We choose to predict the latency of building blocks and
then sum them up to get the predicted latency of the overall
network, and this will greatly reduce the encoding sampling
space. For the bottleneck building block of ResNet, we collect
170000 (c, latency) pairs to train the LPNet of bottleneck
building block and 5000 (c, latency) pairs to train the LPNet of
basic building block. We randomly choose 80% of the dataset
as training data and leave the rest as test data. We use Adam
to train the LPNets and ﬁnd the average test errors can quickly
converge to lower than 2%.
For the pruning and the ﬁne-tuning processes, all networks
are trained using stochastic gradient descent (SGD) with
momentum set to 0.9. On ImageNet dataset, we respectively
train the ResNets and MobileNet V2 for 120 epochs and
240 epochs as baselines. For ResNet34/50, we respectively
set 40 and 80 epochs for the pruning and the ﬁne-tuning
processes. The initial learning rates of the two processes above
are respectively set to 10 3and10 2. For MobileNet V2,
we respectively set 120 and 80 epochs for the pruning and
the ﬁne-tuning processes. The initial learning rates of the two
processes above are set to 10 3and10 1, respectively. On
Cifar-10 dataset, for all the models, we set 200 epochs for
the pruning process and the ﬁne-tuning process. The initial
learning rates are set to 10 3and10 2, respectively. On the
two datasets, during the pruning and the ﬁne-tuning processes,
the leraning rates are all divided by 10 at 50% and 75% of
the total number of epochs.
B. Ablation Study on ImageNet
The performance of our method is mainly attributed to
the proposed Weight-dependent Gates (W-Gates). To validate
the effectiveness of W-Gates, we choose the widely used
architecture ResNet50 and conduct a series of ablation studies
on ImageNet dataset. In the following subsections, we ﬁrst
explore the impact of our proposed W-Gates in the training
process. Then, the impact of information learned from ﬁlter
weights is studied. After that, we illustrate the impact of
gate activation function by comparing binary activation with
scaled sigmoid activation. Finally, we compare our W-Gates-
based pruning method with several state-of-the-art gate-based
methods [12], [41], [42].
1) Impact of W-Gates in the Training Process: In this sec-
tion, to demonstrate the impact of W-Gates on ﬁlter selection,
we add them to a pretrained ResNet50 to learn the weights of
each convolutional layer and do ﬁlter selection during training.
Then, we continue training and test the ﬁnal accuracy.
Two sets of experiments are set up to test the effect of W-
Gates in different periods of the model training process. For
one of them, we ﬁrst train the network for 1/3 of total epochs
as a warm-up period and then add W-Gates to the network
to continue training. For the other experiment, we add W-
Gates to a pretrained model and test if it could improve the
training performance. As can be seen in Table I, with the same
number of iterations, adding W-Gates after a warm-up period

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8
TABLE I
THIS TABLE COMPARES THE ACCURACY ON IMAGE NET ABOUT
RESNET50. “W-G ATES (WARM -UP)”DENOTES ADDING W-G ATES TO THE
RESNET50AFTER A WARM -UP PERIOD . “W-G ATES (PRETRAIN )”MEANS
ADDING W-G ATES TO A PRETRAINED RESNET50. “W-G ATES (CONSTANT
INPUT )”ADOPTS A CONSTANT TENSOR INPUT TO REPLACE FILTER
WEIGHTS IN W-G ATES . “PRETRAIN +SAME ITERS ”MEANS CONTINUING
TO TRAIN THE NETWORK WITH THE SAME ITERATIONS AS
“W-G ATES (CONSTANT INPUT )”AND “W-G ATES (PRETRAIN )”.
Top1-Acc Top5-Acc
ResNet50 baseline 76.15% 93.11%
pretrain + same iters 76.58% 93.15%
W-Gates(constant input) 75.32% 92.48%
W-Gates(warm-up) 76.79% 93.27%
W-Gates(pretrain) 77.07% 93.44%
TABLE II
THIS TABLE COMPARES TWO KINDS OF ACTIVATION FUNCTION :BINARY
ACTIVATION AND SCALED SIGMOID ACTIVATION ,IN WHICH THE SCALE
FACTOR IS SET TO 4. (1G: 1 E9)
Top1-Acc Top5-Acc FLOPs
ResNet50 baseline 76.15% 93.11% 4.1G
W-Gates(sigmoid) 75.55% 92.62% 2.7G
W-Gates(binary) 76.01% 92.86% 2.7G
can achieve 0.64% higher Top-1 accuracy than baseline results.
Moreover, equipping W-Gates to a pretrained network can
continue to increase the accuracy by 0.92%, which is 0.49%
higher than the result of adding the same number of iterations.
These results show that adding the W-Gates to a well-trained
network can make better use of its channel selection impact
and obtain more efﬁcient convolutional ﬁlters.
2) Impact of Information from Filter Weights: We are
curious about such a question: Can the W-Gates really learn
information from convolutional ﬁlter weights and give instruc-
tion to ﬁlter selection? An ablation study is conducted to
answer this question. First, we add the modules to a pretrained
network and adopt constant tensors of the same size to replace
the convolutional ﬁlter weights tensors as the input of W-
Gates, and all the values of these constant tensors are set to
ones. Then we continue to train the network for the same
number of iterations with “W-Gates(pretrain)” in Table I.
From Table I, we see that the W-Gates with a constant tensor
as input achieves 1.75% lower Top-1 accuracy than the W-
Gates that input ﬁlter weights. The results above indicate that
W-Gates can learn information from ﬁlter weights and do a
good ﬁlter selection automatically, which contributes to the
network training.
3) Choice of Gate Activation Function: There are two kinds
of activation functions that can be used to obtain the ﬁlter
gates, scaled sigmoid [14] and binary activation [28], [54].
Previous methods adopt a sigmoid function or a scaled sigmoid
function to generate an approximate binary vector. In this
kind of method, a threshold needs to be set and the values
smaller than it are set to 0. Quantized model works propose
binary activation, which directly obtain a real binary vector and
design a differentiable function to approximate the gradient of
its activation function. We choose binary activation here as ourTABLE III
THIS TABLE COMPARES OUR W-G ATES METHOD WITH SEVERAL
STATE -OF-THE-ART GATE -BASED METHODS . FOR A FAIR COMPARISON ,
WE DO NOT ADD LPN ET TO OPTIMIZE THE PRUNING PROCESS AND ONLY
PRUNE THE NETWORK WITH L1- NORM .
Top1-Acc Top5-Acc FLOPs
ResNet50 baseline 76.15% 93.11% 4.1G
SFP [42] 74.61% 92.87% 2.4G
Thinet-70 [41] 75.31% - 2.9G
Slimming [12] 74.79% 92.21% 2.8G
W-Gates 76.01% 92.86% 2.7G
W-Gates 75.74% 92.62% 2.3G
gate activation function to generate the binary ﬁlter gates in
W-Gates.
To test the impact of gate function choice in our method, we
compare the pruning results of W-Gates with binary activation
and W-Gates with scaled sigmoid. The scale factor of sigmoid
kis set to 4, which follows the setting in [14]. To keep only
one factor changed, we do not adopt our LPNet to give the
latency constraint, but simply imposes L1 regularization on the
ﬁlter gates of each layer to obtain a sparse CNN model. As
can be seen in Table II, with the same FLOPs, W-Gates with
binary activation achieves 0.46% higher top-1 accuracy than
W-Gates with k-sigmoid activation, which proves the binary
activation is more suitable for our method.
4) Compare with Other Gate-based Methods: To further
examine whether the proposed W-Gates works well on ﬁlter
pruning, we compare our method with state-of-the-art gate-
based methods [12], [41], [42]. For Slimming [12], there
are no pruning results on ImageNet. To keep our setup as
close to the original paper as possible, we adopt the original
implementation publicly available and execute on Imagenet.
For a fair comparison, we do not add LPNet to optimize
the pruning process but simply prune the network with L1-
norm, which is consistent with other gate-based methods. The
results for ImageNet dataset are summarized in Table III. W-
Gates achieves superior results than SFP [42], Thinet [41] and
Slimming [12]. The results show that the proposed Weight-
dependent Gates can help the network to do a better ﬁlter
self-selection during training.
C. Pruning Results under Latency Constraint
The inconsistency between hardware agnostic metrics and
actual efﬁciency leads an increasing attention in directly
optimizing the latency on the target devices. Taking the CNN
as a black box, we train a LPNet to predict the real latency in
the target device. For ResNet34 and ResNet50, we train two
LPNets ofﬂine to predict the latency of basic building blocks
and bottleneck building blocks, respectively. To fully consider
all the factors and decode the sparse architecture, we add the
factors of feature map size and downsampling to the network
encoding vector. For these architectures with shortcut, we do
not prune the output channels of the last layer in each building
block to avoid mismatching with the shortcut channels.
1) Pruning results on ResNet34: We ﬁrst employ the
pruning experiments on a medium depth network ResNet34.

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9
TABLE IV
THIS TABLE COMPARES THE TOP-1ACCURACY AND LATENCY ON
IMAGE NET ABOUT RESNET34. W E SET THE SAME COMPRESSION RATIO
FOR EACH LAYER AS UNIFORM BASELINE . THE INPUT BATCH SIZE IS SET
TO100 AND THE LATENCY IS MEASURED USING PYTORCH ON NVIDIA
RTX 2080 T IGPU.
Uniform Baselines W-Gates
FLOPs Top1-Acc Latency FLOPs Top1-Acc Latency
3.7G (1X) 73.88% 54.04ms - - -
2.9G 72.56% 49.23ms 2.8G 73.76% 46.67ms
2.4G 72.05% 44.27ms 2.3G 73.35% 40.75ms
2.1G 71.32% 43.47ms 2.0G 72.65% 37.30ms
TABLE V
THIS TABLE COMPARES THE TOP-1ACCURACY AND LATENCY ON
IMAGE NET ABOUT RESNET50. T HE RESULTS SHOW THAT ,WITH THE
SAME FLOP S,OUR METHOD OUTPERFORMS THE UNIFORM BASELINES BY
A LARGE MARGIN IN TERMS OF ACCURACY AND LATENCY .
Uniform Baselines W-Gates
FLOPs Top1-Acc Latency FLOPs Top1-Acc Latency
4.1G (1X) 76.15% 105.75ms - - -
3.1G 75.59% 97.87ms 3.0G 76.26% 95.15ms
2.6G 74.77% 91.53ms 2.6G 76.05% 88.00ms
2.1G 74.42% 85.20ms 2.1G 75.14% 80.17ms
ResNet34 consists of basic building blocks, each basic build-
ing block contains two 33convolutional layers. We add the
designed W-Gates to the ﬁrst layer of each basic building block
to learn the information and do ﬁlter selection automatically.
LPNets are also added to the W-Gates framework to predict
the latency of each building block. Then we get the latency of
the whole network to guide the network pruning and optimize
the pruning ratio of each layer. The pruning results are shown
in Table IV. We set the same compression ratio for each layer
in ResNet34 as uniform baselines and prune the same layers
with W-Gates. We measure the real hardware latency using
Pytorch on NVIDIA RTX 2080 Ti GPU and the batch size of
input images is set to 100. It can be observed that our method
can save 25% hardware latency with only 0.5% accuracy loss
on ImageNet dataset. With the same FLOPs, W-Gates achieves
1.1% to 1.3% higher Top-1 accuracy than uniform baseline,
and the hardware latency is also lower, which shows that
our W-Gates method can automatically prune and obtain the
efﬁcient architectures.
2) Pruning results on ResNet50: For the deeper net-
work ResNet50, we adopt the same setting with ResNet34.
ResNet50 consists of bottleneck building blocks, each of
which contains a 33layer and two 11layers. We
employ W-Gates to prune the ﬁlters of the ﬁrst two layers in
each bottleneck module during training. The Top-1 accuracy
and hardware latency of pruned models are shown in Table
V. When pruning 37% FLOPs, we can save 17% hardware
latency without notable accuracy loss. Then we plot two sets of
pruning results on ResNet34 and ResNet50 in Fig. 5, in which
in Function (12) is set to 1.5 to modulate the magnitude of
latency term. As can be seen from the accuracy and latency
trends, during the training and selection process, as the latency
decreases, the accuracy ﬁrst drops and then rises. These twoTABLE VI
THE COEFFICIENT VALUES AND THE CORRESPONDING LATENCY OF
COMPACT NETWORK WHEN PRUNING RESNET34ONIMAGE NET. THE
COEFFICIENT CAN MODULATE THE MAGNITUDE OF LATENCY TERM IN
THE LOSS FUNCTION . AS SHOWN IN THE RESULTS ,THE FINAL ACTUAL
LATENCY WILL GRADUALLY DECREASE AS ALPHA INCREASES .
 0 (Baseline) 1.5 2.0 3.0
Top1 Acc 73.88% 73.76% 73.35% 72.65%
Latency 54.04ms 46.67ms 40.75ms 37.30ms
ﬁgures show that W-Gates tries to search for a better accuracy-
efﬁciency trade-off via ﬁlter learning and selection in the
training process.
3) The Effect of Coefﬁcient on the Final Trade-off:
As the whole pruning framework is fully differentiable, we
can simultaneously impose the gradients of accuracy loss
and latency loss to optimize the W-Gates of each layer. The
accuracy loss pulls the binary gates to more ones and the
latency loss pulls the binary gates to more zeros. They compete
against each other during training and ﬁnally obtain a compact
network with a better accuracy-efﬁciency trade-off.
In the proposed latency-aware loss function, the coefﬁcient
can modulate the magnitude of the latency term. Now,
we are interested in the effect of for the ﬁnal trade-off.
Table VI shows the values and the corresponding latency of
compact network when pruning ResNet34 on ImageNet. The
input batch size is set to 100 and the latency is measured using
Pytorch on NVIDIA RTX 2080 Ti GPU. It shows that the ﬁnal
actual latency will gradually decrease as alpha increases. For
a given alpha, the latency will gradually decrease with the
network training, and eventually converge to a best accuracy-
latency trade-off. It can be seen that our method achieves
1.33X latency acceleration with only 0.53% accuracy loss,
which shows that W-Gates is very robust for the choice of
and can consistently deliver lower latency without notable
accuracy loss.
4) Comparisons with State-of-the-arts.: In this section,
we compare the proposed method with several state-of-the-
art ﬁlter pruning methods, including manual-designed indi-
cators (IENNP [16], FPGM [17], VCNNP [22], and HRank
[18]), optimization-based methods (CCP [20], RRBP [23], C-
SGD [21]), and search-based methods (DMCP [19], and S-
MobileNet V2 [24]), on ResNet34, ResNet50, and MobileNet
V2. ‘W-Gates’ in Table VII denote our pruning results with
different hardware latency and FLOPs. As there is no latency
data provided in these works, we only compare the Top1
accuracy with the same FLOPs. It can be observed that,
compared with state-of-the-art ﬁlter pruning methods, W-Gates
achieves higher or comparable accuracy with the same FLOPs.
In particular, compared with C-SGD [21], W-Gates achieves
higher or comparable results (75.96% vs 75.27%, 75.14%vs
74.93% , 74.32% vs 74.54% ). Compared with Hrank [18], W-
Gates achieve higher accuracy under 2.4G FLOPs (75.96% vs
74.98%). When the FLOPs of Hrank is compressed to 1.6G, its
accuracy is only 71.98%, which is 2.34% lower than the 1.9G
model of W-Gates, although their FLOPs gap is only 0.3G.
This is because that W-Gates is trying to ﬁnd a better trade-

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10
(a)Resnet34(b) Resnet50
Fig. 5. Two sets of pruning experiments on ResNet34 and ResNet50 on ImageNet. We set in Eq.(12) to 1.5, which is used to modulate the magnitude of
the latency term. In the training process, as the latency decreases, the accuracy ﬁrst drops and then rises. These two ﬁgures show that W-Gates tries to ﬁnd a
better accuracy-latency trade-off via ﬁlter learning and selection in the training process.
TABLE VII
THIS TABLE COMPARES THE TOP-1 I MAGE NET ACCURACY OF OUR
W-G ATES METHOD AND STATE -OF-THE-ART PRUNING METHODS IENNP
[16], FPGM [17], VCNNP [22], HR ANK [18], DMCP [19], CCP [20],
RRBP [23], C-SGD [21], S-M OBILE NETV2 [24], SRR-GR [44],
CLR-RNF [65], HAP [66], F ILTER SKETCH [67] ONRESNET34,
RESNET50, AND MOBILE NETV2.
Model Methods FLOPs Top1 Acc
ResNet34IENNP [16] 2.8G 72.83%
FPGM [17] 2.2G 72.63%
W-Gates 2.8G 73.76%
W-Gates 2.3G 73.35%
ResNet50VCNNP [22] 2.4G 75.20%
FPGM [17] 2.4G 75.59%
FPGM [17] 1.9G 74.83%
IENNP [16] 2.2G 74.50%
DMCP [19] 2.2G 76.20%
CCP [20] 2.1G 75.50%
RRBP [23] 1.9G 73.00%
C-SGD-70 [21] 2.6G 75.27%
C-SGD-60 [21] 2.2G 74.93%
C-SGD-50 [21] 1.8G 74.54%
SRR-GR [44] 2.3G 75.76%
CLR-RNF [65] 2.5G 74.85%
HAP [66] 2.7G 75.12%
FilterSketch [67] 2.6G 75.22%
FilterSketch [67] 2.2G 74.68%
HRank [18] 2.3G 74.98%
HRank [18] 1.6G 71.98%
W-Gates 3.0G 76.26%
W-Gates 2.4G 75.96%
W-Gates 2.1G 75.14%
W-Gates 1.9G 74.32%
MobileNet V2S-MobileNet V2 [24] 0.30G 70.5%
S-MobileNet V2 [24] 0.21G 68.9%
0.75x MobileNetV2 [8] 0.22G 69.8%
W-Gates 0.29G 73.2%
W-Gates 0.22G 70.9%
off between performance and efﬁciency during the pruning
process.
5) Pruning Results on Cifar-10: In this section, we eval-
uate the proposed method on Cifar-10 dataset. The pruning
experiments are conducted on two general models on Cifar-
10, VGG16 and ResNet56. The results are shown in Table
VIII. It can be observed that the proposed W-Gates canTABLE VIII
THIS TABLE COMPARES THE PRUNING RESULTS UNDER LATENCY
CONSTRAINT ON CIFAR -10 ABOUT VGG16 AND RESNET56.
CONSIDERING THAT THE IMAGE SIZE OF CIFAR -10 IS VERY SMALL (ONLY
3232),WE SET THE TEST BATCH SIZE TO 104TO ENSURE THE
STABILITY OF THE LATENCY TEST RESULTS . (1M: 1 E6)
Model Methods FLOPs Top1 Acc Latency
VGG16baseline 313M 93.72% 391.1ms
VCNNP [22] 190M 93.18% -
HRank [18] 146M 93.43% -
W-Gates 162M 93.61% 245.0ms
ResNet56baseline 126.8M 93.85% 501.8ms
He et al. [9] 63.4M 92.64% -
He et al. [9] 62.0M 90.80% -
FPGM [17] 60.1M 92.89% -
FSDP [68] 64.4M 92.64% -
PARI [69] 60.1M 93.05% -
CHIP [70] 34.8M 92.05% -
ResRep [48] 28.1M 92.66% -
FilterSketch [67] 32.5M 91.20% -
HRank [18] 32.5M 90.72% -
W-Gates 60.0M 93.54% 366.6ms
W-Gates 31.1M 92.66% 319.7ms
also achieve good performance on Cifar-10 dataset, which
outperforms state-of-the-art methods VCNNP [22], HRank
[18], He et al. [9], FSDP [68], FPGM [17], PARI [69],
CHIP [70], ResRep [48], and FilterSketch [67]. For the plain
structure VGG16, W-Gates can reduce 37% hardware latency
and 48% FLOPs with only 0.11% accuracy loss. Similarly,
for the ResNet56 architecture with shortcut connections, W-
Gates can reduce 36% hardware latency and 75% FLOPs
with only 1.2% accuracy loss. Compared with state-of-the-art
methods ( [9], [17], [18], [22], [48], [67], [68], [69], [70]), W-
Gates achieves higher or comparable results. Compared with
ResRep [48], W-Gates has two advantages. First, in addition
to FLOPs optimization, W-Gates can directly optimize the
inference latency of CNNs on the target hardware platform in a
gradient-based manner, which is not available in ResRep [48].
Second, ResRep [48] gradually increases the pruning ratio of
each layer with a predeﬁned ﬁlter pruning step (step=4), while
W-gates can ﬂexibly increase or decrease the pruning ratio of

--- PAGE 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11
TABLE IX
THIS TABLE COMPARES THE PRUNING RESULTS UNDER FLOP S
CONSTRAINT ON CIFAR -10 ABOUT VGG16 AND RESNET56.
Model Methods FLOPs Top1 Acc
VGG16baseline 313M 93.72%
VCNNP [22] 190M 93.18%
HRank [18] 146M 93.43%
W-Gates 176M 93.49%
ResNet56baseline 126.8M 93.85%
He et al. [9] 62.0M 90.80%
HRank [18] 32.5M 90.72%
W-Gates 27.1M 91.60%
TABLE X
THIS TABLE COMPARES THE TOP -1ACCURACY OF THE PRUNED MODELS
BEFORE AND AFTER THE FINE -TUNING PROCESS UNDER FLOP S
CONSTRAINT .
Model Top-1 Acc (before) Top-1 Acc (after) 
VGG16 92.82% 93.49% 0.67%
ResNet56 91.20% 91.60% 0.40%
each layer based on the gradient in the backward propagation
to obtain a more hardware-friendly CNN structure. The results
in Table VIII can also prove the good generalization of the
proposed W-Gates.
D. Pruning Results under FLOPs Constraint
For scenarios that the hardware information is unavailable,
we can not directly optimize the pruning process under the
latency constraint. With the proposed Efﬁciency Module, we
can switch it to the FLOPs Estimation to cope well with
these scenarios. In this section, we evaluate the proposed
pruning framework under FLOPs constraint. The experiments
are conducted on CIFAR10.
Table IX summarizes the achieved improvement via apply-
ing FLOPs constraint on ﬁlter pruning. Our primary observa-
tion is that our method can have substantial FLOPs reduction
with negligible accuracy loss compared to other state-of-the-
art methods. For the plain architecture VGG16, our method
reduces 43.8% FLOPs with only 0.23% accuracy loss. For a
deeper model ResNet56, our method achieves 78.6% FLOPs
reduction with only 2.25% accuracy loss. From the results we
can also see that the accuracy in Table IX is slightly lower than
the performance of pruning under latency constraint in Table
VIII. The reason is that, FLOPs optimization may mainly
focuses on pruning the shallow layers of the network (the
primary FLOPs hotspots), which may damage the diversity
of low-level features extracted by the shallow layers of the
network. In contrast, the latency optimization pays more
attention to the hardware friendliness of the whole pruned
architecture, rather than only focusing on local parts of the
network. Fortunately, compared with state-of-the-art methods
[9], [18], [22], our method under FLOPs constraint can also
achieve superior or comparable performance, suggesting that
it is a good auxiliary constraint.
Table X shows the top-1 accuracy of the pruned models
before and after the ﬁne-tuning process under FLOPs con-
straint. It is observed that when the pruning process is just
5 10 15 20 25 30
Convolutional Layer Index0100200300400500600Number of Filters
64 64 64128Stride=2
128 128 128256Stride=2
256 256 256 256 256512Stride=2
512 512
W-Gates tends to keep more filters when stride is 2.3.7G(1X)
2.8G
2.3G
2.0GFig. 6. Visualization of the pruning results on ResNet34. For such an
architecture consisting of basic building blocks, our W-Gates method learns
to keep more ﬁlters where there is a downsampling operation.
0 10 20 30 40 50
Convolutional Layer Index0100200300400500600Number of Filters
646464128128128128256256256256256256512512512
W-Gates tends to keep more filters with 1x1 size.4.1G(1X)
2.2G
2.1G
2.0G
1.9G
Fig. 7. Visualization of the pruning results on ResNet50. We prune ﬁlters of
the ﬁrst two layers in each bottleneck building block. For such an architecture
consisting of bottleneck building blocks, our W-Gates method learns that the
33convolutional layers have larger information redundancy than 11
convolutional layers, and contribute more to the hardware latency.
completed, W-Gates has achieved a good performance on
VGG16 and ResNet56. After the ﬁne-tuning process, the Top-
1 accuracy of VGG16 and ResNet56 can be further regained
by 0.67% and 0.40%, respectively. These results indicate that
the performance of W-Gates mainly comes from the pruning
stage. The reason is that, with the proposed efﬁciency-aware
loss function, W-Gates can carry out ﬁlter pruning tasks
with consideration of the overall network structure, which is
beneﬁcial for obtain a better trade-off between accuracy and
efﬁciency and ﬁnding optimal solutions for network pruning.
E. Visualization Analysis
In the ﬁlter pruning process, we are curious about what
W-Gates have learned from the network and what kind of
architectures have a better trade-off in accuracy-latency. In vi-
sualizing the pruned architectures of ResNet34 and ResNet50,
we ﬁnd that the W-Gates did learn something interesting.

--- PAGE 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12
The visualization of ResNet34 pruned results under latency
constraint on ImageNet is shown in Fig. 6. It can be observed
that for the architecture ResNet34 consisting of basic building
blocks, W-Gates trends not to prune the layer with the down-
sampling operation, although a large ratio of ﬁlters in the other
layers have been pruned. This is similar to the results in [13]
on MobileNet, but is more extreme in our experiments on
ResNet34. It is possibly due to that the network needs more
ﬁlters to retain the information to compensate for the loss of
information caused by feature map downsampling.
However, for ResNet50 architecture consisting of bottleneck
building blocks, the phenomenon is different. As can be seen in
Fig. 7, the W-Gates trends not to prune the 11convolutional
layers and prune the 33convolutional layers with a large
ratio. It shows that W-Gates learn automatically that the 33
convolutional layers have larger information redundancy than
11convolutional layers, and contribute more to the hardware
latency.
V. C ONCLUSIONS
In this paper, we propose a novel ﬁlter pruning method to
address the problems on pruning indicator, pruning ratio, and
platform constraint at the same time. We ﬁrst propose weight-
dependent gates to learn the information from convolutional
weights and generate novel weights-dependent ﬁlter gates.
Then, we construct a switchable Efﬁciency Module to predict
the latency or FLOPs of the candidate pruned networks and
provide efﬁciency constraint for the weight-dependent gates
during the pruning process. The entire framework is fully
differentiable with respect to ﬁlter choices and pruning ratios,
which can be optimized by a gradient-based method to achieve
better pruning results.
However, the proposed method also has some limitations.
On the one hand, for the ResNet architecture with shortcut
connections, we did not prune the output channels of the last
layer in each building block, which left some compression
space unused. In our future study, we will try to ﬁnd a better
way to solve this issue. On the other hand, we only focused on
classiﬁcation tasks in this paper and did not apply the proposed
method to other types of applications. In future work, we
will try to transfer the proposed method to other application
scenarios, such as object detection, semantic segmentation, etc.
REFERENCES
[1] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2016, pp. 770–778.
[2] K. Wang, D. Zhang, Y . Li, R. Zhang, and L. Lin, “Cost-effective active
learning for deep image classiﬁcation,” IEEE Transactions on Circuits
and Systems for Video Technology , vol. 27, no. 12, pp. 2591–2600, 2016.
[3] J. Zhuang, Z. Wang, and B. Wang, “Video semantic segmentation with
distortion-aware feature correction,” IEEE Transactions on Circuits and
Systems for Video Technology , 2020.
[4] X. Wang, Z. Chen, J. Tang, B. Luo, Y . Wang, Y . Tian, and F. Wu,
“Dynamic attention guided multi-trajectory analysis for single object
tracking,” IEEE Transactions on Circuits and Systems for Video Tech-
nology , 2021.
[5] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
hierarchies for accurate object detection and semantic segmentation,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2014, pp. 580–587.[6] J. Guo, W. Zhang, W. Ouyang, and D. Xu, “Model compression
using progressive channel pruning,” IEEE Transactions on Circuits and
Systems for Video Technology , vol. 31, no. 3, pp. 1114–1124, 2020.
[7] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning ﬁlters
for efﬁcient convnets,” in Proceedings of International Conference on
Learning Representations , 2017.
[8] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
“Mobilenetv2: Inverted residuals and linear bottlenecks,” in The IEEE
Conference on Computer Vision and Pattern Recognition , June 2018.
[9] Y . He, X. Zhang, and J. Sun, “Channel pruning for accelerating
very deep neural networks,” in Proceedings of the IEEE international
conference on computer vision , 2017.
[10] Y . Li, S. Lin, B. Zhang, J. Liu, D. Doermann, Y . Wu, F. Huang, and
R. Ji, “Exploiting kernel sparsity and entropy for interpretable cnn
compression,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , 2019, pp. 2800–2809.
[11] Z. Huang and N. Wang, “Data-driven sparse structure selection for
deep neural networks,” in Proceedings of the European Conference on
Computer Vision , 2018, pp. 304–320.
[12] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, “Learning efﬁ-
cient convolutional networks through network slimming,” in Proceedings
of the IEEE International Conference on Computer Vision , 2017, pp.
2736–2744.
[13] Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, K.-T. Cheng, and J. Sun,
“Metapruning: Meta learning for automatic neural network channel
pruning,” in Proceedings of the IEEE international conference on
computer vision , 2019, pp. 3296–3305.
[14] J.-H. Luo and J. Wu, “Autopruner: An end-to-end trainable ﬁlter pruning
method for efﬁcient deep model inference,” Pattern Recognition , vol.
107, p. 107461, 2020.
[15] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.
[16] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, “Importance
estimation for neural network pruning,” in The IEEE Conference on
Computer Vision and Pattern Recognition , June 2019.
[17] Y . He, P. Liu, Z. Wang, Z. Hu, and Y . Yang, “Filter pruning via
geometric median for deep convolutional neural networks acceleration,”
inProceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , 2019, pp. 4340–4349.
[18] M. Lin, R. Ji, Y . Wang, Y . Zhang, B. Zhang, Y . Tian, and L. Shao,
“Hrank: Filter pruning using high-rank feature map,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2020, pp. 1529–1538.
[19] S. Guo, Y . Wang, Q. Li, and J. Yan, “Dmcp: Differentiable markov
channel pruning for neural networks,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2020, pp.
1539–1547.
[20] H. Peng, J. Wu, S. Chen, and J. Huang, “Collaborative channel pruning
for deep networks,” in International conference on machine learning .
PMLR, 2019, pp. 5113–5122.
[21] X. Ding, G. Ding, Y . Guo, and J. Han, “Centripetal sgd for pruning very
deep convolutional networks with complicated structure,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition ,
2019, pp. 4943–4953.
[22] C. Zhao, B. Ni, J. Zhang, Q. Zhao, W. Zhang, and Q. Tian, “Variational
convolutional neural network pruning,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2019, pp.
2780–2789.
[23] Y . Zhou, Y . Zhang, Y . Wang, and Q. Tian, “Accelerate cnn via recursive
bayesian pruning,” in Proceedings of the IEEE international conference
on computer vision , 2019, pp. 3306–3315.
[24] J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang, “Slimmable neural
networks,” in International Conference on Learning Representations ,
2019.
[25] Y . Li, W. Wu, Z. Liu, C. Zhang, X. Zhang, H. Yao, and B. Yin, “Weight-
dependent gates for differentiable neural network pruning,” in European
Conference on Computer Vision Workshops . Springer, 2020, pp. 23–37.
[26] K. Jia, D. Tao, S. Gao, and X. Xu, “Improving training of deep neural
networks via singular value bounding,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , vol. 2017,
2017, pp. 3994–4002.
[27] X. Yu, T. Liu, X. Wang, and D. Tao, “On compressing deep models
by low rank and sparse decomposition,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2017, pp.
7370–7379.

--- PAGE 13 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13
[28] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y . Bengio, “Bi-
narized neural networks,” in Advances in neural information processing
systems , 2016, pp. 4107–4115.
[29] M. Rastegari, V . Ordonez, J. Redmon, and A. Farhadi, “Xnor-net:
Imagenet classiﬁcation using binary convolutional neural networks,” in
European Conference on Computer Vision . Springer, 2016, pp. 525–
542.
[30] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
network,” arXiv preprint arXiv:1503.02531 , 2015.
[31] K. Zhang, C. Zhanga, S. Li, D. Zeng, and S. Ge, “Student network
learning via evolutionary knowledge distillation,” IEEE Transactions on
Circuits and Systems for Video Technology , vol. 32, no. 4, pp. 2251–
2263, 2021.
[32] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufﬂenet: An extremely efﬁ-
cient convolutional neural network for mobile devices,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition ,
2018, pp. 6848–6856.
[33] J. Guo, J. Liu, and D. Xu, “Jointpruning: Pruning networks along multi-
ple dimensions for efﬁcient point cloud processing,” IEEE Transactions
on Circuits and Systems for Video Technology , 2021.
[34] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y . Wei, and J. Sun, “Single
path one-shot neural architecture search with uniform sampling,” in
European Conference on Computer Vision . Springer, 2020, pp. 544–
560.
[35] M. Xu, M. Zhu, Y . Liu, F. X. Lin, and X. Liu, “Deepcache: Principled
cache for mobile deep vision,” in Proceedings of the 24th Annual
International Conference on Mobile Computing and Networking , 2018,
pp. 129–144.
[36] Y . Li, C. Zhang, S. Han, L. L. Zhang, B. Yin, Y . Liu, and M. Xu, “Boost-
ing mobile cnn inference through semantic memory,” in Proceedings of
the 29th ACM International Conference on Multimedia , 2021, pp. 2362–
2371.
[37] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural networks with pruning, trained quantization and huffman
coding,” in Proceedings of International Conference on Learning Rep-
resentations , 2016.
[38] H.-J. Kang, “Accelerator-aware pruning for convolutional neural net-
works,” IEEE Transactions on Circuits and Systems for Video Technol-
ogy, vol. 30, no. 7, pp. 2093–2103, 2019.
[39] Y . Li, L. Wang, S. Peng, A. Kumar, and B. Yin, “Using feature
entropy to guide ﬁlter pruning for efﬁcient convolutional networks,” in
International Conference on Artiﬁcial Neural Networks . Springer, 2019,
pp. 263–274.
[40] A. Kumar, A. M. Shaikh, Y . Li, H. Bilal, and B. Yin, “Pruning ﬁlters with
l1-norm and capped l1-norm for cnn compression,” Applied Intelligence ,
pp. 1–9, 2020.
[41] J.-H. Luo, H. Zhang, H.-Y . Zhou, C.-W. Xie, J. Wu, and W. Lin, “Thinet:
pruning cnn ﬁlters for a thinner net,” IEEE transactions on pattern
analysis and machine intelligence , vol. 41, no. 10, pp. 2525–2538, 2018.
[42] Y . He, G. Kang, X. Dong, Y . Fu, and Y . Yang, “Soft ﬁlter pruning for
accelerating deep convolutional neural networks,” in Proceedings of the
27th International Joint Conference on Artiﬁcial Intelligence , 2018, pp.
2234–2240.
[43] S. Lin, R. Ji, Y . Li, Y . Wu, F. Huang, and B. Zhang, “Accelerating convo-
lutional networks via global & dynamic ﬁlter pruning.” in Proceedings of
the 27th International Joint Conference on Artiﬁcial Intelligence , 2018,
pp. 2425–2432.
[44] Z. Wang, C. Li, and X. Wang, “Convolutional neural network pruning
with structural redundancy reduction,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2021, pp.
14 913–14 922.
[45] J.-H. Luo and J. Wu, “Neural network pruning with residual-connections
and limited-data,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , June 2020.
[46] Y . Guo, H. Yuan, J. Tan, Z. Wang, S. Yang, and J. Liu, “Gdp: Stabilized
neural network pruning via gates with differentiable polarization,” in
Proceedings of the IEEE/CVF International Conference on Computer
Vision , 2021, pp. 5239–5250.
[47] Y . Tang, Y . Wang, Y . Xu, Y . Deng, C. Xu, D. Tao, and C. Xu, “Manifold
regularized dynamic network pruning,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2021, pp.
5018–5028.
[48] X. Ding, T. Hao, J. Tan, J. Liu, J. Han, Y . Guo, and G. Ding, “Resrep:
Lossless cnn pruning via decoupling remembering and forgetting,” in
Proceedings of the IEEE/CVF International Conference on Computer
Vision , 2021, pp. 4510–4520.[49] G. Ding, S. Zhang, Z. Jia, J. Zhong, and J. Han, “Where to prune: Using
lstm to guide data-dependent soft pruning,” IEEE Transactions on Image
Processing , vol. 30, pp. 293–304, 2020.
[50] M. Gong, K.-y. Feng, X. Fei, A. Qin, H. Li, and Y . Wu, “An auto-
matically layer-wise searching strategy for channel pruning based on
task-driven sparsity optimization,” IEEE Transactions on Circuits and
Systems for Video Technology , 2022.
[51] D. Soudry, I. Hubara, and R. Meir, “Expectation backpropagation:
Parameter-free training of multilayer neural networks with continuous or
discrete weights,” in Advances in neural information processing systems ,
2014, pp. 963–971.
[52] B. Wu, X. Dai, P. Zhang, Y . Wang, F. Sun, Y . Wu, Y . Tian, P. Vajda,
Y . Jia, and K. Keutzer, “Fbnet: Hardware-aware efﬁcient convnet design
via differentiable neural architecture search,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2019, pp.
10 734–10 742.
[53] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with
gumbel-softmax,” arXiv preprint arXiv:1611.01144 , 2016.
[54] Z. Liu, B. Wu, W. Luo, X. Yang, W. Liu, and K.-T. Cheng, “Bi-real net:
Enhancing the performance of 1-bit cnns with improved representational
capability and advanced training algorithm,” in Proceedings of the
European Conference on Computer Vision , 2018, pp. 722–737.
[55] Y . He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han, “Amc: Automl
for model compression and acceleration on mobile devices,” in The
European Conference on Computer Vision , 2018, pp. 784–800.
[56] T.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, M. Sandler, V . Sze,
and H. Adam, “Netadapt: Platform-aware neural network adaptation for
mobile applications,” in Proceedings of the European Conference on
Computer Vision , 2018, pp. 285–300.
[57] X. Dai, P. Zhang, B. Wu, H. Yin, F. Sun, Y . Wang, M. Dukhan,
Y . Hu, Y . Wu, Y . Jia et al. , “Chamnet: Towards efﬁcient network design
through platform-aware model adaptation,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2019, pp.
11 398–11 407.
[58] H. Yang, Y . Zhu, and J. Liu, “Ecc: Platform-independent energy-
constrained deep neural network compression via a bilinear regression
model,” in Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , 2019, pp. 11 206–11 215.
[59] S. Gao, F. Huang, W. Cai, and H. Huang, “Network pruning via perfor-
mance maximization,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2021, pp. 9270–9280.
[60] H. Cai, L. Zhu, and S. Han, “Proxylessnas: Direct neural architecture
search on target task and hardware,” in International Conference on
Learning Representations , 2019.
[61] Y . Yang, J. Wu, H. Li, X. Li, T. Shen, and Z. Lin, “Dynamical system
inspired adaptive time stepping controller for residual network families,”
inProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 34,
no. 04, 2020, pp. 6648–6655.
[62] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efﬁcient convo-
lutional neural networks for mobile vision applications,” arXiv preprint
arXiv:1704.04861 , 2017.
[63] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in IEEE conference on
computer vision and pattern recognition , 2009, pp. 248–255.
[64] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from
tiny images,” Citeseer, Tech. Rep., 2009.
[65] M. Lin, L. Cao, Y . Zhang, L. Shao, C.-W. Lin, and R. Ji, “Pruning
networks with cross-layer ranking & k-reciprocal nearest ﬁlters,” IEEE
Transactions on Neural Networks and Learning Systems , 2022.
[66] S. Yu, Z. Yao, A. Gholami, Z. Dong, S. Kim, M. W. Mahoney, and
K. Keutzer, “Hessian-aware pruning and optimal neural implant,” in
Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision (WACV) , 2022, pp. 3880–3891.
[67] M. Lin, L. Cao, S. Li, Q. Ye, Y . Tian, J. Liu, Q. Tian, and R. Ji, “Filter
sketch for network pruning,” IEEE Transactions on Neural Networks
and Learning Systems , 2021.
[68] N. Gkalelis and V . Mezaris, “Fractional step discriminant pruning: A
ﬁlter pruning framework for deep convolutional neural networks,” in
2020 IEEE International Conference on Multimedia & Expo Workshops
(ICMEW) . IEEE, 2020, pp. 1–6.
[69] Y . Cai, Z. Yin, K. Guo, and X. Xu, “Pruning the unimportant or
redundant ﬁlters? synergy makes better,” in 2021 International Joint
Conference on Neural Networks (IJCNN) . IEEE, 2021, pp. 1–8.
[70] Y . Sui, M. Yin, Y . Xie, H. Phan, S. Aliari Zonouz, and B. Yuan, “Chip:
Channel independence-based pruning for compact neural networks,”
Advances in Neural Information Processing Systems , vol. 34, 2021.
