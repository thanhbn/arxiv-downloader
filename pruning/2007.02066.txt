# 2007.02066.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2007.02066.pdf
# File size: 937022 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1
Weight-dependent Gates for Network Pruning
Yun Li, Zechun Liu, Weiqun Wu, Haotian Yao, Xiangyu Zhang, Chi Zhang, and Baoqun Yin
Abstract â€”In this paper, a simple yet effective network pruning
framework is proposed to simultaneously address the problems of
pruning indicator, pruning ratio, and efï¬ciency constraint. This
paper argues that the pruning decision should depend on the
convolutional weights, and thus proposes novel weight-dependent
gates (W-Gates) to learn the information from ï¬lter weights and
obtain binary gates to prune or keep the ï¬lters automatically.
To prune the network under efï¬ciency constraints, a switchable
Efï¬ciency Module is constructed to predict the hardware latency
or FLOPs of candidate pruned networks. Combined with the
proposed Efï¬ciency Module, W-Gates can perform ï¬lter pruning
in an efï¬ciency-aware manner and achieve a compact network
with a better accuracy-efï¬ciency trade-off. We have demonstrated
the effectiveness of the proposed method on ResNet34, ResNet50,
and MobileNet V2, respectively achieving up to 1.33/1.28/1.1
higher Top-1 accuracy with lower hardware latency on ImageNet.
Compared with state-of-the-art methods, W-Gates also achieves
superior performance.
Index Terms â€”Weight-dependent gates, switchable Efï¬ciency
Module, Accuracy-efï¬ciency trade-off, Network pruning
I. I NTRODUCTION
IN recent years, convolutional neural networks (CNNs)
have achieved state-of-the-art performance in many tasks,
including but not limited to image classiï¬cation [1], [2],
semantic segmentation [3], and object detection [4], [5], etc.
Despite their great success, billions of ï¬‚oat-point-operations
(FLOPs) cost and long inference latency are still prohibitive
for CNNs to deploy on many resource-constraint hardware.
As a result, a signiï¬cant amount of effort has been invested in
CNNs compression and acceleration, in which ï¬lter pruning
[6], [7] is seen as an intuitive and effective network compres-
sion method.
However, ï¬lter pruning is non-trivial and faces three major
challenges. 1) Pruning indicator: CNNs are usually seen as a
black box, and individual ï¬lters may play different roles within
and across different layers in the network. Thus, it is difï¬cult
to manually design indicators that can fully quantify the
importance of their internal convolutional ï¬lters and feature
maps. 2) Pruning ratio: How many ï¬lters should be pruned
in each layer? The redundancy varies from different layers,
making it a challenging problem to set appropriate pruning
ratios for different layers. 3) Efï¬ciency constraint: Most pre-
vious works only adopt hardware-agnostic metrics such as
Yun Li and Baoqun Yin are with University of Science and Technology of
China, Hefei, China (e-mail: yli001@mail.ustc.edu.cn, bqyin@ustc.edu.cn).
(Corresponding author: Baoqun Yin.)
Zechun Liu is with Hong Kong University of Science and Technology,
Hong Kong, China (e-mail: zliubq@connect.ust.hk).
Weiqun Wu is with Chongqing University, Chongqing, China (e-mail:
wuwq@cqu.edu.cn)
Haotian Yao, Xiangyu Zhang, and Chi Zhang are with Megvii Inc.,
Beijing, China (email: yaohaotian@megvii.com, zhangxiangyu@megvii.com,
zhangchi@megvii.com).parameters or FLOPs to evaluate the efï¬ciency of a CNN.
But the inconsistency between hardware-agnostic metrics and
actual efï¬ciency [8] lead to an increasing industrial demand
on directly optimizing the hardware latency.
Previous works have tried to address these issues from dif-
ferent perspectives. Conventional ï¬lter pruning works mainly
rely on manual-designed indicators [7], [9], [10] or data-driven
indicators [11], [12]. However, manual-designed indicators
usually involve human participation, and data-driven pruning
indicators may be affected by the feature maps. Besides, in
previous works, the pruning ratio of each layer or a global
pruning threshold is usually human-speciï¬ed, making the
results prone to be trapped in sub-optimal solutions [13].
In this paper, we propose a simple yet effective ï¬lter pruning
method, which can automatically obtain the pruning decision
and the pruning ratio of each layer while considering the
overall efï¬ciency of the network, as shown in Fig. 1.
To address the issue of the pruning indicator, we propose
weight-dependent gates (W-Gates). Instead of designing a
manual indicator or data-driven scale factors, we argue that
the pruning decision should depend on the ï¬lter itself, in
other words, it should be a learnable function of ï¬lter weights.
Thus, we propose a type of novel weight-dependent gates to
directly learn a mapping from ï¬lter weights to ï¬lter gates. W-
Gates takes the weights of a convolutional layer as input to
learn information from them, and outputs binary gates to open
or close the corresponding ï¬lters automatically (0: close, 1:
open) during the training process. Each W-Gates consists of a
fully connected layer and binary activation, which is simple to
implement and train given the pre-trained CNN model. More
importantly, the output ï¬lter gates here are weights-dependent,
which is the essential difference between the proposed W-
Gates and conventional data-driven methods [11], [12], [14].
To address the issue of the efï¬ciency constraint, we propose
a switchable Efï¬ciency Module, which provides two options
to cope with different scenarios, a Latency Prediction Net-
work (LPNet) and FLOPs Estimation. To prune the network
under hardware constraint, we propose LPNet to predict
the hardware latency of candidate pruned networks. LPNet
takes the network encoding vector as input and outputs a
predicted latency. After trained ofï¬‚ine based on the latency
data collected from the target hardware, the LPNet is able to
provide latency guidance for network pruning. Considering
that hardware information is not always available, we add
the FLOPs Estimation as an auxiliary unit in the Efï¬ciency
Module. For scenarios without hardware information, we can
switch the Efï¬ciency Module to the FLOPs Estimation and
provide FLOPs guidance in a gradient manner for the pruning.
With the proposed W-Gates and the switchable Efï¬cient
Module, we construct a differential pruning framework to
perform ï¬lter pruning. We ï¬nd that the output of W-GatesarXiv:2007.02066v4  [cs.CV]  14 May 2022

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2
ReshapeÃ—Filter GatesFilter GatesWeight-dependent Gates
Weight-dependent GatesWeight-dependent GatesBinary ActivationInputâ€¦ð‘ªð’Šð’ð‘ªð’ð’–ð’•â€¦Weights Tensor ð‘¾âˆ—FC layerð’‡ð‘¾âˆ—ð’”ðŸð’”ðŸâ€¦ð’”ð‘ªð’ð’–ð’•Scoresâ€¦ð‘ªð’Šð’â€¦
[3   20   16 â€¦64  ]Network Encodingâ€¦ð‘ªð’Šð’â€¦Accuracy Lossð‘®ð’‚ð’•ð’†ð’”
Sumi-thconv-layer(i+1)-thconv-layer(i+2)-thconv-layer
Layer EncodingLayer EncodingLayer Encodingð‘®ð’‚ð’•ð’†ð’”ð‘®ð’‚ð’•ð’†ð’”ð‘ªð’ð’–ð’•ð‘ªð’ð’–ð’•ð‘ªð’ð’–ð’•Ã—Ã—
Efficiency LossAccuracy GradientEfficiency LossLatency Prediction NetworkFLOPs EstimationEfficiency Module
[10011]
Fig. 1. Pruning framework overview. There are two main parts in our framework, Weight-dependent Gates (W-Gates) and a switchable Efï¬ciency Module.
The W-Gates learns the information from the ï¬lter weights of each layer and generates binary ï¬lter gates to open or close corresponding ï¬lters automatically
(0: close, 1: open). The ï¬lter gates of each layer are then summed to obtain a layer encoding and all the layer encodings constitute a network encoding. Next,
the network encoding of the candidate pruned network is fed into Efï¬ciency Module to get the predicted latency or FLOPs of the candidate pruned network.
During the pruning process, accuracy loss and efï¬ciency loss compete against each other to obtain a compact model with a better accuracy-efï¬ciency trade-off.
can be summed as a layer encoding to determine the pruning
ratio of each layer, and all the layer encoding can constitute the
network encoding vector of the candidate pruned architecture.
Therefore, the network encoding is fed to the Efï¬ciency
Module to obtain the latency or FLOPs guidance. Then, Given
the pre-trained CNN model and the LPNet, we can carry out
network pruning in an end-to-end manner. To perform ï¬lter
pruning during training, we deï¬ne an efï¬ciency-aware loss
function that consists of the accuracy loss and the efï¬ciency
loss. The whole pruning framework is fully differentiable,
allowing us to simultaneously impose the gradients of accuracy
loss and efï¬ciency loss to optimize the W-Gates of each
layer. Retaining more ï¬lters will keep more information in
the network, which is beneï¬cial to the ï¬nal accuracy. On the
contrary, pruning more ï¬lters will improve the efï¬ciency of the
network inference. To this end, during training, the accuracy
loss will pull the binary gates to more â€˜1â€™s (retaining more
ï¬lters), while the efï¬ciency loss pulls the binary gates to more
â€˜0â€™s (pruning more ï¬lters). They compete against each other
and ï¬nally obtain a compact network with the better accuracy-
efï¬ciency trade-off.
It is worth mentioning that the pruning decision and the
pruning ratio of each layer can all be obtained automatically,
which involves little human partipication. As the input of
the Efï¬ciency Module is the network encoding, the pruning
framework can carry out ï¬lter pruning with consideration of
the overall network architecture, which is beneï¬cial for ï¬nding
optimal pruning ratios for different layers. The proposed W-
Gates can be applied in various CNN-based computer vision
tasks to compress and accelerate their backbones or the entire
networks.
We evaluate our method on ResNets [1], MobileNet V2[8], and VGG [15]. Comparing with uniform baselines, we
consistently deliver much higher accuracy and lower latency.
With lower latency, we achieve 1.09%-1.33% higher accuracy
than ResNet34, 0.67%-1.28% higher accuracy than ResNet50,
and 1.1% higher accuracy than MobileNet V2 on ImageNet
dataset. Compared to other state-of-the-art pruning methods
[16]â€“[24], our method also produces superior results.
The main contributions of our paper are three-fold:
We propose a kind of novel weight-dependent gates (W-
Gates) to directly learn a mapping from ï¬lter weights to
ï¬lter gates. W-Gates takes the convolutional weights as
input and generates binary ï¬lter gates to prune or keep
the ï¬lters automatically during training.
A switchable Efï¬ciency Module is constructed to predict
the hardware latency or FLOPs of candidate pruned
networks. Efï¬ciency Module is fully differentiable with
respect to W-Gates, allowing us to impose efï¬ciency
constraints based on gradients and obtain a compact
network with a better accuracy-efï¬ciency trade-off.
The proposed method simultaneously addresses three
challenges in ï¬lter pruning (pruning indicator, pruning
ratio, efï¬ciency constraint). Compared with state-of-the-
art ï¬lter pruning methods, the proposed method achieves
superior performance.
This paper extends the preliminary workshop paper [25] in
the following aspects. 1) We propose a switchable Efï¬ciency
Module to cope with various scenarios, so that the pruning
framework can not only perform ï¬lter pruning under the
latency constraint, but also under FLOPs constraints. 2) We
generalize our idea of weight-dependent gates and latency
predict network, enabling the application of our W-Gates from
a single ResNet architecture to more types of CNN archi-

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3
tectures, such as MobileNetv2, VGGNet, etc. 3) We conduct
four important ablation studies to illustrate the rationality of
the W-Gates design in our method. We show that W-Gates
can indeed learn information from ï¬lter weights and make a
good ï¬lter selection automatically, which helps the network
training and obtains higher accuracy. 4) We investigate the
effects of the key coefï¬cient in the proposed loss function
on the ï¬nal accuracy-latency trade-off. 5) We generalize our
idea from a single dataset to more dataset scenarios. 6) We
conduct visualization analysis on the pruned architectures. We
show that W-Gates trends not to prune the layers with the
downsample operations, and prunes more 33convolutional
layers than 11layers.
The rest of this paper is structured as follows. In Section
II, we brieï¬‚y review the related work. Section III details our
proposed method. Experimental settings, ablation study, and
results analysis are presented in Section IV followed by the
conclusion in Section V .
II. R ELATED WORK
A signiï¬cant amount of effort has been devoted to deep
model compression, such as matrix decomposition [26], [27],
quantization [28], [29], knowledge distillation [30], [31], com-
pact architecture learning [8], [32], neural network search [33],
[34], inference acceleration [35], [36], and network pruning
[37]â€“[39]. Pruning is an intuitive and effective network com-
pression method. Prior works devote to weights pruning. [37]
proposes to prune unimportant connections whose absolute
weights are smaller than a given threshold, which achieves
good performance on parameter compression. However, it is
not implementation friendly and can not obtain faster inference
without dedicated sparse matrix operation hardware. To tackle
this problem, some ï¬lter pruning methods [7], [40], [41] have
been explored recently. These methods prune or sparse parts
of the network structures (e.g., neurons, channels) instead of
individual weights, so they usually require less specialized
libraries to achieve inference speedup. In this paper, our work
also falls into ï¬lter pruning. Next, we mainly discuss the works
which are most related to our work.
A. Manual-designed indicator
Many excellent ï¬lter pruning methods based on manual-
designed indicators have been proposed to compress large
CNNs. [7] present an acceleration method for CNNs, where we
prune ï¬lters from CNNs that are identiï¬ed as having a small
effect on the output accuracy. [9] proposes an iterative two-
step algorithm to effectively prune each layer, by a LASSO
regression based channel selection and least square reconstruc-
tion. [41] formally establishes ï¬lter pruning as an optimization
problem, and reveals that we need to prune ï¬lters based on
statistics information computed from its next layer, not the
current layer. The above two methods all prune ï¬lters based on
feature maps. Different from the above data-driven methods,
[10] proposes a feature-agnostic method, which prunes ï¬lter
based on a kernel sparsity and entropy indicator. [42] proposes
a soft ï¬lter pruning method, which prunes ï¬lters based on
L2norm and updates the pruned model when training themodel after pruning. [43] introduces a binary global mask
after each ï¬lter to dynamically and iteratively prune and tune
the network, with the mechanism to recall ï¬lters that are
mistakenly pruned in the previous iterations. [17] proposes a
Geometric Median based ï¬lter pruning method, which prunes
ï¬lters with relatively less contribution and chooses the ï¬lters
with the most replaceable contribution. These methods above
need to manually set a global pruning ratio [10], [43] or
layer pruning ratios [7], [9], [17], [41], which is difï¬cult to
fully consider the redundancy of different layers. To solve
this problem, [44] establishes a graph for each convolutional
layer of a CNN and uses two quantities associated with the
graph to obtain the redundancy and the pruning ratio of each
layer. However, a common problem of these manual-designed
pruning indicators is that they usually involve human partici-
pation, which makes them prone to be trapped in sub-optimal
solutions. To tackle this issue, our proposed method introduces
Weight-dependent Gates to learn the pruning decision from
ï¬lter weights automatically and cooperate with Efï¬ciency
Module to determine the pruning ratio of each layer, which
involves little human participation.
B. Data-driven Pruning Methods
There are some other ï¬lter pruning methods that propose
data-driven indicators. [12] imposes L1 regularization on the
scaling factors in batch normalization (BN) layers to identify
insigniï¬cant ï¬lters. Similarly, [11] introduces scaling factors
to scale the outputs of speciï¬c structures, such as neurons,
groups, or residual blocks, and then do network pruning in an
end-to-end manner. [45] proposes to prune both channels in-
side and outside the residual connections via a KL-divergence
based criterion to compensate for the weakness of limited data.
[46] proposes gates with differentiable polarization to control
the on-and-off of each channel or whole layer block, which is
essentially a kind of data-driven gates. ManiDP [47] explores
a data-driven paradigm for dynamic pruning, which explores
the manifold information of all samples in the training process
and derives the corresponding sub-networks to preserve the
relationship between different instances. ResRep [48] proposes
to re-parameterize a CNN into the remembering parts and
forgetting parts, where the former learn to maintain the per-
formance and the latter learn to prune. [49] proposes a data-
dependent soft pruning method, dubbed Squeeze-Excitation-
Pruning (SEP), which does not physically prune any ï¬lters
but selects different ï¬lters for different inputs. [50] proposes
a data-driven pruning indicator, in which the task-irrelevant
channels are removed in a task-driven manner. AutoPruner
[14] proposed a data-driven channel selection layer, which
takes the feature map of the previous layer as input and
generates a binary index code for pruning. AutoPrunner [14]
are most similar to our W-Gates. The major difference is
that our W-Gates is weight-dependent, which directly learns
information from ï¬lter weights instead of feature maps. The
second difference is that AutoPrunner adopts a scaled sigmoid
function as soft binarization to generate an approximate binary
vector, while we adopt hard binarization (a variant of sign
function) to obtain true binary gates during training.

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4
C. Quantization and Binary Activation.
Quantization [29] proposes to quantize the real value
weights into binary/ternary weights to yield a large amount of
model size saving. [51] proved that good performance could
be achieved even if all neurons and weights are binarized to
1. [28] proposes to use the derivative of the clip function to
approximate the derivative of the sign function in the binarized
networks. [52] relaxes the discrete mask variables to be con-
tinuous random variables computed by the Gumbel Softmax
[53] to make them differentiable. [54] uses an identity shortcut
to add the real-valued outputs to the next most adjacent real-
valued outputs, and then adopt a tight approximation to the
derivative of the non-differentiable sign function with respect
to real-valued activation, which inspired our binary activation
and gradient estimation in W-Gates.
D. Resource-Constrained Compression
Recently, real hardware performance has attracted more
attention compared to FLOPs. AutoML methods [55], [56]
propose to prune ï¬lters iteratively in different layers of a
CNN via reinforcement learning or an automatic feedback
loop, which take real-time latency as a constraint. Some recent
works [52], [57] introduce a look-up table to record the latency
of each operation or each layer and sum them to obtain the
latency of the whole network. This method is valid for many
CPUs and DSPs, but may not for parallel computing devices
such as GPUs. [58] treats the hardware platform as a black
box and creates an energy estimation model to predict the
latency of speciï¬c hardware as an optimization constraint. [19]
proposes DMCP to searching optimal sub-structure from un-
pruned networks under FLOPs constraint. [59] trains a stand-
alone neural network to predict sub-networksâ€™ performance
and then maximize the output of the network as a proxy of
accuracy to guide pruning. [13] proposes PruningNet, which
takes the network encoding vector as input and output weight
parameters of the pruned network. These above works inspired
our Efï¬ciency Module and the LPNet training. We train the
LPNet to predict the latency of the target hardware platform,
which takes the network encoding vector as input and outputs
the predicted latency.
III. P ROPOSED METHOD
In this section, we introduce the proposed method which
adopt an efï¬ciency-aware approach to prune the CNN archi-
tecture under multiple constraints. Following the work [13], we
formulate the network pruning as a constrained optimization
problem:
arg min
wc`(c;wc)
s:t: Inference (c)Const(1)
where`is the loss function speciï¬c to a given learning task,
andcis the network encoding vector, which is a set of the
pruned network channel width (c1;c2;;cl).Inference (c)
denotes the real latency or FLOPs of the pruned network,
which depends on the network channel width set c.wcmeans
Ã—inputâ€¦ð‘ªð’Šð’ð‘ªð’ð’–ð’•â€¦ð‘®ð’‚ð’•ð’†ð’”i-thconv-layer
Binary ActivationReshapeð’‡ð‘¾âˆ—Weight-dependent GatesWeights tensor ð‘¾âˆ—ð‘ªð’ð’–ð’•FC layerð’”ðŸð’”ðŸâ€¦ð’”ð‘ªð’ð’–ð’•output
Scores[10011]Fig. 2. The proposed Weight-dependent Gates. It introduces a fully connected
layer to learn the information from the reshaped ï¬lter weights Wand
generates a score for each ï¬lter. After binary activation, we obtain the ï¬lter
gates (0 or 1) to open or close the corresponding ï¬lters automatically (0:
close, 1: open). The gates are placed after the BN transform and ReLU.
the weights of the remained channels and Const is the given
latency or FLOPs constraint.
To solve this problem, we propose an efï¬ciency-aware net-
work pruning framework, in which the weight-dependent gates
is the key part. Firstly, we propose the Weight-dependent Gates
Module (W-Gates), which is adopted to learn the information
from ï¬lter weights and generate binary gates to determine
which ï¬lters to prune automatically. The W-Gates works
as adaptive pruning indicator. Then, a Efï¬ciency Module is
constructed to provide the latency or FLOPs constraint to the
pruning indicator and guide the pruning ratios of each layer.
As key parts of the efï¬cient module, a Latency Prediction
Network (LPNet) is trained ofï¬‚ine to predict the real latency
of a given architecture in speciï¬c hardware, and FLOPs
Estimation is set to predict FLOPs for candidate architectures.
The two modules above complement each other to generate
the pruning strategy and obtain the best CNN model under
efï¬ciency constraint.
A. Weight-dependent Gates
The convolutional layer has always been adopted as a black
box, and we could only judge from the output what it has done.
Conventional ï¬lter pruning methods mainly rely on hand-craft
indicators or optimization-based indicators. They share one
common motivation: they are essentially looking for a pruning
function that can map ï¬lter weights to ï¬lter gates. However,
their pruning function usually involves human participation.
We argue that the gates should depend on the ï¬lters them-
selves, in other words, it is a learnable function of ï¬lter
weights. Thus, instead of designing a manual indicator, we

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5
xy
xy
xy-1012
0 1ðœ†(ð‘¥)
0 1 0 1 0 1ðœŽ(ð‘¥)ðœ•ðœŽ(ð‘¥)
ðœ•ð‘¥ðœ•ðœ†(ð‘¥)
ðœ•ð‘¥
(a) (b)
xy
Fig. 3. (a) The proposed binary activation function and its derivative. (b) The designed differentiable piecewise polynomial function and its derivative, and
this derivative is used to approximate the derivative of binary activation function in gradients computation.
directly learn the pruning function from the ï¬lter weights,
which is a direct reï¬‚ection of the characteristics of ï¬lters.
To achieve the above goal, we propose the Weight-dependent
Gates (W-Gates). W-Gates takes the weights of a convolutional
layer as input to learn information, and output binary ï¬lter
gates as the pruning function to remove ï¬lters adaptively.
Filter Gates Learning. LetW2RClCl 1KlKldenotes
the weights of a convolutional layer, which can usually be
modeled as Clï¬lters and each ï¬lter Wi2RCi 1KlKl;i=
1;2;;Cl.Klis the ï¬lter size of the l-th layer. To ex-
tract the information in each ï¬lter, a fully-connected layer,
whose weights are denoted as_W2R(Cl 1KlKl)1, is
introduced here. Then, reshaped to two-dimensional tensor
W2RCl(Cl 1KlKl), the ï¬lter weights are input to the
fully-connected layer to generate the score of each ï¬lter:
sr=f(W) =W_W; (2)
wheresr=
sr
1;sr
2;:::;sr
Cl
denotes the score set of the ï¬lters
in this convolutional layer.
To suppress the expression of ï¬lters with lower scores and
obtain binary ï¬lter gates, we introduce the following activation
function:
(x) =Sign (x) + 1
2: (3)
The curve of Eq. (3) is shown in Fig. 3(a). We can see that
after processing by the activation function, the negative scores
will be converted to 0, and positive scores will be converted
to 1. Then, we get the binary ï¬lter gates of this layer:
gatesb=(sr) =(f(W)): (4)
Different from the path binarization [60] in neural architec-
ture search, in ï¬lter pruning tasks, the pruning decision should
depend on the ï¬lter weights, in other words, our proposed
binary ï¬lter gates are weights-dependent, as shown in Eq.
(4). Different from the time stepping controller [61] which
is weight-dependent, the weight-dependent gates are proposed
as pruning indicators.
Next, we sum the binary ï¬lter gates of each layer to obtain
the layer encoding, and all the layer encodings form the
network encoding vector c. The layer encoding here denotes
the number of ï¬lters kept, which can also determine the
pruning ratio of each layer.
Gradient Estimation. As can be seen from Fig. 3, the
derivative of function ()is an impulse function, whichcannot be used directly during the training process. Inspired by
the recent Quantized Model works [28], [54], speciï¬cally Bi-
Real Net [54], we introduce a differentiable approximation of
the non-differentiable function (x). The gradient estimation
process is as follows:
@L
@Xr=@L
@Xb@Xb
@Xr=@L
@Xb@(Xr)
@Xr@L
@Xb@(Xr)
@Xr;(5)
whereXrdenotes the real value output sr
i,Xbmeans the
binary output. (Xr)is the approximation function we de-
signed, which is a piecewise polynomial function:
(Xr) =8
>><
>>:0; if X r< 1
2
2Xr+ 2X2
r+1
2; if 1
2Xr<0
2Xr 2X2
r+1
2; if 0Xr<1
2
1; otherwise;(6)
and the gradient of above approximation function is:
@(Xr)
@Xr=8
<
:2 + 4Xr; if 1
2Xr<0
2 4Xr; if 0Xr<1
2
0; otherwise:(7)
As discussed above, we can adopt the binary activation
function Eq. (3) to obtain the binary ï¬lter gates in the
forward propagation, and then update the weights of the fully-
connected layer with an approximate gradient Eq. (7) in the
backward propagation.
B. Efï¬ciency Module
With the proposed W-Gates, we can carry out ï¬lter scor-
ing and selection based on the information in convolutional
weights. However, to determine which structure is more ef-
ï¬cient for inference, the hardware or FLOPs information is
also necessary. For the motivation above, in this section, we
introduce an Efï¬ciency Module, which contains a Latency
Prediction Network and a FLOPs Estimation unit to provide
efï¬ciency guidance for the proposed W-Gates. As shown in
Fig. 1, the Efï¬ciency Module is a switchable module, in which
the Latency Prediction Network is the main unit to select
hardware-friendly architectures and the FLOPs Estimation is
the substitute unit to provide FLOPs constraint.
Latency Prediction Network. Previous works on model
compression aim primarily to reduce the parameters and
calculations, but they does not always reï¬‚ect the actual latency
on hardware. Therefore, some recent NAS-based methods [13],
[52] pay more attention to adopt the hardware latency as a

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6
Predicted LatencyFCLatency Prediction NetworkNetwork EncodingHardwareDevicesDecodingReal  LatencyMSE LossFCFC[â€¦64  ]320[â€¦96  ]1816[â€¦22  ]48
Fig. 4. The ofï¬‚ine training process of LPNet. ReLU is placed after the ï¬rst two FC layers. LPNet takes network encoding vectors as the input data and the
measured hardware latency as the ground truth. Mean Squared Error (MSE) loss function is adopted here.
direct evaluation indicator than parameters and FLOPs. [52]
proposes to build a latency lookup table to estimate the overall
latency of a network, as they assume that the runtime of
each operator is independent of other operators, which works
well on many mobile serial devices, such as CPUs and DSPs.
Previous works [13], [52] have demonstrated the effectiveness
of this method. However, the latency generated by the lookup
table is not differentiable with respect to the ï¬lter selection
and the pruning ratio of each layer.
To address the above problem, we construct LPNet to
predict the real latency of the whole network or building
blocks. The proposed LPNet is fully differentiable with respect
to ï¬lter gates and the pruning ratio of each layer. As shown
in Fig. 4, the LPNet consists of three fully-connected layers,
which takes a network encoding vector c= (c1;c2;:::;c L)as
input and output the latency for speciï¬ed hardware platform:
lat(c) = LPNet ( c1;c2;:::;c L); (8)
where
cl=ClX
i=1gatesb
i (9)
in the pruning framework.
To pretrain the LPNet, we sample network encoding vectors
cfrom the search space and decode the corresponding network
to test their real latency on speciï¬c hardware platforms. As
shown in Fig. 4, during the training process, the network
encoding vector is adopted as input and the latency on speciï¬c
hardware is used as the label. As there is no need to train the
decoding network, it takes only a few milliseconds to get a
latency label and the training of LPNet is also very efï¬cient.
For deeper building block architecture, such as ResNet50,
the network encoding sampling space is huge. We choose to
predict the latency of building blocks and then sum them up
to get the predicted latency of the overall network, and this
will greatly reduce the encoding sampling space. Besides, the
LPNet of building blocks can also be reused across models
of different depths and different tasks on the same type of
hardware.
As a result, training such a LPNet makes the latency
constraint differentiable with respect to the network encoding
and binary ï¬lter gates shown in Fig. 1. Thus we can use
gradient-based optimization to adjust the ï¬lter pruning ratioof each convolutional layer and obtain the best pruning ratio
automatically.
FLOPs Estimation. Although the hardware latency is the
most direct reï¬‚ection of inference efï¬ciency for the pruned
models, it is not always available. For scenarios with unknown
hardware information, we add the FLOPs Estimation in the
Efï¬cient Module as an alternative unit to optimize FLOPs by
gradient descent in the backward propagation.
Letcl 1denotes the number of input channels of layer l,
that is, the output channels of layer l 1.cldenotes the number
of the output channels of layer l. Inspired by [19], [62], the
FLOPs of convolutional layer lin the pruned network can bu
formulated as:
Fl=Ml
hMl
wKl
hKl
wcl 1cl; (10)
in which,Ml
handMl
ware the height and the width of the
input feature map in layer l, respectively. Kl
handKl
wdenote
the height and the width of the ï¬lter size, respectively.
Among all layer types, convolutional layers are the primary
performance hotspots [35]. Therefore, we focus on optimizing
the FLOPs of convolutional layers when carrying out pruning.
The FLOPs of a CNN with Lconvolutional layers can be
estimated as follows:
Flops (c) =LX
l=1Fi=LX
l=1Ml
hMl
wKl
hKl
wcl 1cl;(11)
wherec0= 3 (the number of channels of the input image)
andcl=PCl
i=1gatesb
iin the pruning framework.
The FLOPs Estimation above is based on ï¬lter gates and
training-free, which can be adopted to optimize the pruning
process by gradient descent in the scenario with unknown
hardware information.
C. Efï¬ciency-aware Filter Pruning
If hardware information is available, the proposed method
consists of three main stages. First, training the LPNet ofï¬‚ine,
as described in Section III-B. With the pretrained LPNet, we
can obtain the latency by inputting the encoding vector of a
candidate pruned network. Second, pruning the network under
latency constraint. We add W-Gates and the Efï¬ciency Module
(the LPNet) to a pretrained network to do ï¬lter pruning, in
which the weights of LPNet are ï¬xed. As shown in Fig. 1, W-
Gates learns the information from convolutional weights and

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7
generates binary ï¬lter gates to determine which ï¬lters to prune.
Next, LPNet takes the network encoding of candidate pruned
net as input and output a predicted latency to optimize the
pruning ratio and ï¬lter gates of each layer. Then, the accuracy
loss and the efï¬ciency loss compete against each other during
training and ï¬nally obtain a compact network with the best
accuracy while meeting the latency constraint. Third, ï¬ne-
tuning the network. After getting the pruned network, a ï¬ne-
tuning process with only a few epochs follows to regain
accuracy and obtain a better performance, which is less time-
consuming than training from scratch.
Furthermore, to make a better accuracy-efï¬ciency trade-off,
we deï¬ne the following efï¬ciency-aware loss function:
`(c;wc) =Acc(c;wc) +log (1 + Eff(c)); (12)
where Acc (c;wc)denotes the accuracy loss of an architecture
with a network encoding cand parameters wc. Eff (c)is
the latency prediction lat(c)or FLOPs estimation Flops (c)
of the pruned architecture with network encoding vector c.
The coefï¬cient can modulate the magnitude of the latency
term. Such a loss function can carry out ï¬lter pruning tasks
with consideration of the overall network structure, which is
beneï¬cial for ï¬nding optimal solutions for network pruning.
In addition, this function is differentiable with respect to layer-
wise ï¬lter choices cand the number of ï¬lters, which allows us
to use a gradient-based method to optimize them and obtain
a better trade-off between accuracy and efï¬ciency.
IV. E XPERIMENTS
In this section, we demonstrate the effectiveness of our
method. First, we give a detailed description of our experiment
settings. Next, we carry out four ablation studies on the Ima-
geNet dataset to illustrate the effect of the key part W-Gates in
our method. Then, we prune ResNet34, ResNet50, MobileNet
V2, and VGG16 under latency constraint, and compare our
method with several state-of-the-art ï¬lter pruning methods.
Afterward, we prune VGG16 and ResNet56 under FLOPs
constraint. Finally, we visualize the pruned architectures to
explore what our method has learned from the network and
what kind of architectures have a better trade-off between
accuracy and efï¬ciency.
A. Experiment Settings
We carry out experiments on the ImageNet ILSVRC 2012
dataset [63] and Cifar-10 dataset [64]. ImageNet contains 1.28
million training images and 50,000 validation images, which
are categorized into 1000 classes. The resolution of the input
images is set to 224224. Cifar-10 consisting of 50,000
training images, 10,000 test images, which are categorized
into 10 classes. All images of Cifar-10 are cropped randomly
into3232with four paddings and horizontal ï¬‚ip is also
applied. All the experiments are implemented with PyTorch
framework and networks are trained using stochastic gradient
descent (SGD) with momentum set to 0.9. For ResNet34 and
ResNet50, we adopt the same training scheme in [1].
To train the LPNet ofï¬‚ine, we sample network encoding
vectorscfrom the search space and decode the correspondingnetwork to test their real latency on one NVIDIA RTX 2080 Ti
GPU as latency labels. For deeper building block architecture,
such as ResNet50, the network encoding sampling space is
huge. We choose to predict the latency of building blocks and
then sum them up to get the predicted latency of the overall
network, and this will greatly reduce the encoding sampling
space. For the bottleneck building block of ResNet, we collect
170000 (c, latency) pairs to train the LPNet of bottleneck
building block and 5000 (c, latency) pairs to train the LPNet of
basic building block. We randomly choose 80% of the dataset
as training data and leave the rest as test data. We use Adam
to train the LPNets and ï¬nd the average test errors can quickly
converge to lower than 2%.
For the pruning and the ï¬ne-tuning processes, all networks
are trained using stochastic gradient descent (SGD) with
momentum set to 0.9. On ImageNet dataset, we respectively
train the ResNets and MobileNet V2 for 120 epochs and
240 epochs as baselines. For ResNet34/50, we respectively
set 40 and 80 epochs for the pruning and the ï¬ne-tuning
processes. The initial learning rates of the two processes above
are respectively set to 10 3and10 2. For MobileNet V2,
we respectively set 120 and 80 epochs for the pruning and
the ï¬ne-tuning processes. The initial learning rates of the two
processes above are set to 10 3and10 1, respectively. On
Cifar-10 dataset, for all the models, we set 200 epochs for
the pruning process and the ï¬ne-tuning process. The initial
learning rates are set to 10 3and10 2, respectively. On the
two datasets, during the pruning and the ï¬ne-tuning processes,
the leraning rates are all divided by 10 at 50% and 75% of
the total number of epochs.
B. Ablation Study on ImageNet
The performance of our method is mainly attributed to
the proposed Weight-dependent Gates (W-Gates). To validate
the effectiveness of W-Gates, we choose the widely used
architecture ResNet50 and conduct a series of ablation studies
on ImageNet dataset. In the following subsections, we ï¬rst
explore the impact of our proposed W-Gates in the training
process. Then, the impact of information learned from ï¬lter
weights is studied. After that, we illustrate the impact of
gate activation function by comparing binary activation with
scaled sigmoid activation. Finally, we compare our W-Gates-
based pruning method with several state-of-the-art gate-based
methods [12], [41], [42].
1) Impact of W-Gates in the Training Process: In this sec-
tion, to demonstrate the impact of W-Gates on ï¬lter selection,
we add them to a pretrained ResNet50 to learn the weights of
each convolutional layer and do ï¬lter selection during training.
Then, we continue training and test the ï¬nal accuracy.
Two sets of experiments are set up to test the effect of W-
Gates in different periods of the model training process. For
one of them, we ï¬rst train the network for 1/3 of total epochs
as a warm-up period and then add W-Gates to the network
to continue training. For the other experiment, we add W-
Gates to a pretrained model and test if it could improve the
training performance. As can be seen in Table I, with the same
number of iterations, adding W-Gates after a warm-up period

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8
TABLE I
THIS TABLE COMPARES THE ACCURACY ON IMAGE NET ABOUT
RESNET50. â€œW-G ATES (WARM -UP)â€DENOTES ADDING W-G ATES TO THE
RESNET50AFTER A WARM -UP PERIOD . â€œW-G ATES (PRETRAIN )â€MEANS
ADDING W-G ATES TO A PRETRAINED RESNET50. â€œW-G ATES (CONSTANT
INPUT )â€ADOPTS A CONSTANT TENSOR INPUT TO REPLACE FILTER
WEIGHTS IN W-G ATES . â€œPRETRAIN +SAME ITERS â€MEANS CONTINUING
TO TRAIN THE NETWORK WITH THE SAME ITERATIONS AS
â€œW-G ATES (CONSTANT INPUT )â€AND â€œW-G ATES (PRETRAIN )â€.
Top1-Acc Top5-Acc
ResNet50 baseline 76.15% 93.11%
pretrain + same iters 76.58% 93.15%
W-Gates(constant input) 75.32% 92.48%
W-Gates(warm-up) 76.79% 93.27%
W-Gates(pretrain) 77.07% 93.44%
TABLE II
THIS TABLE COMPARES TWO KINDS OF ACTIVATION FUNCTION :BINARY
ACTIVATION AND SCALED SIGMOID ACTIVATION ,IN WHICH THE SCALE
FACTOR IS SET TO 4. (1G: 1 E9)
Top1-Acc Top5-Acc FLOPs
ResNet50 baseline 76.15% 93.11% 4.1G
W-Gates(sigmoid) 75.55% 92.62% 2.7G
W-Gates(binary) 76.01% 92.86% 2.7G
can achieve 0.64% higher Top-1 accuracy than baseline results.
Moreover, equipping W-Gates to a pretrained network can
continue to increase the accuracy by 0.92%, which is 0.49%
higher than the result of adding the same number of iterations.
These results show that adding the W-Gates to a well-trained
network can make better use of its channel selection impact
and obtain more efï¬cient convolutional ï¬lters.
2) Impact of Information from Filter Weights: We are
curious about such a question: Can the W-Gates really learn
information from convolutional ï¬lter weights and give instruc-
tion to ï¬lter selection? An ablation study is conducted to
answer this question. First, we add the modules to a pretrained
network and adopt constant tensors of the same size to replace
the convolutional ï¬lter weights tensors as the input of W-
Gates, and all the values of these constant tensors are set to
ones. Then we continue to train the network for the same
number of iterations with â€œW-Gates(pretrain)â€ in Table I.
From Table I, we see that the W-Gates with a constant tensor
as input achieves 1.75% lower Top-1 accuracy than the W-
Gates that input ï¬lter weights. The results above indicate that
W-Gates can learn information from ï¬lter weights and do a
good ï¬lter selection automatically, which contributes to the
network training.
3) Choice of Gate Activation Function: There are two kinds
of activation functions that can be used to obtain the ï¬lter
gates, scaled sigmoid [14] and binary activation [28], [54].
Previous methods adopt a sigmoid function or a scaled sigmoid
function to generate an approximate binary vector. In this
kind of method, a threshold needs to be set and the values
smaller than it are set to 0. Quantized model works propose
binary activation, which directly obtain a real binary vector and
design a differentiable function to approximate the gradient of
its activation function. We choose binary activation here as ourTABLE III
THIS TABLE COMPARES OUR W-G ATES METHOD WITH SEVERAL
STATE -OF-THE-ART GATE -BASED METHODS . FOR A FAIR COMPARISON ,
WE DO NOT ADD LPN ET TO OPTIMIZE THE PRUNING PROCESS AND ONLY
PRUNE THE NETWORK WITH L1- NORM .
Top1-Acc Top5-Acc FLOPs
ResNet50 baseline 76.15% 93.11% 4.1G
SFP [42] 74.61% 92.87% 2.4G
Thinet-70 [41] 75.31% - 2.9G
Slimming [12] 74.79% 92.21% 2.8G
W-Gates 76.01% 92.86% 2.7G
W-Gates 75.74% 92.62% 2.3G
gate activation function to generate the binary ï¬lter gates in
W-Gates.
To test the impact of gate function choice in our method, we
compare the pruning results of W-Gates with binary activation
and W-Gates with scaled sigmoid. The scale factor of sigmoid
kis set to 4, which follows the setting in [14]. To keep only
one factor changed, we do not adopt our LPNet to give the
latency constraint, but simply imposes L1 regularization on the
ï¬lter gates of each layer to obtain a sparse CNN model. As
can be seen in Table II, with the same FLOPs, W-Gates with
binary activation achieves 0.46% higher top-1 accuracy than
W-Gates with k-sigmoid activation, which proves the binary
activation is more suitable for our method.
4) Compare with Other Gate-based Methods: To further
examine whether the proposed W-Gates works well on ï¬lter
pruning, we compare our method with state-of-the-art gate-
based methods [12], [41], [42]. For Slimming [12], there
are no pruning results on ImageNet. To keep our setup as
close to the original paper as possible, we adopt the original
implementation publicly available and execute on Imagenet.
For a fair comparison, we do not add LPNet to optimize
the pruning process but simply prune the network with L1-
norm, which is consistent with other gate-based methods. The
results for ImageNet dataset are summarized in Table III. W-
Gates achieves superior results than SFP [42], Thinet [41] and
Slimming [12]. The results show that the proposed Weight-
dependent Gates can help the network to do a better ï¬lter
self-selection during training.
C. Pruning Results under Latency Constraint
The inconsistency between hardware agnostic metrics and
actual efï¬ciency leads an increasing attention in directly
optimizing the latency on the target devices. Taking the CNN
as a black box, we train a LPNet to predict the real latency in
the target device. For ResNet34 and ResNet50, we train two
LPNets ofï¬‚ine to predict the latency of basic building blocks
and bottleneck building blocks, respectively. To fully consider
all the factors and decode the sparse architecture, we add the
factors of feature map size and downsampling to the network
encoding vector. For these architectures with shortcut, we do
not prune the output channels of the last layer in each building
block to avoid mismatching with the shortcut channels.
1) Pruning results on ResNet34: We ï¬rst employ the
pruning experiments on a medium depth network ResNet34.

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9
TABLE IV
THIS TABLE COMPARES THE TOP-1ACCURACY AND LATENCY ON
IMAGE NET ABOUT RESNET34. W E SET THE SAME COMPRESSION RATIO
FOR EACH LAYER AS UNIFORM BASELINE . THE INPUT BATCH SIZE IS SET
TO100 AND THE LATENCY IS MEASURED USING PYTORCH ON NVIDIA
RTX 2080 T IGPU.
Uniform Baselines W-Gates
FLOPs Top1-Acc Latency FLOPs Top1-Acc Latency
3.7G (1X) 73.88% 54.04ms - - -
2.9G 72.56% 49.23ms 2.8G 73.76% 46.67ms
2.4G 72.05% 44.27ms 2.3G 73.35% 40.75ms
2.1G 71.32% 43.47ms 2.0G 72.65% 37.30ms
TABLE V
THIS TABLE COMPARES THE TOP-1ACCURACY AND LATENCY ON
IMAGE NET ABOUT RESNET50. T HE RESULTS SHOW THAT ,WITH THE
SAME FLOP S,OUR METHOD OUTPERFORMS THE UNIFORM BASELINES BY
A LARGE MARGIN IN TERMS OF ACCURACY AND LATENCY .
Uniform Baselines W-Gates
FLOPs Top1-Acc Latency FLOPs Top1-Acc Latency
4.1G (1X) 76.15% 105.75ms - - -
3.1G 75.59% 97.87ms 3.0G 76.26% 95.15ms
2.6G 74.77% 91.53ms 2.6G 76.05% 88.00ms
2.1G 74.42% 85.20ms 2.1G 75.14% 80.17ms
ResNet34 consists of basic building blocks, each basic build-
ing block contains two 33convolutional layers. We add the
designed W-Gates to the ï¬rst layer of each basic building block
to learn the information and do ï¬lter selection automatically.
LPNets are also added to the W-Gates framework to predict
the latency of each building block. Then we get the latency of
the whole network to guide the network pruning and optimize
the pruning ratio of each layer. The pruning results are shown
in Table IV. We set the same compression ratio for each layer
in ResNet34 as uniform baselines and prune the same layers
with W-Gates. We measure the real hardware latency using
Pytorch on NVIDIA RTX 2080 Ti GPU and the batch size of
input images is set to 100. It can be observed that our method
can save 25% hardware latency with only 0.5% accuracy loss
on ImageNet dataset. With the same FLOPs, W-Gates achieves
1.1% to 1.3% higher Top-1 accuracy than uniform baseline,
and the hardware latency is also lower, which shows that
our W-Gates method can automatically prune and obtain the
efï¬cient architectures.
2) Pruning results on ResNet50: For the deeper net-
work ResNet50, we adopt the same setting with ResNet34.
ResNet50 consists of bottleneck building blocks, each of
which contains a 33layer and two 11layers. We
employ W-Gates to prune the ï¬lters of the ï¬rst two layers in
each bottleneck module during training. The Top-1 accuracy
and hardware latency of pruned models are shown in Table
V. When pruning 37% FLOPs, we can save 17% hardware
latency without notable accuracy loss. Then we plot two sets of
pruning results on ResNet34 and ResNet50 in Fig. 5, in which
in Function (12) is set to 1.5 to modulate the magnitude of
latency term. As can be seen from the accuracy and latency
trends, during the training and selection process, as the latency
decreases, the accuracy ï¬rst drops and then rises. These twoTABLE VI
THE COEFFICIENT VALUES AND THE CORRESPONDING LATENCY OF
COMPACT NETWORK WHEN PRUNING RESNET34ONIMAGE NET. THE
COEFFICIENT CAN MODULATE THE MAGNITUDE OF LATENCY TERM IN
THE LOSS FUNCTION . AS SHOWN IN THE RESULTS ,THE FINAL ACTUAL
LATENCY WILL GRADUALLY DECREASE AS ALPHA INCREASES .
 0 (Baseline) 1.5 2.0 3.0
Top1 Acc 73.88% 73.76% 73.35% 72.65%
Latency 54.04ms 46.67ms 40.75ms 37.30ms
ï¬gures show that W-Gates tries to search for a better accuracy-
efï¬ciency trade-off via ï¬lter learning and selection in the
training process.
3) The Effect of Coefï¬cient on the Final Trade-off:
As the whole pruning framework is fully differentiable, we
can simultaneously impose the gradients of accuracy loss
and latency loss to optimize the W-Gates of each layer. The
accuracy loss pulls the binary gates to more ones and the
latency loss pulls the binary gates to more zeros. They compete
against each other during training and ï¬nally obtain a compact
network with a better accuracy-efï¬ciency trade-off.
In the proposed latency-aware loss function, the coefï¬cient
can modulate the magnitude of the latency term. Now,
we are interested in the effect of for the ï¬nal trade-off.
Table VI shows the values and the corresponding latency of
compact network when pruning ResNet34 on ImageNet. The
input batch size is set to 100 and the latency is measured using
Pytorch on NVIDIA RTX 2080 Ti GPU. It shows that the ï¬nal
actual latency will gradually decrease as alpha increases. For
a given alpha, the latency will gradually decrease with the
network training, and eventually converge to a best accuracy-
latency trade-off. It can be seen that our method achieves
1.33X latency acceleration with only 0.53% accuracy loss,
which shows that W-Gates is very robust for the choice of
and can consistently deliver lower latency without notable
accuracy loss.
4) Comparisons with State-of-the-arts.: In this section,
we compare the proposed method with several state-of-the-
art ï¬lter pruning methods, including manual-designed indi-
cators (IENNP [16], FPGM [17], VCNNP [22], and HRank
[18]), optimization-based methods (CCP [20], RRBP [23], C-
SGD [21]), and search-based methods (DMCP [19], and S-
MobileNet V2 [24]), on ResNet34, ResNet50, and MobileNet
V2. â€˜W-Gatesâ€™ in Table VII denote our pruning results with
different hardware latency and FLOPs. As there is no latency
data provided in these works, we only compare the Top1
accuracy with the same FLOPs. It can be observed that,
compared with state-of-the-art ï¬lter pruning methods, W-Gates
achieves higher or comparable accuracy with the same FLOPs.
In particular, compared with C-SGD [21], W-Gates achieves
higher or comparable results (75.96% vs 75.27%, 75.14%vs
74.93% , 74.32% vs 74.54% ). Compared with Hrank [18], W-
Gates achieve higher accuracy under 2.4G FLOPs (75.96% vs
74.98%). When the FLOPs of Hrank is compressed to 1.6G, its
accuracy is only 71.98%, which is 2.34% lower than the 1.9G
model of W-Gates, although their FLOPs gap is only 0.3G.
This is because that W-Gates is trying to ï¬nd a better trade-

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10
(a)Resnet34(b) Resnet50
Fig. 5. Two sets of pruning experiments on ResNet34 and ResNet50 on ImageNet. We set in Eq.(12) to 1.5, which is used to modulate the magnitude of
the latency term. In the training process, as the latency decreases, the accuracy ï¬rst drops and then rises. These two ï¬gures show that W-Gates tries to ï¬nd a
better accuracy-latency trade-off via ï¬lter learning and selection in the training process.
TABLE VII
THIS TABLE COMPARES THE TOP-1 I MAGE NET ACCURACY OF OUR
W-G ATES METHOD AND STATE -OF-THE-ART PRUNING METHODS IENNP
[16], FPGM [17], VCNNP [22], HR ANK [18], DMCP [19], CCP [20],
RRBP [23], C-SGD [21], S-M OBILE NETV2 [24], SRR-GR [44],
CLR-RNF [65], HAP [66], F ILTER SKETCH [67] ONRESNET34,
RESNET50, AND MOBILE NETV2.
Model Methods FLOPs Top1 Acc
ResNet34IENNP [16] 2.8G 72.83%
FPGM [17] 2.2G 72.63%
W-Gates 2.8G 73.76%
W-Gates 2.3G 73.35%
ResNet50VCNNP [22] 2.4G 75.20%
FPGM [17] 2.4G 75.59%
FPGM [17] 1.9G 74.83%
IENNP [16] 2.2G 74.50%
DMCP [19] 2.2G 76.20%
CCP [20] 2.1G 75.50%
RRBP [23] 1.9G 73.00%
C-SGD-70 [21] 2.6G 75.27%
C-SGD-60 [21] 2.2G 74.93%
C-SGD-50 [21] 1.8G 74.54%
SRR-GR [44] 2.3G 75.76%
CLR-RNF [65] 2.5G 74.85%
HAP [66] 2.7G 75.12%
FilterSketch [67] 2.6G 75.22%
FilterSketch [67] 2.2G 74.68%
HRank [18] 2.3G 74.98%
HRank [18] 1.6G 71.98%
W-Gates 3.0G 76.26%
W-Gates 2.4G 75.96%
W-Gates 2.1G 75.14%
W-Gates 1.9G 74.32%
MobileNet V2S-MobileNet V2 [24] 0.30G 70.5%
S-MobileNet V2 [24] 0.21G 68.9%
0.75x MobileNetV2 [8] 0.22G 69.8%
W-Gates 0.29G 73.2%
W-Gates 0.22G 70.9%
off between performance and efï¬ciency during the pruning
process.
5) Pruning Results on Cifar-10: In this section, we eval-
uate the proposed method on Cifar-10 dataset. The pruning
experiments are conducted on two general models on Cifar-
10, VGG16 and ResNet56. The results are shown in Table
VIII. It can be observed that the proposed W-Gates canTABLE VIII
THIS TABLE COMPARES THE PRUNING RESULTS UNDER LATENCY
CONSTRAINT ON CIFAR -10 ABOUT VGG16 AND RESNET56.
CONSIDERING THAT THE IMAGE SIZE OF CIFAR -10 IS VERY SMALL (ONLY
3232),WE SET THE TEST BATCH SIZE TO 104TO ENSURE THE
STABILITY OF THE LATENCY TEST RESULTS . (1M: 1 E6)
Model Methods FLOPs Top1 Acc Latency
VGG16baseline 313M 93.72% 391.1ms
VCNNP [22] 190M 93.18% -
HRank [18] 146M 93.43% -
W-Gates 162M 93.61% 245.0ms
ResNet56baseline 126.8M 93.85% 501.8ms
He et al. [9] 63.4M 92.64% -
He et al. [9] 62.0M 90.80% -
FPGM [17] 60.1M 92.89% -
FSDP [68] 64.4M 92.64% -
PARI [69] 60.1M 93.05% -
CHIP [70] 34.8M 92.05% -
ResRep [48] 28.1M 92.66% -
FilterSketch [67] 32.5M 91.20% -
HRank [18] 32.5M 90.72% -
W-Gates 60.0M 93.54% 366.6ms
W-Gates 31.1M 92.66% 319.7ms
also achieve good performance on Cifar-10 dataset, which
outperforms state-of-the-art methods VCNNP [22], HRank
[18], He et al. [9], FSDP [68], FPGM [17], PARI [69],
CHIP [70], ResRep [48], and FilterSketch [67]. For the plain
structure VGG16, W-Gates can reduce 37% hardware latency
and 48% FLOPs with only 0.11% accuracy loss. Similarly,
for the ResNet56 architecture with shortcut connections, W-
Gates can reduce 36% hardware latency and 75% FLOPs
with only 1.2% accuracy loss. Compared with state-of-the-art
methods ( [9], [17], [18], [22], [48], [67], [68], [69], [70]), W-
Gates achieves higher or comparable results. Compared with
ResRep [48], W-Gates has two advantages. First, in addition
to FLOPs optimization, W-Gates can directly optimize the
inference latency of CNNs on the target hardware platform in a
gradient-based manner, which is not available in ResRep [48].
Second, ResRep [48] gradually increases the pruning ratio of
each layer with a predeï¬ned ï¬lter pruning step (step=4), while
W-gates can ï¬‚exibly increase or decrease the pruning ratio of

--- PAGE 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11
TABLE IX
THIS TABLE COMPARES THE PRUNING RESULTS UNDER FLOP S
CONSTRAINT ON CIFAR -10 ABOUT VGG16 AND RESNET56.
Model Methods FLOPs Top1 Acc
VGG16baseline 313M 93.72%
VCNNP [22] 190M 93.18%
HRank [18] 146M 93.43%
W-Gates 176M 93.49%
ResNet56baseline 126.8M 93.85%
He et al. [9] 62.0M 90.80%
HRank [18] 32.5M 90.72%
W-Gates 27.1M 91.60%
TABLE X
THIS TABLE COMPARES THE TOP -1ACCURACY OF THE PRUNED MODELS
BEFORE AND AFTER THE FINE -TUNING PROCESS UNDER FLOP S
CONSTRAINT .
Model Top-1 Acc (before) Top-1 Acc (after) 
VGG16 92.82% 93.49% 0.67%
ResNet56 91.20% 91.60% 0.40%
each layer based on the gradient in the backward propagation
to obtain a more hardware-friendly CNN structure. The results
in Table VIII can also prove the good generalization of the
proposed W-Gates.
D. Pruning Results under FLOPs Constraint
For scenarios that the hardware information is unavailable,
we can not directly optimize the pruning process under the
latency constraint. With the proposed Efï¬ciency Module, we
can switch it to the FLOPs Estimation to cope well with
these scenarios. In this section, we evaluate the proposed
pruning framework under FLOPs constraint. The experiments
are conducted on CIFAR10.
Table IX summarizes the achieved improvement via apply-
ing FLOPs constraint on ï¬lter pruning. Our primary observa-
tion is that our method can have substantial FLOPs reduction
with negligible accuracy loss compared to other state-of-the-
art methods. For the plain architecture VGG16, our method
reduces 43.8% FLOPs with only 0.23% accuracy loss. For a
deeper model ResNet56, our method achieves 78.6% FLOPs
reduction with only 2.25% accuracy loss. From the results we
can also see that the accuracy in Table IX is slightly lower than
the performance of pruning under latency constraint in Table
VIII. The reason is that, FLOPs optimization may mainly
focuses on pruning the shallow layers of the network (the
primary FLOPs hotspots), which may damage the diversity
of low-level features extracted by the shallow layers of the
network. In contrast, the latency optimization pays more
attention to the hardware friendliness of the whole pruned
architecture, rather than only focusing on local parts of the
network. Fortunately, compared with state-of-the-art methods
[9], [18], [22], our method under FLOPs constraint can also
achieve superior or comparable performance, suggesting that
it is a good auxiliary constraint.
Table X shows the top-1 accuracy of the pruned models
before and after the ï¬ne-tuning process under FLOPs con-
straint. It is observed that when the pruning process is just
5 10 15 20 25 30
Convolutional Layer Index0100200300400500600Number of Filters
64 64 64128Stride=2
128 128 128256Stride=2
256 256 256 256 256512Stride=2
512 512
W-Gates tends to keep more filters when stride is 2.3.7G(1X)
2.8G
2.3G
2.0GFig. 6. Visualization of the pruning results on ResNet34. For such an
architecture consisting of basic building blocks, our W-Gates method learns
to keep more ï¬lters where there is a downsampling operation.
0 10 20 30 40 50
Convolutional Layer Index0100200300400500600Number of Filters
646464128128128128256256256256256256512512512
W-Gates tends to keep more filters with 1x1 size.4.1G(1X)
2.2G
2.1G
2.0G
1.9G
Fig. 7. Visualization of the pruning results on ResNet50. We prune ï¬lters of
the ï¬rst two layers in each bottleneck building block. For such an architecture
consisting of bottleneck building blocks, our W-Gates method learns that the
33convolutional layers have larger information redundancy than 11
convolutional layers, and contribute more to the hardware latency.
completed, W-Gates has achieved a good performance on
VGG16 and ResNet56. After the ï¬ne-tuning process, the Top-
1 accuracy of VGG16 and ResNet56 can be further regained
by 0.67% and 0.40%, respectively. These results indicate that
the performance of W-Gates mainly comes from the pruning
stage. The reason is that, with the proposed efï¬ciency-aware
loss function, W-Gates can carry out ï¬lter pruning tasks
with consideration of the overall network structure, which is
beneï¬cial for obtain a better trade-off between accuracy and
efï¬ciency and ï¬nding optimal solutions for network pruning.
E. Visualization Analysis
In the ï¬lter pruning process, we are curious about what
W-Gates have learned from the network and what kind of
architectures have a better trade-off in accuracy-latency. In vi-
sualizing the pruned architectures of ResNet34 and ResNet50,
we ï¬nd that the W-Gates did learn something interesting.

--- PAGE 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12
The visualization of ResNet34 pruned results under latency
constraint on ImageNet is shown in Fig. 6. It can be observed
that for the architecture ResNet34 consisting of basic building
blocks, W-Gates trends not to prune the layer with the down-
sampling operation, although a large ratio of ï¬lters in the other
layers have been pruned. This is similar to the results in [13]
on MobileNet, but is more extreme in our experiments on
ResNet34. It is possibly due to that the network needs more
ï¬lters to retain the information to compensate for the loss of
information caused by feature map downsampling.
However, for ResNet50 architecture consisting of bottleneck
building blocks, the phenomenon is different. As can be seen in
Fig. 7, the W-Gates trends not to prune the 11convolutional
layers and prune the 33convolutional layers with a large
ratio. It shows that W-Gates learn automatically that the 33
convolutional layers have larger information redundancy than
11convolutional layers, and contribute more to the hardware
latency.
V. C ONCLUSIONS
In this paper, we propose a novel ï¬lter pruning method to
address the problems on pruning indicator, pruning ratio, and
platform constraint at the same time. We ï¬rst propose weight-
dependent gates to learn the information from convolutional
weights and generate novel weights-dependent ï¬lter gates.
Then, we construct a switchable Efï¬ciency Module to predict
the latency or FLOPs of the candidate pruned networks and
provide efï¬ciency constraint for the weight-dependent gates
during the pruning process. The entire framework is fully
differentiable with respect to ï¬lter choices and pruning ratios,
which can be optimized by a gradient-based method to achieve
better pruning results.
However, the proposed method also has some limitations.
On the one hand, for the ResNet architecture with shortcut
connections, we did not prune the output channels of the last
layer in each building block, which left some compression
space unused. In our future study, we will try to ï¬nd a better
way to solve this issue. On the other hand, we only focused on
classiï¬cation tasks in this paper and did not apply the proposed
method to other types of applications. In future work, we
will try to transfer the proposed method to other application
scenarios, such as object detection, semantic segmentation, etc.
REFERENCES
[1] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for image
recognition,â€ in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2016, pp. 770â€“778.
[2] K. Wang, D. Zhang, Y . Li, R. Zhang, and L. Lin, â€œCost-effective active
learning for deep image classiï¬cation,â€ IEEE Transactions on Circuits
and Systems for Video Technology , vol. 27, no. 12, pp. 2591â€“2600, 2016.
[3] J. Zhuang, Z. Wang, and B. Wang, â€œVideo semantic segmentation with
distortion-aware feature correction,â€ IEEE Transactions on Circuits and
Systems for Video Technology , 2020.
[4] X. Wang, Z. Chen, J. Tang, B. Luo, Y . Wang, Y . Tian, and F. Wu,
â€œDynamic attention guided multi-trajectory analysis for single object
tracking,â€ IEEE Transactions on Circuits and Systems for Video Tech-
nology , 2021.
[5] R. Girshick, J. Donahue, T. Darrell, and J. Malik, â€œRich feature
hierarchies for accurate object detection and semantic segmentation,â€ in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2014, pp. 580â€“587.[6] J. Guo, W. Zhang, W. Ouyang, and D. Xu, â€œModel compression
using progressive channel pruning,â€ IEEE Transactions on Circuits and
Systems for Video Technology , vol. 31, no. 3, pp. 1114â€“1124, 2020.
[7] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, â€œPruning ï¬lters
for efï¬cient convnets,â€ in Proceedings of International Conference on
Learning Representations , 2017.
[8] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
â€œMobilenetv2: Inverted residuals and linear bottlenecks,â€ in The IEEE
Conference on Computer Vision and Pattern Recognition , June 2018.
[9] Y . He, X. Zhang, and J. Sun, â€œChannel pruning for accelerating
very deep neural networks,â€ in Proceedings of the IEEE international
conference on computer vision , 2017.
[10] Y . Li, S. Lin, B. Zhang, J. Liu, D. Doermann, Y . Wu, F. Huang, and
R. Ji, â€œExploiting kernel sparsity and entropy for interpretable cnn
compression,â€ in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , 2019, pp. 2800â€“2809.
[11] Z. Huang and N. Wang, â€œData-driven sparse structure selection for
deep neural networks,â€ in Proceedings of the European Conference on
Computer Vision , 2018, pp. 304â€“320.
[12] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, â€œLearning efï¬-
cient convolutional networks through network slimming,â€ in Proceedings
of the IEEE International Conference on Computer Vision , 2017, pp.
2736â€“2744.
[13] Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, K.-T. Cheng, and J. Sun,
â€œMetapruning: Meta learning for automatic neural network channel
pruning,â€ in Proceedings of the IEEE international conference on
computer vision , 2019, pp. 3296â€“3305.
[14] J.-H. Luo and J. Wu, â€œAutopruner: An end-to-end trainable ï¬lter pruning
method for efï¬cient deep model inference,â€ Pattern Recognition , vol.
107, p. 107461, 2020.
[15] K. Simonyan and A. Zisserman, â€œVery deep convolutional networks for
large-scale image recognition,â€ arXiv preprint arXiv:1409.1556 , 2014.
[16] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, â€œImportance
estimation for neural network pruning,â€ in The IEEE Conference on
Computer Vision and Pattern Recognition , June 2019.
[17] Y . He, P. Liu, Z. Wang, Z. Hu, and Y . Yang, â€œFilter pruning via
geometric median for deep convolutional neural networks acceleration,â€
inProceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , 2019, pp. 4340â€“4349.
[18] M. Lin, R. Ji, Y . Wang, Y . Zhang, B. Zhang, Y . Tian, and L. Shao,
â€œHrank: Filter pruning using high-rank feature map,â€ in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2020, pp. 1529â€“1538.
[19] S. Guo, Y . Wang, Q. Li, and J. Yan, â€œDmcp: Differentiable markov
channel pruning for neural networks,â€ in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2020, pp.
1539â€“1547.
[20] H. Peng, J. Wu, S. Chen, and J. Huang, â€œCollaborative channel pruning
for deep networks,â€ in International conference on machine learning .
PMLR, 2019, pp. 5113â€“5122.
[21] X. Ding, G. Ding, Y . Guo, and J. Han, â€œCentripetal sgd for pruning very
deep convolutional networks with complicated structure,â€ in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition ,
2019, pp. 4943â€“4953.
[22] C. Zhao, B. Ni, J. Zhang, Q. Zhao, W. Zhang, and Q. Tian, â€œVariational
convolutional neural network pruning,â€ in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2019, pp.
2780â€“2789.
[23] Y . Zhou, Y . Zhang, Y . Wang, and Q. Tian, â€œAccelerate cnn via recursive
bayesian pruning,â€ in Proceedings of the IEEE international conference
on computer vision , 2019, pp. 3306â€“3315.
[24] J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang, â€œSlimmable neural
networks,â€ in International Conference on Learning Representations ,
2019.
[25] Y . Li, W. Wu, Z. Liu, C. Zhang, X. Zhang, H. Yao, and B. Yin, â€œWeight-
dependent gates for differentiable neural network pruning,â€ in European
Conference on Computer Vision Workshops . Springer, 2020, pp. 23â€“37.
[26] K. Jia, D. Tao, S. Gao, and X. Xu, â€œImproving training of deep neural
networks via singular value bounding,â€ in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , vol. 2017,
2017, pp. 3994â€“4002.
[27] X. Yu, T. Liu, X. Wang, and D. Tao, â€œOn compressing deep models
by low rank and sparse decomposition,â€ in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2017, pp.
7370â€“7379.

--- PAGE 13 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13
[28] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y . Bengio, â€œBi-
narized neural networks,â€ in Advances in neural information processing
systems , 2016, pp. 4107â€“4115.
[29] M. Rastegari, V . Ordonez, J. Redmon, and A. Farhadi, â€œXnor-net:
Imagenet classiï¬cation using binary convolutional neural networks,â€ in
European Conference on Computer Vision . Springer, 2016, pp. 525â€“
542.
[30] G. Hinton, O. Vinyals, and J. Dean, â€œDistilling the knowledge in a neural
network,â€ arXiv preprint arXiv:1503.02531 , 2015.
[31] K. Zhang, C. Zhanga, S. Li, D. Zeng, and S. Ge, â€œStudent network
learning via evolutionary knowledge distillation,â€ IEEE Transactions on
Circuits and Systems for Video Technology , vol. 32, no. 4, pp. 2251â€“
2263, 2021.
[32] X. Zhang, X. Zhou, M. Lin, and J. Sun, â€œShufï¬‚enet: An extremely efï¬-
cient convolutional neural network for mobile devices,â€ in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition ,
2018, pp. 6848â€“6856.
[33] J. Guo, J. Liu, and D. Xu, â€œJointpruning: Pruning networks along multi-
ple dimensions for efï¬cient point cloud processing,â€ IEEE Transactions
on Circuits and Systems for Video Technology , 2021.
[34] Z. Guo, X. Zhang, H. Mu, W. Heng, Z. Liu, Y . Wei, and J. Sun, â€œSingle
path one-shot neural architecture search with uniform sampling,â€ in
European Conference on Computer Vision . Springer, 2020, pp. 544â€“
560.
[35] M. Xu, M. Zhu, Y . Liu, F. X. Lin, and X. Liu, â€œDeepcache: Principled
cache for mobile deep vision,â€ in Proceedings of the 24th Annual
International Conference on Mobile Computing and Networking , 2018,
pp. 129â€“144.
[36] Y . Li, C. Zhang, S. Han, L. L. Zhang, B. Yin, Y . Liu, and M. Xu, â€œBoost-
ing mobile cnn inference through semantic memory,â€ in Proceedings of
the 29th ACM International Conference on Multimedia , 2021, pp. 2362â€“
2371.
[37] S. Han, H. Mao, and W. J. Dally, â€œDeep compression: Compressing
deep neural networks with pruning, trained quantization and huffman
coding,â€ in Proceedings of International Conference on Learning Rep-
resentations , 2016.
[38] H.-J. Kang, â€œAccelerator-aware pruning for convolutional neural net-
works,â€ IEEE Transactions on Circuits and Systems for Video Technol-
ogy, vol. 30, no. 7, pp. 2093â€“2103, 2019.
[39] Y . Li, L. Wang, S. Peng, A. Kumar, and B. Yin, â€œUsing feature
entropy to guide ï¬lter pruning for efï¬cient convolutional networks,â€ in
International Conference on Artiï¬cial Neural Networks . Springer, 2019,
pp. 263â€“274.
[40] A. Kumar, A. M. Shaikh, Y . Li, H. Bilal, and B. Yin, â€œPruning ï¬lters with
l1-norm and capped l1-norm for cnn compression,â€ Applied Intelligence ,
pp. 1â€“9, 2020.
[41] J.-H. Luo, H. Zhang, H.-Y . Zhou, C.-W. Xie, J. Wu, and W. Lin, â€œThinet:
pruning cnn ï¬lters for a thinner net,â€ IEEE transactions on pattern
analysis and machine intelligence , vol. 41, no. 10, pp. 2525â€“2538, 2018.
[42] Y . He, G. Kang, X. Dong, Y . Fu, and Y . Yang, â€œSoft ï¬lter pruning for
accelerating deep convolutional neural networks,â€ in Proceedings of the
27th International Joint Conference on Artiï¬cial Intelligence , 2018, pp.
2234â€“2240.
[43] S. Lin, R. Ji, Y . Li, Y . Wu, F. Huang, and B. Zhang, â€œAccelerating convo-
lutional networks via global & dynamic ï¬lter pruning.â€ in Proceedings of
the 27th International Joint Conference on Artiï¬cial Intelligence , 2018,
pp. 2425â€“2432.
[44] Z. Wang, C. Li, and X. Wang, â€œConvolutional neural network pruning
with structural redundancy reduction,â€ in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2021, pp.
14 913â€“14 922.
[45] J.-H. Luo and J. Wu, â€œNeural network pruning with residual-connections
and limited-data,â€ in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , June 2020.
[46] Y . Guo, H. Yuan, J. Tan, Z. Wang, S. Yang, and J. Liu, â€œGdp: Stabilized
neural network pruning via gates with differentiable polarization,â€ in
Proceedings of the IEEE/CVF International Conference on Computer
Vision , 2021, pp. 5239â€“5250.
[47] Y . Tang, Y . Wang, Y . Xu, Y . Deng, C. Xu, D. Tao, and C. Xu, â€œManifold
regularized dynamic network pruning,â€ in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2021, pp.
5018â€“5028.
[48] X. Ding, T. Hao, J. Tan, J. Liu, J. Han, Y . Guo, and G. Ding, â€œResrep:
Lossless cnn pruning via decoupling remembering and forgetting,â€ in
Proceedings of the IEEE/CVF International Conference on Computer
Vision , 2021, pp. 4510â€“4520.[49] G. Ding, S. Zhang, Z. Jia, J. Zhong, and J. Han, â€œWhere to prune: Using
lstm to guide data-dependent soft pruning,â€ IEEE Transactions on Image
Processing , vol. 30, pp. 293â€“304, 2020.
[50] M. Gong, K.-y. Feng, X. Fei, A. Qin, H. Li, and Y . Wu, â€œAn auto-
matically layer-wise searching strategy for channel pruning based on
task-driven sparsity optimization,â€ IEEE Transactions on Circuits and
Systems for Video Technology , 2022.
[51] D. Soudry, I. Hubara, and R. Meir, â€œExpectation backpropagation:
Parameter-free training of multilayer neural networks with continuous or
discrete weights,â€ in Advances in neural information processing systems ,
2014, pp. 963â€“971.
[52] B. Wu, X. Dai, P. Zhang, Y . Wang, F. Sun, Y . Wu, Y . Tian, P. Vajda,
Y . Jia, and K. Keutzer, â€œFbnet: Hardware-aware efï¬cient convnet design
via differentiable neural architecture search,â€ in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2019, pp.
10 734â€“10 742.
[53] E. Jang, S. Gu, and B. Poole, â€œCategorical reparameterization with
gumbel-softmax,â€ arXiv preprint arXiv:1611.01144 , 2016.
[54] Z. Liu, B. Wu, W. Luo, X. Yang, W. Liu, and K.-T. Cheng, â€œBi-real net:
Enhancing the performance of 1-bit cnns with improved representational
capability and advanced training algorithm,â€ in Proceedings of the
European Conference on Computer Vision , 2018, pp. 722â€“737.
[55] Y . He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han, â€œAmc: Automl
for model compression and acceleration on mobile devices,â€ in The
European Conference on Computer Vision , 2018, pp. 784â€“800.
[56] T.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, M. Sandler, V . Sze,
and H. Adam, â€œNetadapt: Platform-aware neural network adaptation for
mobile applications,â€ in Proceedings of the European Conference on
Computer Vision , 2018, pp. 285â€“300.
[57] X. Dai, P. Zhang, B. Wu, H. Yin, F. Sun, Y . Wang, M. Dukhan,
Y . Hu, Y . Wu, Y . Jia et al. , â€œChamnet: Towards efï¬cient network design
through platform-aware model adaptation,â€ in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2019, pp.
11 398â€“11 407.
[58] H. Yang, Y . Zhu, and J. Liu, â€œEcc: Platform-independent energy-
constrained deep neural network compression via a bilinear regression
model,â€ in Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , 2019, pp. 11 206â€“11 215.
[59] S. Gao, F. Huang, W. Cai, and H. Huang, â€œNetwork pruning via perfor-
mance maximization,â€ in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2021, pp. 9270â€“9280.
[60] H. Cai, L. Zhu, and S. Han, â€œProxylessnas: Direct neural architecture
search on target task and hardware,â€ in International Conference on
Learning Representations , 2019.
[61] Y . Yang, J. Wu, H. Li, X. Li, T. Shen, and Z. Lin, â€œDynamical system
inspired adaptive time stepping controller for residual network families,â€
inProceedings of the AAAI Conference on Artiï¬cial Intelligence , vol. 34,
no. 04, 2020, pp. 6648â€“6655.
[62] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, â€œMobilenets: Efï¬cient convo-
lutional neural networks for mobile vision applications,â€ arXiv preprint
arXiv:1704.04861 , 2017.
[63] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, â€œImagenet:
A large-scale hierarchical image database,â€ in IEEE conference on
computer vision and pattern recognition , 2009, pp. 248â€“255.
[64] A. Krizhevsky and G. Hinton, â€œLearning multiple layers of features from
tiny images,â€ Citeseer, Tech. Rep., 2009.
[65] M. Lin, L. Cao, Y . Zhang, L. Shao, C.-W. Lin, and R. Ji, â€œPruning
networks with cross-layer ranking & k-reciprocal nearest ï¬lters,â€ IEEE
Transactions on Neural Networks and Learning Systems , 2022.
[66] S. Yu, Z. Yao, A. Gholami, Z. Dong, S. Kim, M. W. Mahoney, and
K. Keutzer, â€œHessian-aware pruning and optimal neural implant,â€ in
Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision (WACV) , 2022, pp. 3880â€“3891.
[67] M. Lin, L. Cao, S. Li, Q. Ye, Y . Tian, J. Liu, Q. Tian, and R. Ji, â€œFilter
sketch for network pruning,â€ IEEE Transactions on Neural Networks
and Learning Systems , 2021.
[68] N. Gkalelis and V . Mezaris, â€œFractional step discriminant pruning: A
ï¬lter pruning framework for deep convolutional neural networks,â€ in
2020 IEEE International Conference on Multimedia & Expo Workshops
(ICMEW) . IEEE, 2020, pp. 1â€“6.
[69] Y . Cai, Z. Yin, K. Guo, and X. Xu, â€œPruning the unimportant or
redundant ï¬lters? synergy makes better,â€ in 2021 International Joint
Conference on Neural Networks (IJCNN) . IEEE, 2021, pp. 1â€“8.
[70] Y . Sui, M. Yin, Y . Xie, H. Phan, S. Aliari Zonouz, and B. Yuan, â€œChip:
Channel independence-based pruning for compact neural networks,â€
Advances in Neural Information Processing Systems , vol. 34, 2021.
