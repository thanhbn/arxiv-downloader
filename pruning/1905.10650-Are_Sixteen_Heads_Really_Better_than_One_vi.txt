Mười sáu đầu có thực sự tốt hơn một đầu không?

Paul Michel
Viện Công nghệ Ngôn ngữ
Đại học Carnegie Mellon
Pittsburgh, PA
pmichel1@cs.cmu.edu

Omer Levy
Nghiên cứu Trí tuệ Nhân tạo Facebook
Seattle, WA
omerlevy@fb.com

Graham Neubig
Viện Công nghệ Ngôn ngữ
Đại học Carnegie Mellon
Pittsburgh, PA
gneubig@cs.cmu.edu

Tóm tắt

Attention là một cơ chế mạnh mẽ và phổ biến cho phép các mô hình neural tập trung vào những phần thông tin nổi bật cụ thể bằng cách lấy trung bình có trọng số của chúng khi đưa ra dự đoán. Đặc biệt, multi-headed attention là động lực đằng sau nhiều mô hình xử lý ngôn ngữ tự nhiên (NLP) tiên tiến gần đây như các mô hình MT dựa trên Transformer và BERT. Những mô hình này áp dụng nhiều cơ chế attention song song, với mỗi "đầu" attention có khả năng tập trung vào các phần khác nhau của đầu vào, điều này giúp có thể biểu hiện các hàm phức tạp vượt ra ngoài trung bình có trọng số đơn giản. Trong bài báo này, chúng tôi đưa ra quan sát đáng ngạc nhiên rằng ngay cả khi các mô hình đã được huấn luyện sử dụng nhiều đầu, trong thực tế, một tỷ lệ lớn các đầu attention có thể được loại bỏ tại thời điểm kiểm tra mà không ảnh hưởng đáng kể đến hiệu suất. Trên thực tế, một số lớp thậm chí có thể được giảm xuống chỉ còn một đầu duy nhất. Chúng tôi tiếp tục kiểm tra các thuật toán tham lam để cắt giảm mô hình, và những cải thiện tiềm năng về tốc độ, hiệu quả bộ nhớ và độ chính xác có thể đạt được từ đó. Cuối cùng, chúng tôi phân tích kết quả về việc những phần nào của mô hình phụ thuộc nhiều hơn vào việc có nhiều đầu, và cung cấp bằng chứng sơ bộ rằng động lực huấn luyện đóng vai trò trong những lợi ích được cung cấp bởi multi-head attention.

1 Giới thiệu

Transformers (Vaswani et al., 2017) đã thể hiện hiệu suất tốt nhất trên nhiều tác vụ NLP khác nhau, bao gồm nhưng không giới hạn ở dịch máy (Vaswani et al., 2017; Ott et al., 2018), trả lời câu hỏi (Devlin et al., 2018), phân loại văn bản (Radford et al., 2018), và gán nhãn vai trò ngữ nghĩa (Strubell et al., 2018). Trọng tâm của những cải tiến kiến trúc, Transformer mở rộng cơ chế attention tiêu chuẩn (Bahdanau et al., 2015; Cho et al., 2014) thông qua multi-headed attention (MHA), nơi attention được tính toán độc lập bởi Nh cơ chế attention song song (đầu). Đã được chứng minh rằng ngoài việc cải thiện hiệu suất, MHA có thể giúp với sự nhất quán chủ-động từ (Tang et al., 2018) và một số đầu có thể dự đoán cấu trúc phụ thuộc (Raganato và Tiedemann, 2018). Kể từ đó, một số mở rộng cho phương pháp tổng quát đã được đề xuất (Ahmed et al., 2017; Shen et al., 2018).

Tuy nhiên, vẫn chưa hoàn toàn rõ ràng: nhiều đầu trong những mô hình này mang lại cho chúng ta điều gì? Trong bài báo này, chúng tôi đưa ra quan sát đáng ngạc nhiên rằng – trong cả các mô hình dựa trên Transformer cho dịch máy và suy luận ngôn ngữ tự nhiên dựa trên BERT (Devlin et al., 2018) – hầu hết các đầu attention có thể được loại bỏ riêng lẻ sau khi huấn luyện mà không có bất kỳ tác dụng phụ đáng kể nào về hiệu suất kiểm tra (§3.2). Đáng chú ý, nhiều lớp attention thậm chí có thể được giảm riêng lẻ xuống một đầu attention duy nhất mà không ảnh hưởng đến hiệu suất kiểm tra (§3.3).

Dựa trên quan sát này, chúng tôi tiếp tục đề xuất một thuật toán đơn giản cắt giảm một cách tham lam và lặp đi lặp lại các đầu attention có vẻ đóng góp ít hơn cho mô hình. Bằng cách loại bỏ đồng thời các đầu attention từ toàn bộ mạng, mà không hạn chế việc cắt giảm ở một lớp duy nhất, chúng tôi thấy rằng các phần lớn của mạng có thể được loại bỏ với ít hoặc không có hậu quả nào, nhưng phần lớn các đầu phải được giữ lại để tránh sụt giảm thảm khốc về hiệu suất (§4). Chúng tôi tiếp tục thấy rằng điều này có lợi ích đáng kể cho hiệu quả thời gian suy luận, dẫn đến tăng tốc độ suy luận lên đến 17,5% cho mô hình dựa trên BERT.

Sau đó chúng tôi đi sâu vào phân tích thêm. Nhìn kỹ hơn vào trường hợp dịch máy cho thấy rằng các lớp attention encoder-decoder đặc biệt nhạy cảm với việc cắt giảm, nhiều hơn các lớp self-attention, gợi ý rằng multi-headedness đóng vai trò quan trọng trong thành phần này (§5). Cuối cùng, chúng tôi cung cấp bằng chứng rằng sự khác biệt giữa các đầu quan trọng và không quan trọng tăng lên khi quá trình huấn luyện tiến triển, gợi ý một tương tác giữa multi-headedness và động lực huấn luyện (§6).

2 Bối cảnh: Attention, Multi-headed Attention, và Masking

Trong phần này chúng tôi trình bày nền tảng ký hiệu về attention, và cũng mô tả phương pháp của chúng tôi để che (mask) các đầu attention.

2.1 Single-headed Attention

Chúng tôi nhắc lại ngắn gọn cách attention cơ bản hoạt động. Chúng tôi tập trung vào scaled bilinear attention (Luong et al., 2015), biến thể được sử dụng phổ biến nhất trong các lớp MHA. Cho một chuỗi n vector d chiều x = x₁; :::; xₙ ∈ Rᵈ, và một vector truy vấn q ∈ Rᵈ, lớp attention được tham số hóa bởi Wₖ; Wᵧ; Wᵥ; Wₒ ∈ Rᵈˣᵈ tính toán tổng có trọng số:

Att_{Wₖ,Wᵧ,Wᵥ,Wₒ}(x; q) = Wₒ ∑ⁿᵢ₌₁ αᵢWᵥxᵢ

trong đó αᵢ = softmax(q^T W^T_q Wₖxᵢ/√d)

Trong self-attention, mỗi xᵢ được sử dụng làm truy vấn q để tính toán một chuỗi biểu diễn mới, trong khi ở các mô hình sequence-to-sequence q thường là trạng thái decoder trong khi x tương ứng với đầu ra encoder.

2.2 Multi-headed Attention

Trong multi-headed attention (MHA), Nₕ lớp attention được tham số hóa độc lập được áp dụng song song để có được kết quả cuối cùng:

MHAtt(x; q) = ∑ᴺʰₕ₌₁ Att_{Wʰₖ,Wʰᵧ,Wʰᵥ,Wʰₒ}(x; q)  (1)

trong đó Wʰₖ; Wʰᵧ; Wʰᵥ ∈ Rᵈʰˣᵈ và Wʰₒ ∈ Rᵈˣᵈʰ. Khi dₕ = d, MHA có tính biểu đạt nghiêm ngặt hơn attention cơ bản. Tuy nhiên, để giữ số lượng tham số không đổi, dₕ thường được đặt bằng d/Nₕ, trong trường hợp đó MHA có thể được xem như một ensemble của các lớp attention cơ bản low-rank². Trong phần tiếp theo, chúng tôi sử dụng Attₕ(x) như một ký hiệu ngắn gọn cho đầu ra của đầu h trên đầu vào x.

Để cho phép các đầu attention khác nhau tương tác với nhau, transformers áp dụng một mạng feed-forward phi tuyến trên đầu ra của MHA, tại mỗi lớp transformer (Vaswani et al., 2017).

2.3 Masking Attention Heads

Để thực hiện các thí nghiệm ablation trên các đầu, chúng tôi sửa đổi công thức cho MHAtt:

MHAtt(x; q) = ∑ᴺʰₕ₌₁ γₕAtt_{Wʰₖ,Wʰᵧ,Wʰᵥ,Wʰₒ}(x; q)

trong đó γₕ là các biến mask với giá trị trong {0; 1}. Khi tất cả γₕ bằng 1, điều này tương đương với công thức ở Phương trình 1. Để che đầu h, chúng tôi đơn giản đặt γₕ = 0.

3 Tất cả các Đầu Attention có Quan trọng không?

Chúng tôi thực hiện một loạt thí nghiệm trong đó chúng tôi loại bỏ một hoặc nhiều đầu attention từ một kiến trúc cho trước tại thời điểm kiểm tra, và đo lường sự khác biệt về hiệu suất. Trước tiên chúng tôi loại bỏ một đầu attention duy nhất mỗi lần (§3.2) và sau đó loại bỏ mọi đầu trong toàn bộ lớp trừ một đầu (§3.3).

3.1 Thiết lập Thí nghiệm

Trong tất cả các thí nghiệm sau đây, chúng tôi xem xét hai mô hình đã được huấn luyện:

WMT Đây là kiến trúc transformer "large" gốc từ Vaswani et al. 2017 với 6 lớp và 16 đầu mỗi lớp, được huấn luyện trên corpus WMT2014 English to French. Chúng tôi sử dụng mô hình được huấn luyện trước của Ott et al. 2018³ và báo cáo điểm BLEU trên bộ kiểm tra newstest2013. Theo Ott et al. 2018, chúng tôi tính điểm BLEU trên đầu ra đã được tokenize của mô hình sử dụng Moses (Koehn et al., 2007). Ý nghĩa thống kê được kiểm tra với paired bootstrap resampling (Koehn, 2004) sử dụng compare-mt⁴ (Neubig et al., 2019) với 1000 resamples. Một đặc điểm của mô hình này là nó có 3 cơ chế attention riêng biệt: encoder self-attention (Enc-Enc), encoder-decoder attention (Enc-Dec) và decoder self-attention (Dec-Dec), tất cả đều sử dụng MHA.

BERT BERT (Devlin et al., 2018) là một transformer đơn được huấn luyện trước trên một tác vụ "masked language modeling" kiểu cloze không giám sát và sau đó được fine-tune trên các tác vụ cụ thể. Tại thời điểm ra đời, nó đạt được hiệu suất tốt nhất trên nhiều tác vụ NLP khác nhau. Chúng tôi sử dụng mô hình base-uncased được huấn luyện trước của Devlin et al. 2018 với 12 lớp và 12 đầu attention mà chúng tôi fine-tune và đánh giá trên MultiNLI (Williams et al., 2018). Chúng tôi báo cáo độ chính xác trên tập validation "matched". Chúng tôi kiểm tra ý nghĩa thống kê sử dụng t-test. Trái ngược với mô hình WMT, BERT chỉ có một cơ chế attention (self-attention trong mỗi lớp).

3.2 Ablating One Head

Để hiểu đóng góp của một đầu attention cụ thể h, chúng tôi đánh giá hiệu suất của mô hình trong khi che đầu đó (tức là thay thế Attₕ(x) bằng zeros). Nếu hiệu suất không có h giảm đáng kể so với mô hình đầy đủ, h rõ ràng là quan trọng; nếu hiệu suất tương đương, h là dư thừa cho phần còn lại của mô hình.

Hình 1a và 1b cho thấy phân phối các đầu theo điểm số mô hình sau khi che, cho WMT và BERT tương ứng. Chúng tôi quan sát thấy rằng phần lớn các đầu attention có thể được loại bỏ mà không lệch quá nhiều từ điểm số gốc. Đáng ngạc nhiên, trong một số trường hợp loại bỏ một đầu attention dẫn đến tăng BLEU/độ chính xác.

Để có cái nhìn chi tiết hơn về những kết quả này, chúng tôi phóng to các lớp encoder self-attention của mô hình WMT trong Bảng 1. Đáng chú ý, chúng tôi thấy rằng chỉ có 8 (trong 96) đầu gây ra thay đổi có ý nghĩa thống kê về hiệu suất khi chúng được loại bỏ khỏi mô hình, một nửa trong số đó thực sự dẫn đến điểm BLEU cao hơn. Điều này dẫn chúng tôi đến quan sát đầu tiên: tại thời điểm kiểm tra, hầu hết các đầu là dư thừa cho phần còn lại của mô hình.

3.3 Ablating All Heads but One

Quan sát này dẫn đến câu hỏi: có cần hơn một đầu không? Do đó, chúng tôi tính toán sự khác biệt về hiệu suất khi tất cả các đầu trừ một đầu được loại bỏ, trong một lớp duy nhất. Trong Bảng 2 và Bảng 3, chúng tôi báo cáo điểm số tốt nhất cho mỗi lớp trong mô hình, tức là điểm số khi giảm toàn bộ lớp xuống đầu quan trọng nhất duy nhất.

Chúng tôi thấy rằng, đối với hầu hết các lớp, một đầu thực sự đủ tại thời điểm kiểm tra, mặc dù mạng được huấn luyện với 12 hoặc 16 đầu attention. Điều này đáng chú ý bởi vì những lớp này có thể được giảm xuống single-headed attention chỉ với 1/16 (tương ứng 1/12) số lượng tham số của một lớp attention cơ bản. Tuy nhiên, một số lớp yêu cầu nhiều đầu attention; ví dụ, thay thế lớp cuối trong encoder-decoder attention của WMT bằng một đầu duy nhất làm giảm hiệu suất ít nhất 13,5 điểm BLEU. Chúng tôi phân tích thêm khi nào các thành phần mô hình khác nhau phụ thuộc vào nhiều đầu hơn trong §5.

Ngoài ra, chúng tôi xác minh rằng kết quả này vẫn đúng ngay cả khi chúng tôi không có quyền truy cập vào tập đánh giá khi chọn đầu "tốt nhất độc lập". Với mục đích này, chúng tôi chọn đầu tốt nhất cho mỗi lớp trên tập validation (newstest2013 cho WMT và một tập con 5.000 được chọn ngẫu nhiên từ tập huấn luyện của MNLI cho BERT) và đánh giá hiệu suất của mô hình trên tập kiểm tra (newstest2014 cho WMT và tập validation MNLI-matched cho BERT). Chúng tôi quan sát thấy những phát hiện tương tự: chỉ giữ một đầu không dẫn đến thay đổi có ý nghĩa thống kê về hiệu suất cho 50% (tương ứng 100%) các lớp của WMT (tương ứng BERT). Kết quả chi tiết có thể được tìm thấy trong Phụ lục A.

3.4 Các Đầu Quan trọng có Giống nhau trên các Dataset không?

Có một điều cần lưu ý trong hai thí nghiệm trước của chúng tôi: những kết quả này chỉ có giá trị trên các tập kiểm tra cụ thể (và khá nhỏ), gây nghi ngờ về khả năng tổng quát hóa của chúng đối với các dataset khác. Như một bước đầu để hiểu liệu một số đầu có quan trọng một cách phổ quát hay không, chúng tôi thực hiện cùng một nghiên cứu ablation trên một tập kiểm tra thứ hai, ngoài miền. Cụ thể, chúng tôi xem xét tập validation MNLI "mismatched" cho BERT và tập kiểm tra MTNT English to French (Michel và Neubig, 2018) cho mô hình WMT, cả hai đều được tập hợp với mục đích cung cấp các bộ kiểm tra tương phản, ngoài miền cho các tác vụ tương ứng của chúng.

Chúng tôi thực hiện cùng nghiên cứu ablation như §3.2 trên mỗi dataset này và báo cáo kết quả trong Hình 2a và 2b. Chúng tôi nhận thấy rằng có một tương quan dương, >0.5 (p < 0.01) giữa hiệu ứng của việc loại bỏ một đầu trên cả hai dataset. Hơn nữa, các đầu có hiệu ứng cao nhất về hiệu suất trên một miền có xu hướng có cùng hiệu ứng trên miền khác, điều này cho thấy rằng các đầu quan trọng nhất từ §3.2 thực sự quan trọng "phổ quát".

4 Cắt giảm Lặp lại các Đầu Attention

Trong các thí nghiệm ablation của chúng tôi (§3.2 và §3.3), chúng tôi quan sát hiệu ứng của việc loại bỏ một hoặc nhiều đầu trong một lớp duy nhất, mà không xem xét điều gì sẽ xảy ra nếu chúng tôi thay đổi hai hoặc nhiều lớp khác nhau cùng một lúc. Để kiểm tra hiệu ứng tích lũy của việc cắt giảm nhiều đầu từ toàn bộ mô hình, chúng tôi sắp xếp tất cả các đầu attention trong mô hình theo điểm số quan trọng proxy (được mô tả bên dưới), và sau đó loại bỏ từng đầu một. Chúng tôi sử dụng cách tiếp cận lặp lại, heuristic này để tránh tìm kiếm tổ hợp, điều không thực tế cho số lượng đầu và thời gian cần để đánh giá mỗi mô hình.

4.1 Điểm số Quan trọng của Đầu cho Cắt giảm

Như một điểm số proxy cho tầm quan trọng của đầu, chúng tôi xem xét độ nhạy cảm dự kiến của mô hình đối với các biến mask γₕ được định nghĩa trong §2.3:

Iₕ = E_{x~X} |∂L(x)/∂γₕ|  (2)

trong đó X là phân phối dữ liệu và L(x) là loss trên mẫu x. Trực quan, nếu Iₕ có giá trị cao thì việc thay đổi γₕ có khả năng có hiệu ứng lớn lên mô hình. Đặc biệt chúng tôi thấy giá trị tuyệt đối là quan trọng để tránh các datapoint có đóng góp âm hoặc dương cao triệt tiêu lẫn nhau trong tổng. Thay Phương trình 1 vào Phương trình 2 và áp dụng quy tắc chuỗi cho ra biểu thức cuối cùng sau đây cho Iₕ:

Iₕ = E_{x~X} |Attₕ(x)^T ∂L(x)/∂Attₕ(x)|

Công thức này gợi nhớ đến kho tài liệu phong phú về cắt giảm mạng neural (LeCun et al., 1990; Hassibi và Stork, 1993; Molchanov et al., 2017, và các tác giả khác). Đặc biệt, nó tương đương với phương pháp Taylor expansion từ Molchanov et al. (2017).

Về hiệu suất, ước lượng Iₕ chỉ cần thực hiện một lượt forward và backward pass, và do đó không chậm hơn việc huấn luyện. Trong thực tế, chúng tôi tính kỳ vọng trên dữ liệu huấn luyện hoặc một tập con của nó⁵. Như được khuyến nghị bởi Molchanov et al. (2017) chúng tôi chuẩn hóa điểm số quan trọng theo lớp (sử dụng chuẩn ℓ₂).

4.2 Hiệu ứng của Cắt giảm lên BLEU/Độ chính xác

Hình 3a (cho WMT) và 3b (cho BERT) mô tả hiệu ứng của việc cắt giảm attention-head lên hiệu suất mô hình trong khi loại bỏ tăng dần 10% tổng số đầu theo thứ tự tăng dần của Iₕ tại mỗi bước. Chúng tôi cũng báo cáo kết quả khi thứ tự cắt giảm được xác định bởi sự khác biệt điểm số từ §3.2 (trong các đường đứt nét), nhưng thấy rằng sử dụng Iₕ nhanh hơn và cho kết quả tốt hơn.

Chúng tôi quan sát thấy rằng cách tiếp cận này cho phép chúng tôi cắt giảm lên đến 20% và 40% các đầu từ WMT và BERT (tương ứng), mà không phải chịu bất kỳ tác động tiêu cực đáng chú ý nào. Hiệu suất giảm mạnh khi cắt giảm thêm, có nghĩa là không mô hình nào có thể được giảm xuống mô hình attention đơn đầu hoàn toàn mà không cần huấn luyện lại hoặc chịu tổn thất đáng kể về hiệu suất. Chúng tôi tham khảo Phụ lục B cho các thí nghiệm trên bốn dataset bổ sung.

4.3 Hiệu ứng của Cắt giảm lên Hiệu quả

Ngoài hiệu suất tác vụ downstream, có những ưu điểm bản chất của việc cắt giảm đầu. Đầu tiên, mỗi đầu đại diện cho một tỷ lệ không đáng kể của tổng tham số trong mỗi lớp attention (6.25% cho WMT, 8.34% cho BERT), và do đó của tổng mô hình (nói chung, trong cả hai mô hình của chúng tôi, khoảng một phần ba tổng số tham số được dành cho MHA trên tất cả các lớp)⁶. Điều này hấp dẫn trong bối cảnh triển khai mô hình trong các thiết lập hạn chế bộ nhớ.

Hơn nữa, chúng tôi thấy rằng việc thực sự cắt giảm các đầu (và không chỉ che) dẫn đến tăng tốc độ suy luận đáng kể. Bảng 4 báo cáo số lượng ví dụ mỗi giây được xử lý bởi BERT, trước và sau khi cắt giảm 50% tất cả các đầu attention. Các thí nghiệm được tiến hành trên hai máy khác nhau, cả hai đều được trang bị GPU GeForce GTX 1080Ti. Mỗi thí nghiệm được lặp lại 3 lần trên mỗi máy (tổng cộng 6 điểm dữ liệu cho mỗi thiết lập). Chúng tôi thấy rằng cắt giảm một nửa số đầu của mô hình tăng tốc suy luận lên đến 17.5% cho batch size cao hơn (sự khác biệt này biến mất đối với batch size nhỏ hơn).

5 Khi nào Nhiều Đầu Quan trọng? Trường hợp Dịch máy

Như được thể hiện trong Bảng 2, không phải tất cả các lớp MHA đều có thể được giảm xuống một đầu attention duy nhất mà không ảnh hưởng đáng kể đến hiệu suất. Để có ý tưởng tốt hơn về mức độ mỗi phần của mô hình dịch dựa trên transformer phụ thuộc vào multi-headedness, chúng tôi lặp lại thí nghiệm cắt giảm heuristic từ §4 cho mỗi loại attention riêng biệt (Enc-Enc, Enc-Dec, và Dec-Dec).

Hình 4 cho thấy rằng hiệu suất giảm nhanh hơn nhiều khi các đầu được cắt giảm từ các lớp attention Enc-Dec. Đặc biệt, cắt giảm hơn 60% các đầu attention Enc-Dec sẽ dẫn đến suy giảm hiệu suất thảm khốc, trong khi các lớp self-attention của encoder và decoder vẫn có thể tạo ra các bản dịch hợp lý (với điểm BLEU khoảng 30) chỉ với 20% số đầu attention gốc. Nói cách khác, encoder-decoder attention phụ thuộc nhiều hơn vào multi-headedness so với self-attention.

6 Động lực của Tầm quan trọng Đầu trong Quá trình Huấn luyện

Các phần trước cho chúng ta biết rằng một số đầu quan trọng hơn những đầu khác trong các mô hình đã được huấn luyện. Để có thêm hiểu biết về động lực của tầm quan trọng đầu trong quá trình huấn luyện, chúng tôi thực hiện cùng thí nghiệm cắt giảm tăng dần của §4.2 tại mỗi epoch. Chúng tôi thực hiện thí nghiệm này trên một phiên bản nhỏ hơn của mô hình WMT (6 lớp và 8 đầu mỗi lớp), được huấn luyện cho dịch German to English trên dataset IWSLT 2014 nhỏ hơn Cettolo et al. (2015). Chúng tôi gọi mô hình này là IWSLT.

Hình 5 báo cáo, cho mỗi mức cắt giảm (theo bước tăng 10% — 0% tương ứng với mô hình gốc), sự tiến triển của điểm số mô hình (trên newstest2013) cho mỗi epoch. Để dễ đọc hơn, chúng tôi hiển thị các epoch trên thang logarithmic, và chỉ báo cáo điểm số mỗi 5 epoch sau epoch thứ 10). Để làm cho điểm số có thể so sánh được qua các epoch, trục Y báo cáo sự suy giảm tương đối của điểm BLEU so với mô hình không bị cắt giảm tại mỗi epoch. Đáng chú ý, chúng tôi thấy rằng có hai chế độ riêng biệt: trong các epoch rất sớm (đặc biệt là 1 và 2), hiệu suất giảm tuyến tính với tỷ lệ cắt giảm, tức là sự giảm tương đối về hiệu suất độc lập với Iₕ, cho thấy hầu hết các đầu ít nhiều quan trọng như nhau. Từ epoch 10 trở đi, có sự tập trung của các đầu không quan trọng có thể được cắt giảm trong khi vẫn duy trì trong vòng 85-90% điểm BLEU gốc (lên đến 40% tổng số đầu).

Điều này cho thấy rằng các đầu quan trọng được xác định sớm (nhưng không ngay lập tức) trong quá trình huấn luyện. Hai giai đoạn huấn luyện gợi nhớ đến phân tích của Shwartz-Ziv và Tishby (2017), theo đó việc huấn luyện mạng neural phân tách thành giai đoạn "minimization rủi ro thực nghiệm", nơi mô hình tối đa hóa thông tin tương hỗ của các biểu diễn trung gian với nhãn, và giai đoạn "nén" nơi thông tin tương hỗ với đầu vào được tối thiểu hóa. Một điều tra có nguyên tắc hơn về hiện tượng này được để lại cho công việc tương lai.

7 Công trình liên quan

Việc sử dụng cơ chế attention trong NLP và đặc biệt là dịch máy neural (NMT) có thể được truy nguyên từ Bahdanau et al. (2015) và Cho et al. (2014), và hầu hết các implementation đương thời đều dựa trên công thức từ Luong et al. (2015). Attention nhanh chóng được điều chỉnh (thành công) cho các tác vụ NLP khác, thường đạt được hiệu suất tốt nhất khi đó trong đọc hiểu (Cheng et al., 2016), suy luận ngôn ngữ tự nhiên (Parikh et al., 2016) hoặc tóm tắt trừu tượng (Paulus et al., 2017) để kể một vài. Multi-headed attention lần đầu được giới thiệu bởi Vaswani et al. (2017) cho NMT và phân tích cú pháp thành phần tiếng Anh, và sau đó được áp dụng cho transfer learning (Radford et al., 2018; Devlin et al., 2018), mô hình hóa ngôn ngữ (Dai et al., 2019; Radford et al., 2019), hoặc gán nhãn vai trò ngữ nghĩa (Strubell et al., 2018), trong số những cái khác.

Có một tài liệu phong phú về cắt giảm mạng neural đã được huấn luyện, có từ LeCun et al. (1990) và Hassibi và Stork (1993) vào đầu những năm 90 và được tái sinh sau sự xuất hiện của deep learning, với hai cách tiếp cận trực giao: cắt giảm "weight-by-weight" chi tiết (Han et al., 2015) và cắt giảm có cấu trúc (Anwar et al., 2017; Li et al., 2016; Molchanov et al., 2017), trong đó toàn bộ các phần của mô hình được cắt giảm. Trong NLP, cắt giảm có cấu trúc cho việc tự động định kích thước các mô hình ngôn ngữ feed-forward lần đầu được điều tra bởi Murray và Chiang (2015). Gần đây hơn, các cách tiếp cận cắt giảm chi tiết đã được phổ biến bởi See et al. (2016) và Kim và Rush (2016) (chủ yếu trên NMT).

Đồng thời với công việc của chúng tôi, Voita et al. (2019) đã đưa ra quan sát tương tự về multi-head attention. Cách tiếp cận của họ liên quan đến việc sử dụng LRP (Binder et al., 2016) để xác định các đầu quan trọng và xem xét các thuộc tính cụ thể như chú ý đến các vị trí liền kề, từ hiếm hoặc từ liên quan về mặt cú pháp. Họ đề xuất một cơ chế cắt giảm thay thế dựa trên việc thực hiện gradient descent trên các biến mask γₕ. Trong khi cách tiếp cận và kết quả của họ bổ sung cho bài báo này, nghiên cứu của chúng tôi cung cấp bằng chứng bổ sung về hiện tượng này ngoài NMT, cũng như phân tích về động lực huấn luyện của việc cắt giảm các đầu attention.

8 Kết luận

Chúng tôi đã quan sát thấy rằng MHA không phải lúc nào cũng tận dụng tính biểu đạt vượt trội về lý thuyết của nó so với attention cơ bản đến mức tối đa. Cụ thể, chúng tôi đã chứng minh rằng trong nhiều thiết lập khác nhau, một số đầu có thể được loại bỏ khỏi các mô hình transformer đã được huấn luyện mà không bị suy giảm có ý nghĩa thống kê về hiệu suất kiểm tra, và một số lớp có thể được giảm xuống chỉ một đầu. Ngoài ra, chúng tôi đã chỉ ra rằng trong các mô hình dịch máy, các lớp attention encoder-decoder phụ thuộc nhiều hơn vào multi-headedness so với các lớp self-attention, và cung cấp bằng chứng rằng tầm quan trọng tương đối của mỗi đầu được xác định trong các giai đoạn đầu của huấn luyện. Chúng tôi hy vọng rằng những quan sát này sẽ thúc đẩy hiểu biết của chúng ta về MHA và truyền cảm hứng cho các mô hình đầu tư tham số và attention của chúng hiệu quả hơn.

Lời cảm ơn

Các tác giả muốn gửi lời cảm ơn đến các phản biện ẩn danh vì phản hồi sâu sắc của họ. Chúng tôi cũng đặc biệt biết ơn Thomas Wolf từ Hugging Face, người mà những nỗ lực tái tạo độc lập đã cho phép chúng tôi tìm và sửa một lỗi trong các thí nghiệm so sánh tốc độ. Nghiên cứu này được hỗ trợ một phần bởi một món quà từ Facebook.
