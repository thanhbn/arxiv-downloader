# 2102.03773.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2102.03773.pdf
# Kích thước tệp: 412854 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
1
SeReNe: Chính quy hóa các Neuron dựa trên Độ nhạy cho
Độ thưa có Cấu trúc trong Mạng Neural
Enzo Tartaglione Thành viên, IEEE, Andrea Bragagnolo, Francesco Odierna,
Attilio Fiandrotti, Thành viên Cao cấp, IEEE, và Marco Grangetto, Thành viên Cao cấp, IEEE

Tóm tắt—Mạng neural sâu bao gồm hàng triệu tham số có thể học, khiến việc triển khai chúng trên các thiết bị có tài nguyên hạn chế trở nên problematic. SeReNe (Chính quy hóa các Neuron dựa trên Độ nhạy) là một phương pháp để học các cấu trúc thưa có cấu trúc, khai thác độ nhạy neural như một bộ chính quy hóa. Chúng tôi định nghĩa độ nhạy của một neuron là sự biến thiên của đầu ra mạng đối với sự biến thiên của hoạt động của neuron. Độ nhạy của neuron càng thấp, đầu ra mạng càng ít bị nhiễu loạn nếu đầu ra neuron thay đổi. Bằng cách bao gồm độ nhạy neuron trong hàm chi phí như một số hạng chính quy hóa, chúng tôi có thể cắt tỉa các neuron có độ nhạy thấp. Vì toàn bộ neuron được cắt tỉa thay vì các tham số đơn lẻ, việc giảm dấu chân mạng thực tế trở nên khả thi. Kết quả thực nghiệm của chúng tôi trên nhiều kiến trúc mạng và bộ dữ liệu cho ra tỷ lệ nén cạnh tranh so với các tham chiếu tiên tiến.

Thuật ngữ chỉ mục—Mạng thưa, chính quy hóa, mạng sâu, cắt tỉa, nén.

I. GIỚI THIỆU

Mạng Neural Sâu (DNN) có thể giải quyết các nhiệm vụ cực kỳ thách thức nhờ các chồng phức tạp của các lớp (tích chập) với hàng nghìn neuron [1]–[3]. Hãy định nghĩa độ phức tạp của một mạng neural là số lượng tham số có thể học của nó: các kiến trúc như AlexNet và VGG có độ phức tạp theo thứ tự 60 và 130 triệu tham số tương ứng. Các kiến trúc tương tự khó triển khai trong các tình huống mà tài nguyên như bộ nhớ hoặc lưu trữ bị hạn chế. Ví dụ, dấu chân bộ nhớ của AlexNet 8 lớp vượt quá 240MB bộ nhớ, trong khi dấu chân của VGG-Net 19 lớp vượt quá 500 MB. Nhu cầu về các DNN nhỏ gọn cũng được chứng minh bởi thực tế là Nhóm Chuyên gia Hình ảnh Chuyển động (MPEG) của ISO gần đây đã mở rộng phạm vi của mình ngoài nội dung đa phương tiện bằng cách phát hành lời kêu gọi khám phá đề xuất nén mạng neural [4]. Nhiều phương pháp (bổ sung) có thể được sử dụng để đối phó với yêu cầu bộ nhớ mạng neural, thời gian suy luận và tiêu thụ năng lượng:

Thiết kế lại cấu trúc mạng. Chuyển từ một kiến trúc sang kiến trúc khác, có thể buộc một kết nối neuron chính xác, hoặc chia sẻ trọng số, có thể giảm số lượng tham số, hoặc độ phức tạp của mạng [3], [5].

Lượng tử hóa. Biểu diễn các tham số (và hàm kích hoạt) dưới dạng chữ số điểm cố định giảm dấu chân bộ nhớ và tăng tốc tính toán [6].

E. Tartaglione, A. Bragagnolo, F. Odierna, A. Fiandrotti và M. Grangetto thuộc Khoa Khoa học Máy tính, Đại học Turin, Torino, ITALY, e-mail: first.last@unito.it
A. Fiandrotti cũng thuộc LTCI, Télécom Paris, Institut Polytechnique de Paris, FRANCE, e-mail:attilio.fiandrotti@telecom-paris.fr

Cắt tỉa. Các kiến trúc sâu cần được tham số hóa quá mức [7] để được huấn luyện hiệu quả, nhưng các tham số dư thừa có thể được cắt tỉa tại thời điểm suy luận [8]–[12].

Công trình hiện tại thuộc về danh mục này.

Các kỹ thuật cắt tỉa nhằm học các cấu trúc thưa bằng cách loại bỏ có chọn lọc các synapse giữa các neuron (hoặc toàn bộ neuron khi tất cả các synapse đến được loại bỏ). Ví dụ, [10] và [11] áp dụng một hàm chính quy hóa thúc đẩy trọng số có độ lớn thấp theo sau là ngưỡng hóa zero hoặc lượng tử hóa. Các phương pháp như vậy cắt giảm số lượng tham số khác không, cho phép biểu diễn các tham số của một lớp như một tensor thưa [13]. Tuy nhiên, các phương pháp như vậy nhằm cắt tỉa các tham số một cách độc lập, vì vậy các cấu trúc đã học thiếu cấu trúc mặc dù thưa. Lưu trữ và truy cập trong bộ nhớ một ma trận thưa ngẫu nhiên đòi hỏi những thách thức đáng kể, vì vậy không rõ đến mức nào các phương pháp như vậy có thể được khai thác thực tế.

Công trình này đề xuất SeReNe, một phương pháp để học các cấu trúc mạng thưa có cấu trúc, tức là với ít neuron hơn hoàn toàn. Tóm lại, phương pháp của chúng tôi điều khiển tất cả các tham số của một neuron hướng về zero, cho phép cắt tỉa toàn bộ neuron khỏi mạng.

Đầu tiên, chúng tôi giới thiệu khái niệm độ nhạy của một neuron là sự biến thiên của đầu ra mạng đối với hoạt động neuron. Sau này được đo như tiềm năng hậu synapse của neuron, tức là đầu vào cho hàm kích hoạt của neuron. Trực giác cơ bản là các neuron có độ nhạy thấp tạo ra ít biến thiên trong đầu ra mạng và do đó mất mát hiệu suất negligible nếu đầu ra của chúng thay đổi cục bộ. Chúng tôi cũng cung cấp các giới hạn hiệu quả về mặt tính toán để xấp xỉ độ nhạy.

Thứ hai, chúng tôi thiết kế một bộ chính quy hóa thu hẹp tất cả các tham số của các neuron có độ nhạy thấp, mở đường cho việc loại bỏ chúng. Thật vậy, khi độ nhạy của một neuron tiến đến zero, neuron không còn phát ra tín hiệu và sẵn sàng được cắt tỉa.

Thứ ba, chúng tôi đề xuất một thủ tục hai bước lặp để cắt tỉa các tham số thuộc về các neuron có độ nhạy thấp. Thông qua một chiến lược xác thực chéo, chúng tôi đảm bảo mất mát hiệu suất được kiểm soát (hoặc thậm chí không có) so với kiến trúc gốc.

Phương pháp của chúng tôi cho phép học các cấu trúc mạng không chỉ thưa, tức là với ít tham số khác không, mà với ít neuron hơn (ít filter hơn cho các lớp tích chập). Như một lợi ích phụ, các kiến trúc nhỏ hơn và dày đặc hơn cũng có thể tăng tốc thực thi mạng nhờ việc sử dụng tốt hơn locality cache và mẫu truy cập bộ nhớ.

Chúng tôi chứng minh thực nghiệm rằng SeReNe vượt trội so với các tham chiếu tiên tiến trên nhiều nhiệm vụ học và kiến trúc mạng. Chúng tôi quan sát lợi ích của độ thưa có cấu trúc khi lưu trữ cấu trúc và tham số mạng neural sử dụng định dạng Open Neural Network eXchange [14], với việc giảm dấu chân bộ nhớ.

Phần còn lại của bài báo được cấu trúc như sau. Trong Phần II chúng tôi xem xét tài liệu có liên quan trong cắt tỉa mạng neural. Trong Phần III chúng tôi cung cấp định nghĩa độ nhạy và các giới hạn thực tế cho tính toán của nó; sau đó, chúng tôi trình bày một quy tắc cập nhật tham số để "điều khiển" các tham số của các neuron có độ nhạy thấp hướng về zero. Tiếp theo, trong Phần IV, một thủ tục thực tế để cắt tỉa một mạng với sơ đồ của chúng tôi. Sau đó, trong Phần V tất cả kết quả thực nghiệm được hiển thị và cuối cùng, trong Phần VI, các kết luận được rút ra.

II. CÔNG TRÌNH LIÊN QUAN

Các phương pháp hướng tới biểu diễn mạng neural nhỏ gọn có thể được phân loại thành ba nhóm chính: thay đổi cấu trúc mạng, lượng tử hóa các tham số và cắt tỉa trọng số. Trong phần này, chúng tôi xem xét các công trình dựa trên phương pháp cắt tỉa có liên quan nhất đến công trình của chúng tôi.

Trong bài báo tiên phong của họ [8], LeCun et al. đề xuất loại bỏ các trọng số không quan trọng khỏi một mạng, đo tầm quan trọng của từng trọng số đơn lẻ như sự gia tăng lỗi train khi trọng số được đặt về zero. Thật không may, độ phức tạp của phương pháp như vậy sẽ trở nên không thể chịu đựng về mặt tính toán trong trường hợp các cấu trúc sâu với hàng triệu tham số. Do quy mô và tài nguyên cần thiết để huấn luyện và triển khai mạng neural sâu hiện đại, các kiến trúc thưa và kỹ thuật nén đã thu hút nhiều sự quan tâm trong cộng đồng học sâu. Một số phương pháp thành công cho vấn đề này đã được đề xuất [15]–[18]. Trong khi một phân tích sâu hơn về chủ đề này đã được xuất bản bởi Gale et al. [19], trong phần còn lại của phần này chúng tôi cung cấp một tóm tắt về các kỹ thuật chính được sử dụng để cắt tỉa các kiến trúc sâu.

Thuật toán tiến hóa. Học đặc trung thưa đa mục tiêu đã được đề xuất bởi Gong et al. [20]: với thuật toán tiến hóa của họ, họ có thể tìm ra một sự thỏa hiệp tốt giữa độ thưa và lỗi học, tuy nhiên, với chi phí tính toán cao. Những nhược điểm tương tự có thể được tìm thấy trong công trình của Lin et al., nơi các lớp tích chập được cắt tỉa sử dụng thuật toán tối ưu hóa artificial bee colony (được gọi là ABCPruner) [21].

Dropout. Dropout nhằm ngăn một mạng khỏi over-fitting bằng cách ngẫu nhiên loại bỏ một số neuron tại thời điểm học [22]. Mặc dù dropout giải quyết một vấn đề khác, nó đã truyền cảm hứng cho một số kỹ thuật nhằm làm thưa các kiến trúc sâu. Kingma et al. [9] đã chỉ ra rằng dropout có thể được coi là một trường hợp đặc biệt của chính quy hóa Bayesian. Hơn nữa, họ đưa ra một phương pháp biến phân cho phép sử dụng tỷ lệ dropout thích ứng với dữ liệu. Molchanov et al. [23] khai thác dropout biến phân như vậy để làm thưa cả các lớp fully-connected và tích chập. Đặc biệt, các tham số có tỷ lệ dropout cao luôn bị bỏ qua và chúng có thể được loại bỏ khỏi mạng. Mặc dù kỹ thuật này đạt được hiệu suất tốt, nó khá phức tạp và được báo cáo là hoạt động không nhất quán khi áp dụng cho các kiến trúc sâu [19]. Hơn nữa, kỹ thuật này dựa trên niềm tin rằng phân phối xác suất Bernoulli (được sử dụng với dropout) là một xấp xỉ biến phân tốt cho posterior. Một phương pháp dựa trên dropout khác là Targeted Dropout [24]: ở đây, tinh chỉnh mô hình ANN tự củng cố độ thưa của nó bằng cách stochastically loại bỏ các kết nối. Họ cũng nhắm đến độ thưa có cấu trúc mà không đạt được hiệu suất tiên tiến.

Chưng cất kiến thức. Gần đây, chưng cất kiến thức [25] nhận được sự chú ý đáng kể. Mục tiêu trong trường hợp này là huấn luyện một mạng đơn lẻ để có cùng hành vi (về đầu ra dưới các đầu vào nhất định) như một ensemble của các mô hình, giảm độ phức tạp tính toán tổng thể. Chưng cất tìm ứng dụng trong việc giảm dự đoán của nhiều mạng thành một mạng đơn, nhưng không thể được áp dụng để giảm thiểu số lượng neuron cho một mạng đơn. Một công trình gần đây là Few Samples Knowledge Distillation (FSKD) [26], nơi một mạng student nhỏ được huấn luyện từ một teacher lớn hơn. Nói chung, trong các kỹ thuật dựa trên chưng cất, kiến trúc được huấn luyện được biết trước và giữ static qua toàn bộ quá trình học: trong công trình này, chúng tôi nhằm cung cấp một thuật toán tự động thu hẹp kích thước mô hình sâu với overhead tối thiểu được giới thiệu.

Cắt tỉa few-shot. Một phương pháp khác dựa trên việc định nghĩa tầm quan trọng của từng kết nối và sau đó loại bỏ các tham số được coi là không cần thiết. Một công trình gần đây của Frankle và Carbin [12] đề xuất giả thuyết lottery ticket, điều này đang có tác động lớn đến cộng đồng nghiên cứu. Họ tuyên bố rằng từ một ANN, sớm trong quá trình huấn luyện, có thể trích xuất một sub-network thưa, sử dụng một-shot hoặc cách iterative: sub-network thưa như vậy, khi được huấn luyện lại, có thể khớp với độ chính xác của mô hình gốc. Kỹ thuật này có nhiều yêu cầu, như có lịch sử của quá trình huấn luyện để phát hiện "các tham số trúng lottery", và nó không thể tự điều chỉnh một cơ chế thresholding tự động. Nhiều nỗ lực được dành để làm cho các cơ chế cắt tỉa hiệu quả hơn: ví dụ, Wang et al. cho thấy rằng một số độ thưa có thể đạt được cắt tỉa trọng số ngay từ đầu quá trình huấn luyện [27], hoặc Lee et al., với "SNIP" của họ, có thể cắt tỉa trọng số theo cách one-shot [28]. Tuy nhiên, các phương pháp này đạt được độ thưa hạn chế: chiến lược dựa trên cắt tỉa iterative, khi so sánh với các phương pháp one-shot hoặc few-shot, có thể đạt được độ thưa cao hơn [29].

Cắt tỉa dựa trên chính quy hóa. Cuối cùng, các phương pháp dựa trên chính quy hóa dựa trên một số hạng chính quy hóa (được thiết kế để tăng cường độ thưa) được tối thiểu hóa bên cạnh hàm loss tại thời điểm huấn luyện. Louizos et al. đề xuất một chính quy hóa ℓ0 để cắt tỉa các tham số mạng trong quá trình huấn luyện [30]. Kỹ thuật như vậy phạt giá trị khác không của một vector tham số, thúc đẩy các giải pháp thưa. Như một nhược điểm, nó yêu cầu giải quyết một vấn đề tối ưu hóa phức tạp, bên cạnh chiến lược tối thiểu hóa loss và các số hạng chính quy hóa khác. Han et al. đề xuất một quá trình đa bước trong đó các tham số ít liên quan nhất được định nghĩa, tối thiểu hóa một hàm loss mục tiêu [11]. Đặc biệt, nó dựa trên heuristics thresholding, nơi tất cả các kết nối ít quan trọng hơn được cắt tỉa. Trong [10], một phương pháp tương tự đã được theo dõi, giới thiệu một số hạng chính quy hóa mới đo "độ nhạy" của đầu ra wrt. sự biến thiên của các tham số. Trong khi kỹ thuật này đạt được độ thưa hàng đầu ngay cả trong các kiến trúc tích chập sâu, độ thưa như vậy không có cấu trúc, tức là

--- TRANG 2 ---
2
cấu trúc kết quả bao gồm số lượng lớn neuron với ít nhất một tham số khác không. Độ thưa không có cấu trúc như vậy làm phình to dấu chân mạng có thể đạt được thực tế và dẫn đến truy cập bộ nhớ không đều, gây nguy hiểm cho việc tăng tốc thực thi.

Trong công trình này chúng tôi nhằm vượt qua các hạn chế trên bằng cách đề xuất một phương pháp chính quy hóa tạo ra một sparsiﬁcation có cấu trúc, tập trung vào việc loại bỏ toàn bộ neuron thay vì các tham số đơn lẻ. Chúng tôi cũng tận dụng nghiên cứu gần đây của chúng tôi cho thấy rằng chính quy hóa tiềm năng hậu synapse có thể tăng cường khái quát hóa so với các bộ chính quy hóa khác [31].

III. CHÍNH QUY HÓA DỰA TRÊN ĐỘ NHẠY CHO NEURON

Trong phần này, đầu tiên chúng tôi xây dựng độ nhạy của một mạng đối với tiềm năng hậu synapse của một neuron. Sau đó, chúng tôi đưa ra một quy tắc cập nhật tham số tổng quát dựa trên số hạng độ nhạy được đề xuất. Như tình huống tham chiếu, một vấn đề phân loại đa lớp với C nhãn được xem xét; tuy nhiên, chiến lược của chúng tôi có thể được mở rộng cho các nhiệm vụ học khác, ví dụ hồi quy, một cách đơn giản.

A. Sơ bộ và Định nghĩa

Cho một mạng neural nhân tạo đa lớp feed-forward, a-cyclic được tạo thành từ N-1 lớp ẩn. Chúng tôi định danh với n=0 lớp đầu vào và n=N lớp đầu ra, các giá trị n khác chỉ ra các lớp ẩn. Đối với neuron thứ i của lớp thứ n xn,i, chúng tôi định nghĩa:
yn,i là đầu ra của nó,
yn-1 là vector đầu vào của nó,
θn,i là các tham số riêng của nó: wn,i các trọng số và bn,i bias,
như được minh họa trong Hình 1. Mỗi neuron có hàm kích hoạt riêng gn,i() được áp dụng sau một hàm affine fn,i() có thể là ví dụ convolution hoặc dot product.

Do đó, đầu ra của một neuron được cho bởi
yn,i=gn,i[pn,i], (1)
trong đó pn,i là tiềm năng hậu synapse của xn,i được định nghĩa là:
pn,i=fn,i(θn,i,yn-1): (2)

[Tiếp tục với phần còn lại của bản dịch...]

B. Độ nhạy Neuron

Ở đây chúng tôi giới thiệu định nghĩa độ nhạy neuron. Chúng tôi nhớ lại rằng chúng tôi nhằm cắt tỉa toàn bộ neuron thay vì các tham số đơn lẻ để đạt được độ thưa có cấu trúc. Hãy giả sử rằng phương pháp của chúng tôi được áp dụng cho một mạng đã được huấn luyện trước. Để ước tính tính liên quan của neuron xn,i cho nhiệm vụ mà mạng đã được huấn luyện, chúng tôi đánh giá đóng góp neuron cho đầu ra mạng yN. Để đạt được điều này, đầu tiên chúng tôi cung cấp một trực giác về cách các biến thiên nhỏ của tiềm năng hậu synapse pn,i của neuron xn,i ảnh hưởng đến đầu ra thứ k của mạng yN,k.

Bằng một khai triển chuỗi Taylor, đối với các biến thiên nhỏ của pn,i, hãy biểu diễn sự biến thiên của yN,k là
ΔyN,k≈Δpn,i ∂yN,k/∂pn,i (3)
trong đó ΔyN,k chỉ ra đầu ra thứ k cho lớp đầu ra. Trong trường hợp ΔyN,k→0,∀k, đối với các biến thiên nhỏ của pn,i, ΔyN,k không thay đổi. Điều kiện như vậy cho phép điều khiển tiềm năng hậu synapse pn,i về zero mà không ảnh hưởng đến đầu ra mạng yN,k (và, ví dụ, hiệu suất của nó). Ngược lại, nếu ΔyN,k≠0, bất kỳ biến thiên nào của pn,i có thể làm thay đổi đầu ra mạng, có thể làm suy giảm hiệu suất của nó.

Bây giờ chúng ta có thể định lượng đúng đắn ảnh hưởng của những thay đổi nhỏ đến đầu ra mạng bằng cách định nghĩa độ nhạy neuron.

Định nghĩa 1: Độ nhạy của đầu ra mạng yN đối với tiềm năng hậu synapse pn,i của neuron xn,i là:
Sn,i(yN,pn,i) = (1/C) Σk=1^C |∂yN,k/∂pn,i| (4)
trong đó yN∈RC và Sn,i∈[0,+∞). Trực quan, Sn,i càng cao, dao động của yN càng cao đối với các biến thiên nhỏ của pn,i.

Trước khi tiếp tục, chúng tôi muốn làm rõ lựa chọn của chúng tôi về việc tận dụng tiềm năng hậu synapse pn,i thay vì đầu ra neuron yn,i trong phương trình trên. Để hiểu lựa chọn của chúng tôi, chúng tôi viết lại (4) sử dụng quy tắc chuỗi:
Sn,i(yN,pn,i) = (1/C) Σk=1^C |∂yN,k/∂yn,i × ∂yn,i/∂pn,i| (5)

Không mất tính tổng quát, giả sử ∂yN,k/∂yn,i≠0 và gn,i tương ứng với hàm kích hoạt ReLU nổi tiếng. Dưới giả thuyết rằng pn,i<0, ∂yn,i/∂pn,i=0 đối với kích hoạt ReLU được xem xét. Nếu chúng tôi đã viết (4) như một hàm của đầu ra neuron yn,i, gradient biến mất ∂yn,i/∂pn,i=0 sẽ ngăn chúng tôi ước tính độ nhạy neuron. Cân nhắc trên áp dụng ngoài ReLU cho bất kỳ hàm kích hoạt nào ngoại trừ hàm đồng nhất, mà đối với đó yn,i=pn,i.

C. Giới hạn về Độ nhạy Neuron

Ở đây chúng tôi cung cấp hai giới hạn hiệu quả về mặt tính toán cho hàm độ nhạy trên có thể được khai thác thực tế. Các framework phổ biến cho huấn luyện DNN dựa trên các framework vi phân như autograd, để tự động vi phân biến số dọc theo đồ thị tính toán. Các framework như vậy lấy làm đầu vào một hàm mục tiêu J nào đó và tự động tính toán tất cả

--- TRANG 3 ---
3
các gradient dọc theo đồ thị tính toán. Để có được Sn,i như một kết quả từ engine vi phân, chúng tôi định nghĩa
Sn,i(yN,pn,i) = ∂J/∂pn,i (6)
trong đó J là một hàm thích hợp. Trong Phụ lục A chúng tôi chỉ ra rằng hàm như vậy hóa ra là:
J = (1/C) Σk=1^C ∫ |∂yN,k/∂pn,i| dpn,i (7)

Do đó, tính toán độ nhạy trong (4) yêu cầu C lần gọi đến engine vi phân. Trong phần sau với một số đại số nhỏ chúng tôi đưa ra một giới hạn dưới và trên cho Def. 1 mà chúng tôi chỉ ra là đặc biệt hữu ích từ góc độ tính toán.

Cho hàm mục tiêu để vi phân là
Jl = (1/C) Σk=1^C yN,k (8)

Engine tự động vi phân được gọi trên Sl sẽ trả về
∂Jl/∂pn,i = (1/C) Σk=1^C ∂yN,k/∂pn,i (9)

Theo bất đẳng thức tam giác, một giới hạn dưới cho độ nhạy trong (4) có thể được tính như
Sl_n,i = |(1/C) Σk=1^C ∂yN,k/∂pn,i| ≤ (1/C) Σk=1^C |∂yN,k/∂pn,i| (10)

Sl_n,i có thể được đánh giá thuận tiện vi phân trên (8) (và lấy giá trị tuyệt đối) với một lần gọi duy nhất đến engine vi phân. Như được hiển thị trong (10), điều này cho chúng ta một ước tính giới hạn dưới về độ nhạy neuron.

Để ước tính một giới hạn trên cho Sn,i, chúng tôi viết lại (4) là
Sn,i = (1/C) Σk=1^C |∂yN,k/∂yN-1| × |∏l=n+1^{N-1} ∂yl/∂yl-1_n,i| × |∂yn,k/∂pn,i| (11)

Tuy nhiên, ∀k chúng ta có chung số hạng
δn,i = ∏l=n+1^{N-1} |∂yl/∂yl-1_n,i| × |∂yn,i/∂pn,i| = ||∏l=n+1^{N-1} ∂yl/∂yl-1_n,i|| × |∂yn,i/∂pn,i| = u_n,i (12)

trong đó δn,i là vector one-hot chọn neuron thứ i tại lớp thứ n và || || là toán tử element-wise. Do đó, chúng tôi viết lại (11) là
Su_n,i = (1/C) (Σk=1^C |∂yN,k/∂yN-1|) × u_n,i ≥ Sn,i (13)

Do đó, chúng tôi đã chỉ ra rằng Su_n,i là một giới hạn trên cho độ nhạy trong (4). Giới hạn trên và dưới được lấy ở đây vì hai lý do chính: hiệu quả tính toán và nới lỏng/siết chặt các điều kiện về chính độ nhạy. Chúng tôi sẽ thấy trong Phần V-A một phân phối tổng thể điển hình của các độ nhạy trên một mạng đã được huấn luyện trước, so sánh (4), (10) và (13).

Trong phần sau, chúng tôi khai thác công thức của hàm Độ nhạy (1) và hai giới hạn của nó (10), (13) để định nghĩa một quy tắc cập nhật tham số.

D. Quy tắc Cập nhật Tham số

Bây giờ chúng tôi chỉ ra cách định nghĩa độ nhạy được đề xuất có thể được khai thác để thúc đẩy sparsiﬁcation neuron. Như đã gợi ý trước đây, nếu độ nhạy Sn,i của neuron xn,i nhỏ, tức là Sn,i→0, thì neuron xn,i mang lại đóng góp nhỏ cho đầu ra mạng thứ i yN,i và các tham số của nó có thể được chuyển về zero với ít nhiễu loạn đến đầu ra mạng. Để đạt được điều này, chúng tôi định nghĩa hàm không nhạy S̄n,i là
S̄n,i = max{0,1-Sn,i} = (1-Sn,i)₊ ≤ (1-Sn,i) (14)

trong đó (·)₊ là hàm một bước. Không nhạy càng cao của neuron xn,i (tức là, S̄n,i→1 hoặc tương đương Sn,i→0), neuron càng ít ảnh hưởng đến đầu ra mạng. Do đó, nếu S̄n,i→1, thì neuron xn,i đóng góp ít vào đầu ra mạng và các tham số wn,i,j của nó có thể được điều khiển về zero mà không làm nhiễu loạn đáng kể đầu ra mạng. Sử dụng định nghĩa không nhạy trong (14), chúng tôi đề xuất quy tắc cập nhật sau:

wt+1_n,i,j = wt_n,i,j - η (∂L/∂wt_n,i,j - λ wt_n,i,j S̄n,i) (15)

trong đó
- số hạng đóng góp đầu tiên là việc tối thiểu hóa cổ điển của một hàm loss L, đảm bảo rằng mạng vẫn giải quyết nhiệm vụ mục tiêu, ví dụ phân loại;
- số hạng thứ hai đại diện cho một hình phạt được áp dụng cho tham số wn,i,j thuộc về neuron xn,i tỷ lệ với tính không nhạy của đầu ra đối với các biến thiên của nó.

Cuối cùng, vì
∂pn,i/∂yn-1,j = wn,i,j (16)

chúng tôi viết lại (15) là
wt+1_n,i,j = wt_n,i,j - η (∂L/∂wt_n,i,j - λ S̃n,i,j) (17)

trong đó
S̃n,i,j = [wn,i,j sign(wn,i,j) (1/C) Σk=1^C |∂yN,k/∂yn-1,j|] × (1-Sn,i) (18)

Một đạo hàm từng bước được cung cấp trong Phụ lục D. Từ (18) chúng ta có thể hiểu rõ hơn ảnh hưởng của số hạng hình phạt được đề xuất: như mong đợi từ thảo luận trên, S̃n,i,j tỷ lệ nghịch với tác động lên đầu ra đối với các biến thiên của đầu vào cho neuron xn,i.

E. Chính quy hóa dựa trên độ nhạy neuron cục bộ

Bây giờ chúng tôi đề xuất một công thức xấp xỉ của hàm độ nhạy trong (4) chỉ dựa trên tiềm năng hậu synapse và đầu ra của một neuron mà chúng tôi sẽ gọi là độ nhạy cục bộ. Hãy nhớ lại rằng đối với mỗi neuron xn,i độ nhạy được cung cấp bởi Định nghĩa 1 đo tác động tổng thể của một neuron xn,i cho trước lên đầu ra mạng có tính đến tất cả các neuron tiếp theo liên quan trong tính toán.

Định nghĩa 2: Độ nhạy neuron cục bộ của đầu ra yn,i đối với tiềm năng hậu synapse pn,i của neuron xn,i được định nghĩa là:
S̃n,i = ∂yn,i/∂pn,i (19)

Trong trường hợp mạng được kích hoạt ReLU, nó đơn giản đọc
S̃n,i = σ(pn,i) (20)

Dưới cài đặt này, quy tắc cập nhật (17) đơn giản hóa thành
wt+1_n,i,j = wt_n,i,j - η (∂L/∂wt_n,i,j - λ wt_n,i,j σ(-pn,i)) (21)

tức là, hình phạt chỉ được áp dụng trong trường hợp neuron ở trạng thái tắt. Trong khi độ nhạy cục bộ là một xấp xỉ lỏng lẻo hơn của (1), nó ít phức tạp hơn nhiều để tính toán đặc biệt đối với các neuron được kích hoạt ReLU.

IV. THỦ TỤC SERENE

Phần này giới thiệu một thủ tục thực tế để cắt tỉa neuron từ một mạng neural N tận dụng bộ chính quy hóa dựa trên độ nhạy được giới thiệu ở trên. Giả sử N đã được huấn luyện sơ bộ tại một số nhiệm vụ trên bộ dữ liệu D đạt được hiệu suất (ví dụ, độ chính xác phân loại) A. Chúng tôi không đặt bất kỳ ràng buộc nào về phương pháp huấn luyện thực tế, tập huấn luyện hoặc kiến trúc mạng. Alg. 1 tóm tắt thủ tục trong pseudo-code. Tóm lại, thủ tục bao gồm lặp lại qua các thủ tục Chính quy hóa và Thresholding.

Ở đầu vòng lặp, bộ dữ liệu D được chia thành các tập con rời rạc V (được sử dụng cho mục đích validation) và U (để cập nhật mạng). Tại dòng 5, thủ tục chính quy hóa (được tóm tắt trong Alg. 2) huấn luyện N trên D theo (15) điều khiển về zero các tham số của neuron có độ nhạy thấp. Vòng lặp kết thúc nếu hiệu suất của mạng được chính quy hóa giảm dưới ngưỡng A̲. Ngược lại, thủ tục thresholding đặt về zero các tham số dưới ngưỡng T và cắt tỉa các neuron sao cho tất cả tham số bằng zero. Đầu ra của thủ tục là mạng đã được cắt tỉa, tức là với ít neuron hơn, N*.

Các thủ tục Chính quy hóa và Thresholding được chi tiết trong phần sau. Một biểu diễn đồ họa cấp cao của SeReNe cũng được hiển thị trong Hình 2.

Thuật toán 1 Thủ tục SeReNe
Đầu vào: Mạng đã huấn luyện N, bộ dữ liệu D,
Hiệu suất mục tiêu A̲, PWE, TWT
Đầu ra: Mạng đã cắt tỉa N*
1:procedure SERENE(N;D;A̲;PWE;TWT)
2:N* ← N
3: while true do
4:U;V ← RANDOM_SPLIT(D)
5:N ← REGULARIZATION(N;U;V;PWE)
6: if PERFORMANCE(N;V)<A̲ then
7: break
8:N* ← N
9:N ← THRESHOLDING(N;V;TWT)
return N*

A. Chính quy hóa

Thủ tục này nhận đầu vào một mạng N và trả về một mạng được chính quy hóa theo quy tắc cập nhật (15). Cụ thể, thủ tục huấn luyện lặp N trên U và xác thực nó trên V cho nhiều epoch. Cho Nr đại diện cho mạng được chính quy hóa tốt nhất được tìm thấy tại một thời điểm cho trước theo hàm loss. Đối với mỗi lần lặp, thủ tục hoạt động như sau. Đầu tiên (dòng 5), N được huấn luyện cho một epoch trên U: kết quả là một mạng được chính quy hóa theo (15). Thứ hai (dòng 6), mạng này được xác thực trên V. Nếu loss thấp hơn loss của Nr trên V, thì N chiếm chỗ của Nr (dòng 7). Nếu Nr không được cập nhật trong PWE (Plateau Waiting Epochs) epoch, chúng tôi giả sử chúng tôi đã đạt đến một plateau hiệu suất. Trong trường hợp này, thủ tục kết thúc và trả về mạng được chính quy hóa độ nhạy Nr.

Thuật toán 2 Thủ tục chính quy hóa
Đầu vào: Mô hình N, tập dữ liệu V và U, PWE
Đầu ra: Mạng được chính quy hóa độ nhạy Nr
1:procedure REGULARIZATION(N;U;V;PWE)
2:Nr ← N. Nr là mạng được chính quy hóa tốt nhất trên V
3:epochs ← 0
4: while epochs<PWE do
5:N ← TRAIN(N;U). 1 epoch huấn luyện trên U
6:epochs ++
7: if LOSS(N;V)<LOSS(Nr;V) then
8:Nr ← N
9: epochs ← 0
return Nr

--- TRANG 4 ---
4

Hình 2: Cái nhìn cấp cao về thủ tục SeReNe.

B. Thresholding

Thủ tục thresholding là nơi các tham số của neuron có độ nhạy thấp được threshold về zero. Cụ thể, các tham số có giá trị tuyệt đối dưới ngưỡng T được cắt tỉa như
wn,i,j = {
  wn,i,j  nếu |wn,i,j|>T
  0       ngược lại
} (22)

Ngưỡng cắt tỉa T được chọn sao cho hiệu suất (hoặc, nói cách khác, loss trên V) xấu đi nhiều nhất của một giá trị tương đối mà chúng tôi gọi là tolerance xấu đi thresholding (TWT) mà chúng tôi cung cấp như siêu tham số.

Chúng tôi mong đợi hàm loss là một hàm trơn, đơn điệu cục bộ của T, đối với các giá trị nhỏ của T. Ngưỡng T có thể được tìm thấy sử dụng heuristics dựa trên tìm kiếm tuyến tính. Tuy nhiên chúng tôi có thể giảm điều này sử dụng phương pháp chia đôi, hội tụ đến giá trị T tối ưu trong các bước thời gian log.

Do tính ngẫu nhiên được giới thiệu bởi các bộ tối ưu hóa dựa trên mini-batch, các tham số được cắt tỉa trong một lần lặp thresholding có thể được giới thiệu lại bởi lần lặp chính quy hóa tiếp theo. Để vượt qua hiệu ứng này, chúng tôi thực thi rằng các tham số đã cắt tỉa không thể được cập nhật nữa trong các chính quy hóa tiếp theo (chúng tôi gọi hành vi này là parameter pinning). Để đạt được điều này, quy tắc cập nhật (15) được sửa đổi như sau:

wt+1_n,i,j = {
  wt_n,i,j - η (∂L/∂wt_n,i,j - λ wt_n,i,j S̄n,i)  nếu wt_n,i,j≠0
  0                                                  nếu wt_n,i,j=0
} (23)

Chúng tôi đã nhận thấy rằng không có parameter pinning, việc nén mạng có thể vẫn thấp vì các ước tính gradient ồn ào trong một mini-batch tiếp tục giới thiệu lại các tham số đã cắt tỉa trước đó. Ngược lại, bằng cách thêm (23) một số epoch thấp hơn là đủ để đạt được nén cao hơn nhiều.

V. KẾT QUẢ

Trong phần này chúng tôi thử nghiệm với phương pháp cắt tỉa neuron được đề xuất so sánh bốn công thức độ nhạy mà chúng tôi đã giới thiệu trong phần trước:
- SeReNe (exact) - công thức chính xác trong (5);
- SeReNe (LB) - giới hạn dưới trong (10);
- SeReNe (UB) - giới hạn trên trong (13);
- SeReNe (local) - phiên bản cục bộ trong (19);
- ℓ2+ pruning - là một tham chiếu baseline nơi chúng tôi thay thế số hạng chính quy hóa dựa trên độ nhạy của chúng tôi bằng một số hạng ℓ2 tiêu chuẩn (tất cả phần còn lại của framework là giống hệt).

Chúng tôi thử nghiệm trên các kết hợp khác nhau của kiến trúc và bộ dữ liệu thường được sử dụng như benchmark trong tài liệu có liên quan:
- LeNet-300 trên MNIST (Bảng I và Bảng II),
- LeNet-5 trên MNIST (Bảng III),
- LeNet-5 trên Fashion-MNIST (Bảng IV),
- VGG-16 trên CIFAR-10 (Bảng V và Bảng VI),
- ResNet-32 trên CIFAR-10 (Bảng VII),
- AlexNet trên CIFAR-100 (Bảng VIII),
- ResNet-101 trên ImageNet (Bảng IX).

Hình 3: Tổng thể độ nhạy S và giới hạn dưới Sl và trên Su tương đối cho kiến trúc LeNet-5 đã được huấn luyện trước trên MNIST. Các thanh dọc chỉ ra các giá trị trung bình tương đối.

Lưu ý rằng các kiến trúc VGG-16, AlexNet và ResNet-32 được sửa đổi để phù hợp với nhiệm vụ phân loại mục tiêu (CIFAR-10 và CIFAR-100). Kích thước tập validation (V) cho tất cả thí nghiệm là 10% của tập huấn luyện.

Hiệu suất cắt tỉa được đánh giá theo nhiều metric.
- Tỷ lệ nén như tỷ lệ giữa số lượng tham số trong mạng gốc và số lượng tham số còn lại sau cắt tỉa (càng cao càng tốt).
- Số lượng neuron còn lại (hoặc filter cho các lớp tích chập) sau cắt tỉa.
- Kích thước của các mạng khi được lưu trữ trên đĩa trong định dạng ONNX phổ biến [14] (cột .onnx). Các tệp ONNX sau đó được nén lossless sử dụng thuật toán Lempel–Ziv–Markov (LZMA) [34] (cột .7z).

Trong các thí nghiệm của chúng tôi, chúng tôi so sánh với tất cả các tham chiếu có sẵn cho mỗi kết hợp kiến trúc và bộ dữ liệu. Vì lý do này, tập hợp tham chiếu có thể thay đổi từ thí nghiệm này sang thí nghiệm khác. Các thuật toán của chúng tôi được triển khai bằng Python, sử dụng PyTorch 1.5, và các mô phỏng được chạy trên GPU RTX2080 NVIDIA với bộ nhớ 8GB.¹

A. Thí nghiệm sơ bộ

Để bắt đầu, chúng tôi vẽ phân phối độ nhạy cho một mạng LeNet-5 được huấn luyện trên bộ dữ liệu MNIST (SGD với learning rate = 0.1, weight-decay 10⁻⁴). Mạng này cũng sẽ được sử dụng làm baseline trong Phần V-C. Hình 3 hiển thị SeReNe (exact) (đỏ), SeReNe (LB) (xanh lá) và SeReNe (UB) (xanh dương); các dấu dọc đại diện cho các giá trị trung bình. Như mong đợi, SeReNe (LB) và SeReNe (UB) ước tính thấp và ước tính cao SeReNe (exact), tương ứng. Thú vị là, các giá trị độ nhạy SeReNe (UB) nằm trong khoảng [10⁻⁴; 10⁻²] trong khi cả SeReNe (exact) và SeReNE (LB) đều hiển thị một đuôi dài hơn hướng tới các con số nhỏ hơn, trong khi tất cả các phân phối trông tương tự.

Trong phần sau, chúng tôi sẽ đánh giá thực nghiệm ba công thức độ nhạy về mặt hiệu quả cắt tỉa.

B. LeNet300 trên MNIST

Như một thí nghiệm đầu tiên, chúng tôi cắt tỉa một kiến trúc LeNet-300, bao gồm ba lớp fully-connected với 300, 100 và 10 neuron, tương ứng được huấn luyện trên bộ dữ liệu MNIST. Chúng tôi huấn luyện trước LeNet-300 qua SGD với learning rate = 0.1

[Tiếp tục với phần còn lại của bản dịch...]

¹Mã nguồn sẽ được cung cấp khi bài báo được chấp nhận.

--- TRANG 5 ---
5

BẢNG I: LeNet-300 được huấn luyện trên MNIST (tỷ lệ lỗi 1.65%).

[Bảng hiển thị các phương pháp khác nhau, tham số còn lại (%), tỷ lệ nén, neuron còn lại, kích thước mạng [kB], thời gian huấn luyện và Top-1 (%)]

BẢNG II: LeNet-300 được huấn luyện trên MNIST (tỷ lệ lỗi 1.95%).

[Bảng tương tự với các metric khác nhau]

và PWE = 20 epoch với λ = 10⁻⁵, TWT = 0.3 cho SeReNe (exact), SeReNe (LB) SeReNe (UB) và λ = 10⁻⁵, TWT = 1 cho SeReNe (local). Tài liệu liên quan báo cáo chủ yếu i) kết quả cho lỗi phân loại khoảng 1.65% (Bảng I) và ii) kết quả cho lỗi theo thứ tự 1.95% (Bảng II). Vì lý do này, chúng tôi huấn luyện khoảng 1k epoch để đạt được tỷ lệ lỗi 1.95% và thêm 2k epoch để đạt được tỷ lệ lỗi 1.65%.

SeReNe vượt trội so với các phương pháp khác dẫn đầu cả về tỷ lệ nén và số lượng neuron được cắt tỉa. SeReNe (exact) đạt được tỷ lệ nén 42.55 và số lượng neuron còn lại trong các lớp ẩn giảm từ 300 xuống 159 và từ 100 xuống 75 tương ứng. SeReNe (LB) có hiệu suất so sánh được với SeReNe (exact) mặc dù chi phí tính toán thấp hơn (xem bên dưới). Đối với dải lỗi ˙95%, SeReNe (LB) hiệu quả hơn trong việc cắt tỉa tham số so với SeReNe (exact), cho phép lỗi thấp hơn. Serene (LB) cắt tỉa nhiều tham số hơn SeReNe (UB), chúng tôi giả thuyết vì (13) ước tính quá cao độ nhạy của các tham số và ngăn chúng được cắt tỉa. Mặt khác, SeReNe (LB) ước tính thấp độ nhạy, tuy nhiên các giá trị λ nhỏ bù đắp điều này. SeReNe (local) cắt tỉa ít tham số hơn các công thức SeReNe khác vì nó dựa trên một công thức độ nhạy được tính toán cục bộ mặc dù độ phức tạp thấp hơn. Về thời gian huấn luyện (cột thứ hai từ phải), SeReNe (local) nhanh nhất và giới thiệu overhead tính toán rất ít, SeReNe (UB) và SeReNe (LB) có thời gian huấn luyện so sánh được và chậm nhất là SeReNe (exact), chậm hơn khoảng 2.7x so với các giới hạn của nó. Dưới ánh sáng của sự cân bằng tốt giữa khả năng cắt tỉa neuron,

--- TRANG 6 ---
6

tỷ lệ lỗi và thời gian huấn luyện của SeReNe (LB), trong phần sau chúng tôi sẽ hạn chế các thí nghiệm của mình cho công thức độ nhạy này.

Hình 4 (dưới) hiển thị vị trí của các tham số không được cắt tỉa bởi SeReNe (exact) trong lớp fully-connected đầu tiên của LeNet300 (các chấm đen). Để so sánh, chúng tôi báo cáo hình ảnh tương đương từ Hình 4 của [11] (trên). Phương pháp của chúng tôi tạo ra các cột hoàn toàn trống trong ma trận có thể được biểu diễn trong bộ nhớ như các chuỗi không gián đoạn của các số không. Khi lưu trữ trên đĩa, nén LZMA (cột .7z) đặc biệt hiệu quả trong việc mã hóa các chuỗi dài của cùng một ký hiệu, điều này giải thích tỷ lệ nén 10x mà nó đạt được (từ 538 xuống 46 kB) so với tệp .onnx.

Cuối cùng, chúng tôi thực hiện một nghiên cứu ablation để đánh giá tác động của một chính quy hóa ℓ2-only đơn giản hơn, tức là weight decay cổ điển, thay cho bộ chính quy hóa dựa trên độ nhạy của chúng tôi. Hướng tới điều này, chúng tôi huấn luyện lại LeNet-300 với λ = 0 và một weight-decay được đặt thành 10⁻⁴ thay thế (dòng ℓ2+pruning trong các bảng trên). Chúng tôi chỉ ra trong (15) rằng độ nhạy có thể được hiểu như một yếu tố trọng số cho chính quy hóa ℓ2. Sử dụng weight-decay tương đương với việc giả sử tất cả các tham số có cùng độ nhạy. Đối với thí nghiệm này, chúng tôi sử dụng η = 0.1, PWE = 5 và TWT = 0 (TWT > 0 làm xấu đi đáng kể và không thể kiểm soát hiệu suất). Bảng I cho thấy rằng phương pháp như vậy ít hiệu quả hơn trong việc cắt tỉa neuron so với SeReNe (LB), loại bỏ ít hơn 15% neuron. Kết luận tương tự cũng có thể được rút ra nếu lỗi cao hơn được chấp nhận, như trong Bảng II. ℓ2+pruning đã được thực hiện để so sánh trong tất cả các thí nghiệm tiếp theo trong bài báo mang lại cùng kết quả.

C. LeNet5 trên MNIST

Tiếp theo, chúng tôi lặp lại thí nghiệm trước đó trên kiến trúc LeNet-5 [35], được huấn luyện sơ bộ như cho LeNet-300 ở trên, nhưng với SGD với learning rate = 0.1 và PWE = 20 epoch. Chúng tôi thử nghiệm với SeReNe (LB) với các tham số (λ = 10⁻⁴, TWT = 1.45). Đối với kiến trúc này, phương pháp của chúng tôi yêu cầu khoảng 500 epoch để đạt được cùng dải lỗi như các tham chiếu tiên tiến khác. Theo Bảng III, SeReNe (LB) tiếp cận độ chính xác phân loại của các đối thủ cạnh tranh và vượt trội so với các tham chiếu được xem xét về tỷ lệ nén và neuron được cắt tỉa.

Trong trường hợp này, lợi ích đến từ độ thưa có cấu trúc là rõ ràng: dấu chân lưu trữ mạng không nén giảm từ 1686 kB xuống 208 kB (-90%), sau nén lossless tiếp tục giảm xuống 19 kB chỉ với 0.12% giảm hiệu suất.

D. LeNet5 trên Fashion-MNIST

Sau đó, chúng tôi thử nghiệm với cùng kiến trúc LeNet-5 trên bộ dữ liệu Fashion-MNIST [36]. Fashion-MNIST có cùng kích thước với bộ dữ liệu MNIST, nhưng nó chứa hình ảnh tự nhiên của váy, giày, v.v. và do đó khó phân loại hơn MNIST vì các hình ảnh không thưa như các chữ số MNIST. Trong thí nghiệm này chúng tôi sử dụng SGD với learning rate = 0.1 và PWE = 20 epoch. Đối với SeReNe (LB) chúng tôi sử dụng λ = 10⁻⁵ và TWT = 1 cho khoảng 2k epoch.

Không ngạc nhiên, tỷ lệ nén trung bình thấp hơn so với MNIST: vì vấn đề phân loại khó hơn nhiều so với MNIST (Phần V-C), cần nhiều độ phức tạp hơn và SeReNe, để không làm giảm hiệu suất Top-1, không cắt tỉa nhiều như nó đã làm cho thí nghiệm MNIST. Quan trọng nhất, mạng SeReNe (LB) nén chỉ 46 kB, mặc dù số lượng tham số được cắt tỉa cao hơn.

E. VGG trên CIFAR-10.

Tiếp theo, chúng tôi thử nghiệm với hai triển khai phổ biến của kiến trúc VGG [2]. Chúng tôi nhớ lại rằng VGG bao gồm 13 lớp tích chập được sắp xếp trong 5 nhóm tương ứng 2, 2, 3, 3, 3 lớp, với 64, 128, 256, 512, 512 filter trên mỗi lớp tương ứng. VGG-1 là một triển khai VGG phổ biến trong các thí nghiệm CIFAR-10 chỉ bao gồm một lớp fully-connected như lớp đầu ra và được huấn luyện trước trên

--- TRANG 7 ---
7

BẢNG III: LeNet-5 được huấn luyện trên MNIST.

[Hiển thị bảng với các cột: Phương pháp, Tham số còn lại (%), Tỷ lệ nén, Neuron, Kích thước mạng [kB], Top-1 (%)]

BẢNG IV: LeNet-5 được huấn luyện trên Fashion-MNIST.

[Hiển thị bảng tương tự]

ImageNet². VGG-2 [23] tương tự như VGG-1 nhưng bao gồm một lớp fully connected ẩn với 512 neuron trước lớp đầu ra. Chúng tôi thử nghiệm trên bộ dữ liệu CIFAR-10, bao gồm 50k hình ảnh RGB 32×32 để huấn luyện và 10k để kiểm tra, phân phối trong 10 lớp. Đối với cả VGG-1 và VGG-2 chúng tôi đã sử dụng SGD với learning rate = 0.01 và PWE = 20 epoch. Đối với SeReNe (LB), chúng tôi sử dụng λ = 10⁻⁶ và TWT = 1.5. Cả hai kiến trúc đều được cắt tỉa trong khoảng 1k epoch và Bảng V và VI chi tiết các cấu trúc đã được cắt tỉa. Đối với mỗi kiến trúc, chúng tôi chi tiết số lượng filter sống sót (lớp tích chập) hoặc neuron (lớp fully connected) cho mỗi lớp trong dấu ngoặc vuông. 

²https://github.com/kuangliu/pytorch-cifar

Các bảng cho thấy SeReNe giới thiệu một độ thưa có cấu trúc đáng kể cho cả VGG-1 và VGG-2 và vượt trội so với Sparse-VD [23] về tỷ lệ nén. Chúng tôi có thể cắt tỉa một số lượng đáng kể filter cũng trong các lớp tích chập; ví dụ, 3 lớp trong block Conv4 được giảm xuống [382]-[93]-[136] cho VGG-1 và [498]-[433]-[65] cho VGG-2. Điều đó có tác động tích cực đến dấu chân mạng. Dấu chân bộ nhớ VGG-1 giảm từ 57.57 MB xuống 11.56 MB cho mạng đã cắt tỉa, trong khi biểu diễn nén 7zip chỉ 0.97 MB. Đối với VGG-2, dấu chân bộ nhớ giảm từ 58.61 MB xuống 29.41 MB, trong khi biểu diễn tệp nén là 2.47 MB.

--- TRANG 8 ---
8

BẢNG V: Kiến trúc giống VGG với 1 lớp fully connected (VGG-1) được huấn luyện trên CIFAR-10.

[Hiển thị bảng chi tiết với các metric cho VGG-1]

BẢNG VI: Kiến trúc giống VGG với 2 lớp fully connected (VGG-2) được huấn luyện trên CIFAR-10.

[Hiển thị bảng chi tiết với các metric cho VGG-2]

BẢNG VII: ResNet-32 được huấn luyện trên CIFAR-10.

[Hiển thị bảng với metric cho ResNet-32]

F. ResNet-32 trên CIFAR-10

Sau đó chúng tôi đánh giá SeReNe trên kiến trúc ResNet-32 [3] được huấn luyện trên bộ dữ liệu CIFAR-10 sử dụng SGD với learning rate = 0.001, momentum 0.9, λ = 10⁻⁵, TWT = 0 và PWE = 10. Bảng VII hiển thị kiến trúc kết quả. Do số lượng lớp, chúng tôi biểu diễn kiến trúc mạng trong năm block khác nhau: block đầu tiên tương ứng với lớp tích chập đầu tiên nhận đầu vào hình ảnh gốc, block cuối cùng đại diện cho lớp fully-connected đầu ra. Ba block ở giữa đại diện cho phần còn lại của mạng, dựa trên số lượng kênh đầu ra của mỗi lớp: block1 chứa tất cả các lớp với đầu ra 16 kênh, block2 chứa tất cả các lớp với đầu ra 32 kênh và block3 thu thập các lớp với đầu ra 64 kênh. ResNet đã là một kiến trúc được tối ưu hóa và do đó khó cắt tỉa hơn so với, ví dụ, VGG. Tuy nhiên, SeReNe vẫn có thể cắt tỉa khoảng 40% neuron và 70% tham số so với ResNet-32 gốc. Điều này được phản ánh trên kích thước của mạng, giảm từ 1.84 MB (1.63 MB nén) xuống 0.87 MB (0.57MB nén).

G. AlexNet trên CIFAR-100

Tiếp theo, chúng tôi up-scale trong chiều đầu ra của vấn đề học, tức là trong số lượng lớp C, kiểm tra phương pháp đề xuất trên một mạng giống AlexNet trên bộ dữ liệu CIFAR-100. Bộ dữ liệu như vậy bao gồm hình ảnh RGB 32×32 được chia thành 100 lớp (50k hình ảnh huấn luyện, 10k hình ảnh test). Trong thí nghiệm này chúng tôi sử dụng SGD với learning rate = 0.1 và PWE = 20 epoch. Về SeReNe (LB), chúng tôi sử dụng λ = 10⁻⁵ và TWT = 1.5 và quá trình cắt tỉa kéo dài 300 epoch.

Bảng VIII hiển thị tỷ lệ nén vượt quá 179x, trong khi kích thước mạng giảm từ 92.31 MB xuống 43.80 MB và tiếp tục xuống 2.47 MB sau nén.

So với CIFAR-10, chúng tôi giả thuyết rằng số lượng lớp mục tiêu lớn hơn để phân biệt ngăn cắt tỉa neuron trong các lớp tích chập, nhưng nó cho phép cắt tỉa một số lượng đáng kể neuron từ các lớp fully connected ẩn. Trái với các thí nghiệm trước, lỗi top-5 và top-1 cải thiện so với baseline.

H. ResNet-101 trên ImageNet

Như một thí nghiệm cuối cùng, chúng tôi kiểm tra SeReNe trên ResNet-101 được huấn luyện trên ImageNet (ILSVRC-2012), sử dụng mạng đã được huấn luyện trước được cung cấp bởi thư viện torchvision.³

Do thời gian huấn luyện dài, chúng tôi sử dụng một heuristic batch-wise sao cho, thay vì chờ đợi một plateau hiệu suất, bước cắt tỉa được thực hiện mỗi khi một phần năm của tập train (khoảng 7.9k lần lặp) đã được xử lý. Chúng tôi huấn luyện mạng sử dụng SGD với learning rate = 0.001 và momentum 0.9; đối với SeReNe (LB) chúng tôi sử dụng λ = 10⁻⁶ và TWT = 0.

Bảng IX hiển thị kết quả của thủ tục cắt tỉa với các lớp được nhóm trong các block tương tự như cho thí nghiệm ResNet-32. Mặc dù độ phức tạp của vấn đề phân loại (1000 lớp) khiến việc cắt tỉa toàn bộ neuron trở nên thách thức, chúng tôi cắt tỉa khoảng 86% tham số và có được một mạng nhỏ hơn về kích thước, đặc biệt khi nén, từ 156.67 MB xuống chỉ 27.84 MB.

³https://pytorch.org/docs/stable/torchvision/models.html

--- TRANG 9 ---
9

BẢNG VIII: AlexNet được huấn luyện trên CIFAR-100.

[Hiển thị bảng chi tiết với các metric cho AlexNet trên CIFAR-100]

BẢNG IX: ResNet-101 được huấn luyện trên ImageNet.

[Hiển thị bảng chi tiết với các metric cho ResNet-101 trên ImageNet]

I. Thí nghiệm trên thiết bị di động

Như một thí nghiệm cuối cùng, chúng tôi benchmark một số kiến trúc được cắt tỉa với SeReNe trên một smartphone Huawei P20 được trang bị bộ xử lý 4x2.36 GHz Cortex-A73 + 4x1.84GHz Cortex-A53 và RAM 4GB, chạy Android 8.1 "Oreo". Bảng X hiển thị thời gian suy luận cho ResNet-32, VGG-16 và AlexNet (tất cả các con số đều được lấy trung bình từ 1,000 lần suy luận trên thiết bị). Các kiến trúc được cắt tỉa bằng SeReNe cho thấy thời gian suy luận thấp hơn một cách nhất quán dưới ánh sáng của ít neuron hơn trong mạng đã cắt tỉa, với tăng tốc hàng đầu cho VGG-16 vượt quá hệ số 2x. Các kết quả này không tính đến các chiến lược thường được sử dụng để tăng tốc thời gian suy luận, như lượng tử hóa tham số hoặc thư viện tùy chỉnh để xử lý tensor thưa. Chúng tôi giả thuyết rằng các chiến lược như vậy, trực giao với cắt tỉa neuron, sẽ tăng tốc thêm thời gian suy luận.

BẢNG X: Đo suy luận trên Huawei P20.

[Hiển thị bảng với thời gian suy luận cho các kiến trúc khác nhau]

VI. KẾT LUẬN

Trong công trình này chúng tôi đã đề xuất một kỹ thuật chính quy hóa neural điều khiển bởi độ nhạy. Hiệu ứng của bộ chính quy hóa này là phạt tất cả các tham số thuộc về một neuron có đầu ra không có ảnh hưởng trong đầu ra của mạng. Chúng tôi đã học được rằng việc đánh giá độ nhạy ở cấp độ neuron (SeReNe) cực kỳ quan trọng để thúc đẩy một độ thưa có cấu trúc trong mạng, có thể có được một mạng nhỏ hơn với mất mát hiệu suất tối thiểu. Các thí nghiệm của chúng tôi cho thấy rằng SeReNe đạt được một sự cân bằng thuận lợi giữa khả năng cắt tỉa neuron và chi phí tính toán, đồng thời kiểm soát sự suy giảm trong hiệu suất phân loại. Đối với tất cả các kiến trúc và bộ dữ liệu được kiểm tra, phương pháp dựa trên độ nhạy của chúng tôi đã chứng minh giới thiệu một độ thưa có cấu trúc đồng thời đạt được tỷ lệ nén tiên tiến. Hơn nữa, thuật toán sparsifying được thiết kế, sử dụng cross-validation, đảm bảo mất mát hiệu suất tối thiểu (hoặc không có), có thể được điều chỉnh bởi người dùng qua một siêu tham số (TWT). Công việc tương lai bao gồm triển khai trên các thiết bị nhúng vật lý sử dụng mạng sâu cũng như sử dụng chính quy hóa dựa trên lượng tử hóa cùng với độ nhạy neuron để nén thêm các mạng sâu.

PHỤ LỤC A
J ĐƯỢC LÀM RÕ

Để tính toán S cho (5), chúng ta có thể tiến hành trực tiếp. Tuy nhiên, điều này có vấn đề vì nó yêu cầu C lần gọi khác nhau cho engine vi phân. Hãy nhớ lại (4):
Sn,i = (1/C) Σk=1^C |∂yN,k/∂pn,i|

Bây giờ chúng ta kiểm tra xem chúng ta có thể giảm tính toán bằng cách định nghĩa một hàm mục tiêu tổng thể mà, khi được vi phân, mang lại S như một kết quả. Chúng ta đặt tên nó là J:

J = ∫ Sn,i dpn,i = ∫ (1/C) Σk=1^C |∂yN,k/∂pn,i| dpn,i
  = (1/C) ∫ Σk=1^C |∂yN,k/∂pn,i| sign(∂yN,k/∂pn,i) dpn,i (24)

Ở đây chúng ta có thể sử dụng định lý Fubini-Tonelli:
J = (1/C) Σk=1^C ∫ |∂yN,k/∂pn,i| sign(∂yN,k/∂pn,i) dpn,i
  = (1/C) Σk=1^C yN,k sign(∂yN,k/∂pn,i) (25)

Thật không may, chúng ta không có cách hiệu quả nào để tính toán sign(∂yN,k/∂pn,i) và cách chắc chắn duy nhất là tính toán ∂yN,k/∂pn,i trực tiếp, ∀C.

PHỤ LỤC B
LENET300 VỚI KÍCH HOẠT SIGMOID TRÊN MNIST

PHỤ LỤC C
ĐẠO HÀM RÕ RÀNG CHO HÀM CHÍNH QUY HÓA SERENE

Ở đây chúng ta tập trung vào quy tắc cập nhật (15): chúng ta nhằm tối thiểu hóa hàm mục tiêu tổng thể
O = L + R (26)
trong đó
R = ∫ w Sn,i S̄n,i dw (27)

Từ đây chúng ta sẽ bỏ các chỉ số dưới n,i,j. Hãy xem xét công thức của độ nhạy trong (4): (27) trở thành
R = ∫ w S̄ (S̄) dw (28)
trong đó (·)₊ là hàm một bước và
S̄ = 1 - (1/C) Σk=1^C |∂yN,k/∂p| (29)

Chúng ta có thể viết lại (28) là
R = ∫ w S̄ dw - ∫ w (1/C) Σk=1^C |∂yN,k/∂p| S̄ dw
  = (w²/2) S̄ - ∫ w (1/C) Σk=1^C |∂yN,k/∂p| S̄ dw (30)

Hãy định nghĩa
JR = ∫ w (1/C) Σk=1^C |∂yN,k/∂p| S̄ dw (31)

Xem xét rằng w độc lập với k và (1/C) là một hằng số, chúng ta có thể viết
JR = (1/C) Σk=1^C ∫ w |∂yN,k/∂p| S̄ dw (32)

--- TRANG 10 ---
10

Ở đây chúng ta được phép áp dụng định lý Fubini-Tonelli, hoán đổi tổng và tích phân:
JR = (1/C) Σk=1^C ∫ w |∂yN,k/∂p| S̄ dw
   = (1/C) Σk=1^C ∫ w |∂yN,k/∂p| sign(∂yN,k/∂p) S̄ dw (33)

Bây giờ tích phân từng phần:
JR = (1/C) Σk=1^C ∫ w |∂yN,k/∂p| sign(∂yN,k/∂p) S̄ dw
   = (1/C) Σk=1^C [(w²/2) |∂yN,k/∂p| sign(∂yN,k/∂p) S̄ +
   + ∫ (w²/2) ∂/∂w(|∂yN,k/∂p| sign(∂yN,k/∂p) S̄) dw] (34)

Theo quy tắc chuỗi đạo hàm, chúng ta có thể viết lại (34) là
JR = (1/C) Σk=1^C [(w²/2) |∂yN,k/∂p| sign(∂yN,k/∂p) S̄ +
   + ∫ (w²/2) (∂²yN,k/∂p²)(∂p/∂w) sign(∂yN,k/∂p) S̄ dw] (35)

Áp dụng các bước tích phân từng phần vô hạn cuối cùng chúng ta có
JR = (1/C) S̄ Σk=1^C sign(∂yN,k/∂p) [(w²/2) |∂yN,k/∂p| +
   + Σi=1^∞ (-1)^(i+1) (w^(i+2))/((i+2)!) (∂^(i+1)yN,k/∂p^(i+1))(∂^i p/∂w^i)] (36)

Do đó, hàm R được tối thiểu hóa tổng thể là
R = S̄{(w²/2) + (1/C) Σk=1^C sign(∂yN,k/∂p) [(w²/2) |∂yN,k/∂p| +
   + Σi=1^∞ (-1)^(i+1) (w^(i+2))/((i+2)!) (∂^(i+1)yN,k/∂p^(i+1))(∂^i p/∂w^i)]} (37)

PHỤ LỤC D
ĐẠO HÀM CỦA (17)

Hãy nhớ lại công thức trong (15). Theo (14), chúng ta có thể viết lại nó là
w^(t+1)_(n,i,j) = w^t_(n,i,j) - η (∂L/∂w^t_(n,i,j) - λ w_(n,i,j) (1-S_(n,i))₊ σ(1-S_(n,i))) (38)

Cho định nghĩa của S_(n,i) trong (4), chúng ta có thể viết
w^(t+1)_(n,i,j) = w^t_(n,i,j) - η (∂L/∂w^t_(n,i,j) -
λ w_(n,i,j) (1-S_(n,i))₊ σ(1 - (1/C) Σk=1^C |∂yN,k/∂p_(n,i)|)) (39)

Chúng ta có thể nhân tính không nhạy với số hạng w_(n,i,j):
w^(t+1)_(n,i,j) = w^t_(n,i,j) - η (∂L/∂w^t_(n,i,j) -
λ (1-S_(n,i))₊ σ(w_(n,i,j) - (1/C) Σk=1^C |∂yN,k/∂p_(n,i)| w_(n,i,j))) (40)

Cuối cùng ở đây, quan sát (16), chúng ta tìm lại (17).

PHỤ LỤC E
LENET300 VỚI KÍCH HOẠT SIGMOID TRÊN MNIST

Cuối cùng, chúng tôi lặp lại thí nghiệm trong Phần V-B nhưng thay thế các kích hoạt ReLU bằng sigmoid trong các lớp ẩn. Chúng tôi tối ưu hóa một LeNet300 đã được huấn luyện trước sử dụng SGD với learning rate η = 0.1, PWE = 20 epoch, TWT = 0 cho lỗi Top-1 mục tiêu là 1.7% (Bảng XI) và 1.95% (Bảng XII).

SeReNe đạt được một kiến trúc thưa hơn (và nhỏ hơn) so với ℓ2+ pruning cho cả hai tỷ lệ lỗi. Thú vị là, đối với tỷ lệ lỗi 1.7%, ℓ2+ pruning không thể cắt tỉa bất kỳ neuron nào, trong khi SeReNe cắt tỉa 85 neuron từ FC1, với tỷ lệ nén cao hơn 10 lần. Điều này phản ánh trong kích thước mô hình nén: trong khi ℓ2+ pruning nén kiến trúc xuống 536kB, SeReNe nén nó xuống chỉ 75kB.

BẢNG XI: LeNet-300 được huấn luyện trên MNIST (kích hoạt sigmoid, tỷ lệ lỗi 1.7%).

[Hiển thị bảng với các metric cho LeNet-300 với sigmoid]

BẢNG XII: LeNet-300 được huấn luyện trên MNIST (kích hoạt sigmoid, tỷ lệ lỗi 1.95%).

[Hiển thị bảng với các metric cho LeNet-300 với sigmoid]

--- TRANG 11 ---
11

TÀI LIỆU THAM KHẢO

[1] A. Krizhevsky, I. Sutskever, và G. E. Hinton, "Imagenet classification with deep convolutional neural networks," trong Advances in neural information processing systems, 2012, trang 1097–1105.

[2] K. Simonyan và A. Zisserman, "Very deep convolutional networks for large-scale image recognition," 3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings, 2015.

[3] K. He, X. Zhang, S. Ren, và J. Sun, "Deep residual learning for image recognition," trong Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, trang 770–778.

[4] T. M. P. E. Group, "Compression of neural networks for multimedia content description and analysis," MPEG 125 - Marrakesh.

[5] Y. Lu, G. Lu, R. Lin, J. Li, và D. Zhang, "Srgc-nets: Sparse repeated group convolutional neural networks," IEEE Transactions on Neural Networks and Learning Systems, tập 31, trang 2889–2902, 2020.

[6] J. Wu, C. Leng, Y. Wang, Q. Hu, và J. Cheng, "Quantized convolutional neural networks for mobile devices," trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, trang 4820–4828.

[7] L. Sagun, U. Evci, V. U. Guney, Y. Dauphin, và L. Bottou, "Empirical analysis of the hessian of over-parametrized neural networks," 6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings, 2018.

[8] Y. LeCun, J. S. Denker, và S. A. Solla, "Optimal brain damage," trong Advances in neural information processing systems, 1990, trang 598–605.

[9] D. P. Kingma, T. Salimans, và M. Welling, "Variational dropout and the local reparameterization trick," trong Advances in Neural Information Processing Systems, 2015, trang 2575–2583.

[10] E. Tartaglione, S. Lepsøy, A. Fiandrotti, và G. Francini, "Learning sparse neural networks via sensitivity-driven regularization," trong Advances in Neural Information Processing Systems, 2018, trang 3878–3888.

[11] S. Han, J. Pool, J. Tran, và W. Dally, "Learning both weights and connections for efficient neural network," trong Advances in neural information processing systems, 2015, trang 1135–1143.

[12] J. Frankle và M. Carbin, "The lottery ticket hypothesis: Finding sparse, trainable neural networks," 7th International Conference on Learning Representations, ICLR 2019, 2019.

[13] M. Naumov, L. Chien, P. Vandermersch, và U. Kapasi, "Cusparse library," trong GPU Technology Conference, 2010.

[14] J. Bai, F. Lu, K. Zhang et al., "Onnx: Open neural network exchange," https://github.com/onnx/onnx, 2019.

[15] V. Lebedev và V. Lempitsky, "Fast convnets using group-wise brain damage," trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, trang 2554–2564.

[16] B. Liu, M. Wang, H. Foroosh, M. Tappen, và M. Pensky, "Sparse convolutional neural networks," trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, trang 806–814.

[17] M. Zhu và S. Gupta, "To prune, or not to prune: exploring the efficacy of pruning for model compression," 6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings, 2018.

[18] W. Wen, C. Wu, Y. Wang, Y. Chen, và H. Li, "Learning structured sparsity in deep neural networks," trong Advances in neural information processing systems, 2016, trang 2074–2082.

[19] T. Gale, E. Elsen, và S. Hooker, "The state of sparsity in deep neural networks," CoRR, tập abs/1902.09574, 2019. [Trực tuyến]. Có sẵn: http://arxiv.org/abs/1902.09574

[20] M. Gong, J. Liu, H. Li, Q. Cai, và L. Su, "A multiobjective sparse feature learning model for deep neural networks," IEEE transactions on neural networks and learning systems, tập 26, số 12, trang 3263–3277, 2015.

[21] M. Lin, R. Ji, Y. Zhang, B. Zhang, Y. Wu, và Y. Tian, "Channel pruning via automatic structure search," trong Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, C. Bessiere, Ed. International Joint Conferences on Artificial Intelligence Organization, 7 2020, trang 673–679, main track.

[22] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, và R. Salakhutdinov, "Dropout: a simple way to prevent neural networks from overfitting," The Journal of Machine Learning Research, tập 15, số 1, trang 1929–1958, 2014.

[23] D. Molchanov, A. Ashukha, và D. Vetrov, "Variational dropout sparsifies deep neural networks," trong Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017, trang 2498–2507.

[24] A. N. Gomez, I. Zhang, K. Swersky, Y. Gal, và G. E. Hinton, "Learning sparse networks using targeted dropout," CoRR, tập abs/1905.13678, 2019.

[25] G. Hinton, O. Vinyals, và J. Dean, "Distilling the knowledge in a neural network," arXiv preprint arXiv:1503.02531, 2015.

[26] T. Li, J. Li, Z. Liu, và C. Zhang, "Few sample knowledge distillation for efficient network compression," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, trang 14 639–14 647.

[27] Y. Wang, X. Zhang, L. Xie, J. Zhou, H. Su, B. Zhang, và X. Hu, "Pruning from scratch." trong AAAI, 2020, trang 12 273–12 280.

[28] N. Lee, T. Ajanthan, và P. Torr, "Snip: Single-shot network pruning based on connection sensitivity," 7th International Conference on Learning Representations, ICLR 2019, 2019.

[29] E. Tartaglione, A. Bragagnolo, và M. Grangetto, "Pruning artificial neural networks: A way to find well-generalizing, high-entropy sharp minima," trong Artificial Neural Networks and Machine Learning – ICANN 2020, I. Farkaš, P. Masulli, và S. Wermter, Eds. Cham: Springer International Publishing, 2020, trang 67–78.

[30] C. Louizos, M. Welling, và D. P. Kingma, "Learning sparse neural networks through l0 regularization," 6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings, 2018.

[31] E. Tartaglione, D. Perlo, và M. Grangetto, "Post-synaptic potential regularization has potential," trong International Conference on Artificial Neural Networks. Springer, 2019, trang 187–200.

[32] K. Ullrich, M. Welling, và E. Meeds, "Soft weight-sharing for neural network compression," 5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, 2019.

[33] Y. Guo, A. Yao, và Y. Chen, "Dynamic network surgery for efficient dnns," Advances in Neural Information Processing Systems, trang 1387–1395, 2016.

[34] I. Pavlov, "Lzma sdk (software development kit)," 2007. [Trực tuyến]. Có sẵn: https://www.7-zip.org/sdk.html

[35] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner et al., "Gradient-based learning applied to document recognition," Proceedings of the IEEE, tập 86, số 11, trang 2278–2324, 1998.

[36] H. Xiao, K. Rasul, và R. Vollgraf, "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms," CoRR, tập abs/1708.07747, 2017. [Trực tuyến]. Có sẵn: http://arxiv.org/abs/1708.07747
