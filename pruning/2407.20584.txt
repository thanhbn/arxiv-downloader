# 2407.20584.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2407.20584.pdf
# File size: 871854 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pruning Large Language Models with Semi-Structural Adaptive Sparse Training
Weiyu Huang, Hu Yuezhou, Guohao Jian, Jun Zhu, Jianfei Chen†
Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab,
Tsinghua-Bosch Joint ML Center, Tsinghua University
{hwy23, huyz21, jgh22 }@mails.tsinghua.edu.cn; {dcszj, jianfeic }@tsinghua.edu.cn
Abstract
The remarkable success of Large Language Models (LLMs)
relies heavily on their substantial scale, which poses signifi-
cant challenges during model deployment in terms of latency
and memory consumption. Recently, numerous studies have
attempted to compress LLMs using one-shot pruning meth-
ods. However, these methods often suffer from considerable
performance degradation on complex language understand-
ing tasks, raising concerns about the feasibility of pruning
in LLMs. To address this issue, we propose Adaptive Sparse
Trainer (AST), a novel and efficient retraining framework tai-
lored for semi-structured sparse models. AST enables mod-
els to learn optimal masks during the weight update process
without incurring additional computational overhead. Fur-
thermore, we demonstrate that incorporating knowledge dis-
tillation significantly improves retraining efficiency and en-
hances model performance under fixed computational con-
straints. Additionally, a supplementary set of well-initialized
parameters is integrated to further augment the model’s effi-
cacy. AST achieves state-of-the-art performance with mini-
mal training cost. When applied to the LLaMA2-7B model,
AST reduces the perplexity and zero-shot accuracy gap be-
tween dense and 2:4 semi-structured sparse models to 0.6 and
1.16%, respectively, utilizing less than 0.4% of the pretrain-
ing tokens and GPU hours. Our work demonstrates the feasi-
bility of deploying semi-structured sparse LLMs and offers a
promising alternative for achieving highly compressed mod-
els when combined with existing quantization techniques.
Code — https://github.com/thu-ml/Adaptive-Sparse-Trainer
1 Introduction
Transformer-based Large Language Models (LLMs) are
equipped to handle complex tasks (Devlin et al. 2018;
Brown et al. 2020; Achiam et al. 2023) and exhibit emergent
abilities (Wei et al. 2022) due to their expanding parameter
count. However, this continual growth in model size poses
significant challenges for practical deployment. In particu-
lar, inference speed suffers due to the increasing computa-
tional and memory demands. This has spurred a series of
efforts to develop effective model compression techniques
†Corresponding Author.
Copyright © 2025, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.aimed at reducing memory footprints and easing the con-
straints associated with deploying these large-scale models.
Model pruning (Frantar and Alistarh 2023; Han, Mao,
and Dally 2016; Sun et al. 2023) is an effective technique
for compressing LLMs by setting a proportion of weights
to zero, thereby adhering to a specific sparsity pattern.
Recently, N:M sparsity has emerged as a type of semi-
structured sparsity pattern that offers an optimal balance be-
tween precision and hardware efficiency. Specifically, N:M
sparsity retains only N nonzero elements out of every group
of M elements. This sparsity pattern can accelerate both ma-
trix multiplication and memory access, potentially enhanc-
ing the performance of both pre-filling and decoding pro-
cesses on off-the-shelf GPUs. Despite the promise of N:M
sparsity, current state-of-the-art methods for pruning LLMs,
such as SparseGPT (Frantar and Alistarh 2023) and Wanda
(Sun et al. 2023), employ a post-training approach that deter-
mines the sparsity pattern in a layer-by-layer fashion with-
out back-propagation. Although these methods improve ef-
ficiency, they can lead to significant performance degra-
dation, particularly in knowledge-intensive tasks (Nowak
et al. 2024), raising concerns about the feasibility of prun-
ing LLMs. Additionally, while retraining pruned models has
been successful in the pre-LLM era (Wang, Wohlwend, and
Lei 2019; Lee, Ajanthan, and Torr 2019; Evci et al. 2020),
its application to models with billions of parameters remains
under-explored.
Although retraining sparse LLMs holds significant poten-
tial, it introduces several unique challenges: (1) Retraining is
computationally expensive, necessitating techniques that en-
sure rapid convergence. (2) The output model must adhere
to strict sparsity pattern, which adds additional constraints
to the retraining process. (3) In order to achieve optimal per-
formance, both masks and weights should be learnable dur-
ing training. (4) Pruning LLMs may compromise essential
language understanding and reasoning abilities, which are
significantly harder to restore compared to simpler metrics
such as perplexity. To this end, we propose a novel and ef-
ficient training method, Adaptive Sparse Trainer (AST), de-
signed to produce high-performance sparse LLMs. For the
first time, we demonstrate that pretrained 2:4 sparse LLMs
can achieve competitive performance not only in terms of
perplexity but also in more demanding knowledge-intensive
tasks , making them viable for practical deployment. ASTarXiv:2407.20584v3  [cs.CL]  18 Dec 2024

--- PAGE 2 ---
integrates the transition from dense to sparse models di-
rectly into the training process, gradually decaying unim-
portant weights to zero through a carefully designed decay
mechanism. This approach allows for the revival of pruned
weights during training, enabling a smooth and dynamic ad-
justment of the model’s connectivity pattern while adher-
ing to the N:M sparsity structure. Furthermore, AST applies
knowledge distillation using dense model as the teacher,
which can significantly speed up model convergence and
prevent sparse models from settling into local optima. It also
helps the sparse model retain the valuable world knowledge
and performance characteristics of the original dense model,
thereby enhancing its generalization ability and compensat-
ing for the use of weaker training datasets. To further en-
hance performance, a supplementary set of parameters is in-
tegrated using information from pruned weights. When ap-
plied to the LLaMA2-7B model, AST achieves a 2:4 sparse
configuration with minimal performance loss, demonstrat-
ing that compressed models can still perform effectively in
practice. Our model further benefits from AWQ quantiza-
tion, achieving competitive compression rates with state-of-
the-art performance.
The key contributions of this work are as follows:
• We propose Adaptive Sparse Trainer (AST), an novel
semi-structured pruning framework designed to com-
press large language models efficiently while maintain-
ing high performance.
• AST introduces a gradual decay scheduler (Annealing
SR-STE) combined with knowledge distillation to accel-
erate model convergence and improve performance on
complex tasks.
• We introduce SLoRB, a technique that boosts model
performance by adding a supplementary set of well-
initialized parameters.
• When applied to LLaMA2-7B model, our 2:4 sparse
model experiences only a 0.6 increase in perplexity and
a 1.16% accuracy drop in zero-shot tasks, with less than
0.4% of the pretraining cost.
2 Related Work
Network Pruning Pruning is a prevalent technique aimed
at reducing model size and computational costs. It origi-
nated from methods like OBD (LeCun, Denker, and Solla
1990) and OBS (Hassibi, Stork, and Wolff 1993). Based
on sparsity patterns, pruning methods can be broadly cat-
egorized into unstructured, structured, and semi-structured
pruning. Unstructured pruning removes individual weights
(Han, Mao, and Dally 2016; Paul et al. 2023), which can
maintain performance even with high sparsity. However, due
to its random pattern, unstructured models are difficult to
accelerate. Structured pruning (Liu et al. 2017; Molchanov
et al. 2019; Nova, Dai, and Schuurmans 2023; Shen et al.
2022), on the other hand, removes entire neurons, filters, or
attention heads, resulting in models that are easier to accel-
erate on standard hardware but often suffer from severe per-
formance loss. Semi-structured sparsity (e.g., N:M sparsity)
(Hubara et al. 2021) has been applied as a trade-off betweenperformance and achieving actual speedup. Recently, a se-
ries of works (Frantar and Alistarh 2023; Sun et al. 2023;
Zhang et al. 2024a,b) have made progress in pruning LLMs
with billions of parameters. However, these pruned models
from training-free methods still fall short in complex zero-
shot performance.
Retraining Pruned Models Another line of research
(Han et al. 2015; Singh and Alistarh 2020; Renda, Frankle,
and Carbin 2020; Zhou et al. 2023) has focused on retraining
pruned models to enhance performance. While retraining
has been shown to be effective with smaller models and sim-
pler tasks (Kurtic and Alistarh 2022; Zhu and Gupta 2018), it
often involves additional training steps (Frankle and Carbin
2019) or introduces extra parameters for the pruning process
(Shi et al. 2023), which limits its application to large-scale
models due to the substantial computational resources re-
quired. Other retraining methods focus on unstructured spar-
sity, which may struggle when given stricter sparsity pat-
terns (Lee, Ajanthan, and Torr 2019; Evci et al. 2020). Re-
cently, Sheared LLaMA (Xia et al. 2024) employed a two-
stage structured pruning process to train models that outper-
form others of similar size. However, they employ a compu-
tationally intensive pruning stage, unsuitable for more fine-
grained sparsity. Our work proposes a lightweight pruning
process that enables the retraining of semi-structured sparse
large language models with minimal training costs.
Combining Pruning with Quantization Pruned models
can be further compressed using quantization. Earlier meth-
ods like Deep Compression (Han, Mao, and Dally 2016)
combined pruning and quantization to significantly reduce
the size of neural networks. More recent work (Frantar and
Alistarh 2023; Sun et al. 2023; Guo et al. 2024) has com-
bined sparsity with quantization in large language models.
In our work, we report results using AWQ quantization (Lin
et al. 2024) with our semi-structured sparse model.
3 Methods
We begin by revisiting a naive approach to training sparse
models before introducing our method. The full training
pipeline is illustrated in Figure 1.
3.1 Sparse Training
Pruning model weights is equivalent to multiplying them
by an element-wise mask. For example, consider the matrix
multiplication in a linear layer:
Z=XW⊤, Z∈RN×D, X∈RN×C, W ∈RD×C,
where X,W, and Zrepresent the model input, weight ma-
trix, and output activation, respectively. The pruned weight
matrix can be expressed as:
˜W=m(W)⊙W, m (W)∈ {0,1}D×C,
where m(·)is a mapping that selects a mask based on the
weight matrix W. In this work, we focus on N:M sparsity
(Hubara et al. 2021), where there are Nnonzero elements
in every Mconsecutive weights in the same row. How-
ever, when implementing the backward pass for a sparse

--- PAGE 3 ---
Figure 1: (Left) In the naive training baseline, the mask remains constant during training, which can result in suboptimal
performance. (Right) Adaptive sparse training enables both mask and weight learning through a scheduled decay term. AST
also utilizes distillation and SLoRB parameters to speed up convergence and improve performance.
model with automatic differentiation, the gradient cannot
flow through the discrete mask m(W). A straightforward
approach to solve this issue is the straight-through estimator
(STE) (Bengio, L ´eonard, and Courville 2013) which updates
the parameters using the gradient with respect to the masked
weight ˜Wt, where Wtis the parameter at iteration t:
Wt+1←Wt−γtg(˜Wt), (1)
hereg(˜Wt)represents the gradient with respect to ˜Wtand
γtindicates the learning rate at iteration t.
A previous method (Sun et al. 2023) employs a fixed
mask derived from one-shot pruning techniques and up-
dates only the remaining parameters using language mod-
eling loss. However, this strategy has two significant limita-
tions. First, it discards the valuable information embedded
in the pruned weights and prevents a smooth transition from
a dense to a sparse model, resulting in slower convergence
and suboptimal performance. Second, relying solely on the
language modeling loss during retraining increases the risk
of the sparse model becoming trapped in a local optimum,
leading to poor outcomes.
To address these issues, we maintain all model weights
and select the mask on-the-fly while gradually decaying
non-essential weights to zero, thereby enabling a smooth
transition ( §3.2). Additionally, we leverage knowledge dis-
tillation to guide the model toward a global optimum, signifi-
cantly accelerating convergence under a fixed computational
budget ( §3.3). Finally, we offer an optional method to further
enhance model performance by incorporating an additional
set of well-initialized parameters, as detailed in ( §3.4).
3.2 Adaptive Mask Learning with Annealing
SR-STE
As highlighted in previous work (Frankle and Carbin 2019;
Evci et al. 2020), the connectivity pattern is as crucial as
the weights themselves in determining the performance of
sparse models. Therefore, we enable the model to adaptively
learn the optimal mask during training instead of fixing it
permanently. Specifically, we recalculate m(Wt)based on
the magnitude criterion every ∆titerations and add a de-
cay term to the masked weights. The intuition behind thisapproach is straightforward: important weights typically re-
ceive strong gradients of the same sign during training, caus-
ing their magnitude to grow despite the decay, and they will
eventually be revived. In contrast, less important weights
tend to receive weaker, mixed gradient signals, leading them
to decay to zero and remain masked under our settings. We
build upon SR-STE (Zhou et al. 2021) by applying L2 decay
to masked weights, as this approach helps preserve overall
model performance.
However, selecting the appropriate decay strength is chal-
lenging. If the decay signal is too strong, masked weights
will struggle to regenerate, leading to an almost fixed mask.
On the other hand, if the decay signal is too weak, the
model will fail to filter out important weights, resulting in
heavy oscillation that hinders model convergence. To ad-
dress this dilemma between stability and exploration, we
propose adding a moving decay schedule, with weaker de-
cay strength at the beginning to encourage the model to ex-
plore various connectivity patterns and a stronger decay sig-
nal toward the end to facilitate model convergence.
Formally, we update the model weights with the following
equation:
λW(t) =αt, if0≤t≤T0,
αT0,ifT0≤t≤T1,(2)
Wt+1←Wt−γt
g(˜Wt) +λW(t)
m(Wt)⊙Wt
,
(3)
where λW(t)denotes the decay factor, m(Wt)denotes the
mask for pruned weights at iteration t,T1denotes the to-
tal number of training batches, and T0represents the batch
at which the decay factor stops increasing. Compared with
STE in Equation 1, masked weights receive a decay signal
proportional to the decay factor λW(t), which starts small
at the beginning and increases later on. This method, termed
Annealing SR-STE (ARS-STE), has been shown to improve
model performance compared to naive SR-STE. We also
provide a detailed analysis of mask oscillation behavior dur-
ing training in Section 7 of the Appendix.
We experimented with various one-shot pruning criteria
incorporating activation information (Frantar and Alistarh

--- PAGE 4 ---
2023; Zhang et al. 2024a) for mask selection. However,
theclassic magnitude criterion consistently outperformed
these methods in both computational efficiency and perfor-
mance. As activation-based approaches incur higher compu-
tational costs and are less reliable due to activation fluctua-
tions caused by weight updates, leading to degraded perfor-
mance in later training stages.
Since the naive SR-STE is only compatible with the
SGD optimizer, we introduce further adjustments to support
more advanced optimizers such as AdamW. Specifically, as
AdamW maintains both first-order and second-order mo-
ments of the gradient, we decouple the weight decay term
from the first-order moment instead of directly adding de-
cay to the gradient, as done in (Hu et al. 2024). This sepa-
ration ensures that the weight decay term depends solely on
the current weight, avoiding interference with momentum
calculations. We then use the decoupled first-order signal to
update the second-order moment. Detailed expressions and
explanations are provided in Section 6 of the Appendix.
3.3 Alleviating the Retraining Dilemma through
Knowledge Distillation
A key distinction between pretraining and retraining pruned
models lies in how their parameters are initialized. In the
pretraining setting, parameters are typically randomly sam-
pled, whereas in retraining, they are inherited from a well-
trained model. Consequently, pruned models retain a portion
of the patterns acquired during pretraining.
We discovered that this feature tends to trap pruned mod-
els in local optima. Specifically, while retraining can achieve
much faster initial convergence, it often fails to reach an op-
timal state later on; we refer to this phenomenon as the Re-
training Dilemma . As illustrated in Figure 2, although us-
ing cross-entropy loss during retraining initially leads to a
quicker reduction in training loss compared to pretraining,
the test set perplexity remains unstable and elevated. We at-
tribute this to the fact that, unlike randomly initialized mod-
els, the pruned model is more prone to overfitting on the
limited data available at the start due to its prior knowledge.
This overfitting hampers the model’s ability to learn global
features and achieve optimal convergence. Notably, this is-
sue persists even when using a significantly smaller learning
rate than in pretraining, suggesting that simply reducing the
learning rate is insufficient to resolve the problem.
As a solution, we found that applying KL-divergence loss
(Kullback and Leibler 1951) can address this issue. Unlike
the language modeling loss, KL-divergence loss measures
the difference between two probability distributions, offer-
ing richer signals that help mitigate overfitting in the early
stages of training. Consequently, we employ the following
loss function:
Llogit=DKL(pθt||pθs) =1
B×SB×seqX
i=1pθt(xi) logpθt(xi)
pθs(xi),
L=αLlogit+ (1−α)Ltask, (4)
where Ltaskis the cross-entropy loss, Bis the batch size, S
is the sequence length, and pθtandpθsare the probability
distributions of the teacher and student models, respectively.
Figure 2: ( Upper ) The Wikitext perplexity curve for retrain-
ing GPT2 with and without knowledge distillation. ( Lower )
The training loss curve for pretraining and retraining.
We observe that the Retraining Dilemma is more pro-
nounced in smaller models; therefore, we typically apply a
larger αfor these models. We also experimented with vari-
ous distillation methods that utilize intermediate information
for distillation. However, we found that adding constraints
to intermediate outputs hinders the model’s generalization
ability and leads to undesirable outcomes.
3.4 Sparse Low-Rank Boosting
The pruned model with 2:4 sparsity, which retains only half
of the parameters under strict structural constraints, expe-
riences a reduction in expressive capacity. To mitigate this,
we incorporate a LoRA (Low-Rank Adaptation) (Hu et al.
2022) shape parameter that is trained alongside the origi-
nal parameters, helping to bridge the performance gap be-
tween dense and sparse models with minimal memory over-
head. Unlike traditional LoRA fine-tuning, where the orig-
inal model parameters are frozen and only the adapters are
trained, our approach trains both the sparse parameters and
the adapter weights simultaneously. This approach recog-
nizes that while the classic LoRA method freezes the origi-
nal model to prevent overfitting and catastrophic forgetting
during downstream tasks, our method enhances generaliza-
tion by training both the additional and original weights on
a pretraining dataset, thereby enabling concurrent training.
Another notable aspect of our method is the initializa-
tion strategy. Classic LoRA typically employs random ini-
tialization techniques, such as Xavier initialization (Glorot
and Bengio 2010) or Kaiming initialization (He et al. 2015).
However, in our approach, we leverage the pruned weights
as additional information to initialize the adapter weights,
thereby accelerating the training process. Given that in 2:4
sparsity, each neuron can only access half of the inputs, we
find that incorporating additional information to retain the
weight’s first-order information helps preserve the model’s

--- PAGE 5 ---
Figure 3: Visualization of SLoRB Initialization : Consider
a weight matrix of size 3 by 8, with k= 4. Weight Sijis
broadcasted within group Gijafter multiplication with the
projection matrix.
capacity. Specifically, for a weight matrix Wof size N×d
and its corresponding mask matrix M, we select the rank r
asd
k, where kcan be 64, 32, 16, and so on. A smaller kim-
proves performance but also increases memory usage. We
select a projection matrix Xof sized
k×dand a SLoRB
weight matrix Sof size N×d
k. The matrices XandSare
defined as follows:
xij=1,ifi·k≤j≤(i+ 1)·k−1,
0,otherwise ,(5)
Sij=1
k(j+1)·k−1X
p=j·kWip· ¬Mip. (6)
We define Group Gijas the elements from j·kto
(i+ 1)·k−1in row i. As illustrated in Fig 3, each
SLoRB weight Sijis broadcast within Group Gij. By set-
tingSijas the mean of all pruned weights in Group Gij,
this design ensures that the total mean of groups Gijre-
mains consistent after pruning. Although different initializa-
tion methods may ultimately converge to similar outcomes
given sufficient training steps, our method converges more
rapidly, making it particularly advantageous when the com-
putational budget is limited, as demonstrated in Section 4
of the Appendix. We refer to this method as Sparse Low-
Rank Boosting (SLoRB) , which is an optional technique
that trades off memory overhead for enhanced performance.
The complete pseudo-code for our method is provided in Al-
gorithm 1.
4 Experiments
4.1 Experiment Setup
Model Configurations. We report the performance of
Adaptive Sparse Trainer (AST) across three different LLM
model families: LLaMA2-7B (Touvron et al. 2023), OPT
(Zhang et al. 2022), and GPT2 (Brown et al. 2020). We
present results for two different sparsity patterns: AST-
Naive, which adheres to a strict 2:4 sparsity pattern with-
out additional weights, and AST-Boosted, which incorpo-
rates extra SLoRB weights. For AST-Boosted models, we
selected k= 16 , introducing an additional 12.5% of pa-
rameters. Optimal hyperparameters were identified through
a grid search, with the specific hyperparameters and training
details provided in Section 3 of the Appendix.Algorithm 1: Training Process for AST
Input : Weight of linear layers W(0); Mask update frequency
∆t; Total training iteration T0; SLoRB parameter k; Decay
increase iteration T1;
1:forW(0)∈ W(0)do
2: Initialize the mask m(W(0))based on magnitude;
3: ifUse SLoRB then
4: Initialize SLoRB weight S(0)∈ S(0)andX(0)∈
X(0)by Equation 5 and 6;
5: end if
6:end for
7:fort = 1,2,.. T0do
8: iftmod ∆ t==0 then
9: forW(t)∈ W(t)do
10: Update model mask mask m(W(t));
11: end for
12: end if
13: Compute decay for term λW(t)from Equation 2;
14: Compute gradient g(W(t))by back-propagation on
distillation loss Lfrom Equation 4;
15: Update weight W(t)by Equation 3;
16: ifUse SLoRB then
17: Update S(t)andX(t)by gradient;
18: end if
19:end for
20:return the pruned model.
Data. For training smaller models like the OPT and
GPT2 model families, we utilized the C4 dataset (Raffel
et al. 2020). For the LLaMA2-7B model, we employed
a more comprehensive dataset, RedPajama-v11, which en-
compasses data from seven domains: CommonCrawl, C4,
GitHub, Wikipedia, Books, ArXiv, and StackExchange. Ad-
ditionally, we leveraged the dynamic batching feature pro-
vided in the ShearedLLaMA (Xia et al. 2024) codebase.
Baseline. For the GPT2 and OPT models, we compare
our methods with both training-free and training-based ap-
proaches. Among training-free methods, we include com-
parisons with SparseGPT (Frantar and Alistarh 2023) and
Wanda (Sun et al. 2023). For training-based methods, given
the lack of existing results for retraining generative language
models with N:M sparsity, we implemented iterative magni-
tude pruning (Frankle and Carbin 2019) and gradual mag-
nitude pruning (Kurtic and Alistarh 2022), both of which
can be adapted to achieve the desired sparsity. To ensure
a fair comparison in terms of computational cost, we re-
port results for training-based methods that include distil-
lation, as we have observed that incorporating distillation
significantly enhances performance. Due to computational
constraints, we report the results of training-based baselines
only for the GPT2 and OPT models.
For the LLaMA2-7B model, we compare our approach
against strong dense pre-trained LLMs with a similar num-
ber of non-zero parameters. Additionally, we include a com-
1https://huggingface.co/datasets/togethercomputer/RedPajama-
Data-1T

--- PAGE 6 ---
Table 1: Perplexity results on raw-WikiText2 on 2:4 sparsified language models. AST outperforms both training-free and other
training-based methods with similar computational costs.
OPT GPT2
Method Training 125M 350M 1.3B 124M 350M 774M 1.5B
Dense - 27.76 22.00 14.62 29.95 21.72 19.43 17.40
SparseGPT % 45.58 40.33 29.03 50.09 31.03 25.98 21.14
Wanda % 60.91 50.16 23.92 115.64 63.71 49.97 30.44
Iterative Magnitude Pruning " 38.37 30.29 23.94 40.08 29.86 24.31 20.83
Gradual Magnitude Pruning " 31.51 25.98 16.78 33.48 24.83 22.01 18.96
AST-Naive(Ours) " 30.22 24.65 15.85 32.23 23.65 21.29 18.33
AST-Boosted(Ours) " 28.68 24.03 15.43 31.13 23.03 20.66 18.01
Table 2: Accuracy (%) of various open-sourced models on seven zero-shot tasks.
Models (Sparsity Pattern) Parameters BoolQ RTE HellaSwag WinoGrande ARC-e ARC-c OBQA Mean
LLaMA2-7B (Dense) 6.7B 77.73 63.89 57.18 69.04 76.17 42.91 31.60 59.78
Sheared-LLaMA-2.7B (Dense) 2.7B 65.99 50.54 51.21 65.03 67.29 33.27 28.80 51.73
INCITE-Base-3B (Dense) 2.8B 67.40 52.34 47.91 62.98 67.68 31.74 27.60 51.09
Open-LLaMA-3B-v2 (Dense) 3.4B 65.89 55.69 52.12 63.59 68.34 34.32 26.00 52.13
LLaMA-7B (Sparse) 3.4B 73.21 61.34 54.86 66.18 70.24 35.68 31.80 56.19
LLaMA2-7B (Sparse) 3.4B 73.12 66.06 54.66 67.87 73.61 39.93 28.60 57.68
LLaMA2-7B (Sparse+SLoRB) 4.2B 75.04 66.06 55.24 68.48 74.91 41.11 29.40 58.62
parison with the LLaMA-7B 2:4 sparsity model provided in
the Wanda (Sun et al. 2023) paper.
Evaluation. We evaluated the WikiText-2 perplexity for
all models and assessed zero-shot and few-shot performance
on LLaMA2-7B using EleutherAI’s LM Harness (Gao et al.
2021). Our evaluation tasks included zero-shot ARC Easy
(Clark et al. 2018), OpenBookQA (Mihaylov et al. 2018),
WinoGrande (Sakaguchi et al. 2021), RTE from the GLUE
benchmark (Wang et al. 2018), HellaSwag (Zellers et al.
2019), ARC Challenge (Clark et al. 2018), BoolQ (Clark
et al. 2019), as well as more complex tasks like the MMLU
benchmark (Hendrycks et al. 2021a), GSM8K (Cobbe et al.
2021), and MATH (Hendrycks et al. 2021b).
4.2 Main Results
Language Modeling In Table 1, we compare the perplex-
ity results of our sparse models with those of the baselines.
Our methods significantly outperform both training-free and
training-based approaches across various models, achieving
results close to those of the dense baseline. A plausible rea-
son why training-based methods like IMP fail is that while
a lottery ticket may exist for a randomly initialized model,
this may not be the case for a well-trained model. As for the
gradual baseline, applying a proportion of training steps at
low sparsity may not provide the model with sufficient time
to converge.
Zero-shot Result In Table 2, we present the accuracy re-
sults of seven zero-shot tasks for AST and baseline mod-
els. Our method performed best in most of the tasks. It
should be noted that our sparse models consistently outper-
form smaller dense models with a similar parameter count,suggesting a promising approach for obtaining parameter-
efficient models. Moreover, our method requires only a min-
imal amount of training tokens to converge. For example,
when training the LLaMA2-7B model, we utilized only 7B
tokens, which is less than 0.4% of those used in pretrain-
ing, making AST applicable to open-source models.
Our method can also be seamlessly adapted to current
quantization techniques to achieve extremely compressed
models with minimal performance drop. We provide results
using AWQ quantization methods in section 8 of the Ap-
pendix.
4.3 Additional Results for LLaMA2-7B
Recent findings (Nowak et al. 2024) have shown that previ-
ous pruning methods suffer significant performance degra-
dation in knowledge-intensive tasks. To address this, we
tested our LLaMA2-7B model on more complex tasks. As
shown in Table 3, our model preserves most of the knowl-
edge from the dense model and continues to perform well
in knowledge-intensive tasks compared with previous prun-
ing methods. This provides strong evidence that 2:4 pruned
models possess considerable potential, contrary to the obser-
vations in previous studies.
4.4 Ablation Study
Training Sparse Models. Our AST-Naive method em-
ployed distillation, Annealing SR-STE, and adaptive mask-
ing during training. In Table 4, we present an ablation study
to assess the effects of each of these components. For a
fair comparison, we apply additional training steps to non-
distillation methods. Specifically, we analyze the impact of

--- PAGE 7 ---
Table 3: Results of perplexity and knowledge-intensive tasks
for LLaMA2-7B models with 2:4 sparsity. The symbols ↓
and↑indicate that lower and higher values are better, re-
spectively. (LoRA methods are finetuned with r=64)
LLaMA2-7B Dense Wanda AST-Naive AST-Boosted
Perplexity ↓ 5.12 11.02 5.82 5.69
MMLU (5-shot)↑ 45.3 27.6 37.9 38.2
MATH (4-shot)↑ 5.38 2.86 4.42 4.64
GMS8K (LoRA Finetuned) ↑ 40.3 32.1 35.6 36.2
training without distillation, using a naive SR-STE decay
factor, and fixing the mask during training, each component
in isolation. Additionally, we provide results for the naive
training baseline in Figure 1 mentioned above. We demon-
strate that all three approaches contribute to performance
gains compared to naive training across various models.
Table 4: Ablation study of different methods in training
sparse models.
GPT OPT
Method 124M 350M 774M 125M
Dense 29.95 21.72 19.43 27.76
Naive Training 40.34 29.79 28.65 39.46
No distillation 39.29 29.08 27.21 36.97
Static SR-STE 32.84 24.04 21.73 31.08
Fixed Mask 32.93 24.18 21.95 31.06
AST-Naive(Ours) 32.23 23.65 21.29 30.22
Distillation Functions We also conducted ablation studies
on different distillation functions used in previous work, in-
cluding TinyBERT (Jiao et al. 2020), MobileBERT (Sun
et al. 2020), and Sparse-Finetuning (Kurtic et al. 2023),
which use attention and hidden states for distillation. Addi-
tionally, we examined MiniLLM (Gu et al. 2024), which em-
ploys reverse KL divergence. We find that using intermedi-
ate information during the distillation of generative language
models is detrimental, and that KL loss is sufficient for op-
timal performance. Reverse-KL yields performance similar
to that of forward KL. Detailed descriptions of the distilla-
tion loss functions for each of these methods are provided in
Section 5 of the Appendix.
4.5 Speedup
Table 6 presents speedup results obtained using TensorRT-
LLM2. We evaluate the actual end-to-end decoding speedup
on two GPU architectures with the LLaMA2-7B 2:4 sparse
model. We employ throughput, measured as the number of
tokens processed per second, as the primary evaluation met-
ric. Across a range of input and output lengths, the 2:4
sparse model demonstrates an overall acceleration of 1.33×
to 1.83× compared to its dense counterpart, highlighting its
potential for practical deployment.
2https://github.com/NVIDIA/TensorRT-LLMTable 5: Ablation study on different distillation loss for train-
ing sparse models.
GPT OPT
Method 124M 350M 774M 125M
Dense 29.95 21.72 19.43 27.76
TinyBERT 42.75 33.35 30.86 39.39
MobileBERT 44.87 31.67 29.75 40.33
Sparse-Fintuning 41.19 29.42 26.19 38.96
MiniLLM 32.20 23.64 21.31 30.24
KL Loss(Ours) 32.23 23.65 21.29 30.22
Table 6: Speedup results using TensorRT-LLM on RTX4090
and L20 GPUs with different input and output sequence
lengths, measured by throughput (tokens/s).
RTX4090
Inp Len, Out Len Sparse Dense Speedup
128, 128 70.23 52.94 1.33x
128, 1024 69.11 52.00 1.33x
1024, 128 68.06 51.10 1.33x
1024, 1024 67.41 50.37 1.34x
L20
Inp Len, Out Len Sparse Dense Speedup
128, 128 54.75 29.86 1.83x
128, 1024 53.81 29.57 1.82x
1024, 128 52.49 29.18 1.80x
1024, 1024 51.64 28.94 1.78x
5 Conclusion
In this paper, we introduce the Adaptive Sparse Trainer
(AST), a novel and efficient training pipeline for semi-
structured sparse models. AST effectively narrows the pre-
cision gap between dense and sparse LLMs in terms of
perplexity and accuracy on zero-shot tasks, while keeping
training costs minimal. Our results demonstrate that prun-
ing LLMs is feasible with minimal performance loss on
knowledge-intensive tasks, and that large semi-structured
sparse models can outperform dense models of similar de-
coding speed when supported by the appropriate software
and hardware. Although our findings contribute to advanc-
ing the retraining of pruned models with billions of param-
eters, we acknowledge that the limited number of training
tokens, due to computational constraints, remains an area
for future exploration. Expanding this work to larger mod-
els or increasing the number of training tokens could pro-
vide valuable insights and further enhance the effectiveness
of our methods.

--- PAGE 8 ---
Acknowledgements
The authors would like to thank Ziteng Wang, Bingrui Li
and Pengle Zhang for helpful discussions and suggestions
on code implementation. This work was supported by NSFC
Project (No. 62376131).
6 Appendix
6.1 Compressing Models with Different
Semi-structured Sparsity Pattern
In previous sections, we only examined 2:4 patterns, but the-
oretically, any N:M pattern can enhance matrix multiplica-
tion with appropriate hardware support. With that said, pat-
terns with looser constraints (i.e., a larger M) often perform
better but are more challenging to compress effectively. We
aim to identify the optimal trade-off and. To maintain fair-
ness in comparison, we focus only on patterns that achieve a
50% sparsity ratio, such as 2:4, 4:8, etc.
It should be noted that deploying semi-structured sparse
models can lead to potential memory overhead. Take the 2:4
sparsity pattern as an example: although only two non-zero
elements out of every four weights need to be stored, addi-
tional positional information is required to correctly restore
these compressed weights to the original matrix for calcula-
tions. Storing these indices will consume an additional pro-
portion of memory. A naive method would be to store one
bit as a mask for each element, which results in a 50% mem-
ory overhead with a 4-bit quantized model at 50% sparsity.
However, a more efficient approach exists: since there are
2 non-zero elements, there are a total of 4
2
= 6 possible
positions, hence only an additional 3 bits are sufficient.
We define the compression ratio as the percentage of
memory latency of the compressed model compared to the
original model. In our setting, we compress dense mod-
els with FP32 precision to a 4-bit model with n:2nsemi-
structured sparsity; therefore, the compression ratio can be
formulated as:
Cn=n∗4 +⌈log2 2n
n
⌉(bit)
2n∗32(bit)=1
16+⌈log2 2n
n
⌉
64n.
Through mathematical formulation (provided in Section 2 of
the Appendix), we can provide an approximate upper bound
for the compression ratio as well as a predicted actual com-
pression ratio when extending to sparsity patterns with larger
NandM
log22n
n
= log2(2n)!
(n!)2≤log24n
√πn= 2n−log2√πn.
If we remove the ceiling function for approximation:
Cn≈3
32−log2√πn
64n.
Therefore, the compression ratio is approximately an in-
creasing function with an upper bound of C∗=3
32≈
9.375% . In practice, to further enhance compression, we
can employ Huffman Encoding, similar to what was done
in Deep Compression (Han, Mao, and Dally 2016). We con-
struct a Huffman Tree with evenly distributed frequencies, asempirical results support our claim. The actual compression
ratios are shown in Table 7. We observe that as nincreases,
performance improves; however, although the compression
ratio increases, it remains below the upper bound.
6.2 Proof for Upper Bound of Combination
Number
Proofs are sourced from Stack Exchange3. We have the fol-
low equation:
1
4n2n
n
=(2n−1)!!
(2n)!!=nY
k=1(1−1
2k).
We square both sides of the equation:
(1
4n2n
n
)2=1
4nY
k=2(1−1
2k)2=1
4nn−1Y
k=1(1−1
(2k+ 1)2)−1.
Using the Weierstrass product for the cosine function, we
obtain:
∞Y
k=1(1−1
(2k+ 1)2)−1=4
π.
Hence, it follows that:
(1
4n2n
n
)2=1
πnnY
k≥n(1−1
(2k+ 1)2) =1
πnnY
k≥n(1 +1
(2k+ 2)2 k)−1.
Therefore we have
1√πn≥1
4n2n
n
.
6.3 Hyperparamters
We present the hyper-parameters and the number of tokens
used to train our model in Table 8. The increase iteration
number T0is selected to be one-fourth of the total training
iteration T1.
6.4 SLoRB Initialization
We conducted an ablation study on various initialization
methods for SLoRB. We report the results on GPT2 model
under specified computation limits of 1B, 2.5B, and 5B to-
kens, as shown in Table 9. ’Mean’ refers to the initializa-
tion method discussed in the paper, while ’Zero’ indicates
the initialization of both matrices from zero. Our initializa-
tion method assists in maintaining model performance at the
outset, thereby accelerating convergence.
6.5 Different Distillation Function
In this section, we formally present the loss functions ap-
plied in our ablation study, focusing on different distillation
methods. We include results from previous works such as
TinyBERT (Jiao et al. 2020), MobileBERT (Sun et al. 2020),
Sparse-Finetuning (Kurtic et al. 2023), and MiniLLM (Gu
et al. 2024).
3https://math.stackexchange.com/questions/1442351/stirling-
approximation-of-binom2nn

--- PAGE 9 ---
Table 7: Comparison of perplexity and compression ratio of different sparsity pattern.
Sparsity Pattern 1:2 2:4 4:8 8:16 16:32 32:64Unstructured
50%
Perplexity 32.56 31.13 30.73 30.37 30.34 30.32 30.18
Actual Compression
Ratio7.81% 8.33% 8.66% 8.93% 9.10% 9.22% -
Table 8: Summary of hyperparameters and the number of tokens used for training.
OPT GPT2 LLAMA2
125M 350M 1.3B 124M 350M 774M 1.5B 7B
Learning Rate 1e-4 1e-4 2e-5 2e-4 1e-4 1e-4 6e-5 2e-5
Maximal Decay Factor 3e-4 1e-4 6e-5 1e-4 6e-5 6e-5 6e-5 6e-5
Training Steps 40k 40k 20k 40k 40k 40k 20k 15k
Total Flipped Ratio 11.1% 9.3% 4.4% 6.7% 3.6% 3.4% 4.2% 3.8%
Kl loss parameter 2/3 2/3 2/3 2/3 2/3 2/3 2/3 1/3
Tokens Trained 10B 10B 5B 5B 5B 5B 2.5B 7.5B
Table 9: Results of perplexity using different initialization
methods with fixed training tokens.
Trained Token 0 1B 2.5B 5B
Mean 475.05 32.78 31.78 31.13
Xavier Uniform 720.74 33.22 31.82 31.15
Zero 720.74 33.43 32.93 32.46
TinyBERT In TinyBERT, we utilize information from in-
termediate multi-head attentions and hidden states using the
mean square error (MSE) loss. Specifically, the attention
loss is defined as:
Lattn=1
hhX
i=1MSE(AS
i, AT
i), (7)
where AS
iandAT
irepresent the layer-wise attention outputs
of head ifrom the student model and the teacher model, re-
spectively.
The hidden state loss is defined as:
Lhidn=MSE(HSWh, HT), (8)
where HSandHTrepresent the layer-wise hidden states of
the student model and teacher model, respectively. Here, Wh
is a learnable projection matrix. However, since the shapes
of the hidden states are identical in our settings, we fix Wh=
I.
TinyBERT also uses cross-entropy loss:
Lpred=CE(zT/τ, zS/τ), (9)
where zTandzSare the output logits of the teacher and stu-
dent models, respectively. We do not utilize embedding dis-
tillation, as the embeddings are well-trained and not pruned.The distillation function for each layer is defined as:
Llayer =Lpred if it is the last layer ,
Lhidn+Lattn otherwise .(10)
The final loss function is the sum of the layer-wise loss
functions.
MobileBERT MobileBERT is designed for distilling
BERT by utilizing information from feature maps and atten-
tion. The feature map distillation in MobileBERT is identi-
cal to that used in TinyBERT, as shown in Equation 8. For
attention knowledge transfer at layer l, the loss function is
defined as:
Ll
AT=1
ATTX
t=1AX
a=1DKL(atr
t,l,a∥ast
t,l,a), (11)
where Ais the number of attention heads, Tis the sequence
length, and at,l,arepresents the attention output for layer l,
heada, and token t. The total distillation loss is the sum of
the feature map loss and attention transfer.
Sparse-Finetuning Sparse-Finetuning employs both
SquareHead Loss and KL-divergence loss. It measures
the intermediate representation using mean squared error
(MSE):
Lfeat=MSE(fl
t, fl
s)
MSE(fl
t,0), (12)
where fl
tandfl
sare the hidden states of the l-th layer at step
tfor the teacher and student models, respectively.
KL-divergence loss (Kullback and Leibler 1951) is used
to measure the difference in output probability distributions
between the student and teacher models:
Llogit=DKL(pθt∥pθs), (13)

--- PAGE 10 ---
Table 10: Wikitext perplexity for quantized sparse models.
LLAMA2 OPT GPT2
MethodTheoretical
Compression Rate7B 125M 1.3B 124M 350M 774M 1.5B
Dense 1.0x 5.12 27.76 14.62 29.95 21.72 19.43 17.40
AWQ-4bit 0.125x 5.68 29.12 14.95 31.93 22.66 19.89 17.80
AWQ-2bit 0.0675x 2.2e5 216.11 53.42 751.15 453.45 70.36 46.17
AST-Naive-8bit
(Ours)0.125x 6.37 30.26 15.86 32.36 23.66 21.29 18.34
AST-Naive-4bit
(Ours)0.0675x 6.48 31.28 16.05 34.00 24.39 21.72 18.60
AST-Boosted-4bit
(Ours)0.078x 6.25 29.88 15.63 32.76 23.87 21.40 18.31
where pθtandpθsare the probability distributions of the
teacher and student models, respectively.
The total distillation loss is the weighted sum:
L=α1Llogit+α2Lfeat. (14)
MiniLLM MiniLLM uses reverse KL-divergence instead
of forward KL. Specifically, the loss function is defined as:
Llogit=DKL(pθs∥pθt), (15)
where pθtandpθsare the probability distributions of the
teacher and student models, respectively.
6.6 Adding Decay for AdamW Optimizer
Recent work (Hu et al. 2024) has shown that applying SR-
STE with momentum-based optimizers can weaken the ef-
fectiveness of the decay term, resulting in suboptimal perfor-
mance. In earlier implementations, SR-STE added the decay
term directly to the weight. Specifically, we denote the gra-
dient with respect to weight Wtat iteration tasgtwhich is
further adjusted using the AdamW optimizer, followed by
the weight update using:
AdamW (gt) =(utβ1+ (1−β1)gt)
(1−βt
1)(√vt+ϵ)(16)
Wt+1←Wt−γt
AdamW (gt) +λW(m(Wt)⊙Wt)
.
(17)
where utandvtare the first-order and second-order mo-
menta of gt, respectively. γtis the current learning rate.
However, results indicate that this approach causes fre-
quent mask oscillations details can be found in previous
work (Hu et al. 2024). To mitigate this issue, the decay term
is instead applied directly to the weight using the following
update rule:
˜gt←gt+λW(m(Wt)⊙Wt), (18)
Wt+1←Wt−γtAdamW ( ˜gt), (19)
We make further improvement to this method, since the
decay term is only dependent to the parameter’s current
value it should not be entangled with first order momentum.
Therefore we employ the following update role:ut=ut−1β1+ (1−β1)gt−1 (20)
˜ut=ut+λW(m(Wt)⊙Wt) (21)
Furthermore we use the decayed gradient to calculate the
second-order momentum and update weights accordingly.
vt=vt−1β2+ (1−β2) ˜ut−12(22)
Wt+1←Wt−γt˜ut
(1−βt
1)(√vt+ϵ). (23)
6.7 Mask Oscillation Behavior During Training
Several metrics have been introduced to measure the stabil-
ity of masks during training, such as flip rate (Hu et al. 2024)
and SAD (Zhou et al. 2021). In this paper, we primarily fo-
cus on measuring the flip rate and initial flip rate, which
quantify the mask changes between the current and previ-
ous steps, as well as between the current and initial steps,
respectively.
rt=||m(Wt)−m(Wt−1)||1/D
it=||m(Wt)−m(W0)||1/D
where Drepresents the total number of parameters in the
linear layer. The flip rate reflects the stability of the model
masks, while the initial flip rate indicates the overall extent
of weight removal and revival.
We compare the flip rates and initial flip rates of static SR-
STE and Annealing SR-STE. In our experiments, the mask
is updated, and the flip rate and initial flip rate are calculated
every 10 batches during training, as more frequent recalcu-
lations do not improve accuracy and only increase computa-
tional overhead. Figure 4 illustrates the flip rates and initial
flip rates during the retraining of the GPT2 model. Unlike
traditional static decay factors, Annealing SR-STE modifies
a higher percentage of the mask at the beginning, allowing
the model to explore various mask patterns. Additionally,
Annealing SR-STE enables a more stable mask towards the
end of training, promoting better convergence. As a result,
Annealing SR-STE supports a higher rate of mask changes
(e.g., initial flip rate) while maintaining overall stability.

--- PAGE 11 ---
Figure 4: (Upper) Flip rate for static and Annealing SR-STE
during the training of the GPT2 model. (Lower) Initial flip
rate for static and Annealing SR-STE during the training of
the GPT2 model.
6.8 Model Compression Using A WQ
In this section, we present the perplexity results for quan-
tized models using AWQ. As shown in Table 10, our model
maintains its performance even under extreme compression.
For the AST-Boosted model, we also quantized the addi-
tional SLoRB parameters.
6.9 Frequently Asked Questions
This section examines and addresses key concerns raised in
prior discussions.
Justification of Knowledge Distillation One concern is
that the use of distillation may introduce additional com-
putational costs. However, we have found that, under the
same total FLOPs, distillation methods still outperform non-
distillation methods. We present a performance comparison
under identical computational costs, and the results in Table
11 demonstrate that the distilled model significantly outper-
forms the non-distilled model.
Table 11: Perplexity for distillation and distillation-free
methods under the same computational cost.
Module Tokens Total(TFLOPs) PPL
LLaMA2 w/ Distil 5B 3e8 5.97
LLaMA2 w/o Distil 7.5B 3e8 6.12Justification for SLoRB One concern is the timing of
training the additional SLoRB component. Through exper-
imentation, we observed that fine-tuning the LoRA com-
ponent after initial training did not yield further perfor-
mance improvements. We hypothesize that this is because
the model has already converged on the pretraining dataset,
making it difficult to further enhance performance through
fine-tuning the LoRA module. In contrast, keeping both the
model and LoRA modules unfrozen during retraining offers
greater flexibility and potential for improvement. To sup-
port this hypothesis, we present the perplexity results for the
GPT2 model in Table 12.”
Table 12: SLoRB ablation results on GPT2.
Model SLoRB w/o LoRA Finetune Aft. Retrain Dense
GPT2 31.13 32.23 32.07 29.95
Mask Selection We present computational complexity
and perplexity results for different mask selection criteria,
such as the Wanda and SparseGPT metrics, in Table 13. The
results demonstrate that the magnitude metric outperforms
the others in both performance and speed.
Table 13: Wikitext perplexity for quantized sparse models.
Metric PPL Complexity
Magnitude 32.23 O( h2)
SparseGPT 32.58 O( h3)
Wanda 32.51 O( h2)
Effects of αWe provide results using different methods
on full GPT2 retraining and LLaMA2 retraining with lim-
ited training tokens in Table 14. When α= 1, meaning we
only use KD loss, we observe slightly better performance
on smaller models. However, it performs less effectively on
larger models like LLaMA2-7B, where tuning αleads to
improvements in performance. This suggests that αplays a
more critical role in optimizing results for larger-scale mod-
els.
Table 14: Wikitext perplexity for different αon LLaMA2-
7B and GPT2 model.
Model Tokens Trained α= 0 α=1
3α=2
3α= 1
LLaMA2-7B 3B 6.26 6.25 6.22 6.62
GPT2 5B 32.18 32.23 33.57 40.34
References
Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;
Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.;
Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv
preprint arXiv:2303.08774 .

--- PAGE 12 ---
Bengio, Y .; L ´eonard, N.; and Courville, A. 2013. Estimat-
ing or propagating gradients through stochastic neurons for
conditional computation. arXiv preprint arXiv:1308.3432 .
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners.
NeurIPS , 1877–1901.
Clark, C.; Lee, K.; Chang, M.-W.; Kwiatkowski, T.; Collins,
M.; and Toutanova, K. 2019. BoolQ: Exploring the surpris-
ing difficulty of natural Yes/No questions. In ACL, 2924–
2936.
Clark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.;
Schoenick, C.; and Tafjord, O. 2018. Think you have solved
question answering? Try arc, the ai2 reasoning challenge.
arXiv preprint arXiv:1803.05457 .
Cobbe, K.; Kosaraju, V .; Bavarian, M.; Chen, M.; Jun, H.;
Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.;
et al. 2021. Training verifiers to solve math word problems.
arXiv preprint arXiv:2110.14168 .
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
Bert: Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805 .
Evci, U.; Gale, T.; Menick, J.; Castro, P. S.; and Elsen, E.
2020. Rigging the lottery: Making all tickets winners. In
ICML , 2943–2952.
Frankle, J.; and Carbin, M. 2019. The lottery ticket hypoth-
esis: Finding sparse, trainable neural networks. In ICLR .
Frantar, E.; and Alistarh, D. 2023. Sparsegpt: Massive lan-
guage models can be accurately pruned in one-shot. In
ICML , 10323–10337.
Gao, L.; Tow, J.; Biderman, S.; Black, S.; DiPofi, A.; Fos-
ter, C.; Golding, L.; Hsu, J.; McDonell, K.; Muennighoff,
N.; et al. 2021. A framework for few-shot language model
evaluation. Version v0. 0.1. Sept , 8.
Glorot, X.; and Bengio, Y . 2010. Understanding the diffi-
culty of training deep feedforward neural networks. In AIS-
TATS , 249–256.
Gu, Y .; Dong, L.; Wei, F.; and Huang, M. 2024. MiniLLM:
Knowledge distillation of large language models. In ICLR .
Guo, J.; Wu, J.; Wang, Z.; Liu, J.; Yang, G.; Ding, Y .; Gong,
R.; Qin, H.; and Liu, X. 2024. Compressing large language
models by joint sparsification and quantization. In ICML .
Han, S.; Mao, H.; and Dally, W. J. 2016. Deep compression:
Compressing deep neural networks with pruning, trained
quantization and huffman coding. In ICLR .
Han, S.; Pool, J.; Tran, J.; and Dally, W. J. 2015. Learning
both weights and connections for efficient neural networks.
InNeurIPS , 1135–1143.
Hassibi, B.; Stork, D. G.; and Wolff, G. J. 1993. Optimal
brain surgeon and general network pruning. In ICNN , 293–
299.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Delving deep
into rectifiers: Surpassing human-level performance on ima-
genet classification. In ICCV , 1026–1034.Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;
Song, D.; and Steinhardt, J. 2021a. Measuring massive mul-
titask language understanding. In ICLR .
Hendrycks, D.; Burns, C.; Kadavath, S.; Arora, A.; Basart,
S.; Tang, E.; Song, D.; and Steinhardt, J. 2021b. Measur-
ing Mathematical Problem Solving With the MATH Dataset.
arXiv:2103.03874.
Hu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,
S.; Wang, L.; and Chen, W. 2022. LoRA: Low-rank adapta-
tion of large language models. In ICLR .
Hu, Y .; Zhao, K.; Huang, W.; Chen, J.; and Zhu, J. 2024.
Accelerating transformer pre-training with 2:4 sparsity. In
ICML .
Hubara, I.; Chmiel, B.; Island, M.; Banner, R.; Naor, J.; and
Soudry, D. 2021. Accelerated sparse neural training: A prov-
able and efficient method to find n: m transposable masks. In
NeurIPS , 21099–21111.
Jiao, X.; Yin, Y .; Shang, L.; Jiang, X.; Chen, X.; Li, L.;
Wang, F.; and Liu, Q. 2020. Tinybert: Distilling bert for
natural language understanding. In EMNLP , 4163–4174.
Kullback, S.; and Leibler, R. A. 1951. On information and
sufficiency. The Annals of Mathematical Statistics , 22(1):
79–86.
Kurtic, E.; and Alistarh, D. 2022. GMP*: Well-Tuned
Gradual Magnitude Pruning Can Outperform Most BERT-
Pruning Methods. arXiv preprint arXiv:2210.06384 .
Kurtic, E.; Kuznedelev, D.; Frantar, E.; Goin, M.; and Alis-
tarh, D. 2023. Sparse finetuning for inference acceleration
of large language models. In NeurIPS .
LeCun, Y .; Denker, J.; and Solla, S. 1990. Optimal brain
damage. In NeurIPS , 598–605.
Lee, N.; Ajanthan, T.; and Torr, P. H. 2019. SNIP: Single-
shot network pruning based on connection sensitivity. In
ICLR .
Lin, J.; Tang, J.; Tang, H.; Yang, S.; Dang, X.; and Han, S.
2024. Awq: Activation-aware weight quantization for LLM
compression and acceleration. In MLSys , 87–100.
Liu, Z.; Li, J.; Shen, Z.; Huang, G.; Yan, S.; and Zhang,
C. 2017. Learning efficient convolutional networks through
network slimming. In ICCV , 2736–2744.
Mihaylov, T.; Clark, P.; Khot, T.; and Sabharwal, A. 2018.
Can a suit of armor conduct electricity? a new dataset for
open book question answering. In EMNLP , 2381–2391.
Molchanov, P.; Mallya, A.; Tyree, S.; Frosio, I.; and Kautz,
J. 2019. Importance estimation for neural network pruning.
InCVPR , 11264–11272.
Nova, A.; Dai, H.; and Schuurmans, D. 2023. Gradient-free
structured pruning with unlabeled data. In ICML , 26326–
26341.
Nowak, A. I.; Grooten, B.; Mocanu, D. C.; and Tabor, J.
2024. Compressing LLMs: The truth is rarely pure and never
simple. In ICLR .
Paul, M.; Chen, F.; Larsen, B. W.; Frankle, J.; Ganguli, S.;
and Dziugaite, G. K. 2023. Unmasking the lottery ticket
hypothesis: What’s encoded in a winning ticket’s mask? In
ICLR .

--- PAGE 13 ---
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Ex-
ploring the limits of transfer learning with a unified text-
to-text transformer. Journal of Machine Learning Research ,
21(140): 1–67.
Renda, A.; Frankle, J.; and Carbin, M. 2020. Comparing
rewinding and fine-tuning in neural network pruning. In
ICLR .
Sakaguchi, K.; Bras, R. L.; Bhagavatula, C.; and Choi, Y .
2021. Winogrande: An adversarial winograd schema chal-
lenge at scale. Communications of the ACM , 64(9): 99–106.
Shen, M.; Yin, H.; Molchanov, P.; Mao, L.; Liu, J.; and Al-
varez, J. M. 2022. Structural pruning via latency-saliency
knapsack. NeurIPS , 12894–12908.
Shi, D.; Tao, C.; Jin, Y .; Yang, Z.; Yuan, C.; and Wang, J.
2023. Upop: Unified and progressive pruning for compress-
ing vision-language transformers. In ICML , 31292–31311.
Singh, S. P.; and Alistarh, D. 2020. Woodfisher: Efficient
second-order approximation for neural network compres-
sion. In NeurIPS , 18098–18109.
Sun, M.; Liu, Z.; Bair, A.; and Kolter, J. Z. 2023. A simple
and effective pruning approach for large language models.
InICML .
Sun, Z.; Yu, H.; Song, X.; Liu, R.; Yang, Y .; and Zhou,
D. 2020. Mobilebert: A compact task-agnostic bert for
resource-limited devices. In ACL, 2158–2170.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; et al. 2023. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288 .
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and
Bowman, S. R. 2018. GLUE: A multi-task benchmark and
analysis platform for natural language understanding. In
ICLR .
Wang, Z.; Wohlwend, J.; and Lei, T. 2019. Structured prun-
ing of large language models. In EMNLP , 6151–6162.
Wei, J.; Tay, Y .; Bommasani, R.; Raffel, C.; Zoph, B.;
Borgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler,
D.; Chi, E. H.; Hashimoto, T.; Vinyals, O.; Liang, P.; Dean,
J.; and Fedus, W. 2022. Emergent abilities of large language
models. In TMLR .
Xia, M.; Gao, T.; Zeng, Z.; and Chen, D. 2024. Sheared
llama: Accelerating language model pre-training via struc-
tured pruning. In ICLR .
Zellers, R.; Holtzman, A.; Bisk, Y .; Farhadi, A.; and Choi,
Y . 2019. HellaSwag: Can a machine really finish your sen-
tence? In ACL, 4791–4800.
Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;
Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; Mi-
haylov, T.; Ott, M.; Shleifer, S.; Shuster, K.; Simig, D.;
Koura, P. S.; Sridhar, A.; Wang, T.; and Zettlemoyer, L.
2022. OPT: Open Pre-trained Transformer Language Mod-
els. arXiv:2205.01068.
Zhang, Y .; Bai, H.; Lin, H.; Zhao, J.; Hou, L.; and Cannis-
traci, C. V . 2024a. Plug-and-play: An efficient post-training
pruning method for large language models. In ICLR .Zhang, Y .; Zhao, L.; Lin, M.; Sun, Y .; Yao, Y .; Han, X.; Tan-
ner, J.; Liu, S.; and Ji, R. 2024b. Dynamic sparse no training:
Training-free fine-tuning for sparse llms. ICLR .
Zhou, A.; Ma, Y .; Zhu, J.; Liu, J.; Zhang, Z.; Yuan, K.; Sun,
W.; and Li, H. 2021. Learning n:m fine-grained structured
sparse neural networks from scratch. In ICLR .
Zhou, Y .; Yang, Y .; Chang, A.; and Mahoney, M. W. 2023. A
three-regime model of network pruning. In ICML , 42790–
42809.
Zhu, M.; and Gupta, S. 2018. To prune, or not to prune:
Exploring the efficacy of pruning for model compression. In
ICLR .
