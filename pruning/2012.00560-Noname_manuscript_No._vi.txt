# 2012.00560.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2012.00560.pdf
# Kích thước tệp: 2322376 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Bản thảo không tên Số
(sẽ được chèn bởi biên tập viên)
Lựa chọn đặc trưng nhanh và mạnh mẽ: sức mạnh của
huấn luyện thưa thớt tiết kiệm năng lượng cho Autoencoder
Zahra Atashgahi Ghada Sokar Tim
van der Lee Elena Mocanu Decebal
Constantin Mocanu Raymond Veldhuis 
Mykola Pechenizkiy
Nhận: ngày / Chấp nhận: ngày
Tóm tắt Những biến chứng chính phát sinh từ sự gia tăng gần đây của lượng dữ liệu
có chiều cao, bao gồm chi phí tính toán cao và yêu cầu bộ nhớ. Lựa chọn đặc trưng,
nhằm xác định các thuộc tính liên quan và thông tin nhất của một tập dữ liệu, đã được
giới thiệu như một giải pháp cho vấn đề này. Hầu hết các phương pháp lựa chọn đặc
trưng hiện có đều không hiệu quả về mặt tính toán; các thuật toán không hiệu quả dẫn
đến tiêu thụ năng lượng cao, điều này không mong muốn đối với các thiết bị có tài
nguyên tính toán và năng lượng hạn chế. Trong bài báo này, một phương pháp mới và
linh hoạt cho lựa chọn đặc trưng không giám sát được đề xuất. Phương pháp này, có
tên QuickSelection1, giới thiệu sức mạnh của neuron trong mạng neural thưa thớt như
một tiêu chí để đo tầm quan trọng của đặc trưng. Tiêu chí này, kết hợp với autoencoder
khử nhiễu kết nối thưa thớt được huấn luyện với thủ tục huấn luyện tiến hóa thưa thớt,
rút ra tầm quan trọng của tất cả các đặc trưng đầu vào đồng thời. Chúng tôi triển khai
QuickSelection theo cách hoàn toàn thưa thớt trái ngược với cách tiếp cận điển hình
sử dụng mặt nạ nhị phân trên các kết nối để mô phỏng độ thưa thớt. Điều này dẫn đến
sự gia tăng tốc độ đáng kể và giảm bộ nhớ. Khi được thử nghiệm trên một số tập dữ
liệu chuẩn, bao gồm năm tập dữ liệu chiều thấp và ba tập dữ liệu chiều cao, phương
pháp được đề xuất có thể đạt được sự đánh đổi tốt nhất về độ chính xác phân loại và
phân cụm, thời gian chạy và sử dụng bộ nhớ tối đa, trong số các phương pháp được
sử dụng rộng rãi cho lựa chọn đặc trưng. Bên cạnh đó, phương pháp được đề xuất của
chúng tôi yêu cầu lượng năng lượng ít nhất trong số các phương pháp lựa chọn đặc
trưng dựa trên autoencoder tối tân.
Bài báo này đã được chấp nhận xuất bản trên Tạp chí Machine Learning (ECML-PKDD
2022 Journal Track)
Z. AtashgahiE. MocanuD.C. MocanuR.N.J. Veldhuis
Khoa Kỹ thuật Điện, Toán học và Khoa học Máy tính, Đại học Twente,
Enschede, 7500AE, Hà Lan
E-mail: z.atashgahi@utwente.nl
G.A.Z.N. Sokar1T. Lee2D.C. Mocanu1M. Pechenizkiy1
Đại học Công nghệ Eindhoven, 5600 MB Eindhoven, Hà Lan
1Khoa Toán học và Khoa học Máy tính2Khoa Kỹ thuật Điện
M. Pechenizkiy
Khoa Công nghệ Thông tin, Đại học Jyväskylä, 40014 Jyväskylä, Phần Lan
1Mã nguồn có sẵn tại: https://github.com/zahraatashgahi/QuickSelectionarXiv:2012.00560v2  [cs.LG]  13 Sep 2021

--- TRANG 2 ---
2 Zahra Atashgahi et al.
Từ khóa Lựa chọn đặc trưng Deep LearningSparse Autoencoders Sparse
Training
1 Giới thiệu
Trong vài năm qua, sự chú ý đáng kể đã được dành cho vấn đề giảm chiều và nhiều phương pháp đã được đề xuất [ 53]. Có hai kỹ thuật chính để giảm số lượng đặc trưng của tập dữ liệu có chiều cao: trích xuất đặc trưng và lựa chọn đặc trưng. Trích xuất đặc trưng tập trung vào việc biến đổi dữ liệu thành không gian có chiều thấp hơn. Phép biến đổi này được thực hiện thông qua một ánh xạ dẫn đến một tập hợp đặc trưng mới [ 40]. Lựa chọn đặc trưng giảm không gian đặc trưng bằng cách chọn một tập con của các thuộc tính gốc mà không tạo ra đặc trưng mới [12]. Dựa trên tính khả dụng của nhãn, các phương pháp lựa chọn đặc trưng được chia thành ba loại: có giám sát [ 2,12], bán giám sát [ 58,48], và không giám sát [43,16]. Các thuật toán lựa chọn đặc trưng có giám sát cố gắng tối đa hóa một số hàm độ chính xác dự đoán đã cho nhãn lớp. Trong học không giám sát, việc tìm kiếm các đặc trưng phân biệt được thực hiện một cách mù quáng, không có nhãn lớp. Do đó, lựa chọn đặc trưng không giám sát được coi là một vấn đề khó hơn nhiều [16].

Các phương pháp lựa chọn đặc trưng cải thiện khả năng mở rộng của các thuật toán học máy vì chúng giảm chiều của dữ liệu. Bên cạnh đó, chúng giảm nhu cầu ngày càng tăng về tài nguyên tính toán và bộ nhớ được giới thiệu bởi sự xuất hiện của dữ liệu lớn. Điều này có thể dẫn đến giảm đáng kể tiêu thụ năng lượng trong các trung tâm dữ liệu. Điều này có thể giảm bớt không chỉ vấn đề chi phí năng lượng cao trong các trung tâm dữ liệu mà còn các thách thức quan trọng được áp đặt lên môi trường [ 56]. Như được nêu bởi Nhóm Chuyên gia Cấp cao về Trí tuệ Nhân tạo (AI) [ 22], phúc lợi môi trường là một trong những yêu cầu của hệ thống AI đáng tin cậy. Việc phát triển, triển khai và quá trình của một hệ thống AI nên được đánh giá để đảm bảo rằng chúng sẽ hoạt động theo cách thân thiện với môi trường nhất có thể. Ví dụ, việc sử dụng tài nguyên và tiêu thụ năng lượng thông qua huấn luyện có thể được đánh giá.

Tuy nhiên, một vấn đề thách thức phát sinh trong lĩnh vực lựa chọn đặc trưng là việc chọn đặc trưng từ các tập dữ liệu chứa một số lượng lớn đặc trưng và mẫu, có thể yêu cầu một lượng lớn tài nguyên bộ nhớ, tính toán và năng lượng. Vì hầu hết các kỹ thuật lựa chọn đặc trưng hiện có được thiết kế để xử lý dữ liệu quy mô nhỏ, hiệu quả của chúng có thể bị giảm với dữ liệu có chiều cao [9]. Chỉ có một vài nghiên cứu tập trung vào việc thiết kế các thuật toán lựa chọn đặc trưng có hiệu quả về mặt tính toán [ 52,1]. Những đóng góp chính của bài báo này có thể được tóm tắt như sau:

Chúng tôi đề xuất một phương pháp lựa chọn đặc trưng không giám sát mới nhanh và mạnh mẽ, có tên QuickSelection. Như được phác thảo ngắn gọn trong Hình 1, Nó có hai thành phần chính:
(1) Lấy cảm hứng từ sức mạnh nút trong lý thuyết đồ thị, phương pháp đề xuất sức mạnh neuron của mạng neural thưa thớt như một tiêu chí để đo tầm quan trọng của đặc trưng; và (2) Phương pháp giới thiệu Autoencoder Khử nhiễu kết nối thưa thớt (sparse DAEs) được huấn luyện từ đầu với thủ tục huấn luyện tiến hóa thưa thớt để mô hình hóa phân phối dữ liệu một cách hiệu quả. Độ thưa thớt được áp đặt trước khi huấn luyện cũng giảm lượng bộ nhớ cần thiết và thời gian chạy huấn luyện.

Chúng tôi triển khai QuickSelection theo cách hoàn toàn thưa thớt trong Python sử dụng thư viện SciPy và Cython thay vì sử dụng mặt nạ nhị phân trên các kết nối

--- TRANG 3 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 3
Epoch 5
(c) Tính toán
Sức mạnh Neuron(d) Lựa chọn
Đặc trưngs1 = 0.2
s2 = 0.5
s5 = 0.3s4 = 0s3 = 1.3f 1
f 2
f 3
f 4
f 5f 2
f 3
(b) Huấn luyện Sparse-DAE(a) Khởi tạo
Sparse-DAEEpoch 10 Epoch 0
Hình 1: Tổng quan cấp cao về phương pháp được đề xuất, "QuickSelection". (a) Tại
epoch 0, các kết nối được khởi tạo ngẫu nhiên. (b) Sau khi khởi tạo cấu trúc thưa
thớt, chúng ta bắt đầu quy trình huấn luyện. Sau 5 epoch, một số kết nối thay đổi
trong quá trình huấn luyện, và kết quả là sức mạnh của một số neuron đã tăng hoặc
giảm. Tại epoch 10, mạng đã hội tụ, và chúng ta có thể quan sát neuron nào quan
trọng (vòng tròn xanh lớn hơn và đậm hơn) và neuron nào không. (c) Khi mạng đã
hội tụ, chúng ta tính toán sức mạnh của tất cả neuron đầu vào. (d) Cuối cùng, chúng
ta chọn K đặc trưng tương ứng với các neuron có giá trị sức mạnh cao nhất.

để mô phỏng độ thưa thớt. Điều này đảm bảo yêu cầu tài nguyên tối thiểu, tức là chỉ
Bộ nhớ Truy cập Ngẫu nhiên (RAM) và Bộ xử lý Trung tâm (CPU), mà không yêu
cầu Bộ xử lý Đồ họa (GPU).

Các thí nghiệm được thực hiện trên 8 tập dữ liệu chuẩn cho thấy QuickSelection có
một số ưu điểm so với các phương pháp tối tân, như sau:
Nó là người thực hiện tốt nhất hoặc tốt thứ hai về độ chính xác phân loại và phân
cụm trong gần như tất cả các tình huống được xem xét.
Nó là người thực hiện tốt nhất về sự đánh đổi giữa độ chính xác phân loại và phân
cụm, thời gian chạy và yêu cầu bộ nhớ.
Kiến trúc thưa thớt được đề xuất cho lựa chọn đặc trưng có ít nhất một bậc độ lớn
ít hơn các tham số so với tương đương dày đặc của nó. Điều này dẫn đến thực tế
nổi bật rằng thời gian huấn luyện tường thực của QuickSelection chạy trên CPU
nhỏ hơn thời gian huấn luyện tường thực của các đối thủ cạnh tranh dựa trên
autoencoder chạy trên GPU trong hầu hết các trường hợp.
Cuối cùng nhưng không kém phần quan trọng, hiệu quả tính toán của QuickSelection
làm cho nó có mức tiêu thụ năng lượng tối thiểu trong số các phương pháp lựa chọn
đặc trưng dựa trên autoencoder được xem xét.

2 Công trình liên quan
2.1 Lựa chọn đặc trưng
Văn học về lựa chọn đặc trưng cho thấy nhiều phương pháp khác nhau có thể được
chia thành ba loại chính, bao gồm các phương pháp filter, wrapper và embedded.
Các phương pháp filter sử dụng một tiêu chí xếp hạng để chấm điểm các đặc trưng và
sau đó loại bỏ các đặc trưng có điểm số dưới ngưỡng. Các tiêu chí này có thể là điểm số Laplacian

--- TRANG 4 ---
4 Zahra Atashgahi et al.
[27], Tương quan, Thông tin Tương hỗ [ 12], và nhiều phương pháp chấm điểm khác
như hàm chấm điểm Bayesian, chấm điểm t-test, và các tiêu chí dựa trên lý thuyết
Thông tin [33]. Các phương pháp này thường nhanh và hiệu quả về mặt tính toán.
Các phương pháp wrapper đánh giá các tập con khác nhau của đặc trưng để phát hiện
tập con tốt nhất. Các phương pháp wrapper thường cho hiệu suất tốt hơn các phương
pháp filter; chúng sử dụng một mô hình dự đoán để chấm điểm mỗi tập con đặc trưng.
Tuy nhiên, điều này dẫn đến độ phức tạp tính toán cao. Những đóng góp quan trọng
cho loại lựa chọn đặc trưng này đã được thực hiện bởi Kohavi và John [ 30]. Trong
[30], các tác giả sử dụng một cấu trúc cây để đánh giá các tập con của đặc trưng.
Các phương pháp embedded thống nhất quá trình học và lựa chọn đặc trưng [ 31].
Lựa chọn Đặc trưng Đa Cụm (MCFS) [ 11] là một phương pháp không giám sát cho
lựa chọn đặc trưng embedded, chọn đặc trưng sử dụng hồi quy phổ với regularization
L1-norm. Một hạn chế chính của thuật toán này là nó tốn kém về mặt tính toán vì
nó phụ thuộc vào việc tính toán các eigenvector của ma trận tương tự dữ liệu và sau
đó giải quyết một vấn đề hồi quy L1-regularized cho mỗi eigenvector [ 19]. Lựa chọn
Đặc trưng Phân biệt Không giám sát (UDFS) [ 57] là một thuật toán lựa chọn đặc
trưng embedded không giám sát khác đồng thời sử dụng cả thông tin đặc trưng và
phân biệt để chọn đặc trưng [37].

2.2 Autoencoder cho Lựa chọn Đặc trưng
Trong vài năm qua, nhiều mô hình dựa trên học sâu đã được phát triển để chọn
đặc trưng từ dữ liệu đầu vào sử dụng quy trình học của mạng neural sâu [38].
Trong [42], một Perceptron Đa lớp (MLP) được tăng cường với một lớp ghép cặp
để đưa mỗi đặc trưng đầu vào cùng với đối tác knockoff của nó vào mạng. Sau
khi huấn luyện, các tác giả sử dụng trọng số filter của lớp ghép cặp để xếp hạng
các đặc trưng đầu vào. Autoencoder được biết đến chung như một công cụ mạnh
mẽ để trích xuất đặc trưng [ 8], đang được khám phá để thực hiện lựa chọn đặc
trưng không giám sát. Trong [24], các tác giả kết hợp hồi quy autoencoder và nhiệm
vụ group lasso cho lựa chọn đặc trưng không giám sát có tên AutoEncoder Feature
Selector (AEFS). Trong [ 15], một autoencoder được kết hợp với ba biến thể của
regularization cấu trúc để thực hiện lựa chọn đặc trưng không giám sát. Các regularization
này dựa trên các biến slack, trọng số và gradient tương ứng. Một phương pháp
embedded dựa trên autoencoder được đề xuất gần đây khác là lựa chọn đặc trưng với
Concrete Autoencoder (CAE) [ 5]. Phương pháp này chọn đặc trưng bằng cách học
một phân phối concrete trên các đặc trưng đầu vào. Họ đề xuất một lớp chọn lọc
concrete chọn một tổ hợp tuyến tính của các đặc trưng đầu vào hội tụ thành một
tập rời rạc K đặc trưng trong quá trình huấn luyện. Trong [ 49], các tác giả đã cho
thấy rằng một tập lớn các tham số trong CAE có thể dẫn đến overfitting trong trường
hợp có số lượng mẫu hạn chế. Ngoài ra, CAE có thể chọn đặc trưng nhiều lần một
vì không có tương tác giữa các neuron của lớp chọn lọc. Để giảm thiểu những vấn
đề này, họ đề xuất một phương pháp lựa chọn đặc trưng mạng neural concrete (FsNet),
bao gồm một lớp chọn lọc và một mạng neural sâu có giám sát. Quy trình huấn luyện
của FsNet xem xét việc giảm loss tái tạo và tối đa hóa độ chính xác phân loại đồng
thời. Trong nghiên cứu của chúng tôi, chúng tôi tập trung chủ yếu vào các phương
pháp lựa chọn đặc trưng không giám sát.

Autoencoder Khử nhiễu (DAE) được giới thiệu để giải quyết vấn đề học hàm đồng
nhất trong autoencoder. Vấn đề này rất có thể xảy ra khi chúng ta có nhiều neuron
ẩn hơn đầu vào [ 4]. Kết quả là, đầu ra mạng

--- TRANG 5 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 5
có thể bằng với đầu vào, điều này làm cho autoencoder trở nên vô dụng. DAE giải
quyết vấn đề nói trên bằng cách giới thiệu nhiễu trên dữ liệu đầu vào và cố gắng
tái tạo đầu vào gốc từ phiên bản có nhiễu của nó [ 54]. Kết quả là, DAE học một
biểu diễn của dữ liệu đầu vào mạnh mẽ đối với những thay đổi nhỏ không liên quan
trong đầu vào. Trong nghiên cứu này, chúng tôi sử dụng khả năng của loại mạng
neural này để mã hóa phân phối dữ liệu đầu vào và chọn các đặc trưng quan trọng
nhất. Hơn nữa, chúng tôi chứng minh tác động của việc thêm nhiễu đối với kết quả
lựa chọn đặc trưng.

2.3 Huấn luyện Thưa thớt
Mạng neural sâu thường có ít nhất một số lớp kết nối đầy đủ, dẫn đến một số lượng
lớn tham số. Trong không gian có chiều cao, điều này không mong muốn vì nó có
thể gây ra sự giảm đáng kể tốc độ huấn luyện và tăng yêu cầu bộ nhớ. Để giải quyết
vấn đề này, mạng neural thưa thớt đã được đề xuất. Tỉa mạng neural dày đặc là một
trong những phương pháp nổi tiếng nhất để đạt được mạng neural thưa thớt [ 35,26].
Trong [25], Han et al. bắt đầu từ một mạng đã được huấn luyện trước, tỉa các trọng
số không quan trọng và huấn luyện lại mạng. Mặc dù phương pháp này có thể đầu
ra một mạng với mức độ thưa thớt mong muốn, chi phí tính toán tối thiểu bằng chi
phí huấn luyện một mạng dày đặc. Để giảm chi phí này, Lee et al. [ 36] bắt đầu với
một mạng neural dày đặc, và tỉa nó trước khi huấn luyện dựa trên độ nhạy kết nối;
sau đó, mạng thưa thớt được huấn luyện theo cách tiêu chuẩn. Tuy nhiên, bắt đầu
từ một mạng neural dày đặc yêu cầu ít nhất kích thước bộ nhớ của mạng neural dày
đặc và tài nguyên tính toán cho một lần lặp huấn luyện của mạng dày đặc. Do đó,
phương pháp này có thể không phù hợp cho các thiết bị có tài nguyên thấp.

Vào năm 2016, Mocanu et al. [ 44] đã giới thiệu ý tưởng huấn luyện mạng neural
thưa thớt từ đầu, một khái niệm gần đây đã bắt đầu được biết đến như huấn luyện
thưa thớt. Mẫu kết nối thưa thớt được cố định trước khi huấn luyện sử dụng lý thuyết
đồ thị, khoa học mạng và thống kê dữ liệu. Mặc dù nó cho thấy kết quả đầy hứa
hẹn, vượt trội hơn đối tác dày đặc, mẫu thưa thớt tĩnh không phải lúc nào cũng mô
hình hóa dữ liệu một cách tối ưu. Để giải quyết những vấn đề này, vào năm 2018,
Mocanu et al. [45] đã đề xuất thuật toán Huấn luyện Tiến hóa Thưa thớt (SET) sử
dụng độ thưa thớt động trong quá trình huấn luyện. Ý tưởng là bắt đầu với một mạng
neural thưa thớt trước khi huấn luyện và thay đổi động các kết nối của nó trong quá
trình huấn luyện để tự động mô hình hóa phân phối dữ liệu. Điều này dẫn đến sự giảm
đáng kể số lượng tham số và tăng hiệu suất. SET phát triển các kết nối thưa thớt tại
mỗi epoch huấn luyện bằng cách loại bỏ một phần kết nối có độ lớn nhỏ nhất, và thêm
ngẫu nhiên các kết nối mới trong mỗi lớp. Bourgin et al. [ 10] đã chỉ ra rằng một MLP
thưa thớt được huấn luyện với SET đạt được kết quả tối tân trên dữ liệu dạng bảng
trong việc dự đoán quyết định của con người, vượt trội hơn mạng neural kết nối đầy
đủ và Random Forest, trong số những phương pháp khác.

Trong công trình này, chúng tôi giới thiệu lần đầu tiên huấn luyện thưa thớt trong
thế giới autoencoder khử nhiễu, và chúng tôi đặt tên cho mô hình mới được giới thiệu
là autoencoder khử nhiễu thưa thớt (sparse DAE). Chúng tôi huấn luyện sparse DAE
với thuật toán SET để giữ số lượng tham số thấp, trong quá trình huấn luyện. Sau đó,
chúng tôi khai thác mạng đã được huấn luyện để chọn các đặc trưng quan trọng nhất.

--- TRANG 6 ---
6 Zahra Atashgahi et al.
3 Phương pháp đề xuất
Để giải quyết vấn đề chiều cao của dữ liệu, chúng tôi đề xuất một phương pháp mới,
có tên "QuickSelection", để chọn các thuộc tính thông tin nhất từ dữ liệu, dựa trên
sức mạnh (tầm quan trọng) của chúng. Tóm lại, chúng tôi huấn luyện một mạng
autoencoder khử nhiễu thưa thớt từ đầu theo cách không giám sát thích ứng. Sau đó,
chúng tôi sử dụng mạng đã được huấn luyện để rút ra sức mạnh của mỗi neuron
trong các đặc trưng đầu vào.

Ý tưởng cơ bản của phương pháp được đề xuất của chúng tôi là áp đặt các kết nối
thưa thớt trên DAE, đã chứng minh thành công của nó trong lĩnh vực liên quan của
trích xuất đặc trưng, để xử lý hiệu quả độ phức tạp tính toán của dữ liệu có chiều
cao về mặt tài nguyên bộ nhớ. Các kết nối thưa thớt được phát triển theo cách thích
ứng giúp xác định các đặc trưng thông tin.

Một số phương pháp đã được đề xuất để huấn luyện mạng neural sâu từ đầu sử
dụng các kết nối thưa thớt và huấn luyện thưa thớt [ 14,45,7,46,17,59]. Tất cả các
phương pháp này được triển khai sử dụng mặt nạ nhị phân trên các kết nối để mô
phỏng độ thưa thớt vì tất cả các thư viện học sâu và phần cứng tiêu chuẩn (ví dụ:
GPU) không được tối ưu hóa cho các phép toán ma trận trọng số thưa thớt. Khác với
các phương pháp nói trên, chúng tôi triển khai phương pháp được đề xuất của chúng
tôi theo cách hoàn toàn thưa thớt để đáp ứng mục tiêu thực sự sử dụng các ưu điểm
của một số lượng rất nhỏ các tham số trong quá trình huấn luyện. Chúng tôi quyết
định sử dụng SET trong việc huấn luyện sparse DAE của chúng tôi.

Việc chọn SET là do đặc tính mong muốn của nó. SET là một phương pháp đơn
giản nhưng đạt được hiệu suất thỏa đáng. Khác với các phương pháp khác tính toán
và lưu trữ thông tin cho tất cả các trọng số mạng, bao gồm cả những trọng số không
tồn tại, SET tiết kiệm bộ nhớ. Nó chỉ lưu trữ trọng số cho các kết nối thưa thớt hiện
tại. Nó không cần bất kỳ độ phức tạp tính toán cao nào vì quy trình tiến hóa chỉ phụ
thuộc vào độ lớn của các kết nối hiện tại. Đây là một ưu điểm thuận lợi cho phương
pháp được đề xuất của chúng tôi để chọn các đặc trưng thông tin một cách nhanh
chóng. Trong các phần con sau, chúng tôi trước tiên trình bày cấu trúc của mạng
autoencoder khử nhiễu thưa thớt được đề xuất của chúng tôi và sau đó giải thích
phương pháp lựa chọn đặc trưng. Mã giả của phương pháp được đề xuất của chúng
tôi có thể được tìm thấy trong Thuật toán 1.

3.1 Sparse DAE
Cấu trúc Vì mục tiêu của phương pháp được đề xuất của chúng tôi là thực hiện lựa
chọn đặc trưng nhanh theo cách tiết kiệm bộ nhớ, chúng tôi xem xét ở đây mô hình
với số lượng lớp ẩn ít nhất có thể, một lớp ẩn, vì nhiều lớp hơn có nghĩa là nhiều
tính toán hơn. Ban đầu, các kết nối thưa thớt giữa hai lớp neuron liên tiếp được khởi
tạo với một đồ thị ngẫu nhiên Erdős–Rényi, trong đó xác suất kết nối giữa hai neuron
được cho bởi

P(Wl
ij) =(nl1+nl)
nl1nl; (1)

trong đó ε biểu thị tham số kiểm soát mức độ thưa thớt, nl biểu thị số neuron tại
lớp l, và Wl
ij là kết nối giữa neuron i trong lớp l−1 và neuron j trong lớp l, được lưu
trữ trong ma trận trọng số thưa thớt Wl.

Khử nhiễu đầu vào Chúng tôi sử dụng mô hình nhiễu cộng để làm hỏng dữ liệu gốc:
ex=x+nfN(; 2); (2)

--- TRANG 7 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 7
trong đó x là vector dữ liệu đầu vào từ tập dữ liệu X, nf (hệ số nhiễu) là một siêu
tham số của mô hình xác định mức độ hỏng hóc, và N(μ; σ2) là một nhiễu Gaussian.
Sau khi khử nhiễu dữ liệu, chúng tôi rút ra biểu diễn ẩn h sử dụng đầu vào bị hỏng
này. Sau đó, đầu ra z được tái tạo từ biểu diễn ẩn. Chính thức, biểu diễn ẩn h và
đầu ra z được tính toán như sau:

h=a(W1ex+b1); (3)
z=a(W2h+b2); (4)

trong đó W1 và W2 là các ma trận trọng số thưa thớt của lớp ẩn và lớp đầu ra tương
ứng, b1 và b2 là các vector bias của lớp tương ứng của chúng, và a là hàm kích hoạt
của mỗi lớp. Mục tiêu của mạng của chúng tôi là tái tạo các đặc trưng gốc trong đầu
ra. Vì lý do này, chúng tôi sử dụng sai số bình phương trung bình (MSE) làm hàm
loss để đo sự khác biệt giữa các đặc trưng gốc x và đầu ra tái tạo z:

LMSE =||z−x||22: (5)

Cuối cùng, các trọng số có thể được tối ưu hóa sử dụng các thuật toán huấn luyện
tiêu chuẩn (ví dụ: Stochastic Gradient Descent (SGD), AdaGrad, và Adam) với sai
số tái tạo ở trên.

Quy trình huấn luyện Chúng tôi thích ứng quy trình huấn luyện SET [ 45] trong việc
huấn luyện mạng được đề xuất của chúng tôi cho lựa chọn đặc trưng. SET hoạt động
như sau. Sau mỗi epoch huấn luyện, một phần của các trọng số dương nhỏ nhất và
một phần của các trọng số âm lớn nhất tại mỗi lớp được loại bỏ. Việc chọn lựa dựa
trên độ lớn của các trọng số. Các kết nối mới với cùng số lượng như những kết nối
đã loại bỏ được thêm ngẫu nhiên trong mỗi lớp. Do đó tổng số kết nối trong mỗi lớp
vẫn giữ nguyên, trong khi số kết nối trên mỗi neuron thay đổi, như được thể hiện
trong Hình 1. Các trọng số của những kết nối mới này được khởi tạo từ một phân
phối chuẩn. Việc thêm ngẫu nhiên các kết nối mới không có rủi ro cao là không tìm
thấy một kết nối thưa thớt tốt ở cuối quá trình huấn luyện vì đã được chỉ ra trong
[ 41] rằng huấn luyện thưa thớt có thể khám phá ra một số lượng lớn các tối ưu cục
bộ kết nối thưa thớt rất khác nhau đạt được hiệu suất rất tương tự.

3.2 Lựa chọn Đặc trưng
Chúng tôi chọn các đặc trưng quan trọng nhất của dữ liệu dựa trên trọng số của
các neuron đầu vào tương ứng của sparse DAE đã được huấn luyện. Lấy cảm hứng
từ sức mạnh nút trong lý thuyết đồ thị [ 6], chúng tôi xác định tầm quan trọng của
mỗi neuron dựa trên sức mạnh của nó. Chúng tôi ước tính sức mạnh của mỗi neuron
( si) bằng tổng giá trị tuyệt đối của các trọng số kết nối đi ra của nó.

si=∑n1j=1|W1ij|; (6)

trong đó n1 là số neuron của lớp ẩn thứ nhất, và W1ij biểu thị trọng số của kết nối
liên kết neuron đầu vào i với neuron ẩn j.

--- TRANG 8 ---
8 Zahra Atashgahi et al.
  Khởi tạo ngẫu nhiên  Sau 3 epoch  Sau 6 Epoch  Sau 10 epoch    chiều cao (pixel)  
     
Sức mạnh   chiều rộng (pixel)  chiều rộng (pixel)  chiều rộng (pixel)  chiều rộng (pixel)    
Hình 2: Sức mạnh của neuron trên tập dữ liệu MNIST. Các bản đồ nhiệt ở trên là
biểu diễn 2D của sức mạnh neuron đầu vào. Có thể quan sát thấy rằng sức mạnh của
neuron là ngẫu nhiên ở đầu quá trình huấn luyện. Sau một vài epoch, mẫu thay đổi,
và neuron ở trung tâm trở nên quan trọng hơn và tương tự như mẫu dữ liệu MNIST.

Như được thể hiện trong Hình 1, sức mạnh của các neuron đầu vào thay đổi trong
quá trình huấn luyện; chúng tôi đã mô tả sức mạnh của neuron theo kích thước và màu
sắc của chúng. Sau khi hội tụ, chúng tôi tính toán sức mạnh cho tất cả các neuron
đầu vào; mỗi neuron đầu vào tương ứng với một đặc trưng. Sau đó, chúng tôi chọn
các đặc trưng tương ứng với các neuron có K giá trị sức mạnh lớn nhất:

F∗s= argmaxFs⊆F;|Fs|=k∑fi∈Fssi; (7)

trong đó F và F∗s là tập đặc trưng gốc và tập đặc trưng được chọn cuối cùng tương
ứng, fi là đặc trưng thứ i của F, và K là số đặc trưng cần được chọn. Ngoài ra, bằng
cách sắp xếp tất cả các đặc trưng dựa trên sức mạnh của chúng, chúng tôi sẽ rút ra
tầm quan trọng của tất cả các đặc trưng trong tập dữ liệu. Tóm lại, chúng tôi sẽ có
thể xếp hạng tất cả các đặc trưng đầu vào bằng cách huấn luyện chỉ một lần một mô
hình sparse DAE duy nhất.

Thuật toán 1 QuickSelection
1:Đầu vào:Tập dữ liệu X, hệ số nhiễu nf, siêu tham số độ thưa thớt ε, số neuron ẩn
nh, số đặc trưng được chọn K
2: Khử nhiễu đầu vào: ex=x+nfN(μ; σ2)
3:Khởi tạo cấu trúc: Khởi tạo sparse-DAE với nh neuron ẩn và mức độ thưa thớt
được xác định bởi ε
4:procedure Huấn luyện sparse-DAE
5: Cho loss là LMSE =||z−x||22 trong đó z là đầu ra của sparse-DAE
6: for i∈{1;:::;epochs} do
7: Thực hiện lan truyền tiến và lan truyền ngược tiêu chuẩn
8: Thực hiện loại bỏ và thêm trọng số để tối ưu hóa cấu trúc
9:procedure QuickSelection
10: Tính toán sức mạnh neuron:
11: for i∈{1;:::; #inputfeatures} do
12:si=∑nhj=1|W1ij|
13: Chọn K đặc trưng: F∗s= argmaxFs⊆F;|Fs|=K∑fi∈Fssi;

--- TRANG 9 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 9
Để hiểu sâu hơn về quá trình trên, chúng tôi phân tích sức mạnh của mỗi neuron
đầu vào trong bản đồ 2D trên tập dữ liệu MNIST. Điều này được minh họa trong
Hình 2. Ở đầu quá trình huấn luyện, tất cả các neuron có sức mạnh nhỏ do việc khởi
tạo ngẫu nhiên mỗi trọng số với một giá trị nhỏ. Trong quá trình phát triển mạng,
các kết nối mạnh hơn được liên kết với các đặc trưng quan trọng dần dần. Chúng ta
có thể quan sát thấy rằng sau mười epoch, các neuron ở trung tâm bản đồ trở nên
mạnh hơn. Mẫu này tương tự như mẫu của dữ liệu MNIST trong đó hầu hết các chữ
số xuất hiện ở giữa hình ảnh.

Chúng tôi đã nghiên cứu các chỉ số khác để ước tính tầm quan trọng neuron như
sức mạnh của neuron đầu ra, bậc của neuron đầu vào và đầu ra, và sức mạnh và bậc
của neuron đồng thời. Tuy nhiên, trong các thí nghiệm của chúng tôi, tất cả các
phương pháp này đã bị vượt trội bởi sức mạnh của neuron đầu vào về độ chính xác
và tính ổn định.

4 Thí nghiệm
Để xác minh tính hợp lệ của phương pháp được đề xuất của chúng tôi, chúng tôi thực
hiện một số thí nghiệm. Trong phần này, trước tiên, chúng tôi nêu các cài đặt của
thí nghiệm, bao gồm siêu tham số và tập dữ liệu. Sau đó, chúng tôi thực hiện lựa
chọn đặc trưng với QuickSelection và so sánh kết quả với các phương pháp khác,
bao gồm MCFS, Laplacian Score, và ba phương pháp lựa chọn đặc trưng dựa trên
autoencoder. Sau đó, chúng tôi thực hiện các phân tích khác nhau trên QuickSelection
để hiểu hành vi của nó. Cuối cùng, chúng tôi thảo luận về khả năng mở rộng của
QuickSelection và so sánh nó với các phương pháp khác được xem xét.

4.1 Cài đặt
Các cài đặt thí nghiệm, bao gồm giá trị của siêu tham số, chi tiết triển khai, cấu trúc
của sparse DAE, tập dữ liệu chúng tôi sử dụng để đánh giá, và chỉ số đánh giá, như
sau.

4.1.1 Siêu tham số và Triển khai
Đối với lựa chọn đặc trưng, chúng tôi xem xét trường hợp của sparse DAE đơn giản
nhất với một lớp ẩn gồm 1000 neuron. Lựa chọn này được thực hiện do mục tiêu
chính của chúng tôi là giảm độ phức tạp mô hình và số lượng tham số. Hàm kích
hoạt được sử dụng cho neuron lớp ẩn và đầu ra là "Sigmoid" và "Linear" tương ứng,
ngoại trừ tập dữ liệu Madelon nơi chúng tôi sử dụng "Tanh" cho hàm kích hoạt đầu
ra. Chúng tôi huấn luyện mạng với SGD và tốc độ học 0.01. Siêu tham số ζ, phần
trăm trọng số cần loại bỏ trong quy trình SET, là 0.2. Ngoài ra, ε, xác định mức độ
thưa thớt, được đặt thành 13. Chúng tôi đặt hệ số nhiễu ( nf) thành 0.2 trong các
thí nghiệm. Để cải thiện quá trình học của mạng, chúng tôi chuẩn hóa các đặc trưng
của tập dữ liệu sao cho mỗi thuộc tính có trung bình bằng không và phương sai đơn
vị. Tuy nhiên, đối với tập dữ liệu SMK và PCMAC, chúng tôi sử dụng scaling Min-Max.
Phương pháp tiền xử lý cho mỗi tập dữ liệu được xác định bằng một thí nghiệm nhỏ
của hai phương pháp tiền xử lý.

--- TRANG 10 ---
10 Zahra Atashgahi et al.
Chúng tôi triển khai sparse DAE và QuickSelection2 theo cách hoàn toàn thưa thớt
trong Python, sử dụng thư viện Scipy [ 28] và Cython. Chúng tôi so sánh phương
pháp được đề xuất của chúng tôi với MCFS, Laplacian score (LS), AEFS và CAE,
đã được đề cập trong Phần 2. Chúng tôi cũng đã thực hiện một số thí nghiệm với
UDFS; tuy nhiên, vì chúng tôi không thể có được nhiều kết quả trong giới hạn thời
gian được xem xét (24 giờ), chúng tôi không bao gồm kết quả trong bài báo. Chúng
tôi đã sử dụng kho scikit-feature cho việc triển khai MCFS và Laplacian score [ 37].
Ngoài ra, chúng tôi sử dụng việc triển khai lựa chọn đặc trưng với CAE và AEFS từ
Github3. Ngoài ra, để làm nổi bật lợi ích của việc sử dụng các lớp thưa thớt, chúng
tôi so sánh kết quả của chúng tôi với một autoencoder kết nối đầy đủ (FCAE) sử
dụng sức mạnh neuron như một thước đo tầm quan trọng của mỗi đặc trưng. Để có
một so sánh công bằng, cấu trúc của mạng này được xem xét tương tự như DAE của
chúng tôi, một lớp ẩn chứa 1000 neuron được triển khai sử dụng TensorFlow. Hơn
nữa, chúng tôi đã nghiên cứu tác động của các thành phần khác của QuickSelection,
bao gồm khử nhiễu đầu vào và thuật toán huấn luyện SET, trong Phụ lục B.1 và F
tương ứng.

Đối với tất cả các phương pháp khác (ngoại trừ FCAE mà tất cả siêu tham số và
tiền xử lý tương tự như QuickSelection), chúng tôi chia tỷ lệ dữ liệu giữa không và
một, vì nó mang lại hiệu suất tốt hơn chuẩn hóa dữ liệu cho các phương pháp này.
Các siêu tham số của các phương pháp nói trên đã được đặt tương tự như những cái
được báo cáo trong mã hoặc bài báo tương ứng. Đối với AEFS, chúng tôi điều chỉnh
siêu tham số regularization giữa 0.0001 và 1000, vì phương pháp này nhạy cảm với
giá trị này. Chúng tôi thực hiện các thí nghiệm trên một lõi CPU duy nhất, Intel
Xeon Processor E5 v4, và đối với các phương pháp yêu cầu GPU, chúng tôi sử dụng
NVIDIA TESLA P100.

4.1.2 Tập dữ liệu
Chúng tôi đánh giá hiệu suất của phương pháp được đề xuất của chúng tôi trên tám
tập dữ liệu, bao gồm năm tập dữ liệu chiều thấp và ba tập dữ liệu chiều cao. Bảng
1 minh họa các đặc tính của những tập dữ liệu này.
– COIL-20 [47] bao gồm 1440 hình ảnh được chụp từ 20 đối tượng ( 72 tư thế cho
mỗi đối tượng).
– Madelon [23] là một tập dữ liệu nhân tạo với 5 đặc trưng thông tin và 15 tổ hợp
tuyến tính của chúng. Phần còn lại của các đặc trưng là các đặc trưng gây nhiễu
vì chúng không có sức mạnh dự đoán.
– Nhận dạng Hoạt động Con người (HAR) [3] được tạo ra bằng cách thu thập các
quan sát của 30 đối tượng thực hiện 6 hoạt động như đi bộ, đứng và ngồi. Dữ
liệu được ghi lại bởi một điện thoại thông minh được kết nối với cơ thể của đối
tượng.
– Isolet [18] đã được tạo ra với tên được nói của mỗi chữ cái của bảng chữ cái
tiếng Anh.
– MNIST [34] là một cơ sở dữ liệu về hình ảnh 28x28 của các chữ số viết tay.
– SMK-CAN-187 [50] là một tập dữ liệu biểu hiện gen với 19993 đặc trưng. Tập
dữ liệu này so sánh người hút thuốc có và không có ung thư phổi.

2Việc triển khai QuickSelection có sẵn tại: https://github.com/
zahraatashgahi/QuickSelection
3Việc triển khai AEFS và CAE có sẵn tại: https://github.com/mfbalin/
Concrete-Autoencoders

--- TRANG 11 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 11
Bảng 1: Đặc tính tập dữ liệu.
Tập dữ liệu Chiều Loại Mẫu Huấn luyện Kiểm tra Lớp
Coil20 1024 Hình ảnh 1440 1152 288 20
Isolet 617 Giọng nói 7737 6237 1560 26
HAR 561 Chuỗi thời gian 10299 7352 2947 6
Madelon 500 Nhân tạo 2600 2000 600 2
MNIST 784 Hình ảnh 70000 60000 10000 10
SMK-CAN-187 19993 Microarray 187 149 38 2
GLA-BRA-180 49151 Microarray 180 144 36 4
PCMAC 3289 Văn bản 1943 1554 389 2
– GLA-BRA-180 [51] bao gồm hồ sơ biểu hiện của Yếu tố tế bào gốc hữu ích để
xác định sự tạo mạch máu khối u.
– PCMAC [32] là một tập con của dữ liệu 20 Newsgroups.

4.1.3 Chỉ số Đánh giá
Để đánh giá mô hình của chúng tôi, chúng tôi tính toán hai chỉ số: độ chính xác phân
cụm và độ chính xác phân loại. Để rút ra độ chính xác phân cụm [ 37], trước tiên,
chúng tôi thực hiện K-means sử dụng tập con của tập dữ liệu tương ứng với các đặc
trưng được chọn và lấy nhãn cụm. Sau đó, chúng tôi tìm sự khớp tốt nhất giữa nhãn
lớp và nhãn cụm và báo cáo độ chính xác phân cụm. Chúng tôi lặp lại thuật toán
K-means 10 lần và báo cáo kết quả phân cụm trung bình vì K-means có thể hội tụ
về một tối ưu cục bộ.

Để tính toán độ chính xác phân loại, chúng tôi sử dụng một mô hình phân loại có
giám sát có tên "Extremely randomized trees" (ExtraTrees), là một phương pháp học
tập hợp phù hợp với một số cây quyết định ngẫu nhiên trên các phần khác nhau của
dữ liệu [ 21]. Việc chọn phương pháp phân loại được thực hiện do hiệu quả tính toán
của bộ phân loại ExtraTrees. Để tính toán độ chính xác phân loại, trước tiên, chúng
tôi rút ra K đặc trưng được chọn sử dụng mỗi phương pháp lựa chọn đặc trưng được
xem xét. Sau đó, chúng tôi huấn luyện bộ phân loại ExtraTrees với 50 cây như các
ước lượng trên K đặc trưng được chọn của tập huấn luyện. Cuối cùng, chúng tôi tính
toán độ chính xác phân loại trên dữ liệu kiểm tra chưa thấy. Đối với các tập dữ liệu
không chứa tập kiểm tra, chúng tôi chia dữ liệu thành tập huấn luyện và kiểm tra,
bao gồm 80% tổng số mẫu gốc cho tập huấn luyện và 20% còn lại cho tập kiểm tra.
Ngoài ra, chúng tôi đã đánh giá độ chính xác phân loại của lựa chọn đặc trưng sử
dụng bộ phân loại random forest [ 39] trong Phụ lục G.

4.2 Lựa chọn Đặc trưng
Chúng tôi chọn 50 đặc trưng từ mỗi tập dữ liệu ngoại trừ Madelon, mà chúng tôi
chỉ chọn 20 đặc trưng vì hầu hết các đặc trưng của nó là nhiễu không thông tin. Sau
đó, chúng tôi tính toán độ chính xác phân cụm và phân loại trên tập con được chọn
của các đặc trưng; các đặc trưng thông tin được chọn càng nhiều, độ chính xác đạt
được sẽ càng cao. Kết quả độ chính xác phân cụm và phân loại của mô hình của
chúng tôi và các phương pháp khác được tóm tắt trong Bảng 2 và 3 tương ứng. Những
kết quả này là trung bình của 5 lần chạy cho mỗi trường hợp. Đối với các phương
pháp lựa chọn đặc trưng dựa trên autoencoder, bao gồm CAE, AEFS và FCAE, chúng
tôi xem xét 100 epoch huấn luyện. Tuy nhiên, chúng tôi trình bày

--- TRANG 12 ---
12 Zahra Atashgahi et al.
Bảng 2: Độ chính xác phân cụm (%) sử dụng 50 đặc trưng được chọn (ngoại trừ
Madelon mà chúng tôi chọn 20 đặc trưng). Trên mỗi tập dữ liệu, mục in đậm là
người thực hiện tốt nhất, và mục in nghiêng là người thực hiện tốt thứ hai.
Phương pháp COIL-20 Isolet HAR Madelon MNIST SMK GLA PCMAC
MCFS67.00.7 33.80.562.40.057.20.0 35.2 0 51.6 0.265.80.350.60.0
LS 55.5 0.4 33.2 0.2 61.20.0 58.1 0.014.90.1 51.6 0.4 55.5 0.4 50.6 0.0
CAE 60.0 1.1 31.6 1.3 51.4 0.4 56.9 3.649.21.5 60.7 0.455.41.3 52.01.2
AEFS 51.2 1.7 31.0 2.7 55.0 2.2 50.8 0.2 40.0 1.9 52.4 1.8 56.1 5.2 50.9 0.5
FCAE 60.21.728.72.5 49.5 8.7 50.9 0.4 28.2 8.5 51.5 0.8 53.5 3.0 50.9 0.1
QS1059.52.1 32.5 2.8 56.0 2.6 57.5 3.8 45.4 3.9 54.03.151.36.7 50.9 0.5
QS100 60.22.035.12.754.64.558.21.5 48.32.451.80.8 59.51.852.51.1
QSbest63.81.5 42.2 2.6 59.5 4.3 58.6 0.9 48.3 2.4 54.9 1.39 59.5 1.8 53.1 0

Bảng 3: Độ chính xác phân loại (%) sử dụng 50 đặc trưng được chọn (ngoại trừ
Madelon mà chúng tôi chọn 20 đặc trưng). Trên mỗi tập dữ liệu, mục in đậm là
người thực hiện tốt nhất, và mục in nghiêng là người thực hiện tốt thứ hai.
Phương pháp COIL-20 Isolet HAR Madelon MNIST SMK GLA PCMAC
MCFS 99.2 0.3 79.5 0.4 88.9 0.3 81.7 0.8 88.7 0 75.8 1.5 70.6 3.8 55.5 0.0
LS 89.8 0.4 83.0 0.2 86.4 0.491.40.920.70.1 71.6 5.6 71.7 1.1 50.4 0.0
CAE 99.60.389.80.6 91.7 1.087.52.095.40.171.63.1 70.0 4.159.91.5
AEFS 93.0 2.7 85.1 2.4 87.7 1.4 52.1 2.8 86.1 2.0 76.34.468.93.7 57.1 3.6
FCAE99.70.281.65.9 87.4 2.4 53.5 8.1 68.8 28.7 71.6 3.5 72.84.858.11.9
QS1098.80.6 86.9 1.1 88.8 0.7 86.6 3.6 93.80.676.94.669.43.0 58.94.4
QS10099.70.3 89.01.3 90.2 1.2 90.3 0.793.50.5 75.7 3.973.33.358.02.9
QSbest99.70.3 89.0 1.3 90.5 1.6 90.9 0.5 94.2 0.5 81.6 2.9 73.3 3.3 61.3 6.1

kết quả của QuickSelection tại epoch 10 và 100 có tên QuickSelection 10 và
QuickSelection 100 tương ứng. Điều này chủ yếu do thực tế là phương pháp được
đề xuất của chúng tôi có thể đạt được độ chính xác hợp lý sau vài epoch đầu tiên.
Hơn nữa, chúng tôi thực hiện điều chỉnh siêu tham số cho ζ và ε sử dụng phương
pháp tìm kiếm lưới trên một số lượng hạn chế các giá trị cho tất cả các tập dữ liệu;
kết quả tốt nhất được trình bày trong Bảng 2 và 3 như QuickSelection best. Kết quả
của việc chọn siêu tham số có thể được tìm thấy trong Phụ lục B.2. Tuy nhiên, chúng
tôi không thực hiện tối ưu hóa siêu tham số cho các phương pháp khác (ngoại trừ
AEFS). Do đó, để có một so sánh công bằng giữa tất cả các phương pháp, chúng tôi
không so sánh kết quả của QuickSelection best với các phương pháp khác.

Từ Bảng 2, có thể quan sát thấy rằng QuickSelection vượt trội hơn tất cả các phương
pháp khác trên Isolet, Madelon và PCMAC về độ chính xác phân cụm, trong khi là
người thực hiện tốt thứ hai trên Coil20, MNIST, SMK và GLA. Hơn nữa, trên tập
dữ liệu HAR, nó là người thực hiện tốt nhất trong số tất cả các phương pháp lựa
chọn đặc trưng dựa trên autoencoder được xem xét. Như được hiển thị trong Bảng
3, QuickSelection vượt trội hơn tất cả các phương pháp khác trên Coil20, SMK và
GLA về độ chính xác phân loại, trong khi là người thực hiện tốt thứ hai trên các
tập dữ liệu khác. Từ những Bảng này, rõ ràng là QuickSelection có thể vượt trội hơn
mạng dày đặc tương đương của nó (FCAE) về độ chính xác phân loại và phân cụm
trên tất cả các tập dữ liệu.

--- TRANG 13 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 13
0 100 200 300 400 500
# đặc trưng được loại bỏ0.20.40.60.81.0Độ chính xác (%) Từ ít quan trọng nhất đến quan trọng nhất
0 100 200 300 400 500
# đặc trưng được loại bỏ0.20.40.60.81.0Độ chính xác (%) Từ quan trọng nhất đến ít quan trọng nhất
Hình 3: Ảnh hưởng của việc loại bỏ đặc trưng trên tập dữ liệu Madelon. Sau khi
rút ra tầm quan trọng của các đặc trưng với QuickSelection, chúng tôi sắp xếp và
sau đó loại bỏ chúng dựa trên hai phương pháp trên.

Có thể quan sát trong Bảng 2 và 3, rằng Lap_score có hiệu suất kém khi số lượng
mẫu lớn (ví dụ: MNIST). Tuy nhiên, trong các nhiệm vụ có số lượng mẫu và đặc
trưng thấp, ngay cả trong môi trường có nhiễu như Madelon, Lap_score có hiệu suất
tương đối tốt. Ngược lại, CAE có hiệu suất kém trong môi trường có nhiễu (ví dụ:
Madelon), trong khi nó có độ chính xác phân loại tốt trên các tập dữ liệu khác được
xem xét. Nó là người thực hiện tốt nhất hoặc tốt thứ hai trên năm tập dữ liệu về độ
chính xác phân loại, khi K= 50. AEFS và FCAE cũng không thể đạt được hiệu suất
tốt trên Madelon. Chúng tôi tin rằng các lớp dày đặc là nguyên nhân chính của hành
vi này; các kết nối dày đặc cố gắng học tất cả các đặc trưng đầu vào, ngay cả các
đặc trưng có nhiễu. Do đó, chúng thất bại trong việc phát hiện các thuộc tính quan
trọng nhất của dữ liệu. MCFS hoạt động tốt trên hầu hết các tập dữ liệu về độ chính
xác phân cụm. Điều này là do mục tiêu chính của MCFS là bảo tồn cấu trúc đa cụm
của dữ liệu. Tuy nhiên, phương pháp này cũng có hiệu suất kém trên các tập dữ liệu
có số lượng mẫu lớn (ví dụ: MNIST) và các đặc trưng có nhiễu (ví dụ: Madelon).

Tuy nhiên, vì việc đánh giá các phương pháp chỉ sử dụng một giá trị K có thể không
đủ để so sánh, chúng tôi đã thực hiện một thí nghiệm khác sử dụng các giá trị K
khác nhau. Trong Phụ lục A.1, chúng tôi kiểm tra các giá trị khác cho K trên tất cả
các tập dữ liệu, và so sánh các phương pháp về độ chính xác phân loại, độ chính
xác phân cụm, thời gian chạy và sử dụng bộ nhớ tối đa. Tóm tắt kết quả của Phụ
lục này đã được tóm tắt trong Phần 5.1.

4.2.1 Mức độ liên quan của Đặc trưng được chọn
Để minh họa khả năng của QuickSelection trong việc tìm các đặc trưng thông tin,
chúng tôi phân tích kỹ lưỡng kết quả tập dữ liệu Madelon, có tính chất thú vị là
chứa nhiều đặc trưng có nhiễu. Chúng tôi thực hiện các thí nghiệm sau; trước tiên,
chúng tôi sắp xếp các đặc trưng dựa trên sức mạnh của chúng. Sau đó, chúng tôi
loại bỏ các đặc trưng từng cái một từ đặc trưng ít quan trọng nhất đến quan trọng
nhất. Trong mỗi bước, chúng tôi huấn luyện một bộ phân loại ExtraTrees với các
đặc trưng còn lại. Chúng tôi lặp lại thí nghiệm này bằng cách loại bỏ đặc trưng từ
những cái quan trọng nhất đến ít quan trọng nhất. Kết quả về độ chính xác phân
loại cho cả hai thí nghiệm có thể được thấy trong Hình 3. Ở phía bên trái của Hình
3, chúng ta có thể quan sát thấy rằng việc loại bỏ các đặc trưng ít quan trọng nhất,
là nhiễu, làm tăng độ chính xác. Độ chính xác tối đa xảy ra sau khi chúng ta loại bỏ
480 đặc trưng nhiễu. Điều này tương ứng với thời điểm

--- TRANG 14 ---
14 Zahra Atashgahi et al.
      
      
 
Hình 4: Giá trị trung bình của tất cả các mẫu dữ liệu của mỗi lớp tương ứng với 50
đặc trưng được chọn trên MNIST sau 100 epoch huấn luyện (phía dưới), cùng với
trung bình của các mẫu dữ liệu thực tế của mỗi lớp (phía trên).

khi tất cả các đặc trưng nhiễu được cho là đã loại bỏ. Trong Hình 3 (phải), có thể
thấy rằng việc loại bỏ các đặc trưng theo thứ tự ngược lại dẫn đến sự giảm đột ngột
độ chính xác phân loại. Sau khi loại bỏ 20 đặc trưng (được chỉ ra bởi đường thẳng
xanh), bộ phân loại hoạt động như một bộ phân loại ngẫu nhiên. Chúng tôi kết luận
rằng QuickSelection có thể tìm ra các đặc trưng thông tin nhất theo thứ tự tốt.

Để thể hiện tốt hơn mức độ liên quan của các đặc trưng được tìm thấy bởi QuickSelection,
chúng tôi trực quan hóa 50 đặc trưng được chọn trên tập dữ liệu MNIST theo từng
lớp, bằng cách tính trung bình các giá trị tương ứng của chúng từ tất cả các mẫu dữ
liệu thuộc về một lớp. Như có thể quan sát trong Hình 4, hình dạng kết quả giống
với các mẫu thực tế của chữ số tương ứng. Chúng tôi thảo luận về kết quả của tất
cả các lớp tại các epoch huấn luyện khác nhau chi tiết hơn trong Phụ lục C.

5 Thảo luận
5.1 Đánh đổi Độ chính xác và Hiệu quả Tính toán
Trong phần này, chúng tôi thực hiện so sánh toàn diện giữa các mô hình về thời gian
chạy, tiêu thụ năng lượng, yêu cầu bộ nhớ, độ chính xác phân cụm và độ chính xác
phân loại. Tóm lại, chúng tôi thay đổi số lượng đặc trưng cần chọn (K) và đo độ
chính xác, thời gian chạy và sử dụng bộ nhớ tối đa trên tất cả các phương pháp.
Sau đó, chúng tôi tính toán hai điểm số để tóm tắt kết quả và so sánh các phương
pháp.

Chúng tôi phân tích tác động của việc thay đổi K đối với hiệu suất QuickSelection
và so sánh với các phương pháp khác; kết quả được trình bày trong Hình 10 trong
Phụ lục A.1. Hình 10a so sánh hiệu suất của tất cả các phương pháp khi K thay đổi
giữa 5 và 100 trên các tập dữ liệu chiều thấp, bao gồm Coil20, Isolet, HAR và Madelon.
Hình 10b minh họa so sánh hiệu suất cho K giữa 5 và 300 trên tập dữ liệu MNIST,
cũng là một tập dữ liệu chiều thấp. Chúng tôi thảo luận về tập dữ liệu này riêng biệt
vì nó có số lượng mẫu lớn làm cho nó khác với các tập dữ liệu chiều thấp khác. Hình
10c thể hiện so sánh tương tự trên ba tập dữ liệu chiều cao, bao gồm SMK, GLA và
PCMAC. Cần lưu ý rằng để có so sánh công bằng, chúng tôi sử dụng một lõi CPU
duy nhất để chạy các phương pháp này; tuy nhiên, vì việc triển khai CAE và AEFS
được tối ưu hóa cho xử lý song song

--- TRANG 15 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 15
/uni00000029/uni00000026/uni00000024/uni00000028 /uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048
/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c
(a) Điểm 1. Điểm số được đưa ra
dựa trên xếp hạng của các
phương pháp.
/uni00000029/uni00000026/uni00000024/uni00000028 /uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000014/uni00000017/uni00000013/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048
/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c(b) Điểm 2. Điểm số được đưa ra
dựa trên giá trị chuẩn hóa
của mỗi mục tiêu.
/uni00000029/uni00000026/uni00000024/uni00000028 /uni00000026/uni00000024/uni00000028/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000024/uni00000028/uni00000029/uni00000036/uni0000000b/uni0000002a/uni00000033/uni00000038/uni0000000c /uni00000030/uni00000026/uni00000029/uni00000036 /uni0000002f/uni00000044/uni00000053/uni00000042/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048 /uni00000034/uni00000036
/uni00000030/uni00000048/uni00000057/uni0000004b/uni00000052/uni00000047/uni00000056/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000026/uni0000004f/uni00000058/uni00000056/uni00000057/uni00000048/uni00000055/uni0000004c/uni00000051/uni0000004a
/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000026/uni0000004f/uni00000044/uni00000056/uni00000056/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000044/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c
/uni00000035/uni00000058/uni00000051/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000057/uni0000004c/uni00000050/uni00000048
/uni00000030/uni00000048/uni00000050/uni00000052/uni00000055/uni0000005c
/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013
/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000003/uni00000014
(c) Trực quan hóa bản đồ nhiệt
của Điểm 1.
Hình 5: So sánh lựa chọn đặc trưng về độ chính xác phân loại, độ chính xác phân
cụm, tốc độ và yêu cầu bộ nhớ, trên mỗi tập dữ liệu và cho các giá trị K khác nhau,
sử dụng hai biến thể tính điểm.

tính toán, chúng tôi sử dụng GPU để chạy các phương pháp này. Chúng tôi cũng đo
thời gian chạy của lựa chọn đặc trưng với CAE trên CPU.
Để so sánh yêu cầu bộ nhớ của mỗi phương pháp, chúng tôi lập hồ sơ sử dụng bộ
nhớ tối đa trong quá trình lựa chọn đặc trưng cho các giá trị K khác nhau. Kết quả
được trình bày trong Hình 11 trong Phụ lục A.1, được tạo ra sử dụng thư viện Python
có tên resource4. Bên cạnh đó, để so sánh bộ nhớ được chiếm bởi các mô hình dựa
trên autoencoder, chúng tôi đếm số lượng tham số cho mỗi mô hình. Kết quả được
hiển thị trong Hình 14 trong Phụ lục A.3.

Tuy nhiên, việc so sánh tất cả các phương pháp này chỉ bằng cách nhìn vào các đồ
thị trong Hình 10 và Hình 11 là không dễ dàng, và sự đánh đổi giữa các yếu tố
không rõ ràng. Vì lý do này, chúng tôi tính toán hai điểm số để tính đến tất cả các
chỉ số này đồng thời.

Điểm 1. Để tính toán điểm số này, trên mỗi tập dữ liệu và cho mỗi giá trị K, chúng
tôi xếp hạng các phương pháp dựa trên thời gian chạy, yêu cầu bộ nhớ, độ chính
xác phân cụm và độ chính xác phân loại. Sau đó, chúng tôi cho điểm 1 cho người
thực hiện tốt nhất và tốt thứ hai; điều này chủ yếu do thực tế là trong hầu hết các
trường hợp, sự khác biệt giữa hai người này là không đáng kể. Sau đó, chúng tôi
tính toán tổng của các điểm số này cho mỗi phương pháp trên tất cả các tập dữ liệu.
Kết quả được trình bày trong Hình 5a; để dễ dàng so sánh các thành phần khác nhau
trong điểm số, một trực quan hóa bản đồ nhiệt của các điểm số được trình bày trong
Hình 5c. Điểm số tích lũy cho mỗi phương pháp bao gồm bốn phần tương ứng với
mỗi chỉ số được xem xét. Như rõ ràng trong Hình này, QuickSelection (điểm số tích
lũy của QuickSelection 10 và QuickSelection 100) vượt trội hơn tất cả các phương
pháp khác với một khoảng cách đáng kể. Phương pháp được đề xuất của chúng tôi
có thể đạt được sự đánh đổi tốt nhất giữa độ chính xác, thời gian chạy và sử dụng
bộ nhớ, trong số tất cả các phương pháp này. Laplacian score, người thực hiện tốt
thứ hai, có hiệu suất tốt về thời gian chạy và bộ nhớ, trong khi nó không thể hoạt
động tốt về độ chính xác. Mặt khác, CAE có hiệu suất thỏa đáng về độ chính xác.
Tuy nhiên, nó không nằm trong hai người thực hiện tốt nhất về tài nguyên tính toán
cho bất kỳ giá trị K nào. Cuối cùng, FCAE và AEFS không thể đạt được hiệu suất
tốt so với các phương pháp khác. Phiên bản chi tiết hơn của Hình 5a có sẵn trong
Hình 12 trong Phụ lục A.1.

Điểm 2. Ngoài điểm số dựa trên xếp hạng, chúng tôi tính toán một điểm số khác để
xem xét tất cả các phương pháp, ngay cả những phương pháp xếp hạng thấp hơn.
Với mục đích này, trên mỗi tập dữ liệu và giá trị K, chúng tôi chuẩn hóa mỗi chỉ số
hiệu suất giữa 0 và 1, sử dụng các giá trị của người thực hiện tốt nhất và người thực
hiện tệ nhất trên mỗi chỉ số. Giá trị 1 trong điểm độ chính xác có nghĩa là độ chính
xác cao nhất. Tuy nhiên, đối với bộ nhớ và thời gian chạy, giá trị 1 có nghĩa là yêu
cầu bộ nhớ ít nhất và thời gian chạy ít nhất tương ứng. Sau khi chuẩn hóa các chỉ
số, chúng tôi tích lũy các giá trị chuẩn hóa cho mỗi phương pháp và trên tất cả các
tập dữ liệu. Kết quả được mô tả trong Hình 5b. Như có thể thấy trong biểu đồ này,
QuickSelection (chúng tôi xem xét kết quả của QuickSelection 100) vượt trội hơn
các phương pháp khác với một khoảng cách lớn. CAE có hiệu suất gần với QuickSelection
về cả hai chỉ số độ chính xác, trong khi nó có hiệu suất kém về bộ nhớ và thời gian
chạy. Ngược lại, Lap_score hiệu quả về mặt tính toán trong khi có điểm độ chính
xác thấp nhất. Tóm lại, có thể quan sát trong Hình 5b, rằng QuickSelection đạt
được sự đánh đổi tốt nhất của bốn mục tiêu trong số các phương pháp được xem xét.

Tiêu thụ Năng lượng. Phân tích tiếp theo chúng tôi thực hiện liên quan đến tiêu thụ
năng lượng của mỗi phương pháp. Chúng tôi ước tính tiêu thụ năng lượng của mỗi
phương pháp sử dụng thời gian chạy của thuật toán tương ứng cho mỗi tập dữ liệu
và giá trị K. Chúng tôi giả định rằng mỗi phương pháp sử dụng công suất tối đa
của tài nguyên tính toán tương ứng trong thời gian chạy của nó. Do đó, chúng tôi
rút ra mức tiêu thụ điện năng của mỗi phương pháp, sử dụng thời gian chạy và mức
tiêu thụ điện năng tối đa của CPU và/hoặc GPU, có thể được tìm thấy trong thông
số kỹ thuật của mô hình CPU hoặc GPU tương ứng. Như được hiển thị trong Hình
13 trong Phụ lục A.2, lựa chọn đặc trưng Laplacian score cần lượng năng lượng ít
nhất trong số các phương pháp trên tất cả các tập dữ liệu ngoại trừ tập dữ liệu MNIST.
QuickSelection 10 là người thực hiện tốt nhất trên MNIST về tiêu thụ năng lượng.
Laplacian score và MCFS nhạy cảm với số lượng mẫu. Chúng không thể hoạt động
tốt trên MNIST, cả về độ chính xác lẫn hiệu quả. Sử dụng bộ nhớ tối đa trong quá
trình lựa chọn đặc trưng cho Laplacian score và MCFS trên MNIST lần lượt là 56
GB và 85 GB. Do đó, chúng không phải là lựa chọn tốt trong trường hợp có số lượng
mẫu lớn. QuickSelection là người thực hiện tốt thứ hai về tiêu thụ năng lượng, và
cũng là người thực hiện tốt nhất trong số các phương pháp dựa trên autoencoder.
QuickSelection không nhạy cảm với số lượng mẫu hoặc số lượng chiều.

Hiệu quả vs Độ chính xác. Để nghiên cứu sự đánh đổi giữa độ chính xác và hiệu
quả tài nguyên, chúng tôi thực hiện một phân tích sâu khác. Trong phân tích này,
chúng tôi vẽ biểu đồ sự đánh đổi giữa độ chính xác (bao gồm độ chính xác phân
loại và phân cụm) và yêu cầu tài nguyên (bao gồm bộ nhớ và tiêu thụ năng lượng).
Kết quả được hiển thị trong Hình 6 và 7 tương ứng với sự đánh đổi năng lượng-độ
chính xác và bộ nhớ-độ chính xác. Mỗi điểm trong những biểu đồ này đề cập đến
kết quả của một tổ hợp cụ thể giữa một phương pháp và tập dữ liệu cụ thể khi chọn
50 đặc trưng (ngoại trừ Madelon, mà chúng tôi chọn 20 đặc trưng). Như có thể quan
sát trong những biểu đồ này, QuickSelection, MCFS và Lap_score thường có sự đánh
đổi tốt giữa các chỉ số được xem xét. Một sự đánh đổi tốt giữa một cặp chỉ số là
tối đa hóa độ chính xác (độ chính xác phân loại hoặc phân cụm) trong khi tối thiểu
hóa chi phí tính toán (tiêu thụ điện năng hoặc yêu cầu bộ nhớ). Tuy nhiên, khi số
lượng mẫu tăng (trên tập dữ liệu MNIST), cả MCFS và Lap_score đều thất bại trong
việc duy trì chi phí tính toán thấp và độ chính xác cao. Do đó, khi kích thước tập
dữ liệu tăng, hai phương pháp này không phải là lựa chọn tối ưu. Trong số các phương
pháp dựa trên autoencoder, trong hầu hết các trường hợp QuickSelection 10

--- TRANG 16 ---
16 Zahra Atashgahi et al.
20 30 40 50 60
Độ chính xác phân cụm (%)104
103
102
101
100101102103Tiêu thụ điện năng (Kwh)
20 30 40 50 60 70 80 90 100
Độ chính xác phân loại (%)104
103
102
101
100101102103Tiêu thụ điện năng (Kwh)Coil20
Isolet
HAR
Madelon
MNIST
SMK
GLA
PCMAC
FCAE
CAE(GPU)
AEFS(GPU)
MCFS
Lap_score
QS_10
QS_100
Hình 6: Ước tính tiêu thụ điện năng (Kwh) vs. độ chính xác (%) khi chọn 50 đặc
trưng (ngoại trừ Madelon mà chúng tôi chọn 20 đặc trưng). Mỗi điểm đề cập đến
kết quả của một tập dữ liệu đơn lẻ (được chỉ định bởi màu sắc) và phương pháp
(được chỉ định bởi markers) trong đó trục x và y cho thấy độ chính xác và ước tính
tiêu thụ điện năng tương ứng.

20 30 40 50 60
Độ chính xác phân cụm (%)02468Yêu cầu bộ nhớ tối đa (Kb)1e7
20 30 40 50 60 70 80 90 100
Độ chính xác phân loại (%)02468Yêu cầu bộ nhớ tối đa (Kb)1e7
20 30 40 50 60
Độ chính xác phân cụm (%)0.00.51.01.52.02.53.0Yêu cầu bộ nhớ tối đa (Kb)1e6
20 30 40 50 60 70 80 90 100
Độ chính xác phân loại (%)0.00.51.01.52.02.53.0Yêu cầu bộ nhớ tối đa (Kb)1e6
Coil20
Isolet
HAR
Madelon
MNIST
SMK
GLA
PCMAC
FCAE
CAE(GPU)
AEFS(GPU)
MCFS
Lap_score
QS
Hình 7: Yêu cầu bộ nhớ tối đa (Kb) vs. độ chính xác (%) khi chọn 50 đặc trưng
(ngoại trừ Madelon mà chúng tôi chọn 20 đặc trưng). Mỗi điểm đề cập đến kết quả
của một tập dữ liệu đơn lẻ (được chỉ định bởi màu sắc) và phương pháp (được chỉ
định bởi markers) trong đó trục x và y cho thấy độ chính xác và yêu cầu bộ nhớ
tối đa tương ứng. Do yêu cầu bộ nhớ cao của MCFS và Lap_score trên tập dữ liệu
MNIST làm cho việc so sánh các kết quả khác trở nên khó khăn (biểu đồ trên), chúng
tôi phóng to phần này trong biểu đồ dưới.

và QuickSelection 100 nằm trong số các điểm tối ưu Pareto. Một lợi thế đáng kể
khác của phương pháp được đề xuất của chúng tôi là nó đưa ra xếp hạng của các
đặc trưng như đầu ra. Do đó, khác với MCFS hoặc CAE cần giá trị K như đầu vào
của chúng, QuickSelection không phụ thuộc vào K và chỉ cần một lần huấn luyện
duy nhất của mô hình sparse DAE cho bất kỳ giá trị K nào. Do đó, chi phí tính toán
của QuickSelection là như nhau cho tất cả các giá trị K, và chỉ cần một lần chạy
duy nhất của thuật toán này để có được tầm quan trọng phân cấp của các đặc trưng.

--- TRANG 17 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 17
0 5000 10000 15000 20000 25000 30000 35000 40000
# đặc trưng02500500075001000012500150001750020000thời gian chạy (s)
QS_10 (K=All, n=1000, CPU)
QS_100 (K=All, n=1000, CPU)
QS_100 (K=All, n=10000, CPU)
CAE (K=100, n=150, GPU)
CAE (K=300, n=450, GPU)
CAE (K=100, n=1000, GPU)
CAE (K=100, n=10000, GPU)
CAE (K=100, n=150, CPU)
AEFS (K=All, n=300, GPU)
AEFS (K=All, n=1000, GPU)
AEFS (K=All, n=10000, GPU)
FCAE (K=All, n=1000, CPU)
FCAE (K=All, n=10000, CPU)
Hình 8: So sánh thời gian chạy trên tập dữ liệu được tạo nhân tạo. Các đặc trưng
được tạo sử dụng phân phối chuẩn và số lượng mẫu cho mỗi trường hợp là 5000.

5.2 So sánh Thời gian Chạy trên Tập dữ liệu được Tạo Nhân tạo
Trong phần này, chúng tôi thực hiện so sánh thời gian chạy của các phương pháp
lựa chọn đặc trưng dựa trên autoencoder trên một tập dữ liệu được tạo nhân tạo.
Vì trên các tập dữ liệu chuẩn cả số lượng đặc trưng và mẫu đều khác nhau, không
dễ dàng có thể so sánh rõ ràng hiệu quả của các phương pháp. Thí nghiệm này nhằm
so sánh thời gian huấn luyện tường thực thực tế của các mô hình trong một môi
trường được kiểm soát liên quan đến số lượng đặc trưng đầu vào và neuron ẩn.
Ngoài ra, trong Phụ lục E, chúng tôi đã thực hiện một thí nghiệm khác liên quan
đến đánh giá các phương pháp trên một tập dữ liệu nhân tạo rất lớn, cả về tài nguyên
tính toán và độ chính xác.

Trong thí nghiệm này, chúng tôi nhằm so sánh tốc độ của QuickSelection so với
các phương pháp lựa chọn đặc trưng dựa trên autoencoder khác cho số lượng đặc
trưng đầu vào khác nhau. Chúng tôi chạy tất cả chúng trên một tập dữ liệu được
tạo nhân tạo với số lượng đặc trưng khác nhau và 5000 mẫu, trong 100 epoch huấn
luyện (10 epoch cho QuickSelection 10). Các đặc trưng của tập dữ liệu này được
tạo sử dụng phân phối chuẩn. Ngoài ra, chúng tôi nhằm so sánh thời gian chạy của
các cấu trúc khác nhau cho các thuật toán này. Thông số kỹ thuật của cấu trúc mạng
cho mỗi phương pháp, tài nguyên tính toán được sử dụng cho lựa chọn đặc trưng,
và kết quả tương ứng có thể được thấy trong Hình 8.

Đối với CAE, chúng tôi xem xét hai giá trị K khác nhau. Cấu trúc của CAE phụ
thuộc vào giá trị này. CAE có hai lớp ẩn bao gồm một bộ chọn concrete và một bộ
giải mã có K và 1.5K neuron tương ứng. Do đó, bằng cách tăng số lượng đặc trưng
được chọn, thời gian chạy của mô hình cũng sẽ tăng. Ngoài ra, chúng tôi xem xét
các trường hợp của CAE với 1000 và 10000 neuron ẩn trong lớp giải mã (được thay
đổi thủ công trong mã) để có thể so sánh nó với các mô hình khác. Chúng tôi cũng
đo thời gian chạy của việc thực hiện lựa chọn đặc trưng với CAE chỉ sử dụng một
lõi CPU duy nhất. Có thể thấy từ Hình 8 rằng thời gian chạy của nó là đáng kể
cao. Cấu trúc chung của AEFS, QuickSelection và FCAE tương tự về số lượng lớp
ẩn. Chúng là autoencoder cơ bản với một lớp ẩn duy nhất. Đối với AEFS, chúng tôi
xem xét ba cấu trúc với số lượng neuron ẩn khác nhau, bao gồm 300, 1000 và 10000.
Cuối cùng, đối với QuickSelection và FCAE, chúng tôi xem xét hai giá trị khác nhau
cho số lượng neuron ẩn, bao gồm 1000 và 10000.

Có thể quan sát thấy rằng thời gian chạy của AEFS với 1000 và 10000 neuron ẩn
sử dụng GPU, lớn hơn nhiều so với thời gian chạy của QuickSelection 100 với số
lượng neuron ẩn tương tự chỉ sử dụng một lõi CPU duy nhất tương ứng. Mẫu tương
tự cũng có thể thấy trong trường hợp của CAE với 1000 và 10000 neuron ẩn. Mẫu
này cũng lặp lại trong trường hợp của FCAE với 10000 neuron ẩn. Thời gian chạy
của FCAE với 1000 neuron ẩn gần tương tự với QuickSelection 100. Tuy nhiên, sự
khác biệt giữa hai phương pháp này đáng kể hơn khi chúng tôi tăng số lượng neuron
ẩn lên 10000. Điều này chủ yếu do thực tế là sự khác biệt giữa số lượng tham số của
QuickSelection và các phương pháp khác trở nên cao hơn nhiều đối với các giá trị
K lớn. Bên cạnh đó, những quan sát này mô tả rằng thời gian chạy của QuickSelection
không thay đổi đáng kể bằng cách tăng số lượng neuron ẩn.

Như chúng tôi cũng đã đề cập trước đây, QuickSelection đưa ra xếp hạng của các
đặc trưng như đầu ra. Do đó, khác với CAE phải được chạy riêng biệt cho các giá
trị K khác nhau, QuickSelection không bị ảnh hưởng bởi việc chọn K vì nó tính toán
tầm quan trọng của tất cả các đặc trưng cùng một lúc và sau khi hoàn thành việc
huấn luyện. Tóm lại, QuickSelection 10 có thời gian chạy ít nhất trong số các phương
pháp dựa trên autoencoder khác trong khi độc lập với giá trị K. Ngoài ra, khác với
các phương pháp khác, thời gian chạy của QuickSelection không nhạy cảm với số
lượng neuron ẩn vì số lượng tham số thấp ngay cả đối với một lớp ẩn rất lớn.

5.3 Phân tích Sức mạnh Neuron
Trong phần này, chúng tôi thảo luận về tính hợp lệ của sức mạnh neuron như một
thước đo tầm quan trọng của đặc trưng. Chúng tôi quan sát sự phát triển của mạng
trong quá trình huấn luyện để phân tích cách sức mạnh neuron của các neuron quan
trọng và không quan trọng thay đổi trong quá trình huấn luyện.

Chúng tôi lập luận rằng các đặc trưng quan trọng nhất dẫn đến độ chính xác cao
nhất của lựa chọn đặc trưng là các đặc trưng tương ứng với các neuron có sức mạnh
cao nhất. Trong một mạng neural, độ lớn trọng số là một chỉ số cho thấy tầm quan
trọng của mỗi kết nối [ 29]. Điều này xuất phát từ thực tế là các trọng số có độ lớn
nhỏ có tác động nhỏ đến hiệu suất của mô hình. Ở đầu quá trình huấn luyện, chúng
tôi khởi tạo tất cả các kết nối với một giá trị ngẫu nhiên nhỏ. Do đó, tất cả các neuron
có gần như cùng sức mạnh/tầm quan trọng. Khi quá trình huấn luyện tiến triển, một
số kết nối phát triển thành giá trị lớn hơn trong khi một số khác bị tỉa khỏi mạng
trong quá trình loại bỏ và tăng trưởng lại kết nối động của quy trình huấn luyện
SET. Sự phát triển của các trọng số kết nối ổn định chứng minh tầm quan trọng của
chúng trong hiệu suất của mạng. Kết quả là, các neuron được kết nối với những
trọng số quan trọng này chứa thông tin quan trọng. Ngược lại, độ lớn của các trọng
số được kết nối với các neuron không quan trọng giảm dần cho đến khi chúng bị
loại bỏ khỏi mạng. Tóm lại, các neuron quan trọng nhận được các kết nối với độ
lớn lớn hơn. Kết quả là, sức mạnh neuron, là tổng độ lớn của các trọng số được
kết nối với một neuron, có thể là một thước đo tầm quan trọng của một neuron đầu
vào và đặc trưng tương ứng của nó.

--- TRANG 18 ---
18 Zahra Atashgahi et al.
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000038/uni00000051/uni0000004c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni00000031/uni00000048/uni00000058/uni00000055/uni00000052/uni00000051/uni00000056/uni00000003/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000010/uni00000003/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni0000002c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni00000031/uni00000048/uni00000058/uni00000055/uni00000052/uni00000051/uni00000056/uni00000003/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000010/uni00000003/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000038/uni00000051/uni0000004c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni00000031/uni00000048/uni00000058/uni00000055/uni00000052/uni00000051/uni00000056/uni00000003/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000010/uni00000003/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000003/uni0000000b/uni00000006/uni0000000c/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni0000002c/uni00000050/uni00000053/uni00000052/uni00000055/uni00000057/uni00000044/uni00000051/uni00000057/uni00000003/uni00000031/uni00000048/uni00000058/uni00000055/uni00000052/uni00000051/uni00000056/uni00000003/uni00000036/uni00000057/uni00000055/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000010/uni00000003/uni00000034/uni00000036/uni00000042/uni00000014/uni00000013/uni00000013
Hình 9: Sức mạnh của 20 đặc trưng thông tin và không thông tin nhất của tập dữ
liệu Madelon, được chọn bởi QS10 và QS100. Mỗi đường trong biểu đồ tương ứng
với giá trị sức mạnh của một đặc trưng được chọn bởi QS10/QS100 trong quá trình
huấn luyện. Các đặc trưng được chọn bởi QS10 đã được quan sát cho đến epoch
100 để so sánh chất lượng của những đặc trưng này với QS100.

Để hỗ trợ tuyên bố của chúng tôi, chúng tôi quan sát sự phát triển của sức mạnh
neuron trên tập dữ liệu Madelon. Lựa chọn này được thực hiện do sự phân biệt của
các đặc trưng thông tin và không thông tin trong tập dữ liệu Madelon. Như đã mô
tả trước đây, tập dữ liệu này có 20 đặc trưng thông tin, và phần còn lại của các đặc
trưng là nhiễu không thông tin. Chúng tôi xem xét 20 đặc trưng thông tin và không
thông tin nhất được phát hiện bởi QS10 và QS100, và theo dõi sức mạnh của chúng
trong quá trình huấn luyện (như quan sát trong Hình 3, độ chính xác tối đa đạt được
bằng cách sử dụng 20 đặc trưng thông tin nhất, trong khi độ chính xác ít nhất đạt
được bằng cách sử dụng các đặc trưng ít quan trọng nhất). Các đặc trưng được chọn
bởi QS10 cũng được theo dõi sau khi thuật toán kết thúc (epoch 10) cho đến epoch
100, để so sánh chất lượng của các đặc trưng được chọn bởi QS10 với QS100. Nói
cách khác, chúng tôi trích xuất chỉ số của các đặc trưng quan trọng sử dụng QS10,
và tiếp tục huấn luyện mà không thực hiện bất kỳ thay đổi nào trong mạng và theo
dõi cách sức mạnh của các neuron tương ứng với chỉ số được chọn sẽ phát triển sau
epoch 10. Kết quả được trình bày trong Hình 9. Tại việc khởi tạo (epoch 0), sức
mạnh của tất cả các neuron này gần như tương tự và dưới 5. Khi quá trình huấn
luyện bắt đầu, sức mạnh của các neuron quan trọng tăng, trong khi sức mạnh của
các neuron không quan trọng không thay đổi đáng kể. Như có thể thấy trong Hình
9, một số đặc trưng quan trọng được chọn bởi QS10 không nằm trong số những cái
của QS100; điều này có thể giải thích sự khác biệt về hiệu suất của hai phương pháp
này trong Bảng 2 và 3. Tuy nhiên, QS10 có thể phát hiện một đa số lớn các đặc
trưng được tìm thấy bởi QS100; những đặc trưng này nằm trong số những cái quan
trọng nhất trong số 20 đặc trưng được chọn cuối cùng. Do đó, chúng tôi có thể kết
luận rằng hầu hết các đặc trưng quan trọng có thể phát hiện được bởi QuickSelection,
ngay cả trong vài epoch đầu tiên của thuật toán.

6 Kết luận
Trong bài báo này, một phương pháp mới (QuickSelection) cho lựa chọn đặc trưng
không giám sát tiết kiệm năng lượng đã được đề xuất. Nó giới thiệu sức mạnh neuron
trong mạng neural thưa thớt như một thước đo tầm quan trọng của đặc trưng. Bên
cạnh đó, nó đề xuất sparse DAE để mô hình hóa chính xác phân phối dữ liệu và xếp
hạng tất cả các đặc trưng đồng thời dựa trên tầm quan trọng của chúng. Bằng cách
sử dụng các lớp thưa thớt thay vì các lớp dày đặc từ đầu, số lượng tham số giảm
đáng kể. Kết quả là, QuickSelection yêu cầu ít bộ nhớ và tài nguyên tính toán hơn
nhiều so với mô hình dày đặc tương đương và các đối thủ cạnh tranh của nó. Ví dụ,
trên các tập dữ liệu chiều thấp, bao gồm Coil20, Isolet, HAR và Madelon, và cho
tất cả các giá trị K, QuickSelection 100 chạy trên một lõi CPU nhanh hơn ít nhất
4 lần so với đối thủ cạnh tranh trực tiếp của nó, CAE, chạy trên GPU, trong khi có
hiệu suất gần gũi về độ chính xác phân loại và phân cụm. Chúng tôi chứng minh
thực nghiệm rằng QuickSelection đạt được sự đánh đổi tốt nhất giữa độ chính xác
phân cụm, độ chính xác phân loại, yêu cầu bộ nhớ tối đa và thời gian chạy, trong
số các phương pháp khác được xem xét. Bên cạnh đó, phương pháp được đề xuất
của chúng tôi yêu cầu lượng năng lượng ít nhất trong số các phương pháp dựa trên
autoencoder được xem xét.

Nhược điểm chính của phương pháp được đề xuất là thiếu việc triển khai song
song. Thời gian chạy của QuickSelection có thể được giảm thêm bằng một việc triển
khai tận dụng CPU đa lõi hoặc GPU. Chúng tôi tin rằng nghiên cứu tương lai thú
vị sẽ là nghiên cứu tác động của huấn luyện thưa thớt

--- TRANG 19 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 19
và sức mạnh neuron trong các loại autoencoder khác cho lựa chọn đặc trưng, ví dụ:
CAE. Tuy nhiên, bài báo này chỉ mới bắt đầu khám phá một trong những đặc tính
quan trọng nhất của QuickSelection, tức là khả năng mở rộng, và chúng tôi dự định
khám phá thêm tiềm năng đầy đủ của nó trên các tập dữ liệu với hàng triệu đặc
trưng. Bên cạnh đó, bài báo này đã chỉ ra rằng chúng ta có thể thực hiện lựa chọn
đặc trưng sử dụng mạng neural một cách hiệu quả về chi phí tính toán và yêu cầu
bộ nhớ. Điều này có thể mở đường cho việc giảm chi phí tính toán ngày càng tăng
của các mô hình học sâu áp đặt lên các trung tâm dữ liệu. Kết quả là, điều này sẽ
không chỉ tiết kiệm chi phí năng lượng của việc xử lý dữ liệu có chiều cao mà còn
sẽ giảm bớt những thách thức của tiêu thụ năng lượng cao áp đặt lên môi trường.

Lời cảm ơn Nghiên cứu này đã được tài trợ một phần bởi dự án NWO EDIC.

Tài liệu tham khảo
1.Amirali Aghazadeh, Ryan Spring, Daniel Lejeune, Gautam Dasarathy, An-
shumali Shrivastava, et al. Mission: Ultra large-scale feature selection using
count-sketches. In International Conference on Machine Learning , pages 80–88,
2018.
2.Jun Chin Ang, Andri Mirzal, Habibollah Haron, and Haza Nuzly Abdull
Hamed. Supervised, unsupervised, and semi-supervised feature selection: a

--- TRANG 20 ---
20 Zahra Atashgahi et al.
review on gene selection. IEEE/ACM transactions on computational biology
and bioinformatics , 13(5):971–989, 2015.
3.Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge Luis
Reyes-Ortiz. A public domain dataset for human activity recognition using
smartphones. In Esann, 2013.
4.Pierre Baldi. Autoencoders, unsupervised learning, and deep architectures. In
Proceedings of ICML workshop on unsupervised and transfer learning , pages
37–49, 2012.
5.Muhammed Fatih Balın, Abubakar Abid, and James Zou. Concrete autoen-
coders: Differentiable feature selection and reconstruction. In International
Conference on Machine Learning , pages 444–453, 2019.
6.Alain Barrat, Marc Barthelemy, Romualdo Pastor-Satorras, and Alessandro
Vespignani. The architecture of complex weighted networks. Proceedings of
the national academy of sciences , 101(11):3747–3752, 2004.
7.GuillaumeBellec,DavidKappel,WolfgangMaass,andRobertLegenstein. Deep
rewiring: Training very sparse deep networks. arXiv preprint arXiv:1711.05136 ,
2017.
8.Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning:
A review and new perspectives. IEEE transactions on pattern analysis and
machine intelligence , 35(8):1798–1828, 2013.
9.Verónica Bolón-Canedo, Noelia Sánchez-Maroño, and Amparo Alonso-Betanzos.
Feature selection for high-dimensional data . Springer, 2015.
10.David D. Bourgin, Joshua C. Peterson, Daniel Reichman, Stuart J. Rus-
sell, and Thomas L. Griffiths. Cognitive model priors for predicting hu-
man decisions. In Kamalika Chaudhuri and Ruslan Salakhutdinov, edi-
tors,Proceedings of the 36th International Conference on Machine Learn-
ing, volume 97 of Proceedings of Machine Learning Research , pages 5133–
5141, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http:
//proceedings.mlr.press/v97/peterson19a.html .
11.Deng Cai, Chiyuan Zhang, and Xiaofei He. Unsupervised feature selection for
multi-cluster data. In Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining , pages 333–342. ACM,
2010.
12.Girish Chandrashekar and Ferat Sahin. A survey on feature selection methods.
Computers & Electrical Engineering , 40(1):16–28, 2014.
13. François Chollet et al. Keras. https://keras.io , 2015.
14.Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster
training without losing performance. arXiv preprint arXiv:1907.04840 , 2019.
15.Guillaume Doquet and Michèle Sebag. Agnostic feature selection. In Joint Eu-
ropean Conference on Machine Learning and Knowledge Discovery in Databases ,
pages 343–358. Springer, 2019.
16.Jennifer G Dy and Carla E Brodley. Feature selection for unsupervised learning.
Journal of machine learning research , 5(Aug):845–889, 2004.
17.Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich
Elsen. Rigging the lottery: Making all tickets winners. arXiv preprint
arXiv:1911.11134 , 2019.
18.Mark Fanty and Ronald Cole. Spoken letter recognition. In Advances in Neural
Information Processing Systems , pages 220–226, 1991.

--- TRANG 21 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 21
19.Ahmed K Farahat, Ali Ghodsi, and Mohamed S Kamel. Efficient greedy feature
selection for unsupervised learning. Knowledge and information systems , 35
(2):285–310, 2013.
20.Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding
sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 , 2018.
21.Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized
trees.Machine learning , 63(1):3–42, 2006.
22.AI High-Level Expert Group. Assessment list for trustworthy artificial intelli-
gence (ALTAI) for self-assessment, 2020.
23.Isabelle Guyon, Steve Gunn, Masoud Nikravesh, and Lofti A Zadeh. Feature
extraction: foundations and applications , volume 207. Springer, 2008.
24.Kai Han, Yunhe Wang, Chao Zhang, Chao Li, and Chao Xu. Autoencoder
inspired unsupervised feature selection. In 2018 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) , pages 2941–2945. IEEE,
2018.
25.Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and
connections for efficient neural network. In Advances in neural information
processing systems , pages 1135–1143, 2015.
26.Babak Hassibi and David G Stork. Second order derivatives for network
pruning: Optimal brain surgeon. In Advances in neural information processing
systems, pages 164–171, 1993.
27.Xiaofei He, Deng Cai, and Partha Niyogi. Laplacian score for feature selection.
InAdvances in neural information processing systems , pages 507–514, 2006.
28.Eric Jones, Travis Oliphant, and Pearu Peterson. Scipy: Open source scientific
tools for python. 2001.
29.Taskin Kavzoglu and Paul M Mather. Assessing artificial neural network prun-
ing algorithms. In Proceedings of the 24th Annual Conference and Exhibition
of the Remote Sensing Society , pages 9–11, 1998.
30.RonKohaviandGeorgeHJohn. Wrappersforfeaturesubsetselection. Artificial
intelligence , 97(1-2):273–324, 1997.
31.Thomas Navin Lal, Olivier Chapelle, Jason Weston, and André Elisseeeff.
Embedded methods. In Feature extraction , pages 137–165. Springer, 2006.
32.Ken Lang. Newsweeder: Learning to filter netnews. In Machine Learning
Proceedings 1995 , pages 331–339. Elsevier, 1995.
33.Cosmin Lazar, Jonatan Taminau, Stijn Meganck, David Steenhoff, Alain Co-
letta, Colin Molter, Virginie de Schaetzen, Robin Duque, Hugues Bersini, and
Ann Nowe. A survey on filter techniques for feature selection in gene expression
microarray analysis. IEEE/ACM Transactions on Computational Biology and
Bioinformatics , 9(4):1106–1119, 2012.
34.Yann LeCun. The mnist database of handwritten digits. http://yann. lecun.
com/exdb/mnist/ , 1998.
35.Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In
Advances in neural information processing systems , pages 598–605, 1990.
36.Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-
shot network pruning based on connection sensitivity. arXiv preprint
arXiv:1810.02340 , 2018.
37.Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P Trevino,
Jiliang Tang, and Huan Liu. Feature selection: A data perspective. ACM
Computing Surveys (CSUR) , 50(6):94, 2018.

--- TRANG 22 ---
22 Zahra Atashgahi et al.
38.Yifeng Li, Chih-Yu Chen, and Wyeth W Wasserman. Deep feature selec-
tion: theory and application to identify enhancers and promoters. Journal of
Computational Biology , 23(5):322–336, 2016.
39.Andy Liaw, Matthew Wiener, et al. Classification and regression by random-
forest.R news, 2(3):18–22, 2002.
40.Huan Liu and Hiroshi Motoda. Feature extraction, construction and selection:
A data mining perspective , volume 453. Springer Science & Business Media,
1998.
41.Shiwei Liu, Tim van der Lee, Anil Yaman, Zahra Atashgahi, Davide Ferrar,
Ghada Sokar, Mykola Pechenizkiy, and Decebal C Mocanu. Topological insights
into sparse neural networks. In Proceedings of the European Conference on
Machine Learning and Principles and Practice of Knowledge Discovery in
Databases (ECML PKDD) 2020. , 2020.
42.Yang Lu, Yingying Fan, Jinchi Lv, and William Stafford Noble. Deeppink:
reproducible feature selection in deep neural networks. In Advances in Neural
Information Processing Systems , pages 8676–8686, 2018.
43.Jianyu Miao and Lingfeng Niu. A survey on feature selection. Procedia
Computer Science , 91:919–926, 2016.
44.Decebal Constantin Mocanu, Elena Mocanu, Phuong H Nguyen, Madeleine
Gibescu, and Antonio Liotta. A topological insight into restricted boltzmann
machines. Machine Learning , 104(2-3):243–270, 2016.
45. Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen,
Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial neural
networks with adaptive sparse connectivity inspired by network science. Nature
communications , 9(1):2383, 2018.
46.Hesham Mostafa and Xin Wang. Parameter efficient training of deep convo-
lutional neural networks by dynamic sparse reparameterization. In Kamalika
Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th Interna-
tional Conference on Machine Learning , volume 97 of Proceedings of Machine
Learning Research , pages 4646–4655, Long Beach, California, USA, 09–15 Jun
2019. PMLR. URL http://proceedings.mlr.press/v97/mostafa19a.html .
47.Sameer A Nene, Shree K Nayar, Hiroshi Murase, et al. Columbia object image
library (coil-20). 1996.
48.Razieh Sheikhpour, Mehdi Agha Sarram, Sajjad Gharaghani, and Mohammad
Ali Zare Chahooki. A survey on semi-supervised feature selection methods.
Pattern Recognition , 64:141–158, 2017.
49.Dinesh Singh and Makoto Yamada. Fsnet: Feature selection network on high-
dimensional biological data. arXiv preprint arXiv:2001.08322 , 2020.
50.Avrum Spira, Jennifer E Beane, Vishal Shah, Katrina Steiling, Gang Liu, Frank
Schembri, Sean Gilman, Yves-Martine Dumas, Paul Calner, Paola Sebastiani,
et al. Airway epithelial gene expression in the diagnostic evaluation of smokers
with suspect lung cancer. Nature medicine , 13(3):361–366, 2007.
51.Lixin Sun, Ai-Min Hui, Qin Su, Alexander Vortmeyer, Yuri Kotliarov, Sandra
Pastorino, Antonino Passaniti, Jayant Menon, Jennifer Walling, Rolando Bailey,
et al. Neuronal and glioma-derived stem cell factor induces angiogenesis within
the brain. Cancer cell , 9(4):287–300, 2006.
52.Mingkui Tan, Ivor W Tsang, and Li Wang. Towards ultrahigh dimensional
feature selection for big data. Journal of Machine Learning Research , 2014.

--- TRANG 23 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 23
53.LaurensVanDerMaaten,EricPostma,andJaapVandenHerik. Dimensionality
reduction: a comparative. J Mach Learn Res , 10(66-71):13, 2009.
54.Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
Extracting and composing robust features with denoising autoencoders. In
Proceedings of the 25th international conference on Machine learning , pages
1096–1103. ACM, 2008.
55.Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis.
Chemometrics and intelligent laboratory systems , 2(1-3):37–52, 1987.
56.JunYang,WenjingXiao,ChunJiang,MShamimHossain,GhulamMuhammad,
and Syed Umar Amin. Ai-powered green cloud and data center. IEEE Access ,
7:4195–4203, 2018.
57.Yi Yang, Heng Tao Shen, Zhigang Ma, Zi Huang, and Xiaofang Zhou. L2,
1-norm regularized discriminative feature selection for unsupervised. In Twenty-
Second International Joint Conference on Artificial Intelligence , 2011.
58.Zheng Zhao and Huan Liu. Semi-supervised feature selection via spectral
analysis. In Proceedings of the 2007 SIAM international conference on data
mining, pages 641–646. SIAM, 2007.
59.Hangyu Zhu and Yaochu Jin. Multi-objective evolutionary federated learning.
IEEE transactions on neural networks and learning systems , 2019.

--- TRANG 24 ---
24 Zahra Atashgahi et al.
Phụ lục
A Đánh giá Hiệu suất
Trong phụ lục này, chúng tôi so sánh tất cả các phương pháp từ các khía cạnh khác nhau bao gồm độ chính xác, sử dụng bộ nhớ, thời gian chạy, tiêu thụ năng lượng, và số lượng tham số. Chúng tôi thực hiện các thí nghiệm khác nhau để có cái nhìn sâu sắc về hiệu suất của QuickSelection.

A.1 Thảo luận: Đánh đổi Độ chính xác và Hiệu quả Tính toán
Trong phần này, chúng tôi so sánh hiệu suất của tất cả các phương pháp chi tiết hơn. Chúng tôi chạy lựa chọn đặc trưng cho các giá trị K khác nhau trên mỗi tập dữ liệu và sau đó đo hiệu suất.

Như được hiển thị trong Hình 10, chúng tôi so sánh độ chính xác phân cụm, độ chính xác phân loại, và thời gian chạy giữa các phương pháp cho các giá trị K khác nhau. So sánh về yêu cầu bộ nhớ tối đa (RAM) cũng được mô tả trong Hình 11. Đối với tất cả các phương pháp ngoại trừ CAE và AEFS, chúng tôi chạy các thí nghiệm trên một lõi CPU duy nhất. Vì việc triển khai CAE và AEFS được tối ưu hóa cho GPU, chúng tôi đo thời gian chạy của các phương pháp này sử dụng GPU. Tuy nhiên, chúng tôi cũng xem xét thời gian chạy của CAE sử dụng một lõi CPU duy nhất. Cần lưu ý rằng vì Laplacian score, AEFS, FCAE, và QuickSelection đưa ra xếp hạng của các đặc trưng như đầu ra của quá trình lựa chọn đặc trưng, chúng tôi chỉ cần chạy chúng một lần cho tất cả các giá trị K. Tuy nhiên, MCFS và CAE cần giá trị K như đầu vào của thuật toán của chúng. Vì vậy, thời gian chạy phụ thuộc vào giá trị K. Trong việc triển khai AEFS, K được sử dụng để đặt số lượng giá trị ẩn. Tuy nhiên, nó không phải là yêu cầu của thuật toán.

Chúng tôi tóm tắt kết quả của các biểu đồ nói trên trong Hình 12; chúng tôi so sánh các phương pháp sử dụng điểm số 1, được giới thiệu trong Phần 5.1. Điểm số này được tính toán dựa trên xếp hạng của các phương pháp về độ chính xác phân cụm, độ chính xác phân loại, thời gian chạy và bộ nhớ. Như được giải thích trong Phần 5.1, chúng tôi cho điểm một cho mỗi phương pháp là người thực hiện tốt nhất hoặc tốt thứ hai trong mỗi chỉ số được xem xét. Sau đó, chúng tôi tính toán tổng của tất cả các điểm số này trên tất cả các tập dữ liệu và trên tất cả các giá trị K; điểm số cuối cùng cho mỗi phương pháp có thể được thấy trong Hình 12. Cột đầu tiên mô tả kết quả trên các tập dữ liệu chiều thấp với số lượng mẫu thấp, bao gồm Coil20, Isolet, HAR và Madelon. Cột thứ hai hiển thị kết quả tương ứng với MNIST. Tương tự, cột thứ ba tương ứng với các tập dữ liệu chiều cao, bao gồm SMK, GLA và PCMAC. Tổng điểm số trên tất cả các tập dữ liệu này được hiển thị trong cột thứ 4. Trong Hình 12, có bốn hàng; hàng đầu tiên tương ứng với việc xem xét QuickSelection 10 và QuickSelection 100 đồng thời, và tổng điểm số của chúng được mô tả trong hàng thứ hai. Hai hàng cuối tương ứng với việc xem xét mỗi phương pháp này riêng biệt.

Tuy nhiên, vì hiệu suất của mỗi phương pháp có thể khác nhau trong mỗi nhóm ba tập dữ liệu, chúng tôi tính toán một phiên bản chuẩn hóa của điểm số 1, dựa trên số lượng tập dữ liệu trong mỗi nhóm. Ví dụ, Laplacian score có hiệu suất kém trên MNIST, và mẫu này sẽ tương tự trên các tập dữ liệu khác với số lượng mẫu lớn. Tuy nhiên, chỉ có một tập dữ liệu với số lượng mẫu lớn trong thí nghiệm này. Mặt khác, trên các tập dữ liệu chiều cao với số lượng mẫu thấp, phương pháp này có hiệu suất tốt về thời gian chạy, và chúng tôi có ba tập dữ liệu với những đặc tính như vậy. Vì vậy, chúng tôi chuẩn hóa các giá trị của điểm số 1, sao cho thay vì cho điểm một cho mỗi phương pháp, chúng tôi cho điểm một chia cho số lượng tập dữ liệu trong nhóm tương ứng. Kết quả của điểm số 1 chuẩn hóa được hiển thị trong cột cuối cùng của Hình 12.

A.2 Tiêu thụ Năng lượng
Chúng tôi thực hiện một thí nghiệm khác liên quan đến so sánh tiêu thụ năng lượng giữa tất cả các phương pháp. Kết quả được trình bày trong Hình 13. Chi tiết hơn về biểu đồ này được đưa ra trong bài báo trong Phần 5.1.

--- TRANG 25 ---
Lựa chọn đặc trưng nhanh và mạnh mẽ 25
[Nội dung từ trang 25 đến trang 26 chứa các biểu đồ và hình ảnh phức tạp mô tả so sánh hiệu suất. Tôi sẽ tiếp tục với văn bản chính]

--- TRANG 26 ---
26 Zahra Atashgahi et al.
Phụ lục
A Đánh giá Hiệu suất
Trong phụ lục này, chúng tôi so sánh tất cả các phương pháp từ các khía cạnh khác nhau bao gồm độ chính xác, sử dụng bộ nhớ, thời gian chạy, tiêu thụ năng lượng, và số lượng tham số. Chúng tôi thực hiện các thí nghiệm khác nhau để có cái nhìn sâu sắc về hiệu suất của QuickSelection.

A.1 Thảo luận: Đánh đổi Độ chính xác và Hiệu quả Tính toán
Trong phần này, chúng tôi so sánh hiệu suất của tất cả các phương pháp chi tiết hơn. Chúng tôi chạy lựa chọn đặc trưng cho các giá trị K khác nhau trên mỗi tập dữ liệu và sau đó đo hiệu suất.
Như được hiển thị trong Hình 10, chúng tôi so sánh độ chính xác phân cụm, độ chính xác phân loại, và thời gian chạy giữa các phương pháp cho các giá trị K khác nhau. So sánh về yêu cầu bộ nhớ tối đa (RAM) cũng được mô tả trong Hình 11. Đối với tất cả các phương pháp ngoại trừ CAE và AEFS, chúng tôi chạy các thí nghiệm trên một lõi CPU duy nhất. Vì việc triển khai CAE và AEFS được tối ưu hóa cho GPU, chúng tôi đo thời gian chạy của các phương pháp này sử dụng GPU. Tuy nhiên, chúng tôi cũng xem xét thời gian chạy của CAE sử dụng một lõi CPU duy nhất. Cần lưu ý rằng vì Laplacian score, AEFS, FCAE, và QuickSelection đưa ra xếp hạng của các đặc trưng như đầu ra của quá trình lựa chọn đặc trưng, chúng tôi chỉ cần chạy chúng một lần cho tất cả các giá trị K. Tuy nhiên, MCFS và CAE cần giá trị K như đầu vào của thuật toán của chúng. Vì vậy, thời gian chạy phụ thuộc vào giá trị K. Trong việc triển khai AEFS, K được sử dụng để đặt số lượng giá trị ẩn. Tuy nhiên, nó không phải là yêu cầu của thuật toán.

[Tiếp tục với phần còn lại của văn bản...]

A.3 Số lượng Tham số
Trong Hình 14, chúng tôi so sánh số lượng tham số của các phương pháp dựa trên autoencoder. FCAE, một autoencoder kết nối đầy đủ với 1000 neuron ẩn, có số lượng tham số cao nhất trên tất cả các tập dữ liệu. Mạng được đề xuất của chúng tôi, sparse DAE, có số lượng tham số thấp nhất trong hầu hết các trường hợp. Nó có 1000 neuron ẩn được kết nối thưa thớt với neuron đầu vào và đầu ra. Số lượng tham số của AEFS và CAE phụ thuộc vào số lượng đặc trưng được chọn. Như cũng đã đề cập trước đây, cấu trúc của AEFS tương tự như FCAE với sự khác biệt về số lượng neuron ẩn. Số lượng neuron ẩn trong việc triển khai AEFS được đặt thành K.
