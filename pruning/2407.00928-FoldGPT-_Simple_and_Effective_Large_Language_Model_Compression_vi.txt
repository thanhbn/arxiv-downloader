# 2407.00928.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2407.00928.pdf
# Kích thước tệp: 1309243 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
FoldGPT: Phương Án Nén Mô Hình Ngôn Ngữ Lớn Đơn Giản và Hiệu Quả

Songwei Liu∗Chao Zeng* Lianqiang Li Chenqian Yan
Lean Fu Xing Mei Fangmin Chen†
ByteDance Inc.
{ zengchaocs, cfangmin}@gmail.com,
{liusongwei.zju, lilianqiang, yanchenqian.i, fulean, xing.mei}@bytedance.com

Tóm tắt
Nhu cầu triển khai các mô hình ngôn ngữ lớn (LLMs) trên thiết bị di động tiếp tục gia tăng, được thúc đẩy bởi những lo ngại ngày càng tăng về bảo mật dữ liệu và chi phí đám mây. Tuy nhiên, băng thông mạng và hạn chế bộ nhớ đặt ra thách thức cho việc triển khai các mô hình hàng tỷ tham số trên thiết bị di động. Trong nghiên cứu này, chúng tôi điều tra các đầu ra của các lớp khác nhau trên nhiều quy mô LLMs khác nhau và phát hiện rằng đầu ra của hầu hết các lớp thể hiện sự tương đồng đáng kể. Hơn nữa, sự tương đồng này trở nên rõ rệt hơn khi kích thước mô hình tăng lên, cho thấy sự dư thừa đáng kể theo chiều sâu của các LLMs. Dựa trên quan sát này, chúng tôi đề xuất một chiến lược nén khối lượng mô hình hiệu quả, được gọi là FoldGPT, kết hợp việc loại bỏ khối và chia sẻ tham số khối. Chiến lược này bao gồm ba phần: (1) Dựa trên các tham số gating có thể học được, chúng tôi xác định thứ hạng tầm quan trọng của khối trong khi mô hình hóa hiệu ứng ghép nối giữa các khối. Sau đó chúng tôi xóa một số lớp dư thừa dựa trên tỷ lệ loại bỏ đã cho. (2) Đối với các khối được giữ lại, chúng tôi áp dụng chiến lược chia sẻ tham số nhóm được thiết kế đặc biệt, trong đó các khối trong cùng một nhóm chia sẻ trọng số giống hệt nhau, nén đáng kể số lượng tham số và giảm nhẹ chi phí độ trễ. (3) Sau khi chia sẻ các Khối này, chúng tôi "chữa trị" sự không khớp do độ thưa gây ra bằng một lượng nhỏ tinh chỉnh và giới thiệu chiến lược chưng cất lớp đuôi để cải thiện hiệu suất. Các thí nghiệm chứng minh rằng FoldGPT vượt trội hơn các phương pháp tiên tiến (SOTA) trước đây trong nén mô hình hiệu quả, chứng minh tính khả thi của việc đạt được giảm nhẹ mô hình thông qua việc loại bỏ khối và chia sẻ tham số đơn giản.

1 Giới thiệu
Gần đây, các LLMs đã thu hút sự chú ý to lớn trong lĩnh vực trí tuệ nhân tạo, điều này có thể được quy cho thành công của các mô hình như ChatGPT (Brown et al., 2020). Theo các quy luật tỷ lệ (Kaplan et al., 2020), các mô hình hàng đầu (như OPT (Zhang et al., 2022), BLOOM (Workshop et al., 2022), và LLaMA (Touvron et al., 2023)) có xu hướng tăng kích thước để cải thiện hiệu suất. Tuy nhiên, sự tăng trưởng đáng kể này trong quy mô mô hình đặt ra những thách thức nghiêm trọng cho việc triển khai đám mây. MobileLLM (Liu et al., 2024b) chỉ ra rằng nếu 5% thời gian của các cá nhân con người sẽ sử dụng dịch vụ LLM và gọi GPT-4 với tốc độ 50 Token/s, thì sẽ cần 100 triệu GPU H100 để đáp ứng yêu cầu tính toán. Việc tiêu thụ năng lượng và phát thải carbon dioxide kèm theo sẽ đưa ra những thách thức môi trường đáng kinh ngạc.

Để giải quyết những thách thức này, các nhà nghiên cứu đã bắt đầu triển khai LLMs trên điện thoại thông minh và thiết bị di động. Cách tiếp cận này tận dụng sức mạnh tính toán biên để giảm chi phí suy luận đám mây và bảo vệ quyền riêng tư của người dùng. Tuy nhiên, các thiết bị di động bị hạn chế bởi sức mạnh tính toán hạn chế, dung lượng bộ nhớ chính (DRAM), và nguồn điện. Ví dụ, dung lượng DRAM của các điện thoại hàng đầu chính thống dao động từ 6GB đến 12GB, và các APP chỉ có thể sử dụng 10% đến 20% trong số đó (Liu et al., 2024b). Do đó, khối lượng mô hình đã trở thành vấn đề cốt lõi cản trở việc triển khai LLMs trên thiết bị di động. Các nhà nghiên cứu đã đề xuất nhiều kỹ thuật khác nhau để giảm bớt gánh nặng tính toán và kích thước mô hình của LLMs trong khi bảo tồn hiệu suất của chúng, bao gồm cắt tỉa mô hình (Wang et al., 2019)(Xia et al., 2022)(Kurtic et al., 2022)(Ma et al., 2023), lượng tử hóa (Frantar et al., 2022), chưng cất (Sun et al., 2019)(Wang et al., 2020), v.v. Các phương pháp lượng tử hóa giảm yêu cầu truy cập bộ nhớ của mô hình bằng cách giảm độ rộng bit trọng số và kích hoạt, cho phép thích nghi với các đơn vị tính toán phần cứng mới để đạt được lợi ích tăng tốc và nén lưu trữ (Lin et al., 2024). Các thuật toán cắt tỉa có được các mô hình nhẹ hơn và thân thiện với phần cứng hơn bằng cách trực tiếp loại bỏ các tham số không quan trọng khỏi mô hình.

--- TRANG 2 ---
Hình 1: Tổng quan về FoldGPT của chúng tôi. (a) Chiến lược nén khối lượng hai bước, bao gồm loại bỏ khối có cổng và chia sẻ tham số theo nhóm. (b) Cấu trúc khối với các tham số gating có thể học được. (c) Cấu trúc chia sẻ tham số theo nhóm. Khối đầu tiên trong nhóm được gọi là khối cha, và các khối còn lại chia sẻ các tham số trọng số của khối cha, được gọi là các khối con.

Nghiên cứu cắt tỉa LLMs trước đây tập trung vào phân tích sự dư thừa trong chiều rộng của mô hình (Zhu et al., 2023). Việc giảm tham số được đạt được bằng cách loại bỏ head trong module Self-Attention và kênh trong module FFN, điều này phát sinh chi phí đào tạo lại cao (Ma et al., 2023).

Trong bài báo này, chúng tôi phân tích sự tương đồng của các giá trị kích hoạt trên mỗi khối trong LLMs và phát hiện rằng nhiều khối đóng góp tối thiểu, cho thấy sự dư thừa đáng kể trong chiều sâu của mô hình. Phát hiện này cũng được chứng minh bởi ShortGPT (Men et al., 2024), một công trình cùng thời gian. Để giải quyết sự dư thừa sâu này, chúng tôi đề xuất một chiến lược nén khối lượng dựa trên học tập cho các mô hình được đào tạo trước, được gọi là FoldGPT, bao gồm hai bước chính: loại bỏ khối và chia sẻ tham số khối. Như được hiển thị trong Hình 1(a), trong bước đầu tiên, tầm quan trọng của mỗi khối được đo lường bằng một tham số gating có thể học được. Với việc tinh chỉnh tối thiểu, chúng tôi có thể nhanh chóng có được thứ hạng tầm quan trọng của các khối, và loại bỏ các lớp dư thừa dựa trên tỷ lệ loại bỏ đã cho. Trong bước thứ hai, đối với các khối được bảo lưu, chúng tôi áp dụng chiến lược chia sẻ tham số nhóm được thiết kế đặc biệt, có nghĩa là nhiều khối sẽ chia sẻ cùng một bộ tham số trọng số. Cách tiếp cận này giảm đáng kể tổng lượng tham số và cải thiện tỷ lệ trúng cache (Liu et al., 2024b). Cuối cùng, chúng tôi giới thiệu các tham số có thể học được bổ sung cho mỗi khối được chia sẻ, điều này không ảnh hưởng đến hiệu quả tính toán, và sau đó thông qua tinh chỉnh chưng cất, chúng tôi nhanh chóng khôi phục mất mát hiệu suất do chia sẻ tham số gây ra. Theo hiểu biết tốt nhất của chúng tôi, FoldGPT là khung đầu tiên tích hợp loại bỏ khối và chia sẻ tham số để nén khối lượng LLM tối ưu. Các đóng góp chính của bài báo chúng tôi được tóm tắt như sau:

• Chúng tôi phân tích sự tương đồng của đầu ra khối trong LLMs, với kích thước tham số dao động từ hàng trăm megabyte đến hàng tỷ, chứng minh rằng LLM sở hữu sự dư thừa rộng rãi trong chiều sâu của chúng. Phát hiện này cho thấy rằng việc cắt tỉa mô hình có thể đạt được bằng cách trực tiếp loại bỏ các khối dư thừa.

• Chúng tôi đề xuất một tiêu chí đánh giá tầm quan trọng khối có thể học được và một chiến lược tinh chỉnh phân cực thích ứng. Không giống như metric BI (Men et al., 2024) được sử dụng bởi ShortGPT, cách tiếp cận của chúng tôi mô hình hóa các tương tác giữa các khối, dẫn đến hiệu suất vượt trội.

• Chúng tôi đề xuất chiến lược chia sẻ tham số nhóm cho các mô hình được đào tạo trước. Các khối trong một nhóm chia sẻ cùng một bộ trọng số, được bổ sung bởi một vài tham số có thể học được bổ sung, và sau đó một chiến lược chưng cất lớp đuôi được thiết kế đặc biệt để cải thiện hiệu suất.

• Chúng tôi tiến hành các thí nghiệm rộng rãi trên các mô hình đa quy mô: LLaMA-2-7B, Gemma-2B, TinyLLaMA-1.1B. Kết quả thí nghiệm chứng minh rằng ngay cả với việc loại bỏ 36% tham số, mô hình được cắt tỉa vẫn duy trì 96.25% hiệu suất của mô hình gốc, vượt trội hơn thuật toán cắt tỉa SOTA hiện tại.

2 Các công trình liên quan
Số lượng tham số đáng kể trong các mô hình hiệu suất cao đòi hỏi lưu trữ và bộ nhớ đáng kể, khiến các mô hình này không phù hợp cho các thiết bị có tài nguyên tính toán hạn chế. Do đó, nén mô hình đã trở thành một giải pháp đầy hứa hẹn cho vấn đề này. Trong phần này, chúng tôi cung cấp một cái nhìn tổng quan ngắn gọn về các công trình liên quan.

Lượng tử hóa Lượng tử hóa có thể được phân loại thành hai loại chính: Đào tạo Nhận thức Lượng tử hóa (QAT) (Liu et al., 2023; Ma et al., 2024) và Lượng tử hóa Sau Đào tạo (PTQ) (Dettmers et al., 2024; Frantar et al., 2022; Xiao et al., 2023; Lin et al., 2023a). Tuy nhiên, trong lĩnh vực LLMs, do chi phí đào tạo đáng kể, cả học thuật và công nghiệp chủ yếu tập trung vào các phương pháp PTQ. Trong số đó, (Dettmers et al., 2024) đề xuất LLM.int8(). Đầu tiên họ sử dụng lượng tử hóa theo vector cho hầu hết ma trận trọng số. Đối với các outlier nổi lên, họ kết hợp một sơ đồ phân tách độ chính xác hỗn hợp mới, tách biệt các chiều đặc trưng outlier thành một phép nhân ma trận 16-bit. Cùng với LLM.int8(), GPTQ (Frantar et al., 2022) đề xuất một cách tiếp cận nhận thức dữ liệu chính xác hơn thông qua một bộ giải quy mô lớn xấp xỉ để tối thiểu hóa lỗi l2 theo lớp. Hơn nữa, SmoothQuant (Xiao et al., 2023) không chỉ lượng tử hóa cả trọng số và kích hoạt, mà còn di chuyển offline khó khăn lượng tử hóa từ kích hoạt sang trọng số với một phép biến đổi tương đương toán học. Nhiều thí nghiệm đã chứng minh rằng các phương pháp PTQ đã trở thành các kỹ thuật hàng đầu để nén và triển khai các LLMs quy mô lớn một cách hiệu quả.

Cắt tỉa và Chia sẻ Tham số Các kỹ thuật cắt tỉa nhằm xác định và loại bỏ các tham số dư thừa hoặc ít quan trọng khỏi các mô hình, dẫn đến một ma trận trọng số thưa hơn. Các phương pháp này sử dụng các phương pháp phỏng đoán, như cắt tỉa dựa trên độ lớn, hoặc các cách tiếp cận tinh vi hơn như cắt tỉa dựa trên học tập, để xác định trọng số nào cần loại bỏ. Nghiên cứu trước đây chủ yếu phân tích sự dư thừa mô hình về mặt chiều rộng. Ví dụ, SparseGPT (Frantar and Alistarh, 2023) kết hợp nghịch đảo Hessian để cắt tỉa và cập nhật trọng số dư thừa tiếp theo, trong khi Wanda (Sun et al., 2023) đạt được một mô hình LLM thưa bằng cách sử dụng tiêu chí dựa trên tích của các giá trị tuyệt đối của trọng số và kích hoạt của chúng để bảo tồn outliers. (Ma et al., 2023) có được một mô hình nhẹ bằng cách loại bỏ các head dư thừa trong self-attention và các kênh dư thừa trong FFN, tiếp theo là tinh chỉnh chi phí thấp để khôi phục độ chính xác. Ngược lại, FoldGPT và ShortGPT (Men et al., 2024) của chúng tôi khai thác sự dư thừa chiều sâu mô hình để có được các mô hình nhẹ. Cụ thể, FoldGPT sử dụng các tham số gating có thể học được để mô hình hóa việc ghép nối giữa các khối và sử dụng chiến lược chia sẻ tham số để nén các tham số hiệu quả. Ý tưởng cơ bản của chia sẻ tham số là sử dụng cùng một bộ tham số cho nhiều phần của một LLM. Trong nghiên cứu hiện có (Ullrich et al., 2017; Lin et al., 2023b; Su et al., 2024; Liu et al., 2024a), các chiến lược chia sẻ tham số chủ yếu được sử dụng trong thiết kế cơ sở hạ tầng mô hình và đào tạo trước để tăng cường hiệu quả tính toán và giảm rủi ro overfitting, đặc biệt với dữ liệu hạn chế. Ví dụ, ALBERT (Liu et al., 2024a)

--- TRANG 3 ---
(a) LLaMA-2-7B
(b) Gemma-2B
(c) TinyLLaMA-1.1B

Hình 2: Phân tích dư thừa khối của các mô hình có kích thước từ 1B đến 7B. Đường màu đỏ biểu thị độ tương đồng cosine của đầu vào và đầu ra của Khối hiện tại, trong khi đường màu xanh biểu thị độ tương đồng cosine của đầu ra của Khối hiện tại và đầu vào của điểm bắt đầu.

sử dụng chiến lược chia sẻ tham số liên lớp để giảm hiệu quả số lượng tham số mô hình, đạt được kết quả đào tạo tốt hơn so với baseline có cùng số lượng tham số (Su et al., 2024). Hơn nữa, (Liu et al., 2024b) xác nhận rằng các mô hình sử dụng chia sẻ tham số thường đạt được hiệu suất tốt hơn trong quá trình đào tạo trước so với những mô hình không có chia sẻ. Không giống như những cách tiếp cận này, chúng tôi áp dụng chia sẻ tham số để nén mô hình đã được đào tạo. Bằng cách tích hợp một module chia sẻ chuyên biệt và chiến lược tinh chỉnh chưng cất, chúng tôi đạt được hiệu suất tiên tiến.

Chưng cất Chưng cất Tri thức (KD) (Hinton et al., 2015) được sử dụng rộng rãi để chuyển giao tri thức từ một mô hình lớn (giáo viên) sang một mô hình nhỏ hơn (học sinh) để cải thiện hiệu quả. Trong bối cảnh của LLMs, KD bảo tồn sự hiểu biết ngữ nghĩa và ngữ cảnh phong phú của các mô hình này. Các công trình trước sử dụng xác suất mục tiêu mềm hoặc biểu diễn trung gian từ mô hình giáo viên để hướng dẫn việc đào tạo mô hình học sinh bất khả tri nhiệm vụ. Ví dụ, DistilBERT (Sanh et al., 2019) giảm một nửa các lớp transformer trong mạng giáo viên, khởi tạo học sinh bằng cách chọn một lớp trong mỗi hai lớp từ giáo viên. Điều này đảm bảo học sinh giữ lại kiến trúc của giáo viên. TinyBERT (Jiao et al., 2019) và MobileBERT (Sun et al., 2020) chuyển giao tri thức tinh vi, bao gồm các trạng thái ẩn và phân phối self-attention, sử dụng các lớp tuyến tính hoặc các module bottleneck để căn chỉnh. Ngược lại, MiniLM (Wang et al., 2020) đơn giản hóa quy trình bằng cách chưng cất tri thức duy nhất từ module self-attention của khối Transformer cuối cùng, giảm bớt thách thức ánh xạ lớp. Loại bỏ khối và chia sẻ tham số nhóm dựa trên mô hình được đào tạo trước giới thiệu sự suy giảm hiệu suất bổ sung. Được truyền cảm hứng bởi MiniLM, chúng tôi sử dụng chưng cất q-k-v của khối cuối cùng trong quá trình tinh chỉnh. Kết quả thí nghiệm cho thấy rằng chiến lược này hiệu quả tăng cường hiệu suất của mô hình được nén.

3 Phương pháp luận

3.1 Phân tích dư thừa
Các LLMs chính thống thường được xếp chồng bởi các khối giải mã lặp lại, được gọi là Transformers (Lagler et al., 2013). Như được hiển thị trong Hình 1(b), một khối giải mã tiêu chuẩn chứa một module attention (ATT) và một module feed-forward (FFN). Đối với một LLM dựa trên transformer L-lớp, đầu vào của lớp thứ i được ký hiệu bởi Xi ∈ Rb×s×d, trong đó b, s và d lần lượt biểu thị kích thước batch, số lượng token, và các chiều ẩn. Đầu vào của lớp i+1 có thể được biểu diễn như sau:

Xi^ATT = Xi + Attention(LN(Xi))
Xi+1 = Xi^ATT + FFN(LN(Xi^ATT))  (1)

Vì các khối giải mã xếp chồng thường có cùng tham số cấu trúc, điều này có nghĩa là Xi và Xi+1 chia sẻ cùng các chiều, khiến chúng tôi điều tra vai trò mà mỗi khối đóng trong đường dẫn luồng thông tin. Như được hiển thị trong Hình 2, chúng tôi đã chọn ba mô hình: LLaMA-2 (Roumeliotis et al., 2023), Gemma (Team et al., 2024), và TinyLLaMA (Zhang et al., 2024), với kích thước tham số lần lượt là 1.1B, 2B, và 7B, để phân tích độ tương đồng cosine của các giá trị kích hoạt khác nhau trong mô hình. Các đường màu đỏ trong ba hình phụ phản ánh tầm quan trọng của một khối riêng lẻ mà không xem xét việc ghép nối liên khối, trong khi đường màu xanh mô hình hóa tầm quan trọng của các nhóm khối liên tiếp. Phân tích của chúng tôi đã đưa ra những phát hiện sau: (1) Độ tương đồng cosine của hầu hết các khối trung gian trong LLMs lớn hơn 0.9, cho thấy vai trò tối thiểu của chúng trong đường dẫn truyền thông tin. (2) Trong LLaMA-2-7B và Gemma-2B, độ tương đồng cosine của các kích hoạt trên nhiều khối liên tiếp vẫn trên 0.8, gợi ý tiềm năng cho việc loại bỏ khối theo nhóm. Dư thừa khối nhóm giảm khi kích thước mô hình giảm. Ví dụ, dưới tiêu chí độ tương đồng cosine lớn hơn 0.8, TinyLLaMA chỉ có thể loại bỏ hai khối liên tiếp. Dựa trên những phát hiện này, chúng tôi đề xuất FoldGPT, một chiến lược nén mô hình mới, kết hợp loại bỏ khối và chia sẻ tham số khối theo nhóm. Cái trước nhắm vào một số lượng nhỏ các khối đặc biệt dư thừa, và cái sau nhắm vào nhóm khối có độ dư thừa tương đối thấp hơn. Ngược lại, ShortGPT (Men et al., 2024) chỉ tập trung vào sự dư thừa của khối riêng lẻ và không thể thực hiện loại bỏ tham số khối theo nhóm do sự suy giảm hiệu suất đáng kể.

3.2 Loại bỏ khối có cổng
Trong bước đầu tiên của FoldGPT, chúng tôi trực tiếp loại bỏ một số khối theo tỷ lệ thưa đã chỉ định, có nghĩa là chúng tôi cần đánh giá chính xác tầm quan trọng của các khối và sắp xếp chúng. ShortGPT (Men et al., 2024) sử dụng Block Influence (BI), về cơ bản là độ tương đồng cosine, để đo lường tầm quan trọng của mỗi khối. Tuy nhiên, metric này có một khiếm khuyết đáng kể: nó không tính đến việc ghép nối giữa các khối, dẫn đến các giải pháp không tối ưu khi loại bỏ nhiều khối.

Bảng 1: Perplexity của WikiText2 và PTB trên LLaMA-2-7B với các tỷ lệ cắt tỉa khác nhau.

Tỷ lệ | Phương pháp | WikiText2 ↓ | PTB ↓
0 | - | 5.5012 | 20.642
15% | BI | 10.4837 | 32.980
 | FoldGPT | 7.430 | 24.537
27% | BI | 35.962 | 85.896
 | FoldGPT | 18.500 | 55.939

Để giải quyết vấn đề này, chúng tôi đề xuất chiến lược loại bỏ khối có cổng, giới thiệu một tham số gating có thể học được cho mỗi thành phần tối thiểu. Cách tiếp cận này cho phép xếp hạng chính xác hơn về tầm quan trọng của việc ghép nối liên khối thông qua tối ưu hóa chung. Như được hiển thị trong Hình 1(b), chúng tôi tái công thức hóa quá trình tính toán của một khối như sau:

Xi^ATT = Xi * (1 - g(αi^1)) + ATT(Xi) * g(αi^1)
Xi+1 = Xi^ATT * (1 - g(αi^2)) + FFN(Xi^ATT) * g(αi^2)  (2)

trong đó αi^1 và αi^2 lần lượt biểu thị tham số gating của module ATT và module FFN trong Khối giải mã thứ i, và g(.) ký hiệu hàm kích hoạt gating. Việc giới thiệu một hàm gating bổ sung nhằm khuyến khích các giá trị cổng cập nhật một cách mượt mà hướng tới phân cực trong quá trình đào tạo, sao cho một số giá trị hội tụ về chính xác bằng không trong khi những giá trị khác vẫn khác biệt đáng kể so với không. Quá trình này cho phép chúng tôi có được một xếp hạng chính xác về tầm quan trọng khối dựa trên các giá trị cổng sau đào tạo. Được truyền cảm hứng bởi (Guo et al., 2021), chúng tôi sử dụng công thức L0 làm mượt từ các phương pháp tối ưu hóa có nguyên tắc để đạt được mục tiêu này:

geps(x) = x² / (x² + eps)
ϕ(geps(x)) = 2 × x × eps / (x² + eps)  (3)

trong đó geps(x) và ϕ(geps(x)) biểu thị tính toán thuận và ngược của hàm cổng, tương ứng, trong đó eps là một siêu tham số cân bằng ổn định đào tạo và phân cực. Như được hiển thị trong Hình 3, chúng ta có thể quan sát rằng khi eps đủ nhỏ, geps(x) trở nên phân cực, và ϕ(geps(x)) bằng không chỉ tại các điểm cụ thể. Đặc tính này gợi ý rằng chúng ta có thể sử dụng geps(x) như một cơ chế gating, tích hợp nó vào mạng neural mà chúng ta muốn nén. Trong nghiên cứu của chúng tôi, eps được khởi tạo ở 0.1 và giảm dần trong quá trình đào tạo. Cách tiếp cận này đảm bảo cả ổn định đào tạo sớm và sự phân cực của các tham số gating vào cuối đào tạo. Để tiếp tục tăng cường sự phân cực của các tham số gating, chúng tôi cũng giới thiệu các ràng buộc tài nguyên vào hàm tối ưu hóa. Trong bài báo của chúng tôi, để đơn giản hóa vấn đề, chúng tôi trực tiếp sử dụng FLOPs (Floating Point Operations) như là thước đo tiêu thụ tài nguyên cho mỗi khối và xem xét việc giữ lại hoặc loại bỏ các module ATT và FFN đồng thời. Đối với một LLM L-lớp, hàm mục tiêu cuối cùng có thể được công thức hóa như dưới đây:

min_α L̄(W; α) = L(W; g(a)) + λ Σ(i=0 to L-1) g(αi)Si  (4)

trong đó L là hàm mất mát gốc, và α = {α0, ..., αL-1} biểu thị điểm số tầm quan trọng cuối cùng của mỗi khối, và λ là một yếu tố cân bằng. Vì có ít tham số để tối ưu hóa, quá trình tối ưu hóa là hiệu quả. Có được xếp hạng tầm quan trọng dựa trên các tham số gating ở bước 1K và trực tiếp so sánh nó với chuỗi dựa trên BI. Như được hiển thị trong Bảng 1, chúng tôi trực tiếp áp dụng chuỗi loại bỏ thu được thông qua chiến lược học tập có cổng mà không có bất kỳ tinh chỉnh nào và so sánh nó với các metric BI của ShortGPT (Xiao et al., 2023). Chiến lược của chúng tôi vượt trội đáng kể so với BI. Ở tỷ lệ loại bỏ 15%, perplexity (ppl) trên tập dữ liệu WikiText2 giảm 29%, và perplexity trên tập dữ liệu Penn Treebank (PTB) giảm 25%. Khi tỷ lệ cắt tỉa tiếp tục tăng lên 27%, lợi thế tiếp tục mở rộng đến 48.5% và 34.88%.

--- TRANG 4 ---
(a) Thuận dưới eps khác nhau
(b) Ngược dưới eps khác nhau

Hình 3: Đặc tính phân cực trên eps khác nhau.

3.3 Chia sẻ tham số theo nhóm
Trong bước thứ hai của FoldGPT, chúng tôi thực hiện chia sẻ tham số nhóm cho các khối còn lại. Cách tiếp cận này tiếp tục nén lượng tham số mô hình. Mặc dù nó không giảm tải tính toán, về mặt lý thuyết nó có thể tăng cường hiệu suất trên các thiết bị có tài nguyên lưu trữ hạn chế do tỷ lệ trúng cache được cải thiện (Liu et al., 2024b). Hình 1(c) minh họa chi tiết chia sẻ tham số khối cho kích thước nhóm là 2. Cụ thể, mỗi khối con chứa 4 lớp kết nối đầy đủ có tham số trọng số được tái sử dụng từ các lớp tương ứng của khối cha. Để cải thiện độ chính xác của việc khớp trọng số khối con bằng cách sử dụng trọng số khối cha, chúng tôi giới thiệu các hệ số tỷ lệ có thể học được bổ sung cho các trọng số trong mỗi khối con.

Bảng 2: So sánh perplexity khi lớp layernorm được đóng băng và không đóng băng.

Mô hình | Tỷ lệ | WikiText2 | PTB
--- | --- | --- | ---
 | 0% | 8.93 | 28.97
Gemma | 26.34% wo ln | 30.52 | 199.15
 | 26.34% w ln | 28.39 | 180.85
 | 0% | 7.93 | 19.39
TinyLLaMA | 36% wo ln | 80.43 | 223.14
 | 36% w ln | 70.82 | 199.01

Giả sử rằng trọng số thứ i trong một khối con là Wi^child ∈ R^(dout×din), trong đó dout và din biểu thị các kênh đầu ra và đầu vào tương ứng. Sau đó, tính toán thuận bây giờ có thể được biểu diễn như sau:

Wi^child = Wi^parent * Si^child
fi^child = (Wi^parent * Si^child) * X + B  (5)

trong đó Wi^parent ∈ R^(dout×din), Si^child ∈ R^dout và fi^child biểu thị tương ứng các trọng số được tái sử dụng, các tham số tỷ lệ bổ sung được giới thiệu bởi lớp con, và quá trình thuận của lớp con. Khi thực hiện tính toán thuận của lớp con, tham số tỷ lệ không giới thiệu thời gian tính toán bổ sung vì nó có thể được tích hợp một cách liền mạch vào quá trình xử lý đuôi. Ngoài ra, chúng tôi áp dụng hai kỹ thuật để cải thiện hiệu suất mô hình: thích ứng lại layernorm và tinh chỉnh trọng số cha. Điều này bao gồm việc mở hoàn toàn các tham số của lớp chuẩn hóa (LN) và mở một phần trọng số của lớp dense cha trong quá trình đào tạo tinh chỉnh tiếp theo. Để chứng minh hiệu quả của các kỹ thuật này, chúng tôi đã tiến hành các thí nghiệm loại bỏ trên Gemma-2B và TinyLLaMA-1.1B. Như được hiển thị trong Bảng 2, việc cho phép đào tạo LN có thể cải thiện hiệu quả chất lượng tạo sinh. Perplexity trên WikiText2 giảm lần lượt 2.13 và 9.61, trong khi perplexity trên PTB giảm lần lượt 18.3 và 24.13.

3.4 Tinh chỉnh chưng cất
Sau khi hoàn thành hai giai đoạn nén tham số, chúng tôi cần khôi phục độ chính xác thông qua tinh chỉnh. Để đẩy nhanh quá trình phục hồi mô hình và tăng cường hiệu quả dưới điều kiện dữ liệu hạn chế, chúng tôi sử dụng xấp xỉ thứ hạng thấp (LoRA) (Hu et al., 2021) để đào tạo sau cho mô hình được cắt tỉa. Đáng chú ý, trong quá trình tinh chỉnh, chỉ lớp dense trong khối cha và chuẩn hóa lớp trong khối con được kích hoạt. LoRA chỉ được áp dụng cho trọng số của lớp dense. Tính toán thuận của lớp phụ dưới LoRA có thể được biểu diễn như sau:

fi^child = ((Wi^parent + ΔWi^parent) * Si^child) * X + B  (6)

ΔWi^parent là giá trị cập nhật của trọng số lớp cha, có thể được phân tách thành ΔWi^parent = P * Q, trong đó P ∈ R^(dout×d⁻) và Q ∈ R^(d⁻×din). Vì d⁻ nhỏ hơn nhiều so với din và dout, việc phân tách này giảm độ phức tạp đào tạo và nhu cầu dữ liệu đào tạo quy mô lớn. Để tiếp tục đẩy nhanh tinh chỉnh và cải thiện hiệu suất mô hình, chúng tôi giới thiệu chiến lược chưng cất tri thức được truyền cảm hứng bởi (Wang et al., 2020). Do sự không căn chỉnh cấu trúc giữa mô hình gốc và mô hình nén sau khi loại bỏ khối và gấp, chúng tôi áp dụng mất mát chưng cất chỉ cho lớp cuối cùng. Cụ thể, mất mát chưng cất bao gồm hai thành phần: self-attention và quan hệ giá trị self-attention, được biểu thị bởi L(ΔW)^AT và L(ΔW)^VR, tương ứng. Mất mát cuối cùng có thể được biểu diễn như:

L̂ = L + λ(L(ΔW)^AT + L(ΔW)^VR)  (7)

4 Thí nghiệm

4.1 Thiết lập thí nghiệm
Mô hình Để đánh giá toàn diện hiệu suất của thuật toán chúng tôi, chúng tôi đã chọn LLaMA-2-7B (Touvron et al., 2023), Gemma-2B (Team et al., 2024), và TinyLLaMA-1.1B (Zhang et al., 2024) làm các mô hình cơ sở. Gemma-2B là một loạt mô hình mở nhẹ, tiên tiến được phát triển bởi Google, dựa trên cùng nghiên cứu và công nghệ như mô hình Gemini. TinyLLaMA-1.1B được đào tạo trước trên khoảng 3 nghìn tỷ token, tận dụng kiến trúc và tokenizer LLaMA-2. Kích thước nhỏ của nó làm cho nó phù hợp cho các ứng dụng với tài nguyên tính toán và bộ nhớ hạn chế. Cho rằng các mô hình nhỏ hơn thường thể hiện độ dư thừa thấp hơn, các so sánh liên quan đến các mô hình này đặc biệt có ý nghĩa để đánh giá hiệu quả của thuật toán chúng tôi.

Đào tạo Trong giai đoạn loại bỏ tham số gating, chúng tôi khởi tạo eps ở 0.1 và giảm nó bằng tỷ lệ giảm 0.97 mỗi 120 lần lặp. Trong quá trình đào tạo, tất cả các tham số mô hình được đóng băng, chỉ các tham số điều khiển cổng được giải phóng. Trong giai đoạn phục hồi, chúng tôi sử dụng phiên bản được làm sạch của tập dữ liệu Alpaca (Taori et al., 2023), bao gồm khoảng 50k mẫu. Đối với đào tạo LoRA, chúng tôi đặt thứ hạng LoRA d⁻ ở 8 và khởi tạo hệ số mất mát chưng cất λ ở 1e-5. Tỷ lệ học được đặt ở 1e-5 với 100 bước làm ấm. Kích thước batch đào tạo được chọn từ {32, 64} và bộ tối ưu hóa AdamW (Zhuang et al., 2022) được sử dụng trong các thí nghiệm của chúng tôi. Ngoài ra, chúng tôi phát hiện rằng thời gian đào tạo tối ưu là 2 epochs, vì đào tạo nhiều epochs hơn thậm chí có tác động tiêu cực đến hiệu suất mô hình. Các thí nghiệm của chúng tôi được tiến hành trên một GPU NVIDIA A100 với bộ nhớ 80GB, mất khoảng 3 giờ.

Nhiệm vụ được đánh giá Theo công trình trước, chúng tôi đánh giá mô hình dựa trên hai chỉ số: perplexity và điểm số lý luận thông thường zero-shot. Perplexity được tính toán dựa trên các tập dữ liệu WikiText-2 (Merity et al., 2016) và Penn Treebank (PTB) (Marcus et al., 1993), với độ dài cắt ngắn 2048 token mỗi mẫu. Để đánh giá hiệu suất trên các nhiệm vụ lý luận thông thường zero-shot, chúng tôi chọn một số nhiệm vụ phổ biến bao gồm BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), SCIQ (Welbl et al., 2017), ARC (Clark et al., 2018), WinoGrande (Sakaguchi et al., 2021) và MMLU (Hendrycks et al., 2020). Chúng tôi tuân thủ các thiết lập GPTQ (Frantar et al., 2022) cho các thí nghiệm tạo ngôn ngữ và sử dụng khung lm-eval-harness (Gao et al., 2021) để thực hiện tất cả các nhiệm vụ zero-shot.

4.2 Kết quả chính
Để xác thực lợi thế của thuật toán chúng tôi, chúng tôi đã so sánh FoldGPT với các thuật toán cắt tỉa có cấu trúc tiên tiến, bao gồm LLMPruner (Ma et al., 2023), SliceGPT (Ashkboos et al., 2024), LaCo (Yang et al., 2024) và ShortGPT (Men et al., 2024). LLMPruner và SliceGPT chủ yếu khai thác sự dư thừa trong chiều rộng mạng bằng cách nén các chiều embedding, trong khi Laco và ShortGPT tập trung vào loại bỏ sự dư thừa trong chiều sâu mạng. Để đảm bảo công bằng trong việc thiếu tinh chỉnh trong nghiên cứu so sánh, đầu tiên chúng tôi áp dụng loại bỏ khối dựa trên cơ chế gating cho LLaMA-2-7, đạt được độ thưa 27%. Như được hiển thị trong Bảng 3, so với các thuật toán cắt tỉa có cấu trúc dựa trên chiều rộng SliceGPT và LLMPruner, FoldGPT cho thấy những cải thiện đáng kể lần lượt là 13.71 và 8.9. Điều này cho thấy rằng cắt tỉa có cấu trúc dựa trên chiều sâu vượt trội hơn cắt tỉa dựa trên chiều rộng truyền thống trong LLMs, hỗ trợ quan điểm của chúng tôi rằng dư thừa chiều sâu cao hơn dư thừa chiều rộng. Trong so sánh với LaCo và ShortGPT, chiến lược loại bỏ khối dựa trên gating của FoldGPT vượt trội đáng kể so với chiến lược loại bỏ theo lớp của LaCo, chứng minh lợi thế đáng kể của cơ chế gating trong phân tích tầm quan trọng khối. Để tiếp tục khám phá tỷ lệ nén cực đoan của FoldGPT dưới chia sẻ tham số nhóm và đảm bảo so sánh công bằng, chúng tôi áp dụng cùng chiến lược tinh chỉnh cho cả ShortGPT và FoldGPT. Kết quả cho thấy FoldGPT đạt được cải thiện hiệu suất 1.33 so với ShortGPT với 9% chia sẻ tham số nhóm. Hơn nữa, duy trì tỷ lệ thưa 36%, FoldGPT đạt được cải thiện hiệu suất đáng kể 5.24% so với ShortGPT bằng cách phân bổ linh hoạt tỷ lệ loại bỏ khối và chia sẻ tham số nhóm, ngay cả với 9% độ thưa bổ sung so với ShortGPT.

Cuối cùng, để điều tra lợi thế hiệu suất của FoldGPT trong triển khai biên, chúng tôi đã tiến hành so sánh chi tiết với thuật toán cắt tỉa có cấu trúc hàng đầu hiện tại, ShortGPT, sử dụng mô hình Gemma-2B. Như được hiển thị trong Bảng 4, chúng tôi áp dụng kết hợp 17.56% loại bỏ khối và 4.39% gấp tham số. So với ShortGPT, chỉ sử dụng 17.56% loại bỏ khối, FoldGPT cải thiện độ chính xác benchmark 5.31% và tăng tỷ lệ nén 4.39%. Hơn nữa, trong so sánh giữa FoldGPT*2 và FoldGPT*3, FoldGPT*3 đạt được kết quả có thể so sánh với FoldGPT*2 bằng cách tiếp tục tăng tỷ lệ gấp khối, đồng thời cũng chứng minh cải thiện hiệu suất 8.35% và tăng 8.73% độ thưa so với ShortGPT. Những phát hiện này xác thực cách tiếp cận của chúng tôi trong các mô hình LLM biên và tiếp tục xác nhận lợi thế của FoldGPT so với các thuật toán cắt tỉa có cấu trúc khác.

--- TRANG 5 ---
Bảng 3: Hiệu suất Zero-shot trên LLaMA-2-7B. * chỉ ra rằng mô hình đã được tinh chỉnh với cùng cấu hình. Các siêu tham số chi tiết được mô tả trong phần 4.1

Mô hình | Phương pháp | Tỷ lệ | Benchmarks | Avg. Per. 
--- | --- | --- | --- | ---
 | | | HeSw | PIQA | BoolQ | MMLU | 
LLaMA-2-7B | Dense | 0.00% | 71.26 | 77.91 | 71.62 | 45.39 | 66.55 | 100%
 | LLMPruner | 27.0% | 56.46 | 71.22 | 55.20 | 23.33 | 51.55 | 77.46%
 | SliceGPT | 26.4% | 50.27 | 66.21 | 38.32 | 28.92 | 45.93 | 69.01%
 | LaCo | 27.1% | 55.69 | 69.80 | 64.07 | 26.45 | 54.00 | 81.14%
 | ShortGPT | 27.1% | 53.02 | 66.43 | 74.71 | 43.96 | 59.53 | 89.45%
 | FoldGPT | 27.1% | 53.22 | 67.00 | 73.21 | 45.14 | 59.64 | 89.61%
 | ShortGPT* | 27.1% | 57.7 | 67.70 | 73.20 | 43.70 | 60.57 | 91.01%
 | FoldGPT*1 | 27%+9% | 59.67 | 68.85 | 75.20 | 43.88 | 61.90 | 93.01%
 | FoldGPT*2 | 21%+15% | 61.25 | 73.22 | 75.20 | 44.04 | 63.41 | 95.28%
 | FoldGPT*3 | 15%+21% | 63.10 | 74.34 | 74.30 | 44.50 | 64.06 | 96.25%

Bảng 4: Hiệu suất Zero-shot trên Gemma-2B. * chỉ ra rằng mô hình đã được tinh chỉnh với cùng cấu hình. Các siêu tham số chi tiết được mô tả trong phần 4.1

Mô hình | Phương pháp | Tỷ lệ | Benchmarks | Avg. Per.
--- | --- | --- | --- | ---
 | | | WinGran | SCIQ | RACE | PIQA | BOOLQ | 
Gemma-2B | Dense | 0.00% | 65.50 | 90.21 | 36.26 | 76.93 | 69.41 | 66.44 | 100%
 | ShortGPT*1 | 13.17% | 52.22 | 74.9 | 25.55 | 65.58 | 52.61 | 54.17 | 81.53%
 | ShortGPT*2 | 17.56% | 51.01 | 69.7 | 24.49 | 63.27 | 50.70 | 51.83 | 78.02%
 | ShortGPT*3 | 21.95% | 48.51 | 68.8 | 25.26 | 61.61 | 49.04 | 50.64 | 76.23%
 | FoldGPT*1 | 8.78%+8.78% | 57.69 | 88.1 | 33.49 | 71.43 | 62.87 | 62.72 | 94.39%
 | FoldGPT*2 | 17.56%+4.39% | 54.77 | 78.2 | 28.42 | 63.76 | 60.58 | 57.15 | 86.00%
 | FoldGPT*3 | 17.56%+8.78% | 54.69 | 78.7 | 27.94 | 63.54 | 62.04 | 57.38 | 86.37%

5 Kết luận
Trong bài báo này, dựa trên sự tương đồng của đầu ra mỗi khối, chúng tôi đề xuất một phương pháp nén LLMs hiệu quả, FoldGPT, để giải quyết những thách thức của việc triển khai LLMs trên thiết bị di động. FoldGPT đầu tiên đề xuất một tiêu chí đánh giá tầm quan trọng khối có thể học được và một chiến lược tinh chỉnh phân cực thích ứng, có thể loại bỏ các khối tương đối không quan trọng một cách hợp lý hơn. Đối với các khối còn lại, FoldGPT sau đó đề xuất chiến lược chia sẻ tham số nhóm với một số lượng nhỏ tham số có thể học được bổ sung, có thể tiếp tục nén dấu chân LLMs. Để giảm bớt mất mát hiệu suất do hai chiến lược trên, FoldGPT giới thiệu chiến lược chưng cất lớp đuôi được thiết kế đặc biệt để cải thiện hiệu suất. Kết quả thí nghiệm rộng rãi chứng minh rằng FoldGPT có thể vượt trội hơn các thuật toán cắt tỉa SOTA đối với cùng tỷ lệ cắt tỉa.

Hạn chế
FoldGPT có thể giảm sử dụng dấu chân và cải thiện tốc độ suy luận để triển khai LLMs hiệu quả hơn. Tuy nhiên, vẫn còn một số hạn chế. Thứ nhất, FoldGPT được đề xuất cho các LLMs nơi các khối cơ bản được xếp chồng lặp đi lặp lại. Do đó, chiến lược này không thể được áp dụng cho các LLMs, tức là OpenELM (Mehta et al., 2024) nơi cấu hình tham số cấu trúc của mỗi khối khác nhau. Thứ hai, do độ chi tiết lớn của việc gấp tham số, tồn tại một số mất mát đối với hiệu suất mô hình, vì vậy chúng tôi cần hợp tác với tinh chỉnh để khôi phục độ chính xác. Thứ ba, trong bài báo này, chúng tôi chủ yếu tập trung vào việc thực hiện thuật toán vì hạn chế thời gian. Trong tương lai, chúng tôi sẽ tiếp tục thực hiện engine suy luận tương ứng và đạt được lợi ích tăng tốc hiệu suất thực tế.

Tuyên bố Đạo đức
Bài báo này trình bày các giải pháp cho những thách thức của việc cắt tỉa Mô hình Ngôn ngữ Lớn (LLMs) để tạo điều kiện cho việc áp dụng và ứng dụng rộng rãi của chúng. Hiện tại, các mối quan tâm đạo đức liên quan đến LLMs, chẳng hạn như các thiên kiến ẩn được mã hóa trong các mô hình, đang nhận được sự chú ý ngày càng tăng. Điều tra của chúng tôi cho thấy rằng phương pháp đề xuất của chúng tôi không khuếch đại những thiên kiến này hoặc vi phạm bất kỳ tiêu chuẩn đạo đức nào.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được dịch như sau:]

Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, và James Hensman. 2024. Slicegpt: Nén các mô hình ngôn ngữ lớn bằng cách xóa hàng và cột. arXiv preprint arXiv:2401.15024.

Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, và Yejin Choi. 2020. Piqa: Lý luận về thông thường vật lý trong ngôn ngữ tự nhiên. Trong Hội nghị AAAI lần thứ ba mươi tư về Trí tuệ Nhân tạo.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Các mô hình ngôn ngữ là những người học few-shot. Tiến bộ trong xử lý thông tin neural, 33:1877–1901.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, và Kristina Toutanova. 2019. Boolq: Khám phá độ khó đáng ngạc nhiên của các câu hỏi có/không tự nhiên. Trong Kỷ yếu Hội nghị 2019 của North American Chapter của Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 2924–2936. Association for Computational Linguistics.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, và Oyvind Tafjord. 2018. Nghĩ rằng bạn đã giải quyết việc trả lời câu hỏi? thử arc, thách thức lý luận ai2. arXiv preprint arXiv:1803.05457.

Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. 2024. Llm.int8(): Nhân ma trận 8-bit cho transformers ở quy mô. Trong Kỷ yếu Hội nghị Quốc tế lần thứ 36 về Hệ thống Xử lý Thông tin Neural, NIPS '22. Curran Associates Inc.

Elias Frantar và Dan Alistarh. 2023. Sparsegpt: Các mô hình ngôn ngữ khổng lồ có thể được cắt tỉa chính xác trong một lần. Trong Hội nghị Quốc tế về Học máy, trang 10323–10337. PMLR.

Elias Frantar, Saleh Ashkboos, Torsten Hoefler, và Dan Alistarh. 2022. Gptq: Lượng tử hóa sau đào tạo chính xác cho các transformer tạo sinh được đào tạo trước. arXiv preprint arXiv:2210.17323.

Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. 2021. Một khung cho đánh giá mô hình ngôn ngữ few-shot. Phiên bản v0. 0.1. Sept, trang 8.

Yi Guo, Huan Yuan, Jianchao Tan, Zhangyang Wang, Sen Yang, và Ji Liu. 2021. Gdp: Cắt tỉa mạng neural ổn định thông qua các cổng với phân cực có thể vi phân. Trong Kỷ yếu Hội nghị IEEE/CVF Quốc tế về Thị giác Máy tính, trang 5239–5250.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. 2020. Đo lường hiểu biết ngôn ngữ đa nhiệm khổng lồ. arXiv preprint arXiv:2009.03300.

Geoffrey Hinton, Oriol Vinyals, và Jeff Dean. 2015. Chưng cất tri thức trong mạng neural. arXiv preprint arXiv:1503.02531.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. 2021. Lora: Thích ứng thứ hạng thấp của các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2106.09685.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, và Qun Liu. 2019. Tinybert: Chưng cất bert cho hiểu biết ngôn ngữ tự nhiên. arXiv preprint arXiv:1909.10351.

--- TRANG 6 ---
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. 2020. Quy luật tỷ lệ cho các mô hình ngôn ngữ neural. arXiv preprint arXiv:2001.08361.

Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, và Dan Alistarh. 2022. Bác sĩ phẫu thuật bert tối ưu: Cắt tỉa bậc hai có thể mở rộng và chính xác cho các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2203.07259.

Klemens Lagler, Michael Schindelegger, Johannes Böhm, Hana Krásná, và Tobias Nilsson. 2013. Gpt2: Mô hình độ trễ xiên thực nghiệm cho các kỹ thuật trắc địa không gian radio. Geophysical research letters, 40(6):1069–1073.

Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, và Song Han. 2023a. Awq: Lượng tử hóa trọng số nhận thức kích hoạt để nén và tăng tốc llm. arXiv preprint arXiv:2306.00978.

Ye Lin, Mingxuan Wang, Zhexi Zhang, Xiaohui Wang, Tong Xiao, và Jingbo Zhu. 2023b. Hiểu chia sẻ tham số trong transformers. arXiv preprint arXiv:2306.09380.

Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, và Song Han. 2024. Qserve: Lượng tử hóa W4a8kv4 và thiết kế đồng hệ thống để phục vụ llm hiệu quả. arXiv preprint arXiv:2405.04532.

Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, et al. 2024a. Hiểu llms: Tổng quan toàn diện từ đào tạo đến suy luận. arXiv preprint arXiv:2401.02038.

Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, và Vikas Chandra. 2023. Llm-qat: Đào tạo nhận thức lượng tử hóa không dữ liệu cho các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2305.17888.

Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. 2024b. Mobilellm: Tối ưu hóa các mô hình ngôn ngữ dưới tỷ tham số cho các trường hợp sử dụng trên thiết bị. arXiv preprint arXiv:2402.14905.

Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, và Furu Wei. 2024. Kỷ nguyên của llms 1-bit: Tất cả các mô hình ngôn ngữ lớn đều trong 1.58 bit. arXiv preprint arXiv:2402.17764.

Xinyin Ma, Gongfan Fang, và Xinchao Wang. 2023. Llm-pruner: Về việc cắt tỉa cấu trúc của các mô hình ngôn ngữ lớn. Tiến bộ trong xử lý thông tin neural, 36:21702–21720.

Mitch Marcus, Beatrice Santorini, và Mary Ann Marcinkiewicz. 1993. Xây dựng một kho ngữ liệu tiếng Anh được chú thích lớn: Penn treebank. Computational linguistics, 19(2):313–330.

Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. 2024. Openelm: Một họ mô hình ngôn ngữ hiệu quả với khung đào tạo và suy luận mã nguồn mở. arXiv preprint arXiv:2404.14619.

Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, và Weipeng Chen. 2024. Shortgpt: Các lớp trong các mô hình ngôn ngữ lớn dư thừa hơn bạn mong đợi. arXiv preprint arXiv:2403.03853.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. 2016. Các mô hình hỗn hợp pointer sentinel. arXiv preprint arXiv:1609.07843.

Konstantinos I. Roumeliotis, Nikolaos D. Tselikas, và Dimitrios K. Nasiopoulos. 2023. Llama 2: Việc sử dụng mô hình được đào tạo trước mã nguồn mở mới của meta bởi những người áp dụng sớm. Preprints.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. 2021. Winogrande: Một thách thức sơ đồ winograd đối nghịch ở quy mô. Communications of the ACM, 64(9):99–106.

Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. 2019. Distilbert, một phiên bản chưng cất của bert: nhỏ hơn, nhanh hơn, rẻ hơn và nhẹ hơn. arXiv preprint arXiv:1910.01108.

Xiu Su, Shan You, Hongyan Xu, Xiuxing Li, Jun Long, Yi Chen, và Chang Xu. 2024. Vượt ra ngoài giới hạn của chia sẻ trọng số: Tiên phong nas tiến hóa không gian với các mô hình ngôn ngữ lớn. Trong ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), trang 6225–6229. IEEE.

Mingjie Sun, Zhuang Liu, Anna Bair, và J Zico Kolter. 2023. Một cách tiếp cận cắt tỉa đơn giản và hiệu quả cho các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2306.11695.

Siqi Sun, Yu Cheng, Zhe Gan, và Jingjing Liu. 2019. Chưng cất tri thức bệnh nhân để nén mô hình bert. arXiv preprint arXiv:1908.09355.

Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, và Denny Zhou. 2020. Mobilebert: một BERT bất khả tri nhiệm vụ nhỏ gọn cho các thiết bị hạn chế tài nguyên. Trong Kỷ yếu Cuộc họp Thường niên lần thứ 58 của Association for Computational Linguistics, trang 2158–2170. Association for Computational Linguistics.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B Hashimoto. 2023. Stanford alpaca: một mô hình llama tuân theo hướng dẫn (2023). URL https://github. com/tatsu-lab/stanford_alpaca.

Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Các mô hình mở dựa trên nghiên cứu và công nghệ gemini. arXiv preprint arXiv:2403.08295.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Các mô hình ngôn ngữ nền tảng mở và hiệu quả. arXiv preprint arXiv:2302.13971.

Karen Ullrich, Edward Meeds, và Max Welling. 2017. Chia sẻ trọng số mềm để nén mạng neural. arXiv preprint arXiv:1702.04008.

Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, và Ming Zhou. 2020. Minilm: Chưng cất self-attention sâu để nén transformer được đào tạo trước bất khả tri nhiệm vụ. Tiến bộ trong Xử lý Thông tin Neural, 33:5776–5788.

Ziheng Wang, Jeremy Wohlwend, và Tao Lei. 2019. Cắt tỉa có cấu trúc của các mô hình ngôn ngữ lớn. arXiv preprint arXiv:1910.04732.

Johannes Welbl, Nelson F Liu, và Matt Gardner. 2017. Crowdsourcing các câu hỏi khoa học trắc nghiệm. arXiv preprint arXiv:1707.06209.

BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022. Bloom: Một mô hình ngôn ngữ đa ngôn ngữ truy cập mở 176b-tham số. arXiv preprint arXiv:2211.05100.

Mengzhou Xia, Zexuan Zhong, và Danqi Chen. 2022. Cắt tỉa có cấu trúc học các mô hình nhỏ gọn và chính xác. arXiv preprint arXiv:2204.00408.

Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, và Song Han. 2023. Smoothquant: Lượng tử hóa sau đào tạo chính xác và hiệu quả cho các mô hình ngôn ngữ lớn. Trong Hội nghị Quốc tế về Học máy, trang 38087–38099. PMLR.

Yifei Yang, Zouying Cao, và Hai Zhao. 2024. Laco: Cắt tỉa mô hình ngôn ngữ lớn thông qua sụp đổ lớp. arXiv preprint arXiv:2402.11187.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. 2019. Hellaswag: Một máy có thể thực sự hoàn thành câu của bạn không? Trong Kỷ yếu Cuộc họp Thường niên lần thứ 57 của Association for Computational Linguistics.

Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, và Wei Lu. 2024. Tinyllama: Một mô hình ngôn ngữ nhỏ mã nguồn mở. arXiv preprint arXiv:2401.02385.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Các mô hình ngôn ngữ transformer được đào tạo trước mở. arXiv preprint arXiv:2205.01068.

Xunyu Zhu, Jian Li, Yong Liu, Can Ma, và Weiping Wang. 2023. Một khảo sát về nén mô hình cho các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2308.07633.

Zhenxun Zhuang, Mingrui Liu, Ashok Cutkosky, và Francesco Orabona. 2022. Hiểu adamw thông qua các phương pháp proximal và tính không tỷ lệ. Transactions on Machine Learning Research.
