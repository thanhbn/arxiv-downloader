# 2301.10835.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2301.10835.pdf
# Kích thước tệp: 1056283 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Khi Các Tầng Chơi Xổ Số, Tất Cả Vé Đều Thắng Từ Khởi Tạo
Artur Jordao1, George Corrêa de Araújo2, Helena de Almeida Maia2 và Helio Pedrini2
1Escola Politécnica, Universidade de São Paulo
2Institute of Computing, University of Campinas
Tóm tắt
Cắt tỉa là một kỹ thuật tiêu chuẩn để giảm chi phí tính toán của các mạng sâu. Nhiều tiến bộ trong cắt tỉa tận dụng các khái niệm từ Giả thuyết Vé Số (LTH). LTH tiết lộ rằng bên trong một mạng dày đặc đã được huấn luyện tồn tại các mạng con thưa thớt (vé) có thể đạt được độ chính xác tương tự (tức là thắng xổ số – vé thắng). Cắt tỉa tại khởi tạo tập trung vào việc tìm vé thắng mà không cần huấn luyện một mạng dày đặc. Các nghiên cứu về những khái niệm này có chung xu hướng là các mạng con đến từ cắt tỉa trọng số hoặc bộ lọc. Trong công trình này, chúng tôi nghiên cứu LTH và cắt tỉa tại khởi tạo từ góc độ cắt tỉa tầng. Đầu tiên, chúng tôi xác nhận sự tồn tại của vé thắng khi quá trình cắt tỉa loại bỏ các tầng. Dựa trên quan sát này, chúng tôi đề xuất khám phá những vé thắng này tại khởi tạo, loại bỏ yêu cầu về tài nguyên tính toán nặng để huấn luyện mạng dày đặc ban đầu (có quá nhiều tham số). Các thí nghiệm mở rộng cho thấy vé thắng của chúng tôi tăng tốc đáng kể giai đoạn huấn luyện và giảm tới 51% lượng khí thải carbon, một bước quan trọng hướng tới dân chủ hóa và Trí tuệ Nhân tạo xanh. Ngoài lợi ích tính toán, vé thắng của chúng tôi thể hiện tính mạnh mẽ chống lại các ví dụ đối kháng và ngoài phân phối. Cuối cùng, chúng tôi chỉ ra rằng các mạng con của chúng tôi dễ dàng thắng xổ số tại khởi tạo trong khi vé từ loại bỏ bộ lọc (LTH có cấu trúc tiêu chuẩn) khó trở thành vé thắng.

1 Giới thiệu
Giả thuyết Vé Số (LTH) giả định rằng các mạng dày đặc (đã được huấn luyện trước) chứa các mạng con thưa thớt có khả năng đạt được cùng độ chính xác khi được huấn luyện từ khởi tạo ban đầu của đối tác dày đặc của chúng [Frankle và Carbin, 2019]. Các mạng con (vé) thỏa mãn tính chất này được gọi là vé thắng. Nhiều tiến bộ xuất hiện từ LTH, ví dụ, chúng ta có thể giảm chi phí tính toán của việc học một mạng dày đặc bằng cách thay thế nó bằng một mạng con thưa thớt trong [You et al., 2020; Chen et al., 2022] hoặc trước quá trình huấn luyện [Lee et al., 2019; Wang et al., 2020;

LTH Có Cấu Trúc Của Chúng Tôi
(Cắt Tỉa Tầng) | LTH Có Cấu Trúc
(Cắt Tỉa Neuron) | LTH Không Cấu Trúc Gốc
(Cắt Tỉa Trọng Số)

Tăng Tốc
Huấn Luyện: 1.98× | 2.03×
Khí Thải
Carbon: 48.64% | 51.11%
Tính Mạnh Mẽ
Đối Kháng: 0.71pp | 3.73pp

Hình 1: Các quan điểm về Giả thuyết Vé Số (LTH) theo cấu trúc (trọng số, neuron/bộ lọc hoặc tầng) mà quá trình cắt tỉa đang loại bỏ (vùng trong suốt). Trên-trái. LTH không cấu trúc gốc: cắt tỉa loại bỏ trọng số và tạo ra vé không cấu trúc; do đó, vé chỉ cung cấp lợi ích thực tế trên các framework chuyên dụng cho tính toán thưa thớt. Trên-phải. LTH có cấu trúc: cắt tỉa loại bỏ neuron/bộ lọc. Trong thiết lập này, vé có cấu trúc và thúc đẩy lợi thế tính toán cho các framework học sâu tiêu chuẩn. Dưới-trái: LTH có cấu trúc của chúng tôi: cắt tỉa loại bỏ toàn bộ tầng, khuyến khích lợi ích hiệu suất bổ sung vì nó giảm xử lý tuần tự (độ trễ). Dưới-phải. Lợi ích cao nhất (càng cao càng tốt) thu được bởi một vé thắng so với đối tác dày đặc của nó. Vé thắng của chúng tôi thành công xuất hiện tại khởi tạo, có nghĩa là chúng ta có thể khám phá các mạng con hiệu quả mà không cần huấn luyện một mạng dày đặc. Theo hướng này, chúng ta có thể tăng tốc đáng kể giai đoạn học bằng cách thay thế một mạng dày đặc bằng phiên bản thưa thớt của nó trước khi huấn luyện bắt đầu. Vé thắng của chúng tôi cũng thể hiện tính mạnh mẽ chống lại các cuộc tấn công đối kháng.

Tanaka et al., 2020]. Trong bối cảnh tấn công đối kháng, chúng ta có thể tăng tính mạnh mẽ của các mạng dày đặc bằng cách xem xét các phiên bản thưa thớt của chúng [Diffenderfer et al., 2021; Liu et al., 2022; T et al., 2022].

LTH truyền thống tìm kiếm vé thắng bằng cách loại bỏ (tức là cắt tỉa) trọng số của một mạng dày đặc và gán trọng số còn sót lại về khởi tạo ban đầu của chúng [Frankle và Carbin, 2019] – hoàn trả trọng số. Các biến thể của cơ chế này đề xuất nới lỏng ràng buộc hoàn trả trọng số và cho phép các mạng con kế thừa trọng số từ các epoch huấn luyện khác nhau [Frankle et al., 2020; Renda et al., 2020] và thậm chí

--- TRANG 2 ---
từ khởi tạo ngẫu nhiên (tái) [Liu et al., 2019]. Một biến thể khác trong LTH truyền thống là việc thay thế cắt tỉa trọng số (không cấu trúc) bằng cắt tỉa bộ lọc (có cấu trúc) [Liu et al., 2019; Renda et al., 2020; You et al., 2020]. Ví dụ, Prasanna et al. [2020] nghiên cứu LHN trên cắt tỉa có cấu trúc bằng cách loại bỏ các module tự chú ý (tức là các đầu từ các tầng chú ý đa đầu [Vaswani et al., 2017]).

Một nhược điểm thực tế của cắt tỉa không cấu trúc là các framework học sâu hiện có (ví dụ: TensorFlow và Pytorch) không hỗ trợ tính toán tensor thưa thớt. Do đó, để đạt được lợi ích hiệu suất, họ cắt tỉa này yêu cầu các framework hoặc phần cứng chuyên dụng (GPU Nvidia A100) để tối ưu hóa tính toán thưa thớt [Han et al., 2016; Niu et al., 2020; Zhou et al., 2021].

Bất kể cấu trúc, chúng ta có thể phân loại LTH theo giai đoạn mà thuật toán cắt tỉa chọn các mạng con (vé): sau, trong, hoặc trước huấn luyện. Lớp đầu tiên huấn luyện một mạng dày đặc và sau đó trích xuất một mạng con theo tiêu chí quan trọng (tiêu chí cắt tỉa), các bước như vậy tạo thành LTH gốc [Frankle và Carbin, 2019]. Lớp thứ hai cắt tỉa một mạng dày đặc trong quá trình huấn luyện, thu được một mạng con khi giai đoạn học kết thúc [You et al., 2020; Chen et al., 2022]. Cuối cùng, cắt tỉa trước huấn luyện tạo thành một hình thức cắt tỉa tương đối gần đây có tên là cắt tỉa tại khởi tạo [Lee et al., 2019; Tanaka et al., 2020; Wang et al., 2020]. Ý tưởng đằng sau danh mục này là tìm các mạng con trước khi huấn luyện, có nghĩa là chọn các mạng con sử dụng các tham số được khởi tạo ngẫu nhiên (không có bất kỳ cập nhật nào) để hướng dẫn thuật toán cắt tỉa. Nhiều nỗ lực đã được đầu tư vào cắt tỉa tại khởi tạo vì nó cung cấp tất cả lợi ích của các mạng con thưa thớt mà không tiêu tốn tài nguyên tính toán để huấn luyện một mạng dày đặc (một quá trình yêu cầu bởi các danh mục khác).

Quan trọng là, tất cả các danh mục cắt tỉa trong LTH chia sẻ cùng một đặc điểm: cắt tỉa loại bỏ các cấu trúc nhỏ như trọng số hoặc bộ lọc. Ngoài bối cảnh LTH, các công trình trước đây đã chứng minh lợi ích đáng kể của việc loại bỏ toàn bộ tầng thay vì các cấu trúc nhỏ. Cụ thể, loại cắt tỉa có cấu trúc này thúc đẩy lợi ích hiệu suất bổ sung vì nó giảm xử lý tuần tự (độ trễ) của một mạng [Li et al., 2021; Zhou et al., 2022]. Do đó, nhiều nghiên cứu đã tập trung vào loại bỏ tầng thay vì các cấu trúc khác [Veit và Belongie, 2020; Fan et al., 2020; Zhou et al., 2022; Zhang và Liu, 2022]. Thật không may, không có nỗ lực nào trong số này được thực hiện theo hướng LTH và cắt tỉa tại khởi tạo. Để thu hẹp khoảng cách này, chúng tôi thực hiện một bước hướng tới hiểu biết về hành vi của LTH khi quá trình cắt tỉa loại bỏ các tầng. Trong thiết lập này, đầu tiên chúng tôi xác minh sự tồn tại của vé thắng. Sau đó, chúng tôi đề xuất một chiến lược hệ thống để khám phá vé thắng trước huấn luyện, có nghĩa là tìm các mạng con thưa thớt (từ loại bỏ tầng) tại khởi tạo phù hợp với khả năng dự đoán của tương đương dày đặc của chúng.

Đóng góp. Chúng tôi liệt kê các đóng góp chính sau đây. Đầu tiên, chúng tôi chứng minh sự tồn tại của vé thắng – các mạng con thưa thớt đạt được cùng độ chính xác như tương đương dày đặc của chúng – khi quá trình cắt tỉa tính đến việc loại bỏ tầng. Từ góc độ thực tế, đóng góp như vậy cho phép vé thắng hiệu quả hơn về tiêu thụ bộ nhớ, thời gian suy luận (độ trễ) và khí thải carbon, vì loại bỏ tầng cung cấp lợi ích tính toán vượt trội hơn các hình thức cắt tỉa tiêu chuẩn được sử dụng trong LTH (xem Hình 1). Thứ hai, chúng tôi thành công tìm vé thắng từ loại bỏ tầng tại khởi tạo mà không cần bất kỳ huấn luyện nào. Đóng góp này đóng vai trò trong huấn luyện chi phí thấp và tiết kiệm năng lượng, vì chúng ta có thể thay thế việc học một mạng dày đặc bằng phiên bản thưa thớt của nó. Trái ngược với các nghiên cứu LTH trước đây [Frankle et al., 2020; Renda et al., 2020], đóng góp này loại bỏ yêu cầu về tài nguyên tính toán nặng để học mạng dày đặc ban đầu (có quá nhiều tham số). Cuối cùng, chúng tôi chỉ ra rằng họ mạng con mới này chia sẻ một số tính chất của LTH có cấu trúc tiêu chuẩn (cắt tỉa bộ lọc), ví dụ, chúng xuất hiện sớm trong huấn luyện. Mặt khác, chúng tôi quan sát thấy rằng các mạng con từ cắt tỉa bộ lọc thể hiện hiệu suất kém khi áp dụng cho các mạng nông (ít tham số) và trước huấn luyện, điều này phù hợp với bằng chứng trước đây [Liu et al., 2022]. Nói cách khác, vé từ cắt tỉa bộ lọc khó trở thành vé thắng. Tuy nhiên, vé của chúng tôi (loại bỏ tầng) dễ dàng thắng xổ số, đặt ra câu hỏi liệu cấu trúc bị loại bỏ có đóng vai trò trong LTH và cắt tỉa tại khởi tạo hay không.

Theo phân tích của chúng tôi, vé thắng từ cắt tỉa tầng chính xác và mạnh mẽ đối với nhiều thiết lập như tiêu chí cắt tỉa và mật độ, và hoàn trả trọng số. Khi được tìm thấy tại khởi tạo, những vé thắng này tăng tốc thời gian huấn luyện lên tới 2× và tiết kiệm đáng kể FLOPs và tiêu thụ bộ nhớ. Trên một tiêu chí cắt tỉa cụ thể, tất cả vé vượt trội khả năng dự đoán của đối tác dày đặc của chúng – tất cả vé trở thành vé thắng. Ngoài ra, vé thắng của chúng tôi giảm tới 51% khí thải carbon trong giai đoạn huấn luyện, một bước quan trọng hướng tới dân chủ hóa và tính bền vững của Trí tuệ Nhân tạo (AI) – AI xanh. Như được đề xuất bởi các công trình trước đây [Liu et al., 2022; T et al., 2022; Chen et al., 2022], chúng tôi cũng đánh giá vé thắng của chúng tôi trên các hình ảnh đối kháng và các ví dụ ngoài phân phối sử dụng CIFAR-C [Hendrycks và Dietterich, 2019] và CIFAR-10.2 [Lu et al., 2020], tương ứng. Trên huấn luyện sạch tiêu chuẩn (tức là không có bất kỳ cơ chế phòng thủ nào), vé từ cắt tỉa tầng đạt được tính mạnh mẽ cao hơn tương đương dày đặc của chúng; do đó, xác nhận tính phù hợp của chúng cho các ứng dụng quan trọng về an toàn. Vì mục đích tái tạo, mã của chúng tôi được công khai: https://github.com/arturjordao/LayerLottery.

2 Công trình liên quan
Frankle et al. [2019] giới thiệu Giả thuyết Vé Số và quan sát thấy rằng các mạng con từ một mạng dày đặc có thể đạt được độ chính xác tương tự kể từ khi được huấn luyện từ cùng khởi tạo ban đầu. Sau đó, Frankle et al. [2020] và Renda et al. [2020] xác nhận rằng hoàn trả trọng số về các epoch khác của huấn luyện, thay vì khởi tạo ban đầu, cho phép các mạng con phù hợp với độ chính xác của đối tác dày đặc của chúng.

Công trình của Achille et al. [2019] nghiên cứu tính nhạy cảm của mạng trong quá trình huấn luyện. Các tác giả quan sát thấy rằng các nhiễu động, như làm mờ và nhãn ngẫu nhiên, ảnh hưởng ít hơn đến động lực huấn luyện khi được áp dụng trong các epoch sớm. Bằng cách xem xét cắt tỉa như một hình thức nhiễu động (ví dụ, do loại bỏ cấu trúc), bằng chứng như vậy củng cố hiệu suất vượt trội của các mạng con được hoàn trả về các epoch huấn luyện sớm, như được lập luận bởi các nghiên cứu tiếp theo [You et al., 2020]. Trong khi hoàn trả trọng số nằm ở trung tâm của LTH, Liu et al. [2019] chứng minh rằng vé thắng tồn tại ngay cả trong chế độ khởi tạo ngẫu nhiên. Trong suốt nghiên cứu này, chúng tôi chỉ ra rằng vé thắng của chúng tôi (kết quả từ cắt tỉa tầng) xuất hiện từ các epoch hoàn trả khác nhau, khởi tạo ngẫu nhiên và trước huấn luyện, với cái sau đạt được lợi ích tốt nhất về độ chính xác và hiệu suất tính toán.

Liu et al. [2021] đề xuất rằng sự tồn tại của vé thắng tương quan với quá trình chuyển đổi từ tham số ban đầu và cuối cùng của mạng dày đặc. Từ góc độ khác, Paul et al. [2022] chỉ ra rằng số lượng và chất lượng dữ liệu đóng vai trò trong LTH. Cụ thể hơn, các tác giả quan sát thấy rằng huấn luyện trên dữ liệu dễ hoặc trên một phần nhỏ dữ liệu được chọn ngẫu nhiên thúc đẩy khởi tạo tốt cho các mạng dày đặc sao cho các mạng con của chúng trở thành vé thắng. Phù hợp với Liu et al. [2021], You et al. [2020] đề xuất rút ra các mạng con thưa thớt sớm trong huấn luyện bằng cách xác định khi độ lớn của trọng số trở nên ổn định (tức là chịu ít thay đổi) trong quá trình huấn luyện. Sau khi định vị một mạng con, được đặt tên là chim sớm, các tác giả thay thế việc huấn luyện mạng dày đặc bằng mạng con; do đó dẫn đến huấn luyện nhanh hơn và chi phí thấp hơn. Dựa trên ý tưởng này, Chen et al. [2022] đề xuất tìm chim sớm để giảm chi phí huấn luyện đối kháng. Tuy nhiên, các tác giả xác định mạng con bằng cách cắt tỉa trọng số (không cấu trúc) trong khi You et al. [2020] tập trung vào loại bỏ bộ lọc (cắt tỉa có cấu trúc). Thú vị là, kết quả của Chen et al. [2022] củng cố quan sát của các công trình trước đây [Diffenderfer et al., 2021; Liu et al., 2022], nêu rằng cắt tỉa không chỉ giảm chi phí tính toán mà còn thúc đẩy tính mạnh mẽ chống lại hình ảnh đối kháng. Ngoài ra, T et al. [2022] xác nhận rằng vé thắng tổng quát hóa tốt hơn các mạng dày đặc trong các chế độ dữ liệu hạn chế và các tình huống ngoài phân phối.

Ngoài lợi ích tính toán, You et al [2020] và Chen et al. [2022] chỉ ra rằng vé thắng được rút ra sớm trong huấn luyện đạt được độ chính xác cao hơn. Từ góc độ hoàn trả LTH, điều này có nghĩa là đặt lại trọng số của các mạng con về các epoch sớm của mạng dày đặc cho phép các mạng con đạt được khả năng dự đoán cao hơn, do đó, chúng có nhiều khả năng trở thành vé thắng hơn. Kết quả của chúng tôi phù hợp với những phát hiện này: các mạng con từ cắt tỉa tầng đạt được độ chính xác vượt trội khi được hoàn trả về các epoch sớm. Trái ngược với những công trình này, chúng tôi cũng đề xuất xác định vé thắng tại khởi tạo (trước huấn luyện). Nói một cách không chính thức, các mạng con của chúng tôi là sớm nhất có thể.

Liên quan chặt chẽ đến LTH, một số nỗ lực đã được đầu tư vào cắt tỉa tại khởi tạo. Trong chiến lược này, thuật toán cắt tỉa loại bỏ các cấu trúc không quan trọng trước bất kỳ huấn luyện nào [Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020]. Nghiên cứu của Frankle et al. [2021] chỉ ra những khó khăn và tính chất vốn có của nhiều chiến lược cắt tỉa tại khởi tạo. Các tác giả chỉ ra rằng họ chiến lược này kém hiệu quả hơn cắt tỉa tiêu chuẩn (tức là LTH) về cả độ chính xác và độ thưa thớt. Jorge et al. [2021] củng cố những phát hiện như vậy; hơn nữa, họ quan sát thấy rằng cắt tỉa tại khởi tạo không tốt hơn cắt tỉa ngẫu nhiên trong các chế độ độ thưa thớt cao. Liu et al. [2022] đề xuất một hành vi ngược lại: cắt tỉa tại khởi tạo hoạt động tốt trên các mạng sâu và rộng, thậm chí chọn ngẫu nhiên các cấu trúc không quan trọng. Ngoài ra, các tác giả chỉ ra rằng sử dụng mật độ cắt tỉa theo tầng tạo ra vé thắng chính xác hơn. Chúng tôi quan sát thấy rằng bằng chứng của Liu et al. [2022] đúng khi cắt tỉa tính đến bộ lọc; tuy nhiên, chúng tôi thành công tìm vé thắng tại khởi tạo trên cả kiến trúc nông (tức là ResNet32) và sâu (tức là ResNet56), điều này gợi ý một hình thức khám phá vé thắng không phụ thuộc kiến trúc trước huấn luyện. Mặc dù có vẻ như một hiện tượng phản trực giác, có một nhóm nghiên cứu xác nhận lợi ích của việc loại bỏ tầng thay vì bộ lọc [Zhang và Liu, 2022; Zhou et al., 2022]. Tóm lại, công trình của chúng tôi khác với các nghiên cứu trước đây về LTH và cắt tỉa tại khởi tạo về cấu trúc mà chúng tôi tập trung loại bỏ – tầng.

Theo hiểu biết tốt nhất của chúng tôi, ý tưởng đằng sau cắt tỉa tầng có từ năm 2016, khi Veit et al. [2016] và Huang et al. [2016] chứng minh rằng các kiến trúc dựa trên residual không thể hiện suy giảm khi chúng ta loại bỏ một số tầng của chúng. Sau đó, Dong et al. [2021] xác nhận rằng các kiến trúc tự chú ý cũng chia sẻ hành vi tương tự. Kể từ những nghiên cứu tiên phong này, nhiều công trình đã đề xuất loại bỏ tầng tĩnh hoặc động. Điều quan trọng là phải đề cập đến sự khác biệt giữa các danh mục cắt tỉa tầng này. Trong trường hợp trước, thuật toán cắt tỉa loại bỏ tầng theo cách tương tự như cắt tỉa trọng số và bộ lọc, tạo ra một mạng nông vĩnh viễn [Zhang và Liu, 2022; Zhou et al., 2022]. Khác biệt, lý luận đằng sau cắt tỉa tầng động bao gồm việc bỏ qua (tức là vô hiệu hóa) một số tầng theo mẫu đầu vào mà mạng nhận được [Han et al., 2022]. Công trình của chúng tôi thuộc về danh mục đầu tiên của cắt tỉa tầng. Cụ thể, do bản chất động của nó, chúng tôi tin rằng việc nghiên cứu LTH và cắt tỉa tại khởi tạo trong nhóm thứ hai là không thực tế.

3 Kiến thức cơ bản và Phát biểu vấn đề
Định nghĩa. Cho F và F' là một mạng dày đặc và thưa thớt (mạng con), tương ứng. Cái sau là một phiên bản của F mà không có một số cấu trúc (neuron, bộ lọc hoặc tầng), có nghĩa là F sau một quá trình cắt tỉa nào đó. Giả sử θi là các tham số từ F, trong đó chỉ số dưới i chỉ ra trọng số tại epoch huấn luyện i. Chúng tôi biểu thị trọng số được khởi tạo ngẫu nhiên và những trọng số sau giai đoạn huấn luyện là θ0 và θn, theo thứ tự này.

Thuật toán Cắt tỉa. Một thuật toán cắt tỉa xác định và loại bỏ các cấu trúc không quan trọng tạo thành một mạng. Vì mục đích này, nó đo tầm quan trọng của mỗi cấu trúc theo tiêu chí quan trọng c. Cho S là một tập hợp (được sắp xếp) điểm số chỉ ra tầm quan trọng của mỗi cấu trúc của F. Cho S, thuật toán cắt tỉa loại bỏ các cấu trúc ít quan trọng nhất để thỏa mãn mật độ cắt tỉa p (tức là tỷ lệ phần trăm hoặc số lượng cấu trúc được loại bỏ). Trong tài liệu về cắt tỉa, thông thường là che giấu các cấu trúc bị loại bỏ bằng giá trị không. Ngược lại, chúng tôi thực sự loại bỏ các cấu trúc (tầng) để đạt được lợi ích hiệu suất thực tế mà không cần các framework hoặc phần cứng chuyên dụng cho tính toán thưa thớt. Chúng tôi giới thiệu độc giả quan tâm đến Phụ lục A.1 để biết thêm chi tiết về quá trình kỹ thuật này.

Giả thuyết Vé Số. Theo các định nghĩa trước đây, Giả thuyết Vé Số (LTH) gốc [Frankle

--- TRANG 4 ---
Thuật toán 1 LTH Loại bỏ Tầng của Mạng Sâu
Đầu vào: Mạng Tích chập F được huấn luyện trong n epoch
Đầu vào: Hoàn trả Trọng số θi
Đầu vào: Tiêu chí Cắt tỉa c
Đầu vào: Mật độ Cắt tỉa p
Đầu ra: Mạng con (Vé) F'
S ← c(F, θn) ▷ Gán tầm quan trọng cho mỗi tầng
I ← p tầng ít quan trọng nhất dựa trên S
F' ← F \ I ▷ Loại bỏ các tầng được lập chỉ mục bởi I
Đặt trọng số của F' là θi ▷ Hoàn trả trọng số
Huấn luyện F' qua huấn luyện tiêu chuẩn trong n-i epoch

và Carbin, 2019] nêu rằng bên trong F tồn tại các mạng con thưa thớt F' có thể đạt được độ chính xác tương tự (hoặc, lý tưởng nhất, vượt trội) kể từ khi được huấn luyện từ khởi tạo giống hệt θ0. Trong định nghĩa này, các mạng con được đặt tên là vé và những vé thỏa mãn tính chất như vậy được đặt tên là vé thắng. Nói cách khác, một vé thắng là một mạng con có cùng khả năng dự đoán (tức là độ chính xác) như tương đương dày đặc của nó. Trong thực tế, chúng ta có thể định nghĩa sự tồn tại của vé thắng theo:

Độ chính xác(F') + ξ ≥ Độ chính xác(F), ξ ≥ 0. (1)

Như chúng tôi đã đề cập trước đây, các biến thể của LTH cho phép các mạng con F' kế thừa trọng số từ các epoch huấn luyện khác nhau (tức là θi với i > 0) [Frankle et al., 2020; Frankle et al., 2021]. Bước này tạo thành quá trình hoàn trả trọng số. Thuật toán 1 tóm tắt các bước của LTH (cắt tỉa tầng) của chúng tôi.

Từ mô tả trên, chúng tôi nêu bật hai quan sát. (i) Trong Phương trình 1, ξ cho phép kiểm soát (nới lỏng) mức độ chính xác của một vé có thể khác với mạng dày đặc của nó và vẫn được coi là vé thắng, trong đó các giá trị thông thường là một điểm phần trăm hoặc một độ lệch chuẩn [Chen et al., 2020; Frankle et al., 2021]. (ii) Trước khi trích xuất một vé thắng tiềm năng, chúng ta cần huấn luyện F đến hoàn thành (tức là huấn luyện trong n epoch để thu được θn) – xem đầu vào đầu tiên trong Thuật toán 1.

Cắt tỉa tại Khởi tạo. Danh mục cắt tỉa tương đối gần đây này ước tính cấu trúc nào cần loại bỏ trước huấn luyện. Về mặt kỹ thuật, nó tập trung vào khám phá các mạng con sử dụng khởi tạo ngẫu nhiên (θ0) của F để hướng dẫn thuật toán cắt tỉa. Từ góc độ LTH, cắt tỉa tại khởi tạo nhằm tạo ra vé thắng mà không huấn luyện F.

Thuật toán 2 tóm tắt các bước của cắt tỉa tại khởi tạo của chúng tôi. Trong Thuật toán 2, điều quan trọng cần quan sát là nó nhận một mạng chưa được huấn luyện, trong khi trong LTH tiêu chuẩn của chúng tôi (Thuật toán 1) đầu vào là một mạng đã được huấn luyện.

Câu hỏi Nghiên cứu. Từ các định nghĩa trên, câu hỏi nghiên cứu của chúng tôi như sau. (i) Chúng tôi hỏi liệu có vé thắng khi F' xuất phát từ quá trình cắt tỉa tầng hay không. Nói cách khác, chúng tôi nghiên cứu liệu Phương trình 1 có đúng khi cắt tỉa loại bỏ tầng hay không. Chúng tôi xác nhận rằng câu trả lời là tích cực; do đó, đặt ra câu hỏi thứ hai: (ii) Có thể khám phá những vé thắng như vậy (từ cắt tỉa tầng) tại khởi tạo không? Chính thức, có thể trích xuất một F' thỏa mãn Phương trình 1 mà không huấn luyện F không? Trả lời điều này cho phép chúng ta tránh việc huấn luyện đắt đỏ về mặt tính toán của một mạng dày đặc bằng cách thay thế nó trực tiếp bằng phiên bản thưa thớt của nó trước khi huấn luyện bắt đầu. Nhìn chung, cả hai câu hỏi đều tập trung vào phân tích Phương trình 1 sau khi thực hiện Thuật toán 1 và Thuật toán 2.

Thuật toán 2 Vé Thắng tại Khởi tạo
Đầu vào: Mạng Tích chập F chưa được huấn luyện
Đầu vào: Tiêu chí Cắt tỉa c ▷ SNIP hoặc GraSP
Đầu vào: Mật độ Cắt tỉa p
Đầu ra: Mạng con (Vé) F'
S ← c(F, θ0) ▷ Gán tầm quan trọng cho mỗi tầng
I ← p tầng ít quan trọng nhất dựa trên S
F' ← F \ I ▷ Loại bỏ các tầng được lập chỉ mục bởi I
Huấn luyện F' qua huấn luyện tiêu chuẩn trong n epoch

4 Thí nghiệm
Thiết lập Thí nghiệm. Chúng tôi tiến hành thí nghiệm trên CIFAR-10 [Krizhevsky et al., 2009] sử dụng mạng ResNet32 và ResNet56 [He et al., 2016]. Các thiết lập như vậy là lựa chọn phổ biến cho đánh giá LTH và cắt tỉa tổng quát [Blalock et al., 2020]. Trong LTH của chúng tôi, chúng tôi sử dụng lược đồ hoàn trả tốc độ học được đề xuất bởi Renda et al. [2020], vì nó dẫn đến kết quả tốt hơn so với lược đồ được đề xuất trong LTH gốc [Frankle và Carbin, 2019]. Trái ngược với hầu hết các nghiên cứu LTH, chúng tôi đánh giá LTH trên nhiều tiêu chí thay vì chỉ sử dụng tiêu chí chuẩn ℓ1 (cắt tỉa độ lớn lặp lại — IMP – được đề xuất ban đầu bởi Frankle et al. [2019]). Lý do cho lựa chọn này là để đánh giá sự tồn tại của vé thắng trên nhiều thiết lập cắt tỉa. Ngoài ra, các tầng ở độ sâu khác nhau thể hiện độ lớn khác nhau, do đó, không thể so sánh chuẩn ℓ1 từ nhiều tầng (xem Phụ lục A.2). Cụ thể, chúng tôi sử dụng các tiêu chí cắt tỉa được đề xuất bởi Lin et al. [2020] (HRank), Tan và Motani [2020] (expectABS), Jordao et al. [2020] (PLS) và Luo và Wu [2020] (KL). Ngoài ra, chúng tôi sử dụng tiêu chí ngẫu nhiên thường được sử dụng.

Chúng tôi báo cáo (bằng điểm phần trăm – pp) sự cải thiện và suy giảm của các mạng con (vé) so với đối tác dày đặc của chúng bằng cách sử dụng các ký hiệu (+) và (-), theo thứ tự này. Do đó, ký hiệu (+) đại diện cho vé thắng vì những mạng này phù hợp hoặc vượt trội độ chính xác của tương đương dày đặc của chúng. Vì quá trình cắt tỉa của chúng tôi xem xét loại bỏ tầng, nó không thể được đánh giá trên các kiến trúc giống VGG (mạng đơn giản và không residual) do các chi tiết kỹ thuật và lý thuyết. Chúng tôi giới thiệu độc giả quan tâm đến Phụ lục A.1 để biết thêm thông tin.

Trừ khi có ghi chú khác, thuật ngữ mạng con chỉ các mạng được tạo ra bởi quá trình loại bỏ tầng, đây là phạm vi của công trình chúng tôi. Trong thiết lập này, mật độ cắt tỉa p đại diện cho số lượng tầng được loại bỏ, ví dụ, p = 1 chỉ ra rằng cắt tỉa đã loại bỏ 1 khối residual. (Trong ResNet32-56 cũng như các biến thể nông hơn và sâu hơn của nó, một khối residual (tầng) tương ứng với hai tầng tích chập.)

Sự tồn tại của Vé Thắng và Hoàn trả Trọng số. Thí nghiệm đầu tiên của chúng tôi xác minh liệu các mạng con có trở thành vé thắng khi cắt tỉa loại bỏ tầng hay không. Một trong những sự thật quan trọng nhất để một mạng con trở thành vé thắng là epoch mà chúng ta hoàn trả trọng số của nó (hoàn trả trọng số), có nghĩa là θi mà mạng con kế thừa từ phiên bản dày đặc của nó.

Các nghiên cứu trước đây về LTH xác nhận rằng kế thừa trọng số từ các epoch sớm cho phép các mạng con thành công trở thành vé thắng [You et al., 2020; Frankle et al., 2021]. Từ

--- TRANG 5 ---
Bảng 1: Khả năng dự đoán của các vé khi hoàn trả trọng số của chúng về epoch huấn luyện thứ i (θi) của mạng dày đặc (ResNet32). Với mỗi tiêu chí, chúng tôi tô đậm và gạch chân epoch hoàn trả dẫn đến kết quả tốt nhất top-1 và top-2, tương ứng.

θ0 θ25 θ50 θ75
HRank (-) 0.01 (+) 0.21 (+) 0.10 (+) 0.12
expectABS (+) 0.33 (+) 0.13 (-) 0.48 (-) 0.62
PLS (+) 0.06 (+) 0.31 (+) 0.10 (+) 0.10
Random (+) 0.38 (+) 0.15 (+) 0.26 (-) 0.07
KL (+) 0.74 (+) 0.46 (-) 0.03 (-) 0.17

góc độ thực tế, đặt lại trọng số về những epoch này yêu cầu nhiều lần lặp huấn luyện hơn để hoàn thành (xem bước cuối cùng trong Thuật toán 1). Trong thí nghiệm này, chúng tôi đánh giá hiệu ứng của việc đặt trọng số của các mạng con từ cắt tỉa tầng về các epoch khác nhau. Nói cách khác, chúng tôi nghiên cứu hành vi của các mạng con khi chúng kế thừa các θi khác nhau.

Bảng 1 tóm tắt kết quả. Từ bảng này, chúng tôi nêu bật các quan sát sau. Đầu tiên, hoàn trả các mạng con về cùng khởi tạo (ngẫu nhiên) của mạng dày đặc, θ0, cho phép tất cả mạng con trở thành vé thắng (trừ HRank có sự sụt giảm không đáng kể). Điều này có nghĩa là, hoàn trả về θ0, tất cả mạng con thỏa mãn Phương trình 1. Thứ hai, hầu hết các mạng con đạt độ chính xác thấp nhất khi hoàn trả sau 50 epoch. Điều này phù hợp với các phát hiện của Achille et al. [2019] và You et al. [2020]: hoàn trả về các epoch muộn, các mạng con có độ chính xác thấp hơn, vì chúng có thời gian ngắn (epoch) để phục hồi từ thiệt hại trong cấu trúc của chúng. Nhìn chung, hoàn trả trọng số của LTH khi cắt tỉa loại bỏ bộ lọc và tầng chia sẻ hành vi tương tự.

Thảo luận trước đây tính đến thiết lập gốc của Frankle et al. [2019] và các biến thể của nó [Frankle et al., 2020; Renda et al., 2020]. Liu et al. [2019] đề xuất một giải pháp thay thế để khôi phục trọng số cho một số khởi tạo tạo thành quỹ đạo huấn luyện (θi) của mạng dày đặc. Các tác giả chứng minh rằng khởi tạo lại ngẫu nhiên các mạng con cho phép chúng trở thành vé thắng. Từ góc độ cắt tỉa tầng, chúng tôi quan sát thấy các mạng con chia sẻ cùng xu hướng. Cụ thể hơn, trên hoàn trả của Liu et al. [2019], các mạng con của chúng tôi vượt trội mạng dày đặc lên tới 0.85 điểm phần trăm.

Tổng thể, kết quả trước đây xác nhận sự tồn tại của vé thắng khi quá trình cắt tỉa loại bỏ toàn bộ tầng; do đó trả lời câu hỏi nghiên cứu đầu tiên của chúng tôi.

Chọn Vé Thắng tại Khởi tạo. Cho đến nay, thí nghiệm của chúng tôi xác nhận sự tồn tại của vé thắng với chiến lược hoàn trả trọng số, như trong giả thuyết LTH truyền thống. Ngoài ra, chúng tôi quan sát thấy rằng nới lỏng hoàn trả trọng số và khởi tạo lại các mạng con cho phép chúng trở thành vé thắng. Cả hai thiết lập đều giả định rằng vé thắng xuất hiện từ một mạng dày đặc được huấn luyện tốt. Trong thí nghiệm này, chúng tôi loại bỏ ràng buộc này và đề xuất khám phá vé thắng mà không cần bất kỳ huấn luyện nào. Nói cách khác, chúng tôi đề xuất tìm vé thắng trước khi huấn luyện bắt đầu. Thiết lập như vậy loại bỏ yêu cầu về tài nguyên tính toán nặng để huấn luyện mạng dày đặc ban đầu (có quá nhiều tham số).

Bảng 2: Khả năng dự đoán của vé từ ResNet32 khi LTH của chúng tôi khám phá chúng tại khởi tạo.

Mật độ Cắt tỉa SNIP GraSP
1 (+) 0.08 (+) 0.40
2 (+) 0.00 (+) 0.72
3 (-) 0.50 (+) 0.34
4 (-) 0.53 (+) 0.41
5 (-) 1.00 (+) 0.12

mạng dày đặc, một bước quan trọng hướng tới dân chủ hóa và học máy xanh. Như chúng tôi đã thảo luận trước đây, các công trình sớm đã đề xuất chọn vé thắng tại khởi tạo [Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020; Liu et al., 2022]. Tuy nhiên, những công trình này tập trung vào loại bỏ trọng số trong khi chúng tôi nhắm đến loại bỏ tầng, đây là một hình thức cắt tỉa hiệu quả hơn và không phụ thuộc phần cứng.

Một câu hỏi liên quan để tìm các mạng con từ khởi tạo là việc lựa chọn tiêu chí để ước tính tầm quan trọng cấu trúc. Hóa ra hầu hết các tiêu chí thất bại trong việc đo tầm quan trọng mà không có bất kỳ huấn luyện nào vì trọng số thay đổi mạnh (do đó điểm quan trọng). Tóm lại, các tiêu chí được đánh giá trong Bảng 1 không phù hợp để cắt tỉa tại khởi tạo.

Để đối mặt với vấn đề trên, chúng tôi sử dụng hai tiêu chí được thiết kế đặc biệt để ước tính tầm quan trọng tại khởi tạo. Tiêu chí đầu tiên, được đặt tên là SNIP, tính toán tầm quan trọng bằng cách nhân trọng số với gradient của nó (tại cấu hình ban đầu – ngẫu nhiên); sau đó, nó lấy giá trị tuyệt đối của phép toán kết quả [Lee et al., 2019]. Tiêu chí thứ hai, được gọi là GraSP, là một biến thể của SNIP, trong đó tầm quan trọng xem xét tín hiệu của trọng số [Wang et al., 2020]. Quan trọng là, SNIP và GraSP là các tiêu chí dựa trên dữ liệu vì cả hai đều chuyển tiếp dữ liệu (và nhãn) qua mạng để tính toán gradient trong quá trình ước tính tầm quan trọng.

Bảng 2 tóm tắt kết quả. Từ bảng này, chúng tôi nêu bật các quan sát sau. (i) SNIP là một tiêu chí không hiệu quả để khám phá các mạng con rất thưa thớt (vé) vì vé của nó không còn là vé thắng sau khi loại bỏ ba tầng. (ii) Tất cả vé từ GraSP thắng xổ số. Cụ thể, GraSP cung cấp các mạng con có khả năng dự đoán vượt trội so với mạng dày đặc ngay cả ở mật độ cắt tỉa cao nhất mà chúng tôi xem xét (5 tầng – 66% tầng của ResNet32).

Để xác nhận tính vượt trội của Grasp so với SNIP, chúng tôi so sánh các mạng con được tạo ra bởi mỗi tiêu chí với mạng dày đặc thông qua góc độ tương tự biểu diễn giữa các mô hình. Vì mục đích này, chúng tôi sử dụng phương pháp của Kornblith et al. [2019] (được đặt tên là CKA) đo sự tương tự biểu diễn của hai mô hình (kiến trúc bằng hoặc khác nhau). Ý tưởng là xác minh liệu sự tương tự này có tương quan với kết quả trong Bảng 2 hay không. Ở mức độ cắt tỉa thấp nhất (p = 1), sự khác biệt về tương tự CKA giữa mạng dày đặc và các mạng con từ GraSP và SNIP ít hơn một điểm phần trăm, với mạng con từ GRASP đạt được sự tương tự cao hơn. Tuy nhiên, ở mức độ cắt tỉa cao nhất (p = 5), mạng con từ SNIP thể hiện sự khác biệt gần hai điểm phần trăm. Một mặt, sự tương tự biểu diễn giữa các mạng con được tạo ra bởi SNIP và mạng dày đặc giảm như một hàm của mức độ nghiêm trọng cắt tỉa. Mặt khác, theo sự tương tự CKA, biểu diễn nội bộ của các mạng con từ GraSP vẫn (một phần) không bị hư hại so với mạng dày đặc. Quan sát này củng cố kết quả tích cực của GraSP trong Bảng 2 và chỉ ra rằng chúng ta có thể hiệu quả trích xuất vé thắng tại khởi tạo bằng GraSP. Trừ khi có ghi chú khác, chúng tôi sử dụng GraSP để khám phá vé thắng tại khởi tạo trong các thí nghiệm tiếp theo.

So sánh với LTH (có cấu trúc) tiêu chuẩn. Thí nghiệm này so sánh vé thắng của chúng tôi (cắt tỉa tầng) với LTH có cấu trúc tiêu chuẩn (cắt tỉa bộ lọc). Trong quá trình đánh giá này, chúng tôi trích xuất vé thắng của cắt tỉa tầng và bộ lọc tại khởi tạo. Ở đây, một khía cạnh quan trọng là cách tạo ra các mạng con có cùng độ thưa thớt (về bộ lọc) khi cắt tỉa loại bỏ các cấu trúc khác nhau. Hóa ra khi nó loại bỏ tầng, độ thưa thớt trở nên không linh hoạt. Ví dụ, bằng cách loại bỏ 1 và 2 tầng, số lượng bộ lọc còn lại là 1200 và 1168 và chúng ta không thể có độ thưa thớt bộ lọc giữa các giá trị này (tức là 1190). Vì tính công bằng của so sánh, chúng tôi áp dụng quy trình sau. Đầu tiên, chúng tôi loại bỏ tầng từ một mạng dày đặc và tính toán số lượng bộ lọc trong mạng con thu được (tức là số lượng bộ lọc được giữ lại). Sau đó, để tạo ra các mạng con có thể so sánh chỉ loại bỏ bộ lọc, chúng tôi chạy quá trình cắt tỉa buộc nó loại bỏ số lượng bộ lọc gần nhất của các mạng con từ cắt tỉa tầng. Do đó, các mạng con được tạo ra từ một mạng dày đặc mà không có tầng và bộ lọc gần nhất có thể về độ thưa thớt bộ lọc. Chúng tôi nêu bật rằng quá trình ngược lại – đầu tiên loại bỏ bộ lọc rồi sau đó tầng – làm suy giảm độ thưa thớt có thể so sánh.

Bảng 3 cho thấy kết quả trong chế độ độ thưa thớt thấp nhất (một tầng được loại bỏ – p = 1). Trên cả tiêu chí SNIP và GraSP, vé thắng của chúng tôi tại khởi tạo tạo ra vé thắng sớm với cải thiện tốt hơn. Cụ thể, các mạng con từ LTH có cấu trúc tiêu chuẩn trở thành vé thắng chỉ khi chúng ta nới lỏng Phương trình 1 bằng cách tăng giá trị ξ.

Các quan sát trước đây tiết lộ khó khăn trong việc trích xuất vé thắng tại khởi tạo và xác nhận rằng nó không tốt hơn mô hình huấn luyện và cắt tỉa tiêu chuẩn [Frankle et al., 2021; de Jorge et al., 2021]; do đó củng cố kết quả kém của cắt tỉa bộ lọc trong Bảng 3. Vé thắng của chúng tôi (cắt tỉa tầng) đưa ra một góc nhìn khác cho vấn đề như vậy: bộ lọc có thể không phải là cấu trúc hiệu quả nhất trong LTH có cấu trúc. Về mặt kỹ thuật, loại bỏ một tầng bao gồm

Bảng 3: So sánh giữa vé từ LTH có cấu trúc tiêu chuẩn (cắt tỉa bộ lọc) và LTH của chúng tôi (cắt tỉa tầng). Trên cả hai tiêu chí cho cắt tỉa tại khởi tạo, các mạng con của chúng tôi vượt trội đối tác dày đặc (ResNet32) của chúng. Nói cách khác, vé của chúng tôi trở thành vé thắng (chúng thắng xổ số). Tuy nhiên, vé từ LTH có cấu trúc tiêu chuẩn khó trở thành vé thắng.

SNIP GraSP
Bộ lọc (LTH Tiêu chuẩn) (-) 0.79 (-) 0.11
Tầng (LTH Của Chúng tôi) (+) 0.08 (+) 0.40

việc loại bỏ một nhóm bộ lọc (cụ thể, tất cả bộ lọc) từ một vị trí cụ thể, xem Phụ lục A.1. Do đó, có vẻ như một hành vi phản trực giác rằng vé thắng của chúng tôi có độ chính xác vượt trội so với LTH tiêu chuẩn. Tuy nhiên, kết quả của chúng tôi phù hợp với một nhóm nghiên cứu xác nhận lợi ích của việc loại bỏ tầng so với các cấu trúc khác [Zhang và Liu, 2022; Zhou et al., 2022; Han et al., 2022]. Để củng cố những tuyên bố này, chúng tôi đo sự tương tự biểu diễn giữa các mô hình của Bảng 3 và mạng dày đặc, tương tự như chúng tôi đã thực hiện trong các thí nghiệm trước đây. Trên chỉ số này, các mạng con của chúng tôi thể hiện sự tương tự cao hơn so với các mạng con từ cắt tỉa bộ lọc, bất kể tiêu chí quan trọng (SNIP hoặc GraSP). Các giá trị như vậy gợi ý rằng vé thắng của chúng tôi giữ biểu diễn nội bộ tương tự hơn với mạng dày đặc gốc so với những vé được tìm thấy bởi LTH có cấu trúc tiêu chuẩn, điều này hỗ trợ độ chính xác cao nhất của chúng trong Bảng 3.

Nhìn chung, chúng tôi tin rằng các phát hiện trên mở ra một hướng nghiên cứu mới cho LTH: ảnh hưởng của cấu trúc được tính đến trong quá trình cắt tỉa.

Chi phí Tính toán của Vé Thắng tại Khởi tạo. Thảo luận trên xác nhận sự tồn tại của vé thắng tại khởi tạo khi cắt tỉa loại bỏ tầng (nhớ rằng các mạng con của chúng tôi đến từ cắt tỉa tầng – xem Thuật toán 1 và 2). Từ góc độ thực tế, xác định vé thắng tại khởi tạo có nghĩa là chúng ta có thể tăng tốc giai đoạn huấn luyện và tiết kiệm tính toán về các chỉ số hiệu suất khác nhau, vì chúng ta có thể thay thế việc học một mạng dày đặc bằng phiên bản thưa thớt của nó. Những thành tựu như vậy trở nên khả thi vì chúng ta khám phá các mạng con mà (i) hiệu quả hơn mạng dày đặc nặng và có quá nhiều tham số; (ii) phù hợp với khả năng dự đoán của tương đương dày đặc của chúng (tức là các mạng con là vé thắng) và (iii) không tiêu tốn chi phí bổ sung vì chúng ta khám phá chúng trước bất kỳ epoch huấn luyện nào.

Bảng 4 báo cáo lợi ích hiệu suất của vé thắng của chúng tôi tại khởi tạo trên các chỉ số cắt tỉa tiêu chuẩn như phép toán dấu phẩy động (FLOPs), tiêu thụ bộ nhớ và tăng tốc huấn luyện. Theo xu hướng hiện đại [Lacoste et al., 2019; Strubell et al., 2019; You et al., 2020], chúng tôi cũng báo cáo khí thải CO2 trong giai đoạn huấn luyện. Theo Bảng 4, lợi thế của việc khám phá vé thắng tại khởi tạo rõ ràng. Ví dụ, vé thắng của chúng tôi tăng tốc thời gian huấn luyện của ResNet32 và ResNet56 lên tới 1.57× và 2.03×, có nghĩa là chúng ta có thể giảm đáng kể các chi phí liên quan trong bước huấn luyện như tiêu thụ năng lượng và khí thải CO2. Quan trọng là, tất cả những cải thiện này đến mà không có chi phí bổ sung.

Các nỗ lực trước đây đã đề xuất tìm vé thắng sớm trong giai đoạn huấn luyện [You et al., 2020; Chen et al., 2022]. Công trình của chúng tôi khác với những công trình này, vì chúng tôi khám phá vé thắng trước bất kỳ huấn luyện nào (nói một cách khái quát, các mạng con của chúng tôi là sớm nhất có thể), đây là một chiến lược hiệu quả hơn vì lợi ích trong Bảng 4 xảy ra trước khi huấn luyện bắt đầu.

Tính Mạnh mẽ của Vé Thắng tại Khởi tạo. Có một nhóm công trình ngày càng phát triển đánh giá các phương pháp cắt tỉa trong hình ảnh đối kháng vì kỹ thuật xuất hiện như một cơ chế phòng thủ mạnh mẽ và hiệu quả chống lại các cuộc tấn công đối kháng và các ví dụ ngoài phân phối [Diffenderfer et al., 2021;

--- TRANG 7 ---
Bảng 4: Lợi ích hiệu suất khi cắt tỉa khám phá vé thắng (cắt tỉa tầng) tại khởi tạo. Các giá trị chỉ ra sự cải thiện (tính bằng phần trăm – càng cao càng tốt) của vé so với mạng dày đặc.

Mật độ Cắt tỉa
(p) | FLOPs | Tiêu thụ Bộ nhớ | Khí thải CO2 | Tăng tốc Huấn luyện

Vé Thắng
tại Khởi tạo
(ResNet32)
1 | 6.82 | 10.13 | 11.11 | 1.10
3 | 20.47 | 25.29 | 25.92 | 1.35
4 | 34.13 | 30.11 | 33.33 | 1.57

Vé Thắng
tại Khởi tạo
(ResNet56)
3 | 11.25 | 15.15 | 13.33 | 1.16
4 | 15.00 | 19.42 | 20.00 | 1.24
5 | 18.76 | 23.69 | 24.44 | 1.34
12 | 45.00 | 45.82 | 51.11 | 2.03

Liu et al., 2022; T et al., 2022; Chen et al., 2022]. Trong thí nghiệm này, chúng tôi chứng minh liệu có tồn tại một số nhạy cảm của vé thắng của chúng tôi đối với những tình huống này hay không. Vì mục đích này, chúng tôi sử dụng bộ dữ liệu CIFAR-C [Hendrycks và Dietterich, 2019] và CIFAR-10.2 [Lu et al., 2020]. Điều quan trọng cần đề cập là giai đoạn huấn luyện của chúng tôi sử dụng huấn luyện sạch tiêu chuẩn trên CIFAR-10, có nghĩa là chúng tôi không sử dụng bất kỳ cơ chế phòng thủ nào (tức là huấn luyện đối kháng).

Bảng 5 (cột thứ ba) cho thấy sự cải thiện về tính mạnh mẽ của vé thắng tại khởi tạo (những vé trong Bảng 4) so với mạng sâu được sử dụng để khám phá chúng. Theo kết quả, vé thắng của chúng tôi hiệu quả tăng tính mạnh mẽ đối với hình ảnh đối kháng. Cụ thể, vé thắng của chúng tôi vượt trội mạng dày đặc tương ứng lên tới 1.61 và 3.73pp cho ResNet32 và ResNet56, tương ứng. Thú vị là, với mỗi kiến trúc, một vé thắng thể hiện tính mạnh mẽ thấp, trong đó sự suy giảm cao nhất chỉ là 0.16pp.

Về tổng quát hóa ngoài phân phối (cột cuối cùng trong Bảng 5), vé thắng của chúng tôi tăng khả năng tổng quát hóa của ResNet56 lên tới 1.75. Tuy nhiên, trên ResNet32, chúng tôi quan sát thấy rằng kết quả trở nên tiêu cực như một hàm của mức độ nghiêm trọng cắt tỉa; do đó đặt ra câu hỏi liệu vé của chúng tôi có tổng quát hóa tốt trong các ví dụ ngoài phân phối khi được trích xuất từ các kiến trúc nông hay không. Phân tích hành vi như vậy là một hướng thú vị cho công trình tương lai.

Đối với một số thiết lập (5 trên 14 cấu hình), vé thắng của chúng tôi kém hiệu suất so với tương đương dày đặc của chúng. Chúng tôi không quan sát

Bảng 5: Tính mạnh mẽ chống lại các ví dụ đối kháng và ngoài phân phối của vé thắng của chúng tôi tại khởi tạo. Trên cả bộ dữ liệu CIFAR-C và CIFAR-10.2, chúng tôi báo cáo sự cải thiện (tính bằng điểm phần trăm – càng cao càng tốt) của các mạng con so với đối tác dày đặc của chúng.

p | CIFAR-C | CIFAR-10.2

Vé Thắng
tại Khởi tạo
(ResNet32)
1 | (+) 1.13 | (+) 0.00
3 | (-) 0.05 | (-) 1.15
4 | (+) 1.61 | (-) 0.65

Vé Thắng
tại Khởi tạo
(ResNet56)
3 | (+) 0.59 | (+) 0.70
4 | (+) 3.73 | (+) 1.75
5 | (-) 0.16 | (+) 0.70
12 | (+) 1.98 | (-) 1.95

thấy bất kỳ mẫu nào trong những vé hiệu suất thấp này và để lại cuộc điều tra sâu về vấn đề này cho nghiên cứu tương lai. Nhìn chung, thành tựu về tính mạnh mẽ chống lại hình ảnh đối kháng và các ví dụ ngoài phân phối gợi ý rằng lợi ích của vé thắng của chúng tôi vượt ra ngoài việc giảm hiệu suất tính toán, điều này phù hợp với các phát hiện của các công trình trước đây: loại bỏ cấu trúc khỏi mạng tăng tính mạnh mẽ và tạo ra ngoài phân phối [Diffenderfer et al., 2021; Chen et al., 2022; T et al., 2022].

5 Kết luận
Trong công trình này, chúng tôi khám phá các khái niệm về Giả thuyết Vé Số (LTH) và cắt tỉa tại khởi tạo từ góc độ cắt tỉa tầng. Từ hình thức cắt tỉa này, chúng tôi mang lại cả lợi ích lý thuyết và thực tế. Đầu tiên, chúng tôi xác minh hành vi của LTH khi cắt tỉa tạo ra các mạng con (vé) bằng cách loại bỏ tầng từ một mạng dày đặc, trong đó chúng tôi xác nhận rằng các vé thành công trở thành vé thắng. Dựa trên điều này, chúng tôi đề xuất khám phá vé thắng một cách hệ thống tại khởi tạo, có nghĩa là xác định các mạng con thưa thớt (từ cắt tỉa tầng) mà không cần huấn luyện một mạng dày đặc. Theo các thí nghiệm mở rộng, vé thắng của chúng tôi tại khởi tạo tăng tốc giai đoạn học lên tới 2×, giảm khí thải carbon lên tới 51%. Những thành tựu như vậy đến mà không có giá cả bổ sung, vì chúng tôi trích xuất vé thắng trước khi huấn luyện mạng dày đặc. Ngoài ra, vé thắng của chúng tôi trở nên mạnh mẽ hơn chống lại hình ảnh đối kháng và tổng quát hóa tốt hơn trong các ví dụ ngoài phân phối so với đối tác dày đặc của chúng. Chúng tôi hy vọng những lợi ích này thu hút nhiều nghiên cứu hơn về LTH từ góc độ cắt tỉa tầng.

Hạn chế. Chiến lược của chúng tôi khám phá vé thắng tại khởi tạo sử dụng các tiêu chí được thiết lập tốt để gán tầm quan trọng cho mỗi tầng trước huấn luyện (tức là sử dụng các tham số được khởi tạo ngẫu nhiên để hướng dẫn thuật toán cắt tỉa). Do đó, nó kế thừa hạn chế của việc không thể chạy trên một lược đồ cắt tỉa lặp lại, điều đã được chứng minh là hiệu quả hơn cắt tỉa một lần. Do các chi tiết lý thuyết và kỹ thuật liên quan đến cắt tỉa tầng, LTH có cấu trúc của chúng tôi bị giới hạn trong lĩnh vực các kiến trúc giống residual. Do đó, LTH của chúng tôi không khả thi đối với các mạng đơn giản (ví dụ: VGG-16), cũng như các công trình khác của danh mục cắt tỉa này. May mắn thay, hầu hết các kiến trúc hiện đại sử dụng một số loại kết nối residual.

--- TRANG 8 ---
Câu hỏi Mở. Trong suốt các thí nghiệm, chúng tôi quan sát thấy rằng các mạng con của chúng tôi trở thành vé thắng dễ dàng hơn so với những mạng con được tạo ra bởi cắt tỉa bộ lọc. Phát hiện như vậy đặt ra câu hỏi liệu cấu trúc được loại bỏ có quan trọng trong thành công của LTH tại khởi tạo hay không. Phù hợp với vấn đề này, một hướng thú vị cho điều tra tương lai là nghiên cứu hành vi của LTH khi thuật toán cắt tỉa loại bỏ nhiều cấu trúc (trọng số, bộ lọc và tầng) đồng thời cũng như cách cắt tỉa những cấu trúc này cùng nhau tại khởi tạo.

Lời cảm ơn
Các tác giả muốn cảm ơn CNPq (tài trợ #309330/2018-1).

Tài liệu tham khảo
[Achille et al., 2019] Alessandro Achille, Matteo Rovere, và Stefano Soatto. Critical learning periods in deep networks. Trong International Conference on Learning Representations (ICLR), 2019.

[Blalock et al., 2020] Davis W. Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, và John V. Guttag. What is the state of neural network pruning? Trong Conference on Machine Learning and Systems (MLSys), 2020.

[Chen et al., 2020] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, và Michael Carbin. The lottery ticket hypothesis for pre-trained BERT networks. Trong Neural Information Processing Systems (NeurIPS), 2020.

[Chen et al., 2022] Tianlong Chen, Zhenyu Zhang, Pengjun Wang, Santosh Balachandra, Haoyu Ma, Zehao Wang, và Zhangyang Wang. Sparsity winning twice: Better robust generalization from more efficient training. Trong International Conference on Learning Representations (ICLR), 2022.

[de Jorge et al., 2021] Pau de Jorge, Amartya Sanyal, Harkirat S. Behl, Philip H. S. Torr, Grégory Rogez, và Puneet K. Dokania. Progressive skeletonization: Trimming more fat from a network at initialization. Trong International Conference on Learning Representations (ICLR), 2021.

[Diffenderfer et al., 2021] James Diffenderfer, Brian R. Bartoldson, Shreya Chaganti, Jize Zhang, và Bhavya Kailkhura. A winning hand: Compressing deep networks can improve out-of-distribution robustness. Trong Neural Information Processing Systems (NeurIPS), 2021.

[Dong et al., 2021] Yihe Dong, Jean-Baptiste Cordonnier, và Andreas Loukas. Attention is not all you need: pure attention loses rank doubly exponentially with depth. Trong International Conference on Machine Learning (ICML), 2021.

[Fan et al., 2020] Angela Fan, Edouard Grave, và Armand Joulin. Reducing transformer depth on demand with structured dropout. Trong International Conference on Learning Representations (ICLR), 2020.

[Frankle và Carbin, 2019] Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. Trong International Conference on Learning Representations (ICLR), 2019.

[Frankle et al., 2020] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, và Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. Trong International Conference on Machine Learning (ICML), 2020.

[Frankle et al., 2021] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, và Michael Carbin. Pruning neural networks at initialization: Why are we missing the mark? Trong International Conference on Learning Representations (ICLR), 2021.

[Han et al., 2016] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, và William J. Dally. EIE: efficient inference engine on compressed deep neural network. Trong International Symposium on Computer Architecture (ISCA), 2016.

[Han et al., 2022] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, và Yulin Wang. Dynamic neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong Computer Vision and Pattern Recognition (CVPR), 2016.

[Hendrycks và Dietterich, 2019] Dan Hendrycks và Thomas G. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Trong International Conference on Learning Representations (ICLR), 2019.

[Huang et al., 2016] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, và Kilian Q. Weinberger. Deep networks with stochastic depth. Trong European Conference on Computer Vision (ECCV), 2016.

[Jordao et al., 2020] Artur Jordao, Fernando Yamada, và William Robson Schwartz. Deep network compression based on partial least squares. Neurocomputing, 2020.

[Kornblith et al., 2019] Simon Kornblith, Mohammad Norouzi, Honglak Lee, và Geoffrey E. Hinton. Similarity of neural network representations revisited. Trong International Conference on Machine Learning (ICML), 2019.

[Krizhevsky et al., 2009] Alex Krizhevsky, Vinod Nair, và Geoffrey Hinton.. cifar-10 (canadian institute for advanced research). 2009.

[Lacoste et al., 2019] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, và Thomas Dandres. Quantifying the carbon emissions of machine learning. Trong NeurIPS. 2019.

[Lee et al., 2019] Namhoon Lee, Thalaiyasingam Ajanthan, và Philip H. S. Torr. Snip: single-shot network pruning based on connection sensitivity. Trong International Conference on Learning Representations (ICLR), 2019.

--- TRANG 9 ---
[Li et al., 2021] Zhengang Li, Geng Yuan, Wei Niu, Pu Zhao, Yanyu Li, Yuxuan Cai, Xuan Shen, Zheng Zhan, Zhenglun Kong, Qing Jin, Zhiyu Chen, Sijia Liu, Kaiyuan Yang, Bin Ren, Yanzhi Wang, và Xue Lin. NPAS: A compiler-aware framework of unified network pruning and architecture search for beyond real-time mobile acceleration. Trong Conference on Computer Vision and Pattern Recognition (CVPR), 2021.

[Lin et al., 2020] Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, và Ling Shao. Hrank: Filter pruning using high-rank feature map. Trong Conference on Computer Vision and Pattern Recognition (CVPR), 2020.

[Liu et al., 2019] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, và Trevor Darrell. Rethinking the value of network pruning. Trong International Conference on Learning Representations (ICLR), 2019.

[Liu et al., 2021] Ning Liu, Geng Yuan, Zhengping Che, Xuan Shen, Xiaolong Ma, Qing Jin, Jian Ren, Jian Tang, Sijia Liu, và Yanzhi Wang. Lottery ticket preserves weight correlation: Is it desirable or not? Trong International Conference on Machine Learning (ICML), 2021.

[Liu et al., 2022] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, và Mykola Pechenizkiy. The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. Trong International Conference on Learning Representations (ICLR), 2022.

[Lu et al., 2020] Shangyun Lu, Bradley Nott, Aaron Olson, Alberto Todeschini, Hossein Vahabi, Yair Carmon, và Ludwig Schmidt. Harder or different? a closer look at distribution shift in dataset reproduction. Trong International Conference on Machine Learning (ICML) Workshop on Uncertainty and Robustness in Deep Learning, 2020.

[Luo và Wu, 2020] Jian-Hao Luo và Jianxin Wu. Neural network pruning with residual-connections and limited-data. Trong Conference on Computer Vision and Pattern Recognition (CVPR), 2020.

[Niu et al., 2020] Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin, Yanzhi Wang, và Bin Ren. Patdnn: Achieving real-time DNN execution on mobile devices with pattern-based weight pruning. Trong Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2020.

[Paul et al., 2022] Mansheej Paul, Brett W. Larsen, Surya Ganguli, Jonathan Frankle, và Gintare Karolina Dziugaite. Lottery tickets on a data diet: Finding initializations with sparse trainable networks. Trong Neural Information Processing Systems (NeurIPS), 2022.

[Prasanna et al., 2020] Sai Prasanna, Anna Rogers, và Anna Rumshisky. When BERT plays the lottery, all tickets are winning. Trong Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.

[Renda et al., 2020] Alex Renda, Jonathan Frankle, và Michael Carbin. Comparing rewinding and fine-tuning in neural network pruning. Trong International Conference on Learning Representations (ICLR), 2020.

[Strubell et al., 2019] Emma Strubell, Ananya Ganesh, và Andrew McCallum. Energy and policy considerations for deep learning in NLP. Trong ACL, 2019.

[T et al., 2022] Mukund Varma T, Xuxi Chen, Zhenyu Zhang, Tianlong Chen, Subhashini Venugopalan, và Zhangyang Wang. Sparse winning tickets are data-efficient image recognizers. Trong Neural Information Processing Systems (NeurIPS), 2022.

[Tan và Motani, 2020] Chong Min John Tan và Mehul Motani. Dropnet: Reducing neural network complexity via iterative pruning. Trong International Conference on International Conference on Machine Learning (ICML). 2020.

[Tanaka et al., 2020] Hidenori Tanaka, Daniel Kunin, Daniel L. Yamins, và Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. Trong Neural Information Processing Systems (NeurIPS), 2020.

[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Neural Information Processing Systems (NeurIPS), 2017.

[Veit và Belongie, 2020] Andreas Veit và Serge J. Belongie. Convolutional networks with adaptive inference graphs. International Journal of Computer Vision (IJCV), 128:730–741, 2020.

[Veit et al., 2016] Andreas Veit, Michael J. Wilber, và Serge J. Belongie. Residual networks behave like ensembles of relatively shallow networks. Trong Neural Information Processing Systems (NeurIPS), 2016.

[Wang et al., 2020] Chaoqi Wang, Guodong Zhang, và Roger B. Grosse. Picking winning tickets before training by preserving gradient flow. Trong International Conference on Learning Representations (ICLR), 2020.

[You et al., 2020] Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Yingyan Lin, Zhangyang Wang, và Richard G. Baraniuk. Drawing early-bird tickets: Toward more efficient training of deep networks. Trong International Conference on Learning Representations (ICLR), 2020.

[Zhang và Liu, 2022] Ke Zhang và Guangzhe Liu. Layer pruning for obtaining shallower resnets. IEEE Signal Processing Letters, 2022.

[Zhou et al., 2021] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, và Hongsheng Li. Learning N: M fine-grained structured sparse neural networks from scratch. Trong International Conference on Learning Representations (ICLR), 2021.

[Zhou et al., 2022] Yao Zhou, Gary G. Yen, và Zhang Yi. Evolutionary shallowing deep neural networks at block levels. IEEE Transactions on Neural Networks and Learning Systems, 2022.

--- TRANG 10 ---
A Phụ lục
A.1 Cắt tỉa Tầng
Vấn đề Lý thuyết. Giả sử một mạng F có L tầng như một tập hợp L phép biến đổi fi(.). Vì sự đơn giản, fi bao gồm một chuỗi các phép toán tích chập, chuẩn hóa batch, và kích hoạt. Trong định nghĩa này, chúng ta thu được đầu ra mạng (y) bằng cách chuyển tiếp dữ liệu đầu vào qua các tầng tuần tự f, trong đó đầu vào của tầng i là đầu ra của tầng trước đó i-1; do đó, y=f(x) fL(...f2(f1(.))). Điều này tạo thành ý tưởng đằng sau các mạng đơn giản (tức là VGG).

Trong các mạng giống residual, đầu ra của tầng i, yi, bao gồm phép biến đổi fi cộng với đầu vào mà nó nhận được yi-1 (xem Hình 4). Chính thức, chúng ta có thể định nghĩa đầu ra của tầng thứ i là

yi = fi(y-1) + yi-1. (2)

Phương trình 2 tạo thành một module residual, trong đó phần bên phải được đặt tên là shortcut identity-mapping (hoặc identity cho ngắn gọn). Điều quan trọng cần quan sát trong Phương trình 2 là nếu chúng ta vô hiệu hóa f(i) (một tầng) thì yi = yi-1.

Veit et al. [2016] chỉ ra rằng identity cho phép thông tin đi theo các đường dẫn khác nhau trong mạng, theo nghĩa là, chúng ta có thể vô hiệu hóa một số fi mà không làm suy giảm (hoặc với thiệt hại không đáng kể) biểu diễn mong đợi của các tầng tiếp theo (tức là fi+1). Nói cách khác, một số tầng fi không phụ thuộc mạnh vào nhau; do đó, chúng ta có thể loại bỏ chúng. Ví dụ, trong Hình 4, chúng ta có thể loại bỏ tầng i mà không mất khả năng dự đoán của mạng. Mặt khác, do thiếu identity, các mạng đơn giản gặp sụp đổ trong biểu diễn nếu chúng ta chỉ loại bỏ một trong các tầng của chúng. Chúng tôi giới thiệu độc giả quan tâm đến Hình 3 của nghiên cứu bởi Veit et al. [2016] để so sánh sự sụt giảm độ chính xác giữa các mạng residual và đơn giản.

Vấn đề Kỹ thuật. Chúng ta có thể vô hiệu hóa tầng i bằng cách đặt trọng số của nó thành không (lược đồ zeroed-out được sử dụng rộng rãi). Theo cách này, đầu ra của tầng i-1 được kết nối trực tiếp với tầng i+1 (xem Hình 4 giữa). Tuy nhiên, quá trình như vậy không đạt được lợi ích hiệu suất mà không có các framework hoặc phần cứng chuyên dụng cho tính toán thưa thớt. Thay vì làm không trọng số, chúng ta có thể thực hiện quy trình sau. Sau khi xác định tầng nào cần loại bỏ (tức là nạn nhân), chúng ta tạo một mạng mới mà không có tầng i và chuyển trọng số của các tầng được giữ lại sang mạng mới. Ví dụ, nếu chúng ta có một mạng với L tầng và muốn loại bỏ k tầng, thì, chúng ta tạo một mạng mới với L-k tầng. Tóm lại, mạng được cắt tỉa (dưới trong Hình 2) kế thừa trọng số của các tầng được giữ lại của mạng gốc (trên trong Hình 2).

Chúng tôi nêu bật rằng cắt tỉa không thể loại bỏ một số tầng do kích thước không tương thích của các tensor (đầu vào/đầu ra). Sự không tương thích như vậy đến từ tầng độ phân giải không gian (tầng downsampling). Cụ thể hơn, chúng ta không thể loại bỏ các tầng trước và sau các tầng downsampling. Quan trọng là, cắt tỉa bộ lọc cũng gặp vấn đề này.

A.2 Chuẩn ℓ1
Hình 3 minh họa điểm số chuẩn ℓ1 của các tầng (những tầng mà cắt tỉa có thể loại bỏ) của ResNet32. Từ hình này, chúng ta thấy rằng độ lớn của điểm số các tầng tương quan với giai đoạn (nhóm tầng hoạt động trên cùng độ phân giải của bản đồ đặc trưng) mà chúng thuộc về. Vì chiến lược cắt tỉa của chúng tôi tính đến tất cả các tầng (tức là tất cả điểm số) cùng một lúc, tiêu chí này không khả thi. Cụ thể, có một thiên kiến đối với các tầng của các giai đoạn sớm (tức là chúng sẽ luôn được chọn làm nạn nhân).

Chuyển
Trọng số

Hình 2: Quy trình tổng thể để loại bỏ tầng (mô hình residual) từ một mạng residual. Sau khi xác định một tầng nạn nhân (hình chữ nhật nét đứt), chúng ta tạo một mạng mới (dưới) mà không có nó. Cuối cùng, chúng ta chuyển trọng số (mũi tên đỏ) của các tầng được giữ lại từ mạng chưa được cắt tỉa gốc (trên) sang mạng mới.

0 2 4 6 8 10 12
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1

Chỉ số Tầng (được chuẩn hóa)

Độ lớn chuẩn ℓ1

Giai đoạn 1
(32x32)
Giai đoạn 2
(16x16)
Giai đoạn 3
(8x8)

Hình 3: Điểm số chuẩn ℓ1 của các tầng của ResNet32. Các tầng trong một giai đoạn hoạt động trên cùng độ phân giải không gian đầu vào/đầu ra (tức là kích thước của bản đồ đặc trưng – giá trị trong ngoặc đơn).

--- TRANG 11 ---
Tầng Conv.
Tầng Conv.
. . .
Tầng Conv.
Tầng Conv.
Tầng Conv. | Tầng Conv. | Tầng i - 1 | Tầng i | Tầng i + 1 | . . . | . . .

yk | yi + 1

fi - 1 | fi | fi + 1

yk | yi | yi - 1 | yi - 1

yi

Hình 4: Kiến trúc của một mạng giống residual. Lý luận đằng sau kiến trúc này là đầu ra của một tầng tính đến phép biến đổi được thực hiện bởi nó (f) cộng (⊕) với đầu vào (y) mà nó nhận được. Do bản chất này, khi chúng ta vô hiệu hóa tầng i (phép biến đổi của nó – đường nét đứt), đầu ra (biểu diễn) của tầng i-1 được truyền đến tầng i+1, có nghĩa là đầu ra yi thuộc về yi-1. Vì sự đơn giản, chúng tôi bỏ qua các tầng chuẩn hóa batch và kích hoạt.
