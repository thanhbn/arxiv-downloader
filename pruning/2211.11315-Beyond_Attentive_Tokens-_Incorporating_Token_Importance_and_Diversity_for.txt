# 2211.11315.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2211.11315.pdf
# File size: 1223000 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Beyond Attentive Tokens: Incorporating Token Importance and Diversity for
EfÔ¨Åcient Vision Transformers
Sifan Long1,2*Zhen Zhao3,2*Jimin Pi2Shengsheng Wang1‚Ä†Jingdong Wang2‚Ä†
1Jilin University2Baidu VIS3University of Sydney
longsf22@mails.jlu.edu.cn zhen.zhao@sydney.edu.au
wss@jlu.edu.cn fpijimin01, wangjingdong g@baidu.com
Abstract
Vision transformers have achieved signiÔ¨Åcant improve-
ments on various vision tasks but their quadratic interac-
tions between tokens signiÔ¨Åcantly reduce computational ef-
Ô¨Åciency. Many pruning methods have been proposed to re-
move redundant tokens for efÔ¨Åcient vision transformers re-
cently. However, existing studies mainly focus on the token
importance to preserve local attentive tokens but completely
ignore the global token diversity. In this paper, we empha-
size the cruciality of diverse global semantics and propose
an efÔ¨Åcient token decoupling and merging method that can
jointly consider the token importance and diversity for to-
ken pruning. According to the class token attention, we
decouple the attentive and inattentive tokens. In addition
to preserve the most discriminative local tokens, we merge
similar inattentive tokens and match homogeneous atten-
tive tokens to maximize the token diversity. Despite its sim-
plicity, our method obtains a promising trade-off between
model complexity and classiÔ¨Åcation accuracy. On DeiT-S,
our method reduces the FLOPs by 35% with only a 0.2%
accuracy drop. Notably, beneÔ¨Åting from maintaining the to-
ken diversity, our method can even improve the accuracy of
DeiT-T by 0.1% after reducing its FLOPs by 40%.
1. Introduction
Transformer [29] has become the most popular archi-
tecture in both natural language processing and computer
vision communities. Vision transformers (ViTs) [8] have
achieved superior performance and outperformed standard
CNNs in different vision tasks such as image classiÔ¨Åca-
tion [10, 28, 31, 38], semantic segmentation [17, 19, 30, 33],
and object detection [1, 5]. The most remarkable advantage
of transformer is its ability to effectively capture long-range
dependencies between patches in the input image through
*Equal contribution.
‚Ä†Corresponding authors.
Token
Minority Fusion ClusterDiversity-basedToken
Minority FusionImportance-based
Preserve
Discard
Token
Minority FusionIncorporate both
ClusterMatch(a)
(b)
(c)Figure 1. The ImageNet accuracy and keep rate of the pruned
DeiT-S. (a) Importance-based method preserves attentive tokens
based on the class token attention and masks all inattentive tokens;
(b) Diversity-based method clusters similar tokens into a group
and then combines tokens from the same group into a new token.
(c) Incorporate method decouples and merges tokens to consider
token importance and diversity simultaneously.
the self-attention mechanism [23]. However, quadratic in-
teractions between tokens signiÔ¨Åcantly degrade the compu-
tational efÔ¨Åciency [36], which motivates many researches
on exploring efÔ¨Åcient transformers.
As one of the most direct and effective ways to reduce
computational complexity, token pruning has been widely
studied recently. Existing studies mainly focus on designing
different importance-evaluating strategies to retain atten-
tive tokens and prune inattentive tokens [18, 21, 23, 35, 37].
In these importance-based works, DyViT [23] introduces
an extra module to estimate the importance of each token
while EViT [18] reorganizes image tokens based on the
class attention importance score. However, inspired by re-
cent diversity-preserving studies in ViT variants [9, 11‚Äì13,
25,27], we argue that promoting token diversity is also cru-
1arXiv:2211.11315v1  [cs.CV]  21 Nov 2022

--- PAGE 2 ---
cial for token pruning. Though inattentive tokens like the
image background and low-level textures are not directly
related to the classiÔ¨Åcation objects, they can increase the
token diversity and improve the expressivity of the model.
As discussed in [32], image backgrounds ( e.g., the grass
and leaves in Fig. 2) can improve the classiÔ¨Åcation accuracy
due to their potential relations to foreground objects. To this
end, we Ô¨Årst investigate a diversity-based pruning strategy
on DeiT-S [28] with different keep rates. SpeciÔ¨Åcally, in-
stead of highlighting the token importance, it directly clus-
ters and combines similar tokens into a single one, hereby
maximizing the token diversity. Surprisingly, as shown in
Fig. 1, such an intuitive strategy can achieve comparable
and even better performance than SOTA importance-based
pruning methods, especially at the low keep rate.
Despite its promising performance, the diversity-based
strategy cannot retain original attentive tokens and may con-
sequently weaken the discriminative ability of the model.
As shown in Fig. 2 (c), the most representative tokens, e.g.,
eyes and ears of the dog or beaks of two birds, contain crit-
ical semantic information for classiÔ¨Åcation tasks but cannot
be preserved by the diversity-based strategy. To address this
issue, we naturally tend to keep all these dominant tokens
while maintaining the token diversity, as shown in Fig. 2
(d). In short, a satisÔ¨Åed pruning method should jointly take
the token importance and diversity into account, such that
the most important local information and the diverse global
information can be preserved simultaneously.
Motivated by these above observations, in this paper, we
propose a novel pruning method that incorporates the token
importance and diversity through efÔ¨Åcient token decoupling
and merging. As shown in Figure 1 (c), we Ô¨Årst decouple
the origin token sequence into attentive and inattentive por-
tions based on class token attention. Instead of discarding
inattentive tokens completely, we apply a simpliÔ¨Åed density
peak clustering algorithm [24] to efÔ¨Åciently cluster similar
inattentive tokens and combine these tokens from the same
group into a new one. In addition, unlike existing methods
that preserve all attentive tokens, we design a straightfor-
ward matching algorithm to fuse homogeneous attentive to-
kens and improve the calculation efÔ¨Åciency further. In this
way, we can effectively prune tokens while maximizing the
preservation of token diversity. We conduct extensive to-
ken pruning experiments to validate the effectiveness of our
method. Despite its simplicity, our method achieves supe-
rior pruning performance on ImageNet [6] for two different
vision transformers, DeiT [28] and LV-ViT [15]. Our main
contributions are summarized as follows:
‚Ä¢ To the best of our knowledge, we are the Ô¨Årst to em-
phasize the token diversity for pruning ViT. We also
demonstrate its cruciality through numerical and em-
pirical analysis.
(a) (b) (c) (d)Figure 2. Visualizations of pruning results of different methods on
ImageNet with DeiT-S. (a) Original image. (b) Importance-based
method masks inattentive tokens. (c) Diversity-based method clus-
ters similar tokens and visualizes the same group of tokens as one
colour. (d) Our method preserves the most discriminative tokens,
e.g., the heads of birds and dogs. In addition, we merge similar
inattentive tokens and match homogeneous attentive tokens, e.g.,
the grass and leaves.
‚Ä¢ We propose a simple yet effective decoupling and
merging method that can simultaneously preserve the
most attentive local tokens and diverse global seman-
tics without imposing extra parameters.
‚Ä¢ BeneÔ¨Åting from incorporating token importance and
diversity, our method achieves new SOTA performance
on the trade-off between accuracy and FLOPs. It
can also be deployed to other token pruning methods,
achieving excellent performance improvement.
2. Related work
Vision Transformers. Different from convolution net-
works, the transformer has a signiÔ¨Åcant ability to model
long-range dependencies and minimal inductive bias [35].
Recent advances suggest that the variants of transformers
could be a competitive alternative to CNNs. Visual trans-
former (ViT) [8] is the Ô¨Årst work to apply transformer ar-
chitecture to achieve STOA performance, but it only re-
places the standard convolution in the deep neural network
on large-scale image datasets. To free ViT from dependence
on large datasets, DeiT [28] incorporates an additional to-
ken for knowledge distillation to improve the training efÔ¨Å-
ciency of vision transformers. LV-ViT [15] further improves
the performance by utilizing all the image patch tokens to
calculate the training loss intensively. It is equivalent to
converting the image classiÔ¨Åcation problem into each token
recognition problem. While ViT and its follow-ups achieve
excellent performance, the complexity quadratic with the
number of tokens incurs high computational costs. Token
pruning aims to reduce redundant tokens and improve the
inference efÔ¨Åciency of various ViT backbones.
2

--- PAGE 3 ---
Figure 3. Illustration of our approach. (top) Employ our method at the 4th, 7th, and 10th layers of the DeiT-S model. (bottom) Model
structure within a single transformer block. We decouple the attentive and inattentive tokens according to class token attention. Then, we
cluster inattentive tokens and combine the tokens from the same group into a new token. Meanwhile, we match homogeneous attentive
tokens and combine the same pair of tokens.
ViT Token pruning. Though ViT has achieved compet-
itive accuracy in vision tasks [1, 5, 10, 28, 31, 38], it needs
huge memory and computational resources. Therefore, how
to build a more efÔ¨Åcient transformer draws researchers‚Äô in-
terest. Compared with CNN, the higher computing cost of
the transformers is mainly due to the quadratic time com-
plexity of multi-head self-attention (MHSA). Accordingly,
some work [18, 21, 23, 35] attempts to prune tokens based
on importance score in transformer. Based on whether ex-
tra parameters need to be introduced to the model, we di-
vide the existing token pruning methods into the following
two groups. One group performs token pruning by inserting
prediction modules. DyViT [23] designs a lightweight pre-
diction module inserted into different layers to estimate the
importance score of each token to prune redundant tokens
given the current features. IA-RED2 [21] introduces inter-
pretable modules to dynamically delete redundant patches
that are not related to the input. AdaViT [20] connects a
lightweight decision network to the backbone to dynam-
ically generate decisions. The other group leverages the
class token attention to keep attentive tokens. EViT [18]
divides image tokens into attentive and inattentive tokensaccording to class token attention, retains attentive tokens
and discards inattentive image tokens to reorganize image
tokens. Evo-ViT [35] distinguishes informative and un-
informative tokens through global class attention for slow
and fast updates, respectively. A-ViT [37] designs an adap-
tive token pruning mechanism based on class token atten-
tion, which dynamically adjusts the calculation cost of im-
ages with different complexity. Unlike these token prun-
ing methods, which only focus on the importance of tokens,
our method also considers the diversity of token semantic
information. Therefore our method achieve incredible per-
formance.
3. Preliminaries
In standard vision transformers [8], each input image
I2RHWCis Ô¨Årst converted into a single-dimensional
patch sequence X2RNP2C. Then all patches are
mapped into D-dimensional token embeddings via a train-
able linear layer. Additionally, a learnable position embed-
ding Epos2R(N+1)Dis added to token embedding to
retain position information. Formally, the input patch se-
quence can be represented as:
3

--- PAGE 4 ---
X = [x cls; x1;:::; xN] + E pos; (1)
where xclsdenotes the learnable class token that serves as
the image representation, and xidenotes the token of the i-
th patch with i0. Afterwards, such token sequence is fed
into a ViT model with Lstacked transformer blocks, each of
which consists of a multi-head self-attention (MHSA) mod-
ule and a feed forward network (FFN).
3.1. MHSA & FFN
In MHSA, the input token sequence is linearly mapped
into three different matrices of query Q, key K, and value
V, respectively. MHSA can be formulated as:
MHSA (Z) = Concat"
softmax 
Qh 
Kh>
p
d!
Vh#H
h=1;
(2)
where Zis the token sequence of N + 1 tokens. Concat []
outputs the feature concatenation of Hheads. Qh,Khand
Vhare projection matrices of Q,K, and Vin the h-th head,
respectively. dis the feature dimension of the single head.
FFN typically consists of two fully-connected layers and
a nonlinear mapping layer, which can expressed as:
FFN (Z) = Sigmoid(Linear(GeLU(Linear(Z)))) ;(3)
where Linear denotes the fully-connected layers and GeLU
is an non-linear activation function.
3.2. Computation Complexity
The dimension of the input token sequence is ND,
whereNis the number of tokens and Dis the embed-
ding dimension of each token. Thus the calculational costs
of MSHA and FFN modules are O 
4ND2+ 2N2D
and
O 
8ND2
, respectively. Obviously, vision transformers
require very intensive computational costs, with the total
computational complexity of O 
12ND2+ 2N2D
. Since
reducing the channel dimension Donly affects the calcu-
lation of current matrix multiplication, most related works
tend to prune tokens, e.g. reducing the number of N, to re-
duce all operations linearly or even quadratically.
4. Methodology
4.1. Overview
Different from existing works only focus on attentive to-
kens, our method incorporating token importance and di-
versity to obtain efÔ¨Åcient and accurate vision transformers.
To this end, we propose the token decoupling and merging
method, achieving promising trade-offs between the FLOPs
and accuracy. As shown in Figure 3, we insert our approach
at the 4th, 7th, and 10th layers of the DeiT-S model. The
56606468727680
0.680.720.760.80.840.880.92
0.7 0.6 0.5 0.4 0.3 0.2 0.1
Accuracy(%)Diversity Score
Keep RateOur method (Diversity Score)
EViT method (Diversity Score)
Our method (Accuracy)
EViT method (Accuracy)Figure 4. Comparing the pruning results of our method and the
EViT method with different keep rates on DEiT-S in terms of the
diversity score and classiÔ¨Åcation accuracy on ImageNet. The di-
versity score is obtained at the Ô¨Ånal pruning layer.
approach has two main components: the token decoupler
andthe token merger . The decoupler divides the origin
token sequence into attentive and inattentive sections based
on class token attention. Then the merger clusters similar
inattentive tokens and matches homogeneous attentive to-
kens. In this section, we Ô¨Årst demonstrate how preserving
token diversity beneÔ¨Åts token pruning and then present the
two main components in detail.
4.2. Token diversity matters
In the literature, most work only emphasize retaining im-
portant tokens but directly discard all the remaining ones
to achieve satisfactory token keep rates. However, inspired
by the observations in [32], that even the image back-
ground can help improve foreground-instance classiÔ¨Åcation,
we argue that the less important tokens could also contain
useful semantic information and be an effective comple-
mentary to the information diversity. Also, as discussed
in [9, 11‚Äì13, 25, 27], the token diversity is very critical to
optimize transformer structures. Therefore, appropriately
preserving these inattentive tokens augments the diversity
of semantic information, which can be beneÔ¨Åcial to token
pruning. On the contrary, blindly discarding tokens will
cause irreversible loss of semantic information, especially
at the low keep rates. Referring to [7, 11, 26, 27], we lever-
age the difference between the token and a rank-1 matrix to
measure the diversity of token sequence Z. The diversity
4

--- PAGE 5 ---
scoresr(Z)can be calculated as:
r(Z) =Z 1z>;where z= argminz0Z 1z0>;
(4)
wherekk representsl1norm. Z2RNCis the token se-
quence ofNtokens and z;z02RCis one of the tokens. z>
is the matrix transpose of zand 1 is an all-ones vector. The
rank of matrix 1z>is 1. A larger diversity score indicates a
more diverse token sequence.
We investigate how the diversity scores affect the token
pruning performance. In Figure 4, we examine the classi-
Ô¨Åcation accuracy and the diversity score at different keep
rates. Obviously, we can see that the token sequence di-
versity score is positively correlated with classiÔ¨Åcation ac-
curacy. In either the EViT method or our proposed method,
higher diversity score consistently result in higher accuracy.
In addition, it can also be observed that, as for the EViT
method, the diversity score and classiÔ¨Åcation accuracy drop
rapidly as the keep rate decreases. Differently, beneÔ¨Åting
from our diversity-preserving token merging strategy, our
method can maintain relatively higher diversity scores at
different keep-rates and thus consistently outperform the
EViT method, especially at the low keep-rate. Therefore,
maintaining higher token diversity is crucial to improve
classiÔ¨Åcation accuracy.
4.3. Token Decoupler
In order to fully consider token importance while main-
taining diversity, we prioritize the attentive tokens to pre-
serve the most important semantic information. Therefore,
we decouple original token sequence into attentive and inat-
tentive tokens so that we maintain token diversity and im-
portance simultaneously. Same as [29], we split the tokens
into two groups by comparing their similarities with the
class tokens. Mathematically, the similarity scores Attn cls
between the class token and other tokens as class token at-
tention can be calculated by
Attn cls= SoftmaxqclsK>
p
d
; (5)
where qclsdenotes the class token of query vector. With N
tokens in total and the keep rate of , we choose the top-K
tokens as attentive tokens according to attention scores. The
remainingN-Ktokens are identiÔ¨Åed as inattentive tokens
that contain less information. Moreover, in the multi-head
self-attention layer, we calculate the average of the attention
scores of all heads.
4.4. Token Merger
We apply different strategies for attentive and inatten-
tive tokens when merging tokens. For inattentive tokens,
we Ô¨Årst apply density peak clustering algorithm to clus-
ter inattentive tokens and then combine the tokens fromthe same group into new token by weighted sum. In this
way, we can integrate a new inattentive token sequence
T = [t 1;:::; tn]. For attentive tokens, we adapt a straight-
forward matching algorithm to fuse homogeneous attentive
tokens. The fused token sequence is P = [p 1;:::; pm].
We concat TandPto obtain the pruned token sequence
Z = [z cls; p1;:::; pm; t1;:::; tn].
Inattentive Token Clustering. Common K-means clus-
tering algorithm requires multiple iterations to obtain satis-
factory results, reducing throughput in practice and defeat-
ing the intent of speeding up the model. After research, we
found that the density clustering algorithm can quickly Ô¨Ånd
classes of arbitrary shape by exploiting the density connec-
tivity of classes. Therefore, we simplify an efÔ¨Åcient density
peak clustering algorithm (DPC) with neither an iterative
process nor more parameters. We follow the DPC algorithm
proposed in [24]. It assumes that the cluster center is sur-
rounded by low-density neighbours, and that the distance
between the cluster center and any high-density points is
relatively large. We calculate two variables for each token
i: the density and the minimum distance from the higher
density token . Given a set of tokens x, we calculate the
density of each token by
i= exp0
@X
zj2Zkzi zjk2
21
A: (6)
where Zdenotes the set of tokens. ziandzjare correspond-
ing token features.
For the token with the highest density, its minimum dis-
tance is set to the maximum distance between it and any
other tokens. We deÔ¨Åne ias the minimum distance be-
tween the token iand any other token with higher density.
The minimum distance of each token is:
i=minj:j>ikzi zjk2;if9js.t.j>i
maxjkzi zjk2;otherwise:(7)
We denote the clustering center score of the i-th token
asii. Higher scores mean higher potential to be clus-
ter centers. We select top-K-scored tokens as cluster cen-
ters. The DPC algorithm assigns each remaining token to
the cluster whose cluster center is closest to the token and
has a higher density.
Attentive Token Matching. See example images in Fig-
ure 5. Homogeneous tokens are also present in foreground
objects (attentive tokens), such as the cheeks of animals.
This redundancy makes it possible to fuse homogeneous at-
tentive tokens to reduce the number of tokens while main-
taining accuracy. We could apply the same token cluster-
ing strategy as did for inattentive tokens. However, since
5

--- PAGE 6 ---
Model Method Top-1 Acc. (%) Params (M) FLOPs (G) FLOPs ‚Üì(%) Throughput (img/s)
DeiT-TDeiT-T [28] 72.2 5.6 1.3 0.0 2536
DyViT [23] 71.2 5.9 0.9 30.8 3542
PS-ViT [26] 72.0 5.6 0.9 30.8 3563
SViTE [3] 70.1 4.2 0.9 30.8 2836
SPViT [16] 72.2 5.6 1.0 23.1 -
Evo-ViT [35] 72.0 5.6 0.8 38.5 3627
Ours-DeiT-T 72.3 5.6 0.8 38.5 3641
DeiT-SDeiT-S [28] 79.8 22.1 4.6 0.0 943
DyViT [23] 79.3 22.8 2.9 37.0 1420
PS-ViT [26] 79.4 22.1 2.6 43.5 1392
IA-RED2 [21] 79.1 22.1 3.2 30.4 1362
Evo-ViT [35] 79.4 22.1 3.0 34.8 1449
EViT [18] 79.5 22.1 3.0 34.8 1455
A-ViT [37] 78.6 22.1 3.6 21.7 -
Ours-DeiT-S 79.6 22.1 3.0 34.8 1468
EViT+Ours 79.6 22.1 3.0 34.8 1459
DeiT-BDeiT-B [28] 81.8 86.6 17.6 0.0 302
DyViT [23] 81.3 - 11.6 34.1 454
PS-ViT [26] 81.5 86.6 11.6 34.1 445
IA-RED2 [21] 80.9 86.6 11.6 34.1 453
Evo-ViT [35] 81.3 86.6 11.6 34.1 448
EViT [18] 81.3 86.6 11.6 34.1 450
Ours-DeiT-B 82.0 86.6 11.6 34.1 462
Table 1. Comparisons with existing token pruning methods on DeiT. We report the top-1 classiÔ¨Åcation accuracy, FLOPs, and throughput
on ImageNet. ‚ÄòFLOPs ‚Üì‚Äô denotes the reduction ratio of FLOPs.
the attentive tokens contain critical semantic information
for the Ô¨Ånal classiÔ¨Åcation task, it would be best if we can
preserve the original tokens. To address this problem, we
customize a straightforward matching algorithm that keeps
the most important tokens while fusing homogeneous to-
kens. SpeciÔ¨Åcally, we deÔ¨Åne the cosine similarity metric to
determine the similarity between different tokens and calcu-
late cosine similarity scores between attentive tokens. Then
we select top-K most similar token pairs as homogeneous
tokens. Finally, we combine each pair of tokens into a new
token and contact the remaining attentive tokens.
Although tokens in the same set have similar seman-
tic information, the semantic importance of each token is
not necessarily the same. Instead of blindly averaging
the tokens in the same set, we combine these tokens by a
weighted sum. By introducing a class token attention to
represent the importance, we combine the same set of to-
kens into a new token by
pi=X
j2Cisjzj; (8)
wheresjdenotes the importance score of token zj, andCi
denotes thei-th set.5. Experiments
5.1. Setup
Dataset and evaluation metrics. Our experiments are
conducted on ImageNet-1K [6] with 1.28 million training
images and 50000 validation images. We report the top-
1 classiÔ¨Åcation accuracy and the Ô¨Çoating-point operations
(FLOPs) to evaluate model efÔ¨Åciency. In addition, we mea-
sure the throughput of the models on a single NVIDIA
V100 GPU with batch size Ô¨Åxed to 256.
Implementation details. To demonstrate the generaliza-
tion of our method, we conduct token pruning on differ-
ent ViT models including DeiT-T, DeiT-S, DeiT-B [28], and
LV-ViT-S [15]. Following [18], we employ our method at
the4th,7th, and 10thlayers of the DeiT-T/S/B model and
at the 4th,8th, and 12thlayers for LV-ViT-S [15]. We
utilize the same training settings as the original papers of
DeiT [28] and LV-ViT [15]. Following [38], we incorpo-
rate a cosine scheduler into our learning strategy where the
keep-rate gradually decreases from 1 to the target value for
100 epochs. For fair comparisons, all of our models are
trained from scratch for 300 epochs on 8 NVIDIA V100.
6

--- PAGE 7 ---
Figure 5. Visualization of token merger results on DeiT-S. The masked areas of different colours represent the inattentive tokens are divided
into dissimilar token groups. Our method clusters similar inattentive tokens into a group and matches homogeneous attentive tokens. We
visualize the same groups/pairs of tokens as the same colour.
Method Top-1 Acc FLOPs Params
(%) (G) (M)
ViT-Base/16 [8] 77.9 17.6 86.6
DeiT-S [28] 79.8 4.6 22.1
DeiT-Base/16 [28] 81.8 17.6 86.6
PVT-Small [30] 79.8 3.8 24.5
PVT-Medium [30] 81.2 6.7 44.2
CPVT-Small-GAP [4] 81.5 4.6 23.0
CoaT Mini [34] 80.8 6.8 10.0
CoaT-Lite Small [34] 81.9 4.0 20.0
CrossViT-S [2] 81.0 5.6 26.7
CrossViT-B [2] 82.3 14.1 64.1
Swin-T [19] 81.3 4.5 29.0
Swin-S [19] 83.0 8.7 50.0
Swin-B [19] 83.3 15.4 88.0
T2T-ViT-14 [38] 81.5 5.2 22.0
T2T-ViT-19 [38] 81.9 8.9 39.2
T2T-ViT-24 [4] 82.2 21.2 104.7
RegNetY-8G [22] 81.7 8.0 39.0
RegNetY-16G [22] 82.9 16.0 84.0
LV-ViT-S [14] 83.3 6.6 26.2
DyViT-LV-S 83.0 4.6 26.2
EViT-LV-S 83.0 4.7 26.2
Ours-LV-S 83.1 4.7 26.2
Table 2. Comparisons with state-of-the-art vision transformers on
ImageNet. We prune the LV-ViT-S as the base model.
5.2. Main Results
Comparisons with the-state-of-the-arts. We compare
our method with SOTA token pruning methods, results are
shown in Table 1. We leveraged the indicates the keep
rate. We report the top-1 accuracy, FLOPs, and throughput
for each model. Compared to previous work, our method
achieves new SOTA performance with similar computation
3.0 2.6 2.3 2.0 1.8 1.6
FLOPs(G)6668707274767880Accuracy(%)79.679.379.078.6
77.8
76.479.479.1
78.4
77.6
76.1
73.279.3
78.7
77.9
77.0
73.4
68.9Ours EViT DyViTFigure 6. Performance comparisons of DyViT, EViT, and our
method with different FLOPs.
costs. SpeciÔ¨Åcally, on the classic model DeiT-S [28], the
top-1 accuracy degradation of our pruned models is con-
trolled within 0.2% when the computation costs decreases
by 35%. In addition, the superiority of our method is more
obvious at lower keep-rates. When the compression ratio
of DeiT-S is increased to 50%, we improve 0.5% compared
to the best counterpart. In particular, the compression ratio
of our method is close to 40% on the DeiT-T [28] model,
and the accuracy is even better than the original model.
Owing to the token diversity and importance are orthog-
onal for token pruning, our method can be plugged into
EViT and achieve a performance improvement of 0.1%. As
shown in Table 2, we further conduct experiments on the
deep-narrow transformer LV-ViT [15]. We observe that our
method achieves better accuracy-complexity trade-offs on
different keep rates compared to the current foremost CNN
and ViT architectures.
Performance of existing methods on each keep rate. As
shown in Figure 6, our method consistently achieves the
best performance while the other two methods obtain close
performance. In addition, with the decrease of keep rate,
7

--- PAGE 8 ---
Strategy Acc (%) FLOPs (G)
Deit-S 79.8 4.6
DeiT-S/=0.7
+ attentive token preservation 79.3 3.0
+ inattentive token pack 79.3 3.0
+ inattentive token clustering 79.5 3.0
+ attentive token matching 79.6 3.0
DeiT-S/=0.5
+ attentive token preservation 78.2 2.3
+ inattentive token pack 78.4 2.3
+ inattentive token clustering 78.8 2.3
+ attentive token matching 79.0 2.3
Table 3. Ablation study on our method with different keep-rate .
the classiÔ¨Åcation accuracy of existing token pruning meth-
ods drops sharply. Fortunately, our method alleviates the
phenomenon by preserving the diversity of token seman-
tic information. Especially when the FLOPs of DyViT is
reduced to 1.6G, the classiÔ¨Åcation accuracy drops by more
than 10%. This is because completely discarding inattentive
tokens greatly decreases token diversity, resulting in the re-
duction of the semantic information of the original token
sequence. We apply the token decoupling and merging to
simultaneously consider the token importance and diversity,
achieving incredible accuracy at low keep rates. Intuitively,
when we only keep a few tokens, merging tokens obviously
makes more sense than keeping only top-K attentive tokens.
Visualization of the token merger results. To further
show the interpretability of our method, we visualize the
Ô¨Ånal token merger results back to its original input patches
in Figure 5. We notice that our method pays attention to
the regions that contribute more to image prediction instead
of uninformative backgrounds. e.g., the animal‚Äôs Ô¨Åve sense
organs are preserved. It demonstrates that our method effec-
tively decouple the attentive and inattentive tokens. Unlike
other methods that mask all inattentive tokens, our method
combines background patches with similar semantics. e.g.,
the animal‚Äôs fur is merged into a single token. It implies that
our method not only focuses on attentive tokens but also
maintains the diversity of token semantics. It is also worth
pointing out that the paired eyes are matched and combined
into a token in the Ô¨Åfth and sixth image. It indicates that our
method effectively fuse homogeneous attentive tokens and
reduces the potential redundancy.
5.3. Ablation Analysis
Effectiveness of each module. As shown in Table 3, we
add the sub-modules one by one to evaluate the effective-
ness of each module. i) Attentive token preservation. Dis-Method Acc FLOPs Throughput
(%) (G) (img/s)
Pooling strategy
Average pooling 78.1 2.3 1630
Max pooling 78.1 2.3 1623
Spatial pooling 78.2 2.3 1605
Sub-sampling strategy
Convolution 78.2 2.3 1454
MLP 78.3 2.3 1447
Cluster strategy
K-means(1 iter) 78.6 2.3 1386
K-means(3 iter) 78.8 2.3 1231
Ours 79.0 2.3 1670
Table 4. Different token merger strategies on DeiT-S.
card inattentive tokens based on the class token attention in
pruning layers; ii) Inattentive token pack. Pack all inatten-
tive tokens into one token; iii) Inattentive token clustering.
Cluster inattentive tokens and combine the tokens of the
same group into a new token; iv) Attentive token matching.
Match attentive tokens and combine the tokens of the same
pair into a new token; It is evident that since the clustering
module preserves token diversity, the accuracy is improved
by 0.2% and 0.8% at keep rates of 0.7 and 0.5, respectively.
Noteworthy, the lower keep rates, the better our method
works. In addition, the attentive token matching module
further reduces the FLOPs of the model while maintaining
accuracy by fusing homogeneous attentive tokens.
Different Token Merger Strategy. As presented in Ta-
ble 4, we compare several common token merging strate-
gies to assess the effectiveness of our approach. i) Pooling
strategy. Utilize the pooling operation to reduce the num-
ber of tokens. ii) Sub-sampling strategy. Adding a series
of convolution layers between MHSA and FFN to decrease
the token dimension. iii) Cluster strategy. Cluster the to-
kens and combine the tokens of the same group into a new
token. We observe that the clustering strategy generally
improves the accuracy by 0.4% compared to other token
merging strategies. A possible reason is that the cluster-
ing strategy obtains more inductive bias at the same com-
putational cost. However, we Ô¨Ånd that the throughput of
K-means algorithm is lower, indicating that it does not per-
form well in terms of model acceleration in practice. Fur-
thermore, we discover that the throughput of the K-means
algorithm decreases with the number of iterations. There-
fore we simplify an efÔ¨Åcient DPC algorithm with neither an
iterative process nor more parameters, which outperforms
other strategies on both accuracy and efÔ¨Åciency.
8

--- PAGE 9 ---
6. Conclusion
In this paper, we propose a token decoupling and merg-
ing method to simultaneously consider the token impor-
tance and diversity. Since token importance and diversity
are orthogonal for token pruning, our method can be em-
ployed into exisiting token pruning methods to further im-
prove the performance. We demonstrate that our method
achieved the SOTA performance trade-off between accu-
racy and FLOPs without imposing extra parameters. We
hope that this paper, which incorporates token importance
and diversity, will provide insights for the future work of
pruning visual transformers.
References
[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In European confer-
ence on computer vision , pages 213‚Äì229. Springer, 2020. 1,
3
[2] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda.
Crossvit: Cross-attention multi-scale vision transformer for
image classiÔ¨Åcation. In Proceedings of the IEEE/CVF in-
ternational conference on computer vision , pages 357‚Äì366,
2021. 7
[3] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang,
and Zhangyang Wang. Chasing sparsity in vision transform-
ers: An end-to-end exploration. Advances in Neural Infor-
mation Processing Systems , 34:19974‚Äì19988, 2021. 6
[4] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-
aolin Wei, Huaxia Xia, and Chunhua Shen. Conditional po-
sitional encodings for vision transformers. arXiv preprint
arXiv:2102.10882 , 2021. 7
[5] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen.
Up-detr: Unsupervised pre-training for object detection with
transformers. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 1601‚Äì
1610, 2021. 1, 3
[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248‚Äì255. Ieee, 2009. 2, 6
[7] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
Attention is not all you need: Pure attention loses rank dou-
bly exponentially with depth. In International Conference
on Machine Learning , pages 2793‚Äì2803. PMLR, 2021. 4
[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 1, 2, 3, 7
[9] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and
Qiang Liu. Vision transformers with patch diversiÔ¨Åcation.
arXiv preprint arXiv:2104.12753 , 2021. 1, 4[10] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron,
Pierre Stock, Armand Joulin, Herv ¬¥e J¬¥egou, and Matthijs
Douze. Levit: a vision transformer in convnet‚Äôs clothing for
faster inference. In Proceedings of the IEEE/CVF interna-
tional conference on computer vision , pages 12259‚Äì12269,
2021. 1, 3
[11] Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang, and En-
hua Wu. Vision gnn: An image is worth graph of nodes.
arXiv preprint arXiv:2206.00272 , 2022. 1, 4
[12] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing
Xu, and Chang Xu. Ghostnet: More features from cheap
operations. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 1580‚Äì1589,
2020. 1, 4
[13] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,
and Yunhe Wang. Transformer in transformer. Advances
in Neural Information Processing Systems , 34:15908‚Äì15919,
2021. 1, 4
[14] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie
Jin, Anran Wang, and Jiashi Feng. Token labeling: Training
a 85.5% top-1 accuracy vision transformer with 56m param-
eters on imagenet. arXiv preprint arXiv:2104.10858 , 3(6):7,
2021. 7
[15] Zi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun
Shi, Xiaojie Jin, Anran Wang, and Jiashi Feng. All tokens
matter: Token labeling for training better vision transform-
ers. Advances in Neural Information Processing Systems ,
34:18590‚Äì18602, 2021. 2, 6, 7
[16] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei
Niu, Mengshu Sun, Bin Ren, Minghai Qin, Hao Tang, and
Yanzhi Wang. Spvit: Enabling faster vision transformers via
soft token pruning. arXiv preprint arXiv:2112.13890 , 2021.
6
[17] Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima
Anandkumar, Jose M Alvarez, Ping Luo, and Tong Lu.
Panoptic segformer: Delving deeper into panoptic segmen-
tation with transformers. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1280‚Äì1289, 2022. 1
[18] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song,
Jue Wang, and Pengtao Xie. Not all patches are what you
need: Expediting vision transformers via token reorganiza-
tions. arXiv preprint arXiv:2202.07800 , 2022. 1, 3, 6
[19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 10012‚Äì10022, 2021. 1, 7
[20] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan,
Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. Adavit:
Adaptive vision transformers for efÔ¨Åcient image recognition.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 12309‚Äì12318, 2022.
3
[21] Bowen Pan, Rameswar Panda, Yifan Jiang, Zhangyang
Wang, Rogerio Feris, and Aude Oliva. Ia-red2:
Interpretability-aware redundancy reduction for vision trans-
9

--- PAGE 10 ---
formers. Advances in Neural Information Processing Sys-
tems, 34:24898‚Äì24911, 2021. 1, 3, 6
[22] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,
Kaiming He, and Piotr Doll ¬¥ar. Designing network design
spaces. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 10428‚Äì10436,
2020. 7
[23] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie
Zhou, and Cho-Jui Hsieh. Dynamicvit: EfÔ¨Åcient vision
transformers with dynamic token sparsiÔ¨Åcation. Advances
in neural information processing systems , 34:13937‚Äì13949,
2021. 1, 3, 6
[24] Alex Rodriguez and Alessandro Laio. Clustering by fast
search and Ô¨Ånd of density peaks. science , 344(6191):1492‚Äì
1496, 2014. 2, 5
[25] Michael S Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa
Dehghani, and Anelia Angelova. Tokenlearner: What can
8 learned tokens do for images and videos? arXiv preprint
arXiv:2106.11297 , 2021. 1, 4
[26] Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan
Guo, Chao Xu, and Dacheng Tao. Patch slimming for ef-
Ô¨Åcient vision transformers. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 12165‚Äì12174, 2022. 4, 6
[27] Yehui Tang, Kai Han, Chang Xu, An Xiao, Yiping Deng,
Chao Xu, and Yunhe Wang. Augmented shortcuts for vision
transformers. Advances in Neural Information Processing
Systems , 34:15316‚Äì15327, 2021. 1, 4
[28] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ¬¥e J¬¥egou. Training
data-efÔ¨Åcient image transformers & distillation through at-
tention. In International Conference on Machine Learning ,
pages 10347‚Äì10357. PMLR, 2021. 1, 2, 3, 6, 7
[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 1, 5
[30] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 568‚Äì578, 2021. 1, 7
[31] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,
Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing
convolutions to vision transformers. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 22‚Äì31, 2021. 1, 3
[32] Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander
Madry. Noise or signal: The role of image backgrounds in
object recognition. arXiv preprint arXiv:2006.09994 , 2020.
2, 4
[33] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,
Jose M Alvarez, and Ping Luo. Segformer: Simple and
efÔ¨Åcient design for semantic segmentation with transform-
ers. Advances in Neural Information Processing Systems ,
34:12077‚Äì12090, 2021. 1[34] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-
scale conv-attentional image transformers. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 9981‚Äì9990, 2021. 7
[35] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke
Li, Weiming Dong, Liqing Zhang, Changsheng Xu, and
Xing Sun. Evo-vit: Slow-fast token evolution for dynamic
vision transformer. In Proceedings of the AAAI Conference
on ArtiÔ¨Åcial Intelligence , 2022. 1, 2, 3, 6
[36] Huanrui Yang, Hongxu Yin, Pavlo Molchanov, Hai Li, and
Jan Kautz. Nvit: Vision transformer compression and param-
eter redistribution. arXiv preprint arXiv:2110.04869 , 2021.
1
[37] Hongxu Yin, Arash Vahdat, Jose M Alvarez, Arun Mallya,
Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for
efÔ¨Åcient vision transformer. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 10809‚Äì10818, 2022. 1, 3, 6
[38] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,
Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng
Yan. Tokens-to-token vit: Training vision transformers from
scratch on imagenet. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 558‚Äì567,
2021. 1, 3, 6, 7
10
