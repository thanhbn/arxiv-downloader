# 2203.16965.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2203.16965.pdf
# File size: 1463533 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PADA: PRUNING ASSISTED DOMAIN ADAPTATION FOR SELF-SUPERVISED SPEECH
REPRESENTATIONS
Vasista Sai Lodagala1, Sreyan Ghosh2, S. Umesh1
1Indian Institute of Technology, Madras
2University of Maryland, College Park
ABSTRACT
While self-supervised speech representation learning (SSL)
models serve a variety of downstream tasks, these models
have been observed to overﬁt to the domain from which the
unlabeled data originates. To alleviate this issue, we pro-
pose PADA (Pruning Assisted Domain Adaptation). Before
performing the target-domain ASR ﬁne-tuning, we discover
the redundant weights from pre-trained wav2vec 2.0 models
through various pruning strategies. We investigate the effect
ofTask-Agnostic andTask-Aware pruning and propose a new
pruning paradigm called, Cross-Domain Task-Aware Pruning
(CD-TAW). CD-TAW obtains the initial pruning mask from a
well ﬁne-tuned out-of-domain (OOD) model, thereby making
use of the readily available ﬁne-tuned models from the web.
The proposed CD-TAW method achieves up to 20.6% rela-
tive WER improvement over our baseline when ﬁne-tuned on
a 2-hour subset of Switchboard data without language model
(LM) decoding.
Index Terms —domain adaptation, pruning, self-supervised
learning, automatic speech recognition, telephone speech
1. INTRODUCTION
Over the past decade, Automatic Speech Recognition (ASR)
has drawn the attention of researchers from various ﬁelds
owing to its potential applications in various Natural Lan-
guage Understanding (NLU) systems having speech as the
primary modality of communication [1, 2, 3, 4]. The advent
of Deep Neural Networks (DNNs) has pushed the state-of-
the-art (SOTA) in speech recognition in a variety of settings
[5, 6]. However, DNNs are resource-hungry, and building
efﬁcient ASR systems requires a lot of compute and supervi-
sion in the form of labeled data. Thus, recent Self-Supervised
Learning (SSL) approaches [6, 7, 8, 9] which can learn rep-
resentations from unlabeled audio data directly, have been
gaining much traction. The primary aim of SSL is to use raw
speech [6], or other low-level features like Filter Banks [10],
to learn high-level representations that prove to be effective
in other downstream speech processing tasks. SSL has shown
considerable performance boosts in building ASR systems,
especially in settings where labeled data is scarce (as low as10 minutes), and has been known to generalize better than
supervised learning.
However, several recent studies have highlighted the
drawbacks of SSL. Firstly, the pretext tasks that the system
is subjected to solve under the SSL paradigm are compute-
intensive [6] and require a lot of unlabeled data. Secondly, a
recent study reveals that similar to supervised learning, SSL
too gets biased to the domain from which the unlabeled data
originates [11]. Thirdly, as SSL implicitly learns a language
model and other semantic information through the tasks it is
subjected to solve [12], the generalizability of these models
is only to the extent where data from a similar language or
phonetic structure is introduced to it at the ﬁne-tuning stage.
Thus, as correctly pointed out by [13], SSL for speech suf-
fers from scale problems, and SSL generalizability can be
improved with more efﬁcient training procedures. Prior work
for domain adaptation with self-supervised models mostly
employ continued pre-training orcombined data pre-training
approaches [11]. However, both assume the existence of
high-resource unlabeled target-domain data, which is not
always the case in a real-world scenario (for example, tele-
phonic conversational speech is very difﬁcult to procure due
to privacy issues and not more than 1000 hours is freely
available online).
Building on the second and third problems mentioned
above, in this paper, we try to address the problem of do-
main bias in pre-trained SSL models and try to devise an
algorithm, that can allow pre-trained models trained on OOD
data to easily adapt to the target domain and with improved
performance, using only the supervised In-Domain data.
To achieve this objective, we take the help of Unstructured
Magnitude Pruning (UMP), wherein we select model param-
eters of “least magnitude”, which we hypothesize to be of
“least importance” to the domain, and propose to zero them
out so that weights that are important for the downstream
task should emerge with gradient updates, and those that are
irrelevant should decrease in magnitude. This was ﬁrst em-
pirically proven in [14]. The zeroed-out parameters are also
kept trainable, which makes this different from generic DNN
pruning, where the pruning mask does not allow gradient
updates. More details about the different pruning strategies
can be found in Alg.1. We are motivated by the ﬁndings
978-1-6654-7189-3/22/$31.00 ©2023 IEEEarXiv:2203.16965v4  [cs.CL]  13 May 2023

--- PAGE 2 ---
Fig. 1 . The white blocks represent the weights that have been
zeroed out.
from the Lottery Ticket Hypothesis [15], which suggests that
a randomly initialized network contains sparse sub-networks
which can be trained in isolation to match the result of a full
model. Also, [16] explores pruning for monolingual language
adaptation in multilingual pre-trained models. To summarize,
our main contributions are as follows:
• We analyze the performance of different pruning strate-
gies and frequencies for domain adaptation on pre-
trained speech SSL models. We base our experiments
on the practical assumption that only limited amounts
of target-domain labeled data is available, and no other
large-scale unlabeled corpus is available from the target
domain.
• We propose Cross-Domain Task-Aware pruning (CD-
TAW), a ﬁrst-of-its-kind method that uses readily avail-
able models ﬁne-tuned on high-resource labeled OOD
data to obtain the initial pruning mask relevant to the
downstream ASR task. Experimental results reveal
that CD-TAW performs better than generic ﬁne-tuning,
TAG, and TAW approaches.
2. RELATED WORK
2.1. Self-Supervised Speech Representation Learning
SSL for speech representation learning has been explored
primarily under 3 main paradigms, each solving a form of
Masked Acoustic Modelling (MAM). The ﬁrst and the most
common in this space is based on Contrastive Predictive Cod-
ing (CPC), which minimizes the InfoNCE loss [17, 6]. Thesecond paradigm learns by minimizing the Cross-Entropy
loss, wherein the primary task is to predict the correct pseudo-
label assigned to a frame, masked at the input to the model,
leveraging the contextualized embedding of that frame ob-
tained from a transformer encoder [8, 9]. The third and
the ﬁnal paradigm solves the reconstruction-based objective
function [10, 18].
2.2. Pruning
The concept of pruning DNNs has been extensively studied in
the past [15, 19, 20]. Authors of [21] show that large sparse
models obtained through pruning generally perform better
when compared to smaller but dense models. One of the ﬁrst
works that proposed pruning for domain adaptation was by
[22]. However, they propose an entirely different methodol-
ogy using knowledge distillation and employ it for text-based
Neural Machine Translation (NMT). [16] propose a pruning
approach to improve monolingual ASR performance in mul-
tilingual SSL pre-trained models. Authors of PARP [14] pro-
posed the ﬁrst work on pruning large self-supervised speech
pre-trained models. They intend to ﬁnd a sparse ﬁne-tuned
sub-network at some target sparsity level, with improved
performance than using a full model. However, our work
differs from theirs primarily from the perspective of the ﬁnal
task we try to solve. We approach iterative UMP [15] from
a domain adaptation perspective and devise our algorithm
by hypothesizing accordingly. Our end goal is to adapt the
pre-trained SSL model to the target-domain, and the focus is
not on obtaining a sparse model. One key difference from
PARP is that our task-aware pruning strategies work better
for domain adaptation. At the same time, PARP resorts to
task-agnostic pruning strategies as they do not observe ma-
jor differences between the two, given that they ﬁne-tune on
the data from the same domain as the pre-trained model’s
unlabeled data. Another key difference from PARP is that,
while they progressively increase the pruning percentage to
achieve the target sparsity in PARP-P, we dynamically de-
cay the pruning percentage for better domain adaptation as
explained in Section 3.2.4.
2.3. Domain Adaptation
Domain Adaptation (DA) for building efﬁcient ASR systems
has been a well-studied topic in literature, with early work
focusing on regularization [23, 24], teacher-student learning,
[25, 26] or adversarial learning [27, 28]. Lately, unsupervised
domain adaptation of ASR models has been gaining traction,
and researchers have been trying to ﬁnd ways to use huge
amounts of unlabeled data from the target domain for domain
adaptation [26, 29, 30]. Continued pre-training is another
common approach used [31]. We emphasize that our work
is one of the ﬁrst to approach domain adaptation from a prun-
ing perspective, involving no continued pre-training or OOD
unlabeled data.

--- PAGE 3 ---
3. PROPOSED METHODOLOGY
3.1. Problem Formulation
Suppose we have a high-resource OOD unlabeled dataset P
and a medium-to-high resource OOD labeled dataset J, both
from domain D1. We also have a low-resource labeled dataset
L. Letp()be the neural network pre-trained using self-
supervision on the dataset Pfrom domain D1and letf(j)
represent the resultant neural network after CTC based ASR
ﬁne-tuning has been performed on p()on the dataset Jfrom
domainD1, such that,j2Rd, represents the dnumber of
network parameters. As a part of this work, the models p()
andf(j)that we use are made available in the public domain
through the works on wav2vec-2.0 and fairseq [6, 32].
Our primary aim is to adapt our model p(), to domainD2
by just using the low-resource dataset L, with performance
better than generic ﬁne-tuning of p()onL. We achieve this
objective by identifying the weights having the least impor-
tance and zeroing them out. Weights relevant for the down-
stream ASR task on Lwould emerge with gradient updates,
and those irrelevant are zeroed out to facilitate better adap-
tation of the model. As we aim to get a model that delivers
better performance without on any focus on achieving a target
sparsity, weights that have been zeroed out remain trainable
onL, thereby making use of the entire model capacity.
3.2. Pruning Assisted Domain Adaptation (PADA)
The generic pruning strategy in modern deep learning toolk-
its involves obtaining a mask Mfor a subset of model param-
eters, which intuitively results in multiplying by a value of
zero in the forward pass step. Additionally, the mask does
not allow gradient updates for these parameters during the
backward pass. As part of this work, we explore different
pruning strategies based on UMP. The focus of PADA is to
discover the “least important weights” in a pre-trained SSL
modelp(). Unlike generic pruning, PADA does not retain
the mask and lifts the mask after a round of UMP, followed
by zeroing out the identiﬁed parameters, and keeps the pa-
rameters trainable. PADA can be achieved in multiple ways,
primarily differing in the procedure of obtaining the initial
pruning masks at the beginning of training and the rate of
pruning employed at every training iteration. Fig. 1 illustrates
the three major pruning strategies. In the following four sub-
sections, we brieﬂy discuss the pruning strategies involved in
PADA and the algorithm to implement the same.
3.2.1. Task-Agnostic Pruning (TAG)
In this approach we perform UMP at a pruning percentage
r1directly onp(), and ther1percent of weights with the
least magnitude are zeroed out, resulting in the model p(0).
This approach does not consider the downstream ASR ﬁne-
tuning thatp()would be subjected to. Weights that havebeen zeroed out are identiﬁed to be of least importance in
terms of magnitude, only based on the pre-training task. One
must note that pre-training is used to learn representations that
serve multiple downstream tasks, and ﬁne-tuning the model
on speciﬁc downstream tasks and datasets could lead to a dif-
ferent set of weights gaining importance.
3.2.2. Task-Aware Pruning (TAW)
The Task-Aware Pruning strategy aims to zero out those
weights from p(), that are of least importance speciﬁc to
the downstream ASR task performed on the dataset L. To
this end, we ﬁrst ﬁne-tune p()onLwhich results in the
ﬁne-tuned model p(f). We then perform UMP at a pruning
percentager1onp(f), to obtain the mask M. The mask
Mcarries the information about the least important weights
whenp()is exposed to the ASR downstream task on the
dataset L. The exact same weights speciﬁed by the mask M
are zeroed out from p(), resulting in the model p(0).
3.2.3. Cross-Domain Task-Aware Pruning (CD-TAW)
While TAW focuses on identifying and zeroing the weights of
least importance speciﬁc to the downstream ASR task on the
dataset L, it is evident that such an approach requires ﬁne-
tuning twice on the dataset L(once to obtain the mask and
the second time to ﬁne-tune the pre-trained model carrying r1
percent of zeroed weights). To alleviate this compute-heavy
issue, we propose a method where UMP is performed at a
pruning percentage r1onf(j), to obtain the mask M. Sim-
ilar to TAW, the exact same weights speciﬁed by the mask M
are zeroed out from p(), resulting in the model p(0). The
advantage of using f(j)to obtain the mask Mis that, Jis a
relatively high resource dataset (though OOD) in comparison
with L. This results in the ﬁne-tuning mask being more suited
to the downstream ASR task, as f(j)has seen more “task”
related data. The reason behind mentioning that CD-TAW
requires only one round of ﬁne-tuning is that, when f(j)is
readily available online, it saves the explicit ﬁne-tuning on the
downstream ASR task to obtain a task-aware mask.
3.2.4. Algorithm
Alg. 1 shows the algorithm of PADA where we describe
PADA with notations from earlier subsections.
We ﬁx the initial pruning percentage r1. We then choose
the pruning frequency Pfreq from the 3 variants (Once / It-
erative / Dynamic Iterative). We also ﬁx the total number of
ﬁne-tuning updates to N.
When an Iterative PADA is used, frequency ricorre-
sponds tor1=r1=r2==rk. However, when the Dy-
namic Iterative PADA is chosen, r1> r 2> r 2>> rk.
Dynamic Iterative PADA has the fundamental advantage in
which fewer weights are zeroed out in the future training
iterations where the model has “already been trained on the

--- PAGE 4 ---
Algorithm 1 Pruning Assisted Domain Adaptation (PADA)
1:Based on the pruning strategy chosen (TAG / TAW / CD-
TAW), obtain the model p(0)after zeroing out r1per-
centage of weights.
2:ifPfreq is Once then
3: Fine-tunep(0)onLforNmodel updates to obtain
the ﬁnal domain adapted model p(N).
4:else ifPfreq is Iterative or Dynamic Iterative then
5: Choose the number of ﬁne-tune updates nafter which
you wish to re-prune and zero out some model weights.
6: Fix the pruning percentages r2;r3;;rk.
7: Setupdates = 0;n1 = 0 andu= 1.
8: whileupdatesNdo
9: Trainp(n1)onL, fornupdates to obtain p(n2).
10: Setn1 =n2;updates =updates +n.
11: ifupdatesNandu<k then
12: Perform UMP on p(n1)at a pruning percent-
ageruand zero out the pruned weights.
13: Setu=u+ 1.
14: end if
15: end while
16:end if
target domain” and needs “lesser space” to adapt. Precisely,
each of the pruning strategies mentioned in sections 3.2.1,
3.2.2, 3.2.3 differ only in step 1 of the PADA algorithm.
However, it is worth mentioning that step 1 of PADA is the
most important step, which makes all the difference in terms
of performance.
4. EXPERIMENTAL SETUP
4.1. Datasets and Pre-trained models
The pre-trained models which we choose for PADA are:
•wav2vec-2.0 LV-60k : A LARGE model [6] pre-trained
on 60k hours of Libri-Light audio data.
•wav2vec-2.0 LS-960 : A BASE model [6] pre-trained
on 960 hours of LibriSpeech audio data.
•XLSR-53 : A LARGE model pre-trained on 56k hours
of data from 53 different languages [33]. The datasets
used include Multilingual LibriSpeech (MLS), Com-
monV oice (CV), and Babel.
The target domain datasets we choose for PADA are:
•Switchboard data : It is a corpus of telephonic speech
conversations [34], a domain completely different from
the Libri-Light data which is made up of read speech
from audiobooks [35]. We choose two subsets of the
Switchboard data for our experiments, one of 30 hours
and another of 2 hours, representing the low-resource
Fig. 2 . Comparison of various pruning strategies and frequen-
cies on the wav2vec-2.0 large LV-60k model.
and the extreme low-resource settings of domain adap-
tation. We report the word error rate (WER) results on
the Switchboard dev set.
•Hindi Challenge Data : It is a corpus of Hindi read
speech data released as a part of the Hindi ASR Chal-
lenge1, which has 50 hours of speech data sourced from
varied domains like politics, sports, entertainment etc.
We choose a 7-hour subset of this data for the purpose
of domain adaptation and report the character error rate
(CER) results on the evaluation set released as a part of
the challenge.
4.2. Fine-tuning conﬁguration
The baseline approach chosen for comparison with PADA is
the one where we directly ﬁne-tune the pre-trained model on
the target domain dataset L. This approach is referred to as
Direct Fine-tuning (DFT), the baseline in Table 1.
We add a task-speciﬁc linear layer on top of the model
and jointly ﬁne-tune with CTC loss. As zeroing out weights
is involved in PADA, we ﬁne-tune the weights during each
iteration on both DFT and PADA with no freezing of layers
or updates being involved. The Nandnvalues used in the
Alg.1 are mentioned in the Table 1.
Extensive experimentation on the 30h Switchboard data,
as depicted in Fig. 2, helps us conclude with the best settings
to go for, with the different pruning frequencies. The LARGE
models use r1= 40 ,r1=r2=r3= 30 ,r1= 40;r2=
20;r3= 10 whereas the BASE models use r1= 30 ,r1=r2
1https://sites.google.com/view/asr-challenge

--- PAGE 5 ---
Table 1 . Performance of Direct Fine-tuning (DFT) compared to PADA with TAG, TAW, and CD-TAW.
Pre-trained model DFT performance Pruning Strategy Performance at different pruning frequencies
Once Iterative Dynamic Iterative
Finetuning on 30h Switchboard (WER)
wav2vec-2.0 LV-60k
N= 27000
n= 2400TAG 12.1 12.0 12.1
11.8 TAW 11.8 11.7 11.8
CD-TAW(LS-100h) 11.4 11.4 11.3
CD-TAW(LS-960h) 11.1 11.2 11.0
Finetuning on 2h Switchboard (WER)
wav2vec-2.0 LV-60k
N= 10000
n= 1000TAG 22.4 22.2 22.5
22.2 TAW 21.2 21.3 21.3
CD-TAW(LS-100h) 19.0 19.1 18.9
CD-TAW(LS-960h) 17.6 18.2 17.6
wav2vec-2.0 LS-960
N= 10000
n= 1000TAG 29.8 29.9 29.8
29.2 TAW 29.7 29.7 29.8
CD-TAW(LS-100h) 27.4 27.3 27.3
Finetuning on 7h Hindi data (CER)
wav2vec-2.0 LV-60k
N= 9000
n= 1000TAG 18.7 14.1 13.6
16.3 TAW 13.2 13.4 13.2
CD-TAW(LS-100h) 13.2 13.4 13.0
XLSR-53
N= 9000
n= 1000TAG 12.0 11.9 12.0
11.8 TAW 11.4 11.5 11.3
CD-TAW(CV ,
Babel)12.0 11.9 12.0
=r3= 30 andr1= 30;r2= 25;r3= 20;r4= 10 for
theOnce ,Iterative andDynamic Iterative pruning frequen-
cies respectively. The rest of the parameters follow standard
conﬁgurations on fairseq [32] from the works of [6, 33] and
are made available on GitHub2 3.
5. RESULTS
As evident from Table 1, CD-TAW implemented with wav2vec-
2.0 LV-60k and the mask taken from 100hr LibriSpeech
ﬁne-tuning achieves a 20.6% relative WER improvement
on the 2h Switchboard ﬁne-tuning and 19.8% relative CER
improvement on the 7h Hindi data ﬁne-tuning.
6. RESULT ANALYSIS
6.1. Key observations
In this section we try to elaborate and discuss on some of our
key observations from our results in Table 1. They are as
follows:
•CD-TA W almost always outperforms TA W. This cor-
responds to the fact that being more “task-aware” is
more efﬁcient while making parameters free to adapt
to the new domain downstream than being “domain-
aware”.
2https://github.com/Speech-Lab-IITM/PADA
3Correspondence to Vasista Sai Lodagala: vasista.lodagala@gmail.com•Dynamic Iterative Pruning in most cases outper-
forms other ﬁxed pruning frequency approaches. This
re-conﬁrms our hypothesis that, for domain adapta-
tion, iteratively decreasing the pruning rate across ﬁne-
tuning iterations works well as the model gets more
adapted to the new domain with time.
•CD-TA W beneﬁts with larger OOD labeled datasets.
We base this conclusion from the Table 1 where CD-
TAW on 2h Switchboard beneﬁts more when the initial
mask Mis taken from the model ﬁne-tuned on 960h of
LibriSpeech than 100h. It may seem unfair to compare
masks coming from models ﬁne-tuned on few hours of
data with those ﬁne-tuned on hundreds of hours of data.
However, the point we are trying to emphasize is that,
using such readily available models ﬁne-tuned on large
amounts of OOD data, brings better task-awareness and
also avoids the need to ﬁne-tune the models multiple
times as in the case of TAW.
•Beyond domain adaptation, PADA also helps in
cross-language adaptation . This is very evident from
our experiments on the Hindi 7h labeled data. Us-
ing CD-TAW over TAG for ﬁne-tuning on our low-
resource Hindi dataset where the mask was taken from
a LirbiSpeech ﬁne-tuned model, gives us an absolute
improvement of 5.5% CER.
•Language supervision impacts CD-TA W perfor-
mance. On ﬁne-tuning the XLSR-53 model on the

--- PAGE 6 ---
Fig. 3 . Layerwise MMA and IOU for 2h Switchboard ﬁne-
tuning.
7h Hindi data we notice that CD-TAW under-performs
TAW as the amount of Hindi data in the OOD dataset
(CV and Babel) is minimal. This is also in-lines with
ﬁndings from [16].
6.2. Layer-wise Pruning Mask Comparison
Next, we try to ﬁnd the similarity between a mask that pro-
vides us with the best initial sub-network for ﬁne-tuning our
network on Switchboard using PADA, and all other masks
from our other experiments. As evident from Table 1, our pro-
posed CD-TAW provides the best initial sub-network mask
when the mask is obtained from a converged model ﬁne-tuned
on 960hrs of LibriSpeech. To compare the similarity between
masks, we use Intersection Over Union (IOU) and Mutual
Mask Agreement (MMA) as the similarity measures. IOU for
quantifying subnetwork masks, is deﬁned in [14] as follows:
IOU 
ma;mb
=(ma= 1)\ 
mb= 1
j(ma= 1)[(mb= 1)j(1)
wheremaandmbare the two masks we want to compare.
We ﬁnd that IOU scores are not completely reﬂective of
the similarity between masks and deﬁne MMA as follows:
Agg 1 
ma;mb
=(ma= 1)\ 
mb= 1
Agg 0 
ma;mb
=(ma= 0)\ 
mb= 0
MMA 
ma;mb
=Agg 1+ Agg 0
N(2)
whereNis the total number of weights in maormb.MMA provides a better comparison between masks com-
pared to IOU because, while IOU focuses only in the un-
masked regions, MMA measures the similarity between
masks in both the masked and unmasked regions. For in-
stance, letma= [1;0;1;0]and letmb= [1;1;0;0]. In this
case though the two masks are similar in the ﬁrst and last
positions, the IOU score turns out to be 0:33, while MMA
gives the desired score of 0:5.
Fig.3 depicts the Layer-wise MMA and IOU scores be-
tween our best sub-network mask and experiments from other
masks. As clearly evident, both IOU and MMA fall sharply
towards the last layers, which also learn the highest task-
speciﬁc information [12] (in our case, ASR task through CTC
ﬁne-tuning). Alternatively, our best-performing model, CD-
TAW, re-conﬁrms the hypothesis that irrespective of the do-
main, the more “task-aware” the model is, the better the initial
sub-network mask. Finally, though CD-TAW from a model
ﬁne-tuned on 100 hours performs worse than its 960hrs coun-
terpart, it shows the highest IOU and MMA values out of all
the masks, which again asserts that better masks can be ob-
tained from more “task-awareness”.
7. CONCLUSION AND FUTURE WORK
In this paper, we propose PADA, a novel paradigm for low-
resource ASR domain adaptation. Models pre-trained with
SSL on high amounts of unlabeled OOD data show signiﬁ-
cant improvements when ﬁne-tuned on ASR using the PADA
framework. As part of the future work, we would like to
explore if structured pruning and unsupervised SSL domain
adaptation can help boost the performance in this learning
paradigm.
8. ACKNOWLEDGEMENT
A part of this work was funded by “Bhashini: National Lan-
guage translation Mission” project of the Ministry of Elec-
tronics and Information Technology (MeitY), Government of
India.
9. REFERENCES
[1] Santosh K Gaikwad, Bharti W Gawali, and Pravin Yan-
nawar, “A review on speech recognition technique,” In-
ternational Journal of Computer Applications , vol. 10,
no. 3, pp. 16–24, 2010.
[2] Hermann Ney, “Speech translation: Coupling of recog-
nition and translation,” in IEEE ICASSP 1999 . IEEE,
1999, vol. 1, pp. 517–520.
[3] George Saon, Gakuto Kurata, Tom Sercu, Kartik Au-
dhkhasi, Samuel Thomas, Dimitrios Dimitriadis, Xi-
aodong Cui, Bhuvana Ramabhadran, Michael Picheny,

--- PAGE 7 ---
Lynn-Li Lim, et al., “English conversational telephone
speech recognition by humans and machines,” arXiv
preprint arXiv:1703.02136 , 2017.
[4] Hemant Yadav, Sreyan Ghosh, Yi Yu, and Rajiv Ratn
Shah, “End-to-end named entity recognition from en-
glish speech,” arXiv preprint arXiv:2005.11184 , 2020.
[5] Amodei et al., “Deep speech 2 : End-to-end speech
recognition in english and mandarin,” in ICML 2016 ,
pp. 173–182.
[6] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,
and Michael Auli, “wav2vec 2.0: A framework for self-
supervised learning of speech representations,” NeurIPS
2020 , vol. 33, pp. 12449–12460, 2020.
[7] Shu wen Yang, Po-Han Chi, Yung-Sung Chuang,
Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y . Lin, Andy T.
Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-
Hsien Huang, Wei-Cheng Tseng, Ko tik Lee, Da-Rong
Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji
Watanabe, Abdelrahman Mohamed, and Hung yi Lee,
“SUPERB: Speech Processing Universal PERformance
Benchmark,” in Interspeech 2021 , 2021, pp. 1194–
1198.
[8] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,
Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrah-
man Mohamed, “Hubert: Self-supervised speech rep-
resentation learning by masked prediction of hidden
units,” IEEE/ACM Transactions on Audio, Speech, and
Language Processing , vol. 29, pp. 3451–3460, 2021.
[9] Sanyuan Chen, Chengyi Wang, Zhengyang Chen,
Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki
Kanda, Takuya Yoshioka, Xiong Xiao, et al., “Wavlm:
Large-scale self-supervised pre-training for full stack
speech processing,” arXiv preprint arXiv:2110.13900 ,
2021.
[10] Andy T Liu, Shu-wen Yang, Po-Han Chi, Po-chun Hsu,
and Hung-yi Lee, “Mockingjay: Unsupervised speech
representation learning with deep bidirectional trans-
former encoders,” in IEEE ICASSP 2020 , pp. 6419–
6423.
[11] Wei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Ta-
tiana Likhomanenko, Qiantong Xu, Vineel Pratap, Jacob
Kahn, Ann Lee, Ronan Collobert, Gabriel Synnaeve,
and Michael Auli, “Robust wav2vec 2.0: Analyzing
Domain Shift in Self-Supervised Pre-Training,” in In-
terspeech 2021 , 2021, pp. 721–725.
[12] Ankita Pasad, Ju-Chieh Chou, and Karen Livescu,
“Layer-wise analysis of a self-supervised speech rep-
resentation model,” in 2021 IEEE Automatic SpeechRecognition and Understanding Workshop (ASRU) .
IEEE, 2021, pp. 914–921.
[13] Awni Hannun, “The history of speech recognition to the
year 2030,” arXiv preprint arXiv:2108.00084 , 2021.
[14] Cheng-I Jeff Lai, Yang Zhang, Alexander H. Liu, Shiyu
Chang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi Qian,
Sameer Khurana, David Cox, and James Glass, “Parp:
Prune, adjust and re-prune for self-supervised speech
recognition,” in NeurIPS 2021 .
[15] Jonathan Frankle and Michael Carbin, “The lottery
ticket hypothesis: Finding sparse, trainable neural net-
works,” in ICLR 2019 .
[16] Yizhou Lu, Mingkun Huang, Xinghua Qu, Pengfei
Wei, and Zejun Ma, “Language adaptive cross-lingual
speech representation learning with sparse sharing sub-
networks,” arXiv preprint arXiv:2203.04583 , 2022.
[17] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Rep-
resentation learning with contrastive predictive coding,”
arXiv preprint arXiv:1807.03748 , 2018.
[18] Andy T Liu, Shang-Wen Li, and Hung-yi Lee, “Tera:
Self-supervised learning of transformer encoder repre-
sentation for speech,” IEEE/ACM Transactions on Au-
dio, Speech, and Language Processing , vol. 29, pp.
2351–2366, 2021.
[19] Trevor Gale, Erich Elsen, and Sara Hooker, “The state
of sparsity in deep neural networks,” arXiv preprint
arXiv:1902.09574 , 2019.
[20] Song Han, Jeff Pool, John Tran, and William Dally,
“Learning both weights and connections for efﬁcient
neural network,” NeurIPS 2015 , vol. 28, 2015.
[21] Suyog Gupta Michael H. Zhu, “To prune, or not to
prune: Exploring the efﬁcacy of pruning for model com-
pression,” in ICLR 2018 .
[22] Shuhao Gu, Yang Feng, and Wanying Xie, “Pruning-
then-expanding model for domain adaptation of neural
machine translation,” in NAACL 2021 .
[23] Dong Yu, Kaisheng Yao, Hang Su, Gang Li, and Frank
Seide, “Kl-divergence regularized deep neural network
adaptation for improved large vocabulary speech recog-
nition,” in IEEE ICASSP 2013 , pp. 7893–7897.
[24] Hank Liao, “Speaker adaptation of context dependent
deep neural networks,” in IEEE ICASSP 2013 , pp.
7947–7951.
[25] Zhong Meng, Jinyu Li, Yifan Gong, and Biing-Hwang
Juang, “Adversarial teacher-student learning for unsu-
pervised domain adaptation,” in IEEE ICASSP 2018 ,
pp. 5949–5953.

--- PAGE 8 ---
[26] Vimal Manohar, Pegah Ghahremani, Daniel Povey, and
Sanjeev Khudanpur, “A teacher-student learning ap-
proach for unsupervised domain adaptation of sequence-
trained asr models,” in IEEE SLT Workshop 2018 , pp.
250–257.
[27] Yusuke Shinohara, “Adversarial multi-task learning of
deep neural networks for robust speech recognition.,” in
Interspeech 2016 , pp. 2369–2372.
[28] Zhong Meng, Jinyu Li, Zhuo Chen, Yang Zhao,
Vadim Mazalov, Yifan Gong, and Biing-Hwang Juang,
“Speaker-invariant training via adversarial learning,” in
IEEE ICASSP 2018 , pp. 5969–5973.
[29] CS Anoop, AP Prathosh, and AG Ramakrishnan, “Un-
supervised domain adaptation schemes for building asr
in low-resource languages,” in IEEE ASRU Workshop
2021 .
[30] Dongseong Hwang, Ananya Misra, Zhouyuan Huo,
Nikhil Siddhartha, Shefali Garg, David Qiu, Khe Chai
Sim, Trevor Strohman, Franc ¸oise Beaufays, and
Yanzhang He, “Large-scale asr domain adaptation us-
ing self-and semi-supervised learning,” arXiv preprint
arXiv:2110.00165 , 2021.
[31] Suchin Gururangan, Ana Marasovi ´c, Swabha
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A. Smith, “Don’t stop pretraining: Adapt
language models to domains and tasks,” in ACL 2020 ,
pp. 8342–8360.
[32] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli, “fairseq: A fast, extensible toolkit for sequence
modeling,” in NAACL-HLT 2019: Demonstrations .
[33] Arun Babu, Changhan Wang, Andros Tjandra, Kushal
Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh,
Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei
Baevski, Alexis Conneau, and Michael Auli, “Xls-
r: Self-supervised cross-lingual speech representation
learning at scale,” arXiv , vol. abs/2111.09296, 2021.
[34] John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel, “Switchboard: Telephone speech corpus for re-
search and development,” in IEEE ICASSP 1992 , vol. 1,
pp. 517–520.
[35] Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny
Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazar ´e,
Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert,
Christian Fuegen, et al., “Libri-light: A benchmark for
asr with limited or no supervision,” in IEEE ICASSP
2020 , pp. 7669–7673.
