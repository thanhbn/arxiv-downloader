# 2111.00162.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2111.00162.pdf
# File size: 646384 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
You are caught stealing my winning lottery ticket!
Making a lottery ticket claim its ownership
Xuxi Chen1*, Tianlong Chen1*, Zhenyu Zhang2, Zhangyang Wang1
1University of Texas at Austin,2University of Science and Technology of China
{xxchen,tianlong.chen,atlaswang}@utexas.edu,zzy19969@mail.ustc.edu.cn
Abstract
Despite tremendous success in many application scenarios, the training and in-
ference costs of using deep learning are also rapidly increasing over time. The
lottery ticket hypothesis (LTH) emerges as a promising framework to leverage a
special sparse subnetwork (i.e., winning ticket ) instead of a full model for both
training and inference, that can lower both costs without sacriÔ¨Åcing the perfor-
mance. The main resource bottleneck of LTH is however the extraordinary cost to
Ô¨Ånd the sparse mask of the winning ticket. That makes the found winning ticket
become a valuable asset to the owners, highlighting the necessity of protecting its
copyright. Our setting adds a new dimension to the recently soaring interest in
protecting against the intellectual property (IP) infringement of deep models and
verifying their ownerships, since they take owners‚Äô massive/unique resources to
develop or train. While existing methods explored encrypted weights or predic-
tions, we investigate a unique way to leverage sparse topological information to
perform lottery veriÔ¨Åcation , by developing several graph-based signatures that can
be embedded as credentials. By further combining trigger set-based methods, our
proposal can work in both white-box and black-box veriÔ¨Åcation scenarios. Through
extensive experiments, we demonstrate the effectiveness of lottery veriÔ¨Åcation in
diverse models (ResNet-20, ResNet-18, ResNet-50) on CIFAR-10 and CIFAR-100.
SpeciÔ¨Åcally, our veriÔ¨Åcation is shown to be robust to removal attacks such as
model Ô¨Åne-tuning and pruning, as well as several ambiguity attacks. Our codes are
available at https://github.com/VITA-Group/NO-stealing-LTH .
1 Introduction
Deep neural networks (DNNs) have dramatically raised the state-of-the-art performance in various
Ô¨Åelds. However, the over-parameterization of DNNs becomes a non-negligible problem. The amount
of parameters now is often on the billion scale, which signiÔ¨Åcantly increases the inference cost when
using these models. An emerging Ô¨Åeld of lottery ticket hypothesis (LTH) explores a new scheme
for pruning the model without sacriÔ¨Åcing performance. The core idea is to identify the sparsity
pattern ahead of training (or in its early stage) and train a sparse network from scratch. It has been
hypothesized [ 1] that DNNs contain sparse networks named winning tickets that can be trained to
match the test accuracy of the full model. These winning tickets hence have comparable or even
better inference performance while potentially reducing the computational footprints.
However, Ô¨Ånding winning tickets is a non-trivial task: it involves the training-prune-retraining cycle
for several times [ 1], which is burdensome and computation-consuming. Although other works [ 2‚Äì4]
have shown that sparsity might emerge at the initialization or at the early stage of training, the iterative
magnitude pruning (IMP) still outperforms these alternatives by clear margins [ 5]. Yet, the powerful
IMP method requires multiple rounds of train-prune-train process on the original training set, which
*Equal Contribution.
35th Conference on Neural Information Processing Systems (NeurIPS 2021).arXiv:2111.00162v1  [cs.LG]  30 Oct 2021

--- PAGE 2 ---
is even much more expensive than training a dense network. That makes a found winning ticket a
valuable asset to the owners, highlighting the necessity of protecting the winning tickets‚Äô copyright.
Embed
Original Mask  Mask w . Signatur e
Figure 1: Illustration of embedding signa-
tures into the original sparse mask. These
visualizations are projected from 4D tensors.
Dark entries are pruned elements. Note that
the actual sparsity of the subnetwork is un-
changed after encoding credentials.Previous works [ 6‚Äì9] have shown that deep networks
are vulnerable to intellectual property (IP) infringement.
For example, one can use transfer learning to adapt a
trained model onto a new task or use model compres-
sion techniques to create a new sparse model based on
the target model. Fortunately, in recent years the own-
ership veriÔ¨Åcation problem has been addressed with a
number of solutions proposed. The key idea is to embed
veriÔ¨Åable information, or called signatures , into models‚Äô
weights [ 6,10,11] or predictions [ 7] without visibly affect-
ing the original performance. By extracting the embedded
information from models, one can verify the ownership
of models and hence protect their IPs. For the methods
that embed information in weights, additional weights reg-
ularizers are often used to enforce certain patterns, such as signs. As for the prediction methods, a
special training set, which is often called a trigger set, is used as additional training data. The model
trained upon both the original data and the trigger set can generate desired prediction labels for the
privately-held trigger set, while preserving the performance on the original training set. However,
those general methods did not take any structural property(e.g., sparsity) into account, leaving chance
for improving their gains in the winning ticket mask protection.
On protecting the IP of winning tickets, we investigate a novel way to leverage sparse structural
information for ownership veriÔ¨Åcation (Fig. 1). This structural information embedded in winning
tickets is a good "credential" for ownership veriÔ¨Åcation since the winning ticket at extreme sparsity is
naturally robust to Ô¨Åne-tuning and (further) pruning attacks. The winning ticket at extreme sparsity
cannot be pruned further; otherwise, the inference performance will drop (hence losing its "value").
Meanwhile, Ô¨Åne-tuning the winning tickets can only tune the weights, but the sparsity pattern will not
be changed. However, there remain some key questions to answer: How to formulate the ownership
veriÔ¨Åcation process under the context of the lottery ticket hypothesis? What kind of structural
information should be used? How to inject user-speciÔ¨Åc information into the structure of winning
tickets? We present answers to these questions in this paper. We summarize our Ô¨Åndings as follows:
‚Ä¢We formulate the lottery veriÔ¨Åcation problem and deÔ¨Åne two different protection scenarios. We
show that even without speciÔ¨Åc protection, the extremely sparse winning ticket can partly claim its
ownership because of the critical role of its sparse structure in the Ô¨Ånal inference performance.
‚Ä¢We further propose a new mask embedding method that is capable of embedding ownership
‚Äúsignatures" in the subnetwork‚Äôs sparse structural connectivity (Fig. 1), without much affecting its
performance. The signature is robust, e.g., it can be extracted and decoded even after pruning or
Ô¨Åne-tuning attacks. Combined further with the trigger set-based method, our mask embedding
method can work under both white-box and black-box veriÔ¨Åcation frameworks.
‚Ä¢We investigate several veriÔ¨Åcation schemes, i.e., separate masks, embedding signatures, and
embedding signatures with the trigger set. We show that these schemes are robust to the common
removal and ambiguity attacks, as well as a new type of ‚Äúadd-on" attacks. Extensive experiment
results demonstrate their competence on protecting the winning tickets. For example, on ResNet-20,
our veriÔ¨Åcation framework can defend Ô¨Åne-tuning attacks intrinsically, as well as pruning attacks
and as add-on attack under all levels of pruning ratios.
2 Related Work
Pruning and Lottery Ticket Hypothesis. Pruning algorithms can be roughly classiÔ¨Åed into two
types: pruning after training and pruning before training. Conventional pruning after algorithms
assign scores to individual weights and remove weights with the lowest scores [ 12]. There are
a large number of scoring algorithms of this kind, including weights magnitude [ 13,14], Taylor
coefÔ¨Åcients [ 15‚Äì18], and other variants [ 19,20]. Pruning-before-training methods also play important
roles in the community [ 1]. However, it requires multiple cycles of training and pruning process [ 12],
making it sometimes tedious in practical use. Pruning-at-initialization methods alleviate such cost by
2

--- PAGE 3 ---
pruning initial weights such as observing initial gradients of weights [ 2,3], but the quality of sparse
networks found by these methods are mediocre in return.
A representative pruning-before-training method is the lottery ticket hypothesis [ 1], which hypoth-
esizes the existence of a sparse network in dense networks that can be trained to match the test
accuracy of dense networks in isolation. It was later veriÔ¨Åed that the original hypothesis was too
strong, and early rewind techniques [ 21] help scale up the original version. Later on, the lottery ticket
hypothesis and its variants have been explored and extended to various Ô¨Åelds [ 22‚Äì30] such as image
generation [ 31,27,32] and natural language processing [ 22,24]. However, it is currently non-trivial
to Ô¨Ånd winning tickets, especially at high sparsity since multiple train-prune-train processes are
required, which suggests the practical value of protecting the found sparse networks.
Ownership VeriÔ¨Åcation. Ownership veriÔ¨Åcation has drawn attention from both the industry and
academia. Many works have been proposed to address IP protection. One most popular way is
to use watermarking algorithm: [ 6] proposed to embed watermarks in the form of bits into deep
networks‚Äô weights by an additional regularization term. [ 10] embedded information into model
weights by regularizing on the signs of weights. Besides watermarking on weights, [ 7,33] embedded
watermarks in the labels of certain examples in a trigger set , which makes it possible to extract
watermarks through a service interface without directly accessing the models‚Äô weights (black-box
setting). Following somehow different pathways, [ 34] proposed a passport-based approach that
encodes signatures with special passport layers. [ 11] presented passport-embedded normalization
whose parameters are associated with signatures. However, none of these methods leverages structural
information for ownership veriÔ¨Åcation besides assuming general dense networks. For sparse networks,
the sparse mask patterns represent the key information and can vary across models and tasks.
3 The Lottery Ticket Claims its Ownership
3.1 The Lottery Ticket Hypothesis
¬∂Sparse Masks, Subnetworks and Winning Tickets. We deÔ¨Åne a neural network parameterized
byWasN[W](). With a slight abuse of notion, we deÔ¨Åne a subnetwork of N[W]asN[W;M] :=
N[WM]()where Mis a sparse mask whose shape is the same as Wbut the value of each
entry in which can only be either 0 or 1. Given W0the initialization of N, ifN[W0;M]can be re-
trained tomatch the test performance of the dense model training from N[W0], we call N[W0;M]
awinning ticket . The term re-train above is used to distinguish between the training process to
Ô¨Ånd the winning ticket. The criterion for matching can be set as, e.g., no lower than 1%than dense
models‚Äô performance. ¬∑Sparsity Comparison. The sparsity of a sparse mask Mcan be deÔ¨Åned
asspar(M) :=kMk0=kM+ 1k0wherekk 0represents the non-zero values of the input matrix,
and the relative sparsity can be deÔ¨Åned as rspar(M1;M2) := spar(M1)=(spar(M2) +). We call a
sparse mask M1issparser thanMif and only ifkM1k0<kMk0. Asub-mask M2is a mask that
has the same shape as Msatisfying that all the elements in M M2are non-negative. ¬∏Extremely
Sparse Condition. Given a sparsity difference threshold t, we call N[W0;Me]anextremely sparse
winning ticket (or referred to as an extreme ticket hereinafter for conciseness) if N[W0;Me]can
match the performance of N[W0], but pruning the model ( i.e., increase the sparsity of Me)t100%
further cannot produce a winning ticket. In our experiments, we set tto be 0:01.
3.2 VeriÔ¨Åcation Framework for Extremely Sparse Winning Tickets
A ownership veriÔ¨Åcation framework for extremely sparse winning tickets can be formulated as a
tupleV= (ME;WE;F;V;I ), each item of which is a process:
‚Ä¢Amask embedding processME(M0;s)(optionally) for sparse masks. sis an optional string that
can be encoded into masks. The output of this process is a new mask M.Mcan either be a mask
withsembedded or contains other structural information that is useful for ownership veriÔ¨Åcation.
We call the veriÔ¨Åcation method with ME(M0;s)enabled a ‚Äúmask-based‚Äù method.
‚Ä¢Aweight embedding processWE (Dtr,T,s,N[],L,W0,M), which is a learning process for the
lottery ticket model. Dtr=fx;ygis the training dataset, T=fTx;Tygis an optional trigger
set provided to the training process, sis an optional signature for the weights embedding process.
Lis the loss function for model training (usually the cross-entropy loss), N[]deÔ¨Ånes the model
structure, W0is the initialization of weights, and Mis the sparse mask for the model. The output
ofEis a model Nwith sparse weights WMwhere Wrepresents the trained weights. The
3

--- PAGE 4 ---
trigger set T(or/and) the signature sare embedded in WMafter this process and can be veriÔ¨Åed
with the veriÔ¨Åcation process introduced next.
‚Ä¢AÔ¨Ådelity evaluation processF(N[];W;M;Dte;Af;f)is to evaluate whether the performance
discrepancy of model N[]is less than a pre-deÔ¨Åned threshold f,i.e.,jA(N[W;M];Dtr) Afj<
f, in whichA(;)is the inference performance on the test dataset Dte, andAfis a target inference
performance associated with the model.
‚Ä¢AveriÔ¨Åcation processV(N[];W;M;T;s;s)checks whether the sparse mask Mor the trigger
setTcan be successfully veriÔ¨Åed for a given model N[]. For the mask-based methods, the process
is to check if Mandsmatches by evaluating N[;M]onDteto see if the performance gap is
smaller than a pre-deÔ¨Åned threshold s, and(or) extract information in Mand compare it with s.
For the trigger set-based methods, an inference is Ô¨Årst executed on the trigger set images Tx, and
then the prediction will be compared with trigger set labels Tyto see if the false detection rate is
lesser than a threshold s[34].
‚Ä¢Aninvert processI(N[W;M];T;s)exists and will enable a successful ambiguity attack [ 34] if:
a) a set of new trigger set T0, a new signature s0, or a new mask M0can be reverse-engineered for
the given mask Mand weights W. b) the forged T0,s0,M0can be veriÔ¨Åed with respect to Mand
W. c) the Ô¨Ådelity evaluation outcome F(N[];W;M0;Dte;At;f)remains True.
The high-level deÔ¨Ånitions above are general and can work with any concrete implementation. We
will introduce several methods that use sparse structural information in the next following sections.
3.3 Structural Information As Signatures
Our motivation is originated from the nature of winning tickets that the sparse structure of winning
tickets is critical to their performance. As the sparse masks found by IMP outperform other pruning
methods by clear margins [ 5], incorrect masks will lead to degraded test accuracy. In the next few
sections, we demonstrate how to use the sparse structure of extreme tickets, i.e.,both the sparse
masks and weights, to perform ownership veriÔ¨Åcation under different veriÔ¨Åcation schemes. The
ownership veriÔ¨Åcation can be performed in two different scenarios: (a) protecting the sparse masks of
the extremely sparse winning tickets; and (b) protecting the trained extremely sparse winning tickets.
3.3.1 Protecting the Sparse Masks: Splitting Signature from Sparse Model
Such sparse masks play a crucial role in achieved outstanding generalization [ 1] and transferability [ 24,
25], and thus draws our attention to prevent them from being illegal distributed or used. Given a Ô¨Åxed
initialization, correct masks are essential for training the extremely sparse winning tickets to match
the performance of the dense network. If we split the sparse masks into two parts, neither part is
intact and correct so neither can be trained to match the performance of the dense model with the
given initialization. Recall the mechanism of one lock can be unlocked by one key generally, we
adopt the concept of keys and locks and propose a new ownership veriÔ¨Åcation method for the masks
of extremely sparse winning tickets.
Denote the sparse mask of extremely sparse winning ticket by fMlgN
l=1and the weights by fWlgN
l=1,
whereNis the number of layers. To sparsify a model, fMlgN
l=1is applied to the model‚Äôs weight
fWlgN
l=1by conducting an element-wise product ( fWlMlgN
l=1). Our goal is to Ô¨Ånd key masks
i.e.,sub-masksfMs
lgN
l=1that contain as few elements as possible while the performance of the sparse
network with the locked masks ,i.e.,the remaining masks, degrade as much as possible. Meanwhile,
fewer elements in key masks reduce the cost of storing, distributing, and using the key masks.
We next describe the algorithms needed to discover key masks. An algorithm is used to split the
masks of extremely sparse winning tickets ( fMlgN
l=1) into key masksfMs
lgN
l=1and locked masks
under the constraint of rspar(fMs
lgN
l=1;fMlgN
l=1)< ns.nsis a hyper-parameter controlling the
relative sparsity of the key masks. Score functions are used to decide which part should be split into
the key masks. The pipeline is described in Algorithm 1.
We study several score functions in our experiments: 1) One-Shot Magnitude (OMP): the absolute
values of each weight; 2) Edge-Weight-Product (EWP) [ 35] which measures the importance of paths
from models‚Äô input to output. The EWP score is deÔ¨Åned as the multiplication of weights along the
paths; 3) Edge betweenness centrality (Betweenness). The edge betweenness centrality measures the
importance of each edge inside a graph. For convolutional layers, we deÔ¨Åne the weight of each ‚Äúedge‚Äù
to be the summation of absolute values of each element; and 4) random scoring.
4

--- PAGE 5 ---
Algorithm 1: Splitting Key Masks
input : A sets of masks M=fMlgN
l=1, initialization weights W=fWlgN
l=1, number of
non-zero elements n, and a function score ()for scoring.
output : Key masksfMs
lgN
l=1and locked masksfMl Ms
lgN
l=1
1Derive the score matrices by applying score ()overfWlMlgN
l=1and getfSlgN
l=1
2Set the values of entries in fSlgN
l=1to negative inÔ¨Ånity if the corresponding entries at the same
position infWlMlgN
l=1is zero ( i.e., already pruned).
3Calculate the nthlargest number across the score matrix fSlgN
l=1and record it as T.
4SetMs
l IMl>Tand let the key masks be fMs
lgN
l=1. The comparison between MlandT
(Ml>T)is performed element-wise.
3.3.2 Protecting the Trained Tickets: Embedding Signature into Sparsity Masks
Another scenario is to protect the trained extremely sparse winning ticket since a superior performance
on certain large-scale datasets usually comes with a huge economic and ecological cost. Although
directly splitting the masks provides a solution to the ownership veriÔ¨Åcation problem, it has some
drawbacks. It delivers extra cost to users since they need to recover the masks. Such a method is
also intrusive and requires additional responsibility from the users‚Äô side for storing the key masks
safely. To render the extreme tickets capable of self-veriÔ¨Åcation and free of key masks , we propose a
novel pruning method that is able to ‚Äúabsorb‚Äù secret information ( e.g., signatures) into models‚Äô sparse
masks. The core concept is to enforce the sparsity masks to follow certain ‚Äú0-1‚Äù patterns, which can
be extracted from masks and further decoded back to the original form of information.
A function encode ()is used to transform a string sinto a matrix Ms2f0;1gd1d2which we call
signature mask . Our goal is to embed encode (s)into the sparsity masks fMlgN
l=1. One critical
question is where to embed the signature mask Ms. Empirically, low-level convolutional layers are
less sparser, which means they are more unlikely to be pruned. Therefore, information embedded in
the low-level convolutional layers is more difÔ¨Åcult to be removed if using the pruning method. Based
on such observation, we decide to embed Msin low-level convolutional layers. To minimize mask
changes, we Ô¨Årst Ô¨Ånd a region in fMlgwith the highest similarity with Msand tune the sub-mask of
that region. For masks that have a dimension of two, we directly replace the region with Ms; for
masks that have a dimension of more than two, we raise their dimension by using random connections.
Our detailed workÔ¨Çow is shown in Algorithm 2.
The choices of function encode ()are various but there is one common choice in our daily life: QR
code [ 36]. QR code has multiple advantages: 1) QR code is naturally seen as a pattern with only zeros
and ones; 2) QR code has the ability to correct the error if the code is dirty or damaged. For example,
the H correction level can tolerate up to 30% of error [ 36]; 3) QR codes can be small in size which
can be easily Ô¨Åt into sparse masks. The size of the QR code generated can be as small as 2121
while the numbers of channels in convolutional kernels in deep learning models are typically greater
than21, showing an abundant space for Ô¨Åtting the QR code in inside models‚Äô sparsity masks; and 4)
The QR code without the Ô¨Ånder, alignment and version patterns are imperceptible when Ô¨Åtted into
sparse masks since there are no ‚Äúregular‚Äù patterns left. Based on these merits, we choose encode ()
to be the QR code generation function. SpeciÔ¨Åcally, the encode function we use will return a QR
code without Ô¨Ånder, alignment, and version patterns. When extracting the code, the above patterns
will be added back for decoding the credential information behind the QR code.
Algorithm 2: Embed Signature Into Sparse Masks
input : A set of masks M=fMlgN
l=1, a signature s
output : A set of masks with signature embedded fMe
lgN
l=1
1Calculate Ms encode (s).
2Squeeze each Mlinto a two-dimensional matrix Mf
lby setting (Mf
l)ij=Ik(Ml)ijk0>0.
3Calculate the similarity (percentage of matched 0-1 patterns) between each Mf
landMsand
name the one with the largest similarity Mf
max.
4Change the dimension of Msand Ô¨Åt it into Mf
maxto the region where the similarity is the largest.
5

--- PAGE 6 ---
3.4 Ownership VeriÔ¨Åcation with Sparse Structural Information
Next, we propose three different veriÔ¨Åcation schemes based on sparse structural information, as
summarized in Table A9. Under our unique context, we further introduce a new Add-on Attacks
which aims to create ambiguity against lottery veriÔ¨Åcation by ‚Äúrecovering" several pruned weights
and manipulating the sparsity patterns. More details can be found in Appendix A1.
SchemeV1: Distribute the extreme tickets with key masks. SchemeV1is designed to protect
the sparsity masks. We separate the sparsity mask Minto two parts: MlandMs, where l/ssubscripts
denote ‚Äúlarge‚Äù/‚Äúsmall‚Äù, respectively. The small mask is sparser than the large one, which is used as
the key mask while the large counterpart is the locked mask. We apply these two masks on weights
and get two separate parts (WMl;WMs). Before re-training, legitimate users should merge
the two weights by adding them up to recover the original sparse weights WM. The ownership
can be automatically veriÔ¨Åed by the inference performance since an incorrect provided mask-weight
pair will deteriorate accuracies after re-training.
SchemeV2: Embed signatures in sparse masks. We apply the signature mask embedding method
to embed credentials into the extreme ticket. Then we train the model and dispatch it to legitimate
users. No further action is required at the users‚Äô side. For the veriÔ¨Åcation process, one can use extract
the signature from the sparse model and validate the ownership of the extreme tickets. Compared with
SchemeV1, SchemeV2is more user-friendly since no extra action is performed at the users‚Äô side.
The application scenarios of Scheme V1andV2are also different: the latter focuses on protecting the
trained weights. It also shows great defense ability towards removal and ambiguity attacks. However,
this scheme works under the white-box veriÔ¨Åcation setting only, which means that access to models‚Äô
weights has to be assumed. To overcome that assumption, we combine Scheme V2with a trigger
set-based method and propose Scheme V3in the next section.
SchemeV3: Combining trigger set-based methods. SchemeV3is more sophisticated than
SchemeV2as a set of trigger images and labels are used during the (re-)training process. With
the help of this trigger set, Scheme V3is now capable of black-box veriÔ¨Åcation. By using remote
calls of service APIs, the owner can Ô¨Årst probe and claim the ownership in a black-box regime and
further request a white-box veriÔ¨Åcation if the black-box mode has raised a red Ô¨Çag. The white-box
veriÔ¨Åcation part for Scheme V3remains the same as Scheme V2.
4 Experiments
In this section, we will list the details of our experiments and show the results to prove the effectiveness
of our ownership veriÔ¨Åcation methods, as well as the robustness to removal attacks ( e:g:, model
pruning, Ô¨Åne-tuning, and add-on attacks) and ambiguity attacks ( e:g:, fake paths, fake code).
General Settings. We use three networks architectures (ResNet-20, ResNet-18 and ResNet-50)
and two benchmarks (CIFAR-10 [ 37] and CIFAR-100 [ 37]) in our experiments. For all experiments,
we follow the same (re-)training and testing protocol. The optimizer we use is an SGD optimizer
with a momentum factor of 0.9 and a weight decay factor of 1e-4 for ResNet-20 and ResNet-18,
and 5e-4 for ResNet-50. We train the model for 182 epochs with an initial learning rate of 0.1, and
we decay the learning rate at 91stand 136thepoch by 0.1. We use a late rewinding technique that
rewinds the weight to the 3rdcheckpoint. More results on ResNet-50 are deferred to Appendix A2.
Our experiments are run with 16 pieces of NVIDIA RTX GeForce 2080 Ti.
Types of Attacks and Trigger Set. We study two types of attacks: 1) removal attacks. This type
of attack aims at removing the embedded watermarks from the model‚Äôs weights or data. Available
methods for removal attacks include pruning, which removes a proportion of parameters of the model,
and Ô¨Åne-tuning, which performs training on new data for a few steps. Both methods can modify the
weights and potentially make the watermark undetectable. 2) ambiguity attacks. This type of attack
aims at confusing the veriÔ¨Åcation schemes, i.e.,no one can tell which is the real watermark/signatures.
This type of attack needs techniques like reverse engineering and does not have a certain form. We
explore several attack methods in our paper.
The trigger set we use is the same as the trigger set used in [ 7], which contains abstract images that
are different than the training images.
New type of attack: Add-on Attacks. Although the extremely sparse winning tickets are naturally
robust to Ô¨Åne-tuning and pruning attacks due to their unique properties, the veriÔ¨Åcation schemes based
on the sparse structure will potentially suffer from another kind of attacks, i.e., trying to ‚Äúrecover‚Äù
6

--- PAGE 7 ---
some pruned weights and change the sparse structure of the extremely sparse winning tickets. We
name it add-on attacks . Such a new attack type targets mask-based veriÔ¨Åcation schemes and aims at
creating ambiguity against veriÔ¨Åcation.
We propose a pipeline defending against such attacks. We can Ô¨Årst prune weights whose magnitudes
are smaller than t.tis known to the owner of the model since the owner has the authentic sparse
masks. This can detect any noise with magnitude smaller than t, that the attackers add to the mask.
For noises of moderate level, their magnitudes become comparable to with the benign weights, hence
the prediction quality will be signiÔ¨Åcantly degraded as the noise increases.
4.1 Finding Extreme Tickets
Table 1: Performance of dense models and extremely
sparse winning tickets, and the pruning speciÔ¨Åcation.
The performance are expressed in terms of the test ac-
curacy of the dense model and the extremely sparse
winning ticket. The pruning speciÔ¨Åcation includes the
proportion of remaining weight as well as the number of
pruning with four different pruning ratios (in brackets).
Model Dataset Performance Pruning SpeciÔ¨Åcation
ResNet-20 CIFAR-10 91.67%,91.66% 19.369% (5,1,8,1)
ResNet-20 CIFAR-100 66.36%,66.39% 19.901% (6,0,4,7)
ResNet-18 CIFAR-10 93.67%,93.60% 1.236% (18,3,0,6)
ResNet-18 CIFAR-100 72.44%,72.59% 2.251% (17,0,0,0)To Ô¨Ånd extreme tickets, multiple rounds of
the train-prune-retrain process are usually re-
quired. Once the test performance of the cur-
rently trained model cannot match the perfor-
mance of the dense model, we revert the prun-
ing process back for one time and reduce the
pruning ratio. The choices of pruning ratio of
weights are [0:2;0:1;0:05;0:1]. The results are
shown in Table 1. We also report the prun-
ing speciÔ¨Åcation, which includes the remaining
weights of the extreme tickets, as well as the
pruning times for each pruning rate we choose.
Notice that ResNet-20 needs at least 15times of pruning before we identify the extreme tickets, and
for ResNet-18, such number increases to 17. Numerous pruning rounds exemplify the effort to Ô¨Ånd
the extreme tickets and emphasizes the importance of protecting them.
4.2 Effectiveness of Different Schemes
SchemeV1To verify that extreme tickets without correct key masks will have degraded performance
after retraining, we conduct experiments that directly re-train the models without the key masks on
ResNet-20 and ResNet-18. Fig. 2 show the performance of extreme tickets after retraining. It can
be seen that more ‚Äú1‚Äùs in key masks can increase the performance divergence. Different splitting
functions do not make an essential difference, showing that the choice of score functions is Ô¨Çexible.
868890
0 5 10 15 20
Res20s, CIFAR‚àí10Test AccuracyType
Betweenness
EWP
OMP
Random 57.560.062.565.0
0 5 10 15 20
Res20s, CIFAR‚àí100Type
Betweenness
EWP
OMP
Random
848688909294
0 5 10 15 20
Res18, CIFAR‚àí10Type
Betweenness
EWP
OMP
Random62.565.067.570.072.5
0.0 2.5 5.0 7.5 10.0
Res18, CIFAR‚àí100Type
Betweenness
EWP
OMP
Random
Figure 2: Effectiveness of Scheme V1: Re-training without key masks generated by four methods: Betweenness,
EWP, OMP, Random (Paths). The x-axis is the relative sparsity w.r.t the extreme ticket.
SchemeV2To show that our proposal can embed information into models‚Äô sparse structures without
signiÔ¨Åcantly harming their performance, we conduct experiments to compare the performance before
and after a signature string is embedded. Table 2 shows the test accuracy of two different models.
We can see from the table that the performance of extreme tickets with the string embedded is only
slightly lower than the original one, which proves that using Scheme V2endows owners the ability to
embed information at a little cost of performance.
SchemeV3We use a set of trigger images during the re-training of extremely sparse winning tickets
under SchemeV3. The inference performance on the original task ( i.e., CIFAR-10 or CIFAR-100)
should not be greatly affected with trigger sets enabled. Table 3 shows the inference performance on
both the original images and the trigger set. We can see that the performance only drops 0.2% on
CIFAR-10 and 0.97% on CIFAR-100 for ResNet-20, while the detection rates of the trigger set images
are high (91.0% on CIFAR-10 and 90.0% on CIFAR-100). At the same time, the extreme tickets
trained from CIFAR-10 and CIFAR-100 without a trigger set can only have trigger set accuracies of
7

--- PAGE 8 ---
Table 2: Effectiveness of Scheme V2: performance
of extreme tickets after embedding QR codes. We
study two different models and compare their infer-
ence performance. The performance after embedding
and the performance drop are reported (in brackets).
ModelAccuracy after embedding
CIFAR-10 CIFAR-100
ResNet-20 91.37% (#0.29%) 66.14% (#0.25%)
ResNet-18 93.56% (#0.11%) 72.35% (#0.24%)Table 3: Effectiveness of Scheme V3: ResNet-20
on CIFAR-10 and CIFAR-100. ESWT is the abbre-
viation of Extremely Sparse Winning Tickets. We
re-train the two extremely sparse winning tickets
with QR code embedded found with the trigger set
enabled.
ModelTest Accuracy
CIFAR-10 CIFAR-100
ESWT 91.66% (16.0%) 66.36% (0.0%)
ESWT + Ms+T 91.46% (91.0%) 65.39% (90.0%)
16.0% and 0.0%, respectively. This suggests the Scheme V3can work as expected, i.e.,perform well
on the trigger set while not signiÔ¨Åcantly harming the performance on the original dataset.
4.3 Robustness Against Removal Attacks
Fine-tuning Attacks Fine-tuning the model can only change the values of weights while not
changing the sparse structure of extreme tickets. As a consequence, Scheme V2andV3is resistant to
Ô¨Åne-tuning attacks under the white-box veriÔ¨Åcation setting.
For SchemeV1, users are required to provide the key masks to recover the correct masks and then
re-train the extreme tickets. One key property we need to verify is that attackers cannot bypass the
requirement of key masks by Ô¨Åne-tuning the model on a new dataset. To this end, we conduct transfer
experiments described as follows: on CIFAR-100, we train the model with the locked mask generated
on the extreme tickets identiÔ¨Åed on CIFAR-10; on CIFAR-10, we conduct a similar experiment with
the locked mask from CIFAR-100. The results are shown in Table 4. From the table, we can see that
even transferring the sparse mask cannot bypass the requirement of key masks. The performance
gaps between the transferred model and the extreme tickets found on each set are greater than 3% on
both datasets, much higher than the 1% criterion we set for matching performance. Such big gaps
prove that the model after Ô¨Åne-tuning attacks is not useful in practice.
Table 4: Fine-tuning Attacks on Scheme V1: Trans-
ferring extreme tickets of ResNet-20 found on CIFAR-
10/100. 10 !100 means transferring from CIFAR-10
to CIFAR-100 and vice versa. The percentage inside
brackets denotes the relative sparsity of the key mask
w.r.t the extremely sparse winning ticket.
ModelTest Accuracy
10!100 100!10
OMP (5%) 59.80% 87.66%
EWP (5%) 60.27% 88.21%
Betweenness (5%) 59.61% 87.22%Table 5: Pruning Attacks on Scheme V3: Perfor-
mance of ResNet-20 on CIFAR-10/100 after prun-
ing with different pruning ratios. The accuracy on
CIFAR-10/100 are shown outside the brackets and the
accuracy on trigger images are inside the brackets.
ModelAccuracy
CIFAR-10 CIFAR-100
Original model 91.46% (91.0%) 65.39% (90.0%)
Pruning 5% 91.33% (89.0%) 64.78% (91.0%)
Pruning 10% 90.66% (90.0%) 62.96% (73.0%)
Pruning 20% 87.86% (81.0%) 50.14% (16.0%)
Pruning 50% 33.04% (18.0%) 8.56% (0.00%)
We also have conducted experiments to study if Scheme V3can resist Ô¨Åne-tuning attacks under
black-box veriÔ¨Åcation. We Ô¨Årst retrain the extreme tickets under Scheme V3on CIFAR-10/-100,
and continue to Ô¨Åne-tune it on CIFAR-100/-10. The extreme tickets trained on CIFAR-10 can only
achieve 61.59% test accuracy on CIFAR-100, and the extreme tickets on CIFAR-100 can only achieve
88.21% test accuracy on CIFAR-10. The strong bond between sparse structure (masks of extreme
tickets) and datasets on which the extreme tickets we found brings performance drop when Ô¨Åne-tuning
them on a new dataset, which devalues such attack and also highlight the robustness of the Scheme
V3against Ô¨Åne-tuning attacks.
Model pruning Pruning the model under Scheme V1is meaningless since pruning cannot recover
the full masks. So we focus on Scheme V2andV3for model pruning attacks. Pruning the trained
model leaves more ‚Äú0‚Äù in the trained model, which might change the extracted QR code and makes it
unable to decode. To study if our model can resist the pruning attack, we conduct experiments with
different pruning methods (one-shot magnitude and random pruning) and different pruning ratios
(5%, 10%, 20%, 30%, 50%).
We Ô¨Årst examine our proposal for black-box veriÔ¨Åcation (Scheme V3). In Table 5 we show the results
of SchemeV3against pruning attacks. The accuracy on trigger set images drops after the accuracy
on the original dataset (CIFAR-10/CIFAR-100) has decreased considerably, which means that the
8

--- PAGE 9 ---
user-speciÔ¨Åc information cannot be removed without sacriÔ¨Åcing its performance and demonstrates its
resilience against pruning attack.
We then test our proposal for white-box veriÔ¨Åcation (Scheme V2). In Table 6 we show the inference
performance on original datasets after pruning, and also show the QR codes extracted from masks in
Figure 3. We can see that the performance of the pruned model will degrade dramatically after 20%
percent of one-shot magnitude pruning and 5% percent of random pruning. On the contrary, the QR
code extracted from the ResNet-20 can be decoded even after 20% percent of one-shot magnitude
pruning. Figure 4 shows the QR code extracted from ResNet-18. At the 5% pruning ratio, the string
can be easily decoded into a readable string. At the 10% pruning ratio, the string can still be partly
decoded, although the readability has been reduced. For the pruning ratio greater than 10%, the
inference performance has signiÔ¨Åcantly dropped, making it meaningless to conduct such attack.
Table 6: Inference performance of extremely sparse
winning tickets on ResNet-20 and ResNet-18 after
model pruning attacks under different pruning meth-
ods and pruning ratios. OMP stands for one-shot
magnitude pruning. The numbers in brackets stand
for the pruning ratios.
Method (Percent)Performance
CIFAR-10 CIFAR-100
SchemeV2 91.37% 72.35%
OMP (5%) 91.25% 72.27%
OMP (10%) 90.72% 71.42%
OMP (20%) 88.03% 69.51%
OMP (30%) 80.08% 60.31%
OMP (50%) 36.62% 9.24%
Random Pruning (5%) 60.87% 58.23%
Random Pruning (10%) 30.49% 22.67%
Random Pruning (20%) 11.95% 3.23%
Random Pruning (30%) 12.05% 1.0%
Random Pruning (50%) 10.00% 1.0%Table 7: Summary of different types of ambiguity
attacks. We show the speciÔ¨Åcation of each attack,
i.e., the accessibility of each component to attackers,
the attack methods, and the targeted schemes.
Attack name Attackers can access How to attack Attack Scheme
fake 1 WMl Forge WMs SchemeV1
fake 2 WM Add noise WnoiseMnoise SchemeV2andV3
fake 3 WMandencode () Replace Ms SchemeV2andV3
Table 8: Test accuracy and remaining weights after
add-on attacks under different rates on ResNet-20,
with the matching condition and the decode-ability
of the QR code extracted from the masks.
Add-on Rate Test Accuracy (% r remain )Decode-able? Match?
0% 91.53% (19.369%) 3 3
0.5% 91.04% (19.789%) 3 3
1% 90.23% (20.179%) 3 7
2% 86.64% (21.009%) 7 7
5% 79.49% (23.386%) 7 7
10% 71.06% (27.402%) 7 7
Original
 Prune 5%
 Prune 10%
 Prune 20%
 Prune 30%
 Prune 50%
Figure 3: QR code extracted from ResNet-20 under pruning attacks with different ratios. The codes extracted
under 5% and 10% pruning ratio can be easily decoded into readable strings ‚Äúsignature‚Äù, and the code under
20% pruning ratio can be decoded into ‚Äúsigiature‚Äù with tools at https://github.com/merricx/qrazybox/ .
Original Prune 5% Prune 10% Prune 20% Prune 30% Prune 50%
Figure 4: QR code extracted from ResNet-18 under pruning attacks with different ratios. The code extracted
under 5% can be easily decoded into a readable string ‚Äúsignature‚Äù, and the code under 10% pruning ratio can be
decoded into ‚Äúsim√ßi@5re‚Äù with tools at https://github.com/merricx/qrazybox/ .
4.4 Resilience Against Ambiguity Attacks
In this section, we will evaluate the robustness against ambiguity attacks summarized in Table 7.
SchemeV1(fake1: Attackers can access WMlonly) The goal of fake1is to forge a new key
maskM0
swith new underlying weights W0. As the attacker has no prior information on (WMs),
the forging process can only be performed randomly. From Figure 5 we can see that such an attack
method is not practical as the performance gap is much greater than f(= 1%) after using random key
masks. For example, if we adopt OMP as the scoring function to construct the key masks, we only
need a key mask of around 10% relatively sparsity to make the model resistant to random attacks.
9

--- PAGE 10 ---
868890
0 5 10 15 20
Res20s, CIFAR‚àí10Test AccuracyType
Betweenness
EWP
OMP
Random 57.560.062.565.0
0 5 10 15 20
Res20s, CIFAR‚àí100Type
Betweenness
EWP
OMP
Random
848688909294
0 5 10 15 20
Res18, CIFAR‚àí10Type
Betweenness
EWP
OMP
Random62.565.067.570.072.5
0.0 2.5 5.0 7.5 10.0
Res18, CIFAR‚àí100Type
Betweenness
EWP
OMP
RandomFigure 5: Random attacks on Scheme V1. The x-axis is the relative sparsity of the key masks. The solid/dashed
lines represent the performance before /after random attacks.
SchemeV2andV3(fake2: Attackers can access WMbut not knowing encode ()) One might
use add-on attacks and try to ‚Äúcontaminate‚Äù the information we embed in the sparse mask. SpeciÔ¨Åcally,
we randomly add noises to the position where the weights are pruned. We test with add-on rates
ranging from 0% to 10% since a 10% efÔ¨Åciency gap will diminish the value of attacking the model.
The results are shown in Table 8 and Figure 6. From the table, we can see that introducing 1% of
noise to the trained model will un-match the attacked model ( i.e.,the performance gap becomes
greater than 1%). For the add-on rates smaller than 1%, the QR code embedded in the sparse mask
can be normally decoded into a normal string. Such results prove that both Scheme V2andV3are
resistant to attack fake2.
Original
 Add 0.5% Add 1%
 Add 2%
 Add 5%
 Add 10%
Figure 6: Visualization of QR code extracted and processed under different add-on rates.
(fake3: Attackers can access WMandencode ()) If the attacker knows about the encode function
for generating the Ms, a similar but fake signature mask M0
swhich contains a different signature
can be generated in the same way. However, as shown in Figure 1, without the Ô¨Ånder, alignment,
and version patterns, one can hardly tell which part belongs to a QR code. Even if the attacker
knows the position where the code is embedded (namely an insider attack [ 34]), directly replacing
the embedded region with a new signature mask M0
sandW0(noise) will also considerably degrade
the performance of the attacked model since a large amount of ‚Äúincorrect‚Äù weights are introduced.
For example, for ResNet-20 on CIFAR-10, the test accuracy of the attacked model will drop from
91:37% to57:00%, which is nearly a 50% degradation in performance. Such a big loss shows that it
is infeasible to perform the insider attack.
5 Conclusion and Discussion of Broad Impact
LTH offers superior sparse models through burdensome explorations, serving as an intriguing yet
expansive solution for resource-constrained applications. It motivates the necessity of protecting the
copyright of these precious winning tickets. We investigate a brand new veriÔ¨Åcation technique by
leveraging the sparse structural information, which embeds signatures into lottery tickets‚Äô typologies.
Extensive results verify our proposal‚Äôs effectiveness and robustness against diverse malicious attacks.
This work is scientiÔ¨Åc in nature and should bring positive societal impacts. Note that every second,
giant and start-up companies have invested billions of dollars to identify superior yet light-weight
compact deep neural networks virtually. We believe our new lottery veriÔ¨Åcation mechanism can assist
both industry and academia in defending their interests from illegal distribution or usage.
References
[1]Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In International Conference on Learning Representations , 2018.
[2]Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network
pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340 , 2018.
10

--- PAGE 11 ---
[3]Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient Ô¨Çow. In International Conference on Learning Representations , 2019.
[4]Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G.
Baraniuk, Zhangyang Wang, and Yingyan Lin. Drawing early-bird tickets: Toward more efÔ¨Åcient
training of deep networks. In 8th International Conference on Learning Representations , 2020.
[5]Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Pruning neu-
ral networks at initialization: Why are we missing the mark? arXiv preprint arXiv:2009.08576 ,
2020.
[6]Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin‚Äôichi Satoh. Embedding watermarks
into deep neural networks. In Proceedings of the 2017 ACM on International Conference on
Multimedia Retrieval , pages 269‚Äì277, 2017.
[7]Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. Turning
your weakness into a strength: Watermarking deep neural networks by backdooring. In 27th
fUSENIXgSecurity Symposium ( fUSENIXgSecurity 18) , pages 1615‚Äì1631, 2018.
[8]Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin, Heqing Huang, and
Ian Molloy. Protecting intellectual property of deep neural networks with watermarking. In
Proceedings of the 2018 on Asia Conference on Computer and Communications Security , pages
159‚Äì172, 2018.
[9]Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie, and Zhangyang
Wang. Undistillable: Making a nasty teacher that cannot teach students. arXiv preprint
arXiv:2105.07381 , 2021.
[10] Bita Darvish Rouhani, Huili Chen, and Farinaz Koushanfar. Deepsigns: An end-to-end water-
marking framework for ownership protection of deep neural networks. In Proceedings of the
Twenty-Fourth International Conference on Architectural Support for Programming Languages
and Operating Systems , pages 485‚Äì497, 2019.
[11] Jie Zhang, Dongdong Chen, Jing Liao, Weiming Zhang, Gang Hua, and Nenghai Yu. Passport-
aware normalization for deep model protection. Advances in Neural Information Processing
Systems , 33, 2020.
[12] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic Ô¨Çow. Advances in Neural Information
Processing Systems , 33, 2020.
[13] Steven A Janowsky. Pruning versus clipping in neural networks. Physical Review A , 39(12):6600,
1989.
[14] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections
for efÔ¨Åcient neural network. In NIPS , 2015.
[15] Michael C Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from
a network via relevance assessment. In Advances in neural information processing systems ,
pages 107‚Äì115, 1989.
[16] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing systems , pages 598‚Äì605, 1990.
[17] Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal
brain surgeon . Morgan Kaufmann, 1993.
[18] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efÔ¨Åcient inference. arXiv preprint arXiv:1611.06440 , 2016.
[19] Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via
layer-wise optimal brain surgeon. arXiv preprint arXiv:1705.07565 , 2017.
11

--- PAGE 12 ---
[20] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han, Mingfei
Gao, Ching-Yung Lin, and Larry S Davis. Nisp: Pruning networks using neuron importance
score propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 9194‚Äì9203, 2018.
[21] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode
connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning ,
pages 3259‚Äì3269. PMLR, 2020.
[22] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv ,
abs/1902.09574, 2019.
[23] Zhenyu Zhang, Xuxi Chen, Tianlong Chen, and Zhangyang Wang. EfÔ¨Åcient lottery ticket
Ô¨Ånding: Less data is more. In Marina Meila and Tong Zhang, editors, Proceedings of the
38th International Conference on Machine Learning , volume 139 of Proceedings of Machine
Learning Research , pages 12380‚Äì12390. PMLR, 18‚Äì24 Jul 2021.
[24] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and
Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks, 2020.
[25] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and
Zhangyang Wang. The lottery tickets hypothesis for supervised and self-supervised pre-training
in computer vision models. arXiv preprint arXiv:2012.06908 , 2020.
[26] Haonan Yu, Sergey Edunov, Yuandong Tian, and Ari S. Morcos. Playing the lottery with
rewards and multiple languages: lottery tickets in rl and nlp. In 8th International Conference on
Learning Representations , 2020.
[27] Xuxi Chen, Zhenyu Zhang, Yongduo Sui, and Tianlong Chen. {GAN}s can play lottery tickets
too. In International Conference on Learning Representations , 2021.
[28] Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie, and Zhangyang Wang.
Good students play big lottery better. arXiv preprint arXiv:2101.03255 , 2021.
[29] Zhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu Cheng, Shuohang Wang, and Jingjing
Liu. Playing lottery tickets with vision and language. arXiv preprint arXiv:2104.11832 , 2021.
[30] Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. A uniÔ¨Åed lottery
ticket hypothesis for graph neural networks. arXiv preprint arXiv:2102.06790 , 2021.
[31] Neha Mukund Kalibhat, Yogesh Balaji, and Soheil Feizi. Winning lottery tickets in deep
generative models, 2021.
[32] Tianlong Chen, Yu Cheng, Zhe Gan, Jingjing Liu, and Zhangyang Wang. Ultra-data-efÔ¨Åcient gan
training: Drawing a lottery ticket Ô¨Årst, then training it toughly. arXiv preprint arXiv:2103.00397 ,
2021.
[33] Erwan Le Merrer, Patrick Perez, and Gilles Tr√©dan. Adversarial frontier stitching for remote
neural network watermarking. Neural Computing and Applications , 32(13):9233‚Äì9244, 2020.
[34] Lixin Fan, Kam Woh Ng, and Chee Seng Chan. Rethinking deep neural network ownership
veriÔ¨Åcation: Embedding passports to defeat ambiguity attacks. 2019.
[35] Shreyas Malakarjun Patil and Constantine Dovrolis. Phew: Paths with higher edge-weights give
"winning tickets" without training data, 2020.
[36] Tan Jin Soon. Qr code. Synthesis Journal , 2008:59‚Äì78, 2008.
[37] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
12

--- PAGE 13 ---
Checklist
1. For all authors...
(a)Do the main claims made in the abstract and introduction accurately reÔ¨Çect the paper‚Äôs
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] Please refer to Section 5.
(c)Did you discuss any potential negative societal impacts of your work? [Yes] Please
refer to Section 5.
(d)Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results...
(a)Did you state the full set of assumptions of all theoretical results? [N/A] Our work
completely focuses on empirical investigation.
(b)Did you include complete proofs of all theoretical results? [N/A] Our work completely
focuses on empirical investigation.
3. If you ran experiments...
(a)Did you include the code, data, and instructions needed to reproduce the main exper-
imental results (either in the supplemental material or as a URL)? [Yes] We use the
publicly available datasets (section 4) and attach our codes with instructions to the
supplement for better reproducibility.
(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] We detail our experiment settings in section 4
(c)Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [No] We conducted a single run for each experiment due to the
limited resources. We will repeat experiments and report error bars in the future.
(d)Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] Please refer to section 4
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a)If your work uses existing assets, did you cite the creators? [Yes] As shown in section 4,
we use publicly available datasets and cite the creators.
(b)Did you mention the license of the assets? [No] The licenses of used datasets are
provided in the cited paper.
(c)Did you include any new assets either in the supplemental material or as a URL?
[Yes] All used datasets are publicly available, and all our codes are provided at https:
//github.com/VITA-Group/NO-stealing-LTH .
(d)Did you discuss whether and how consent was obtained from people whose data you‚Äôre
using/curating? [N/A] We did not use/curate new data.
(e)Did you discuss whether the data you are using/curating contains personally identiÔ¨Åable
information or offensive content? [N/A] All adopted datasets are publicly available,
and we believe there are no issues of personally identiÔ¨Åable information or offensive
content.
5. If you used crowdsourcing or conducted research with human subjects...
(a)Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b)Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c)Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]
A1 More Methodology Details
More of ownership veriÔ¨Åcation schemes. Table A9 summarizes our proposed ownership veriÔ¨Åca-
tion regimes. There are Ô¨Åve different phases in each of our schemes: 1) Ticket Ô¨Ånding : Ô¨Ånding the
A13

--- PAGE 14 ---
extremely sparse winning tickets. Multiple rounds of the train-prune-retrain process are involved in
this phase for Ô¨Ånding the extremely sparse winning tickets; 2) Pre-Process : pre-process the extremely
sparse winning ticket for applying each scheme. For example, we need to construct the key masks
if using the Scheme V1; 3)Re-training : this process is unique for the winning tickets that we will
train the extremely sparse winning ticket again to match the performance of the dense model; 4)
Inference : the inference process is to perform an inference process on the test dataset; 5) Validation :
This process is to validate the ownership of the (trained/untrained) extremely sparse winning ticket.
Table A9: Summary of different ownership veriÔ¨Åcation schemes. The re-training phase can be either
done by the ticket owner or the legitimate users.
SchemeV1 SchemeV2 SchemeV3
Ticket Finding No additional technique No additional technique No additional technique
Pre-ProcessSplit key masks and locked masks Calculate Msusing encode () Calculate Msusing encode ()
Distribute both the masks Embed MsintoMand distribute Embed MsintoMand distribute
Re-training Recover the masks No additional technique Training with the trigger set T
InferenceKeys masks are requiredNo additional technique No additional techniqueSlight overhead for recovering the masks
Validation Auto-veriÔ¨Åed by performance Extract Msand decodeExtract Msand decode
Inference on trigger set T
A2 More Experimental Results
Extremely sparse winning tickets on ResNet-50. On CIFAR-10, the remaining weights of the
extremely sparse winning ticket is 13.19% (pruning speciÔ¨Åcation: (7,1,6,0)) while the performance is
94.38% (0.04% drop). On CIFAR-100, the proportion of remaining weights of the extremely sparse
winning ticket is 43.926% (pruning speciÔ¨Åcation: (2,3,0,6)) while the performance is 75.84% (0.03%
drop). On ImageNet, the proportion of remaining weights of the extremely sparse winning ticket is
16.97%, and the performance is 75.97% (0.01% higher).
Extremely sparse winning tickets on VGG-16. On CIFAR-10, the proportion of the remaining
weights of the extremely sparse winning ticket is 1.44%, while the performance is 93.10% (0.04%
higher). On Tiny-ImageNet, the proportion of the remaining weights of the extremely sparse winning
ticket is 6.81%, while the performance is 58.12% (0.19% higher).
SchemeV1on ResNet-50. Figure A7 shows the results of retraining the extremely sparse winning
tickets without key masks. Multiple scoring functions (OMP, EWP, Random) are explored. It can
be seen from the graph that on CIFAR-10, we need key masks with an approximately 15% relative
sparsity to create a 1% performance gap, while on CIFAR-100, we need key masks with a relative
sparsity of 5% approximately. ResNet-50 has greater model capacity than ResNet-20 and ResNet-18,
so it is reasonable that we need more elements removed to reduce the performance signiÔ¨Åcantly.
93.693.994.2
0 5 10 15
Res50, CIFAR‚àí10Test AccuracyType
EWP
OMP
Random7475
0 1 2 3 4 5
Res50, CIFAR‚àí100Type
EWP
OMP
Random
Figure A7: Effectiveness of Scheme V1: Re-training without key masks generated by three methods: EWP,
OMP, Random. The x-axis is the relative sparsity w.r.t the extreme ticket.
On ImageNet, the performance of the retrained model is 75.39% when the relative sparsity is 0.4%,
and the performance is 72.88%, which is nearly 3 percent lower when the relative sparsity is 5%. It
proves that our Scheme V1can work on large-scale datasets.
A14

--- PAGE 15 ---
Random ambiguity attacks on ResNet-50 under scheme V1.Figure A8 shows the results of
using random key masks for retraining the extremely sparse winning ticket for ResNet-50 on CIFAR-
10 and CIFAR-100. It can be clearly seen from the graph that the random key masks will not
contribute to recovering the performance of the trained model and even harm the test accuracy under
some circumstances. On ImageNet, the accuracy of recovering masks with random connections is
75.32% and 74.57% when the relative sparsity is 0.4% and 5%, respectively. The performance gaps,
which can be seen easily from the graphs and numbers, have demonstrated the robustness of Scheme
V1against the ambiguity attack.
93.693.994.2
0 5 10 15
Res50, CIFAR‚àí10Test AccuracyType
EWP
OMP
Random7475
0 1 2 3 4 5
Res50, CIFAR‚àí100Type
EWP
OMP
Random
Figure A8: Random attacks on Scheme V1on ResNet-50. The x-axis is the relative sparsity of the key masks.
The solid/dashed lines represent the performance before /after random attacks.
SchemeV1on VGG-16. On CIFAR-10, the performance of the retrained model without key masks
is 88.63% when the relative sparsity of the key masks is 8%, and the performance after recovering
with random connections is only 91.96%. On Tiny-ImageNet, the performance of the retrained model
without key masks/with random key masks is 48.97%/52.86%. These results show the effectiveness
and robustness of our Scheme V1.
SchemeV2andV3on VGG-16. We further examine the effectiveness and the robustness of the
SchemeV2andV3. The QR code embedded we put in the sparse mask of VGG-16 can still be partly
decoded when the pruning ratio is 10%, while the test accuracy is 57.26% after pruning (0.7% lower)
on Tiny-ImageNet. As for the Scheme V3, the test accuracy on Tiny-ImageNet decreases to 56.44%
(over 1.5% lower) after pruning 20% of the trained model while the test accuracy on the trigger set is
still 100%. All these phenomena show the effectiveness and robustness of our Scheme V2andV3on
VGG-16.
A15
