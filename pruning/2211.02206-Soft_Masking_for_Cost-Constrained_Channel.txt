# 2211.02206.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2211.02206.pdf
# File size: 687655 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Soft Masking for Cost-Constrained Channel
Pruning
Ryan Humble1⋆, Maying Shen2, Jorge Albericio Latorre2, Eric Darve1, and
Jose Alvarez2
1Stanford University, Stanford CA 94305, USA
{ryhumble,darve }@stanford.edu
2NVIDIA, Santa Clara CA 95051, USA
{mshen,jalbericiola,josea }@nvidia.com
Abstract. Structured channel pruning has been shown to significantly
accelerate inference time for convolution neural networks (CNNs) on
modern hardware, with a relatively minor loss of network accuracy. Re-
cent works permanently zero these channels during training, which we
observe to significantly hamper final accuracy, particularly as the frac-
tion of the network being pruned increases. We propose Soft Masking for
cost-constrained Channel Pruning (SMCP) to allow pruned channels to
adaptively return to the network while simultaneously pruning towards
a target cost constraint. By adding a soft mask re-parameterization of
the weights and channel pruning from the perspective of removing input
channels, we allow gradient updates to previously pruned channels and
the opportunity for the channels to later return to the network. We then
formulate input channel pruning as a global resource allocation problem.
Our method outperforms prior works on both the ImageNet classification
and PASCAL VOC detection datasets.
Keywords: Neural network pruning, Model compression
1 Introduction
Deep neural networks have rapidly developed over the last decade and come to
dominate many traditional algorithms in a wide range of tasks. In particular,
convolutional neural networks (CNNs) have shown state-of-the-art results on a
range of computer vision tasks, including classification, detection, and segmenta-
tion. However, modern CNNs have grown in size, computation, energy require-
ment, and prediction latency, as researchers push for accuracy improvements.
Unfortunately, these models can now easily exceed the capabilities of many edge
computing devices and requirements of real-time inference tasks, such as those
found in autonomous vehicle applications.
Since neural networks have been shown to be heavily over-parameterizered
[55], one popular method for reducing the computation and prediction latency is
⋆Work performed during a NVIDIA internshiparXiv:2211.02206v1  [cs.CV]  4 Nov 2022

--- PAGE 2 ---
2 R. Humble et al.
1,000 1,500 2,000 2,500727374757677
FPSTop-1 Accuracy (%)
1 2 3 4
FLOPs (G)SMCP (Ours)
AutoSlim
MetaPruning
GReg
r x RN50
Original Model
Fig. 1. Top-1 accuracy tradeoff curve for pruning ResNet50 on the ImageNet classi-
fication dataset using a latency cost constraint. Baseline is from PyTorch [37] model
hub. Accuracy against FPS speed (left) and FLOPs (right) show the benefit of our
method, particularly at high pruning ratios. For FPS, top-right is better. For FLOPs,
top-left is better. FPS measured on an NVIDIA TITAN V GPU.
to prune (or remove) portions of the neural network, ultimately yielding a model
with fewer parameters. Due to the strict requirements for many deployment
applications, a large fraction of the parameters often must be removed; we focus
on this regime, which we refer to as the high pruning ratio regime. Towards
this aim, many pruning methods have been proposed to identify and remove
those parameters that are least important for inference [1,16,30,22,32,50,54].
Since each layer of the network involves a different computation and associated
computational burden, each parameter does not contribute equally to the final
network inference cost, typically measured as FLOPs or latency, so more recent
works have focused on pruning the network subject to explicit cost constraints.
To maximize inference speedup on modern hardware (e.g., GPUs), these works
largely focus on channel pruning [23,30,32,40,45,53].
However, in general, existing pruning works permanently remove the network
parameters along these channels, zeroing the network weights and preventing the
channel from being used during the rest of training. Particularly at high pruning
ratios, where a significant fraction of the total channels in the network must be
removed, the decisions on which channels to remove early during pruning are
potentially myopic. Moreover, as a large number of channels are removed, the
gradients to the remaining channels in each layer are significantly disrupted and
can grow quite substantially due to the batch normalization layers ubiquitous in
modern CNNs. This interferes with both network training and the identification
of which further channels to remove.
In this work, we introduce a novel channel pruning approach for neural net-
works that is particularly suitable for large pruning ratios. The core of our ap-
proach relies on regularly rewiring the network sparsity, through soft masking
of the network weights, to minimize the accuracy drop for large pruning ratios.
The introduction of soft masking allows previously pruned channels to later be

--- PAGE 3 ---
Soft Masking for Cost-Constrained Channel Pruning 3
restored to the network, instead of being permanently pruned. Additionally, to
mitigate the effect of large gradient magnitudes caused by removing many chan-
nels, we incorporate a new batch normalization scaling approach. Lastly, we
formulate channel pruning under a cost constraint as a resource allocation prob-
lem and show it can be efficiently solved. All together, we refer to this method
as Soft Masking for cost-constrained Channel Pruning (SMCP).
Our main contributions are:
1. We demonstrate that a network’s channel sparsity can be adaptively rewired,
using a soft mask re-parameterization of the network weights, and that this
requires channel pruning to be performed along input, instead of output,
channels, see Sec. 3.1.
2. We propose a new scaling technique for the batch normalization weights to
mitigate a gradient instability at high channel pruning ratios, see Sec. 3.2.
3. We perform channel pruning subject to a cost constraint by encoding it as
a resource allocation problem, which automatically allocates cost across the
network instead of relying on manual or heuristic-based layer-wise pruning
ratios. We show this allocation problem is a variant of the classic 0-1 knap-
sack problem, called the multiple-choice knapsack problem [41], which can
be efficiently solved for our experiments, see Sec. 3.3.
4. We analyze our method’s accuracy and cost improvements for the ImageNet
and PASCAL VOC datasets for ResNet, MobileNet, and SSD architectures.
We outperform prior pruning approaches, as shown in Fig. 1 and more exten-
sively in Sec. 4. In particular, at high pruning ratios for ResNet50/ResNet101
on ImageNet, SMCP can achieve up to an additional 20% speedup at the
same Top-1 accuracy level or up to a 0.6%Top-1 accuracy improvement at
the same FPS (frames per second). SMCP can also prune an SSD512 with
a ResNet50 backbone to achieve a speedup of 2.12×, exceeding the FPS
(frames per second) of the smaller SSD300-ResNet50 model by 12%, while
simultaneously improving on the mAP of the baseline model.
2 Related Work
2.1 Soft pruning
Most pruning methods start with a dense pretrained network and prune itera-
tively over a schedule to obtain a final network with the desired cost, where at
each pruning step parameters are permanently zeroed (or masked). This effec-
tively limits the model capacity as pruning occurs. Stosic and Stosic [42] argue
that preserving the larger model capacity is critical to sparse model training
by forming new paths for optimization that are not available for permanently
pruned networks; they suggest it is important to allow gradient flow to previously
pruned parameters and to rewire the sparsity occasionally.
Along these lines, several works have proposed soft pruning methods where
parameters can be pruned and later unpruned if desirable. He et al. [15] zero
weights during pruning but allows gradients to update them in an effort to

--- PAGE 4 ---
4 R. Humble et al.
maintain model capacity. Dettmers and Zettlemoyer [6], Evci et al. [9], Mostafa
and Wang [34], and Wortsman et al. [46] allow previously pruned weights to
be regrown. Kusupati et al. [21] used a soft thresholding operator to achieve
state-of-the-art results for unstructured and low-rank structured pruning. Kang
and Han [20] introduces soft channel pruning by adding a differentiable mask in
the batch normalization layers; however, their approach is limited to an implicit
cost constraint on the total number of neurons. Our approach though is most
similar to Guo et al. [11], Lin et al. [25], De Jorge et al. [19], and Zhou et al. [56],
which explicitly or implicitly use the Straight-through Estimator (STE) [2] to
adaptively prune parameters during training. The first three target unstructured
sparsity, and the last targets N:M structured sparsity. In our work, we extend
the use of the STE to channel pruning, show this requires pruning to be formu-
lated along input channels, and embed this soft masking into a general-purpose,
explicit cost-constrained formulation.
2.2 Cost-constrained and structured pruning
The goal of most pruning methods is to maximize network accuracy subject to
low memory, computation, and/or latency requirements. Although unstructured
sparsity approaches have proven to be very successfully in removing upwards of
95% of weights without affecting network accuracy [13], modern hardware has
poor support for unstructured sparsity and therefore this rarely translates to ac-
tual speedup. Therefore, it is common to choose a pruning sparsity structure that
can actually be accelerated in hardware, typically channel pruning for CNNs.
There is now some hardware support for other sparsity structures, such as the
N:M structured sparsity of [31], but we limit our focus to channel pruning in this
work. Both Li et al. [23] and Yang et al. [48,49] select the best constraint-abiding
network from a large number of candidate networks, which can be prohibitively
expensive. Yu and Huang et al. [53], Tan et al. [44], and Wu et al. [47] pose
cost-constrained optimization problems but use a greedy selection or cost-aware
importance score to approximately select the best channels to prune. Chen et
al. [3] presents a Bayesian optimization approach to determine compression hy-
perparameters that satisfy a cost constraint while maximizing network accuracy.
Liu et al. [28] linked network pruning to Neural Architecture Search (NAS),
arguing that the resulting pruned architectures are the novel contribution in-
stead of the trained weights themselves. However, most NAS methods, such as
those in [5,7,43], remain more computationally expensive than network pruning
approaches. Our approach is most similar to the concurrent work of Shen et
al. [40], called HALP, which also poses a cost-constrained resource allocation
problem. There are however several major differences. First, we reduce our allo-
cation problem to the multiple choice knapsack problem [41] and solve it with
a meet-in-the-middle algorithm, which provides both optimality guarantees and
efficient ( <1second) solutions for general cost-constraints. HALP solves their
allocation problem with a custom augmented knapsack solver, which gives no
optimality guarantees and requires significant extra computation ( 1+minute
for each pruning step on ResNet50 [14], even after a large GPU-specific neuron

--- PAGE 5 ---
Soft Masking for Cost-Constrained Channel Pruning 5
grouping step). Second, our method uses soft input channel masking as opposed
to the permanent output channel pruning of HALP; we show this change alone
yields performance gains in Sec. 4.3. Lastly, we use a new batch normalization
scaling technique to stabilize training at high pruning ratios.
2.3 Pruning impact on batch normalization layers
Channel pruning can have a significant impact on the batch normalization statis-
tics, which therefore strongly affects the network gradients to the remaining
channels. This effect is particularly pronounced at high pruning ratios, since a
large number of channels are being removed from most layers. Several pruning
methods note this phenomenon and describe mitigation strategies. Li et al. [23]
demonstrated the need to update the batch normalization statistics after prun-
ing, as they can be significantly impacted, before evaluating possible pruned
candidate networks. This approach does not however alleviate the issue of large
gradients. Instead of immediately removing pruned weights and incurring the
disruption, Wang et al. [45] slowly regularized them away, noticing significant
performance gains particularly at high pruning ratios. They do not connect this
to a sudden change in batch normalization statistics and gradients caused by
pruning. They also use a non-gradient based importance so the impact on the
importance of the remaining parameters is somewhat subdued. Since we are
adaptively adjusting the sparsity and want to preserve the ability for pruned
weights to become later unpruned, we do not want to regularize away pruned
weights. We instead adopt a scaling technique on the batch normalization weights
to stabilize training at high pruning ratios.
2.4 Parameter importance scoring
In order to decide which parameters of the network can be pruned while least
harming network accuracy, most pruning methods define an importance for each
parameter (or set of parameters) that approximates the effect of removal on
the network’s loss. Many importance scores have been proposed, largely falling
into three groups: (i) based on weight magnitude [1,12,24,27,50,54]; (ii) based
on a reconstruction-based objective [16,30]; and (iii) based on network gradi-
ents [22,32,33]. We adopt the Taylor first-order importance [32] due to its com-
putational simplicity and its strong correlation with the true impact on the
network’s loss.
3 Soft masking for cost-constrained channel pruning
We propose a novel input channel pruning approach targeted towards high prun-
ing ratios. Our method is initialized with a pretrained CNN model, and the de-
sired network cost function and target cost constraint. We first re-parameterize
the network weights with input channel masking variables, as shown in Sec. 3.1,
to enable adaptive channel pruning. Then, after a warmup period, we iteratively

--- PAGE 6 ---
6 R. Humble et al.
W(l−1)X
C(l)
inW(l)
C(l)
inK(l)K(l)Y
C(l)
out
Fig. 2. Input channel pruning of a convolutional layer. Removing an input channel
from weight W(l)∈RC(l)
out×C(l)
in×K(l)×K(l)in layer lremoves the corresponding channel
in the input feature map Xand the corresponding output channel in the previous
weight W(l−1). The shape of the output feature map Yis unaffected.
prune every rminibatches by solving a resource allocation optimization prob-
lem, discussed in Sec. 3.3, to update the channel masks. After each mask update,
we apply the batch normalization scaling described in Sec. 3.2, which stabilizes
training at high pruning ratios. Finally, we fix the masks for a cooldown and
fine-tuning period. We present the full algorithm and pseudocode in Sec. 3.4.
3.1 Soft input channel pruning
We specifically consider input channel pruning, as previously done in [16] and
shown in Fig. 2, where we mask and later remove input channels to spar-
sify the CNN. As we will shortly show, channel pruning with a soft mask re-
parameterization requires it to be done along input channels, as this approach
does not work when performing output channel pruning. This is a departure from
the many output channel pruning approaches. From a global view of network
sparsity, pruning one layer’s input channel is equivalent to pruning the previous
layer’s output channel; however, the approaches are distinct when considering
the effect on each individual layer.
For soft input channel masking, we consider a neural network with weights
W={W(l)}, where W(l)∈RC(l)
out×C(l)
in×K(l)×K(l)is the weight for layer lof the
network and has C(l)
ininput channels and C(l)
outoutput channels. To allow input
channels to be pruned and later unpruned, we introduce an input channel mask
m(l)∈ {0,1}C(l)
infor each layer l. Using these masks, we re-parameterize the
weights so that the network’s sparse weights are
fW(l)=W(l)⊙m(l). (1)
where m(l)is broadcasted to match the shape of W(l). Instead of permanently
zeroing a channel when pruning, the underlying network weights can be preserved
and merely the masks set to zero. This has two distinct advantages. First, it helps
preserve the full capacity of the original model while training towards a sparse
model. Second, by allowing channels to be restored to their original values at a
later time, poor early decisions on where to allocate the sparsity across the layers

--- PAGE 7 ---
Soft Masking for Cost-Constrained Channel Pruning 7
can be undone. This is particularly important for high pruning ratios where a
large portion of the network’s channels must be removed.
As written though, our masking definition would define the gradient with
respect to W(l)asgW(l)=gfW(l)⊙m(l). This masks the gradients as they flow
back to the completely dense weights W, rendering masked weights unused in the
forward pass and left untouched by the backward pass. Following the argument
by Stosic and Stosic [42] that updating parameters not currently participating
in the forward pass offers additional optimization paths that improve training of
sparse networks, we adopt the Straight-through Estimator (STE) [2]. The STE
has been successfully used in model quantization [38] and Ampere 2:4 structured
pruning [56] for sparse parameter updates. The STE defines the gradient as
gW(l)=gfW(l), (2)
where gradients on the sparse weights pass straight through to the underlying,
dense weights. Note that we still use the masks when computing the gradient
with respect to the input feature map of the layer.
However, for this STE to have a useful impact in a modern CNN with the
ubiquitious Conv-BN-ReLU pattern, it requires that channel pruning must be
posed as input-oriented. Since gfW(l)is defined by a matrix multiplication using
the input feature map and the gradient of the output feature map, a masked
input channel still receives non-zero gradients, except under a few edge cases. If
we had instead masked output channels, the elements of gW(l)would be either 0
or∞, depending on the value of the batch normalization bias. Alternatively, if
we instead tried to directly mask the batch normalization weight γ(l)and bias
β(l)to emulate pruning the channel, we would get gγ(l)=gβ(l)= 0due to the
ReLU. In either of these cases, the gradient gW(l)is not useful.
Finally then, for input channel pruning with soft masking, we define the im-
portance of each input channel, a proxy for the effect of removing this channel on
the network’s loss, according to the group first-order Taylor importance of [32]:
I(l)
i=X
o,r,sW(l)
o,i,r,sgW(l)
o,i,r,s(3)
where I(l)
iis the importance of the ith input channel to layer l. Under certain
conditions, this is in fact equivalent to the first-order batch normalization-based
Taylor importance of [32], as shown in the supplementary materials.
3.2 Batch normalization scaling
When channel pruning at high ratios, there are many layers where a significant
number of channels must be pruned. As a result of pruning these channels, either
by zeroing them out or by applying masking, the subsequent gradient magni-
tudes to the remaining unpruned channels can be excessively large, which we
show in the supplementary materials. We propose a batch normalization scaling
technique that adjusts the batch normalization weight γ(l)of layer lto mitigate

--- PAGE 8 ---
8 R. Humble et al.
large gradients and stabilize the network sparsity and training. Specifically, we
scale γ(l)according to the fraction of channels left unpruned by the current input
channel mask m(l)∈ {0,1}C(l)
in
γ(l)←γ(l)
origP
im(l)
i
C(l)
in. (4)
In practice, we always treat γ(l)
origas the parameter under optimization and vary
a scaling variable s(l)to adjust the weight used by the network.
Moderating gradient magnitudes is particularly consequential since we em-
ploy the gradient-based importance score shown in Eq. (3). Even without soft
masking and the STE, the large gradients cause importance accumulation in the
remaining channels as pruning iteratively proceeds, artificially inhibiting addi-
tional channels in the layer from being pruned. When employing soft masking
without this scaling technique, the large gradients cause large network sparsity
thrashing. For example, if at one pruning iteration a large number of the chan-
nels are pruned, the importance to every channel, not only those left unpruned,
is boosted by the resulting large gradient magnitudes. At the very next prun-
ing iteration, those channels appear quite important and are restored to the
network, causing other portions of the network to be pruned to still meet the
cost constraint. This can oscillate, inhibiting network convergence and the final
network accuracy. Moreover, for architectures in which pruning entire layers is
possible, such as ResNet due to the skip connections, the infinite gradient mag-
nitudes cause numerical overflow in updating the weights or even calculating the
importance of channels. As shown in our experiments in Sec. 4, the proposed
batch normalization scaling is crucial to overcome these training issues.
3.3 Cost-constrained channel pruning
At each pruning iteration, we seek to both minimize the impact on the net-
work’s loss as a result of pruning and sparsify the network towards the final
cost constraint (e.g., latency constraints). We therefore formulate pruning as a
cost-constrained importance maximization problem
max
m(2),...,m(L)LX
l=1C(l)
inX
i=1I(l)
im(l)
i (5)
s.t.LX
l=1T(l)m(l)
1,m(l+1)
1
≤τ
m(l)
1∈ P(l),
where Lis the number of layers in the network, layer lhasC(l)
ininput channels,
I(l)
iis the importance of input channel iof layer l,m(l)∈ {0,1}C(l)
inis the

--- PAGE 9 ---
Soft Masking for Cost-Constrained Channel Pruning 9
input channel mask for layer l,T(l)is the cost function for layer l,τis the cost
constraint, and P(l)is the set of permitted values for the number of channels
kept by mask m(l). By definition, m(1)
i= 1andm(L+1)
i = 1since those are the
unprunable inputs and outputs of the network. A complete derivation of Eq. (5)
can be found in the supplementary materials, as well as a discussion on how to
handle skip connections in architectures like ResNet [14].
The final constraint, on the set of permitted values P(l), is optional but use-
ful in several situations. First, it can be used to disallow pruning the entire
layer: by omitting 0fromP(l)we prevent m(l)= 0. As explained in the sup-
plementary materials, layer pruning violates a key assumption of the derivation
of Eq. (5). Second, it can be used to ensure the number of remaining chan-
nels is hardware-friendly, such as 8×multiples for GPU tensorcores [35] with
P(l)={0,8,16, . . . ,⌊C(l)
in/8⌋}.
We can further reduce this to an optimization over only the number of chan-
nelsp(l), as the most important channels will always be kept in each layer:
max
p(2),...,p(L)LX
l=1p(l)X
i=1I(l)
(i)(6)
s.t.LX
l=1T(l)
p(l),p(l+1)
≤τ
p(l)∈ P(l)
where p(l)=m(l)
1andI(l)
(i)is the ith largest value in I(l). We also approxi-
mated the constraint using the current channel counts p(l)to decouple the cost
impact of masks in consecutive layers, which is required to pose this as an ex-
ample of the following class of optimization problems.
Multiple-choice knapsack problem The optimization problem in Eq. (6)
is an example of a generalization of the classic 0-1 knapsack problem called
the multiple-choice knapsack (MCK) problem [41]. We show this connection
explicitly in the supplementary materials. The MCK problem takes the form
max
xLX
l=1nlX
i=1vl,ixl,i (7)
s.t.LX
l=1nlX
i=1cl,ixl,i≤C
xl,i∈ {0,1},nlX
i=1xl,i= 1
where Lis the number of groups, group lhas size nl, and the items have value vl,i
and cost cl,i≥0. The additional constraint relative to the classic 0-1 knapsack
problem enforces that we select exactly one item from each group.

--- PAGE 10 ---
10 R. Humble et al.
Algorithm 1 Soft masking for cost-constrained channel pruning
Inputs : Pretrained network weights W, training set D, total number of epochs E,
pruning schedule (r, Kw, Kt, Kc), target cost τ
1: Initialize masks m(l)= 1
2: Re-parameterize the weights (Eq. (1))
3: Train the network as usual for Kwepochs
4: Calculate the pruning schedule {τe}
5:forepoch e∈[Kw, E−Kc)do
6: forstepsin epoch edo
7: Perform the forward pass and backward pass, using Eqs. (1) and (2)
8: Calculate and accumulate I(l)
i(Eq. (3))
9: ifs%r= 0then
10: Solve the optimization problem (Eq. (6)) using target cost τe
11: Update the masks m(l)accordingly
12: Scale the BN weights γ(l)(Eq. (4))
13: Reset the accumulated importance
14: end if
15: end for
16:end for
17: Train the network as usual for Kcepochs
18: Apply the masks to the weights permanently
19:return Sparse network weights W
We solve Eq. (7) with a GPU-implemented meet-in-the-middle algorithm,
presented in full in the supplementary materials. Our approach generalizes the
standard meet-in-the-middle algorithm for the classic 0-1 knapsack problem,
does not require integer costs, and very efficiently solves the MCK problem for
our use cases. For example, for a ResNet50 [14], our approach solves the MCK
problem in under 1second. We present more complete timing details in the
supplementary materials.
3.4 Overall method
We present our full method in Algorithm 1. We start with a pretrained network,
layer-wise cost functions T(l), and a global cost constraint τ. We define our prun-
ing schedule by: (i) Kw: the number of warmup epochs before starting pruning;
(ii)Kt: the number of epochs after the warmup to reach the target cost τ; (iii)
r: the number of steps between recomputing the channel masks; and (iv) Kc:
the number of cooldown epochs where the masks are kept fixed. During those
Ktepochs to reach the target cost, we define intermediate cost constraints {τe}
using the exponential scheduler of [19]. Additionally, to stabilize the importance
scores, which can be noisy due to the stochastic minibatches, we calculate and
accumulate the importance score in Eq. (3) every minibatch between pruning
iterations according to the exponential momentum approach of [32].

--- PAGE 11 ---
Soft Masking for Cost-Constrained Channel Pruning 11
4 Results
We evaluate our method on both the ImageNet and PASCAL VOC benchmark
datasets3. Full details on training settings and architectures can be found in the
supplementary materials. We use a latency cost constraint, defined by a layer-
wise lookup table (LUT) as previously described in [51,40,48]. We target and
measure latency speed on a NVIDIA TITAN V GPU with cudNN V7.6.5 [4].
4.1 ImageNet results
We compare SMCP with several prior works on the ImageNet ILSVRC2012 [39]
classification dataset. In Tab. 1, we compare the results of pruning ResNet50,
ResNet101 [14], and MobileNet-V1 [17] at a number of pruning thresholds. We
refer to SMCP- X% as retaining X% of the full model’s original latency and
calculate the frames per second (FPS) and speedup of the final network. For
ResNet50, we show results for two different baseline models for a better com-
parison with prior works. The first baseline is from the PyTorch [37] model hub,
with a Top-1 accuracy of 76.15%; the second baseline is the one used as a baseline
for EagleEye [23] and has a Top-1 accuracy of 77.2%. We prune and fine-tune
following the training setup of [36].
Our method performs comparably to prior works at low pruning ratios and
outperforms them for large pruning ratios. For the PyTorch ResNet50 baseline
model, we achieve a 0.3%higher Top-1 accuracy with a higher FPS at 2G and
1G FLOPs with an additional 0.04×and0.19×speedup respectively. For the
EagleEye [23] baseline, our method produces models near 1G FLOPs that have a
0.6%higher Top-1 accuracy for nearly the same FPS or a similar Top-1 accuracy
while being 19% (or0.5×) faster. The results are similar for ResNet101, which is
based on the PyTorch model hub baseline model. At 2G FLOPs, we get a 0.3%
higher Top-1 accuracy and an additional 0.03×speedup. On the already compact
MobileNet-V1 model, where the desired pruning ratios are smaller, our method
performs comparably to prior works; at the highest pruning ratio, we show a
minor FPS improvement of 0.07×despite a higher FLOPs count, demonstrating
the ability of the optimization problem in Sec. 3.3 to choose cost-constraint
aware masks.
The benefits of our method, particularly at high pruning ratios, are possibly
more easily seen when plotting the tradeoff curve for Top-1 accuracy versus FPS,
as shown in Fig. 1 for the PyTorch baseline and Fig. 3 for the EagleEye baseline.
For example in Fig. 3, at the 75% latency reduction level (or 3102 FPS), our
method outperforms the nearest HALP [40] model with a 0.2%higher Top-1
accuracy and a 15% higher FPS; compared to EagleEye [23], we show a 0.23%
higher Top-1 accuracy and a 26% higher FPS.
Moreover, our method can aggressively prune large, over-parameterized mod-
els to outperform smaller unpruned models. As shown in Tab. 1 and Fig. 3, a 50%
pruned ResNet101 achieves a 1.6%Top-1 improvement over a baseline ResNet50,
3Our code can be accessed at https://github.com/NVlabs/SMCP .

--- PAGE 12 ---
12 R. Humble et al.
Table 1. Pruning results on the ImageNet classification dataset considering two dif-
ferent ResNet50 baseline models as well as ResNet101 and MobileNetV1. We group
results by those with similar FLOP counts, and refer to SMCP- X% as retaining X%
of the full model’s original latency. Results for prior works are as shown in [40].
MethodFLOPs Top1 Top5 FPSSpeedup(G) (%) (%) (im/s)
ResNet50
No pruning 4 .1 76 .2 92.87 1019 1 ×
ThiNet-70 [30] 2 .9 75 .8 90.67 - -
AutoSlim [53] 3 .0 76 .0 - 1215 1 .14×
MetaPruning [28] 3 .0 76 .2 - - -
GReg-1 [45] 2 .7 76 .3 - 1171 1 .15×
HALP-80% [40] 3 .177.2 93.471256 1 .23×
SMCP- 80%(Ours) 3.0 77 .1 93.431292 1 .27×
0.75×ResNet50 [14] 2 .3 74 .8 - 1467 1 .44×
ThiNet-50 [30] 2 .1 74 .7 90.02 - -
AutoSlim [53] 2 .0 75 .6 - 1592 1 .56×
MetaPruning [28] 2 .0 75 .4 - 1604 1 .58×
GBN [52] 2 .4 76 .2 92.83 - -
GReg-2 [45] 1 .8 75 .4 - 1414 1 .39×
HALP-55% [40] 2 .0 76 .5 93.05 1630 1 .60×
SMCP- 55%(Ours) 2.076.8 93.22 1673 1 .64×
0.50×ResNet50 [14] 1 .1 72 .0 - 2498 2 .45×
ThiNet-30 [30] 1 .2 72 .1 88.30 - -
AutoSlim [53] 1 .0 74 .0 - 2390 2 .45×
MetaPruning [28] 1 .0 73 .4 - 2381 2 .34×
GReg-2 [45] 1 .3 73 .9 - 1514 1 .49×
HALP-30% [40] 1 .0 74 .3 91.81 2755 2 .70×
SMCP- 30%(Ours) 1.074.6 92.00 2947 2 .89×
ResNet50 - EagleEye [23] baseline
No pruning 4 .1 77 .2 93.70 1019 1 ×
EagleEye-3G [23] 3 .0 77 .1 93.37 1165 1 .14×
HALP-80% [40] 3 .0 77 .5 93.60 1203 1 .18×
SMCP- 80%(Ours) 3.177.6 93.61 1263 1 .23×
EagleEye-2G [23] 2 .1 76 .4 92.89 1471 1 .44×
HALP-55% [40] 2 .176.693.16 1672 1.64×
SMCP- 50%(Ours) 1.976.6 93.17 1706 1 .67×
EagleEye-1G [23] 1 .0 74 .2 91.77 2429 2 .38×
HALP-30% [40] 1 .2 74 .5 91.87 2597 2 .55×
SMCP- 30%(Ours) 1.175.1 92.292589 2 .51×
SMCP- 25%(Ours) 0.9 74 .4 91.983102 3 .01×MethodFLOPs Top1 FPSSpeedup(G) (%) (im/s)
ResNet101
No pruning 7 .8 77 .4 620 1 ×
Taylor-75% [32] 4 .7 77 .4 750 1 .21×
HALP-60% [40] 4 .378.3847 1 .37×
SMCP- 60%(Ours) 4.0 78 .1 951 1 .53×
HALP-50% [40] 3.677.8 994 1 .60×
SMCP- 50%(Ours) 3 .677.81016 1 .64×
Taylor-55% [32] 2 .9 76 .0 908 1 .47×
HALP-40% [40] 2 .7 77 .2 1180 1 .90×
SMCP- 30%(Ours) 2.677.31273 2 .05×
HALP-30% [40] 2.076.5 1521 2 .45×
SMCP- 25%(Ours) 2 .076.81535 2 .48×
MethodFLOPs Top1 FPSSpeedup(M) (%) (im/s)
MobileNet-V1
No pruning 569 72 .6 3415 1 ×
0.75×MobileNetV1 325 68 .4 4678 1 .37×
NetAdapt [48] 284 69 .1 - -
MetaPruning [28] 316 70 .9 4838 1 .42×
EagleEye [23] 284 70 .9 5020 1 .47×
HALP-60% [40] 297 71.35754 1 .68×
SMCP- 60%(Ours) 356 71 .05870 1 .72×
MetaPruning [28] 142 66 .1 7050 2 .06×
AutoSlim [53] 150 67 .9 7743 2 .27×
HALP-42% [40] 171 68.37940 2 .32×
SMCP- 40%(Ours) 208 68.3 8163 2 .39×
with no performance loss, and a 80% pruned ResNet50 achieves a similar Top-1
to an unpruned MobileNet-V1 while achieving a 10% FPS speedup.
Lastly, the accuracy and performance gains are in part due to the final net-
work architecture chosen by our method. In particular, since we solve a global
resource allocation problem during training, our method automatically deter-
mines the layer-wise pruning ratios for the given cost function and constraint.
For example, on ResNet50, we find that SMCP is aggressive in pruning the
early convolution layers and leaves the later layers better preserved; we provide
additional analysis and figures in the supplementary materials.

--- PAGE 13 ---
Soft Masking for Cost-Constrained Channel Pruning 13
1,000 1,500 2,000 2,500 3,000 3,500737475767778
FPSTop-1 Accuracy (%)SMCP (Ours)
HALP
EagleEye
Original Model
40 60 80 100 120 140 16076777879
SSD512-RN50
SSD512-RN50-slimRetinaNet-RN50
SSD300-VGG16
SSD300-RN50
FPSmAP (%)SMCP (Ours)
Fig. 3. (Left) Top-1 accuracy tradeoff curve for pruning ResNet50 on the ImageNet
classification dataset using a latency cost constraint. Baseline model is from Eagle-
Eye [23]. (Right) mAP accuracy tradeoff curve for pruning SSD512-RN50 on the PAS-
CAL VOC object detection dataset using a latency cost constraint. Top-right is better.
4.2 PASCAL VOC results
To analyze our method beyond image classification, we also analyze SMCP on the
PASCAL VOC object detection dataset [10]. Specifically, we consider whether
a large model, such as SSD512 [26] with a ResNet50 backbone, can be pruned
at a high ratio to match the FPS of smaller models while retaining a superior
mAP (mean average precision). We use the “07+12” train and test setup of [26]
and prune both the backbone and feature layers.
As shown in Fig. 3, our method can prune an SSD512-RN50 to have a higher
mAP than the pretrained model and a faster FPS than the much smaller SSD300-
RN50 model, again showing the ability of our method to aggressively prune
large over-parameterized models to outperform smaller models. In particular,
our fastest pruned model has a 2.63point higher mAP score while achieving
12% higher FPS. Critically, the latency reduction to achieve this is 75%, demon-
strating the strength of our approach in the high pruning ratio regime. We also
compare to and outperform a number of other common detector models.
4.3 Ablation study
We also study the effect of our contributions on the accuracy results shown above,
specifically at high pruning ratios. We run our method again on the ImageNet
classification dataset, starting from the ResNet50 EagleEye [23] baseline. We first
remove the batch normalization scaling technique from Sec. 3.2 while keeping the
soft input channel masking re-parameterization of Sec. 3.1. We then additionally
remove the soft input channel masking, reverting to permanent pruning. We keep
the solver and latency constraint in Sec. 3.3 unchanged. The ablation results are
shown in Fig. 4. Removing the batch normalization scaling generally leads to
marginally worse results, due to the training instability described in Sec. 3.2.
Additionally removing the soft input masking, thereby using permanent channel
pruning, degrades accuracy and performance further.

--- PAGE 14 ---
14 R. Humble et al.
1,500 2,000 2,500 3,000 3,5007374757677
FPSTop-1 Accuracy (%)SMCP
Without BN scaling
Without soft input masking and BN scaling
Fig. 4. Ablation study for SMCP at high pruning ratios on ResNet50 using the Eagle-
Eye [23] baseline. We remove consecutively two major components of our method, soft
input masking and batch normalization scaling, and observed worse Top-1 accuracy
and FPS than the full SMCP method.
4.4 Choice of latency cost constraint
Although our cost-constrained formulation is general to any number of cost func-
tions, the benefits of our approach are most pronounced under challenging, non-
linear latency cost landscapes (i.e., latency cliffs for GPUs). Linear constraints
(i.e., parameter/FLOP constraints) lessen the need for soft masking and an ef-
ficient and global resource allocation: removed channels are more likely to stay
pruned once removed and the number of remaining channels in each layer tends
to change slowly. Despite training against a latency constraint, Tab. 1 shows
that SMCP is comparable to or even outperforms previous methods under low
FLOP constraints.
5 Conclusion
By applying channel pruning, modern CNNs can be significantly accelerated,
with a smaller memory footprint, computational cost, and inference time. In
this work, we presented a novel structured input channel pruning approach,
called SMCP, that combines soft masking of input channels, a batch normal-
ization scaling technique, and the solution to a resource allocation problem to
outperform prior works. We motivate the use of each component of our method
and demonstrate their effectiveness on both the ImageNet and PASCAL VOC
datasets. Although we only consider channel pruning in this work, our approach
can be extended to jointly consider both channel and N:M structured prun-
ing [31] to satisfy an explicit cost-constraint. This can be viewed as an extension
of both this work and that of [56] and is left for a future work.

--- PAGE 15 ---
Soft Masking for Cost-Constrained Channel Pruning 15
References
1. Alvarez, J.M., Salzmann, M.: Learning the number of neurons in deep networks.
In: NeurIPS. pp. 2262–2270 (2016) 2, 5
2. Bengio, Y., L´ eonard, N., Courville, A.C.: Estimating or propagating gradients
through stochastic neurons for conditional computation. CoRR abs/1308.3432
(2013) 4, 7
3. Chen, C., Tung, F., Vedula, N., Mori, G.: Constraint-aware deep neural network
compression. In: ECCV. pp. 409–424 (2018) 4
4. Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catan-
zaro, B., Shelhamer, E.: cudnn: Efficient primitives for deep learning. CoRR
abs/1410.0759 (2014) 11
5. Dai, X., Zhang, P., Wu, B., Yin, H., Sun, F., Wang, Y., Dukhan, M., Hu, Y., Wu,
Y., Jia, Y., Vajda, P., Uyttendaele, M., Jha, N.K.: Chamnet: Towards efficient
network design through platform-aware model adaptation. In: CVPR. pp. 11398–
11407 (2019) 4
6. Dettmers, T., Zettlemoyer, L.: Sparse networks from scratch: Faster training with-
out losing performance. CoRR abs/1907.04840 (2019) 4
7. Dong, J., Cheng, A., Juan, D., Wei, W., Sun, M.: Dpp-net: Device-aware progres-
sive search for pareto-optimal neural architectures. In: ECCV. pp. 540–555 (2018)
4
8. Dudzi´ nski, K., Walukiewicz, S.: Exact methods for the knapsack problem and its
generalizations. European Journal of Operational Research 28(1), 3–21 (1987) 24,
25, 27
9. Evci, U., Gale, T., Menick, J., Castro, P.S., Elsen, E.: Rigging the lottery: Making
all tickets winners. In: ICML. pp. 2943–2952 (2020) 4
10. Everingham, M., Gool, L.V., Williams, C.K.I., Winn, J.M., Zisserman, A.: The
pascal visual object classes (VOC) challenge. Int. J. Comput. Vis. 88(2), 303–338
(2010) 13
11. Guo, Y., Yao, A., Chen, Y.: Dynamic network surgery for efficient dnns. In: Lee,
D.D., Sugiyama, M., von Luxburg, U., Guyon, I., Garnett, R. (eds.) NeurIPS. pp.
1379–1387 (2016) 4
12. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In: ICLR (2016) 5
13. Han, S., Pool, J., Tran, J., Dally, W.J.: Learning both weights and connections for
efficient neural network. In: NeurIPS. pp. 1135–1143 (2015) 4
14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR. pp. 770–778 (2016) 4, 9, 10, 11, 12
15. He, Y., Kang, G., Dong, X., Fu, Y., Yang, Y.: Soft filter pruning for accelerating
deep convolutional neural networks. In: IJCAI. pp. 2234–2240 (2018) 3
16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural
networks. In: ICCV. pp. 1398–1406 (2017) 2, 5, 6
17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-
dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for
mobile vision applications. CoRR abs/1704.04861 (2017) 11
18. Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I.,
Wojna, Z., Song, Y., Guadarrama, S., Murphy, K.: Speed/accuracy trade-offs for
modern convolutional object detectors. In: CVPR. pp. 3296–3297 (2017) 18
19. de Jorge, P., Sanyal, A., Behl, H.S., Torr, P.H.S., Rogez, G., Dokania, P.K.: Pro-
gressive skeletonization: Trimming more fat from a network at initialization. In:
ICLR (2021) 4, 10

--- PAGE 16 ---
16 R. Humble et al.
20. Kang, M., Han, B.: Operation-aware soft channel pruning using differentiable
masks. In: ICML. pp. 5122–5131 (2020) 4
21. Kusupati, A., Ramanujan, V., Somani, R., Wortsman, M., Jain, P., Kakade, S.M.,
Farhadi, A.: Soft threshold weight reparameterization for learnable sparsity. In:
ICML. pp. 5544–5555 (2020) 4
22. LeCun, Y., Denker, J.S., Solla, S.A.: Optimal brain damage. In: NeurIPS. pp.
598–605 (1989) 2, 5
23. Li, B., Wu, B., Su, J., Wang, G.: Eagleeye: Fast sub-net evaluation for efficient
neural network pruning. In: ECCV. pp. 639–654 (2020) 2, 4, 5, 11, 12, 13, 14, 19,
27, 30
24. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning filters for efficient
convnets. In: ICLR (2017) 5
25. Lin, T., Stich, S.U., Barba, L., Dmitriev, D., Jaggi, M.: Dynamic model pruning
with feedback. In: ICLR (2020) 4
26. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.E., Fu, C., Berg, A.C.:
SSD: single shot multibox detector. In: ECCV. vol. 9905, pp. 21–37 (2016) 13, 18
27. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient con-
volutional networks through network slimming. In: ICCV. pp. 2755–2763 (2017)
5
28. Liu, Z., Sun, M., Zhou, T., Huang, G., Darrell, T.: Rethinking the value of network
pruning. In: ICLR (2019) 4, 12
29. Loshchilov, I., Hutter, F.: SGDR: stochastic gradient descent with warm restarts.
In: 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net (2017)
18
30. Luo, J., Wu, J., Lin, W.: Thinet: A filter level pruning method for deep neural
network compression. In: ICCV. pp. 5068–5076 (2017) 2, 5, 12
31. Mishra, A.K., Latorre, J.A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G.,
Yu, C., Micikevicius, P.: Accelerating sparse deep neural networks. CoRR
abs/2104.08378 (2021) 4, 14
32. Molchanov, P., Mallya, A., Tyree, S., Frosio, I., Kautz, J.: Importance estimation
for neural network pruning. In: CVPR. pp. 11264–11272 (2019) 2, 5, 7, 10, 12, 19
33. Molchanov, P., Tyree, S., Karras, T., Aila, T., Kautz, J.: Pruning convolutional
neural networks for resource efficient inference. In: ICLR (2017) 5
34. Mostafa, H., Wang, X.: Parameter efficient training of deep convolutional neural
networks by dynamic sparse reparameterization. In: ICML. pp. 4646–4655 (2019)
4
35. NVIDIA Deep Learning Performance Guide: Convolutional Layers
User Guide. https://docs.nvidia.com/deeplearning/performance/
dl-performance-convolutional/index.html , accessed: 2021-11-15 9
36. NVIDIA Deep Learning Examples: ResNet50 v1.5 For Pytorch. https:
//github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/
Classification/ConvNets/resnet50v1.5/README.md , accessed: 2021-11-15
11, 18
37. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,
Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K¨ opf, A., Yang, E.Z., DeVito,
Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala,
S.: Pytorch: An imperative style, high-performance deep learning library. In: Wal-
lach, H.M., Larochelle, H., Beygelzimer, A., d’Alch´ e-Buc, F., Fox, E.B., Garnett,
R. (eds.) NeurIPS 2019. pp. 8024–8035 (2019) 2, 11, 18

--- PAGE 17 ---
Soft Masking for Cost-Constrained Channel Pruning 17
38. Rastegari, M., Ordonez, V., Redmon, J., Farhadi, A.: Xnor-net: Imagenet classifi-
cation using binary convolutional neural networks. In: ECCV. pp. 525–542 (2016)
7
39. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M.S., Berg, A.C., Fei-Fei, L.: Imagenet large
scale visual recognition challenge. Int. J. Comput. Vis. 115(3), 211–252 (2015) 11
40. Shen, M., Yin, H., Molchanov, P., Mao, L., Liu, J., Alvarez, J.M.: HALP: hardware-
aware latency pruning. CoRR abs/2110.10811 (2021) 2, 4, 11, 12
41. Sinha, P., Zoltners, A.A.: The multiple-choice knapsack problem. Oper. Res. 27(3),
503–515 (1979) 3, 4, 9, 23, 24
42. Stosic, D., Stosic, D.: Search spaces for neural model training. CoRR
abs/2105.12920 (2021) 3, 7
43. Su, X., You, S., Wang, F., Qian, C., Zhang, C., Xu, C.: Bcnet: Searching for network
width with bilaterally coupled network. In: CVPR. pp. 2175–2184 (2021) 4
44. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le, Q.V.:
Mnasnet: Platform-aware neural architecture search for mobile. In: CVPR. pp.
2820–2828 (2019) 4
45. Wang, H., Qin, C., Zhang, Y., Fu, Y.: Neural pruning via growing regularization.
In: ICLR (2021) 2, 5, 12
46. Wortsman, M., Farhadi, A., Rastegari, M.: Discovering neural wirings. In: NeurIPS.
pp. 2680–2690 (2019) 4
47. Wu, Y., Liu, C., Chen, B., Chien, S.: Constraint-aware importance estimation for
global filter pruning under multiple resource constraints. In: CVPR. pp. 2935–2943
(2020) 4
48. Yang, T., Howard, A.G., Chen, B., Zhang, X., Go, A., Sandler, M., Sze, V., Adam,
H.: Netadapt: Platform-aware neural network adaptation for mobile applications.
In: ECCV. pp. 289–304 (2018) 4, 11, 12
49. Yang, T., Liao, Y., Sze, V.: Netadaptv2: Efficient neural architecture search with
fast super-network training and architecture optimization. In: CVPR. pp. 2402–
2411. Computer Vision Foundation / IEEE (2021) 4
50. Ye, J., Lu, X., Lin, Z., Wang, J.Z.: Rethinking the smaller-norm-less-informative
assumption in channel pruning of convolution layers. In: ICLR (2018) 2, 5
51. Yin, H., Molchanov, P., Alvarez, J.M., Li, Z., Mallya, A., Hoiem, D., Jha, N.K.,
Kautz, J.: Dreaming to distill: Data-free knowledge transfer via deepinversion. In:
CVPR. pp. 8712–8721 (2020) 11
52. You, Z., Yan, K., Ye, J., Ma, M., Wang, P.: Gate decorator: Global filter pruning
method for accelerating deep convolutional neural networks. In: NeurIPS. pp. 2130–
2141 (2019) 12
53. Yu, J., Huang, T.S.: Network slimming by slimmable networks: Towards one-shot
architecture search for channel numbers. CoRR abs/1903.11728 (2019) 2, 4, 12
54. Yu, R., Li, A., Chen, C., Lai, J., Morariu, V.I., Han, X., Gao, M., Lin, C., Davis,
L.S.: NISP: pruning networks using neuron importance score propagation. In:
CVPR. pp. 9194–9203 (2018) 2, 5
55. Zhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O.: Understanding deep
learning requires rethinking generalization. In: ICLR (2017) 1
56. Zhou, A., Ma, Y., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., Li, H.: Learning
N: M fine-grained structured sparse neural networks from scratch. In: ICLR (2021)
4, 7, 14

--- PAGE 18 ---
18 R. Humble et al.
6 Supplementary Materials
6.1 Experimental settings
We define our networks and pruning operations using PyTorch [37] and run our
experiments across 8NVIDIA Tesla V100 GPUs using automatic mixed precision
and DDP (distributed data parallel) training.
ImageNet experiments For all of the ImageNet experiments, across ResNet50,
ResNet101, and MobileNet-v1, we follow NVIDIA’s ResNet50 training setup [36]
with a batch size of 256per GPU, a linear learning rate warmup period of 8
epochs, a cosine decay learning rate schedule [29], and a 90epoch training time.
We start pruning after Kw= 10 epochs, reach the target Kt= 30 epochs
later at epoch 40, and allow the masks to continue to refine until fixing the
masks for the final Kc= 45 epochs. We recompute the masks every r= 80
steps. (We conduct a small sensitivity study on these hyperparameters later in
the supplementary materials). To ensure a GPU-friendly setting of the masks,
we set P(l)={i:i%8 = 0 , i≤C(l)
in}for every layer but the first two. We prohibit
any pruning of the first convolution by setting P(1)={3},P(2)={C(2)
in}and
prohibit any layer pruning by ensuring 0̸∈ P(l).
We build the lookup table for ResNet50 and ResNet101 with a batch size of
256and MobileNet-V1 with a batch size of 512; we assess the latency of the final
pruned networks under these settings as well.
PASCAL VOC experiments We use the SSD512 model described in [26],
swapping the VGG16 backbone for a ResNet50 backbone. As in [18], we keep
only the first three stages of the convolutions and change the strides in the third
to stage to 1×1. We then add 6pairs of feature detection layers as in [26] and the
localization and confidence heads to generate the boxes and their scores. We train
for800epochs with a batch size of 16per GPU, using PyTorch’s SyncBatchNorm
to synchronize the batch normalization statistics. We use a learning rate of 8e-3
for the total batch size 128, a linear warmup to that rate over 50epochs, and
reduce the rate by a multiple of 3/8,1/3,2/5,1/10at600,700,740,770epochs
respectively. For network biases, we double the learning rate. We also set the
weight decay to 2e-3except for the batch normalization parameters. We use the
SGD optimizer with a momentum of 0.9
We start pruning after Kw= 60 epochs, reach the target Kt= 250 epochs
later at epoch 310, and allow the masks to continue to refine until fixing the
masks for the final Kc= 350 epochs. We recompute the masks every r= 100
steps. We use the same rules for P(l)as with the ImageNet experiments.
We assess the latency of the final pruned SSD512-ResNet50 at a batch size
of1for comparison with other detectors.

--- PAGE 19 ---
Soft Masking for Cost-Constrained Channel Pruning 19
6.2 ResNet50 layer-wise pruning ratios
Since we solve a global resource allocation problem, there are no preset layer-wise
pruning ratios. We therefore analyze the final pruning ratios for each layer to
derive insights into our method. Fig. 5 plots the fraction of channels remaining
in each layer relative to the original unpruned model. Generally, we find that
SMCP prunes heavily in the early layers of the network and preserves more
channels in the later layers of the network and in the second convolution layer
in each residual block throughout (i.e., the conv2 layers).
We also compare our pruning ratios to the EagleEye [23] models of compa-
rable FLOPs in Fig. 6. The general pattern of pruning heavily early and lighter
later still holds. In particular, at the high pruning ratios, SMCP is able to keep
a much larger number of channels in the later layers of the network, often twice
as many as the EagleEye model, which seems to convey a Top-1 accuracy im-
provement as shown in Tab. 1.
6.3 Pruning schedule hyperparameters
Our algorithm only introduces new additional hyperparameters for the pruning
schedule: the number of steps between recomputing the masks rand the target
schedule defined by ( Kw, Kt, Kc). To determine if our results are sensitive to
these choices, we train a ResNet50 model at a 70% reduction while varying rfor
a fixed schedule and varying the schedule for fixed r; the results are shown in
Fig. 7. We find that varying the target schedule, from the original schedule of
(10,30,45) to (10 ,35,45),(10,40,35),(5,20,60),(5,30,50), has little impact on
the final accuracy and FPS, with Top-1 by at most 0 .07% with a FPS difference at
most 46 FPS. Changing rhas a much bigger impact. To understand why, recall
that in order to define the knapsack-like optimization problem in Eq. (6) we
had to make an approximation T(l) 
p(l), p(l−1)
≈ T(l)
p(l),p(l−1)
. Therefore
solving the optimization problem less often results in a worse approximation of
the loss landscape and a final model cost that can vary more from the desired
cost. Nonetheless the resulting pruned model is quite accurate for its cost and
lives on a Pareto frontier of possible models of different costs.
6.4 Equivalence of Taylor FO importance scores
Theorem 1. For a network composed of Conv-BN-ReLU blocks and without a
mask re-parameterization, the first-order Taylor importance on the batch nor-
malization weight and bias (Taylor-FO-BN [32]) of layer lis equivalent to a
first-order Taylor importance on the weights of the downstream input channel in
layer l+ 1.
γ(l)
igγ(l)
i+β(l)
igβ(l)
i=X
o,r,sW(l+1)
o,i,r,sgW(l+1)
o,i,r,s(8)
Proof. Let the output BN layer be parameterized by γandβand the following
convolution layer be parameterized by W. Suppose yis the input to the BN layer,

--- PAGE 20 ---
20 R. Humble et al.
zis the output of the BN layer and x=ReLU (y)is the input to the convolution
layer. From the definition of a batch normalization layer, it follows that the
gradient of the batch normalization weight and bias are given by
gγi=X
m,ngzi,m,nˆyi,m,n (9)
gβi=X
m,ngzi,m,n (10)
where ˆyis the value of yafter normalization. Then,
γigγi+βigβi=X
m,n(γiˆyi,m,n +βi)gzi,m,n (11)
=X
m,nzi,m,ngzi,m,n (12)
=X
m,nxi,m,ngxi,m,n (13)
=X
o,r,sWo,i,r,sgWo,i,r,s (14)
where the last step proceeds from the definition of the convolution.
Corollary 1. This holds even in the presence of skip connections in architec-
tures like ResNet.
Proof. Suppose for example that there are kBN layers whose outputs z(k)are
added to create z, applied through a ReLU nonlinearity to create x, and then
distributed as x(j)=xtojdifferent convolution layers with weights W(j). Using
results from the proof of Theorem 1, we have
X
kγ(k)
igγ(k)
i+β(k)
igβ(k)
i=X
k,m,nz(k)
i,m,ngz(k)
i,m,n(15)
=X
k,m,nz(k)
i,m,ngzi,m,n (16)
=X
m,nzi,m,ngzi,m,n (17)
=X
m,nxi,m,ngxi,m,n (18)
=X
j,m,nx(j)
i,m,ngx(j)
i,m,n(19)
=X
j,o,r,sW(j)
o,i,r,sgW(j)
o,i,r,s(20)
where we use gradient rules for both the addition and ReLU operations to sim-
plify.

--- PAGE 21 ---
Soft Masking for Cost-Constrained Channel Pruning 21
Corollary 2. Theorem 1 also holds under hard masking (masking without the
STE) and for unpruned channels under soft masking. For pruned channels under
soft masking, γigγi+βigβi= 0since it is the sparse weights that determine the
gradient of input feature map but the dense weights receive a dense gradient
update.
6.5 Exploding gradients without batch normalization scaling
Let’s consider the popular Conv-BN pattern, with weights W, γ, β and statistics
µ(t), σ(t)at step t. Suppose that a fraction 1−αof the input channels are
pruned at the end of one training step. During the next training step, there are
fewer non-zero weights, which can intuitively cause the batch variance to shrink:
σ(t+1)< σ(t). This does not affect the forward pass significantly, in that the
output of the BN layer is still roughly N 
β, γ2
, but it affects the gradients
quite noticeably. In fact, this causes the gradients flowing to the remaining,
unpruned channels to be boosted quite significantly as αnears 1.
Concretely, let the Conv-BN pair be characterized by z=W∗x,ˆz=z−µ
σ, y=
γˆz+β. The gradient to the intermediate feature map zis equal to
gzo,h,w =γo
σo
gyo,h,w−1
Sgβo−1
Sˆzo,h,wgγo
(21)
where Sis the size of each channel in the feature map y. When σoshrinks, the
gradient magnitude is inversely increased, causing gzo,h,w at time t+ 1to be
roughly a factor of st=σ(t)/σ(t+1)larger than it was at time t. In the extreme
case with α→1, we can get exploding gradients, with a degeneracy at α= 1
which corresponds to layer pruning.
6.6 Derivation of the cost-constrained optimization problem
We start from the idea cost-constrained network objective for neural network
f:X→Y
arg min
WL(W,D) s.t.T(f(W, x i))≤τ (22)
where Lis the network loss function, W={W(l)}are the network’s weights,
D={(xi, yi)}is the training set, Tis the network’s cost function, and τis the
cost constraint. Using the input channel masks M={m(l)}and sparse weights
fW(l)=W(l)⊙m(l), as described in Sec. 3.1, we get a joint optimization problem
over the weights and masks
arg min
W,ML
fW,D
s.t.T(M)≤τ (23)
where the cost constraint is now only a function of the masks. This is a com-
binatorially hard discrete optimization problem over the masks, so to make it

--- PAGE 22 ---
22 R. Humble et al.
tractable, we make several changes. First, we replace the loss minimization ob-
jective with an importance maximization objective and linearize it with a per-
channel importance score I(l)
idefined in Eq. (3). This assumes, that despite the
nonlinearities of f, the importance score I(l)
iis a good approximation of the ef-
fect of removing that channel from the network. This makes the objective linear
in the masking variables:
arg max
MLX
l=1C(l)
inX
i=1I(l)
im(l)
is.t.T(M)≤τ (24)
Second, we assume the cost function Tis layer-wise separable into constituent
cost functions T(l)that depend only on the number of input and output channel
masks for that layer. The output channel mask is defined by the input channel
mask of the downstream layer. This yields
arg max
MLX
l=1C(l)
inX
i=1I(l)
im(l)
i (25)
s.t.LX
l=1T(l)m(l)
1,m(l+1))
1
≤τ
Lastly, we add the additional constraint on the allowable values for the number
of input channels,m(l)
1∈ P(l), to get Eq. (5).
Skip connections For architectures that have skip connections or other struc-
tural branching features, the optimization problem in Eq. (5) needs one addi-
tional constraint. Specifically, all layers that share the same input channels must
prune identically to one another. For example, in a ResNet bottleneck block
that performs downsampling, both the downsample convolution and the first
convolution in the branch share the same input channels. Therefore, their masks
m(down )andm(conv 1)must be equal to each. More formally, let gkbe a group
of layers that share input channels. Then, we must add the constraint
m(l)=m(gk)∀l∈gk (26)

--- PAGE 23 ---
Soft Masking for Cost-Constrained Channel Pruning 23
for every group gkof layers in the network. For G={gk}, the optimization
problem reduces to choice of masks over each group instead of each layer
arg max
M|G|X
k=1C(gk)
inX
i=1
X
l∈gkI(l)
i
m(gk)
i (27)
s.t.|G|X
k=1X
l∈gkT(l)m(gk)
1,m(l+1))
1
≤τ
m(gk)
1∈[
l∈gkP(l)
m(l)=m(gk)∀l∈gk
where we decouple the cost impacts of masks in consecutive layers as in Eq. (6).
This is a simply a generalization of Eq. (5) where we consider groups of layers
together instead of each layer individually.
6.7 Cost-constrained channel pruning as multiple-choice knapsack
Our cost-constrained resource allocation problem in Eq. (6) is an example of a
general class called the multiple-choice knapsack problem of [41]. We now show
this connection explicitly. We start with Eq. (6), reproduced here for readability,
max
p(2),...,p(L)LX
l=1p(l)X
i=1I(l)
(i)
s.t.LX
l=1T(l)
p(l),p(l+1)
≤τ
p(l)∈ P(l).
Now, we define
vl,j=jX
i=1I(l)
(i)(28)
cl,j=T(l)(j,p(l+1)) (29)
xl,j=1
j=p(l)
(30)

--- PAGE 24 ---
24 R. Humble et al.
where 1(·)is the indicator function. This yields the the equivalent optimization
problem
max
xLX
l=1nlX
j=1vl,jxl,j (31)
s.t.LX
l=1nlX
j=1cl,jxl,j≤τ
xl,j∈ {0,1},nlX
j=1xl,j= 1
X
j∈P(l)xl,j= 1
where nl=C(l)
inis the maximum number of input channels for layer l. By
trimming the problem to only the values vl,jand costs cl,jwhere j∈ P(l),
we recover the general form of the multiple choice knapsack problem shown
in Eq. (7).
6.8 Solving the multiple-choice knapsack problem
The standard method for solving the classic 0-1 knapsack problem is a dy-
namic programming algorithm. For the multiple-choice knapsack problem [41],
Dudzi´ nski and Walukiewicz [8] define a similar dynamic programming solution.
It requires integer costs cl,i, C∈Z≥0and has solution runtime complexity O(nC)
where n=P
gnland space complexity O(GC)(in order to recover the items
used) when cl,i, C∈Z≥0. When the costs are not integer or are incredibly
large integers, this becomes intractable unless a scaling and rounding step is
performed. Even still, for problems with very sparse values vl,iand costs cl,i,
the dynamic programming approach is inefficient. We instead solve the MCK
problem using a GPU-implemented generalization of the meet-in-the-middle al-
gorithm used for the classic 0-1 knapsack problem. The full algorithm is defined
in Algorithm 2. The benefit of this approach is we only store feasible values
and costs and aggressively reject suboptimal solutions with the condense step.
However, the runtime and space complexities are asymptotically much worse in
general, as shown in Theorem 2.
Theorem 2. The meet-in-the-middle algorithm in Algorithm 2 has worst case
runtime complexity O 
LBLlog(B)
and space complexity O 
BL
where B=
max lnland we assume B≫L.
Proof. We start by deriving the complexity of the merge function for a group of
sizeMand another of size N.
Tmerge (M, N ) =O(MN) +Tcondense (MN) (32)
=O(MN log(MN)) (33)

--- PAGE 25 ---
Soft Masking for Cost-Constrained Channel Pruning 25
since the condense function requires and is dominated by a sort of the values.
The runtime complexity of the full multiple-choice knapsack solver can be
bounded by assuming every group takes the maximum size B. Without loss of
generality, we also assume L= 2jfor some j. Then, for a multiple-choice knap-
sack (MCK) problem with Lgroups of size B, we have
Tmck(L, B) =TmckL
2, B2
+L
2Tmerge (B, B) (34)
≤Tmck
1, B2j
(35)
+jX
i=1L
2iTmerge
B2i−1, B2i−1
(36)
≤O
B2j
(37)
+jX
i=1L
2i2iO
B2ilog(B)
(38)
≤O 
LBLlog(B)
(39)
since the merge can create a new item for every combination of items in the two
merging groups (squaring the size of the group), Tmck(1, B) =O(B), and the
runtime is dominated by the final merge of groups.
The space complexity proceeds similarly, except for the logfactor which is
due to the sort in the condense step.
Corollary 3. The runtime and space complexity of Algorithm 2 can be improved
toO 
LBL/2log(B)
andO 
BL/2
respectively by replacing the final merge with
aO 
BL/2log 
BL/2
sort and sweep over the last two groups.
Since the multiple-choice knapsack problem reverts to the original knapsack
problem under B= 2, vl,1= 0, cl,1= 0, Algorithm 2 recovers the runtime and
space complexities of the standard meet-in-the-middle knapsack solver, which
areO 
L2L/2
andO 
2L/2
respectively, using the sort and sweep improvement
of Corollary 3.
6.9 Multiple-choice knapsack solve effort
We present representative timings for the solve effort of the meet-in-the-middle
multiple-choice knapsack solver in Algorithm 2. When pruning a ResNet50, there
are 38 pruning groups (see Sec. 6.6 for more details). There are a total of 22,531
input channels, including the 3image channels of the first convolution layer.
The largest layer has 2048 input channels. Restricting the possible masks to 8x
for GPU tensorcores, we test the solution time for both the meet-in-the-middle
solver in Algorithm 2 and an implementation of the dynamic programming (DP)
approach [8]. We use a representative latency capacity of 255.4, in units of mil-
liseconds, and randomly choose the current mask settings of the network. To

--- PAGE 26 ---
26 R. Humble et al.
Algorithm 2 MCK meet-in-the-middle solver
Input : Number of groups L, group value vectors vl, group cost vectors cl≤C, and
capacity C
Output : Best value vbest, best cost cbest, and used items kl=is.t.xl,i= 1.
function MCK (v, c, C )
L←len(v)
kl←0∀l∈[L]
ifL== 1 then
k1←arg maxiv1,i
return v1,k1, c1,k1, k
end if
forl∈range(1 ,⌊L/2⌋)do
vl, cl, ul, uL−l+1
←MERGE ( vl, cl, vL−l+1, cL−l+1, C)
end for
v← {v1, . . . , v ⌈L/2⌉},c← {c1, . . . , c ⌈L/2⌉}
vbest, cbest, krest←MCK( v, c, C )
forl∈range(1 ,⌊L/2⌋)do
i←kl
kl←ul,i
kL−l+1←uL−l+1,i
end for
return vbest, cbest, k
end function
function merge (v1, c1, v2, c2, C)
vnew,N (i−1)+j←v1,i+v2,j∀i∈[M], j∈[N]
cnew,N (i−1)+j←c1,i+c2,j
vnew, cnew, u←CONDENSE( vnew, cnew, C)
u1← ⌊u
N⌋
u2←u%N
return vnew, cnew, u1, u2
end function
function condense (v, c, C )
u← {i:vi> vj∀j̸=is.t.cj≤ci}
∪{i:vi≥vj∀j̸=is.t.cj=ci}
u←u∩ {i:ci≤C}
return {vi:i∈u},{ci:i∈u}, u
end function

--- PAGE 27 ---
Soft Masking for Cost-Constrained Channel Pruning 27
Method Accuracy Solve time (s)
DP (s= 10) 1decimal <1second
DP (s= 100) 2decimal ∼6second
DP (s= 500) 2−3decimals ∼30second
DP (s= 1000) 3decimals ∼70second
Algorithm 2 machine roundoff <1second
Table 2. Representative time to solve Eq. (6) under different settings. DP is our
implementation of the algorithm in [8].
use the DP approach, we define a scaling factor sand convert all costs to in-
tegers according to ⌊cl,is⌋. For s= 10d, the DP solution will be correct to d
digits; Algorithm 2 is correct up to machine roundoff. The solution times are
shown in Tab. 2.
6.10 Allowing layer pruning
In deriving our optimization problem in Eq. (5), as shown in Sec. 6.6, we had
to assume that the importance and cost functions were layer-wise separable,
meaning the input mask m(l)for layer lonly affects layer land the layer(s)
immediately upstream. This assumption is obviously broken when we allow layer
pruning, m(l)= 0, as pruning the entire layer effectively prunes all layers in that
branch of the network.
Nonetheless, we ran several experiments where we allowed layer pruning to
occur, if so chosen by the knapsack solver, when pruning the ResNet50 from the
EagleEye [23] baseline. The results are shown in Fig. 8. Breaking the theoretical
layer-wise separability assumption yields poor experimental results.

--- PAGE 28 ---
28 R. Humble et al.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1
conv1
layer1.0.conv1
layer1.0.conv2
layer1.0.conv3
layer1.1.conv1
layer1.1.conv2
layer1.1.conv3
layer1.2.conv1
layer1.2.conv2
layer1.2.conv3
layer1.0.downsample.0
layer2.0.conv1
layer2.0.conv2
layer2.0.conv3
layer2.1.conv1
layer2.1.conv2
layer2.1.conv3
layer2.2.conv1
layer2.2.conv2
layer2.2.conv3
layer2.3.conv1
layer2.3.conv2
layer2.3.conv3
layer2.0.downsample.0
layer3.0.conv1
layer3.0.conv2
layer3.0.conv3
layer3.1.conv1
layer3.1.conv2
layer3.1.conv3
layer3.2.conv1
layer3.2.conv2
layer3.2.conv3
layer3.3.conv1
layer3.3.conv2
layer3.3.conv3
layer3.4.conv1
layer3.4.conv2
layer3.4.conv3
layer3.5.conv1
layer3.5.conv2
layer3.5.conv3
layer3.0.downsample.0
layer4.0.conv1
layer4.0.conv2
layer4.0.conv3
layer4.1.conv1
layer4.1.conv2
layer4.1.conv3
layer4.2.conv1
layer4.2.conv2
layer4.2.conv3
layer4.0.downsample.01Fraction of Channels Remaining
SMCP-80%
SMCP-50%
SMCP-30%
SMCP-25%
Fig. 5. Fraction of remaining channels per layer for SMCP models on the ImageNet
classification dataset.

--- PAGE 29 ---
Soft Masking for Cost-Constrained Channel Pruning 29
0 0.5 1 1.5 2 2.5 3 3.5 4
conv1
layer1.0.conv1
layer1.0.conv2
layer1.0.conv3
layer1.1.conv1
layer1.1.conv2
layer1.1.conv3
layer1.2.conv1
layer1.2.conv2
layer1.2.conv3
layer1.0.downsample.0
layer2.0.conv1
layer2.0.conv2
layer2.0.conv3
layer2.1.conv1
layer2.1.conv2
layer2.1.conv3
layer2.2.conv1
layer2.2.conv2
layer2.2.conv3
layer2.3.conv1
layer2.3.conv2
layer2.3.conv3
layer2.0.downsample.0
layer3.0.conv1
layer3.0.conv2
layer3.0.conv3
layer3.1.conv1
layer3.1.conv2
layer3.1.conv3
layer3.2.conv1
layer3.2.conv2
layer3.2.conv3
layer3.3.conv1
layer3.3.conv2
layer3.3.conv3
layer3.4.conv1
layer3.4.conv2
layer3.4.conv3
layer3.5.conv1
layer3.5.conv2
layer3.5.conv3
layer3.0.downsample.0
layer4.0.conv1
layer4.0.conv2
layer4.0.conv3
layer4.1.conv1
layer4.1.conv2
layer4.1.conv3
layer4.2.conv1
layer4.2.conv2
layer4.2.conv3
layer4.0.downsample.01Ratio of Remaining Channels Compared to Comparable EagleEye Model
SMCP-80%
SMCP-50%
SMCP-30%
SMCP-25%
Fig. 6. Relative comparison of number of remaining channels per layer for SMCP and
EagleEye ResNet50 models on the ImageNet classification dataset. Each SMCP model
is compared to the EagleEye model of comparable FLOPs.

--- PAGE 30 ---
30 R. Humble et al.
2,300 2,400 2,500 2,600 2,700 2,800 2,900 3,000 3,10074.474.674.87575.275.4
0.5×0.8×1.3×
2×Original 10−35−45
10−40−355−20−605−30−50
FPSTop-1 Accuracy (%)
Fig. 7. Ablation study for SMCP’s pruning schedule hyperparameters. Baseline model
is EagleEye’s ResNet50 model [23]. Multiple denotes changing the rewiring frequency
by the given multiple. Triples denote changing the target schedule hyperparameters.
FPS measured on an NVIDIA TITAN V GPU.
2,000 2,500 3,000 3,500 4,000717273747576
FPSTop-1 Accuracy (%)SMCP
With layer pruning
Fig. 8. Comparison of allowing versus disallowing layer pruning at high pruning ratios
for ResNet50 on the ImageNet classification dataset using a latency cost constraint.
Baseline model is from EagleEye [23]. Accuracy against FPS speed shows the disad-
vantage of allowing layer pruning. Top-right is better. FPS measured on an NVIDIA
TITAN V GPU.
