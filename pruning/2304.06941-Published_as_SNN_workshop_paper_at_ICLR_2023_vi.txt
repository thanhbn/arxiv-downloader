# 2304.06941.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2304.06941.pdf
# Kích thước tệp: 1046482 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023
AUTOSPARSE : HƯỚNG TỚI VIỆC HUẤN LUYỆN THƯA TỰ ĐỘNG
CỦA MẠNG NƠRON SÂU
Abhisek Kundu
Phòng thí nghiệm Tính toán Song song, Intel Labs
abhisekkundu@gmail.comNaveen K. Mellempudi
Phòng thí nghiệm Tính toán Song song, Intel Labs
naveen.k.mellempudi@intel.com
Dharma Teja Vooturi
Phòng thí nghiệm Tính toán Song song, Intel Labs
dharma.teja.vooturi@intel.comBharat Kaul
Phòng thí nghiệm Tính toán Song song, Intel Labs
bharat.kaul@intel.com
Pradeep Dubey
Phòng thí nghiệm Tính toán Song song, Intel Labs
pradeep.dubey@intel.com
TÓM TẮT
Huấn luyện thưa đang nổi lên như một hướng tiếp cận đầy hứa hẹn để giảm chi phí tính toán của việc huấn luyện mạng nơron. Một số nghiên cứu gần đây đã đề xuất các phương pháp cắt tỉa sử dụng ngưỡng có thể học để khám phá hiệu quả phân bố không đồng nhất của độ thưa vốn có trong các mô hình. Trong bài báo này, chúng tôi đề xuất Gradient Annealing (GA), trong đó gradient của các trọng số bị che được chia tỷ lệ xuống theo cách phi tuyến. GA cung cấp một sự cân bằng tao nhã giữa độ thưa và độ chính xác mà không cần thêm regularization gây ra độ thưa. Chúng tôi tích hợp GA với các phương pháp cắt tỉa có thể học mới nhất để tạo ra một thuật toán huấn luyện thưa tự động gọi là AutoSparse, thuật toán này đạt được độ chính xác tốt hơn và/hoặc giảm FLOPS huấn luyện/suy luận so với các phương pháp cắt tỉa có thể học hiện có cho ResNet50 và MobileNetV1 thưa trên ImageNet-1K: AutoSparse đạt được giảm (2,7) FLOPS (huấn luyện,suy luận) cho ResNet50 trên ImageNet ở độ thưa 80%. Cuối cùng, AutoSparse vượt trội so với phương pháp SotA thưa-đến-thưa MEST (độ thưa đồng nhất) cho ResNet50 thưa 80% với độ chính xác tương tự, trong đó MEST sử dụng nhiều hơn 12% FLOPS huấn luyện và 50% FLOPS suy luận.

1 GIỚI THIỆU
Các mô hình học sâu (DNN) đã nổi lên như giải pháp ưa thích cho nhiều vấn đề quan trọng trong các lĩnh vực thị giác máy tính, mô hình hóa ngôn ngữ, hệ thống đề xuất và học tăng cường. Các mô hình đã trở nên lớn hơn và phức tạp hơn qua các năm, khi chúng được áp dụng cho các vấn đề ngày càng khó khăn trên các tập dữ liệu ngày càng lớn. Ngoài ra, DNN được thiết kế để hoạt động trong chế độ tham số hóa quá mức Arora et al. (2018); Belkin et al. (2019); Ardalani et al. (2019) để tạo điều kiện cho việc tối ưu hóa dễ dàng hơn bằng các phương pháp gradient descent. Do đó, chi phí tính toán (bộ nhớ và số phép toán dấu phẩy động FLOPS) để thực hiện các tác vụ huấn luyện và suy luận trên các mô hình tiên tiến (SotA) đã tăng với tốc độ theo cấp số nhân (Amodei & Hernandez).

Hai kỹ thuật chính để làm cho DNN hiệu quả hơn là (1) độ chính xác giảm Micikevicius et al. (2017); Wang et al. (2018); Sun et al. (2019); Das et al. (2018); Kalamkar et al. (2019); Mellempudi et al. (2020), và (2) biểu diễn thưa. Ngày nay, phần cứng huấn luyện SotA bao gồm nhiều FLOPS độ chính xác giảm hơn đáng kể so với các tính toán FP32 truyền thống, trong khi hỗ trợ cho độ thưa có cấu trúc cũng đang nổi lên Mishra et al. (2021). Trong biểu diễn thưa, các tham số mô hình được chọn bị che/cắt tỉa dẫn đến việc giảm đáng kể FLOPS, có tiềm năng tăng throughput hơn 10 lần. Trong khi các mô hình tham số hóa quá mức giúp làm cho việc huấn luyện dễ dàng hơn, chỉ một phần của các tham số mô hình là cần thiết để thực hiện suy luận chính xác Ström (1997); Han et al. (2015); Narang et al. (2017); Gale et al. (2019); Li et al. (2020). DNN hiện đại có số lượng lớn các lớp với phân bố không đồng nhất về số lượng tham số mô hình cũng như FLOPS trên mỗi lớp. Các thuật toán huấn luyện thưa nhằm đạt được ngân sách độ thưa tổng thể bằng cách sử dụng (a) độ thưa đồng nhất theo lớp (b) độ thưa không đồng nhất heuristic, hoặc (c) độ thưa không đồng nhất đã học. Hai kỹ thuật đầu có thể gây ra phân bổ tham số không tối ưu dẫn đến giảm độ chính xác và/hoặc hiệu quả FLOPS thấp hơn.

Các phương pháp huấn luyện thưa có thể được phân loại dựa trên khi nào và cách cắt tỉa. Tạo ra mô hình thưa từ mô hình dense đã được huấn luyện đầy đủ (Han et al. (2015)) hữu ích cho suy luận hiệu quả và không phải cho huấn luyện hiệu quả. Các phương pháp thưa-đến-thưa MEST (Yuan et al. (2021)), mặc dù hiệu quả trong mỗi lần lặp, đòi hỏi các epoch huấn luyện dài hơn (giảm lợi ích trong FLOPS huấn luyện) hoặc cập nhật gradient dense thường xuyên TopKAST (Jayakumar et al. (2021)) (nhiều FLOPS hơn trong backpass) trong khi giữ lớp đầu/cuối dense để đạt kết quả độ chính xác cao. Các phương pháp dense-đến-thưa tăng độ thưa một cách đơn điệu từ một mô hình dense, chưa được huấn luyện trong suốt quá trình huấn luyện GMP (Zhu & Gupta (2017)) sử dụng một số lịch trình, và/hoặc sử dụng cập nhật gradient đầy đủ cho các trọng số bị che OptG (Zhang et al. (2022)) để cải thiện độ chính xác. Những phương pháp này thường có số lượng FLOPS huấn luyện cao hơn.

Học Trade-off Độ thưa-Độ chính xác: Các phương pháp gần đây học phân bố độ thưa trong quá trình huấn luyện là STR (Kusupati et al. (2020)), DST (Liu et al. (2020)), LTP (Azarian et al. (2021)), CS (Savarese et al. (2021)) và SCL (Tang et al. (2022)). Các phương pháp độ thưa đã học cung cấp lợi thế kép so với các phương pháp có độ thưa đồng nhất và độ thưa được phân bổ heuristic: (1) hiệu quả về mặt tính toán, vì chi phí phụ của việc tính toán mặt nạ cắt tỉa (ví dụ: chọn k lớn nhất) được loại bỏ, (2) học một phân bố độ thưa không đồng nhất vốn có trong mô hình, tạo ra một mô hình thưa hiệu quả FLOPS hơn cho suy luận. Tuy nhiên, thách thức chính với các phương pháp này là xác định phân bố độ thưa trong các epoch đầu, để trở nên cạnh tranh với các phương pháp thưa-đến-thưa về mặt FLOPS huấn luyện, trong khi đạt được trade-off độ thưa-độ chính xác. Để làm điều này, các phương pháp này thường áp dụng regularizer gây ra độ thưa với hàm mục tiêu (trừ STR). Tuy nhiên, các phương pháp này hoặc tạo ra độ thưa/FLOPS không tối ưu (SCL) hoặc gặp phải mất mát độ chính xác đáng kể (LTP), hoặc dễ bị 'độ thưa chạy trốn' trong khi trích xuất độ thưa cao từ các lần lặp đầu (STR, DST). Để đối phó với sự tăng trưởng độ thưa không kiểm soát được, DST thực hiện kiểm tra giới hạn trên cứng (ví dụ: 99%) về độ thưa để kích hoạt việc đặt lại ngưỡng vi phạm (rơi vào chế độ huấn luyện dense và dẫn đến số lượng FLOPS huấn luyện cao hơn) để ngăn chặn mất mát độ chính xác. Tương tự, STR sử dụng kết hợp của các ngưỡng ban đầu nhỏ với weight decay thích hợp để trì hoãn việc gây ra độ thưa cho đến sau trong chu kỳ huấn luyện (ví dụ: 30 epoch) để kiểm soát sự tăng trưởng độ thưa không kiềm chế.

Được thúc đẩy bởi những thách thức trên, chúng tôi đề xuất phương pháp Gradient Annealing (GA), để giải quyết các vấn đề nêu trên liên quan đến việc huấn luyện các mô hình thưa. So với các phương pháp hiện có, GA cung cấp tính linh hoạt lớn hơn để khám phá trade-off giữa độ thưa mô hình và độ chính xác, và cung cấp tính ổn định lớn hơn bằng cách ngăn chặn sự phân kỳ do độ thưa chạy trốn. Chúng tôi cũng đề xuất một thuật toán huấn luyện thống nhất gọi là AutoSparse, kết hợp những điều tốt nhất của các phương pháp ngưỡng có thể học STR với GA nhằm mở đường cho việc tự động hóa hoàn toàn của việc huấn luyện thưa-đến-thưa. Ngoài ra, chúng tôi cũng chứng minh rằng khi kết hợp với các phương pháp cắt tỉa đồng nhất TopKAST, GA có thể trích xuất độ chính xác tốt hơn ở ngân sách độ thưa 80%. Các đóng góp chính của công trình này như sau:

Chúng tôi trình bày một phương pháp Gradient Annealing (GA) mới (với siêu tham số α) là một gradient approximator tổng quát hơn STE (Bengio et al. (2013)) và ReLU. Để huấn luyện các mô hình thưa end-to-end, GA cung cấp tính linh hoạt lớn hơn cho trade-off độ thưa-độ chính xác so với các phương pháp khác.

Chúng tôi đề xuất AutoSparse, một thuật toán kết hợp GA với phương pháp độ thưa có thể học STR để tạo ra một khung tự động cho việc huấn luyện thưa end-to-end. AutoSparse vượt trội so với các phương pháp độ thưa có thể học SotA về mặt độ chính xác và FLOPS huấn luyện/suy luận bằng cách duy trì độ thưa cao một cách nhất quán trong suốt quá trình huấn luyện cho ResNet50 và MobileNetV1 trên Imagenet-1K.

Chúng tôi chứng minh hiệu quả của Gradient Annealing như một kỹ thuật học tổng quát độc lập với AutoSparse bằng cách áp dụng nó cho phương pháp TopKAST để cải thiện độ chính xác Top-1 của ResNet50 lên 0.3% trên tập dữ liệu ImageNet-1K sử dụng ngân sách độ thưa đồng nhất 80%.

Cuối cùng, AutoSparse vượt trội so với phương pháp huấn luyện thưa-đến-thưa SotA MEST (độ thưa đồng nhất) trong đó MEST sử dụng (12%, 50%) FLOPS nhiều hơn cho (huấn luyện, suy luận) so với AutoSparse cho ResNet50 thưa 80% với độ chính xác tương đương.

--- TRANG 2 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

2 CÔNG TRÌNH LIÊN QUAN

Các phương pháp huấn luyện thưa có thể được phân loại theo cách sau dựa trên khi nào và cách cắt tỉa.

Cắt tỉa sau Huấn luyện: Ở đây các mô hình thưa được tạo ra bằng pipeline ba giai đoạn– huấn luyện một mô hình dense, cắt tỉa, và huấn luyện lại (Han et al. (2015); Guo et al. (2016)). Những phương pháp này tốn kém về mặt tính toán hơn so với huấn luyện dense và chỉ hữu ích cho việc giảm FLOPS và bộ nhớ trong suy luận.

Cắt tỉa trước Huấn luyện: Những phương pháp như vậy được lấy cảm hứng từ 'Giả thuyết Vé số' (Frankle & Carbin (2019)), cố gắng tìm một mặt nạ trọng số thưa (subnetwork) có thể được huấn luyện từ đầu để khớp với chất lượng của mô hình dense đã được huấn luyện. Tuy nhiên, chúng sử dụng một sơ đồ cắt tỉa lặp được lặp lại cho một số lần huấn luyện đầy đủ để tìm mặt nạ. SNIP (Lee et al. (2019)) bảo tồn mất mát sau khi cắt tỉa dựa trên độ nhạy kết nối. GraSP (Wang et al. (2020)) cắt tỉa kết nối sao cho nó bảo tồn luồng gradient của mạng. SynFlow (Tanaka et al. (2020)) sử dụng cắt tỉa luồng synaptic lặp để bảo tồn tổng luồng của sức mạnh synaptic (cắt tỉa toàn cục tại khởi tạo không có dữ liệu và không có backpropagation). 3SP (van Amersfoort et al. (2020)) giới thiệu cắt tỉa kênh nhận biết tính toán (cắt tỉa có cấu trúc). Lợi thế chính của những phương pháp này là chúng hiệu quả về tính toán và bộ nhớ trong mỗi lần lặp. Tuy nhiên, chúng thường gặp phải mất mát độ chính xác đáng kể. Lưu ý rằng, mặt nạ cắt tỉa trọng số được cố định trong suốt quá trình huấn luyện.

Cắt tỉa trong quá trình Huấn luyện: Đối với những phương pháp này, mặt nạ cắt tỉa trọng số phát triển động với việc huấn luyện. Các phương pháp thuộc loại này có thể thuộc về huấn luyện thưa-đến-thưa hoặc dense-đến-thưa.

Trong huấn luyện thưa-đến-thưa, chúng ta có một mô hình thưa để bắt đầu (dựa trên ngân sách độ thưa) và ngân sách độ thưa được duy trì trong suốt quá trình huấn luyện. SET (Mocanu et al. (2018)) đi tiên phong trong cách tiếp cận này khi họ thay thế một phần của các trọng số có độ lớn nhỏ nhất bằng các trọng số ngẫu nhiên để khám phá tốt hơn. DSR (Mostafa & Wang (2019)) cho phép ngân sách độ thưa không đồng nhất giữa các lớp theo heuristic, ví dụ: độ thưa cao hơn cho các lớp sau. SNFS (Dettmers & Zettlemoyer (2019)) đề xuất sử dụng momentum của mỗi tham số như một tiêu chí để phát triển các trọng số dẫn đến tăng độ chính xác. Tuy nhiên, phương pháp này đòi hỏi tính gradient đầy đủ cho tất cả các trọng số ngay cả đối với các trọng số bị che. RigL (Evci et al. (2021)) kích hoạt/hồi sinh các trọng số mới được xếp hạng theo độ lớn gradient (sử dụng tính toán gradient đầy đủ không thường xuyên), tức là các trọng số bị che chỉ nhận gradient sau một số lần lặp nhất định. Họ giảm số lượng trọng số được hồi sinh này theo thời gian. Phân bố độ thưa có thể đồng nhất hoặc không đồng nhất (sử dụng ERK trong đó độ thưa được chia tỷ lệ bằng số lượng neuron và kích thước kernel). MEST (Yuan et al. (2021)) luôn duy trì độ thưa cố định trong forward và backward pass bằng cách chỉ tính gradient của các trọng số còn sống sót. Để khám phá tốt hơn, họ loại bỏ một số trọng số ít quan trọng nhất (được xếp hạng tỷ lệ thuận với độ lớn cộng với độ lớn gradient) và giới thiệu cùng số lượng trọng số 'zero' ngẫu nhiên để duy trì ngân sách độ thưa. Phương pháp này phù hợp cho huấn luyện thưa trên các thiết bị edge có bộ nhớ hạn chế. Việc hạn chế luồng gradient gây mất mát độ chính xác trong các phương pháp thưa-đến-thưa mặc dù có các lần lặp hiệu quả về mặt tính toán, và chúng cần nhiều epoch huấn luyện dài hơn (250 cho MEST và 500 cho RigL) để lấy lại độ chính xác với chi phí FLOPS huấn luyện cao hơn. TopKAST (Jayakumar et al. (2021)) luôn cắt tỉa các trọng số có độ lớn cao nhất (độ thưa đồng nhất), nhưng cập nhật một tập con của các trọng số hoạt động dựa trên gradient của tập con này (độ thưa backward) để các trọng số bị cắt tỉa có thể được hồi sinh. Để có kết quả chất lượng cao, họ phải sử dụng gradient trọng số đầy đủ (không có độ thưa backward) trong suốt quá trình huấn luyện. PowerPropagation (PP) (Schwarz et al. (2021)) biến đổi các trọng số như w=v|v|^α−1, sao cho nó tạo ra một phân bố heavy-tailed của các trọng số đã được huấn luyện. Họ quan sát thấy rằng các trọng số được khởi tạo gần 0 có khả năng bị cắt tỉa, và các trọng số ít có khả năng thay đổi dấu. PP, khi được áp dụng trên TopKAST, tức là TopKAST + PP, cải thiện độ chính xác của TopKAST. RigL và MEST giữ lớp đầu dense trong khi TopKAST và các biến thể của nó làm cho lớp đầu và cuối dense. Gradmax (Evci et al. (2022)) đề xuất phát triển mạng bằng cách thêm nhiều trọng số dần dần với các epoch huấn luyện để giảm tổng FLOPS huấn luyện.

Huấn luyện Dense-đến-Thưa: GMP (Zhu & Gupta (2017)) là một cắt tỉa trọng số dựa trên độ lớn đơn giản được áp dụng dần dần trong suốt quá trình huấn luyện. Gradient không chảy đến các trọng số bị che. Gale et al. (2019)) cải thiện độ chính xác của GMP bằng cách giữ lớp đầu dense và độ thưa lớp cuối ở 80%. DNW (Wortsman et al. (2019)) cũng sử dụng cắt tỉa độ lớn trong khi cho phép gradient chảy đến trọng số bị che thông qua STE (Bengio et al. (2013)). DPF (Lin et al. (2020)) cập nhật các trọng số dense bằng full-gradient của các trọng số thưa trong khi đồng thời duy trì mô hình dense. OptG (Zhang et al. (2022)) học cả trọng số và một supermask cắt tỉa theo cách được điều khiển bởi gradient. Họ lập luận ủng hộ việc để gradient chảy đến các trọng số bị cắt tỉa để nó giải quyết vấn đề 'nghịch lý độc lập' ngăn cản việc đạt được kết quả thưa độ chính xác cao. Tuy nhiên, họ đạt được ngân sách độ thưa nhất định bằng cách tăng độ thưa theo một lịch trình sigmoid (độ thưa chỉ được trích xuất sau 40 epoch). Điều này gặp phải số lượng FLOPS huấn luyện lớn hơn. Phân bố độ thưa đạt được là đồng nhất giữa các lớp. SWAT (Raihan & Aamodt (2020)) làm thưa cả trọng số và activation bằng cách giữ các phần tử có độ lớn Top để giảm thêm FLOPS.

Cắt tỉa Đã học: Đối với một ngân sách độ thưa nhất định, các phương pháp được thảo luận ở trên hoặc giữ độ thưa đồng nhất ở mỗi lớp hoặc tạo ra phân bố độ thưa không đồng nhất theo heuristic. Các phương pháp sau học một phân bố độ thưa không đồng nhất thông qua một số tối ưu hóa gây ra độ thưa.

DST (Liu et al. (2020)) học cả trọng số và ngưỡng cắt tỉa (tạo mặt nạ trọng số) trong đó họ áp đặt exponential decay của các ngưỡng như regularizer. Một siêu tham số kiểm soát lượng regularization, dẫn đến sự cân bằng giữa độ thưa và độ chính xác. Để giảm độ nhạy của siêu tham số, họ đặt lại độ thưa theo cách thủ công nếu nó vượt quá một số giới hạn được xác định trước (do đó thỉnh thoảng rơi vào chế độ huấn luyện dense). Ngoài ra, xấp xỉ gradient của hàm step cắt tỉa giúp một số phần tử bị che nhận gradient mất mát.

STR (Kusupati et al. (2020)) là một phương pháp SotA học trọng số và ngưỡng cắt tỉa bằng ReLU như một hàm mặt nạ, trong đó STE của ReLU được sử dụng để xấp xỉ gradient của các trọng số còn sống sót và các trọng số bị che không nhận gradient mất mát. Nó không sử dụng regularizer gây ra độ thưa rõ ràng. Tuy nhiên, việc trích xuất độ thưa cao từ các lần lặp đầu dẫn đến độ thưa chạy trốn; điều này buộc STR phải chạy huấn luyện dense hoàn toàn trong nhiều epoch trước khi việc trích xuất độ thưa bắt đầu.

SCL (Tang et al. (2022)) học trọng số và một mặt nạ (cùng kích thước với trọng số) được nhị phân hóa trong forward pass. Mặt nạ này cùng với một siêu tham số connectivity giảm dần được sử dụng như một regularizer gây ra độ thưa trong hàm mục tiêu. Mặt nạ đã học tăng kích thước mô hình hiệu quả trong quá trình huấn luyện, có thể tạo ra overhead di chuyển tham số từ bộ nhớ.

LTP (Azarian et al. (2021)) học các ngưỡng cắt tỉa bằng cách sử dụng soft pruning và soft L0 regularization trong đó sigmoid được áp dụng trên các trọng số đã được biến đổi và độ thưa được kiểm soát bởi một siêu tham số. CS (Savarese et al. (2021)) sử dụng hàm soft-threshold sigmoid như một regularization gây ra độ thưa.

Để thảo luận chi tiết về công trình liên quan, xem Hoefler et al. (2021).

3 GRADIENT ANNEALING (GA)

Một bước cắt tỉa điển hình của mạng sâu bao gồm việc che các trọng số dưới một ngưỡng T nào đó. Biểu diễn thưa của các trọng số này được hưởng lợi từ tính toán thưa trong forward pass và trong tính toán gradient của các đầu vào. Chúng tôi đề xuất bước cắt tỉa sau, trong đó w là một trọng số và T là một ngưỡng có thể là xác định (ví dụ: độ lớn TopK) hoặc có thể học:

(thưa) ~w=sign(w)h(|w|−T)

Forward pass h(x) = {x; x > 0
                     0; x ≤ 0}

(Proxy) Gradient ∂h(x)/∂x = {1; x > 0
                              α; x ≤ 0}    (1)

trong đó 0 ≤ α ≤ 1. ~w là 0 nếu |w| dưới ngưỡng T. Cắt tỉa dựa trên độ lớn là một góc nhìn tham lam, tạm thời về tầm quan trọng của tham số. Tuy nhiên, một số trọng số bị cắt tỉa (trong các epoch huấn luyện đầu) có thể trở nên quan trọng trong các epoch sau khi một mẫu thưa chính xác hơn xuất hiện. Để làm điều này, h() trong phương trình (1) cho phép gradient mất mát chảy đến các trọng số bị che để tránh việc cắt tỉa vĩnh viễn một số trọng số quan trọng. Xấp xỉ gradient được đề xuất được lấy cảm hứng từ Straight Through Estimator (STE) Bengio et al. (2013) thay thế gradient zero của các hàm không khả vi con rời rạc bằng gradient proxy trong back-propagation. Hơn nữa, chúng tôi giảm α này khi quá trình huấn luyện tiến triển. Chúng tôi gọi kỹ thuật này là Gradient Annealing.

Chúng tôi giảm α ở đầu mỗi epoch và giữ giá trị này cố định cho tất cả các lần lặp trong epoch đó. Chúng tôi muốn giảm chậm trong các epoch đầu và sau đó giảm đều đặn. Để làm điều này, chúng tôi so sánh một số lựa chọn để giảm α: tỷ lệ cố định (không giảm), giảm tuyến tính, giảm cosine (giống như learning rate (2)), giảm sigmoid (được định nghĩa trong (3)) và giảm Sigmoid-Cosine (được định nghĩa trong (4)). Đối với giảm sigmoid trong (3), L₀ = 6 và L₁ = -6. Đối với tổng số epoch T, tỷ lệ cho α trong epoch i là

Cosine-Decay α(i,T) = αcᵢ = (1 + cosine(πi/T))/2     (2)
Sigmoid-Decay α(i,T) = αsᵢ = 1 - sigmoid(L₀ + (L₁-L₀)i/T)     (3)
Sigmoid-Cosine-Decay α(i,T) = max{αsᵢ, αcᵢ}     (4)

--- TRANG 4 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

Hình 1 cho thấy ảnh hưởng của việc annealing tuyến tính và phi tuyến khác nhau của α lên độ thưa động. Tỷ lệ cố định không giảm (STE) không cho chúng ta kiểm soát tốt độ thưa động. Giảm tuyến tính tốt hơn điều này nhưng gặp phải giảm độ thưa về cuối quá trình huấn luyện. Các giảm phi tuyến trong phương trình (2, 3, 4) cung cấp trade-off vượt trội nhiều giữa độ thưa và độ chính xác. Trong khi phương trình (2) và phương trình (4) cho thấy hành vi rất tương tự, việc giảm mạnh của phương trình (3) về cuối quá trình huấn luyện đẩy độ thưa lên một chút (gây ra ít giảm độ chính xác hơn). Những biểu đồ này phù hợp với phân tích hội tụ của chúng tôi về GA trong phương trình (5). Lịch trình annealing của α theo sát lịch trình giảm learning rate.

Phân tích Gradient Annealing Ở đây chúng tôi phân tích ảnh hưởng của phép biến đổi h() lên sự hội tụ của quá trình học bằng một ví dụ đơn giản như sau. Cho v = |w| - T, u = h(v), trọng số tối ưu là w* và ngưỡng tối ưu là T*, tức là v* = |w*| - T*. Hãy xem xét hàm mất mát là

min L(v) = 0.5(h(v) - v*)²

và cho ∂h(v) biểu thị gradient ∂h(v)/∂v. Chúng tôi xem xét các trường hợp sau cho gradient mất mát cho v.

∂L/∂v = ∂h(v)(h(v) - v*) = {
    ∂h(v) · 0 = 0 nếu h(v) = v*
    ∂h(v)(v - v*) = 1(v - v*) nếu v > 0 và v* > 0
    ∂h(v)(v + |v*|) = 1(v + |v*|) nếu v > 0 và v* ≤ 0
    ∂h(v)(-v*) = α(-v*) nếu v ≤ 0 và v* > 0
    ∂h(v)(-|v*|) = α(-|v*|) nếu v ≤ 0 và v* ≤ 0
}    (5)

Gradient proxy đúng cho h() nên di chuyển v về phía v* trong quá trình cắt tỉa (ví dụ: hướng ngược lại của gradient cho gradient descent) và ổn định nó tại optima của nó (không còn cập nhật nữa). Do đó, ∂h(v) > 0 nên được thỏa mãn để hội tụ tốt hơn của v đến v*. h() của chúng tôi thỏa mãn điều kiện này cho α > 0. Hơn nữa, đối với v > 0, v được cập nhật tỷ lệ thuận với v - v*, tức là v cách v* bao xa. Khi quá trình huấn luyện tiến triển và v tiến gần hơn đến v*, v nhận gradient nhỏ dần để cuối cùng hội tụ đến v*. Tuy nhiên, đối với v ≤ 0, v nhận gradient tỷ lệ thuận với độ lớn của v*, bất kể v gần v* đến mức nào. Ngoài ra, lưu ý rằng chúng ta được hưởng lợi từ tính toán thưa khi v ≤ 0.

Chúng tôi đặt T ban đầu cao để đạt được độ thưa từ các epoch đầu. Tuy nhiên, điều này có khả năng dẫn đến một số lượng lớn trọng số theo điều kiện 4 trong phương trình (5). α cố định, lớn (gần 1) tạo ra sự điều chỉnh lớn cho v và di chuyển nó đến v* một cách nhanh chóng. Do đó, v di chuyển từ điều kiện 4 sang điều kiện 2, mất đi lợi ích của tính toán thưa. α thấp hơn 'trì hoãn' sự chuyển đổi này và tận hưởng lợi ích của độ thưa. Đây là lý do tại sao chúng tôi chọn α < 1 thay vì identity STE như gradient proxy (không giống như Tang et al. (2022)). Tuy nhiên, khi quá trình huấn luyện tiến triển, ngày càng nhiều trọng số di chuyển từ điều kiện 4 sang điều kiện 2 dẫn đến giảm độ thưa. Hành vi này không mong muốn để đạt được độ thưa mục tiêu ở cuối quá trình huấn luyện. Để khắc phục điều này, chúng tôi đề xuất giảm α với các epoch huấn luyện sao cho chúng ta tận hưởng lợi ích của tính toán thưa trong khi v gần v*. Tức là, GA cung cấp một trade-off được kiểm soát và ổn định hơn giữa độ thưa và độ chính xác trong suốt quá trình huấn luyện.

Lưu ý rằng, GA có thể áp dụng khi chúng ta tính gradient mất mát cho một tập con của các trọng số hoạt động (khác không) tham gia vào tính toán thưa forward bằng gradient descent. Đối với một lần lặp t, cho độ thưa forward là S. Nếu α = 0, thì chúng ta cần tính gradient chỉ cho những trọng số khác không vì các trọng số khác sẽ không nhận gradient do ReLU STE. Để được hưởng lợi từ việc giảm tính toán như vậy, chúng ta có thể đặt α = 0 sau một số epoch annealing.

4 AUTOSPARSE : HUẤN LUYỆN THƯA VỚI GRADIENT ANNEALING

AutoSparse là thuật toán huấn luyện thưa kết hợp những điều tốt nhất của kỹ thuật cắt tỉa ngưỡng có thể học STR với Gradient Annealing (GA). AutoSparse đáp ứng các yêu cầu cần thiết cho việc huấn luyện hiệu quả các mạng nơron thưa.

Ngưỡng Cắt tỉa Có thể học: Loại bỏ việc tính toán ngưỡng dựa trên sắp xếp để giảm overhead sparification so với các phương pháp cắt tỉa đồng nhất. Học phân bố không đồng nhất của độ thưa giữa các lớp.

--- TRANG 5 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

(a) FLOPS (M) cho ResNet50 thưa 80% được tạo ra
bởi các phương pháp MEST đồng nhất và độ thưa đã học

(b) Độ thưa đạt được cho các gradient annealing khác nhau

Hình 1: Huấn luyện ResNet50 thưa trên ImageNet

Khám phá Mô hình Thưa: Khám phá một trade-off tao nhã giữa độ chính xác mô hình so với mức độ thưa bằng cách áp dụng phương pháp Gradient Annealing (như được hiển thị trong Hình 1). Tạo ra một mô hình thưa ở cuối quá trình huấn luyện với mức độ thưa mong muốn được hướng dẫn bởi siêu tham số α.

Tăng tốc Huấn luyện/Suy luận: Giảm FLOPS huấn luyện bằng cách huấn luyện với trọng số thưa từ đầu, duy trì mức độ thưa cao trong suốt quá trình huấn luyện, và sử dụng độ thưa trong cả forward và backward pass. Tạo ra mô hình thưa hiệu quả FLOPS cho suy luận.

Các đề xuất trước đó sử dụng các phương pháp ngưỡng có thể học như DST và STR giải quyết tiêu chí đầu tiên nhưng không xử lý hiệu quả trade-off độ chính xác so với độ thưa. Những phương pháp này cũng không tăng tốc huấn luyện bằng cách giảm FLOPS hiệu quả như phương pháp của chúng tôi. Các phương pháp cắt tỉa đồng nhất như TopKAST, RigL, MEST giải quyết tiêu chí thứ ba của việc tăng tốc huấn luyện, nhưng gây ra overhead sparification để tính toán các giá trị ngưỡng và không thể tự động khám phá phân bố không đồng nhất của độ thưa. Điều này dẫn đến các mô hình thưa không tối ưu cho suy luận (Hình 1a).

Công thức: Cho D := {(xi ∈ R^d, yi ∈ R)} là dữ liệu quan sát được, W là các tham số mạng có thể học, L là một hàm mất mát. Đối với một DNN L-lớp, W được chia thành các tensor tham số có thể huấn luyện theo lớp, [W_ℓ]^L_ℓ=1. Vì các lớp khác nhau có thể có số lượng tham số rất khác nhau và cũng có độ nhạy không bằng nhau đối với việc thay đổi tham số, chúng tôi sử dụng một tham số cắt tỉa có thể huấn luyện, s_ℓ cho mỗi lớp ℓ, tức là s = [s₁, ..., s_L] là vector của tham số cắt tỉa có thể huấn luyện. Cho g: R → R được áp dụng element-wise. Đối với lớp ℓ, T_ℓ = g(s_ℓ) là ngưỡng cắt tỉa cho W_ℓ. Chúng tôi tìm cách tối ưu hóa:

min L(S_{h,g}(W,s), D)    (6)
W,s

trong đó, hàm S_{h,g}, được tham số hóa bởi h: R → R được áp dụng element-wise.

Ŵ_ℓ = S_{h,g}(W_ℓ, s_ℓ) = sign(W_ℓ) ⊙ h(|W_ℓ| - g(s_ℓ))    (7)

Gradient annealing được áp dụng thông qua h() như đã thảo luận trước đó.

Forward Pass Thưa: Tại lần lặp t, đối với lớp ℓ, trọng số thưa là Ŵ^(t)_ℓ = S_{h,g}(W^(t)_ℓ, s^(t)_ℓ) như được định nghĩa trong phương trình (7). Cho tập hợp các trọng số khác không (hoạt động) A^(t)_ℓ = {i : Ŵ^(t)_{ℓ,i} > 0}. Để đơn giản, hãy bỏ các ký hiệu subscript. Ŵ được sử dụng trong forward pass như Y = X ⊛ Ŵ trong đó ⊛ là phép toán tensor MAC, ví dụ: convolution. Cho A biểu thị phần của các phần tử khác không của W thuộc về A, tức là độ thưa forward của chúng ta là 1 - A. Ŵ cũng được sử dụng trong tính toán gradient đầu vào trong backward pass. Mỗi lần lặp, chúng ta cập nhật W và xây dựng Ŵ từ W đã cập nhật và ngưỡng đã học.

Tính toán Thưa của Gradient Đầu vào: Đối với một lần lặp và một lớp (chúng tôi bỏ subscript t, ℓ để đơn giản), đầu ra của các phép toán thưa trong forward pass không cần phải thưa, tức là Y = X ⊛ Ŵ thường dense. Do đó, gradient của đầu ra ∇Y cũng dense. Chúng ta tính gradient của đầu vào ∇X như Ŵ^T ⊛ ∇Y. Tính toán ∇X là thưa do độ thưa trong Ŵ.

Gradient Trọng số Thưa: Gradient của trọng số ∇W được tính như X^T ⊛ ∇Y. Lưu ý rằng, đối với độ thưa forward S, α = 0 có nghĩa là độ thưa gradient trọng số S vì không có gradient chảy đến các trọng số bị cắt tỉa. Chúng ta có thể có một siêu tham số chỉ định ở epoch nào chúng ta đặt α = 0, để tận hưởng lợi ích của gradient trọng số thưa. Tuy nhiên, chúng ta cần giữ α ≠ 0 trong một số epoch để đạt được kết quả độ chính xác cao, mất đi lợi ích của gradient trọng số thưa. Để khắc phục điều này, chúng tôi đề xuất như sau cho các epoch khi α ≠ 0. Chúng ta có thể làm thưa ∇W nếu chúng ta tính gradient mất mát bằng một tập con B của W.

B = {i : W_i ∈ TopK(|W|, |B|)}; B ⊇ A    (8)

trong đó |B| là một tập con của A và TopK(|W|, k) chọn các chỉ số của k phần tử có độ lớn lớn nhất từ |W|. Điều này tạo thành độ thưa gradient trọng số 1 - |B| của chúng tôi (tương tự như TopKAST). Chúng tôi áp dụng gradient annealing trên tập B \ A, tức là gradient của các trọng số trong B \ A được giảm bằng α. Lưu ý rằng, α = 0 có nghĩa là B = A.

Tính toán FLOPS: Đối với một mô hình dense với FLOPS f^D_ℓ và phiên bản thưa với FLOPS f^S_ℓ cho lớp ℓ, tổng FLOPS huấn luyện dense cho một mẫu đơn là 3∑_ℓ f^D_ℓ và FLOPS huấn luyện thưa là 3∑_ℓ f^S_ℓ khi chỉ có gradient trọng số thưa (AutoSparse với α = 0), nếu không thì là 2∑_ℓ f^S_ℓ + ∑_ℓ f^D_ℓ cho gradient trọng số dense đầy đủ (AutoSparse với α ≠ 0). Ngoài ra, đối với AutoSparse, f^S_ℓ thay đổi với các lần lặp, vì vậy chúng tôi tổng nó cho tất cả các lớp, tất cả các lần lặp và tất cả các điểm dữ liệu để có được số lượng FLOPS huấn luyện cuối cùng. Đối với độ thưa được đặt rõ ràng f^B cho gradient trọng số, FLOPS huấn luyện AutoSparse cho một mẫu là 2∑_ℓ f^S_ℓ + ∑_ℓ f^B (cho α ≠ 0) và 2∑_ℓ f^S_ℓ + ∑_ℓ max(f^B, f^S_ℓ) (cho α = 0).

5 THÍ NGHIỆM

Đối với Mô hình Thị giác, chúng tôi hiển thị kết quả huấn luyện thưa trên ImageNet-1K (Deng et al. (2009)) cho hai kiến trúc CNN phổ biến: ResNet50 He et al. (2016) và MobileNetV1 Howard et al. (2017), để chứng minh tính tổng quát của phương pháp chúng tôi. Đối với huấn luyện AutoSparse, chúng tôi sử dụng SGD như optimizer, momentum 0.875, learning rate (max) 0.256 sử dụng cosine annealing với warm up của 5 epoch. Chúng tôi chạy tất cả các thí nghiệm trong 100 epoch sử dụng batch size 256. Chúng tôi sử dụng weight decay λ = 0.000030517578125 (được chọn từ STR Kusupati et al. (2020)), label smoothing 0.1, s₀ = -5. Chúng tôi trình bày kết quả của chúng tôi chỉ sử dụng Sigmoid-Cosine decay của α (được định nghĩa trong phương trình (4)).

Đối với Mô hình Ngôn ngữ, chúng tôi chọn các mô hình Transformer Vaswani et al. (2017) cho dịch ngôn ngữ trên dữ liệu WMT14 English-German. Chúng tôi có 6 lớp encoder và 6 lớp decoder với thiết lập siêu tham số tiêu chuẩn: optimizer là ADAM với beta (0.9, 0.997), token size 10240, warm up 4000, learning rate 0.000846 theo inverse square root decay. Chúng tôi áp dụng AutoSparse bằng cách giới thiệu một ngưỡng có thể học cho mỗi lớp tuyến tính, và chúng tôi khởi tạo chúng như s₀ = -7.0. Ngoài ra, α₀ ban đầu = 0.4 được annealed theo exponential decay như sau. Đối với epoch t và β > 0 (chúng tôi sử dụng β = 1):

Exponential-Decay α(t, β) = e^(-βt)    (9)

Chúng tôi giữ các lớp đầu và cuối của transformer dense và áp dụng AutoSparse để huấn luyện nó trong 44 epoch. Chúng tôi lặp lại các thí nghiệm nhiều lần với các random seed khác nhau và báo cáo các số trung bình.

Ký hiệu: Chúng tôi định nghĩa các ký hiệu sau được sử dụng trong các bảng. 'Base': Độ chính xác dense baseline, 'Top1(S)': Độ chính xác Top-1 cho các mô hình thưa, 'Drop%': giảm tương đối trong độ chính xác cho các mô hình thưa từ Base, 'S%': phần trăm độ thưa mô hình, 'Train F': phần của FLOPS huấn luyện so với FLOPS baseline, 'Test F': phần của FLOPS suy luận so với FLOPS baseline, 'BackS%': độ thưa được đặt rõ ràng trong gradient trọng số (|B| trong phương trình (8)), 'BLEU(S)': điểm BLEU cho các mô hình thưa. Các giá trị nhỏ hơn của 'Train F' và 'Test F' gợi ý việc giảm tính toán lớn hơn.

5.1 HIỆU QUẢ CỦA GRADIENT ANNEALING

Chúng tôi so sánh kết quả AutoSparse với STR và DST. Việc thiếu luồng gradient đến các phần tử bị cắt tỉa ngăn STR đạt được trade-off độ thưa-độ chính xác tối ưu. Ví dụ, họ cần huấn luyện dense trong nhiều epoch để đạt được kết quả độ chính xác cao, mất đi lợi ích của huấn luyện thưa. Gradient annealing của chúng tôi khắc phục những vấn đề như vậy và đạt được trade-off độ thưa-độ chính xác vượt trội nhiều (Bảng 1, 3). Tương tự, phương pháp của chúng tôi đạt được độ chính xác cao hơn DST cho cả ngân sách độ thưa 80% và 90% cho ResNet50 (Bảng 1). Tuy nhiên, DST sử dụng regularizer gây ra độ thưa riêng biệt, trong khi gradient annealing của chúng tôi tự thực hiện thủ thuật. SCL báo cáo giảm 0.23% độ chính xác ở độ thưa 74% cho ResNet50 trên ImageNet với FLOPS suy luận 0.21 của baseline). LTP tạo ra ResNet50 thưa 89% gặp phải giảm 3.2% độ chính xác từ baseline (tệ hơn của chúng tôi). Continuous Sparsification (CS) gây ra huấn luyện thưa bằng soft-thresholding như một regularizer. Mô hình thưa được tạo ra bởi họ được sử dụng để kiểm tra Giả thuyết Vé số (được huấn luyện lại). Việc thiếu số FLOPS làm cho việc so sánh trực tiếp với phương pháp của chúng tôi khó khăn hơn. GDP (Guo et al. (2021)) cắt tỉa

--- TRANG 6 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

| Phương pháp | Base | Top1(S) | Drop% | S% | Train F | Test F | bình luận |
|---|---|---|---|---|---|---|---|
| RigL | 76.8 | 74.6 | 2.86 | 80 | 0.33 | 0.22 | độ thưa đồng nhất |
| TopKAST* | 76.8 | 75.7 | 0.94 | 80 | 0.48 | 0.22 | độ thưa đồng nhất |
| TopKAST*+PP | 76.8 | 76.24 | 0.73 | 80 | 0.48 | 0.22 | độ thưa đồng nhất |
| TopKAST+GA | 76.8 | 76.47 | 0.43 | 80 | 0.48 | 0.22 | độ thưa đồng nhất |
| MEST 1.7+EM | 76.9 | 76.71 | 0.25 | 80 | 0.57 | 0.21 | độ thưa đồng nhất |
| DST | 74.95 | 74.02 | 1.24 | 80.4 | – | 0.15 | độ thưa có thể học |
| STR | 77.01 | 76.19 | 1.06 | 79.55 | 0.54 | 0.18 | độ thưa có thể học |
| AutoSparse | 77.01 | 76.77 | 0.31 | 79.67 | 0.51 | 0.14 | α₀=.75, α=0@epoch90 |
| AutoSparse | 77.01 | 76.59 | 0.55 | 80.78 | 0.46 | 0.14 | α₀=.8, α=0@epoch70 |
| MEST 1.7+EM | 76.9 | 75.91 | 1.29 | 90 | 0.28 | 0.11 | độ thưa đồng nhất |
| DST | 74.95 | 72.78 | 2.9 | 90.13 | – | 0.087 | độ thưa có thể học |
| STR | 77.01 | 74.31 | 3.51 | 90.23 | 0.44 | 0.083 | độ thưa có thể học |
| AutoSparse | 77.01 | 75.9 | 1.44 | 85.1 | 0.42 | 0.096 | α₀=.9, α=0@epoch50 |
| AutoSparse | 77.01 | 75.19 | 2.36 | 89.94 | 0.40 | 0.081 | α₀=.9, α=0@epoch45 |
| STR | 77.01 | 70.4 | 8.58 | 95.03 | 0.28 | 0.039 | độ thưa có thể học |
| AutoSparse | 77.01 | 70.84 | 8.01 | 95.09 | 0.2 | 0.036 | α₀=0.8, α=0@epoch20 |

Bảng 1: ResNet50 trên ImageNet: So sánh độ chính xác, độ thưa và FLOPS (dense 1×) cho huấn luyện và suy luận cho các phương pháp huấn luyện thưa được chọn. TopKAST*: TopKAST với 0% độ thưa backward. TopKAST*+GA: TopKAST* với Gradient Annealing tăng độ chính xác mặc dù có cùng FLOPS huấn luyện/suy luận. Đối với AutoSparse độ thưa 79.67%, α₀=0.75 được giảm đến epoch 90 và sau đó đặt thành 0 (có nghĩa là độ thưa forward và backward 80% sau epoch 90). Tương tự, đối với AutoSparse độ thưa 95.09%, α₀=0.8 được giảm đến epoch 20 và sau đó đặt thành 0. Độ lệch chuẩn của kết quả cho AutoSparse nhỏ hơn 0.03.

| Phương pháp | S% | Top1(S) | Drop% | Train F | Test F | Back(S) | bình luận |
|---|---|---|---|---|---|---|---|
| AutoSparse | 83.74 | 75.02 | 2.58 | 0.37 | 0.128 | 50 | α₀=1.0, s₀=-8, w grad 50% thưa |
| TopKAST | 80 | 75 | 2.34 | 0.33 | 0.22 | 50 | độ thưa đồng nhất fwd & in grad 80% w grad 50% |

Bảng 2: ResNet50 trên ImageNet: AutoSparse với độ thưa được đặt rõ ràng cho gradient trọng số. Đối với AutoSparse độ thưa 83.74%, α₀=1 được giảm đến epoch 100 dẫn đến độ thưa giống hệt nhau cho forward và input gradient, cùng với độ thưa 50% cho gradient trọng số trong suốt quá trình huấn luyện (bao gồm việc gọi phương pháp TopK).

các kênh sử dụng số lượng FLOPS như một regularizer để đạt được trade-off tối ưu độ chính xác so với tính toán (giảm 0.4% độ chính xác với 0.49× FLOPS suy luận). Phương pháp này không thể so sánh trực tiếp với kết quả của chúng tôi cho độ thưa không có cấu trúc. Cuối cùng, MEST (SOTA cho huấn luyện thưa đồng nhất) đạt được độ chính xác tương đương cho Resnet50 thưa 80%, tuy nhiên, sử dụng nhiều hơn 12% FLOPS huấn luyện và 50% FLOPS suy luận (vì mô hình thưa của họ không hiệu quả FLOPS).

Kết quả cho AutoSparse với Độ thưa Backward Được xác định trước

Trong AutoSparse, độ thưa forward được xác định bởi các trọng số dưới ngưỡng đã học (gọi độ thưa động này là S). Chúng tôi giảm α = 0.85 trong suốt quá trình huấn luyện thưa và áp dụng TopK để chọn các chỉ số của 50% trọng số có độ lớn lớn nhất (trong forward pass) được sử dụng trong tính toán gradient mất mát (S≥50%). Những trọng số này là tập con của các trọng số khác không được sử dụng trong tính toán forward. Bằng cách này, chúng ta có độ thưa S cho tính toán forward và input gradient, và độ thưa 50% cho gradient trọng số. Gradient annealing được áp dụng trên gradient của những trọng số xuất hiện trong top 50% này nhưng bị cắt tỉa trong forward pass. Chúng tôi so sánh kết quả của chúng tôi với TopKAST độ thưa forward 80% và độ thưa backward 50% trong Bảng 2. Chúng tôi tạo ra mô hình thưa 83.74% giảm đáng kể FLOPS suy luận (≈ 0.128× của baseline) trong khi đạt được độ chính xác và FLOPS huấn luyện tương tự như TopKAST mặc dù TopKAST tốn nhiều hơn 1.7× FLOPS suy luận so với chúng tôi.

--- TRANG 7 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

| Phương pháp | Base | Top1(S) | Drop% | S% | Train F | Test F | bình luận |
|---|---|---|---|---|---|---|---|
| STR | 71.95 | 68.35 | 5 | 75.28 | 0.43 | 0.18 | độ thưa có thể học |
| AutoSparse | 71.95 | 69.97 | 2.75 | 74.98 | 0.53 | 0.21 | α₀=.4, α=0 @ epoch 90 |
| STR | 71.95 | 64.83 | 9.9 | 85.8 | 0.37 | 0.1 | độ thưa có thể học |
| AutoSparse | 71.95 | 64.87 | 9.84 | 86.36 | 0.30 | 0.1 | α₀=.8, α=0 @ epoch 20 |
| AutoSparse | 71.95 | 64.18 | 10.8 | 87.72 | 0.25 | 0.08 | α₀=.6, α=0 @ epoch 20 |

Bảng 3: MobileNetV1 trên ImageNet: So sánh độ chính xác, độ thưa và FLOPS (dense 1×) cho huấn luyện và suy luận cho các phương pháp huấn luyện thưa được chọn. AutoSparse đạt được độ chính xác cao hơn đáng kể cho độ thưa tương đương. Đối với AutoSparse độ thưa 74.98%, α = 0.4 được giảm đến epoch 90 và sau đó đặt thành 0. Đối với AutoSparse (87.72%, 86.36%) độ thưa, (α = 0.6, α = 0.8) được giảm đến epoch 20 và sau đó đặt thành 0. Đặt α = 0 tại epoch t có nghĩa là độ thưa forward và backward giống hệt nhau sau epoch t. Độ lệch chuẩn cho AutoSparse là 0.04.

| Phương pháp | BLEU | S% | BLEU(S) | Drop% | bình luận |
|---|---|---|---|---|---|
| STR | 27.83 | 59.25 | 27.49 | 1.22 | triển khai của chúng tôi |
| AutoSparse | 27.83 | 60.99 | 27.57 | 0.93 | α₀=0.4, α=0 tại epoch 10 |

Bảng 4: Transformer trên WMT: AutoSparse đạt được độ chính xác tốt hơn ở độ thưa cao hơn so với STR. Độ lệch chuẩn cho BLEU AutoSparse và BLEU STR lần lượt là 0.08 và 0.09.

5.2 AUTOSPARSE CHO MÔ HÌNH NGÔN NGỮ

Đối với Mô hình ngôn ngữ, chúng tôi áp dụng AutoSparse của chúng tôi trên mô hình Transformer sử dụng exponential decay cho α₀ = 0.4. Lịch trình annealing này dẫn đến trade-off tốt hơn giữa độ chính xác và độ thưa. Ngoài ra, chúng tôi đặt α = 0 sau 10 epoch để tận dụng lợi thế của tính toán thưa hơn trong backward pass. Với BLEU dense baseline 27.83, AutoSparse đạt được điểm BLEU 27.57 (giảm 0.93%) với độ thưa tổng thể 60.99%. Đối với STR (α = 0), chúng tôi sử dụng weight decay 0.01 với s ban đầu = -20, và chúng tôi đạt được điểm BLEU 27.49 (giảm 1.22%) ở độ thưa 59.25%.

6 THẢO LUẬN

Mục đích của gradient annealing là thiết lập một trade-off giữa độ thưa động, có thể học và độ chính xác (cao) trong đó chúng ta có thể được hưởng lợi từ tính toán thưa từ các epoch đầu. AutoSparse của chúng tôi có thể học một phân bố độ thưa hiệu quả FLOPS đáng kể hơn cho suy luận so với các phương pháp độ thưa đồng nhất. Tuy nhiên, ở độ thưa cực độ (khi chúng ta hy sinh độ chính xác) chúng tôi đã quan sát thấy rằng các phương pháp huấn luyện thưa-đến-thưa sử dụng các epoch dài hơn nhiều (~2.5-5×) với độ thưa đồng nhất khôi phục mất mát độ chính xác đáng kể (MEST, RigL), và có thể vượt trội so với các phương pháp độ thưa có thể học về mặt FLOPS huấn luyện. Các phương pháp độ thưa có thể học đạt được độ thưa mục tiêu theo cách dần dần gây ra số lượng FLOPS nhiều hơn ở các epoch đầu, trong khi các phương pháp độ thưa đồng nhất có ít FLOPS hơn mỗi lần lặp (bỏ qua các overhead khác cho huấn luyện dài hơn). Chúng tôi muốn nghiên cứu AutoSparse của chúng tôi cho các epoch huấn luyện mở rộng, cùng với (a) cập nhật gradient dense không thường xuyên, (b) cập nhật gradient thưa để cải thiện độ chính xác, FLOPS huấn luyện và dấu chân bộ nhớ. Ngoài ra, lựa chọn thích hợp của gradient annealing có thể phụ thuộc vào các siêu tham số học, ví dụ: giảm learning rate, lựa chọn optimizer, lượng luồng gradient, v.v. Sẽ thú vị khi tự động hóa các lựa chọn khác nhau của gradient annealing qua các workload.

--- TRANG 8 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

TÀI LIỆU THAM KHẢO

D. Amodei và D. Hernandez. AI and Compute. https://openai.com/blog/ai-and-compute/.

N. Ardalani, J. Hestness, và G. Diamos. Empirically Characterizing Overparameterization Impact on Convergence. Trong https://openreview.net/forum?id=S1lPShAqFm, 2019.

S. Arora, N. Cohen, và E. Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. Trong Jennifer Dy và Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 của Proceedings of Machine Learning Research, pp. 244–253. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/arora18a.html.

K. Azarian, Y. Bhalgat, J. Lee, và T. Blankevoort. Learned Threshold Pruning. Trong https://arxiv.org/pdf/2003.00075.pdf, 2021.

M. Belkin, D. Hsu, S. Ma, và S. Mandal. Reconciling modern machine-learning practice and the classical bias-variance trade-off. Proceedings of the National Academy of Sciences, 116(32): 15849–15854, 2019. doi: 10.1073/pnas.1903070116. URL https://www.pnas.org/doi/abs/10.1073/pnas.1903070116.

Y. Bengio, N. Léonard, và A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. Trong arXiv preprint arXiv:1308.3432, 2013.

Dipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar, Sasikanth Avancha, Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat Kaul, Evangelos Georganas, et al. Mixed precision training of convolutional neural networks using integer operations. arXiv preprint arXiv:1802.00930, 2018.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, và L. Fei-Fei. A large-scale hierarchical image database. Trong IEEE conference on computer vision and pattern recognition, 2009.

T. Dettmers và L. Zettlemoyer. Sparse Networks from Scratch: Faster Training without Losing Performance. Trong https://arxiv.org/pdf/1907.04840.pdf, 2019.

U. Evci, T. Gale, J. Menick, P.S. Castro, và E. Elsen. Rigging the Lottery: Making All Tickets Winners. Trong https://arxiv.org/pdf/1911.11134.pdf, 2021.

U. Evci, B. van Merriënboer, T. Unterthiner, M. Vladymyrov, và F. Pedregosa. GRADMAX: Growing Neural Networks Using Gradient Information. Trong International Conference on Learning Representations (ICLR), 2022.

J. Frankle và M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. Trong International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rJl-b3RcF7.

T. Gale, E. Elsen, và S. Hooker. The State of Sparsity in Deep Neural Networks. Trong https://arxiv.org/pdf/1902.09574.pdf, 2019.

Y. Guo, A. Yao, và Y. Chen. Dynamic Network Surgery for Efficient DNNs. Trong Advances in Neural Information Processing Systems (NeurIPS), 2016.

Y. Guo, H. Yuan, J. Tan, Z. Wang, S. Yang, và J. Liu. GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization. Trong https://arxiv.org/pdf/2109.02220.pdf, 2021.

S. Han, J. Pool, J. Tran, và W. Dally. Learning both weights and connections for efficient neural network. Trong In Advances in Neural Information Processing Systems (NeurIPS), 2015.

K. He, X. Zhang, S. ren, và J. Sun. Deep residual learning for image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.

--- TRANG 9 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

T. Hoefler, D Alistarh, T. Ben-Nun, N. Dryden, và A. Peste. Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks. Journal of Machine Learning Research, pp. 1–124, 2021.

A.G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, và H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. Trong arXiv preprint arXiv:1704.04861, 2017.

S.M. Jayakumar, R. Pascanu, J.W. Rae, S. Osindero, và E. Elsen. Top-KAST: Top-K Always Sparse Training. Trong, 2021.

Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy, Bharat Kaul, và Pradeep Dubey. A study of bfloat16 for deep learning training, 2019. URL https://arxiv.org/abs/1905.12322.

A. Kusupati, V. Ramanujan, R. Somani, M. Wortsman, P. Jain, S. Kakade, và A. Farhadi. Soft Threshold Weight Reparameterization for Learnable Sparsity. Trong https://arxiv.org/abs/2002.03231, 2020.

N. Lee, T. Ajanthan, và P.H.S Torr. SNIP: Single-shot Network Pruning based on Connection Sensitivity. Trong International Conference on Learning Representations (ICLR), 2019.

Z. Li, E. Wallace, S. Shen, K. Lin, K. Keutzer, D. Klein, và J. Gonzalez. Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers. Trong https://arxiv.org/pdf/2002.11794.pdf, 2020.

T. Lin, S. U. Stich, L. Barba, D. Dmitriev, và M. Jaggi. Dynamic model pruning with feedback. Trong International Conference on Learning Representations (ICLR), 2020.

J. Liu, Z. Xu, R. Shi, R.C.C Cheung, và H.K.H So. Dynamic Sparse Training: Find Efficient Sparse Network from Scratch with Trainable Masked Layers. Trong International Conference on Learning Representations (ICLR), 2020.

Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, và Bharat Kaul. Mixed precision training with 8-bit floating point, 2020. URL https://openreview.net/forum?id=HJe88xBKPr.

P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al. Mixed precision training. Trong arXiv preprint arXiv:1710.03740, 2017.

A. Mishra, J. A. Latorre, J. Pool, D. Stosic, D. Stosic, G. Venkatesh, C. Yu, và P. Micikevicius. Accelerating Sparse Deep Neural Networks. Trong https://arxiv.org/abs/2104.08378, 2021.

D.C. Mocanu, E. Mocanu, P. Stone, P.H. Nguyen, M. Gibescu, và A. Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Trong https://www.nature.com/articles/s41467-018-04316-3, 2018.

H. Mostafa và X. Wang. Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization. Trong In International Conference on Machine Learning (ICML), 2019.

S. Narang, G. F. Diamos, S. Sengupta, và E. Elsen. Exploring sparsity in recurrent neural networks. CoRR, abs/1704.05119, 2017. URL http://arxiv.org/abs/1704.05119.

Md. A. Raihan và T.M. Aamodt. Sparse Weight Activation Training. Trong Conference on Neural Information Processing Systems (NeurIPS 2020), 2020.

P. Savarese, H. Silva, và M. Maire. Winning the Lottery with Continuous Sparsification. Trong https://arxiv.org/pdf/1912.04427.pdf, 2021.

--- TRANG 10 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

J. Schwarz, S. M. Jayakumar, R. Pascanu, P.E. Latham, và Y. W. Teh. Powerpropagation: A sparsity inducing weight reparameterisation. Trong https://arxiv.org/pdf/2110.00296.pdf, 2021.

N. Ström. Sparse Connection And Pruning In Large Dynamic Artificial Neural Networks. Trong http://www.nikkostrom.com/publications/euro97/euro97.pdf, 1997.

X. Sun, J. Choi, C-Y Chen, N. Wang, S. Venkataramani, V. Srinivasan, X. Cui, W. Zhang, và K. Gopalakrishnan. Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks. Trong In The Conference and Workshop on Neural Information Processing Systems, 2019.

H. Tanaka, D. Kunin, D.L. Yamins, và S. Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. Trong Advances in Neural Information Processing Systems (NeurIPS), 2020.

Z. Tang, L. Luo, B. Xie, Y. Zhu, R. Zhao, L. Bi, và C. Lu. Automatic Sparse Connectivity Learning for Neural Networks. Trong https://arxiv.org/pdf/2201.05020.pdf, 2022.

J. van Amersfoort, M. Alizadeh, S. Farquhar, N. Lane, và Y. Gal. Single shot structured pruning before training. Trong https://arxiv.org/abs/2007.00389, 2020.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, Ł. Kaiser, và I. Polosukhin. Attention Is All You Need. Trong Neural Information Processing Systems, 2017.

C. Wang, G. Zhang, và R. Grosse. PICKING WINNING TICKETS BEFORE TRAINING BY PRESERVING GRADIENT FLOW. Trong International Conference on Learning Representations (ICLR), 2020.

N. Wang, J. Choi, D. Brand, C-Y Chen, và K. Gopalakrishnan. Training Deep Neural Networks with 8-bit Floating Point Numbers. Trong https://arxiv.org/pdf/1812.08011.pdf, 2018.

M. Wortsman, A. Farhadi, và M. Rastegari. Discovering neural wirings. Trong Conference on Neural Information Processing Systems (NeurIPS, 2019.

G. Yuan, X. Ma, W. Niu, Z. Li, Z. Kong, N. Liu, Y. Gong, Z. Zhan, C. He, Q. Jin, S. Wang, M. Qin, B. Ren, Y. Wang, S. Liu, và X. Lin. MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the Edge. Trong Neural Information Processing Systems (NeurIPS), 2021.

Y. Zhang, M. Lin, M. Chen, Z. Xu, F. Chao, Y. Shen, K. Li, Y. Wu, và R. Ji. Optimizing Gradient-driven Criteria in Network Sparsity: Gradient is All You Need. Trong https://arxiv.org/pdf/2201.12826.pdf, 2022.

M.H. Zhu và S. Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. Trong https://arxiv.org/pdf/1710.01878.pdf, 2017.

--- TRANG 11 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

7 PHỤ LỤC A

7.1 TÍNH TOÁN GRADIENT

Cho I(·) biểu thị một hàm chỉ thị, ⟨·,·⟩ biểu thị tích vô hướng, ⊙ biểu thị phép nhân theo từng phần tử, và S biểu thị Sh,g. g nên liên tục để chúng ta có thể áp dụng gradient descent cho việc học. Sử dụng định nghĩa trong phương trình (1), gradient mất mát cho W_ℓ và s_ℓ là

L^(t) ≜ L(S(W^(t),s^(t)),D); G_ℓ^(t) ≜ ∇_S(W_ℓ,s_ℓ)L^(t)
∇_{W_ℓ^(t)}S(W_ℓ,s_ℓ) ≜ G_ℓ^(t) ⊙ G_I^(t); G_I^(t) ≜ I{S(W_ℓ^(t),s_ℓ^(t)) ≠ 0} + αI{S(W_ℓ^(t),s_ℓ^(t)) = 0}

(Gradient mất mát cho W) ∇_{W_ℓ^(t)}L^(t) ≜ G_ℓ^(t) ⊙ G_I^(t)     (10)

(Gradient mất mát cho s) ∇_{s_ℓ^(t)}L^(t) ≜ g'(s_ℓ^(t)) ⟨G_ℓ^(t), sign(W_ℓ^(t)) ⊙ G_I^(t)⟩     (11)

Ngoài mất mát phân loại, regularization L2 tiêu chuẩn được thêm vào W_ℓ và s_ℓ, ∀ℓ ∈ [L] với siêu tham số weight decay λ. Gradient được nhận bởi W_ℓ và s_ℓ cho regularization lần lượt là λW_ℓ và λs_ℓ. Cập nhật tham số bao gồm việc thêm momentum vào gradient, và nhân với learning rate η. Chúng tôi chọn hàm Sigmoid cho g(s) như trong Kusupati et al. (2020).

Hình 2: So sánh Độ thưa Mô hình Động cho AutoSparse, STR và OptG.

7.2 NGHIÊN CỨU LOẠI BỎ

Chúng tôi thực hiện các nghiên cứu bổ sung trên phương pháp STR (Kusupati et al. (2020)) để kiểm tra xem độ thưa huấn luyện trung bình có thể được cải thiện bằng cách chọn các siêu tham số khác nhau. Chúng tôi khởi tạo tham số ngưỡng s_init với các giá trị lớn hơn để giới thiệu độ thưa trong các epoch đầu – chúng tôi cũng điều chỉnh giá trị λ một cách thích hợp. Hình 3 cho thấy rằng khi giá trị s_init được tăng lên 5 từ giá trị gốc -3 (≈ -200), phương pháp giới thiệu độ thưa sớm trong quá trình huấn luyện – tuy nhiên mô hình nhanh chóng phân kỳ sau khoảng 5 epoch.

7.3 HỌC PHÂN BỐ ĐỘ THƯA

AutoSparse học một phân bố độ thưa khác đáng kể so với STR mặc dù đạt được độ thưa mô hình tương tự. AutoSparse có thể tạo ra độ thưa cao hơn trong các lớp trước dẫn đến giảm đáng kể FLOPS (Hình 4).

7.4 HỌC NGÂN SÁCH ĐỘ THƯA BẰNG TỰ ĐỘNG ĐIỀU CHỈNH (AUTOSPARSE-AT)

Những thách thức chính của huấn luyện thưa thực sự là (1) độ thưa vốn có (cả ngân sách và phân bố) của các mô hình khác nhau (mới) được huấn luyện trên dữ liệu khác nhau (mới) có thể không được biết, (2) độ chính xác baseline của huấn luyện dense không được biết (trừ khi chúng ta thực hiện huấn luyện dense). Làm thế nào chúng ta có thể tạo ra một giải pháp thưa chất lượng cao thông qua huấn luyện thưa? Chúng tôi tìm cách trả lời điều này bằng cách điều chỉnh siêu tham số GA α của chúng tôi như được hiển thị trong Thuật toán 1. Chúng tôi đã quan sát thấy rằng việc tăng α giảm độ thưa (cải thiện độ chính xác) và ngược lại. Để tự động điều chỉnh α (cân bằng động độ thưa và độ chính xác) chúng tôi muốn mất mát thưa theo궤 đạo mất mát của huấn luyện dense. Để làm điều này, chúng tôi ghi nhận mất mát của một vài epoch

--- TRANG 12 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

Hình 3: Nghiên cứu loại bỏ về huấn luyện thưa sớm trên STR và AutoSparse. s_init = 5 cho tất cả các trường hợp. STR 80λ = 0.000017 (được sử dụng cho STR 80%), STR λ = 0.000030517578125/209, AutoSparse 80 và AutoSparse 90 đều có λ = 0.000030517578125/209. Độ thưa cho cả STR và STR 80 tăng nhanh chóng theo cách không kiểm soát được để cắt tỉa tất cả các tham số trong vài epoch, trong khi AutoSparse đạt được độ thưa theo cách được kiểm soát.

Hình 4: AutoSparse học một phân bố độ thưa khác với STR. STR đạt được độ thưa cao hơn cho các lớp sau (mật độ tham số cao hơn) trong khi AutoSparse tạo ra độ thưa nhiều hơn cho các lớp trước, dẫn đến giảm FLOP đáng kể mặc dù có độ thưa mô hình cuối cùng giống nhau.

của huấn luyện dense (gây ra số lượng FLOPS nhiều hơn) và sử dụng nó như một tham chiếu để đo chất lượng của huấn luyện thưa của chúng tôi. Đối với AutoSparse, chúng tôi bắt đầu huấn luyện thưa với α = 0.5 (giá trị không thiên vị vì α ∈ [0,1]) và ghi nhận mất mát huấn luyện thưa trung bình ở mỗi epoch. Nếu mất mát huấn luyện thưa vượt quá mất mát dense vượt quá một số dung sai, chúng tôi tăng α để giảm độ thưa để mất mát thưa theo sát mất mát dense trong epoch tiếp theo. Mặt khác, khi mất mát thưa gần với mất mát dense, chúng tôi giảm α để khám phá độ thưa có sẵn nhiều hơn. Việc điều chỉnh này chỉ xảy ra trong vài epoch đầu của huấn luyện AutoSparse + AutoTune và phần còn lại của huấn luyện thưa áp dụng gradient annealing với giá trị α đã được điều chỉnh.

7.4.1 THÍ NGHIỆM AUTOSPARSE + TỰ ĐỘNG ĐIỀU CHỈNH

Chúng tôi thực hiện tự động điều chỉnh α cho Resnet50 và MobileNetV1 trên tập dữ liệu ImageNet. Chúng tôi sử dụng các siêu tham số huấn luyện giống hệt nhau cho cả ResNet50 và MobileNetV1 cho AutoSparse + Auto-Tuning của chúng tôi cùng với α₀ = 0.5 ban đầu không thiên vị giống hệt nhau để hiểu hiệu quả của việc tự động điều chỉnh gradient annealing. Điều này đơn giản hóa các vấn đề về việc điều chỉnh thủ công nhiều siêu tham số.

Bắt đầu từ một giá trị không thiên vị 0.5, α được điều chỉnh trong 9 epoch đầu (10% của các epoch huấn luyện dense dự định) của huấn luyện thưa bằng Thuật toán 1. Hình 5 cho thấy cách α được điều chỉnh khác nhau cho ResNet50 và MobileNetV1 theo cách động. Điều này là do ngân sách độ thưa vốn có cho ResNet50 khác với MobileNetV1 để tạo ra một mô hình thưa có độ chính xác cao. Hình 6a và 6b cho thấy cách trade-off độ thưa-độ chính xác được khám phá từ các epoch đầu cho cả ResNet50 và MobileNetV1. Bảng 5) tóm tắt các thí nghiệm khác nhau.

--- TRANG 13 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

Thuật toán 1 AutoSparse + AutoTune
• Đầu vào Mô hình M, dữ liệu D, tổng số epoch T, epoch điều chỉnh T', α₀, mất mát tham chiếu L ∈ R^T'
• α ← α₀ /* khởi tạo */
• Cho t = 0, ..., T-1 epoch
• Huấn luyện mô hình M trên D và tính mất mát huấn luyện L_t trong (6)
• Nếu t < T' /* Giai đoạn tự động điều chỉnh cho α */
• Nếu L_t > (1 + ε₀)L[t], α ← (1 + ε₁)α /* giảm độ thưa để kiểm soát mất mát */
• Ngược lại α ← (1 - ε₂)α /* giảm α để khám phá độ thưa nhiều hơn */
• Ngược lại
• Nếu t == T', α₀ ← α
• α ← α₀ Sigmoid-Cosine-Decay(t-T', T-T') trong (4)
• Nếu t >= 90, α ← 0 /* đặt lại α sau epoch nhất định để có lợi ích tính toán */
• Đầu ra Mô hình thưa đã huấn luyện M

Hình 5: Tự động điều chỉnh siêu tham số α điều khiển động trade-off giữa độ chính xác và độ thưa bằng mất mát huấn luyện trung bình chỉ của 9 epoch (~10% ngân sách của các epoch dense) của quá trình huấn luyện dense. Trong các epoch còn lại, α được giảm theo phương trình 4. α₀ = 0.5 bất kể loại mạng.

(a) AutoSparse+AT ResNet50     (b) AutoSparse+AT MobileNetV1

Hình 6: AutoSparse+AT: AutoSparse tự động điều chỉnh của chúng tôi gần như khớp với độ chính xác dense trong khi khám phá động lượng độ thưa vốn có phù hợp bằng cách điều chỉnh α sử dụng mất mát huấn luyện trung bình chỉ của 9 epoch (~10% ngân sách của các epoch dense) của quá trình chạy dense.

AutoSparse + AutoTune của chúng tôi có thể khám phá điều này theo cách tự động trong một thí nghiệm duy nhất. Điều này tiết kiệm rất nhiều chi phí huấn luyện của việc điều chỉnh thủ công và chạy lại các thí nghiệm để tìm sự cân bằng đúng của độ thưa-độ chính xác.

--- TRANG 14 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

| Mạng | Base | (ε₀, ε₁, ε₂) | Top-1 (S) | Độ thưa | Train F | Test F |
|---|---|---|---|---|---|---|
| ResNet50 | 77.01 | (0.01, 0.05, 0.005) | 76.74 | 78.6 | – | – |
| | 77.01 | (0.01, 0.05, 0.01) | 76.58 | 80.1 | 0.59 | 0.14 |
| MobileNetV1 | 71.95 | (0.01, 0.05, 0.005) | 70.5 | 70.5 | 0.67 | 0.26 |
| | 71.95 | (0.01, 0.05, 0.01) | 70.43 | 71.2 | – | – |

Bảng 5: AutoSparse + AutoTune (Thuật toán 1) trade-off độ chính xác so với độ thưa cho ngân sách 100 epoch. Trong quá trình tự động điều chỉnh α (9 epoch đầu), chúng tôi đặt dung sai mất mát ε₀ = 1%, tăng hoặc giảm α bằng ε₁ hoặc ε₂, tương ứng. Chúng tôi sử dụng ε₁ = 5% và ε₂ = 0.5% hoặc 1%. α₀ = 0.5 ban đầu được điều chỉnh động (Thuật toán 1) và nó được đặt thành 0 tại epoch 90. Độ lệch chuẩn AutoSparse cho ResNet50 và MobileNetV1 lần lượt là 0.07 và 0.08.

--- TRANG 15 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

8 PHỤ LỤC B

8.1 MÃ PYTORCH CHO AUTOSPARSE VỚI AUTOTUNE

Mã này được xây dựng bằng code base cho STR Kusupati et al. (2020). Những thay đổi chính là: (1) thay thế ReLU bằng ReLU không bão hòa (class NSReLU), (2) giảm gradient scaling của các đầu vào âm của NSReLU (neg_grad được ký hiệu bằng α trong bài báo) mỗi epoch, (3) tự động điều chỉnh α này dựa trên ref_loss (dense) trong vài epoch đầu (hàm grad_annealing)

```python
#—————————————————
def set_neg_grad(neg_grad_val = 0.5):
    NSReLU.neg_grad = neg_grad_val

def get_neg_grad():
    return NSReLU.neg_grad

def set_neg_grad_max(neg_grad_val = 0.5):
    NSReLU.neg_grad_max = neg_grad_val

def get_neg_grad_max():
    return NSReLU.neg_grad_max

#—————————————————
class NSReLU(torch.autograd.Function):
    neg_grad = 0.5
    neg_grad_max = 0.5
    #topk = 0.5 # cho độ thưa backward
    
    @staticmethod
    def forward(self,x):
        self.neg = x < 0
        #k = int(NSReLU.topk * x.numel()) # cho độ thưa backward
        #kth_val, kth_id = torch.kthvalue(x.view(-1), k) # cho độ thưa backward
        #self.exclude = x < kth_val # cho độ thưa backward
        return x.clamp(min=0.0)
    
    @staticmethod
    def backward(self,grad_output):
        grad_input = grad_output.clone()
        grad_input[self.neg] *= NSReLU.neg_grad
        #grad_input[self.exclude] *= 0.0 # cho độ thưa backward
        return grad_input

#—————————————————
def non_sat_relu(x):
    return NSReLU.apply(x)

def sparseFunction(x, s, activation=torch.relu, g=torch.sigmoid):
    return torch.sign(x)*activation(torch.abs(x)-g(s))

def initialize_sInit():
    if parser_args.sInit_type == "constant":
        return parser_args.sInit_value*torch.ones([1, 1])

#—————————————————
class STRConv(nn.Conv2d):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.activation = non_sat_relu
        if parser_args.sparse_function == 'sigmoid':
```

--- TRANG 16 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

```python
            self.g = torch.sigmoid
            self.s = nn.Parameter(initialize_sInit())
        else:
            self.s = nn.Parameter(initialize_sInit())
    
    def forward(self, x):
        sparseWeight = sparseFunction(self.weight, self.s, self.activation, self.g)
        x = F.conv2d( x, sparseWeight, self.bias, self.stride, self.padding, self.dilation, self.groups )
        return x

#—————————————————
#—————————————————
from utils.conv_type import get_neg_grad, set_neg_grad, get_neg_grad_max, set_neg_grad_max

#—————————————————
def grad_annealing(epoch, loss, ref_loss):
    # ref_loss: mất mát dense tham chiếu cho tự động điều chỉnh
    # số epoch tự động điều chỉnh
    old_neg_grad = get_neg_grad()
    new_neg_grad = 0.0
    
    if epoch < len(ref_loss):
        dense_loss = float(ref_loss[str(epoch)])
        eps_0 = 0.01
        eps_1 = 0.05
        eps_2 = 0.005
        
        if loss > dense_loss * (1.0 + eps_0):
            new_neg_grad = old_neg_grad * (1.0 + eps_1)
        else:
            new_neg_grad = old_neg_grad * (1.0 - eps_2)
    else:
        if epoch == len(ref_loss):
            set_neg_grad_max(get_neg_grad())
        new_neg_grad = _sigmoid_cosine_decay(
            args.epochs - len(ref_loss), epoch - len(ref_loss), get_neg_grad_max())
    
    set_neg_grad(new_neg_grad)

#—————————————————
def _cosine_decay(total_epochs, epoch, neg_grad_max):
    PI = torch.tensor(math.pi)
    return 0.5 * neg_grad_max * (1 + torch.cos(PI * epoch / float(total_epochs)))

def _sigmoid_decay( rem_total_epochs, rem_epoch, neg_grad_max):
    Lmax = 6
    Lmin = -6
    return neg_grad_max * (1 - torch.sigmoid(torch.tensor(Lmin+(Lmax - Lmin) *
        ( float ( rem_epoch ) / rem_total_epochs ) ) ) )

def _sigmoid_cosine_decay(rem_total_epochs, rem_epoch, neg_grad_max):
    cosine_scale = _cosine_decay(rem_total_epochs, rem_epoch, get_neg_grad_max())
    sigmoid_scale = _sigmoid_decay(rem_total_epochs,
        rem_epoch, get_neg_grad_max())
    return max(cosine_scale, sigmoid_scale)

#—————————————————
def train(train_loader, model, criterion, optimizer, epoch, ref_loss, args):
    losses = AverageMeter("Loss", ":.3f")
    top1 = AverageMeter("Acc@1", ":6.2f")
    top5 = AverageMeter("Acc@5", ":6.2f")
```

--- TRANG 17 ---
Được xuất bản như bài báo workshop SNN tại ICLR 2023

```python
    # chuyển sang chế độ huấn luyện
    model.train()
    
    if epoch == 0:
        set_neg_grad(args.init_neg_grad)
    
    batch_size = train_loader.batch_size
    num_batches = len(train_loader)
    
    for i, (images, target) in tqdm.tqdm( enumerate(train_loader),
            ascii=True, total=len(train_loader) ):
        
        output = model(images)
        loss = criterion(output, target.view(-1))
        
        acc1, acc5 = accuracy(output, target, topk=(1,5))
        losses.update(loss.item(), images.size(0))
        top1.update(acc1.item(), images.size(0))
        top5.update(acc5.item(), images.size(0))
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # — anneal gradient mỗi epoch —
    grad_annealing(epoch, losses.avg, ref_loss)
    
    return top1.avg, top5.avg
```
