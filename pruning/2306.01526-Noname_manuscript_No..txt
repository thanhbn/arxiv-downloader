# 2306.01526.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2306.01526.pdf
# File size: 18664045 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Noname manuscript No.
(will be inserted by the editor)
Group channel pruning and spatial attention distilling
for object detection
Yun Chu ·Pu Li ·Yong Bai ·Zhuhua
Hu ·Yongqing Chen ·Jiafeng Lu
Received: date 2021.09 / Accepted: 2022.03
Abstract Due to the over-parameterization of neural networks, many model com-
pression methods based on pruning and quantization have emerged. They are re-
markable in reducing the size, parameter number, and computational complexity
of the model. However, most of the models compressed by such methods need the
support of special hardware and software, which increases the deployment cost.
Moreover, these methods are mainly used in classification tasks, and rarely di-
rectly used in detection tasks. To address these issues, for the object detection
network we introduce a three-stage model compression method: dynamic sparse
training, group channel pruning, and spatial attention distilling. Firstly, to select
out the unimportant channels in the network and maintain a good balance be-
tween sparsity and accuracy, we put forward a dynamic sparse training method,
which introduces a variable sparse rate, and the sparse rate will change with the
training process of the network. Secondly, to reduce the effect of pruning on net-
work accuracy, we propose a novel pruning method called group channel pruning.
In particular, we divide the network into multiple groups according to the scales of
the feature layer and the similarity of module structure in the network, and then
we use different pruning thresholds to prune the channels in each group. Finally,
to recover the accuracy of the pruned network, we use an improved knowledge dis-
tillation method for the pruned network. Especially, we extract spatial attention
information from the feature maps of specific scales in each group as knowledge for
distillation. In the experiments, we use YOLOv4 as the object detection network
and PASCAL VOC as the training dataset. Our method reduces the parameters
of the model by 64.7% and the calculation by 34.9%. When the input image size
is 416×416, compared with the original network model with 256MB size and 87.1
accuracies, our compressed model achieves 86.6 accuracies with 90MB size. To
demonstrate the generality of our method, we replace the backbone to Darknet53
and Mobilenet and also achieve satisfactory compression results.
Yun Chu, Yong Bai, Zhuhua Hu, Yongqing Chen, Jiafeng Lu are with School of Information
and Communication Engineering, Hainan University, Haikou, 570288, China
·Pu Li is with the School of Software and Microelectronics, Peking University, Beijing, 100871,
China.
Correspondence should be addressed to Yong Bai, E-mail: bai@hainanu.edu.cnarXiv:2306.01526v1  [cs.CV]  2 Jun 2023

--- PAGE 2 ---
2 Yun Chu et al.
Keywords model compression ·object detection ·group channel pruning ·
knowledge distillation
1 Introduction
In recent years, CNNs (Convolutional Neural Networks) have become the dominant
methods for various computer vision tasks, such as image classification [1], object
detection [2], and segmentation [3]. The classification networks include AlexNet
[4], ResNet [5], MobileNets [6], and the object detection networks include Faster-
RCNN [7], SSD [8], YOLOv3 ∼v4 [9], [10]. The neural network models for those
tasks have evolved from 8 layers to more than 100 layers.
Though the large networks have strong feature representation ability, they con-
sume more resources. As an example, the depth of the YOLOv4 network reaches
162 layers, the size of the model is 256 MB, and the number of parameters is 64
million. When processing a picture of 416 ×416 size, it needs 29G FLOPs (Float-
ing Point Operations Per Second), and the intermediate variables will occupy
more memory. Taking into account the size of the model, the memory needed for
inferencing and the amount of computation are unbearable for resource-limited
embedded devices.
To address the deployment problem of neural networks in mobile or embedded
equipment, many model compression works are based on pruning, quantification,
knowledge distillation, and lightweight network design methods. In the pruning
work, the pruning methods based on weight level were proposed [11] and [12] to
reduce the number of parameters of the model without affecting the accuracy of
the network. However, the pruned models based on weight level requires special
hardware accelerators to be deployed, such as [13]. To save the cost of deploy-
ment, pruning methods based on filter level were proposed in [14], [15], and these
methods will not require special hardware support. In the quantification work, the
binary network and ternary network were proposed in [16] and [17], respectively.
In [18] and [19], they combine the pruning with quantization and apply it to the
classification network. In the work [19], the information of pre-trained parameters
is used to assign the compression ratio of each layer, and the method of the shared
codebook is used to quantify and they achieve a good compression effect. Though
the low-bit quantification network can reduce the size of the model, they bring
great accuracy loss and usually need a special software acceleration library to sup-
port deployment. The above pruning and quantization investigations are mainly
used in classification networks, and not much has been examined for applying
them to detection networks.
Pruning and quantization are to compress the existing network structure and
parameters. In contrast, knowledge distillation and lightweight network design op-
timize or directly design a new network structure, that is avoid the accuracy loss
caused by pruning or quantization. Knowledge distillation is an approach to im-
prove the performance of the student network by using the teacher network. Hinton
first applied the idea of distillation to the classification network [20].Afterwards,
knowledge distillation has been widely used in computer vision [21], [22], [23],
natural language processing [24], speech recognition [25]. Although knowledge dis-
tillation can improve the performance of the student network to a certain extent,
its effect on reducing the parameters and size of the model is far from pruning.

--- PAGE 3 ---
Group channel pruning and spatial attention distilling for object detection 3
Moreover, how to represent the knowledge to be distilled between the teacher net-
work and the student network is a problem. The structural difference between
the teacher and the student network has a great influence on the distillation ef-
fect. In [26] knowledge distillation is combined with representation learning to
reduce the influence of structural difference on distillation. The work [27] gives a
reference to use the representation learning to solve the problem of partial view
alignment. [28] present a weakly supervised in object detection, this method re-
quire only image labels and counts of the objects of each class in the image, by
this combination produce a clear localization of objects in the scene through a
masking technique between class activation maps and regression activation maps,
these work may solve the problem of knowledge representation in distillation to
some extent. Lightweight network design is a direct design of small networks or
modules, the article [29] presents a lightweight network application in object de-
tection tasks, including attention feature module to improve network accuracy,
constant channel module to save memory access costs. Two different encoder-
decoder lightweight architectures for semantic segmentation tasks were proposed
in [30] and [31], respectively. The former work up-samples the convolution features
of deep layer to the shallow deconvolution layers to enhance the contextual cues,
and the later work uses channel split and shuffle modules in the encoder to reduce
the number of parameters, and introduces an attention module in the decoder to
improve the accuracy. By directly designing lightweight modules, a good balance
between the accuracy and the size of the model can be achieved, these lightweight
networks can be well combined with tasks in autonomous driving, like [32]. Such
these well-designed modules or networks [33], [34], [35], [36], [37], can normally
run on laboratory host machines. However, if we want to deploy these networks
successfully on edge devices, a lot of experiments and modifications are needed
to verify their effectiveness. Before the network is deployed to the edge device,
the parameters and network need to be quantified and compiled. Usually, some
innovative modules or network layers cannot be compiled and passed (due to the
limitation of the instruction set and basic operators on the hardware device), which
hinders the deployment of these lightweight networks to edge devices and reduces
the versatility of these modules.
In the object detection task, model compression is mainly realized by knowl-
edge distillation, and there exists little work to combine pruning with knowledge
distillation. In short, the existing model compression works have the following lim-
itations: 1. Most pruning and quantization models need special hardware circuits
or software acceleration library support, which increases the cost to deploy these
models to the edge device. 2. In the object detection networks, the model compres-
sion methods are mainly realized by knowledge distillation, and the compression
effect is not satisfactory. Pruning is widely used in the classification network but
directly applied in object detection.
To tackle the above problems, we propose a three-stage model compression
method for object detection tasks: dynamic sparse training, grouping channel prun-
ing, and spatial attention distillation. As shown in Fig.1, we briefly describe the
implementation of the proposed three-stage model compression method.
Firstly, we sparsely train the network. Sparse training is to make the distribu-
tion of γcoefficient in the BN layer close to 0, and then the value of γcoefficient
is used as the importance scale factor of the channel to select out the insignificant
channels in the network. The traditional sparse training method uses a constant

--- PAGE 4 ---
4 Yun Chu et al.
Fig. 1: Flow-chart of three-stage model compression: dynamic sparse training,
group channel pruning, and spatial attention distilling.
sparse rate in the training process, which is time-consuming and difficult to make
a good balance between the sparsity and accuracy of the network. Therefore, we
introduce a variable sparse rate to accelerate the sparse training of the network
and achieve a good balance between the sparsity and accuracy of the network,
details are in Section 4.1
Next, we prune the network. Most of the traditional pruning methods are used
in classification networks, and all channels in the network are pruned with the
same threshold. In contrast, we divide the detection network into multiple groups.
In grouping, we mainly consider the scale of feature layers and the similarity of
module structure in the network, the feature layers with the same scale and the
layers have similar module structure are assigned to the same one group. After
that, each group obtains the pruning threshold according to the current group’s
pruning ratio, then we prune the channels according to the pruning threshold in
each group, thus achieving more accurate and efficient pruning of the detection
network, details are in Section 4.2.
At last, when pruning the channel of the detection network, we notice that with
the increase of the detection category and pruning ratio, pruning will bring greater
accuracy loss to the model. To recover the accuracy of the network after grouping
pruning, we introduce knowledge distillation to the pruned network. Particularly,
we extract spatial attention information from the feature maps of specific scales
in each group as knowledge and distill the pruned network, details are in Section
4.3.
To the best of our knowledge, the work to combine pruning with spatial atten-
tion distillation and apply it to object detection tasks is currently rarely explored.
The main contributions of this paper are summarized as follows:
1) To improve the efficiency of sparse training, we design a dynamic sparse train-
ing method which use variable sparsity rate to accelerate the process of sparse
training, the network achieves a better trade-off between sparsity and accuracy.
2) For the object detection network, we propose a novel pruning method, called
group channel pruning. We divide the detection network into multiple groups.
During the group, we comprehensively consider the scale of feature layers and
the similarity of module structure in the network. After that, each group ob-
tains the pruning threshold according to different pruning ratios, then we prune
the channels in each group to achieve more accurate and efficient pruning of
the detection network.
3) To recover the accuracy of the pruning network model, we introduce knowledge
distillation to the network after grouping pruning. In particular, we extract spa-
tial attention information only from the feature maps of specific scales in each
group as knowledge and distill the pruned network. Furthermore, we demon-
strate that our distillation method is not only suited for our pruning method
but also can combine with other common pruning methods.

--- PAGE 5 ---
Group channel pruning and spatial attention distilling for object detection 5
4) We conduct extensive experiments on the PASCAL VOC data set with the
YOLOV4 network to verify the effectiveness of the proposed method. To demon-
strate the generality of our method, we also use Darknet53 and Mobilenet as
the backbone to construct the detection network, the experimental results show
that our method has other applicability. In addition, we deploy the compres-
sion model on the edge device Jetson nano, which proves that our compression
model can be deployed without special hardware support and can achieve an
acceleration effect.
2 Related work
In this section, we briefly review the related works of pruning and knowledge
distillation.
2.1 Network Pruning
The idea of pruning is to reduce the redundancy of structure and parameters in
neural networks so that the network becomes more lightweight and efficient. The
research of pruning focuses on two aspects. One is what kind of objects in the
network can be pruned, and the other is how to measure the importance of the
content being pruned. From the perspective of the object being pruned, the current
pruning method can be divided into unstructured pruning and structured prun-
ing. Unstructured pruning refers to that the topological structure of the network
becomes irregular and unstructured after the network is pruned, and they often
prune the connection weights between neurons. For example, in [11] and [12], the
absolute value of the weight is taken as the metric of its pruning. The advantage
of unstructured pruning is that the pruning rate can reach a high level without af-
fecting the precision of the network. The disadvantage is that it needs the support
of special hardware circuits, which increases the cost of deployment.
Structured pruning means that the network topology has not changed after
pruning. Usually pruning at the filter [38], channel [39], layer [40] levels. The
work in [38] prunes the unimportant filter in the current layer by calculating the
statistical information of the subsequent layer. Liu et al [39] propose a structured
channel pruning method for classification networks, and the compressed model
does not require special hardware and software support. The work in [40] uses
the subspace projection method to measure the importance of the network layer,
prunes the layer in the network, and verifies that the layer pruning is better than
the filter pruning in resource utilization. The advantage of the structured pruning
method is that the network after pruning does not need the support of special
hardware circuits, and the disadvantage is that the pruning rate cannot reach
very high.
Rethinking the Value of Network Pruning [41] discussed the significance of
network pruning. Their work pointed out that the role played by pruning was
similar to that of network architecture search (NAS). After that, [42] bring the
idea of NAS into pruning, proposes a batch normalization module to measure the
importance of each structure in the network, and uses this NAS pruning method
to the classification network. In this paper, we inherit the idea of sparse training

--- PAGE 6 ---
6 Yun Chu et al.
in [39], different from that, we introduce variable sparse rate to conduct dynamic
sparse training on the network, which improves the trade-off between sparsity and
accuracy of the network.
2.2 Knowledge distillation
The purpose of knowledge distillation is to transfer the knowledge learned from
the teacher network to the student network to improve the performance of the
student network. The research of knowledge distillation focuses on two aspects.
One is which object in the network is selected as knowledge. The other is how to
measure whether the student network learns the knowledge, which is reflected in
how to design the loss function of distillation. Concerning what to be selected as
knowledge, current distillation methods can be divided into three categories: 1.
Using the final class information output of the teacher network as knowledge as in
[43], [44], [45]. 2. Using the middle feature layer of teachers’ network as knowledge
as in [46], [47]. 3. Using the structural relationship between the layers of teacher
network as knowledge as in [24]. In the classification network, [47] proposed to
extract the attention from the feature layer and express the attention information
in a heat map. Then, the loss function is constructed using the attention of the
teacher network and the student network.
Search to Distill: [48] introduced the knowledge distillation method into NAS,
and obtained the following conclusions through experiments, the structure of stu-
dent network determines the upper bound that the distillation effect can reach,
and the distillation effect is better when the network structure of students and
teachers is similar. Inspired by the above research work, we combine pruning with
knowledge distillation. In this paper, the idea of our distillation is inspired by [47],
and we improved the method to make it suitable for object detection. Especially,
we extract spatial attention from the feature layers of each group and give each
group spatial attention with different weights for distilling.
3 Network Architecture
This paper takes YOLOv4 as an example to illustrate our pruning and distillation
methods. Our pruning method can be applied to networks with BN modules. In
this section, we briefly introduce the five core basic components of the network
and the overall architecture of the network.
3.1 Basic Components
As shown in Fig.2, the five modules are CBM, Res Unit, CSP X, CBL, and SPP.

--- PAGE 7 ---
Group channel pruning and spatial attention distilling for object detection 7
Fig. 2: The five basic modules of YOLOv4 Network.
Among them, the CBM module is composed of Conv, Batch Normalization
[49] and Mish activation [50] function. Res Unit is composed of CBM and add
operations, where the res block is derived from [5]. CSP X module is composed of
CBM, X Res units, and concatenate operations. CBL module is composed of Conv,
BN, and Leaky-Relu [51]. Spatial pyramid pooling (SPP) is proposed in [52], herein
SPP refers to feature fusion by pooling at four scales: 1 ×1, 5×5, 9×9, 13×13.
3.2 Detection Network
Through the above five basic components constitute the three parts of YOLOv4,
i.e., backbone network, feature enhancement network, and detection head. These
three parts have a total of 162 layers. Firstly, the backbone network is used to
extract the feature of the object. Then, the feature enhancement network further
fuses and enhances the features. At last, the detection head is responsible for
classifying the input features and returning the location and size of the target.
As shown in Fig.3m when the input size is 416 ×416, the down-sampling feature
maps of 8, 16, 32 times are obtained through the backbone network, specifically,
feature maps at scales 52 ×52, 26×26, 13×13, and then these feature maps are
fed into the feature enhancement network. Finally, the detection head outputs the
prediction at three scales.

--- PAGE 8 ---
8 Yun Chu et al.
Fig. 3: Dividing YOLOv4 into five groups. In the graph, the blue dotted box
contains three main parts of the network, the red dotted boxes represent the five
groups. When grouping, we comprehensively consider the scale of feature layers
and the similarity of module structure in the network. For example, when the
input image size is 416 ×416, group1: including the scales of feature layer from
416×416 to 52 ×52 and CSP module, group2: only include the feature layer at
26×26 scales along with the CSP module, group3:only include the feature layer
at 13×13 scales along with the CSP and CBL module, same for the group4 and
group5. Notice that, CSP module includes the CBM module as shown in Fig.2,
the SPP, Concat, Upsample, and Downsample these modules are just doing the
calculation, they do not contain the trainable parameters.
4 Proposed Approach
In this section, we describe the proposed three-stage model compression method in
detail. Firstly, we train the network sparsely with the dynamic sparse rate. Then,
the object detection network is divided into five groups, and each group uses
different thresholds for channel pruning. After that, using the pruned network as
the student network for knowledge distillation, the details are described as follows.
4.1 Dynamic Sparse Training
The purpose of sparse training is to select out the insignificant channels in the
network layer. Referring to the [39] , we use γas the important factor of the
channel. The distribution of γcoefficients of all BN layers in the original network

--- PAGE 9 ---
Group channel pruning and spatial attention distilling for object detection 9
is in different ranges. Sparse training is to sparse the γcoefficient, making the
distribution of the γcoefficient close to zero. The smaller γvalue indicates the
lower importance of the corresponding channel. As shown in (1), γis the scale
parameter of the BN layer, βis the offset parameter of the BN layer, the value of
γandβare obtained by training the network.
yi=γˆxi+β, ˆxi=xi−µBp
σ2
B+ε, (1)
where ˆ xidenotes the normalized output of a channel, and yidenotes the output
of ˆxiafter γscaling and βtranslation. xidenotes a specified channel on the feature
layer. As shown in (2), µBis the mean value of the specified channel under batch-
size number, σ2
Bis the variance of the specified channel under batch-size number.
To prevent denominator being 0, εcan be set a value at 1e-16.
µB=1
mmX
i=1xi, σ2
B=1
mmX
i=1(xi−µB)2, (2)
In the process of dynamic sparse training, the L1 norm of γis used as the
regularization term, and the variable sparse rate s dis introduced, which is added
to the loss function for training, as shown in (3).
L =X
(x,y)l(f(x, W ), y) + s dX
γ∈Γg(γ), (3)
where ( x, y) represents the input of the network and the label of the data,
Wrepresents the parameters that can be trained, and the first summation term
represents the original loss during CNN network training. The second summation,
g(γ) represents the regularization term introduced, we use g( γ) =|γ|. sdis a
variable sparse rate. In the training process, the network will dynamically adjust its
sparse rate according to the number of epochs of current training. When training
to half the number of epochs, 70 % of the channel maintains the original sparse
rate, 30 % of the channel sparse rate decay to 1 % of the original sparse rate,
so that the final training network achieves a good balance between sparsity and
accuracy.
4.2 Group channel pruning
In this section, we focus on the proposed group channel pruning method. It can
be divided into three steps. Firstly, we divide the structure of the object detection
network into five groups. Secondly, we obtain five different pruning thresholds
according to the pruning proportion of each group and then generate the pruning
mask matrix for most of the convolution layer in each group, which is used to prune
the channels in the convolution layer. At last, we generate the public pruning mask
matrix for the convolution layers associated with the shortcut layers.

--- PAGE 10 ---
10 Yun Chu et al.
4.2.1 Network group
Our grouping channel pruning is to divide the detection network layers into multi
groups. In grouping, we comprehensively consider the scale of feature layers and
the similarity of module structure in the network which means that the feature
layers with the same scale and the layers that have similar module structure are
assigned to the same group. In the experiment, we observe that when the unified
pruning threshold is used for all structures, two detrimental situations will occur.
One is that in structures with high redundancy, the real pruning threshold is higher
than the unified pruning threshold, and the redundant channels in such structures
will not be pruned. In the other case, in the structure with low redundancy, the real
pruning threshold is lower than the unified pruning threshold and the significant
channels may be pruned in this structure, which seriously affects the accuracy of
the network.
To solve this problem, we group the object detection network first. As shown in
Fig.3, the blue dotted boxes represent the YOLOv4 network’s backbone part, the
feature enhancement part, and the detection part. The red dotted boxes represent
the five groups. When input the size of 416 ×416 images into the network, the
group1 include the scales of the feature layer from 416 ×416 to 52 ×52 and CSP
module, group2 only include the feature layer at 26 ×26 scales along with the CSP
module, group3 only include the feature layer at 13 ×13 scales along with the CSP
and CBL module, group4 and group5 both include the feature layer from 52 ×52
scales to 13 ×13 scales and CBL module,however, for more precise pruning, we
divide this two parts into two groups. Besides, the CSP module includes the CBM
module as shown in Fig.2. The SPP, Concat, Upsample, and Downsample these
modules are just operators, they do not contain the trainable parameters.
The first three groups, Group1 ∼3 belong to the backbone network and are
responsible for extracting the features of the object, these three groups contain
all the residual modules in the network. Group4 belongs to the feature enhance-
ment network, which is responsible for further enhancement and fusion of features.
Group5 includes the detection head part which realizes the classification of features
and the regression of object location.
4.2.2 Pruning thread and mask matrix
Due to the different redundancy in five groups, we need to obtain five pruning
thresholds and a pruning mask matrix from five groups. Here we illustrate the
steps using one group. Firstly, we calculate the ratio of the number of channels
in the current group to the total number of channels that can be pruned in the
whole network, we denote this ratio as pi. Then, given a total pruning ratio of
the whole network, we denote it as P. By multiplying pibyP, we can obtain the
pruning proportion of the current group, denote as gi. After that, we sort all the γ
coefficients in that group, according to the current group’s pruning proportion( gi)
we can get the current group’s pruning threshold(denote as ti). Finally, we use the
pruning threshold tito compare with all the γcoefficients in this group to obtain
the pruning mask matrix of the convolution layers, in that pruning mask matrix,
the number 1 represents the channel in the corresponding position retained, and
the number 0 represents the channel in the corresponding position will be pruned.

--- PAGE 11 ---
Group channel pruning and spatial attention distilling for object detection 11
As shown in Fig.4, we use the γcoefficient as the scale factor of the channel and
compare the scale factor with the pruning threshold of the current group. When
the value of the scale factor is lower than the pruning threshold, the channel
corresponding to the scale factor will be pruned. Using the above method, we
obtain the five pruning thresholds and the mask matrix of the convolution layer
in each group.
Fig. 4: The red dotted box represents that the feature layer is assigned to group
I, the green dotted box represents that the convolution layer next to this feature
layer. They are both in group I, and in this group, all the channels will share the
same pruning threshold. We use the γas the scale factor of the channel. When
the channel scaling factors are lower than the current group’s pruning threshold,
the corresponding channels will be pruned.
4.2.3 Public mask matrix
The pruning mask matrix obtained by the above method can be used as the final
pruning matrix of the most convolution layers in the network. However, another
convolution layer associated with the shortcut layer needs to use the public mask
pruning matrix. Because the number of channels to be added between the two
layers must be consistent in the shortcut layer to perform the addition operation.
Considering that the source layer of the shortcut may still be a shortcut layer,
which will involve multiple convolution layers forward, and the pruning mask ma-
trix of these convolution layers needs to be consistent. How to generate this public
mask pruning matrix so that the channel pruning in each convolution layer reaches
a higher amount and has little effect on precision. This is a question worth con-
sidering.
To address such a question, we propose a voting method to generate the public
pruning mask matrix. As shown in Fig.5. Firstly, we count the total number of
convolution layers associated with the shortcut layer and denoted as N conv. Then
we count the total number of zeros at position ( i, j) in the pruning mask matrix and
denoted as Z(i,j). The value of the public mask matrix at ( i, j) position denoted
asp(i,j). When the Z(i,j)≥(Nconv/2),then p(i,j)= 0 ; otherwise, the p(i,j)= 1.

--- PAGE 12 ---
12 Yun Chu et al.
Fig. 5: Generating the public pruning mask matrix. In the figure, N conv denotes
the total number of convolution layers associated with the shortcut layer. Z(i,j)
denotes the total number of zeros at position ( i, j) in the pruning mask matrix.
p(i,j)denotes the value of the public mask matrix at ( i, j) position. If
Z(i,j)≥(Nconv/2),then p (i,j)= 0; else, the p (i,j)= 1.
4.3 Knowledge distillation loss
In this section, we introduce the three parts of distillation loss. As shown in Fig.6,
we use the original network as the teacher network, and the pruned network as the
student network for knowledge distillation, the distillation loss includes three parts:
1. The difference in spatial attention between student network and teacher network
denoted as LAT; 2. The predicted value differences between student networks and
teacher networks in object classification and location regression are denoted as
Lsoft; 3. The loss between the predicted value of the student network and the
ground truth is denoted as Lhard.
As in (4), we use Ltotal to represent the total loss of the student network, and
we will mainly consider the loss of LATandLsoft.
Ltotal=LAT+Lsoft+Lhard, (4)

--- PAGE 13 ---
Group channel pruning and spatial attention distilling for object detection 13
Fig. 6: Distilling for the group pruned network. Qi
TandQi
Sare the spatial
attention information extract from the feature maps of five specific scales in each
group of the teacher and student network, respectively. Three red boxes
demonstrated the three-loss parts of the student network. 1. LATdenotes the
difference in spatial attention between student network and teacher network;
2.Lsoftdenotes the predicted value differences between student network and
teacher network in object classification and location regression; 3. Lharddenotes
the loss between the predicted value of the student network and the ground
truth.
4.3.1 Group spatial attention loss
As in (5), LATdenotes the difference in spatial attention information between
student network and teacher network. We reduce this difference by allowing the
student network to imitate the spatial attention of the teacher network.
LAT=5X
i=1βiQi
TQi
Ti
2−Qi
SQi
S
2
2, (5)
where ibelongs to 1 ∼5, representing the five groups in the network. From
these five groups, we extract the spatial attention only at specific scale feature
maps as the knowledge, the scale feature maps are 208 ×208, 104 ×104, 52
×52, 26 ×26 and 13 ×13, respectively, We extract these feature maps from
five groups. βidenotes the five group’s loss gain coefficient, we give the different
group’s spatial attention with different weight. Qi
TandQi
Sare the 1-dimensional
tensor forms of teacher and student network spatial attention, and each element
inQiis normalized.

--- PAGE 14 ---
14 Yun Chu et al.
As in (6), one-dimensional Qiis converted from the two-dimensional F 
Ai
by
the flattening operation. F 
Ai
T
andF 
Ai
S
are two-dimensional matrix forms of
spatial attention in the network of teacher and students, respectively.
Qi
T= vec
F
Ai
T
, Qi
S= vec
F
Ai
S
, (6)
The mapping function F(.) is given in (7), where Adenotes the feature map
on the channel, Ahas H ×W size, and Crepresents the number of all channels
on the feature layer. The value of pis 2, which represents a power of 2 for each
element in A.
F(A) =Fp
sum(A) =cX
j=1|Aj|p, (7)
Spatial attention refers to extracting the spatial information of all channels on
a certain feature layer in the form of a heat map. The extraction process is shown
in Fig.7. One feature layer is selected from the network, and the size of the feature
layer is C ×H×W. C represents the number of channels on the feature layer.
Through the mapping function F: F: AC×H×W→AH×W, the 3-dimensional
feature layer tensor is mapped to a 2-dimensional tensor on the channel dimension,
which represents the spatial attention map of the feature layer.
Fig. 7: Generating the spatial attention map from the feature layer.
4.3.2 Soft target loss
As in (8), Lsoftis composed of two kinds of prediction differences. One is the
prediction difference of teacher and student networks in object classification. The
other is the prediction difference of teacher and student networks in the location
and size of the object box.
Lsoft=l(t−s)(cls) +l(t−s)(box), (8)

--- PAGE 15 ---
Group channel pruning and spatial attention distilling for object detection 15
l(t−s)(cls) denotes the prediction difference in object classification between
teacher networks and student networks, as shown in (9).
l(t−s)(cls) =1
k3X
i=1kX
j=1M(i,j)
logM(i,j)−N(i,j)
, (9)
where idenotes the prediction of the network at three scales, kdenotes the
number of all prior boxes at the current scale. M(i,j)denotes the predicted output
of the teacher network after distillation. N(i,j)denotes the predicted output of the
student network after distillation.
As in (10) (11), M(i,j),N(i,j)is obtained by softmax andlogsoftmax func-
tion, where P(i,j)
t(cls) denotes the classification probability predicted for each prior
box in the teacher network and P(i,j)
s(cls) denotes the classification probability
predicted for each prior box in the student network. Tis a temperature parameter
used to make the output distribution of teacher and student network prediction
more uniform.
M(i,j)= soft max
P(i,j)
t(cls)/T
, (10)
N(i,j)= log−soft max
P(i,j)
s(cls)/T
, (11)
As in (12), l(t−s)(box) denotes the prediction difference between teacher net-
work and student network on the location and size of the object box.
l(t−s)(box) =3X
i=1kX
j=1P(i,j)
t(box)−P(i,j)
s(box)
2, (12)
where idenotes the prediction of the network at three scales, and kdenotes the
number of remaining candidate boxes after meeting the IOU thread at the current
scale. In the position corresponding to the object candidate box, in the student
network the position and size of the predicted box was denoted by the P(i,j)
s(box).
In the teacher network, the position and size of the predicted box was denoted by
theP(i,j)
t(box).
5 Experiments
In the experiment, we take the YOLOv4 detection network and PASCAL VOC
data sets as an example to illustrate and validate the effectiveness of the proposed
model compression. Firstly, we sparsely train the network with a dynamic sparse
rate. Secondly, we quantitatively analyze the effect of different pruning proportions
on the model size, accuracy, and calculation. Then, we compare our group pruning
with other current pruning methods on object detection data sets. At last, to prove
the superiority of the distillation method, we compare the accuracy of the pruned
network after fine-tuning and distilling. Furthermore, we combine our distillation
method with other common pruning methods to demonstrate that our distillation
method was suitable for other common pruning methods.

--- PAGE 16 ---
16 Yun Chu et al.
5.1 Dataset and evaluation metrics
For the dataset, we use Pascal VOC [53]. The voc2012train, voc2012val, voc2007train,
and voc2007val, these four parts have 16551 pictures and we combine them used
as the final training set. The voc2007test included 4952 pictures and was used as
the final test set. Our experimental environment is Ubuntu 18.04, PyTorch = 1.8
version, GPU is a single RTX3090. For the evaluation metrics, we evaluate the
performance of the pruned model from four aspects: model size, the number of
parameters, calculation amount, and mAP@0.5. The computation is measured by
FLOPS. The mAP@0.5 represents the average value of all categories of AP when
the IOU threshold is 0.5, AP represents the average precision of one category, and
the specific calculation details are referred to in reference [54].
5.2 Dynamic Sparse training
In the process of sparse training, the L1 norm of γis added into the loss function
as a regularization term to train together. As shown in Fig.8, the distribution of
γcoefficients of all BN layers in the original network is in different ranges. The
sparse training process is to make the γcoefficient distribution close to zero, so
that is convenient to select out the unimportant channels in the network.
Fig. 8: The distribution of γcoefficients in the original network.
During the experiment, we found that sparse training is the trade-off between
accuracy and sparsity. A larger sparse rate scan bring a better sparse effect, but
the accuracy loss is also large, even if the number of epochs of sparse training is
increased in the future, the model still can not restore to good accuracy. A smaller
sparse rate shas little effect on the accuracy but leads to a worse sparse effect. To
solve this problem and make a good balance between sparse effect and accuracy,
we put forward a dynamic sparse training method, which introduces a variable
sparse rate s, and the sparse rate swill change with the training process of the
network.
During the dynamic sparse training, the degree of network sparsity can be
adjusted by setting the sparsity rate s, the network will dynamically adjust its

--- PAGE 17 ---
Group channel pruning and spatial attention distilling for object detection 17
sparse rate according to the number of epochs of current training. For the YOLOv4
network, we set the initial sparse rate s= 0.00075, the initial learning rate lr0 =
0.002 and train 200 epochs. When training to half the number of epochs, 70 %
of the channel maintains the original sparse rate, 30 % of the channel sparse rate
decay to 1 % of the original sparse rate. Besides, the learning rate is updated
by cosine annealing. When the input image size is 416 ×416, the batch size is set
to 16, as shown in Fig.9, this figure represents the γcoefficient distribution in
the network layers after dynamic sparse training, compared to the original the γ
coefficient distribution (as shown in Fig.8), most of these γcoefficient is more close
to the zero and it is convenient to select out insignificant channels.
Fig. 9: The distribution of the γcoefficient in the network after dynamic sparse
training.
5.3 Group channel pruning
In this section, we divide the network layers into five groups. YOLOv4 has 162
layers, during the group, we comprehensively consider the scale of feature layers
and the similarity of module structure in the network which means that the feature
layers with the same scale and the layers that have similar module structure are
assigned to the same one group.
When input the size of 416 ×416 images into the network, the group1 include
the scales of the feature layer from 416 ×416 to 52 ×52 and the CSP module,
group2 only include the feature layer at 26 ×26 scales along with the CSP module,
group3 only include the feature layer at 13 ×13 scales along with the CSP and CBL
module, group4 and group5 both include the feature layer from 52 ×52 scales to
13×13 scales and CBL module. The specific layers in each group are the following,
including Group1: 0 ∼55 layers, Group2:56 ∼85 layers, Group3: 86 ∼116 layers,
Group4:117 ∼136 layers, and Group5:136 ∼161 layers.

--- PAGE 18 ---
18 Yun Chu et al.
Table 1: The Total represents the total pruning proportion of the whole
detection network, the Model represents the compressed model under the
corresponding total pruning proportion and the Group1 ∼5 shows the specific
pruning proportion in 5 groups.
Total Model Group1 Group2 Group3 Group4 Group5 Model Size mAP@0.5
0 Base 0 0 0 0 0 256M 79.8
38% Model1 10% 20% 96% 87% 45% 98MB 73.6
40% Model2 10% 25% 96% 87% 50% 90MB 56.3
42% Model3 10% 25% 97% 85% 55% 84MB 28.3
45% Model4 15% 25% 96% 85% 70% 77MB 9.2
50% Model5 15% 27% 96% 85% 90% 68MB 6.1
5.3.1 Reducing model’s parameters and computations
Given a total pruning proportion of the whole network, according to the total prun-
ing proportion the algorithm will calculate the five groups’ pruning proportion. As
shown in Table1, we demonstrate the details of five groups’ pruning proportion. It
can be seen from Table1 that in the backbone network part, the function of feature
extraction is mainly realized by Group1 and Group2, and the redundancy in these
two groups reaches 10% ∼25 %. The redundancy in Group3 reached more than
90%, this indicating that the channels in these feature layers have little effect on
feature extraction in Group3. The redundancy in Group4 reaches more than 90%,
and most channels in this Group play a little effect on feature enhancement. The
redundancy in Group5 reaches about 45%.
Through the above analysis, which is sufficient to indicate that the redundancy
in various structural parts of the network is different. We use group pruning to
make each group has different pruning thresholds, thus achieving more accurate
and efficient pruning. In Table2, we present the effects of different pruning propor-
tions on the model’s parameters and computation, except the pruning proportion
is different, we keep the size of input images is 416 ×416. Table2 demonstrates
the effectiveness of our group channel pruning.
Table 2: Comparing to the original network, the reduction in model parameters
and computations by using our group pruning method with different pruning
proportions. Table2 ∼Table 5, for all models, the size of the input image is 416
×416.
Model Model Size Flops Pruned Parameters Pruned
Base 256MB 29.9G 0 64.1M 0
Model1 98MB 20.35G 31.9% 24.51M 61.7%
Model2 90MB 19.46G 34.9% 22.56M 64.7%
Model3 84MB 19.35G 35.2% 20.96M 67.2%
Model4 77MB 17.94G 40.0% 19.27M 69.9%
Model5 68MB 16.57G 44.6% 17.07M 73.3%

--- PAGE 19 ---
Group channel pruning and spatial attention distilling for object detection 19
5.3.2 Comparing with other current pruning methods
As shown in Fig.10, to demonstrate the superiority of our proposed group channel
pruning method in object detection, we quantitatively compare our method with
other current pruning methods like Network Slimming [39], Thinet [38], Layer
pruning [40] and Eagle eye [42]. We compare them at two aspects: model size and
accuracy (map@0.5).
Fig. 10: The figure shows the accuracy of the object detection network changes
with the pruning ratio, and the dot radius represent the size of the model
changes with the pruning ratio, the input resolution is 416 ×416.
During the pruning process, we keep the pruning proportion as the same for
different pruning methods, the size of input images is 416 ×416. It can be seen
from Fig.10, under the same pruning proportion, our method can obtain the best
trade-off between the pruned model’s accuracy and pruned model’s size.
In addition, in [40] they use a subspace projection approach to estimate the
importance of the network layers, when using this pruning way, the layers can be
pruned is limited since that if the pruning proportion is above 0.6 this pruning way
will change the network architecture and accuracy significantly, this is inconvenient
to recover the pruned models’ accuracy.In [42], they use a way similar to the
network architecture search, during the pruning process, they search the pruned
network not only consider the pruned model’s size, accuracy, but also consider
the pruned model’s computations and selected the best trade-off model from 1000
candidate models. To ensure the work [42] pruning method is carried out under the
same hardware conditions and the computational environment with other pruning
methods (Our experimental environment is Ubuntu 18.04, PyTorch = 1.8 version,

--- PAGE 20 ---
20 Yun Chu et al.
GPU is a single RTX3090), we choose the best model only from the five candidate
pruning models, we also need to declare that this pruning method of architecture
search has better performance when the number of candidate models is more, but
this situation also puts forward higher requirements for computational power.
5.4 Group spatial attention distilling
In this section, we use the sparse network to conduct the group channel pruning
and obtain the pruned network. The accuracy of the original network and the
network after sparse training are 87.1 and 79.8, respectively.
During the distillation experiments, we set the original network as the teacher
network and the pruned network as the student network, the spatial attention
information was extracted only at specific scale feature maps from five groups as
the knowledge, the scale of feature maps are 208 ×208, 104 ×104, 52 ×52, 26×
26 and 13 ×13, respectively. Besides, we give the five group’s loss gain coefficient
βiwith different weight, we set the β1-β3as 1000 and the β4-β5as 10000.
5.4.1 Group spatial attention distilling with our pruning method
To verify the effectiveness of group spatial attention distilling, we use it for the
pruned model with our pruning method. Then, we fine-tune and group attention
distills the compressed model, respectively.
The comparison results were shown in Table 3, the mAP@0.5 as the accuracy
evaluation metric. It can be seen from Table 3 when directly fine-tune the pruned
network, the highest accuracy restored can only reach 81.1. In contrast, using group
spatial attention distilling can make the pruned network obtain higher accuracy.
Table 3: Combing the group spatial attention distilling with our pruning method,
the pruned Model1-5 achieved by ours’ pruning method, the accuracy represent
the Model after through the fine-tuning and group spatial attention (denote as
GSA) distillation.
Total Model Model SizemAP@0.5
Fine tuning GSA Distilling
38% Model1 98MB 80.4 86.5
40% Model2 90MB 81.1 86.6
42% Model3 84MB 80.5 84.8
45% Model4 77MB 79.5 83.3
50% Model5 68MB 75.9 76.3
5.4.2 Group spatial attention distilling combine with other common pruning
methods
To show that our group spatial attention distillation scheme is not only suited for
our pruning method, we combine the group spatial attention distillation method
with other common pruning methods like Network Slimming [39] and Thinet [38].

--- PAGE 21 ---
Group channel pruning and spatial attention distilling for object detection 21
The comparison results were shown in Table 4 ∼5, it can be seen from Table 4
∼5 , when we directly fine-tune the pruned network, the highest accuracy restored
can only reach 80.9. In contrast, using group spatial attention distilling can make
the pruned network obtain higher accuracy. Besides, combing the group channel
pruning with our pruning method can achieve a better effect.
Table 4: Combing the group spatial attention (denote as GSA) distilling with
Network Sliming [39] pruning method, the pruned Model1-5 achieved by [39] .
Total Model Model SizemAP@0.5
Fine tuning GSA Distilling
38% Model1 100MB 80.4 83.1
40% Model2 94MB 79.8 83.7
42% Model3 86MB 80.4 84.8
45% Model4 78MB 79.6 82.8
50% Model5 65MB 79.3 80.9
Table 5: Combing the group spatial attention distilling (denote as GSA) with
Thinet [38] pruning method, the pruned Model1-5 achieved by [38].
Total Model Model SizemAP@0.5
Fine tuning GSA Distilling
38% Model1 116MB 80.9 84.2
40% Model2 110MB 80.4 83.9
42% Model3 104MB 80.9 81.8
45% Model4 96MB 80.7 82.1
50% Model5 83MB 79.4 82.1
5.4.3 Comparing the compressed model with other object detection network on
PASCAL and COCO
To verify the effectiveness of our final compressed model, we compare the final
compressed model with other normal network and lightweight detectors on PAS-
CAL and COCO, respectively.
The comparison results were shown in Table 6 and Table7, the mAP@0.5
and mAP@0.5:0.95 as the accuracy evaluation metric for the PASCAL VOC and
COCO dataset, respectively. The symbol * represent the final compressed model
which has been used our group channel pruning, spatial attention distillation. It
can be seen from above table, our method can achieve a best trade off between
the network’s accuracy and calculation or parameters.
5.5 Deployment on the edge device
In this section, we introduce the deployment of the pruned model on edge device-
Jetson Nano. Jetson Nano is a small, powerful computer for embedded applica-

--- PAGE 22 ---
22 Yun Chu et al.
Table 6: Comparing with normal and lightweight detectors on PASCAL VOC.
Method Backbone Input size Flops Parameters mAP
SSD Lite [33] VGG16 512 ×512 99.5B 36.1M 80.7
SSD Lite [33] MobilenetV2 320 ×320 0.8B 4.3M 71.8
Tiny-DSOD [34] G/32-48-64-801 300 ×300 1.06B 0.95M 72.1
ThunderNet [35] SNet146 416 ×416 0.461B — 73.8
Pelee [36] PeleeNet 304 ×304 1.21B 5.43M 70.9
Ours CSPDarknet* 320 ×320 18.2B 20.9M 82.7
Ours CSPDarknet* 416 ×416 19.3B 20.9M 84.8
Table 7: Comparing with normal and lightweight detectors on on COCO.
Method Backbone Input size Flops Parameters mAP(0.5:0.95) mAP(0.5)
SSD [8] VGG16 300 ×300 70.4B 34.3M 25.7 43.9
YOLOV3 [9] Darknet53 416 ×416 65.9B 62.3M 31 55.3
PANet [37] CSPResNeXt50 416 ×416 47.1B 56.9M 36.6 58.1
SSD lite [33] Mobilenet 320 ×320 0.8B 4.3M 22.1 –
ThunderNet [35] SNet146 320 ×320 0.95B — 23.6 40.2
Pelee [36] PeleeNet 304 ×304 2.58B 5.98M 22.4 38.3
Ours CSPDarknet* 320 ×320 18.6B 23.63M 30.2 48.8
Ours CSPDarknet* 416 ×416 19.43B 23.63M 33.4 53.5
tions, it has 128 NVIDIA CUDA cores and 4 GB memories. We deployed the
original network and five compressed models (using our group channel pruning
method) on this edge device, and test the inference time of each model.
The specific deployment steps are as follows: Firstly, on the host machine,
we prepare the network model file and the corresponding weight file, then using
PyTorch to convert it to the ONNX format model. After that, on the target device-
Jetson Nano, we use TensorRT to generate the engine files according to the ONNX
model. At last, we run the engine files of five compression models on Jetson Nano,
and the inference results are shown in Table 6.
It can be seen from Table 8 that the inference time of the original network
model needs 414ms and the compressed model(68M) only needs 274ms. The above
experiments show that the proposed group channel pruning method can be de-
ployed to the edge device without designing special hardware or software and has
an acceleration effect.
Table 8: The inference time of the pruned model on edge device-Jeston Nano, the
input resolution is 416 ×416.
Model Model Size Inference time
Base 256MB 414 ms
Model1 98MB 311 ms
Model2 90MB 305 ms
Model3 84MB 303 ms
Model4 77MB 291 ms
Model5 68MB 274 ms

--- PAGE 23 ---
Group channel pruning and spatial attention distilling for object detection 23
6 Ablation Studies
To demonstrate the generality and effectiveness of our method. In this section, we
first use MobileNet, DarkNet53, and CSPDarknet as the backbone to construct
the detection network. Next, we introduce the ablation experiments of dynamic
sparse training, group channel pruning, and spatial attention distilling. At last,
we test the pruning model after distillation.
6.1 Ablation Studies for Dynamic Sparse Training
During the sparse training, we set the total number of epochs for the sparse training
and dynamic sparse training both are 200 epochs, all model’s initial learning rate
set as lr0 = 0 .002 and the size of input images is 416 ×416.
As shown in Fig.11, the top three pictures represent the common sparse train-
ing and the bottom three pictures represent dynamic sparse training for the
CSPDarknet-Yolov4. In the dynamic sparse training process, the batch size =16,
the initial sparse rate s= 0.00075, when it comes to 40 % of the total number
epochs, 70 % of the channel maintains the initial sparse rate s, 30 % of the channel
sparse rate decay to 1 % of the initial sparse rate s. It can be seen from Fig.11,
the accuracy of the network through the common sparse training is 71.2, while the
accuracy of the network through the dynamic sparse training is 79.8. Besides, the
initial sparse rate sfor DarkNet53-Yolov3 and MobileNet-Yolov3 are 0.003 and
0.005, respectively. The batch size for these two networks are both 32.
Table 9: Ablation Studies for sparse training and dynamic sparse training.
Model Sparse training Dynamic Sparse training mAP@0.5 Model-size
CSPDarkNet-Yolov4 ✓ 71.2 256MB
CSPDarkNet-Yolov4 ✓ 79.8 256MB
DarkNet53-Yolov3 ✓ 59.2 246MB
DarkNet53-Yolov3 ✓ 66.5 246MB
MobileNet-Yolov3 ✓ 71.3 95MB
MobileNet-Yolov3 ✓ 72.8 95MB
For the MobileNet-Yolov3, DarkNet53-Yolov3, CSPDarknet-Yolov4, they have
96, 106, 162 network layers, respectively. It can be seen from the Table 9, the
accuracy of the MobileNet-Yolov3, DarkNet53-Yolov3, CSPDarknet-Yolov4 are
72.8, 66.5, 79.8, respectively. With the increase of the number detection network
layers, the dynamic sparse training method has achieved a better trade-off between
sparsity and accuracy compare to the common sparse training. Besides, when
network layers are less than 100 layers, we notice that increasing the batch size
also can improve the trade-off between sparsity and accuracy.
6.2 Ablation Studies for Group Channel Pruning
In the group channel pruning experiments, we chose the dynamic sparse training
model as the network to be pruned and use the same pruning proportion for the

--- PAGE 24 ---
24 Yun Chu et al.
(a) Image 1
 (b) Image 2
(c) image 3
 (d) image 4
Fig. 11: The top two figures show the process of the common sparse
training(stable sparse rate s) and the bottom two figures show the process of the
dynamic sparse training(variable sparse rate s). (a), (c) represents the accuracy
of the network changes with the training epochs and (b), (d) represents the
distribution of γcoefficient changes with the training epochs.
common pruning and group channel pruning. For the CSPDarknet-Yolov4, the
pruning proportion was 40 %. For the DarkNet53-Yolov3 and MobileNet-Yolov3,
the pruning proportion was 64 % and 65 %, respectively.
Table 10: Ablation Studies for common pruning [39] and group channel pruning
Model Pruning Group channel pruning mAP@0.5 Model-size
CSPDarkNet-Yolov4 ✓ 48.5 94 MB
CSPDarkNet-Yolov4 ✓ 56.2 90 MB
DarkNet53-Yolov3 ✓ 60.9 62 MB
DarkNet53-Yolov3 ✓ 65.2 21 MB
MobileNet-Yolov3 ✓ 71.3 17 MB
MobileNet-Yolov3 ✓ 72.2 15 MB
It can be seen from the Table 10, comparing with the common pruning method
[39], our group channel pruning method can achieve a better balance between
model’s size and accuracy for different detection networks.

--- PAGE 25 ---
Group channel pruning and spatial attention distilling for object detection 25
6.3 Ablation Studies for Group Spatial Attention Distillation
During the group spatial experiments, we choose the original network as the
teacher network, the pruned network (through the group channel pruning method)
as the student network.
For the CSPDarknet-Yolov4 and Mobilenet-Yolov3, we extracted the spatial
attention information at specific scale feature maps from five groups as the knowl-
edge, the scales of feature maps are 208 ×208, 104 ×104, 52 ×52, 26×26 and 13
×13, respectively. For the Darknet53-Yolov3, we extracted the spatial attention
information at 104 ×104, 52 ×52, 26×26, and 13 ×13 scale feature maps from
five groups.
Table 11: Ablation Studies for fine tuning and group spatial attention
Distillation (denoted as GSA)
Model Fine tune GSA Distilling mAP@0.5 Model-size
CSPDarkNet-Yolov4 ✓ 81.1 90 MB
CSPDarkNet-Yolov4 ✓ 86.6 90 MB
DarkNet53-Yolov3 ✓ 66.8 22 MB
DarkNet53-Yolov3 ✓ 68.2 22 MB
MobileNet-Yolov3 ✓ 72.1 15 MB
MobileNet-Yolov3 ✓ 73.2 15 MB
As shown in Table 11, comparing to fine tune the pruned network, group
spatial attention distillation can achieve a better accuracy for different detection
networks.
In addition, we qualitatively demonstrate the effectiveness of the pruned net-
work through group spatial attention distilling. As shown in Fig.13, (a),(b),(c)
show the original CSPDarknet-Yolov4, DarkNet53-Yolov3, MobileNet-Yolov3, de-
tection results, respectively. And (d),(e),(f) show the detection results of the
pruned network.
7 Conclusions
In this paper, we present a three-stage model compression approach for the object
detection network, which is dynamic sparse training, group channel pruning, and
spatial attention distillation. Firstly, we introduce dynamic sparse training to select
out the insignificant channels in the layers and maintain a good balance of net-
works’ sparsity and accuracy. Next, we propose a group channel pruning method.
Under the same pruning rate, our group pruning method has less influence on
the accuracy of the network and can obtain considerable model compression com-
paring with other pruning methods. After that, we extract each group’s spatial
attention information as the knowledge for distillation. Compared with the direct
fine-tuning of the pruned model, our group spatial attention distilling method can
recover the pruned network to higher accuracy. Furthermore, we deploy the com-
pressed model on the edge device Jetson Nano to demonstrate that our method
can be directly deployed without the support of special hardware or software and

--- PAGE 26 ---
26 Yun Chu et al.
(a) Image 1
 (b) Image 2
 (c) Image 3
(d) Image 4
 (e) Image 5
 (f) Image 6
Fig. 12: The top three figures show the detection results of the original
CSPDarkNet-Yolov4(a), DarkNet-Yolov3(b), MobileNet-Yolov3(c) network, and
the bottom three figures show the detection results of the pruned
CSPDarkNet-Yolov4(d), DarkNet-Yolov3(e), MobileNet-Yolov3(f) network.
can achieve the acceleration effect. To demonstrate the generality and effective-
ness of our proposed approach, in our experiments, we replace the backbone to
MobileNet, DarkNet53, and CSPDarknet to construct the detection network and
then use our proposed methods, the experimental results are satisfactory. We be-
lieve that the proposed methodology and approach are promising to be evaluated
for compressing other object detection networks.
Acknowledgements This work was supported in part by the National Natural Science Foun-
dation of China under Grant 61961014, 61963012 and the Hainan Provincial Natural Science
Foundation of China under Grant 620RC556, 620RC564.
References
1. D. P. Sullivan, C. F. Winsnes, L. ˚Akesson, M. Hjelmare, M. Wiking, R. Schutten, L. Camp-
bell, H. Leifsson, S. Rhodes, A. Nordgren, etal., “Deep learning is combined with massive-
scale citizen science to improve large-scale image classification,” Nature biotechnology,
vol. 36, no. 9, pp. 820–828, 2018.
2. L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietik¨ ainen, “Deep
learning for generic object detection: A survey,” International journal ofcomputer vision,
vol. 128, no. 2, pp. 261–318, 2020.

--- PAGE 27 ---
Group channel pruning and spatial attention distilling for object detection 27
3. F. Sultana, A. Sufian, and P. Dutta, “Evolution of image segmentation using deep convo-
lutional neural network: a survey,” Knowledge-Based Systems, vol. 201, p. 106062, 2020.
4. A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convo-
lutional neural networks,” Communications oftheACM, vol. 60, no. 6, pp. 84–90, 2017.
5. K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in
Proceedings oftheIEEE conference oncomputer vision andpattern recognition, pp. 770–
778, 2016.
6. A. Howard, A. Zhmoginov, L.-C. Chen, M. Sandler, and M. Zhu, “Inverted residuals and
linear bottlenecks: Mobile networks for classification, detection and segmentation,” 2018.
7. S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: towards real-time object detec-
tion with region proposal networks,” IEEE transactions onpattern analysis andmachine
intelligence, vol. 39, no. 6, pp. 1137–1149, 2016.
8. W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, “Ssd:
Single shot multibox detector,” in European conference oncomputer vision, pp. 21–37,
Springer, 2016.
9. A. Farhadi and J. Redmon, “Yolov3: An incremental improvement,” in Computer Vision
andPattern Recognition, pp. 1804–02767, 2018.
10. A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, “Yolov4: Optimal speed and accuracy
of object detection,” arXiv preprint arXiv:2004.10934, 2020.
11. S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J. Dally, “Eie:
Efficient inference engine on compressed deep neural network,” ACM SIGARCH Computer
Architecture News, vol. 44, no. 3, pp. 243–254, 2016.
12. N. Rathi, P. Panda, and K. Roy, “Stdp-based pruning of connections and weight quanti-
zation in spiking neural networks for energy-efficient recognition,” IEEE Transactions on
Computer-Aided Design ofIntegrated Circuits andSystems, vol. 38, no. 4, pp. 668–677,
2018.
13. N. Abderrahmane, E. Lemaire, and B. Miramond, “Design space exploration of hardware
spiking neurons for embedded artificial intelligence,” Neural Networks, vol. 121, pp. 366–
386, 2020.
14. J.-H. Luo and J. Wu, “Autopruner: An end-to-end trainable filter pruning method for
efficient deep model inference,” Pattern Recognition, vol. 107, p. 107461, 2020.
15. F. E. Fernandes Jr and G. G. Yen, “Pruning deep convolutional neural networks architec-
tures with evolution strategy,” Information Sciences, vol. 552, pp. 29–47, 2021.
16. Y. Cheng, M. Lin, J. Wu, H. Zhu, and X. Shao, “Intelligent fault diagnosis of rotating ma-
chinery based on continuous wavelet transform-local binary convolutional neural network,”
Knowledge-Based Systems, vol. 216, p. 106796, 2021.
17. L. Deng, P. Jiao, J. Pei, Z. Wu, and G. Li, “Gxnor-net: Training deep neural networks
with ternary weights and activations without full-precision memory under a unified dis-
cretization framework,” Neural Networks, vol. 100, pp. 49–58, 2018.
18. F. Tung and G. Mori, “Deep neural network compression by in-parallel pruning-
quantization,” IEEE transactions onpattern analysis andmachine intelligence, vol. 42,
no. 3, pp. 568–579, 2018.
19. P. Hu, X. Peng, H. Zhu, M. M. S. Aly, and J. Lin, “Opq: Compressing deep neural networks
with one-shot pruning-quantization,” in Proceedings oftheThirty-Fifth AAAI Conference
onArtificial Intelligence (AAAI-21), Vancouver, VN, Canada, pp. 2–9, 2021.
20. G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” stat,
vol. 1050, p. 9, 2015.
21. T.-B. Xu, P. Yang, X.-Y. Zhang, and C.-L. Liu, “Lightweightnet: Toward fast
and lightweight convolutional neural networks via architecture distillation,” Pattern
Recognition, vol. 88, pp. 272–284, 2019.
22. H. Zhang, Z. Hu, W. Qin, M. Xu, and M. Wang, “Adversarial co-distillation learning for
image recognition,” Pattern Recognition, vol. 111, p. 107659, 2021.
23. D. Song, J. Xu, J. Pang, and H. Huang, “Classifier-adaptation knowledge distillation
framework for relation extraction and event detection with imbalanced data,” Information
Sciences, vol. 573, pp. 222–238, 2021.
24. Z.-R. Wang and J. Du, “Joint architecture and knowledge distillation in cnn for chinese
text recognition,” Pattern Recognition, vol. 111, p. 107722, 2021.
25. Z. Li, Y. Ming, L. Yang, and J.-H. Xue, “Mutual-learning sequence-level knowledge dis-
tillation for automatic speech recognition,” Neurocomputing, vol. 428, pp. 259–267, 2021.
26. P. Shen, X. Lu, S. Li, and H. Kawai, “Knowledge distillation-based representation learning
for short-utterance spoken language identification,” IEEE/ACM Transactions onAudio,
Speech, andLanguage Processing, vol. 28, pp. 2674–2683, 2020.

--- PAGE 28 ---
28 Yun Chu et al.
27. M. Yang, Y. Li, Z. Huang, Z. Liu, P. Hu, and X. Peng, “Partially view-aligned repre-
sentation learning with noise-robust contrastive loss,” in Proceedings oftheIEEE/CVF
Conference onComputer Vision andPattern Recognition, pp. 1134–1143, 2021.
28. H. Ibrahem, A. D. A. Salem, and H.-S. Kang, “Real-time weakly supervised object detec-
tion using center-of-features localization,” IEEE Access, vol. 9, pp. 38742–38756, 2021.
29. Q. Zhou, J. Wang, J. Liu, S. Li, W. Ou, and X. Jin, “Rsanet: Towards real-time ob-
ject detection with residual semantic-guided attention feature pyramid network,” Mobile
Networks andApplications, vol. 26, no. 1, pp. 77–87, 2021.
30. Q. Zhou, X. Wu, S. Zhang, B. Kang, Z. Ge, and L. J. Latecki, “Contextual ensemble
network for semantic segmentation,” Pattern Recognition, vol. 122, p. 108290, 2022.
31. Q. Zhou, Y. Wang, Y. Fan, X. Wu, S. Zhang, B. Kang, and L. J. Latecki, “Aglnet: Towards
real-time semantic segmentation of self-driving images via attention-guided lightweight
network,” Applied SoftComputing, vol. 96, p. 106682, 2020.
32. S.-W. Kim, K. Ko, H. Ko, and V. C. Leung, “Edge-network-assisted real-time object
detection framework for autonomous driving,” IEEE Network, vol. 35, no. 1, pp. 177–183,
2021.
33. M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2: Inverted
residuals and linear bottlenecks,” 2019.
34. Y. Li, J. Li, W. Lin, and J. Li, “Tiny-dsod: Lightweight object detection for resource-
restricted usages,”
35. Z. Qin, Z. Li, Z. Zhang, Y. Bao, G. Yu, Y. Peng, and J. Sun, “Thundernet: Towards
real-time generic object detection on mobile devices,” in Proceedings oftheIEEE/CVF
International Conference onComputer Vision, pp. 6718–6727, 2019.
36. R. J. Wang, X. Li, and C. X. Ling, “Pelee: A real-time object detection system on mobile
devices,” Advances inNeural Information Processing Systems, vol. 31, pp. 1963–1972,
2018.
37. S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network for instance segmen-
tation,”
38. J.-H. Luo, J. Wu, and W. Lin, “Thinet: A filter level pruning method for deep neural
network compression,” in Proceedings oftheIEEE international conference oncomputer
vision, pp. 5058–5066, 2017.
39. Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, “Learning efficient convolutional
networks through network slimming,” in Proceedings oftheIEEE international conference
oncomputer vision, pp. 2736–2744, 2017.
40. A. Jordao, M. Lie, and W. R. Schwartz, “Discriminative layer pruning for convolutional
neural networks,” IEEE Journal ofSelected Topics inSignal Processing, vol. 14, no. 4,
pp. 828–837, 2020.
41. Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the value of network
pruning,” in International Conference onLearning Representations, 2018.
42. B. Li, B. Wu, J. Su, and G. Wang, “Eagleeye: Fast sub-net evaluation for efficient neural
network pruning,” in European Conference onComputer Vision, pp. 639–654, Springer,
2020.
43. J. Zhou, S. Zeng, and B. Zhang, “Two-stage knowledge transfer framework for image
classification,” Pattern Recognition, vol. 107, p. 107529, 2020.
44. J.-W. Jung, H.-S. Heo, H.-J. Shim, and H.-J. Yu, “Knowledge distillation in acoustic scene
classification,” IEEE Access, vol. 8, pp. 166870–166879, 2020.
45. G. Chen, X. Zhang, X. Tan, Y. Cheng, F. Dai, K. Zhu, Y. Gong, and Q. Wang, “Training
small networks for scene classification of remote sensing images via knowledge distillation,”
Remote Sensing, vol. 10, no. 5, p. 719, 2018.
46. G. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker, “Learning efficient object detection
models with knowledge distillation,” Advances inneural information processing systems,
vol. 30, 2017.
47. N. Komodakis and S. Zagoruyko, “Paying more attention to attention: improving the
performance of convolutional neural networks via attention transfer,” in ICLR, 2017.
48. Y. Liu, X. Jia, M. Tan, R. Vemulapalli, Y. Zhu, B. Green, and X. Wang, “Search to distill:
Pearls are everywhere but not the eyes,” in Proceedings oftheIEEE/CVF Conference on
Computer Vision andPattern Recognition, pp. 7539–7548, 2020.
49. S. Santurkar, D. Tsipras, A. Ilyas, and A. Mdry, “How does batch normalization help
optimization,” in Proceedings ofthe32nd international conference onneural information
processing systems, pp. 2488–2498, 2018.

--- PAGE 29 ---
Group channel pruning and spatial attention distilling for object detection 29
50. R. Dasgupta, Y. S. Chowdhury, and S. Nanda, “Performance comparison of benchmark
activation function relu, swish and mish for facial mask detection using convolutional
neural network,” in Intelligent Systems, pp. 355–367, Springer, 2021.
51. Y. Liu, X. Wang, L. Wang, and D. Liu, “A modified leaky relu scheme (mlrs) for topology
optimization with multiple materials,” Applied Mathematics andComputation, vol. 352,
pp. 188–204, 2019.
52. Z. Huang, J. Wang, X. Fu, T. Yu, Y. Guo, and R. Wang, “Dc-spp-yolo: Dense connec-
tion and spatial pyramid pooling based yolo for object detection,” Information Sciences,
vol. 522, pp. 241–258, 2020.
53. M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-
man, “The pascal visual object classes challenge: A retrospective,” International journal
ofcomputer vision, vol. 111, no. 1, pp. 98–136, 2015.
54. R. Padilla, S. L. Netto, and E. A. da Silva, “A survey on performance metrics for object-
detection algorithms,” in 2020 International Conference onSystems, Signals andImage
Processing (IWSSIP), pp. 237–242, IEEE, 2020.
