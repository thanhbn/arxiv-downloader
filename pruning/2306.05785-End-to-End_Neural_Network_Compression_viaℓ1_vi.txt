# Nén Mạng Nơ-ron Đầu-cuối thông qua Hàm Thay thế Độ trễ được Điều chỉnh bởi ℓ1/ℓ2

Anshul Nasery
Google Research India
anshulnasery@google.com

Hardik Shah
Google Research India
hardiknshah@google.com

Arun Sai Suggala
Google Research India
arunss@google.com

Prateek Jain
Google Research India
prajain@google.com

## Tóm tắt

Nén mạng nơ-ron (NN) thông qua các kỹ thuật như cắt tỉa, lượng tử hóa đòi hỏi phải thiết lập các siêu tham số nén (ví dụ: số kênh cần cắt tỉa, độ rộng bit cho lượng tử hóa) cho mỗi lớp một cách thủ công hoặc thông qua tìm kiếm kiến trúc mạng nơ-ron (NAS) có thể tốn kém về mặt tính toán. Chúng tôi giải quyết vấn đề này bằng cách cung cấp một kỹ thuật đầu-cuối tối ưu hóa cho Các Phép Toán Dấu Phẩy Động (FLOPs) của mô hình hoặc cho độ trễ trên thiết bị thông qua một hàm thay thế độ trễ ℓ1/ℓ2 mới. Thuật toán của chúng tôi linh hoạt và có thể được sử dụng với nhiều phương pháp nén phổ biến bao gồm cắt tỉa, phân tích thừa số hạng thấp và lượng tử hóa. Quan trọng là, nó nhanh và chạy trong cùng lượng thời gian như việc huấn luyện một mô hình đơn; đây là một sự gia tăng tốc độ huấn luyện đáng kể so với các phương pháp NAS tiêu chuẩn. Đối với nén BERT trên các tác vụ tinh chỉnh GLUE, chúng tôi đạt được giảm 50% FLOPs chỉ với 1% giảm hiệu suất. Đối với nén MobileNetV3 trên ImageNet-1K, chúng tôi đạt được giảm 15% FLOPs và giảm 11% độ trễ trên thiết bị mà không giảm độ chính xác, trong khi vẫn yêu cầu ít hơn 3× tính toán huấn luyện so với các kỹ thuật nén SOTA. Cuối cùng, đối với học chuyển giao trên các tập dữ liệu nhỏ hơn, kỹ thuật của chúng tôi xác định các kiến trúc rẻ hơn 1.2×-1.4× so với MobileNetV3 tiêu chuẩn, bộ kiến trúc EfficientNet với chi phí huấn luyện và độ chính xác gần như tương tự.

## 1 Giới thiệu

Các mạng nơ-ron quy mô lớn liên tục cung cấp hiệu suất tối ưu trên các tác vụ học tập phức tạp [1,2,3]. Nhưng chúng đặt gánh nặng lớn lên các tài nguyên tính toán như pin, bộ nhớ hoặc bộ xử lý khiến chúng khó triển khai trên các thiết bị biên như điện thoại, máy ảnh và thiết bị đeo. Một số nghiên cứu gần đây đã thiết kế các kỹ thuật để nén các mô hình ML và làm cho chúng hiệu quả cho suy luận. Tuy nhiên, như được mô tả chi tiết dưới đây, nhiều kỹ thuật này khó sử dụng trong thực tế và thường đạt được sự đánh đổi không tối ưu giữa độ chính xác và thời gian suy luận.

**Tìm kiếm siêu tham số cho nén.** Các nghiên cứu hiện tại thường dựa vào một trong các khối xây dựng sau để thiết kế các mô hình hiệu quả: độ thưa trọng số không có cấu trúc [4,5], cắt tỉa toàn bộ nơ-ron hoặc phân tích thừa số hạng thấp [6], lượng tử hóa [7], chưng cất [8]. Việc tìm ra cách tối ưu để kết hợp các khối xây dựng này (hoặc tìm ra các siêu tham số như lượng độ thưa liên quan đến mỗi khối) trong khi thỏa mãn một ràng buộc toàn cục về độ trễ/FLOPs/tài nguyên là khó khăn và liên quan đến tìm kiếm tổ hợp. Vấn đề này càng trở nên trầm trọng hơn khi nhiều khối xây dựng được sử dụng để nén mô hình (ví dụ: phân tích thừa số hạng thấp đồng thời, độ thưa/cắt tỉa trọng số).

**Cắt tỉa nơ-ron:** Trong số các kỹ thuật loại (a) được đề cập ở trên, một hướng nghiên cứu nổi bật đã tập trung vào cắt tỉa trọng số không có cấu trúc với phân bổ ngân sách không đồng đều giữa các lớp [4,9,10,5]. Tuy nhiên, bất kỳ lợi ích nào trong FLOPs sử dụng cắt tỉa không có cấu trúc đều khó chuyển đổi thành lợi ích độ trễ thực tế vì phần cứng hiện đại - như GPU, TPU - được thiết kế nhiều hơn cho các phép toán ma trận dày đặc. Vì vậy, việc tập trung vào cắt tỉa nơ-ron, loại bỏ toàn bộ nơ-ron/kênh, và phân tích thừa số hạng thấp của trọng số, có liên quan chặt chẽ đến cắt tỉa nơ-ron, sẽ có lợi hơn. Các kỹ thuật gần đây trong hướng nghiên cứu này thêm một bộ điều chỉnh độ trễ/FLOPs vào hàm mất mát entropy chéo tiêu chuẩn [11,12] để thiên hướng mô hình về số lượng nơ-ron thấp hơn. Thật không may, mục tiêu kết quả là rời rạc và khó tối ưu hóa. Để giảm bớt điều này, các nghiên cứu hiện tại đã thiết kế các hàm thay thế liên tục dễ tối ưu hóa hơn với kiểu SGD. Các phương pháp này hoặc làm việc trong không gian phân phối xác suất trên các mô hình đã cắt tỉa và tối ưu hóa "mục tiêu kỳ vọng" [12,13,6] hoặc thay thế bộ điều chỉnh FLOPs không liên tục bằng một hàm thay thế liên tục như chuẩn ℓ1 của trọng số mạng [11]. Tuy nhiên, lớp kỹ thuật đầu thường không ổn định, khó thực hiện trong thực tế, và các nghiên cứu thực nghiệm cho thấy hiệu suất của chúng tương tự như cắt tỉa dựa trên độ lớn đơn giản [14] (xem thêm biểu đồ bên trái của Hình 1). Hơn nữa, như chúng tôi chỉ ra trong nghiên cứu này, lớp kỹ thuật sau không thể thực thi độ thưa khi có bình thường hóa batch, layer (xem Phần 3).

**NAS:** Một số nghiên cứu trong loại (b) định hình nén mô hình như một vấn đề Tìm kiếm Kiến trúc Mạng Nơ-ron (NAS) hộp đen và dựa vào các kỹ thuật NAS tối ưu để tìm kiếm các mô hình hiệu quả [15,16,17,18,19]. Các kỹ thuật này trực tiếp tính đến độ trễ/FLOPs và có tiềm năng xác định phân bổ ngân sách tối ưu cho mỗi lớp cho nhiều loại khối/cơ chế nén hiệu quả. Tuy nhiên, các phương pháp này thường tốn kém về mặt tính toán vì chúng có quan điểm hộp đen về vấn đề và thực hiện tìm kiếm tổ hợp trong không gian kiến trúc. Mặc dù có những tiến bộ gần đây như TuNAS [18] và DARTS [20], các kỹ thuật này có thể chậm hơn và kém chính xác hơn một bậc so với phương pháp đề xuất của chúng tôi (xem Hình 1).

**Phương pháp của chúng tôi:** Trong nghiên cứu này, chúng tôi đề xuất một phương pháp nằm ngay giữa hai loại được đề cập ở trên. Tức là, phương pháp của chúng tôi áp dụng cho một lớp lớn các khối xây dựng hiệu quả - như độ thưa không có cấu trúc, cắt tỉa nơ-ron, lượng tử hóa - mà chúng tôi có thể viết tính toán FLOPs với một hàm thay thế liên tục (xem Bảng 1). Hơn nữa, để đảm bảo rằng các bộ điều chỉnh FLOPs, độ trễ của chúng tôi hoạt động ngay cả khi có batchnorm, layernorm, chúng tôi đề xuất một hàm thay thế mới dựa trên chuẩn ℓ1/ℓ2. Trong khi các hàm thay thế của chúng tôi liên tục, chúng không khả vi. Trong những trường hợp như vậy, các trình tối ưu tiêu chuẩn như SGD, Adam có thể khá chậm hội tụ [24]. Để khắc phục điều này, chúng tôi đề xuất một phép chiếu trên các biến mặt nạ, sau mỗi bước SGD. Phương pháp đề xuất của chúng tôi tăng tốc độ hội tụ và cũng xuất ra các giải pháp thưa chính xác do đó loại bỏ nhu cầu ngưỡng hóa hậu-hoc, trong khi đủ đơn giản để không tăng thời gian huấn luyện đáng kể.

Chúng tôi thực hiện thuật toán của mình với nhiều khối xây dựng bao gồm cắt tỉa, phân tích thừa số hạng thấp, lượng tử hóa, và áp dụng nó trên nhiều vấn đề trong lĩnh vực phân loại hình ảnh và NLP. Đặc biệt, chúng tôi chứng minh tính hiệu quả của kỹ thuật này cho nén MobileNetV3 trên ImageNet (xem Hình 1), nơi phương pháp của chúng tôi có thể học một kiến trúc với FLOPs (độ trễ) thấp hơn tới 15% (11%) trên điện thoại di động Pixel 6, mà không có bất kỳ giảm độ chính xác nào. Ở đây phương pháp của chúng tôi chính xác hơn MorphNet, một kỹ thuật SOTA tập trung độc quyền vào cắt tỉa nơ-ron, cũng như TuNAS, một kỹ thuật NAS SOTA. Hơn nữa, về thời gian huấn luyện, phương pháp của chúng tôi rẻ hơn 3× so với TuNAS. Chúng tôi muốn nhấn mạnh rằng MobileNetv3 là một kiến trúc được tối ưu hóa cao được tìm thấy bằng các kỹ thuật NAS hiệu quả [25], và kỹ thuật của chúng tôi có thể nén kiến trúc này thêm nữa.

Một ứng dụng thú vị của nghiên cứu chúng tôi là có thể áp dụng nó để tối ưu hóa một số mô hình cơ sở "nền tảng" nhất định cho các tác vụ tinh chỉnh cá nhân. Ví dụ, đối với nén BERT trên các tiêu chuẩn GLUE, phương pháp của chúng tôi đạt được giảm 40-50% FLOPs chỉ với 1% giảm độ chính xác (xem Hình 1). Hơn nữa, kỹ thuật của chúng tôi vượt trội hơn các baseline nén mô hình tiêu chuẩn. Tương tự đối với các tác vụ phân loại thị giác nhỏ hơn, kỹ thuật của chúng tôi nén MobileNetV3, bộ kiến trúc EfficientNet và xác định các kiến trúc rẻ hơn 1.2×-1.4× mà không mất độ chính xác đáng kể (xem Hình 3). Chúng tôi muốn lưu ý rằng tất cả các kết quả này được thu được với chi phí gần như bằng với việc huấn luyện một mô hình đơn cho tác vụ. Cuối cùng, chúng tôi cũng chứng minh tính linh hoạt của phương pháp bằng cách sử dụng nó để lượng tử hóa một CNN trên CIFAR-10, và học độ rộng bit (2,4,8,16) cho mỗi lớp của nó. Kỹ thuật của chúng tôi tìm thấy một mô hình nhỏ hơn 55% so với mô hình float-16 baseline, trong khi đạt được cùng độ chính xác (xem Hình 5). Trong khi lượng tử hóa bit thấp thường không được khai thác bởi các accelerator đa mục đích để tăng tốc tính toán, nó vẫn có thể dẫn đến giảm thời gian suy luận của các mô hình ngôn ngữ lớn như GPT vì các mô hình này bị ràng buộc bởi băng thông bộ nhớ [26]. Đây là tóm tắt các đóng góp của chúng tôi:

(1). Chúng tôi cung cấp một kỹ thuật nén mạng nơ-ron đầu-cuối trực tiếp tối ưu hóa mục tiêu được điều chỉnh FLOPs/độ trễ trong quá trình dẫn đến nén trong huấn luyện. Thuật toán của chúng tôi có thể được sử dụng với nhiều khối xây dựng hiệu quả phổ biến bao gồm cắt tỉa, phân tích thừa số hạng thấp, lượng tử hóa, và có thể tối ưu hóa cho độ trễ suy luận trên thiết bị.

(2). Chúng tôi thiết kế một hàm thay thế được điều chỉnh ℓ1/ℓ2 mới cho độ trễ hoạt động ngay cả khi có batchnorm, layernorm. Thuật toán của chúng tôi nhanh và chạy trong cùng lượng thời gian như huấn luyện một mô hình đơn, và không yêu cầu bất kỳ bước hậu xử lý nào.

(3). Chúng tôi chứng minh hiệu suất của kỹ thuật trên cả tác vụ ngôn ngữ và thị giác. Hơn nữa, đối với cài đặt học chuyển giao nơi mục tiêu là lấy một kiến trúc baseline và tối ưu hóa nó cho các tác vụ cá nhân, các kỹ thuật của chúng tôi vượt trội hơn các kỹ thuật SOTA trong lĩnh vực rộng của nén nơ-ron tự động.

## 2 Nghiên cứu Liên quan

### 2.1 Tìm kiếm Kiến trúc Mạng Nơ-ron

Các nghiên cứu đầu về NAS coi vấn đề như một vấn đề tối ưu hóa hộp đen thuần túy (BO). Các nghiên cứu này dựa vào các kỹ thuật BO như tìm kiếm ngẫu nhiên [27], tối ưu hóa quy trình Gaussian [17], và gradient descent bậc không [15,16], thuật toán tiến hóa để tối ưu hóa mục tiêu NAS và xác định một kiến trúc tốt. Một số nghiên cứu đã cải thiện các thuật toán này bằng các heuristic như dừng sớm [27]. Tuy nhiên, các kỹ thuật này tốn kém về mặt tính toán, vì việc đánh giá mục tiêu tối ưu hóa tại bất kỳ điểm nào đòi hỏi huấn luyện một mạng nơ-ron từ đầu. Hơn nữa, do độ phức tạp tính toán, các kỹ thuật này thực hiện tìm kiếm rất thô và không phù hợp cho tìm kiếm chi tiết về cấu trúc thưa hoặc hạng thấp.

Các nghiên cứu gần đây đã cố gắng mở hộp đen một chút. Trong các kỹ thuật này, không gian tìm kiếm đầu tiên được chuyển đổi thành không gian phân phối xác suất trên kiến trúc. Tiếp theo, một mô hình thay thế (nhận một kiến trúc làm đầu vào và cố gắng xuất ra tập trọng số tối ưu cho kiến trúc) được huấn luyện để nhanh chóng đánh giá mục tiêu tối ưu hóa tại bất kỳ đầu vào nào [18,20,28,29,12]. Trong khi các kỹ thuật này nhanh, chúng liên quan đến huấn luyện chung của mô hình thay thế trong quá trình tìm kiếm. Việc huấn luyện chung này thường làm cho quá trình tối ưu hóa không ổn định [30].

**NAS cho ML Hiệu quả.** Một số nghiên cứu gần đây tại giao điểm của ML hiệu quả và NAS đã nhận ra tầm quan trọng của việc tính toán rõ ràng phần cứng trong quá trình tìm kiếm [15,31,32,33,34,35]. Các nghiên cứu này kết hợp thời gian suy luận thực tế vào mục tiêu tìm kiếm của họ, thay vì các hàm thay thế như FLOPs. Thời gian suy luận có thể được ước tính bằng một mạng nơ-ron khác, hoặc thông qua bảng độ trễ cho các phép toán số học cơ bản trên nền tảng đích [19]. Nhiều nghiên cứu này dựa vào các heuristic tìm kiếm tham lam, ngẫu nhiên để giải quyết mục tiêu kết quả [32, 33]. Tuy nhiên, các heuristic này hoặc mất nhiều thời gian để tìm kiến trúc tối ưu hoặc không được đảm bảo hội tụ đến giải pháp tối ưu. Có một số nghiên cứu dựa vào các thuật toán NAS được mô tả ở trên [15,31,18]. Tuy nhiên, các kỹ thuật này gặp phải cùng các vấn đề như đã đề cập trước đó.

**Thiết kế đồng thời Phần cứng, Kiến trúc Mạng Nơ-ron.** Một số tham số cấp phần cứng như cấu hình chia nhỏ tensor ảnh hưởng đáng kể đến thời gian suy luận của mô hình. Các kỹ thuật NAS nhận biết phần cứng gần đây tiết lộ các tham số cấp phần cứng này cho thuật toán NAS và đồng thời tìm kiếm trên kiến trúc mạng nơ-ron và cấu hình phần cứng [35]. Các kỹ thuật này có tiềm năng đạt được hiệu suất tốt hơn so với các kỹ thuật NAS vanilla không tìm kiếm trên cấu hình phần cứng.

### 2.2 Nén Mô hình

Lĩnh vực nén mô hình rất rộng lớn. Ở đây, chúng tôi tập trung vào các kỹ thuật thực hiện nén thời gian huấn luyện (trái ngược với nén hậu huấn luyện) sử dụng các khối xây dựng sau: độ thưa không có cấu trúc, cắt tỉa và phân tích thừa số hạng thấp. Các nghiên cứu đầu về độ thưa không có cấu trúc và cắt tỉa dựa vào cắt tỉa dựa trên độ lớn, gradient [4,36,14]. Một số nghiên cứu đã khám phá các chỉ số chấm điểm tinh vi hơn cho cắt tỉa [37,38,39,40,41]. Các kỹ thuật khác bao gồm thêm các chuẩn cảm ứng độ thưa như ℓ0, ℓ1 vào mục tiêu huấn luyện [13,5]. Một số nghiên cứu cũng đã khám phá phân tích thừa số hạng thấp để nén mô hình [42,43,44]. Một số kỹ thuật này lại dựa vào các bộ điều chỉnh cảm ứng độ thưa để cảm ứng cấu trúc hạng thấp [6]. Những kỹ thuật khác dựa vào cắt tỉa dựa trên SVD. Một số nghiên cứu gần đây cố gắng tối ưu hóa mục tiêu được điều chỉnh FLOPs để thực hiện cắt tỉa, phân tích thừa số hạng thấp [11,12]. Tuy nhiên, như chúng tôi đã thảo luận trong phần giới thiệu, các kỹ thuật tối ưu hóa kết quả thường không ổn định và khó sử dụng trong thực tế.

## 3 Phương pháp

Trong phần này, chúng tôi mô tả phương pháp nén mô hình của mình. Để đơn giản trong trình bày, chúng tôi minh họa kỹ thuật trên các mạng feed-forward và giới hạn bản thân trong cắt tỉa. Các ý tưởng ở đây có thể được mở rộng cho các kiến trúc khác (ví dụ: convolution 1x1 trong CNN), và các khối xây dựng hiệu quả khác (ví dụ: độ thưa không có cấu trúc, phân tích thừa số hạng thấp, lượng tử hóa) một cách đơn giản (xem Bảng 1 để biết chi tiết). Xem xét vấn đề sau: chúng ta được cho một mạng nơ-ron feed forward (FFN) đã được huấn luyện trước f*(x) = σ(W*D σ(W*D-1 σ(... σ(W*1 x)))), trong đó W*i ∈ R^(di+1×di) cho tất cả i ∈ [D], và một tập dữ liệu {(xi, yi)}^n_i=1. Mục tiêu của chúng ta là nén f* trong khi đồng thời hoạt động tốt trên tác vụ học tập. Vấn đề này có thể được công thức hóa như vấn đề tối ưu hóa sau:

min W (1/n) Σ^n_i=1 ℓ(xi, yi; W) + λ × Latency(W). (1)

Ở đây W = {Wi}^D_i=1, với Wi ∈ R^(d'i+1×d'i) là ma trận trọng số tại lớp i, λ là tham số điều chỉnh đánh đổi độ trễ với độ chính xác và ℓ là hàm mất mát có giám sát. Tối ưu hóa trực tiếp mục tiêu trên là không thể thực hiện được vì Latency(W) là một hàm rời rạc của các chiều của ma trận trọng số, và phụ thuộc vào phần cứng cụ thể.

Bây giờ chúng tôi trình bày kỹ thuật của mình để giải Phương trình (1). Để bắt đầu, chúng tôi thay thế Latency(W) bằng FLOPs(W). Sau này, chúng tôi mở rộng nó cho độ trễ thực tế. Mục tiêu trong trường hợp này được cho bởi:

min W (1/n) Σ^n_i=1 ℓ(xi, yi; W) + λ Σ^D_i=1 d'i d'i+1. (2)

Để giải quyết mục tiêu này, chúng tôi liên kết mặt nạ với mỗi nơ-ron trong mạng. Đặc biệt, chúng tôi tham số hóa ma trận trọng số trong lớp thứ i là Wi × diag(αi). Ở đây αi ∈ {0,1}^di là các biến mặt nạ của lớp i. Nếu αi,j được đặt bằng 0, thì nơ-ron thứ j trong lớp thứ (i-1) sẽ bị cắt tỉa. Bộ điều chỉnh FLOPs giờ đây có thể được viết theo mặt nạ như Σ^D_i=1 ||αi||0 ||αi+1||0, trong đó αD+1 là vector tĩnh toàn số 1. Tuy nhiên, mục tiêu kết quả không liên tục. Để làm cho nó liên tục và phù hợp với tối ưu hóa dựa trên gradient, một lớp kỹ thuật đặt phân phối Bernoulli Bern(pi,j) trên mỗi mặt nạ αi,j và giải quyết mục tiêu làm mịn sau [12, 13, 6]:

min W,p E[(1/n) Σ^n_i=1 ℓ(xi, yi; p, W) + λ Σ^D_i=1 ||αi||0 ||αi+1||0].

Kỳ vọng ở trên được lấy w.r.t các mặt nạ ngẫu nhiên αi. Dễ thấy rằng mục tiêu trên tương đương với Phương trình (2), và do đó khó giải quyết như phương trình sau. Thực tế, vấn đề trên có thể được chứng minh là NP-hard bằng cách sử dụng quan sát rằng hồi quy tuyến tính thưa là một trường hợp đặc biệt của nó [45]. Hơn nữa, bản chất rời rạc của αi làm cho quá trình tối ưu hóa không ổn định [13]. Để khắc phục điều này, [12,13,6] dựa vào một heuristic bao gồm việc nới lỏng phân phối Bernoulli thành một phân phối liên tục như LogisticSigmoid. Tuy nhiên, nhược điểm chính của thuật toán kết quả là nó khó thực hiện trong thực tế và đòi hỏi việc ủ rất cẩn thận các tham số của phân phối LogisticSigmoid. Một nhược điểm khác của lớp kỹ thuật này là hiệu suất của chúng không được hiểu rõ về mặt lý thuyết, ngay cả đối với các vấn đề đơn giản và cơ bản như hồi quy tuyến tính thưa.

Một phương pháp khác để chuyển đổi mục tiêu rời rạc trong Phương trình (2) thành một hàm liên tục là thay thế chuẩn ℓ0 trên αi bằng chuẩn ℓ1:

min W,αi∈R^di (1/n) Σ^n_i=1 ℓ(xi, yi; α, W) + λ Σ^D_i=1 ||αi||1 ||αi+1||1. (3)

Phương pháp này hấp dẫn hơn nhiều so với phương pháp trước vì nó được biết là khôi phục các giải pháp thưa tối ưu cho nhiều vấn đề thống kê bao gồm hồi quy tuyến tính thưa, hoàn thành ma trận hạng thấp [46,47]. Hơn nữa, nó đơn giản hơn nhiều để thực hiện trong thực tế, với nhiều thuật toán được đề xuất để hội tụ nhanh đến các điểm dừng của mục tiêu [24,48]. Do đó, các kỹ thuật nén SOTA gần đây dựa vào các hàm thay thế chuẩn ℓ1 để tính toán bộ điều chỉnh FLOPs [11]. Một nhược điểm lớn của chuẩn ℓ1 là nó không thúc đẩy độ thưa khi có bình thường hóa batch và layer [49,50]. Để thấy điều này, hãy xem xét mạng 1 lớp ẩn sau: σ(BN(W2 diag(α2) σ(BN(W1 diag(α1) x)))). Người ta có thể thu nhỏ tất cả các mục của α1 và tăng tỷ lệ trọng số W1 mà không ảnh hưởng đến đầu ra của mạng. Làm như vậy giảm giá trị mục tiêu trong Phương trình (3), nhưng không gây ra bất kỳ độ thưa nào trong mạng. Trong thực tế, chúng tôi thực sự nhận thấy hành vi này trong quá trình tối ưu hóa Phương trình (3), dẫn đến các giải pháp không tối ưu (xem Phần 3.2). Lưu ý rằng việc thêm penalty ℓ2 trên trọng số (tức là weight decay) không giảm thiểu vấn đề này vì bất kỳ tỷ lệ nào của α có thể được hấp thụ bởi các tham số batch norm mà không thay đổi đầu ra của mạng.

### 3.1 Cảm ứng độ thưa thông qua bộ điều chỉnh ℓ1/ℓ2

Bây giờ chúng tôi giới thiệu phương pháp của mình để làm cho mục tiêu trong Phương trình (2) liên tục. Chúng tôi thay thế chuẩn ℓ0 trên mặt nạ (||αi||0) bằng penalty ℓ1/ℓ2 (√di ||αi||1/||αi||2) và giải quyết vấn đề tối ưu hóa sau:

min W,αi∈R^di (1/n) Σ^n_i=1 ℓ(xi, yi; α, W) + λ Σ^D_i=1 (√di ||αi||1/||αi||2) (√di+1 ||αi+1||1/||αi+1||2). (4)

Số hạng √di trong tử số chuẩn hóa penalty để nằm giữa [0, di]. Khi αi đều là 1, bộ điều chỉnh đánh giá thành FLOPs. Quan sát rằng bộ điều chỉnh này bất biến với việc tỷ lệ của α. Do đó, giá trị của bộ điều chỉnh không thể đơn giản được giảm bằng cách thu nhỏ αi. Trong các thí nghiệm của chúng tôi trong phần 3.2 và Phụ lục C.2, chúng tôi chỉ ra rằng điều này xử lý bình thường hóa batch, layer tốt hơn so với bộ điều chỉnh ℓ1. Một số nghiên cứu đã nghiên cứu bộ điều chỉnh này trong bối cảnh hồi quy tuyến tính thưa và chỉ ra rằng nó khôi phục tín hiệu thưa cơ bản dưới các điều kiện nhẹ trên dữ liệu [51,52,53]. [54] đã sử dụng một bộ điều chỉnh ℓ1/ℓ2 tương tự cho cắt tỉa mạng, nhưng kỹ thuật của họ không tối ưu hóa độ trễ hoặc FLOPs, và dựa vào ngưỡng hóa hậu huấn luyện để có độ thưa.

Vì một số lý do kỹ thuật được mô tả sau, chúng tôi thêm một ràng buộc tích cực trên αi và giải quyết mục tiêu sau:

min W,αi∈R^di_+ (1/n) Σ^n_i=1 ℓ(xi, yi; α, W) + λ Σ^D_i=1 (√di Σ^di_j=1 αi,j/||αi||2) (√di+1 Σ^di+1_j=1 αi+1,j/||αi+1||2). (5)

Lưu ý rằng chúng tôi xem xét α ∈ R^di_+ thay vì các giá trị rời rạc hoặc bị ràng buộc. Chúng tôi muốn nhấn mạnh rằng sự thay đổi này không làm giảm sức mạnh đại diện của mô hình của chúng tôi. Nó chủ yếu được thực hiện vì lý do tính toán. Trong phần tiếp theo, chúng tôi sử dụng ký hiệu ngắn gọn ||αi||1p (p cho positive) để biểu thị Σ^di_j=1 αi,j.

**Tầm quan trọng của các ràng buộc tích cực.** Mục tiêu trong Phương trình (4) liên tục, nhưng không trơn. Đối với những hàm mất mát như vậy, các kỹ thuật tối ưu hóa tiêu chuẩn như SGD, Adam chậm hội tụ đến các điểm dừng [55]. Hơn nữa, các thuật toán này không xuất ra các giải pháp thưa chính xác. Điều này buộc phải đưa các bước hậu xử lý bổ sung vào pipeline nén. Ví dụ, [11,54] dựa vào trình tối ưu Adam và thêm một bước cắt tỉa ở cuối, nơi các mặt nạ gần với 0 được cắt tỉa đi. Điều này khá rườm rà trong thực tế vì người ta cần chọn các ngưỡng phù hợp để cắt tỉa, điều này đưa ra một siêu tham số có thể điều chỉnh bổ sung, và cần huấn luyện lại sau khi cắt tỉa.

Để khắc phục điều này, chúng tôi thêm một ràng buộc tích cực vào các biến mặt nạ và sửa đổi mục tiêu thành Phương trình (5). Điều này làm cho bộ điều chỉnh trơn (trừ tại vector toàn số 0), và dễ tối ưu hóa bằng SGD, Adam. Sau mỗi cập nhật SGD/Adam, chúng tôi đơn giản chiếu các mặt nạ trở lại không gian số thực dương. Cập nhật tổng thể trông như sau:

W ← W - η∇W(L(α,W) + λR(α)), α ← max(0, α - η∇α(L(α,W) + λR(α))).

Ở đây L(α,W) là rủi ro thực nghiệm và R(α) là bộ điều chỉnh. Chú ý, bước bổ sung duy nhất so với tối ưu hóa truyền thống, là việc cắt của α. Trong các nghiên cứu tiêu giảm của chúng tôi trong Phần 3.2 và Phụ lục C.2, chúng tôi xác nhận tầm quan trọng của bước chiếu này, cùng với chuẩn ℓ1/ℓ2, trong việc khuyến khích các giải pháp thưa.

### 3.2 Xác minh các lựa chọn thiết kế

Để chứng minh thực nghiệm các nhược điểm của việc sử dụng penalty ℓ1 để nén mô hình, chúng tôi thực hiện thí nghiệm trên tập dữ liệu FashionMNIST với một mạng kết nối đầy đủ một lớp ẩn có một lớp batch norm sau lớp tuyến tính đầu tiên. Chúng tôi cắt tỉa đầu vào cho mạng bằng cách sử dụng một mặt nạ α trên đầu vào. Chúng tôi so sánh hiệu suất của các mạng được nén bằng cách sử dụng bộ điều chỉnh FLOPs được cảm ứng bởi các chuẩn ℓ1 và ℓ1/ℓ2. Chúng tôi sử dụng SGD để tối ưu hóa cả hai mục tiêu. Hơn nữa, chúng tôi huấn luyện trước mạng bằng cách sử dụng hàm mất mát CE tiêu chuẩn, và khởi tạo α = 1. Chúng tôi theo dõi phương sai của các giá trị tuyệt đối của các mục của α, tức là Σ^d_i=1(|αi| - μα)²/d, trong đó μα = Σ^d_i=1|αi|/d. Chúng tôi cũng theo dõi trung bình μα của các giá trị tuyệt đối của các mục của α. Cuối cùng, chúng tôi vẽ đường cong giữa FLOPs và chuẩn được xem xét của α (tức là ℓ1, ℓ1/ℓ2). Hình 2 trình bày kết quả từ các thí nghiệm này. Chúng ta có thể thấy rằng mục tiêu ℓ1 không được căn chỉnh với giá trị thực tế của FLOPs, trong khi bộ điều chỉnh được tính toán bằng ℓ1/ℓ2 là một proxy tốt hơn. Chúng tôi cũng thấy rằng trung bình và phương sai của α giảm mạnh khi bộ điều chỉnh FLOPs được cảm ứng ℓ1 được sử dụng để nén. Điều này cho thấy rằng tất cả các mục của α được thu nhỏ đồng đều xuống một giá trị nhỏ, khác không, làm giảm giá trị mục tiêu, trong khi không cung cấp bất kỳ độ thưa nào. Như được thấy từ hình, ℓ1/ℓ2 không bị nhược điểm này. Cuối cùng, chúng tôi lưu ý rằng chuẩn frobenius của ma trận trọng số W tăng khi điều chỉnh ℓ1 được sử dụng trên α, cho thấy rằng mạng đơn giản là thu nhỏ α và tăng tỷ lệ trọng số để tránh bộ điều chỉnh.

### 3.3 Nén mô hình nhận biết phần cứng

Trong phần này, chúng tôi mở rộng bộ điều chỉnh FLOPs để tính đến độ trễ trên phần cứng đích. Bộ điều chỉnh kết quả đặc biệt hữu ích để thực hiện nén mạng nhận biết phần cứng. Quan sát chính của chúng tôi là suy luận trên một mạng nơ-ron có thể được chia thành một loạt các phép toán nhân ma trận. Ví dụ, suy luận trên một FFN độ sâu D liên quan đến D phép nhân ma trận-vector, chiếm phần lớn thời gian. Vì vậy, việc có một ước tính tốt về thời gian suy luận của toàn bộ mạng quy về việc có một ước tính tốt về độ trễ của phép nhân ma trận-vector. Để làm điều này, chúng tôi dựa vào các bảng tra cứu. Trước khi bắt đầu giai đoạn cắt tỉa, chúng tôi xây dựng một bảng tra cứu 2 chiều T mà mục thứ (d1, d2) là độ trễ trên thiết bị của việc nhân một ma trận kích thước d1×d2 với một vector kích thước d2. Một bảng như vậy dễ xây dựng, với quyền truy cập vào thiết bị đích. Tiếp theo, để kết hợp bảng tra cứu T vào thuật toán cắt tỉa của chúng tôi, chúng tôi chuyển đổi nó thành một hàm liên tục bằng cách thực hiện nội suy tuyến tính trên các mục trong bảng [56]. Cụ thể, đối với bất kỳ (x, y) ∈ [d1, d1 + 1] × [d2, d2 + 1], trong đó d1, d2 ∈ N ∪ {0}, chúng tôi định nghĩa T(x, y) như: T(x, y) = t1 + (t2 - t1)(y - d2), trong đó t1 = T(d1, d2) + (T(d1 + 1, d2) - T(d1, d2))(x - d1), và t2 = T(d1, d2 + 1) + (T(d1 + 1, d2 + 1) - T(d1, d2 + 1))(x - d1). Lưu ý rằng trái ngược với các kỹ thuật NAS hộp đen như [19] tìm kiếm trong không gian rời rạc số bộ lọc cho mỗi khối, phương pháp của chúng tôi cần hàm thay thế độ trễ có thể vi phân, và do đó chúng tôi cần các bảng độ trễ được nội suy. Xem phụ lục để biết chi tiết về cách chúng tôi xây dựng các bảng.

Chúng tôi sử dụng bảng tra cứu được nội suy này để xây dựng bộ điều chỉnh độ trễ của chúng tôi như sau:

Σ^D_i=1 T((√di ||αi||1p/||αi||2), (√di+1 ||αi+1||1p/||αi+1||2)). (6)

Trong biểu thức trên, hàm thay thế có thể vi phân của chúng tôi cho ||αi||0 (tức là √di ||αi||1p/||αi||2), được sử dụng để lập chỉ mục bảng tra cứu. Chúng tôi lưu ý rằng chuẩn ℓ1/ℓ2 rất quan trọng để kỹ thuật này thành công. Điều này là do √di ||αi||1p/||αi||2 được chuẩn hóa và luôn nằm giữa [0, di]. Ngược lại, việc sử dụng hàm thay thế chuẩn ℓ1 trong bộ điều chỉnh cho chúng ta T(||αi||1, ||αi+1||1). Việc tỷ lệ αi bằng một hằng số có thể thay đổi đáng kể bộ điều chỉnh này, và làm cho tối ưu hóa không ổn định.

## 4 Thí nghiệm

Trong phần này, chúng tôi áp dụng framework của mình cho các tác vụ huấn luyện trước và học chuyển giao quy mô lớn trên các tiêu chuẩn ngôn ngữ và thị giác tiêu chuẩn. Để chứng minh tính linh hoạt của kỹ thuật, chúng tôi thực hiện thí nghiệm trên nhiều họ mô hình (MobileNet, EfficientNet [2], BERT), và nhiều khối xây dựng (cắt tỉa, phân tích thừa số hạng thấp, lượng tử hóa). Chúng tôi cũng trình bày một nghiên cứu tình huống sử dụng độ trễ thực tế trên thiết bị thay vì FLOPs. Xem Phụ lục C.2 cho các nghiên cứu tiêu giảm khác.

### 4.1 Huấn luyện trước ImageNet

Chúng tôi bắt đầu bằng cách so sánh hiệu suất của kỹ thuật với các baseline trên nén MobileNetV3, cho phân loại ImageNet. Chúng tôi dựa vào phân tích thừa số hạng thấp + cắt tỉa để nén. Kết quả từ thí nghiệm này được trình bày trong Hình 1. Bằng cách thay đổi cường độ điều chỉnh của chúng tôi, chúng tôi thu được các mô hình với MAC và độ chính xác khác nhau. Chúng tôi thấy rằng các mô hình được tạo ra bởi phương pháp của chúng tôi vượt trội đáng kể so với MobileNetV3 và TuNAS trong chế độ MAC cao và trung bình. Đặc biệt, đối với cùng độ chính xác như MobileNetV3Large, phương pháp của chúng tôi tìm thấy một mô hình với 15% MAC ít hơn. So với TuNAS, chúng tôi đạt được giảm 30% MAC ở cùng mức độ chính xác. Tuy nhiên, chúng tôi thấy rằng mô hình của chúng tôi ngang bằng với MobileNetV3Small trong chế độ MAC thấp, cho thấy rằng mô hình trước đã được điều chỉnh tốt cho tác vụ này. Về mặt tính toán cần thiết để huấn luyện, TuNAS là đắt nhất trong số tất cả các kỹ thuật chúng tôi đã thử; nó mất 2 ngày để huấn luyện với thiết lập phần cứng của chúng tôi. Ngược lại, phương pháp của chúng tôi mất 13 giờ (nhanh hơn 3-4× so với TuNAS), và MorphNet mất 10 giờ.

### 4.2 Học Chuyển giao

Một mô hình phổ biến trong việc triển khai các mô hình học máy ngày nay là đầu tiên huấn luyện trước chúng trên một tập dữ liệu quy mô lớn như ImageNet, và sau đó tinh chỉnh chúng cho tác vụ đích mong muốn. Tuy nhiên, việc triển khai các mô hình lớn không khả thi trên các thiết bị biên. Kỹ thuật của chúng tôi cung cấp một sửa đổi nhẹ cho quy trình tinh chỉnh tiêu chuẩn bằng cách tạo ra một mô hình nén với hiệu suất học chuyển giao tương đương trên tác vụ cụ thể. Chúng tôi chứng minh điều này trên các tác vụ thị giác và ngôn ngữ.

**Các tác vụ thị giác.** Chúng tôi xem xét tác vụ tinh chỉnh một mô hình đã được huấn luyện trước ImageNet cho một tập dữ liệu nhỏ hơn. Chúng tôi xem xét Cars196 [57] và Food101 [58] làm các tập dữ liệu đích, và so sánh với các họ mô hình MobileNetV3 và EfficientNet. Chúng tôi sử dụng các mô hình đã được huấn luyện trước ImageNet để khởi tạo. Chúng tôi vẽ các đường cong FLOP-độ chính xác trong Hình 3. Chúng tôi nén các kiến trúc MobileNetv3Large và EfficientNet-B4 và EfficientNet-B2 trong khi chuyển giao chúng đến tác vụ đích. Chúng tôi thấy rằng phương pháp của chúng tôi liên tục cải thiện so với các kiến trúc baseline qua các chế độ FLOP khác nhau. Điều này là do kỹ thuật của chúng tôi có thể cắt tỉa mô hình một cách thích ứng dựa trên độ khó của tác vụ phân loại. Trên cả hai tác vụ, chúng tôi thấy lợi ích độ chính xác 1% so với MobileNetV3 small. Lợi ích độ chính xác duy trì ở dấu chân độ trễ của MobileNetV3Large-0.75, nơi chúng tôi thấy lợi ích độ chính xác hơn 1,5% trên cả hai tập dữ liệu. Trên EfficientNet, chúng tôi thấy giảm tới 40% FLOP mà không có bất kỳ giảm độ chính xác nào trên Food101, và khoảng 20% giảm FLOP trên tập dữ liệu Cars196 cho các mô hình lớn nhất (B4). Chúng tôi cũng thấy khoảng 30% giảm FLOP trong khi duy trì hiệu suất học chuyển giao của các biến thể B1 và B0. Điều này chứng minh rằng các mô hình đã học của chúng tôi có thể mở rộng tốt hơn so với việc mở rộng heuristic được mô tả trong [2]. Xem phụ lục để biết kết quả bổ sung.

**Tinh chỉnh BERT trên GLUE.** Chúng tôi xem xét 5 tập dữ liệu của tiêu chuẩn GLUE [59] thường được sử dụng trong tài liệu, và tinh chỉnh một mô hình BERT-Base đã được huấn luyện trước với bộ điều chỉnh FLOPs của chúng tôi. Chúng tôi tham số hóa lại các ma trận trọng số của mạng feed forward của mỗi khối transformer với tham số hóa hạng thấp+thưa của chúng tôi. Chúng tôi so sánh phương pháp của mình với cắt tỉa mô hình, trong đó các số SOTA được lấy từ Hình 6 của [21], báo cáo độ chính xác tối đa giữa [60,61,62,63,64,65]. Chúng tôi cũng báo cáo hiệu suất của các baseline dựa trên chưng cất được sử dụng rộng rãi [22,23]. Hình 1 trình bày hiệu suất trung bình trên 5 tập dữ liệu, và Hình 6 trong phụ lục trình bày hiệu suất cá nhân. Trong cả hai hình này, chúng tôi vẽ FLOP tương đối của mô hình nén w.r.t BERT-base so với sự giảm độ chính xác w.r.t BERT-base (tương tự như [21]). Chúng tôi thấy rằng trên 4 trong 5 tập dữ liệu được xem xét, kỹ thuật của chúng tôi cung cấp độ chính xác cao hơn cho cùng số FLOP, cho thấy hiệu quả của phương pháp. Trên MRPC, một tập dữ liệu với rất ít mẫu, phương pháp của chúng tôi tệ hơn trên FLOP cao hơn, nhưng vượt trội hơn các baseline trong chế độ FLOP thấp.

### 4.3 Thí nghiệm Bổ sung

**Sử dụng bộ điều chỉnh độ trễ.** Trong Phương trình 6, chúng tôi đề xuất một hàm thay thế độ trễ để tối ưu hóa độ trễ suy luận thực tế trên thiết bị. Trong phần này, chúng tôi cung cấp bằng chứng thực nghiệm về hiệu quả của phương pháp này cho MobileNetv3 trên Pixel 6. Chúng tôi so sánh các đường cong độ chính xác-độ trễ của các mô hình được tạo ra bằng cách sử dụng các bộ điều chỉnh FLOPs, độ trễ (xem Hình 4). Quan sát rằng việc sử dụng bộ điều chỉnh độ trễ dẫn đến các mô hình với độ trễ nhỏ hơn và do đó sự đánh đổi độ trễ-độ chính xác tốt hơn so với việc sử dụng bộ điều chỉnh FLOP. Chúng tôi cũng thấy những mô hình này có hiệu suất tốt hơn MobileNetV3 (cải thiện độ chính xác 0,5-2% cho độ trễ tương tự), mặc dù MobileNetv3 được chế tạo thủ công để suy luận nhanh hơn trên các thiết bị di động.

**Lượng tử hóa.** Trong tập thí nghiệm này, chúng tôi xem xét phân loại CIFAR-10 và nén một CNN 3 lớp bằng lượng tử hóa. Chúng tôi sử dụng công thức lượng tử hóa được trình bày trong Bảng 1 và tìm kiếm trên lượng tử hóa {2,4,8,16} bit cho mỗi lớp. Chúng tôi so sánh với một baseline sử dụng cùng mức lượng tử hóa tại mỗi lớp. Hình 5 trình bày kết quả từ thí nghiệm này. Chi tiết về việc thực hiện có thể được tìm thấy trong phụ lục. Chúng tôi thấy rằng kỹ thuật của chúng tôi nén kích thước mô hình gần 55% mà không giảm độ chính xác (so với một mô hình với trọng số 16-bit). Kỹ thuật của chúng tôi cũng xuất ra một mô hình chính xác hơn 1,4% so với một mô hình lượng tử hóa 2-bit chỉ với 4% FLOP nhiều hơn. Trong biểu đồ bên phải trong Hình 5, chúng tôi hình dung độ rộng bit đã học của các mô hình của chúng tôi. Chúng tôi thấy rằng các lớp sau được gán độ rộng bit nhỏ hơn, cho thấy tầm quan trọng của việc học các bộ lọc biểu cảm sớm trong mạng. Các mô hình khác nhau trong biểu đồ của chúng tôi được tìm thấy bằng cách thay đổi giá trị của hệ số điều chỉnh, và do đó không cần tìm kiếm tổ hợp trên độ rộng bit.

## 5 Kết luận và Nghiên cứu Tương lai

Trong nghiên cứu này, chúng tôi đã trình bày một kỹ thuật đầu-cuối để nén mạng nơ-ron. Phương pháp của chúng tôi áp dụng cho nhiều loại khối hiệu quả bao gồm cắt tỉa, độ thưa không có cấu trúc, lượng tử hóa. Cốt lõi của thuật toán là một hàm thay thế mới cho FLOPs, độ trễ dựa trên các chuẩn ℓ1/ℓ2, và hoạt động với batchnorm, layernorm. Thuật toán của chúng tôi hiệu quả về mặt tính toán và chạy trong cùng lượng thời gian cần thiết để huấn luyện một mô hình đơn. Chúng tôi chứng minh hiệu quả của phương pháp trên các tác vụ huấn luyện trước và học chuyển giao khác nhau trên các tiêu chuẩn ngôn ngữ và thị giác tiêu chuẩn. Như một nghiên cứu tương lai, sẽ hữu ích khi kết hợp thêm các khối xây dựng hiệu quả như ma trận đường chéo khối vào framework của chúng tôi. Một hướng thú vị khác sẽ là làm cho kỹ thuật của chúng tôi nhận biết phần cứng hơn bằng cách kết hợp các tham số cấp phần cứng như tiling vào quá trình tìm kiếm của chúng tôi.
