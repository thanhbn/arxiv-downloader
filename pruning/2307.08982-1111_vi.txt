# 2307.08982.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2307.08982.pdf
# Kích thước tệp: 5242057 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
1111
Cắt tỉa Mạng nơ-ron như Quá trình Bảo tồn Phổ
SHIBO YAO, New Jersey Institute of Technology, USA
DANTONG YU, New Jersey Institute of Technology, USA
IOANNIS KOUTIS, New Jersey Institute of Technology, USA
Mạng nơ-ron đã đạt được hiệu suất đáng chú ý trong các lĩnh vực ứng dụng khác nhau. Tuy nhiên, một lượng lớn trọng số trong các mạng nơ-ron sâu đã được huấn luyện trước ngăn cản chúng được triển khai trên điện thoại thông minh và hệ thống nhúng. Việc có được các phiên bản nhẹ của mạng nơ-ron để suy luận trên các thiết bị biên là rất mong muốn. Nhiều phương pháp hiệu quả về chi phí đã được đề xuất để cắt tỉa các lớp dày đặc và tích chập phổ biến trong mạng nơ-ron sâu và chiếm ưu thế trong không gian tham số. Tuy nhiên, một nền tảng lý thuyết thống nhất cho vấn đề này chủ yếu bị thiếu. Trong bài báo này, chúng tôi xác định mối liên hệ chặt chẽ giữa việc học phổ ma trận và huấn luyện mạng nơ-ron cho các lớp dày đặc và tích chập và lập luận rằng cắt tỉa trọng số về bản chất là một quá trình thưa hóa ma trận để bảo tồn phổ. Dựa trên phân tích này, chúng tôi cũng đề xuất một thuật toán thưa hóa ma trận được thiết kế riêng cho việc cắt tỉa mạng nơ-ron mang lại kết quả cắt tỉa tốt hơn. Chúng tôi thiết kế cẩn thận và tiến hành các thí nghiệm để hỗ trợ cho các lập luận của chúng tôi. Do đó, chúng tôi cung cấp một quan điểm tổng hợp cho việc cắt tỉa mạng nơ-ron và nâng cao khả năng diễn giải của mạng nơ-ron sâu bằng cách xác định và bảo tồn các trọng số nơ-ron quan trọng.
Khái niệm CCS: •Phương pháp tính toán →Phương pháp phổ ;Thuật toán đại số tuyến tính ;Mạng nơ-ron .
Từ khóa và Cụm từ bổ sung: cắt tỉa mạng nơ-ron, khả năng diễn giải, thưa hóa ma trận, thuật toán ngẫu nhiên
Định dạng Tham chiếu ACM:
Shibo Yao, Dantong Yu, và Ioannis Koutis. 2022. Cắt tỉa Mạng nơ-ron như Quá trình Bảo tồn Phổ.
ACM Trans. Knowl. Discov. Data. 37, 4, Bài viết 111 (Tháng 8 2022), 17 trang. https://doi.org/10.1145/1122445.
1122456
1 GIỚI THIỆU
Cắt tỉa mạng nơ-ron sâu[ 17] [20] [19] [29] đã là một chủ đề thiết yếu trong những năm gần đây do nhu cầu mới nổi về việc triển khai hiệu quả các mô hình đã được huấn luyện trước trên các thiết bị nhẹ, ví dụ, điện thoại thông minh, thiết bị biên (Nvidia Jetson và Raspberry Pi), và Internet of Things (IoTs). Cắt tỉa mạng nơ-ron có từ thế kỷ trước khi những nỗ lực ban đầu được thực hiện bởi optimal brain damage [28] và optimal brain surgeon [21], và đã đạt được kết quả ấn tượng.
Một nhóm công việc lớn hơn, cụ thể là nén mạng nơ-ron [ 9][23][11][26], nhằm loại bỏ một số lượng lớn tham số mà không làm giảm đáng kể hiệu suất trong khi được hưởng lợi từ việc giảm dấu chân lưu trữ cho các mạng đã được huấn luyện trước và sức mạnh tính toán. Bởi vì

--- TRANG 2 ---
111:2 Shibo Yao, Dantong Yu, và Ioannis Koutis
các lớp dày đặc và lớp tích chập thường chiếm ưu thế về độ phức tạp không gian và thời gian trong mạng nơ-ron, nhiều phương pháp đã được đề xuất để nén hai loại mạng này. Những phương pháp này đã thể hiện sự đơn giản đáng ngạc nhiên và hiệu quả vượt trội trong nhiều tình huống cắt tỉa mạng nơ-ron.
Tuy nhiên, các công việc hiện tại chủ yếu hướng thí nghiệm và rơi vào các danh mục riêng biệt. Nghiên cứu kỹ lưỡng những công việc liên quan này đã tiết lộ một số câu hỏi. Liệu các phương pháp tương tự có thể được áp dụng cho cả lớp dày đặc và lớp tích chập? Lý thuyết và cơ chế nào biện minh cho các hoạt động cắt tỉa đã chọn và cung cấp đảm bảo hiệu suất của các mạng đã được cắt tỉa? Và chúng ta có thể có được hiểu biết tốt hơn để diễn giải các mô hình học sâu [ 35][13] từ việc nghiên cứu các chủ đề về cắt tỉa mạng nơ-ron không?
Trong bài báo này, chúng tôi sử dụng lý thuyết phổ và kỹ thuật thưa hóa ma trận để cung cấp một góc nhìn thay thế cho việc cắt tỉa mạng nơ-ron và cố gắng trả lời những câu hỏi được đề cập ở trên. Đóng góp của chúng tôi như sau:
•Chúng tôi khám phá mối quan hệ giữa huấn luyện mạng nơ-ron và quá trình học phổ của ma trận trọng số. Bằng cách theo dõi sự tiến hóa của phổ ma trận, chúng tôi hình thức hóa việc cắt tỉa mạng nơ-ron như một quá trình bảo tồn phổ.
•Chúng tôi minh họa chi tiết sự tương đồng giữa lớp dày đặc và lớp tích chập và về cơ bản cả hai đều là phép nhân ma trận. Do đó, một quan điểm thống nhất, cụ thể là thưa hóa ma trận, được đề xuất để cắt tỉa cả hai loại lớp.
•Dựa trên phân tích của chúng tôi, chúng tôi cho thấy tiềm năng tùy chỉnh thuật toán thưa hóa ma trận để cắt tỉa mạng nơ-ron tốt hơn bằng cách đề xuất một thuật toán thưa hóa được thiết kế riêng.
•Chúng tôi tiến hành toàn diện các nhiệm vụ thí nghiệm, mỗi nhiệm vụ nhắm đến một lập luận cụ thể để cung cấp hỗ trợ thực nghiệm phù hợp và vững chắc.
Dàn ý của bài báo này như sau: Phần 2 tóm tắt ngắn gọn tài liệu liên quan. Trong phần 3, chúng tôi xây dựng vấn đề cắt tỉa mạng nơ-ron và giải thích cách chúng tôi giải quyết nó từ góc độ lý thuyết phổ; Phần 4 cung cấp các kỹ thuật thưa hóa ma trận phù hợp cho việc cắt tỉa mạng nơ-ron. Trong phần 5, chúng tôi tổng quát hóa lược đồ phân tích sang các lớp tích chập. Phần 6 trình bày nghiên cứu thực nghiệm chi tiết; và cuối cùng các kết luận được đưa ra.

--- TRANG 3 ---
Cắt tỉa Mạng nơ-ron như Quá trình Bảo tồn Phổ 111:3
của công việc cụ thể là nén mạng nơ-ron [ 9]. Xấp xỉ hạng thấp được áp dụng cho ma trận trọng số hoặc tensor [ 24] để giảm yêu cầu lưu trữ hoặc thời gian suy luận cho các mô hình lớn đã được huấn luyện trước. Một số công việc khác về nén mạng nơ-ron cũng bao gồm chia sẻ trọng số [7] và hash trick [8], nơi họ cũng xem xét vấn đề trong miền tần số.

2.2 Thưa hóa Ma trận
Thưa hóa ma trận quan trọng trong nhiều vấn đề số, ví dụ xấp xỉ hạng thấp, lập trình bán xác định và hoàn thành ma trận, tồn tại rộng rãi trong các vấn đề khai phá dữ liệu và học máy. Thưa hóa ma trận là việc giảm số lượng mục nhập khác không trong ma trận mà không thay đổi phổ của nó. Vấn đề gốc là NP-khó [ 31][18]. Nghiên cứu về các giải pháp xấp xỉ cho vấn đề này được khởi xướng bởi [ 3], và mở rộng thêm trong [ 2] [4] [1] [33] [12]. Một nghiên cứu mở rộng về giới hạn lỗi được thực hiện trong [16].
Vì phổ của ma trận thưa hóa không lệch đáng kể so với ma trận gốc, phục vụ như một toán tử tuyến tính, ma trận giữ được chức năng của nó, tức là 𝐴∈R𝑚×𝑛là một ánh xạ R𝑚→R𝑛. Chúng ta có thể định nghĩa quá trình thưa hóa ma trận như bài toán tối ưu hóa sau:
min∥˜𝐴∥0
s.t.∥𝐴−˜𝐴∥≤𝜖(1)
trong đó 𝐴 là ma trận gốc, ˜𝐴 là ma trận thưa hóa, ∥·∥ 0 là 0-norm bằng số lượng mục nhập khác không trong ma trận, ∥·∥ biểu thị chuẩn ma trận, 𝜖≥0 là dung sai lỗi.
Trong thưa hóa ma trận, chúng ta thường sử dụng chuẩn phổ (2-norm) ∥·∥ 2 và chuẩn Frobenius (F-norm)∥·∥𝐹 để đo độ lệch của ma trận thưa hóa so với ma trận gốc.

3 XÂY DỰNG VẤN ĐỀ
Cho một lớp dày đặc z𝑡=𝜎(z𝑇𝑡−1𝐴+b), trong đó z𝑡−1∈R𝑚 là tín hiệu đầu vào, z𝑡∈R𝑛 là tín hiệu đầu ra, 𝐴∈R𝑚×𝑛 là ma trận trọng số, b∈R𝑛 là bias, 𝜎 biểu thị một hàm kích hoạt nào đó, chúng ta mong muốn có được một phiên bản thưa của 𝐴 được ký hiệu bởi ˜𝐴 sao cho 𝐴 và ˜𝐴 có cấu trúc phổ tương tự và z𝑇𝑡−1˜𝐴 gần với z𝑇𝑡−1𝐴 nhất có thể.
Tương tự đối với một tích chập 𝑧𝑡=𝑇∗𝑧𝑡−1 chúng ta muốn tìm một phiên bản thưa của 𝑇 sao cho kết quả tích chập gần nhất có thể, trong đó 𝑇 có thể là vector, ma trận hoặc mảng bậc cao hơn tùy thuộc vào bậc của tín hiệu đầu vào và số kênh đầu ra. Bằng sự gần gũi, chúng ta sử dụng chuẩn làm thước đo. Chúng ta bắt đầu từ việc điều tra các lớp dày đặc và sau đó tổng quát hóa trên các lớp tích chập.

3.1 Huấn luyện Mạng nơ-ron như Quá trình Học Phổ
Trong một lớp dày đặc, chúng ta tập trung vào phần z𝑇𝐴 vì nó chứa hầu hết các tham số. Mạng nơ-ron về cơ bản là một mô phỏng hàm học một số tính năng nhân tạo, được thực hiện bằng ánh xạ tuyến tính, kích hoạt phi tuyến và một số đơn vị tùy chỉnh khác (ví dụ đơn vị tái phát). Đối với ánh xạ tuyến tính, phân tích thường được thực hiện trên miền phổ.
Nhớ lại rằng Phân tách Giá trị Đơn (SVD) là tối ưu dưới cả chuẩn phổ [ 14] và chuẩn Frobenius [32]. Ma trận trọng số 𝐴∈R𝑚×𝑛 như một toán tử tuyến tính có thể được phân tách như
𝐴=𝑈Σ𝑉𝑇=𝑚𝑖𝑛(𝑚,𝑛)∑︁𝑖=1𝜎𝑖u𝑖v𝑇𝑖
trong đó 𝑈=[u1,u2,...,u𝑚]∈R𝑚×𝑚 là ma trận đơn trị trái, 𝑉=[v1,v2,...,v𝑛]∈R𝑛×𝑛 là ma trận đơn trị phải và Σ chứa các giá trị đơn 𝑑𝑖𝑎𝑔(𝜎1,𝜎2,...,𝜎𝑛) theo thứ tự không tăng.
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- TRANG 4 ---
111:4 Shibo Yao, Dantong Yu, và Ioannis Koutis
Lưu ý rằng một tín hiệu đầu vào z∈R𝑚 có thể được viết thành tổ hợp tuyến tính của u𝑖, tức là z𝑖=Í𝑚𝑖𝑐𝑖u𝑖 trong đó 𝑐𝑖 là các hệ số. Do đó, ánh xạ từ z∈R𝑚 tới z′∈R𝑛 là
z′=z𝑇𝐴=𝑚∑︁𝑖=1𝑐𝑖u𝑇𝑖𝑚𝑖𝑛(𝑚,𝑛)∑︁𝑗=1𝜎𝑗u𝑗v𝑇𝑗=𝑚𝑖𝑛(𝑚,𝑛)∑︁𝑗=1𝑐𝑗𝜎𝑗v𝑗
trong đó 𝜎𝑗 được sắp xếp theo thứ tự không tăng, vì u𝑇𝑖u𝑗=0,∀𝑖≠𝑗 và u𝑇𝑖u𝑖=1,∀𝑖.
Chúng tôi đặc biệt quan tâm đến việc tìm hiểu cách phổ Σ, tức là các giá trị đơn, của ánh xạ tuyến tính tiến hóa trong quá trình huấn luyện mạng nơ-ron. Trên thực tế, nghiên cứu thực nghiệm của chúng tôi cho thấy quá trình huấn luyện mạng nơ-ron là một quá trình học phổ. Để điều tra cấu trúc phổ, chúng tôi thiết kế nhiệm vụ sau:
Nhiệm vụ 1 : kiểm tra phổ và chuẩn của ma trận trọng số lớp dày đặc và cách chúng thay đổi trong quá trình huấn luyện mạng nơ-ron.

[Hình 1. (Nhiệm vụ1) Chuẩn Ma trận trong LeNet ổn định trong quá trình huấn luyện. trục y biểu thị chuẩn ma trận và trục x biểu thị epoch huấn luyện.]

[Hình 2. (Nhiệm vụ1) Chuẩn Ma trận trong VGG19 ổn định trong quá trình huấn luyện.]

3.2 Cắt tỉa Mạng nơ-ron như Quá trình Bảo tồn Phổ
Theo logic này, nếu huấn luyện mạng nơ-ron là một quá trình học phổ, khi chúng ta thực hành cắt tỉa mạng nơ-ron, chúng ta sẽ muốn bảo tồn phổ để bảo tồn hiệu suất mạng nơ-ron. Nói cách khác, chúng ta muốn có được một ˜𝐴 thưa có các giá trị đơn tương tự như 𝐴. Làm thế nào để đo lường mức độ tốt của việc bảo tồn phổ? Chúng ta có thể sử dụng chuẩn phổ

--- TRANG 5 ---
Cắt tỉa Mạng nơ-ron như Quá trình Bảo tồn Phổ 111:5
(2-norm)∥𝐴∥2=𝜎1 là giá trị đơn lớn nhất vì chúng ta quan tâm đến thành phần chính thống trị, và chuẩn Frobenius (F-norm)
∥𝐴∥𝐹=(𝑚∑︁𝑖=1𝑛∑︁𝑗=1𝐴2𝑖𝑗)1/2=(𝑇𝑟(𝐴𝑇𝐴))1/2=(𝑚𝑖𝑛(𝑚,𝑛)∑︁𝑖=1𝜎2𝑖)1/2
thường được coi là tổng hợp của toàn bộ phổ. Lưu ý rằng ∥𝐴∥2≤∥𝐴∥𝐹 và ∥𝐴∥𝐹≤√︁𝑚𝑖𝑛(𝑚,𝑛)∥𝐴∥2.
Do đó mục tiêu là tìm một ˜𝐴 thưa sao cho ∥𝐴−˜𝐴∥2≤𝜖 hoặc ∥𝐴−˜𝐴∥𝐹≤𝜖. Để chỉ ra rằng quá trình cắt tỉa là một quá trình bảo tồn phổ, chúng tôi thiết kế nhiệm vụ sau.
Nhiệm vụ2 : áp dụng ngưỡng dựa trên độ lớn ở các mức độ thưa khác nhau và kiểm tra mối quan hệ giữa phổ ma trận trọng số kết quả và hiệu suất của các mạng nơ-ron đã được cắt tỉa.

[Hình 3. (Nhiệm vụ2)∥𝐴−˜𝐴∥2 và ∥𝐴−˜𝐴∥𝐹 so với độ chính xác mạng nơ-ron khi độ thưa tăng (LeNet trên MNIST). Khi chúng ta tăng độ thưa, 2-norm∥𝐴−˜𝐴∥2 và 𝐹-norm∥𝐴−˜𝐴∥𝐹 tăng trong khi hiệu suất mạng nơ-ron giảm tương ứng. trục y biểu thị độ chính xác và trục x biểu thị chuẩn ma trận.]

3.3 Cắt tỉa và Huấn luyện lại Lặp lại
Từ góc độ tối ưu hóa, mạng nơ-ron đã được huấn luyện trước đạt được tối ưu cục bộ thỏa mãn. Một khi ma trận trọng số được thưa hóa, phổ ở một mức độ nào đó lệch và tính tối ưu không còn đúng. Chúng ta muốn huấn luyện lại mạng nơ-ron sao cho nó trở về tối ưu cục bộ thỏa mãn và hiệu suất được bảo tồn. Rõ ràng chúng ta không muốn mạng nơ-ron lệch khỏi tối ưu cục bộ quá xa, nếu không sẽ khó trở về tối ưu do quá trình tối ưu hóa không lồi của mạng nơ-ron. Do đó, cắt tỉa và huấn luyện lại lặp lại đã là một thực hành phổ biến cho việc cắt tỉa mạng nơ-ron. Chúng tôi cũng thiết kế một nhiệm vụ để elaborat từ góc độ phổ.
Nhiệm vụ 3: Kiểm tra phổ ma trận trọng số trong mỗi lần lặp cắt tỉa, tức là phổ ma trận trọng số sau khi cắt tỉa và sau khi huấn luyện lại tương ứng, và so sánh hiệu suất mạng nơ-ron tương ứng. Chúng tôi cũng bao gồm so sánh với cắt tỉa một lần.

--- TRANG 6 ---
111:6 Shibo Yao, Dantong Yu, và Ioannis Koutis

[Hình 4. (Nhiệm vụ2)∥𝐴−˜𝐴∥2 và ∥𝐴−˜𝐴∥𝐹 so với độ chính xác mạng nơ-ron khi độ thưa tăng (VGG19 trên CIFAR10). Nhãn trục giống như trong Hình 3.]

4 THUẬT TOÁN THƯA HÓA MA TRẬN
Trong phần trước, chúng tôi xác định mối quan hệ giữa cắt tỉa mạng nơ-ron (loại bỏ tham số/kết nối) và quá trình bảo tồn phổ. Thưa hóa ma trận đóng vai trò chính trong quá trình bảo tồn phổ và sẽ hướng dẫn quá trình cắt tỉa mạng. Trong phần này, chúng tôi trình bày các kỹ thuật thực tế của thưa hóa ma trận.

4.1 Ngưỡng Dựa trên Độ lớn
Cắt tỉa mạng nơ-ron dựa trên độ lớn đã thu hút nhiều sự chú ý và cho thấy sự đơn giản đáng ngạc nhiên và hiệu quả vượt trội. Trong bối cảnh thưa hóa ma trận, đây là một phương pháp trực tiếp, cụ thể là thưa hóa ma trận dựa trên độ lớn hoặc hard thresholding. Cho một ma trận 𝐴, gọi ˜𝐴 là sparsifier của nó. Theo từng mục ta có
˜𝐴𝑖𝑗=(
𝐴𝑖𝑗|𝐴𝑖𝑗|>𝑡
0𝑒𝑙𝑠𝑒

--- TRANG 7 ---
Cắt tỉa Mạng nơ-ron như Quá trình Bảo tồn Phổ 111:7

[Hình 5. (Nhiệm vụ3) Cắt tỉa lặp lại cho LeNet trên MNIST. Số phía trên mỗi cột biểu thị phần trăm tham số còn lại trong ma trận trọng số. Bên phải nhất là kết quả cắt tỉa một lần. Chúng ta có thể thấy rằng phổ lệch sau khi cắt tỉa (màu cam) và có xu hướng phục hồi sau khi huấn luyện lại (màu xanh). Một khi độ thưa đạt đến một điểm nhất định, phổ sụp đổ tương ứng với việc hiệu suất mô hình giảm đáng kể.]

[Hình 6. (Nhiệm vụ3) Cắt tỉa lặp lại cho LeNet với Batch Normalization trên MNIST. So với LeNet không có Batch Normalization, hành vi phục hồi phổ không đáng kể.]

Sự thật. Ngưỡng dựa trên độ lớn luôn đạt được tối ưu thưa hóa theo F-norm.
Sự thật này có thể được xác minh một cách tầm thường vì việc sử dụng |𝐴𝑖𝑗| và sử dụng 𝐴2𝑖𝑗 (mà F-norm dựa trên) là tương đương về mặt quyết định các mục nhập nhỏ trong ma trận. Tuy nhiên việc loại bỏ các mục nhập nhỏ không phải lúc nào cũng đảm bảo kết quả thưa hóa tối ưu theo 2-norm. Và trong nhiều tình huống, chúng ta quan tâm nhiều hơn đến giá trị đơn thống trị thay vì toàn bộ phổ.

4.2 Thuật toán Ngẫu nhiên
Trong thưa hóa ma trận ngẫu nhiên, mỗi mục được lấy mẫu theo một phân phối nào đó một cách độc lập và sau đó được tái chia tỷ lệ. Ví dụ mỗi mục được lấy mẫu theo phân phối Berboulli, và chúng ta hoặc đặt nó về không hoặc tái chia tỷ lệ nó.
˜𝐴𝑖𝑗=(
𝐴𝑖𝑗/𝑝𝑖𝑗𝑝𝑖𝑗
0 1−𝑝𝑖𝑗

--- TRANG 8 ---
111:8 Shibo Yao, Dantong Yu, và Ioannis Koutis
trong đó 𝑝𝑖𝑗 có thể là hằng số hoặc tương quan dương với độ lớn của mục. Định lý sau cung cấp biện minh cho loại thưa hóa ma trận này.
Định lý 4.1. Một ma trận trong đó mỗi mục được lấy mẫu từ phân phối có phương sai bị chặn và trung bình bằng không sở hữu phổ yếu với xác suất lớn.
Bằng phổ yếu, có nghĩa là chuẩn ma trận nhỏ. Để cụ thể hơn, vì chuẩn ma trận là một thước đo và bất đẳng thức tam giác áp dụng, chúng ta có
∥𝐴∥≤∥ ˜𝐴∥+∥𝐴−˜𝐴∥
. Chúng ta cần chỉ ra rằng 𝑁=𝐴−˜𝐴 thuộc danh mục ma trận được mô tả trong Định lý 4.1.
Vì
𝐸(𝑁𝑖𝑗)=𝐸(𝐴𝑖𝑗−˜𝐴𝑖𝑗)=𝐴𝑖𝑗−𝐴𝑖𝑗/𝑝𝑖𝑗·𝑝𝑖𝑗=0
và
𝑣𝑎𝑟(𝑁𝑖𝑗)=𝑣𝑎𝑟(˜𝐴𝑖𝑗)=(𝐴𝑖𝑗/𝑝𝑖𝑗)2·𝑝𝑖𝑗=𝐴2𝑖𝑗/𝑝𝑖𝑗
, miễn là 𝐴2𝑖𝑗/𝑝𝑖𝑗 bị chặn trên, điều này đúng hầu hết thời gian, 𝑣𝑎𝑟(𝑁𝑖𝑗) bị chặn. Do đó thưa hóa ma trận ngẫu nhiên có thể đảm bảo giới hạn lỗi.

4.3 Tùy chỉnh Thuật toán Thưa hóa Ma trận cho Cắt tỉa Mạng nơ-ron
Trong phần này, chúng tôi đề xuất một thuật toán thưa hóa ma trận tùy chỉnh để chỉ ra tiềm năng thiết kế một quá trình bảo tồn phổ tốt hơn trong việc cắt tỉa mạng nơ-ron. Chúng tôi không có ý định trình bày một thuật toán cắt tỉa mạng nơ-ron tiên tiến mới. Có hai điểm quan trọng trong thuật toán đề xuất của chúng tôi: cắt ngắn và lấy mẫu dựa trên các thành phần chính của SVD cắt ngắn rõ ràng. Theo hiểu biết tốt nhất của chúng tôi, lấy mẫu dựa trên xác suất tỷ lệ với các thành phần chính được sử dụng lần đầu tiên trong thiết kế thưa hóa ma trận cho việc cắt tỉa mạng nơ-ron.
Đầu tiên, chúng tôi áp dụng thủ thuật cắt ngắn phổ biến trong công việc hiện tại. Như được chỉ ra rõ ràng bởi [1], phổ của ma trận ngẫu nhiên 𝑁=𝐴−˜𝐴 được xác định bởi giới hạn phương sai của nó. Thường thì phương sai càng lớn, phổ của ma trận ngẫu nhiên càng mạnh. Các công việc hiện tại đã tận dụng phát hiện này và đề xuất cắt ngắn [ 4][12] trong thưa hóa, tức là đặt các mục nhập nhỏ về không trong khi để các mục nhập lớn như cũ và lấy mẫu trên những mục còn lại.
˜𝐴𝑖𝑗= 
𝐴𝑖𝑗 |𝐴𝑖𝑗|>𝑡
0 𝑝𝑖𝑗<𝑐
𝐴𝑖𝑗/𝑝𝑖𝑗·𝐵𝑒𝑟𝑛(𝑝𝑖𝑗)𝑒𝑙𝑠𝑒
trong đó 𝑝𝑖𝑗∝|𝐴𝑖𝑗|, 𝑡 được quyết định bởi quantile (để lại các mục nhập lớn như cũ), và 𝑐, ngưỡng dưới cho việc đặt trọng số về không, như một hằng số có thể được đặt thủ công, và 𝐵𝑒𝑟𝑛(·) biểu thị phân phối Bernoulli.
Thứ hai, thay vì lấy mẫu dựa trên xác suất được tính từ độ lớn của mục ma trận gốc, chúng tôi thực hiện lấy mẫu dựa trên xác suất được tính từ độ lớn mục ma trận thành phần chính với một chút thỏa hiệp về độ phức tạp, để bảo tồn tốt hơn các giá trị đơn thống trị. Thưa hóa ma trận ban đầu được đề xuất cho xấp xỉ hạng thấp nhanh trên các ma trận rất lớn, do sự thật rằng độ thưa tăng tốc phép nhân ma trận-vector trong lặp lũy thừa. Về cơ bản, chúng ta mong muốn tìm bản phác thảo thưa ˜𝐴 của 𝐴 bảo tồn tốt các giá trị đơn thống trị. Điều này phù hợp với mục tiêu cắt tỉa mạng nơ-ron theo lớp từ quan điểm bảo tồn phổ – chúng ta mong muốn bảo tồn các giá trị đơn thống trị, dựa trên sự thật rằng chúng ta thường coi thông tin nằm trong miền tần số thấp trong khi nhiễu ở miền tần số cao. Sự khác biệt chính là ma trận trọng số trong mạng nơ-ron, hoặc từ các lớp dày đặc hoặc các lớp tích chập, thường

--- TRANG 9 ---
Cắt tỉa Mạng nơ-ron như Quá trình Bảo tồn Phổ 111:9
không quá lớn, và do đó SVD rõ ràng hoặc SVD cắt ngắn trên chúng khá có thể chi trả được. Một khi chúng ta có quyền truy cập vào các thành phần chính của ma trận trọng số, chúng ta có thể bảo tồn chúng tốt hơn trong quá trình thưa hóa. Lưu ý rằng việc bảo tồn các giá trị đơn thống trị là một phương pháp hài hòa giữa việc bảo tồn 2-norm và F-norm, vì ∥𝐴∥2=lim𝑝→∞(Í𝑖𝜎𝑝𝑖)1/𝑝 và ∥𝐴∥𝐹=lim𝑝→2(Í𝑖𝜎𝑝𝑖)1/𝑝.
Phần quan trọng là tìm xấp xỉ hạng thấp 𝐵 của 𝐴, trong đó 𝐵=Í𝐾𝑖=1𝜎𝑖u𝑖v𝑇𝑖 và 𝜎𝑖u𝑖v𝑇𝑖 từ SVD trên A. Chúng ta đặt xác suất lấy mẫu theo từng mục dựa trên |𝐵𝑖𝑗|, tức là 𝑝𝑖𝑗∝|𝐵𝑖𝑗|. Alg 1 trình bày thuật toán thưa hóa. Hàm phân vùng là hàm được sử dụng trong quicksort.

Thuật toán 1: Thưa hóa
đầu vào : (𝐴,𝑐,𝑞,𝐾)
đầu ra: ˜𝐴
; //𝑞: quantile phía trên giữ nguyên không thay đổi
1𝐵=truncated-SVD( 𝐴,𝐾); ; //𝐵: xấp xỉ hạng thấp của 𝐴
2𝑚,𝑛 = shape của 𝐵;
3𝑡= partition({|𝐵𝑖𝑗|}, int(𝑚×𝑛×𝑞));
4for𝑖←1 to 𝑚do
5 for𝑗←1 to 𝑛do
6 if|𝐵𝑖𝑗|<𝑡 then
7 𝑝𝑖𝑗=(𝐵𝑖𝑗/𝑡)2;
8 if𝑝𝑖𝑗<𝑐 then
9 𝐴𝑖𝑗=0;
10 else
11 𝐴𝑖𝑗=𝐴𝑖𝑗/𝑝𝑖𝑗·𝐵𝑒𝑟𝑛(𝑝𝑖𝑗);
12 end
13end

[Hình 7. (Nhiệm vụ4) Hiệu suất Kiểm tra Mạng Đã Cắt tỉa được cung cấp bởi Ngưỡng Dựa trên Độ lớn (màu cam) so với Thuật toán1 (màu xanh). 𝑞 càng lớn, độ thưa càng lớn.]

Chúng ta cần chứng minh rằng thuật toán thưa hóa đề xuất bảo tồn các giá trị đơn thống trị tốt hơn và cải thiện hiệu suất tổng quát của mạng đã cắt tỉa. Chúng tôi đề xuất nhiệm vụ sau để kiểm tra xem nó có cải thiện dựa trên phân tích của chúng tôi không.
Nhiệm vụ 4 Áp dụng thuật toán trên vào thưa hóa ma trận trọng số theo lớp VGG19, so sánh hiệu suất tổng quát của mạng đã cắt tỉa và mạng đã cắt tỉa được cung cấp bởi ngưỡng ở cùng mức độ thưa.

--- TRANG 10 ---
111:10 Shibo Yao, Dantong Yu, và Ioannis Koutis
Ở đây chúng tôi cung cấp một chứng minh cấp cao về lỗi thưa hóa bị chặn trên. Gọi 𝐷 biểu thị bản phác thảo thưa được tạo bằng cách đặt các mục nhập nhỏ nhất trong 𝐴 về 0 và ˜𝐴 như thường lệ là kết quả thưa hóa cuối cùng. Từ Sự thật 4.1 chúng ta biết rằng 𝐷 là bản phác thảo tối ưu của 𝐴 theo F-norm, tức là ∥𝐴−𝐷∥𝐹=𝜖∗. Dựa trên Định lý 4.1 và minh họa của nó, chúng ta biết rằng 𝑁=˜𝐴−𝐷 thỏa mãn điều kiện trung bình bằng không và phương sai bị chặn. Do đó ∥˜𝐴−𝐷∥𝐹≤𝜖0. Do đó nếu chúng ta áp dụng đẳng thức tam giác cho chuẩn ma trận là một thước đo,
∥𝐴−˜𝐴∥𝐹≤∥𝐴−𝐷∥𝐹+∥𝐷−˜𝐴∥𝐹≤𝜖∗+𝜖0.
Một số kỹ thuật khác, ví dụ lượng tử hóa[ 17][19], có thể được sử dụng cùng với thưa hóa để nén thêm ma trận và mạng nơ-ron. Về cơ bản chúng cũng là các kỹ thuật bảo tồn phổ [3][4].

5 TỔNG QUÁT HÓA SANG TÍCH CHẬP

[Hình 8. Tích chập như Phép nhân Ma trận Dày đặc]
Nhiều tài liệu mở rộng lập luận rằng nén lớp tích chập có thể được hình thức hóa như các vấn đề đại số tensor [ 26][11] [23][30][37]. Tuy nhiên, việc giải thích cắt tỉa lớp tích chập từ quan điểm ma trận có lợi thế vì đại số tuyến tính có nhiều tính chất đẹp không có trong đại số đa tuyến. Chúng ta muốn hỏi: liệu chúng ta vẫn có thể cung cấp hỗ trợ lý thuyết cho việc cắt tỉa lớp tích chập bằng đại số tuyến tính mà chúng ta đã thảo luận cho đến nay không?

5.1 Cắt tỉa trên Bộ lọc Tích chập
Trong phần này chúng tôi phát biểu và minh họa sự thật sau.
Sự thật. Tích chập rời rạc trong mạng nơ-ron có thể được biểu diễn bằng tích vô hướng giữa hai ma trận dày đặc.
Để thấy điều này, giả sử chúng ta có một lớp tích chập với kích thước tín hiệu đầu vào 𝑊×𝐻 như độ rộng nhân chiều cao, và với 𝐶 kênh đầu vào và 𝑂 kênh đầu ra. Ở đây chúng ta xem xét tích chập 2-d trên tín hiệu. Kernel có kích thước 𝐶×𝐾×𝐾 và có 𝑂 kernel như vậy. Để đơn giản trong ký hiệu, giả sử bước tiến là 1, half-padding được áp dụng và không có dilation (đối với 𝑊 và 𝐻 chẵn, thiết lập trên dẫn đến tín hiệu đầu ra có kích thước 𝑊×𝐻 như độ rộng nhân chiều cao). Tích chập 2-d có nghĩa là kernel di chuyển theo hai hướng. Sự thật 5.1 đã được sử dụng để tối ưu hóa cài đặt cấp thấp hơn của CNN trên phần cứng [ 6][10]. Ở đây chúng tôi tận dụng ý tưởng để thống nhất việc cắt tỉa mạng nơ-ron trên các lớp dày đặc và lớp tích chập với thưa hóa ma trận.
Hãy tập trung vào một kênh đầu ra duy nhất, một bước của hoạt động tích chập là tổng của tích theo từng phần tử của hai mảng bậc cao hơn, tức là kernel 𝐺∈R𝐶×𝐾×𝐾 và

--- TRANG 11 ---
Cắt tỉa Mạng nơ-ron như Quá trình Bảo tồn Phổ 111:11
trường tiếp nhận của tín hiệu cùng kích thước 𝑋∈R𝐶×𝐾×𝐾. Lưu ý rằng việc lấy tổng của tích theo từng phần tử tương đương với tích vô hướng vector. Do đó nếu chúng ta mở ra kernel cho một kênh đầu ra thành một vector và sắp xếp lại trường tiếp nhận của tín hiệu tương ứng thành một vector khác, một bước tích chập đơn có thể được xử lý như tích vô hướng hai vector, tức là 𝐺∗𝑋=g𝑇x trong đó g,x∈R𝐶𝐾𝐾. Vì chúng ta có tổng cộng 𝑂 kênh đầu ra, có 𝑂 kernel cùng kích thước như vậy. Tất cả chúng được mở ra, sau đó chúng ta có thể chuyển đổi tích chập thành tích ma trận 𝑍𝑇𝐴, trong đó 𝐴∈R𝐶𝐾𝐾×𝑂 là các kernel và 𝑍∈R𝐶𝐾𝐾×𝑊𝐻 là các tín hiệu đầu vào được sắp xếp lại. Và do đó tín hiệu đầu ra 𝑌∈R𝑊𝐻×𝑂 (như đã đề cập trước đó, bước tiến 1, half padding và không dilation dẫn đến tín hiệu đầu vào và tín hiệu đầu ra có cùng hình dạng). Hình 8 hình dung tích chập như phép nhân ma trận.
Biểu diễn phép nhân ma trận của tích chập được thảo luận ở trên tổng quát hóa cho bất kỳ thiết lập tích chập nào khác. Cũng lưu ý rằng cách chúng ta mở ra các bộ lọc không ảnh hưởng đến phổ của ma trận kết quả, vì hoán vị hàng và cột không thay đổi phổ ma trận. Do đó, tất cả các phân tích dựa trên đại số tuyến tính đơn giản mà chúng ta đã thảo luận cho đến nay tổng quát hóa sang việc cắt tỉa lớp tích chập. Để xác minh các phân tích của chúng tôi,
Nhiệm vụ1,2,3 cũng sẽ được tiến hành trên các lớp tích chập. Khi chúng ta nói ma trận trọng số trong bối cảnh tích chập, nó đề cập đến ma trận được mở ra từ mảng bậc cao hơn tích chập theo cách được mô tả trong Hình 8.

5.2 Cắt tỉa Kênh Bộ lọc Tích chập

[Hình 9. (Nhiệm vụ5) Cắt tỉa Kênh VGG19 dựa trên Í𝑖|𝑇𝑖|. trục y biểu thị ∥˜𝐴∥𝐹 và trục x biểu thị Í𝑖|𝑇𝑖|. Í𝑖|𝑇𝑖| càng nhỏ, ∥𝐴−˜𝐴∥𝐹 càng nhỏ, ∥˜𝐴∥𝐹 càng lớn, phổ được bảo tồn càng tốt. Do đó phân tích của chúng tôi làm cầu nối khoảng cách giữa Í𝑖|𝑇𝑖| nhỏ và việc bảo tồn hiệu suất mạng nơ-ron tốt.]
Cắt tỉa theo từng mục nhập gần như luôn dẫn đến độ thưa không có cấu trúc đòi hỏi thiết kế cấu trúc dữ liệu cụ thể trong triển khai mạng để nhận ra việc giảm độ phức tạp từ cắt tỉa.

--- TRANG 12 ---
111:12 Shibo Yao, Dantong Yu, và Ioannis Koutis
Do đó mong muốn cắt tỉa toàn bộ kênh từ các lớp tích chập để đạt hiệu quả cao hơn. Có một công việc quan trọng khác về cắt tỉa kênh [ 29]. Phương pháp là lấy Í𝑖𝑗𝑘|𝑇𝑖𝑗𝑘| nhỏ trong đó 𝑇 biểu thị bộ lọc cho một kênh cụ thể. Điều này tương đương với loại bỏ một cột trong 𝐴 mà chúng ta vừa thảo luận với các giá trị độ lớn nhỏ. Nó cũng là một quá trình bảo tồn phổ vì Í𝑖𝑗𝑘|𝑇𝑖𝑗𝑘| khá gần với Í𝑖𝑗𝑘(𝑇𝑖𝑗𝑘)2 mà F-norm dựa trên. Do đó cắt tỉa toàn bộ bộ lọc với Í𝑖𝑗𝑘|𝑇𝑖𝑗𝑘| nhỏ là để bảo tồn F-norm của ma trận tích chập mà chúng ta đã thảo luận trong phần trước. Để kiểm tra mối quan hệ giữa Í𝑖𝑗𝑘|𝑇𝑖𝑗𝑘| và F-norm của ˜𝐴, chúng tôi đề xuất nhiệm vụ sau.
Nhiệm vụ5 Cắt tỉa các bộ lọc khác nhau và kiểm tra mối quan hệ giữa Í𝑖𝑗𝑘|𝑇𝑖𝑗𝑘| và F-norm ma trận tích chập kết quả ∥˜𝐴∥𝐹.

6 CHI TIẾT NGHIÊN CỨU THỰC NGHIỆM
Trong phần này, chúng tôi trình bày chi tiết nhiệm vụ đề xuất và kết quả. Các thí nghiệm chủ yếu dựa trên LeNet [ 27] trên MNIST và VGG19 [ 36] trên bộ dữ liệu CIFAR10 [ 25]. Chúng tôi huấn luyện các mạng nơ-ron từ đầu dựa trên cài đặt PyTorch [ 34] chính thức. Sau đó chúng tôi tiến hành các thí nghiệm dựa trên các mạng nơ-ron đã được huấn luyện trước.
Chúng tôi huấn luyện LeNet với số epoch giảm là 10. Tất cả các thiết lập siêu tham số khác là những thiết lập được sử dụng trong cài đặt gốc của ví dụ PyTorch. VGG19 được huấn luyện với thiết lập siêu tham số sau: batch size 128, momentum 0.9, weight decay 5𝑒−4, và learning rate 0.1 cho 50 epoch, 0.01 cho 50 epoch, và 0.001 cho 50 epoch nữa. Độ chính xác kiểm tra cuối cùng cho LeNet trên MNIST và VGG19 trên FICAR10 lần lượt là 99.14% và 92.66%. Chúng tôi lưu ma trận trọng số lớp cho mỗi epoch huấn luyện, bao gồm hai lớp kết nối đầy đủ và hai lớp tích chập cho LeNet và bốn lớp tích chập đầu tiên cho VGG19 (do hạn chế trong không gian báo cáo). Sau đó chúng tôi nghiên cứu sự tiến hóa của 2-norm và F-norm của ma trận trọng số trong quá trình huấn luyện.
Trong Hình 1 và 2, chúng ta quan sát thấy 2-norm và F-norm của một ma trận trọng số cụ thể thay đổi nhanh ở đầu huấn luyện và có xu hướng ổn định khi huấn luyện tiếp tục. Quan sát này cung cấp bằng chứng cụ thể rằng huấn luyện mạng về cơ bản là một quá trình học phổ. Lưu ý rằng phổ ban đầu không nhất thiết phẳng (xem Hình 10 và 11 trong Phụ lục), mà phụ thuộc vào khởi tạo. Sự ổn định cũng có giải thích từ góc độ tối ưu hóa: khi huấn luyện tiến triển, chúng ta bị mắc kẹt trong tối ưu cục bộ thỏa mãn và gradient gần như bằng không cho các lớp khi áp dụng quy tắc chuỗi, có nghĩa là ma trận trọng số không được cập nhật đáng kể.
Nhiệm vụ 2 Chúng tôi điều tra mối quan hệ giữa bảo tồn phổ và hiệu suất của mạng nơ-ron đã được cắt tỉa. Thưa hóa ma trận (hard thresholding) được sử dụng để cắt tỉa mạng nơ-ron. Chúng tôi thay đổi phần trăm bảo tồn tham số từ 20% đến 1% để có được các độ thưa khác nhau (càng thưa, ∥𝐴−˜𝐴∥2 và ∥𝐴−˜𝐴∥𝐹 càng lớn). Chúng tôi cắt tỉa các lớp khác nhau trong LeNet và VGG19 đã được huấn luyện trước và kiểm tra hiệu suất của chúng mà không huấn luyện lại.
Hình 3 và hình 4 cho thấy khi ∥𝐴−˜𝐴∥2 tăng, hiệu suất mạng nơ-ron xấu đi gần như đơn điệu. Điều này cũng đúng cho ∥𝐴−˜𝐴∥𝐹. Phát hiện này xác nhận rằng phổ của ma trận trọng số được bảo tồn tốt hơn trong quá trình cắt tỉa, hiệu suất mạng nơ-ron đã được cắt tỉa được bảo tồn tốt hơn. Cũng thú vị khi lưu ý rằng các quan sát thí nghiệm (điểm dữ liệu) có xu hướng tập trung ở phía trên bên trái, thay vì phía dưới bên phải của đồ thị, và có sự giảm mạnh trong mỗi đồ thị. Mẫu này chỉ ra rằng phổ và hiệu suất mạng nơ-ron sụp đổ khi độ thưa của bản phác thảo ma trận vượt qua một điểm nhất định.
Nhiệm vụ 3 Chúng tôi cũng kiểm tra phổ của ma trận trọng số trong quá trình cắt tỉa và huấn luyện lại lặp lại. Cụ thể, chúng tôi áp dụng cắt tỉa dựa trên độ lớn (thưa hóa ma trận hard thresholding) để giữ lại 30% tham số trong mỗi lần lặp. Chúng tôi tiến hành thí nghiệm trên lớp tích chập thứ hai và lớp dày đặc đầu tiên chứa hầu hết tham số trong LeNet. Một khi các tham số được cắt tỉa, chúng tôi áp dụng mask trên ma trận đó để cố định các giá trị không tương ứng với các kết nối đã được cắt tỉa giữa các nơ-ron. Chúng tôi cũng áp dụng mask trên tất cả các lớp khác trong quá trình huấn luyện lại để có được đánh giá tốt hơn về thay đổi phổ ma trận trọng số nhất định do thưa hóa. Quá trình huấn luyện lại được thực hiện với hai epoch bổ sung.
Hình 5 cho thấy một mẫu rõ ràng về cách phổ của ma trận trọng số tiến hóa một khi chúng ta cắt tỉa nó lặp lại. Đồ thị đầu tiên trong mỗi hàng là phổ của ma trận trọng số gốc. Các chấm cam biểu thị phổ của ma trận trọng số sau mỗi vòng cắt tỉa, và các chấm xanh đại diện cho phổ sau khi huấn luyện lại trong mỗi lần lặp. Ban đầu, chúng ta có thể cắt tỉa một số lượng lớn tham số từ ma trận trọng số mà không thay đổi phổ đáng kể, và chúng ta vẫn có thể phục hồi phổ ở một mức độ nào đó bằng cách huấn luyện lại. Khi độ thưa đạt đến một điểm nhất định, phổ dường như sụp đổ và thất bại trong việc phục hồi hình dạng và vị trí ban đầu ngay cả với huấn luyện lại, điều này phù hợp với quan sát chung rằng hiệu suất mạng nơ-ron giảm đáng kể khi nó quá thưa.
Hành vi phục hồi phổ như vậy không đáng kể trên VGG19. Phổ lệch khỏi những phổ gốc trong việc cắt tỉa nhưng được sửa đổi khá nhẹ trong quá trình huấn luyện lại. Điều này chủ yếu do Batch Normalization được áp dụng rộng rãi[ 22], điều này tái chia tỷ lệ các batch huấn luyện thành các batch có trung bình bằng không và phương sai đơn vị và do đó loại bỏ đáng kể nhu cầu định hình phổ ma trận. Ban đầu, chúng tôi đã quản lý để loại bỏ batch normalization đi kèm sau mỗi lớp tích chập trong VGG19. Tuy nhiên, điều này dẫn đến tình huống huấn luyện khó khăn cho mạng nơ-ron sâu, như cộng đồng đã biết rõ [ 22]. Do đó, chúng tôi áp dụng phương pháp thay thế để chứng minh tác động của batch normalization lên quá trình học phổ. Chúng tôi thêm batch normalization sau mỗi lớp tích chập trong LeNet, lặp lại thí nghiệm nói trên và so sánh kết quả. Để xử lý chi tiết batch normalization, vui lòng tham khảo[22][5].
Trong Hình 6, chúng ta quan sát thấy khi batch normalization được áp dụng, việc phục hồi phổ ít đáng kể hơn so với không có batch normalization, mặc dù cả quỹ đạo và trạng thái kết thúc của phổ cho cắt tỉa lặp lại và cắt tỉa một lần đều giống nhau một cách đáng ngạc nhiên.
Nhiệm vụ 4 Chúng tôi áp dụng thuật toán1 trên tất cả các lớp tích chập trong VGG19 cùng một lúc, thay đổi thiết lập thuật toán để có được các độ thưa khác nhau, ghi lại hiệu suất kiểm tra tương ứng của mạng đã được cắt tỉa, và so sánh với hiệu suất của mạng đã được cắt tỉa qua ngưỡng ở cùng mức độ thưa. Do tính ngẫu nhiên trong thuật toán đề xuất của chúng tôi, độ thưa trong các lớp khác nhau cũng khác nhau. Chúng tôi trình bày độ thưa tổng hợp, tức là tổng số tham số khác không chia cho tổng số tham số trong tất cả ma trận trọng số tích chập, trong kết quả nghiên cứu thực nghiệm của chúng tôi. Để dễ dàng cài đặt và tập trung vào các lập luận của chúng tôi, chúng tôi cố định tham số 𝑐=0.5, thay đổi tham số quantile 𝑞 và số thành phần chính 𝐾.
Từ Hình 7 chúng ta có thể thấy rằng, thuật toán đề xuất của chúng tôi gần như luôn dẫn đến hiệu suất tổng quát mạng đã cắt tỉa tốt hơn mà không huấn luyện lại so với hiệu suất được cung cấp bởi ngưỡng, ở các mức độ thưa khác nhau. Điều này chứng minh tiềm năng thiết kế và tùy chỉnh thuật toán thưa hóa ma trận cho các phương pháp cắt tỉa mạng nơ-ron tốt hơn. Ngoài ra, chúng tôi cũng quan sát thấy Alg 1 gần như luôn tạo ra lỗi thưa hóa nhỏ hơn so với ngưỡng theo 2-norm, đó chính xác là động lực thiết kế thuật toán.
Nhiệm vụ 5 Để hỗ trợ diễn giải cơ chế cắt tỉa kênh, chúng tôi kiểm tra mối quan hệ giữa Í𝑖|𝑇𝑖| và ∥˜𝐴∥𝐹. Đối với nhiệm vụ này, chúng tôi kiểm tra bốn lớp tích chập đầu tiên trong VGG19. Dựa trên minh họa của chúng tôi về lớp tích chập như phép nhân ma trận dày đặc, chúng tôi mở ra mỗi bộ lọc kênh từ mảng bậc 3 thành vector, và sau đó nối tất cả các vector kênh thành

--- TRANG 13 ---
Cắt tỉa Mạng nơ-ron như Quá trình Bảo tồn Phổ 111:13
ma trận. Do đó loại bỏ một kênh bộ lọc tương đương với loại bỏ một cột trong cái gọi là ma trận tích chập trong hình 8. Chúng tôi đã chỉ ra mối quan hệ giữa bảo tồn phổ và bảo tồn hiệu suất mạng nơ-ron. Miễn là chúng ta có thể chỉ ra mối quan hệ giữa Í𝑖|𝑇𝑖| và ∥˜𝐴∥𝐹, chúng ta có thể làm cầu nối khoảng cách và liên kết Í𝑖|𝑇𝑖| với hiệu suất mạng nơ-ron.
Hình 9 cho thấy rằng ít nhiều ∥˜𝐴∥𝐹 của ma trận tích chập tương quan âm với Í𝑖|𝑇𝑖|, điều này phù hợp với phân tích của chúng tôi.

7 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI
Trong công việc này, chúng tôi lập luận rằng huấn luyện mạng nơ-ron có sự phụ thuộc lẫn nhau mạnh mẽ với việc học phổ. Mối quan hệ này cung cấp cho việc cắt tỉa mạng nơ-ron một xây dựng lý thuyết dựa trên bảo tồn phổ, cụ thể hơn là thưa hóa ma trận. Chúng tôi xem xét các nỗ lực chính hiện tại về cắt tỉa mạng nơ-ron và đề xuất một quan điểm thống nhất cho cả cắt tỉa lớp dày đặc và lớp tích chập. Chúng tôi cũng thiết kế và tiến hành các thí nghiệm để hỗ trợ các lập luận, và do đó cung cấp khả năng diễn giải nhiều hơn cho chủ đề liên quan đến học sâu.
Chúng tôi dự đoán rằng thiết kế thuật toán vượt trội cho việc cắt tỉa mạng nơ-ron dựa trên thưa hóa ma trận hiệu quả và hiệu suất sử dụng lý thuyết phổ và cài đặt cấp thấp hơn của mạng nơ-ron thưa cho các hệ thống được nhắm mục tiêu. Đối với các công việc tương lai, chúng tôi sẽ tiến hành điều tra thêm về cách các hàm kích hoạt ảnh hưởng đến việc cắt tỉa và nhiều thuật toán cắt tỉa hơn trên các loại lớp mạng khác.

TÀI LIỆU THAM KHẢO
[1]Dimitris Achlioptas, Zohar Karnin, và Edo Liberty. 2013. Matrix entry-wise sampling: Simple is best. Submitted to KDD 2013, 1.1 (2013), 1–4.
[2]Dimitris Achlioptas, Zohar S Karnin, và Edo Liberty. 2013. Near-optimal entrywise sampling for data matrices. In Advances in Neural Information Processing Systems . 1565–1573.
[3]Dimitris Achlioptas và Frank McSherry. 2007. Fast computation of low-rank matrix approximations. Journal of the ACM (JACM) 54, 2 (2007), 9.
[4]Sanjeev Arora, Elad Hazan, và Satyen Kale. 2006. A fast random sampling algorithm for sparsifying matrices. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques . Springer, 272–279.
[5]Nils Bjorck, Carla P Gomes, Bart Selman, và Kilian Q Weinberger. 2018. Understanding batch normalization. In Advances in Neural Information Processing Systems . 7694–7705.
[6]Kumar Chellapilla, Sidd Puri, và Patrice Simard. 2006. High performance convolutional neural networks for document processing.
[7]Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, và Yixin Chen. 2015. Compressing neural networks with the hashing trick. In International conference on machine learning . 2285–2294.
[8]Wenlin Chen, James Wilson, Stephen Tyree, Kilian Q Weinberger, và Yixin Chen. 2016. Compressing convolutional neural networks in the frequency domain. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining . 1475–1484.
[9]Yu Cheng, Duo Wang, Pan Zhou, và Tao Zhang. 2017. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282 (2017).
[10] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, và Evan Shelhamer. 2014. cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759 (2014).
[11] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, và Rob Fergus. 2014. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in neural information processing systems . 1269–1277.
[12] Petros Drineas và Anastasios Zouzias. 2011. A note on element-wise matrix sparsification via a matrix-valued Bernstein inequality. Inform. Process. Lett. 111, 8 (2011), 385–389.
[13] Mengnan Du, Ninghao Liu, và Xia Hu. 2019. Techniques for interpretable machine learning. Commun. ACM 63, 1 (2019), 68–77.
[14] Carl Eckart và Gale Young. 1936. The approximation of one matrix by another of lower rank. Psychometrika 1, 3 (1936), 211–218.
[15] Jonathan Frankle và Michael Carbin. 2018. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 (2018).

--- TRANG 14 ---
111:14 Shibo Yao, Dantong Yu, và Ioannis Koutis

--- TRANG 15 ---
Cắt tỉa Mạng nơ-ron như Quá trình Bảo tồn Phổ 111:15
[16] Alex Gittens và Joel A Tropp. 2009. Error bounds for random matrix approximation schemes. arXiv preprint arXiv:0911.4108 (2009).
[17] Yunchao Gong, Liu Liu, Ming Yang, và Lubomir Bourdev. 2014. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115 (2014).
[18] Lee-Ad Gottlieb và Tyler Neylon. 2010. Matrix sparsification and the sparse null space problem. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques . Springer, 205–218.
[19] Song Han, Huizi Mao, và William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015).
[20] Song Han, Jeff Pool, John Tran, và William Dally. 2015. Learning both weights and connections for efficient neural network. In Advances in neural information processing systems . 1135–1143.
[21] Babak Hassibi và David G Stork. 1993. Second order derivatives for network pruning: Optimal brain surgeon. In Advances in neural information processing systems . 164–171.
[22] Sergey Ioffe và Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015).
[23] Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, và Dongjun Shin. 2015. Compression of deep convolutional neural networks for fast and low power mobile applications. arXiv preprint arXiv:1511.06530 (2015).
[24] Tamara G Kolda và Brett W Bader. 2009. Tensor decompositions and applications. SIAM review 51, 3 (2009), 455–500.
[25] Alex Krizhevsky, Geoffrey Hinton, và các cộng sự .2009. Learning multiple layers of features from tiny images . Báo cáo Kỹ thuật. Citeseer.
[26] Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, và Victor Lempitsky. 2014. Speeding-up convolutional neural networks using fine-tuned cp-decomposition. arXiv preprint arXiv:1412.6553 (2014).
[27] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, và các cộng sự .1998. Gradient-based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278–2324.
[28] Yann LeCun, John S Denker, và Sara A Solla. 1990. Optimal brain damage. In Advances in neural information processing systems . 598–605.
[29] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. 2016. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710 (2016).
[30] Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, và Marianna Pensky. 2015. Sparse convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 806–814.
[31] S Thomas McCormick. 1983. A Combinatorial Approach to Some Sparse Matrix Problems. Báo cáo Kỹ thuật. STANFORD UNIV CA SYSTEMS OPTIMIZATION LAB.
[32] L. Mirsky. 1960. Symmetric gauge functions and unitarily invariant norms. QJ Math., Oxf. II. Ser. 11 (1960), 50–59. https://doi.org/10.1093/qmath/11.1.50
[33] NH Nguyen, Petros Drineas, và TD Tran. 2009. Matrix sparsification via the Khintchine inequality. (2009).
[34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, và các cộng sự .2019. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems . 8024–8035.
[35] Wojciech Samek, Thomas Wiegand, và Klaus-Robert Müller. 2017. Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. arXiv preprint arXiv:1708.08296 (2017).
[36] Karen Simonyan và Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[37] Yi Sun, Xiaogang Wang, và Xiaoou Tang. 2016. Sparsifying neural network connections for face recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 4856–4864.
[38] M Alex O Vasilescu và Demetri Terzopoulos. 2002. Multilinear analysis of image ensembles: Tensorfaces. In European Conference on Computer Vision . Springer, 447–460.
[39] Hattie Zhou, Janice Lan, Rosanne Liu, và Jason Yosinski. 2019. Deconstructing lottery tickets: Zeros, signs, and the supermask. arXiv preprint arXiv:1905.01067 (2019).
ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.

--- TRANG 16 ---
111:16 Shibo Yao, Dantong Yu, và Ioannis Koutis
A TÀI LIỆU BỔ SUNG CHO TÍNH TÁI TẠO
A.1 mã python cho Alg1
def lowRankSampling(A, tao=0.3, n_comp=5):
A_ = np.array([i.flatten() for i in A])
U,sigma,V = randomized_svd(A_, n_comp)
B = np.zeros(A_.shape)
for i in range(n_comp):
B += np.outer(U[:,i],V[i])*sigma[i]
flat_B = abs(B.flatten())
thresh_B = int(flat_B.size*tao)-1
t_B = np.partition(flat_B, thresh_B)[thresh_B]
with np.nditer(A, op_flags=["readwrite"]) as it:
for x in it:
if abs(x) < t_B:
p = np.square(x/t_B)
if p < 0.5:
x[...] = 0
else:
x[...] = np.random.binomial(1,p,1) * x / p
return A

A.2 Kết quả Nghiên cứu Thực nghiệm Bổ sung

[Hình 10. (Nhiệm vụ1) Phổ Ma trận trong LeNet ổn định trong quá trình huấn luyện. Mỗi chấm biểu thị một giá trị đơn của ma trận trọng số.]

--- TRANG 17 ---
Cắt tỉa Mạng nơ-ron như Quá trình Bảo tồn Phổ 111:17

[Hình 11. (Nhiệm vụ1) Phổ Ma trận trong VGG19 ổn định trong quá trình huấn luyện.]

ACM Trans. Knowl. Discov. Data., Vol. 37, No. 4, Article 111. Publication date: August 2022.
