# 2106.02914.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2106.02914.pdf
# Kích thước tệp: 2856634 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Bản thảo
ĐIỀU CHỈNH DÒNG ĐẶC TRƯNG: CẢI THIỆN 
ĐỘ THƯA THỚT CÓ CẤU TRÚC TRONG MẠNG NEURAL SÂU
Yue Wu
Khoa Toán học
HKUST
ywudb@connect.ust.hk

Yuan Lan
Khoa Toán học
HKUST
ylanaa@connect.ust.hk

Luchan Zhang
Khoa Toán học
HKUST
malczhang@ust.hk

Yang Xiang
Khoa Toán học
HKUST
maxiang@ust.hk

TÓM TẮT
Cắt tỉa là một phương pháp nén mô hình loại bỏ các tham số dư thừa và tăng tốc độ suy luận của mạng neural sâu (DNN) trong khi duy trì độ chính xác. Hầu hết các phương pháp cắt tỉa hiện có áp đặt các điều kiện khác nhau trực tiếp lên tham số hoặc đặc trưng. Trong bài báo này, chúng tôi đề xuất một chiến lược điều chỉnh đơn giản và hiệu quả để cải thiện độ thưa thớt có cấu trúc và cắt tỉa có cấu trúc trong DNN từ góc nhìn mới về sự tiến hóa của đặc trưng. Cụ thể, chúng tôi xem xét các quỹ đạo kết nối các đặc trưng của các lớp ẩn liền kề, gọi là dòng đặc trưng. Chúng tôi đề xuất điều chỉnh dòng đặc trưng (FFR) để phạt độ dài và tổng độ cong tuyệt đối của các quỹ đạo, điều này ngầm tăng độ thưa thớt có cấu trúc của các tham số. Nguyên lý đằng sau FFR là các quỹ đạo ngắn và thẳng sẽ dẫn đến một mạng hiệu quả tránh các tham số dư thừa. Các thí nghiệm trên bộ dữ liệu CIFAR-10 và ImageNet cho thấy FFR cải thiện độ thưa thớt có cấu trúc và đạt được kết quả cắt tỉa tương đương hoặc thậm chí tốt hơn so với các phương pháp tiên tiến hiện tại.

1 GIỚI THIỆU
Mạng neural sâu (DNN) đã đạt được thành công to lớn trong nhiều ứng dụng. Đồng thời, DNN đòi hỏi chi phí tính toán và không gian lưu trữ đáng kể hơn khi chúng trở nên sâu hơn để đạt được độ chính xác cao hơn. Denil et al. (2013) đã chứng minh rằng có sự dư thừa đáng kể trong việc tham số hóa DNN. Giả thuyết Vé số may mắn (Frankle & Carbin, 2019) phỏng đoán rằng tồn tại các mạng con thưa thớt có thể đạt được độ chính xác tương đương với mạng gốc khi được huấn luyện độc lập.

Các phương pháp nén mô hình đã được đề xuất để cân bằng độ chính xác và độ phức tạp của mô hình, ví dụ như cắt tỉa trọng số (LeCun et al., 1990; Hassibi & Stork, 1993; Han et al., 2015; Guo et al., 2016; Hu et al., 2016; Li et al., 2016) và lượng tử hóa (Gong et al., 2014), xấp xỉ hạng thấp (Denton et al., 2014; Jaderberg et al., 2014; Liu et al., 2015), và học cấu trúc thưa thớt (Wen et al., 2016; Alvarez & Salzmann, 2016; Zhou et al., 2016; Liu et al., 2017; Louizos et al., 2018; Gao et al., 2019; Yuan et al., 2020). Cắt tỉa trọng số loại bỏ các tham số ít quan trọng trong mạng. Cụ thể, cắt tỉa bộ lọc (Li et al., 2016) loại bỏ toàn bộ các bộ lọc trong mạng cùng với các kênh liên quan, có thể nén và tăng tốc DNN một cách hiệu quả.

Các phương pháp cắt tỉa có cấu trúc hiện có có thể được chia thành hai loại: các phương pháp dựa trên tham số (Li et al., 2016; Molchanov et al., 2016; Liu et al., 2017; He et al., 2018; Lin et al., 2019; He et al., 2019; Zhuang et al., 2020; Liebenwein et al., 2020) sử dụng một số tiêu chí để xác định các bộ lọc không quan trọng và loại bỏ chúng, và các phương pháp dựa trên đặc trưng (Luo et al., 2017; He et al., 2017; Zhuang et al., 2018; Ye et al., 2018; Li et al., 2020; Lin et al., 2020) chọn các bản đồ đặc trưng không quan trọng rồi loại bỏ các bộ lọc và kênh liên quan. Ví dụ, Li et al. (2020) đã kết hợp hai cách chọn bản đồ đặc trưng: khám phá các đặc trưng có độ đa dạng thấp và loại bỏ các đặc trưng có độ tương tự cao với các đặc trưng khác.

--- TRANG 2 ---
Bản thảo
Trong bài báo này, chúng tôi đề xuất một phương pháp điều chỉnh mới trên quỹ đạo kết nối các đặc trưng của các lớp ẩn liền kề, gọi là điều chỉnh dòng đặc trưng (FFR). FFR làm mượt quỹ đạo của các đặc trưng, điều này ngầm cải thiện độ thưa thớt có cấu trúc trong DNN. Động lực của chúng tôi là quỹ đạo của dữ liệu dọc theo mạng phản ánh cấu trúc DNN. Quỹ đạo ngắn hơn và thẳng hơn tương ứng với cấu trúc DNN hiệu quả và thưa thớt. Một minh họa được đưa ra trong Hình 1b.

Những đóng góp chính của chúng tôi là: (1) Chúng tôi đề xuất một điều chỉnh mới (FFR) trên quỹ đạo kết nối các đặc trưng của các lớp ẩn, để cải thiện độ thưa thớt có cấu trúc trong DNN từ góc nhìn quỹ đạo của dữ liệu dọc theo mạng. Phương pháp này khác với các phương pháp học cấu trúc thưa thớt hiện có, vốn áp đặt điều chỉnh hoặc ràng buộc trực tiếp lên các tham số. Phương pháp của chúng tôi cũng khác với những phương pháp cắt tỉa dựa trên bản đồ đặc trưng, vốn sử dụng thông tin của bản đồ đặc trưng một cách riêng lẻ hoặc theo cặp (cho độ tương tự) mà không có mối quan hệ toàn cục. (2) Chúng tôi phân tích hiệu ứng của FFR áp dụng cho lớp tích chập và lớp dư, và cho thấy FFR khuyến khích DNN học một cấu trúc thưa thớt trong quá trình huấn luyện bằng cách phạt độ thưa thớt của cả tham số và đặc trưng. (3) Kết quả thí nghiệm cho thấy FFR đạt được tỷ lệ cắt tỉa tương đương hoặc thậm chí tốt hơn về tham số và FLOP so với các phương pháp cắt tỉa tiên tiến gần đây.

2 CÔNG TRÌNH LIÊN QUAN
Cắt tỉa bộ lọc. Nhiều tiêu chí khác nhau cho việc chọn bộ lọc trong cắt tỉa đã được đề xuất. Li et al. (2016) sử dụng chuẩn L1 để chọn các bộ lọc không quan trọng và loại bỏ các bộ lọc có chuẩn thấp hơn ngưỡng đã cho cùng với các bản đồ đặc trưng kết nối của chúng. Molchanov et al. (2016) đo tầm quan trọng của các bộ lọc dựa trên sự thay đổi trong hàm chi phí gây ra bởi cắt tỉa. Luo et al. (2017); He et al. (2017) công thức hóa cắt tỉa như một bài toán tối ưu hóa có ràng buộc và chọn các neuron đại diện nhất dựa trên việc tối thiểu hóa lỗi tái tạo. Lin et al. (2019) cắt tỉa các bộ lọc cũng như các cấu trúc khác bằng học đối thủ sinh. He et al. (2019) cắt tỉa các bộ lọc dư thừa sử dụng tương quan hình học giữa các bộ lọc trong cùng một lớp. Hu et al. (2016); Zhuang et al. (2018); Li et al. (2020); Lin et al. (2020) loại bỏ các bộ lọc dựa trên thông tin, ví dụ như độ thưa thớt, hạng hoặc độ đa dạng, của các bản đồ đặc trưng được tạo ra bởi các bộ lọc. Phương pháp của chúng tôi học DNN thưa thớt trong quá trình huấn luyện và áp dụng sơ đồ cắt tỉa dựa trên độ lớn sau huấn luyện.

Điều chỉnh độ thưa thớt. Một số nghiên cứu đã giới thiệu điều chỉnh độ thưa thớt để tìm cấu trúc thưa thớt của DNN. Một chiến lược thường được sử dụng là áp đặt nhóm Lasso và điều chỉnh L0 nới lỏng (Zhou et al., 2016; Wen et al., 2016; Alvarez & Salzmann, 2016; Louizos et al., 2018). Liu et al. (2017); Huang & Wang (2018) liên kết một yếu tố tỷ lệ với các bản đồ đặc trưng và áp đặt điều chỉnh lên các yếu tố tỷ lệ này trong quá trình huấn luyện để tự động xác định các kênh không quan trọng. Các bản đồ đặc trưng có giá trị yếu tố tỷ lệ nhỏ sẽ bị cắt tỉa. Gao et al. (2019) áp đặt một nhóm Liên-lớp và một điều chỉnh Nhận thức Phương sai lên các tham số để cải thiện độ thưa thớt có cấu trúc cho các mô hình dư. Zhuang et al. (2020) sử dụng bộ điều chỉnh phân cực trên các yếu tố tỷ lệ. Yuan et al. (2020) đề xuất một phương pháp để tăng trưởng động các mạng sâu bằng cách liên tục làm thưa thớt các tập tham số có cấu trúc. Khác với các phương pháp hiện có này trực tiếp giới thiệu điều chỉnh lên các tham số hoặc yếu tố tỷ lệ, FFR của chúng tôi áp đặt điều chỉnh lên quỹ đạo kết nối các đặc trưng của các lớp ẩn để kiểm soát các tham số và thúc đẩy độ thưa thớt có cấu trúc một cách ngầm.

3 PHƯƠNG PHÁP
3.1 ĐIỀU CHỈNH DÒNG ĐẶC TRƯNG
Chúng tôi định nghĩa dòng đặc trưng là quỹ đạo được hình thành bằng cách kết nối các đặc trưng đầu ra của các lớp ẩn liền kề. Đối với một DNN, tập hợp các quỹ đạo với dữ liệu đầu vào khác nhau từ tập dữ liệu huấn luyện phản ánh cấu trúc mạng. Chúng tôi kiểm soát các quỹ đạo này để có được một mạng thưa thớt.

Xem xét sự lan truyền tiến của một DNN với L lớp {xl}l=0,1,...,L:
xl+1 = hl(xl, wl), (1)

--- TRANG 3 ---
Bản thảo
trong đó xl+1 là đặc trưng đầu ra của lớp thứ l, hl là ánh xạ trong lớp thứ l, và wl là tập hợp các tham số có thể huấn luyện. Giới thiệu một phân chia thời gian: {tl = l/L}L l=0 với khoảng thời gian Δt = 1/L; và coi xl như giá trị của một hàm x(t) tại bước thời gian tl, mà không xem xét tính nhất quán về chiều, Phương trình (1) có thể được viết lại như (He et al., 2016; E, 2017; Lu et al., 2017; Chen et al., 2018)

x(tl+1) = x(tl) + Δt ĥl(x(tl), wl), (2)

trong đó ĥl = (hl − xl)/Δt. Điều này có thể được hiểu như một rời rạc hóa của sự tiến hóa dọc theo quỹ đạo của mạng được mô tả bởi một phương trình vi phân thường (E, 2017; Lu et al., 2017; Chen et al., 2018)

dx(t)/dt = ĥ(x(t), w(t), t). (3)

Dòng đặc trưng là quỹ đạo được hình thành bằng cách kết nối các đặc trưng của {xl}, và được ký hiệu là {xl}. Xem Hình 1a để minh họa dòng đặc trưng.

Chúng tôi coi quỹ đạo x(t) cũng như quỹ đạo dòng đặc trưng {xl} là một "đường cong". Nhớ lại rằng đối với một đường cong Γ(t) : [0,1] → RD với tham số độ dài cung s, độ dài C(Γ) và tổng độ cong tuyệt đối K(Γ) của nó là

C(Γ) := ∫₀¹ ||Γ'(t)||dt, K(Γ) := ∫₀^C(Γ) |κ(s)|ds, (4)

trong đó κ(s) = ||Γ''(s)|| là độ cong của đường cong.

Chúng tôi giới thiệu điều chỉnh dòng đặc trưng (FFR) để cải thiện độ thưa thớt có cấu trúc của DNN, mượn các định nghĩa về độ dài và tổng độ cong tuyệt đối của một đường cong cho dòng đặc trưng, tức là quỹ đạo được hình thành bằng cách kết nối các đặc trưng của các lớp ẩn. Đối với một dòng đặc trưng liên kết với các đặc trưng lớp ẩn {xl}l=0,1,...,L, FFR là

R(x) := κ₁C(x) + κ₂K(x), (5)

trong đó

C(x) = Σ^(L-1)_(l=0) ||xl+1 - xl||, K(x) = Σ^(L-1)_(l=1) ||xl+1 - 2xl + xl-1||, (6)

với ||·|| là chuẩn L1, và κ₁, κ₂ > 0 là các siêu tham số. Ở đây, với một số yếu tố hằng số, C(x) ≈ Σ^(L-1)_(l=0) ||x'(tl)|| là một xấp xỉ của tổng độ dài của quỹ đạo x(t), và K(x) ≈ Σ^(L-1)_(l=1) |κ(tl)| là một xấp xỉ của tổng độ cong tuyệt đối của nó, trong đó {xl = x(tl)} là một rời rạc hóa của quỹ đạo x(t) với phân chia thời gian {tl = l/L}L l=0.

FFR làm mượt dòng đặc trưng bằng cách kiểm soát độ dài và độ cong. Trực quan, số hạng độ dài trong Phương trình (6) làm cho dòng đặc trưng ngắn, và số hạng độ cong trong Phương trình (6) giữ cho dòng đặc trưng không bị uốn cong quá nhiều. Kết quả là, DNN được huấn luyện dưới FFR có cấu trúc thưa thớt hơn; Xem minh họa trong Hình 1b. Phân tích định lượng hơn được đưa ra trong Mục 4. Chúng tôi tiếp tục đưa ra một ví dụ minh họa trong Hình 1c, đây là một hình ảnh hóa hai chiều cho thấy hiệu ứng làm mượt của FFR trên dòng đặc trưng của một ResNet. Trong ví dụ này, đầu vào, đặc trưng và đầu ra đều là các điểm trong hai chiều, và dòng đặc trưng là các đường cong thực tế trong hai chiều. Chúng ta có thể thấy rằng dòng đặc trưng dưới FFR ngắn hơn và thẳng hơn. Chi tiết hơn về ví dụ minh họa này được đưa ra trong Phụ lục Mục A.1.

3.2 FFR ÁP DỤNG CHO DNN VÀ CẮT TỈA

Đối với một DNN và tập dữ liệu huấn luyện {(x^(j), y^(j))}^N_(j=1), sử dụng Phương trình (5), FFR là:

R(X^(j)) = κ₁ Σ^(L-1)_(i=0) ||x^(j)_(l+1) - x^(j)_l|| + κ₂ Σ^(L-1)_(i=1) ||x^(j)_(l+1) - 2x^(j)_l + x^(j)_(l-1)||, (7)

trong đó X^(j) = {xl, l = 0,...,L}^(j) biểu thị tập hợp các đặc trưng đầu ra của các lớp ẩn cho dữ liệu đầu vào thứ j. Hàm mất mát với FFR trong huấn luyện là:

1/N Σ^N_(j=1) [J(x^(j), y^(j), W) + R(X^(j))], (8)

--- TRANG 4 ---
Bản thảo

(a)

(c)

(b)

Hình 1: (a) Minh họa dòng đặc trưng: Mỗi đường cong đại diện cho một dòng đặc trưng, tức là quỹ đạo kết nối các đặc trưng của các lớp ẩn, trong đó các nút là đầu vào, đặc trưng và đầu ra. (b) Minh họa điều chỉnh dòng đặc trưng: Dòng đặc trưng dưới FFR ngắn hơn và thẳng hơn do phạt độ dài và độ cong. Kết quả là, FFR cải thiện độ thưa thớt có cấu trúc và dẫn đến cắt tỉa bộ lọc hiệu quả. (c) Một ví dụ minh họa trong không gian hai chiều cho thấy hiệu ứng làm mượt của FFR, đây là một ResNet năm khối được huấn luyện với FFR và không có FFR (cơ sở). Dữ liệu đầu vào, đặc trưng và mục tiêu đều là các điểm trong hai chiều, và dòng đặc trưng là các đường cong trong hai chiều. Cụm xanh lá và cụm đỏ chứa các điểm dữ liệu đầu vào và các mục tiêu, tương ứng.

trong đó J(x, y, W) là hàm mất mát trước khi áp dụng FFR.

Lưu ý rằng hai trạng thái ẩn xl, xl+1 có thể có các chiều khác nhau. Để khắc phục vấn đề không khớp chiều, chúng tôi áp dụng cùng chiến lược như He et al. (2016), tức là sử dụng một phép chiếu tuyến tính Pl bằng các kết nối tắt để khớp các chiều. Chúng tôi thay thế xl trong Phương trình (7) bằng Plxl, trong đó Pl là ma trận chiếu đã học và sẽ được coi như các tham số có thể học trong huấn luyện. Trong việc triển khai, trước tiên chúng tôi nhóm các đặc trưng theo giai đoạn (chiều đặc trưng):

X^(j) = ∪^G_(g=1) {xg,1, xg,2, ..., xg,lg}^(j); L = Σ^G_(g=1) lg. (9)

Ở đây G là số giai đoạn trong X^(j) và lg là số lớp ẩn trong giai đoạn g. Thứ hai, chúng tôi sử dụng ma trận chiếu để liên kết các nhóm khác nhau vì việc không khớp chiều chỉ xảy ra ở đặc trưng đầu tiên của mỗi giai đoạn. Sử dụng phương pháp này, FFR của X^(j) trở thành:

R(X^(j)) = κ₁ Σ^G_(g=1) [||x^(j)_(g,1) - Pgx^(j)_(g-1,l(g-1))|| + Σ^(lg-1)_(i=1) ||x^(j)_(g,i+1) - x^(j)_(g,i)||]
           + κ₂ Σ^G_(g=1) [||x^(j)_(g,2) - 2x^(j)_(g,1) + Pgx^(j)_(g-1,l(g-1))|| + Σ^(lg-1)_(i=2) ||x^(j)_(g,i+1) - 2x^(j)_(g,i) + x^(j)_(g,i-1)||]. (10)

Đồng thời, các siêu tham số κ₁, κ₂ có thể thay đổi theo chiều đặc trưng để FFR có thể kiểm soát đồng đều các đặc trưng ở các giai đoạn khác nhau. Trong các thí nghiệm của chúng tôi, chúng tôi điều chỉnh κ₁, κ₂ tỷ lệ nghịch với quy mô của các bản đồ đặc trưng. Hơn nữa, quá trình FFR có thể được tổng quát hóa cho trường hợp chúng ta chọn các đặc trưng mỗi vài lớp. Chúng ta có thể ký hiệu các lớp ẩn được chọn

--- TRANG 5 ---
Bản thảo
như L = {li, i = 0,1,...,m; m = #L}. Sau đó chúng ta áp dụng FFR cho dòng đặc trưng kết nối các đặc trưng trong tập X^(j) = {xl, l ∈ L}.

Sau huấn luyện, chúng tôi tiến hành cắt tỉa bộ lọc một lần: loại bỏ các bộ lọc có độ lớn nhỏ và loại bỏ các kênh trong lớp tiếp theo tích chập với các bản đồ đặc trưng được tạo ra bởi các bộ lọc đã cắt tỉa. Cuối cùng, chúng tôi tinh chỉnh mạng đã cắt tỉa trong vài epoch.

Phương pháp huấn luyện FFR và cắt tỉa của chúng tôi được tóm tắt trong Thuật toán 1.

Thuật toán 1 Huấn luyện FFR và Cắt tỉa Một lần
Yêu cầu: tập dữ liệu huấn luyện {(x^(j), y^(j))}^N_(j=1); một mạng neural và các siêu tham số κ₁, κ₂.
Bước chuẩn bị 1: nhóm các đặc trưng trong X = {xl, l ∈ L} theo giai đoạn như trong Phương trình (9),
Bước chuẩn bị 2: viết FFR R(X^(j)) trong Phương trình (10) cho mỗi cặp dữ liệu (x^(j), y^(j)).
Bước huấn luyện: huấn luyện mạng dưới hàm mất mát với FFR cho trong Phương trình (8).
Bước cắt tỉa: loại bỏ các bộ lọc và các kênh tương ứng trong mô hình đã huấn luyện,
tinh chỉnh mô hình đã cắt tỉa trong vài epoch.
trả về một mạng neural nhỏ gọn.

3.3 DÒNG ĐẶC TRƯNG CỦA VGGNET VÀ RESNET

Trong mục này, chúng tôi trình bày cách xây dựng dòng đặc trưng của VGGNet (Simonyan & Zisserman, 2015) và ResNet (He et al., 2016), đây là hai kiến trúc mạng thường được sử dụng. Việc xây dựng tương tự có thể áp dụng cho các mạng neural khác.

(a)

(b)

Hình 2: (a) Một khối tích chập trong VGGNet. (b) Các khối dư trong ResNet.

VGGNet. VGGNet là một mạng neural tích chập với cấu trúc đơn giản. Mỗi lớp tích chập được theo sau bởi một số hàm kích hoạt: chuẩn hóa batch, hàm kích hoạt tuyến tính chỉnh lưu (ReLU), và đôi khi một lớp maxpooling. Chúng tôi coi lớp tích chập với tất cả các kích hoạt của nó như một khối; xem Hình 2a. Chúng tôi thu thập đầu ra của mỗi khối tích chập trong VGGNet để tạo thành dòng đặc trưng {xl}. Gọi xl là đặc trưng đầu ra của khối thứ l, thì đặc trưng của khối tích chập thứ l+1 là

xl+1 = σ(Max pooling ∘ BN ∘ (Wl ⊗ xl)), (11)

trong đó Wl là các trọng số tích chập, BN là chuẩn hóa batch, và σ(·) là ReLU. Max pooling chỉ xuất hiện một lần mỗi vài lớp. ⊗ biểu thị phép toán tích chập và ∘ biểu thị phép toán chuẩn hóa batch.

ResNet. ResNet lấy các khối dư làm khối xây dựng, mỗi khối chứa hai hoặc ba lớp tích chập; xem Hình 2b. Chúng tôi thu thập đầu ra của mỗi khối dư để tạo thành dòng đặc trưng {xl}. Đặc trưng của khối dư thứ l+1 là

xl+1 = σ(F(xl, Wl) + Wslxl), (12)

trong đó F(xl, Wl) là hàm dư, Wl = {Wl,k|k = 1,...,K} là tập hợp các trọng số tích chập, K = 2 hoặc 3 là số lớp tích chập trong mỗi đơn vị dư. Wsl là ma trận đơn vị nếu xl và F(xl, Wl) có cùng chiều, và là ma trận chiếu đã học trong trường hợp ngược lại.

--- TRANG 6 ---
Bản thảo
4 PHÂN TÍCH ĐỘ THƯA THỚT

Trong mục này, chúng tôi trình bày cách FFR cải thiện độ thưa thớt có cấu trúc. Như đã giải thích trong mục trước, ý tưởng của FFR là rút ngắn và làm thẳng quỹ đạo của đầu vào dọc theo mạng. Chúng tôi sẽ cho thấy rằng hiệu ứng này của FFR khuyến khích độ thưa thớt đặc trưng ngoài việc phạt độ thưa thớt của các tham số. Các đặc trưng thưa thớt góp phần vào độ thưa thớt có cấu trúc: đối với bản đồ đặc trưng có giá trị zero, chúng ta có thể loại bỏ bộ lọc tạo ra bản đồ đặc trưng và kênh tích chập với bản đồ đặc trưng. Do đó, FFR cải thiện độ thưa thớt có cấu trúc trong mạng.

4.1 ĐỘ THƯA THỚT ĐẶC TRƯNG VÀ THAM SỐ

Chúng tôi phân tích hiệu ứng của FFR khi nó được áp dụng cho các đặc trưng đầu ra của khối tích chập và khối dư, thường được sử dụng trong DNN. Chúng tôi tập trung vào số hạng độ dài trong Phương trình (6) trong FFR.

Khối tích chập. Bỏ qua các hàm kích hoạt trong Phương trình (11), số hạng độ dài trong FFR được rút gọn thành

||xl+1 - xl|| = ||Wl ⊗ xl - xl|| = ||(Wl - I) ⊗ xl||, (13)

trong đó I biểu thị các kernel tích chập 'đơn vị': đối với bộ lọc thứ i, chỉ kênh thứ i là ma trận khác zero và tất cả các kênh khác là ma trận zero. Số hạng độ dài này đẩy tham số Wl gần với kernel đơn vị I, vốn có độ thưa thớt có cấu trúc cao. Đồng thời, số hạng độ dài cũng đẩy đặc trưng xl để trở nên thưa thớt. Hình 3a cho thấy chuẩn L1 của 512 bản đồ đặc trưng được tạo ra bởi lớp tích chập cuối cùng trong VGG16 được huấn luyện có và không có FFR trên CIFAR-10. Như được hiển thị trong hình, mạng cơ sở (VGG16 được huấn luyện không có FFR) có ít bản đồ đặc trưng có giá trị zero. Ngược lại, mạng được huấn luyện dưới FFR học được các đặc trưng thưa thớt hơn nhiều: hầu hết các bản đồ đặc trưng có chuẩn giá trị zero có thể được loại bỏ. Sự cải thiện đáng kể về độ thưa thớt đặc trưng dưới FFR này cũng được quan sát thấy trong so sánh cho các lớp khác của VGG16; xem mục Phụ lục A.2 để có thêm biểu đồ.

Khối dư. Số hạng độ dài trong FFR tăng cường độ thưa thớt của khối dư bằng cách đẩy hàm dư F(xl, Wl) và đặc trưng xl về zero. Chúng tôi bỏ qua ma trận chiếu Ws trong kết nối tắt vì nó chỉ xuất hiện khi giai đoạn thay đổi. Từ Phương trình (12), số hạng độ dài trong FFR được rút gọn thành

||xl+1 - xl|| = ||(F(xl, Wl) + xl) - xl|| = { ||F(xl, Wl)||, F(xl, Wl) + xl ≥ 0,
                                                  ||xl||,        F(xl, Wl) + xl < 0. (14)

Dưới giả thuyết rằng hàm tối ưu gần với ánh xạ đơn vị hơn so với ánh xạ zero (He et al., 2016), trường hợp đầu tiên F(xl, Wl) + xl ≥ 0 xảy ra hầu hết thời gian, và trong trường hợp này, FFR phạt chuẩn của hàm dư. Đối với một khối dư có hai hoặc ba lớp tích chập, và bỏ qua các kích hoạt, hàm dư có thể được rút gọn thành

F(xl, Wl) = σ(BN₂ ∘ Wl,2 ⊗ (σ(BN₁ ∘ Wl,1 ⊗ xl))) ≈ Wl,2 ⊗ Wl,1 ⊗ xl, hoặc (15)
F(xl, Wl) = σ(BN₃ ∘ Wl,3 ⊗ (σ(BN₂ ∘ Wl,2 ⊗ (σ(BN₁ ∘ Wl,1 ⊗ xl))))) ≈ Wl,3 ⊗ Wl,2 ⊗ Wl,1 ⊗ xl. (16)

Do đó, áp đặt phạt lên hàm dư có thể làm cho cả tham số và đặc trưng trở nên thưa thớt. Trong trường hợp thứ hai, trong đó F(xl, Wl) < -xl, FFR khuyến khích đặc trưng xl trở nên thưa thớt.

Hình 3b cho thấy chuẩn L1 của 64 bản đồ đặc trưng của khối dư đầu tiên trong ResNet56 được huấn luyện có và không có FFR trên CIFAR-10. Như được hiển thị trong hình, mạng được huấn luyện dưới FFR đầu ra nhiều bản đồ đặc trưng có giá trị zero hơn so với cơ sở (được huấn luyện không có FFR). Trong Hình 3c, chúng tôi hình ảnh hóa các tham số có hình dạng 16×16×3×3 từ lớp tích chập đầu tiên trong khối dư đầu tiên của ResNet56 được huấn luyện với FFR trên CIFAR-10. Chúng tôi hiển thị bộ lọc theo hàng theo độ lớn: từ trên xuống dưới, chuẩn của bộ lọc đang tăng. Có thể thấy từ hình rằng các tham số của mạng được huấn luyện với FFR có độ thưa thớt có cấu trúc cao. Trong hình, các bộ lọc có giá trị nhỏ được khoanh tròn bằng hình vuông ngang màu đỏ, và các kênh có giá trị nhỏ được tích chập với cùng bản đồ đặc trưng được khoanh tròn tương ứng bằng các hình vuông dọc màu cam.

--- TRANG 7 ---
Bản thảo
(a) VGG16

(b) ResNet56

(c) Hình ảnh hóa bộ lọc: 16×16×3×3.

Hình 3: (a) Bản đồ đặc trưng VGG16 được huấn luyện có và không có FFR: biểu đồ chuẩn L1, với 512 bản đồ đặc trưng trong đặc trưng của lớp tích chập cuối cùng. (b) Bản đồ đặc trưng ResNet56 được huấn luyện có và không có FFR: biểu đồ chuẩn L1, với 64 bản đồ đặc trưng trong đặc trưng của khối dư thứ mười chín. (c) Hình ảnh hóa các bộ lọc trong khối dư đầu tiên trong ResNet56 được huấn luyện với FFR. Từ trên xuống dưới, các bộ lọc được hiển thị theo chuẩn của chúng từ nhỏ đến lớn. Hình vuông biểu thị các bộ lọc (đỏ, ngang) và kênh (cam, dọc) có độ lớn nhỏ và có thể được loại bỏ. Cả VGG16 và ResNet56 đều được huấn luyện trên CIFAR-10.

4.2 CẢI THIỆN ĐỘ THƯA THỚT CÓ CẤU TRÚC

Để tiếp tục cho thấy khả năng của FFR trong việc cải thiện độ thưa thớt có cấu trúc, chúng tôi kiểm tra mối quan hệ giữa độ chính xác và độ thưa thớt có cấu trúc. Chúng tôi so sánh VGG16 và ResNet56 có và không có FFR được huấn luyện trên CIFAR-10. Sau huấn luyện, chúng tôi sử dụng một ngưỡng tăng dần để làm zero các bộ lọc có độ lớn dưới ngưỡng và các kênh tương ứng. Sau đó chúng tôi tính toán độ chính xác và độ thưa thớt có cấu trúc được định nghĩa là phần trăm các tham số được làm zero. Hình 4a và 4b cho thấy các đường cong đánh đổi độ chính xác-độ thưa thớt của VGG16 và ResNet56 trên CIFAR-10, tương ứng. Có thể thấy rằng mạng được huấn luyện với FFR sẽ không gặp phải sự suy giảm độ chính xác cho đến khi nó có độ thưa thớt có cấu trúc lớn đáng kể so với trường hợp cơ sở.

(a) VGG16

(b) ResNet56

Hình 4: Đường cong đánh đổi độ chính xác-độ thưa thớt của (a) VGG16 và (b) ResNet56 được huấn luyện với FFR (xanh dương) và không có FFR (cam, Cơ sở) trên CIFAR-10.

5 THÍ NGHIỆM
5.1 TRIỂN KHAI

Mô hình và bộ dữ liệu. Để chứng minh hiệu quả của FFR trong cắt tỉa, chúng tôi thực hiện các thí nghiệm với VGGNet và ResNet trên các bộ dữ liệu CIFAR-10 (Krizhevsky & Hinton, 2009) và ImageNet (Deng et al., 2009). VGG16 là một VGGNet được sửa đổi trong Simonyan & Zisserman (2015): chúng tôi loại bỏ hai lớp kết nối đầy đủ và chỉ sử dụng một lớp kết nối đầy đủ cho phân loại. Chúng tôi so sánh mạng được huấn luyện với FFR và không có FFR như một cơ sở. Chúng tôi áp dụng khởi tạo trọng số như trong He et al. (2015).

--- TRANG 8 ---
Bản thảo
Cài đặt huấn luyện. FFR và cơ sở có cùng cài đặt huấn luyện. Đối với CIFAR-10, kích thước mini-batch là 128. Chúng tôi huấn luyện VGG16 và ResNet56 200 epoch và đặt tốc độ học ban đầu là 0.1 và chia cho 10 ở epoch thứ 80, 120 và 160. Đối với ImageNet, kích thước mini-batch là 256. Chúng tôi huấn luyện ResNet50 90 epoch và chia tốc độ học ban đầu 0.1 cho 10 ở epoch thứ 30 và 60. Chúng tôi sử dụng bộ tối ưu SGD với weight decay 5e−4 cho CIFAR-10, 1e−4 cho ImageNet, và momentum 0.9. Lưu ý rằng điều chỉnh L2 được triển khai bằng cách áp dụng weight decay trong SGD. Chúng tôi chạy các thí nghiệm với pytorch.

Cài đặt cắt tỉa. Chúng tôi thực hiện cắt tỉa có cấu trúc một lần: loại bỏ các bộ lọc dựa trên độ lớn và các kênh tương ứng của chúng trong lớp tiếp theo. Đối với tất cả các thí nghiệm trên CIFAR-10 và ImageNet, chúng tôi chỉ tinh chỉnh mô hình đã cắt tỉa 30 epoch với tốc độ học 1e−4 trên CIFAR-10 và 1e−3 trên ImageNet. Đối với VGGNet, chúng tôi cắt tỉa mạng trực tiếp, và đối với ResNet, chúng tôi sử dụng zero padding trong chiều đã cắt tỉa. Đầu ra của đường tắt và lớp tích chập cuối cùng trong khối dư trong ResNet phải có cùng chiều vì phép cộng giữa chúng. Sử dụng zero padding, chúng ta có thể loại bỏ các bộ lọc một cách linh hoạt. Hơn nữa, nếu tất cả các bộ lọc của lớp tích chập đầu tiên bị cắt tỉa, thì khối dư có thể được cắt tỉa.

5.2 CẮT TỈA CÓ CẤU TRÚC

Chúng tôi so sánh kết quả cắt tỉa của FFR với các phương pháp cắt tỉa tiên tiến gần đây trong Hình 5 và Bảng 1 và 2. Chúng tôi trình bày nghiên cứu ablation về các siêu tham số κ₁, κ₂ trong Phụ lục A.4.

Kết quả trên CIFAR-10. Bảng 1 cho thấy so sánh kết quả cắt tỉa của VGG16 và ResNet56 trên CIFAR-10. Để so sánh tốt hơn, trong Hình 5, chúng tôi vẽ các đường cong đánh đổi lỗi-giảm tham số và lỗi-giảm FLOP của VGG16 được huấn luyện với FFR cùng với kết quả của các phương pháp khác được hiển thị trong Bảng 1. Chúng ta có thể thấy rằng FFR có thể đạt được tỷ lệ cắt tỉa lớn hơn về tham số và FLOP. Cụ thể, chúng tôi cắt tỉa VGG16 với 90.2% giảm tham số và 61.2% giảm FLOP với sự giảm nhẹ (0.66%) về độ chính xác, và ResNet56 ở mức giảm FLOP lớn 60.6% với sự giảm độ chính xác 1.05%. Từ so sánh kết quả VGG16 trong Hình 5, phương pháp FFR có giảm lỗi thấp hơn về tham số và FLOP so với hầu hết các phương pháp hiện có, và đạt được tỷ lệ cắt tỉa lớn hơn. Lưu ý rằng phương pháp DCP (Zhuang et al., 2018) tiến hành cắt tỉa kênh và tinh chỉnh mạng từng giai đoạn để đạt được lỗi thấp hơn, trong khi trong FFR, chúng tôi chỉ thực hiện cắt tỉa có cấu trúc một lần với vài epoch tinh chỉnh. Chúng tôi cũng hình ảnh hóa các bản đồ đặc trưng của các bộ lọc được bảo tồn và đã cắt tỉa của lớp tích chập đầu tiên trong VGG16 sử dụng FFR trong Phụ lục A.3, Hình 7.

Hình 5: Đường cong đánh đổi lỗi-giảm tham số và lỗi-giảm FLOP của VGG16 được huấn luyện với FFR trên CIFAR-10, và so sánh với kết quả của SSS (Huang & Wang, 2018), DCP (Zhuang et al., 2018), FPGM (He et al., 2019), Hrank (Lin et al., 2020), PR (Zhuang et al., 2020).

Kết quả trên ImageNet. Bảng 2 cho thấy so sánh kết quả cắt tỉa của ResNet50 trên ImageNet, đây là một bộ dữ liệu quy mô lớn. Mặc dù chỉ có cắt tỉa một lần và vài epoch tinh chỉnh trong FFR, ResNet50 đã cắt tỉa FFR của chúng tôi đạt được giảm tham số và FLOP tương đương với các phương pháp cắt tỉa được đề xuất gần đây.

So sánh phương pháp luận. Về mặt phương pháp luận, phương pháp FFR của chúng tôi thao tác quỹ đạo kết nối các đặc trưng của các lớp ẩn, điều này khác với các phương pháp cắt tỉa dựa trên thông tin bản đồ đặc trưng, ví dụ Hu et al. (2016); Li et al. (2020); Lin et al. (2020) và khác với các phương pháp áp đặt điều chỉnh lên các tham số, ví dụ Huang & Wang (2018); Zhuang et al. (2020). Về tính đơn giản của triển khai, chúng tôi sử dụng SGD trong quá trình huấn luyện và áp dụng chiến lược cắt tỉa một lần thay vì các bước tối ưu hóa bổ sung (ví dụ, He et al. (2019)) hoặc cắt tỉa lặp đi lặp lại (Zhuang et al., 2018).

Bảng 1: Kết quả cắt tỉa trên CIFAR-10. '-' đại diện cho kết quả không được báo cáo. 'M' đại diện cho 1e6.

Mô hình | Phương pháp | Cơ sở | Đã cắt tỉa | Giảm Tham số | Giảm FLOP
--------|-------------|--------|------------|---------------|------------
        |             | Lỗi (%) | Lỗi (%) |              |           
VGG16   | Cơ sở (Của chúng tôi) | 6.10 | 6.10 | 0% (14.72M) | 0% (313M)
        | SSS (Huang & Wang, 2018) | 6.10 | 6.98 | 73.8% | 41.6%
        | DCP (Zhuang et al., 2018) | 6.01 | 5.84 | 47.9% | 50.0%
        | FPGM (He et al., 2019) | 6.42 | 6.46 | - | 34.2%
        | Hrank (Lin et al., 2020) | 6.04 | 7.66 | 82.1% | 65.3%
        | PR (Zhuang et al., 2020) | 6.12 | 6.08 | - | 54.0%
        | FFR(κ₁,κ₂ = 2e−7) | 6.10 | 6.76 | 90.2% | 61.2%
ResNet56| Cơ sở (Của chúng tôi) | 6.60 | 6.60 | 0% (0.86M) | 0% (126M)
        | CP (He et al., 2017) | 7.20 | 8.20 | - | 50.0%
        | DCP (Zhuang et al., 2018) | 6.20 | 6.51 | 49.2% | 49.7%
        | SFP (He et al., 2018) | 6.41 | 6.65 | - | 52.6%
        | FPGM (He et al., 2019) | 6.41 | 6.51 | - | 52.6%
        | Hrank (Lin et al., 2020) | 6.74 | 6.83 | 42.4% | 50.0%
        | PR (Zhuang et al., 2020) | 6.20 | 6.17 | - | 47.0%
        | FFR(κ₁,κ₂ = 1e−7) | 6.60 | 7.65 | 49.7% | 60.6%

Bảng 2: Kết quả cắt tỉa trên ImageNet. '-' đại diện cho kết quả không được báo cáo. 'M' đại diện cho 1e6.

Mô hình | Phương pháp | Giảm Top-1 Acc. | Giảm Top-5 Acc. | Giảm Tham số | Giảm FLOP
--------|-------------|------------------|------------------|----------------|------------
        |             | (%) | (%) |              |           
ResNet50| Cơ sở (Của chúng tôi) | 0 (71.56) | 0 (90.28) | 0% (25.56M) | 0% (4089M)
        | SSS (Huang & Wang, 2018) | 4.30 | 2.07 | 38.8% | 43.0%
        | DCP (Zhuang et al., 2018) | 1.06 | 0.61 | 51.5% | 55.6%
        | SFP (He et al., 2018) | 14.01 | 8.27 | - | 41.8%
        | FPGM (He et al., 2019) | 1.32 | 0.55 | - | 53.5%
        | Hrank (Lin et al., 2020) | 4.17 | 1.86 | 62.1% | 46.0%
        | PR (Zhuang et al., 2020) | 0.52 | - | - | 54.0%
        | FFR(κ₁,κ₂ = 5e−8) | 2.68 | 1.40 | 52.3% | 47.2%

6 KẾT LUẬN VÀ THẢO LUẬN

Trong bài báo này, chúng tôi đề xuất một phương pháp điều chỉnh đơn giản và hiệu quả (FFR) từ góc nhìn mới về quỹ đạo dữ liệu dọc theo mạng. FFR làm mượt quỹ đạo bằng cách áp đặt kiểm soát lên độ dài và tổng độ cong tuyệt đối của dòng đặc trưng, dẫn đến sự tăng đáng kể về độ thưa thớt có cấu trúc trong DNN. Chúng tôi thực hiện phân tích độ thưa thớt của FFR cho VGGNet và ResNet để xác thực phương pháp này. Kết quả thí nghiệm cho thấy FFR có thể tăng cường đáng kể độ thưa thớt có cấu trúc, cho phép chúng ta cắt tỉa các bộ lọc một cách hiệu quả trong một lần. Công việc tương lai có thể bao gồm: (1) thực hiện phân tích độ thưa thớt nghiêm ngặt cho FFR, và (2) tích hợp FFR với các phương pháp cắt tỉa khác để cải thiện thêm kết quả cắt tỉa. Vì trong FFR, điều chỉnh được áp đặt lên các đặc trưng, không dễ để xác định trước độ thưa thớt, mặc dù kết quả thí nghiệm sử dụng FFR đã cho thấy cải thiện độ thưa thớt đáng kể. Để cân bằng độ thưa thớt và độ chính xác, chúng ta cần điều chỉnh các siêu tham số κ₁, κ₂, vốn thay đổi theo mạng và bộ dữ liệu. Những cải thiện thêm về các khía cạnh này cũng sẽ được khám phá trong công việc tương lai.

--- TRANG 10 ---
Bản thảo
TÀI LIỆU THAM KHẢO

Jose M Alvarez và Mathieu Salzmann. Learning the number of neurons in deep networks. Trong D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, và R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/6e7d2da6d3953058db75714ac400b584-Paper.pdf.

Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, và David K Duvenaud. Neural ordinary differential equations. Trong S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, và R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.

Misha Denil, Babak Shakibi, Laurent Dinh, Marc Aurelio Ranzato, và Nando de Freitas. Predicting parameters in deep learning. Trong C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, và K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper/2013/file/7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf.

Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, và Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. CoRR, abs/1404.0736, 2014. URL http://arxiv.org/abs/1404.0736.

Weinan E. A proposal on machine learning via dynamical systems. Communications in Mathematics and Statistics, 5(1), 2017. ISSN 2194-6701. doi: 10.1007/s40304-017-0103-z.

Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. Trong 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=rJl-b3RcF7.

Susan Gao, Xin Liu, Lung-Sheng Chien, William Zhang, và Jose M. Alvarez. VACL: variance-aware cross-layer regularization for pruning deep residual networks. Trong 2019 IEEE/CVF International Conference on Computer Vision Workshops, ICCV Workshops 2019, Seoul, Korea (South), October 27-28, 2019, pp. 2980–2988. IEEE, 2019. doi: 10.1109/ICCVW.2019.00360. URL https://doi.org/10.1109/ICCVW.2019.00360.

Yunchao Gong, Liu Liu, Ming Yang, và Lubomir D. Bourdev. Compressing deep convolutional networks using vector quantization. CoRR, abs/1412.6115, 2014. URL http://arxiv.org/abs/1412.6115.

Yiwen Guo, Anbang Yao, và Yurong Chen. Dynamic network surgery for efficient dnns. Trong D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, và R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/2823f4797102ce1a1aec05359cc16dd9-Paper.pdf.

Song Han, Jeff Pool, John Tran, và William Dally. Learning both weights and connections for efficient neural network. Trong C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, và R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf.

Babak Hassibi và David Stork. Second order derivatives for network pruning: Optimal brain surgeon. Trong S. Hanson, J. Cowan, và C. Giles (eds.), Advances in Neural Information Processing Systems, volume 5. Morgan-Kaufmann, 1993. URL https://proceedings.neurips.cc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf.

--- TRANG 11 ---
Bản thảo

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. Trong 2015 IEEE International Conference on Computer Vision (ICCV), pp. 1026–1034, 2015. doi: 10.1109/ICCV.2015.123.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90.

Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, và Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks, 2018.

Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, và Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. Trong 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4335–4344, 2019. doi: 10.1109/CVPR.2019.00447.

Yihui He, Xiangyu Zhang, và Jian Sun. Channel pruning for accelerating very deep neural networks. Trong Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.

Hengyuan Hu, Rui Peng, Yu-Wing Tai, và Chi-Keung Tang. Network trimming: A data-driven neuron pruning approach towards efficient deep architectures. CoRR, abs/1607.03250, 2016. URL http://arxiv.org/abs/1607.03250.

Zehao Huang và Naiyan Wang. Data-driven sparse structure selection for deep neural networks. Trong Proceedings of the European Conference on Computer Vision (ECCV), September 2018.

Max Jaderberg, Andrea Vedaldi, và Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. Trong British Machine Vision Conference, 2014.

A. Krizhevsky và G. Hinton. Learning multiple layers of features from tiny images. Master's thesis, Department of Computer Science, University of Toronto, 2009.

Yann LeCun, John Denker, và Sara Solla. Optimal brain damage. Trong D. Touretzky (ed.), Advances in Neural Information Processing Systems, volume 2. Morgan-Kaufmann, 1990. URL https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf.

Hang Li, Chen Ma, Wei Xu, và Xue Liu. Feature statistics guided efficient filter pruning. CoRR, abs/2005.12193, 2020. URL https://arxiv.org/abs/2005.12193.

Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for efficient convnets. CoRR, abs/1608.08710, 2016. URL http://arxiv.org/abs/1608.08710.

Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, và Daniela Rus. Provable filter pruning for efficient neural networks. Trong 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=BJxkOlSYDH.

Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, và Ling Shao. Hrank: Filter pruning using high-rank feature map, 2020.

Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao, Qixiang Ye, Feiyue Huang, và David S. Doermann. Towards optimal structured CNN pruning via generative adversarial learning. Trong IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 2790–2799. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00290. URL http://openaccess.thecvf.com/content_CVPR_2019/html/Lin_Towards_Optimal_Structured_CNN_Pruning_via_Generative_Adversarial_Learning_CVPR_2019_paper.html.

Baoyuan Liu, Min Wang, H. Foroosh, M. Tappen, và M. Penksy. Sparse convolutional neural networks. Trong 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 806–814, 2015. doi: 10.1109/CVPR.2015.7298681.

--- TRANG 12 ---
Bản thảo

Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. Learning efficient convolutional networks through network slimming. Trong ICCV, 2017.

Christos Louizos, Max Welling, và Diederik P. Kingma. Learning sparse neural networks through l0 regularization, 2018.

Yiping Lu, Aoxiao Zhong, Quanzheng Li, và Bin Dong. Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations. CoRR, abs/1710.10121, 2017. URL http://arxiv.org/abs/1710.10121.

Jian-Hao Luo, Jianxin Wu, và Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. Trong IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pp. 5068–5076. IEEE Computer Society, 2017. doi: 10.1109/ICCV.2017.541. URL https://doi.org/10.1109/ICCV.2017.541.

Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, và Jan Kautz. Pruning convolutional neural networks for resource efficient transfer learning. CoRR, abs/1611.06440, 2016. URL http://arxiv.org/abs/1611.06440.

Karen Simonyan và Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. Trong Yoshua Bengio và Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.1556.

Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, và Hai Li. Learning structured sparsity in deep neural networks. Trong D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, và R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf.

Jianbo Ye, Xin Lu, Zhe L. Lin, và James Z. Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers. CoRR, abs/1802.00124, 2018. URL http://arxiv.org/abs/1802.00124.

Xin Yuan, Pedro Savarese, và Michael Maire. Growing efficient deep networks by structured continuous sparsification. CoRR, abs/2007.15353, 2020. URL https://arxiv.org/abs/2007.15353.

Hao Zhou, Jose M. Alvarez, và Fatih Porikli. Less is more: Towards compact cnns. Trong Bastian Leibe, Jiri Matas, Nicu Sebe, và Max Welling (eds.), Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, volume 9908 của Lecture Notes in Computer Science, pp. 662–677. Springer, 2016. doi: 10.1007/978-3-319-46493-0_40. URL https://doi.org/10.1007/978-3-319-46493-0_40.

Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Shuang, và Xiang Li. Neuron-level structured pruning using polarization regularizer. Trong H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, và H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9865–9877. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/703957b6dd9e3a7980e040bee50ded65-Paper.pdf.

Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, và Jin-Hui Zhu. Discrimination-aware channel pruning for deep neural networks. CoRR, abs/1810.11809, 2018. URL http://arxiv.org/abs/1810.11809.

--- TRANG 13 ---
Bản thảo
A PHỤ LỤC

A.1 CÀI ĐẶT CỦA VÍ DỤ MINH HỌA TRONG MỤC 3.1

Để hình ảnh hóa hiệu ứng làm mượt của FFR, chúng tôi sử dụng một ví dụ hai chiều. Trong Hình 1c, các cụm xanh lá và đỏ chứa 50 điểm dữ liệu được ghép đôi trong không gian hai chiều. Cụm xanh lá được phân bố đều trong một hình tròn có tâm (2,6) và bán kính 0.5. Cụm đỏ được tạo ra bằng cách dịch chuyển chính xác cụm xanh lá bốn đơn vị sang phải và bốn đơn vị xuống dưới, tức là trong một hình tròn có tâm (6,2). Chúng tôi sử dụng một ResNet để học ánh xạ dịch chuyển từ cụm xanh lá sang cụm đỏ. ResNet có năm khối dư kết nối đầy đủ. Ở đây mỗi khối dư chứa hai lớp tuyến tính và đầu ra một đặc trưng hai chiều. Chúng tôi huấn luyện ResNet không có và có FFR để học ánh xạ dưới cùng cài đặt để so sánh. Một quỹ đạo được tạo ra bằng cách bắt đầu từ một điểm dữ liệu đầu vào, kết nối năm đặc trưng đầu ra và kết thúc với đầu ra. Từ Hình 1c, các quỹ đạo của mạng được huấn luyện với FFR, được biểu thị bằng các đường cong màu cam, ngắn hơn và thẳng hơn so với những quỹ đạo của mạng được huấn luyện không có FFR, được biểu thị bằng các đường cong màu xanh dương. Chúng tôi vẽ dòng đặc trưng của ba dữ liệu thử nghiệm để so sánh. Nó cho thấy rằng FFR thực sự làm mượt dòng đặc trưng một cách hiệu quả.

A.2 BIỂU ĐỒ CHUẨN CÁC BẢN ĐỒ ĐẶC TRƯNG TRONG MỤC 4.1

Chúng tôi hiển thị biểu đồ chuẩn L1 của 256 bản đồ đặc trưng được tạo ra bởi lớp tích chập thứ năm và 512 bản đồ đặc trưng được tạo ra bởi lớp tích chập thứ tám trong VGG16 được huấn luyện có và không có FFR trên CIFAR-10 trong Hình 6a và 6b tương ứng. Phù hợp với tuyên bố trong văn bản chính, mạng được huấn luyện dưới FFR học được các đặc trưng thưa thớt hơn nhiều với chuẩn giá trị zero so với mạng cơ sở (VGG16 được huấn luyện không có FFR). FFR cải thiện độ thưa thớt đặc trưng góp phần vào độ thưa thớt cấu trúc trong DNN.

(a) 256 bản đồ đặc trưng của lớp tích chập thứ năm.

(b) 512 bản đồ đặc trưng của lớp tích chập thứ tám.

Hình 6: Bản đồ đặc trưng VGG16 được huấn luyện có và không có FFR trên CIFAR-10: biểu đồ chuẩn L1.

--- TRANG 14 ---
Bản thảo
A.3 HÌNH ẢNH HÓA CÁC BẢN ĐỒ ĐẶC TRƯNG

(a) Hình ảnh đầu vào

(b) Bản đồ đặc trưng của các bộ lọc được bảo tồn

(c) Bản đồ đặc trưng của các bộ lọc đã cắt tỉa

Hình 7: Hình ảnh hóa bản đồ đặc trưng của các lớp tích chập đầu tiên trong VGG16 sử dụng FFR.

A.4 NGHIÊN CỨU ABLATION

Chúng tôi điều chỉnh các siêu tham số κ₁, κ₂ với các thí nghiệm. Việc lựa chọn các siêu tham số phụ thuộc vào kiến trúc của DNN và quy mô của đầu vào: đối với DNN có nhiều lớp hơn và đầu vào có độ phân giải cao hơn, chúng tôi sử dụng κ₁, κ₂ nhỏ hơn. Chúng tôi huấn luyện VGG16 dưới FFR với các siêu tham số khác nhau trên CIFAR-10 và tiến hành các thí nghiệm cắt tỉa.

Chúng tôi liệt kê kết quả cắt tỉa của VGG16 trên CIFAR-10. Dữ liệu giảm tham số, giảm FLOP và độ chính xác được thu thập bằng cách sử dụng cùng ngưỡng để cắt tỉa VGG16 được huấn luyện dưới FFR với các siêu tham số κ₁, κ₂ khác nhau và tinh chỉnh mô hình đã cắt tỉa 30 epoch.

Như được hiển thị trong Bảng 3, cơ sở (VGG16 được huấn luyện không có FFR) không thể bảo tồn độ chính xác dưới cùng cài đặt cắt tỉa và tinh chỉnh. Hơn nữa, mạng được huấn luyện dưới FFR với κ₁, κ₂ phù hợp đạt được tỷ lệ cắt tỉa lớn trong khi duy trì độ chính xác.

Bảng 3: Nghiên cứu ablation về κ₁, κ₂. Kết quả cắt tỉa của VGG16 trên CIFAR10. 'M' đại diện cho 1e6.

Siêu tham số | Lỗi(%) | Giảm Tham số | Giảm FLOP
-------------|---------|---------------|------------
Cơ sở | 6.10 | 0%(14.72M) | 0%(313M)
κ₁,κ₂ = 0 | 34.57 | 74.1% | 35.0%
κ₁,κ₂ = 5e⁻⁸ | 6.36 | 76.5% | 36.4%
κ₁,κ₂ = 1e⁻⁷ | 6.45 | 77.5% | 38.6%
κ₁,κ₂ = 2e⁻⁷ | 6.32 | 85.0% | 47.5%
