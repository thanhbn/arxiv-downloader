Cắt tỉa mạng thần kinh sử dụng suy luận Bayesian

Sunil Mathew
Khoa Khoa học Toán học và Thống kê, Đại học Marquette
và
Daniel B. Rowe
Khoa Khoa học Toán học và Thống kê, Đại học Marquette
7 tháng 8, 2023

Tóm tắt
Cắt tỉa mạng thần kinh là một kỹ thuật cực kỳ hiệu quả nhằm giảm yêu cầu tính toán và bộ nhớ của các mạng thần kinh lớn. Trong bài nghiên cứu này, chúng tôi trình bày một phương pháp mới để cắt tỉa mạng thần kinh sử dụng suy luận Bayesian, có thể tích hợp liền mạch vào quy trình huấn luyện. Phương pháp được đề xuất của chúng tôi tận dụng các xác suất hậu nghiệm của mạng thần kinh trước và sau khi cắt tỉa, cho phép tính toán các hệ số Bayes. Các hệ số Bayes được tính toán hướng dẫn việc cắt tỉa lặp đi lặp lại. Thông qua các đánh giá toàn diện được thực hiện trên nhiều tiêu chuẩn, chúng tôi chứng minh rằng phương pháp của chúng tôi đạt được các mức độ thưa thớt mong muốn trong khi duy trì độ chính xác cạnh tranh.

Từ khóa: Lựa chọn mô hình Bayesian, Hệ số Bayes, Lịch trình cắt tỉa Bayesian

1 Giới thiệu

Trong mạng thần kinh nhân tạo (ANN) và học máy (ML), các tham số đại diện cho những gì mạng đã học được từ dữ liệu. Số lượng tham số trong một mạng thần kinh có thể xác định khả năng học của nó. Với những tiến bộ trong khả năng phần cứng, hiện tại chúng ta có thể định nghĩa các mô hình lớn hơn với hàng triệu tham số. Thử thách Nhận dạng Thị giác Quy mô Lớn ImageNet (ILSVRC) và những người chiến thắng của nó qua các năm cho thấy cách tỷ lệ lỗi đã giảm khi số lượng tham số và kết nối trong mạng thần kinh tăng lên. Ví dụ, vào năm 2012, AlexNet (Krizhevsky et al., 2012), một trong những mạng thần kinh tích chập (CNN), có hơn 60M tham số. Mô hình ngôn ngữ lớn, Transformer Tiền huấn luyện Sinh thành 3 (GPT-3) (Brown et al., 2020), bao gồm 175 tỷ tham số.

Mặc dù các mạng thần kinh sâu với số lượng lớn tham số nắm bắt được các mẫu phức tạp bên dưới, số lượng lớn các kết nối có thể gây ra các thách thức tính toán, quá khớp và thiếu khả năng tổng quát hóa. Để giải quyết những vấn đề này, nhiều phương pháp khác nhau đã được phát triển.

Cắt tỉa mạng thần kinh là một phương pháp được sử dụng rộng rãi để giảm kích thước của các mô hình học sâu, từ đó giảm độ phức tạp tính toán và dấu chân bộ nhớ (LeCun et al., 1989; Han et al., 2015; Liu et al., 2018). Cắt tỉa rất quan trọng để triển khai các mô hình lớn trên các thiết bị có tài nguyên hạn chế như máy tính cá nhân, điện thoại di động và máy tính bảng. Cắt tỉa cũng có thể được sử dụng để giảm dấu chân carbon của các mô hình học sâu bằng cách giảm yêu cầu tính toán (Strubell et al., 2019). Cắt tỉa cũng có thể được sử dụng để cải thiện khả năng diễn giải của các mô hình học sâu bằng cách loại bỏ các neuron hoặc kết nối thừa (Han et al., 2015).

Các phương pháp cắt tỉa có thể được phân loại thành ba loại chính, cắt tỉa trọng số, cắt tỉa neuron và cắt tỉa bộ lọc (Han et al., 2015; Srivastava et al., 2014; Li et al., 2017; He et al., 2018). Cắt tỉa trọng số liên quan đến việc loại bỏ các trọng số riêng lẻ khỏi mạng dựa trên độ lớn hoặc các tiêu chí khác, cắt tỉa neuron và cắt tỉa bộ lọc liên quan đến việc loại bỏ toàn bộ các neuron hoặc bộ lọc không quan trọng. Mặc dù các phương pháp cắt tỉa có thể giảm hiệu quả kích thước mạng và cải thiện hiệu suất, chúng thường thiếu một phương pháp có nguyên tắc để lựa chọn các trọng số hoặc neuron quan trọng nhất (Blalock et al., 2020).

Trong các mạng thần kinh Bayesian, các trọng số của mạng được coi như các biến ngẫu nhiên với phân phối tiên nghiệm, có thể được cập nhật để có được phân phối hậu nghiệm sử dụng quy tắc Bayes. Nó cho phép chúng ta định lượng sự không chắc chắn liên quan đến mỗi trọng số và lựa chọn các trọng số quan trọng nhất dựa trên mức độ liên quan của chúng đối với nhiệm vụ mà mạng đang được huấn luyện. Phân phối hậu nghiệm phản ánh niềm tin cập nhật của chúng ta về các trọng số dựa trên dữ liệu quan sát được và có thể được sử dụng để tính toán xác suất của mỗi trọng số quan trọng cho nhiệm vụ hiện tại. Suy luận biến phân, liên quan đến việc tối thiểu hóa sự khác biệt Kullback-Leibler (KL) giữa hậu nghiệm thực và hậu nghiệm xấp xỉ, là một phương pháp phổ biến để xấp xỉ phân phối hậu nghiệm cho việc cắt tỉa mạng thần kinh (Dusenberry et al., 2019; Blundell et al., 2015). Các phương pháp khác bao gồm các phương pháp Monte Carlo và lấy mẫu Markov chain Monte Carlo (MCMC) (Molchanov et al., 2019). Tuy nhiên, những phương pháp này tốn kém về mặt tính toán và có thể khó mở rộng cho các mạng lớn.

Trong công trình này, chúng tôi đề xuất một thuật toán cắt tỉa Bayesian dựa trên kiểm định giả thuyết Bayesian. Nó cung cấp một phương pháp có nguyên tắc để cắt tỉa mạng thần kinh đến kích thước mong muốn mà không hy sinh độ chính xác. Chúng tôi so sánh hai mô hình mạng thần kinh tại mỗi lần lặp huấn luyện, mạng gốc chưa cắt tỉa và mạng đã cắt tỉa. So sánh này giúp chúng ta xác định mô hình nào phù hợp với dữ liệu tốt hơn. Tỷ lệ của các xác suất hậu nghiệm của mạng đã cắt tỉa với các xác suất hậu nghiệm của mạng chưa cắt tỉa (hệ số Bayes) sau đó có thể được sử dụng để xác định có nên cắt tỉa mạng thêm hoặc bỏ qua việc cắt tỉa tại lần lặp tiếp theo. Phương pháp này cho phép chúng ta triển khai phương pháp này trong các mạng thần kinh thường xuyên mà không cần tham số hóa bổ sung như trong trường hợp của các mạng thần kinh Bayesian.

1.1 Cắt tỉa Mạng thần kinh sử dụng Suy luận Bayesian

Hệ thống cắt tỉa, được thấy trong Hình 1, tích hợp cắt tỉa vào quá trình huấn luyện. Dữ liệu huấn luyện được chia thành các lô và được xử lý bởi mạng thần kinh thông qua một lượt truyền tiến, bao gồm phép nhân ma trận và các kích hoạt phi tuyến. Đầu ra của mạng sau đó được so sánh với các nhãn thực tế để tính toán tổn thất. Các trọng số của mạng được điều chỉnh thông qua một lượt truyền ngược sử dụng một bộ tối ưu hóa như Stochastic Gradient Descent (SGD) hoặc Adam (Kingma và Ba, 2015). Sau mỗi epoch, các trọng số được cắt tỉa sử dụng thuật toán cắt tỉa, và các trọng số đã cắt tỉa được sử dụng trong các epoch tiếp theo. Thuật toán cắt tỉa dựa trên kiểm định giả thuyết Bayesian, đây là một khung thống kê có thể được sử dụng để so sánh hai mô hình, hai cấu hình mạng trong trường hợp này, để xác định mô hình nào phù hợp với dữ liệu tốt hơn.

Hình 1: Sơ đồ khối hệ thống cắt tỉa.

Để kiểm tra giả thuyết rằng mạng đã cắt tỉa phù hợp với dữ liệu tốt hơn mạng chưa cắt tỉa, chúng tôi định nghĩa giả thuyết null là mạng chưa cắt tỉa phù hợp với dữ liệu tốt hơn (θ=ψ) và giả thuyết thay thế là mạng đã cắt tỉa phù hợp với dữ liệu tốt hơn (θ=ϕ). Hệ số Bayes, là tỷ lệ của xác suất hậu nghiệm của giả thuyết thay thế với xác suất hậu nghiệm của giả thuyết null, được tính như sau:

Hệ số Bayes = P(θ=ϕ|D) / P(θ=ψ|D)

Ở đây, D đại diện cho dữ liệu huấn luyện.

Xác suất hậu nghiệm của giả thuyết null (P(θ=ψ|D)) được tính như:

P(θ=ψ|D) = P(D|θ=ψ)P(θ=ψ) / P(D)

Tương tự, xác suất hậu nghiệm của giả thuyết thay thế (P(θ=ϕ|D)) được tính như:

P(θ=ϕ|D) = P(D|θ=ϕ)P(θ=ϕ) / P(D)

Hệ số Bayes sau đó được tính như tỷ lệ của các xác suất hậu nghiệm:

Hệ số Bayes = P(D|θ=ϕ)P(θ=ϕ) / P(D|θ=ψ)P(θ=ψ)

Hệ số Bayes lớn hơn 1 cho thấy mạng đã cắt tỉa phù hợp với dữ liệu tốt hơn, trong khi giá trị nhỏ hơn 1 cho thấy mạng chưa cắt tỉa phù hợp với dữ liệu tốt hơn.

Đối với một bài toán phân loại, khả năng của dữ liệu được cho bởi hàm tổn thất categorical cross-entropy:

logp(ypred|ytrue) = log C(softmax(ypred))ytrue

Ở đây, ypred đại diện cho dự đoán của mạng thần kinh cho các lớp, và ytrue là thực tế. Một tiên nghiệm Gaussian với trung bình µ và phương sai σ² được sử dụng cho các trọng số:

p(w) = N(µ, σ²)

Log tiên nghiệm và log khả năng cho các tham số trọng số được sử dụng để tính toán phân phối log hậu nghiệm của các trọng số:

logp(w|D) = log p(D|w) + log p(w)

Log hậu nghiệm được tính trước và sau khi cắt tỉa trọng số để tính toán hệ số Bayes. Nếu hệ số Bayes vượt quá một ngưỡng được định nghĩa trước, một phần trăm nhất định (r) của các trọng số được cắt tỉa như:

wnew = wold ⊙ m                    (1)

trong đó ⊙ đại diện cho phép nhân từng phần tử, wold là ma trận trọng số cũ, và m là mặt nạ nhị phân cho biết những trọng số nào nên được cắt tỉa (tức là có giá trị 0) và những trọng số nào nên được giữ lại (tức là có giá trị 1). Ma trận kết quả wnew có cùng kích thước với wold, nhưng với một số trọng số đã được cắt tỉa. Thuật toán 1 phác thảo quá trình cắt tỉa Bayesian.

Thuật toán 1 Thuật toán Cắt tỉa Bayesian
Đầu vào: Mạng thần kinh đã huấn luyện f(·, w), tỷ lệ cắt tỉa r, tập dữ liệu D = (xi, yi)ⁿi=1, β ngưỡng hệ số Bayes
Đầu ra: Mạng thần kinh đã cắt tỉa fr(·, w)
Tính toán xác suất hậu nghiệm của các trọng số trước khi cắt tỉa
if BF01 > β then
    Cắt tỉa r phần trăm trọng số của f(·, w)
    Trả về fr(·, w)
end if
Tính toán xác suất hậu nghiệm của các trọng số sau khi cắt tỉa
Tính toán hệ số Bayes sử dụng các xác suất hậu nghiệm trước và sau khi cắt tỉa

Trong các phần tiếp theo, chúng tôi giới thiệu hai thuật toán cắt tỉa sử dụng khung này: cắt tỉa ngẫu nhiên, lựa chọn ngẫu nhiên các trọng số để cắt tỉa, và cắt tỉa theo độ lớn, cắt tỉa các trọng số dựa trên độ lớn của chúng.

Cắt tỉa ngẫu nhiên Bayesian

Cắt tỉa ngẫu nhiên là một thuật toán cắt tỉa đơn giản lựa chọn ngẫu nhiên các trọng số để cắt tỉa. Ở đây chúng tôi đặt tỷ lệ cắt tỉa là mức độ thưa thớt mong muốn mà chúng ta đang tìm cách đạt được. Sau một epoch, chúng ta đếm số lượng tham số khác không trong mạng và ngẫu nhiên đặt bằng không đủ tham số để đạt được mức độ thưa thớt mong muốn. Thuật toán được tóm tắt trong Thuật toán 2.

Thuật toán 2 Cắt tỉa Ngẫu nhiên Bayesian
1: f(·, w): Mô hình mạng thần kinh với tham số w
2: r: Mức độ thưa thớt mong muốn, β ngưỡng hệ số Bayes
3: Tính toán xác suất hậu nghiệm log p(w|D)
4: if BF01 > β then
5:     for all trọng số wi ∈ w do
6:         n ← size(wi)
7:         số lượng trọng số cần cắt tỉa, k ← (n × r)
8:         I ← chỉ số của các trọng số khác không
9:         nz ← số lượng trọng số bằng không
10:        k' ← k - nz
11:        J ← mẫu ngẫu nhiên(I, k')
12:        đặt các phần tử trong wi tại chỉ số J bằng không
13:    end for
14: end if
15: Tính toán xác suất hậu nghiệm log p(w|D) sau khi cắt tỉa
16: Tính toán hệ số Bayes BF01

Cắt tỉa theo độ lớn Bayesian

Cắt tỉa theo độ lớn là một thuật toán cắt tỉa lựa chọn các trọng số để cắt tỉa dựa trên độ lớn của chúng. Điều này có thể được xem như cắt tỉa các trọng số ít quan trọng hơn. Ở đây chúng tôi đặt tỷ lệ cắt tỉa là mức độ thưa thớt mong muốn mà chúng ta đang tìm cách đạt được. Các trọng số thấp nhất tương ứng với mức độ thưa thớt mong muốn được cắt tỉa để có được mạng đã cắt tỉa. Thuật toán được tóm tắt trong Thuật toán 3.

Thuật toán 3 Cắt tỉa theo Độ lớn Bayesian
1: f(·, w): Mô hình mạng thần kinh với tham số w
2: r: Mức độ thưa thớt mong muốn, β ngưỡng hệ số Bayes
3: Tính toán xác suất hậu nghiệm log p(w|D)
4: if BF01 > β then
5:     for all trọng số wi ∈ w do
6:         n ← size(wi)
7:         số lượng trọng số cần cắt tỉa, k ← (n × r)
8:         wi ← sort(wi)
9:         đặt k phần tử trong wi bằng không
10:    end for
11: end if
12: Tính toán xác suất hậu nghiệm log p(w|D) sau khi cắt tỉa
13: Tính toán hệ số Bayes BF01

Thiết lập Thí nghiệm

Để đánh giá hiệu suất của Cắt tỉa Ngẫu nhiên Bayesian và Cắt tỉa theo Độ lớn Bayesian, chúng tôi tiến hành các thí nghiệm trên ba tập dữ liệu và hai kiến trúc mạng thần kinh cho năm mức độ thưa thớt mong muốn khác nhau. Các tập dữ liệu được sử dụng là MNIST (Lecun et al., 1998), MNIST Fashion (Xiao et al., 2017) và CIFAR-10 (Krizhevsky, 2009). Các kiến trúc mạng thần kinh là Mạng Kết nối Đầy đủ (FCN) và Mạng Thần kinh Tích chập (CNN). Năm mức độ thưa thớt khác nhau là 25%, 50%, 75%, 90% và 99%. Chúng tôi sử dụng tỷ lệ học 0.001 và kích thước lô 64 cho tất cả các thí nghiệm. Tiền xử lý dữ liệu chỉ bao gồm chuẩn hóa tập dữ liệu và không bao gồm bất kỳ tăng cường dữ liệu nào như cắt ngẫu nhiên hoặc lật hình ảnh để có ít biến nhiễu hơn trong các nghiên cứu chúng tôi tiến hành để quan sát tác động của thuật toán cắt tỉa. Chúng tôi huấn luyện mạng trong 25 epoch trên tập huấn luyện và đánh giá hiệu suất của nó trên tập kiểm tra. Chúng tôi đánh giá hiệu suất của mỗi phương pháp dựa trên độ chính xác của các dự đoán mà nó đưa ra cho các lớp mục tiêu sử dụng tập kiểm tra. Mỗi thí nghiệm được lặp lại 5 lần và trung bình và độ lệch chuẩn của độ chính xác được báo cáo.

Các phần tiếp theo mô tả các kiến trúc mạng thần kinh được sử dụng trong các thí nghiệm của chúng tôi.

Kiến trúc Mạng Thần kinh

Hai kiến trúc mạng thần kinh được sử dụng trong các thí nghiệm của chúng tôi là Mạng Kết nối Đầy đủ (FCN) và Mạng Thần kinh Tích chập (CNN). Cùng một kiến trúc được sử dụng cho cả ba tập dữ liệu. FCN bao gồm hai lớp ẩn. Đầu ra của lớp kết nối đầy đủ cuối cùng được đưa vào một lớp softmax để có được các xác suất lớp. CNN bao gồm hai lớp tích chập với 32 và 64 bộ lọc tương ứng theo sau là hai lớp kết nối đầy đủ. Mỗi lớp tích chập được theo sau bởi một lớp max pooling với kích thước kernel là 2 và stride là 2. Đầu ra của lớp max pooling thứ hai được làm phẳng và đưa vào các lớp kết nối đầy đủ. Đầu ra của lớp kết nối đầy đủ được đưa vào một lớp softmax để có được các xác suất lớp.

Kiến trúc mạng của mạng kết nối đầy đủ (FCN) được thấy trong Hình 2.

Hình 2: Kiến trúc mạng thần kinh kết nối đầy đủ

Kiến trúc mạng của mạng thần kinh tích chập (CNN) được thấy trong Hình 3.

Hình 3: Kiến trúc mạng thần kinh tích chập

1.2 Kết quả

Các phần tiếp theo trình bày kết quả của các thí nghiệm. Kết quả được trình bày theo thứ tự sau: (1) tập dữ liệu MNIST, (2) tập dữ liệu MNIST-Fashion, và (3) tập dữ liệu CIFAR-10. Kết quả được trình bày dưới dạng các đường cong học và một bảng với độ chính xác cho các mức độ thưa thớt khác nhau cho mô hình FCN và CNN. Độ chính xác là phần trăm hình ảnh được phân loại đúng trong tập kiểm tra. Độ thưa thớt là phần trăm trọng số được cắt tỉa trong mạng. Kết quả được so sánh với đường cơ sở, là mô hình được huấn luyện mà không cắt tỉa, và phiên bản không Bayesian của phương pháp cắt tỉa.

MNIST

Hình 4 hiển thị các đường cong học cho cắt tỉa ngẫu nhiên, cắt tỉa theo độ lớn dưới khung Bayesian so với đường cơ sở trong một mạng kết nối đầy đủ (FCN) được huấn luyện trên tập dữ liệu MNIST. Ở đây mức độ thưa thớt mong muốn là 75%. Hình có hai biểu đồ con. Một biểu đồ hiển thị tổn thất huấn luyện và xác thực như một hàm của số epoch, biểu đồ khác (bên phải) hiển thị hệ số Bayes, độ thưa thớt như một hàm của số epoch.

Hình 4: Đường cong học MNIST (FCN 75%) cho phương pháp cắt tỉa Bayesian.

Tổn thất huấn luyện là tổn thất trung bình trên tập huấn luyện, và tổn thất xác thực là tổn thất trung bình trên tập xác thực. Hình cho thấy tổn thất huấn luyện giảm khi số epoch tăng, và tổn thất xác thực bắt đầu giảm trong khoảng 5 epoch. Tổn thất huấn luyện giảm nhanh hơn tổn thất xác thực, điều này cho thấy mô hình đang quá khớp dữ liệu huấn luyện. Khi việc cắt tỉa bắt đầu, nó ảnh hưởng đến tổn thất huấn luyện và xác thực của cả cắt tỉa ngẫu nhiên và cắt tỉa theo độ lớn như thấy trong các đường cong. Có những dao động lớn trong các giá trị tổn thất cho cắt tỉa ngẫu nhiên như thấy trong hình. Hệ số Bayes bắt đầu giảm khi số epoch tăng và độ thưa thớt của mạng trở nên ổn định đối với cắt tỉa theo độ lớn, nhưng nó vẫn dao động đối với cắt tỉa ngẫu nhiên và cho thấy xu hướng tăng cho hệ số Bayes gợi ý rằng cắt tỉa ngẫu nhiên Bayesian phù hợp với dữ liệu tốt hơn các phương pháp khác.

Hình 5 hiển thị độ chính xác xác thực của cắt tỉa ngẫu nhiên cho các mức độ thưa thớt khác nhau. Đối với độ thưa thớt 25%, độ chính xác xác thực có vẻ cao nhất. Sau đó khi mức độ thưa thớt tăng, độ chính xác xác thực bắt đầu giảm. Cho đến độ thưa thớt 90%, độ chính xác xác thực vẫn có xu hướng giảm và chống lại quá khớp so với đường cơ sở. Mạng chỉ bắt đầu trở nên tệ hơn ở độ thưa thớt 99%.

Hình 5: Tổn thất xác thực của cắt tỉa ngẫu nhiên cho các mức độ thưa thớt khác nhau.

Hình 6: Tổn thất xác thực của cắt tỉa theo độ lớn cho các mức độ thưa thớt khác nhau.

Hình 6 hiển thị độ chính xác xác thực của cắt tỉa theo độ lớn cho các mức độ thưa thớt khác nhau. Đối với độ thưa thớt 25%, độ chính xác xác thực vẫn tương tự như đường cơ sở. Sau đó khi mức độ thưa thớt tăng, độ chính xác xác thực bắt đầu cải thiện, nhưng mạng vẫn quá khớp dữ liệu cho đến khi 99% tham số được cắt tỉa.

Hình 7 hiển thị các đường cong học cho cắt tỉa ngẫu nhiên, cắt tỉa theo độ lớn dưới khung Bayesian so với đường cơ sở trong một mạng thần kinh tích chập (CNN) được huấn luyện trên tập dữ liệu MNIST. Số lượng tham số trong CNN tương đối lớn hơn so với FCN. Điều này khiến tác động của quá khớp được thấy muộn hơn một chút trong giai đoạn huấn luyện và ít quá khớp hơn so với FCN ở độ thưa thớt 75%. Hệ số Bayes cho cắt tỉa ngẫu nhiên cao hơn so với cắt tỉa theo độ lớn, điều này gợi ý rằng cắt tỉa ngẫu nhiên Bayesian phù hợp với dữ liệu tốt hơn.

Hình 7: Đường cong học MNIST (CNN 75%) cho phương pháp cắt tỉa Bayesian.

Hình 8: Tổn thất xác thực của cắt tỉa ngẫu nhiên cho các mức độ thưa thớt khác nhau.

Hình 8 hiển thị độ chính xác xác thực của cắt tỉa ngẫu nhiên cho các mức độ thưa thớt khác nhau. Vì số lượng tham số của CNN lớn hơn so với FCN, độ chính xác xác thực vẫn tương tự như đường cơ sở cho đến độ thưa thớt 90%. Sau đó khi mức độ thưa thớt tăng, độ chính xác xác thực bắt đầu giảm.

Hình 9: Tổn thất xác thực của cắt tỉa theo độ lớn cho các mức độ thưa thớt khác nhau.

Hình 9 hiển thị độ chính xác xác thực của cắt tỉa theo độ lớn cho các mức độ thưa thớt khác nhau. Thậm chí cắt tỉa 99% tham số cũng không ảnh hưởng đến độ chính xác xác thực của CNN. Điều này là do CNN có số lượng tham số khổng lồ và mạng quá khớp dữ liệu ngay cả sau khi cắt tỉa 99% tham số.

MNIST Fashion

Hình 10 hiển thị các đường cong học cho cắt tỉa ngẫu nhiên, cắt tỉa theo độ lớn dưới khung Bayesian so với đường cơ sở trong một mạng kết nối đầy đủ (FCN) được huấn luyện trên tập dữ liệu MNIST Fashion. Ở đây mức độ thưa thớt mong muốn là 90%. Hình có hai biểu đồ con. Một biểu đồ hiển thị tổn thất huấn luyện và xác thực như một hàm của số epoch, biểu đồ khác (bên phải) hiển thị hệ số Bayes, độ thưa thớt như một hàm của số epoch.

Hình 10: Đường cong học MNIST-Fashion (FCN 90%) cho phương pháp cắt tỉa Bayesian.

Tổn thất huấn luyện là tổn thất trung bình trên tập huấn luyện, và tổn thất xác thực là tổn thất trung bình trên tập xác thực. Hình cho thấy tổn thất huấn luyện giảm khi số epoch tăng, và tổn thất xác thực bắt đầu giảm trong khoảng 5 epoch. Tổn thất huấn luyện giảm nhanh hơn tổn thất xác thực, điều này cho thấy mô hình đang quá khớp dữ liệu huấn luyện. Khi việc cắt tỉa bắt đầu, nó ảnh hưởng đến tổn thất huấn luyện và xác thực của cả cắt tỉa ngẫu nhiên và cắt tỉa theo độ lớn như thấy trong các đường cong. Có những dao động lớn trong các giá trị tổn thất cho cắt tỉa ngẫu nhiên. Hệ số Bayes bắt đầu giảm khi số epoch tăng và độ thưa thớt của mạng trở nên ổn định đối với cắt tỉa theo độ lớn, nhưng nó vẫn dao động đối với cắt tỉa ngẫu nhiên. Mô hình cắt tỉa ngẫu nhiên Bayesian phù hợp với dữ liệu tốt hơn mô hình cắt tỉa theo độ lớn.

Hình 11: Tổn thất xác thực của cắt tỉa ngẫu nhiên cho các mức độ thưa thớt khác nhau.

Hình 11 hiển thị độ chính xác xác thực của cắt tỉa ngẫu nhiên cho các mức độ thưa thớt khác nhau. Tương tự như tập dữ liệu MNIST, tổn thất xác thực thấp nhất đối với độ thưa thớt 25%. Sau đó khi mức độ thưa thớt tăng, độ chính xác xác thực bắt đầu giảm.

Hình 12: Tổn thất xác thực của cắt tỉa theo độ lớn cho các mức độ thưa thớt khác nhau.

Hình 12 hiển thị độ chính xác xác thực của cắt tỉa theo độ lớn cho các mức độ thưa thớt khác nhau. Các mức độ thưa thớt cao hơn cải thiện độ chính xác xác thực của FCN. Tác động của quá khớp được giảm khi số lượng tham số được giảm.

Hình 13: Đường cong học MNIST-Fashion (CNN 90%) cho phương pháp cắt tỉa Bayesian.

Hình 13 hiển thị các đường cong học cho cắt tỉa ngẫu nhiên, cắt tỉa theo độ lớn dưới khung Bayesian so với đường cơ sở trong một mạng thần kinh tích chập (CNN) được huấn luyện trên tập dữ liệu MNIST Fashion. Ở đây mức độ thưa thớt mong muốn là 90%. Hình có hai biểu đồ con. Một biểu đồ hiển thị tổn thất huấn luyện và xác thực như một hàm của số epoch, biểu đồ khác (bên phải) hiển thị hệ số Bayes, độ thưa thớt như một hàm của số epoch.

Số lượng tham số trong CNN tương đối lớn hơn so với FCN. Điều này khiến tác động của quá khớp được thấy muộn hơn một chút trong giai đoạn huấn luyện. Xu hướng trong các đường cong học tương tự như FCN. Độ chính xác xác thực cho cắt tỉa ngẫu nhiên giảm ở đầu huấn luyện và bắt đầu cải thiện khi huấn luyện tiến triển. Hệ số Bayes bắt đầu giảm khi số epoch tăng và độ thưa thớt của mạng trở nên ổn định đối với cắt tỉa theo độ lớn, nhưng nó vẫn dao động đối với cắt tỉa ngẫu nhiên và cho thấy xu hướng tăng cho hệ số Bayes. Mô hình cắt tỉa ngẫu nhiên Bayesian phù hợp với dữ liệu tốt hơn mô hình cắt tỉa theo độ lớn.

Hình 14: Tổn thất xác thực của cắt tỉa ngẫu nhiên cho các mức độ thưa thớt khác nhau.

Hình 14 hiển thị độ chính xác xác thực của cắt tỉa ngẫu nhiên cho các mức độ thưa thớt khác nhau. Xu hướng tương tự như tập dữ liệu MNIST. Độ chính xác xác thực tốt hơn đối với độ thưa thớt 25% và giảm khi mức độ thưa thớt tăng. Các mức độ thưa thớt lên đến 90% giúp giảm tác động của quá khớp.

Hình 15: Tổn thất xác thực của cắt tỉa theo độ lớn cho các mức độ thưa thớt khác nhau.

Hình 15 hiển thị độ chính xác xác thực của cắt tỉa theo độ lớn cho các mức độ thưa thớt khác nhau. Tương tự như tập dữ liệu MNIST, cắt tỉa theo độ lớn giúp giảm tác động của quá khớp. Tổn thất xác thực tiếp tục cải thiện khi đạt được độ thưa thớt 99%.

CIFAR-10

Hình 16 hiển thị các đường cong học cho cắt tỉa ngẫu nhiên, cắt tỉa theo độ lớn dưới khung Bayesian so với đường cơ sở trong một mạng kết nối đầy đủ (FCN) được huấn luyện trên tập dữ liệu CIFAR-10. Ở đây mức độ thưa thớt mong muốn được đặt là 90%. Hình có hai biểu đồ con. Một biểu đồ hiển thị tổn thất huấn luyện và xác thực như một hàm của số epoch, biểu đồ khác (bên phải) hiển thị hệ số Bayes, độ thưa thớt như một hàm của số epoch.

Hình 16: Đường cong học CIFAR-10 (FCN 90%) cho phương pháp cắt tỉa Bayesian.

Không giống như tập dữ liệu MNIST, Fashion, các hình ảnh đầu vào của tập dữ liệu CIFAR-10 có kích thước 32x32x3. Điều này khiến số lượng tham số trong FCN lớn hơn nhiều so với tập dữ liệu MNIST, Fashion. Điều này khiến tác động của quá khớp được thấy muộn hơn một chút trong giai đoạn huấn luyện. Xu hướng trong các đường cong học tương tự như tập dữ liệu MNIST, Fashion. Độ chính xác xác thực cho cắt tỉa ngẫu nhiên giảm ở đầu huấn luyện và bắt đầu cải thiện khi huấn luyện tiến triển. Hệ số Bayes bắt đầu giảm khi số epoch tăng và độ thưa thớt của mạng trở nên ổn định cho cả cắt tỉa theo độ lớn và cắt tỉa ngẫu nhiên.

Hình 17: Tổn thất xác thực của cắt tỉa ngẫu nhiên cho các mức độ thưa thớt khác nhau.

Hình 17 hiển thị độ chính xác xác thực của cắt tỉa ngẫu nhiên cho các mức độ thưa thớt khác nhau. Do kích thước mạng lớn hơn, tác động của quá khớp cao hơn. Xu hướng cho cắt tỉa ngẫu nhiên vẫn tương tự như tập dữ liệu MNIST, Fashion. Độ chính xác xác thực tốt hơn đối với độ thưa thớt 25% và giảm khi mức độ thưa thớt tăng.

Hình 18: Tổn thất xác thực của cắt tỉa theo độ lớn cho các mức độ thưa thớt khác nhau.

Hình 18 hiển thị độ chính xác xác thực của cắt tỉa theo độ lớn cho các mức độ thưa thớt khác nhau. Xu hướng vẫn như tập dữ liệu MNIST, Fashion. Cả cắt tỉa ngẫu nhiên Bayesian và cắt tỉa theo độ lớn Bayesian đều giúp giảm tác động của quá khớp. Tổn thất xác thực tiếp tục cải thiện khi đạt được độ thưa thớt 99%. Mô hình cắt tỉa ngẫu nhiên Bayesian phù hợp với dữ liệu tốt hơn mô hình cắt tỉa theo độ lớn.

Hình 19: Đường cong học CIFAR-10 (CNN 90%) cho phương pháp cắt tỉa Bayesian.

Hình 19 hiển thị các đường cong học cho cắt tỉa ngẫu nhiên, cắt tỉa theo độ lớn dưới khung Bayesian so với đường cơ sở trong một mạng thần kinh tích chập (CNN) được huấn luyện trên tập dữ liệu CIFAR-10. Ở đây mức độ thưa thớt mong muốn được đặt là 90%. Hình có hai biểu đồ con. Một biểu đồ hiển thị tổn thất huấn luyện và xác thực như một hàm của số epoch, biểu đồ khác (bên phải) hiển thị hệ số Bayes, độ thưa thớt như một hàm của số epoch. Xu hướng học tương tự như FCN. Độ chính xác xác thực cho cắt tỉa ngẫu nhiên giảm ở đầu huấn luyện và bắt đầu cải thiện khi huấn luyện tiến triển. Hệ số Bayes bắt đầu tăng đối với cắt tỉa theo độ lớn và độ thưa thớt dao động khi huấn luyện tiến triển. Đối với cắt tỉa ngẫu nhiên, hệ số Bayes bắt đầu giảm khi số epoch tăng và độ thưa thớt của mạng trở nên ổn định.

Hình 20: Tổn thất xác thực của cắt tỉa ngẫu nhiên cho các mức độ thưa thớt khác nhau.

Hình 20 hiển thị độ chính xác xác thực của cắt tỉa ngẫu nhiên cho các mức độ thưa thớt khác nhau. Xu hướng của cắt tỉa ngẫu nhiên tương tự như tập dữ liệu MNIST, Fashion. Tác động của quá khớp được giảm bởi việc cắt tỉa. Độ chính xác xác thực giảm khi mức độ thưa thớt tăng đến 99%.

Hình 21: Tổn thất xác thực của cắt tỉa theo độ lớn cho các mức độ thưa thớt khác nhau.

Hình 21 hiển thị độ chính xác xác thực của cắt tỉa theo độ lớn cho các mức độ thưa thớt khác nhau. Xu hướng tương tự như tập dữ liệu MNIST, Fashion. Cắt tỉa theo độ lớn giúp giảm tác động của quá khớp. Tổn thất xác thực tiếp tục cải thiện khi đạt được độ thưa thớt 99%.

Bảng 1: Giá trị độ chính xác tại các mức độ thưa thớt khác nhau

Tập dữ liệu | Mô hình | Chưa cắt tỉa | Độ thưa thớt | Ngẫu nhiên | Bayes Ngẫu nhiên | Độ lớn | Bayes Độ lớn
MNIST | FCN | 0.9782 | 25.0% | 0.9684 | 0.9747 | 0.9801 | 0.9759
| | | 50.0% | 0.9684 | 0.9710 | 0.9791 | 0.9791
| | | 75.0% | 0.9578 | 0.9706 | 0.9779 | 0.9812
| | | 90.0% | 0.9624 | 0.9657 | 0.9768 | 0.9772
| | | 99.0% | 0.9433 | 0.9439 | 0.9743 | 0.9767
| CNN | 0.9918 | 25.0% | 0.9908 | 0.9835 | 0.9910 | 0.992
| | | 50.0% | 0.9858 | 0.9906 | 0.9900 | 0.9901
| | | 75.0% | 0.9872 | 0.9905 | 0.9905 | 0.9892
| | | 90.0% | 0.9806 | 0.9791 | 0.9880 | 0.9888
| | | 99.0% | 0.1135 | 0.1135 | 0.9826 | 0.9804
Fashion | FCN | 0.8733 | 25.0% | 0.8699 | 0.8739 | 0.8744 | 0.8778
| | | 50.0% | 0.8659 | 0.8566 | 0.8725 | 0.8753
| | | 75.0% | 0.8535 | 0.8558 | 0.8800 | 0.8799
| | | 90.0% | 0.8416 | 0.8443 | 0.8750 | 0.8675
| | | 99.0% | 0.8076 | 0.8212 | 0.8573 | 0.8573
| CNN | 0.9028 | 25.0% | 0.8905 | 0.9030 | 0.8959 | 0.9002
| | | 50.0% | 0.8957 | 0.9021 | 0.8906 | 0.8982
| | | 75.0% | 0.8838 | 0.8773 | 0.8894 | 0.8974
| | | 90.0% | 0.8520 | 0.8589 | 0.8986 | 0.9022
| | | 99.0% | 0.7851 | 0.7083 | 0.8595 | 0.8768
CIFAR-10 | FCN | 0.4869 | 25.0% | 0.5233 | 0.5227 | 0.4857 | 0.4908
| | | 50.0% | 0.5136 | 0.5111 | 0.4981 | 0.5010
| | | 75.0% | 0.4950 | 0.4972 | 0.5109 | 0.5086
| | | 90.0% | 0.4643 | 0.4589 | 0.5314 | 0.5198
| | | 99.0% | 0.4158 | 0.4381 | 0.4973 | 0.4932
| CNN | 0.6606 | 25.0% | 0.6558 | 0.6574 | 0.6522 | 0.6557
| | | 50.0% | 0.6732 | 0.6764 | 0.6391 | 0.6570
| | | 75.0% | 0.6205 | 0.6526 | 0.6409 | 0.6528
| | | 90.0% | 0.5169 | 0.5092 | 0.6467 | 0.6437
| | | 99.0% | 0.1000 | 0.1000 | 0.5172 | 0.5537

Các giá trị độ chính xác tại các mức độ thưa thớt khác nhau cho các mạng đã cắt tỉa được trình bày trong Bảng 1. Các mạng được huấn luyện trong 25 epoch, và thí nghiệm được lặp lại 5 lần với các hạt giống ngẫu nhiên khác nhau để tính trung bình kết quả. Bảng cho thấy phương pháp cắt tỉa Bayesian đạt được các mức độ thưa thớt cao hơn mà không hy sinh độ chính xác. Nó vượt trội hơn các mạng chưa cắt tỉa và cho thấy độ chính xác tương đương hoặc tốt hơn so với các kỹ thuật cắt tỉa mạng thần kinh truyền thống.

1.3 Thảo luận

Các mạng thần kinh với số lượng lớn tham số có thể học các hàm phức tạp nhưng dễ bị quá khớp và không phù hợp cho các thiết bị có tài nguyên tính toán hạn chế. Cắt tỉa mạng thần kinh giải quyết cả hai thách thức này bằng cách giảm kích thước mạng. Phương pháp cắt tỉa lặp mà chúng tôi đã giới thiệu cho phép cắt tỉa đến mức độ thưa thớt mong muốn mà không mất độ chính xác so với đường cơ sở. Nó cho phép mạng học một hàm với ít kết nối hơn theo cách có nguyên tắc vì nó kiểm tra xem cấu hình mạng có phù hợp tốt với dữ liệu hay không. Các thí nghiệm rộng rãi được thực hiện trên ba tập dữ liệu khác nhau, hai loại mạng khác nhau, cho thấy đây là một phương pháp hiệu quả để huấn luyện mạng thần kinh mà không cần tham số hóa bổ sung.

Tài liệu tham khảo

Blalock, D. W., J. J. G. Ortiz, J. Frankle, và J. V. Guttag (2020). Tình trạng của việc cắt tỉa mạng thần kinh như thế nào? CoRR abs/2003.03033.

Blundell, C., J. Cornebise, K. Kavukcuoglu, và D. Wierstra (2015). Sự không chắc chắn về trọng số trong mạng thần kinh. Trong Hội nghị quốc tế về học máy, tr. 1613–1622. PMLR.

Brown, T., B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, và cộng sự (2020). Các mô hình ngôn ngữ là những người học ít mẫu. Advances in neural information processing systems 33, 1877–1901.

Dusenberry, M. W., D. Tran, D. Hafner, L.-P. Brunel, D. Ho, và D. Erhan (2019). Nén Bayesian cho học sâu. Trong Advances in Neural Information Processing Systems, tr. 3294–3305.

Han, S., H. Mao, và W. J. Dally (2015). Nén sâu: Nén mạng thần kinh sâu với cắt tỉa, lượng tử hóa được huấn luyện và mã hóa huffman. arXiv preprint arXiv:1510.00149.

Han, S., J. Pool, J. Tran, và W. Dally (2015). Học cả trọng số và kết nối cho mạng thần kinh hiệu quả. Advances in neural information processing systems 28.

He, Y., X. Zhang, và J. Sun (2018). Cắt tỉa kênh để tăng tốc các mạng thần kinh rất sâu. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, tr. 1389–1398.

Kingma, D. P. và J. Ba (2015). Adam: Một phương pháp cho tối ưu hóa ngẫu nhiên. arXiv preprint arXiv:1412.6980.

Krizhevsky, A. (2009). Học nhiều lớp đặc trưng từ hình ảnh nhỏ.

Krizhevsky, A., I. Sutskever, và G. E. Hinton (2012). Phân loại imagenet với mạng thần kinh tích chập sâu. Trong F. Pereira, C. Burges, L. Bottou, và K. Weinberger (Biên tập), Advances in Neural Information Processing Systems, Tập 25. Curran Associates, Inc.

Lecun, Y., L. Bottou, Y. Bengio, và P. Haffner (1998). Học dựa trên gradient áp dụng cho nhận dạng tài liệu. Proceedings of the IEEE 86 (11), 2278–2324.

LeCun, Y., J. Denker, và S. Solla (1989). Tổn thương não tối ưu. Advances in neural information processing systems 2.

Li, H., A. Kadav, I. Durdanovic, H. Samet, và H. P. Graf (2017). Cắt tỉa bộ lọc cho convnets hiệu quả. arXiv preprint arXiv:1608.08710.

Liu, Z., M. Sun, T. Zhou, G. Huang, và T. Darrell (2018). Suy nghĩ lại về giá trị của việc cắt tỉa mạng. arXiv preprint arXiv:1810.05270.

Molchanov, D., A. Ashukha, và D. Vetrov (2019). Dropout biến phân làm thưa thớt các mạng thần kinh sâu. Trong Proceedings of the 36th International Conference on Machine Learning, tr. 5234–5243.

Srivastava, N., G. Hinton, A. Krizhevsky, I. Sutskever, và R. Salakhutdinov (2014). Dropout: một cách đơn giản để ngăn mạng thần kinh quá khớp. The journal of machine learning research 15 (1), 1929–1958.

Strubell, E., A. Ganesh, và A. McCallum (2019). Cân nhắc về năng lượng và chính sách cho học sâu trong nlp. arXiv preprint arXiv:1906.02243.

Xiao, H., K. Rasul, và R. Vollgraf (2017). Fashion-mnist: một tập dữ liệu hình ảnh mới để đánh giá các thuật toán học máy. arXiv preprint arXiv:1708.07747.
