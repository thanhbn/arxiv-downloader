# HỌC CẤU TRÚC TỈA GỌN VÀ TRỌNG SỐ
ĐỒNG THỜI TỪ ĐẦU: PHƯƠNG PHÁP DỰA TRÊN CƠ CHẾ CHÚ Ý

Qisheng He, Weisong Shi, Ming Dong
Khoa Khoa học Máy tính
Đại học Bang Wayne
9 tháng 6, 2022

TÓM TẮT

Vì một mô hình học sâu thông thường chứa hàng triệu trọng số có thể huấn luyện, đã có nhu cầu ngày càng tăng về một cấu trúc mạng hiệu quả hơn với không gian lưu trữ giảm và hiệu suất thời gian chạy được cải thiện. Tỉa gọn là một trong những kỹ thuật nén mạng phổ biến nhất. Trong bài báo này, chúng tôi đề xuất một pipeline tỉa gọn không có cấu trúc mới, Học Đồng thời Cấu trúc thưa thớt và Trọng số dựa trên Chú ý (ASWL). Khác với cơ chế chú ý theo kênh hoặc theo trọng số truyền thống, ASWL đề xuất một thuật toán hiệu quả để tính toán tỷ lệ tỉa gọn thông qua chú ý theo lớp cho mỗi lớp, và cả trọng số cho mạng dày đặc và mạng thưa thớt đều được theo dõi để cấu trúc đã tỉa gọn được học đồng thời từ các trọng số được khởi tạo ngẫu nhiên. Các thí nghiệm của chúng tôi trên MNIST, Cifar10, và ImageNet cho thấy ASWL đạt được kết quả tỉa gọn vượt trội về độ chính xác, tỷ lệ tỉa gọn và hiệu quả hoạt động khi so sánh với các phương pháp tỉa gọn mạng tiên tiến.

1 Giới thiệu

Các mô hình học sâu như Mạng Nơ-ron Tích chập (CNN) đã đạt được thành công lớn trong các nhiệm vụ thị giác máy tính khác nhau như nhận dạng hình ảnh Russakovsky et al. [2015], He et al. [2016], phát hiện đối tượng Ren et al. [2015], He et al. [2017], và phân đoạn ngữ nghĩa Noh et al. [2015]. Một hướng quan trọng để triển khai các mô hình học sâu là thiết bị di động hoặc máy chủ biên để giảm độ trễ và đảm bảo quyền riêng tư dữ liệu cho người dùng. Vì một mô hình học sâu thông thường chứa hàng triệu trọng số có thể huấn luyện, thực hành này đã được đi kèm với nhu cầu ngày càng tăng về một cấu trúc mạng hiệu quả hơn với không gian lưu trữ giảm và hiệu suất hoạt động thời gian chạy được cải thiện.

Gần đây, đã có sự hồi sinh trong các kỹ thuật nén mạng nơ-ron (ví dụ: tỉa gọn và lượng tử hóa) Salehinejad and Valaee [2020]. Người ta có thể nén một kiến trúc mạng nơ-ron cho trước thành kích thước cực kỳ nhỏ mà không ảnh hưởng đến hiệu suất mô hình. Ngay cả các mạng hiệu quả Howard et al. [2017] cũng có thể được nén thêm mặc dù chúng đã có dung lượng nhỏ. Ví dụ, người ta đã chỉ ra rằng SqueezeNet Iandola et al. [2016] có thể được nén xuống 0.5MB, nhỏ hơn 510 lần so với AlexNet Krizhevsky et al. [2012].

Tỉa gọn, loại bỏ các kết nối hoặc nơ-ron trong mạng, là một trong những kỹ thuật nén mạng phổ biến nhất. Nó giảm cả kích thước và độ phức tạp tính toán của mô hình vì các trọng số đã tỉa gọn không cần được tính toán hoặc lưu trữ. Hơn nữa, tỉa gọn hiệu quả về kích thước vì các mô hình thưa thớt sâu luôn vượt trội hơn các mô hình dày đặc nông với hầu như không mất độ chính xác hoặc thậm chí độ chính xác cao hơn Zhu and Gupta [2017]. Khi tỉa gọn mạng lần đầu được giới thiệu để nén, các kết nối dư thừa được tỉa gọn bằng pipeline ba bước được minh họa trong Hình 1(a): đầu tiên, mạng được huấn luyện để học những kết nối nào quan trọng; tiếp theo, các kết nối không quan trọng được tỉa gọn; cuối cùng, mạng được huấn luyện lại để tinh chỉnh trọng số của các kết nối còn lại Han et al. [2015]. Cùng pipeline ba bước sau đó được sử dụng để thực hiện tỉa gọn theo lớp trong một phẫu thuật não tối ưu theo lớp Dong et al. [2017].

Người ta đã chỉ ra rằng học cấu trúc và tối ưu hóa trọng số có thể được kết hợp để đơn giản hóa quá trình tỉa gọn như Hình 1(b) cho thấy. Tỉa gọn tinh (Fine-pruning) Tung et al. [2017] đề xuất một phương pháp có nguyên tắc trong đó tỷ lệ tỉa gọn của mỗi lớp được

dự đoán đầu tiên bởi tối ưu hóa Bayesian với một mạng đã được huấn luyện trước, và sau đó mạng được tinh chỉnh và nén chung với các tỷ lệ tỉa gọn đã dự đoán. Huấn luyện chung thường được sử dụng trong các phương pháp lượng tử hóa được gọi là Ước lượng Thẳng-Qua (STE) Yin et al. [2019] để tránh gradient bằng không trong quá trình lan truyền ngược. Ưu điểm của pipeline như vậy là nó cho phép trạng thái tỉa gọn thích ứng trong quá trình thay đổi trọng số Tung and Mori [2018].

Tuy nhiên, tỉa gọn với huấn luyện trước là một quy trình tốn thời gian. Người ta cần lặp lại tỉa gọn và tinh chỉnh nhiều lần để tìm một cấu hình thưa thớt tốt Dettmers and Zettlemoyer [2019]. Trong khi đó, việc sử dụng hàm phần thưởng để tìm tỷ lệ tỉa gọn cho mỗi lớp cũng là một quy trình đắt đỏ Zhou et al. [2019].

Do đó, các nghiên cứu sau này cho thấy rằng trọng số của một mô hình và cấu trúc đã tỉa gọn của nó có thể được học trực tiếp từ các trọng số được khởi tạo ngẫu nhiên (Hình 1(c)). Tỉa gọn từ Đầu Wang et al. [2020] đề xuất một pipeline tỉa gọn mạng mới mà đầu tiên học cấu trúc đã tỉa gọn trực tiếp từ các trọng số được khởi tạo ngẫu nhiên và sau đó tối ưu hóa trọng số của mạng đã tỉa gọn. Loại pipeline này bỏ qua quy trình huấn luyện trước tốn thời gian. Tuy nhiên, việc học cấu trúc và tối ưu hóa trọng số diễn ra riêng biệt ở đây nên nó chỉ tỉa gọn trọng số dựa trên chính quy hóa cố định trước khi huấn luyện trọng số Liu et al. [2020].

Gần đây, cơ chế chú ý đã được đưa vào tỉa gọn mạng và đạt được thành công lớn Yamamoto and Maeno [2018], Chin et al. [2020]. Trong bài báo này, chúng tôi đề xuất một pipeline tỉa gọn mới, Học Đồng thời Cấu trúc thưa thớt và Trọng số dựa trên Chú ý (ASWL). Như được hiển thị trong Hình 1(d), trong ASWL, độ thưa thớt theo lớp và trọng số được học chung từ đầu trong một pipeline huấn luyện thống nhất. Cụ thể, chúng tôi đầu tiên sử dụng cơ chế chú ý để học tầm quan trọng của mỗi lớp trong mạng và xác định tỷ lệ tỉa gọn tương ứng. Sau đó, chúng tôi cùng lúc tỉa gọn lớp dưới sự hướng dẫn của tỷ lệ tỉa gọn và cập nhật các trọng số chưa được tỉa gọn. Các đóng góp chính của công trình của chúng tôi được tóm tắt như sau:

• ASWL cung cấp một khung thống nhất kết hợp cả tỉa gọn theo lớp và tối ưu hóa trọng số để học một mạng đã tỉa gọn từ các trọng số được khởi tạo ngẫu nhiên. Trong quá trình huấn luyện ASWL, cả trọng số của mạng dày đặc và mạng thưa thớt đều được theo dõi để tỷ lệ tỉa gọn cho mỗi lớp được học đồng thời khi trọng số thay đổi.

• Trong ASWL, các quyết định tỉa gọn theo lớp được thực hiện thông qua một phương pháp dựa trên chú ý mới. Các tỷ lệ chú ý trong cơ chế chú ý được đặt trên mỗi lớp thay vì cơ chế chú ý theo kênh hoặc theo trọng số trước đây. Một tỷ lệ tỉa gọn sau đó được tính toán trực tiếp dựa trên giá trị chú ý đã học cho mỗi lớp.

• ASWL không còn cần quy trình huấn luyện trước tốn thời gian trong tỉa gọn mạng nhưng cung cấp kết quả tỉa gọn tương đương. Thông qua các thí nghiệm rộng rãi trên các bộ dữ liệu chuẩn, chúng tôi chứng minh rằng ASWL dẫn đến kết quả tỉa gọn vượt trội về độ chính xác, kích thước mạng và hiệu quả hoạt động khi so sánh với các phương pháp tỉa gọn tiên tiến.

2 Nghiên cứu liên quan

Các kỹ thuật tỉa gọn mạng nơ-ron được sử dụng để tránh vấn đề overfitting trong các nghiên cứu đầu. Ví dụ, phẫu thuật não tối ưu Hassibi and Stork [1993] và mạng nơ-ron nhân tạo kết nối thưa thớt động Ström [1997] tỉa gọn những kết nối dẫn đến thay đổi hạn chế của lỗi xác thực. Dropout được giới thiệu để tỉa gọn hoặc loại bỏ các nơ-ron ngẫu nhiên với một tỷ lệ cho trước để giảm độ phức tạp của mô hình. Sau đó, các phương pháp tỉa gọn được sử dụng để xác định và loại bỏ các trọng số không cần thiết nhằm giảm kích thước cũng như độ trễ thời gian chạy của mô hình mục tiêu. Trong khi một số phương pháp tỉa gọn như ThiNet Luo et al. [2017] và Tỉa gọn theo Dải Meng et al. [2020] tỉa gọn các bản đồ đặc trưng hoặc dải bộ lọc, hầu hết các phương pháp khác loại bỏ các kết nối/nơ-ron vì các cấu trúc thưa thớt được cung cấp bởi tỉa gọn không có cấu trúc thường dẫn đến độ chính xác cao hơn với độ dư thừa trọng số thấp hơn Liu et al. [2020].

Khi các phương pháp tỉa gọn lần đầu được giới thiệu vào nén sâu, chúng loại bỏ tất cả các kết nối có trọng số thấp hơn một ngưỡng Han et al. [2015]. Người ta đã chỉ ra rằng tỉa gọn các trọng số không quan trọng dẫn đến hầu như không mất độ chính xác vì các trọng số trong mạng nơ-ron sâu có độ dư thừa cao. Sau đó, các phương pháp được đề xuất để xác định các trọng số không cần thiết theo lớp Yang et al. [2017], Dong et al. [2017], Yu et al. [2018], cung cấp tỷ lệ tỉa gọn linh hoạt trên các lớp khác nhau và do đó đạt được kết quả tốt hơn. Học Máy Tự động (AutoML) cũng được giới thiệu vào tỉa gọn He et al. [2018], nơi không gian thiết kế được lấy mẫu hiệu quả bằng học tăng cường để tìm cấu trúc đã tỉa gọn tốt nhất của một mô hình đã được huấn luyện trước. Ngoài ra, cơ chế chú ý gần đây đã được sử dụng để tìm các trọng số không quan trọng Hacene et al. [2019], Wu et al. [2018], Lin et al. [2019]. Hầu hết chúng giới thiệu các tỷ lệ chú ý trên mỗi kênh hoặc mỗi trọng số của lớp mục tiêu. Ví dụ, tỉa gọn dựa trên LeGR Chin et al. [2020] huấn luyện các cặp tham số (tỷ lệ và dịch chuyển) trên mỗi kênh và sau đó tỉa gọn các đặc trưng đầu ra tương ứng.

Gần đây hơn, pipeline tỉa gọn ba bước thông thường, tức là huấn luyện trước, tỉa gọn và tinh chỉnh, đã được đơn giản hóa. Tỉa gọn tinh Tung et al. [2017] cùng lúc tinh chỉnh và nén mạng đã được huấn luyện trước với tỷ lệ tỉa gọn của mỗi lớp được dự đoán bởi tối ưu hóa Bayesian. Sau đó, giả thuyết vé số tuyên bố rằng các mạng nơ-ron dày đặc được khởi tạo ngẫu nhiên chứa các mạng con, được gọi là vé thắng cuộc, đạt độ chính xác tương đương với mạng gốc của nó với số lần lặp huấn luyện tương tự Frankle and Carbin [2018]. Giả thuyết đã được chứng minh toán học trong Malach et al. [2020]. Dựa trên những phát hiện như vậy, Tỉa gọn từ Đầu Wang et al. [2020] đề xuất tỉa gọn mạng từ các trọng số được khởi tạo ngẫu nhiên. Cụ thể, nó đầu tiên học tầm quan trọng của kênh cho mỗi lớp với một bộ chính quy hóa độ thưa thớt, sau đó tìm kiếm một cấu trúc đã tỉa gọn với một phương pháp được áp dụng từ Làm Mỏng Mạng Liu et al. [2017] nhưng sử dụng số phép toán dấu phẩy động mỗi giây (FLOPS) như là ràng buộc. Cấu trúc đã tỉa gọn sau đó được tối ưu hóa từ các trọng số ngẫu nhiên. Tuy nhiên, kiểm tra tính hợp lý Su et al. [2020] cho thấy rằng các vé ban đầu khó được học từ dữ liệu huấn luyện. Do đó, cần thiết phải giữ cấu trúc đã tỉa gọn linh hoạt trong quá trình huấn luyện. Để đạt được điều này, nghiên cứu gần đây hơn đã huấn luyện các mạng thưa thớt từ đầu bằng cách phát triển lại một số kết nối sau một số lần lặp cố định (ví dụ: mỗi epoch) dựa trên động lượng gradient, hay còn gọi là động lượng thưa thớt, của các trọng số trong những lần lặp này Dettmers and Zettlemoyer [2019]. Tuy nhiên, khi một trong các kết nối hội tụ trong quá trình huấn luyện, động lượng thưa thớt của nó cũng sẽ có xu hướng về không, thường dẫn đến cấu trúc và trọng số dưới tối ưu trong quá trình huấn luyện.

3 Học Đồng thời dựa trên Chú ý

Mục tiêu của pipeline huấn luyện mới của chúng tôi, Học Đồng thời Cấu trúc thưa thớt và Trọng số dựa trên Chú ý (ASWL), là huấn luyện cả cấu trúc đã tỉa gọn và trọng số mô hình từ các trọng số được khởi tạo ngẫu nhiên. Khác với các phương pháp tỉa gọn hiện có, ASWL có được các mô hình học sâu đã tỉa gọn theo lớp mà không cần huấn luyện trước. Như được hiển thị trong Hình 2, chúng tôi đầu tiên chuyển đổi một mô hình thành mô hình dựa trên chú ý bằng cách thay thế các lớp có trọng số truyền thống của nó bằng các lớp có trọng số dựa trên chú ý. Thay vì học cấu trúc đã tỉa gọn bằng cách phát triển lại các kết nối sau một số lần lặp cố định, chúng tôi tỉa gọn các trọng số theo lớp dựa trên tỷ lệ tỉa gọn được tính toán từ chú ý đã học của lớp mục tiêu trong mỗi lần lặp. Trong khi đó, các tham số chưa được tỉa gọn được theo dõi và học trực tiếp từ quá trình truyền tới của cấu trúc đã tỉa gọn. Học đồng thời như vậy theo dõi cả trọng số đã tỉa gọn và chưa tỉa gọn trong quá trình huấn luyện và do đó đưa ra quyết định tỉa gọn thích ứng trên cấu trúc thưa thớt.

3.1 Mạng Nơ-ron dựa trên Chú ý

Độ thưa thớt của một mạng đã tỉa gọn có liên quan yếu đến đầu ra của nó và do đó khó được học trực tiếp Lin et al. [2020]. Nghiên cứu gần đây cho thấy rằng cơ chế chú ý cung cấp một giải pháp đầy hứa hẹn Yamamoto and Maeno [2018], Hacene et al. [2019]. Khác với các phương pháp truyền thống đặt các vô hướng chú ý theo kênh hoặc theo trọng số, chúng tôi giới thiệu một cơ chế chú ý theo lớp đơn giản hơn nhiều, nơi một giá trị vô hướng chú ý được định nghĩa trên tất cả L lớp trong mạng nơ-ron dựa trên chú ý của chúng tôi. Mô hình như vậy được ký hiệu là f(x;w;A) trong đó x là đầu vào của mô hình, w là các trọng số có thể huấn luyện truyền thống, và A={a1;a2;...;aL} là các giá trị chú ý theo lớp. Cho một giá trị cổng vô hướng al ∈ (0;1] cho lớp thứ l như là chú ý, nó được nhân với đầu ra của lớp. Tức là, giả sử đầu ra của lớp thứ l gốc là fl(xl-1;wl), đầu ra được chú ý được sửa đổi thành f̂l(xl-1;wl;al) = al fl(xl-1;wl). Mỗi giá trị chú ý được khởi tạo bằng 0.5 ở đầu. Trong quá trình huấn luyện, cả trọng số và chú ý đều được cập nhật. Với việc áp dụng tỉa gọn, mục tiêu tối ưu hóa của một mạng nơ-ron dựa trên chú ý với các trọng số nén ŵ là:

min ŵ,A ∑i L(f(xi;ŵ;A)) + α Ω(A) + λ ∑j ŵj² (1)

trong đó L() biểu thị mất mát entropy chéo, Ω() là bộ chính quy hóa độ thưa thớt cho việc học cấu trúc (được thảo luận trong phần tiếp theo), bộ chính quy hóa L2 khuyến khích tất cả trọng số nhỏ Krogh and Hertz [1992], và α và λ là các hệ số cho bộ chính quy hóa độ thưa thớt và bộ chính quy hóa L2, tương ứng.

3.2 Tỷ lệ Tỉa gọn và Bộ chính quy hóa Độ thưa thớt

Các phương pháp tỉa gọn theo lớp thường yêu cầu một mô hình đã được huấn luyện trước làm điểm khởi đầu để tìm kiếm cấu trúc đã tỉa gọn. Nghiên cứu gần đây trong Tung et al. [2017] và He et al. [2018] sử dụng Bayesian ngây thơ để tối ưu hóa tỷ lệ tỉa gọn theo lớp trong một mạng cho trước. Tuy nhiên, việc sử dụng loại tối ưu hóa này trong mỗi lần lặp huấn luyện là cực kỳ tốn thời gian. Để tạo điều kiện cho việc tối ưu hóa đồng thời cấu trúc và trọng số trong mỗi lần lặp, chúng tôi đề xuất một thuật toán hiệu quả hơn để tính toán tỷ lệ tỉa gọn trực tiếp từ các chú ý trong mô hình dựa trên chú ý của chúng tôi.

Trong Wang et al. [2020], Hacene et al. [2019], người ta đã chỉ ra rằng việc tỷ lệ hóa trọng số mạng sẽ ức chế những trọng số không quan trọng, dẫn đến hiệu ứng tỉa gọn. Kết quả là, các chú ý đã học có thể được sử dụng để biểu diễn tầm quan trọng của mỗi lớp. Tức là, nếu một lớp có chú ý lớn hơn, nó được coi là quan trọng hơn nên cần được tỉa gọn ít hơn, và ngược lại. Ở đây, chúng tôi giới thiệu một siêu tham số dương, hệ số tỉa gọn ρ, để có thêm kiểm soát khi tính toán tỷ lệ tỉa gọn pl dựa trên chú ý al cho trước của lớp thứ l:

pl(al) = ρ(1-al); (2)

trong đó al là chú ý cho lớp thứ l và ρ là hệ số tỉa gọn. Để đảm bảo không phải tất cả trọng số đều được tỉa gọn, chúng tôi giới hạn tỷ lệ tỉa gọn tối đa là 99%. Vì các chú ý của chúng tôi được áp dụng dọc theo chiều lớp khi trọng số được tỉa gọn, độ thưa thớt tổng thể S của mô hình dựa trên chú ý của chúng tôi được tính như sau:

S(A) = ∑l(1-pl(al))nwl / ∑l nwl; (3)

trong đó pl là tỷ lệ tỉa gọn và nwl là tổng số trọng số chưa được tỉa gọn trong lớp thứ l. Lưu ý rằng nwl là một hằng số vì nó được xác định trước cho mỗi lớp.

Các bộ chính quy hóa như L1 hoặc L2 khuyến khích trọng số mạng w nhỏ, nhưng không nhất thiết phải bằng không. Do đó, một bộ chính quy hóa bổ sung được yêu cầu trong ASWL để khuyến khích quy trình tỉa gọn loại bỏ các trọng số không quan trọng trong mỗi lớp. Hơn nữa, bộ chính quy hóa L1/L2 khuyến khích các chú ý bằng không mà không xem xét độ thưa thớt của toàn bộ mô hình, đặc biệt khi hệ số tỉa gọn không được đặt bằng 1. Kết quả là, chúng tôi áp dụng bộ chính quy hóa độ thưa thớt được đề xuất trong Wang et al. [2020] như sau và kết hợp nó với L2 trong ASWL:

Ω(A) = S(A)² (4)

Bộ chính quy hóa độ thưa thớt bình phương có thể vi phân Luo and Wu [2020] và sẽ giúp giảm thiểu độ thưa thớt theo lớp trong quá trình tối ưu hóa.

3.3 Học Cấu trúc Thưa thớt Đồng thời và Tối ưu hóa Trọng số

Các phương pháp tỉa gọn truyền thống học cấu trúc đã tỉa gọn trước, và sau đó tối ưu hóa trọng số dựa trên cấu trúc đã tỉa gọn. Cấu trúc đã tỉa gọn có thể dễ dàng được tìm thấy nếu có sẵn một mô hình đã được huấn luyện trước. Tuy nhiên, với việc huấn luyện trọng số từ các giá trị được khởi tạo ngẫu nhiên, các trọng số hiệu quả có thể thay đổi dần dần từ lớp nông đến lớp sâu Dettmers and Zettlemoyer [2019]. Trong khi đó, một mạng nơ-ron dày đặc chứa một mạng con có cùng độ chính xác ngay cả khi không tinh chỉnh Malach et al. [2020]. Những kết quả này thúc đẩy chúng tôi thực hiện học đồng thời cả cấu trúc thưa thớt và trọng số trong ASWL bằng cách theo dõi cả mạng dày đặc (thông qua lan truyền ngược) và mạng con thưa thớt (thông qua quá trình truyền tới).

Trong huấn luyện ASWL, cả trọng số cho mạng dày đặc và mạng thưa thớt đều được theo dõi để cấu trúc được học đồng thời khi trọng số thay đổi. Các tham số của mạng thưa thớt được tỉa gọn bằng cách loại bỏ ít nhất pi phần trăm trọng số dày đặc thấp nhất trong lớp thứ i dựa trên các giá trị trọng số tuyệt đối |wi|. Sau khi huấn luyện, các giá trị chú ý được áp dụng cho trọng số và bị loại bỏ cùng với trọng số trong mạng dày đặc. Chỉ có trọng số trong mạng thưa thớt được lưu trữ.

Mỗi lần lặp huấn luyện trong ASWL chứa bốn bước: (1) mất mát phân loại mạng được tính toán thông qua một quá trình truyền tới dựa trên trọng số nén ŵ và chú ý A, (2) cả trọng số chưa nén w và chú ý A đều được cập nhật thông qua lan truyền ngược, (3) tỷ lệ tỉa gọn p được tính toán bởi chú ý mới A và hệ số tỉa gọn ρ, và (4) mô hình được nén theo lớp sử dụng tỷ lệ tỉa gọn p, và trọng số nén ŵ được cập nhật. Quy trình huấn luyện chi tiết được tóm tắt trong Thuật toán 1. Trong ASWL, thông qua tối ưu hóa đồng thời, các trọng số đã được tỉa gọn lúc đầu có thể được khôi phục sau này, và các trọng số được định nghĩa là quan trọng lúc đầu có thể được tỉa gọn, tất cả phụ thuộc vào sự phát triển của cấu trúc mạng.

**Thuật toán 1** Thuật toán Huấn luyện cho Học Đồng thời Cấu trúc thưa thớt và Trọng số dựa trên Chú ý
1: **function** TRAIN(iterations, ρ)
2:    w, A ← initialize()
3:    
4:    **for** i **from** 1 **to** iterations **do**
5:        p ← ρ(1-A)
6:        ŵ ← prune(w, p)
7:        
8:        x ← getNextBatch()
9:        l ← ∑ᴺᵢ L(f(xᵢ; ŵ, A)) + α Ω(A) + λ ∑ᵢ ŵᵢ²
10:       
11:       G ← calculateGradients(l; ŵ, A)
12:       w, A ← updateWeights(G; w, A)
13:   **end for**
14: **end function**

4 Kết quả Thí nghiệm

Trong phần này, chúng tôi thực hiện các thí nghiệm rộng rãi với ASWL trên VGG-16 Simonyan and Zisserman [2014], ResNet50 He et al. [2016], và MobileNetV2 Sandler et al. [2018]. Đối với VGG-16 và ResNet50, chúng tôi đơn giản thay thế lớp tích chập truyền thống và lớp dày đặc bằng lớp tích chập dựa trên chú ý và lớp dày đặc dựa trên chú ý của chúng tôi. Đối với MobileNetV2, chúng tôi thay thế lớp tích chập điểm 1×1 bằng một lớp tích chập dựa trên chú ý nhưng để nguyên lớp chiều sâu không được nén vì 99% tham số và tính toán được chứa trong các lớp tích chập điểm.

Có tổng cộng ba siêu tham số trong ASWL: các hệ số của bộ chính quy hóa độ thưa thớt và L2 (α và λ), và hệ số tỉa gọn ρ. Chúng được chỉ định sau trong các thí nghiệm khác nhau.

Pipeline huấn luyện ASWL của chúng tôi đã được triển khai trong TensorFlow Abadi et al. [2016]. Tất cả các mô hình được huấn luyện trên máy tính với CPU Intel i7 8700K, 16GB RAM, và hai card đồ họa NVIDIA RTX 2080 Ti, mỗi card có 11GB GDDR SDRAM. Mã nguồn của công trình này có sẵn trong tài liệu bổ sung với các bình luận chi tiết và sẽ được công khai sau thời gian đánh giá của AAAI 2022.

Cụ thể, chúng tôi tiến hành các thí nghiệm trên các bộ dữ liệu chuẩn MNIST LeCun et al. [1998], Cifar-10 Krizhevsky and Hinton [2009] và ImageNet (ILSVRC-2012 phiên bản 2.0.1) Russakovsky et al. [2015] và so sánh với các phương pháp tỉa gọn sau: Học cả Trọng số và Kết nối cho Mạng Nơ-ron Hiệu quả (ENN) Han et al. [2015], Suy nghĩ lại về Tỉa gọn Mạng (RNP) Liu et al. [2018], Tỉa gọn Kênh nhận biết Phân biệt (DCP) Zhuang et al. [2018], Tỉa gọn mạng nơ-ron tích chập với giảm dư thừa cấu trúc (SRR) Wang et al. [2021], Tỉa gọn Mạng thông qua Tối đa hóa Hiệu suất (NPPM) Gao et al. [2021], Tỉa gọn Ngưỡng Đã học (LTP) Azarian et al. [2020], Tỉa gọn từ Đầu (PFS) Wang et al. [2020], Tái tham số hóa Thưa thớt Động (DS) Mostafa and Wang [2019], Tỉa gọn Bộ lọc trong Bộ lọc (PFF) Meng et al. [2020], WoodFisher (WF) Singh and Alistarh [2020], Mạng Thưa thớt từ Đầu (SNS) Dettmers and Zettlemoyer [2019], và Tỉa gọn Động với Phản hồi (DPF) Lin et al. [2020].

4.1 Kết quả trên MNIST

Chúng tôi đầu tiên huấn luyện các mô hình đã chọn với ASWL trên MNIST, chứa 10 chữ số viết tay khác nhau với 60,000 hình ảnh huấn luyện và 10,000 hình ảnh kiểm tra. Mỗi mô hình được huấn luyện với các hệ số tỉa gọn khác nhau là 1, 1.5, và 2 trong 100 epoch bởi bộ tối ưu hóa Adam với tốc độ học 0.001 và độ suy giảm 0.98 cho mỗi epoch. Các chú ý của mỗi lớp được khởi tạo ở 0.5. Siêu tham số α (hệ số bộ chính quy hóa độ thưa thớt) được sử dụng chủ yếu để đạt được tỷ lệ tỉa gọn mong muốn và được đặt ở 0.5 cho tất cả các mô hình. Theo Simonyan and Zisserman [2014], He et al. [2016], và Sandler et al. [2018], siêu tham số khác λ (hệ số bộ chính quy hóa L2) được đặt ở 5×10⁻⁴, 0.0001, và 0.00004 cho VGG16, ResNet 56, MobileNetV2, tương ứng. Các mô hình với kết quả tốt nhất được chọn.

Bảng 1 cho thấy kết quả huấn luyện ASWL trên MNIST với VGG16 (Trên), MobileNetV2 (Giữa), và ResNet56 (Dưới). Trong hầu hết các tình huống, tỷ lệ tỉa gọn tăng dần khi chúng tôi giảm hệ số tỉa gọn. Không có mối quan hệ rõ ràng nào được tìm thấy giữa độ chính xác mô hình và hệ số tỉa gọn, trong khi hệ số tỉa gọn nhỏ hơn cung cấp tỷ lệ tỉa gọn lớn hơn. Với khoảng 10% đến 25% trọng số trong các mô hình dày đặc gốc, ASWL của chúng tôi cung cấp độ chính xác tương tự hoặc cao hơn nhiều lần.

Hình 3 (a) cho thấy chi tiết tỉa gọn của mỗi lớp trong VGG-16 với hệ số tỉa gọn ρ = 1.5. Các lớp tích chập dựa trên chú ý sâu hơn được tỉa gọn nhiều hơn các lớp nông hơn, trong khi hai lớp dày đặc dựa trên chú ý cuối cùng có tỷ lệ tỉa gọn ít hơn nhiều so với các lớp tích chập. Hình 3 (b) minh họa tỷ lệ tỉa gọn của mỗi lớp tích chập dựa trên chú ý thông thường (không chiều sâu) trong MobileNetV2 với hệ số tỉa gọn ρ = 1. Lớp tích chập đầu tiên với kích thước bộ lọc 3×3 có tỷ lệ tỉa gọn ít nhất, chỉ trên 50%. Trong khi đó, tỷ lệ tỉa gọn có vẻ lớn hơn ở các lớp tích chập sâu hơn, tương tự như trường hợp VGG-16. Chi tiết tỉa gọn theo lớp của ResNet56 được hiển thị trong Hình 3 (c), nơi lớp cuối cùng là một lớp dày đặc dựa trên chú ý. ResNet56 có nhiều lớp hơn nhiều so với VGG-16 và MobileNetV2 nhưng vẫn có xu hướng rõ ràng là tỷ lệ tỉa gọn cao hơn cho lớp sâu hơn. Hình 4 (a) cho thấy đồ thị huấn luyện của VGG-16 với hệ số tỉa gọn ρ = 1.5. Nhìn chung, tỷ lệ tỉa gọn tiếp tục tăng khi mô hình đang được huấn luyện, và độ chính xác xác thực ổn định sau 30 epoch. Chúng tôi quan sát điều tương tự trong đồ thị huấn luyện của MobileNetV2 với ρ = 1 và ResNetV2 với ρ = 1 (Hình 4 (b) và (c)).

4.2 Kết quả trên Cifar10

Cifar10 là một bộ dữ liệu chứa 10 lớp khác nhau với 50,000 hình ảnh huấn luyện và 10,000 hình ảnh kiểm tra. Tương tự như thí nghiệm của chúng tôi trên MNIST, chúng tôi huấn luyện các mô hình đã chọn với các tỷ lệ tỉa gọn khác nhau là 1, 1.5, và 2. Đối với VGG16, chúng tôi sử dụng tốc độ học ban đầu 0.1 và nhân 0.5 cho mỗi 20 epoch với bộ tối ưu hóa SGD (động lượng 0.9), và huấn luyện trong 250 epoch với kích thước batch 128. Đối với ResNet 56, chúng tôi tuân theo cùng cài đặt trong He et al. [2016]. Đối với MobileNetV2, chúng tôi huấn luyện trong 350 epoch trên bộ tối ưu hóa SGD với tốc độ học ban đầu 0.1, được chia cho 10 sau 150 và 250 epoch. Một lần nữa, siêu tham số α (hệ số bộ chính quy hóa độ thưa thớt) được sử dụng để giúp chúng tôi đạt được tỷ lệ tỉa gọn mong muốn và được đặt ở 2.5, 5 và 0.5 cho VGG16, ResNet 56, MobileNetV2, tương ứng. Theo Simonyan and Zisserman [2014], He et al. [2016], Sandler et al. [2018], siêu tham số khác λ được đặt ở 5×10⁻⁴, 0.0001, và 0.00004 cho VGG16, ResNet 56, và MobileNetV2, tương ứng. Các mô hình với kết quả tốt nhất được chọn.

Bảng 2 cho thấy kết quả huấn luyện ASWL trên Cifar10 với VGG16 (Trên), MobileNetV2 (Giữa), và ResNet56 (Dưới). Tương tự như MNIST, kết quả cho thấy hệ số tỉa gọn nhỏ thường dẫn đến tỷ lệ tỉa gọn cao hơn. Ngoài ra, chúng tôi quan sát thấy ASWL cung cấp cải thiện về độ chính xác mô hình với kích thước mạng giảm và hiệu quả thời gian chạy cao hơn so với đường cơ sở không nén trong tất cả các trường hợp. Chi tiết tỉa gọn của ba mô hình được hiển thị trong Hình 1 trong tài liệu bổ sung. Tương tự như trường hợp MNIST, các lớp sâu hơn được tỉa gọn nhiều hơn các lớp nông hơn trừ lớp dày đặc cuối cùng để phân loại. Đồ thị huấn luyện tương tự (Hình 2 trong tài liệu bổ sung) của ba mô hình được quan sát như trong MNIST rằng mô hình hội tụ sau khoảng 30 epoch và đạt độ chính xác xác thực cao nhất ở khoảng 130 epoch, tại thời điểm đó tỷ lệ tỉa gọn tổng thể cũng ngừng tăng.

Bảng 3 so sánh ASWL với các phương pháp tỉa gọn tiên tiến trên Cifar10 với ResNet56 (trên) và VGG16 (dưới). Rõ ràng, ASWL đạt được độ chính xác cao hơn đường cơ sở trên cả hai mô hình. Nhìn chung, mô hình ASWL của chúng tôi đạt được tỷ lệ tỉa gọn xuất sắc với mức tăng độ chính xác cao nhất khi so sánh với đường cơ sở. Những kết quả này rõ ràng chứng minh những ưu điểm của huấn luyện đồng thời và tỉa gọn dựa trên chú ý theo lớp trong ASWL.

4.3 Kết quả trên ImageNet

So với MNIST và Cifar10, ImageNet là một bộ dữ liệu lớn hơn nhiều chứa 1000 lớp với 1.2M hình ảnh huấn luyện và 50K hình ảnh kiểm tra. Chúng tôi huấn luyện ResNet-50 sử dụng ASWL theo cùng cài đặt huấn luyện trong He et al. [2016] cho ResNet-50. Siêu tham số α được đặt ở 0.5. Ngoài ra, hệ số tỉa gọn được đặt bằng 1. Các mô hình với kết quả tốt nhất được chọn.

Bảng 4 so sánh ASWL với các phương pháp tỉa gọn tiên tiến trên ResNet-50. Thông qua huấn luyện đồng thời với tỉa gọn theo lớp từ các trọng số được khởi tạo ngẫu nhiên, ASWL đạt được độ chính xác top-1 là 76.5% với tỷ lệ tỉa gọn lớn (~86.1%). Lưu ý rằng độ chính xác này cao hơn mô hình đường cơ sở ResNet-50 không nén (~76.1%). Xem xét sự cân bằng giữa độ chính xác top-1 và tỷ lệ tỉa gọn, ASWL cung cấp kết quả tỉa gọn vượt trội khi so sánh với tất cả các phương pháp tỉa gọn hiện có. Nếu chúng ta chỉ xem xét độ chính xác, nó là tốt thứ hai.

5 Kết luận

Trong bài báo này, chúng tôi đề xuất một pipeline tỉa gọn mới, Học Đồng thời Cấu trúc thưa thớt và Trọng số dựa trên Chú ý (ASWL). Trong ASWL, chúng tôi đầu tiên sử dụng cơ chế chú ý để học tầm quan trọng của mỗi lớp trong mạng và xác định tỷ lệ tỉa gọn tương ứng. Sau đó, độ thưa thớt theo lớp và trọng số được học chung từ đầu trong một quy trình huấn luyện thống nhất. Các thí nghiệm rộng rãi của chúng tôi trên các bộ dữ liệu chuẩn cho thấy ASWL đạt được kết quả tỉa gọn xuất sắc về độ chính xác, tỷ lệ tỉa gọn và hiệu quả hoạt động.

**Tài liệu tham khảo**

[Danh sách tài liệu tham khảo được giữ nguyên với tên tác giả và tiêu đề tiếng Anh]
