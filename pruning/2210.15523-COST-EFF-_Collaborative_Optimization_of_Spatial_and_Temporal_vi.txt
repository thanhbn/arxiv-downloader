# 2210.15523.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2210.15523.pdf
# Kích thước tệp: 700520 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
COST-EFF: Tối ưu hóa cộng tác về hiệu quả không gian và thời gian
với các mô hình ngôn ngữ đa lối ra được thu gọn
Bowen Shen1;2, Zheng Lin1;2, Yuanxin Liu1;3, Zhengxiao Liu1,
Lei Wang1, Weiping Wang1
1Viện Kỹ thuật Thông tin, Viện Hàn lâm Khoa học Trung Quốc
2Trường An ninh mạng, Đại học Viện Hàn lâm Khoa học Trung Quốc
3Phòng thí nghiệm trọng điểm Bộ Giáo dục về Ngôn ngữ học Tính toán, Đại học Bắc Kinh
{shenbowen, linzheng, liuzhengxiao, wanglei, wangweiping}@iie.ac.cn
liuyuanxin@stu.pku.edu.cn
Tóm tắt
Các mô hình ngôn ngữ được tiền huấn luyện dựa trên Transformer (PLM) chủ yếu gặp phải tình trạng quá tải mặc dù có khả năng tiên tiến. Đối với các thiết bị có hạn chế về tài nguyên, có nhu cầu cấp thiết cho một mô hình hiệu quả về không gian và thời gian vẫn duy trì được khả năng chính của PLM. Tuy nhiên, các mô hình nén tĩnh hiện có không nhận thức được sự phức tạp đa dạng giữa các thể hiện đầu vào, có thể dẫn đến sự dư thừa và không đầy đủ cho các đầu vào đơn giản và phức tạp. Ngoài ra, các mô hình thu nhỏ với việc thoát sớm gặp phải thách thức trong việc cân bằng giữa việc đưa ra dự đoán và phục vụ các lớp sâu hơn. Được thúc đẩy bởi những cân nhắc như vậy, chúng tôi đề xuất một tối ưu hóa cộng tác cho PLM tích hợp nén mô hình tĩnh và tăng tốc suy luận động. Cụ thể, PLM được thu gọn về chiều rộng trong khi chiều sâu vẫn nguyên vẹn, bổ sung việc thoát sớm theo lớp để tăng tốc suy luận một cách động. Để giải quyết sự cân bằng của việc thoát sớm, chúng tôi đề xuất một phương pháp huấn luyện kết hợp hiệu chỉnh việc thu gọn và bảo tồn các cấu trúc đóng góp cho mỗi lối ra thay vì chỉ lớp cuối cùng. Các thí nghiệm được tiến hành trên benchmark GLUE và kết quả xác minh tính tối ưu Pareto của phương pháp chúng tôi ở mức nén và tăng tốc cao với 1/8 tham số và 1/19 FLOP của BERT.

1 Giới thiệu
Việc tiền huấn luyện các mô hình ngôn ngữ tổng quát và tinh chỉnh chúng trên các tác vụ downstream cụ thể đã trở thành mô hình thống trị trong xử lý ngôn ngữ tự nhiên (NLP) kể từ sự ra đời của Transformer (Vaswani et al., 2017) và BERT (Devlin et al., 2019). Tuy nhiên, các mô hình ngôn ngữ được tiền huấn luyện (PLM) chủ yếu được thiết kế để có quy mô lớn trong việc theo đuổi khả năng và tính tổng quát của mô hình. Với mối quan tâm như vậy, việc lưu trữ mô hình và thời gian suy luận của PLM thường cao, khiến chúng trở nên thách thức khi triển khai trên các thiết bị có hạn chế về tài nguyên (Sun et al., 2020).

Các nghiên cứu gần đây cho thấy PLM dựa trên Transformer có sự dư thừa về không gian và thời gian đến từ chiều rộng và chiều sâu quá mức của mô hình (Michel et al., 2019; Xin et al., 2021). Với các phương pháp nén tĩnh bao gồm cắt tỉa mạng (Xia et al., 2022) và chưng cất kiến thức (Jiao et al., 2020), chi phí không gian của PLM (tức là tham số mô hình) có thể được giảm xuống một cài đặt cố định. Từ góc độ của các thể hiện đầu vào hơn là mô hình, việc thoát sớm mà không đi qua tất cả các lớp mô hình cho phép tăng tốc động tại thời điểm suy luận và giảm thiểu chi phí thời gian (Zhou et al., 2020).

Tuy nhiên, nén tĩnh khó có thể tìm được một cài đặt tối ưu vừa hiệu quả trên các thể hiện đầu vào đơn giản vừa chính xác trên các thể hiện phức tạp, trong khi việc thoát sớm không thể giảm thiểu sự dư thừa trong chiều rộng mô hình và bất lực trong việc giảm khối lượng thực tế của mô hình. Hơn nữa, các nghiên cứu về khả năng diễn giải cho thấy rằng các đặc trưng attention và ngữ nghĩa qua các lớp là khác nhau trong BERT (Clark et al., 2019). Do đó, việc tạo ra một mô hình đa lối ra từ một mô hình tiền huấn luyện đơn lối ra như BERT gây ra sự không nhất quán trong mục tiêu huấn luyện, nơi mỗi lớp đồng thời đưa ra dự đoán và phục vụ các lớp sâu hơn (Xin et al., 2021). Theo kinh nghiệm, chúng tôi thấy rằng BERT không nén không bị ảnh hưởng nghiêm trọng bởi sự không nhất quán như vậy, trong khi các mô hình có khả năng nhỏ không có khả năng cân bằng các lớp nông và sâu. Việc cắm các lối ra sau khi nén sẽ dẫn đến suy giảm hiệu suất nghiêm trọng, điều này cản trở sự bổ sung của hai tối ưu hóa.

Để khai thác đầy đủ hiệu quả của PLM và giảm thiểu các vấn đề được đề cập ở trên, chúng tôi thiết kế một mô hình đa lối ra được thu gọn và đề xuất một phương pháp Tối ưu hóa Cộng tác về Hiệu quả Không gian và Thời gian (COST-EFF) như được mô tả trong Hình 1. Không giống như các công trình trước đây, ví dụ như DynaBERT (Hou et al., 2020) và CoFi (Xia et al., 2022), thu được một mô hình thấp, chúng tôi giữ nguyên chiều sâu trong khi thu gọn PLM. Sự ưu việt của kiến trúc mảnh khảnh so với kiến trúc thấp được hỗ trợ bởi (Bengio et al., 2007) và (Turc et al., 2019) trong học máy tổng quát và thiết kế PLM. Để giải quyết sự không nhất quán trong mô hình đa lối ra nén, trước tiên chúng tôi chưng cất một BERT đa lối ra từ PLM gốc làm cả trợ lý giảng dạy (TA) và xương sống thu gọn, hiệu quả hơn trong việc cân bằng sự cân bằng giữa các lớp so với các mô hình nén. Sau đó, chúng tôi đề xuất một phương pháp cộng tác thu gọn xương sống với việc hiệu chỉnh các lối ra. Việc thu gọn như vậy giảm thiểu các cấu trúc ít đóng góp cho mỗi lối ra cũng như sự dư thừa về chiều rộng. Sau khi thu gọn, chưng cất kiến thức cụ thể tác vụ được tiến hành với các mục tiêu của biểu diễn ẩn và dự đoán của mỗi lớp để phục hồi.

Cụ thể, những đóng góp của bài báo này như sau:
• Để tối ưu hóa toàn diện hiệu quả không gian và thời gian của PLM, chúng tôi tận dụng cả thu gọn tĩnh và tăng tốc động từ góc độ quy mô mô hình và tính toán biến đổi.
• Chúng tôi đề xuất một phương pháp huấn luyện cộng tác hiệu chỉnh việc thu gọn dưới sự hướng dẫn của các lối ra trung gian và giảm thiểu sự không nhất quán của việc thoát sớm.
• Các thí nghiệm được tiến hành trên benchmark GLUE xác minh tính tối ưu Pareto của phương pháp chúng tôi. COST-EFF đạt được hiệu suất 96.5% của BERT Base đã được tinh chỉnh với khoảng 1/8 tham số và 1/19 FLOP mà không có bất kỳ hình thức tăng cường dữ liệu nào.

2 Công trình liên quan
Việc nén và tăng tốc PLM gần đây đã được nghiên cứu để vô hiệu hóa chi phí của các mô hình lớn bằng nhiều phương tiện khác nhau.

Cắt tỉa có cấu trúc nhắm mục tiêu từ nhỏ đến lớn, bao gồm các chiều ẩn (Wang et al., 2020), các đầu attention (Michel et al., 2019), các mô-đun attention đa đầu (MHA) và mạng feed-forward (FFN) (Xia et al., 2022) và toàn bộ các lớp Transformer (Fan et al., 2020). Xem xét lợi ích của cấu trúc tổng thể, chúng tôi giữ tất cả các mô-đun trong khi giảm kích thước của chúng. Ngoài việc cắt tỉa các cấu trúc, một phương pháp tinh vi là cắt tỉa không có cấu trúc cắt tỉa các trọng số. Cắt tỉa không có cấu trúc có thể đạt được độ thưa thớt cao 97% (Xu et al., 2022) nhưng chưa thích ứng được với các nền tảng tính toán và phần cứng tổng quát.

Trong quá trình huấn luyện phục hồi của các mô hình nén, các mục tiêu chưng cất kiến thức bao gồm dự đoán của các bộ phân loại (Sanh et al., 2020), các đặc trưng của biểu diễn trung gian (Jiao et al., 2020) và mối quan hệ giữa các mẫu (Tung và Mori, 2019). Ngoài ra, dịp chưng cất thay đổi từ tiền huấn luyện tổng quát và tinh chỉnh cụ thể tác vụ (Turc et al., 2019). Chưng cất cho phép huấn luyện mà không có nhãn thật bổ sung tăng cường dữ liệu. Trong bài báo này, tăng cường dữ liệu không được tận dụng vì nó đòi hỏi thời gian huấn luyện dài, nhưng phương pháp của chúng tôi thích ứng tốt với nó nếu muốn theo đuổi hiệu suất tốt hơn.

Các lối ra sớm động đến từ BranchyNet (Teerapittayanon et al., 2016), giới thiệu các nhánh thoát sau các lớp convolution cụ thể của mô hình CNN. Ý tưởng này được áp dụng cho PLM như việc thoát sớm theo lớp Transformer (Xin et al., 2021; Zhou et al., 2020; Liu et al., 2020). Tuy nhiên, việc thoát sớm chỉ tăng tốc suy luận nhưng không giảm kích thước mô hình và sự dư thừa về chiều rộng. Hơn nữa, do sự không nhất quán giữa các lớp nông và sâu, rất khó đạt được tốc độ cao chỉ bằng việc thoát sớm.

Các PLM phổ biến, ví dụ như RoBERTa (Liu et al., 2019) và XLNet (Yang et al., 2019) là các biến thể của Transformer với cấu trúc tổng thể tương tự, thích ứng tốt với các tối ưu hóa mà chúng tôi đề xuất.

--- TRANG 2 ---
Ngoài PLM với kích thước ngày càng tăng, ALBERT (Lan et al., 2020) đặc biệt với khối lượng nhỏ 18M (Triệu) tham số thu được từ việc chia sẻ trọng số của các lớp Transformer. Chia sẻ trọng số cho phép mô hình chỉ lưu trữ các tham số một lần, giảm đáng kể chi phí lưu trữ. Tuy nhiên, các trọng số được chia sẻ không đóng góp vào việc tăng tốc suy luận. Thay vào đó, thời gian cần thiết để ALBERT đạt được độ chính xác giống BERT tăng lên.

3 Phương pháp luận
Trong phần này, chúng tôi phân tích các cấu trúc chính của PLM dựa trên Transformer và đưa ra các tối ưu hóa tương ứng. COST-EFF được đề xuất có ba thuộc tính chính, đó là thu gọn tĩnh, tăng tốc động và huấn luyện cộng tác.

3.1 Kiến thức cơ bản
Trong bài báo này, chúng tôi tập trung vào việc tối ưu hóa PLM dựa trên Transformer chủ yếu bao gồm embedding, MHA và FFN. Cụ thể, embedding chuyển đổi mỗi token đầu vào thành một tensor có kích thước H (tức là chiều ẩn). Với kích thước từ vựng phổ biến |V| = 30,522, ma trận word embedding chiếm <22% tham số của BERT Base.

Bên trong Transformer, MHA có bốn ma trận WQ, WK, WV và WO, tất cả đều có kích thước đầu vào và đầu ra là H. FFN có hai ma trận WFI và WFO với kích thước H×F. Là các thành phần chính của Transformer, MHA và FFN chiếm <26% và <52% tham số của BERT Base, tương ứng.

Dựa trên phân tích, chúng tôi có các phương án thu gọn và tăng tốc sau đây. (1) Ma trận word embedding Wt được phân tách thành phép nhân của hai ma trận theo (Lan et al., 2020). Do đó, kích thước từ vựng |V| và kích thước ẩn H không thay đổi. (2) Đối với các ma trận biến đổi của MHA và FFN, cắt tỉa có cấu trúc được áp dụng để giảm chiều đầu vào hoặc đầu ra của chúng. (3) Suy luận được tăng tốc thông qua việc thoát sớm vì chúng tôi duy trì chiều sâu mô hình được tiền huấn luyện. Để tránh đưa vào các tham số bổ sung, chúng tôi loại bỏ ma trận pooler được tiền huấn luyện trước các bộ phân loại. (4) Chưng cất kiến thức về logit dự đoán và trạng thái ẩn của mỗi lớp được tận dụng làm thay thế cho việc tinh chỉnh thông thường. Kiến trúc tổng thể của COST-EFF được mô tả trong Hình 2.

3.2 Thu gọn tĩnh

3.2.1 Phân tách ma trận của Embedding
Như đã đề cập trước đây, word embedding chiếm hơn 1/5 tham số của BERT Base. Chiều đầu ra của word embedding bằng kích thước ẩn, mà chúng tôi không sửa đổi, chúng tôi sử dụng phân tách giá trị đơn cắt ngắn (TSVD) để nén word embedding matrix một cách nội bộ.

TSVD đầu tiên phân tách ma trận như Am×n = Um×m Σm×n Vn×n, trong đó Σm×n là ma trận đường chéo giá trị đơn. Sau đó, ba ma trận được cắt ngắn theo rank đã cho. Do đó, việc phân tách word embedding như:

W|V|×H_t → W|V|×R_t1 W^R×H_t2 = ŨÃ|V|×R Σ̃R×R Ṽ^R×H; (1)

trong đó chúng tôi nhân Ũ và Σ̃ matrices làm ma trận embedding đầu tiên W|V|×R_t1 và W^R×H_t2 = Ṽ là một biến đổi tuyến tính không có bias.

3.2.2 Cắt tỉa có cấu trúc của MHA và FFN
Để nén các ma trận trong MHA và FFN đóng góp cho hầu hết tham số của PLM, chúng tôi áp dụng cắt tỉa có cấu trúc để nén một chiều của các ma trận. Như được mô tả trong Hình 2, độ chi tiết cắt tỉa của MHA và FFN lần lượt là attention head và chiều ẩn.

Theo (Molchanov et al., 2017), COST-EFF có mục tiêu cắt tỉa là giảm thiểu sự khác biệt giữa mô hình đã cắt tỉa và mô hình gốc, được tính bằng khai triển Taylor bậc nhất:

|ΔL(S)| = |L(X) - L(X|hi = 0, hi ∈ S)|
≈ |∑(hi∈S) ∂L/∂hi (hi - 0) + R(1)|
≈ |∑(hi∈S) ∂L/∂hi hi|, (2)

trong đó S biểu thị một cấu trúc cụ thể, tức là một tập hợp trọng số, L(·) là hàm mất mát và ∂L/∂hi là gradient của mất mát đối với trọng số hi. |ΔL(S)| là tầm quan trọng của cấu trúc S được đo bằng giá trị tuyệt đối của số hạng bậc nhất. Để đơn giản, chúng tôi bỏ qua phần dư R(1) trong khai triển Taylor.

Trong mỗi lớp Transformer, cấu trúc S của MHA là attention head trong khi của FFN là chiều ẩn như được mô tả trong phần dưới của Hình 2. Cụ thể, các chiều đầu ra của WQ, WK, WV và WFI được nén. Ngược lại, các chiều đầu vào của WO và WFO được nén. Do đó, chiều của các trạng thái ẩn vẫn nguyên vẹn trong COST-EFF. Ngoài ra, vì một lần cắt tỉa đơn lẻ nhưng mạnh mẽ thường gây ra thiệt hại khó phục hồi, chúng tôi sử dụng cắt tỉa lặp (Tan và Motani, 2020) trong COST-EFF dần dần cắt tỉa các mô-đun không đáng kể.

3.3 Tăng tốc động

3.3.1 Suy luận với việc thoát sớm
Không giống như nén tĩnh, việc thoát sớm xác định tính toán một cách động tại thời điểm suy luận, tùy thuộc vào độ phức tạp của đầu vào và sự phức tạp của mô hình. Cụ thể, chúng tôi sử dụng việc thoát sớm theo lớp, như được hiển thị trong Hình 1, bằng cách cắm một bộ phân loại tại mỗi lớp Transformer.

Theo kết quả thử nghiệm của ElasticBERT (Liu et al., 2022), việc thoát dựa trên entropy thường vượt trội hơn dựa trên patience, chúng tôi sử dụng entropy của đầu ra bộ phân loại làm điều kiện thoát được định nghĩa là H(x) = -∑^C_{i=1} p(i) ln p(i), trong đó p(·) là phân phối xác suất được tính bằng hàm softmax và H(x) là entropy của phân phối xác suất x. Nếu entropy lớn hơn ngưỡng cho trước HT, mô hình khó đưa ra dự đoán ở trạng thái đó. Ngược lại, mô hình có xu hướng đưa ra dự đoán chắc chắn với entropy nhỏ, nơi sự khác biệt trong phân phối xác suất lớn và chiếm ưu thế.

3.3.2 Huấn luyện đa lối ra
Khi huấn luyện mô hình với đa lối ra, hàm mất mát của mỗi lối ra được tính đến. DeeBERT (Xin et al., 2020) đã giới thiệu một phương án huấn luyện hai giai đoạn nơi mô hình xương sống và các lối ra được huấn luyện riêng biệt. Tuy nhiên, chỉ với mất mát của bộ phân loại cuối cùng và các gradient lan truyền ngược, các lớp nông của mô hình xương sống không có khả năng đưa ra dự đoán tự tin mà thay vào đó phục vụ các lớp sâu. Do đó, cần thiết phải đưa vào mất mát của các bộ phân loại trung gian trong khi huấn luyện và tính toán tầm quan trọng cấu trúc dựa trên khai triển Taylor như Phương trình 2 trong COST-EFF.

Để cân bằng gradient từ đa mất mát bộ phân loại, chúng tôi sử dụng cân bằng gradient theo (Li et al., 2019) và điều chỉnh gradient của lớp k thành:

∇'_{wk}L = 1/(L-k+1) ∑^L_{i=k} ∇_{wk}Li, (3)

trong đó L là chiều sâu mô hình, ∇_{wk}Li là gradient lan truyền từ lớp i xuống lớp k và ∇'_{wk}L là gradient được điều chỉnh lại.

3.4 Huấn luyện cộng tác của COST-EFF

3.4.1 Huấn luyện với chưng cất kiến thức
Kích thước nhỏ và khả năng của mô hình nén khiến việc khôi phục hiệu suất chỉ bằng tinh chỉnh trở nên khó khăn. Trong khi chưng cất kiến thức được sử dụng như một sự bổ sung chuyển giao kiến thức từ mô hình giáo viên gốc sang mô hình học sinh nén. Trong bài báo này, chúng tôi nhắm đến việc chưng cất dự đoán và các đặc trưng trung gian (tức là trạng thái ẩn) như được mô tả trong Hình 2.

Vì sự không nhất quán giữa các lớp được quan sát (Xin et al., 2021), việc đơn giản sử dụng nhãn thật để huấn luyện một mô hình đa lối ra nén sẽ dẫn đến mâu thuẫn nghiêm trọng. Với điều này, trước tiên chúng tôi chưng cất mô hình gốc thành một mô hình BERT Base đa lối ra với cùng số lớp làm TA. Sau đó, mỗi đầu ra lớp của TA được sử dụng làm nhãn mềm của lớp tương ứng trong COST-EFF như:

L_pred = ∑^L_{i=1} CELoss(z^{TA}_i/T, z^{CE}_i/T), (4)

trong đó z^{TA}_i và z^{CE}_i là các đầu ra dự đoán của TA và COST-EFF tại lớp thứ i, tương ứng. T là yếu tố nhiệt độ thường được đặt là 1. Ngoài việc chưng cất các dự đoán, COST-EFF chưng cất các trạng thái ẩn để chuyển giao hiệu quả các biểu diễn của TA sang mô hình học sinh. Các đầu ra trạng thái ẩn, ký hiệu là Hi (i = 0, 1, ..., L+1) bao gồm đầu ra embedding H0 và mỗi đầu ra lớp Transformer, được tối ưu hóa như:

L_feat = ∑^{L+1}_{i=0} MSELoss(H^{TA}_i, H^{CE}_i). (5)

3.4.2 Quy trình COST-EFF
Như đã đề cập trong Phần 3.4.1, COST-EFF trước tiên chưng cất mô hình thành một mô hình TA đa lối ra với cùng số lớp. Cụ thể, chúng tôi chưng cất các dự đoán ở giai đoạn này. Mặc dù chưng cất đặc trưng thường mạnh mẽ hơn, các biểu diễn của mô hình đơn lối ra không được căn chỉnh với mô hình đa lối ra và sẽ đưa vào sự không nhất quán trong quá trình huấn luyện. Việc chưng cất như vậy che giấu các triển khai tầm thường của các PLM khác nhau cần được nén, cũng như sơ bộ giảm thiểu sự không nhất quán giữa các lớp với một mô hình lớn hơn và mạnh mẽ hơn. Sau đó, mô hình TA được sử dụng làm cả xương sống thu gọn và giáo viên của việc chưng cất kiến thức tiếp theo.

Trong quá trình thu gọn, chúng tôi tích hợp mất mát của các lối ra vào việc tính toán tầm quan trọng cấu trúc dựa trên khai triển Taylor. So với việc đơn giản sử dụng mất mát của bộ phân loại cuối cùng, mất mát đa lối ra giúp hiệu chỉnh việc thu gọn bằng cách cân nhắc đóng góp của các cấu trúc cho mỗi lối ra tiếp theo thay vì chỉ lớp cuối cùng. Bằng cách này, sự cân bằng giữa các lớp có thể được cân bằng tốt hơn trong mô hình được thu gọn. Sau khi thu gọn, việc huấn luyện phục hồi là một chuyển giao kiến thức theo lớp từ TA sang COST-EFF với mục tiêu giảm thiểu tổng của L_pred và L_feat giảm thiểu các mâu thuẫn của việc huấn luyện nhãn thật trên mô hình đa lối ra được thu gọn.

4 Đánh giá thực nghiệm

4.1 Thiết lập thí nghiệm
Bộ dữ liệu Chúng tôi sử dụng bốn tác vụ của benchmark GLUE (Wang et al., 2019), đó là SST-2, MRPC, QNLI và MNLI. Chi tiết của các tác vụ này được hiển thị trong Bảng 1 và hầu hết các danh mục của GLUE được bao phủ.

Phương pháp so sánh Chúng tôi so sánh các phương pháp baseline và phương pháp sau đây. (1) Các kích thước khác nhau của mô hình BERT, đó là BERT Base, BERT 6L-768H và BERT 8L-256H, được tinh chỉnh dựa trên các mô hình được tiền huấn luyện của (Turc et al., 2019). (2) Các phương pháp nén tĩnh đại diện. DistilBERT (Sanh et al., 2020) và TinyBERT (Jiao et al., 2020). (3) Các phương pháp tăng tốc động. DeeBERT (Xin et al., 2020), PABEE (Zhou et al., 2020) và mô hình đa lối ra được tiền huấn luyện ElasticBERT (Liu et al., 2022).

Cài đặt mô hình Vì số lượng tham số ảnh hưởng sâu sắc đến khả năng và hiệu suất, chúng tôi có hai nhóm so sánh với kích thước mô hình tương tự bên trong mỗi nhóm. Các mô hình trong nhóm đầu tiên có ít hơn 20M tham số và nhóm thứ hai các mô hình có kích thước lớn hơn trên 50M tham số. Chi tiết về cài đặt mô hình có thể được tìm thấy trong Bảng 2. Đáng chú ý, kết quả của DistilBERT được trích xuất từ bài báo gốc và những cái khác được chúng tôi triển khai vì các thí nghiệm liên quan đến các mô hình xương sống và dữ liệu huấn luyện khác nhau. Việc triển khai sử dụng bộ tối ưu AdamW trên một GPU RTX 3090 24GB, trong khi kích thước batch huấn luyện trong {32, 64} và tốc độ học trong {2e-5, 3e-5, 4e-5} thay đổi theo tác vụ.

--- TRANG 3 ---
4.2 Kết quả thí nghiệm

4.2.1 Kết quả tổng thể
Kết quả của COST-EFF và các phương pháp so sánh được liệt kê trong Bảng 3. Khi đếm tham số, chúng tôi bao gồm các tham số của embedding và sử dụng kích thước từ vựng 30.522 làm mặc định. FLOP được đánh giá bởi PyTorch profiler với các chuỗi đầu vào được đệm hoặc cắt ngắn đến độ dài mặc định 128 token và được tính trung bình theo tác vụ.

Trong nhóm đầu tiên, các mô hình được nén và tăng tốc cao, trong khi hiệu suất được duy trì ở khoảng 96.5% bởi COST-EFF 8, tốt hơn nhiều so với việc tiền huấn luyện và tinh chỉnh thông thường của BERT 8L-256H. Cụ thể, COST-EFF 8 vượt trội hơn TinyBERT 4 trong tất cả bốn tác vụ, cho thấy rằng một mô hình được thu gọn bảo tồn tất cả các lớp vượt trội hơn một mô hình thấp. Kiến trúc được thu gọn có nhiều khả năng trích xuất các đặc trưng phân cấp cho các thể hiện khó trong khi xử lý nhanh chóng các thể hiện đơn giản. Đối với các mô hình lớn hơn, TinyBERT 6 với chưng cất tổng quát có lợi thế nhẹ so với COST-EFF 2. Trong khi COST-EFF 2 có khối lượng nhỏ hơn TinyBERT 6 và không yêu cầu chưng cất tổng quát, khoảng cách hiệu suất không đáng kể. Trong khi đó, TinyBERT 6 không có chưng cất tổng quát bị COST-EFF 2 vượt trội trong cả hiệu quả và hiệu suất, cho thấy sự cần thiết của chưng cất tổng quát TinyBERT. Tuy nhiên, một nỗ lực lớn được yêu cầu bởi chưng cất tổng quát tiền huấn luyện một mô hình đơn lẻ có kích thước và tính toán cố định. Trong trường hợp nhu cầu tính toán thay đổi, việc tiền huấn luyện một mô hình khác có thể cực kỳ tốn thời gian. So với TinyBERT, COST-EFF có lợi thế trong cả hiệu suất và suy luận linh hoạt.

Để chứng minh hiệu quả của tăng tốc động, chúng tôi chọn thực nghiệm các thể hiện đơn giản từ tập phát triển ngắn hơn (tức là thấp hơn độ dài trung vị không đệm sau khi tokenization). Kết quả trên các thể hiện đơn giản thể hiện những cải thiện bổ sung được quy cho suy luận động, khó có được với các mô hình tĩnh. Đáng chú ý, độ dài ngắn hơn không phải lúc nào cũng cho thấy sự đơn giản. Đối với các tác vụ entailment như QNLI, đầu vào ngắn hơn sẽ chứa ít thông tin hơn, có thể làm trầm trọng thêm sự phức tạp của các mô hình ngôn ngữ. Ngoài ra, chúng tôi vẽ các đường cong hiệu suất liên quan đến điểm GLUE và FLOP trong Hình 3 và 4. Các đường cong hiệu suất là hai chiều và thể hiện tính tối ưu của các phương pháp khác nhau. Nhằm mục đích có được mô hình với tính toán và hiệu suất nhỏ hơn, chúng tôi tập trung vào các mô hình ở phần trên bên trái của hình, đó là biên Pareto được vẽ bằng các đường chấm xanh.

Như được mô tả trong Hình 3 và 4, cả COST-EFF 8 và COST-EFF 2 đều vượt trội hơn DistilBERT, DeeBERT, PABEE và các baseline BERT. So với TinyBERT và ElasticBERT, COST-EFF nói chung là tối ưu. Chúng tôi thấy rằng việc thoát sớm làm giảm giới hạn trên của hiệu suất NLI, nơi cả COST-EFF 2 và ElasticBERT 6L đều kém hơn TinyBERT 6. Vấn đề này có thể xuất phát từ sự không nhất quán giữa các lớp. Cho rằng các mẫu phức tạp trong tác vụ NLI dựa vào ngữ nghĩa cấp cao, các lớp nông nên phục vụ các lớp sâu hơn thay vì tự giải quyết tác vụ. Tuy nhiên, vấn đề này không ảnh hưởng đến tính tối ưu toàn cục. Như được hiển thị trong Hình 3, COST-EFF 8 có hiệu suất không bị chi phối so với TinyBERT 4 trên QNLI và MNLI, chứng minh tính linh hoạt của phương pháp chúng tôi.

Hiệu suất của các mô hình kết hợp việc thoát sớm bị ảnh hưởng đáng kể bởi mỗi lối ra. Trong Hình 5, chúng tôi vẽ hiệu suất theo lớp của các mô hình với việc thoát sớm trong nhóm đầu tiên và hiệu suất cuối cùng của TinyBERT 4. COST-EFF 8 đạt được hiệu suất chiếm ưu thế so với DeeBERT và PABEE. So với TinyBERT 4, COST-EFF 8 có thể đạt được hiệu suất tốt hơn từ lớp 7 đến 12, tiếp tục xác minh tuyên bố của chúng tôi rằng các mô hình mảnh khảnh vượt trội hơn các mô hình thấp về hiệu suất, được hưởng lợi từ kiến trúc được bảo tồn và khả năng trích xuất ngữ nghĩa cấp cao. Một cách khác để có được các mô hình đa lối ra mạnh mẽ là thay thế xương sống từ BERT sang ElasticBERT được tiền huấn luyện (Liu et al., 2022). Xét về tính công bằng, chúng tôi sử dụng đồng nhất BERT làm xương sống của COST-EFF và các phương pháp so sánh. Đáng chú ý, phương pháp của chúng tôi thích ứng tốt với ElasticBERT và hiệu suất tiên tiến được thể hiện trong Phụ lục A.

4.2.2 Nghiên cứu loại bỏ
Tác động của chưng cất kiến thức Các thí nghiệm loại bỏ của các chiến lược chưng cất nhằm đánh giá hiệu quả của chưng cất dự đoán và đặc trưng. Trong nghiên cứu loại bỏ này, các phương pháp so sánh là (1) loại bỏ chưng cất đặc trưng và (2) thay thế chưng cất dự đoán bằng huấn luyện nhãn thật. Kết quả được hiển thị trong Bảng 4 cho thấy rằng cả hai mục tiêu đều quan trọng.

Quy cho việc bắt chước các biểu diễn ẩn, COST-EFF 8 có lợi thế 1.6% hiệu suất so với huấn luyện không có chưng cất đặc trưng. Không có chưng cất dự đoán, hiệu suất giảm hơn 3.4%. Các công trình trước đây về nén tĩnh, ví dụ như TinyBERT (Jiao et al., 2020) và CoFi (Xia et al., 2022), thường không nhạy cảm với chưng cất dự đoán trong các tác vụ GLUE, vì phân phối đầu ra của mô hình giáo viên đơn lối ra thường phù hợp với nhãn thật. Tuy nhiên, một sự giảm lớn trong hiệu suất COST-EFF được quan sát trong Bảng 4 nếu phân phối dự đoán bị loại bỏ. Kết quả cho thấy rằng việc theo đuổi sự thật ở các lớp nông có thể làm xấu đi hiệu suất của các lớp sâu. Sự không nhất quán như vậy giữa các lớp nông và sâu thường tồn tại trong các mô hình thoát sớm, đặc biệt khó cân bằng bởi các mô hình nén với khả năng nhỏ. Thay vào đó, COST-EFF giới thiệu một mô hình TA không nén để giảm thiểu mâu thuẫn ở giai đoạn đầu và chuyển giao sự cân bằng thông qua chưng cất dự đoán.

Tác động của huấn luyện cộng tác Trong bài báo này, chúng tôi đề xuất một phương pháp cộng tác cho việc thu gọn mô hình và huấn luyện lối ra, nhằm hiệu chỉnh việc cắt tỉa của các mô-đun nông. Để xác thực hiệu quả của chiến lược huấn luyện, chúng tôi loại bỏ huấn luyện cộng tác tại các thời điểm khác nhau. Đầu tiên, chúng tôi triển khai chế độ huấn luyện hai giai đoạn như DeeBERT làm. Ngoài ra, chúng tôi triển khai COST-EFF 8 với mất mát lối ra được loại bỏ trước và trong quá trình thu gọn. So sánh theo lớp của các phương pháp trên được hiển thị trong Hình 6.

Trực quan, huấn luyện hai giai đoạn có lợi thế trên lớp cuối cùng so với huấn luyện cộng tác, vì sự không nhất quán giữa các lớp không được đưa vào. Tuy nhiên, lợi thế giảm dần ở các lớp nông, khiến hiệu suất tổng thể không thể chấp nhận được. So với thu gọn không có mất mát lối ra, phương pháp của chúng tôi có lợi thế từ 1.1% đến 2.3%. Đáng chú ý, thu gọn không có hiệu chỉnh lối ra vẫn có thể đạt được hiệu suất tương tự như COST-EFF ở các lớp nông, cho thấy rằng huấn luyện dựa trên chưng cất có hiệu quả trong việc khôi phục hiệu suất. Tuy nhiên, hiệu suất kém hơn của các lớp sâu cho thấy rằng sự cân bằng giữa các lớp không được cân bằng tốt, vì việc thu gọn được tiến hành nhằm tối ưu hóa hiệu suất của bộ phân loại cuối cùng.

5 Kết luận
Trong bài báo này, chúng tôi thu gọn tĩnh và tăng tốc động PLM trong việc theo đuổi hiệu quả suy luận cũng như bảo tồn khả năng. Để tích hợp các góc nhìn, chúng tôi đề xuất một phương pháp tối ưu hóa cộng tác đạt được lợi ích tương hỗ của thu gọn tĩnh và tăng tốc động. Cụ thể, kích thước của PLM được giảm trong chiều rộng mô hình, và suy luận có thể thích ứng với độ phức tạp của đầu vào mà không đưa vào sự dư thừa cho đầu vào đơn giản và không đầy đủ cho đầu vào khó. Các thí nghiệm so sánh được tiến hành trên benchmark GLUE và xác minh tính tối ưu Pareto của phương pháp chúng tôi ở mức nén và tăng tốc cao.

Hạn chế
COST-EFF hiện tại có những hạn chế sau đây. Nếu chúng được giải quyết trong các công trình tương lai, khả năng tiềm năng của COST-EFF có thể được giải phóng. (1) Trong quá trình suy luận của các mô hình thoát sớm động, thực hành thông thường là đặt kích thước batch là 1 để điều chỉnh tính toán tốt hơn theo các mẫu đầu vào riêng lẻ. Tuy nhiên, cài đặt như vậy không phải lúc nào cũng hiệu quả vì kích thước batch lớn hơn có khả năng giảm thời gian suy luận, trong khi độ phức tạp đầu vào bên trong một batch có thể khác nhau đáng kể. Do đó, thật thú vị khi nghiên cứu một pipeline tập hợp các mẫu với kỳ vọng tương tự về độ phức tạp vào một batch trong khi kiểm soát ưu tiên của các batch với độ phức tạp khác nhau để đạt được song song. (2) Chúng tôi chọn các tác vụ hiểu ngôn ngữ tự nhiên (NLU) để nghiên cứu nén và tăng tốc theo các baseline mạnh TinyBERT (Jiao et al., 2020) và ElasticBERT (Liu et al., 2022). Tuy nhiên, tính mở rộng của COST-EFF vẫn chưa được khám phá trong các tác vụ phức tạp hơn bao gồm tạo ngôn ngữ tự nhiên, dịch thuật, v.v. Cho đến nay, nén mô hình tĩnh được chứng minh là hiệu quả trong các tác vụ phức tạp (Gupta và Agrawal, 2022) và chúng tôi đang tìm kiếm việc mở rộng tăng tốc suy luận động trên các tác vụ khác nhau sử dụng các mô hình với một quy trình lặp.

--- TRANG 4 ---
Lời cảm ơn
Công trình này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 61976207, Số 61906187).

Tài liệu tham khảo
[Tài liệu tham khảo từ trang 9-12 được dịch tiếp theo...]
