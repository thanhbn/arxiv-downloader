# 2310.06694.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2310.06694.pdf
# Kích thước tệp: 1270497 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
SHEARED LLAMA: TĂNG TỐC TIỀN HUẤN LUYỆN 
MÔ HÌNH NGÔN NGỮ THÔNG QUA CẮT TỈA CÓ CẤU TRÚC
Mengzhou Xia1, Tianyu Gao1, Zhiyuan Zeng2, Danqi Chen1
1Princeton Language and Intelligence, Princeton University
2Department of Computer Science and Technology, Tsinghua University
{mengzhou,tianyug,danqic }@cs.princeton.edu
zengzy20@mails.tsinghua.edu.cn
TÓM TẮT
Sự phổ biến của LLaMA (Touvron et al., 2023a;b) và các mô hình ngôn ngữ lớn 
(LLM) có kích thước vừa phải khác được xuất hiện gần đây làm nổi bật tiềm 
năng xây dựng các LLM nhỏ hơn nhưng mạnh mẽ. Bất kể như vậy, chi phí huấn 
luyện các mô hình như vậy từ đầu trên hàng nghìn tỷ token vẫn cao. Trong công 
trình này, chúng tôi nghiên cứu cắt tỉa có cấu trúc như một phương tiện hiệu 
quả để phát triển các LLM nhỏ hơn từ các mô hình lớn hơn đã được tiền huấn 
luyện. Phương pháp của chúng tôi sử dụng hai kỹ thuật chính: (1) cắt tỉa có 
cấu trúc có mục tiêu, cắt tỉa một mô hình lớn hơn thành một hình dạng mục tiêu 
được chỉ định bằng cách loại bỏ các lớp, đầu, và chiều trung gian và ẩn theo 
cách end-to-end, và (2) tải batch động, cập nhật động thành phần của dữ liệu 
được lấy mẫu trong mỗi batch huấn luyện dựa trên các mất mát khác nhau trên 
các miền khác nhau. Chúng tôi chứng minh hiệu quả của phương pháp bằng cách 
trình bày chuỗi Sheared-LLaMA, cắt tỉa mô hình LLaMA2-7B xuống còn 1.3B và 
2.7B tham số. Các mô hình Sheared-LLaMA vượt trội hơn các mô hình nguồn mở 
tiên tiến có kích thước tương đương, như Pythia, INCITE, OpenLLaMA và các mô 
hình TinyLlama đồng thời, trên nhiều đánh giá downstream và instruction tuning, 
trong khi chỉ yêu cầu 3% tính toán so với huấn luyện các mô hình như vậy từ 
đầu. Công trình này cung cấp bằng chứng thuyết phục rằng tận dụng các LLM 
hiện có với cắt tỉa có cấu trúc là một phương pháp hiệu quả về chi phí hơn 
nhiều để xây dựng các LLM quy mô nhỏ cạnh tranh.1

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) có hiệu suất cực kỳ tốt trên nhiều tác vụ ngôn ngữ tự nhiên, 
nhưng chúng đòi hỏi lượng tính toán khổng lồ để huấn luyện (OpenAI, 2023; Anthropic, 2023). 
Do đó, có sự quan tâm ngày càng tăng trong việc xây dựng các mô hình có kích thước vừa phải 
mạnh mẽ, như LLaMA (Touvron et al., 2023a;b), MPT (MosaicML, 2023), và Falcon (Almazrouei 
et al., 2023), cho phép suy luận và fine-tuning hiệu quả. Các LLM này có sẵn ở nhiều kích thước 
khác nhau phù hợp cho các trường hợp sử dụng khác nhau, nhưng huấn luyện mỗi mô hình riêng 
lẻ từ đầu—thậm chí các mô hình tỷ tham số nhỏ nhất—đòi hỏi tài nguyên tính toán đáng kể có 
chi phí cấm đoán đối với hầu hết các tổ chức. Trong công trình này, chúng tôi tìm cách giải 
quyết câu hỏi sau:

Liệu chúng ta có thể tạo ra một LLM nhỏ hơn, đa mục đích và cạnh tranh bằng cách tận 
dụng các LLM đã được tiền huấn luyện hiện có, trong khi sử dụng ít tính toán hơn nhiều 
so với huấn luyện một mô hình từ đầu?

Chúng tôi khám phá cắt tỉa có cấu trúc như một phương tiện để đạt được mục tiêu này. Cắt tỉa 
thường được xem như một giải pháp để nén các mô hình cụ thể cho tác vụ (Han et al., 2016; Li 
et al., 2016; Lagunas et al., 2021; Xia et al., 2022; Kurtic et al., 2023), loại bỏ các tham số 
dư thừa và tăng tốc suy luận mà không hy sinh hiệu suất tác vụ. Tuy nhiên, đối với các LLM đa 
mục đích, cắt tỉa không thể tránh khỏi dẫn đến suy giảm hiệu suất so với các mô hình gốc 
(Frantar & Alistarh, 2023; Sun et al., 2023; Ma et al., 2023), đặc biệt khi không có đầu tư 
tính toán đáng kể sau cắt tỉa. Trong công trình này, chúng tôi sử dụng cắt tỉa như một phương 
pháp hiệu quả để phát triển các LLM nhỏ hơn nhưng cạnh tranh chỉ yêu cầu một phần nhỏ tính 
toán huấn luyện so với huấn luyện chúng từ đầu.

1Vui lòng tìm mã và mô hình của chúng tôi tại https://github.com/princeton-nlp/LLM-Shearing.
Chúng tôi trình bày các câu hỏi thường gặp và câu trả lời trong Phụ lục G.
1arXiv:2310.06694v2 [cs.CL] 11 Apr 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Chúng tôi xác định hai thách thức kỹ thuật chính trong vấn đề này. Đầu tiên, làm thế nào 
chúng ta có thể quyết định các kiến trúc được cắt tỉa cuối cùng mạnh mẽ về hiệu suất và 
hiệu quả cho suy luận? Các kỹ thuật cắt tỉa có cấu trúc hiện có cho LLM (Xia et al., 2022; 
Ma et al., 2023) không chỉ định các cấu trúc mục tiêu và dẫn đến các mô hình được cắt tỉa 
không tối ưu về hiệu suất và tốc độ suy luận (Bảng 4 và Phụ lục F.2). Thứ hai, làm thế nào 
chúng ta có thể tiếp tục tiền huấn luyện mô hình được cắt tỉa để đạt hiệu suất mong muốn? 
Chúng tôi quan sát rằng huấn luyện sử dụng dữ liệu tiền huấn luyện gốc dẫn đến tỷ lệ giảm 
mất mát không cân bằng trên các miền khác nhau, so với khi huấn luyện các mô hình như vậy 
từ đầu. Điều này cho thấy mô hình được cắt tỉa giữ lại các mức độ kiến thức khác nhau cho 
các miền khác nhau (ví dụ, GitHub vs. C4) và chỉ đơn giản sử dụng tỷ lệ miền tiền huấn 
luyện dẫn đến việc sử dụng dữ liệu không hiệu quả (Hình 4). Để giải quyết các vấn đề này, 
chúng tôi đề xuất "LLM-shearing", một thuật toán bao gồm hai thành phần sau:

OPTPythiaINCITEOpenLLaMA v1Sheared-LLaMA (của chúng tôi)OpenLLaMA v2nhanh hơn 32 lần
Hình 1: Sheared-LLaMA-2.7B vượt qua một 
loạt các mô hình nguồn mở ở quy mô tương 
tự và chỉ yêu cầu 1/32 (3%) ngân sách để 
đạt hiệu suất ngang bằng với OpenLLaMA-3B-v2.

• Chúng tôi đề xuất một thuật toán cắt tỉa 
mới, được gọi là cắt tỉa có cấu trúc có mục 
tiêu, cắt tỉa một mô hình nguồn thành một 
kiến trúc mục tiêu được chỉ định. Kiến trúc 
mục tiêu được xác định bằng cách tận dụng 
các cấu hình của các mô hình đã được tiền 
huấn luyện hiện có. Phương pháp cắt tỉa của 
chúng tôi tìm kiếm các cấu trúc con trong 
mô hình nguồn để bảo tồn hiệu suất tối đa 
trong khi tuân thủ các ràng buộc đã cho.

• Chúng tôi thiết kế một thuật toán tải batch 
động tải dữ liệu huấn luyện từ mỗi miền theo 
tỷ lệ với tỷ lệ giảm mất mát của nó, do đó 
sử dụng dữ liệu hiệu quả và tăng tốc cải 
thiện hiệu suất tổng thể.

Chúng tôi chứng minh hiệu quả của phương pháp đề xuất bằng cách cắt tỉa một mô hình LLaMA2-7B 
(Touvron et al., 2023b) thành hai LLM nhỏ hơn: Sheared-LLaMA-1.3B và Sheared-LLaMA-2.7B. 
Mặc dù chỉ sử dụng 50 tỷ token bổ sung (tức là 5% ngân sách tiền huấn luyện của OpenLLaMA) 
cho cắt tỉa và tiếp tục tiền huấn luyện, Sheared-LLaMA-1.3B và Sheared-LLaMA-2.7B vượt trội 
hơn các LLM phổ biến khác ở quy mô tương tự, bao gồm Pythia (Biderman et al., 2023), INCITE 
(TogetherAI, 2023b), và OpenLLaMA (Geng & Liu, 2023), trên 11 tác vụ downstream đại diện 
(Hình 1; hiểu biết thông thường, đọc hiểu, và kiến thức thế giới) và instruction tuning cho sinh 
ra mở. Ngoài ra, quỹ đạo hiệu suất downstream gợi ý rằng tiếp tục huấn luyện mô hình được cắt 
tỉa với nhiều token hơn sẽ dẫn đến thành tích thậm chí lớn hơn. Trong khi chúng tôi chỉ tiến 
hành thí nghiệm với các mô hình lên đến 7B tham số, thuật toán LLM-shearing của chúng tôi có 
tính tổng quát cao và có thể được mở rộng cho các mô hình ngôn ngữ lớn có kích thước bất kỳ 
trong công việc tương lai.

2 LLM-SHEARING
Cho một mô hình lớn hiện có MS (mô hình nguồn), chúng tôi nghiên cứu cách hiệu quả tạo ra một 
mô hình nhỏ hơn, mạnh mẽ MT (mô hình mục tiêu). Chúng tôi xem xét điều này như một quá trình 
hai giai đoạn: (1) Cắt tỉa MS thành MT. Điều này giảm số lượng tham số nhưng không thể tránh 
khỏi gây ra sụt giảm hiệu suất. (2) Tiếp tục tiền huấn luyện MT với một mục tiêu mô hình hóa 
ngôn ngữ tiêu chuẩn để đạt hiệu suất mục tiêu. Trong khi hầu hết các nỗ lực gần đây (Xia et al., 
2022; Ma et al., 2023) tập trung vào giai đoạn trước, chúng tôi thấy giai đoạn sau rất quan trọng 
để tạo ra các LLM đa mục đích cạnh tranh từ cắt tỉa có cấu trúc.

2.1 CẮT TỈA CÓ CẤU TRÚC CÓ MỤC TIÊU
Cắt tỉa có cấu trúc loại bỏ các nhóm tham số mô hình để nén mô hình và tăng tốc suy luận. Tuy 
nhiên, các phương pháp cắt tỉa có cấu trúc hiện có thường dẫn đến các cấu hình mô hình không 
thông thường khác biệt với các kiến trúc phổ biến. Ví dụ, CoFiPruning (Xia et al., 2022) tạo 
ra các mô hình với cấu hình lớp không đồng nhất (ví dụ, số lượng đầu khác nhau trên các lớp), 
gây ra chi phí suy luận so với các cấu hình lớp đồng nhất tiêu chuẩn (Phần 4.2).

2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
MHA 1EMBFFN 1MHA 1EMBFFN 1MHA 2FFN 2MHA 3FFN 3MHA 2FFN 2Cắt Tỉa 
Có Cấu TrúcMô Hình NgồnMô Hình Mục Tiêu
Hình 2: Cắt tỉa có cấu trúc có mục tiêu tạo ra một mô hình compact và dense có hình dạng được 
chỉ định trước. Màu sáng cho thấy các cấu trúc con được cắt tỉa. Các biến masking z được học 
để kiểm soát liệu một cấu trúc con được cắt tỉa (z = 0) hay giữ lại (z = 1).

Trong công trình này, chúng tôi nhằm cắt tỉa mô hình nguồn thành bất kỳ cấu hình mục tiêu nào 
mà chúng tôi chỉ định. Mục tiêu này thách thức vì nó đòi hỏi phẫu thuật thu nhỏ tất cả các chiều 
trong kiến trúc transformer, một nỗ lực mà theo hiểu biết của chúng tôi, chưa được hoàn thành 
trước đây cho các mô hình ngôn ngữ lớn. Chúng tôi tận dụng các cấu hình của các mô hình đã 
được tiền huấn luyện hiện có như các kiến trúc mục tiêu, dựa trên trực giác rằng các cấu hình 
này đã được tối ưu hóa tốt để cân bằng tính biểu đạt mô hình và hiệu quả suy luận. Ví dụ, 
chúng tôi sử dụng kiến trúc INCITE-Base-3B (TogetherAI, 2023a) như cấu trúc mục tiêu khi tạo 
ra một mô hình 2.7B.

Phương pháp của chúng tôi học một tập hợp các mask cắt tỉa trên các tham số mô hình ở các độ 
chi tiết khác nhau—từ những cái toàn cục như lớp và chiều ẩn (tồn tại trên tất cả các lớp), 
đến những cái cục bộ như đầu attention và chiều trung gian. Giả sử rằng mô hình nguồn MS có LS 
lớp, với mỗi lớp bao gồm một module multi-head attention (MHA) và một mạng feed-forward (FFN). 
MS có chiều trạng thái ẩn là dS, HS đầu trong mỗi MHA, và chiều trung gian là mS trong mỗi FFN. 
Chúng tôi giới thiệu các biến mask sau:

Độ chi tiết    Lớp    Chiều ẩn    Đầu    Chiều trung gian
Mask cắt tỉa   zlayer∈RLS   zhidden∈RdS   zhead∈RHS(×LS)   zint∈RmS(×LS)

Mỗi biến mask kiểm soát liệu cấu trúc con liên quan được cắt tỉa hay giữ lại. Ví dụ, chúng tôi 
loại bỏ một lớp nếu zlayer tương ứng = 0. Hình 2 minh họa một ví dụ về cách các mask cắt tỉa 
kiểm soát các cấu trúc được cắt tỉa.

Chúng tôi hình thức hóa cắt tỉa như một bài toán tối ưu có ràng buộc (Platt & Barr, 1987) nơi 
chúng tôi học các mask cắt tỉa để tìm kiếm một mạng con phù hợp với kiến trúc mục tiêu được 
chỉ định trước trong khi tối đa hóa hiệu suất.2 Theo phương pháp điều hòa ℓ0 (Louizos et al., 
2018), chúng tôi tham số hóa các mask cắt tỉa để mô hình hóa các phân phối concrete cứng. Các 
phân phối này có hỗ trợ trên [0,1] nhưng tập trung khối lượng xác suất của chúng tại 0 hoặc 1, 
cho phép các quyết định cắt tỉa hoặc giữ lại rời rạc. Trong khi công việc trước thường kiểm soát 
độ thưa thớt mục tiêu (Wang et al., 2020; Xia et al., 2022), chúng tôi sử dụng một cặp nhân tử 
Lagrange để áp đặt ràng buộc trực tiếp lên hình dạng mô hình được cắt tỉa. Ví dụ, cho số lượng 
đầu mục tiêu HT (và chúng tôi sử dụng LT, dT, và mT để biểu thị số lượng lớp mục tiêu, chiều 
ẩn, và chiều trung gian tương ứng), chúng tôi có ràng buộc được áp đặt trên một lớp đơn như:

˜Lhead(λ, ϕ, z) = λhead · ∑ zhead − HT + ϕhead · (∑ zhead − HT)²

Các ràng buộc tương tự được áp dụng để cắt tỉa các cấu trúc con khác. Nhìn chung, chúng tôi tối 
ưu hóa đồng thời trọng số mô hình và mask cắt tỉa bằng một mục tiêu min-max min_{θ,z} max_{λ,ϕ} L_prune(θ, z, λ, ϕ):

L_prune(θ, z, λ, ϕ) = L(θ, z) + ∑_{j=1}^{LS} ˜L^j_head + ∑_{j=1}^{LS} ˜L^j_int + ˜L_layer + ˜L_hidden,

nơi L(θ, z) là mất mát mô hình hóa ngôn ngữ được tính với trọng số mô hình được mask. Mục tiêu 
này sẽ tạo ra một mô hình được cắt tỉa với hình dạng mục tiêu. Lý tưởng, chạy thuật toán cắt 
tỉa này trên một lượng lớn dữ liệu sẽ trực tiếp tạo ra một mô hình compact mạnh mẽ. Trong thực 
tế, giai đoạn cắt tỉa tốn kém (khoảng 5× chậm hơn so với huấn luyện LM tiêu chuẩn), và chúng 
tôi thấy rằng các mask đã học

2Vui lòng tìm một giải thích chi tiết hơn về thuật toán trong Phụ lục A.

3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Thuật toán 1: Tải Batch Động
Yêu cầu: Dữ liệu huấn luyện của k miền D1, D2,..., Dk, dữ liệu validation D^val_1, D^val_2,..., D^val_k,
trọng số tải dữ liệu ban đầu w0∈R^k, mất mát tham chiếu ℓref∈R^k, mất mát LM L hoặc mất mát cắt tỉa
L_prune, bước huấn luyện T, đánh giá mỗi m bước, tham số mô hình θ (θ, z, ϕ, λ cho cắt tỉa)
for t = 1,..., T do
    if t mod m = 0 then
        ℓt[i] ← L(θ, z, D^val_i) nếu cắt tỉa else L(θ, D^val_i)
        ∆t[i] ← max{ℓt[i] − ℓref[i], 0} ▷ Tính chênh lệch mất mát
        wt ← UpdateWeight(wt−m, ∆t) ▷ Cập nhật tỷ lệ tải dữ liệu
    end
    Lấy mẫu một batch dữ liệu B từ D1, D2,..., Dk với tỷ lệ wt;
    if cắt tỉa then
        Cập nhật θ, z, ϕ, λ với L_prune(θ, z, ϕ, λ) trên B
    else
        Cập nhật θ với L(θ, B)
    end
end

Chương trình con UpdateWeight(w, ∆)
α ← w · exp(∆) ▷ Tính trọng số chưa chuẩn hóa
w ← α / ∑i α[i] return w ▷ Chuẩn hóa lại tỷ lệ tải dữ liệu
return θ

thường hội tụ nhanh. Do đó, chúng tôi chỉ phân bổ một ngân sách hạn chế cho cắt tỉa (xem Bảng 5).
Sau cắt tỉa, chúng tôi hoàn thiện kiến trúc được cắt tỉa bằng cách bảo tồn các thành phần có điểm 
cao nhất liên quan đến các biến mask trong mỗi cấu trúc con, và tiếp tục tiền huấn luyện mô hình 
được cắt tỉa với mục tiêu mô hình hóa ngôn ngữ. Chúng tôi gọi giai đoạn thứ hai này là tiếp tục 
tiền huấn luyện.

2.2 TỞI BATCH ĐỘNG
Tiếp tục tiền huấn luyện trên một lượng lớn dữ liệu rất quan trọng để khôi phục hiệu suất mô hình 
được cắt tỉa. Chúng tôi quan sát một phát hiện đáng ngạc nhiên trong các thí nghiệm sơ bộ của 
chúng tôi: tiếp tục tiền huấn luyện các mô hình được cắt tỉa của chúng tôi trên một bộ dữ liệu 
tiền huấn luyện hiện có RedPajama (TogetherAI, 2023b; bộ dữ liệu tiền huấn luyện được sao chép 
của LLaMA) giảm mất mát ở các tỷ lệ khác nhau trên các miền so với tiền huấn luyện một mô hình 
từ đầu, điều này biểu thị việc sử dụng dữ liệu không hiệu quả.

Để cụ thể hơn, chúng tôi bắt đầu bằng cách fitting một hàm scaling (Hoffmann et al., 2022; chi 
tiết trong Phụ lục B) trên chuỗi các mô hình LLaMA2 cho mỗi miền. Sử dụng hàm này, chúng tôi 
dự đoán mất mát của một mô hình LLaMA2 1.3B giả định nếu nó được huấn luyện từ đầu trên cùng 
dữ liệu. Sau đó chúng tôi so sánh các mất mát tham chiếu ước tính này với các mất mát của mô 
hình được cắt tỉa của chúng tôi sau tiếp tục tiền huấn luyện. Hình 4 (trái) cho thấy mất mát của 
mô hình chúng tôi trên GitHub tốt hơn mất mát tham chiếu, trong khi nó tệ hơn đáng kể so với 
mất mát tham chiếu trên C4. Quan sát này cho thấy cắt tỉa bảo tồn một lượng kiến thức lớn hơn 
trong các miền entropy thấp và nhỏ hơn (ví dụ, GitHub) so với các miền entropy cao và lớn hơn 
(ví dụ, C4). Đơn giản tái sử dụng phân phối dữ liệu tiền huấn luyện gốc³ dẫn đến việc sử dụng 
dữ liệu không hiệu quả và hiệu suất downstream tồi tệ hơn, ngay cả khi mất mát tổng thể có vẻ 
thấp, như được chứng minh sau trong Phần 4.1.

Lấy cảm hứng từ công việc gần đây (Xie et al., 2023), chúng tôi đề xuất tải batch động, một thuật 
toán hiệu quả để điều chỉnh tỷ lệ miền ngay lập tức dựa trên mất mát. Mục tiêu là đảm bảo mô 
hình đạt mất mát tham chiếu vào khoảng cùng thời gian trên các miền. Chúng tôi giới thiệu thuật 
toán dưới đây.

Thiết lập bài toán. Dữ liệu tiền huấn luyện bao gồm k miền D1, D2,..., Dk và chúng tôi có một 
bộ dữ liệu validation tách riêng cho mỗi miền, ký hiệu là D^val_i. Ở mỗi bước huấn luyện t, một 
tỷ lệ wt[i] của dữ liệu đến từ miền Di. Chúng tôi đặt một mất mát validation tham chiếu ℓref(Di) 
cho mỗi miền và huấn luyện mô hình được cắt tỉa để đạt mất mát tham chiếu.

³Dữ liệu tiền huấn luyện LLaMA2 không công khai. Chúng tôi đã tiến hành phân tích tương tự trên các mô 
hình LLaMA1 và quan sát hiện tượng tương tự, cho thấy đây là vấn đề phổ biến không liên quan đến dữ liệu 
tiền huấn luyện cụ thể.

4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Tải batch động. Chúng tôi trình bày thuật toán đầy đủ trong Thuật toán 1. Tóm tắt, cứ mỗi m 
bước, chúng tôi đánh giá mô hình để có mất mát validation ℓt (bước t) trên D^val, và cập nhật wt 
dựa trên sự khác biệt ∆t(Di) giữa ℓref[i] và ℓt[i] trên mỗi miền. Quy tắc cập nhật là tăng theo 
hàm mũ theo Xie et al. (2023),

αt = wt−m · exp(∆t); wt = αt / ∑i αt[i].

Chúng tôi áp dụng tải batch động cho cả giai đoạn cắt tỉa và giai đoạn tiếp tục tiền huấn luyện. 
Đối với cắt tỉa, chúng tôi sử dụng trọng số miền của dữ liệu tiền huấn luyện gốc làm w0. Đối với 
tiếp tục tiền huấn luyện, chúng tôi sử dụng trọng số cuối cùng từ giai đoạn cắt tỉa làm w0. Tải 
batch động là một giải pháp ngay lập tức điều chỉnh tỷ lệ dữ liệu trong quá trình huấn luyện mà 
không cần huấn luyện các mô hình phụ trợ. Nó tận dụng mất mát tham chiếu trên các tập validation 
và điều chỉnh trọng số động, thêm chi phí tối thiểu vào huấn luyện tiêu chuẩn. Phương pháp này 
khác với Xie et al. (2023), yêu cầu một quá trình đa giai đoạn phức tạp để huấn luyện các mô hình 
tham chiếu và proxy.

Rộng hơn, tải batch động có thể huấn luyện một LLM để khớp với hiệu suất của bất kỳ mô hình tham 
chiếu nào bằng cách sử dụng các bộ dữ liệu tiền huấn luyện nguồn mở như RedPajama, ngay cả khi 
không biết dữ liệu huấn luyện chính xác của mô hình tham chiếu.

Lựa chọn mất mát tham chiếu. Theo mặc định, chúng tôi sử dụng mất mát được dự đoán bởi hàm 
scaling được fitted làm tham chiếu (ký hiệu là tham chiếu scaling). Chúng tôi cũng thử nghiệm 
với một lựa chọn thay thế nơi chúng tôi trực tiếp sử dụng mất mát validation miền của mô hình 
nguồn làm tham chiếu (ký hiệu là tham chiếu nguồn). Chúng tôi cho thấy trong F.4 rằng trong khi 
cả hai biến thể đều hoạt động tốt, sử dụng tham chiếu scaling dẫn đến kết quả downstream tốt 
hơn một chút, đặc biệt là trên các tác vụ toán học và lập trình. Tuy nhiên, tham chiếu nguồn là 
một lựa chọn thay thế khả thi khi một chuỗi các mô hình nguồn ở các quy mô khác nhau không có 
sẵn.

3 THÍ NGHIỆM
3.1 THIẾT LẬP
Cấu hình mô hình. Chúng tôi sử dụng mô hình LLaMA2-7B (Touvron et al., 2023b) làm mô hình 
nguồn trong suốt tất cả các thí nghiệm chính của chúng tôi.⁴ Sau đó chúng tôi tiến hành các thí 
nghiệm cắt tỉa có cấu trúc để nén mô hình này xuống hai kích thước mục tiêu nhỏ hơn—2.7B và 
1.3B tham số. Chúng tôi so sánh với các mô hình ngôn ngữ đã được tiền huấn luyện mạnh mẽ có 
kích thước tương tự, bao gồm OPT-1.3B (Zhang et al., 2022), Pythia-1.4B (Biderman et al., 2023), 
TinyLlama-1.1B (Zhang et al., 2024), OPT-2.7B, Pythia-2.8B, INCITE-Base-3B (TogetherAI, 2023b), 
OpenLLaMA-3B-v1, và OpenLLaMA-3B-v2 (Geng & Liu, 2023). Chúng tôi sử dụng Pythia-1.4B và 
INCITE-Base-3B làm kiến trúc mục tiêu cho mô hình 1.3B và 2.7B tương ứng. Bảng 8 tóm tắt chi 
tiết kiến trúc mô hình của tất cả các mô hình này.

Bảng 1: Tóm tắt các bộ dữ liệu tiền 
huấn luyện được sử dụng bởi Sheared-LLaMA 
và các mô hình khác.
Mô hình    Dữ liệu Tiền huấn luyện    #Token
LLaMA1     Dữ liệu LLaMA              1T
LLaMA2     Không rõ                   2T
OPT        Dữ liệu OPT⁵               300B
Pythia     The Pile                   300B
INCITE-Base RedPajama                 800B
OpenLLaMA v1 RedPajama                1T
OpenLLaMA v2 Dữ liệu OpenLLaMA⁶       1T
TinyLlama   Dữ liệu TinyLlama⁷        3T
Sheared-LLaMA RedPajama               50B

Dữ liệu. Vì dữ liệu huấn luyện cho LLaMA2 không 
có sẵn công khai, chúng tôi sử dụng RedPajama 
(TogetherAI, 2023b), là một bộ dữ liệu tiền huấn 
luyện được sao chép của các mô hình LLaMA1 
(Touvron et al., 2023a), để cắt tỉa và tiếp tục 
tiền huấn luyện. Bộ dữ liệu này bao gồm dữ liệu 
huấn luyện từ bảy miền: CommonCrawl, C4, Github, 
Wikipedia, Books, ArXiv, và StackExchange. Chúng 
tôi xây dựng một tập validation tách riêng với 2 
triệu token (tương đương với 500 chuỗi 4,096 token) 
cho mỗi miền. Chúng tôi phân bổ 0.4 tỷ token cho 
giai đoạn cắt tỉa và 50 tỷ token cho quá trình 
tiếp tục tiền huấn luyện. Theo quy ước của LLaMA2, 
chúng tôi duy trì độ dài chuỗi 4,096 token. Bảng 1 
cung cấp tóm tắt dữ liệu tiền huấn luyện được sử 
dụng bởi các mô hình của chúng tôi và các mô hình 
baseline.

⁴Vui lòng tìm kết quả trên các mô hình LLaMA1 trong Phụ lục F.6 và các mô hình Pythia trong Phụ lục F.5.
⁵Dữ liệu OPT chứa BookCorpus (Zhu et al., 2015), Stories (Trinh & Le, 2018), CCNews (Hamborg et al., 
2017), the Pile (Gao et al., 2020), và PushShift.io Reddit (Baumgartner et al., 2020).
⁶OpenLLaMA v2 được tiền huấn luyện với hỗn hợp RefinedWeb (Penedo et al., 2023), StarCoder (Li et al., 
2023), và một phần của RedPajama.
⁷Dữ liệu TinyLlama là hỗn hợp của SlimPajama (Shen et al., 2023) và dữ liệu StarCoder.

5

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 2: Sheared-LLaMA vượt trội hơn các mô hình có sẵn công khai có kích thước tương đương trên các tác vụ downstream. Số shot được sử dụng được ghi chú trong ngoặc đơn, với 0-shot nếu không được chỉ định. Các mô hình với † sử dụng dữ liệu huấn luyện khác với RedPajama. Vui lòng tham khảo Bảng 1 để biết chi tiết.

Hiểu biết Thông thường & Đọc hiểu
Mô hình (#token để huấn luyện)    SciQ  PIQA  WinoGrande  ARC-E  ARC-C (25)  HellaSwag (10)
LLaMA2-7B (2T)†                  93.7  78.1  69.3        76.4   53.0        78.6
OPT-1.3B (300B)†                 84.3  71.7  59.6        57.0   29.7        54.5
Pythia-1.4B (300B)†              86.4  70.9  57.4        60.7   31.2        53.0
TinyLlama-1.1B (3T)†             88.9  73.3  58.8        55.3   30.1        60.3
Sheared-LLaMA-1.3B (50B)         87.3  73.4  57.9        61.5   33.5        60.7
OPT-2.7B (300B)†                 85.8  73.7  60.8        60.8   34.0        61.5
Pythia-2.8B (300B)†              88.3  74.0  59.7        64.4   36.4        60.8
INCITE-Base-3B (800B)            90.7  74.6  63.5        67.7   40.2        64.8
Open-LLaMA-3B-v1 (1T)            91.3  73.7  61.5        67.6   39.6        62.6
Open-LLaMA-3B-v2 (1T)†           91.8  76.2  63.5        66.5   39.0        67.6
Sheared-LLaMA-2.7B (50B)         90.8  75.8  64.2        67.0   41.2        70.8

Tiếp tục LM    Kiến thức Thế giới
Mô hình (#token để huấn luyện)    LogiQA  BoolQ (32)  LAMBADA  NQ (32)  MMLU (5)  Trung bình
LLaMA2-7B (2T)†                  30.7    82.1        28.8     73.9     46.6      64.6
OPT-1.3B (300B)†                 26.9    57.5        58.0     6.9      24.7      48.2
Pythia-1.4B (300B)†              27.3    57.4        61.6     6.2      25.7      48.9
TinyLlama-1.1B (3T)†             26.3    60.9        58.8     12.1     25.5      50.0
Sheared-LLaMA-1.3B (50B)         26.9    64.0        61.0     9.6      25.7      51.0
OPT-2.7B (300B)†                 26.0    63.4        63.6     10.1     25.9      51.4
Pythia-2.8B (300B)†              28.0    66.0        64.7     9.0      26.9      52.5
INCITE-Base-3B (800B)            27.7    65.9        65.3     14.9     27.0      54.7
Open-LLaMA-3B-v1 (1T)            28.4    70.0        65.4     18.6     27.0      55.1
Open-LLaMA-3B-v2 (1T)†           28.1    69.6        66.5     17.1     26.9      55.7
Sheared-LLaMA-2.7B (50B)         28.9    73.7        68.4     16.5     26.4      56.7

Đánh giá. Chúng tôi sử dụng gói lm-evaluation-harness (Gao et al., 2021) để đánh giá trên một 
bộ tác vụ downstream rộng lớn: (1) Chúng tôi theo Pythia và LLaMA2 để báo cáo độ chính xác 
0-shot của ARC easy (ARC-E; Clark et al., 2018), LAMBADA (Paperno et al., 2016), LogiQA (Liu 
et al., 2020), PIQA (Bisk et al., 2020), SciQ (Welbl et al., 2017), và WinoGrande (Sakaguchi 
et al., 2021). (2) Chúng tôi báo cáo độ chính xác của các tác vụ được sử dụng bởi Open LLM 
Leaderboard (Beeching et al., 2023), bao gồm 10-shot HellaSwag (Zellers et al., 2019), 25-shot 
ARC Challenge (ARC-C; Clark et al., 2018), và 5-shot MMLU (Hendrycks et al., 2021). (3) Chúng 
tôi cũng báo cáo exact match của 32-shot Natural Questions (NQ; Kwiatkowski et al., 2019) để 
đo kiến thức thực tế trong mô hình. Vì huấn luyện các mô hình để theo hướng dẫn đã trở thành 
một ứng dụng quan trọng của LLM (Ouyang et al., 2022; Taori et al., 2023), chúng tôi đánh giá 
các mô hình của chúng tôi trên instruction tuning và fine-tune cả các mô hình baseline và 
Sheared-LLaMA trên các cặp instruction-response được lấy mẫu từ bộ dữ liệu ShareGPT.⁸ Vui lòng 
tham khảo Phụ lục E để biết thêm chi tiết.

3.2 SHEARED-LLAMA VƯỢT TRỘI HƠN CÁC LM CÓ KÍCH THƯỚC TƯƠNG ĐƯƠNG
Chúng tôi chứng minh rằng Sheared-LLaMA vượt trội hơn các LLM hiện có có kích thước tương tự 
trên cả các benchmark LLM tiêu chuẩn và instruction tuning, trong khi chỉ sử dụng một phần nhỏ 
ngân sách tính toán cần thiết để huấn luyện các mô hình đó từ đầu.

Các tác vụ downstream. Bảng 2 trình bày hiệu suất tác vụ downstream zero-shot và few-shot của 
Sheared-LLaMA và các mô hình đã được tiền huấn luyện có kích thước tương tự. Ngay cả với ngân 
sách hạn chế khoảng 50B token để cắt tỉa và tiếp tục tiền huấn luyện, các mô hình Sheared-LLaMA 
vượt trội hơn các mô hình hiện có được tiền huấn luyện trên tính toán lớn hơn đáng kể. 
Sheared-LLaMA-1.3B vượt trội hơn OPT-1.3B, Pythia-1.4B (được tiền huấn luyện với 300B token), 
và TinyLlama-1.1B (được tiền huấn luyện

⁸https://sharegpt.com/

6

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
57.4%42.7%25.0%50.0%75.0%Sheared-LLaMA-1.3BPythia-1.4B63.5%36.6%25.0%50.0%75.0%Sheared-LLaMA-2.7BINCITE-Base-3B54.3%45.8%25.0%50.0%75.0%Sheared-LLaMA-2.7BOpen-LLaMA-v2-3B56.6%43.5%25.0%50.0%75.0%Sheared-LLaMA-2.7BOpen-LLaMA-v1-3B

Hình 3: Các Sheared-LLaMA vượt trội hơn Pythia-1.4B, INCITE-Base-3B, OpenLLaMA-3B-v1 và 
OpenLLaMA-3B-v2 trong instruction tuning.

trên 3T token). Sheared-LLaMA-2.7B vượt trội hơn INCITE-Base-3B (được tiền huấn luyện trên 800B 
token RedPajama), OpenLLaMA-3B-v1 (được tiền huấn luyện trên 1T token RedPajama), và 
OpenLLaMA-3B-v2 (được huấn luyện trên 1T token từ RedPajama, RefinedWeb, và StarCoder). Kết 
quả đáng chú ý nhất là Sheared-LLaMA-1.3B vượt trội hơn TinyLlama-1.1B, mặc dù TinyLlama-1.1B 
được tiền huấn luyện trên 3T token—nhiều hơn tổng dữ liệu được sử dụng để tiền huấn luyện 
LLaMA2-7B và quá trình cắt tỉa của chúng tôi cộng lại. Điều này chứng minh rằng cắt tỉa có cấu 
trúc là một phương pháp hiệu quả mẫu hơn để huấn luyện các LLM quy mô nhỏ.

Instruction tuning. Như được hiển thị trong Hình 3, Sheared-LLaMA được instruction-tuned đạt 
tỷ lệ thắng cao hơn so với tất cả các mô hình đã được tiền huấn luyện khác ở quy mô tương đương. 
Điều này chứng minh rằng mô hình 2.7B của chúng tôi có thể phục vụ như một nền tảng mạnh mẽ 
cho instruction tuning và có khả năng tạo ra các phản hồi dài, mạch lạc và thông tin (Xem ví dụ 
trong Phụ lục E).

4 PHÂN TÍCH
4.1 HIỆU QUẢ CỦA TẢI BATCH ĐỘNG
Chúng tôi phân tích hiệu quả của tải batch động bằng cách kiểm tra tác động của nó đối với ba 
khía cạnh: (1) mất mát LM cuối cùng trên các miền, (2) việc sử dụng dữ liệu của mỗi miền trong 
suốt quá trình huấn luyện, (3) hiệu suất tác vụ downstream. Tất cả kết quả trong phần này dựa 
trên Sheared-LLaMA-1.3B.⁹

Sự khác biệt mất mát trên các miền. Tải batch động nhằm cân bằng tỷ lệ giảm mất mát trên các 
miền, đảm bảo rằng các mất mát đạt giá trị tham chiếu vào khoảng cùng thời gian. Hình 4 cho 
thấy sự khác biệt giữa mất mát của mô hình chúng tôi (với cả tải batch gốc và động) và mất mát 
tham chiếu, được ước tính bằng cách fitting một hàm scaling cho một mô hình LLaMA2 1.3B tham 
số giả định. Tải batch gốc dẫn đến sự khác biệt mất mát rất khác nhau trên các miền; ví dụ, 
mất mát GitHub giảm dưới giá trị tham chiếu, trong khi mất mát C4 tụt lại phía sau. Tuy nhiên, 
tải batch động giảm mất mát đều đặn và dẫn đến sự khác biệt mất mát rất tương tự trên các 
miền, gợi ý việc sử dụng dữ liệu hiệu quả hơn.

Sử dụng dữ liệu. Bảng 3 so sánh tỷ lệ dữ liệu của RedPajama và việc sử dụng dữ liệu của phương 
pháp tải động của chúng tôi (Hình 6 minh họa cách trọng số miền thay đổi trong quá trình huấn 
luyện). Nó cho thấy tải batch động tải nhiều dữ liệu hơn từ các tập con Book và C4, cho thấy 
các miền này thách thức hơn đối với một mô hình được cắt tỉa để khôi phục.

Bảng 3: Sử dụng dữ liệu miền với tải batch động so với tỷ lệ gốc.
                    CC     GitHub  Book   StackExchange  Wiki   ArXiv  C4
RedPajama (Gốc)     67.0%  4.5%    4.5%   2.0%          4.5%   2.5%   15.0%
Tải Batch Động     36.1%  0.8%    9.1%   1.0%          3.1%   0.7%   49.2%

Hiệu suất downstream. Như được hiển thị trong Hình 5, các mô hình được cắt tỉa được huấn luyện 
với tải batch động đạt hiệu suất downstream tốt hơn so với khi được huấn luyện trên phân phối 
RedPajama gốc. Điều này gợi ý rằng việc giảm mất mát cân bằng hơn từ tải batch động chuyển 
đổi thành khả năng downstream được cải thiện.

⁹Chúng tôi cũng thử nghiệm với một phương pháp heuristic để loại trừ các miền dễ khỏi cắt tỉa, nhưng thấy 
rằng vấn đề chênh lệch mất mát vẫn tồn tại sau tiếp tục tiền huấn luyện. Vui lòng tham khảo Phụ lục F.8 
để biết thêm chi tiết.

7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
CC
GitHubBookSEWikiArXivC40.00.1Chênh lệch Mất mátGốc
CC
GitHubBookSEWikiArXivC40.00.1Tải Batch Động

Hình 4: Chênh lệch mất mát giữa mô hình được 
cắt tỉa (1.3B) và mất mát tham chiếu ước tính, 
với tải batch gốc vs. động.

10 20 30 40 50
#Token để Huấn luyện (B)4748495051Độ chính xác Downstream Trung bình (%)Gốc 
Tải Batch Động

Hình 5: Hiệu suất tác vụ downstream của 
Sheared-LLaMA-1.3B với tỷ lệ dữ liệu gốc 
và tải batch động.

4.2 SO SÁNH VỚI CÁC PHƯƠNG PHÁP CẮT TỈA KHÁC
Chúng tôi so sánh LLM-shearing của chúng tôi với các phương pháp cắt tỉa khác về perplexity 
validation, một chỉ số mạnh mẽ của khả năng mô hình tổng thể (Xia et al., 2023).

Các mô hình được cắt tỉa có mục tiêu có tốc độ suy luận cao hơn. Các công trình trước như 
CoFiPruning (Xia et al., 2022) tạo ra các mô hình được cắt tỉa có cấu trúc, nhưng các mô hình 
này thường có cấu hình lớp không đồng nhất (ví dụ, số lượng đầu khác nhau trên các lớp). Sự 
không đồng nhất như vậy trên các lớp gây ra chi phí huấn luyện và suy luận do tính bất thường 
trong kiến trúc mô hình. Chúng tôi thử nghiệm với cả CoFiPruning và cắt tỉa có cấu trúc có mục 
tiêu, với ngân sách cắt tỉa 0.4B với tỷ lệ dữ liệu RedPajama để so sánh công bằng. Bảng 4 cho 
thấy các mô hình được cắt tỉa có mục tiêu của chúng tôi có tốc độ suy luận cao hơn so với mô 
hình CoFiPruning được cắt tỉa không đồng nhất ở cùng độ thưa thớt, mặc dù có perplexity cao 
hơn một chút. Cắt tỉa có cấu trúc có mục tiêu cần khoảng 0.5B token nhiều hơn trong tiếp tục 
tiền huấn luyện để khớp với perplexity của CoFiPruning. Tuy nhiên, tính toán bổ sung một lần 
này trong huấn luyện được biện minh, vì nó dẫn đến kiến trúc mô hình hiệu quả hơn rất cần 
thiết cho các ứng dụng thực tế và sử dụng thực tế hiệu quả. Vui lòng tìm thêm chi tiết về 
tốc độ suy luận của các phương pháp cắt tỉa khác nhau trong Phụ lục F.9.

Bảng 4: Perplexity validation và tốc độ sinh ra trong suy luận (token/giây) của cắt tỉa có cấu 
trúc có mục tiêu với cấu hình lớp đồng nhất, và CoFiPruning, với cấu hình lớp không đồng nhất. 
Tốc độ suy luận được đo trên GPU Nvidia A100 (80G), trên một instance đơn sinh ra lên đến 512 
token.

Cấu hình Lớp    PPL ↓   Tốc độ↑   Cấu hình Lớp    PPL ↓   Tốc độ↑
1.3B CoFiPruning   9.1    51        2.7B CoFiPruning   7.0    37
Cắt tỉa có mục tiêu 10.3   58        Cắt tỉa có mục tiêu 7.7    43

So sánh với LLM-Pruner (Ma et al., 2023). Chúng tôi so sánh cắt tỉa có cấu trúc có mục tiêu với 
LLM-Pruner, một công trình gần đây về cắt tỉa có cấu trúc, trong Phụ lục F.2. Chúng tôi chứng 
minh rằng, với cùng ngân sách tính toán, mức độ thưa thớt, và phân phối dữ liệu huấn luyện, các 
mô hình được cắt tỉa của chúng tôi đạt perplexity thấp hơn, có kiến trúc được tối ưu hóa hơn, 
và tốc độ suy luận nhanh hơn.

4.3 PHÂN TÍCH BỔ SUNG
Bảng 5: Phân bổ ngân sách dữ liệu cho 
cắt tỉa và tiếp tục tiền huấn luyện (CT) 
và perplexity tương ứng.
# Token    PPL
Cắt tỉa  CT    Cắt tỉa  CT
0.2B     4.6B  12.99    7.46
0.4B     4.4B  10.29    7.32
0.8B     4.0B  9.01     7.23
1.6B     3.2B  8.04     7.08

Phân bổ ngân sách cho cắt tỉa và tiếp tục tiền 
huấn luyện. Trực quan, phân bổ nhiều tính toán 
hơn cho giai đoạn cắt tỉa giúp xác định các cấu 
trúc mạng con tốt hơn. Chúng tôi khám phá việc 
phân phối dữ liệu khác nhau trên các giai đoạn 
cắt tỉa và tiếp tục tiền huấn luyện, trong một 
ngân sách cố định 5B token. Bảng 5 cho thấy khi 
kiểm soát tổng số token, tăng ngân sách cắt tỉa 
liên tục cải thiện perplexity. Tuy nhiên, vì cắt 
tỉa đắt hơn tiếp tục tiền huấn luyện, chúng tôi 
quyết định phân bổ 0.4B token cho cắt tỉa. Vui 
lòng tham khảo Phụ lục C để biết chi tiết về 
throughput huấn luyện.

8

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Phân tích khác. Chúng tôi cung cấp phân tích sâu hơn trong phụ lục: (1) Đánh giá Sheared-LLaMA 
về toán học và lập trình (Phụ lục F.3), (2) Cắt tỉa mô hình Pythia (Phụ lục F.5), và (3) tác động 
của việc loại trừ các miền dễ trong quá trình cắt tỉa (Phụ lục F.8).

5 CÔNG TRÌNH LIÊN QUAN
Cắt tỉa. Cắt tỉa có cấu trúc đã được nghiên cứu rộng rãi như một kỹ thuật nén mô hình trong thị 
giác máy tính và xử lý ngôn ngữ tự nhiên, nơi các mô hình cụ thể cho tác vụ như phân loại thường 
được tham số hóa quá mức và có thể được cắt tỉa đáng kể với tác động tối thiểu đến hiệu suất 
(Han et al., 2016; Wen et al., 2016; Liu et al., 2017; Luo et al., 2017; Cai et al., 2019; Deng 
et al., 2020; Hou et al., 2020; Wang et al., 2020; Lagunas et al., 2021; Xia et al., 2022; Kurtic 
et al., 2023). Cắt tỉa không có cấu trúc (Frankle & Carbin, 2018; Li et al., 2020; Chen et al., 
2020; Sanh et al., 2020) cắt tỉa các neuron riêng lẻ thay vì các khối có cấu trúc. Mặc dù cắt 
tỉa không có cấu trúc thường đạt tỷ lệ nén cao hơn, chúng không thực tế để tăng tốc mô hình.

Trong kỷ nguyên LLM, pipeline NLP phổ biến đã chuyển từ các mô hình cụ thể cho tác vụ sang các 
LM đa mục đích, để lại ít chỗ cho sự dư thừa. Cả cắt tỉa không có cấu trúc, cắt tỉa bán cấu trúc 
(Frantar & Alistarh, 2023; Sun et al., 2023), và cắt tỉa có cấu trúc (Ma et al., 2023) đều dẫn 
đến sụt giảm hiệu suất đáng kể trên LLM ngay cả ở độ thưa thớt khiêm tốn. Đáng chú ý, tất cả 
các công trình trước đều cố định các mô hình gốc hoặc điều chỉnh chúng tối thiểu. Chúng tôi xem 
cắt tỉa như một khởi tạo và xem xét việc chi tiêu tính toán đáng kể để tiếp tục tiền huấn luyện 
mô hình để khôi phục hiệu suất là cần thiết.

Các phương pháp tiền huấn luyện hiệu quả. Như trực giao với phương pháp cắt tỉa của chúng tôi, 
có một khối lượng công trình rộng lớn về cải thiện hiệu quả huấn luyện LLM. Ví dụ, lượng tử 
hóa giảm độ chính xác số của trọng số mô hình và activations và tăng tốc huấn luyện và suy luận 
(Dettmers et al., 2022; 2023; Xiao et al., 2023). Chưng cất kiến thức (Hinton et al., 2015; 
Sanh et al., 2019; Jiao et al., 2020; Sun et al., 2020), huấn luyện một mô hình nhỏ hơn trên 
dự đoán của mô hình lớn hơn, được chứng minh là hiệu quả cho các mô hình cụ thể cho tác vụ 
(Xia et al., 2022). Đối với tiền huấn luyện LLM, mặc dù chưng cất từ một mô hình thầy được chứng 
minh cải thiện chất lượng của các mô hình học sinh với cùng số bước huấn luyện (Rae et al., 2021; 
Blakeney et al., 2022), nó kém hiệu quả về chi phí hơn cắt tỉa và huấn luyện tiếp tục do chi 
phí suy luận vượt quá được gây ra bởi mô hình thầy (Jha et al., 2023). Nhiều phương pháp hơn 
đã được giới thiệu để nâng cao hiệu quả huấn luyện LM, như kiến trúc động (Gong et al., 2019; 
Zhang & He, 2020) và các optimizer hiệu quả (Chen et al., 2023; Liu et al., 2023). Tuy nhiên, 
như được chỉ ra bởi (Kaddour et al., 2023; Bartoldson et al., 2023), các lợi ích được hứa hẹn 
trong hiệu quả huấn luyện có thể không được thực hiện một cách nhất quán.

Cũng có các phương pháp dựa trên dữ liệu để nâng cao hiệu quả huấn luyện. Loại bỏ dữ liệu trùng 
lặp được thấy là hiệu quả (Lee et al., 2021). Các kỹ thuật lựa chọn batch khác nhau đề xuất ưu 
tiên dữ liệu dựa trên tiêu chí như mất mát cao hơn (Jiang et al., 2019) hoặc mất mát có thể 
giảm lớn hơn (Mindermann et al., 2022). Xie et al. (2023) đề xuất tối ưu hỗn hợp dữ liệu bằng 
cách huấn luyện một mô hình proxy để ước tính trọng số dữ liệu tối ưu của mỗi miền.

6 THẢO LUẬN
Hạn chế và công việc tương lai. Đầu tiên, phương pháp phụ thuộc nhiều vào sự có sẵn của các 
bộ dữ liệu và mô hình tiền huấn luyện nguồn mở. Nếu một miền cụ thể không được bao phủ trong 
dữ liệu tiền huấn luyện, phương pháp có thể không khôi phục hiệu suất tốt trên miền đó. Thứ hai, 
do ràng buộc tính toán, chúng tôi chỉ thử nghiệm với một mô hình 7B tham số làm mô hình nguồn. 
Tuy nhiên, phương pháp của chúng tôi có tính tổng quát cao và có thể được mở rộng lên các mô hình 
lớn hơn trong nghiên cứu tương lai.

Kết luận. Công trình này đề xuất cắt tỉa có cấu trúc như một phương pháp hiệu quả để tạo ra các 
LLM quy mô nhỏ cạnh tranh. Phương pháp hai giai đoạn của chúng tôi kết hợp cắt tỉa có cấu trúc 
có mục tiêu và tiếp tục tiền huấn luyện (tiếp tục tiền huấn luyện), và chúng tôi giới thiệu tải 
batch động để cải thiện hiệu quả dữ liệu tiền huấn luyện. Chúng tôi huấn luyện một chuỗi các mô 
hình Sheared-LLaMA cạnh tranh sử dụng một phần nhỏ tính toán cần thiết cho tiền huấn luyện tiêu 
chuẩn. Kết quả của chúng tôi cho thấy một con đường đầy hứa hẹn để tạo ra các LLM nhỏ chi phí 
thấp khi có các mô hình quy mô lớn mạnh mẽ. Khi các LLM có khả năng hơn và các bộ dữ liệu tiền 
huấn luyện lớn hơn xuất hiện, phương pháp của chúng tôi có thể dễ dàng mở rộng cho các tiến bộ 
này để tạo ra các mô hình nhỏ thậm chí tốt hơn.

9

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
LỜI CẢM ƠN
Chúng tôi bày tỏ lòng biết ơn đến Sadhika Malladi, Tanya Goyal, Ofir Press, Adithya Bhaskar, và 
nhóm Princeton NLP vì đã xem xét bài báo và cung cấp phản hồi hữu ích. Chúng tôi cũng cảm ơn 
đội ngũ kỹ thuật tại MosaicML vì sự hỗ trợ vô giá của họ với các chi tiết triển khai sử dụng gói 
Composer. Mengzhou Xia được hỗ trợ bởi Bloomberg Data Science Ph.D. Fellowship, và Tianyu Gao 
được hỗ trợ bởi IBM PhD Fellowship. Nghiên cứu này cũng được hỗ trợ bởi Microsoft Azure credits 
thông qua sáng kiến "Accelerate Foundation Models Academic Research".

TÀI LIỆU THAM KHẢO
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co-
jocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic,
Badreddine Noune, Baptiste Pannier, và Guilherme Penedo. Falcon-40B: an open large lan-
guage model with state-of-the-art performance. 2023.

Anthropic. Introducing claude, 2023.

Brian R Bartoldson, Bhavya Kailkhura, và Davis Blalock. Compute-efficient deep learning: Algo-
rithmic trends and opportunities. Journal of Machine Learning Research, 24:1–77, 2023.

Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, và Jeremy Blackburn. The
pushshift reddit dataset. ArXiv, abs/2001.08435, 2020.

Edward Beeching, Clémentiné Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Ra-
jani, Omar Sanseviero, Lewis Tunstall, và Thomas Wolf. Open llm leaderboard. https:
//huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023.

Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric
Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.
Pythia: A suite for analyzing large language models across training and scaling. In International
Conference on Machine Learning, pp. 2397–2430. PMLR, 2023.

Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical com-
monsense in natural language. In Proceedings of the AAAI conference on artificial intelligence,
volume 34, pp. 7432–7439, 2020.

Cody Blakeney, Jessica Zosa Forde, Jonathan Frankle, Ziliang Zong, và Matthew L Leavitt. Reduce, reuse, recycle: Improving training efficiency with distillation. arXiv preprint
arXiv:2211.00683, 2022.

Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, và Song Han. Once-for-all: Train one
network and specialize it for efficient deployment. In International Conference on Learning
Representations, 2019.

Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, và
Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. In Advances in
Neural Information Processing Systems, 2020.

Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xu-
anyi Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms.
arXiv preprint arXiv:2302.06675, 2023.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, và
Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.
arXiv preprint arXiv:1803.05457, 2018.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. Flashattention: Fast and memory-
efficient exact attention with io-awareness. Advances in Neural Information Processing Systems,
35:16344–16359, 2022.

10

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Lei Deng, Guoqi Li, Song Han, Luping Shi, và Yuan Xie. Model compression and hardware
acceleration for neural networks: A comprehensive survey. Proceedings of the IEEE, 108(4):
485–532, 2020.

Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. Llm.int8(): 8-bit matrix
multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. Qlora: Efficient finetuning
of quantized llms. arXiv preprint arXiv:2305.14314, 2023.

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
Guestrin, Percy Liang, và Tatsunori B Hashimoto. Alpacafarm: A simulation framework for
methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.

Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In International Conference on Learning Representations, 2018.

Elias Frantar và Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in
one-shot, 2023. arXiv preprint arXiv:2301.00774, 2023.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text
for language modeling. arXiv preprint arXiv:2101.00027, 2020.

Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric
Tang, Anish Thite, Ben Wang, Kevin Wang, và Andy Zou. A framework for few-shot language
model evaluation, September 2021.

Xinyang Geng và Hao Liu. Openllama: An open reproduction of llama, May 2023.

Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, và Tieyan Liu. Efficient training of
bert by progressively stacking. In International conference on machine learning, pp. 2337–2346.
PMLR, 2019.

Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats L Richter, Quentin Anthony, Eugene
Belilovsky, Irina Rish, và Timothée Lesort. Continual pre-training of large language models:
How to (re) warm your model? arXiv preprint arXiv:2308.04014, 2023.

Felix Hamborg, Norman Meuschke, Corinna Breitinger, và Bela Gipp. news-please: A generic
news crawler and extractor. In Proceedings of the 15th International Symposium of Information
Science, pp. 218–223, 2017.

Song Han, Huizi Mao, Dally, và William Dally. Deep compression: Compressing deep neural
networks with pruning, trained quantization and huffman coding. In International Conference on
Learning Representations, 2016.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob
Steinhardt. Measuring massive multitask language understanding. In International Conference on
Learning Representations, 2021.

Geoffrey Hinton, Oriol Vinyals, và Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train-
ing compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, và Qun Liu. Dynabert: Dynamic
bert with adaptive width and depth. Advances in Neural Information Processing Systems, 33:
9782–9793, 2020.

Ananya Harsh Jha, Dirk Groeneveld, Emma Strubell, và Iz Beltagy. Large language model distil-
lation doesn't need a teacher. arXiv preprint arXiv:2305.14864, 2023.

11

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Angela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean, Gregory R
Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C Lipton, et al. Accelerating
deep learning by focusing on the biggest losers. arXiv preprint arXiv:1910.00762, 2019.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, và Qun Liu.
Tinybert: Distilling bert for natural language understanding. In Findings of the Association for
Computational Linguistics: EMNLP 2020, pp. 4163–4174, 2020.

Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, và Matt J Kusner. No train no gain:
Revisiting efficient training algorithms for transformer-based language models. arXiv preprint
arXiv:2307.06440, 2023.

Eldar Kurtic, Elias Frantar, và Dan Alistarh. Ziplm: Hardware-aware structured pruning of lan-
guage models. arXiv preprint arXiv:2302.04089, 2023.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion
Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, và Slav
Petrov. Natural questions: A benchmark for question answering research. Transactions of the
Association for Computational Linguistics, 7:452–466, 2019.

François Lagunas, Ella Charlaix, Victor Sanh, và Alexander M Rush. Block pruning for faster
transformers. arXiv preprint arXiv:2109.04838, 2021.

Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-
Burch, và Nicholas Carlini. Deduplicating training data makes language models better. arXiv
preprint arXiv:2107.06499, 2021.

Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for
efficient convnets. In International Conference on Learning Representations, 2016.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,
Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with
you! arXiv preprint arXiv:2305.06161, 2023.

Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, và Joey Gonzalez.
Train big, then compress: Rethinking model size for efficient training and inference of transform-
ers. In International Conference on machine learning, pp. 5958–5968. PMLR, 2020.

Hong Liu, Zhiyuan Li, David Hall, Percy Liang, và Tengyu Ma. Sophia: A scalable stochastic
second-order optimizer for language model pre-training. arXiv preprint arXiv:2305.14342, 2023.

Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, và Yue Zhang. Logiqa: A
challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of
the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 3622–
3628, 2020.

Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. Learn-
ing efficient convolutional networks through network slimming. In Proceedings of the IEEE
international conference on computer vision, pp. 2736–2744, 2017.

Christos Louizos, Max Welling, và Diederik P Kingma. Learning sparse neural networks through
l0 regularization. In International Conference on Learning Representations, 2018.

Jian-Hao Luo, Jianxin Wu, và Weiyao Lin. Thinet: A filter level pruning method for deep neural
network compression. In Proceedings of the IEEE international conference on computer vision,
pp. 5058–5066, 2017.

Xinyin Ma, Gongfan Fang, và Xinchao Wang. Llm-pruner: On the structural pruning of large
language models. arXiv preprint arXiv:2305.11627, 2023.

Sören Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Win-
nie Xu, Benedikt Höltgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized
training on points that are learnable, worth learning, and not yet learnt. In International Confer-
ence on Machine Learning, pp. 15630–15649. PMLR, 2022.

12

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
MosaicML. composer, 2021.

MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.
Truy cập: 2023-05-05.

OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in neural information processing systems, 35, 2022.

Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi,
Sandro Pezzelle, Marco Baroni, Gemma Boleda, và Raquel Fernández. The LAMBADA dataset:
Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1525–1534, 2016.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,
Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, và Julien Launay. The refinedweb
dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv
preprint arXiv:2306.01116, 2023.

John Platt và Alan Barr. Constrained differential optimization. In Neural Information Processing
Systems, 1987.

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. Winogrande: An adver-
sarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.

Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

Victor Sanh, Thomas Wolf, và Alexander Rush. Movement pruning: Adaptive sparsity by fine-
tuning. Advances in Neural Information Processing Systems, 33:20378–20389, 2020.

Noam M. Shazeer. Glu variants improve transformer. ArXiv, abs/2002.05202, 2020.

Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria
Soboleva, và Eric Xing. Slimpajama-dc: Understanding data combinations for llm training.
arXiv preprint arXiv:2309.10818, 2023.

Mingjie Sun, Zhuang Liu, Anna Bair, và J Zico Kolter. A simple and effective pruning approach
for large language models. arXiv preprint arXiv:2306.11695, 2023.

Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, và Denny Zhou. Mobilebert:
a compact task-agnostic bert for resource-limited devices. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pp. 2158–2170, 2020.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, và Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model,
2023.

TogetherAI. Redpajama-incite-base-3b-v1, 2023a.

TogetherAI. Redpajama: An open source recipe to reproduce llama training dataset, 2023b.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.

13

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Trieu H. Trinh và Quoc V. Le. A simple method for commonsense reasoning. ArXiv,
abs/1806.02847, 2018.

Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, và
Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926,
2023a.

Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao Wang, Hailin Hu,
Zheyuan Bai, Yun Wang, et al. Pangu-pi: Enhancing language model architectures via nonlinear-
ity compensation. arXiv preprint arXiv:2312.17276, 2023b.

Ziheng Wang, Jeremy Wohlwend, và Tao Lei. Structured pruning of large language models. In
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 6151–6162, 2020.

Johannes Welbl, Nelson F. Liu, và Matt Gardner. Crowdsourcing multiple choice science questions.
In Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 94–106, 2017.

Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, và Hai Li. Learning structured sparsity in
deep neural networks. Advances in neural information processing systems, 29, 2016.

Mengzhou Xia, Zexuan Zhong, và Danqi Chen. Structured pruning learns compact and accurate
models. In Proceedings of the 60th Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pp. 1513–1528, Dublin, Ireland, May 2022. Association for
Computational Linguistics. doi: 10.18653/v1/2022.acl-long.107.

Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen,
Luke Zettlemoyer, và Veselin Stoyanov. Training trajectories of language models across scales.
In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pp. 13711–13738, Toronto, Canada, July 2023. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2023.acl-long.767.

Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, và Song Han. Smoothquant:
Accurate and efficient post-training quantization for large language models. In International
Conference on Machine Learning, pp. 38087–38099. PMLR, 2023.

Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang,
Quoc V Le, Tengyu Ma, và Adams Wei Yu. Doremi: Optimizing data mixtures speeds up
language model pretraining. arXiv preprint arXiv:2305.10429, 2023.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. HellaSwag: Can a ma-
chine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pp. 4791–4800, 2019.

Minjia Zhang và Yuxiong He. Accelerating training of transformer-based language models with
progressive layer dropping. Advances in Neural Information Processing Systems, 33:14011–
14023, 2020.

Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, và Wei Lu. Tinyllama: An open-source small
language model. arXiv preprint arXiv:2401.02385, 2024.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-
pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer
language models. arXiv preprint arXiv:2205.01068, 2022.

Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright,
Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully
sharded data parallel. arXiv preprint arXiv:2304.11277, 2023.

Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-
ralba, và Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. 2015 IEEE International Conference on Computer Vision
(ICCV), pp. 19–27, 2015.

14

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
MỤC LỤC
1 Giới thiệu 1
2 LLM-Shearing 2
2.1 Cắt Tỉa Có Cấu Trúc Có Mục Tiêu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2.2 Tải Batch Động . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3 Thí nghiệm 5
3.1 Thiết lập . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Sheared-LLaMA Vượt Trội Hơn Các LM Có Kích Thước Tương Đương . . . . . . . . . . . . 6
4 Phân tích 7
4.1 Hiệu Quả của Tải Batch Động . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.2 So Sánh với Các Phương Pháp Cắt Tỉa Khác . . . . . . . . . . . . . . . . . . . . . . . 8
4.3 Phân Tích Bổ Sung . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 Công Trình Liên Quan 9
6 Thảo Luận 9
A Giải Thích Chi Tiết về Tham Số Hóa Mask Cắt Tỉa 16
B Mất Mát Tham Chiếu Được Dự Đoán bởi Luật Scaling 16
C Chi Tiết Huấn Luyện 17
D Cấu Hình Mô Hình 17
E Instruction Tuning 17
F Kết Quả Thí Nghiệm Bổ Sung 18
F.1 Sử Dụng Dữ Liệu trong Tiếp Tục Tiền Huấn Luyện . . . . . . . . . . . . . . . . . . . . 18
F.2 So Sánh với LLM-Pruner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
F.3 Lập Trình và Lý Luận Toán Học . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
F.4 Tham Chiếu Scaling vs. Tham Chiếu Nguồn . . . . . . . . . . . . . . . . . . . . . . . 20
F.5 Cắt Tỉa Mô Hình Pythia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
F.6 Cắt Tỉa từ LLaMA1 vs LLaMA2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
F.7 So Sánh với Tiếp Tục Tiền Huấn Luyện Thêm INCITE-Base-3B . . . . . . . . . . . . . . 22
F.8 Loại Trừ Các Miền Dễ Trong Quá Trình Cắt Tỉa . . . . . . . . . . . . . . . . . . . . . 23
F.9 Phân Tích Tốc Độ Suy Luận . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
G Câu Hỏi Thường Gặp 24

15

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
A GIẢI THÍCH CHI TIẾT VỀ THAM SỐ HÓA MASK CẮT TỈA
Ý tưởng chính đằng sau thuật toán cắt tỉa là áp dụng mask cho các tham số mô hình. Sau khi học 
một mask nhị phân, nó tương đương với việc loại bỏ các tham số tương ứng. Mask được tham số 
hóa sử dụng phân phối concrete cứng được giới thiệu trong Louizos et al. (2018). Cho một biến 
masking z được tham số hóa bởi α, phân phối concrete cứng được định nghĩa như sau:

u = U(0,1)
s = Sigmoid(1/β * (log(u/(1-u)) + log α))
s̄ = s(ζ - γ) + γ
z = min(1, max(0, s̄))

nơi U là phân phối đều, β là tham số nhiệt độ, s là mask nhị phân được làm mềm tuân theo phân 
phối concrete cứng, và ζ và γ là các giới hạn của phân phối concrete cứng. Phân phối concrete 
cứng phục vụ như một relaxation liên tục của mask nhị phân, cho phép mô hình học mask nhị phân 
theo cách liên tục trong quá trình huấn luyện. Hiệu quả của thủ thuật này trong việc học các cấu 
trúc thưa thớt trong mạng neural đã được chứng minh trong các nghiên cứu trước (Wang et al., 
2020; Xia et al., 2022). Trong các thí nghiệm của chúng tôi, chúng tôi đặt β = 0.83, ζ = 1.1, và 
γ = -0.1.

Để thực thi ràng buộc độ thưa thớt, các mask được huấn luyện cùng với các nhân tử Lagrange λ, 
như được định nghĩa trong Phương trình (1). Sau cắt tỉa, các tham số tương ứng với các mask đã 
học được loại bỏ để đảm bảo rằng hình dạng mô hình kết quả khớp với mô hình mục tiêu. Trong 
triển khai thực tế, chúng tôi đặt một ngưỡng để nhị phân hóa các mask. Do việc áp dụng phân 
phối concrete cứng, các mask thường hội tụ đến các giá trị nhị phân khớp với hình dạng mô hình 
mục tiêu trong hầu hết các trường hợp, do đó tránh được bất kỳ sự không nhất quán nào. Tuy 
nhiên, trong các trường hợp hiếm khi các mask không hội tụ đến chính xác 0 hoặc 1, các biến 
masking cần được hấp thụ vào các tham số mô hình kết quả.

Như thảo luận trong Phần 2, chúng tôi áp dụng mask cho đầu, chiều trung gian, lớp và chiều ẩn. 
Đối với đầu, chúng tôi đơn giản nhân đầu ra đầu với mask. Đối với chiều trung gian, chúng tôi áp 
dụng mask cho đầu ra trung gian. Đối với lớp, chúng tôi áp dụng mask cho đầu ra lớp. Đối với 
chiều ẩn, chúng tôi áp dụng mask cho cả đầu ra đầu và mlp. Áp dụng mask cho đầu ra tương đương 
với việc loại bỏ các tham số tương ứng. Vui lòng tham khảo composer llama.py để biết thêm chi tiết.

B MẤT MÁT THAM CHIẾU ĐƯỢC DỰ ĐOÁN BỞI LUẬT SCALING
Luật scaling của mô hình hóa ngôn ngữ là một hàm của kích thước mô hình N và kích thước bộ dữ 
liệu D:

L(N, D) = E + A/N^α + B/D^β

nơi E bắt giữ mất mát cho phân phối ngôn ngữ thực trong một quá trình sinh lý tưởng, và A, α, 
B, β là các hệ số scaling liên quan đến quy mô mô hình hoặc kích thước dữ liệu. Các mô hình trong 
cùng một họ mô hình thường được huấn luyện với cùng số lượng token trên cùng phân phối dữ liệu. 
Trong trường hợp này, chúng ta cần tối thiểu ba mô hình để ước tính hằng số E + B/D^β, A và α. 
Nếu các mô hình được huấn luyện với số lượng token khác nhau, chúng ta có thể ước tính E, A, α, 
B, β với tối thiểu 5 mô hình. Lưu ý rằng chúng tôi sẽ ước tính các hệ số scaling cho mỗi miền 
riêng biệt.

Các mô hình LLaMA2 đã được huấn luyện trên cùng 2T token (Touvron et al., 2023b). Chúng tôi lấy 
các checkpoint LLaMA2-7B, LLaMA2-13B, và LLaMA2-70B, đánh giá chúng trên tập validation của 
mỗi miền, và fit các hệ số scaling với mất mát tương ứng. Cho các điểm dữ liệu hạn chế để ước 
tính hằng số luật scaling, mất mát được chiếu của một mô hình LLaMA-2.7B giả định có thể thiên 
lệch so với giá trị thực. Bảng 6 trình bày mất mát được dự đoán. Quá trình đánh giá mất ít hơn 
4 giờ GPU A100 để hoàn thành.

16

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 6: Mất mát tham chiếu ước tính của các mô hình LLaMA2-1.3B và LLaMA2-2.7B giả định.
                CC     GitHub  Book    StackExchange  Wiki    ArXiv   C4
LLaMA2-1.3B     1.964  0.746   2.139   1.612         1.759   1.445   2.125
LLaMA2-2.7B     1.871  0.688   2.033   1.535         1.630   1.356   2.033

C CHI TIẾT HUẤN LUYỆN
Chúng tôi trình bày các siêu tham số được sử dụng trong các thí nghiệm của chúng tôi trong Phụ 
lục C. Chúng tôi sử dụng fully sharded data parallel (Zhao et al., 2023) để huấn luyện các mô 
hình của chúng tôi song song. Chúng tôi sử dụng FlashAttention V1 (Dao et al., 2022) để tăng 
tốc huấn luyện. Chúng tôi sử dụng một scheduler learning rate cosine và giảm learning rate xuống 
tối thiểu 10% giá trị đỉnh. Chúng tôi tiến hành một số thí nghiệm sơ bộ để xác định learning rate 
đỉnh cho việc học các biến masking và nhân tử Lagrange, và chúng tôi thấy rằng learning rate 1.0 
hoạt động tốt cho cắt tỉa. Chúng tôi không điều chỉnh bất kỳ siêu tham số nào khác. Throughput 
phụ thuộc vào việc triển khai và chúng tôi tin rằng throughput của chúng tôi có thể được cải thiện 
thêm bằng cách áp dụng các tối ưu hóa tiên tiến gần đây hơn như FlashAttention V2 (Dao et al., 
2022) và phiên bản Composer gần đây hơn (MosaicML, 2021).

Bảng 7: Siêu tham số huấn luyện và throughput.
                           Cắt Tỉa    Tiếp Tục Tiền Huấn Luyện
Ngân sách huấn luyện       0.4B       50B
Learning rate của z, ϕ, λ  1.0        -
Learning Rate của θ        0.0001     0.0001
Tỷ lệ warmup LR           10%        3%
Batch size (token)        131K       1M
Khoảng đánh giá m (bước)  50         400
Bước                      3,200      51,200
# GPU                     8          16
Throughput (token/s)      15K        145K (1.3B) / 77K (2.7B)

D CẤU HÌNH MÔ HÌNH
Trong phần này, chúng tôi cung cấp các cấu hình mô hình cho cả các mô hình Sheared-LLaMA của 
chúng tôi và các mô hình baseline, như được minh họa trong Bảng 8. Thiết kế của chúng tôi tuân 
thủ chặt chẽ kiến trúc của Pythia-1.4B và INCITE-Base-3B, mặc dù có một số khác biệt tinh tế. 
Một sự khác biệt đáng chú ý được tìm thấy trong kích thước trung gian của Sheared-LLaMA, là 
hệ quả của nguồn gốc từ LLaMA2-7B. Đáng chú ý, LLaMA2-7B sử dụng một biến thể GLU (Shazeer, 
2020) trong lớp feed-forward của nó, bao gồm một ma trận gate, một ma trận upward-projection, 
và một ma trận downward-projection. Ngược lại, các mô hình khác sử dụng cấu trúc lớp feed-forward 
hai ma trận thông thường. Hơn nữa, chúng tôi thừa nhận rằng thuật toán shearing sẽ phải kế thừa 
chiều đầu của mô hình nguồn. Thay vì chỉ định rõ ràng số lượng đầu dựa trên các mô hình ngôn 
ngữ hiện có, chúng tôi đặt số lượng đầu mục tiêu bằng chiều ẩn mục tiêu chia cho chiều đầu của 
mô hình nguồn.

E INSTRUCTION TUNING
Chúng tôi đánh giá các mô hình của chúng tôi trên instruction tuning và fine-tune cả các mô hình 
Sheared-LLaMA và baseline trên 10,000 cặp instruction-response được lấy mẫu từ bộ dữ liệu 
ShareGPT¹⁰. Để đánh giá, chúng tôi lấy mẫu thêm 1,000 instruction từ ShareGPT, sinh ra phản 
hồi từ các mô hình được fine-tune của chúng tôi và các mô hình baseline khác, và sử dụng GPT-4 
làm evaluator để so sánh hai phản hồi (Dubois et al., 2023). Chúng tôi báo cáo tỷ lệ thắng của 
mô hình chúng tôi so với mô hình baseline.

Trong quá trình huấn luyện instruction tuning, instruction được đặt trước với "You are a helpful 
assistant. Write a response that appropriately completes the request.". Để đánh giá instruction 
tuning

¹⁰https://sharegpt.com. Chúng tôi chỉ sử dụng vòng đầu tiên trong lịch sử chat đa vòng.

17

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 8: Cấu hình mô hình của Sheared-LLaMA và các mô hình baseline của chúng tôi.
Mô hình                    #Param  #Lớp  Ẩn    Trung gian  #Đầu  Chiều Đầu
OPT-1.3B                   1.3B    24    2048   8192        32    64
Pythia-1.4B                1.4B    24    2048   8192        16    128
TinyLlama-1.1B             1.1B    22    2048   5632        32    64
Sheared-LLaMA-1.3B         1.3B    24    2048   5504        16    128
OPT-2.7B                   2.7B    32    2560   10240       32    80
Pythia-2.8B                2.8B    32    2560   10240       32    80
INCITE-Base-3B             2.8B    32    2560   10240       32    80
OpenLLaMA-3B               2.7B    26    3200   8640        32    100
Sheared-LLaMA-2.7B         2.7B    32    2560   6912        20    128
LLaMA2-7B                  6.7B    32    4096   11008       32    128

generations, Wang et al. (2023a) quan sát việc sử dụng các mô hình GPT làm thẩm phán có thể 
thay đổi sở thích của nó khi hoán đổi thứ tự trình bày của hai đầu ra. Do đó, chúng tôi so sánh 
mỗi cặp đầu ra hai lần bằng cách hoán đổi thứ tự trình bày của hai đầu ra và cuối cùng báo cáo 
tỷ lệ thắng trung bình của hai vòng để loại bỏ bias vị trí.

Chúng tôi chọn ngẫu nhiên một đầu ra được tạo bởi Sheared-LLaMA-1.3B và Sheared-LLaMA-2.7B 
để phản hồi một instruction đã cho, và trình bày các generation trong Bảng 10. Các phát hiện 
của chúng tôi chứng minh rằng, sau instruction tuning, Sheared-LLaMA-2.7B liên tục tạo ra các 
đầu ra dài, mạch lạc và thông tin để phản hồi instruction.

Bảng 9: Siêu tham số huấn luyện cho instruction tuning.
Instruction Tuning
Learning Rate của θ      5e−5
Tỷ lệ warmup LR         3%
Batch size (token)      128
# GPU                   8

F KẾT QUẢ THÍ NGHIỆM BỔ SUNG
F.1 SỬ DỤNG DỮ LIỆU TRONG TIẾP TỤC TIỀN HUẤN LUYỆN
Hình 6 minh họa sự tiến hóa của trọng số miền trong suốt quá trình huấn luyện và việc sử dụng 
dữ liệu tích lũy cuối cùng cho mỗi miền. Quỹ đạo cho thấy các trọng số miền ổn định sau khoảng 
30% huấn luyện. Trái ngược với các miền khác, Wikipedia thể hiện một spike bất thường sớm trong 
quá trình huấn luyện. Các miền còn lại cho thấy một sự thay đổi ổn định, đơn điệu trong việc tải 
dữ liệu theo thời gian, như mong đợi.

0 10 20 30 40 50
#Token để Huấn luyện (B)0.00.10.20.30.40.50.60.70.8Trọng Số Miền
0 10 20 30 40 50 60 70
Tỷ Lệ Dữ Liệu trong Huấn luyện (%)CC
GitHub
Book
SE
Wiki
Arxiv
C467.0%
4.5%
4.5%
2.0%
4.5%
2.5%
15.0%36.1%
0.8%
9.1%
1.0%
3.1%
0.7%
49.2%Gốc
Tải Batch Động

Hình 6: Trái: Trọng số dữ liệu của mỗi batch trong giai đoạn tiếp tục tiền huấn luyện. Phải: Sử 
dụng dữ liệu tích lũy cho mỗi miền.

18

--- TRANG 19 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 10: Một ví dụ đầu ra từ Sheared-LLaMA-1.3B và Sheared-LLaMA-2.7B. Nó cho thấy 
Sheared-LLaMA có thể tạo ra các phản hồi dài, mạch lạc và thông tin sau instruction tuning.

Prompt: Act as semiconductor industry analyst
Sheared-LLaMA-1.3B: Với tư cách là một nhà phân tích ngành bán dẫn, tôi sẽ 
chịu trách nhiệm tiến hành nghiên cứu thị trường và phân tích để giúp các 
công ty trong ngành hiểu và phản ứng với xu hướng thị trường, sở thích khách 
hàng và động lực cạnh tranh. Điều này sẽ bao gồm phân tích dữ liệu thị trường 
như doanh số, thị phần và xu hướng ngành, cũng như tiến hành phỏng vấn với 
các nhà lãnh đạo ngành, giám đốc điều hành và khách hàng để có được insight 
về nhu cầu và sở thích của khách hàng. Tôi cũng sẽ chịu trách nhiệm phát 
triển và triển khai các chiến lược marketing để quảng bá sản phẩm và dịch vụ, 
và để xác định và theo đuổi các cơ hội kinh doanh mới.

Sheared-LLaMA-2.7B: Với tư cách là một nhà phân tích ngành bán dẫn, trách 
nhiệm chính của tôi là cung cấp phân tích sâu sắc và chính xác về thị trường 
bán dẫn toàn cầu. Điều này bao gồm đánh giá hiệu suất của các công ty riêng 
lẻ, ngành và xu hướng trong thị trường.
Đây là một số lĩnh vực chính mà tôi sẽ tập trung phân tích:
1. Quy mô và tăng trưởng thị trường: Tôi sẽ theo dõi quy mô của thị trường 
bán dẫn toàn cầu và phân tích triển vọng tăng trưởng cho mỗi phân khúc, như 
bộ nhớ, logic và cảm biến.
2. Bối cảnh cạnh tranh: Tôi sẽ đánh giá bối cảnh cạnh tranh, bao gồm các 
player hàng đầu, thị phần của họ và các động thái chiến lược.
3. Xu hướng công nghệ: Tôi sẽ theo dõi sự phát triển của các công nghệ mới, 
như 5G, AI và IoT, và tác động của chúng đối với ngành bán dẫn.
4. Xu hướng ngành: Tôi sẽ theo dõi xu hướng ngành, như sự chuyển đổi hướng 
tới các công nghệ bền vững, và đánh giá tác động của chúng đối với thị trường.
5. Dự báo thị trường: Tôi sẽ cung cấp dự báo cho thị trường bán dẫn, bao 
gồm tăng trưởng doanh thu và đơn vị vận chuyển, dựa trên nhiều kịch bản và 
giả định.
Ngoài các lĩnh vực này, tôi cũng sẽ theo dõi tin tức và phát triển ngành, và 
cung cấp insight về tác động của chúng đối với thị trường.

F.2 SO SÁNH VỚI LLM-PRUNER
Để đảm bảo so sánh công bằng với phương pháp LLM-Pruner, chúng tôi khớp các tham số (loại 
trừ embedding) để gần bằng với mô hình cuối cùng của chúng tôi (1.23B), vì kích thước embedding 
không ảnh hưởng đến tốc độ suy luận. Chúng tôi tiếp tục tiền huấn luyện các mô hình được cắt 
tỉa có nguồn gốc từ cả LLM-Pruner và phương pháp cắt tỉa có cấu trúc có mục tiêu đề xuất của 
chúng tôi. Tổng số token cho cắt tỉa và tiếp tục tiền huấn luyện được kiểm soát giống nhau, và 
dữ liệu từ bộ dữ liệu RedPajama được sử dụng trực tiếp mà không áp dụng tải batch động. Chúng 
tôi chứng minh rằng phương pháp cắt tỉa có cấu trúc có mục tiêu đề xuất của chúng tôi là một 
phương pháp tốt hơn so với LLM-Pruner từ ba khía cạnh: quỹ đạo mất mát, kiến trúc mô hình và 
tốc độ suy luận.

Về quỹ đạo mất mát, Hình 7 cho thấy phương pháp cắt tỉa có cấu trúc có mục tiêu đề xuất của 
chúng tôi đạt mất mát thấp hơn so với LLM-Pruner khi tiêu thụ cùng lượng dữ liệu.

Bảng 11 so sánh cấu hình mô hình cho một mô hình được cắt tỉa LLM-Pruner và mô hình được cắt 
tỉa của chúng tôi. Mô hình LLM-Pruner có kiến trúc không thông thường nơi kích thước trung gian 
nhỏ hơn kích thước ẩn, phần lớn do khả năng của thuật toán không thể cắt tỉa chiều ẩn và lớp, 
tiết lộ một hạn chế của LLM-Pruner.

Về throughput huấn luyện và tốc độ suy luận, chúng tôi thấy các cấu trúc Sheared-LLaMA chạy 
hiệu quả hơn so với các mô hình LLM-Pruner. Chúng tôi đã thực hiện phân tích tốc độ suy luận 
so sánh kiến trúc mô hình LLM-pruner và Sheared-LLaMA sử dụng một GPU A100 đơn để tạo ra 
lên đến 2048 token. Như được hiển thị trong Bảng 12, kiến trúc mô hình được cắt tỉa của chúng 
tôi hiệu quả hơn đáng kể so với LLM-Pruner tại thời điểm suy luận. Ngoài ra, kiến trúc mô hình 
của LLM-Pruner gây ra chi phí đáng kể trong quá trình tiếp tục tiền huấn luyện (Đo với 16 GPU 
A100 80GB.), với throughput huấn luyện khoảng 60% của Sheared-LLaMA. Nhìn chung, kiến trúc 
Sheared-LLaMA của chúng tôi cho phép throughput cao hơn cho cả suy luận và tiếp tục huấn luyện 
so với LLM-Pruner.

Tóm lại, chúng tôi đã chứng minh rằng ở cùng quy mô tham số, phương pháp cắt tỉa của chúng 
tôi tạo ra một mô hình có perplexity (mất mát) thấp hơn, kiến trúc mô hình cuối cùng hợp lý hơn 
và tốc độ suy luận nhanh hơn. Chúng tôi đã hiệu quả cho thấy thuật toán cắt tỉa có cấu trúc có 
mục tiêu của chúng tôi hiệu quả hơn cho việc cắt tỉa LLM quy mô lớn so với LLM-Pruner.

1 2 3 4 5 6
#Token để Huấn luyện (B)1.92.12.32.5Mất mátLLM-Pruner (Ma et al., 2023)
Của chúng tôi

Hình 7: Mất mát của LLM-Pruner và Sheared-LLaMA trong giai đoạn tiếp tục tiền huấn luyện. Lưu 
ý rằng chúng tôi loại trừ tải batch động và sử dụng cùng phân phối dữ liệu để huấn luyện cả hai 
mô hình để so sánh công bằng.

Bảng 11: Cấu trúc mô hình của Pythia-1.4B, LLM-pruner (1.6B), và Của chúng tôi (1.3B).
          Lớp  Đầu  Kích thước Đầu  Kích thước Trung gian  Kích thước Ẩn  Tham số
Pythia-1.4B       24   16   128              8192                  2048         1.4B
LLM-pruner (1.6B) 32   7    128              2201                  4096         1.6B
Sheared-LLaMA (1.3B) 24 16   128              5504                  2048         1.3B

Bảng 12: Throughput huấn luyện và tốc độ sinh ra của LLM-pruner (1.6B) và Sheared-LLaMA 
(1.3B). Với số lượng tham số tương tự, cấu trúc mô hình được cắt tỉa của chúng tôi có perplexity 
thấp hơn khi được fine-tune với cùng lượng token (khoảng 6B token). Tuy nhiên, kiến trúc mô 
hình được cắt tỉa của chúng tôi hiệu quả hơn nhiều cho cả huấn luyện và suy luận.

                    Tốc độ Sinh ra  Throughput Huấn luyện  PPL
LLM-Pruner          43 token/s      83K token/s            7.09
Sheared-LLaMA       58 token/s      139K token/s           6.85

F.3 LẬP TRÌNH VÀ LÝ LUẬN TOÁN HỌC
Chúng tôi kiểm tra khả năng toán học và lập trình của các mô hình được cắt tỉa của chúng tôi so 
với các mô hình ngôn ngữ khác. Chúng tôi thấy rằng khả năng toán học của các mô hình 3B tham 
số hiện có, bao gồm Sheared-LLaMA, vẫn còn xa so với các mô hình lớn hơn. Chúng tôi cũng thấy 
rằng khả năng lập trình của Sheared-LLaMA tụt lại phía sau các mô hình được biết là được huấn 
luyện trên nhiều dữ liệu code hơn, như Pythia-1.4B và Open-LLaMA-3B-v2. Khả năng lập trình 
của Sheared-LLaMA có thể đến từ mô hình LLaMA2 gốc, được suy đoán đã sử dụng nhiều dữ liệu 
code hơn, và dữ liệu code tối thiểu được sử dụng trong các thí nghiệm cắt tỉa của chúng tôi.

F.4 THAM CHIẾU SCALING VS. THAM CHIẾU NGUỒN
Hình 8 Phần này so sánh hiệu suất của Sheared-LLaMA khi được huấn luyện với tham chiếu scaling 
và tham chiếu nguồn trong tải batch động. Tham chiếu scaling sử dụng mất mát được dự đoán từ 
luật scaling làm mất mát tham chiếu, trong khi tham chiếu nguồn sử dụng mất mát của

20

--- TRANG 20 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 13: Kết quả đánh giá trên GSM8K và HumanEval và tỷ lệ huấn luyện và token trong ArXiv và GitHub.

                            GSM8K (8)  HumanEval           ArXiv  Github  ArXiv  GitHub
Mô hình                     EM         Pass@1  Pass@5     Tỷ lệ  Tỷ lệ   Token  Token
LLaMA2-7B                   13.7       12.8    23.8       -      -       -      -
OPT-2.7B                    0.1        0.0     0.0        -      -       -      -
Pythia-2.8B                 1.7        5.1     14.6       9.0%   7.6%    26.9   22.8
INCITE-Base-3B              1.8        4.3     4.9        2%     4.5%    16.0   36.0
Open-LLaMA-3B-v1            2.5        0.0     1.2        2%     4.5%    20.0   45.0
Open-LLaMA-3B-v2            2.7        10.4    20.1       -      -       -      -
Sheared-LLaMA-2.7B (Nguồn)  2.7        3.7     5.5        0.7%   0.4%    0.3    0.2
Sheared-LLaMA-2.7B (Scaling) 2.4       4.9     9.2        1.0%   0.8%    0.5    0.4

mô hình nguồn làm mất mát tham chiếu. Mặc dù cả hai phương pháp đều huấn luyện mô hình hiệu 
quả, tham chiếu scaling liên tục đạt hiệu suất downstream tốt hơn một chút.

10 20 30 40 50
#Token để Huấn luyện (B)51525354555657Độ chính xác Downstream Trung bình (%)Tham chiếu nguồn
Tham chiếu scaling

Hình 8: Hiệu suất downstream trung bình của 
Sheared-LLaMA-1.3B với tham chiếu scaling 
và tham chiếu nguồn.

F.5 CẮT TỈA MÔ HÌNH PYTHIA
Trong quá trình phát triển ban đầu của phương pháp, chúng tôi đã thử nghiệm với một mô hình 
quy mô nhỏ hơn trên Pythia (Biderman et al., 2023), một chuỗi các mô hình nguồn mở với dữ 
liệu huấn luyện nguồn mở trên các quy mô từ 70M đến 13B. Chúng tôi lấy mô hình Pythia-440M, 
cắt tỉa nó xuống còn 160M tham số, và tiếp tục tiền huấn luyện nó sử dụng dữ liệu huấn luyện 
của các mô hình Pythia Gao et al. (2020). Cụ thể, chúng tôi sử dụng 0.4B token để cắt tỉa và 
33B token (32,000 bước) để tiếp tục tiền huấn luyện mô hình được cắt tỉa. Bảng 14 cho thấy mô 
hình được cắt tỉa đạt perplexity thấp hơn so với mô hình gốc, và tiếp tục tiền huấn luyện cải 
thiện hiệu suất thêm. Đáng chú ý, với việc tiêu thụ tính toán tối thiểu (10B token), việc cắt 
tỉa một mô hình Pythia-410M đạt hiệu suất gần bằng với việc tiền huấn luyện Pythia-160M từ đầu. 
Thêm nhiều token hơn nữa cải thiện hiệu suất.

Bảng 14: Hiệu suất zero-shot của Pythia-160M và Sheared-Pythia.
                               Token Huấn luyện  Hiệu suất
Pythia-160M                    300B             43.56
Sheared-Pythia (300B) + 10B                     43.51
Sheared-Pythia (300B) + 33B                     45.78

Ngoài ra, chúng tôi so sánh Sheared-Pythia-160M với việc tiếp tục tiền huấn luyện mô hình 
Pythia-160M với cùng lượng token. Từ Hình 9, chúng ta có thể thấy việc tiếp tục tiền huấn luyện 
Pythia-160M bắt đầu hoạt động tốt hơn, tuy nhiên, Sheared-Pythia-160M học nhanh hơn và cuối 
cùng vượt qua hiệu suất của việc tiếp tục tiền huấn luyện trên Pythia-160M. Đây là một số kết 
quả sơ bộ mà chúng tôi thấy trong thiết lập cụ thể này.

21

--- TRANG 21 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
10 20 30
#Token để Huấn luyện (B)42.543.043.544.044.545.045.546.0Độ chính xác Downstream Trung bình (%)Tiếp tục tiền huấn luyện Pythia-160M
Sheared-Pythia-160M

Hình 9: Hiệu suất downstream của việc tiếp tục tiền huấn luyện Pythia-160M và Sheared-Pythia-160M. 
Sheared-Pythia-160M cuối cùng vượt trội hơn hiệu suất của việc tiếp tục tiền huấn luyện Pythia-160M.

Chúng tôi nghĩ rằng lợi ích của việc cắt tỉa một mô hình lớn hơn sẽ còn đáng kể hơn, dựa trên 
kết luận từ một công trình trước (Li et al., 2020) cho thấy cắt tỉa lớn hơn nén dẫn đến hiệu suất 
tốt hơn vì các mô hình lớn hơn dễ tối ưu hóa hơn. Tuy nhiên, chúng tôi muốn để lại phân tích 
chi tiết hơn cho công việc tương lai.

F.6 CẮT TỈA TỪ LLAMA1 VS LLAMA2
Phần này so sánh hiệu suất của việc cắt tỉa từ LLaMA1 và LLaMA2. Cả hai mô hình đều chứng minh 
hiệu suất tác vụ downstream mạnh mẽ, mặc dù việc cắt tỉa từ LLaMA2 không ngạc nhiên mang lại 
lợi thế nhất quán. Tuy nhiên, đáng chú ý rằng sự khác biệt hiệu suất giữa hai mô hình không phải 
là rất lớn.

10 20 30
#Token để Huấn luyện (B)47484950Độ chính xác Downstream Trung bình (%)Cắt tỉa từ LLaMA 1
Cắt tỉa từ LLaMA 2

Hình 10: So sánh giữa việc cắt tỉa từ 
LLaMA1 và LLaMA2 với tải động cho 1.3B.

F.7 SO SÁNH VỚI TIẾP TỤC TIỀN HUẤN LUYỆN THÊM INCITE-BASE-3B
Chúng tôi kiểm tra liệu cắt tỉa có tạo ra khởi tạo tốt hơn cho việc tiếp tục tiền huấn luyện so 
với một LLM hiện có có kích thước tương đương bằng cách so sánh hiệu suất của một mô hình 
INCITE-Base-3B được tiếp tục tiền huấn luyện và Sheared-LLaMA-2.7B. Chúng tôi trình bày các 
đường cong mất mát trong Hình 11 và hiệu suất downstream trong Hình 12. Mô hình INCITE-Base-3B 
bắt đầu với độ chính xác tác vụ cao hơn nhưng ổn định sau huấn luyện, trong khi Sheared-LLaMA 
cải thiện nhanh chóng và vượt qua mô hình INCITE-Base-3B, gợi ý rằng các mô hình được cắt tỉa 
từ một mô hình cơ sở mạnh phục vụ như một khởi tạo tốt hơn.¹¹

¹¹Trong các trường hợp mà mô hình nhỏ hiện có cạnh tranh so với mô hình nguồn cắt tỉa, mô hình nhỏ có 
thể cung cấp điểm khởi đầu tốt hơn so với mô hình được cắt tỉa. Trực quan, càng lớn sự chênh lệch hiệu 
suất giữa mô hình nguồn và mô hình nhỏ, mô hình được cắt tỉa càng có nhiều lợi thế.

22

--- TRANG 22 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
10 20 30
#Token để Huấn luyện (B)1.71.81.92.02.12.2Mất mátTiếp tục tiền huấn luyện INCITE
Tiếp tục tiền huấn luyện mô hình được cắt tỉa của chúng tôi

Hình 11: Mất mát của việc tiếp tục tiền huấn 
luyện INCITE-3B và mô hình LLaMA được cắt 
tỉa của chúng tôi. Cả hai mô hình đều có 
khoảng 2.7B tham số.

10 20 30
#Token để Huấn luyện (B)515253545556Độ chính xác Downstream Trung bình (%)Tiếp tục tiền huấn luyện INCITE
Tiếp tục tiền huấn luyện mô hình được cắt tỉa của chúng tôi

Hình 12: Hiệu suất downstream trung bình 
của việc tiếp tục tiền huấn luyện Sheared-LLaMA 
vs INCITE-Base-3B.

Chúng tôi đã sử dụng learning rate 1e−5 để tiếp tục tiền huấn luyện INCITE-Base-3B, cùng với 
một scheduler để warm up learning rate lên 1e−5 trong 3% đầu của các bước huấn luyện, và theo 
một lịch trình giảm cosine. Nhìn lại, cách chúng tôi tiếp tục tiền huấn luyện mô hình INCITE-Base-3B 
có thể không tối ưu theo nghiên cứu gần đây (Gupta et al., 2023).

F.8 LOẠI TRỪ CÁC MIỀN DỄ TRONG QUÁ TRÌNH CẮT TỈA
Trong quá trình phát triển dự án này, chúng tôi đã khám phá một ý tưởng dễ dàng và trực quan 
để giải quyết tỷ lệ giảm mất mát không cân bằng trong quá trình cắt tỉa và tiếp tục tiền huấn 
luyện. Cụ thể, chúng tôi loại trừ dữ liệu GitHub, StackExchange, và ArXiv trong quá trình cắt 
tỉa vì ba miền này có mất mát giảm nhanh nhất. Chúng tôi cắt tỉa LLaMA1-13B xuống còn 7B sử 
dụng một bộ dữ liệu tổng hợp của C4, CC, Wiki, và Books, với tỷ lệ được xây dựng heuristically 
là 40%, 40%, 10%, 10%, tương ứng. Sau đó chúng tôi tiếp tục tiền huấn luyện mô hình được cắt 
tỉa trên bộ dữ liệu RedPajama, bao gồm các miền bị loại trừ trong quá trình cắt tỉa.

Kết quả cho thấy sự khác biệt perplexity cân bằng hơn trên các miền khi cắt tỉa mà không sử 
dụng dữ liệu từ ba miền này. Tuy nhiên, sau tiếp tục tiền huấn luyện với tất cả dữ liệu từ bảy 
miền trong bộ dữ liệu RedPajama, sự chênh lệch mất mát tăng lên, với sự khác biệt GitHub nhỏ 
hơn nhiều so với các miền như C4. Những kết quả này chứng minh rằng đơn giản loại trừ các miền 
dễ khôi phục trong giai đoạn cắt tỉa không giải quyết được vấn đề mất cân bằng của sự khác 
biệt mất mát trên các miền một cách bản chất.

Tập hợp thí nghiệm này đã thúc đẩy chúng tôi phát triển tải batch động như một phương pháp hiệu 
quả và có nguyên tắc hơn để giải quyết các sự chênh lệch mất mát cụ thể theo miền phát sinh 
trong quá trình cắt tỉa và tiếp tục tiền huấn luyện.

Bảng 15: Cắt tỉa LLaMA1-13B với một tổng hợp 40% CC, 40% C4, 10% Books và 10% Wikipedia thành 
một mô hình 7B. Chúng tôi trình bày mất mát miền của mô hình nguồn (LLaMA1-13B), mất mát của 
mô hình được cắt tỉa và mất mát sau tiếp tục tiền huấn luyện của mô hình được cắt tỉa. Sự khác 
biệt mất mát từ mô hình mục tiêu (LLaMA1-7B) cân bằng hơn sau cắt tỉa, nhưng chênh lệch hơn 
sau tiếp tục tiền huấn luyện với tất cả các miền.

                           CC     GitHub  Book    StackExchange  Wikipedia  ArXiv   C4
LLaMA1-13B                 1.7585 0.6673  1.9499  1.4207        1.4331     1.3855  1.8619
LLaMA1-7B                  1.8366 0.7108  2.0322  1.5112        1.5291     1.4340  1.9331
Mô hình được cắt tỉa (w/o ba miền) 2.1849 1.0971 2.3726 1.9080 2.1151 1.7542 2.3187
khác biệt từ LLaMA1-7B     0.3483 0.3863  0.3404  0.3968        0.5860     0.3202  0.3857
Tiếp tục Tiền huấn luyện (w RP) 1.8344 0.6325 2.0984 1.4542 1.4549 1.4460 2.0395
khác biệt từ LLaMA1-7B     -0.0022 -0.0783 0.0661 -0.0570 -0.0743 0.0120 0.1064

23

--- TRANG 23 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
F.9 PHÂN TÍCH TỐC ĐỘ SUY LUẬN
Trong phần này, chúng tôi phân tích tốc độ suy luận của các phương pháp cắt tỉa khác nhau, bao 
gồm các mô hình sau:
• Mô hình nguồn, tức là LLaMA2-7B.
• Sheared-LLaMA-1.3B và Sheared-LLaMA-2.7B.
• Cắt tỉa Wanda (Sun et al., 2023) để cắt tỉa LLM thành một mẫu thưa thớt bán cấu trúc 2:4 
và 4:8 trong một lần.
• LLM-Pruner (Ma et al., 2023), tạo ra một mô hình có cùng số lượng tham số không phải 
embedding như Sheared-LLaMA.

Chúng tôi sử dụng một GPU A100 để kiểm tra tốc độ sinh ra (token/giây) của tất cả các mô hình 
được cắt tỉa này. Chúng tôi sinh ra lên đến 2048 token với kích thước batch là 1. Chúng tôi trình 
bày kết quả trong Bảng 16. Tốc độ của Sheared-LLaMA tốt hơn so với LLM-Pruner, phần lớn do 
kiến trúc kết quả được tối ưu hóa hơn. Như được hiển thị trong Bảng 11, LLM-pruner tạo ra một 
cấu trúc mô hình với kích thước trung gian nhỏ hơn kích thước ẩn, điều này trái ngược với thiết 
kế transformer nơi kích thước trung gian ít nhất gấp 3-4 lần kích thước ẩn.

Cắt tỉa bán cấu trúc kiểu Wanda cũng đạt được tăng tốc suy luận so với mô hình nguồn. Tuy nhiên, 
nó không nhanh bằng các mô hình dense nhỏ và ít linh hoạt hơn vì tăng tốc suy luận chỉ khả 
thi khi độ thưa thớt ở mức 50%.

Bảng 16: Tốc độ suy luận (token/s) của các phương pháp cắt tỉa khác nhau.
Mô hình               Throughput
7B
LLaMA-7B              37
1.3B  2.7B
LLM Pruner            41    40
Sheared-LLaMA         62    47
50% độ thưa thớt
Wanda (2:4)           -     42
Wanda (4:8)           -     42

G CÂU HỎI THƯỜNG GẶP
Trong phần này, chúng tôi cung cấp câu trả lời cho các câu hỏi thường gặp về công trình của chúng tôi.

▷ Liệu có công bằng khi nói rằng các mô hình Sheared-LLaMA có thể được tạo ra chỉ với 50B 
token, mặc dù mô hình nguồn (LLaMA2) được huấn luyện trên 2T token?

Tại thời điểm nộp bài báo của chúng tôi, không có mô hình nào được huấn luyện đủ cho 2T token 
ở quy mô 1.3B và 2.7B để cho phép so sánh công bằng. Tuy nhiên, các mô hình TinyLlama-1.1B 
được phát hành gần đây, được huấn luyện trên 3T token, cung cấp một điểm tham chiếu phù hợp. 
Chúng tôi quan sát rằng hiệu suất của TinyLlama-1.1B tương đương với Sheared-LLaMA-1.3B trên 
các benchmark downstream khi được sử dụng như các mô hình cơ sở, và một quan sát tương tự có 
thể được tìm thấy trong Wang et al. (2023b). Xem xét rằng TinyLlama-1.1B được huấn luyện với 
3T token, vượt quá tổng lượng tiền huấn luyện và cắt tỉa được sử dụng bởi Sheared-LLaMA-1.3B 
(2T cho tiền huấn luyện mô hình nguồn, và 50.4B cho cắt tỉa và huấn luyện tiếp tục), chúng tôi 
coi đây là bằng chứng mạnh mẽ gợi ý rằng cắt tỉa có thể là một phương pháp hiệu quả và hiệu 
quả hơn bản chất để huấn luyện các LM quy mô vừa.

▷ Tải batch động khác với Doremi (Xie et al., 2023) như thế nào?

Tải batch động và Doremi chia sẻ cùng nguyên tắc, điều chỉnh phân phối dữ liệu của mỗi miền 
dựa trên mất mát của mô hình sử dụng thuật toán tăng theo hàm mũ. Tuy nhiên, tải batch động 
cung cấp một phương pháp linh hoạt và ít phức tạp hơn có thể được áp dụng cho các kịch bản 
khác nhau.

Doremi theo một quá trình đa bước: (1) Huấn luyện một mô hình tham chiếu. (2) Huấn luyện một 
mô hình proxy để ước tính tỷ lệ dữ liệu từ mỗi miền bằng cách điều chỉnh tỷ lệ dựa trên mất 
mát của mô hình proxy. (3) Huấn luyện mô hình cuối cùng sử dụng phân phối dữ liệu ước tính. 
Ngược lại, tải batch động có thể được áp dụng trực tiếp cho bất kỳ mô hình nào mà không cần 
mô hình tham chiếu hoặc proxy. Tải batch động bắt đầu bằng cách suy ra mất mát tham chiếu dựa 
trên một tập đánh giá cố định. Mất mát tham chiếu này có thể được ước tính sử dụng luật scaling 
hoặc đơn giản bằng cách sử dụng mất mát đánh giá của mô hình nguồn. Trong quá trình huấn 
luyện, tỷ lệ dữ liệu được điều chỉnh theo thời gian thực dựa trên mất mát đánh giá được đo 
định kỳ. Quá trình tải batch động có thể được tích hợp một cách liền mạch vào pipeline tiền huấn 
luyện tiêu chuẩn, vì việc đánh giá mất mát có hiệu quả tính toán và không gây ra chi phí đáng 
kể. Mặc dù tải batch động dựa trên một tập đánh giá cố định, có thể không đại diện đầy đủ cho 
hiệu suất của mô hình trên toàn bộ bộ dữ liệu, vấn đề này có thể được giảm thiểu bằng cách 
cập nhật định kỳ tập đánh giá trong quá trình huấn luyện.

▷ Khi có nhiều kích thước mô hình nguồn, làm thế nào bạn chọn kích thước mô hình nguồn để 
cắt tỉa?

Xác định kích thước mô hình nguồn tối ưu để cắt tỉa là thách thức. Tuy nhiên, chúng ta có thể 
thực hiện một thí nghiệm tư duy bằng cách xem xét mỗi tham số như một "đơn vị thông tin" đồng 
nhất. Ví dụ, nếu một mô hình nguồn với 7B tham số được huấn luyện sử dụng 2T token, chúng ta 
có thể giả định rằng mỗi tham số mang khoảng 285 token thông tin, giả sử phân phối thông tin đồng 
nhất trên các tham số. Khi cắt tỉa ngẫu nhiên mô hình này xuống còn 1.3B tham số, tổng lượng 
thông tin được giảm xuống 1.3B × 285 = 0.37T token. Ngược lại, nếu chúng ta cắt tỉa một mô 
hình 13B (cũng được huấn luyện với 2T token) xuống còn 1.3B tham số, tổng lượng thông tin được 
giảm xuống 1.3B × (2T / 13B) = 0.2T token. Mặc dù ước tính này thô, nó gợi ý rằng cắt tỉa từ 
một mô hình lớn hơn có thể kém hiệu quả, đặc biệt khi các mô hình nguồn được huấn luyện với 
cùng số lượng token. Quan trọng là lưu ý rằng đây là một ước tính đơn giản hóa, và giả định 
phân phối thông tin đồng nhất trên các tham số có thể không đúng trong thực tế. Hơn nữa, quá 
trình cắt tỉa có cấu trúc tự nó rõ ràng phá vỡ giả định này. Tuy nhiên, thí nghiệm tư duy này 
cung cấp cảm nhận chung về cách kích thước mô hình nguồn có thể tác động đến hiệu quả của 
cắt tỉa.

25
