# 2007.15353.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2007.15353.pdf
# File size: 1205658 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published as a conference paper at ICLR 2021
GROWING EFFICIENT DEEPNETWORKS
BYSTRUCTURED CONTINUOUS SPARSIFICATION
Xin Yuan
University of Chicago
yuanx@uchicago.eduPedro Savarese
TTI-Chicago
savarese@ttic.eduMichael Maire
University of Chicago
mmaire@uchicago.edu
ABSTRACT
We develop an approach to growing deep network architectures over the course of
training, driven by a principled combination of accuracy and sparsity objectives.
Unlike existing pruning or architecture search techniques that operate on full-sized
models or supernet architectures, our method can start from a small, simple seed
architecture and dynamically grow and prune both layers and filters. By combining
a continuous relaxation of discrete network structure optimization with a scheme
for sampling sparse subnetworks, we produce compact, pruned networks, while
also drastically reducing the computational expense of training. For example, we
achieve 49.7%inference FLOPs and 47.4%training FLOPs savings compared to a
baseline ResNet-50 on ImageNet, while maintaining 75.2%top-1 accuracy â€” all
without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet,
PASCAL VOC, and Penn Treebank, with convolutional networks for image classi-
fication and semantic segmentation, and recurrent networks for language modeling,
demonstrate that we both train faster and produce more efficient networks than
competing architecture pruning or search methods.
1 I NTRODUCTION
Deep neural networks are the dominant approach to a variety of machine learning tasks, including im-
age classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015), object detection (Girshick,
2015; Liu et al., 2016), semantic segmentation (Long et al., 2015; Chen et al., 2017) and language
modeling (Zaremba et al., 2014; Vaswani et al., 2017; Devlin et al., 2019). Modern neural networks
are overparameterized and training larger networks usually yields improved generalization accuracy.
Recent work (He et al., 2016; Zagoruyko & Komodakis, 2016; Huang et al., 2017) illustrates this
trend through increasing depth andwidth of convolutional neural networks (CNNs). Yet, training is
compute-intensive, and real-world deployments are often limited by parameter and compute budgets.
Neural architecture search (NAS) (Zoph & Le, 2017; Liu et al., 2019; Luo et al., 2018; Pham et al.,
2018; Savarese & Maire, 2019) and model pruning (Han et al., 2016; 2015; Guo et al., 2016) methods
aim to reduce these burdens. NAS addresses an issue that further compounds training cost: the
enormous space of possible network architectures. While hand-tuning architectural details, such
as the connection structure of convolutional layers, can improve performance (Iandola et al., 2016;
Sifre & Mallat, 2014; Chollet, 2017; Howard et al., 2017; Zhang et al., 2018; Huang et al., 2018), a
principled way of deriving such designs remains elusive. NAS methods aim to automate exploration
of possible architectures, producing an efficient design for a target task under practical resource
constraints. However, during training, most NAS methods operate on a large supernet architecture,
which encompasses candidate components beyond those that are eventually selected for inclusion
in the resulting network (Zoph & Le, 2017; Liu et al., 2019; Luo et al., 2018; Pham et al., 2018;
Savarese & Maire, 2019). Consequently, NAS-based training may typically be more thorough, but
more computationally expensive, than training a single hand-designed architecture.
Model pruning techniques similarly focus on improving the resource efficiency of neural networks
during inference, at the possible expense of increased training cost. Common strategies aim to
generate a lighter version of a given network architecture by removing individual weights (Han et al.,
2015; 2016; Molchanov et al., 2017) or structured parameter sets (Li et al., 2017; He et al., 2018; Luo
et al., 2017). However, the majority of these methods train a full-sized model prior to pruning and,
1arXiv:2007.15353v2  [cs.LG]  6 Jun 2023

--- PAGE 2 ---
Published as a conference paper at ICLR 2021
Level 1: Channel-wiseTrainingEpochs T=0,1,2â€¦
Level 2: Layer-wiseâ€¦
(a) Growing CNN Layers and Filters
Training EpochsFLOPsOur Method
Pruning
Architecture Search (b) Epoch-wise Training Cost
NAS Pruning OursFLOPs (c) Total Training Cost
Figure 1: Growing Networks during Training. We define an architecture configuration space and simultane-
ously adapt network structure and weights. (a)Applying our approach to CNNs, we maintain auxiliary variables
that determine how to grow and prune both filters ( i.e.channel-wise) and layers, subject to practical resource
constraints. (b)By starting with a small network and growing its size, we utilize fewer resources in early training
epochs, compared to pruning or NAS methods. (c)Consequently, our method significantly reduces the total
computational cost of training, while delivering trained networks of comparable or better size and accuracy.
after pruning, utilize additional fine-tuning phases in order to maintain accuracy. Hubara et al. (2016)
and Rastegari et al. (2016) propose the use of binary weights and activations, allowing inference to
benefit from reduced storage costs and efficient computation through bit-counting operations. Yet,
training still involves tracking high-precision weights alongside lower-precision approximations.
We take a unified view of pruning and architecture search, regarding both as acting on a configuration
space, and propose a method to dynamically grow deep networks by continuously reconfiguring
their architecture during training. Our approach not only produces models with efficient inference
characteristics, but also reduces the computational cost of training; see Figure 1. Rather than starting
with a full-sized network or a supernet, we start from simple seed networks and progressively adjust
(grown and prune) them. Specifically, we parameterize an architectural configuration space with
indicator variables governing addition or removal of structural components. Figure 2(a) shows an
example, in the form of a two-level configuration space for CNN layers and filters. We enable learning
of indicator values (and thereby, architectural structure) via combining a continuous relaxation with
binary sampling, as illustrated in Figure 2(b). A per-component temperature parameter ensures that
long-lived structures are eventually baked into the networkâ€™s discrete architectural configuration.
While the recently proposed AutoGrow (Wen et al., 2020) also seeks to grow networks over the
course of training, our technical approach differs substantially and leads to significant practical
advantages. At a technical level, AutoGrow implements an architecture search procedure over a
predefined modular structure, subject to hand-crafted, accuracy-driven growing and stopping policies.
In contrast, we parameterize architectural configurations and utilize stochastic gradient descent to
learn the auxiliary variables that specify structural components, while simultaneously training the
weights within those components. Our unique technical approach yields the following advantages:
â€¢Fast Training by Growing: Training is a unified procedure, from which one can request a network
structure and associated weights at any time. Unlike AutoGrow and the majority of pruning
techniques, fine-tuning to optimize weights in a discovered architecture is optional. We achieve
excellent results even without any fine-tuning stage.
â€¢Principled Approach via Learning by Continuation + Sampling: We formulate our approach in
the spirit of learning by continuation methods, which relax a discrete optimization problem to an
increasingly stiff continuous approximation. Critically, we introduce an additional sampling step
to this strategy. From this combination, we gain the flexibility of exploring a supernet architecture,
but the computational efficiency of only actually training a much smaller active subnetwork.
â€¢Budget-Aware Optimization Objectives: The parameters governing our architectural configu-
ration are themselves updated via gradient decent. We have flexibility to formulate a variety of
resource-sensitive losses, such as counting total FLOPs, in terms of these parameters.
â€¢Broad Applicability: Though we use progressive growth of CNNs in width and depth as a
motivating example, our technique applies to virtually any neural architecture. One has flexibility
in how to parameterize the architecture configuration space. We also show results with LSTMs.
We demonstrate these advantages while comparing to recent NAS and pruning methods through
extensive experiments on classification, semantic segmentation, and word-level language modeling.
2

--- PAGE 3 ---
Published as a conference paper at ICLR 2021
ð’™ð’Šð’ð’™ð’ð’–ð’•ActiveFilters = 11000â€¦ð’™ð’Šð’ð’™ð’ð’–ð’•1100â€¦
ð’™ð’Šð’ð’™ð’ð’–ð’•ð’™ð’Šð’ð’™ð’ð’–ð’•1010â€¦GrowDetachGrowChannel IndicatorChannel IndicatorChannel IndicatorT=0T=1Growing Epochs T=2â€¦Level 1: Channel-wise ConfigurationSpace
Level 2: Layer-wise ConfigurationSpaceActiveFilters = 2ActiveFilters = 2
1000â€¦Layer IndicatorActiveLayers = 1ð’™ð’Šð’ð’™ð’ð’–ð’•1100â€¦Layer IndicatorActiveLayers = 2ð’™ð’ð’–ð’•0111â€¦ActiveLayers = 3Layer Indicatorð’™ð’Šð’GrowGrowDetachGrow
(a) Architectural Configuration Space for CNNs
Trainable MaskVariableð’”DiscreteConfigurationSpaceProgressiveContinuousRelaxationBinary Indicator
11000ð’’âˆ¼ð‘©ð’†ð’“ð’(ðˆ(ðœ·ð’”))ContinuousmaskðœŽ(ð›½ð‘ )Sampling000ðˆðœ·ð’”âŠ™ð’’ (b) Optimization with Structured Continuation
Figure 2: Technical Framework. (a) We periodically restructure a CNN by querying binary indicators that
define a two-level configuration space for filters and layers. (b)To make optimization feasible while growing
networks, we derive these binary indicators from trainable continuous mask variables. We employ a structured
extension of continuous sparsification (Savarese et al., 2020), combined with sampling. Binary stochastic
auxiliary variables q, sampled according to Ïƒ(Î²s), generate the discrete components active at a particular time.
2 R ELATED WORK
Network Pruning. Pruning methods can be split into two groups: those pruning individual weights
and those pruning structured components. Individual weight-based pruning methods vary on the
removal criteria. For example, Han et al. (2015) propose to prune network weights with small
magnitude, and subsequently quantize those remaining (Han et al., 2016). Louizos et al. (2018) learn
sparse networks by approximating â„“0-regularization with a stochastic reparameterization. However,
sparse weights alone often only lead to speedups on dedicated hardware with supporting libraries.
In structured methods, pruning is applied at the level of neurons, channels, or even layers. For
example, L1-pruning (Li et al., 2017) removes channels based on the norm of their filters. He et al.
(2018) use group sparsity to smooth the pruning process after training. MorphNet (Gordon et al.,
2018) regularizes weights towards zero until they are small enough such that the corresponding output
channels are marked for removal from the network. Intrinsic Structured Sparsity (ISS) (Wen et al.,
2018) works on LSTMs (Hochreiter & Schmidhuber, 1997) by collectively removing the columns
and rows of the weight matrices via group LASSO. Although structured pruning methods and our
algorithm share the same spirit of generating efficient models, we gain training cost savings by
growing networks from small initial architectures instead of pruning full-sized ones.
Neural Architecture Search. NAS methods have greatly improved the performance achieved by
small network models. Pioneering NAS approaches use reinforcement learning (Zoph et al., 2018;
Zoph & Le, 2017) and genetic algorithms (Real et al., 2019; Xie & Yuille, 2017) to search for
transferable network blocks whose performance surpasses many manually designed ones. However,
such approaches require massive computation during the search â€” typically thousands of GPU days.
To reduce computational cost, recent efforts utilize more efficient search techniques, such as direct
gradient-based optimization (Liu et al., 2019; Luo et al., 2018; Pham et al., 2018; Tan et al., 2019; Cai
et al., 2019; Wortsman et al., 2019). Nevertheless, most NAS methods perform search in a supernet
space which requires more computation than training typically-sized architectures.
Network Growing. Network Morphism (Wei et al., 2016) searches for efficient deep networks by
extending layers while preserving the parameters. Recently proposed Autogrow (Wen et al., 2020)
takes an AutoML approach to growing layers. These methods either require a specially-crafted policy
to stop growth ( e.g., after a fixed number of layers) or rely on evaluating accuracy during training,
incurring significant additional computational cost.
Learning by Continuation. Continuation methods are commonly used to approximate intractable
optimization problems by gradually increasing the difficulty of the underlying objective, for example
by adopting gradual relaxations to binary problems. Wu et al. (2019); Xie et al. (2019b; 2020)
use gumbel-softmax (Jang et al., 2017) to back-propagate errors during architecture search and
spatial feature sparsification. Savarese et al. (2020) propose continuous sparsification to speed up
pruning and ticket search (Frankle & Carbin, 2019). Despite the success of continuation methods in
producing sparse networks upon the completion of training, they do not operate on sparse networks
3

--- PAGE 4 ---
Published as a conference paper at ICLR 2021
during training and instead work with a real-valued relaxation. Postponing actual elimination of near
zeroed-out components prevents naive application of these methods from reducing training costs.
3 M ETHOD
3.1 A RCHITECTURAL CONFIGURATION SPACE
A network topology can be seen as a directed acyclic graph consisting of an ordered sequence of nodes.
Each node x(i)
inis an input feature and each edge is a computation cell with structured hyperparameters
(e.g., filter and layer numbers in convolutional networks). An architectural configuration space can be
parameterized by associating a mask variable mâˆˆ {0,1}with each computation cell (edge), which
enables training-time pruning ( m= 1â†’0) and growing ( m= 0â†’1) dynamics.
As a running example, we consider a two-level configuration space for CNN architectures, depicted
in Figure 2(a), that enables dynamically growing networks in both width (channel-wise) and depth
(layer-wise). Alternative configuration spaces are possible; we defer to the Appendix details on how
we parameterize the design of LSTM architectures.
CNN Channel Configuration Space: For a convolutional layer with lininput channels, loutoutput
channels (filters) and kÃ—ksized kernels, the i-th output feature is computed based on the i-th filter,
i.e.foriâˆˆ {1, . . . , l out}:
x(i)
out=f(xin,F(i)Â·m(i)
c), (1)
where m(i)
câˆˆ {0,1}is a binary parameter that removes the i-th output channel when set to zero and
fdenotes the convolutional operation. m(i)
cis shared across a filter and broadcasts to the same shape
as the filter tensor F(i), enabling growing/pruning of the entire filter. As Figure 2(a) (top) shows,
we start from a slim channel configuration. We then query the indicator variables and perform state
transitions : (1) When flipping an indicator variable from 0to1for the first time, we grow a randomly
initialized filter and concatenate it to the network. (2) If an indicator flips from 1to0, we temporarily
detach the corresponding filter from the computational graph; it will be grown back to the its original
position if its indicator flips back to 1, or otherwise be permanently pruned at the end of training. (3)
For other cases, the corresponding filters either survive and continue training or remain detached
pending the next query to their indicators. Our method automates architecture evolution, provided we
can train the indicators.
CNN Layer Configuration Space: To grow network depth, we design a layer configuration space in
which an initial shallow network will progressively expand into a deep trained model, as shown in
Figure 2(a) (bottom). Similar to channel configuration space, where filters serve as basic structural
units, we require a unified formulation to support the growing of popular networks with shortcut
connections ( e.g., ResNets) and without ( e.g., VGG-like plain nets). We first introduce an abstract
layer class flayer as a basic structural unit, which operates on input features xinand generates output
features xout.flayer can be instantiated as convolutional layers for plain nets or residual blocks for
ResNets, respectively. We define the layer configuration space as:
xout=g(xin;flayerÂ·m(j)
l) =(
flayer(xin),ifm(j)
l= 1
xin, ifm(j)
l= 0, (2)
where m(j)
lâˆˆ {0,1}is the binary indicator for j-th layer flayer , with which we perform state
transitions analogous to the channel configuration space. Layer indicators have priority over channel
indicators: if m(j)
lis set as 0, all filters contained in the corresponding layer will be detached,
regardless of the state their indicators. We do not detach layers that perform changes in resolution
(e.g., strided convolution).
3.2 G ROWING WITH STRUCTURED CONTINUOUS SPARSIFICATION
We can optimize a trade-off between accuracy and structured sparsity by considering the objective:
min
w,mc,l,flayerLE(g(f(x;wâŠ™mc);flayerÂ·ml)) +Î»1âˆ¥mcâˆ¥0+Î»2âˆ¥mlâˆ¥0, (3)
4

--- PAGE 5 ---
Published as a conference paper at ICLR 2021
where fis the operation in Eq. (1) or Eq. (9) (in Appendix A.6), while gis defined in Eq. (2). wâŠ™mc
andflayerÂ·mlare general expressions of structured sparsified filters and layers and LEdenotes a
loss function ( e.g., cross-entropy loss for classification). The â„“0terms encourage sparsity, while Î»1,2
are trade-off parameters between LEand the â„“0penalties.
Algorithm 1 : Optimization
Input: DataX=(xi)n
i=1, labels Y=(yi)n
i=1
Output: Grown efficient model G
Initialize: G,w,u,Î»base
1andÎ»base
2.
Settsas all 0 vectors associating Ïƒfunctions.
forepoch = 1toTdo
Evaluate Gâ€™s sparsity uGand calculate
âˆ†u=uâˆ’uG
Update Î»1â†Î»base
1Â·âˆ†u;Î»2â†Î»base
2Â·âˆ†u
in Eq. (6) using Eq. (4)
forr= 1toRdo
Sample mini-batch xi, yifromX,Y
Train Gusing Eq. (6) with SGD
end for
Sample indicators qc,lâˆ¼Bern(Ïƒ(Î²sc,l))
and record the index idxwhere qvalue is 1.
Update ts[idx] =ts[idx] + 1
Update Î²using Eq. (7)
end for
return GBudget-aware Growing. In practice, utilizing
Eq. (3) might require a grid search on Î»1andÎ»2
until a network with desired sparsity is produced.
To avoid such a costly procedure, we propose
a budget-aware growing process, guided by a
target budget in terms of model parameters or
FLOPs. Instead of treating Î»1andÎ»2as con-
stants, we periodically update them as:
Î»1â†Î»base
1Â·âˆ†u, Î» 2â†Î»base
2Â·âˆ†u , (4)
where âˆ†uis calculated as the target sparsity u
minus current network sparsity uG, and Î»base
1,
Î»base
2are initial base constants. In early growing
stages, since the network is too sparse and âˆ†uis
negative, the optimizer will drive the network to-
wards a state with more capacity (wider/deeper).
The regularization effect gradually weakens as
the networkâ€™s sparsity approaches the budget
(andâˆ†uapproaches zero). This allows us to
adaptively grow the network and automatically
adjust its sparsity level while simultaneously
training model weights. Appendix A.1 provides more detailed analysis. Our experiments default to
defining budget by parameter count, but also investigate alternative notions of budget.
Learning by Continuation. Another issue in optimizing Eq. (3) is that âˆ¥mcâˆ¥0andâˆ¥mlâˆ¥0make
the problem computationally intractable due to the combinatorial nature of binary states. To make
the configuration space continuous and the optimization feasible, we borrow the concept of learning
by continuation (Cao et al., 2017; Wu et al., 2019; Savarese et al., 2020; Xie et al., 2020). We
reparameterize mas the binary sign of a continuous variable s:sign(s)is 1 if s >0and 0 if s <0.
We rewrite the objective in Eq. (3) as:
min
w,sc,lÌ¸=0,flayerLE
g 
f(x;wâŠ™sign(sc));flayerÂ·sign(sl)
+Î»1âˆ¥sign(sc)âˆ¥1+Î»2âˆ¥sign(sl)âˆ¥1.(5)
We attack the hard and discontinuous optimization problem in Eq. (5) by starting with an easier
objective which becomes harder as training proceeds. We use a sequence of functions whose
limit is the sign operation: for any sÌ¸= 0,limÎ²â†’âˆžÏƒ(Î²s) = sign( s)ifÏƒis sigmoid function or
limÎ²â†’0Ïƒ(Î²s) = sign( s)ifÏƒis gumbel-softmaxexp((âˆ’log(s0)+g1(s))/Î²)P
jâˆˆ{0,1}exp((âˆ’log(sj)+gj(s))/Î²)(Jang et al., 2017),
where Î² > 0is a temperature parameter and g0,1is gumbel. By periodically changing Î²,Ïƒ(Î²s)
becomes harder to optimize, while the objectives converges to original discrete one.
Maintaining Any-time Sparsification. Although continuation methods can make the optimization
feasible, they only conduct sparsification via a thresholding criterion in the inference phase. In this
case, the train-time architecture is dense and not appropriate in the context of growing a network. To
effectively reduce computational cost of training, we maintain a sparse architecture by introducing an
0-1 sampled auxiliary variable qbased on the probability value Ïƒ(Î²s). Our final objective becomes:
min
w,sc,lÌ¸=0,flayerLE
g 
f(x;wâŠ™Ïƒ(Î²sc)âŠ™qc);flayerÂ·Ïƒ(Î²sl)Â·ql
+Î»1âˆ¥Ïƒ(Î²sc)âˆ¥1+Î»2âˆ¥Ïƒ(Î²sl)âˆ¥1,(6)
where qcandqlare random variables sampled from Bern(Ïƒ(Î²sc))andBern(Ïƒ(Î²sl)), which effec-
tively maintains any-time sparsification and avoids sub-optimal thresholding, as shown in Figure 2(b).
Improved Temperature Scheduler. In existing continuation methods, the initial Î²value is usually
set as Î²0= 1and a scheduler is used at the end of each training epoch to update Î²in all activation
functions Ïƒ, typically following Î²=Î²0Â·Î³t, where tis current epoch and Î³is a hyperparameter ( >1
when Ïƒis the sigmoid function, <1when Ïƒis gumbel softmax). Both Î³andtcontrol the speed
5

--- PAGE 6 ---
Published as a conference paper at ICLR 2021
Table 1: Comparison with the channel pruning methods L1-Pruning (Li et al., 2017), SoftNet (He et al., 2018),
ThiNet (Luo et al., 2017), Provable (Liebenwein et al., 2020) and BAR (Lemaire et al., 2019) on CIFAR-10.
Model Method Val Acc(%) Params(M) FLOPs(%) Train-Cost Savings( Ã—)
Original 92.9 Â±0.16 (-0.0) 14.99 (100%) 100 1.0 Ã—
L1-Pruning 91.8 Â±0.12 (-1.1) 2.98 (19.9%) 19.9 2.5 Ã—
VGG SoftNet 92.1 Â±0.09 (-0.8) 5.40 (36.0%) 36.1 1.6 Ã—
-16 ThiNet 90.8 Â±0.11 (-2.1) 5.40 (36.0%) 36.1 1.6 Ã—
Provable 92.4 Â±0.12 (-0.5) 0.85 (5.7%) 15.0 3.5Ã—
Ours 92.50Â±0.10 (-0.4) 0.754 Â±0.005 (5.0%) 13.55 Â±0.03 4.95 Â±0.17Ã—
Original 91.3 Â±0.12 (-0.0) 0.27 (100%) 100 1.0 Ã—
L1-Pruning 90.9 Â±0.10 (-0.4) 0.15 (55.6%) 55.4 1.1 Ã—
ResNet SoftNet 90.8 Â±0.13 (-0.5) 0.14 (53.6%) 50.6 1.2Ã—
-20 ThiNet 89.2 Â±0.18 (-2.1) 0.18 (67.1%) 67.3 1.1 Ã—
Provable 90.8 Â±0.08 (-0.5) 0.10 (37.3%) 54.5 1.7 Ã—
Ours 90.91Â±0.07 (-0.4) 0.096 Â±0.002 (35.8%) 50.20 Â±0.01 2.40 Â±0.09Ã—
WRNOriginal 96.2 Â±0.10 (-0.0) 36.5 (100%) 100 1.0 Ã—
-28L1-Pruning 95.2 Â±0.10 (-1.0) 7.6 (20.8%) 49.5 1.5 Ã—
-10BAR(16x V) 92.0 Â±0.08 (-4.2) 2.3 (6.3%) 1.5 2.6Ã—
Ours 95.32Â±0.11 (-0.9) 3.443Â±0.010 (9.3%) 28.25Â±0.04 3.12Â±0.11Ã—
at which the temperature increases during training. Continuation methods with global temperature
schedulers have been successfully applied in pruning and NAS. However, in our case, a global
schedule leads to unbalanced dynamics between variables with low and high sampling probabilities:
increasing the temperature of those less sampled at early stages may hinder their training altogether,
as towards the end of training the optimization difficulty is higher. To overcome this issue, we propose
a separate, structure-wise temperature scheduler by making a simple modification: for each mask
variable, instead of using the current epoch number tto compute its temperature, we set a separate
counter tswhich is increased only when its associated indicator variable is sampled as 1 in Eq. (6).
We define our structure-wise temperature scheduler as
Î²=Î²0Â·Î³ts, (7)
where tsare vectors associated with the Ïƒfunctions. Experiments use this separate scheduler by
default, but also compare the two alternatives. Algorithm 1 summarizes our optimization procedure.
4 E XPERIMENTS
We evaluate our method against existing channel pruning, network growing, and neural architecture
search (NAS) methods on: CIFAR-10 (Krizhevsky et al., 2014) and ImageNet (Deng et al., 2009) for
image classification, PASCAL (Everingham et al., 2015) for semantic segmentation and the Penn
Treebank (PTB) (Marcus et al., 1993) for language modeling. See dataset details in Appendix A.2. In
tables, best results are highlighted in bold and second best are underlined.
4.1 C OMPARING WITH CHANNEL PRUNING METHODS
Implementation Details. For fair comparison, we only grow filters while keeping other structured
parameters of the network (number of layers/blocks) the same as unpruned baseline models. Our
method involves two kinds of trainable variables: model weights and mask weights. For model
weights, we adopt the same hyperparameters used to train the corresponding unpruned baseline
models, except for setting the dropout keep probability for language modeling to 0.65. We initialize
mask weights such that a single filter is activated in each layer. We train with SGD, an initial learning
rate of 0.1, weight decay of 10âˆ’6and momentum 0.9. Trade-off parameter Î»base
1is set to 0.5 on all
tasks; Î»2is not used since we do not perform layer growing here. We set Ïƒas the sigmoid function
andÎ³as1001
Twhere Tis the total number of epochs.
VGG-16, ResNet-20, and WideResNet-28-10 on CIFAR-10. Table 1 summarizes the models
produced by our method and competing channel pruning approaches. Note that training cost is
calculated based on overall FLOPs during pruning and growing stages. Our method produces sparser
networks with less accuracy degradation, and, consistently saves more computation during training â€”
a consequence of growing from a simple network. For a aggressively pruned WideResNet-28-10,
we observe that BAR (Lemaire et al., 2019) might not have enough capacity to achieve negligible
accuracy drop, even with knowledge distillation (Hinton et al., 2015) during training. Note that we
6

--- PAGE 7 ---
Published as a conference paper at ICLR 2021
Table 2: Comparison with channel pruning methods: L1-Pruning (Li et al., 2017), SoftNet (He et al., 2018) and
Provable (Liebenwein et al., 2020) on ImageNet.
Model Method Top-1 Acc(%) Params(M) FLOPs(%) Train-Cost Savings( Ã—)
Original 76.1 (-0.0) 23.0 (100%) 100 1.0( Ã—)
ResNetL1-Pruning 74.7 (-1.4) 19.6 (85.2%) 77.5 1.1( Ã—)
-50SoftNet 74.6 (-1.5) N/A 58.2 1.2(Ã—)
Provable 75.2 (-0.9) 15.2 (65.9%) 70.0 1.2 (Ã—)
Ours 75.2 (-0.9) 14.1 (61.2%) 50.3 1.9 (Ã—)
Table 3: Results comparing with AutoGrow (Wen et al., 2020) on CIFAR-10 and ImageNet.
Dataset Methods Variants Found Net Val Acc(%) Depth Sparse Channel
OursBasic3ResNet 23-29-31 94.50 83 âœ“
CIFAR-10Plain3Net 11-14-19 90.99 44 âœ“
AutoGrowBasic3ResNet 42-42-42 94.27 126 âœ—
Plain3Net 23-22-22 90.82 67 âœ—
OursBottleneck4ResNet 5-6-5-7 77.41 23 âœ“
ImageNetPlain4Net 3-4-4-5 70.79 16 âœ“
AutoGrowBottleneck4ResNet 6-7-3-9 77.33 25 âœ—
Plain4Net 5-5-5-4 70.54 19 âœ—
report our methodâ€™s performance as mean Â±standard deviation, computed over 5 runs with different
random seeds. The small observed variance shows that our method performs consistently across runs.
ResNet-50 and MobileNetV1 on ImageNet. To validate effectiveness on large-scale datasets, we
grow, from scratch, filters of the widely used ResNet-50 on ImageNet; we do not fine-tune. Table 2
shows our results best those directly reported in papers of respective competing methods. Our
approach achieves 49.7%inference and 47.4%training cost savings in terms of FLOPs while main-
taining 75.2%top-1 accuracy, without any fine-tuning stage. Appendix A.4 shows our improvements
on the challenging task of growing channels of an already compact MobileNetV1. In addition,
Figure 3 shows the top-1 accuracy/FlOPs trade-offs for MobileNetV1 on ImageNet, demonstrating
that our method dominates competing approaches.
Deeplab-v3-ResNet-101 on PASCAL VOC. Appendix A.5 provides semantic segmentation results.
2-Stacked-LSTMs on PTB: We detail the extensions to recurrent cells and compare our proposed
method with ISS based on vanilla two-layer stacked LSTM in Appendix A.6. As shown in Table 8,
our method finds more compact model structure with lower training cost, while achieving similar
perplexity on both validation and test sets.
4.2 C OMPARING WITH AUTOGROW
Implementation Details. We grow both filters and layers. We follow AutoGrowâ€™s settings in
exploring architectural variations that define our initial seed network, layer-wise configuration
space and basic structural units flayer :Basic3ResNet ,Bottleneck4ResNet ,Plain3Net ,Plain4Net .
Different from the initialization of AutoGrow using full-sized filters in each layer, our channel-wise
configuration space starts from single filter and expands simultaneously with layers. Appendix A.7
contains a detailed comparison of seed architectures. For training model weights, we adopt the
hyperparameters of the ResNet or VGG models corresponding to initial seed variants. For layer-wise
and channel-wise mask variables, we initialize the weights such that only a single filter in each layer
and one basic unit in each stage ( e.g., BasicBlock in Basic3ResNet ) is active. We use SGD training
with initial learning rate of 0.1, weight decay of 10âˆ’6and momentum of 0.9 on all datasets. The
learning rate scheduler is the same as for the corresponding model weights. Trade-off parameters
Î»base
1andÎ»base
2are set as 1.0 and 0.1 on all datasets. For fair comparison, we fine-tune our final
models with 40 epochs and 20 epochs on CIFAR-10 and ImageNet, respectively.
Results on CIFAR-10 and ImageNet. Table 3 compares our results to those of AutoGrow. For
all layer-wise growing variants across both datasets, our method consistently yields a better depth
and width configuration than AutoGrow, in terms of accuracy and training/inference costs trade-off.
Regarding the training time of Bottleneck4ResNet on ImageNet, AutoGrow requires 61.6hours for
the growing phase and 78.6hours for fine-tuning on 4 TITAN V GPUs, while our method takes 48.2
and31.3hours, respectively. Our method offers 43% more train-time savings than AutoGrow. We not
only require fewer training epochs, but also grow from a single filter to a relatively sparse network,
while AutoGrow always keeps full-sized filter sets without any reallocation.
7

--- PAGE 8 ---
Published as a conference paper at ICLR 2021
4.3 C OMPARING WITH NAS M ETHODS
As a fair comparison with NAS methods involving search and re-training phases, we also divide our
method into growing and training phases. Specifically, we grow layers and channels from the Bot-
tleneck4ResNet seed architecture directly on ImageNet by setting Î»base
1= 2.0,Î»base
2= 0.1and the
parameter budget under 7M. Then we resume training the grown architecture and compare with exist-
ing NAS methods in terms of parameters, top-1 validation accuracy and V100 GPU hours required by
the search or growing stages, as shown in Table 4. Note that DARTS (Liu et al., 2019) conducts search
on CIFAR-10, then transfers to ImageNet instead of direct search. This is because DARTS operates
on a supernet by including all the candidate paths and suffers from GPU memory explosion. In terms
of epoch-wise FLOPs, results shown in Figure 1(c) are for training an equivalent of ResNet-20 on
CIFAR-10 in comparison with DARTS and Provable channel pruning approach (Liebenwein et al.,
2020). Also note that the EfficientNet-B0 architecture, included in Table 4, is generated by grid
search in the MnasNet search space, thus having the same heavy search cost. To achieve the re-
ported performance, EfficientNet-B0 utilizes additional squeeze-and-excitation (SE) (Hu et al., 2018)
modules, AutoAugment (Cubuk et al., 2019), as well as much longer re-training epochs on ImageNet.
Table 4: Performance comparing with NAS methods AmoebaNet-
A (Real et al., 2019), MnasNet (Tan et al., 2019), EfficientNet-
B0 (Tan & Le, 2019), DARTS (Liu et al., 2019) and Proxyless-
Net (Cai et al., 2019) on ImageNet.
Method Params Top-1 Search/Grow Cost
AmoebaNet-A 5.1M 74.5% 76K GPU hours
MnasNet 4.4M 74.0% 40K GPU hours
EfficientNet-B0 5.3M 77.1% (+SE) 40K GPU hours
DARTS 4.7M 73.1% N/A
ProxylessNet(GPU) 7.1M 75.1% 200 GPU hours
Ours 6.8M 74.3% 80GPU hours
Ours 6.7M 74.8% 110 GPU hours
Ours 6.9M 75.1% 140 GPU hoursProxylessNet still starts with an over-
parameterized supernet, but applies
a pruning-like search method by bi-
narizing the architecture parameters
and forcing only one path to be acti-
vated at search-time. This enables di-
rectly searching on ImageNet, achiev-
ing200Ã—more search cost savings
than MnasNet. Contrasting with
ProxylessNet, our method progres-
sively adds filters and layers to sim-
ple seed architectures while maintain-
ing sparsification, which leads to sav-
ings of not only epoch-wise compu-
tation but also memory consumption,
enabling faster, larger-batch training. As such, we further save 45% of the GPU search hours, while
achieving comparable accuracy-parameter trade-offs.
4.4 A NALYSIS
Training Cost Savings. Figure 4 illustrates our sparsification dynamics, showing epoch-wise FLOPs
while growing a ResNet-20. Appendix A.8 presents additional visualizations. Even with fully parallel
GPU hardware, starting with few filters and layers in the network will ultimately save wall-clock
time, as larger batch training (Goyal et al., 2017) can always be employed to fill the hardware.
Figure 5 shows validation accuracy, model complexity, and layer count while growing Basic3ResNet .
Complexity is measured as the model parameters ratio of AutoGrowâ€™s target model. At the end of 160
epochs, our methodâ€™s validation accuracy is 92.36% , which is higher than AutoGrowâ€™s 84.65% at
360 epochs. We thus require fewer fine-tuning epochs to achieve a final 94.50% accuracy on CIFAR.
Table 5: Comparison with random pruning baseline on CIFAR-10.
Model Method Val Acc(%) Params(M)
VGG-16Random 90.01 Â±0.69 0.770 Â±0.050
Ours 92.50Â±0.10 0.754Â±0.005
ResNet-20Random 89.18 Â±0.55 0.100 Â±0.010
Ours 90.91Â±0.07 0.096Â±0.002
WRN-28-10Random 92.26 Â±0.87 3.440 Â±0.110
Ours 95.32Â±0.11 3.443Â±0.010Budget-Aware Growing. In Fig-
ure 6, for ResNet-20 on CIFAR-10,
we compare architectures obtained
by (1) uniform pruning : a naive pre-
defined pruning method that prunes
the same percentage of channels in
each layer, (2) ours: variants of our
method by setting different model pa-
rameter sparsities as target budgets
during growing, and (3) direct design :
our grown architectures re-initialized
with random weights and re-trained.
In most budget settings, our growing method outperforms direct design and uniform pruning, demon-
8

--- PAGE 9 ---
Published as a conference paper at ICLR 2021
60 70 80 90 100 110
FLOPs (% of 0.25 MobileNetV1(128))4344454647ImageNet Top-1 Acc (%)
Top-1 Acc vs FLOPs (%))
Ours
NetAdapt
MorphNet
0.25 MobileNetV1(128)
Figure 3: Performance/FLOPs trade-offs for
pruned MobileNetV1 on ImageNet.
0 20 40 60 80 100 120 140 160
Epochs020406080100Retained FLOPs Ratio(%)Epoch-wise FLOPs of Each Conv
conv1
layer1.0.conv1
layer1.0.conv2
layer1.1.conv1
layer1.1.conv2
layer1.2.conv1
layer1.2.conv2
layer2.0.conv1
layer2.0.conv2
layer2.1.conv1
layer2.1.conv2
layer2.2.conv1
layer2.2.conv2
layer3.0.conv1
layer3.0.conv2
layer3.1.conv1
layer3.1.conv2
layer3.2.conv1
layer3.2.conv2Figure 4: Epoch-wise training FLOPs for channel growing a
ResNet-20.
0 20 40 60 80 100 120 140 160
Growing Epochs020406080100 Accuracy(%) and Complexity(%)CIFAR-10 val accuracy
Growing Model Complexity
Growing Layer Numbers
0255075100125150175
Layer Numbers
Figure 5: Tracking validation accuracy, com-
plexity and layers for Basic3ResNet growing.
10 20 30 40 50 60 70 80
Param Sparsity (%pruned)85868788899091CIFAR-10 Val Acc (%)
Val Acc vs Param Sparsity
Ours Growing
Uniform Pruning
Ours Direct DesignFigure 6: Pruned architectures obtained by ablated methods
with different parameter sparsity.
strating higher parameter efficiency. Our method also appears to have positive effect in terms of
regularization or optimization dynamics, which are lost if one attempts to directly train the final
compact structure. Appendix A.9 investigates FLOPs-based budget targets.
Comparing with Random Baseline. In addition to the uniform pruning baseline in Figure 6, we also
compare with a random sampling baseline to further separate the contribution of our configuration
space and growing method, following the criterion in (Xie et al., 2019a; Li & Talwalkar, 2019; Yu
et al., 2020; Radosavovic et al., 2019). Specifically, this random baseline replaces the procedure
for sampling entries of qin Eq. 6. Instead of using sampling probabilities derived from the learned
mask parameters s, it samples with fixed probability. As shown in Table 5, our method consistently
performs much better than this random baseline. These results, as well as the more sophisticated
baselines in Figure 6, demonstrate the effectiveness of our growing and pruning approach.
Temperature Scheduler. We compare our structure-wise temperature control to a global one in
channel growing experiments on CIFAR-10 using VGG-16, ResNet-20, and WideResNet-28-10.
Table 1 results use our structure-wise scheduler. To achieve similar sparsity with the global scheduler,
the corresponding models suffer accuracy drops of 1.4%,0.6%, and 2.5%. With the global scheduler,
optimization of mask variables stops early in training and the following epochs are equivalent to
directly training a fixed compact network. This may force the network to be stuck with a suboptimal
architecture. Appendix A.10 investigates learning rate and temperature schedule interactions.
5 C ONCLUSION
We propose a simple yet effective method to grow efficient deep networks via structured continuous
sparsification, which decreases the computational cost not only of inference but also of training. The
method is simple to implement and quick to execute; it automates the network structure reallocation
process under practical resource budgets. Application to widely used deep networks on a variety of
tasks shows that our method consistently generates models with better accuracy-efficiency trade-offs
than competing methods, while achieving considerable training cost savings.
Acknowledgments. This work was supported by the University of Chicago CERES Center
for Unstoppable Computing and the National Science Foundation under grant CNS-1956180.
9

--- PAGE 10 ---
Published as a conference paper at ICLR 2021
REFERENCES
Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task
and hardware. In ICLR , 2019.
Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Philip S. Yu. Hashnet: Deep learning to hash by
continuation. In ICCV , 2017.
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous
convolution for semantic image segmentation. arXiv:1706.05587 , 2017.
Kyunghyun Cho, Bart van Merrienboer, C Â¸aglar G Â¨ulc Â¸ehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for
statistical machine translation. In EMNLP , 2014.
Franc Â¸ois Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR , 2017.
Ekin Dogus Cubuk, Barret Zoph, Dandelion Man Â´e, Vijay Vasudevan, and Quoc V . Le. Autoaugment:
Learning augmentation policies from data. In CVPR , 2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchical image database. In CVPR , 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. In NAACL , 2019.
Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and
Andrew Zisserman. The PASCAL visual object classes challenge: A retrospective. IJCV , 2015.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In ICLR , 2019.
Ross B. Girshick. Fast R-CNN. In ICCV , 2015.
Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi.
MorphNet: Fast & simple resource-constrained structure learning of deep networks. In CVPR ,
2018.
Priya Goyal, Piotr Doll Â´ar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet
in 1 hour. arXiv:1706.02677 , 2017.
Sam Gross and Michael Wilber. Training and investigating residual nets. http://torch.ch/
blog/2016/02/04/resnets.html , 2016.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient DNNs. In
NeurIPS , 2016.
Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for
efficient neural networks. In NeurIPS , 2015.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. In ICLR , 2016.
Bharath Hariharan, Pablo Arbelaez, Lubomir D. Bourdev, Subhransu Maji, and Jitendra Malik.
Semantic contours from inverse detectors. In ICCV , 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR , 2016.
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating
deep convolutional neural networks. IJCAI , 2018.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network.
InNeurIPS Deep Learning and Representation Learning Workshop , 2015.
10

--- PAGE 11 ---
Published as a conference paper at ICLR 2021
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation , 1997.
Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv:1704.04861 , 2017.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR , 2018.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with
stochastic depth. In ECCV , 2016.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In CVPR , 2017.
Gao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q. Weinberger. CondenseNet: An
efficient DenseNet using learned group convolutions. In CVPR , 2018.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks. In NeurIPS , 2016.
Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt
Keutzer. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size.
arXiv:1602.07360 , 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML , 2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
ICLR , 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convolu-
tional neural networks. In NeurIPS , 2012.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset. http://www.cs.
toronto.edu/ Ëœkriz/cifar.html , 2014.
Carl Lemaire, Andrew Achkar, and Pierre-Marc Jodoin. Structured pruning of neural networks with
budget-aware regularization. In CVPR , 2019.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for
efficient ConvNets. In ICLR , 2017.
Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. In
UAI, 2019.
Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus. Provable filter pruning
for efficient neural networks. In ICLR , 2020.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv:1312.4400 , 2013.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In ICLR ,
2019.
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu,
and Alexander C. Berg. SSD: single shot multibox detector. In ECCV , 2016.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In CVPR , 2015.
Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through
l0regularization. In ICLR , 2018.
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. ThiNet: A filter level pruning method for deep neural
network compression. In ICCV , 2017.
11

--- PAGE 12 ---
Published as a conference paper at ICLR 2021
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization.
InNeurIPS , 2018.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. Computational Linguistics , 1993.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry P. Vetrov. Variational dropout sparsifies deep
neural networks. In ICML , 2017.
Hieu Pham, Melody Y . Guan, Barret Zoph, Quoc V . Le, and Jeff Dean. Efficient neural architecture
search via parameter sharing. In ICML , 2018.
Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Doll Â´ar. On network design
spaces for visual recognition. In ICCV , 2019.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: Imagenet
classification using binary convolutional neural networks. In ECCV , 2016.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V . Le. Regularized evolution for image
classifier architecture search. In AAAI , 2019.
Pedro Savarese and Michael Maire. Learning implicitly recurrent CNNs through parameter sharing.
InICLR , 2019.
Pedro Savarese, Hugo Silva, and Michael Maire. Winning the lottery with continuous sparsification.
InNeurIPS , 2020.
Laurent Sifre and PS Mallat. Rigid-motion scattering for image classification . PhD thesis, Ecole
Polytechnique, CMAP, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR , 2015.
Mingxing Tan and Quoc V . Le. Efficientnet: Rethinking model scaling for convolutional neural
networks. In ICML , 2019.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In CVPR , 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS , 2017.
Tao Wei, Changhu Wang, Yong Rui, and Chang Wen Chen. Network morphism. In ICML , 2016.
Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang, Fang Liu, Bin Hu, Yiran
Chen, and Hai Li. Learning intrinsic sparse structures within long short-term memory. In ICLR ,
2018.
Wei Wen, Feng Yan, Yiran Chen, and Hai Li. Autogrow: Automatic layer growing in deep convolu-
tional networks. In KDD , 2020.
Mitchell Wortsman, Ali Farhadi, and Mohammad Rastegari. Discovering neural wirings. In NeurIPS ,
2019.
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian,
Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via
differentiable neural architecture search. In CVPR , 2019.
Lingxi Xie and Alan L Yuille. Genetic cnn. In ICCV , 2017.
Saining Xie, Alexander Kirillov, Ross B. Girshick, and Kaiming He. Exploring randomly wired
neural networks for image recognition. In ICCV , 2019a.
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search.
InICLR , 2019b.
12

--- PAGE 13 ---
Published as a conference paper at ICLR 2021
Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, and Stephen Lin. Spatially adaptive inference
with stochastic feature sampling and interpolation. In ECCV , 2020.
Tien-Ju Yang, Andrew G. Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and
Hartwig Adam. NetAdapt: Platform-aware neural network adaptation for mobile applications. In
ECCV , 2018.
Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, and Mathieu Salzmann. Evaluating the
search phase of neural architecture search. In ICLR , 2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC , 2016.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.
arXiv:1409.2329 , 2014.
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShuffleNet: An extremely efficient
convolutional neural network for mobile devices. In CVPR , 2018.
Barret Zoph and Quoc V . Le. Neural architecture search with reinforcement learning. In ICLR , 2017.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition. In CVPR , 2018.
13

--- PAGE 14 ---
Published as a conference paper at ICLR 2021
A A PPENDIX
A.1 M ORE DETAILED ANALYSIS FOR BUDGET -AWARE GROWING
Conducting grid search on trade-off parameters Î»1andÎ»2is prohibitively laborious and time-
consuming. For example, to grow an efficient network on CIFAR-10, one needs to repeat many times
a run of 160-epochs training, and then pick the best model from all grown candidates. To avoid this
tedious iterative process, instead of using constants Î»1andÎ»2, we dynamically update Î»1andÎ»2in
our one-shot budget-aware growing optimization.
Here we discuss about how budget-aware dynamic growing works in our method. Without loss of
generality, we derive the mcâ€™s SGD update rule for the â„“0regularization term in Eq. 3 as:
mcâ†mcâˆ’Î·Î»base
1âˆ†uÎ´â„“
Î´mcâˆ’Î·ÂµÎ»base
1âˆ†umc (8)
where Î·is the learning rate and Âµis the weight decay factor. At the beginning of growing epochs,
when the architecture is very over-sparsified, âˆ†uandÎ»base
1âˆ†uare negative values. Then mcâ€™s
update is along the opposite direction of the â„“0regularization termâ€™s gradients, encouraging mcâ€™s
sparsification. As a result, some zero-valued mcwill be activated and the model complexity is strongly
increased to acquire enough capacity for successful training. Then, growing becomes gradually
weaker as the networkâ€™s sparsity approaches the budget ( âˆ†uto zero). Note that if the architecture
is over-parameterized, âˆ†uandÎ»base
1âˆ†ubecome positive and SGDâ€™s update rule is the same as that
ofâ„“0regularization. As such, our budget-aware growing can automatically and dynamically adapt
the architecture complexity not only based on the task loss LEbut also on the practical budget
requirements in the one-shot training process.
We also note that NAS methods usually use the validation accuracy as a target during their architecture
optimization phase, which may require some prior knowledge of validation accuracy on a given
dataset. Our growing procedure chooses sparsity budget instead of accuracy as target because: (1)
During growing, validation accuracy is influenced not only by architectures but also model weights.
Directly using âˆ†accmay lead to sub-optimal architecture optimization. (2) A sparsity budget target
is more practical and easier to set according to target devices for deployment.
A.2 D ETAILS OF EVALUATION DATASETS
Evaluation is conducted on various tasks to demonstrate the effectiveness of our proposed method.
For image classification, we use CIFAR-10 (Krizhevsky et al., 2014) and ImageNet (Deng et al.,
2009): CIFAR-10 consists of 60,000 images of 10 classes, with 6,000 images per class. The train
and test sets contain 50,000 and 10,000 images respectively. ImageNet is a large dataset for visual
recognition which contains over 1.2M images in the training set and 50K images in the validation set
covering 1,000 categories. For semantic segmentation, we use the PASCAL VOC 2012 (Everingham
et al., 2015) benchmark which contains 20 foreground object classes and one background class. The
original dataset contains 1,464 (train), 1,449 (val), and 1,456 (test) pixel-level labeled images for
training, validation, and testing, respectively. The dataset is augmented by the extra annotations
provided by (Hariharan et al., 2011), resulting in 10,582 training images. For language modeling, we
use the word level Penn Treebank (PTB) dataset (Marcus et al., 1993) which consists of 929k training
words, 73k validation words, and 82k test words, with 10,000 unique words in its vocabulary.
A.3 U NPRUNED BASELINE MODELS
For CIFAR-10, we use VGG-16 (Simonyan & Zisserman, 2015) with BatchNorm (Ioffe & Szegedy,
2015), ResNet-20 (He et al., 2016) and WideResNet-28-10 (Zagoruyko & Komodakis, 2016) as
baselines. We adopt a standard data augmentation scheme (shifting/mirroring) following (Lin et al.,
2013; Huang et al., 2016), and normalize the input data with channel means and standard deviations.
Note that we use the CIFAR version of ResNet-201, VGG-162, and WideResNet-28-103. VGG-16,
ResNet-20, and WideResNet-28-10 are trained for 160, 160, and 200 epochs, respectively, with a
1https://github.com/akamaster/pytorch resnet cifar10/blob/master/resnet.py
2https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py
3https://github.com/meliketoy/wide-resnet.pytorch/blob/master/networks/wide resnet.py
14

--- PAGE 15 ---
Published as a conference paper at ICLR 2021
batch size of 128 and initial learning rate of 0.1. For VGG-16 and ResNet-20, we divide learning
rate by 10 at epochs 80 and 120, and set the weight decay and momentum as 10âˆ’4and 0.9. For
WideResNet-28-10, the learning rate is divided by 5 at epochs 60, 120, and 160; the weight decay
and momentum are set to 5Ã—10âˆ’4and 0.9. For ImageNet, we train the baseline ResNet-50 and
MobileNetV1 models following the respective papers. We adopt the same data augmentation scheme
as in (Gross & Wilber, 2016) and report top-1 validation accuracy. For semantic segmentation, the
performance is measured in terms of pixel intersection-over-union (IOU) averaged across the 21
classes (mIOU). We use Deeplab-v3-ResNet-1014(Chen et al., 2017) as the baseline model following
the training details in (Chen et al., 2017). For language modeling, we use vanilla two-layer stacked
LSTM (Zaremba et al., 2014) as a baseline. The dropout keep ratio is 0.35 for the baseline model.
The vocabulary size, embedding size, and hidden size of the stacked LSTMs are set as 10,000, 1,500,
and 1,500, respectively, which is consistent with the settings in (Zaremba et al., 2014).
A.4 M OBILE NETV1 C HANNEL GROWING ON IMAGE NET
To further validate the effectiveness of the proposed method on compact networks, we grow the filters
of MobileNetV1 on ImageNet and compare the performance of our method to the results reported
directly in the respective papers, as shown in Table 6. In MobileNetV1 experiments, following
the same setting with Netadapt (Yang et al., 2018), we apply our method on both (1) small setting:
growing MobileNetV1(128) with 0.5 multiplier while setting the original modelâ€™s multiplier as 0.25
for comparison and (2) large setting: growing standard MobileNetV1(224) while setting the original
modelâ€™s multiplier as 0.75 for comparison. Note that MobileNetV1 is one of the most compact
networks, and thus is more challenging to simplify than other larger networks. Our lower-cost
growing method can still generate a sparser MobileNetV1 model compared with competing methods.
Table 6: Overview of the pruning performance of each algorithm on MobileNetV1 ImageNet.
Model Method Top-1 Val Acc(%) FLOPs(%) Train-Cost Savings( Ã—)
Original(25%) 45.1 (+0.0) 100 1.0( Ã—)
MobileNet MorphNet 46.0 (+0.9) 110 0.9( Ã—)
V1(128) Netadapt 46.3 (+1.2) 81 1.1(Ã—)
Ours 46.0 (+0.9) 73 1.7 (Ã—)
MobileNetOriginal(75%) 68.8 (+0.0) 100 1.0( Ã—)
V1(224)Netadapt 69.1 (+0.3) 87 1.2(Ã—)
Ours 69.3 (+0.5) 83 1.5 (Ã—)
A.5 D EEPLAB -V3-R ESNET-101 ONPASCAL VOC 2012
We also test the effectiveness of our proposed method on a semantic segmentation task by growing a
Deeplab-v3-ResNet-101 modelâ€™s filter numbers from scratch directly on the PASCAL VOC 2012
dataset. We apply our method to both the ResNet-101 backbone and ASPP module. Compared to
the baseline, the final generated network reduces the FLOPs by 58.5% and the parameter count by
49.8%, while approximately maintaining mIoU (76.5% to 76.4%). See Table 7.
Table 7: Results on the PASCAL VOC dataset.
Model Method mIOU Params(M) FLOPs(%) Train-Cost Savings( Ã—)
Deeplab Original 76.5 (-0.0) 58.0 (100%) 100 1.0( Ã—)
-v3- L1-Pruning 75.1 (-1.4) 45.7 (78.8%) 62.5 1.3( Ã—)
ResNet101 Ours 76.4 (-0.1) 29.1 (50.2%) 41.5 2.3 (Ã—)
4https://github.com/chenxi116/DeepLabv3.pytorch
15

--- PAGE 16 ---
Published as a conference paper at ICLR 2021
A.6 E XTENSION TO RECURRENT CELLS ON PTB DATASET
We focus on LSTMs (Hochreiter & Schmidhuber, 1997) with lhhidden neurons, a common variant5
of RNNs that learns long-term dependencies:
ft=Ïƒg((WfâŠ™(emT
c))xt+ (UfâŠ™(mcmT
c))htâˆ’1+bf)
it=Ïƒg((WiâŠ™(emT
c))xt+ (UiâŠ™(mcmT
c))htâˆ’1+bi)
ot=Ïƒg((WoâŠ™(emT
c))xt+ (UoâŠ™(mcmT
c))htâˆ’1+bo)
Ëœct=Ïƒh((WcâŠ™(emT
c))xt+ (UcâŠ™(mcmT
c))htâˆ’1+bc)
ct=ftâŠ™ctâˆ’1+itâŠ™Ëœct, h t=otâŠ™Ïƒh(ct)s.t. m câˆˆ {0,1}lh,e= 1lh, (9)
where Ïƒgis the sigmoid function, âŠ™denotes element-wise multiplication and Ïƒhis the hyperbolic
tangent function. xtdenotes the input vector at the time-step t,htdenotes the current hidden state,
andctdenotes the long-term memory cell state. Wf, Wi, Wo, Wcdenote the input-to-hidden weight
matrices and Uf, Ui, Uo, Ucdenote the hidden-to-hidden weight matrices. mcis binary indicator and
shared across all the gates to control the sparsity of hidden neurons.
We compare our proposed method with ISS based on vanilla two-layer stacked LSTM. As shown
in Table 8, our method finds more compact model structure at lower training cost, while achieving
similar perplexity on both validation and test sets. These improvements may be due to the fact that
our method dynamically grows and prunes the hidden neurons from very simple status towards a
better trade-off between model complexity and performance than that of ISS, which simply uses the
group lasso to penalize the norms of all groups collectively for compactness.
Table 8: Results on the PTB dataset.
Method Perplexity (val,test) Final Structure Weight(M) FLOPs(%) Train-Cost Savings( Ã—)
Original (82.57, 78.57) (1500, 1500) 66.0M (100%) 100 1.0( Ã—)
ISS (82.59, 78.65 ) (373, 315) 21.8M (33.1%) 13.4 3.8( Ã—)
Ours ( 82.46 , 78.68) (310, 275) 20.6M (31.2%) 11.9 5.1 (Ã—)
A.7 V ARIANTS OF INITIAL SEED ARCHITECTURE
In Table 9, we make a detailed comparison among initial seed architecture variants of ours and
AutoGrow (Wen et al., 2020). For both ours and AutoGrow, â€œBasicâ€ and â€œBottleneckâ€ refer to
ResNets with standard basic and bottleneck residual blocks, while â€œPlainLayersâ€ refers to stacked
convolutional, batch normalization, and ReLU layer combinations. Similar with standard ResNets, for
variants of the seed architecture, we adopt three stages for CIFAR-10 and four stages for ImageNet.
PlainNets can be obtained by simply removing shortcuts from these ResNet seed variants with equal
stage numbers. For each stage, we start from only one growing unit, within which initial filter
numbers are also initialized at one for channel growing.
A.8 T RACK OF ANY-TIME SPARSIFICATION DURING CHANNEL GROWING
Figure 7 and Figure 8 show the dynamics of train-time growing channel ratios of ResNet-20 and
VGG-16 on CIFAR-10, respectively. To better analyze the growing patterns, we visualize the channel
dynamics grouped by stages in Figure 9 for ResNet-20 and Figure 10 for VGG-16, respectively.
Note that, for VGG-16, we divide it into 5 stages based on the pooling layer positions and normalize
channel ratios by 0.5 for better visualization. We see that our method grows more channels of earlier
layers within each stage of ResNet-20. Also, the final channel sparsity of ResNet-20 is more uniform
due to the residual connections.
16

--- PAGE 17 ---
Published as a conference paper at ICLR 2021
Table 9: A detailed comparison among seed architecture variants of our method and AutoGrow (Wen
et al., 2020). In growing units term, â€œBasicâ€ and â€œBottleneckâ€ refer to ResNets with standard basic
and bottleneck residual blocks while â€œPlainLayersâ€ refers to standard convolutional layer, BN, and
ReLu layer combinations in VGG-like networks without shortcuts.
Families Variants Methods Channel Growing Growing Units Stages Shortcuts
Basic3ResNetOurs âœ“Basic 3âœ“
ResNetAutoGrow âœ— âœ“
Bottleneck4ResNetOurs âœ“Bottleneck 4âœ“
AutoGrow âœ— âœ“
Plain3NetOurs âœ“PlainLayers 3âœ—
VGG-likeAutoGrow âœ— âœ—
Plain4NetOurs âœ“PlainLayers 4âœ—
AutoGrow âœ— âœ—
0 20 40 60 80 100 120 140 160
Epochs020406080100Retained Channels Ratio(%)Epoch-wise Channels Dynamics of Each Conv
conv1
layer1.0.conv1
layer1.0.conv2
layer1.1.conv1
layer1.1.conv2
layer1.2.conv1
layer1.2.conv2
layer2.0.conv1
layer2.0.conv2
layer2.1.conv1
layer2.1.conv2
layer2.2.conv1
layer2.2.conv2
layer3.0.conv1
layer3.0.conv2
layer3.1.conv1
layer3.1.conv2
layer3.2.conv1
layer3.2.conv2
Figure 7: Epoch-wise retained channel ratio dy-
namics for each layer in ResNet-20.
0 20 40 60 80 100 120 140 160
Epochs020406080100Retained Channels Ratio(%)Epoch-wise Channels Dynamics of Each Conv
feature.0
feature.3
feature.7
feature.10
feature.14
feature.17
feature.20
feature.24
feature.27
feature.30
feature.34
feature.37
feature.40Figure 8: Epoch-wise retained channel ratio dy-
namics for each layer in VGG-16.
A.9 FLOP S-BASED BUDGET -AWARE GROWING
We also investigate the effectiveness of setting a FLOPs target for budget-aware growing in Figure 11.
We observe similar trends among uniform pruning ,ours growing , and ours direct design : in most
FLOPs budget settings, our growing method outperforms direct design and uniform pruning. We
also observe that when setting extreme sparse FLOPs target ( e.g.,85%), our method achieves lower
accuracy than the other two variants. The reason is that our channel growing is forced to only grow
architectures from âˆ¼99% sparsity up to âˆ¼85% FLOPs and âˆ¼90% parameters sparsity, during
which models cannot acquire enough capacity to be well trained.
A.10 I NTERACTIONS BETWEEN LEARNING RATE AND TEMPERATURE SCHEDULERS
Two factors influence the growing optimization speed in our method: temperature and learning
rate, which are hyperparameters controlled by their respective schedulers. We first visualize the
structure-wise separate temperature dynamics in Figure 12 by averaging temperatures per layer during
ResNet-20 channel growing on CIFAR-10. We see that temperatures are growing with different
rates for channels. Usually, low learning rate and high temperature in late training epochs make
the network growing optimization become very stable. In Figure 13, we deliberately decay Î³in the
temperature scheduler, mirroring the learning rate decay schedule, in order to force growing until
the end. As shown in Figure 14, our method is still adapting some layers even at the last epoch. We
find that such instability degrades performance, since some newly grown filters may not have enough
time to become well trained.
5The proposed configuration space can be readily applied to the compression of GRUs (Cho et al., 2014) and
vanilla RNNs.
17

--- PAGE 18 ---
Published as a conference paper at ICLR 2021
Figure 9: Visualization of retained channel ratio dynamics for each stage in ResNet-20.
Figure 10: Visualization of retained channel ratio dynamics for each stage in VGG-16.
10 20 30 40 50 60 70 80
FLOPs Sparsity (%pruned)8485868788899091CIFAR-10 Val Acc (%)
Val Acc vs FLOPs Sparsity
Ours Growing
Uniform Pruning
Ours Direct Design
Figure 11: Pruned architectures obtained
by ablated methods with different FLOPs
sparsity.
0 20 40 60 80 100 120 140 160
Epochs010203040506070Average TempuratureAverage Tempurature of Each Conv
conv1
layer1.0.conv1
layer1.0.conv2
layer1.1.conv1
layer1.1.conv2
layer1.2.conv1
layer1.2.conv2
layer2.0.conv1
layer2.0.conv2
layer2.1.conv1
layer2.1.conv2
layer2.2.conv1
layer2.2.conv2
layer3.0.conv1
layer3.0.conv2
layer3.1.conv1
layer3.1.conv2
layer3.2.conv1
layer3.2.conv2Figure 12: Structure-wise separate temperature dynamics
in channel growing.
0 20 40 60 80 100 120 140 160
Epochs12345678Average TempuratureAverage Tempurature of Each Conv
conv1
layer1.0.conv1
layer1.0.conv2
layer1.1.conv1
layer1.1.conv2
layer1.2.conv1
layer1.2.conv2
layer2.0.conv1
layer2.0.conv2
layer2.1.conv1
layer2.1.conv2
layer2.2.conv1
layer2.2.conv2
layer3.0.conv1
layer3.0.conv2
layer3.1.conv1
layer3.1.conv2
layer3.2.conv1
layer3.2.conv2
Figure 13: Structure-wise separate decayed tem-
perature dynamics in channel growing.
0 20 40 60 80 100 120 140 160
Epochs020406080100Retained FLOPs Ratio(%)Epoch-wise FLOPs of Each Conv
conv1
layer1.0.conv1
layer1.0.conv2
layer1.1.conv1
layer1.1.conv2
layer1.2.conv1
layer1.2.conv2
layer2.0.conv1
layer2.0.conv2
layer2.1.conv1
layer2.1.conv2
layer2.2.conv1
layer2.2.conv2
layer3.0.conv1
layer3.0.conv2
layer3.1.conv1
layer3.1.conv2
layer3.2.conv1
layer3.2.conv2Figure 14: Track of epoch-wise train-time FLOPs
for channel growing in ResNet-20.
18
