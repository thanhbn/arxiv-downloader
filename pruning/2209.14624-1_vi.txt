Có cần thiết phải phức tạp để cắt tỉa mạng nơ-ron không?
Một nghiên cứu tình huống về cắt tỉa độ lớn toàn cục

Manas Gupta, Efe Camci, Vishandi Rudy Keneta, Abhishek Vaidyanathan, Ritwik Kanodia, Chuan-Sheng Foo,
Min Wu và Jie Lin

Tóm tắt - Cắt tỉa mạng nơ-ron đã trở nên phổ biến trong thập kỷ qua khi nó được chứng minh rằng một số lượng lớn trọng số có thể được loại bỏ an toàn khỏi các mạng nơ-ron hiện đại mà không ảnh hưởng đến độ chính xác. Nhiều phương pháp cắt tỉa đã được đề xuất kể từ đó, mỗi phương pháp đều tuyên bố tốt hơn so với các nghiên cứu trước đó, tuy nhiên, với chi phí là các phương pháp cắt tỉa ngày càng phức tạp hơn. Các phương pháp này bao gồm việc sử dụng điểm số quan trọng, nhận phản hồi thông qua lan truyền ngược hoặc có các quy tắc cắt tỉa dựa trên kinh nghiệm trong số những cách khác. Trong nghiên cứu này, chúng tôi đặt câu hỏi liệu mô hình giới thiệu độ phức tạp này có thực sự cần thiết để đạt được kết quả cắt tỉa tốt hơn không. Chúng tôi so sánh các kỹ thuật SOTA này với một cơ sở cắt tỉa đơn giản, cụ thể là Cắt tỉa Độ lớn Toàn cục (Global MP), xếp hạng các trọng số theo thứ tự độ lớn của chúng và cắt tỉa những trọng số nhỏ nhất. Đáng ngạc nhiên, chúng tôi thấy rằng Global MP thuần túy hoạt động rất tốt so với các kỹ thuật SOTA. Khi xem xét sự đánh đổi giữa độ thưa và độ chính xác, Global MP hoạt động tốt hơn tất cả các kỹ thuật SOTA ở tất cả các tỷ lệ độ thưa. Khi xem xét sự đánh đổi giữa FLOPs và độ chính xác, một số kỹ thuật SOTA vượt trội hơn Global MP ở các tỷ lệ độ thưa thấp hơn, tuy nhiên, Global MP bắt đầu hoạt động tốt ở các tỷ lệ độ thưa cao và hoạt động rất tốt ở các tỷ lệ độ thưa cực cao. Hơn nữa, chúng tôi thấy rằng một vấn đề chung mà nhiều thuật toán cắt tỉa gặp phải ở tỷ lệ độ thưa cao, cụ thể là sự sụp đổ lớp, có thể được khắc phục dễ dàng trong Global MP. Chúng tôi khám phá lý do tại sao sự sụp đổ lớp xảy ra trong các mạng và cách nó có thể được giảm thiểu trong Global MP bằng cách sử dụng một kỹ thuật gọi là Ngưỡng Tối thiểu. Chúng tôi trình bày các phát hiện trên trên nhiều mô hình khác nhau (WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1 và FastGRNN) và nhiều tập dữ liệu (CIFAR-10, ImageNet và HAR-2). Mã nguồn có sẵn tại https://github.com/manasgupta-1/GlobalMP.

Từ khóa chỉ mục - Cắt tỉa Độ lớn Toàn cục, Mạng Nơ-ron Hiệu quả, Độ thưa, ImageNet.

I. GIỚI THIỆU

Việc mở rộng quy mô của các mạng nơ-ron đang trở thành một cách phổ biến để tăng hiệu suất mô hình [1], [2], [3]. Tuy nhiên, điều này gây ra chi phí đáng kể cho môi trường [4] và làm cho việc triển khai trên các thiết bị biên trở nên khó khăn [5]. Do đó, cắt tỉa mạng nơ-ron đã nổi lên như một công cụ thiết yếu để giảm kích thước của các mạng nơ-ron hiện đại. Các phương pháp mới đã sử dụng vô số kỹ thuật cắt tỉa bao gồm các phương pháp dựa trên gradient, độ nhạy cảm với hoặc phản hồi từ một hàm mục tiêu, các thước đo khoảng cách hoặc độ tương tự, các kỹ thuật dựa trên chính quy hóa, trong số những cách khác. Các kỹ thuật cắt tỉa tiên tiến (SOTA) sử dụng các quy tắc phức tạp như cắt tỉa lặp đi lặp lại và tái sinh các tham số trọng số bằng cách sử dụng các quy tắc kinh nghiệm mỗi vài trăm lần lặp, như trong DSR [6]. SM [7] sử dụng động lượng thưa thớt được hưởng lợi từ các gradient được làm mịn theo cấp số nhân (động lượng) để tìm các lớp và trọng số làm giảm lỗi và sau đó phân phối lại các trọng số đã cắt tỉa trên các lớp bằng cách sử dụng độ lớn động lượng trung bình của mỗi lớp. Đối với mỗi lớp, động lượng thưa thớt phát triển các trọng số bằng cách sử dụng độ lớn động lượng của các trọng số có giá trị bằng không. Một kỹ thuật SOTA phổ biến khác, RigL [8], cũng cắt tỉa và tái sinh trọng số một cách lặp đi lặp lại mỗi vài lần lặp. Họ sử dụng đồng nhất hoặc Erdos-Renyi-Kernel (ERK) để cắt tỉa các kết nối và tái sinh các kết nối dựa trên các gradient có độ lớn cao nhất. Trong số các kỹ thuật gần đây, DPF [9] sử dụng phân bổ động của mô hình độ thưa và kết hợp tín hiệu phản hồi để kích hoạt lại các trọng số bị cắt tỉa quá sớm, trong khi STR [10] sử dụng Tái tham số hóa Ngưỡng Mềm và sử dụng lan truyền ngược để tìm tỷ lệ độ thưa cho mỗi lớp.

Mặc dù có số lượng cao các thuật toán cắt tỉa mới được đề xuất, lợi ích hữu hình của nhiều thuật toán trong số đó vẫn còn đáng ngờ. Ví dụ, gần đây đã được chứng minh rằng nhiều lược đồ cắt tỉa tại khởi tạo (PAI) không hoạt động tốt như mong đợi [11]. Trong bài báo đó, nó được chứng minh thông qua một số thí nghiệm rằng các lược đồ PAI này thực tế không tốt hơn cắt tỉa ngẫu nhiên, đây là một trong những cơ sở cắt tỉa ngây thơ nhất mà không có độ phức tạp nào liên quan. Phát hiện này thực sự đặt ra một câu hỏi khác trong tâm trí chúng tôi: nếu một PAI được thiết kế tốt thậm chí không thể sánh với hiệu suất của cắt tỉa ngẫu nhiên, liệu các phương pháp cắt tỉa đơn giản như cắt tỉa toàn cục hoặc các biến thể của chúng có thể vượt trội hơn các thuật toán hiện có khác không? Trong nghiên cứu này, chúng tôi đặt câu hỏi về xu hướng đề xuất các thuật toán cắt tỉa ngày càng phức tạp và đánh giá liệu độ phức tạp như vậy có thực sự cần thiết để đạt được kết quả vượt trội không. Chúng tôi so sánh các kỹ thuật cắt tỉa tiên tiến (SOTA) phổ biến với một cơ sở cắt tỉa ngây thơ, cụ thể là Cắt tỉa Độ lớn Toàn cục (Global MP). Global MP xếp hạng tất cả các trọng số trong một mạng nơ-ron theo độ lớn của chúng và cắt tỉa những trọng số nhỏ nhất (Hình 1).

Mặc dù đơn giản, Global MP chưa được phân tích và đánh giá một cách toàn diện trong tài liệu. Mặc dù một số nghiên cứu trước đây đã sử dụng Global MP làm cơ sở [12], [13], [14], [15], [16], [17], họ đã bỏ lỡ việc tiến hành các thí nghiệm nghiêm ngặt với nó; ví dụ, trong các cài đặt của cả cắt tỉa dần dần và một lần hoặc so sánh nó với SOTA. Tương tự, nhiều bài báo SOTA không sử dụng Global MP để so sánh và bỏ lỡ việc nắm bắt hiệu suất đáng chú ý của nó [8], [10], [18], [2], [19]. Chúng tôi lấp đầy khoảng trống này trong việc đánh giá hiệu quả của Global MP và chứng minh hiệu suất vượt trội của nó dưới nhiều điều kiện thí nghiệm.

Chúng tôi chứng minh rằng liên quan đến sự đánh đổi giữa độ thưa và độ chính xác, Global MP luôn vượt trội hơn tất cả các kỹ thuật tiên tiến (SOTA) trên các tỷ lệ độ thưa khác nhau. Về sự đánh đổi giữa FLOPs và độ chính xác, một số kỹ thuật SOTA thể hiện hiệu suất vượt trội hơn Global MP ở các tỷ lệ độ thưa thấp hơn. Tuy nhiên, Global MP thể hiện hiệu quả đáng kể ở các tỷ lệ độ thưa cao hơn và xuất sắc đặc biệt tốt ở các mức độ thưa cực cao. Trong khi đạt được hiệu suất như vậy, Global MP không yêu cầu bất kỳ siêu tham số cụ thể của thuật toán nào phải được điều chỉnh. Chúng tôi cũng làm sáng tỏ một vấn đề tiềm ẩn với cắt tỉa, được gọi là sự sụp đổ lớp, trong đó toàn bộ một lớp bị cắt tỉa, dẫn đến mất mát đáng kể về độ chính xác. Việc khắc phục điều này trong Global MP rất đơn giản thông qua việc giới thiệu Ngưỡng Tối thiểu (MT) để giữ lại một số lượng tối thiểu trọng số trong mỗi lớp. Chúng tôi tiến hành thí nghiệm trên các mô hình WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1 và FastGRNN, và trên các tập dữ liệu CIFAR-10, ImageNet và HAR-2. Chúng tôi kiểm tra Global MP cho cả cài đặt không có cấu trúc và có cấu trúc cũng như cài đặt một lần và dần dần, và chia sẻ các phát hiện của chúng tôi.

II. NGHIÊN CỨU LIÊN QUAN

Nén các mạng nơ-ron đã trở thành một lĩnh vực nghiên cứu quan trọng do sự gia tăng nhanh chóng về kích thước của các mạng nơ-ron [20], nhu cầu về suy luận nhanh [21], ứng dụng vào các nhiệm vụ thực tế [22], [23], [24], [25], [26] và mối quan tâm về dấu chân carbon của việc huấn luyện các mạng nơ-ron lớn [4]. Qua nhiều năm, một số kỹ thuật nén đã xuất hiện trong tài liệu [27], [28], như lượng tử hóa, phân tích nhân tử, chú ý, chưng cất kiến thức, tìm kiếm kiến trúc và cắt tỉa [29], [30], [31], [32]. So với các danh mục khác, cắt tỉa có tính chất tổng quát hơn và đã cho thấy hiệu suất mạnh mẽ [2].

Nhiều kỹ thuật cắt tỉa đã được phát triển qua nhiều năm, sử dụng đạo hàm bậc nhất hoặc bậc hai [33], [34], [1], các phương pháp dựa trên gradient [35], [36], [37], độ nhạy cảm với hoặc phản hồi từ một số hàm mục tiêu [9], [38], [39], [40], [41], các thước đo khoảng cách hoặc độ tương tự [42], tối ưu hóa Bayesian [43], các kỹ thuật dựa trên chính quy hóa [44], [10], [45], [46], [47], và tiêu chí dựa trên độ lớn [8], [17], [18], [48], [49]. Một thủ thuật quan trọng đã được khám phá trong [50] để cắt tỉa và huấn luyện lại một mạng một cách lặp đi lặp lại, do đó bảo tồn độ chính xác cao. Cắt tỉa Mạng Nơ-ron Thời gian chạy [51] cố gắng sử dụng học tăng cường (RL) để nén bằng cách huấn luyện một tác nhân RL để chọn các mạng con nhỏ hơn trong quá trình suy luận. [52] thiết kế phương pháp đầu tiên sử dụng RL để cắt tỉa. Tuy nhiên, các phương pháp huấn luyện RL thường yêu cầu ngân sách huấn luyện RL bổ sung và thiết kế không gian hành động và trạng thái RL cẩn thận [53], [54].

Mặt khác, Global MP hoạt động bằng cách xếp hạng tất cả các tham số trong một mạng theo độ lớn tuyệt đối của chúng và sau đó cắt tỉa những tham số nhỏ nhất. Do đó, nó khá trực quan, logic và đơn giản. Nó cũng không nên nhầm lẫn với các phương pháp sử dụng cắt tỉa toàn cục nhưng không tiến hành cắt tỉa độ lớn, ví dụ, SNIP [35]. Các phương pháp này sử dụng tiêu chí phức tạp, đầu tiên để xác định tính hiển nhiên của các trọng số một cách toàn cục và sau đó áp dụng cắt tỉa. Chúng tôi trình bày ở đây một so sánh sâu về các kỹ thuật SOTA so với Global MP. Cắt tỉa Độ lớn Dần dần (GMP) [18] sử dụng một lịch trình cắt tỉa đồng nhất và cắt tỉa mỗi lớp với cùng một lượng, do đó, không tính đến tầm quan trọng tương đối của các lớp. Mặt khác, Global MP cắt tỉa mỗi lớp khác nhau. Tái tham số hóa Thưa thớt Động (DSR) [6] cắt tỉa và tái sinh trọng số mỗi vài trăm lần lặp. Nó cũng sử dụng Global MP để cắt tỉa các trọng số. Tuy nhiên, nó áp đặt một số ràng buộc bổ sung dựa trên kinh nghiệm lên quá trình cắt tỉa, chẳng hạn như không cắt tỉa một số lớp được chọn trong mạng. Trong trường hợp các trọng số cần tái sinh vượt quá khả năng của một lớp, nó sau đó sử dụng các kinh nghiệm bổ sung để phân phối lại các trọng số bổ sung. Những loại kinh nghiệm này vừa tăng thêm độ phức tạp vừa hạn chế các hành động cắt tỉa tiềm năng có thể được thực hiện. Khám phá Dây nối Thần kinh (DNW) [19] tập trung vào việc học kết nối của các kênh trong một mạng. Nó không chủ yếu là một kỹ thuật cắt tỉa và giống hơn với Tìm kiếm Kiến trúc Mạng Nơ-ron (NAS).

Động lượng Thưa thớt (SM) [7] sử dụng phương pháp dựa trên kinh nghiệm để cắt tỉa và tái sinh trọng số. Họ sử dụng động lượng trung bình để đánh giá tầm quan trọng của mỗi lớp và gán tham số tương ứng. Tương tự như DSR, một số lớp nhất định không bao giờ bị cắt tỉa và trong trường hợp tái sinh vượt quá khả năng của một lớp, các kinh nghiệm bổ sung được sử dụng để phân phối lại trọng số. Phương pháp này cũng tốn kém về mặt tính toán vì các gradient cần được lưu trữ trong bộ nhớ và FLOPs bổ sung được yêu cầu để tính động lượng trung bình cho mỗi tham số. Do đó, nó không hiệu quả bằng Global MP, ít tốn kém về mặt tính toán và không dựa vào kinh nghiệm để cắt tỉa. Thao túng Xổ số (RigL) [8] phân bổ độ thưa dựa trên số lượng tham số trong một lớp. Do đó, tầm quan trọng của một lớp dựa trên kích thước của nó, điều này không nhất thiết là thước đo chính xác cho tất cả các trường hợp. Cắt tỉa Động với Phản hồi (DPF) [9] cũng sử dụng Global MP để cắt tỉa. Tuy nhiên, nó áp đặt một ràng buộc bổ sung là giữ lớp cuối cùng hoàn toàn dày đặc. Do đó, nó không linh hoạt bằng Global MP thuần túy cho phép tất cả các lớp được cắt tỉa.

Tái tham số hóa Ngưỡng Mềm (STR) [10] là một kỹ thuật dựa trên chính quy hóa trừ đi một giá trị nhất định từ các trọng số trong mỗi epoch. Mục tiêu độ thưa chính xác không thể được kiểm soát trong STR, và nó yêu cầu điều chỉnh nặng các siêu tham số để đạt được mục tiêu độ thưa yêu cầu. Do đó, nó không linh hoạt và tổng quát bằng Global MP. Xấp xỉ Không ma trận của Thông tin Bậc hai (M-FAC) [55] và tiền thân của nó (WoodFisher) [56] sử dụng xấp xỉ của Hessian bậc hai để cắt tỉa trọng số. Cắt tỉa dựa trên Hessian có nguyên tắc nhưng không thể tính toán được. Do đó, các xấp xỉ cần được thực hiện cho Hessian. Các xấp xỉ này dẫn đến điểm số quan trọng không quá chính xác cho các trọng số, trong khi các xấp xỉ như vậy không cần thiết trong Global MP. Do đó, Global MP nói chung có nguyên tắc hơn, không có kinh nghiệm và ít tốn kém về mặt tính toán hơn so với các phương pháp SOTA và có thể là lý do cơ bản cho hiệu suất vượt trội của nó.

III. PHƯƠNG PHÁP

Trong phần này, chúng tôi giải thích cách Global MP hoạt động bằng cách mô tả các thành phần chính của nó. Chúng tôi cũng giới thiệu một cơ chế ngưỡng đơn giản, được gọi là Ngưỡng Tối thiểu (MT), để tránh vấn đề sụp đổ lớp ở mức độ thưa cao.

A. Cắt tỉa Độ lớn Toàn cục (Global MP)

Global MP là một phương pháp cắt tỉa dựa trên độ lớn, trong đó các trọng số lớn hơn một ngưỡng nhất định được giữ lại, và các trọng số nhỏ hơn ngưỡng được cắt tỉa trên toàn bộ mạng nơ-ron. Ngưỡng được tính toán dựa trên tỷ lệ độ thưa mục tiêu và không phải là siêu tham số cần được điều chỉnh hoặc học. Cho một tỷ lệ độ thưa mục tiêu κtarget, ngưỡng t đơn giản được tính toán như độ lớn trọng số phục vụ như một điểm phân cách giữa κtarget phần trăm trọng số nhỏ nhất và phần còn lại, một khi tất cả các trọng số được sắp xếp thành một mảng dựa trên độ lớn của chúng. Chính thức, cho một ngưỡng t được tính toán và mỗi trọng số riêng lẻ w trong bất kỳ lớp nào, trọng số mới wnew được định nghĩa như sau:

wnew = {
0 nếu |w| < t,
w nếu ngược lại.
}                                                    (1)

Trong Global MP, một ngưỡng duy nhất được đặt cho toàn bộ mạng dựa trên độ thưa mục tiêu cho mạng. Điều này trái ngược với cắt tỉa theo lớp, trong đó các giá trị ngưỡng khác nhau phải được tìm kiếm cho từng lớp riêng lẻ. Trong trường hợp cắt tỉa đồng nhất mặt khác, một ngưỡng cho mỗi lớp cần được tính toán dựa trên mục tiêu độ thưa được gán cho các lớp một cách đồng nhất trên mạng. Trong khía cạnh này, Global MP hiệu quả hơn cắt tỉa theo lớp hoặc đồng nhất vì ngưỡng không cần được tìm kiếm hoặc tính toán cho từng lớp riêng lẻ.

B. Ngưỡng Tối thiểu (MT)

Ngưỡng Tối thiểu (MT) đề cập đến số lượng trọng số cố định được bảo tồn trong mỗi lớp của mạng nơ-ron sau cắt tỉa. MT là một giá trị vô hướng được cố định trước khi bắt đầu chu kỳ cắt tỉa. Các trọng số trong một lớp được sắp xếp theo độ lớn của chúng và số lượng MT lớn nhất của các trọng số được bảo tồn. Ví dụ, MT của 500 có nghĩa là 500 trọng số lớn nhất trong mỗi lớp cần được bảo tồn sau cắt tỉa. Nếu một lớp ban đầu có số lượng trọng số nhỏ hơn số MT, thì tất cả các trọng số của lớp đó sẽ được bảo tồn. Điều này tương ứng với:

‖Wl‖0 ≥ {
σ nếu m ≥ σl,
m nếu ngược lại.
}                                                    (2)

Thuật ngữ Wl ∈ Rm biểu thị vector trọng số cho lớp l, σ là giá trị MT về số lượng trọng số và ‖Wl‖0 chỉ ra số lượng phần tử khác không trong Wl. Chúng tôi giải thích trong phần tiếp theo cách thực hiện cắt tỉa thực tế bằng MT.

C. Quy trình Cắt tỉa

Quy trình cắt tỉa cho Global MP bao gồm việc cắt tỉa một mô hình cho đến khi đạt được mục tiêu độ thưa mong muốn và huấn luyện hoặc tinh chỉnh nó trong số lần lặp được chỉ định. Nó hỗ trợ cả cài đặt cắt tỉa một lần và dần dần cũng như có hoặc không có MT. Người dùng có thể chọn bất kỳ cài đặt cắt tỉa nào phù hợp với trường hợp sử dụng của họ. Quy trình bắt đầu bằng việc chọn một mô hình đã được huấn luyện trước trong cắt tỉa một lần, hoặc một mô hình chưa được huấn luyện trong cắt tỉa dần dần. Tiếp theo, độ thưa của mô hình được kiểm tra và nếu độ thưa thấp hơn độ thưa mục tiêu, thì mô hình được cắt tỉa bằng Global MP thuần túy hoặc Global MP với MT, theo lựa chọn của người dùng. Một khi mô hình được cắt tỉa, nó được huấn luyện cho trường hợp cắt tỉa dần dần hoặc tinh chỉnh cho trường hợp cắt tỉa một lần. Khung Global MP cho phép linh hoạt cho các trọng số đã được cắt tỉa trước đó tái sinh, nếu chúng trở nên hoạt động hơn trong các epoch sau, cho trường hợp cắt tỉa dần dần. Không có cắt tỉa cứng nào được thực hiện trong đó các trọng số bị đặt về không vĩnh viễn. Mặt nạ cắt tỉa được tính toán lại trong mỗi epoch do đó cho phép các trọng số đã được cắt tỉa trước đó tái sinh. Quy trình trên lặp lại cho đến khi đạt được epoch cuối cùng. Đối với trường hợp cắt tỉa một lần, các epoch sau chỉ được sử dụng để tinh chỉnh vì cắt tỉa xảy ra một lần trong epoch đầu tiên. Điều này kết thúc quy trình và kết quả cuối cùng là một mô hình đã được cắt tỉa và huấn luyện (hoặc tinh chỉnh).

IV. THÍ NGHIỆM

Dưới đây chúng tôi mô tả các thí nghiệm liên quan đến Global MP so với các thuật toán cắt tỉa tiên tiến (SOTA).

A. So sánh với SOTA

Chúng tôi so sánh Global MP với nhiều thuật toán SOTA phổ biến được biết đến với cắt tỉa, như SNIP [35], SM [7], DSR [6], DPF [9], GMP [18], DNW [19], RigL [8], và STR [10]. Những thuật toán này bao gồm một phổ rộng các phương pháp liên quan đến cắt tỉa lặp đi lặp lại và tái sinh trọng số mỗi vài lần lặp, cắt tỉa tại khởi tạo, sử dụng gradient và tín hiệu phản hồi để cắt tỉa, và cắt tỉa bằng chính quy hóa. Chúng tôi báo cáo kết quả từ các thuật toán này bất cứ khi nào họ báo cáo kết quả cho tập dữ liệu cụ thể đang được thí nghiệm.

1) CIFAR-10: Chúng tôi tiến hành thí nghiệm để so sánh Global MP với các thuật toán cắt tỉa SOTA trên tập dữ liệu CIFAR-10. Chúng tôi so sánh Global MP Một lần với bốn thuật toán trong trường hợp này: SNIP [35], SM [7], DSR [6], và DPF [9]. Chúng tôi báo cáo kết quả trên hai kiến trúc mạng phổ biến và được cắt tỉa rộng rãi, cụ thể là WideResNet-28-8 (WRN-28-8) và ResNet-32 [57]. Đối với cả hai kiến trúc, chúng tôi bắt đầu với mô hình gốc có cùng độ chính xác ban đầu như các thuật toán khác để có so sánh công bằng. Đối với thí nghiệm WRN-28-8 Bảng I, Global MP hoạt động tốt hơn so với phần còn lại của các đối thủ cạnh tranh ở cả mức độ thưa 90% và 95%. Global MP vượt trội hơn DSR và SM trong tất cả các trường hợp, do các ràng buộc bổ sung dựa trên kinh nghiệm của chúng hạn chế việc lựa chọn các lớp cần cắt tỉa. Đối với ResNet-32 (Bảng I), Global MP vượt trội ở độ thưa 95% và đứng thứ hai tốt nhất ở độ thưa 90%. Global MP vượt trội hơn DSR và SM cho tất cả các mức độ thưa trong trường hợp này. Đây là dấu hiệu cho thấy khả năng của Global MP so với các thuật toán khác, trong khi không có độ phức tạp bổ sung nào.

2) ImageNet: Theo hiệu suất thuận lợi trên tập dữ liệu CIFAR-10, chúng tôi so sánh Global MP với các đối thủ cạnh tranh khác trong tài liệu trên tập dữ liệu ImageNet. Đây là một tập dữ liệu rất thách thức so với CIFAR-10, có khoảng 1,3 triệu hình ảnh RGB với 1.000 lớp. Sử dụng tập dữ liệu này, chúng tôi so sánh Global MP với các thuật toán SOTA như GMP [18], DSR [6], DNW [19], SM [7], RigL [8], WoodFisher [56], MFAC [55], DPF [9], và STR [10]. Hai kiến trúc mạng chúng tôi sử dụng cho so sánh này là ResNet-50 và MobileNet-V1 [58], hai kiến trúc phổ biến nhất để so sánh các thuật toán cắt tỉa trên ImageNet [14]. Đối với ResNet-50, chúng tôi thêm vào một cài đặt thí nghiệm bổ sung của Global MP dần dần để cung cấp so sánh kỹ lưỡng hơn với các phương pháp SOTA. Chúng tôi bắt đầu lại từ cùng độ chính xác ban đầu cho các mô hình chưa cắt tỉa cho tất cả các thuật toán, bằng cách khớp kết quả trong các bài báo gốc của họ hoặc tái tạo kết quả của họ bất cứ khi nào mã của họ có sẵn. Chúng tôi lấy mẫu bốn mức độ thưa từ độ thưa thấp (80%) đến độ thưa cực cao (98%) để cung cấp ảnh chụp toàn diện trên các mức độ thưa khác nhau.

Hiệu suất đáng chú ý của Global MP trở nên rõ ràng trong các thí nghiệm ResNet-50 trên ImageNet. Như có thể thấy từ Bảng II, đối với sự đánh đổi độ thưa-độ chính xác, Global MP vượt trội hoặc đạt được độ chính xác tương đương với tất cả các đối thủ cạnh tranh khác ở mọi mức độ thưa từ 80% đến 98% (xem kết quả in đậm). MFAC hoạt động gần gũi ở độ thưa 95% tuy nhiên, đó không phải là so sánh tương tự vì độ thưa của họ hơi thấp hơn (95%) so với Global MP (95,3%). Chúng tôi lấy mục tiêu độ thưa giới hạn trên cho mỗi mức độ thưa cho Global MP để khớp với phương pháp có độ thưa cao nhất được báo cáo trong mức độ thưa đó. Đối với trường hợp độ thưa cực cao (98%), Global MP vượt qua thuật toán tốt thứ hai (STR) với biên độ lớn 5,11%. Đối với sự đánh đổi FLOPs-độ chính xác, việc so sánh khó khăn hơn vì các phương pháp báo cáo các mục tiêu FLOPs khác nhau và không thể thực hiện so sánh tương tự đơn giản. Tuy nhiên, một người thực hành có kinh nghiệm có thể đánh giá sơ bộ hiệu quả của các phương pháp dựa trên tỷ lệ giữa FLOPs bổ sung được cắt tỉa so với sự giảm độ chính xác của các phương pháp. Dựa trên điều này, chúng tôi thấy rằng một số kỹ thuật SOTA thể hiện hiệu suất FLOPs-độ chính xác vượt trội hơn Global MP ở các tỷ lệ độ thưa thấp hơn là 80% và 90%. Tuy nhiên, Global MP trở nên cạnh tranh ở độ thưa 95% và hoạt động rất tốt ở tỷ lệ độ thưa cực cao là 98%, đạt được 5,11% độ chính xác so với giảm 2% FLOPs so với phương pháp tốt thứ hai (STR).

Chúng tôi cũng thấy rằng Global MP dần dần nói chung hoạt động tốt hơn Global MP một lần ở các tỷ lệ độ thưa cao và cực cao. Điều này là do mặt nạ cắt tỉa được phép thay đổi nhiều lần trong Global MP dần dần so với chỉ một lần trong Global MP một lần, và do đó, hội tụ đến một giá trị được tối ưu hóa hơn. Hầu hết các phương pháp SOTA cũng theo cùng một phương pháp trong đó họ cho phép mặt nạ cắt tỉa thay đổi mỗi epoch hoặc đôi khi nhiều lần trong một epoch. Nhìn chung, Global MP vượt trội hơn tất cả các thuật toán SOTA về sự đánh đổi độ thưa-độ chính xác và đứng thứ hai, sau STR, về sự đánh đổi FLOPs-độ chính xác. Đó là một phát hiện quan trọng rằng một thuật toán đơn giản như Global MP có thể vượt trội hơn các đối thủ cạnh tranh SOTA khác kết hợp các lựa chọn thiết kế rất phức tạp hoặc các quy trình đòi hỏi tính toán cao.

Chúng tôi cũng kiểm tra một kiến trúc khác trên ImageNet, MobileNet-V1, đây là một kiến trúc nhỏ hơn và hiệu quả hơn nhiều so với ResNet-50. Trong trường hợp này, các đối thủ cạnh tranh mạnh bị hạn chế trong tài liệu; chỉ có hai trong số các thuật toán đã nề trên có thể trình bày kết quả cạnh tranh do thực tế rằng kiến trúc này có ít dư thừa hơn. Chúng tôi so sánh Global MP với hai đối thủ cạnh tranh khác ở hai mức độ thưa mục tiêu: 75% và 90%. Như có thể thấy trong Bảng III, Global MP vượt trội hơn các thuật toán SOTA về sự đánh đổi độ thưa-độ chính xác với biên độ hơn 2% ở độ thưa 75%, đây là một kết quả đáng kể cho thấy MobileNet-V1 compact như thế nào. Ở độ thưa 90% mặt khác, tính compact cùng đó khiến Global MP cắt tỉa quá mức một số lớp trong mạng, dẫn đến giảm độ chính xác đáng kể. Đây là vấn đề sụp đổ lớp đã đề cập ở trên, và nó được khắc phục dễ dàng khi MT được giới thiệu vào Global MP. Chúng tôi sử dụng giá trị MT là 0,2% được xác định bằng cách sử dụng cùng quy trình tìm kiếm lưới như bất kỳ siêu tham số nào khác. Thường các giá trị từ 0,01% đến 0,3% hoạt động tốt cho MT bất kể mô hình và tập dữ liệu.

Chúng tôi thấy rằng MT hoạt động như một siêu tham số điển hình. Khi tăng giá trị MT ban đầu, độ chính xác tăng, cho đến khi nó đạt đến giá trị tối đa. Sau đó, việc tăng giá trị MT dẫn đến giảm độ chính xác. Do đó, một giá trị phù hợp có thể được tìm thấy bằng cách tìm kiếm trên các giá trị MT. Độ chính xác của Global MP ở độ thưa 90% vượt xa SOTA một lần nữa với việc khắc phục đơn giản như vậy, và biên độ độ chính xác so với đối thủ cạnh tranh tiếp theo trở nên cao hơn 2%. Đối với sự đánh đổi FLOPs-độ chính xác, việc so sánh tương tự một lần nữa khó thực hiện, nhưng STR dường như hoạt động tốt hơn, do độ thưa nhỏ hơn mà MobileNet được cắt tỉa, so với ResNet-50. MT cũng có chi phí giảm FLOPs ít hơn, nhưng nó hữu ích đặc biệt cho các ứng dụng quan trọng về độ chính xác nơi việc giảm kích thước của mạng vẫn quan trọng. Tất cả những phát hiện này chỉ ra rõ ràng rằng Global MP là một thuật toán cắt tỉa đơn giản nhưng cạnh tranh. Nó mang lại hiệu suất hàng đầu về sự đánh đổi độ thưa-độ chính xác, và xếp thứ hai về sự đánh đổi FLOPs-độ chính xác, mặc dù không có bất kỳ lựa chọn thiết kế phức tạp hoặc siêu tham số bổ sung nào.

B. Cắt tỉa có cấu trúc và tổng quát hóa cho các miền khác và kiến trúc RNN

Chúng tôi thí nghiệm với Global MP trên các miền khác và mạng không tích chập để đo khả năng tổng quát hóa của thuật toán trên các miền và loại mạng khác nhau. Chúng tôi thí nghiệm trên mô hình FastGRNN [59] trên tập dữ liệu Nhận dạng Hoạt động Con người HAR-2 [60]. Tập dữ liệu HAR-2 là phiên bản nhị phân của tập dữ liệu Nhận dạng Hoạt động Con người 6 lớp. Từ mô hình full-rank với rW = 9 và rU = 80 như được gợi ý trong bài báo STR [10], chúng tôi áp dụng Global MP trên các ma trận W1 và W2. Để làm điều này, chúng tôi tìm mặt nạ trọng số bằng cách xếp hạng các cột của W1 và W2 dựa trên tổng tuyệt đối của chúng, sau đó chúng tôi cắt tỉa 9 - rnewW cột thấp nhất và 80 - rnewU cột thấp nhất từ W1 và W2 tương ứng. Cuối cùng, chúng tôi tinh chỉnh mô hình đã cắt tỉa này bằng cách huấn luyện lại nó với trình huấn luyện của FastGRNN và áp dụng mặt nạ trọng số ở mỗi epoch. Chúng tôi kiểm tra Global MP dưới các cấu hình rw-rv khác nhau. Chúng tôi thấy rằng Global MP vượt qua các cơ sở khác trên tất cả các cấu hình (Bảng IV) và thành công cắt tỉa mô hình trên một kiến trúc và miền rất khác biệt.

C. Giảm thiểu sụp đổ lớp

Sụp đổ lớp là một vấn đề mà nhiều thuật toán cắt tỉa gặp phải [15], [61], [62] và xảy ra khi toàn bộ một lớp bị cắt tỉa bởi thuật toán cắt tỉa, khiến mạng không thể huấn luyện được. Chúng tôi điều tra hiện tượng này và thấy rằng hiệu suất của một thuật toán cắt tỉa có thể bị ảnh hưởng đáng kể bởi kiến trúc của mạng nơ-ron đang được cắt tỉa, đặc biệt trong miền độ thưa cao. Chúng tôi tiến hành thí nghiệm trên các mô hình MobileNet-V2 và WRN-22-8 trên tập dữ liệu CIFAR-10. Chúng tôi báo cáo kết quả trung bình qua nhiều lần chạy trong đó mỗi lần chạy sử dụng một mô hình đã được huấn luyện trước khác nhau để cung cấp sự mạnh mẽ hơn. Đầu tiên chúng tôi cắt tỉa mô hình WRN-22-8 đến độ thưa 99,9%. Chúng tôi thấy rằng ở độ thưa 99,9%, WRN vẫn có thể đạt được độ chính xác tốt (Bảng V). Sau đó chúng tôi cắt tỉa mô hình MobileNet-V2 đến độ thưa 98%. Tuy nhiên, đối với MobileNet, độ chính xác giảm xuống 10% chỉ sử dụng Global MP, và mô hình không thể học được (Bảng VI).

Lý do cho sự khác biệt rộng lớn này trong hành vi học nằm ở các kết nối tắt [57]. Cả WRN-22-8 và MobileNet-V2 đều sử dụng các kết nối tắt, tuy nhiên, vị trí của chúng khác nhau. Tham khảo Hình 2, WRN sử dụng các kết nối tắt đồng nhất từ Lớp 20 đến Lớp 23. Loại kết nối tắt này là các ánh xạ đồng nhất đơn giản và không yêu cầu bất kỳ tham số bổ sung nào, và do đó, chúng không được tính vào các trọng số. Tuy nhiên, MobileNet-V2 sử dụng ánh xạ tắt tích chập từ Lớp 52 đến Lớp 57. Các trọng số trong ánh xạ này được tính vào trọng số của mô hình, và do đó, chúng có thể cắt tỉa được. Global MP hoàn toàn cắt tỉa hai lớp trước lớp cuối cùng. Tuy nhiên, vì WRN sử dụng các ánh xạ đồng nhất, nó vẫn có thể truyền thông tin đến lớp cuối cùng, và mô hình vẫn có thể học, trong khi MobileNet-V2 gặp phải sự giảm độ chính xác thảm khốc do sụp đổ lớp. Các thuật toán cắt tỉa có thể dễ bị ảnh hưởng bởi các vấn đề sụp đổ lớp thảm khốc như vậy đặc biệt trong miền độ thưa cao. Quy tắc MT có thể giúp khắc phục vấn đề này. Giữ lại MT nhỏ 0,02% là đủ cho MobileNet-V2 để tránh sụp đổ lớp và học thành công. Do đó, giữ lại một lượng nhỏ trọng số có thể giúp ích cho động lực học của các mô hình trong cài đặt độ thưa cao.

V. THẢO LUẬN, HẠN CHẾ VÀ NGHIÊN CỨU TƯƠNG LAI

Các quan sát của chúng tôi chỉ ra rằng Global MP hoạt động rất tốt và đạt được hiệu suất vượt trội trên tất cả các tập dữ liệu và kiến trúc được kiểm tra. Nó có thể hoạt động như một thuật toán cắt tỉa một lần hoặc như một thuật toán cắt tỉa dần dần. Nó cũng vượt qua các thuật toán SOTA trên ResNet-50 trên ImageNet về sự đánh đổi độ thưa-độ chính xác và thiết lập kết quả SOTA mới trên nhiều mức độ thưa. Đối với sự đánh đổi FLOPs-độ chính xác, nó đứng thứ hai sau STR, vượt qua nhiều kỹ thuật SOTA. Đồng thời, Global MP có độ phức tạp thuật toán rất thấp và có thể nói là một trong những thuật toán cắt tỉa đơn giản nhất. Nó đơn giản hơn nhiều thuật toán cắt tỉa khác như chính quy hóa dựa trên hàm mất mát tùy chỉnh, quy trình dựa trên RL, tỷ lệ cắt tỉa theo lớp dựa trên kinh nghiệm, v.v. Nó chỉ xếp hạng trọng số dựa trên độ lớn của chúng và loại bỏ những trọng số nhỏ nhất. Điều này đặt ra một câu hỏi quan trọng về việc liệu độ phức tạp có thực sự cần thiết để cắt tỉa hay không và theo kết quả của chúng tôi, có vẻ như độ phức tạp tự nó không đảm bảo hiệu suất tốt. Do đó, các nhà thực hành phát triển thuật toán cắt tỉa mới nên xem xét cẩn thận liệu độ phức tạp có thêm giá trị vào thuật toán của họ hay không và vượt qua các cơ sở như Global MP.

Một hạn chế của Global MP là nền tảng lý thuyết cho nó chưa được thiết lập tốt. Theo nghiên cứu thực nghiệm của chúng tôi trong bản thảo này, chúng tôi dự định tiến hành phân tích lý thuyết để hiểu rõ hơn về động lực của Global MP trong tương lai. Nó sẽ bao gồm việc tìm liên kết phân tích giữa độ lớn của trọng số và tầm quan trọng của chúng trong một mạng, hoặc thậm chí các mối quan hệ phân tích của chúng với độ chính xác kết quả của mô hình. Một lĩnh vực khác cho nghiên cứu tương lai là tối ưu hóa cùng lúc cả trọng số và FLOPs trong quá trình cắt tỉa. Hiện tại, Global MP được sử dụng để đạt được một độ thưa tham số nhất định, và việc giảm FLOPs đến như một sản phẩm phụ. Trong tương lai, FLOPs cũng có thể được thêm vào một hàm tối ưu hóa để thưa thớt cùng lúc cả tham số và FLOPs.

VI. KẾT LUẬN

Trong nghiên cứu này, chúng tôi đặt ra câu hỏi liệu việc sử dụng các thuật toán phức tạp và đòi hỏi tính toán cao có thực sự cần thiết để đạt được kết quả cắt tỉa DNN vượt trội hay không. Điều này xuất phát từ sự gia tăng số lượng thuật toán cắt tỉa mới được đề xuất trong những năm gần đây, mỗi thuật toán với một sự cải thiện hiệu suất biên, nhưng các quy trình cắt tỉa ngày càng phức tạp. Điều này khiến một người thực hành khó lựa chọn thuật toán đúng và bộ siêu tham số cụ thể của thuật toán tốt nhất cho ứng dụng của họ. Chúng tôi so sánh các thuật toán này với một cơ sở ngây thơ, cụ thể là Global MP, không kết hợp bất kỳ quy trình phức tạp hoặc siêu tham số khó điều chỉnh nào. Mặc dù đơn giản, chúng tôi thấy rằng Global MP vượt trội hơn nhiều thuật toán cắt tỉa SOTA trên nhiều tập dữ liệu, như CIFAR-10, ImageNet và HAR-2; với các kiến trúc mạng khác nhau, như ResNet-50 và MobileNet-V1; và ở các mức độ thưa khác nhau từ 50% lên đến 99,9%. Chúng tôi cũng trình bày một vài biến thể của Global MP, tức là một lần và dần dần, cùng với một kỹ thuật mới, bổ sung, MT. Trong khi kết quả của chúng tôi phục vụ như một bằng chứng thực nghiệm rằng một thuật toán cắt tỉa ngây thơ như Global MP có thể đạt được kết quả SOTA, nó vẫn là một hướng nghiên cứu tương lai đầy hứa hẹn để làm sáng tỏ các khía cạnh lý thuyết về cách hiệu suất như vậy có thể với Global MP. Một hướng tương lai khác bao gồm việc mở rộng khả năng của Global MP, chẳng hạn như tối ưu hóa cùng lúc cả FLOPs và số lượng trọng số.

VII. LỜI CẢM ƠN

Nghiên cứu này được hỗ trợ bởi Cơ quan Khoa học, Công nghệ và Nghiên cứu (A*STAR) dưới Quỹ Lập trình AME (Dự án số: A1892b0026 và A19E3b0099). Bất kỳ ý kiến, phát hiện và kết luận hoặc khuyến nghị nào được thể hiện trong tài liệu này là của (các) tác giả và không phản ánh quan điểm của A*STAR.
