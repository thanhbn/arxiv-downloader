# 2102.03773.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2102.03773.pdf
# File size: 412854 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
SeReNe: Sensitivity based Regularization of
Neurons for Structured Sparsity in Neural Networks
Enzo Tartaglione Member, IEEE, Andrea Bragagnolo, Francesco Odierna,
Attilio Fiandrotti, Senior Member, IEEE, and Marco Grangetto, Senior Member, IEEE
Abstract ‚ÄîDeep neural networks include millions of learnable
parameters, making their deployment over resource-constrained
devices problematic. SeReNe (Sensitivity-based Regularization
of Neurons) is a method for learning sparse topologies with a
structure, exploiting neural sensitivity as a regularizer. We deÔ¨Åne
the sensitivity of a neuron as the variation of the network output
with respect to the variation of the activity of the neuron. The
lower the sensitivity of a neuron, the less the network output is
perturbed if the neuron output changes. By including the neuron
sensitivity in the cost function as a regularization term, we are
able to prune neurons with low sensitivity. As entire neurons
are pruned rather then single parameters, practical network
footprint reduction becomes possible. Our experimental results
on multiple network architectures and datasets yield competitive
compression ratios with respect to state-of-the-art references.
Index Terms ‚ÄîSparse networks, regularization, deep networks,
pruning, compression.
I. I NTRODUCTION
DEEP Neural Networks (DNNs) can solve extremely chal-
lenging tasks thanks to complex stacks of (convolutional)
layers with thousands of neurons [1]‚Äì[3]. Let us deÔ¨Åne here the
complexity of a neural network as the number of its learnable
parameters: architectures such as AlexNet and VGG have a
complexity in the order of 60 and 130 million parameters
respectively. Similar architectures are challenging to deploy
in scenarios where resources such as the memory or storage
are limited. For example, the 8-layers AlexNet [1] memory
footprint exceeds 240MB of memory, whereas the 19-layers
VGG-Net [2] footprint exceeds 500 MB. The need for compact
DNNs is witnessed also by the fact that the Moving Pictures
Experts Group (MPEG) of ISO has recently broadened its
scope beyond multimedia contents issuing an exploratory
call for proposal to compress neural networks [4]. Multiple
(complementary) approaches are possible to cope with neural
networks memory requirements, inference time and energy
consumption:
Re-designing the network topology. Moving from one ar-
chitecture to another, possibly forcing a precise neuronal
connectivity, or weight sharing, can reduce the number
of parameters, or the complexity of the network [3], [5].
Quantization. Representing the parameters (and activa-
tion functions) as Ô¨Åxed-point digits reduces the memory
footprint and speeds up computations [6].
E. Tartaglione, A. Bragagnolo, F. Odierna, A. Fiandrotti and M. Grangetto
are with the Computer Science Department, University of Turin, Torino,
ITALY , e-mail: Ô¨Årst.last@unito.it
A. Fiandrotti is also with LTCI, T ¬¥el¬¥ecom Paris, Institut Polytechnique de
Paris, FRANCE, e-mail:attilio.Ô¨Åandrotti@telecom-paris.frPruning. Deep architectures need to be over-
parametrized [7] to be trained effectively, but redundant
parameters can be pruned at inference time [8]‚Äì[12].
The present work falls in this latter category.
Pruning techniques aim at learning sparse topologies by
selectively dropping synapses between neurons (or neurons
altogheter when all incoming synapses are dropped). For
example, [10] and [11] apply a regularization function pro-
moting low magnitude weights followed by zero thresholding
or quantization. Such approaches slash the number of non-zero
parameters, allowing to represent the parameters of a layer as
a sparse tensor [13]. Such methods aim however at pruning
parameters independently, so the learned topologies lacks a
structure despite sparse. Storing and accessing in memory a
randomly sparse matrix entails signiÔ¨Åcant challenges, so it is
unclear to which extent such methods could be practically
exploited.
This work proposes SeReNe, a method for learning sparse
network topologies with a structure, i.e. with fewer neurons
altogether. In a nutshell, our method drives allthe parameters
of a neuron towards zero, allowing to prune entire neurons
from the network.
First, we introduce the notion sensitivity of a neuron as the
variation of the network output with respect to the neuron
activity. The latter is measured as the post-synaptic potential
of the neuron, i.e. the input to the neuron‚Äôs activation function.
The underlying intuition is that neurons with low sensitivity
yield little variation in the network output and thus negligible
performance loss if their output changes locally. We also
provide computationally efÔ¨Åcient bounds to approximate the
sensitivity.
Second, we design a regularizer that shrinks allparameters
of low sensitivity neurons, paving the way to their removal.
Indeed, when the sensitivity of a neuron approaches zero, the
neuron no longer emits signals and is ready to be pruned.
Third, we propose an iterative two-steps procedure to prune
parameters belonging to low sensitivity neurons. Through a
cross-validation strategy, we ensure controlled (or even no)
performance loss with respect to the original architecture.
Our method allows to learn network topologies which are
not only sparse, i.e. with few non-zero parameters, but with
fewer neurons (fewer Ô¨Ålters for convolutional layers). As a
side beneÔ¨Åt, smaller and denser architectures may also speedup
network execution thanks to a better use of cache locality and
memory access pattern.
We experimentally show that SeReNe outperforms state-of-arXiv:2102.03773v1  [cs.LG]  7 Feb 2021

--- PAGE 2 ---
2
the-art references over multiple learning tasks and network
architectures. We observe the beneÔ¨Åt of structured sparsity
when storing the neural network topology and parameters
using the Open Neural Network eXchange format [14], with a
reduction of the memory footprint.
The rest of the paper is structured as follows. In Sec. II
we review the relevant literature in neural network pruning. In
Sec. III we provide the deÔ¨Ånition of sensitivity and practical
bounds for its computation; then, we present a parameter
update rule to ‚Äúdrive‚Äù the parameters of low-sensitivity neu-
rons towards zero. Follows, in Sec. IV, a practical procedure
to prune a network with our scheme. Then, in Sec. V all
the empirical results are shown and Ô¨Ånally, in Sec. VI, the
conclusions are drawn.
II. R ELATED WORK
Approaches towards compact neural networks represen-
tations can be categorized in three major groups: altering
the network structure, quantizing the parameters and pruning
weights. In this section, we review works based on a pruning
approach that are most relevant to our work.
In their seminal paper [8], LeCun et al. proposed to remove
unimportant weights from a network, measuring the impor-
tance of each single weight as the increment on the train error
when the weight is set to zero. Unfortunately, the complexity
of such method would becomes computationally unbearable in
the case of deep topologies with millions of parameters. Due to
the scale and the resources required to train and deploy modern
deep neural networks, sparse architectures and compression
techniques have gained much interest in the deep learning
community. Several successful approaches to this problem
have been proposed [15]‚Äì[18]. While a more in depth analysis
on the topic has been published by Gale et al. [19], in the rest
of this section we provide a summary of the main techniques
used to prune deep architectures.
Evolutionary algorithms. Multi-objective sparse feature
learning has been proposed by Gong et al. [20]: with their
evolutionary algorithm, they were able to Ô¨Ånd a good compro-
mise between sparsity and learning error, at the cost, however,
of high computational cost. Similar drawbacks can be found in
the work by Lin et al. , where convolutional layers are pruned
using the artiÔ¨Åcial bee colony optimization algorithm (dubbed
as ABCPruner) [21].
Dropout. Dropout aims at preventing a network from over-
Ô¨Åtting by randomly dropping some neurons at learning
time [22]. Despite dropout tackles a different problem, it
has inspired some techniques aiming at sparsifying deep
architectures. Kingma et al. [9] have shown that dropout can be
seen as a special case of Bayesian regularization. Furthermore,
they derive a variational method that allows to use dropout
rates adaptively to the data. Molchanov et al. [23] exploited
such variational dropout to sparsify both fully-connected and
convolutional layers. In particular, the parameters having high
dropout rate are always ignored and they can be removed from
the network. Even if this technique obtains good performance,
it is quite complex and it is reported to behave inconsistently
when applied to deep architectures [19]. Furthermore, thistechnique relies on the belief that the Bernoulli probability
distribution (to be used with the dropout) is a good varia-
tional approximation for the posterior. Another dropout-based
approach is Targeted Dropout [24]: here, Ô¨Åne-tuning the ANN
model is self-reinforcing its sparsity by stochastically drop-
ping connections. They also target structured sparsity without,
however, reaching state-of-the-art performance.
Knowledge distillation. Recently, knowledge distillation [25]
received signiÔ¨Åcant attention. The goal in this case is to train a
single network to have the same behavior (in terms of outputs
under certain inputs) as an ensemble of models, reducing the
overall computational complexity. Distillation Ô¨Ånds application
in reducing the prediction of multiple networks into a single
one, but can not be applied to minimize the number of neurons
for a single network. A recent work is Few Samples Knowledge
Distillation (FSKD) [26], where a small student network is
trained from a larger teacher. In general, in distillation-based
techniques, the architecture to be trained is a-priori known,
and kept static through all the learning process: in this work,
we aim at providing an algorithm which automatically shrinks
the deep model‚Äôs size with minimal overhead introduced.
Few-shot pruning. Another approach relies on deÔ¨Åning
the importance of each connection and later remove pa-
rameters deemed unnecessary. A recent work by Fran-
kle and Carbin [12] proposed the lottery ticket hypothesis ,
which is having a large impact on the research community.
They claim that from an ANN, early in the training, it is
possible to extract a sparse sub-network, using a one-shot or
iterative fashion: such sparse network, when re-trained, can
match the accuracy of the original model. This technique has
multiple requirements, like having the history of the training
process in order to detect the ‚Äúlottery winning parameters‚Äù,
and it is not able to self-tune an automatic thresholding mech-
anism. Lots of efforts are devoted towards making pruning
mechanisms more efÔ¨Åcient: for example, Wang et al. show
that some sparsity is achievable pruning weights at the very
beginning of the training process [27], or Lee et al., with their
‚ÄúSNIP‚Äù, are able to prune weights in a one-shot fashion [28].
However, these approaches achieve limited sparsity: iterative
pruning-based strategy, when compared to one-shot or few-
shot approaches, are able to achieve a higher sparsity [29].
Regularization-based pruning. Finally, regularization-based
approaches rely on a regularization term (designed to enhance
sparsity) to be minimized besides the loss function at training
time. Louizos et al. propose an`0regularization to prune the
network parameters during training [30]. Such a technique
penalizes non-zero value of a parameter vector, promoting
sparse solutions. As a drawback, it requires solving a complex
optimization problem, besides the loss minimization strategy
and other regularization terms. Han et al. propose a multi-step
process in which the least relevant parameters are deÔ¨Åned,
minimizing a target loss function [11]. In particular, it relies
on a thresholding heuristics, where all the less important con-
nections are pruned. In [10], a similar approach was followed,
introducing a novel regularization term that measures the
‚Äúsensitivity‚Äù of the output wrt. the variation of the parameters.
While this technique achieves top-notch sparsity even in deep
convolutional architectures, such sparsity is not structured, i.e.

--- PAGE 3 ---
3
Fig. 1: Representation of the neuron xn;iwith activation
functiongn;i.
the resulting topology includes large numbers of neurons with
at least one non-zero parameter. Such unstructured sparsity
bloats the practically attainable network footprint and leads to
irregular memory accesses, jeopardizing execution speedups.
In this work we aim at overcoming the above limitations
proposing a regularization method that produces a structured
sparsiÔ¨Åcation, focusing on removing entire neurons instead
of single parameters. We also leverage our recent research
showing that post-synaptic potential regularization is able to
boost generalization over other regularizers [31].
III. S ENSITIVITY -BASED REGULARIZATION FOR NEURONS
In this section, we Ô¨Årst formulate the sensitivity of a network
with respect to the post-synaptic potential of a neuron. Then,
we derive a general parameter update rule which relies on the
proposed sensitivity term. As reference scenario, a multi-class
classiÔ¨Åcation problem with Clabels is considered; however,
our strategy can be extended to other learning tasks, e.g.
regression, in a straightforward way.
A. Preliminaries and DeÔ¨Ånitions
Let a feed-forward, a-cyclic, multi-layer artiÔ¨Åcial neural
network be composed of N 1hidden layers. We identify
withn= 0 the input layer and n=Nthe output layer, other
nvalues indicate the hidden layers. For the i-th neuron of the
n-th layerxn;i, we deÔ¨Åne:
yn;ias its output,
yn 1as its input vector,
n;ias its own parameters: wn;ithe weights and bn;ithe
bias,
as illustrated in Fig. 1. Each neuron has its own activation
functiongn;i()to be applied after some afÔ¨Åne function fn;i()
which can be for example convolution or dot product.
Hence, the output of a neuron is given by
yn;i=gn;i[pn;i]; (1)
wherepn;iis the post-synaptic potential ofxn;ideÔ¨Åned as:
pn;i=fn;i(n;i;yn 1): (2)B. Neuron Sensitivity
Here we introduce the deÔ¨Ånition of neuron sensitivity. We
recall that we aim at pruning entire neurons rather than
single parameters to achieve structured sparsity. Let us we
assume that our method is applied to a pre-trained network. To
estimate the relevance of neuron xn;ifor the task upon which
the network was trained, we evaluate the neuron contribution
to the network output yN. To this end, we Ô¨Årst provide an
intuition on how small variations of the post-synaptic potential
pn;iof neuronxn;iaffect thek-th output of the network yN;k.
By a Taylor series expansion, for small variations of pn;i, let
us express the variation of yN;kas
yN;kpn;i@yN;k
@pn;i(3)
whereyN;kindicates the k-th output for the output layer. In the
case yN;k!0;8k, for small variations of pn;i,yN;kdoes
not change. Such condition allows to drive the post-synaptic
potentialpn;ito zero without affecting the network output yN;k
(and, for instance, its performance). Otherwise, if yN;k6= 0,
any variation of pn;imight alter the network output, possibly
impairing its performance.
We can now properly quantify the effect of small changes
to the network output by deÔ¨Åning the neuron sensitivity .
DeÔ¨Ånition 1: The sensitivity of the network output yNwith
respect to the post-synaptic potential pn;iof neuronxn;iis:
Sn;i(yN;pn;i) =1
CCX
k=1@yN;k
@pn;i(4)
whereyN2RCandSn;i2[0; +1). Intuitively, the higher
Sn;i, the higher the Ô¨Çuctuation of yNfor small variations of
pn;i.
Before moving on, we would like to clarify our choice of
leveraging the post-synaptic potential pn;irather than the neu-
ron outputyn;iin the equation above. In order to understand
our choice, we re-write (4) using the chain rule:
Sn;i(yN;pn;i) =1
CCX
k=1@yN;k
@yn;i@yn;i
@pn;i: (5)
Without loss of generality, let us assume@yN;k
@yn;i6= 0 andgn;i
corresponds to the well known ReLU activation function. Un-
der the hypothesis that pn;i<0,@yn;i
@pn;i= 0 for the considered
ReLU activation. Had we written (4) as a function of the
neuron output yn;i, the vanishing gradient@yn;i
@pn;i= 0 would
have prevented us from estimating the neuron sensitivity. The
above consideration applies beyond ReLU to any activation
function except for the identity function, for which yn;i=pn;i.
C. Bounds on Neuron Sensitivity
Here we provide two computationally-efÔ¨Åcient bounds to
the sensitivity function above that can be practically exploited.
Popular frameworks for DNN training rely on differentiation
frameworks such as autograd , for automatic variable differen-
tiation along computational graphs. Such frameworks take as
input some objective function Jand automatically compute all

--- PAGE 4 ---
4
the gradients along the computational graph. In order to get
Sn;ias an outcome from the differentiation engine, we deÔ¨Åne
Sn;i(yN;pn;i) =@J
@pn;i(6)
whereJis a proper function. In Appendix A we show that
such function turns to be:
J=1
CCX
k=1Z@yN;k
@pn;idpn;i (7)
Therefore, computing the sensitivity in (4) requires Ccalls
to the differentiation engine. In the following with some
little algebra we derive a lower and upper bound to Def. 1
that we show to be particularly useful from a computational
perspective.
Let the objective function to differentiate be
Jl=1
CCX
k=1yN;k: (8)
The automatic differentiation engine called on Slwill return
@Jl
@pn;i=1
CCX
k=1@yN;k
@pn;i(9)
According to the triangular inequality, a lower bound to the
sensitivity in (4) can be computed as
Sl
n;i=1
CCX
k=1@yN;k
@pn;i1
CCX
k=1@yN;k
@pn;i(10)
Sl
n;ican be conveniently evaluated differentiating over (8)
(and taking the absolute value) with a single call to the
differentiation engine. As shown in (10), this gives us a lower
bound estimation over the neuron sensitivity.
In order to estimate an upper bound to Sn;i, we rewrite (4) as
Sn;i=1
CCX
k=1@yN;k
@yN 1N 1Y
l=n+1@yl
@yl 1n;i@yn;k
@pn;i(11)
However,8kwe have in common the term
 n;i=N 1Y
l=n+1@yl
@yl 1n;i@yn;i
@pn;i
N 1Y
l=n+1@yl
@yl 1n;i@yn;i
@pn;i= u
n;i (12)
wheren;iis a one-hot vector selecting the i-th neuron at the
n-th layer andjj is an element-wise operator. Hence, we
rewrite (11) as
Su
n;i=1
C CX
k=1@yN;k
@yN 1!
 u
n;iSn;i: (13)
Thus, we have shown that Su
n;iis an upper bound to the sensi-
tivity in (4). Upper and lower bounds are here obtained for two
main reasons: computational efÔ¨Åciency and relaxing/tightening
conditions on the sensitivity itself. We will see in Sec. V-A
a typical population distribution of the sensitivities on a pre-
trained network, comparing (4), (10) and (13).In the following, we exploit the formulation of the the
Sensitivity function (1) and its two bounds (10), (13) to deÔ¨Åne
a parameter update rule.
D. Parameters Update Rule
Now we show how the proposed sensitivity deÔ¨Ånition can be
exploited to promote neuron sparsiÔ¨Åcation. As hinted before, if
the sensitivity Sn;iof neuronxn;iis small, i.e Sn;i!0, then
neuronxn;iyields a small contribution to the i-th network
outputyN;iand its parameters may be moved towards zero
with little perturbation to the network output. To this end, we
deÔ¨Åne the insensitivity functionSn;ias
Sn;i= maxf0;1 Sn;ig= (1 Sn;i) (1 Sn;i)(14)
where ()is the one-step function. The higher the insensitiv-
ity of neuron xn;i(i.e.,Sn;i!1or equivalently Sn;i!0),
the less the neuron affects the network output. Therefore, if
Sn;i!1, then neuron xn;icontributes little to the network
output and its parameters wn;i;j can be driven towards zero
without signiÔ¨Åcantly perturbing the network output. Using
the insensitivity deÔ¨Ånition in (14), we propose the following
update rule:
wt+1
n;i;j=wt
n;i;j @L
wt
n;i;j wt
n;i;jSn;i (15)
where
the Ô¨Årst contribution term is the classical minimization of
a loss function L, ensuring that the network still solves
the target task, e.g. classiÔ¨Åcation;
the second one represents a penalty applied to the pa-
rameterwn;i;j belonging to the neuron xn;iwhich is
proportional to the insensitivity of the output to its
variations.
Finally, since
@pn;i
@yn 1;j=wn;i;j (16)
we rewrite (15) as
wt+1
n;i;j=wt
n;i;j @L
wt
n;i;j _Sn;i;j (17)
where
_Sn;i;j="
wn;i;j sign(wn;i;j)
CCX
k=1@yN;k
@yn 1;j#
 (1 Sn;i)
(18)
A step-by-step derivation is provided in Appendix D. From
(18) we can better understand the effect of the proposed
penalty term: as expected by our discussion above, _Sn;i;j
is inversely proportional to the impact on the output for
variations of the input for the neuron xn;i.
E. Local neuron sensitivity-based regularization
We propose now an approximate formulation of the sensi-
tivity function in (4) based only on the post-synaptic potential
and output of a neuron that we will refer to as the local sen-
sitivity. Let us recall that for each neuron xn;ithe sensitivity

--- PAGE 5 ---
5
Fig. 2: High-level view of the SeReNe procedure.
provided by DeÔ¨Ånition 1 measures the overall impact of a
given neuron xn;ion the network output taking into account
all the following neurons involved in the computation.
DeÔ¨Ånition 2: Thelocal neuron sensitivity of the output yn;i
with respect to the post-synaptic potential pn;iof the neuron
xn;iis deÔ¨Åned as:
~Sn;i=@yn;i
@pn;i(19)
In the case of ReLU-activated networks, it simply reads
~Sn;i= (pn;i) (20)
Under this setting, the update rule (17) simpliÔ¨Åes to
wt+1
n;i;j=wt
n;i;j @L
wt
n;i;j wt
n;i;j( pn;i); (21)
i.e., the penalty is applied only in case the neuron stays off.
While local sensitivity is a looser approximation of (1), it is
far less complex to compute especially for ReLU-activated
neurons.
IV. T HESERENE PROCEDURE
This section introduces a practical procedure to prune
neurons from a neural network Nleveraging the sensitivity-
based regularizer introduced above. Let us assume Nhas been
preliminary trained at some task over the dataset Dachieving
performance (e.g., classiÔ¨Åcation accuracy) A. We do not put
any constraint over the actual training method, training set
or network architecture. Alg. 1 summarizes the procedure in
pseudo-code. In a nutshell, the procedure consists in iteratively
looping over the Regularization andThresholding procedures.
At the beginning of the loop, dataset Dis split into disjoint
subsetV(used for validation purposes) and U(to update the
network). At line 5, the regularization procedure (summarizedin Alg. 2) trainsNoverDaccording to (15) driving towards
zero parameters of neurons with low sensitivity. The loop
ends if the performance of the regularized network falls below
thresholdA. Otherwise, the thresholding procedure sets to zero
parameters below threshold Tand prunes neurons such that
all parameters are equal to zero. The output of the procedure
is the pruned network, i.e. with fewer neurons, N?. The
Regularization and Thresholding procedures are detailed in the
following. A graphical high-level representation of SeReNe is
also displayed in Fig. 2.
Algorithm 1 The SeReNe procedure
Input: Trained networkN, dataset D,
Target performance A, PWE, TWT
Output: Pruned networkN?
1:procedure SERENE(N;D;A;PWE;TWT )
2:N? N
3: while true do
4:U;V RANDOM SPLIT(D)
5:N REGULARIZATION (N;U;V;PWE )
6: ifPERFORMANCE (N;V)<A then
7: break
8:N? N
9:N THRESHOLDING (N;V;TWT )
returnN?
A. Regularization
This procedure takes in input a network Nand returns a
regularized network according to the update rule (15). Namely,
the procedure iteratively trains NonUand validates it on V
for multiple epochs. Let Nrrepresent the best regularized
network found at a given time according to the loss function.
For each iteration, the procedure operates as follows. First
(line 5),Nis trained for one epoch over U: the results is a
regularized network according to (15). Second (line 6), this
network is validated on V. If the loss is lower than the loss
ofNroverV, thenNtakes the place of Nr(line 7). If
Nris not updated for PWE ( Plateau Waiting Epochs ) epochs,
we assume we have reached a performance plateau. In this
case, the procedure ends and returns the sensitivity-regularized
networkNr.
Algorithm 2 The regularization procedure
Input: ModelN, data sets V and U, PWE
Output: The sensitivity-regularized network Nr
1:procedure REGULARIZATION (N;U;V;PWE )
2:Nr N.Nrisbest regularized network on V
3:epochs 0
4: whileepochs<PWE do
5:N TRAIN (N;U).1 train epoch on U
6:epochs + +
7: ifLOSS(N;V)<LOSS(Nr;V)then
8:Nr N
9: epochs 0
returnNr

--- PAGE 6 ---
6
B. Thresholding
The thresholding procedure is where the parameters of
neurons with low sensitivity are thresholded to zero. Namely,
parameters whose absolute value is below threshold Tare
pruned as
wn;i;j=
wn;i;jjwn;i;jj>T
0otherwise:(22)
The pruning threshold Tis selected so that the performance
(or, in other words, the loss on V) worsens at most of a relative
value we call thresholding worsening tolerance (TWT ) we
provide as hyper-parameter.
We expect the loss function to be locally a smooth, monotone
function ofT, for small values of T. The threshold Tcan be
found using linear search-based heuristics. We can however
reduce this using a bisection approach, converging to the
optimalTvalue in log-time steps.
Because of the stochasticity introduced by mini-batch based
optimizers, parameters pruned during a thresholding iteration
may be reintroduced by the following regularization iteration.
In order to overcome this effect, we enforce that pruned
parameters can no longer be updated during the following
regularizations (we term this behavior as parameter pinning ).
To this end, the update rule (15) is modiÔ¨Åed as follows:
wt+1
n;i;j=(
wt
n;i;j @L
wt
n;i;j wt
n;i;jSn;iwt
n;i;j6= 0
0 wt
n;i;j= 0
(23)
We have noticed that without parameter pinning, the com-
pression of the network may remain low because the noisy
gradient estimates in a mini-batch that keep reintroducing
previously pruned parameters. On the contrary, by adding (23)
a lower number of epochs are sufÔ¨Åcient to achieve much higher
compression.
V. R ESULTS
In this section we experiment with our proposed neuron
pruning method comparing the four sensitivity formulations
we introduced in the previous section:
SeReNe (exact) - the exact formulation in (5);
SeReNe (LB) - the lower bound in (10);
SeReNe (UB) - the upper bound in (13);
SeReNe (local) - the local version in (19);
`2+ pruning - is a baseline reference where we replace
our sensitivity-based regularization term with a standard
`2term (all the rest of the framework is identical).
We experiment over different combinations of architectures
and datasets commonly used as benchmarks in the relevant
literature:
LeNet-300 on MNIST (Table I and Table II),
LeNet-5 on MNIST (Table III),
LeNet-5 on Fashion-MNIST (Table IV),
VGG-16 on CIFAR-10 (Table V and Table VI),
ResNet-32 on CIFAR-10 (Table VII),
AlexNet on CIFAR-100 (Table VIII),
ResNet-101 on ImageNet (Table IX).
Fig. 3: Population of sensitivities Sand relative lower Sland
upperSubounds for a LeNet-5 architecture pre-trained on
MNIST. Vertical bars indicate relative mean values.
Notice that the VGG-16, AlexNet and ResNet-32 architectures
are modiÔ¨Åed to Ô¨Åt the target classiÔ¨Åcation task (CIFAR-10 and
CIFAR-100). The validation set ( V) size for all experiments
is10% of the training set.
The pruning performance is evaluated according to multiple
metrics.
The compression ratio as the ratio between the number
of parameters in the original network and the number of
remaining parameters after pruning (the higher the better).
The number of remaining neurons (or Ô¨Ålters for convo-
lutional layers) after pruning.
The size of the networks when stored on disk in the popu-
lar ONNX format [14] ( .onnx column). ONNX Ô¨Åles are
then lossless compressed using the Lempel‚ÄìZiv‚ÄìMarkov
algorithm (LZMA) [34] ( .7z column).
In our experiments, we compare with all available references
for each combination of architecture and dataset. For this rea-
son, the reference set may vary from experiment to experiment.
Our algorithms are implemented in Python, using PyTorch 1.5,
and simulations are run on a RTX2080 NVIDIA GPU with
8GB of memory.1
A. Preliminary experiment
To start with, we plot the sensitivity distribution for a
LeNet-5 network trained on the MNIST dataset (SGD with
learning rate = 0:1weight-decay 10 4). This network will
also be used as baseline in Sec. V-C. Fig. 3 shows SeReNe
(exact) (red), SeReNe (LB) (green) and SeReNe (UB) (blue);
the vertical mars represent the mean values. As expected,
SeReNe (LB) and SeReNe (UB) under estimate and over
estimate SeReNe (exact), respectively. Interestingly, SeReNe
(UB) sensitivity values lie in the range [10 4; 10 2]while
both for SeReNe (exact) and SeReNE (LB) show a longer trail
towards smaller Ô¨Ågures, whereas all distributions look similar.
1The source code will be made available upon acceptance of the article.

--- PAGE 7 ---
7
TABLE I: LeNet-300 trained on MNIST (1.65% error rate).
ApproachRemaining parameters (%) Compr. Remaining Network size [kB] Training time Top-1
FC1 FC2 FC3 ratio neurons .onnx .7z (s/epoch) (%)
Baseline 100 100 100 1x [300]-[100]-[10] 1043 ! 933 3.65 1.44
Han et al. [11] 8 9 26 12.2x - - - 1.6
Tartaglione et al. [10] 2.25 11.93 69.3 27.87x - - - 1.65
`2+pruning 2.44 15.76 68.50 23.26x [212]-[82]-[10] 723 ! 64 3.65 1.66
SeReNe (exact) 1.42 9.54 60.9 42.55x [159] -[75]-[10] 538! 46 13.25 1.64
SeReNe (UB) 22.45 60.81 87.75 3.71x [295]-[92]-[10] 1016 ! 324 5.13 1.67
SeReNe (LB) 1.51 10.05 60.53 39.79x [164]-[78]-[10] 557 ! 55 4.88 1.65
SeReNe (local) 3.85 32.53 73.49 13.81x [251]-[86]-[10] 859 ! 119 3.83 1.64
TABLE II: LeNet-300 trained on MNIST (1.95% error rate).
ApproachRemaining parameters (%) Compr. Remaining Network size [kB] Training time Top-1
FC1 FC2 FC3 ratio neurons .onnx .7z (s/epoch) (%)
Baseline 100 100 100 1x [300]-[100]-[10] 1043 ! 933 3.65 1.44
Sparse VD [23] 1.1 2.7 38 68x - - - 1.92
SWS [32] - - - 23x - - - 1.94
Tartaglione et al. [10] 0.93 1.12 5.9 103x - - - 1.95
DNS [33] 1.8 1.8 5.5 56x - - - 1.99
`2+pruning 1.22 8.77 61.10 41.95x [167]-[76]-[10] 566 ! 42 3.65 1.97
SeReNe (exact) 0.76 5.85 49.77 66.28x [148]- [70]-[10] 498 ! 38 13.25 1.93
SeReNe (UB) 13.67 50.76 84.47 5.47x [293]-[91]-[10] 1008 ! 240 5.13 1.95
SeReNe (LB) 0.75 5.79 49.3 66.41x [146] -[70]-[10] 492! 37 4.88 1.95
SeReNe (local) 1.7 19.94 63.59 25.07x [192]-[83]-[10] 656 ! 70 3.83 1.93
In the following, we will experimentally evaluate the three
sensitivity formulations in terms of pruning effectiveness.
B. LeNet300 on MNIST
As a Ô¨Årst experiment, we prune a LeNet-300 architecture,
which consists of three fully-connected layers with 300, 100
and 10 neurons, respectively trained over the MNIST dataset.
We pre-trained LeNet-300 via SGD with learning rate = 0:1
0 50 100 150 200 250 300
Neuron0
100
200
300
400
500
600
700Parameter
Fig. 4: Parameters distribution in FC1 of LeNet-300 trained on
MNIST from Han et al. [11] (top) and he proposed SeReNe
(bottom). In black the remaining parameters.andPWE = 20 epochs with = 10 5,TWT = 0:3for
SeReNe (exact), SeReNe (LB) SeReNe (UB) and = 10 5,
TWT = 1 for SeReNe (local). The related literature reports
mainly i) results for classiÔ¨Åcation errors around 1:65% (Ta-
ble I) and ii) results for errors in the order of 1:95% (Table II).
For this reason, we trained for about 1k epochs to achieve
1:95% error rate and for additional 2k epochs to score a 1:65%
error rate.
SeReNe outperforms the other methods leads both in terms
of compression ratio and number of pruned neurons. SeReNe
(exact) achieves a compression ratio of 42.55 and the number
of remaining neurons in the hidden layers drops from 300
to 159 and from 100 to 75 respectively. SeReNe (LB)
enjoys comparable performance with respect to SeReNe (ex-
act) despite lower computational cost (see below). For the
Àô95% error band, SeReNe (LB) performs is more effective at
pruning parameters than SeReNe (exact), allowing lower error.
Serene (LB) prunes more parameters than SeReNe (UB), we
hypothesize because (13) overstimates the sensitivity of the
parameters and prevents them to be pruned. On the other
side, SeReNe (LB) underestimates the sensitivity, however
smallvalues sets this off. SeReNe (local) prunes less
parameters than the other SeReNe formulations as it relies
on a locally computed sensitivity formulation despite lower
complexity. Concerning training time (second column from
the right), SeReNe (local) is fastest and introduces very little
computational overhead, SeReNe (UB) and SeReNe (LB) have
comparable training times and the slowest is the SeReNe
(exact), approximately 2.7x slower than its boundaries. In the
light of the good tradeoff between ability to prune neurons,

--- PAGE 8 ---
8
TABLE III: LeNet-5 trained on MNIST.
ApproachRemaining parameters (%) Compr.NeuronsNetwork size [kB] Top-1
Conv1 Conv2 FC1 FC2 ratio .onnx .7z (%)
Baseline 100 100 100 100 1x [20]-[50]-[500]-[10] 1686 ! 1510 0.68
Sparse VD [23] 33 2 0.2 5 280x - - 0.75
Han et al. [11] 66 12 8 19 11.9x - - 0.77
SWS [32] - - - - 162x - - 0.97
Tartaglione et al. [10] 67.6 11.8 0.9 31.0 51.1x [20]-[48]-[344]-[10] - 0.78
DNS [33] 14 3 0.7 4 111x - - 0.91
`2+pruning 60.20 7.37 0.61 22.14 72.3 [19]-[37]-[214]-[10] 577 ! 46 0.8
SeReNe (LB) 33.75 3.25 0.27 10.22 177.05x [11]-[26]-[113]-[10] 208 ! 19 0.8
TABLE IV: LeNet-5 trained on Fashion-MNIST.
ApproachRemaining parameters (%) Compr.NeuronsNetwork size [kB] Top-1
Conv1 Conv2 FC1 FC2 ratio .onnx .7z (%)
Baseline 100 100 100 100 1x [20]-[50]-[500]-[10] 1686 ! 1510 8.1
Tartaglione et al. [10] 76.2 32.56 6.5 44.02 11.74x [20]- [47]-[470]-[10] - 8.5
`2+pruning 85.80 34.13 4.57 55.24 14.36x [20]-[50]-[500]-[10] 1496 ! 197 8.44
SeReNe (LB) 85.71 32.14 3.63 52.03 17.04x [20]-[49]- [449] -[10] 1494! 46 8.47
error rate and training time of SeReNe (LB), in the following
we will restrict our experiments to this sensitivity formulation.
Fig. 4 (bottom) shows the location of the parameters not
pruned by SeReNe (exact) in LeNet300 Ô¨Årst fully-connected
layer (black dots). For comparison, we report the equivalent
image from Fig. 4 of [11] (top). Our method yields completely
blank columns in the matrix that can be represented in memory
as uninterrupted sequences of zeroes. When stored on disk,
LZMA compression (.7z column) is particularly effective at
encoding long sequences of the same symbol, which explains
the 10x compression rate it achieves (from 538 to 46 kB) over
the .onnx Ô¨Åle.
Finally, we perform an ablation study to assess the impact
of a simpler `2-only regularization, i.e. classical weight decay,
in place of our sensitivity-based regularizer. Towards this end,
we retrain LeNet-300 with = 0 and a weight-decay set
to10 4in its place (line `2+pruning in the tables above).
We point out in (15) that the sensitivity can be interpreted
as a weighting factor for the `2-regularization. Using weight-
decay is equivalent to assuming all the parameters have the
same sensitivity. For this experiment, we used = 0:1,
PWE = 5 andTWT = 0 (TWT > 0signiÔ¨Åcantly and
uncontrollably worsens the performance). Table I shows that
such method is less effective at pruning neurons than SeReNe
(LB), which removes 15% more neurons. Similar conclusions
can be drawn also if higher error is tolerated, as in Table II.
The`2+pruning has been performed for comparison in all
following experiments in the paper yielding the same results.
C. LeNet5 on MNIST
Next, we repeat the previous experiment over the LeNet-
5 [35] architecture, preliminarily trained as for the LeNet-
300 above, yet with SGD with learning rate = 0:1and
PWE = 20 epochs. We experiment with SeReNe (LB) with
parameters ( = 10 4,TWT = 1:45). For this architecture,
our method requires about 500 epochs to achieve the sameerror range as other state of the art references. According to
Table III, SeReNe (LB) approaches the classiÔ¨Åcation accuracy
of its competitors outperforms the considered references in
terms of compression ratio and pruned neurons.
In this case, the beneÔ¨Åts coming from the structured spar-
sity are evident: the uncompressed network storage footprint
decreases from 1686 kB to 208 kB (-90%), which after
lossless compression further decreases to 19 kB with a 0.12%
performance drop only.
D. LeNet5 on Fashion-MNIST
Then, we experiment with the same LeNet-5 architecture
on the Fashion-MNIST [36] dataset. Fashion-MNIST has the
same size of the MNIST dataset, yet it contains natural images
of dresses, shoes, etc. and so it is harder to classify than
MNIST since the images are not sparse as MNIST digits. In
this experiment we used SGD with learning rate = 0:1and
PWE = 20 epochs. For SeReNe (LB) we used = 10 5
andTWT = 1 for about 2k epochs.
Unsurprisingly, the average compression ratio is lower than for
MNIST: since the classiÔ¨Åcation problem is much harder than
MNIST (Sec. V-C), more complexity is required and SeReNe,
in order not to degrade the Top-1 performance, is not pruning
as much as it did for the MNIST experiment. Most importantly,
the SeReNe (LB) compressed network is 46 kB only, despite
the higher number of pruned parameters.
E. VGG on CIFAR-10.
Next, we experiment with two popular implementations of
the VGG architecture [2]. We recall that VGG consists in
13 convolutional layers arranged in 5 groups of, respectively,
2, 2, 3, 3, 3 layers, with 64, 128, 256, 512, 512 Ô¨Ålters
per layer respectively. VGG-1 is a VGG implementation
popular in CIFAR-10 experiments that includes only one
fully-connected layer as output layer and is pre-trained on

--- PAGE 9 ---
9
TABLE V: VGG-like architecture with 1 fully connected layer ( VGG-1 ) trained on CIFAR-10.
ApproachRemaining parameters (%) [neurons] Compr. Network size [MB] Top-1
Conv1 Conv2 Conv3 Conv4 Conv5 FC1 ratio .onnx .7z (%)
Baseline 100 100 100 100 100 - 1x 57.57 ! 51.51 7.36
[64] [128] [256] [512] [512] [10]
[64] [128] [256] [512] [512]
[256] [512] [512]
`2+pruning 11.86 15.07 6.59 0.36 0.11 66.70 88.84x 13.58 ! 1.14 7.79
[23] [126] [250] [406] [60] [10]
[64] [123] [251] [108] [81]
[250] [128] [398]
SeReNe (LB) 10.18 11.68 4.73 0.20 0.05 61.11 124.82x 11.56 ! 0.97 7.8
[23] [126] [250] [382] [65] [10]
[64] [123] [251] [93] [76]
[250] [136] [373]
TABLE VI: VGG-like architecture with 2 fully connected layers ( VGG-2 ) trained on CIFAR-10.
ApproachRemaining parameters (%) [neurons] Compr. Network size [MB] Top-1
Conv1 Conv2 Conv3 Conv4 Conv5 FC1 FC2 ratio .onnx .7z (%)
Baseline 100 100 100 100 100 100 100 1x 58.61 ! 52.44 6.16
[64] [128] [256] [512] [512] [512] [10]
[64] [128] [256] [512] [512]
[256] [512] [512]
Sparse-VD [23] - - - - - - - 48x - 7.3
`2+pruning 27.62 30.74 13.67 0.88 0.24 1.88 70.78 40.96x 34.42 ! 2.86 7.21
[44] [126] [247] [498] [409] [367] [10]
[60] [120] [247] [463] [417]
[243] [79] [461]
SeReNe (LB) 25.9 26.38 9.75 0.48 0.15 1.24 70 57.99x 29.41 ! 2.47 7.25
[44] [126] [247] [498] [354] [367] [10]
[60] [120] [247] [433] [366]
[243] [65] [459]
TABLE VII: ResNet-32 trained on CIFAR-10.
ApproachRemaining parameters (%) [neurons] Compr. Network size [MB] Top-1
Conv1 Block1 Block2 Block3 FC1 ratio .onnx .7z (%)
Baseline 100 100 100 100 100 1x 1.84 ! 1.63 7.36
[64] [160] [320] [640] [10]
`2+pruning 65.97 33.30 33.41 26.32 88.75 3.51x 1.82 ! 0.54 8.08
[14] [157] [319] [633] [10]
SeReNe (LB) 60.19 24.52 24.14 17.84 81.88 5.03x 0.87 ! 0.37 8.09
[12] [ 93] [ 203] [ 364] [10]
ImageNet2.VGG-2 [23] is similar to VGG-1 but includes
one hidden fully connected layer with 512 neurons before
the output layer. We experiment over the CIFAR-10 dataset,
which consists of 50k 3232, RGB images for training and
10k for testing, distributed in 10 classes. For both VGG-1
and VGG-2 we have used SGD with learning rate = 0:01
andPWE = 20 epochs. For the SeReNe (LB), we used
= 10 6andTWT = 1:5. Both architectures were pruned
for approximately 1k epochs and Tables V and VI detail the
pruned topologies. For each architecture, we detail the number
of surviving Ô¨Ålters (convolutional layers) or neurons (fully
connected layers) for each layer within square brackets. The
2https://github.com/kuangliu/pytorch-cifartables show that SeReNe introduces a signiÔ¨Åcantly structured
sparsity for both VGG-1 and VGG-2 and outperforms Sparse-
VD [23] in terms of compression ratio. We are able to prune
a signiÔ¨Åcant number of Ô¨Ålters also in the convolutional layers;
as an example, the 3 layers in block Conv4 are reduced to
[382]-[93]-[136] for VGG-1 and [498]-[433]-[65] for VGG-2.
That has a positive impact on the networks footprint. VGG-
1 memory footprint drops from 57.57 MB to 11.56 MB for
the pruned network, while the 7zip compressed representation
is 0.97 MB only.For VGG-2, the memory foot print drops
from 58.61 MB to 29.41 MB, while the compressed Ô¨Åle
representation amounts to 2.47 MB.

--- PAGE 10 ---
10
TABLE VIII: AlexNet trained on CIFAR-100.
ApproachRemaining parameters (%) [neurons] Compr. Network size [MB] Top-1 Top-5
Conv1 Conv2 Conv3 Conv4 Conv5 FC1 FC2 FC3 ratio .onnx .7z (%) (%)
Baseline 100 100 100 100 100 100 100 100 1x 92.31 ! 79.27 45.58 20.09
[64] [192] [384] [256] [256] [4096] [4096] [100]
`2+pruning 75.00 21.95 5.21 3.65 5.59 0.62 0.17 6.44 114.45x 60.88 ! 3.56 46.43 19.91
[64] [192] [384] [256] [256] [4094] [2180] [100]
SeReNe (LB) 79.05 20.33 5.72 3.33 2.23 0.18 0.04 2.77 179.52x 43.80 ! 2.47 44.99 17.88
[64] [ 191] [384] [256] [256] [ 3322 ] [ 1310 ] [100]
TABLE IX: ResNet-101 trained on ImageNet.
ApproachRemaining parameters (%) [neurons] Compr. Network size [kB] Top-1 Top-5
Conv1 Block1 Block2 Block3 Block4 FC1 ratio .onnx .7z (%) (%)
Baseline 100 100 100 100 100 100 1x 174.49 ! 156.67 22.63 6.44
[64] [1408] [3584] [36352] [11264] [1000]
`2+pruning 53.12 25.42 25.57 13.71 17.74 51.94 5.75x 172.94 ! 32.93 28.33 9.18
[49] [1241] [3280] [33278] [11250] [1000]
SeReNe (LB) 55.36 24.27 23.79 11.24 14.81 40.82 6.94 x 172.15! 27.84 28.41 9.45
[49] [ 1197 ] [ 3142 ] [ 31948 ] [ 11249 ] [1000]
F . ResNet-32 on CIFAR-10
We then evaluate SeReNe over the ResNet-32 architec-
ture [3] trained on the CIFAR-10 dataset using SGD with
learning rate = 0:001, momentum 0:9,= 10 5,TWT =
0andPWE = 10 . Table VII shows the resulting architecture.
Due to the number of layers, we represent the network archi-
tecture in Ô¨Åve different blocks: the Ô¨Årst correspond to the Ô¨Årst
convolutional layer that takes in input the original input image,
the last represent the fully-connected output layer. The other
three blocks in the middle represent the rest of the network,
based on the number of output channels of each layer: block1
contains all the layers with an output of 16 channels, block2
contains all the layers with an output of 32 channels and block3
collects the layers with an output of 64 channels. ResNet is an
already optimized architecture and so it is more challenging to
prune compared to, e.g, VGG. Nevertheless, SeReNe is still
able to prune about 40% of the neurons and 70 % of the
parameters over the original ResNet-32. This is reÔ¨Çected on
the size of the network, which drops from 1.84 MB (1.63 MB
compressed) to 0.87 MB (0.57MB compressed).
G. AlexNet on CIFAR-100
Next, we up-scale in the output dimensionality of the
learning problem, i.e. in the number of classes C, testing the
proposed method on an AlexNet-like network over the CIFAR-
100 dataset. Such dataset consists of 3232RGB images
divided in 100 classes (50k training images, 10k test images).
In this experiment we use SGD with learning rate = 0:1
andPWE = 20 epochs. Concerning SeReNe (LB), we used
= 10 5andTWT = 1:5and the pruning process lasted
300 epochs.
Table VIII shows compression ratios in excess of 179x,
whereas the network size drops from 92.31 MB to 43.80 MB
and further to 2.47 MB after compression.
With respect to CIFAR-10, we hypothesize that the larger
number of target classes to discriminate prevents pruningneurons in the convolutional layers, yet it allows to prune a
signiÔ¨Åcant number of neurons from the hidden fully connected
layers. Contrarily from the previous experiments, the top-5 and
the top-1 errors improve with respect to the baseline.
H. ResNet-101 on ImageNet
As a last experiment, we test SeReNe on ResNet-101 trained
over ImageNet (ILSVRC-2012), using the pre-trained network
provided by the torchvision library.3
Due to the long training time, we employed a batch-wise
heuristic such that, instead of waiting for a performance
plateau, the pruning step is taken every time a Ô¨Åfth of the train
set (around 7.9k iterations) has been processed. We trained
the network using SGD with a learning rate = 0:001 and
momentum 0:9; for SeReNe (LB) we used = 10 6and
TWT = 0.
Table IX shows the result of the pruning procedure with
the layers grouped in blocks similarly as for the ResNet-
32 experiment. Despite the complexity of the classiÔ¨Åcation
problem (1000 classes) that makes challenging pruning entire
neurons, we prune around 86% of the parameters and obtain
a network that is smaller in size, especially when compressed,
going from 156.67 MB to only 27.84 MB.
I. Experiments on mobile devices
As a last experiment, we benchmark some of the architec-
tures pruned with SeReNe on an a Huawei P20 smartphone
equipped with 4x2.36 GHz Cortex-A73 + 4x1.84GHz Cortex-
A53 processors and 4GB RAM, running Android 8.1 ‚ÄúOreo‚Äù.
Table X shows the the inference time for ResNet-32, VGG-
16 and AlexNet (all Ô¨Ågures are obtained averaging 1,000
inferences on the device). SeReNe-pruned architectures show
consistently lower inference time in the light of the fewer
3https://pytorch.org/docs/stable/torchvision/models.html

--- PAGE 11 ---
11
TABLE X: Inference measures on Huawei P20.
Architecture Approach Inference time [ms]
ResNet-32 Baseline 32:123:62
SeReNe (LB) 24:833:59
VGG-16 (VGG-1) Baseline 204:216:05
SeReNe (LB) 98:678:71
AlexNet Baseline 131:4111:04
SeReNe (LB) 75:278:70
neurons in the pruned network, with a top speedup for VGG-
16 in excess of a 2x factor. These results do not account
for strategies commonly employed to boost inference speed,
like parameters quantization or custom libraries for sparse
tensors processing. We hypothesize that such strategies, being
orthogonal to neuron pruning, would further boost inference
time.
VI. C ONCLUSIONS
In this work we have proposed a sensitivity-driven neural
regularization technique. The effect of this regularizer is to
penalize all the parameters belonging to a neuron whose
output is not inÔ¨Çuential in the output of the network. We
have learned that the evaluation of the sensitivity at the
neuron level (SeReNe) is extremely important in order to
promote a structured sparsity in the network, being able to
obtain a smaller network with minimal performance loss.
Our experiments show that the SeReNe strikes a favorable
trade-off between ability to prune neurons and computational
cost, while controlling the impairment in classiÔ¨Åcation per-
formance. For all the tested architectures and datasets, our
sensitivity-based approach proved to introduce a structured
sparsity while achieving state-of-the-art compression ratios.
Furthermore, the designed sparsifying algorithm, making use
of cross-validation, guarantees minimal (or no) performance
loss, which can be tuned by the user via an hyper-parameter
(TWT). Future work includes deployment on physical embed-
ded devices making use of deep network as well as using
a quantization-based regularization jointly with the neuron
sensitivity to further compress deep networks.
APPENDIX A
JMADE EXPLICIT
In order to compute Sgiven (5), we can proceed directly.
However, this is problematic as it requires Cdifferent calls
for the differentiation engine. Let us recall (4):
Sn;i=1
CCX
k=1@yN;k
@pn;i:
Now we inspect whether we can reduce the computation by
deÔ¨Åning an overall objective function which, when differenti-
ated, yields Sas a result. We name it J:
J=Z
Sn;idpn;i=Z1
CCX
k=1@yN;k
@pn;idpn;i=
=1
CZCX
k=1@yN;k
@pn;isign@yN;k
@pn;i
dpn;i: (24)Here we can use Fubini-Tonelli‚Äôs theorem:
J=1
CCX
k=1Z@yN;k
@pn;isign@yN;k
@pn;i
dpn;i
=1
CCX
k=1yN;ksign@yN;k
@pn;i
: (25)
Unfortunately, we have no efÔ¨Åcient way to compute
sign
@yN;k
@pn;i
and the only certain way is to compute@yN;k
@pn;i
directly,8C.
APPENDIX B
LENET300 WITH SIGMOID ACTIVATION ON MNIST
APPENDIX C
EXPLICIT DERIVATION FOR SERENE REGULARIZATION
FUNCTION
Here we focus on the update rule (15): we aim at minimizing
the overall objective function
O=L+R; (26)
where
R=Z
wn;i;jSn;idwn;i;j: (27)
Here on we will drop the subscripts n;i;j . Let us consider
the formulation of the sensitivity in (4): (27) becomes
R=Z
wS S
dw (28)
where ()is the one-step function and
S= 1 1
CCX
k=1@yN;k
@p: (29)
We can re-write (28) as
R=Z
w"
1 1
CCX
k=1@yN;k
@p#
 S
dw
=Z
w S
dw Z
w1
CCX
k=1@yN;k
@p S
dw
=w2
2 S
 Z
w1
CCX
k=1@yN;k
@p S
dw: (30)
Let us deÔ¨Åne
JR= Z
w1
CCX
k=1@yN;k
@p S
dw: (31)
Considering that wisk-independent and that1
Cis a constant,
we can write
JR= 1
CZCX
k=1w@yN;k
@p S
dw: (32)

--- PAGE 12 ---
12
TABLE XI: LeNet-300 trained on MNIST (sigmoid activation, 1.7% error rate).
ApproachRemaining parameters (%) Compr. Remaining Model size [kB] Top-1
FC1 FC2 FC3 ratio neurons .onnx .7z (%)
Baseline 100 100 100 1x [300]-[100]-[10] 1043 ! 963 1.72
`2+ pruning 46.27 82.87 97.90 1.97x [300]-[100]-[10] 1043 ! 536 1.75
SeReNe 2.44 16.73 85.30 22.31x [215] -[100]-[10] 749! 75 1.72
TABLE XII: LeNet-300 trained on MNIST (sigmoid activation, 1.95% error rate).
ApproachRemaining parameters (%) Compr. Remaining Model size [kB] Top-1
FC1 FC2 FC3 ratio neurons .onnx .7z (%)
Baseline 100 100 100 1x [300]-[100]-[10] 1043 ! 963 1.72
`2+ pruning 4.32 30.53 90.80 12.92x [290]-[100]-[10] 1008 ! 112 1.98
SeReNe 1.15 9.32 76.00 40.66x [179] -[99]-[10] 624! 45 1.95
Here we are allowed to apply Fubini-Tonelli‚Äôs theorem, swap-
ping sum and integral:
JR= 1
CCX
k=1Z
w@yN;k
@p S
dw
= 1
CCX
k=1Z
w@yN;k
@psign@yN;k
@p
 S
dw:
(33)
Now integrating by parts:
JR= 1
CCX
k=1Z
w@yN;k
@psign@yN;k
@p
 S
dw
= 1
CCX
k=1w2
2@yN;k
@psign@yN;k
@p
 S
+
+Zw2
2@
@w@yN;k
@psign@yN;k
@p
 S
dw
:
(34)
According to the derivative chain rule, we can re-write (34)
as
JR= 1
CCX
k=1w2
2@yN;k
@psign@yN;k
@p
 S
+
+Zw2
2@2yN;k
@p2@p
@wsign@yN;k
@p
 S
dw
:
(35)
Applying inÔ¨Ånite steps of integration by parts we have in the
end
JR=1
C SCX
k=1sign@yN;k
@p
 w2
2@yN;k
@p+
+1X
i=1( 1)i+1wi+2
(i+ 2)!@i+1yN;k
@pi+1@ip
@wi#
: (36)
Hence, the overall minimized Rfunction is
R= S(
w2
2+1
CCX
k=1sign@yN;k
@p
 w2
2@yN;k
@p+
+1X
i=1( 1)i+1wi+2
(i+ 2)!@i+1yN;k
@pi+1@ip
@wi#)
: (37)APPENDIX D
DERIVATION OF (17)
Let us recall the formulation in (15). According to (14), we
can re-write it as
wt+1
n;i;j=wt
n;i;j @L
wt
n;i;j wn;i;j(1 Sn;i)(1 Sn;i):(38)
Given the deÔ¨Ånition of Sn;iin (4), we can write
wt+1
n;i;j=wt
n;i;j @L
wt
n;i;j+
 wn;i;j(1 Sn;i) 
1 1
CCX
k=1@yN;k
@pn;i!
:
(39)
We can multiply the insensitivity by the term wn;i;j:
wt+1
n;i;j=wt
n;i;j @L
wt
n;i;j+
 (1 Sn;i) 
wn;i;j 1
CCX
k=1@yN;k
@pn;iwn;i;j!
:
(40)
Finally here, observing (16), we Ô¨Ånd back (17).
APPENDIX E
LENET300 WITH SIGMOID ACTIVATION ON MNIST
Finally, we repeat the experiment in Sec. V-B yet replacing
the ReLU activations with sigmoids in the hidden layers. We
optimize a pre-trained LeNet300 using SGD with learning rate
= 0:1,PWE = 20 epochs,TWT = 0 for target Top-1
errors of 1.7% (Table XI) and 1.95% (Table XII).
SeReNe achieves a sparser (and smaller) architecture than
`2+ pruning for both error rates. Interestingly, for the 1.7%
error rate,`2+ pruning is not able to prune any neuron,
whereas SeReNe prunes 85 neurons from FC1, with a 10
times higher compression ratio. This reÔ¨Çects in the compressed
model size: while `2+ pruning squeezes the architecture to
536kB, SeReNe compresses it to 75kB only.

--- PAGE 13 ---
13
REFERENCES
[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation
with deep convolutional neural networks,‚Äù in Advances in neural infor-
mation processing systems , 2012, pp. 1097‚Äì1105.
[2] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks
for large-scale image recognition,‚Äù 3rd International Conference on
Learning Representations, ICLR 2015 - Conference Track Proceedings ,
2015.
[3] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2016, pp. 770‚Äì778.
[4] T. M. P. E. Group, ‚ÄúCompression of neural networks for multimedia
content description and analysis,‚Äù MPEG 125 - Marrakesh.
[5] Y . Lu, G. Lu, R. Lin, J. Li, and D. Zhang, ‚ÄúSrgc-nets: Sparse repeated
group convolutional neural networks,‚Äù IEEE Transactions on Neural
Networks and Learning Systems , vol. 31, pp. 2889‚Äì2902, 2020.
[6] J. Wu, C. Leng, Y . Wang, Q. Hu, and J. Cheng, ‚ÄúQuantized convolutional
neural networks for mobile devices,‚Äù in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2016, pp.
4820‚Äì4828.
[7] L. Sagun, U. Evci, V . U. Guney, Y . Dauphin, and L. Bottou, ‚ÄúEmpirical
analysis of the hessian of over-parametrized neural networks,‚Äù 6th
International Conference on Learning Representations, ICLR 2018 -
Workshop Track Proceedings , 2018.
[8] Y . LeCun, J. S. Denker, and S. A. Solla, ‚ÄúOptimal brain damage,‚Äù in
Advances in neural information processing systems , 1990, pp. 598‚Äì605.
[9] D. P. Kingma, T. Salimans, and M. Welling, ‚ÄúVariational dropout and
the local reparameterization trick,‚Äù in Advances in Neural Information
Processing Systems , 2015, pp. 2575‚Äì2583.
[10] E. Tartaglione, S. Leps√∏y, A. Fiandrotti, and G. Francini, ‚ÄúLearning
sparse neural networks via sensitivity-driven regularization,‚Äù in Advances
in Neural Information Processing Systems , 2018, pp. 3878‚Äì3888.
[11] S. Han, J. Pool, J. Tran, and W. Dally, ‚ÄúLearning both weights and con-
nections for efÔ¨Åcient neural network,‚Äù in Advances in neural information
processing systems , 2015, pp. 1135‚Äì1143.
[12] J. Frankle and M. Carbin, ‚ÄúThe lottery ticket hypothesis: Finding sparse,
trainable neural networks,‚Äù 7th International Conference on Learning
Representations, ICLR 2019 , 2019.
[13] M. Naumov, L. Chien, P. Vandermersch, and U. Kapasi, ‚ÄúCusparse
library,‚Äù in GPU Technology Conference , 2010.
[14] J. Bai, F. Lu, K. Zhang et al. , ‚ÄúOnnx: Open neural network exchange,‚Äù
https://github.com/onnx/onnx, 2019.
[15] V . Lebedev and V . Lempitsky, ‚ÄúFast convnets using group-wise brain
damage,‚Äù in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , 2016, pp. 2554‚Äì2564.
[16] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky, ‚ÄúSparse
convolutional neural networks,‚Äù in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , 2015, pp. 806‚Äì814.
[17] M. Zhu and S. Gupta, ‚ÄúTo prune, or not to prune: exploring the efÔ¨Åcacy
of pruning for model compression,‚Äù 6th International Conference on
Learning Representations, ICLR 2018 - Workshop Track Proceedings ,
2018.
[18] W. Wen, C. Wu, Y . Wang, Y . Chen, and H. Li, ‚ÄúLearning structured
sparsity in deep neural networks,‚Äù in Advances in neural information
processing systems , 2016, pp. 2074‚Äì2082.
[19] T. Gale, E. Elsen, and S. Hooker, ‚ÄúThe state of sparsity in deep neural
networks,‚Äù CoRR , vol. abs/1902.09574, 2019. [Online]. Available:
http://arxiv.org/abs/1902.09574
[20] M. Gong, J. Liu, H. Li, Q. Cai, and L. Su, ‚ÄúA multiobjective sparse
feature learning model for deep neural networks,‚Äù IEEE transactions on
neural networks and learning systems , vol. 26, no. 12, pp. 3263‚Äì3277,
2015.
[21] M. Lin, R. Ji, Y . Zhang, B. Zhang, Y . Wu, and Y . Tian, ‚ÄúChannel
pruning via automatic structure search,‚Äù in Proceedings of the Twenty-
Ninth International Joint Conference on ArtiÔ¨Åcial Intelligence, IJCAI-
20, C. Bessiere, Ed. International Joint Conferences on ArtiÔ¨Åcial
Intelligence Organization, 7 2020, pp. 673‚Äì679, main track.
[22] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, ‚ÄúDropout: a simple way to prevent neural networks from over-
Ô¨Åtting,‚Äù The Journal of Machine Learning Research , vol. 15, no. 1, pp.
1929‚Äì1958, 2014.
[23] D. Molchanov, A. Ashukha, and D. Vetrov, ‚ÄúVariational dropout spar-
siÔ¨Åes deep neural networks,‚Äù in Proceedings of the 34th International
Conference on Machine Learning-Volume 70 . JMLR. org, 2017, pp.
2498‚Äì2507.[24] A. N. Gomez, I. Zhang, K. Swersky, Y . Gal, and G. E. Hinton, ‚ÄúLearning
sparse networks using targeted dropout,‚Äù CoRR , vol. abs/1905.13678,
2019.
[25] G. Hinton, O. Vinyals, and J. Dean, ‚ÄúDistilling the knowledge in a neural
network,‚Äù arXiv preprint arXiv:1503.02531 , 2015.
[26] T. Li, J. Li, Z. Liu, and C. Zhang, ‚ÄúFew sample knowledge distillation
for efÔ¨Åcient network compression,‚Äù in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2020, pp.
14 639‚Äì14 647.
[27] Y . Wang, X. Zhang, L. Xie, J. Zhou, H. Su, B. Zhang, and X. Hu,
‚ÄúPruning from scratch.‚Äù in AAAI , 2020, pp. 12 273‚Äì12 280.
[28] N. Lee, T. Ajanthan, and P. Torr, ‚ÄúSnip: Single-shot network pruning
based on connection sensitivity,‚Äù 7th International Conference on Learn-
ing Representations, ICLR 2019 , 2019.
[29] E. Tartaglione, A. Bragagnolo, and M. Grangetto, ‚ÄúPruning artiÔ¨Åcial
neural networks: A way to Ô¨Ånd well-generalizing, high-entropy sharp
minima,‚Äù in ArtiÔ¨Åcial Neural Networks and Machine Learning ‚Äì ICANN
2020 , I. Farka Àás, P. Masulli, and S. Wermter, Eds. Cham: Springer
International Publishing, 2020, pp. 67‚Äì78.
[30] C. Louizos, M. Welling, and D. P. Kingma, ‚ÄúLearning sparse neural
networks through l0regularization,‚Äù 6th International Conference on
Learning Representations, ICLR 2018 - Conference Track Proceedings ,
2018.
[31] E. Tartaglione, D. Perlo, and M. Grangetto, ‚ÄúPost-synaptic potential
regularization has potential,‚Äù in International Conference on ArtiÔ¨Åcial
Neural Networks . Springer, 2019, pp. 187‚Äì200.
[32] K. Ullrich, M. Welling, and E. Meeds, ‚ÄúSoft weight-sharing for neural
network compression,‚Äù 5th International Conference on Learning Rep-
resentations, ICLR 2017 - Conference Track Proceedings , 2019.
[33] Y . Guo, A. Yao, and Y . Chen, ‚ÄúDynamic network surgery for efÔ¨Åcient
dnns,‚Äù Advances in Neural Information Processing Systems , pp. 1387‚Äì
1395, 2016.
[34] I. Pavlov, ‚ÄúLzma sdk (software development kit),‚Äù 2007. [Online].
Available: https://www.7-zip.org/sdk.html
[35] Y . LeCun, L. Bottou, Y . Bengio, P. Haffner et al. , ‚ÄúGradient-based
learning applied to document recognition,‚Äù Proceedings of the IEEE ,
vol. 86, no. 11, pp. 2278‚Äì2324, 1998.
[36] H. Xiao, K. Rasul, and R. V ollgraf, ‚ÄúFashion-mnist: a novel image
dataset for benchmarking machine learning algorithms,‚Äù CoRR , vol.
abs/1708.07747, 2017. [Online]. Available: http://arxiv.org/abs/1708.
07747
