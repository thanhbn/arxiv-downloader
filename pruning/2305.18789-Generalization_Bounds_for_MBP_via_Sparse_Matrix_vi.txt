# 2305.18789.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2305.18789.pdf
# Kích thước tệp: 556377 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Ranh giới tổng quát hóa cho MBP thông qua Sparse Matrix
Sketching
Etash Kumar Guha
College of Computing
Georgia Institute of Technology
Atlanta, GA, 30332
etash@gatech.eduPrasanjit Dubey
School of Industrial and Systems Engineering
Georgia Institute of Technology
Atlanta, GA, 30332
pdubey31@gatech.edu
Xiaoming Huo
School of Industrial and Systems Engineering
Georgia Institute of Technology
Atlanta, GA, 30332
huo@gatech.edu
Tóm tắt
Trong bài báo này, chúng tôi rút ra một ranh giới mới về lỗi tổng quát hóa của các mạng nơ-ron siêu tham số khi chúng đã trải qua Magnitude-Based Pruning (MBP). Công việc của chúng tôi xây dựng trên các ranh giới trong Arora et al. [2018], nơi lỗi phụ thuộc vào một, sự xấp xỉ gây ra bởi việc cắt tỉa, và hai, số lượng tham số trong mô hình đã cắt tỉa, và cải thiện so với các ranh giới tổng quát hóa dựa trên chuẩn tiêu chuẩn. Các ước lượng đã cắt tỉa thu được bằng MBP gần với các hàm chưa cắt tỉa với xác suất cao, điều này cải thiện tiêu chí đầu tiên. Sử dụng Sparse Matrix Sketching, không gian của các ma trận đã cắt tỉa có thể được biểu diễn hiệu quả trong không gian của các ma trận dày đặc với chiều nhỏ hơn nhiều, từ đó cải thiện tiêu chí thứ hai. Điều này dẫn đến ranh giới tổng quát hóa mạnh hơn so với nhiều phương pháp tiên tiến nhất, từ đó mở ra nền tảng mới trong việc phát triển thuật toán cho việc cắt tỉa và giới hạn lỗi tổng quát hóa của các mô hình siêu tham số. Ngoài ra, chúng tôi mở rộng kết quả để thu được ranh giới tổng quát hóa cho Iterative Pruning [Frankle và Carbin, 2018]. Chúng tôi xác minh thực nghiệm sự thành công của phương pháp mới này trên ReLU-activated Feed Forward Networks trên bộ dữ liệu MNIST và CIFAR10.

1 Giới thiệu
Các mạng nơ-ron siêu tham số thường được sử dụng trong thực tế vì chúng đạt được lỗi tổng quát hóa đáng chú ý [Goodfellow et al., 2016]. Tuy nhiên, kích thước khổng lồ của chúng làm cho chúng chậm và tốn kém để chạy trong quá trình suy luận [Han et al., 2015]. Các nhà thực hành học máy (ML) thường sử dụng Magnitude-Based pruning (MBP) để sửa chữa độ phức tạp tính toán này. Sau khi huấn luyện các mạng nơ-ron lớn, các tham số hoặc phần tử ma trận trong mô hình có độ lớn nhỏ nhất được đặt thành 0. Điều này giảm đáng kể yêu cầu bộ nhớ của mô hình và thời gian suy luận. Tuy nhiên, MBP cũng đã được chứng minh là gây ra ít lỗi tổng quát hóa và thực tế thường giảm lỗi tổng quát hóa so với mô hình gốc [Han et al., 2015, Li et al., 2016, Cheng et al., 2017]. Việc xem xét vị trí và lý do tại sao việc tổng quát hóa mạnh xảy ra có thể giúp xây dựng các thuật toán học và mô hình tổng quát hóa tốt hơn [Foret et al., 2020, Le et al., 2018]. Tuy nhiên, các phân tích lý thuyết về lý do tại sao MBP đạt được lỗi tổng quát hóa mạnh vẫn cần được thực hiện. Việc cung cấp các phân tích như vậy là thách thức vì một số lý do. Đầu tiên, việc loại bỏ các trọng số nhỏ nhất là một hoạt động tương đối ít được nghiên cứu trong đại số tuyến tính, và chỉ có ít công cụ có sẵn để phân tích các tính chất của ma trận đã cắt tỉa. Thứ hai, việc đặc trưng hóa phân phối của trọng số sau huấn luyện và cắt tỉa là khó khăn.

Tuy nhiên, Arora et al. [2018] cung cấp một công cụ phân tích lỗi tổng quát hóa của các mô hình với ít tham số hơn một cách hiệu quả và toàn diện hơn. Cụ thể, họ giới hạn trên lỗi tổng quát hóa của một mạng nơ-ron lớn khi được nén. Chúng ta có thể áp dụng trực tiếp kết quả này cho các mô hình đã cắt tỉa vì các mô hình đã cắt tỉa về bản chất có ít tham số hơn. Ranh giới của họ được chia thành hai phần: lượng lỗi do mô hình gây ra thông qua việc nén và số lượng tham số khác nhau trong mô hình nén. Chúng tôi sử dụng công cụ chính này để chỉ ra rằng các đầu ra từ MBP tổng quát hóa tốt vì chúng không gây ra nhiều lỗi và có ít tham số hơn. Chúng tôi chứng minh cả hai hiện tượng này với một vài giả định đơn giản. Cụ thể, với một số giả định có thể biện minh về phân phối của các tham số trọng số đã huấn luyện, chúng tôi phát triển một ranh giới trên của lượng lỗi mà mạng nơ-ron đã cắt tỉa gặp phải với xác suất cao. Chúng tôi cũng chứng minh rằng số lượng tham số cần thiết để biểu diễn đầy đủ không gian của các mô hình đã cắt tỉa là tương đối nhỏ. Cụ thể, chúng tôi chỉ ra rằng tập hợp các ma trận đã cắt tỉa của chúng tôi có thể được biểu diễn hiệu quả trong không gian của các ma trận dày đặc có chiều nhỏ hơn nhiều thông qua Sparse Matrix Sketching.

Kết hợp hai phần của ranh giới, chúng tôi có được một ranh giới lỗi tổng quát hóa mới có khả năng cạnh tranh với các ranh giới tổng quát hóa tiên tiến nhất. Hơn nữa, theo hiểu biết của chúng tôi, đây là ranh giới tổng quát hóa đầu tiên cho MBP sử dụng Compression Bounds. Chúng tôi xác minh thực nghiệm sự thành công của phương pháp mới này trên bộ dữ liệu MNIST và CIFAR10 nơi ranh giới của chúng tôi tốt hơn vài bậc độ lớn (ít nhất 107 lần tốt hơn trên CIFAR10, tham khảo Hình 1b) so với các ranh giới tiêu chuẩn nổi tiếng của Neyshabur et al. [2015], Bartlett et al. [2017], và Neyshabur et al. [2017]. Chúng tôi mở rộng khung làm việc để chỉ ra rằng việc sử dụng Iterative Magnitude Pruning (IMP) hoặc Lottery Tickets [Frankle và Carbin, 2018] cũng tổng quát hóa. Cụ thể, Malach et al. [2020] chỉ ra rằng IMP tạo ra kết quả với lỗi nhỏ và ít tham số khác không. Chúng tôi sử dụng matrix sketching để đếm hiệu quả số lượng tham số theo cách có thể sử dụng được cho ranh giới tổng quát hóa của chúng tôi. Điều này dẫn đến một ranh giới tổng quát hóa mạnh mà theo hiểu biết của chúng tôi, chỉ được phân tích thực nghiệm [Bartoldson et al., 2020, Jin et al., 2022].

Đóng góp Chúng tôi liệt kê chính thức các đóng góp của mình ở đây. Đầu tiên chúng tôi chứng minh lỗi gây ra bởi MBP nhỏ so với mô hình gốc. Hơn nữa, chúng tôi chứng minh rằng MBP của chúng tôi đạt được độ thưa thớt đủ, tức là, tương đối ít tham số khác không còn lại sau khi cắt tỉa. Để thắt chặt các ranh giới tổng quát hóa của chúng tôi, chúng tôi chỉ ra rằng các ma trận đã cắt tỉa từ MBP có thể được sketched thành các ma trận dày đặc nhỏ hơn. Chúng tôi kết hợp các kết quả trên để chứng minh rằng lỗi tổng quát hóa của các mô hình đã cắt tỉa là nhỏ. Chúng tôi mở rộng khung chứng minh trên để thiết lập ranh giới lỗi tổng quát hóa cho IMP. Theo hiểu biết của chúng tôi, đây là những kết quả đầu tiên nghiên cứu tổng quát hóa của các mô hình đã cắt tỉa thông qua MBP hoặc IMP. Chúng tôi xác minh thực nghiệm rằng các ranh giới tổng quát hóa của chúng tôi cải thiện so với nhiều ranh giới lỗi tổng quát hóa tiêu chuẩn cho MLPs trên bộ dữ liệu CIFAR10 và MNIST.

2 Các công trình liên quan

2.1 Ranh giới tổng quát hóa dựa trên chuẩn
Trong những năm gần đây, nhiều công trình đã nghiên cứu cách sử dụng việc đếm tham số và chuẩn trọng số để tạo ra các ranh giới tổng quát hóa chặt chẽ hơn như một sự phát triển từ Rademacher Complexity và VC dimension cổ điển. Galanti et al. [2023] sử dụng Rademacher Complexity để phát triển ranh giới tổng quát hóa cho các mạng thưa thớt tự nhiên như những mạng từ sparse regularization. Neyshabur et al. [2015] nghiên cứu một lớp chung các ranh giới dựa trên chuẩn cho mạng nơ-ron. Hơn nữa, Bartlett và Mendelson [2002] sử dụng Rademacher và Gaussian Complexity để tạo thành các ranh giới tổng quát hóa. Long và Sedghi [2020] đưa ra các ranh giới lỗi tổng quát hóa cho Convolutional Neural Networks (CNNs) sử dụng khoảng cách từ trọng số ban đầu và số lượng tham số độc lập với chiều của feature map và số lượng pixel trong đầu vào. Daniely và Granot [2019] sử dụng approximate description length như một dạng trực quan cho việc đếm tham số.

2.2 Kỹ thuật cắt tỉa
Trong khi MBP là một trong những dạng cắt tỉa phổ biến nhất trong thực tế, các dạng khác cũng tồn tại. Collins và Kohli [2014] tạo ra độ thưa thớt vào CNNs của họ bằng cách sử dụng ℓ1 regularization trong huấn luyện. Molchanov et al. [2017] phát triển khung làm việc cắt tỉa lặp để nén các CNN sâu sử dụng cắt tỉa dựa trên tiêu chí tham lam dựa trên khai triển Taylor và fine-tuning bằng backpropagation. Liu et al. [2017]

--- TRANG 2 ---
sử dụng Filter Sparsity cùng với Network Slimming để cho phép tăng tốc trong CNNs của họ. Ullrich et al. [2017] đặt tên soft-weight sharing như một phương pháp tạo ra độ thưa thớt vào các ranh giới của họ. Hơn nữa, Hooker et al. [2019] nghiên cứu thực nghiệm mẫu dữ liệu nào của các mô hình đã cắt tỉa sẽ khác biệt đáng kể so với các mô hình gốc. Nhiều công trình sử dụng các phương pháp cắt tỉa ít phổ biến hơn như coresets [Mussay et al., 2019] hoặc hiện tượng IMP [Frankle và Carbin, 2018, Malach et al., 2020].

3 Sơ bộ

3.1 Ký hiệu
Chúng tôi xem xét một bài toán phân loại đa lớp tiêu chuẩn nơi cho một mẫu x, chúng ta dự đoán lớp y, là một số nguyên từ 1 đến k. Chúng ta giả định rằng mô hình của chúng ta sử dụng một thuật toán học tạo ra một tập hợp L ma trận M={A1, . . . ,AL} nơi Ai∈Rdi1×di2. Ở đây, di1, di2 là các chiều của lớp thứ i. Do đó, cho một đầu vào x nào đó, đầu ra của mô hình được ký hiệu là M(x) được định nghĩa là

M(x) =ALϕL−1(AL−1ϕL−2(. . .A2ϕ1(A1x))),

từ đó ánh xạ x tới M(x)∈Rk. Ở đây, ϕi là hàm kích hoạt cho lớp thứ i của Li Lipschitz-Smoothness. Khi không mơ hồ, chúng ta sẽ sử dụng ký hiệu x0=x và x1=A1x và x2=A2ϕ1(A1x) và tiếp tục. Cho bất kỳ phân phối dữ liệu D nào, mất mát biên mong đợi cho một biên γ > 0 được định nghĩa là

Rγ(M) =P(x,y)∼D[M(x)[y]≤γ+ maxj̸=yM(x)[j]].

Rủi ro dân số R(M) được thu được như một trường hợp đặc biệt của Rγ(M) bằng cách đặt γ= 0. Mất mát biên thực nghiệm cho một bộ phân loại được định nghĩa là

ˆRγ(M) =1|S|∑(x,y)∈SI[M(x)[y]−maxj̸=y(M(x)[j])≥γ],

cho một biên γ >0 nào đó nơi S là tập dữ liệu được cung cấp (khi γ= 0, điều này trở thành mất mát phân loại). Một cách trực quan, ˆRγ(M) biểu thị số lượng phần tử mà bộ phân loại M dự đoán đúng y với biên lớn hơn hoặc bằng γ. Hơn nữa, chúng ta định nghĩa kích thước của S là |S|=n. Chúng ta sẽ ký hiệu ˆM={ˆA1, . . . , ˆAL} là mô hình nén thu được sau khi cắt tỉa M. Lỗi tổng quát hóa của mô hình đã cắt tỉa sau đó là R0(ˆM). Hơn nữa, chúng ta sẽ định nghĩa ma trận khác biệt tại lớp l là ∆l=Al−ˆAl. Bây giờ chúng ta đã định nghĩa chính thức ký hiệu của mình, chúng ta sẽ tóm tắt ngắn gọn công cụ tổng quát hóa chính trong suốt bài báo này.

3.2 Ranh giới nén
Vì ranh giới nén là một trong những công cụ lý thuyết chính được sử dụng trong suốt bài báo này, chúng ta sẽ tóm tắt ngắn gọn các ranh giới được trình bày trong Arora et al. [2018]. Cho rằng chúng ta đưa một mô hình f vào một thuật toán nén, tập hợp các đầu ra có thể là một tập hợp các mô hình GA,s nơi A là một tập hợp các cấu hình tham số có thể và s là một số thông tin khởi động được cung cấp cho thuật toán nén. Chúng ta sẽ gọi gA là một mô hình như vậy tương ứng với cấu hình tham số A∈ A. Hơn nữa, nếu tồn tại một mô hình nén gA∈GA,s sao cho với bất kỳ đầu vào nào trong tập dữ liệu S, các đầu ra từ gA và f khác nhau nhiều nhất γ, chúng ta nói f là (γ,S) có thể nén. Chính thức, chúng ta làm rõ điều này trong định nghĩa sau.

Định nghĩa 3.1. Nếu f là một bộ phân loại và GA,s={gA|A∈ A} là một lớp các bộ phân loại với một tập hợp các cấu hình tham số có thể huấn luyện A và chuỗi cố định s. Chúng ta nói rằng f là (γ,S)-có thể nén thông qua GA,s nếu tồn tại A∈ A sao cho với bất kỳ x∈ S, chúng ta có với mọi y, |f(x)[y]−gA(x)[y]| ≤γ.

Bây giờ chúng ta giới thiệu ranh giới nén của chúng ta. Lỗi tổng quát hóa của các mô hình nén trong kỳ vọng là, nhiều nhất, lỗi tổng quát hóa thực nghiệm của mô hình gốc nếu mô hình gốc có biên γ. Sử dụng các bất đẳng thức tập trung tiêu chuẩn, chúng ta áp dụng ranh giới này cho tất cả các kết quả mô hình đã cắt tỉa có thể. Ranh giới tổng quát hóa kết quả phụ thuộc vào cả biên và số lượng tham số trong mô hình đã cắt tỉa, như trong định lý sau.

--- TRANG 3 ---
Định lý 3.1. [Arora et al., 2018] Giả sử GA,s={gA,s|A∈ A} nơi A là một tập hợp q tham số mỗi tham số có thể có nhiều nhất r giá trị rời rạc và s là một chuỗi trợ giúp. Cho S là một tập huấn luyện với n mẫu. Nếu bộ phân loại đã huấn luyện f là (γ,S)-có thể nén thông qua GA,s, với chuỗi trợ giúp s, thì tồn tại A∈ A với xác suất cao trên tập huấn luyện,

R0(gA)≤ˆRγ(f) +O(√(qlogr/n)).

Cần lưu ý rằng định lý trên cung cấp một ranh giới tổng quát hóa cho bộ phân loại nén gA, không phải cho bộ phân loại đã huấn luyện f. Do đó, hai phần tạo thành các ranh giới tổng quát hóa chặt chẽ hơn cho một thuật toán nén nhất định bao gồm việc giới hạn lỗi do nén gây ra, γ trong ˆRγ(f), và số lượng tham số q sau khi nén. Chúng tôi chứng minh rằng chúng ta có thể đạt được cả hai với MBP truyền thống.

3.3 Giả định sơ bộ
Việc phân tích các hiệu ứng của cắt tỉa khó khăn mà không hiểu trước phân phối mà trọng số của một mô hình đã huấn luyện nằm trong. Đây là một câu hỏi phức tạp và mở nói chung. Tuy nhiên, Han et al. [2015] đã quan sát thực nghiệm rằng trọng số thường nằm trong phân phối Gaussian có trung bình không, như trong Hình 7 trong Han et al. [2015]. Do đó chúng ta sẽ giả định điều này là đúng, rằng phân phối trọng số tuân theo phân phối chuẩn với trung bình 0 và phương sai Ψ. Ở đây, chúng ta nêu các giả định sơ bộ chính mà chúng ta sẽ sử dụng sau này.

Giả định 3.1. Với bất kỳ l∈[L], i, j∈[dl1]×[dl2], Alij∼ N(0,Ψ).

Giả định này nói rằng mỗi nguyên tử trong ma trận của mô hình đã học tuân theo phần nào phân phối Gaussian tâm tại 0 với phương sai Ψ. Mặc dù là một giả định mạnh, điều này phổ biến. Thực tế, Qian và Klabjan [2021] giả định rằng trọng số tuân theo phân phối đều để phân tích trọng số của các mô hình đã cắt tỉa. Chúng ta giả định phân phối Gaussian vì điều này hợp lý hơn so với giả định phân phối đều. Bây giờ chúng ta có thể trình bày thuật toán MBP mà chúng ta sẽ phân tích trong suốt bài báo này.

4 Thuật toán Magnitude-Based Pruning

Mặc dù nhiều phiên bản của thuật toán MBP tồn tại, tất cả đều dựa trên khung chung của việc loại bỏ trọng số có độ lớn nhỏ để giảm số lượng tham số trong khi đảm bảo mô hình đã cắt tỉa không khác biệt quá nhiều so với bản gốc. Chúng ta muốn chọn một thuật toán cắt tỉa dựa trên khung này thường được sử dụng bởi các nhà thực hành trong khi cùng lúc dễ phân tích. Chúng ta phát triển thuật toán để mô phỏng MBP ngẫu nhiên thường thấy trong các công trình như Han et al. [2015], Qian và Klabjan [2021]. Mặc dù thuật ngữ bên trong biến ngẫu nhiên Bernoulli được sử dụng như một chỉ báo cho cắt tỉa hơi khác so với tài liệu trước đó, đây là một thay đổi nhỏ cho phép chúng ta thoát khỏi giả định phân phối đều từ Qian và Klabjan [2021] thành giả định Gaussian thuận lợi hơn. Chúng ta trình bày chính thức thuật toán trong Thuật toán 1 dưới đây.

Thuật toán 1: MBP
Dữ liệu: {A1, . . . ,AL}, d
Kết quả: {ˆA1, . . . ,ˆAL}
for l∈[L] do
    for i, j∈[dl1]×[dl2] và i̸=j do
        X:=Bernoulli(exp(−[Alij]2/dΨ))
        ˆAlij:= 0 nếu X= 1 ngược lại Alij
    end
end

Nhận xét 4.1. Chúng ta không cắt tỉa các phần tử đường chéo trong Thuật toán 1. Mặc dù không tiêu chuẩn, điều này cho phép sử dụng Matrix Sketching sau này cho ranh giới tổng quát hóa tốt hơn. Tuy nhiên, trong Dasarathy et al. [2013], họ lưu ý sự cần thiết cho việc các phần tử đường chéo khác không là để dễ trình bày chứng minh, và Matrix Sketching vẫn có thể với việc cắt tỉa các phần tử đường chéo.

Xác suất nguyên tử bị nén tương đối nhỏ đối với các nguyên tử lớn hơn. Xác suất bị nén lớn hơn đối với các nguyên tử nhỏ hơn gần 0. Ở đây, d là một siêu tham số hữu ích để điều chỉnh độ mạnh của nén. Sử dụng thuật toán nén này, mô hình đã cắt tỉa có thể duy trì các kết nối giữa các nguyên tử lớn hơn trong khi loại bỏ nhiều tham số nhỏ hơn. Để sử dụng các ranh giới tổng quát hóa từ Phần 3.2, chúng ta cần chỉ ra rằng Thuật toán 1 tạo ra một mô hình đã cắt tỉa ˆM tạo ra đầu ra tương tự như mô hình gốc M. Chúng ta cũng cần chỉ ra rằng số lượng tham số khác không trong các mô hình đã cắt tỉa là nhỏ. Chúng ta chứng minh điều này trong các phần dưới đây.

4.1 Chứng minh lỗi

Chúng ta bắt đầu bằng việc giới hạn sự khác biệt giữa các đầu ra của các lớp tương ứng trong các mô hình đã cắt tỉa và gốc để chứng minh rằng sự khác biệt kỳ vọng giữa các mô hình đã cắt tỉa và gốc là nhỏ. Giả định tính chuẩn từ Giả định 3.1 làm cho điều này dễ tính toán hơn nhiều. Thực vậy, mỗi nguyên tử của ma trận khác biệt ∆l=ˆAl−Al là một biến ngẫu nhiên độc lập và đồng phân. Việc giới hạn chuẩn ℓ2 của ma trận như vậy chỉ dựa vào tài liệu phong phú nghiên cứu chuẩn của ma trận ngẫu nhiên. Thực tế, từ Latala [2005], chúng ta chỉ cần moment thứ hai và thứ tư bị chặn của phân phối của mỗi nguyên tử. Để sử dụng ranh giới này, chúng ta chỉ cần chứng minh rằng ma trận khác biệt ∆l và mô hình đã cắt tỉa thu được bằng cách sử dụng lược đồ nén Thuật toán 1 có các nguyên tử có moment bị chặn và có trung bình không. Cho thuật toán nén được chọn và phân phối trọng số sau huấn luyện sử dụng Giả định 3.1, ma trận ∆l thực sự thỏa mãn các tính chất đó. Chúng ta chứng minh chúng trong bổ đề sau.

Bổ đề 4.1. Giá trị kỳ vọng của bất kỳ phần tử ∆lij nào của ma trận ∆l=ˆAl−Al được cho bởi E(∆lij) = 0 với bất kỳ i, j∈[dl1]×[dl2], l∈[L]. Do đó, E(∆l) =0 là một ma trận đầy 0. Hơn nữa, chúng ta có E((∆lij)2) =d3/2Ψ/(d+2)3/2. Hơn nữa, moment thứ tư của bất kỳ phần tử (∆lij)4 nào của ∆l được cho bởi E((∆lij)4) =3d5/2Ψ2/(d+2)5/2.

Cho các tính chất này của ma trận ∆l, chúng ta có thể sử dụng các bất đẳng thức tập trung đơn giản để giới hạn lỗi tích lũy tại bất kỳ lớp nào giữa các mô hình đã cắt tỉa và gốc. Nếu chúng ta mô phỏng một số đầu vào mẫu x đi qua mô hình đã cắt tỉa, chúng ta có thể chỉ ra rằng lỗi tích lũy qua toàn bộ mô hình bị giới hạn thông qua quy nạp. Chúng ta trình bày chính thức bổ đề như vậy ở đây.

Bổ đề 4.2. Với bất kỳ lớp l∈[L] cho trước, chúng ta có với xác suất ít nhất 1−1/ϵl
∥(ˆAl−Al)∥2≤ϵlΓl nơi Γl=C[√(d3/2Ψ/(d+ 2)3/2)(√dl1+√dl2) + (3dl1dl2d5/2Ψ2/(d+ 2)5/2)1/4].
Ở đây, ˆAl được tạo bởi Thuật toán 1 và C là một hằng số dương phổ quát.

Bây giờ chúng ta có hiểu biết chính thức về cách cắt tỉa một lớp nhất định trong mô hình gốc ảnh hưởng đến kết quả của lớp đó. Chúng ta có thể trình bày ranh giới lỗi cho toàn bộ mạng thưa thớt.

Bổ đề 4.3. Sự khác biệt giữa đầu ra của mô hình đã cắt tỉa và mô hình gốc trên bất kỳ đầu vào x nào bị giới hạn bởi, với xác suất ít nhất 1−∑L l ϵ−1 l,

∥ˆxL−xL∥ ≤ed01 ∏L l=1Ll∥Al∥2 ∑L l=1ϵlΓl/∥Al∥2.

Ranh giới này nói rằng lỗi tích lũy bởi mô hình đã cắt tỉa thu được bằng Thuật toán 1 chỉ phụ thuộc vào chiều của mô hình, hằng số Lipschitz của các hàm kích hoạt, phương sai của các phần tử và lỗi của nén. Ranh giới như vậy là trực quan vì lỗi tích lũy lặp đi lặp lại qua các lớp. Chúng ta cung cấp một phác thảo chứng minh ngắn gọn ở đây.

Phác thảo chứng minh. Bằng Perturbation Bound từ Neyshabur et al. [2017], chúng ta có thể giới hạn lỗi tích lũy qua mô hình bằng cách sử dụng chuẩn của ma trận khác biệt từ Bổ đề 4.2. Chúng ta có thể tạo ra các ranh giới chặt chẽ hơn bằng cách xem xét maximum kỳ vọng của (ˆAl−Al)x là gì với xác suất cao. Nếu dl2< dl1, chúng ta quan sát rằng ma trận ˆAl−Al có nhiều nhất dl2 giá trị kỳ dị khác không. Để biết thêm chi tiết, vui lòng xem Phụ lục C.

--- TRANG 4 ---
Tuy nhiên, ranh giới lỗi này nhiều hơn là cần thiết để chứng minh các ranh giới tổng quát hóa mạnh. Chúng ta yêu cầu số lượng mô hình có thể sau huấn luyện và nén là hữu hạn để sử dụng các ranh giới nén. Do đó, chúng ta cần áp dụng rời rạc hóa cho mô hình nén để đảm bảo rằng số lượng mô hình là hữu hạn. Tuy nhiên, điều này tương đối đơn giản cho nền tảng lý thuyết đã được cung cấp.

4.2 Rời rạc hóa

Bây giờ chúng ta chỉ ra rằng lỗi dự đoán giữa một rời rạc hóa của mô hình đã cắt tỉa và mô hình gốc cũng bị giới hạn. Phương pháp rời rạc hóa của chúng ta đơn giản là làm tròn mỗi giá trị trong lớp l đến bội số gần nhất của ρl. Chúng ta sẽ gọi mô hình đã cắt tỉa rời rạc hóa là ˜M nơi lớp thứ l sẽ được ký hiệu là ˜Al. Chúng ta cung cấp bổ đề sau giới hạn chuẩn của sự khác biệt của các lớp giữa mô hình đã cắt tỉa và mô hình rời rạc hóa. Sử dụng trực quan này, chúng ta có thể chứng minh rằng lỗi gây ra bởi rời rạc hóa là nhỏ.

Bổ đề 4.4. Chuẩn của sự khác biệt giữa lớp đã cắt tỉa và lớp rời rạc hóa bị giới hạn trên là ∥˜Al−ˆAl∥2≤ρlJl nơi Jl là số lượng tham số khác không trong ˆAl (Jl được sử dụng để ngắn gọn ở đây và sẽ được phân tích sau). Với xác suất ít nhất 1−∑L l=1ϵ−1 l, cho rằng tham số ρl cho mỗi lớp được chọn sao cho ρl≤(1/L∥Al∥2−ϵlΓl)/Jl, chúng ta có rằng lỗi gây ra bởi cả rời rạc hóa và cắt tỉa bị giới hạn bởi

∥xL−˜xL∥2≤ed01 ∏L l=1Ll∥Al∥2 ∑L l=1(ϵlΓl+ρlJl)/∥Al∥2.

Bây giờ, chúng ta có một ranh giới lỗi đủ trên thuật toán MBP của chúng ta. Do đó, như bước tiếp theo, chúng ta tập trung vào việc giới hạn số lượng tham số mà mô hình nén của chúng ta sẽ có. Để làm điều này, chúng ta giới thiệu công cụ lý thuyết quan trọng tiếp theo: Matrix Sketching.

5 Sketching Ma trận thưa thớt

Như thấy trong Định lý 3.1, lỗi tổng quát hóa phụ thuộc mạnh vào số lượng tham số. Chúng ta cố gắng đếm số lượng tham số hóa có thể của mô hình đã cắt tỉa ˆM có thể đạt được bằng cách kết hợp thuật toán học và thuật toán nén. Trong phụ lục, chúng ta thảo luận một cách tiếp cận ngây thơ bằng cách đếm số lượng ma trận thưa thớt có thể được tạo ra bởi sự kết hợp của thuật toán học và Thuật toán 1. Điều này đạt được ranh giới tổng quát hóa kém mong muốn, tạo động lực cho Matrix Sketching. Bây giờ chúng ta giới thiệu các sơ bộ và thúc đẩy nhu cầu về matrix sketching.

5.1 Sơ bộ về Matrix Sketching

Ở đây chúng ta giới thiệu các khái niệm sơ bộ của matrix sketching. Cụ thể, chúng ta có thể biểu diễn ma trận thưa thớt X∈Rp1×p2 như Y∈Rm×m nơi p1, p2≥m. Ý tưởng của matrix sketching là tạo ra một embedding cho ma trận này là Y=AXB⊤. Ở đây, các ma trận A∈ {0,1}m×p1,B∈ {0,1}m×p2 được chọn trước khi sketching được thực hiện. Để khôi phục ma trận gốc, chúng ta giải bài toán tối ưu

min˜X∈Rp1×p2∥˜X∥1 s.t. Y=A˜XB⊤. (1)

Nếu bài toán từ Phương trình (1) có minimum duy nhất và minimum duy nhất đó là X thực, chúng ta có thể nói rằng lược đồ sketching này là lossless. Trong trường hợp như vậy, tất cả thông tin trong X được mã hóa trong Y. Cho ánh xạ một-một như vậy, chúng ta có thể sử dụng ánh xạ một-một này giữa Y và X để đếm số lượng tham số hóa của X sử dụng Y, có chiều nhỏ hơn. Chúng ta sử dụng các tính chất từ tài liệu này để giúp phát triển và chứng minh các ranh giới lỗi tổng quát hóa được cải thiện.

Chúng ta tuyên bố với matrix sketching rằng chúng ta có thể biểu diễn không gian của các ma trận thưa thớt lớn có chiều p với tập hợp các ma trận dày đặc nhỏ có chiều √jplogp nơi j là số lượng phần tử khác không tối đa trong bất kỳ hàng hoặc cột nào. Việc đếm số lượng tham số trong các ma trận dày đặc nhỏ hiệu quả hơn về tham số so với việc đếm số lượng ma trận thưa thớt lớn, do đó cung cấp một cách thoát khỏi bùng nổ tổ hợp. Chúng ta chính thức hóa điều này trong phần sau.

5.2 Trường hợp thưa thớt

Để áp dụng tài liệu matrix sketching cho các ma trận thưa thớt của chúng ta, chúng ta cần chứng minh một số tính chất của các ma trận thu được bằng cách sử dụng lược đồ nén Thuật toán 1. Chúng ta giới thiệu một cấu trúc như vậy gọi là jr, jc-distributed-sparsity, đảm bảo rằng sketching có thể được áp dụng cho ma trận. Một cách trực quan, tính chất như vậy đảm bảo rằng bất kỳ hàng hoặc cột nào của ma trận thưa thớt không chứa quá nhiều phần tử khác không. Chúng ta định nghĩa chính thức trực quan như vậy ở đây.

Định nghĩa 5.1. Một ma trận là jr, jc-distributed sparse nếu nhiều nhất jr phần tử trong bất kỳ cột nào khác không, jc phần tử trong bất kỳ hàng nào khác không, và các phần tử đường chéo đều khác không.

Kiến thức chính khác là cách tạo thành A, B cho sparse matrix sketching. Nếu người đọc quan tâm, chúng ta thảo luận cách tạo thành A và B cùng với một số trực quan đằng sau matrix sketching trong Phụ lục E.1.

5.3 Ranh giới cho Sparse Matrix Sketching

Từ Dasarathy et al. [2013], có thể chứng minh rằng sketching tập hợp các ma trận jr, jc-distributed sparse chỉ yêu cầu m nhỏ. Cho một lựa chọn m và thuật ngữ xác suất δ, người ta có thể chỉ ra rằng nghiệm của Phương trình (1) khớp với giá trị chưa nén với xác suất cao. Điều này chủ yếu được chỉ ra bằng cách đầu tiên chứng minh rằng một nghiệm tồn tại và rằng nghiệm tốt nhất cho A−1Y B−1=˜X là nghiệm duy nhất tối thiểu hóa chuẩn ℓ1 với xác suất cao.

Định lý 5.1. (Từ Định lý 1 của Dasarathy et al. [2013]) Cho p= max( dl1, dl2). Giả sử rằng A∈ {0,1}m×dl1,B∈ {0,1}m×dl2 được rút độc lập và đồng đều từ δ-random bipartite ensemble. Sau đó, miễn là m=O(p max( jcdl1, jrdl2) log( p)) và δ=O(log(p)), tồn tại c≥0 sao cho với bất kỳ ma trận jr, jc-distributed sparse X cho trước, sketches AXB thành ˜X dẫn đến một sketch duy nhất cho mỗi X. Phát biểu này đúng với xác suất 1−p−c.

Nhận xét 5.1. Phát biểu định lý cho Định lý 5.1 nói rằng c≥0. Tuy nhiên, trong chứng minh, họ chứng minh tuyên bố mạnh hơn rằng c≥2. Do đó, xác suất Định lý 5.1 đúng là ít nhất 1−p−2. Khi p tăng, xác suất định lý này đúng tiến tới 1.

5.4 Lỗi tổng quát hóa từ Sketching

Để sử dụng các công cụ lý thuyết trên của matrix sketching, chúng ta phải chỉ ra rằng đầu ra từ thuật toán nén Thuật toán 1 thỏa mãn định nghĩa của jr, jc-distributed-sparsity. Tuyên bố như vậy là trực quan và các tính chất tương tự đã được chỉ ra cho ma trận ngẫu nhiên tuân theo các phân phối khác nhau. Cho rằng các ma trận đã huấn luyện thỏa mãn phân phối Gaussian, một hàng hoặc cột không có khả năng chứa nhiều phần tử khác không. Ở đây, chúng ta chứng minh trong bổ đề sau rằng mô hình đã cắt tỉa sử dụng Thuật toán 1 thỏa mãn điều kiện distributed sparsity sử dụng Giả định 3.1.

Bổ đề 5.1. Với xác suất ít nhất 1−1/λl−(dl1)−1/3−(dl2)−1/3, chúng ta có rằng các đầu ra từ Thuật toán 1 là jr, jc-sparsely distributed nơi max( jr, jc)≤3λlmax( dl1, dl2)χ và λl∈R. Ở đây, χ=√d+2−√d/√d+2.

Cho định lượng trên về không gian của ma trận thưa thớt và ranh giới của lỗi của mô hình, chúng ta có thể áp dụng ranh giới nén từ Arora et al. [2018]. Ranh giới nén như vậy một cách trực quan phụ thuộc chủ yếu vào hai giá trị này.

Định lý 5.2. Với mỗi ma trận ˆAl, định nghĩa jl là max( jr, jc) nơi jr và jc là các hệ số distribution-sparsity cho ˆAl. Hơn nữa, với mỗi ma trận ˆAl, định nghĩa pl= max( dl1, dl2). Sau đó chúng ta có

R0(gA)≤ˆRγ(f) +O(√(∑l3λlχdl2dl1log2(pl) log(1/ρl)/n)).

Điều này đúng khi d được chọn sao cho γ≥ed01∏dl=1Ll∥Al∥2∑L l=1(ϵlΓl+ρlJl)/∥Al∥2 nơi Jl≤ O(χdl2dl1). Tuyên bố này đúng với xác suất ít nhất 1−[∑L l=1λ−1 l+ϵ−1 l+p−c l].

--- TRANG 5 ---
Ở đây, χ phụ thuộc vào siêu tham số d. Thực vậy, ranh giới tổng quát hóa này cải thiện đáng kể so với việc áp dụng tầm thường các ranh giới nén như trong Bổ đề D.1. Khá đáng chú ý, điều này loại bỏ bản chất tổ hợp của ranh giới ngây thơ trong Bổ đề D.1 với giá nhỏ của √log2(pl)/n.

6 Tổng quát hóa của Lottery Tickets

Khung chứng minh lỗi tổng quát hóa như vậy cho cắt tỉa áp dụng cho nhiều hơn chỉ Magnitude-based pruning. Một cách tiếp cận cắt tỉa thú vị đơn giản là tạo ra một mô hình rất lớn G sao cho một mô hình mục tiêu nhỏ hơn M ẩn bên trong G và có thể được tìm thấy bằng cắt tỉa. Công thức lottery ticket này cho cắt tỉa đã thấy nhiều lợi ích thực nghiệm. Chính thức, chúng ta sẽ gọi lottery ticket bên trong G là weight-subnetwork ẽG của G. ẽG này là phiên bản đã cắt tỉa của G gốc. Thực tế, Malach et al. [2020] chỉ ra rằng với G đủ lớn, tồn tại với xác suất cao một cắt tỉa ˜G sao cho ˜G và hàm mục tiêu M khác nhau nhiều nhất ϵ. Hơn nữa, ˜G này sẽ có xấp xỉ cùng số lượng tham số khác không như mạng mục tiêu gốc M. Điều này được trình bày chính thức trong Định lý 6.1.

Định lý 6.1. Cố định một số ϵ, δ∈(0,1). Cho M là một mạng mục tiêu có độ sâu L sao cho với mỗi i∈[L] chúng ta có ∥Ai∥2≤1, ∥Ai∥max≤1/√d1,i. Hơn nữa, cho nM là chiều ẩn tối đa của M. Cho G là một mạng nơi mỗi chiều ẩn bị giới hạn trên bởi poly(d1,0, nM, L,1/ϵ,log1/δ):=DG và độ sâu 2L, nơi chúng ta khởi tạo Ai từ phân phối đều U([−1,1]). Sau đó, w.p ít nhất 1−δ tồn tại weight-subnetwork ẽG của G sao cho:

supx∈S|ẽG(x)−M(x)| ≤ϵ.

Hơn nữa, số lượng trọng số active (khác không) trong ẽG là O(d0,1DG+D2GL).

Định lý 6.2. Với xác suất ít nhất 1−δ−LD−cG có lỗi tổng quát hóa của

R0(˜G)≤ˆRϵ+ϵρ(˜G) +O(√([nMd0,1log(DG)2+Ln2Mlog(DG)2] log(1/ρ)/n)).

Ở đây, ϵρ là lỗi nhỏ gây ra bởi rời rạc hóa.

Ở đây, chúng ta có ranh giới tổng quát hóa cho mô hình đã cắt tỉa. Một điều thú vị cần lưu ý là ranh giới này chỉ kém một nhân tử nhỏ log(DG) so với nếu chúng ta áp dụng ranh giới nén cho mô hình có kích thước của hàm mục tiêu M. Theo hiểu biết của chúng tôi, đây là một trong những phân tích tổng quát hóa đầu tiên cho lottery tickets.

7 Phân tích thực nghiệm

Mã Chúng tôi đã cung cấp mã ở đây để tái tạo. Điều này dựa trên một fork của LaBonte [2023].

Chúng tôi nghiên cứu ranh giới tổng quát hóa thu được bằng cách sử dụng Thuật toán 1 cắt tỉa với một số ranh giới tổng quát hóa dựa trên chuẩn tiêu chuẩn nổi tiếng của Neyshabur et al. [2015], Bartlett et al. [2017], và Neyshabur et al. [2017] được sử dụng làm cơ sở. Các thí nghiệm của chúng tôi so sánh lỗi tổng quát hóa thu được bởi các ranh giới này, ranh giới tổng quát hóa của thuật toán (như cung cấp trong 5.2), và lỗi tổng quát hóa thực của mô hình nén, đã huấn luyện. Chúng tôi cũng cung cấp một thí nghiệm chứng minh cách ranh giới tổng quát hóa của chúng tôi mở rộng khi tăng chiều ẩn của mô hình.

Các mô hình của chúng tôi là Multi-Layer Perceptron Models (MLPs) với các lớp kích hoạt ReLU với 5 lớp. Chúng tôi huấn luyện thuật toán với tỷ lệ học 0.02 với bộ tối ưu Adam [Kingma và Ba, 2014] trong 300 epoch. Chúng tôi tiến hành thí nghiệm trên hai bộ dữ liệu phân loại hình ảnh khác nhau: MNIST [LeCun và Cortes, 2010] và CIFAR10 [Krizhevsky et al.]. Chúng tôi sử dụng MLP với chiều ẩn 784 để so sánh ranh giới với các ranh giới tổng quát hóa khác. Cho thí nghiệm về việc mở rộng với kích thước mô hình, chúng tôi thử nghiệm trên các chiều ẩn 500, 1000, 1500, 2000, và 2500 nơi độ sâu được giữ cố định.

--- TRANG 6 ---
[Hình 1: So sánh các ranh giới tổng quát hóa trên thang logarit w.r.t. (a) MNIST, và (b) bộ dữ liệu CIFAR10. Trong (c), chúng ta thấy cách ranh giới phụ thuộc vào kích thước mô hình.]

Kết quả Ranh giới của chúng tôi tốt hơn vài bậc độ lớn so với các ranh giới tổng quát hóa tiên tiến nhất, như có thể suy ra từ Hình 1a và Hình 1b trên. Trong cả hai thí nghiệm, ranh giới gần nhất với chúng tôi là của Bartlett et al. [2017], vẫn ít nhất 103 và 107 lần lớn hơn ranh giới của chúng tôi trên bộ dữ liệu MNIST và CIFAR10 tương ứng. Hơn nữa, ranh giới tổng quát hóa của chúng tôi nhất quán tốt hơn Bartlett et al. [2017] khi chiều ẩn tăng Hình 1c. Điều này chứng minh rằng trên nhiều bộ dữ liệu, ranh giới của chúng tôi chặt chẽ hơn các ranh giới tổng quát hóa dựa trên chuẩn truyền thống và mở rộng tốt hơn với chiều ẩn. Chúng tôi cung cấp một số thí nghiệm bổ sung trong phụ lục. Kết quả, mặc dù đáng chú ý, không đáng ngạc nhiên chủ yếu do việc sử dụng thuật toán cắt tỉa, đảm bảo lỗi do nén thấp, và việc sử dụng Sparse Matrix Sketching, giảm đáng kể chiều của mô hình đã cắt tỉa, một yếu tố quan trọng khi tính toán ranh giới tổng quát hóa.

8 Kết luận

Bài báo này đã đạt được tiến bộ trong vấn đề rút ra ranh giới tổng quát hóa của các mạng nơ-ron siêu tham số. Với thuật toán cắt tỉa hiệu quả và Sparse Matrix Sketching, chúng tôi đã thu được ranh giới cho các mô hình đã cắt tỉa tốt hơn đáng kể so với các ranh giới tổng quát hóa dựa trên chuẩn nổi tiếng và xác minh thực nghiệm hiệu quả của phương pháp này trên dữ liệu thực. Chúng tôi hy vọng những kết quả này sẽ thúc đẩy nghiên cứu thêm trong deep learning để hiểu rõ hơn cách và tại sao các mô hình tổng quát hóa. Sẽ thú vị khi thấy liệu matrix sketching có thể được sử dụng để chứng minh tổng quát hóa cho các loại cắt tỉa khác nhau, như coresets. Hơn nữa, cũng có thể có lợi khi xem liệu matrix sketching có thể được sử dụng cùng với ranh giới PAC-Bayes để tạo ra ranh giới tổng quát hóa. Trong vấn đề này, chúng tôi đã mở rộng khung chung của bài báo này để rút ra ranh giới tổng quát hóa cho lottery tickets trong Phần 6, một kết quả mà theo hiểu biết của chúng tôi, là đầu tiên thuộc loại này. Một khả năng khác sẽ là khám phá cách các ranh giới tổng quát hóa khác nhau có thể được tạo thành cho các phân phối dữ liệu khác nhau từ các thuật toán huấn luyện khác nhau.

Hạn chế Thuật toán magnitude-based pruning của chúng tôi không cắt tỉa đường chéo của ma trận, điều này không tiêu chuẩn. Hơn nữa, sau huấn luyện, chúng tôi giả định rằng mỗi nguyên tử thuộc phân phối Gaussian i.i.d, điều này có thể không luôn đúng. Ngoài ra, tương tự như các ranh giới tiêu chuẩn, ranh giới tổng quát hóa của chúng tôi vẫn vacuous, không giải thích đầy đủ tổng quát hóa của các mô hình.

Tài liệu tham khảo

Sanjeev Arora, Rong Ge, Behnam Neyshabur, và Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. CoRR, abs/1802.05296, 2018. URL http://arxiv.org/abs/1802.05296.

Peter L Bartlett và Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.

--- TRANG 7 ---
Peter L. Bartlett, Dylan J. Foster, và Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. CoRR, abs/1706.08498, 2017. URL http://arxiv.org/abs/1706.08498.

Brian Bartoldson, Ari Morcos, Adrian Barbu, và Gordon Erlebacher. The generalization-stability tradeoff in neural network pruning. Advances in Neural Information Processing Systems, 33: 20852–20864, 2020.

Yu Cheng, Duo Wang, Pan Zhou, và Tao Zhang. A survey of model compression and acceleration for deep neural networks. CoRR, abs/1710.09282, 2017. URL http://arxiv.org/abs/1710.09282.

Maxwell D. Collins và Pushmeet Kohli. Memory bounded deep convolutional networks. CoRR, abs/1412.1442, 2014. URL http://arxiv.org/abs/1412.1442.

Amit Daniely và Elad Granot. Generalization bounds for neural networks via approximate description length. Advances in Neural Information Processing Systems, 32, 2019.

Gautam Dasarathy, Parikshit Shah, Badri Narayan Bhaskar, và Robert D. Nowak. Sketching sparse matrices. CoRR, abs/1303.6544, 2013. URL http://arxiv.org/abs/1303.6544.

Pierre Foret, Ariel Kleiner, Hossein Mobahi, và Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. CoRR, abs/2010.01412, 2020. URL https://arxiv.org/abs/2010.01412.

Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Training pruned neural networks. CoRR, abs/1803.03635, 2018. URL http://arxiv.org/abs/1803.03635.

Tomer Galanti, Mengjia Xu, Liane Galanti, và Tomaso Poggio. Norm-based generalization bounds for compositionally sparse neural networks. arXiv preprint arXiv:2301.12033, 2023.

Ian J. Goodfellow, Yoshua Bengio, và Aaron Courville. Deep Learning. MIT Press, Cambridge, MA, USA, 2016. http://www.deeplearningbook.org.

Song Han, Jeff Pool, John Tran, và William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.

Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, và Andrea Frome. What do compressed deep neural networks forget? arXiv preprint arXiv:1911.05248, 2019.

Tian Jin, Michael Carbin, Daniel M Roy, Jonathan Frankle, và Gintare Karolina Dziugaite. Pruning's effect on generalization through the lens of training and regularization. arXiv preprint arXiv:2210.13738, 2022.

Diederik P Kingma và Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

Alex Krizhevsky, Vinod Nair, và Geoffrey Hinton.. cifar-10 (canadian institute for advanced research). URL http://www.cs.toronto.edu/~kriz/cifar.html.

Tyler LaBonte. Milkshake: Quick and extendable experimentation with classification models. http://github.com/tmlabonte/milkshake, 2023.

Rafal Latala. Some estimates of norms of random matrices. Proceedings of the American Mathematical Society, 133:1273–1282, 05 2005. doi: 10.2307/4097777.

Lei Le, Andrew Patterson, và Martha White. Supervised autoencoders: Improving generalization performance with unsupervised regularizers. Trong S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, và R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/2a38a4a9316c49e5a833517c45d31070-Paper.pdf.

Yann LeCun và Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.lecun.com/exdb/mnist/.

--- TRANG 8 ---
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for efficient convnets. CoRR, abs/1608.08710, 2016. URL http://arxiv.org/abs/1608.08710.

Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. Learning efficient convolutional networks through network slimming. Trong Proceedings of the IEEE international conference on computer vision, trang 2736–2744, 2017.

Philip M. Long và Hanie Sedghi. Generalization bounds for deep convolutional neural networks, 2020.

Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, và Ohad Shamir. Proving the lottery ticket hypothesis: Pruning is all you need. CoRR, abs/2002.00585, 2020. URL https://arxiv.org/abs/2002.00585.

Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, và Jan Kautz. Pruning convolutional neural networks for resource efficient inference, 2017.

Ben Mussay, Samson Zhou, Vladimir Braverman, và Dan Feldman. On activation function coresets for network pruning. CoRR, abs/1907.04018, 2019. URL http://arxiv.org/abs/1907.04018.

Behnam Neyshabur, Ryota Tomioka, và Nathan Srebro. Norm-based capacity control in neural networks. Trong Conference on learning theory, trang 1376–1401. PMLR, 2015.

Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, và Nathan Srebro. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. CoRR, abs/1707.09564, 2017. URL http://arxiv.org/abs/1707.09564.

Xin Qian và Diego Klabjan. A probabilistic approach to neural network pruning. CoRR, abs/2105.10065, 2021. URL https://arxiv.org/abs/2105.10065.

Andréa Richa, Michael Mitzenmacher, và Ramesh Sitaraman. The power of two random choices: A survey of techniques and results. 10 2000. doi: 10.1007/978-1-4615-0013-1_9.

Karen Ullrich, Edward Meeds, và Max Welling. Soft weight-sharing for neural network compression, 2017.

Roman Vershynin. High-dimensional probability. 2019. URL https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf.

--- TRANG 9 ---
A Chi tiết tính toán

Ở đây, chúng tôi cung cấp một số thông tin về phần cứng và cài đặt được sử dụng cho công việc của chúng tôi. Chúng tôi bao gồm một bản sao của mã trong Tài liệu bổ sung để tái tạo. Các thí nghiệm của chúng tôi được chạy với Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz và NVIDIA GeForce RTX 2080 Ti. Hơn nữa, các thí nghiệm được chạy sử dụng Python phiên bản 3.10. Ngoài ra, các thí nghiệm được thực hiện sử dụng batch size 256 cho thí nghiệm trên MNIST và batch size 4 cho CIFAR10. Thư viện phần mềm Deep Learning được sử dụng là PyTorch Lightning. Chúng tôi đã bao gồm thêm mã trong tài liệu bổ sung cùng với README về cách tái tạo thí nghiệm.

B Chứng minh lỗi cắt tỉa và rời rạc hóa

Chúng tôi nói rằng phần lớn phân tích lỗi này được lấy cảm hứng từ các chứng minh trong Qian và Klabjan [2021]. Tuy nhiên, chúng tôi sử dụng giả định phân phối Gaussian hợp lý hơn về trọng số và có thuật toán Magnitude-based Pruning hơi khác.

B.1 Chứng minh Bổ đề 4.1

Bổ đề 4.1. Giá trị kỳ vọng của bất kỳ phần tử ∆lij nào của ma trận ∆l=ˆAl−Al được cho bởi E(∆lij) = 0 với bất kỳ i, j∈[dl1]×[dl2], l∈[L]. Do đó, E(∆l) =0 là ma trận đầy 0. Hơn nữa, chúng ta có E((∆lij)2) =d3/2Ψ/(d+2)3/2. Hơn nữa, moment thứ tư của bất kỳ phần tử (∆lij)4 nào của ∆l được cho bởi E((∆lij)4) =3d5/2Ψ2/(d+2)5/2.

Chứng minh. Theo định nghĩa Thuật toán 1, biến ngẫu nhiên ∆lij phụ thuộc vào biến ngẫu nhiên Alij.

∆lij={Alij w.p. exp(−[Alij]2/dΨ), 0 w.p. 1−exp(−[Alij]2/dΨ)}.

Để tính E(∆lij), chúng ta sẽ sử dụng Luật kỳ vọng toàn phần. Tức là, E(∆lij)=E(E(∆lij|Alij)). Chúng ta có E(∆lij|Alij) =Alij exp(−[Alij]2/dΨ). Do đó, để tính E(E(∆lij|Alij)), chúng ta sử dụng định nghĩa kỳ vọng cho biến liên tục là

E(∆lij) =E(E(∆lij|Alij))
=∫∞−∞ Alij exp(−Alij2/dΨ) (1/√2πΨ) exp(−1/2 (Alij/√Ψ)2) dAlij
= 0.

Bây giờ chúng ta tập trung vào thành phần bình phương của bổ đề. Tương tự, chúng ta có

E((∆lij)2) =E(E((∆lij)2|Alij))
=∫∞−∞(Alij)2 exp(−Alij2/dΨ) (1/√2πΨ) exp(−1/2 (Alij/√Ψ)2) dAlij
=d3/2Ψ/(d+ 2)3/2.

--- TRANG 10 ---
Chúng ta tính toán tương tự moment thứ tư là

E((∆lij)4) =E(E((∆lij)4|Alij))
=∫∞−∞(Alij)4 exp(−Alij2/dΨ) (1/√2πΨ) exp(−1/2 (Alij/√Ψ)2) dAlij
=3d5/2Ψ2/(d+ 2)5/2.

B.2 Chứng minh Bổ đề 4.2

Bổ đề 4.2. Với bất kỳ lớp l∈[L] cho trước, chúng ta có với xác suất ít nhất 1−1/ϵl
∥(ˆAl−Al)∥2≤ϵlΓl nơi Γl=C[√(d3/2Ψ/(d+ 2)3/2)(√dl1+√dl2) + (3dl1dl2d5/2Ψ2/(d+ 2)5/2)1/4].
Ở đây, ˆAl được tạo bởi Thuật toán 1 và C là hằng số dương phổ quát.

Chúng ta sẽ chứng minh rằng lỗi từ nén bị giới hạn. Để làm điều đó trước tiên sẽ sử dụng kết quả rằng chuẩn kỳ vọng của bất kỳ ma trận trung bình không nào có thể bị giới hạn bằng moment thứ hai và thứ tư. Chúng ta phát biểu lại bổ đề kỹ thuật hữu ích này từ Định lý 2 của Latala [2005].

Bổ đề B.1. Cho A là ma trận ngẫu nhiên có các phần tử Ai,j là các biến ngẫu nhiên độc lập trung bình không với moment thứ tư hữu hạn. Sau đó
E∥A∥2≤C[(maxi ∑j EA2i,j)1/2 + (maxj ∑i EA2i,j)1/2 + (∑j EA4i,j)1/4].
Ở đây, C, là hằng số dương phổ quát.

Bây giờ chúng ta sử dụng bổ đề này để giới hạn lỗi do nén sử dụng Thuật toán 1.

Chứng minh. Cho Z là mặt nạ sao cho ˆAlij=Z◦Alij nơi ◦ là tích ma trận theo phần tử. Chúng ta sẽ phân tích ma trận khác biệt ∆ =Z◦Alij−Alij. Lưu ý rằng

E((∆lij)2) =E((∆lij)2|Zi,j= 0)·P(Zi,j= 0) + E((∆lij)2|Zi,j= 1)·P(Zi,j= 1) .

Rõ ràng, nếu mặt nạ cho nguyên tử được đặt thành 1, lỗi bình phương cho nguyên tử đó là 0. Do đó, chúng ta có
E(∆lij|Zi,j= 1)P(Zi,j= 1) = 0 .

Do đó chúng ta chỉ cần phân tích thuật ngữ thứ hai. Chúng ta có

E((∆lij)2|Zi,j= 0)·P(Zi,j= 0) = P(Zi,j= 0)∫∞−∞(Alij)2·P(Alij|Zi,j= 0)dAlij
=∫∞−∞(Alij)2·P(Zi,j= 0|Alij)·P(Alij)dAlij
=∫∞−∞(Alij)2·exp(−[Alij]2/dΨ)·(1/√2πΨ) exp(−1/2 (Alij/√Ψ)2)dAlij
=d3/2Ψ/(d+ 2)3/2.

Sau đó chúng ta có
E((∆lij)2) =d3/2Ψ/(d+ 2)3/2.

Tương tự, chúng ta có thể làm tương tự cho moment thứ tư.

E((∆lij)4|Zi,j= 0)·P(Zi,j= 0) = P(Zi,j= 0)∫∞−∞(Alij)4·P(Alij|Zi,j= 0)dAlij
=∫∞−∞(Alij)4·P(Zi,j= 0|Alij)·P(Alij)dAlij
=∫∞−∞(Alij)4·P(Zi,j= 0|Alij)·P(Alij)dAlij
=∫∞−∞(Alij)4·exp(−[Alij]2/dΨ)·(1/√2πΨ) exp(−1/2 (Alij/√Ψ)2)dAlij
=3d5/2Ψ2/(d+ 2)5/2.

Kết hợp với Bổ đề B.1, chúng ta có,

E∥∆l∥2≤C[√(d3/2Ψ/(d+ 2)3/2)(√dl1+√dl2) + (3dl1dl2d5/2Ψ2/(d+ 2)5/2)1/4].

Sau đó chúng ta có thể áp dụng bất đẳng thức Markov nơi
P(∥∆l∥2≥t)≤E∥∆l∥2/t.

Chúng ta đặt Γl=C[√(d3/2Ψ/(d+2)3/2)√(dl1+√dl2) + (3dl1dl2d5/2Ψ2/(d+2)5/2)1/4] để dễ ký hiệu. Nếu chúng ta đặt t=ϵlΓl, thì chúng ta có với xác suất ít nhất 1−1/ϵl rằng,
∥∆l∥2≤ϵlΓl.

Chúng ta đã chứng minh phát biểu.

B.3 Chứng minh Ranh giới nhiễu loạn thực

Bổ đề B.2. Với trọng số của mô hình M và bất kỳ nhiễu loạn Ul nào cho l∈[h] nơi lớp nhiễu loạn l là Ul+Al, cho rằng ∥Ul∥2≤(1/L)∥Al∥2, chúng ta có với mọi đầu vào x0∈ S,
∥xL−¯xL∥2≤ed01 ∏L l=1κlLl∥Al∥2 ∑L l=1∥Ul∥2/∥Al∥2.
Ở đây, ¯xL biểu thị đầu ra của lớp thứ L của mô hình nhiễu loạn.

Chứng minh. Chứng minh này chủ yếu theo Neyshabur et al. [2017]. Chúng ta phát biểu lại ở đây với ký hiệu khác để rõ ràng và đầy đủ. Chúng ta sẽ chứng minh giả thuyết quy nạp rằng ∥¯xl−xl∥2≤(1 +1/Ll)∥x0∥2 ∏l i=1Li∥Al∥2 ∑l i=1∥Ui∥2/∥Ai∥2. Trường hợp cơ sở của quy nạp tầm thường đúng, cho rằng chúng ta có ∥¯x0−x0∥2= 0 theo định nghĩa. Bây giờ, chúng ta chứng minh bước quy nạp. Giả sử giả thuyết quy nạp đúng cho l. Chúng ta sẽ chứng minh rằng nó đúng cho l+ 1. Chúng ta có

--- TRANG 11 ---
∥xl−¯xl∥2≤ ∥(Al+Ul)ϕl(¯xl−1)−Alϕl(xl−1)∥2
≤ ∥(Al+Ul)(ϕl(¯xl−1)−ϕl(xl−1)) +Ulϕl(xl−1)∥2
≤ (∥Al∥2+∥Ul∥2)∥ϕl(¯xl−1)−ϕl(xl−1)∥2+∥Ul∥2∥ϕl(xl−1)∥2 (2)
≤ (∥Al∥2+∥Ul∥2)∥ϕl(¯xl−1)−ϕl(xl−1)∥2+∥Ul∥2∥ϕl(xl−1)∥2
≤Ll(∥Al∥2+∥Ul∥2)∥¯xl−1−xl−1∥2+Ll∥Ul∥2∥xl−1∥2 (3)
≤Ll(1 +1/d)∥Al∥2∥¯xl−1−xl−1∥2+Ll∥Ul∥2∥xl−1∥2
≤Ll(1 +1/d)∥Al∥2(1 +1/Ll−1)∥x0∥2 ∏l−1 i=1Li∥Ai∥2 ∑l−1 i=1∥Ui∥2/∥Ai∥2+Ll∥Ul∥2∥xl−1∥2 (4)
≤Ll(1 +1/Ll) ∏l−1 i=1Li∥Ai∥2 ∑l−1 i=1∥Ui∥2/∥Ai∥2∥x0∥2+Ll∥x0∥2∥Ul∥2 ∏l−1 i=1Li∥Ai∥2
≤Ll(1 +1/Ll) ∏l−1 i=1Li∥Ai∥2 ∑l−1 i=1∥Ui∥2/∥Ai∥2∥x0∥2+∥x0∥2∥Ul∥2/∥Al∥2 ∏l i=1Li∥Ai∥2
≤(1 +1/Ll) ∏l i=1Li∥Ai∥2 ∑l i=1∥Ui∥2/∥Ai∥2∥x0∥2

Ở đây, Phương trình (2) từ việc áp dụng Bổ đề C.1. Phương trình (3) từ thực tế rằng ϕi là Li-Lipschitz mượt và ϕi(0) = 0. Hơn nữa, Phương trình (4) từ việc áp dụng giả thuyết quy nạp. Do đó, chúng ta đã chứng minh giả thuyết quy nạp cho tất cả các lớp. Bây giờ chúng ta chỉ cần thực tế rằng (1 +1/L)L≤e, và chúng ta có phát biểu cuối cùng.

B.4 Chứng minh Bổ đề B.3

Bổ đề B.3. Sự khác biệt giữa đầu ra của mô hình đã cắt tỉa và mô hình gốc trên bất kỳ đầu vào x nào bị giới hạn bởi, với xác suất ít nhất 1−∑L i ϵ−1 i,
∥ˆxL−xL∥ ≤ed01 ∏L l=1Ll∥Al∥2 ∑L l=1ϵlΓl/∥Al∥2.

Chứng minh. Chúng ta sẽ so sánh đầu ra của mô hình gốc xl với đầu ra của mô hình nén. Chúng ta cần thực tế rằng (1/L)∥Al∥2≥ϵlΓl≥ ∥Al−ˆAl∥2. Từ Vershynin [2019], chúng ta có E((1/L)∥Al∥2)≥(1/4L)√(dl1+√dl2), và ϵlΓl nhỏ hơn điều này trong kỳ vọng cho ϵl đủ nhỏ. Do đó, chúng ta có thể sử dụng Bổ đề B.2 và Bổ đề 4.2. Do đó chúng ta có

∥xl−ˆxl∥2≤ed01 ∏L l=1Ll∥Al∥2 ∑L l=1∥Al−ˆAl∥2/∥Al∥2
≤ed01 ∏d l=1Ll∥Al∥2 ∑L l=1ϵlΓl/∥Al∥2

B.5 Chứng minh Bổ đề 4.4

Bổ đề 4.4. Chuẩn của sự khác biệt giữa lớp đã cắt tỉa và lớp rời rạc hóa bị giới hạn trên là ∥˜Al−ˆAl∥2≤ρlJl nơi Jl là số lượng tham số khác không trong ˆAl (Jl được sử dụng để ngắn gọn ở đây và sẽ được phân tích sau). Với xác suất ít nhất 1−∑L l=1ϵ−1 l, cho rằng tham số ρl cho mỗi lớp được chọn sao cho ρl≤((1/L)∥Al∥2−ϵlΓl)/Jl, chúng ta có rằng lỗi gây ra bởi cả rời rạc hóa và cắt tỉa bị giới hạn bởi

∥xL−˜xL∥2≤ed01 ∏L l=1Ll∥Al∥2 ∑L l=1(ϵlΓl+ρlJl)/∥Al∥2.

Chứng minh. Chúng ta sẽ so sánh đầu ra của mô hình gốc xl với đầu ra của mô hình nén và rời rạc hóa ˜xl. Để sử dụng ranh giới nhiễu loạn từ Bổ đề B.2, chúng ta cần ∥Al−˜Al∥2≤(1/L)∥Al∥2. Cho mỗi lớp, chúng ta có thể chọn tham số rời rạc hóa để thỏa mãn điều này. Chúng ta chứng minh điều này trong:

∥Al−˜Al∥2≤ ∥Al−ˆAl∥2+∥ˆAl−˜Al∥2
≤ϵlΓl+ρlJl

Do đó, miễn là chúng ta chọn
ρl≤((1/L)∥Al∥2−ϵlΓl)/Jl,

chúng ta có tính chất mong muốn. Do đó, sử dụng Bổ đề B.2, chúng ta có

∥xl−˜xl∥2≤ed01 ∏L l=1Ll∥Al∥2 [∑ l=1∥Al−˜Al∥2/∥Al∥2
≤ed01 ∏d l=1Llκl∥Al∥2 ∑L l=1(∥Al−ˆAl∥2+∥ˆAl−˜Al∥2)/∥Al∥2
≤ed01 ∏d l=1Ll∥Al∥2 ∑L l=1(ϵlΓl+ρlJl)/∥Al∥2

Điều này chỉ xảy ra nếu sự kiện từ Bổ đề 4.2 xảy ra cho mỗi lớp. Sử dụng union bound, chúng ta biết điều này xảy ra với xác suất ít nhất 1−∑L l ϵ−1 l

C Ranh giới lỗi dưới điều kiện Subgaussian

Chúng ta có thể tạo ranh giới chặt chẽ hơn bằng cách xem xét maximum kỳ vọng của (ˆAl−Al)x sẽ là gì với xác suất cao. Nếu dl2< dl1, chúng ta quan sát rằng ma trận ˆAl−Al có nhiều nhất dl2 giá trị kỳ dị khác không. Chúng ta cần giả định điều kiện Subgaussian trên đầu vào của mỗi lớp để làm điều này tốt để cải thiện ranh giới.

Điều kiện C.1. Đầu vào đến mỗi lớp l∈[L], thuộc phân phối D, sao cho với một số vector đơn vị v và vector tùy ý x được lấy mẫu từ D thỏa mãn
P(⟨x, v⟩ ≥t)≤ae−bt2dl1.
Ở đây a và b là hằng số phổ quát lớn hơn 0.

Cần lưu ý rằng Điều kiện C.1 thường thấy trong lý thuyết Thống kê Chiều cao. Phân phối đều trên mặt cầu đơn vị thỏa mãn Điều kiện C.1. Cho Điều kiện C.1 này, chúng ta có thể giới hạn lỗi xấp xỉ từ việc tăng đáng kể trong bất kỳ lớp cho trước nào.

Chúng ta muốn làm ranh giới về mức độ lỗi nén gây ra trên biên của tập dữ liệu huấn luyện. Trong khi các ranh giới truyền thống giả định blow-up trường hợp xấu nhất, chúng ta có thể sử dụng thực tế rằng vector gần như trực giao trong không gian chiều cao.

Bổ đề C.1. Giả sử chúng ta được cho ma trận B kích thước dl1×dl2 nơi dl1≥dl2 và S là tập hợp vector từ phân phối thỏa mãn Điều kiện C.1. Với bất kỳ vector x∈ S, tồn tại hằng số a, b sao cho
∥Bx∥2≤√(dl2tl)∥B∥2∥x∥2,
với xác suất ít nhất 1− |S| ae−bt2ldl1. Chúng ta sẽ gọi κl=√(dl2tl) nếu dl2≤dl1 và κl= 1 ngược lại.

--- TRANG 12 ---
Chứng minh. Đầu tiên chúng ta phân tích B=UΣV sử dụng Phân tích Giá trị Kỳ dị. Do đó, với bất kỳ x chúng ta có,
∥Bx∥2=∥UΣV x∥2 =∥ΣV x∥2 =∥Σy∥2.
Đẳng thức thứ hai từ thực tế rằng U là unitary và bảo toàn chuẩn, và đẳng thức thứ ba từ việc đặt y=V x. Bây giờ, nếu x là một số vector ngẫu nhiên chuẩn tiêu chuẩn, thì y cũng là vector ngẫu nhiên chuẩn tiêu chuẩn. Chúng ta cũng quan sát rằng Σ là ma trận đường chéo nơi chỉ dl2 giá trị đầu tiên khác không. Chúng ta sử dụng đồng nhất thức nổi tiếng rằng nếu v là vector có độ lớn 1,
P(⟨v, y⟩ ≥t)≤ae−bt2dl1.
Ở đây, a và b là hằng số toàn cục. Do đó, áp dụng bất đẳng thức này cho các giá trị kỳ dị khác không tương ứng trong Σ, chúng ta có
P(∥Σy∥2≥√(dl2t)∥B∥2)≤ae−bt2dl1,
vì ∥B∥2 là giá trị kỳ dị tối đa. Áp dụng union bound cho mỗi phần tử của S, chúng ta có với mỗi phần tử trong S
∥Bx∥2≤√(dl2t)∥B∥2∥x∥2,
với xác suất ít nhất 1− |S| ae−bt2dl1.

Bổ đề C.2. Với trọng số của mô hình M và bất kỳ nhiễu loạn Ul cho l∈[h] nơi lớp nhiễu loạn l là Ul+Al, cho rằng ∥Ul∥2≤(1/L)∥Al∥2, chúng ta có với mọi đầu vào x0∈ S,
∥xL−¯xL∥2≤ed01 ∏L l=1κlLl∥Al∥2 ∑L l=1∥Ul∥2/∥Al∥2.
Ở đây, ¯xL biểu thị đầu ra của lớp thứ L của mô hình nhiễu loạn. Điều này xảy ra nếu Điều kiện C.1 xảy ra.

Chứng minh. Chứng minh này chủ yếu theo Neyshabur et al. [2017]. Chúng ta phát biểu lại ở đây với ký hiệu khác để rõ ràng và đầy đủ. Chúng ta sẽ chứng minh giả thuyết quy nạp rằng ∥¯xl−xl∥2≤(1 +1/Ll)∥x0∥2 ∏l i=1κiLi∥Al∥2 ∑l i=1∥Ui∥2/∥Ai∥2. Trường hợp cơ sở của quy nạp tầm thường đúng, cho rằng chúng ta có ∥¯x0−x0∥2= 0 theo định nghĩa. Bây giờ, chúng ta chứng minh bước quy nạp. Giả sử giả thuyết quy nạp đúng cho l. Chúng ta sẽ chứng minh rằng nó đúng cho l+ 1. Chúng ta có

--- TRANG 13 ---
∥xl−¯xl∥2≤ ∥(Al+Ul)ϕl(¯xl−1)−Alϕl(xl−1)∥2
≤ ∥(Al+Ul)(ϕl(¯xl−1)−ϕl(xl−1)) +Ulϕl(xl−1)∥2
≤ (∥Al∥2+∥Ul∥2)∥ϕl(¯xl−1)−ϕl(xl−1)∥2+∥Ul∥2∥ϕl(xl−1)∥2 (5)
≤ (∥Al∥2+∥Ul∥2)∥ϕl(¯xl−1)−ϕl(xl−1)∥2+∥Ul∥2∥ϕl(xl−1)∥2
≤Ll(∥Al∥2+∥Ul∥2)∥¯xl−1−xl−1∥2+Ll∥Ul∥2∥xl−1∥2 (6)
≤Ll(1 +1/d)∥Al∥2∥¯xl−1−xl−1∥2+Ll∥Ul∥2∥xl−1∥2
≤Ll(1 +1/d)∥Al∥2(1 +1/Ll−1)∥x0∥2 ∏l−1 i=1Liκi∥Ai∥2 ∑l−1 i=1∥Ui∥2/∥Ai∥2+Ll∥Ul∥2∥xl−1∥2 (7)
≤Ll(1 +1/Ll) ∏l−1 i=1Liκi∥Ai∥2 ∑l−1 i=1∥Ui∥2/∥Ai∥2∥x0∥2+Ll∥x0∥2∥Ul∥2 ∏l−1 i=1Li∥Ai∥2
≤Ll(1 +1/Ll) ∏l−1 i=1Liκi∥Ai∥2 ∑l−1 i=1∥Ui∥2/∥Ai∥2∥x0∥2+∥x0∥2∥Ul∥2/∥Al∥2 ∏l i=1Li∥Ai∥2
≤(1 +1/Ll) ∏l i=1Liκi∥Ai∥2 ∑l i=1∥Ui∥2/∥Ai∥2∥x0∥2

Ở đây, Phương trình (5) từ việc áp dụng Bổ đề C.1. Phương trình (6) từ thực tế rằng ϕi là Li-Lipschitz mượt và ϕi(0) = 0. Hơn nữa, Phương trình (7) từ việc áp dụng giả thuyết quy nạp. Do đó, chúng ta đã chứng minh giả thuyết quy nạp cho tất cả các lớp. Bây giờ chúng ta chỉ cần thực tế rằng (1 +1/L)L≤e, và chúng ta có phát biểu cuối cùng. Nếu Điều kiện C.1 không được thỏa mãn, chúng ta chỉ cần đặt κl= 1 cho tất cả l∈[L] và phân tích sẽ vẫn hợp lệ.

C.1 Chứng minh Bổ đề C.3

Bổ đề C.3. Sự khác biệt giữa đầu ra của mô hình đã cắt tỉa và mô hình gốc trên bất kỳ đầu vào x nào bị giới hạn bởi, với xác suất ít nhất 1−[∑L i ϵ−1 i+|S|ae−bt2ldl1],
∥ˆxL−xL∥ ≤ed01 ∏L l=1Llκl∥Al∥2 ∑L l=1ϵlΓl/∥Al∥2.
Ở đây, a, b là hằng số dương từ phân phối dữ liệu đầu vào.

Chứng minh. Chúng ta sẽ so sánh đầu ra của mô hình gốc xl với đầu ra của mô hình nén. Chúng ta cần thực tế rằng (1/L)∥Al∥2≥ϵlΓl≥ ∥Al−ˆAl∥2. Từ Vershynin [2019], chúng ta có E((1/L)∥Al∥2)≥(1/4L)√(dl1+√dl2), và ϵlΓl nhỏ hơn điều này trong kỳ vọng cho ϵl đủ nhỏ. Do đó, chúng ta có thể sử dụng Bổ đề C.2. Do đó chúng ta có

∥xl−ˆxl∥2≤ed01 ∏L l=1Llκl∥Al∥2 ∑L l=1∥Al−ˆAl∥2/∥Al∥2
≤ed01 ∏d l=1Llκl∥Al∥2 ∑L l=1ϵlΓl/∥Al∥2

Bổ đề C.4. Chuẩn của sự khác biệt giữa lớp đã cắt tỉa và lớp rời rạc hóa bị giới hạn trên là ∥˜Al−ˆAl∥2≤ρlJl nơi Jl là số lượng tham số khác không trong ˆAl (Jl được sử dụng để ngắn gọn ở đây và sẽ được phân tích sau). Với xác suất ít nhất 1−[∑L l=1ϵ−1 l+|S|ae−bt2ldl1], cho rằng tham số ρl cho mỗi lớp được chọn sao cho ρl≤((1/L)∥Al∥2−ϵlΓl)/Jl, chúng ta có rằng lỗi gây ra bởi cả rời rạc hóa và cắt tỉa bị giới hạn bởi

∥xL−˜xL∥2≤ed01 ∏L l=1Llκl∥Al∥2 ∑L l=1(ϵlΓl+ρlJl)/∥Al∥2.

Chứng minh. Chúng ta sẽ so sánh đầu ra của mô hình gốc xl với đầu ra của mô hình nén và rời rạc hóa ˜xl. Để sử dụng ranh giới nhiễu loạn từ Bổ đề C.2, chúng ta cần ∥Al−˜Al∥2≤(1/L)∥Al∥2. Cho mỗi lớp, chúng ta có thể chọn tham số rời rạc hóa để thỏa mãn điều này. Chúng ta chứng minh điều này trong:

∥Al−˜Al∥2≤ ∥Al−ˆAl∥2+∥ˆAl−˜Al∥2 ≤ϵlΓl+ρlJl

Do đó, miễn là chúng ta chọn
ρl≤((1/L)∥Al∥2−ϵlΓl)/Jl,
chúng ta có tính chất mong muốn. Do đó, sử dụng Bổ đề B.2, chúng ta có

∥xl−˜xl∥2≤ed01 ∏L l=1Llκl∥Al∥2 [∑ l=1∥Al−˜Al∥2/∥Al∥2
≤ed01 ∏d l=1Llκl∥Al∥2 ∑L l=1(∥Al−ˆAl∥2+∥ˆAl−˜Al∥2)/∥Al∥2
≤ed01 ∏d l=1Llκl∥Al∥2 ∑L l=1(ϵlΓl+ρlJl)/∥Al∥2

Điều này chỉ xảy ra nếu sự kiện từ Bổ đề 4.2 và Bổ đề C.1 xảy ra cho mỗi lớp. Sử dụng union bound, chúng ta biết điều này xảy ra với xác suất ít nhất 1−[∑L l ϵ−1 l− |S| ae−bt2ldl1].

D Chứng minh tổng quát hóa ngây thơ

Cho giả định Gaussian, việc đếm số lượng kết quả có thể của thuật toán nén bằng cách đếm số lượng cấu hình có thể của nguyên tử khác không trong bất kỳ ma trận nào và sau đó đếm các giá trị có thể mà mỗi nguyên tử có thể có sau lượng tử hóa là tự nhiên. Chúng ta cung cấp ranh giới tổng quát hóa từ trực quan này.

Bổ đề D.1. Sử dụng các lập luận đếm trên tạo ra ranh giới tổng quát hóa
R0(gA)≤ˆRγ(f) +O(√(∑l log(dl1dl2 / α) +αlog(1/ρl) / n)).
Điều này đúng khi d được chọn sao cho γ≥ed01∏L l=1Ll∥Al∥2∑L l=1(ϵlΓl+ρlJl)/∥Al∥2.

Bây giờ chúng ta cung cấp kiến thức cần thiết để chứng minh ranh giới này. Đầu tiên chúng ta phân tích phương pháp ngây thơ để đếm số lượng kết quả có thể từ thuật toán học và lược đồ nén. Đầu tiên chúng ta cung cấp ranh giới tổng quát hóa thay đổi nhẹ để phù hợp với trường hợp sử dụng tốt hơn.

Định lý D.1. Nếu có J tham số hóa khác nhau, lỗi tổng quát hóa của nén ga là, với xác suất ít nhất 1−δ,
L0(gA)≤ˆLγ(f) +√(ln√(J/δ) / n).

--- TRANG 14 ---
Chứng minh. Phần lớn chứng minh này theo Định lý 2.1 từ Arora et al. [2018]. Với mỗi A∈ A, tổn thất huấn luyện ˆR0(gA) là trung bình của n biến ngẫu nhiên Bernoulli i.i.d. với giá trị kỳ vọng bằng R0(gA). Do đó, bằng ranh giới Chernoff tiêu chuẩn, chúng ta có,
P(R0(gA)−ˆR0(gA)≥τ)≤exp(−2τ2n).
Cho f là (γ,S)-có thể nén bởi gA, chúng ta biết tổn thất biên thực nghiệm của gA cho biên 0 nhỏ hơn tổn thất biên thực nghiệm của f với biên γ, tức là ˆR0(gA)≤ˆRγ(f). Cho có J tham số hóa khác nhau, bằng union bound, với xác suất ít nhất 1−Jexp(−2τn), chúng ta có R0(gA)≤τ+ˆR0(gA). Đặt Jexp(−2τn) =δ, chúng ta có τ=√(ln√(J/δ) / n). Do đó, với xác suất 1−δ, chúng ta có
R0(gA)≤ˆRγ(f) +√(ln√(J/δ) / n).

Bây giờ, chúng ta có thể nêu số lượng tham số hóa có thể đạt được bởi thuật toán nén. Nếu có dl1dl2 phần tử trong ma trận và α không bị cắt tỉa sau nén, thì có (dl1dl2 / α) tổng tham số hóa cho mỗi lớp. Hơn nữa, trong mỗi tham số hóa, có rα cách chọn giá trị mà mỗi phần tử khác không có nơi r=O(1/ρl) là số lượng giá trị mà bất kỳ nguyên tử nào có thể có cho rời rạc hóa. Do đó chúng ta cần ranh giới về số lượng phần tử không bị cắt tỉa sau cắt tỉa. Chúng ta đạt được điều này với hai bổ đề sau. Đầu tiên chúng ta sẽ chứng minh rằng ít nhất τ phần tử có xác suất κ bị nén. Sử dụng lập luận đếm như vậy tạo ra ranh giới tổng quát hóa sau.

Bổ đề D.2. Ít nhất τ phần tử của ma trận Al cho trước sẽ có xác suất ít nhất κ bị nén. Sự kiện này xảy ra với xác suất ít nhất 1−I1−p1(d1d2−τ,1 +τ) nơi p1= erf(√(−dln(κ)/2)). Ở đây, erf là Gauss Error Function.

Chứng minh. Để bất kỳ phần tử cho trước có xác suất ít nhất κ bị nén,
exp(−A2i,j/dΨ)≥κ.
Điều này có nghĩa là
|Ai,j| ≤√(−dΨ ln(κ)).
Cho rằng |Ai,j| tuân theo Folded Normal Distribution, xác suất điều này xảy ra là
p1=P(|Ai,j| ≤√(−dΨ ln(κ))) = erf(√(−dΨ ln(κ))/√(2Ψ)) = erf(√(−dln(κ)/2)) (8)

Để dễ ký hiệu, chúng ta ký hiệu tập hợp nguyên tử thỏa mãn tiêu chí này C={‍(i, j)|exp(−A2i,j/dΨ)≥κ}. Do đó, số lượng phần tử τ sẽ có xác suất bị nén lớn hơn κ tuân theo phân phối nhị thức. Do đó,
P(|C| ≥ τ) = 1−I1−p1(d1d2−τ,1 +τ).
Ở đây, I là Regularized Incomplete Beta Function.

--- TRANG 15 ---
Sử dụng ranh giới xác suất trên từ Bổ đề D.2, chúng ta có thể giới hạn trên số lượng phần tử khác không trong bất kỳ ma trận nào.

Bổ đề D.3. Cho sự kiện từ Bổ đề D.2 xảy ra, xác suất ít nhất α phần tử sẽ bị nén là ít nhất 1−I1−κ(τ−α, α+ 1).

Chứng minh. Có ít nhất τ phần tử với xác suất lớn hơn κ. Trong trường hợp xấu nhất, các phần tử dl1dl2−τ khác không bị nén. Phân phối xác suất trên các phần tử còn lại là phân phối nhị thức với xác suất κ. Do đó, xác suất ít nhất α trong số τ phần tử bị nén là ít nhất 1−I1−κ(τ−α, α+ 1).

Bây giờ, chúng ta cuối cùng có thể chứng minh ranh giới tổng quát hóa ngây thơ.

Chứng minh. Từ Định lý D.1, chúng ta có
L0(gA)≤ˆLγ(f) +√(ln√(J/δ) / n),
nơi J là số lượng cấu hình tham số. Mỗi ma trận có (dl1dl2 / α) cách khác nhau để sắp xếp các phần tử khác không. Trong bất kỳ cấu hình như vậy, có rα cách chọn giá trị cho bất kỳ phần tử khác không, nơi rl=O(1/ρl) là số lượng giá trị mà bất kỳ nguyên tử nào có thể có sau rời rạc hóa. Điều này tạo ra ranh giới tổng quát hóa
R0(gA)≤ˆRγ(f) +O(√(log(dl1dl2 / α) +αlogrl / n)).
Ở đây, chúng ta chỉ yêu cầu γ≥ed01∏d l=1Llκl∥Al∥2∑L l=1(ϵlΓl+ρlJl)/∥Al∥2 cho Bổ đề C.4.

Tiếc rằng, ranh giới như vậy khá kém trong sự phụ thuộc vào kích thước của ma trận, chủ yếu do thuật ngữ logarithm của giai thừa, là giá trị lớn đáng kể. Điều này thực tế tệ hơn nhiều ranh giới trước đó trong tài liệu. Điều này do bùng nổ tổ hợp của số lượng ma trận thưa thớt. Tuy nhiên, nếu tồn tại cách thay thế biểu diễn không gian của ma trận thưa thớt trong không gian của ma trận dày đặc có chiều nhỏ hơn nhiều, thì chúng ta có thể tránh bùng nổ tổ hợp như vậy của số lượng tham số. Đây chính xác là mục đích của matrix sketching.

E Chứng minh Matrix Sketching

E.1 Cách chọn A, B

Để tạo A hoặc B, đầu tiên chúng ta có thể lấy mẫu đồ thị lưỡng phân ngẫu nhiên nơi phân vùng trái có kích thước m, và phân vùng phải có kích thước p1 hoặc chiều của ma trận được sketched. Nếu chúng ta nói rằng bất kỳ node nào trong phân vùng trái được kết nối với nhiều nhất δ nodes, chúng ta có thể gọi đồ thị lưỡng phân này là δ-random bipartite ensemble. Chúng ta có định nghĩa kết quả dưới đây.

Định nghĩa E.1. G là đồ thị lưỡng phân G= ([x],[y],E) nơi x và y là kích thước của phân vùng trái và phải, tương ứng, và E là tập hợp các cạnh. Chúng ta gọi G là δ-random bipartite ensemble nếu mỗi node trong [x] được kết nối với nhiều nhất δ nodes trong [y] và mỗi kết nối có thể đều có khả năng như nhau.

Cho thiết lập này, chúng ta có thể chọn ma trận A và B như ma trận kề từ δ-random bipartite ensemble ngẫu nhiên. Một cách trực quan, A và B như vậy được chọn sao cho bất kỳ hàng nào trong A hoặc B có nhiều nhất δ số 1. Bất kỳ phần tử Yij cho trước là ∑kl Aik˜XklBlj. Tuy nhiên, chỉ xấp xỉ δ2 phần tử trong tổng khác không. Do đó, Yij được biểu diễn như tổng của δ2 thuật ngữ từ tổng ∑kl Aik˜XklBlj. Sau đó chúng ta có thể biểu diễn nhiều phần tử từ Y bằng cách thay đổi phần tử nào được đặt hoặc không đặt thành không trong tổng này. Điều này đưa ra giải thích trực quan về cách sketching này hoạt động. Hơn nữa, sức mạnh của tính biểu cảm của sketching phụ thuộc chủ yếu vào các tham số m và δ. Ở đây, chúng ta có thể giới hạn kích thước yêu cầu cho m và δ sao cho nghiệm của Phương trình (1) dẫn đến ánh xạ một-một với xác suất cao.

E.2 Chứng minh còn lại

Cho rằng mỗi nguyên tử được phân phối đồng nhất và độc lập, cho N nguyên tử không bị cắt tỉa, vấn đề đặc trưng hóa cách các nguyên tử này được phân phối giữa các hàng hoặc cột tương tự như bài toán balls-and-bins nổi tiếng. Chúng ta cung cấp bổ đề trợ giúp để chứng minh rằng phương pháp cắt tỉa tạo ra ma trận thưa thớt phân phối. Chúng ta sử dụng bổ đề nổi tiếng từ Richa et al. [2000].

Bổ đề E.1. Xem xét vấn đề ném N bi độc lập và đồng đều ngẫu nhiên vào n thùng. Cho Xj là biến ngẫu nhiên đếm số lượng bi trong thùng thứ j. Với xác suất ít nhất 1−n−1/3, chúng ta có
maxj Xj≤3N/n.

Bây giờ chúng ta sử dụng bổ đề này để chứng minh distributed sparsity.

E.3 Chứng minh Bổ đề 5.1

Bổ đề 5.1. Với xác suất ít nhất 1−1/λl−(dl1)−1/3−(dl2)−1/3, chúng ta có rằng các đầu ra từ Thuật toán 1 là jr, jc-sparsely distributed nơi max( jr, jc)≤3λlmax( dl1, dl2)χ và λl∈R. Ở đây, χ=√(d+2)−√d/√(d+2).

Chứng minh. Đầu tiên chúng ta sẽ chứng minh ranh giới về số lượng nguyên tử không bị nén, biến ngẫu nhiên chúng ta sẽ gọi là N. Xác suất bất kỳ phần tử cho trước bị cắt tỉa là

P(Zi,j= 0) =∫∞−∞P(Zi,j= 0|Alij)·P(Alij)dAlij (9)
=∫∞−∞exp(−(Alij)2/dΨ) (1/√2πΨ) exp(−1/2(Alij)2/Ψ)dAlij (10)
=(√(d+ 2)−√d)/√(d+ 2) (11)

Do đó, số lượng phần tử khác không kỳ vọng sau cắt tỉa là E(N) =dl1dl2(√(d+2)−√d)/√(d+2). Sử dụng bất đẳng thức Markov, chúng ta có
P(N≥t)≤E(N)/t.
Ở đây, chúng ta đặt t=λidl1dl2(√(d+2)−√d)/√(d+2). Sử dụng điều này, chúng ta có với xác suất ít nhất 1−1/λi,
N≤λidl1dl2(√(d+ 2)−√d)/√(d+ 2).

Ở đây chúng ta có thể sử dụng Bổ đề E.1. Với các hàng, với xác suất ít nhất 1−(dl1)−1/3, chúng ta có số lượng nguyên tử không bị cắt tỉa tối đa trong bất kỳ hàng nào nhiều nhất
3N/dl1= 3λidl2(√(d+ 2)−√d)/√(d+ 2).

Tương tự, chúng ta có số lượng nguyên tử không bị cắt tỉa tối đa trong bất kỳ cột nào nhiều nhất
3N/dl2= 3λidl1(√(d+ 2)−√d)/√(d+ 2).

Do đó, chúng ta có điều này xảy ra với xác suất ít nhất 1−1/λi−(dl1)−1/3−(dl2)−1/3.

--- TRANG 16 ---
E.4 Chứng minh Định lý 5.2

Định lý 5.2. Với mỗi ma trận ˆAl, định nghĩa jl là max( jr, jc) nơi jr và jc là các hệ số distribution-sparsity cho ˆAl. Hơn nữa, với mỗi ma trận ˆAl, định nghĩa pl= max( dl1, dl2). Sau đó chúng ta có

R0(gA)≤ˆRγ(f) +O(√(∑l3λlχdl2dl1log2(pl) log(1/ρl)/n)).

Điều này đúng khi d được chọn sao cho γ≥ed01∏d l=1Ll∥Al∥2∑L l=1(ϵlΓl+ρlJl)/∥Al∥2 nơi Jl≤ O(χdl2dl1). Tuyên bố này đúng với xác suất ít nhất 1−[∑L l=1λ−1 l+ϵ−1 l+p−c l].

Chứng minh. Từ Bổ đề 5.1, chúng ta biết max( jr, jc)≤3λimax( dl2,dl1)(√(d+2)−√d)/√(d+2). Do đó, chúng ta có thể nén bất kỳ ma trận Al nào thành ma trận thưa thớt ˆAl và sau đó thêm vào ma trận nhỏ có kích thước (√(jlpllog(pl)))2 từ Định lý 5.1. Do đó, chúng ta có

(√(jlpllog(pl)))2≤3λidl2dl1(√(d+ 2)−√d)/√(d+ 2)log2(pl).

Bằng Định lý 3.1, chúng ta có
L0(gA)≤ˆLγ(f) +O(√(∑l3λidl2dl1(√(d+2)−√d)/√(d+2)log2(pl) log(1/ρl)/n)).

E.5 Chứng minh Định lý 6.2

Định lý 6.2. Với xác suất ít nhất 1−δ−LD−cG có lỗi tổng quát hóa của

R0(˜G)≤ˆRϵ+ϵρ(˜G) +O(√([nMd0,1log(DG)2+Ln2Mlog(DG)2] log(1/ρ)/n)).

Ở đây, ϵρ là lỗi nhỏ gây ra bởi rời rạc hóa.

Chứng minh. Chứng minh ranh giới tổng quát hóa bằng khung của chúng ta thường bao gồm một, chứng minh lỗi do nén bị giới hạn, và hai, thu được ranh giới về số lượng tham số. Malach et al. [2020] may mắn chứng minh cả hai. Chúng ta phát biểu lại ranh giới từ Arora et al. [2018]:

R0(˜G)≤ˆRγ(˜G) +O(√(qlogr/n)).

Từ Định lý 6.1, chúng ta có
supx∈X|F(x)−˜G(x)| ≤ϵ.

Đặt trực tiếp γ=ϵ+ϵρ thỏa mãn yêu cầu lỗi, nơi ϵρ là lỗi nhỏ gây ra do rời rạc hóa. Bây giờ, chúng ta phải tập trung vào việc giới hạn số lượng tham số trong mô hình. May mắn, Malach et al. [2020] cung cấp ranh giới hữu ích. Họ chỉ ra rằng lớp đầu tiên có xấp xỉ O(DFd10) tham số khác không, và các lớp còn lại của ˜G có xấp xỉ O(D2F) tham số khác không. Hơn nữa, từ chứng minh Định lý 2.1, họ chỉ ra rằng các tham số khác không này được phân phối đều trên các hàng và cột. Do đó, chúng ta có thể sử dụng khung matrix sketching để chỉ ra rằng chúng ta có thể nén tập hợp đầu ra từ Iterative Pruning thành tập hợp ma trận dày đặc nhỏ hơn. Cụ thể, các lớp giữa của ˜G như W˜Gi có thể được biểu diễn như ma trận nhỏ hơn có chiều m=O(DFlog(DG)) từ Định lý 5.1. Với lớp đầu tiên, chúng ta cũng có thể sử dụng matrix sketching để biểu diễn nó như ma trận có kích thước O(√(DFd0,1log(DG))). Bây giờ chúng ta có ranh giới thích hợp về số lượng tham số trong các mô hình sketched. Chúng ta áp dụng rời rạc hóa tầm thường bằng cách làm tròn giá trị gần nhất của ρ. Do đó, chúng ta có từ Arora et al. [2018]

R0(˜G)≤ˆRϵ+ϵρ(˜G) +O(√([DFd0,1log(DG)2+LD2Flog(DG)2] log(1/ρ)/n)).

Chúng ta có thể áp dụng matrix sketching cho mỗi hàng L với xác suất ít nhất 1−D−cG theo Định lý 5.1. Lỗi của mô hình đã cắt tỉa cũng bị giới hạn bởi ϵ với ít nhất xác suất 1−δ. Union bound các điều này lại với nhau chỉ ra rằng ranh giới này đúng với xác suất ít nhất 1−δ−LD−cG.

F Kết quả thực nghiệm bổ sung

Chúng tôi chỉ ra kết quả thực nghiệm chi tiết trên bộ dữ liệu MNIST và CIFAR10 trong Bảng 1 và Bảng 2 tương ứng, và bổ sung cho kết quả thu được trong Phần 7. Tất cả ranh giới được hiển thị trên thang logarit. Chúng tôi so sánh ranh giới với một số ranh giới tổng quát hóa dựa trên chuẩn tiêu chuẩn của Neyshabur et al. [2015], Bartlett et al. [2017], và Neyshabur et al. [2017]. Để so sánh ranh giới trên MNIST, chúng tôi sử dụng MLP với chiều ẩn 500, 784, 1000, 1500, 2000, và 2500 nơi độ sâu được giữ cố định. Chi tiết huấn luyện mô hình được chi tiết trong Phần 7. Chúng ta thấy rằng trên các chiều ẩn khác nhau, ranh giới tổng quát hóa của chúng tôi nhất quán tốt hơn các ranh giới tổng quát hóa khác. Trên các chiều ẩn khác nhau, lỗi tổng quát hóa thực dường như tương đối ổn định. So với các ranh giới khác, ranh giới tổng quát hóa của chúng tôi dường như ổn định hơn các ranh giới khác, tăng với tỷ lệ thấp hơn so với các ranh giới khác khi chiều ẩn tăng. Tuy nhiên, tiếc rằng chúng tôi không nắm bắt được sự độc lập có vẻ giữa chiều ẩn và lỗi tổng quát hóa thực. Với ranh giới của chúng tôi, điều này do thực tế rằng biên của mô hình đã huấn luyện không tăng đủ với sự tăng kích thước mô hình. Ranh giới của chúng tôi dự đoán lỗi tổng quát hóa của cắt tỉa theo biên của mô hình gốc. Nếu biên của mô hình gốc không tăng trong khi kích thước mô hình tăng, ranh giới của chúng tôi sẽ tăng. Do đó, ranh giới này cần thêm thông tin để nắm bắt tổng quát hóa chính xác hơn.

Ngoài ra, chúng tôi chỉ ra sự phụ thuộc của ranh giới vào số lượng epoch huấn luyện trong Hình 2, nơi chúng tôi lấy MLP gốc có độ sâu 5 và so sánh cách ranh giới tổng quát hóa và lỗi tổng quát hóa thực thay đổi qua các epoch. Cần lưu ý rằng ranh giới của chúng tôi được thu nhỏ để nằm trong cùng phạm vi với lỗi tổng quát hóa thực. Có sự khác biệt giữa các đường cong, cho thấy ranh giới của chúng tôi cần bao gồm thông tin bổ sung cần thiết để giải thích tổng quát hóa đầy đủ. Ranh giới của chúng tôi thực sự giảm theo thời gian khi biên tăng, mô phỏng lỗi tổng quát hóa thực. Sự khác biệt thú vị chính là đường cong xuống cho ranh giới của chúng tôi xảy ra ở hai nơi. Lần giảm đầu tiên trong ranh giới tổng quát hóa chỉ xảy ra do giảm lỗi tổng quát hóa, nhưng biên vẫn âm. Khi biên trở thành dương và tăng, ranh giới của chúng tôi từ từ bắt đầu giảm. Tại thời điểm này, tuy nhiên, lỗi tổng quát hóa thực dường như đã đạt đến minimum.

Trong Bảng 2, tất cả insight được nhận thấy trên bộ dữ liệu MNIST dường như mở rộng sang CIFAR10. Ranh giới tổng quát hóa của chúng tôi chặt chẽ hơn các ranh giới tổng quát hóa dựa trên chuẩn tiên tiến hiện có. Thực vậy lỗi của chúng tôi tốt hơn vài bậc độ lớn so với các ranh giới tổng quát hóa khác. Chúng tôi lưu ý rằng trong khi tất cả ranh giới tổng quát hóa ở đây đều kém hơn nhiều so với các đối tác MNIST, ranh giới tổng quát hóa của chúng tôi phản ánh chính xác nhất bước nhảy thực trong lỗi tổng quát hóa giữa MNIST và CIFAR10. Với cả chúng tôi và lỗi tổng quát hóa thực, các ranh giới khác nhau một bậc độ lớn giữa MNIST và CIFAR10. Tuy nhiên, các ranh giới khác khác nhau ít nhất 7 bậc độ lớn. Ranh giới của chúng tôi dường như nắm bắt nhiều hành vi của lỗi tổng quát hóa thực hơn các ranh giới khác trong vấn đề này.

--- TRANG 17 ---
PHƯƠNG PHÁP MNIST 500 MNIST 784 MNIST 1000 MNIST 1500 MNIST 2000 MNIST 2500
Neyshabur 2015 22.29 23.56 24.42 25.12 27.03 27.72
Neyshabur 2017 17.91 18.34 18.70 18.81 21.50 21.57
Bartlett 2017 11.51 11.68 11.87 11.70 13.96 13.81
Chúng tôi 3.36 3.77 4.00 4.40 4.73 4.96
Lỗi thực -3.76 -3.84 -3.80 -3.85 -3.86 -3.87

Bảng 1: Ranh giới tổng quát hóa trên thang logarit w.r.t. MNIST sử dụng MLP với các chiều khác nhau.

PHƯƠNG PHÁP CIFAR10
Neyshabur 2015 33.19
Neyshabur 2017 30.10
Bartlett 2017 22.40
Chúng tôi 4.68
Lỗi thực -2.41

Bảng 2: So sánh các ranh giới tổng quát hóa khác nhau trên bộ dữ liệu CIFAR10 trên thang logarit

[Hình 2: So sánh ranh giới trên MNIST.]
