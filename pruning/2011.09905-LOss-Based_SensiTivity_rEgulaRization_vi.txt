# 2011.09905.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2011.09905.pdf
# Kích thước tệp: 424969 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
LOBSTER: Điều chỉnh Độ nhạy dựa trên Hàm mất mát
hướng tới Mạng nơ-ron Thưa thớt Sâu
Enzo Tartaglione, Andrea Bragagnolo, Attilio Fiandrotti và Marco Grangetto
Đại học Torino, Italia
Tóm tắt
LOBSTER (Điều chỉnh Độ nhạy dựa trên Hàm mất mát) là một
phương pháp huấn luyện mạng nơ-ron có cấu trúc thưa thớt.
Gọi độ nhạy của một tham số mạng là sự biến thiên của
hàm mất mát so với sự biến thiên của tham số đó.
Các tham số có độ nhạy thấp, tức là có tác động ít đến
hàm mất mát khi bị nhiễu loạn, sẽ được thu nhỏ và sau đó
cắt tỉa để tạo ra mạng thưa thớt. Phương pháp của chúng tôi
cho phép huấn luyện mạng từ đầu, tức là không cần huấn luyện
sơ bộ hoặc quay lại. Các thử nghiệm trên nhiều kiến trúc và
bộ dữ liệu cho thấy tỷ lệ nén cạnh tranh với chi phí tính toán
tối thiểu.

1 Giới thiệu
Mạng nơ-ron nhân tạo (ANN) đạt được hiệu suất tiên tiến
trong nhiều tác vụ với cái giá của cấu trúc phức tạp có hàng
triệu tham số học được. Ví dụ, ResNet (He et al. 2016) bao
gồm hàng chục triệu tham số, tăng lên hàng trăm triệu cho
VGG-Net (Simonyan và Zisserman 2014). Tuy nhiên, số lượng
tham số lớn làm tổn hại khả năng triển khai mạng trên thiết bị
hạn chế bộ nhớ (ví dụ: nhúng, di động), đòi hỏi các kiến trúc
gọn nhẹ hơn với ít tham số hơn.

Độ phức tạp của ANN có thể được giảm bằng cách thực thi
cấu trúc mạng thưa thớt. Cụ thể, một số kết nối giữa các nơ-ron
có thể được cắt tỉa bằng cách đặt các tham số tương ứng bằng
không. Ngoài việc giảm tham số, một số nghiên cứu cũng đề
xuất các lợi ích khác từ việc cắt tỉa ANN, như cải thiện hiệu
suất trong các tình huống học chuyển giao (Liu, Wang, và Qiao
2017). Các phương pháp phổ biến như (Han et al. 2015), ví dụ,
giới thiệu một thuật ngữ điều chuẩn trong hàm chi phí với mục
tiêu thu nhỏ một số tham số về không. Tiếp theo, toán tử ngưỡng
xác định các tham số đã thu nhỏ thành không, cuối cùng thực
thi cấu trúc thưa thớt mong muốn. Tuy nhiên, các phương pháp
như vậy yêu cầu cấu trúc cần cắt tỉa phải được huấn luyện sơ
bộ thông qua gradient descent chuẩn, điều này cộng thêm vào
tổng thời gian học.

Công trình này đóng góp LOBSTER (Điều chỉnh Độ nhạy dựa
trên Hàm mất mát), một phương pháp học cấu trúc nơ-ron thưa
thớt. Trong bối cảnh này, hãy định nghĩa độ nhạy của tham số
của một ANN là đạo hàm của hàm mất mát so với tham số đó.
Một cách trực quan, các tham số có độ nhạy thấp có tác động
không đáng kể đến hàm mất mát khi bị nhiễu loạn, và do đó
phù hợp để thu nhỏ mà không làm tổn hại hiệu suất mạng.
Thực tế, LOBSTER thu nhỏ các tham số có độ nhạy thấp về
không bằng phương pháp điều chuẩn và cắt tỉa, đạt được cấu
trúc mạng thưa thớt.

So với các tài liệu tương tự (Han, Mao, và Dally 2016; Guo,
Yao, và Chen 2016; Gomez et al. 2019), LOBSTER không yêu
cầu giai đoạn huấn luyện sơ bộ để học cấu trúc tham chiếu
dày đặc cần cắt tỉa. Hơn nữa, khác với các phương pháp dựa
trên độ nhạy khác, LOBSTER tính toán độ nhạy bằng cách khai
thác gradient của hàm mất mát đã có sẵn, tránh tính toán đạo
hàm bổ sung (Mozer và Smolensky 1989; Tartaglione et al.
2018), hoặc đạo hàm bậc hai (LeCun, Denker, và Solla 1990).
Các thử nghiệm của chúng tôi, được thực hiện trên các cấu
trúc mạng và bộ dữ liệu khác nhau, cho thấy LOBSTER vượt
trội hơn nhiều đối thủ cạnh tranh trong nhiều tác vụ.

Phần còn lại của bài báo này được tổ chức như sau. Trong
Mục 2, chúng tôi xem xét tài liệu liên quan về kiến trúc nơ-ron
thưa thớt. Tiếp theo, trong Mục 3, chúng tôi mô tả phương pháp
huấn luyện mạng nơ-ron sao cho cấu trúc của nó thưa thớt.
Chúng tôi cung cấp tổng quan chung về kỹ thuật trong Mục 4.
Sau đó, trong Mục 5, chúng tôi thử nghiệm với sơ đồ huấn
luyện được đề xuất trên một số ANN sâu trên nhiều bộ dữ liệu
khác nhau. Cuối cùng, Mục 6 rút ra kết luận đồng thời cung
cấp hướng phát triển tiếp theo cho nghiên cứu tương lai.

2 Các Nghiên cứu Liên quan
Người ta đều biết rằng nhiều ANN, được huấn luyện trên một
số tác vụ, thường được tham số hóa quá mức (Mhaskar và
Poggio 2016; Brutzkus et al. 2018). Có nhiều cách để giảm
kích thước của một ANN. Trong nghiên cứu này, chúng tôi tập
trung vào vấn đề cắt tỉa: nó bao gồm việc phát hiện và loại bỏ
các tham số khỏi ANN mà không ảnh hưởng quá mức đến hiệu
suất của nó. Trong một nghiên cứu gần đây (Frankle và Carbin
2019), người ta quan sát thấy rằng chỉ một vài tham số thực
sự được cập nhật trong quá trình huấn luyện: điều này cho
thấy tất cả các tham số khác có thể được loại bỏ khỏi quá
trình học mà không ảnh hưởng đến hiệu suất. Mặc dù các
phương pháp tương tự đã được áp dụng từ nhiều năm trước
(Karnin 1990), phát hiện của họ đã đánh thức sự quan tâm
nghiên cứu xung quanh chủ đề này. Nhiều nỗ lực được dành
để làm cho các cơ chế cắt tỉa hiệu quả hơn: ví dụ, Wang et al.
cho thấy một số độ thưa thớt có thể đạt được bằng cách cắt
tỉa trọng số ngay từ đầu quá trình huấn luyện (Wang et al.
2020), hoặc Lee et al., với "SNIP" của họ, có thể cắt tỉa trọng
số theo cách một lần duy nhất (Lee, Ajanthan, và Torr 2019).

--- TRANG 2 ---
Tuy nhiên, các phương pháp này đạt được độ thưa thớt hạn
chế: chiến lược dựa trên cắt tỉa lặp lại, khi so sánh với các
phương pháp một lần hoặc vài lần, có thể đạt được độ thưa
thớt cao hơn (Tartaglione, Bragagnolo, và Grangetto 2020).
Mặc dù những tiến bộ công nghệ gần đây làm cho vấn đề này
trở nên thực tế và có liên quan đối với cộng đồng hướng tới
tối ưu hóa kiến trúc ANN, nó đã có nguồn gốc sâu xa trong
quá khứ.

Trong Le Cun et al. (LeCun, Denker, và Solla 1990), thông
tin từ đạo hàm bậc hai của hàm lỗi được tận dụng để xếp
hạng các tham số của mô hình đã huấn luyện trên cơ sở độ
nổi bật: điều này cho phép lựa chọn sự cân bằng giữa kích
thước của mạng (về số lượng tham số) và hiệu suất. Trong
cùng những năm đó, Mozer và Smolensky đề xuất kỹ thuật
tạo khung xương, một kỹ thuật để xác định, trên một mô hình
đã huấn luyện, các nơ-ron ít liên quan hơn, và loại bỏ chúng
(Mozer và Smolensky 1989). Điều này được thực hiện bằng
cách đánh giá tác động toàn cục của việc loại bỏ một nơ-ron
nhất định, được đánh giá như là phạt hàm lỗi từ một mô hình
đã được huấn luyện trước.

Những tiến bộ công nghệ gần đây cho phép các mô hình ANN
rất lớn, và đặt ra câu hỏi về hiệu quả của các thuật toán cắt
tỉa: mục tiêu của kỹ thuật là đạt được độ thưa thớt cao nhất
(tức là tỷ lệ phần trăm tối đa của các tham số bị loại bỏ) với
mất mát hiệu suất tối thiểu (mất mát độ chính xác từ mô hình
"không bị cắt tỉa"). Hướng tới mục tiêu này, một số phương
pháp khác nhau để cắt tỉa tồn tại.

Các phương pháp dựa trên dropout tạo thành một khả năng
khác để đạt được độ thưa thớt. Ví dụ, Sparse VD dựa vào
variational dropout để thúc đẩy độ thưa thớt (Molchanov,
Ashukha, và Vetrov 2017), đồng thời cung cấp một giải thích
Bayesian cho Gaussian dropout. Một phương pháp dựa trên
dropout khác là Targeted Dropout (Gomez et al. 2019): ở đó,
việc tinh chỉnh mô hình ANN tự củng cố độ thưa thớt của nó
bằng cách ngẫu nhiên loại bỏ các kết nối (hoặc toàn bộ đơn vị).

Một số phương pháp để giới thiệu độ thưa thớt trong ANN
cố gắng dựa vào bộ điều chuẩn ℓ₀ tối ưu, tuy nhiên, đây là
một thước đo không khả vi. Một nghiên cứu gần đây (Louizos,
Welling, và Kingma 2017) đề xuất một thước đo proxy khả vi
để khắc phục vấn đề này, tuy nhiên, giới thiệu một số chi phí
tính toán đáng kể. Có một phương pháp tổng thể tương tự,
trong một nghiên cứu khác, một bộ điều chuẩn dựa trên group
lasso có nhiệm vụ phân cụm các bộ lọc trong các lớp tích chập
được đề xuất (Wen et al. 2016). Tuy nhiên, kỹ thuật như vậy
không thể tổng quát hóa trực tiếp cho các lớp kết nối đầy đủ
cồng kềnh, nơi hầu hết độ phức tạp (về số lượng tham số) nằm.

Một phương pháp hợp lý hướng tới cắt tỉa tham số bao gồm
việc khai thác bộ điều chuẩn ℓ₂ trong khung thu nhỏ và cắt
tỉa. Cụ thể, một thuật ngữ điều chuẩn ℓ₂ chuẩn được đưa
vào hàm chi phí được tối thiểu hóa (để phạt độ lớn của các
tham số): tất cả các tham số giảm xuống dưới một ngưỡng
nào đó được xác định thành không, do đó học một cấu trúc
thưa thớt hơn (Han et al. 2015). Phương pháp như vậy hiệu
quả vì điều chuẩn thay thế các vấn đề không ổn định (ill-
posed) bằng các vấn đề gần đó và ổn định (well-posed) bằng
cách giới thiệu một prior trên các tham số (Groetsch 1993).
Tuy nhiên, như một nhược điểm, phương pháp này yêu cầu
huấn luyện sơ bộ để học giá trị ngưỡng; hơn nữa, tất cả các
tham số bị phạt một cách mù quáng, bình đẳng bởi chuẩn ℓ₂
của chúng: một số tham số, có thể gây ra lỗi lớn (nếu bị loại
bỏ), có thể giảm xuống dưới ngưỡng vì thuật ngữ điều chuẩn:
điều này gây ra sự không tối ưu cũng như không ổn định trong
quá trình cắt tỉa. Guo et al. đã cố gắng giải quyết vấn đề này
với DNS của họ (Guo, Yao, và Chen 2016): họ đề xuất một
quy trình thuật toán để sửa chữa việc cắt tỉa quá mức bằng
cách cho phép khôi phục các kết nối bị cắt đứt. Chuyển sang
các phương pháp thưa thớt không dựa trên cắt tỉa, Soft Weight
Sharing (SWS) (Ullrich, Welling, và Meeds 2019) chia sẻ các
tham số dư thừa giữa các lớp, dẫn đến ít tham số hơn cần
được lưu trữ. Các phương pháp dựa trên knowledge distillation,
như Few Samples Knowledge Distillation (FSKD) (Li et al.
2020), cũng là một lựa chọn thay thế để giảm kích thước của
một mô hình: có thể huấn luyện thành công một mạng học
sinh nhỏ từ một giáo viên lớn hơn, đã được huấn luyện trực
tiếp trên tác vụ. Lượng tử hóa cũng có thể được xem xét cho
cắt tỉa: Yang et al., ví dụ, xem xét vấn đề ternary hóa và cắt
tỉa một mô hình sâu đã được huấn luyện trước (Yang, He, và
Fan 2020). Các phương pháp gần đây khác chủ yếu tập trung
vào việc cắt tỉa các lớp tích chập hoặc tận dụng thuật toán
tối ưu hóa artificial bee colony (được gọi là ABCPruner) (Lin
et al. 2020) hoặc sử dụng một tập nhỏ đầu vào để đánh giá
điểm số nổi bật và xây dựng phân phối lấy mẫu (Liebenwein
et al. 2020).

Trong một nghiên cứu gần đây khác (Tartaglione et al. 2018),
người ta đề xuất đo lường mức độ thay đổi đầu ra mạng đối
với các nhiễu loạn nhỏ của một số tham số, và lặp lại việc phạt
chỉ những tham số tạo ra ít hoặc không có mất mát hiệu suất.
Tuy nhiên, phương pháp như vậy yêu cầu mạng đã được huấn
luyện để đo lường sự biến thiên của đầu ra mạng khi một tham
số bị nhiễu loạn, làm tăng tổng thời gian học.

Trong nghiên cứu này, chúng tôi khắc phục hạn chế cơ bản
của việc huấn luyện trước mạng, giới thiệu khái niệm độ nhạy
dựa trên hàm mất mát: nó chỉ phạt các tham số mà nhiễu loạn
nhỏ gây ra ít hoặc không có mất mát hiệu suất tại thời điểm
huấn luyện.

3 Điều chuẩn được đề xuất
Độ nhạy dựa trên Hàm mất mát

ANN thường được huấn luyện thông qua tối ưu hóa dựa trên
gradient descent, tức là tối thiểu hóa hàm mất mát. Các phương
pháp dựa trên mini-batch mẫu đã trở nên phổ biến vì chúng
cho phép tổng quát hóa tốt hơn so với học ngẫu nhiên trong
khi chúng hiệu quả về bộ nhớ và thời gian. Trong khung như
vậy, một tham số mạng wᵢ được cập nhật theo hướng trung
bình tối thiểu hóa mất mát trung bình cho minibatch, tức là
sử dụng stochastic gradient descent nổi tiếng hoặc các biến
thể của nó. Nếu độ lớn gradient gần bằng không, thì tham số
không được sửa đổi. Mục tiêu cuối cùng của chúng tôi là đánh
giá mức độ biến thiên giá trị của wᵢ sẽ ảnh hưởng đến lỗi
trên đầu ra mạng y. Chúng tôi thực hiện nỗ lực đầu tiên hướng
tới mục tiêu này bằng cách giới thiệu một nhiễu loạn nhỏ Δwᵢ
trên wᵢ và đo lường sự biến thiên của y như

Δy = Σₖ |∂yₖ/∂wᵢ| Δwᵢ ≈ Σₖ |∂yₖ/∂wᵢ|. (1)

--- TRANG 3 ---
[THIS IS TABLE: Table showing behavior of learning rate compared to standard value, with columns for conditions and corresponding values]

Bảng 1: Hành vi của η̃ so với η (η > 0).

Thật không may, việc đánh giá (1) là cụ thể và bị hạn chế
đối với vùng lân cận của đầu ra mạng. Chúng tôi muốn đánh
giá trực tiếp lỗi của đầu ra của mô hình ANN trên dữ liệu đã
học.

Hướng tới mục tiêu này, chúng tôi ước tính lỗi trên đầu ra
mạng gây ra bởi nhiễu loạn trên wᵢ như:

ΔL ≈ Δwᵢ ∂L/∂y ∂y/∂wᵢ = Δwᵢ ∂L/∂wᵢ. (2)

Việc sử dụng (2) thay cho (1) chuyển tiêu điểm từ đầu ra
sang lỗi của mạng. Cái sau là thông tin chính xác hơn để
đánh giá tác động thực sự của nhiễu loạn của một tham số
wᵢ nhất định. Hãy định nghĩa độ nhạy S cho một tham số
wᵢ nhất định như

S(L; wᵢ) = |∂L/∂wᵢ|. (3)

Các giá trị S lớn cho biết sự biến thiên lớn của hàm mất mát
đối với nhiễu loạn nhỏ của wᵢ.

Với định nghĩa độ nhạy ở trên, chúng ta có thể thúc đẩy cấu
trúc thưa thớt bằng cách cắt tỉa các tham số có cả độ nhạy
S thấp (tức là, trong một vùng phẳng của gradient hàm mất
mát, nơi một nhiễu loạn nhỏ của tham số có tác động không
đáng kể đến mất mát) và độ lớn thấp, giữ nguyên những
tham số có S lớn. Hướng tới mục tiêu này, chúng tôi đề xuất
quy tắc cập nhật tham số sau để thúc đẩy độ thưa thớt:

wᵢᵗ⁺¹ := wᵢᵗ - η ∂L/∂wᵢᵗ + λ wᵢᵗ/(1 + γS(L; wᵢᵗ))P(γS(L; wᵢᵗ)), (4)

trong đó

P(x) = α[1 - |x|], (5)

θ(·) là hàm một bước và λ, γ là hai siêu tham số dương.

Quy tắc Cập nhật

Trước đây chúng tôi đã giới thiệu một thước đo cho độ nhạy
có thể được sử dụng cả tại thời điểm huấn luyện. Cụ thể,
thay (3) vào (4) chúng ta có thể viết lại quy tắc cập nhật như:

wᵢᵗ⁺¹ = wᵢᵗ - η ∂L/∂wᵢᵗ - Λ(L; wᵢᵗ), (6)

trong đó

Λ(y; x) = λx P(γ|∂y/∂x|). (7)

[Các hình ảnh a, b, c, d cho thấy hiệu ứng của quy tắc cập nhật trên các tham số]

Hình 1: Hiệu ứng quy tắc cập nhật trên các tham số. Đường
đứt nét màu đỏ là tiếp tuyến với hàm mất mát tại điểm đen,
màu xanh là đóng góp SGD chuẩn, màu tím là weight decay
trong khi màu cam là đóng góp LOBSTER. Ở đây chúng ta
giả sử P(γ|∂L/∂wᵢ|) = 1.

Sau một số phép biến đổi đại số, chúng ta có thể viết lại (6) như

wᵢᵗ⁺¹ = wᵢᵗ - η̃Λ(L; wᵢᵗ) + η ∂L/∂wᵢᵗ [sign(∂L/∂wᵢᵗ) - λ̃Λ(L; wᵢᵗ)]. (8)

Trong (8), chúng ta quan sát hai thành phần khác nhau của
thuật ngữ điều chuẩn được đề xuất:

• một thuật ngữ giống weight decay Λ(L; wᵢ) được bật/tắt
bởi độ lớn của gradient trên tham số;
• một thuật ngữ hiệu chỉnh cho tốc độ học. Cụ thể, toàn bộ
quá trình học tuân theo một tốc độ học tương đương
η̃ = η [sign(∂L/∂wᵢ) - λ̃Λ(L; wᵢ)]. (9)

Hãy phân tích các hiệu chỉnh trong tốc độ học. Nếu |∂L/∂wᵢ| ≥ 1
(wᵢ có độ nhạy lớn), thì P(γ|∂L/∂wᵢ|) = 0 và Λ(L; wᵢ) = 0
và đóng góp chủ đạo đến từ gradient. Trong trường hợp này
quy tắc cập nhật của chúng ta giảm về GD cổ điển:

wᵢᵗ⁺¹ = wᵢᵗ - η ∂L/∂wᵢᵗ. (10)

Khi chúng ta xem xét wᵢ ít nhạy cảm hơn với |∂L/∂wᵢ| < 1,
chúng ta có Λ(L; wᵢ) = λwᵢ (thuật ngữ weight decay) và chúng
ta có thể phân biệt hai trường hợp phụ cho tốc độ học:

--- TRANG 4 ---
• nếu sign(∂L/∂wᵢ) = sign(wᵢ), thì η̃ < η (Hình 1a và
Hình 1d),
• nếu sign(∂L/∂wᵢ) ≠ sign(wᵢ), thì η̃ > η (Hình 1b và
Hình 1c).

Một sơ đồ của tất cả các trường hợp này có thể được tìm
thấy trong Bảng 1 và biểu diễn của các hiệu ứng có thể có
được hiển thị trong Hình 1.

Đóng góp đến từ Λ(L; wᵢ) nhằm tối thiểu hóa độ lớn tham
số, bỏ qua việc tối thiểu hóa mất mát. Nếu việc tối thiểu hóa
mất mát cũng có xu hướng tối thiểu hóa độ lớn, thì tốc độ
học tương đương được giảm. Ngược lại, khi gradient descent
có xu hướng tăng độ lớn, tốc độ học được tăng lên, để bù
đắp đóng góp đến từ Λ(L; wᵢ). Cơ chế này cho phép chúng
ta thành công trong tác vụ học đồng thời giới thiệu độ thưa
thớt.

Trong mục tiếp theo, chúng tôi sẽ trình bày chi tiết chiến
lược huấn luyện tổng thể, bao gồm một giai đoạn học và một
giai đoạn cắt tỉa nối tiếp.

4 Quy trình Huấn luyện

Mục này mô tả một quy trình để huấn luyện một mạng nơ-ron
thưa thớt N tận dụng quy tắc dựa trên độ nhạy ở trên để cập
nhật các tham số mạng. Chúng tôi giả sử rằng các tham số
đã được khởi tạo ngẫu nhiên, mặc dù quy trình cũng áp dụng
nếu mạng đã được huấn luyện trước. Quy trình được minh
họa trong Hình 2a và lặp lại qua hai giai đoạn như sau.

Giai đoạn Học

Trong giai đoạn học, ANN được huấn luyện lặp lại theo quy
tắc cập nhật (4) trên một số tập huấn luyện. Gọi e chỉ ra lần
lặp giai đoạn học hiện tại (tức là, epoch) và Nₑ đại diện cho
mạng (tức là, tập hợp các tham số học được) ở cuối lần lặp
thứ e. Cũng gọi Lₑ là mất mát được đo trên một số tập xác
thực ở cuối lần lặp thứ e và L̂ là mất mát tốt nhất (thấp nhất)
đo được cho đến nay trên N̂ (mạng với mất mát xác thực
thấp nhất cho đến nay). Như điều kiện ban đầu, chúng ta giả
sử N̂ = N₀. Nếu Lₑ < L̂, tham chiếu đến mạng tốt nhất được
cập nhật là N̂ = Nₑ, L̂ = Lₑ. Chúng ta lặp lại giai đoạn học
Nu cho đến khi mất mát xác thực tốt nhất Lₑ không giảm trong
PWE lần lặp liên tiếp của giai đoạn học (chúng ta nói bộ điều
chuẩn đã đạt đến một cao nguyên hiệu suất). Tại thời điểm
đó, chúng ta chuyển sang giai đoạn cắt tỉa.

Chúng tôi cung cấp N̂ làm đầu vào cho giai đoạn cắt tỉa,
nơi một số tham số đã được thu nhỏ về không bởi bộ điều
chuẩn dựa trên độ nhạy của chúng tôi.

Giai đoạn Cắt tỉa

Tóm lại, trong giai đoạn cắt tỉa, các tham số có độ lớn dưới
giá trị ngưỡng T được xác định thành không, cuối cùng làm
thưa thớt cấu trúc mạng như được hiển thị trong Hình 2b.
Cụ thể, chúng ta tìm kiếm T lớn nhất làm xấu đi mất mát
phân loại L̂ nhiều nhất bởi một lượng tương đối TWT:

L̂ᵇ = (1 + TWT)L̂, (11)

trong đó L̂ᵇ được gọi là biên giới mất mát. T được tìm thấy
bằng phương pháp chia đôi, khởi tạo T với độ lớn trung bình
của các tham số khác không trong mạng. Sau đó, chúng ta
áp dụng ngưỡng T lên N̂ thu được mạng cắt tỉa N' với mất
mát L' trên tập xác thực. Tại lần lặp cắt tỉa tiếp theo, chúng
ta cập nhật T như sau:

• nếu L̂ᵇ ≥ L' mạng chịu đựng được việc cắt tỉa nhiều tham
số hơn, vậy T được tăng lên;
• nếu L̂ᵇ < L' thì quá nhiều tham số đã bị cắt tỉa và chúng
ta cần khôi phục một số: chúng ta giảm T.

Giai đoạn cắt tỉa kết thúc khi L̂ᵇ = L' và chúng ta quan sát
rằng L̂ᵇ < L' cho bất kỳ ngưỡng mới T + ε với ε > 0. Một
khi T được tìm thấy, tất cả các tham số có độ lớn dưới T
được xác định thành không, tức là chúng bị cắt tỉa vĩnh viễn.
Nếu ít nhất một tham số đã bị cắt tỉa trong lần lặp cuối cùng
của giai đoạn cắt tỉa, một lần lặp mới của giai đoạn điều
chuẩn theo sau; nếu không, quy trình kết thúc trả về mạng
đã huấn luyện, thưa thớt.

5 Kết quả

Trong mục này, chúng tôi đánh giá thực nghiệm LOBSTER
trên nhiều kiến trúc và bộ dữ liệu thường được sử dụng làm
benchmark trong tài liệu:

• LeNet-300 trên MNIST (Hình 3a),
• LeNet-5 trên MNIST (Hình 3b),
• LeNet-5 trên Fashion-MNIST (Hình 3c),
• ResNet-32 trên CIFAR-10 (Hình 3d),
• ResNet-18 trên ImageNet (Hình 3e),
• ResNet-101 trên ImageNet (Hình 3f).

Chúng tôi so sánh với các phương pháp tiên tiến khác được
giới thiệu trong Mục 2 bất cứ khi nào có số liệu công khai.
Ngoài những điều này, chúng tôi cũng thực hiện một nghiên
cứu ablation với bộ điều chuẩn dựa trên ℓ₂ và chiến lược cắt
tỉa được đề xuất của chúng tôi (như đã thảo luận trong Mục
4). Hiệu suất được đo bằng độ thưa thớt mô hình đạt được
so với lỗi phân loại (lỗi Top-1 hoặc Top-5). Độ thưa thớt
mạng được định nghĩa ở đây là tỷ lệ phần trăm các tham số
bị cắt tỉa trong mô hình ANN. Các thuật toán của chúng tôi
được thực hiện bằng Python, sử dụng PyTorch 1.2 và các
mô phỏng được chạy trên GPU RTX2080 TI NVIDIA. Tất cả
các siêu tham số đã được điều chỉnh thông qua grid-search.
Kích thước tập xác thực cho tất cả các thử nghiệm là 5k.¹
Đối với tất cả các bộ dữ liệu, các giai đoạn học và cắt tỉa
diễn ra trên một phần chia ngẫu nhiên của tập huấn luyện,
trong khi các số được báo cáo dưới đây liên quan đến tập
kiểm tra.

LeNet-300 trên MNIST

Như một thử nghiệm đầu tiên, chúng tôi huấn luyện một kiến
trúc LeNet-300 thưa thớt (LeCun et al. 1998), bao gồm ba
lớp kết nối đầy đủ với lần lượt 300, 100 và 10 nơ-ron. Chúng
tôi huấn luyện mạng trên bộ dữ liệu MNIST, gồm 60k ảnh
huấn luyện và 10k ảnh kiểm tra thang độ xám 28×28 pixel,
mô tả các chữ số viết tay. Bắt đầu từ một mạng được khởi
tạo ngẫu nhiên, chúng tôi huấn luyện LeNet-300 thông qua
SGD với tốc độ học η = 0.1, λ = 10⁻⁴, PWE = 20 epoch
và TWT = 0.05.

Tài liệu liên quan báo cáo một số kết quả nén có thể được
phân nhóm thành hai nhóm tương ứng với tỷ lệ lỗi phân loại
khoảng 1.65% và 1.95%. Hình 3a cung cấp kết quả cho quy
trình được đề xuất. Phương pháp của chúng tôi đạt được độ
thưa thớt cao hơn so với các phương pháp được tìm thấy
trong tài liệu. Điều này đặc biệt đáng chú ý xung quanh lỗi
phân loại 1.65% (dưới bên trái trong Hình 3a), nơi chúng
tôi đạt được gần gấp đôi độ thưa thớt của phương pháp tốt
thứ hai. LOBSTER cũng đạt được độ thưa thớt cao nhất cho
phạm vi lỗi cao hơn (phía bên phải của biểu đồ), đặc biệt
thắng về số lượng tham số được loại bỏ khỏi lớp kết nối
đầy đủ đầu tiên (lớp lớn nhất, bao gồm 235k tham số), trong
đó chúng tôi quan sát chỉ 0.59% tham số sống sót.

¹Mã nguồn được cung cấp trong tài liệu bổ sung và sẽ được
công khai khi bài báo được chấp nhận.

--- TRANG 5 ---
(a)
(b)

Hình 2: Quy trình huấn luyện hoàn chỉnh của LOBSTER (a) và chi tiết giai đoạn cắt tỉa (b).

LeNet-5 trên MNIST và Fashion-MNIST

Tiếp theo, chúng tôi thử nghiệm trên phiên bản caffe của kiến
trúc LeNet-5, bao gồm hai lớp tích chập và hai lớp kết nối
đầy đủ. Một lần nữa, chúng tôi sử dụng một mạng được khởi
tạo ngẫu nhiên, được huấn luyện thông qua SGD với tốc độ
học η = 0.1, λ = 10⁻⁴, PWE = 20 epoch và TWT = 0.05.
Kết quả được hiển thị trong Hình 3b. Ngay cả với kiến trúc
tích chập, chúng tôi thu được một mạng nhỏ cạnh tranh với
độ thưa thớt 99.57%. Ở tỷ lệ nén cao hơn, Sparse VD nhỉnh
hơn tất cả các phương pháp khác trong thử nghiệm LeNet5-
MNIST. Chúng tôi quan sát rằng LOBSTER, trong thử nghiệm
này, làm thưa thớt lớp tích chập đầu tiên (~22% độ thưa
thớt) nhiều hơn so với giải pháp Sparse VD (~33%). Cụ thể,
LOBSTER cắt tỉa 14 bộ lọc trong số 20 bộ lọc ban đầu ở lớp
đầu tiên (hay nói cách khác, chỉ 6 bộ lọc sống sót, và chứa
tất cả các tham số không bị cắt tỉa). Chúng tôi đưa ra giả
thuyết rằng, trong trường hợp Sparse VD và cho bộ dữ liệu
cụ thể này, việc trích xuất một loạt tính năng đa dạng hơn
ở lớp tích chập đầu tiên, vừa giúp ích cho tác vụ phân loại
(do đó lỗi Top-1 thấp hơn) và cho phép loại bỏ nhiều tham
số hơn ở các lớp tiếp theo (độ thưa thớt được cải thiện nhẹ).
Tuy nhiên, vì chúng ta đã trên 99% độ thưa thớt, sự khác
biệt giữa hai kỹ thuật là tối thiểu.

Để tăng độ khó của tác vụ huấn luyện, chúng tôi thử nghiệm
trên phân loại bộ dữ liệu Fashion-MNIST (Xiao, Rasul, và
Vollgraf 2017), một lần nữa sử dụng LeNet5. Bộ dữ liệu này
có cùng kích thước và định dạng ảnh với bộ dữ liệu MNIST,
nhưng nó chứa hình ảnh các mặt hàng quần áo, dẫn đến
phân phối không thưa thớt của giá trị cường độ pixel. Vì
các hình ảnh không thưa thớt như vậy, bộ dữ liệu này khó
phân loại hơn MNIST một cách nổi tiếng. Đối với thử nghiệm
này, chúng tôi huấn luyện mạng từ đầu bằng SGD với η =
0.1, λ = 10⁻⁴, PWE = 20 epoch và TWT = 0.1. Kết quả được
hiển thị trong Hình 3c.

F-MNIST là một bộ dữ liệu thách thức hơn về bản chất so
với MNIST, vì vậy độ thưa thớt có thể đạt được thấp hơn.
Tuy nhiên, phương pháp được đề xuất vẫn đạt được độ thưa
thớt cao hơn so với các phương pháp khác, loại bỏ tỷ lệ
phần trăm tham số cao hơn, đặc biệt là trong các lớp kết
nối đầy đủ, trong khi duy trì khả năng tổng quát hóa tốt.
Trong trường hợp này, chúng tôi quan sát rằng lớp đầu tiên
ít bị thưa thớt nhất: đây là tác động của độ phức tạp cao
hơn của tác vụ phân loại, đòi hỏi nhiều tính năng hơn cần
được trích xuất.

ResNet-32 trên CIFAR-10

Để đánh giá cách phương pháp của chúng tôi mở rộng cho
các kiến trúc hiện đại, sâu hơn, chúng tôi áp dụng nó trên
triển khai PyTorch của mạng ResNet-32 (He et al. 2015)
phân loại bộ dữ liệu CIFAR-10.² Bộ dữ liệu này bao gồm
60k ảnh RGB 32×32 được chia thành 10 lớp (50k ảnh huấn
luyện và 10k ảnh kiểm tra). Chúng tôi huấn luyện mạng
bằng SGD với momentum μ = 0.9, λ = 10⁻⁶, PWE = 10 và
TWT = 0. Việc huấn luyện đầy đủ được thực hiện trong 11k
epoch. Phương pháp của chúng tôi hoạt động tốt trên tác vụ
này và vượt trội hơn các kỹ thuật tiên tiến khác. Hơn nữa,
LOBSTER cải thiện khả năng tổng quát hóa của mạng bằng
cách giảm lỗi Top-1 cơ sở từ 7.37% xuống 7.33% của mạng
thưa thớt trong khi loại bỏ 80.11% tham số. Hiệu ứng này
có thể là do bản thân kỹ thuật LOBSTER, tự điều chỉnh

²https://github.com/akamaster/pytorch_resnet_cifar10

--- TRANG 6 ---
[Các biểu đồ hiệu suất (Lỗi Top-1) so với tỷ lệ tham số bị cắt tỉa cho LOBSTER và các phương pháp tiên tiến khác trên các kiến trúc và bộ dữ liệu khác nhau được hiển thị trong 6 hình từ (a) đến (f)]

Hình 3: Hiệu suất (lỗi Top-1) so với tỷ lệ tham số bị cắt tỉa cho LOBSTER và các phương pháp tiên tiến khác trên các kiến trúc và bộ dữ liệu khác nhau.

--- TRANG 7 ---
[THIS IS TABLE: Comparison table between LOBSTER and ℓ₂+pruning showing performance metrics across different datasets and architectures]
Bảng 2: So sánh giữa LOBSTER và ℓ₂+cắt tỉa như trong Hình 3 (chỉ báo cáo kết quả độ thưa thớt tốt nhất).

điều chuẩn trên các tham số như được giải thích trong Mục 3.

ResNet trên ImageNet

Cuối cùng, chúng tôi tiếp tục mở rộng cả đầu ra và độ phức tạp của bài toán phân loại bằng cách kiểm tra phương pháp được đề xuất trên mạng qua bộ dữ liệu ImageNet nổi tiếng (ILSVRC-2012), bao gồm hơn 1.2 triệu ảnh huấn luyện, tổng cộng 1k lớp. Đối với kiểm tra này, chúng tôi sử dụng SGD với momentum μ = 0.9, λ = 10⁻⁶ và TWT = 0. Việc huấn luyện đầy đủ kéo dài 95 epoch. Do hạn chế về thời gian, chúng tôi quyết định sử dụng mạng đã được huấn luyện trước do thư viện torchvision cung cấp.³ Hình 3e hiển thị kết quả cho ResNet-18 trong khi Hình 3f hiển thị kết quả cho ResNet-101. Ngay cả trong tình huống này, LOBSTER chứng tỏ đặc biệt hiệu quả: chúng tôi có thể loại bỏ, không có mất mát hiệu suất, 37.04% tham số từ ResNet-18 và 81.58% từ ResNet-101.

Nghiên cứu Ablation

Như một nghiên cứu ablation cuối cùng, chúng tôi thay thế bộ điều chuẩn dựa trên độ nhạy của mình bằng một bộ điều chuẩn ℓ₂ đơn giản hơn trong sơ đồ học của chúng tôi trong Hình 2. Sơ đồ như vậy "ℓ₂+cắt tỉa" áp dụng đồng nhất phạt ℓ₂ cho tất cả các tham số bất kể đóng góp của chúng vào mất mát. Sơ đồ này có thể so sánh với (Han et al. 2015), nhưng được tăng cường với cùng chiến lược cắt tỉa với ngưỡng thích ứng được hiển thị trong Hình 2b. Một so sánh giữa LOBSTER và ℓ₂+cắt tỉa được báo cáo trong Bảng 2. Trong tất cả các thử nghiệm, chúng tôi quan sát rằng việc loại bỏ bộ điều chuẩn dựa trên độ nhạy làm suy giảm hiệu suất. Thử nghiệm này xác minh vai trò của điều chuẩn dựa trên độ nhạy trong hiệu suất của sơ đồ chúng tôi. Cuối cùng, Bảng 2 cũng báo cáo độ phức tạp suy luận tương ứng theo FLOPs. Đối với cùng hoặc thấp hơn lỗi Top-1, LOBSTER mang lại lợi ích như ít phép toán hơn tại thời điểm suy luận và gợi ý sự hiện diện của một số cấu trúc trong độ thưa thớt đạt được bởi LOBSTER.

³https://pytorch.org/docs/stable/torchvision/models.html

6 Kết luận

Chúng tôi đã trình bày LOBSTER, một phương pháp điều chuẩn phù hợp để huấn luyện mạng nơ-ron với cấu trúc thưa thớt mà không cần huấn luyện sơ bộ. Khác với điều chuẩn ℓ₂, LOBSTER nhận thức được đóng góp toàn cục của tham số vào hàm mất mát và tự điều chỉnh hiệu ứng điều chuẩn trên tham số tùy thuộc vào các yếu tố như kiến trúc ANN hoặc bản thân bài toán huấn luyện (nói cách khác, bộ dữ liệu). Hơn nữa, việc điều chỉnh các siêu tham số của nó dễ dàng và ngưỡng tối ưu cho cắt tỉa tham số được tự xác định bởi phương pháp được đề xuất sử dụng tập xác thực. LOBSTER đạt được kết quả cạnh tranh từ các kiến trúc nông như LeNet-300 và LeNet-5 đến các cấu trúc sâu hơn như ResNet trên ImageNet. Trong những tình huống này, chúng tôi đã quan sát được sự thúc đẩy do phương pháp điều chuẩn được đề xuất cung cấp so với các phương pháp kém nhận thức hơn như điều chuẩn ℓ₂, về mặt độ thưa thớt đạt được.

Nghiên cứu tương lai bao gồm việc mở rộng LOBSTER để đạt được độ thưa thớt với một cấu trúc và đánh giá kỹ lưỡng về tiết kiệm về dung lượng bộ nhớ.

Tài liệu tham khảo

Brutzkus, A.; Globerson, A.; Malach, E.; và Shalev-Shwartz, S. 2018. SGD learns over-parameterized networks that provably generalize on linearly separable data.

Frankle, J.; và Carbin, M. 2019. The lottery ticket hypothesis: Finding sparse, trainable neural networks.

Gomez, A. N.; Zhang, I.; Swersky, K.; Gal, Y.; và Hinton, G. E. 2019. Learning Sparse Networks Using Targeted Dropout. CoRR abs/1905.13678.

Groetsch, C. W. 1993. Inverse Problems in the Mathematical Sciences. Vieweg.

Guo, Y.; Yao, A.; và Chen, Y. 2016. Dynamic network surgery for efficient dnns. In Advances In Neural Information Processing Systems, 1379-1387.

Han, S.; Mao, H.; và Dally, W. 2016. Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding.

--- TRANG 8 ---
Han, S.; Pool, J.; Tran, J.; và Dally, W. 2015. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems, 1135-1143.

He, K.; Zhang, X.; Ren, S.; và Sun, J. 2015. Deep Residual Learning for Image Recognition. CoRR abs/1512.03385.

He, K.; Zhang, X.; Ren, S.; và Sun, J. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 770-778.

Karnin, E. D. 1990. A simple procedure for pruning back-propagation trained neural networks. IEEE transactions on neural networks 1(2): 239-242.

LeCun, Y.; Bottou, L.; Bengio, Y.; và Haffner, P. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11): 2278 - 2324.

LeCun, Y.; Denker, J. S.; và Solla, S. A. 1990. Optimal brain damage. In Advances in neural information processing systems, 598-605.

Lee, N.; Ajanthan, T.; và Torr, P. 2019. SNIP: Single-shot Network Pruning based on Connection Sensitivity. ArXiv abs/1810.02340.

Li, T.; Li, J.; Liu, Z.; và Zhang, C. 2020. Few sample knowledge distillation for efficient network compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14639-14647.

Liebenwein, L.; Baykal, C.; Lang, H.; Feldman, D.; và Rus, D. 2020. Provable Filter Pruning for Efficient Neural Networks. In International Conference on Learning Representations.

Lin, M.; Ji, R.; Zhang, Y.; Zhang, B.; Wu, Y.; và Tian, Y. 2020. Channel Pruning via Automatic Structure Search. In Bessiere, C., ed., Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, 673-679. International Joint Conferences on Artificial Intelligence Organization. doi:10.24963/ijcai.2020/94. Main track.

Liu, J.; Wang, Y.; và Qiao, Y. 2017. Sparse deep transfer learning for convolutional neural network. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, 2245-2251.

Louizos, C.; Welling, M.; và Kingma, D. P. 2017. Learning Sparse Neural Networks through L₀ Regularization. arXiv preprint arXiv:1712.01312.

Mhaskar, H. N.; và Poggio, T. 2016. Deep vs. shallow networks: An approximation theory perspective. Analysis and Applications 14(06): 829-848.

Molchanov, D.; Ashukha, A.; và Vetrov, D. 2017. Variational dropout sparsifies deep neural networks. volume 5, 3854-3863.

Mozer, M. C.; và Smolensky, P. 1989. Skeletonization: A technique for trimming the fat from a network via relevance assessment. In Advances in neural information processing systems, 107-115.

Simonyan, K.; và Zisserman, A. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.

Tartaglione, E.; Bragagnolo, A.; và Grangetto, M. 2020. Pruning artificial neural networks: a way to find well-generalizing, high-entropy sharp minima. arXiv preprint arXiv:2004.14765.

Tartaglione, E.; Lepsøy, S.; Fiandrotti, A.; và Francini, G. 2018. Learning sparse neural networks via sensitivity-driven regularization. In Advances in Neural Information Processing Systems, 3878-3888.

Ullrich, K.; Welling, M.; và Meeds, E. 2019. Soft weight-sharing for neural network compression.

Wang, Y.; Zhang, X.; Xie, L.; Zhou, J.; Su, H.; Zhang, B.; và Hu, X. 2020. Pruning from Scratch. In AAAI, 12273-12280.

Wen, W.; Wu, C.; Wang, Y.; Chen, Y.; và Li, H. 2016. Learning structured sparsity in deep neural networks. In Advances in Neural Information Processing Systems, 2074-2082.

Xiao, H.; Rasul, K.; và Vollgraf, R. 2017. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. CoRR abs/1708.07747.

Yang, L.; He, Z.; và Fan, D. 2020. Harmonious Coexistence of Structured Weight Pruning and Ternarization for Deep Neural Networks. In AAAI, 6623-6630.
