VỀ MẠNG CON MỀM CHO HỌC TĂNG DẦN LỚP VỚI ÍT MẪU

Haeyong Kang, Jaehong Yoon, Sultan Rizky Madjid, Sung Ju Hwang, và Chang D. Yoo
Viện Khoa học và Công nghệ Tiên tiến Hàn Quốc (KAIST)
291 Đại học-ro, Yuseong-gu, Daejeon
{haeyong.kang,jaehong.yoon,suulkyy,sjhwang82,cd_yoo}@kaist.ac.kr

TÓM TẮT

Được truyền cảm hứng từ Giả thuyết Vé số May mắn Có điều chỉnh, nói rằng các mạng con mượt mà (không nhị phân) cạnh tranh tồn tại trong một mạng dày đặc, chúng tôi đề xuất một phương pháp học tăng dần lớp với ít mẫu được gọi là Mạng Con Mềm (SoftNet). Mục tiêu của chúng tôi là học một chuỗi các phiên một cách tăng dần, trong đó mỗi phiên chỉ bao gồm một vài trường hợp huấn luyện cho mỗi lớp trong khi bảo tồn kiến thức của những phiên đã học trước đó. SoftNet học đồng thời các trọng số mô hình và mặt nạ mềm không nhị phân thích ứng tại một phiên huấn luyện cơ sở trong đó mỗi mặt nạ bao gồm mạng con chính và mạng con phụ; mạng con trước nhằm giảm thiểu quên thảm khốc trong quá trình huấn luyện, và mạng con sau nhằm tránh quá khớp với một vài mẫu trong mỗi phiên huấn luyện mới. Chúng tôi cung cấp các xác thực thực nghiệm toàn diện chứng minh rằng SoftNet của chúng tôi giải quyết hiệu quả vấn đề học tăng dần với ít mẫu bằng cách vượt qua hiệu suất của các baseline tiên tiến nhất trên các bộ dữ liệu chuẩn. Mã nguồn công khai có sẵn tại https://github.com/ihaeyong/SoftNet-FSCIL.

1 GIỚI THIỆU

Học Suốt đời, hay Học Liên tục, là một mô hình học tập để mở rộng kiến thức và kỹ năng thông qua việc huấn luyện tuần tự nhiều nhiệm vụ (Thrun, 1995). Theo khả năng tiếp cận định danh nhiệm vụ trong quá trình huấn luyện và suy luận, cộng đồng thường phân loại lĩnh vực này thành các vấn đề cụ thể, như tăng dần theo nhiệm vụ (Pfülb và Gepperth, 2019; Delange et al., 2021; Yoon et al., 2020; Kang et al., 2022), tăng dần theo lớp (Chaudhry et al., 2018; Kuzborskij et al., 2013; Li và Hoiem, 2017; Rebuffi et al., 2017; Kemker và Kanan, 2017; Castro et al., 2018; Hou et al., 2019; Wu et al., 2019), và học liên tục không nhiệm vụ (Aljundi et al., 2019; Jin et al., 2021; Pham et al., 2022; Harrison et al., 2020). Trong khi các kịch bản tiêu chuẩn cho học liên tục giả định có một số lượng đủ lớn các trường hợp cho mỗi nhiệm vụ, một người học suốt đời cho các ứng dụng thực tế thường gặp khó khăn với việc thiếu các trường hợp huấn luyện cho mỗi vấn đề cần giải quyết. Bài báo này nhằm giải quyết vấn đề hạn chế các trường hợp huấn luyện cho Học Tăng dần Lớp (CIL) thực tế, được gọi là CIL Ít Mẫu (FSCIL) (Ren et al., 2019; Chen và Lee, 2020; Tao et al., 2020; Zhang et al., 2021; Cheraghian et al., 2021; Shi et al., 2021).

Tuy nhiên, có hai thách thức quan trọng trong việc giải quyết các vấn đề FSCIL: quên thảm khốc và quá khớp. Quên thảm khốc (Goodfellow et al., 2013; Kirkpatrick et al., 2017) hay Giao thoa Thảm khốc McCloskey và Cohen (1989) là một hiện tượng trong đó một người học liên tục mất kiến thức nhiệm vụ đã học trước đó bằng cách cập nhật các trọng số để thích ứng với các nhiệm vụ mới, dẫn đến suy giảm hiệu suất đáng kể trên các nhiệm vụ trước đó. Sự trôi dạt kiến thức không mong muốn như vậy là không thể đảo ngược vì kịch bản không cho phép mô hình xem lại dữ liệu nhiệm vụ trong quá khứ. Các công trình gần đây đề xuất giảm thiểu quên thảm khốc cho học tăng dần lớp, thường được phân loại theo nhiều hướng, như các phương pháp dựa trên ràng buộc (Rebuffi et al., 2017; Castro et al., 2018; Hou et al., 2018; 2019; Wu et al., 2019), dựa trên bộ nhớ (Rebuffi et al., 2017; Chen và Lee, 2020; Mazumder et al., 2021; Shi et al., 2021), và dựa trên kiến trúc (Mazumder et al., 2021; Serra et al., 2018; Mallya và Lazebnik, 2018; Kang et al., 2022). Tuy nhiên, chúng tôi lưu ý rằng quên thảm khốc trở nên thách thức hơn nữa trong FSCIL. Do lượng dữ liệu huấn luyện nhỏ cho các nhiệm vụ mới, mô hình có xu hướng quá khớp nghiêm trọng với các lớp mới và nhanh chóng quên các lớp cũ, làm xấu đi hiệu suất mô hình.

Trong khi đó, một số công trình giải quyết các vấn đề quá khớp cho học liên tục từ nhiều góc độ khác nhau. NCM (Hou et al., 2019) và BiC (Wu et al., 2019) nêu bật vấn đề thiên lệch dự đoán trong quá trình huấn luyện tuần tự mà các mô hình có xu hướng dự đoán dữ liệu thành các lớp trong các nhiệm vụ được huấn luyện gần đây. OCS (Yoon et al., 2022) giải quyết các vấn đề mất cân bằng lớp cho học liên tục dựa trên rehearsal, trong đó số lượng trường hợp tại mỗi lớp thay đổi theo nhiệm vụ để mô hình sẽ thực hiện huấn luyện thiên lệch trên các lớp chiếm ưu thế. Tuy nhiên, các công trình này không xem xét các vấn đề quá khớp gây ra bởi việc huấn luyện một chuỗi các nhiệm vụ ít mẫu. FSLL (Mazumder et al., 2021) giải quyết quá khớp cho CIL ít mẫu bằng cách chia một phần các tham số mô hình cho các phiên khác nhau thông qua nhiều bước con của việc tái định danh và lựa chọn trọng số lặp đi lặp lại. Tuy nhiên, điều này dẫn đến tính toán không hiệu quả.

Để triển khai một mô hình CIL ít mẫu thực tế, chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả được đặt tên là SoftNet, giảm thiểu hiệu quả quên thảm khốc và quá khớp. Được thúc đẩy bởi Giả thuyết Vé số May mắn (Frankle và Carbin, 2019), giả thuyết về sự tồn tại của các mạng con cạnh tranh (vé thắng) trong mạng nơ-ron dày đặc được khởi tạo ngẫu nhiên, chúng tôi đề xuất một mô hình mới cho CIL Ít mẫu, được gọi là Giả thuyết Vé số May mắn Có điều chỉnh:

Giả thuyết Vé số May mắn Có điều chỉnh (RLTH). Một mạng nơ-ron dày đặc được khởi tạo ngẫu nhiên chứa một mạng con có điều chỉnh có thể giữ lại kiến thức lớp trước trong khi cung cấp không gian để học kiến thức lớp mới thông qua việc huấn luyện cô lập của mạng con.

Dựa trên RLTH, chúng tôi đề xuất một phương pháp, được gọi là Mạng Con Mềm (SoftNet), được minh họa trong Hình 1. Đầu tiên, SoftNet học đồng thời mô hình dày đặc được khởi tạo ngẫu nhiên (Hình 1 (a)) và mặt nạ mềm m ∈ [0,1]^|θ| liên quan đến Mạng con mềm (Hình 1 (b)) trên việc huấn luyện phiên cơ sở; mặt nạ mềm bao gồm phần chính của các tham số mô hình m = 1 và phần phụ m < 1 trong đó m = 1 được lấy bởi c% hàng đầu của các tham số mô hình và m < 1 được lấy bởi phần còn lại (100-top-c%) được lấy mẫu từ phân phối đều. Sau đó, chúng tôi đóng băng phần chính của các trọng số mạng con được huấn luyện trước để duy trì kiến thức lớp trước và chỉ cập nhật phần phụ của các trọng số cho kiến thức lớp mới (Hình 1 (c)).

Chúng tôi tóm tắt các đóng góp chính như sau:
• Bài báo này trình bày một phương pháp dựa trên masking mới, Mạng Con Mềm (SoftNet), giải quyết hai thách thức quan trọng trong học tăng dần lớp với ít mẫu (FSCIL), được biết đến là quên thảm khốc và quá khớp.
• SoftNet của chúng tôi huấn luyện hai loại mặt nạ không nhị phân khác nhau (mạng con) để giải quyết FSCIL, ngăn người học liên tục quên các phiên trước và quá khớp đồng thời.
• Chúng tôi thực hiện một nghiên cứu thực nghiệm toàn diện về SoftNet với nhiều phương pháp học tăng dần lớp. Phương pháp của chúng tôi vượt trội đáng kể so với các baseline mạnh trên các nhiệm vụ chuẩn cho các vấn đề FSCIL.

2 CÔNG TRÌNH LIÊN QUAN

Quên Thảm khốc. Nhiều công trình gần đây đã đạt được tiến bộ đáng kể trong việc giải quyết các thách thức của quên thảm khốc trong học suốt đời. Cụ thể, các phương pháp Dựa trên Kiến trúc (Mallya et al., 2018; Serrà et al., 2018; Li et al., 2019) sử dụng khả năng bổ sung để mở rộng (Xu và Zhu, 2018; Yoon et al., 2018) hoặc cô lập (Rusu et al., 2016) các tham số mô hình, do đó tránh sự giao thoa kiến thức trong quá trình học liên tục; SupSup (Wortsman et al., 2020) phân bổ các tham số mô hình dành riêng cho các nhiệm vụ khác nhau. Gần đây, Chen et al. (2021); Kang et al. (2022) cho thấy sự tồn tại của một mạng con thưa thớt, được gọi là vé thắng, hoạt động tốt trên tất cả các nhiệm vụ trong quá trình học liên tục. Tuy nhiên, nhiều phương pháp dựa trên mạng con không tương thích với thiết lập FSCIL vì việc thực hiện suy luận nhiệm vụ dưới sự mất cân bằng dữ liệu là thách thức. FSLL (Mazumder et al., 2021) nhằm tìm kiếm các mạng con cụ thể cho phiên trong khi bảo tồn trọng số cho các phiên trước đó cho học tăng dần ít mẫu. Tuy nhiên, quá trình mở rộng bao gồm một loạt các bước huấn luyện và cắt tỉa khác, đòi hỏi thời gian huấn luyện và chi phí tính toán quá mức. Ngược lại, phương pháp đề xuất của chúng tôi, SoftNet, học đồng thời mô hình và mặt nạ mượt mà (tức là, không nhị phân) thích ứng nhiệm vụ của mạng con liên quan đến phiên cơ sở trong khi lựa chọn một tập con thiết yếu của các trọng số mô hình cho phiên sắp tới. Hơn nữa, các mặt nạ mượt mà hoạt động như các bộ điều chỉnh ngăn chặn quá khớp khi học các lớp mới.

Mạng con mềm. Các công trình gần đây với việc cổng phụ thuộc bối cảnh của các không gian con (He và Jaeger, 2018), tham số (Mallya và Lazebnik, 2018; He et al., 2019; Mazumder et al., 2021), hoặc các lớp (Serra et al., 2018) của một mạng nơ-ron sâu duy nhất đã chứng minh hiệu quả của nó trong việc giải quyết quên thảm khốc trong quá trình học liên tục. Masse et al. (2018) kết hợp cổng phụ thuộc bối cảnh với các ràng buộc ngăn chặn thay đổi đáng kể trong trọng số mô hình, như SI (Zenke et al., 2017) và EWC (Kirkpatrick et al., 2017), đạt được sự gia tăng hiệu suất hơn nữa so với việc sử dụng chúng một mình. Một minima phẳng cũng có thể được coi là việc có được các không gian con. Các công trình trước đây đã cho thấy rằng một bộ tối thiểu hóa phẳng mạnh mẽ hơn đối với các nhiễu loạn ngẫu nhiên (Hinton và Van Camp, 1993; Hochreiter và Schmidhuber, 1994; Jiang et al., 2019). Gần đây, Shi et al. (2021) cho thấy rằng việc có được minima mất mát phẳng trong phiên cơ sở, đại diện cho phiên nhiệm vụ đầu tiên với đủ trường hợp huấn luyện, là cần thiết để giảm thiểu quên thảm khốc trong FSCIL. Để giảm thiểu việc quên, họ đã cập nhật các trọng số mô hình trên đường viền mất mát phẳng đã có được. Trong công trình của chúng tôi, bằng cách lựa chọn các mạng con (Frankle và Carbin, 2019; Zhou et al., 2019; Wortsman et al., 2019; Ramanujan et al., 2020; Kang et al., 2022; Chijiwa et al., 2022) và tối ưu hóa các tham số mạng con trong một không gian con, chúng tôi đề xuất một phương pháp mới để bảo tồn kiến thức đã học từ một phiên cơ sở trên một mạng con chính và học các phiên mới thông qua các mạng con phụ có điều chỉnh.

3 MẠNG CON MỀM CHO HỌC TĂNG DẦN LỚP VỚI ÍT MẪU

3.1 PHÁT BIỂU VẤN ĐỀ

Nhiều công trình đã cố gắng giảm thiểu các vấn đề quên thảm khốc trong học tăng dần lớp bằng cách sử dụng chưng cất kiến thức, xem lại một tập con của các mẫu trước, hoặc cô lập các tham số mô hình thiết yếu để giữ lại kiến thức lớp trước ngay cả sau khi mô hình mất khả năng tiếp cận chúng. Tuy nhiên, vì kịch bản học tăng dần lớp với ít mẫu xem xét các nhiệm vụ/phiên tiếp theo chứa một lượng nhỏ dữ liệu huấn luyện, mô hình có xu hướng quá khớp nghiêm trọng với các lớp mới, khiến việc tinh chỉnh mô hình đã huấn luyện trước trên một vài mẫu trở nên khó khăn. Ngoài ra, quá trình tinh chỉnh thường dẫn đến quên thảm khốc kiến thức lớp cơ sở. Kết quả là, việc điều chỉnh là không thể thiếu trong các mô hình để tránh quên và ngăn mô hình quá khớp với các mẫu lớp mới bằng cách chỉ cập nhật các tham số được chọn để học trong phiên mới.

Học Tăng dần Lớp với Ít mẫu (FSCIL) nhằm học các phiên mới chỉ với một vài ví dụ một cách liên tục. Một mô hình FSCIL học một chuỗi T phiên huấn luyện {D₁, ..., D_T}, trong đó D_t = {z_t^i = (x_t^i, y_t^i)}_{i=1}^{n_t} là dữ liệu huấn luyện của phiên t và x_t^i là một ví dụ của lớp y_t^i ∈ O_t. Trong FSCIL, phiên cơ sở D₁ thường chứa một số lượng lớn các lớp với đủ dữ liệu huấn luyện cho mỗi lớp. Ngược lại, các phiên tiếp theo (t ≥ 2) sẽ chỉ chứa một số ít các lớp với một vài mẫu huấn luyện cho mỗi lớp, ví dụ, phiên thứ t D_t thường được trình bày như một nhiệm vụ N-way K-shot. Trong mỗi phiên huấn luyện t, mô hình chỉ có thể truy cập dữ liệu huấn luyện D_t và một vài ví dụ được lưu trữ trong phiên trước. Khi việc huấn luyện phiên t hoàn thành, chúng tôi đánh giá mô hình trên các mẫu kiểm tra từ tất cả các lớp O = ⋃_{i=1}^t O_i, trong đó O_i ∩ O_j≠i = ∅ cho ∀i, j ≤ T.

Xem xét một thiết lập học có giám sát trong đó T phiên đến với một người học suốt đời f(·; θ) được tham số hóa bởi các trọng số mô hình θ theo thứ tự tuần tự. Một kịch bản học tăng dần lớp với ít mẫu nhằm học các lớp trong một chuỗi các phiên mà không quên thảm khốc. Trong phiên huấn luyện t, mô hình giải quyết thủ tục tối ưu hóa sau:

θ* = minimize_θ (1/n_t) ∑_{i=1}^{n_t} L_t(f(x_t^i; θ), y_t^i), (1)

trong đó L_t là mất mát phân loại như entropy chéo, và n_t là số lượng trường hợp cho phiên t.

3.2 HUẤN LUYỆN DỰA TRÊN MẠNG CON CHO HỌC TĂNG DẦN LỚP VỚI ÍT MẪU

Vì những người học suốt đời thường áp dụng các mạng nơ-ron dày đặc có quá nhiều tham số để cho phép tự do tài nguyên cho các lớp hoặc nhiệm vụ tương lai, việc cập nhật toàn bộ trọng số trong mạng nơ-ron cho các nhiệm vụ ít mẫu thường không được ưa thích và thường dẫn đến vấn đề quá khớp. Để vượt qua các hạn chế trong FSCIL, chúng tôi tập trung vào việc cập nhật các trọng số một phần trong mạng nơ-ron khi một nhiệm vụ mới đến. Tập hợp các trọng số một phần mong muốn, được gọi là mạng con, có thể đạt được hiệu suất ngang bằng hoặc thậm chí tốt hơn với các động lực sau: (1) Giả thuyết Vé số May mắn (Frankle và Carbin, 2019) cho thấy sự tồn tại của một mạng con hoạt động tốt như mạng dày đặc, và (2) Mạng con được giảm kích thước đáng kể từ mạng dày đặc làm giảm kích thước mở rộng của bộ giải quyết trong khi cung cấp thêm khả năng để học các phiên hoặc nhiệm vụ mới.

Chúng tôi đầu tiên đề xuất mục tiêu được gọi là HardNet như sau: cho các tham số mạng nơ-ron dày đặc θ, mặt nạ chú ý nhị phân m_t mô tả mạng con tối ưu cho phiên t sao cho |m_t| nhỏ hơn khả năng mô hình dày đặc |θ|. Tuy nhiên, các mạng con nhị phân hóa như vậy m_t ∈ {0,1}^|θ| không thể điều chỉnh các tham số còn lại trong mạng dày đặc cho các phiên tương lai trong khi giải quyết các vấn đề nhiệm vụ quá khứ một cách hiệu quả về chi phí và bộ nhớ. Trong FSCIL, độ chính xác kiểm tra của phiên cơ sở giảm đáng kể khi nó tiến hành học các phiên tuần tự vì mạng con của m = 1 đóng vai trò quan trọng trong việc duy trì kiến thức lớp cơ sở. Để kết thúc điều này, chúng tôi đề xuất một mạng con mềm m_t ∈ [0,1]^|θ| thay vì mạng con nhị phân hóa. Nó mang lại sự linh hoạt hơn để tinh chỉnh một phần nhỏ của mạng con mềm trong khi cố định phần còn lại để giữ lại kiến thức lớp cơ sở cho FSCIL. Như vậy, chúng tôi tìm mạng con mềm thông qua mục tiêu sau:

m_t* = minimize_{m_t∈[0,1]^|θ|} (1/n_t) ∑_{i=1}^{n_t} L_t(f(x_t^i; θ ⊙ m_t), y_t^i) ≡ J
subject to |m_t| ≤ c · |θ|: (2)

trong đó mất mát phiên J = L_t(f(x_t^i; θ); y_t^i), độ thưa thớt mạng con c · |θ| (được sử dụng như tỷ lệ phần trăm % của các tham số mô hình được chọn trong phần tiếp theo), và ⊙ biểu thị tích element-wise. Trong phần tiếp theo, chúng tôi mô tả cách có được mạng con mềm m_t* sử dụng tiêu chí dựa trên độ lớn (RLTH) trong khi giảm thiểu mất mát phiên đồng thời.

3.3 CÓ ĐƯỢC MẠNG CON MỀM THÔNG QUA VÉ THẮNG BỔ SUNG

Hãy để mỗi trọng số được liên kết với một tham số có thể học mà chúng tôi gọi là điểm trọng số, xác định số lượng tầm quan trọng của trọng số liên quan. Nói cách khác, chúng tôi tuyên bố một trọng số có điểm cao hơn là quan trọng hơn. Lúc đầu, chúng tôi tìm một mạng con θ* = θ ⊙ m_t* của mạng nơ-ron dày đặc và sau đó gán nó như một bộ giải quyết của phiên hiện tại t. Các mạng con liên quan đến mỗi phiên học đồng thời trọng số mô hình θ và mặt nạ nhị phân m_t. Cho một mục tiêu L_t, chúng tôi tối ưu hóa như sau:

θ*, m_t* = minimize_{θ,s} L_t(θ ⊙ m_t, D_t): (3)

trong đó m_t được thu được bằng cách áp dụng hàm chỉ thị 1_c trên điểm trọng số s. Lưu ý 1_c(s) = 1 nếu s thuộc về c% điểm hàng đầu và 0 ngược lại.

Trong quá trình tối ưu hóa cho FSCIL, tuy nhiên, chúng tôi xem xét hai vấn đề chính: (1) Quên thảm khốc: cập nhật tất cả θ ⊙ m_{t-1} khi huấn luyện cho các phiên mới sẽ gây ra sự giao thoa với các trọng số được phân bổ cho các nhiệm vụ trước; do đó, chúng tôi cần đóng băng tất cả các tham số đã học trước đó θ ⊙ m_{t-1}; (2) Quá khớp: mạng con cũng gặp phải các vấn đề quá khớp khi huấn luyện một nhiệm vụ tăng dần trên một vài mẫu, như vậy, chúng tôi cần cập nhật một vài tham số không liên quan đến kiến thức nhiệm vụ trước., tức là, θ ⊙ (1 - m_{t-1}).

Để có được các mạng con tối ưu giảm thiểu hai vấn đề, chúng tôi định nghĩa một mạng con mềm bằng cách chia mạng nơ-ron dày đặc thành hai phần - một là mạng con chính m_{major}, và một khác là mạng con phụ m_{minor}. Mạng con mềm được định nghĩa tuân theo:

m_{soft} = m_{major} ⊕ m_{minor}, (4)

trong đó m_{major} là một mặt nạ nhị phân và m_{minor} ∼ U(0,1) và ⊕ biểu thị tổng element-wise. Như vậy, một mặt nạ mềm được cho như m_t* ∈ [0,1]^|θ| trong Eq.3. Trong tất cả thiết lập thực nghiệm FSCIL, m_{major} duy trì kiến thức nhiệm vụ cơ sở t = 1 trong khi m_{minor} có được kiến thức nhiệm vụ mới t ≥ 2. Sau đó, với tốc độ học phiên cơ sở α, θ được cập nhật như sau: θ ← θ - α ∂L/∂m_{soft} ⊙ ∂m_{soft}/∂θ hiệu quả điều chỉnh các trọng số của các mạng con cho học tăng dần. Các mạng con được thu được bởi hàm chỉ thị luôn có giá trị gradient là 0; do đó, việc cập nhật điểm trọng số s với gradient mất mát của nó là không thể. Để cập nhật điểm trọng số, chúng tôi sử dụng Ước lượng Straight-through (Hinton, 2012; Bengio et al., 2013; Ramanujan et al., 2020) trong lượt truyền ngược. Cụ thể, chúng tôi bỏ qua các đạo hàm của hàm chỉ thị và cập nhật điểm trọng số s ← s - α ∂L/∂s ⊙ ∂m_{soft}/∂s, trong đó m_{soft} = 1 để khám phá mạng con tối ưu cho huấn luyện phiên cơ sở. Thủ tục tối ưu hóa Mạng con mềm của chúng tôi được tóm tắt trong Thuật toán 1. Một khi một mạng con mềm duy nhất m_{soft} được thu được trong phiên cơ sở, sau đó chúng tôi sử dụng mạng con mềm cho toàn bộ các phiên mới mà không cập nhật.

Thuật toán 1 Mạng Con Mềm (SoftNet)
input {D_t}_{t=1}^T, trọng số mô hình θ, và trọng số điểm s, khả năng theo lớp c
1: // Huấn luyện trên các lớp cơ sở t = 1
2: Khởi tạo ngẫu nhiên θ và s.
3: for epoch e = 1, 2, ... do
4: Thu được mặt nạ mềm m_{soft} của m_{major} và m_{minor} ∼ U(0,1) tại mỗi lớp
5: for batch b_t ∈ D_t do
6: Tính L_{base}(θ ⊙ m_{soft}, b_t) bởi Eq. 3
7: θ ← θ - α ∂L/∂m_{soft} ⊙ ∂m_{soft}/∂θ
8: s ← s - α ∂L/∂s ⊙ ∂m_{soft}/∂s
9: end for
10: end for
11: // Học tăng dần t ≥ 2
12: Kết hợp dữ liệu huấn luyện D_t và các mẫu được lưu trong các phiên ít mẫu trước
13: for epoch e = 1, 2, ... do
14: for batch b_t ∈ D_t do
15: Tính L_m(θ ⊙ m_{soft}, b_t) bởi Eq. 5
16: θ ← θ - α ∂L/∂m_{minor} ⊙ ∂m_{minor}/∂θ
17: end for
18: end for
output các tham số mô hình θ, s, và m_{soft}.

4 HỌC TĂNG DẦN CHO MẠNG CON MỀM

Bây giờ chúng tôi mô tả thủ tục tổng thể của phương pháp học/suy luận tăng dần dựa trên cắt tỉa mềm của chúng tôi, bao gồm giai đoạn huấn luyện với một phép đo thông tin được chuẩn hóa trong Phần 4.1, như được theo dõi bởi công trình trước (Shi et al., 2021), và giai đoạn suy luận trong Phần 4.2.

4.1 HUẤN LUYỆN MẠNG CON MỀM TĂNG DẦN

Huấn luyện Cơ sở (t = 1). Trong phiên học cơ sở, chúng tôi tối ưu hóa tham số mạng con mềm θ (bao gồm một lớp kết nối đầy đủ như một bộ phân loại) và điểm trọng số s với mất mát entropy chéo đồng thời sử dụng các ví dụ huấn luyện của D₁.

Huấn luyện Tăng dần (t ≥ 2). Trong các phiên học ít mẫu tăng dần (t ≥ 2), được tận dụng bởi m_{soft}, chúng tôi tinh chỉnh một vài tham số phụ m_{minor} của mạng con mềm để học các lớp mới. Vì m_{minor} < 1, mạng con mềm giảm thiểu quá khớp của một vài mẫu. Hơn nữa, thay vì khoảng cách Euclidean (Shi et al., 2021), chúng tôi sử dụng một thuật toán phân loại dựa trên metric với khoảng cách cosine để tinh chỉnh một vài tham số được chọn. Trong một số trường hợp, khoảng cách Euclidean không thể đưa ra khoảng cách thực giữa các biểu diễn, đặc biệt khi hai điểm có cùng khoảng cách từ các nguyên mẫu không rơi vào cùng một lớp. Ngược lại, các biểu diễn có khoảng cách cosine thấp được đặt trong cùng hướng từ gốc, cung cấp một phép đo thông tin được chuẩn hóa. Chúng tôi định nghĩa hàm mất mát là:

L_m(z; θ ⊙ m_{soft}) = ∑_{z∈D} ∑_{o∈O} 1(y=o) log(e^{-d(p_o,f(x;θ⊙m_{soft}))} / ∑_{o_k∈O} e^{-d(p_{o_k},f(x;θ⊙m_{soft}))}) (5)

trong đó d(·,·) biểu thị khoảng cách cosine, p_o là nguyên mẫu của lớp o, O = ⋃_{i=1}^t O_i đề cập đến tất cả các lớp gặp phải, và D = D_t ∪ P biểu thị sự hợp nhất của dữ liệu huấn luyện hiện tại D_t và tập mẫu P = {p₂, ..., p_{t-1}}, trong đó P_{te} (2 ≤ te < t) là tập hợp các mẫu được lưu trong phiên te. Lưu ý rằng các nguyên mẫu của các lớp mới được tính bởi p_o = (1/N_o) ∑_i 1(y_i = o)f(x_i; θ ⊙ m_{soft}) và những của các lớp cơ sở được lưu trong phiên cơ sở, và N_o biểu thị số lượng hình ảnh huấn luyện của lớp o. Chúng tôi cũng lưu các nguyên mẫu của tất cả các lớp trong O_t cho đánh giá sau này.

4.2 SUY LUẬN CHO MẠNG CON MỀM TĂNG DẦN

Trong mỗi phiên, suy luận cũng được tiến hành bởi một thuật toán phân loại trung bình lớp gần nhất đơn giản (NCM) (Mensink et al., 2013; Shi et al., 2021) để so sánh công bằng. Cụ thể, tất cả các mẫu huấn luyện và kiểm tra được ánh xạ đến không gian nhúng của bộ trích xuất đặc trưng f, và khoảng cách Euclidean d_u(·,·) được sử dụng để đo sự tương tự giữa chúng. Bộ phân loại đưa ra chỉ số nguyên mẫu thứ k o_k* = arg min_{o∈O} d_u(f(x; θ ⊙ m_{soft}), p_o) như đầu ra.

5 THỰC NGHIỆM

Chúng tôi giới thiệu thiết lập thực nghiệm trong Phần 5.1. Sau đó, chúng tôi đánh giá thực nghiệm các mạng con mềm của chúng tôi cho học tăng dần ít mẫu và chứng minh hiệu quả của nó thông qua so sánh với các phương pháp tiên tiến nhất và các mạng con vanilla trong các phần tiếp theo.

5.1 THIẾT LẬP THỰC NGHIỆM

Bộ dữ liệu. Để xác thực hiệu quả của mạng con mềm, chúng tôi tuân theo thiết lập thực nghiệm FSCIL tiêu chuẩn. Chúng tôi chọn ngẫu nhiên 60 lớp làm lớp cơ sở và 40 lớp còn lại làm lớp mới cho CIFAR-100 và miniImageNet. Trong mỗi phiên học tăng dần, chúng tôi xây dựng các nhiệm vụ 5-way 5-shot bằng cách chọn ngẫu nhiên năm lớp và lấy mẫu năm ví dụ huấn luyện cho mỗi lớp.

Baseline. Chúng tôi chủ yếu so sánh SoftNet của chúng tôi với các phương pháp dựa trên kiến trúc cho FSCIL: FSLL (Mazumder et al., 2021) lựa chọn các tham số quan trọng cho mỗi phiên, và HardNet, đại diện cho một mạng con nhị phân. Hơn nữa, chúng tôi so sánh các phương pháp FSCIL khác như iCaRL (Rebuffi et al., 2017), Rebalance (Hou et al., 2019), TOPIC (Tao et al., 2020), IDLVQ-C (Chen và Lee, 2020), và F2M (Shi et al., 2021). Chúng tôi cũng bao gồm một phương pháp huấn luyện chung (Shi et al., 2021) sử dụng tất cả dữ liệu đã thấy trước đó, bao gồm các nhiệm vụ cơ sở và ít mẫu tiếp theo để huấn luyện như một tham chiếu. Hơn nữa, chúng tôi cố định phương pháp huấn luyện lại bộ phân loại (cRT) (Kang et al., 2019) cho phân loại long-tailed được huấn luyện với tất cả dữ liệu gặp phải như giới hạn trên được ước lượng.

Chi tiết thực nghiệm. Các thực nghiệm được tiến hành với NVIDIA GPU RTX8000 trên CUDA 11.0. Chúng tôi cũng chia ngẫu nhiên mỗi bộ dữ liệu thành nhiều phiên. Chúng tôi chạy mỗi thuật toán mười lần cho mỗi bộ dữ liệu và báo cáo độ chính xác trung bình của chúng. Chúng tôi áp dụng ResNet18 (He et al., 2016) làm mạng backbone. Để tăng cường dữ liệu, chúng tôi sử dụng cắt ngẫu nhiên tiêu chuẩn và lật ngang. Trong giai đoạn huấn luyện phiên cơ sở, chúng tôi chọn c% trọng số hàng đầu tại mỗi lớp và có được các mạng con mềm tối ưu với độ chính xác xác thực tốt nhất. Trong mỗi phiên học tăng dần ít mẫu, tổng số epoch huấn luyện là 6, và tốc độ học là 0.02. Chúng tôi huấn luyện các mẫu phiên lớp mới sử dụng một vài trọng số phụ của mạng con mềm (lớp Conv4x của ResNet18 và lớp Conv3x của ResNet20) được thu được bởi việc học phiên cơ sở. Chúng tôi chỉ định thêm chi tiết thực nghiệm trong Phụ lục A.

5.2 KẾT QUẢ VÀ SO SÁNH

Chúng tôi so sánh SoftNet với các phương pháp dựa trên kiến trúc - FSLL và HardNet. Chúng tôi chọn FSLL làm baseline dựa trên kiến trúc vì nó lựa chọn các tham số quan trọng để có được kiến thức lớp cũ/mới. Kết quả dựa trên kiến trúc trên CIFAR-100 và miniImageNet được trình bày trong Bảng 1 và Bảng 2 tương ứng. Hiệu suất của HardNet cho thấy hiệu quả của các mạng con đi với ít khả năng mô hình hơn so với các mạng dày đặc. Để nhấn mạnh quan điểm của chúng tôi, chúng tôi phát hiện rằng ResNet18, với khoảng 50% tham số, đạt được hiệu suất tương đương với FSLL trên CIFAR-100 và miniImageNet. Ngoài ra, hiệu suất của ResNet20 với 30% tham số (HardNet) tương đương với những của FSLL trên CIFAR-100, như được ghi chú trong Phụ lục của Bảng 9 và Bảng 11, bao gồm hiệu suất (Hình 4 và Hình 5) và độ mượt trong các biểu đồ t-SNE (Hình 6).

Kết quả thực nghiệm được chuẩn bị để phân tích hiệu suất tổng thể của SoftNet theo độ thưa thớt và bộ dữ liệu như được hiển thị trong Hình 2. Khi chúng tôi tăng số lượng tham số được sử dụng bởi SoftNet, chúng tôi đạt được lợi ích hiệu suất trên cả hai bộ dữ liệu chuẩn. Phương sai hiệu suất của độ thưa thớt SoftNet dường như phụ thuộc vào bộ dữ liệu từ thực tế rằng phương sai hiệu suất trên CIFAR-100 ít hơn so với trên miniImageNet. Ngoài ra, SoftNet giữ lại kiến thức phiên trước thành công trong cả hai thực nghiệm như được mô tả trong đường nét đứt, và hiệu suất của SoftNet (c = 60.0%) trên phiên lớp mới (8, 9) của CIFAR-100 so với những của SoftNet (c = 80.0%) như được mô tả trong đường nét đứt-chấm. Từ các kết quả này, chúng tôi có thể mong đợi rằng hiệu suất tốt nhất phụ thuộc vào số lượng tham số và tính chất của bộ dữ liệu. Chúng tôi có thêm kết quả về so sánh HardNet và SoftNet trong Phụ lục B.

SoftNet của chúng tôi vượt trội hơn các phương pháp tiên tiến nhất và cRT, được sử dụng như giới hạn trên ước lượng của FSCIL (Shi et al., 2021) như được hiển thị trong Bảng 1 và Bảng 2. Hơn nữa, Hình 3 thể hiện hiệu suất xuất sắc của SoftNet trên CIFAR-100 và miniImageNet. SoftNet cung cấp một giới hạn trên mới trên mỗi bộ dữ liệu, vượt trội hơn cRT, trong khi HardNet cung cấp các baseline mới giữa các phương pháp dựa trên cắt tỉa.

5.3 ĐỘ CHÍNH XÁC THEO LỚP

Trong các phiên học tăng dần ít mẫu, chúng tôi huấn luyện các mẫu phiên lớp mới sử dụng một vài trọng số phụ m_{minor} của lớp cụ thể. Đồng thời, chúng tôi hoàn toàn cố định các trọng số còn lại để điều tra hiệu suất tốt nhất như được hiển thị trong Bảng 3. Hiệu suất tốt nhất bao gồm tinh chỉnh tại lớp Conv5x với c = 97%. Điều đó có nghĩa là các đặc trưng được tính bởi lớp thấp hơn là chung và có thể tái sử dụng trong các lớp khác nhau. Mặt khác, các đặc trưng từ lớp cao hơn là cụ thể và phụ thuộc cao vào bộ dữ liệu.

5.4 ĐỘ CHÍNH XÁC THEO KIẾN TRÚC

Tùy thuộc vào kiến trúc, hiệu suất của các mạng con thay đổi, và độ thưa thớt cũng là một khác: ResNet18 có xu hướng sử dụng tham số dày đặc, trong khi ResNet20 có xu hướng sử dụng tham số thưa thớt trên CIFAR-100 cho 5-way 5-shot như được hiển thị trong Bảng 4. Chúng tôi quan sát thấy rằng SoftNet với ResNet20 có một giải pháp thưa thớt hơn như c = 90% so với ResNet18 trên thiết lập CIFAR-100 FSCIL này. Từ các quan sát này, SoftNet của chúng tôi có thể tác động đáng kể đến tìm kiếm kiến trúc mạng nơ-ron sâu - nó giúp tìm kiếm kiến trúc thưa thớt và cụ thể cho nhiệm vụ.

5.5 THẢO LUẬN

Dựa trên nghiên cứu thực nghiệm kỹ lưỡng của chúng tôi, chúng tôi khám phá các sự thật sau: (1) Tùy thuộc vào kiến trúc, hiệu suất của các mạng con thay đổi, và độ thưa thớt cũng là một khác: ResNet18 có xu hướng sử dụng tham số dày đặc, trong khi ResNet20 có xu hướng sử dụng tham số thưa thớt trên các thiết lập CIFAR-100 FSCIL. Kết quả này cung cấp cho mô hình dựa trên cắt tỉa chung một manh mối ẩn. (2) Nói chung, các chiến lược tinh chỉnh là thiết yếu trong việc giữ lại kiến thức trước và học kiến thức mới. Chúng tôi phát hiện rằng hiệu suất thay đổi tùy thuộc vào việc tinh chỉnh một lớp Conv thông qua việc kiểm tra theo lớp. Cuối cùng, (3) từ kết quả thực nghiệm tổng thể, việc học phiên cơ sở là quan trọng đối với những người học suốt đời để có được hiệu suất tổng quát trong FSCIL.

6 KẾT LUẬN

Được truyền cảm hứng từ Giả thuyết Vé số May mắn Có điều chỉnh (RLTH), giả thuyết rằng các mạng con mượt mà tồn tại trong một mạng dày đặc, chúng tôi đề xuất Mạng Con Mềm (SoftNet); một chiến lược học tăng dần bảo tồn kiến thức lớp đã học và học những kiến thức mới hơn. Cụ thể hơn, SoftNet học đồng thời các trọng số mô hình và mặt nạ mềm thích ứng để giảm thiểu quên thảm khốc và tránh quá khớp các mẫu ít mới trong FSCIL. Cuối cùng, chúng tôi so sánh một nghiên cứu thực nghiệm toàn diện về SoftNet với nhiều phương pháp học tăng dần lớp. Các thực nghiệm mở rộng trên các nhiệm vụ chuẩn chứng minh cách phương pháp của chúng tôi đạt được hiệu suất vượt trội so với các phương pháp học tăng dần lớp tiên tiến nhất. Chúng tôi cũng khám phá cách các mạng con hoạt động khác nhau dưới các kiến trúc và bộ dữ liệu được chỉ định thông qua các nghiên cứu ablation. Ngoài ra, chúng tôi nhấn mạnh tầm quan trọng của tinh chỉnh và học phiên cơ sở trong việc đạt được hiệu suất tối ưu cho FSCIL. Chúng tôi tin rằng những phát hiện của chúng tôi có thể mang lại một bước ngoặt về tìm kiếm kiến trúc mạng nơ-ron sâu, cả về kiến trúc cụ thể cho nhiệm vụ và việc sử dụng các mô hình thưa thớt.
