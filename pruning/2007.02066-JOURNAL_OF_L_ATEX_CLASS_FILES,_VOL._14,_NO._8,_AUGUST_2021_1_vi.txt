# 2007.02066.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2007.02066.pdf
# Kích thước tệp: 937022 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 1
Cổng phụ thuộc trọng số cho cắt tỉa mạng
Yun Li, Zechun Liu, Weiqun Wu, Haotian Yao, Xiangyu Zhang, Chi Zhang, và Baoqun Yin

Tóm tắt —Trong bài báo này, một khung cắt tỉa mạng đơn giản nhưng hiệu quả được đề xuất để đồng thời giải quyết các vấn đề về chỉ báo cắt tỉa, tỷ lệ cắt tỉa, và ràng buộc hiệu quả. Bài báo này lập luận rằng quyết định cắt tỉa nên phụ thuộc vào trọng số tích chập, và do đó đề xuất các cổng phụ thuộc trọng số mới (W-Gates) để học thông tin từ trọng số bộ lọc và tự động thu được các cổng nhị phân để cắt tỉa hoặc giữ lại các bộ lọc. Để cắt tỉa mạng dưới các ràng buộc hiệu quả, một Mô-đun Hiệu quả có thể chuyển đổi được xây dựng để dự đoán độ trễ phần cứng hoặc FLOPs của các mạng được cắt tỉa ứng viên. Kết hợp với Mô-đun Hiệu quả được đề xuất, W-Gates có thể thực hiện cắt tỉa bộ lọc theo cách nhận biết hiệu quả và đạt được một mạng nhỏ gọn với sự cân bằng độ chính xác-hiệu quả tốt hơn. Chúng tôi đã chứng minh hiệu quả của phương pháp được đề xuất trên ResNet34, ResNet50, và MobileNet V2, tương ứng đạt được độ chính xác Top-1 cao hơn tới 1.33/1.28/1.1 với độ trễ phần cứng thấp hơn trên ImageNet. So với các phương pháp hiện đại, W-Gates cũng đạt được hiệu suất vượt trội.

Từ khóa chỉ mục —Cổng phụ thuộc trọng số, Mô-đun Hiệu quả có thể chuyển đổi, Cân bằng độ chính xác-hiệu quả, Cắt tỉa mạng

I. GIỚI THIỆU

TRONG những năm gần đây, các mạng nơ-ron tích chập (CNNs) đã đạt được hiệu suất hiện đại trong nhiều tác vụ, bao gồm nhưng không giới hạn ở phân loại hình ảnh [1], [2], phân đoạn ngữ nghĩa [3], và phát hiện đối tượng [4], [5], v.v. Mặc dù thành công lớn, hàng tỷ phép toán điểm nổi (FLOPs) và độ trễ suy luận dài vẫn cản trở việc triển khai CNNs trên nhiều phần cứng hạn chế tài nguyên. Do đó, một lượng nỗ lực đáng kể đã được đầu tư vào nén và tăng tốc CNNs, trong đó cắt tỉa bộ lọc [6], [7] được xem là một phương pháp nén mạng trực quan và hiệu quả.

Tuy nhiên, cắt tỉa bộ lọc không phải là điều tầm thường và đối mặt với ba thách thức chính. 1) Chỉ báo cắt tỉa: CNNs thường được xem như một hộp đen, và các bộ lọc riêng lẻ có thể đóng những vai trò khác nhau trong và giữa các lớp khác nhau trong mạng. Do đó, rất khó để thiết kế thủ công các chỉ báo có thể định lượng đầy đủ tầm quan trọng của các bộ lọc tích chập và bản đồ đặc trưng bên trong của chúng. 2) Tỷ lệ cắt tỉa: Bao nhiêu bộ lọc nên được cắt tỉa trong mỗi lớp? Sự dư thừa khác nhau giữa các lớp khác nhau, khiến việc đặt tỷ lệ cắt tỉa phù hợp cho các lớp khác nhau trở thành một vấn đề thách thức. 3) Ràng buộc hiệu quả: Hầu hết các công trình trước đây chỉ áp dụng các thước đo không phụ thuộc phần cứng như

Yun Li và Baoqun Yin thuộc Đại học Khoa học và Công nghệ Trung Quốc, Hefei, Trung Quốc (e-mail: yli001@mail.ustc.edu.cn, bqyin@ustc.edu.cn). (Tác giả liên hệ: Baoqun Yin.)

Zechun Liu thuộc Đại học Khoa học và Công nghệ Hồng Kông, Hồng Kông, Trung Quốc (e-mail: zliubq@connect.ust.hk).

Weiqun Wu thuộc Đại học Trùng Khánh, Trùng Khánh, Trung Quốc (e-mail: wuwq@cqu.edu.cn)

Haotian Yao, Xiangyu Zhang, và Chi Zhang thuộc Megvii Inc., Bắc Kinh, Trung Quốc (email: yaohaotian@megvii.com, zhangxiangyu@megvii.com, zhangchi@megvii.com).

tham số hoặc FLOPs để đánh giá hiệu quả của một CNN. Nhưng sự không nhất quán giữa các thước đo không phụ thuộc phần cứng và hiệu quả thực tế [8] dẫn đến nhu cầu công nghiệp ngày càng tăng về việc tối ưu hóa trực tiếp độ trễ phần cứng.

Các công trình trước đây đã cố gắng giải quyết những vấn đề này từ các góc độ khác nhau. Các công trình cắt tỉa bộ lọc thông thường chủ yếu dựa vào các chỉ báo được thiết kế thủ công [7], [9], [10] hoặc các chỉ báo dựa trên dữ liệu [11], [12]. Tuy nhiên, các chỉ báo được thiết kế thủ công thường liên quan đến sự tham gia của con người, và các chỉ báo cắt tỉa dựa trên dữ liệu có thể bị ảnh hưởng bởi các bản đồ đặc trưng. Bên cạnh đó, trong các công trình trước đây, tỷ lệ cắt tỉa của mỗi lớp hoặc ngưỡng cắt tỉa toàn cục thường được con người chỉ định, khiến kết quả dễ bị mắc kẹt trong các giải pháp không tối ưu [13].

Trong bài báo này, chúng tôi đề xuất một phương pháp cắt tỉa bộ lọc đơn giản nhưng hiệu quả, có thể tự động thu được quyết định cắt tỉa và tỷ lệ cắt tỉa của mỗi lớp đồng thời xem xét hiệu quả tổng thể của mạng, như được hiển thị trong Hình 1.

Để giải quyết vấn đề chỉ báo cắt tỉa, chúng tôi đề xuất các cổng phụ thuộc trọng số (W-Gates). Thay vì thiết kế một chỉ báo thủ công hoặc các hệ số tỷ lệ dựa trên dữ liệu, chúng tôi lập luận rằng quyết định cắt tỉa nên phụ thuộc vào chính bộ lọc, nói cách khác, nó nên là một hàm có thể học được của trọng số bộ lọc. Do đó, chúng tôi đề xuất một loại cổng phụ thuộc trọng số mới để trực tiếp học ánh xạ từ trọng số bộ lọc đến cổng bộ lọc. W-Gates lấy trọng số của một lớp tích chập làm đầu vào để học thông tin từ chúng, và xuất ra các cổng nhị phân để mở hoặc đóng các bộ lọc tương ứng tự động (0: đóng, 1: mở) trong quá trình huấn luyện. Mỗi W-Gates bao gồm một lớp kết nối đầy đủ và kích hoạt nhị phân, đơn giản để triển khai và huấn luyện với mô hình CNN được huấn luyện trước. Quan trọng hơn, các cổng bộ lọc đầu ra ở đây là phụ thuộc trọng số, đây là sự khác biệt cốt lõi giữa W-Gates được đề xuất và các phương pháp dựa trên dữ liệu thông thường [11], [12], [14].

Để giải quyết vấn đề ràng buộc hiệu quả, chúng tôi đề xuất một Mô-đun Hiệu quả có thể chuyển đổi, cung cấp hai tùy chọn để đối phó với các tình huống khác nhau, một Mạng Dự đoán Độ trễ (LPNet) và Ước tính FLOPs. Để cắt tỉa mạng dưới ràng buộc phần cứng, chúng tôi đề xuất LPNet để dự đoán độ trễ phần cứng của các mạng được cắt tỉa ứng viên. LPNet lấy vector mã hóa mạng làm đầu vào và xuất ra độ trễ được dự đoán. Sau khi được huấn luyện ngoại tuyến dựa trên dữ liệu độ trễ thu thập từ phần cứng đích, LPNet có thể cung cấp hướng dẫn độ trễ cho việc cắt tỉa mạng. Xem xét rằng thông tin phần cứng không phải lúc nào cũng có sẵn, chúng tôi thêm Ước tính FLOPs như một đơn vị phụ trợ trong Mô-đun Hiệu quả. Đối với các tình huống không có thông tin phần cứng, chúng tôi có thể chuyển Mô-đun Hiệu quả sang Ước tính FLOPs và cung cấp hướng dẫn FLOPs theo cách gradient cho việc cắt tỉa.

Với W-Gates được đề xuất và Mô-đun Hiệu quả có thể chuyển đổi, chúng tôi xây dựng một khung cắt tỉa khác biệt để thực hiện cắt tỉa bộ lọc. Chúng tôi phát hiện rằng đầu ra của W-Gates

--- TRANG 2 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 2

[Hình 1 mô tả khung cắt tỉa tổng quan với các thành phần chính]

có thể được tổng hợp như một mã hóa lớp để xác định tỷ lệ cắt tỉa của mỗi lớp, và tất cả mã hóa lớp có thể tạo thành vector mã hóa mạng của kiến trúc được cắt tỉa ứng viên. Do đó, mã hóa mạng được đưa vào Mô-đun Hiệu quả để có được hướng dẫn độ trễ hoặc FLOPs. Sau đó, với mô hình CNN được huấn luyện trước và LPNet, chúng tôi có thể thực hiện cắt tỉa mạng theo cách đầu cuối. Để thực hiện cắt tỉa bộ lọc trong quá trình huấn luyện, chúng tôi định nghĩa một hàm mất mát nhận biết hiệu quả bao gồm mất mát độ chính xác và mất mát hiệu quả. Toàn bộ khung cắt tỉa có thể khác biệt đầy đủ, cho phép chúng tôi đồng thời áp dụng gradient của mất mát độ chính xác và mất mát hiệu quả để tối ưu hóa W-Gates của mỗi lớp. Giữ lại nhiều bộ lọc hơn sẽ giữ được nhiều thông tin hơn trong mạng, có lợi cho độ chính xác cuối cùng. Ngược lại, cắt tỉa nhiều bộ lọc hơn sẽ cải thiện hiệu quả suy luận của mạng. Để đạt được điều này, trong quá trình huấn luyện, mất mát độ chính xác sẽ kéo các cổng nhị phân về nhiều '1' hơn (giữ lại nhiều bộ lọc hơn), trong khi mất mát hiệu quả kéo các cổng nhị phân về nhiều '0' hơn (cắt tỉa nhiều bộ lọc hơn). Chúng cạnh tranh với nhau và cuối cùng thu được một mạng nhỏ gọn với sự cân bằng độ chính xác-hiệu quả tốt hơn.

Đáng chú ý là quyết định cắt tỉa và tỷ lệ cắt tỉa của mỗi lớp đều có thể được thu được tự động, với ít sự tham gia của con người. Vì đầu vào của Mô-đun Hiệu quả là mã hóa mạng, khung cắt tỉa có thể thực hiện cắt tỉa bộ lọc với việc xem xét kiến trúc mạng tổng thể, có lợi cho việc tìm ra tỷ lệ cắt tỉa tối ưu cho các lớp khác nhau. W-Gates được đề xuất có thể được áp dụng trong các tác vụ thị giác máy tính dựa trên CNN khác nhau để nén và tăng tốc các backbone hoặc toàn bộ mạng của chúng.

Chúng tôi đánh giá phương pháp của mình trên ResNets [1], MobileNet V2[8], và VGG [15]. So với các baseline đồng nhất, chúng tôi liên tục mang lại độ chính xác cao hơn nhiều và độ trễ thấp hơn. Với độ trễ thấp hơn, chúng tôi đạt được độ chính xác cao hơn 1.09%-1.33% so với ResNet34, cao hơn 0.67%-1.28% so với ResNet50, và cao hơn 1.1% so với MobileNet V2 trên tập dữ liệu ImageNet. So với các phương pháp cắt tỉa hiện đại khác [16]–[24], phương pháp của chúng tôi cũng tạo ra kết quả vượt trội.

Những đóng góp chính của bài báo chúng tôi gồm ba khía cạnh:

Chúng tôi đề xuất một loại cổng phụ thuộc trọng số mới (W-Gates) để trực tiếp học ánh xạ từ trọng số bộ lọc đến cổng bộ lọc. W-Gates lấy trọng số tích chập làm đầu vào và tạo ra các cổng bộ lọc nhị phân để cắt tỉa hoặc giữ lại các bộ lọc tự động trong quá trình huấn luyện.

Một Mô-đun Hiệu quả có thể chuyển đổi được xây dựng để dự đoán độ trễ phần cứng hoặc FLOPs của các mạng được cắt tỉa ứng viên. Mô-đun Hiệu quả có thể khác biệt đầy đủ đối với W-Gates, cho phép chúng tôi áp dụng các ràng buộc hiệu quả dựa trên gradient và thu được một mạng nhỏ gọn với sự cân bằng độ chính xác-hiệu quả tốt hơn.

Phương pháp được đề xuất đồng thời giải quyết ba thách thức trong cắt tỉa bộ lọc (chỉ báo cắt tỉa, tỷ lệ cắt tỉa, ràng buộc hiệu quả). So với các phương pháp cắt tỉa bộ lọc hiện đại, phương pháp được đề xuất đạt được hiệu suất vượt trội.

Bài báo này mở rộng bài báo hội thảo sơ bộ [25] trong các khía cạnh sau. 1) Chúng tôi đề xuất một Mô-đun Hiệu quả có thể chuyển đổi để đối phó với các tình huống khác nhau, để khung cắt tỉa không chỉ có thể thực hiện cắt tỉa bộ lọc dưới ràng buộc độ trễ, mà còn dưới các ràng buộc FLOPs. 2) Chúng tôi khái quát hóa ý tưởng về cổng phụ thuộc trọng số và mạng dự đoán độ trễ, cho phép áp dụng W-Gates từ một kiến trúc ResNet đơn lẻ sang nhiều loại kiến trúc CNN hơn, như MobileNetv2, VGGNet, v.v. 3) Chúng tôi tiến hành bốn nghiên cứu bóc tách quan trọng để minh họa tính hợp lý của thiết kế W-Gates trong phương pháp của chúng tôi. Chúng tôi cho thấy rằng W-Gates thực sự có thể học thông tin từ trọng số bộ lọc và tự động thực hiện lựa chọn bộ lọc tốt, giúp đào tạo mạng và đạt được độ chính xác cao hơn. 4) Chúng tôi điều tra ảnh hưởng của hệ số quan trọng trong hàm mất mát được đề xuất đối với sự cân bằng độ chính xác-độ trễ cuối cùng. 5) Chúng tôi khái quát hóa ý tưởng từ một tập dữ liệu đơn lẻ sang nhiều tình huống tập dữ liệu hơn. 6) Chúng tôi tiến hành phân tích trực quan hóa trên các kiến trúc được cắt tỉa. Chúng tôi cho thấy rằng W-Gates có xu hướng không cắt tỉa các lớp có các phép toán downsample, và cắt tỉa nhiều lớp tích chập 3×3 hơn so với các lớp 1×1.

Phần còn lại của bài báo này được cấu trúc như sau. Trong Phần II, chúng tôi tóm tắt ngắn gọn các công trình liên quan. Phần III trình bày chi tiết phương pháp được đề xuất của chúng tôi. Cài đặt thực nghiệm, nghiên cứu bóc tách, và phân tích kết quả được trình bày trong Phần IV theo sau là kết luận trong Phần V.

II. CÔNG TRÌNH LIÊN QUAN

Một lượng nỗ lực đáng kể đã được dành cho nén mô hình sâu, như phân tách ma trận [26], [27], lượng tử hóa [28], [29], chưng cất kiến thức [30], [31], học kiến trúc nhỏ gọn [8], [32], tìm kiếm mạng nơ-ron [33], [34], tăng tốc suy luận [35], [36], và cắt tỉa mạng [37]–[39]. Cắt tỉa là một phương pháp nén mạng trực quan và hiệu quả. Các công trình trước đó tập trung vào cắt tỉa trọng số. [37] đề xuất cắt tỉa các kết nối không quan trọng có trọng số tuyệt đối nhỏ hơn một ngưỡng cho trước, đạt được hiệu suất tốt trong nén tham số. Tuy nhiên, nó không thân thiện với việc triển khai và không thể đạt được suy luận nhanh hơn mà không có phần cứng hoạt động ma trận thưa thớt chuyên dụng. Để giải quyết vấn đề này, một số phương pháp cắt tỉa bộ lọc [7], [40], [41] đã được khám phá gần đây. Những phương pháp này cắt tỉa hoặc làm thưa thớt các phần của cấu trúc mạng (ví dụ: nơ-ron, kênh) thay vì trọng số riêng lẻ, vì vậy chúng thường yêu cầu ít thư viện chuyên biệt hơn để đạt được tăng tốc suy luận. Trong bài báo này, công trình của chúng tôi cũng thuộc về cắt tỉa bộ lọc. Tiếp theo, chúng tôi chủ yếu thảo luận về các công trình liên quan nhất đến công trình của chúng tôi.

A. Chỉ báo được thiết kế thủ công

Nhiều phương pháp cắt tỉa bộ lọc xuất sắc dựa trên các chỉ báo được thiết kế thủ công đã được đề xuất để nén các CNN lớn. [7] trình bày một phương pháp tăng tốc cho CNNs, trong đó chúng tôi cắt tỉa các bộ lọc từ CNNs được xác định là có ảnh hưởng nhỏ đến độ chính xác đầu ra. [9] đề xuất một thuật toán hai bước lặp để cắt tỉa hiệu quả mỗi lớp, bằng một lựa chọn kênh dựa trên hồi quy LASSO và tái tạo bình phương tối thiểu. [41] chính thức thiết lập cắt tỉa bộ lọc như một bài toán tối ưu hóa, và tiết lộ rằng chúng ta cần cắt tỉa các bộ lọc dựa trên thông tin thống kê được tính toán từ lớp tiếp theo của nó, không phải lớp hiện tại. Hai phương pháp trên đều cắt tỉa bộ lọc dựa trên bản đồ đặc trưng. Khác với các phương pháp dựa trên dữ liệu trên, [10] đề xuất một phương pháp bất khả tri đặc trưng, cắt tỉa bộ lọc dựa trên chỉ báo entropy và độ thưa thớt kernel. [42] đề xuất một phương pháp cắt tỉa bộ lọc mềm, cắt tỉa bộ lọc dựa trên chuẩn L2 và cập nhật mô hình được cắt tỉa khi huấn luyện mô hình sau khi cắt tỉa. [43] giới thiệu một mặt nạ toàn cục nhị phân sau mỗi bộ lọc để cắt tỉa và điều chỉnh mạng một cách động và lặp, với cơ chế để gọi lại các bộ lọc bị cắt tỉa nhầm trong các lần lặp trước đó. [17] đề xuất một phương pháp cắt tỉa bộ lọc dựa trên Trung vị Hình học, cắt tỉa các bộ lọc có đóng góp tương đối ít hơn và chọn các bộ lọc có đóng góp có thể thay thế nhất. Những phương pháp trên cần đặt thủ công một tỷ lệ cắt tỉa toàn cục [10], [43] hoặc tỷ lệ cắt tỉa lớp [7], [9], [17], [41], khó có thể xem xét đầy đủ sự dư thừa của các lớp khác nhau. Để giải quyết vấn đề này, [44] thiết lập một đồ thị cho mỗi lớp tích chập của một CNN và sử dụng hai đại lượng liên quan đến đồ thị để có được sự dư thừa và tỷ lệ cắt tỉa của mỗi lớp. Tuy nhiên, một vấn đề chung của những chỉ báo cắt tỉa được thiết kế thủ công này là chúng thường liên quan đến sự tham gia của con người, khiến chúng dễ bị mắc kẹt trong các giải pháp không tối ưu. Để giải quyết vấn đề này, phương pháp được đề xuất của chúng tôi giới thiệu Cổng phụ thuộc Trọng số để học quyết định cắt tỉa từ trọng số bộ lọc tự động và hợp tác với Mô-đun Hiệu quả để xác định tỷ lệ cắt tỉa của mỗi lớp, với ít sự tham gia của con người.

B. Phương pháp Cắt tỉa dựa trên Dữ liệu

Có một số phương pháp cắt tỉa bộ lọc khác đề xuất các chỉ báo dựa trên dữ liệu. [12] áp dụng chuẩn hóa L1 trên các hệ số tỷ lệ trong các lớp chuẩn hóa batch (BN) để xác định các bộ lọc không đáng kể. Tương tự, [11] giới thiệu các hệ số tỷ lệ để chia tỷ lệ đầu ra của các cấu trúc cụ thể, như nơ-ron, nhóm, hoặc khối dư, và sau đó thực hiện cắt tỉa mạng theo cách đầu cuối. [45] đề xuất cắt tỉa cả kênh bên trong và bên ngoài các kết nối dư thông qua một tiêu chí dựa trên phân kỳ KL để bù đắp cho điểm yếu của dữ liệu hạn chế. [46] đề xuất các cổng với phân cực khác biệt để kiểm soát việc bật-tắt của mỗi kênh hoặc toàn bộ khối lớp, về bản chất là một loại cổng dựa trên dữ liệu. ManiDP [47] khám phá một mô hình dựa trên dữ liệu cho cắt tỉa động, khám phá thông tin đa tạp của tất cả các mẫu trong quá trình huấn luyện và tạo ra các mạng con tương ứng để bảo tồn mối quan hệ giữa các trường hợp khác nhau. ResRep [48] đề xuất tái tham số hóa một CNN thành các phần ghi nhớ và các phần quên, trong đó phần trước học để duy trì hiệu suất và phần sau học để cắt tỉa. [49] đề xuất một phương pháp cắt tỉa mềm phụ thuộc dữ liệu, được gọi là Squeeze-Excitation-Pruning (SEP), không cắt tỉa vật lý bất kỳ bộ lọc nào mà chọn các bộ lọc khác nhau cho các đầu vào khác nhau. [50] đề xuất một chỉ báo cắt tỉa dựa trên dữ liệu, trong đó các kênh không liên quan đến tác vụ được loại bỏ theo cách hướng tác vụ. AutoPruner [14] đề xuất một lớp lựa chọn kênh dựa trên dữ liệu, lấy bản đồ đặc trưng của lớp trước làm đầu vào và tạo ra một mã chỉ mục nhị phân để cắt tỉa. AutoPrunner [14] tương tự nhất với W-Gates của chúng tôi. Sự khác biệt chính là W-Gates của chúng tôi phụ thuộc trọng số, trực tiếp học thông tin từ trọng số bộ lọc thay vì bản đồ đặc trưng. Sự khác biệt thứ hai là AutoPrunner áp dụng một hàm sigmoid được chia tỷ lệ như nhị phân hóa mềm để tạo ra một vector nhị phân xấp xỉ, trong khi chúng tôi áp dụng nhị phân hóa cứng (một biến thể của hàm sign) để có được các cổng nhị phân thực sự trong quá trình huấn luyện.

--- TRANG 3 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 3

kiến trúc, như MobileNetv2, VGGNet, v.v. 3) Chúng tôi tiến hành bốn nghiên cứu bóc tách quan trọng để minh họa tính hợp lý của thiết kế W-Gates trong phương pháp của chúng tôi. Chúng tôi cho thấy rằng W-Gates thực sự có thể học thông tin từ trọng số bộ lọc và tự động thực hiện lựa chọn bộ lọc tốt, giúp đào tạo mạng và đạt được độ chính xác cao hơn. 4) Chúng tôi điều tra ảnh hưởng của hệ số quan trọng trong hàm mất mát được đề xuất đối với sự cân bằng độ chính xác-độ trễ cuối cùng. 5) Chúng tôi khái quát hóa ý tưởng từ một tập dữ liệu đơn lẻ sang nhiều tình huống tập dữ liệu hơn. 6) Chúng tôi tiến hành phân tích trực quan hóa trên các kiến trúc được cắt tỉa. Chúng tôi cho thấy rằng W-Gates có xu hướng không cắt tỉa các lớp có các phép toán downsample, và cắt tỉa nhiều lớp tích chập 3×3 hơn so với các lớp 1×1.

Phần còn lại của bài báo này được cấu trúc như sau. Trong Phần II, chúng tôi tóm tắt ngắn gọn các công trình liên quan. Phần III trình bày chi tiết phương pháp được đề xuất của chúng tôi. Cài đặt thực nghiệm, nghiên cứu bóc tách, và phân tích kết quả được trình bày trong Phần IV theo sau là kết luận trong Phần V.

II. CÔNG TRÌNH LIÊN QUAN

Một lượng nỗ lực đáng kể đã được dành cho nén mô hình sâu, như phân tách ma trận [26], [27], lượng tử hóa [28], [29], chưng cất kiến thức [30], [31], học kiến trúc nhỏ gọn [8], [32], tìm kiếm mạng nơ-ron [33], [34], tăng tốc suy luận [35], [36], và cắt tỉa mạng [37]–[39]. Cắt tỉa là một phương pháp nén mạng trực quan và hiệu quả. Các công trình trước đó tập trung vào cắt tỉa trọng số. [37] đề xuất cắt tỉa các kết nối không quan trọng có trọng số tuyệt đối nhỏ hơn một ngưỡng cho trước, đạt được hiệu suất tốt trong nén tham số. Tuy nhiên, nó không thân thiện với việc triển khai và không thể đạt được suy luận nhanh hơn mà không có phần cứng hoạt động ma trận thưa thớt chuyên dụng. Để giải quyết vấn đề này, một số phương pháp cắt tỉa bộ lọc [7], [40], [41] đã được khám phá gần đây. Những phương pháp này cắt tỉa hoặc làm thưa thớt các phần của cấu trúc mạng (ví dụ: nơ-ron, kênh) thay vì trọng số riêng lẻ, vì vậy chúng thường yêu cầu ít thư viện chuyên biệt hơn để đạt được tăng tốc suy luận. Trong bài báo này, công trình của chúng tôi cũng thuộc về cắt tỉa bộ lọc. Tiếp theo, chúng tôi chủ yếu thảo luận về các công trình liên quan nhất đến công trình của chúng tôi.

A. Chỉ báo được thiết kế thủ công

Nhiều phương pháp cắt tỉa bộ lọc xuất sắc dựa trên các chỉ báo được thiết kế thủ công đã được đề xuất để nén các CNN lớn. [7] trình bày một phương pháp tăng tốc cho CNNs, trong đó chúng tôi cắt tỉa các bộ lọc từ CNNs được xác định là có ảnh hưởng nhỏ đến độ chính xác đầu ra. [9] đề xuất một thuật toán hai bước lặp để cắt tỉa hiệu quả mỗi lớp, bằng một lựa chọn kênh dựa trên hồi quy LASSO và tái tạo bình phương tối thiểu. [41] chính thức thiết lập cắt tỉa bộ lọc như một bài toán tối ưu hóa, và tiết lộ rằng chúng ta cần cắt tỉa các bộ lọc dựa trên thông tin thống kê được tính toán từ lớp tiếp theo của nó, không phải lớp hiện tại. Hai phương pháp trên đều cắt tỉa bộ lọc dựa trên bản đồ đặc trưng. Khác với các phương pháp dựa trên dữ liệu trên, [10] đề xuất một phương pháp bất khả tri đặc trưng, cắt tỉa bộ lọc dựa trên chỉ báo entropy và độ thưa thớt kernel. [42] đề xuất một phương pháp cắt tỉa bộ lọc mềm, cắt tỉa bộ lọc dựa trên chuẩn L2 và cập nhật mô hình được cắt tỉa khi huấn luyện

mô hình sau khi cắt tỉa. [43] giới thiệu một mặt nạ toàn cục nhị phân sau mỗi bộ lọc để cắt tỉa và điều chỉnh mạng một cách động và lặp, với cơ chế để gọi lại các bộ lọc bị cắt tỉa nhầm trong các lần lặp trước đó. [17] đề xuất một phương pháp cắt tỉa bộ lọc dựa trên Trung vị Hình học, cắt tỉa các bộ lọc có đóng góp tương đối ít hơn và chọn các bộ lọc có đóng góp có thể thay thế nhất. Những phương pháp trên cần đặt thủ công một tỷ lệ cắt tỉa toàn cục [10], [43] hoặc tỷ lệ cắt tỉa lớp [7], [9], [17], [41], khó có thể xem xét đầy đủ sự dư thừa của các lớp khác nhau. Để giải quyết vấn đề này, [44] thiết lập một đồ thị cho mỗi lớp tích chập của một CNN và sử dụng hai đại lượng liên quan đến đồ thị để có được sự dư thừa và tỷ lệ cắt tỉa của mỗi lớp. Tuy nhiên, một vấn đề chung của những chỉ báo cắt tỉa được thiết kế thủ công này là chúng thường liên quan đến sự tham gia của con người, khiến chúng dễ bị mắc kẹt trong các giải pháp không tối ưu. Để giải quyết vấn đề này, phương pháp được đề xuất của chúng tôi giới thiệu Cổng phụ thuộc Trọng số để học quyết định cắt tỉa từ trọng số bộ lọc tự động và hợp tác với Mô-đun Hiệu quả để xác định tỷ lệ cắt tỉa của mỗi lớp, với ít sự tham gia của con người.

B. Phương pháp Cắt tỉa dựa trên Dữ liệu

Có một số phương pháp cắt tỉa bộ lọc khác đề xuất các chỉ báo dựa trên dữ liệu. [12] áp dụng chuẩn hóa L1 trên các hệ số tỷ lệ trong các lớp chuẩn hóa batch (BN) để xác định các bộ lọc không đáng kể. Tương tự, [11] giới thiệu các hệ số tỷ lệ để chia tỷ lệ đầu ra của các cấu trúc cụ thể, như nơ-ron, nhóm, hoặc khối dư, và sau đó thực hiện cắt tỉa mạng theo cách đầu cuối. [45] đề xuất cắt tỉa cả kênh bên trong và bên ngoài các kết nối dư thông qua một tiêu chí dựa trên phân kỳ KL để bù đắp cho điểm yếu của dữ liệu hạn chế. [46] đề xuất các cổng với phân cực khác biệt để kiểm soát việc bật-tắt của mỗi kênh hoặc toàn bộ khối lớp, về bản chất là một loại cổng dựa trên dữ liệu. ManiDP [47] khám phá một mô hình dựa trên dữ liệu cho cắt tỉa động, khám phá thông tin đa tạp của tất cả các mẫu trong quá trình huấn luyện và tạo ra các mạng con tương ứng để bảo tồn mối quan hệ giữa các trường hợp khác nhau. ResRep [48] đề xuất tái tham số hóa một CNN thành các phần ghi nhớ và các phần quên, trong đó phần trước học để duy trì hiệu suất và phần sau học để cắt tỉa. [49] đề xuất một phương pháp cắt tỉa mềm phụ thuộc dữ liệu, được gọi là Squeeze-Excitation-Pruning (SEP), không cắt tỉa vật lý bất kỳ bộ lọc nào mà chọn các bộ lọc khác nhau cho các đầu vào khác nhau. [50] đề xuất một chỉ báo cắt tỉa dựa trên dữ liệu, trong đó các kênh không liên quan đến tác vụ được loại bỏ theo cách hướng tác vụ. AutoPruner [14] đề xuất một lớp lựa chọn kênh dựa trên dữ liệu, lấy bản đồ đặc trưng của lớp trước làm đầu vào và tạo ra một mã chỉ mục nhị phân để cắt tỉa. AutoPrunner [14] tương tự nhất với W-Gates của chúng tôi. Sự khác biệt chính là W-Gates của chúng tôi phụ thuộc trọng số, trực tiếp học thông tin từ trọng số bộ lọc thay vì bản đồ đặc trưng. Sự khác biệt thứ hai là AutoPrunner áp dụng một hàm sigmoid được chia tỷ lệ như nhị phân hóa mềm để tạo ra một vector nhị phân xấp xỉ, trong khi chúng tôi áp dụng nhị phân hóa cứng (một biến thể của hàm sign) để có được các cổng nhị phân thực sự trong quá trình huấn luyện.

--- TRANG 4 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 4

C. Lượng tử hóa và Kích hoạt Nhị phân.

Lượng tử hóa [29] đề xuất lượng tử hóa các trọng số giá trị thực thành trọng số nhị phân/tam phân để tạo ra lượng lớn tiết kiệm kích thước mô hình. [51] chứng minh rằng hiệu suất tốt có thể đạt được ngay cả khi tất cả nơ-ron và trọng số được nhị phân hóa thành ±1. [28] đề xuất sử dụng đạo hàm của hàm clip để xấp xỉ đạo hàm của hàm sign trong các mạng nhị phân hóa. [52] nới lỏng các biến mặt nạ rời rạc để trở thành các biến ngẫu nhiên liên tục được tính bằng Gumbel Softmax [53] để làm cho chúng có thể khác biệt. [54] sử dụng một shortcut đồng nhất để thêm các đầu ra giá trị thực vào các đầu ra giá trị thực liền kề tiếp theo nhất, và sau đó áp dụng một xấp xỉ chặt chẽ cho đạo hàm của hàm sign không khác biệt đối với kích hoạt giá trị thực, điều này đã truyền cảm hứng cho kích hoạt nhị phân và ước tính gradient trong W-Gates của chúng tôi.

D. Nén có Ràng buộc Tài nguyên

Gần đây, hiệu suất phần cứng thực đã thu hút nhiều sự chú ý hơn so với FLOPs. Các phương pháp AutoML [55], [56] đề xuất cắt tỉa bộ lọc lặp đi lặp lại trong các lớp khác nhau của một CNN thông qua học tăng cường hoặc một vòng lặp phản hồi tự động, lấy độ trễ thời gian thực làm ràng buộc. Một số công trình gần đây [52], [57] giới thiệu một bảng tra cứu để ghi lại độ trễ của mỗi phép toán hoặc mỗi lớp và tổng chúng để có được độ trễ của toàn bộ mạng. Phương pháp này có hiệu lực cho nhiều CPU và DSP, nhưng có thể không dành cho các thiết bị tính toán song song như GPU. [58] xem nền tảng phần cứng như một hộp đen và tạo ra một mô hình ước tính năng lượng để dự đoán độ trễ của phần cứng cụ thể như một ràng buộc tối ưu hóa. [19] đề xuất DMCP để tìm kiếm cấu trúc con tối ưu từ các mạng chưa được cắt tỉa dưới ràng buộc FLOPs. [59] huấn luyện một mạng nơ-ron độc lập để dự đoán hiệu suất của các mạng con và sau đó tối đa hóa đầu ra của mạng như một proxy của độ chính xác để hướng dẫn cắt tỉa. [13] đề xuất PruningNet, lấy vector mã hóa mạng làm đầu vào và xuất ra các tham số trọng số của mạng được cắt tỉa. Những công trình trên đã truyền cảm hứng cho Mô-đun Hiệu quả và việc huấn luyện LPNet của chúng tôi. Chúng tôi huấn luyện LPNet để dự đoán độ trễ của nền tảng phần cứng đích, lấy vector mã hóa mạng làm đầu vào và xuất ra độ trễ được dự đoán.

III. PHƯƠNG PHÁP ĐỀ XUẤT

Trong phần này, chúng tôi giới thiệu phương pháp được đề xuất áp dụng một cách tiếp cận nhận biết hiệu quả để cắt tỉa kiến trúc CNN dưới nhiều ràng buộc. Theo công trình [13], chúng tôi công thức hóa việc cắt tỉa mạng như một bài toán tối ưu hóa có ràng buộc:

arg min wc ℓ(c; wc)
s.t: Inference(c) ≤ Const                    (1)

trong đó ℓ là hàm mất mát cụ thể cho một tác vụ học nhất định, và c là vector mã hóa mạng, là một tập hợp của độ rộng kênh mạng được cắt tỉa (c1, c2, ..., cl). Inference(c) biểu thị độ trễ thực hoặc FLOPs của mạng được cắt tỉa, phụ thuộc vào tập hợp độ rộng kênh mạng c. wc có nghĩa là

[Hình 2 mô tả Cổng phụ thuộc Trọng số được đề xuất]

trọng số của các kênh còn lại và Const là ràng buộc độ trễ hoặc FLOPs cho trước.

Để giải quyết vấn đề này, chúng tôi đề xuất một khung cắt tỉa mạng nhận biết hiệu quả, trong đó cổng phụ thuộc trọng số là phần quan trọng. Đầu tiên, chúng tôi đề xuất Mô-đun Cổng phụ thuộc Trọng số (W-Gates), được áp dụng để học thông tin từ trọng số bộ lọc và tạo ra các cổng nhị phân để tự động xác định bộ lọc nào cần cắt tỉa. W-Gates hoạt động như chỉ báo cắt tỉa thích ứng. Sau đó, một Mô-đun Hiệu quả được xây dựng để cung cấp ràng buộc độ trễ hoặc FLOPs cho chỉ báo cắt tỉa và hướng dẫn tỷ lệ cắt tỉa của mỗi lớp. Như những phần quan trọng của mô-đun hiệu quả, một Mạng Dự đoán Độ trễ (LPNet) được huấn luyện ngoại tuyến để dự đoán độ trễ thực của một kiến trúc nhất định trong phần cứng cụ thể, và Ước tính FLOPs được đặt để dự đoán FLOPs cho các kiến trúc ứng viên. Hai mô-đun trên bổ sung cho nhau để tạo ra chiến lược cắt tỉa và có được mô hình CNN tốt nhất dưới ràng buộc hiệu quả.

A. Cổng phụ thuộc Trọng số

Lớp tích chập luôn được áp dụng như một hộp đen, và chúng ta chỉ có thể đánh giá từ đầu ra những gì nó đã làm. Các phương pháp cắt tỉa bộ lọc thông thường chủ yếu dựa vào các chỉ báo thủ công hoặc các chỉ báo dựa trên tối ưu hóa. Chúng chia sẻ một động cơ chung: về bản chất chúng đang tìm kiếm một hàm cắt tỉa có thể ánh xạ trọng số bộ lọc thành cổng bộ lọc. Tuy nhiên, hàm cắt tỉa của chúng thường liên quan đến sự tham gia của con người. Chúng tôi lập luận rằng các cổng nên phụ thuộc vào chính các bộ lọc, nói cách khác, nó là một hàm có thể học được của trọng số bộ lọc. Do đó, thay vì thiết kế một chỉ báo thủ công, chúng tôi

--- TRANG 5 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 5

[Hình 3 mô tả hàm kích hoạt nhị phân và đạo hàm]

trực tiếp học hàm cắt tỉa từ trọng số bộ lọc, là một phản ánh trực tiếp của đặc tính của các bộ lọc. Để đạt được mục tiêu trên, chúng tôi đề xuất Cổng phụ thuộc Trọng số (W-Gates). W-Gates lấy trọng số của một lớp tích chập làm đầu vào để học thông tin, và xuất ra các cổng bộ lọc nhị phân như hàm cắt tỉa để loại bỏ các bộ lọc một cách thích ứng.

Học Cổng Bộ lọc. Cho W ∈ R^(Cl×Cl-1×Kl×Kl) biểu thị trọng số của một lớp tích chập, thường có thể được mô hình hóa như Cl bộ lọc và mỗi bộ lọc Wi ∈ R^(Ci-1×Kl×Kl); i = 1,2,...,Cl. Kl là kích thước bộ lọc của lớp thứ l. Để trích xuất thông tin trong mỗi bộ lọc, một lớp kết nối đầy đủ, có trọng số được ký hiệu là _W ∈ R^((Cl-1×Kl×Kl)×1), được giới thiệu ở đây. Sau đó, được định hình lại thành tensor hai chiều W* ∈ R^(Cl×(Cl-1×Kl×Kl)), trọng số bộ lọc được đưa vào lớp kết nối đầy đủ để tạo ra điểm số của mỗi bộ lọc:

s^r = f(W*) = W* · _W;                    (2)

trong đó s^r = {s^r_1, s^r_2, ..., s^r_Cl} biểu thị tập hợp điểm số của các bộ lọc trong lớp tích chập này.

Để ức chế biểu thức của các bộ lọc có điểm số thấp hơn và có được các cổng bộ lọc nhị phân, chúng tôi giới thiệu hàm kích hoạt sau:

λ(x) = (Sign(x) + 1)/2;                   (3)

Đường cong của Phương trình (3) được hiển thị trong Hình 3(a). Chúng ta có thể thấy rằng sau khi được xử lý bởi hàm kích hoạt, các điểm số âm sẽ được chuyển đổi thành 0, và các điểm số dương sẽ được chuyển đổi thành 1. Sau đó, chúng ta có được các cổng bộ lọc nhị phân của lớp này:

gates^b = λ(s^r) = λ(f(W*));             (4)

Khác với nhị phân hóa đường dẫn [60] trong tìm kiếm kiến trúc mạng nơ-ron, trong các tác vụ cắt tỉa bộ lọc, quyết định cắt tỉa nên phụ thuộc vào trọng số bộ lọc, nói cách khác, các cổng bộ lọc nhị phân được đề xuất của chúng tôi là phụ thuộc trọng số, như được hiển thị trong Phương trình (4). Khác với bộ điều khiển bước thời gian [61] phụ thuộc trọng số, các cổng phụ thuộc trọng số được đề xuất như các chỉ báo cắt tỉa.

Tiếp theo, chúng tôi tổng các cổng bộ lọc nhị phân của mỗi lớp để có được mã hóa lớp, và tất cả các mã hóa lớp tạo thành vector mã hóa mạng c. Mã hóa lớp ở đây biểu thị số lượng bộ lọc được giữ lại, cũng có thể xác định tỷ lệ cắt tỉa của mỗi lớp.

Ước tính Gradient. Như có thể thấy từ Hình 3, đạo hàm của hàm λ(·) là một hàm xung, không thể được sử dụng trực tiếp trong quá trình huấn luyện. Lấy cảm hứng từ các công trình Mô hình Lượng tử hóa gần đây [28], [54], cụ thể là Bi-Real Net [54], chúng tôi giới thiệu một xấp xỉ khác biệt của hàm không khác biệt λ(x). Quá trình ước tính gradient như sau:

∂L/∂X^r = (∂L/∂X^b)(∂X^b/∂X^r) = (∂L/∂X^b)(∂λ(X^r)/∂X^r) ≈ (∂L/∂X^b)(∂ρ(X^r)/∂X^r);    (5)

trong đó X^r biểu thị đầu ra giá trị thực s^r_i, X^b có nghĩa là đầu ra nhị phân. ρ(X^r) là hàm xấp xỉ chúng tôi thiết kế, là một hàm đa thức từng khúc:

ρ(X^r) = {
    0;                          if X^r < -1/2
    2X^r + 2(X^r)^2 + 1/2;     if -1/2 ≤ X^r < 0
    2X^r - 2(X^r)^2 + 1/2;     if 0 ≤ X^r < 1/2      (6)
    1;                          otherwise;
}

và gradient của hàm xấp xỉ trên là:

∂ρ(X^r)/∂X^r = {
    2 + 4X^r;    if -1/2 ≤ X^r < 0
    2 - 4X^r;    if 0 ≤ X^r < 1/2         (7)
    0;           otherwise;
}

Như đã thảo luận ở trên, chúng ta có thể áp dụng hàm kích hoạt nhị phân Phương trình (3) để có được các cổng bộ lọc nhị phân trong lan truyền tiến, và sau đó cập nhật trọng số của lớp kết nối đầy đủ với một gradient xấp xỉ Phương trình (7) trong lan truyền ngược.

B. Mô-đun Hiệu quả

Với W-Gates được đề xuất, chúng ta có thể thực hiện chấm điểm và lựa chọn bộ lọc dựa trên thông tin trong trọng số tích chập. Tuy nhiên, để xác định cấu trúc nào hiệu quả hơn cho suy luận, thông tin phần cứng hoặc FLOPs cũng cần thiết. Với động cơ trên, trong phần này, chúng tôi giới thiệu một Mô-đun Hiệu quả, chứa một Mạng Dự đoán Độ trễ và một đơn vị Ước tính FLOPs để cung cấp hướng dẫn hiệu quả cho W-Gates được đề xuất. Như được hiển thị trong Hình 1, Mô-đun Hiệu quả là một mô-đun có thể chuyển đổi, trong đó Mạng Dự đoán Độ trễ là đơn vị chính để chọn các kiến trúc thân thiện với phần cứng và Ước tính FLOPs là đơn vị thay thế để cung cấp ràng buộc FLOPs.

Mạng Dự đoán Độ trễ. Các công trình trước đây về nén mô hình chủ yếu nhằm giảm tham số và tính toán, nhưng chúng không phải lúc nào cũng phản ánh độ trễ thực tế trên phần cứng. Do đó, một số phương pháp dựa trên NAS gần đây [13], [52] chú ý nhiều hơn đến việc áp dụng độ trễ phần cứng như một

--- TRANG 6 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 6

[Hình 4 mô tả quá trình huấn luyện ngoại tuyến của LPNet]

chỉ báo đánh giá trực tiếp hơn so với tham số và FLOPs. [52] đề xuất xây dựng một bảng tra cứu độ trễ để ước tính độ trễ tổng thể của một mạng, vì họ giả định rằng thời gian chạy của mỗi toán tử độc lập với các toán tử khác, hoạt động tốt trên nhiều thiết bị nối tiếp di động, như CPU và DSP. Các công trình trước đây [13], [52] đã chứng minh hiệu quả của phương pháp này. Tuy nhiên, độ trễ được tạo ra bởi bảng tra cứu không thể khác biệt đối với việc lựa chọn bộ lọc và tỷ lệ cắt tỉa của mỗi lớp.

Để giải quyết vấn đề trên, chúng tôi xây dựng LPNet để dự đoán độ trễ thực của toàn bộ mạng hoặc các khối xây dựng. LPNet được đề xuất có thể khác biệt đầy đủ đối với các cổng bộ lọc và tỷ lệ cắt tỉa của mỗi lớp. Như được hiển thị trong Hình 4, LPNet bao gồm ba lớp kết nối đầy đủ, lấy một vector mã hóa mạng c = (c1, c2, ..., cL) làm đầu vào và xuất ra độ trễ cho nền tảng phần cứng được chỉ định:

lat(c) = LPNet(c1, c2, ..., cL);                (8)

trong đó
cl = ∑(i=1 to Cl) gates^b_i                     (9)

trong khung cắt tỉa.

Để huấn luyện trước LPNet, chúng tôi lấy mẫu các vector mã hóa mạng c từ không gian tìm kiếm và giải mã mạng tương ứng để kiểm tra độ trễ thực của chúng trên các nền tảng phần cứng cụ thể. Như được hiển thị trong Hình 4, trong quá trình huấn luyện, vector mã hóa mạng được áp dụng làm đầu vào và độ trễ trên phần cứng cụ thể được sử dụng làm nhãn. Vì không cần huấn luyện mạng giải mã, chỉ mất vài mili giây để có được một nhãn độ trễ và việc huấn luyện LPNet cũng rất hiệu quả.

Đối với kiến trúc khối xây dựng sâu hơn, như ResNet50, không gian lấy mẫu mã hóa mạng là rất lớn. Chúng tôi chọn dự đoán độ trễ của các khối xây dựng và sau đó tổng chúng lại để có được độ trễ dự đoán của toàn bộ mạng, và điều này sẽ giảm đáng kể không gian lấy mẫu mã hóa. Bên cạnh đó, LPNet của các khối xây dựng cũng có thể được tái sử dụng trên các mô hình có độ sâu khác nhau và các tác vụ khác nhau trên cùng loại phần cứng.

Kết quả là, việc huấn luyện một LPNet như vậy làm cho ràng buộc độ trễ có thể khác biệt đối với mã hóa mạng và các cổng bộ lọc nhị phân được hiển thị trong Hình 1. Do đó chúng ta có thể sử dụng tối ưu hóa dựa trên gradient để điều chỉnh tỷ lệ cắt tỉa bộ lọc

của mỗi lớp tích chập và tự động có được tỷ lệ cắt tỉa tốt nhất.

Ước tính FLOPs. Mặc dù độ trễ phần cứng là phản ánh trực tiếp nhất của hiệu quả suy luận cho các mô hình được cắt tỉa, nó không phải lúc nào cũng có sẵn. Đối với các tình huống có thông tin phần cứng không rõ, chúng tôi thêm Ước tính FLOPs trong Mô-đun Hiệu quả như một đơn vị thay thế để tối ưu hóa FLOPs bằng gradient descent trong lan truyền ngược.

Cho cl-1 biểu thị số kênh đầu vào của lớp l, tức là số kênh đầu ra của lớp l-1. cl biểu thị số kênh đầu ra của lớp l. Lấy cảm hứng từ [19], [62], FLOPs của lớp tích chập l trong mạng được cắt tỉa có thể được công thức hóa như:

Fl = M^l_h × M^l_w × K^l_h × K^l_w × cl-1 × cl;        (10)

trong đó, M^l_h và M^l_w tương ứng là chiều cao và chiều rộng của bản đồ đặc trưng đầu vào trong lớp l. K^l_h và K^l_w biểu thị chiều cao và chiều rộng của kích thước bộ lọc, tương ứng.

Trong tất cả các loại lớp, các lớp tích chập là điểm nóng hiệu suất chính [35]. Do đó, chúng tôi tập trung vào tối ưu hóa FLOPs của các lớp tích chập khi thực hiện cắt tỉa. FLOPs của một CNN với L lớp tích chập có thể được ước tính như sau:

Flops(c) = ∑(l=1 to L) Fl = ∑(l=1 to L) M^l_h × M^l_w × K^l_h × K^l_w × cl-1 × cl;    (11)

trong đó c0 = 3 (số kênh của hình ảnh đầu vào) và cl = ∑(i=1 to Cl) gates^b_i trong khung cắt tỉa.

Ước tính FLOPs ở trên dựa trên các cổng bộ lọc và không cần huấn luyện, có thể được áp dụng để tối ưu hóa quá trình cắt tỉa bằng gradient descent trong tình huống có thông tin phần cứng không rõ.

C. Cắt tỉa Bộ lọc nhận biết Hiệu quả

Nếu thông tin phần cứng có sẵn, phương pháp được đề xuất bao gồm ba giai đoạn chính. Đầu tiên, huấn luyện LPNet ngoại tuyến, như được mô tả trong Phần III-B. Với LPNet được huấn luyện trước, chúng ta có thể có được độ trễ bằng cách đưa vào vector mã hóa của một mạng được cắt tỉa ứng viên. Thứ hai, cắt tỉa mạng dưới ràng buộc độ trễ. Chúng tôi thêm W-Gates và Mô-đun Hiệu quả (LPNet) vào một mạng được huấn luyện trước để thực hiện cắt tỉa bộ lọc, trong đó trọng số của LPNet được cố định. Như được hiển thị trong Hình 1, W-Gates học thông tin từ trọng số tích chập và

--- TRANG 7 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 7

tạo ra các cổng bộ lọc nhị phân để xác định bộ lọc nào cần cắt tỉa. Tiếp theo, LPNet lấy mã hóa mạng của mạng được cắt tỉa ứng viên làm đầu vào và xuất ra một độ trễ dự đoán để tối ưu hóa tỷ lệ cắt tỉa và các cổng bộ lọc của mỗi lớp. Sau đó, mất mát độ chính xác và mất mát hiệu quả cạnh tranh với nhau trong quá trình huấn luyện và cuối cùng thu được một mạng nhỏ gọn với độ chính xác tốt nhất đồng thời đáp ứng ràng buộc độ trễ. Thứ ba, tinh chỉnh mạng. Sau khi có được mạng được cắt tỉa, một quá trình tinh chỉnh chỉ với vài epoch tiếp theo để khôi phục độ chính xác và có được hiệu suất tốt hơn, ít tốn thời gian hơn so với huấn luyện từ đầu.

Hơn nữa, để tạo ra một sự cân bằng độ chính xác-hiệu quả tốt hơn, chúng tôi định nghĩa hàm mất mát nhận biết hiệu quả sau:

ℓ(c; wc) = ℓAcc(c; wc) + α log(1 + ℓEff(c));     (12)

trong đó ℓAcc(c; wc) biểu thị mất mát độ chính xác của một kiến trúc với mã hóa mạng c và tham số wc. ℓEff(c) là dự đoán độ trễ lat(c) hoặc ước tính FLOPs Flops(c) của kiến trúc được cắt tỉa với vector mã hóa mạng c. Hệ số α có thể điều chỉnh độ lớn của thuật ngữ độ trễ. Một hàm mất mát như vậy có thể thực hiện các tác vụ cắt tỉa bộ lọc với việc xem xét cấu trúc mạng tổng thể, có lợi cho việc tìm ra các giải pháp tối ưu cho cắt tỉa mạng. Ngoài ra, hàm này có thể khác biệt đối với lựa chọn bộ lọc theo lớp c và số lượng bộ lọc, cho phép chúng ta sử dụng phương pháp dựa trên gradient để tối ưu hóa chúng và có được sự cân bằng tốt hơn giữa độ chính xác và hiệu quả.

IV. THỰC NGHIỆM

Trong phần này, chúng tôi chứng minh hiệu quả của phương pháp của mình. Đầu tiên, chúng tôi đưa ra mô tả chi tiết về cài đặt thực nghiệm của chúng tôi. Tiếp theo, chúng tôi thực hiện bốn nghiên cứu bóc tách trên tập dữ liệu ImageNet để minh họa hiệu ứng của phần quan trọng W-Gates trong phương pháp của chúng tôi. Sau đó, chúng tôi cắt tỉa ResNet34, ResNet50, MobileNet V2, và VGG16 dưới ràng buộc độ trễ, và so sánh phương pháp của chúng tôi với một số phương pháp cắt tỉa bộ lọc hiện đại. Sau đó, chúng tôi cắt tỉa VGG16 và ResNet56 dưới ràng buộc FLOPs. Cuối cùng, chúng tôi trực quan hóa các kiến trúc được cắt tỉa để khám phá những gì phương pháp của chúng tôi đã học được từ mạng và loại kiến trúc nào có sự cân bằng tốt hơn giữa độ chính xác và hiệu quả.

A. Cài đặt Thực nghiệm

Chúng tôi thực hiện các thực nghiệm trên tập dữ liệu ImageNet ILSVRC 2012 [63] và tập dữ liệu Cifar-10 [64]. ImageNet chứa 1.28 triệu hình ảnh huấn luyện và 50,000 hình ảnh xác thực, được phân loại thành 1000 lớp. Độ phân giải của các hình ảnh đầu vào được đặt thành 224×224. Cifar-10 bao gồm 50,000 hình ảnh huấn luyện, 10,000 hình ảnh kiểm tra, được phân loại thành 10 lớp. Tất cả hình ảnh của Cifar-10 được cắt ngẫu nhiên thành 32×32 với bốn phần đệm và lật ngang cũng được áp dụng. Tất cả các thực nghiệm được triển khai với framework PyTorch và các mạng được huấn luyện bằng stochastic gradient descent (SGD) với momentum được đặt thành 0.9. Đối với ResNet34 và ResNet50, chúng tôi áp dụng cùng sơ đồ huấn luyện trong [1].

Để huấn luyện LPNet ngoại tuyến, chúng tôi lấy mẫu các vector mã hóa mạng c từ không gian tìm kiếm và giải mã mạng tương ứng để kiểm tra độ trễ thực của chúng trên một GPU NVIDIA RTX 2080 Ti làm nhãn độ trễ. Đối với kiến trúc khối xây dựng sâu hơn, như ResNet50, không gian lấy mẫu mã hóa mạng là rất lớn. Chúng tôi chọn dự đoán độ trễ của các khối xây dựng và sau đó tổng chúng lại để có được độ trễ dự đoán của toàn bộ mạng, và điều này sẽ giảm đáng kể không gian lấy mẫu mã hóa. Đối với khối xây dựng bottleneck của ResNet, chúng tôi thu thập 170000 cặp (c, độ trễ) để huấn luyện LPNet của khối xây dựng bottleneck và 5000 cặp (c, độ trễ) để huấn luyện LPNet của khối xây dựng cơ bản. Chúng tôi chọn ngẫu nhiên 80% tập dữ liệu làm dữ liệu huấn luyện và để lại phần còn lại làm dữ liệu kiểm tra. Chúng tôi sử dụng Adam để huấn luyện các LPNet và thấy rằng các lỗi kiểm tra trung bình có thể nhanh chóng hội tụ xuống dưới 2%.

Đối với các quá trình cắt tỉa và tinh chỉnh, tất cả các mạng được huấn luyện bằng stochastic gradient descent (SGD) với momentum được đặt thành 0.9. Trên tập dữ liệu ImageNet, chúng tôi tương ứng huấn luyện ResNets và MobileNet V2 trong 120 epoch và 240 epoch làm baseline. Đối với ResNet34/50, chúng tôi tương ứng đặt 40 và 80 epoch cho các quá trình cắt tỉa và tinh chỉnh. Tỷ lệ học ban đầu của hai quá trình trên tương ứng được đặt thành 10^-3 và 10^-2. Đối với MobileNet V2, chúng tôi tương ứng đặt 120 và 80 epoch cho các quá trình cắt tỉa và tinh chỉnh. Tỷ lệ học ban đầu của hai quá trình trên được đặt thành 10^-3 và 10^-1, tương ứng. Trên tập dữ liệu Cifar-10, đối với tất cả các mô hình, chúng tôi đặt 200 epoch cho quá trình cắt tỉa và quá trình tinh chỉnh. Tỷ lệ học ban đầu được đặt thành 10^-3 và 10^-2, tương ứng. Trên hai tập dữ liệu, trong các quá trình cắt tỉa và tinh chỉnh, tỷ lệ học đều được chia cho 10 ở 50% và 75% tổng số epoch.

B. Nghiên cứu Bóc tách trên ImageNet

Hiệu suất của phương pháp chúng tôi chủ yếu được quy cho Cổng phụ thuộc Trọng số được đề xuất (W-Gates). Để xác thực hiệu quả của W-Gates, chúng tôi chọn kiến trúc được sử dụng rộng rãi ResNet50 và tiến hành một loạt nghiên cứu bóc tách trên tập dữ liệu ImageNet. Trong các phần tiếp theo, đầu tiên chúng tôi khám phá tác động của W-Gates được đề xuất trong quá trình huấn luyện. Sau đó, tác động của thông tin học được từ trọng số bộ lọc được nghiên cứu. Sau đó, chúng tôi minh họa tác động của hàm kích hoạt cổng bằng cách so sánh kích hoạt nhị phân với kích hoạt sigmoid được chia tỷ lệ. Cuối cùng, chúng tôi so sánh phương pháp cắt tỉa dựa trên W-Gates của chúng tôi với một số phương pháp dựa trên cổng hiện đại [12], [41], [42].

1) Tác động của W-Gates trong Quá trình Huấn luyện: Trong phần này, để chứng minh tác động của W-Gates đối với lựa chọn bộ lọc, chúng tôi thêm chúng vào một ResNet50 được huấn luyện trước để học trọng số của mỗi lớp tích chập và thực hiện lựa chọn bộ lọc trong quá trình huấn luyện. Sau đó, chúng tôi tiếp tục huấn luyện và kiểm tra độ chính xác cuối cùng.

Hai bộ thực nghiệm được thiết lập để kiểm tra hiệu ứng của W-Gates trong các giai đoạn khác nhau của quá trình huấn luyện mô hình. Đối với một trong số chúng, đầu tiên chúng tôi huấn luyện mạng trong 1/3 tổng số epoch như một giai đoạn khởi động và sau đó thêm W-Gates vào mạng để tiếp tục huấn luyện. Đối với thực nghiệm khác, chúng tôi thêm W-Gates vào một mô hình được huấn luyện trước và kiểm tra xem liệu nó có thể cải thiện hiệu suất huấn luyện không. Như có thể thấy trong Bảng I, với cùng số lần lặp, việc thêm W-Gates sau giai đoạn khởi động

--- TRANG 8 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 8

BẢNG I
BẢNG NÀY SO SÁNH ĐỘ CHÍNH XÁC TRÊN IMAGE NET VỀ RESNET50. "W-GATES (WARM-UP)" BIỂU THỊ VIỆC THÊM W-GATES VÀO RESNET50 SAU MỘT GIAI ĐOẠN KHỞI ĐỘNG. "W-GATES (PRETRAIN)" CÓ NGHĨA LÀ THÊM W-GATES VÀO MỘT RESNET50 ĐƯỢC HUẤN LUYỆN TRƯỚC. "W-GATES (CONSTANT INPUT)" ÁP DỤNG MỘT ĐẦU VÀO TENSOR KHÔNG ĐỔI ĐỂ THAY THẾ TRỌNG SỐ BỘ LỌC TRONG W-GATES. "PRETRAIN +SAME ITERS" CÓ NGHĨA LÀ TIẾP TỤC HUẤN LUYỆN MẠNG VỚI CÙNG SỐ LẦN LẶP NHƯ "W-GATES (CONSTANT INPUT)" VÀ "W-GATES (PRETRAIN)".

Top1-Acc Top5-Acc
ResNet50 baseline 76.15% 93.11%
pretrain + same iters 76.58% 93.15%
W-Gates(constant input) 75.32% 92.48%
W-Gates(warm-up) 76.79% 93.27%
W-Gates(pretrain) 77.07% 93.44%

BẢNG II
BẢNG NÀY SO SÁNH HAI LOẠI HÀM KÍCH HOẠT: KÍCH HOẠT NHỊ PHÂN VÀ KÍCH HOẠT SIGMOID ĐƯỢC CHIA TỶ LỆ, TRONG ĐÓ HỆ SỐ TỶ LỆ ĐƯỢC ĐẶT THÀNH 4. (1G: 1 E9)

Top1-Acc Top5-Acc FLOPs
ResNet50 baseline 76.15% 93.11% 4.1G
W-Gates(sigmoid) 75.55% 92.62% 2.7G
W-Gates(binary) 76.01% 92.86% 2.7G

có thể đạt được độ chính xác Top-1 cao hơn 0.64% so với kết quả baseline. Hơn nữa, trang bị W-Gates cho một mạng được huấn luyện trước có thể tiếp tục tăng độ chính xác lên 0.92%, cao hơn 0.49% so với kết quả của việc thêm cùng số lần lặp. Những kết quả này cho thấy rằng việc thêm W-Gates vào một mạng được huấn luyện tốt có thể tận dụng tốt hơn tác động lựa chọn kênh của nó và có được các bộ lọc tích chập hiệu quả hơn.

2) Tác động của Thông tin từ Trọng số Bộ lọc: Chúng tôi tò mò về câu hỏi như vậy: Liệu W-Gates có thể thực sự học thông tin từ trọng số bộ lọc tích chập và đưa ra hướng dẫn cho việc lựa chọn bộ lọc không? Một nghiên cứu bóc tách được tiến hành để trả lời câu hỏi này. Đầu tiên, chúng tôi thêm các mô-đun vào một mạng được huấn luyện trước và áp dụng các tensor không đổi có cùng kích thước để thay thế các tensor trọng số bộ lọc tích chập làm đầu vào của W-Gates, và tất cả các giá trị của những tensor không đổi này được đặt thành một. Sau đó chúng tôi tiếp tục huấn luyện mạng trong cùng số lần lặp với "W-Gates(pretrain)" trong Bảng I.

Từ Bảng I, chúng ta thấy rằng W-Gates với tensor không đổi làm đầu vào đạt được độ chính xác Top-1 thấp hơn 1.75% so với W-Gates đầu vào trọng số bộ lọc. Các kết quả trên cho thấy rằng W-Gates có thể học thông tin từ trọng số bộ lọc và tự động thực hiện lựa chọn bộ lọc tốt, góp phần vào việc huấn luyện mạng.

3) Lựa chọn Hàm Kích hoạt Cổng: Có hai loại hàm kích hoạt có thể được sử dụng để có được các cổng bộ lọc, sigmoid được chia tỷ lệ [14] và kích hoạt nhị phân [28], [54]. Các phương pháp trước đây áp dụng một hàm sigmoid hoặc một hàm sigmoid được chia tỷ lệ để tạo ra một vector nhị phân xấp xỉ. Trong loại phương pháp này, cần đặt một ngưỡng và các giá trị nhỏ hơn nó được đặt thành 0. Các công trình mô hình lượng tử hóa đề xuất kích hoạt nhị phân, trực tiếp có được một vector nhị phân thực và thiết kế một hàm có thể khác biệt để xấp xỉ gradient của hàm kích hoạt của nó. Chúng tôi chọn kích hoạt nhị phân ở đây như

BẢNG III
BẢNG NÀY SO SÁNH PHƯƠNG PHÁP W-GATES CỦA CHÚNG TÔI VỚI MỘT SỐ PHƯƠNG PHÁP DỰA TRÊN CỔNG HIỆN ĐẠI. ĐỂ SO SÁNH CÔNG BẰNG, CHÚNG TÔI KHÔNG THÊM LPNET ĐỂ TỐI ƯU HÓA QUÁ TRÌNH CẮT TỈA VÀ CHỈ CẮT TỈA MẠNG VỚI L1-NORM.

Top1-Acc Top5-Acc FLOPs
ResNet50 baseline 76.15% 93.11% 4.1G
SFP [42] 74.61% 92.87% 2.4G
Thinet-70 [41] 75.31% - 2.9G
Slimming [12] 74.79% 92.21% 2.8G
W-Gates 76.01% 92.86% 2.7G
W-Gates 75.74% 92.62% 2.3G

hàm kích hoạt cổng của chúng tôi để tạo ra các cổng bộ lọc nhị phân trong W-Gates.

Để kiểm tra tác động của việc lựa chọn hàm cổng trong phương pháp của chúng tôi, chúng tôi so sánh kết quả cắt tỉa của W-Gates với kích hoạt nhị phân và W-Gates với sigmoid được chia tỷ lệ. Hệ số tỷ lệ của sigmoid k được đặt thành 4, theo cài đặt trong [14]. Để chỉ giữ một yếu tố thay đổi, chúng tôi không áp dụng LPNet của chúng tôi để đưa ra ràng buộc độ trễ, mà đơn giản áp dụng chuẩn hóa L1 lên các cổng bộ lọc của mỗi lớp để có được một mô hình CNN thưa thớt. Như có thể thấy trong Bảng II, với cùng FLOPs, W-Gates với kích hoạt nhị phân đạt được độ chính xác top-1 cao hơn 0.46% so với W-Gates với kích hoạt k-sigmoid, chứng minh rằng kích hoạt nhị phân phù hợp hơn cho phương pháp của chúng tôi.

4) So sánh với Các Phương pháp Dựa trên Cổng Khác: Để kiểm tra thêm xem W-Gates được đề xuất có hoạt động tốt trong cắt tỉa bộ lọc hay không, chúng tôi so sánh phương pháp của mình với các phương pháp dựa trên cổng hiện đại [12], [41], [42]. Đối với Slimming [12], không có kết quả cắt tỉa trên ImageNet. Để giữ cài đặt của chúng tôi gần với bài báo gốc nhất có thể, chúng tôi áp dụng triển khai gốc có sẵn công khai và thực thi trên Imagenet. Để so sánh công bằng, chúng tôi không thêm LPNet để tối ưu hóa quá trình cắt tỉa mà đơn giản cắt tỉa mạng với L1-norm, phù hợp với các phương pháp dựa trên cổng khác. Kết quả cho tập dữ liệu ImageNet được tóm tắt trong Bảng III. W-Gates đạt được kết quả vượt trội hơn SFP [42], Thinet [41] và Slimming [12]. Kết quả cho thấy rằng Cổng phụ thuộc Trọng số được đề xuất có thể giúp mạng thực hiện lựa chọn bộ lọc tự tốt hơn trong quá trình huấn luyện.

C. Kết quả Cắt tỉa dưới Ràng buộc Độ trễ

Sự không nhất quán giữa các thước đo bất khả tri phần cứng và hiệu quả thực tế dẫn đến sự chú ý ngày càng tăng trong việc tối ưu hóa trực tiếp độ trễ trên các thiết bị đích. Lấy CNN như một hộp đen, chúng tôi huấn luyện một LPNet để dự đoán độ trễ thực trong thiết bị đích. Đối với ResNet34 và ResNet50, chúng tôi huấn luyện hai LPNet ngoại tuyến để dự đoán độ trễ của các khối xây dựng cơ bản và các khối xây dựng bottleneck, tương ứng. Để xem xét đầy đủ tất cả các yếu tố và giải mã kiến trúc thưa thớt, chúng tôi thêm các yếu tố của kích thước bản đồ đặc trưng và downsampling vào vector mã hóa mạng. Đối với những kiến trúc có shortcut, chúng tôi không cắt tỉa các kênh đầu ra của lớp cuối trong mỗi khối xây dựng để tránh không khớp với các kênh shortcut.

1) Kết quả cắt tỉa trên ResNet34: Chúng tôi đầu tiên thực hiện các thực nghiệm cắt tỉa trên một mạng độ sâu trung bình ResNet34.

--- TRANG 9 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 9

BẢNG IV
BẢNG NÀY SO SÁNH ĐỘ CHÍNH XÁC TOP-1 VÀ ĐỘ TRỄ TRÊN IMAGE NET VỀ RESNET34. CHÚNG TÔI ĐẶT CÙNG TỶ LỆ NÉN CHO MỖI LỚP NHƯ BASELINE ĐỒNG NHẤT. KÍCH THƯỚC BATCH ĐẦU VÀO ĐƯỢC ĐẶT THÀNH 100 VÀ ĐỘ TRỄ ĐƯỢC ĐO BẰNG PYTORCH TRÊN GPU NVIDIA RTX 2080 TI.

Uniform Baselines W-Gates
FLOPs Top1-Acc Latency FLOPs Top1-Acc Latency
3.7G (1X) 73.88% 54.04ms - - -
2.9G 72.56% 49.23ms 2.8G 73.76% 46.67ms
2.4G 72.05% 44.27ms 2.3G 73.35% 40.75ms
2.1G 71.32% 43.47ms 2.0G 72.65% 37.30ms

BẢNG V
BẢNG NÀY SO SÁNH ĐỘ CHÍNH XÁC TOP-1 VÀ ĐỘ TRỄ TRÊN IMAGE NET VỀ RESNET50. KẾT QUẢ CHO THẤY RẰNG, VỚI CÙNG FLOPS, PHƯƠNG PHÁP CỦA CHÚNG TÔI VƯỢT TRỘI HƠN CÁC BASELINE ĐỒNG NHẤT VỀ ĐỘ CHÍNH XÁC VÀ ĐỘ TRỄ VỚI MỘT BIÊN ĐỘ LỚN.

Uniform Baselines W-Gates
FLOPs Top1-Acc Latency FLOPs Top1-Acc Latency
4.1G (1X) 76.15% 105.75ms - - -
3.1G 75.59% 97.87ms 3.0G 76.26% 95.15ms
2.6G 74.77% 91.53ms 2.6G 76.05% 88.00ms
2.1G 74.42% 85.20ms 2.1G 75.14% 80.17ms

ResNet34 bao gồm các khối xây dựng cơ bản, mỗi khối xây dựng cơ bản chứa hai lớp tích chập 3×3. Chúng tôi thêm W-Gates được thiết kế vào lớp đầu tiên của mỗi khối xây dựng cơ bản để học thông tin và tự động thực hiện lựa chọn bộ lọc. LPNets cũng được thêm vào khung W-Gates để dự đoán độ trễ của mỗi khối xây dựng. Sau đó chúng ta có được độ trễ của toàn bộ mạng để hướng dẫn việc cắt tỉa mạng và tối ưu hóa tỷ lệ cắt tỉa của mỗi lớp. Kết quả cắt tỉa được hiển thị trong Bảng IV. Chúng tôi đặt cùng tỷ lệ nén cho mỗi lớp trong ResNet34 như các baseline đồng nhất và cắt tỉa các lớp tương tự với W-Gates. Chúng tôi đo độ trễ phần cứng thực bằng Pytorch trên GPU NVIDIA RTX 2080 Ti và kích thước batch của hình ảnh đầu vào được đặt thành 100. Có thể quan sát thấy rằng phương pháp của chúng tôi có thể tiết kiệm 25% độ trễ phần cứng chỉ với 0.5% mất mát độ chính xác trên tập dữ liệu ImageNet. Với cùng FLOPs, W-Gates đạt được độ chính xác Top-1 cao hơn 1.1% đến 1.3% so với baseline đồng nhất, và độ trễ phần cứng cũng thấp hơn, cho thấy rằng phương pháp W-Gates của chúng tôi có thể tự động cắt tỉa và có được các kiến trúc hiệu quả.

2) Kết quả cắt tỉa trên ResNet50: Đối với mạng sâu hơn ResNet50, chúng tôi áp dụng cùng cài đặt với ResNet34. ResNet50 bao gồm các khối xây dựng bottleneck, mỗi khối chứa một lớp 3×3 và hai lớp 1×1. Chúng tôi sử dụng W-Gates để cắt tỉa các bộ lọc của hai lớp đầu tiên trong mỗi mô-đun bottleneck trong quá trình huấn luyện. Độ chính xác Top-1 và độ trễ phần cứng của các mô hình được cắt tỉa được hiển thị trong Bảng V. Khi cắt tỉa 37% FLOPs, chúng ta có thể tiết kiệm 17% độ trễ phần cứng mà không mất mát độ chính xác đáng chú ý. Sau đó chúng tôi vẽ hai bộ kết quả cắt tỉa trên ResNet34 và ResNet50 trong Hình 5, trong đó α trong Hàm (12) được đặt thành 1.5 để điều chỉnh độ lớn của thuật ngữ độ trễ. Như có thể thấy từ xu hướng độ chính xác và độ trễ, trong quá trình huấn luyện và lựa chọn, khi độ trễ giảm, độ chính xác đầu tiên giảm và sau đó tăng. Hai

BẢNG VI
GIÁ TRỊ HỆ SỐ VÀ ĐỘ TRỄ TƯƠNG ỨNG CỦA MẠNG NHỎ GỌN KHI CẮT TỈA RESNET34 TRÊN IMAGE NET. HỆ SỐ CÓ THỂ ĐIỀU CHỈNH ĐỘ LỚN CỦA THUẬT NGỮ ĐỘ TRỄ TRONG HÀM MẤT MÁT. NHƯ CHỈ RA TRONG KẾT QUẢ, ĐỘ TRỄ THỰC TẾ CUỐI CÙNG SẼ GIẢM DẦN KHI ALPHA TĂNG.

α 0 (Baseline) 1.5 2.0 3.0
Top1 Acc 73.88% 73.76% 73.35% 72.65%
Latency 54.04ms 46.67ms 40.75ms 37.30ms

hình này cho thấy rằng W-Gates cố gắng tìm kiếm sự cân bằng độ chính xác-hiệu quả tốt hơn thông qua việc học và lựa chọn bộ lọc trong quá trình huấn luyện.

3) Hiệu ứng của Hệ số α đối với Sự Cân bằng Cuối cùng: Vì toàn bộ khung cắt tỉa có thể khác biệt đầy đủ, chúng ta có thể đồng thời áp dụng gradient của mất mát độ chính xác và mất mát độ trễ để tối ưu hóa W-Gates của mỗi lớp. Mất mát độ chính xác kéo các cổng nhị phân về nhiều số một hơn và mất mát độ trễ kéo các cổng nhị phân về nhiều số không hơn. Chúng cạnh tranh với nhau trong quá trình huấn luyện và cuối cùng thu được một mạng nhỏ gọn với sự cân bằng độ chính xác-hiệu quả tốt hơn.

Trong hàm mất mát nhận biết độ trễ được đề xuất, hệ số α có thể điều chỉnh độ lớn của thuật ngữ độ trễ. Bây giờ, chúng tôi quan tâm đến hiệu ứng của α đối với sự cân bằng cuối cùng. Bảng VI hiển thị các giá trị α và độ trễ tương ứng của mạng nhỏ gọn khi cắt tỉa ResNet34 trên ImageNet. Kích thước batch đầu vào được đặt thành 100 và độ trễ được đo bằng Pytorch trên GPU NVIDIA RTX 2080 Ti. Nó cho thấy rằng độ trễ thực tế cuối cùng sẽ giảm dần khi alpha tăng. Đối với một alpha cho trước, độ trễ sẽ giảm dần theo việc huấn luyện mạng, và cuối cùng hội tụ đến một sự cân bằng độ chính xác-độ trễ tốt nhất. Có thể thấy rằng phương pháp của chúng tôi đạt được tăng tốc độ trễ 1.33X chỉ với 0.53% mất mát độ chính xác, cho thấy rằng W-Gates rất mạnh mẽ đối với việc lựa chọn α và có thể liên tục mang lại độ trễ thấp hơn mà không mất mát độ chính xác đáng chú ý.

4) So sánh với Các Phương pháp Hiện đại.: Trong phần này, chúng tôi so sánh phương pháp được đề xuất với một số phương pháp cắt tỉa bộ lọc hiện đại, bao gồm các chỉ báo được thiết kế thủ công (IENNP [16], FPGM [17], VCNNP [22], và HRank [18]), các phương pháp dựa trên tối ưu hóa (CCP [20], RRBP [23], C-SGD [21]), và các phương pháp dựa trên tìm kiếm (DMCP [19], và S-MobileNet V2 [24]), trên ResNet34, ResNet50, và MobileNet V2. 'W-Gates' trong Bảng VII biểu thị kết quả cắt tỉa của chúng tôi với độ trễ phần cứng và FLOPs khác nhau. Vì không có dữ liệu độ trễ được cung cấp trong những công trình này, chúng tôi chỉ so sánh độ chính xác Top1 với cùng FLOPs. Có thể quan sát thấy rằng, so với các phương pháp cắt tỉa bộ lọc hiện đại, W-Gates đạt được độ chính xác cao hơn hoặc tương đương với cùng FLOPs. Đặc biệt, so với C-SGD [21], W-Gates đạt được kết quả cao hơn hoặc tương đương (75.96% so với 75.27%, 75.14% so với 74.93%, 74.32% so với 74.54%). So với Hrank [18], W-Gates đạt được độ chính xác cao hơn dưới 2.4G FLOPs (75.96% so với 74.98%). Khi FLOPs của Hrank được nén xuống 1.6G, độ chính xác của nó chỉ là 71.98%, thấp hơn 2.34% so với mô hình 1.9G của W-Gates, mặc dù khoảng cách FLOPs của chúng chỉ là 0.3G. Điều này là do W-Gates đang cố gắng tìm một sự cân bằng tốt hơn

--- TRANG 10 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 10

[Hình 5 hiển thị hai bộ thực nghiệm cắt tỉa trên ResNet34 và ResNet50]

BẢNG VII
BẢNG NÀY SO SÁNH ĐỘ CHÍNH XÁC TOP-1 IMAGE NET CỦA PHƯƠNG PHÁP W-GATES VÀ CÁC PHƯƠNG PHÁP CẮT TỈA HIỆN ĐẠI

Model Methods FLOPs Top1 Acc
ResNet34 IENNP [16] 2.8G 72.83%
FPGM [17] 2.2G 72.63%
W-Gates 2.8G 73.76%
W-Gates 2.3G 73.35%

ResNet50 VCNNP [22] 2.4G 75.20%
FPGM [17] 2.4G 75.59%
FPGM [17] 1.9G 74.83%
IENNP [16] 2.2G 74.50%
DMCP [19] 2.2G 76.20%
CCP [20] 2.1G 75.50%
RRBP [23] 1.9G 73.00%
C-SGD-70 [21] 2.6G 75.27%
C-SGD-60 [21] 2.2G 74.93%
C-SGD-50 [21] 1.8G 74.54%
SRR-GR [44] 2.3G 75.76%
CLR-RNF [65] 2.5G 74.85%
HAP [66] 2.7G 75.12%
FilterSketch [67] 2.6G 75.22%
FilterSketch [67] 2.2G 74.68%
HRank [18] 2.3G 74.98%
HRank [18] 1.6G 71.98%
W-Gates 3.0G 76.26%
W-Gates 2.4G 75.96%
W-Gates 2.1G 75.14%
W-Gates 1.9G 74.32%

MobileNet V2 S-MobileNet V2 [24] 0.30G 70.5%
S-MobileNet V2 [24] 0.21G 68.9%
0.75x MobileNetV2 [8] 0.22G 69.8%
W-Gates 0.29G 73.2%
W-Gates 0.22G 70.9%

giữa hiệu suất và hiệu quả trong quá trình cắt tỉa.

5) Kết quả Cắt tỉa trên Cifar-10: Trong phần này, chúng tôi đánh giá phương pháp được đề xuất trên tập dữ liệu Cifar-10. Các thực nghiệm cắt tỉa được tiến hành trên hai mô hình tổng quát trên Cifar-10, VGG16 và ResNet56. Kết quả được hiển thị trong Bảng VIII. Có thể quan sát thấy rằng W-Gates được đề xuất có thể

BẢNG VIII
BẢNG NÀY SO SÁNH KẾT QUẢ CẮT TỈA DƯỚI RÀNG BUỘC ĐỘ TRỄ TRÊN CIFAR-10 VỀ VGG16 VÀ RESNET56. XEM XÉTĈNGKÍCH THƯỚC HÌNH ẢNH CỦA CIFAR-10 RẤT NHỎ (CHỈ 32×32), CHÚNG TÔI ĐẶT KÍCH THƯỚC BATCH KIỂM TRA THÀNH 10^4 ĐỂ ĐẢM BẢO TÍNH ỔN ĐỊNH CỦA KẾT QUẢ KIỂM TRA ĐỘ TRỄ. (1M: 1 E6)

Model Methods FLOPs Top1 Acc Latency
VGG16 baseline 313M 93.72% 391.1ms
VCNNP [22] 190M 93.18% -
HRank [18] 146M 93.43% -
W-Gates 162M 93.61% 245.0ms

ResNet56 baseline 126.8M 93.85% 501.8ms
He et al. [9] 63.4M 92.64% -
He et al. [9] 62.0M 90.80% -
FPGM [17] 60.1M 92.89% -
FSDP [68] 64.4M 92.64% -
PARI [69] 60.1M 93.05% -
CHIP [70] 34.8M 92.05% -
ResRep [48] 28.1M 92.66% -
FilterSketch [67] 32.5M 91.20% -
HRank [18] 32.5M 90.72% -
W-Gates 60.0M 93.54% 366.6ms
W-Gates 31.1M 92.66% 319.7ms

cũng đạt được hiệu suất tốt trên tập dữ liệu Cifar-10, vượt trội hơn các phương pháp hiện đại VCNNP [22], HRank [18], He et al. [9], FSDP [68], FPGM [17], PARI [69], CHIP [70], ResRep [48], và FilterSketch [67]. Đối với cấu trúc đơn giản VGG16, W-Gates có thể giảm 37% độ trễ phần cứng và 48% FLOPs chỉ với 0.11% mất mát độ chính xác. Tương tự, đối với kiến trúc ResNet56 có kết nối shortcut, W-Gates có thể giảm 36% độ trễ phần cứng và 75% FLOPs chỉ với 1.2% mất mát độ chính xác. So với các phương pháp hiện đại ([9], [17], [18], [22], [48], [67], [68], [69], [70]), W-Gates đạt được kết quả cao hơn hoặc tương đương. So với ResRep [48], W-Gates có hai ưu điểm. Đầu tiên, ngoài tối ưu hóa FLOPs, W-Gates có thể tối ưu hóa trực tiếp độ trễ suy luận của CNNs trên nền tảng phần cứng đích theo cách dựa trên gradient, điều này không có sẵn trong ResRep [48]. Thứ hai, ResRep [48] tăng dần tỷ lệ cắt tỉa của mỗi lớp với một bước cắt tỉa bộ lọc được định nghĩa trước (bước=4), trong khi W-gates có thể linh hoạt tăng hoặc giảm tỷ lệ cắt tỉa của

--- TRANG 11 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 11

BẢNG IX
BẢNG NÀY SO SÁNH KẾT QUẢ CẮT TỈA DƯỚI RÀNG BUỘC FLOPS TRÊN CIFAR-10 VỀ VGG16 VÀ RESNET56.

Model Methods FLOPs Top1 Acc
VGG16 baseline 313M 93.72%
VCNNP [22] 190M 93.18%
HRank [18] 146M 93.43%
W-Gates 176M 93.49%

ResNet56 baseline 126.8M 93.85%
He et al. [9] 62.0M 90.80%
HRank [18] 32.5M 90.72%
W-Gates 27.1M 91.60%

BẢNG X
BẢNG NÀY SO SÁNH ĐỘ CHÍNH XÁC TOP-1 CỦA CÁC MÔ HÌNH ĐƯỢC CẮT TỈA TRƯỚC VÀ SAU QUÁ TRÌNH TINH CHỈNH DƯỚI RÀNG BUỘC FLOPS.

Model Top-1 Acc (trước) Top-1 Acc (sau) Δ
VGG16 92.82% 93.49% 0.67%
ResNet56 91.20% 91.60% 0.40%

mỗi lớp dựa trên gradient trong lan truyền ngược để có được cấu trúc CNN thân thiện với phần cứng hơn. Kết quả trong Bảng VIII cũng có thể chứng minh khả năng khái quát hóa tốt của W-Gates được đề xuất.

D. Kết quả Cắt tỉa dưới Ràng buộc FLOPs

Đối với các tình huống mà thông tin phần cứng không có sẵn, chúng ta không thể tối ưu hóa trực tiếp quá trình cắt tỉa dưới ràng buộc độ trễ. Với Mô-đun Hiệu quả được đề xuất, chúng ta có thể chuyển nó sang Ước tính FLOPs để đối phó tốt với những tình huống này. Trong phần này, chúng tôi đánh giá khung cắt tỉa được đề xuất dưới ràng buộc FLOPs. Các thực nghiệm được tiến hành trên CIFAR10.

Bảng IX tóm tắt cải thiện đạt được thông qua việc áp dụng ràng buộc FLOPs trong cắt tỉa bộ lọc. Quan sát chính của chúng tôi là phương pháp của chúng tôi có thể có giảm FLOPs đáng kể với mất mát độ chính xác không đáng kể so với các phương pháp hiện đại khác. Đối với kiến trúc đơn giản VGG16, phương pháp của chúng tôi giảm 43.8% FLOPs chỉ với 0.23% mất mát độ chính xác. Đối với mô hình sâu hơn ResNet56, phương pháp của chúng tôi đạt được giảm 78.6% FLOPs chỉ với 2.25% mất mát độ chính xác. Từ kết quả chúng ta cũng có thể thấy rằng độ chính xác trong Bảng IX thấp hơn một chút so với hiệu suất cắt tỉa dưới ràng buộc độ trễ trong Bảng VIII. Lý do là tối ưu hóa FLOPs có thể chủ yếu tập trung vào cắt tỉa các lớp nông của mạng (các điểm nóng FLOPs chính), có thể làm hỏng tính đa dạng của các đặc trưng mức thấp được trích xuất bởi các lớp nông của mạng. Ngược lại, tối ưu hóa độ trễ chú ý nhiều hơn đến tính thân thiện với phần cứng của toàn bộ kiến trúc được cắt tỉa, thay vì chỉ tập trung vào các phần cục bộ của mạng. May mắn thay, so với các phương pháp hiện đại [9], [18], [22], phương pháp của chúng tôi dưới ràng buộc FLOPs cũng có thể đạt được hiệu suất vượt trội hoặc tương đương, cho thấy rằng nó là một ràng buộc phụ trợ tốt.

Bảng X hiển thị độ chính xác top-1 của các mô hình được cắt tỉa trước và sau quá trình tinh chỉnh dưới ràng buộc FLOPs. Quan sát thấy rằng khi quá trình cắt tỉa vừa

hoàn thành, W-Gates đã đạt được hiệu suất tốt trên VGG16 và ResNet56. Sau quá trình tinh chỉnh, độ chính xác Top-1 của VGG16 và ResNet56 có thể được khôi phục thêm tương ứng 0.67% và 0.40%. Những kết quả này cho thấy rằng hiệu suất của W-Gates chủ yếu đến từ giai đoạn cắt tỉa. Lý do là, với hàm mất mát nhận biết hiệu quả được đề xuất, W-Gates có thể thực hiện các tác vụ cắt tỉa bộ lọc với việc xem xét cấu trúc mạng tổng thể, có lợi cho việc có được sự cân bằng tốt hơn giữa độ chính xác và hiệu quả và tìm ra các giải pháp tối ưu cho cắt tỉa mạng.

E. Phân tích Trực quan hóa

Trong quá trình cắt tỉa bộ lọc, chúng tôi tò mò về những gì W-Gates đã học được từ mạng và loại kiến trúc nào có sự cân bằng tốt hơn về độ chính xác-độ trễ. Trong việc trực quan hóa các kiến trúc được cắt tỉa của ResNet34 và ResNet50, chúng tôi thấy rằng W-Gates đã học được điều gì đó thú vị.

--- TRANG 12 ---
TẠP CHÍ CÁC TỆP LỚP L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 12

[Hình 6 và 7 hiển thị trực quan hóa kết quả cắt tỉa]

Trực quan hóa kết quả cắt tỉa ResNet34 dưới ràng buộc độ trễ trên ImageNet được hiển thị trong Hình 6. Có thể quan sát thấy rằng đối với kiến trúc ResNet34 bao gồm các khối xây dựng cơ bản, W-Gates có xu hướng không cắt tỉa lớp có phép toán downsampling, mặc dù một tỷ lệ lớn bộ lọc trong các lớp khác đã được cắt tỉa. Điều này tương tự như kết quả trong [13] trên MobileNet, nhưng cực đoan hơn trong các thực nghiệm của chúng tôi trên ResNet34. Có thể do mạng cần nhiều bộ lọc hơn để giữ lại thông tin để bù đắp cho việc mất thông tin do downsampling bản đồ đặc trưng.

Tuy nhiên, đối với kiến trúc ResNet50 bao gồm các khối xây dựng bottleneck, hiện tượng này khác. Như có thể thấy trong Hình 7, W-Gates có xu hướng không cắt tỉa các lớp tích chập 1×1 và cắt tỉa các lớp tích chập 3×3 với tỷ lệ lớn. Nó cho thấy rằng W-Gates học tự động rằng các lớp tích chập 3×3 có sự dư thừa thông tin lớn hơn so với các lớp tích chập 1×1, và đóng góp nhiều hơn vào độ trễ phần cứng.

V. KẾT LUẬN

Trong bài báo này, chúng tôi đề xuất một phương pháp cắt tỉa bộ lọc mới để đồng thời giải quyết các vấn đề về chỉ báo cắt tỉa, tỷ lệ cắt tỉa, và ràng buộc nền tảng. Chúng tôi đầu tiên đề xuất các cổng phụ thuộc trọng số để học thông tin từ trọng số tích chập và tạo ra các cổng bộ lọc phụ thuộc trọng số mới. Sau đó, chúng tôi xây dựng một Mô-đun Hiệu quả có thể chuyển đổi để dự đoán độ trễ hoặc FLOPs của các mạng được cắt tỉa ứng viên và cung cấp ràng buộc hiệu quả cho các cổng phụ thuộc trọng số trong quá trình cắt tỉa. Toàn bộ khung có thể khác biệt đầy đủ đối với lựa chọn bộ lọc và tỷ lệ cắt tỉa, có thể được tối ưu hóa bằng phương pháp dựa trên gradient để đạt được kết quả cắt tỉa tốt hơn.

Tuy nhiên, phương pháp được đề xuất cũng có một số hạn chế. Một mặt, đối với kiến trúc ResNet có kết nối shortcut, chúng tôi không cắt tỉa các kênh đầu ra của lớp cuối trong mỗi khối xây dựng, để lại một số không gian nén chưa được sử dụng. Trong nghiên cứu tương lai của chúng tôi, chúng tôi sẽ cố gắng tìm một cách tốt hơn để giải quyết vấn đề này. Mặt khác, chúng tôi chỉ tập trung vào các tác vụ phân loại trong bài báo này và không áp dụng phương pháp được đề xuất cho các loại ứng dụng khác. Trong công trình tương lai, chúng tôi sẽ cố gắng chuyển phương pháp được đề xuất sang các tình huống ứng dụng khác, như phát hiện đối tượng, phân đoạn ngữ nghĩa, v.v.

TÀI LIỆU THAM KHẢO

[1] K. He, X. Zhang, S. Ren, và J. Sun, "Deep residual learning for image recognition," trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, trang 770–778.

[2] K. Wang, D. Zhang, Y. Li, R. Zhang, và L. Lin, "Cost-effective active learning for deep image classification," IEEE Transactions on Circuits and Systems for Video Technology, tập 27, số 12, trang 2591–2600, 2016.

[3] J. Zhuang, Z. Wang, và B. Wang, "Video semantic segmentation with distortion-aware feature correction," IEEE Transactions on Circuits and Systems for Video Technology, 2020.

[4] X. Wang, Z. Chen, J. Tang, B. Luo, Y. Wang, Y. Tian, và F. Wu, "Dynamic attention guided multi-trajectory analysis for single object tracking," IEEE Transactions on Circuits and Systems for Video Technology, 2021.

[5] R. Girshick, J. Donahue, T. Darrell, và J. Malik, "Rich feature hierarchies for accurate object detection and semantic segmentation," trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, trang 580–587.

[6] J. Guo, W. Zhang, W. Ouyang, và D. Xu, "Model compression using progressive channel pruning," IEEE Transactions on Circuits and Systems for Video Technology, tập 31, số 3, trang 1114–1124, 2020.

[7] H. Li, A. Kadav, I. Durdanovic, H. Samet, và H. P. Graf, "Pruning filters for efficient convnets," trong Proceedings of International Conference on Learning Representations, 2017.

[8] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, và L.-C. Chen, "Mobilenetv2: Inverted residuals and linear bottlenecks," trong The IEEE Conference on Computer Vision and Pattern Recognition, tháng 6 2018.

[9] Y. He, X. Zhang, và J. Sun, "Channel pruning for accelerating very deep neural networks," trong Proceedings of the IEEE international conference on computer vision, 2017.

[10] Y. Li, S. Lin, B. Zhang, J. Liu, D. Doermann, Y. Wu, F. Huang, và R. Ji, "Exploiting kernel sparsity and entropy for interpretable cnn compression," trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, trang 2800–2809.

[11] Z. Huang và N. Wang, "Data-driven sparse structure selection for deep neural networks," trong Proceedings of the European Conference on Computer Vision, 2018, trang 304–320.

[12] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, và C. Zhang, "Learning efficient convolutional networks through network slimming," trong Proceedings of the IEEE International Conference on Computer Vision, 2017, trang 2736–2744.

[13] Z. Liu, H. Mu, X. Zhang, Z. Guo, X. Yang, K.-T. Cheng, và J. Sun, "Metapruning: Meta learning for automatic neural network channel pruning," trong Proceedings of the IEEE international conference on computer vision, 2019, trang 3296–3305.
