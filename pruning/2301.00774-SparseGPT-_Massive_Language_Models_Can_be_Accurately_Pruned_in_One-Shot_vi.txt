--- TRANG 7 ---
SparseGPT: Các Mô hình Ngôn ngữ Lớn Có thể được Cắt tỉa Chính xác trong Một lần

Thí nghiệm ZeroShot. Để bổ sung cho các đánh giá perplexity, chúng tôi cung cấp kết quả trên một số tác vụ ZeroShot. Các đánh giá này được biết là tương đối nhiễu (Dettmers et al., 2022), nhưng có thể diễn giải hơn. Vui lòng xem Bảng 2.

Tổng thể, một xu hướng tương tự xảy ra, với các mô hình được cắt tỉa magnitude sụp đổ xuống gần hiệu suất ngẫu nhiên, trong khi các mô hình SparseGPT duy trì gần với độ chính xác ban đầu. Tuy nhiên, như mong đợi, các số này nhiễu hơn: cắt tỉa 2:4 xuất hiện đạt được độ chính xác cao hơn đáng kể so với mô hình dày đặc trên Lambada, bất chấp là mẫu độ thưa bị ràng buộc nhất. Các hiệu ứng này cuối cùng được cân bằng khi xem xét nhiều tác vụ khác nhau, điều này phù hợp với văn học (Yao et al., 2022; Dettmers et al., 2022; Dettmers & Zettlemoyer, 2022).

101102
#Tham số tính bằng Tỷ810121416Perplexity trên raw-WikiText2
3bit GPTQ
50% + 4bit
Dày đặc

Hình 6. So sánh 50% độ thưa + lượng tử hóa 4-bit kết hợp với 3-bit tương đương kích thước trên họ OPT cho 2.7B tham số.

Làm thưa & Lượng tử hóa Kết hợp. Một hướng nghiên cứu thú vị khác là sự kết hợp của độ thưa và lượng tử hóa, điều này sẽ cho phép kết hợp tăng tốc tính toán từ độ thưa (Kurtz et al., 2020; Elsen et al., 2020) với tiết kiệm bộ nhớ từ lượng tử hóa (Frantar et al., 2022a; Dettmers et al., 2022; Dettmers & Zettlemoyer, 2022). Cụ thể, nếu chúng ta nén một mô hình thành 50% thưa + trọng số 4-bit, chỉ lưu trữ các trọng số khác không và sử dụng một bitmask để chỉ ra vị trí của chúng, thì điều này có cùng mức tiêu thụ bộ nhớ tổng thể như lượng tử hóa 3-bit.

Do đó, trong Hình 6 (phải) chúng tôi so sánh SparseGPT 50% + 4-bit với các số GPTQ (Frantar et al., 2022a) 3-bit tiên tiến. Có thể thấy rằng các mô hình 50% + 4-bit chính xác hơn các phiên bản 3-bit tương ứng của chúng cho các mô hình 2.7B+ tham số, bao gồm 175B với 8.29 vs. 8.68 3-bit. Chúng tôi cũng thử nghiệm 2:4 và 4:8 kết hợp với 4-bit trên OPT-175B mang lại 8.55 và 8.85 perplexity, cho thấy rằng lượng tử hóa trọng số 4bit chỉ mang lại sự gia tăng perplexity 0:1 trên đầu độ thưa bán cấu trúc.

Độ nhạy & Độ thưa N:M Một phần. Một câu hỏi thực tế quan trọng liên quan đến cắt tỉa n:m là phải làm gì khi mô hình được làm thưa hoàn toàn không đủ chính xác? Mức độ thưa tổng thể không thể được giảm xuống đồng đều, thay vào đó người ta phải chọn một tập con các lớp để n:m-làm thưa hoàn toàn. Bây giờ chúng tôi điều tra lựa chọn tốt nào trong bối cảnh các mô hình ngôn ngữ cực lớn: chúng tôi giả định rằng 2/3 các lớp của OPT-175B/BLOOM-176B nên được cắt tỉa thành độ thưa 2:4 và xem xét việc bỏ qua một trong các loại lớp (attention, fully-connected-1, fully-connected-2) hoặc bỏ qua một phần ba các lớp liên tiếp (đầu, giữa, cuối). Kết quả được hiển thị trong Hình 7.

Mặc dù độ nhạy của các loại lớp khác nhau đáng kể giữa các mô hình, dường như có một xu hướng rõ ràng khi nói đến các phần mô hình: các lớp sau nhạy cảm hơn các lớp trước; bỏ qua một phần ba cuối của mô hình cho độ chính xác tốt nhất. Điều này có một hệ quả thực tế quan trọng là, do tính chất tuần tự của SparseGPT, chúng ta có thể tạo ra một chuỗi các mô hình được làm thưa 2:4 ngày càng tăng (ví dụ 1/2, 2/3, 3/4, . . . ) trong một lượt cắt tỉa duy nhất bằng cách kết hợp x lớp đầu tiên từ một lần chạy SparseGPT với n-x lớp cuối của mô hình ban đầu. Độ chính xác của các chuỗi mô hình như vậy được hiển thị trong Phụ lục D.

5. Công việc Liên quan

Phương pháp Cắt tỉa. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên điều tra cắt tỉa các mô hình GPT quy mô lớn, ví dụ với hơn 10 tỷ tham số. Một lý do cho khoảng trống đáng ngạc nhiên này là thực tế rằng hầu hết các phương pháp cắt tỉa hiện có, ví dụ (Han et al., 2016; Gale et al., 2019; Kurtic & Alistarh, 2022), yêu cầu huấn luyện lại rộng rãi sau bước cắt tỉa để phục hồi độ chính xác, trong khi các mô hình quy mô GPT thường yêu cầu lượng lớn tính toán và điều chỉnh tham số cả cho huấn luyện hoặc tinh chỉnh (Zhang et al., 2022). SparseGPT là một phương pháp hậu huấn luyện cho các mô hình quy mô GPT, vì nó không thực hiện bất kỳ tinh chỉnh nào. Cho đến nay, các phương pháp cắt tỉa hậu huấn luyện chỉ được điều tra ở quy mô các mô hình CNN cổ điển hoặc loại BERT (Hubara et al., 2021a; Frantar et al., 2022b; Kwon et al., 2022), có 100-1000× ít trọng số hơn các mô hình quan tâm của chúng tôi. Chúng tôi đã thảo luận về những thách thức của việc mở rộng các phương pháp này, và mối quan hệ của chúng với SparseGPT, trong Mục 2.

Lượng tử hóa Hậu Huấn luyện. Ngược lại, đã có công việc đáng kể về các phương pháp hậu huấn luyện để lượng tử hóa các mô hình GPT quy mô mở (Zhang et al., 2022; Scao et al., 2022). Cụ thể, các phương pháp ZeroQuant (Yao et al., 2022), LLM.int8() (Dettmers et al., 2022) và nuQmm (Park et al., 2022a) đã điều tra tính khả thi của lượng tử hóa làm tròn gần nhất cho các mô hình hàng tỷ tham số, cho thấy rằng lượng tử hóa 8-bit cho trọng số là khả thi thông qua phương pháp này, nhưng lượng tử hóa kích hoạt có thể khó khăn do sự tồn tại của các đặc trưng ngoại lệ. Frantar et al. (2022a) tận dụng thông tin bậc hai xấp xỉ để lượng tử hóa chính xác trọng số xuống 2-4 bit, cho các mô hình lớn nhất, và cho thấy tăng tốc suy luận sinh tạo batch-size 1 từ 2-5× khi kết hợp với các kernel GPU hiệu quả. Công việc tiếp theo (Xiao et al., 2022) đã điều tra lượng tử hóa kích hoạt và trọng số kết hợp xuống 8 bit, đề xuất một lược đồ dựa trên làm mịn giảm độ khó của lượng tử hóa kích hoạt và được bổ sung bởi các kernel GPU hiệu quả. Park et al. (2022b) giải quyết độ khó của việc lượng tử hóa các ngoại lệ kích hoạt thông qua quadapters, các tham số có thể học có mục tiêu là chia tỷ lệ kích hoạt theo kênh, trong khi giữ các tham số mô hình khác không thay đổi. Dettmers & Zettlemoyer (2022) điều tra mối quan hệ mở rộng giữa kích thước mô hình, bit lượng tử hóa, và các khái niệm độ chính xác khác nhau cho LLM lớn, quan sát các tương quan cao giữa điểm perplexity và độ chính xác zero-shot tổng hợp trên các tác vụ. Như chúng tôi đã chỉ ra trong Mục 3.5, thuật toán SparseGPT có thể được áp dụng kết hợp với GPTQ, thuật toán tiên tiến hiện tại cho lượng tử hóa trọng số, và nên tương thích với các phương pháp lượng tử hóa kích hoạt (Xiao et al., 2022; Park et al., 2022b).

6. Thảo luận

Chúng tôi đã cung cấp một phương pháp cắt tỉa hậu huấn luyện mới được gọi là SparseGPT, được thiết kế riêng cho các mô hình ngôn ngữ lớn từ họ GPT. Kết quả của chúng tôi cho thấy lần đầu tiên rằng các mô hình họ Transformer sinh tạo được tiền huấn luyện quy mô lớn có thể được nén đến độ thưa cao thông qua cắt tỉa trọng số trong một lần, mà không cần bất kỳ huấn luyện lại nào, với mất mát độ chính xác thấp, khi được đo cả về perplexity và hiệu suất zero-shot. Cụ thể, chúng tôi đã chỉ ra rằng các mô hình họ GPT mã nguồn mở lớn nhất (ví dụ OPT-175B và BLOOM-176B) có thể đạt 50-60% độ thưa, loại bỏ hơn 100B trọng số, với biến động độ chính xác thấp.

Công việc của chúng tôi cho thấy rằng mức độ tham số hóa cao của các mô hình GPT lớn cho phép cắt tỉa trực tiếp xác định các mô hình thưa chính xác trong "vùng lân cận gần" của mô hình dày đặc, mà không cần thông tin gradient. Đáng chú ý, đầu ra của các mô hình thưa như vậy tương quan cực kỳ chặt chẽ với mô hình dày đặc. Chúng tôi cũng cho thấy rằng các mô hình lớn hơn dễ làm thưa hơn: ở một mức độ thưa cố định, sự giảm độ chính xác tương đối cho các mô hình thưa lớn hơn thu hẹp khi chúng ta tăng kích thước mô hình, đến mức mà việc tạo ra 50% độ thưa dẫn đến hầu như không có sự giảm độ chính xác trên các mô hình lớn nhất, điều này nên được xem là rất khuyến khích cho công việc tương lai về nén các mô hình lớn như vậy.

7. Lời cảm ơn

Các tác giả biết ơn sự tài trợ từ Hội đồng Nghiên cứu Châu Âu (ERC) trong chương trình Horizon 2020 của Liên minh Châu Âu (thỏa thuận tài trợ số 805223 ScaleML), cũng như hỗ trợ thí nghiệm từ Eldar Kurtic, và từ bộ phận IT IST Austria, đặc biệt Stefano Elefante, Andrei Hornoiu, và Alois Schloegl.

Tài liệu tham khảo

Blumensath, T. and Davies, M. E. Lặp ngưỡng cho xấp xỉ thưa. Tạp chí Phân tích Fourier và Ứng dụng, 14(5-6):629–654, 2008.

Boratko, M., Padigela, H., Mikkilineni, D., Yuvraj, P., Das, R., McCallum, A., Chang, M., Fokoue-Nkoutche, A., Kaplanipathi, P., Mattei, N., et al. Một phân loại hệ thống về kiến thức, lý luận, và bối cảnh trong tập dữ liệu ARC. arXiv preprint arXiv:1806.00358, 2018.

Dettmers, T. and Zettlemoyer, L. Trường hợp cho độ chính xác 4-bit: luật mở rộng suy luận k-bit. arXiv preprint arXiv:2212.09720, 2022.

Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. LLM.int8(): phép nhân ma trận 8-bit cho transformer ở quy mô lớn. arXiv preprint arXiv:2208.07339, 2022.

EleutherAI. EleutherAI LM Evaluation Harness, 2022. URL https://github.com/EleutherAI/lm-evaluation-harness.

Elsen, E., Dukhan, M., Gale, T., and Simonyan, K. Convnet thưa nhanh. Trong Hội nghị về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), 2020.

Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Gian lận xổ số: Làm cho tất cả vé đều thắng. Trong Hội nghị Quốc tế về Học Máy (ICML), 2020.

Frantar, E. and Alistarh, D. SPDY: Cắt tỉa chính xác với đảm bảo tăng tốc. arXiv preprint arXiv:2201.13096, 2022.

Frantar, E., Kurtic, E., and Alistarh, D. M-FAC: Xấp xỉ hiệu quả không ma trận của thông tin bậc hai. Trong Hội nghị về Hệ thống Xử lý Thông tin Neural (NeurIPS), 2021.

Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. GPTQ: Nén chính xác sau huấn luyện cho transformer sinh tạo được tiền huấn luyện. arXiv preprint arXiv:2210.17323, 2022a.

Frantar, E., Singh, S. P., and Alistarh, D. Nén Não Tối ưu: Một khung cho lượng tử hóa và cắt tỉa chính xác sau huấn luyện. arXiv preprint arXiv:2208.11580, 2022b. Được chấp nhận tại NeurIPS 2022, sẽ xuất hiện.

Gale, T., Elsen, E., and Hooker, S. Trạng thái của độ thưa trong mạng neural sâu. Trong Hội nghị Quốc tế về Học Máy (ICML), 2019.

Hagiwara, M. Một phương pháp đơn giản và hiệu quả để loại bỏ các đơn vị ẩn và trọng số. Neurocomputing, 6(2):207–218, 1994. ISSN 0925-2312. Lan truyền ngược, Phần IV.

Han, S., Pool, J., Tran, J., and Dally, W. J. Học cả trọng số và kết nối cho mạng neural hiệu quả. Trong Hội nghị về Hệ thống Xử lý Thông tin Neural (NeurIPS), 2015.

Han, S., Mao, H., and Dally, W. J. Nén sâu: Nén mạng neural sâu với cắt tỉa, lượng tử hóa được huấn luyện và mã hóa Huffman. Trong Hội nghị Quốc tế về Biểu diễn Học (ICLR), 2016.

Hassibi, B., Stork, D. G., and Wolff, G. J. Phẫu thuật não tối ưu và cắt tỉa mạng tổng quát. Trong Hội nghị Quốc tế IEEE về Mạng Neural, 1993.

He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S. AMC: AutoML cho nén và tăng tốc mô hình trên thiết bị di động. Trong Hội nghị Châu Âu về Thị giác Máy tính (ECCV), 2018.

Hoefler, T., Alistarh, D., Ben-Nun, T., Dryden, N., and Peste, A. Độ thưa trong học sâu: Cắt tỉa và phát triển cho suy luận và huấn luyện hiệu quả trong mạng neural. arXiv preprint arXiv:2102.00554, 2021.

Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, S., and Soudry, D. Huấn luyện neural thưa được tăng tốc: Một phương pháp có thể chứng minh và hiệu quả để tìm mặt nạ có thể chuyển đổi N:M. Trong Hội nghị về Hệ thống Xử lý Thông tin Neural (NeurIPS), 2021a.

--- TRANG 8 ---
SparseGPT: Các Mô hình Ngôn ngữ Lớn Có thể được Cắt tỉa Chính xác trong Một lần

Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry, D. Lượng tử hóa chính xác sau huấn luyện với các tập hiệu chuẩn nhỏ. Trong Hội nghị Quốc tế về Học Máy (ICML), 2021b.

HuggingFace. Tính toán Perplexity của HuggingFace, 2022. URL https://huggingface.co/docs/transformers/perplexity.

Kingdon, J. Giả thuyết Mạng Neural, trang 81–106. Springer London, London, 1997. ISBN 978-1-4471-0949-5. doi: 10.1007/978-1-4471-0949-5_5.

Kurtic, E. and Alistarh, D. Gmp*: Cắt tỉa độ lớn toàn cục được điều chỉnh tốt có thể vượt trội hầu hết các phương pháp cắt tỉa bert. arXiv preprint arXiv:2210.06384, 2022.

Kurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz, M., Fineran, B., Goin, M., and Alistarh, D. Phẫu thuật viên BERT Tối ưu: Cắt tỉa bậc hai có thể mở rộng và chính xác cho các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2203.07259, 2022.

Kurtz, M., Kopinsky, J., Gelashvili, R., Matveev, A., Carr, J., Goin, M., Leiserson, W., Moore, S., Nell, B., Shavit, N., and Alistarh, D. Tạo ra và khai thác độ thưa kích hoạt cho suy luận nhanh trên mạng neural sâu. Trong Hội nghị Quốc tế về Học Máy (ICML), 2020.

Kwon, W., Kim, S., Mahoney, M. W., Hassoun, J., Keutzer, K., and Gholami, A. Một khung cắt tỉa nhanh sau huấn luyện cho transformer. arXiv preprint arXiv:2204.09656, 2022.

LeCun, Y., Denker, J. S., and Solla, S. A. Tổn thương não tối ưu. Trong Hội nghị về Hệ thống Xử lý Thông tin Neural (NeurIPS), 1989.

Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu, F., Wang, W., and Gu, S. BRECQ: Đẩy giới hạn của lượng tử hóa sau huấn luyện bằng tái tạo khối. Trong Hội nghị Quốc tế về Biểu diễn Học (ICLR), 2021.

Liu, L., Zhang, S., Kuang, Z., Zhou, A., Xue, J.-H., Wang, X., Chen, Y., Yang, W., Liao, Q., and Zhang, W. Cắt tỉa fisher nhóm cho nén mạng thực tế. Trong Hội nghị Quốc tế về Học Máy (ICML), 2021.

Marcus, M., Kim, G., Marcinkiewicz, M. A., MacIntyre, R., Bies, A., Ferguson, M., Katz, K., and Schasberger, B. Penn treebank: Chú thích cấu trúc luận cứ vị từ. Trong Công nghệ Ngôn ngữ Con người: Kỷ yếu một Hội thảo được tổ chức tại Plainsboro, New Jersey, 8-11 tháng 3, 1994, 1994.

Merity, S., Xiong, C., Bradbury, J., and Socher, R. Các mô hình hỗn hợp sentinel con trỏ. arXiv preprint arXiv:1609.07843, 2016.

Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G., Yu, C., and Micikevicius, P. Tăng tốc mạng neural sâu thưa. arXiv preprint arXiv:2104.08378, 2021.

Mostafazadeh, N., Roth, M., Louis, A., Chambers, N., and Allen, J. Lsdsem 2017 nhiệm vụ chia sẻ: Bài kiểm tra hoàn thành câu chuyện. Trong Kỷ yếu Hội thảo thứ 2 về Liên kết Mô hình Ngữ nghĩa cấp Từ vựng, Câu và Diễn ngôn, trang 46–51, 2017.

Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Lên hay xuống? Làm tròn thích ứng cho lượng tử hóa sau huấn luyện. Trong Hội nghị Quốc tế về Học Máy (ICML), 2020.

NeuralMagic. DeepSparse, 2022. URL https://github.com/neuralmagic/deepsparse.

Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernández, R. Tập dữ liệu LAMBADA: Dự đoán từ yêu cầu bối cảnh diễn ngôn rộng. arXiv preprint arXiv:1606.06031, 2016.

Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee, D. nuQmm: Matmul lượng tử hóa cho suy luận hiệu quả của các mô hình ngôn ngữ sinh tạo quy mô lớn. arXiv preprint arXiv:2206.09557, 2022a.

Park, M., You, J., Nagel, M., and Chang, S. Quadapter: Adapter cho lượng tử hóa gpt-2. arXiv preprint arXiv:2211.16912, 2022b.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: Một thư viện học sâu hiệu suất cao, kiểu mệnh lệnh. Trong Hội nghị về Hệ thống Xử lý Thông tin Neural (NeurIPS), 2019.

Peste, A., Iofinova, E., Vladu, A., and Alistarh, D. AC/DC: Huấn luyện nén/giải nén xen kẽ của mạng neural sâu. Trong Hội nghị về Hệ thống Xử lý Thông tin Neural (NeurIPS), 2021.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. Khám phá giới hạn của học chuyển giao với một transformer văn bản-sang-văn bản thống nhất. Tạp chí Nghiên cứu Học Máy, 21(140):1–67, 2020.

Sanh, V., Wolf, T., and Rush, A. M. Cắt tỉa chuyển động: Độ thưa thích ứng bằng tinh chỉnh. arXiv preprint arXiv:2005.07683, 2020.

--- TRANG 9 ---
SparseGPT: Các Mô hình Ngôn ngữ Lớn Có thể được Cắt tỉa Chính xác trong Một lần

Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., et al. Bloom: Một mô hình ngôn ngữ đa ngữ mở với 176b tham số. arXiv preprint arXiv:2211.05100, 2022.

Singh, S. P. and Alistarh, D. WoodFisher: Xấp xỉ bậc hai hiệu quả cho nén mạng neural. Trong Hội nghị về Hệ thống Xử lý Thông tin Neural (NeurIPS), 2020.

Tata, S. and Patel, J. M. PiQA: Một đại số để truy vấn các tập dữ liệu protein. Trong Hội nghị Quốc tế về Quản lý Cơ sở dữ liệu Khoa học và Thống kê, 2003.

Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Transformer của huggingface: Xử lý ngôn ngữ tự nhiên hiện đại. arXiv preprint arXiv:1910.03771, 2019.

Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S. Smoothquant: Lượng tử hóa chính xác và hiệu quả sau huấn luyện cho các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2211.10438, 2022.

Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and He, Y. ZeroQuant: Lượng tử hóa hiệu quả và giá cả phải chăng sau huấn luyện cho transformer quy mô lớn. arXiv preprint arXiv:2206.01861, 2022.

Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. OPT: Các mô hình ngôn ngữ transformer được tiền huấn luyện mở. arXiv preprint arXiv:2205.01068, 2022.

Zhou, A., Ma, Y., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H. Học mạng neural thưa có cấu trúc chi tiết N:M từ đầu. Trong Hội nghị Quốc tế về Biểu diễn Học (ICLR), 2021.

Zhu, M. and Gupta, S. Cắt tỉa, hay không cắt tỉa: khám phá hiệu quả của cắt tỉa cho nén mô hình. arXiv preprint arXiv:1710.01878, 2017.

--- TRANG 10 ---
SparseGPT: Các Mô hình Ngôn ngữ Lớn Có thể được Cắt tỉa Chính xác trong Một lần

A. Nghiên cứu Loại bỏ

Trong phần này, chúng tôi tiến hành các nghiên cứu loại bỏ đối với một số tham số chính của SparseGPT. Để có thời gian lặp nhanh và làm cho có thể khám phá các thiết lập tính toán và bộ nhớ tốn kém hơn, chúng tôi tập trung vào mô hình OPT-2.7B ở đây. Trừ khi được nêu khác, chúng tôi luôn cắt tỉa đồng đều đến độ thưa 50% mặc định. Để ngắn gọn, chúng tôi chỉ hiển thị kết quả raw-WikiText2 ở đây, nhưng muốn lưu ý rằng hành vi trên các tập dữ liệu khác rất tương tự.

2 4 8 16 32 64 128 256
#mẫu hiệu chuẩn1314151617181920Perplexity trên raw-WikiText2

Hình 8. Loại bỏ mẫu hiệu chuẩn.

0.001 0.010 0.100 1.000
Làm tắt Hessian1314151617181920Perplexity trên raw-WikiText2

Hình 9. Loại bỏ làm tắt Hessian.

1 4 16 64 256 1024 4096
Kích thước khối lựa chọn mặt nạ1314151617181920Perplexity trên raw-WikiText2

Hình 10. Loại bỏ kích thước khối lựa chọn mặt nạ.

Lượng Dữ liệu Hiệu chuẩn. Đầu tiên, chúng tôi điều tra cách độ chính xác của SparseGPT mở rộng với số lượng mẫu dữ liệu hiệu chuẩn, mà chúng tôi thay đổi theo lũy thừa của hai. Kết quả được hiển thị trong Hình 8. Thú vị là, SparseGPT đã có thể đạt được kết quả khá tốt ngay cả với chỉ vài đoạn 2048-token; tuy nhiên, việc sử dụng nhiều mẫu hơn mang lại những cải thiện đáng kể thêm, nhưng chỉ đến một điểm nhất định vì đường cong làm phẳng khá nhanh. Do đó, vì việc sử dụng nhiều mẫu hơn cũng tăng chi phí tính toán và bộ nhớ, chúng tôi gắn bó với 128 mẫu trong tất cả các thí nghiệm của mình.

Làm tắt Hessian. Tiếp theo, chúng tôi nghiên cứu tác động của việc làm tắt Hessian bằng cách thử nghiệm các giá trị thay đổi theo lũy thừa của mười (xem Hình 9) được nhân với giá trị đường chéo trung bình, theo (Frantar et al., 2022a). Tổng thể, tham số này dường như không quá nhạy cảm, 0:001 đến 0:1 xuất hiện hoạt động khá tương tự; chỉ khi việc làm tắt rất cao, chất lượng giải pháp giảm đáng kể. Chúng tôi chọn 1% (tức là 0:01) làm tắt để an toàn đối với các tính toán nghịch đảo cũng cho các mô hình rất lớn nhất.

Kích thước Khối Lựa chọn Mặt nạ. Một thành phần quan trọng khác của phương pháp chúng tôi là lựa chọn mặt nạ thích ứng như được hiển thị trong Hình 10 nơi chúng tôi thay đổi tham số kích thước khối tương ứng với lũy thừa của hai. Cả theo cột (kích thước khối 1) cũng như gần kích thước khối đầy đủ (4096 và 8192) đều hoạt động kém hơn đáng kể so với việc chặn hợp lý. Thú vị là, một phạm vi rộng các kích thước khối dường như hoạt động tốt, với những cái xung quanh vài trăm chính xác hơn một chút. Do đó chúng tôi chọn kích thước khối 128 nằm trong phạm vi đó trong khi cũng đơn giản hóa một chút việc triển khai thuật toán vì nó khớp với kích thước lô cập nhật trọng số lười mặc định.

Độ nhạy với Hạt giống Ngẫu nhiên. Cuối cùng, chúng tôi xác định mức độ nhạy cảm của kết quả thuật toán chúng tôi đối với tính ngẫu nhiên; cụ thể, liên quan đến việc lấy mẫu ngẫu nhiên của dữ liệu hiệu chuẩn. Chúng tôi lặp lại một lần chạy cắt tỉa 50% tiêu chuẩn 5 lần với các hạt giống ngẫu nhiên khác nhau cho việc lấy mẫu dữ liệu và nhận được 13:52±0:075 (trung bình/độ lệch chuẩn) cho thấy rằng SparseGPT khá mạnh mẽ đối với dữ liệu hiệu chuẩn chính xác được sử dụng, điều này phù hợp với các quan sát trong các công việc hậu huấn luyện khác (Nagel et al., 2020; Hubara et al., 2021b; Frantar et al., 2022b).

A.1. Chất lượng Xấp xỉ

Trong phần này chúng tôi điều tra mức độ mất mát bởi xấp xỉ cập nhật một phần được sử dụng bởi SparseGPT, so với tái tạo chính xác (tốn kém hơn nhiều). Chúng tôi một lần nữa xem xét mô hình OPT-2.7B ở độ thưa 50% và vẽ lỗi bình phương theo lớp của SparseGPT so với lỗi của tái tạo chính xác (với cùng mặt nạ và Hessian) cho nửa đầu của mô hình trong Hình 11. Ngoài một số ngoại lệ dưới dạng các lớp chiếu ra attention sớm, các lỗi tái tạo cuối cùng của SparseGPT dường như trung bình chỉ khoảng 20% tệ hơn tái tạo chính xác; trên các lớp fully-connected-2 sau, lỗi xấp xỉ thậm chí đến gần chỉ 10%, có lẽ vì các lớp này có một số lượng rất lớn tổng đầu vào và do đó mất mát bằng cách chỉ xem xét tương quan trong tập con ít nghiêm trọng hơn trên các lớp nhỏ hơn. Tổng thể, các kết quả này cho thấy rằng, bất chấp tăng tốc đáng kể của nó, SparseGPT cũng vẫn khá chính xác.

B. Chi tiết Đánh giá

Perplexity. Như được đề cập trong văn bản chính, tính toán perplexity của chúng tôi được thực hiện theo cách tiêu chuẩn, theo đúng mô tả của (HuggingFace, 2022). Cụ thể, điều đó có nghĩa là chúng tôi nối tất cả các mẫu trong tập thử nghiệm/xác thực,

--- TRANG 11 ---
SparseGPT: Các Mô hình Ngôn ngữ Lớn Có thể được Cắt tỉa Chính xác trong Một lần

0 20 40 60 80
Chỉ số lớp1.01.21.41.61.82.0SparseOPT so với tái tạo chính xác
k
v
q
out
fc1
fc2

Hình 11. Lỗi của tái tạo SparseGPT so với tái tạo chính xác cho nửa đầu của OPT-2.7B ở độ thưa 50%.

mã hóa kết quả với tokenizer phù hợp của mô hình và sau đó chia nó thành các đoạn không chồng lấp của 2048 token (lịch sử tối đa của các mô hình chúng tôi nghiên cứu). Những cái đó được chạy qua mô hình để tính toán hàm mất mát mô hình ngôn ngữ trung bình tương ứng. Số được lũy thừa là perplexity mà chúng tôi báo cáo.

Tập dữ liệu. Về mặt tập dữ liệu, chúng tôi sử dụng phiên bản thô của tập thử nghiệm WikiText2 và nối các mẫu, như được khuyến nghị bởi mô tả HuggingFace được tham chiếu ở trên, với " nnnn" để tạo ra markdown được định dạng đúng. Đối với PTB, chúng tôi sử dụng tập thử nghiệm của phiên bản "ptb textonly" của HuggingFace và nối các mẫu trực tiếp, không có bộ tách, vì PTB không được cho là chứa bất kỳ dấu câu nào. Tập con C4 của chúng tôi bao gồm 256 lần 2048 token được mã hóa bắt đầu (tập dữ liệu đi theo thứ tự ngẫu nhiên) trong shard đầu tiên của tập xác thực được nối trực tiếp; lựa chọn này được thực hiện để giữ chi phí đánh giá có thể quản lý được.

C. Kết quả Bổ sung

Mở rộng Độ khó Cắt tỉa trên PTB & C4. Bảng 3 và 4 trình bày các kết quả tương đương với Bảng 1 trong văn bản chính, nhưng trên PTB và tập con C4 của chúng tôi, tương ứng. Tổng thể, chúng theo các xu hướng rất tương tự như những xu hướng được thảo luận trong Mục 4.1. Sự khác biệt đáng chú ý chính là không có sự giảm perplexity nhẹ so với baseline dày đặc được quan sát ở độ thưa 50% cho các mô hình lớn nhất, do đó chúng tôi đã gắn nhãn điều này như một hiện tượng cụ thể của tập dữ liệu.

Bảng 3. Kết quả perplexity OPT trên PTB.

OPT - 50% 125M 350M 1.3B
Dày đặc 38.99 31.07 20.29
Magnitude 276. 126. 3.1e3
AdaPrune 92.14 64.64 41.60
SparseGPT 55.06 43.80 25.80

OPT Độ thưa 2.7B 6.7B 13B 30B 66B 175B
Dày đặc 0% 17.97 15.77 14.52 14.04 13.36 12.01
Magnitude 50% 262. 613. 1.8e4 221. 4.0e3 2.3e3
SparseGPT 50% 20.45 17.44 15.97 14.98 14.15 12.37
SparseGPT 4:8 23.02 18.84 17.23 15.68 14.68 12.78
SparseGPT 2:4 26.88 21.57 18.71 16.62 15.41 13.24

Bảng 4. Kết quả perplexity OPT trên một tập con C4.

OPT - 50% 125M 350M 1.3B
Dày đặc 26.56 22.59 16.07
Magnitude 141. 77.04 403.
AdaPrune 48.84 39.15 28.56
SparseGPT 33.42 29.18 19.36

OPT Độ thưa 2.7B 6.7B 13B 30B 66B 175B
Dày đặc 0% 14.32 12.71 12.06 11.45 10.99 10.13
Magnitude 50% 63.43 334. 1.1e4 98.49 2.9e3 1.7e3
SparseGPT 50% 15.78 13.73 12.97 11.97 11.41 10.36
SparseGPT 4:8 17.21 14.77 13.76 12.48 11.77 10.61
SparseGPT 2:4 19.36 16.40 14.85 13.17 12.25 10.92

50% Thưa + 3-bit. Bài báo chính chỉ trình bày kết quả gần không mất mát cho 50% + 4-bit làm thưa và lượng tử hóa kết hợp, tương ứng với lượng tử hóa 3-bit về mặt lưu trữ. Đối với 50% + 3-bit (tương ứng với 2.5-bit), OPT-175B đạt 8.60 PPL trên raw-WikiText2, cũng chính xác hơn kết quả 2.5-bit 8.94 tiên tiến của GPTQ (Frantar et al., 2022a). SparseGPT ghi được cùng 8.93 cho 4:8 + 3-bit. Dựa trên các điều tra ban đầu này, chúng tôi tin rằng việc kết hợp độ thưa + lượng tử hóa là một hướng đầy hứa hẹn hướng tới nén cực đoan hơn nữa của các mô hình ngôn ngữ rất lớn.

--- TRANG 12 ---
SparseGPT: Các Mô hình Ngôn ngữ Lớn Có thể được Cắt tỉa Chính xác trong Một lần

D. Kết quả 2:4 Một phần

Bảng 5 và 6 hiển thị hiệu suất của một chuỗi các mô hình thưa 2:4 một phần trên ba tập dữ liệu mô hình ngôn ngữ khác nhau. Phần đầu của các lớp được làm thưa hoàn toàn trong khi phần còn lại được giữ dày đặc. Theo cách này, tăng tốc và độ chính xác có thể được đánh đổi cũng từ các lựa chọn nén nhị phân, chẳng hạn như cắt tỉa n:m.

Bảng 5. Cắt tỉa các phần khác nhau (như các đoạn liên tiếp từ đầu) của các lớp OPT-175B thành mẫu 2:4.

OPT-175B – 2:4 dày đặc 1/2 2/3 3/4 4/5 đầy đủ
raw-WikiText2 8.34 8.22 8.38 8.49 8.52 8.74
PTB 12.01 12.15 12.80 13.02 13.12 13.25
C4-subset 10.13 10.22 10.41 10.52 10.59 10.92

Bảng 6. Cắt tỉa các phần khác nhau (như các đoạn liên tiếp từ đầu) của các lớp BLOOM-176B thành mẫu 2:4.

BLOOM-176B – 2:4 dày đặc 1/2 2/3 3/4 4/5 đầy đủ
raw-WikiText2 8.11 8.20 8.50 8.67 8.74 9.20
PTB 14.58 14.78 15.44 15.84 15.96 16.42
C4-subset 11.71 11.81 12.06 12.23 12.32 12.67

E. Tăng tốc Độ thưa

Cuối cùng, chúng tôi thực hiện một nghiên cứu sơ bộ về mức độ các mô hình ngôn ngữ thưa có thể được tăng tốc trong thực tế với các công cụ có sẵn, cho cả suy luận CPU và GPU. Chúng tôi nghĩ rằng các kết quả này có thể được cải thiện đáng kể với tối ưu hóa cụ thể cho mô hình hơn, mà chúng tôi nghĩ là một chủ đề quan trọng cho công việc tương lai.

Tăng tốc CPU. Đầu tiên, chúng tôi điều tra tăng tốc của độ thưa không có cấu trúc cho suy luận CPU. Cho việc đó chúng tôi sử dụng engine DeepSparse tiên tiến (NeuralMagic, 2022) và chạy suy luận đầu cuối đến cuối trên OPT-2.7B (hỗ trợ cho các biến thể lớn hơn dường như vẫn đang được phát triển) cho một lô đơn của 400 token, trên Intel(R) Core(TM) i9-7980XE CPU @ 2.60GHz sử dụng 18 lõi. Bảng 7 hiển thị tăng tốc đầu cuối đến cuối của việc chạy các mô hình thưa so với mô hình dày đặc, được thực thi trong cùng engine/môi trường. (Để tham khảo, DeepSparse dày đặc nhanh hơn 1:5× so với ONNXRuntime tiêu chuẩn.) Các tăng tốc đạt được gần với tối ưu lý thuyết, điều này cho thấy rằng tăng tốc độ thưa không có cấu trúc cho suy luận LLM trên CPU đã khá thực tế.

Độ thưa 40% 50% 60%
Tăng tốc 1:57 1:82 2:16

Bảng 7. Tăng tốc so với phiên bản dày đặc khi chạy các mô hình OPT-2.7 được làm thưa trong DeepSparse.

Tăng tốc GPU. Độ thưa 2:4 như được hỗ trợ bởi GPU NVIDIA thế hệ Ampere và mới hơn về lý thuyết cung cấp tăng tốc 2× của phép nhân ma trận. Bây giờ chúng tôi đánh giá mức độ lớn của những tăng tốc này trong thực tế cho các kích thước bài toán matmul xảy ra trong các mô hình cụ thể quan tâm của chúng tôi. Chúng tôi sử dụng thư viện CUTLASS chính thức của NVIDIA (chọn cấu hình kernel tối ưu được trả về bởi profiler tương ứng) và so sánh với các số cuBLAS dày đặc được tối ưu hóa cao (cũng được sử dụng bởi PyTorch). Chúng tôi giả định kích thước lô 2048 token và benchmark ba hình dạng ma trận xảy ra trong OPT-175B; kết quả được hiển thị trong Bảng 8. Chúng tôi đo được các tăng tốc đáng tôn trọng thông qua độ thưa 2:4 giữa 54-79%, cho các lớp riêng lẻ (tăng tốc đầu cuối đến cuối có thể sẽ thấp hơn một chút do một số overhead bổ sung từ ví dụ attention).

Trọng số Q/K/V/Out FC1 FC2
Dày đặc 2.84ms 10.26ms 10.23ms
2:4 Thưa 1.59ms 6.15ms 6.64ms
Tăng tốc 1:79 1:67 1:54

Bảng 8. Thời gian chạy và tăng tốc cho các hình dạng lớp khác nhau xảy ra trong OPT-175B sử dụng 2048 token.
