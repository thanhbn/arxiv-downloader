# 1806.06457.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/1806.06457.pdf
# File size: 1354442 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Fast Convex Pruning of Deep Neural Networks
Alireza AghasiAfshin AbdiyJustin Rombergy
Abstract
We develop a fast, tractable technique called Net-Trim for simplifying a trained neural network. The
method is a convex post-processing module, which prunes (sparsiﬁes) a trained network layer by layer, while
preserving the internal responses. We present a comprehensive analysis of Net-Trim from both the algorithmic
and sample complexity standpoints, centered on a fast, scalable convex optimization program. Our analysis
includes consistency results between the initial and retrained models before and after Net-Trim application
and guarantees on the number of training samples needed to discover a network that can be expressed using a
certain number of nonzero terms. Speciﬁcally, if there is a set of weights that uses at most sterms that can
re-create the layer outputs from the layer inputs, we can ﬁnd these weights from O(slogN=s)samples, where
Nis the input size. These theoretical results are similar to those for sparse regression using the Lasso, and our
analysis uses some of the same recently-developed tools (namely recent results on the concentration of measure
and convex analysis). Finally, we propose an algorithmic framework based on the alternating direction method
of multipliers (ADMM), which allows a fast and simple implementation of Net-Trim for network pruning and
compression.
Keywords: Pruning Neural Networks, Deep Neural Networks, Compressed Sensing, Bowling Scheme,
Rademacher Complexity
1 Introduction
Deep neural networks are becoming a prominent tool to learn data structures of arbitrary complexity. This success
is mainly thanks to their ﬂexible, yet compact nonlinear formulation, and the development of computational
and architectural techniques to improve their training (c.f. [ 27,11] for a comprehensive review). Increasing the
number of layers, and the number of neurons within each layer is generally the most standard way of adding more
ﬂexibility to a neural network. While adding such ﬂexibility is capable of improving the ﬁtting of the model to the
training data (i.e., reducing the model bias), it makes the models prone to over-parameterization and overﬁtting
(i.e., increasing the model variance), which in turn can degrade the predictive capability of the network.
To simplify or stabilize neural networks, various regularizing techniques and pruning strategies have been
considered. Inspired by the classic regularizers for linear models, such as Ridge [ 16] and Lasso [ 29], the training of
neural networks is also equipped with `2or`1penalties [ 26,10] to control their variance and complexity. Adding
randomness to the training process is also shown to have regularizing eﬀects, relevant to which we may refer to
Dropout [ 28] and DropConnect [ 33], which randomly remove active connections in the training phase and are
likely to produce pruned networks. Batch normalization [ 18], associated with stochastic gradient descent-type
ﬁtting techniques, can also be considered as a tool of similar nature, where in the training process the updates of
the hidden units are weighted by the standard deviation of the random examples included in the mini-batch.
In this paper, we advocate a diﬀerent approach. We train the network using standard techniques. We then
extract the internal outputs (the intermediate features) at each layer and ﬁnd a sparse set of weights that
reproduces these features across all the training data. The philosophy here is that the most important product of
training the network is the features that it extracts, not the weights that it settles on to produce those features.
(Corresponding Author) Robinson College of Business, Georgia State University, Atlanta, GA. Email: aaghasi@gsu.edu
ySchool of Electrical and Computer Engineering, Georgia Tech, Atlanta, GA. Emails: {abdi,jrom}@ece.gatech.edu .
1arXiv:1806.06457v2  [cs.LG]  26 Feb 2019

--- PAGE 2 ---
For large networks, there will be many sets of weights that produce exactly the same internal features; of those
weights, we choose the simplest.
Our method for ﬁnding sparse sets of weights, presented in detail in Section 3, is related to well-known
techniques for sparse regression, e.g. the Lasso [ 29] in statistics and compressed sensing [ 6] in signal processing.
The main diﬀerence is the non-linearity in the mapping of internal features from one layer to another. If this
non-linearity is piecewise linear and convex (as is the rectiﬁed linear unit, ReLU (x) =max(x;0), that we use in
all of our analysis below), then there is a natural way to recast the condition that the outputs and inputs of a
layer match as a set of linear inequality constraints. There is a similar way to recast an approximate matching
as inclusion in a convex set. Using the `1norm as a proxy for sparsity, the entire program becomes convex.
This opens the door for a thorough analysis of how well and under what conditions we can expect Net-Trim to
perform well, and allows us to leverage decades of research in convex optimization to ﬁnd a scalable algorithm
with predictable convergence behavior.
The theory in Section 4 presents an upper bound on the number of training samples needed to discover a
weight matrix that is sparse. Given a set of layer input vectors x1;:::;xPand output vectors y1;:::;yP, we
solve the program
minimize
WkWk1subject to ReLU (W>xp) =yp;(p= 1;:::;P ); (1)
wherekWk1=P
n;mjwn;mjis the sum of the absolute values of the entries in a matrix W2RNM. As the
`1norm is convex and the ReLU()function is piecewise linear, meaning that constraints in the program above
can be broken into a series of linear equality and inequality constraints, the program above is convex. We show
that if thexpare independent samples of a subgaussian random vector that is non-degenerate (meaning that the
correlation matrix is full-rank) and there exists a W?with maximally s-sparse columns that does indeed satisfy
yp=ReLU(W>
?xp)for allp, then the solution to (1)is exactlyW?when the number of training samples Pis
(almost) proportional to the sparsity s: we require
P&slog(N=s):
We also show that if the xpare subgaussian, then so are the yp. As a results, the theory can be applied
layer-by-layer, yielding a sampling result for networks of arbitrary depth. (When we apply the algorithm in
practice, the equality constraints in (1)are relaxed; this is discussed in detail in Section 3.1.) Along with these
theoretical guarantees, Net-Trim oﬀers state-of-the-art performance on realistic networks. In Section 6, we present
some numerical experiments that show that compression factors between 10x and 50x (removing 90% to 98% of
the connections) are possible with very little loss in test accuracy.
Contributions and relations to previous work This paper provides a full description of the Net-Trim
method from both a theoretical and algorithmic perspective. In Section 3, we present our convex formulation
for sparsifying the weights in the linear layers of a network; we describe how the procedure can be applied
layer-by-layer in a deep network either in parallel or serially (cascading the results), and present consistency
bounds for both approaches. Section 4 presents our main theoretical result, stated precisely in Theorem 4. This
result derives an upper bound on the number of data samples we need to reliably discover a layer that has at
mostsconnections in its linear layer — we show that if the data samples are random, then these weights can be
learned fromO(slogN=s)samples. Mathematically, this result is comparable to the sample complexity bounds
for the Lasso in performing sparse regression on a linear model (also known as the compressed sensing problem).
Our analysis is based on the bowling scheme [ 30,24]; the main technical challenges are adapting this technique to
the piecewise linear constraints in the program (1), and the fact that the input vectors fxpginto each layer are
non-centered in a way that cannot be accounted for easily.
2

--- PAGE 3 ---
There are several other examples of techniques for simplifying networks by re-training in the recent literature.
These techniques are typically presented as model compression tools (e.g., [ 15,8,14]) for removing the inherent
model redundancies. In what is perhaps the most closely related work to what we present here, [ 15] proposes
a pruning scheme that simply truncates small weights of an already trained network, and then re-adjusts the
remaining active weights using another round of training. In contrast, our optimization scheme ensures that the
layer inputs and outputs stay consistent as the network is pruned.
The Net-Trim framework was ﬁrst presented in [ 2]. This paper provides a far more rigorous and complete
analysis (sample complexity bound) of the Net-Trim algorithm for networks with multiple layers (the previous
work only considered a single layer of the network). In addition, we present a scalable (yet relatively simple)
implementation of Net-Trim using the alternation direction method of multipliers (ADMM). This is an iterative
method with each iteration requiring a small number of matrix-vector multiplies. The code, along with all the
examples presented in the paper, is available online1.
Notation We use lowercase and uppercase boldface for vectors and matrices, respectively. Speciﬁcally, the
notationIis reserved for the identity matrix. For a matrix A,A 1;:denotes the submatrix formed by restricting
the rows ofAto the index set  1. Similarly,A:; 2restricts the columns of Ato 2, andA 1; 2is formed by
extracting both rows and columns. Given a vector x(or matrixX),suppx(orsuppX) is the set of indices with
non-zero entries, and suppcx(or suppcX) is the complement set.
ForX= [xm;n]2RMN,thematrixtraceisdenotedby tr(X). Furthermore,weuse kXk1,PM
m=1PN
n=1jxm;nj
as a notation for the sum of absolute entries2, andkXkFas the Frobenius norm. The neural network activation
used throughout the paper is the rectiﬁed linear unit (ReLU), which is applied component-wise to vectors and
matrices,
(ReLU (X))m;n= max (xm;n;0):
We will sometimes use the notation X+as shorthand for ReLU (X). For an index set 
f1;;Mgf1;;Ng,
W
represents a matrix of identical size as W= [wm;n]with entries
(W
)m;n=wm;n (m;n)2
0 (m;n)=2
:
Finally, we use SNto denotes the unit sphere in RN+1; and the notation f&^f(orf.^f) when there exists
an absolute constant Csuch thatfC^f(orfC^f).
Outline. The remainder of the paper is structured as follows. In Section 2, we brieﬂy overview the neural
network architecture considered. Section 3 presents the pruning idea and the consistency results between the
initial and retrained networks. The statistical architecture of the network and the general sample complexity
results are presented in Section 4. To implement the Net-Trim underlying convex program, in Section 5 we present
an ADMM scheme applicable to the original Net-Trim formulation. Finally, Section 6 presents some experiments,
along with concluding remarks. All the technical proofs of the theorems and results presented in this paper are
moved to Section 7.
1The link to the code and related material: https://dnntoolbox.github.io/Net-Trim/
2The notationkXk1should not be confused with the matrix induced `1norm
3

--- PAGE 4 ---
2 Feedforward Network Model
In this section, we brieﬂy overview the topology of the feedforward network model considered. The training of
the network is performed via Psamplesxp,p= 1;;P, wherexp2RNis the network input. To compactly
represent the training samples, we form a matrix X2RNP, structured as X=[x1;;xP]. Considering L
layers in the network, the output of the network at the ﬁnal layer is denoted by X(L)2RNLP, where each
column inX(L)is a response to the corresponding training column in X.
In a ReLU network, the output of the `-th layer isX(`)2RN`P, generated by applying the aﬃne transfor-
mationW>
`() +b(`)to each column of the previous layer X(` 1), followed by a ReLU activation:
X(`)=ReLU
W>
`X(` 1)+b(`)1>
; ` = 1;;L: (2)
HereW`2RN` 1N`,X(0)=XandN0=N. By adding an additional row to W`andX(` 1), one can absorb
the intercept term and compactly rewrite (2) as
X(`)=ReLU
W>
`X(` 1)
; ` = 1;;L: (3)
Often the last layer of a neural network skips an activation by merely going through the aﬃne transformation. As
a matter of fact, the results presented in this paper also apply to such architecture (see analysis examples in [ 2]).
A neural network that follows the model in (3)can be fully identiﬁed by Xand ^W`,`= 1;;L. Throughout
the paper, such network will be denoted by Net(fW`gL
`=1;X).
3 The Net-Trim Pruning Algorithm
Net-Trim is a post processing scheme which prunes a neural network after the training phase. Similar to many
other regularization techniques, Net-Trim is capable of simplifying trained models at the expense of a controllable
increase in the bias.
After the training phase and learning W`, Net-Trim retrains the network so that for the same training data
the layer outcomes stay more or less close to the initial model, while the redesigned network is sparser, i.e.,
`= 1;;L:nnz
^W`
nnz(W`);while ^X(`)X(`):
Here, nnz (:)denotes the number of nonzero entries, and ^W`and ^X(`)are respectively the redesigned layer
matrices and the corresponding layer outcomes.
Aside from the post-processing nature and some diﬀerences in the convex formulations, Net-Trim shares
many similarities with the Lasso (least absolute shrinkage and selection operator [ 29]), as they both use an `1
proxy to promote model sparsity. In the remainder of this section we overview the Net-Trim formulation and the
corresponding pruning schemes.
3.1 Pruning a Single Layer
ConsiderXin2RNPandXout2RMPto be a layer input and output matrices after the training, which based
on the model in (2) (or (3)) are connected via
Xout=ReLU
W>Xin
:
4

--- PAGE 5 ---
To explore a sparser coeﬃcient matrix, we may consider the minimization
minimize
UkUk1subject toReLU
U>Xin
 Xout
F; (4)
which may potentially generate a sparser W-matrix relating XinandXout, at the expense of a (controllable)
discrepancy between the layer outcomes before and after the retraining.
Despite the convex objective, the constraint set in (4)is non-convex. Using the fact that the entries of Xout
are either zero or strictly positive quantities, [2] propose the following convex proxy to (4):
^W= arg min
UkUk1subject to8
<
:
U>Xin Xout


F
U>Xin

c0; (5)
where

 =suppXout=n
(m;p) :
Xout
m;p>0o
:
The main idea behind this convex surrogate is imposing similar activation patterns before and after the retraining
via the second inequality in (5), i.e.,

W>Xin+

c=
^W>Xin+

c=0;
and allowing the -discrepancy only on the set 
. For a more compact presentation of the convex constraint set,
for given matrices X;YandVwe use the notation
U2C(X;Y;V)()8
<
:
U>X Y


F
U>X

cV
c;for
 =suppY: (6)
Using this notation, the convex program in (5) may be cast as
^W= arg min
UkUk1subject toU2C 
Xin;Xout;0
: (7)
3.2 Pruning the Network
Having access to the tools to retrain any layer within the network, exclusively based on the input and the output,
we may consider parallelorcascadeframeworks to retrain the entire network.
The parallel Net-Trim is a straightforward application of the convex program (7)to each layer in the network.
Basically, each layer is processed independently based on the initial model input and output, without taking into
account the retraining result from the previous layers. Speciﬁcally, denoting X(` 1)andX(`)as the input and
output of the `-th layer of the initial trained network, we propose to retrain the coeﬃcient matrix W`via the
convex program
^W`= arg min
UkUk1subject toU2C`
X(` 1);X(`);0
; ` = 1;;L: (8)
An immediate question would be if each layer of a network is retrained via (8)and one replacesNet(fW`gL
`=1;X)
with the retrained network Net(f^W`gL
`=1;X), how do the discrepancies `propagate across the network, and how
far apart would be the ﬁnal responses of the two networks to X? The following result addresses this question.
5

--- PAGE 6 ---
Theorem 1 (Parallel Net-Trim) Consider a normalized network Net(fW`gL
`=1;X), such thatkW`k1= 1
for`= 1;;L. Solve (8)for each layer and form the retrained network Net(f^W`gL
`=1;X). Denoting by
^X(`)=ReLU (^W`>^X(` 1))the outcomes of the retrained network, where ^X(0)=X(0)=X, the layer outcomes
of the original and retrained networks obey
^X(`) X(`)
F`X
j=1j; ` = 1;;L: (9)
It is noteworthy that the normalization assumption kW`k1= 1in Theorem 1 is made with no loss in generality,
and is only a way of presenting the result in a standard form. This is simply because ReLU(jjx) =jjReLU(x),
and a scaling of any of the weight matrices W`would scaleX(L)(orX(`0)where`0`) by the same amount.
Speciﬁcally, the outcomes of the network before and after the process obey
^X(L) X(L)
FLX
j=1j;
which makes parallel Net-Trim a stable process, producing a controllable overall discrepancy.
A more adaptive way of retraining a network, which we would refer to as the cascade Net-Trim, incorporates
the outcome of the previously pruned layers to retrain a target layer. Basically, in a cascade Net-Trim, retraining
W`takes place by exploring a path between the input/output pairs (^X(` 1);X`)instead of (X(` 1);X`). Due
to some feasibility concerns, that will be detailed in the sequel, a cascade formulation does not simply happen by
replacingX(` 1)with ^X(` 1)in (8), and the formulation requires some modiﬁcations.
To derive the cascade formulation, consider starting the process by retraining the ﬁrst layer via
^W1= arg min
UkUk1subject toU2C1
X;X(1);0
: (10)
Setting ^X(1)=ReLU (^W1>X), to adaptively prune the second layer, one would ideally consider the program
minimize
UkUk1subject toU2C2
^X(1);X(2);0
: (11)
It is not hard to see that the simple generalization in (11)is not guaranteed to be feasible, that is, there exists a
matrixWsuch that for 
 =suppX(2):
8
<
:
W>^X(1) X(2)


F2
W>^X(1)

c0: (12)
If instead of ^X(1)the constraint set (12)was parameterized by X(1), a natural feasible point would have been
W=W2. Now that ^X(1)is a perturbed version of X(1), the constraint set needs to be properly slacked to
maintain the feasibility of W2. In this context, one may easily verify that W2is feasible for the slacked program
minimize
UkUk1subject toU2C2
^X(1);X(2);W>
2^X(1)
; (13)
as long as for some 1,
2=
W>
2^X(1) X(2)


F:
6

--- PAGE 7 ---
The-coeﬃcient is a free parameter, which we refer to as the inﬂation rate . When= 1, the matrixW2is only
tightly feasible for (13)and the feasible set can at the very least become a singleton. However, increasing the
inﬂation rate would expand the set of permissible matrices and makes (13)capable of producing sparser solutions.
The process applied to the second layer may be generalized to the subsequent layers and form a cascade
paradigm to prune the network layer by layer. The pseudocode in Algorithm 1 summarizes the Net-Trim cascade
scheme, where we set 1=for the ﬁrst layer, and consider the inﬂation rates `,`= 2;;L, for the subsequent
layers.
Algorithm 1: Cascade Net-Trim
^W1 arg minUkUk1subject toU2C
X;X(1);0
^X(1) ReLU
^W1>X
for`= 2;;Ldo

 suppX(`);` `
W>
`^X(` 1) X(`)


F
^W` arg minUkUk1subject toU2C`
^X(` 1);X(`);W>
`^X(` 1)
^X(`) ReLU
^W`>^X(` 1)
end for
Similar to the parallel scheme, we can show a bounded discrepancy between the outcomes of the initial network
Net(fW`gL
`=1;X)and the retrained network Net(f^W`gL
`=1;X), as follows.
Theorem 2 (Cascade Net-Trim) Consider a normalized network Net(fW`gL
`=1;X), such thatkW`k1= 1
for`= 1;;L. If the network is retrained according to Algorithm 1, the layer outcomes of the original and
retrained networks will obey
^X(`) X(`)
F`Y
j=2j: (14)
Speciﬁcally, when an identical inﬂation rate is used across all the layers, one would have k^X(L) X(L)kF(L 1),
which is a controllably small quantity, given that can be selected arbitrarily close to 1. For instance when
= 1:01andL= 10, the total network discrepancy would be still less than 1:1. As will be demonstrated in the
experiments section, for the same level of total network discrepancy, the cascade Net-Trim is capable of producing
sparser networks. However, such reduction is achieved at the expense of the loss in distributability, which makes
the parallel scheme computationally more attractive for big data problems.
4 Sample Complexity Bounds Using Subgaussian Random Flow
In the previous section we discussed and analyzed the convex retraining scheme and its consistency with the
reference model. In this section we analyze the sample complexity of the proposed retraining framework. Basically,
the goal of this section is addressing the following question: if there exists a sparse transformation matrix relating
the input and output of a layer, how many random samples are suﬃcient to recover it via the proposed retraining
scheme?
As will be detailed in the sequel, we will show that retraining each neuron within the network is possible with
fewer samples than the neuron degrees of freedom. More speciﬁcally, for a trained neuron with Ninput ports, if
7

--- PAGE 8 ---
generating an identical response is possible with sNnonzero weights, Net-Trim is able to recover such model
with onlyO(slog(N=s))random samples. This result is valid for the neurons of any layer within the network, as
long as some standard statistical properties can be established for the input samples.
Unlike the previous work [ 2], which establishes a similar result for only the neurons within the ﬁrst layer,
here, due to some favorable tail properties of subgaussian random vectors, we are able to generalize the result to
the entire network. Basically, we will show that when the network input samples are independently drawn from
a standard normal (or any other subgaussian) distribution, the input samples at all subsequent layers remain
independent and subgaussian (what we refer to as a subgaussian ﬂow). By carefully using some technical tools
from the structured signal recovery literature [ 30,24], we are able to present the main sample complexity result in
a general form.
To present the results, we ﬁrst start with a brief overview of subgaussian random variables. For a more
comprehensive overview, the reader is referred to [32] and §2.2 of [31].
Deﬁnition 1 (subgaussian random variable) A random variable 'is subgaussian3if there exists a constant
, such that for all t0,
Pfj'j>tgexp
1 t2
2
: (15)
Equivalently, 'is subgaussian if there exists a constant ^such that
Eexp'2
^2
e: (16)
The subgaussian norm of ', also referred to as the Orlicz norm, is denoted by k'k 2, and deﬁned as
k'k 2,sup
p1p 1
2(Ej'jp)1
p:
While calculating the exact Orlicz norm can be challenging, if either one of the properties (15)or(16)hold,k'k 2
is the smallest possible number ( or^) in either one of these inequalities, up to an absolute constant.
Deﬁnition 2 (subgaussian random vector) A random vector '2RNis subgaussian if for all 2RN(or
equivalently all 2SN 1), the one-dimensional marginals >'are subgaussian.
The notion of Orlicz norm also generalizes to the vector case as
k'k 2,sup
2SN 1k>'k 2= sup
2SN 1sup
p1p 1
2
E>'p1
p: (17)
We are now ready to state the ﬁrst result, which warrants a subgaussian random ﬂow across the network, as long
as the network input samples are independently drawn from a standard Gaussian (or subgaussian) distribution.
Theorem 3 Consider a network with ﬁxed parameters W`,b(`), where the input and output to each layer are
related via
x(`)=ReLU
W>
`x(` 1)+b(`)
; ` = 1;;L: (18)
If the network is fed with i.i.d sample vectors x(0)
1;;x(0)
PN (0;I), the response samples at each layer output
remain i.i.d subgaussian.
3In general, the right-hand expression in (15) can be replaced with cexp 
 t2=2
using two absolute constants cand
8

--- PAGE 9 ---
xPx2x1
x(1)
Px(1)
2x(1)
1
x(L)
Px(L)
2x(L)
1
!!!!!!!!!!!!
W>
1x+b(1)+

W>
2x(1)+b(2)+

W>
Lx(L 1)+b(L 1)+
or
W>
Lx(L 1)+b(L 1)
     |{z}
i:i:d SG     |{z}
i:i:d SG     |{z}
i:i:d SG
Figure 1: When x1;x2;;xP, the input samples to the proposed neural network, are independently drawn from
a subgaussian distribution, the input/output vectors of every subsequent layer remain i.i.d and subgaussian
As shown in the proof, the result of Theorem 3 still holds when the network input samples are independently
drawn from a subgaussian distribution instead of a standard normal, and/or when the last layer skips a ReLU
activation. Speciﬁcally, when the network is fed with x(0)
1;x(0)
2;;x(0)
P, independently drawn from a subgaussian
distribution, the resulting responses x(`)
1;x(`)
2;;x(`)
Pat any layer `remain independent and subgaussian. The
diagram in Figure 1 demonstrates such statistical structure among the layer inputs across the network.
Having independent subgaussian samples at any layer input port allows us to relate the number of samples to
the recovery of a reduced model. Exchanging the layer index with the general input/output notation, when Xin2
RNPandXout2RMPare respectively the input and output to a layer, related via Xout=ReLU(W>Xin),
obtaining the pruned layer matrix ^W2RNMis performed via
^W= arg min
WkWk1subject toW2C 
Xin;Xout;0
: (19)
When= 0, the program in (19)decouples into Mindividual convex programs each retraining a column in W.
Basically, instead of solving (19)for^W, ifxout>2RPis a row inXout, the corresponding column in ^Wcan be
calculated via
minimize
wkwk1subject tow2C0
Xin;xout>;0
; (20)
reducing (19)to retraining each of the Moutput neurons, individually. Our focus on the case of = 0(which also
makes the cascade and parallel schemes equivalent) is working in an underdetermined regime, where the required
samples are shown to be much less than the layer (neuron) degrees of freedom. In this case, the relationship
betweenXinandXoutcan be established via inﬁnitely many Wmatrices and one seeks a unique sparse solution
via (19).
Before stating the main technical result, we would like to introduce some notions used in the presentation.
When a neuron is initially trained via a vector w02RNand fed with i.i.d instances of x, the activation pattern of
the neuron is fully controlled by the sign of w>
0x. In this case, one expects to gain the main retraining information
from the cases when ReLU is in the linear mode (i.e., w>
0x>0). In this regard, corresponding to the random
inputx, we deﬁne the random virtual input as
=x1w>
0x>0=(
x w>
0x>0
0w>
0x0:
The virtual random vector plays a key role in our presentation. Our presentation also depends on the smallest
9

--- PAGE 10 ---
eigenvalue of the virtual covariance matrix, which follows the standard deﬁnition:
min(cov ()) = inf
2SN 1>cov ();where cov () =E 
>
 E()E()>:
Theorem 4 For the model (3), consider a trained neuron obeying xout=ReLU(Xin>w0), whereXin=
[x1;;xP]2RNPandx;x1;;xPare independent samples of a subgaussian distribution. Assume, an
s-sparse vector w2RNis capable of generating an identical response to Xinasxout. Fix1=2andt0,
then if
P&C;
slogN
s
+s+ 1 +t
; (21)
retraining the neuron via (20)recoverswwith probability exceeding 1 e ct. The absolute constant cis universal
and the constant C;depends on the statistics of the virtual input =x1w>
0x>0via
C;= (1 +)2 
k Ek2
 2
min(cov())!3+1

: (22)
We would like to highlight some technical details related to Theorem 4. To establish the result we use the
bowling scheme proposed by [ 30], which discusses the recovery of a structured (e.g., sparse) signal from independent
linear measurements. Below, we make a connection between our problem with nonlinear constraints to the problem
with linear constraints described there. While we used the compact model (3)for a more concise presentation, the
model in (2)is still covered by Theorem 4, treating the intercept as a constant feature appended to the neuron
input.
It is important to note that due to the application of the ReLU at each layer, the random samples entering
the next layer are non-centered and this requires a careful analysis of the problem. In fact, the majority of the
measurement systems in the structured recovery literature work with centered random measurements, as some of
the powerful analysis tools, such as the restricted isometry property [ 4,6], the certiﬁcate of duality [ 5,12], and the
Mendelson’s small ball method – which stands as the backbone for the bowling scheme [ 19,24,25] rely critically
on the random vectors being centered. In the presentation of Theorem 4, the constant is related to the statistics
of the centered virtual input, regardless of the mean shift that the previous activation units have caused to the
input4.
Finally, Theorem 4 can be used as a general and powerful tool to estimate the retraining sample complexity
for any layer within the network. To establish the O(slog(N=s))rate for a given layer, we only need to show
that for the corresponding input xand initially trained weights w0, the virtual input =x1w>
0x>0satisﬁes the
following two conditions:
min(cov ())&1;andk Ek 2.1: (23)
As an insightful example, we go through the exercise of establishing the bounds in (23)for a layer fed with i.i.d
Gaussian samples; this is not an unreasonable scenario for the ﬁrst layer of a neural network. As will be detailed
in Section 4.1 below, using standard tools to verify the conditions in (23), conveniently proves the O(slog(N=s))
rate for such layer.
For a network fed with i.i.d Gaussian samples, going through a similar exercise for the subsequent layers (say
layer`>1, withindependentcopiesoftherandominput x(`)), requirestracingthestatisticsof (`)=x(`)1w>
0x(`)>0
down to the Gaussian input x(0). In such case, warranting the conditions in (23)would require stating realistic
conditions on the initially trained Wjforj= 1;;`. Such generalization could be application speciﬁc and
beyond the current load of the paper, which is left as a potential future work.
4This is important because the Orlicz norm of a noncentered random vector can easily become dimension-dependent. For instance,
if the components of x2RNare i.i.d standard Gaussians, one can easily verify thatx+
 2=O(p
N), whilex+ Ex+
 2=O(1).
10

--- PAGE 11 ---
4.1 Feeding a Neuron with i.i.d Gaussian Samples
In this section we go through the exercise of establishing the conditions in (23)for a neuron fed with independent
copies ofxN(0;I),x2RN. Below, we go through each bound in (23), separately. In all the calculations,
w06=0is a ﬁxed vector that corresponds to the initially trained model. In [ 2], the authors go through a chain of
techniques to prove an O(slogN)sample complexity by carefully constructing a dual certiﬁcate for the convex
program. Here we will see that thanks to Theorem 4, such process is markedly reduced to establishing the
conditions in (23), which is conveniently fulﬁlled using standard tools.
4.1.1 Step 1: Bounding the Covariance Matrix
To evaluate the virtual input covariance matrix we have
min
cov
x1w>
0x>0
=min
Exx>1w>
0x>0 
Ex1w>
0x>0
Ex1w>
0x>0>
min
Exx>1w>
0x>0
+min
 
Ex1w>
0x>0
Ex1w>
0x>0>
=min
Exx>1w>
0x>0
 Ex1w>
0x>02
; (24)
where the second line follows from Weyl’s inequality. To conveniently calculate the required moments, we can
make use of the following lemma, which reduces the calculations to the bivariate case.
Lemma 1 Considerx= (x1;;xN)>N(0;I)and letg(:) :R!Rbe a real-valued function. Then, for any
ﬁxed vectors;2SN 1:
Exg 
>x
1>x>0=Ex1;x2g 
>
x1+q
1 (>)2x2
1x1>0: (25)
With no loss of generality we can assume w02SN 1, and apply (25)to the ﬁrst right-hand side term in (24)to
get
min
Exx>1w>
0x>0
= inf
2SN 1E 
>x21w>
0x>0
= inf
2SN 1Ex1;x2 
>w0
x1+q
1 (>w0)2x22
1x1>0
= inf
2SN 11
2 
>w02+1
2
1  
>w02
=1
2
11

--- PAGE 12 ---
For the second term in (24) we have
Ex1w>
0x>0= sup
2SN 1E 
>x
1w>
0x>0
= sup
2SN 1Ex1;x2 
>w0
x1+q
1 (>w0)2x2
1x1>0
= sup
2SN 11p
2 
>w0
=1p
2;
as a result of which one has min
cov
x1w>
0x>0
1=2 1=(2).
4.1.2 Step 2: Bounding the Orlicz Norm
To bound the Orlicz norm of the centered virtual input by a constant, we only need to introduce a constant 
such that for all 2SN 1the marginals >(x1w>
0x>0 Ex1w>
0x>0)obey (15). To this end, one has
82SN 1:>
x1w>
0x>0 Ex1w>
0x>0>x1w>
0x>0+Ex1w>
0x>0
>x+1p
2:
As a result, for xN(0;I)and any ﬁxed 2SN 1:
8t0 :Pn>
x1w>
0x>0 Ex1w>
0x>0>to
P>x+1p
2>t
=P>x>max
t 1p
2;0
exp 
 1
2max
t 1p
2;02!
;
where in the last inequality we used the fact that >x N (0;1)and for a standard normal variable z,
Pfjzjtgexp( t2=2)for allt0. Finally we can use the basic inequality stated in Lemma 2 of the proofs
section to get
8t0 :Pn>
x1w>
0x>0 Ex1w>
0x>0>to
exp
1 t2
2 +1
2
;
which implies thatx1w>
0x>0 Ex1w>
0x>0
 2.1.
5 Net-Trim Implementation
In this section we discuss details of an ADMM implementation for the Net-Trim convex program. The approach
that we suggest here is based on the global variable consensus (see §7.1 of [ 3]). This technique is useful in
addressing convex optimizations with additively separable objectives.
12

--- PAGE 13 ---
ForW2RNM,Xin2RNP, and 
f1;;Mgf 1;;Pgthe Net-Trim central program
minimize
WkWk1subject to8
<
:
W>Xin Xout


F
W>Xin

cV
c; (26)
can be cast as the equivalent form
minimize
W(1)2RMP
W(2);W(3)2RNMf1
W(1)
+f2
W(2)
subject to(
W(1)=W(3)>Xin
W(2)=W(3); (27)
where
f1(W) =IkW
 Xout

kF(W) +IW
cV
c(W);andf2(W) =kWk1:
HereIC()represents the indicator function of the set C,
IC(W) =0W2C
+1W=2C:
For the convex program (27), the ADMM update for each variable at the k-th iteration follows the standard forms
W(1)
k+1= arg min
Wf1(W) +
2W+U(1)
k W(3)
k>Xin2
F; (28)
W(2)
k+1= arg min
Wf2(W) +
2W+U(2)
k W(3)
k2
F; (29)
W(3)
k+1= 
XinXin>+I 1
Xin
W(1)
k+1+U(1)
k>
+W(2)
k+1+U(2)
k
; (30)
and the dual updates are performed via
U(1)
k+1=U(1)
k+W(1)
k+1 W(3)
k+1>Xin;U(2)
k+1=U(2)
k+W(2)
k+1 W(3)
k+1:
The update stated in (30)is derived by ﬁnding the minimizer of the augmented Lagrangian with respect to W(3),
which amounts to the minimization
minimize
W
2W(1)
k+1+U(1)
k W>Xin2
F+
2W(2)
k+1+U(2)
k W2
F:
While the updates for W(1)andW(2), as in(28)and(29), are stated in the general form, they can be further
simpliﬁed and presented in closed form. To this end, a ﬁrst observation is that (28)can be decoupled into
independent minimizations in terms of W
andW
c, i.e.,
W(1)
k+1= arg min
W
:kW
 Xout

kF
2W
+
U(1)
k W(3)
k>Xin

2
F
+ arg min
W
c:W
cV
c
2W
c+
U(1)
k W(3)
k>Xin

c2
F: (31)
13

--- PAGE 14 ---
The ﬁrst minimization on the right-hand side of (31)is basically the problem of ﬁnding the closest point of an
-radius Euclidean ball to a given point. For the non-trivial case that the given point is outside the ball, the
solution is the intersection of the ball surface with the line connecting the point to the center of the ball. More
speciﬁcally, for ﬁxed YandZ,
arg min
W
:kW
 Z
kF
2kW
 Y
k2
F=(
Y
 ifkY
 Z
kF
Z
+Y
 Z
kY
 Z
kFelse:
The second term in (31) is an instance of a projection onto an orthant and can be delivered in closed form as
arg min
W
c:W
cV
c
2kW
c Y
ck2
F=Y
c (Y
c V
c)+:
Finally, the solution to (29)is the standard soft thresholding operator (e.g., see §4.4.3 of [ 3]), which reduces the
update to

W(2)
k+1
n;m=S1=
W(3)
k U(2)
k
n;m
;whereSc(w) =8
<
:w c w>c
0jwjc
w+c w< c:
After combining the steps above, we propose Algorithm 2 as a computational scheme to address the Net-Trim
central program. The only computational load of the proposed scheme is the linear solve (30), for which the
coeﬃcient matrix XinXin>+Ionly needs to be calculated once. As observable, the processing time for each
ADMM step is relatively low, and only involves few matrix multiplications.
Algorithm 2: Implementation of the Net-Trim Central Program
input: Xin2RNP,Xout2RMP,
,V
,,
initialize :U(1);U(2)andW(3)% all initializations can be with 0
C XinXin>+I
whilenot converged do
Y W(3)>Xin U(1)
ifY
 Xout


Fthen
W(1)

 Y
else
W(1)

 Xout

+Y
 Xout

 1
F 
Y
 Xout


end if
W(1)

c Y
c (Y
c V
c)+
W(2) S1=(W(3) U(2)) %S1=applies to each element of the matrix
W(3) C 1(Xin(W(1)+U(1))>+W(2)+U(2))
U(1) U(1)+W(1) W(3)>Xin
U(2) U(2)+W(2) W(3)
end while
return W(3)
5.1 Net-Trim for Convolutional Layers
Since the convolution operator is linear, similar steps as the ones above can be taken to implement a version
of Net-Trim for convolutional layers and inputs in the form of tensors. The main diﬀerence is addressing the
least-squares update in (30), which can be performed by incorporating the adjoint operator. The details of
implementing Net-Trim for convolutional layers are presented in Section 8.1 of the Supplementary Materials.
14

--- PAGE 15 ---
6 Experiments and Remarks
While the main purpose of this paper is introducing a theoretical framework for a class of pruning techniques in
deep learning, we brieﬂy present some experiments which highlight the performance of Net-Trim in real-world
problems. Due to space limitation, most details of the simulations along with additional experiments are presented
in Section 8.2 of the Supplementary Materials. Also Net-Trim implementation is made publicly available online5.
Our ﬁrst set of experiments corresponds to a comparison between the cascade and parallel frameworks. For
this experiment we use a fully connected (FC) neural network of size 784300100010010(composed of
four layers), trained to classify the MNIST dataset. Throughout the section we refer to this network as the FC
model. While the theory supports retraining the network with new samples, in practice Net-Trim can be applied
to the dataset used to train the original network. In this experiment we also assess the possibility of applying
Net-Trim to only a portion of the training data (i.e., working with a subset of columns in X). Clearly, working
with smaller Xmatrices is computationally more desirable.
Figure 2 summarizes the parallel and cascade pruning results. A quick comparison between the range of
relative discrepancies in panels (a) and (c) (calculated as k^X(L) X(L)kF=kX(L)kF) reveals that for more or
less similar sparsity rates, cascade Net-Trim produces a smaller overall discrepancy compared to the parallel
scheme (note the axis ranges). This may be considered as the return for going through a non-distributable scheme.
However, a comparison of the test accuracies in panels (b) and (d), and especially for larger values of the sparsity
ratio, shows a less signiﬁcant diﬀerence between the test accuracies of the two schemes; speciﬁcally that using the
parallel scheme and its distributable nature is more desirable for big data.
Our next set of experiments corresponds to the application of Net-Trim to the LeNet convolutional network
[22] to highlight its performance against well-established methods of Dropout and `1regularization. In these
experiments the mean test accuracy and initial model sparsity are reported for the cases of Dropout, `1penalty,
and a combination of both. For each run, the tuning parameters ( : the coeﬃcient of `1-penalty,p: the Dropout
probability, or both) are varied in a range of values and the mean quantities are reported. It is noteworthy that
Net-Trim can always be followed by an optional ﬁne-tuning step (FT), which performs few training iterations on
the weights that Net-Trim has left nonzero. The plots in Figure 3 show how the application of Net-Trim can
further contribute to the sparsity and accuracy of the network. For instance panel (c) indicates that without a
loss in the accuracy, applying Net-Trim to a network, where almost 88% of the weights are pruned via Dropout
and`1regularization, can elevate the sparsity to almost 98%.
Another well-known scheme in model pruning is the algorithm by Han, Pool, Tran and Dally (HPTD: [ 15]).
The HPTD algorithm is a heuristic tool used for network compression, which truncates the small weights across a
trained network and performs another round of training on the active weights (same as the ﬁne-tuning scheme
explained above). Figure 4 presents a comprehensive comparison between the Net-Trim and HPTD on the FC,
LeNet, and a CIFAR-10 model. The initial CIFAR-10 model uses an augmented training set of size 6.4M samples,
to retrain which Net-Trim uses 50K samples. One of the main drawbacks with the HPTD is the truncation based
on the magnitude of the weights, which in many cases may discard connections to the important features and
variables in the network. That is mainly the reason that Net-Trim consistently outperforms this method. In
fact, Net-Trim can also present vital information about the data structure and important features that are not
immediately available using other techniques.
In Figure 5 we have depicted the retrained ^W1matrix of the FC model after applying Net-Trim and HPTD. In
panel (b) we can see many columns that are fully zero. After plotting the histogram of the MNIST samples (as in
panel (d)), one would immediately observe that the zero columns in ^W1correspond to the boundary pixels with
the least level of information. As HPTD only relies on the truncation based on the weight magnitudes, despite
5To access the algorithm implementation, visit: https://dnntoolbox.github.io/Net-Trim/
15

--- PAGE 16 ---
30 40 50 60 70 80 90
Layer Percentage of Zeros00.10.20.30.4Relative Total Discrepancy10K
20K
30K
55K
30 40 50 60 70 80 90
Layer Percentage of Zeros97.497.697.89898.298.498.6Test Accuracy (%)10K
20K
30K
55KLayer 1
0 10 20 30 40 50 60
Layer Percentage of Zeros00.10.20.30.4Relative Total Discrepancy10K
20K
30K
55K
0 10 20 30 40 50 60
Layer Percentage of Zeros97.497.697.89898.298.498.6Test Accuracy (%)10K
20K
30K
55K Layer 2
20 40 60 80 100
Layer Percentage of Zeros00.10.20.30.4Relative Total Discrepancy10K
20K
30K
55K
20 40 60 80 100
Layer Percentage of Zeros97.497.697.89898.298.498.6Test Accuracy (%)10K
20K
30K
55K Layer 3
(b)(a)
30 40 50 60 70 80 90 100
Layer Percentage of Zeros00.10.20.30.4Relative Total Discrepancy10K
20K
30K
55K
30 40 50 60 70 80 90 100
Layer Percentage of Zeros97.497.697.89898.298.498.6Test Accuracy (%)10K
20K
30K
55KLayer 4
30 40 50 60 70 80 90
Layer Percentage of Zeros00.050.10.15Relative Total Discrepancy10K
20K
30K
55K
30 40 50 60 70 80 90
Layer Percentage of Zeros9898.198.298.398.498.598.6Test Accuracy (%)10K
20K
30K
55K
0 10 20 30 40
Layer Percentage of Zeros00.050.10.15Relative Total Discrepancy10K
20K
30K
55K
0 10 20 30 40
Layer Percentage of Zeros9898.198.298.398.498.598.6Test Accuracy (%)10K
20K
30K
55K
0 20 40 60 80
Layer Percentage of Zeros00.050.10.15Relative Total Discrepancy10K
20K
30K
55K
0 20 40 60 80
Layer Percentage of Zeros9898.198.298.398.498.598.6Test Accuracy (%)10K
20K
30K
55K
(d)(c)
0 20 40 60 80 100
Layer Percentage of Zeros00.050.10.15Relative Total Discrepancy10K
20K
30K
55K
0 20 40 60 80 100
Layer Percentage of Zeros9898.198.298.398.498.598.6Test Accuracy (%)10K
20K
30K
55K
Figure 2: Retraining the FC network that is initially trained with the full MNIST training set and retrained with
10K, 20K, 30K and 55K samples (each column corresponds to a layer); (a) network relative total discrepancy
(RTD) vs the layer percentage of zeros (LPZ) after a parallel scheme; (b) test accuracy vs LPZ after a parallel
scheme; (c) RTD vs LPZ after a cascade scheme; (d) test accuracy vs LPZ after a cascade scheme;
the similar number of zeros in panels (b) and (c), the latter does not highlight such data structure. To obtain
a similar pattern as in panel (b), the authors in [ 15] suggest an iterative pruning path with a ﬁne-tuning after
truncating a portion of the network weights. However, this is not a computationally eﬃcient path as it requires
retraining the network multiple times, which can take a lot of time for large data sets and is not guaranteed to
identify the right structures.
16

--- PAGE 17 ---
40 50 60 70 80 90
Percentage of Zeros98.598.698.798.898.99999.199.299.399.499.5Test Accuracy (%)
Mean Initial Model Accuracy
Net-Trim Retrained (No Fine Tuning)
Net-Trim Retrained (With Fine Tuning)
Mean Initial Model Percentage of Zeros = 0.07%(a)
99.2 99.3 99.4 99.5 99.6 99.7 99.8 99.9
Percentage of Zeros98.598.698.798.898.99999.199.299.399.499.5Test Accuracy (%)Mean Initial Model Accuracy
Net-Trim Retrained (No Fine Tuning)
Net-Trim Retrained (With Fine Tuning)
Mean Initial Model Percentage of Zeros = 99.1% (b)
94.5 95 95.5 96 96.5 97 97.5 98 98.5 99
Percentage of Zeros98.598.698.798.898.99999.199.299.399.499.5Test Accuracy (%)
Mean Initial Model Accuracy
Net-Trim Retrained (No Fine Tuning)
Net-Trim Retrained (With Fine Tuning)
Mean Initial Model Percentage of Zeros = 88.05% (c)
Figure 3: Mean test accuracy vs mean model sparsity after the application of Net-Trim to the LeNet network
initially regularized via `1penalty, Dropout, or both (the regularization parameter and Dropout probability are
picked from a range of values and the mean accuracy and sparsity are reported); (a) a model trained with Dropout
only: 0:3p0:8; (b) a model trained with `1penalty only: 10 5510 3; (c) a model trained with
Dropout and `1:10 5210 4,0:5p0:75;
40 45 50 55 60 65 70 75 80 85 90
Pruning Percentage2030405060708090100Accuracy Percentage
Net-Trim without fine-tuning
HPTD without fine-tuning
Net-Trim with fine-tuning, 10 epochs
HPTD with fine-tuning, 10 epochs
Net-Trim with fine-tuning, 30 epochs
HPTD with fine-tuning, 30 epochsTest Accuracy (%)
Percentage of Zeros
40 50 60 70 80 90 100
Pruning Percentage2030405060708090100Accuracy Percentage
Net-Trim without fine-tuning
HPTD without fine-tuning
Net-Trim with fine-tuning, 10 epochs
HPTD with fine-tuning, 10 epochs
Net-Trim with fine-tuning, 30 epochs
HPTD with fine-tuning, 30 epochs
Test Accuracy (%)
Percentage of Zeros
40 45 50 55 60 65 70 75 80 85 90
Pruning Percentage102030405060708090Accuracy Percentage
Net-Trim without fine-tuning
HPTD without fine-tuning
Net-Trim with fine-tuning, 10 epochs
HPTD with fine-tuning, 10 epochs
Net-Trim with fine-tuning, 30 epochs
HPTD with fine-tuning, 30 epochs
Test Accuracy (%)
Percentage of Zeros
Figure 4: Comparison of Net-Trim and HPTD test accuracy vs percentage of zeros, without ﬁne-tuning, and with
ﬁne-tuning using 10 and 30 epochs (a) FC model, (b) LeNet model; (c) CIFAR-10 model;
6.1 Concluding Remarks
Net-Trim can be generalized to a large class of problems, where the architecture of each layer in a trained network
is restructured via a program of the type
minimize
U2RNMR(U)subject to 
U>Xin
Xout: (32)
17

--- PAGE 18 ---
5 10 15 20 255
10
15
20
25
100 200 300 400 500 600 70050
100
150
200
250
300
100 200 300 400 500 600 70050
100
150
200
250
300
5 10 15 20 25252015105(a)(b)
(c)(d)
(e)
Figure 5: Instant identiﬁcation of important features using Net-Trim; (a) samples from MNIST dataset; (b)
visualization of ^W1>in the FC retrained model using Net-Trim; (c) similar visualization of ^W1>in the FC
retrained model, using HPTD with a single ﬁne-tuning step; (d) histogram of the pixel values in the MNIST data
set; (e) the green mask corresponding to the zero columns in panel (b);
The objectiveR()aims to promote a desired structure, and the constraint enforces a consistency between the
initial and retrained models. While in this paper we merely emphasized on R(U) =kUk1, a variety of other
structures may be explored by adaptively selecting the objective. For instance, other than the Ridge and the
elastic net penalties as regularizing tools, choosing R(U) =kUk2;1=PN
n=1kUn;:kcan promote selection of a
subset of the rows in Xin, and act as a feature selection or node-dropping tool for each layer. Total variation, or
rank penalizing objectives may also directly apply to network compression problems.
While in this paper we speciﬁcally focused on =ReLU ()to exploit the convex formulation, in principal other
forms of activation may be explored. Even if a convex (re)formulation is suboptimal or not possible, powerful
tools from non-convex analysis would still allow us to have an understanding of when and how well programs
of type(32)work. Clearly, the techniques used for such type of analysis might be initialization-sensitive, and
diﬀerent than those used in this paper.
Net-Trim can speciﬁcally become a useful tool when the number of training samples is limited. While overﬁtting
is likely to happen in this situation, Net-Trim allows reducing the complexity of the models, yet maintaining
the consistency with the original model. From a diﬀerent perspective, Net-Trim may simplify the process of
determining the network size. For large networks that are trained with insuﬃcient samples, employing Net-Trim
can reduce the size of the models to an order matching the data.
7 Proofs
Before we start a detailed proof of the results, we would like to state two inequalities that will be frequently used
throughout this section:
8X2Rd1d2:X+
FkXkF; (33)
8X;Y2Rd1d2:X+ Y+
FkX YkF: (34)
18

--- PAGE 19 ---
The ﬁrst inequality is straightforward to verify. To verify (34) we note that for all x;y2R:
x+= (x y+y)+(x y)++y+jx yj+y+;
which is interchangeable in xandy, and yieldsjx+ y+jjx yj.
7.1 Proof of Theorem 1
The central convex program (7) requires that for 
 =suppXout:

^W>Xin Xout


F;and
^W>Xin

c0: (35)
As the ﬁrst step, notice that for ~Xout= (^W>Xin)+one has
~Xout Xout2
F=
~Xout Xout

2
F+
~Xout Xout

c2
F
=
^W>Xin+

 Xout

2
F
=
^W>Xin+

  
Xout+

2
F

^W>Xin Xout

2
F
2; (36)
where the ﬁrst inequality is thanks to (34). Now consider ^Xinbe any matrix such that k^Xin XinkFin,
then for ^Xout= (^W>^Xin)+one has
^Xout Xout
F^Xout ~Xout
F+~Xout Xout
F

^W>^Xin+
 
^W>Xin+
F+
^W>
^Xin Xin
F+
^W
F^Xin Xin
F+
in+: (37)
To present the last inequality we used the fact that
^W
F^W
1kWk1= 1:
We may now complete the proof via a simple induction. For the parallel scheme sketched in (8), inequality
(36)implies thatk^X(1) X(1)kF1. Also, (36)requires thatk^X(`) X(`)kF`, and assuming that
k^X(` 1) X(` 1)kFP` 1
j=1j, (37) yields
^X(`) X(`)
F`X
j=1j:
19

--- PAGE 20 ---
7.2 Proof of Theorem 2
For the cascade scheme outlined in Algorithm 1, replacing the `indexing with the in=outnotation, the layer
retraining takes place by addressing the convex program
^W= arg min
UkUk1subject toU2C
^Xin;Xout;W>^Xin
; (38)
where ^Xinis the retrained model input, Xout= (W>Xin)+is the initially trained model output, and for

 =suppXout,
=
W>^Xin Xout


F:
The central convex program (38), hence requires that

^W>^Xin Xout


F
W>^Xin Xout


F; (39)

^W>^Xin

c
W>^Xin

c: (40)
For the output of the initial and retrained models, one has
^Xout Xout2
F=
^W>^Xin+

 Xout

2
F+
^W>^Xin+

c2
F: (41)
For the ﬁrst term in (41) thanks to (34) and (39), one has

^W>^Xin+

 Xout

2
F=
^W>^Xin+

  
Xout+

2
F

^W>^Xin Xout

2
F
2
W>^Xin Xout

2
F: (42)
The second term in (41) can also be bounded by

^W>^Xin+

c2
F
W>^Xin+

c2
F=
W>^Xin+

c 
W>Xin+

c2
F

W>^Xin W>Xin

c2
F: (43)
UsingXout

= (W>Xin)
, and applying the results of (42) and (43) to (41) yields
^Xout Xout2
F2
W>^Xin W>Xin

2
F+
W>^Xin W>Xin

c2
F
2W>
^Xin Xin2
F
2^Xin Xin2
F: (44)
In a cascade Net-Trim, the ﬁrst layer goes through the standard retraining (10)with1=, for which Theorem 1
warrantsk^X(1) X(1)kF. Ontheotherhand, for `2,(44)warrantsk^X(`) X(`)kF`k^X(` 1) X(` 1)kF,
which together with the discrepancy of the ﬁrst layer yield the advertised result in (14).
20

--- PAGE 21 ---
7.3 Proof of Theorem 3
It suﬃces to show the following statements:
–Ifx2RNis a subgaussian vector, then for given W2RNMandb2RM, the random vector y=W>x+b
is subgaussian.
– Ifx2RNis a subgaussian vector, y=x+is also subgaussian.
We start by proving the ﬁrst statement. The subgaussianity of ximplies that there exists a constant such that
for any given 2SN 1:
8t0 :P>x>t	
cexp
 t2
2
: (45)
Now considering 2SN 1we have
>y(W)>x+>b
=kWkW
kWk>
x+>b
kWkW
kWk>
x+kbk;
which immediately implies that
82SN` 1:n
x:>
W>x+b>to
(
x:kWkW
kWk>
x+kbk>t)
:
By the measure comparison we get
Pn>
W>x+b>to
P(
kWkW
kWk>
x+kbk>t)
=P(W
kWk>
x>maxt kbk
kWk;0)
cexp 
 maxt kbk
kWk;02!
:
Using Lemma 2 below, for 022kWk2+kbk2andc0=ce, the following should hold:
8t0 :Pn>
W>x+b>to
c0exp
 t2
02
;
which completes the ﬁrst part of the proof.
Lemma 2 Fixa>0andb0. Then, for c2a2+b2,
8t0 : exp 
 maxt b
a;02!
exp
1 t2
c2
: (46)
21

--- PAGE 22 ---
Proof:
Fortb, the proposed conditions require 1 t2=c2>0, for which (46)automatically holds. In the case of t>b,
to establish (46) it suﬃces to show thatt b
a2
t2
c2 1;
or in a simpliﬁed form
1 a2
c2
t2 2bt+a2+b20: (47)
The discriminant of the quadratic expression in (47)is4a2 
(a2+b2)=c2 1
, which is never positive and the
expression always takes an identical sign to 1 a2=c2.
We next show the subgaussianity of x+for a subgaussian random vector x2RN. For this purpose we
introduce a constant 0such that Eexp((>x+)2=02)efor all2SN 1. To this end, we ﬁrst bound the
magnitude of the marginals as
(>x+)2kk2kx+k2kxk2: (48)
We now make use of the following lemma borrowed from [17] (see Theorem 2.1 and Remark 2.3 therein).
Lemma 3 LetA2RNNbe a matrix and =A>A. Supposex2RNis a random vector such that for some
2RNand0
82RN:Eexp 
>(x )
exp 
kk22=2
; (49)
then for0222kk,
Eexp 
kAxk2
02!
exp 
2tr()0 2+4tr 
2
0 4+kAk20 2
1 22kk0 2!
:
Condition (49)is technically a certiﬁcate of the subgaussianity of x[17,32]. SettingAin Lemma 3 to the identity
matrix and making use of (48) verify that for 2SN 1and0222,
Eexp(>x+)2
02
Eexp 
kxk2
02!
exp 
N20 2+N40 4+kk20 2
1 220 2!
: (50)
By selecting 0suﬃciently large, speciﬁcally 0&max(p
N;kk), one can upper bound the right-hand side
expression of (50) by e.
7.4 Proof of Theorem 4
With reference to (20), for Xin= [x1;;xP]2RNPand

 =
p:xout
p>0	
=
p:x>
pw0>0	
;
we need to derive the conditions that wis the unique solution to
min
wkwk1subject to(
Xin
:;
>w=xout

Xin
:;
c>w0: (51)
22

--- PAGE 23 ---
For generalX;X02RNPand
f1;;Pg, consider the operator
TX0

X,Xdiag(1
) +X0;
where 1
2RPis the indicator of the set 
. Simply,T0

Xreplaces columns of Xindexed by 
cwith zero vectors.
Exploiting the notion of minimum conic singular value, we ﬁrst state a unique optimality result for (51), which
generally holds regardless of the speciﬁc structure of Xin.
Lemma 4 Fix2RNand2R f0g. Considerw2RNto be a (sparse) feasible vector for (51), and deﬁne
the descent cone
D=[
>0y
z
2RN+1:kw+yk1kwk1
:
For=T 1>

Xin, if
inf 
>1
v:v2D\ SN	
>0; (52)
thenwis the unique solution to (51).
Proof:
Showing the following three statements would complete the proof:
(S.1) Ifwis feasible for (51), then the pair (w; 1>w)is feasible for the convex program:
minimize
(w;u)kwk1subject to 
>1w
u
=xout

0
: (53)
(S.2)For any pair (w;u)that is feasible for (53), if condition (52)holds, then (w;u)is the unique solution to
(53).
(S.3) If (w; 1>w)is the unique solution to (53), then wis the unique solution to (51).
Based on the deﬁnition =T 1>

Xin, verifying (S.1) is trivial. Claim (S.2) is a direct application of the
minimum conic singular value result (e.g., see Prop. 2.2 of [ 7], or Prop. 2.6 of [ 30]). To prove (S.3), suppose under
the proposed assumption, (51)has a diﬀerent solution ^w, wherek^wk1kwk1. Then (S.1) requires (^w; 1>^w)
to be feasible for (53). However the objective for this feasible point is less than kwk1, which is in contradiction
with (w; 1>w)being the unique solution to (53). 
Using Lemma 4 and the bowling scheme sketched in [ 30], we continue with lower-bounding the minimum conic
singular value away from zero, and relating the conditions to the number of samples, P.
To this end, we may look into the structure of the matrix in Lemma 4 as being populated with independent
copies ofx1w>
0x>0 as the columns, and exploit the independence required for the bowling scheme. To assure
centered columns, we choose =Ex1w>
0x>0, making columns of = ['1;;'P]independent copies of the
centered subgaussian6random vector
',x1w>
0x>0 Ex1w>
0x>0:
For reasons that become apparent later in the proof, our arbitrary choice of in Lemma 4 is narrowed to
0,p
2k'k 2:
In a random setting, to lower-bound the minimum conic singular value, we adapt the following result from [ 30],
Prop. 5.1 (or see Theorem 5.4 of [24] for the original statement).
6Sincej>x1w>
0x>0jj>xjand fort0,Pfj>x1w>
0x>0j> tgPfj>xj> tg,(15)conﬁrms that xbeing subgaussian
impliesx1w>
0x>0to be subgaussian.
23

--- PAGE 24 ---
Theorem 5 Fix a setERd. Letbe a random vector on Rd, and let1;;Pbe independent copies of .
For0, suppose the marginal tail relation below holds:
inf
v2EPn>vo
C>0:
Let"1;;"Pbe independent Rademacher random variables, independent from everything else, and deﬁne the
mean empirical width of the set E:
WP(E;),Esup
v2Ehh;vi;whereh=1p
PPX
p=1"pp: (54)
Then, for any >0andt>0, with probability at least 1 exp( t2=2):
inf
v2E PX
p=1
>
pv2!1
2

2Cp
P 2WP(E;) 
2t: (55)
For a more compact (and inline) notation, we use the following notation for the concatenation of a vector w
and a scalar u,
w_u,w
u
:
Also, for a given objective and point v02Rd, we denote the descent cone by
Dv(f(v);v0) =[
>0
y2Rd:f(v0+y)f(v0)	
:
To show that condition (52)holds for the prescribed s-sparse vector w, we will show that for suﬃciently large P,
the right-hand side expression in (55)can be bounded away from zero. To apply Theorem 5 to our problem in
(52), the random vector and the set Eto consider are
='_0;andE=Dw_u(kwk1;w_u)\SN;
where
Dw_u(kwk1;w_u) =[
>0
y_z2RN+1:kw+yk1kwk1	
=Dw(kwk1;w)R;
andu= 1
0
Ex1w>
0x>0>
w. Note that in the formulation above, Dw_u(:;:)RN+1, while Dw(:;:)RN.
The remainder of the proof focuses on bounding the contributing terms on the right-hand side expression of (55).
7.4.1 Bounding the Mean Empirical Width
In this section of the proof, we aim to upper-bound
WP Dw_u(kwk1;w_u)\SN;'_0
;
24

--- PAGE 25 ---
where following the formulation in (54) we have
h=1p
PPX
p=1"p'_
p0= 
0
0p
PPP
p=1"p!
|{z}
hu+ 
1p
PPP
p=1"p'p
0!
|{z}
h_
w0:
Using the compact notations
Kw;u=Dw_u(kwk1;w_u);andKw=Dw(kwk1;w);
ones has
WP 
Kw;u\SN;'_0
=E sup
v2Kw;u\SNhh;vi
E sup
v2Kw;u\SNhhu;vi+E sup
v2Kw;u\SNhh_
w0;vi: (56)
For the ﬁrst term in (56) note that
E sup
w_u2Kw;u\Snhhu;w_ui=E sup
w_u2Kw;u\SN 
0p
PPX
p=1"p!
u
=E0p
PPX
p=1"p
0p
P0
@E PX
p=1"p!21
A1
2
=0: (57)
To bound the second term in (56), we proceed by ﬁrst showing that for any ﬁxed hw2RN,
sup
w_u2Kw;u\SNhh_
w0;w_ui= sup
w2Kw\SN 1hhw;wi: (58)
For this purpose only the following two cases need to be considered:
–case 1:hhw;wi0;8w2Kw:
In this case the supremum value for both sides of (58)is zero, which may be attained by picking w=0andu= 1.
–case 2:9w2Kw, such thathhw;wi>0:
To show the equality in this case, we only need to show that if ^v=^w_^uis a point at which the (positive)
supremum is attained, i.e.,
sup
v2Kw;u\SNhh_
w0;vi=hh_
w0;^vi=hhw;^wi;
then we must have ^u= 0. If^u6= 0, then the condition ^v2SNrequires thatk^wk<1. In this case the alternative
feasible point ~v=k^wk 1^w_0produces a greater inner product:
hh_
w0;~vi=1
k^wkhhw;^wi>hhw;^wi;
25

--- PAGE 26 ---
which cannot be possible. Therefore ^u= 0, and for both sides of (58)the supremum value is hhw;^wi. Combining
cases 1 and 2 establishes the claim in (58).
Now, employing (58) and (57) in (56) certiﬁes that for
hw=1p
PPX
p=1"p'p;
and some absolute constant C, one has
WP 
Kw;u\SN;'_0
0+E sup
w2Kw\SN 1hhw;wi
Ck'k 2 s
slogN
s
+s+ 1!
: (59)
The last line in (59)is thanks to the following inequality (see §6.6 of [ 30]), which relates the mean empirical width
of a centered subgaussian random vector 'to the Gaussian width:
E sup
w2Kw\SN 1hhw;wi.k'k 2E sup
w2Kw\SN 1
gN(0;I)hg;wi.k'k 2s
slogN
s
+s:
7.4.2 Relating the Marginal Tail Bound and the Virtual Covariance
As the next step in lower-bounding the right-hand side expression in (55), noting that
inf
v2Kw;u\SNPv>'_0	
inf
v2SNPv>'_0	
; (60)
in this section we focus on lower bounding the right-hand side expression in (60)in terms ofk'k 2and the
minimum eigenvalue of the virtual covariance matrix. To this end, using the notation
~min,min(cov ()) =min 
E''>
;
one has
E'_0'_0>=E''>0
0>2
0
min
~min;2
0
I: (61)
On the other hand, from the subgaussian properties of 'we have
k'k 2sup
w2SN 12 1
2
Ew>'21
2inf
w2SN 12 1
2
Ew>'21
2=s
~min
2;
which simply implies that 2
0~minand combining with (61) yields
inf
v2SNEv>'_02min
~min;2
0
=~min: (62)
26

--- PAGE 27 ---
Considering a positive random variable and a ﬁxed 0, we can derive a variant of the Paley-Zygmund
inequality by writing 2=21f2<2g+21f22g, which using the Hölder’s inequality naturally yields
E22+ (Pfg)
1+
E2(1+)1
1+;  > 0:
Subsequently, selecting 2[0;p~min]warrants that
8v2SN:Pv>'_0	
0
B@~min 2

Ejv>'_0j2(1+)1
+11
CA1+1

:
We can also use the subgaussian properties of 'to bound the denominator as follows
8w_u2SN; 1 :
Ew_u>'_01
=
Ew>'+0u1


Ew>'1
+0juj
=kwk
Ew>
kwk'1

+0juj
pk'k 2kwk+p
2k'k 2juj
p
+ 2k'k 2;
where the ﬁrst inequality is a direct application of the Minkowski inequality, the second inequality uses the
subgaussian deﬁnition (17)and the last bound is thanks to the Cauchy-Schwarz inequality. As a result for
2[0;p~min]
inf
v2SNPv>'_0	
 ~min 2
2(2 +)k'k2
 2!1+1

: (63)
7.4.3 Combining the Bounds
We can now combine the bounds (59)and(63), and use Theorem 5 to state that with probability at least
1 exp( t2=2):
inf
v2Kw;u\SN PX
p=1 
'_
p>
0v2!1
2

2p
P ~min 2
2(2 +)k'k2
 2!1+1

 2Ck'k 2 s
slogN
s
+s+ 1!
 
2t:
Selecting=p~min=3would bound the expression above away from zero, as long as
P36
9
1 +
2
k'k2
 22+2

~min
2~min2+2
 
2Ck'k 2 s
slogN
s
+s+ 1!
+p~min
6t!2
: (64)
27

--- PAGE 28 ---
Noting that ~min2k'k2
 2and using the basic inequality (a+b)22a2+ 2b2twice yields
 
2Ck'k 2 s
slogN
s
+s+ 1!
+p~min
6t!2
.k'k2
 2
slogN
s
+s+ 1 +t2
72C2
:
Also, since (1 +
2)2
.1for1, one has
36
9
1 +
2
k'k2
 22+2

~min
2~min2+2
.
1 +
22k'k4+4

 2
~3+2

min:
Therefore, the desired condition in (52) holds, as long as
P&
1 +
22k'k6+4

 2
~3+2

min
slogN
s
+s+ 1 +t2
72C2
:
Finally, setting 0==2andt0=t2=(72C2)yields the advertised claim in (21).
7.5 Proof of Lemma 1
We follow a similar line of argument as §5.3.1 of [23]. To evaluate
I=Exg 
>x
1>x>0=1
(2)N
2Z
>x>0g 
>x
exp 
 kxk2
2!
dx;
we assume that andare not aligned (for the aligned case a similar procedure applies to merely ). We
consider the unitary matrix E= [e1;;eN], where
e1=;e2= 
>

r
1 
>2;
ande3;eNare any completion of the ortho-basis. Setting x=Ezyields>x=z1and
>x=
>
z1+r
1 
>2
z2:
Taking into account the injectivity of the linear map E, we can reformulate the integral in the z-domain as (see
Theorem 263D of [9] for the formal statement)
I=1
2Z
z1>0g 
>
z1+r
1 
>2
z2!
exp
 z2
1+z2
2
2
dz1dz2:
Acknowledgement: A. Aghasi would like to thank Roman Vershynin and Richard Kueng for the insightful
suggestions and communications.
28

--- PAGE 29 ---
8 Supplementary Materials
8.1 Net-Trim for Convolutional Layers
WhenXin2Tin,W2TwandXout2Toutare tensors, and 
indicates a subset of the tensor elements, our
central program takes the following form:
minimize
WkWk1subject to(
k(AXin(W) Xout)
kF
(AXin(W))
cV
c; (65)
wherekk1andkkFnaturally apply to the vectorized tensors. As before, for a given tensor Z,Z
is a tensor of
similar size, which takes identical values as Zon
and zero values on 
c.
The operatorAXin:Tw!Toutis a linear operator that is parameterized by Xin. For instance in convolutional
layers it is a tensor convolution operator with one of the operands being Xin. Throughout the text we assume
thatA
Xin:Tout!Twis the adjoint operator. The adjoint operator needs to satisfy the following property:
8W2Tw;8Z2Tout:hAXin(W);ZiTout=hW;A
Xin(Z)iTw:
Going through an identical line of argument as Section 5 yields similar ADMM steps, only diﬀerent in the way
thatXininteracts with W(3). More speciﬁcally,
W(1)
k+1= arg min
Wf1(W) +
2W+U(1)
k AXin
W(3)
k2
F; (66)
W(2)
k+1= arg min
Wf2(W) +
2W+U(2)
k W(3)
k2
F; (67)
W(3)
k+1= arg min
W
2W(1)
k+1+U(1)
k AXin(W)2
F+
2W(2)
k+1+U(2)
k W2
F; (68)
and the dual updates are performed via
U(1)
k+1=U(1)
k+W(1)
k+1 AXin
W(3)
k
;U(2)
k+1=U(2)
k+W(2)
k+1 W(3)
k+1:
Based on the steps above, the following algorithm is a straightforward modiﬁcation of the original Net-Trim
29

--- PAGE 30 ---
implementation presented in operator form.
Algorithm 3: Implementation of the Net-Trim for Convolutional Layers
input: Xin2Tin,Xout2Tout,
,V
,,
initialize :U(1)2Tout;U(2)2TwandW(3)2Tw % all initializations can be with 0
whilenot converged do
Y AXin
W(3)
 U(1)
ifY
 Xout


Fthen
W(1)

 Y
else
W(1)

 Xout

+Y
 Xout

 1
F 
Y
 Xout


end if
W(1)

c Y
c (Y
c V
c)+
W(2) S1=(W(3) U(2)) %S1=applies to each element of the matrix
W(3) arg minW1
2AXin(W) 
W(1)+U(1)2
F+1
2W 
W(2)+U(2)2
F
U(1) U(1)+W(1) AXin
W(3)
U(2) U(2)+W(2) W(3)
end while
return W(3)
The only undiscussed part in this algorithm is the update for W(3), which we address using an operator form of
the conjugate gradient algorithm.
8.1.1 Least Squares Update Using an Operator Conjugate Gradient
In this section we address the minimization
minimize
W2Tw1
2kAXin(W) Bk2
F+1
2kW Ck2
F; (69)
which is central to the W(3)update in (68). HereB2ToutandC2Tware tensors, andAXin:Tw!Toutis .
The minimizer to (69) can be found by taking a derivative and setting it to zero, i.e.,
A
Xin(AXin(W) B) +W C=0; (70)
or
A
Xin(AXin(W)) +W=A
Xin(B) +C: (71)
Solving(71)forWis eﬃciently possible via the method of conjugate gradient. The following algorithm outlines
the process of solving (71), which is a variant of the original CG algorithm (e.g., see [ 1]) reformulated in operator
30

--- PAGE 31 ---
form.
Algorithm 4: Least Squares Update in the Net-Trim Using Conjugate Gradient
initialize :W0=0;R0=A
Xin(B) +C;P0=R0
fork= 1;:::;K maxdo
Tk 1=AXin(Pk 1)
k=kRk 1k2
F
kTk 1k2
F+kPk 1k2
F
Wk=Wk 1+kPk 1
Rk=Rk 1 k 
A
Xin(Tk 1) +Pk 1
k=kRkk2
F
kRk 1k2
F
Pk=Rk+kPk 1
end for
Output: WKmax
8.2 Experiments
In this section, we present more details of the experimental setup and provide additional simulations which were
excluded from the original manuscript due to space limitation.
In our ﬁrst set of experiments, we presented a comparison between the cascade and parallel frameworks. As
stated, for this purpose we use the FC network of size 784300100010010(composed of four layers:
W12R784300;W22R3001000, etc), trained to classify the MNIST dataset. Panels (a) and (b) in Figure 2 of
the paper summarize the outcome of applying the Net-Trim parallel scheme to the trained FC model. By varying
the value of , one may explore diﬀerent levels of layer sparsity and discrepancy. Panel (a) reports the relative
value of the overall discrepancy as a function of the relative sparsity at each layer (i.e., percentage of zeros in
^W`). Each plot is obtained by varying for a range of values and retraining the FC model with 10K, 20K, 30K
and the entire 55K training samples. As expected, allowing more discrepancy improves the level of sparsity. Since
practically the overall discrepancy is not a good indication of the changes in the model accuracy, in panel (b) we
replace it with the test accuracy of the retrained models. An interesting observation is that retraining the models
with fewer samples does not signiﬁcantly degrade the test accuracies and even in some cases (e.g., 30K versus
55K) it causes a slight improvement in the accuracy of the retrained models. Panels (c) and (d) report a similar
set of experiments for the cascade Net-Trim, where increasing the inﬂation rate away from one allows producing
sparser networks.
Employing the parallel scheme (thanks to its distributable nature), and the use of a subset of the training
data in the Net-Trim retraining process are both computationally attractive paths, and the experiments in Figure
2 indicate that at least for a reasonable sparsity range, they could be both explored without much degradation of
the model accuracies. In the remainder of the experiments in this section, we will consistently use the parallel
scheme for our retraining purposes, and will no more reference to the Net-Trim parallel or cascade nature.
In the next set of experiments, we investigate the additional pruning that Net-Trim brings to the architecture
of neural networks beyond Dropout and `1regularization. For this purpose we consider the application of an `1
regularization, Dropout and a combination of both to the training of our standard FC model. We also apply a
similar set of tools to the LeNet convolutional network [ 22], which is composed of two convolutional layers (32
ﬁlters of size 55at the ﬁrst layer, and 64 ﬁlters of similar size at the second layer, both followed by 22max
pooling units), and two fully connected layers ( 313651210). While the linearity of the convolution operator
immediately allows the application of Net-Trim, in our experiments we omit retraining the convolutional layers as
31

--- PAGE 32 ---
the number of parameters in such layers is much less than the fully connected layers.
For both network architectures we vary (the`1penalty), and p(the Dropout probability of keeping) in a
range of values that tend to produce reasonably high test accuracies. The statistics reported in Table 1 correspond
to the FC and LeNet models, which resulted in the highest test accuracies. For both architectures the best results
Table 1: Application of Net-Trim for diﬀerent values of to the standard (FC) and convolutional (LeNet) networks
trained via careful choice of the `1regularization and Dropout probability ( = 10 5,p= 0:75for the FC model,
and= 10 5,p= 0:5for the LeNet architecture); improved quantities compared to the initial models are
highlighted in bold
FC LeNet
Network Test Acc. Test Acc. Network Test Acc. Test Acc.
Zeros ( %)(No FT) With FT Zeros ( %)(No FT) With FT
Initial Model 43.69 98.65 – 33.65 99.57 –Net-Trim= 0:01 71.93 98.65 98.76 83.80 99.59 99.60
= 0:02 76.13 98.65 98.72 88.76 99.60 99.57
= 0:04 80.02 98.56 98.66 92.75 99.54 99.53
= 0:06 81.98 98.54 98.59 94.46 99.49 99.47
= 0:08 83.34 98.36 98.48 95.40 99.35 99.44
= 0:1 84.30 98.08 98.38 96.01 98.26 99.35
= 0:2 86.99 96.76 97.88 97.37 98.83 99.22
= 0:3 88.61 94.69 97.31 97.89 98.61 99.07
happened when the Dropout and `1regularization were applied simultaneously. The ﬁrst row reports the initial
model statistics and the subsequent rows correspond to the application of the Net-Trim using diﬀerent values
of. In this experiment the third column of each architecture section corresponds to an additional ﬁne-tuning
step after Net-Trim prunes the network. This (optional) step uses the Net-Trim solution as an initialization for a
secondary training, which only applies to the non-zero weights identiﬁed by the Net-Trim. Such ﬁne-tuning often
results in an improvement in the generalization error without changing the sparsity of the network.
A quick assessment of Table 1 reveals that applying Net-Trim can signiﬁcantly improve the sparsity (and even
at the same time the accuracy) of the models. For instance, in the FC model we can improve the test accuracy to
98.76%, and at the same time increase the percentage of network zeros from 43.69% to 71.93%. A similar trend
holds for the LeNet model. If we allow some degradation in the test accuracy, the percentage of zeros can be
signiﬁcantly increased to 88.61% in the FC model, and 97.89% in the LeNet architecture.
Table 1 only reports the Net-Trim performance on the most accurate models. In Figure 3 of the paper we
presented a more comprehensive set of experiments on the LeNet Network. Figure 6 shows a similar set of
experiments on the FC network. In these experiments the mean test accuracy and initial model sparsity are
reported for the cases of Dropout, `1regularization, and a combination of both. For each setup the tuning
parameters ( ,p, or both) are varied in a range of values and unlike Table 1, the mean quantities are reported.
For instance, panel (a) indicates that applying the Dropout to the FC model with 0:3p0:8yields an average
network zero percentage of 0:07%, and approximately 97.5% test accuracy. However, applying Net-Trim along
with the FT step, can elevate the average accuracy to around 98%, and at the same time increase the network
percentage of zeros to almost 45%. The plot also reveals that with no loss in the model accuracies, we can improve
the sparsity of the models to up to 56% (corresponding to the point where the red and the dashed lines intersect).
An assessment of all panels (speciﬁcally the crossing of the red curves and the dashed lines) reveals that in all
three scenarios (Dropout, `1regularization and a combination of both), and for both architectures (FC and LeNet),
an additional application of Net-Trim can improve the models both in terms of accuracy and the number of
underlying parameters. Even in cases that the accuracy is degraded to some extent, but the model is signiﬁcantly
pruned, the pruned network may be considered a more reliable model. In Figure (7)we have demonstrated the
32

--- PAGE 33 ---
30 35 40 45 50 55 60 65 70 75
Percentage of Zeros90919293949596979899Test Accuracy (%)
Mean Initial Model Accuracy
Net-Trim Retrained (No Fine Tuning)
Net-Trim Retrained (With Fine Tuning)
Mean Initial Model Percentage of Zeros = 0.07%(a)
97.8 98 98.2 98.4 98.6 98.8
Percentage of Zeros90919293949596979899Test Accuracy (%)Mean Initial Model Accuracy
Net-Trim Retrained (No Fine Tuning)
Net-Trim Retrained (With Fine Tuning)
Mean Initial Model Percentage of Zeros = 93.7% (b)
82 84 86 88 90 92
Percentage of Zeros90919293949596979899Test Accuracy (%)
Mean Initial Model Accuracy
Net-Trim Retrained (No Fine Tuning)
Net-Trim Retrained (With Fine Tuning)
Mean Initial Model Percentage of Zeros = 67.52% (c)
Figure 6: Mean test accuracy vs mean model sparsity after the application of Net-Trim for FC network initially
regularized via `1penalty, Dropout, or both (the regularization parameter and Dropout probability are picked
from a range of values and mean quantities are reported); (a) a model trained with Dropout only: 0:3p0:8;
(b) a model trained with `1penalty only: 10 5510 3; (c) a model trained with Dropout and `1:
10 5210 4,0:5p0:75;
FC and LeNet models initially trained with Dropout and retrained using Net-Trim. Despite an accuracy loss of
1.3% for the FC model, and 1.7% for the LeNet model, the percentage of zeros have been increased to 63.32% and
96.8%, respectively. As a result of this reduction, when the models are tested with diﬀerent noisy versions of the
original test set, the reduced models exhibit a lower accuracy degradation (i.e., more robustness) to the noise
increase.
0 50 100 150 200 250 300
Noise Ratio (%)405060708090100Test Accuracy (%)Initial Model
Retrained Model
(a)
0 20 40 60 80 100 120 140 160 180 200
Noise Ratio (%)30405060708090100Test Accuracy (%)Initial Model
Retrained Model (b)
Figure 7: Noise robustness of initial and retrained networks; (a) FC; (b) LeNet
Thanks to the simple implementation of Net-Trim, in the aforementioned experiments, the retraining of the
layer matrices was only in order of few minutes on a standard desktop computer, while in the majority of the
33

--- PAGE 34 ---
cases, the initial training of the networks took much longer. We would like to note that we did not make any
eﬀorts to optimize the Net-Trim code and fully exploit the parallel features (e.g., matrix products, processing of
layers in parallel, etc). The distributable nature of our implementation supports yet much faster software than
the one currently present.
In the paper we also compared Net-Trim with the HPTD [ 15]. The HPTD algorithm does not come with any
performance guarantees, however, the basic implementation idea has made it a widespread tool in the network
compression community. Using tools such as quantization and Huﬀman coding, more advanced frameworks such
as the Deep Compression [ 13] have been developed later. However, their focus is mainly compressing the network
parameters on the memory, and HPTD pruning scheme is yet the most relevant single-module framework that
could be compared with Net-Trim.
With reference to Figure 4 of the paper, Figure 8 presents more details of the comparison between the Net-Trim
and HPTD on the FC and LeNet models. For the Net-Trim we use diﬀerent values of to prune the trained
40 45 50 55 60 65 70 75 80 85 90
Pruning Percentage2030405060708090100Accuracy Percentage
Net-Trim without fine-tuning
HPTD without fine-tuning
Net-Trim with fine-tuning, 10 epochs
HPTD with fine-tuning, 10 epochs
Net-Trim with fine-tuning, 30 epochs
HPTD with fine-tuning, 30 epochsTest Accuracy (%)
Percentage of Zeros
0 10 20 30 40 50
Training Epoch50556065707580859095100 Accuracy Percentage
Net-Trim
HPTD
(a)Test Accuracy (%)
Fine-Tuning Epoch
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7102030405060708090Pruning Percentage Percentage of Zeros
40 50 60 70 80 90 100
Pruning Percentage2030405060708090100Accuracy Percentage
Net-Trim without fine-tuning
HPTD without fine-tuning
Net-Trim with fine-tuning, 10 epochs
HPTD with fine-tuning, 10 epochs
Net-Trim with fine-tuning, 30 epochs
HPTD with fine-tuning, 30 epochsTest Accuracy (%)
Percentage of Zeros
0 10 20 30 40 50
Training Epoch556065707580859095100 Accuracy Percentage
Net-Trim
HPTD
(b)Test Accuracy (%)
Fine-Tuning Epoch
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7405060708090100 Pruning Percentage 1 Percentage of Zeros
Figure 8: Comparison of Net-Trim and HPTD in diﬀerent settings for (a) FC model, (b) LeNet model; the
left panels compare Net-Trim and HPTD test accuracy vs percentage of zeros, without ﬁne-tuning, and with
ﬁne-tuning using 10 and 30 epochs; middle panels show the number of ﬁne-tuning epochs and the acquired
accuracy using Net-Trim and HPTD; the right panels indicate the percentage of zeros as a function for Net-Trim
34

--- PAGE 35 ---
networks. To compare the method with the HPTD, after each application of the Net-Trim and counting the
number of zeros, the same number of elements are truncated from the initial network to be used for the HPTD
implementation. HPTD is followed by a ﬁne-tuning step after the truncation, which is also an optional task for
Net-Trim. Nevertheless, both algorithms are compared without ﬁne-tuning, or with ﬁne-tuning using 10 or 30
epochs. The left plots in panels (a) and (b) show that in all scenarios Net-Trim outperforms HPTD in generating
more accurate models when the levels of sparsity are matched. The middle plots also show the improvements
in the accuracy as a function of the number of epochs required in the ﬁne-tuning process for the two schemes.
In both scenarios, Net-Trim requires only few epochs to achieve the top accuracy, while achieving such level of
accuracy for the HPTD is either not feasible or takes many ﬁne-tuning epochs.
Figure 9 demonstrates another set of comparative experiments between Net-Trim and HPTD, performed on a
much larger augmented dataset. The reference training set is the CIFAR10 color-image database, which contains
50K samples of size 3232from ten classes [20, 21].
In order to obtain higher test accuracies, the training images are multiplicated by taking 2424windows to
randomly crop them, and each cropped image is horizontally ﬂipped with probability 0:5. This process augments
the training set to 6,400,000 samples. The neural network employed to address the initial classiﬁcation problem is
convolutional, where the ﬁrst layer of the trained network uses 64 ﬁlters of size 553, followed by a max pooling
unit (size: 33, stride: 22). The second layer is also convolutional with 64 ﬁlters of size 5564and a similar
max pooling unit. The remainder of the network contains three fully connected layers ( 313638419210).
For this relatively large dataset we also go through the exercise of retraining the Net-Trim with only part
of the training samples, speciﬁcally 25K, 50K and 75K samples of the entire 6.4M training set. A similar set
of comparisons between the Net-Trim and the HPTD as in Figure 8 is performed, noting that the ﬁne-tuning
step for both schemes is carried out using all the training samples. Similar to the previous experiment, Net-Trim
consistently outperforms HPTD in all similar setups. Aside from such superiority, we highlight the possibility of
retraining Net-Trim using only part of the training samples. For instance, a comparison of panels (a) and (b)
shows that almost identical results can be achieved in terms of accuracy versus sparsity, when Net-Trim is solved
with 25K samples instead of 50K samples. For instance, for both panels, a Net-Trim application followed by a
single ﬁne-tuning step can increase the percentage of the zeros in the network to more than 80%, with almost no
loss in the model accuracy. Basically, as also discussed previously with reference to Figure 2, for large data sets
formulating the Net-Trim with only a portion of the data can be considered as a general computation shortcut.
Another interesting observation, which is more apparent on the left plot of panel (c), is that ﬁne-tuning does
not always improve the accuracy of the models after the application of Net-Trim, and especially in low pruning
regimes may cause degrading the accuracy due to phenomena such as overﬁtting. For example, in panel (c), up to
a pruning percentage of almost 65%, a ﬁne-tuning step after the Net-Trim slightly degrades the accuracy. While a
ﬁne-tuning step is likely to help in the majority of cases, our access to both Net-Trim’s plain outcome, and the
ﬁne-tuned version provides the ﬂexibility of picking the most compressed and accurate model among the two.
References
[1]Conjugate gradient algorithm .https://math.aalto.fi/opetus/inv/CGalgorithm.pdf . Accessed: 2018-07-
18.
[2]A. Aghasi, A. Abdi, N. Nguyen, and J. Romberg ,Net-trim: Convex pruning of deep neural networks
with performance guarantee , in Advances in Neural Information Processing Systems 31, Curran Associates,
Inc., 2017, pp. 3180–3189.
35

--- PAGE 36 ---
40 45 50 55 60 65 70 75 80 85 90
Pruning Percentage102030405060708090Accuracy Percentage
Net-Trim without fine-tuning
HPTD without fine-tuning
Net-Trim with fine-tuning, 10 epochs
HPTD with fine-tuning, 10 epochs
Net-Trim with fine-tuning, 30 epochs
HPTD with fine-tuning, 30 epochs
Test Accuracy (%)
Percentage of Zeros
0 10 20 30 40 50
Training Epoch404550556065707580Accuracy Percentage
Net-Trim
HPTD
(a)Test Accuracy (%)
Fine-Tuning Epoch
0 0.1 0.2 0.3 0.4 0.5 0.6 0.730405060708090Pruning Percentage Percentage of Zeros
40 45 50 55 60 65 70 75 80 85 90
Pruning Percentage102030405060708090Accuracy Percentage
Net-Trim without fine-tuning
HPTD without fine-tuning
Net-Trim with fine-tuning, 10 epochs
HPTD with fine-tuning, 10 epochs
Net-Trim with fine-tuning, 30 epochs
HPTD with fine-tuning, 30 epochsTest Accuracy (%)
Percentage of Zeros
0 10 20 30 40 50
Training Epoch455055606570758085Accuracy Percentage
Net-Trim
HPTD
(b)Test Accuracy (%)
Fine-Tuning Epoch
0 0.1 0.2 0.3 0.4 0.5 0.6 0.730405060708090Pruning Percentage Percentage of Zeros
40 45 50 55 60 65 70 75 80 85 90
Pruning Percentage102030405060708090Accuracy Percentage
Net-Trim without fine-tuning
HPTD without fine-tuning
Net-Trim with fine-tuning, 10 epochs
HPTD with fine-tuning, 10 epochs
Net-Trim with fine-tuning, 30 epochs
HPTD with fine-tuning, 30 epochsTest Accuracy (%)
Percentage of Zeros
0 10 20 30 40 50
Training Epoch404550556065707580Accuracy Percentage
Net-Trim
HPTD
(c)Test Accuracy (%)
Fine-Tuning Epoch
0 0.1 0.2 0.3 0.4 0.5 0.6 0.730405060708090Pruning Percentage Percentage of Zeros
Figure 9: Similar comparison plots as in Figure 8 for CIFAR-10: (a) retraining of Net-Trim and HPTD performed
using 25K samples; (b) retraining performed using 50K samples; (c) retraining performed using 75K samples
36

--- PAGE 37 ---
[3]S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein ,Distributed optimization and statistical
learning via the alternating direction method of multipliers , Foundations and Trends Rin Machine Learning,
3 (2011), pp. 1–122.
[4]E. Candes ,The restricted isometry property and its implications for compressed sensing , Comptes Rendus
Mathematique, 346 (2008), pp. 589–592.
[5]E. Candes and Y. Plan ,A probabilistic and ripless theory of compressed sensing , IEEE Transactions on
Information Theory, 57 (2011), pp. 7235–7254.
[6]E. Candes, J. Romberg, and T. Tao ,Stable signal recovery from incomplete and inaccurate measurements ,
Communications on pure and applied mathematics, 59 (2006), pp. 1207–1223.
[7]V. Chandrasekaran, B. Recht, P. Parrilo, and A. Willsky ,The convex geometry of linear inverse
problems , Foundations of Computational mathematics, 12 (2012), pp. 805–849.
[8]W. Chen, J. Wilson, S. Tyree, K. Weinberger, and Y. Chen ,Compressing neural networks with the
hashing trick , in International Conference on Machine Learning, 2015, pp. 2285–2294.
[9]D. Fremlin ,Measure theory , Torres Fremlin, 2 (2000).
[10]F. Girosi, M. Jones, and T. Poggio ,Regularization theory and neural networks architectures , Neural
computation, 7 (1995), pp. 219–269.
[11]I. Goodfellow, Y. Bengio, and A. Courville ,Deep Learning , MIT Press, 2016.
[12]D. Gross ,Recovering low-rank matrices from few coeﬃcients in any basis , IEEE Transactions on Information
Theory, 57 (2011), pp. 1548–1566.
[13]S. Han, H. Mao, and W. Dally ,Deep compression: Compressing deep neural networks with pruning,
trained quantization and huﬀman coding , International Conference on Learning Representations (ICLR),
(2016).
[14]S. Han, H. Mao, and W. J. Dally ,Deep compression: Compressing deep neural networks with pruning,
trained quantization and huﬀman coding , arXiv preprint arXiv:1510.00149, (2015).
[15]S. Han, J. Pool, J. Tran, and W. Dally ,Learning both weights and connections for eﬃcient neural
network, in Advances in Neural Information Processing Systems, 2015, pp. 1135–1143.
[16]A. Hoerl and R. Kennard ,Ridge regression: Biased estimation for nonorthogonal problems , Technometrics,
12 (1970), pp. 55–67.
[17]D. Hsu, S. Kakade, and T. Zhang ,A tail inequality for quadratic forms of subgaussian random vectors ,
Electronic Communications in Probability, 17 (2012).
[18]S. Ioffe and C. Szegedy ,Batch normalization: Accelerating deep network training by reducing internal
covariate shift , arXiv preprint arXiv:1502.03167, (2015).
[19]V. Koltchinskii and S. Mendelson ,Bounding the smallest singular value of a random matrix without
concentration , International Mathematics Research Notices, 2015 (2015), pp. 12991–13008.
[20]A. Krizhevsky ,Convolutional deep belief networks on cifar-10 , (2010).
37

--- PAGE 38 ---
[21]A. Krizhevsky, I. Sutskever, and G. Hinton ,Imagenet classiﬁcation with deep convolutional neural
networks , in Advances in Neural Information Processing Systems, 2012.
[22]Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner ,Gradient-based learning applied to document
recognition , Proceedings of the IEEE, 86 (1998), pp. 2278–2324.
[23]C. Louart, Z. Liao, and R. Couillet ,A random matrix approach to neural networks , arXiv preprint
arXiv:1702.05419, (2017).
[24]S. Mendelson ,Learning without concentration , in Conference on Learning Theory, 2014, pp. 25–39.
[25]S. Mendelson ,Learning without concentration for general loss functions , Probability Theory and Related
Fields, (2017).
[26]S. Nowlan and G. Hinton ,Simplifying neural networks by soft weight-sharing , Neural computation, 4
(1992), pp. 473–493.
[27]J. Schmidhuber ,Deep learning in neural networks: An overview , Neural Networks, 61 (2015), pp. 85–117.
[28]N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov ,Dropout: a simple
way to prevent neural networks from overﬁtting , The Journal of Machine Learning Research, 15 (2014),
pp. 1929–1958.
[29]R. Tibshirani ,Regression shrinkage and selection via the lasso , Journal of the Royal Statistical Society.
Series B (Methodological), (1996), pp. 267–288.
[30]J. Tropp ,Convex recovery of a structured signal from independent random linear measurements , in Sampling
Theory, a Renaissance, Springer, 2015, pp. 67–101.
[31]A. van der Vaart and J. Wellner ,Weak Convergence and Empirical Processes: With Applications to
Statistics , Springer Science & Business Media, 1996.
[32]R. Vershynin ,Introduction to the non-asymptotic analysis of random matrices , Cambridge University Press,
2012, pp. 210–268, https://doi.org/10.1017/CBO9780511794308.006 .
[33]L. Wan, M. Zeiler, S. Zhang, Y. LeCun, and R. Fergus ,Regularization of neural networks using
dropconnect , in Proceedings of the 33rd International Conference on Machine Learning, 2016.
38
