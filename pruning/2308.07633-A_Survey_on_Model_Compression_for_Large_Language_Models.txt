# 2308.07633.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2308.07633.pdf
# File size: 504718 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
A Survey on Model Compression for Large Language Models
Xunyu Zhu1,2, Jian Li1,2∗, Yong Liu3, Can Ma1,2, Weiping Wang1,2
1Institute of Information Engineering, Chinese Academy of Sciences
2School of Cyber Security, University of Chinese Academy of Sciences
3Gaoling School of Artificial Intelligence, Renmin University of China
{zhuxunyu, lijian9026, macan, wangweiping}@iie.ac.cn, liuyonggsai@ruc.edu.cn
Abstract
Large Language Models (LLMs) have trans-
formed natural language processing tasks
successfully. Yet, their large size and high
computational needs pose challenges for
practical use, especially in resource-limited
settings. Model compression has emerged
as a key research area to address these chal-
lenges. This paper presents a survey of
model compression techniques for LLMs.
We cover methods like quantization, prun-
ing, and knowledge distillation, highlight-
ing recent advancements. We also dis-
cuss benchmarking strategies and evaluation
metrics crucial for assessing compressed
LLMs. This survey offers valuable insights
for researchers and practitioners, aiming to
enhance efficiency and real-world applica-
bility of LLMs while laying a foundation for
future advancements.
1 Introduction
Large Language Models (LLMs) (Touvron et al.,
2023a,b; Zhang et al., 2022; Scao et al., 2022;
Wang and Komatsuzaki, 2021; OpenAI, 2024) re-
fer to Transformer language models that contain
billions (or more) of parameters, which are trained
on massive text data. LLMs consistently exhibit
remarkable performance across various tasks, but
their exceptional capabilities come with signif-
icant challenges stemming from their extensive
size and computational requirements. For in-
stance, the GPT-175B model (Brown et al., 2020),
with an impressive 175 billion parameters, de-
mands a minimum of 350GB of memory in half-
precision (FP16) format. Furthermore, deploy-
ing this model for inference necessitates at least
five A100 GPUs, each featuring 80GB of mem-
ory, to efficiently manage operations. To tackle
these issues, a prevalent approach known as model
∗Corresponding authorcompression (Han et al., 2016) offers a solution.
Model compression involves transforming a large,
resource-intensive model into a compact version
suitable for deployment on resource-constrained
devices. Additionally, model compression can
enhance LLM inference speed and optimizes re-
source efficiency.
In our paper, our primary objective is to il-
luminate the recent strides made in the domain
of model compression techniques tailored specif-
ically for LLMs. Our work conducts an exhaus-
tive survey of methodologies, metrics, and bench-
marks of model compression for LLMs. Figure 1
shows the taxonomy of model compression meth-
ods for LLMs, including quantization, pruning,
knowledge distillation, and low-rank factorization.
Figure 2 further shows basic flow of these model
compression methods for LLMs. Furthermore,
our study sheds light on prevailing challenges
and offers a glimpse into potential future research
trajectories in this evolving field. We advocate
for collaborative efforts within the community to
pave the way for an ecologically conscious, all-
encompassing, and sustainable future for LLMs.
While there were previous surveys on neural net-
works model compression (Li et al., 2023c) and
it has been lightly discussed in prior surveys on
LMs (Rogers et al., 2020) and LLMs (Zhao et al.,
2023), our work is the inaugural survey dedicated
solely to model compression for LLMs.
2 Metrics and Benchmarks
2.1 Metrics
Model compression of LLMs can be measured us-
ing various metrics, which capture different as-
pects of performance. These metrics are com-
monly presented alongside accuracy and zero-shot
ability to comprehensively evaluate the LLM.
Model Size in a LLM typically is measured
by the number of total parameters of the LLM.arXiv:2308.07633v4  [cs.CL]  30 Jul 2024

--- PAGE 2 ---
Model Compression for Large Language ModelsQuantization ( §3)Quantization-Aware
Training ( §3.1)LLM-QAT (Liu et al., 2023b), BitDistiller (Du et al., 2024), OneBit (Xu et al., 2024)
Post-Training
Quantization ( §3.2)Weight-Only
Quantization ( §3.2.1)LUT-GEMM (Park et al., 2024), GPTQ (Frantar et al., 2023), QuIP (Chee et al., 2023),
AWQ (Lin et al., 2023), OWQ (Lee et al., 2024), SpQR (Dettmers et al., 2024),
SqueezeLLM (Kim et al., 2023b)
Weight-Activation
Quantization ( §3.2.2)ZeroQuant (Yao et al., 2022), LLM.int8() (Dettmers et al., 2022), SmoothQuant (Xiao et al., 2023),
RPTQ (Yuan et al., 2023a), OliVe (Guo et al., 2023), OS+ (Wei et al., 2023), LLM-FP4 (Liu et al., 2023a),
OmniQuant (Shao et al., 2024b)
KV Cache
Quantization ( §3.2.3)KVQuant (Hooper et al., 2024), KIVI (Liu et al., 2024), WKVQuant (Yue et al., 2024)
Pruning ( §4)Unstructured Pruning
(§4.1)SparseGPT (Frantar and Alistarh, 2023), Wanda (Sun et al., 2024), SAMSP (Shao et al., 2024a), DSnoT (Zhang et al., 2024),
Flash-LLM (Xia et al., 2023)
Structured Pruning
(§4.2)LLM-Pruner (Ma et al., 2023), Shortened LLaMA (Kim et al., 2024), FLAP (An et al., 2024), SliceGPT (Ashkboos et al., 2024),
Sheared LLaMA (Xia et al., 2024)
Semi-structured
Pruning ( §4.3)E-Sparse (Li et al., 2023b), SparseGPT (Frantar and Alistarh, 2023), Wanda (Sun et al., 2024)
Knowledge
Distillation ( §5)Black-box KD ( §5.1)Chain-of-Thought
(§5.1.1)MT-COT (Li et al., 2024b), CoT Prompting (Magister et al., 2023), Fine-tune-CoT (Ho et al., 2023),
SSLM (Fu et al., 2023), SCOTT (Wang et al., 2023a), Distilling Step-by-Step (Hsieh et al., 2023),
SOCRATIC CoT (Shridhar et al., 2023), PaD (Zhu et al., 2024), DRA (Wang et al., 2023f),
TDIG (Li et al., 2024c)
In-Context Learning
(§5.1.2)In-context Learning Distillation (Huang et al., 2022), AICD (Liu, 2024)
Instruction Following
(§5.1.3)Lion (Jiang et al., 2023), LaMini-LM (Wu et al., 2024), SELF-INSTRUCT (Wang et al., 2023d),
Selective Reflection-Tuning (Li et al., 2024a)
White-box KD ( §5.2) MINILLM (Gu et al., 2024), GKD (Agarwal et al., 2024), TED (Liang et al., 2023)
Low-Rank
Factorization ( §6)LPLR (Saha et al., 2023), ASVD (Yuan et al., 2023b), LSAER (Sharma et al., 2024)
Figure 1: Taxonomy of Model Compression methods for Large Language Models.
In general, LLMs with more parameters often re-
quires more computational resources and memory
for both training and inference.
Floating Point Operations (FLOPs) is an in-
dicator that measures the computational efficiency
of LLMs, representing the number of floating-
point operations required for the LLM to per-
form an instance. In model compression, reduc-
ing FLOPs helps to make the LLM run faster and
more efficiently.
Mean FLOPS Utilization (MFU) quantifies
the practical efficiency of computational resource
utilization by LLMs during tasks. MFU measures
the ratio of actual FLOPS utilized by the LLM to
the maximum theoretical FLOPS of a device. Un-
like FLOPs, which estimates the maximum oper-
ations an LLM might perform, MFU assesses the
actual effectiveness of resource use in operation.
Essentially, while FLOPs measures a LLM’s the-
oretical compute needs, MFU shows how effec-
tively these computations are utilized in practice.
Inference time (i.e., latency) measures the time
taken by the LLM to process and generate re-
sponses for input data during inference. Inference
time is particularly crucial for real-world applica-
tions where the LLM needs to respond for user
queries or process large amounts of data in real-
time.
Speedup Ratio measures how much faster a
compressed LLM performs tasks compared to theoriginal LLM. Specifically, it measures the ratio
of the inference time of the uncompressed model
over the inference time of the compressed model.
Higher ratios mean greater efficiency and reduced
computation time, highlighting effective compres-
sion.
Compression Ratio measures how much a
LLM’s size is reduced through compression, cal-
culated as the original size divided by the com-
pressed size. Higher ratios mean greater size re-
duction, showing the compression’s effectiveness
in saving storage and memory.
2.2 Benchmarks and Datasets
The main goal of these benchmarks and datasets
is to measure the efficiency and performance of
compressed LLMs in comparison to their un-
compressed counterparts. These benchmarks and
datasets typically consist of diverse tasks and
datasets that cover a range of natural language pro-
cessing challenges.
2.2.1 Common Benchmarks and Datasets
The majority of research evaluates compressed
LLMs on well-established NLP benchmarks and
datasets. For instance, WikiText-2 (Merity et al.,
2017), C4 (Raffel et al., 2020), and PTB (Marcus
et al., 1993) are designed for evaluating the per-
plexity performance of language models. LAM-
BADA (Paperno et al., 2016), PIQA (Tata and

--- PAGE 3 ---
QATTraining
Data
Update(Optional)
Calibration
Data
Update(Optional)
(Optional)
Weight and Activation
Quantization
PTQTokens
Channels Keys
Quantized
KeysTokens
ChannelsValues
Quantized Values
KV Cache QuantizationCalibration
Data 
Update
Structur ed
PruningUnstructur ed
PruningSemi-Structur ed
Pruning 
U
V
Decompose
Training
Data
Augmented
Training
DataWhite-box KD Black-box KDWhite-box TeacherTeacher  LLMs
Student LMsBlack-box TeacherQuantization Pruning
Knowledge Distillation  Low-Rank Appr oximationHigh-Pr ecision Weight High-Pr ecision Activation Low-Pr ecision Weight Low-Pr ecision Activation Zero-Value Weight
（Optional ）Figure 2: Illustrations of model compression methods for LLMs. In these methods, Quantization-Aware Training
(QAT) and Knowledge Distillation (KD) stand out as task-based model compression techniques, tailored for spe-
cific tasks. Conversely, other model compression methods are task-agnostic, designed to operate independently of
specific tasks.
Patel, 2003), and OpenBookQA (Mihaylov et al.,
2018) are designed to evaluate the zero-shot ability
of language models. GSM8K (Cobbe et al., 2021),
CommonsenseQA (Talmor et al., 2019) and Strat-
egyQA (Geva et al., 2021) are designed to evaluate
the reasoning ability of language models.
2.2.2 BIG-Bench
BIG-Bench (BBH) (Srivastava et al., 2023) is a
benchmark suite designed for LLMs, covering
over 200 NLP tasks, e.g., Text Comprehension
Tasks, Inference Tasks, Mathematical Reasoning
Tasks. The aim of BBH is to evaluate the per-
formance of LLMs across these various complex
tasks. The compressed LLMs use BBH to measure
their capability across a multidimensional spec-
trum of tasks.
2.2.3 Unseen Instructions Datasets
Unseen instructions datasets are used to evaluate
the performance of LLMs on unseen tasks. For
instance, the Vicuna-Instructions (Zheng et al.,
2023) dataset created by GPT-4 includes 80 com-
plex questions across nine different categories like
generic, knowledge-based, and writing tasks. An-
other dataset, User-Oriented-Instructions (Wang
et al., 2023e), consists of 252 carefully selected
instructions inspired by various user-focused ap-plications such as Grammarly, StackOverflow, and
Overleaf. These datasets evaluate how well com-
pact LLMs can handle and carry out new tasks by
presenting them with unfamiliar instructions.
2.2.4 EleutherAI LM Harness
The EleutherAI LM Harness (Gao et al., 2023)
is an advanced framework for evaluating LLMs,
providing a unified testing platform that supports
over 60 standard academic benchmarks along with
hundreds of subtasks and variants. The standard-
ized evaluation tasks provided by the harness en-
sure the reproducibility and comparability of eval-
uation, which is essential for implementing fair
and reproducible evaluations for the compressed
LLMs.
3 Quantization
Quantization (Gray and Neuhoff, 1998) refers to
the process of reducing the number of bits (i.e.,
precision) in the parameters of the model with
minimal loss in inference performance. Quan-
tization can be categorized into two main ap-
proaches: Quantization-Aware Training (QAT) ,
andPost-Training Quantization (PTQ) . The pri-
mary distinction between the two approaches lies
in whether retraining is needed during quantiza-

--- PAGE 4 ---
Table 1: The performance of various representative LLM quantization methods.
Category†Methods LLMBit Width Perplexity Difference‡
Speedup
Weights Activations KV Cache Wikitext-2 C4
QATLLM-QAT LLaMA-30B 4 8 16 0.5 0.9 -
BitDistiller LLaMA2-13B 2 16 16 1.9 - -
OneBit LLaMA-13B 1 16 16 4.09 3.64 -
Weight-Only QuantizationLUT-GEMM LLaMA-65B 3 16 16 0.14 - 2.04 ×
SqueezeLLM LLaMA-13B 3 16 16 0.51 0.67 2.4 ×
GPTQ OPT-175B 3 16 16 0.34 0.23 3.24 ×
AWQ LLaMA2-70B 3 16 16 0.42 - 3.2 ×
OWQ LLaMA-65B 3.01 16 16 0.72 - -
SpQR LLaMA-30B 3.89 16 16 0.15 0.1 2.0 ×
QuIP LLaMA2-70B 2 16 16 3.007 3.228 -
Weight-Activation QuantizationZeroQuant GPT-J-6B 8 8 16 0.16 - 3.67 ×
LLM.int8() OPT-13B 8 8 16 - 0.00 1.22 ×
SmoothQuant OPT-175B 8 8 16 0.18 - 1.56 ×
RPTQ OPT-175b 4 4 16 2.26 2.15 -
Olive BLOOM-7B 4 4 16 2.11 2.24 4.5 ×
OS+ LLaMA-65B 4 4 16 5.77 - -
QT OPT-1.3B 8 8 16 17.74 - -
ZeroQuant-FP LLaMA-30B 4 8 16 0.18 0.13 -
OmniQuant LLaMA-7B 4 6 16 0.41 0.55 -
KV Cache QuantizationKVQuant LLaMA-65B 16 16 2 0.19 0.11 1.4 ×
WKVQuant LLaMA-13B 4 16 4 0.12 0.14 -
†: The results presented in the table are solely derived from the original papers.
‡: (The perplexity of the quantized LLM) - (The perplexity of the origin LLM).
tion. PTQ enables direct use of quantized mod-
els in inference, while QAT requires retraining
to rectify errors introduced by quantization. Ta-
ble 1 shows the performance of many representa-
tive LLM quantization methods.
3.1 Quantization-Aware Training
QAT involves retraining a quantized model to
counteract performance degradation caused by
quantization. For instance, LLM-QAT (Liu et al.,
2023b) implements the standard QAT framework
directly onto LLMs. LLM-QAT distills knowl-
edge by generating data from the LLM itself, and
train the quantized LLM to align with the output
distribution of the original LLM based on the gen-
erated data. BitDistiller (Du et al., 2024) merges
QAT with self-distillation, enhancing LLM per-
formance at sub-4-bit precisions. It employs tai-
lored asymmetric quantization, clipping, and a
Confidence-Aware Kullback-Leibler Divergence
objective for faster convergence and superior re-
sults. OneBit (Xu et al., 2024) introduces a novel
1-bit parameter representation method and an ef-
fective parameter initialization method to imple-
ment 1-bit quantization for LLM weight matrices,
paving the way for the extremely low bit-width de-
ployment of LLMs.
Remark 1. While QAT can mitigate quantiza-tion’s accuracy degradation, retraining demands
a lot of effort due to tens or hundreds of bil-
lions of parameters in LLMs. A practical solu-
tion is to incorporate Parameter-Efficient Fine-
Tuning (PEFT) into the retraining process of QAT.
Currently, methods like QLORA (Dettmers et al.,
2023), PEQA (Kim et al., 2023a) and LoftQ (Li
et al., 2023a) combine quantization with PEFT
for model fine-tuning efficiency. However, these
methods are typically task-dependent. L4Q (Jeon
et al., 2024) makes a preliminary attempt to en-
hance generality by leveraging LoRA-wise learned
quantization step size for LLMs. We think that in-
troducing PEFT to enhance QAT efficiency is not
only feasible but also holds significant promise,
warranting thorough exploration.
3.2 Post-Training Quantization
PTQ efficiently converts a full-precision LLM to
low-precision without retraining, saving memory
and computational costs. We categorize PTQ for
LLMs into three groups: Weight-Only Quan-
tization ,Weight-Activation Quantization , and
KV Cache Quantization . The disparity between
these groups lies in their quantization objectives.
Weight-only quantization focuses solely on quan-
tizing weights, whereas weight-activation quanti-
zation extends its objective to both weights and
activations. Prior research (Yao et al., 2023) in-

--- PAGE 5 ---
dicates that activation quantization is typically
more sensitive to weight quantization, allowing
weight-only quantization to achieve lower bit-
width. However, since quantized weights necessi-
tate dequantization before multiplication with ac-
tivations, weight-only quantization inevitably in-
troduces additional computational overhead dur-
ing inference and cannot enjoy the accelerated
low-bit operation supported by specific hardware.
Furthermore, kv cache quantization targets the
KV cache, which stores keys and values of at-
tention layers. The KV cache often consumes
lots of memory, acting as a bottleneck for input
streams containing lengthy tokens. By implement-
ing kv cache quantization, it is possible to increase
throughput and accommodate inputs with longer
tokens more efficiently.
3.2.1 Weight-Only Quantization
Weight-only quantization is the most conventional
and widespread method. For example, LUT-
GEMM (Park et al., 2024) uses binary-coding
quantization (BCQ) (Rastegari et al., 2016) for-
mat, which factorizes the parameters of LLMs
into binary parameters and a set of scaling fac-
tors, to accelerate quantized matrix multiplica-
tions in weight-only quantization. GPTQ (Fran-
tar et al., 2023) proposes a layer-wise quantiza-
tion method based on Optimal Brain Quantization
(OBQ) (Frantar and Alistarh, 2022), which up-
dates weights with inverse Hessian information,
and quantizes LLMs into 3/4-bit. QuIP (Chee
et al., 2023) optimally adjusts weights by utiliz-
ing the LDL decomposition of the Hessian ma-
trix derived from vectors drawn uniformly at ran-
dom from a calibration set, and multiplies weight
and Hessian matrices with a Kronecker product of
random orthogonal matrices to ensure incoherence
between weight and Hessian matrices. Combin-
ing these two steps, QuIP successfully quantizes
LLMs into 2-bits with minimal performance loss.
To further minimize quantization errors in the
weight-only quantization of LLMs, lots of works
identify sensitive weights, which have an impor-
tant effect on LLMs’ quantization performance,
and store these sensitive weights in high preci-
sion. For example, AWQ (Lin et al., 2023) stores
the top 1% of weights that have the most sig-
nificant impact on LLM performance in high-
precision, and integrates a per-channel scaling
method to identify optimal scaling factors. Here,
"channel" denotes individual dimensions or fea-ture maps within the model. Similar with AWQ,
OWQ (Lee et al., 2024) store weights sensitive
to activation outliers in high-precision, and quan-
tizes other non-sensitive weights. Different from
OWQ, SpQR (Dettmers et al., 2024) employs the
L2 error between the original and quantized pre-
dictions as a weight sensitivity metric. Further-
more, SqueezeLLM (Kim et al., 2023b) introduces
a weights clusters algorithm based on sensitiv-
ity, using k-means centroids as quantized weight
values, to identify sensitive weights. The sen-
sitivity is approximated by the Hessian matrix
of weights. Then, SqueezeLLM stores sensitive
weights in an efficient sparse format, and quantize
other weights. SqueezeLLM quantizes LLMs in
3-bit, and achieves a more than 2× speedup com-
pared to the FP16 baseline.
3.2.2 Weight-Activation Quantization
Alongside works centered on weight-only quan-
tization in LLMs, there is a plethora of research
focusing primarily on weight-activation quantiza-
tion in LLMs. For example, ZeroQuant (Yao
et al., 2022) is the first work to implement weight-
activation quantization for LLMs, which uses
group-wise quantization for weight and token-
wise quantization for activations, and reduces the
precision for weights and activations of LLMs to
INT8.
LLMs have outliers in activations, and the per-
formance of LLMs declines a lot, if these acti-
vations with outliers are directly quantized. Re-
cent works try to treat these outliers specially
to reduce quantization errors in weight-activation
quantization. For example, LLM.int8() (Dettmers
et al., 2022) stores these outlier feature dimen-
sions into high-precision, and uses vector-wise
quantization, which assigns separate normaliza-
tion constants to each inner product within ma-
trix multiplication, to quantize other features.
LLM.int8() quantizes weights and activations of
LLMs into 8-bit without any performance degra-
dation. SmoothQuant (Xiao et al., 2023) designs a
per-channel scaling transformation to smooths the
activation outliers based on the discovery that dif-
ferent tokens have similar variations across chan-
nels of activations. RPTQ (Yuan et al., 2023a)
finds that the range of values varies greatly be-
tween different channels, and integrates a chan-
nel reordering method, which clusters and re-
orders the channels in the activation and uses the
same quantization parameters to quantize the val-

--- PAGE 6 ---
ues in each cluster, into layer normalization and
linear layer weights to efficiently reduce the ef-
fect of numerical range differences between chan-
nels. OliVe (Guo et al., 2023) thinks that out-
liers are more important than the normal values,
and uses an outlier-victim pair (OVP) quantiza-
tion to handle outlier values locally with low hard-
ware overheads and significant performance bene-
fits. OS+ (Wei et al., 2023) further finds that out-
liers are concentrated in specific and asymmetric
channels. Based on the findings, OS+ incorpo-
rates channel-wise shifting to eliminate the impact
of asymmetry and channel-wise scaling to bal-
ance the distribution of outliers. LLM-FP4 (Liu
et al., 2023a) uses floating-point formats (specif-
ically FP8 and FP4) to address the limitations of
traditional integer quantization (such as INT8 and
INT4) to deal with outliers. Furthermore, LLM-
FP4 (Liu et al., 2023a) points out that exponent
bits and clipping range are important factors that
effect the performance of FP quantization, and in-
troduces a search-based framework for determin-
ing the optimal exponent bias and maximal quanti-
zation value. OmniQuant (Shao et al., 2024b) han-
dles the activation outliers by equivalently shifting
the challenge of quantization from activations to
weights, and optimizes the clipping threshold to
adjust the extreme values of the weights.
3.2.3 KV Cache Quantization
With the increasing number of input tokens sup-
ported by LLMs, the memory usage of the KV
cache also increases. Recent efforts begin to fo-
cus on kv cache quantization to reduce the mem-
ory footprint of LLMs and accelerate their in-
ference. For example, KVQuant (Hooper et al.,
2024) proposes several KV Cache Quantization
methods, such as Per-Channel Key Quantization,
PreRoPE Key Quantization, and Non-Uniform kv
cache quantization, to implement 10 million con-
text length LLM inference. Through an in-depth
analysis of the element distribution within the
KV cache, KIVI (Liu et al., 2024) finds that key
caches should be quantized per-channel, while
value caches should be quantized per-token. Fi-
nally, KIVI succeeds in quantizing the KV cache
to 2 bits without fine-tuning. WKVQuant (Yue
et al., 2024) presents an innovative approach for
quantizing large language models (LLMs) by in-
tegrating past-only quantization to refine atten-
tion computations, employing a two-dimensional
quantization strategy to manage the distributionof key/value (KV) caches effectively, and utilizing
cross-block reconstruction regularization for opti-
mizing parameters. This method enables the quan-
tization of both weights and KV caches, result-
ing in memory savings that rival those of weight-
activation quantization, while nearly matching the
performance levels of weight-only quantization.
4 Pruning
Pruning (LeCun et al., 1989) is a powerful tech-
nique to reduce the size or complexity of a model
by removing redundant components. Pruning can
be divided into Unstructured Pruning ,Semi-
Structured Pruning , and Structured Pruning .
Structured pruning removes entire components
like neurons, attention heads, or layers based on
specific rules while preserving the overall network
structure. On the other hand, unstructured prun-
ing prunes individual parameters, resulting in an
irregular sparse structure. Semi-structured prun-
ing is a method that lies between structured prun-
ing and unstructured pruning, capable of achiev-
ing fine-grained pruning and structural regulariza-
tion simultaneously. It prunes partial parameters
based on specific patterns rather than entire chan-
nels, filters, or neurons, making it a fine-grained
form of structured pruning. Table 2 shows the
performance of many representative LLM pruning
methods.
4.1 Unstructured Pruning
Unstructured pruning preserves the pruned
model’s performance, hence, works related to
unstructured pruning of LLMs often dispense with
retraining to restore performance. Nevertheless,
unstructured pruning renders the pruned model
irregular, necessitating specialized handling or
software optimizations for inference accelera-
tion. An innovative approach in this domain is
SparseGPT (Frantar and Alistarh, 2023), which
introduces a one-shot pruning strategy without
retraining. SparseGPT frames pruning as an
extensive sparse regression problem and solves
it using an approximate sparse regression solver.
SparseGPT achieves significant unstructured
sparsity, even up to over 50% on the largest GPT
models like OPT-175B and BLOOM-176B, with
minimal increase in perplexity. To reduce the
cost about the weight update process required by
SparseGPT, Wanda (Sun et al., 2024) achieves
model sparsity by pruning weights with the

--- PAGE 7 ---
Table 2: The performance of various representative LLM pruning methods.
Category†Methods LLMPerplexity Difference
(WikiText-2)‡ Compression Rate Speed up
Unstructured PruningSparseGPT OPT-175B -0.14 50% -
Wanda LLaMA-65B 1.01 50% -
SAMSP LLaMA2-13B 0.63 50% -
DSnoT LLaMA-65B 2.08e4 90% -
Structured PruningLLM-Pruner LLaMA-13B 3.6 20% -
Shortened LLaMA LLaMA-7B 10.5 35% -
FLAP LLaMA-65B 7.09 50% -
SliceGPT LLaMA2-70B 1.73 30% 1.87 ×
Semi-Structured PruningE-Sparse LLaMA-65B 2.13 2:4 1.53 ×
SparseGPT OPT-175B 0.39 2:4 2 ×
Wanda LLaMA-65B 2.69 2:4 1.24 ×
†: The results presented in the table are solely derived from the original papers.
‡: (The perplexity of the pruned LLM) - (The perplexity of the origin LLM).
smallest magnitudes multiplied by the norm of
the corresponding input activations, without the
need for retraining or weight updates. To further
minimize pruning-induced errors while upholding
the desired overall sparsity level, SAMSP (Shao
et al., 2024a) utilizes the Hessian matrix as a
metric for weight matrix sensitivity evaluation,
and dynamically adjusts sparsity allocation based
on sensitivity. Furthermore, DSnoT (Zhang et al.,
2024) minimizes the reconstruction error between
dense and sparse models through iterative weight
pruning-and-growing on top of sparse LLMs
to enhance LLM performance across various
sparsity rates, especially at high sparsity levels.
To provide hardware support for handling unstruc-
tured pruning on the GPU Tensor Core hardware,
Flash-LLM (Xia et al., 2023) introduces an
unstructured sparse matrix multiplication method,
which loads weight matrices in a sparse format
from global memory and reconstructs them in a
dense format within high-speed on-chip buffers
for computation using tensor cores.
4.2 Structured Pruning
Compared to unstructured pruning, structured
pruning offers the advantage of being hardware-
agnostic, allowing for accelerated inference on
traditional hardware post-pruning. However, the
removal of larger and potentially more criti-
cal components in structured pruning may re-
sult in performance degradation, typically re-
quiring efficient parameter fine-tuning for recov-
ery. We divide LLMs structured pruning works
into several groups based on pruning metrics:
Loss-based Pruning ,Magnitude-based Prun-
ing,Regularization-based Pruning .
Loss-based Pruning (Molchanov et al., 2019)assesses the significance of a pruning unit by mea-
suring its impact on loss or gradient informa-
tion (e.g., first-order or second-order derivatives of
loss). For example, LLM-Pruner (Ma et al., 2023)
introduces a one-shot structured pruning on LLMs
based on gradient information. Specifically, LLM-
Pruner identifies dependent structures via a de-
pendency detection algorithm and selects optimal
pruning groups using gradient information, rather
than solely relying on loss changes, in a task-
agnostic manner. Different from LLM-Pruner,
which focuses on narrowing LLMs’ width, Short-
ened LLaMA (Kim et al., 2024) introduces a one-
shot depth pruning on LLMs. Shortened LLaMA
chooses the Transformer block as the prunable
unit, and prunes these unimportant Transformer
blocks, where the importance of Transformer
blocks is evaluated by loss and its second-order
derivative. After Pruning, both LLM-Pruner and
Shortened LLaMA utilize LoRA to rapidly re-
cover the performance of the pruned model.
Magnitude-based Pruning (Han et al., 2015)
involves devising a heuristic metric based on the
magnitudes of pruning units, and use the met-
ric to assess the importance of pruning units,
subsequently pruning those units whose scores
fall below a predefined threshold. For exam-
ple, FLAP (An et al., 2024) utilizes a structured
fluctuation metric to assess and identify columns
in the weight matrix suitable for pruning, mea-
suring the variation of each input feature rela-
tive to a baseline value to estimate the impact
of removing a column of weights. Additionally,
FLAP uses an adaptive structure search to op-
timize global model compression, and restores
the model’s performance post-pruning through a

--- PAGE 8 ---
baseline bias compensation mechanism, avoiding
the need for fine-tuning. To further maintain the
pruned model’s performance, SliceGPT (Ashk-
boos et al., 2024) leverages the computational in-
variance of transformer networks and optimizes
the pruning process through Principal Component
Analysis (PCA). Specifically, SliceGPT employs
PCA as the pruning metric, applying it at each
layer of the transformer network to project the
signal matrix onto its principal components and
eliminate insignificant columns or rows from the
transformed weight matrices, ultimately aiming to
compress the model effectively.
Regularization-based Pruning (Wen et al.,
2016) typically adds a regularization term (e.g.,
L0,L1, and L2regularization) into the loss func-
tion to induce sparsity for LLMs. For example,
Sheared LLaMA (Xia et al., 2024) uses a pair
of Lagrange multipliers based on pruning masks
to impose constraints on the pruned model shape
directly, thereby formulating pruning as a con-
strained optimization problem. Through solv-
ing this optimization problem, Sheared LLaMA
derives optimal pruning masks. Additionally,
Sheared LLaMA introduces dynamic batch load-
ing, a strategy that adapts training data loading
based on each domain’s loss reduction rate, en-
hancing the efficiency of data utilization during
training.
Remark 2. Structured pruning typically reduces
model size by removing redundant parameters, but
it may degrade model performance. A novel ap-
proach is to combine knowledge distillation (Hin-
ton et al., 2015) with structured pruning. Knowl-
edge distillation allows knowledge extracted from
a LLM to be transferred to a smaller model, help-
ing the smaller model maintain its performance
while reducing its size.
4.3 Semi-Structured Pruning
Apart from unstructured pruning and structured
pruning, there are many works which use semi-
structured pruning to prune partial weights of
LLMs based on specific patterns. N:M spar-
sity, where every M contiguous elements leave
N non-zero elements, is an example of semi-
structured pruning. For example, E-Sparse (Li
et al., 2023b) implements N:M sparsity by intro-
ducing information entropy as a metric for evalu-
ating parameter importance to enhances the sig-
nificance of parameter weights and input fea-ture norms. E-Sparse incorporates global naive
shuffle and local block shuffle to efficiently op-
timize information distribution and mitigate the
impact of N:M sparsity on LLM accuracy. Fur-
thermore, many pruning works can also be gen-
eralized to semi-structured patterns. For exam-
ple, SparseGPT (Frantar and Alistarh, 2023) and
Wanda (Sun et al., 2024) also explore N:M spar-
sity of LLMs. SparseGPT (Frantar and Alis-
tarh, 2023) employs block-wise weight partition-
ing, with each block containing M weights. It
identifies and prunes N weights with the lowest re-
construction error(based on Hessian information),
ensuring a sparsity ratio of N:M. This process it-
eratively prunes and updates model weights, ad-
dressing one block at a time until the desired
sparsity level is achieved across the entire model.
Wanda (Sun et al., 2024) achieves structured N:M
pruning by dividing the weight matrix into groups
of M consecutive weights and computing an im-
portance score for each weight. The score is deter-
mined by the product of the weight’s magnitude
and the norm of the corresponding input activa-
tions. Within each weight group, the N weights
with the highest scores are retained, while the rest
are set to zero, thereby implementing structured
N:M pruning. Furthermore, choosing the optimal
pruning strategy is crucial for compatibility with
the target hardware. For instance, Choquette et al.
(2021) introduce the Ampere Tensor Core GPU
architecture (e.g., A100 GPUs) and propose 2:4
fine-grained semi-structured sparsity to accelerate
Sparse Neural Networks on this hardware. How-
ever, the current implementation of the Ampere
architecture supports only the 2:4 ratio, leaving
other ratios without acceleration.
Remark 3. LLMs often perform well on multiple
tasks, which means they contain a multitude of pa-
rameters for various tasks. Dynamic pruning (Xia
et al., 2020) methods can dynamically prune dif-
ferent parts of the model based on the current
task’s requirements to provide better performance
on specific tasks. This helps strike a balance be-
tween performance and efficiency.
Remark 4. For PTQ and pruning, preparing a
high-quality calibration dataset to assist in im-
proving the performance of compressed LLMs is
crucial. Specifically, Williams and Aletras (2023)
make a extensive empirical study on the effect of
calibration data upon model compression meth-
ods, and find that the performance of downstream

--- PAGE 9 ---
tasks can vary significantly depending on the cal-
ibration data selected. High-quality calibration
data can improve the performance and accuracy
of the compressed model, so careful selection and
preparation of calibration data are necessary.
5 Knowledge Distillation
Knowledge Distillation (KD) (Hinton et al., 2015)
is a technique aimed at transferring knowledge
from a large and complex model (i.e., teacher
model) to a smaller and simpler model (i.e., stu-
dent model). We classify these methods into two
clear categories (Gu et al., 2024): Black-box KD ,
where only the teacher’s outputs are accessible,
typically from closed-source LLMs, and White-
box KD , where the teacher’s parameters or out-
put distribution are available, usually from open-
source LLMs.
5.1 Black-box KD
Black-box KD usually prompts the teacher LLM
to generate a distillation dataset for fine-tune the
student LM, thereby transfering capabilities from
teacher LLM to the student LM. In Black-box KD,
teacher LLMs such as ChatGPT (gpt-3.5-turbo)
and GPT4 (OpenAI, 2024) are typically em-
ployed, while smaller LMs (SLMs), such as GPT-
2 (Radford et al., 2019), T5 (Raffel et al., 2020),
FlanT5 (Chung et al., 2024), and CodeT5 (Wang
et al., 2021), are commonly utilized as student
LMs. On the other hand, researchers find that
LLMs have emergent abilities, which refers to a
significant improvement in performance when the
model reaches a certain scale, showcasing surpris-
ing capabilities. Lots of Black-box KD methods
try to distill emergent abilities from LLMs to stu-
dent LMs, and we introduce three commonly used
emergent ability distillation methods: Chain-of-
Thought (CoT) Distillation, In-Context Learning
(ICL) Distillation, and Instruction Following (IF)
Distillation.
5.1.1 Chain-of-Thought Distillation
CoT (Wei et al., 2022; Wang et al., 2023c) prompts
LLMs to generate intermediate reasoning steps,
enabling them to tackle complex reasoning tasks
step by step. Li et al. (2024b) and Hsieh et al.
(2023) employ LLMs to prompt the generation
of explanations and leverage a multi-task learn-
ing framework to bolster the reasoning capabili-
ties of smaller models while enhancing their ca-
pacity for generating explanations. Magister et al.(2023) show that LLMs’ reasoning capability can
be transferred to SLMs via knowledge distilla-
tion, but there’s a trade-off between model and
dataset size in reasoning ability. Ho et al. (2023)
use zero-shot CoT techniques to prompt LLMs
to generate diverse rationales to enrich the dis-
tillation dataset for the student models. Shrid-
har et al. (2023) distill two student models: a
problem decomposer and a subproblem solver,
which the problem decomposer decomposes com-
plex problems into a sequence of subproblems,
and the subproblem solver solves these subprob-
lems step by step. Wang et al. (2023a) incorporate
contrastive decoding during rationale generation
for teacher models and address shortcut issues by
introducing a counterfactual reasoning objective
during student model training. Fu et al. (2023)
demonstrate that increasing task-specific capabili-
ties through distillation may inadvertently lead to
reduced performance in solving generalized prob-
lems, and focus on improving mathematical capa-
bility of student LMs via distillation. PaD (Zhu
et al., 2024) prompts LLMs to generate Program-
of-Thought (PoT) rationales instead of Chain-of-
Thought (CoT) rationales to construct distillation
dataset, and fine-tunes SLMs with the distillation
dataset. Wang et al. (2023f) establishes a multi-
round interactive learning paradigm that enables
student LMs to provide feedback to teacher LLMs
during the distillation process, thereby obtaining
tailored training data. Additionally, DRA intro-
duces a self-reflection learning mechanism, allow-
ing the student LMs to learn from their mistakes
and enhance their reasoning abilities. Li et al.
(2024c) finds that negative data generated from
teacher LMs also has reasoning knowledge, and
guides student LMs to learn knowledge from both
negative samples besides positive ones.
5.1.2 In-Context Learning Distillation
ICL (Dong et al., 2023; Wang et al., 2023b) em-
ploys structured prompts with task descriptions
and examples for LLMs to learn new tasks without
gradient updates. Huang et al. (2022) introduce
a method called in-context learning distillation,
which transfers in-context learning ability from
LLMs to smaller models by combining in-context
learning objectives with language modeling objec-
tives. Specifically, it trains the student model to
improve its generalization across various tasks by
imitating the soft label predictions of the teacher
model and the hard label ground truth values.

--- PAGE 10 ---
Additionally, the method incorporates two few-
shot learning paradigms: Meta In-context Tun-
ing (Meta-ICT) and Multitask In-context Tuning
(Multitask-ICT). In Meta-ICT, the student model
adapts to new tasks with in-context learning and
guidance from the teacher. Conversely, Multitask-
ICT treats all target tasks as training tasks, directly
using examples from them in distillation. The out-
comes show that Multitask-ICT is more effective,
despite its increased computational requirements.
AICD (Liu, 2024) leverages the autoregressive na-
ture of LLMs to perform meta-teacher forcing on
CoTs within the context, jointly optimizing the
likelihood of all in-context CoTs, thereby distill-
ing the capabilities of in-context learning and rea-
soning into smaller models.
5.1.3 Instruction Following Distillation
IF (Ouyang et al., 2022; Brooks et al., 2023) aims
to bolster the zero-shot ability of LLMs through
fine-tuning using a collection of instruction-like
prompt-response pairs. For instance, Lion (Jiang
et al., 2023) prompts the LLM to identify and
generate the "hard" instructions, which are then
utilized to enhance the student model’s capabil-
ities. LaMini-LM (Wu et al., 2024) develops
an extensive collection of 2.58 million instruc-
tions, comprising both existing and newly gen-
erated instructions, and fine-tunes a diverse ar-
ray of models by using these instructions. SELF-
INSTRUCT (Wang et al., 2023d) uses student
LMs themselves as teachers to generate instruc-
tion following dataset, and fine-tunes students
themselves with the dataset. Selective Reflection-
Tuning (Li et al., 2024a) leverages the teacher
LLMs to reflect on and improve existing data,
while the student LMs assess and selectively in-
corporate these improvements, thereby increasing
data quality and compatibility with the student
LMs.
Remark 5. Black-Box Distillation uses the
teacher model’s outputs as supervision, but the
teacher model’s outputs may not cover all possible
input scenarios. Thus, understanding how to han-
dle a student model’s generalization on unknown
data and how to increase data diversity is an area
that requires further investigation.
5.2 White-box KD
White-box KD enables the student LM to gain
a deeper understanding of the teacher LLM’s in-
ternal structure and knowledge representations,often resulting in higher-level performance im-
provements. An representative example is
MINILLM (Gu et al., 2024), which the first work
to study distillation from the Open-source gener-
ative LLMs. MINILLM use a reverse Kullback-
Leibler divergence objective, which is more suit-
able for KD on generative language models, to
prevent the student model from overestimating the
low-probability regions of the teacher distribution,
and derives an effective optimization approach to
learn the objective. Further, GKD (Agarwal et al.,
2024) explores distillation from auto-regressive
models, where generative language models are
a subset. GKD trains the student using self-
generated outputs, incorporating teacher feedback,
and allows flexibility in using different loss func-
tions when the student cannot fully replicate the
teacher’s distribution. Different from the above
works, which focus on learning the teacher distri-
bution, TED (Liang et al., 2023) proposes a task-
aware layer-wise distillation method, which de-
signs task-aware filters, which align the hidden
representations of the teacher and student models
at each intermediate layer, to reduce the knowl-
edge gap between the student and teacher models.
Remark 6. Although white-box distillation allows
student LMs to learn the knowledge of teacher
LLMs more deeply compared to black-box distilla-
tion, currently, open-source LLMs perform worse
than closed-source ones, limiting the improvement
of student LMs performance in white-box distilla-
tion. This is one of the barren factors hindering
the development of white-box distillation. A fea-
sible solution is to distill knowledge from closed-
source LLMs to open-source LLMs through black-
box distillation, and then use white-box distilla-
tion to transfer knowledge from open-source LLMs
to student LLMs.
Remark 7. White-box distillation often involves
understanding and utilizing the internal structure
of LLMs, such as layer connections and parameter
settings. A more in-depth exploration of different
network structures and interactions between lay-
ers can improve the effectiveness of white-box dis-
tillation.
6 Low-Rank Factorization
Low-Rank Factorization (Srebro and Jaakkola,
2003) reduces a large matrix into smaller ones to
save space and computational effort. For example,
it decomposes a large matrix Winto two small

--- PAGE 11 ---
matrices UandV(i.e.,W≈UV), where U
ism×kandVisk×n, with kmuch smaller
thanmandn. Recent works try to employ low-
rank factorization to compress LLMs and achieve
significant success in this regard. For example,
LPLR (Saha et al., 2023) compresses weight ma-
trices of LLMs through randomized low-rank and
low-precision factorization. Specifically, LPLR
approximates the column space of the matrix us-
ing random sketching techniques, quantizes these
columns, and then projects the original columns
onto this quantized space to create two low-rank
factors stored in low-precision. ASVD (Yuan
et al., 2023b) finds that the activation distribution
has an effect on the compression performance. To
sovle the problem, ASVD proposes to scale the
weight matrix with a diagonal matrix that contains
scaling factors corresponding to the activation dis-
tribution of the input feature channels. Moreover,
ASVD assigns the most suitable compression ratio
to different layers by analyzing the singular val-
ues distribution in each layer’s weight matrix, en-
suring minimal loss of model performance during
the compression process. Furthermore, Sharma
et al. (2024) demonstrates that the performance of
LLMs can be significantly improved by applying
Layer-Selective Rank Reduction (LASER) to spe-
cific layers of Transformer models. LASER in-
volves selectively reducing the rank higher-order
components of weight matrices, which is shown
to improve the model’s handling of rare training
data and its resistance to question paraphrasing.
7 Challenges and Future Directions
7.1 More Advanced Methods
The research on model compression techniques
for LLMs is still in its early stages. These com-
pressed LLMs, as demonstrated in prior studies
(Frantar and Alistarh, 2023; Liu et al., 2023b;
Ho et al., 2023), continue to exhibit a significant
performance gap when compared to their uncom-
pressed counterparts. By delving into more ad-
vanced model compression methods tailored for
LLMs, we have the potential to enhance the per-
formance of these uncompressed LLMs.
7.2 Scaling up Model Compression Methods
from Other Models
In our paper, we introduce several representative
model compression methods for LLMs. However,
many classic model compression methods remainprevalent in traditional small models. For exam-
ple, lottery tickets (Frankle and Carbin, 2019) and
parameter sharing (Savarese and Maire, 2019) are
widely used model compression methods in small
models. These methods still hold significant po-
tential in the era of LLMs. Future work should fo-
cus on exploring how to extend these compression
methods to LLMs to achieve further compression.
7.3 LLM Inference and Deployment
The efficiency of compressed LLMs during de-
ployment is also a significant area for explo-
ration. This involves multiple evaluation metrics,
including arithmetic intensity, memory size, and
throughput. Furthermore, we can use an analytical
tool, the Roofline Model (Williams et al., 2009),
to assess the resource efficiency of compressed
LLMs on specific hardware. Evaluating the de-
ployment efficiency of compressed LLMs on spe-
cific hardware can guide researchers in selecting
and analyzing the advantages and disadvantages
of various model compression methods and fur-
ther optimizing these methods.
7.4 The Effect of Scaling Law
The scaling law (Kaplan et al., 2020) underscores
the significant impact of model size, dataset size,
and compute resources on the performance of
LLMs. However, the scaling law presents a funda-
mental challenge for LLM compression, i.e., there
is a trade-off between model size and performance
in compressed LLMs. Delving into the mecha-
nisms and theories underpinning the scaling law is
crucial for elucidating and potentially overcoming
this limitation.
7.5 AutoML for LLM Compression
Existing compression techniques have made re-
markable progress, but they still heavily depend
on manual design. For instance, designing ap-
propriate student architectures for knowledge dis-
tillation requires a significant amount of human
effort. To reduce this reliance on manual de-
sign, a feasible solution is to combine Automated
Machine Learning (AutoML) techniques such as
Meta-Learning (Finn et al., 2017) and Neural Ar-
chitecture Search (NAS) (Zoph and Le, 2017) with
model compression. By combining with AutoML
techniques, model compression can automatically
select appropriate hyperparameters and tailor ar-
chitectures and scales of compressed models, thus
minimizing human involvement and lowering the

--- PAGE 12 ---
associated costs. Furthermore, AutoML can iden-
tify optimal model compression strategies tailored
to specific task requirements, thereby further en-
hancing compression rates without compromising
model performance.
7.6 Explainability of LLM Compression
Earlier research (Stanton et al., 2021; Xu et al.,
2021) has raised significant concerns regarding the
explainability of model compression techniques
applied to Pre-trained Language Models (PLMs).
Notably, these same challenges extend to LLM
compression methods as well. For example, CoT-
distillation can enhance SLMs’ reasoning perfor-
mance, yet the mechanism through which it im-
parts CoT ability remains unclear. This challenge
underscores the importance of integrating explain-
ability with model compression approaches for the
advancement of LLM compression applications.
Explainability not only clarifies the changes and
trade-offs in the compression process but also en-
hances efficiency and accuracy. Additionally, in-
terpretability aids in evaluating the compressed
model’s performance to ensure it aligns with prac-
tical requirements.
8 Conclusion
In the survey, we have explored model compres-
sion techniques for LLMs. Our coverage spanned
compression methods, metrics, and benchmark
datasets. By diving into LLM compression, we’ve
highlighted its challenges and opportunities. This
survey aims to be a valuable reference, providing
insights into the current landscape and promoting
ongoing exploration of this pivotal topic.
Acknowledgments
We would like to thank the anonymous reviewers
and the Action Editor for their valuable feedback
and discussions. The work of Jian Li is supported
partially by National Natural Science Foundation
of China (No. 62106257). The work of Yong
Liu is supported partially by National Natural Sci-
ence Foundation of China (No. 62076234), Bei-
jing Outstanding Young Scientist Program (No.
BJJWZYJH012019100020098), the Unicom In-
novation Ecological Cooperation Plan, and the
CCF-Huawei Populus Grove Fund.References
Rishabh Agarwal, Nino Vieillard, Yongchao
Zhou, Piotr Stanczyk, Sabela Ramos Garea,
Matthieu Geist, and Olivier Bachem. 2024.
Generalized knowledge distillation for auto-
regressive language models. In The Twelfth In-
ternational Conference on Learning Represen-
tations .
Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jin-
qiao Wang. 2024. Fluctuation-based adaptive
structured pruning for large language models.
InThirty-Eighth AAAI Conference on Artificial
Intelligence, AAAI 2024, Thirty-Sixth Confer-
ence on Innovative Applications of Artificial In-
telligence, IAAI 2024, Fourteenth Symposium
on Educational Advances in Artificial Intelli-
gence, EAAI 2014, February 20-27, 2024, Van-
couver, Canada , pages 10865–10873. AAAI
Press.
Saleh Ashkboos, Maximilian L. Croci,
Marcelo Gennari do Nascimento, Torsten
Hoefler, and James Hensman. 2024. SliceGPT:
Compress large language models by deleting
rows and columns. In The Twelfth International
Conference on Learning Representations .
Tim Brooks, Aleksander Holynski, and Alexei A.
Efros. 2023. Instructpix2pix: Learning to fol-
low image editing instructions. In IEEE/CVF
Conference on Computer Vision and Pattern
Recognition, CVPR 2023, Vancouver, BC,
Canada, June 17-24, 2023 , pages 18392–
18402. IEEE.
Tom B. Brown, Benjamin Mann, Nick Ry-
der, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam,
Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-V oss, Gretchen Krueger,
Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Win-
ter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot
learners. In Advances in Neural Information
Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual .

--- PAGE 13 ---
Jerry Chee, Yaohui Cai, V olodymyr Kuleshov, and
Christopher De Sa. 2023. QuIP: 2-bit quantiza-
tion of large language models with guarantees.
InThirty-seventh Conference on Neural Infor-
mation Processing Systems .
Jack Choquette, Wishwesh Gandhi, Olivier
Giroux, Nick Stam, and Ronny Krashinsky.
2021. NVIDIA A100 tensor core GPU: perfor-
mance and innovation. IEEE Micro , 41(2):29–
35.
Hyung Won Chung, Le Hou, Shayne Longpre,
Barret Zoph, Yi Tay, William Fedus, Yunx-
uan Li, Xuezhi Wang, Mostafa Dehghani,
Siddhartha Brahma, Albert Webson, Shixi-
ang Shane Gu, Zhuyun Dai, Mirac Suzgun,
Xinyun Chen, Aakanksha Chowdhery, Alex
Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra,
Adams Yu, Vincent Zhao, Yanping Huang, An-
drew Dai, Hongkun Yu, Slav Petrov, Ed H.
Chi, Jeff Dean, Jacob Devlin, Adam Roberts,
Denny Zhou, Quoc V . Le, and Jason Wei.
2024. Scaling instruction-finetuned language
models. Journal of Machine Learning Re-
search , 25(70):1–53.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
ian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton,
Reiichiro Nakano, Christopher Hesse, and John
Schulman. 2021. Training verifiers to solve
math word problems. CoRR , abs/2110.14168.
Tim Dettmers, Mike Lewis, Younes Belkada, and
Luke Zettlemoyer. 2022. Gpt3.int8(): 8-bit
matrix multiplication for transformers at scale.
InAdvances in Neural Information Processing
Systems 35: Annual Conference on Neural In-
formation Processing Systems 2022, NeurIPS
2022, New Orleans, LA, USA, November 28 -
December 9, 2022 .
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman,
and Luke Zettlemoyer. 2023. Qlora: Efficient
finetuning of quantized llms. In Advances in
Neural Information Processing Systems 36: An-
nual Conference on Neural Information Pro-
cessing Systems 2023, NeurIPS 2023, New Or-
leans, LA, USA, December 10 - 16, 2023 .
Tim Dettmers, Ruslan A. Svirschevski, Vage
Egiazarian, Denis Kuznedelev, Elias Frantar,Saleh Ashkboos, Alexander Borzunov, Torsten
Hoefler, and Dan Alistarh. 2024. SpQR:
A sparse-quantized representation for near-
lossless LLM weight compression. In The
Twelfth International Conference on Learning
Representations .
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng,
Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing
Xu, Lei Li, and Zhifang Sui. 2023. A survey for
in-context learning. CoRR , abs/2301.00234.
Dayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo,
Ting Cao, Xiaowen Chu, and Ningyi Xu.
2024. Bitdistiller: Unleashing the potential
of sub-4-bit llms via self-distillation. CoRR ,
abs/2402.10631.
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
2017. Model-agnostic meta-learning for fast
adaptation of deep networks. In Proceedings of
the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia,
6-11 August 2017 , volume 70 of Proceedings of
Machine Learning Research , pages 1126–1135.
PMLR.
Jonathan Frankle and Michael Carbin. 2019. The
lottery ticket hypothesis: Finding sparse, train-
able neural networks. In 7th International
Conference on Learning Representations, ICLR
2019, New Orleans, LA, USA, May 6-9, 2019 .
OpenReview.net.
Elias Frantar and Dan Alistarh. 2022. Optimal
brain compression: A framework for accurate
post-training quantization and pruning. In Ad-
vances in Neural Information Processing Sys-
tems.
Elias Frantar and Dan Alistarh. 2023. Sparsegpt:
Massive language models can be accurately
pruned in one-shot. In International Confer-
ence on Machine Learning, ICML 2023, 23-29
July 2023, Honolulu, Hawaii, USA , volume 202
ofProceedings of Machine Learning Research ,
pages 10323–10337. PMLR.
Elias Frantar, Saleh Ashkboos, Torsten Hoefler,
and Dan Alistarh. 2023. OPTQ: Accurate quan-
tization for generative pre-trained transformers.
InThe Eleventh International Conference on
Learning Representations .

--- PAGE 14 ---
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal,
and Tushar Khot. 2023. Specializing smaller
language models towards multi-step reasoning.
InProceedings of the 40th International Con-
ference on Machine Learning , volume 202 of
Proceedings of Machine Learning Research ,
pages 10421–10430. PMLR.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Bi-
derman, Sid Black, Anthony DiPofi, Charles
Foster, Laurence Golding, Jeffrey Hsu, Alain
Le Noac’h, Haonan Li, Kyle McDonell, Niklas
Muennighoff, Chris Ociepa, Jason Phang, Laria
Reynolds, Hailey Schoelkopf, Aviya Skowron,
Lintang Sutawika, Eric Tang, Anish Thite, Ben
Wang, Kevin Wang, and Andy Zou. 2023. A
framework for few-shot language model evalu-
ation.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar
Khot, Dan Roth, and Jonathan Berant. 2021.
Did aristotle use a laptop? A question answer-
ing benchmark with implicit reasoning strate-
gies. Trans. Assoc. Comput. Linguistics , 9:346–
361.
R.M. Gray and D.L. Neuhoff. 1998. Quantiza-
tion. IEEE Transactions on Information The-
ory, 44(6):2325–2383.
Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang.
2024. MiniLLM: Knowledge distillation of
large language models. In The Twelfth Interna-
tional Conference on Learning Representations .
Cong Guo, Jiaming Tang, Weiming Hu, Jing-
wen Leng, Chen Zhang, Fan Yang, Yunxin Liu,
Minyi Guo, and Yuhao Zhu. 2023. Olive: Ac-
celerating large language models via hardware-
friendly outlier-victim pair quantization. In
Proceedings of the 50th Annual International
Symposium on Computer Architecture, ISCA
2023, Orlando, FL, USA, June 17-21, 2023 ,
pages 3:1–3:15. ACM.
Song Han, Huizi Mao, and William J. Dally. 2016.
Deep compression: Compressing deep neural
network with pruning, trained quantization and
huffman coding. In 4th International Confer-
ence on Learning Representations, ICLR 2016,
San Juan, Puerto Rico, May 2-4, 2016, Confer-
ence Track Proceedings .Song Han, Jeff Pool, John Tran, and William
Dally. 2015. Learning both weights and con-
nections for efficient neural network. In Ad-
vances in Neural Information Processing Sys-
tems, volume 28. Curran Associates, Inc.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey
Dean. 2015. Distilling the knowledge in a neu-
ral network. CoRR , abs/1503.02531.
Namgyu Ho, Laura Schmid, and Se-Young Yun.
2023. Large language models are reasoning
teachers. In Proceedings of the 61st Annual
Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL
2023, Toronto, Canada, July 9-14, 2023 , pages
14852–14882. Association for Computational
Linguistics.
Coleman Hooper, Sehoon Kim, Hiva Moham-
madzadeh, Michael W. Mahoney, Yakun Sophia
Shao, Kurt Keutzer, and Amir Gholami. 2024.
Kvquant: Towards 10 million context length
LLM inference with KV cache quantization.
CoRR , abs/2401.18079.
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,
Hootan Nakhost, Yasuhisa Fujii, Alex Ratner,
Ranjay Krishna, Chen-Yu Lee, and Tomas Pfis-
ter. 2023. Distilling step-by-step! outperform-
ing larger language models with less training
data and smaller model sizes. In Findings of
the Association for Computational Linguistics:
ACL 2023, Toronto, Canada, July 9-14, 2023 ,
pages 8003–8017. Association for Computa-
tional Linguistics.
Yukun Huang, Yanda Chen, Zhou Yu, and Kath-
leen R. McKeown. 2022. In-context learn-
ing distillation: Transferring few-shot learning
ability of pre-trained language models. CoRR ,
abs/2212.10670.
Hyesung Jeon, Yulhwa Kim, and Jae-Joon Kim.
2024. L4Q: parameter efficient quantization-
aware training on large language models via
lora-wise LSQ. CoRR , abs/2402.04902.
Yuxin Jiang, Chunkit Chan, Mingyang Chen, and
Wei Wang. 2023. Lion: Adversarial distilla-
tion of proprietary large language models. In
Proceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing ,
pages 3134–3154, Singapore. Association for
Computational Linguistics.

--- PAGE 15 ---
Jared Kaplan, Sam McCandlish, Tom Henighan,
Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and
Dario Amodei. 2020. Scaling laws for neural
language models. CoRR , abs/2001.08361.
Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim,
Thibault Castells, Shinkook Choi, Junho Shin,
and Hyoung-Kyu Song. 2024. Shortened llama:
A simple depth pruning for large language mod-
els.ICLR Workshop on Mathematical and Em-
pirical Understanding of Foundation Models
(ME-FoMo) .
Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim,
Joonsuk Park, Kang Min Yoo, Se Jung Kwon,
and Dongsoo Lee. 2023a. Memory-efficient
fine-tuning of compressed large language mod-
els via sub-4-bit integer quantization. In Thirty-
seventh Conference on Neural Information Pro-
cessing Systems .
Sehoon Kim, Coleman Hooper, Amir Gho-
lami, Zhen Dong, Xiuyu Li, Sheng Shen,
Michael W. Mahoney, and Kurt Keutzer. 2023b.
Squeezellm: Dense-and-sparse quantization.
CoRR , abs/2306.07629.
Yann LeCun, John S. Denker, and Sara A. Solla.
1989. Optimal brain damage. In Advances
in Neural Information Processing Systems 2,
[NIPS Conference, Denver, Colorado, USA,
November 27-30, 1989] , pages 598–605. Mor-
gan Kaufmann.
Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun
Kim, and Eunhyeok Park. 2024. OWQ: outlier-
aware weight quantization for efficient fine-
tuning and inference of large language models.
InThirty-Eighth AAAI Conference on Artificial
Intelligence, AAAI 2024, Thirty-Sixth Confer-
ence on Innovative Applications of Artificial In-
telligence, IAAI 2024, Fourteenth Symposium
on Educational Advances in Artificial Intelli-
gence, EAAI 2014, February 20-27, 2024, Van-
couver, Canada , pages 13355–13364. AAAI
Press.
Ming Li, Lichang Chen, Jiuhai Chen, Shwai He,
Jiuxiang Gu, and Tianyi Zhou. 2024a. Se-
lective reflection-tuning: Student-selected data
recycling for LLM instruction-tuning. CoRR ,
abs/2402.10110.Shiyang Li, Jianshu Chen, yelong shen, Zhiyu
Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing
Qian, Baolin Peng, Yi Mao, Wenhu Chen, and
Xifeng Yan. 2024b. Explanations from large
language models make small reasoners better.
In2nd Workshop on Sustainable AI .
Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan
Pan, Bin Sun, Xinglin Wang, Heda Wang,
and Kan Li. 2024c. Turning dust into gold:
Distilling complex reasoning capabilities from
llms by leveraging negative data. In Thirty-
Eighth AAAI Conference on Artificial Intel-
ligence, AAAI 2024, Thirty-Sixth Conference
on Innovative Applications of Artificial Intel-
ligence, IAAI 2024, Fourteenth Symposium on
Educational Advances in Artificial Intelligence,
EAAI 2014, February 20-27, 2024, Vancouver,
Canada , pages 18591–18599. AAAI Press.
Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He,
Nikos Karampatziakis, Weizhu Chen, and Tuo
Zhao. 2023a. Loftq: Lora-fine-tuning-aware
quantization for large language models. CoRR ,
abs/2310.08659.
Yun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen
Zhu, and Zhanhui Kang. 2023b. E-sparse:
Boosting the large language model inference
through entropy-based N: M sparsity. CoRR ,
abs/2310.15929.
Zhuo Li, Hengyi Li, and Lin Meng. 2023c. Model
compression for deep neural networks: A sur-
vey. Comput. , 12(3):60.
Chen Liang, Simiao Zuo, Qingru Zhang,
Pengcheng He, Weizhu Chen, and Tuo Zhao.
2023. Less is more: Task-aware layer-wise
distillation for language model compression.
InInternational Conference on Machine
Learning, ICML 2023, 23-29 July 2023,
Honolulu, Hawaii, USA , volume 202 of Pro-
ceedings of Machine Learning Research , pages
20852–20867. PMLR.
Ji Lin, Jiaming Tang, Haotian Tang, Shang
Yang, Xingyu Dang, and Song Han. 2023.
AWQ: activation-aware weight quantization for
LLM compression and acceleration. CoRR ,
abs/2306.00978.
Shih-yang Liu, Zechun Liu, Xijie Huang,
Pingcheng Dong, and Kwang-Ting Cheng.

--- PAGE 16 ---
2023a. LLM-FP4: 4-bit floating-point quan-
tized transformers. In Proceedings of the 2023
Conference on Empirical Methods in Natural
Language Processing , pages 592–605, Singa-
pore. Association for Computational Linguis-
tics.
Yuxuan Liu. 2024. Learning to reason with au-
toregressive in-context distillation. In The Sec-
ond Tiny Papers Track at ICLR 2024 .
Zechun Liu, Barlas Oguz, Changsheng Zhao,
Ernie Chang, Pierre Stock, Yashar Mehdad,
Yangyang Shi, Raghuraman Krishnamoorthi,
and Vikas Chandra. 2023b. LLM-QAT: data-
free quantization aware training for large lan-
guage models. CoRR , abs/2305.17888.
Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen
Zhong, Zhaozhuo Xu, Vladimir Braverman,
Beidi Chen, and Xia Hu. 2024. KIVI: A tuning-
free asymmetric 2bit quantization for KV cache.
CoRR , abs/2402.02750.
Xinyin Ma, Gongfan Fang, and Xinchao Wang.
2023. LLM-pruner: On the structural prun-
ing of large language models. In Thirty-seventh
Conference on Neural Information Processing
Systems .
Lucie Charlotte Magister, Jonathan Mallinson,
Jakub Adámek, Eric Malmi, and Aliaksei Sev-
eryn. 2023. Teaching small language mod-
els to reason. In Proceedings of the 61st An-
nual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers),
ACL 2023, Toronto, Canada, July 9-14, 2023 ,
pages 1773–1781. Association for Computa-
tional Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics ,
19(2):313–330.
Stephen Merity, Caiming Xiong, James Bradbury,
and Richard Socher. 2017. Pointer sentinel
mixture models. In 5th International Confer-
ence on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference
Track Proceedings . OpenReview.net.
Todor Mihaylov, Peter Clark, Tushar Khot, and
Ashish Sabharwal. 2018. Can a suit of armorconduct electricity? A new dataset for open
book question answering. In Proceedings of the
2018 Conference on Empirical Methods in Nat-
ural Language Processing, Brussels, Belgium,
October 31 - November 4, 2018 , pages 2381–
2391. Association for Computational Linguis-
tics.
Pavlo Molchanov, Arun Mallya, Stephen Tyree,
Iuri Frosio, and Jan Kautz. 2019. Importance
estimation for neural network pruning. In IEEE
Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA,
USA, June 16-20, 2019 , pages 11264–11272.
Computer Vision Foundation / IEEE.
OpenAI. 2024. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo
Almeida, Carroll L. Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal,
Katarina Slama, Alex Ray, John Schulman, Ja-
cob Hilton, Fraser Kelton, Luke Miller, Maddie
Simens, Amanda Askell, Peter Welinder, Paul F.
Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions
with human feedback. In NeurIPS .
Denis Paperno, Germán Kruszewski, Ange-
liki Lazaridou, Quan Ngoc Pham, Raffaella
Bernardi, Sandro Pezzelle, Marco Baroni,
Gemma Boleda, and Raquel Fernández. 2016.
The LAMBADA dataset: Word prediction re-
quiring a broad discourse context. In Proceed-
ings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2016,
August 7-12, 2016, Berlin, Germany, Volume
1: Long Papers . The Association for Computer
Linguistics.
Gunho Park, Baeseong park, Minsub Kim, Sung-
jae Lee, Jeonghoon Kim, Beomseok Kwon,
Se Jung Kwon, Byeongwook Kim, Youngjoo
Lee, and Dongsoo Lee. 2024. LUT-GEMM:
Quantized matrix multiplication based on LUTs
for efficient inference in large-scale generative
language models. In The Twelfth International
Conference on Learning Representations .
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Lan-
guage models are unsupervised multitask learn-
ers.OpenAI blog , 1(8):9.

--- PAGE 17 ---
Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.
2020. Exploring the limits of transfer learning
with a unified text-to-text transformer. J. Mach.
Learn. Res. , 21:140:1–140:67.
Mohammad Rastegari, Vicente Ordonez, Joseph
Redmon, and Ali Farhadi. 2016. Xnor-net:
Imagenet classification using binary convolu-
tional neural networks. In Computer Vision -
ECCV 2016 - 14th European Conference, Ams-
terdam, The Netherlands, October 11-14, 2016,
Proceedings, Part IV , volume 9908 of Lecture
Notes in Computer Science , pages 525–542.
Springer.
Anna Rogers, Olga Kovaleva, and Anna
Rumshisky. 2020. A primer in BERTol-
ogy: What we know about how BERT works.
Transactions of the Association for Computa-
tional Linguistics , 8:842–866.
Rajarshi Saha, Varun Srivastava, and Mert Pi-
lanci. 2023. Matrix compression via random-
ized low rank and low precision factorization.
InAdvances in Neural Information Processing
Systems 36: Annual Conference on Neural In-
formation Processing Systems 2023, NeurIPS
2023, New Orleans, LA, USA, December 10 -
16, 2023 .
Pedro Savarese and Michael Maire. 2019. Learn-
ing implicitly recurrent cnns through parame-
ter sharing. In 7th International Conference
on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019 . OpenRe-
view.net.
Teven Le Scao, Angela Fan, Christopher Akiki,
Ellie Pavlick, Suzana Ilic, Daniel Hesslow,
Roman Castagné, Alexandra Sasha Luccioni,
François Yvon, Matthias Gallé, Jonathan Tow,
Alexander M. Rush, Stella Biderman, Al-
bert Webson, Pawan Sasanka Ammanamanchi,
Thomas Wang, Benoît Sagot, Niklas Muen-
nighoff, Albert Villanova del Moral, Olatunji
Ruwase, Rachel Bawden, Stas Bekman, An-
gelina McMillan-Major, Iz Beltagy, Huu
Nguyen, Lucile Saulnier, Samson Tan, Pe-
dro Ortiz Suarez, Victor Sanh, Hugo Lau-
rençon, Yacine Jernite, Julien Launay, Mar-
garet Mitchell, Colin Raffel, Aaron Gokaslan,Adi Simhi, Aitor Soroa, Alham Fikri Aji,
Amit Alfassy, Anna Rogers, Ariel Kreisberg
Nitzav, Canwen Xu, Chenghao Mou, Chris
Emezue, Christopher Klamm, Colin Leong,
Daniel van Strien, David Ifeoluwa Adelani, and
et al. 2022. BLOOM: A 176b-parameter open-
access multilingual language model. CoRR ,
abs/2211.05100.
Hang Shao, Bei Liu, and Yanmin Qian. 2024a.
One-shot sensitivity-aware mixed sparsity prun-
ing for large language models. In ICASSP
2024 - 2024 IEEE International Conference
on Acoustics, Speech and Signal Processing
(ICASSP) , pages 11296–11300.
Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang,
Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng
Zhang, Peng Gao, Yu Qiao, and Ping Luo.
2024b. Omniquant: Omnidirectionally cal-
ibrated quantization for large language mod-
els. In The Twelfth International Conference on
Learning Representations .
Pratyusha Sharma, Jordan T. Ash, and Dipendra
Misra. 2024. The truth is in there: Improv-
ing reasoning with layer-selective rank reduc-
tion. In The Twelfth International Conference
on Learning Representations .
Kumar Shridhar, Alessandro Stolfo, and Mrin-
maya Sachan. 2023. Distilling reasoning capa-
bilities into smaller language models. In Find-
ings of the Association for Computational Lin-
guistics: ACL 2023, Toronto, Canada, July 9-
14, 2023 , pages 7059–7073. Association for
Computational Linguistics.
Nathan Srebro and Tommi S. Jaakkola. 2003.
Weighted low-rank approximations. In Ma-
chine Learning, Proceedings of the Twentieth
International Conference (ICML 2003), August
21-24, 2003, Washington, DC, USA , pages 720–
727. AAAI Press.
Aarohi Srivastava, Abhinav Rastogi, Abhishek
Rao, Abu Awal Md Shoeb, Abubakar Abid,
Adam Fisch, Adam R. Brown, Adam San-
toro, Aditya Gupta, Adrià Garriga-Alonso,
Agnieszka Kluska, Aitor Lewkowycz, Ak-
shat Agarwal, Alethea Power, Alex Ray,
Alex Warstadt, Alexander W. Kocurek, Ali
Safaya, Ali Tazarv, Alice Xiang, Alicia Par-
rish, Allen Nie, Aman Hussain, Amanda Askell,

--- PAGE 18 ---
Amanda Dsouza, Ambrose Slone, Ameet Ra-
hane, Anantharaman S. Iyer, Anders Johan An-
dreassen, Andrea Madotto, Andrea Santilli, An-
dreas Stuhlmüller, Andrew M. Dai, Andrew
La, Andrew Lampinen, Andy Zou, Angela
Jiang, Angelica Chen, Anh Vuong, Animesh
Gupta, Anna Gottardi, Antonio Norelli, Anu
Venkatesh, Arash Gholamidavoodi, Arfa Tabas-
sum, Arul Menezes, Arun Kirubarajan, Asher
Mullokandov, Ashish Sabharwal, Austin Her-
rick, Avia Efrat, Aykut Erdem, Ayla Karaka¸ s,
B. Ryan Roberts, Bao Sheng Loe, Barret
Zoph, Bartłomiej Bojanowski, Batuhan Özyurt,
Behnam Hedayatnia, Behnam Neyshabur, Ben-
jamin Inden, Benno Stein, Berk Ekmekci,
Bill Yuchen Lin, Blake Howald, Bryan Orin-
ion, Cameron Diao, Cameron Dour, Cather-
ine Stinson, Cedrick Argueta, Cesar Ferri,
Chandan Singh, Charles Rathkopf, Chenlin
Meng, Chitta Baral, Chiyu Wu, Chris Callison-
Burch, Christopher Waites, Christian V oigt,
Christopher D Manning, Christopher Potts,
Cindy Ramirez, Clara E. Rivera, Clemen-
cia Siro, Colin Raffel, Courtney Ashcraft,
Cristina Garbacea, Damien Sileo, Dan Gar-
rette, Dan Hendrycks, Dan Kilman, Dan Roth,
C. Daniel Freeman, Daniel Khashabi, Daniel
Levy, Daniel Moseguí González, Danielle Per-
szyk, Danny Hernandez, Danqi Chen, Daphne
Ippolito, Dar Gilboa, David Dohan, David
Drakard, David Jurgens, Debajyoti Datta, Deep
Ganguli, Denis Emelin, Denis Kleyko, Deniz
Yuret, Derek Chen, Derek Tam, Dieuwke
Hupkes, Diganta Misra, Dilyar Buzan, Dim-
itri Coelho Mollo, Diyi Yang, Dong-Ho Lee,
Dylan Schrader, Ekaterina Shutova, Ekin Do-
gus Cubuk, Elad Segal, Eleanor Hagerman,
Elizabeth Barnes, Elizabeth Donoway, Ellie
Pavlick, Emanuele Rodolà, Emma Lam, Eric
Chu, Eric Tang, Erkut Erdem, Ernie Chang,
Ethan A Chi, Ethan Dyer, Ethan Jerzak,
Ethan Kim, Eunice Engefu Manyasi, Evgenii
Zheltonozhskii, Fanyue Xia, Fatemeh Siar,
Fernando Martínez-Plumed, Francesca Happé,
Francois Chollet, Frieda Rong, Gaurav Mishra,
Genta Indra Winata, Gerard de Melo, Ger-
mán Kruszewski, Giambattista Parascandolo,
Giorgio Mariani, Gloria Xinyue Wang, Gon-
zalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-
Ari, Hana Galijasevic, Hannah Kim, Hannah
Rashkin, Hannaneh Hajishirzi, Harsh Mehta,Hayden Bogar, Henry Francis Anthony Shevlin,
Hinrich Schuetze, Hiromu Yakura, Hongming
Zhang, Hugh Mee Wong, Ian Ng, Isaac No-
ble, Jaap Jumelet, Jack Geissinger, Jackson
Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fer-
nández Fisac, James B Simon, James Kop-
pel, James Zheng, James Zou, Jan Kocon,
Jana Thompson, Janelle Wingfield, Jared Ka-
plan, Jarema Radom, Jascha Sohl-Dickstein, Ja-
son Phang, Jason Wei, Jason Yosinski, Jekate-
rina Novikova, Jelle Bosscher, Jennifer Marsh,
Jeremy Kim, Jeroen Taal, Jesse Engel, Jesu-
joba Alabi, Jiacheng Xu, Jiaming Song, Jillian
Tang, Joan Waweru, John Burden, John Miller,
John U. Balis, Jonathan Batchelder, Jonathan
Berant, Jörg Frohberg, Jos Rozen, Jose
Hernandez-Orallo, Joseph Boudeman, Joseph
Guerr, Joseph Jones, Joshua B. Tenenbaum,
Joshua S. Rule, Joyce Chua, Kamil Kanclerz,
Karen Livescu, Karl Krauth, Karthik Gopalakr-
ishnan, Katerina Ignatyeva, Katja Mark-
ert, Kaustubh Dhole, Kevin Gimpel, Kevin
Omondi, Kory Wallace Mathewson, Kristen
Chiafullo, Ksenia Shkaruta, Kumar Shrid-
har, Kyle McDonell, Kyle Richardson, Laria
Reynolds, Leo Gao, Li Zhang, Liam Dugan,
Lianhui Qin, Lidia Contreras-Ochando, Louis-
Philippe Morency, Luca Moschella, Lucas Lam,
Lucy Noble, Ludwig Schmidt, Luheng He, Luis
Oliveros-Colón, Luke Metz, Lütfi Kerem Senel,
Maarten Bosma, Maarten Sap, Maartje Ter Ho-
eve, Maheen Farooqi, Manaal Faruqui, Man-
tas Mazeika, Marco Baturan, Marco Marelli,
Marco Maru, Maria Jose Ramirez-Quintana,
Marie Tolkiehn, Mario Giulianelli, Martha
Lewis, Martin Potthast, Matthew L Leavitt,
Matthias Hagen, Mátyás Schubert, Medina Or-
duna Baitemirova, Melody Arnaud, Melvin
McElrath, Michael Andrew Yee, Michael Co-
hen, Michael Gu, Michael Ivanitskiy, Michael
Starritt, Michael Strube, Michał Sw˛ edrowski,
Michele Bevilacqua, Michihiro Yasunaga, Mi-
hir Kale, Mike Cain, Mimee Xu, Mirac Suz-
gun, Mitch Walker, Mo Tiwari, Mohit Bansal,
Moin Aminnaseri, Mor Geva, Mozhdeh Gheini,
Mukund Varma T, Nanyun Peng, Nathan An-
drew Chi, Nayeon Lee, Neta Gur-Ari Krakover,
Nicholas Cameron, Nicholas Roberts, Nick Do-
iron, Nicole Martinez, Nikita Nangia, Niklas
Deckers, Niklas Muennighoff, Nitish Shirish
Keskar, Niveditha S. Iyer, Noah Constant, Noah

--- PAGE 19 ---
Fiedel, Nuan Wen, Oliver Zhang, Omar Agha,
Omar Elbaghdadi, Omer Levy, Owain Evans,
Pablo Antonio Moreno Casares, Parth Doshi,
Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah
Alipoormolabashi, Peiyuan Liao, Percy Liang,
Peter W Chang, Peter Eckersley, Phu Mon
Htut, Pinyu Hwang, Piotr Miłkowski, Piyush
Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu
Mei, Qing Lyu, Qinlang Chen, Rabin Ban-
jade, Rachel Etta Rudolph, Raefer Gabriel,
Rahel Habacker, Ramon Risco, Raphaël Mil-
lière, Rhythm Garg, Richard Barnes, Rif A.
Saurous, Riku Arakawa, Robbe Raymaekers,
Robert Frank, Rohan Sikand, Roman Novak,
Roman Sitelew, Ronan Le Bras, Rosanne Liu,
Rowan Jacobs, Rui Zhang, Russ Salakhut-
dinov, Ryan Andrew Chi, Seungjae Ryan
Lee, Ryan Stovall, Ryan Teehan, Rylan Yang,
Sahib Singh, Saif M. Mohammad, Sajant
Anand, Sam Dillavou, Sam Shleifer, Sam
Wiseman, Samuel Gruetter, Samuel R. Bow-
man, Samuel Stern Schoenholz, Sanghyun
Han, Sanjeev Kwatra, Sarah A. Rous, Sarik
Ghazarian, Sayan Ghosh, Sean Casey, Se-
bastian Bischoff, Sebastian Gehrmann, Sebas-
tian Schuster, Sepideh Sadeghi, Shadi Hamdan,
Sharon Zhou, Shashank Srivastava, Sherry Shi,
Shikhar Singh, Shima Asaadi, Shixiang Shane
Gu, Shubh Pachchigar, Shubham Toshniwal,
Shyam Upadhyay, Shyamolima Shammie Deb-
nath, Siamak Shakeri, Simon Thormeyer, Si-
mone Melzi, Siva Reddy, Sneha Priscilla
Makini, Soo-Hwan Lee, Spencer Torene, Sri-
harsha Hatwar, Stanislas Dehaene, Stefan Di-
vic, Stefano Ermon, Stella Biderman, Stephanie
Lin, Stephen Prasad, Steven Piantadosi, Stu-
art Shieber, Summer Misherghi, Svetlana Kir-
itchenko, Swaroop Mishra, Tal Linzen, Tal
Schuster, Tao Li, Tao Yu, Tariq Ali, Tat-
sunori Hashimoto, Te-Lin Wu, Théo Desbor-
des, Theodore Rothschild, Thomas Phan, Tianle
Wang, Tiberius Nkinyili, Timo Schick, Tim-
ofei Kornev, Titus Tunduny, Tobias Gersten-
berg, Trenton Chang, Trishala Neeraj, Tushar
Khot, Tyler Shultz, Uri Shaham, Vedant Misra,
Vera Demberg, Victoria Nyamai, Vikas Rau-
nak, Vinay Venkatesh Ramasesh, vinay uday
prabhu, Vishakh Padmakumar, Vivek Srikumar,
William Fedus, William Saunders, William
Zhang, Wout V ossen, Xiang Ren, Xiaoyu Tong,
Xinran Zhao, Xinyi Wu, Xudong Shen, Yadol-lah Yaghoobzadeh, Yair Lakretz, Yangqiu Song,
Yasaman Bahri, Yejin Choi, Yichi Yang, Yid-
ing Hao, Yifu Chen, Yonatan Belinkov, Yu Hou,
Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye
Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang,
and Ziyi Wu. 2023. Beyond the imitation game:
Quantifying and extrapolating the capabilities
of language models. Transactions on Machine
Learning Research .
Samuel Stanton, Pavel Izmailov, Polina
Kirichenko, Alexander A. Alemi, and An-
drew Gordon Wilson. 2021. Does knowledge
distillation really work? In Advances in Neural
Information Processing Systems 34: Annual
Conference on Neural Information Processing
Systems 2021, NeurIPS 2021, December 6-14,
2021, virtual , pages 6906–6919.
Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico
Kolter. 2024. A simple and effective pruning
approach for large language models. In The
Twelfth International Conference on Learning
Representations .
Alon Talmor, Jonathan Herzig, Nicholas Lourie,
and Jonathan Berant. 2019. CommonsenseQA:
A question answering challenge targeting com-
monsense knowledge. In Proceedings of the
2019 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics: Human Language Technologies, Vol-
ume 1 (Long and Short Papers) , pages 4149–
4158, Minneapolis, Minnesota. Association for
Computational Linguistics.
Sandeep Tata and Jignesh M. Patel. 2003. Piqa:
An algebra for querying protein data sets. In
Proceedings of the 15th International Con-
ference on Scientific and Statistical Database
Management (SSDBM 2003), 9-11 July 2003,
Cambridge, MA, USA , pages 141–150. IEEE
Computer Society.
Hugo Touvron, Thibaut Lavril, Gautier Izac-
ard, Xavier Martinet, Marie-Anne Lachaux,
Timothée Lacroix, Baptiste Rozière, Naman
Goyal, Eric Hambro, Faisal Azhar, Aurélien
Rodriguez, Armand Joulin, Edouard Grave, and
Guillaume Lample. 2023a. Llama: Open and
efficient foundation language models. CoRR ,
abs/2302.13971.

--- PAGE 20 ---
Hugo Touvron, Louis Martin, Kevin Stone, Pe-
ter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal
Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher, Cristian Canton-Ferrer, Moya Chen,
Guillem Cucurull, David Esiobu, Jude Fernan-
des, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-
thia Gao, Vedanuj Goswami, Naman Goyal,
Anthony Hartshorn, Saghar Hosseini, Rui
Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez, Madian Khabsa, Isabel Kloumann,
Artem Korenev, Punit Singh Koura, Marie-
Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao,
Xavier Martinet, Todor Mihaylov, Pushkar
Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta,
Kalyan Saladi, Alan Schelten, Ruan Silva,
Eric Michael Smith, Ranjan Subramanian, Xi-
aoqing Ellen Tan, Binh Tang, Ross Taylor,
Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-
gela Fan, Melanie Kambadur, Sharan Narang,
Aurélien Rodriguez, Robert Stojnic, Sergey
Edunov, and Thomas Scialom. 2023b. Llama
2: Open foundation and fine-tuned chat models.
CoRR , abs/2307.09288.
Ben Wang and Aran Komatsuzaki. 2021. GPT-
J-6B: A 6 Billion Parameter Autoregressive
Language Model. https://github.com/
kingoflolz/mesh-transformer-jax .
Peifeng Wang, Zhengyang Wang, Zheng Li, Yi-
fan Gao, Bing Yin, and Xiang Ren. 2023a.
SCOTT: self-consistent chain-of-thought distil-
lation. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 1: Long Papers), ACL 2023,
Toronto, Canada, July 9-14, 2023 , pages 5546–
5558. Association for Computational Linguis-
tics.
Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark
Steyvers, and William Yang Wang. 2023b.
Large language models are latent variable mod-
els: Explaining and finding good demonstra-
tions for in-context learning. In Thirty-seventh
Conference on Neural Information Processing
Systems .
Xuezhi Wang, Jason Wei, Dale Schuurmans,
Quoc V . Le, Ed H. Chi, Sharan Narang,Aakanksha Chowdhery, and Denny Zhou.
2023c. Self-consistency improves chain of
thought reasoning in language models. In The
Eleventh International Conference on Learning
Representations, ICLR 2023, Kigali, Rwanda,
May 1-5, 2023 . OpenReview.net.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra,
Alisa Liu, Noah A. Smith, Daniel Khashabi,
and Hannaneh Hajishirzi. 2023d. Self-instruct:
Aligning language models with self-generated
instructions. In Proceedings of the 61st An-
nual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) ,
pages 13484–13508, Toronto, Canada. Associ-
ation for Computational Linguistics.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra,
Alisa Liu, Noah A. Smith, Daniel Khashabi,
and Hannaneh Hajishirzi. 2023e. Self-instruct:
Aligning language models with self-generated
instructions. In Proceedings of the 61st An-
nual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
ACL 2023, Toronto, Canada, July 9-14, 2023 ,
pages 13484–13508. Association for Computa-
tional Linguistics.
Yue Wang, Weishi Wang, Shafiq R. Joty,
and Steven C. H. Hoi. 2021. Codet5:
Identifier-aware unified pre-trained encoder-
decoder models for code understanding and
generation. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2021, Virtual Event
/ Punta Cana, Dominican Republic, 7-11
November, 2021 , pages 8696–8708. Associa-
tion for Computational Linguistics.
Zhaoyang Wang, Shaohan Huang, Yuxuan Liu,
Jiahai Wang, Minghui Song, Zihan Zhang,
Haizhen Huang, Furu Wei, Weiwei Deng, Feng
Sun, and Qi Zhang. 2023f. Democratizing rea-
soning ability: Tailored learning from large lan-
guage model. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2023, Singapore,
December 6-10, 2023 , pages 1948–1966. Asso-
ciation for Computational Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans,
Maarten Bosma, Brian Ichter, Fei Xia, Ed H.
Chi, Quoc V . Le, and Denny Zhou. 2022.

--- PAGE 21 ---
Chain-of-thought prompting elicits reasoning in
large language models. In NeurIPS .
Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiang-
guo Zhang, Ruihao Gong, Jinyang Guo, and Xi-
anglong Liu. 2023. Outlier suppression+: Ac-
curate quantization of large language models by
equivalent and effective shifting and scaling. In
Proceedings of the 2023 Conference on Empiri-
cal Methods in Natural Language Processing ,
pages 1648–1665, Singapore. Association for
Computational Linguistics.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran
Chen, and Hai Li. 2016. Learning structured
sparsity in deep neural networks. In Advances
in Neural Information Processing Systems , vol-
ume 29. Curran Associates, Inc.
Miles Williams and Nikolaos Aletras. 2023. How
does calibration data affect the post-training
pruning and quantization of large language
models? CoRR , abs/2311.09755.
Samuel Williams, Andrew Waterman, and
David A. Patterson. 2009. Roofline: an insight-
ful visual performance model for multicore
architectures. Commun. ACM , 52(4):65–76.
Minghao Wu, Abdul Waheed, Chiyu Zhang,
Muhammad Abdul-Mageed, and Alham Aji.
2024. LaMini-LM: A diverse herd of distilled
models from large-scale instructions. In Pro-
ceedings of the 18th Conference of the Euro-
pean Chapter of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) ,
pages 944–964, St. Julian’s, Malta. Association
for Computational Linguistics.
Haojun Xia, Zhen Zheng, Yuchao Li, Donglin
Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong
Li, Wei Lin, and Shuaiwen Leon Song. 2023.
Flash-llm: Enabling cost-effective and highly-
efficient large generative model inference with
unstructured sparsity. Proc. VLDB Endow. ,
17(2):211–224.
Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and
Danqi Chen. 2024. Sheared LLaMA: Acceler-
ating language model pre-training via structured
pruning. In The Twelfth International Confer-
ence on Learning Representations .Patrick Xia, Shijie Wu, and Benjamin Van Durme.
2020. Which *bert? A survey organizing con-
textualized encoders. In Proceedings of the
2020 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2020, On-
line, November 16-20, 2020 , pages 7516–7533.
Association for Computational Linguistics.
Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao
Wu, Julien Demouth, and Song Han. 2023.
Smoothquant: Accurate and efficient post-
training quantization for large language mod-
els. In International Conference on Machine
Learning, ICML 2023, 23-29 July 2023, Hon-
olulu, Hawaii, USA , volume 202 of Proceed-
ings of Machine Learning Research , pages
38087–38099. PMLR.
Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu,
Julian J. McAuley, and Furu Wei. 2021. Be-
yond preserved accuracy: Evaluating loyalty
and robustness of BERT compression. In Pro-
ceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing,
EMNLP 2021, Virtual Event / Punta Cana, Do-
minican Republic, 7-11 November, 2021 , pages
10653–10659. Association for Computational
Linguistics.
Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo
Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu,
and Wanxiang Che. 2024. Onebit: Towards ex-
tremely low-bit large language models. CoRR ,
abs/2402.11295.
Zhewei Yao, Reza Yazdani Aminabadi, Minjia
Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong
He. 2022. Zeroquant: Efficient and affordable
post-training quantization for large-scale trans-
formers. In NeurIPS .
Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen
Youn, and Yuxiong He. 2023. Zeroquant-
v2: Exploring post-training quantization in llms
from comprehensive study to low rank compen-
sation. CoRR , abs/2303.08302.
Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu,
Xinggang Wang, Yuzhang Shang, Guangyu
Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe
Wu. 2023a. RPTQ: reorder-based post-training
quantization for large language models. CoRR ,
abs/2304.01089.

--- PAGE 22 ---
Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang
Wu, Yan Yan, and Guangyu Sun. 2023b.
ASVD: activation-aware singular value decom-
position for compressing large language mod-
els.CoRR , abs/2312.05821.
Yuxuan Yue, Zhihang Yuan, Haojie Duanmu,
Sifan Zhou, Jianlong Wu, and Liqiang Nie.
2024. Wkvquant: Quantizing weight and
key/value cache for large language models gains
more. CoRR , abs/2402.12065.
Susan Zhang, Stephen Roller, Naman Goyal,
Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona T. Diab, Xian Li,
Xi Victoria Lin, Todor Mihaylov, Myle Ott,
Sam Shleifer, Kurt Shuster, Daniel Simig,
Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. OPT:
open pre-trained transformer language models.
CoRR , abs/2205.01068.
Yuxin Zhang, Lirui Zhao, Mingbao Lin, Sun Yun-
yun, Yiwu Yao, Xingjia Han, Jared Tanner, Shi-
wei Liu, and Rongrong Ji. 2024. Dynamic
sparse no training: Training-free fine-tuning for
sparse LLMs. In The Twelfth International
Conference on Learning Representations .
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi
Tang, Xiaolei Wang, Yupeng Hou, Yingqian
Min, Beichen Zhang, Junjie Zhang, Zican
Dong, Yifan Du, Chen Yang, Yushuo Chen,
Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yi-
fan Li, Xinyu Tang, Zikang Liu, Peiyu Liu,
Jian-Yun Nie, and Ji-Rong Wen. 2023. A
survey of large language models. CoRR ,
abs/2303.18223.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng,
Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, Zi Lin, Zhuohan Li, Dacheng Li,
Eric P. Xing, Hao Zhang, Joseph E. Gonzalez,
and Ion Stoica. 2023. Judging llm-as-a-judge
with mt-bench and chatbot arena. In Advances
in Neural Information Processing Systems 36:
Annual Conference on Neural Information Pro-
cessing Systems 2023, NeurIPS 2023, New Or-
leans, LA, USA, December 10 - 16, 2023 .
Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xinwei
Long, Zhouhan Lin, and Bowen Zhou. 2024.
PaD: Program-aided distillation can teach small
models reasoning better than chain-of-thoughtfine-tuning. In Proceedings of the 2024 Con-
ference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies (Volume 1: Long
Papers) , pages 2571–2597, Mexico City, Mex-
ico. Association for Computational Linguistics.
Barret Zoph and Quoc V . Le. 2017. Neural ar-
chitecture search with reinforcement learning.
In5th International Conference on Learning
Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceed-
ings. OpenReview.net.
