# 2211.12551.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2211.12551.pdf
# File size: 1553622 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Sparse Probabilistic Circuits via
Pruning and Growing
Meihua Dang
CS Department
UCLA
mhdang@cs.ucla.eduAnji Liu
CS Department
UCLA
liuanji@cs.ucla.eduGuy Van den Broeck
CS Department
UCLA
guyvdb@cs.ucla.edu
Abstract
Probabilistic circuits (PCs) are a tractable representation of probability distributions
allowing for exact and efﬁcient computation of likelihoods and marginals. There
has been signiﬁcant recent progress on improving the scale and expressiveness of
PCs. However, PC training performance plateaus as model size increases. We dis-
cover that most capacity in existing large PC structures is wasted: fully-connected
parameter layers are only sparsely used. We propose two operations: pruning
andgrowing , that exploit the sparsity of PC structures. Speciﬁcally, the pruning
operation removes unimportant sub-networks of the PC for model compression
and comes with theoretical guarantees. The growing operation increases model ca-
pacity by increasing the size of the latent space. By alternatingly applying pruning
and growing, we increase the capacity that is meaningfully used, allowing us to
signiﬁcantly scale up PC learning. Empirically, our learner achieves state-of-the-art
likelihoods on MNIST-family image datasets and on Penn Tree Bank language data
compared to other PC learners and less tractable deep generative models such as
ﬂow-based models and variational autoencoders (V AEs).
1 Introduction
0.00 0.01 0.02 0.03 0.04 0.05
Parameter Values0.00.10.20.30.40.5Percentage< 0.001 (54%)
< 0.01 (76%)
 0.05 (5%)
Figure 1: Histogram of parameter values for a state-
of-the-art PC with 2.18M parameters on MNIST.
95% of the parameters have close-to-zero values.Probabilistic circuits (PCs) [ 44,3] are a uni-
fying framework to abstract from a multitude
of tractable probabilistic models. The key
property that separates PCs from other deep
generative models such as ﬂow-based mod-
els [31] and V AEs [ 19] is their tractability . It
enables them to compute various queries, in-
cluding marginal probabilities, exactly and ef-
ﬁciently [ 45]. Therefore, PCs are increasingly
used in inference-demanding applications such
as enforcing algorithmic fairness [ 2,4], making
predictions under missing data [ 6,18,23], data
compression [26], and anomaly detection [13].
Recent advancements in PC learning and reg-
ularization [ 40,25], and efﬁcient implementa-
tions [ 33,30,8] have been pushing the limits
of PC’s expressiveness and scalability such that
they can even match the performance of less
tractable deep generative models, including ﬂow-based models and V AEs. However, the performance
36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2211.12551v1  [cs.LG]  22 Nov 2022

--- PAGE 2 ---
of PCs plateaus as model size increases. This suggests that to further boost the performance of PCs,
simply scaling up the model size does not sufﬁce and we need to better utilize the available capacity.
We discover that this might be caused by the fact that the capacity of large PCs is wasted. As shown in
Figure 1, most parameters in a PC with 2.18M parameters have close-to-zero values, which have little
effect on the PC distribution. Since existing PC structures usually have fully-connected parameter
layers [25, 36], this indicates that the parameter values are only sparsely used.
In this work, we propose to better exploit the sparsity of large PC models by two structure learning
primitives — pruning andgrowing . Speciﬁcally, the goal of the pruning operation is to identify
and remove unimportant sub-networks of a PC. This is done by quantifying the importance of PC
parameters w.r.t. a dataset using circuit ﬂows , a theoretically-grounded metric that upper bounds the
drop of log-likelihood caused by pruning. Compared to L1 regularization, the proposed pruning
operator is more informed by the PC semantics, and hence quantiﬁes the global effects of pruning
much more effectively. Empirically, the proposed pruning method achieves a compression rate of
80-98% with at most 1% drop in likelihood on various PCs.
The proposed growing operation increases the model size by copying its existing components and
injecting noise. In particular, when applied to PCs compressed by the pruning operation, growing
produces larger PCs that can be optimized to achieve better performance. Applying pruning and
growing iteratively can greatly reﬁne the structure and parameters of a PC. Empirically, the log-
likelihoods metric can improve by 2% to 10% after a few iterations. Compared to existing PC
learners as well as less tractable deep generative models such as V AEs and ﬂow-based models, our
proposed method achieves state-of-the-art density estimation results on image datasets including
MNIST, EMNIST, FashionMNIST, and the Penn Tree Bank language modeling task.1
2 Probabilistic Circuits
Probabilistic circuits (PCs) [44,3] model probability distributions with a structured computation
graph. They are an umbrella term for a large family of tractable probabilistic models including
arithmetic circuits [ 9,10], sum-product networks (SPNs) [ 35], cutset networks [ 36], and-or search
spaces [ 28], and probabilistic sentential decision diagrams [ 21]. The syntax and semantics of PCs are
deﬁned as follows.
Deﬁnition 1 (Probabilistic Circuit) .A PCC:=(G;)represents a joint probability distribution p(X)
over random variables Xthrough a directed acyclic (computation) graph (DAG) Gparameterized by
. Similar to neural networks, each node in the DAG deﬁnes a computational unit. Speciﬁcally, the
DAGGconsists of three types of units — input ,sum, and product . Every leaf node in Gis an input
unit; every inner unit n(i.e., sum or product) receives inputs from its children in(n), and computes
output , which encodes a probability distribution pndeﬁned recursively as follows:
pn(x) :=8
><
>:fn(x) ifnis an input unit,Q
c2in(n)pc(x) ifnis a product unit,P
c2in(n)cjnpc(x)ifnis a sum unit,(1)
wherefn(x)is a univariate input distribution (e.g, Gaussian, Categorical), and cjndenotes the
parameter that corresponds to edge (n;c)in the DAG. For every sum unit n, its input parameters sum
up to one, i.e.,P
c2in(n)cjn= 1. Intuitively, a product unit deﬁnes a factorized distribution over its
inputs, and a sum unit represents a mixture over its input distributions with weights fcjn:c2in(n)g.
Finally, the probability distribution of a PC (i.e., pC) is deﬁned as the distribution represented by its
root unitr(i.e.,pr(x)), that is, its output neuron. The size of a PC, denoted jCj=jj, is the number
of parameters inC. We assume w.l.o.g. that a PC alternates between layers of sum and product units
before reaching its inputs. Figure 2 shows an example of a PC.
Computing the (log)likelihood of a PC Cgiven a sample xis equivalent to evaluating its computation
units inGin a feedforward manner following Equation 1. The key property that separates PCs from
other deep probabilistic models such as ﬂows [ 14] and V AEs [ 19] is their tractability , which is the
ability to exactly and efﬁciently answer various probabilistic queries. This paper focuses on PCs that
support linear time (w.r.t. model size) marginal probability computation, as they are increasingly used
1Code and experiments are available at https://github.com/UCLA-StarAI/SparsePC .
2

--- PAGE 3 ---
Z1X1
Z2X3
X4X2
(a):12:279
:0140:4
0:6:388
:066X1B(:1) X3B(:2)
:9:8
X1B(:7) X3B(:3):3:7:48
:020:8
0:20:1
0:9X2B(:6)
X4B(:8):6
:8
X2B(:1):1
X4B(:2):2
(b)
Figure 2: A smooth and decomposable PC (b) and an equivalent Bayesian network (a). The Bayesian
network is over 4 variables X=fX1;X2;X3;X4gand 2 hidden variables Z=fZ1;Z2gwith
h= 2hidden states. The feedforward computation order is from left to right;Jare input Bernoulli
distributions,Nare product units, andLare sum units; parameter values are annotated in the box.
The probability of each unit given input assignment fX1=0;X2=1;X3=0;X4=1gis labeled red.
(a) PC with fully connected layers (b) PC after pruning operation (c) PC after growing operation
Figure 3: A demonstration of the pruning and growing operation. From 3a to 3b, the red edges are
pruned. From 3b to 3c, the nodes are doubled, and each parameter is copied 3 times.
in downstream applications such as data compression [ 26] and making predictions under missing
data [ 18], and also achieve on-par expressiveness [ 26,25,24]. To support efﬁcient marginal inference,
PCs need to be smooth anddecomposable .
Deﬁnition 2 (Smoothness and Decomposability [ 11]).Thescope(n)of a PC unit nis the set of
input variables that it depends on; then, (1) a product unit is decomposable if its children have disjoint
scope; (2) a sum unit is smooth if its children have identical scope. A PC is decomposable if all of its
product units are decomposable; a PC is smooth if all of its sum units are smooth.
Decomposability ensures that every product unit encodes a well-deﬁned factorized distribution over
disjoint sets of variables; smoothness ensures that the mixture components of every sum units are
well-deﬁned over the same set of variables. Both structural properties will be the key to guaranteeing
the effectiveness of the structure learning algorithms proposed in the following sections.
3 Probabilistic Circuit Model Compression via Pruning
Figure 1 shows that most parameters in a large PC are very close to zero. Given that these parameters
are weights associated with mixture (sum unit) components, the corresponding edges and sub-circuits
have little impact on the sum unit output. Hence, by pruning away these unimportant components,
it is possible to signiﬁcantly reduce model size while retaining model expressiveness. Figure 3b
illustrates the result of pruning ﬁve (red) edges from the PC in Figure 3a. Given a PC and a dataset,
our goal is to efﬁciently identify a set of edges to prune, such that the log-likelihood gap between the
pruned PC and the original PC on the given dataset is minimized.
Pruning by parameters. The parameter value statistics in Figure 1 suggest that a natural criterion
is to prune edges by the magnitude of their corresponding parameter. This leads to the EPARAM (edge
parameters) heuristic, which selects the set of edges with the smallest parameters. However, edge
parameters themselves are insufﬁcient to quantify the importance of inputs to a sum unit in the entire
PC’s distribution. The parameters of a sum unit are normalized to be 1 so they only contain local
3

--- PAGE 4 ---
:114:279
:00420:4
0:6:388
:02X1B(:1) X3B(:2)
:9 :8
X1B(:7) X3B(:3):3 :7:48
:020:8
0:20:1
1:0X2B(:6)
X4B(:8):6
:8
X2B(:1):1
X4B(:2):2
(a)EPARAM removes the edge with =0:1:147:346
:0140:4
0:6:48
:066X1B(:1) X3B(:2)
:9 :8
X1B(:7) X3B(:3):3 :7:48
:021:0
0:20:1
0:9X2B(:6)
X4B(:8):6
:8
X2B(:1):1
X4B(:2):2
(b)EFLOW removes the edge with =0:2
Figure 4: A case study comparing pruning heuristics ( EPARAM and EFLOW ) on the PC in Figure 2
given samplefX1=0;X2=1;X3=0;X4=1g. The pruned edges are dashed and parameters are
re-normalized. Compared to the likelihood computed in Figure 2, the changed likelihoods are in red,
showing that pruning by ﬂows results in less likelihood decrease.
Algorithm 1: PC sampling
Input : a PC representing joint probability pC(X)
Output : an instance xsampled from pC
1Function SAMPLE (n)
2 ifnis a an input unit then
3fn(X) univariate distribution of n;return samplexfn(X)
4 else ifnis a product unit then
5 xc SAMPLE (c) foreachc2in(n);return Concatenate (fxcgc2in(n))
6 elsenis a sum unit
7 sample an input cproportional tofcjngc2in(n);return SAMPLE (c)
8return SAMPLE (r) whereris the root of PCC
information about the mixture components. Speciﬁcally, cjnmerely deﬁnes the relative importance
of edge (n;c)in the conditional distribution represented by its corresponding sum unit n, not the
joint distribution of the entire PC. Figure 4a illustrates what happens when the edge with the smallest
parameter is pruned from the PC in Figure 2.
However, as shown in Figure 4b, pruning another edge delivers better likelihoods as it accounts more
for the “global inﬂuence” of edges on the PC’s output. This global inﬂuence is highly related to the
probabilistic “circuit ﬂow” semantics of PCs. We will introduce circuit ﬂows later in this section,
along with their corresponding heuristics EFLOW . Before that, we ﬁrst introduce an intermediate
concept based on the notion of generative signiﬁcance of PCs.
Pruning by generative signiﬁcance. A more informed pruning strategy needs to consider the
global impact of edges on the distribution represented by the output of the PC. To achieve this,
instead of viewing the distribution pCin a feedforward manner following Equation 1, we quantify
the signiﬁcance of a unit or edge by the probability that it will be “activated” when drawing samples
from the PC. Indeed, if the presence of an edge is hardly ever relevant to the generative sampling
process, removing it will not signiﬁcantly affect the PC’s distribution.
Algorithm 1 shows how to draw samples from a PC distribution through a recursive implementation:
(1) for an input unit ndeﬁned on variable X(line 3), the algorithm randomly samples value x
according to its input univariate distribution; (2) for a product unit (line 5), by decomposability its
children have disjoint scope, thus we draw samples from all input units and then concatenate the
samples together; (3) for a sum unit n(line 7), by smoothness its children have identical scope, thus
we ﬁrst randomly sample one of its input units according to the categorical distribution deﬁned by
sum parametersfcjn:c2in(n)g, and then sample from this input unit recursively. Besides actually
drawing samples from the PC, we can also compute the probability that nwill be visited during the
sampling process. This provides a good measure of the importance of unit nto the PC distribution as
a whole, which we deﬁne as the top-down probability .
4

--- PAGE 5 ---
Deﬁnition 3 (Top-down Probability) .The top-down probability of each unit nin a PC with parame-
tersis deﬁned recursively as follows, assuming alternating sum and product layers:
q(n;) :=8
><
>:1 ifnis the root unit,P
m2out(n)q(m;) ifnis a sum unit,P
m2out(n)njmq(m;)ifnis a product unit,
where out(n)are the units that take nas input in the feedforward computation. Moreover, the
top-down probability of a sum edge (n;c)is deﬁned as q(n;c;) =cjnq(n;).
The top-down probability of the root is always 1; a product unit passes its top-down probability to
all its inputs, and a sum unit distributes its top-down probability to its inputs proportional to the
corresponding edge weights. Therefore, the top-down probability of a non-root unit is summing over
all probabilities it receives from its outputs.
The top-down probability of all PC units and sum edges can be computed in a single backward pass
over the PC’s computation graph. Following the intuition that the top-down probability deﬁnes the
probability that units will be visited during the sampling process, pruning edges with the smallest
top-down probability constitutes a reasonable pruning strategy.
Pruning by circuit ﬂows. The top-down probability q(n;)represents the probability of reaching
unitnin an unconditional random sampling process. Despite its ability to capture global information
of PC parameters, the top-down probability is not tailored to a speciﬁc dataset. Therefore, to further
utilize the dataset information, we can measure the probability of reaching certain units/edges in the
sampling process conditioning on some instance xbeing sampled . To bridge this gap, we deﬁne
circuit ﬂow as a sample-dependent version of the top-down probability.
Deﬁnition 4 (Circuit Flow2).For a given PC with parameters and examplex, the circuit ﬂow
of unitnon examplexis the probability that nwill be visited during the sampling procedure
conditioned on xbeing sampled. This can be computed recursively as follows, assuming alternating
sum and product layers:
Fn(x) =8
><
>:1 ifnis the root unit,P
m2out(n)Fm(x) ifnis a sum unit,
P
m2out(n)njmpn(x)
pm(x)Fm(x)ifnis a product unit.
Similarly, the edge ﬂow Fn;c(x)on samplexis deﬁned by Fn;c(x) =cjnpc(x)=pn(x)Fn(x).
We further deﬁne Fn;c(D) =P
x2DFn;c(x)as the aggregate edge ﬂow over datasetD.
Effectively, we can think of x
njm:=njmpn(x)=pm(x)as the posterior probability of component n
in the mixture of sum unit mconditioned on observing sample x. Then, circuit ﬂow is the top-
down probability under this xreparameterization of the circuit: Fn(x) =q(n;x)andFn;c(x) =
q(n;c;x).
Circuit ﬂow Fn(x)deﬁnes the probability of reaching unit nin the top-down sampling procedure of
Algorithm 1, given that the sampled instance is x. Therefore, edge ﬂow Fn;c(x)is a natural metric
of the importance of edge (n;c)givenx. Intuitively, the aggregate circuit ﬂow measures how many
expected samples “ﬂow” through certain edges. We write EFLOW to refer to the heuristic that prunes
edges with the smallest aggregate circuit ﬂow.
Empirical Analysis. Figure 5a compares the effect of pruning heuristics EPARAM ,EFLOW , as
well as an uninformed strategy, prune randomly, which we denote as ERAND. It shows that both
EPARAM and EFLOW are reasonable pruning strategy, however, as we increase the percentage of
pruned parameters, EFLOW has less log-likelihoods drop compared with EPARAM . Using EFLOW
heuristics we can pruning up to 80% of the parameters without much log-likelihoods drop. As
shown in Figure 5b, the parameter distribution is more balanced after pruning compared to Figure 1,
indicating a higher signiﬁcance of each edge. Section 6 will provide more empirical results. Before
that, we ﬁrst theoretically verify the effectiveness of the EFLOW heuristic in the next section.
2Earlier work deﬁned “circuit ﬂow” or “expected circuit ﬂow” in the context of parameter learning [ 4,25,7],
without observing the connection to sampling. We contribute its more intuitive sampling semantics here.
5

--- PAGE 6 ---
0 40 80
Pruning %120
110
100
Log-likelihoods
eRand
eFlow
eParams(a) Comparison of heuristics ERAND,EPARAM , and
EFLOW . Heuristic EFLOW can prune up to 80% of the
parameters without much loglikelihoods decrease.
0.0 0.5 1.0
Parameter Values0.000.050.100.15PercentageInit PC
After Prune(b) Histogram of parameters before (the same as in
Figure 1) and after pruning. The parameter values
take higher signiﬁcance after pruning.
Figure 5: Empirical evaluation of the pruning operation.
4 Bounding and Approximating the Loss of Likelihood
In this section, we theoretically quantify the impact of edge pruning on model performance. In
particular, we establish an upper bound on the log-likelihood drop LLon a given dataset Dby
comparing (i) the original PC Cand (ii) the pruned PC CnEcaused by pruning away edges E:
LL(D;C;E)=LL(D;C) LL (D;CnE): (2)
We start from the case of pruning one edge (i.e., jEj= 1in Equation 2). In this case, the loss of
likelihood can be quantiﬁed exactly using ﬂows and edge parameters:
Theorem 1 (Log-likelihood drop of pruning one edge) .For a PCCand a datasetD, the loss of
log-likelihood by pruning away edge (n;c)is
LL(D;C;f(n;c)g)=1
jDjX
x2Dlog1 cjn
1 cjn+cjnFn(x) Fn;c(x)
 1
jDjX
x2Dlog(1 Fn;c(x)):
See proof in Appendix B.1. By computing the second term in Theorem 1, we can pick the edge with
the smallest log-likelihood drop. Additionally, the third term characterizes the log-likelihood drop
without re-normalizing parameters of jn. It suggests pruning the edge with the smallest edge ﬂow.
A key insight from Theorem 1 is that the log-likelihood drop depends explicitly on the edge ﬂow
Fn;c(x)and unit ﬂow Fn(x). This matches the intuition from Section 3 and suggests that the circuit
ﬂow heuristic proposed in the previous section is a good approximation of the derived upper bound.
Next, we bound the log-likelihood drop of pruning multiple edges.
Theorem 2 (Log-likelihood drop of pruning multiple edges) .LetCbe a PC andDbe a dataset. For
any set of edgesEinC, if8x2D;P
(n;c)2EFn;c(x)<1, the log-likelihood drop by pruning away E
is bounded and approximated by
LL(D;C;E) 1
jDjX
xlog(1 X
(n;c)2EFn;c(x))1
jDjX
(n;c)2EFn;c(D):(3)
0 50 100
Pruning %104
102
100102 Log-likelihoods
LL
eFlow
Figure 6: Comparing the actual loglikeli-
hood drop ( LL) and EFLOW heuristics
(the approximated upper bound in Equa-
tion 3). The approximate bound matches
closely to the actual loglikelihood drop.Proof of this theorem is provided in Appendix B.2. We
ﬁrst look at the second term of Equation 3. Although
it provides an upper bound to the performance drop, it
cannot be used as a pruning heuristic since the bound
does not decompose over edges. And hence ﬁnding the
set of edges with the lowest score requires evaluating
the bound exponentially many times with respect to the
number of pruned edges. Therefore, we do an additional
approximation step of the bound via Taylor expansion,
which leads to the third term of Equation 3. This ap-
proximation matches the EFLOW heuristic by a constant
factor 1=jDj, which theoretically justiﬁes the effective-
ness of the heuristic. Figure 6 empirically compares the
6

--- PAGE 7 ---
actual log-likelihood drop and the quantity computed
from the circuit ﬂow heuristic (that is, the approximate
upper bound) for different percentages of pruned parame-
ters. We see that the approximate bound matches closely
to the actual log-likelihood drop.
5 Scalable Structure Learning
The pruning operator improves two aspects of PCs. First, as shown in Figure 5b, model parameters
are more balanced after pruning. Second, pruning removes sub-circuits with negligible contributions
to the model’s distribution. If we treat PCs as hierarchical mixtures of components, pruning can
be regarded as an implicit structure learning step that removes the “unimportant” components for
each mixture. However, since pruning only decreases model capacity, it is impossible to get a more
expressive PC than the original one. To mitigate this problem, we propose a growing operation to
increase the capacity of a PC by introducing more components for each mixture. Pruning and growing
together deﬁne a scalable structure learning algorithm for PCs.
n𝚗𝚎𝚠c𝚗𝚎𝚠cn
Figure 7: Growing operation. Each unit
is doubled, and each parameterized edge
is copied 3 times: (nnew;cnew)(orange),
(nnew;c)(purple), and (n;cnew)(green).Growing. Growing is an operator that increases model
size by copying its existing components and injecting
noise. As shown in Figure 3, after applying the grow-
ing operation on the original PC in Figure 3b, we can get
a new grown PC as in Figure 7. Speciﬁcally, the growing
operation is applied to units, edges, and parameters respec-
tively: (1) for units, growing operates on every PC unit n
and creates another copy nnew; (2) for edges, the sum edge
(n;c)from the original PC (Figure 3b) are copied three
times to the grown PC (Figure 7): from new parent to new
child (nnew;cnew), from old parent to new child (n;cnew),
and from new parent to old child (nnew;c); product edges
are added to connect the copied version of a product unit
and its copied inputs; (3) a new parameter new
cjnis a noisy
copy of an old parameter cjn, that isnew
cjn cjnwhereN (1;2)and2controls the
Gaussian noise variance. Gaussian noise is added to the copied parameters to ensure that after we
apply the growing operation, parameter learning algorithms can ﬁnd diverse parameters for different
copies. After a growing operation, the PC size is 4 times the original PC size. Algorithm 3 in
appendix shows a feedforward implementation of the growing operation.
Structure Learning through Pruning and Growing. The proposed pruning and growing algo-
rithms can be applied iteratively to reﬁne the structure and parameters of an initial PC. Speciﬁcally,
since the growing operator increases the number of PC parameters by a factor of 4, applying growing
after pruning 75% of the edges from an initial PC keeps the number of parameters unchanged. We
propose a joint structure and parameter learning algorithm for PCs that uses these two operations.
Speciﬁcally, starting from an initial PC, we apply 75% pruning, growing, and parameter learning
iteratively until convergence. We utilize HCLTs [ 25] as initial PC structure as it has the state-of-the-art
likelihood performance. Note that this structure learning pipeline can be applied to any PC structure.
Parameter Estimation. We use a stochastic mini-batch version of Expectation-Maximization
optimization [ 2]. Speciﬁcally, at each iteration, we draw a mini-batch of samples DB, compute
aggregated circuit ﬂows Fn;c(DB)andFn(DB)of these samples (E-step), and then compute new
parameternew
cjn= Fn;c(DB)=Fn(DB). The parameters are then updated with learning rate :
t+1 new+ (1 )t(M-step). Empirically this approach converges faster and is better
regularized compared to full-batch EM.
Parallel Computation. Existing approaches to scaling up learning and inference with PCs, such as
Einsum networks [ 33], utilize fully connected parametrized layers (Figure 3a) of PC structures such
as HCLT [ 25] and RatSPN [ 34]. These structures can be easily vectorized to utilize deep learning
packages such as PyTorch. However, the sparse structure learned by pruning and growing is not
7

--- PAGE 8 ---
easily vectorized as a dense matrix operation. We therefore implement customized GPU kernels to
parallelize the computation of parameter learning and inference based on Juice.jl [ 8], an open-source
Julia package for learning PCs. The kernels segment PC units into layers such that the units in each
layer are independent. Thus, the computation can be fully parallelized on the GPU. As a result, we
can train PCs with millions of parameters in less than half an hour.
6 Experiments
We now evaluate our proposed method pruning and growing on two different sets of density estimation
benchmarks: (1) the MNIST-family image generation datasets including MNIST [ 22], EMNIST [ 5],
and FashionMNIST [46]; (2) the character-level Penn Tree Bank language modeling task [27].
Section 6.1 ﬁrst reports the best results we get on image datasets and language modeling tasks via
the structure learning procedure proposed in Section 5. Section 6.2 then shows the effect of pruning
and growing operations via two detailed experimental settings. It studies two different constrained
optimization problems: ﬁnding the smallest PC for a given likelihood via model compression and
ﬁnding the best PC of a given size via structure learning.
Settings. For all experiments, we use hidden Chow-Liu Trees (HCLTs) [ 25] with the number
of latent states inf16;32;64;128gas initial PC structures. We train the parameters of PCs with
stochastic mini-batch EM (cf. Section 5). We perform early stopping and hyperparameter search
using a validation set and report results on the test set. Please refer to Appendix C for more
details. We use mean test set bits-per-dimension (bpd) as the evaluation criteria, where bpd(D;C) =
 LL (D;C)=(log(2)m)andmis the number of features in dataset D.
6.1 Density Estimation Benchmarks
Image Datasets. The MNIST-family datasets contain gray-scale pixel images of size 2828where
each pixel takes values in [0;255]. We split out 5% of training data as a validation set. We compare
with two competitive PC learning algorithms: HCLT [ 25] and RatSPN [ 34], one ﬂow-based model:
IDF [ 17], and three V AE-based methods: BitSwap [ 20], BB-ANS [ 41], and McBits [ 38]. For a
fair comparison, we implement RatSPN structures ourselves and use the same training pipeline and
EM optimizer as our proposed method. Note that EinsumNet [ 33] also uses RatSPN structures but
with a PyTorch implementation so its comparison is subsumed by comparison with RatSPN. All 7
methods are tested on MNIST, 4 splits of EMNIST and FashionMNIST. As shown in Table 1, the
best results are bold. We see that our proposed method signiﬁcantly outperforms all other baselines
on all datasets, and establishes new state-of-the-art results among PCs, ﬂows, and V AE models. More
experiment details are in Appendix C.
Table 1: Density estimation performance on MNIST-family datasets in test set bpd.
Dataset Sparse PC (ours) HCLT RatSPN IDF BitSwap BB-ANS McBits
MNIST 1.14 1.20 1.67 1.90 1.27 1.39 1.98
EMNIST(MNIST) 1.52 1.77 2.56 2.07 1.88 2.04 2.19
EMNIST(Letters) 1.58 1.80 2.73 1.95 1.84 2.26 3.12
EMNIST(Balanced) 1.60 1.82 2.78 2.15 1.96 2.23 2.88
EMNIST(ByClass) 1.54 1.85 2.72 1.98 1.87 2.23 3.14
FashionMNIST 3.27 3.34 4.29 3.47 3.28 3.66 3.72
Language Modeling Task. We use the Penn Tree Bank dataset with standard processing
from Mikolov et al. [29], which contains around 5M characters and a character-level vocabulary size
of50. The data is split into sentences with a maximum sequence length of 288. We compare with
three competitive normalizing-ﬂow-based models: Bipartite ﬂow [ 42] and latent ﬂows [ 48] including
AF/SCF and IAF/SCF, since they are the only comparable work with non-autoregressive language
modeling. As shown in Table 2, the proposed method outperforms all three baselines.
8

--- PAGE 9 ---
Table 2: Character-level language modeling results on Penn Tree Bank in test set bpd.
Dataset Sparse PC (ours) Bipartite ﬂow [42] AF/SCF [48] IAF/SCF [48]
Penn Tree Bank 1.35 1.38 1.46 1.63
6.2 Evaluating Pruning and Growing
What is the Smallest PC for the Same Likelihood? We evaluate the ability of pruning based
on circuit ﬂows to do effective model compression by iteratively pruning a k-fraction of the PC
parameters and then ﬁne-tuning them until the ﬁnal training log-likelihood does not decrease by more
than1%. Speciﬁcally, we take pruning percentage kfromf0:05;0:1;0:3g. As shown in Figure 8,
we can achieve a compression rate of 80-98% with negligible performance loss on PCs. Besides, by
ﬁxing the number of latent parameters (x-axis) and comparing bpp across different numbers of latent
states (legend), we discover that compressing a large PC to a get smaller PC yields better likelihoods
compared to directly training an HCLT with the same number of parameters from scratch. This can
be explained by the sparsity of compressed PC structures, as well as a smarter way of ﬁnding good
parameters: learning a better PC with larger size and compressing it down to a smaller one.
104105106
# Parameters1.11.2Train bpd0.88
0.91
0.95
0.98mnist
105106
# Parameters1.61.80.81
0.86
0.93
0.95emnist_mnist
105106
# Parameters1.61.80.81
0.91
0.93
0.97emnist_letters
105106
# Parameters1.61.80.81
0.91
0.93
0.97emnist_balanced
104105106
# Parameters1.61.80.88
0.91
0.93
0.3emnist_byclass
104105106107
# Parameters3.23.4
0.88
0.91
0.96
0.97fashionmnist
latents
16
32
64
128
Figure 8: Model compression via pruning and ﬁnetuning. We report the training set bpd (y-axis) in
terms of the number of parameters (x-axis) for different numbers of latent states. For each curve,
compression starts from the right (initial PC #Params jCinitj) and ends at the left (compressed PC
#ParamsjCcomj); compression rate (1 - jCcomj/jCinitj) is annotated next to each curve.
What is the Best PC for the Same Size? We evaluate structure learning that combines pruning
and growing as proposed in Section 5. Starting from an initial HCLT, we iteratively prune 75% of
the parameters, grow again, and ﬁne-tune until meeting the stopping criteria. As shown in Figure 9,
our method consistently improve the likelihoods of initial PCs for different numbers of latent states
among all datasets.
8 16 32 64 128
Latents1.11.21.3Bpd
mnist
8 16 32 64 128
Latents1.501.752.00
emnist_mnist
8 16 32 64 128
Latents1.61.82.0
emnist_letters
8 16 32 64 128
Latents1.752.00
emnist_balanced
8 16 32 64
Latents1.61.82.0
emnist_byclass
8 16 32 64 128
Latents3.23.4
fashionmnist
train
test
Figure 9: Structure learning via 75% pruning, growing and ﬁnetuning. We report bpd (y-axis) on
both train (red) and test set (green) in terms of the number of latent states (x-axis). For each curve,
training starts from the top (large bpd) and ends at the bottom (small bpd).
7 Related Work
Improving the expressiveness of PCs has been a central topic in the literature. Predominant works
focus on the structure learning algorithms that iteratively modify PC structures to progressively ﬁt the
data [ 7,24,15]. Alternatively, a recent trend is to construct PCs with good initial structures and only
perform parameter learning afterwards [ 36,1,47]. For example, EiNets [ 33], RAT-SPNs [ 34], and
XPCs [ 12] use randomly generated structures, HCLTs [ 25] and ID-SPNs [ 37] deﬁne PCs dependent
on the pairwise correlation on variables, and learn direct and indirect variable correlations among
variables. There are also a few works that boost PC performance with the expressive power of neural
networks. CSPNs [ 39] harness neural networks to learn expressive conditional density estimators,
and HyperSPN [40] utilizes neural networks for better PC regularization.
9

--- PAGE 10 ---
Pruning and growing have been introduced in deep neural networks to exploit sparsity [ 16]. Similar
strategies can be adopted in probabilistic circuits. Patil et al. [32] prune decision trees according
to validation accuracy reduction. ResSPNs uses the lottery ticket hypothesis for weight pruning to
gain more compact PCs [ 43]. However, the neural network pruning methods and above PC pruning
methods mainly focus on pruning by parameters. In contrast to these, our work develops better
pruning strategies based on semantic properties of PCs.
8 Conclusions
We propose structure learning of probabilistic circuits by combining pruning and growing operations
to exploit the sparsity of PC structures. We show signiﬁcant empirical improvements in the density
estimation tasks of PCs compared to existing PC learners and competing ﬂow-based models and V AEs.
All our Sparse-PC learning and inference algorithms are available as GPU-parallel implementations.
Acknowledgements This work was funded in part by the DARPA Perceptually-enabled Task
Guidance (PTG) Program under contract number HR00112220005, NSF grants #IIS-1943641, #IIS-
1956441, #CCF-1837129, Samsung, CISCO, a Sloan Fellowship, and a UCLA Samueli Fellowship.
We thank Honghua Zhang for proofreading and insightful comments on this paper’s ﬁnal version.
References
[1]Tameem Adel, David Balduzzi, and Ali Ghodsi. Learning the structure of sum-product networks
via an svd-based algorithm. In Proceedings of the 31st Conference on Uncertainty in Artiﬁcial
Intelligence (UAI) , 2015.
[2]YooJung Choi, Golnoosh Farnadi, Behrouz Babaki, and Guy Van den Broeck. Learning fair
naive bayes classiﬁers by discovering and eliminating discrimination patterns. In Proceedings
of the 34th AAAI Conference on Artiﬁcial Intelligence , 2020.
[3]YooJung Choi, Antonio Vergari, and Guy Van den Broeck. Probabilistic circuits: A unifying
framework for tractable probabilistic models. Technical report , 2020.
[4]YooJung Choi, Meihua Dang, and Guy Van den Broeck. Group fairness by probabilistic
modeling with latent fair decisions. In Proceedings of the 35th AAAI Conference on Artiﬁcial
Intelligence , 2021.
[5]Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending
mnist to handwritten letters. In 2017 International Joint Conference on Neural Networks
(IJCNN) , 2017.
[6]Alvaro Correia, Robert Peharz, and Cassio P de Campos. Joints in random forests. In Advances
in Neural Information Processing Systems 33 (NeurIPS) , 2020.
[7]Meihua Dang, Antonio Vergari, and Guy Van den Broeck. Strudel: Learning structured-
decomposable probabilistic circuits. In Proceedings of the 10th International Conference on
Probabilistic Graphical Models (PGM) , 2020.
[8]Meihua Dang, Pasha Khosravi, Yitao Liang, Antonio Vergari, and Guy Van den Broeck. Juice:
A julia package for logic and probabilistic circuits. In Proceedings of the 35th AAAI Conference
on Artiﬁcial Intelligence (Demo Track) , 2021.
[9]Adnan Darwiche. A logical approach to factoring belief networks. In Proceedings of the 8th
International Conference on Principles of Knowledge Representation and Reasoning (KR) ,
2002.
[10] Adnan Darwiche. A differential approach to inference in bayesian networks. Journal of the
ACM , 2003.
[11] Adnan Darwiche and Pierre Marquis. A knowledge compilation map. Journal of Artiﬁcial
Intelligence Research , 2002.
10

--- PAGE 11 ---
[12] Nicola Di Mauro, Gennaro Gala, Marco Iannotta, and Teresa M.A. Basile. Random probabilistic
circuits. In Proceedings of the 37th Conference on Uncertainty in Artiﬁcial Intelligence (UAI) ,
2021.
[13] Marc Dietrichstein, David Major, Martin Trapp, Maria Wimmer, Dimitrios Lenis, Philip Winter,
Astrid Berg, Theresa Neubauer, and Katja Bühler. Anomaly detection using generative models
and sum-product networks in mammography scans. In MICCAI Workshop on Deep Generative
Models , 2022.
[14] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516 , 2014.
[15] Robert Gens and Domingos Pedro. Learning the structure of sum-product networks. In
Proceedings of the 30th International Conference on Machine Learning (ICML) , 2013.
[16] Torsten Hoeﬂer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity
in deep learning: Pruning and growth for efﬁcient inference and training in neural networks.
Journal of Machine Learning Research , 2021.
[17] Emiel Hoogeboom, Jorn Peters, Rianne Van Den Berg, and Max Welling. Integer discrete ﬂows
and lossless compression. In Advances in Neural Information Processing Systems 32 (NeurIPS) ,
2019.
[18] Pasha Khosravi, YooJung Choi, Yitao Liang, Antonio Vergari, and Guy Van den Broeck. On
tractable computation of expected predictions. In Advances in Neural Information Processing
Systems 32 (NeurIPS) , 2019.
[19] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 , 2013.
[20] Friso Kingma, Pieter Abbeel, and Jonathan Ho. Bit-swap: Recursive bits-back coding for
lossless compression with hierarchical latent variables. In Proceedings of the 36th International
Conference on Machine Learning (ICML) , 2019.
[21] Doga Kisa, Guy Van den Broeck, Arthur Choi, and Adnan Darwiche. Probabilistic sentential
decision diagrams. In Proceedings of the 14th International Conference on Principles of
Knowledge Representation and Reasoning (KR) , 2014.
[22] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs
[Online]. Available: http://yann.lecun.com/exdb/mnist , 2, 2010.
[23] Wenzhe Li, Zhe Zeng, Antonio Vergari, and Guy Van den Broeck. Tractable computation of
expected kernels. In Proceedings of the 37th Conference on Uncertainty in Aritiﬁcal Intelligence
(UAI) , 2021.
[24] Yitao Liang, Jessa Bekker, and Guy Van den Broeck. Learning the structure of probabilistic
sentential decision diagrams. In Proceedings of the 33rd Conference on Uncertainty in Artiﬁcial
Intelligence (UAI) , 2017.
[25] Anji Liu and Guy Van den Broeck. Tractable regularization of probabilistic circuits. In Advances
in Neural Information Processing Systems 34 (NeurIPS) , 2021.
[26] Anji Liu, Stephan Mandt, and Guy Van den Broeck. Lossless compression with probabilistic
circuits. In Proceedings of the 10th International Conference on Learning Representations
(ICLR) , 2022.
[27] Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large
annotated corpus of english: The penn treebank. Computational Linguistics , 1993.
[28] Radu Marinescu and Rina Dechter. And/or branch-and-bound for graphical models. In Pro-
ceedings of the 19th International Joint Conference on Artiﬁcial Intelligence (IJCAI) , 2005.
[29] Tomáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, and Stefan Kombrink. Subword lan-
guage modeling with neural networks. preprint (http://www.ﬁt.vutbr.cz/imikolov/rnnlm/char.pdf) ,
2012.
11

--- PAGE 12 ---
[30] Alejandro Molina, Antonio Vergari, Karl Stelzner, Robert Peharz, Pranav Subramani, Nicola
Di Mauro, Pascal Poupart, and Kristian Kersting. Spﬂow: An easy and extensible library for
deep probabilistic learning using sum-product networks. arXiv preprint arXiv:1901.03704 ,
2019.
[31] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji
Lakshminarayanan. Normalizing ﬂows for probabilistic modeling and inference. Journal of
Machine Learning Research , 2021.
[32] Dipti D Patil, VM Wadhai, and JA Gokhale. Evaluation of decision tree pruning algorithms for
complexity and classiﬁcation accuracy. International Journal of Computer Applications , 2010.
[33] Robert Peharz, Steven Lang, Antonio Vergari, Karl Stelzner, Alejandro Molina, Martin Trapp,
Guy Van den Broeck, Kristian Kersting, and Zoubin Ghahramani. Einsum networks: Fast and
scalable learning of tractable probabilistic circuits. In Proceedings of the 37th International
Conference on Machine Learning (ICML) , 2020.
[34] Robert Peharz, Antonio Vergari, Karl Stelzner, Alejandro Molina, Xiaoting Shao, Martin Trapp,
Kristian Kersting, and Zoubin Ghahramani. Random sum-product networks: A simple and
effective approach to probabilistic deep learning. In Proceedings of the 36th Conference on
Uncertainty in Aritiﬁcal Intelligence (UAI) , 2020.
[35] Hoifung Poon and Pedro Domingos. Sum-product networks: A new deep architecture. In 2011
IEEE International Conference on Computer Vision Workshops (ICCV Workshops) , 2011.
[36] Tahrima Rahman, Prasanna Kothalkar, and Vibhav Gogate. Cutset networks: A simple, tractable,
and scalable approach for improving the accuracy of chow-liu trees. In Joint European confer-
ence on machine learning and knowledge discovery in databases , 2014.
[37] Amirmohammad Rooshenas and Daniel Lowd. Learning sum-product networks with direct and
indirect variable interactions. In Proceedings of the 31st International Conference on Machine
Learning (ICML) , 2014.
[38] Yangjun Ruan, Karen Ullrich, Daniel S Severo, James Townsend, Ashish Khisti, Arnaud Doucet,
Alireza Makhzani, and Chris Maddison. Improving lossless compression rates via monte carlo
bits-back coding. In Proceedings of the 38th International Conference on Machine Learning
(ICML) , 2021.
[39] Xiaoting Shao, Alejandro Molina, Antonio Vergari, Karl Stelzner, Robert Peharz, Thomas
Liebig, and Kristian Kersting. Conditional sum-product networks: Modular probabilistic
circuits via gate functions. International Journal of Approximate Reasoning , 2022.
[40] Andy Shih, Dorsa Sadigh, and Stefano Ermon. Hyperspns: Compact and expressive probabilistic
circuits. In Advances in Neural Information Processing Systems 34 (NeurIPS) , 2021.
[41] James Townsend, Thomas Bird, and David Barber. Practical lossless compression with latent
variables using bits back coding. In Proceedings of the 6th International Conference on Learning
Representations (ICLR) , 2018.
[42] Dustin Tran, Keyon Vafa, Kumar Agrawal, Laurent Dinh, and Ben Poole. Discrete ﬂows:
Invertible generative models of discrete data. In Advances in Neural Information Processing
Systems 32 (NeurIPS) , 2019.
[43] Fabrizio Ventola, Karl Stelzner, Alejandro Molina, and Kristian Kersting. Residual sum-product
networks. In Proceedings of the 10th International Conference on Probabilistic Graphical
Models (PGM) , 2020.
[44] Antonio Vergari, YooJung Choi, Robert Peharz, and Guy Van den Broeck. Probabilistic circuits:
Representations, inference, learning and applications. AAAI Tutorial , 2020.
[45] Antonio Vergari, YooJung Choi, Anji Liu, Stefano Teso, and Guy Van den Broeck. A composi-
tional atlas of tractable circuit operations for probabilistic inference. In Advances in Neural
Information Processing Systems 34 (NeurIPS) , 2021.
12

--- PAGE 13 ---
[46] Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
[47] Honghua Zhang, Brendan Juba, and Guy Van den Broeck. Probabilistic generating circuits. In
Proceedings of the 38th International Conference on Machine Learning (ICML) , 2021.
[48] Zachary Ziegler and Alexander Rush. Latent normalizing ﬂows for discrete sequences. In
Proceedings of the 36th International Conference on Machine Learning (ICML) , 2019.
13

--- PAGE 14 ---
Checklist
1. For all authors...
(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
contributions and scope? [Yes] Contributions are clearly stated in the third and fourth
paragraphs of the introduction.
(b)Did you describe the limitations of your work? [Yes] The proposed learning algorithms
are designed for smooth and decomposable PCs.
(c) Did you discuss any potential negative societal impacts of your work? [N/A]
(d)Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results...
(a)Did you state the full set of assumptions of all theoretical results? [Yes] All theorems
in the main text formally state all assumptions.
(b)Did you include complete proofs of all theoretical results? [Yes] Proofs are included
in the appendix.
3. If you ran experiments...
(a)Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] Experiment
setup is detailed in the main text and the appendix.
(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] Training details are provided in the appendix.
(c)Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [N/A]
(d)Did you include the total amount of compute and the type of resources used (e.g.,
type of GPUs, internal cluster, or cloud provider)? [Yes] Details about computational
resources can be found in the appendix.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a)If your work uses existing assets, did you cite the creators? [Yes] We cited all PC
learning algorithms as well as the adopted datasets.
(b)Did you mention the license of the assets? [Yes] We speciﬁed that the used algorithms
are publicly available.
(c)Did you include any new assets either in the supplemental material or as a URL? [Yes]
We included our code in the supplementary material.
(d)Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e)Did you discuss whether the data you are using/curating contains personally identiﬁable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects...
(a)Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b)Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c)Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]
14

--- PAGE 15 ---
A Pseudocode
In this section, we list the detailed algorithms for pruning (Section 3), growing (Section 5), circuit
ﬂows computation (Deﬁnition 4), and mini-batch Expectation Maximization (Section 5).
Algorithm 2 shows how to prune kpercentage edges from PC Cfollowing heuristic h.
Algorithm 2: Prune (C,h,k)
Input : a non-deterministic PC C, heuristichdeciding which edge to prune, hcan be EFLOW ,
ERAND, or EPARAM , percentage of edges to prune k
Output : a PCC’ after pruned
1old2new mapping from input PC n2Cto pruned PC
2s(n;c) compute a score for each edge (n;c)based on heuristic h
3f(n;c) false
4f(n;c) true ifs(n;c)ranks the last k
5// visit children before parents
6foreachn2Cdo
7 ifnis a leaf then
8 old2new [n] n
9 else ifnis a sum then
10 old2new [n] L([old2new (c)forc2in(n)and iff(n;c)])
11 elsenis a product
12 old2new [n] N([old2new (c)forc2in(n)])
13return old2new [nr]wherenris the root ofC
Algorithm 3 shows show a feedforward implementation of growing operation.
Algorithm 3: Grow (C,2)
Input : a PCC, Gaussian noisy variance 2
Output : a PCC’ after growing operation
1old2new a dictionary mapping input PC units n2Cto units of the growed PC
2foreachn2Cdo// visit children before parents
3 ifnis an input unit then old2new [n] (n;deepcopy (n))
4 else
5 chs_1;chs_2 [old2new [c][0] forcinin(n)];[old2new [c][1] forcinin(n)]
6 ifnis a product unit then old2new [n] (N(chs_1);N(chs_2))
7 else ifnis a sum unit then
8 n1;n2 L([chs_1;chs_2]);L([chs_1;chs_2])
9jni normalize ([jn;jn]))N(1;2) foriin [1;2]
10 old2new [n] (n1;n2)
11return old2new [r][0]//ris the root unit of C
15

--- PAGE 16 ---
Algorithm 4 computes the circuit ﬂows of a sample xgiven PCCwith parameters though one
forward pass (line 1) and one backward pass (line 2-8).
Algorithm 4: CircuitFlow (C,,x)
Input : a PCCwith parameters ; samplex
Output : circuit ﬂow ow[n;c]for each edge (n;c)andow[n]for each node n
18n2C;p[n] pn(x)computed as in Equation 1
2For rootnr;ow[n] 1
3forn2C in backward order do
4 ow[n] P
g2out(n)ow[g]
5 ifn is a sum node then
68c2in(n);ow[n;c] cjnp[c]
p[n]ow[n]
7 else
88c2in(n);ow[n;c] ow[n]
Algorithm 5 shows the pipeline of mini-batches Expectation Maximization algorithm given PC C,
datasetD, batch sizeBand learning rate .
Algorithm 5: StochasticEM (C,D;B,)
Input : a PCC; datasetD; batch sizeB; learning rate 
Output : parametersestimated fromD
1 random initialization
2For rootnr;ow[n] 1
3while not converged or early stopped do
4D0 Brandom samples from D
5 ow P
x2D0CircuitFlow (C;;x)
6 forsum unitnand its child cdo
7new
cjn ow[n;c]=ow[n]
8cjn (new)
cjn+ (1 )cjn
B Proofs
In this section, we provide detailed proofs of Theorem 1 (Section B.1) and Theorem 2 (Section B.2).
B.1 Pruning One Edge over One Example
Lemma 1 (Pruning One Edge Log-Likelihood Lower Bound) .For a PCCand a samplex, the loss
of log-likelihood by pruning away edge (n;c)is
LL(fxg;C;f(n;c)g) = log1 cjn
1 cjn+cjnFn(x) Fn;c(x)
 log(1 Fn;c(x)):
Proof. For notation simplicit, denote the probability of units m(resp.n) in the original (resp. pruned)
PC given sample xaspm(x)(resp.p0
n(x)). As a slight extension of Deﬁnition 4, we deﬁne Fn(x;m)
as the ﬂow of unit nw.r.t. the PC rooted at m.
The proof proceeds by induction over the PC’s root unit. That is, we ﬁrst consider pruning (n;c)w.r.t.
the PC rooted at n. Then, in the induction step, we prove that if the lemma holds for PC rooted at m,
then it also holds for PC rooted at any parent unit of m. Instead of directly proving the statement in
Lemma 1, we ﬁrst prove that for any root node m, the following holds:
pm(x) p0
m(x) =Fn(x;m)pm(x)1
1 Fn;c(x;m)
Fn(x;m) 
1 
: (4)
16

--- PAGE 17 ---
Base case: pruning an edge of the root unit. That is, the root unit of the PC is n. In this case, we have
pn(x) p0
n(x) =X
c02in(n)c0jnpc(x) X
c02in(n)nc0
c0jnp0
c(x)
=cjnpc(x) +X
c02in(n)ncc0jnpc(x) X
c02in(n)nc0
c0jnpc(x); (5)
where0
cjndenotes the normalized parameter corresponding to edge (n;c)in the pruned PC. Speciﬁ-
cally, we have
8m2in(n)nc; 0
mjn=mjnP
c02in(n)ncc0jn=mjn
1 cjn:
For notation simplicity, denote :=cjn. Plug in the above deﬁnition into Equation 5, we have
pn(x) p0
n(x) =cjnpc(x) +X
c02in(n)ncc0jnpc(x) 1
1 X
c02in(n)ncc0jnpc(x)
=cjnpc(x) 
1 X
c02in(n)ncc0jnpc(x)
=cjnpc(x) 
1 (pn(x) cjnpc(x))
=1
1 cjnpc(x) 
1 pn(x)
(a)=1
1 pn(x)Fn;c(x;n)
Fn(x;n) 
1 pn(x)
=Fn(x;n)pn(x)1
1 Fn;c(x;n)
Fn(x;n) 
1 
; (6)
where (a)follows from the fact that Fn(x;n) = 1 andFn;c(x;n) =cjnpc(x)=pn(x).
Inductive case #1: suppose Equation 4 holds for m. If product unit dis a parent of m, we show that
Equation 4 also holds for d:
pd(x) p0
d(x) =Y
n02in(d)pn0(x) Y
n02in(d)p0
n0(x)
= (pm(x) p0
m(x))Y
n02in(d)nmpn0(x)
(a)=Fn(x;m)pm(x)1
1 Fn;c(x;m)
Fn(x;m) 
1 
Y
n02in(d)nmpn0(x)
(b)=Fn(x;d)pd(x)1
1 Fn;c(x;d)
Fn(x;d) 
1 
;
where (a)is the inductive step that applies Equation 6; (b)follows from the fact that (note that dis a
product unit) Fn(x;m) =Fn(x;d)andFn;c(x;m) =Fn;c(x;d).
Inductive case #2: for sum unit d, suppose Equation 4 holds for m, wherem2A iffm2in(d)and
mis an ancester of nandc. Assume all other children of dare not ancestoer of n, we show that
17

--- PAGE 18 ---
Equation 4 also holds for d:
pd(x) p0
d(x) =mjd(pm(x) p0
m(x))
=mjdFn(x;m)pm(x)1
1 Fn;c(x;m)
Fn(x;m) 
1 
=mjdFn(x;m)pm(x)1
1 Fn;c(x;d)
Fn(x;d) 
1 
=mjdFn(x;d)P
m02in(d)m0jdpm0(x)
mjdpm(x)pm(x)1
1 Fn;c(x;d)
Fn(x;d) 
1 
=Fn(x;d)X
m02in(d)m0jdpm0(x)
1
1 Fn;c(x;d)
Fn(x;d) 
1 
=Fn(x;d)pd(x)1
1 Fn;c(x;d)
Fn(x;d) 
1 
:
Therefore, following Equation 4 for root r, we have
pr(x) p0
r(x)
pr(x)=1
1 Fn;c(x;r) 
1 Fn(x;r)
,p0
r(x)
pr(x)= 1 +
1 Fn(x;r) 1
1 Fn;c(x;r)
Therefore, we have
LL(fxg;C;f(n;c)g) = logpr(x) logp0
r(x)
=1
jDjX
x2Dlog1 cjn
1 cjn+cjnFn(x;r) Fn;c(x;r)
(a)
  log(1 Fn;c(x));
where (a)follows from the fact that Fn;c(x)Fn(x).
Theorem 1 follows directly from Lemma 1 by noting that for any dataset D,LL(D;C;f(n;c)g) =
1
jDjLL(fxg;C;f(n;c)g).
B.2 Pruning Multiple Edges
Proof. Similar to the proof of Lemma 1, we prove Theorem 2 by induction. Different from Lemma 1,
we induce a slightly different objective:
pm(x) p0
m(x)X
(n;c)2E\ des(m)Fn(x;m)pm(x)1
1 cjnFn;c(x;m)
Fn(x;m) cjn
1 cjn
;(7)
where des(n)is the set of descendent units of n.
Base case: the base case follows directly from the proof of Lemma 1, and lead to the conclusion in
Equation 6.
18

--- PAGE 19 ---
Inductive case #1: suppose for all children of a product unit d, Equation 7 holds, we show that
Equation 7 also holds for d:
pd(x) p0
d(x) =Y
m2in(d)pm(x) Y
m2in(d)p0
m(x)
=Y
m2in(d)pm(x) Y
m2in(d)
pm(x) (pm(x) p0
m(x))
X
m2in(d)
pm(x) p0
m(x))
Y
m02in(d)nmpm0(x)
(a)
X
m2in(d)X
(n;c)2E\ des(m)Fn(x;d)pd(x)1
1 cjnFn;c(x;m)
Fn(x;m) cjn
1 cjn
X
(n;c)2E\ des(d)Fn(x;d)pd(x)1
1 cjnFn;c(x;d)
Fn(x;d) cjn
1 cjn
;
where (a)uses the deﬁnition that pd(x) =Q
m2in(d)pm(x).
Inductive case #2: suppose for all children of a sum unit d, Equation 7 holds, we show that Equation 7
also holds for d:
pd(x) p0
d(x) =X
m2in(d)\(d;m)62Emjd
pm(x) p0
m(x)
+X
m2in(d)\(d;m)2Emjd
pm(x) p0
m(x)
(a)=X
m2in(d)\(d;m)62Emjd
pm(x) p0
m(x)
+X
m2in(d)\(d;m)2EmjdFn(x;m)pm(x)1
1 cjnFn;c(x;m)
Fn(x;m) cjn
1 cjn
;
where (a)follows from the base case of the induction. Next, we focus on the ﬁrst term of the above
equation:X
m2in(d)\(d;m)62Emjd
pm(x) p0
m(x)
X
m2in(d)\(d;m)62EX
(n;c)2E\ des(m)mjd
pm(x) p0
m(x)
X
m2in(d)\(d;m)62EX
(n;c)2E\ des(m)mjdFn(x;m)pm(x)1
1 cjnFn;c(x;m)
Fn(x;m) cjn
1 cjn
X
(n;c)2E\ des(d)Fn(x;d)pd(x)1
1 cjnFn;c(x;d)
Fn(x;d) cjn
1 cjn
;
where the derivation of the last inequality follows from the corresponding steps in the proof of
Lemma 1.
Therefore, from Equation 7, we can conclude that
LL(D;C;E) 1
jDjX
xlog(1 X
(n;c)2EFn;c(x)):
Finally, we prove the approximation step in Equation 3. Let () =P
(n;c)2EFn;c()2[0;1). We
have,
RHS = X
x2Dlog(1 (x)) = X
x2D1X
k=1 (x)k
k(Taylor expansion )X
x2D1X
k=1(x)k
=X
x2D(x)
1 (x)=1
1 X
x2D(x) =1
1 X
(n;c)2EX
x2DFn;c(x) =1
1 X
(n;c)2EFn;c(D):
19

--- PAGE 20 ---
C Experiments Details
Hardware speciﬁcations All experiments are performed on a server with 32 CPUs, 126G Memory,
and NVIDIA RTX A5000 GPUs with 26G Memory. In all experiments, we only use a single GPU on
the server.
C.1 Datasets
For MNIST-family datasets, we split 5% of training set as validation set for early stopping. For Penn
Tree Bank dataset, we follow the setting in Mikolov et al. [29] to split a training, validation, and test
set. Table 3 lists the all the dataset statistics.
Table 3: Dataset statistics including number of variables ( #vars ), number of categories for each
variable ( #cat), and number of samples for training, validation and test set ( #train ,#valid ,#test ).
Dataset n(#vars )k(#cat) #train #valid #test
MNIST 2828 256 57000 3000 10000
EMNIST(MNIST) 2828 256 57000 3000 10000
EMNIST(Letters) 2828 256 118560 6240 20800
EMNIST(Balanced) 2828 256 107160 5640 18800
EMNIST(ByClass) 2828 256 663035 34897 116323
FashionMNIST 2828 256 57000 3000 10000
Penn Tree Bank 288 50 42068 3370 3761
C.2 Learning Hidden Chow-Liu Trees
HCLT structures. Adopting hidden chow liu tree (HCLT) PC architecture as in Liu and Van den
Broeck [25], we reimplement the learning process to speed it up and use a different training pipeline
and hyper-parameters tuning.
EM parameter learning We adopt the EM parameter learning algorithm introduced in Choi et al.
[4], which computes the EM update target parameters using circuit ﬂows. We use a stochastic
mini-batches EM algorithm. Denoting newas the EM update target computed from a mini-batch of
samples, and we update the targeting parameter with a learning rate :t+1 new+ (1 )t.
is piecewise-linearly annealed from [1:0;0:1],[0:1;0:01],[0:01;0:001], and each piece is trained T
epochs.
Hyper-parameters searching. For all the experiments, the hyper-parameters are searched from
•h2f8;16;32;64;128;256g, the hidden size of HCLT structures;
•2f0:0001;0:001;0:01;0:1;1:0g, Laplace smoothing factor;
•B2f128;256;512;1024g, batch-size in mini-batches EM algorithm;
•piecewise-linearly annealed from [1:0;0:1],[0:1;0:01],[0:01;0:001], where each piece is
called one mini-batch EM phase. Usually the algorithm will start to overﬁt as validation set
and stop at the third phase;
•T= 100 , number of epochs for each mini-batch EM phase.
The PC size is quadratically growing with hidden size h, thus it is inefﬁcient to do a grid search
among the entire hyper-parameters space. What we do is to ﬁst do a grid search when h= 8 or
h= 16 to ﬁnd the best Laplace smoothing factor and batch-size Bfor each dataset, and then
ﬁxandBto train a PC with larger hidden size h2f32;64;128;256g. The best tuned Bis in
f256;512g, which is different for different hidden size h, and the best tuned is0:01.
20

--- PAGE 21 ---
C.3 Details of Section 6.1
Sparse PC (ours). Given an HCLT learned in Section C.2 as initial PC, we use the structure
learning process proposed in Section 5. Speciﬁcally, starts from initial HCLT, for each iteration, we
(1) prune 75% of the PC parameters, and (2) grow PC size with Gaussian variance , (3) ﬁnetuing PC
using mini-batches EM parameter learning with learning rate . We prune and grow PC iteratively
until the validation set likelihood is overﬁtted . The hyper-parameters are searched from
•2f0:1;0:3;0:5g, Gaussian variance in growing operation;
•, piecewise-linearly annealed from [0:1;0:01],[0:01;0:001];
•T= 50 , number of epochs for each mini-batch EM phase;
• forandB, we use the tuned best number from Section C.2.
HCLT. The HLCT experiments in Table 1 are performed following the original paper (Code https:
//github.com/UCLA-StarAI/Tractable-PC-Regularization ), which is different from the
leaning pipeline we use as our inital PC (Section C.2).
SPN. We reimplement the SPN architecture ourselves following Peharz et al. [34] and train it with
the same mini-batch pipeline as HCLT.
IDF. We run all experiments with the code in the GitHub repo provided by the authors. We adopt
an IDF model with the following hyperparameters: 8 ﬂow layers per level; 2 levels; densenets with
depth 6 and 512 channels; base learning rate 0.001; learning rate decay 0.999. The algorithm adopts
an CPU-based entropy coder rANS.
BitSwap. We train all models using the following author-provided script: https://github.com/
fhkingma/bitswap/blob/master/model/mnist_train .
BB-ANS. All experiments are performed using the following ofﬁcial code https://github.com/
bits-back/bits-back .
McBits. All experiments are performed using the following ofﬁcial code https://github.com/
ryoungj/mcbits .
C.4 Details of Section 6.2
For all experiments in Section 6.2, we use the best tuned andBfrom Section C.2 and hidden size h
ranging fromf16;32;64;128g. For experiments “What is the Smallest PC for the Same Likelihood?”,
the hyper-parameters are searched from
•k2f0:05;0:1;0:3g, percentage of parameters to prune each iteration;
•, piecewise-linearly annealed from [0:3;0:1],[0:1;0:01],[0:01;0:001];
•T= 50 , number of epochs for each mini-batch EM phase;
For experiments “What is the Best PC Given the Same Size?”, we use the same setting as in
Section C.3.
21
