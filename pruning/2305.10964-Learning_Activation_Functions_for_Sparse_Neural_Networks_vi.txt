# Học Hàm Kích Hoạt cho Mạng Neuron Thưa
Mohammad Loni∗1Aditya Mohan∗2Mehdi Asadi3Marius Lindauer2
1Khoa Khoa học Máy tính và Kỹ thuật Phần mềm, Đại học Mälardalen, Thụy Điển
2Viện Trí tuệ Nhân tạo, Đại học Leibniz Hannover, Đức
3Khoa Kỹ thuật Điện, Đại học Tarbiat Modares, Tehran, Iran

Tóm tắt Mạng Neuron Thưa (SNNs) có thể tiềm năng thể hiện hiệu suất tương tự như các đối tác dày đặc của chúng trong khi tiết kiệm đáng kể năng lượng và bộ nhớ khi suy luận. Tuy nhiên, việc giảm độ chính xác gây ra bởi SNNs, đặc biệt ở tỷ lệ cắt tỉa cao, có thể là một vấn đề trong điều kiện triển khai quan trọng. Trong khi các nghiên cứu gần đây giảm thiểu vấn đề này thông qua các kỹ thuật cắt tỉa tinh vi, chúng tôi chuyển hướng tập trung sang một yếu tố bị bỏ qua: các siêu tham số và hàm kích hoạt. Các phân tích của chúng tôi đã cho thấy việc giảm độ chính xác có thể được quy cho thêm vào (i) Sử dụng ReLU làm lựa chọn mặc định cho hàm kích hoạt một cách nhất quán, và (ii) Tinh chỉnh SNNs với cùng siêu tham số như các đối tác dày đặc. Do đó, chúng tôi tập trung vào việc học một cách thức mới để tinh chỉnh hàm kích hoạt cho mạng thưa và kết hợp chúng với một chế độ tối ưu hóa siêu tham số (HPO) riêng biệt cho mạng thưa. Bằng cách tiến hành thí nghiệm trên các mô hình DNN phổ biến (LeNet-5, VGG-16, ResNet-18, và EfficientNet-B0) được huấn luyện trên các bộ dữ liệu MNIST, CIFAR-10, và ImageNet-16, chúng tôi cho thấy sự kết hợp mới của hai phương pháp này, được gọi là Tìm kiếm Hàm Kích hoạt Thưa, viết tắt: SAFS, mang lại cải thiện độ chính xác tuyệt đối lên đến 15.53%, 8.88%, và 6.33% cho LeNet-5, VGG-16, và ResNet-18 so với các giao thức huấn luyện mặc định, đặc biệt ở tỷ lệ cắt tỉa cao.

1 Giới thiệu
Mạng Neuron Sâu, trong khi đã thể hiện hiệu suất mạnh mẽ trên nhiều tác vụ khác nhau, lại tốn kém tính toán để huấn luyện và triển khai. Khi kết hợp với các mối quan tâm về quyền riêng tư, hiệu quả năng lượng, và thiếu kết nối ổn định, điều này đã dẫn đến sự quan tâm gia tăng trong việc triển khai DNNs trên các thiết bị có tài nguyên hạn chế như vi điều khiển và FPGAs (Chen và Ran, 2019).

Các nghiên cứu gần đây đã cố gắng giải quyết vấn đề này bằng cách giảm dấu chân bộ nhớ khổng lồ và tiêu thụ điện năng của DNNs. Chúng bao gồm lượng tử hóa (Zhou et al., 2017), chưng cất kiến thức (Hinton et al., 2015), phân tích thứ hạng thấp (Jaderberg et al., 2014), và thưa thớt hóa mạng sử dụng cắt tỉa không có cấu trúc (còn gọi là Mạng Neuron Thưa) (Han et al., 2015).

Trong số này, Mạng Neuron Thưa (SNNs) đã cho thấy lợi ích đáng kể thông qua khả năng loại bỏ các trọng số dư thừa (Hoefler et al., 2021). Tuy nhiên, chúng gặp phải việc giảm độ chính xác, đặc biệt ở tỷ lệ cắt tỉa cao; ví dụ, Mousavi et al. (2022) báo cáo giảm ≈54% trong độ chính xác top-1 cho MobileNet-v2 (Sandler et al., 2018) được huấn luyện trên ImageNet so với không cắt tỉa.

Trong khi phần lớn lỗi cho việc giảm độ chính xác này thuộc về việc thưa thớt hóa, chúng tôi đã xác định hai yếu tố liên quan chưa được khám phá đầy đủ có thể tác động thêm vào nó: (i) Các hàm kích hoạt của các đối tác thưa không bao giờ được tối ưu hóa, với Đơn vị Tuyến tính Chỉnh lưu (ReLU) (Nair và Hinton, 2010) là lựa chọn mặc định. (ii) Các siêu tham số huấn luyện của mạng neuron thưa thường được giữ giống như các đối tác dày đặc của chúng.

Một bước tự nhiên, do đó, là hiểu cách các hàm kích hoạt tác động đến quá trình học cho SNNs. Trước đây, Jaiswal et al. (2022) và Tessera et al. (2021) đã chứng minh rằng ReLU làm giảm khả năng huấn luyện của SNNs vì những thay đổi đột ngột trong gradient xung quanh số không dẫn đến việc chặn luồng gradient. Ngoài ra, Apicella et al. (2021) đã cho thấy rằng một hàm kích hoạt phổ biến không thể ngăn chặn các vấn đề học tập điển hình như gradient biến mất. Trong khi lĩnh vực Học Máy Tự động (AutoML) (Hutter et al., 2019) trước đây đã khám phá việc tối ưu hóa hàm kích hoạt của DNNs dày đặc (Ramachandran et al., 2018; Loni et al., 2020; Bingham et al., 2020), hầu hết các phương pháp này đòi hỏi một lượng lớn tài nguyên tính toán (lên đến 2000 giờ GPU (Bingham et al., 2020)), dẫn đến thiếu quan tâm trong việc tối ưu hóa hàm kích hoạt cho các vấn đề học sâu khác nhau. Mặt khác, các nỗ lực cải thiện độ chính xác của SNNs hoặc sử dụng tìm kiếm kiến trúc thưa (Fedorov et al., 2019; Mousavi et al., 2022) hoặc chế độ huấn luyện thưa (Srinivas et al., 2017). Theo hiểu biết của chúng tôi, không có phương pháp hiệu quả nào để tối ưu hóa hàm kích hoạt trong huấn luyện SNN.

Đóng góp của Bài báo: (i) Chúng tôi phân tích tác động của hàm kích hoạt và siêu tham số huấn luyện lên hiệu suất của kiến trúc CNN thưa. (ii) Chúng tôi đề xuất một phương pháp AutoML mới, được gọi là SAFS, để điều chỉnh hàm kích hoạt và siêu tham số huấn luyện của mạng neuron thưa để tách biệt khỏi các giao thức huấn luyện của các đối tác dày đặc của chúng. (iii) Chúng tôi chứng minh cải thiện hiệu suất đáng kể khi áp dụng SAFS với cắt tỉa độ lớn không có cấu trúc cho LeNet-5 trên bộ dữ liệu MNIST (LeCun et al., 1998), mạng VGG-16 và ResNet-18 được huấn luyện trên bộ dữ liệu CIFAR-10 (Krizhevsky et al., 2014), và mạng ResNet-18 và EfficientNet-B0 được huấn luyện trên bộ dữ liệu ImageNet-16 (Chrabaszcz et al., 2017), khi so sánh với các giao thức huấn luyện mặc định, đặc biệt ở mức độ thưa thớt cao.

2 Công trình Liên quan
Theo hiểu biết tốt nhất của chúng tôi, SAFS là khung công tác tự động đầu tiên điều chỉnh các hàm kích hoạt của mạng neuron thưa sử dụng phương pháp tối ưu hóa đa giai đoạn. Nghiên cứu của chúng tôi cũng làm sáng tỏ thực tế rằng việc điều chỉnh siêu tham số đóng vai trò quan trọng trong độ chính xác của mạng neuron thưa. Cải thiện độ chính xác của mạng neuron thưa đã được nghiên cứu rộng rãi trong quá khứ. Các nghiên cứu trước đây chủ yếu được phân loại như (i) đề xuất các tiêu chí khác nhau để chọn trọng số không quan trọng, (ii) cắt tỉa khi khởi tạo hoặc huấn luyện, và (iii) tối ưu hóa các khía cạnh khác của mạng thưa ngoài tiêu chí cắt tỉa. Trong phần này, chúng tôi thảo luận về các phương pháp này và so sánh chúng với SAFS, và xem xét ngắn gọn nghiên cứu tiên tiến về tối ưu hóa hàm kích hoạt của mạng dày đặc.

2.1 Tối ưu hóa Mạng Neuron Thưa
Cắt tỉa Trọng số Không quan trọng. Một số nghiên cứu đã đề xuất cắt tỉa các tham số trọng số dưới một ngưỡng cố định, bất kể mục tiêu huấn luyện (Han et al., 2015; Li et al., 2016; Zhou et al., 2019). Gần đây, Azarian et al. (2020) và Kusupati et al. (2020) đề xuất ngưỡng có thể huấn luyện theo từng lớp để xác định giá trị tối ưu cho mỗi lớp.

Cắt tỉa khi Khởi tạo hoặc Huấn luyện. Các phương pháp này nhằm bắt đầu thưa thay vì trước tiên tiền huấn luyện một mạng dày đặc rồi sau đó cắt tỉa. Để xác định trọng số nào nên giữ hoạt động khi khởi tạo, chúng sử dụng các tiêu chí như sử dụng độ nhạy kết nối (Lee et al., 2018) và bảo toàn tính nổi bật synapse (Tanaka et al., 2020). Mặt khác, Mostafa và Wang (2019); Mocanu et al. (2018); Evci et al. (2020) đề xuất tận dụng thông tin thu thập được trong quá trình huấn luyện để cập nhật động mẫu thưa thớt của kernels.

Tối ưu hóa Mạng Thưa Khác. Evci et al. (2019) điều tra cảnh quan mất mát của mạng neuron thưa và Frankle et al. (2020) giải quyết cách nó bị tác động bởi nhiễu của Gradient Descent Ngẫu nhiên (SGD). Cuối cùng, Lee et al. (2020) nghiên cứu tác động của khởi tạo trọng số lên hiệu suất của mạng thưa. Trong khi công trình của chúng tôi cũng nhằm cải thiện hiệu suất của mạng thưa và cho phép chúng đạt được hiệu suất tương tự như các đối tác dày đặc của chúng, chúng tôi thay vào đó tập trung vào tác động của việc tối ưu hóa hàm kích hoạt và siêu tham số của mạng neuron thưa trong một thiết lập HPO kết hợp.

2.2 Tìm kiếm Hàm Kích hoạt
Việc lựa chọn hàm kích hoạt không phù hợp dẫn đến mất thông tin trong quá trình lan truyền tiến và các vấn đề gradient biến mất và/hoặc bùng nổ trong quá trình lan truyền ngược (Hayou et al., 2019). Để tìm các hàm kích hoạt tối ưu, một số nghiên cứu đã tự động tinh chỉnh hàm kích hoạt cho DNNs dày đặc, dựa trên tính toán tiến hóa (Bingham et al., 2020; Basirat và Roth, 2021; Nazari et al., 2019), học tăng cường (Ramachandran et al., 2018), hoặc gradient descent để thiết kế các hàm tham số (Tavakoli et al., 2021; Zamora et al., 2022).

Mặc dù thành công của các phương pháp này, việc tinh chỉnh tự động hàm kích hoạt cho mạng dày đặc là không đáng tin cậy cho bối cảnh thưa vì không gian tìm kiếm cho hàm kích hoạt cho mạng dày đặc không tối ưu cho mạng thưa (Dubowski, 2020). Các hoạt động tương tự thành công trong mạng dày đặc có thể làm giảm mạnh luồng gradient mạng trong mạng thưa (Tessera et al., 2021). Ngoài ra, các phương pháp hiện tại gặp phải chi phí tìm kiếm đáng kể; ví dụ, Bingham et al. (2020) đòi hỏi 1000 giờ GPU mỗi lần chạy trên NVIDIA®GTX 1080Ti. Jin et al. (2016) cho thấy sự vượt trội của SReLU so với ReLU khi huấn luyện mạng thưa vì nó cải thiện luồng gradient của mạng. Tuy nhiên, SReLU đòi hỏi học bốn tham số bổ sung cho mỗi neuron. Trong trường hợp triển khai mạng có hàng triệu đơn vị ẩn, điều này có thể dễ dàng dẫn đến chi phí tính toán và bộ nhớ đáng kể khi suy luận. SAFS, mặt khác, thống nhất tìm kiếm cục bộ ở mức meta với gradient descent để tạo ra chiến lược tối ưu hóa hai tầng và đạt được hiệu suất vượt trội với hội tụ tìm kiếm nhanh hơn so với tiên tiến.

3 Kiến thức Chuẩn bị
Trong phần này, chúng tôi phát triển ký hiệu cho các phần sau bằng cách giới thiệu chính thức hai vấn đề mà chúng tôi giải quyết: Thưa thớt hóa Mạng và Tối ưu hóa Siêu tham số.

3.1 Thưa thớt hóa Mạng
Thưa thớt hóa mạng là một kỹ thuật hiệu quả để cải thiện hiệu quả của DNNs cho các ứng dụng có tài nguyên tính toán hạn chế. Zhan và Cao (2019) báo cáo rằng thưa thớt hóa mạng có thể tạo điều kiện tiết kiệm thời gian suy luận ResNet-18 được huấn luyện trên ImageNet trên thiết bị di động lên đến 29.5×. Thưa thớt hóa mạng thường bao gồm ba giai đoạn:

1. Tiền huấn luyện: Huấn luyện một mô hình lớn, quá tham số hóa. Cho một metric mất mát L_train và tham số mạng θ, điều này có thể được công thức hóa như nhiệm vụ tìm các tham số θ*_pre tối thiểu hóa L_train trên dữ liệu huấn luyện D_train:
θ*_pre ∈ argmin_{θ∈Θ} L_train(θ; D_train)     (1)

2. Cắt tỉa: Sau khi huấn luyện mô hình dày đặc, bước tiếp theo là loại bỏ các tensor trọng số có tầm quan trọng thấp của mạng đã được tiền huấn luyện. Điều này có thể được thực hiện theo từng lớp, theo kênh, và toàn mạng. Các cơ chế thông thường hoặc đơn giản đặt một phần trăm nhất định trọng số (tỷ lệ cắt tỉa) về zero, hoặc học một mặt nạ Boolean m* trên vector trọng số. Cả hai khái niệm này có thể được nắm bắt một cách tổng quát theo cách tương tự như công thức huấn luyện dày đặc nhưng với một metric mất mát riêng biệt L_prune. Mục tiêu ở đây là thu được một mặt nạ cắt tỉa m*, trong đó ⊙ đại diện cho hoạt động masking và N đại diện cho kích thước của mặt nạ:
m* ∈ argmin_{m∈{0,1}^N} L_prune(θ*_pre ⊙ m; D_train)
s.t. ||m*||_0 ≤ ε     (2)
trong đó ε là một ngưỡng về số lượng tối thiểu trọng số được mask.

3. Tinh chỉnh: Bước cuối cùng là huấn luyện lại mạng đã cắt tỉa để lấy lại độ chính xác ban đầu sử dụng mất mát tinh chỉnh L_fine, có thể giống với mất mát huấn luyện hoặc khác loại:
θ*_fine ∈ argmin_{θ∈Θ} L_fine(θ; θ*_pre ⊙ m*, D_train)     (3)

Đối với giai đoạn cắt tỉa, SAFS sử dụng phương pháp cắt tỉa độ lớn phổ biến (Han et al., 2015) bằng cách loại bỏ một phần trăm nhất định trọng số có độ lớn thấp hơn. So với các phương pháp cắt tỉa có cấu trúc (Liu et al., 2018), phương pháp cắt tỉa độ lớn cung cấp tính linh hoạt cao hơn và tỷ lệ nén tốt hơn |θ*_fine|/|θ*_pre| × 100. Quan trọng, SAFS độc lập với thuật toán cắt tỉa; do đó, nó có thể tối ưu hóa bất kỳ mạng thưa nào.

3.2 Tối ưu hóa Siêu tham số (HPO)
Chúng tôi ký hiệu không gian siêu tham số của mô hình là Λ từ đó chúng tôi lấy mẫu một cấu hình siêu tham số λ = (λ_1, ..., λ_d) để được tinh chỉnh bởi một số phương pháp HPO. Chúng tôi giả sử c: λ → R là một hàm chi phí hộp đen ánh xạ cấu hình được chọn λ đến một metric hiệu suất, như model-error. Mục tiêu của HPO sau đó có thể được tóm tắt như nhiệm vụ tìm một cấu hình tối ưu λ* tối thiểu hóa c. Cho các tham số tinh chỉnh θ*_fine thu được trong Phương trình (3), chúng tôi định nghĩa chi phí như tối thiểu hóa một mất mát L_hp trên bộ dữ liệu validation D_val như một vấn đề tối ưu hóa hai cấp:
λ* ∈ argmin_{λ∈Λ} c(λ) = argmin_{λ∈Λ} L_hp(θ*_fine(λ); D_val)     (4)
s.t.
θ*_fine(λ) ∈ argmin_{θ∈Θ} L_fine(θ; θ*_pre ⊙ m*, D_train, λ)

Chúng tôi lưu ý rằng về nguyên tắc HPO cũng có thể được áp dụng cho việc huấn luyện mô hình gốc (Phương trình (1)), nhưng chúng tôi giả sử rằng bản gốc đã được đưa ra và chúng tôi chỉ quan tâm đến việc thưa thớt hóa.

4 Tìm Hàm Kích hoạt cho Mạng Thưa
Mục đích của SAFS là tìm một cấu hình siêu tham số tối ưu cho mạng đã cắt tỉa với trọng tâm vào hàm kích hoạt. Cho thiết lập HPO được mô tả trong Phần 3.2, chúng tôi bây giờ giải thích cách công thức hóa vấn đề tìm kiếm hàm kích hoạt và những gì cần thiết để giải quyết nó.

4.1 Mô hình hóa Hàm Kích hoạt
Sử dụng kỹ thuật tối ưu hóa đòi hỏi tạo ra một không gian tìm kiếm chứa các hàm kích hoạt ứng viên đầy hứa hẹn. Không gian tìm kiếm quá ràng buộc có thể không chứa các hàm kích hoạt mới (tính biểu đạt) trong khi tìm kiếm trong không gian tìm kiếm quá lớn có thể khó khăn (kích thước) (Ramachandran et al., 2018). Do đó, việc đạt được sự cân bằng giữa tính biểu đạt và kích thước của không gian tìm kiếm là một thách thức quan trọng trong việc thiết kế không gian tìm kiếm.

Để giải quyết vấn đề này, chúng tôi mô hình hóa các hàm kích hoạt tham số như một sự kết hợp của một toán tử unary f và hai yếu tố tỷ lệ có thể học α, β. Do đó, cho một đầu vào x và đầu ra y, hàm kích hoạt có thể được công thức hóa như y = αf(βx), có thể được đại diện thay thế như một đồ thị tính toán được hiển thị trong Hình 3a.

Hình 1 minh họa một ví dụ về việc điều chỉnh các tham số có thể học α và β của hàm kích hoạt Swish. Chúng ta có thể trực quan thấy rằng việc sửa đổi các tham số có thể học được đề xuất cho một toán tử unary mẫu cung cấp cho mạng thưa sự linh hoạt bổ sung để tinh chỉnh hàm kích hoạt (Godfrey, 2019; Bingham và Miikkulainen, 2022). Các ví dụ về hàm kích hoạt mà chúng tôi xem xét trong công trình này đã được liệt kê trong Phụ lục E.

Đối với mạng thưa, biểu diễn này cho phép triển khai hiệu quả cũng như tham số hóa hiệu quả. Như chúng tôi giải thích thêm trong Phần 4.2, bằng cách coi điều này như một quá trình tối ưu hóa hai giai đoạn, trong đó việc tìm kiếm f là một vấn đề tối ưu hóa rời rạc và việc tìm kiếm α, β được xen kẽ với tinh chỉnh, chúng tôi có thể làm cho quá trình tìm kiếm hiệu quả trong khi nắm bắt bản chất của tỷ lệ đầu vào-đầu ra và biến đổi chức năng phổ biến với hàm kích hoạt.

Lưu ý rằng SAFS thuộc loại hàm kích hoạt thích ứng do giới thiệu các tham số có thể huấn luyện (Dubey et al., 2022). Các tham số này cho phép hàm kích hoạt điều chỉnh mô hình một cách mượt mà với độ phức tạp của bộ dữ liệu (Zamora et al., 2022). Trái ngược với các hàm kích hoạt thích ứng phổ biến như PReLU và Swish, SAFS tự động hóa việc tinh chỉnh hàm kích hoạt trên một họ đa dạng các hàm kích hoạt cho mỗi lớp của mạng với siêu tham số được tối ưu hóa.

4.2 Quy trình Tối ưu hóa
SAFS thực hiện tối ưu hóa theo từng lớp tức là chúng tôi dự định tìm hàm kích hoạt cho mỗi lớp. Cho các chỉ số lớp i = 1, ..., L của mạng có độ sâu L, một thuật toán tối ưu hóa cần có khả năng chọn một toán tử unary f*_i và tìm các yếu tố tỷ lệ phù hợp (α*_i, β*_i). Chúng tôi công thức hóa chúng như hai hàm mục tiêu độc lập, được giải quyết trong một quy trình tối ưu hóa hai giai đoạn kết hợp tối ưu hóa rời rạc và ngẫu nhiên. Hình 2 hiển thị tổng quan về pipeline SAFS.

Giai đoạn 1: Tìm kiếm Toán tử Unary. Giai đoạn đầu tiên là tìm các toán tử unary sau khi mạng đã được cắt tỉa. Quan trọng, bước tinh chỉnh chỉ xảy ra sau khi tối ưu hóa này cho hàm kích hoạt đã được hoàn thành. Chúng tôi mô hình hóa nhiệm vụ tìm toán tử unary tối ưu cho mỗi lớp như một vấn đề tối ưu hóa rời rạc. Cho một tập hợp các hàm được định nghĩa trước F = {f_1, f_2, ..., f_n}, chúng tôi định nghĩa một không gian F của các chuỗi toán tử có thể ψ = ⟨f_i|f_i ∈ F⟩_{i∈{1,...,L}} ∈ F có kích thước L. Nhiệm vụ của chúng tôi là tìm một chuỗi ψ sau giai đoạn cắt tỉa (Mục 2). Vì các tham số mạng được tiền huấn luyện θ*_pre và mặt nạ cắt tỉa m* đã được khám phá, chúng tôi giữ chúng cố định và sử dụng chúng như một điểm khởi tạo cho tối ưu hóa hàm kích hoạt. Nhiệm vụ được công thức hóa như tìm các toán tử tối ưu cho các tham số mạng, như được hiển thị trong Phương trình (5). Trong bước này, các tham số α và β được đặt về 1 để tập trung vào lớp hàm trước.

ψ* ∈ argmin_{ψ∈F} L_train(θ*_pre ⊙ m*, ψ; D_train)     (5)

Cho bản chất rời rạc của Phương trình (5), chúng tôi sử dụng Late Acceptance Hill Climbing (LAHC) (Burke và Bykov, 2017) để giải quyết nó một cách lặp đi lặp lại (Vui lòng tham khảo Phụ lục A để so sánh với các thuật toán tìm kiếm khác). LAHC là một thuật toán Hill Climbing sử dụng một bản ghi lịch sử - Độ dài Lịch sử - của các giá trị mục tiêu của các giải pháp đã gặp trước đó để quyết định có chấp nhận một giải pháp mới hay không. Nó cung cấp cho chúng tôi hai lợi ích: (i) Là một phương pháp tìm kiếm bán cục bộ, LAHC hoạt động trên không gian rời rạc và nhanh chóng tìm kiếm không gian để tìm toán tử unary. (ii) LAHC mở rộng thuật toán hill-climbing vanilla (Selman và Gomes, 2006) bằng cách cho phép các giải pháp tồi tệ hơn với hy vọng tìm một giải pháp tốt hơn trong tương lai. Chúng tôi đại diện cho không gian thiết kế của LAHC sử dụng một chromosome là một danh sách các hàm kích hoạt tương ứng với mỗi lớp của mạng. Hình 3b hiển thị một ví dụ về một giải pháp trong không gian thiết kế. Lợi ích của biểu diễn này là tính linh hoạt và đơn giản của nó. Để tạo ra một ứng viên tìm kiếm mới (hoạt động đột biến), trước tiên chúng tôi hoán đổi hai gen được chọn ngẫu nhiên từ chromosome, và sau đó, chúng tôi thay đổi ngẫu nhiên một gen từ chromosome với một ứng viên mới từ danh sách.

Phụ lục E liệt kê các toán tử unary được xem xét trong nghiên cứu này. Để tránh bất ổn trong quá trình huấn luyện, chúng tôi đã bỏ qua các toán tử tuần hoàn (ví dụ, cos(x)) và các toán tử chứa asymptotes ngang (y = 0) hoặc dọc (x = 0) (ví dụ, y = 1/x).

Quá trình chọn toán tử để tạo thành chromosome được lặp lại trong một số lần lặp được định trước (tham khảo Phụ lục E để biết cấu hình của LAHC). Cho rằng chúng tôi chỉ có hai đột biến mỗi lần lặp tìm kiếm, toàn bộ chromosome không bị ảnh hưởng đáng kể. Dựa trên các lần chạy thử nghiệm, chúng tôi xác định ngân sách 20 lần lặp tìm kiếm để cung cấp cải thiện tốt cùng với việc giảm chi phí tìm kiếm. Mỗi lần lặp bao gồm việc huấn luyện mạng sử dụng các hàm kích hoạt lựa chọn và đo lường mất mát huấn luyện L_train như một metric fitness cần được tối thiểu hóa.

Một nhược điểm của quá trình này là nhu cầu huấn luyện lại mạng cho mỗi lần lặp tìm kiếm, có thể tốn nhiều thời gian và tài nguyên tính toán. Chúng tôi khắc phục vấn đề này bằng cách tận dụng ước lượng độ tin cậy thấp hơn của hiệu suất cuối cùng. Cho rằng hiệu suất mạng không thay đổi sau một số epoch nhất định, chúng tôi tận dụng công trình của Loni et al. (2020) và chỉ huấn luyện mạng đến một điểm nhất định sau đó hiệu suất sẽ giữ ổn định.

Giai đoạn 2: Yếu tố Tỷ lệ và HPO Cho một chuỗi toán tử tối ưu đã học ψ, bước tiếp theo là tìm một chuỗi ψ' = ⟨(α_i, β_i)|α_i, β_i ∈ R⟩_{i∈{1,...,L}} đại diện cho các yếu tố tỷ lệ cho mỗi lớp. Chúng tôi thực hiện quá trình này cùng với giai đoạn tinh chỉnh (Phương trình (3)) và HPO để khám phá các tham số tinh chỉnh θ*_fine và siêu tham số λ* như được hiển thị trong Phương trình (6).

λ* ∈ argmin_{λ∈Λ} c(λ; D_val) s.t. ψ'*, θ*_fine(λ) ∈ argmin_{θ∈Θ, ψ'∈R^{(2,L)}} L_fine(θ|θ*_pre ⊙ m*), ψ'; ψ, D_train)     (6)

Do bản chất liên tục của giai đoạn này, chúng tôi sử dụng Stochastic Gradient Descent (SGD) để giải quyết Phương trình (6), và sử dụng độ chính xác validation như một metric fitness cho cấu hình siêu tham số.

Coi các yếu tố tỷ lệ như các tham số có thể học cho phép chúng tôi học chúng trong trạng thái tinh chỉnh. Do đó, tối ưu hóa bên trong trong bước này hầu như không có chi phí overhead. Chi phí bổ sung duy nhất là của HPO, mà chúng tôi chứng minh trong thí nghiệm của mình là quan trọng và đáng giá vì các siêu tham số từ việc huấn luyện mô hình gốc có thể không tối ưu cho tinh chỉnh.

5 Thí nghiệm
Chúng tôi phân loại các thí nghiệm dựa trên các câu hỏi nghiên cứu mà công trình này nhằm trả lời. Phần 5.1 giới thiệu thiết lập thí nghiệm. Phần 5.2 động lực cho vấn đề tinh chỉnh hàm kích hoạt cho SNNs. Phần 5.3 giới thiệu nhu cầu cho HPO với tinh chỉnh kích hoạt cho SNNs. Trong Phần 5.4, chúng tôi so sánh SAFS với các baseline khác nhau. Phụ lục D cung cấp một sự đánh đổi cải thiện độ chính xác vs. tỷ lệ nén để so sánh SAFS với các phương pháp nén mạng tiên tiến. Trong Phần 5.5 chúng tôi so sánh hiệu suất của SAFS cho các tỷ lệ cắt tỉa khác nhau. Trong Phần 5.6 chúng tôi cung cấp thông tin chi tiết về các hàm kích hoạt được học bởi SAFS. Cuối cùng, chúng tôi ablate SAFS trong Phần 5.7 để xác định tác động của các lựa chọn thiết kế khác nhau.

5.1 Thiết lập Thí nghiệm
Bộ dữ liệu. Để đánh giá SAFS, chúng tôi sử dụng các bộ dữ liệu phân loại công khai MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky et al., 2014) và ImageNet-16 (Chrabaszcz et al., 2017). Lưu ý rằng ImageNet-16 bao gồm tất cả hình ảnh của bộ dữ liệu ImageNet gốc, được thay đổi kích thước thành 16 × 16 pixels. Tất cả thí nghiệm HPO được tiến hành sử dụng SMAC3 (Lindauer et al., 2022). Phụ lục E trình bày phần còn lại của thiết lập thí nghiệm.

5.2 Tác động của việc Điều chỉnh Hàm Kích hoạt lên Độ chính xác của SNNs
Để xác thực giả định rằng hàm kích hoạt thực sự tác động đến độ chính xác, chúng tôi điều tra xem liệu hàm kích hoạt hiện được sử dụng cho mạng dày đặc (Evci et al., 2022) có còn đáng tin cậy trong bối cảnh thưa hay không. Hình 4a hiển thị tác động của năm hàm kích hoạt khác nhau lên độ chính xác của kiến trúc thưa với các tỷ lệ cắt tỉa khác nhau. Để đo lường hiệu suất trong giai đoạn tìm kiếm, chúng tôi sử dụng phương pháp validation ba fold. Tuy nhiên, chúng tôi báo cáo độ chính xác test của SAFS để so sánh kết quả của chúng tôi với các baseline khác.

Kết luận của chúng tôi từ thí nghiệm này có thể được tóm tắt như sau: (i) ReLU không hoạt động tốt nhất trong tất cả các tình huống. Chúng ta thấy rằng SRS, Swish, Tanh, Symlog, FLAU, và PReLU vượt trội hơn ReLU ở mức độ thưa thớt cao hơn. Do đó, quyết định sử dụng ReLU một cách nhất quán có thể hạn chế lợi ích tiềm năng trong độ chính xác. (ii) Khi chúng ta tăng tỷ lệ cắt tỉa lên 99% (mạng cực kỳ thưa), mặc dù có sự giảm chung trong độ chính xác, sự khác biệt trong độ chính xác của mạng thưa và dày đặc thay đổi rất nhiều tùy thuộc vào hàm kích hoạt. Do đó, việc lựa chọn hàm kích hoạt cho mạng có độ thưa cao trở thành một tham số quan trọng. Chúng tôi cần đề cập rằng mặc dù thành công của SAFS trong việc cung cấp độ chính xác cao hơn, nó cần 47 giờ GPU tổng cộng để học hàm kích hoạt và HPs tối ưu. Mặt khác, tinh chỉnh một mạng neuron thưa mất ≈3.9 giờ GPU.

5.3 Khó khăn của việc Huấn luyện Mạng Neuron Thưa
Hiện tại, hầu hết các thuật toán để huấn luyện DNNs thưa sử dụng cấu hình được tùy chỉnh cho các đối tác dày đặc của chúng, ví dụ, bắt đầu từ một bộ lập lịch học cố định. Để xác thực nhu cầu tối ưu hóa các siêu tham số huấn luyện của mạng thưa, chúng tôi đã sử dụng cấu hình dày đặc như một baseline so với siêu tham số được học bởi một phương pháp HPO. Hình 4b hiển thị các đường cong tinh chỉnh VGG-16 thưa thớt hóa với tỷ lệ cắt tỉa 99% được huấn luyện trên CIFAR-10. Việc huấn luyện đã được thực hiện với siêu tham số của mạng dày đặc (Xanh), và siêu tham số huấn luyện được tối ưu hóa sử dụng SMAC3 (Cam).

Chúng tôi đã tối ưu hóa learning rate, learning rate scheduler, và siêu tham số optimizer với phạm vi được chỉ định trong Phụ lục E (Bảng 4). Loại và phạm vi của siêu tham số được chọn dựa trên phạm vi được khuyến nghị từ văn học học sâu (Simonyan và Zisserman, 2014; Subramanian et al., 2022), tài liệu SMAC3 (Lindauer et al., 2022), và từ các thư viện mã nguồn mở khác nhau được sử dụng để triển khai VGG-16. Để ngăn chặn overfitting trên dữ liệu test, chúng tôi đã tối ưu hóa siêu tham số trên dữ liệu validation và kiểm tra hiệu suất cuối cùng trên dữ liệu test. Hiệu suất kém (giảm 7.17% độ chính xác) của chiến lược học SNN sử dụng tham số dày đặc tạo động lực cho nhu cầu một chế độ HPO nhận biết thưa thớt riêng biệt.

5.4 So sánh với Magnitude Pruning Baselines
Bảng 1 hiển thị kết quả tối ưu hóa hàm kích hoạt VGG-16 thưa được huấn luyện trên CIFAR-10 sử dụng SAFS với tỷ lệ cắt tỉa 99%. Trung bình của ba lần chạy đã được báo cáo. Kết quả hiển thị rằng SAFS cung cấp cải thiện độ chính xác tuyệt đối 8.88% cho VGG-16 và 6.33% cho ResNet-18 được huấn luyện trên CIFAR-10 khi so sánh với baseline cắt tỉa độ lớn vanilla. SAFS bổ sung mang lại cải thiện độ chính xác Top-1 tuyệt đối 1.8% cho ResNet-18 và 1.54% cho EfficientNet-B0 được huấn luyện trên ImageNet-16 khi so sánh với baseline cắt tỉa độ lớn vanilla. SReLU (Jin et al., 2016) là một hàm kích hoạt tuyến tính từng đoạn được công thức hóa bởi bốn tham số có thể học. Mocanu et al. (2018); Curci et al. (2021); Tessera et al. (2021) đã cho thấy SReLU hoạt động xuất sắc cho mạng neuron thưa do cải thiện luồng gradient của mạng. Kết quả hiển thị rằng SAFS cung cấp độ chính xác cao hơn 15.99% và 19.17% so với huấn luyện VGG-16 và ResNet-18 với hàm kích hoạt SReLU trên CIFAR-10. Thêm vào đó, SAFS cung cấp độ chính xác tốt hơn 0.88% và 1.28% so với huấn luyện ResNet-18 và EfficientNet-B0 với hàm kích hoạt SReLU trên bộ dữ liệu ImageNet-16. Cuối cùng, Phụ lục B hiển thị rằng SAFS cải thiện đáng kể luồng gradient của mạng neuron thưa, điều này liên quan đến hàm kích hoạt được tối ưu hóa và giao thức huấn luyện hiệu quả.

5.5 Đánh giá SAFS với Các Tỷ lệ Cắt tỉa Khác nhau
Hình 4a so sánh hiệu suất của VGG-16 được tinh chỉnh bởi SAFS và giao thức huấn luyện mặc định trên CIFAR-10 qua ba tỷ lệ cắt tỉa khác nhau bao gồm 90%, 95%, và 99%. Kết quả hiển thị rằng SAFS cực kỳ hiệu quả bằng cách đạt được độ chính xác cao hơn 1.65%, 7.45%, và 8.88% so với VGG-16 với hàm kích hoạt ReLU được tinh chỉnh với giao thức huấn luyện mặc định ở tỷ lệ cắt tỉa 90%, 95%, và 99%. Thêm vào đó, SAFS tốt hơn hàm kích hoạt được thiết kế cho mạng dày đặc, đặc biệt cho mạng với tỷ lệ cắt tỉa 99%.

5.6 Thông tin chi tiết về việc Tìm kiếm Hàm Kích hoạt
Hình 5 trình bày mẫu thống trị của mỗi toán tử unary trong giai đoạn học đầu tiên (α = β = 1) cho bộ dữ liệu CIFAR-10. Kết quả là trung bình của ba lần chạy với các random seeds khác nhau. Đơn vị của thanh màu là số lần nhìn thấy một hàm kích hoạt cụ thể qua tất cả các lần lặp tìm kiếm cho giai đoạn học đầu tiên. Theo kết quả, rõ ràng rằng (i) Symexp và ELU là các hàm kích hoạt không thuận lợi, (ii) Symlog và Acon là các hàm kích hoạt thống trị trong khi được sử dụng trong các lớp đầu, và (iii) Nhìn chung Swish và HardSwish là tốt, nhưng chúng chủ yếu xuất hiện trong các lớp giữa.

5.7 Nghiên cứu Ablation
Chúng tôi nghiên cứu tác động của mỗi giai đoạn tối ưu hóa riêng lẻ của SAFS lên hiệu suất của VGG-16 và ResNet-18 thưa được huấn luyện trên CIFAR-10 trong Bảng 2. Kết quả hiển thị rằng mỗi đóng góp riêng lẻ cung cấp độ chính xác cao hơn cho cả VGG-16 và ResNet-18. Tuy nhiên, hiệu suất tối đa được đạt bởi SAFS (+15.53%, +8.88%, +6.33%, và +1.54% cho LeNet-5, VGG-16, ResNet-18, và EfficientNet-B0), nơi chúng tôi trước tiên học toán tử unary chính xác nhất cho mỗi lớp và sau đó tinh chỉnh các yếu tố tỷ lệ với siêu tham số được tối ưu hóa.

6 Kết luận
Trong bài báo này, chúng tôi đã nghiên cứu tác động của hàm kích hoạt lên việc huấn luyện mạng neuron thưa và sử dụng điều này để học các hàm kích hoạt mới. Với mục đích này, chúng tôi đã chứng minh rằng việc giảm độ chính xác gây ra bởi việc huấn luyện SNNs một cách đồng nhất với ReLU cho tất cả các đơn vị có thể được giảm thiểu một phần bằng cách tìm kiếm hàm kích hoạt theo từng lớp. Chúng tôi đề xuất một pipeline tối ưu hóa hai giai đoạn mới kết hợp tối ưu hóa rời rạc và ngẫu nhiên để chọn một chuỗi hàm kích hoạt cho mỗi lớp của một SNN, cùng với việc khám phá các siêu tham số tối ưu cho tinh chỉnh. Phương pháp SAFS của chúng tôi cung cấp cải thiện đáng kể bằng cách đạt được độ chính xác cao hơn lên đến 8.88% và 6.33% cho VGG-16 và ResNet-18 trên CIFAR-10 so với các giao thức huấn luyện mặc định, đặc biệt ở tỷ lệ cắt tỉa cao. Quan trọng, vì SAFS độc lập với thuật toán cắt tỉa, nó có thể tối ưu hóa bất kỳ mạng thưa nào.

7 Hạn chế và Tác động Rộng lớn
Tác động Rộng lớn. Các tác giả đã xác định rằng công trình này sẽ không có tác động tiêu cực lên xã hội hoặc môi trường, vì công trình này không giải quyết bất kỳ ứng dụng cụ thể nào.

Công trình Tương lai và Hạn chế. Mạng Neuron Thưa (SNNs) cho phép triển khai các mô hình lớn trên thiết bị có tài nguyên hạn chế bằng cách tiết kiệm chi phí tính toán và tiêu thụ bộ nhớ. Ngoài ra, điều này trở nên quan trọng trong việc xem xét giảm dấu chân carbon và sử dụng tài nguyên của DNNs khi suy luận. Chúng tôi tin rằng điều này mở ra các hướng nghiên cứu mới về các phương pháp có thể cải thiện độ chính xác của SNNs. Chúng tôi hy vọng rằng công trình của chúng tôi thúc đẩy các kỹ sư sử dụng SNNs nhiều hơn trước đây trong các sản phẩm thực tế vì SAFS cung cấp cho SNNs hiệu suất tương tự như các đối tác dày đặc. Một số hướng ngay lập tức để mở rộng công trình của chúng tôi là (i) tận dụng ý tưởng của các bộ dự đoán độ chính xác (Li et al., 2023) để đẩy nhanh quy trình tìm kiếm. (ii) SNNs gần đây đã cho thấy hứa hẹn trong ứng dụng vào các kỹ thuật cho các vấn đề ra quyết định tuần tự như Học Tăng cường (Vischer et al., 2022; Graesser et al., 2022). Chúng tôi tin rằng việc kết hợp SAFS vào các tình huống như vậy có thể giúp tính khả thi triển khai của các pipeline như vậy.

SAFS đã được đánh giá trên các bộ dữ liệu đa dạng, bao gồm MNIST, CIFAR-10, và ImageNet-16, và các kiến trúc mạng khác nhau như LeNet-5, VGG-16, ResNet-18, và EfficientNet-B0. Trong khi các kết quả hiện tại chứng minh tính ứng dụng tổng quát của phương pháp của chúng tôi và dấu hiệu của khả năng mở rộng, chúng tôi tin rằng các thí nghiệm thêm trên các bộ dữ liệu lớn hơn và mạng có khả năng mở rộng hơn sẽ là một hướng thú vị cho công trình tương lai.
