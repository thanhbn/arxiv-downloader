# TRP: Trained Rank Pruning cho Mạng Neural Sâu Hiệu quả
Yuhui Xu1,Yuxi Li1,Shuai Zhang2,Wei Wen3,Botao Wang2,
Yingyong Qi2,Yiran Chen3,Weiyao Lin1,Hongkai Xiong1
1Đại học Giao thông Thượng Hải2Nghiên cứu AI Qualcomm3Đại học Duke
fyuhuixu, lyxok1, wylin, xionghongkai g@sjtu.edu.cn,
fshuazhan, botaow, yingyong g@qti.qualcomm.com, fwei.wen, yiran.chen g@duke.edu

## Tóm tắt
Để cho phép DNN hoạt động trên các thiết bị biên như điện thoại di động, xấp xỉ rank thấp đã được áp dụng rộng rãi vì có lý thuyết vững chắc và triển khai hiệu quả. Một số nghiên cứu trước đó đã cố gắng xấp xỉ trực tiếp mô hình đã được huấn luyện trước bằng phân tích rank thấp; tuy nhiên, sai số xấp xỉ nhỏ trong tham số có thể lan truyền thành tổn thất dự đoán lớn. Kết quả là, hiệu suất thường giảm đáng kể và cần nỗ lực tinh chỉnh phức tạp để khôi phục độ chính xác. Rõ ràng, việc tách biệt xấp xỉ rank thấp khỏi quá trình huấn luyện là không tối ưu. Khác với các nghiên cứu trước, bài báo này tích hợp xấp xỉ rank thấp và điều chuẩn vào quá trình huấn luyện. Chúng tôi đề xuất Trained Rank Pruning (TRP), xen kẽ giữa xấp xỉ rank thấp và huấn luyện. TRP duy trì khả năng của mạng gốc trong khi áp đặt ràng buộc rank thấp trong quá trình huấn luyện. Điều chuẩn nuclear được tối ưu hóa bằng gradient ngẫu nhiên dưới được sử dụng để thúc đẩy rank thấp hơn nữa trong TRP. Mạng được huấn luyện TRP vốn có cấu trúc rank thấp, và được xấp xỉ với tổn thất hiệu suất không đáng kể, do đó loại bỏ quá trình tinh chỉnh sau phân tích rank thấp. Phương pháp đề xuất được đánh giá toàn diện trên CIFAR-10 và ImageNet, vượt trội hơn các phương pháp nén trước đó sử dụng xấp xỉ rank thấp.

## 1 Giới thiệu
Mạng Neural Sâu (DNN) đã cho thấy thành công đáng kể trong nhiều tác vụ thị giác máy tính. Mặc dù có hiệu suất cao trong DNN dựa trên máy chủ được hỗ trợ bởi phần cứng tính toán song song tiên tiến, hầu hết các kiến trúc tiên tiến vẫn chưa sẵn sàng để triển khai trên thiết bị di động do hạn chế về khả năng tính toán, bộ nhớ và năng lượng.

Để giải quyết vấn đề này, nhiều phương pháp nén và tăng tốc mạng đã được đề xuất. Các phương pháp dựa trên pruning [Han et al., 2015b; He et al., 2017; Liu et al., 2017; Luo et al., 2017] khám phá tính thưa thớt trong trọng số và bộ lọc. Các phương pháp dựa trên lượng tử hóa [Han et al., 2015b; Zhou et al., 2017; Courbariaux và Bengio, 2016; Rastegari et al., 2016; Xu et al., 2018] giảm độ rộng bit của tham số mạng. Phân tích rank thấp [Denton et al., 2014; Jaderberg et al., 2014; Guo et al., 2018; Wen et al., 2017; Alvarez và Salzmann, 2017] giảm thiểu dư thừa theo kênh và không gian bằng cách phân tích mạng gốc thành mạng compact với các lớp rank thấp. Ngoài ra, các kiến trúc hiệu quả [Sandler et al., 2018; Ma et al., 2018] được thiết kế cẩn thận để tạo thuận lợi cho việc triển khai mạng neural sâu trên di động. Khác với các nghiên cứu trước, bài báo này đề xuất một cách tiếp cận mới để thiết kế mạng rank thấp.

Mạng rank thấp có thể được huấn luyện trực tiếp từ đầu. Tuy nhiên, khó để có được kết quả thỏa mãn vì một số lý do. (1) Khả năng thấp: so với mạng rank đầy đủ gốc, khả năng của mạng rank thấp bị hạn chế, gây khó khăn trong việc tối ưu hóa hiệu suất. (2) Cấu trúc sâu: phân tích rank thấp thường làm tăng gấp đôi số lượng lớp trong mạng. Các lớp bổ sung làm cho tối ưu hóa số học dễ bị tổn thương hơn với việc bùng nổ và/hoặc biến mất gradient. (3) Lựa chọn rank heuristic: rank của mạng phân tích thường được chọn làm siêu tham số dựa trên mạng đã huấn luyện trước; điều này có thể không phải là rank tối ưu cho mạng được huấn luyện từ đầu.

Thay vào đó, một số nghiên cứu trước [Zhang et al., 2016; Guo et al., 2018; Jaderberg et al., 2014] đã cố gắng phân tích các mô hình đã huấn luyện trước để có được mạng rank thấp ban đầu. Tuy nhiên, rank thấp được áp đặt một cách heuristic có thể gây ra tổn thất độ chính xác lớn và cần huấn luyện lại mạng để khôi phục hiệu suất của mạng gốc càng nhiều càng tốt. Một số nỗ lực đã được thực hiện để sử dụng điều chuẩn thưa thớt [Wen et al., 2017; Chen et al., 2015] để ràng buộc mạng vào không gian rank thấp. Mặc dù điều chuẩn thưa thớt giảm lỗi gây ra bởi phân tích ở một mức độ nào đó, hiệu suất vẫn giảm nhanh chóng khi tỷ lệ nén tăng.

Bài báo này là phần mở rộng của [Xu et al., 2019]. Trong bài báo này, chúng tôi đề xuất một phương pháp mới, cụ thể là Trained Rank Pruning (TRP), để huấn luyện mạng rank thấp. Chúng tôi nhúng phân tích rank thấp vào quá trình huấn luyện bằng cách dần đẩy phân phối trọng số của mạng hoạt động tốt vào dạng rank thấp, nơi tất cả tham số của mạng gốc được giữ và tối ưu hóa để duy trì khả năng của nó. Chúng tôi cũng đề xuất điều chuẩn nuclear được tối ưu hóa gradient ngẫu nhiên dưới để ràng buộc thêm các trọng số trong không gian rank thấp nhằm tăng cường TRP. Giải pháp đề xuất được minh họa trong Hình 1.

Nhìn chung, đóng góp của chúng tôi được tóm tắt dưới đây:
1. Một phương pháp huấn luyện mới gọi là TRP được trình bày bằng cách nhúng rõ ràng phân tích rank thấp vào huấn luyện mạng;
2. Điều chuẩn nuclear được tối ưu hóa bằng gradient ngẫu nhiên dưới để tăng hiệu suất của TRP;
3. Cải thiện tăng tốc suy luận và giảm tổn thất độ chính xác xấp xỉ trong cả phương pháp phân tích theo kênh và theo không gian.

## 2 Các Nghiên cứu Liên quan
Nhiều nghiên cứu đã được đề xuất để tăng tốc quá trình suy luận của mạng neural sâu. Tóm lại, các nghiên cứu này có thể được phân loại thành ba danh mục chính: lượng tử hóa, pruning và phân tích rank thấp.

**Lượng tử hóa** Các phương pháp lượng tử hóa trọng số bao gồm huấn luyện mô hình lượng tử hóa từ đầu [Chen et al., 2015; Courbariaux và Bengio, 2016; Rastegari et al., 2016] hoặc chuyển đổi mô hình đã huấn luyện trước thành biểu diễn lượng tử hóa [Zhou et al., 2017; Han et al., 2015a; Xu et al., 2018]. Biểu diễn trọng số lượng tử hóa bao gồm giá trị nhị phân [Rastegari et al., 2016; Courbariaux và Bengio, 2016] hoặc bucket hash [Chen et al., 2015]. Lưu ý rằng phương pháp của chúng tôi được lấy cảm hứng từ sơ đồ kết hợp lượng tử hóa với quá trình huấn luyện, tức là chúng tôi nhúng phân tích rank thấp vào quá trình huấn luyện để hướng dẫn rõ ràng tham số thành dạng rank thấp.

**Pruning** Tính thưa thớt không có cấu trúc và có cấu trúc được giới thiệu bằng pruning. [Han et al., 2015b] đề xuất loại bỏ các kết nối không quan trọng giữa các đơn vị neural với trọng số nhỏ trong CNN đã huấn luyện trước. [Wen et al., 2016] sử dụng chiến lược group Lasso để học tính thưa thớt cấu trúc của mạng. [Liu et al., 2017] áp dụng chiến lược tương tự bằng cách áp đặt rõ ràng các yếu tố tỷ lệ trên mỗi kênh để đo tầm quan trọng của mỗi kết nối và loại bỏ những kết nối có trọng số nhỏ. Trong [He et al., 2017], vấn đề pruning được công thức hóa như một vấn đề khôi phục dữ liệu. Các bộ lọc đã huấn luyện trước được đánh trọng số lại bằng cách giảm thiểu hàm mục tiêu khôi phục dữ liệu. Các kênh có trọng số nhỏ hơn được loại bỏ. [Luo et al., 2017] chọn bộ lọc theo heuristic sử dụng thay đổi đầu ra của lớp tiếp theo làm tiêu chí.

**Phân tích rank thấp** Các mô hình gốc được phân tích thành những mô hình compact với các lớp nhẹ hơn. [Jaderberg et al., 2014] xem xét cả dư thừa theo không gian và theo kênh và đề xuất phân tích bộ lọc thành hai bộ lọc bất đối xứng nối tiếp. [Zhang et al., 2016] tiếp tục giả định rằng bản đồ đặc trưng nằm trong không gian con rank thấp và phân tích bộ lọc convolution thành k×k theo sau bởi 1×1 bộ lọc qua SVD. [Guo et al., 2018] khai thác giả định rank thấp của bộ lọc convolution và phân tích convolution thông thường thành một số cấu trúc convolution depth-wise và point-wise. Mặc dù các nghiên cứu này đạt được hiệu suất đáng chú ý trong nén mạng, tất cả đều dựa trên giả định rank thấp. Khi giả định như vậy không được thỏa mãn hoàn toàn, lỗi dự đoán lớn có thể xảy ra.

Thay vào đó, một số nghiên cứu khác [Wen et al., 2017; Alvarez và Salzmann, 2017] sử dụng ngầm điều chuẩn thưa thớt để hướng dẫn quá trình huấn luyện mạng neural học biểu diễn rank thấp. Nghiên cứu của chúng tôi tương tự với phương pháp điều chuẩn rank thấp này. Tuy nhiên, ngoài việc thêm điều chuẩn ngầm trong quá trình huấn luyện, chúng tôi áp đặt ràng buộc thưa thớt rõ ràng trong quá trình huấn luyện và chứng minh rằng cách tiếp cận của chúng tôi có thể đẩy phân phối trọng số vào dạng rank thấp khá hiệu quả.

## 3 Phương pháp

### 3.1 Kiến thức Cơ bản
Về mặt hình thức, các bộ lọc convolution trong một lớp có thể được ký hiệu bởi tensor W ∈ R^(n×c×kw×kh), trong đó n và c là số lượng bộ lọc và kênh đầu vào, kh và kw là chiều cao và chiều rộng của bộ lọc. Đầu vào của lớp convolution Fi ∈ R^(c×x×y) tạo ra đầu ra là Fo = W ∗ Fi. Tương quan theo kênh [Zhang et al., 2016] và tương quan theo không gian [Jaderberg et al., 2014] được khám phá để xấp xỉ bộ lọc convolution trong không gian rank thấp. Trong bài báo này, chúng tôi tập trung vào hai sơ đồ phân tích này. Tuy nhiên, khác với các nghiên cứu trước, chúng tôi đề xuất sơ đồ huấn luyện mới TRP để có được mạng rank thấp mà không cần huấn luyện lại sau phân tích.

### 3.2 Trained Rank Pruning
Trained Rank Pruning (TRP) được thúc đẩy bởi các chiến lược huấn luyện mạng lượng tử hóa. Một trong các sơ đồ cập nhật gradient để huấn luyện mạng lượng tử hóa từ đầu [Li et al., 2017] là

w^(t+1) = Q(w^t - α∇f(w^t))    (1)

trong đó Q(·) là hàm lượng tử hóa, w^t biểu thị tham số trong lần lặp thứ t. Các tham số được lượng tử hóa bởi Q(·) trước khi cập nhật gradient.

Ngược lại, chúng tôi đề xuất sơ đồ huấn luyện đơn giản nhưng hiệu quả gọi là Trained Rank Pruning (TRP) theo kiểu định kỳ:

W^(t+1) = {
    W^t - α∇f(W^t)           nếu t mod m ≠ 0
    T^z - α∇f(T^z)          nếu t mod m = 0
}
T^z = D(W^t); z = ⌊t/m⌋    (2)

trong đó D(·) là toán tử xấp xỉ tensor rank thấp, α là tốc độ học, t là chỉ số lần lặp và z là lần lặp của toán tử D, với m là chu kỳ cho xấp xỉ rank thấp.

Thoạt nhìn, TRP này trông rất đơn giản. Một mối quan tâm ngay lập tức phát sinh: liệu các lần lặp có thể đảm bảo rank của tham số hội tụ, và quan trọng hơn là sẽ không tăng khi chúng được cập nhật theo cách này? Câu trả lời tích cực (xem Định lý 2) được đưa ra trong phân tích lý thuyết của chúng tôi sẽ chứng minh tính hợp pháp của thuật toán này.

Đối với lượng tử hóa mạng, nếu gradient nhỏ hơn mức lượng tử hóa, thông tin gradient sẽ bị mất hoàn toàn và trở thành zero. Tuy nhiên, điều này sẽ không xảy ra trong TRP vì toán tử rank thấp được áp dụng trên tensor trọng số. Hơn nữa, chúng tôi áp dụng xấp xỉ rank thấp mỗi m lần lặp SGD. Điều này tiết kiệm thời gian huấn luyện rất nhiều. Như được minh họa trong Hình 1, cứ mỗi m lần lặp, chúng tôi thực hiện xấp xỉ rank thấp trên các bộ lọc gốc, trong khi gradient được cập nhật trên dạng rank thấp kết quả. Nếu không, mạng được cập nhật qua SGD bình thường. Sơ đồ huấn luyện của chúng tôi có thể được kết hợp với bất kỳ toán tử rank thấp nào.

Trong nghiên cứu được đề xuất, chúng tôi chọn các kỹ thuật rank thấp được đề xuất trong [Jaderberg et al., 2014] và [Zhang et al., 2016], cả hai đều chuyển đổi bộ lọc 4 chiều thành ma trận 2D và sau đó áp dụng phân tích giá trị kỳ dị cắt ngắn (TSVD). SVD của ma trận W^t có thể được viết như:

W^t = Σ(i=1 to rank(W^t)) σ_i U_i (V_i)^T    (3)

trong đó σ_i là giá trị kỳ dị của W^t với σ_1 ≥ σ_2 ≥ ... ≥ σ_rank(W^t), và U_i và V_i là các vector kỳ dị. TSVD có tham số (W^t; ε) là tìm số nguyên nhỏ nhất k sao cho

Σ(j=k+1 to rank(W^t)) (σ_j)^2 ≤ ε Σ(i=1 to rank(W^t)) (σ_i)^2    (4)

trong đó ε là siêu tham số được định nghĩa trước của tỷ lệ pruning năng lượng, ε ∈ (0,1).

Sau khi cắt ngắn n-k giá trị kỳ dị cuối cùng, chúng tôi chuyển đổi ma trận 2D rank thấp trở lại tensor 4D. So với việc huấn luyện trực tiếp cấu trúc rank thấp từ đầu, TRP được đề xuất có những ưu điểm sau.

(1) Khác với việc cập nhật các bộ lọc đã phân tích độc lập với huấn luyện mạng trong tài liệu [Zhang et al., 2016; Jaderberg et al., 2014], chúng tôi cập nhật mạng trực tiếp trên hình dạng 4D gốc của các tham số đã phân tích, cho phép phân tích mạng và huấn luyện cùng nhau bằng cách bảo tồn khả năng phân biệt của nó càng nhiều càng tốt.

(2) Vì cập nhật gradient được thực hiện dựa trên cấu trúc mạng gốc, sẽ không có vấn đề bùng nổ và biến mất gradient do các lớp bổ sung gây ra.

(3) Rank của mỗi lớp được tự động chọn trong quá trình huấn luyện. Chúng tôi sẽ chứng minh một định lý chứng nhận sự hội tụ rank của trọng số mạng và sẽ không tăng trong phần 3.4.

### 3.3 Điều chuẩn Chuẩn Nuclear
Chuẩn nuclear được sử dụng rộng rãi trong các vấn đề hoàn thiện ma trận. Gần đây, nó được giới thiệu để ràng buộc mạng vào không gian rank thấp trong quá trình huấn luyện [Alvarez và Salzmann, 2017].

min(ℓ(f(x;w)) + λ Σ(l=1 to L) ||W_l||_*)    (5)

trong đó ℓ(·) là hàm tổn thất mục tiêu, chuẩn nuclear ||W_l||_* được định nghĩa là ||W_l||_* = Σ(i=1 to rank(W_l)) σ_i^l, với σ_i^l là các giá trị kỳ dị của W_l. λ là siêu tham số thiết lập ảnh hưởng của chuẩn nuclear. Trong [Alvarez và Salzmann, 2017] toán tử proximity được áp dụng trong mỗi lớp độc lập để giải Phương trình (5). Tuy nhiên, toán tử proximity được tách khỏi quá trình huấn luyện và không xem xét ảnh hưởng trong các lớp.

Trong bài báo này, chúng tôi sử dụng gradient ngẫu nhiên dưới [Avron et al., 2012] để tối ưu hóa điều chuẩn chuẩn nuclear trong quá trình huấn luyện. Gọi W = UΣV^T là SVD của W và gọi U_tru, V_tru là U, V được cắt ngắn đến rank(W) cột hoặc hàng đầu tiên, thì U_truV_tru^T là gradient dưới của ||W||_* [Watson, 1992]. Do đó, gradient dưới của Phương trình (5) trong một lớp là

∇ℓ + λU_truV_tru^T    (6)

Chuẩn nuclear và hàm tổn thất được tối ưu hóa đồng thời trong quá trình huấn luyện mạng và có thể được kết hợp thêm với TRP được đề xuất.

### 3.4 Phân tích Lý thuyết
Trong phần này, chúng tôi phân tích sự hội tụ rank của TRP từ góc độ lý thuyết nhiễu loạn ma trận [Stewart, 1990]. Chúng tôi chứng minh rằng rank trong TRP giảm một cách đơn điệu, tức là mô hình dần hội tụ thành mô hình thưa thớt hơn.

Gọi A là ma trận m×n, không mất tính tổng quát, m ≥ n. Σ = diag(σ_1,...,σ_n) và σ_1 ≥ σ_2 ≥ ... ≥ σ_n. Σ là ma trận chéo được tạo thành bởi tất cả giá trị kỳ dị của A. Gọi Ã = A + E là nhiễu loạn của A, và E là ma trận nhiễu. Σ̃ = diag(σ̃_1,...,σ̃_n) và σ̃_1 ≥ σ̃_2 ≥ ... ≥ σ̃_n. σ̃_i là giá trị kỳ dị của Ã. Các giới hạn nhiễu loạn cơ bản cho giá trị kỳ dị của ma trận được đưa ra bởi

**Định lý 1.** Định lý Mirsky [Mirsky, 1960]:
√(Σ_i |σ̃_i - σ_i|^2) ≤ ||E||_F    (7)

trong đó ||·||_F là chuẩn Frobenius. Sau đó có thể suy ra hệ quả sau từ Định lý 1,

**Hệ quả 1.** Gọi B là bất kỳ ma trận m×n có rank không lớn hơn k, tức là giá trị kỳ dị của B có thể được ký hiệu là σ'_1 ≥ ... ≥ σ'_k ≥ 0 và σ'_{k+1} = ... = σ'_n = 0. Khi đó

||B - A||_F ≥ √(Σ_{i=1}^n |σ'_i - σ_i|^2) ≥ √(Σ_{j=k+1}^n σ_j^2)    (8)

Dưới đây, chúng tôi sẽ phân tích quy trình huấn luyện của TRP được đề xuất. Lưu ý rằng W dưới đây đều được chuyển đổi thành ma trận 2D. Về Phương trình (2), quá trình huấn luyện giữa hai thao tác TSVD liên tiếp có thể được viết lại như Phương trình (9)

W^t = T^z = TSVD(W^t; ε)
W^{t+m} = T^z - Σ_{i=0}^{m-1} α∇f(W^{t+i})
T^{z+1} = TSVD(W^{t+m}; ε)    (9)

trong đó W^t là ma trận trọng số trong lần lặp thứ t. T^z là ma trận trọng số sau khi áp dụng TSVD lên W^t. ∇f(W^{t+i}) là gradient được lan truyền ngược trong lần lặp thứ (t+i). ε ∈ (0,1) là ngưỡng năng lượng được định nghĩa trước. Khi đó chúng ta có định lý sau.

**Định lý 2.** Giả sử ||∇f||_F có giới hạn trên G, nếu G < √ε/(m||W^{t+m}||_F), thì rank(T^z) ≥ rank(T^{z+1}).

**Chứng minh.** Chúng tôi ký hiệu σ_j^t và σ_j^{t+m} là giá trị kỳ dị của W^t và W^{t+m} tương ứng. Khi đó tại lần lặp thứ t, cho ngưỡng tỷ lệ năng lượng ε, thao tác TSVD cố gắng tìm chỉ số giá trị kỳ dị k ∈ [0, n-1] sao cho:

Σ_{j=k+1}^n (σ_j^t)^2 < ε||W^t||_F^2
Σ_{j=k}^n (σ_j^t)^2 ≥ ε||W^t||_F^2    (10)

Về Phương trình (10), T^z là ma trận rank k, tức là n-k giá trị kỳ dị cuối cùng của T^z bằng 0. Theo Hệ quả 1, chúng ta có thể suy ra:

||W^{t+m} - T^z||_F = ||Σ_{i=0}^{m-1} α∇f^{t+i}||_F ≥ √(Σ_{j=k+1}^n (σ_j^{t+m})^2)    (11)

Cho giả thiết G < √ε/(m||W^{t+m}||_F), chúng ta có thể nhận được:

√(Σ_{j=k+1}^n (σ_j^{t+m})^2)/||W^{t+m}||_F ≤ ||Σ_{i=0}^{m-1} α∇f^{t+i}||_F/||W^{t+m}||_F ≤ Σ_{i=0}^{m-1} ||α∇f^{t+i}||_F/||W^{t+m}||_F ≤ mαG/||W^{t+m}||_F < √ε    (12)

Phương trình (12) chỉ ra rằng vì nhiễu loạn của giá trị kỳ dị bị giới hạn bởi gradient tham số, nếu chúng ta chọn phù hợp ngưỡng tỷ lệ năng lượng TSVD ε, chúng ta có thể đảm bảo rằng nếu n-k giá trị kỳ dị được pruning bởi lần lặp TSVD trước, thì trước TSVD tiếp theo, năng lượng cho n-k giá trị kỳ dị cuối cùng vẫn nhỏ hơn ngưỡng năng lượng được định nghĩa trước ε. Do đó TSVD nên giữ số lượng giá trị kỳ dị đã pruning hoặc loại bỏ nhiều hơn để đạt được tiêu chí trong Phương trình (10), do đó thu được ma trận trọng số có rank thấp hơn hoặc bằng, tức là Rank(T^z) ≥ Rank(T^{z+1}). Chúng tôi tiếp tục xác nhận phân tích về sự thay đổi của phân phối rank trong Phần 4.

## 4 Thí nghiệm

### 4.1 Tập dữ liệu và Baseline
Chúng tôi đánh giá hiệu suất của sơ đồ TRP trên hai tập dữ liệu phổ biến, CIFAR-10 [Krizhevsky và Hinton, 2009] và ImageNet [Deng et al., 2009]. Tập dữ liệu CIFAR-10 bao gồm các hình ảnh tự nhiên màu với độ phân giải 32×32 và có tổng cộng 10 lớp. Tập dữ liệu ImageNet bao gồm 1000 lớp hình ảnh cho tác vụ nhận dạng. Đối với cả hai tập dữ liệu, chúng tôi áp dụng ResNet [He et al., 2016] làm mô hình baseline vì nó được sử dụng rộng rãi trong các tác vụ thị giác khác nhau. Chúng tôi sử dụng ResNet-20, ResNet-56 cho CIFAR-10 và ResNet-18, ResNet-50 cho ImageNet. Để đánh giá metric, chúng tôi áp dụng độ chính xác top-1 trên CIFAR-10 và độ chính xác top-1, top-5 trên ImageNet. Để đo hiệu suất tăng tốc, chúng tôi tính tỷ lệ FLOP giữa baseline và các mô hình đã phân tích để có được tỷ lệ tăng tốc cuối cùng. Thời gian CPU và GPU thực tế cũng được so sánh. Ngoài các phương pháp phân tích cơ bản, chúng tôi so sánh hiệu suất với các thuật toán tăng tốc tiên tiến khác [He et al., 2017; Li et al., 2016; Luo et al., 2017; Zhou et al., 2019].

### 4.2 Chi tiết Triển khai
Chúng tôi triển khai sơ đồ TRP với GPU NVIDIA 1080 Ti. Để huấn luyện trên CIFAR-10, chúng tôi bắt đầu với tốc độ học cơ sở 0.1 để huấn luyện 164 epoch và giảm giá trị theo hệ số 10 tại epoch thứ 82 và 122. Đối với ImageNet, chúng tôi trực tiếp tinh chỉnh mô hình với sơ đồ TRP từ baseline đã huấn luyện trước với tốc độ học 0.0001 trong 10 epoch. Chúng tôi áp dụng solver SGD để cập nhật trọng số và thiết lập giá trị weight decay là 10^-4 và giá trị momentum là 0.9. Cải thiện độ chính xác được kích hoạt bởi phân tích phụ thuộc dữ liệu biến mất sau tinh chỉnh. Vì vậy chúng tôi đơn giản áp dụng phân tích độc lập dữ liệu đã huấn luyện lại làm phương pháp cơ bản.

### 4.3 Kết quả trên CIFAR-10
**Cài đặt.** Thí nghiệm trên phân tích theo kênh (TRP1) và phân tích theo không gian (TRP2) đều được xem xét. Ngưỡng năng lượng TSVD trong TRP và TRP+Nu là 0.02 và trọng số chuẩn nuclear được thiết lập là 0.0003. Chúng tôi phân tích cả lớp 1×1 và 3×3 trong ResNet-56.

**Kết quả.** Như được hiển thị trong Bảng 1, đối với cả phân tích theo không gian và theo kênh, TRP được đề xuất vượt trội hơn các phương pháp cơ bản [Zhang et al., 2016; Jaderberg et al., 2014] trên ResNet-20 và ResNet-56. Kết quả thậm chí còn tốt hơn khi điều chuẩn nuclear được sử dụng. Ví dụ, trong phân tích theo kênh (TRP2) của ResNet-56, kết quả của TRP kết hợp với điều chuẩn nuclear thậm chí có thể đạt được tỷ lệ tăng tốc gấp 2 lần so với [Zhang et al., 2016] với cùng mức giảm độ chính xác. TRP cũng vượt trội hơn filter pruning [Li et al., 2016] và channel pruning [He et al., 2017]. ResNet-56 được huấn luyện TRP phân tích theo kênh có thể đạt được độ chính xác 92.77% với tăng tốc 2.31×, trong khi [He et al., 2017] là 91.80% và [Li et al., 2016] là 91.60%. Với điều chuẩn nuclear, phương pháp của chúng tôi có thể tăng gấp đôi tỷ lệ tăng tốc của [He et al., 2017] và [Li et al., 2016] với độ chính xác cao hơn.

### 4.4 Kết quả trên ImageNet
**Cài đặt.** Chúng tôi chọn ResNet-18 và ResNet-50 làm mô hình baseline. Ngưỡng năng lượng TSVD ε được thiết lập là 0.005. λ của điều chuẩn chuẩn nuclear là 0.0003 cho cả ResNet-18 và ResNet-50. Chúng tôi phân tích cả lớp Convolution 3×3 và 1×1 trong ResNet-50. TRP1 là phân tích theo kênh và TRP2 là phân tích theo không gian.

**Kết quả.** Kết quả trên ImageNet được hiển thị trong Bảng 2 và Bảng 3. Đối với ResNet-18, phương pháp của chúng tôi vượt trội hơn các phương pháp cơ bản [Zhang et al., 2016; Jaderberg et al., 2014]. Ví dụ, trong phân tích theo kênh, TRP có được tỷ lệ tăng tốc 1.81× với độ chính xác Top5 86.48% trên ImageNet vượt trội hơn cả phương pháp data-driven [Zhang et al., 2016] và data independent [Zhang et al., 2016] với khoảng cách lớn. Điều chuẩn nuclear có thể tăng tỷ lệ tăng tốc với cùng độ chính xác.

Đối với ResNet-50, để xác thực tốt hơn hiệu quả của phương pháp, chúng tôi cũng so sánh TRP với các phương pháp dựa trên pruning. Với tăng tốc 1.80×, ResNet-50 đã phân tích của chúng tôi có thể có được độ chính xác Top1 74.06% và Top5 92.07% cao hơn nhiều so với [Luo et al., 2017]. TRP đạt được tăng tốc 2.23× cao hơn [He et al., 2017] với cùng mức giảm Top5 1.4%. Bên cạnh đó, với cùng tỷ lệ tăng tốc 2.30×, hiệu suất của chúng tôi tốt hơn [Zhou et al., 2019].

### 4.5 Biến đổi Rank
Để phân tích sự thay đổi của phân phối rank trong quá trình huấn luyện, chúng tôi tiến hành thêm thí nghiệm trên tập dữ liệu CIFAR-10 với ResNet-20 và trích xuất trọng số từ lớp convolution res3-1-2 với phân tích theo kênh là sơ đồ TRP của chúng tôi. Sau mỗi TSVD, chúng tôi tính tỷ lệ năng lượng chuẩn hóa ER(i) cho mỗi giá trị kỳ dị σ_i như Phương trình (13).

ER(i) = σ_i^2 / Σ_{j=0}^{rank(T^z)} σ_j^2    (13)

chúng tôi ghi lại tổng cộng 40 lần lặp TSVD với chu kỳ m = 20, tương đương với 800 lần lặp huấn luyện, và ngưỡng năng lượng ε được định nghĩa trước là 0.05. Sau đó chúng tôi trực quan hóa sự thay đổi của ER trong Hình 2. Trong quá trình huấn luyện, chúng tôi quan sát rằng giá trị giới hạn lý thuyết max_t mαG/||W^t||_F ≈ 0.092 < √ε ≈ 0.223, điều này chỉ ra rằng giả thiết cơ bản trong định lý 2 luôn đúng cho giai đoạn huấn luyện ban đầu.

Và hiện tượng này cũng được phản ánh trong Hình 2, ban đầu, phân phối năng lượng gần như đồng nhất đối với mỗi giá trị kỳ dị, và số lượng giá trị kỳ dị bị loại bỏ tăng lên sau mỗi lần lặp TSVD và phân phối năng lượng trở nên dày đặc hơn giữa các giá trị kỳ dị có chỉ số nhỏ hơn. Cuối cùng, phân phối rank hội tụ đến một điểm nhất định nơi tỷ lệ năng lượng nhỏ nhất chính xác đạt ngưỡng ε của chúng tôi và TSVD sẽ không cắt thêm giá trị kỳ dị nào nữa.

### 4.6 Nghiên cứu Ablation
Để thể hiện hiệu quả của các thành phần khác nhau trong phương pháp, chúng tôi so sánh bốn sơ đồ huấn luyện, phương pháp cơ bản [Zhang et al., 2016; Jaderberg et al., 2014], phương pháp cơ bản kết hợp với điều chuẩn chuẩn nuclear, TRP và TRP kết hợp với điều chuẩn chuẩn nuclear. Kết quả được hiển thị trong Hình 3. Chúng ta có thể có những quan sát sau:

(1) **Điều chuẩn chuẩn nuclear** Sau khi kết hợp điều chuẩn chuẩn nuclear, các phương pháp cơ bản cải thiện với khoảng cách lớn. Vì điều chuẩn chuẩn nuclear ràng buộc các bộ lọc vào không gian rank thấp, tổn thất gây ra bởi TSVD nhỏ hơn so với các phương pháp cơ bản.

(2) **Trained rank pruning** Như được mô tả trong Hình 3, khi tỷ lệ tăng tốc tăng, hiệu suất của các phương pháp cơ bản và các phương pháp cơ bản kết hợp với điều chuẩn chuẩn nuclear giảm mạnh. Tuy nhiên, TRP được đề xuất giảm rất chậm. Điều này chỉ ra rằng bằng cách tái sử dụng khả năng của mạng, TRP có thể học biểu diễn đặc trưng rank thấp tốt hơn so với các phương pháp cơ bản. Lợi ích của điều chuẩn chuẩn nuclear đối với TRP không lớn bằng các phương pháp cơ bản vì TRP đã cảm ứng các tham số vào không gian rank thấp bằng cách nhúng TSVD trong quá trình huấn luyện.

### 4.7 Tăng tốc Runtime thực tế của Mạng đã Phân tích
Chúng tôi tiếp tục đánh giá tăng tốc runtime thực tế của Mạng nén như được hiển thị trong Bảng 4. Thí nghiệm của chúng tôi được tiến hành trên nền tảng với một GPU Nvidia 1080Ti và CPU Xeon E5-2630. Các mô hình chúng tôi sử dụng là ResNet-18 gốc và các mô hình đã phân tích bởi TRP1+Nu và TRP2+Nu. Từ kết quả, chúng tôi quan sát rằng trên CPU, sơ đồ TRP của chúng tôi đạt được hiệu suất tăng tốc nổi bật hơn. Nhìn chung, phân tích theo không gian kết hợp với sơ đồ TRP+Nu có hiệu suất tốt hơn. Vì cuDNN không thân thiện với kernel 1×3 và 3×1, tăng tốc thực tế của phân tích theo không gian không rõ ràng bằng việc giảm FLOP.

## 5 Kết luận
Trong bài báo này, chúng tôi đề xuất sơ đồ mới Trained Rank Pruning (TRP) để huấn luyện mạng rank thấp. Nó tận dụng khả năng và cấu trúc của mạng gốc bằng cách nhúng xấp xỉ rank thấp vào quá trình huấn luyện. Hơn nữa, chúng tôi đề xuất điều chuẩn chuẩn nuclear được tối ưu hóa gradient ngẫu nhiên dưới để tăng cường TRP. TRP được đề xuất có thể được kết hợp với bất kỳ phương pháp phân tích rank thấp nào. Trên các tập dữ liệu CIFAR-10 và ImageNet, chúng tôi đã chỉ ra rằng các phương pháp của chúng tôi có thể vượt trội hơn các phương pháp cơ bản và các phương pháp dựa trên pruning khác trong cả phân tích theo kênh và phân tích theo không gian.
