# 2111.05754.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2111.05754.pdf
# Kích thước tệp: 373423 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cắt tỉa một lần cho tất cả: Các mô hình ngôn ngữ thưa được huấn luyện trước
Oﬁr Zafrir
Intel Labs, Israel
ofir.zafrir@intel.comAriel Larey
Intel Labs, Israel
ariel.larey@intel.comGuy Boudoukh
Intel Labs, Israel
guy.boudoukh@intel.com
Haihao Shen
Intel Corporation
haihao.shen@intel.comMoshe Wasserblat
Intel Labs, Israel
moshe.wasserblat@intel.com
Tóm tắt
Các mô hình ngôn ngữ dựa trên Transformer được áp dụng cho nhiều ứng dụng trong xử lý ngôn ngữ tự nhiên. Tuy nhiên, chúng không hiệu quả và khó triển khai. Trong những năm gần đây, nhiều thuật toán nén đã được đề xuất để tăng hiệu quả triển khai của các mô hình Transformer lớn trên phần cứng đích. Trong công trình này, chúng tôi trình bày một phương pháp mới để huấn luyện các mô hình ngôn ngữ Transformer thưa được huấn luyện trước bằng cách tích hợp cắt tỉa trọng số và chưng cất mô hình. Những mô hình thưa được huấn luyện trước này có thể được sử dụng để học chuyển giao cho nhiều nhiệm vụ khác nhau trong khi duy trì mẫu thưa của chúng. Chúng tôi chứng minh phương pháp của mình với ba kiến trúc đã biết để tạo ra BERT-Base, BERT-Large và DistilBERT thưa được huấn luyện trước. Chúng tôi cho thấy cách các mô hình thưa được huấn luyện trước nén mà chúng tôi đã huấn luyện chuyển giao kiến thức của chúng cho năm nhiệm vụ ngôn ngữ tự nhiên hạ nguồn khác nhau với mất mát độ chính xác tối thiểu. Hơn nữa, chúng tôi cho thấy cách nén thêm trọng số của các mô hình thưa xuống độ chính xác 8bit bằng cách sử dụng huấn luyện nhận biết lượng tử hóa. Ví dụ, với BERT-Large thưa được huấn luyện trước được tinh chỉnh trên SQuADv1.1 và lượng tử hóa xuống 8bit, chúng tôi đạt được tỷ lệ nén 40X cho bộ mã hóa với ít hơn 1% mất mát độ chính xác. Theo hiểu biết tốt nhất của chúng tôi, kết quả của chúng tôi cho thấy tỷ lệ nén-độ chính xác tốt nhất cho BERT-Base, BERT-Large và DistilBERT.

1 Giới thiệu
Các mô hình ngôn ngữ được huấn luyện trước dựa trên Transformer (LM) như BERT [Devlin et al., 2019] và RoBERTa [Liu et al., 2019] đã trở thành phương pháp tiêu chuẩn cho nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP). Gần đây, chúng ta chứng kiến sự xuất hiện của các mô hình lớn hơn vài bậc độ lớn như GPT-2 [Radford et al., 2019], T-NLG [Rosset, 2020], GPT-3 [Brown et al., 2020] và Switch-C [Fedus et al., 2021]. Những mô hình này cải thiện kết quả tiên tiến nhất trong một số nhiệm vụ NLP như trả lời câu hỏi và phân loại văn bản. Tuy nhiên, xu hướng hướng tới các mô hình lớn hơn này làm nảy sinh một số lo ngại. Khi tài nguyên tính toán và bộ nhớ cần thiết để chạy suy luận tăng theo kích thước mô hình, việc triển khai những mô hình này trong môi trường sản xuất và trên các thiết bị biên trở nên rất tốn kém và thách thức. Hơn nữa, lượng lớn tài nguyên tính toán này gây ra chi phí môi trường đáng kể [Strubell et al., 2019].

Nén mô hình của các LM lớn là một lĩnh vực nghiên cứu đang phát triển do những lo ngại này. Cắt tỉa trọng số là một phương pháp nén đã được chứng minh là rất hiệu quả trong việc giảm dung lượng bộ nhớ của mô hình [Han et al., 2015, Zhu and Gupta, 2018]. Tuy nhiên, cắt tỉa trọng số của các LM dựa trên Transformer lớn đến tỷ lệ thưa cao đòi hỏi các phương pháp cắt tỉa chuyên biệt [Sanh et al.,

35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.arXiv:2111.05754v1 [cs.CL] 10 Nov 2021

--- TRANG 2 ---
2020, Chen et al., 2020, Gordon et al., 2020, Lagunas et al., 2021]. Hơn nữa, hầu hết các phương pháp cắt tỉa đều yêu cầu các sửa đổi và điều chỉnh cụ thể cho nhiệm vụ để tạo ra kết quả chất lượng.

Gordon et al. [2020] phát hiện rằng, về mặt độ chính xác, việc BERT được cắt tỉa trong giai đoạn huấn luyện trước hay trong giai đoạn học chuyển giao không quan trọng. Điều này cho thấy rằng một LM có thể được cắt tỉa một lần trong quá trình huấn luyện trước và sau đó được tinh chỉnh cho bất kỳ nhiệm vụ hạ nguồn nào mà không cần điều chỉnh cụ thể cho nhiệm vụ.

Trong bài báo này, chúng tôi trình bày một phương pháp mới, Prune Once for All (Prune OFA), tận dụng cắt tỉa trọng số và chưng cất mô hình để tạo ra các mô hình ngôn ngữ dựa trên Transformer được huấn luyện trước với tỷ lệ thưa cao. Chúng tôi áp dụng phương pháp của mình cho BERT-Base, BERT-Large và DistilBERT [Sanh et al., 2019] để tạo ra các mô hình thưa được huấn luyện trước cho những kiến trúc mô hình này. Sau đó chúng tôi cho thấy cách những mô hình thưa này có thể được tinh chỉnh để tạo ra các mô hình thưa cụ thể cho nhiệm vụ với mất mát độ chính xác tối thiểu cho SQuADv1.1 [Rajpurkar et al., 2016] cũng như cho bốn nhiệm vụ từ GLUE Benchmark [Wang et al., 2018]. Chúng tôi cũng cho thấy rằng có thể nén thêm các mô hình bằng cách sử dụng huấn luyện nhận biết lượng tử hóa để đạt được kết quả tiên tiến nhất về tỷ lệ nén-độ chính xác.

Những đóng góp chính của công trình này gồm ba phần: 1) Chúng tôi giới thiệu một phương pháp mới bất khả tri kiến trúc để huấn luyện các mô hình ngôn ngữ thưa được huấn luyện trước. 2) Chúng tôi chứng minh cách tinh chỉnh những mô hình thưa này trên các nhiệm vụ hạ nguồn để tạo ra các mô hình thưa và lượng tử hóa, loại bỏ gánh nặng cắt tỉa và điều chỉnh cho một nhiệm vụ ngôn ngữ cụ thể. 3) Chúng tôi công bố thư viện nghiên cứu nén của mình với các script ví dụ để tái tạo công trình của chúng tôi cho các kiến trúc khác, cùng với các mô hình thưa được huấn luyện trước được trình bày trong bài báo này.

2 Công trình liên quan
Các mô hình ngôn ngữ lớn được tham số hóa quá mức và khó triển khai. Do đó, vấn đề nén những mô hình này với mất mát độ chính xác tối thiểu cho các nhiệm vụ hạ nguồn được khám phá rộng rãi.

Sanh et al. [2020] đề xuất phương pháp Movement Pruning được thiết kế đặc biệt cho học chuyển giao. Neural Magic triển khai Gradual Magnitude Pruning.¹ Cả hai phương pháp đều đề xuất cắt tỉa BERT-Base trong khi tinh chỉnh cho các nhiệm vụ hạ nguồn kết hợp với chưng cất mô hình, và trình bày kết quả cho thấy độ thưa 90% cho một số nhiệm vụ. Tuy nhiên, cả hai phương pháp đều đòi hỏi thời gian tinh chỉnh dài cũng như điều chỉnh các siêu tham số liên quan đến cắt tỉa cho mỗi nhiệm vụ. Mặt khác, phương pháp của chúng tôi không đòi hỏi điều chỉnh các siêu tham số cắt tỉa đặc biệt cho từng nhiệm vụ vì chúng tôi cắt tỉa mô hình một lần cho tất cả nhiệm vụ. Hơn nữa, chúng tôi trình bày kết quả tốt hơn hoặc tương đương với ngân sách tính toán thấp hơn nhiều ở giai đoạn học chuyển giao. Gordon et al. [2020] khám phá hiệu ứng của cắt tỉa trọng số trong quá trình học chuyển giao và kết luận rằng cắt tỉa BERT-Base ở giai đoạn huấn luyện trước không làm giảm hiệu suất của mô hình so với cắt tỉa ở giai đoạn tinh chỉnh. Chúng tôi cải thiện phương pháp được đề xuất và trình bày kết quả tốt hơn ở tỷ lệ thưa cao hơn nhiều. Chen et al. [2020] khám phá Giả thuyết Vé số [Frankle and Carbin, 2018] cho các mô hình BERT được huấn luyện trước. Cụ thể hơn, họ phân tích khả năng tìm ra các vé thắng trong mô hình BERT-Base được huấn luyện trước có thể chuyển giao sang các nhiệm vụ hạ nguồn khác. Các tác giả kết luận rằng các vé thắng được tìm thấy trong khi huấn luyện trước trên nhiệm vụ Masked-LM chuyển giao tốt sang các nhiệm vụ hạ nguồn khác. Lagunas et al. [2021] trình bày một phương pháp cắt tỉa có cấu trúc, loại bỏ các hàng, cột và đầu attention, trong khi đạt được ít hơn 1% mất mát F1 cho kiến trúc BERT trên SQuADv1.1. Mishra et al. [2021] thực hiện cắt tỉa có cấu trúc 2:4 trên BERT trong khi tiếp tục huấn luyện trước BERT; Phương pháp này tạo ra mô hình thưa 50% có thể được tinh chỉnh mà không mất mát độ chính xác. Michel et al. [2019] khám phá tầm quan trọng của mỗi đầu trong cơ chế multi-head attention của BERT và trình bày phương pháp cắt tỉa các đầu attention với các trọng số liên quan.

Các công trình khác đề xuất chưng cất kiến thức để nén các mô hình Transformer thành một đối tác dày đặc nhỏ hơn có thể được điều chỉnh cho các nhiệm vụ hạ nguồn [Sanh et al., 2019, Jiao et al., 2020, Sun et al., 2020]. Lượng tử hóa các mô hình ngôn ngữ dựa trên Transformer cũng là một phương pháp nén nổi tiếng. Shen et al. [2020] đề xuất phương pháp lượng tử hóa BERT ở độ rộng bit khác nhau cho mỗi lớp. Các công trình khác triển khai huấn luyện nhận biết lượng tử hóa để lượng tử hóa BERT xuống 8bits [Kim et al., 2021, Zafrir et al., 2019]. Zhang et al. [2020] tạo ra phương pháp tạo BERT trọng số ternary. Kim and Hassan [2020] trình bày một pipeline nén cho các mô hình Transformer bao gồm chưng cất mô hình, lượng tử hóa và cắt tỉa đầu.

¹https://github.com/neuralmagic/sparseml/tree/main/integrations/huggingface-transformers

--- TRANG 3 ---
3 Cắt tỉa trọng số
Cắt tỉa trọng số là quá trình ép một số trọng số của mạng neural về zero. Cắt tỉa trọng số có thể là không có cấu trúc khi các trọng số riêng lẻ được cắt tỉa, hoặc có cấu trúc khi các nhóm trọng số có cấu trúc được cắt tỉa, ví dụ: khối, kênh, lớp. Cắt tỉa trọng số dẫn đến các mạng neural thưa giảm tính toán và dung lượng bộ nhớ của mô hình được huấn luyện.

Trong bài báo này chúng tôi tập trung vào cắt tỉa trọng số không có cấu trúc. Zhu and Gupta [2018] trình bày phương pháp Gradual Magnitude Pruning (GMP) để dần dần cắt tỉa các trọng số có độ lớn thấp trong quá trình huấn luyện. Trong quá trình huấn luyện, mỗi f bước, các trọng số có độ lớn thấp nhất được cắt tỉa cho đến khi đạt được tỷ lệ thưa tạm thời st cho bước thời gian t, được định nghĩa bởi

st=sf+ (si−sf)
(1−(t−ts)/(te−ts))³
(1)

trong đó si và sf là tỷ lệ thưa ban đầu và cuối cùng, và ts và te là các bước thời gian bắt đầu và kết thúc cắt tỉa.

Trong một bài báo gần đây, Renda et al. [2020] trình bày thuật toán cắt tỉa dựa trên IMP (Iterative Magnitude Pruning) [Han et al., 2015] và Learning Rate Rewinding (LRR). IMP bao gồm hai bước: cắt tỉa một phần của mô hình và tiếp tục tinh chỉnh nó để phục hồi từ lỗi cắt tỉa được tạo ra. Hai bước này được lặp lại cho đến khi đạt được tỷ lệ thưa mong muốn. Trong LRR, bộ lập lịch tốc độ học được quay lại trạng thái của nó trước bước cắt tỉa ở đầu bước tinh chỉnh. Chúng tôi đề xuất kết hợp nguyên lý quay lại tốc độ học vào GMP bằng cách quay lại bộ lập lịch tốc độ học về trạng thái của nó tại thời điểm ts mỗi f bước. Sau te, bộ lập lịch tiếp tục với cài đặt ban đầu cho đến khi kết thúc huấn luyện. Phụ lục C minh họa cách LRR kết hợp với GMP sửa đổi bộ lập lịch tốc độ học.

4 Chưng cất kiến thức
Chưng cất kiến thức, được giới thiệu bởi Hinton et al. [2015], là quá trình huấn luyện một mạng học sinh để tái tạo hành vi của mô hình giáo viên. Khi chưng cất được sử dụng để khớp các dự đoán của mô hình giáo viên, mất mát cross-entropy mềm giữa các xác suất mềm của học sinh và giáo viên được tính như sau:

Lkd=∑i ti log (si)    (2)

trong đó si là xác suất mềm được ước tính bởi học sinh, và ti là xác suất mềm tương ứng được ước tính bởi giáo viên cho cùng một mẫu đầu vào. Các xác suất mềm được tính bằng hàm softmax với nhiệt độ T.

Thông thường, giáo viên là một mô hình lớn đạt hiệu suất cao, và học sinh dựa trên kiến trúc nhỏ hơn. Trong bài báo này, chúng tôi đề xuất tận dụng phương pháp chưng cất mô hình cho quá trình cắt tỉa. Chúng tôi tập trung vào phương pháp mà cả giáo viên và học sinh đều chia sẻ cùng kiến trúc, nhưng khác nhau về tỷ lệ thưa. Trong trường hợp này, giáo viên là một mô hình dày đặc được huấn luyện trên một nhiệm vụ đích, và học sinh là một mô hình với độ thưa cố định hoặc đang trải qua cắt tỉa. Chưng cất-trong-quá-trình-cắt-tỉa có thể được áp dụng cho các mô hình ngôn ngữ trong cả giai đoạn huấn luyện trước và tinh chỉnh. Trong giai đoạn huấn luyện trước, giáo viên là một mô hình ngôn ngữ được huấn luyện trước, và trong giai đoạn tinh chỉnh, giáo viên là một mô hình ngôn ngữ được tinh chỉnh cho một nhiệm vụ đích.

5 Prune Once for All
Khái niệm cắt tỉa các mô hình ngôn ngữ như BERT [Devlin et al., 2019] trong khi huấn luyện trước đã được Chen et al. [2020] và Gordon et al. [2020] khám phá. Tuy nhiên, tinh chỉnh mô hình thưa cho một nhiệm vụ ngôn ngữ cụ thể dẫn đến kết quả kém hoặc tỷ lệ thưa thấp. Trong phần này chúng tôi sẽ giới thiệu phương pháp mới của mình, Prune OFA, để tạo ra các mô hình ngôn ngữ thưa được huấn luyện trước có thể được tinh chỉnh sau này cho các nhiệm vụ hạ nguồn với mất mát độ chính xác tối thiểu ở tỷ lệ thưa cao.

Một hình ảnh minh họa phương pháp của chúng tôi được trình bày trong Hình 1. Phương pháp này lấy một mô hình ngôn ngữ được huấn luyện trước làm đầu vào và xuất ra một mô hình ngôn ngữ thưa có cùng kiến trúc. Phương pháp bao gồm hai bước, chuẩn bị giáo viên và cắt tỉa học sinh. Mô hình thưa được huấn luyện trước mà chúng tôi huấn luyện là

--- TRANG 4 ---
Prune Once for All
Học chuyển giao
[+ chưng cất]
Tập dữ liệu
huấn luyện trước
Chuẩn bị
giáo viên
Mô hình ngôn ngữ
được huấn luyện trước
Cắt tỉa
học sinh
Khởi tạo
Mô hình ngôn ngữ được
huấn luyện trước đã tinh chỉnh
Mô hình ngôn ngữ thưa
được huấn luyện trước
Chưng cất
Giáo viên
Mô hình thưa
cuối cùng
Tập dữ liệu nhiệm vụ
Chưng cất
Giáo viên nhiệm vụ
Khóa mẫu

Hình 1: Phương pháp Prune OFA

mô hình chúng tôi sử dụng cho học chuyển giao trong khi duy trì mẫu thưa của nó. Chúng tôi gọi phương pháp này là Prune Once for All vì chúng tôi cho thấy cách tinh chỉnh các mô hình thưa được huấn luyện trước cho một số nhiệm vụ ngôn ngữ trong khi chúng tôi chỉ cắt tỉa mô hình được huấn luyện trước một lần.

Chuẩn bị giáo viên Bước đầu tiên của Prune OFA là thu được một mô hình được tối ưu hóa trên tập dữ liệu huấn luyện trước cho một số nhiệm vụ huấn luyện trước với mục tiêu LPT như được hiển thị trong Hình 1.² Cùng một tập dữ liệu sẽ được sử dụng để cắt tỉa học sinh trong bước tiếp theo. Mô hình này sẽ khởi tạo các mô hình học sinh và giáo viên trong bước cắt tỉa học sinh.

Cắt tỉa học sinh Một mô hình học sinh được khởi tạo từ giáo viên được chuẩn bị trong bước chuẩn bị giáo viên. Sau đó học sinh được tinh chỉnh trên một tổ hợp tuyến tính của nhiệm vụ huấn luyện trước, từ bước chuẩn bị giáo viên, và mục tiêu chưng cất kiến thức Lkd:

L=λPTLPT+λkdLkd    (3)

trong khi được cắt tỉa bằng các phương pháp GMP + LRR. Mô hình đầu ra của quá trình này là một LM thưa được huấn luyện trước có thể được sử dụng mà không cần cắt tỉa bổ sung cho học chuyển giao để tạo ra các mô hình thưa cho một nhiệm vụ hạ nguồn cụ thể.

Khóa mẫu Chúng tôi muốn giữ mẫu thưa của mô hình thưa được huấn luyện trước được tạo bởi Prune OFA tại chỗ trong quá trình tinh chỉnh. Chúng tôi đề xuất một phương pháp gọi là khóa mẫu ngăn các số không được tìm thấy trong mô hình thay đổi trong khi huấn luyện mô hình. Khóa mẫu được mô tả chi tiết hơn trong Phụ lục B.

6 Thiết lập thí nghiệm
Tập dữ liệu Chúng tôi sử dụng tập dữ liệu Wikipedia tiếng Anh (2500M từ) để huấn luyện các mô hình trên nhiệm vụ huấn luyện trước. Chúng tôi chia dữ liệu thành tập huấn luyện (~95%) và tập kiểm tra (~5%). Cả hai tập đều được tiền xử lý như được mô tả trong các bài báo gốc của mô hình [Devlin et al., 2019, Sanh et al., 2019]. Chúng tôi xử lý dữ liệu để sử dụng độ dài chuỗi tối đa cho phép bởi các mô hình, tuy nhiên, chúng tôi cho phép các chuỗi ngắn hơn với xác suất 0.1. Chúng tôi đánh giá các mô hình thưa được huấn luyện trước của mình trên một số điểm chuẩn phổ biến cho học chuyển giao; một nhiệm vụ trả lời câu hỏi, SQuADv1.1 chứa 89K ví dụ huấn luyện [Rajpurkar et al., 2016], và các nhiệm vụ phân loại văn bản sau từ GLUE Benchmark: MNLI, QQP, QNLI và SST-2 chứa tương ứng 393K, 364K, 105K, và 67K ví dụ huấn luyện [Wang et al., 2018, Williams et al., 2018, Iyer et al., 2017, Socher et al., 2013].

Áp dụng Prune Once for All Chúng tôi trình bày phương pháp của mình bằng cách áp dụng Prune OFA trên ba kiến trúc khác nhau có kích thước khác nhau; BERT-Base, BERT-Large và DistilBERT. Vì chúng tôi không có dữ liệu huấn luyện đã xử lý ban đầu được sử dụng để huấn luyện BERT-Base, BERT-Large và DistilBERT, chúng tôi chạy một bước bổ sung để tinh chỉnh các mô hình được huấn luyện trước bằng dữ liệu huấn luyện đã xử lý mà chúng tôi chuẩn bị. Tiếp theo, chúng tôi thực hiện bước cắt tỉa học sinh để thu được các mô hình thưa được huấn luyện trước của chúng tôi. Chúng tôi cắt tỉa BERT-Base và DistilBERT đến tỷ lệ thưa {85%; 90%} và BERT-Large đến tỷ lệ thưa 90%. Cắt tỉa được áp dụng cho tất cả các lớp Linear trong bộ mã hóa Transformer bao gồm lớp pooler nếu nó tồn tại. Các siêu tham số chính xác và chi tiết bổ sung được tóm tắt trong Phụ lục E.

²Ví dụ, nhiệm vụ huấn luyện trước cho BERT-Base là mô hình hóa ngôn ngữ có mặt nạ kết hợp với dự đoán câu tiếp theo.

--- TRANG 5 ---
Bảng 1: Kết quả Prune OFA BERT-Base so sánh với các phương pháp cắt tỉa khác

[BẢNG ĐỀ CẬP ĐẾN CÁC KẾT QUẢ SO SÁNH HIỆU SUẤT CỦA CÁC PHƯƠNG PHÁP KHÁC NHAU]

Bảng 2: Kết quả Prune OFA BERT-Large

[BẢNG KẾT QUẢ CHO BERT-LARGE]

Học chuyển giao Sau khi tạo ra các mô hình thưa được huấn luyện trước, chúng tôi tinh chỉnh chúng cho các nhiệm vụ NLP sau: SQuADv1.1, QNLI, MNLI, SST-2 và QQP. Chúng tôi sử dụng các siêu tham số mặc định cho mỗi nhiệm vụ và tiến hành tìm kiếm lưới cho các siêu tham số tốc độ học, weight decay, tỷ lệ warmup và số epoch huấn luyện. Đối với mỗi nhiệm vụ, chúng tôi báo cáo trung bình của hai lần chạy khác nhau với các seed khác nhau đạt được kết quả tốt nhất trên tập phát triển của nhiệm vụ. Chúng tôi cải thiện thêm kết quả của các mô hình thưa bằng cách tích hợp chưng cất kiến thức. Đối với mỗi nhiệm vụ và mô hình, chúng tôi tạo ra một giáo viên nhiệm vụ dựa trên mô hình dày đặc được huấn luyện trước ban đầu được tinh chỉnh cho nhiệm vụ. Đối với SQuADv1.1 và QQP, chúng tôi báo cáo kết quả tối đa hóa F1, và đối với MNLI, chúng tôi báo cáo kết quả tối đa hóa độ chính xác mismatched. Để biết các siêu tham số chính xác và chi tiết bổ sung, xem Phụ lục E.

So sánh với cắt tỉa tinh chỉnh Chúng tôi so sánh phương pháp Prune OFA của mình với cắt tỉa tinh chỉnh nơi chúng tôi cắt tỉa mô hình dày đặc được huấn luyện trước trong quá trình tinh chỉnh cho một nhiệm vụ hạ nguồn. Với mục đích đó, chúng tôi triển khai cắt tỉa GMP kết hợp với chưng cất kiến thức và chạy thí nghiệm sử dụng cùng giáo viên và siêu tham số được sử dụng trong các thí nghiệm học chuyển giao Prune OFA.

Lượng tử hóa Chúng tôi triển khai huấn luyện nhận biết lượng tử hóa tương tự như Q8BERT [Zafrir et al., 2019]. Để biết chi tiết về sự khác biệt giữa phương pháp của chúng tôi và Q8BERT, xem Phụ lục D. Đối với mỗi nhiệm vụ, chúng tôi chọn mô hình có hiệu suất tốt nhất cho nhiệm vụ này và thực hiện huấn luyện nhận biết lượng tử hóa trên nó. Chúng tôi sử dụng các siêu tham số hơi khác nhau cho phiên huấn luyện này như được mô tả trong Phụ lục E.2. Chúng tôi báo cáo trung bình của hai lần chạy khác nhau với các seed khác nhau đạt được kết quả tốt nhất.

7 Kết quả
Trong Bảng 1, chúng tôi trình bày kết quả thí nghiệm của mình cho việc cắt tỉa BERT-Base đến tỷ lệ thưa 85% và 90% bằng Prune OFA. Chúng tôi cũng trình bày kết quả của các phương pháp cắt tỉa khác được áp dụng cho BERT-Base cũng như kết quả của các thí nghiệm cắt tỉa tinh chỉnh mà chúng tôi đã tiến hành. Các kết quả không được đánh dấu trong cột Transfer with KD không sử dụng chưng cất mô hình trong giai đoạn học chuyển giao. Kết quả tốt nhất trong mỗi danh mục được đánh dấu bằng chữ đậm. Chúng tôi quan sát thấy rằng phương pháp của chúng tôi đạt được kết quả tốt hơn so với các công trình cắt tỉa trước đây khác trong khi huấn luyện trước ở tỷ lệ thưa cao hơn. Khi so sánh phương pháp Prune OFA của chúng tôi với các phương pháp cắt tỉa tinh chỉnh khác, chúng tôi quan sát thấy rằng phương pháp của chúng tôi tạo ra kết quả tốt nhất ở tỷ lệ thưa 85% và 90%. Hơn nữa, chúng tôi cho thấy sự suy giảm độ chính xác thấp hơn

³Kết quả được lấy từ kho mô hình thưa của Neural Magic: https://sparsezoo.neuralmagic.com/

--- TRANG 6 ---
Bảng 3: Kết quả Prune OFA DistilBERT so sánh với cắt tỉa tinh chỉnh

[BẢNG SO SÁNH KẾT QUẢ CHO DISTILBERT]

1% tương đối so với kết quả của mô hình dày đặc được huấn luyện trước ở độ thưa 85% ngoại trừ điểm chuẩn MNLI-matched. Lưu ý rằng đối với MNLI, các kết quả được báo cáo được chọn dựa trên độ chính xác mismatched tốt nhất của mô hình được tìm thấy trong tìm kiếm lưới của chúng tôi; khi tìm kiếm kết quả matched tốt nhất, chúng tôi giảm khoảng cách độ chính xác xuống còn 1% mất mát độ chính xác với chi phí tăng mất mát độ chính xác cho mismatched: 83.09/83.36(m/mm).

Kết quả cho việc cắt tỉa BERT-Large đến tỷ lệ thưa 90% được trình bày trong Bảng 2. Những kết quả này nằm trong phạm vi 1% mất mát độ chính xác cho tất cả các nhiệm vụ trừ nhiệm vụ MNLI. Chúng tôi kết luận rằng mô hình BERT-Large thưa 90% (~30.2M tham số khác không) mà chúng tôi huấn luyện có độ chính xác tốt hơn so với BERT-Base dày đặc (~85M tham số khác không).

Kết quả của chúng tôi cho việc cắt tỉa DistilBERT đến tỷ lệ thưa 85% và 90% được trình bày trong Bảng 3 với kết quả của chúng tôi cho các thí nghiệm cắt tỉa tinh chỉnh mà chúng tôi đã tiến hành. Ở cả hai tỷ lệ thưa, phương pháp của chúng tôi tạo ra kết quả độ chính xác tốt hơn so với cắt tỉa tinh chỉnh (kết quả tốt nhất trong mỗi danh mục được đánh dấu bằng chữ đậm). Hơn nữa, ở tỷ lệ thưa 85%, kết quả của chúng tôi nằm trong phạm vi 1% mất mát độ chính xác tương đối trong tất cả các nhiệm vụ trừ QQP.

Bảng 1, 2 và 3 trình bày kết quả lượng tử hóa, được chỉ định với hậu tố +QAT. Áp dụng huấn luyện nhận biết lượng tử hóa trên các mô hình thưa kết quả của chúng tôi làm giảm độ chính xác của mô hình thêm trung bình 0.67% tương đối so với độ chính xác của mô hình độ chính xác đầy đủ. Kết quả cho mô hình thưa 85% +QAT tốt hơn so với mô hình thưa 90% với độ chính xác đầy đủ trong tất cả các nhiệm vụ cho BERT-Base và trong 3/5 nhiệm vụ cho DistilBERT. Hơn nữa, mô hình thưa và lượng tử hóa 85% nhỏ hơn mô hình thưa 90% theo hệ số 0.375.

Một nghiên cứu loại bỏ đã được tiến hành để kiểm tra cách mỗi thành phần của phương pháp Prune OFA ảnh hưởng đến khả năng của mô hình được huấn luyện trước để chuyển giao kiến thức của nó cho các nhiệm vụ hạ nguồn, như được mô tả trong Phụ lục A.

8 Kết luận và công việc tương lai
Chúng tôi đã giới thiệu Prune OFA, một phương pháp bất khả tri kiến trúc để tạo ra các mô hình ngôn ngữ thưa được huấn luyện trước. Chúng tôi cũng đã cho thấy cách những mô hình thưa này có thể được sử dụng để thu được các mô hình thưa được tinh chỉnh mà không cần gánh nặng cắt tỉa cụ thể cho nhiệm vụ. Kết quả của chúng tôi cho thấy rằng việc sử dụng những mô hình thưa được huấn luyện trước này cho học chuyển giao tạo ra kết quả với sự suy giảm hiệu suất tối thiểu so với đối tác dày đặc của chúng cho nhiều nhiệm vụ NLP khác nhau. Chúng tôi đã chứng minh thêm rằng việc tích hợp lượng tử hóa có thể dẫn đến các mô hình thưa và lượng tử hóa hiệu quả hơn với chi phí nhỏ cho độ chính xác của mô hình.

Một hướng nghiên cứu có thể trong tương lai là khám phá liệu một mô hình lớn và thưa được huấn luyện trước có tốt hơn trong việc nắm bắt và chuyển giao kiến thức ngôn ngữ tự nhiên so với một mô hình dày đặc nhỏ hơn có cùng kiến trúc với số lượng tham số khác không tương tự hay không.

Chúng tôi hy vọng rằng việc phát hành mã và các mô hình thưa được huấn luyện trước của chúng tôi cho cộng đồng sẽ giúp phát triển các mô hình hiệu quả hơn.

--- TRANG 7 ---
9 Lời cảm ơn
Chúng tôi biết ơn Ella Charlaix của HuggingFace vì những bình luận và chỉnh sửa hữu ích của cô ấy.

Tài liệu tham khảo
[Tài liệu tham khảo được liệt kê với định dạng học thuật tiêu chuẩn]

--- TRANG 8-12 ---
[Các trang tiếp theo chứa thêm các bảng chi tiết, nghiên cứu loại bỏ, và phụ lục kỹ thuật với các tham số và cài đặt thí nghiệm cụ thể]
