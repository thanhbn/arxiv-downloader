# 2212.12651.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2212.12651.pdf
# File size: 437183 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pruning On-the-Fly: A Recoverable Pruning Method without Fine-tuning
Dan Liu, Xue Liu
McGill University
daniel.liu@mail.mcgill.ca, xue.liu@mcgill.ca
Abstract
Most existing pruning works are resource intensive, as they
require retraining or ﬁne-tuning of the pruned models for the
purpose of accuracy. We propose a retraining-free pruning
method based on hyperspherical learning and loss penalty
terms. The proposed loss penalty term pushes some of the
model weights far away from zero, while the rest weight val-
ues are pushed near zero and can be safely pruned with no
need of retraining and a negligible accuracy drop. In addition,
our proposed method can instantly recover the accuracy of a
pruned model by replacing the pruned values with their mean
value. Our method obtains state-of-the-art results in terms of
retraining-free pruning and is evaluated on ResNet-18/50 and
MobileNetV2 with ImageNet dataset. One can easily get a
50% pruned ResNet18 model with a 0.47% accuracy drop. If
with ﬁne-tuning, the experiment results show that our method
can signiﬁcantly boost the accuracy of the pruned models
compared with existing works. For example, the accuracy
of a 70% pruned (except the ﬁrst convolutional layer) Mo-
bileNetV2 model only drops 3.5%, much less than the 7% 
10% accuracy drop with conventional methods.
Introduction
Deep neural network (DNN) models contain millions of pa-
rameters, making them impossible to deploy on edge de-
vices. Model size and inference efﬁciency are major con-
cerns when deploying under resources constraints. Signif-
icant research efforts have been made to compress DNN
models. Quantization and pruning are popular as they can
reduce the model size and computational overhead.
There are two interesting research topics in pruning: how
to reduce the ﬁne-tuning time and how to rapidly recover
the network’s accuracy from pruning. The purpose of model
pruning is to get a DNN model with maximum accuracy and
compression ratio. Finding a proper pruning strategy is the
main challenge. Most of the existing works need ﬁne-tuning.
The steps of pruning and ﬁne-tuning are repeated multiple
times to gradually reduce the model size and maintain a
higher accuracy. The ﬁne-tuning process is time consuming
and requires the whole training dataset. Therefore, studies
have been made to explore ways to improve the ﬁne-tuning
efﬁciency and the recovery ability of neural networks with
only a few training data. However, some of the weight values
are ﬁxated to zero permanently during pruning. The neuralnetwork is changing during training, ﬁxing some weight val-
ues to zero may restrict its learning ability. The incorrectly
pruned weight values are inevitable and hard to be recov-
ered or corrected because the original weight information is
lost. Therefore, some researchers propose pruning before or
during training so that the network can adapt to pruning.
In this work, we aim to eliminate the ﬁne-tuning after
pruning, i.e., leveraging ﬁne-tuning before pruning to for-
mulate reliable and accurate potential pruning candidates.
More speciﬁcally, compared with other works that prune the
dense model directly, our method reduces the cosine dis-
tance between the dense weight values and its pruning mask
before the pruning action. Our method is less prone to false
pruning as the pruning mask is constantly adapting to the
training and the potential pruned weights are pushed near
zero. Once the cosine distance is small enough, the model
can be pruned to many different sparse levels without any
ﬁne-tuning. With our method, a pruned ResNet-18 model
can reach up to 50% sparsity with less than 0.5% accuracy
drop. Combined with our proposed instant recovery method,
this sparsity can be pushed up to 70% with 0.3% accuracy
drop. Our main contributions are as follows:
• We propose a on-the-ﬂy pruning method which uses reg-
ularization terms to minimize the cosine distance be-
tween weight values and its pruning mask during train-
ing. Once the training is completed, the weight values
will be separated into two groups, one being close to zero
and the other being far from zero. The processed model
can be pruned instantly without any ﬁne-tuning.
• We propose a method to increase the model’s recov-
ery ability. We show that replacing part of the pruned
weights with their mean values can recover part of the
model’s performance immediately. Our pruning method
can greatly improve this instant recovery ability. In addi-
tion, our method can signiﬁcantly improve the pruning
potential under high sparsity settings with ﬁne-tuning.
For example, for the MobileNetV2 structure with 70%
sparsity (except for the ﬁrst convolutional layer), the ﬁne-
tuned accuracy of other methods drops by 7-10%, while
ours only drops by 3.5%.arXiv:2212.12651v1  [cs.CV]  24 Dec 2022

--- PAGE 2 ---
0.100
 0.075
 0.050
 0.025
 0.000 0.025 0.050 0.075 0.100
Weight0.000.250.500.751.001.251.501.752.00 Count1e4(a)tr=0
0.100
 0.075
 0.050
 0.025
 0.000 0.025 0.050 0.075 0.100
Weight0.000.250.500.751.001.251.501.752.00 Count1e4 (b)tr=0.9
0.100
 0.075
 0.050
 0.025
 0.000 0.025 0.050 0.075 0.100
Weight0.000.250.500.751.001.251.501.752.00 Count1e4 (c)tr=0.7
0.100
 0.075
 0.050
 0.025
 0.000 0.025 0.050 0.075 0.100
Weight0.000.250.500.751.001.251.501.752.00 Count1e4 (d)tr=0.5
0.100
 0.075
 0.050
 0.025
 0.000 0.025 0.050 0.075 0.100
Weight0.000.250.500.751.001.251.501.752.00 Count1e4
(e)tr=0
0.100
 0.075
 0.050
 0.025
 0.000 0.025 0.050 0.075 0.100
Weight0.000.250.500.751.001.251.501.752.00 Count1e4 (f)tr=0.9
0.100
 0.075
 0.050
 0.025
 0.000 0.025 0.050 0.075 0.100
Weight0.000.250.500.751.001.251.501.752.00 Count1e4 (g)tr=0.7
0.100
 0.075
 0.050
 0.025
 0.000 0.025 0.050 0.075 0.100
Weight0.000.250.500.751.001.251.501.752.00 Count1e4 (h)tr=0.5
Figure 1: The weight distribution of a layer of the baseline models (a), on-the-ﬂy pruning models (b, c, d) and their ﬁne-tuned
versions (e, f, g, h). The red columns are the pruned weight values ( r=0.6, Eq. (4)). The purple columns are the remaining weight
values without ﬁne-tuning. The green columns are the ﬁne-tuned weight values. The weight values of (b, c, d) are separated
into three parts by the regularization term Ltrbefore pruning and close to their ﬁne-tuned versions (f, g, h). The trcontrols the
portions of near-zero values (red areas). All of the models in this ﬁgure use ResNet-18 structure and the baseline is obtained
from PyTorch Zoo.
Related Work
Model compression techniques, such as quantization (Wu
et al. 2016; Li, Zhang, and Liu 2016) and pruning (Han,
Mao, and Dally 2015; Li et al. 2016), have been a trend-
ing research topic as they contribute to smaller model sizes
and faster inference. A comprehensive overview of model
pruning can be found in (Liang et al. 2021; Blalock et al.
2020).
Pruning Approaches Pruning can be categorised by
structure and scheduling. The pruning structure speciﬁes
whether to prune the layer (Chin, Zhang, and Marculescu
2018; Dong, Chen, and Pan 2017), the entire kernel (Li et al.
2016; Hu et al. 2016; Alvarez and Salzmann 2017), or par-
ticular weight values (Han, Mao, and Dally 2015). Pruning
scheduling determines the percentages of the weight values
to be removed through each phase. Some techniques per-
form a one-step pruning to the target weights (Liu et al.
2018). Others change the pruning ratio during training (Han
et al. 2016; Gale, Elsen, and Hooker 2019) or iteratively
prune a ﬁxed portion of the weight values across a number
of iterations (Han et al. 2015).
Pruning Without Fine-tuning Pruning decreases the
model’s accuracy. Fine-tuning, despite being time consum-
ing, is frequently performed to recover accuracy. Many
works explore the possibilities of pruning during or even
prior to training without ﬁne-tuning (Guo, Yao, and Chen
2016; Molchanov, Ashukha, and Vetrov 2017; Lee, Ajan-
than, and Torr 2018; Gale, Elsen, and Hooker 2019). Guo,
Yao, and Chen (2016) propose a recoverable pruning method
using binary mask matrices with a score function to deter-
mine whether a single weight value is to be pruned or not.Soft Filter Pruning (SFP) (He et al. 2018) expands recover-
able pruning even further by enabling update of the pruned
ﬁlters. Sparse variational dropout (Molchanov, Ashukha,
and Vetrov 2017) employs a dropout hyperparameter that
encourages sparsity and acts as a basis for scoring to deter-
mine which weights to prune. Dynamic pruning (Lin et al.
2017; Wu et al. 2018), which chooses the pruned weight val-
ues using decision components, is another ﬁeld of research
in ﬁne-tuning-free pruning. However, some of the dynamic
pruning works are resource intensive as the pruning decision
is made in real-time (Leroux et al. 2017; Li et al. 2019; Gao
et al. 2018).
In this work, we study the instant pruning and recovery
ability of the neural networks. Compared to the methods
stated above, our method aims at using loss penalty to re-
duce the cosine distance between the dense weight values
and its binary mask. Therefore, the optimization process can
change the weight distribution(Figure 1) and beneﬁt prun-
ing. Our method has an advantage over the existing prun-
ing approaches in that it prevents incorrectly pruning and
restores the accuracy of the model. Moreover, our strategy
does not require a complicated training schedule or sophis-
ticated score function. A simple magnitude-base one-time
pruning can produce remarkable outcomes.
Pruning, Recovery and Fine-tuning
In this section, we show how to perform on-the-ﬂy pruning,
and how to recover the model’s accuracy instantly or slowly.
Before pruning, we apply a regularization term to the ob-
jective function to manipulate the weight distribution (Fig-
ure 1). Then the magnitude-based, unstructured pruning can
be performed with a negligible accuracy drop. The pruned

--- PAGE 3 ---
model can be recovered by replacing the pruned values with
their mean values immediately or recovered by conventional
ﬁne-tuning with much higher sparsity.
Regularized Hyperspherical Learning
Hyperspherical learning (Liu et al. 2017) restricts the mag-
nitude of the input and weight vectors to one. A general rep-
resentation of a hyperspherical layer is deﬁned as:
y=(W>x); (1)
wheredenotes a nonlinear activation function, W2
Rmnis the weight matrix, x2Rmis the input vector
to the layer, and y2Rnis the output vector. Each column
weight vector wj2RmofWsubjects tokwjk2= 1for all
j= 1;:::;n , and the input vector xsatisﬁeskxk2= 1.
Given a regular objective function L, we formulate the
optimization process as:
min
WJ(W) =L(W) +Ltr(W;tr) (2)
s:t:W2R;0<tr< 1:
The regularization term Ltris deﬁned as:
Ltr(W;r) =1
n 
trace (W>M I)2(3)
s:t:M=HyperSign (Prune (W;tr));
where Prune ()denotes magnitude-based unstructured
pruning with sparsity tr,Mis a mask, and trace ()re-
turns the trace of a matrix. The quadratic term and are
applied to keep L(W)andLtr(W;r)at the same scale.
With hyperspherical learning, HyperSign ()returns the
normalized Sign (), i.e., a pruning mask Mon the hy-
persphere. More speciﬁcally, mij2 f0;1p
kmjk1g, where
kmjk1denotes the number of non-zero elements in the j-th
column vector mjofM, andkmjk2= 1for allj= 1;:::;n .
For example, tr= 0:9indicates that 90% of Mis zero.
The diagonal elements (trace) of W>Min Eq. (3) de-
notes the cosine similarities between wjandmj. Minimiz-
ingLtr(Eq. (3)) is equivalent to pushing wjclose to mj,
namely, making part of the magnitude of weight values close
to1p
kmjk1while the rest to zero. Adjusting trwill change
the shape of the weight distributions (Figure 1). The regu-
larization term Ltrcan be applied to pre-trained and train-
from-scratch models. In practice, gradually decreasing tr
from a higher value, e.g., 0:9, to target value, e.g., 0:7, per-
forms better than using a ﬁxed tr= 0:7. We compare the
mentioned settings in the experiment section.
Pruning and Recovery
On-the-ﬂy Pruning Having adjusting the weight distribu-
tion via Eq. (3), we can prune the model by the magnitude-
based unstructured pruning Prune (), and:
W=W0+Prune (W;r) (4)
where 0<r< 1is the pruning ratio, W0denotes the pruned
weight values, and Prune ()returns the remaining weights.Regarding hyperspherical learning, given W, removing
its near-zero values W0or re-scaling the magnitude of wj
will not affect the model’s performance as long as the di-
rections of Wremain unchanged. This allows for on-the-
ﬂy pruning with no more ﬁne-tuning afterwards, which is
not feasible with conventional pruning methods due to the
impact of weight magnitude changes. However, based on
our experiment results and other works (Lazarevich, Kozlov,
and Malinin 2021), one of the limitations of pruning with-
out ﬁne-tuning is that the accuracy still drops signiﬁcantly
as soon as the pruning ratio exceeds 50%. We explore two
ways to address this issue.
Instant Recovery with Mean Values To recover a pruned
model with higher sparsity, we replace the pruned weights
W0with a replacement sign (W0)and minimize the
Euclidian distance between them along with a layer-wise
scaling factor :
min
kW0 W0
sgnk2
2; (5)
s:t:W0
sgn2f  1;1g;> 0:
Andhas a closed form solution (Li, Zhang, and Liu 2016):
=1
kWsgnk1X
jw0j; (6)
which is the mean magnitude value of the pruned weights.
TheW0
sgnis important as it keeps the direction informa-
tion. The work of (Zhou et al. 2019) also supports this in-
tuition by pointing out that the sign is crucial to recover a
pruned model. Compared with ﬁlling zero, ﬁlling the pruned
weights with W0
sgnprovides a more accurate approxima-
tion of the direction information of W. It is worth noting
that this recovery method only works well when W0’s de-
viation is minor, as a higher deviation indicates a change in
direction. Therefore it is preferable to replace part of rather
than all of the pruned weight values. This recovery property
is related to the pruning ratio rand the regularization term
Ltr. We perform ablation study on them in the experiment
section.
Our experimental results further reveal that the instant
recovery property exists in both hyperspherical and non-
hyperspherical settings. The regularization term Ltrwith hy-
perspherical learning can stabilize this recovery ability as it
produces more near-zero weight values and is less sensitive
to weight magnitudes changes.
The instant recovery ability is promising since we only
need to store a binary mask W0
sgnand a mean value for
each layer to compress models. With the proposed instant
recovery method, a hyperspherical ResNet-18 model can be
pruned up to 70% sparsity with 0.3% accuracy drop with-
out any ﬁne-tuning, which outperforms most of the existing
ﬁne-tuning-based pruning works. However, in practice, we
observe that the instant recovery only works well on Mo-
bileNetV2 with sparsity less than 30%.
Slow Recovery with Fine-tuning The instant recovery
method only beneﬁts model size compression, while it does
not reduce FLOPs as the zero values are replaced by mean
values. Therefore, we further explore the impact of Ltron

--- PAGE 4 ---
ﬁne-tuning-based pruning. We follow the conventional way
in to ﬁne-tune the processed models (Blalock et al. 2020).
Training Details
Our proposed method can be applied to from-scratch-
training, with more training efforts and slightly lower ac-
curacy though, as the Ltrterm relies on converged model
weights as do conventional pruning methods. All of the
models in our experiment are initialized from PyTorch Zoo
except the training-from-scratch ones in ablation study. The
recommended initial settings for Ltraretr= 0:9, and
0:5<  < 2. The initial depends on different network
structure. For example, = 2 for ResNet18/MobileNetV2,
and= 1 for ResNet50. The overall process can be sum-
marized as: i) Fine-tuning with hyperspherical learning (Liu
et al. 2017) and Ltrfrom pre-trained PyTorch Zoo models
with speciﬁc tr(Eq. (3)); ii) Apply instant recovery or ﬁne-
tuning after unstructured pruning.
We use PyTorch mixed precision with 8 Nvidia A100
GPU for training. We use the cosine annealing schedule with
restarts (every 10 epochs) (Loshchilov and Hutter 2016) to
adjust the learning rates. The initial learning rate is 0.01 and
the batch size is 256. Gradually decreasing trfrom 0:9to
0:7within 90 epochs can obtain a good result.
Experiment
In this section, we study the impact of trto the accuracy
of on-the-ﬂy pruning, instant recovery, and ﬁne-tuning. We
perform image classiﬁcation task to evaluate our proposed
method on the ImageNet dataset (Russakovsky et al. 2015)
with ResNet-18/50 (He et al. 2016) and MobileNetV2 (San-
dler et al. 2018) architectures. We use “+” to denote train-
ing from scratch models. The other models are initialized
by pre-trained weights provided by the PyTorch zoo.“ !”
denotes gradually decrease. “ tr=0” denotes baseline model
obtained from PyTorch.
Experimental Setup
The batch size is 256. The weight decay is 0.0001, and the
momentum of stochastic gradient descent (SGD) is 0.9. Dur-
ing ﬁne-tuning, we use the cosine annealing schedule with
restarts (Loshchilov and Hutter 2016) to adjust the learning
rates. The initial learning rate is 0.01. When training from
scratch, we follow the recipe from PyTorch. We prune all of
the linear and convolutional layers except the ﬁrst convolu-
tional layer.
Training from Scratch Our proposed Ltrcan be applied
directly to training-from-scratch. Table 1 shows the compar-
ison results of pre-trained and trained-from-scratch method.
As we stated above, the on-the-ﬂy pruning accuracy of the
training-from-scratch model is slightly worse than the pre-
trained ones. In the Table 4, we also compare the recovery
ability between them. Although the training-from-scratch
models are not as good as the pre-trained ones, they still can
outperform the baseline model. In addition, when training
from scratch, the ﬁxed tr= 0:95performs better than the
gradually decreased ones, i.e. tr= 0:9!0:7.Pruning On-the-ﬂy
Pruning on-the-ﬂy means pruning directly by using unstruc-
tured (Han, Mao, and Dally 2015) method. We compare dif-
ferent settings of sparsity and trto study their impact to the
performance. The results are shown in Table 1, 2 and 3.
The impact of different trand sparsity is listed in the Ta-
ble 1. The pre-trained models with gradually decreased tr
signiﬁcantly outperform the baseline models. We also com-
pare the impact on ResNet-50 (Table 2), and MobileNetV2
(Table 3).
Sparsitytr=0tr=0:95tr=0:9tr=0:95!0:9tr=0:9!0:7tr=0:9!0:7+tr=0:9+
Dense 69.54 68.80 69.74 69.47 69.74 69.03 69.19
30% 68.98 68.50 69.41 69.31 69.55 68.61 68.78
50% 65.20 66.65 60.00 68.14 68.97 67.13 66.79
70% 40.03 53.44 9.70 62.99 64.82 24.08 54.77
Table 1: The instant pruning accuracy of ResNet18 on the
ImageNet dataset. “+” denotes training from scratch. “ !”
denotes gradually decrease. “ tr=0” denotes baseline model
from PyTorch.
Sparsity Baseline tr=0 tr=0:9!0:7
Dense 76.15 77.15
30% 75.28 76.94
50% 72.90 76.62
70% 43.82 69.01
Table 2: The instant pruning accuracy of ResNet-50 on the
ImageNet dataset.
Sparsity Baseline tr=0 tr=0:9!0:7
Dense 71.35 70.75
10% 71.28 70.59
20% 69.50 68.57
30% 59.12 60.60
40% 13.80 34.85
Table 3: The instant pruning accuracy of MobileNetV2 on
the ImageNet dataset.
Instant Recovery
In this section, we compare the instant recovery ability. We
ﬁll part of the pruned weight values. “ Sp1” denotes the start-
ing point and “ Sp2” denotes the target sparsity. For exam-
ple, “ Sp1=0.3” and “ Sp2=0.5” means the pruned weight val-
ues in the pruning range from 30% to 50% are replaced by
Eq. (6), i.e., W0=Prune (W;0:3) Prune (W;0:5); the
sparsity is 50% and with one extra mask W0.
We compare with baseline models from PyTorch, DPF
(Lin et al. 2020), and DSR (Mostafa and Wang 2019). Un-
like other ﬁne-tuning-free methods, our method can produce
models with different sparsity and accuracy. We also observe

--- PAGE 5 ---
that once the sparsity is close to tr, the instant recovery abil-
ity drops quickly (Table 5). The MobileNetV2 is very sensi-
tive to the instant pruning (Table 6) and the performance is
not as good as the baseline models.
Sp1 Sp2 tr=0tr=0:9tr=0:95 tr=0:9!0:7tr=0:95!0:9tr=0:9!0:7+tr=0:9+
Dense 69.54 68.80 69.74 69.47 69.74 69.03 69.19
10% 40% 69.41 69.71 68.81 68.75 69.45 68.86 69.11
60% 68.67 69.46 68.42 63.69 69.13 66.07 68.49
80% 65.15 47.28 66.38 53.62 67.71 38.45 66.00
30% 50% 68.98 69.54 68.51 68.99 69.29 68.71 68.77
70% 68.32 69.07 68.29 67.51 69.08 61.35 68.36
90% 62.78 50.07 63.68 58.45 35.56 31.15 60.00
Table 4: The instant recovery accuracy of ResNet18 on the
ImageNet dataset.“+” denotes training from scratch. “ !”
denotes gradually decrease. “ tr=0” denotes baseline model
from PyTorch.
Sp1 Sp2 Baseline tr=0 tr=0:9!0:7
Dense 76.15 77.15
10% 40% 75.69 77.15
60% 75.44 76.91
80% 73.09 33.16
30% 50% 75.26 76.93
70% 74.93 76.28
90% 71.43 2.36
Table 5: The instant recovery accuracy of ResNet-50 on the
ImageNet dataset.
Sp1 Sp2 Baseline tr=0 tr=0:9!0:7
Dense 71.88 70.75
10% 40% 69.47 66.73
60% 59.84 6.72
80% 8.25 -
30% 50% 58.71 55.1
70% 47.8 3.82
90% 1.99 -
Table 6: The instant recovery accuracy of MobileNetV2 on
the ImageNet dataset.
Methods Dense Sparse Diff. Sparsity
DSR (M OSTAFA AND WANG 2019) 74.9 71.6 3.30 80%
DPF (L IN ET AL . 2020) 75.95 75.13 0.82 80%
POF (O URS)tr=0:95 75.85 75.27 0.58 35-80%
DSR (M OSTAFA AND WANG 2019) 74.9 73.3 1.6 71.4%
DPF (L IN ET AL . 2020) 75.95 75.48 0.47 73.5%
POF (O URS)tr=0:9 75.85 75.51 0.34 40-70%
Table 7: The instant recovery accuracy of ResNet50 on the
ImageNet dataset. “Diff.” denotes the accuracy difference
between the dense and sparse models. “35-80%” means
W0=Prune (W;0:35) Prune (W;0:8)(Eq. (6)).Fine-tuning
We compare our result with one-shot pruning (Han, Mao,
and Dally 2015), gradual pruning(Zhu and Gupta 2017), and
cyclical pruning (Srinivas et al. 2022). The ﬁne-tuning accu-
racy is outstandingly improved by our proposed method. For
example, the MobileNetV2 with 50% sparsity even outper-
forms the accuracy of the original dense model (Table 10);
with 70% sparsity, our method only brings 3.54% accuracy
drop, whereas the conventional pruning methods reduce 7%
- 10% of accuracy.
Methods Dense Sparse Diff. Sparsity
ONE-SHOT (HAN ET AL . 2015) 69.70 63.50 4.2 90%
GRADUAL (ZHU AND GUPTA 2017) 69.70 63.60 4.1 90%
CYCLICAL (SRINIVAS ET AL . 2022) 69.70 64.90 4.8 90%
POF (O URS)tr=0:95 68.86 65.69 3.17 90%
POF (O URS)tr=0:9 69.73 64.95 4.78 90%
POF (O URS)tr=0:7 69.73 63.02 6.68 90%
ONE-SHOT (HAN ET AL . 2015) 69.70 68.20 1.5 80%
GRADUAL (ZHU AND GUPTA 2017) 69.70 67.80 1.9 80%
CYCLICAL (SRINIVAS ET AL . 2022) 69.70 68.30 1.4 80%
POF (O URS)tr=0:95 68.86 68.67 0.19 80%
POF (O URS)tr=0:9 69.73 69.09 0.64 80%
POF (O URS)tr=0:7 69.73 68.89 0.84 80%
ONE-SHOT (HAN ET AL . 2015) 69.70 69.20 0.5 70%
GRADUAL (ZHU AND GUPTA 2017) 69.70 69.20 0.5 70%
CYCLICAL (SRINIVAS ET AL . 2022) 69.70 69.40 0.3 70%
POF (O URS)tr=0:95 68.86 69.99 -1.13 70%
POF (O URS)tr=0:9 69.73 70.44 -0.71 70%
POF (O URS)tr=0:7 69.73 69.97 -0.24 70%
ONE-SHOT (HAN ET AL . 2015) 69.70 69.90 -0.2 60%
GRADUAL (ZHU AND GUPTA 2017) 69.70 69.90 -0.2 80%
CYCLICAL (SRINIVAS ET AL . 2022) 69.70 69.60 0.1 60%
POF (O URS)tr=0:95 68.86 70.42 -1.56 60%
POF (O URS)tr=0:9 69.73 70.48 -0.75 60%
POF (O URS)tr=0:7 69.73 70.28 -0.55 60%
Table 8: The ﬁne-tuned Top-1 test accuracy of ResNet18
on the ImageNet dataset. Our method (POF) outperforms
the existing methods in terms of accuracy and difference
(“Diff.”). In addition, our method has a relative lower start-
ing accuracy.
Conclusion
In this paper, we show that hyperspherical learning with loss
regularization term can greatly improve the performance of
model pruning. Our method uses the regularization term to
control the distribution of weights, with no need for ﬁne-
tuning after pruning if the sparsity is less than 50%. Com-
pared with existing ﬁne-tuning based methods, our method
can signiﬁcantly improve the ﬁne-tuned accuracy. We also
explore the recovery ability of the pruned models. Our re-
sults show that the pruned models can be recovered by re-
placing the pruned values with their mean value. Combined
with the proposed on-the-ﬂy pruning and instant recovery,
our method can generate various sparsity and accuracy mod-
els instantly.

--- PAGE 6 ---
Methods Dense Sparse Diff. Sparsity
ONE-SHOT (HAN ET AL . 2015) 76.16 72.8 3.36 90%
GRADUAL (ZHU AND GUPTA 2017) 76.16 71.9 4.26 90%
CYCLICAL (SRINIVAS ET AL . 2022) 76.16 73.3 2.86 90%
POF (O URS)tr=0:95 77.00 73.79 3.21 90%
POF (O URS)tr=0:9 76.62 74.68 1.94 90%
POF (O URS)tr=0:7 77.15 5.82 71.33 90%
ONE-SHOT (HAN ET AL . 2015) 76.16 75.4 0.76 80%
GRADUAL (ZHU AND GUPTA 2017) 76.16 74.9 1.26 80%
CYCLICAL (SRINIVAS ET AL . 2022) 76.16 75.3 0.86 80%
POF (O URS)tr=0:95 77.00 76.24 0.39 80%
POF (O URS)tr=0:9 76.62 76.23 0.64 80%
POF (O URS)tr=0:7 77.15 76.02 1.13 80%
ONE-SHOT (HAN ET AL . 2015) 76.16 75.9 0.26 70%
GRADUAL (ZHU AND GUPTA 2017) 76.16 75.8 0.36 70%
CYCLICAL (SRINIVAS ET AL . 2022) 76.16 75.7 0.46 70%
POF (O URS)tr=0:95 77.00 76.81 0.19 70%
POF (O URS)tr=0:9 76.62 76.5 0.12 70%
POF (O URS)tr=0:7 77.15 76.74 0.41 70%
Table 9: The ﬁne-tuned Top-1 test accuracy of ResNet50 on
the ImageNet dataset. “POF” denotes our proposed method.
“Diff.” denotes the accuracy difference between the dense
and sparse models.
Methods Dense Sparse Diff. Sparsity
ONE-SHOT (HAN ET AL . 2015) 71.7 61.3 10.4 70%
GRADUAL (ZHU AND GUPTA 2017) 71.7 62.7 9 70%
CYCLICAL (SRINIVAS ET AL . 2022) 71.7 64.4 7.3 70%
POF (O URS)tr=0:9 70.43 66.89 3.54 70%
ONE-SHOT (HAN ET AL . 2015) 71.7 66.7 5 60%
GRADUAL (ZHU AND GUPTA 2017) 71.7 67.6 4.1 60%
CYCLICAL (SRINIVAS ET AL . 2022) 71.7 68.4 3.3 60%
POF (O URS)tr=0:9 70.43 70.22 0.21 60%
ONE-SHOT (HAN ET AL . 2015) 71.7 67.6 4.1 50%
GRADUAL (ZHU AND GUPTA 2017) 71.7 69.8 1.9 50%
CYCLICAL (SRINIVAS ET AL . 2022) 71.7 70.1 1.6 50%
POF (O URS)tr=0:9 70.43 71.77 -1.34 50%
Table 10: The ﬁne-tuned Top-1 test accuracy of Mo-
bileNetV2 on the ImageNet dataset. Our method (“POF”)
signiﬁcantly outperform the existing results.
References
Alvarez, J. M.; and Salzmann, M. 2017. Compression-aware
training of deep networks. In Advances in Neural Informa-
tion Processing Systems , 856–867.
Blalock, D.; Gonzalez Ortiz, J. J.; Frankle, J.; and Guttag,
J. 2020. What is the state of neural network pruning? Pro-
ceedings of machine learning and systems , 2: 129–146.
Chin, T.-W.; Zhang, C.; and Marculescu, D. 2018. Layer-
compensated pruning for resource-constrained convolu-
tional neural networks. arXiv preprint arXiv:1810.00518 .
Dong, X.; Chen, S.; and Pan, S. 2017. Learning to prune
deep neural networks via layer-wise optimal brain surgeon.
InAdvances in Neural Information Processing Systems ,
4857–4867.
Gale, T.; Elsen, E.; and Hooker, S. 2019. The state
of sparsity in deep neural networks. arXiv preprint
arXiv:1902.09574 .Gao, X.; Zhao, Y .; Dudziak, Ł.; Mullins, R.; and Xu, C.-
z. 2018. Dynamic channel pruning: Feature boosting and
suppression. arXiv preprint arXiv:1810.05331 .
Guo, Y .; Yao, A.; and Chen, Y . 2016. Dynamic network
surgery for efﬁcient dnns. Advances in neural information
processing systems , 29.
Han, S.; Mao, H.; and Dally, W. J. 2015. Deep com-
pression: Compressing deep neural networks with pruning,
trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149 .
Han, S.; Pool, J.; Narang, S.; Mao, H.; Gong, E.; Tang, S.;
Elsen, E.; Vajda, P.; Paluri, M.; Tran, J.; et al. 2016. Dsd:
Dense-sparse-dense training for deep neural networks. arXiv
preprint arXiv:1607.04381 .
Han, S.; Pool, J.; Tran, J.; and Dally, W. 2015. Learning
both weights and connections for efﬁcient neural network. In
Advances in neural information processing systems , 1135–
1143.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-
ual learning for image recognition. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, 770–778.
He, Y .; Kang, G.; Dong, X.; Fu, Y .; and Yang, Y . 2018. Soft
ﬁlter pruning for accelerating deep convolutional neural net-
works. arXiv preprint arXiv:1808.06866 .
Hu, H.; Peng, R.; Tai, Y .-W.; and Tang, C.-K. 2016.
Network trimming: A data-driven neuron pruning ap-
proach towards efﬁcient deep architectures. arXiv preprint
arXiv:1607.03250 .
Lazarevich, I.; Kozlov, A.; and Malinin, N. 2021. Post-
training deep neural network pruning via layer-wise calibra-
tion. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , 798–805.
Lee, N.; Ajanthan, T.; and Torr, P. H. 2018. Snip: Single-
shot network pruning based on connection sensitivity. arXiv
preprint arXiv:1810.02340 .
Leroux, S.; Bohez, S.; De Coninck, E.; Verbelen, T.;
Vankeirsbilck, B.; Simoens, P.; and Dhoedt, B. 2017. The
cascading neural network: building the internet of smart
things. Knowledge and Information Systems , 52(3): 791–
814.
Li, F.; Zhang, B.; and Liu, B. 2016. Ternary weight net-
works. arXiv preprint arXiv:1605.04711 .
Li, H.; Kadav, A.; Durdanovic, I.; Samet, H.; and Graf, H. P.
2016. Pruning ﬁlters for efﬁcient convnets. arXiv preprint
arXiv:1608.08710 .
Li, H.; Zhang, H.; Qi, X.; Yang, R.; and Huang, G. 2019.
Improved techniques for training adaptive deep networks. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , 1891–1900.
Liang, T.; Glossner, J.; Wang, L.; Shi, S.; and Zhang, X.
2021. Pruning and quantization for deep neural network ac-
celeration: A survey. Neurocomputing , 461: 370–403.
Lin, J.; Rao, Y .; Lu, J.; and Zhou, J. 2017. Runtime neural
pruning. Advances in neural information processing sys-
tems, 30.

--- PAGE 7 ---
Lin, T.; Stich, S. U.; Barba, L.; Dmitriev, D.; and Jaggi,
M. 2020. Dynamic model pruning with feedback. arXiv
preprint arXiv:2006.07253 .
Liu, W.; Zhang, Y .-M.; Li, X.; Yu, Z.; Dai, B.; Zhao, T.; and
Song, L. 2017. Deep hyperspherical learning. Advances in
neural information processing systems , 30.
Liu, Z.; Sun, M.; Zhou, T.; Huang, G.; and Darrell, T. 2018.
Rethinking the value of network pruning. arXiv preprint
arXiv:1810.05270 .
Loshchilov, I.; and Hutter, F. 2016. Sgdr: Stochas-
tic gradient descent with warm restarts. arXiv preprint
arXiv:1608.03983 .
Molchanov, D.; Ashukha, A.; and Vetrov, D. 2017. Vari-
ational dropout sparsiﬁes deep neural networks. arXiv
preprint arXiv:1701.05369 .
Mostafa, H.; and Wang, X. 2019. Parameter efﬁcient
training of deep convolutional neural networks by dynamic
sparse reparameterization. In International Conference on
Machine Learning , 4646–4655. PMLR.
Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;
Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;
Berg, A. C.; and Fei-Fei, L. 2015. ImageNet Large Scale Vi-
sual Recognition Challenge. International Journal of Com-
puter Vision (IJCV) , 115(3): 211–252.
Sandler, M.; Howard, A.; Zhu, M.; Zhmoginov, A.; and
Chen, L.-C. 2018. Mobilenetv2: Inverted residuals and lin-
ear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , 4510–4520.
Srinivas, S.; Kuzmin, A.; Nagel, M.; van Baalen, M.; Skliar,
A.; and Blankevoort, T. 2022. Cyclical Pruning for Sparse
Neural Networks. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , 2762–
2771.
Wu, J.; Leng, C.; Wang, Y .; Hu, Q.; and Cheng, J. 2016.
Quantized convolutional neural networks for mobile de-
vices. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , 4820–4828.
Wu, Z.; Nagarajan, T.; Kumar, A.; Rennie, S.; Davis, L. S.;
Grauman, K.; and Feris, R. 2018. Blockdrop: Dynamic in-
ference paths in residual networks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, 8817–8826.
Zhou, H.; Lan, J.; Liu, R.; and Yosinski, J. 2019. Decon-
structing lottery tickets: Zeros, signs, and the supermask.
Advances in neural information processing systems , 32.
Zhu, M.; and Gupta, S. 2017. To prune, or not to prune: ex-
ploring the efﬁcacy of pruning for model compression. arXiv
preprint arXiv:1710.01878 .
