# Cắt tỉa bằng cách Giải thích: Một Tiêu chí Mới cho
# Cắt tỉa Mạng Neural Sâu

Seul-Ki Yeoma,i, Philipp Seegerera,h, Sebastian Lapuschkinc, Alexander Binderd,e,
Simon Wiedemannc, Klaus-Robert M ¨ullera,f,g,b,, Wojciech Samekc,b,

aNhóm Học máy, Đại học Kỹ thuật Berlin, 10587 Berlin, Đức
bBIFOLD – Viện Berlin về Nền tảng Học tập và Dữ liệu, Berlin, Đức
cKhoa Trí tuệ Nhân tạo, Viện Heinrich Hertz Fraunhofer, 10587 Berlin, Đức
dCột ISTD, Đại học Công nghệ và Thiết kế Singapore, Singapore 487372, Singapore
eKhoa Tin học, Đại học Oslo, 0373 Oslo, Na Uy
fKhoa Trí tuệ Nhân tạo, Đại học Korea, Seoul 136-713, Hàn Quốc
gViện Max Planck về Tin học, 66123 Saarbr¨ ucken, Đức
hAignostics GmbH, 10557 Berlin, Đức
iNota AI GmbH, 10117 Berlin, Đức

## Tóm tắt

Sự thành công của mạng neural tích chập (CNN) trong các ứng dụng khác nhau đi kèm với sự gia tăng đáng kể về chi phí tính toán và lưu trữ tham số. Những nỗ lực gần đây để giảm những chi phí này bao gồm cắt tỉa và nén trọng số của các lớp khác nhau trong khi cùng lúc nhằm không hy sinh hiệu suất. Trong bài báo này, chúng tôi đề xuất một tiêu chí mới cho việc cắt tỉa CNN được truyền cảm hứng từ khả năng diễn giải mạng neural: Các đơn vị có liên quan nhất, tức là trọng số hoặc bộ lọc, được tự động tìm thấy bằng cách sử dụng điểm liên quan của chúng thu được từ các khái niệm của AI có thể giải thích (XAI). Bằng cách khám phá ý tưởng này, chúng tôi kết nối các hướng nghiên cứu về khả năng diễn giải và nén mô hình. Chúng tôi cho thấy rằng phương pháp đề xuất của chúng tôi có thể cắt tỉa hiệu quả các mô hình CNN trong thiết lập học chuyển giao trong đó các mạng được đào tạo trước trên các kho dữ liệu lớn được điều chỉnh cho các nhiệm vụ chuyên biệt. Phương pháp được đánh giá trên một loạt rộng các bộ dữ liệu thị giác máy tính. Đáng chú ý, tiêu chí mới của chúng tôi không chỉ có tính cạnh tranh hoặc tốt hơn so với các tiêu chí cắt tỉa hiện đại khi thực hiện đào tạo lại liên tiếp, mà còn vượt trội rõ ràng so với các tiêu chí trước đây trong kịch bản ứng dụng bị hạn chế tài nguyên trong đó dữ liệu của nhiệm vụ cần chuyển giao rất khan hiếm và người ta chọn từ bỏ tinh chỉnh. Phương pháp của chúng tôi có thể nén mô hình một cách lặp đi lặp lại trong khi duy trì hoặc thậm chí cải thiện độ chính xác. Cùng lúc, nó có chi phí tính toán theo thứ tự của tính toán gradient và tương đối đơn giản để áp dụng mà không cần điều chỉnh siêu tham số cho việc cắt tỉa.

**Từ khóa:** Cắt tỉa, Lan truyền Liên quan Theo lớp (LRP), Mạng Neural Tích chập (CNN), Diễn giải Mô hình, AI có thể Giải thích (XAI)

## 1. Giới thiệu

CNN sâu đã trở thành một công cụ không thể thiếu cho một loạt rộng các ứng dụng [1], chẳng hạn như phân loại hình ảnh, nhận dạng giọng nói, xử lý ngôn ngữ tự nhiên, hóa học, khoa học thần kinh, y học và thậm chí được áp dụng để chơi các trò chơi như Go, poker hoặc Super Smash Bros. Chúng đã đạt được hiệu suất dự đoán cao, đôi khi thậm chí vượt trội con người. Hơn nữa, trong các lĩnh vực chuyên biệt nơi dữ liệu đào tạo hạn chế có sẵn, ví dụ, do chi phí và khó khăn của việc tạo dữ liệu (hình ảnh y tế từ fMRI, EEG, PET, v.v.), học chuyển giao có thể cải thiện hiệu suất CNN bằng cách trích xuất kiến thức từ các nhiệm vụ nguồn và áp dụng nó cho một nhiệm vụ đích có dữ liệu đào tạo hạn chế.

Tuy nhiên, hiệu suất dự đoán cao của CNN thường đi kèm với chi phí lưu trữ và tính toán cao, những chi phí này liên quan đến việc tiêu thụ năng lượng của mạng được tinh chỉnh. Những kiến trúc sâu này được cấu thành từ hàng triệu tham số cần được đào tạo, dẫn đến tình trạng tham số hóa quá mức (tức là có nhiều tham số hơn mẫu đào tạo) của mô hình [2]. Thời gian chạy thường bị chi phối bởi việc đánh giá các lớp tích chập, trong khi các lớp dày đặc thì rẻ nhưng tốn bộ nhớ [3]. Ví dụ, mô hình VGG-16 có khoảng 138 triệu tham số, chiếm hơn 500MB không gian lưu trữ, và cần 15,5 tỷ phép toán dấu phẩy động (FLOP) để phân loại một hình ảnh duy nhất. ResNet50 có khoảng 23 triệu tham số và cần 4,1 tỷ FLOP. Lưu ý rằng tham số hóa quá mức hữu ích cho việc đào tạo hiệu quả và thành công các mạng neural, tuy nhiên, một khi cấu trúc mạng được đào tạo và tổng quát hóa tốt được thiết lập, việc cắt tỉa có thể giúp giảm dư thừa trong khi vẫn duy trì hiệu suất tốt [4].

Việc giảm yêu cầu lưu trữ và chi phí tính toán của mô hình trở nên quan trọng cho khả năng áp dụng rộng rãi hơn, ví dụ, trong các hệ thống nhúng, tác nhân tự động, thiết bị di động, hoặc thiết bị biên [5]. Cắt tỉa mạng neural có lịch sử hàng thập kỷ với sự quan tâm từ cả học thuật và công nghiệp [6] nhằm loại bỏ tập con các đơn vị mạng (tức là trọng số hoặc bộ lọc) ít quan trọng nhất đối với nhiệm vụ dự định của mạng. Đối với việc cắt tỉa mạng, điều quan trọng là quyết định cách xác định tập con "không liên quan" của các tham số dành cho việc xóa. Để giải quyết vấn đề này, các nghiên cứu trước đây đã đề xuất các tiêu chí cụ thể dựa trên khai triển Taylor, trọng số, gradient, và các tiêu chí khác, để giảm độ phức tạp và chi phí tính toán trong mạng. Các công trình liên quan được giới thiệu trong Phần 2.

Từ quan điểm thực tế, toàn bộ khả năng (về mặt trọng số và bộ lọc) của một mô hình tham số hóa quá mức có thể không được yêu cầu, ví dụ, khi (1) các phần của mô hình nằm im sau khi đào tạo (tức là bị "tắt" vĩnh viễn), (2) người dùng không quan tâm đến toàn bộ mảng đầu ra có thể của mô hình, đây là một kịch bản phổ biến trong học chuyển giao (ví dụ: người dùng chỉ cần sử dụng 2 trong số 10 đầu ra mạng có sẵn), hoặc (3) người dùng thiếu dữ liệu và tài nguyên để tinh chỉnh và chạy mô hình tham số hóa quá mức.

Trong những kịch bản này, các phần dư thừa của mô hình vẫn sẽ chiếm không gian trong bộ nhớ, và thông tin sẽ được lan truyền qua những phần đó, tiêu thụ năng lượng và tăng thời gian chạy. Do đó, các tiêu chí có thể giảm đáng kể và ổn định độ phức tạp tính toán của mạng neural sâu trên các ứng dụng là có liên quan đối với các nhà thực hành.

Trong bài báo này, chúng tôi đề xuất một khung cắt tỉa mới dựa trên Lan truyền Liên quan Theo lớp (LRP) [7]. LRP ban đầu được phát triển như một phương pháp giải thích để gán điểm quan trọng, được gọi là độ liên quan, cho các chiều đầu vào khác nhau của một mạng neural phản ánh đóng góp của một chiều đầu vào đối với quyết định của mô hình, và đã được áp dụng trong các lĩnh vực khác nhau của thị giác máy tính (ví dụ, [8,9,10]). Độ liên quan được lan truyền ngược từ đầu ra đến đầu vào và qua đó được gán cho mỗi đơn vị của mô hình sâu. Vì điểm liên quan được tính toán cho mọi lớp và neuron từ đầu ra mô hình đến đầu vào, những điểm liên quan này về cơ bản phản ánh tầm quan trọng của từng đơn vị riêng lẻ của mô hình và đóng góp của nó đối với luồng thông tin qua mạng — một ứng cử viên tự nhiên để được sử dụng làm tiêu chí cắt tỉa. Tiêu chí LRP có thể được động lực về mặt lý thuyết thông qua khái niệm Phân tách Taylor Sâu (DTD) (tham khảo [11,12,13]). Hơn nữa, LRP có thể mở rộng và dễ áp dụng, và đã được triển khai trong các khung phần mềm như iNNvestigate [14]. Hơn nữa, nó có chi phí tính toán tuyến tính theo chi phí suy luận mạng, tương tự như lan truyền ngược.

Chúng tôi đánh giá có hệ thống hiệu quả nén của tiêu chí LRP so với các tiêu chí cắt tỉa thông thường cho hai kịch bản khác nhau.

**Kịch bản 1:** Chúng tôi cắt tỉa CNN được đào tạo trước sau đó tiến hành tinh chỉnh tiếp theo. Đây là thiết lập thông thường trong cắt tỉa CNN và đòi hỏi một lượng dữ liệu và sức mạnh tính toán đủ.

**Kịch bản 2:** Trong kịch bản này, một mô hình được đào tạo trước cần được chuyển giao đến một bài toán liên quan cũng như vậy, nhưng dữ liệu có sẵn cho nhiệm vụ mới quá khan hiếm để tinh chỉnh đúng cách và/hoặc việc tiêu thụ thời gian, sức mạnh tính toán hoặc tiêu thụ năng lượng bị hạn chế. Học chuyển giao với những hạn chế như vậy phổ biến trong các ứng dụng di động hoặc nhúng.

Kết quả thí nghiệm của chúng tôi trên các bộ dữ liệu chuẩn khác nhau và bốn kiến trúc CNN phổ biến khác nhau cho thấy rằng tiêu chí LRP cho cắt tỉa có khả năng mở rộng và hiệu quả hơn, và dẫn đến hiệu suất tốt hơn các tiêu chí hiện có bất kể loại dữ liệu và kiến trúc mô hình nếu việc đào tạo lại được thực hiện (Kịch bản 1). Đặc biệt, nếu việc đào tạo lại bị cấm do các ràng buộc bên ngoài sau khi cắt tỉa, tiêu chí LRP vượt trội rõ ràng so với các tiêu chí trước đây trên tất cả các bộ dữ liệu (Kịch bản 2). Cuối cùng, chúng tôi muốn lưu ý rằng khung cắt tỉa đề xuất của chúng tôi không giới hạn ở LRP và dữ liệu hình ảnh, mà cũng có thể được sử dụng với các kỹ thuật giải thích khác và các loại dữ liệu khác.

Phần còn lại của bài báo này được tổ chức như sau: Phần 2 tóm tắt các công trình liên quan về nén mạng và giới thiệu các tiêu chí điển hình cho cắt tỉa mạng. Phần 3 mô tả khung và chi tiết của phương pháp của chúng tôi. Kết quả thí nghiệm được minh họa và thảo luận trong Phần 4, trong khi phương pháp của chúng tôi được thảo luận liên quan đến các nghiên cứu trước đây trong Phần 5. Phần 6 đưa ra kết luận và triển vọng cho công việc tương lai.

## 2. Công trình Liên quan

Chúng tôi bắt đầu thảo luận về nghiên cứu liên quan trong lĩnh vực nén mạng với các phương pháp lượng tử hóa mạng đã được đề xuất để nén không gian lưu trữ bằng cách giảm số lượng giá trị có thể và duy nhất cho các tham số [15,16]. Các phương pháp phân tách tensor phân tách ma trận mạng thành nhiều ma trận nhỏ hơn để ước tính các tham số thông tin của CNN sâu với xấp xỉ/phân tích hạng thấp [17].

Gần đây hơn, [18] cũng đề xuất một khung chưng cất kiến trúc dựa trên thay thế theo lớp, được gọi là LightweightNet để tiết kiệm bộ nhớ và thời gian. Các thuật toán để thiết kế các mô hình hiệu quả tập trung nhiều hơn vào tăng tốc thay vì nén bằng cách tối ưu hóa các phép toán tích chập hoặc kiến trúc trực tiếp (ví dụ [19]).

Các phương pháp cắt tỉa mạng loại bỏ các đơn vị dư thừa hoặc không liên quan — tức là nút, bộ lọc, hoặc lớp — khỏi mô hình không quan trọng đối với hiệu suất [6,20]. Cắt tỉa mạng mạnh mẽ với các thiết lập khác nhau và cho tỷ lệ nén hợp lý trong khi không (hoặc tối thiểu) làm tổn hại độ chính xác của mô hình. Nó cũng có thể hỗ trợ cả đào tạo từ đầu và học chuyển giao từ các mô hình được đào tạo trước. Các công trình đầu tiên đã cho thấy rằng cắt tỉa mạng hiệu quả trong việc giảm độ phức tạp mạng và đồng thời giải quyết các vấn đề quá khớp. Các kỹ thuật cắt tỉa mạng hiện tại làm cho trọng số hoặc kênh thưa thớt bằng cách loại bỏ các kết nối không thông tin và đòi hỏi một tiêu chí phù hợp để xác định những đơn vị nào của mô hình không liên quan để giải quyết một bài toán. Do đó, điều quan trọng là quyết định cách định lượng mức độ liên quan của các tham số (tức là trọng số hoặc kênh) trong trạng thái hiện tại của quá trình học để xóa mà không hy sinh hiệu suất dự đoán. Trong các nghiên cứu trước đây, các tiêu chí cắt tỉa đã được đề xuất dựa trên độ lớn của 1) trọng số, 2) gradient, 3) khai triển Taylor/đạo hàm, và 4) các tiêu chí khác, như được mô tả trong phần sau.

**Khai triển Taylor:** Các phương pháp đầu tiên hướng tới cắt tỉa mạng neural — optimal brain damage [4] và optimal brain surgeon [21] — tận dụng khai triển Taylor bậc hai dựa trên ma trận Hessian của hàm mất mát để chọn tham số để xóa. Tuy nhiên, việc tính toán nghịch đảo của Hessian tốn kém về mặt tính toán. Công trình của [22,23] sử dụng khai triển Taylor bậc nhất như một tiêu chí để xấp xỉ sự thay đổi mất mát trong hàm mục tiêu như một hiệu ứng của việc cắt tỉa các đơn vị mạng. Chúng tôi so sánh tiêu chí mới của mình với khai triển Taylor bậc nhất có thể so sánh về mặt tính toán từ [22].

**Gradient:** Liu và Wu [24] đề xuất một chiến lược cắt tỉa toàn cục phân cấp bằng cách tính toán gradient trung bình của bản đồ đặc trưng trong mỗi lớp. Họ áp dụng một chiến lược cắt tỉa toàn cục phân cấp giữa các lớp có độ nhạy tương tự. Sun và cộng sự [25] đề xuất một phương pháp lan truyền ngược thưa thớt để đào tạo mạng neural sử dụng độ lớn của gradient để tìm các đặc trưng thiết yếu và không thiết yếu trong các mô hình Perceptron Đa lớp (MLP) và Mạng Bộ nhớ Ngắn hạn Dài (LSTM), có thể được sử dụng để cắt tỉa. Chúng tôi triển khai tiêu chí cắt tỉa dựa trên gradient theo [25].

**Trọng số:** Một xu hướng gần đây là cắt tỉa các trọng số dư thừa, không thông tin trong các mô hình CNN được đào tạo trước, dựa trên độ lớn của chính các trọng số. Han và cộng sự [26] và Han và cộng sự [27] đề xuất việc cắt tỉa các trọng số mà độ lớn dưới một ngưỡng nhất định, và sau đó tinh chỉnh với chính quy hóa lp-norm. Chiến lược cắt tỉa này đã được sử dụng trên các lớp kết nối đầy đủ và giới thiệu các kết nối thưa thớt với thư viện BLAS, hỗ trợ phần cứng chuyên biệt để đạt được tăng tốc của nó. Trong cùng bối cảnh, Học Thưa thớt Có cấu trúc (SSL) đã thêm chính quy hóa thưa thớt nhóm để phạt các tham số không quan trọng bằng cách loại bỏ một số trọng số [28]. Li và cộng sự [29], mà chúng tôi so sánh trong các thí nghiệm của mình, đề xuất một phương pháp cắt tỉa kênh một lần sử dụng lpnorm của trọng số để chọn bộ lọc, với điều kiện rằng những kênh có trọng số nhỏ hơn luôn tạo ra các kích hoạt yếu hơn.

**Các tiêu chí khác:** [30] đề xuất thuật toán Lan truyền Điểm Quan trọng Neuron (NISP) để lan truyền điểm quan trọng của các phản hồi cuối cùng trước lớp phân loại softmax trong mạng. Phương pháp này dựa trên — trái ngược với thước đo đề xuất của chúng tôi — một quy trình cắt tỉa theo lớp không xem xét tầm quan trọng toàn cục trong mạng. Luo và cộng sự [31] đề xuất ThiNet, một kỹ thuật cắt tỉa kênh thống kê dựa trên dữ liệu dựa trên các thống kê được tính toán từ lớp tiếp theo. Các phương pháp lai khác có thể được tìm thấy trong, ví dụ [32], đề xuất một phương pháp kết hợp để kết hợp với cắt tỉa kênh dựa trên trọng số và lượng tử hóa mạng. Gần đây hơn, Dai và cộng sự [33] đề xuất một mô hình tiến hóa để cắt tỉa dựa trên trọng số và tăng trưởng dựa trên gradient để giảm mạng một cách heuristic.

## 3. Cắt tỉa Mạng dựa trên LRP

Một CNN feedforward bao gồm các neuron được thiết lập trong một chuỗi nhiều lớp, trong đó mỗi neuron nhận dữ liệu đầu vào từ một hoặc nhiều lớp trước đó và lan truyền đầu ra của nó đến mọi neuron trong các lớp tiếp theo, sử dụng một ánh xạ có thể phi tuyến. Cắt tỉa mạng nhằm làm thưa thớt các đơn vị này bằng cách loại bỏ các trọng số hoặc bộ lọc không thông tin (theo một tiêu chí nhất định). Chúng tôi tập trung cụ thể các thí nghiệm của mình vào học chuyển giao, trong đó các tham số của một mạng được đào tạo trước trên một miền nguồn sau đó được tinh chỉnh trên một miền đích, tức là, dữ liệu cuối cùng hoặc nhiệm vụ dự đoán. Ở đây, quy trình cắt tỉa chung được phác thảo trong Thuật toán 1.

**Thuật toán 1** Cắt tỉa Mạng Neural
1: **Đầu vào:** mô hình được đào tạo trước net, dữ liệu tham chiếu xr, dữ liệu đào tạo xt
2: ngưỡng cắt tỉa t, tiêu chí cắt tỉa c, tỷ lệ cắt tỉa r
3: **while** t không đạt được **do**
4: //Bước 1: đánh giá tầm quan trọng cấu trúc con mạng
5: **for** tất cả layer trong net **do**
6: **for** tất cả units trong layer **do**
7: ▷ tính tầm quan trọng của unit w.r.t. c (và xr)
8: **end for**
9: **if** yêu cầu cho c **then**
10: ▷ chính quy hóa toàn cục tầm quan trọng cho mỗi unit
11: **end if**
12: **end for**
13: //Bước 2: xác định và loại bỏ các đơn vị ít quan trọng nhất theo nhóm r
14: ▷ loại bỏ r units từ net nơi tầm quan trọng là tối thiểu
15: ▷ loại bỏ các kết nối mồ côi của mỗi đơn vị bị loại bỏ
16: **if** mong muốn **then**
17: //Bước 2.1: tinh chỉnh tùy chọn để khôi phục hiệu suất
18: ▷ tinh chỉnh net trên xt
19: **end if**
20: **end while**
21: //trả về mạng đã cắt tỉa khi đạt ngưỡng t (ví dụ: hiệu suất hoặc kích thước mô hình)
22: **return** net

Mặc dù hầu hết các phương pháp sử dụng một quy trình giống hệt nhau, việc chọn một tiêu chí cắt tỉa phù hợp để định lượng tầm quan trọng của các tham số mô hình để xóa trong khi giảm thiểu sự sụt giảm hiệu suất (Bước 1) có tầm quan trọng quyết định, chi phối sự thành công của phương pháp.

### 3.1. Lan truyền Liên quan Theo lớp

Trong bài báo này, chúng tôi đề xuất một tiêu chí mới để cắt tỉa các đơn vị mạng neural: lượng liên quan được tính toán với LRP [7]. LRP phân tách một quyết định phân loại thành các đóng góp tỷ lệ của mỗi đơn vị mạng đối với điểm phân loại tổng thể, được gọi là "mức độ liên quan". Khi được tính toán cho các chiều đầu vào của một CNN và được hiển thị dưới dạng bản đồ nhiệt, những mức độ liên quan này làm nổi bật các phần của đầu vào quan trọng đối với quyết định phân loại. LRP do đó ban đầu phục vụ như một công cụ để diễn giải các máy học phi tuyến và đã được áp dụng như vậy trong các lĩnh vực khác nhau, trong số những lĩnh vực khác cho nhận dạng hình ảnh chung, hình ảnh y tế và xử lý ngôn ngữ tự nhiên, tham khảo [34]. Mối liên kết trực tiếp của các mức độ liên quan với đầu ra bộ phân loại, cũng như ràng buộc bảo toàn được áp đặt trên việc lan truyền mức độ liên quan giữa các lớp, làm cho LRP không chỉ hấp dẫn để giải thích mô hình, mà cũng có thể tự nhiên phục vụ như tiêu chí cắt tỉa (xem Phần 4.1).

Đặc điểm chính của LRP là một lượt truyền ngược qua mạng trong đó đầu ra mạng được phân phối lại cho tất cả các đơn vị của mạng theo cách từ lớp này đến lớp khác. Lượt truyền ngược này có cấu trúc tương tự như lan truyền ngược gradient và do đó có thời gian chạy tương tự. Việc phân phối lại dựa trên một nguyên tắc bảo toàn sao cho các mức độ liên quan có thể được giải thích ngay lập tức như đóng góp mà một đơn vị đóng góp cho đầu ra mạng, do đó thiết lập một kết nối trực tiếp với đầu ra mạng và do đó hiệu suất dự đoán của nó. Do đó, như một tiêu chí cắt tỉa, phương pháp này hiệu quả và dễ dàng mở rộng cho các cấu trúc mạng chung. Độc lập với loại lớp mạng neural — đó là pooling, kết nối đầy đủ, lớp tích chập — LRP cho phép định lượng tầm quan trọng của các đơn vị trong toàn bộ mạng, với bối cảnh dự đoán toàn cục.

### 3.2. Cắt tỉa dựa trên LRP

Quy trình cắt tỉa dựa trên LRP được tóm tắt trong Hình 1. Trong giai đoạn đầu tiên, một lượt truyền tiến tiêu chuẩn được thực hiện bởi mạng và các kích hoạt tại mỗi lớp được thu thập. Trong giai đoạn thứ hai, điểm f(x) thu được tại đầu ra của mạng được lan truyền ngược qua mạng theo các quy tắc lan truyền LRP [7]. Trong giai đoạn thứ ba, mô hình hiện tại được cắt tỉa bằng cách loại bỏ các đơn vị không liên quan (w.r.t. lượng "mức độ liên quan" R thu được qua LRP) và được tinh chỉnh thêm (tùy chọn).

LRP dựa trên một nguyên tắc bảo toàn theo lớp cho phép lượng được lan truyền (ví dụ: mức độ liên quan cho một lớp được dự đoán) được bảo tồn giữa các neuron của hai lớp liền kề. Hãy để R(l)i là mức độ liên quan của neuron i tại lớp l và R(l+1)j là mức độ liên quan của neuron j tại lớp tiếp theo l+1. Các định nghĩa bảo toàn nghiêm ngặt hơn chỉ liên quan đến các tập con neuron có thể áp đặt thêm rằng mức độ liên quan được phân phối lại cục bộ trong các lớp thấp hơn và chúng ta định nghĩa R(l)ij là phần của R(l+1)j được phân phối lại cho neuron i trong lớp thấp hơn. Tính chất bảo toàn luôn thỏa mãn

∑i R(l)ij = R(l+1)j, (1)

trong đó tổng chạy trên tất cả các neuron i của lớp l (trong quá trình suy luận) đi trước. Khi sử dụng mức độ liên quan như một tiêu chí cắt tỉa, tính chất này giúp bảo tồn lượng của nó từ lớp này đến lớp khác, bất kể kích thước lớp ẩn và số lượng neuron được cắt tỉa lặp đi lặp lại cho mỗi lớp. Tại mỗi lớp l, chúng ta có thể trích xuất tầm quan trọng toàn cục của nút i như mức độ liên quan được gán R(l)i.

Trong bài báo này, chúng tôi cụ thể áp dụng các lượng mức độ liên quan được tính toán với quy tắc LRP-ε10 như tiêu chí cắt tỉa. Quy tắc LRP-ε được phát triển với DNN feedforward với kích hoạt ReLU trong tâm trí và giả định các kích hoạt logit dương (trước softmax) flogit(x) > 0 để phân tách. Quy tắc đã được chứng minh là hoạt động tốt trong thực tế trong một thiết lập như vậy [35]. Biến thể cụ thể này của LRP có gốc rễ chặt chẽ trong DTD [11], và khác với các tiêu chí dựa trên đạo hàm mạng mà chúng tôi so sánh [25,22], luôn tạo ra các giải thích liên tục, ngay cả khi lan truyền ngược được thực hiện qua tính phi liên tục ReLU (và thường được sử dụng) [12]. Khi được sử dụng như một tiêu chí để cắt tỉa, việc đánh giá tầm quan trọng đơn vị mạng của nó sẽ thay đổi ít đột ngột hơn với những thay đổi (nhỏ) trong việc chọn các mẫu tham chiếu, so với các tiêu chí dựa trên gradient.

Quy tắc lan truyền thực hiện hai bước lan truyền mức độ liên quan riêng biệt mỗi lớp: một cái chỉ xem xét các phần kích hoạt của các lượng được lan truyền tiến (tức là tất cả a(l)iwij > 0) và một cái khác chỉ xử lý các phần ức chế (a(l)iwij < 0) sau đó được hợp nhất trong một tổng với các thành phần được trọng số bởi ε và β (sao cho ε + β = 1) tương ứng.

Bằng cách chọn ε = 1, quy tắc lan truyền được đơn giản hóa thành

R(l)i = ∑j (a(l)iwij+)/(∑i' a(l)i'wi'j+) R(l+1)j, (2)

trong đó R(l)i biểu thị mức độ liên quan được gán cho neuron thứ i tại lớp l, như một tập hợp các thông điệp mức độ liên quan được lan truyền xuống R(l;l+1)ij. Các thuật ngữ (·)+ chỉ ra phần dương của việc kích hoạt trước được lan truyền tiến từ lớp l, đến lớp (l+1). i' là một chỉ số chạy trên tất cả các kích hoạt đầu vào a. Lưu ý rằng việc chọn ε = 1 chỉ phân tách w.r.t. các phần của tín hiệu suy luận hỗ trợ quyết định mô hình cho lớp quan tâm.

Phương trình (2) là bảo toàn cục bộ, tức là không có lượng mức độ liên quan nào bị mất hoặc được tiêm trong quá trình phân phối Rj trong đó mỗi thuật ngữ của tổng tương ứng với một thông điệp mức độ liên quan Rjk. Vì lý do này, LRP có những lợi thế kỹ thuật sau đây so với các kỹ thuật cắt tỉa khác như các phương pháp dựa trên gradient hoặc dựa trên kích hoạt: (1) Bảo toàn mức độ liên quan cục bộ ngầm đảm bảo việc phân phối lại toàn cục được chính quy hóa theo lớp của tầm quan trọng từ mỗi đơn vị mạng. (2) Bằng cách tính tổng mức độ liên quan trong mỗi kênh bộ lọc (tích chập), tiêu chí dựa trên LRP có thể áp dụng trực tiếp như một thước đo tổng mức độ liên quan cho mỗi nút/bộ lọc, mà không yêu cầu chính quy hóa theo lớp hậu hoc, ví dụ, qua lpnorm. (3) Việc sử dụng điểm mức độ liên quan không bị hạn chế vào một ứng dụng toàn cục của cắt tỉa mà có thể dễ dàng áp dụng cho cắt tỉa bị hạn chế cục bộ và (theo neuron hoặc bộ lọc) theo nhóm mà không cần chính quy hóa. Các chiến lược khác nhau để chọn (các) phần (con) của mô hình vẫn có thể được xem xét, ví dụ, áp dụng các trọng số/ưu tiên khác nhau để cắt tỉa các phần khác nhau của mô hình: Nếu mục tiêu của cắt tỉa là giảm FLOP yêu cầu trong quá trình suy luận, người ta sẽ thích tập trung vào việc cắt tỉa chủ yếu các đơn vị của các lớp tích chập. Trong trường hợp mục tiêu là giảm yêu cầu bộ nhớ, cắt tỉa nên tập trung vào các lớp kết nối đầy đủ thay thế.

Trong bối cảnh của Thuật toán 1, Bước 1 của việc đánh giá tầm quan trọng neuron và bộ lọc dựa trên LRP được thực hiện như một lượt truyền ngược LRP đơn qua mô hình, với một tập hợp mức độ liên quan cho mỗi kênh bộ lọc như được mô tả ở trên, cho các lớp tích chập, và không yêu cầu chuẩn hóa hoặc chính quy hóa bổ sung. Chúng tôi muốn chỉ ra rằng thay vì lan truyền ngược đầu ra mô hình fc(x) cho lớp thực c của bất kỳ mẫu x nào đã cho (như thường được thực hiện khi LRP được sử dụng để giải thích một dự đoán [7,8]), chúng tôi khởi tạo thuật toán với R(L)c = 1 tại lớp đầu ra L. Do đó chúng ta đạt được sự mạnh mẽ chống lại (sự thiếu) tự tin của mô hình trong các dự đoán của nó trên các mẫu tham chiếu xem trước x và đảm bảo trọng số bằng nhau của ảnh hưởng của tất cả các mẫu tham chiếu trong việc xác định các đường dẫn neural có liên quan.

## 4. Thí nghiệm

Chúng tôi bắt đầu bằng một nỗ lực làm sáng tỏ trực quan các tính chất của các tiêu chí cắt tỉa khác nhau, cụ thể là độ lớn trọng số, Taylor, gradient và LRP, qua một loạt các bộ dữ liệu đồ chơi. Sau đó chúng tôi cho thấy hiệu quả của tiêu chí LRP để cắt tỉa trên các bộ dữ liệu chuẩn nhận dạng hình ảnh được sử dụng rộng rãi — tức là Scene 15 [36], Event 8 [37], Cats & Dogs [38], Oxford Flower 102 [39], CIFAR-10, và ILSVRC 2012 [40] — và bốn kiến trúc mạng neural feedforward sâu được đào tạo trước, AlexNet và VGG-16 chỉ với một chuỗi lớp duy nhất, và ResNet-18 và ResNet-50 [41], cả hai đều chứa nhiều nhánh song song của các lớp và kết nối bỏ qua.

Kịch bản đầu tiên tập trung cụ thể vào cắt tỉa các CNN được đào tạo trước với tinh chỉnh tiếp theo, như thường thấy trong nghiên cứu cắt tỉa [22]. Chúng tôi so sánh phương pháp của mình với một số tiêu chí hiện đại để chứng minh hiệu quả của LRP như một tiêu chí cắt tỉa trong CNN. Trong kịch bản thứ hai, chúng tôi đã thử nghiệm xem liệu tiêu chí cắt tỉa đề xuất cũng hoạt động tốt nếu chỉ có một số lượng rất hạn chế các mẫu có sẵn để cắt tỉa mô hình. Điều này có liên quan trong trường hợp các thiết bị có sức mạnh tính toán, năng lượng và lưu trữ hạn chế như thiết bị di động hoặc ứng dụng nhúng.

### 4.1. Cắt tỉa Mô hình Đồ chơi

Đầu tiên, chúng tôi so sánh có hệ thống các tính chất và hiệu quả của các tiêu chí cắt tỉa khác nhau trên nhiều bộ dữ liệu đồ chơi để thúc đẩy trực giác về các tính chất của tất cả các phương pháp, trong một thiết lập có thể kiểm soát và không tốn kém về mặt tính toán. Để đạt được mục đích này, chúng tôi đánh giá tất cả bốn tiêu chí trên các phân phối dữ liệu đồ chơi khác nhau cả về mặt định tính và định lượng. Chúng tôi đã tạo ra ba bộ dữ liệu đồ chơi k-lớp ("moon" (k=2), "circle" (k=2) và "multi" (k=4)), sử dụng các hàm tạo tương ứng.

Mỗi bộ dữ liệu 2D được tạo ra bao gồm 1000 mẫu đào tạo cho mỗi lớp. Chúng tôi đã xây dựng và đào tạo các mô hình như một chuỗi ba lớp dày đặc liên tiếp được kích hoạt ReLU với mỗi lớp có 1000 neuron ẩn. Sau lớp tuyến tính đầu tiên, chúng tôi đã thêm một lớp DropOut với xác suất dropout 50%. Mô hình nhận đầu vào từ R² và có — tùy thuộc vào bộ bài toán đồ chơi — k ∈ {2,4} neuron đầu ra:

Dense(1000) -> ReLU -> DropOut(0.5) -> Dense(1000) ->
-> ReLU -> Dense(1000) -> ReLU -> Dense(k)

Sau đó chúng tôi lấy mẫu một số điểm dữ liệu mới (không nhìn thấy trong quá trình đào tạo) để tính toán các tiêu chí cắt tỉa. Trong quá trình cắt tỉa, chúng tôi đã loại bỏ một số cố định 1000 trong số 3000 neuron ẩn có mức độ liên quan ít nhất để dự đoán theo mỗi tiêu chí. Điều này tương đương với việc loại bỏ 1000 bộ lọc đã học (nhưng không đáng kể, theo tiêu chí) khỏi mô hình. Sau khi cắt tỉa, chúng tôi quan sát các thay đổi trong ranh giới quyết định và đánh giá lại độ chính xác phân loại bằng cách sử dụng các mẫu đào tạo ban đầu và các điểm dữ liệu được lấy mẫu lại trên các tiêu chí. Thí nghiệm này được thực hiện với n ∈ [1,2,5,10,20,50,100,200] mẫu tham chiếu để thử nghiệm và tính toán các tiêu chí cắt tỉa. Mỗi thiết lập được lặp lại 50 lần, sử dụng cùng một bộ hạt giống ngẫu nhiên (tùy thuộc vào chỉ số lặp lại) cho mỗi n trên tất cả các tiêu chí cắt tỉa để duy trì khả năng so sánh.

Hình 2 cho thấy các phân phối dữ liệu của các bộ dữ liệu đồ chơi được tạo ra, một bộ mẫu n=5 mẫu được tạo ra để tính toán tiêu chí, cũng như tác động định tính đến ranh giới quyết định của các mô hình khi loại bỏ một bộ cố định 1000 neuron như được chọn qua các tiêu chí được so sánh. Hình 3 điều tra cách các tiêu chí cắt tỉa bảo tồn khả năng giải quyết vấn đề của các mô hình như một hàm của số lượng mẫu được chọn để tính toán các tiêu chí. Hình 4 sau đó tóm tắt định lượng kết quả cho số lượng mẫu chưa nhìn thấy cụ thể (n ∈ [1,5,20,100]) để tính toán các tiêu chí. Ở đây chúng tôi báo cáo độ chính xác mô hình trên bộ đào tạo để liên hệ việc bảo tồn hàm quyết định như đã học từ dữ liệu giữa các mô hình chưa cắt tỉa (cột thứ 2) với các mô hình đã cắt tỉa và tiêu chí cắt tỉa (các cột còn lại).

Kết quả trong Hình 4 cho thấy rằng, trong số tất cả các tiêu chí dựa trên mẫu tham chiếu để tính toán mức độ liên quan, thước đo dựa trên LRP liên tục vượt trội so với tất cả các tiêu chí khác trong tất cả các kích thước bộ tham chiếu và bộ dữ liệu. Chỉ trong trường hợp n=1 mẫu tham chiếu cho mỗi lớp, tiêu chí trọng số bảo tồn mô hình tốt nhất. Lưu ý rằng việc sử dụng độ lớn trọng số như một thước đo tầm quan trọng đơn vị mạng là một phương pháp tĩnh, độc lập với việc chọn mẫu tham chiếu. Với n=5 điểm tham chiếu cho mỗi lớp, tiêu chí dựa trên LRP đã vượt trội so với độ lớn trọng số như một tiêu chí để cắt tỉa các cấu trúc mạng neural không quan trọng, trong khi thành công bảo tồn lõi chức năng của bộ dự đoán. Hình 2 chứng minh cách ranh giới quyết định của các mô hình đồ chơi thay đổi dưới ảnh hưởng của cắt tỉa với tất cả bốn tiêu chí. Chúng ta có thể quan sát rằng tiêu chí trọng số và LRP bảo tồn ranh giới quyết định đã học của các mô hình tốt. Cả hai thước đo Taylor và gradient đều làm suy giảm mô hình đáng kể. So với các tiêu chí dựa trên trọng số và LRP, các mô hình được cắt tỉa bởi các tiêu chí dựa trên gradient phân loại sai một phần lớn các mẫu.

Hàng đầu tiên của Hình 3 cho thấy rằng tất cả các thước đo (phụ thuộc dữ liệu) đều được hưởng lợi từ việc tăng số lượng điểm tham chiếu. LRP có thể tìm thấy và bảo tồn các thành phần mạng quan trọng về mặt chức năng chỉ với rất ít dữ liệu, trong khi cùng lúc ít nhạy cảm đáng kể với việc chọn điểm tham chiếu hơn các thước đo khác, có thể thấy trong độ lệch chuẩn của các thước đo. Cả hai thước đo dựa trên gradient và Taylor đều không đạt được hiệu suất của cắt tỉa dựa trên LRP, ngay cả với 200 mẫu tham chiếu cho mỗi lớp. Hiệu suất của cắt tỉa với thước đo dựa trên độ lớn trọng số là không đổi, vì nó chỉ phụ thuộc vào chính các trọng số đã học. Hàng dưới của Hình 3 cho thấy hiệu suất thử nghiệm của các mô hình đã cắt tỉa như một hàm của số lượng mẫu được sử dụng để tính toán tiêu chí. Ở đây, chúng tôi đã thử nghiệm trên 500 mẫu cho mỗi lớp, được rút ra từ các phân phối tương ứng của bộ dữ liệu, và bị nhiễu loạn với nhiễu gaussian bổ sung (N(0,0.3)) được thêm vào sau khi tạo dữ liệu. Do lượng nhiễu lớn được thêm vào dữ liệu, chúng ta thấy hiệu suất dự đoán của các mô hình đã cắt tỉa và chưa cắt tỉa giảm trong tất cả các thiết lập. Ở đây chúng ta có thể quan sát rằng hai trong số ba lần các mô hình đã cắt tỉa LRP vượt trội so với tất cả các tiêu chí khác. Chỉ một lần, trên bộ dữ liệu "moon", cắt tỉa dựa trên tiêu chí trọng số mang lại hiệu suất cao hơn so với mô hình đã cắt tỉa LRP. Đáng chú ý nhất, chỉ có các mô hình được cắt tỉa với tiêu chí dựa trên LRP thể hiện hiệu suất dự đoán và hành vi — được đo bằng giá trị trung bình và độ lệch chuẩn của độ chính xác được đo trên tất cả 50 hạt giống ngẫu nhiên cho mỗi n mẫu tham chiếu trên dữ liệu cố ý nhiễu nặng — rất giống với mô hình gốc và chưa cắt tỉa, từ chỉ n=5 mẫu tham chiếu cho mỗi lớp trở lên, trên tất cả các bộ dữ liệu. Điều này mang lại một chỉ báo mạnh mẽ khác rằng LRP, trong số các tiêu chí được so sánh, có khả năng nhất trong việc bảo tồn lõi có liên quan của hàm mạng đã học, và để loại bỏ các phần không quan trọng của mô hình trong quá trình cắt tỉa.

Kết quả mạnh mẽ của LRP, và sự tương đồng một phần giữa kết quả trên các bộ dữ liệu đào tạo giữa LRP và trọng số đặt ra câu hỏi về nơi và cách cả hai thước đo (và Taylor và gradient) khác biệt, vì có thể mong đợi rằng cả hai thước đo ít nhất chọn các bộ đơn vị mạng có nhiều điểm chung cao để cắt tỉa và bảo tồn. Do đó chúng tôi điều tra trong tất cả ba thiết lập đồ chơi — trên các số lượng mẫu tham chiếu khác nhau và hạt giống ngẫu nhiên — các (dis)tương đồng và (in)nhất quán trong việc chọn và xếp hạng neuron bằng cách đo các tương đồng tập hợp (S1 ∩ S2)/min(|S1|, |S2|) của k neuron được chọn để cắt tỉa (xếp hạng đầu tiên) và bảo tồn (xếp hạng cuối cùng) giữa và trong các tiêu chí.

Vì tiêu chí trọng số không bị ảnh hưởng bởi việc chọn mẫu tham chiếu để tính toán, người ta mong đợi rằng thứ tự neuron kết quả hoàn toàn nhất quán với chính nó trong tất cả các thiết lập (tham khảo Bảng 2). Tuy nhiên điều bất ngờ, với kết quả trong Hình 3 và Hình 4 chỉ ra hành vi mô hình tương tự sau cắt tỉa được mong đợi giữa các tiêu chí dựa trên LRP và dựa trên trọng số, ít nhất trên dữ liệu đào tạo, là sự chồng chéo tập hợp tối thiểu giữa LRP và trọng số, với các tương đồng tập hợp cao hơn giữa LRP và các tiêu chí gradient và Taylor, như được hiển thị trong Bảng 1. Nhìn chung, sự chồng chéo tập hợp giữa các neuron được xếp hạng ở các cực của các thứ tự cho thấy rằng các chiến lược cắt tỉa có nguồn gốc LRP có rất ít điểm chung với những chiến lược bắt nguồn từ các tiêu chí khác. Quan sát này cũng có thể được thực hiện trên các mạng phức tạp hơn trong tay của Hình 7, như được hiển thị và thảo luận sau trong Phần này.

Bảng 2 báo cáo tự tương đồng trong việc chọn neuron ở các cực của việc xếp hạng trên các hạt giống ngẫu nhiên (và do đó các bộ mẫu tham chiếu), cho tất cả các tiêu chí và thiết lập đồ chơi. Trong khi LRP mang lại sự nhất quán cao trong việc chọn neuron cho cả việc cắt tỉa (first-k) và bảo tồn (last-k) các đơn vị mạng neural, cả gradient và đặc biệt là Taylor đều thể hiện tự tương đồng thấp hơn. Sự nhất quán thấp hơn của cả hai tiêu chí sau trong các thành phần mô hình được xếp hạng cuối cùng (tức là được bảo tồn trong mô hình lâu nhất trong quá trình cắt tỉa) mang lại một lời giải thích cho sự biến đổi lớn trong kết quả được quan sát trước đó: mặc dù gradient và Taylor rất nhất quán trong việc loại bỏ các neuron được đánh giá là không liên quan, tính bất ổn của chúng trong việc bảo tồn các neuron tạo thành lõi chức năng của mạng sau cắt tỉa mang lại sự khác biệt trong hàm dự đoán kết quả. Sự nhất quán cao được báo cáo cho LRP về mặt bộ neuron được chọn để cắt tỉa và bảo tồn, với hệ số tương quan Spearman tương đối thấp chỉ ra chỉ những nhiễu loạn cục bộ nhỏ của thứ tự cắt tỉa do việc chọn mẫu tham chiếu. Chúng tôi tìm thấy một tương ứng trực tiếp giữa (in)sự nhất quán của hành vi cắt tỉa được báo cáo ở đây cho ba tiêu chí phụ thuộc dữ liệu, và "tính liên tục giải thích" quan sát được cho LRP (và tính không liên tục cho gradient và Taylor) trong các mạng neural chứa hàm kích hoạt ReLU thường được sử dụng, điều này cung cấp một lời giải thích cho sự nhất quán cắt tỉa cao thu được với LRP, và tính bất ổn cực kỳ cao cho gradient và Taylor. Một phân tích bổ sung về tính nhất quán chọn neuron của LRP trên các số lượng mẫu tham chiếu n khác nhau, chứng minh yêu cầu chỉ rất ít mẫu tham chiếu cho mỗi lớp để thu được kết quả cắt tỉa ổn định, có thể được tìm thấy trong Kết quả Bổ sung 1.

Tổng hợp lại, kết quả của Bảng 1 đến 2 và Bảng Bổ sung 1 và 2 làm sáng tỏ rằng LRP tạo thành — so với các phương pháp khác — một tiêu chí cắt tỉa trực giao rất nhất quán trong việc chọn các đơn vị mạng neural (không) quan trọng, trong khi vẫn thích ứng với việc chọn mẫu tham chiếu để tính toán tiêu chí. Đặc biệt là sự tương đồng trong hiệu suất mô hình sau cắt tỉa với tiêu chí trọng số tĩnh chỉ ra rằng cả hai thước đo đều có thể tìm thấy các giải pháp cắt tỉa hợp lệ, nhưng hoàn toàn khác nhau. Tuy nhiên, vì LRP vẫn có thể được hưởng lợi từ ảnh hưởng của các mẫu tham chiếu, chúng tôi sẽ cho thấy trong Phần 4.2.2 rằng tiêu chí đề xuất của chúng tôi có thể vượt trội không chỉ trọng số, mà tất cả các tiêu chí khác trong Kịch bản 2, nơi cắt tỉa được sử dụng thay vì tinh chỉnh như một phương tiện thích ứng miền. Điều này sẽ được thảo luận trong các phần sau.

### 4.2. Cắt tỉa Bộ phân loại Hình ảnh Sâu cho Dữ liệu Chuẩn Quy mô lớn

Bây giờ chúng tôi đánh giá hiệu suất của tất cả các tiêu chí cắt tỉa trên các CNN, VGG-16, AlexNet cũng như ResNet-18 và ResNet-50, — các mô hình phổ biến trong nghiên cứu nén [42] — tất cả đều được đào tạo trước trên ILSVRC 2012 (ImageNet). VGG-16 bao gồm 13 lớp tích chập với 4224 bộ lọc và 3 lớp kết nối đầy đủ và AlexNet chứa 5 lớp tích chập với 1552 bộ lọc và 3 lớp kết nối đầy đủ. Trong các lớp dày đặc, có tồn tại 4,096 + 4,096 + k neuron (tức là bộ lọc), tương ứng, trong đó k là số lượng lớp đầu ra. Về mặt độ phức tạp của mô hình, VGG-16 và AlexNet được đào tạo trước trên ImageNet ban đầu bao gồm 138.36/60.97 triệu tham số và 154.7/7.27 Giga Multiply-Accumulate Operations per Second (GMACS) (như một thước đo FLOP), tương ứng. ResNet-18 và ResNet-50 bao gồm 20/53 lớp tích chập với 4,800/26,560 bộ lọc. Về mặt độ phức tạp của mô hình, ResNet-18 và ResNet-50 được đào tạo trước trên ImageNet ban đầu bao gồm 11.18/23.51 triệu tham số và 1.82/4.12 GMACS (như một thước đo FLOP), tương ứng.

Hơn nữa, vì điểm LRP không bất biến triển khai và phụ thuộc vào các quy tắc LRP được sử dụng cho các lớp chuẩn hóa theo batch (BN), chúng tôi chuyển đổi ResNet được đào tạo thành một phiên bản chính tắc, mang lại các dự đoán giống nhau đến sai số số học. Việc chính tắc hóa hợp nhất một chuỗi của một lớp tích chập và một lớp BN thành một lớp tích chập với trọng số được cập nhật và đặt lại lớp BN thành hàm đồng nhất. Điều này loại bỏ lớp BN một cách hiệu quả bằng cách viết lại một chuỗi hai ánh xạ affine thành một ánh xạ affine được cập nhật [43]. Thay đổi thứ hai thay thế các lệnh gọi đến các phương thức torch.nn.functional và phép tổng trong kết nối dư thừa bằng các lớp được dẫn xuất từ torch.nn.Module sau đó được bao bọc bởi các lệnh gọi đến torch.autograd.function để cho phép tính toán ngược tùy chỉnh phù hợp cho tính toán quy tắc LRP.

Thí nghiệm được thực hiện trong khung PyTorch và torchvision dưới Intel(R) Xeon(R) CPU E5-2660 2.20GHz và NVIDIA Tesla P100 với 12GB để xử lý GPU. Chúng tôi đánh giá các tiêu chí trên sáu bộ dữ liệu công cộng (Scene 15 [36], Event 8, Cats and Dogs [38], Oxford Flower 102 [39], CIFAR-10, và ILSVRC 2012 [40]). Để biết thêm chi tiết về các bộ dữ liệu và tiền xử lý, xem Phương pháp Bổ sung 1. Thiết lập thí nghiệm hoàn chỉnh của chúng tôi bao gồm các bộ dữ liệu này có sẵn công khai tại https://github.com/seulkiyeom/LRP_pruning.

Để chuẩn bị các mô hình cho đánh giá, trước tiên chúng tôi tinh chỉnh các mô hình trong 200 epoch với tốc độ học hằng số 0.001 và kích thước batch 20. Chúng tôi sử dụng bộ tối ưu hóa Stochastic Gradient Descent (SGD) với động lượng 0.9. Ngoài ra, chúng tôi cũng áp dụng dropout cho các lớp kết nối đầy đủ với xác suất 0.5. Tinh chỉnh và cắt tỉa được thực hiện trên bộ đào tạo, trong khi kết quả được đánh giá trên mỗi bộ dữ liệu thử nghiệm. Trong suốt các thí nghiệm, chúng tôi lặp đi lặp lại cắt tỉa 5% tất cả các bộ lọc trong mạng bằng cách loại bỏ các đơn vị bao gồm kết nối đầu vào và đầu ra của chúng. Trong Kịch bản 1, chúng tôi sau đó tinh chỉnh và đánh giá lại mô hình để tính đến sự phụ thuộc giữa các tham số và lấy lại hiệu suất, như thường thấy.

#### 4.2.1. Kịch bản 1: Cắt tỉa với Tinh chỉnh

Trong kịch bản đầu tiên, chúng tôi đào tạo lại mô hình sau mỗi lần lặp cắt tỉa để lấy lại hiệu suất bị mất. Sau đó chúng tôi đánh giá hiệu suất của các tiêu chí cắt tỉa khác nhau sau mỗi bước cắt tỉa-đào tạo lại. Đó là, chúng tôi định lượng tầm quan trọng của mỗi bộ lọc bằng độ lớn của tiêu chí tương ứng và lặp đi lặp lại cắt tỉa 5% tất cả các bộ lọc (w.r.t. số lượng bộ lọc ban đầu trong mô hình) được đánh giá là ít quan trọng nhất trong mỗi bước cắt tỉa. Sau đó, chúng tôi tính toán và ghi lại tổn thất đào tạo, độ chính xác thử nghiệm, số lượng tham số còn lại và tổng FLOP ước tính. Chúng tôi giả định rằng các bộ lọc ít quan trọng nhất chỉ nên có ít ảnh hưởng đến dự đoán và do đó gây ra sự sụt giảm hiệu suất thấp nhất nếu chúng bị loại bỏ khỏi mạng.

Hình 5 (và Hình Bổ sung 2) mô tả độ chính xác thử nghiệm với tỷ lệ cắt tỉa tăng trong VGG-16 và ResNet-50 (và AlexNet và ResNet-18, tương ứng) sau tinh chỉnh cho mỗi bộ dữ liệu và mỗi tiêu chí. Người ta quan sát rằng LRP đạt được độ chính xác thử nghiệm cao hơn so với các tiêu chí khác trong đa số lớn các trường hợp (xem Hình 6 và Hình Bổ sung 1). Những kết quả này chứng minh rằng hiệu suất của cắt tỉa dựa trên LRP ổn định và độc lập với bộ dữ liệu được chọn. Ngoài hiệu suất, chính quy hóa theo lớp là một ràng buộc quan trọng cản trở việc mở rộng một số tiêu chí hướng tới nhiều chiến lược cắt tỉa như cắt tỉa cục bộ, cắt tỉa toàn cục, v.v. Ngoại trừ tiêu chí LRP, tất cả các tiêu chí đều hoạt động kém đáng kể mà không có chính quy hóa lp so với những tiêu chí có chính quy hóa lp và dẫn đến sự gián đoạn bất ngờ trong quá trình cắt tỉa do việc phân phối lại thiên lệch tầm quan trọng trong mạng (tham khảo hàng trên của Hình 5 và Hình Bổ sung 2).

Bảng 3 cho thấy hiệu suất dự đoán của các tiêu chí khác nhau về mặt tổn thất đào tạo, độ chính xác thử nghiệm, số lượng tham số còn lại và FLOP, cho các mô hình VGG-16 và ResNet-50. Kết quả tương tự cho AlexNet và ResNet-18 có thể được tìm thấy trong Bảng Bổ sung 2. Ngoại trừ CIFAR-10, tỷ lệ nén cao nhất (tức là số lượng tham số thấp nhất) có thể đạt được bằng tiêu chí dựa trên LRP được đề xuất (hàng "Params") cho VGG-16, nhưng không cho ResNet-50. Tuy nhiên, về mặt FLOP, tiêu chí đề xuất chỉ vượt trội so với tiêu chí trọng số, nhưng không so với các tiêu chí Taylor và Gradient (hàng "FLOPs"). Điều này là do thực tế rằng việc giảm số lượng FLOP phụ thuộc vào vị trí nơi cắt tỉa được áp dụng trong mạng: Hình 7 cho thấy rằng các tiêu chí LRP và trọng số tập trung cắt tỉa vào các lớp trên gần đầu ra mô hình hơn, trong khi các tiêu chí Taylor và Gradient tập trung nhiều hơn vào các lớp dưới.

Trong suốt quá trình cắt tỉa thường có thể quan sát thấy sự giảm hiệu suất dần dần. Tuy nhiên, với các bộ dữ liệu Event 8, Oxford Flower 102 và CIFAR-10, cắt tỉa dẫn đến sự gia tăng hiệu suất ban đầu, cho đến khi đạt được tỷ lệ cắt tỉa khoảng 30%. Hành vi này đã được báo cáo trước đây trong tài liệu và có thể bắt nguồn từ sự cải thiện cấu trúc mô hình thông qua việc loại bỏ các bộ lọc liên quan đến các lớp trong bộ dữ liệu nguồn (tức là, ILSVRC 2012) không còn hiện diện trong bộ dữ liệu đích nữa [44]. Bảng Bổ sung 3 và Hình Bổ sung 2 tương tự cho thấy rằng LRP đạt được độ chính xác thử nghiệm cao nhất trong AlexNet và ResNet-18 cho gần như tất cả các tỷ lệ cắt tỉa với hầu như mọi bộ dữ liệu.

Hình 7 cho thấy số lượng các bộ lọc tích chập còn lại cho mỗi lần lặp. Chúng ta quan sát rằng, một mặt, khi tỷ lệ cắt tỉa tăng, các bộ lọc tích chập trong các lớp trước liên quan đến các đặc trưng rất chung, chẳng hạn như các bộ phát hiện cạnh và blob, có xu hướng thường được bảo tồn trái ngược với những bộ lọc trong các lớp sau liên quan đến các đặc trưng trừu tượng, cụ thể cho nhiệm vụ. Mặt khác, tiêu chí LRP- và trọng số đầu tiên giữ các bộ lọc trong các lớp đầu ở giai đoạn đầu, nhưng sau đó cắt tỉa mạnh mẽ các bộ lọc gần đầu vào hiện đã mất chức năng như đầu vào cho các lớp sau, so với các tiêu chí dựa trên gradient như các phương pháp dựa trên gradient và Taylor. Mặc dù các tiêu chí dựa trên gradient cũng áp dụng phương pháp tham lam từ lớp này đến lớp khác, chúng ta có thể thấy rằng các tiêu chí dựa trên gradient đã cắt tỉa các bộ lọc ít quan trọng gần như đồng đều trên tất cả các lớp do việc chuẩn hóa lại tiêu chí trong mỗi lần lặp. Tuy nhiên, kết quả này trái ngược với các công trình dựa trên gradient trước đây [22,25] đã chỉ ra rằng các đơn vị được coi là không quan trọng trong các lớp trước, đóng góp đáng kể so với các đơn vị được coi là quan trọng trong các lớp sau. Ngược lại với điều này, LRP có thể bảo tồn hiệu quả các đơn vị trong các lớp đầu — miễn là chúng phục vụ một mục đích — bất chấp cắt tỉa toàn cục lặp đi lặp lại.

#### 4.2.2. Kịch bản 2: Cắt tỉa mà không có Tinh chỉnh

Trong phần này, chúng tôi đánh giá liệu cắt tỉa có hoạt động tốt nếu chỉ có một số lượng (rất) hạn chế mẫu có sẵn để định lượng các tiêu chí cắt tỉa. Theo hiểu biết tốt nhất của chúng tôi, không có nghiên cứu trước đây nào cho thấy hiệu suất của các phương pháp cắt tỉa khi hoạt động w.r.t. một lượng dữ liệu rất nhỏ. Với một lượng lớn dữ liệu có sẵn (và mặc dù chúng ta có thể mong đợi hiệu suất hợp lý sau cắt tỉa), một quy trình cắt tỉa và tinh chỉnh mạng lặp đi lặp lại có thể trở thành một quá trình rất tốn thời gian và nặng về mặt tính toán. Từ quan điểm thực tế, vấn đề này trở thành một vấn đề đáng kể, ví dụ với tài nguyên tính toán hạn chế (thiết bị di động hoặc nói chung; phần cứng cấp người tiêu dùng) và dữ liệu tham chiếu (ví dụ, bộ sưu tập ảnh riêng tư), nơi các phương pháp cắt tỉa một lần có khả năng và hiệu quả được mong muốn và chỉ có rất ít dư địa (hoặc không có gì cả) cho các chiến lược tinh chỉnh sau cắt tỉa có sẵn.

Để điều tra xem liệu cắt tỉa có thể thực hiện được cũng trong những kịch bản này, chúng tôi đã thực hiện thí nghiệm với một số lượng tương đối nhỏ dữ liệu trên 1) Cats & Dogs và 2) các tập con từ các lớp ILSVRC 2012. Trên bộ dữ liệu Cats & Dogs, chúng tôi chỉ sử dụng 10 mẫu mỗi loại từ các lớp "cat" và "dog" để cắt tỉa các mạng AlexNet, VGG-16, ResNet-18 và ResNet-50 được đào tạo trước (trên ImageNet) với mục tiêu thích ứng miền/bộ dữ liệu. Phân loại nhị phân (tức là "cat" vs. "dog") là một nhiệm vụ con trong phân loại ImageNet và các neuron đầu ra tương ứng có thể được xác định bằng các liên kết WordNet của nó. Thí nghiệm này triển khai nhiệm vụ thích ứng miền.

Trong một thí nghiệm thứ hai trên bộ dữ liệu ILSVRC 2012, chúng tôi đã chọn ngẫu nhiên k=3 lớp cho nhiệm vụ chuyên biệt hóa mô hình, chỉ chọn n=10 hình ảnh cho mỗi lớp từ bộ đào tạo và sử dụng chúng để so sánh các tiêu chí cắt tỉa khác nhau. Đối với mỗi tiêu chí, chúng tôi đã sử dụng cùng một lựa chọn các lớp và mẫu. Trong cả hai thiết lập thí nghiệm, chúng tôi không tinh chỉnh các mô hình sau mỗi lần lặp cắt tỉa, trái ngược với Kịch bản 1 trong Phần 4.2.1. Hiệu suất mô hình sau cắt tỉa thu được được tính trung bình trên 20 lựa chọn ngẫu nhiên các lớp (ImageNet) và mẫu (Cats & Dogs) để tính đến tính ngẫu nhiên. Xin lưu ý rằng trước khi cắt tỉa, trước tiên chúng tôi tái cấu trúc các lớp đầu ra kết nối đầy đủ của các mô hình để chỉ bảo tồn k đầu ra mạng liên quan đến nhiệm vụ bằng cách loại bỏ 1000-k neuron đầu ra dư thừa.

Hơn nữa, vì các bộ dữ liệu đích của chúng tôi tương đối nhỏ và chỉ có một bộ lớp đích cực kỳ giảm, các mô hình đã cắt tỉa vẫn có thể rất nặng w.r.t. yêu cầu bộ nhớ nếu quá trình cắt tỉa bị giới hạn ở các lớp tích chập, như trong Phần 4.2.1. Cụ thể hơn, trong khi các lớp tích chập chủ yếu tạo thành nguồn chi phí tính toán (FLOP), các lớp kết nối đầy đủ được chứng minh là dư thừa hơn [29]. Về mặt này, chúng tôi đã áp dụng các quy trình cắt tỉa trong cả lớp kết nối đầy đủ và lớp tích chập kết hợp cho VGG-16.

Đối với cắt tỉa, chúng tôi lặp đi lặp lại một chuỗi đầu tiên cắt tỉa các bộ lọc từ các lớp tích chập, tiếp theo là một bước cắt tỉa các neuron từ các lớp kết nối đầy đủ của mô hình. Lưu ý rằng cả hai kiến trúc ResNet được đánh giá chủ yếu bao gồm các lớp tích chập và pooling, và kết thúc trong một lớp dày đặc duy nhất, trong đó bộ neuron đầu vào chỉ bị ảnh hưởng qua đầu vào của chúng bằng cách cắt tỉa ngăn xếp tích chập bên dưới. Do đó chúng tôi hạn chế việc cắt tỉa lặp đi lặp lại các bộ lọc từ chuỗi các lớp dày đặc của kiến trúc feedforward của VGG-16.

Hiệu suất mô hình sau khi áp dụng mỗi tiêu chí để phân loại một số lượng nhỏ lớp (k=3) từ bộ dữ liệu ILSVRC 2012 được chỉ ra trong Hình 8 cho VGG 16 và Hình 9 cho ResNet (xin lưu ý một lần nữa rằng ResNet không có các lớp kết nối đầy đủ). Trong quá trình cắt tỉa tại các lớp kết nối đầy đủ, không thể quan sát thấy sự khác biệt đáng kể trên các tỷ lệ cắt tỉa khác nhau. Mà không cần tinh chỉnh thêm, việc cắt tỉa trọng số/bộ lọc tại các lớp kết nối đầy đủ có thể duy trì hiệu suất một cách hiệu quả. Tuy nhiên, có một sự khác biệt nhất định giữa LRP và các tiêu chí khác với tỷ lệ cắt tỉa tăng của các lớp tích chập cho VGG-16/ResNet-18/ResNet-50, tương ứng: (LRP vs. Taylor với l2-norm; lên đến 9.6/61.8/51.8%, LRP vs. gradient với l2-norm; lên đến 28.0/63.6/54.5%, LRP vs. trọng số với l2-norm; lên đến 27.1/48.3/30.2%). Hơn nữa, việc cắt tỉa các lớp tích chập cần được quản lý cẩn thận so với việc cắt tỉa các lớp kết nối đầy đủ. Chúng ta có thể quan sát rằng LRP có thể áp dụng để cắt tỉa bất kỳ loại lớp nào (tức là kết nối đầy đủ, tích chập, pooling, v.v.) một cách hiệu quả. Ngoài ra, như đã đề cập trong Phần 3.1, phương pháp của chúng tôi có thể được áp dụng cho các kiến trúc mạng chung vì nó có thể tự động đo tầm quan trọng của trọng số hoặc bộ lọc trong bối cảnh toàn cục (toàn mạng) mà không cần chuẩn hóa thêm.

Hình 10 cho thấy độ chính xác thử nghiệm như một hàm của tỷ lệ cắt tỉa, trong bối cảnh một nhiệm vụ thích ứng miền từ ImageNet hướng tới bộ dữ liệu Cats & Dogs cho tất cả các mô hình. Khi tỷ lệ cắt tỉa tăng, chúng ta có thể thấy rằng ngay cả khi không có tinh chỉnh, việc sử dụng LRP như tiêu chí cắt tỉa có thể giữ độ chính xác thử nghiệm không chỉ ổn định, mà gần 100%, với sự khan hiếm cực kỳ của dữ liệu trong thí nghiệm này. Ngược lại, hiệu suất giảm đáng kể khi sử dụng các tiêu chí khác đòi hỏi một ứng dụng của chuẩn l2. Ban đầu, hiệu suất thậm chí còn tăng nhẹ khi cắt tỉa với LRP. Trong quá trình cắt tỉa lặp đi lặp lại, những thay đổi bất ngờ trong độ chính xác với LRP (đối với 2 trong số 20 lần lặp lại thí nghiệm) đã được hiển thị xung quanh tỷ lệ cắt tỉa 50 - 55%, nhưng độ chính xác được phục hồi nhanh chóng một lần nữa. Tuy nhiên, chỉ có mô hình VGG-16 dường như bị ảnh hưởng, và không có mô hình nào khác cho nhiệm vụ này. Đối với cả hai mô hình ResNet, hiện tượng này xảy ra cho các tiêu chí khác thay thế. Một loạt các điều tra sâu về sự giảm tạm thời này trong hiệu suất đã không dẫn đến bất kỳ hiểu biết nào và sẽ là chủ đề của công việc tương lai.

Bằng cách cắt tỉa hơn 99% các bộ lọc tích chập trong các mạng sử dụng phương pháp đề xuất của chúng tôi, chúng ta có thể có 1) chi phí tính toán giảm đáng kể, 2) xử lý tiến và lùi nhanh hơn (ví dụ cho mục đích đào tạo thêm, suy luận hoặc tính toán bản đồ phân bổ), và 3) một mô hình nhẹ hơn ngay cả trong trường hợp mẫu nhỏ, tất cả trong khi thích ứng các mô hình ImageNet được đào tạo trước có sẵn hướng tới một nhiệm vụ phân loại chó-vs.-mèo.

## 5. Thảo luận

Các thí nghiệm của chúng tôi chứng minh rằng tiêu chí LRP mới đã hoạt động tốt một cách nhất quán so với các tiêu chí khác trên các bộ dữ liệu khác nhau, kiến trúc mô hình và thiết lập thí nghiệm, và thường xuyên vượt trội so với các tiêu chí cạnh tranh. Điều này đặc biệt rõ ràng trong Kịch bản 2 của chúng tôi (tham khảo Phần 4.2.2), nơi chỉ có rất ít tài nguyên có sẵn để tính toán tiêu chí, và không có tinh chỉnh sau cắt tỉa được cho phép. Ở đây, LRP vượt trội đáng kể so với các thước đo khác trên dữ liệu đồ chơi (tham khảo Phần 4.1) và dữ liệu chuẩn xử lý hình ảnh (tham khảo Phần 4.2.2). Kết quả rất tương tự giữa các tiêu chí được quan sát trong Kịch bản 1 (tham khảo Phần 4.2.2) cũng không đáng ngạc nhiên, vì một bước tinh chỉnh bổ sung sau cắt tỉa có thể cho phép mô hình mạng neural đã cắt tỉa khôi phục hiệu suất ban đầu của nó, miễn là mô hình có khả năng để làm như vậy [22].

Từ kết quả của Bảng 3 và Bảng Bổ sung 3, chúng ta có thể quan sát rằng với một mục tiêu cắt tỉa cố định n% bộ lọc bị loại bỏ, LRP có thể không luôn dẫn đến mạng con rẻ nhất sau cắt tỉa về mặt số lượng tham số và FLOP cho mỗi suy luận, tuy nhiên nó liên tục có thể xác định các thành phần mạng để loại bỏ và bảo tồn dẫn đến mô hình hoạt động tốt nhất sau cắt tỉa. Kết quả sau cũng cộng hưởng mạnh mẽ trong các thí nghiệm của chúng tôi về Kịch bản 2 trên cả dữ liệu hình ảnh và đồ chơi, nơi, mà không có bước tinh chỉnh bổ sung, các mô hình đã cắt tỉa LRP vượt trội rất nhiều so với các đối thủ cạnh tranh của chúng. Kết quả thu được trong nhiều thiết lập đồ chơi xác minh rằng chỉ có tiêu chí cắt tỉa dựa trên LRP mới có thể bảo tồn cấu trúc ban đầu của hàm dự đoán (tham khảo Hình 2 và 3).

Không giống như tiêu chí trọng số, đây là một lượng tĩnh một khi mạng không còn trong quá trình đào tạo nữa, các tiêu chí Taylor, gradient và LRP đòi hỏi mẫu tham chiếu để tính toán, điều này có thể ảnh hưởng đến việc ước tính tầm quan trọng neuron. Tuy nhiên, từ ba tiêu chí sau, chỉ có LRP cung cấp một thước đo liên tục về tầm quan trọng cấu trúc mạng (tham khảo Sec 7.2 trong [12]) không bị ảnh hưởng bởi những thay đổi đột ngột trong các thước đo tầm quan trọng ước tính chỉ với các bước biên giữa các mẫu tham chiếu. Chất lượng liên tục này được phản ánh trong sự ổn định và chất lượng của kết quả LRP được báo cáo trong Phần 4.1, so với tính bất ổn cao trong việc chọn neuron để cắt tỉa và hiệu suất mô hình sau cắt tỉa có thể quan sát được đối với các tiêu chí gradient và Taylor. Từ quan sát này cũng có thể suy ra rằng LRP đòi hỏi tương đối ít điểm dữ liệu để hội tụ đến một giải pháp cắt tỉa có hành vi dự đoán tương tự như mô hình ban đầu. Do đó, chúng tôi kết luận rằng LRP là một tiêu chí cắt tỉa mạnh mẽ có thể áp dụng rộng rãi trong thực tế. Đặc biệt trong một kịch bản nơi không có tinh chỉnh được áp dụng sau cắt tỉa (xem Sec. 4.2.2), tiêu chí LRP cho phép cắt tỉa một phần lớn của mô hình mà không có sự sụt giảm độ chính xác đáng kể.

Về mặt chi phí tính toán, LRP có thể so sánh với các tiêu chí Taylor và Gradient vì những tiêu chí này đòi hỏi cả một lượt truyền tiến và một lượt truyền ngược cho tất cả các mẫu tham chiếu. Tiêu chí trọng số rẻ hơn đáng kể để tính toán vì nó không đòi hỏi phải đánh giá bất kỳ mẫu tham chiếu nào; tuy nhiên, hiệu suất của nó kém trong hầu hết các thí nghiệm của chúng tôi. Ngoài ra, các thí nghiệm của chúng tôi chứng minh rằng LRP đòi hỏi ít mẫu tham chiếu hơn các tiêu chí khác (tham khảo Hình 3 và Hình 4), do đó chi phí tính toán yêu cầu thấp hơn trong các kịch bản thực tế, và hiệu suất tốt hơn có thể được mong đợi nếu chỉ có số lượng thấp mẫu tham chiếu có sẵn (tham khảo Hình 10).

Không giống như tất cả các tiêu chí khác, LRP không đòi hỏi chính quy hóa rõ ràng qua chuẩn hóa lp, vì nó được chuẩn hóa tự nhiên qua nguyên tắc bảo toàn mức độ liên quan được áp đặt trong quá trình lan truyền ngược mức độ liên quan, dẫn đến việc bảo tồn các cấu trúc con mạng quan trọng và nút thắt cổ chai trong bối cảnh mô hình toàn cục. Phù hợp với các phát hiện của [22], kết quả của chúng tôi trong Hình 5 và Hình Bổ sung 2 cho thấy rằng chuẩn hóa bổ sung sau khi tính toán tiêu chí cho trọng số, gradient và Taylor không chỉ quan trọng để có được hiệu suất tốt, mà còn để tránh các phân đoạn mô hình bị ngắt kết nối — điều gì đó được ngăn chặn ngay từ đầu với LRP.

Tuy nhiên, tiêu chí đề xuất của chúng tôi vẫn cung cấp một số câu hỏi mở mà đáng được điều tra sâu hơn trong công việc tương lai. Trước hết, LRP không bất biến triển khai, tức là, cấu trúc và thành phần của mạng được phân tích có thể ảnh hưởng đến việc tính toán tiêu chí LRP và "chuẩn hóa mạng" — một việc tái cấu trúc tương đương về mặt chức năng của mô hình — có thể được yêu cầu để có kết quả tối ưu, như đã thảo luận sớm trong Phần 4 và [43]. Hơn nữa, trong khi tiêu chí LRP của chúng tôi không đòi hỏi siêu tham số bổ sung, ví dụ, để chuẩn hóa, kết quả cắt tỉa có thể vẫn phụ thuộc vào biến thể LRP được chọn. Trong bài báo này, chúng tôi đã chọn quy tắc ε10 trong tất cả các lớp, vì tham số hóa cụ thể này xác định các đường dẫn neural của mạng đóng góp tích cực cho các neuron đầu ra được chọn mà các mẫu tham chiếu được cung cấp, mạnh mẽ với các hiệu ứng có hại của gradient bị tan vỡ ảnh hưởng đặc biệt đến CNN rất sâu [11] (tức là, khác với các phương pháp dựa trên gradient, nó không bị ảnh hưởng bởi các tính không liên tục tiềm ẩn trong các lượng được lan truyền ngược), và có một nền tảng được động lực tốt về mặt toán học trong DTD [11,12]. Tuy nhiên, công việc khác từ tài liệu cung cấp [14] hoặc đề xuất [9,8] các tham số hóa thay thế để tối ưu hóa phương pháp cho mục đích giải thích. Đó là một hướng thú vị cho công việc tương lai để kiểm tra xem liệu những phát hiện này cũng áp dụng cho LRP như một tiêu chí cắt tỉa.

## 6. Kết luận

CNN hiện đại thường có dung lượng cao với hàng triệu tham số vì điều này cho phép có được kết quả tối ưu hóa tốt trong quá trình đào tạo. Tuy nhiên, sau khi đào tạo, chi phí suy luận cao vẫn còn, bất chấp thực tế rằng số lượng tham số hiệu quả trong mô hình sâu thực sự thấp hơn đáng kể (xem ví dụ [45]). Để giảm bớt điều này, cắt tỉa nhằm nén và tăng tốc các mô hình đã cho mà không hy sinh nhiều hiệu suất dự đoán. Trong bài báo này, chúng tôi đã đề xuất một tiêu chí mới cho việc cắt tỉa lặp đi lặp lại các CNN dựa trên phương pháp giải thích LRP, lần đầu tiên liên kết hai dòng nghiên cứu từ trước đến nay bị ngắt kết nối. LRP có một ý nghĩa được định nghĩa rõ ràng, đó là đóng góp của một đơn vị mạng riêng lẻ, tức là trọng số hoặc bộ lọc, đối với đầu ra mạng. Việc loại bỏ các đơn vị theo điểm LRP thấp do đó có nghĩa là loại bỏ tất cả các khía cạnh trong mô hình không đóng góp mức độ liên quan vào việc ra quyết định của nó. Do đó, như một tiêu chí, các điểm mức độ liên quan được tính toán có thể dễ dàng và rẻ cung cấp tỷ lệ nén hiệu quả mà không cần xử lý hậu kỳ thêm, chẳng hạn như chuẩn hóa theo lớp. Bên cạnh đó, về mặt kỹ thuật LRP có thể mở rộng cho các cấu trúc mạng chung và chi phí tính toán của nó tương tự như của một lượt truyền ngược gradient.

Trong các thí nghiệm của chúng tôi, tiêu chí LRP đã cho thấy hiệu suất nén thuận lợi trên nhiều bộ dữ liệu khác nhau cả với và không có đào tạo lại sau cắt tỉa. Đặc biệt khi cắt tỉa mà không có đào tạo lại, kết quả của chúng tôi cho các bộ dữ liệu nhỏ cho thấy rằng tiêu chí LRP vượt trội so với hiện đại và do đó, việc áp dụng của nó đặc biệt được khuyến nghị trong các thiết lập học chuyển giao nơi chỉ có một bộ dữ liệu đích nhỏ có sẵn.

Ngoài cắt tỉa, cùng một phương pháp có thể được sử dụng để diễn giải trực quan mô hình và giải thích các quyết định riêng lẻ như bản đồ nhiệt mức độ liên quan trực quan. Do đó, trong công việc tương lai, chúng tôi đề xuất sử dụng những bản đồ nhiệt này để làm sáng tỏ và giải thích những đặc trưng hình ảnh nào bị ảnh hưởng mạnh nhất bởi cắt tỉa để tránh thêm rằng quá trình cắt tỉa dẫn đến hiện tượng Clever Hans không mong muốn [8].

## Lời cảm ơn

Công việc này được hỗ trợ bởi Bộ Giáo dục và Nghiên cứu Đức (BMBF) thông qua BIFOLD (refs. 01IS18025A và 01IS18037A), MALT III (ref. 01IS17058), Patho234 (ref. 031L0207D) và TraMeExCo (ref. 01IS18056A), cũng như các Khoản trợ cấp 01GQ1115 và 01GQ0850; và bởi Deutsche Forschungsgesellschaft (DFG) dưới Grant Math+, EXC 2046/1, Project ID 390685689; bởi grant Viện Quy hoạch & Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi Chính phủ Hàn Quốc (Số 2019-0-00079, Chương trình Trường Đại học Trí tuệ Nhân tạo, Đại học Korea); và bởi Phòng thí nghiệm Công ty An ninh mạng STE-SUTD; grant AcRF Tier2 MOE2016-T2-2-154; dự án TL Intent Inference; và grant nội bộ SUTD Fundamentals and Theory of AI Systems. Các tác giả muốn bày tỏ lời cảm ơn đến Christopher J Anders cho các cuộc thảo luận sâu sắc.
