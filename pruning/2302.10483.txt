# 2302.10483.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/2302.10483.pdf
# File size: 2502734 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF XXX 1
Structured Bayesian Compression for Deep Neural
Networks Based on The Turbo-VBI Approach
Chengyu Xia, Danny H. K. Tsang, Fellow, IEEE, and Vincent K. N. Lau,Fellow, IEEE
Abstract ‚ÄîWith the growth of neural network size, model
compression has attracted increasing interest in recent research.
As one of the most common techniques, pruning has been studied
for a long time. By exploiting the structured sparsity of the
neural network, existing methods can prune neurons instead of
individual weights. However, in most existing pruning methods,
surviving neurons are randomly connected in the neural network
without any structure, and the non-zero weights within each
neuron are also randomly distributed. Such irregular sparse
structure can cause very high control overhead and irregular
memory access for the hardware and even increase the neural
network computational complexity. In this paper, we propose a
three-layer hierarchical prior to promote a more regular sparse
structure during pruning. The proposed three-layer hierarchical
prior can achieve per-neuron weight-level structured sparsity and
neuron-level structured sparsity. We derive an efÔ¨Åcient Turbo-
variational Bayesian inferencing (Turbo-VBI) algorithm to solve
the resulting model compression problem with the proposed prior.
The proposed Turbo-VBI algorithm has low complexity and can
support more general priors than existing model compression
algorithms. Simulation results show that our proposed algorithm
can promote a more regular structure in the pruned neural
networks while achieving even better performance in terms of
compression rate and inferencing accuracy compared with the
baselines.
Index Terms ‚ÄîDeep neural networks, Group sparsity, Model
compression, Pruning.
I. I NTRODUCTION
DEEP neural networks (DNNs) have been extremely suc-
cessful in a wide range of applications. However, to
achieve good performance, state-of-the-art DNNs tend to have
huge numbers of parameters. Deployment and transmission of
such big models pose signiÔ¨Åcant challenges for computation,
memory and communication. This is particularly important for
edge devices, which have very restricted resources. For the
above reasons, model compression has become a hot topic in
deep learning.
A variety of research focused on model compression exists.
In [1]-[3], the weight pruning approaches are adopted to re-
duce the complexity of DNN inferencing. In [1], the threshold
is set to be the weight importance, which is measured by
introducing an auxiliary importance variable for each weight.
With the auxiliary variables, the pruning can be performed
in one shot and before the formal training. In [2] and [3],
Hessian matrix of the weights are used as a criterion to perform
Vincent Lau is the corresponding author.
Chengyu Xia, Danny Tsang and Vincent Lau are with the Department of Elec-
tronic and Computer Engineering, The Hong Kong University of Science and
Technology, Hong Kong, e-mail: cxiaab@connect.ust.hk; eetsang@ece.ust.hk;
eeknlau@ece.ust.hk.pruning. These aforementioned approaches, however, cannot
proactively force some of the less important weights to have
small values and hence the approaches just passively prune
small weights. In [4]-[13], systematic approaches of model
compression using optimization of regularized loss functions
are adopted. In [11], an `2-norm based regularizer is applied
to the weights of a DNN and the weights that are close
to zero are pruned. In [4], an `0-norm regularizer, which
forces the weights to be exactly zero, is proposed. The `0-
norm regularization is proved to be more efÔ¨Åcient to enforce
sparsity in the weights but the training problem is notoriously
difÔ¨Åcult to solve due to the discontinuous nature of the `0-
norm. In these works, the regularization only promotes sparsity
in the weights. Yet, sparsity in the weights is not equivalent
to sparsity in the neurons. In [9], polarization regularization
is proposed, which can push a proportion of weights to zero
and others to values larger than zero. Then, all the weights
connected to the same neuron are assigned with a common
polarization regularizer, such that some neurons can be entirely
pruned and some can be pushed to values larger than zero. In
this way, the remaining weights do not have to be pushed to
small values and hence can improve the expressing ability of
the network. In [5], the authors use a group Lasso regularizer
to remove entire Ô¨Ålters in CNNs and show that their proposed
group sparse regularization can even increase the accuracy
of a ResNet. However, the resulting neurons are randomly
connected in the neural network and the resulting datapath of
the pruned neural network is quite irregular, making it difÔ¨Åcult
to implement in hardware [5], [6].
In addition to the aforementioned deterministic regulariza-
tion approaches, we can also impose sparsity in the weights
of the neural network using Bayesian approaches. In [14],
weight pruning and quantization are realized at the same time
by assigning a set of quantizing gates to the weight value.
A prior is further designed to force the quantizing gates to
‚Äúclose‚Äù, such that the weights are forced to zero and the non-
zero weights are forced to a low bit precision. By designing
an appropriate prior distribution for the weights, one can
enforce more reÔ¨Åned structures in the weight vector. In [15],
a simple sparse prior is proposed to promote weight-level
sparsity, and variational Bayesian inference (VBI) is adopted
to solve the model compression problem. In [16], group
sparsity is investigated from a Bayesian perspective. A two-
layer hierarchical sparse prior is proposed where the weights
follow Gaussian distributions and all the output weights of a
neuron share the same prior variance. Thus, the output weights
of a neuron can be pruned at the same time and neuron-level
sparsity is achieved.arXiv:2302.10483v1  [cs.LG]  21 Feb 2023

--- PAGE 2 ---
JOURNAL OF XXX 2
In this paper, we consider model compression of DNNs
from a Bayesian perspective. Despite various existing works
on model compression, there are still several technical issues
to be addressed.
Structured datapath in the pruned neural network: In
deterministic regularization approaches, we can achieve
group sparsity in a weight matrix. For example, under
`2;1-norm regularization [5], [7], some rows can be zeroed
out in the weight matrix after pruning and this results
in neuron-level sparsity. However, surviving neurons are
randomly connected in the neural network without any
structure. Moreover, the non-zero weights within each
neuron are also randomly distributed. In the existing
Bayesian approaches, both the single-layer priors in [15]
and the two-layer hierarchical priors in [16] also cannot
promote structured neuron connectivity in the pruned
neural network. As such, the random and irregular neuron
connectivity in the datapath of the DNN poses challenges
in the hardware implementation.1
EfÔ¨Åciency of weight pruning: With existing model com-
pression approaches, the compressed model can still be
too large for efÔ¨Åcient implementation on mobile devices.
Moreover, the existing Bayesian model compression ap-
proaches tend to have high complexity and low robustness
with respect to different data sets, as they typically adopt
Monte Carlo sampling in solving the optimizing problem
[15], [16]. Thus, a more efÔ¨Åcient model compression
solution that can achieve a higher compression rate and
lower complexity is required.
To overcome the above challenges, we propose a three-layer
hierarchical sparse prior that can exploit structured weight-
level and neuron-level sparsity during training. To handle
the hierarchical sparse prior, we propose a Turbo-VBI [17]
algorithm to solve the Bayesian optimization problem in the
model compression. The main contributions are summarized
below.
Three-layer Hierarchical Prior for Structured Weight-
level and Neuron-level Sparsity: The design of an
appropriate prior distribution in the weight matrix is
critical to achieve more reÔ¨Åned structures in the weights
and neurons. To exploit more structured weight-level
and neuron-level sparsity, we propose a three-layer hi-
erarchical prior which embraces both of the traditional
two-layer hierarchical prior [16] and support-based prior
[18]. The proposed three-layer hierarchical prior has
the following advantages compared with existing sparse
priors in Bayesian model compression: 1) It promotes ad-
vanced weight-level structured sparsity as well as neuron-
level structured sparsity, as illustrated in Fig. 1c. Unlike
existing sparse priors, our proposed prior not only prunes
the neurons, but also promotes regular structures in the
unpruned neurons as well as the weights of a neuron.
Thus, it can achieve more regular structure in the overall
1When the pruned DNN has randomly connected neurons and irregular
weights within each neuron, very high control overhead and irregular memory
access will be involved for the datapath to take advantage of the compression
in the computation of the output.neural network. 2) It is Ô¨Çexible to promote different
sparse structures. The group size as well as the average
gap between two groups can be adjusted via tuning the
hyper parameters. 3) It is optimizing-friendly. It allows
the application of low complexity training algorithms.
Turbo-VBI Algorithm for Bayesian Model Compres-
sion: Based on the 3-layer hierarchical prior, the model
training and compression is formulated as a Bayesian
inference problem. We propose a low complexity Turbo-
VBI algorithm with some novel extensions compared
to existing approaches: 1) It can support more general
priors. Existing Bayesian compression methods [15], [16]
cannot handle the three-layer hierarchical prior due to
the extra layer in the prior. 2) It has higher robustness
and lower complexity compared with existing Bayesian
compression methods. Existing Bayesian compression
methods use Monte Carlo sampling to approximate the
log-likelihood term in the objective function, which has
low robustness and high complexity [19]. To overcome
this drawback, we propose a deterministic approximation
for the log-likelihood term.
Superior Model Compression Performance: The pro-
posed solution can achieve a highly compressed neural
network compared to the state-of-the-art baselines and
achieves a similar inferencing performance. Therefore,
the proposed solution can substantially reduce the com-
putational complexity in the inferencing of the neural
network and the resulting pruned neural networks are
hardware friendly. Consequently, it can fully unleash the
potential of AI applications over mobile devices.
The rest of the paper is organized as follows. In Section II,
the existing structures and the desired structure in the weight
matrix are elaborated. The three-layer hierarchical prior for
the desired structure is also introduced. In Section III, the
model compression problem is formulated. In Section IV, the
Turbo-VBI algorithm for model compression under the three-
layer hierarchical prior is presented. In Section V and Section
VI, numerical experiments and conclusions, respectively, are
provided.
II. S TRUCTURES IN THEWEIGHT MATRIX
Model compression is realized by enforcing sparsity in the
weight matrix. We Ô¨Årst review the main existing structured
sparsity of the weight matrix in the literature. Based on this,
we elaborate the desired structures in the weight matrix that are
not yet considered. Finally, we propose a 3-layer hierarchical
prior model to enforce such a desired structure in the weight
matrix.
A. Review of Existing Sparse Structured Weight Matrix
There are two major existing sparse structures in the weight
matrix, namely random sparsity and group sparsity. We focus
on a fully connected layer to elaborate these two sparse
structures.
1) Random Sparsity in the Weight Matrix: In ran-
dom sparsity, the weights are pruned individually. `1-norm

--- PAGE 3 ---
JOURNAL OF XXX 3
weight matrix topology
Totally random‚Ä¶ ‚Ä¶
neural network topology :
Totally random
weight matrix topology
1.Simple weight -level structure :
Elements canbepruned inrow-
level, butsurviving elements of
each rowarenon-structured .
2.No neuron -level structure :
Surviving rows are randomly
distributed .‚Ä¶ ‚Ä¶
neural network topology:
1.Simple weight -level structure :
Weights can be pruned in
groups, butsurviving weights of
unpruned neuron are non-
structured .
2.No neuron -level structure :
Surviving neurons arerandomly
distributed .
Weight matrix topology
1.Per-neuron weight -level structure :
Surviving elements ofeach row
tend togather ingroups .
2.Neuron -level structure :Surviving
rows tend togather ingroups .
3.Structure isflexible .‚Ä¶ ‚Ä¶
neural network topology:
1.Per-neuron weight -level structure :
Surviving weights ofaneuron
tend togather ingroups .
2.Neuron -level structure :Surviving
neurons ineach layer tend to
gather ingroups .
3.Structure isflexible .
(a) Random sparsity structure.
weight matrix topology
Totally random‚Ä¶ ‚Ä¶
neural network topology :
Totally random
weight matrix topology
1.Simple weight -level structure :
Elements canbepruned inrow-
level, butsurviving elements of
each rowarenon-structured .
2.No neuron -level structure :
Surviving rows are randomly
distributed .‚Ä¶ ‚Ä¶
neural network topology:
1.Simple weight -level structure :
Weights can be pruned in
groups, butsurviving weights of
unpruned neuron are non-
structured .
2.No neuron -level structure :
Surviving neurons arerandomly
distributed .
Weight matrix topology
1.Per-neuron weight -level structure :
Surviving elements ofeach row
tend togather ingroups .
2.Neuron -level structure :Surviving
rows tend togather ingroups .
3.Structure isflexible .‚Ä¶ ‚Ä¶
neural network topology:
1.Per-neuron weight -level structure :
Surviving weights ofaneuron
tend togather ingroups .
2.Neuron -level structure :Surviving
neurons ineach layer tend to
gather ingroups .
3.Structure isflexible .
(b) Group sparsity structure.
weight matrix topology
Totally random‚Ä¶ ‚Ä¶
neural network topology :
Totally random
weight matrix topology
1.Simple weight -level structure :
Elements canbepruned inrow-
level, butsurviving elements of
each rowarenon-structured .
2.No neuron -level structure :
Surviving rows are randomly
distributed .‚Ä¶ ‚Ä¶
neural network topology:
1.Simple weight -level structure :
Weights can be pruned in
groups, butsurviving weights of
unpruned neuron are non-
structured .
2.No neuron -level structure :
Surviving neurons arerandomly
distributed .
Weight matrix topology
1.Per-neuron weight -level structure :
Surviving elements ofeach row
tend togather ingroups .
2.Neuron -level structure :Surviving
rows tend togather ingroups .
3.Structure isflexible .‚Ä¶ ‚Ä¶
neural network topology:
1.Per-neuron weight -level structure :
Surviving weights ofaneuron
tend togather ingroups .
2.Neuron -level structure :Surviving
neurons ineach layer tend to
gather ingroups .
3.Structure isflexible .
(c) Proposed multi-level structure.
Fig. 1: Comparison of different structures in the weight matrix
of a fully connected layer with 6 input neurons and 6 output
neurons. Each row denotes the weights connected to one input
neuron.
regularization and one-layer prior [15] are often adopted to
regularize individual weights. Such regularization is simple
and optimization-friendly; however, both resulting weights
and neurons are completely randomly connected since there
is no extra constraint on the pruned weights. In the weight
matrix, this means that the elements are set to zero without
any structure, which results in the remaining elements also
having no speciÔ¨Åc structures. When translated into neural
network topology, this means that the surviving weights and
neurons are randomly connected, as illustrated in Fig. 1a.
Because of the random connections in the pruned network,
such a structure is not friendly to practical applications such
as storage, computation and transmission [5], [20].
2) Group Sparsity in the Weight Matrix: In group
sparsity, the weights are pruned in groups. Particularly, in fullyconnected layers, a group is often deÔ¨Åned as the outgoing
weights of a neuron to promote neuron-level sparsity. To
promote group sparsity, sparse group Lasso regularization
[7], [21] and two-layer priors are proposed. Generally, such
regularization often has two folds: one regularizer for group
pruning and one regularizer for individual weight pruning.
Thus, the weights can not only be pruned in groups, but can
also be pruned individually in case the group cannot be totally
removed. However, when a neuron cannot be totally pruned,
the weights connected to it are randomly pruned, and the
neurons are also pruned in random. In Fig. 1b, we illustrate
an example of group sparsity. In the weight matrix, the non-
zero elements in a row are randomly distributed and the non-
zero rows are also randomly distributed. When translated into
neural network topology, this means the unpruned weights
connected to a neuron are randomly distributed, and in each
layer, the surviving neurons are also randomly distributed.
Thus, group sparsity is still not fully structured.
On the other hand, recent research has pointed out that the
irregular connections in the neural network can bring various
disadvantages in reducing the computational complexity or
inferencing time [20], [22]-[25]. As discussed above, existing
sparsity structures are not regular enough, and the drawbacks
of such irregular structures are listed as follows.
1)Low decoding and computing parallelism: Sparse weight
matrix are usually stored in a sparse format on the
hardware, e.g., compressed sparse row (CSR), com-
pressed sparse column (CSC), coordinate format (COO).
However, as illustrated in Fig. 2 (a), when decoding
from the sparse format, the number of decoding steps for
each row (column) can vastly differ due to the irregular
structure. This can greatly harm the decoding parallelism
[23]. Moreover, the irregular structure also hampers the
parallel computation such as matrix tiling [24], [20], as
illustrated in Fig. 2 (b).
2)InefÔ¨Åcient pruning of neurons: As illustrated in Fig. 2
(c), the unstructured surviving weights from the previous
layer tend to activate random neurons in the next layer,
which is not efÔ¨Åcient in pruning the neurons in the
next layer. This may lead to inefÔ¨Åciency in reducing the
Ô¨Çoating point operations (FLOPs) of the pruned neural
network.
3)Large storage burden: Since the surviving weights of
existing model compression methods are unstructured,
the exact location for each surviving weight has to be
recorded. This leads to a huge storage overhead, which
can be even greater than storing the dense matrix [24],
[20]. The huge storage overhead can also affect the
inferencing time by increasing the memory access and
decoding time.
B. Multi-level Structures in the Weight Matrix
Based on the above discussion, a multi-level structured
sparse neural network should meet the following three criteria:
1) Per-neuron weight-level structured sparsity: The surviv-
ing weights connected to a neuron should exhibit a regular
structure rather than be randomly distributed as in traditional

--- PAGE 4 ---
JOURNAL OF XXX 4
group sparsity. In a weight matrix, this means the non-zero
elements of each row should follow some regular structure.
2) Neuron-level structured sparsity: The surviving neurons
of each layer should also have some regular structures. In
a weight matrix, this means that the non-zero rows should
also follow some structure. 3) Flexibility: The structure of
the resulting weight matrix should be sufÔ¨Åciently Ô¨Çexible so
that we can achieve a trade off between model complexity and
predicting accuracy in different scenarios. Fig. 1c illustrates
an example of a desired structure in the weight matrix. In
each row, the signiÔ¨Åcant values tend to gather in groups.
When translated into neural network topology, this enables
the advanced weight-level structure: The surviving weights of
each neuron tend to gather in groups. In each column, the sig-
niÔ¨Åcant values also tend to gather in groups. When translated
into neural network topology, this enables the neuron-level
structure: The surviving neurons in each layer tend to gather
in groups. Moreover, the group size in each row and column
can be tuned to achieve a trade off between model complexity
and predicting accuracy. Such proposed multi-level structures
enable the datapath to exploit the compression to simplify the
computation with very small control overheads. SpeciÔ¨Åcally,
the advantages of our proposed multi-level structure are listed
as follows.
1)High decoding and computing parallelism: As illustrated
in Fig. 2 (a), with the blocked structure, the decoding
can be performed in block wise such that all the weights
in the same block can be decoded simultaneously. More-
over, the regular structure can bring more parallelism in
computation. For example, more efÔ¨Åcient matrix tiling
can be applied, where the zero blocks can be skipped
during matrix multiplication, as illustrated in Fig. 2 (b).
Also, the kernels can be compressed into smaller size,
such that the inputs corresponding to the zero weights
can be skipped [20].
2)EfÔ¨Åcient neuron pruning: As illustrated in Fig. 2 (c), the
structured surviving weights from the previous layer can
lead to a structured and compact neuron activation in the
next layer, which is beneÔ¨Åcial for more efÔ¨Åcient neuron
pruning.
3)More efÔ¨Åcient storage: With the blocked structure, we
can simply record the location and size for each block
rather than recording the exact location for each weight.
For a weight matrix with average cluster size of mm
and number of clusters P, the coding gain over tradi-
tional COO format is O 
Pm2
.
In this paper, we focus on promoting a multi-level group
structure in model compression. In our proposed structure,
the surviving weights connected to a neuron tend to gather in
groups and the surviving neurons also tend to gather in groups.
Additionally, the group size is tunable so that our proposed
structure is Ô¨Çexible.
C. Three-Layer Hierarchical Prior Model
The probability model of the sparse prior provides the foun-
dation of speciÔ¨Åc sparse structures. As previously mentioned,existing sparse priors for model compression cannot promote
the multi-level structure in the weight matrix. To capture
the multi-level structured sparsity, we propose a three-layer
hierarchical prior by combining the two-layer prior and the
support based prior [18].
Letwdenote a weight in the neural network. For each
weightw, we introduce a support s2 f0;1gto indicate
whether the weight wis active (s= 1) or inactive (s= 0) .
SpeciÔ¨Åcally, let denote the precision for the weight w. That
is,1=is the variance of w. Whens= 0, the distribution
ofis chosen to satisfy E[]1, so that the variance of
the corresponding weight wis very small. When s= 1, the
distribution of is chosen to satisfy E[] =O(1), so that
whas some probability to take signiÔ¨Åcant values. Then, the
three-layer hierarchical prior (joint distribution of w;;s) is
given by
p(w;;s) =p(s)p(js)p(wj); (1)
where wdenotes all the weights in the neural network, 
denotes the corresponding precisions, and sdenotes the corre-
sponding supports. The distribution of each layer is elaborated
below.
Probability Model for p(s):p(s)can be decomposed into
each layer by p(s) =QL
l=1p(sl), whereLis the total layer
number of the neural network and sldenotes the supports
of the weights in the l-th layer. Now we focus on the l-th
layer. Suppose the dimension of the weight matrix is KM,
i.e., the layer has Kinput neurons and Moutput neurons.
The distribution p(sl)of the supports is used to capture the
multi-level structure in this layer. Since we focus on a speciÔ¨Åc
layer now, we use p(s)to replacep(sl)in the following
discussion for notation simplicity. In order to promote the
aforementioned multi-level structure, the active elements in
each row of the weight matrix should gather in clusters and
the active elements in each column of the weight matrix should
also gather in clusters. Such a block structure can be achieved
by modeling the support matrix as a Markov random Ô¨Åeld
(MRF). SpeciÔ¨Åcally, each row of the support matrix is modeled
by a Markov chain as
p(srow) =p(srow;1)MY
m=1p(srow;m +1jsrow;m ); (2)
where srowdenotes the row vector of the support matrix, and
srow;m denotes them-th element in srow. The transition prob-
ability is given by p(srow;m +1= 1jsrow;m = 0) =prow
01and
p(srow;m +1= 0jsrow;m = 1) =prow
10. Generally, a smaller
prow
01leads to a larger average gap between two clusters, and a
smallerprow
10leads to a larger average cluster size. Similarly,
each column of the support matrix is also modeled by a
Markov chain as
p(scol) =p(scol;1)KY
k=1p(scol;k +1jscol;k); (3)
where scoldenotes the column vector of the support matrix
andscol;k denotes the n-th element in scol. The transition
probability is given by p(scol;k +1= 1jscol;k= 0) =pcol
01and
p(scol;k +1= 0jscol;k= 1) =pcol
10. Such an MRF model is also

--- PAGE 5 ---
JOURNAL OF XXX 5
Fig. 2: (a) Illustration of decoding parallelism. In existing sparse structures, the decoding step for each row can be vastly
different because of the irregular structure. In our proposed structure, each block can be decoded simultaneously because
each block can be coded as a whole. (b) Illustration of computing parallelism. In existing sparse structures, the matrix-matrix
multiplication has to be performed element-wise because each non-zero weight is individually encoded. The processor has to
fetch individual weights and multiply them to individual input elements to get the individual output element (green square). In
our proposed structure, parallel techinics for dense matrix such as matrix tiling can be applied. Moreover, due to the clustered
structure, the zero block (gray block) can be skipped during computing the output (green block). (c) Illustration of efÔ¨Åcient
neuron pruning. In existing sparse structures, the surviving weights of each neuron are randomly connected to the neurons in
the next layer, which will randomly activate neurons in the next layer. In our proposed structure, the surviving weights of each
neuron tend to activate the same neurons in the next layer, which can lead to a structured and compact neuron activation in
the next layer.
Fig. 3: Illustration of the support prior for a 66weight
matrix.
known as a 2D Ising model. An illustration of the MRF prior
is given in Fig. 3. Note that p(s)can also be modeled with
other types of priors, such as Markov tree prior and hidden
Markov prior, to promote other structures.
Probability Model for p(js):The conditional probability
p(js)is given by
p(js) =NY
n=1(  (n;an;bn))sn 
  
n;an;bn1 sn;(4)
whereNis the total number of weights in the neural network.
ntakes two different Gamma distributions according to the
value ofsn. Whensn= 1, the corresponding weight wnisactive. In this case, the shape parameter anand rate parameter
bnshould satisfy thatan
bn=E[n] =O(1)such that the
variance of wnisO(1). Whensn= 0, the corresponding
weightwnis inactive. In this case, the shape parameter anand
rate parameter bnshould satisfy thatan
bn=E[n]1such
that the variance of wnis very close to zero. The motivation
for choosing Gamma distribution is that Gamma distribution
is conjugate to Gaussian distribution. Thus, it can leads to a
closed form solution in Bayesian inference [17].
Probability Model for p(wj):The conditional probability
p(wj)is given by
p(wj) =NY
n=1p(wnjn); (5)
wherep(wnjn)is assumed to be a Gaussian distribution:
p(wnjn) =N
wnj0;1
n
: (6)
Although the choice of p(wnjn)is not restricted to Gaussian
distribution [16], the motivation for choosing Gaussian is still
reasonable. First, according to existing simulations, the com-
pression performance is not so sensitive to the distribution type
ofp(wnjn)[16], [15]. Moreover, assuming p(wnjn)to be
a Gaussian distribution also contributes to easier optimization
in Bayesian inference, as shown in Section IV.
Remark 1. (Comparison with existing sparse priors) One
of the main differences between our work and the existing
works [15], [16] is the design of the prior. Because of the
MRF support layer, our proposed three-layer prior is more
general and can capture more regular structures such as the

--- PAGE 6 ---
JOURNAL OF XXX 6
desired multi-level structure. Such a multi-level structure in
the weight matrix cannot be modeled by the existing priors in
[15] and [16] but our proposed prior can easily support the
sparse structures in them. In our work, if the prior p(w;;s)
reduces to one layer p(w)and takes a Laplace distribution,
it is equivalent to a Lasso regularization. In this way, random
sparsity can be realized. If the prior p(w;;s)reduces to
one layerp(w)and takes an improper log-scale uniform
distribution, it is exactly the problem of variational dropout
[15]. In this way, random sparsity can be realized. If the
priorp(w;;s)reduces to a two-layer p(wj), and takes
the group improper log-uniform distribution or the group
horseshoe distribution, it is exactly the problem in [16]. In
this way, group sparsity can be realized. Moreover, since our
proposed prior is more complicated, it also leads to a different
algorithm design in Section IV.
III. B AYESIAN MODEL COMPRESSION FORMULATION
Bayesian model compression handles the model compres-
sion problem from a Bayesian perspective. In Bayesian model
compression, the weights follow a prior distribution, and our
primary goal is to Ô¨Ånd the posterior distribution conditioned
on the dataset. In the following, we elaborate on the neural
network model and the Bayesian training of the DNN.
A. Neural Network Model
We Ô¨Årst introduce the neural network model. Let D=
f(x1;y1);(x2;y2);:::;(xD;yD)gdenote the dataset, which
containsDpairs of training input (xd)and training output
(yd). LetNN(x;w)denote the output of the neural network
with input xand weights w, whereNN()is the neural
network function. Note that NN(x;w)is a generic neural
network model, which can embrace various commonly used
structures, some examples are listed as follows.
Multi-Layer Perceptron (MLP): MLP is a representa-
tive class of feedforward neural network, which consists
of an input layer, output layer and hidden layers. As
illustrated in Fig. 4a, we can use NN(x;w)to represent
the feedforward calculation in an MLP.
Convolutional Neural Network (CNN): A CNN con-
sists of convolutional kernels and dense layers. As illus-
trated in Fig. 4b, we can use NN(x;w)to represent the
complicated calculation in a CNN.
Since the data points from the training set are generally
assumed to be independent, we can use the following stochas-
tic model to describe the observations from a general neural
network:
yp(Djw) =DY
d=1p(ydjxd;w); (7)
where the likelihood p(ydjxd;w)can be different for regres-
sion and classiÔ¨Åcation tasks [26]. In regression tasks, it can be
modeled as a Gaussian distribution:
p(ydjxd;w) =N 
NN(xd;w);2
d
; (8)
(a) Illustration of NN(x;w)in MLP.
(b) Illustration of NN(x;w)in CNN.
Fig. 4: Illustration of NN(x;w)in different neural networks.
where2
dis the noise variance in training data, while in
classiÔ¨Åcation tasks, it can be modeled as
p(ydjxd;w) = exp ( G(ydjxd;w)); (9)
whereG(ydjxd;w)is the cross-entropy error function.
B. Bayesian Training of DNNs
Bayesian training of a DNN is actually calculating the pos-
terior distribution p(w;;sjD)based on the prior distribution
p(w;;s)and the neural network stochastic model (7). In our
research, after we obtain the posterior p(w;;sjD), we use
the maximum a posteriori (MAP) estimate of w;andsas a
deterministic estimate of the weights, precision and support,
respectively:
(w;;s)= arg max
w;;sp(w;;sjD): (10)
According to Bayesian rule, the posterior distribution of
w;andscan be derived as
p(w;;sjD) =p(Djw;;s)p(w;;s)
p(D); (11)
wherep(Djw;;s)is the likelihood term. Based on (7) and
(8), it is given by:
p(Djw;;s) =DY
d=1p(ydjxd;w;;s); (12)
p(ydjxd;w;;s) =N 
NN(xd;w;;s);2
d
: (13)
Equation (11) mainly contains two terms: 1) Likelihood
p(Djw;;s), which is related to the expression of dataset.
2) Priorp(w;;s), which is related to the sparse structure
we want to promote. Intuitively, by Bayesian training of the
DNN, we are actually simultaneously 1) tuning the weights to
express the dataset and 2) tuning the weights to promote the
structure captured in the prior.
However, directly computing the posterior according to (11)
involves several challenges, summarized as follows.

--- PAGE 7 ---
JOURNAL OF XXX 7
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶1y
2y
Dy1g
2g
Dg1w
2w
3w
4w
2Nw‚àí
1Nw‚àí
Nw1f
2f
3f
4f
2Nf‚àí
1Nf‚àí
Nf1œÅ
2œÅ
3œÅ
4œÅ
2NœÅ‚àí
1NœÅ‚àí
NœÅ1Œ∑
2Œ∑
3Œ∑
4Œ∑
2NŒ∑‚àí
1NŒ∑‚àí
NŒ∑1s
2s
3s
4s
2Ns‚àí
1Ns‚àí
Nsh1x
2x
Dx
()| p wÔÅÑ ()| pwœÅ ()|pœÅs ()ps
Fig. 5: Factor graph of the joint distribution p(w;;s;D).
Intractable multidimensional integral: The calculation
of the exact posterior p(w;;sjD)involves calculating
the so-called evidence term p(D). The calculation of
p(D) =P
sRR
p(Djw;;s)p(w;;s)dwdis usu-
ally intractable as the likelihood term can be very com-
plicated [26].
Complicated prior term: Since the prior term p(w;;s)
contains an extra support layer p(s), the KL-divergence
between the approximate distribution and the posterior
distribution is difÔ¨Åcult to calculate or approximate. Tra-
ditional VBI method cannot be directly applied to achieve
an approximate posterior distribution.
In the next section, we propose a Turbo-VBI based model
compression method, which approximately calculates the
marginal posterior distribution of w;ands.
IV. T URBO -VBI A LGORITHM FOR MODEL COMPRESSION
To handle the aforementioned two challenges, we pro-
pose a Turbo-VBI [17] based model compression algorithm.
The basic idea of the proposed Turbo-VBI algorithm is to
approximate the intractable posterior p(w;;sjD)with a
variational distribution q(w;;s). The factor graph of the
joint distribution p(w;;s;D)is illustrated in Fig. 5, where
the variable nodes are denoted by white circles and the factor
nodes are denoted by black squares. SpeciÔ¨Åcally, the factor
graph is derived based on the factorization
p(w;;s;D) =p(Djw)p(wj)p(js)p(s): (14)
As illustrated in Fig. 5, the variable nodes are w;and
s,gdenotes the likelihood function, fdenotes the prior
distribution p(wnjn)for weights w,denotes the prior
distribution p(njsn)for precision, andhdenotes the joint
prior distribution p(s)for support s. The detailed expression
of each factor node is listed in Table I.
A. Top Level Modules of the Turbo-VBI Based Model Com-
pression Algorithm
Since the factor graph in Fig. 5 has loops, directly applying
the message passing algorithm usually cannot achieve a good
performance. In addition, since the probability model is more
complicated than the existing ones in [15] and [16], it is
difÔ¨Åcult to directly apply the VBI algorithm. Thus, we consider
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
1y
2y
Dy
1g
2g
Dg
1w
2w
3w
4w
2Nw‚àí
1Nw‚àí
Nw
1f
2f
3f
4f
2Nf‚àí
1Nf‚àí
Nf
1ÔÅ≤
2ÔÅ≤
3ÔÅ≤
4ÔÅ≤
2NÔÅ≤‚àí
1NÔÅ≤‚àí
NÔÅ≤
1ÔÅ®
2ÔÅ®
3ÔÅ®
4ÔÅ®
2NÔÅ®‚àí
1NÔÅ®‚àí
NÔÅ®
1s
2s
3s
4s
2Ns‚àí
1Ns‚àí
Ns
h
1x
2x
Dx‚Ä¶
1s
2s
3s
4s
2Ns‚àí
1Ns‚àí
Ns
,1Ah
,2Ah
,3Ah
,4Ah
,2ANh‚àí
,1ANh‚àí
,ANh
,BNh
,1BNh‚àí
,2BNh‚àí
,4Bh
,3Bh
,2Bh
,1Bh
Part A Part B
Variational 
Bayesian 
inferenceSupport 
estimator
()qw
()qœÅ
ÔÅªÔÅΩddsvÔÅ®‚Üí
ext
ÔÅªÔÅΩd hsv‚Üí
Module A Module BFig. 6: Illustration of the two modules in the Turbo-VBI
algorithm.
separating the complicated probability model in Fig. 5 into
two parts, and perform Bayesian inference respectively, as
illustrated in Fig. 6. SpeciÔ¨Åcally, we follow the turbo frame-
work and divide the factor graph into two parts. One is the
support part (Part B), which contains the prior information.
The other one is the remaining part (Part A). Correspondingly,
the proposed Turbo-VBI algorithm is also divided into two
modules such that each module performs Bayesian inference
on its corresponding part respectively. Module A and Module
B also need to exchange messages. SpeciÔ¨Åcally, the output
messagevh!snof Module B refers to the message from factor
nodehA;nto variable node snin Part A, and is equivalent to
the message from variable node snto factor node hB;n in
Part B, which refers to the conditional marginal probability
p(snjv!s). It is calculated by sum-product message passing
(SPMP) on part B, and acts as the input of Module A to
facilitate the VBI on Part A. The output message vn!snof
Module A refers to the posterior probability subtracting the
output of Module B:q(sn)
vh!sn, and acts as the input of Module
B to facilitate SPMP on Part B.
SpeciÔ¨Åcally, the probability model for the prior distribution
in Part A is assumed as
^p(w;;s) = ^p(s)p(js)p(wj); (15)
where
^p(s) =NY
n=1(n)sn(1 n)1 sn
is a new prior for support s.nis the probability that sn= 1,
which is deÔ¨Åned as:
n=p(sn= 1) =vh!sn(1)
vh!sn(1) +vh!sn(0): (16)
Note that the only difference between the new prior in (15) and
that in (1) is that the complicated p(s)is replaced by a simpler
distribution ^p(s)with independent entries. The correlations
among the supports are separated into Part B. Then, based on
the new prior ^p(w;;s), Module A performs a VBI algorithm.

--- PAGE 8 ---
JOURNAL OF XXX 8
TABLE I: Detailed expression of each factor node.
Factor node Distribution Function
gd(xd;yd;w)p(ydjxd;w) N 
NN(xd;w);2
d
fn(wn;n)p(wnjn) N
wnj0;1
n
n(n;sn)p(njsn) (  (n;an;bn))sn
 
n;an;bn1 sn
h p(s) MRF prior.p(srow;m +1jsrow;m );p 
scol;k +1jscol;k
In the VBI algorithm, variational distributions q(w),q()
andq(s)are used to approximate the posterior. After that, the
approximate posterior q(s)is delivered from Module A back
into Module B. The delivered message vn!snis deÔ¨Åned as
vn!sn=q(sn)
vh!sn; (17)
according to the sumproduct rule.
With the input message vn!sn, Module B further performs
SPMP over Part B to exploit the structured sparsity, which is
contained in the prior distribution p(s):SpeciÔ¨Åcally, in Part
B, the factor nodes
hB;n=vn!sn;n= 1;:::;N (18)
carry the information of the variational posterior distribution
q(s), and the factor node hcarries the structured prior
information of p(s). By performing SPMP over Part B, the
messagevh!sncan be calculated and then delivered to Mod-
ule A. These two modules exchange messages iteratively until
convergence or the maximum iteration number is exceeded.
After this, the Ô¨Ånal outputs q(w),q()andq(s)of Module
A are the approximate posterior distribution for w,;ands,
respectively. Note that although the SPMP is not guaranteed
to converge on Part B, because our p(s)is an MRF prior, the
desired structure is usually well promoted in the Ô¨Ånal output
of Module A, as illustrated in the simulation results. This is
because SPMP can achieve good results on 2-D Ising model
and there have been many solid works concerning SPMP
on loopy graphs [27]. And our proposed VBI compression
algorithm in Module A can achieve good enough performance
to capture the structure. In addition, since the design of p(s)is
Ô¨Çexible, one can model it with Markov chain priors or Markov
tree priors. In this case, the convergence is guaranteed. In the
following, we elaborate the two modules in details.
B. Sparse VBI Estimator (Module A)
In Module A, we want to use a tractable variational dis-
tributionq(w;;s)to approximate the intractable posterior
distribution ^p(w;;sjD)under the prior ^p(w;;s). The
quality of this approximation is measured by the Kullback-
Leibler divergence (KL divergence):
DKL(q(w;;s)jj^p(w;;sjD))
=X
sZ Z
q(w;;s) lnq(w;;s)
^p(w;;sjD)dwd:(19)
Thus, the goal is to Ô¨Ånd the optimal variational distribution
q(w;;s)that minimizes this KL divergence. However, the
KL divergence still involves calculating the intractable pos-
terior ^p(w;;sjD). In order to overcome this challenge, theminimization of KL divergence is usually transformed into a
minimization of the negative evidence lower bound (ELBO),
subject to a factorized form constraint [28].
Sparse VBI Problem:
min
q(w;;s) ELBO;
s:t:q(w;;s) =NY
n=1q(wn)q(n)q(sn);(20)
whereELBO =P
sRR
q(w;;s) lnp(Djw;;s)dwd 
DKL(q(w;;s)jj^p(w;;s)):The constraint means all indi-
vidual variables w; andsare assumed to be independent.
Such an assumption is known as the mean Ô¨Åeld assumption
and is widely used in VBI methods [16], [28].
The problem in (20) is non-convex and generally we can
only achieve a stationary solution q(w;;s). A stationary
solution can be achieved by applying a block coordinate
descent (BCD) algorithm to the problem in (20), as will
be proved in Lemma 1. According to the idea of the BCD
algorithm, to solve (20) is equivalent to iteratively solving the
following three subproblems.
Subproblem 1 (update of ):
min
q()DKL(q()jjep);
s:t:q() =NY
n=1q(n);(21)
where lnep=hln ^p(w;;s;D)iq(s)q(w):
Subproblem 2 (update of s):
min
q(s)DKL(q(s)jjeps);
s:t:q(s) =NY
n=1q(sn);(22)
where lneps=hln ^p(w;;s;D)iq()q(w):
Subproblem 3 (update of w):
min
q(w)DKL(q(w)jjepw);
s:t:q(w) =NY
n=1q(wn);(23)
where lnepw=hln ^p(w;;s;D)iq()q(s)andhf(x)iq(x)=R
f(x)q(x)dx:The detailed derivations of the three subprob-
lems are provided in the Appendix A. In the following, we
elaborate on solving the three subproblems respectively.

--- PAGE 9 ---
JOURNAL OF XXX 9
1) Update of q():In subproblem 1, obviously, the optimal
solutionq?()is achieved when q?() =ep. In this case,
q?()can be derived as
q?() =NY
n=1 
n;ean;ebn
; (24)
whereeanandebnare given by:
ean=hsnian+h1 snian+ 1
=enan+ (1 en)an+ 1;(25)
ebn=
jwnj2
+hsnibn+h1 snibn
=jnj2+2
n+enbn+ (1 en)bn;(26)
wherenand2
nare the posterior mean and variance of
weightwnrespectively, and enis the posterior expectation
ofsn.
2) Update of q(s):In subproblem 2, we can achieve the
optimal solution q?(s)by lettingq?(s) =eps. In this case,
q?(s)can be derived as
q?(s) =NY
n=1(en)sn(1 en)1 sn; (27)
whereen=C1
C1+C2: C1andC2are given by:
C1=nbann
  (an)e(an 1)hlnni bnhni; (28)
C2=(1 n)ban
n
  (an)e(an 1)hlnni bnhni; (29)
wherehlnni= (ean) ln
ebn
, (x) =d
dxln (  (x))is
the digamma function. The detailed derivation of q?()and
q?(s)is provided in the Appendix B.
3) Update of q(w):Expanding the objective function in
subproblem 3, we have
DKL(q(w)jjepw)
=Eq(w)lnq(w) Eq(w)h
hlnp(w;;s;D)iq()q(s)i
=Eq(w)
lnq(w) hlnp(wj)iq()
 Eq(w)lnp(Djw) +const:
(30)
Similar to traditional variational Bayesian model compression
[16], [15], the objective function (30) contains two parts:
the ‚Äúprior part‚Äù Eq(w)
lnq(w) hlnp(wj)iq()
and the
‚Äúdata part‚Äù Eq(w)lnp(Djw). The prior part forces the weights
to follow the prior distribution while the data part forces
the weights to express the dataset. The only difference from
traditional variational Bayesian model compression [16], [15]
is that the prior term hlnp(wj)iq()is parameterized by
andcarries the structure captured by the three layer
hierarchical prior. Our aim is to Ô¨Ånd the optimal q(w)
that minimizes DKL(q(w)jjepw), subject to the factorized
constraintq(w) =QN
n=1q(wn). It can be observed that the
objective function contains the likelihood term lnp(Djw),
which can be very complicated for complex models. Thus,
subproblem 3 is non-convex and it is difÔ¨Åcult to obtain
the optimal solution. In the following, we aim at Ô¨Ånding astationary solution q(w)for subproblem 3. There are several
challenges in solving subproblem 3, summarized as follows.
Challenge 1: Closed form for the prior part
Eq(w)
lnq(w) hlnp(wj)iq()
.In traditional varia-
tional Bayesian model compression, this expectation is
usually calculated by approximation [15], which involves
approximating error. To calculate the expectation accurately,
a closed form for this expectation is required.
Challenge 2: Low-complexity deterministic approxima-
tion for the data part Eq(w)lnp(Djw).In traditional
variational Bayesian model compression [16], [15], this
intractable expectation is usually approximated by Monte
Carlo sampling. However, Monte Carlo approximation has
been pointed out to have high complexity and low robust-
ness. In addition, the variance of the Monte Carlo gradient
estimates is difÔ¨Åcult to control [19]. Thus, a low-complexity
deterministic approximation for the likelihood term is re-
quired.
Closed form for Eq(w)
lnq(w) hlnp(wj)iq()
.To
overcome Challenge 1, we propose to use a Gaussian dis-
tribution as the approximate posterior. That is, q(wn) =
N 
n;2
n
,q(w) =QN
n=1q(wn),nand2
nare the
variational parameters that we want to optimize. Then, since
the priorp(wj)is also chosen as a Gaussian distribution,
Eq(w)
lnq(w) hlnp(wj)iq()
can be written as the KL-
divergence between two Gaussian distributions. Thus, the
expectation has a closed form and can be calculated up to
a constant. SpeciÔ¨Åcally, by expanding p(wj), we have
hlnp(wj)i=NX
n=1
lnrn
2e w2
n
2n
=NX
n=11
2lnn w2
n
2n+const:
=NX
n=11
2hlnni w2
n
2hni+const:
=NX
n=1(lnp(wnjhni) +const: )
= lnp(wjhi) +const:;(31)
where we usehito replacehiq()orhiq()for simplicity.
Thus, we have
Eq(w)
lnq(w) hlnp(wj)iq()
=Eq(w)(lnq(w) lnp(wjE[])) +const:
=DKL(q(w)jjp(wjE[])) +const:: (32)
Sinceq(w)andp(wjE[])are both Gaussian distributions,
the KL-divergence between them has a closed form:
DKL(q(w)jjp(wjE[])) =NX
n=1
lnen
n+2
n+2
n
2e2n 1
2
;
(33)

--- PAGE 10 ---
JOURNAL OF XXX 10
wheree2
n=1
E[n]is the variance of the prior p(wjE[]).
Since our aim is to minimize (30), the constant term in (32)
can be omitted.
Low-complexity deterministic approximation for
Eq(w)lnp(Djw).To overcome Challenge 2, we propose
a low-complexity deterministic approximation for
Eq(w)lnp(Djw)based on Taylor expansion.
For simplicity, let g(w)denote the complicated function
lnp(Djw). SpeciÔ¨Åcally, we want to construct a deterministic
approximation for Ewq(w)g(w)with the variational parame-
tersnand2
n,n= 1;:::;N . Following the reparameterization
trick in [15] and [29], we represent wnbyn+n2
n,
nN (0;1). In this way, wnis transformed into a determin-
istic differentiable function of a non-parametric noise n, and
Ewq(w)g(w)is transformed into EN(0;1)g 
+2
.
Note that in traditional variational Bayesian model com-
pression, this expectation is approximated by sampling .
However, to construct a deterministic approximation, we take
the Ô¨Årst-order Taylor approximation of EN(0;1)g 
+2
at the point=E[] =0, and we have
Eg 
+2
g 
+E[]2
=g(): (34)
That is, we use the Ô¨Årst-order Taylor approximation to replace
Eg 
+2
. Simulation results show that our proposed
approximation can achieve good performance. Additionally,
since our proposed approximation is deterministic, the gradient
variance can be eliminated.
Based on the aforementioned discussion , subproblem 3 can
be solved by solving an approximated problem as follows.
Approx. Subproblem 3:
min
;2NX
n=1
lnen
n+2
n+2
n
2e2n 1
2
 lnp(Dj): (35)
This problem is equivalent to training a Bayesian neural
network with (35) as the loss function.
4) Convergence of Sparse VBI: The sparse VBI in Module
A is actually a BCD algorithm to solve the problem in (20).
By the physical meaning of ELBO and KL divergence, the
objective function is continuous on a compact level set. In
addition, the objective function has a unique minimum for
two coordinate blocks ( q()andq(s)) [17], [28]. Since
we can achieve a stationary point for the third coordinate
blockq(w), the objective function is regular at the cluster
points generated by the BCD algorithm. Based on the above
discussion, our proposed BCD algorithm satisÔ¨Åes the BCD
algorithm convergence requirements in [30]. Thus, we have
the following convergence lemma.
Lemma 1. (Convergence of Sparse VBI): Every cluster point
q(w;;s) =q?()q?(s)q(w)generated by the sparse
VBI algorithm is a stationary solution of problem (20).
C. Message Passing (Module B)
In Module B, SPMP is performed on the support factor
graphGsand the normalized message vsn!hBis fed back to
Module A as the prior probability ^p(sn).
We focus on one fully connected layer with Kinput neurons
andMoutput neurons for easy elaboration. The support prior
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶‚Ä¶‚Ä¶
1,1s
1,2s
1,Ms
2,1s
2,2s
2,Ms
,1Ks
,2Ks
,KMs
,1,1Bh
,2,1Bh
, ,1BKh
,1,2Bh
,2,2Bh
, ,2BKh
,1,BMh
,2,BMh
,,B K Mh
Support factor graph      of a layer with K 
input neuron and M output neuron
01 10,col colpp
01 10,col colpp
01 10,row rowpp
01 10,row rowpp
s
Fig. 7: Illustration of the support factor graph in Module B
withKinput neurons and Moutput neurons.
Algorithm 1 Turbo-VBI Algorithm for Model Compression
Input: training setD, priorsp(w),p(),p(s), maximum
iteration number Imax.
Output: w,,s.
1:Initializen.
2:fork= 1;:::;Imdo
3: Module A:
4: Initialize the variational distribution q()andq(s).
5: while not converged do
6: Updateq(),q(s)andq(w).
7: end while
8: Calculatevn!snbased on (17) and sendvn!snto
Module B.
9: Module B:
10: Perform SPMP on Gs, sendvh!snto Module A.
11: ifconverge then
12: break
13: end if
14:k=k+ 1.
15:end for
16:Output w= arg max wq(w),= arg max q?(),
s= arg max sq?(s).
p(s)for thisKMlayer is modeled as an MRF. The factor
graphGsis illustrated in Fig. 7. Here we use si;jto denote
the support variable in the i-th row and j-th column. The
unary factor node hB;i;j carries the probability given by (18).
The pair-wise factor nodes carry the row and column tran-
sition probability p(srow;m +1jsrow;m )andp(scol;k +1jscol;k),
respectively. The parameters prow
01;prow
10andpcol
01;pcol
10for the
MRF model are given in Section II-C. Let vs!f(s)denote
the message from the variable node sto the factor node f,
andvf!s(s)denote the message from the factor node fto
the variable node s. According to the sum-product law, the

--- PAGE 11 ---
JOURNAL OF XXX 11
messages are updated as follows:
vs!f(s) =Y
h2n(s)nffgvh!s(s); (36)
vf!s(s) =(
f(s) f isunaryfactornode;
f(s)vt!f(t)f ispairwisefactornode;
(37)
wheren(s)nffgdenotes the neighbour factor nodes of s
except node f, andtdenotes the other neighbour variable node
offexcept node s. After the SPMP algorithm is performed,
the Ô¨Ånal message vs!hBfrom each variable node si;jto the
connected unary factor node hB;i;j is sent back to Module A.
The overall algorithm is summarized in Algorithm 1.
Remark 2. (Comparison with traditional variational in-
ference) Traditional variational inference cannot handle the
proposed three-layer sparse prior p(w;;s)while our pro-
posed Turbo-VBI is specially designed to handle it. In existing
Bayesian compression works [14]-[16], the KL-divergence be-
tween the true posterior p(wjD)and the variational posterior
q(w)is easy to calculate, so that the KL-divergence can be
directly optimized. Thus, simple variational inference can be
directly applied. The fundamental reason behind this is that
the priorp(w)in these works are relatively simple, and they
usually only have one layer [14], [15] or two layers [16]. Thus,
the KL-divergence between the prior p(w)and the variational
posteriorq(w)is easy to calculate, which is a must in calculat-
ing the KL-divergence between the true posterior p(wjD)and
the variational posterior q(w). However, these simple priors
cannot promote the desired regular structure in the weight
matrix and lack the Ô¨Çexibility to conÔ¨Ågure the pruned structure.
Thus, we introduce the extra support layer s, which leads to
the three layer hierarchical prior p(w;;s). With this three-
layer prior, traditional variational inference cannot be applied
because the KL-divergence between the prior p(w;;s)and
the variational posterior q(w;;s)is difÔ¨Åcult to calculate. In
order to inference the posterior, we propose to separate the
factor graph into two parts and iteratively perform Bayesian
inference, which leads to our proposed Turbo-VBI algorithm.
V. P ERFORMANCE ANALYSIS
In this section, we evaluate the performance of our proposed
Turbo-VBI based model compression method on some stan-
dard neural network models and datasets. We also compare
with several baselines, including classic and state-of-the-art
model compression methods. The baselines considered are
listed as follows:
1)Variational dropout (VD): This is a very classic method
in the area of Bayesian model compression, which is
proposed in [15]. A single-layer improper log-uniform
prior is assigned to each weight to induce sparsity during
training.
2)Group variational dropout (GVD): This is proposed in
[16], which can be regarded as an extension of VD into
group pruning. In this algorithm, each weight follows
a zero-mean Gaussian prior. The prior scales of the
weights in the same group jointly follow an improperlog-uniform distribution or a proper half-Cauchy distri-
bution to realize group pruning. In this paper, we choose
the improper log-uniform prior for comparison. Note
that some more recent Bayesian compression methods
mainly focus on a quantization perspective [14], [31],
which is a different scope from our work. Although
our work can be further extended into quantization
area, we only focus on structured pruning here. To
our best knowledge, GVD is still one of the state-of-
the-art Bayesian pruning methods without considering
quantization.
3)Polarization regularizer based pruning (polarization):
This is proposed in [9]. The algorithm can push some
groups of weights to zero while others to larger values
by assigning a polarization regularizer to the batch norm
scaling factor of each weight group
min
w1
DDX
d=1L(NN(xd;w);yd) +R(w)
+(tkk1 k 1nk1);
where(tkk1 k 1nk1)is the proposed polar-
ization regularizer and is the batch norm scaling factor
of each weight.
4)Single-shot network pruning (SNIP): This is proposed
in [1]. The algorithm measures the weight importance
by introducing an auxiliary variable to each weight. The
importance of each weight is achieved before training
by calculating the gradient of its ‚Äúimportance variable‚Äù.
After this, the weights with less importance are pruned
before the normal training.
We perform experiments on LeNet-5, AlexNet, and VGG-11
architectures, and use the following benchmark datasets for
the performance comparison.
1) Fashion-MNIST [32]: It consists of a training set of
60000 examples and a test set of 10000 examples. Each
example is a 2828grayscale image of real life fashion
apparel, associated with a label from 10 classes.
2) CIFAR-10 [33]: It is the most widely used dataset to
compare neural network pruning methods. It consists of
a training set of 50000 examples and a test set of 10000
examples. Each example is a three-channel 3232RGB
image of real life object, associated with a label from
10 classes.
3) CIFAR-100 [33]: It is considered as a harder version of
CIFAR-10 dataset. Instead of 10 classes in CIFAR-10,
it consists of 100 classes with each class containing 600
images. There are 500 training images and 100 testing
images in each class.
We will compare our proposed Turbo-VBI based model com-
pression method with the baselines from different dimensions,
including 1) an overall performance comparison; 2) visual-
ization of the weight matrix structure; 3) CPU and GPU
acceleration and 4) robustness of the pruned neural network.
We run LeNet-5 on Fashion-MNIST, and AlexNet on
CIFAR-10 and VGG on CIFAR-100. Throughout the experi-
ments, we set the learning rate as 0.01, and a=b=a= 1;b=
110 3. For LeNet-5+Fashion-MNIST, we set the batch size

--- PAGE 12 ---
JOURNAL OF XXX 12
as 64, and for AlexNet+CIFAR-10 and VGG+CIFAR-100, we
set the batchsize as 128 . All the methods except for SNIP go
through a Ô¨Åne tuning after pruning. For LeNet, the Ô¨Åne tuning
is 15 epochs while for AlexNet and VGG, the Ô¨Åne tuning
is 30 epochs. Note that the models and parameter settings
may not achieve state-of-the-art accuracy but it is enough for
comparison.
A. Overall Performance Comparison
We Ô¨Årst give an overall performance comparison of different
methods in terms of accuracy, sparsity rate, pruned structure
and FLOPs reduction. The results are summarized in Table II.
The structure is measured by output channels for convolutional
layers and neurons for fully connected layers. It can be ob-
served that our proposed method can achieve an extremely low
sparsity rate, which is at the same level (or even lower than)
the unstructured pruning methods (VD and SNIP). Moreover,
it can also achieve an even more compact pruned structure than
the structured pruning methods (polarization and GVD). At the
same time, our proposed method can maintain a competitive
accuracy. We summarize that the superior performance of
our proposed method comes from two aspects: 1) superior
individual weight pruning ability, which leads to low sparsity
rate, and 2) superior neuron pruning ability, which leads to
low FLOPs. Compared to random pruning methods like VD
and SNIP which generally can achieve lower sparsity rate
than group pruning methods, our proposed method can achieve
even lower sparsity rate. This shows that our proposed sparse
prior has high efÔ¨Åciency in pruning individual weights. This is
because the regularization ability of p(wj)can be conÔ¨Ågured
by setting small b. Compared to group pruning methods like
GVD and polarization, our proposed method can achieve a
more compact structure. This shows our proposed method is
also highly efÔ¨Åcient in pruning neurons. The reason is two
fold: First, each row in the prior p(s)can be viewed as a
Markov chain. When some weights of a neuron are pruned,
the other weights will also have a very high probability to
be pruned. Second, the surviving weights in a weight matrix
tend to gather in clusters, which means the surviving weights
of different neurons in one layer tend to activate the same
neurons in the next layer. This greatly improves the regularity
from the input side, while the existing group pruning methods
like GVD and polarization can only regularize the output side
of a neuron.Visualization of Pruned Structure
In this subsection, we visualize the pruned weight matrix
of our proposed method and compare to the baselines to
further elaborate the beneÔ¨Åts brought by the proposed regular
structure. Fig. 8 illustrates the 6 kernels in the Ô¨Årst layer of
LeNet. We choose GVD and polarization for comparison here
because these two are also structured pruning methods. It
can be observed that GVD and polarization can both prune
the entire kernels but the weights in the remaining kernels
cannot be pruned and have no structure at all. However, in
our proposed method, the weights in the remaining kernels are
clearly pruned in a structured manner. In particular, we Ô¨Ånd
that the three remaining 55kernels all Ô¨Åt in smaller kernels
Fig. 8: Kernel structures of CONV_1 in LeNet.
(a) Avg. inferencing time on CPU.
(b) Avg. inferencing time on GPU.
Fig. 9: Avg. time costed by one forward pass of a batch
on CPU and GPU. For LeNet and AlexNet, the batch size
is 10000 examples and each results is averaged on 1000
experiments. For VGG, the batch size is 1000 examples and
each result is averaged on 100 experiments.
with size 33. This clearly illustrates the clustered structure
promoted by the MRF prior. With the clustered pruning, we
can directly replace the original large kernels with smaller
kernels, and enjoy the storage and computation beneÔ¨Åts of the
smaller kernel size.
B. Acceleration Performance
In this subsection, we compare the acceleration performance
of our proposed method to the baselines. Note that the primary
goal of the structured pruning is to reduce the computational
complexity and accelerate the inference of the neural networks.
In the weight matrix of our proposed method, we record the
clusters larger than 33as an entirety. That is, we record
the location and size of the clusters instead of recording the
exact location for each element. In Fig. 9, we plot the average
time costed by one forward pass of a batch on CPU and GPU
respectively. It can be observed that our proposed method

--- PAGE 13 ---
JOURNAL OF XXX 13
Network Method Accuracy (%)jw6=0j
jwj(%) Pruned structure FLOPs reduction (%)
No pruning 89.01 100 6-16-120-84 0
VD 88.91 6.16 6-15-49-51 11.82
LeNet-5 GVD 88.13 4.20 5-7-21-23 53.89
6-16-120-84 Polarization 87.27 17.96 4-8-46-27 59.04
SNIP 82.83 9.68 5-16-73-57 19.82
Proposed 88.77 1.14 3-5-16-17 76.03
(a) Results on LeNet+Fashion-MNIST.
Network Method Accuracy (%)jw6=0j
jwj(%) Pruned structure FLOPs reduction (%)
No pruning 66.18 100 6-16-32-64-128-120-84 0
VD 64.91 3.17 6-16-31-61-117-42-74 6.12
AlexNet GVD 64.82 9.30 6-16-27-31-21-17-14 39.29
6-16-32-64-128-120-84 Polarization 63.95 25.95 4-15-16-20-19-58-53 65.27
SNIP 63.19 11.75 6-16-30-57-102-56-68 12.51
Proposed 65.57 3.42 6-16-25-20-7-7-7 45.94
(b) Results on AlexNet+CIFAR-10.
Network Method Accuracy (%)jw6=0j
jwj(%) Pruned structure FLOPs reduction (%)
No pruning 68.28 100 64-128-256-256-512-
512-512-512-1000
VD 67.24 5.39 64-128-256-211-447-
411-293-279-4416.63
VGG-11 GVD 66.52 9.05 63-112-204-134-271-
174-91-77-3360.90
64-128-256-256-512-
512-512-512-100Polarization 64.41 26.74 62-122-195-148-
291-133-76-74-6859.54
SNIP 65.20 17.79 64-128-254-246-441-
479-371-274-697.92
Proposed 67.12 4.68 60-119-191-148-
181-176-44-64-5963.14
(c) Results on VGG11+CIFAR-100.
TABLE II: Overall performance comparisons to the baselines on different network models and datasets. Top-1 accuracy is
reported. The best results are bolded.
can achieve the best acceleration performance on different
models. For LeNet, our proposed method can achieve a 1:50
gain on CPU and 2:85gain on GPU. For AlexNet, the
results are 1:63and1:64. For VGG, the results are 2:19
and1:88. Note that the unstructured pruning methods show
little acceleration gain because their sparse structure is totally
random. We summarize the reason for our acceleration gain
from two aspects: 1) More efÔ¨Åcient neuron pruning. As shown
in subsection V-A, our proposed method can result in a more
compact model and thus leading to a larger FLOPs reduction.
2) More efÔ¨Åcient decoding of weight matrix. Existing methods
require to store the exact location for each unpruned weight,
no matter in CSR, CSC or COO format because the sparse
structure is irregular. However, in our proposed method, we
only need the location and size for each cluster to decode
all the unpruned weights in the cluster. Thus, the decoding
is much faster. Note that the performance of our proposed
method can be further improved if the structure is explored
during the matrix-matrix multiplication. For example, the
zero blocks can be skipped in matrix tiling, as discussed in
subsection V-A. However, this involves modiÔ¨Åcation in lower-
level CUDA codes so we do not study it here.
C. Insight Into the Proposed Algorithm
In this subection, we provide some insight into the proposed
method. First, Fig. 10 illustrates the convergence behavior of
our proposed method on different tasks. It can be observed that
Fig. 10: Convergence of proposed algorithm. The convergence
if measured by the change in message vh!sn.
generally our proposed method can converge in 15 iterations
in all the tasks. Second, we illustrate how the structure is
gradually captured by the prior information vh!snfrom
Module B and how the posterior in Module A is regularized
to this structure. As the clustered structure in kernels has been
illustrated in Fig. 8, here in Fig. 11, we visualize the message
vh!snandvn!snof the FC_3 layer in AlexNet. It can
be observed that in iteration 1, the message vn!snhas no
structure because the prior ^p(s)in Module A is randomly
initialized. During the iterations of the algorithm, vn!sn
from Module B gradually captures the structure from the MRF
prior and acts as a regularization in Module A, so that structure
is gradually promoted in vh!sn. Finally, the support matrix

--- PAGE 14 ---
JOURNAL OF XXX 14
unpruned
pruned
Iteration=1 Iteration=3 Iteration=7Iteration=10
Final weight matrixIteration=1 Iteration=3
unpruned
pruned
Iteration=1 Iteration=3 Iteration=7Iteration=10
Final weight matrixIteration=7
Fig. 11: Visualization of the message vh!snandvn!snfor the Dense_3 layer of AlexNet. vh!sncarries the prior information
of support from Module B and vn!sncarries the posterior information of support from Module A. It can be clearly observed
that the structured prior captured from Module B gradually regularizes the posterior. Eventually both vh!snandvn!sn
converge to a stable state.
0 0.05 0.1 0.15 0.2 0.25 0.3
 0.10.20.30.40.50.60.70.80.9AccuracySNIP (w/o. adv. fine tune)
VD (w/o. adv. fine tune)
polarization (w/o. adv. fine tune)
proposed (w/o. adv. fine tune)SNIP (w/. adv. fine tune)
VD (w/. adv. fine tune)
polarization (w/. adv. fine tune)
proposed (w/. adv. fine tune)
Sparsity rate: SNIP: 9.82%, VD: 8.50%, polarization: 18.29%, proposed: 4.75%
Fig. 12: Accuracy vs for LeNet model.
has a clustered structure and the weight matrix also has a
similar structure.
D. Robustness of Pruned Model
In this subsection, we evaluate the robustness of the pruned
model. The robustness of the pruned model is often neglected
in model pruning methods while it is of highly importance
[34], [35], [36]. We compare the model robustness of our
proposed method to the baselines on LeNet and Fashion-
MNIST dataset. We generate 10000 adversarial examples by
fast gradient sign method (FGSM) and pass them throughthe pruned models. In Fig. 12, we plot the model accuracy
under different . Both the accuracy with and without an
adversarial Ô¨Åne tuning is reported. It can be observed that all
methods show a rapid drop without Ô¨Åne tuning. SpeciÔ¨Åcally,
our proposed method shows an accuracy drop of 72.1% from
89.01% when = 0 to 24.86% when = 0:05, while
SNIP reports the largest drop of 73.4%. However, we also
observe that the structured pruning methods (proposed and
polarization) show stronger robustness than the unstructured
ones (VD and SNIP). We think the reasons why our proposed
method and polarization perform better may be different. For
polarization, the unpruned weights are signiÔ¨Åcantly more than
other baselines, which means the remaining important Ô¨Ålters
have signiÔ¨Åcantly more parameters. This allows the model to
have a strong ability to resist the noise. For our proposed
method, the possible reason is that it can enforce the weights to
gather in groups. Thus, the model can ‚Äúconcentrate‚Äù on some
speciÔ¨Åc features even when they are added by noise. It can
be observed that after an adversarial Ô¨Åne tuning, all methods
show an improvement. VD shows the largest improvement of
77.31% at= 0:1while our proposed method only shows a
moderate improvement of 56.26%. The possible reason behind
this is our proposed method is ‚Äútoo‚Äù sparse and the remaining
weights cannot learn the new features. Note that SNIP shows
the smallest improvement. The possible reason is that in
SNIP, the weights are pruned before training and the auxiliary
importance variables may not measure the weight importance
accurately, which results in some important weights being
pruned. Thus, the expressing ability of the model may be
harmed.
VI. C ONCLUSIONS AND FUTURE WORK
Conclusions
We considered the problem of model compression from a
Bayesian perspective. We Ô¨Årst proposed a three-layer hierar-

--- PAGE 15 ---
JOURNAL OF XXX 15
chical sparse prior in which an extra support layer is used
to capture the desired structure for the weights. Thus, the
proposed sparse prior can promote both per-neuron weight-
level structured sparsity and neuron-level structured sparsity.
Then, we derived a Turbo-VBI based Bayesian compression
algorithm for the resulting model compression problem. Our
proposed algorithm has low complexity and high robustness.
We further established the convergence of the sparse VBI
part in the Turbo-VBI algorithm. Simulation results show that
our proposed Turbo-VBI based model compression algorithm
can promote a more regular structure in the pruned neural
networks while achieving even better performance in terms of
compression rate and inferencing accuracy compared to the
baselines.
Future Work
With the proposed Turbo-VBI based structured model com-
pression method, we can promote more regular and more
Ô¨Çexible structures and achieve superior compressing perfor-
mance during neural network pruning. This also opens more
possibilities for future research, as listed below:
Communication EfÔ¨Åcient Federated Learning: Our proposed
method shows huge potential in a federated learning scenario.
First, it can promote a more regular structure in the weight
matrix such that the weight matrix has a lower entropy.
Thus, the regular structure can be further leveraged to reduce
communication overhead in federated learning. Second, the
two-module design of the Turbo-VBI algorithm naturally Ô¨Åts
in a federated learning setting. The server can promote a global
sparse structure by running Module B while the clients can
perform local Bayesian training by running Module A.
Joint Design of Quantization and Pruning: Existing works
[16], [14] have pointed out that quantization and pruning can
be achieved simultaneously by Bayesian model compression.
Thus, it is also possible to further compress the model by
combining quantization with our proposed method.
APPENDIX
A. Derivation of Three Subproblems
Here, we provide the derivation from the original problem
(20) to the three subproblems (21), (22) and (23). For expres-
sion simplicity, let z= [z1;z2;z3]denote all the variables
w;ands, where z1=w,z2=andz3=s. Then, the
original problem (20) can be equivalently written as
Sparse VBI Problem:
min
q(z) ELBO;
s:t:q(z) =3NY
n=1q(zn):(38)Then, by (15) in [28], we have
ELBO
=Z
q(z) ln ^p(Djz)dz DKL(q(z)jj^p(z))
=Z
q(z) ln^p(D;z)
q(z)dz
=Z3Y
i=1q(zi)"
ln ^p(D;z) 3X
i=1lnq(zi)#
dz
=Z3Y
i=1q(zi) ln ^p(D;z)3Y
i=1dzi
 3X
i=1Z 3Y
j=1q(zj) lnq(zi)dzi
=Z
q(zj)2
4ln ^p(D;z)Y
i6=jq(zi)dzi3
5dzj
 Z
q(zj) lnq(zj)dzj X
i6=jZ
q(zi) lnq(zi)dzi
=Z
q(zj) lnepjdzj Z
q(zj) lnq(zj)dzj
 X
i6=jZ
q(zi) lnq(zi)dzi
= DKL(q(zj)jjepj) X
i6=jZ
q(zi) lnq(zi)dzi;
where lnepj=R
ln ^p(D;z)Q
i6=jq(zi)dzi=h^p(D;z)ii6=j. It
is obvious that ELBO is minimized when DKL(q(zj)jjepj)
is minimized. Thus, the Sparse VBI Problem (38) can be
solved by iteratively minimizing DKL(q(zj)jjepj)forj=
1;2;3.
B. Derivation of (25) - (30)
Letq?() =ep, we have
lnq?()
/lnep
/hlnp(w;;s;D)iq(s)q(w)
/hlnp(wj)iq(w)+hlnp(js)iq(s)
/NX
n=1(hsnian+h1 snian) lnn
  
jwnj2
+hsnibn+h1 snibn
n:
Thus, we have that q?()follows a Gamma distribution in
(25), with parameters in (26) and (27).
Letq?(s) =eps, we have
lnq?(s)
/lneps
/hlnp(w;;s;D)iq()q(w)
/hlnp(js)iq()+ ln ^p(s)
/NX
n=1sn(lnban
n+ (an 1)hlnni bnhni ln   (an))

--- PAGE 16 ---
JOURNAL OF XXX 16
+ (1 sn)
lnban
n+ (an 1)hlnni bnhni ln   (an)
+NX
n=1(snlnn+ (1 sn) ln (1 n))
/lnNY
n=1(en)sn(1 en)1 sn:
Thus, we have that q?(s)follows a Bernoulli distribution
in (28), with parameters in (29) and (30).
REFERENCES
[1] N. Lee, T. Ajanthan, and P. H. Torr, ‚ÄúSnip: Single-shot network pruning
based on connection sensitivity,‚Äù International Conference on Learning
Representations (ICLR) , 2019.
[2] Y . LeCun, J. S. Denker, and S. A. Solla, ‚ÄúOptimal brain damage,‚Äù in
Advances in Neural Information Processing Systems (NeurIPS) , 1990,
pp. 598‚Äì605.
[3] B. Hassibi and D. G. Stork, Second Order Derivatives for Network
Pruning: Optimal Brain Surgeon . Morgan Kaufmann, 1993.
[4] C. Louizos, M. Welling, and D. P. Kingma, ‚ÄúLearning sparse neural
networks through l_0regularization,‚Äù arXiv preprint arXiv:1712.01312 ,
2017.
[5] W. Wen, C. Wu, Y . Wang, Y . Chen, and H. Li, ‚ÄúLearning structured
sparsity in deep neural networks,‚Äù arXiv preprint arXiv:1608.03665 ,
2016.
[6] J. O. Neill, ‚ÄúAn overview of neural network compression,‚Äù arXiv preprint
arXiv:2006.03669 , 2020.
[7] S. Scardapane, D. Comminiello, A. Hussain, and A. Uncini, ‚ÄúGroup
sparse regularization for deep neural networks,‚Äù Neurocomputing , vol.
241, pp. 81‚Äì89, 2017.
[8] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang,
Y . Zhu, R. Pang, V . Vasudevan et al. , ‚ÄúSearching for mobilenetv3,‚Äù in
Proceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV) , 2019, pp. 1314‚Äì1324.
[9] T. Zhuang, Z. Zhang, Y . Huang, X. Zeng, K. Shuang, and X. Li,
‚ÄúNeuron-level structured pruning using polarization regularizer,‚Äù Ad-
vances in Neural Information Processing Systems (NeurIPS) , vol. 33,
pp. 9865‚Äì9877, 2020.
[10] L. Yang, Z. He, and D. Fan, ‚ÄúHarmonious coexistence of structured
weight pruning and ternarization for deep neural networks,‚Äù in Proceed-
ings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , vol. 34, no. 04,
2020, pp. 6623‚Äì6630.
[11] A. S. Weigend, D. E. Rumelhart, and B. A. Huberman, ‚ÄúGeneralization
by weight-elimination with application to forecasting,‚Äù in Advances in
Neural Information Processing Systems (NeurIPS) , 1991, pp. 875‚Äì882.
[12] J. Frankle and M. Carbin, ‚ÄúThe lottery ticket hypothesis: Finding sparse,
trainable neural networks,‚Äù arXiv preprint arXiv:1803.03635 , 2018.
[13] J. Frankle, G. K. Dziugaite, D. Roy, and M. Carbin, ‚ÄúLinear mode con-
nectivity and the lottery ticket hypothesis,‚Äù in International Conference
on Machine Learning (ICML) . PMLR, 2020, pp. 3259‚Äì3269.
[14] M. Van Baalen, C. Louizos, M. Nagel, R. A. Amjad, Y . Wang,
T. Blankevoort, and M. Welling, ‚ÄúBayesian bits: Unifying quantization
and pruning,‚Äù Advances in Neural Information Processing Systems
(NeurIPS) , vol. 33, pp. 5741‚Äì5752, 2020.
[15] D. Molchanov, A. Ashukha, and D. Vetrov, ‚ÄúVariational dropout spar-
siÔ¨Åes deep neural networks,‚Äù in International Conference on Machine
Learning (ICML) . PMLR, 2017, pp. 2498‚Äì2507.
[16] C. Louizos, K. Ullrich, and M. Welling, ‚ÄúBayesian compression for deep
learning,‚Äù arXiv preprint arXiv:1705.08665 , 2017.
[17] A. Liu, G. Liu, L. Lian, V . K. Lau, and M.-J. Zhao, ‚ÄúRobust recovery
of structured sparse signals with uncertain sensing matrix: A turbo-vbi
approach,‚Äù IEEE Trans. Wireless Commun. , vol. 19, no. 5, pp. 3185‚Äì
3198, 2020.
[18] P. Schniter, ‚ÄúTurbo reconstruction of structured sparse signals,‚Äù in 2010
44th Annual Conference on Information Sciences and Systems (CISS) .
IEEE, 2010, pp. 1‚Äì6.
[19] A. Wu, S. Nowozin, E. Meeds, R. E. Turner, J. M. Hernandez-
Lobato, and A. L. Gaunt, ‚ÄúDeterministic variational inference for robust
Bayesian neural networks,‚Äù arXiv preprint arXiv:1810.03958 , 2018.
[20] J. Zhang, X. Chen, M. Song, and T. Li, ‚ÄúEager pruning: Algorithm
and architecture support for fast training of deep neural networks,‚Äù in
2019 ACM/IEEE 46th Annual International Symposium on Computer
Architecture (ISCA) . IEEE, 2019, pp. 292‚Äì303.[21] J. Friedman, T. Hastie, and R. Tibshirani, ‚ÄúA note on the group lasso
and a sparse group lasso,‚Äù arXiv preprint arXiv:1001.0736 , 2010.
[22] T.-J. Yang, Y .-H. Chen, and V . Sze, ‚ÄúDesigning energy-efÔ¨Åcient convo-
lutional neural networks using energy-aware pruning,‚Äù in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) , 2017, pp. 5687‚Äì5695.
[23] S. J. Kwon, D. Lee, B. Kim, P. Kapoor, B. Park, and G.-Y . Wei,
‚ÄúStructured compression by weight encryption for unstructured pruning
and quantization,‚Äù in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2020, pp. 1909‚Äì
1918.
[24] J. Yu, A. Lukefahr, D. Palframan, G. Dasika, R. Das, and S. Mahlke,
‚ÄúScalpel: Customizing dnn pruning to the underlying hardware paral-
lelism,‚Äù in 2017 ACM/IEEE 44th Annual International Symposium on
Computer Architecture (ISCA) , 2017, pp. 548‚Äì560.
[25] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag, ‚ÄúWhat is
the state of neural network pruning?‚Äù Proceedings of Machine Learning
and Systems , vol. 2, pp. 129‚Äì146, 2020.
[26] L. V . Jospin, W. Buntine, F. Boussaid, H. Laga, and M. Bennamoun,
‚ÄúHands-on Bayesian neural networks‚Äìa tutorial for deep learning users,‚Äù
arXiv preprint arXiv:2007.06823 , 2020.
[27] K. Murphy, Y . Weiss, and M. I. Jordan, ‚ÄúLoopy belief propaga-
tion for approximate inference: An empirical study,‚Äù arXiv preprint
arXiv:1301.6725 , 2013.
[28] D. G. Tzikas, A. C. Likas, and N. P. Galatsanos, ‚ÄúThe variational
approximation for Bayesian inference,‚Äù IEEE Signal Processing Mag. ,
vol. 25, no. 6, pp. 131‚Äì146, 2008.
[29] D. P. Kingma, T. Salimans, and M. Welling, ‚ÄúVariational dropout and
the local reparameterization trick,‚Äù Advances in Neural Information
Processing Systems (NeurIPS) , vol. 28, pp. 2575‚Äì2583, 2015.
[30] P. Tseng, ‚ÄúConvergence of a block coordinate descent method for
nondifferentiable minimization,‚Äù J. Optim. Theory Appl. , vol. 109, no. 3,
pp. 475‚Äì494, 2001.
[31] X. Yuan, L. Ren, J. Lu, and J. Zhou, ‚ÄúEnhanced bayesian compression
via deep reinforcement learning,‚Äù in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , 2019,
pp. 6946‚Äì6955.
[32] H. Xiao, K. Rasul, and R. V ollgraf, ‚ÄúFashion-mnist: a novel image
dataset for benchmarking machine learning algorithms,‚Äù arXiv preprint
arXiv:1708.07747 , 2017.
[33] A. Krizhevsky, V . Nair, and G. Hinton, ‚ÄúThe cifar-10 and cifar-100
dataset.‚Äù cs.toronto.edu. http://www.cs.toronto.edu/ kriz/cifar.html , (ac-
cessed August 21 2022).
[34] V . Sehwag, S. Wang, P. Mittal, and S. Jana, ‚ÄúHydra: Pruning adversari-
ally robust neural networks,‚Äù Advances in Neural Information Processing
Systems (NeurIPS) , vol. 33, pp. 19 655‚Äì19 666, 2020.
[35] J. Li, R. Drummond, and S. R. Duncan, ‚ÄúRobust error bounds for
quantised and pruned neural networks,‚Äù in Learning for Dynamics and
Control . PMLR, 2021, pp. 361‚Äì372.
[36] L. Wang, G. W. Ding, R. Huang, Y . Cao, and Y . C. Lui, ‚ÄúAdversarial
robustness of pruned neural networks,‚Äù 2018.
