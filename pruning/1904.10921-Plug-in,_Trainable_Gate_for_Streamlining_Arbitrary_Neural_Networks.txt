# 1904.10921.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/pruning/1904.10921.pdf
# File size: 3763726 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Plug-in, Trainable Gate for Streamlining Arbitrary Neural Networks
Jaedeok Kim1Chiyoun Park1Hyun-Joo Jung1Yoonsuck Choe1;2
1ArtiÔ¨Åcial Intelligence Center, Samsung Research, Samsung Electronics Co.
56 Seongchon-gil, Secho-gu, Seoul, Korea, 06765
2Department of Computer Science and Engineering, Texas A&M University
College Station, TX, 77843, USA
fjd05.kim, chiyoun.park, hj34.jung g@samsung.com, choe@tamu.edu
Abstract
Architecture optimization, which is a technique for Ô¨Ånding
an efÔ¨Åcient neural network that meets certain requirements,
generally reduces to a set of multiple-choice selection prob-
lems among alternative sub-structures or parameters. The dis-
crete nature of the selection problem, however, makes this
optimization difÔ¨Åcult. To tackle this problem we introduce a
novel concept of a trainable gate function. The trainable gate
function, which confers a differentiable property to discrete-
valued variables, allows us to directly optimize loss func-
tions that include non-differentiable discrete values such as
0-1 selection. The proposed trainable gate can be applied
to pruning. Pruning can be carried out simply by append-
ing the proposed trainable gate functions to each intermedi-
ate output tensor followed by Ô¨Åne-tuning the overall model,
using any gradient-based training methods. So the proposed
method can jointly optimize the selection of the pruned chan-
nels while Ô¨Åne-tuning the weights of the pruned model at the
same time. Our experimental results demonstrate that the pro-
posed method efÔ¨Åciently optimizes arbitrary neural networks
in various tasks such as image classiÔ¨Åcation, style transfer,
optical Ô¨Çow estimation, and neural machine translation.
Introduction
Deep neural networks have been widely used in many appli-
cations such as image classiÔ¨Åcation, image generation, and
machine translation. However, in order to increase accuracy
of the models, the neural networks have to be made larger
and require a huge amount of computation (He et al. 2016a;
Simonyan and Zisserman 2014). Because it is often not fea-
sible to load and execute such a large model on an on-
device platform such as mobile phones or IoT devices, vari-
ous architecture optimization methods have been proposed
for Ô¨Ånding an efÔ¨Åcient neural network that meets certain
design requirements. In particular, pruning methods can re-
duce both model size and computational costs effectively,
but the discrete nature of the binary selection problems
makes such methods difÔ¨Åcult and inefÔ¨Åcient (He et al. 2018;
Luo and Wu 2018).
Equal contribution.
Copyright c2020, Association for the Advancement of ArtiÔ¨Åcial
Intelligence (www.aaai.org). All rights reserved.Gradient descent methods can solve a continuous opti-
mization problem efÔ¨Åciently by minimizing the loss func-
tion, but such methods are not directly applicable to discrete
optimization problems because they are not differentiable.
While many alternative solutions such as simulated anneal-
ing (Kirkpatrick, Gelatt, and Vecchi 1983) have been pro-
posed to handle discrete optimization problems, they are too
cost-inefÔ¨Åcient in deep learning because we need to train al-
ternative choices to evaluate the sample‚Äôs accuracy.
In this paper, we introduce a novel concept of a trainable
gate function (TGF) that confers a differentiable property to
discrete-valued variables. It allows us to directly optimize,
through gradient descent, loss functions that include discrete
choices that are non-differentiable. By applying TGFs each
of which connects a continuous latent parameter to a discrete
choice, a discrete optimization problem can be relaxed to a
continuous optimization problem.
Pruning a neural network is a problem that decides which
channels (or weights) are to be retained. In order to obtain an
optimal pruning result for an individual model, one needs to
compare the performance of the model induced by all com-
binations of retained channels. While specialized structures
or searching algorithms have been proposed for pruning,
they have complex structures or their internal parameters
need to be set manually (He et al. 2018). The key problem
of channel pruning is that there are discrete choices in the
combination of channels, which makes the problem of chan-
nel selections non-differentiable. Using the proposed TGF
allows us to reformulate discrete choices as a simple differ-
entiable learning problem, so that a general gradient descent
procedure can be applied, end-to-end.
Our main contributions in this paper are in three fold.
We introduce the concept of a TGF which makes a
discrete selection problem solvable by a conventional
gradient-based learning procedure.
We propose a pruning method with which a neural net-
work can be directly optimized in terms of the number of
parameters or that of FLOPs. The proposed method can
prune and train a neural network simultaneously, so that
the further Ô¨Åne-tuning step is not needed.
Our proposed method is task-agnostic so that it can be
easily applied to many different tasks.arXiv:1904.10921v2  [cs.LG]  14 Nov 2019

--- PAGE 2 ---
Simply appending TGFs, we have achieved competitive
results in compressing neural networks with minimal degra-
dation in accuracy. For instance, our proposed method com-
presses ResNet-56 (He et al. 2016a) on CIFAR-10 dataset
(Krizhevsky and Hinton 2009) by half in terms of the num-
ber of FLOPs with negligible accuracy drop. In a style trans-
fer task, we achieved an extremely compressed network
which is more than 35 times smaller and 3 times faster
than the original network. Moreover, our pruning method
has been effectively applied to other practical tasks such as
optical Ô¨Çow estimation and neural machine translation.
By connecting discrete and continuous domains through
the concept of TGF, we are able to obtain competitive results
on various applications in a simple way. Not just a contin-
uous relaxation, it directly connects the deterministic deci-
sion to a continuous and differentiable domain. By doing so,
the proposed method in this paper could help us solve more
practical applications that have difÔ¨Åculties due to discrete
components in the architecture.
Related Work
Architecture optimization can be considered as a combina-
torial optimization problem. The most important factors are
to determine which channels should be pruned within a layer
in order to minimize the loss of acquired knowledge.
These can be addressed as a problem that Ô¨Ånds the
best combination of retained channels where it requires ex-
tremely heavy computation. As an alternative, heuristic ap-
proaches have been proposed to select channels to be pruned
(He, Zhang, and Sun 2017; Li et al. 2017). Although these
approaches provide rich intuition about neural networks and
can be easily adopted to compress a neural network quickly,
such methods tend to be sub-optimal for a given task in prac-
tice.
The problem of Ô¨Ånding the best combination can be for-
mulated as a reinforcement learning (RL) problem and then
be solved by learning a policy network. Bello et al. (Bello
et al. 2017) proposed a method to solve combinatorial op-
timization problems including traveling salesman and knap-
sack problems by training a policy network. Zoph and Le
(Zoph and Le 2016) proposed an RL based method to Ô¨Ånd
the most suitable architecture. The same approach can be
applied to Ô¨Ånd the best set of compression ratio for each
layer that satisÔ¨Åes the overall compression and performance
targets, as proposed in (He et al. 2018; Zhong et al. 2018).
However, RL based methods still require extremely heavy
computation.
To tackle the scalability issue, a differentiable approach
has been considered in various research based on continuous
relaxation (Liu, Simonyan, and Yang 2019; Liu et al. 2017;
Louizos, Ullrich, and Welling 2017). To relax a discrete
problem to be differentiable, Liu et al. (Liu, Simonyan, and
Yang 2019) proposed a method that places a mixture of can-
didate operations by using softmax. Luo and Wu (Luo and
Wu 2018) proposed a type of a self-attention module with
a scaled sigmoid function as an activation function to re-
tain channels from probabilistic decision. However, in these
methods it is essential to carefully initialize and control the
parameters of the attention layers and the scaled sigmoid
Figure 1: Explanation of how to shape a gate function to be
trainable. The original gate function has zero derivative as
in the left. We add to the original gate function the shap-
ing function s(w)multiplied by a desirable derivative shape
g(w)which changes the derivative of the gate function. The
resulting function is a TGF that has a desirable derivative.
function. While a differentiable approach is scalable to a
large search space, existing approaches determine the set of
selected channels in a probabilistic way so that they require
an additional step to decide whether to prune each channel
or not.
The method we propose here allows us to Ô¨Ånd the set
of channels deterministically by directly optimizing the ob-
jective function which confers a differentiable property to
discrete-valued variables, thus bypassing the additional step
required in probabilistic approaches. The proposed opti-
mization can be performed simply by appending TGFs to
a target layer and train it using a gradient descent optimiza-
tion. The proposed method does not depend on additional
parameters, so that it does not need a careful initialization or
specialized annealing process for stabilization.
Differentially Trainable Gate Function
Consider combinatorial optimization in a selection problem.
min
2;b2f0;1gnL(;b) (1)
where b= (b1;;bn)is a vector of binary selections.
L: f0;1gn!Ris an objective function parameterized
by. The optimization problem (1) is a generalized form
of a selection problem that can cover a parameterized loss
function such as a neural network pruning problem. In case
of a pure selection problem we set the domain to be a
singleton set.
To make the problem differentiable, we consider bias an
output of a binary gate function b:R! f0;1gparame-
terized by an auxiliary variable wi. We will let bbe a step
function for convenience.1Then the optimization problem
(1) is equivalent to the following.
min
2;w2RnL(;b(w1);;b(wn)) (2)
where the problem is deÔ¨Åned in continuous domain. That is,
if(;w)is a global minimum of (2), then (;b(w))is a
global minimum of (1).
While the continuous relaxation (2) enables the optimiza-
tion problem (1) to be solved by gradient descent, a gate
1Although we only consider a step function as a gate function b,
the same argument can be easily applied to an almost everywhere
differentiable binary gate function.

--- PAGE 3 ---
functionbi()has derivative of zero wherever differentiable
and consequently
@L
@wi=@L
@b@b
@wi= 0;wherewi6= 0: (3)
So, a gradient descent optimization does not work for such
a function.
In order to resolve this issue, we consider a new type
of gate function which has non-zero gradient and is differ-
entiable almost everywhere. Motivated by (Hahn and Choi
2018), we Ô¨Årst deÔ¨Åne a gradient shaping function s:R!R
by
s(M)(w) :=Mw bMwc
M(4)
whereMis a large positive integer and bwcis the greatest
integer less than or equal to w. Note that this function has
near-zero value for all w, and its derivative is always one
wherever differentiable. Using (4) we consider a trainable
gate deÔ¨Åned as the following (see Figure 1).
DeÔ¨Ånition 1 A function TG(M):R!Ris said to be a
trainable gate of a gate function b:R!f0;1gwith respect
to a gradient shape g:R!Rif
TG(M)(w;g) :=b(w) +s(M)(w)g(w): (5)
Then a trainable gate TG(M)satisÔ¨Åes the following propo-
sition.
Proposition 1 For any bounded derivative shape gwhose
derivative is also bounded, TG(M)(w;g)uniformly con-
verges tob(w)asM!1 . Moreover, TG(M)0(w;g)uni-
formly converges to g(w).
Proof. By deÔ¨Ånition (5), it satisÔ¨Åes that for all w2R
jTG(M)(w;g) b(w)j=js(M)(w)g(w)j
1
Mjg(w)j!0;
asM!1 .
Also,s0(w) = 1 ifs(w)is differentiable at w, which
yields for all w2R
jTG(M)(w;g)0(w) g(w)j
=js(M)0(w)g(w) +s(M)(w)g0(w) g(w)j
=js(M)(w)g0(w)j
1
Mjg0(w)j!0;
asM!1 .
Proposition 1 guarantees that the trainable gate
TG(M)(w;g)can approximate the original gate func-
tionb(w), while its derivative still approximates the
desired derivative shape g. It is now possible to control
the derivative of the given kernel b(w)as we want and
hence a gradient descent optimization is applicable to the
TGF TG(M)(w;g). For convenience, we will drop the
superscript and the desired function gfrom TG(M)(w;g)
unless there is an ambiguity.
Figure 2: Difference between probabilistic and deterministic
decisions. Each bar in the above graph indicates the decision
weight of a hidden node learnt by a probabilistic method.
Theith label on the x-axis represents the learnt weights wi
andciof theith hidden node. The graph below shows the
results from a TGF where there is no redundant hidden node.
Difference between Probabilistic and Deterministic
Decisions
The proposed TGF directly learns a deterministic deci-
sion during the training phase unlike existing methods.
While a probabilistic decision has been considered in ex-
isting differentiable methods (Jang, Gu, and Poole 2017;
Liu, Simonyan, and Yang 2019; Liu et al. 2017; Louizos,
Ullrich, and Welling 2017), probabilistic decisions are not
clear to select and hence it needs further decision steps. Due
to the on-off nature of our TGF‚Äôs decision, we can include
more decisive objectives, such as the number of channels,
FLOPs or parameters, without requiring approximation to
expectation of the distribution or smoothing to non-discrete
values.
We performed a synthetic experiment in order to see
the difference between probabilistic and deterministic ap-
proaches. To this end, we generate a training dataset to learn
the sine function sin(). Consider a neural network having a
single fully-connected layer having 20 hidden nodes each of
which adopts the sine function as its activation. Since a train-
ing sample in our synthetic dataset is of the form (x;sin(x)),
it is enough to use only one hidden node to express the rela-
tion between input xand outputy.
We consider a selection layer consisting of a continu-
ous relaxation function with 20 hidden nodes each of which
learns whether to retain the corresponding hidden node or
not. Two different types of a relaxation function are ad-
dressed: a softmax function for a probabilistic decision (Liu,
Simonyan, and Yang 2019) and the proposed TGFs for a de-
terministic decision.
As we can see in Figure 2, the probabilistic method found
a solution that uses more than one node. In particular, the top
5 hidden nodes based on the decision weight have similar
weight values wiandci. In a training phase the probabilistic
decision uses a linear combination of options so that error
can be canceled out and as a result the selections will be re-

--- PAGE 4 ---
Figure 3: The overview of the proposed TGL. The Ô¨Ågure
shows an example of a TGL appending to a convolution
layer. The TGL consists of a set of gate functions each of
which determines whether to prune or not. Each gate func-
tion outputs a value of 0 or 1 which is multiplied to the cor-
responding Ô¨Ålter of the convolution kernel of the target layer.
dundant. On the other hand, the deterministic decision (our
proposed TGF) selects only one node exactly. Due to the on-
off nature of the deterministic decision, the TGF learns by
incorporating the knowledge of selections. So the determin-
istic decision can choose effectively without redundancy.
While we have considered the binary selection problem so
far, it is straightforward to extend the concept of a trainable
gate to ann-ary case by using n-simplex. However, in order
to show the practical usefulness of the proposed concept, we
will consider the pruning problem, an important application
of a TGF, in which it is enough to use a binary gate function.
Differentiable Pruning Method
In this section we develop a method to efÔ¨Åciently and auto-
matically prune a neural network as an important application
of the proposed TGF. To this end, using the concept of a TGF
we propose a trainable gate layer (TGL) which learns how
to prune channels from its previous layer.
Design of a Trainable Gate Layer
The overall framework of our proposed TGL is illustrated
in Figure 3. Channel pruning can be formulated by a func-
tion that zeros out certain channels of an output tensor in a
convolutional neural network and keeps the rest of the val-
ues. We thus design a TGL as a set of TGFs whose elements
correspond to the output channels of a target layer. Let the
lth target layer map an input tensor xlto an output tensor
yl:= (yl;1;;yl;nl)that hasnlchannels using a kernel
Kl
yl;i=Kl;ixl:
fori= 1;;nl. A fully connected layer uses the multipli-
cationinstead of a convolution operation , but we here
simply useto represent both cases.
Let a TGLPlprune thelth target layer where the TGL Pl
consists of trainable weights wl:= (wl;1;;wl;nl)and a
function TG(). The weight wl;i,i= 1;;nl, is used to
learn whether the corresponding channel should be masked
by zero or not. The TGL Plmasks the output tensor ylof the
lth target layer as
~yl;i= TG(wl;i)yl;i; (6)where ~yl:= (~yl;1;;~yl;nl)is the pruned output tensor by
Pl. Since we have yl;i=Kl;ixl, (6) can be rewritten as
TG(wl;i)yl;i= TG(wl;i)(Kl;ixl)
= (TG(wl;i)Kl;i)xl:
So multiplying TG(wl;i)toyl;imasks theith channel from
the kernelKl. While TG(wl;i)might not be exactly zero
due to the gradient shaping, its effect can be negligible by
letting the value of Mbe large enough.
From (6), TG(wl;i) = 0 yields ~yl;i= 0. So the value
of the weight wl;ican control the ith channel of the output
tensoryl. If a step function b(w) =1[w>0]is used as the gate
function in the TGL, wl;i<0implies that the TGL zeros out
theith channel from the lth layer. Otherwise, the channel
remains identical. Hence, by updating the weights wl, we
can make the TGL learn the best combination of channels to
prune.
The proposed channel pruning method can be extended to
weight pruning or layer pruning in a straightforward way. It
can be achieved by applying the trainable gate functions for
each elements of the kernel or each layer.
Compression Ratio Control
The purpose of channel pruning is to reduce neural net-
work size or computational cost. However, simply adding
the above TGL without any regularization does not ensure
that channels are pruned out as much as we need. Unless
there is a signiÔ¨Åcant amount of redundancy in the network,
having more Ô¨Ålters is often advantageous to obtain higher
accuracy, so the layer will not be pruned. Taking this issue
into account, we add a regularization factor to the loss func-
tion that controls the compression ratio of a neural network
as desired.
Letbe the target compression ratio. In case of reduc-
ing the number of FLOPs, the target compression ratio is
deÔ¨Åned by the ratio between the number of the remaining
FLOPs and the total number Ctotof FLOPs of the neural
network. The weight values w:= (w1;;wL)of TGLs
determine the remaining number of FLOPs, denoted by
C(w). We want to reduce the FLOPs of the pruned model by
the factor of , that is, we want it to satisfy C(w)=Ctot.
LetL(;w)be the original loss function to be minimized
wheredenotes the weights of layers in the neural network
except the TGLs. We add a regularization term to the loss
function as follows in order to control the compression ratio.
L(;w;) :=L(;w) +k C(w)=Ctotk2
2(7)
wherekk2denotes thel2-norm andis a regularization pa-
rameter. The added regularization will ensure that the num-
ber of channels will be reduced to meet our desired com-
pression ratio.
Note that minimization of the loss function (7) does not
only update the weights of TGLs but also those of the nor-
mal layers. A training procedure, therefore, jointly optimizes
the selection of the pruned channels while Ô¨Åne-tuning the
weights of the pruned model at the same time. In traditional
approaches where channel pruning procedure is followed by
a separate Ô¨Åne-tuning stage, the importance of each channel

--- PAGE 5 ---
(a) ResNet-56 (CIFAR-10)
 (b) VGG-16 (ImageNet)
Figure 4: Pruning results without Ô¨Åne-tuning. (a) Channel
pruning of ResNet-56 and (b) Weight pruning of VGG-16
Thex-axis represents the compression ratio which is the
ratio of the remaining of a pruned model over the original
number of FLOPs or parameters.
may change during Ô¨Åne-tuning stage, which leads to sub-
optimal compression. Our proposed method, however, does
not fall into such a problem since each TGL automatically
takes into account the importance of each channel while ad-
justing the weights of the original model based on the pruned
channels. The loss function indicates that there is a trade-off
between the compression ratio and accuracy.
While we have considered the number of FLOPs in this
subsection, we can extend easily to other objectives such as
the number of weight parameters or channels by replacing
the regularization target in (7).
Experimental Results
In this section, we will demonstrate the effectiveness of the
proposed TGF through various applications in the image and
language domains. We implemented our experiments using
Keras (Chollet and others 2015) unless mentioned other-
wise. In order to shape the derivative of the gate function b,
a constant derivative shape g(w) = 1 andM= 105is used
and a simple random weight initialization are used. In all
experiments, only convolution or fully-connected layers are
considered in calculating the number of FLOPs of a model
since the other type of layers, e.g., batch normalization, re-
quires relatively negligible amount of computation.
Image ClassiÔ¨Åcation
We used CIFAR-10 and ImageNet datasets for our im-
age classiÔ¨Åcation experiments. We used pretrained weights
of ResNet-56 on CIFAR-10 that trained from scratch with
usual data augmentations (normalization and random crop-
ping). For VGG-16 on ImageNet we used pre-trained
weights that was published in Keras. Although we found that
the accuracy of each model in our experimental setup dif-
fers slightly from the reported value, the original pre-trained
weights were used in our experiments without modiÔ¨Åcation
since we wanted to investigate the performance of our pro-
posed method in terms of the accuracy drop.
Pruning without Fine-tuning In order to show the effect
of the TGFs, we Ô¨Årst considered a selection problem. WeTable 1: Channel pruning of ResNet-56 on the CIFAR-10
and weight pruning of VGG-16 on ImageNet. acc and acc
denote the accuracy after pruning and the accuracy drop, re-
spectively.FLOPs (params , resp.) means the number of re-
maining FLOPs (parameters, resp.) over the total number
of FLOPs (parameters, resp.) in each model, where a small
value means more pruned. Methods: FP (Li et al. 2017), CP
(He, Zhang, and Sun 2017), VCP (Zhao et al. 2019), AMC
(He et al. 2018), ADMM and PWP (Ye et al. 2018).
Model Method acc(acc) (%) 
ResNet-56
(=FLOPs )FP 0.02 (93.06) 0.72
VCP -0.8 (92.26) 0.79
Proposed 0.04 (92.7) 0.71
CP -1.0 (91.8) 0.5
AMC -0.9 (91.9) 0.5
Proposed -0.3(92.36) 0.51
VGG-16
(=params )ADMM 0.0 (88.7) 1/19
Proposed 0.76 (89.0) 1/20
PWP -0.5 (88.2) 1/34
Proposed -0.06 (88.18) 1/50
kept the pre-trained weights of a model and only pruned the
channels or weights without Ô¨Åne-tuning by appending TGLs
to convolution and fully-connected layers. That is, we Ô¨Åx
the weights and optimize win (2). Figure 4a shows the
results of channel pruning in ResNet-56 (He et al. 2016b) on
CIFAR-10. It can be observed that the number of FLOPs can
be reduced by half without noticeable change in accuracy
even when we do not apply Ô¨Åne-tuning, which implies that
the TGFs works as expected.
We also applied weight pruning mentioned in the pre-
vious section to VGG-16 on ImageNet (Figure 4b). Even
though there are more than 1:38108weight parameters in
the model whether to be retained or not, simply plugging-in
TGLs to the model allows us to Ô¨Ånd a selection of redun-
dant weight parameters. Note that the accuracy of VGG-16
even increases to 89.1% from 88.24% when only 10% of
the parameters are retained. This phenomenon is due to the
fact that reducing the number of non-gated parameters are
equivalent to applying L0regularization to the neural net-
work, and so adding the gating function to each parameter
improves the generalization property of the model further.
Pruning with Fine-tuning In the next example, we
jointly optimized the weights and selection at the same time
in order to incorporate Ô¨Åne-tuning to the selections. Like
the previous example we appended TGLs to a model, but
we jointly trained both the TGLs and the weight parameters
of the model. Table 1 summarizes the pruning results. As
shown in the table, our results are competitive with existing
methods. For example, in ResNet-56, the number of FLOPs
is reduced by half while maintaining the original accuracy.
It is also noticeable that we achieve higher accuracy on the
compressed VGG-16 model, even if the accuracy of our ini-
tial model was worse.
While in our experiments we used a constant function to
shape the derivative within the TGFs, The proposed method
can adopt any derivative shape by changing g(w)in (5). Fig-

--- PAGE 6 ---
Figure 5: Comparing the effect of gradient shaping in TGFs.
sigmoid‚Äô denotes the derivative of the sigmoid function
(w) = 1=(1 +e x)andtanh‚Äô denotes the derivative of
the hyperbolic tangent function.
Table 2: A summary of style transfer model compression
results. We measured the size of model Ô¨Åles which are saved
in binary format (.pb). The inference time was averaged over
10 frames on the CPU of a Galaxy S10. The frame size is
256256 pixels.
ModelFile size
(MB)FLOPs
(G)Params
(M)Time
(ms)
Original 6.9 10.079 1.674 549
ch= 0:4 1.8 2.470 0.405 414
ch= 0:1 0.2 0.227 0.020 160
ure 5 compares the effect of different shaping functions. It
shows that the derivative shape gdoes not affect the results
critically, which implies that our proposed method is stable
over a choice of g(w). It can be concluded that a simple
constant derivative shape g(w) = 1 can be adopted without
signiÔ¨Åcant loss of accuracy.
Image Generation
We further applied our proposed pruning method to style
transfer and optical Ô¨Çow estimation models which are the
most popular applications in image generation.
Style Transfer Style transfer (Dumoulin, Shlens, and
Kudlur 2017; Gatys, Ecker, and Bethge 2015) generates a
new image by synthesizing contents in given content image
with styles in given style image. Because a style transfer net-
work is heavily dependent on the selection of which styles
are used, it is not easy to obtain proper pre-trained weights.
So, we started from N-style transfer network (Dumoulin,
Shlens, and Kudlur 2017) as an original architecture2with
randomly initialized weights . It is of course possible to start
from pre-trained weights if available.
To select which channels are retained, a TGL is plugged
into the output of each convolution layer in the original ar-
chitecture except the last one for preserving its generation
performance. In training phase, we used ImageNet as a set
of content images and chose 5 style images (i.e., N= 5)
2Source code: https://github.com/tensorÔ¨Çow/magenta/tree/
master/magenta/models/image stylization.
Figure 6: Results of the style transfer task. The rightmost
column represents a content image and the top row repre-
sents 5 style images. Second row : Transferred images pro-
duced by the original model. Third row : Transferred images
produced by the compressed model ( ch= 0:4).Fourth
row: Transferred images produced by the compressed model
(ch= 0:1).
manually as shown in Figure 6. We trained both original and
compressed model from scratch for 20K iterations with a
batch size of 16. The number of pruned channels is used as
a regularization factor with regularization weight = 0:1.
The compressed model ( ch= 0:1) is34.5 times
smaller than the original model in terms of Ô¨Åle size (Table
2). In order to see the actual inference time, we measured the
inference time on the CPU of a Galaxy S10. The compressed
model (ch= 0:1) is more than3 times faster in terms of
the inference time as shown although the generation quality
preserved as shown in Figure 6.
Figure 7 shows the number of retained channels in each
layer. The TGL does not select the number of pruned chan-
nels uniformly and it automatically selects which channels
are pruned according to the objective function. Without the
further pruning steps, our proposed method can train and
prune simultaneously the model with a consideration of the
target compression ratio as we mentioned in the previous
section.
Optical Flow Estimation We next consider a task that
learns the optical Ô¨Çow between images (Dosovitskiy et al.
2015; Ilg et al. 2017). In this experimentation, we used
FlowNetSimple (Dosovitskiy et al. 2015), which is the same
as FlowNetS3in (Ilg et al. 2017). FlowNetS stacks two con-
secutive input images together and feed them into the net-
work to extract motion information between these images.
Starting from the pre-trained model, a TGL is plugged
into the output of every convolution and deconvolution lay-
ers except the last one for preserving its generation perfor-
mance. We trained the model with TGLs for 1.2M iterations
with a batch size of 8. The Adam optimizer (Kingma and Ba
2015) was used with initial learning rate 0.0001 and it was
halved every 200K iterations after the Ô¨Årst 400K iterations.
3Source code: https://github.com/sampepose/Ô¨Çownet2-tf

--- PAGE 7 ---
Figure 7: The number of retained channels in compressed
models. Layers that should match the dimension of outputs
share a single TGL (conv3, res1-2, res2-2, res3-2, res4-2,
and res5-2).
Table 3: A summary of the optical Ô¨Çow estimation results.
We measured the size of model Ô¨Åles which are saved in bi-
nary format (.pb). For inference time, we ran both model
on Intel Core i7-7700@3.60GHz CPU. The size of image is
384x512 pixels. EPE was averaged over validation data in
Flying Chairs dataset.
Model File size (MB) EPE Time (ms)
Original 148 3.15 292.7
ch= 0:60 54 2.91 177.8
ch= 0:45 30 3.13 155.9
As in the style transfer task, the number of pruned channels
is used as a regularization factor with regularization weight
= 0:1. We used the Flying Chairs dataset (Dosovitskiy et
al. 2015) for training and testing. The performance of model
is measured in average end-point-error (EPE) of validation
data.
Table 3 shows the compression results. As we can see in
the table, the compressed model ( ch= 0:45) is4.93 times
smaller than the original model in terms of Ô¨Åle size and more
than1.88 times faster in terms of inference time. Note that
the EPE of the compressed model (3.13) is almost same with
that of the original model (3.15). But it is only small a bit
worse than EPE reported (2.71) in the paper (Dosovitskiy et
al. 2015).
Our experimental results demonstrate that the compressed
model pruned by TGL automatically Ô¨Ånds which channels to
be retained for reducing model Ô¨Åle size, inference time, and
FLOPs, while minimizing performance degradation.
Neural Machine Translation
While we have considered various applications, all of those
applications are in the image domain. So as the last appli-
cation we applied our pruning method to a neural machine
translation task in the language domain. We tried to com-
press the transformer model (Vaswani et al. 2017) that hasTable 4: Results of pruning attention heads of a transformer
model. The BLEU score measure on English-to-German
newstest2014. Endenotes the total number of retained atten-
tion heads in the encoder self-attention heads. DeandEn-De
respectively are deÔ¨Åned for the decoder self-attention and
the encoder-decoder attention. attndenotes the the fraction
of the total number of retained attention heads.
Model En/De/En-De BLEU (BLEU)
Original 48/48/48 ‚Äì (27.32)
attn= 0:83 30/41/48 -0.06 (27.26)
attn= 0:39 11/20/25 -0.18 (27.14)
been most widely used.
The transformer model consists of an encoder and a de-
coder. Each layer of the encoder has multiple self-attention
heads, whereas each layer of the decoder has multiple
self-attention heads and multiple encoder-decoder attention
heads. To make each layer compact, we append TGFs each
of which masks the corresponding attention head. Note that
unlike in the previous tasks, our pruning method can prune
at a block-level, an attention head, not just at the level of a
single weight or channel.
We performed WMT 2014 English-to-German translation
task as our benchmark and implemented on fairseq (Ott et al.
2019). We trained the model over 472,000 iterations from
scratch. As we can see in Table 4, the BLEU score of a
pruned model does not degrade much. In particular, although
only 38% of attention heads are retained, the BLEU score
degrades only 0.18. Our pruning method improved the com-
putational efÔ¨Åciency in the language domain as well, from
which we can conclude that the proposed pruning method is
task-agnostic.
Conclusion
In this paper, we introduced the concept of a TGF and a dif-
ferentiable pruning method as an application of the proposed
TGF. The introduction of a TGF allowed us to directly op-
timize the loss function based on the number of parameters
or FLOPs which are non-differentiable discrete values. Our
proposed pruning method can be easily implemented by ap-
pending TGLs to the target layers, and the TGLs do not
need additional internal parameters that need careful tun-
ing. We showed the effectiveness of the proposed method
by applying to important applications including image clas-
siÔ¨Åcation and image generation. Despite its simplicity, our
experiments show that the proposed method achieves better
compression results on various deep learning models. We
have also shown that the proposed method is task-agnostic
by performing various applications including image classi-
Ô¨Åcation, image generation, and neural machine translation.
We expect that the TGF can be applied to many more ap-
plications where we need to train discrete choices and turn
them into differentiable training problems.

--- PAGE 8 ---
Acknowledgements
We would like to thank Sunghyun Choi and Haebin Shin for
supports on our machine translation experiments. Kibeom
Lee and Jungmin Kwon supports the mobile phone experi-
ments of the style transfer.
References
Bello, I.; Pham, H.; Le, Q. V .; Norouzi, M.; and Bengio,
S. 2017. Neural combinatorial optimization with reinforce-
ment learning. In International Conference on Learning
Representations .
Chollet, F., et al. 2015. Keras. https://keras.io.
Dosovitskiy, A.; Fischer, P.; Ilg, E.; Hausser, P.; Hazirbas,
C.; Golkov, V .; Van Der Smagt, P.; Cremers, D.; and Brox,
T. 2015. Flownet: Learning optical Ô¨Çow with convolutional
networks. In Proceedings of the IEEE international confer-
ence on computer vision , 2758‚Äì2766.
Dumoulin, V .; Shlens, J.; and Kudlur, M. 2017. A learned
representation for artistic style. In International Conference
on Learning Representations .
Gatys, L. A.; Ecker, A. S.; and Bethge, M. 2015. A neural
algorithm of artistic style. arXiv preprint arXiv:1508.06576 .
Hahn, S., and Choi, H. 2018. Gradient acceleration in acti-
vation functions. arXiv preprint arXiv:1806.09783 .
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016a. Deep resid-
ual learning for image recognition. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) , 770‚Äì
778.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016b. Identity map-
pings in deep residual networks. In European Conference on
Computer Vision (ECCV) , 630‚Äì645.
He, Y .; Lin, J.; Liu, Z.; Wang, H.; Li, L.-J.; and Han, S. 2018.
Amc: Automl for model compression and acceleration on
mobile devices. In European Conference on Computer Vi-
sion (ECCV) , 784‚Äì800.
He, Y .; Zhang, X.; and Sun, J. 2017. Channel pruning for ac-
celerating very deep neural networks. In IEEE International
Conference on Computer Vision (ICCV) .
Ilg, E.; Mayer, N.; Saikia, T.; Keuper, M.; Dosovitskiy, A.;
and Brox, T. 2017. Flownet 2.0: Evolution of optical
Ô¨Çow estimation with deep networks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, 2462‚Äì2470.
Jang, E.; Gu, S.; and Poole, B. 2017. Categorical reparam-
eterization with gumbel-softmax. In International Confer-
ence on Learning Representations .
Kingma, D. P., and Ba, J. 2015. Adam: A method for
stochastic optimization. In International Conference on
Learning Representations .
Kirkpatrick, S.; Gelatt, C. D.; and Vecchi, M. P. 1983. Opti-
mization by simulated annealing. Science 220 4598:671‚Äì80.
Krizhevsky, A., and Hinton, G. 2009. Learning multiple lay-
ers of features from tiny images. Technical report, Citeseer.Li, H.; Kadav, A.; Durdanovic, I.; Samet, H.; and Graf, H. P.
2017. Pruning Ô¨Ålters for efÔ¨Åcient convnets. In International
Conference on Learning Representations .
Liu, Z.; Li, J.; Shen, Z.; Huang, G.; Yan, S.; and Zhang,
C. 2017. Learning efÔ¨Åcient convolutional networks through
network slimming. In IEEE International Conference on
Computer Vision (ICCV) , 2755‚Äì2763. IEEE.
Liu, H.; Simonyan, K.; and Yang, Y . 2019. DARTS: differ-
entiable architecture search. In International Conference on
Learning Representations .
Louizos, C.; Ullrich, K.; and Welling, M. 2017. Bayesian
compression for deep learning. In Advances in Neural In-
formation Processing Systems , 3288‚Äì3298.
Luo, J.-H., and Wu, J. 2018. Autopruner: An end-to-end
trainable Ô¨Ålter pruning method for efÔ¨Åcient deep model in-
ference. arXiv preprint arXiv:1805.08941 .
Ott, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.;
Grangier, D.; and Auli, M. 2019. fairseq: A fast, extensible
toolkit for sequence modeling. In Proceedings of NAACL-
HLT 2019: Demonstrations .
Simonyan, K., and Zisserman, A. 2014. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 .
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.
Attention is all you need. In Advances in Neural Information
Processing Systems 30 , 5998‚Äì6008.
Ye, S.; Zhang, T.; Zhang, K.; Li, J.; Xu, K.; Yang, Y .; Yu, F.;
Tang, J.; Fardad, M.; Liu, S.; Chen, X.; Lin, X.; and Wang, Y .
2018. Progressive weight pruning of deep neural networks
using ADMM. arXiv preprint arXiv:1810.07378 .
Zhao, C.; Ni, B.; Zhang, J.; Zhao, Q.; Zhang, W.; and Tian,
Q. 2019. Variational convolutional neural network pruning.
InThe IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) .
Zhong, J.; Ding, G.; Guo, Y .; Han, J.; and Wang, B. 2018.
Where to prune: Using lstm to guide end-to-end pruning. In
IJCAI , 3205‚Äì3211.
Zoph, B., and Le, Q. V . 2016. Neural architecture search
with reinforcement learning. In International Conference
on Learning Representations .
