# 2308.10438.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2308.10438.pdf
# Kích thước tệp: 814349 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Tối ưu hóa chung hiệu quả của cắt tỉa trọng số thích ứng từng lớp
trong mạng nơ-ron sâu
Kaixin Xu1,2,∗Zhe Wang1,2,∗Xue Geng1Jie Lin1,†Min Wu1Xiaoli Li1,2Weisi Lin2
1Viện Nghiên cứu Thông tin liên lạc (I2R), Cơ quan Khoa học, Công nghệ và Nghiên cứu (A*STAR),
1 Fusionopolis Way, 138632, Singapore
2Đại học Công nghệ Nanyang, Singapore
{xuk,wangz,geng xue,wumin,xlli }@i2r.a-star.edu.sg, jie.dellinger@gmail.com, wslin@ntu.edu.sg
Tóm tắt
Trong bài báo này, chúng tôi đề xuất một phương pháp cắt tỉa trọng số thích ứng từng lớp mới cho Mạng nơ-ron sâu (DNNs) nhằm giải quyết thách thức tối ưu hóa việc giảm thiểu sai lệch đầu ra trong khi tuân thủ ràng buộc tỷ lệ cắt tỉa mục tiêu. Phương pháp của chúng tôi xem xét ảnh hưởng tập thể của tất cả các lớp để thiết kế một lược đồ cắt tỉa thích ứng từng lớp. Chúng tôi khám phá và sử dụng một tính chất cộng tính rất quan trọng của sai lệch đầu ra do cắt tỉa trọng số trên nhiều lớp. Tính chất này cho phép chúng tôi công thức hóa việc cắt tỉa như một bài toán tối ưu hóa tổ hợp và giải hiệu quả thông qua quy hoạch động. Bằng cách phân tách bài toán thành các bài toán con, chúng tôi đạt được độ phức tạp thời gian tuyến tính, làm cho thuật toán tối ưu hóa của chúng tôi nhanh chóng và khả thi để chạy trên CPU. Các thí nghiệm rộng rãi của chúng tôi chứng minh tính ưu việt của phương pháp so với các phương pháp hiện có trên tập dữ liệu ImageNet và CIFAR-10. Trên CIFAR-10, phương pháp của chúng tôi đạt được những cải thiện đáng kể, vượt trội hơn các phương pháp khác lên đến 1.0% cho ResNet-32, 0.5% cho VGG-16, và 0.7% cho DenseNet-121 về độ chính xác top-1. Trên ImageNet, chúng tôi đạt được độ chính xác top-1 cao hơn lên đến 4.7% và 4.6% so với các phương pháp khác cho VGG-16 và ResNet-50 tương ứng. Những kết quả này nêu bật tính hiệu quả và thực tiễn của phương pháp chúng tôi trong việc nâng cao hiệu suất DNN thông qua cắt tỉa trọng số thích ứng từng lớp. Mã nguồn sẽ có sẵn tại https://github.com/Akimoto-Cris/RD_VIT_PRUNE .

1. Giới thiệu
Mạng nơ-ron sâu (DNNs) [22, 34, 35, 17, 19] đóng vai trò quan trọng trong các nhiệm vụ thị giác máy tính khác nhau. Tuy nhiên, để đạt được độ chính xác cao, DNNs thường đòi hỏi số lượng lớn các tham số, điều này làm cho chúng rất tốn năng lượng và khó triển khai trên các thiết bị di động có tài nguyên hạn chế [16, 15]. Cắt tỉa là một trong những cách mạnh mẽ để giảm độ phức tạp của DNNs. Bằng cách loại bỏ các tham số dư thừa, các phép tính có thể được giảm đáng kể (ví dụ, FLOPs), dẫn đến tốc độ nhanh hơn và ít tốn năng lượng hơn. Thông thường, các phương pháp cắt tỉa có thể được chia thành hai loại: cắt tỉa có cấu trúc [14, 1, 9, 31, 18, 30] và cắt tỉa trọng số (không có cấu trúc) [27, 32, 28, 39, 16, 15].

Các phương pháp cắt tỉa có cấu trúc xem xét một kênh hoặc một hạt nhân như một đơn vị cắt tỉa cơ bản, trong khi các phương pháp cắt tỉa trọng số xem xét một trọng số như một đơn vị cắt tỉa cơ bản. Phương pháp trước thân thiện với phần cứng hơn và phương pháp sau có thể đạt được tỷ lệ cắt tỉa cao hơn.

Trong bài báo này, chúng tôi tập trung vào việc cải thiện cắt tỉa trọng số và đề xuất một phương pháp thích ứng từng lớp được tối ưu hóa chung mới để đạt được kết quả tiên tiến giữa FLOPs và độ chính xác. Các khám phá gần đây [10, 13, 25] chứng minh rằng độ thưa thích ứng từng lớp là lược đồ cắt tỉa ưu việt. Tuy nhiên, một nhược điểm trong các phương pháp thích ứng từng lớp trước đây là chúng chỉ xem xét tác động của một lớp đơn lẻ khi quyết định tỷ lệ cắt tỉa của lớp đó. Tác động lẫn nhau giữa các lớp khác nhau bị bỏ qua. Hơn nữa, một thách thức khác là không gian tìm kiếm của tỷ lệ cắt tỉa cho mỗi lớp tăng theo cấp số nhân theo số lớp. Trong một mạng nơ-ron sâu, số lượng lớp có thể là hàng trăm hoặc thậm chí hàng nghìn, điều này làm cho việc tìm giải pháp một cách hiệu quả rất khó khăn.

Trong phương pháp của chúng tôi, chúng tôi định nghĩa một mục tiêu học tập chung để học lược đồ cắt tỉa thích ứng từng lớp. Chúng tôi nhằm mục đích giảm thiểu sai lệch đầu ra của mạng khi cắt tỉa trọng số trên tất cả các lớp dưới ràng buộc của tỷ lệ cắt tỉa mục tiêu. Vì sai lệch đầu ra có liên quan chặt chẽ đến độ chính xác, phương pháp của chúng tôi có thể duy trì độ chính xác ngay cả ở tỷ lệ cắt tỉa cao. Chúng tôi khám phá một tính chất quan trọng của sai lệch đầu ra và tìm thấy rằng tính chất cộng tính [42, 41, 38] có hiệu lực khi chúng tôi cắt tỉa trọng số trên nhiều lớp. Nói cách khác, sai lệch đầu ra do cắt tỉa trọng số của tất cả các lớp

--- TRANG 2 ---
(a) Độ thưa= 0.1.
 (b) Độ thưa= 0.2.
 (c) Độ thưa= 0.5.
 (d) Độ thưa= 0.8.
Hình 1: Một ví dụ về tính chất cộng tính được thu thập trên ResNet-32 trên CIFAR-10. Trục dọc cho thấy sai lệch đầu ra khi chỉ cắt tỉa hai lớp liên tiếp. Trục ngang cho thấy tổng sai lệch đầu ra do cắt tỉa hai lớp liên quan một cách riêng lẻ. Các hình phụ hiển thị các tình huống khi tất cả các lớp trong mô hình được gán với độ thưa tương ứng.

trọng số bằng tổng sai lệch đầu ra do cắt tỉa từng lớp riêng lẻ. Chúng tôi cung cấp một đạo hàm toán học cho tính chất cộng tính bằng cách sử dụng khai triển chuỗi Taylor.

Hơn nữa, sử dụng tính chất cộng tính, chúng tôi phát triển một phương pháp rất nhanh để giải tối ưu hóa thông qua quy hoạch động, chỉ có độ phức tạp thời gian tuyến tính. Chúng tôi viết lại hàm mục tiêu như một bài toán tối ưu hóa tổ hợp. Bằng cách định nghĩa hàm trạng thái và phương trình đệ quy giữa các trạng thái khác nhau, chúng tôi có thể phân tách toàn bộ bài toán thành các bài toán con và giải nó thông qua quy hoạch động. Trong thực tế, phương pháp của chúng tôi có thể tìm giải pháp trong vài phút trên CPU cho các mạng nơ-ron sâu. Lưu ý rằng khác với các thuật toán xấp xỉ khác, quy hoạch động có thể tìm giải pháp tối ưu toàn cục, có nghĩa là phương pháp của chúng tôi cung cấp lược đồ cắt tỉa tối ưu với sai lệch đầu ra tối thiểu. Chúng tôi tóm tắt các đóng góp chính của bài báo như sau:

• Chúng tôi đề xuất một lược đồ cắt tỉa thích ứng từng lớp mới nhằm giảm thiểu chung sai lệch đầu ra khi cắt tỉa trọng số trong tất cả các lớp. Vì sai lệch đầu ra có liên quan chặt chẽ đến độ chính xác, phương pháp của chúng tôi duy trì độ chính xác cao ngay cả khi hầu hết trọng số bị cắt tỉa. Chúng tôi cũng khám phá một tính chất cộng tính quan trọng cho sai lệch đầu ra dựa trên khai triển chuỗi Taylor.

• Chúng tôi phát triển một thuật toán nhanh để giải tối ưu hóa thông qua quy hoạch động. Ý tưởng chính là viết lại hàm mục tiêu như một bài toán tối ưu hóa tổ hợp và sau đó làm giãn toàn bộ bài toán thành các bài toán con có thể giải được. Phương pháp của chúng tôi có thể tìm giải pháp của một mạng nơ-ron sâu trong vài phút.

• Phương pháp của chúng tôi cải thiện các kết quả tiên tiến trên các mạng nơ-ron sâu và tập dữ liệu khác nhau.

Phần còn lại của bài báo được tổ chức như sau. Chúng tôi thảo luận các công trình liên quan trong phần 2. Trong phần 3, chúng tôi phát triển phương pháp chi tiết. Chúng tôi trình bày hàm mục tiêu, phương pháp tối ưu hóa, và phân tích độ phức tạp thời gian của thuật toán. Trong phần cuối, chúng tôi cung cấp các kết quả thí nghiệm toàn diện.

2. Các công trình liên quan
Trọng tâm của công trình này nói chung thuộc về hướng cắt tỉa dựa trên độ lớn (MP) trong nén mô hình của mạng nơ-ron, với các công trình đầu tiên như OBD [24]. MP được thực hiện bằng cách xếp hạng hoặc phạt trọng số theo một tiêu chí nào đó (ví dụ độ lớn) và loại bỏ các trọng số xếp hạng thấp. Nhiều nỗ lực đã được thực hiện kể từ đó trong bối cảnh của [24, 16], có thể được chia thô thành các phương pháp sau tùy thuộc vào thời điểm cắt tỉa được nhúng trong việc huấn luyện mạng.

Cắt tỉa sau huấn luyện. Lược đồ cắt tỉa sau huấn luyện cắt tỉa các tham số mạng sau khi huấn luyện mạng chuẩn, tức là cắt tỉa từ một mô hình hội tụ đã được huấn luyện trước. Trong bối cảnh này, các tham số có thể được cắt tỉa ngay lập tức để đạt được ràng buộc độ thưa mục tiêu (cắt tỉa một lần), hoặc được cắt tỉa dần dần trong quá trình tinh chỉnh mô hình thưa (cắt tỉa lặp). [16] đề xuất một lược đồ cắt tỉa lặp xác định độ thưa theo từng lớp sử dụng heuristic thống kê lớp. [45, 10] áp dụng một ngưỡng cắt tỉa toàn cục qua tất cả các lớp trong mạng để đáp ứng ràng buộc độ thưa mô hình. [5] [33] gộp tất cả các lớp lại với nhau và xác định ngưỡng cắt tỉa cho các lớp khác nhau theo cách tích hợp. [12] đề xuất tua lại trọng số từ giai đoạn cắt tỉa lặp trước đó dựa trên giả thuyết vé số. LAMP[25] đạo hàm một lựa chọn độ thưa theo từng lớp dạng đóng từ một bài toán giảm thiểu sai lệch l2 theo từng lớp được làm giãn tương thích với các lược đồ cắt tỉa sau huấn luyện khác nhau bao gồm cắt tỉa lặp và một lần. PGMPF [4] áp dụng tiêu chí cắt tỉa theo từng lớp dựa trên l2 đơn giản

--- TRANG 3 ---
tiêu chí cắt tỉa và cải thiện các quy tắc che mặt nạ và cập nhật trọng số trong quá trình tinh chỉnh. [6] áp dụng một phương pháp cắt tỉa một lần bằng cách tận dụng các nhóm bất biến zero. [23] đề xuất hiệu chỉnh lại các độ lệch và phương sai của trọng số mô hình và kích hoạt, tương tự như hiệu chỉnh độ lệch được áp dụng rộng rãi trong lượng tử hóa mô hình [11, 2]. [32] trình bày một phương pháp cắt tỉa lặp tận dụng khai triển taylor của loss mô hình và đạo hàm một tiêu chí cắt tỉa dựa trên gradient. Phương pháp của chúng tôi tận dụng khai triển taylor trên sai lệch đầu ra được tham số hóa bởi trọng số lớp, về cơ bản khác với [32]. SuRP [20] áp dụng đệ quy bất đẳng thức tam giác và giả định phân phối laplacian để xấp xỉ sai lệch đầu ra nhằm đạt được tối ưu hóa chung tương tự như chúng tôi. Tuy nhiên, xấp xỉ của chúng tôi trực tiếp hơn và không cần bất kỳ giả định nào về phân phối.

Cắt tỉa tại khởi tạo. Trái ngược với lược đồ trước, có một hướng công trình mới nổi nhằm loại bỏ các kết nối hoặc nơ-ron từ đầu tại bước khởi tạo huấn luyện, với ưu điểm tránh huấn luyện trước và các lịch trình cắt tỉa phức tạp. SNIP [26] cắt tỉa tham số chỉ một lần tại giai đoạn khởi tạo huấn luyện. Độ lớn chuẩn hóa của đạo hàm các tham số được định nghĩa là tiêu chí cắt tỉa. [7] trình bày một metric saliency được sửa đổi dựa trên SNIP [26], cho phép tính toán saliences của các mạng được cắt tỉa một phần. [36] thiết kế dòng gradient khi huấn luyện mạng thưa từ đầu để đạt được hội tụ tốt hơn. Vì cắt tỉa tại khởi tạo nằm ngoài phạm vi nghiên cứu của chúng tôi, người ta có thể tham khảo các khảo sát liên quan [37] để có giới thiệu toàn diện hơn.

Các lược đồ cắt tỉa khác. [3] xen kẽ việc cắt tỉa giữa quá trình huấn luyện bình thường, dần dần cắt tỉa nhiều kết nối và nơ-ron hơn từ mạng. Lược đồ này tương tự như cắt tỉa lặp trước đây, tuy nhiên, ở đây mô hình được huấn luyện từ đầu. ProbMask [43] tương tự tận dụng gradient descent được chiếu với chiến lược cắt tỉa tiến bộ để trực tiếp huấn luyện mạng thưa. [40] tích hợp huấn luyện supermask với độ thưa theo gradient để huấn luyện mạng thưa.

Vì đóng góp chính của chúng tôi là cải thiện tiêu chí cắt tỉa, chúng tôi chủ yếu đánh giá phương pháp của mình dưới các paradigm cắt tỉa không có cấu trúc sau huấn luyện, như cắt tỉa lặp và cắt tỉa một lần. Mặc dù phương pháp của chúng tôi có thể có hiệu quả tiềm năng tương đương trên các cấu trúc độ thưa và lược đồ cắt tỉa khác như Cắt tỉa tại Khởi tạo, chúng tôi để lại những xác minh như vậy cho các công trình tương lai.

3. Phương pháp
Trong phần này, chúng tôi trình bày phương pháp chi tiết. Trước tiên chúng tôi đưa ra công thức của hàm mục tiêu và sau đó cung cấp phương pháp tối ưu hóa. Một tính chất cộng tính được đạo hàm dựa trên xấp xỉ chuỗi Taylor. Chi tiết triển khai quy hoạch động và phân tích độ phức tạp thời gian cũng được cung cấp.

3.1. Hàm mục tiêu
Theo ký hiệu trong [25], cho f ký hiệu một mạng nơ-ron, định nghĩa W(1:l)=
W(1), ..., W(l)
như tất cả các tham số của f, trong đó l là số lớp và W(i) là trọng số trong lớp i. Khi chúng tôi cắt tỉa một phần tham số, chúng tôi sẽ nhận được một mạng nơ-ron được sửa đổi với tập tham số mới ˜W(1:l). Chúng tôi xem tác động của việc cắt tỉa như khoảng cách giữa đầu ra mạng f(x;W(1:l)) và f(x;˜W(1:l)). Mục tiêu học tập là giảm thiểu sai lệch đầu ra do cắt tỉa dưới ràng buộc của tỷ lệ cắt tỉa,

min∥f(x;W(1:l))−f(x;˜W(1:l))∥2s.t.∥˜W(1:l)∥0
∥W(1:l)∥0≤R, (1)

trong đó R ký hiệu tỷ lệ cắt tỉa cho toàn bộ mạng.
Một tính chất quan trọng chúng tôi khám phá là kỳ vọng của sai lệch đầu ra, do cắt tỉa trọng số của tất cả các lớp, bằng tổng kỳ vọng của sai lệch đầu ra do cắt tỉa từng lớp riêng lẻ,

E
∥f(x;W(1:l))−f(x;˜W(1:l))∥2
=lX
i=1E(δi),(2)

trong đó δi ký hiệu sai lệch đầu ra khi chỉ cắt tỉa trọng số trong lớp i.

3.2. Phân tích
Chúng tôi cung cấp một đạo hàm toán học cho tính chất cộng tính. Chúng tôi đưa ra hai giả định sau để chứng minh tính chất cộng tính:

Giả định 1 Khai triển bậc nhất Taylor : Mạng nơ-ron f được tham số hóa bởi W(1:l) khi được cho một nhiễu loạn nhỏ ∆W(1:l) dẫn đến ˜W(1:l)=W(1:l)+ ∆W(1:l) có thể được khai triển như sau:

f(x;˜W(1:l)) =f(x;W(1:l)) +lX
i=1∂f
∂W(i)∆W(i).(3)

Giả định 2 Nhiễu loạn trọng số i.d.d. qua các lớp[44]:∀0< i̸=j < L, E (∆W(i))E(∆W(j)) = 0 .

Theo Eq. (3), δ=∥f(x;W(1:l))−f(x;˜W(1:l))∥2 có thể được viết là

δ=lX
i=1∆W(i)⊤∂f
∂W(i)⊤ lX
j=1∂f
∂W(j)∆W(j)
.(4)

Khi chúng tôi lấy kỳ vọng của Eq. (4) cho cả hai vế, vế phải có thể được mở ra thành các số hạng cộng tính (chuyển vị vector không có tác động bên trong kỳ vọng):

E(δ) =X
1≤i,j≤lE
∆W(i)∂f
∂W(i)
E
∆W(j)∂f
∂W(j)
.
(5)

--- TRANG 4 ---
Hơn nữa, vì đạo hàm ∂f/∂W(i) là một hằng số khi chúng tôi xem xét trọng số mạng cố định đã được huấn luyện, chúng tôi có thể đạo hàm điều sau từ Giả định 2:

E
∆W(i)∂f
∂W(i)
E
∆W(j)∂f
∂W(j)
= 0.(6)

Do đó, các số hạng chéo ( i̸=j) trong Eq. (5) biến mất, thu được:

E(δ) =lX
i=1E
∥∂f
∂W(i)∆W(i)∥2
. (7)

Eq. (7) là kết quả chúng tôi muốn bởi vì, một lần nữa, theo Giả định 1,

∂f
∂W(i)∆W(i)=f(x;W(1:i−1),˜W(i), W(i+1,l))
−f(x;W(1;l)).(8)

Do đó, vế trái của Eq. (7) trở thành sai lệch đầu ra thực δ khi tất cả các lớp được cắt tỉa, và vế phải trở thành tổng sai lệch đầu ra do cắt tỉa riêng lẻ trọng số của từng lớp đơn, có thể được sử dụng để xấp xỉ sai lệch đầu ra.

Chúng tôi đã thực hiện một kiểm tra thực nghiệm về tính chất cộng tính được đề xuất lý thuyết của chúng tôi trên mạng thực. Như được thể hiện trong Hình 1, khi chúng tôi kiểm tra các trường hợp chỉ cắt tỉa hai lớp liền kề mỗi lần trong một mô hình đã được huấn luyện trước, đóng góp vào các số hạng sai lệch có thể cộng vế phải trong khi các lớp khác đóng góp bằng không vào xấp xỉ, chúng tôi quan sát thấy tính cộng tính hoạt động khá tốt với các phần dư biên, trong đó hầu như tất cả các điểm quan sát nằm gần đường đồng nhất.

3.3. Tối ưu hóa thông qua Quy hoạch động
Bằng cách sử dụng tính chất cộng tính, chúng tôi có thể viết lại hàm mục tiêu như một bài toán tối ưu hóa tổ hợp và giải hiệu quả sử dụng quy hoạch động. Hàm mục tiêu được viết lại là,

minδ1+δ2+...+δls.t. t 1+t2+...+tl=T,(9)

trong đó T ký hiệu tổng số trọng số cần cắt tỉa và ti ký hiệu số trọng số cần cắt tỉa trong lớp i. Chúng tôi giải (9) bằng cách phân tách toàn bộ bài toán thành các bài toán con. Ý tưởng cơ bản là chúng tôi định nghĩa một hàm trạng thái và tìm phương trình đệ quy giữa các trạng thái. Bài toán được giải dựa trên phương trình đệ quy.

Cụ thể, định nghĩa g như hàm trạng thái, trong đó gj
i có nghĩa là sai lệch tối thiểu gây ra khi cắt tỉa j trọng số tại i lớp đầu tiên. Mục tiêu của chúng tôi là tính gT
l.

Để khởi tạo, chúng tôi có,
gj
1=δ1(j), cho 1≤j≤T, (10)

Thuật toán 1 Tối ưu hóa thông qua quy hoạch động.
Đầu vào: Sai lệch đầu ra δi(j) khi cắt tỉa j trọng số trong lớp đơn i, cho 1≤i≤l và 1≤j≤T.
Đầu ra: Số trọng số pi được cắt tỉa trong lớp i.
Khởi tạo sai lệch đầu ra tối thiểu gj
i= 0 khi cắt tỉa j trọng số trong i lớp đầu tiên.
Khởi tạo hàm trạng thái sj
i=−1 trong đó sj
i ký hiệu số trọng số được cắt tỉa trong lớp i khi cắt tỉa j trọng số trong i lớp đầu tiên.
cho i từ 1 đến l thực hiện
cho j từ 0 đến T thực hiện
Nếu i= 1:gj
1=δ1(j),sj
1=j.
Khác: gj
i= min {gj−k
i−1+δi(k)},sj
i= arg mink{gj
i}.
kết thúc cho
kết thúc cho
Số trọng số được cắt tỉa trong lớp l là pl=sT
l.
Cập nhật T=T−sT
l.
cho i từ l−1 đến 1 thực hiện
Số trọng số được cắt tỉa trong lớp i là pi=sT
i.
Cập nhật T=T−sT
i.
kết thúc cho

trong đó δi(j) ký hiệu sai lệch gây ra khi cắt tỉa j trọng số tại lớp i. Sau đó chúng tôi có phương trình đệ quy giữa các trạng thái gi và gi−1, đó là,

gj
i= min {gj−k
i−1+δi(k)}, trong đó 1≤k≤j. (11)

Các hàm trạng thái được tính dựa trên phương trình (11) theo cách từ dưới lên từ g1 đến gl. Trong thực tế, chúng tôi cần một biến khác s để lưu trữ quyết định của mỗi trạng thái để biết số trọng số được cắt tỉa trong mỗi lớp. s được định nghĩa là

sj
i= arg min
k{gj
i=gj−k
i−1+δi(k)}. (12)

Thuật toán 1 hiển thị mã giả để tính hàm trạng thái và tìm giải pháp cắt tỉa.

3.4. Phân tích độ phức tạp thời gian
Độ phức tạp thời gian của thuật toán tối ưu hóa sử dụng quy hoạch động là O(l×T2), vì chúng tôi có l×T trạng thái khác nhau, và mỗi trạng thái cần liệt kê số trọng số được cắt tỉa trong một lớp. Trong thực tế, thuật toán này rất nhanh chỉ mất vài giây trên CPU cho các mạng nơ-ron sâu. Chúng tôi hiển thị kết quả chi tiết về tốc độ trong phần thí nghiệm.

4. Kết quả thí nghiệm
Chi tiết triển khai. Vì đóng góp của chúng tôi đối với các lược đồ cắt tỉa hiện có là về lựa chọn độ thưa theo từng lớp, chúng tôi đánh giá phương pháp cắt tỉa dựa trên tỷ lệ-sai lệch của mình dưới các cài đặt thí nghiệm khác nhau, bao gồm cắt tỉa lặp và cắt tỉa một lần, cũng như trên nhiều kiến trúc mạng

--- TRANG 5 ---
[Bảng kết quả phức tạp với nhiều cột về Dataset, Arch, Method, Sparsity (%), Remaining FLOPs (%), Top-1 (%) ↑, Top-1 drop (%)↓ cho CIFAR-10 với ResNet-32, VGG-16, DenseNet-121 và ImageNet với VGG-16-BN, ResNet-50]

Bảng 1: Kết quả cắt tỉa lặp. In đậm ký hiệu độ chính xác Top-1 cao nhất hoặc độ giảm chính xác thấp nhất trong số các kết quả với khoảng FLOPs còn lại tương tự; Đỏ ký hiệu Top-1 cao nhất trong số các kết quả có độ thưa tương tự. Dense ký hiệu độ chính xác Top-1 của mô hình chưa cắt tỉa.

--- TRANG 6 ---
(a) ResNet-32 trên CIFAR-10.
 (b) DenseNet-121 trên CIFAR-10.
(c) VGG-16 trên ImageNet.
 (d) ResNet-50 trên ImageNet.
Hình 2: Quá trình cắt tỉa lặp của các mô hình phân loại và tập dữ liệu khác nhau.

kiến trúc và tập dữ liệu phân loại hình ảnh. Chúng tôi xem xét 3 mô hình trên tập dữ liệu CIFAR-10 [21], tức là VGG-16 theo các kiến trúc được điều chỉnh trong [25], ResNet-32 [17], DenseNet-121 [19], trong khi trên tập dữ liệu ImageNet [8], chúng tôi đánh giá VGG-16 với BatchNorm [34] và ResNet-50 [17]. Trên CIFAR-10, theo phương pháp baseline [25], chúng tôi thực hiện năm thử nghiệm độc lập cho mỗi phương pháp, và chúng tôi báo cáo trung bình và độ lệch chuẩn giữa các thử nghiệm. Trên ImageNet quy mô lớn hơn nhiều, chúng tôi chỉ thực hiện một thử nghiệm cho mỗi phương pháp. Để biết chi tiết triển khai khác, vui lòng tham khảo tài liệu bổ sung.

Chi tiết khi tạo đường cong tỷ lệ-sai lệch. Trong các thí nghiệm, chúng tôi cần tạo đường cong tỷ lệ-sai lệch cho mỗi lớp để cho phép tối ưu hóa độ thưa, trong đó các điểm trên đường cong là một cặp mức độ thưa và sai lệch đầu ra mô hình khi lớp nhất định được cắt tỉa đến độ thưa đó. Đối với lược đồ không miễn dữ liệu, các đường cong được lấy mẫu trên một tập hiệu chỉnh được chọn ngẫu nhiên từ tập dữ liệu huấn luyện, trong khi cũng có thể làm cho nó miễn dữ liệu bằng cách tận dụng dữ liệu tổng hợp được lấy mẫu từ phân phối nhất định, ví dụ phân phối chuẩn tiêu chuẩn. Kích thước của tập hiệu chỉnh được đặt thành 1024 mẫu cho CIFAR-10 và 256 cho ImageNet tương ứng. Tuy nhiên, các đường cong tỷ lệ-sai lệch thu được bằng quy trình trên có thể bị can thiệp bởi các yếu tố thế giới thực dẫn đến đường cong nhiễu. Do đó, chúng tôi thiết kế các chiến lược khác nhau để tinh chỉnh các đường cong tỷ lệ-sai lệch thô và hỗ trợ tối ưu hóa tốt hơn sau đó. Cụ thể, (1) Lấy mẫu trường hợp xấu nhất : lấy cảm hứng từ LAMP [25], chúng tôi tính sai lệch như norm bình phương tối đa giữa tất cả các mẫu hiệu chỉnh thay vì tính MSE cho toàn bộ tập hiệu chỉnh; (2) Lọc ngoại lai: xử lý các điểm cực đại địa phương trên đường cong phá vỡ tính đơn điệu như nhiễu ngoại lai và loại bỏ chúng để tạo thuận lợi cho Thuật toán 1, đặc biệt để thực hiện hiệu quả Eq. (12). Chúng tôi cung cấp các nghiên cứu ablation trong Sec. 4.4 sau để thảo luận các hiệu ứng riêng của các chiến lược này.

4.1. Kết quả cắt tỉa lặp
Trong lược đồ cắt tỉa lặp, người ta bắt đầu với một mô hình dung lượng đầy đủ đã được huấn luyện trước. Trong quá trình tinh chỉnh của mô hình đã huấn luyện trước, chúng tôi dần dần cắt tỉa các tham số khỏi mô hình một lượng nhất định tại mỗi giai đoạn lặp.

--- TRANG 7 ---
Dưới các giai đoạn khác nhau của cắt tỉa lặp, chúng tôi có một tập hợp các mô hình thưa với độ thưa tăng dần và độ phức tạp tính toán giảm dần (FLOPs). Theo cài đặt cắt tỉa lặp trong LAMP [25], chúng tôi cắt tỉa 20% tham số còn lại từ mô hình mỗi lần sau một vòng tinh chỉnh. Thiết lập siêu tham số của tinh chỉnh được chi tiết trong tài liệu bổ sung. Bảng 1 so sánh kết quả độ chính xác mô hình được tạo ra trong quá trình cắt tỉa lặp bằng phương pháp của chúng tôi và đối tác phương pháp cắt tỉa khác.

Với việc áp dụng không chuẩn hóa các mô hình CNN cho thí nghiệm trong các công trình cắt tỉa sau huấn luyện, chúng tôi đã kiểm tra càng nhiều mô hình càng tốt như xuất hiện trong các tài liệu khác nhau và thêm chúng làm baseline trong so sánh của chúng tôi, bao gồm Global [33], Uniform [45], Uniform+ [13], LAMP [25], E-R ker. [10], đây là một phương pháp Erdős-Rényi mở rộng cho cắt tỉa CNN, trong đó độ thưa theo từng lớp được chọn bằng một tiêu chí dạng đóng chỉ phụ thuộc vào kiến trúc lớp (ví dụ, số kênh đầu vào và đầu ra, kích thước kernel tích chập). Hình 2 tiếp tục chứng minh các quy trình cắt tỉa lặp chi tiết của các phương pháp khác nhau, trong đó FLOPs còn lại (trục X) dần giảm trong quá trình tinh chỉnh.

Kết quả trên CIFAR. Từ Bảng 1, chúng tôi quan sát thấy phương pháp của chúng tôi liên tục tạo ra các mô hình được cắt tỉa với hiệu suất kiểm tra cao hơn và ít giảm độ chính xác kiểm tra hơn với cùng độ phức tạp tính toán (FLOPs) so với các phương pháp khác. Hình 2 tiếp tục xác minh rằng quan sát này đúng trong suốt quy trình cắt tỉa. Ví dụ cho ResNet-32 trên CIFAR-10, phương pháp của chúng tôi đạt được độ chính xác Top-1 trung bình 92.56 với 11.25% FLOPs còn lại, trong khi kết quả baseline [25] chỉ là 90.04 với 11% FLOPs; Khi FLOPs còn lại khoảng 3%, chúng tôi thậm chí cải thiện độ chính xác 7.17, tức là chỉ giảm 3.16 độ chính xác với chỉ 4.4% tham số sống sót. Đối với VGG-16 trên CIFAR-10, chúng tôi cũng quan sát được kết quả tương tự, trong đó phương pháp của chúng tôi đạt được ít giảm độ chính xác nhất trong các đối tác khác nhau, ví dụ, khi FLOPs trong phạm vi 33±2%, mà không có thiết kế nâng cao của che mặt nạ gradient mềm và các chiến lược cập nhật trọng số được áp dụng trong [4], phương pháp của chúng tôi đạt được -1.05% giảm Top-1 với 35.49% FLOPs, có nghĩa là mạng được cắt tỉa hoạt động tốt hơn 1.05% so với mạng chưa cắt tỉa. PGMPF [4] đạt được điểm độ chính xác cao hơn chúng tôi trên mô hình VGG-16 với 33% FLOPs còn lại, được thu được từ mô hình chưa cắt tỉa có hiệu suất cao hơn, nhưng vẫn kém hiệu quả hơn chúng tôi về giảm độ chính xác (Top-1 giảm 0.08%).

Kết quả trên ImageNet. Trên tập dữ liệu quy mô lớn hơn ImageNet, chúng tôi cũng quan sát được các hành vi tương tự từ phương pháp của mình. Đối với VGG16-BN, chúng tôi vượt trội hơn các phương pháp khác trên cả nhóm 35±2% và 16±2 FLOPs. Đáng chú ý, khi độ thưa mô hình cao đến 98.85%, tức là chỉ 1.15% tham số sống sót, phương pháp của chúng tôi vẫn có 59.41% độ chính xác, trong khi LAMP đã giảm xuống khoảng 52. Điều này cũng được quan sát thấy trên ResNet-50, nơi chúng tôi vượt trội hơn LAMP một khoảng cách lớn ở nhóm 6% FLOPs. Từ Hình 2c, có một quan sát nhỏ rằng mặc dù liên tục có độ chính xác kiểm tra cao hơn với <50% FLOPs, VGG-16-BN hoạt động hơi thấp hơn trong phạm vi 30 50% FLOPs trước khi tăng trở lại trong các lần lặp tinh chỉnh tiếp theo. Người ta suy đoán rằng VGG-16-BN nhạy cảm hơn với các thay đổi cấu trúc lớn đối với cắt tỉa sau huấn luyện.

Nhìn chung, đối với cả hai tập dữ liệu, chúng tôi quan sát thấy phương pháp của mình tạo ra các mô hình thưa có độ chính xác cao hơn với cùng ràng buộc FLOPs hoặc độ thưa.

4.2. Kết quả cắt tỉa một lần
[THIS IS TABLE: Bảng 2 hiển thị kết quả cắt tỉa một lần của ResNet-50 trên ImageNet với các phương pháp khác nhau]

Trong lược đồ cắt tỉa một lần, chúng tôi trực tiếp cắt tỉa mô hình đến ràng buộc tính toán hoặc tham số mục tiêu, theo sau bởi một lần tinh chỉnh. Bảng 2 tóm tắt kết quả cắt tỉa một lần sử dụng các thuật toán cắt tỉa không có cấu trúc khác nhau. Chúng tôi thực hiện so sánh trên ResNet-50 trên ImageNet. Kết quả xác minh rằng phương pháp của chúng tôi vẫn phù hợp với lược đồ cắt tỉa một lần, với độ chính xác cao hơn ở 34.5% FLOPs so với cả hai baseline [25, 6].

4.3. Cắt tỉa không dữ liệu
[THIS IS TABLE: Bảng 3 hiển thị kết quả cắt tỉa một lần không dữ liệu của ResNet-50 trên tập dữ liệu ImageNet]

Để đánh giá xem phương pháp của chúng tôi có tương thích với lược đồ cắt tỉa không dữ liệu hay không, điều này hứa hẹn đạt được khả năng tổng quát hóa tốt hơn so với các lược đồ cắt tỉa tiêu chuẩn thường phụ thuộc vào dữ liệu, chúng tôi cố gắng áp dụng phương pháp của mình vào cắt tỉa không dữ liệu, bằng cách thay thế tập hình ảnh hiệu chỉnh được lấy mẫu từ tập kiểm tra thực bằng nhiễu trắng (pixel trong mỗi kênh màu được tạo độc lập bởi cùng phân phối được yêu cầu bởi mô hình phân loại, ví dụ, phân phối chuẩn hóa tiêu chuẩn N(0,1)).

Bảng 3 tóm tắt kết quả của biến thể không dữ liệu của phương pháp chúng tôi so với kết quả baseline [23] với cùng chiến lược tổng hợp dữ liệu (nhiễu-trắng). Chúng tôi cũng bao gồm LAMP [25] trong so sánh vì nó tình cờ không yêu cầu tập hiệu chỉnh để có được ngưỡng cắt tỉa theo từng lớp tốt. Phương pháp của chúng tôi vẫn đạt được kết quả ưu việt dưới kịch bản không dữ liệu, chỉ với 1.01% giảm hiệu suất. Điều này nằm trong kỳ vọng của chúng tôi vì thuật toán dựa trên lý thuyết tỷ lệ-sai lệch của chúng tôi không phụ thuộc vào bất kỳ phân phối dữ liệu đầu vào cụ thể nào.

4.4. Nghiên cứu Ablation
[THIS IS TABLE: Bảng 4, 5, 6 hiển thị các so sánh ablation và chiến lược hậu xử lý khác nhau]

Vì đóng góp chính của chúng tôi là chiến lược tối ưu hóa chung, trước tiên chúng tôi tiến hành so sánh với trường hợp không sử dụng tối ưu hóa chung trong đó chúng tôi trực tiếp giải độ thưa theo từng lớp trên các đặc trưng đầu ra của mỗi lớp, dẫn đến hiệu suất được thể hiện trong Bảng 4. Như được chỉ ra trong bảng, chúng tôi quan sát hiệu suất xấu đi cho tối ưu hóa một lớp như vậy trên cả hai mô hình được kiểm tra, cho thấy rằng chiến lược tối ưu hóa chung của chúng tôi là tối ưu.

Chúng tôi cũng đánh giá hiệu quả riêng của các chiến lược tinh chỉnh đường cong tỷ lệ-sai lệch đã nêu. Chúng tôi thực hiện đầu tiên ablation trên tập dữ liệu CIFAR-10. Từ Bảng 5, chúng tôi quan sát thấy ở cùng độ thưa mô hình 89%, tương đối cao cho lược đồ cắt tỉa một lần, cả hai chiến lược được chứng minh hoạt động tích cực cho phương pháp của chúng tôi. Do đó, chúng tôi bao gồm cả hai chiến lược để tiến hành thí nghiệm kết quả chính. Chúng tôi cũng quan sát tương tự trên tập dữ liệu ImageNet, như được thể hiện trong Bảng 6. Đặc biệt, chiến lược Lọc ngoại lai mang lại cải thiện nhiều hơn một chút trên cả CIFAR-10 và ImageNet, trong khi Lấy mẫu trường hợp xấu nhất không tạo ra sự khác biệt ở mục tiêu độ thưa cụ thể này.

4.5. Các thảo luận khác
Cũng có một quan sát thú vị từ Bảng 1 rằng với cùng độ thưa mô hình, phương pháp của chúng tôi liên tục giảm nhiều FLOPs hơn từ mô hình. Để phân tích tốt hơn hiện tượng này, chúng tôi xem xét kỹ hơn giải pháp độ thưa theo từng lớp được đưa ra bởi các phương pháp khác nhau. Như được thể hiện trong Hình 3, phương pháp của chúng tôi cắt tỉa nhiều tham số hơn từ các lớp sâu hơn so với LAMP [25]. Vì các kích hoạt trong các lớp sâu hơn trong CNN thường có nhiều kênh và đặc trưng hơn các lớp nông, việc cắt tỉa nhiều tham số hơn từ các lớp sâu sẽ giảm nhiều phép tính hơn, dẫn đến ít FLOPs còn lại hơn. Từ Hình 3, một quan sát khác là cả hai phương pháp đều cắt tỉa nhiều tham số hơn từ lớp cuối của ResNet-32 là lớp kết nối đầy đủ, ngụ ý rằng các tham số của lớp cuối chứa dư thừa lớn. Đồng thời, chúng tôi quan sát thấy DenseNet-121 trên CIFAR-10 không hiển thị hiện tượng trên, trong đó phương pháp của chúng tôi giảm cùng mức FLOPs so với LAMP dưới cùng độ thưa. Chúng tôi giải thích chi tiết điều này trong tài liệu bổ sung.

4.6. Độ phức tạp thời gian
Chúng tôi cung cấp phân tích độ phức tạp thời gian tối ưu hóa thực nghiệm trong Bảng 7. Trong thực tế, chúng tôi sử dụng thuật toán tìm kiếm tam phân để tìm kiếm giải pháp của sj
i trong Eq. (12), có độ phức tạp thời gian logarit với phạm vi tìm kiếm. Trên các tập dữ liệu nhỏ như CIFAR-10, với 35 lớp, phương pháp của chúng tôi mất ít hơn một giây để tính độ thưa theo từng lớp,

--- TRANG 9 ---
(a) LAMP.
 (b) Chúng tôi.
Hình 3: Thống kê độ thưa theo từng lớp của ResNet-32 trên CIFAR-10 của các phương pháp khác nhau trong quá trình cắt tỉa lặp. Chiều cao của các thanh ký hiệu tỷ lệ cắt tỉa với độ thưa mô hình {0.36,0.74,0.89,0.96,0.98}.

[THIS IS TABLE: Bảng 7 - Thời gian dành cho tối ưu hóa độ thưa theo từng lớp]
Cấu hình | Số lớp | Độ thưa (%) | Thời gian (s)
ResNet-32@CIFAR-10 | 35 | 20 | 0.46±0.09
ResNet-50@ImageNet | 54 | 50 | 2.08±0.21

trong khi trên ImageNet lớn hơn, phương pháp của chúng tôi vẫn chỉ mất vài giây.

[THIS IS TABLE: Bảng 8 - So sánh chi phí thời gian tạo đường cong RD và tối ưu hóa]
Cấu hình | Tạo đường cong (s) | Tối ưu (s)
ResNet-18@CIFAR-10 | 1052.64 | 0.84
VGG-16@CIFAR-10 | 664.19 | 2.20

Chúng tôi cũng phân tích chi phí tạo đường cong. Đối với mỗi lớp, chúng tôi duyệt qua tất cả các mẫu tập hiệu chỉnh để tính sai lệch đầu ra ở tất cả các mức độ thưa để tạo đường cong tỷ lệ-sai lệch. Do đó, chi phí tạo đường cong tỷ lệ-sai lệch trở thành O(lSN), trong đó l là số lớp, S là số mức độ thưa (chúng tôi đặt S= 100 trong thực tế), và N là kích thước của tập hiệu chỉnh. Chúng tôi cung cấp chi phí thời gian thực tế cho hai mô hình CIFAR-10 trong Bảng 8. Trong thực tế, chúng tôi đã sử dụng dataloader được tối ưu hóa và tạo đường cong song song của các lớp khác nhau để cắt giảm thời gian suy luận mỗi mẫu.

4.7. Phân tích lỗi xấp xỉ
Với tính chất địa phương của khai triển taylor, chúng tôi mong đợi một sự khác biệt ngày càng tăng của xấp xỉ taylor dưới sai lệch lớn. Chúng tôi phân tích lỗi xấp xỉ thực nghiệm trong Hình 4. Hình bên trái hiển thị mối quan hệ giữa sai lệch đầu ra xấp xỉ dựa trên taylor (trục X) và sai lệch đầu ra thực (trục Y), chúng tôi nhận thấy rằng các điểm dữ liệu trong hình rất gần với đường chéo. Hình bên phải vẽ lỗi xấp xỉ ở các mức độ thưa khác nhau. Lỗi xấp xỉ tăng lên ở độ thưa lớn, ví dụ >50%.

Hình 4: Phân tích lỗi xấp xỉ thực nghiệm.

5. Kết luận
Chúng tôi đã trình bày một tiêu chí cắt tỉa không có cấu trúc dựa trên tỷ lệ-sai lệch mới. Chúng tôi đã tiết lộ tính cộng tính sai lệch đầu ra của việc cắt tỉa không có cấu trúc các mô hình CNN, được hỗ trợ bởi lý thuyết và thí nghiệm. Chúng tôi đã khai thác tính chất này để đơn giản hóa bài toán tối ưu hóa độ thưa theo từng lớp NP-hard thành một tiêu chí cắt tỉa nhanh chỉ với độ phức tạp O(l×T2). Hưởng lợi từ việc tối ưu hóa trực tiếp trên sai lệch đầu ra, tiêu chí được đề xuất của chúng tôi thể hiện tính ưu việt so với các phương pháp hiện có trong các lược đồ cắt tỉa sau huấn luyện khác nhau. Tiêu chí của chúng tôi ưu tiên cắt tỉa các lớp sâu và lớn, dẫn đến giảm đáng kể kích thước mô hình và FLOPs.

Lời cảm ơn
Nghiên cứu này được hỗ trợ bởi Cơ quan Khoa học, Công nghệ và Nghiên cứu (A*STAR) dưới Quỹ của nó (Số dự án A1892b0026 và C211118009 và Quỹ chương trình MTC (Số tài trợ M23L7b0021)). Bất kỳ ý kiến, phát hiện và kết luận hoặc khuyến nghị nào được thể hiện trong tài liệu này là của (các) tác giả và không phản ánh quan điểm của A*STAR.

--- TRANG 10 ---
Tài liệu tham khảo
[1] Alireza Aghasi, Afshin Abdi, Nam Nguyen, and Justin Romberg. Net-trim: Convex pruning of deep neural networks with performance guarantee. Advances in neural information processing systems, 30, 2017.
[2] Ron Banner, Yury Nahshan, and Daniel Soudry. Post training 4-bit quantization of convolutional networks for rapid-deployment. Advances in Neural Information Processing Systems, 32, 2019.
[3] Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training very sparse deep networks. In International Conference on Learning Representations, 2018.
[4] Linhang Cai, Zhulin An, Chuanguang Yang, Yangchun Yan, and Yongjun Xu. Prior gradient mask guided pruning-aware fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 1, 2022.
[5] Miguel A Carreira-Perpinán and Yerlan Idelbayev. "learning-compression" algorithms for neural net pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8532–8541, 2018.
[6] Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin Shi, Sheng Yi, and Xiao Tu. Only train once: A one-shot neural network training and pruning framework. Advances in Neural Information Processing Systems, 34:19637–19651, 2021.
[7] Pau de Jorge, Amartya Sanyal, Harkirat Behl, Philip Torr, Grégory Rogez, and Puneet K. Dokania. Progressive skeletonization: Trimming more fat from a network at initialization. In International Conference on Learning Representations, 2021.
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.
[9] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. Advances in Neural Information Processing Systems, 30, 2017.
[10] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pages 2943–2952. PMLR, 2020.
[11] Alexander Finkelstein, Uri Almog, and Mark Grobman. Fighting quantization bias with bias. arXiv preprint arXiv:1906.03193, 2019.
[12] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2019.
[13] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019.
[14] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. Advances in neural information processing systems, 29, 2016.
[15] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In ICLR, 2016.
[16] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[18] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE international conference on computer vision, pages 1389–1397, 2017.
[19] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708, 2017.
[20] Berivan Isik, Tsachy Weissman, and Albert No. An information-theoretic justification for model pruning. In International Conference on Artificial Intelligence and Statistics, pages 3821–3846. PMLR, 2022.
[21] Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
[22] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.
[23] Ivan Lazarevich, Alexander Kozlov, and Nikita Malinin. Post-training deep neural network pruning via layer-wise calibration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 798–805, 2021.
[24] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989.
[25] Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin. Layer-adaptive sparsity for the magnitude-based pruning. In International Conference on Learning Representations, 2020.
[26] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. Snip: Single-shot network pruning based on connection sensitivity. In International Conference on Learning Representations, 2018.
[27] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
[28] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. Advances in neural information processing systems, 30, 2017.
[29] Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning with feedback. In International Conference on Learning Representations, 2020.
[30] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE international conference on computer vision, pages 2736–2744, 2017.

--- TRANG 11 ---
[31] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In Proceedings of the IEEE international conference on computer vision, pages 5058–5066, 2017.
[32] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.
[33] Ari Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian. One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers. Advances in neural information processing systems, 32, 2019.
[34] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, May 2015.
[35] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.
[36] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. In International Conference on Learning Representations, 2019.
[37] Huan Wang, Can Qin, Yue Bai, Yulun Zhang, and Yun Fu. Recent advances on neural network pruning at initialization. arXiv e-prints, pages arXiv–2103, 2021.
[38] Zhe Wang, Jie Lin, Xue Geng, Mohamed M Sabry Aly, and Vijay Chandrasekhar. Rdo-q: Extremely fine-grained channel-wise quantization via rate-distortion optimization. In European Conference on Computer Vision, pages 157–172. Springer, 2022.
[39] Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural networks using energy-aware pruning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5687–5695, 2017.
[40] Yuxin Zhang, Mingbao Lin, Mengzhao Chen, Fei Chao, and Rongrong Ji. Optg: Optimizing gradient-driven criteria in network sparsity. arXiv preprint arXiv:2201.12826, 2022.
[41] Wang Zhe, Jie Lin, Mohamed Sabry Aly, Sean Young, Vijay Chandrasekhar, and Bernd Girod. Rate-distortion optimized coding for efficient cnn compression. In 2021 Data Compression Conference (DCC), pages 253–262. IEEE, 2021.
[42] Wang Zhe, Jie Lin, Vijay Chandrasekhar, and Bernd Girod. Optimizing the bit allocation for compression of weights and activations of deep neural networks. In 2019 IEEE International Conference on Image Processing (ICIP), pages 3826–3830. IEEE, 2019.
[43] Xiao Zhou, Weizhong Zhang, Hang Xu, and Tong Zhang. Effective sparsification of neural networks with global sparsity constraint. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3599–3608, 2021.
[44] Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal Frossard. Adaptive quantization for deep neural network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.
[45] Michael H Zhu and Suyog Gupta. To prune, or not to prune: Exploring the efficacy of pruning for model compression. 2018.
