# 2306.03805.pdf
# Chuyển đổi từ PDF sang TXT  
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/pruning/2306.03805.pdf
# Kích thước tệp: 1243835 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Sự Xuất Hiện của Độ Thưa Thiết Yếu trong
Các Mô Hình Tiền Huấn Luyện Lớn: Những Trọng Số Quan Trọng
Ajay Jaiswal, Shiwei Liu, Tianlong Chen, Zhangyang Wang
Nhóm VITA, Đại học Texas tại Austin
{ajayjaiswal, shiwei.liu, tianlong.chen, atlaswang}@utexas.edu

Tóm tắt
Các transformer tiền huấn luyện lớn là những người chiếm spotlight trong học sâu hiện đại, và việc hiểu được các mẫu tiết kiệm tồn tại trong chúng trở nên quan trọng khi chúng tăng về quy mô. Với số lượng tham số bùng nổ, Giả thuyết Vé Số (LTH) và các biến thể của nó đã mất đi tính thực tế trong việc làm thưa chúng do tắc nghẽn tính toán và bộ nhớ cao của quy trình huấn luyện-cắt tỉa-huấn luyện lại lặp đi lặp lại của cắt tỉa độ lớn lặp (IMP) - điều này trở nên tệ hơn khi kích thước mô hình tăng. Bài báo này nghiên cứu toàn diện các mẫu thưa được tạo ra trên nhiều transformer tiền huấn luyện lớn về thị giác và ngôn ngữ. Chúng tôi đề xuất sự tồn tại của "độ thưa thiết yếu" được định nghĩa bằng một điểm giảm mạnh mà sau đó hiệu suất giảm nhanh hơn nhiều so với sự tăng của mức độ thưa khi chúng tôi trực tiếp loại bỏ các trọng số có độ lớn nhỏ nhất trong một lần mà không cần huấn luyện lại. Chúng tôi cũng thấy độ thưa thiết yếu vẫn hợp lệ cho các mẫu thưa N:M cũng như trên các mô hình ngôn ngữ lớn quy mô hiện đại (Vicuna-7B). Chúng tôi cũng trình bày một hiện tượng xuất hiện hấp dẫn về sự thưa hóa đột ngột trong quá trình tiền huấn luyện của BERT, tức là BERT đột nhiên trở nên thưa nhiều trong tiền huấn luyện sau một số lần lặp nhất định. Hơn nữa, các quan sát của chúng tôi cũng chỉ ra một phát hiện phản trực giác rằng BERT được huấn luyện với lượng dữ liệu tiền huấn luyện lớn hơn có xu hướng có khả năng tốt hơn trong việc cô đặc kiến thức trong tương đối ít tham số hơn. Cuối cùng, chúng tôi điều tra ảnh hưởng của mất mát tiền huấn luyện đối với độ thưa thiết yếu và phát hiện rằng các mục tiêu học tự giám sát (SSL) kích hoạt tính chất thưa hóa xuất hiện mạnh hơn so với học có giám sát (SL). Mã nguồn của chúng tôi có sẵn tại https://github.com/VITA-Group/essential_sparsity.

1 Giới thiệu
Sự gia tăng to lớn về quy mô thường thấm vào các hệ thống với hành vi mới độc đáo. Transformer [1], nhanh chóng sau khi được giới thiệu đang mở rộng quy mô hàng ngày, cải thiện đáng kể hiệu suất tiên tiến trên một loạt rộng các ứng dụng thị giác máy tính [2,3,4,5,6,7], xử lý ngôn ngữ tự nhiên [8,9,10,11,12,13,14,15,16] trong thế giới thực và bảng xếp hạng. Với sự bùng nổ đáng kinh ngạc của số lượng tham số (từ hàng triệu đến hàng tỷ) trong vài năm qua, trong khi theo đuổi lợi ích hiệu suất, việc tinh chỉnh các mô hình tiền huấn luyện lớn này với phần cứng không tiêu chuẩn công nghiệp đang trở nên dường như không thể, ngoài việc suy luận đắt đỏ và chi phí môi trường cao. Ví dụ, GPT-3 [17] chứa 175 tỷ tham số và cần ít nhất năm GPU A100 80GB để suy luận [18].

Trong cuộc chạy đua xây dựng các mô hình khổng lồ, một lĩnh vực song song và đang phát triển của nén mô hình đã khám phá triển vọng nén các mô hình khổng lồ này với chi phí hy sinh hiệu suất nhỏ/không có. Trong số nhiều nỗ lực nén mô hình và tăng tốc suy luận, cắt tỉa mạng [19,20,21,22,23,24,25,26], loại bỏ các thành phần ít quan trọng nhất khỏi mô hình, thay đổi từ độ chi tiết thấp (ví dụ: trọng số riêng lẻ) đến độ chi tiết cao (như khối, bộ lọc và đầu attention), nổi bật như một trong những kỹ thuật hiệu quả nhất. Mặc dù có

Bản thảo trước. Đang xem xét. arXiv:2306.03805v2 [cs.LG] 9 Aug 2023

--- TRANG 2 ---
sự thành công to lớn của Giả thuyết Vé Số (LTH) [27] và các biến thể của nó cho các mạng nơ-ron quy mô nhỏ, vẫn tồn tại một khoảng cách lớn trong việc áp dụng thực tế cho các transformer tiền huấn luyện lớn với hàng tỷ tham số do quy trình huấn luyện dày đặc hoàn toàn của cắt tỉa độ lớn lặp (IMP), điều này trở nên tồi tệ hơn với sự gia tăng dung lượng mô hình. Gần đây, nhiều công trình [28,29,30,20] đang điều tra các chiến lược để làm cho IMP thực tế cho các mô hình tiền huấn luyện lớn. Tuy nhiên, chúng vẫn dựa vào quy trình huấn luyện-cắt tỉa-huấn luyện lại của IMP và các tác vụ downstream để xác định mạng con. Trong khi theo đuổi mặt nạ thưa tiêu chuẩn vàng với các phương pháp tính toán đắt đỏ, việc hiểu các mẫu thưa chất lượng cao đã tồn tại trong các transformer lớn này là cực kỳ quan trọng.

Trong công trình của chúng tôi, chúng tôi tiết lộ và nghiên cứu một tính chất hấp dẫn, trước đây bị bỏ qua của các transformer tiền huấn luyện lớn - "độ thưa thiết yếu". Nói một cách đơn giản (định nghĩa chính thức trong Mục 3), chúng tôi định nghĩa độ thưa thiết yếu như điểm giảm mạnh, sau đó hiệu suất tinh chỉnh sau cắt tỉa một lần giảm nhanh hơn nhiều so với sự tăng mức độ thưa. Chúng tôi trực tiếp loại bỏ các trọng số có độ lớn nhỏ nhất trong một lần từ các checkpoint tiền huấn luyện, do đó mặt nạ mạng con được xác định không cần chi phí tính toán bổ sung để phát hiện và vẫn giống hệt nhau trên tất cả các tác vụ downstream.

Độ thưa thiết yếu là một tính chất được tạo ra quan trọng nhưng chưa được nghiên cứu kỹ của các transformer tiền huấn luyện lớn, cho thấy rằng bất cứ lúc nào, một tỷ lệ đáng kể các trọng số trong chúng có thể được loại bỏ miễn phí, mặc dù tỷ lệ có thể thay đổi tùy thuộc vào độ phức tạp của tác vụ downstream. Công trình của chúng tôi, lần đầu tiên, tiến hành một nghiên cứu toàn diện về sự tồn tại của độ thưa thiết yếu trên nhiều transformer thị giác và ngôn ngữ với quy mô và chiến lược huấn luyện khác nhau. Bên cạnh đó, chúng tôi quan sát độ thưa thiết yếu vẫn hợp lệ cho các mẫu thưa N:M khác nhau với tiềm năng tăng tốc phần cứng.

Ngoài ra, các thí nghiệm của chúng tôi sử dụng benchmark MMLU phổ biến [31] trên Vicuna-7B minh họa các quan sát độ thưa thiết yếu vẫn đúng cho các mô hình ngôn ngữ lớn (LLM) quy mô hiện đại, cho thấy sự tồn tại của các mạng con thưa chất lượng cao trong checkpoint tiền huấn luyện dày đặc.

Tiếp theo, chúng tôi nghiên cứu sự xuất hiện của những tính chất thưa hóa đó trong động lực tiền huấn luyện, sử dụng BERT làm chủ đề nghiên cứu tập trung. Chúng tôi phát hiện một hiện tượng hấp dẫn về thưa hóa đột ngột, tức là BERT đột nhiên trở nên thưa nhiều sau một số lần lặp huấn luyện nhất định. Khi chúng tôi thay đổi kích thước bộ dữ liệu tiền huấn luyện, quan sát của chúng tôi chỉ ra một phát hiện phản trực giác khác rằng BERT được huấn luyện với lượng dữ liệu tiền huấn luyện lớn hơn có xu hướng có khả năng tốt hơn trong việc cô đặc kiến thức trong tương đối ít tham số hơn. Chúng tôi cũng đi sâu vào ảnh hưởng của mất mát tiền huấn luyện đối với độ thưa thiết yếu và phát hiện rằng các mục tiêu học tự giám sát (SSL) kích hoạt tính chất thưa hóa xuất hiện mạnh hơn so với học có giám sát (SL).

Các đóng góp chính cho công trình của chúng tôi có thể được tóm tắt như:

• Chúng tôi phát hiện sự tồn tại phổ biến của độ thưa thiết yếu trên các mô hình transformer tiền huấn luyện lớn với quy mô khác nhau cho thị giác và ngôn ngữ, bất kể chiến lược huấn luyện được sử dụng để tiền huấn luyện chúng. Các mặt nạ thưa chất lượng cao có thể so sánh với vé số [27] trong phạm vi độ thưa thiết yếu có thể được phát hiện miễn phí bằng cách chỉ chọn các trọng số có độ lớn thấp nhất, điều này không cần quy trình huấn luyện-cắt tỉa-huấn luyện lại lặp đi lặp lại đắt đỏ. Hành vi được tạo ra này cực kỳ quan trọng do quy mô bùng nổ của các mô hình tiền huấn luyện hiện đại. Quan sát này đúng cho cả mẫu thưa không có cấu trúc và có cấu trúc N:M.

• So sánh của chúng tôi về các mặt nạ thưa thu được bằng cách chọn các trọng số có độ lớn thấp nhất với các vé số trong phạm vi độ thưa thiết yếu, đáng ngạc nhiên tiết lộ một độ tương tự cosine cao đáng chú ý (>98%) trên các tác vụ downstream khác nhau từ NLP và CV. Quan sát này gửi một thông điệp mạnh mẽ rằng trong các mô hình tiền huấn luyện lớn, LTH có lẽ ít được hưởng đặc quyền bổ sung mặc dù sử dụng chi phí tính toán khổng lồ.

• Trong khi nghiên cứu sự xuất hiện của các tính chất thưa hóa trong động lực tiền huấn luyện trong BERT, chúng tôi phát hiện một hiện tượng hấp dẫn về thưa hóa đột ngột, tức là BERT đột nhiên trở nên thưa nhiều sau một số lần lặp huấn luyện nhất định. Chúng tôi cũng phát hiện thêm một quan sát phản trực giác rằng BERT được tiền huấn luyện với lượng dữ liệu lớn hơn có xu hướng thưa hơn, tức là chúng trở nên tốt hơn trong việc trừu tượng hóa kiến thức với ít tham số hơn.

• Chúng tôi cũng nghiên cứu ảnh hưởng của mất mát tiền huấn luyện đối với độ thưa xuất hiện của FM. Khi chúng tôi chuyển đổi giữa các mục tiêu học có giám sát (SL) và học tự giám sát (SSL) trên ViT, chúng tôi quan sát thấy SSL có xu hướng có tính chất thưa hóa xuất hiện tốt hơn, do đó thân thiện hơn với cắt tỉa. Chúng tôi tiếp tục cung cấp hình ảnh hóa theo lớp để hiểu độ thưa được học bởi SSL so với SL trông như thế nào.

--- TRANG 3 ---
2 Công trình liên quan

Mạng Nơ-ron Thưa (SNN). Độ thưa trong mạng nơ-ron sâu thường được đưa vào bằng nén mô hình [32,33] để loại bỏ các tham số dư thừa. Dựa trên loại mẫu thưa, nó có thể được phân loại thành hai họ: (i) độ thưa không có cấu trúc [33,34,35] trong đó các phần tử khác không được phân bố bất thường; (ii) độ thưa có cấu trúc [36,37,38] trong đó một nhóm tham số bị loại bỏ như một kernel tích chập trong mạng nơ-ron tích chập hoặc một đầu attention trong transformer. Nói chung, mẫu thưa trước có được hiệu suất tốt hơn nhờ tính linh hoạt của nó, trong khi mẫu thưa sau có xu hướng thân thiện với phần cứng hơn. Nhiều công trình SNN quan trọng bắt đầu bằng việc nghiên cứu mẫu đầu và sau đó chuyển sang mẫu sau như một trường hợp đặc biệt.

Đồng thời, theo thời điểm thực hiện giảm chiều, độ thưa có thể được thu được trong sau-huấn luyện, trong-huấn luyện và trước-huấn luyện của mạng nơ-ron sâu. (i) Sau-Huấn luyện. Để theo đuổi hiệu quả thời gian suy luận, các mô hình đã được huấn luyện có thể được cắt tỉa đáng kể với mất mát hiệu suất nhỏ so với các phương pháp heuristic nhất định, bao gồm tiêu chí bậc không [33], bậc một [39,40,41] và bậc hai [32,42,43]. Trong số các thuật toán này, phương pháp dựa trên độ lớn trọng số (ví dụ cắt tỉa độ lớn lặp) là một lựa chọn phổ biến. Đặc biệt, nó thường được áp dụng bởi Giả thuyết Vé Số [19] để tạo ra các SNN thưa với khả năng huấn luyện và biểu đạt không bị hư hại. (ii) Trong-Huấn luyện. Ngược lại với việc cắt tỉa một mô hình được huấn luyện tốt để hiệu quả suy luận, thưa hóa trong-huấn luyện [44] cũng tận hưởng tiết kiệm tính toán cho việc huấn luyện mô hình. Nó thường đầu tiên tối ưu hóa một mạng dày đặc trong vài lần lặp, sau đó dần dần thưa hóa nó với một lịch trình được định nghĩa trước, và cuối cùng dẫn đến các SNN thưa nhẹ. Ví dụ, [45,46,47] tận dụng các hình phạt ℓ0 hoặc ℓ1 để khuyến khích độ thưa trọng số trong quá trình huấn luyện mạng, hy vọng làm cho các tham số không quan trọng về không. [48,49] đưa nó thành một bài toán tối ưu hóa với các dạng tái tham số hóa và hai cấp, tương ứng. Một cách khác là khám phá độ thưa động [50,51,52,53,54,55,56,57,58,59] cho phép các kết nối đã cắt tỉa có thể được kích hoạt lại ở giai đoạn huấn luyện sau. (iii) Trước-Huấn luyện. Xác định các mẫu thưa quan trọng ở khởi tạo là một lĩnh vực con thú vị đang phát triển. Nó xác định các kết nối thưa ở giai đoạn rất sớm, như một lần lặp [25,60] hoặc vài lần lặp [61,62]. Trong bài báo này, chúng tôi chủ yếu điều tra các SNN không có cấu trúc sau-huấn luyện.

Độ thưa trong Transformer Tiền Huấn luyện. Transformer tiền huấn luyện đã trở thành lựa chọn de facto cho nhiều ứng dụng xử lý ngôn ngữ tự nhiên (NLP) [8,9,10,15,16] và thị giác máy tính [2,7,63]. Hiệu suất ấn tượng của chúng một phần được ghi nhận cho khả năng học tập to lớn được trao quyền bởi lượng lớn tham số mô hình. Thật không may, những thành công như vậy đi kèm với việc đốt cháy hàng nghìn GPU/TPU trong hàng nghìn giờ [64,17], ngay cả chỉ cho một vòng huấn luyện mô hình duy nhất. Để giải quyết mối quan ngại náo nhiệt tài nguyên này và cải thiện khả năng chi trả của các transformer này, nhiều nhà nghiên cứu tiên phong cống hiến trong lĩnh vực này [40,20,30,65,66,67,68,69,70]. Thay vì đề xuất một phương pháp thưa hóa mới, bài báo này tiết lộ những phước lành của độ thưa thiết yếu được tạo ra trong quá trình tiền huấn luyện và cách chúng ta có thể tận dụng nó để cắt tỉa các mô hình tiền huấn luyện lớn mà không có chi phí tính toán nào.

3 Cài đặt Thí nghiệm

Mạng và Transformer Tiền Huấn luyện. Chúng tôi xem xét {BERT Base [64], BERT Large [64], OPT125M [71], OPT350M [71], và OPT1.3B [71]} và {ViT Base [2], ViT Large [2], và DiNO Base [63] cho các ứng dụng NLP và CV tương ứng. Các mô hình tiền huấn luyện chính thức của chúng được áp dụng trong các thí nghiệm của chúng tôi. Cụ thể, hãy để f(x;θp) là đầu ra của một transformer với các tham số mô hình tiền huấn luyện θp và mẫu đầu vào x.

Bộ dữ liệu, Huấn luyện và Đánh giá. Cho các tác vụ downstream trong NLP, chúng tôi xem xét {MNLI, QNLI, QQP, SST-2} từ GLUE [13], RTE từ SuperGLUE [72], và SQuAD v1.1 [73]. Đối với các ứng dụng downstream thị giác, chúng tôi kiểm tra {CIFAR-10, CIFAR-100, Tiny-ImageNet [74]}. Chúng tôi cũng sử dụng tác vụ Lý luận Số học từ SMC-Benchmark được đề xuất gần đây [75] và xem xét các bộ dữ liệu bài toán từ toán học phổ biến: (1) benchmark MAWPS được sử dụng rộng rãi [76] gồm 2.373 bài toán; (2) tập con số học của ASDiv-A [77] - với 1.218 bài toán toán; (3) bộ dữ liệu SVAMP [78] đầy thách thức hơn được tạo ra bằng cách áp dụng các loại biến thể phức tạp cho các mẫu từ ASDiv-A. Độ khó tác vụ tăng đơn điệu từ MAWPS đến ASDiv-A, và đến SVAMP. Chúng tôi áp dụng phân tách bộ dữ liệu mặc định để huấn luyện và đánh giá cho các

1 Kiểm tra tài liệu bổ sung để biết chi tiết mô hình tiền huấn luyện.

--- TRANG 4 ---
Bảng 1: Chi tiết tinh chỉnh tác vụ downstream. Tốc độ học giảm tuyến tính từ giá trị ban đầu xuống 0.

Cài đặt | Xử lý Ngôn ngữ Tự nhiên | Thị giác Máy tính
MNLI | QNLI | QQP | RTE | SST-2 | SQuAD v1.1 | CIFAR-10 | CIFAR-100 | Fashion-MNIST | Tiny-ImageNet
# Ví dụ Huấn luyện | 392,704 | 104,768 | 363,872 | 2,496 | 67,360 | 88,656 | 45,000 | 45,000 | 55,000 | 90,000
# Epoch | 3 | 4 | 3 | 5 | 5 | 3 | 8 | 8 | 8 | 5
Kích thước Batch | 32 | 32 | 32 | 32 | 32 | 16 | 64 | 64 | 64 | 64
Tốc độ Học | 2e−5 | 2e−5 | 2e−5 | 2e−5 | 2e−5 | 3e−5 | 2e−5 | 2e−5 | 2e−5 | 2e−5
Bộ tối ưu | AdamW với decay (α) = 1×10−8 | AdamW với decay (α) = 2×10−8
Chỉ số Đánh giá | Matched Acc. | Accuracy | F1-score | Accuracy (Top-1)

Hình 1: Các mẫu thưa được tạo ra tự nhiên của bert-base-uncased trên các thành phần của khối transformer. Mô hình tiền huấn luyện được cắt tỉa 21,50% bằng cắt tỉa độ lớn một lần. Các chấm vàng chỉ vị trí của các trọng số có độ lớn thấp đã được cắt tỉa.

ứng dụng downstream. Cấu hình chi tiết hơn được thu thập trong Bảng 1. Cho SMC-benchmark, chúng tôi tuân thủ nghiêm ngặt các cài đặt được đề xuất trong triển khai chính thức.2

Mạng Nơ-ron Thưa (SNN). Các trọng số của một SNN có thể được mô tả như m⊙θ, trong đó m∈{0,1}|θ| là một mặt nạ nhị phân có cùng chiều với θ và ⊙ là tích theo phần tử. Hãy để ET(f(x;θ)) biểu thị hàm đánh giá của đầu ra mô hình f(x;θ) trên tác vụ tương ứng T (có thể liên quan đến tinh chỉnh). Pρ(·) là thuật toán thưa hóa biến một phần ρ của các phần tử "1" trong mặt nạ thưa m thành "0". Đây là định nghĩa chính thức của Độ thưa Thiết yếu dưới đây.

Độ thưa Thiết yếu. Nếu ET(f(x;m⊙θ)) ≥ ET(f(x;θ))−ϵ, và ET(f(x;Pρ(m)⊙θ)) < ET(f(x;θ))−ϵ trong đó giá trị của ρ và ϵ là nhỏ. Thì độ thưa tương ứng 1−∥m∥0/|m| được gọi là Độ thưa Thiết yếu cho mô hình f trên tác vụ T.

Như đã nêu chi tiết ở trên, mô hình ở độ thưa thiết yếu thường có một điểm hiệu suất bước ngoặt, có nghĩa là việc cắt tỉa thêm ngay cả một phần nhỏ ρ trọng số dẫn đến ít nhất ϵ giảm hiệu suất, so với đối tác dày đặc ET(f(x,θ)). Trong trường hợp của chúng tôi, ϵ được đặt là 1%.

Phương pháp Cắt tỉa. Để tìm mặt nạ thưa mong muốn m, chúng tôi sử dụng hai thuật toán cắt tỉa độ lớn trọng số cổ điển [33,19]. Cắt tỉa Độ lớn Một lần (OMP): chúng tôi trực tiếp loại bỏ một phần được định nghĩa trước của các tham số từ θp với độ lớn tuyệt đối nhỏ nhất. Cắt tỉa Vé Số (LTP): (i) đầu tiên chúng tôi huấn luyện mô hình để hội tụ trên một tác vụ downstream T; (ii) sau đó loại bỏ một phần của các trọng số nhỏ nhất và đặt lại trọng số còn lại về giá trị khởi tạo từ tiền huấn luyện θp; (iii) các quá trình như vậy sẽ được lặp lại cho đến khi đạt được độ thưa mong muốn.

Hai sự thật quan trọng đáng chú ý. Thứ nhất, bắt đầu từ mô hình tiền huấn luyện, không giống như LTP tìm kiếm mặt nạ cụ thể cho downstream với huấn luyện bổ sung, các mặt nạ thưa OMP không cần huấn luyện bổ sung để xác định và không phụ thuộc vào ứng dụng downstream. Vì chúng được ước tính trực tiếp từ checkpoint trọng số tiền huấn luyện, mặt nạ thưa OMP vẫn giống nhau cho tất cả các ứng dụng downstream (chúng tôi chỉ tiếp tục tinh chỉnh các trọng số khác không với cùng mặt nạ, trên các bộ dữ liệu downstream khác nhau). Thứ hai, vì lý do tương tự ở trên, một mặt nạ OMP có độ thưa cao hơn tự nhiên được lồng trong một mặt nạ có độ thưa thấp hơn, miễn là chúng được cắt tỉa từ cùng một checkpoint tiền huấn luyện bằng OMP. Tính chất lồng nhau tương tự không tồn tại cho các mặt nạ thu được bằng LTP.

4 Độ thưa Thiết yếu Tồn tại trong các Mô hình Ngôn ngữ và Thị giác Tiền huấn luyện

Xem xét lại độ thưa trong Transformer Tiền huấn luyện: Sự tăng trưởng không hạn chế về tham số đã dẫn đến chi phí tính toán đáng kể và nhu cầu bộ nhớ quá mức. Xu hướng này chắc chắn tiếp tục với transformer ở tiền tuyến, nơi ngày càng nhiều lớp được xếp chồng với các khối attention dày đặc (ví dụ GPT-3 có 175 tỷ tham số đáng ngạc nhiên) đòi hỏi tài nguyên tính toán đáng kể và thời gian huấn luyện hoặc tinh chỉnh kéo dài. Không giống như các nỗ lực rộng lớn được khám phá trong quá khứ để cắt tỉa ResNet và các họ mạng liên quan, cắt tỉa các mô hình transformer tiền huấn luyện quy mô lớn chưa nhận được đủ sự chú ý.

Các phương pháp dựa trên Giả thuyết Vé Số (LTH) gần đây đã được khám phá để cắt tỉa các mô hình tiền huấn luyện quy mô BERT-base. Tuy nhiên, chúng mất tính thực tế do quy trình huấn luyện-cắt tỉa-huấn luyện lại dày đặc đắt đỏ của IMP và các đặc tính không thể chuyển giao của các mạng con được xác định trên các tác vụ tương tự [20]. oBERT [66] đề xuất bậc hai đến quy mô cấp BERT bằng cách cho phép cắt tỉa các khối trọng số. Lưu ý rằng, mặc dù các phương pháp này cung cấp các cách tiếp cận thú vị để cắt tỉa transformer quy mô BERT, chúng không nhận ra đầy đủ sức mạnh của các mẫu thưa chất lượng cao đã tồn tại, dễ tiếp cận hơn trong các mô hình lớn tiền huấn luyện bất biến với quy mô, phương thức dữ liệu huấn luyện và chiến lược. Mục tiêu chính của bài báo này là đưa sự chú ý của cộng đồng thưa về sức mạnh của các mẫu thưa được tạo ra phổ biến trong quá trình tiền huấn luyện các transformer lớn và khuyến khích họ tận dụng hiệu quả nó trong việc thiết kế các thuật toán cắt tỉa tinh vi hơn.

Độ thưa Thiết yếu: Như đã định nghĩa trong Mục 3, độ thưa thiết yếu chỉ ra sự hiện diện của các cấu trúc thưa được tạo ra tự nhiên trong các mô hình tiền huấn luyện lớn liên quan đến các trọng số có độ lớn thấp nhất. Hình 1 thể hiện phân phối độ thưa được tạo ra của mô hình tiền huấn luyện bert-base-uncased với 21,50% trọng số gần bằng không, mà chúng tôi có thể loại bỏ mà KHÔNG có bất kỳ sự sụt giảm hiệu suất nào trên bất kỳ bộ dữ liệu đánh giá downstream nào của chúng tôi. Điều này tìm thấy một thông điệp mạnh mẽ về sự tồn tại của mặt nạ thưa miễn phí, phổ quát và có sẵn để tiêu thụ (m·θp) mà không có bất kỳ chi phí tính toán nào được tạo ra bởi các thuật toán cắt tỉa tinh vi. Ngoài ra, một quan sát gần hơn về các tỷ lệ độ thưa được phân phối trên các mô-đun khác nhau của khối transformer chỉ ra rằng phần lớn các trọng số có độ lớn thấp tập trung ở các lớp feed-forward dày đặc. Những phát hiện như vậy phù hợp với thành công của Mixture-of-Experts [79] và cung cấp gợi ý cho việc phát triển tương lai của các thuật toán cắt tỉa để khai thác sự tham số hóa quá mức của các lớp feed-forward dày đặc.

--- TRANG 5 ---
Hình 2: Giảm hiệu suất tinh chỉnh được ước tính so với đối tác dày đặc cho các tác vụ downstream khác nhau của các mô hình tiền huấn luyện NLP (bert-base, OPT-125m, OPT-350m, OPT-1.3B). Lưu ý rằng để đánh giá công bằng, chúng tôi đã sử dụng chính xác cùng cài đặt tinh chỉnh trên tất cả các tỷ lệ cắt tỉa.

Hình 3: Giảm hiệu suất tinh chỉnh được ước tính so với đối tác dày đặc cho các tác vụ downstream khác nhau của các mô hình tiền huấn luyện CV (ViT-base & ViT-large).

Chúng tôi hiện liệt kê các phát hiện chính liên quan đến độ thưa thiết yếu trong các mô hình transformer tiền huấn luyện lớn:

• Trong tất cả các mô hình thị giác và ngôn ngữ, chúng tôi tìm thấy sự tồn tại của độ thưa thiết yếu và điểm bước ngoặt mạnh của đường cong độ thưa-hiệu suất.

• Điểm bước ngoặt mạnh của độ thưa thiết yếu phụ thuộc vào tác vụ downstream và có thể thay đổi tùy thuộc vào độ phức tạp của tác vụ.

• Hành vi mạnh của độ thưa thiết yếu sâu sắc hơn trong các mô hình tiền huấn luyện thị giác so với các mô hình ngôn ngữ.

• Độ thưa thiết yếu vẫn hợp lệ cho benchmark cắt tỉa được đề xuất gần đây SMC-bench [75].

• Độ thưa thiết yếu vẫn hợp lệ cho các mẫu thưa có cấu trúc N:M [80] với tiềm năng tăng tốc.

• Mục tiêu học tự giám sát kích hoạt tính chất thưa hóa xuất hiện mạnh hơn so với học có giám sát trong các mô hình có chính xác cùng kiến trúc.

Hình 4: Giảm hiệu suất tinh chỉnh của bert-base trên các bộ dữ liệu Lý luận Số học trong SMC-benchmark [75].

Sự Tồn tại Phổ quát của Độ thưa Thiết yếu: Chúng tôi đánh giá toàn diện mức độ tồn tại của độ thưa thiết yếu trong các mô hình ngôn ngữ tiền huấn luyện lớn (CV và NLP) với khởi tạo tiền huấn luyện tiêu chuẩn θp. Đối với NLP, chúng tôi nghiên cứu bert-base, OPT-125M, OPT-350M, OPT-1.3B và sử dụng MNLI, QNLI, QQP, SST-2, RTE, và SQuAD1.1 để ước tính ảnh hưởng của việc loại bỏ các trọng số có độ lớn thấp đối với hiệu suất downstream. Mặt khác, đối với CV, chúng tôi dựa vào các mô hình ViT-Base và ViT-Large và sử dụng các bộ dữ liệu CIFAR-10, CIFAR-100, TinyImageNet. Hình 2 minh họa giảm hiệu suất (trục y) của các tác vụ downstream NLP khác nhau tinh chỉnh với mặt nạ (m·θx%p), khi chúng tôi loại bỏ x% trọng số có độ lớn thấp nhất. Tương tự, Hình 3 minh họa ảnh hưởng của việc cắt tỉa x% trọng số có độ lớn thấp đối với hiệu suất tinh chỉnh của các tác vụ downstream CV. Hơn nữa, Hình 4 trình bày sự tồn tại của độ thưa thiết yếu cho một benchmark độ thưa được đề xuất gần đây, có tên SMC-Bench [75]. Thú vị khi quan sát rằng độ thưa thiết yếu tồn tại trong tất cả các mô hình tiền huấn luyện CV và NLP, và ∼30−50% trọng số có thể được loại bỏ miễn phí mà không có bất kỳ sự giảm đáng kể nào trong hiệu suất. Lưu ý rằng những mặt nạ này được xác định trước tinh chỉnh, trên các trọng số tiền huấn luyện và do đó vẫn giống nhau cho tất cả các tác vụ downstream, cho thấy không cần yêu cầu để giữ sổ các mặt nạ cụ thể cho tác vụ kiểu LTH. Cũng quan trọng để đánh giá cao tầm quan trọng của việc loại bỏ ∼40% (tương đương với > 500 triệu tham số) của OPT-1.3B mà không tốn chi phí mà không có bất kỳ sự giảm hiệu suất đáng kể nào.

Hình 5: Giảm hiệu suất tinh chỉnh của OPT-350M tại bộ dữ liệu GLUE so với đối tác dày đặc trên nhiều mặt nạ mẫu thưa N:M [80].

Độ thưa Thiết yếu cho Các Mẫu Thưa N:M Có cấu trúc: Xem xét nhu cầu tăng tốc thực tế, chúng tôi cũng tiến hành đánh giá để hiểu độ thưa thiết yếu cho độ thưa N:M có cấu trúc thân thiện với phần cứng. Một mạng nơ-ron với độ thưa N:M thỏa mãn rằng, trong mỗi nhóm M trọng số liên tiếp của mạng, có nhiều nhất N trọng số có giá trị khác không. Chúng tôi nghiên cứu OPT-350M và sử dụng MNLI, QNLI, RTE, SST-2 để ước tính ảnh hưởng của việc loại bỏ các trọng số có độ lớn thấp theo cách có cấu trúc N:M theo [80]. Hình 5 minh họa giảm hiệu suất (trục y) của OPT-350M trên các bộ dữ liệu downstream khác nhau với nhiều mặt nạ thưa N:M (m·θx%p). Nói chung có thể quan sát thấy độ thưa thiết yếu vẫn hợp lệ cho các mẫu thưa có cấu trúc N:M, và một phần lớn các trọng số có độ lớn thấp có thể được loại bỏ miễn phí mà không có sự giảm đáng kể trong hiệu suất downstream.

--- TRANG 6 ---
Hình 6: Độ thưa Thiết yếu và so sánh hiệu suất ViT-base và DINO-base chia sẻ cùng kiến trúc nhưng được tiền huấn luyện bằng các mục tiêu học có giám sát (SL) và tự giám sát (SSL). Có thể quan sát thấy rằng SSL tạo ra khả năng thưa hóa tốt hơn trong checkpoint tiền huấn luyện.

Hình 7: Phân phối trọng số theo lớp của ViT-base và DINO-base được huấn luyện bằng mục tiêu học có giám sát và tự giám sát. Lưu ý rằng các trọng số của cả hai mô hình tiền huấn luyện được chuẩn hóa bằng sklearn để so sánh công bằng. Ngoài ra, DINO có 14,37% trọng số bằng không nhiều hơn ViT.

Độ thưa Thiết yếu và Hành vi Bước ngoặt Mạnh: Trong phần này, chúng tôi cố gắng thu hút sự chú ý về hành vi điểm bước ngoặt mạnh của độ thưa thiết yếu. Trên tất cả các mô hình thị giác và ngôn ngữ và các tác vụ downstream liên quan (Hình 2, 3, & 4), chúng tôi quan sát thấy rằng sau khi loại bỏ một tỷ lệ nhất định của các trọng số có độ lớn thấp, khả năng tinh chỉnh downstream của các mô hình tiền huấn luyện giảm mạnh. Đây là một dấu hiệu rõ ràng rằng kiến thức tiền huấn luyện nằm trong một phần của các vùng trọng số có độ lớn cao và nếu cắt tỉa một lần của chúng ta chạm vào vùng đó, nó tác động phi tuyến đến khả năng học chuyển giao của checkpoint tiền huấn luyện. Các thí nghiệm của chúng tôi tiết lộ rằng hành vi bước ngoặt mạnh này không phụ thuộc vào kích thước mô hình mà phụ thuộc vào tác vụ downstream. Mô hình lớn hơn không có nghĩa là nó có thể được cắt tỉa cho tỷ lệ trọng số có độ lớn thấp cao hơn, mà không quan sát sự giảm mạnh. Ví dụ, bert-base quan sát điểm bước ngoặt mạnh ở khoảng 60% độ thưa trong khi OPT-1.3B không thể được cắt tỉa vượt quá 40% mà không quan sát sự giảm hiệu suất mạnh trên tác vụ RTE, mặc dù nó có ∼10 lần tham số hơn bert-base. Cũng thú vị khi quan sát rằng mặc dù OPT-125M và bert-base có số lượng hiệu suất tương tự, bert-base minh họa hành vi thân thiện hơn với cắt tỉa, và có thể được cắt tỉa đến tỷ lệ độ thưa cao hơn tương đối so với OPT-125M trên tất cả các tác vụ downstream đánh giá của chúng tôi.

Ảnh hưởng của Mục tiêu Học Có giám sát so với Tự giám sát: Với thành công gần đây của học tự giám sát (SSL) trong việc tiền huấn luyện các mô hình transformer lớn và khả năng mở rộng quy mô của nó đến dữ liệu kích thước internet khổng lồ, thú vị khi hiểu SSL ủng hộ độ thưa như thế nào. Do không có sẵn các bộ dữ liệu có giám sát để tiền huấn luyện các mô hình quy mô BERT, chúng tôi chuyển sang transformer Thị giác, và sử dụng ViT-base và phiên bản tự giám sát DINO-base kế thừa chính xác cùng kiến trúc. Hình 6 minh họa giảm hiệu suất tinh chỉnh của ViT-base và DINO với sự gia tăng tỷ lệ loại bỏ trọng số có độ lớn thấp từ các checkpoint tiền huấn luyện. Trên tất cả các tác vụ, hiệu suất tinh chỉnh không bị giảm gì cho đến khi 30−40% trọng số được loại bỏ.

Một quan sát thú vị là DINO-base có xu hướng mạnh mẽ hơn với việc loại bỏ trọng số thấp, và có độ thưa thiết yếu tương đối tốt hơn trên tất cả các tác vụ. Để điều tra thêm tại sao chúng ta có thể cắt tỉa DINO-base nhiều hơn ViT-base, chúng tôi kiểm tra phân phối độ lớn trọng số qua các lớp của các khối transformer từ các checkpoint tiền huấn luyện. Hình 7 trình bày phân phối trọng số theo lớp được chuẩn hóa của DINO-base và ViT-base. Có thể quan sát rõ ràng rằng DINO dựa trên SSL có xu hướng có phân phối trọng số thân thiện hơn với cắt tỉa với lượng trọng số đáng kể tập trung xung quanh độ lớn bằng không. Cụ thể hơn, chúng tôi phát hiện rằng DINO-base có ∼14% trọng số bằng không nhiều hơn ViT-base, điều này biện minh cho độ thưa thiết yếu cao hơn của nó.

5 Độ thưa Thiết yếu Xuất hiện như thế nào trong Động lực Tiền huấn luyện

Mở rộng quy mô khối lượng dữ liệu tiền huấn luyện được tin rộng rãi là ủng hộ hiệu suất chuyển giao trong nhiều tác vụ downstream theo cách mong muốn [81]. Mặc dù, mối quan hệ này đã được nghiên cứu rộng rãi gần đây trong nhiều công trình [81,82,83,84], vai trò của khối lượng dữ liệu tiền huấn luyện trong việc tạo ra các tính chất thưa trong các transformer lớn vẫn chưa được khám phá. Trong phần này, chúng tôi đặt một câu hỏi quan trọng: Khối lượng dữ liệu tiền huấn luyện tác động như thế nào đến các mẫu thưa xuất hiện trong các transformer lớn, và liệu nó có cải thiện khả năng cắt tỉa của chúng không?

Để làm điều này, chúng tôi thiết kế các thí nghiệm tùy chỉnh để tiền huấn luyện bert-base từ đầu bằng HuggingFace bookcorpus với kích thước từ vựng 30,522. Chúng tôi tạo ra 5 bộ dữ liệu tiền huấn luyện khác nhau bằng cách chọn ngẫu nhiên 100%, 90%, 80%, 70%, 60% mẫu huấn luyện từ bookcorpus và tiền huấn luyện trong 50k lần lặp mỗi cái để đảm bảo rằng tất cả các mô hình nhận được cùng lượng gradient. Lưu ý rằng chúng tôi duy trì chính xác cùng cài đặt huấn luyện cho tất cả các mô hình để giữ so sánh công bằng và lưu các checkpoint tiền huấn luyện mỗi 1000 lần lặp. Chúng tôi hiện liệt kê các phát hiện chính liên quan đến khối lượng dữ liệu tiền huấn luyện và các mẫu thưa được tạo ra trong transformer:

• Chúng tôi quan sát một hiện tượng mới thú vị về thưa hóa đột ngột, tức là việc giới thiệu độ thưa cao đột ngột, trong quá trình tiền huấn luyện các mô hình bert-base, bất kể kích thước dữ liệu.

• Chúng tôi cũng quan sát thêm một phát hiện phản trực giác rằng bert-base được huấn luyện với lượng dữ liệu tiền huấn luyện lớn hơn có xu hướng có sự xuất hiện tốt hơn của độ thưa được tạo ra.

• Trên tất cả mức độ thưa, chúng tôi thấy bert-base được huấn luyện với khối lượng dữ liệu huấn luyện lớn hơn, tận hưởng khả năng cắt tỉa tốt hơn, và đạt hiệu suất tốt hơn trên tác vụ downstream (MNLI).

Hình 8(i) minh họa sự xuất hiện của các mẫu thưa trong quá trình tiền huấn luyện bert-base với khối lượng dữ liệu tiền huấn luyện khác nhau. Chúng tôi vẽ số lượng trọng số bằng không xuất hiện trong các checkpoint tiền huấn luyện mỗi 1k lần lặp. Có thể quan sát rõ ràng rằng trong tất cả các cài đặt, số lượng trọng số bằng không đột nhiên tăng lên, cho thấy các mô hình bắt đầu trừu tượng hóa kiến thức dữ liệu tiền huấn luyện thành ít tập tham số hơn. Ví dụ, chúng tôi tìm thấy bước ngoặt mạnh này xung quanh 22-25k lần lặp cho cài đặt 100%, 90%, 80% trong khi xung quanh 40k lần lặp cho cài đặt 70%, 60%. Ngoài ra, chúng tôi phát hiện rằng bert được huấn luyện với lượng dữ liệu tiền huấn luyện lớn hơn có xu hướng có sự xuất hiện tốt hơn của độ thưa được tạo ra. Chúng tôi lập luận rằng bert-base có xu hướng học các đặc trưng tổng quát hơn và phát triển khả năng trừu tượng hóa kiến thức. Để điều tra thêm sự xuất hiện độ thưa này ảnh hưởng như thế nào đến khả năng cắt tỉa của các mô hình tiền huấn luyện, chúng tôi kiểm tra hiệu suất của chúng trên nhiều tỷ lệ độ thưa (Hình 8(ii)) trên MNLI và thấy nó hài hòa với độ lớn của các mẫu độ thưa được tạo ra, tức là các mô hình với các mẫu độ thưa được tạo ra cao trong quá trình tiền huấn luyện có xu hướng hoạt động tốt hơn khi chúng ta loại bỏ các trọng số có độ lớn thấp hiện có và tinh chỉnh chúng trên các tác vụ downstream. Trong cài đặt dày đặc trên QNLI, QQP, RTE (Hình 8(iii)), chúng tôi thấy rằng sự gia tăng khối lượng dữ liệu tiền huấn luyện có lợi ích thuận lợi cho hiệu suất downstream với các siêu tham số tinh chỉnh tương tự.

--- TRANG 7 ---
Hình 8: Mô tả biểu đồ theo thứ tự trái-phải. (i) Số lượng trọng số bằng không của 5 thí nghiệm tiền huấn luyện Bert-base sử dụng bộ dữ liệu bookcorpus từ HuggingFace với các tỷ lệ phần trăm khác nhau của khối lượng dữ liệu được chọn ngẫu nhiên với chính xác cùng cài đặt tiền huấn luyện. (ii) Hiệu suất downstream của các mô hình Bert-base tiền huấn luyện với khối lượng dữ liệu khác nhau trên các tỷ lệ độ thưa khác nhau trên MNLI. (iii) Hiệu suất downstream của 5 mô hình tiền huấn luyện dày đặc trên QNLI, QQP, RTE.

6 Độ thưa Thiết yếu và Giả thuyết Vé Số

Giả thuyết Vé Số sai lệch khỏi quy ước cắt tỉa sau-huấn luyện, và chỉ ra sự tồn tại của các mạng con thưa có thể huấn luyện độc lập từ đầu có thể khớp với hiệu suất của các mạng dày đặc. Kể từ khi được giới thiệu, nó đã rất thành công và được áp dụng rộng rãi trong việc nén các mô hình quy mô nhỏ (ví dụ họ ResNet). Tuy nhiên, nó bị hạn chế đáng kể trong việc áp dụng thực tế cho các mô hình tiền huấn luyện lớn do quy trình huấn luyện-cắt tỉa-huấn luyện lại của IMP. Trong phần này, chúng tôi hỏi: LTH hiệu quả như thế nào trong phạm vi độ thưa thiết yếu, và liệu nó có mang lại đặc quyền bổ sung nào có thể chứng minh cho chi phí tính toán của nó không? Điểm mấu chốt của chúng tôi có thể được tóm tắt như:

• Trong phạm vi độ thưa thiết yếu của bert-base và ViT-base, chúng tôi không tìm thấy bất kỳ sự khác biệt hiệu suất tinh chỉnh đáng kể nào của mặt nạ được xác định bởi LTH và một bởi loại bỏ trọng số có độ lớn thấp nhất (OMP) trên nhiều tác vụ downstream.

• Chúng tôi đáng ngạc nhiên tìm thấy sự tồn tại của độ tương tự cosine cao (>96%) trên các mặt nạ được xác định bởi LTH và OMP trong phạm vi độ thưa thiết yếu.

• Độ tương tự mặt nạ giữa LTH và OMP giảm với sự gia tăng tỷ lệ độ thưa. Điều này xác nhận với lợi ích của LTH trong phạm vi độ thưa cao, nơi LTH có thể xác định các mặt nạ phù hợp với tác vụ để giữ hiệu suất do quy trình cắt tỉa và huấn luyện lại lặp đi lặp lại.

Hình 9 minh họa so sánh sự khác biệt hiệu suất của tinh chỉnh các mặt nạ được xác định bởi LTH và OMP từ bert-base và ViT-base trên nhiều tác vụ downstream. Có thể quan sát rõ ràng rằng đối với tỷ lệ độ thưa dưới 50%, chúng tôi không tìm thấy bất kỳ sự khác biệt đáng kể nào trong hiệu suất giữa LTH đắt đỏ và quy trình cắt tỉa OMP miễn phí. Một phân tích sâu hơn (Hình 10) trên các mặt nạ từ LTH và OMP tiết lộ một quan sát đáng ngạc nhiên về sự tồn tại của độ tương tự cosine cao đáng kể. Quan sát này hỗ trợ các phát hiện trong Hình 9 về hiệu suất khớp của LTH và OMP và truyền đạt một thông điệp mạnh mẽ và ấn tượng rằng với phạm vi độ thưa thiết yếu, LTH không cung cấp bất kỳ đặc quyền bổ sung nào. Tuy nhiên, quan trọng là lưu ý rằng LTH rất hiệu quả trong phạm vi độ thưa cao vượt quá độ thưa thiết yếu, do khả năng tìm mặt nạ phù hợp với tác vụ bằng quy trình huấn luyện-cắt tỉa-huấn luyện lại nhưng chúng có xu hướng không thể chuyển giao trên các tác vụ downstream khác nhau [20]. Hơn nữa, có thể quan sát từ Hình 10, ngay cả trong phạm vi độ thưa thiết yếu, độ tương tự mặt nạ trên các tác vụ bởi LTH tương đối thấp hơn mặt nạ dựa trên OMP do sự gắn kết mạnh mẽ của LTH với các tác vụ downstream.

--- TRANG 8 ---
Hình 9: So sánh sự khác biệt hiệu suất của tinh chỉnh các mặt nạ thu được bởi LTH (phụ thuộc vào tác vụ downstream) và OMP trên bert-base (trái) và ViT-base (phải) trên nhiều tác vụ downstream.

Hình 10: Độ tương tự cosine giữa các mặt nạ thu được bởi LTH (phụ thuộc vào tác vụ downstream) và OMP trên bert-base (Hàng 1) và ViT-base (Hàng 2) cho tỷ lệ độ thưa s∈{10%,20%,30%,40%}. Độ tương tự cosine cao chỉ ra các mặt nạ được xác định bởi LTH và OMP tương tự đáng kể.

Hình 11: Giảm hiệu suất của Vicuna 7B trên benchmark MMLU [31] so với đối tác dày đặc với OMP và SparseGPT được đề xuất gần đây [85]. Điều này chỉ ra một phần lớn trọng số của Vicuna-7B có thể được loại bỏ miễn phí mà không có bất kỳ sự giảm đáng kể nào trong hiệu suất.

7 Mở rộng Độ thưa Thiết yếu đến LLM Quy mô Hiện đại: Một Nghiên cứu Trường hợp

Các Mô hình Ngôn ngữ Lớn (LLM) gần đây đang định hình lĩnh vực NLP với lợi ích hiệu suất đáng chú ý trên một loạt các benchmark ngôn ngữ phức tạp. Tuy nhiên, do kích thước khổng lồ và chi phí tính toán; chúng vẫn nằm ngoài tầm với của nhiều nhà nghiên cứu và công nghiệp nhỏ. Ví dụ, GPT-175B cần ít nhất năm GPU A100 với 80GB bộ nhớ cho mỗi lần suy luận. Tài nguyên tính toán khổng lồ được yêu cầu bởi LLM loại trừ hầu hết các thuật toán cắt tỉa tiên tiến có sẵn do không thể thực hiện bất kỳ quy trình huấn luyện hoặc tinh chỉnh nào [86].

Trong phần này, chúng tôi điều tra sự hiện diện của độ thưa thiết yếu trong một LLM phổ biến (Vicuna-7B) với cắt tỉa độ lớn một lần. Lưu ý rằng không giống như cài đặt trước đó của chúng tôi, nơi chúng tôi thực hiện tinh chỉnh thưa sau khi mặt nạ được xác định; ở đây chúng tôi không thực hiện tinh chỉnh cụ thể cho tác vụ downstream và đánh giá hiệu suất trực tiếp "zero-shot" trên checkpoint tiền huấn luyện được thưa hóa.

Hình 11(a) minh họa giảm hiệu suất của Vicuna-7B trên benchmark MMLU phổ biến (Stem, Humanities, Social Science, Others) khi x% trọng số có độ lớn thấp nhất được loại bỏ khỏi checkpoint tiền huấn luyện dày đặc. Thú vị khi thấy rằng các quan sát độ thưa thiết yếu của chúng tôi vẫn đúng ngay cả đối với LLM hiện đại, gửi một tín hiệu thuận lợi về sự tồn tại ẩn của mạng con thưa chất lượng cao có thể được xác định miễn phí trong các checkpoint tiền huấn luyện dày đặc. Để làm phong phú thêm nghiên cứu của chúng tôi, chúng tôi thay thế OMP bằng SparseGPT được đề xuất gần đây [85] và thấy nó có xu hướng nhất quán chung với OMP (Hình 11(b)). Ngoài ra, thú vị khi quan sát rằng các chiến lược cắt tỉa được thiết kế tốt hơn như SparseGPT có thể đẩy ranh giới của độ thưa thiết yếu và xác định các mạng con thưa tốt hơn ở tỷ lệ độ thưa cao hơn tương đối: nhưng với chi phí tính toán cao hơn. Chúng tôi để lại như công việc tương lai về cách thu hẹp khoảng cách hiệu suất giữa OMP và SparseGPT.

--- TRANG 9 ---
8 Kết luận

Trong công trình này, chúng tôi nghiên cứu toàn diện các mẫu thưa được tạo ra trên nhiều transformer tiền huấn luyện lớn về thị giác và ngôn ngữ. Chúng tôi đã xác thực thực nghiệm sự tồn tại phổ biến của độ thưa thiết yếu trên các mô hình transformer tiền huấn luyện lớn với quy mô khác nhau cho thị giác và ngôn ngữ (bao gồm LLM), bất kể chiến lược huấn luyện được sử dụng để tiền huấn luyện chúng. Chúng tôi cũng trình bày một hiện tượng xuất hiện hấp dẫn về thưa hóa đột ngột trong quá trình tiền huấn luyện transformer và khả năng trừu tượng hóa kiến thức trong tập con tham số ít với việc tăng khối lượng dữ liệu tiền huấn luyện. Cuối cùng, chúng tôi nghiên cứu hiệu suất của LTH liên quan đến độ thưa thiết yếu. Công việc tương lai của chúng tôi sẽ nhằm mở rộng các quan sát độ thưa thiết yếu đến các mô hình nền tảng khổng lồ hơn, và đẩy cho các phạm vi độ thưa cao hơn.

Tài liệu tham khảo

[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Advances in neural information processing systems, trang 5998–6008, 2017.

[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

[3] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, và Dacheng Tao. A survey on visual transformer. ArXiv, abs/2012.12556, 2020.

[4] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Herv'e J'egou. Training data-efficient image transformers & distillation through attention. Trong ICML, 2021.

[5] Zhiyuan Mao, Ajay Jaiswal, Zhangyang Wang, và Stanley H. Chan. Single frame atmospheric turbulence mitigation: A benchmark study and a new physics-inspired transformer model. ArXiv, abs/2207.10040, 2022.

[6] Minghang Zheng, Peng Gao, Renrui Zhang, Xiaogang Wang, Hongsheng Li, và Hao Dong. End-to-end object detection with adaptive clustering transformer. ArXiv, abs/2011.09315, 2021.

[7] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam M. Shazeer, Alexander Ku, và Dustin Tran. Image transformer. Trong ICML, 2018.

[8] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, và Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.

[9] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

[10] Alon Talmor, Jonathan Herzig, Nicholas Lourie, và Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.

[11] Ajay Jaiswal, Liyan Tang, Meheli Ghosh, Justin Rousseau, Yifan Peng, và Ying Ding. Radbert-cl: Factually-aware contrastive learning for radiology report classification. Proceedings of machine learning research, 158:196–208, 2021.

[12] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, và Jimmy Lin. End-to-end open-domain question answering with bertserini. arXiv preprint arXiv:1902.01718, 2019.

[13] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.

[14] Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, và Jie Tang. Cognitive graph for multi-hop reading comprehension at scale. arXiv preprint arXiv:1905.05460, 2019.

[15] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[16] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, và Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.

[17] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

[18] Elias Frantar và Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023.

[19] Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. Trong International Conference on Learning Representations, 2019.

[20] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, và Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. Advances in neural information processing systems, 33:15834–15846, 2020.

[21] Ajay Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding, và Zhangyang Wang. Spending your winning lottery better after drawing it, 2022.

[22] Lu Yin, Shiwei Liu, Fang Meng, Tianjin Huang, Vlado Menkovski, và Mykola Pechenizkiy. Lottery pools: Winning more by interpolating tickets without increasing training or inference cost. arXiv preprint arXiv:2208.10842, 2022.

[23] Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G Baraniuk, Zhangyang Wang, và Yingyan Lin. Drawing early-bird tickets: Towards more efficient training of deep networks. arXiv preprint arXiv:1909.11957, 2019.

[24] Ajay Kumar Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding, và Zhangyang Wang. Training your sparse neural network better with any mask. Trong International Conference on Machine Learning, trang 9833–9844. PMLR, 2022.

[25] Namhoon Lee, Thalaiyasingam Ajanthan, và Philip Torr. Snip: Single-shot network pruning based on connection sensitivity. Trong International Conference on Learning Representations, 2019.

[26] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, và Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:5824–5836, 2020.

[27] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, và Michael Carbin. The lottery ticket hypothesis at scale. arXiv preprint arXiv:1903.01611, 2019.

[28] Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, và Jingjing Liu. Earlybert: Efficient bert training via early-bird lottery tickets. arXiv preprint arXiv:2101.00063, 2020.

[29] Sai Prasanna, Anna Rogers, và Anna Rumshisky. When bert plays the lottery, all tickets are winning. arXiv preprint arXiv:2005.00561, 2020.

[30] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, và Zhangyang Wang. Chasing sparsity in vision transformers: An end-to-end exploration. Advances in Neural Information Processing Systems, 34:19974–19988, 2021.

[31] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.

[32] Yann LeCun, John S Denker, và Sara A Solla. Optimal brain damage. Trong Advances in neural information processing systems, trang 598–605, 1990.

[33] Song Han, Huizi Mao, và William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. Trong International Conference on Learning Representations, 2016.

[34] Song Han, Jeff Pool, John Tran, và William Dally. Learning both weights and connections for efficient neural network. Trong C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, và R. Garnett, editors, Advances in Neural Information Processing Systems 28, trang 1135–1143. Curran Associates, Inc., 2015.

[35] Yann LeCun, John S. Denker, và Sara A. Solla. Optimal brain damage. Trong D. S. Touretzky, editor, Advances in Neural Information Processing Systems 2, trang 598–605. Morgan-Kaufmann, 1990.

[36] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. Learning efficient convolutional networks through network slimming. Trong Proceedings of the IEEE International Conference on Computer Vision, trang 2736–2744, 2017.

[37] Yihui He, Xiangyu Zhang, và Jian Sun. Channel pruning for accelerating very deep neural networks. Trong Proceedings of the IEEE International Conference on Computer Vision, 2017.

[38] Hao Zhou, Jose M Alvarez, và Fatih Porikli. Less is more: Towards compact cnns. Trong European Conference on Computer Vision, trang 662–677. Springer, 2016.

[39] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, và Jan Kautz. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.

[40] Victor Sanh, Thomas Wolf, và Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning. Trong H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, và H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, trang 20378–20389. Curran Associates, Inc., 2020.

[41] Hao Jiang, Ke Zhan, Jianwei Qu, Yongkang Wu, Zhaoye Fei, Xinyu Zhang, Lei Chen, Zhicheng Dou, Xipeng Qiu, Zikai Guo, et al. Towards more effective and economic sparsely-activated model. arXiv preprint arXiv:2110.07431, 2021.

[42] Babak Hassibi và David Stork. Second order derivatives for network pruning: Optimal brain surgeon. Advances in neural information processing systems, 5, 1992.

[43] Xin Dong, Shangyu Chen, và Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. Advances in Neural Information Processing Systems, 30, 2017.

[44] William Finnoff, Ferdinand Hergert, và Hans Georg Zimmermann. Improving model selection by nonconvergent methods. Neural Networks, 6(6):771–783, 1993.

[45] Christos Louizos, Max Welling, và Diederik P Kingma. Learning sparse neural networks through l_0 regularization. arXiv preprint arXiv:1712.01312, 2017.

[46] Jian-Hao Luo và Jianxin Wu. Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference. Pattern Recognition, 107:107461, 2020.

[47] Pedro Savarese, Hugo Silva, và Michael Maire. Winning the lottery with continuous sparsification. Trong Advances in Neural Information Processing Systems 33 pre-proceedings, 2020.

[48] Vikash Sehwag, Shiqi Wang, Prateek Mittal, và Suman Jana. Hydra: Pruning adversarially robust neural networks. Advances in Neural Information Processing Systems, 33, 2020.

[49] Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, và Sijia Liu. Advancing model pruning via bi-level optimization. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.

[50] Michael Zhu và Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017.

[51] Trevor Gale, Erich Elsen, và Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019.

[52] Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, và Martin Jaggi. Dynamic model pruning with feedback. Trong International Conference on Learning Representations, 2020.

[53] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, và Decebal Constantin Mocanu. Sparse training via boosting pruning plasticity with neuroregeneration. Advances in Neural Information Processing Systems (NeurIPs)., 2021.

[54] Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, và Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 9(1):2383, 2018.

[55] Hesham Mostafa và Xin Wang. Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization. Trong International Conference on Machine Learning, 2019.

[56] Tim Dettmers và Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. arXiv preprint arXiv:1907.04840, 2019.

[57] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, và Erich Elsen. Rigging the lottery: Making all tickets winners. Trong International Conference on Machine Learning, trang 2943–2952. PMLR, 2020.

[58] Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, và Mykola Pechenizkiy. Do we actually need dense over-parameterization? in-time over-parameterization in sparse training. arXiv preprint arXiv:2102.02887, 2021.

[59] Jonathan Schwarz, Siddhant Jayakumar, Razvan Pascanu, Peter E Latham, và Yee Teh. Power-propagation: A sparsity inducing weight reparameterisation. Advances in neural information processing systems, 34:28889–28903, 2021.

[60] Chaoqi Wang, Guodong Zhang, và Roger Grosse. Picking winning tickets before training by preserving gradient flow. Trong International Conference on Learning Representations, 2020.

[61] Hidenori Tanaka, Daniel Kunin, Daniel LK Yamins, và Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. Trong Advances in Neural Information Processing Systems 33 pre-proceedings, 2020.

[62] Pau de Jorge, Amartya Sanyal, Harkirat S Behl, Philip HS Torr, Gregory Rogez, và Puneet K Dokania. Progressive skeletonization: Trimming more fat from a network at initialization. arXiv preprint arXiv:2006.09081, 2020.

[63] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, và Armand Joulin. Emerging properties in self-supervised vision transformers. Trong Proceedings of the IEEE/CVF international conference on computer vision, trang 9650–9660, 2021.

[64] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[65] Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, và Moshe Wasserblat. Prune once for all: Sparse pre-trained language models. arXiv preprint arXiv:2111.05754, 2021.

[66] Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, và Dan Alistarh. The optimal bert surgeon: Scalable and accurate second-order pruning for large language models. arXiv preprint arXiv:2203.07259, 2022.

[67] Dongkuan Xu, Ian EH Yen, Jinxi Zhao, và Zhibin Xiao. Rethinking network pruning–under the pre-train and fine-tune paradigm. arXiv preprint arXiv:2104.08682, 2021.

[68] François Lagunas, Ella Charlaix, Victor Sanh, và Alexander M Rush. Block pruning for faster transformers. arXiv preprint arXiv:2109.04838, 2021.

[69] Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, và Tuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight importance. Trong International Conference on Machine Learning, trang 26809–26823. PMLR, 2022.

[70] Elias Frantar, Eldar Kurtic, và Dan Alistarh. M-fac: Efficient matrix-free approximations of second-order information. Advances in Neural Information Processing Systems, 34:14873–14886, 2021.

[71] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

[72] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, và Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, trang 4938–4947, 2020.

[73] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.

[74] Ya Le và Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.

[75] Shiwei Liu, Tianlong Chen, Zhenyu Zhang, Xuxi Chen, Tianjin Huang, Ajay Jaiswal, và Zhangyang Wang. Sparsity may cry: Let us fail (current) sparse neural networks together! arXiv preprint arXiv:2303.02141, 2023.

[76] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, và Hannaneh Hajishirzi. MAWPS: A math word problem repository. Trong Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 1152–1157, San Diego, California, June 2016. Association for Computational Linguistics.

[77] Shen-Yun Miao, Chao-Chun Liang, và Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv:2106.15772, 2021.

[78] Arkil Patel, Satwik Bhattamishra, và Navin Goyal. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191, 2021.

[79] William Fedus, Barret Zoph, và Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232–5270, 2022.

[80] Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, và Hongsheng Li. Learning n: m fine-grained structured sparse neural networks from scratch. arXiv preprint arXiv:2102.04010, 2021.

[81] Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, và Hanie Sedghi. Exploring the limits of large scale pre-training. arXiv preprint arXiv:2110.02095, 2021.

[82] Donghyun Kim, Kaihong Wang, Stan Sclaroff, và Kate Saenko. A broad study of pre-training for domain generalization and adaptation. Trong European Conference on Computer Vision, 2022.

[83] Simon Kornblith, Jonathon Shlens, và Quoc V Le. Do better imagenet models transfer better? Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 2661–2671, 2019.

[84] Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Antoni Chan, và Rong Jin. Improved fine-tuning by leveraging pre-training data: Theory and practice. arXiv preprint arXiv:2111.12292, 2021.

[85] Elias Frantar và Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.

[86] Xinyin Ma, Gongfan Fang, và Xinchao Wang. Llm-pruner: On the structural pruning of large language models. arXiv preprint arXiv:2305.11627, 2023.
