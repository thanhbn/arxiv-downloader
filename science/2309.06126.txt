# 2309.06126.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/science/2309.06126.pdf
# File size: 1979196 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
AstroLLaMA
 : Towards Specialized Foundation Models in Astronomy
Tuan Dung Nguyen1, 2*, Yuan-Sen Ting2, 3*, Ioana Ciuc ˘a2*
Charles O’Neill2†,Ze-Chang Sun4†,Maja Jabło ´nska2†,Sandor Kruk5†
Ernest Perkowski5,Jack Miller2,Jason Li6,Josh Peek7Kartheik Iyer8,
Tomasz Ró ˙za´nski2,9,Pranav Khetarpal10,Sharaf Zaman2,David Brodrick2
Sergio J. Rodríguez Méndez2,Thang Bui2,Alyssa Goodman11,Alberto Accomazzi12,
Jill Naiman13,Jesse Cranney2,Kevin Schawinski14,UniverseTBD
1University of Pennsylvania, United States2Australian National University, Australia
3Ohio State University, United States4Tsinghua University, China
5European Space Astronomy Centre, Spain
6Learning Machines, Australia7Space Telescope Science Institute, United States
8Columbia University, United States9Wrocław University, Poland
10Indian Institute of Technology Delhi, India11Harvard University, United States
12NASA Astrophysics Data System, Harvard & Smithsonian, United States
13University of Illinois at Urbana-Champaign14Modulos AG
Abstract
Large language models excel in many human-
language tasks but often falter in highly spe-
cialized domains like scholarly astronomy. To
bridge this gap, we introduce AstroLLaMA,
a 7-billion-parameter model fine-tuned from
LLaMA-2 using over 300,000 astronomy ab-
stracts from arXiv. Optimized for tradi-
tional causal language modeling, AstroLLaMA
achieves a 30% lower perplexity than Llama-
2, showing marked domain adaptation. Our
model generates more insightful and scientif-
ically relevant text completions and embed-
ding extraction than state-of-the-arts founda-
tion models despite having significantly fewer
parameters. AstroLLaMA serves as a robust,
domain-specific model with broad fine-tuning
potential. Its public release aims to spur
astronomy-focused research, including auto-
matic paper summarization and conversational
agent development.
1 Introduction
The advent of Large Language Models (LLMs) has
sparked interdisciplinary interest thanks to a conflu-
ence of factors: accumulation of massive datasets,
leaps in computational power and breakthroughs in
neural architectures. Flagship models like GPT-4
(OpenAI, 2023), PaLM (Chowdhery et al., 2022;
Goo) and LLaMA (Touvron et al., 2023; Meta,
2023) have exhibited exceptional versatility in a
variety of tasks from logical reasoning and compre-
hension to creative writing, often accomplished via
*Lead contribution. Email: joshtn@seas.upenn.edu
†Major contribution.methods like prompting, fine-tuning, and human-
in-the-loop reinforcement learning.
The astronomy discipline presents both a unique
challenge and a fertile ground for the application
of LLMs. First, the corpus of scholarly texts in
astronomy likely constitutes but a minuscule por-
tion of the data on which generic LLMs are trained,
resulting in limitations like hallucinations in fa-
vor of more “generic” responses. Second, the na-
ture of astronomical research often involves cross-
disciplinary insights due to universally applicable
physical processes. When well-curated, LLMs
could meaningfully assist in hypothesis generation.
Existing scales based on in-context prompting
and instruction learning, primarily involving GPT-
4, have already demonstrated significant potential
for generating substantive hypotheses (Ciuc ˘a and
Ting, 2023; Ciuc ˘a et al., 2023). Further, the as-
tronomy community’s “open sky” policy, which
grants public access to the majority of its datasets
either immediately or after a brief proprietary pe-
riod (Almeida et al., 2023; Fabricius et al., 2021),
pairs well with the wealth of resources available
in archives like NASA’s Astrophysics Data System
(Accomazzi et al., 2015; Borgman and Wofford,
2021). Such an open-access policy can facilitate
deep engagement with the astronomical literature.
Despite their general capabilities, LLMs fre-
quently lag behind specialized, smaller models in
domain-specific applications. This disparity stems
from two primary factors: (i) the eclectic nature
of the training datasets, which dilutes the focus
on specialized subjects, and (ii) the design ethosarXiv:2309.06126v1  [astro-ph.IM]  12 Sep 2023

--- PAGE 2 ---
0 50 100 150 200
Processed tokens (millions)78910Training perplexityEpoch 1 Epoch 2 Epoch 3Figure 1: Learning curve of AstroLLaMA during its
fine-tuning on the arXiv astrophysics dataset. The
Fig.tracks the evolution of perplexity, a measure of
the model’s next-token prediction performance. The
light blue curve shows the training perplexity at each
AdamW update step, while the dark black curve pro-
vides a smoothed average taken over 10-step intervals.
of LLMs as “foundation models” meant for sub-
sequent fine-tuning tailored to specific tasks. The
existing landscape for fine-tuned LLMs in astron-
omy remains limited, however. To our knowledge,
the only existing specialized model is astroBERT
(Grezes et al., 2021), which has 110 million pa-
rameters, trained on nearly 400,000 ADS papers.
But as an non-generative model, the utility of as-
troBERT remains limited to discriminative tasks.
Motivated by these gaps, we present AstroL-
LaMA, a state-of-the-art generative language
model fine-tuned from LLaMA-2. Our model lever-
ages a corpus of 300,000 astronomy abstracts from
arXiv and boasts an architecture approximately 67
times larger than that of astroBERT. AstroLLaMA
aspires to build upon astroBERT’s foundation by
offering improved performance in generating spe-
cialized information.
2 AstroLLaMA
In this section, we discuss AstroLLaMA’s imple-
mentation, focusing on the curation of its dataset,
base model architecture, and fine-tuning settings.
2.1 Dataset
We derive our dataset from the arXiv repository,
available on Kaggle.†Our curated subset focuses
on papers classified under the astrophysics category
(astro-ph ), resulting in a collection of 326,238
articles spanning from April 1992 to July 2023. We
extract the these papers’ abstracts to form a corpus
†https://www.kaggle.com/Cornell-University/
arxivconsisting of approximately 95 million tokens. The
median length of these abstracts is 291 tokens. To
enable effective model evaluation, we randomly
designate 20% of this curated dataset for testing.
2.2 Base Model
Our base model is LLaMA-2, a 6.7 billion-
parameter model developed by Meta (Meta, 2023).
Originally trained on a corpus containing 2 trillion
tokens, LLaMA-2 features a context window of
4,096 tokens. For tokenization, the model employs
a bytepair encoding strategy (Sennrich et al., 2016;
Kudo and Richardson, 2018), incorporating a vo-
cabulary set of 32,000 unique tokens.
2.3 Fine-tuning Settings
For the fine-tuning phase, we rely on our curated
training set described in Section 2.1, which in-
cludes 77 million tokens. Special [BOS] (Begin-
ning Of Sequence) and [EOS] (End Of Sequence)
tokens are prepended and appended to each training
sequence. These sequences are then concatenated
and divided into fixed-length chunks, each compris-
ing 512 tokens.
The fine-tuning process follows the causal lan-
guage modeling objective employed during the
model’s pre-training phase. We use the AdamW
optimizer (Loshchilov and Hutter, 2018) with hy-
perparameters β1= 0.9, β2= 0.95, ϵ= 10−5and
a batch size of 32. The learning rate follows a co-
sine schedule with a linear warmup to a peak value
of3×10−4in the first 10% of the optimization
steps and a final learning rate of 10% of its peak.
Additional settings include weight decay and gra-
dient clipping values of 0.1 and 1.0, respectively.
We fine-tune LLaMA over nearly three epochs,
corresponding to about 230 million processed
tokens, using four NVIDIA A100 GPUs, each
equipped with 40GB of VRAM. To maximize re-
source efficiency, we employ 4-bit quantization
and utilize LoRA, a technique based on low-rank
matrix decomposition (Hu et al., 2021). We set
LoRA’s hyperparameters αand dropout rate to 32
and 0.05, respectively. The entire process is facili-
tated through the Hugging Face Python library.
2.4 Fine-Tuning Evaluation
Fig. 1 depicts the performance of AstroLLaMA
during its fine-tuning phase. Here, we present per-
plexity, a commonly used metric for evaluating
causal language models. Perplexity is defined as

--- PAGE 3 ---
The Magellanic Stream (MS) - an enormous ribbon of gas spanning 140∘ of the southern sky trailing the Magellanic Clouds - has been exquisitely mapped in the five decades since its discovery. However, despite concerted efforts, no stellar counterpart to the MS has been conclusively identified. This stellar stream would reveal the distance and 6D kinematics of the MS, constraining its formation and the past orbital history of the Clouds. We have been conducting a spectroscopic survey of the most distant and luminous red giant stars in the Galactic outskirts. From this dataset, we have discovered a prominent population of 13 stars matching the extreme angular momentum of the Clouds, spanning up to 100∘ along the MS at distances of 60−120 kpc. Furthermore, these kinemetically-selected stars lie along a [α/Fe]-deficient track in chemical space from −2.5<[Fe/H]<−0.5, consistent with their formation in the Clouds themselves. We identify these stars as high-confidence members of the Magellanic Stellar Stream. Half of these stars are metal-rich and closely follow the gaseous MS, whereas the other half are more scattered and metal-poor. We argue that the metal-rich stream is the recently-formed tidal counterpart to the MS, and speculate that the metal-poor population was thrown out of the SMC outskirts during an earlier interaction between the Clouds. The Magellanic Stellar Stream provides a strong set of constraints - distances, 6D kinematics, and birth locations - that will guide future simulations towards unveiling the detailed history of the Clouds. TheMagellanicStream(MS)-anenormousribbonofgasspanning140∘ofthesouthernskytrailingtheMagellanicClouds-hasbeenexquisitelymappedinthefivedecadessinceitsdiscovery.However,despiteconcertedefforts,nostellarcounterparttotheMShasbeenconclusivelyidentified.Thisstellarstreamwouldrevealthedistanceand6DkinematicsoftheMS,constrainingitsformationandthepastorbitalhistoryoftheClouds.Wepropose a novel approach to search for the elusive stellar counterpart of the Magellanic Stream. By employing advanced photometric and spectroscopic techniques, combined with state-of-the-art data mining and machine learning algorithms, we aim to systematically analyze and cross-reference multi-wavelength survey data to identify potential candidate stars belonging to the hypothesized stellar stream. Our methodology involves leveraging the distinctive chemical signatures and proper motions associated with stars originating from the Magellanic Clouds, allowing us to differentiate them from the foreground and background contaminants. If successful, this discovery would not only provide essential insights into the origin and evolution of the Magellanic Stream, but also enhance our understanding of the interaction between the Magellanic Clouds and the Milky Way. Furthermore, the proposed approach establishes a powerful framework for investigating other unresolved galactic structures and tracing their underlying dynamical histories.TheMagellanicStream(MS)-anenormousribbonofgasspanning140∘ofthesouthernskytrailingtheMagellanicClouds-hasbeenexquisitelymappedinthefivedecadessinceitsdiscovery.However,despiteconcertedefforts,nostellarcounterparttotheMShasbeenconclusivelyidentified.Thisstellarstreamwouldrevealthedistanceand6DkinematicsoftheMS,constrainingitsformationandthepastorbitalhistoryoftheClouds.We1) present a new method to select MS candidate stars from wide field optical imaging, 2) apply this method to the MS, 3) compare the results to existing catalogs, and 4) identify the most promising MS candidate stars. We apply our method to 2547 stars in the Gaia EDR3 catalog that have Gaia proper motions consistent with the MS and use these as a pilot sample to optimize our method. We then apply our method to the entire Gaia EDR3 catalog to identify 46,000 stars that are candidates for the MS, and we use the Gaia-ESO Survey to identify 13,000 of these stars that are likely to be MS candidates. We identify 266 stars with (G-K) colours and [Fe/H] that are consistent with previous estimates of the MS. These stars have proper motions and radial velocities that are consistent with the MS, and we place them on a 3D map of the MS. We find a previously unknown stellar stream, which we call the Southeastern Stream (SES), in the direction of the Clouds and MS. The SES has an M31 distance of 32.2+/-0.5 kpc and a 6D velocity of 136+/-10 km/s. The SES is about 20 times brighter in stars than the MS and about 40 times brighter in stars than the SMC. The SES is also 2.7 times more metal-rich than the MS, which suggests that it is a separate population. The MagellanicStream (MS) -an enormous ribbon of gas spanning 140∘of the southern sky trailing the MagellanicClouds -has been exquisitely mapped in the five decades since its discovery. However, despite concerted efforts, no stellar counterpart to the MS has been conclusively identified. This stellar stream would reveal the distance and 6D kinematics of the MS, constraining its formation and the past orbital history of the Clouds. We ﬁnd a stellar stream in the MS, with a 6D velocity of 100 km s−1, and a distance of 100 kpc. The stream is 100 pc wide and 1000 pc long, and is consistent with the MS. The stream is 1000 times more massive than the Magellanic Clouds, and is likely to be a tidal stream from the Large Magellanic Cloud.Original abstractCompleted by GPT-4
Completed by LLaMA-2Completed by AstroLLaMAFigure 2: Completion of an abstract from the arXiv database (ID: 2306.15719) using three different models: GPT-4,
LLaMA-2, and AstroLLaMA. Each model is prompted with the same short text snippet, highlighted in their
respective boxes. GPT-4 tends to produce more generic statements, lacking domain-specific nuance. AstroLLaMA
demonstrates the most robust completion, offering more relevant concepts and deeper insights specific to the field of
astronomy, thus significantly outperforming LLaMA-2 and GPT-4.
the exponentiation of the training loss, with lower
values indicating a better fit.
Our initial observations reveal that LLaMA-2
performs suboptimally on our dataset, with an av-
erage perplexity close to 10. By the conclusion
of three epoch, AstroLLaMA achieves an average
perplexity of 6.55. This represents a 32.5% reduc-
tion in perplexity compared to the base LLaMA-2
model, signifying a substantial improvement in the
model’s predictive accuracy.
3 Results
As illustrated in the previous section, AstroL-
LaMA outperforms its non-fine-tuned counterpart,
LLaMA-2, in terms of context-awareness during
token prediction within astronomy abstracts. To
delve deeper into the advantages of fine-tuning, we
assess AstroLLaMA’s general abilities in two key
aspects: text generation andembedding space
quality . We compare its performance against mul-
tiple models, including LLaMA-2, GPT-4 and GPT-
3 (ada-002) to provide a comprehensive evaluation.
Regarding text generation , we task AstroL-
LaMA, LLaMA-2 and GPT-4 with completing var-
ious astronomy-related abstracts, an example of
which is presented in Fig. 2. Each model is given
the first few sentences of an abstract as a prompt,allowing us to gauge its ability to comprehend the
context and generate a meaningful continuation.
For GPT-4, we utilize ChatGPT and specifically
prompt it to limit the completion to a single para-
graph. AstroLLaMA and LLaMA-2 are deployed
using standard sampling methods, with the temper-
ature set to 0.3 and a maximum new tokens limit of
1,024. We find that altering the temperature setting
does not substantively improve LLaMA-2’s results.
Our observations largely echo the patterns de-
picted in Fig. 2. LLaMA-2 often deviates from the
intended context after generating only a short and
often off-topic continuation, resulting in inferior
completions. While GPT-4 produces more coher-
ent text, its responses are too generic to capture
the nuanced understanding required in the astron-
omy domain. Even when explicitly prompted to
focus on astronomy-related topics, GPT-4’s gener-
ated text remains largely off-target or generically
applicable rather than domain-specific.
In stark contrast, AstroLLaMA exhibits remark-
able context-awareness in its completions by show-
ing a deep understanding of astronomical concepts.
For example, in Fig. 2, AstroLLaMA comprehends
that an effective search for stars in the Magellanic
Stream involves a three-step process: initial wide-
field imaging, followed by refinement using astro-

--- PAGE 4 ---
metric data from Gaia, and then further curation
with spectroscopic data. The model also under-
stands Gaia-ESO is surveying the southern sky
and hence can observe (part of) the Magellanic
Stream. It also demonstrates nuanced knowledge
of the Magellanic Stream, understanding the impor-
tance of bifurcation within the stream. As a result,
it appropriately completes the text by discussing
the southeast stream and exploring metallicity dif-
ferences to ascertain their origins.
Regarding embedding space quality , we as-
sess models’ ability to reflect semantic similari-
ties among astronomy texts. We randomly choose
10,000 abstracts from our dataset and embed them
using AstroLLaMA and GPT-3. Specifically, we
use OpenAI’s API to invoke the text embedding
function for GPT-3 (ada-002). To get text embed-
dings from AstroLLaMA, we pass an input through
the model and extract its final hidden states, which
contain embeddings for all tokens in the input.
Then, we omit the [BOS] token and take the av-
erage of all other tokens’ embeddings to get the
final result. Finally, for each pair of abstracts we
calculate their cosine similarity (the normalised dot
product) between on their vector embeddings.
The top panel of Fig. 3 presents the distribution
of these pairwise similarities for the two embed-
ding methods. We find that the embeddings by
GPT-3 are overly generic with similarities cluster-
ing around relatively high values of 0.7–0.9, sug-
gesting a lack of discriminative power (most papers
are embedded very similarly). AstroLLaMA’s em-
beddings, on the other hand, exhibit much higher
variance within each bin. This suggests that our
fine-tuned model is more adept at representing the
specialized semantic variance inherent to the field
of astronomy, which may enable a more granu-
lar representation of astronomical content and can
facilitate better document retrieval and semantic
analysis.
The bottom panel of Fig. 3 provides two repre-
sentative examples where AstroLLaMA and GPT-3
classifications diverge. In the first example, GPT-3
fixates on the keyword ‘magnetized,’ resulting in
an inflated similarity score, despite the contexts be-
ing markedly different. AstroLLaMA, on the other
hand, successfully distinguishes between these dis-
parate contexts. In the second example, AstroL-
LaMA accurately identifies that the study of Spitzer
is closely related to star formation. GPT-3, how-
ever, fails to make this connection due to the ab-
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Pairwise cosine similarity0100200300400500Density + ShiftGPT-3 (ada) embedding
AstroLLaMA embedding
Figure 3: Top: Distribution of pairwise cosine similari-
ties among 10,000 randomly selected abstracts from our
corpus, divided into 10 equal bins based on similarity
levels from GPT-3. Bottom: Two representative exam-
ples illustrating divergent cosine similarity values when
comparing AstroLLaMA and GPT-3 embeddings.
sence of matching keywords.
4 Limitations and Future Directions
In this work, we introduce AstroLLaMA, a 7-
billion-parameter language model fine-tuned on a
dataset encompassing over 300,000 abstracts from
astronomical research papers. Compared to its
base model, LLaMA-2, and even GPT-4, a cur-
rent state-of-the-art general LLM, AstroLLaMA
exhibits marked improvements in generating high-
quality abstracts with a competent grasp of relevant
information in this literature.
AstroLLaMA is not without limitations, never-
theless. The most salient is the model’s knowledge
gaps in certain areas of astronomy: in Fig. 2, As-
troLLaMA’s estimation of potential star candidates
from Gaia-ESO data is notably inaccurate. To ad-
dress such issues, we are in the process of enriching
AstroLLaMA’s training set with not just abstracts
but the full LaTeX sources of existing astronomy
articles, thereby expanding the token count by ap-
proximately two orders of magnitude. Another
concern lies in the model’s tendency to generate
hallucinated or fictitious numerical data, an issue
likely attributed to our focus on reducing perplexity
rather than explicitly steering the model towards
factual accuracy. The release of AstroLLaMA aims
to facilitate community engagement, both for ad-
dressing these inaccuracies and for refining its bal-

--- PAGE 5 ---
ance between “faithfulness” (respecting scientific
evidence and accuracy) and “creativity” (being able
to come up with interesting hypotheses).
AstroLLaMA stands as a compelling prototype
for specialized LLMs in astronomy, showing supe-
rior context-aware capabilities compared to GPT-
4 despite having much fewer parameters. It not
only paves the way for improved performance in
tasks like question-answering, scientific summa-
rization and hypothesis generation but applies also
to multi-modal models (Liu et al., 2023). We have
made the AstroLLaMA’s weights and its training
data publicly available†for researchers interested
in leveraging LLMs for astronomy-centric applica-
tions. Along with this, we are establishing various
“playgrounds” on Hugging Face to invite interested
readers to further adapt and refine this robust start-
ing point for a variety of relevant downstream tasks.
Acknowledgments
We are deeply grateful to the Microsoft Accelerate
Foundation Models Research Initiative for enabling
us to fast-track our project. Thanks to advanced AI
platform from Microsoft Research, we have been
able to significantly expedite our efforts in using
language models to analyze astronomical literature.
†https://huggingface.co/universeTBD/astrollamaEthics Statement
We obtain the pre-trained weights for LLaMA-2
from Meta, which offers these models for down-
load on Hugging Face. The arXiv dataset used in
this paper is publicly available on Kaggle. While
we have demonstrated that AstroLLaMA is capa-
ble of generating high-quality, relevant abstracts
for astronomical research papers, we have noted
that it has the potential to generate inaccurate data
and measurements. This should serve as a cau-
tion for researchers aiming to use this model for
downstream tasks, and we invite the adoption of
alignment strategies in future work to ameliorate
this issue.
References
Google AI PaLM 2. https://ai.google/discover/palm2/.
A. Accomazzi, M. J. Kurtz, E. A. Henneken, R. Chyla,
J. Luker, C. S. Grant, D. M. Thompson, A. Holachek,
R. Dave, and S. S. Murray. 2015. ADS: The Next
Generation Search Platform. In Open Science at the
Frontiers of Librarianship , volume 492 of Astronom-
ical Society of the Pacific Conference Series , page
189.
Andrés Almeida, Scott F. Anderson, Maria Argudo-
Fernández, Carles Badenes, Kat Barger, Jorge K.
Barrera-Ballesteros, Chad F. Bender, Erika Benitez,
Felipe Besser, Dmitry Bizyaev, Michael R. Blan-
ton, John Bochanski, Jo Bovy, William Nielsen
Brandt, Joel R. Brownstein, Johannes Buchner, Esra
Bulbul, Joseph N. Burchett, Mariana Cano Díaz,
Joleen K. Carlberg, Andrew R. Casey, Vedant Chan-
dra, Brian Cherinka, Cristina Chiappini, Abigail A.
Coker, Johan Comparat, Charlie Conroy, Gabriella
Contardo, Arlin Cortes, Kevin Covey, Jeffrey D.
Crane, Katia Cunha, Collin Dabbieri, James W.
Davidson Jr. au2, Megan C. Davis, Nathan De
Lee, José Eduardo Méndez Delgado, Sebastian De-
masi, Francesco Di Mille, John Donor, Peter Dow,
Tom Dwelly, Mike Eracleous, Jamey Eriksen, Xiao-
hui Fan, Emily Farr, Sara Frederick, Logan Fries,
Peter Frinchaboy, Boris T. Gaensicke, Junqiang
Ge, Consuelo González Ávila, Katie Grabowski,
Catherine Grier, Guillaume Guiglion, Pramod Gupta,
Patrick Hall, Keith Hawkins, Christian R. Hayes,
J. J. Hermes, Lorena Hernández-García, David W.
Hogg, Jon A. Holtzman, Hector Javier Ibarra-
Medel, Alexander Ji, Paula Jofre, Jennifer A. John-
son, Amy M. Jones, Karen Kinemuchi, Matthias
Kluge, Anton Koekemoer, Juna A. Kollmeier, Marina
Kounkel, Dhanesh Krishnarao, Mirko Krumpe, Ivan
Lacerna, Paulo Jakson Assuncao Lago, Chervin La-
porte, Ang Liu, Chao Liu, Xin Liu, Alexandre Roman
Lopes, Matin Macktoobian, Viktor Malanushenko,
Dan Maoz, Thomas Masseron, Karen L. Masters,

--- PAGE 6 ---
Gal Matijevic, Aidan McBride, Ilija Medan, An-
drea Merloni, Sean Morrison, Natalie Myers, Sz-
abolcs Mészáros, C. Alenka Negrete, David L. Nide-
ver, Christian Nitschelm, Audrey Oravetz, Daniel
Oravetz, Kaike Pan, Yingjie Peng, Marc H. Pin-
sonneault, Rick Pogge, Dan Qiu, Anna Barbara
de Andrade Queiroz, Solange V . Ramirez, Hans-
Walter Rix, Daniela Fernández Rosso, Jessie Run-
noe, Mara Salvato, Sebastian F. Sanchez, Felipe A.
Santana, Andrew Saydjari, Conor Sayres, Kevin C.
Schlaufman, Donald P. Schneider, Axel Schwope,
Javier Serna, Yue Shen, Jennifer Sobeck, Ying-Yi
Song, Diogo Souto, Taylor Spoo, Keivan G. Stassun,
Matthias Steinmetz, Ilya Straumit, Guy Stringfellow,
José Sánchez-Gallego, Manuchehr Taghizadeh-Popp,
Jamie Tayar, Ani Thakar, Patricia B. Tissera, An-
drew Tkachenko, Hector Hernandez Toledo, Benny
Trakhtenbrot, Jose G. Fernandez Trincado, Nicholas
Troup, Jonathan R. Trump, Sarah Tuttle, Natalie Ul-
loa, Jose Antonio Vazquez-Mata, Pablo Vera Alfaro,
Sandro Villanova, Stefanie Wachter, Anne-Marie
Weijmans, Adam Wheeler, John Wilson, Leigh Wo-
jno, Julien Wolf, Xiang-Xiang Xue, Jason E. Ybarra,
Eleonora Zari, and Gail Zasowski. 2023. The eigh-
teenth data release of the sloan digital sky surveys:
Targeting and first spectra from sdss-v.
Christine L. Borgman and Morgan F. Wofford. 2021.
From Data Processes to Data Products: Knowledge
Infrastructures in Astronomy. arXiv e-prints , page
arXiv:2109.01707.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. PaLM: Scaling Language
Modeling with Pathways.
Ioana Ciuc ˘a and Yuan-Sen Ting. 2023. Galactic
ChitChat: Using Large Language Models to Con-
verse with Astronomy Literature. arXiv e-prints ,
page arXiv:2304.05406.
Ioana Ciuc ˘a, Yuan-Sen Ting, Sandor Kruk, and Kartheik
Iyer. 2023. Harnessing the Power of Adversarial
Prompting and Large Language Models for RobustHypothesis Generation in Astronomy. arXiv e-prints ,
page arXiv:2306.11648.
C. Fabricius, X. Luri, F. Arenou, C. Babusiaux,
A. Helmi, T. Muraveva, C. Reylé , F. Spoto,
A. Vallenari, T. Antoja, E. Balbinot, C. Barache,
N. Bauchet, A. Bragaglia, D. Busonero, T. Cantat-
Gaudin, J. M. Carrasco, S. Diakité, M. Fabrizio,
F. Figueras, A. Garcia-Gutierrez, A. Garofalo,
C. Jordi, P. Kervella, S. Khanna, N. Leclerc, E. Li-
cata, S. Lambert, P. M. Marrese, A. Masip, P. Ramos,
N. Robichon, A. C. Robin, M. Romero-Gómez,
S. Rubele, and M. Weiler. 2021. Gaia early data
release 3. Astronomy & Astrophysics , 649:A5.
Felix Grezes, Sergi Blanco-Cuaresma, Alberto Acco-
mazzi, Michael J. Kurtz, Golnaz Shapurian, Edwin
Henneken, Carolyn S. Grant, Donna M. Thomp-
son, Roman Chyla, Stephen McDonald, Timothy W.
Hostetler, Matthew R. Templeton, Kelly E. Lockhart,
Nemanja Martinovic, Shinyi Chen, Chris Tanner, and
Pavlos Protopapas. 2021. Building astroBERT, a
language model for Astronomy & Astrophysics.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2021. LoRA: Low-Rank Adaptation
of Large Language Models.
Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for Neural Text Processing.
InProceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 66–71.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023. Visual instruction tuning.
Ilya Loshchilov and Frank Hutter. 2018. Decoupled
Weight Decay Regularization. In International Con-
ference on Learning Representations .
Meta. 2023. Llama 2: Open Foundation and
Fine-Tuned Chat Models | Meta AI Research.
https://ai.meta.com/research/publications/llama-2-
open-foundation-and-fine-tuned-chat-models/.
OpenAI. 2023. GPT-4 Technical Report.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 1715–1725.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. LLaMA: Open
and Efficient Foundation Language Models.
