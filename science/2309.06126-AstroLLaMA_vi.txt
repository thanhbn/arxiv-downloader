# AstroLLaMA: Hướng tới các Mô hình Nền tảng Chuyên biệt trong Thiên văn học

Tuan Dung Nguyen1, 2*, Yuan-Sen Ting2, 3*, Ioana Ciucă2*
Charles O'Neill2†,Ze-Chang Sun4†,Maja Jabłońska2†,Sandor Kruk5†
Ernest Perkowski5,Jack Miller2,Jason Li6,Josh Peek7Kartheik Iyer8,
Tomasz Różański2,9,Pranav Khetarpal10,Sharaf Zaman2,David Brodrick2
Sergio J. Rodríguez Méndez2,Thang Bui2,Alyssa Goodman11,Alberto Accomazzi12,
Jill Naiman13,Jesse Cranney2,Kevin Schawinski14,UniverseTBD

1Đại học Pennsylvania, Hoa Kỳ 2Đại học Quốc gia Úc, Úc
3Đại học Bang Ohio, Hoa Kỳ 4Đại học Thanh Hoa, Trung Quốc
5Trung tâm Thiên văn học Không gian Châu Âu, Tây Ban Nha
6Learning Machines, Úc 7Viện Khoa học Kính thiên văn Không gian, Hoa Kỳ
8Đại học Columbia, Hoa Kỳ 9Đại học Wrocław, Ba Lan
10Viện Công nghệ Ấn Độ Delhi, Ấn Độ 11Đại học Harvard, Hoa Kỳ
12Hệ thống Dữ liệu Vật lý Thiên văn NASA, Harvard & Smithsonian, Hoa Kỳ
13Đại học Illinois tại Urbana-Champaign 14Modulos AG

## Tóm tắt

Các mô hình ngôn ngữ lớn thể hiện xuất sắc trong nhiều nhiệm vụ ngôn ngữ con người nhưng thường gặp khó khăn trong các lĩnh vực chuyên biệt cao như thiên văn học học thuật. Để khắc phục khoảng cách này, chúng tôi giới thiệu AstroLLaMA, một mô hình 7 tỷ tham số được tinh chỉnh từ LLaMA-2 sử dụng hơn 300.000 tóm tắt thiên văn học từ arXiv. Được tối ưu hóa cho mô hình ngôn ngữ nhân quả truyền thống, AstroLLaMA đạt được độ phức tạp thấp hơn 30% so với Llama-2, cho thấy sự thích ứng lĩnh vực rõ rệt. Mô hình của chúng tôi tạo ra các hoàn thành văn bản và trích xuất nhúng có tính hiểu biết sâu sắc và liên quan khoa học hơn so với các mô hình nền tảng tiên tiến nhất mặc dù có ít tham số hơn đáng kể. AstroLLaMA đóng vai trò như một mô hình mạnh mẽ, chuyên biệt lĩnh vực với tiềm năng tinh chỉnh rộng rãi. Việc phát hành công khai của nó nhằm mục đích thúc đẩy nghiên cứu tập trung vào thiên văn học, bao gồm tóm tắt bài báo tự động và phát triển tác nhân đối thoại.

## 1 Giới thiệu

Sự ra đời của các Mô hình Ngôn ngữ Lớn (LLM) đã khơi dậy sự quan tâm liên ngành nhờ vào sự hội tụ của các yếu tố: tích lũy các bộ dữ liệu khổng lồ, những bước nhảy vọt trong sức mạnh tính toán và các đột phá trong kiến trúc mạng nơ-ron. Các mô hình hàng đầu như GPT-4 (OpenAI, 2023), PaLM (Chowdhery et al., 2022; Goo) và LLaMA (Touvron et al., 2023; Meta, 2023) đã thể hiện tính linh hoạt đặc biệt trong nhiều nhiệm vụ khác nhau từ lý luận logic và hiểu biết đến viết sáng tạo, thường được thực hiện thông qua các phương pháp như định hướng, tinh chỉnh và học tăng cường có con người tham gia.

Ngành thiên văn học đưa ra cả thách thức độc đáo và nền tảng màu mỡ cho việc ứng dụng LLM. Thứ nhất, tập hợp các văn bản học thuật trong thiên văn học có thể chỉ chiếm một phần rất nhỏ trong dữ liệu mà các LLM chung được huấn luyện, dẫn đến những hạn chế như ảo giác có lợi cho các phản hồi "chung chung" hơn. Thứ hai, bản chất của nghiên cứu thiên văn học thường liên quan đến các hiểu biết liên ngành do các quá trình vật lý có thể áp dụng phổ quát. Khi được tuyển chọn tốt, LLM có thể hỗ trợ có ý nghĩa trong việc tạo ra giả thuyết.

Các quy mô hiện tại dựa trên định hướng trong ngữ cảnh và học tập hướng dẫn, chủ yếu liên quan đến GPT-4, đã chứng minh tiềm năng đáng kể trong việc tạo ra các giả thuyết quan trọng (Ciucă và Ting, 2023; Ciucă et al., 2023). Hơn nữa, chính sách "bầu trời mở" của cộng đồng thiên văn học, cho phép truy cập công khai vào phần lớn các bộ dữ liệu của họ ngay lập tức hoặc sau một thời gian độc quyền ngắn (Almeida et al., 2023; Fabricius et al., 2021), kết hợp tốt với sự phong phú của các tài nguyên có sẵn trong các kho lưu trữ như Hệ thống Dữ liệu Vật lý Thiên văn của NASA (Accomazzi et al., 2015; Borgman và Wofford, 2021). Chính sách truy cập mở như vậy có thể tạo điều kiện cho việc tham gia sâu sắc với tài liệu thiên văn học.

Mặc dù có khả năng chung, LLM thường tụt hậu so với các mô hình chuyên biệt, nhỏ hơn trong các ứng dụng cụ thể theo lĩnh vực. Sự chênh lệch này xuất phát từ hai yếu tố chính: (i) bản chất chiết trung của các bộ dữ liệu huấn luyện, làm loãng sự tập trung vào các chủ đề chuyên biệt, và (ii) tinh thần thiết kế của LLM như "mô hình nền tảng" có nghĩa là để tinh chỉnh tiếp theo phù hợp với các nhiệm vụ cụ thể. Tuy nhiên, bối cảnh hiện tại cho các LLM được tinh chỉnh trong thiên văn học vẫn còn hạn chế. Theo hiểu biết của chúng tôi, mô hình chuyên biệt duy nhất hiện có là astroBERT (Grezes et al., 2021), có 110 triệu tham số, được huấn luyện trên gần 400.000 bài báo ADS. Nhưng là một mô hình không tạo sinh, tiện ích của astroBERT vẫn còn hạn chế đối với các nhiệm vụ phân biệt.

Được thúc đẩy bởi những khoảng trống này, chúng tôi trình bày AstroLLaMA, một mô hình ngôn ngữ tạo sinh tiên tiến được tinh chỉnh từ LLaMA-2. Mô hình của chúng tôi tận dụng một tập hợp 300.000 tóm tắt thiên văn học từ arXiv và tự hào có kiến trúc lớn hơn khoảng 67 lần so với astroBERT. AstroLLaMA mong muốn xây dựng dựa trên nền tảng của astroBERT bằng cách cung cấp hiệu suất cải thiện trong việc tạo ra thông tin chuyên biệt.

## 2 AstroLLaMA

Trong phần này, chúng tôi thảo luận về việc triển khai AstroLLaMA, tập trung vào việc tuyển chọn bộ dữ liệu, kiến trúc mô hình cơ sở và các thiết lập tinh chỉnh.

### 2.1 Bộ dữ liệu

Chúng tôi lấy bộ dữ liệu từ kho lưu trữ arXiv, có sẵn trên Kaggle. Tập con được tuyển chọn của chúng tôi tập trung vào các bài báo được phân loại dưới danh mục vật lý thiên văn (astro-ph), dẫn đến một bộ sưu tập 326.238 bài viết trải dài từ tháng 4 năm 1992 đến tháng 7 năm 2023. Chúng tôi trích xuất tóm tắt của những bài báo này để tạo thành một tập hợp gồm khoảng 95 triệu token. Độ dài trung vị của những tóm tắt này là 291 token. Để cho phép đánh giá mô hình hiệu quả, chúng tôi ngẫu nhiên chỉ định 20% bộ dữ liệu được tuyển chọn này cho kiểm tra.

### 2.2 Mô hình cơ sở

Mô hình cơ sở của chúng tôi là LLaMA-2, một mô hình 6,7 tỷ tham số được phát triển bởi Meta (Meta, 2023). Ban đầu được huấn luyện trên một tập hợp chứa 2 nghìn tỷ token, LLaMA-2 có cửa sổ ngữ cảnh 4.096 token. Đối với tokenization, mô hình sử dụng chiến lược mã hóa bytepair (Sennrich et al., 2016; Kudo và Richardson, 2018), kết hợp một tập từ vựng gồm 32.000 token duy nhất.

### 2.3 Thiết lập tinh chỉnh

Đối với giai đoạn tinh chỉnh, chúng tôi dựa vào tập huấn luyện được tuyển chọn được mô tả trong Phần 2.1, bao gồm 77 triệu token. Các token đặc biệt [BOS] (Bắt đầu Chuỗi) và [EOS] (Kết thúc Chuỗi) được thêm vào đầu và cuối mỗi chuỗi huấn luyện. Những chuỗi này sau đó được nối lại và chia thành các khối có độ dài cố định, mỗi khối gồm 512 token.

Quá trình tinh chỉnh tuân theo mục tiêu mô hình ngôn ngữ nhân quả được sử dụng trong giai đoạn tiền huấn luyện của mô hình. Chúng tôi sử dụng bộ tối ưu AdamW (Loshchilov và Hutter, 2018) với các siêu tham số β1= 0,9, β2= 0,95, ϵ= 10−5 và kích thước batch là 32. Tốc độ học tập tuân theo lịch trình cosine với khởi động tuyến tính đến giá trị đỉnh 3×10−4 trong 10% đầu của các bước tối ưu hóa và tốc độ học tập cuối cùng là 10% của đỉnh. Các thiết lập bổ sung bao gồm giá trị phân rã trọng số và cắt gradient lần lượt là 0,1 và 1,0.

Chúng tôi tinh chỉnh LLaMA qua gần ba epoch, tương ứng với khoảng 230 triệu token đã xử lý, sử dụng bốn GPU NVIDIA A100, mỗi GPU được trang bị 40GB VRAM. Để tối đa hóa hiệu quả tài nguyên, chúng tôi sử dụng lượng tử hóa 4-bit và sử dụng LoRA, một kỹ thuật dựa trên phân tách ma trận thứ hạng thấp (Hu et al., 2021). Chúng tôi đặt các siêu tham số α và tỷ lệ dropout của LoRA lần lượt là 32 và 0,05. Toàn bộ quá trình được hỗ trợ thông qua thư viện Python Hugging Face.

### 2.4 Đánh giá tinh chỉnh

Hình 1 mô tả hiệu suất của AstroLLaMA trong giai đoạn tinh chỉnh. Ở đây, chúng tôi trình bày độ phức tạp, một chỉ số thường được sử dụng để đánh giá các mô hình ngôn ngữ nhân quả. Độ phức tạp được định nghĩa là số mũ của mất mát huấn luyện, với các giá trị thấp hơn cho thấy độ phù hợp tốt hơn.

Các quan sát ban đầu của chúng tôi cho thấy LLaMA-2 hoạt động không tối ưu trên bộ dữ liệu của chúng tôi, với độ phức tạp trung bình gần 10. Đến cuối ba epoch, AstroLLaMA đạt được độ phức tạp trung bình là 6,55. Điều này thể hiện sự giảm 32,5% độ phức tạp so với mô hình LLaMA-2 cơ sở, biểu thị sự cải thiện đáng kể trong độ chính xác dự đoán của mô hình.

## 3 Kết quả

Như được minh họa trong phần trước, AstroLLaMA vượt trội hơn đối tác chưa được tinh chỉnh, LLaMA-2, về mặt nhận thức ngữ cảnh trong quá trình dự đoán token trong các tóm tắt thiên văn học. Để đi sâu hơn vào các lợi ích của tinh chỉnh, chúng tôi đánh giá khả năng chung của AstroLLaMA trong hai khía cạnh chính: tạo văn bản và chất lượng không gian nhúng. Chúng tôi so sánh hiệu suất của nó với nhiều mô hình, bao gồm LLaMA-2, GPT-4 và GPT-3 (ada-002) để cung cấp đánh giá toàn diện.

Về tạo văn bản, chúng tôi giao nhiệm vụ cho AstroLLaMA, LLaMA-2 và GPT-4 hoàn thành các tóm tắt liên quan đến thiên văn học khác nhau, một ví dụ được trình bày trong Hình 2. Mỗi mô hình được đưa ra vài câu đầu của một tóm tắt như một gợi ý, cho phép chúng tôi đánh giá khả năng hiểu ngữ cảnh và tạo ra sự tiếp nối có ý nghĩa.

Đối với GPT-4, chúng tôi sử dụng ChatGPT và cụ thể gợi ý nó giới hạn hoàn thành trong một đoạn văn duy nhất. AstroLLaMA và LLaMA-2 được triển khai sử dụng các phương pháp lấy mẫu tiêu chuẩn, với nhiệt độ được đặt ở 0,3 và giới hạn token mới tối đa là 1.024. Chúng tôi thấy rằng việc thay đổi thiết lập nhiệt độ không cải thiện đáng kể kết quả của LLaMA-2.

Các quan sát của chúng tôi phần lớn phản ánh các mẫu được mô tả trong Hình 2. LLaMA-2 thường lệch khỏi ngữ cảnh dự định sau khi chỉ tạo ra một sự tiếp nối ngắn và thường ngoài chủ đề, dẫn đến các hoàn thành kém. Trong khi GPT-4 tạo ra văn bản nhất quán hơn, các phản hồi của nó quá chung chung để nắm bắt được sự hiểu biết tinh tế cần thiết trong lĩnh vực thiên văn học. Ngay cả khi được gợi ý rõ ràng tập trung vào các chủ đề liên quan đến thiên văn học, văn bản được tạo ra bởi GPT-4 vẫn phần lớn không đúng mục tiêu hoặc có thể áp dụng chung chung thay vì cụ thể theo lĩnh vực.

Trái ngược hoàn toàn, AstroLLaMA thể hiện khả năng nhận thức ngữ cảnh đáng chú ý trong các hoàn thành của nó bằng cách cho thấy sự hiểu biết sâu sắc về các khái niệm thiên văn học. Ví dụ, trong Hình 2, AstroLLaMA hiểu rằng việc tìm kiếm hiệu quả các ngôi sao trong Dòng Magellanic liên quan đến một quy trình ba bước: chụp ảnh trường rộng ban đầu, sau đó tinh chỉnh bằng dữ liệu thiên văn từ Gaia, và sau đó tuyển chọn thêm với dữ liệu quang phổ. Mô hình cũng hiểu Gaia-ESO đang khảo sát bầu trời phía nam và do đó có thể quan sát (một phần) Dòng Magellanic. Nó cũng thể hiện kiến thức tinh tế về Dòng Magellanic, hiểu tầm quan trọng của sự phân nhánh trong dòng. Kết quả là, nó hoàn thành văn bản một cách thích hợp bằng cách thảo luận về dòng đông nam và khám phá sự khác biệt về kim loại để xác định nguồn gốc của chúng.

Về chất lượng không gian nhúng, chúng tôi đánh giá khả năng của các mô hình phản ánh sự tương tự ngữ nghĩa giữa các văn bản thiên văn học. Chúng tôi ngẫu nhiên chọn 10.000 tóm tắt từ bộ dữ liệu của chúng tôi và nhúng chúng bằng AstroLLaMA và GPT-3. Cụ thể, chúng tôi sử dụng API của OpenAI để gọi hàm nhúng văn bản cho GPT-3 (ada-002). Để có được nhúng văn bản từ AstroLLaMA, chúng tôi truyền đầu vào qua mô hình và trích xuất các trạng thái ẩn cuối cùng, chứa nhúng cho tất cả các token trong đầu vào. Sau đó, chúng tôi bỏ qua token [BOS] và lấy trung bình của nhúng tất cả các token khác để có kết quả cuối cùng. Cuối cùng, cho mỗi cặp tóm tắt, chúng tôi tính toán độ tương tự cosine (tích vô hướng chuẩn hóa) giữa các nhúng vector của chúng.

Bảng trên của Hình 3 trình bày phân phối của những tương tự cặp đôi này cho hai phương pháp nhúng. Chúng tôi thấy rằng các nhúng của GPT-3 quá chung chung với độ tương tự tập trung xung quanh các giá trị tương đối cao 0,7–0,9, gợi ý thiếu sức mạnh phân biệt (hầu hết các bài báo được nhúng rất tương tự). Mặt khác, các nhúng của AstroLLaMA thể hiện phương sai cao hơn nhiều trong mỗi bin. Điều này gợi ý rằng mô hình được tinh chỉnh của chúng tôi khéo léo hơn trong việc đại diện cho sự biến thiên ngữ nghĩa chuyên biệt vốn có của lĩnh vực thiên văn học, có thể cho phép biểu diễn nội dung thiên văn học chi tiết hơn và có thể tạo điều kiện cho việc truy xuất tài liệu và phân tích ngữ nghĩa tốt hơn.

Bảng dưới của Hình 3 cung cấp hai ví dụ đại diện nơi phân loại AstroLLaMA và GPT-3 khác nhau. Trong ví dụ đầu tiên, GPT-3 tập trung vào từ khóa 'từ hóa', dẫn đến điểm tương tự tăng cao, mặc dù các ngữ cảnh rõ ràng khác nhau. Mặt khác, AstroLLaMA thành công phân biệt giữa những ngữ cảnh khác biệt này. Trong ví dụ thứ hai, AstroLLaMA chính xác xác định rằng nghiên cứu về Spitzer có liên quan chặt chẽ đến sự hình thành sao. Tuy nhiên, GPT-3 không thể tạo ra kết nối này do thiếu các từ khóa phù hợp.

## 4 Hạn chế và Hướng phát triển tương lai

Trong công trình này, chúng tôi giới thiệu AstroLLaMA, một mô hình ngôn ngữ 7 tỷ tham số được tinh chỉnh trên một bộ dữ liệu bao gồm hơn 300.000 tóm tắt từ các bài báo nghiên cứu thiên văn học. So với mô hình cơ sở của nó, LLaMA-2, và thậm chí GPT-4, một LLM chung tiên tiến hiện tại, AstroLLaMA thể hiện những cải thiện rõ rệt trong việc tạo ra các tóm tắt chất lượng cao với khả năng nắm bắt thông tin liên quan trong tài liệu này.

Tuy nhiên, AstroLLaMA không phải không có hạn chế. Hạn chế nổi bật nhất là những khoảng trống kiến thức của mô hình trong một số lĩnh vực thiên văn học: trong Hình 2, ước tính của AstroLLaMA về các ứng viên sao tiềm năng từ dữ liệu Gaia-ESO rõ ràng không chính xác. Để giải quyết những vấn đề như vậy, chúng tôi đang trong quá trình làm phong phú tập huấn luyện của AstroLLaMA không chỉ với tóm tắt mà còn với các nguồn LaTeX đầy đủ của các bài viết thiên văn học hiện có, từ đó mở rộng số lượng token khoảng hai bậc độ lớn. Một mối quan tâm khác nằm ở xu hướng của mô hình tạo ra dữ liệu số ảo giác hoặc hư cấu, một vấn đề có thể được quy cho việc chúng tôi tập trung vào giảm độ phức tạp thay vì hướng mô hình rõ ràng về độ chính xác thực tế. Việc phát hành AstroLLaMA nhằm mục đích tạo điều kiện cho sự tham gia của cộng đồng, cả cho việc giải quyết những không chính xác này và tinh chỉnh sự cân bằng giữa "trung thực" (tôn trọng bằng chứng khoa học và độ chính xác) và "sáng tạo" (có khả năng đưa ra các giả thuyết thú vị).

AstroLLaMA đóng vai trò như một nguyên mẫu hấp dẫn cho các LLM chuyên biệt trong thiên văn học, cho thấy khả năng nhận thức ngữ cảnh vượt trội so với GPT-4 mặc dù có ít tham số hơn nhiều. Nó không chỉ mở đường cho hiệu suất cải thiện trong các nhiệm vụ như hỏi đáp, tóm tắt khoa học và tạo giả thuyết mà còn áp dụng cho các mô hình đa phương thức (Liu et al., 2023). Chúng tôi đã công khai các trọng số của AstroLLaMA và dữ liệu huấn luyện của nó cho các nhà nghiên cứu quan tâm đến việc tận dụng LLM cho các ứng dụng tập trung vào thiên văn học. Cùng với điều này, chúng tôi đang thiết lập nhiều "sân chơi" khác nhau trên Hugging Face để mời những độc giả quan tâm tiếp tục thích ứng và tinh chỉnh điểm khởi đầu mạnh mẽ này cho nhiều nhiệm vụ hạ nguồn liên quan.

## Lời cảm ơn

Chúng tôi vô cùng biết ơn Sáng kiến Nghiên cứu Mô hình Nền tảng Tăng tốc Microsoft đã cho phép chúng tôi đẩy nhanh dự án. Nhờ vào nền tảng AI tiên tiến từ Microsoft Research, chúng tôi đã có thể đẩy nhanh đáng kể những nỗ lực trong việc sử dụng các mô hình ngôn ngữ để phân tích tài liệu thiên văn học.

## Tuyên bố Đạo đức

Chúng tôi thu được các trọng số được tiền huấn luyện cho LLaMA-2 từ Meta, công ty cung cấp các mô hình này để tải xuống trên Hugging Face. Bộ dữ liệu arXiv được sử dụng trong bài báo này có sẵn công khai trên Kaggle. Mặc dù chúng tôi đã chứng minh rằng AstroLLaMA có khả năng tạo ra các tóm tắt chất lượng cao, liên quan cho các bài báo nghiên cứu thiên văn học, chúng tôi đã lưu ý rằng nó có tiềm năng tạo ra dữ liệu và đo lường không chính xác. Điều này nên được coi như một cảnh báo cho các nhà nghiên cứu nhằm sử dụng mô hình này cho các nhiệm vụ hạ nguồn, và chúng tôi mời gọi việc áp dụng các chiến lược liên kết trong công việc tương lai để cải thiện vấn đề này.
