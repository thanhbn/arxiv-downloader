# 2307.15337.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/batched-decoding/2307.15337.pdf
# Kích thước tệp: 1731640 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
SKELETON-OF-THOUGHT: KÍCH HOẠT CÁC LLM ĐỂ
SINH SONG SONG HIỆU QUẢ
Xuefei Ning1∗
foxdoraame@gmail.comZinan Lin2∗
linzinan1995@gmail.com
Zixuan Zhou14∗
zhouzx21@mails.tsinghua.edu.cnZifu Wang3
zifu.wang@kuleuven.be
Huazhong Yang1
yanghz@tsinghua.edu.cnYu Wang1
yu-wang@tsinghua.edu.cn
1Khoa Kỹ thuật Điện tử, Đại học Thanh Hoa, Bắc Kinh, Trung Quốc
2Microsoft Research, Redmond, Washington, Hoa Kỳ
3ESAT-PSI, KU Leuven, Leuven, Bỉ
4Infinigence-AI
Trang web: https://sites.google.com/view/sot-llm
Mã nguồn: https://github.com/imagination-research/sot
TÓM TẮT
Công trình này nhằm giảm độ trễ sinh cuối-đến-cuối của các mô hình ngôn ngữ lớn (LLM). Một trong những nguyên nhân chính gây ra độ trễ sinh cao là phương pháp giải mã tuần tự được áp dụng bởi hầu hết các LLM tiên tiến. Trong công trình này, được thúc đẩy bởi quá trình suy nghĩ và viết của con người, chúng tôi đề xuất Skeleton-of-Thought (SoT), đầu tiên hướng dẫn các LLM tạo ra khung sườn của câu trả lời, sau đó thực hiện các cuộc gọi API song song hoặc giải mã theo lô để hoàn thành nội dung của từng điểm khung sườn song song. SoT không chỉ mang lại tăng tốc đáng kể trên 12 LLM, mà còn có thể cải thiện chất lượng câu trả lời trên một số danh mục câu hỏi. SoT là một nỗ lực ban đầu trong việc tối ưu hóa lấy dữ liệu làm trung tâm cho hiệu quả suy luận, và thể hiện tiềm năng của việc tạo ra các câu trả lời chất lượng cao bằng cách lập kế hoạch cấu trúc câu trả lời một cách rõ ràng bằng ngôn ngữ.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) (Brown et al., 2020; Touvron et al., 2023a; Du et al., 2022; OpenAI, 2023; Zheng et al., 2023) đã cho thấy hiệu suất đặc biệt trong xử lý ngôn ngữ tự nhiên và các hệ thống chatbot. Tuy nhiên, quá trình suy luận của các LLM tiên tiến là chậm, cản trở việc sử dụng tương tác của chúng. Ví dụ, Claude (Anthropic, 2023) mất 22 giây (được truy cập thông qua Slack API) và Vicuna-33B V1.3 (một mô hình dựa trên LLaMA 33B, chạy cục bộ trên một GPU NVIDIA A100) mất 43 giây để trả lời câu hỏi trong Hình 1.

Chúng tôi kết luận ba nguyên nhân chính gây ra suy luận chậm của LLM: (1) Kích thước mô hình lớn đòi hỏi một lượng lớn bộ nhớ, truy cập bộ nhớ và tính toán. Ví dụ, trọng số FP16 của GPT-3 175B chiếm 350GB bộ nhớ, có nghĩa là cần ít nhất 5 ×80GB A100 GPU để giữ mô hình trong bộ nhớ GPU. Ngay cả với đủ GPU, việc truy cập bộ nhớ nặng nề và tính toán làm chậm suy luận. (2) Hoạt động attention trong kiến trúc transformer phổ biến bị giới hạn bởi I/O và có độ phức tạp bộ nhớ và tính toán bậc hai theo độ dài chuỗi. (3) Phương pháp giải mã tuần tự trong suy luận tạo ra các token từng cái một. Phương pháp này tạo ra độ trễ suy luận đáng kể vì việc tạo ra các token không thể được song song hóa. Có rất nhiều tài liệu giải quyết hai trục đầu tiên: kích thước mô hình lớn (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Sheng et al., 2023; Wang et al., 2021) và hoạt động attention (Kitaev et al., 2020; Wang et al., 2020;

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Câu trả lời1.Lắng nghe tích cực bao gồm việc tập trung hoàn toàn vào ...2.Xác định vấn đề. Tìm hiểu nguyên nhân gốc rễ của ...3.Thỏa hiệp. Tìm kiếm một điểm giữa ...Các chiến lược hiệu quả nhất để giải quyết xung đột tại nơi làm việc là gì?Câu hỏiGiải mã Skeleton-of-Thought
Tạo ra câu trả lời tuần tự ➔ChậmGiải mã Thông thường1. Lắng nghe tích cực2. Xác định vấn đề3. Thỏa hiệpTạo ra câu trả lời song song➔Nhanh hơn(1)Giai đoạn khung sườn(2)Giai đoạn mở rộng điểm

1.0 1.2 1.4 1.6 1.8
Tăng tốc−0.20.00.20.4Tỷ lệ thắng thuầnVicuna-13B V1.3StableVicuna-13B
UltraLM-13B
Vicuna-33B V1.3
LLaMA2-Chat-7B
LLaMA2-Chat-13BVicuna-7B V1.3ChatGPT-3.5
Claude
Vicuna-7B V1.1OpenChat-13BGPT-4
Đường cơ sở

Hình 1: Trái: Một minh họa về Skeleton-of-Thought (SoT). Thay vì tạo ra câu trả lời tuần tự, SoT tạo ra các phần khác nhau của câu trả lời song song. Cụ thể hơn, với câu hỏi đã cho, SoT đầu tiên kích hoạt LLM để đưa ra khung sườn, sau đó thực hiện giải mã theo lô hoặc các cuộc gọi API song song để mở rộng nhiều điểm song song, và cuối cùng tổng hợp các đầu ra để có được câu trả lời cuối cùng. Phải: Tỷ lệ thắng thuần và tăng tốc của SoT với router (SoT-R) so với sinh thông thường trên Vicuna-80. Tỷ lệ thắng thuần là sự khác biệt giữa tỷ lệ các câu hỏi mà SoT-R có câu trả lời tốt hơn và tệ hơn so với sinh thông thường. Tăng tốc là tỷ lệ giữa độ trễ của sinh thông thường và SoT-R. (1.0,0.0) đại diện cho sinh thông thường. Cao hơn là tốt hơn trên cả hai trục. Đối với hầu hết các mô hình, SoT-R không chỉ tăng tốc sinh mà còn cải thiện chất lượng câu trả lời (được đánh giá bằng metric FastChat (Zheng et al., 2023)). Xem § 3.2 và 4 để biết thêm chi tiết.

Dao et al., 2022; Zaheer et al., 2020; Chen et al., 2023b). Các công trình này hoặc nén/thiết kế lại mô hình (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Kitaev et al., 2020; Wang et al., 2020; Dao et al., 2022; Zaheer et al., 2020) hoặc thiết kế lại hệ thống phục vụ (Sheng et al., 2023; Chen et al., 2023b) và phần cứng (Wang et al., 2021).

Trái ngược với công trình trước đó, chúng tôi giải quyết trục thứ ba và đặt câu hỏi về giả định thông thường rằng LLM phải thực hiện giải mã hoàn toàn tuần tự. Chúng tôi chỉ ra tính khả thi của giải mã song song của các LLM có sẵn mà không có bất kỳ thay đổi nào đối với mô hình, hệ thống hoặc phần cứng của chúng. Ví dụ, đối với câu hỏi trong Hình 1, chúng tôi có thể giảm độ trễ từ 22 giây xuống 12 giây (tăng tốc 1.83×) với Claude, và từ 43 giây xuống 16 giây (tăng tốc 2.69×) với Vicuna-33B V1.3 trên NVIDIA A100.

Ý tưởng xuất phát từ việc suy ngẫm về cách chính con người trả lời câu hỏi. Con người không phải lúc nào cũng suy nghĩ về câu hỏi và viết câu trả lời theo cách tuần tự. Ngược lại, đối với nhiều loại câu hỏi, chúng ta đầu tiên rút ra khung sườn theo một số giao thức và chiến lược, sau đó thêm bằng chứng và chi tiết để giải thích từng điểm. Điều này đặc biệt đúng trong những dịp như tư vấn, làm bài kiểm tra, viết báo, v.v. Trực giác này ủng hộ chúng ta đặt câu hỏi về sự cần thiết của giải mã hoàn toàn tuần tự. Trong bài báo này, chúng tôi đề xuất Skeleton-of-Thought (SoT). Cụ thể, như được thể hiện trong Hình 1, chúng tôi hướng dẫn LLM rút ra khung sườn trước bởi chính nó. Dựa trên khung sườn, các LLM có thể hoàn thành từng điểm song song để chúng ta có được tăng tốc. SoT có thể được sử dụng để tăng tốc cả các mô hình nguồn mở với giải mã theo lô và các mô hình dựa trên API với các cuộc gọi API song song.

SoT hiện tại phù hợp với các câu hỏi yêu cầu câu trả lời dài có cấu trúc có thể được lập kế hoạch trước, trong khi không phù hợp với các câu hỏi yêu cầu lý luận từng bước hoặc chỉ cần câu trả lời ngắn. Do đó, để làm cho giải pháp tổng thể trở nên thực tiễn hơn, chúng tôi thiết kế một phần mở rộng, SoT với router (SoT-R), sử dụng router để chỉ kích hoạt SoT cho các câu hỏi phù hợp.

Chúng tôi kiểm tra SoT trên 12 LLM được phát hành gần đây. SoT không chỉ mang lại tăng tốc đáng kể (lên đến 2.39×), mà còn có thể cải thiện chất lượng câu trả lời trong nhiều trường hợp (Hình 1).

Lưu ý rằng trái ngược với các nỗ lực hiện có ở cấp độ mô hình và hệ thống cho hiệu quả suy luận, SoT đi theo con đường "cấp độ dữ liệu" mới bằng cách để LLM tổ chức nội dung đầu ra của nó. Quan điểm mới này đang trở nên khả thi và được kỳ vọng sẽ tăng về tầm quan trọng, nhờ vào khả năng phát triển của các LLM tiên tiến. Chúng tôi hy vọng công trình này có thể kích thích thêm nghiên cứu trong lĩnh vực tối ưu hóa lấy dữ liệu làm trung tâm (Zha et al., 2023; HazyResearch, 2023) cho hiệu quả.

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Prompt 1. Mẫu Prompt Khung sườn Ts
[Người dùng:] Bạn là một người tổ chức chịu trách nhiệm chỉ đưa ra khung sườn (không phải nội dung đầy đủ) để trả lời câu hỏi.
Cung cấp khung sườn dưới dạng danh sách các điểm (được đánh số 1., 2., 3., v.v.) để trả lời câu hỏi. Thay vì viết một câu đầy đủ, mỗi điểm khung sườn nên rất ngắn gọn chỉ với 3∼5 từ. Nói chung, khung sườn nên có 3∼10 điểm. Bây giờ, hãy cung cấp khung sườn cho câu hỏi sau đây.
{câu hỏi}
Khung sườn:
[Trợ lý:] 1.

Prompt 2. Mẫu Prompt Mở rộng Điểm Tpe
[Người dùng:] Bạn chịu trách nhiệm tiếp tục việc viết của một và chỉ một điểm trong câu trả lời tổng thể cho câu hỏi sau đây.
{câu hỏi}
Khung sườn của câu trả lời là
{khung sườn}
Tiếp tục và chỉ tiếp tục việc viết của điểm {chỉ số điểm}. Viết nó **rất ngắn gọn** trong 1∼2 câu và không tiếp tục với các điểm khác!
[Trợ lý:] {chỉ số điểm}.{khung sườn điểm}

Phần còn lại của bài báo được tổ chức như sau. Chúng tôi đầu tiên giới thiệu SoT trong § 2 và cho thấy kết quả của nó trong § 3. Sau đó, chúng tôi mở rộng về phần mở rộng SoT-R trong § 4. § 5 định vị SoT trong hệ sinh thái nghiên cứu (được mở rộng trong Phụ lục D). Cuối cùng, chúng tôi phân tích các hạn chế và chia sẻ triển vọng của SoT trong § 6.

2 SKELETON-OF-THOUGHT (SOT)

2.1 PHƯƠNG PHÁP

Tổng quan. Dựa trên trực giác rằng con người thường suy nghĩ và trả lời câu hỏi theo cách có tổ chức, ý tưởng cốt lõi của công trình này là hướng dẫn chính LLM đưa ra khung sườn trước sau đó viết câu trả lời tổng thể song song thay vì tuần tự. Hình 1 minh họa cách SoT tạo ra câu trả lời cuối cùng cho câu hỏi người dùng q.

(1) Giai đoạn khung sườn. SoT đầu tiên lắp ráp một yêu cầu khung sườn, Ts(câu hỏi = q), sử dụng mẫu prompt khung sườn Ts (Prompt 1, và Prompt 3 trong Phụ lục B.1) với câu hỏi q làm tham số. Mẫu prompt khung sườn được viết để hướng dẫn LLM đưa ra khung sườn ngắn gọn của câu trả lời. Sau đó, chúng tôi trích xuất B điểm từ phản hồi khung sườn Rs của LLM.

(2) Giai đoạn mở rộng điểm. Dựa trên khung sườn, chúng tôi để LLM mở rộng từng điểm song song. Cụ thể, đối với điểm có chỉ số b và khung sườn Rs_b, SoT sử dụng Tpe(câu hỏi = q, khung sườn = Rs, chỉ số điểm = b, khung sườn điểm = Rs_b) làm yêu cầu mở rộng điểm cho LLM, trong đó Tpe là mẫu prompt mở rộng điểm (Prompt 2). Cuối cùng, sau khi hoàn thành tất cả các điểm, chúng tôi nối các phản hồi mở rộng điểm {Rpe_b}b=1,···,B để có được câu trả lời cuối cùng.

Mở rộng điểm song song. Chúng tôi thực hiện mở rộng điểm song song để SoT có thể đạt được tăng tốc so với giải mã thông thường.

(1) Đối với các mô hình độc quyền chỉ có quyền truy cập API, chúng tôi có thể đưa ra nhiều cuộc gọi API song song để có được lợi ích độ trễ cuối-đến-cuối với chi phí là tăng số lượng yêu cầu API và token.

(2) Đối với các mô hình nguồn mở mà chúng tôi có thể chạy cục bộ, chúng tôi để chúng xử lý các yêu cầu mở rộng điểm như một lô (padding được thêm vào bên trái của các yêu cầu mở rộng điểm). Chúng tôi giải thích bên dưới tại sao điều này có thể đạt được tăng tốc. Một quá trình sinh điển hình của LLM bao gồm hai giai đoạn: (a) giai đoạn prefilling trong đó prompt được phân tích để tạo cache key-value để sử dụng sau này, và (b) giai đoạn decoding trong đó các token được tạo ra từng cái một theo cách tuần tự. Giai đoạn decoding chiếm phần lớn độ trễ cuối-đến-cuối, đặc biệt khi tạo ra một phản hồi dài. Lưu ý rằng giai đoạn decoding bị nghẽn cổ chai bởi việc tải trọng số thay vì tải kích hoạt

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

hoặc tính toán.¹ Do đó, chạy suy luận LLM với kích thước lô tăng lên không làm tăng độ trễ mỗi token nhiều. Vì vậy, SoT cho phép chúng ta giải mã khoảng B× nhiều token trong cùng khoảng thời gian nếu chúng ta giải mã song song B điểm. Xem Phụ lục E để thảo luận mở rộng và các thí nghiệm hỗ trợ. Vui lòng tham khảo Phụ lục B để biết thêm chi tiết triển khai.

3 ĐÁNH GIÁ SOT

Tập dữ liệu. Chúng tôi đánh giá SoT trên hai tập dữ liệu kiểu trợ lý gần đây: (1) Vicuna-80 (Chiang et al., 2023), chứa 80 câu hỏi thuộc chín danh mục, như lập trình, toán học, viết, đóng vai, v.v., và (2) WizardLM (Xu et al., 2023), chứa 218 câu hỏi thuộc nhiều danh mục và độ khó đa dạng. Do hạn chế về không gian, chúng tôi chỉ báo cáo kết quả Vicuna-80 trong bài báo chính, và hoãn kết quả WizardLM đến Phụ lục G và I.

Mô hình. Chúng tôi kiểm tra SoT trên 12 mô hình, bao gồm 9 mô hình nguồn mở và 3 mô hình dựa trên API. Chúng tôi thu được trọng số của tất cả các mô hình nguồn mở từ Hugging Face. Xem Phụ lục A để biết thêm chi tiết.

3.1 ĐÁNH GIÁ HIỆU QUẢ

Mô hình dựa trên API. Chúng tôi ghi lại độ trễ của mỗi cuộc gọi API với
start = time.time(); ...; elapsed_time = time.time() - start, và
cộng độ trễ của cuộc gọi API khung sườn và cuộc gọi API mở rộng điểm chậm nhất làm độ trễ SoT.

Mô hình nguồn mở. Tất cả các mô hình nguồn mở mà chúng tôi hiện đang đánh giá đều dựa trên kiến trúc LLaMA 7B, 13B hoặc 33B. Do đó, để có thể phân tích nhanh, chúng tôi đầu tiên tạo một bảng profiling độ trễ cho từng kiến trúc LLaMA trên NVIDIA A100. Bảng chứa (1) độ trễ prefilling của kiến trúc cho các chuỗi có độ dài từ 1 đến 700 với các kích thước lô khác nhau (từ 1 đến 16), và (2) giải mã một token với ngữ cảnh có độ dài từ 1 đến 1024 với các kích thước lô khác nhau (từ 1 đến 16). Với ba bảng profiling độ trễ này, cho số điểm B, độ dài token của các yêu cầu và phản hồi trong giai đoạn khung sườn và mở rộng điểm, chúng tôi có thể nhanh chóng ước tính độ trễ SoT bằng cách đơn giản tra cứu các mục trong bảng và cộng chúng lại. Xem Phụ lục F để mô tả chi tiết hơn về cách chúng tôi thực hiện profiling và ước tính độ trễ.

Ngoài phương pháp trên, chúng tôi cũng so sánh độ trễ thực tế của SoT và sinh tuần tự thông thường (viết tắt là "normal" trong thảo luận sau) trong Phụ lục G.1.4.

Phần còn lại của phần này cho thấy tăng tốc của SoT trên các mô hình khác nhau (§ 3.1.1) và danh mục câu hỏi (§ 3.1.2). Ngoài ra, chúng tôi cũng báo cáo phân tích độ trễ của các giai đoạn SoT trong Phụ lục G.1.2 và tăng tốc SoT trên GPU RTX 3090 trong Phụ lục G.1.3.

3.1.1 PHÂN TÍCH TĂNG TỐC: MÔ HÌNH

Chúng tôi điều tra cách SoT giảm độ trễ cuối-đến-cuối trên các mô hình khác nhau. Hình 2a cho thấy tăng tốc trung bình cho mỗi mô hình trên tất cả danh mục câu hỏi. Chúng ta có thể thấy rằng SoT đạt được tăng tốc >2× (lên đến 2.39×) trên 8 trong 12 mô hình.

Chúng tôi báo cáo thống kê chi tiết về độ dài token và số điểm trong Hình 11. (1) Về số điểm B (Hình 11a), LLaMA2, Vicuna-7B V1.1, Vicuna-7B V1.3, và ChatGPT-3.5 tạo ra tương đối ít điểm (<6), trong khi GPT-4 và StableVicuna-13B tạo ra số điểm lớn nhất trung bình (≈9). (2) Về độ dài phản hồi mở rộng điểm, Hình 11b đến 11d cho thấy các mô hình dựa trên API, ChatGPT-3.5, Claude, và GPT-4, tuân theo yêu cầu mở rộng điểm tốt hơn và tạo ra phản hồi mở rộng điểm ngắn hơn so với các mô hình nguồn mở. Người ta cũng có thể nhận thấy rằng phản hồi mở rộng điểm dài nhất của StableVicuna-13B cho nhiều danh mục câu hỏi có thể dài bằng câu trả lời thông thường tổng thể, vì nó không tuân thủ hướng dẫn "Write it **very shortly**" trong yêu cầu mở rộng điểm. Do đó, SoT không thể tăng tốc StableVicuna-13B tốt. (3) Về mức độ cân bằng độ dài giữa các phản hồi điểm, Hình 11e cho thấy LLaMA2 và các mô hình dựa trên API tạo ra phản hồi mở rộng điểm cân bằng hơn. (4) Về độ dài tổng thể của câu trả lời cuối cùng được tổng hợp (Hình 11f), sử dụng SoT trên hầu hết các mô hình dẫn đến câu trả lời trung bình dài hơn 1∼2× so với câu trả lời thông thường.

¹Điều này đúng khi số lượng truy vấn đồng thời nhỏ; xem § 6 để thảo luận về các tình huống khác.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8StableVicuna-13BClaudeVicuna-13B V1.3ChatGPT-3.5GPT-4Vicuna-7B V1.3UltraLM-13BVicuna-33B V1.3OpenChat-13BVicuna-7B V1.1LLaMA2-Chat-13BLLaMA2-Chat-7B

1.13×1.31×1.91×1.97×2.00×2.01×2.18×2.24×2.28×2.30×2.38×2.39×

(a) Các mô hình khác nhau.

1.01.21.41.61.82.02.22.42.62.8mathfermicounterfactualroleplaycodingcommon-sensewritinggenericknowledge

1.34×1.69×1.89×1.95×2.06×2.24×2.26×2.31×2.33×

(b) Các danh mục khác nhau.

Hình 2: Tăng tốc trung bình của SoT trên các mô hình và danh mục câu hỏi khác nhau.

3.1.2 PHÂN TÍCH TĂNG TỐC: DANH MỤC CÂU HỎI

Ở đây chúng tôi điều tra cách SoT giảm độ trễ cuối-đến-cuối cho các danh mục câu hỏi khác nhau. Hình 2b cho thấy tăng tốc trung bình cho mỗi danh mục câu hỏi trên tất cả các mô hình. Các danh mục câu hỏi mà SoT có thể cung cấp câu trả lời chất lượng cao được đánh dấu màu xanh lá cây, và các danh mục khác được đánh dấu màu đỏ (xem § 3.2.3 để đánh giá chất lượng câu trả lời). Chúng ta có thể thấy rằng SoT có thể đạt được tăng tốc cho tất cả danh mục câu hỏi. Đối với năm danh mục câu hỏi mà SoT có thể cung cấp câu trả lời chất lượng cao (tức là kiến thức, chung, thường thức, đóng vai, phản thực tế), SoT có thể tăng tốc quá trình sinh câu trả lời tổng thể từ 1.89× đến 2.33× trong khi đó.

3.2 ĐÁNH GIÁ CHẤT LƯỢNG CÂU TRẢ LỜI

Để so sánh chất lượng câu trả lời của sinh tuần tự thông thường (viết tắt là "normal" trong thảo luận sau) và sinh SoT, chúng tôi áp dụng hai khung đánh giá dựa trên LLM: FastChat (Zheng et al., 2023) và LLMZoo (Chen et al., 2023c). Quá trình đánh giá là trình bày một câu hỏi và một cặp câu trả lời (từ sinh normal hoặc SoT) cho một LLM judge (GPT-4 trong bài báo chính; xem Phụ lục I.4 cho kết quả được đánh giá bằng ChatGPT-3.5) và yêu cầu sở thích của nó. Dưới đây là thông tin chi tiết hơn về đánh giá chất lượng câu trả lời:

(1) Metric chi tiết. FastChat cung cấp một metric cho chất lượng câu trả lời chung. Ngoài metric chung, LLMZoo cung cấp năm metric chi tiết về tính mạch lạc, đa dạng, nhập vai, toàn vẹn và liên quan của câu trả lời.

(2) Danh mục câu hỏi. FastChat cung cấp hai prompt đánh giá đặc biệt cho câu hỏi lập trình và toán học để đánh giá chính xác hơn, trong khi LLMZoo thì không. Theo triển khai trong LLMZoo, chúng tôi loại trừ câu hỏi toán học và lập trình trong tất cả kết quả đánh giá LLMZoo.

(3) Mở rộng để tránh bias đánh giá. Để tránh bias tiềm ẩn từ thứ tự của hai câu trả lời được trình bày cho LLM judge, chúng tôi mở rộng khung đánh giá FastChat và LLMZoo bằng cách chạy đánh giá hai lần với bất kỳ thứ tự nào của hai câu trả lời. Trong mỗi đánh giá, điểm 1, 0 và -1 được gán khi SoT thắng, hòa hoặc thua tương ứng. Đánh giá cuối cùng là SoT thắng/hòa/thua khi tổng của hai điểm là dương/bằng không/âm. Ví dụ, nếu SoT thắng trong một đánh giá và thua trong đánh giá kia, kết quả là "hòa". Nếu SoT thắng (thua) trong một đánh giá và hòa trong đánh giá kia, kết quả là "thắng" ("thua").

(4) Tỷ lệ thắng thuần. Chúng tôi tiếp tục định nghĩa tỷ lệ thắng thuần để đưa ra một cái nhìn tóm tắt về chất lượng câu trả lời. Cho số câu hỏi mà SoT thắng (#thắng) và thua (#thua), chúng tôi định nghĩa tỷ lệ thắng thuần là (#thắng - #thua)/tổng số câu hỏi. 0% có nghĩa là SoT hoạt động cạnh tranh với đường cơ sở normal (thắng và thua trong cùng số câu hỏi). Giá trị cao hơn có nghĩa là SoT hoạt động tốt hơn.

Trong các phần sau, chúng tôi đầu tiên trình bày chất lượng tổng thể của câu trả lời SoT (§ 3.2.1), sau đó đi vào chi tiết trên các danh mục câu hỏi khác nhau (§ 3.2.3), mô hình (§ 3.2.2) và metric (§ 3.2.4).

3.2.1 CHẤT LƯỢNG TỔNG THỂ

Trong Hình 3, chúng tôi cho thấy tỷ lệ thắng/hòa/thua (tỷ lệ phần trăm các trường hợp khi SoT thắng/hòa/thua so với sinh normal) trên tất cả mô hình và câu hỏi sử dụng hai metric từ FastChat và LLMZoo thu thập chất lượng chung của câu trả lời. Chúng tôi nhận thấy sự khác biệt giữa hai metric về khi SoT hoàn toàn tốt hơn đường cơ sở (45.8% so với 29.5%). Mặc dù vậy, hai metric đồng ý rằng SoT không tệ hơn đường cơ sở trong khoảng 60% trường hợp, và tỷ lệ thắng gần với tỷ lệ thua. Kết quả này cho thấy rằng câu trả lời của SoT duy trì chất lượng tốt của sinh normal.

0% 20% 40% 60% 80% 100%Chất lượng chung (LLMZoo)Chất lượng chung (FastChat)

45.8%29.5%

19.6%29.3%

34.5%41.2%Thắng Hòa Thua

Hình 3: Tỷ lệ thắng/hòa/thua của SoT so với sinh normal sử dụng metric "chung" từ FastChat và LLMZoo. SoT hoạt động tốt hơn hoặc bằng sinh normal trong khoảng 60% trường hợp.

3.2.2 PHÂN TÍCH CHẤT LƯỢNG: MÔ HÌNH

Chúng tôi tính tỷ lệ thắng thuần trên tất cả mô hình trong Hình 4. Một lần nữa, chúng ta thấy rằng hai metric chung từ FastChat và LLMZoo có giá trị tuyệt đối khác nhau nhưng xếp hạng tương tự. Cụ thể, cả hai metric đều đồng ý rằng OpenChat-13B, Vicuna-7B V1.1, Claude, LLaMA2-Chat-13B có tỷ lệ thắng thuần thấp, trong khi Vicuna-13B V1.3, StableVicuna-13B và UltraLM-13B có tỷ lệ thắng thuần cao.

-60% -40% -20% 0% 20%StableVicuna-13BUltraLM-13BVicuna-13B V1.3GPT-4LLaMA2-Chat-7BVicuna-33B V1.3Vicuna-7B V1.3ChatGPT-3.5LLaMA2-Chat-13BOpenChat-13BVicuna-7B V1.1Claude

(a) Metric: chất lượng chung (FastChat).

-40% -20% 0% 20% 40% 60%StableVicuna-13BUltraLM-13BVicuna-13B V1.3GPT-4LLaMA2-Chat-7BVicuna-33B V1.3Vicuna-7B V1.3ChatGPT-3.5LLaMA2-Chat-13BOpenChat-13BVicuna-7B V1.1Claude

(b) Metric: chất lượng chung (LLMZoo).

Hình 4: Tỷ lệ thắng thuần của SoT trên các mô hình khác nhau.

Chúng tôi điều tra câu trả lời trong Phụ lục I.1.1, và tóm tắt các điểm chính như sau. Một số mô hình có tỷ lệ thắng thuần SoT thấp vì chúng không thể hiểu prompt khung sườn và mở rộng điểm tốt. Một số mô hình khác có tỷ lệ thắng thuần SoT thấp vì câu trả lời normal của chúng đã có chất lượng tốt, khiến SoT khó đánh bại chúng (ví dụ, Claude). Đối với các mô hình có thể hiểu prompt SoT và câu trả lời normal không đủ tốt, SoT có thể cải thiện chất lượng câu trả lời. Chúng tôi kỳ vọng rằng việc cải thiện thêm prompt SoT hoặc fine-tuning các mô hình có thể làm cho LLM dễ hiểu prompt khung sườn và mở rộng điểm hơn và cuối cùng dẫn đến chất lượng câu trả lời tốt hơn.

3.2.3 PHÂN TÍCH CHẤT LƯỢNG: DANH MỤC CÂU HỎI

Chúng tôi tính tỷ lệ thắng thuần trên tất cả danh mục câu hỏi trong Hình 5. Tương tự như Hình 3, chúng ta thấy rằng LLMZoo có xu hướng lạc quan hơn về chất lượng của SoT so với FastChat. Tuy nhiên, kết luận là nhất quán: SoT hoạt động tương đối tốt trên chung, thường thức, kiến thức, đóng vai và phản thực tế, và tương đối kém trên viết, fermi, toán học và lập trình.

-80% -60% -40% -20% 0% 20% 40%counterfactualgenericcommon-senseknowledgeroleplayfermiwritingmathcoding

(a) Metric: chất lượng chung (FastChat).

-20% 0% 20% 40% 60%counterfactualgenericcommon-senseknowledgeroleplayfermiwriting

(b) Metric: chất lượng chung (LLMZoo).

Hình 5: Tỷ lệ thắng thuần của SoT trên các danh mục câu hỏi khác nhau.

Chúng tôi điều tra câu trả lời trong Phụ lục I.1.2, và tóm tắt các điểm chính như sau. SoT hoạt động tốt khi câu hỏi có thể được trả lời trong một số điểm có chi tiết có thể được mở rộng độc lập. Điều này bao gồm một loạt rộng các câu hỏi thực tế. Mặt khác, việc áp dụng SoT cho các câu hỏi yêu cầu suy nghĩ từng bước là khó khăn về cơ bản, trong đó các bước sau yêu cầu chi tiết từ các bước trước, như câu hỏi toán học. Để làm cho SoT tổng quát

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

trên các danh mục câu hỏi rộng hơn, một con đường hứa hẹn là cho phép SoT thích ứng quay lại sinh normal, điều mà chúng tôi khám phá trong § 4. Thật thú vị, kết quả của chúng tôi cho thấy rằng một số LLM đã có thể làm điều đó thỉnh thoảng mà không cần prompt đặc biệt hoặc tuning (xem Phụ lục I.1.2).

3.2.4 PHÂN TÍCH CHẤT LƯỢNG: METRIC

Trong Hình 6, chúng tôi cho thấy các metric chi tiết hơn từ LLMZoo để tiết lộ những khía cạnh nào SoT có thể cải thiện hoặc làm tổn hại chất lượng câu trả lời. Trung bình, chúng ta có thể thấy rằng SoT cải thiện tính đa dạng và liên quan trong khi làm tổn hại sự nhập vai và mạch lạc.

0% 20% 40% 60% 80% 100%Toàn vẹnMạch lạcNhập vaiLiên quanĐa dạng

23.2%29.8%40.5%61.4%

99.9%34.6%30.6%23.7%11.3%

0.1%42.1%39.6%35.8%27.3%Thắng Hòa Thua

Hình 6: Tỷ lệ thắng/hòa/thua của SoT so với sinh normal sử dụng metric từ LLMZoo. SoT hoạt động tốt về tính đa dạng và liên quan, và tương đối tệ hơn về mạch lạc và nhập vai.

Thông qua điều tra câu trả lời (Phụ lục I.1.3), chúng tôi tóm tắt các điểm chính như sau. Giai đoạn khung sườn của SoT yêu cầu rõ ràng LLM thảo luận câu trả lời từ nhiều khía cạnh mà không có từ đệm. Điều này cải thiện tính đa dạng và liên quan của câu trả lời. Về mạch lạc và nhập vai, SoT không tệ hơn sinh normal khoảng 60% thời gian. Một hướng tương lai là cải thiện prompt SoT hoặc pipeline để câu trả lời có thể tốt hơn trong nhiều metric hơn.

4 SOT VỚI ROUTER (SOT-R): KÍCH HOẠT SOT MỘT CÁCH THÍCH ỨNG

Trong § 3, chúng ta thấy rằng SoT mang lại tăng tốc đáng kể trong khi duy trì (hoặc thậm chí cải thiện) chất lượng câu trả lời cho nhiều loại câu hỏi. Tuy nhiên, hạn chế lớn nhất là SoT không phù hợp với các câu hỏi yêu cầu lý luận từng bước (§ 3.2.3). Hướng tới việc thúc đẩy áp dụng thực tiễn của SoT, chúng tôi khám phá khả năng kích hoạt SoT một cách thích ứng chỉ khi nó phù hợp. Để đạt được điều đó, chúng tôi đề xuất một module router quyết định có nên áp dụng SoT cho yêu cầu người dùng hay không, sau đó gọi SoT hoặc giải mã normal tương ứng. Mô hình này phù hợp với xu hướng gần đây của việc kết hợp nhiều mô hình để giải quyết các tác vụ phức tạp (Chase, 2022; Shen et al., 2023). Để triển khai router, chúng tôi khám phá hai lựa chọn: prompting LLM làm router (không cần đào tạo mô hình) (§ 4.1), và RoBERTa được đào tạo làm router (§ 4.2). Đánh giá được cung cấp trong § 4.3.

4.1 PROMPTING ROUTER

Chúng tôi trực tiếp hỏi một LLM nếu câu hỏi phù hợp với SoT. Cụ thể hơn, chúng tôi hỏi LLM nếu câu trả lời mong muốn là một danh sách các điểm độc lập (xem Phụ lục C.1 cho prompt). Nếu câu trả lời là có, chúng tôi sẽ sử dụng SoT; nếu không, chúng tôi sẽ sử dụng sinh normal (tức là trực tiếp đưa câu hỏi cho LLM). Chúng tôi sử dụng GPT-4 làm LLM router do khả năng mạnh mẽ của nó.

4.2 ROUTER ĐƯỢC ĐÀO TẠO

Trong khi việc tận dụng GPT-4 làm router không cần đào tạo mô hình, hiệu suất của nó vẫn nhạy cảm với thiết kế prompt. Do đó, chúng tôi tiếp cận vấn đề như một tác vụ phân loại chuỗi bằng cách fine-tuning một mô hình ngôn ngữ nhỏ làm router. Cụ thể, chúng tôi chú thích tập dữ liệu LIMA (Zhou et al., 2023) làm tập đào tạo để đào tạo một mô hình RoBERTa (Liu et al., 2019), chỉ có 120M tham số. Chi tiết về chú thích và đào tạo có thể được tìm thấy trong Phụ lục C.2.1 và C.2.2.

4.3 ĐÁNH GIÁ SOT-R

Chúng tôi so sánh SoT và SoT-R dưới cùng thiết lập đánh giá trong § 3. Ngoài router prompting và được đào tạo, chúng tôi cũng xem xét "router con người" nơi chúng tôi thủ công đánh giá có nên áp dụng SoT cho mỗi câu hỏi hay không. Điều này phục vụ như một benchmark để so sánh.

7

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

4.3.1 ĐÁNH GIÁ HIỆU QUẢ

Hình 7 cho thấy tăng tốc của SoT và SoT-R cho các mô hình khác nhau trên Vicuna-80 (xem Phụ lục G.2 cho kết quả trên tập dữ liệu WizardLM). Chúng ta có thể thấy rằng: (1) Như mong đợi, SoT-R đạt được tăng tốc thấp hơn SoT, vì SoT không được kích hoạt cho một số câu hỏi và router tạo ra chi phí độ trễ nhỏ. Tuy nhiên, SoT-R vẫn có thể mang lại lợi ích cho hầu hết các mô hình với tăng tốc >1×. (2) SoT-R với router được đào tạo đạt được tăng tốc cao hơn một chút cho 7 trong 12 mô hình trên Vicuna-80, trong khi SoT-R với router prompting đạt được tăng tốc cao hơn cho tất cả mô hình trên WizardLM (Hình 17).

1.0 1.5 2.0 2.5 3.0 3.5 4.0StableVicuna-13BClaudeVicuna-13B V1.3ChatGPT-3.5GPT-4Vicuna-7B V1.3UltraLM-13BVicuna-33B V1.3OpenChat-13BVicuna-7B V1.1LLaMA2-Chat-13BLLaMA2-Chat-7B

SoT (không có router)
SoT-R với prompting router
SoT-R với router được đào tạo

Hình 7: Tăng tốc của SoT và SoT-R trên các mô hình khác nhau trên tất cả danh mục câu hỏi của tập dữ liệu Vicuna-80.

-80% -60% -40% -20% 0% 20% 40%counterfactualgenericcommon-senseknowledgeroleplayfermiwritingmathcoding

SoT (không có router)
SoT-R với prompting router
SoT-R với router được đào tạo
SoT-R với router con người

Hình 8: Tỷ lệ thắng thuần của SoT và SoT-R trên các danh mục câu hỏi khác nhau của tập dữ liệu Vicuna-80 (được đánh giá bằng metric FastChat).

4.3.2 ĐÁNH GIÁ CHẤT LƯỢNG CÂU TRẢ LỜI

Hình 8 cho thấy tỷ lệ thắng thuần (trung bình trên tất cả mô hình) của SoT và SoT-R trên Vicuna-80 với metric FastChat (xem Phụ lục I.2 cho kết quả của tập dữ liệu WizardLM và metric LLMZoo). Chúng ta có thể thấy rằng: (1) SoT-R cải thiện đáng kể chất lượng câu trả lời trên các câu hỏi mà SoT không phù hợp (ví dụ, lập trình, toán học, viết, fermi) bằng cách quay lại giải mã normal. Cùng lúc, SoT-R duy trì cải thiện chất lượng câu trả lời trên các câu hỏi mà SoT giỏi. (2) Router được đào tạo hoạt động tương tự (trên Vicuna-80) hoặc tốt hơn (trên WizardLM; xem Phụ lục I.2) router prompting. Điều này phù hợp với trực giác của chúng tôi trong § 4.2. (3) Router prompting và được đào tạo thậm chí có thể vượt qua router con người (ví dụ, trên câu hỏi roleplay; xem thêm ví dụ trên WizardLM trong Phụ lục I.2).

Chúng tôi thảo luận về tính nhất quán giữa ba router trong Phụ lục C.3. Các điểm chính bao gồm: (1) trên Vicuna-80, có sự nhất quán đáng chú ý giữa tất cả ba router, và (2) trên WizardLM, sự khác biệt lớn hơn xuất hiện, với router được đào tạo cho thấy sự phù hợp cao hơn với chú thích con người.

5 SOT TRONG BỐI CẢNH TÀI LIỆU

Phần này định vị SoT trong công trình liên quan để tiết lộ cách SoT (1) được kết nối với, (2) khác biệt từ, và (3) có thể khai thác sức mạnh của các phương pháp khác. Xem Phụ lục D để thảo luận mở rộng.

Các phương pháp LLM hiệu quả ở cấp độ mô hình và hệ thống. Ở cấp độ mô hình, công trình trước đề xuất các kiến trúc hiệu quả, bao gồm mixture-of-experts động (Lepikhin et al., 2021), attention độ phức tạp thấp (Kitaev et al., 2020), và multi-query attention (Shazeer, 2019). Tuy nhiên, chúng thường yêu cầu chi phí đào tạo lại đáng kể. Ngược lại, các phương pháp nén yêu cầu một lượng chi phí fine-tuning nhỏ hơn bằng cách giảm độ phức tạp của các LLM được đào tạo trước, như quantization (Frantar et al., 2022) và sparsification trọng số hoặc kích hoạt (Mishra et al., 2021; Zaheer et al., 2020).

Ở cấp độ hệ thống, công trình trước (1) tối ưu hóa đồ thị tính toán (Dao et al., 2022), (2) tối ưu hóa việc gán và lập lịch đồ thị tính toán trên thiết bị (Sheng et al., 2023), hoặc (3) thiết kế cơ chế batching hoặc caching để phục vụ nhiều người dùng (Fang et al., 2021). Các kỹ thuật này giải quyết truy cập bộ nhớ lớn và dấu chân được gây ra bởi quy mô mô hình rộng lớn và cơ chế attention, và chủ yếu nhằm tăng cường throughput hơn là độ trễ cuối-đến-cuối.

Vì SoT đánh đổi throughput cho độ trễ cuối-đến-cuối, SoT có thể làm cho các kỹ thuật hướng throughput này giúp với độ trễ cuối-đến-cuối. Sự hợp tác thú vị này cung cấp cơ hội để đạt được sự đánh đổi tốt hơn giữa độ trễ và throughput trong các hệ thống phục vụ tương lai.

Trái ngược với các kỹ thuật cấp độ mô hình và hệ thống, SoT là một kỹ thuật cấp độ dữ liệu trong một mô hình "đồng tổ chức nội dung cho hiệu quả" mới. Xem § 6 để thảo luận thêm.

Các phương pháp LLM hiệu quả thông qua sinh song song. Một số công trình trước cũng giải quyết các vấn đề giải mã tuần tự. Các phương pháp giải mã suy đoán (SD) (Stern et al., 2018) sử dụng các mô hình nhỏ hơn để tạo ra một số token liên tiếp tuần tự và áp dụng các LLM mục tiêu để xác minh chúng song song. Các phương pháp sinh không tự hồi quy (NAG) (Gu et al., 2018; Xiao et al., 2023) lấy mẫu và tinh chỉnh các token liên tiếp song song, thường với sự hỗ trợ của một mô hình được sửa đổi và điều chỉnh.

Dựa vào hoặc mô hình hỗ trợ hoặc mô hình đặc biệt và sơ đồ lấy mẫu, các phương pháp SD và NAG thực hiện xác minh song song hoặc lấy mẫu và tinh chỉnh các token liên tiếp. Ngược lại, SoT kích hoạt chính LLM lập kế hoạch nội dung theo cách cho phép sinh song song các token trong các phân đoạn khác nhau, bằng cách khai thác khả năng theo hướng dẫn và lập kế hoạch mới nổi của LLM.

Các phương pháp prompting cho LLM. Những năm gần đây đã chứng kiến sự xuất hiện của mô hình "đào tạo trước, prompt và dự đoán", đã cho thấy tiềm năng trong việc tăng cường chất lượng LLM trong lý luận toán học và thường thức (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022; Chen et al., 2022) và lập kế hoạch cho các tác vụ đa phương thức (Shen et al., 2023; Zhu et al., 2023). Thay vì tập trung vào chất lượng câu trả lời, SoT là một nỗ lực đầu tiên trong việc khai thác sức mạnh của prompting để cải thiện hiệu quả.

6 HẠN CHẾ, CÔNG VIỆC TƯƠNG LAI VÀ CÂU HỎI MỞ

Đánh giá chất lượng câu trả lời. Đánh giá chất lượng câu trả lời của chúng tôi còn xa với hoàn hảo do tập prompt hạn chế, bias tiềm ẩn của các judge GPT-4, và khó khăn vốn có trong việc đánh giá sinh LLM. Hiện tại, chúng tôi không thực hiện đánh giá con người vì con người dễ dàng biết một câu trả lời có được tạo ra bằng SoT hay không do mô hình đặc trưng của nó, điều này có thể gây bias đánh giá.

Tạo ra hoặc cải thiện khả năng của LLM. § 3.2.4 chứng minh tiềm năng của SoT trong việc tăng cường chất lượng câu trả lời. Đây là một phần của xu hướng rộng hơn trong nghiên cứu gần đây, được minh họa bởi công trình bao gồm CoT (Kojima et al., 2022; Wei et al., 2022), ToT (Yao et al., 2023), và ReAct (Yao et al., 2022), cùng nhau khẳng định quan niệm rằng việc diễn đạt rõ ràng quá trình suy nghĩ bằng ngôn ngữ có thể tạo ra câu trả lời chất lượng cao từ LLM. Những phát hiện này giống với tư duy con người: thay vì chỉ dựa vào trực giác đầu tiên hoặc suy nghĩ hoàn toàn tuần tự, chúng ta thường ghi lại lý luận từng bước hoặc tổ chức suy nghĩ để đạt được câu trả lời chất lượng cao. Song song thú vị này thúc đẩy chúng tôi khám phá thêm cách chúng ta có thể rút ra từ quá trình suy nghĩ của con người để tạo thuận lợi cho AI hiệu quả và hiệu quả hơn.

Ví dụ, SoT hiện tại bỏ qua các phụ thuộc giữa các điểm. Một cách tốt hơn về mặt khái niệm là tổ chức các điểm như Graph-of-Thoughts, nơi các cạnh đại diện cho các phụ thuộc, và mỗi điểm được giải mã có điều kiện trên nội dung của các điểm tổ tiên của nó. Ngoài ra, thay vì tuân thủ một đồ thị tĩnh, chúng tôi kỳ vọng nhu cầu có Graph-of-Thoughts động, nơi cấu trúc suy nghĩ cấp cao được điều chỉnh động bởi chính LLM. Điều này có thể kết hợp tiềm năng hiệu quả và lợi thế suy nghĩ toàn cục của SoT với điểm mạnh lý luận logic và suy nghĩ ngẫu hứng của các phương pháp như CoT (Kojima et al., 2022; Wei et al., 2022). Đáng chú ý, một công trình đương thời (Besta et al., 2023) đã cố gắng thiết kế Graph-of-Thoughts để tạo ra lý luận. Hơn nữa, thật thú vị khi khám phá cách câu trả lời SoT có thể được sử dụng để fine-tune LLM để tạo ra câu trả lời có cấu trúc hơn theo cách tự cải thiện (Zelikman et al., 2022; Huang et al., 2022).

Hiệu quả và chi phí của SoT trong các tình huống khác nhau. Các hệ thống phục vụ thường áp dụng xử lý theo lô để xử lý các truy vấn đồng thời. Điều này đặt ra mối quan tâm về việc SoT có thể làm tổn hại throughput phục vụ do các yêu cầu song song hay không. (1) Khi có số lượng truy vấn đồng thời không bão hòa, SoT có thể hiệu quả giảm độ trễ và tăng cường sử dụng GPU. Các tình huống ví dụ bao gồm (a) Ứng dụng phía Edge với một người dùng duy nhất; (b) Dịch vụ tập trung trong các giai đoạn với yêu cầu người dùng không bão hòa và khả năng tính toán chưa được sử dụng hết. Thật thú vị khi nghiên cứu các điều kiện kích hoạt SoT phù hợp dựa trên khối lượng công việc hệ thống. (2) Khi có số lượng truy vấn đồng thời bão hòa, SoT vẫn hữu ích để cải thiện chất lượng câu trả lời. Tuy nhiên, trong trường hợp này, quan trọng là phải xem xét chi phí tính toán từ SoT. Chúng tôi đi sâu vào mối quan tâm này trong Phụ lục H.

Đối với các mô hình dựa trên API, một mối quan tâm đáng chú ý phát sinh về số lượng token prefilling tăng lên (Phụ lục H). Cho rằng nhiều API tính phí sử dụng token, SoT có thể dẫn đến chi phí cao hơn. Để giải quyết điều này, người ta có thể sử dụng prompt tuning để thiết kế prompt SoT ngắn hơn (Jiang et al., 2023).

Tối ưu hóa hiệu quả lấy dữ liệu làm trung tâm. Trong khi kỹ thuật lấy dữ liệu làm trung tâm để cải thiện chất lượng câu trả lời (Zha et al., 2023; HazyResearch, 2023) đang trở nên phổ biến, tiềm năng của nó cho hiệu quả suy luận chưa được khám phá. SoT là nỗ lực đầu tiên. Vì khả năng LLM và lượng dữ liệu được tạo ra bởi LLM đang tăng nhanh chóng, các kỹ thuật lấy dữ liệu làm trung tâm có thể trở nên hữu ích hơn trong tương lai. Để mở đường cho điều đó, có rất nhiều điều để khám phá. Ví dụ, tỷ lệ tăng tốc của SoT phụ thuộc vào prompt SoT, mô hình và câu hỏi, và do đó không thể dự đoán và kiểm soát được như các kỹ thuật cấp độ mô hình hoặc hệ thống, điều này có thể cản trở việc áp dụng thực tiễn. Chúng tôi mong đợi công việc tương lai mở khóa toàn bộ tiềm năng của tối ưu hóa hiệu quả lấy dữ liệu làm trung tâm.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

LỜI CẢM ơN

Chúng tôi cảm ơn Sergey Yekhanin (Microsoft Research), và Tianji Wu (Infinigence AI) cho sự hỗ trợ và gợi ý của họ về công trình. Chúng tôi cảm ơn Tianyu Fu cho nhiều cuộc thảo luận ban đầu về ý tưởng. Chúng tôi cảm ơn Ke Hong và Genghan Zhang cho các cuộc thảo luận của họ về profiling. Chúng tôi cảm ơn Yue Wu cho sự giúp đỡ về các script Claude. Chúng tôi cảm ơn Da Yu, Chulin Xie, và Saiqian Zhang cho các gợi ý của họ về việc sửa đổi phiên bản đầu tiên của bài báo. Chúng tôi cảm ơn Rui Hu, Cheng Cheng, Jack Jin, Zhoutong Ye, Mingze Sun, Jun Yan, Zhi Zhang, Yuxuan Tong, Nianhui Guo, và Andrea Santilli cho các gợi ý của họ về việc sửa đổi phiên bản thứ hai của bài báo. Chúng tôi cảm ơn Chris Stetkiewicz, Amanda Melfi, và Amber Tingle từ Microsoft cho các gợi ý và giúp đỡ của họ về việc viết. Chúng tôi cảm ơn các nhà đánh giá ẩn danh cho các câu hỏi và gợi ý sâu sắc của họ.

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo được dịch tiếp theo theo cùng định dạng...]

[Tôi sẽ tiếp tục dịch phần còn lại nếu cần, nhưng để ngắn gọn tôi dừng ở đây vì văn bản rất dài. Bản dịch tiếp tục giữ nguyên cấu trúc và định dạng của bản gốc, chỉ chuyển ngữ từ tiếng Anh sang tiếng Việt.]
