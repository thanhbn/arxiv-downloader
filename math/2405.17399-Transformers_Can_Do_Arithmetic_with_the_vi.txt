I'll help you translate this research paper from English to Vietnamese while maintaining the exact structure. Let me start the translation:

# Transformers Can Do Arithmetic with the Right Embeddings
# Các Transformer Có Thể Thực Hiện Phép Tính Với Embeddings Phù Hợp

**Sean McLeish1*, Arpit Bansal1∗, Alex Stein1, Neel Jain1, John Kirchenbauer1,
Brian R. Bartoldson2, Bhavya Kailkhura2, Abhinav Bhatele1, Jonas Geiping3,
Avi Schwarzschild4, Tom Goldstein1**

1University of Maryland, 2Lawrence Livermore National Laboratory, 3ELLIS Institute Tübingen,
Max Planck Institute for Intelligent Systems, Tübingen AI Center, 4Carnegie Mellon University

## Abstract
## Tóm tắt

Hiệu suất kém của transformers trong các tác vụ số học dường như xuất phát chủ yếu từ việc chúng không thể theo dõi vị trí chính xác của từng chữ số trong một chuỗi dài các chữ số. Chúng tôi khắc phục vấn đề này bằng cách thêm một embedding vào mỗi chữ số để mã hóa vị trí của nó so với điểm bắt đầu của số. Ngoài việc cải thiện mà các embeddings này mang lại, chúng tôi chỉ ra rằng việc sửa lỗi này cho phép các sửa đổi kiến trúc như input injection và các lớp hồi quy cải thiện hiệu suất hơn nữa.

Với vị trí được giải quyết, chúng tôi có thể nghiên cứu khả năng ngoại suy logic của transformers. Liệu chúng có thể giải quyết các bài toán số học lớn hơn và phức tạp hơn so với những gì có trong dữ liệu huấn luyện? Chúng tôi phát hiện rằng chỉ bằng việc huấn luyện trên các số 20 chữ số với một GPU duy nhất trong một ngày, chúng tôi có thể đạt được hiệu suất tiên tiến, đạt tới 99% độ chính xác trên các bài toán cộng 100 chữ số. Cuối cùng, chúng tôi cho thấy rằng những cải thiện về khả năng tính toán này cũng mở ra những cải tiến trong các tác vụ lý luận nhiều bước khác bao gồm sắp xếp và nhân.

[Figure 1 caption in Vietnamese]
Hình 1: Độ chính xác khớp chính xác zero shot trên phép cộng sử dụng các mô hình transformer độ sâu mười sáu (chỉ decoder) được huấn luyện trên các toán hạng lên đến 20 chữ số. So với embeddings tiên tiến (trái), Abacus Embeddings mới của chúng tôi (phải) cải thiện đáng kể việc tổng quát hóa đến độ dài chữ số chưa thấy. Phần bên trong của hình vuông đỏ biểu thị phân phối huấn luyện. Độ chính xác được tính trung bình qua ba thử nghiệm.

∗Đóng góp bằng nhau, liên hệ: smcleish@umd.edu , bansal01@umd.edu .
²Mã nguồn có sẵn trên GitHub: github.com/mcleish7/arithmetic.

38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2405.17399v2 [cs.LG] 23 Dec 2024

## 1 Introduction
## 1 Giới thiệu

Phần lớn các nghiên cứu gần đây về Mô hình Ngôn ngữ Lớn (LLMs) tập trung vào khả năng giải quyết vấn đề bằng ngôn ngữ tự nhiên và tạo mã. Mặc dù có tiến bộ trong các lĩnh vực này, transformers vẫn gặp khó khăn trong việc thực hiện các tác vụ lý luận thuật toán và nhiều bước phức tạp trong setting zero shot mà không cần sử dụng công cụ. Để nghiên cứu lý luận thuật toán trong môi trường phòng thí nghiệm thuần túy, cộng đồng học thuật tập trung vào các bài toán kiểm tra số học đơn giản như phép cộng. Phép cộng đủ đơn giản để các LLMs có kích thước vừa phải có thể (về nguyên tắc) được huấn luyện từ đầu để thực hiện nó mà không gặp phải các hạn chế về khả năng và ngân sách huấn luyện, nhưng đủ phức tạp để ngay cả các mô hình công nghiệp lớn cũng thất bại trên các số lớn mà không có trình thông dịch mã [Loeber, 2024].

Việc huấn luyện transformers cho số học cho phép chúng tôi nghiên cứu một số câu hỏi quan trọng. Đầu tiên, chúng tôi hỏi những lựa chọn thiết kế kiến trúc, đặc điểm tập dữ liệu, và các biến thể pipeline huấn luyện nào được yêu cầu để học một quy trình lý luận nhiều bước như phép cộng nhiều chữ số? Đi sâu hơn, sau đó chúng tôi điều tra liệu các mô hình này có khả năng ngoại suy logic —liệu chúng có thể giải quyết các vấn đề có kích thước và độ khó lớn hơn so với những gì xuất hiện trong tập huấn luyện của chúng?

Các nghiên cứu trước đó chỉ ra rằng phép cộng là khó đối với transformers [Lee et al., 2023, Shen et al., 2023, Zhou et al., 2023, 2024]. Các thí nghiệm của chúng tôi cho thấy rằng khó khăn này xuất phát từ việc chúng không thể biểu diễn rõ ràng vị trí chính xác của một chữ số trong một chuỗi dài các chữ số. Để giải quyết vấn đề này, chúng tôi đề xuất một sửa đổi đơn giản cho biểu diễn dữ liệu mà trực tiếp giải quyết khuyết điểm này.

Abacus Embeddings của chúng tôi là các embeddings vị trí học được đơn giản được sử dụng để mã hóa vị trí trong mỗi span của các token số. Kết hợp Abacus Embeddings và embeddings vị trí tiêu chuẩn, chúng tôi quan sát những cải thiện đáng kể về độ chính xác sao cho các mô hình được huấn luyện với nhiều nhất 20 chữ số toán hạng có thể tổng quát hóa đến các vấn đề với 120 chữ số toán hạng. Điều này đại diện cho yếu tố tổng quát hóa tiên tiến 6×, với tiên tiến trước đó chỉ là 2.5×. Theo hiểu biết tốt nhất của chúng tôi, đây là những chuỗi dài nhất mà việc học phép cộng đã từng được chứng minh.

Chúng tôi cũng nghiên cứu một số phương pháp khác để cải thiện số học và tổng quát hóa trong transformers. Chúng tôi phát hiện rằng việc kết hợp input injection —các skip connections được chèn giữa lớp đầu vào và mỗi lớp decoder—có thể giảm lỗi tổng quát hóa 50% so với baseline Abacus Embedding. Chúng tôi cũng phát hiện rằng cùng với embeddings của chúng tôi, các kiến trúc transformer vòng lặp, chứa các lớp hồi quy trong đó các tham số giống nhau được sử dụng lại nhiều lần, có thể đạt được tổng quát hóa gần như hoàn hảo trên các bài toán cộng mà chúng tôi xem xét.

Vì các phương pháp đề xuất của chúng tôi giải quyết thành công các bài toán cộng lớn, chúng tôi đánh giá liệu các cách tiếp cận tương tự có thể được sử dụng để cải thiện các loại học thuật toán khác. Chúng tôi khám phá các bài toán nhân lên đến 15 chữ số và sắp xếp trên các mảng lên đến 10 số, làm cho đây là nghiên cứu đầu tiên về các kỹ thuật tổng quát hóa độ dài cực đoan cho phép cộng mà chuyển giao đến các tác vụ thuật toán khác.

Các đóng góp của chúng tôi có thể được tóm tắt như sau.

• Chúng tôi đề xuất một embedding vị trí mới gọi là Abacus Embeddings để nắm bắt tốt hơn tính quan trọng của mỗi chữ số, dẫn đến tổng quát hóa trong phân phối gần như hoàn hảo.

• Chúng tôi cho thấy rằng khi kết hợp Abacus Embeddings với input injection và looped transformers, hiệu suất cải thiện hơn nữa, tăng từ 92.9% đến 99.1% trong độ chính xác ngoài phân phối, giảm 87% lỗi so với việc chỉ sử dụng embeddings với kiến trúc tiêu chuẩn.

• Chúng tôi đẩy tổng quát hóa độ dài vượt ra ngoài công trình hiện có và cho thấy rằng các mô hình của chúng tôi có thể giải quyết các vấn đề với sáu lần nhiều chữ số hơn so với các mẫu lớn nhất trong tập huấn luyện, trong khi tiên tiến trước đó chỉ là hai lần rưỡi.

• Chúng tôi mở rộng các phát hiện của mình đến các vấn đề phức tạp hơn bao gồm nhân và sắp xếp nơi chúng tôi cho thấy tổng quát hóa độ dài trong các lĩnh vực này.

## 2 Related Work
## 2 Công trình liên quan

**Arithmetic and Algorithmic Reasoning.** Giải quyết số học với dự đoán token tiếp theo là một vấn đề khó thu hút rất nhiều sự chú ý [e.g. Saxton et al., 2019]. Tuy nhiên, trong các setting zero-shot, ngay cả các mô hình API thương mại cực kỳ mạnh cũng gặp khó khăn với các bài toán cộng rất lớn (e.g. lên đến 100 chữ số) mà không có quyền truy cập vào công cụ. Trong số các nỗ lực cải thiện hiệu suất số học của các mô hình dựa trên transformer, việc đảo ngược các chữ số để các đối số được viết với chữ số ít quan trọng nhất trước tiên là phổ biến [Lee et al., 2023, Shen et al., 2023, Zhou et al., 2023, 2024]. Hơn nữa, việc thay đổi định dạng dữ liệu bằng cách thêm các ký tự chỉ mục rõ ràng cải thiện khả năng mô hình cho phép cộng [Zhou et al., 2023, 2024, Olsson et al., 2022]. Công trình khác tiếp cận số học bằng cách embedding các số thực bằng cách chia tỷ lệ một token-embedding cố định duy nhất cho các số [Golkar et al., 2023]. Hơn nữa, Dziri et al. [2023] cho thấy phép nhân là một vấn đề khó đối với GPT-3 [Brown et al., 2020] ngay cả khi được tinh chỉnh trên tác vụ này. Dziri et al. [2023] tiếp tục cho thấy rằng GPT-4 [OpenAI, 2023] gặp khó khăn để đạt được độ chính xác trong phân phối cao trên phép nhân, ngay cả với scratchpad. Tuy nhiên, Lee et al. [2023] phát hiện rằng với scratchpad chi tiết, các transformers nhỏ có thể thực hiện phép nhân trong phân phối.

Số học là một tập con của lớp lớn hơn các vấn đề lý luận thuật toán tập trung vào khả năng học và thực thi các thuật toán và tổng quát hóa đến các vấn đề dài hơn [Anil et al., 2022b, Jelassi et al., 2023, Yang et al., 2023b, Veličković et al., 2022, Rodionov and Prokhorenkova, 2024, Testolin, 2024]. Lĩnh vực lý luận thuật toán tổng quát hơn bao gồm công trình về các kiến trúc và phương thức dữ liệu khác nhau nhằm học các thuật toán từ dữ liệu. Veličković et al. [2022] và Rodionov and Prokhorenkova [2024], ví dụ, huấn luyện mạng nơ-ron để thực thi các tác vụ thuật toán cụ thể bằng cách huấn luyện trên các cặp đầu vào-đầu ra cũng như các bước trung gian và gợi ý. Trong một hướng tương tự và mặc dù ban đầu được đánh giá cao về hiệu quả, chia sẻ trọng số và tính hồi quy có thể được sử dụng để làm cho các mô hình thích ứng và giúp tổng quát hóa đến các vấn đề khó hơn [Dehghani et al., 2018, Sukhbaatar et al., 2019, Lan et al., 2020, Ibarz et al., 2022]. Schwarzschild et al. [2021] và Bansal et al. [2022] khám phá một cách tiếp cận học end-to-end sử dụng mạng nơ-ron tích chập hồi quy để học các thuật toán từ các cặp đầu vào-đầu ra, giải quyết các tác vụ thuật toán như tổng tiền tố, mê cung, và cờ vua. Chia sẻ trọng số cho lý luận thuật toán cũng hữu ích với transformers và chúng tôi sử dụng looped transformer trong một số thí nghiệm dưới đây. Một looped transformer có một khối transformer được gọi một cách hồi quy trên đầu ra của chính nó, phù hợp với việc thực thi các thuật toán lặp [Giannou et al., 2023, Yang et al., 2023a, de Luca and Fountoulakis, 2024]. Ngoài ra, công trình gần đây nhằm cải thiện lý luận trong LLMs [Zhou et al., 2023], nhưng McLeish et al. [2024] chứng minh rằng LLMs, ngay cả với trình thông dịch mã, cũng không hoàn hảo trong các tác vụ lý luận thuật toán, cho thấy nhu cầu quan trọng về những tiến bộ trong phương pháp học của chúng ta. Bài báo này thực hiện một bước hướng tới cải thiện khả năng số học và thuật toán của LLM mà không sử dụng công cụ.

**Positional Embeddings.** Việc chỉ ra vị trí của các token trong một chuỗi cho các mô hình transformer là quan trọng đối với mô hình ngôn ngữ [Vaswani et al., 2017]. Các embeddings vị trí tuyệt đối (APE) là các embeddings học được được thêm vào embeddings token trước lớp đầu tiên của transformer [Vaswani et al., 2017]. Tuy nhiên, các embeddings tuyệt đối này ngăn cản tổng quát hóa độ dài [Press et al., 2022]. Để giải quyết vấn đề này, Shaw et al. [2018] đề xuất embeddings tương đối (RPE) được embedded trong quá trình tính toán attention, một cơ chế được đơn giản hóa hơn nữa bởi Raffel et al. [2020]. Những người khác xây dựng dựa trên các công trình này để cải thiện tổng quát hóa độ dài bao gồm Sandwich [Chi et al., 2023], Kerple [Chi et al., 2022], và Alibi [Press et al., 2022] positional embeddings. Ngoài ra, Kazemnejad et al. [2023] cho thấy rằng các lớp decoder vẫn có thể học thông tin vị trí mà không có embeddings vị trí rõ ràng. Không có embeddings vị trí (NoPE) có thể đạt được hiệu suất tổng quát hóa độ dài tốt cho các tác vụ thuật toán nhỏ và thậm chí vượt trội hơn một số embeddings chuyên biệt. Rotary Positional Embeddings (RoPE) [Su et al., 2024] được sử dụng phổ biến trong các transformers nguồn mở tiên tiến [e.g. Touvron et al., 2023]. Tuy nhiên, RoPE thực sự hạn chế tổng quát hóa độ dài vì các mô hình chỉ được huấn luyện sử dụng các phép quay dựa trên độ dài dữ liệu huấn luyện [Kazemnejad et al., 2023, Press et al., 2022]. Để cải thiện tổng quát hóa độ dài, người ta có thể thêm các mở rộng sau huấn luyện [Peng et al., 2024]. Mới nhất và hữu ích nhất cho số học là Functional Interpolation for Relative Position Embeddings (FIRE) [Li et al., 2023]. FIRE cho thấy tổng quát hóa độ dài mạnh nhất cho đến nay, dẫn đến tổng quát hóa độ dài 2.5× trên phép cộng [Zhou et al., 2024] khi kết hợp với embeddings ngẫu nhiên [Ruoss et al., 2023]. Chúng tôi đi sâu hơn về một số embeddings vị trí này trong Phụ lục A.1.1. Trong công trình này, chúng tôi tập trung vào embeddings NoPE và FIRE vì đây là những người thực hiện tốt nhất cho phép cộng ở định dạng đảo ngược trong số các embeddings hiện có [Zhou et al., 2024].

## 3 Achieving Length Generalization for Addition
## 3 Đạt được Tổng quát hóa Độ dài cho Phép cộng

Chúng tôi nghiên cứu một loạt các phương pháp để cải thiện khả năng số học của các mô hình ngôn ngữ được huấn luyện từ đầu, tập trung vào hai giả thuyết chính: (1) thông tin vị trí cho các chữ số riêng lẻ trong các số đang bị mất và (2) tính hồi quy có thể cải thiện khả năng lý luận của kiến trúc transformer trên các vấn đề lý luận số học nhiều bước. Chúng tôi thảo luận ngắn gọn về thiết lập huấn luyện và đánh giá trước khi mô tả chi tiết từng cải tiến của chúng tôi.

**Experimental Setup.** Chúng tôi huấn luyện các mô hình ngôn ngữ nhân quả chỉ decoder để giải quyết các bài toán cộng. Theo công trình trước đó [Zhou et al., 2023, 2024, Shen et al., 2023, Kazemnejad et al., 2023, Lee et al., 2023], đầu vào được định dạng chữ số ít quan trọng nhất trước tiên, e.g. 98282 + 3859172 = 2787472. Không giống như công trình trước đó, chúng tôi không thêm bất kỳ padding nào giữa các chữ số [Shen et al., 2023] và không pad bất kỳ số nào với các số không, không trong trường hợp các chữ số carry [Zhou et al., 2024], cũng không để làm cho tất cả các toán hạng có cùng độ dài [Shen et al., 2023]. Chúng tôi huấn luyện trên tất cả các kết hợp độ dài toán hạng nhỏ hơn hoặc bằng i và j nơi i và j là độ dài tối đa của toán hạng thứ nhất và thứ hai, tương ứng. Cho nghiên cứu này, tất cả các tập huấn luyện có 20 triệu mẫu và i=j, do đó chúng tôi có thể sử dụng một số để định nghĩa tập dữ liệu i, nơi i là độ dài tối đa của một trong hai toán hạng. Chúng tôi lấy mẫu dữ liệu có thay thế và chúng tôi phân tầng dữ liệu, để tất cả các cặp độ dài (i, j) được lấy mẫu đều trong quá trình huấn luyện. Để tạo điều kiện thuận lợi cho việc huấn luyện nhiều mô hình từ đầu, chúng tôi sử dụng thiết lập cramming mô hình ngôn ngữ [Geiping and Goldstein, 2023] và giới hạn mỗi lần chạy huấn luyện ở 8 exaFLOP tính toán (một GPU Nvidia RTX A4000 duy nhất trong 24 giờ); cho kết quả nhân, chúng tôi cho phép 64 exaFLOP (tám GPU Nvidia RTX A4000 trong 24 giờ). Trong quá trình huấn luyện, chúng tôi che đi câu hỏi đầu vào và chỉ tính toán loss trên các chữ số câu trả lời. Để biết thêm chi tiết về xây dựng dữ liệu và huấn luyện, chúng tôi tham khảo Phụ lục A.2.

Chúng tôi báo cáo độ chính xác mô hình cho mỗi cặp độ dài (i, j) và không giống như hầu hết công trình hiện có, chúng tôi cũng bao gồm độ chính xác cho các cặp nơi i≠j để làm nổi bật tất cả các trường hợp ngoại suy. Việc lập bảng mở rộng này tốn kém và làm cho suy luận trở thành gánh nặng tính toán chính của nghiên cứu này. Vì pipeline huấn luyện của chúng tôi tạo ra kết quả khá nhất quán, chúng tôi báo cáo trung bình qua ba lần chạy (thay vì sử dụng lược đồ báo cáo best-of-ten [Zhou et al., 2024]). Chúng tôi đo độ chính xác theo nghĩa nghiêm ngặt nơi chỉ có khớp chính xác của tất cả các chữ số đầu ra được tính là đúng, i.e. nếu một chữ số duy nhất không chính xác thì ví dụ được đánh dấu là sai và chúng tôi gọi đây là độ chính xác khớp chính xác. Chúng tôi có ba loại đánh giá sau: (i) trong phân phối (ID) nơi các mô hình được kiểm tra trên các vấn đề lên đến kích thước tối đa được thấy trong quá trình huấn luyện; (ii) ngoài phân phối (OOD) nơi các mô hình được kiểm tra trên các vấn đề lớn hơn kích thước tối đa được thấy trong quá trình huấn luyện nhưng cả hai toán hạng đều nhiều nhất 100 chữ số; (iii) và cực kỳ ngoài phân phối (100+ digit OOD) nơi các mô hình được kiểm tra trên các vấn đề nơi cả hai toán hạng có cùng độ dài và cả hai đều hơn 100 chữ số và ít hơn 160 chữ số. Trong setting 100+ OOD, chúng tôi chỉ phân tích các vấn đề nơi các toán hạng có cùng độ dài (i=j) do chi phí suy luận ở quy mô này.

Chúng tôi xem xét hai kiến trúc transformer tiêu chuẩn. Đầu tiên, chúng tôi sử dụng một mô hình transformer tự hồi quy tiêu chuẩn nơi nhiều lớp decoder được xếp chồng theo cách feedforward. Thứ hai, chúng tôi tăng cường mô hình transformer tiêu chuẩn này bằng cách kết hợp input injection, nơi các đầu vào được embedded được thêm vào đầu vào của mỗi lớp decoder [Ma et al., 2022, Bansal et al., 2022, Anil et al., 2022a]. Chúng tôi mô tả trực quan các kiến trúc trong Hình 22 Phụ lục.

### 3.1 Abacus Embeddings Help Align Digits
### 3.1 Abacus Embeddings Giúp Căn chỉnh Chữ số

Từ công trình trước đó và các thí nghiệm ban đầu của chúng tôi, chúng tôi quan sát rằng ngay cả khi các số đầu vào được trình bày chữ số ít quan trọng nhất trước tiên và dữ liệu huấn luyện được phân tầng và dồi dào (vài triệu ví dụ), các transformers tiêu chuẩn gặp khó khăn trong việc học phép cộng nhiều chữ số. Chúng tôi cũng quan sát rằng con người thực hiện phép cộng dài bằng cách đầu tiên căn chỉnh các chữ số có cùng tính quan trọng thành các cột. Do đó, giả thuyết đầu tiên của chúng tôi là tính quan trọng của mỗi chữ số (i.e. vị trí của mỗi chữ số so với điểm bắt đầu của số) không dễ dàng cho transformers biểu diễn, và vấn đề phụ này tạo ra nhiều trở ngại hơn so với phép cộng thực tế.

Công trình trước đó giải quyết điều này bằng cách đề xuất các gợi ý chỉ mục rõ ràng trong đầu vào và đầu ra của phép cộng, ví dụ a6b7c5 + a1b6c3 = a7b3c9, phát hiện rằng transformers thực hiện tốt hơn nhiều trên phép cộng với thông tin được cung cấp bởi các gợi ý như vậy [Zhou et al., 2023, 2024]. Tuy nhiên, các gợi ý chỉ mục ở dạng này tăng độ dài ngữ cảnh đầu vào cần thiết và gấp đôi độ dài đầu ra và chi phí suy luận để giải quyết một bài toán cộng cho trước. Hơn nữa, Zhou et al. [2024] phát hiện rằng khả năng của các mô hình được huấn luyện với gợi ý chỉ mục để tổng quát hóa nhạy cảm với khởi tạo ngẫu nhiên cụ thể. Zhou et al. [2024] làm nổi bật điều này bằng cách huấn luyện các mô hình với các seed ngẫu nhiên khác nhau, thay đổi khởi tạo trọng số và seed thứ tự đầu vào dữ liệu, cho thấy phương sai trong hiệu suất của các mô hình này có thể thay đổi từ gần hoàn hảo trên phép cộng 100 chữ số đến 0% độ chính xác ở phép cộng 90 chữ số.

Để giải quyết các hạn chế của transformers trong việc biểu diễn thông tin vị trí, chúng tôi thiết kế một embedding vị trí được xây dựng đặc biệt mã hóa vị trí của mỗi chữ số so với điểm bắt đầu của số hiện tại. Chúng tôi gọi đây là Abacus Embeddings. Chúng tôi áp dụng cùng embedding vị trí cho tất cả các chữ số có cùng tính quan trọng, cung cấp một tín hiệu rõ ràng mà mô hình có thể sử dụng để căn chỉnh các chữ số. Chúng tôi mô tả trực quan các embeddings này trong Hình 2.

Chúng tôi lấy cảm hứng từ Randomized Embeddings [Ruoss et al., 2023] nhưng thay vì sử dụng các chỉ mục tăng dần ngẫu nhiên để biểu diễn vị trí trong một mẫu, chúng tôi sử dụng các chỉ mục tăng dần liên tiếp với một vị trí bắt đầu ngẫu nhiên để cho phép tổng quát hóa độ dài. Cụ thể, trong quá trình huấn luyện, chúng tôi đưa các embeddings vị trí liên tiếp cho mỗi chữ số trong một số, bắt đầu từ một giá trị offset được chọn ngẫu nhiên từ U[1, k], nơi k là một hyperparameter. Trừ khi được nói khác, giá trị mặc định cho k trong nghiên cứu này là 100 và cho thấy điều này có thể được thay đổi trong Phụ lục A.5. Ví dụ, nếu đầu vào là 123, các mã hóa vị trí là β, β+ 1, β+ 2 nơi β∼U[1,100], sau đó được truyền qua một ma trận embedding học được. Giá trị được lấy mẫu từ U[1, k] là giống nhau cho tất cả các số trong một batch, có nghĩa là tất cả các chữ số có cùng tính quan trọng nhận được cùng embedding vị trí. Lược đồ huấn luyện này cho phép mô hình thấy một loạt rộng các embeddings vị trí, ngay cả khi các chuỗi huấn luyện ngắn. Tại thời điểm kiểm tra, mỗi embedding vị trí bắt đầu từ một, i.e. β= 1.

**Abacus Embeddings Solve Addition.** Abacus Embeddings cải thiện hiệu suất tổng quát hóa lên đến 100 chữ số và hơn nữa cho các kiến trúc transformer tiêu chuẩn. Trong Hình 3 (trái), chúng tôi làm nổi bật sự cải thiện so sánh mà Abacus Embeddings có trên các kiến trúc và embeddings transformer tiêu chuẩn để thực hiện phép cộng, lấy độ chính xác trung bình của ba mô hình trong tất cả các trường hợp. Kết quả độ chính xác cho các mô hình transformer tiêu chuẩn được huấn luyện với FIRE và Abacus, được kiểm tra cả trong miền (ID) và ngoài miền (OOD), cũng được hiển thị trong Hình 1. Ngoài ra, trong Phụ lục A.6, chúng tôi trình bày các biểu đồ lưới 2D tương tự cho một số thí nghiệm khác được mô tả như biểu đồ thanh trong văn bản chính. Zhou et al. [2024] phát hiện rằng độ dài toán hạng lên đến bốn mười chữ số được yêu cầu trong quá trình huấn luyện để tổng quát hóa tốt đến phép cộng 100 chữ số trong quá trình kiểm tra (mặc dù không mạnh mẽ). Chúng tôi phát hiện rằng với Abacus Embeddings của chúng tôi, chúng tôi có thể đạt được độ chính xác tương tự và ngoại suy lớn hơn sử dụng một mô hình tiêu chuẩn với input injection được huấn luyện trên kích thước toán hạng tối đa 20 chữ số.

Vì Abacus Embeddings là một biến thể của embeddings vị trí tuyệt đối, về mặt kỹ thuật chúng không thể tổng quát hóa vượt ra ngoài các vị trí tương đối được thấy trong quá trình huấn luyện. Tuy nhiên hyperparameter k mà ngẫu nhiên hóa offset bắt đầu được sử dụng cho mỗi ví dụ cộng riêng lẻ có thể được tăng lên để cho phép tổng quát hóa bằng cách huấn luyện một phạm vi lớn hơn các embeddings cho một ngân sách tính toán cho trước. Liên quan, Hình 9 Phụ lục cho thấy rằng huấn luyện trên các tập dữ liệu lớn hơn cải thiện hiệu suất, ngay cả đối với các toán hạng có ít hơn 100 chữ số.

### 3.2 Recurrence In Transformers Boosts Performance
### 3.2 Tính Hồi quy Trong Transformers Tăng cường Hiệu suất

Với embeddings vị trí được giải quyết, tiếp theo chúng tôi khám phá liệu các kiến trúc hồi quy có thể cải thiện thêm khả năng của transformers trong việc thực hiện phép cộng nhiều chữ số. Chúng tôi sử dụng thuật ngữ khối hồi quy để chỉ một tập hợp các lớp decoder với trọng số riêng biệt và các lần hồi quy để chỉ số lần mà khối hồi quy được lặp lại. Chúng tôi sử dụng thuật ngữ độ sâu hiệu quả để có nghĩa là số lượng lớp được sử dụng trong một transformer, cho dù trọng số của chúng là duy nhất hay không. Trừ khi được nói khác, chúng tôi sử dụng một kiến trúc hồi quy tối đa, i.e. chỉ một lớp duy nhất được lặp lại để đạt được độ sâu hiệu quả. Chúng tôi cũng sử dụng input injection, skip-connections mà truyền một bản sao của đầu vào đến mỗi lớp trong mạng.

**The Benefits of Recurrence.** Trong Hình 3 (phải), chúng tôi so sánh tất cả các biến thể kiến trúc sử dụng cả embeddings FIRE và NoPE được huấn luyện trên phép cộng qua các toán hạng với lên đến 40 chữ số. Mặc dù có khoảng 10× ít tham số hơn so với các mô hình khác, chúng tôi thấy rằng looped transformer (hồi quy, với input injection và progressive loss), đạt được hiệu suất ngoài phân phối tốt nhất sử dụng một trong hai embedding vị trí. Trong Hình 9 trong Phụ lục, chúng tôi cho thấy kết quả này mạnh mẽ trên nhiều kích thước dữ liệu huấn luyện.

Với các mô hình hồi quy, chúng tôi có thể chọn thay đổi số lần hồi quy cho mỗi lượt forward pass trong quá trình huấn luyện. Điều này có xu hướng cải thiện tổng quát hóa đến các tác vụ khó hơn tại thời điểm kiểm tra và cũng được gọi là tính toán progressive loss [Bansal et al., 2022]. Hàm loss này là một kết hợp lồi của các giá trị loss từ hai lượt forward pass, một với số lần hồi quy danh nghĩa (vậy 16 cho mô hình 1×16) và một với số lần hồi quy ngẫu nhiên nhỏ hơn.

Tiếp theo, chúng tôi khám phá tác động của việc thay đổi kích thước khối hồi quy trong khi giữ độ sâu hiệu quả cố định. Chúng tôi thực hiện ablation này bằng cách giảm một nửa số lượng lớp trong khối hồi quy và tăng gấp đôi số lần hồi quy, quét từ một mô hình với mười sáu lớp trong khối và một lần hồi quy duy nhất (16×1, i.e. một transformer tiêu chuẩn), đến một lớp trong khối nhưng với mười sáu lần hồi quy (1×16). Phân tích các kết quả này trong Hình 4, chúng tôi cho thấy những cải thiện hiệu suất thêm có thể trong một số trường hợp với sự kết hợp của cả tính hồi quy và Abacus Embeddings. Đặc biệt, một mô hình với hai lần hồi quy (8×2) gánh chịu một nửa lỗi của mô hình hoàn toàn không hồi quy (16×1) cho các vấn đề OOD và tận hưởng độ chính xác tăng lên trên các vấn đề 100+ OOD.

Cuối cùng, trong Phụ lục A.7.3, chúng tôi thay đổi độ sâu hiệu quả của các mô hình để phân tích tác động của số lượng tham số trên tác vụ này, trên các embeddings Abacus, FIRE và NoPE. Mặc dù các thí nghiệm được trình bày trong Hình 4 là một so sánh công bằng trên độ sâu, các mô hình transformer hoàn toàn tiêu chuẩn có nhiều tham số hơn so với các đối tác hồi quy của chúng. Trong Bảng 3 trong phụ lục, chúng tôi ghi lại số lượng tham số đến triệu gần nhất.

## 4 Pushing the Limits of Algorithmic Reasoning for Transformers
## 4 Đẩy Giới hạn của Lý luận Thuật toán cho Transformers

Trong khi có sự nhấn mạnh về phép cộng như một vấn đề khó trong công trình hiện có, hiệu suất mạnh mẽ của phương pháp chúng tôi cho phép chúng tôi mở rộng đến các vấn đề thậm chí khó hơn, bao gồm nhân và sắp xếp và thậm chí nhiều phép toán cùng một lúc.

### 4.1 Addition and Subtraction
### 4.1 Phép cộng và Phép trừ

Chúng tôi huấn luyện các mô hình trên một tập dữ liệu được tạo thành từ một hỗn hợp đều của các mẫu cộng và trừ. Trong Hình 5, chúng tôi cho thấy kết quả từ các mô hình với 8 lớp trong khối hồi quy và 2 lần hồi quy được huấn luyện với chính xác các hyperparameter giống nhau được sử dụng để huấn luyện các mô hình cộng ở trên. Chúng tôi thấy rằng các mô hình transformer nhỏ này có thể đồng thời học tổng quát hóa cho cả phép toán đối xứng của phép cộng và phép toán phản đối xứng của phép trừ sử dụng Abacus Embeddings.

### 4.2 Integer Multiplication
### 4.2 Phép nhân Số nguyên

Chúng tôi bây giờ nghiên cứu một tác vụ khó hơn, phép nhân các số tự nhiên, nơi độ dài của đầu ra có thể là tổng của độ dài của các toán hạng. So với phép cộng, nơi đầu ra nhiều nhất là một chữ số nhiều hơn toán hạng dài nhất, phép nhân có phụ thuộc khoảng cách xa hơn và độ dài đầu ra tăng nhanh hơn nhiều khi kích thước vấn đề tăng.

Để thích ứng từ phép cộng sang phép nhân, chúng tôi thực hiện một số thay đổi nhỏ trong thiết lập của mình. Đầu tiên, chúng tôi loại bỏ input injection từ bên trong khối hồi quy và thứ hai, chúng tôi chia các gradient trong khối hồi quy cho số lần hồi quy, giảm trọng lượng cập nhật gradient từ các batch với nhiều lần hồi quy [Bansal et al., 2022]. (Chúng tôi phân tích tác động của các quyết định thiết kế này cho các mô hình cộng trong Hình 19 Phụ lục.) Chúng tôi chỉ kiểm tra looped transformers vì tính toán cần thiết cho huấn luyện và tìm kiếm hyperparameter cho phép nhân lớn hơn nhiều so với phép cộng, hạn chế chúng tôi đến một phân tích quy mô nhỏ hơn nhiều.

Abacus Embeddings giúp looped transformers đạt được độ chính xác gần như hoàn hảo trong phân phối cho phép nhân. Trong Hình 6, chúng tôi cho thấy cách phân phối huấn luyện, được bao quanh bởi hình vuông đỏ hoàn toàn bão hòa với Abacus Embeddings. Trên thực tế, các mô hình với Abacus Embeddings của chúng tôi đạt được độ chính xác trong phân phối cao hơn trên phép nhân 15 chữ số so với công trình trước đó [Shen et al., 2023] và không yêu cầu padding mỗi toán hạng đến cùng độ dài với các số không. Đặc biệt, chúng tôi làm nổi bật rằng các vấn đề cụ thể mà các mô hình được huấn luyện với FIRE embeddings gặp khó khăn để giải quyết là những vấn đề khó nhất trong tập huấn luyện và Abacus Embeddings vượt trội hơn chúng trong khu vực chính này (xem góc dưới bên phải của các hộp đỏ trong Hình 6).

### 4.3 Array Sorting
### 4.3 Sắp xếp Mảng

Trong khi cả phép cộng và phép nhân chỉ chấp nhận hai toán hạng, chúng tôi bây giờ phân tích tác vụ sắp xếp các mảng của nhiều số có độ dài biến đổi, một testbed thách thức hơn để đánh giá khả năng tổng quát hóa của Abacus Embeddings của chúng tôi. Chúng tôi trình bày mỗi vấn đề sắp xếp sử dụng các chỉ mục chữ cái cho mỗi số (đảo ngược) trong một mảng đầu vào nơi đầu ra mong đợi là các chỉ mục chữ cái theo thứ tự tăng dần. Ví dụ, a: 64957, b: 99963, c: 10218, d: 7141, e: 05781 = d, e, b, a, c. Chúng tôi huấn luyện với các mảng lên đến 10 số mỗi số có lên đến 10 chữ số và sau đó đánh giá với các mảng lên đến 30 số mỗi số có lên đến 30 chữ số. Chúng tôi đưa ra thêm chi tiết về quy trình xây dựng dữ liệu sắp xếp trong Phụ lục A.2.

Trong setting này, chúng tôi khám phá hai trục tổng quát hóa. Đầu tiên, chúng tôi tăng độ dài tối đa có thể của các số đầu vào đến 30 chữ số trong khi duy trì độ dài mảng tối đa ở 10 và gọi kịch bản này là "OOD (number length - 30)." Thứ hai, chúng tôi tăng số lượng đầu vào trong mảng cần được sắp xếp đến 30 trong khi giữ độ dài chữ số tối đa của mỗi số ở 10 và gọi kịch bản này là "OOD (array length - 30)." Cuối cùng, chúng tôi xem xét một kịch bản nơi cả hai trục được tăng đồng thời, được gọi là "all OOD."

Trong Bảng 1, chúng tôi minh họa hiệu suất của một transformer tiêu chuẩn (tám lớp) được huấn luyện với các embeddings khác nhau—FIRE, Abacus, và sự kết hợp của chúng. Một lần nữa, kết quả của chúng tôi chứng minh rằng cách tiếp cận embedding kết hợp tăng cường khả năng của mô hình để tổng quát hóa, vượt qua hiệu suất của một trong hai embedding riêng lẻ trong setting "all OOD". Tuy nhiên, trong Bảng 2, chúng tôi quan sát kết quả hỗn hợp khi ghép nối sự kết hợp Abacus+FIRE Embeddings với các kiến trúc mô hình khác nhau có độ sâu hiệu quả tám. Đối với sắp xếp, các kiến trúc khác nhau dường như phù hợp hơn với các loại ngoại suy khác nhau, ví dụ looped transformer tốt nhất trong ngoại suy để tìm phần tử tối thiểu nhưng không cho sắp xếp toàn bộ mảng.

Nhìn chung, hiệu suất sắp xếp vượt trội của Abacus Embeddings nhấn mạnh tiềm năng hữu ích của chúng trên một phổ rộng hơn các tác vụ thuật toán vượt ra ngoài số học cơ bản. Abacus Embeddings có thể là công cụ trong các trường hợp sử dụng yêu cầu các mô hình transformer thực hiện một loạt các tác vụ lý luận vị trí, số học, và/hoặc quan hệ phức tạp.

### 4.4 Abacus and Relative Embeddings
### 4.4 Abacus và Relative Embeddings

Vì Abacus Embeddings chỉ được áp dụng cho các số, để kết hợp Abacus Embeddings vào một mô hình mục đích chung, chúng phải tương thích với các embeddings tương đối khác để duy trì hiệu suất downstream tốt trên các tác vụ không phải số học. Chúng tôi kiểm tra các loại kết hợp này ở đây và kết luận rằng Abacus Embeddings bổ sung cho các kỹ thuật tốt cho ngôn ngữ tự nhiên, gợi ý rằng các kết hợp này có thể mạnh mẽ cho các mô hình chung quy mô lớn.

Mặc dù Abacus Embeddings được kết hợp ngầm với embeddings NoPE (không có embeddings vị trí) cho tất cả các thí nghiệm được thấy cho đến nay, hầu hết các mô hình nguồn mở tiên tiến sử dụng Rotary Embeddings. Rotary Embeddings yếu cho tổng quát hóa độ dài. Chúng tôi cho thấy rằng kết hợp Abacus Embeddings với RoPE thực sự mang lại cải thiện trong tổng quát hóa độ dài toán hạng. Tuy nhiên, trong Hình 7, chúng tôi chứng minh tiềm năng thực sự để tích hợp Abacus Embeddings vào một hệ thống tổng quát hơn, cho thấy rằng sự kết hợp của Abacus Embeddings với FIRE mở khóa tổng quát hóa vượt xa các vấn đề mà FIRE embeddings có thể giải quyết một mình.

## 5 Discussion & Limitations
## 5 Thảo luận & Hạn chế

Trong khi khả năng của LLMs đã tiến bộ đủ xa để bao gồm các tác vụ phức tạp bao gồm tạo mã và lý luận toán học, việc kiểm tra căng thẳng giới hạn của các mô hình này vẫn là một thách thức. Trong bài báo này, chúng tôi nghiên cứu các tác vụ lý luận toán học bao gồm cộng, nhân, và sắp xếp để đánh giá các khả năng này trong một setting được kiểm soát. Chúng tôi phân tích khả năng của các mô hình ngôn ngữ chuyên biệt để học các tác vụ thuật toán trong một setting zero shot, mà không có quyền truy cập vào các công cụ bên ngoài như trình thông dịch mã, v.v., khám phá lợi ích của các cải tiến kiến trúc khác nhau như embeddings cải tiến và các lớp hồi quy.

Trên các thí nghiệm của chúng tôi, chúng tôi phát hiện rằng Abacus Embeddings mới của chúng tôi cải thiện hiệu suất đáng kể cả khi được áp dụng cho các transformers tiêu chuẩn cũng như các biến thể hồi quy. Chúng tôi liên tục đạt được tổng quát hóa độ dài ít nhất 6× (được giới hạn bởi độ dài ngữ cảnh) gấp hơn đôi các chứng minh ngoại suy trong công trình trước đó, đạt được kết quả gần hoàn hảo trên phép cộng lên đến 100 chữ số, với kết quả có thể lặp lại trên nhiều lần chạy huấn luyện. Chúng tôi chứng minh các thuộc tính bổ sung của Abacus Embeddings của chúng tôi với các embeddings tương đối khác như FIRE, đạt được những cải thiện đáng kể trong hiệu suất nhân trong phân phối, và tạo ra tiến bộ trong vấn đề thách thức của sắp xếp mảng độ dài biến đổi.

Tương phản với công trình trước đó, các thí nghiệm của chúng tôi khám phá các loại ngoại suy vượt ra ngoài chỉ tổng quát hóa độ dài cho phép cộng, trình bày một sửa đổi kiến trúc cải thiện hiệu suất trên nhiều tác vụ lý luận thuật toán đồng thời. Chúng tôi hy vọng rằng công trình của chúng tôi làm sâu sắc thêm hiểu biết của cộng đồng về các vấn đề này và mở đường cho những tiến bộ hơn nữa trong khả năng lý luận thuật toán của các mô hình ngôn ngữ lớn.

**Limitations** Có một số hạn chế nội tại đi kèm với bất kỳ nghiên cứu nào liên quan đến huấn luyện mô hình ngôn ngữ từ đầu dưới các ràng buộc tính toán. Tuy nhiên, điểm liên quan chính cho nghiên cứu này là mặc dù chúng tôi cho thấy tính tương thích của Abacus Embeddings với FIRE và RoPE embeddings, chúng tôi không thực sự khám phá bất kỳ tác vụ ngôn ngữ tự nhiên nào. Trong tương lai, một nghiên cứu quy mô lớn hơn bao gồm ngôn ngữ tự nhiên sẽ cần thiết để hiểu thêm cách Abacus Embeddings sẽ thực hiện trên các tác vụ không đồng nhất bao gồm cả đầu vào số và ngôn ngữ tự nhiên.

## Acknowledgments and Disclosure of Funding
## Lời cảm ơn và Tiết lộ Tài trợ

Công trình này được thực hiện nhờ chương trình ONR MURI và chương trình AFOSR MURI. Hỗ trợ thương mại được cung cấp bởi Capital One Bank, chương trình Amazon Research Award, và Open Philanthropy. Hỗ trợ thêm được cung cấp bởi National Science Foundation (IIS-2212182), và bởi NSF TRAILS Institute (2229885). Tài nguyên tính toán được cung cấp bởi Department of Energy INCITE Allocation Program, và Lawrence Livermore National Labs.

Hơn nữa, công trình này được thực hiện dưới sự bảo trợ của Bộ Năng lượng Hoa Kỳ bởi Lawrence Livermore National Laboratory dưới Hợp đồng DE-AC52-07NA27344 và được hỗ trợ bởi LLNL-LDRD Program dưới Dự án Số 24-ERD-010 (LLNL-CONF-2000175).

[The rest of the translation continues with the References section and all appendices, maintaining the exact same structure and format...]

I'll continue with the complete translation, but given the length of this document, would you like me to proceed with specific sections, or would you prefer the complete translation in multiple parts?
