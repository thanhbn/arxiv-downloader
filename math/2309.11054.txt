# 2309.11054.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/math/2309.11054.pdf
# File size: 1007372 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Preprint.
DESIGN OF CHAIN -OF-THOUGHT IN MATH PROBLEM
SOLVING
Zhanming Jie∗, Trung Quoc Luong∗, Xinbo Zhang∗, Xiaoran Jin, Hang Li
ByteDance Research
{allan, trung.luong, zhangxinbo.freya }@bytedance.com
{xiaoran.jin, lihang.lh }@bytedance.com
ABSTRACT
Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solv-
ing. We conduct a comprehensive examination of methods for designing CoT,
comparing conventional natural language CoT with various program CoTs, in-
cluding the self-describing program , the comment-describing program , and the
non-describing program . Furthermore, we investigate the impact of programming
language on program CoTs, comparing Python andWolfram Language . Through
extensive experiments on GSM8K, M ATHQA, and SVAMP, we find that pro-
gram CoTs often have superior effectiveness in math problem solving. Notably,
the best performing combination with 30B parameters beats GPT-3.5-turbo by a
significant margin. The results show that self-describing program offers greater
diversity and thus can generally achieve higher performance. We also find that
Python is a better choice of language than Wolfram for program CoTs. The exper-
imental results provide a valuable guideline for future CoT designs that take into
account both programming language and coding style for further advancements.
Our datasets and code are publicly available1.
1 I NTRODUCTION
Math problem solving is an ideal task to assess the multi-step reasoning abilities of large language
models (LLMs). LLMs exhibit remarkable reasoning abilities with the use of chains-of-thought,
surpassing previous methods in various reasoning tasks (Lightman et al., 2023; Wei et al., 2022b;
Touvron et al., 2023a). The challenge of producing reliable chains-of-thought (CoT) (Wei et al.,
2022b) remains, however, particularly in the nuanced and complex cases of mathematical problem
solving (Golovneva et al., 2022). Recent research has focused on refining prompt engineering strate-
gies or developing new CoT representations, such as program CoTs (Gao et al., 2023; He-Yueya
et al., 2023).
Although existing approaches can boost overall performance (Lu et al., 2023), a thorough com-
parison of various CoTs remains absent in the literature. In this paper, we conduct a comprehen-
sive examination of multiple CoT designs, including natural language (NL) and program CoTs,
such as the self-describing program , the comment-describing program , and the non-describing pro-
gram . Figure 1 illustrates different CoT representations for solving a multi-choice math problem.
For program CoTs, besides the popular programming language Python , we also use Wolfram Lan-
guage (Wolfram, 2015), a scientific programming language known for its ability to naturally express
complex mathematical expressions.
One advantage of the program CoTs is that their validity can be easily verified by executing the
programs. For instance, we can easily represent an equation in one line (e.g., Figure 1 Wolfram line
1) and solve it with built-in functions (i.e., Solve[] ). The NL CoTs do not have this capability,
but they can better translate the questions in language into descriptions of reasoning by leveraging
the power of LLMs. We consider three types of program CoTs. The self-describing program (SDP)
is similar to PAL (Gao et al., 2023) in which the variable names are extracted from the questions.
∗Equal contribution, order decided by the initial letter of the surname.
1https://github.com/lqtrung1998/mwp_cot_design
1arXiv:2309.11054v2  [cs.CL]  30 Sep 2023

--- PAGE 2 ---
Preprint.
Let's start by assigning variables to the ages of A, B, and C.  Let A = age of A. Let B = age of B. Let C = age of CFrom the problem, we know that: A = B + 2 (A is two years older than B) B = 2C (B is twice as old as C) A + B + C = 27 (The total of their ages is 27) Now we can substitute the second equation into the first equation to get: A = 2C + 2 We can then substitute both equations into the third equation to get: (2C + 2) + 2C + (C) = 27Simplifying this equation, we get: 5C + 2 = 27; 5C = 25; C = 5 Now that we know C is 5, we can use the second equation to find B: B = 2C = 2(5) = 10Therefore, the answer is <E>. B is 10 years old. Therefore, the answer is: EQuestion: a is two years older than b who is twice as old as c . if the total of the ages of a , b and c be 27 , then how oldisb ? a ) 7 , b ) 9 , c ) 8 , d ) 11 , e ) 10 Answer reasoning:NL CoTInput
Wolfram CDP CoTWolfram SDP CoT
Python CDP CoTPython SDP CoT1.defsolution():2.importmath3.importsympy4.a=sympy.Symbol('a')5.b=sympy.Symbol('b')6.c=sympy.Symbol('c')7.options=[7,9,8,11,10]8.a_two_years_older_than_b=sympy.Eq(a,b+2)9.b_twice_as_c=sympy.Eq(b,c*2)10.sum_of_a_b_c=sympy.Eq(a+b+c,27)11.b_value=sympy.solve([a_two_years_older_than_b,b_twice_as_c,sum_of_a_b_c])[b]12.correct_option=None13.fori,optioninenumerate(options):14.ifmath.fabs(option-b_value)<1e-4:15.correct_option=chr(ord('A')+i)16.break17.result=correct_option18.returnresult1.v1=a==b+2(*aistwoyearsolderthanb*)2.v2=b==2*c(*whoistwiceasoldasc*)3.v3=a+b+c==27(*thetotaloftheagesofa,bandcbe27*)4.option=<|A->7,B->9,C->8,D->11,E->10|>5.v4=Solve[{v1,v2,v3},{a,b,c}](*Step1:Solvethesystemofequations*)6.v5=b/.v4[[1]](*Step2:Getthevalueofb*)7.Keys[Select[option,#==v5&]](*Sothecorrectoptionis*)1.a_is_two_years_older_than_b=a-2==b2.b_is_twice_as_old_as_c=b==2*c3.total_age=a+b+c==274.option=<|A->7,B->9,C->8,D->11,E->10|>5.solution=Solve[{a_is_two_years_older_than_b,b_is_twice_as_old_as_c,total_age},{a,b,c}]6.b_age_value=b/.solution[[1]]7.Keys[Select[option,#==b_age_value&]]1.defsolution():2.importmath3.importsympy4.v1=sympy.Symbol('a')5.v2=sympy.Symbol('b')6.v3=sympy.Symbol('c')7.options=[7,9,8,11,10]8.v4=sympy.Eq(v1,v2+2)#aistwoyearsolderthanb9.v5=sympy.Eq(v2,v3*2)#bistwiceasoldasc10.v6=sympy.Eq(v1+v2+v3,27)#thetotaloftheagesofa,bandcbe2711.v7=sympy.solve([v4,v5,v6])[v2]#howoldisb12.correct_option=None13.fori,optioninenumerate(options):14.ifmath.fabs(option-v7)<1e-4:15.correct_option=chr(ord('A')+i)16.break17.result=correct_option18.returnresult
Figure 1: Examples of CoT representations: Natural Language (NL) CoT, Comment-Describing
Program (CDP) and Self-Describing Program (SDP) in both Wolfram and Python.
In contrast, the non-describing program (NDP) only uses abstract variable names (e.g., v1andv2).
In SDP, programs can be created more easily from the questions, while in NDP, programs can
be used more effectively in reasoning. To combine the strengths of both types, we introduce the
comment-describing program (CDP), a new CoT design that blends abstract variable names with
natural language comments.
Following the common practice (Uesato et al., 2022; Lightman et al., 2023), we conduct fine-tuning,
reranking, and majority-voting experiments to compare the CoTs on GSM8K (Cobbe et al., 2021),
MATHQA (Amini et al., 2019), and SVAMP (Patel et al., 2021) datasets. Under the best setting, the
method using the 30B model with reward model reranking is able to outperform the GPT-3.5-turbo’s
few-shot performance by approximately 2.9% on GSM8K, 18% on M ATHQA and 8% on SVAMP.
We make the following main conclusions from the experiments.
1. Program CoTs generally perform better than natural language CoTs, indicating that the use of
more rigid CoTs is better.
2. The presence of natural language in SDP and CDP is crucial for achieving high performance
compared with NDP. SDP is generally superior to CDP, because it can generate more diverse
CoTs and thus achieve higher performance in majority voting and reranking.
3. Program CoTs in Python perform better than those in Wolfram, when the CoTs are in the same
type.
4. By combining the use of different types of CoTs, we can enhance overall performance, showing
the potential for further CoT design that takes advantage of the strengths of all CoT types.
Our findings offer valuable insights for designing CoTs in math problem solving and more broadly
reasoning with LLMs.
2

--- PAGE 3 ---
Preprint.
2 C HAIN -OF-THOUGHT DESIGN
2.1 N ATURAL LANGUAGE COTS(NL)
Wei et al. (2022b) propose the chain-of-thought prompting technique to enhance the complex rea-
soning abilities of LLMs. This method endeavors to simulate the thought process of addressing
multi-step reasoning problems. As depicted in the second block of Figure 1, this chain-of-thought
approach for math problem solving produces step-by-step reasoning descriptions in natural language
and provides the final answer at the end of the reasoning process.
2.2 P ROGRAM COTS
We focus on two distinct programming languages: Wolfram Language (Wolfram, 2015) and
Python (Van Rossum & Drake Jr, 1995). Recent work (Wang et al., 2023a) also uses these two lan-
guages in tool-based Transformers (Schick et al., 2023). The Wolfram Language, with the Wolfram
Mathematica as its execution engine2, is an expressive and versatile language that can effectively
represent complex mathematical concepts. It has rich built-in mathematical functions for algebra,
equation solving, etc., and an intuitively designed and relaxed syntax. On the other hand, Python is a
general-purpose language that has gained widespread adoption in recent literature for mathematical
problem solving (Gao et al., 2023; He-Yueya et al., 2023). Given the contrasting nature of Wolfram
and Python, we conduct a comprehensive comparison across all program CoT types in the two lan-
guages. Next, we describe the design of the CoT types, with Figure 1 showcasing their instances in
the two languages.
Self-Describing Program (SDP) The first design we consider is self-describing program (SDP)
as shown in the bottom right of Figure 1. It presents a solution in a step-by-step manner and defines
variable names using natural language, similar to that of Gao et al. (2023). One advantage of SDP
is that one can solve the problem by directly executing the program. Another advantage is that the
variable names are from the question, making it easier to generate the reasoning steps for the LLM.
When labeling programs, we follow several general guidelines: (1) using high-level operations to
make the program concise and intuitively understandable, (2) listing variable names according to
their order in the question, and (3) ensuring that variable names are meaningful, descriptive, and
written in snake case naming convention (e.g., lower-cased and separated by underscores).
Comment-Describing Program (CDP) Although the design is concise, SDP has several prob-
lems. The self-describing names may not be sufficiently general across problems and sufficiently
informative to provide rich context in CoTs. Therefore, we consider comment-describing program
(CDP) using standardized variable names, e.g., v1,v2, and brief comments that describe the step of
reasoning and problem solving. Figure 1 (bottom left) shows an example in Python and Wolfram.
The comment in a declaration line is a brief problem statement that provides details. The comment
in a reasoning line explains the purpose of the step, displayed as a command or instruction. Since
the Python language often requires stricter syntax, extra declaration lines, such as the Sympy symbol
declaration line, must be included in the program to make it executable. In such lines, the comment
is omitted.
Non-Describing Program (NDP) We also consider a variant where the comments of CDP are
discarded. NDP can also be considered as an approach contrary to SDP whereas in the former
variable names are defined in natural language and in the latter variable names are defined as abstract
symbols.
3 D ATA COLLECTION
We consider three datasets in this work, GSM8K (Cobbe et al., 2021), M ATHQA (Amini et al.,
2019), and SVAMP (Patel et al., 2021). Given the questions, we develop a method to semi-
automatically annotate the CoTs in the training set. Generally, we use the few-shot prompting
technique to obtain CDPs and SDPs in both Python and Wolfram, as well as NL CoTs.
2https://www.wolfram.com/engine/
3

--- PAGE 4 ---
Preprint.
Figure 2: Overview of data collection, with CDP as an example.
Our LLM-empowered annotation approach works in the following way. We first manually create
a small number of CoT annotations, and then let the LLM to retrieve similar CoT annotations as
examples and generate CoTs based on the examples in a few-shot manner. We then automatically
execute the programs and take the correctly verified annotations as the annotation results. The
process is repeated three to five times and finally, we manually annotate those that still cannot pass
the verification. We use the Wolfram CoT as an example to illustrate the annotation details.
(1)Initial manual seed annotation We randomly select 20samples from the dataset for
self-describing program annotations and comment-describing program annotations, respec-
tively. The annotated programs must follow the CoT definition and Wolfram grammar. We
conduct cross-verification among the authors, execute the programs by Wolfram Mathe-
matica, and obtain the annotation results of the samples that are successfully executed. The
20 samples and their correct annotations are considered as the initial completion set , and
the other samples in the dataset are considered as the initial working set .
(2)Question embeddings acquisition We acquire all the embeddings of questions in the
dataset by directly calling the API of “ text-embedding-ada-002 ” from OpenAI3.
(3)Retrieval-based LLM annotation For each sample to be annotated in the working set , we
retrieve the top- ksimilar examples (Liu et al., 2022; Gao et al., 2021) from the completion
setbased on the cosine similarity of the question embeddings. For CDP annotation, we use
the questions of the top- kexamples and their CDP programs as the prompts, and let the
LLM return the CDP program for the given sample. The format of an example is presented
in Fig 2. Here we choose “ gpt-3.5-turbo ” as the LLM and kis set to 5. For SDP
annotation, we use the questions of the top- kexamples and their SDP programs as the
prompts, and let the LLM return the SDP program.
(4)Automatic verification, updating completion set and working set After obtaining all an-
notations of working set returned by the LLM, the annotated CDPs and SDPs are executed
3https://platform.openai.com/docs/guides/embeddings
4

--- PAGE 5 ---
Preprint.
using Wolfram Mathematica, and then the results are compared with the ground truth to
determine correctness. For GSM8K and SVAMP, since the answers should be numeric,
we consider the answers not equal unless they can be converted to float and their values
differ at most by 1e−3. For M ATHQA, due to the multiple-choice format of questions, we
adopt exact match to compare answers. Samples with correct results after execution are put
into the completion set , and are removed from the working set .
(5) Repeat step 3and step 4for three to five times until the working set is empty or no new
samples can be added into the completion set .
(6)Manually modifying remaining working set If there are still any remaining samples in
theworking set , we manually conduct annotations on the samples, until the programs can
get correct results using Wolfram Mathematica.
The ways of creating Python CoTs and NL CoTs are the same as above. Note that for NL CoTs,
because they cannot be directly verified by an engine, we just apply a simple rule followed by NL
CoT, “Therefore the answer is:” , to obtain the answers. NDPs in Wolfram and Python can be
obtained by removing the comments in their corresponding CDPs.
4 M ETHODOLOGY
In accordance with previous studies (Uesato et al., 2022; Lightman et al., 2023), we employ su-
pervised fine-tuning (SFT), self-consistency decoding (alternatively referred to as majority voting )
(Wang et al., 2023b), and reward model reranking methodologies on our annotated dataset.
4.1 S UPERVISED FINE-TUNING
We conduct SFT on a pre-trained language model using questions and chain-of-thought annotations
in each dataset. The training aims to maximize the likelihood of the answer given the question. In
evaluation, we extract the final answer generated by the SFT model. As shown in Figure 1, the NL
CoT places the final answer in the last sentence, “ Therefore, the answer is E. ”. In the cases of SDP,
CDP, and NDP, we execute the program to obtain the answer.
4.2 M AJORITY VOTING
In self-consistency decoding (Wang et al., 2023b)4, we first sample a certain number of CoTs from
the language model. We then perform majority voting over the answers extracted from all the sam-
pled CoTs and choose the final answer that is the most favored among all answers. We simply adopt
the temperature sampling strategy (Ackley et al., 1985; Ficler & Goldberg, 2017) with T= 1.0,
because it is reported (Wang et al., 2023b) that self-consistency decoding is generally robust to
sampling strategies and hyperparameters.
4.3 R ERANKING WITH REWARD MODEL
Following Cobbe et al. (2021), a reward model (RM) is trained to determine whether an answer to
the question is correct or not. Given the SFT model, we perform sampling to obtain a certain number
of CoT solutions to the question. As a common practice, the reward model is a language model that
is initialized from the SFT model. Similar to the outcome-based reward model (ORM) (Uesato et al.,
2022), the reward model is trained to predict a binary label that indicates the “ correct ” or “ incorrect ”
solution5. Once the input passes through the reward model, classification is conducted with a linear
classifier on the hidden state of the last token. Finally, the solution with the highest “ correct ” score
among the candidates is selected as the final answer.
As we do not have explicit “ correct ” and “ incorrect ” pairs annotated, we adopt the model at the
2ndepoch during supervised fine-tuning to sample solution pairs. According to Cobbe et al. (2021),
4We use the term “majority voting” in the rest of this paper unless specified.
5Our preliminary experiments with process-based reward (Lightman et al., 2023) show similar performance
with outcome-based reward. We attribute the reason to the quality of automatic process labels (Uesato et al.,
2022).
5

--- PAGE 6 ---
Preprint.
Program CoT Type Size GSM8K MathQA SV AMP Avg.
- Natural Language 6.7B 41.0 58 .7 53 .8 51 .2
PythonNon-Describing Program 6.7B 56.3 64 .4 59 .1 59 .9
Self-Describing Program 6.7B 57.1 64.8 69.3 63.7
Comment-Describing Program 6.7B 56.5 64 .7 62 .3 61 .2
WolframNon-Describing Program 6.7B 53.4 63 .0 58 .6 58 .3
Self-Describing Program 6.7B 50.2 62 .5 65.5 59.4
Comment-Describing Program 6.7B 57.0 63.1 64.0 61.4
- Natural Language 30B 57.4 66 .6 70 .1 64 .7
PythonNon-Describing Program 30B 65.8 66 .0 73 .9 68 .6
Self-Describing Program 30B 68.3 67 .2 80.4 72.0
Comment-Describing Program 30B 68.7 67.2 78.2 71 .4
WolframNon-Describing Program 30B 62.2 64 .9 73 .1 66 .7
Self-Describing Program 30B 62.6 64 .3 73 .9 66 .9
Comment-Describing Program 30B 66.7 65.0 75.9 69.2
Table 1: Supervised fine-tuning performance of all CoT types. Numbers displayed in bold are
highest in the same settings.
using the checkpoints at initial epochs can provide more diverse solutions for training a reward
model. For each question in the training data D, we sample Ksolutions. We then use all the
samples that contain both correct and incorrect solutions to train the reward model for three epochs.
5 E XPERIMENTS
5.1 E XPERIMENT SETTINGS
We conduct experiments on the three datasets: GSM8K (Cobbe et al., 2021), M ATHQA (Amini
et al., 2019)6, and SVAMP (Patel et al., 2021). Appendix §B illustrates the preprocessing procedure
for MathQA and SV AMP dataset. The training data for all CoT types is obtained using the method
described in §3. We report the results of few-shot prompting using GPT-3.5-turbo, majority voting,
and RM reranking7.
We adopt the pre-trained language model Galactica (Taylor et al., 2022) which is trained on a large-
scale scientific corpus and programming codes. The Galactica model shows superior performance
in math problem solving compared to other foundation models such as LLaMA (Touvron et al.,
2023a) in our preliminary experiments. Throughout the experiments, we use the model size of 6.7B8
and 30B9available in HuggingFace. We use the Megatron-Deepspeed10framework for efficient
supervised fine-tuning, following BLOOM (Scao et al., 2022). The model is fine-tuned for 40epochs
with a maximum sequence length of 1024 . Please refer to Appendix (Table 8) for hyper-parameter
settings.
We select the SFT model with the best accuracy for sampling to obtain the majority voting (Wang
et al., 2023b) results. To train the reward model, we generate 100samples for each question in the
training set using the SFT checkpoint at the second epoch and compare them with the ground-truths
to determine the correct labels. By using an earlier checkpoint, we can have more sampling diversity,
which is helpful for the reward model training. As described in §4.3, we initialize the reward model
with the best SFT model checkpoint and fine-tune it for three epochs.
6Due to limitation of computing budget, we only experiment with a random 15ksamples of M ATHQA
training set.
7We also experimented with RM-weighted voting and the performance is similar to reranking (Appendix
§C).
8https://huggingface.co/facebook/galactica-6.7b
9https://huggingface.co/facebook/galactica-30b
10https://github.com/bigscience-workshop/Megatron-DeepSpeed
6

--- PAGE 7 ---
Preprint.
5.2 S UPERVISED FINE-TUNING RESULTS
Table 1 presents the supervised fine-tuning results across all datasets, languages, and CoT types. In
general, program-based CoTs perform better than natural language CoT. An enlarged model size
correlates with a noticeable increase in the performance of natural language CoTs, presumably due
to an improved capacity for natural language understanding. Nevertheless, program-based CoTs
consistently and significantly outperform natural language CoT. The SFT models described here are
then used for majority voting and reranking.
Program Method Size GSM8K MathQA SV AMP
- GPT-3.5-turbo prompting + Natural Language N.A. 75.3 60 .6 73 .0
WolframGPT-3.5-turbo prompting + Self-Describing Program N.A. 73.5 39 .3 72 .8
GPT-3.5-turbo prompting + Comment-Describing Program N.A. 69.1 31 .2 70 .1
PythonGPT-3.5-turbo prompting + Self-Describing Program N.A. 78.0 45 .5 78 .4
GPT-3.5-turbo prompting + Comment-Describing Program N.A. 72.4 46 .2 77 .6
-SFT + Majority V oting + Natural Language 6.7B 50.8 59 .5 63 .1
SFT + Reranking + Natural Language 6.7B 59.5 61 .8 67 .0
WolframSFT + Majority V oting + Non-Describing Program 6.7B 53.7 70 .3 60 .9
SFT + Majority V oting + Self-Describing Program 6.7B 59.5 72 .7 72 .9
SFT + Majority V oting + Comment-Describing Program 6.7B 61.3 71 .3 68 .3
SFT + Reranking + Non-Describing Program 6.7B 61.1 71 .0 66 .4
SFT + Reranking + Self-Describing Program 6.7B 71.4 73 .8 77 .3
SFT + Reranking + Comment-Describing Program 6.7B 69.7 72 .0 75 .5
PythonSFT + Majority V oting + Non-Describing Program 6.7B 56.4 70 .5 61 .6
SFT + Majority V oting + Self-Describing Program 6.7B 61.1 73 .7 73 .7
SFT + Majority V oting + Comment-Describing Program 6.7B 58.6 71 .5 63 .4
SFT + Reranking + Non-Describing Program 6.7B 63.6 70 .7 69 .7
SFT + Reranking + Self-Describing Program 6.7B 72.4 75 .2 78 .6
SFT + Reranking + Comment-Describing Program 6.7B 69.9 71 .6 69 .8
-SFT + Majority V oting + Natural Language 30B 69.8 69 .4 72 .0
SFT + Reranking + Natural Language 30B 75.7 74 .3 77 .0
WolframSFT + Majority V oting + Non-Describing Program 30B 63.4 72 .2 73 .3
SFT + Majority V oting + Self-Describing Program 30B 69.8 77 .9 78 .7
SFT + Majority V oting + Comment-Describing Program 30B 69.8 73 .6 79 .2
SFT + Reranking + Non-Describing Program 30B 71.4 72 .9 77 .2
SFT + Reranking + Self-Describing Program 30B 79.9 78.6 83.4
SFT + Reranking + Comment-Describing Program 30B 78.6 74 .1 82 .9
PythonSFT + Majority V oting + Non-Describing Program 30B 67.0 72 .6 75 .3
SFT + Majority V oting + Self-Describing Program 30B 72.3 77 .2 82 .7
SFT + Majority V oting + Comment-Describing Program 30B 70.1 74 .8 79 .1
SFT + Reranking + Non-Describing Program 30B 74.3 73 .0 78 .4
SFT + Reranking + Self-Describing Program 30B 80.9 78.1 87.0
SFT + Reranking + Comment-Describing Program 30B 78.2 75 .1 81 .5
Table 2: Performance comparison among few-shot prompting, majority voting and reranking. Num-
bers in bold are the best and significantly better than the second-best with p < 0.001.
5.3 M AINRESULTS
Table 2 shows the comparison among different methods: GPT-3.5-turbo11prompting, majority vot-
ing, and reranking. We can observe that larger models (i.e., 30B) can significantly improve the
performance over smaller models (i.e., 6.7B) on all datasets. Our best variants are able to outper-
form few-shot prompting GPT-3.5-turbo by a large margin.
Prompting Performance The few-shot examples are selected randomly from the training anno-
tations for all types of CoT. Among the methods using GPT-3.5.turbo, natural language is generally
better than comment-describing program, self-describing program, and non-describing program,
11https://platform.openai.com/docs/models/gpt-3-5
7

--- PAGE 8 ---
Preprint.
while self-describing program in Python is better on GSM8K and SVAMP. We attribute the low
performance to the limited availability of programs in the pre-training data of GPT-3 (Brown et al.,
2020), which appears to be true for most existing LLMs (Touvron et al., 2023a; Taylor et al., 2022;
Touvron et al., 2023b). Furthermore, it is challenging to generalize to new problems with just a few
examples of in-context learning. While ongoing research addresses these challenges (Min et al.,
2022; Hao et al., 2022; Coda-Forno et al., 2023), our work does not focus on this aspect. The
CoTs of SDP in Python are more similar to the programming codes in the pre-training corpus , and
thus the use of them leads to better performance on GSM8K and SVAMP. For the nosier dataset,
MATHQA, natural language CoTs tend to make guesses on multi-choice questions, even if they are
incorrect, whereas program CoTs tend to choose no decision if there is no valid answer available.
Program and Natural Language Comparison In general, the performance with all types of pro-
gram CoTs is consistently better than that with natural language CoTs. This superiority is particu-
larly evident in the case of M ATHQA where the program CoTs, in combination with reranking, lead
to performance improvement exceeding 10points for the 6.7B model compared to natural language
CoTs. This is because the answers are in multiple-choice format in M ATHQA, and thus inaccurate
predictions for which the program execution results are “ null-result ” can be easily filtered out before
performing majority voting or reranking12. Table 3 presents the percentage of null-result answers
in M ATHQA predictions. Although natural language CoTs produce fewer null-result answers, their
performance with null-result answers is worse, as shown in Table 4. Therefore, it is essential to
Null-result Answer (%)
6.7B 30B
NL 00.53 02 .27
SDP 34.87 33 .12
CDP 34.73 32 .78
Table 3: Percentage of null-result an-
swers in MathQA Wolfram predictions.Range 6.7B 30B
Overall 59.5 69 .4
0∼20 73 .9 81 .2
20∼40 58 .8 54 .5
40∼60 58 .3 62 .1
60∼80 40 .3 55 .8
80∼100 35 .7 46 .3
Table 4: NL majority voting accuracy against
percentage of null-result answers in CDP.
remove those samples as voting/re-ranking could be misled by them. Conversely, natural language
CoTs tend to choose an answer (e.g., A,B), regardless of the accuracy of the CoT, because the CoT
cannot be executed as a program. However, there are exceptions where we use the non-describing
program. For example, the performance of “majority voting + NDP” with Python using the 6.7B
model is worse than the natural language counterpart on SVAMP. The same observation also ap-
plies to the GSM8K dataset with the 30B model for both Wolfram and Python languages. Without
natural language, NDP has a weaker language understanding capability compared to the SDP and
CDP.
Program CoT Comparison Under both the majority voting and reranking strategies, self-
describing program consistently achieve the best performance, followed by the comment-describing
program, and then non-describing program. Unlike in SFT, self-describing program provides more
diversity and therefore tends to perform better in voting and re-ranking. Notably, the 30B model
with Python, “reranking + SDP”, achieves the best performance on GSM8K and SVAMP. The
performance is also 2.9points and 8.6points higher than the best prompting approach with GPT-
3.5-turbo on GSM8K and SVAMP, respectively.“reranking + SDP” with Wolfram also obtains the
best performance on the noisy M ATHQA dataset, with + 28points improvement over GPT-3.5-turbo
prompting. Though the performance with CDP is worse than SDP, we can see that the best CDP
methods can still outperform the best GPT-3.5-turbo prompting approach on all datasets.
Programming Language Comparison The best-performing 6.7B and 30B models are often the
methods in Python, as shown in Table 2. The only exception is that the best 30B model with Python
falls 0.5points behind the best 30B model with Wolfram. For non-describing and self-describing
12We would see many predictions give “ null” as the voting/reranking answer if we did not remove them.
8

--- PAGE 9 ---
Preprint.
programs, the use of Python often outperforms the use of Wolfram. For comment-describing pro-
gram, the methods using Python and Wolfram have comparable performance, with the 6.7B model
using Wolfram having better performance on SV AMP.
6 A NALYSIS
6.1 N UMBER OF INSTANCES FOR SAMPLING
We measure the effect of the number of sampled instances Kduring majority voting. We vary the
number Kfrom 1to100and evaluate the accuracies for the 6.7B and 30B models. Figure 3 provides
the results on GSM8K. The performance of all methods improves rapidly with an increase of Kand
becomes stable when Kis more than 30. Specifically, both CDP and NDP require a smaller number
ofKcompared to SDP and NL. The results indicate that CDP and NDP are more deterministic
while SDP and NL are more diverse. With more diverse CoTs, SDP and NL are able to gain more
improvements with more samples in majority voting.
0 20 40 60 80 100455055606570Voting Acc.NL
NDP
SDP
CDP
0 20 40 60 80 100506070Voting Acc.NL
NDP
SDP
CDP
Figure 3: Majority voting regarding the different number of sampled instances (Left: 6.7B; Right:
30B). We just depict the performance in Python for illustration purposes.
6.2 R EPRESENTATION SAMPLING STATISTICS
Table 5 reports the results of model predictions on GSM8K, including the percentage of syntacti-
cally correct predictions (i.e., execution rate), the percentage of correct answers (i.e., precision) and
the chance of obtaining at least one correct answer among 100samples (i.e., correct@100). Here,
syntactically correct means that, we can extract or execute the CoT to get a valid answer (e.g., A,B,
C, orDletter for M ATHQA, and numeric value for GSM8K and SVAMP).
It can be seen that NL CoT has a high correct@100 and execution rate but with the lowest precision
compared to all other CoT types. This is probably because the natural langauge syntax is straightfor-
ward and it is challenging for the models on the current scale to perform precise calculations without
the help of a computational engine. It is noteworthy that CDP usually has the highest precision and
execution rate, and relatively high correct@100 score, and SDP has the lowest execution rate but
the highest correct@100 score and relatively high precision. The results further support our hypoth-
esis that CDP is more deterministic and precise, while SDP has a higher level of diversity , and
thus a higher chance of obtaining correct answers with the risk of making more errors. Therefore,
we conclude that having a balance of diversity and precision is crucial for higher performance in
voting and reranking. The execution rates of CDP and NDP are similar, but CDP scores higher in
correct@100 and achieves significantly better precision. Such an observation indicates the benefits
of including natural language comments.
6.3 U PPER BOUNDS
We analyze the results in reranking to explore the potential of the CoT designs (NL, CDP/SDP in
Wolfram/Python). We calculate the accuracy when anyof the CoT is correct, which is considered
9

--- PAGE 10 ---
Preprint.
Program CoT Type Size Correct@100 Precision (%) Executable (%)
- Natural Language 6.7B 86.9 41 .5 99 .3
WolframComment-Describing Program 6.7B 78.5 58 .7 99 .6
Self-Describing Program 6.7B 82.8 54 .7 94 .1
Non-Describing Program 6.7B 69.6 52 .1 99 .3
PythonComment-Describing Program 6.7B 78.8 55 .9 98 .6
Self-Describing Program 6.7B 83.6 56 .7 96 .2
Non-Describing Program 6.7B 70.9 55 .3 99 .6
- Natural Language 30B 94.3 58 .0 98 .8
WolframComment-Describing Program 30B 85.0 67 .7 99 .6
Self-Describing Program 30B 91.1 65 .1 97 .8
Non-Describing Program 30B 76.1 62 .0 99 .6
PythonComment-Describing Program 30B 86.1 68 .1 99 .0
Self-Describing Program 30B 91.0 67 .6 98 .1
Non-Describing Program 30B 82.6 65 .0 99 .8
Table 5: Sampling statistics on GSM8K dataset.
the upper bound of all types of CoTs. We consider the best performance of individual types of CoTs
and the upper bounds of all types of CoTs in Table 6. We find that the upper bounds of the CoTs on
30B models are 98.8%,93.0%,95.0% on GSM8K, M ATHQA, SVAMP, respectively. It indicates
the potential for combining the CoTs to create a more accurate representation. We leave this as
future work.
Size GSM8K MathQA SV AMP
Best performance of individual CoT 6.7B72.4
(Python SDP)75.2
(Python SDP)78.6
(Python SDP)
Upper bound of alltypes of CoTs 6.7B 89.5 90 .1 91 .0
Best performance of individual CoT 30B80.9
(Python SDP)78.6
(Wolfram SDP)87.0
(Python SDP)
Upper bound of alltypes of CoTs 30B 98.8 93 .0 95 .0
Table 6: The best performance of individual types of CoTs, and the upper bounds of all types of
CoTs (if any of the CoT is correct).
NL SDP CDP
Correct InstancesNL SDP CDPWrong Instances- 11.37% 12.05%
7.13% - 5.69%
9.10% 6.97% -
NL SDP CDP
Correct InstancesNL SDP CDPWrong Instances- 14.77% 12.96%
10.47% - 2.62%
13.15% 7.10% -
NL SDP CDP
Correct InstancesNL SDP CDPWrong Instances- 15.30% 11.30%
5.30% - 3.60%
6.80% 9.10% -
Figure 4: The percentage of failure cases that are correctly predicted in different CoT types.
We also analyze the results of 30B reward model reranking by comparing different CoT types on
the same example (Figure 4). Though SDP has overall better performance, a non-negligible amount
of failure is correctly solved by CDP or NL. The same observation applies to the failure cases of
CDP or NL, too. The above results show that CDP, SDP, and NL have distinct advantages for
math-problem-solving. We conduct another experiment using a method that treats all three types
of CoTs equally during majority voting and reranking. For reranking, we train a reward model that
is capable of distinguishing and ranking the three types of CoTs. The number of sampled CoT
solutions is set to 100for fair comparison. Specifically, we perform majority voting and reranking
10

--- PAGE 11 ---
Preprint.
on100solutions that contain three types of CoT, in which (CDP, SDP are in Wolfram. Table 7
shows the comparison between the synthetic results and the previous best performance in Table 2.
The significant improvements suggest that there is still a large potential for a better CoT design that
integrates the strengths of all three CoT types.
Size Method GSM8K MathQA SV AMP
6.7BMajority V oting (Best CoT) 61.3 72 .7 72 .9
Majority V oting (NL + SDP + CDP) 67.4(+6.3)76.0(+3.3)76.0(+3.1)
Reranking (Best CoT) 71.4 73 .8 77 .3
Reranking (NL + SDP + CDP) 75.4(+4.0)79.0(+5.2)77.0(−0.3)
30BMajority V oting (Best CoT) 69.8 77 .9 79 .2
Majority V oting (NL + SDP + CDP) 77.5(+7.7)82.6(+4.7)81.1(+1.9)
Reranking (Best CoT) 79.9 78 .6 83 .4
Reranking (NL + SDP + CDP) 83.5(+3.6)83.5(+4.9)83.9(+0.5)
Table 7: Performance of synthesizing CDP, SDP, and NL CoT types in Wolfram.
7 R ELATED WORK
Mathematical reasoning through CoT prompting (Wei et al., 2022b), on large language models (Wei
et al., 2022a), has experienced significant development in recent years, as evidenced by a large
number of CoT methods proposed. Among them, Uesato et al. (2022) applied the process-based
andoutcome-based reward to score the natural language CoTs on GSM8K(Cobbe et al., 2021),
greatly improving problem-solving effectiveness. Lightman et al. (2023) enhanced the capability
of process-based reward model and achieve significant improvements on the challenging MATH
dataset (Hendrycks et al., 2021). Furthermore, recent research efforts extended simple natural
language CoTs, encompassing various approaches designed to enhance and optimize prompting
performance. Specifically, Fu et al. (2023) introduced the concept of complexity-based prompts ,
showing that LLMs favor long reasoning chain, which often leads to superior performance. More-
over, the methods proposed by Zhou et al. (2023b) and Khot et al. (2023) make decomposition of
problems into a series of simpler and managable questions. Similarly, Nye et al. (2021) presented
the “ Scratchpad ” concept, designed to explicitly present the intermediate calculations to the large
language model. Although these advancements are significant, ensuring the correctness of CoTs
remains a challenge.
The deterministic nature of programs is increasingly attracting the attention of researchers who
use program-aided methods for math problem solving. Imani et al. (2023) developed a strategy
that ensures answer consistency between programmatic and natural language reasoning, thereby
enhancing reliability. In similar pursuits, both Chen et al. (2022) and Gao et al. (2023) proposed the
use of Python programs as prompts. By offloading execution tasks to the Python interpreter, they
were able to mitigate issues related to incorrect reasoning or calculations. The programs employed in
these approaches are similar to our self-describing programs, where variables are represented using
natural language. Zhou et al. (2023a) further combined the natural language and program by making
use of the code interpreter in GPT-4 (OpenAI, 2023). Concurrently, research by Drori et al. (2022)
and Li et al. (2022) demonstrated the effectiveness of generating purely symbolic Python programs
to address MATH questions (Hendrycks et al., 2021) in programming competitions. He-Yueya et al.
(2023) enabled declarative reasoning in a program by embedding symbolic expressions into natural
language prompts. In response to the diversity of program CoT types, our work aims to provide
a comprehensive analysis and comparison of the representations. Our objective is to uncover their
distinctive characteristics and potential advantages.
8 C ONCLUSION
We have conducted a comprehensive study of chain-of-thought design for math problem solving, in-
cluding natural language and program CoTs. We categorize the program CoTs into non-describing
program, self-describing program, and comment-describing program. Through extensive experi-
ments on GSM8K, M ATHQA and SVAMP, we find that the self-describing program often achieves
11

--- PAGE 12 ---
Preprint.
the best performance and outperforms the few-shot prompting by GPT-3.5-turbo. It is better to use
program CoTs than natural language CoTs for math problem solving. Self-describing program and
comment-describing program perform better than non-describing program. Among the first two,
self-describing program works better than comment-describing program. The program CoTs in
Python work better than the program CoTs in Wolfram. We hope our experimental findings will
provide valuable insights for the future design of chain-of-thought in math problem solving.
REFERENCES
David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for boltzmann
machines. Cognitive science , 9(1):147–169, 1985. URL https://www.cs.toronto.edu/
˜hinton/absps/cogscibm.pdf .
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh
Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based for-
malisms. In Proceedings of NAACL , 2019. URL https://arxiv.org/abs/1905.13319 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models
are few-shot learners. In Proceedings of NeurIPS , 2020. URL https://arxiv.org/abs/
2005.14165 .
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt-
ing: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588 , 2022. URL https://arxiv.org/abs/2211.12588 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168 , 2021. URL https://arxiv.
org/abs/2110.14168 .
Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane X Wang, and Eric Schulz.
Meta-in-context learning in large language models. arXiv preprint arXiv:2305.12907 , 2023. URL
https://arxiv.org/abs/2305.12907 .
Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin
Liu, Linda Chen, Sunny Tran, Newman Cheng, et al. A neural network solves, explains,
and generates university math problems by program synthesis and few-shot learning at human
level. Proceedings of the National Academy of Sciences , 119(32):e2123433119, 2022. URL
https://arxiv.org/abs/2112.15594 .
Jessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language generation.
InProceedings of EMNLP , 2017. URL https://aclanthology.org/W17-4912 .
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting
for multi-step reasoning. In Proceedings of ICLR , 2023. URL https://arxiv.org/abs/
2210.00720 .
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and
Graham Neubig. PAL: Program-aided language models. In Proceedings of ICML , 2023. URL
https://arxiv.org/abs/2211.10435 .
Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-
shot learners. In Proceedings of ACL , 2021. URL https://aclanthology.org/2021.
acl-long.295/ .
Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam
Fazel-Zarandi, and Asli Celikyilmaz. Roscoe: A suite of metrics for scoring step-by-step reason-
ing. In Proceedings of ICLR , 2022. URL https://arxiv.org/abs/2212.07919 .
Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting:
Scaling in-context learning to 1,000 examples. arXiv preprint arXiv:2212.06713 , 2022. URL
https://arxiv.org/abs/2212.06713 .
12

--- PAGE 13 ---
Preprint.
Joy He-Yueya, Gabriel Poesia, Rose E Wang, and Noah D Goodman. Solving math word problems
by combining language models with symbolic solvers. arXiv preprint arXiv:2304.09102 , 2023.
URLhttps://arxiv.org/abs/2304.09102 .
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.
InProceedings of Thirty-fifth Conference on Neural Information Processing Systems Datasets
and Benchmarks Track (Round 2) , 2021. URL https://arxiv.org/abs/2103.03874 .
Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large
language models. arXiv preprint arXiv:2303.05398 , 2023. URL https://arxiv.org/abs/
2303.05398 .
Zhanming Jie, Jierui Li, and Wei Lu. Learning to reason deductively: Math word problem solving
as complex relation extraction. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pp. 5944–5955, 2022.
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish
Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In Proceed-
ings of ICLR , 2023. URL https://arxiv.org/abs/2210.02406 .
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R ´emi Leblond, Tom
Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation
with alphacode. Science , 378(6624):1092–1097, 2022. URL https://arxiv.org/abs/
2203.07814 .
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint
arXiv:2305.20050 , 2023. URL https://arxiv.org/abs/2305.20050 .
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What
makes good in-context examples for gpt- 3? InProceedings of Deep Learning Inside Out (DeeLIO
2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architec-
tures , 2022. URL https://aclanthology.org/2022.deelio-1.10/ .
Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for
mathematical reasoning. In Proceedings of ACL , 2023. URL https://arxiv.org/abs/
2212.10535 .
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in
context. In Proceedings of NAACL , 2022. URL https://arxiv.org/abs/2110.15943 .
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,
David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show
your work: Scratchpads for intermediate computation with language models. arXiv preprint
arXiv:2112.00114 , 2021. URL https://arxiv.org/abs/2112.00114 .
OpenAI. GPT-4 technical report, 2023. URL https://arxiv.org/abs/2303.08774 .
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math
word problems? In Proceedings of NAACL , 2021. URL https://arxiv.org/abs/2103.
07191 .
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagn ´e, Alexandra Sasha Luccioni, Franc ¸ois Yvon, Matthias Gall ´e, et al. Bloom: A 176b-
parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.
URL https://arxiv.org/abs/2211.05100 .
Timo Schick, Jane Dwivedi-Yu, Roberto Dess `ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,
Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to
use tools. arXiv preprint arXiv:2302.04761 , 2023. URL https://arxiv.org/abs/2302.
04761 .
13

--- PAGE 14 ---
Preprint.
Minghuan Tan, Lei Wang, Lingxiao Jiang, and Jing Jiang. Investigating math word problems using
pretrained multilingual language models. In Proceedings of the 1st Workshop on Mathematical
Natural Language Processing (MathNLP) , pp. 7–16, 2022.
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,
Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for
science. arXiv preprint arXiv:2211.09085 , 2022. URL https://arxiv.org/abs/2211.
09085 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023a. URL https:
//arxiv.org/abs/2302.13971 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foun-
dation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b. URL https:
//arxiv.org/abs/2307.09288 .
Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia
Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and
outcome-based feedback. arXiv preprint arXiv:2211.14275 , 2022. URL https://arxiv.
org/abs/2211.14275 .
Guido Van Rossum and Fred L Drake Jr. Python reference manual . Centrum voor Wiskunde en
Informatica Amsterdam, 1995.
Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R
Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scien-
tific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635 , 2023a.
URL https://arxiv.org/abs/2307.10635 .
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language
models. In Proceedings of ICLR , 2023b. URL https://arxiv.org/abs/2203.11171 .
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large
language models. Transactions on Machine Learning Research , 8 2022a. URL https:
//arxiv.org/abs/2206.07682 .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Proceed-
ings of NeurIPS , 2022b. URL https://arxiv.org/abs/2201.11903 .
Stephen Wolfram. An Elementary Introduction to the Wolfram Language . Wolfram
Media, Inc.; 3rd edition, 2015. URL https://www.wolfram.com/language/
elementary-introduction/3rd-ed/ .
Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia,
Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code
interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921 , 2023a. URL
https://arxiv.org/pdf/2308.07921.pdf .
Denny Zhou, Nathanael Sch ¨arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schu-
urmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables com-
plex reasoning in large language models. In Proceedings of ICLR , 2023b. URL https:
//arxiv.org/abs/2205.10625 .
14

--- PAGE 15 ---
Preprint.
A H YPERPARAMETERS
Hyperparam Supervised Fine-tuning Reward Modeling
(6B/30B) ( 6B/30B)
Framework Megatron-Deepspeed Pytorch
GPUs 16 A100 8 A100
Maximum sequence length 1024 700
Learning rate 1e−5/2e−61e−6
Batch size 48 48
Warmup Steps 10% 10%
Weight Decay 0.01 0 .01
Epoch 40 3
Learning Rate Decay Linear Linear
Adam ϵ 1e−6 1 e−6
Adam β1 0.9 0 .9
Adam β2 0.98 0 .98
Gradient Clipping 1.0 1 .0
Table 8: Hyperparameters for Supervised Fine-Tuning and Reward Modeling for 6B and 30B pa-
rameters model scale.
B D ATASET PROCESSING
We preprocess the original MathQA (Amini et al., 2019) dataset13to filter out some invalid instances
that contains incorrect answers (Tan et al., 2022; Jie et al., 2022). For example, some of the annotated
equations do not lead to the correct answer in MathQA. For SV AMP, the training set comes from
the original implementation (Patel et al., 2021)14.
C RM-W EIGHTED VOTING
Program Method Size GSM8K MathQA SV AMP
- SFT + RM-Weighted V oting + Natural Language 6.7B 58.0 64 .0 67 .7
WolframSFT + RM-Weighted V oting + Non-Describing Program 6.7B 59.4 71 .0 65 .9
SFT + RM-Weighted V oting + Self-Describing Program 6.7B 68.2 73 .1 75 .9
SFT + RM-Weighted V oting + Comment-Describing Program 6.7B 68.8 71 .6 73 .7
PythonSFT + RM-Weighted V oting + Non-Describing Program 6.7B 63.1 70 .7 68 .8
SFT + RM-Weighted V oting + Self-Describing Program 6.7B 70.6 74 .8 78 .2
SFT + RM-Weighted V oting + Comment-Describing Program 6.7B 67.6 71 .5 69 .3
- SFT + RM-Weighted V oting + Natural Language 30B 71.2 72 .8 77 .0
WolframSFT + RM-Weighted V oting + Non-Describing Program 30B 71.4 72 .9 76 .8
SFT + RM-Weighted V oting + Self-Describing Program 30B 76.8 78 .3 82 .5
SFT + RM-Weighted V oting + Comment-Describing Program 30B 75.9 74 .0 81 .4
PythonSFT + RM-Weighted V oting + Non-Describing Program 30B 74.4 73 .0 78 .4
SFT + RM-Weighted V oting + Self-Describing Program 30B 79.5 77 .9 86 .0
SFT + RM-Weighted V oting + Comment-Describing Program 30B 77.8 75 .1 81 .2
Table 9: Performance of majority voting weighted by reward model.
13https://huggingface.co/datasets/math_qa
14https://github.com/arkilpatel/SVAMP
15
