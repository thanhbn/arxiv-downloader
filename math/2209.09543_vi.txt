# FACT: Học các Khái niệm Trừu tượng Điều khiển Đằng sau Dãy Số Nguyên

Peter Belcak
ETH Zürich
8092 Zürich, Switzerland
belcak@ethz.ch

Ard Kastrati
ETH Zürich
8092 Zürich, Switzerland
kard@ethz.ch

Flavio Schenker
ETH Zürich
8092 Zürich, Switzerland
flaviosc@ethz.ch

Roger Wattenhofer
ETH Zürich
8092 Zürich, Switzerland
wattenhofer@ethz.ch

## Tóm tắt

Dãy số nguyên có tầm quan trọng trung tâm trong việc mô hình hóa các khái niệm cho phép mô tả hữu hạn hoàn chỉnh. Chúng tôi giới thiệu một góc nhìn mới về việc học các khái niệm như vậy và đặt ra một bộ nhiệm vụ đánh giá nhằm vào sự hiểu biết khái niệm bởi các mô hình học máy. Những nhiệm vụ này đánh giá gián tiếp khả năng trừu tượng hóa của mô hình và thách thức chúng suy luận cả về mặt nội suy và ngoại suy từ kiến thức thu được bằng cách quan sát các ví dụ đại diện. Để hỗ trợ thêm nghiên cứu trong biểu diễn kiến thức và suy luận, chúng tôi trình bày FACT, Bộ công cụ Hiểu biết Trừu tượng Hữu hạn. Bộ công cụ này bao gồm một tập dữ liệu lớn các dãy số nguyên gồm cả các mục tự nhiên và tổng hợp, một thư viện cho việc tiền xử lý và tạo dữ liệu, một bộ công cụ đánh giá hiệu suất mô hình, và một tập hợp các triển khai mô hình cơ sở, cho phép thực hiện các tiến bộ tương lai một cách dễ dàng.

## 1 Giới thiệu

Danh sách có thứ tự của các số nguyên là dạng biểu diễn tự nhiên cho tất cả các khái niệm trừu tượng rời rạc cơ bản. Chúng xuất hiện khi gặp phải sự tiến hóa của các hiện tượng thời gian rời rạc, đối xứng hữu hạn của các mẫu hình trực quan, hoặc tiến trình thuật toán, nơi chúng mô tả sự phát triển của các trạng thái liên tiếp của một hệ thống, tự đồng cấu của R2, hoặc danh sách chương trình, tương ứng. Dãy số nguyên cũng là lựa chọn biểu diễn khi tuyến tính hóa thông tin có cấu trúc để phân tích, nén dữ liệu và truyền thông, với các điểm dữ liệu thường xuất hiện có xu hướng được mã hóa theo dạng đơn giản nhất hoặc ngắn nhất. Chứng minh cho tính hữu ích của chúng trong việc biểu diễn chính xác các khái niệm trừu tượng, các nhiệm vụ hoàn thành và ngoại suy trên dãy số nguyên là một phần thường xuyên trong kiểm tra trí tuệ và năng khiếu chung của con người ([42, 31]).

Mục tiêu và hy vọng của các mô hình học máy là xác định các khái niệm trừu tượng phổ quát đơn giản giải thích dữ liệu huấn luyện, thay vì ghi nhớ vô số các lớp nhỏ của các mẫu và nội suy khi được cho đầu vào chưa thấy trước đó. Việc khám phá và nội hóa các khái niệm điều khiển, hay đơn giản là việc học các quy tắc cơ bản, do đó nằm ở trung tâm của nghiên cứu trí tuệ nhân tạo.

Chúng tôi lưu ý rằng nhiều khái niệm có thể được biểu diễn duy nhất bởi một dãy số nguyên một cách tự nhiên (ví dụ: các bình phương của số tự nhiên xác định đa thức n²; 1,2,3; 3,1,2; 2,3,1; 1,3,2; 3,2,1; 2,1,3 mã hóa các đối xứng của tam giác), trong khi các khái niệm khác (chẳng hạn như quay vật thể trong cảnh) đòi hỏi không gian liên tục để có mô tả đúng, có thể mở rộng. Chúng tôi nhận ra dãy số nguyên như một dạng chung để mô tả các khái niệm có thể biểu diễn hoàn toàn với độ chính xác hữu hạn (hữu hạn về bản chất; hữu hạn) và đặt chúng vào trung tâm nghiên cứu của chúng tôi.

Để phân biệt việc học và hiểu các khái niệm trừu tượng rời rạc này khỏi việc học biểu diễn của chúng trong các phương thức khác nhau, chúng tôi giới thiệu một tập dữ liệu phong phú của dãy số nguyên cùng với một tập hợp các nhiệm vụ tương ứng vốn liên quan đến dãy số nguyên và phù hợp để đánh giá mức độ hiểu biết giống con người được thể hiện bởi các mô hình học máy. Tập trung chỉ vào dãy số nguyên, chúng tôi do đó tách các khái niệm đang được học ra khỏi hầu như tất cả độ phức tạp xuất phát từ việc học biểu diễn, làm cho quá trình học ít tốn tài nguyên hơn và việc diễn giải kết quả đánh giá trở nên đơn giản hơn. Một ví dụ về chương trình học tạo ra sự phân biệt này và tiến tới các khái niệm trừu tượng cấp cao được minh họa trong Hình 1.

Các phương pháp học máy hiện đại đã được chứng minh là có khả năng hiểu (hoặc ít nhất là khớp mẫu) các khái niệm phức tạp xuất hiện trong các phương thức dữ liệu khác nhau, đặc biệt là bằng cách sử dụng học sâu để xây dựng các biểu diễn thông tin của các thực thể được nghiên cứu. Mặc dù làm việc với dãy số, chúng tôi lùi bước khỏi hồi quy tượng trưng (vốn từ trước đến nay chi phối khái niệm hiểu biết trong lĩnh vực này) và, theo xu hướng, thay vào đó sử dụng một cách tiếp cận đa diện trong đó các dãy được đặc trưng bởi tính chất và mối quan hệ của chúng với các dãy khác, thay vì bằng các công thức tượng trưng giải thích dễ hiểu hơn đối với con người. Chúng tôi mở rộng về mối quan hệ của công trình chúng tôi với hồi quy tượng trưng trong Phụ lục F.

Nhằm vào việc hiểu các khái niệm trừu tượng đằng sau các biểu diễn cụ thể của hiện tượng hữu hạn, chúng tôi mở khóa một chế độ mới để đánh giá chất lượng của các khái niệm trừu tượng đã học. Một phỏng đoán, hoặc một ước tính, về quy tắc đằng sau một dãy số nguyên, từ đó dẫn đến dự đoán chính xác các phần tử của dãy trên các đầu vào chưa thấy trước đó, có thể nói là mong muốn hơn một ước tính chỉ mô tả dãy tốt cho các đầu vào đã biết trong huấn luyện. Bản chất thuật toán cơ bản của bài toán học các khái niệm trừu tượng hữu hạn do đó làm cho bài toán tổng quát hóa ngoại suy được xác định rõ ràng, và cho phép chúng tôi xem xét hiệu suất tổng quát hóa ngoại suy như một tiêu chí để đánh giá mô hình.

Đóng góp của chúng tôi là:
• việc giới thiệu một tập dữ liệu lớn các dãy số nguyên bao gồm dữ liệu từ cả nguồn tự nhiên và tổng hợp và được tuyển chọn để sử dụng tiếp theo trong các nhiệm vụ thách thức các mô hình phát triển hiểu biết về các khái niệm xác định dữ liệu (Mục 2, [5]),
• bổ sung cho điều trên, một thư viện tiện ích (FACTLIB [6]) để xử lý và tạo dữ liệu dãy số nguyên,
• việc giới thiệu nhiều loại nhiệm vụ được thiết kế để đánh giá sự hiểu biết của mô hình về các mẫu khái niệm trong dãy số nguyên với thứ tự khó khăn được thiết lập rõ ràng (Mục 3),
• một tập hợp các chỉ số đánh giá được điều chỉnh cho các nhiệm vụ trên để đánh giá hiệu suất mô hình một cách thích hợp và theo dõi tiến bộ trong lĩnh vực con này của biểu diễn kiến thức và suy luận, và
• một tập hợp các mô hình cơ sở, cả cổ điển và neural, được triển khai để tạo điều kiện thử nghiệm liền mạch (Mục 4, [4]).

## 2 Tập dữ liệu

Như một phần của FACT, chúng tôi giới thiệu một tập dữ liệu bao gồm hơn 3,6 triệu dãy số nguyên. Điểm khởi đầu tạo cấu trúc cho tập dữ liệu là dữ liệu được cung cấp bởi Bách khoa toàn thư Trực tuyến về Dãy Số Nguyên ([39]). OEIS là một tài liệu tham khảo toàn diện phát triển tự nhiên về các dãy số nguyên đáng chú ý, được biên soạn qua nhiều thập kỷ để hỗ trợ công việc trong các khoa học toán học. Chúng tôi đã xem xét dữ liệu OEIS⁴, tách ra một tập hợp con phù hợp gồm 341.000 mục, và xử lý nó cụ thể để sử dụng trong học máy, phù hợp với các yêu cầu giấy phép. Mỗi mục của tập dữ liệu hiện được chú thích bởi tối đa 18 đặc trưng truyền đạt thông tin về bản chất, tính chất và mục đích của dãy. Trong Hình 4, chúng tôi đưa ra tổng quan về kết quả của bước xử lý này. Một cuộc thảo luận đầy đủ về công việc tuyển chọn, tinh chỉnh và chú thích tự động mở rộng được thực hiện có thể tìm thấy trong Phụ lục A.

Với các mục bách khoa toàn thư nhằm vào người đọc là con người, chúng tôi quan sát thấy rằng nhiều mục chỉ bao phủ các danh mục tương ứng rất thưa thớt, dựa vào các mô tả ngôn ngữ tự nhiên liên quan và khả năng trừu tượng hóa của con người để tạo ra kết nối phân loại. Các thí nghiệm ban đầu của chúng tôi với các mô hình cơ sở (cf. Mục 4) tiếp tục xác nhận rằng phần lớn dữ liệu không đạt được khối lượng thông tin quan trọng cần thiết để sử dụng đáng tin cậy trong các ứng dụng học máy. Do đó chúng tôi mở rộng tập dữ liệu một cách có hệ thống bằng các nhánh dãy được tạo tổng hợp trong khi tuân theo cấu trúc và bản chất của các mục bách khoa toàn thư gốc và cung cấp các chú thích tự động được thiết kế cẩn thận bất cứ khi nào có thể.

### 2.1 Tạo Tổng hợp

Nguồn cảm hứng chính của chúng tôi là các khái niệm về độ phức tạp Kolmogorov và xác suất Solomonoff [33,20,41]. Bắt đầu ở cấp độ danh mục (cf. Hình 4), chúng tôi định nghĩa một ngữ pháp phi ngữ cảnh Gc cho mỗi danh mục c, và sau đó sử dụng Gc để tạo ra các công thức ngày càng dài hơn, được sử dụng để tạo ra các dãy. Điều này được thực hiện tuân theo khái niệm rằng độ dài tăng của công thức phản ánh chính nó trong việc tăng độ phức tạp của quy tắc và do đó các dãy được tạo. Một ví dụ về ngữ pháp, được sử dụng để sản xuất các công thức đa thức, được đưa ra trong Hình 3.

Đối với mỗi danh mục, tổng cộng 500K dãy tổng hợp đã được tạo ra. Độ dài của các công thức được sử dụng để tạo ra các dãy này được tăng liên tục theo lịch trình logarit, do đó ưu tiên các công thức ngắn hơn trong khi vẫn đảm bảo sự hiện diện của các dãy từ các công thức dài hơn. Chúng tôi đưa ra tất cả chi tiết của quy trình tạo cho mỗi danh mục dãy trong Phụ lục B.

Kết quả kết hợp của quá trình tuyển chọn OEIS và mở rộng tập dữ liệu do đó là một tập dữ liệu lớn được tích hợp liền mạch vào FACT và dễ dàng mở rộng bởi FACTLIB, nếu được yêu cầu bởi các nhiệm vụ phức tạp hơn hoặc ứng dụng lớn hơn.

[Hình 2: Các danh mục trong tập dữ liệu FACT. Nó được tạo thành từ các mục tổng hợp và OEIS. Mỗi nhóm trong phần tổng hợp gồm 500.000 dãy, trong khi kích thước của các nhóm OEIS khác nhau. Các vùng chấm biểu diễn các danh mục chính được xác định trong tập dữ liệu. Hình elip định nghĩa các danh mục con từ bước xử lý của chúng tôi trong OEIS. Các chấm đỏ đánh dấu các nhóm được bổ sung với dữ liệu tổng hợp (và được sử dụng trong thiết lập đánh giá của chúng tôi).]

[Hình 3: Ngữ pháp phi ngữ cảnh được sử dụng để tổng hợp các công thức dùng để tạo dãy Đa thức. NConst biểu thị một hằng số, T biểu thị một số hạng, và N biểu thị non-terminal gốc cho biểu thức đa thức.]

## 3 Đánh giá

Dựa trên tập dữ liệu của phần trước, chúng tôi đề xuất một bộ nhiệm vụ đánh giá và chỉ số đánh giá để đánh giá một loạt rộng các phương pháp về khả năng hiểu các khái niệm điều khiển đằng sau sự tiến hóa của dãy số nguyên.

### 3.1 Động lực

Xem xét đoạn ban đầu 0,2,4,6,8,... của một dãy và giả sử chúng ta được giao nhiệm vụ đề xuất các ứng cử viên hợp lý cho số tiếp theo sau 8.

Chúng ta có thể làm thế nào để đánh giá chất lượng của các gợi ý của mình?

Theo tinh thần của hồi quy tượng trưng, chúng ta có thể chọn khăng khăng rằng phải có một công thức duy nhất tạo ra các thành viên của dãy theo thứ tự. Nhưng, ngay cả dưới điều kiện như vậy, đối với mỗi số nguyên tiếp tục được đề xuất, tồn tại một đa thức bậc 8 chứa dãy tiếp tục. Điều này mặc dù thực tế là con người sẽ trực quan có khả năng đề xuất 10 như một sự tiếp tục hợp lý, thậm chí có thể biện minh bằng quan sát rằng 5 phần tử đầu của dãy tuân theo mẫu (2n)n≥0.

Như được mô tả trong Mục 2, do đó chúng tôi đã thiết lập các phương pháp tạo của phần tổng hợp trong tập dữ liệu của chúng tôi để áp dụng nguyên tắc tiết kiệm một cách rõ ràng bằng cách thay đổi độ dài của các công thức tạo để kiểm soát độ phức tạp của các dãy kết quả. Dữ liệu được tạo theo cách này được đảm bảo bao gồm các dãy đến từ các quy tắc có mức độ phức tạp khác nhau theo thiết kế, thay vì bằng cơ hội.

Tuy nhiên, việc tập trung vào việc ưu tiên các quy tắc ngắn hơn so với các quy tắc dài hơn sẽ quá nhân tạo nếu chỉ sử dụng một mình. Xem xét đoạn ban đầu 1,1,2,3,5,.... Mặc dù có xu hướng nhanh chóng khẳng định rằng các số đến từ dãy Fibonacci và 8 nên theo sau, một câu trả lời xuất hiện tự nhiên hơn trong bối cảnh hóa học là 9, vì dãy tiếp tục biểu diễn số lượng tất cả các ankan carbon-n có thể.

Do đó, vượt xa hồi quy tượng trưng dưới dao cạo Occam, chúng tôi thêm một phần đã xử lý của tập dữ liệu OEIS vào các quy trình đánh giá của chúng tôi như một chỉ báo về sức hấp dẫn của các dãy trên nhiều ngành khoa học. Lưu ý rằng đây là một cải tiến được thực hiện thêm vào thay vì trái ngược với các nguyên tắc tạo dãy tổng hợp, vì nhiều dãy thế giới thực được thu thập trong OEIS thực sự tuân theo các quy tắc đơn giản.

### 3.2 Cấu trúc

Như được thúc đẩy trong Mục 3.1, đối với mỗi nhiệm vụ đánh giá, chúng tôi cung cấp hai bộ đánh giá: một cho dữ liệu tổng hợp và một cho dữ liệu OEIS. Đánh giá của chúng tôi do đó thúc đẩy việc thiết kế các mô hình học máy xác định các khái niệm và quy tắc đơn giản (đánh giá trên dữ liệu tổng hợp) nhưng vẫn giữ đủ tính tổng quát xuyên lĩnh vực để có tác động thực tế trong các ngành khác nhau (đánh giá trên OEIS).

[Hình 4: Các nhiệm vụ của chúng tôi được sắp xếp theo mức độ khó khăn trên hai chiều: loại và phạm vi.] Đánh giá bao gồm các nhiệm vụ với thứ tự khó khăn được thiết lập trên hai chiều: loại nhiệm vụ và phạm vi. Chúng tôi phân biệt giữa các nhiệm vụ phân loại dãy, tương tự dãy, dự đoán phần dãy tiếp theo, tiếp tục dãy, và bỏ ẩn dãy - mỗi cái được chi tiết dưới đây.

Ngoài ra, chúng tôi thực hiện mỗi nhiệm vụ đến mức hai phạm vi khác nhau: trong và xuyên danh mục. Trường hợp thực hiện nhiệm vụ trong danh mục là thiết lập đơn giản hơn trong hai thiết lập, vì danh mục mà dãy xuất phát được biết trước, và thông tin này do đó cũng có sẵn vào thời điểm thiết kế mô hình. Tuy nhiên, các mô hình cơ sở của chúng tôi (cf. Mục 4) không biết thông tin này và thay vào đó xử lý phạm vi danh mục như thể không biết gì về dữ liệu.

Trong thiết lập đánh giá cụ thể của chúng tôi, tập dữ liệu được chia thành bốn phần, cụ thể là các bộ huấn luyện, xác thực, kiểm tra tổng hợp và kiểm tra hữu cơ. Các bộ huấn luyện và xác thực chỉ bao gồm các dãy tổng hợp. Tỷ lệ kích thước giữa các tập dữ liệu huấn luyện, xác thực, kiểm tra tổng hợp và kiểm tra OEIS là 9:1:1:1. Để thuận tiện và tham khảo tương lai, chúng tôi cung cấp dữ liệu được chia này dưới dạng các tập dữ liệu riêng biệt. Tuy nhiên, tập dữ liệu FACT cũng có sẵn trong một phần, cho phép người dùng chọn phân chia riêng của họ hoặc bổ sung dữ liệu riêng được tạo bởi FACTLIB, theo nhu cầu của họ.

### 3.3 Loại Nhiệm vụ

Trong phần này chúng tôi trình bày 5 loại nhiệm vụ theo thứ tự khó khăn, mà chúng tôi lấy cận trên về độ khó của các thể hiện nhiệm vụ tạo thành nhiệm vụ đã cho. Ví dụ: Mọi thể hiện của tiếp tục dãy thuộc về tập hợp các thể hiện bỏ ẩn dãy. Nhưng, đối với mọi thể hiện của tiếp tục dãy (ngoại trừ thể hiện tầm thường nơi chúng ta chỉ bắt đầu với một số), có một thể hiện bỏ ẩn khó hơn. Thể hiện như vậy có thể được hình thành bằng cách tiếp tục yêu cầu bỏ ẩn một phần tử dãy ở đâu đó trong đoạn ban đầu được cung cấp để tiếp tục. Do đó, cận trên về độ khó của bỏ ẩn dãy cao hơn nghiêm ngặt so với cận trên về độ khó của tiếp tục dãy.

#### 3.3.1 Phân loại Dãy

Loại nhiệm vụ đơn giản nhất trong đánh giá của chúng tôi là phân loại dãy thuộc danh mục nào. Mục tiêu chính của nhiệm vụ này là đánh giá liệu các mô hình có thể phân biệt và xác định các mẫu chung trong dãy, cả xuyên và trong các danh mục khác nhau. Lưu ý rằng thành viên danh mục có thể không nhất thiết phải duy nhất. Ví dụ, một dãy có thể bị giới hạn, nhưng cũng có thể chu kỳ. Đối với nhiệm vụ này, chúng tôi sử dụng tất cả các danh mục có đối tác tổng hợp trong tập dữ liệu của chúng tôi. Mỗi mẫu bao gồm một mảng các số nguyên cho lớp dãy đã cho. Chúng tôi phân biệt giữa hai loại con nhiệm vụ và đưa ra mục tiêu của chúng:

• Một-so-với-Còn lại (OvR). Mục tiêu để xác định liệu dãy có thuộc một danh mục được chỉ định hay không. Đối với trường hợp này, chúng tôi cung cấp một tập dữ liệu cân bằng trong mỗi danh mục. Đây là một nhiệm vụ phân loại nhị phân, và như vậy, chúng tôi sử dụng độ chính xác làm chỉ số đánh giá.

• Phân loại đa lớp. Mục tiêu để dự đoán, đối với mỗi dãy, tất cả các danh mục mà nó thuộc về. Hiệu suất được đo bằng điểm F1 trung bình macro (tức là trung bình của các điểm F1 riêng lẻ theo lớp) do sự mất cân bằng vốn có trong tập dữ liệu của chúng tôi.

#### 3.3.2 Tương tự Dãy

Nhiệm vụ tương tự nhằm đánh giá khả năng của mô hình để biểu diễn các dãy theo cách phản ánh sự tương tự của chúng về loại (ví dụ: đồng ý về thành viên danh mục) hoặc tính chất (chẳng hạn như chu kỳ hoặc không bị giới hạn) theo tinh thần của [24]. Mục tiêu là nhúng các dãy vào một không gian nhúng sao cho các dãy thuộc cùng danh mục gần nhau hơn so với các dãy của các danh mục mà chúng không thuộc về. Chúng tôi đánh giá bằng cách sử dụng

• điểm Recall@k, trong đó k ứng cử viên cho một dãy s được đề xuất bằng cách lấy mẫu k dãy từ mỗi danh mục và sau đó sắp xếp các nhãn danh mục theo khoảng cách của các điểm mang từ s; và

• lỗi bình phương trung bình gốc top-k - lỗi bình phương trung bình gốc (RMSE) trên k ứng cử viên tương tự hàng đầu. Cho k dự đoán của một mô hình f {ŷᵢ}ᵢ∈{1,...,k} và sự thật cơ bản y, chúng tôi định nghĩa RMSE top-k là minᵢ∈{1,...,k} RMSE(y, ŷᵢ). Nói cách khác, cho tất cả các dự đoán được tạo, chúng tôi báo cáo RMSE của dự đoán gần nhất với sự thật cơ bản.

Lựa chọn chỉ số đánh giá của chúng tôi được dựa trên quan sát rằng các dãy được tạo từ các quy tắc đơn giản tương tự thường cuối cùng phân kỳ, và ở mức độ lớn. Nhiệm vụ này tổng quát hóa phân loại dãy.

#### 3.3.3 Dự đoán Phần-Dãy Tiếp theo

Như thấy trong xử lý ngôn ngữ tự nhiên [10], việc yêu cầu một mô hình quyết định liệu hai câu có theo sau nhau hay không có thể có lợi cho việc kiểm tra liệu một mô hình có hiểu đầu vào của nó hay không. Cho hai dãy con liền kề s₁ và s₂, mục tiêu của nhiệm vụ dự đoán phần-dãy tiếp theo (NSPP) là xác định liệu dãy con s₂ có phải là sự tiếp tục hợp lệ của s₁ hay không. Chúng tôi tạo ra một tập dữ liệu cân bằng cho nhiệm vụ này, bằng cách sử dụng tất cả các danh mục trong tập dữ liệu của chúng tôi có đối tác tổng hợp. NSPP sau đó đơn giản là một nhiệm vụ phân loại nhị phân và hiệu suất được đo bằng độ chính xác dự đoán. Nhiệm vụ này khó hơn nghiêm ngặt so với nhiệm vụ tương tự, vì nó đòi hỏi mô hình không chỉ hiểu các tính chất chính của dãy mà còn có khả năng phân biệt các kết hợp không chắc chắn hoặc không khả thi của các phần dãy khỏi những kết hợp khả thi.

#### 3.3.4 Tiếp tục Dãy

Loại nhiệm vụ thứ tư trong hệ thống phân cấp khó khăn của chúng tôi là đề xuất mục tiếp theo trong một dãy s đã cho. Nhiệm vụ này có tính chất ngoại suy và nhằm thách thức hiểu biết của mô hình vượt ra ngoài việc đưa ra quyết định nhị phân giữa các gợi ý được cung cấp từ bên ngoài. Như vậy, nhiệm vụ này đòi hỏi hiểu biết tốt hơn về các quy tắc điều khiển dãy so với dự đoán phần-dãy tiếp theo - do đó vị trí của nó ở trên NSPP trong hệ thống phân cấp khó khăn. Chúng tôi phân biệt hai loại con của nhiệm vụ này, cụ thể là tiếp tục một lần và nhiều lần, chỉ khác nhau ở số lượng ứng cử viên mà mô hình được mong đợi cung cấp cho sự tiếp tục. Chúng tôi sử dụng lỗi logarit bình phương trung bình gốc (RMSLE) và RMSE top-k cho mỗi loại con tương ứng.

#### 3.3.5 Bỏ ẩn Phần tử Dãy

Ở đỉnh của hệ thống phân cấp phức tạp của chúng tôi là nhiệm vụ bỏ ẩn các phần tử được đánh dấu của một dãy được cung cấp. Nhiệm vụ tiếp tục dãy có thể được xem như một trường hợp đặc biệt của bỏ ẩn, nơi chỉ phần tử cuối cùng bị ẩn. Chúng tôi chỉ xem xét bỏ ẩn nhiều lần và chọn RMSE top-k làm chỉ số đánh giá.

## 4 Hiệu suất mô hình cơ sở

Chúng tôi chạy các thí nghiệm mở rộng trên đánh giá được đề xuất như một điểm khởi đầu cho nghiên cứu tiếp theo. Chúng tôi xem xét các phương pháp học máy cổ điển cũng như các mạng neural lớn. Các thí nghiệm trong phần này làm nổi bật tính khả thi của việc học nhiều mẫu khác nhau trong dãy số nguyên, nhưng cũng một số hạn chế của các phương pháp hiện có.

### 4.1 Mô hình

Để cung cấp cơ sở cho hiệu suất mô hình trên các nhiệm vụ trên, chúng tôi sử dụng tổng cộng 24 mô hình khác nhau trên các nhiệm vụ đánh giá của chúng tôi, cụ thể là 4 mô hình neural (mạng dày đặc, tái phát và tích chập, và transformer), 9 bộ phân loại cổ điển (k-nearest neighbors, Gaussian naive Bayes, máy vector hỗ trợ tuyến tính, cây quyết định, rừng ngẫu nhiên, gradient boosting, AdaBoost, XG Boost, bộ phân loại giả), và 11 bộ hồi quy tiêu chuẩn (k-nearest neighbors, bộ hồi quy tuyến tính, bộ hồi quy ridge, bộ hồi quy lasso, Elastic Net, cây quyết định đơn, rừng ngẫu nhiên, gradient boosting, AdaBoost, bộ hồi quy giả).

Chúng tôi đưa ra chi tiết về triển khai và cài đặt siêu tham số của chúng trong Phụ lục C.

### 4.2 Kết quả

Một tổng quan đơn giản về hiệu suất của các mô hình cơ sở có thể tìm thấy trong Bảng 1. Một danh sách chi tiết toàn diện về kết quả của chúng tôi, bao gồm kết quả đánh giá ở cấp độ danh mục, có thể tìm thấy trong Phụ lục D.

### 4.3 Diễn giải Chỉ số

Điểm F1 trung bình macro cho nhiệm vụ phân loại dãy trong Bảng 1 gợi ý rằng ngay cả các mô hình có hiệu suất tốt nhất cũng có điểm trung bình chỉ hơn 0,5 một chút. Điểm F1 của 0,5 có thể đạt được theo nhiều cách, nhưng ví dụ sẽ tương ứng với hiệu suất precision-recall của 0,5×0,5.

Trong dự đoán phần-dãy tiếp theo, các RNN dường như đang tiến gần đến điểm độ chính xác hoàn hảo 1,0, mặc dù vẫn còn một số chỗ để cải thiện trong trường hợp tập dữ liệu hữu cơ. Lỗi logarit bình phương trung bình gốc 0,578 (hiệu suất OEIS tốt nhất) và 0,395 (hiệu suất tổng hợp tốt nhất) xuất hiện trong kết quả cho nhiệm vụ tiếp tục tương ứng với sự khác biệt đồng nhất của logarit cùng cường độ. Ngược lại, hai hiệu suất tệ nhất tương ứng trong số các mô hình cơ sở tương ứng với sự khác biệt đồng nhất giữa logarit của 0,923; 0,877.

RMSE bình phương trung bình gốc top-5 dễ diễn giải hơn. Người ta sẽ đạt được RMSE giữa các đoạn dãy ban đầu s₁, s₂ nếu s₁ lớn hơn hoặc nhỏ hơn s₂ chính xác 5 trong mỗi phần tử của nó. RMSE top-5 của 0,267; 0,270 cho nhiệm vụ tương tự dãy do đó tương ứng với sự khác biệt đồng nhất trung bình có cùng cường độ trong mỗi phần tử khi so sánh dãy thực với khớp tốt nhất từ 5 kết quả hàng đầu của tìm kiếm tương tự. Cùng chỉ số được sử dụng cho nhiệm vụ bỏ ẩn. Dưới ánh sáng của sự phân bố các phần tử của các dãy được xem xét bao phủ các giá trị từ 0 đến vài triệu, hiệu suất của các mô hình cơ sở, đặc biệt trong nhiệm vụ tương tự, dường như mạnh đáng kể.

### 4.4 Phân tích So sánh

Xem xét kết quả của Bảng 1 và tiếp tục Phụ lục D, chúng tôi lưu ý rằng ngay cả hiệu suất của các mô hình cơ sở trên tập dữ liệu tổng hợp cũng thường khá mạnh về mặt tuyệt đối. Chúng tôi cũng nhận thấy một xu hướng chung trong tất cả các mô hình để thực hiện tốt hơn trên dữ liệu tổng hợp so với các bộ OEIS hữu cơ. Điều này không bất ngờ, vì dữ liệu OEIS rất đa dạng và đến từ nhiều nguồn khác nhau, trong khi dữ liệu tổng hợp được tạo theo quy trình nghiêm ngặt, đồng nhất, do đó có phân bố đều đặn hơn. Tuy nhiên, chúng tôi quan sát thấy rằng huấn luyện chỉ trên dữ liệu tổng hợp vẫn mang lại hiệu suất vững chắc trên tập dữ liệu hữu cơ trên tất cả các mô hình.

Trong nhiệm vụ phân loại, RNN và rừng ngẫu nhiên đạt được kết quả tốt nhất trên tất cả các danh mục. Không ngạc nhiên, RNN xác định rất chính xác các dãy bị giới hạn và tăng, trong khi rừng ngẫu nhiên dẫn đầu cho các bộ modulo, prime, exponential và trigonometric. RNN cũng cho thấy sự dẫn đầu nhất quán cho dự đoán phần-dãy tiếp theo. Transformer và CNN thống trị kết quả cho tương tự dưới độ chính xác top-k, ngoại trừ các hàm chu kỳ hữu cơ, được xử lý tốt nhất bởi mạng tái phát. Chỉ có Transformer dẫn đầu trong cùng nhiệm vụ khi đánh giá bằng RMLE top-k, và tương tự cho mạng tích chập trong nhiệm vụ bỏ ẩn.

Hiệu suất tốt nhất cho nhiệm vụ tiếp tục gần như được chia đều giữa mạng tái phát và transformer, nơi RNN dẫn đầu cho các dãy polynomial, exponential, trigonometric và chu kỳ (transformer là người thực hiện tệ nhất). Transformer mang lại kết quả tốt nhất modulo, prime, bị giới hạn, tăng và tất cả các dãy cộng lại.

Chúng tôi đặt ra kỳ vọng của chúng tôi về hiệu suất mô hình trên đánh giá này, cũng liên quan đến khả năng của con người, trong Phụ lục E.

## 5 Công trình Liên quan

Gần đây đã có một phong trào đáng chú ý trong nghiên cứu mạng neural hướng tới việc hiểu cách DNN học trừu tượng hóa. Về phía điều tra, các kiến trúc truyền thống được phân tích bởi [21] về mặt sự xuất hiện của kiến thức trên mạng, một chỉ số được xác định rõ ràng cho khả năng tổng quát hóa của mạng neural được giới thiệu trong [11], và một phương pháp để đánh giá biểu diễn kiến thức trong mạng neural sâu được huấn luyện để nhận dạng đối tượng trong thị giác máy tính được đề xuất trong [18]. Sự quan tâm tăng đối với việc học các khái niệm trừu tượng cũng thúc đẩy việc kết hợp các thiên hướng quy nạp liên quan vào các kiến trúc neural sâu và chương trình huấn luyện, như đã thấy trong thu nhận khái niệm [46,14], với việc giới thiệu máy trạng thái neural [16] trong thị giác máy tính, và phân tích trừu tượng nhân quả [2,3,13] trong suy luận ngôn ngữ tự nhiên. Hơn nữa, các nỗ lực đã bắt đầu đánh giá khả năng của các mô hình để thực hiện suy luận trực quan trừu tượng [47, 1, 48, 7].

Một thể hiện chung, cổ điển và vẫn đầy thách thức của việc học trừu tượng hóa trong bối cảnh dãy số là nhiệm vụ hồi quy tượng trưng. Một số mô hình lập trình di truyền và tập dữ liệu có mục đích cụ thể đã được đề xuất trong lĩnh vực này trong hơn 30 năm tồn tại của nó, và một đánh giá có hệ thống, SRBench [22], được giới thiệu gần đây. Nó kết hợp 130 tập dữ liệu số nhỏ hơn, cả phát triển tự nhiên và tạo tổng hợp, với PMLB [26], và đi kèm với đánh giá một loạt mô hình hồi quy tượng trưng sử dụng chỉ số mới được đề xuất. Trọng tâm vốn có của nhiệm vụ vào khả năng diễn giải làm cho nó phù hợp cho việc sử dụng công nghiệp nhưng dẫn đến thách thức trong việc xác định các khái niệm tạo sinh thịnh hành, vì hai dãy xuất phát từ hai thể hiện khác biệt của cùng một quy tắc có thể được khớp tốt nhất bởi hai công thức hoàn toàn khác nhau về bản chất.

Tập trung vào dãy số nguyên, Bách khoa toàn thư Trực tuyến về Dãy Số Nguyên (OEIS) được trình bày trong [39]. Các mục của bách khoa toàn thư đến từ cả những người đóng góp cá nhân và các cơ chế tự động để phát minh ra các dãy "thú vị" [8]. Nó được sử dụng để phân loại dãy kết hợp heuristic và các phương pháp học máy trong [45], cho nhiệm vụ tiếp tục dãy bởi mạng neural được kết nối đầy đủ trong [30], cho hồi quy số hạng dãy cấp chữ số để làm nổi bật các giới hạn tính toán của mạng neural trong [25], và để học các tính chất toán học của số nguyên để sử dụng trong xử lý ngôn ngữ tự nhiên bằng cách huấn luyện các embedding dãy-OEIS trong [32]. Phiên bản mới nhất, OEISv4, là nguồn thông tin chú thích toàn diện nhất về dãy số nguyên, chứa hơn 300.000 mục. Tập dữ liệu tiếp tục được sử dụng trong lĩnh vực con mới nổi của hồi quy tượng trưng sâu [23, 29, 9, 19].

Kinh nghiệm của chúng tôi cho thấy rằng dữ liệu OEIS quá thưa thớt và được điều chỉnh quá gần với nhu cầu của người đọc là con người để hữu ích cho việc huấn luyện các mô hình học máy để hiểu dãy số nguyên. Tuy nhiên, nó vẫn có thể phục vụ như một đại diện thú vị của "tính hữu ích" (như trong [8]) trong đánh giá mô hình khi được lọc và tiền xử lý thích hợp cho mục đích đó.

## 6 Hướng cho Công việc Tương lai

Lợi thế mang lại của việc tập trung vào dãy số nguyên thay vì các phương thức đầu vào khác - có khả năng phong phú hơn - là chúng ta có thể diễn giải trực tiếp điểm hiệu suất của các mô hình riêng lẻ như khả năng hiểu các khái niệm trừu tượng tạo dãy của chúng, và có sự tin tưởng rằng không có hiệu suất mô hình nào bị cản trở bởi sự hiểu biết không đầy đủ về biểu diễn đầu vào. Ở đây, trong khi các mô hình chúng tôi đánh giá dường như cho thấy một mức độ hiểu biết về các mẫu cơ bản của dãy số nguyên, vẫn còn chỗ để cải thiện đáng kể, đặc biệt trong phân loại đa lớp, tiếp tục dãy trên tất cả các lớp, và bỏ ẩn dãy.

Kết quả của Mục 4 đều được tạo ra cho chế độ hoạt động "tĩnh", trong đó tất cả dữ liệu truy vấn (ví dụ: dãy để phân loại hoặc tiền tố dãy để tiếp tục) được cung cấp cho mô hình ngay từ đầu và như một tổng thể. Một chế độ hoạt động có lẽ xuất hiện tự nhiên hơn trong hầu hết các tình huống thực tế là chế độ mà mô hình tích cực trong việc học và tương tác với một oracle, thăm dò thông tin cho đến khi nó tự tin rằng nó có thể cung cấp và trả lời. Các biến thể tương tác như vậy có thể được xây dựng dễ dàng cho tất cả các nhiệm vụ trong Mục 3.3, nhưng đòi hỏi một bộ chỉ số đánh giá tinh vi hơn xem xét lượng thông tin mà mô hình yêu cầu trước khi đưa ra câu trả lời. Chúng tôi tin rằng thiết lập này xứng đáng được chú ý vì nó có thể cung cấp những hiểu biết có giá trị về suy luận mô hình, và chúng tôi nhằm giải quyết nó trong công việc tương lai.

## 7 Kết luận

Dãy số nguyên thường xuất hiện như dạng biểu diễn tự nhiên cho các hiện tượng hữu hạn. Tập trung vào dãy số nguyên cho phép chúng ta giải quyết trực tiếp bài toán học các khái niệm trừu tượng và loại bỏ chi phí cần thiết khác của việc học các biểu diễn cụ thể theo phương thức.

Bộ công cụ đánh giá cho dãy số nguyên được trình bày trong công trình này theo thiết kế có tính tổng quát trong mục đích, nhằm vào hiểu biết cơ bản do việc sử dụng số nguyên như biểu diễn nguyên thủy, và bao gồm theo thứ bậc nhiều nhiệm vụ trước đây xuất hiện riêng lẻ trong tài liệu.

Hy vọng của chúng tôi là công trình của chúng tôi sẽ giúp thu hút sự chú ý đến những thách thức của việc thiết kế các mô hình nhận thức mối quan hệ logic cai trị các tập corpus huấn luyện và suy luận trong quá trình suy luận, do đó giúp tạo điều kiện cho những tiến bộ tương lai trên biên giới của trí tuệ nhân tạo tổng quát.

## Tài liệu tham khảo

[1] David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoning in neural networks. In International conference on machine learning, pages 511–520. PMLR, 2018.

[2] Sander Beckers and Joseph Y Halpern. Abstracting causal models. In Proceedings of the aaai conference on artificial intelligence, volume 33, pages 2678–2685, 2019.

[3] Sander Beckers, Frederick Eberhardt, and Joseph Y Halpern. Approximate causal abstractions. In Uncertainty in Artificial Intelligence, pages 606–615. PMLR, 2020.

[4] Belcak, Peter, Kastrati, Ard, and Schenker, Flavio. Fact benchmarking - the benchmarking baseline models of the finitary abstraction comprehension toolkit, 2022. URL https://doi.org/10.3929/ethz-b-000565644.

[5] Belcak, Peter, Kastrati, Ard, and Schenker, Flavio. Fact dataset - the dataset of the finitary abstraction comprehension toolkit, 2022. URL https://doi.org/10.3929/ETHZ-B-000562705.

[6] Belcak, Peter, Kastrati, Ard, and Schenker, Flavio. Factlib - the library of the finitary abstraction comprehension toolkit, 2022. URL https://doi.org/10.3929/ethz-b-000565638.

[7] François Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.

[8] Simon Colton, Alan Bundy, and Toby Walsh. Automatic invention of integer sequences. In AAAI/IAAI, pages 558–563, 2000.

[9] Stéphane d'Ascoli, Pierre-Alexandre Kamienny, Guillaume Lample, and François Charton. Deep symbolic regression for recurrent sequences. arXiv preprint arXiv:2201.04600, 2022.

[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[11] Alex Gain and Hava Siegelmann. Abstraction mechanisms predict generalization in deep neural networks. In International Conference on Machine Learning, pages 3357–3366. PMLR, 2020.

[12] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018.

[13] Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural networks. Advances in Neural Information Processing Systems, 34, 2021.

[14] Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P Burgess, Matko Bosnjak, Murray Shanahan, Matthew Botvinick, Demis Hassabis, and Alexander Lerchner. Scan: Learning hierarchical compositional visual concepts. arXiv preprint arXiv:1707.03389, 2017.

[15] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9: 1735–80, 12 1997. doi: 10.1162/neco.1997.9.8.1735.

[16] Drew Hudson and Christopher D Manning. Learning by abstraction: The neural state machine. Advances in Neural Information Processing Systems, 32, 2019.

[17] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019.

[18] Roman Ilin, Thomas Watson, and Robert Kozma. Abstraction hierarchy in deep learning neural networks. In 2017 International Joint Conference on Neural Networks (IJCNN), pages 768–774. IEEE, 2017.

[19] Pierre-Alexandre Kamienny, Stéphane d'Ascoli, Guillaume Lample, and François Charton. End-to-end symbolic regression with transformers. arXiv preprint arXiv:2204.10532, 2022.

[20] Andrei N Kolmogorov. Three approaches to the quantitative definition of information'. Problems of information transmission, 1(1):1–7, 1965.

[21] Robert Kozma, Roman Ilin, and Hava T Siegelmann. Evolution of abstraction across layers in deep learning neural networks. Procedia computer science, 144:203–213, 2018.

[22] William La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabrício Olivetti de França, Marco Virgolin, Ying Jin, Michael Kommenda, and Jason H Moore. Contemporary symbolic regression methods and their relative performance. arXiv preprint arXiv:2107.14351, 2021.

[23] Guillaume Lample and François Charton. Deep learning for symbolic mathematics. arXiv preprint arXiv:1912.01412, 2019.

[24] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.

[25] Hyoungwook Nam, Segwang Kim, and Kyomin Jung. Number sequence prediction problems for evaluating computational powers of neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 4626–4633, 2019.

[26] Randal S Olson, William La Cava, Patryk Orzechowski, Ryan J Urbanowicz, and Jason H Moore. Pmlb: a large benchmark suite for machine learning evaluation and comparison. BioData mining, 10(1):1–13, 2017.

[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.

[28] Roger Penrose. The Emperor's New Mind: Concerning Computers, Minds, and the Laws of Physics. Viking Penguin, 1990. ISBN 0140145346.

[29] Brenden K Petersen, Mikel Landajuela Larma, T Nathan Mundhenk, Claudio P Santiago, Soo K Kim, and Joanne T Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. arXiv preprint arXiv:1912.04871, 2019.

[30] Marco Ragni and Andreas Klein. Predicting numbers: an ai approach to solving number series. In Annual Conference on Artificial Intelligence, pages 255–259. Springer, 2011.

[31] Kenneth A Russell and Philip J Carter. The Times book of IQ tests, volume 3. Kogan Page Publishers, 2003.

[32] Maria Ryskina and Kevin Knight. Learning mathematical properties of integers. arXiv preprint arXiv:2109.07230, 2021.

[33] Jürgen Schmidhuber. Discovering neural nets with low kolmogorov complexity and high generalization capability. Neural Networks, 10(5):857–873, 1997. ISSN 0893-6080. doi: https://doi.org/10.1016/S0893-6080(96)00127-X. URL https://www.sciencedirect.com/science/article/pii/S089360809600127X.

[34] Laurence Sigler. Fibonacci's Liber Abaci: a translation into modern English of Leonardo Pisano's book of calculation. Springer Science & Business Media, 2003.

[35] Neil J. A. Sloane. The on-line encyclopedia of integer sequences, . URL https://oeis.org/.

[36] Neil J. A. Sloane. Oeis keywords, . URL https://oeis.org/wiki/Keywords.

[37] Neil J. A. Sloane. Style sheet for contributers., . URL https://oeis.org/wiki/Style_Sheet.

[38] Neil J. A. Sloane. A Handbook of Integer Sequences. Academic Press, 1973. ISBN 0-12-648550-X.

[39] Neil J. A. Sloane. The on-line encyclopedia of integer sequences. In Towards mechanized mathematical assistants, pages 130–130. Springer, 2007.

[40] Neil J. A. Sloane and S. Plouffe. The Encyclopedia of Integer Sequences. Academic Press, 1995. ISBN 0-12-558630-2.

[41] Ray Solomonoff. The application of algorithmic probability to problems in artificial intelligence. In Laveen N. KANAL and John F. LEMMER, editors, Uncertainty in Artificial Intelligence, volume 4 of Machine Intelligence and Pattern Recognition, pages 473–491. North-Holland, 1986. doi: https://doi.org/10.1016/B978-0-444-70058-2.50040-1. URL https://www.sciencedirect.com/science/article/pii/B9780444700582500401.

[42] Claes Strannegård, Mehrdad Amirghasemi, and Simon Ulfsbäcker. An anthropomorphic method for number sequence problems. Cognitive Systems Research, 22:27–34, 2013.

[43] SymPy Development Team. Sympy. URL https://www.sympy.org/en/index.html.

[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706.03762.

[45] Chai Wah Wu. Can machine learning identify interesting mathematics? an exploration using empirically observed laws. arXiv preprint arXiv:1805.07431, 2018.

[46] Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, and Anton Van Den Hengel. What value do explicit high level concepts have in vision to language problems? In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 203–212, 2016.

[47] Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational and analogical visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5317–5327, 2019.

[48] Wenhe Zhang, Chi Zhang, Yixin Zhu, and Song-Chun Zhu. Machine number sense: A dataset of visual arithmetic problems for abstract and relational reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 1332–1340, 2020.

## Lời cảm ơn

Chúng tôi cảm ơn Emanuel Jampen. Công việc của anh ấy về xử lý ban đầu tập dữ liệu OEIS đã cho thấy tính khả thi của dự án này. Chúng tôi cũng cảm ơn Neil Sloane vì đã thành lập dự án OEIS và chia sẻ nó trong miền công cộng.

## A Xử lý OEIS

Bách khoa toàn thư Trực tuyến về Dãy Số Nguyên [35], viết tắt là OEIS, là một tập dữ liệu trực tuyến về dãy số nguyên. Được thành lập năm 1996 bởi Neil Sloane, tập dữ liệu kể từ đó được mở rộng liên tục bởi các nhà toán học nghiệp dư và chuyên nghiệp. Trong 18 cột, tập dữ liệu cung cấp thông tin về các tính chất khác nhau của những dãy đó. 18 tên cột được giải thích ngắn gọn trong Phụ lục A. Chúng tôi làm việc với phiên bản ngoại tuyến của tập dữ liệu OEIS có chứa 342.304 dãy. Về mặt nội dung, chúng có thể từ những dãy nổi tiếng như A000045, Số Fibonacci: F(n) = F(n-1) + F(n-2) với F(0) = 0 và F(1) = 1, qua những dãy đơn giản như A000004: dãy zero và những dãy phức tạp như A339123: số lượng đa đồ thị 2-kết nối với n cạnh và có gốc tại hai đỉnh không thể phân biệt và không có phân tách thành các thành phần song song có gốc tại hai đỉnh được phân biệt, đến những dãy khá sáng tạo như A001049: các điểm dừng được đánh số tại Manhattan trên tàu điện ngầm Lexington Avenue. Một tóm tắt chi tiết có thể tìm thấy trong [39].

### Tên Trường

Trong phần này chúng tôi đưa ra giải thích ngắn gọn về 18 trường trong tập dữ liệu OEIS. Điều này dựa trên trang tính phong cách được cung cấp bởi OEIS [37].

• **oeis_id**: Một số 6 chữ số duy nhất đi trước bởi một "A". Ví dụ A005735

• **identification**: Điều này đề cập đến ID mà dãy đã cho có trong một trong các cuốn sách A Handbook of Integer Sequences [38] (M theo sau bởi số 4 chữ số) hoặc The Encyclopedia of Integer Sequences [40] (N theo sau bởi số 4 chữ số).

• **value_list**: Một danh sách các số nguyên được phân tách bằng dấu phẩy. Dãy thực tế được quan tâm. Tùy thuộc vào dãy được đề cập, độ dài của danh sách này có thể thay đổi theo một số bậc cường độ. Ví dụ A058445 chỉ chứa một phần tử trong khi danh sách giá trị của A175320 có độ dài 1.578.727.

• **name**: Một giải thích ngắn gọn về dãy. Đôi khi, khi có thể, điều này đã chứa một công thức dễ sử dụng để tạo dãy. Ví dụ A005843 có tên "Các số chẵn không âm: a(n) = 2n".

• **comments**: Chi tiết chung thêm và ghi chú bên về dãy sẽ làm cho tên quá dài. Ở đây chúng ta thường có thể tìm thấy các công thức thay thế để tạo dãy hoặc các nơi khác nhau trong toàn bộ toán học nơi dãy này xuất hiện. Ví dụ một trong những bình luận của dãy A000045, Số Fibonacci, là "Cũng là số lượng tập đỉnh độc lập và phủ đỉnh trong đồ thị đường (n-2)."

• **detailed_references**: Tham chiếu đến các bài báo tạp chí và sách không thể được liên kết trong trường "links".

• **links**: Tham chiếu đến tài liệu có thể truy cập trực tuyến.

• **formulas**: Hàm tạo, công thức đóng và các phương pháp khác để tính dãy.

• **examples**: Ví dụ về cách tìm một số hạng của dãy và cách diễn giải giá trị của nó.

• **maple_programs**: Chương trình viết bằng maple để tạo các phần tử của dãy.

• **mathematica_programs**: Chương trình viết bằng mathematica để tạo các phần tử của dãy.

• **other_programs**: Chương trình viết bằng các ngôn ngữ lập trình khác ngoài maple hoặc mathematica (ví dụ python) để tạo các phần tử của dãy.

• **cross_references**: Tham chiếu đến các dãy khác trong tập dữ liệu có liên quan theo một cách nào đó.

• **keywords**: Từ khóa từ một tập hợp ngắn các khả năng. Ví dụ nonn được sử dụng cho các dãy không có giá trị âm hiện tại trong trường value_list tương ứng của chúng.

• **offset_a**: Chỉ số của phần tử đầu tiên trong value_list. Ví dụ A005843, dãy "Các số chẵn không âm: a(n) = 2n" đã đề cập, có offset_a là 0 vì phần tử đầu tiên được tính bởi a(0) = 2×0, tức là chỉ số là 0.

• **offset_b**: Chỉ số của phần tử đầu tiên có giá trị tuyệt đối lớn hơn 1.

• **author**: Tên của người đóng góp ban đầu và ngày đóng góp đầu tiên.

• **extensions_and_errors**: Được sử dụng để tuyên bố quyền lợi cho các bổ sung vào mục không thể được công nhận đúng cách trong các trường khác.

### A.1 Đặc điểm

Trong phần này, chúng tôi giới thiệu một số đặc điểm khác của tập dữ liệu đòi hỏi tiền xử lý và mở rộng.

• **Giá trị NULL**: Không phải mọi dãy đều chứa thông tin trong mọi cột trong 18 cột. Bảng 2 cho thấy số lượng mục hợp lệ mỗi cột trong tập dữ liệu. Chúng tôi nói một mục hợp lệ nếu nó không phải NULL. Khi lập trình các phương pháp phân loại, những mục không hợp lệ này phải được xem xét. Tất nhiên điều này có thể là một hạn chế đối với tính có nghĩa của các phương pháp phân loại như vậy. Ví dụ chúng ta không thể kiểm tra liệu trường formulas có chứa công thức giống Fibonacci hay không khi nó là NULL. Tuy nhiên, thực tế là trường là NULL cũng có thể là một gợi ý rằng không có công thức đóng nào được biết (nếu không ai đó có lẽ sẽ đóng góp nó cho OEIS).

• **Độ dài dãy**: Vì việc tính toán các phần tử cho một số dãy khó hơn so với những dãy khác, độ dài của value_list cho thấy phương sai cao. Hình 5 cho thấy phân bố độ dài dãy trên hai tỷ lệ khác nhau. Như chúng ta có thể thấy có một số lượng đáng kể các dãy có độ dài ngắn (giả sử ít hơn 30). Độ dài dãy ngắn có thể gây ra vấn đề cho các phương pháp phân loại cần nhiều phần tử hơn cho một số tính toán toán học. Ví dụ chúng ta có thể khớp một đa thức bậc 20 với 21 điểm dữ liệu, nhưng sau đó chúng ta không thể xác thực kết quả trên các điểm dữ liệu bổ sung.

[Hình 5: Phân bố độ dài dãy. Trái. Trục ngang tương ứng với độ dài dãy, trong khi trục dọc có thang logarit và đưa ra số lượng các dãy có độ dài như vậy. Phải. Cùng dữ liệu như bên trái nhưng với vai trò của các trục được trao đổi.]

• **Số lượng từ khóa**: Bảng 3 cho thấy số lần xuất hiện của mỗi từ khóa trong 27 từ khóa có thể. Một lần nữa, để có mô tả đầy đủ về ý nghĩa của mỗi từ khóa, xem trang Wiki tương ứng từ OEIS [36].

[Bảng 2: Số lượng mục Hợp lệ và Không hợp lệ mỗi trường]

[Bảng 3: Số lần xuất hiện của mỗi từ khóa]

### A.2 Phương pháp Xử lý

Để nhóm các dãy tương tự từ OEIS với nhau, chúng tôi tạo các danh mục và quyết định cho mỗi dãy liệu chúng có thuộc danh mục này hay không. Vì đối với một số dãy không rõ ràng ngay lập tức liệu chúng có thuộc một danh mục nhất định hay không, chúng tôi nới lỏng yêu cầu của phân loại nhị phân này thành một phân loại được phân cấp tốt hơn. Để làm điều này, chúng tôi có 5 mức độ thành viên khác nhau, được biểu diễn bởi các số nguyên từ 0 đến 4. Những giá trị này cho biết mức độ tin tưởng của phân loại của chúng tôi, như được thấy trong bảng sau:

0 Có khả năng không thuộc danh mục này
1 Có khả năng cao hơn là không thuộc danh mục này
2 Không kết luận được
3 Có khả năng cao hơn là thuộc danh mục này
4 Có khả năng thuộc danh mục này

Trong chương này chúng tôi giới thiệu và giải thích các phương pháp chúng tôi đã sử dụng để định nghĩa các danh mục và tạo các nhãn liên quan cho thành viên. Cả việc lựa chọn danh mục của chúng tôi và các Phương pháp Phân loại khác nhau đều không toàn diện, do đó chúng tôi đã triển khai một khung dễ sử dụng cho phép tạo nhanh các danh mục mới. Khung này được giải thích trong Mục A.3 sau.

### A.3 Lớp Annotator

Chúng tôi đã triển khai Lớp Annotator cho phép người dùng tạo các danh mục dãy mới. Mỗi thể hiện của Lớp Annotator chịu trách nhiệm cho một danh mục. Như được mô tả trong Hình 6, mỗi Annotator được liên kết với một Aggregator và một hoặc nhiều Phương pháp Phân loại (xem Mục A.4 và A.5 dưới đây). Annotator chịu trách nhiệm chạy tất cả các Phương pháp Phân loại của nó cho mỗi dãy, cung cấp kết quả cho Aggregator và lưu trữ kết quả thành viên 5-Mức cho việc sử dụng tương lai.

[Hình 6: Cấu trúc của Lớp Annotator với nhiều Phương pháp Phân loại và một Aggregator]

### A.4 Aggregator

Một Aggregator, như tên gợi ý, tổng hợp kết quả từ các Phương pháp Phân loại và trả về giá trị thành viên 5-Mức.

### A.5 Phương pháp Phân loại

Phương pháp Phân loại là các hàm lấy tất cả thông tin về một dãy (tức là tất cả các trường được mô tả trong Mục A) làm đầu vào và trả về kết quả của một số kiểm tra sau đó được xử lý thêm bởi Aggregator.

Lưu ý, tất cả Phương pháp Phân loại phải kiểm tra liệu các trường phải được truy cập bởi kiểm tra có tồn tại cho dãy đã cho hay không, tức là trường không trống hoặc NULL.

Như chúng ta sẽ thấy trong các phần con sau, một Phương pháp Phân loại có thể là thứ gì đó đơn giản như tìm kiếm một từ nhất định trong tên của dãy hoặc phức tạp hơn như thực hiện một số tính toán nâng cao dựa trên giá trị. Tuy nhiên, Phương pháp Phân loại nên được thiết kế để có thời gian chạy khá ngắn mỗi dãy vì cùng một hàm phải chạy trên tất cả các dãy trong tập dữ liệu. Như một quy tắc ngón tay cái, khi một kiểm tra mất một giây mỗi dãy, tổng thời gian chạy trên tập dữ liệu hiện tại sẽ là khoảng 4 ngày.

**Toán học**: Chúng tôi sử dụng Kiểm tra Toán học để kiểm tra các tính chất của giá trị của các dãy. Những phương pháp này mang một số hạn chế mà người ta cần nhận thức. Một số kiểm tra đòi hỏi số lượng giá trị tối thiểu để trả về kết quả có nghĩa. Ví dụ một dãy chỉ có 5 giá trị được cho không phù hợp để khớp một đa thức bậc 10 với nó. Mặt khác nếu dãy thực sự có dạng đa thức thì các phần tử sẽ dễ tính toán và có lẽ sẽ có nhiều giá trị được cung cấp hơn. Sau đây là mô tả các Phương pháp Phân loại toán học chúng tôi đã sử dụng.

• **Sai phân Chia Newton**: Chúng tôi sử dụng Phương pháp Sai phân Chia Newton để tính bậc mà một đa thức phải có để khớp với các giá trị của dãy. Trực quan, đây là tương tự tín hiệu rời rạc của việc lấy đạo hàm liên tiếp của một đa thức cho đến khi chúng ta kết thúc với hàm hằng số 0. Bằng cách đếm số lần lặp, chúng ta được bậc mà một đa thức khớp cần có, sau đó chúng ta có thể sử dụng cho Nội suy Đa thức.

• **Nội suy Đa thức**: Chúng tôi sử dụng thư viện SymPy [43] để thực hiện nội suy đa thức tượng trưng với bậc được tính bởi Phương pháp Sai phân Chia Newton. Điều này cho phép chúng tôi khớp một đa thức bậc n với n+1 giá trị đầu tiên của dãy và sử dụng các giá trị còn lại để tính lỗi giữa đa thức khớp và giá trị thực. Tất nhiên chúng ta cần khẳng định rằng có đủ giá trị, không được sử dụng trong quá trình nội suy, để có thể tính lỗi có nghĩa. Trong những trường hợp lỗi giữa đa thức khớp và giá trị thực là 0 và chúng ta có đủ giá trị, chúng ta kết luận rằng dãy được đề cập thực sự có dạng đa thức.

• **Hàm mũ**: Để phát hiện các dãy hàm mũ, chẳng hạn như A000244, Lũy thừa của 3, hoặc A007689, a(n) = 2^n + 3^n, trực tiếp từ giá trị aᵢ của chúng, trước tiên chúng tôi tính dãy thương q. Mỗi phần tử của q là thương của hai phần tử liên tiếp của dãy ban đầu:

qᵢ = aᵢ/aᵢ₊₁ (1)

Các dãy hàm mũ thuần túy, như Lũy thừa của 3, tạo ra q có giá trị không đổi. Trong ví dụ của chúng tôi qᵢ = 1/3 cho tất cả i. Những dãy như vậy vượt qua kiểm tra này ngay lập tức.

Các dãy bị chi phối bởi một hàm mũ như A006127, a(n) = 2^n + n, tạo ra một dãy thương tiến đến một giá trị không đổi. Trong ví dụ này chúng ta có

lim(i→∞) qᵢ = lim(i→∞) (2^i + i)/(2^(i+1) + i + 1) = 1/2 (2)

Chúng tôi ước lượng giới hạn của q bằng cách kiểm tra các giá trị cuối cùng của nó có thể được tính từ dữ liệu đã cho. Nếu 30 phần tử cuối cùng đều nằm trong một số phạm vi ngưỡng của giá trị cuối cùng, chúng tôi kết luận rằng dãy có lẽ bị chi phối bởi một hàm hàm mũ.

• **Tính nguyên tố**: Sử dụng SymPy một lần nữa, chúng tôi kiểm tra dãy nào có thể được diễn giải như việc áp dụng các lớp hàm trên vào số nguyên tố. Cách chúng tôi triển khai kiểm tra này bị giới hạn ở các số nhỏ hơn 2^64 (hoặc khoảng 10^19).

• **Tính bị chặn**: Kiểm tra giá trị tuyệt đối của các phần tử dãy để tính bị chặn bởi các giá trị khác nhau, là một loạt kiểm tra dễ dàng nhưng sâu sắc.

• **Palindrome**: Kiểm tra này kiểm tra liệu tất cả các phần tử trong dãy có phải palindromic hay không, tức là nếu tất cả giá trị là các số giống nhau khi đọc từ trái sang phải cũng như từ phải sang trái. Tầm thường điều này đúng cho tất cả các dãy chỉ chứa giá trị nhỏ hơn 10 vì tất cả các số một chữ số đều palindromic (tức là 7 là cùng một số khi đọc từ hai hướng).

Kết quả từ kiểm tra này phụ thuộc vào cơ số của hệ thống số được sử dụng để hiển thị giá trị của dãy. Ví dụ 12321 là palindromic trong cơ số 10 nhưng trong cơ số 2 thì không: 12321₁₀ = 11000000100001₂.

• **Tính chu kỳ**: Chúng tôi kiểm tra liệu một dãy có phần lặp lại lên đến độ dài chu kỳ tối đa 10.000 phần tử. Để vượt qua kiểm tra, một dãy cần có ít nhất 3 chu kỳ đầy đủ trong value_list.

**Tìm kiếm Chuỗi**: Vì hầu hết các trường trong tập dữ liệu được điền bởi văn bản thuần túy, một trong những phương pháp dễ nhất để có thông tin về một dãy là tìm kiếm các từ hoặc chuỗi cụ thể trong một trường đã cho. Phương pháp này có thể là một chỉ báo đáng tin cậy liệu một dãy có thuộc một danh mục hay không, ví dụ thực hiện tìm kiếm chuỗi từ prime trên tên của A098682, Số nguyên tố nhỏ nhất lớn hơn n*n, có thể phục vụ như một chỉ báo tốt rằng A098682 thực sự ít nhất theo một cách nào đó liên quan đến số nguyên tố. Tuy nhiên, kết luận ngược lại khó rút ra hơn, liệu một dãy thực sự không liên quan đến số nguyên tố chỉ vì nó không được đề cập trong tên hoặc bình luận?

**Biểu thức Chính quy**: Biểu thức Chính quy phục vụ như một công cụ mạnh mẽ để kiểm tra các mẫu trong dữ liệu. Ví dụ chúng tôi đã tạo biểu thức chính quy

```
a\(n\)=[0-9]*\*?a\(n[\+\-][0-9]+\)[\+\-][0-9]*\*?
a\(n[\+\-][0-9]+\)([\+\-][0-9]*\*?a\(n[\+\-][0-9]+\))*
```

Khớp các mẫu ký tự như

a(n) = 2a(n-3) + 5a(n-5) - 17a(n-5) (3)

Do sự tương tự với Dãy Fibonacci nổi tiếng (A000045), chúng tôi gọi các dãy có công thức được khớp bởi biểu thức chính quy trên là Giống-Fibonacci.

## B Tạo Tổng hợp

Trong phần này chúng tôi giải thích các bước được thực hiện để tạo các dãy tổng hợp.

Từ các danh mục được xác định (cf. hình 2) trong cơ sở dữ liệu OEIS, chúng tôi đã bầu chọn một tập hợp con quan trọng và đại diện và bổ sung các mục hữu cơ của chúng với dữ liệu tổng hợp. Đối với mỗi danh mục, tổng cộng 500K dãy tổng hợp đã được tạo. Các dãy được tạo với độ dài tăng logarit của công thức tạo. Theo cách này, hầu hết các biểu thức tạo dãy tương đối ngắn (được thúc đẩy bởi độ phức tạp Kolmogorov, cf. Mục 2), nhưng các dãy dài hơn vẫn đóng vai trò quan trọng trong tập dữ liệu. Trong phần sau, chúng tôi cung cấp thông tin về cách các phần mở rộng tổng hợp của mỗi danh mục dãy được tạo sử dụng ngữ pháp phi ngữ cảnh. Thông thường, các "logical" terminal trong ngữ pháp của chúng tôi là {Var, Const} và các non-terminal {Add, Sub, Mult, NConst, Pow}. NConst được định nghĩa như non-terminal biểu diễn các hằng số có nhiều chữ số. Lý do tại sao chúng tôi xem nó như một non-terminal là chúng tôi đếm độ dài của đa thức dựa trên số lượng non-terminal mà thôi.

**Đa thức**: Ngữ pháp phi ngữ cảnh cho đa thức được định nghĩa như:

```
T → Var | Const
N → Add | Sub | Mult | Pow | NConst | T
Add → (N + N)
Sub → (N − N)
Mult → (N*N)
Pow → (N**NConst)
NConst → (ConstPos NConst) | Const
Var → x
Const → 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
ConstPos → 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
```

**Hàm mũ**: Ngữ pháp phi ngữ cảnh của nhóm hàm mũ được định nghĩa như:

```
T → Var | Const
N → Add | Sub | Mult | Pow | NConst | T
Add → (N + N)
Sub → (N − N)
Mult → (N*N)
Pow → (N**N)
NConst → (ConstPos NConst) | Const
Var → x
Const → 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
ConstPos → 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
```

**Nguyên tố**: Trong nhóm này, chúng tôi giới thiệu một non terminal Prime(x), khi được đánh giá tại giá trị x ∈ {1,2,...} trả về số nguyên tố thứ i. Nghĩa là, Prime(1) = 2, Prime(2) = 3, Prime(3) = 5, và như vậy. Theo cách này chúng tôi tiêm vào các dãy của chúng tôi kiến thức tiên nghiệm về số nguyên tố. Cụ thể hơn, ngữ pháp phi ngữ cảnh được định nghĩa như:

```
T → Var | Const
N → Add | Sub | Mult | Pow | NConst | Prime | T
Add → (N + N)
Sub → (N − N)
Mult → (N*N)
Pow → (N**N)
NConst → (ConstPos NConst) | Const
Var → x
Prime → prime(x) # định nghĩa như một hàm
Const → 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
ConstPos → 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
```

**Chu kỳ**: Nhóm chu kỳ sử dụng các ngữ pháp trước đó, nhưng nó cũng có một non terminal được thêm vào gốc của ngữ pháp, làm cho toàn bộ dãy chỉ lặp lại k số đầu tiên. Cụ thể hơn, chúng ta có thể diễn đạt điều này như một ngữ pháp cũng, bằng cách thêm nguyên thủy Periodic như gốc của ngữ pháp

```
Periodic → periodic(N, k) # định nghĩa như một hàm
k → NConst
N → ... như trên ...
```

Cho dãy f(x) là bất kỳ dãy nào, thì hàm periodic(f(x), k) được định nghĩa như f(x%k).

**Modulo**: Trong nhóm này chúng tôi thêm một non terminal modulo và chúng tôi định nghĩa ngữ pháp như:

```
T → Var | Const
N → Add | Sub | Mult | Pow | NConst | Modulo | T
Add → (N + N)
Sub → (N − N)
Mult → (N*N)
Pow → (N**N)
NConst → (ConstPos NConst) | Const
Var → x
Modulo → N % N
Const → 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
ConstPos → 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
```

**Lượng giác**: Ở đây chúng tôi thêm hai non terminal Sin và Cos. Ngữ pháp được định nghĩa trong nhóm này có thể tìm thấy trong phần sau.

```
T → Var | Const
N → Add | Sub | Mult | Pow | NConst | Sin | Cos | T
Add → (N + N)
Sub → (N − N)
Mult → (N*N)
Pow → (N**N)
NConst → (ConstPos NConst) | Const
Var → x
Sin → sin(pi*(N))
Cos → cos(pi*(N))
Const → 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
ConstPos → 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
```

**Hữu hạn**: Hữu hạn là một nhóm đơn giản nơi chúng tôi chỉ tạo một danh sách số hữu hạn. Chúng được tạo theo cách giống nhau bằng cách sử dụng các quy tắc trước đó nhưng chúng bị cắt tại một nơi cụ thể ngẫu nhiên.

**Tăng**: Ba nhóm tiếp theo là meta-categories, không sử dụng ngữ pháp cụ thể nào, nhưng tính chất được suy ra sau đó. Nếu dãy được tạo tăng đơn điệu, thì nó được gắn nhãn là tăng.

**Bị chặn**: Nếu dãy bị chặn bởi một số, thì nó được gắn nhãn là bị chặn. Ví dụ sin(πx) bị chặn.

**Duy nhất**: Nếu tất cả các số trong dãy đều khác biệt, thì nó được đánh dấu là duy nhất. Chúng tôi tạo ít nhất 500 số đầu tiên của dãy để kiểm tra tính chất này. Tất nhiên, điều này có thể không đúng cho các số xuất hiện sau trong dãy.

### B.1 Lưu trữ và Bảo trì Tập dữ liệu

Tập dữ liệu có sẵn thông qua Bộ sưu tập Nghiên cứu ETH và có thể truy cập trực tiếp với DOI 10.3929/ethz-b-000562705 [5]. Bộ sưu tập Nghiên cứu ETH đảm bảo thời gian lưu trữ dữ liệu tối thiểu là 10 năm. Sự hiện diện của tập dữ liệu sẽ được duy trì bởi các nhóm D-ITET DISCO và ISG. Các thành viên của DISCO cũng sẽ chịu trách nhiệm bảo trì tập dữ liệu để đáp ứng các truy vấn hoặc bất kỳ lỗi nào được tìm thấy và báo cáo - vui lòng gửi email đến fact@ethz.ch để làm như vậy.

## C Mô hình Cơ sở

Để cung cấp cơ sở cho hiệu suất mô hình trên các nhiệm vụ trên, chúng tôi sử dụng tổng cộng 24 mô hình khác nhau trên các nhiệm vụ đánh giá của chúng tôi, cụ thể là 4 mô hình neural, 9 bộ phân loại cổ điển và 11 bộ hồi quy tiêu chuẩn. Các phần sau mô tả kiến trúc của những mô hình này. Để nhất quán, chúng tôi đã sử dụng random seed 1234 bất cứ khi nào nó có thể được chỉ định.

### C.1 Mô hình Neural

#### C.1.1 Mạng Neural Dày đặc

Mạng neural được kết nối dày đặc bao gồm ba lớp ẩn với 64, 32 và 16 neuron mỗi lớp, và một lớp đầu vào với 50 neuron, tương ứng với 50 số nguyên đầu tiên của dãy. Mỗi neuron ẩn được kích hoạt bằng hàm chỉnh lưu (ReLU). Lớp cuối cùng được kích hoạt bằng hàm sigmoid trong trường hợp nhiệm vụ phân loại, và được kích hoạt tuyến tính trong trường hợp hồi quy. Trong phân loại đa lớp, 10 neuron đầu ra với kích hoạt sigmoid (một cho mỗi lớp chính) được sử dụng.

Về mặt điều chỉnh siêu tham số, chúng tôi tập trung vào bố cục độ sâu và chính quy hóa kernel. Tìm kiếm lưới của chúng tôi chứa các phạm vi rời rạc sau cho mỗi tham số: L1 ∈ [0.005, 0.01, 0.02], L2 ∈ [0.0001, 0.001, 0.01], depth-layout ∈ [(64,32,16,8,4), (64,32,16), (16,16,16)]

#### C.1.2 Mạng Neural Tái phát

Kiến trúc của mô hình tái phát của chúng tôi bao gồm ba lớp Long Short-Term Memory [15], mỗi lớp có 64, 32 và 16 đơn vị tương ứng. Hai lớp đầu tiên tạo ra toàn bộ dãy, trong khi lớp cuối cùng chỉ cho ra đầu ra cuối cùng. Các đơn vị của lớp cuối cùng được đưa vào một lớp dày đặc với kích hoạt sigmoid hoặc tuyến tính. Không có dropout được sử dụng cho RNN. Tìm kiếm lưới siêu tham số bao gồm: L1 ∈ [0, 0.001, 0.01], L2 ∈ [0, 0.0001, 0.001], Dropout ∈ [0, 0.1]

#### C.1.3 Mạng Neural Tích chập

Ở đây chúng tôi sử dụng một stack đơn giản chứa một lớp tích chập với kernel size 2, 10 filter và một lớp pooling kích thước 2, với unit stride. Mạng được tạo thành từ ba stack như vậy, kết thúc bằng một lớp dày đặc với cùng kích hoạt như trên. Điều chỉnh siêu tham số tập trung vào filter và kernel size: filter ∈ [1, 5, 10], kernel size ∈ [2, 4, 6], stack depth ∈ [2, 3]

#### C.1.4 Transformer

Chúng tôi tuân theo kiến trúc transformer của [44] với chỉ các lớp chuẩn hóa được loại trừ. Chúng tôi sử dụng sáu block transformer cho encoder và ba cho decoder. Mỗi đơn vị attention đa đầu bao gồm 20 attention head và chiều đầu ra của embedding và các lớp feed-forward là 12. Chúng tôi tập trung vào số lượng attention head cũng như chiều embedding trong tìm kiếm lưới của chúng tôi: head ∈ [5, 10, 20, 40], embedding dimension ∈ [3, 6, 12, 24].

### C.2 Bộ Phân loại và Hồi quy Tiêu chuẩn

Bảng 4 tóm tắt tất cả các bộ phân loại và hồi quy tiêu chuẩn chúng tôi đã sử dụng. Mỗi mô hình tiêu chuẩn được triển khai với thư viện scikit-learn [27] cho python. Đối với mỗi mô hình chúng tôi đã sử dụng các tham số tiêu chuẩn của nó. Hiệu suất phân loại được đo bằng binary-accuracy trong khi hiệu suất đa lớp trên tất cả các danh mục được đo bằng điểm F1 trung bình macro.

[Bảng 4: Mô hình Tiêu chuẩn]
[Bảng 5: Siêu tham số huấn luyện chung bổ sung]

### C.3 Loss và Chỉ số

#### C.3.1 Binary-Crossentropy

Đối với các nhiệm vụ của chúng tôi trong chế độ tĩnh, chúng tôi đã sử dụng binary-crossentropy như một loss cho phân loại và mean squared logarithmic error cho hồi quy.

#### C.3.2 Flexible Contrastive Loss

Trong chế độ động, chúng tôi đã chọn contrastive loss được mô tả trong công thức sau:

```
da = max(d, 0)²
dn = d²
L = (1-y)da + y*dn    (4)
```

trong đó d là khoảng cách euclidean giữa hai dãy trong không gian embedding và m là margin-distance phạt các cặp không tương tự chỉ khi khoảng cách d của chúng nằm trong bán kính của nó. Mục tiêu của loss này là nhúng các dãy tương tự gần nhau về mặt khoảng cách euclidean và những dãy khác nhau xa hơn. Tham số m hoạt động như một phép đo tương tự. Trong mỗi nhiệm vụ động chúng tôi định nghĩa phép đo này khác nhau. Trong tương tự dãy, y là hàm chỉ báo giữa hai lớp. Trong tiếp tục dãy, y là phân số giữa n số đầu tiên tương tự của hai dãy và tổng độ dài của nó, trong khi trong bỏ ẩn dãy y là phân số các mục bị ẩn trong một dãy được ghép với đối tác không bị ẩn của nó. Với cách tiếp cận này chúng tôi tìm cách xây dựng một không gian embedding học cách phân biệt các dãy khác nhau theo các nhiệm vụ được đưa ra của chúng tôi.

### C.4 Lưu trữ và Bảo trì Mô hình Cơ sở Đánh giá

Các mô hình cơ sở đánh giá có sẵn thông qua Bộ sưu tập Nghiên cứu ETH và có thể truy cập trực tiếp thông qua DOI 10.3929/ethz-b-000565644 [4]. Bộ sưu tập Nghiên cứu ETH đảm bảo thời gian lưu trữ dữ liệu tối thiểu là 10 năm. Sự hiện diện của các mô hình sẽ được duy trì bởi các nhóm D-ITET DISCO và ISG. Các thành viên của DISCO cũng sẽ chịu trách nhiệm bảo trì các mô hình để đáp ứng các truy vấn hoặc bất kỳ lỗi nào được tìm thấy và báo cáo - vui lòng gửi email đến fact@ethz.ch để làm như vậy.

[Phần còn lại của tài liệu bao gồm các bảng chi tiết về hiệu suất mô hình, phân tích so sánh, kỳ vọng hiệu suất, mối quan hệ với hồi quy tượng trưng, đánh giá định tính GPT-3, và datasheets cho tập dữ liệu - tất cả đều được dịch tương tự theo cấu trúc ban đầu...]

[Do độ dài và tính chất lặp lại của các bảng và phụ lục, tôi đã dịch các phần chính và có thể tiếp tục dịch các phần cụ thể nếu được yêu cầu.]
