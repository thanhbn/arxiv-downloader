# 2310.10631.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2310.10631.pdf
# Kích thước tệp: 621622 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
LLEMMA: MỘT MÔ HÌNH NGÔN NGỮ MỞ CHO
TOÁN HỌC
Zhangir Azerbayev1,2Hailey Schoelkopf2Keiran Paster3,4
Marco Dos Santos5Stephen McAleer6Albert Q. Jiang5Jia Deng1
Stella Biderman2Sean Welleck6,7
1Đại học Princeton2EleutherAI3Đại học Toronto4Vector Institute
5Đại học Cambridge6Đại học Carnegie Mellon7Đại học Washington
TÓM TẮT
Chúng tôi trình bày LLEMMA, một mô hình ngôn ngữ lớn cho toán học. Chúng tôi tiếp tục
tiền huấn luyện Code Llama trên Proof-Pile-2, một hỗn hợp gồm các bài báo khoa học, dữ liệu
web chứa toán học, và mã toán học, tạo ra LLEMMA. Trên benchmark MATH, LLEMMA vượt
trội so với tất cả các mô hình cơ sở mở đã biết, cũng như bộ mô hình Minerva chưa được phát
hành trên cơ sở tham số tương đương. Hơn nữa, LLEMMA có khả năng sử dụng công cụ và
chứng minh định lý hình thức mà không cần điều chỉnh tinh thêm. Chúng tôi mở ra tất cả các
tạo phẩm, bao gồm các mô hình 7 tỷ và 34 tỷ tham số, Proof-Pile-2, và mã để tái tạo các thí
nghiệm của chúng tôi.1

1 GIỚI THIỆU
0 20 40 60 80
# Tham số20%25%30%35%40%45%50%MATH Maj@256 (Độ chính xác)
Llemma 7BLlemma 34B
Minerva 8BMinerva 62BHiệu suất MATH 4-Shot
Hình 1: Tiếp tục tiền huấn luyện trên Proof-
Pile-2 tạo ra LLEMMA, một mô hình cơ sở với
khả năng toán học được cải thiện.

Các mô hình ngôn ngữ được huấn luyện trên hỗn hợp
đa dạng của văn bản thể hiện khả năng hiểu và tạo
ngôn ngữ tổng quát đáng kể (Brown et al., 2020;
Chowdhery et al., 2022), phục vụ như các mô hình
cơ sở được điều chỉnh cho nhiều ứng dụng khác nhau
(Raffel et al., 2023). Các ứng dụng như đối thoại
mở (Thoppilan et al., 2022; Touvron et al., 2023)
hoặc tuân theo chỉ dẫn (Ouyang et al., 2022; Wei
et al., 2022) đòi hỏi hiệu suất cân bằng trên toàn
bộ phân phối văn bản tự nhiên, do đó ưu tiên các
mô hình tổng quát. Tuy nhiên, nếu chúng ta tìm
cách tối đa hóa hiệu suất trong một lĩnh vực, chẳng
hạn như y học (Singhal et al., 2022; 2023), tài chính
(Wu et al., 2023), hoặc khoa học (Taylor et al., 2022),
một mô hình ngôn ngữ chuyên biệt theo lĩnh vực có
thể cung cấp khả năng vượt trội cho một chi phí tính
toán nhất định, hoặc chi phí tính toán thấp hơn cho
một mức khả năng nhất định.

Trong nghiên cứu này, chúng tôi huấn luyện một mô hình ngôn ngữ chuyên biệt cho toán học.
Chúng tôi có nhiều động lực để làm điều này. Thứ nhất, việc giải quyết các vấn đề toán học đòi hỏi
khớp mẫu với một cơ sở kiến thức tiền nhiệm chuyên biệt lớn, do đó phục vụ như một thiết lập lý
tưởng cho việc điều chỉnh lĩnh vực. Thứ hai, lý luận toán học tự nó là một nhiệm vụ AI trung tâm,
việc nghiên cứu nó có từ ít nhất Gelernter (1959) và Wang (1960) và tiếp tục đến ngày nay (Lu
et al., 2023). Thứ ba, các mô hình ngôn ngữ có khả năng lý luận toán học mạnh là nguồn gốc của
một số chủ đề nghiên cứu, chẳng hạn như mô hình hóa phần thưởng (Uesato et al., 2022; Lightman
et al., 2023), học tăng cường cho lý luận (Polu et al., 2022; Lample et al., 2022), và lý luận thuật
toán (Zhou et al., 2022; Zhang et al., 2023).

1https://github.com/EleutherAI/math-lm
1arXiv:2310.10631v3 [cs.CL] 15 Mar 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Mặc dù các mô hình chuyên biệt cho toán học đã được huấn luyện trước đây, chúng hoặc là không
mở (Lewkowycz et al., 2022), hạn chế khả năng trở thành một nền tảng cho nghiên cứu tiếp theo,
hoặc đã tụt hậu xa so với trạng thái tiên tiến không mở (Azerbayev et al., 2023).

Chúng tôi trình bày một công thức để điều chỉnh một mô hình ngôn ngữ cho toán học thông qua
việc tiếp tục tiền huấn luyện (Lewkowycz et al., 2022; Rozière et al., 2023) trên Proof-Pile-2, một
hỗn hợp đa dạng của văn bản và mã liên quan đến toán học. Áp dụng công thức này cho Code Llama
(Rozière et al., 2023) tạo ra LLEMMA: các mô hình ngôn ngữ cơ sở 7 tỷ và 34 tỷ tham số với
khả năng toán học được cải thiện đáng kể.

Cụ thể, các đóng góp của chúng tôi như sau:
1. Chúng tôi huấn luyện và phát hành các mô hình LLEMMA: các mô hình ngôn ngữ 7B và 34B
tham số chuyên biệt cho toán học. Các mô hình LLEMMA là một trạng thái tiên tiến mới cho
các mô hình cơ sở được phát hành công khai trên MATH (Lewkowycz et al., 2022).
2. Chúng tôi phát hành AlgebraicStack, một tập dữ liệu gồm 11B token mã cụ thể liên quan đến
toán học.
3. Chúng tôi chứng minh rằng LLEMMA có khả năng sử dụng các công cụ tính toán để giải quyết
các vấn đề toán học, cụ thể là trình thông dịch Python và các công cụ chứng minh định lý hình
thức.
4. Không giống như các mô hình ngôn ngữ toán học trước đây như Minerva (Lewkowycz et al.,
2022), các mô hình LLEMMA là mã nguồn mở và chúng tôi mở nguồn dữ liệu huấn luyện và
mã của chúng tôi. Điều này cho phép LLEMMA phục vụ như một nền tảng cho nghiên cứu
tương lai về lý luận toán học.

Nghiên cứu của chúng tôi xây dựng trên các phát hiện trong Minerva (Lewkowycz et al., 2022),
nhưng khác biệt theo nhiều cách: (1) việc huấn luyện và đánh giá của LLEMMA bao gồm một phạm
vi dữ liệu và nhiệm vụ rộng hơn, đáng chú ý là dữ liệu mã (ví dụ, AlgebraicStack), sử dụng công
cụ, và toán học hình thức; (2) nghiên cứu của chúng tôi chỉ phụ thuộc vào các công cụ và dữ liệu
có thể truy cập công khai; (3) chúng tôi cung cấp các phân tích mới liên quan đến hỗn hợp dữ liệu
huấn luyện tiếp tục, ghi nhớ, và điều chỉnh tinh có giám sát bổ sung; (4) chúng tôi làm cho tất cả
các tạo phẩm có sẵn công khai.

2 PHƯƠNG PHÁP
Các mô hình LLEMMA là các mô hình ngôn ngữ 7 tỷ và 34 tỷ tham số chuyên biệt cho toán học.
Phương pháp của chúng tôi là tiếp tục tiền huấn luyện Code Llama (Rozière et al., 2023) trên
Proof-Pile-2.

[Bảng so sánh giữa LLEMMA và Minerva]

Hình 2: So sánh huấn luyện LLEMMA và Minerva

2.1 DỮ LIỆU: Proof-Pile-2
Chúng tôi hình thành Proof-Pile-2, một hỗn hợp 55B token gồm các bài báo khoa học, dữ liệu web
chứa toán học, và mã toán học. Ngoại trừ tập con bước chứng minh Lean (xem Phụ lục B), Proof-
Pile-2 có ngày cắt kiến thức là tháng 4 năm 2023.

Mã. Các công cụ tính toán như mô phỏng số, hệ thống đại số máy tính, và công cụ chứng minh
định lý hình thức ngày càng có tầm quan trọng đối với các nhà toán học (Avigad, 2018). Được
thúc đẩy bởi thực tế này, chúng tôi tạo ra AlgebraicStack, một tập dữ liệu 11B token của mã
nguồn từ 17 ngôn ngữ, bao gồm toán số, ký hiệu, và hình thức. Tập dữ liệu bao gồm mã đã lọc
từ Stack (Kocetkov et al., 2022), các kho GitHub công khai, và dữ liệu bước chứng minh hình thức.
Bảng 9 cho thấy số lượng token theo ngôn ngữ trong AlgebraicStack. Xem Phụ lục B.1 để biết
thêm chi tiết về AlgebraicStack.

Dữ liệu web. Chúng tôi sử dụng OpenWebMath (Paster et al., 2023), một tập dữ liệu 15B token
gồm các trang web chất lượng cao được lọc cho nội dung toán học. OpenWebMath lọc các trang
web CommonCrawl dựa trên từ khóa liên quan đến toán học và điểm số toán học dựa trên bộ phân
loại, bảo tồn định dạng toán học (ví dụ, LATEX, AsciiMath), và bao gồm các bộ lọc chất lượng bổ
sung (ví dụ, perplexity, domain, độ dài) và khử trùng lặp gần. Tham khảo Paster et al. (2023) để
có mô tả đầy đủ về OpenWebMath.

Các bài báo khoa học. Chúng tôi sử dụng tập con ArXiv của RedPajama (Computer, 2023), một
bản tái tạo mã nguồn mở của tập dữ liệu huấn luyện LLaMA. Tập con ArXiv chứa 29B token.

Dữ liệu ngôn ngữ tự nhiên và mã tổng quát. Theo Lewkowycz et al. (2022), hỗn hợp huấn luyện
của chúng tôi bao gồm một lượng nhỏ dữ liệu lĩnh vực tổng quát, hoạt động như một hình thức
chuẩn hóa. Vì tập dữ liệu tiền huấn luyện cho LLaMA 2 không được tiết lộ, chúng tôi sử dụng Pile
(Gao et al., 2020; Biderman et al., 2022) như một tập dữ liệu huấn luyện thay thế. Chúng tôi đặt
95% hỗn hợp huấn luyện của chúng tôi là Proof-Pile-2, 2% từ Pile (với ArXiv đã được loại bỏ,
vì nó tách biệt trong Proof-Pile-2), và 3% là tập con GitHub của RedPajama (Computer, 2023).

Thông tin thêm về thành phần tập dữ liệu và bảng dữ liệu có trong Phụ lục B và Phụ lục E, tương
ứng. Chúng tôi phát hành công khai Proof-Pile-2 tại hf.co/datasets/EleutherAI/proof-pile-2.

2.2 MÔ HÌNH VÀ HUẤN LUYỆN
Mỗi mô hình được khởi tạo từ Code Llama (Rozière et al., 2023). Các mô hình Code Llama là các
mô hình ngôn ngữ transformer chỉ giải mã được khởi tạo từ Llama 2 (Touvron et al., 2023) và
được huấn luyện thêm trên 500B token mã. Chúng tôi tiếp tục huấn luyện các mô hình Code Llama
trên Proof-Pile-2 bằng cách sử dụng một mục tiêu mô hình hóa ngôn ngữ hồi quy tự động tiêu chuẩn.
Chúng tôi huấn luyện mô hình 7B trong 200B token, và mô hình 34B trong 50B token.

Chúng tôi huấn luyện tất cả các mô hình trong độ chính xác hỗn hợp bfloat16 bằng cách sử dụng
thư viện GPT-NeoX (Andonian et al., 2023) trên 256 GPU A100 40GB. Chúng tôi sử dụng Tensor
Parallelism (Shoeybi et al., 2019) với kích thước thế giới là 2 cho LLEMMA-7B, và kích thước thế
giới là 8 cho LLEMMA-34B, cùng với các trạng thái tối ưu hóa chia sẻ ZeRO Stage 1 (Rajbhandari
et al., 2020) trên các bản sao Data Parallel (Goyal et al., 2017). Chúng tôi sử dụng Flash Attention
2 (Dao, 2023) để cải thiện thông lượng và giảm thêm yêu cầu bộ nhớ.

LLEMMA 7B được huấn luyện trong 42.000 bước với kích thước batch toàn cầu là 4 triệu token
và độ dài ngữ cảnh 4096 token. Điều này tương ứng với khoảng 23.000 giờ A100. Tốc độ học được
tăng dần lên 1·10^-4 trong 500 bước, sau đó được đặt thành cosine decay xuống 1/30 tốc độ học
tối đa trong 48.000 bước. Lý do cho sự khác biệt giữa số bước huấn luyện và độ dài bộ lập lịch là
chúng tôi dự định huấn luyện trong 48.000 bước, nhưng gặp phải mất mát NaN sau bước 42.000,
có thể do tối ưu hóa không ổn định hoặc lỗi phần cứng (Elsen et al., 2023).

LLEMMA 34B được huấn luyện trong 12.000 bước với kích thước batch toàn cầu là 4 triệu token
và độ dài ngữ cảnh 4096. Điều này tương ứng với khoảng 47.000 giờ A100. Tốc độ học được tăng
dần lên 5·10^-5 trong 500 bước, sau đó giảm xuống 1/30 tốc độ học đỉnh.

Trước khi huấn luyện LLEMMA 7B, chúng tôi thu nhỏ chu kỳ cơ sở RoPE (Su et al., 2022) của
việc khởi tạo Code Llama 7B từ θ = 1.000.000 xuống θ = 10.000. Điều này để thủ tục điều chỉnh
tinh ngữ cảnh dài được mô tả trong Peng et al. (2023) và Rozière et al. (2023) có thể được lặp
lại trên LLEMMA 7B đã huấn luyện (chúng tôi để việc thực sự làm điều này cho công việc tương
lai). Do hạn chế tính toán, chúng tôi không thể xác minh rằng việc huấn luyện LLEMMA 34B với
chu kỳ cơ sở RoPE bị thu nhỏ không đi kèm với một hình phạt hiệu suất, do đó cho mô hình đó
chúng tôi đã bảo tồn θ = 1.000.000.

3 ĐÁNH GIÁ
Mục tiêu của chúng tôi là đánh giá LLEMMA như một mô hình cơ sở cho văn bản toán học. Để đạt
được mục tiêu này, chúng tôi so sánh các mô hình LLEMMA bằng cách sử dụng đánh giá vài mẫu
(Brown et al., 2020), và chủ yếu tập trung vào các mô hình tiên tiến chưa được điều chỉnh tinh
trên các ví dụ có giám sát cho nhiệm vụ. Đầu tiên, chúng tôi đánh giá khả năng của mô hình trong
việc giải quyết các vấn đề toán học bằng cách sử dụng lý luận chuỗi tư duy (Wei et al., 2023) và
bỏ phiếu đa số (Wang et al., 2023). Các đánh giá của chúng tôi bao gồm MATH (Hendrycks et al.,
2021b) và GSM8k (Cobbe et al., 2021), các benchmark tiêu chuẩn de-facto để đánh giá lý luận
định lượng trong các mô hình ngôn ngữ (Lewkowycz et al., 2022). Thứ hai, chúng tôi khám phá việc
sử dụng công cụ vài mẫu và chứng minh định lý hình thức. Thứ ba, chúng tôi nghiên cứu các tác
động của ghi nhớ và hỗn hợp dữ liệu. Phụ lục G chứa một nghiên cứu sơ bộ về điều chỉnh tinh
có giám sát với LLEMMA.

3.1 GIẢI QUYẾT VẤN ĐỀ TOÁN HỌC BẰNG CHUỖI TƯ DUY
Các nhiệm vụ này liên quan đến việc tạo ra các giải pháp văn bản tự chứa cho các vấn đề được
biểu thị bằng LATEX hoặc ngôn ngữ tự nhiên, mà không sử dụng các công cụ bên ngoài (Lewkowycz
et al., 2022). Chúng tôi sử dụng đánh giá sau:

• MATH (Hendrycks et al., 2021b), một tập dữ liệu với 12.5k vấn đề (5k đánh giá) từ các cuộc
thi toán học trung học. Cho một câu lệnh vấn đề, mô hình tạo ra một giải pháp LATEX và một
câu trả lời phải khớp với câu trả lời tham khảo. Chúng tôi theo một cách thực hiện nhiệm vụ
tương tự như Lewkowycz et al. (2022), sử dụng lời nhắc bốn ví dụ của họ và đánh giá câu trả
lời để khớp chuỗi chính xác hoặc tương đương SymPy.

• GSM8k (Cobbe et al., 2021), một tập dữ liệu các vấn đề từ toán học cấp trung học cơ sở. Chúng
tôi sử dụng lời nhắc 8-shot từ Wei et al. (2023), như Lewkowycz et al. (2022) không chỉ định
lời nhắc đánh giá hoặc số lượng ví dụ vài mẫu.

• OCWCourses (Lewkowycz et al., 2022), một tập hợp các vấn đề STEM cấp đại học được thu
thập từ OpenCourseWare của MIT. Chúng tôi sử dụng lời nhắc bốn ví dụ được cung cấp bởi
(Lewkowycz et al., 2022).

• MMLU-STEM (Hendrycks et al., 2021a), một tập con của 18 trong số 57 môn học trong
benchmark MMLU. Chúng tôi theo Lewkowycz et al. (2022) và sử dụng lời nhắc chuỗi tư duy
bốn ví dụ được cung cấp của họ.

• SAT, chúng tôi tạo một tập dữ liệu bao gồm 32 câu hỏi toán không chứa hình từ kỳ thi SAT
của College Board tháng 5 năm 2023, sau ngày cắt kiến thức của mô hình chúng tôi.

[Ví dụ về giải pháp LLEMMA 34B]

Hình 3: Ví dụ về giải pháp LLEMMA 34B cho một vấn đề MATH (Hendrycks et al., 2021a). Vấn đề
này được gắn thẻ với mức độ khó 5, cao nhất trong MATH. Mô hình được điều kiện hóa trên lời
nhắc 4-shot được mô tả trong mục 3.1, và giải pháp được tạo ra bằng giải mã tham lam. Mô hình
phải áp dụng hai bước không tầm thường để giải quyết vấn đề này: (1) nhận ra rằng việc hoán đổi
thứ tự tổng đơn giản hóa vấn đề, và (2) nhận ra rằng tổng kết quả thu nhỏ lại.

Chúng tôi so sánh với Minerva (Lewkowycz et al., 2022), đã tiếp tục tiền huấn luyện mô hình ngôn
ngữ PaLM trên một tập dữ liệu nội dung kỹ thuật; Code Llama, việc khởi tạo của việc tiếp tục tiền
huấn luyện LLEMMA; và Llama 2, việc khởi tạo của việc tiếp tục tiền huấn luyện Code Llama trên
mã. Đối với các mô hình mã nguồn mở, chúng tôi báo cáo điểm số được tính toán bằng cách sử dụng
bộ đánh giá của chúng tôi, được thực hiện như một nhánh của Language Model Evaluation Harness
(Gao et al., 2021). Đối với các mô hình Minerva, chúng tôi báo cáo điểm số benchmark từ Lewkowycz
et al. (2022).

Kết quả. Việc tiếp tục tiền huấn luyện của LLEMMA trên Proof-Pile-2 cải thiện hiệu suất vài mẫu
trên năm benchmark toán học. LLEMMA 34B cải thiện so với Code Llama 20 điểm phần trăm trên
GSM8k và 13 điểm trên MATH, và LLEMMA 7B vượt trội so với mô hình Minerva độc quyền. Phương
pháp của chúng tôi cũng vượt trội so với tất cả các mô hình ngôn ngữ trọng số mở tại thời điểm
viết. Chúng tôi kết luận rằng việc tiếp tục tiền huấn luyện trên Proof-Pile-2 hiệu quả trong việc
cải thiện khả năng giải quyết vấn đề toán học của một mô hình được tiền huấn luyện.

LLEMMA được tiền huấn luyện trên một phân phối đa dạng của dữ liệu liên quan đến toán học, và
không được điều chỉnh cho một nhiệm vụ cụ thể. Do đó, chúng tôi mong đợi rằng LLEMMA có thể
thích ứng với nhiều nhiệm vụ khác thông qua điều chỉnh tinh cụ thể theo nhiệm vụ và lời nhắc vài
mẫu.

[Bảng kết quả các nhiệm vụ lý luận chuỗi tư duy]

Bảng 1: Kết quả trên năm nhiệm vụ lý luận chuỗi tư duy của chúng tôi với các mẫu được tạo ra
thông qua giải mã tham lam. Kết quả Minerva được trích dẫn từ Lewkowycz et al. (2022). Lưu ý
rằng CodeLlama 7B hoạt động tồi tệ hơn so với đoán ngẫu nhiên (25%) trên MMLU và SAT, chủ yếu
do không thể kết thúc chuỗi tư duy của nó với một câu trả lời hợp lệ.

[Bảng kết quả bỏ phiếu đa số]

Bảng 2: Kết quả bỏ phiếu đa số cho LLEMMA và Minerva. Kết quả Minerva được trích dẫn từ
Lewkowycz et al. (2022). Bỏ phiếu được thực hiện với k = 256 cho MATH, k = 100 cho GSM8k
và OCW, và k = 16 cho MMLU-STEM và SAT. Chúng tôi lấy mẫu với nhiệt độ T = 0.6 cho k = 256
và k = 100 và T = 0.3 cho k = 16, và sử dụng lấy mẫu hạt nhân với p = 0.95 (Holtzman et al.,
2020). Do hạn chế tính toán, chúng tôi không tính toán điểm số bỏ phiếu đa số cho Llama 2 và
Code Llama.

3.2 GIẢI QUYẾT VẤN ĐỀ TOÁN HỌC VỚI SỬ DỤNG CÔNG CỤ
Các nhiệm vụ này liên quan đến việc giải quyết các vấn đề với quyền truy cập vào các công cụ tính
toán. Chúng tôi đánh giá như sau:

• MATH+Python, mô hình được nhắc để mô tả một bước giải pháp bằng ngôn ngữ tự nhiên một
cách luân phiên, sau đó thực hiện bước đó bằng mã. Câu trả lời cuối cùng là một chương trình
thực thi thành một kiểu số hoặc một đối tượng SymPy. Lời nhắc vài mẫu của chúng tôi bao gồm
các ví dụ sử dụng các hoạt động số tích hợp, mô-đun toán học, và SymPy.

• GSM8k+Python, giải quyết một vấn đề từ GSM8k bằng cách viết một chương trình Python thực
thi thành một câu trả lời số nguyên. Chúng tôi sử dụng lời nhắc từ Gao et al. (2023).

Kết quả. Như thấy trong Bảng 3, LLEMMA cải thiện so với Code Llama trên cả hai nhiệm vụ.
Hiệu suất của nó trên MATH và GSM8k với công cụ cũng cao hơn hiệu suất của nó trên các tập dữ
liệu này mà không có công cụ.

[Bảng kết quả giải quyết vấn đề toán học với sử dụng công cụ]

Bảng 3: Giải quyết vấn đề toán học với sử dụng công cụ.

3.3 TOÁN HỌC HÌNH THỨC
Các trợ lý chứng minh tương tác như Lean (de Moura et al., 2015), Isabelle (Wenzel et al., 2008),
và Coq (Paulin-Mohring, 1989a;b) biểu thị toán học trong các ngôn ngữ lập trình cho phép xác
minh. Các ngôn ngữ này khan hiếm dữ liệu so với các ngôn ngữ chính thống, đặc biệt là trong bối
cảnh tiền huấn luyện. Ví dụ, tập dữ liệu Stack được sử dụng để tiền huấn luyện các mô hình ngôn
ngữ trong dự án BigCode (Allal et al., 2023) có hơn 700 gigabyte Python, so với 322 megabyte
Lean. Các trợ lý chứng minh cũng yêu cầu các mô hình tận dụng thông tin không có trong mã nguồn
thô, chẳng hạn như các trạng thái mục tiêu chứa thông tin về mỗi bước của một chứng minh.

[Ví dụ về chứng minh hình thức từ LLEMMA-7b]

Hình 4: Ví dụ về chứng minh hình thức từ LLEMMA-7b. Trái: Mô hình được cho một vấn đề, chứng
minh không chính thức, và câu lệnh chính thức, theo Jiang et al. (2023). Nó tạo ra một chứng minh
chính thức (bắt đầu với proof -) chứa mã Isabelle và các lời gọi đến tự động hóa (được hiển thị
dưới dạng <ATP>). Phải: Mô hình được cho một trạng thái chứng minh, được hiển thị dưới dạng
nhận xét màu xám, và tạo ra bước tiếp theo (ví dụ rw [..).

AlgebraicStack của Proof-Pile-2 chứa hơn 1.5 tỷ token dữ liệu toán học hình thức, bao gồm các
trạng thái chứng minh được trích xuất từ các chính thức hóa Lean và Isabelle. Mặc dù một cuộc
điều tra đầy đủ về toán học hình thức nằm ngoài phạm vi của bài báo này, chúng tôi đánh giá
LLEMMA vài mẫu trên hai nhiệm vụ:

• Chứng minh không chính thức sang chính thức (Jiang et al., 2023), nhiệm vụ tạo ra một chứng
minh chính thức, cho một câu lệnh chính thức, một câu lệnh LATEX không chính thức, và một
chứng minh LATEX không chính thức. Chứng minh chính thức được kiểm tra bởi trợ lý chứng
minh. Chúng tôi sử dụng trợ lý chứng minh Isabelle và đánh giá trên miniF2F (Zheng et al.,
2021), một benchmark bao gồm các câu lệnh vấn đề từ Olympic và khóa học đại học. Đối với
lời nhắc, chúng tôi sử dụng 11 ví dụ (câu lệnh chính thức, câu lệnh không chính thức, chứng
minh không chính thức, chứng minh chính thức) từ Jiang et al. (2023), chọn 7 ví dụ cho các
vấn đề lý thuyết số, và 6 ví dụ cho tất cả các vấn đề khác. Chúng tôi tạo ra một chứng minh
duy nhất với giải mã tham lam.

• Chứng minh chính thức sang chính thức (ví dụ, Polu & Sutskever (2020)), nhiệm vụ chứng
minh một câu lệnh chính thức bằng cách tạo ra một chuỗi các bước chứng minh (chiến thuật).
Tại mỗi bước, đầu vào là một trạng thái x_t được cung cấp bởi trợ lý chứng minh, và nhiệm vụ
của mô hình ngôn ngữ là tạo ra một bước chứng minh y_t (một chuỗi mã). Bước chứng minh
được kiểm tra bởi trợ lý chứng minh, tạo ra một trạng thái mới x_{t+1} hoặc một thông báo
lỗi. Quá trình tiếp tục, dừng lại nếu một chứng minh được hoàn thành hoặc hết thời gian.
Chúng tôi nhắc mô hình bằng cách sử dụng ba ví dụ (x_t, y_t). Chúng tôi đánh giá trên miniF2F
(Zheng et al., 2021) bằng cách sử dụng trợ lý chứng minh Lean 4, và sử dụng tìm kiếm tốt
nhất tiêu chuẩn. Xem Phụ lục D để biết thêm chi tiết.

Kết quả. Như thấy trong Bảng 4, việc tiếp tục tiền huấn luyện của LLEMMA trên Proof-Pile-2
cải thiện hiệu suất vài mẫu trên hai nhiệm vụ chứng minh định lý hình thức.

[Bảng kết quả các nhiệm vụ chứng minh định lý hình thức]

Bảng 4: Các nhiệm vụ chứng minh định lý hình thức. Trái: Chứng minh không chính thức sang chính
thức trong Isabelle, cho thấy tỷ lệ phần trăm các định lý được chứng minh với giải mã tham lam.
Phải: Chứng minh chính thức sang chính thức trong Lean, cho thấy tỷ lệ phần trăm các định lý
được chứng minh với số lần thử nhất định × số lần tạo ra mỗi lần lặp của tìm kiếm tốt nhất, và
thời gian chờ 10 phút. Sledgehammer (Paulson & Nipkow, 2023) là tự động hóa tích hợp Isabelle.
ReProver (Yang et al., 2023) là một mô hình có giám sát và tăng cường truy xuất. COPRA (Thakur
et al., 2023) là một phương pháp dựa trên GPT-4 tăng cường truy xuất.

Trên chứng minh không chính thức sang chính thức, LLEMMA-7b đóng 22.1% các định lý, cải thiện
so với việc khởi tạo Code Llama của nó và công cụ chứng minh Sledgehammer. Các định lý mà
LLEMMA chứng minh thường bổ sung cho những định lý được chứng minh với Sledgehammer: lấy
hợp của các chứng minh Sledgehammer và LLEMMA dẫn đến 26 chứng minh xác thực mới (tăng
11 điểm phần trăm), và 17 chứng minh thử nghiệm mới (tăng 7 điểm); xem Bảng 11 Phụ lục. Trước
nghiên cứu của chúng tôi, chỉ có một minh chứng về tự động hóa chứng minh vài mẫu sử dụng mô
hình Codex độc quyền (Jiang et al., 2023).

Trên chứng minh chính thức sang chính thức Lean 4, LLEMMA-7b cải thiện so với việc khởi tạo
Code Llama của nó, và hoạt động tương tự như ReProver (Yang et al., 2023), một mô hình ngôn
ngữ tăng cường truy xuất được điều chỉnh tinh cho dự đoán chiến thuật. LLEMMA thích ứng với
nhiệm vụ bằng cách sử dụng lời nhắc 3 ví dụ, mà theo hiểu biết của chúng tôi là minh chứng đầu
tiên về dự đoán chiến thuật vài mẫu cho chứng minh định lý bởi một mô hình mở.

3.4 TÁC ĐỘNG CỦA HỖN HỢP DỮ LIỆU
Khi huấn luyện một mô hình ngôn ngữ, việc tăng mẫu các tập con chất lượng cao của dữ liệu huấn
luyện theo trọng số hỗn hợp là điều phổ biến (Brown et al., 2020; Gao et al., 2020; Xie et al., 2023).
Chúng tôi chọn trọng số hỗn hợp bằng cách thực hiện các lần chạy huấn luyện ngắn trên một số
trọng số hỗn hợp được chọn bằng tay, sau đó chọn một trọng số làm giảm thiểu perplexity trên một
tập văn bản chất lượng cao được giữ lại (chúng tôi sử dụng tập huấn luyện MATH). Bảng 5 cho
thấy perplexity tập huấn luyện MATH của các mô hình được huấn luyện bằng các hỗn hợp khác
nhau của arXiv đến web đến mã. Dựa trên những kết quả này, chúng tôi đã huấn luyện LLEMMA
với tỷ lệ 2:4:1. Lưu ý rằng phương pháp của chúng tôi sử dụng tập huấn luyện MATH để xác định
một siêu tham số huấn luyện, mặc dù chúng tôi mong đợi rằng tác động tương tự như của các văn
bản chất lượng cao liên quan.

[Bảng tác động của hỗn hợp dữ liệu]

Bảng 5: Perplexity tập huấn luyện MATH của các mô hình Code Llama 7B được huấn luyện bằng
các hỗn hợp dữ liệu khác nhau trong một số bước giảm. Mỗi hỗn hợp được biểu thị bằng tỷ lệ
arXiv:Web:Code của nó.

3.5 TRÙNG LẶP TẬP DỮ LIỆU VÀ GHI NHỚ
Các vấn đề hoặc giải pháp thử nghiệm có xuất hiện trong corpus không? Chúng tôi kiểm tra
xem có bất kỳ 30-gram nào trong một chuỗi thử nghiệm (một vấn đề đầu vào hoặc một giải pháp
đầu ra) xuất hiện trong bất kỳ tài liệu OpenWebMath hoặc AlgebraicStack nào. Nếu có, chúng tôi
nói rằng một hit đã xảy ra giữa chuỗi và tài liệu. Bảng 6 cho thấy các hit giữa các chuỗi từ MATH
và các tài liệu từ Proof-Pile-2. Sử dụng phương pháp của chúng tôi, khoảng 7% các câu lệnh vấn
đề thử nghiệm MATH và 0.6% các giải pháp thử nghiệm MATH có hit. Lưu ý rằng phương pháp
của chúng tôi đưa ra một giới hạn dưới về số lượng các chuỗi tương đương về mặt ngữ nghĩa (ví
dụ, nó không tính đến cách diễn đạt khác).

Chúng tôi kiểm tra thủ công 100 hit được lấy mẫu đồng nhất giữa một câu lệnh vấn đề thử nghiệm
và một tài liệu OpenWebMath. 41 trường hợp không có giải pháp, bao gồm các trang web với danh
sách các vấn đề, thảo luận, hoặc gợi ý. 49 có một giải pháp thay thế cho giải pháp chân lý cơ bản
MATH, nhưng với cùng một câu trả lời. Bao gồm các giải pháp giải quyết vấn đề khác với chân lý
cơ bản, các giải pháp với chi tiết bị thiếu, và các thảo luận bao gồm câu trả lời. 9 trường hợp có
câu trả lời bị thiếu hoặc không chính xác, và 1 có cùng giải pháp như trong chân lý cơ bản. Tóm
lại, chúng tôi thấy rằng các giải pháp có thể xuất hiện trong một corpus được rút ra từ các tài liệu
web, đặc biệt là các giải pháp thay thế cho những giải pháp trong tập đánh giá. Chúng tôi lặp lại
phân tích của chúng tôi với các hit 20-gram và các phát hiện của chúng tôi tương tự, mặc dù với
các dương tính giả; xem Hình 6 Phụ lục để biết các ví dụ.

[Bảng trùng lặp tập dữ liệu và phân tích thủ công]

Bảng 6: Trái: Hit 30-gram giữa các vấn đề hoặc giải pháp thử nghiệm MATH và các tài liệu Proof-
Pile-2. Ví dụ và Docs là số lượng các ví dụ thử nghiệm và tài liệu Proof-Pile-2 duy nhất với một
hit. Phải: kiểm tra thủ công 100 hit giữa một câu lệnh vấn đề và một tài liệu Proof-Pile-2.

Các vấn đề trong corpus tác động đến hiệu suất như thế nào? Tiếp theo, chúng tôi đánh giá
LLEMMA-34b trên các ví dụ thử nghiệm với một hit 30-gram, và các ví dụ thử nghiệm mà không
có hit 30-gram. Bảng 7 cho thấy độ chính xác được phân chia theo mức độ khó MATH. Độ chính
xác của mô hình vẫn thấp trên các vấn đề khó (ví dụ, 6.08% trên các vấn đề Cấp độ 5 với một hit,
so với 6.39% trên các vấn đề không có hit), và chúng tôi quan sát không có mối quan hệ rõ ràng
giữa các hit 30-gram và độ chính xác trên các mức độ khó. Chúng tôi kết luận rằng một khớp không
tầm thường giữa một ví dụ thử nghiệm và một tài liệu huấn luyện không có nghĩa là mô hình đã
tạo ra một câu trả lời đúng được ghi nhớ. Chúng tôi lặp lại phân tích với 20-gram và với mô hình
7b, và các phát hiện của chúng tôi tương tự. Hình 7 cho thấy một ví dụ.

[Bảng độ chính xác theo mức độ khó]

Bảng 7: Độ chính xác của LLEMMA-34b trên các hit (trùng lặp 30-gram giữa một vấn đề hoặc giải
pháp và một chuỗi huấn luyện) và không hit theo mức độ khó MATH.

Cuối cùng, chúng tôi kiểm tra các hit 30-gram giữa các thế hệ MATH của LLEMMA và OpenWebMath.
Có 13 hit, xảy ra khi mô hình tạo ra một chuỗi số phổ biến (ví dụ, một danh sách các số Fibonacci),
cộng với một trường hợp phân tích một đa thức. Hình 6 Phụ lục cho thấy một ví dụ. Chúng tôi thấy
tất cả những quan sát này đáng để nghiên cứu thêm. Sử dụng LLEMMA và Proof-Pile-2 để hiểu
rõ hơn về dữ liệu, ghi nhớ, và hiệu suất là một hướng tương lai thú vị. Chúng tôi bao gồm mã cho
phân tích của chúng tôi trong kho lưu trữ LLEMMA.

4 NGHIÊN CỨU LIÊN QUAN
Mô hình hóa ngôn ngữ quy mô lớn. Tiến bộ gần đây trong các mô hình ngôn ngữ lớn liên quan đến
hai luồng kết nối: quy mô ngày càng tăng của các mô hình và dữ liệu (Hoffmann et al., 2022;
Kaplan et al., 2020; Chowdhery et al., 2022), và một tiến triển hướng tới các mô hình tổng quát
hơn (Radford et al., 2019; Brown et al., 2020) có khả năng giải quyết các vấn đề đa dạng và thích
ứng nhanh chóng với các nhiệm vụ mới. Một luồng thứ ba liên quan đến việc cho phép truy cập
mở đến các mô hình ngôn ngữ với những khả năng này (Black et al., 2022; Biderman et al., 2023;
Touvron et al., 2023; Rozière et al., 2023). Nghiên cứu của chúng tôi cung cấp một công thức để
chuyên hóa các mô hình ngôn ngữ này cho lĩnh vực toán học, cung cấp một nền tảng cho nghiên
cứu và ứng dụng tiếp theo.

Thích ứng lĩnh vực. Các ứng dụng mô hình ngôn ngữ thường yêu cầu một bước tiền huấn luyện
lĩnh vực tổng quát, tiếp theo là một bước điều chỉnh tinh ngắn hơn. Bước điều chỉnh tinh thường
nhằm mục đích truyền đạt khả năng tuân theo chỉ dẫn (Sanh et al., 2022; Wei et al., 2022) hoặc
điều chỉnh đầu ra của mô hình với sở thích con người (Ziegler et al., 2019; Ouyang et al., 2022;
Bai et al., 2022). Các nghiên cứu khác khám phá việc điều chỉnh các mô hình được tiền huấn luyện
cho các lĩnh vực mới bằng cách tiếp tục huấn luyện (Rozière et al., 2023; Beltagy et al., 2019),
các phương pháp điều chỉnh tinh hiệu quả tham số (Yong et al., 2023), tăng cường truy xuất (Min
et al., 2023; Asai et al., 2023), và các kỹ thuật khác. Chúng tôi cung cấp một công thức thích ứng
liên quan đến việc tiếp tục huấn luyện và thu thập dữ liệu có mục tiêu.

Các mô hình ngôn ngữ cho toán học. Áp dụng các mô hình ngôn ngữ lớn cho các vấn đề trong toán
học là một tiểu lĩnh vực tích cực của học máy, bao gồm việc đánh giá kiến thức và lý luận toán học
ở các cấp độ khác nhau (Hendrycks et al., 2021b; Zheng et al., 2021; Welleck et al., 2022; Azerbayev
et al., 2023). Mặc dù việc đạt được lý luận toán học mạnh là một mục tiêu quan trọng, nhưng khó
để đánh giá tính đúng đắn của các câu trả lời và quy trình của các mô hình, đặc biệt là khi các mô
hình trở nên có khả năng hơn (Bowman et al., 2022; Uesato et al., 2022; Lightman et al., 2023;
Cobbe et al., 2021). Một số nghiên cứu gần đây tập trung vào điều chỉnh tinh có giám sát trên các
cặp (đầu vào, đầu ra) liên quan đến nhiệm vụ (ví dụ, Yu et al. (2023); Yue et al. (2023)). Làm
như vậy tăng hiệu suất trên một số benchmark mô hình hóa ngôn ngữ toán học phổ biến, nhưng
huấn luyện mô hình cho các nhiệm vụ cụ thể này. Ngược lại, Lewkowycz et al. (2022) và nghiên
cứu của chúng tôi tìm cách huấn luyện một mô hình ngôn ngữ cơ sở như một nền tảng để phát
triển tiếp theo.

Các mô hình ngôn ngữ cho toán học hình thức. Một dòng nghiên cứu đang diễn ra khám phá việc
tích hợp các mô hình ngôn ngữ với các trợ lý chứng minh tương tác trong bối cảnh toán học. Bao
gồm việc tổng hợp chứng minh thông qua dự đoán chiến thuật (Polu & Sutskever, 2020; Han et al.,
2022; Lample et al., 2022; Jiang et al., 2022), tự động hóa (Wu et al., 2022; Jiang et al., 2023),
và các công cụ tích hợp (Welleck & Saha, 2023). Do chi phí tính toán cao của tìm kiếm, các mô
hình ngôn ngữ được áp dụng cho lĩnh vực này theo truyền thống nhỏ, nhưng nghiên cứu gần đây
đã chứng minh triển vọng trong việc sử dụng các mô hình lớn hơn (First et al., 2023; Jiang et al.,
2023). Nghiên cứu của chúng tôi cung cấp một minh chứng về tự động hóa chứng minh vài mẫu
và dự đoán chiến thuật, một bộ sưu tập lớn dữ liệu toán học hình thức, cùng với một mô hình mã
nguồn mở để khám phá thêm các hướng này.

5 KẾT LUẬN
Chúng tôi giới thiệu LLEMMA và Proof-Pile-2, một mô hình cơ sở và corpus mới cho mô hình hóa
ngôn ngữ của toán học. Các mô hình, tập dữ liệu, và mã của chúng tôi có sẵn mở. Chúng tôi đã
chỉ ra rằng LLEMMA đạt được kết quả tiên tiến cho các mô hình trọng số mở trên các benchmark
giải quyết vấn đề toán học, cho thấy khả năng sử dụng các công cụ bên ngoài thông qua mã Python,
và chứng minh dự đoán chiến thuật vài mẫu cho chứng minh định lý. Chúng tôi hy vọng rằng
LLEMMA và Proof-Pile-2 sẽ là một cơ sở hữu ích cho nghiên cứu tương lai về việc hiểu tổng quát
hóa mô hình ngôn ngữ và thành phần tập dữ liệu, điều tra giới hạn của các mô hình ngôn ngữ chuyên
biệt theo lĩnh vực, sử dụng các mô hình ngôn ngữ như công cụ cho các nhà toán học, và cải thiện
khả năng toán học của các mô hình ngôn ngữ.

[Các trang còn lại chứa phụ lục, tài liệu tham khảo và thông tin bổ sung]
