# 2312.06585.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/math/2312.06585.pdf
# File size: 761238 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
2024-4-19
Beyond Human Data: Scaling Self-Training for
Problem-Solving with Language Models
Avi Singh1,*, John D Co-Reyes1,*, Rishabh Agarwal1,2,*,
Ankesh Anand1, Piyush Patil1, Xavier Garcia1, Peter J. Liu1, James Harrison1, Jaehoon Lee1, Kelvin Xu1,
Aaron Parisi1, Abhishek Kumar1, Alex Alemi1, Alex Rizkowsky1, Azade Nova1, Ben Adlam1, Bernd Bohnet1,
Gamaleldin Elsayed1, Hanie Sedghi1, Igor Mordatch1, Isabelle Simpson1, Izzeddin Gur1, Jasper Snoek1,
Jeffrey Pennington1, Jiri Hron1, Kathleen Kenealy1, Kevin Swersky1, Kshiteej Mahajan1, Laura Culp1, Lechao
Xiao1, Maxwell L Bileschi1, Noah Constant1, Roman Novak1, Rosanne Liu1, Tris Warkentin1, Yundi Qian1,
Yamini Bansal1, Ethan Dyer1, Behnam Neyshabur1, Jascha Sohl-Dickstein1, Noah Fiedel1
*Contributed equally,1Google DeepMind,2Mila
Fine-tuning language models (LMs) on human-generated data remains a prevalent practice. However,
theperformanceofsuchmodelsisoftenlimitedbythequantityanddiversityofhigh-qualityhumandata.
In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar
feedback, for example, on math problems where one can verify correctness. To do so, we investigate a
simple self-training method based on expectation-maximization, which we call ReSTğ¸ğ‘€, where we (1)
generate samples from the model and filter them using binary feedback, (2) fine-tune the model on
these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS
coding benchmarks using PaLM-2 models, we find that ReSTğ¸ğ‘€scales favorably with model size and
significantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with
feedback can reduce dependence on human-generated data.
Keywords: RL from external feedback, EM for RL, Language, LLMs, Reasoning, Coding, Self-Improvement
1. Introduction
Large Language Models (LLMs) are revolutionizing the landscape of deep learning, showcasing
remarkable capabilities in generating human-quality text and tackling diverse language tasks (Google
et al., 2023; OpenAI, 2023). While supervised fine-tuning (SFT) on human-collected data further
boosts their performance on tasks of interest, acquiring high-quality human data poses a significant
bottleneck. This is particularly demanding for complex problem-solving tasks, requiring significant
resources and expert knowledge. To address this hurdle, model-generated synthetic data emerges as
a promising alternative, offering scalability and cost-effectiveness, provided its quality can be ensured.
While LLMs hold the potential to self-evaluate generated data, this paper explores a simpler setting
where an external, scalar feedback signal serves as a quality indicator for each generated sample.
To investigate training on model-generated data, we consider a simple yet powerful self-training
approach for language models that requires only two capabilities: 1) generating samples from the
model and 2) evaluating these samples with a scoring mechanism. This approach shares similarities
withReinforcedSelf-Training(ReST)proposedbyGulcehreetal.(2023). Wemakesomemodifications
to ReST (detailed in Section 3), and call our approach ReSTğ¸ğ‘€. We show that ReSTğ¸ğ‘€can be viewed
as applying expectation-maximization for reinforcement learning (Dayan and Hinton, 1997; Peters
and Schaal, 2007), which we present formally in Section 3. Specifically, ReSTğ¸ğ‘€alternates between
the expectation and maximization steps:
Corresponding author(s): singhavi@google.com, rishabhagarwal@google.com, jcoreyes@google.com
Published in Transactions on Machine Learning Research (04/2024)
Â©2024 Google DeepMind. All rights reservedarXiv:2312.06585v4  [cs.LG]  18 Apr 2024

--- PAGE 2 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
1520253035404-shot T est Accuracy (%)GPT-4
LLaMA-2 70BWizardMath 70BMetaMath 70B
Inflection-1Llemma 34B
Llemma 7B 
Mistral 7B (maj@4)Minerva 62BMinerva 540B
PaLM 2-SPaLM 2-L
Grok-0 (33B)PaLM 2-L (ReSTEM)
PaLM 2-S (ReSTEM)Reasoning: MATH
304050600-shot Accuracy (%)PaLM 2-S*PaLM 2-LGPT-4
GPT-3.5 (ChatGPT)WizardCoder 15B
LLaMA-2 70BCode LLaMA 34BCode Llama Python 34B
Inflection-1
Mistral 7BGrok-0 (33B)PaLM 2-L (ReSTEM)
PaLM 2-S* (ReSTEM)Code Generation: HumanEval
Figure 1|Self-training with ReSTğ¸ğ‘€substantially improves test performance of PaLM 2 models on
two challenging benchmarks: MATH and HumanEval. Results for other models are shown for general
progress on these tasks and are typically not comparable due to difference in model scales. GPT-4
results are taken from Bubeck et al. (2023). The x-axis approximately denotes release time (not to
scale).
1.Generate (E-step): The language model generates multiple output samples for each input
context. Then, we filter these samples using a binary reward to collect the training dataset.
2.Improve (M-step): The original language model is supervised fine-tuned on the training
dataset from the previous Generate step. The fine-tuned model is then used in the next
Generate step.
ReSTğ¸ğ‘€, with its various adaptations (Section 4), has demonstrated success in enhancing language
models across diverse domains, including machine translation (Gulcehre et al., 2023; Norouzi et al.,
2016), semantic parsing (Agarwal et al., 2019), preference alignment (Dong et al., 2023), and ele-
mentary reasoning (Yuan et al., 2023; Zelikman et al., 2022). However, prior works primarily applied
training with self-generated data to relatively small language models (up to 7B parameters), with
limited scalability observed for larger models (Yuan et al., 2023). Complementing these efforts, our
work aims to investigate the effectiveness and scalability of model-generated synthetic data compared
to human-generated data in two challenging, less explored domains: competition-level mathematical
problem-solving (MATH) (Hendrycks et al., 2021b) and code generation (APPS) (Hendrycks et al.,
2021a).
Our empirical findings reveal significant advancements in both mathematical reasoning and code
generation capabilities when applying ReSTğ¸ğ‘€to PaLM 2 models of varying scales (Figure 1). Notably,
models fine-tuned on model-generated synthetic data exhibit remarkably larger performance gains
compared to those trained on human-written data (Figure 2, 3). Interestingly, exceeding a couple
of iterations of ReSTğ¸ğ‘€leads to diminishing improvement, indicating potential overfitting on small
amount of training problems (Figure 4). Additionally, models fine-tuned using ReSTğ¸ğ‘€improve
pass@k as well as majority voting performance. Furthermore, these fine-tuned models demonstrate
enhanced performance on related but held-out benchmarks, including math problems (GSM8K and
Hungarian HS finals), coding (HumanEval), and Big-Bench Hard tasks. We also perform ablation
studies to investigate the effect of number of model-generated solutions, training problems, and
iterations for ReSTğ¸ğ‘€fine-tuning. Overall, our findings suggest self-training with feedback as a
promising approach to reduce dependence on human data.
The key contributions of this work are:
â€¢We introduce ReSTğ¸ğ‘€that enables learning from self-generated data for LLMs, employing a
Published in Transactions on Machine Learning Research (04/2024) 2

--- PAGE 3 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
principled expectation-maximization approach within a reinforcement learning framework.
â€¢Wedemonstratethattrainingonself-generatedsolutionssurpassestrainingonhuman-generated
solutions in problem-solving domains, such as mathematics and code generation.
â€¢Throughcomprehensiveablationstudies,wepinpointthecrucialelementsnecessaryforattaining
optimal performance.
â€¢LLMs fine-tuned with ReSTğ¸ğ‘€exhibit robust transfer capabilities across various held-out tasks.
2. Preliminaries
An autoregressive language model produces an output sequence ğ’š=(ğ‘¦1,ğ‘¦2,....ğ‘¦ğ‘‡)given a context (or
source input) ğ’™=(ğ‘¥1,ğ‘¥2,...ğ‘¥ğ¿), where the tokens ğ‘¥ğ‘™,ğ‘¦ğ‘¡belong to a fixed vocabulary. Auto-regressive
generation involves predicting tokens one at a time, based on the previously generated tokens.
Assuming that the model is parameterized by ğœƒ, the conditional probability distribution of generating
a sequence ğ’šgiven ğ’™is
ğ‘ğœƒ(ğ’š|ğ’™)=ğ‘‡Ã–
ğ‘¡=1ğ‘ğœƒ(ğ‘¦ğ‘¡|ğ’š<ğ‘¡,ğ’™),
with the convention ğ’š1:0=âˆ…and ğ’š1:ğ‘¡âˆ’1=(ğ‘¦1,ğ‘¦2,....ğ‘¦ğ‘¡âˆ’1). For ease of notation, we define ğ‘(ğ‘¦ğ‘¡|ğ‘¥):=
ğ‘(ğ‘¦ğ‘¡|ğ‘¦<ğ‘¡,ğ‘¥). The probability of predicting ğ‘¡ğ‘¡â„tokenğ‘¦ğ‘¡,ğ‘(ğ‘¦ğ‘¡|ğ‘¥), is determined using a softmax with
temperature ğ›¾:ğ‘(ğ‘¦ğ‘¡|ğ‘¥)=exp(ğ‘§ğ‘¡/ğ›¾)Ãğ‘€
ğ‘–=1exp(ğ‘§ğ‘–/ğ›¾), whereğ‘§ğ‘¡is the logit score for the token ğ‘¦ğ‘¡. Higher values of
temperature ğ›¾introduces more randomness, while a lower value makes the output more deterministic
by favoring the most probable words.
Given a datasetDof inputs ğ’™and human-generated outputs ğ’š, supervised fine-tuning (SFT)
trains the policy by minimizing the negative log likelihood loss:
LSFT(ğœƒ)=âˆ’ğ”¼(ğ’™,ğ’š)âˆ¼D"ğ‘‡âˆ‘ï¸
ğ‘¡=1logğ‘ğœƒ(ğ‘¦ğ‘¡|ğ’š1:ğ‘¡âˆ’1,ğ’™)#
. (1)
We also assume access to a deterministic sequence-level (or terminal) reward ğ‘Ÿ(ğ’™,ğ’š). Then, the
reinforcement learning (RL) objective corresponds to:
LRL(ğœƒ)=ğ”¼ğ’™âˆ¼D
ğ”¼ğ’šâˆ¼ğ‘ğœƒ(ğ’š|ğ’™)[ğ‘Ÿ(ğ’™,ğ’š)]
.
OptimizingLRLloss directly using online RL methods, such as policy gradients, requires updating
and sampling from the policy numerous times during training. However, the computational cost of
fine-tuning on a continual flow of new samples becomes a limitation of online methods, especially
when the sizes of the policy network grow to tens or hundreds of billion parameters. We discuss an
alternative to such online RL approaches in the next section.
3. Expectation-Maximization for Reinforced Self-Training
Expectation-Maximization (EM) for RL We first describe the EM-based framework for RL with
language models, building upon the prior work by Dayan and Hinton (1997). Letâ€™s define a binary
optimality variable O, such that ğ‘(ğ‘‚=1|ğ’™,ğ’š)âˆğ‘“(ğ‘Ÿ(ğ’™,ğ’š)), for some non-decreasing non-negative
functionğ‘“:â„â†’â„+. We want to maximize the log-likelihood of observing ğ‘‚=1(obtaining high
reward):
logğ‘(ğ‘‚=1|ğ’™):=logâˆ‘ï¸
ğ’šğ‘ğœƒ(ğ’š|ğ’™)ğ‘(ğ‘‚=1|ğ’™,ğ’š).
Published in Transactions on Machine Learning Research (04/2024) 3

--- PAGE 4 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
However, the sum over all possible sequences ğ’šis typically intractable. Instead of maximizing
logğ‘(ğ‘‚=1;ğ’™), one can consider maximizing its ELBO ğ¿(ğ‘ğœƒ,ğ‘)with respect to parameters ğœƒand
variational distribution ğ‘(ğ‘¦|ğ‘¥). Specifically,
logğ‘(ğ‘‚=1|ğ’™)=logğ”¼ğ‘(ğ’š|ğ’™)ğ‘(ğ‘‚=1|ğ’™,ğ’š)ğ‘ğœƒ(ğ’š|ğ’™)
ğ‘(ğ’š|ğ’™)
â‰¥ğ”¼ğ‘(ğ’š|ğ’™)
logğ‘(ğ‘‚=1|ğ’™,ğ’š)ğ‘ğœƒ(ğ’š|ğ’™)
ğ‘(ğ’š|ğ’™)
(Jensenâ€™s inequality )
=ğ”¼ğ‘(ğ’š|ğ’™)[logğ‘(ğ‘‚=1|ğ’™,ğ’š)]âˆ’KL[ğ‘(ğ’š|ğ’™)||ğ‘ğœƒ(ğ’š|ğ’™)]
=:ğ¿(ğ‘ğœƒ,ğ‘) (2)
The EM algorithm (Dempster et al., 1977) for Equation 2 alternates between an E-step and M-step:
at iteration ğ‘¡, denote the language model parameter to be ğœƒğ‘¡and the variational distribution to be ğ‘ğ‘¡.
â€¢E-step:ğ‘ğ‘¡+1=arg max ğ‘ğ¿(ğ‘ğœƒğ‘¡,ğ‘). Sinceğ¿(ğ‘ğœƒğ‘¡,ğ‘)can be written asâˆ’ğ¾ğ¿[ğ‘(ğ’š|ğ’™)||ğ‘âˆ—(ğ’š|ğ’™)],ğ‘ğ‘¡+1(ğ’š|
ğ’™)âˆğ‘âˆ—(ğ’š|ğ’™):=ğ‘(ğ‘‚=1|ğ’™,ğ’š)ğ‘ğœƒğ‘¡(ğ’š|ğ’™). Thus, this step is equivalent to weighting the output
samples from conditional language model distribution based on their likelihood of obtaining
high rewards.
â€¢M-step:ğœƒğ‘¡+1:=arg max ğœƒğ¿(ğ‘ğœƒ,ğ‘ğ‘¡+1)=arg min ğœƒKL
ğ‘ğ‘¡+1(ğ’š|ğ’™)||ğ‘ğœƒ(ğ’š|ğ’™)
=arg min ğœƒÃ
ğ’šâˆ’ğ‘ğ‘¡+1(ğ’š|
ğ’™)logğ‘ğœƒ(ğ’š|ğ’™). As such, this step corresponds to maximizing a weighted negative log-likelihood
loss.
Alternating between above steps ensures a monotonic improvement in the ELBO: ğ¿(ğ‘ğœƒğ‘¡+1,ğ‘ğ‘¡+1)â‰¥
ğ¿(ğ‘ğœƒğ‘¡,ğ‘ğ‘¡+1)â‰¥ğ¿(ğ‘ğœƒğ‘¡,ğ‘ğ‘¡).
EM with non-negative rewards . If the rewards are non-negative and ğ‘“is set to the identity
function, then ğ‘(ğ‘‚=1|ğ’™,ğ’š)âˆğ‘Ÿ(ğ’™,ğ’š)which implies ğ‘ğ‘¡+1(ğ’š|ğ’™)âˆğ‘Ÿ(ğ’™,ğ’š)ğ‘ğœƒğ‘¡(ğ’š|ğ’™). In this scenario,
the updated policy parameters ğœƒğ‘¡+1resulting from the M-step at iteration ğ‘¡are given by:
ğœƒğ‘¡+1:=arg max
ğœƒğ”¼ğ‘¥âˆ¼Dh
ğ”¼ğ’šâˆ¼ğ‘ğ‘¡
ğœƒ(ğ’š|ğ’™)[ğ‘Ÿ(ğ’™,ğ’š)logğ‘ğœƒ(ğ’š|ğ’™)]i
. (3)
Comparing the above equation with the typical RL objective ( LRL) reveals the key distinction
between standard RL and EM-based RL: how output data is sampled. Standard RL continuously
updates the policy and uses this latest policy to collect data. In contrast, EM-based RL employs a fixed
sampling policy from the previous iteration, decoupling data collection from policy optimization. This
decoupling in EM-based approaches enables easier scaling to large policy networks, such as LLMs.
ReSTğ¸ğ‘€Motivated by the EM framework, we now discuss a simplified version of Reinforced Self-
Training (ReST) approach by Gulcehre et al. (2023). This approach, which we call ReSTğ¸ğ‘€, decouples
data collection (E-step) and policy optimization (M-step) in a typical RL pipeline. Algorithm 1 outlines
the ReSTğ¸ğ‘€algorithm with multiple iterations, where each iteration corresponds to one Generate
andImprove step. We describe these steps in detail below.
â€¢Generate (E-step): In this step, we generate a dataset Dğ‘–by sampling many output sequences
from the current policy ğ‘ğœƒ:Dğ‘–={(ğ’™ğ‘—,ğ’šğ‘—)|ğ‘
ğ‘—=1s.t. ğ’™ğ‘—âˆ¼D,ğ’šğ‘—âˆ¼ğ‘ğœƒ(ğ’š|ğ’™ğ‘—)}. Here, the inputs
are resampled from the original dataset ğ’™ğ‘—âˆ¼D. The output sequences in Dğ‘–are then scored
with a binary reward function ğ‘Ÿ(ğ’™,ğ’š). In our experiments, we condition the language model
using a few-shot prompt with programs for code generation and step-by-step solutions for math
problems.
Published in Transactions on Machine Learning Research (04/2024) 4

--- PAGE 5 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
Algorithm 1: ReST (Expectation-Maximization). Given a initial policy (e.g., pre-trained
LM), ReSTğ¸ğ‘€iteratively applies Generate andImprove steps to update the policy.
Input:D: Training dataset, Dğ‘£ğ‘ğ‘™: Validation dataset, L(ğ’™,ğ’š;ğœƒ): loss,ğ‘Ÿ(ğ’™,ğ’š): Non-negative
reward function, ğ¼: number of iterations, ğ‘: number of samples per context
forğ‘–=1toğ¼do
// Generate (E-step)
Generate datasetDğ‘–by sampling:Dğ‘–={(ğ’™ğ‘—,ğ’šğ‘—)|ğ‘
ğ‘—=1s.t. ğ’™ğ‘—âˆ¼D,ğ’šğ‘—âˆ¼ğ‘ğœƒ(ğ’š|ğ’™ğ‘—)}
AnnotateDğ‘–with the reward ğ‘Ÿ(ğ’™,ğ’š).
// Improve (M-step)
whilereward improves on Dğ‘£ğ‘ğ‘™do
Optimiseğœƒto maximize objective: ğ½(ğœƒ)=ğ”¼(ğ’™,ğ’š)âˆ¼Dğ‘–[ğ‘Ÿ(ğ’™,ğ’š)logğ‘ğœƒ(ğ’š|ğ’™)]
end
end
Output: Policyğ‘ğœƒ
â€¢Improve (M-step): In the ğ‘–ğ‘¡â„iteration, we use the new dataset Dğ‘–from Generate step to
fine-tune the policy ğ‘ğœƒ. To mitigate task-specific over-fitting, we minimize drift from the base
model by always fine tuning the base pretrained language model. For fine-tuning, we minimize
the reward-weighted negative log-likelihood loss ğ½(ğœƒ)=ğ”¼(ğ’™,ğ’š)âˆ¼Dğ‘–[ğ‘Ÿ(ğ’™,ğ’š)logğ‘ğœƒ(ğ’š|ğ’™)]. Once
the policy is improved, a new dataset of better quality samples can be created once again.
Differences with ReST (Gulcehre et al., 2023). Unlike ReST, we refrain from augmenting Dğ‘–in
Generate step with human-generated outputs as such data may not always be optimal for learning
or it might not be easily available. Furthermore, each Improve step fine-tunes the base model instead
of the model obtained from the previous ReST iteration. This results in comparable task-specific
performance but much better transfer performance on held-out tasks (see Figure 7).
Remark. Our experiments focus on problem-solving settings with binary rewards (either 0 or 1),
unlike the bounded real-valued rewards assumed by Gulcehre et al. (2023). Specifically, for each
Generate step, Gulcehre et al. (2023) perform multiple Improve steps, where each Improve step
can be viewed as an M-step with the function ğ‘“(ğ‘Ÿ(ğ’™,ğ’š))=ğ‘Ÿ(ğ’™,ğ’š)> ğœ, whereğœâˆˆâ„+increases in
successive M-steps. However, with binary rewards, any value of ğœâˆˆ(0,1)corresponds to the identical
Improve steps.
4. Related work
Several prior methods can be instantiated using the expectation-maximization framework presented
in Section 3. We discuss methods and their relation to ReSTğ¸ğ‘€in this section.
â€¢Expert Iteration (ExiT) (Anthony et al., 2017) alternates between two steps: expert improve-
ment and policy distillation. During the expert improvement step (E-step), we combine a base
policy with a search procedure to generate samples from a better policy, called the expert policy.
Then, in the policy distillation step (M-step), we use these expert samples to train the base
policy in a supervised way, effectively improving it to match the expert policy. While ExiT used
monte-carlo tree-search, we simply use temperature sampling for collecting samples from the
expert policy in ReST. That said, improving the E-step in ReST using the ExIT framework via
search and planning procedures with language models would be interesting for future work. For
example, Huang et al. (2022) implement a single iteration of ReSTğ¸ğ‘€on simple math reasoning
Published in Transactions on Machine Learning Research (04/2024) 5

--- PAGE 6 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
problems. However, unlike our setup, they do not assume access to a correctness reward and
instead employ majority-voting (Wang et al., 2023) as a search procedure within the E-step.
â€¢Self-Taught Reasoner (STaR) (Zelikman et al., 2022) employed greedy decoding instead of
temperature sampling for the E-step in ReSTğ¸ğ‘€, which is restricted to one model-generated
solution per problem during data collection. Additionally, STaR proposed rationalization as an
alternative to temperature sampling, where the language model is provided with the correct
answer as part of the input to generate correct solutions for difficult problems. However, in our
preliminary experiments, rationalization leads to substantial increase in false positive solutions
that result in correct answer but with incorrect reasoning.
â€¢Rejection Sampling Fine-tuning (RFT) (Yuan et al., 2023) improves reasoning performance
on GSM8K and corresponds to running a single generate (E-step) and improve (M-step) of
ReSTğ¸ğ‘€. WhileRFTdemonstratedlimitedperformanceimprovementsonGSM8Kwithincreasing
language model capacity, ReSTğ¸ğ‘€achieves larger gains on more challenging APPS and MATH
benchmarks when scaling PaLM 2 model capacity. Moreover, we observe that using multiple
iterations of ReSTğ¸ğ‘€result in larger performance gains.
â€¢IterativeMaximumLikelihood (IML)optimizesapolicyusingareward-weightedlog-likelihood
objective on self-collected data. IML has been shown to perform well with relatively small-scale
language models for semantic parsing (Agarwal et al., 2019; Liang et al., 2016), machine
translation (Wu et al., 2016) and simple math reasoning (Ni et al., 2022). Each E-step and
M-step in IML is performed over a mini-batch of training examples instead of the entire training
dataset, as done in ReSTğ¸ğ‘€. In IML, the learned policy can significantly diverge from the initial
pretrained model, which can manifest as task-specific overfitting, where the model performs
well on the target task but loses its ability to generalize to other tasks or domains. Additionally,
the tightly coupled nature of data collection and policy optimization in IML leads to high
computational cost with large LMs, making it significantly more expensive than ReSTğ¸ğ‘€.
â€¢Reward weighted regression (RWR) (Peters and Schaal, 2007) corresponds to EM where
we setğ‘(ğ‘‚=1|ğ’™,ğ’š)âˆexp(ğ‘Ÿ(ğ’™,ğ’š))in Section 3. RWR has been previously applied to robotic
control, as it can be easily applied to non-binary reward functions. Norouzi et al. (2016) build
on RWR to propose a general variant of IML for machine translation.
â€¢Reward ranked fine-tuning (RAFT) (Dong et al., 2023) can be interpreted as alternating
between E-step and M-step over mini-batches, where E-step uses the the output sample with
maximum reward for each input context. For binary reward functions, RAFT is analogous to
IML and as such, can be viewed as an instantiation of ReSTğ¸ğ‘€.
Other related works : TRICE (Phan et al., 2023) proposes an EM-based approach to maximize
the marginal log-likelihood (MML) of generating a correct answer for a reasoning problem, where the
chain-of-thought rationale is treated as a latent variable. While E-step in ReSTğ¸ğ‘€simply corresponds
to sampling from the model and filtering with a binary reward, TRICE uses Markov-chain Monte
Carlo with a control variate to approximate the MML gradient. Sordoni et al. (2023) propose a
gradient-free EM-based approach, similar to RAFT, for prompt-optimization for frozen LLMs.
Inspired by an earlier version of this manuscript, Agarwal et al. (2024) investigated if model-
generated data can outperform human data for few-shot and many-shot prompting. They found that
this is indeed the case, especially for few-shot prompting.
Published in Transactions on Machine Learning Research (04/2024) 6

--- PAGE 7 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
ReSTğ¸ğ‘€ReST STaR RFT
Starts from fine-tuned model âœ— âœ“ âœ— âœ—
Finetunes from base model in each iteration âœ“ âœ— âœ“ N/A
Uses rationalizations for unsolved questions âœ— âœ— âœ“ âœ—
Temperature sampling for exploration âœ“ âœ“ âœ— âœ“
Experiments with Large LMs âœ“ âœ— âœ— âœ“
Multiple iterations âœ“ âœ“ âœ“ âœ—
Larger gains on bigger models âœ“ N/A N/A âœ—
Evaluation on held out tasks âœ“ âœ— âœ— âœ—
Table 1|Differences between ReSTğ¸ğ‘€and other closely related approaches utilizing synthetic data
for advancing language model capabilities.
5. Experiments and analysis
The goal of our experiments is to answer the following questions:
1. How effective is ReSTğ¸ğ‘€compared to fine-tuning on human-generated data?
2.How many iterations are needed for optimal performance? How quickly does ReSTğ¸ğ‘€leads to
overfitting on training set?
3. How does ReSTğ¸ğ‘€affect pass@k and majority voting performance?
4.If we fine-tune using model-generated data on a specific task, do we see positive transfer
to related tasks? Is there any performance degradation compared to the base model when
evaluating our fine-tuned models on a broad suite of tasks?
5.How much input data do we need to get most of the performance gains from ReSTğ¸ğ‘€? Is one
iteration of ReSTğ¸ğ‘€sufficient?
Training Datasets . We evaluate ReSTğ¸ğ‘€primarily on mathematical problem solving using the
Hendrycksâ€™MATHdataset(Hendrycksetal.,2021b)andcodegenerationusingtheAPPS(Introductory)
dataset (Hendrycks et al., 2021a). MATH and APPS (Introductory) contain 7500 and 2342 training
problems respectively. We select these tasks because the model outputs can be automatically evaluated
as correct or incorrect, perfectly suited for ReSTğ¸ğ‘€. Both these datasets offer binary rewards: on
MATH, model-generated answers can be easily verified for correctness using the ground-truth answer,
while on APPS, test cases determine whether the generated code is correct.
Models. We use the PaLM 2 models (Google et al., 2023) with public APIs on Google Cloud for
experiments, including PaLM 2-S (Bison), PaLM 2-S* (Codey), and PaLM 2-L (Unicorn).
Evaluation . We report generalization performance using the test splits of the MATH and APPS
(Introductory) datasets. For measuring transfer performance, we look at GSM8K (Cobbe et al., 2021),
Hungarian HS finals (Paster, 2023), and HumanEval (Chen et al., 2021) datasets. We also evaluate
ourmodelsusingtheBig-BenchHard(Suzgunetal.,2022)benchmarktoevaluategeneralcapabilities.
All evaluations follow the settings from Google et al. (2023), unless specified otherwise.
Implementation Details . During each iteration of ReSTğ¸ğ‘€, we generated a fixed number of
solutions per problem for the E-step: 32 for the MATH dataset and 64 for the APPS dataset. For
generating solutions, we sample from the language model using top-K sampling with K=40 and
temperature of 0.7. However, directly using all these model-generated solutions can lead to an
imbalanced dataset, as we will have a lot more correct solutions for the easier problems. To mitigate
this, we introduced a cut-off threshold for the maximum number of solutions per problem, a design
Published in Transactions on Machine Learning Research (04/2024) 7

--- PAGE 8 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
0 1 2 3
Num iterations2025303540Pass@1 T est Accuracy (%)Hendrycks MATH
Palm-2-S Palm-2-L Palm-2-L-SFT Palm-2-S-SFT
0 1 2 3
Num iterations 607080Pass@1 T est Accuracy (%)Transfer to GSM8K
Figure 2|ReSTğ¸ğ‘€for math problem-solving . Test performance on MATH and GSM8K (transfer) for
PaLM 2-S* and PaLM 2-L as a function of ReSTğ¸ğ‘€iterations. We also report performance of models
fine-tuned via SFT on human-generated data as a baseline. Iteration 0 corresponds to pre-trained
model performance. Following Google et al. (2023), we use greedy decoding for evaluation.
0 1 2
Num iterations1820222426Pass@1 T est Accuracy (%)APPS (Introductory)
Palm-2-S* Palm-2-L Palm-2-L-SFT Palm-2-S*-SFT
0 1 2
Num iterations 40455055Pass@1 T est Accuracy (%)Transfer to HumanEval
Figure 3|ReSTğ¸ğ‘€for code-generation . Test performance on APPS (introductory) and Hu-
manEval (transfer) for PaLM 2-S* and PaLM 2-L as a function of ReSTğ¸ğ‘€iterations.
choice also used by Zelikman et al. (2022), included in the fine-tuning dataset: 10 for both MATH
and APPS. This approach ensures diversity in the training data and safeguards against overfitting
on easier problems. For fine-tuning, we use the few-shot prompt (and the question) as input to the
model, and use the model-generated solutions as targets. We only apply the next token prediction
loss (Equation 1) on the targets.
5.1. ReSTğ¸ğ‘€on MATH and APPS
Figures 2 and 3 show the performance of ReSTğ¸ğ‘€when trained on the MATH and APPS datasets,
respectively. We see that MATH benefits from performing multiple iterations of ReSTğ¸ğ‘€, both in terms
of performance on the MATH test set, as well as transfer to GSM8K. On the other hand, we see that
most of the gains for APPS come from the first iteration, and more iterations lead to a regression on
both APPS and HumanEval.
Interestingly, Figures 2 and 3 demonstrate that fine-tuning on model-generated solutions substan-
Published in Transactions on Machine Learning Research (04/2024) 8

--- PAGE 9 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
0 1 2 3
Num iterations 354045505560Pass@1 Performance (%)Hendrycks MATH
Palm-2-L (Train) Palm-2-L (T est)
0 1 2
Num iterations 2030405060Pass@1 Performance (%)APPS (Introductory)
Palm-2-L (Train) Palm-2-L (T est)
Figure 4|Train-test performance gap on (left) MATH with PaLM-2-L, and (right) APPS with PaLM-
2-S*, as a function of ReSTğ¸ğ‘€iterations.
tially outperforms using human-written solutions, especially for the PaLM 2-L model. This aligns with
findingsofYuanetal.(2023)andrecentworkondistillingLLMsusingmodel-generateddata(Agarwal
et al., 2023; Gu et al., 2023). However, unlike Yuan et al. (2023), who observed diminishing returns
from model-generated data on GSM8K when scaling model capacity, our results suggest an opposite
trend: ReSTğ¸ğ‘€leads to larger performance gains as model capacity increases. On the MATH dataset,
the test accuracy improvement with ReSTğ¸ğ‘€is5.94%for PaLM 2-S compared to 6.34%for the larger
PaLM 2-L model. Similarly, on the APPS dataset, improvements are 5.6%for PaLM 2-S* compared to
6.4% for PaLM 2-L. This is in addition to the fact that the larger models start with a much stronger
initial performance, and improvements on these benchmarks generally get harder as the baseline
performance goes up.
Train-test performance gap . Figure 4 shows that while training performance increases linearly
with the number of ReSTğ¸ğ‘€iterations, test set performance does not. For MATH, test performance
improvements are small after the first iteration, and for APPS, we observe a regression in performance
in the 2ğ‘›ğ‘‘iteration. We suspect that the regression in performance is likely due to overfitting on the
small set of training problems. Since the APPS dataset is about a third of the size of the MATH dataset,
it suffers more from this problem.
5.2. Impact on Pass@K and Majority-Voting Performance
To investigate the impact of fine-tuning with ReSTğ¸ğ‘€on the diversity of the final modelâ€™s generated
outputs, we evaluate pass@k (Chen et al., 2021) and majority voting (Wang et al., 2023) performance
of the fine-tuned PaLM 2-L model relative to the base model.
Pass@K measures the probability that at least one of the K generated solutions for a problem is
correct, that is, outputs the correct answer for math problems or passes all the unit tests for code
generation. Figure 5 shows the performance of Palm-2-L on the pass@K metric. We see that model
obtained after ReSTğ¸ğ‘€fine-tuning is stronger for all values of K, with the performance gap typically
being the highest for K=1.
Majority voting first samples a diverse set of reasoning paths instead of only taking the greedy
one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths.
For Hendrycks MATH, it is possible to use majority voting to maximize Pass@1 performance, and we
find that when using 64 samples per question, the PaLM 2-L fine-tuned with ReSTğ¸ğ‘€obtains a test
accuracy of 48.82, while the base model gets 44.02.
Published in Transactions on Machine Learning Research (04/2024) 9

--- PAGE 10 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
0 20 40 60
Num samples (K)40%60%80%Pass @ K T est Accuracy (%)HumanEval
PaLM-2-L
PaLM-2-L (ReST)
2 4 6 8 10
Num samples (K)10%20%30%40%Pass @ K T est Accuracy (%)APPS (Introductory)
PaLM-2-L
PaLM-2-L (ReST)
0 20 40 60
Num samples (K)20%30%40%50%60%70%80%Pass @ K T est Accuracy (%)Hendrycks MATH
Palm-2-L
Palm-2-L (ReST)
Figure 5|Pass@K results for PaLM-2-L pretrained model as well as model fine-tuned with ReSTğ¸ğ‘€.
ForafixednumberofsamplesK,fine-tuningwithReSTğ¸ğ‘€substantiallyimprovesPass@Kperformance.
We set temperature to 1.0 and use nucleus sampling with ğ‘=0.95.
5.3. Ablation Studies
Impact of multiple iterations Our results show that multiple iterations can sometimes lead to
over-fitting on the train set (Figure 4). This raises the question of whether multiple iterations are
really necessary. Is it better to collect a larger dataset and perform just a single iteration of ReSTğ¸ğ‘€?
To investigate this, we collect a dataset with the base PaLM-2-L model on Hendrycks MATH that is
3Ã—as many solutions per problem as used in a single iteration of ReSTğ¸ğ‘€for the E-step. Fine-tuning
with this dataset results in pass@1 performance of 40.3%, which is lower than the 41%in second
and41.9%in third iteration, as shown in Figure 2. These results indicate that performing multiple
iterations of ReSTğ¸ğ‘€leads to higher performance compared a single iteration with 3x the data.
Comparing model-generated data with human data A key strength of ReSTğ¸ğ‘€is its ability to
generate multiple correct solutions for each problem. This provides valuable additional training data
compared to human-generated data, which typically offers only a single solution per problem. While
this makes a comparison in Figures 2 and 3 not entirely fair, it also highlights the potential of ReSTğ¸ğ‘€
to boost performance with diverse and correct solutions.
In order to enable an apples-to-apples comparison, we conduct the following study: we select all
Hendrycks MATH questions for which we have at least one correct model-generated solution, resulting
in about 5K questions. For these 5K questions, we run two fine-tuning experiments: SFT(5K) where
we fine-tune on human-written solutions (one per question), and ReSTâˆ—(5K) where we fine-tune on
model-generated solutions (also one per question, selected at random).
The results in Figure 6 (right), show that ReSTğ¸ğ‘€outperforms fine-tuning on human data even in
this much more restricted setting. Furthermore, the efficacy of ReST(5K) over ReSTâˆ—(5K) highlights
the additional gain in performance that we can obtain by spending more compute on sampling a
large number of solutions and performing multiple iterations of ReSTğ¸ğ‘€.
Distillation with ReSTğ¸ğ‘€-generated data The above results indicate that self-generated data can
be better than human data for fine-tuning language models. We hypothesize this may be because
model-generated solutions are more in-distribution compared to human-written solutions. This raises
the question of whether ReSTğ¸ğ‘€-generated data can benefit different models than the one generating
the data.
To answer this question, we consider a distillation setup on MATH where we fine-tune PaLM 2-S
using data generated by PaLM 2-L, resulting in solutions for about 5K questions. Specifically, we ran
Published in Transactions on Machine Learning Research (04/2024) 10

--- PAGE 11 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
SFT (7K) SFT (5K)ReST* (5K)ReSTEM (5K)
Method (Num questions)3436384042Pass@1 Performance (%)Hendrycks MATH (T est)
SFT (Human) Distill* (2-L) ReSTEM (2-S)Distill (2-L)
Method (Data Source)15.017.520.022.525.027.530.0Pass@1 Performance (%)PaLM 2-S on Hendrycks MATH (T est)
Figure 6|Left. Comparing ReSTğ¸ğ‘€with SFT on MATH. SFT refers to fine-tuning on human data,
while ReST* refers to a version of ReSTğ¸ğ‘€with one iteration that uses only one correct sample per
problem. Here, ReST denotes ReSTğ¸ğ‘€with 3 iterations. For each method, we denote the number of
questions in parenthesis. Right. Impact of Model-Generated Data for Distillation.
two distillation experiments: Distillâˆ—(2-L) where we fine-tune on teacher-generated solutions (one
per question), similar to ReST (5K), and Distill (2-L), which includes multiple solutions per problem,
generated during the final iteration of ReSTğ¸ğ‘€with PaLM 2-L.
Our results, shown in Figure 6 (right), reveal that Distillâˆ—surpasses the performance achieved
by fine-tuning on human-written solutions, despite having smaller number of training questions.
Additionally, fine-tuning PaLM 2-S with multiple solutions from PaLM 2-L, namely Distill (2-L), is
superior than using self-generated solutions via ReSTğ¸ğ‘€. This improvement is likely due to the larger
number of training questions with solutions in PaLM 2-L generated data compared to 2-S. Overall,
these results indicate that model-generated data can be more effective for fine-tuning smaller models
than relying on human-generated data.
ReSTEMReST
Method21.021.221.421.621.822.0Pass@1 T est Accuracy (%)APPS (Introductory)
Palm-2-S*-SFTReSTEMReST
Method384042444648Pass@1 T est Accuracy (%)Transfer to HumanEval
Figure 7|ReSTğ¸ğ‘€vsReST using PaLM 2-S*.ReST vsReSTğ¸ğ‘€A major difference between
ReSTğ¸ğ‘€and ReST is that while ReSTğ¸ğ‘€always fine-
tunes the base model for each iteration, ReST con-
tinues to finetune the model from the last iteration.
We run an ablation comparing these options us-
ing PaLM 2-S* in Figure 7 and observe that while
ReST and ReSTğ¸ğ‘€have similar performance on
APPS, the transfer performance to HumanEval is
substantially better with ReSTğ¸ğ‘€.
Impact of dataset size Since one of the main ingredients needed for ReSTğ¸ğ‘€is a dataset of input
contexts (e.g., questions for MATH), we are interested in evaluating the effect of number of input
problems. The results from our dataset ablations using the PaLM-2-L model on Hendrycks MATH,
Figure 8 (left), show that utilizing just 1000 MATH questions results in significant gains, implying that
the method is very efficient in the number of prompts needed. However, we noted a slight decrease
in performance when using 4,000 questions compared to 2,000, indicating potential variance in
the fine-tuning process. Ideally, conducting this experiment multiple times would help quantify this
variance, but this is prohibitively resource-intensive. Overall, we find that ReSTğ¸ğ‘€is quite sample
efficient and performance gains from ReSTğ¸ğ‘€improve as we increase the dataset size.
Published in Transactions on Machine Learning Research (04/2024) 11

--- PAGE 12 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
0 1000 2000 4000 7000
Number of Questions3436384042Pass@1 Performance (%)Hendrycks MATH (T est)
Very Hard Hard Medium Easy
Problem Difficulty Level020406080100Average Success Rate (%)Average Success Rate by Difficulty Level
Base Model
ReSTEM
Figure 8|Left. Performance for a single iteration of ReSTğ¸ğ‘€as a function of dataset size (number of
questions) on MATH. Right. Improvement from ReSTğ¸ğ‘€based on the difficulty level of the question.
WhichQuestionsBenefitMostfromReSTğ¸ğ‘€WeevaluatetheperformanceenhancementofReSTğ¸ğ‘€
across different question difficulties in the Hendrycks MATH dataset. Questions are classified based
on success rates from the base model at a temperature setting of T=1.0 into four categories: â€œeasyâ€
(answered correctly 75%-100% of the time), â€œmediumâ€ (50%-75%), â€œhardâ€ (25%-50%), and â€œvery
hardâ€(below25%). Figure8(right)presentstheaveragesuccessratesforthesecategories, comparing
the base model to the ReSTğ¸ğ‘€-finetuned model. The results demonstrate that ReSTğ¸ğ‘€consistently
improves performance across all difficulties, with the highest gains coming for questions categorized
as medium and hard.
5.4. Impact on Reasoning capabilities
Boolean ExpressionsCausal JudgementDate UnderstandingDisambiguation QADyck LanguagesFormal FallaciesGeometric ShapesHyperbaton
Movie RecommendationMulti-step Arithmetic [T wo]Navigate
Object CountingPenguins in a T able
Reasoning about Colored ObjectsRuin Names
Salient Translation Error DetectionSnarks
Sports UnderstandingT emporal SequencesWeb of LiesWord Sorting
Logical Deduction (avg)
Tracking Shuffled Objects (avg)
Big-Bench Hard (BBH) T ask30405060708090100Few-shot Performance with CoTPaLM 2-L PaLM 2-L (MATH) PaLM 2-L (APPS)
CoT Direct
Prompt Type6065707580Average BBH PerformancePaLM 2-L
PaLM 2-L (APPS)
PaLM 2-L (MATH)
Figure 9|Comparing the ReSTğ¸ğ‘€models to the base model on the Big-Bench Hard suite of tasks.
Evaluations were conducted across multiple checkpoints, and the vertical black lines denote standard
deviation.
General capabilities . BIG-Bench provides a suite of over 200 tasks that can be used to probe
LLMsâ€™ performance across a range of fields and capabilities. BIG-Bench Hard (BBH) (Suzgun et al.,
Published in Transactions on Machine Learning Research (04/2024) 12

--- PAGE 13 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
2022) is a subset of 23 BIG-Bench tasks where the previous generation of LLMs, such as Codex
and PaLM 540B, performed below the average human rater. We follow the protocol of Google et al.
(2023) and evaluate on BBH using both few-shot and chain-of-thought prompting. Figure 9 shows the
performance of ReSTğ¸ğ‘€-finetuned models, and compares them against the base PaLM-2 model. We
see no major degradation on any of the BBH tasks. Furthermore, the model fine-tuned on Hendrycks
MATH outperforms the base model on this suite when using chain-of-thought prompting, and the
model fine-tuned on APPS also shows slight performance gains. When using direct prompting, all
three models perform similarly.
Problem-solving . To stress test the math problem-solving capabilities on a held-out â€œreal-world"
evaluation set, we evaluate our model on the 2023 Hungarian high school finals exam in mathematics,
following the evaluation protocol from Paster (2023). Specifically, we evaluate the PaLM 2-L model,
fine-tuned with ReSTğ¸ğ‘€on Hendrycks MATH, using the 1-shot prompt from Grok, sample solutions
using temperature 0.1, and manually grade the outputs using the rubric provided by the examiners.
The results from evaluation are shown in Figure 10. We find that PaLM-2-L fine-tuned with ReSTğ¸ğ‘€
performs well on this exam, surpassing the performance of all existing models except GPT-4.
20 30 40 50 60 70
Hungarian HS Finals Exam Score (%)30405060708090GSM8K Score (%)MetaMath 7BMetaMath Mistral 7B OpenChat 3.5
Code Llama 34BLlemma 34BGPT-3.5 TurboGPT-4
Grok-0 (33B)Grok-1
Qwen 7BClaude 2
Mistral 7BMAmmoTH 7BPaLM 2-L (ReSTEM)Exam Score vs GSM8K Performance of Various Models
Figure 10|Transfer results on Hungarian HS Finals Exam. Results for models other than PaLM-2-L
finetuned with ReSTğ¸ğ‘€are taken from Paster (2023). Several models specialized for mathematics
perform well on the widely-used GSM8K benchmark but perform poorly on the Hungarian exam. In
contrast, PaLM 2-L model fine-tuned with ReSTğ¸ğ‘€performs well on both these benchmarks.
6. Discussion
In this paper, we propose training on model-generated data combined with a reward function,
via ReSTğ¸ğ‘€, for improving the performance of LLMs on problem-solving tasks. Furthermore, we
demonstrate that ReSTğ¸ğ‘€is theoretically grounded in the application of expectation-maximization
to RL. We evaluate ReSTğ¸ğ‘€on mathematical problem solving and code generation, and show that
ReSTğ¸ğ‘€offers significant performance gains at a relatively low computational cost, especially when
compared to the cost of pre-training. Our experiments also show that ReSTğ¸ğ‘€does not lead to
regression on other tasks. We conduct a number of ablations to better understand the strengths and
weaknesses of this method, and find that it is data-efficient, but also requires some vigilance to avoid
over-fitting.
ThereareanumberoflimitationsassociatedwithReSTğ¸ğ‘€. First,thismethodrequiresamoderately-
sized training set of problems or prompts, which would need to be collected (from humans) for any
new task of interest. Second, ReSTğ¸ğ‘€also requires access to a manually-designed or learned reward
Published in Transactions on Machine Learning Research (04/2024) 13

--- PAGE 14 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
function, ideally one that can be computed automatically. Finally, while ReSTğ¸ğ‘€allows significant
performance improvements in pass@1 performance, it may not quite close the gap to pass@K
performance for the same task (with a sufficiently large K). Future research in self-improvement in
language models should focus on automating manual parts of the pipeline (likely through language
models as well), and explore algorithmic improvements that reduce the gap to pass@K performance.
Acknowledgements
We would like to thank Tom Le Paine for providing feedback to an early draft. We also acknowledge
Benjamin Anderson, Sridhar Thiagarajan, Feryal Behbahani, Aleksandra Faust, Doina Precup, Olivier
Bachem, and Slav Petrov for helpful discussions.
Author Contributions
Avi, Rishabh, and JD jointly led the project. Avi was responsible for training and evaluation infrastruc-
ture, ablations and experiments on MATH, JD led the experiments on APPS, Rishabh was responsible
for the paper writing, evaluations, and distillation ablations.
Ankesh, Piyush, Ethan, and Behnam observed preliminary findings about efficacy of model-
generated data on MATH for Minerva models and motivated this research. Piyush also helped Avi
in setting up infrastructure. Xavier, Peter, James, Jaeheoon, Kelvin and Yamini took part in project
discussions. Jascha and Noah sponsored and advised the project. All other authors provided feedback
on this work.
References
R. Agarwal, C. Liang, D. Schuurmans, and M. Norouzi. Learning to generalize from sparse and
underspecified rewards. In International conference on machine learning , pages 130â€“140. PMLR,
2019.
R.Agarwal, N.Vieillard, P.Stanczyk, S.Ramos, M.Geist, andO.Bachem. Gkd: Generalizedknowledge
distillation for auto-regressive sequence models. arXiv preprint arXiv:2306.13649 , 2023.
R. Agarwal, A. Singh, L. M. Zhang, B. Bohnet, S. Chan, A. Anand, Z. Abbas, A. Nova, J. D. Co-Reyes,
E. Chu, F. Behbahani, A. Faust, and H. Larochelle. Many-shot in-context learning, 2024.
T. Anthony, Z. Tian, and D. Barber. Thinking fast and slow with deep learning and tree search.
Advances in neural information processing systems , 30, 2017.
S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. M.
Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks of artificial general intelligence:
Early experiments with GPT-4. CoRR, abs/2303.12712, 2023. doi: 10.48550/ARXIV.2303.12712.
URLhttps://doi.org/10.48550/arXiv.2303.12712 .
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,
N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,
B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P.
Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol,
A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
J.Leike, J.Achiam, V.Misra, E.Morikawa, A.Radford, M.Knight, M.Brundage, M.Murati, K.Mayer,
Published in Transactions on Machine Learning Research (04/2024) 14

--- PAGE 15 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.
K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,
R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv
preprint arXiv:2110.14168 , 2021.
P. Dayan and G. E. Hinton. Using expectation-maximization for reinforcement learning. Neural
Computation , 9(2):271â€“278, 1997.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society: series B (methodological) , 39(1):1â€“22, 1977.
H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang. Raft: Reward ranked
finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767 , 2023.
Google, R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,
Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 , 2023.
Y. Gu, L. Dong, F. Wei, and M. Huang. Knowledge distillation of large language models. arXiv preprint
arXiv:2306.08543 , 2023.
C. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant, A. Ahern,
M. Wang, C. Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint
arXiv:2308.08998 , 2023.
D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He,
D. Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938 ,
2021a.
D.Hendrycks,C.Burns,S.Kadavath,A.Arora,S.Basart,E.Tang,D.Song,andJ.Steinhardt. Measuring
mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 , 2021b.
J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han. Large language models can self-improve.
CoRR, abs/2210.11610, 2022. doi: 10.48550/ARXIV.2210.11610. URL https://doi.org/10.
48550/arXiv.2210.11610 .
C. Liang, J. Berant, Q. Le, K. D. Forbus, and N. Lao. Neural symbolic machines: Learning semantic
parsers on freebase with weak supervision. arXiv preprint arXiv:1611.00020 , 2016.
A. Ni, J. P. Inala, C. Wang, A. Polozov, C. Meek, D. Radev, and J. Gao. Learning math reasoning from
self-sampled correct and partially-correct solutions. In The Eleventh International Conference on
Learning Representations , 2022.
M. Norouzi, S. Bengio, N. Jaitly, M. Schuster, Y. Wu, D. Schuurmans, et al. Reward augmented
maximum likelihood for neural structured prediction. Advances In Neural Information Processing
Systems, 29, 2016.
OpenAI. Gpt-4 technical report, 2023.
K. Paster. Testing language models on a held-out high school national finals exam. https://
huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam , 2023.
J. Peters and S. Schaal. Reinforcement learning by reward-weighted regression for operational space
control. In Proceedings of the 24th international conference on Machine learning , pages 745â€“750,
2007.
Published in Transactions on Machine Learning Research (04/2024) 15

--- PAGE 16 ---
Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models
D. Phan, M. D. Hoffman, D. Dohan, S. Douglas, T. A. Le, A. Parisi, P. Sountsov, C. Sutton, S. Vikram,
and R. A. Saurous. Training chain-of-thought via latent-variable inference. arXiv preprint
arXiv:2312.02179 , 2023.
A. Sordoni, X. Yuan, M.-A. CÃ´tÃ©, M. Pereira, A. Trischler, Z. Xiao, A. Hosseini, F. Niedtner, and
N. Le Roux. Joint prompt optimization of stacked llms using variational inference. In Thirty-seventh
Conference on Neural Information Processing Systems , 2023.
M. Suzgun, N. Scales, N. SchÃ¤rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi,
D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv
preprint arXiv:2210.09261 , 2022.
X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-
consistency improves chain of thought reasoning in language models. In The Eleventh International
Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net,
2023. URL https://openreview.net/pdf?id=1PL1NIMMrw .
Y.Wu,M.Schuster,Z.Chen,Q.V.Le,M.Norouzi,W.Macherey,M.Krikun,Y.Cao,Q.Gao,K.Macherey,
et al. Googleâ€™s neural machine translation system: Bridging the gap between human and machine
translation. arXiv preprint arXiv:1609.08144 , 2016.
Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning mathematical
reasoning with large language models. arXiv preprint arXiv:2308.01825 , 2023.
E. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances
in Neural Information Processing Systems , 35:15476â€“15488, 2022.
Published in Transactions on Machine Learning Research (04/2024) 16
