# 2305.11461.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/math/2305.11461.pdf
# File size: 296230 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Hint of Thought prompting: an explainable and zero-shot approach to
reasoning tasks with LLMs
IokTong Lei
Department of Computer Science
Tsinghua University
China, Beijing
lixt22@mails.tsinghua.edu.cnZhidong Deng
Department of Computer Science
Tsinghua University
China, Beijing
michael@mail.tsinghua.edu.cn
Abstract
Prompting becomes an increasingly important
research topic for better utilization of LLMs.
Although simple prompting performs well on
single-step questions, it cannot permanently ac-
tivate the correct knowledge path for multi-step
reasoning tasks. The chain of thought (CoT),
which often contains zero-shot CoT and few-
shot CoT, is a recently developed prompting
method that can explain the reasoning process
to the LLM and outperforms simple prompting
in three challenging reasoning tasks, including
arithmetic, symbolic, and commonsense rea-
soning. Inspired by zero-shot CoT, and further
extending the zero-shot ability, this paper pro-
poses a novel hint of thought (HoT) prompting
with explain-ability and zero-shot logicalbility.
It is decomposed into three steps: explainable
sub-questions, logical reasoning, and answer-
ing. Such three steps are sequentially ordered
in step-by-step hints, which can be easily ad-
justed and explained to different tasks. Finally,
experimental results demonstrate that our HoT
prompting has a significant advantage on the
zero-shot reasoning task compared to zero-shot
CoT. We did zero-shot experiments on math
tasks like GSM8K, ADDSUB, AQUA, SV AMP,
and commonsense tasks such as StrategyQA.
In particular, the accuracy of the proposed
HoT prompting is improved with GSM8K from
40.50% to 70.65%, with AQUA from 31.9% to
46.4%, with SV AMP from 63.7% to 76.9%, and
with ADDSUB from 74.7% to 87.34%, respec-
tively, which even defeats the competitive PoT
approach on GSM8k, AQUA, and SV AMP.
1 Introduction
Many researchers indicate that scaling up the size
of generative language models and training datasets
plays a critical role in recent NLP research. There
are some powerful examples of large language mod-
els (LLMs), such as ChatGPT (Leiter et al., 2023),
PaLM (Chowdhery et al., 2022), and LAMDA
(Thoppilan et al., 2022). Indicated by the robust-ness of GPT-4 (OpenAI, 2023), many valuable ap-
plications can be developed, and (Bubeck et al.,
2023) believes that it has the potential to lead the
world of artificial intelligence (AI) to the world of
artificial general intelligence (AGI).
The success of LLMs is often related to zero-shot
or few-shot learning (in-context learning). This at-
tribute helps the model to understand and solve
different tasks by sampling a few examples (in-
context learning) or only providing the instructions
(zero-shot). The prompt engineering (Liu et al.,
2021), or prompting, is a method for the users to
interact with LLMs. To implement this method, re-
searchers either design prompts manually (Schick
and Schütze, 2021; Reynolds and McDonell, 2021)
or generate them automatically (Gao et al., 2021;
Shin et al., 2020) per task. Recently, prompt engi-
neering has become a hot topic in NLP.
Although there is a fantastic performance that
LLMs can solve single-step or intuitive tasks very
well with task-specific zero-shot prompting or in-
context learning prompting (Liu et al., 2021), mod-
els with scale up to 100B+ parameters are strug-
gling to solve multi-step reasoning tasks (Rae et al.,
2022). To address this drawback, researchers have
introduced a method called chain of thought (CoT)
prompting (Wei et al., 2022), rather than standard
question and answer examples prompting, and it
feeds LLMs with a chain of reasoning examples.
In other words, CoT demonstrates a reasoning path
that composes an original complex question into
multiple more straightforward steps. With CoT,
the performance of language models on a large
scale has dramatically improved. For example, a gi-
ant PaLM with 540B parameter (Chowdhery et al.,
2022) with CoT can increase the accuracy of math
calculations.
A few-shot prompting with CoT (Wei et al.,
2022) or without CoT (Brown et al., 2020) can
finish many task-specific prompting tasks. Decom-
posed Prompting (Khot et al., 2023) proposed aarXiv:2305.11461v7  [cs.AI]  8 Sep 2024

--- PAGE 2 ---
model to decompose the task into sub-tasks and
solve them iterative; however, it requires few-shot
prompts and has to prompt a few times to get the
result. To do zero-shot reasoning, researchers pro-
posed zero-shot CoT, showing that LLMs are good
zero-shot reasoners by simply adding "let’s think
step by step" (Kojima et al., 2022). However, the
solving process is provided by the LLM without
any explanation between each step. Also, Pro-
gram of Thought (PoT) (Chen et al., 2023) per-
formed better using Python as an extended tool for
zero-shot math reasoning but mainly focused on
math reasoning tasks. Both works show the diffi-
culty of zero-shot reasoning tasks with LLMs. To
build an explainable, logical, and flexible zero-shot
prompt method, we propose the hint of thought
(HoT) prompting. To approach the solution, we
take the traditional zero-shot CoT as an activation
path that activates the correct reasoning path within
LLMs. We propose a hint chain that is explain-
able to humans and efficient for the LLM to un-
derstand. It contains three main parts (1) ask for
five subquestions; (2) ask pseudocode as an an-
swer for subquestions; (3) get the answer in a
wanted way . The hint chain automatically leads
the LLM to do zero-shot reasoning by asking small
sub-questions and answering sub-questions in pseu-
docode. The output of the sub-questions can help
users understand the “mind map” of the LLM, and
pseudocodes provide a more transparent and logi-
cal reasoning process.
Moreover, answering in pseudocode can avoid
semantic ambiguity. Because of this advantage,
HoT can give a more precise reasoning process than
zero-shot CoT. We used GPT-3.5-turbo (Brown
et al., 2020) as our LLM, a fine-tuned question-
answering model. We chose this model because
it is a chat model, and it is more likely to be used
as a chatbot, which is relatively weaker in specific
reasoning tasks. However, HoT can ideally use its
conversation ability to interpret the question and
increase its reasoning ability to avoid its weakness.
2 Background
2.1 Large language models and prompting
A language model (LM) is a model that is designed
to estimate the probability distribution of text. In re-
cent research, they found that scaling up the model
size can help improve the performance (from a few
million (Merity et al., 2016) to hundreds of millions
(Devlin et al., 2019) to hundreds of billions (Brownet al., 2020) parameters). And the training data
also becomes more extensive, e.g., webtest corpora
(Gao et al., 2020). These improve the abilities of
pre-trained LLMs in many downstream NLP tasks.
Unlike the classic paradigm of "pre-train and fine-
tune," an LLM that scales to 100B+ parameters
displays the ability to few-shot learning (Brown
et al., 2020). Using in-context learning, we can
use prompts to strongly lead the generation to out-
put a desired answer to a specific task. It is called
pre-train and prompts (Liu et al., 2021).
2.2 Zero-shot CoT (Kojima et al., 2022)
Based on the drawback of the few-shot CoT that
costs time and people to design the prompt, (Ko-
jima et al., 2022) proposed a zero-shot CoT prompt.
They added "Let’s think step by step" or a similar
text. This work showed that the potential of LLMs
is zero-shot. Compared with standard zero-shot
and zero-shot CoT, the latter significantly improved
with GSM8K from 12.5% to 40.5%. However,
"Let’s think step by step" is a settled prompt that
can not be flexible in every reasoning task. And
this inspired us to invent a more flexible zero-shot
prompt.
2.3 Program of Thought (PoT) (Chen et al.,
2023)
To make more accurate calculations on math tasks.
PoT proposed the use of an extended tool to cal-
culate the answer. In their work, they ask the
LLM to propose and execute a Python code to
get the math answer. In their zero-shot exper-
iment, PoT performs better than zero-shot CoT
on GSM8K (57.0%), AQuA (43.9%), SV AMP
(70.8%), TabMWP (66.5%), MultiArith (92.2%),
and Avg (66.1%). However, this program-based
work does not interpret the answers well. And this
inspired us to invent a more interpretable prompt.
3 HoT
To increase the flexibility of zero-shot CoT and
the interpretation of PoT. We propose an explain-
able and logical hint of thought (HoT) prompting
that uses reasoning paths to represent knowledge to
activate more profound reasoning within an LLM.
In our work, a hint chain (in the step-by-step for-
mat) is used as the input of LLMs, and the LLMs
themselves are required to generate explainable
semantic-level step-by-step reasoning steps with-
out any extra prompts. This employs a chain-like
instruction to activate more profound knowledge

--- PAGE 3 ---
Figure 1: HoT examples on GSM8K
within the LLM. Consequently, the input prompt-
ing to the LLM is " [X]. Hints: hint _chain " where
[X]is the question, and hint _chain is our HoT
prompting.
3.1 Flexible Hint chain
The hint chain is a chain of instructions that deliv-
ers the LLM with instructions step-by-step. The
purpose of the hint chain is to tell the LLM how to
generate a reasoning chain. There are three parts
of such hint chains: (1) Explainable sub-questions:
ask the LLM to partition the question into five step-
by-step sub-questions; (2) Logical reasoning: tell
the LLM to use the step-by-step sub-questions to
generate a pseudocode and calculate the answer;
(3) Answering: ask the LLM to give a customized
format of the answer. The result produced by the
hint chain follows the order of these three parts
and provides a consequential reasoning chain that
supports its answer.
3.2 Explainable question decomposition and
sub-question-wise problem solving
Although the LLM is in the back box represen-
tation, our HoT can produce an understandable
and explainable consequent problem-solving pro-
cedure. It indeed helps the users verify the answer
more intuitively. Sub-questions help the LLM solve
the whole problem and transparently explain the
LLM’s mind map. In our work, a intuitive and
logical reasoning process can be seen.
3.3 Logical pseudo-code reasoning
An interpretable problem-solving chain can en-
hance LLM’s ability to solve reasoning problems
more accurately. However, although LLM is de-
signed to work with language, it is likely to havesemantic ambiguity in semantic-level reasoning.
Therefore, pseudocode, regarded as a more accu-
rate logical language in programming design, is
used in HoT and this produce a higher reasoning
accuracy.
4 Experimental Results
All the experiments done with our HoT are based
on the GPT-3.5 family with GPT-3.5-turbo. Our
baselines are adopted from zero-shot CoT (Kojima
et al., 2022) with text-davinci-002 as well as PoT
(Chen et al., 2023) with code-davinci-002. Ad-
ditionally, text-davinci-002 and code-davinci-002
also belong to the GPT-3.5 family.
4.1 Tasks and datasets
We evaluate the HoT prompting on five datasets for
the four main arithmetic reasoning tasks: GSM8K,
AQUA, SV AMP, and ADDSUB. In addition, we
complete experiments on the big commonsense rea-
soning benchmark StrategyQA. Note that all the
datasets we utilize in our experiments are publicly
released. And LLMs have a randomness property,
and the extract accuracy might not always be repro-
duced.
Table 1: Number of samples in different datasets
Dataset Number of samples
GSM8K 1319
AQUA 254
SV AMP 1000
ADDSUB 395
StrategyQA 2290

--- PAGE 4 ---
4.2 Arithmetic tasks
For the arithmetic reasoning task, the following
four benchmarks are considered: i.e., 1) GSM8K
(Cobbe et al., 2021), 2) AQUA-RAT (Ling et al.,
2017), 3) SV AMP (Patel et al., 2021), and 4)
AddSub (Hosseini et al., 2014). The first three
datasets, especially GSM8K, are more recently pub-
lished benchmarks and have more challenges be-
cause they require multi-step reasoning to solve
problems. Also, the AQUA is a multiple-choice
question dataset. The baseline results are quoted
from zero-shot CoT (Kojima et al., 2022) and zero-
shot experiment done in PoT (Chen et al., 2023).
We can observe that HoT performs the best among
the zero-shot reasoning prompts with GPT3s. We
design the HoT prompts here, requiring a numeral
answer in the third part of the hint chain. From the
experiment results, it is easy to see that HoT has a
higher potential for arithmetic reasoning than the
previous zero-shot approaches.
Table 2: Results on GSM8K and AQUA
Method GSM8K AQUA
zero-shot CoT 40.50% 31.9%
PoT with zero-shot 57.0% 43.9
HoT (Ours) 70.65% 46.4%
Table 3: Results on SV AMP and ADDSUB
Method SV AMP ADDSUB
zero-shot CoT 63.7% 74.7%
PoT with zero-shot 70.8% -
HoT (Ours) 76.9% 87.34%
4.3 Commonsense tasks
We employ StrategyQA (Geva et al., 2021) for the
commonsense reasoning task, a large dataset that
requires the model to conduct implicit multi-hop
reasoning to answer questions. The baseline results
are from zero-shot CoT (Kojima et al., 2022). The
reasoning accuracy with HoT is about 30% higher
than CoT. We make the HoT prompt here, which
requires a Yes/No in the third part of the hint chain.
It is readily observed from the results that HoT has
a higher potential for commonsense reasoning than
the existing zero-shot prompt approach.
4.4 Answer verification
GPT-3.5-turbo always answers (output) in a com-
plete sentence, making it difficult to automaticallyTable 4: Results on StrategyQA
Method StrategyQA
zero-shot CoT 52.3%
HoT (Ours) 82.96%
verify if the output is the same answer as that in the
metrics. To check the answer, we independently
prompt the LLM again with its answer as a record.
In this verification prompt, we take the model an-
swer as user input and verify if it’s the same as the
answer given by the LLM.
To verify the reliability of our verification
method, we produced a correct and wrong vali-
dation by manually setting up the positive and neg-
ative samples in GSM8k, the largest dataset within
our experiment. We manually set a prompt that
always has the fake or correct answer: " Therefore,
the answer is fake or correct_answer " and then we
follow our verification process and gain a result.
In the correct validation test, we obtain 1293 true
positive samples out of 1319 samples. We obtain
1299 false negative samples in the wrong validation
test out of 1319. So, the correct validation rate is
98.02%, and the wrong validation rate is 98.48%.
This shows the reliability of our verification method
is strong.
5 Error Analysis
We divide the reasoning errors into (1) reasoning
errors and (2) calculation errors. Our experimental
results illustrate that reasoning errors always occur
when the question has semantic ambiguity or if it
is too complex. Avoiding such errors is difficult
because it’s challenging even for a human.
6 Ablation study
We remove the subquestions and pseudocode parts
separately and perform a GSM8K test. The result
shows that HoT with only subquestions performs
similarly to zero-shot CoT in terms of accuracy
and interoperability. Meanwhile, HoT with only
pseudo-code produces accuracy similar to that of
HoT but with less interoperability with only out-
putting pseucdocodes. This experiment shows that
the pseudocode part gives the logical reasoning
ability of HoT, and the subquestions part gives in-
teroperability.

--- PAGE 5 ---
Table 5: Ablation study on GSM8K
Method Accuracy
HoT 70.65%
HoT without pseudo-code 42.3%
HoT without subquestions 69.2%
7 Related work
7.1 Complex reasoning with LLMs
Reasoning skills are essential for general intelli-
gence systems, and the ability to reason in LLMs
gained significant attention from the research com-
munity. Several studies (Brown et al., 2020; Cobbe
et al., 2021) have shown that asking pre-trained
models to produce step-by-step reasoning or fine-
tuning can increase their ability on complex rea-
soning tasks. GPT-3 (Brown et al., 2020) has illus-
trated its robust few-shot reasoning that a few exam-
ples in natural language are given to the model to
describe the task. The most classic reasoning task
is mathematical reasoning. PoT (Chen et al., 2023)
has shown great ability on math reasoning tasks
with LLMs with the help of Python programs. They
aim to generate an executive Python program by
the LLM to solve math problems. However, their
work primarily focuses on math reasoning tasks. A
more general approach would be CoT (Wei et al.,
2022), which works well on mathematical, logical,
common sense, and symbolic reasoning tasks with
few-shot prompts.
7.2 Zero-shot reasoning with LLMs
It was indicated that LLMs have excellent zero-
shot abilities in many system-1 tasks, including
reading comprehension, translation, and summa-
rization (Radford et al., 2019). This ability can also
be fine-tuned to get a better performance. However,
we focus on system-2 tasks beyond system-1 tasks.
The recent work, zero-shot CoT, increases zero-
shot performance. Also, PoT in zero-shot format
provides good results in math reasoning tasks.
7.3 Discussion about Existing Work
Recently, there have been many approaches to en-
hance the reasoning ability of LLMs, including
CoT (Wei et al., 2022), zero-shot CoT (Kojima
et al., 2022), Auto-CoT (Shin et al., 2020), PoT
(Chen et al., 2023), decomposed prompting (Khot
et al., 2023). They all aim to provide accurate rea-
soning results. On the other hand, they did notfocus on explanations of answers and the LLM ef-
ficiency. Therefore, HoT is proposed as another
approach to prompt engineering.
8 Discussion
In our work, we approach reasoning tasks from
other perspectives. We care about accuracy and
want a more precise explanation of the reasoning
process. We have verified that HoT can work effi-
ciently on arithmetic and common-sense reasoning
tasks and provide a clear description intuitively. Al-
though LLMs are black boxes, we tried to explain
them with a prompt approach.
9 Conclusion
This paper investigates how to generate more in-
tuitively explainable and flexible reasoning task
prompts. Our experimental results on the mathe-
matical reasoning tasks significantly surpass those
of prior zero-shot methods with the GPT-3.5 fam-
ily. Also, on the common sense benchmark Strat-
egyQA, HoT reached a high accuracy of 82.96%
with GPT-3.5-turbo.
10 Limitation and Social Impact
Our work is based on GPT-3, a pre-trained lan-
guage model trained from various sources and
shown to capture and amplify biases found in the
training data. We use prompting to guarantee our
reasoning answers, which takes advantage of the
patterns learned by language models. However,
our zero-shot approach directly probes complex
reasoning inside pre-trained LLM, which can also
cause bias. Also, our ability to reason is based on
the power of the LLM. Therefore, the accuracy de-
pends on GPT-3, which may cause fluctuation in
different test environments.
References
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, et al. 2020. Language
models are few-shot learners.
Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-
ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,
and Yi Zhang. 2023. Sparks of artificial general in-
telligence: Early experiments with gpt-4.

--- PAGE 6 ---
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W. Cohen. 2023. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, et al. 2022. Palm: Scaling language
modeling with pathways.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy. 2020. The pile: An
800gb dataset of diverse text for language modeling.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 3816–3830, Online. Association for Computa-
tional Linguistics.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies. Transactions of the
Association for Computational Linguistics , 9:346–
361.
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren
Etzioni, and Nate Kushman. 2014. Learning to solve
arithmetic word problems with verb categorization.
InProceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 523–533, Doha, Qatar. Association for Com-
putational Linguistics.
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,
Kyle Richardson, Peter Clark, and Ashish Sabharwal.
2023. Decomposed prompting: A modular approach
for solving complex tasks.
Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. In Advances in
Neural Information Processing Systems , volume 35,
pages 22199–22213. Curran Associates, Inc.Christoph Leiter, Ran Zhang, Yanran Chen, Jonas Be-
louadi, Daniil Larionov, Vivian Fresen, and Stef-
fen Eger. 2023. Chatgpt: A meta-analysis after 2.5
months.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 158–167, Vancouver,
Canada. Association for Computational Linguistics.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2021. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els.
OpenAI. 2023. Gpt-4 technical report.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 2080–2094, Online.
Association for Computational Linguistics.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, et al.
2022. Scaling language models: Methods, analy-
sis and insights from training gopher.
Laria Reynolds and Kyle McDonell. 2021. Prompt
programming for large language models: Beyond the
few-shot paradigm.
Timo Schick and Hinrich Schütze. 2021. It’s not just
size that matters: Small language models are also few-
shot learners. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 2339–2352, Online. Association
for Computational Linguistics.
Taylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric
Wallace, and Sameer Singh. 2020. AutoPrompt: Elic-
iting Knowledge from Language Models with Auto-
matically Generated Prompts. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 4222–4235,
Online. Association for Computational Linguistics.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, et al. 2022. Lamda: Language
models for dialog applications.

--- PAGE 7 ---
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and
Denny Zhou. 2022. Chain-of-thought prompting elic-
its reasoning in large language models.
A Appendix
A.1 Example on GSM8K
Question: Eliza’s rate per hour for the first 40
hours she works each week is $10. She also
receives an overtime pay of 1.2 times her regular
hourly rate. If Eliza worked for 45 hours this week,
how much are her earnings for this week?
Answer: 460
Output:
Sure! Let’s break down the question into step-
by-step sub-questions and provide answers in
pseudocode and numerals:
1. Calculate Eliza’s regular earnings for the first
40 hours at a rate of $10 per hour:
a. Pseudo-code: regular_earnings = rate_per_hour
* 40
b. Numerical calculation: regular_earnings =
$10/hour * 40 hours = $400
2. Calculate Eliza’s overtime earnings for the
additional 5 hours at a rate of 1.2 times her regular
hourly rate:
a. Pseudo-code: overtime_earnings = over-
time_rate * overtime_hours
b. Numerical calculation: overtime_earnings =
$10/hour * 1.2 * 5 hours = $60
3. Calculate Eliza’s total earnings:
a. Pseudo-code: total_earnings = regular_earnings
+ overtime_earnings
b. Numerical calculation: total_earnings = $400 +
$60 = $460
Therefore, Eliza’s earnings for this week are
$460.
A.2 Example on AQUA
Question: Find out which of the following values
is the multiple of X, if it is divisible by 9 and 12?
[’A)36’, ’B)15’, ’C)17’, ’D)5’, ’E)7’]
Answer: A
Output:
Step 1: Check if the given values are divisible by
both 9 and 12.- 36 is divisible by both 9 and 12.
- 15 is not divisible by 9.
- 17 is not divisible by 9 or 12.
- 5 is not divisible by 9.
- 7 is not divisible by 9 or 12.
Step 2: Identify which value is divisible by both
9 and 12.
- The multiple of both 9 and 12 is 36.
Therefore, the answer is A) 36.
A.3 Example on SV AMP
Question: Each pack of dvds costs 76 dollars. If
there is a discount of 25 dollars on each packHow
much do you have to pay to buy each pack?
Answer: 51.0
Output:
Sure! Let’s break down the question into five
step-by-step sub-questions:
1. What is the original cost of each pack of
DVDs?
Answer: $76
2. What is the discount on each pack?
Answer: $25
3. What is the new cost of each pack after the
discount?
Answer: Original cost - Discount = $76 - $25 = $51
4. How much do you have to pay to buy each
pack?
Answer: $51
Now, let me provide you with the answers in
pseudocode :
original_cost = 76
discount = 25
new_cost = original_cost - discount
And finally, in numerals, the answer is: $51.
A.4 Example on ADDSUB
Question: Joan found 70 seashells on the beach .
she gave Sam some of her seashells . She has 27
seashell . How many seashells did she give to Sam

--- PAGE 8 ---
?
Answer: 43
Output:
Sure! Let’s break down the question into step-by-
step sub-questions and provide the answers using
pseudocode first:
1. How many seashells did Joan originally
have?
Answer: Joan originally had 70 seashells.
2. How many seashells does Joan have after
giving some to Sam?
Answer: Joan now has 27 seashells.
3. How many seashells did Joan give to Sam?
Answer: To find the number of seashells Joan gave
to Sam, we can subtract the number of seashells
she has now (27) from the number she originally
had (70).
Pseudocode: Joan’s seashells given to Sam = 70 -
27
4. Convert the pseudocode into numerals:
Answer: Joan gave Sam 43 seashells.
So, Joan gave Sam 43 seashells.
A.5 Example on StrategyQA
Question: Would Firefighters be included in a
September 11th memorial?
Answer: Yes. September 11th is remembered as a
day of mourning for the lives lost during a terrorist
attack in NYC. Firefighters were among the first
responders to the crisis, and many died.
Output:
Sure! Let’s break down your question into five
step-by-step sub-questions and provide answers
using pseudocode :
1. Is September 11th a memorial day?
- Answer: Yes
2. Does the September 11th memorial commem-
orate the tragic events of the terrorist attacks on
September 11, 2001?
- Answer: Yes
3. Were firefighters involved in responding to
the terrorist attacks on September 11, 2001?- Answer: Yes
4. Are individuals who were involved in re-
sponding to the terrorist attacks on September 11th,
including firefighters, honored and remembered in
the September 11th memorial?
- Answer: Yes
5. Are firefighters included as part of the
memorial tribute for their bravery and sacrifice
during the events of September 11th?
- Answer: Yes
So, in summary, the answer to your question is:
Yes, firefighters are included in the September 11th
memorial.
Please note that the pseudocode used here is
to illustrate the logical flow of the sub-questions
and their answers. The actual implementation may
vary based on the specific context or programming
language.
A.6 Error example on GSM8k
Question: An oil pipe in the sea broke . Before
engineers started to fix the pipe , 2475 gallons of
oil leaked into the water . A total of 6206 gallons
of oil leaked before the pipe was fixed . How many
gallons of oil leaked while the engineers were
fixing the pipe ?
Answer: 3731.0
Output:
Sure! Let’s break down the problem into step-by-
step sub-questions:
Step 1: Calculate the total amount of oil leaked
after the engineers fixed the pipe.
This can be found by subtracting the amount
leaked before the engineers started from the total
amount leaked.
Total leaked after fixing = Total leaked - Leaked
before fixing
Total leaked after fixing = 6206 gallons - 2475
gallons
Step 2: Calculate the amount of oil leaked while
the engineers were fixing the pipe.
This can be found by subtracting the total leaked
after fixing from the amount leaked before fixing.
Leaked while fixing = Leaked before fixing - Total
leaked after fixing

--- PAGE 9 ---
Step 3: Convert the answer from step 2 into
numerals.
Now let’s provide you with the pseudocode :
Step 1:
Define the variable "totalLeaked" and assign it the
value of 6206 gallons.
Define the variable "leakedBeforeFixing" and
assign it the value of 2475 gallons.
Define the variable "totalLeakedAfterFixing" and
assign it the result of subtracting leakedBeforeFix-
ing from totalLeaked.
Step 2:
Define the variable "leakedWhileFixing" and
assign it the result of subtracting totalLeakedAfter-
Fixing from leakedBeforeFixing.
Step 3:
Print the value of leakedWhileFixing.
The answer in numerals is the value of leaked-
WhileFixing obtained from the pseudocode.
No, your answer is not correct. Let’s go through
the calculations again:
Step 1: Total leaked after fixing = 6206 gallons -
2475 gallons = 3731 gallons
Step 2: Leaked while fixing = 2475 gallons -
3731 gallons = -1256 gallons
It seems that we made a mistake in the calcula-
tion. The result is a negative value, which doesn’t
make sense in this context. Let’s double-check the
steps:
Step 1: Total leaked after fixing = 6206 gallons -
2475 gallons = 3731 gallons
Step 2: Leaked while fixing = 3731 gallons -
2475 gallons = 1256 gallons
So, the correct answer is 1256 gallons.
