# Large Language Models for Mathematicians
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/math/2312.04556.pdf
# File size: 908332 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Mô hình Ngôn ngữ Lớn cho các Nhà Toán học
Simon Frieder∗, Julius Berner†, Philipp Petersen‡, Thomas Lukasiewicz§
Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) như ChatGPT đã nhận được sự quan tâm to lớn về khả năng hiểu ngôn ngữ đa mục đích và, đặc biệt, khả năng tạo ra văn bản chất lượng cao hoặc mã máy tính. Đối với nhiều nghề nghiệp, LLM đại diện cho một công cụ vô giá có thể tăng tốc và cải thiện chất lượng công việc. Trong ghi chú này, chúng tôi thảo luận về mức độ mà chúng có thể hỗ trợ các nhà toán học chuyên nghiệp. Trước tiên, chúng tôi cung cấp một mô tả toán học về mô hình transformer được sử dụng trong tất cả các mô hình ngôn ngữ hiện đại. Dựa trên các nghiên cứu gần đây, chúng tôi sau đó phác thảo các thực hành tốt nhất và các vấn đề tiềm ẩn và báo cáo về khả năng toán học của các mô hình ngôn ngữ. Cuối cùng, chúng tôi làm sáng tỏ tiềm năng của LLM trong việc thay đổi cách làm việc của các nhà toán học.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) gần đây đã cách mạng hóa lĩnh vực xử lý ngôn ngữ tự nhiên, mang lại tiến bộ chưa từng có cho các ứng dụng như bot đối thoại hoặc tạo văn bản. Những tiến bộ trong thuật toán học sâu, tài nguyên tính toán, cũng như những nỗ lực kỹ thuật đáng kể, đã cho phép huấn luyện các mô hình ngôn ngữ với hàng tỷ tham số và trên các bộ dữ liệu khổng lồ, chẳng hạn như The Pile[14] có kích thước 800GB. Được huấn luyện trên các bộ dữ liệu ngày càng lớn, các mô hình ngôn ngữ suy ra các khía cạnh thiết yếu của cấu trúc ngôn ngữ¹, mở đường từ hiểu biết ngôn ngữ cụ thể cho từng nhiệm vụ đến hiểu biết ngôn ngữ đa mục đích. Như vậy, các mô hình này cũng có thể phân tích các câu hỏi toán học và giải quyết các bài toán và nhiệm vụ toán học (đôi khi phức tạp một cách đáng ngạc nhiên)—tuy nhiên, chúng cũng mắc những sai lầm đáng xấu hổ.

Hãy xem hai ví dụ mà ChatGPT (phiên bản web ngày 30-01-2023) đã tạo ra². Cả hai ví dụ dưới đây đều được lấy từ [13]; đầu vào cho mô hình, cũng như đầu ra của nó, bao gồm mã LATEX, mà chúng tôi đã biên dịch dưới đây để dễ đọc hơn:

"prompt": "Tìm một hàm f:R→R liên tục tại chính xác một điểm.",
"output": "Một ví dụ về hàm như vậy là
f(x) = {
x² nếu x ≠ 0
1 nếu x = 0.
Hàm này liên tục tại x = 0 nhưng không liên tục tại bất kỳ điểm nào khác."

"prompt": "Cho μ là một độ đo, ∥f∥∞ = inf{M: μ({x: |f(x)| > M}) = 0} và ∥g∥₁ = ∫|g|dμ.
Chứng minh rằng ∫|fg|dμ ≤ ∥g∥₁∥f∥∞.",
"output": "Cho M = ∥f∥∞ và ε > 0. Khi đó, theo định nghĩa của ∥f∥∞, ta có
μ({x: |f(x)| > M + ε}) = 0. Do đó,
∫|fg|dμ = ∫_{|f|≤M+ε}|fg|dμ + ∫_{|f|>M+ε}|fg|dμ ≤ (M+ε)∫|g|dμ + ∫_{|f|>M+ε}|f||g|dμ.
Số hạng thứ hai ở vế phải bằng không vì μ(x: |f(x)| > M + ε) = 0. Vậy ta có
∫|fg|dμ ≤ (M+ε)∫|g|dμ = (M+ε)∥g∥₁. Vì ε có thể nhỏ tùy ý, ta có thể cho nó về không và ta được ∫|fg|dμ ≤ M∥g∥₁ = ∥f∥∞∥g∥₁. Vậy là đã chứng minh."

∗Khoa Khoa học Máy tính, Đại học Oxford, Oxford, UK
†Khoa Khoa học Tính toán và Toán học, Caltech, Pasadena, US
‡Khoa Toán học và Mạng lưới Nghiên cứu Khoa học Dữ liệu, Đại học Vienna, Vienna, Austria
§Viện Logic và Tính toán, Đại học Công nghệ Vienna, Vienna, Austria, và Khoa Khoa học Máy tính, Đại học Oxford, Oxford, UK

¹Chúng tôi lưu ý rằng các mô hình ngôn ngữ hoạt động tốt nhất bằng tiếng Anh vì các bộ dữ liệu mở rộng nhất, bao gồm những bộ chứa toán học, có sẵn bằng tiếng Anh.
²Ví dụ đầu tiên hiện tại có thể được trả lời chính xác bởi mô hình GPT-4[26], đã được phát hành từ đó. Chúng tôi giải thích từ viết tắt "GPT" trong Phần 2.

1arXiv:2312.04556v2 [cs.CL] 2 Apr 2024

--- TRANG 2 ---
Phản hồi đầu tiên là vô nghĩa ở tất cả các mức độ, trong khi phản hồi thứ hai là chính xác và hữu ích. Với những ví dụ này trong tâm trí, chúng ta có thể tự hỏi:

Các mô hình ngôn ngữ lớn có thể hỗ trợ các nhà toán học con người trong công việc của họ như thế nào?

Để giải quyết câu hỏi này, bài viết này tiến hành theo cách sau: Đầu tiên, chúng tôi cung cấp tổng quan về các mô hình ngôn ngữ hiện đại. Chúng tôi làm rõ lý thuyết về cách hoạt động của các mô hình ngôn ngữ, cách thức thiết lập khối xây dựng chính của chúng—kiến trúc transformer—và tại sao những mô hình này có thể thực hiện toán học đủ để hỗ trợ các nhà toán học trong công việc hàng ngày. Hiểu được kiến trúc này cũng sẽ làm nổi bật cách một LLM tạo ra câu trả lời cho một câu hỏi toán học – điều này khác rất nhiều so với cách một nhà toán học đến được với câu trả lời. Sau đó, chúng tôi trình bày bằng chứng thực nghiệm chứng thực khả năng của các mô hình ngôn ngữ, đặc biệt là các mô hình tiên tiến như ChatGPT và GPT-4. Chúng tôi kết thúc bằng một cái nhìn về những tác động tiềm năng trong tương lai đối với các nhà toán học và toán học nói chung.

2 Tổng quan về các Mô hình Ngôn ngữ Hiện đại
Khái niệm về mô hình ngôn ngữ có một lịch sử lâu dài. Một trong những thành tựu tiên phong, có từ năm 2000, đã trình bày một trong những thể hiện đầu tiên của những gì chúng ta gọi là nhúng từ trong khuôn khổ mạng nơ-ron [3]; xem Phần 3 để biết định nghĩa.

Hầu hết các phương pháp trước đây đều bắt nguồn từ việc ước tính xác suất trên trigram (hoặc, tổng quát hơn, n-gram). Một n-gram là một chuỗi n phần tử liền kề từ một chuỗi các đoạn từ, được gọi là token, có thể là âm tiết, chữ cái, từ, hoặc cặp cơ sở tùy thuộc vào ngữ cảnh. Trong câu "The quick brown fox jumps over the lazy dog", chuỗi "quick brown fox" là một ví dụ về trigram. Các mô hình dựa trên n-gram có những hạn chế nghiêm trọng: Ví dụ, nếu một trigram không xuất hiện trong kho ngữ liệu huấn luyện (hoặc chứa các từ không có trong từ vựng của kho ngữ liệu), thì không có cách nào có ý nghĩa để ước tính xác suất của nó. Bằng cách sử dụng một dạng nhúng từ, những vấn đề này được khắc phục. Mô hình được đề xuất bởi [3] đã áp đảo tất cả các mô hình n-gram thuần khác. Các tác giả lưu ý rằng những cải tiến có thể được thực hiện về "kiến trúc, hiệu quả tính toán, và tận dụng kiến thức có sẵn".

Việc giới thiệu kiến trúc transformer [40] vào năm 2017 đã đánh dấu sự tiến bộ nổi bật nhất về mặt kiến trúc mạng nơ-ron: Một mặt, cơ chế attention đã mô hình hóa cấu trúc của ngôn ngữ một cách trung thực hơn; mặt khác, đó là một kiến trúc dễ dàng song song hóa trên phần cứng hiện đại (xem Phần 3 để biết chi tiết). Điều này dẫn đến một loạt các cột mốc và cải tiến tiếp theo:

Năm 2018, mô hình Bidirectional Encoder Representations from Transformers (BERT) [7] được giới thiệu, một kế thừa của transformer gốc, đã truyền cảm hứng cho một số lượng lớn các kế thừa của riêng nó, chẳng hạn như RoBERTa [22], hoặc DistilBERT [32]. BERT (và các kế thừa của nó) được chú ý vì các pipeline cổ điển (ví dụ, định nghĩa biểu diễn văn bản, thực hiện gắn thẻ từ loại) đều được các mô hình kiểu BERT [36] bao quát, có thể dễ dàng được tinh chỉnh cho các nhiệm vụ cụ thể. Cùng thời điểm với mô hình BERT, mô hình Generative Pre-Trained Transformer (GPT) được OpenAI giới thiệu [28]. Đây là một biến thể khác của kiến trúc transformer gốc và là phiên bản đầu tiên của mô hình nằm dưới ChatGPT, được phát hành vào năm 2022 [25], và có liên quan chặt chẽ với InstructGPT [27].

Cột mốc cuối cùng bao gồm các mô hình LLaMA [38] và LLaMA2 [39] được giới thiệu vào năm 2023, vài tháng sau GPT-4 [26]. Tầm quan trọng của chúng nằm ở việc là những mô hình đầu tiên được phát hành công khai, mã và trọng số của chúng dễ dàng tiếp cận và có thể sánh được với hiệu suất của GPT-4; trong báo cáo kỹ thuật liên quan đến GPT-4, có ghi: "báo cáo này không chứa thêm chi tiết nào về kiến trúc (bao gồm kích thước mô hình), phần cứng, tính toán huấn luyện, xây dựng bộ dữ liệu, phương pháp huấn luyện, hoặc tương tự". Các mô hình LLaMA đã dẫn đến việc dân chủ hóa các mô hình ngôn ngữ và tạo ra một số lượng lớn các kế thừa tiếp theo, chẳng hạn như mô hình Alpaca³ của Stanford, hoặc mô hình Vicuna⁴, từ đó đã được sử dụng trong một loạt các ngữ cảnh rộng rãi. Khi các mô hình này phát triển, số lượng tham số của chúng, cũng như kích thước của bộ dữ liệu mà chúng được huấn luyện, tiếp tục tăng, từ hàng triệu tham số (trong trường hợp của [3, 40, 7]), đến hàng tỷ [39,38], đến hàng nghìn tỷ [8,30], xem Hình 1. Trong khi xu hướng chính cho thấy kích thước mô hình tăng lên, có một xu hướng ngược lại để làm cho các mô hình nhỏ hơn trong khi vẫn giữ được hiệu suất. Mô hình DistilBERT là một ví dụ về điều này. Việc mở rộng quy mô kiến trúc và lượng dữ liệu huấn luyện đã tạo ra những khả năng chưa từng có cho các LLM kết quả, loại bỏ nhu cầu tinh chỉnh cho các nhiệm vụ cụ thể.

³https://github.com/tatsu-lab/stanford_alpaca
⁴https://lmsys.org/blog/2023-03-30-vicuna/
2

--- TRANG 3 ---
Hình 1: Một lựa chọn các mô hình ngôn ngữ hiện đại đại diện được trình bày cùng với số lượng tham số của chúng (tính bằng tỷ), được hiển thị ở trên. Trục y là trục logarit, và phạm vi mô hình được hiển thị (hai dấu chấm ngang), khi có sẵn, cho mỗi mô hình. Chúng ta quan sát thấy một phạm vi rộng các tham số xuất hiện, từ 28 triệu đến 1,2 nghìn tỷ. Đối với ChatGPT, số lượng tham số chính xác không có sẵn nhưng được lấy từ InstructGPT, là một mô hình anh em mà ChatGPT dựa trên. Đối với GPT-4, số lượng tham số không có sẵn.

3 Nền tảng Kỹ thuật
Trong phần sau, chúng tôi tìm cách đưa ra một giới thiệu ngắn gọn về hoạt động nội bộ của LLM. Chúng tôi tham khảo [45,23] để biết các khảo sát và chi tiết thêm. Chúng tôi không cố gắng trình bày các mô hình và kỹ thuật tiên tiến trong xử lý ngôn ngữ tự nhiên mà tập trung vào hiểu biết khái niệm về chức năng của LLM. Đặc biệt, chúng tôi sẽ giới hạn bài trình bày với một trong những kiến trúc phổ biến nhất, transformer [40] (xem Phần 2 để biết ngữ cảnh về tầm quan trọng của mô hình này). Mô tả của chúng tôi dưới đây là một tóm tắt toán học đơn giản hóa được dựa lỏng lẻo trên mã (mã nguồn mở)⁵ của GPT-2 [29], xem cũng Hình 2 để có cái nhìn tổng quan.

3.1 Kiến trúc Transformer
Hãy khám phá cách một kiến trúc transformer dự đoán văn bản dựa trên một số đầu vào được cung cấp, thường được gọi là prompt. Phù hợp với các mô hình thành công, chẳng hạn như chuỗi GPT và LLaMA, chúng tôi tập trung vào thiết lập, nơi mô hình lặp đi lặp lại dự đoán các đoạn từ tiếp theo (tức là, token) dựa trên một chuỗi token nhất định. Quy trình này được gọi là autoregressive vì việc dự đoán token mới chỉ dựa trên các token trước đó. Các nhiệm vụ tạo chuỗi có điều kiện như vậy sử dụng transformer autoregressive thường được gọi là thiết lập chỉ có decoder. Mặc dù trong thực tế có độ dài ngữ cảnh tối đa, chúng tôi sẽ làm việc với các chuỗi có độ dài tùy ý để dễ trình bày. Chúng tôi định nghĩa ký hiệu viết tắt S* := ⋃_{n∈N} S^n cho một tập S để biểu thị tập các chuỗi s = (s^(i))^n_{i=1} ⊂ S với độ dài tùy ý n ∈ N.

Đối với một hàm F: S₁ → S₂, chúng tôi ký hiệu bằng F*: S*₁ → S*₂ ánh xạ được áp dụng theo từng phần tử được cho bởi
F*(s) := (F(s^(i)))^n_{i=1}. (1)

Token hóa K. Đầu tiên, chúng tôi muốn làm rõ cách chúng tôi định nghĩa các đoạn từ, tức là, token. Về mặt toán học, chúng tôi tìm kiếm một ánh xạ đơn ánh K: A* → T* từ văn bản đã cho, tức là, một chuỗi a = (a^(i))^N_{i=1} các ký tự trong một bảng chữ cái A đến một chuỗi n ≤ N token (t^(i))^n_{i=1}, trong đó thường T := {1,2,...,M}.

⁵https://github.com/openai/gpt-2
3

--- TRANG 4 ---
Token hóa    Nhúng + Mã hóa vị trí    Đầu dự đoán    Các lớp Transformer    Lấy mẫu
Prove that pi    Prove that pi is
(Pro)
(ve)
( that )
( pi)
( is)
('s)
( has)

Chuẩn hóa
Self-Attention
Chuẩn hóa
Perceptron Đa lớp
Kết nối nhảy    Kết nối nhảy    Kết nối nhảy

Hình 2: Minh họa các hoạt động của một LLM cho văn bản đầu vào "Prove that pi". Các chỉ số token, cũng như các xác suất cho token tiếp theo, được lấy từ GPT-2 [29] sử dụng việc thực hiện trong thư viện transformers [42]. Xác suất cao nhất cho token tiếp theo được gán cho 318 tương ứng với từ "is".

Để biểu diễn văn bản trên máy tính, chúng ta có thể mã hóa từng ký tự a^(i) riêng lẻ, tương tự như Unicode. Trong khi điều này sẽ dẫn đến một từ vựng T nhỏ của các token, nó tạo ra các chuỗi dài trong đó các token riêng lẻ không nắm bắt được bất kỳ thông tin ngôn ngữ nào. Đối với LLM, người ta thường sử dụng token hóa từ phụ [34], tạo ra một từ vựng của các từ phụ bằng cách phân tích các kho ngữ liệu văn bản lớn và lặp đi lặp lại hợp nhất các chuỗi ký tự xuất hiện thường xuyên. Giống như một vấn đề nén, người ta cân bằng độ dài n của các chuỗi và kích thước⁶ M của từ vựng. Ví dụ, tokenizer GPT-4⁷ chia từ "discontinuity" thành các từ phụ "dis" (tiền tố), "contin" (từ phụ nắm bắt gốc của "continuous"), và "uity" (hậu tố). Một ví dụ khác có thể được tìm thấy trong Hình 2.

Nhúng E. Để sử dụng các token này (được cho bởi các chỉ số của các từ phụ trong từ vựng) trong một mạng nơ-ron, chúng ta nhúng mỗi token t^(i) vào cùng một không gian Euclidean E := R^d. Trực quan, chúng ta tìm kiếm một ánh xạ E: T → E, sao cho khoảng cách ∥E(t^(i)) - E(t^(j))∥ tương ứng với sự tương tự ngôn ngữ của các từ phụ được biểu diễn bởi các token t^(i) và t^(j). Trong thực tế, một nhúng như vậy thường được khởi tạo với một chuỗi M nhúng ban đầu ngẫu nhiên và được học kết hợp với mô hình transformer từ dữ liệu.

Mã hóa Vị trí P. Vì E hoạt động trên mỗi token t^(i) độc lập, các nhúng E(t^(i)) không chứa thông tin về vị trí i của các (từ phụ) trong một câu⁸. Do đó, người ta thường thêm cái gọi là mã hóa vị trí, có thể được mô tả bởi một ánh xạ P: E* → E*. Một lựa chọn thường được sử dụng có dạng

P((e^(i))^n_{i=1}) := (e^(i) + p^(i)))^n_{i=1}, (2)

trong đó p: N → E có thể là một hàm đơn ánh được quy định, ví dụ, một hàm sin [40], hoặc được học (tương tự như nhúng E) [28].

⁶Các mô hình LLaMA và LLaMA2 sử dụng từ vựng T với M = 32000 token [39]. GPT-2 sử dụng M = 50257, và các mô hình khác trong chuỗi GPT, ví dụ, GPT-3.5-turbo và GPT-4, thậm chí sử dụng M = 100277 token, xem https://github.com/openai/tiktoken.
⁷Xem https://platform.openai.com/tokenizer.
⁸Trong một số thiết lập, nơi kiến trúc transformer là bất biến hoán vị, mã hóa vị trí là hoàn toàn cần thiết. Trong thiết lập chỉ có decoder của chúng ta, điều này không phải như vậy [15]; tuy nhiên, các mã hóa vẫn dường như cải thiện hiệu suất.
4

--- TRANG 5 ---
Tóm lại, token hóa K, theo sau là việc áp dụng E cho mỗi token và mã hóa vị trí P, ánh xạ văn bản a ∈ A* đến một chuỗi các nhúng

e := (P ∘ E* ∘ K)(a) ∈ E*. (3)

trong đó độ dài của e phụ thuộc vào a và thuật toán token hóa.

Transformer T. Transformer có thể được biểu diễn như một mạng nơ-ron T: E* → E*. Nó được huấn luyện để ánh xạ một chuỗi nhúng e đến một chuỗi khác có cùng độ dài chứa thông tin ngữ cảnh. Dựa trên cấu trúc autoregressive mong muốn, nơi việc dự đoán token tiếp theo chỉ phụ thuộc vào các token trước đó, chúng ta muốn phần tử thứ i của T(e) chứa thông tin về tất cả các nhúng (e^(j))_{j≤i}, tuy nhiên, độc lập với (e^(j))_{j>i}.

Transformer thường được định nghĩa bởi một thành phần của L ∈ N khối, bao gồm các ánh xạ self-attention A_ℓ, các lớp chuẩn hóa được áp dụng theo từng phần tử N_{A,ℓ}, N_{M,ℓ}, và các perceptron đa lớp feed-forward M_ℓ, tức là,

T := ((Id + M*_L ∘ N*_{M,L}) ∘ (Id + A_L ∘ N*_{A,L})) ∘ ⋯ ∘ ((Id + M*_1 ∘ N*_{M,1}) ∘ (Id + A_1 ∘ N*_{A,1})). (4)

Trong phần trên, Id biểu thị ánh xạ đồng nhất, thường được biết đến như một kết nối nhảy hoặc kết nối dư, và phép cộng được hiểu theo từng phần tử. Các chỉ số của các lớp N, M, và A trong (4) chỉ ra việc sử dụng các tham số có thể huấn luyện khác nhau trong mỗi lớp. Hãy mô tả các lớp này chi tiết hơn dưới đây.

Các Lớp: Chuẩn hóa N. Lớp chuẩn hóa có thể được diễn giải như một tham số hóa lại với mean và độ lệch chuẩn có thể học để ổn định huấn luyện. Ví dụ, sử dụng chuẩn hóa lớp N: E → E, chúng ta tính

N(e) = diag(s)σ(e - μ) + m, (5)

trong đó μ = 1/d Σ^d_{i=1} e_i và σ² = 1/d Σ^d_{i=1} (e_i - μ)² là mean và phương sai của e ∈ E, và s, m ∈ E là các tham số có thể học [1].

Các Lớp: Perceptron Đa lớp M. Perceptron đa lớp (MLP) là một mạng nơ-ron feed-forward tiêu chuẩn bao gồm các thành phần của các ánh xạ affine và các hàm kích hoạt phi tuyến. Hãy định nghĩa bằng L^{(m,n)}: R^m → R^n một ánh xạ affine L^{(m,n)}(x) := Wx + b, trong đó ma trận trọng số W ∈ R^{n×m} và vector bias b ∈ R^m là có thể học. Hơn nữa, cho ϱ: R → R là một hàm kích hoạt, ví dụ, hàm kích hoạt GELU ϱ(x) := xΦ(x), trong đó Φ là hàm phân phối tích lũy Gaussian tiêu chuẩn [17]. Một MLP điển hình M: E → E được sử dụng trong transformer sau đó được cho bởi

M := L^{(d,D)} ∘ ϱ* ∘ L^{(D,d)}, (6)

trong đó D ∈ N với D ≥ d.

Các Lớp: Self-Attention A. Như có thể thấy trong (4) và Hình 2, lớp self-attention A: E* → E* là lớp duy nhất kết hợp các nhúng của các token khác nhau; nói cách khác, nó chú ý đến các token khác. Hãy ký hiệu đầu vào cho lớp bằng (e^(i))^n_{i=1} và tập trung vào đầu ra thứ i. Trước tiên chúng ta tính các tích vô hướng (được chuẩn hóa)

s^{(i)}_j = 1/√k ⟨L^{(k,d)}_{query}(e^{(i)}), L^{(k,d)}_{key}(e^{(j)})⟩, j = 1,...,i, (7)

với k ∈ N đã cho. Ở mức độ cao, chúng ta có thể diễn giải s^{(i)} = (s^{(i)}_j)^i_{j=1} ⊂ R như sự tương tự giữa nhúng L^{(k,d)}_{query}(e^{(i)}) của token thứ i (tức là, cái gọi là query) và các nhúng L^{(k,d)}_{key}(e^{(j)}) của các token khác (tức là, key); để thỏa mãn cấu trúc autoregressive, chúng ta chỉ xem xét j ≤ i. Để chuẩn hóa s^{(i)} thành xác suất, chúng ta có thể sử dụng thêm một lớp softmax softmax: R* → R* được cho bởi

softmax(s^{(i)})_j := exp(s^{(i)}_j) / Σ^i_{k=1} exp(s^{(i)}_k), j = 1,...,i. (8)

5

--- TRANG 6 ---
Bây giờ chúng ta có thể diễn giải softmax(s^{(i)})_j như xác suất để query thứ i "chú ý" đến key thứ j. Lớp self-attention A sau đó có thể được định nghĩa là

A(e)_i := L^{(k,d)} Σ^i_{j=1} softmax(s^{(i)})_j L^{(k,d)}_{value}(e^{(j)}), i = 1,...,n, (9)

trong đó các đầu ra của L^{(k,d)}_{value} thường được gọi là các giá trị của các nhúng token e^{(j)}, và trong đó lớp affine có thể học L^{(k,d)} ánh xạ trung bình có trọng số của các giá trị trở lại E = R^d.

Lưu ý rằng trong thực tế, người ta thường xem xét một tổng của h ∈ N các lớp attention như vậy (được gọi là head), mỗi lớp có chiều k = d/h [40,21]. Hơn nữa, thay vì xem xét các vector có độ dài biến thiên i, một mask thực thi cấu trúc autoregressive để tất cả các phép toán có thể được nhóm lại hiệu quả.

Đầu Dự đoán H. Đầu dự đoán hoặc lớp un-embedding có thể được biểu diễn như một ánh xạ H: E* → Δ_M, trong đó

Δ_M := {P ∈ [0,1]^M : Σ^M_{i=1} P_i = 1} (10)

biểu thị simplex xác suất trong R^M. Nó ánh xạ chuỗi các nhúng đã được biến đổi (ẽ^{(i)})^n_{i=1} := T(e) thành một vector P ∈ Δ_M, trong đó P_i mô tả xác suất dự đoán i ∈ T như token tiếp theo. Vì nhúng đã được biến đổi của token cuối cùng, tức là, ẽ^{(n)}, chứa thông tin về toàn bộ văn bản đầu vào, một phương pháp đơn giản là sử dụng một ánh xạ tuyến tính được thành phần với một lớp softmax và định nghĩa

P := (softmax ∘ L^{(M,d)})(ẽ^{(n)}). (11)

Lấy mẫu S. Có nhiều chiến lược lấy mẫu S: Δ_M → T để đến dự đoán cuối cùng cho token tiếp theo t^{(n+1)}, xem, ví dụ, [18]; chiến lược đơn giản nhất, được gọi là lấy mẫu tham lam, dự đoán token có xác suất cao nhất, tức là,

t^{(n+1)} = S(P) := arg max_{i=1,...,M} P_i, (12)

xem Hình 2. Sau đó người ta có thể áp dụng các phép toán tương tự cho chuỗi mở rộng t = (t^{(i)})^{n+1}_{i=1}, tức là,

t^{(n+2)} := (S ∘ H ∘ T ∘ P ∘ E*)(t) (13)

để tính lặp các token tiếp theo^9. Do cấu trúc autoregressive, điều này có thể được thực hiện hiệu quả bằng cách cache các kết quả (trung gian) trước đó và chỉ xem xét các tính toán cho token mới.

3.2 Huấn luyện
Trong quá trình huấn luyện, chúng ta biến đổi các kho ngữ liệu văn bản thành các chuỗi token, sao cho, đối với một chuỗi đã cho (t_i)^n_{i=1}, chúng ta đã biết token tiếp theo t_{n+1} dựa trên văn bản cơ bản. Do đó người ta có thể tính độ lệch D giữa các xác suất dự đoán P của token tiếp theo và sự thật cơ bản t_{n+1}, thường sử dụng cross-entropy loss; trong thực tế, quy trình này có thể được song song hóa để tính tổn thất trung bình trên nhiều dự đoán. Sử dụng automatic-differentiation, sau đó người ta tính đạo hàm ∇_θD của tổn thất trung bình D đối với các tham số có thể học θ ∈ R^p của transformer T, nhúng E, đầu dự đoán H (và mã hóa vị trí P nếu nó có thể huấn luyện). Cập nhật tham số bằng cách trừ đi một bội số đủ nhỏ λ ∈ (0,∞) của đạo hàm, tức là, θ_{k+1} = θ_k - λ∇_θD, người ta có thể giảm thiểu tổn thất một cách lặp—một phương pháp được biết đến như stochastic gradient descent. Đây là cơ chế thiết yếu mà qua đó các xác suất xuất hiện từ được ước tính bằng cách huấn luyện từ dữ liệu thô. Với những nỗ lực kỹ thuật đáng kể, các phiên bản phức tạp hơn của các sơ đồ huấn luyện như vậy có thể được song song hóa trên các cụm GPU lớn và mở rộng quy mô đến lượng dữ liệu khổng lồ. Để có ý tưởng về các chiều, mô hình LLaMA2 lớn nhất với p = 70·10^9 tham số đã được huấn luyện trong hơn 1,7 triệu giờ GPU trên khoảng 2 nghìn tỷ token dữ liệu từ các nguồn có sẵn công khai [39].

^9Thường có một tiêu chí dừng dựa, ví dụ, trên một token đặc biệt hoặc entropy của P.
6

--- TRANG 7 ---
3.3 Chi phí Huấn luyện và Phát thải
Huấn luyện LLM, như được mô tả trong phần trước, là một quá trình rất tốn tính toán và, do đó, tốn kém để thực hiện về mặt sử dụng điện (giả sử tất cả phần cứng sẽ có sẵn). Tuy nhiên, thông tin về chi phí huấn luyện và phát thải CO₂ không được cung cấp một cách nhất quán trong tài liệu. Các ngoại lệ đáng chú ý bao gồm mô hình LaMDA. Các tác giả [37] báo cáo rằng tổng cộng 451MWh đã được tiêu thụ trong quá trình huấn luyện, và kết quả là, khoảng 26 tấn CO₂ đã được phát thải. Sử dụng giá lịch sử của Mỹ¹⁰ là 0,148 đô la mỗi kWh, điều này tương đương với chi phí 66.748 đô la. Chúng tôi lưu ý rằng chi phí có thể thay đổi theo quốc gia và theo nguồn năng lượng được sử dụng để sản xuất năng lượng [35]. Mô hình GLaM tiêu thụ, khi được huấn luyện trên bộ dữ liệu lớn nhất, tương tự 456MWh và phát thải 40,2 tấn CO₂, điều này đặt nó vào cùng một danh mục với mô hình LaMDA về mặt chi phí và phát thải.

Tuy nhiên, các LLM hiện đại hơn phát sinh tiêu thụ năng lượng và phát thải nhiều hơn đáng kể. Ví dụ, việc huấn luyện LLaMA2 (sử dụng 1,7 triệu giờ trên GPU với mức tiêu thụ điện khoảng 400W) đã phát thải hơn 291 tấn Carbon dioxide tương đương (CO₂-eq) [39]. Các nhà cung cấp LLM (như OpenAI) thường không phát hành thông tin về chi phí (dù về mặt megawatt-hour tiêu thụ hoặc giờ GPU (thuê)) của việc huấn luyện các mô hình của họ, vì vậy chỉ có thể ước tính mơ hồ, tuy nhiên vẫn đáng kinh ngạc. Ví dụ, việc huấn luyện mô hình GPT-3 thế hệ cũ [4] được ước tính, sử dụng giá giờ GPU từ thời điểm đó, tốn chi phí khoảng 4,6 triệu đô la [20].

4 LLM cho Toán học
Với nền tảng của LLM đã được thiết lập tốt, chúng ta chuyển sự chú ý đến ứng dụng của chúng trong việc hỗ trợ các nhà toán học chuyên nghiệp. Trong khi các nhà toán học tham gia vào một phổ rộng các hoạt động toán học, như thực hiện mô phỏng, mô hình hóa và tính toán, chúng tôi tập trung vào nhiệm vụ có thể được coi là quan trọng nhất: khả năng của LLM để tạo ra các chứng minh toán học. Theo nghĩa này, bài viết của chúng tôi khác với [41], tập trung xem xét các phương pháp học sâu khác để tiếp cận toán học.

Khi sử dụng một LLM để hỗ trợ trong nhiệm vụ chứng minh định lý, cách đơn giản nhất là yêu cầu mô hình trực tiếp chứng minh mệnh đề thay vì sử dụng nó cho các bước riêng lẻ hoặc các nhiệm vụ khác sẽ được mô tả dưới đây. Tuy nhiên, nhiều vấn đề đã được tìm thấy với phương pháp này. Một mối quan tâm chính là các lập luận toán học phụ thuộc vào độ chính xác của logic; một mệnh đề sai duy nhất rất có thể sẽ làm vô hiệu hóa toàn bộ chứng minh. Giả sử rằng LLM có một xác suất lỗi không thể bỏ qua, độc lập với mỗi từ được dự đoán, khả năng tạo ra một chứng minh chính xác giảm theo cấp số nhân với việc tăng độ dài văn bản. Điều này cũng được quan sát thực nghiệm trong [16], nơi một LLM hóa ra có độ chính xác cao hơn trong việc giải quyết các nhiệm vụ tính toán nếu nó được yêu cầu bỏ qua các bước trung gian.

Bản chất autoregressive của LLM đưa ra một vấn đề quan trọng khác. Một khi một mệnh đề được đưa ra, LLM thường không xem xét lại hoặc sửa đổi các lập luận của chúng. Quá trình này khác biệt đáng kể so với phương pháp luận của hầu hết các nhà toán học. Hiếm khi một nhà toán học soạn thảo một chứng minh hoàn chỉnh và chi tiết trong một lần thử duy nhất. Thay vào đó, quá trình thường bao gồm việc tạo ra một phác thảo thô, bỏ qua các bước nhỏ mà chúng ta có tin tưởng, lặp lại, và tinh chỉnh cho đến khi chứng minh hoàn thành.

Hơn nữa, LLM có thể xây dựng các chứng minh hoàn toàn hợp lệ cho các câu hỏi khác với những câu hỏi được đặt ra. Một thể hiện như vậy đã được minh họa trong phần giới thiệu khi LLM được yêu cầu tìm một hàm liên tục tại chỉ một điểm. Với sự phổ biến của các bài toán tương tự nhưng khác biệt trong các bộ dữ liệu huấn luyện, LLM có khả năng phản hồi với các biến thể thường gặp hơn của một câu hỏi. Trong trường hợp này, một câu hỏi về một hàm không liên tục tại chỉ một điểm. Tương tự, chúng có thể chứng minh một định lý dưới các giả thiết mạnh hơn so với được nêu mà không làm cho điều này rõ ràng.

Cuối cùng, chỉ dựa trên các mối quan hệ thống kê của ngôn ngữ, LLM gặp khó khăn đáng kể với các bài toán số học: Những bài toán này thường xảy ra khi một LLM phải hoàn thành một nhiệm vụ, chẳng hạn như thực hiện phép cộng hoặc nhân (đặc biệt, nếu các số liên quan là lớn). Lý do cho điều này là không có bộ giải số nào được tích hợp sẵn trong LLM. Các bước hướng tới việc khắc phục điều này gần đây đã được thực hiện bằng cách sử dụng phương pháp Toolformer [33]. Một thể hiện của điều này là plugin WolframAlpha có sẵn cho GPT-4.

Tóm lại, khi LLM được sử dụng để chứng minh định lý, chúng dễ bị một loạt các lỗi. Những lỗi này đã được kiểm tra trong [13], sẽ được thảo luận trong chương tiếp theo. Do đó, một phương pháp cộng tác hơn, kết hợp chuyên môn của con người, là khuyến khích. Các chiến lược sau đây dường như là hợp lý:

• Tài liệu/công cụ tìm kiếm: LLM có thể được yêu cầu giải thích một định nghĩa, tìm tên đã được thiết lập cho một khái niệm được mô tả mơ hồ, hoặc tìm tài liệu tham khảo cho một mệnh đề nhất định. Trong bối cảnh này, hai cân nhắc quan trọng nảy sinh. Thứ nhất, LLM được biết đến với việc tạo ra nội dung hợp lý nhưng hư cấu. Hiện tượng này thường được gọi là ảo giác. Do đó, câu trả lời của nó cho các truy vấn của chúng ta cần được xác minh. Thứ hai, LLM có thể làm trầm trọng thêm các thiên kiến trong nghiên cứu. Điều này có thể xảy ra khi một LLM bỏ qua công trình được trích dẫn không đầy đủ, hiệu quả che giấu nó trong khi khuyến nghị không cân xứng các bài báo được thổi phồng quá mức.

• Động não/Tạo ý tưởng: Một LLM có thể được yêu cầu cung cấp một ý tưởng cấp cao về cách chứng minh một định lý. Trong khi điều này sẽ không tạo ra kết quả đầy đủ, nó rất giống với phong cách của một nhà toán học. Tuy nhiên, không có đảm bảo rằng ý tưởng này sẽ rất sâu sắc hoặc dẫn đến điều gì đó. Được huấn luyện trên một kho ngữ liệu lớn các lập luận toán học, một LLM có thể sẽ thiên về việc khuyến nghị những ý tưởng tiêu chuẩn nhất. Điều này có thể không hữu ích cho một nhà toán học đã là chuyên gia trong một lĩnh vực cụ thể. Tuy nhiên, nó có thể rất có giá trị cho một nhà toán học đang cố gắng bước vào một lĩnh vực mới.

• Kiểm tra chứng minh: Một LLM có thể được yêu cầu tìm sai lầm trong một chứng minh đã cho. Trong khi không có đảm bảo gì rằng nó sẽ tìm thấy tất cả các lỗi, những lỗi mà nó tìm thấy thường có thể được xác nhận ngay lập tức là những sai lầm thực sự bởi một nhà toán học. Điều này hữu ích nhưng không đáng tin cậy. LLM có thể sẽ tập trung vào tính chính xác cú pháp hơn tính chính xác ngữ nghĩa và do đó bỏ qua các lỗi phức tạp.

• Viết cộng tác: Một LLM có thể được yêu cầu cung cấp các phần hoặc một phác thảo của một chứng minh và sau đó, sau khi nhận phản hồi từ người dùng, cải thiện các phần, sửa chữa lỗi, và thêm chi tiết. Trong [5], hiệu suất tương tác của ba LLM (InstructGPT, ChatGPT, và GPT-4) trên các truy vấn toán học đã được đo lường trên một nhóm người dùng. Đã có nỗ lực giải quyết các bài toán toán học bằng cách sử dụng các mô hình này như một trợ lý. Hỏi định nghĩa, câu hỏi toán học tổng quát (không liên quan chặt chẽ đến bài toán), và các bước chứng minh là ba trường hợp sử dụng phổ biến nhất. Quan trọng là phải nhớ rằng phương pháp này vẫn dễ bị đưa vào lỗi. Nghiên cứu phát hiện rằng tự đánh giá của các cá nhân—liệu họ có giải quyết chính xác bài toán bằng cách sử dụng một LLM hay không—không phải lúc nào cũng chính xác.

Các phương pháp trên là hợp lý cho việc sử dụng thành công các LLM đa mục đích hiện đại. Ngoài ra, chúng tôi dự đoán rằng các LLM được thiết kế đặc biệt để chứng minh định lý sẽ được phát triển trong tương lai. Một con đường để đạt được điều này là bằng cách kết hợp các chứng minh do LLM tạo ra với các bộ chứng minh định lý tương tác [6,2]. Các bước đầu tiên theo hướng này đã được thực hiện và dường như rất hứa hẹn [12, 11, 43, 44].

5 Đo lường Hiệu suất LLM trên Toán học
Trong [13], một nghiên cứu thực nghiệm đã được thực hiện để nghiên cứu khả năng suy luận toán học của ba LLM được coi là tiên tiến về mặt hiệu suất tổng quát: Hai phiên bản ChatGPT (9-01-2023 và 30-01-2023) và GPT-4. Các prompt được sử dụng để thực hiện đánh giá tương ứng với một số trường hợp sử dụng được thảo luận trong Phần 4.

(PP) Tạo chứng minh: Các bài tập từ các sách giáo khoa nổi tiếng (Probability Theory của R. Durret [9], Topology của J. R. Munkres [24], và Functional Analysis của W. Rudin [31]) được đưa cho các LLM.

(FH) Lấp đầy khoảng trống: Các chứng minh có khoảng trống được đưa cho các LLM, và chúng được yêu cầu lấp đầy khoảng trống.

(SE) Hoạt động như một công cụ tìm kiếm toán học: Các LLM được yêu cầu đưa ra định nghĩa của các khái niệm như: "Không gian Banach là gì?". Ngoài ra, chúng được yêu cầu cung cấp tên của các định nghĩa, như trong "Một không gian vector chuẩn hoàn chỉnh được gọi là gì?". Cuối cùng, chúng được yêu cầu cung cấp các ý tưởng chứng minh được sử dụng trong các định lý nổi tiếng.

(CO) Tính toán: Các LLM được giao các nhiệm vụ toán học trong đó các đại lượng phải được tính toán.

Để thực hiện phân tích này, bộ dữ liệu GHOSTS đã được giới thiệu; xem Bảng 1 để biết mô tả chi tiết hơn về các bộ dữ liệu con của nó (tạo nên từ viết tắt "GHOSTS"), cũng như cách chúng tương ứng với các trường hợp sử dụng được liệt kê ở trên. Nó bao gồm 709 prompt, mỗi prompt được đưa cho ba mô hình được xem xét. Các phản hồi của LLM được đánh giá bởi các nhà toán học chuyên nghiệp¹¹ trên thang điểm từ một (thất bại trong việc hiểu truy vấn) đến năm (đầu ra hoàn hảo hoặc gần như hoàn hảo). Các đánh giá thu được được hiển thị trong Hình 3. Chúng ta có thể đưa ra các quan sát sau:

¹¹Các tác giả của bài báo hiện tại tạo thành một tập con của các đánh giá viên.
8

--- TRANG 9 ---
Bảng 1: Tóm tắt tất cả các file từ tất cả các bộ dữ liệu con bao gồm bộ dữ liệu GHOSTS, cùng với kích thước của chúng, tức là, số lượng prompt và các thẻ thuộc tính liên quan.

Tên Bộ dữ liệu con    Kích thước    Loại
Grad-Text    130    PP
Holes-in-Proofs    162    FH  
Olympiad-Problem-Solving    101    PP
Symbolic-Integration    100    CO
MATH    138    CO
Search-Engine-Aspects    78    SE

[Hình 3: Đánh giá trung bình cho mỗi file trong mỗi bộ dữ liệu con (in đậm) của GHOSTS cho các phiên bản đã nghiên cứu của ChatGPT và GPT-4. Xếp hạng tối đa là 5, và xếp hạng tối thiểu, nơi câu hỏi ít nhất được hiểu, là 2; đánh giá 1 chỉ ra rằng câu trả lời hoàn toàn bỏ lỡ câu hỏi. Do đó, một điểm đỗ hợp lý, tức là, 50% điểm, tương ứng với điểm 3.5, được chỉ ra bằng đường chấm. Các thanh lỗi biểu thị khoảng tin cậy 95%.]

Grad-Text
W. Rudin, Functional Analysis (ch. 1)
W. Rudin, Functional Analysis (ch. 2)  
J. Munkres, Topology (ch. 1)
J. Munkres, Topology (ch. 2)
R. Durret, Probability Theory
Holes-in-Proofs
Proofs Collection A
Proofs Collection B Prealgebra
Proofs Collection B Precalculus
Olympiad-Problem-Solving
Symbolic-Integration
MATH
MATH Algebra
MATH Counting and Probability
MATH Prealgebra  
MATH Precalculus
Search-Engine-Aspects
Definition Retrieval
Reverse Definition Retrieval
Named Theorem Proof Completion

Bộ dữ liệu con
Phiên bản
9-Jan.
30-Jan.
GPT-4

9

--- TRANG 10 ---
[Hình 4: Biểu đồ Sankey về cách các đánh giá phát triển từ phiên bản ChatGPT ngày 9-01-2023 sang phiên bản ChatGPT ngày 30-01-2023, và sau đó sang GPT-4 (từ trên xuống dưới), với tất cả các mô hình được đánh giá trên một tập con đại diện của GHOSTS. Mỗi điểm được mã hóa màu, và độ rộng đường tỷ lệ với số lượng đánh giá.]

• LLM hoạt động tốt như một công cụ tìm kiếm: ChatGPT và GPT-4 đạt điểm xuất sắc khi chúng tôi yêu cầu định nghĩa của một khái niệm hoặc tên của một định lý hoặc định nghĩa.

• ChatGPT và GPT-4 gặp khó khăn với các câu hỏi khó: Không có phiên bản nào của các LLM được thử nghiệm đạt kết quả thỏa đáng trên bộ bài toán khó nhất—Olympiad-Problem-Solving. Tương tự, trên các câu hỏi phân tích hàm từ Rudin (Chương 2)—có thể coi là bộ câu hỏi tiên tiến thứ hai trong bộ dữ liệu—kết quả không ấn tượng. Các đánh giá tốt hơn đáng kể cho các câu hỏi đơn giản hơn, chẳng hạn như các bài tập tô-pô, chỉ yêu cầu lý thuyết tập hợp đơn giản và logic Boolean.

• Kết quả tốt cho tính toán đơn giản: Mặc dù không có bộ giải số tích hợp sẵn, GPT-4 hoạt động khá tốt trên các câu hỏi yêu cầu tính toán đơn giản. Đối với tính toán phức tạp hơn liên quan đến Symbolic-Integration, ChatGPT thất bại và GPT-4 chỉ vừa đạt điểm đỗ.

• Đầu vào của người dùng có thể có tác động tích cực: Trên bộ dữ liệu con Holes-in-Proofs, chúng ta thấy kết quả xuất sắc trong một số bài toán. Có vẻ như ngữ cảnh bổ sung được người dùng cung cấp giúp LLM tạo ra các giải pháp trung thực hơn. Một quan sát tương tự đã được thực hiện trong một thí nghiệm riêng biệt nơi các prompt được thiết kế cẩn thận (được gọi là kỹ thuật prompt) đã tăng nhẹ điểm của LLM trên bộ dữ liệu con Olympiad-Problem-Solving [13].

Sự cải thiện trong đánh giá, khi các mô hình trở nên tinh vi hơn, được chỉ ra trong biểu đồ Sankey trong Hình 4, cho thấy cách các đánh giá thay đổi từ phiên bản mô hình này sang phiên bản khác. Chúng tôi sử dụng một tập con đại diện của GHOSTS để tư vấn biểu đồ Sankey. Chúng ta quan sát thấy giữa phiên bản 9-01-2023 và phiên bản 30-01-2023, các điểm được xáo trộn gần như, và không có sự gia tăng đáng kể của điểm ròng xảy ra. Đối với thế hệ mới hơn, tức là, GPT-4, chúng ta có thể quan sát một cải thiện đáng kể trong các đánh giá. Điều này hỗ trợ xu hướng chung rằng các mô hình hiện đại hơn cũng hoạt động tốt hơn trên các nhiệm vụ suy luận toán học thử thách.

Trong [13], một phân chia của mỗi loại câu hỏi (như được chỉ ra trong Hình 3) đã được thực hiện, và một benchmark tinh tế hơn nhiều đã được giới thiệu cũng phân biệt các chế độ thất bại tiềm năng. Chúng tôi giới thiệu người đọc đến [13] để biết thêm thông tin về các phân tích chi tiết hơn.

6 Kết luận
Nghiên cứu của chúng tôi đã làm nổi bật cách LLM sở hữu khả năng đáng chú ý để hỗ trợ các nhà toán học theo nhiều cách khác nhau, từ phát hiện và lấp đầy các khoảng trống trong định lý đến hoạt động như công cụ tìm kiếm và tìm định nghĩa từ mô tả các đối tượng toán học. Chúng tôi đã làm sáng tỏ cơ chế nội bộ của phần cốt lõi của kiến trúc hỗ trợ các LLM hiện đại, transformer, và cách thức chúng tạo ra câu trả lời cho các câu hỏi toán học khác hoàn toàn so với suy luận của con người.

10

--- TRANG 11 ---
Khả năng của LLM tương tác với người dùng bằng định dạng ngôn ngữ tự nhiên đã làm cho toán học trở nên dễ tiếp cận hơn, cho phép một phạm vi rộng hơn các cá nhân tham gia vào nghiên cứu và giáo dục toán học. Trong khi tiềm năng đầy đủ của LLM trong việc tự động hóa toán học vẫn chưa được thực hiện, các phát hiện của chúng tôi gợi ý một sự hợp tác hứa hẹn giữa các nhà toán học con người và trí tuệ nhân tạo. Sự tiếp xúc cao của công việc của các nhà toán học với các tác động của LLM cũng đã được báo cáo trong [10]. Tuy nhiên, chúng tôi muốn cảnh báo rằng, hiện tại, LLM không đang trên quỹ đạo thay thế các nhà toán học. Trong [13], đã được chỉ ra rằng ngay cả mô hình hoạt động tốt nhất cũng gặp khó khăn với toán học ở mức độ khó của sinh viên năm cuối đại học, chẳng hạn như khi nó được giao nhiệm vụ giải quyết các bài tập từ Functional Analysis của W. Rudin [31]. Hiệu suất của LLM đã được báo cáo là thấp hơn so với con người cũng trong các lĩnh vực liên quan, chẳng hạn như các thử thách lập trình trong khoa học máy tính [19]. Tuy nhiên, chúng tôi dự đoán rằng sự xuất hiện của LLM sẽ là một thử thách cho giáo dục và nghiên cứu. Các bài tập hoặc bài tập về nhà đơn giản và các bước riêng lẻ trong nghiên cứu toán học sẽ dần được hỗ trợ bởi tự động hóa hoặc trở nên lỗi thời.

Lời cảm ơn
S. Frieder và T. Lukasiewicz được hỗ trợ một phần bởi AXA Research Fund.

Tài liệu tham khảo
[1] J. L. Ba, J. R. Kiros, và G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

[2] B. Barras, S. Boutin, C. Cornes, J. Courant, J.-C. Filliatre, E. Gimenez, H. Herbelin, G. Huet, C. Munoz, C. Murthy, et al. The Coq proof assistant reference manual: Version 6.1. PhD thesis, Inria, 1997.

[3] Y. Bengio, R. Ducharme, và P. Vincent. A neural probabilistic language model. Advances in Neural Information Processing Systems, 13, 2000.

[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

[5] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong, M. Zilka, U. Bhatt, T. Lukasiewicz, Y. Wu, J. B. Tenenbaum, W. Hart, et al. Evaluating language models for mathematics through interactions. arXiv preprint arXiv:2306.01694, 2023.

[6] L. de Moura, S. Kong, J. Avigad, F. Van Doorn, và J. von Raumer. The Lean theorem prover (system description). In Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, pages 378–388, 2015.

[7] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[8] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, et al. GLaM: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547–5569. PMLR, 2022.

[9] R. Durrett. Probability: Theory and Examples. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.

[10] T. Eloundou, S. Manning, P. Mishkin, và D. Rock. GPTs are GPTs: An early look at the labor market impact potential of large language models. arXiv preprint arXiv:2303.10130, 2023.

[11] E. First, M. N. Rabe, T. Ringer, và Y. Brun. Baldur: whole-proof generation and repair with large language models. arXiv preprint arXiv:2303.04910, 2023.

[12] S. Frieder, M. Alawadhi, Trimmel, Rashid, và K. Gy. LLM vs ITP. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23, 2023.

[13] S. Frieder, L. Pinchetti, R.-R. Griffiths, T. Salvatori, T. Lukasiewicz, P. C. Petersen, A. Chevalier, và J. Berner. Mathematical capabilities of ChatGPT. In Advances in Neural Information Processing Systems, volume 36, 2023.

11

--- TRANG 12 ---
[14] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.

[15] A. Haviv, O. Ram, O. Press, P. Izsak, và O. Levy. Transformer language models without positional encodings still learn positional information. arXiv preprint arXiv:2203.16634, 2022.

[16] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, và J. Steinhardt. Measuring mathematical problem solving with the MATH dataset. arXiv preprint arXiv:2103.03874, 2021.

[17] D. Hendrycks và K. Gimpel. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415, 2016.

[18] A. Holtzman, J. Buys, L. Du, M. Forbes, và Y. Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.

[19] A. Koubaa, B. Qureshi, A. Ammar, Z. Khan, W. Boulila, và L. Ghouti. Humans are still better than ChatGPT: Case of the IEEEXtreme competition. arXiv preprint arXiv:2305.06934, 2023.

[20] C. Li. OpenAI's GPT-3 language model: A technical overview, 2020. https://lambdalabs.com/blog/demystifying-gpt-3.

[21] L. Liu, J. Liu, và J. Han. Multi-head or single-head? an empirical comparison for transformer training. arXiv preprint arXiv:2106.09650, 2021.

[22] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, và V. Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

[23] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heintz, và D. Roth. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys, 56(2):1–40, 2023.

[24] J. R. Munkres. Topology. Prentice-Hall, 2000.

[25] OpenAI. Introducing ChatGPT, 2022. https://openai.com/blog/chatgpt.

[26] OpenAI. GPT-4 technical report. arXiv preprint 2303.0877, 2023.

[27] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.

[28] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training, 2018. https://openai.com/research/language-unsupervised.

[29] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, và I. Sutskever. Language models are unsupervised multitask learners, 2019. https://github.com/openai/gpt-2.

[30] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li, X. Zhang, A. Podolskiy, G. Arshinov, et al. PanGu-Σ: Towards trillion parameter language model with sparse heterogeneous computing. arXiv preprint arXiv:2303.10845, 2023.

[31] W. Rudin. Functional analysis. McgGraw-Hill, 1991.

[32] V. Sanh, L. Debut, J. Chaumond, và T. Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.

[33] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, và T. Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.

[34] R. Sennrich, B. Haddow, và A. Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.

12

--- TRANG 13 ---
[35] E. Strubell, A. Ganesh, và A. McCallum. Energy and policy considerations for deep learning in NLP. arXiv preprint arXiv:1906.02243, 2019.

[36] I. Tenney, D. Das, và E. Pavlick. BERT rediscovers the classical NLP pipeline. arXiv preprint arXiv:1905.05950, 2019.

[37] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.

[38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và I. Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.

[41] G. Williamson. Is deep learning a useful tool for the pure mathematician? arXiv preprint arXiv:2304.12602, 2023.

[42] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, và A. M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020.

[43] K. Yang và J. Deng. Learning to prove theorems via interacting with proof assistants. In International Conference on Machine Learning, pages 6984–6994. PMLR, 2019.

[44] K. Yang, A. M. Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil, R. Prenger, và A. Anandkumar. LeanDojo: Theorem proving with retrieval-augmented language models. arXiv preprint arXiv:2306.15626, 2023.

[45] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.

13
