# 2305.07004.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2305.07004.pdf
# Kích thước tệp: 1840733 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Không phải tất cả ngôn ngữ đều được tạo ra như nhau trong LLM:
Cải thiện khả năng đa ngôn ngữ bằng phương pháp Cross-Lingual-Thought Prompting
Haoyang Huang1∗, Tianyi Tang2∗, Dongdong Zhang1†, Wayne Xin Zhao2
Ting Song1,Yan Xia1,Furu Wei1
1Microsoft Research Asia, China
2Gaoling School of Artificial Intelligence, Renmin University of China
https://github.com/microsoft/unilm

(a)
80.886.6
84.692.6
82.1
5060708090100
MGSM XCOPA XNLI PAWS-X MKQA72.481.9 82.692.2
69.6
5060708090100
MGSM XCOPA XNLI PAWS-X MKQA(%)(b)

Hình 1: So sánh hiệu quả của prompt Cross-Lingual-Thought với prompt cơ bản baseline
trên 7 benchmark đại diện bao gồm 27 ngôn ngữ: (a) Nâng cao khả năng đa ngôn ngữ của
text-davinci-003 trong học zero-shot, và (b) Thu hẹp khoảng cách giữa hiệu suất trung bình và
hiệu suất tốt nhất của mỗi nhiệm vụ trong các ngôn ngữ khác nhau.

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) thể hiện khả năng đa ngôn ngữ ấn tượng, nhưng hiệu suất của chúng thay đổi đáng kể giữa các ngôn ngữ khác nhau. Trong công trình này, chúng tôi giới thiệu một phương pháp đơn giản nhưng hiệu quả, được gọi là cross-lingual-thought prompting (XLT), để cải thiện có hệ thống khả năng đa ngôn ngữ của LLM. Cụ thể, XLT là một template prompt tổng quát kích thích kỹ năng suy luận cross-lingual và logic để nâng cao hiệu suất nhiệm vụ trên các ngôn ngữ. Chúng tôi tiến hành đánh giá toàn diện trên 7 benchmark tiêu biểu liên quan đến các nhiệm vụ suy luận, hiểu biết và sinh tạo, bao gồm cả ngôn ngữ có tài nguyên cao và thấp. Kết quả thực nghiệm cho thấy XLT không chỉ nâng cao đáng kể hiệu suất của các nhiệm vụ đa ngôn ngữ khác nhau mà còn giảm thiểu đáng kể khoảng cách giữa hiệu suất trung bình và hiệu suất tốt nhất của mỗi nhiệm vụ trong các ngôn ngữ khác nhau. Đáng chú ý, XLT mang lại hơn 10 điểm cải thiện trung bình trong các nhiệm vụ suy luận số học và trả lời câu hỏi miền mở.

∗Đóng góp ngang nhau. †Tác giả liên hệ.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) thể hiện khả năng đa ngôn ngữ ấn tượng trong một loạt các nhiệm vụ xử lý ngôn ngữ tự nhiên, bao gồm sinh tạo ngôn ngữ, sử dụng kiến thức và suy luận phức tạp (Zhao et al., 2023). Hiệu suất của chúng trong các nhiệm vụ downstream đã được chứng minh đạt tới hoặc thậm chí vượt qua hiệu suất ở mức con người (Brown et al., 2020; Chowdhery et al., 2022; Scao et al., 2022). Khả năng của LLM xuất phát từ khối lượng dữ liệu huấn luyện khổng lồ mà chúng đã tận dụng (Kaplan et al., 2020). Dữ liệu huấn luyện cho các mô hình hiện tại chủ yếu được thống trị bởi corpus tiếng Anh, nhưng nó cũng bao gồm dữ liệu từ các ngôn ngữ khác, như được mô tả trong GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), và BLOOM (Scao et al., 2022), v.v.

Có hơn 7,000 ngôn ngữ trên toàn thế giới, với phần lớn là ngôn ngữ có tài nguyên thấp hoặc cực thấp (Forkel et al., 2022). Mặc dù mô hình GPT-4 mới nhất (OpenAI, 2023) thể hiện một số khả năng tổng quát hóa trong các nhiệm vụ đa ngôn ngữ như được đánh giá trên benchmark MMLU (Hendrycks et al., 2021), vẫn còn trường hợp LLM không có khả năng bằng nhau để xử lý tất cả các ngôn ngữ, dẫn đến khả năng không cân bằng giữa các ngôn ngữ khác nhau. Hơn nữa, một số kết quả đánh giá (Bang et al., 2023; Jiao et al., 2023; Hendy et al., 2023; Zhu et al., 2023) chỉ ra rằng các mô hình lớn gặp khó khăn trong việc hiểu và sinh tạo các ngôn ngữ không phải tiếng Anh, đặc biệt là trong các ngôn ngữ có tài nguyên thấp hoặc cực thấp.

Do đó, để dân chủ hóa trí tuệ ngôn ngữ và giảm thiểu khoảng cách hiệu suất trong các ngôn ngữ khác nhau, việc kích thích và nâng cao khả năng đa ngôn ngữ của mô hình trong các ngôn ngữ không phải tiếng Anh và có tài nguyên thấp là cần thiết và có ý nghĩa.

Một cách trực quan, LLM có thể cải thiện khả năng đa ngôn ngữ bằng cách tăng cường dữ liệu (Lin et al., 2022) hoặc fine-tuning mô hình (Chen et al., 2021, 2022), nhưng cả hai đều tốn kém về mặt tính toán. Thay vào đó, học trong ngữ cảnh với prompt cũng có thể tăng hiệu suất (Brown et al., 2020; Ahuja et al., 2023; Wei et al., 2022c) nhưng bị giới hạn ở các nhiệm vụ đơn ngôn ngữ (Sanh et al., 2022).

Công trình này khám phá một phương pháp học trong ngữ cảnh phổ quát để nâng cao khả năng đa ngôn ngữ của LLM. Chúng tôi giới thiệu một phương pháp đơn giản nhưng hiệu quả, được gọi là cross-lingual-thought prompting (XLT), để cho phép mô hình xử lý các nhiệm vụ xử lý ngôn ngữ tự nhiên khác nhau trên các ngôn ngữ mục tiêu khác nhau. Phương pháp của chúng tôi sử dụng một prompt tổng quát và độc lập với ngôn ngữ, loại bỏ nhu cầu cập nhật tham số mô hình. Tùy thuộc vào loại đầu vào của nhiệm vụ, cross-lingual-thought prompting hướng dẫn mô hình ngôn ngữ lớn đảm nhận vai trò của một chuyên gia trong một ngôn ngữ cụ thể cho một nhiệm vụ cụ thể. Với thông tin meta được định nghĩa trước, XLT chỉ đạo LLM phản hồi một cách logic thông qua một quy trình bao gồm hiểu vấn đề, suy nghĩ cross-lingual, phân tích nhiệm vụ, thực hiện nhiệm vụ và định dạng đầu ra. Trong quá trình này, phương pháp của chúng tôi được thiết kế để kích thích kỹ năng suy luận cross-lingual và logic của mô hình, cho phép chúng phản hồi các yêu cầu đầu vào bất kể ngôn ngữ nào. Để nâng cao hiệu suất, học few-shot cũng có thể được sử dụng với phương pháp của chúng tôi bằng cách cung cấp đầu ra phản hồi do LLM tạo ra làm minh chứng sử dụng học zero-shot cross-lingual-thought prompting.

Chúng tôi tiến hành đánh giá toàn diện để xác minh hiệu quả của XLT trên bảy benchmark đa ngôn ngữ đại diện về các nhiệm vụ suy luận, hiểu biết và sinh tạo ngôn ngữ tự nhiên. Mỗi benchmark bao gồm dữ liệu đa ngôn ngữ bao gồm cả ngôn ngữ có tài nguyên cao và thấp. Kết quả thực nghiệm chứng minh rằng phương pháp của chúng tôi có thể cải thiện đáng kể hiệu suất của tất cả các benchmark trên các ngôn ngữ trong cả cài đặt học zero-shot và few-shot. Đáng chú ý, XLT đạt được mức tăng trung bình hơn 10 điểm trên các benchmark MGSM và MKQA. Hơn nữa, chúng tôi quan sát thấy phương pháp prompting của chúng tôi giảm đáng kể khoảng cách giữa hiệu suất trung bình và hiệu suất tốt nhất của mỗi nhiệm vụ trong các ngôn ngữ khác nhau, cho thấy tiềm năng của nó để dân chủ hóa trí tuệ ngôn ngữ.

--- TRANG 2 ---
LLM Đầu vào Yêu cầu: 詹姆斯决定每周跑 3 次 3 段冲刺，每段冲刺跑 60 米。他每周一共跑多少米?
Yêu cầu: James quyết định chạy 3 bộ 60 mét sprint ba lần một tuần. Anh ấy chạy tổng cộng bao nhiêu mét mỗi tuần?
Đầu ra 1. James chạy 3 bộ 60 mét sprint, có nghĩa là anh ấy chạy 60 x 3 = 180 mét mỗi lần sprint.
2. James chạy 3 bộ 180 mét mỗi tuần, có nghĩa là anh ấy chạy 180 x 3 = 540 mét mỗi tuần.
Đáp án: James chạy tổng cộng 540 mét mỗi tuần.
Đáp án từng bước: 540

Yêu cầu: 詹姆斯决定每周跑 3 次 3 段冲刺，每段冲刺跑 60 米。他每周一共跑多少米?
XLT Tôi muốn bạn đóng vai một chuyên gia suy luận số học cho tiếng Trung.
Bạn nên kể lại yêu cầu bằng tiếng Anh.
Bạn nên đưa ra đáp án từng bước để có được một đáp án số.
Bạn nên trả lời từng bước cho yêu cầu.
Bạn nên cho tôi biết đáp án theo định dạng này 'Đáp án:'.
yêu cầu suy luận số học tiếng Trung đáp án
Đáp án thực hiện đáp án từng bước để có được một đáp án số

Hình 2: Tổng quan về phương pháp của chúng tôi. Với một yêu cầu, thông tin meta liên quan của nó được điền vào các placeholder của template XLT để tạo thành prompt độc lập với ngôn ngữ, được đưa vào LLM để nâng cao việc tạo ra phản hồi theo định dạng mong muốn.

2 Cross-Lingual-Thought Prompting
Mặc dù LLM có khả năng chấp nhận bất kỳ đầu vào nào và tạo ra phản hồi, người dùng thường cấu trúc yêu cầu của họ dưới dạng prompt để thu được đầu ra mong muốn. Thiết kế của những prompt này rất quan trọng để đạt được hiệu suất tối ưu trong các nhiệm vụ downstream, vì LLM nhạy cảm với định dạng của prompt được chọn (Zhao et al., 2021). Thông qua một quy trình được gọi là instruction tuning (Wei et al., 2022a), mô hình có thể phát triển khả năng tuân theo hướng dẫn ngôn ngữ tự nhiên (Wei et al., 2022b), có thể giảm độ nhạy cảm của chúng đối với prompt engineering (Wei et al., 2022a). Theo hướng dẫn của OpenAI cookbook¹, chúng tôi đề xuất một template cross-lingual thought prompting, được ký hiệu là template XLT. Template tổng quát này cho phép LLM phản hồi các yêu cầu với cross-lingual thought và hỗ trợ một loạt các nhiệm vụ đa ngôn ngữ.

Hình 3 hiển thị template XLT, với các phần được tô màu đại diện cho các placeholder. Hình 2 giới thiệu một ví dụ về prompt được khởi tạo cho yêu cầu tiếng Trung. Phần tiếp theo sẽ giải thích chi tiết về việc xây dựng XLT.

2.1 Xây dựng XLT
Template XLT được thiết kế để mô phỏng quy trình con người sử dụng khi xử lý các nhiệm vụ đa ngôn ngữ. Template của chúng tôi được viết bằng tiếng Anh, vì tiếng Anh là ngôn ngữ thống trị trong quá trình pre-training LLM, và nghiên cứu hiện tại chỉ ra rằng prompting tiếng Anh hiệu quả hơn cho các nhiệm vụ đa ngôn ngữ (Shi et al., 2023). Trái ngược với prompt vanilla chỉ bao gồm mô tả nhiệm vụ, template XLT của chúng tôi nhằm mục đích gợi ra khả năng đa ngôn ngữ thông qua cross-lingual thoughts. Template này bao gồm sáu hướng dẫn logic theo trình tự. Để hoàn thành template, chỉ cần điền vào bảy placeholder dựa trên kiến thức nội tại về nhiệm vụ và yêu cầu, như được mô tả trong Hình 3.

Gán vai trò. Đầu tiên, mô hình nhận được định nghĩa vai trò giúp thiết lập hành vi của mô hình. Khái niệm này tương tự như vai trò hệ thống của ChatGPT². Để đạt được điều này, chúng tôi chỉ cần hoàn thành tên nhiệm vụ với một danh mục đã biết (như suy luận thường thức hoặc nhận dạng paraphrase), cùng với ngôn ngữ của nhiệm vụ trong trường ngôn ngữ nhiệm vụ.

Nhập nhiệm vụ. Thứ hai, chúng tôi rõ ràng thêm yêu cầu làm đầu vào nhiệm vụ. Yêu cầu cơ bản được cấu trúc theo loại nhiệm vụ để đảm bảo mô hình có thể hiểu nó. Ví dụ, trong nhiệm vụ suy luận ngôn ngữ tự nhiên, hai đầu vào câu được chỉ định với "premise" và "hypothesis" tương ứng.

Suy nghĩ Cross-lingual. Chúng tôi khuyến khích mô hình tham gia vào suy nghĩ cross-lingual bằng cách diễn đạt lại nội dung được yêu cầu bằng tiếng Anh, là ngôn ngữ thống trị được sử dụng làm ngôn ngữ chính bởi Shi et al. (2023) và Ahuja et al. (2023). Diễn đạt lại nội dung được yêu cầu được bao trong thẻ input giúp mô hình hiểu rõ hơn yêu cầu bằng ngôn ngữ gốc và kiến thức của nó. Quan sát của chúng tôi cho thấy việc sử dụng các từ khóa như "retell" hoặc "repeat" trong khi diễn đạt lại nội dung có thể dẫn đến hiệu suất tốt hơn trong thực tế.

²https://platform.openai.com/docs/guides/chat/introduction

--- TRANG 3 ---

Tôi muốn bạn đóng vai một chuyên gia task_name cho task_language.
task_input
Bạn nên kể lại/lặp lại input_tag bằng tiếng Anh.
Bạn nên task_goal.
Bạn nên trả lời từng bước cho yêu cầu.
Bạn nên cho tôi biết output_type (output_constraint) theo định dạng này 'output_type:'.

Hình 3: Minh họa template XLT. Tham khảo Hình 2 và Phụ lục để có ví dụ được khởi tạo.

Phân tích nhiệm vụ. Sau khi diễn đạt lại đầu vào nhiệm vụ, chúng ta cần hoàn thành nhiệm vụ trong task_goal. Bước này có thể so sánh với mô tả nhiệm vụ được sử dụng trong các phương pháp prompting thông thường. Trong thực tế, chúng ta có thể lấy thông tin nhiệm vụ từ tài liệu hoặc tìm kiếm sự hỗ trợ từ ChatGPT để tạo ra prompt hiệu quả cho việc giải quyết nhiệm vụ (Jiao et al., 2023).

Giải quyết nhiệm vụ CoT. Sau đó chúng tôi yêu cầu mô hình tuân theo hướng dẫn và hoàn thành nhiệm vụ từng bước. Vì LLM thể hiện khả năng mạnh mẽ trong việc duy trì chuỗi suy nghĩ (Wei et al., 2022c), chúng tôi thiết kế cẩn thận các hướng dẫn để hướng dẫn mô hình, với hy vọng nó sẽ phản hồi theo hướng dẫn của chúng tôi một cách từng bước và sử dụng các đầu ra trung gian để hỗ trợ giải quyết nhiệm vụ.

Định dạng đầu ra. Cuối cùng, chúng tôi nên chuẩn hóa định dạng đầu ra của mô hình để có được câu trả lời chính xác. LLM được sử dụng theo cách zero- hoặc few-shot, và chúng có xu hướng tạo ra văn bản có thể không phù hợp với định dạng của câu trả lời mục tiêu. May mắn thay, LLM có khả năng mạnh mẽ trong việc tuân theo hướng dẫn, và chúng ta có thể định nghĩa định dạng đầu ra theo output_type và output_constraint. Output_type có thể là số, chỉ mục, hoặc văn bản, trong khi output_constraint là tùy chọn và được xác định dựa trên yêu cầu nhiệm vụ. Output_constraint có thể bao gồm giới hạn độ dài, đặc tả ngôn ngữ, và các yếu tố liên quan khác.

2.2 XLT cho Few-shot Learning
Việc xây dựng XLT ở trên có thể được đưa trực tiếp vào LLM để tạo ra đầu ra, điều này được thực hiện trong cài đặt học zero-shot. Ngoài ra, chúng tôi cũng khám phá việc kết hợp minh chứng vào XLT để cho phép học few-shot. Khác với công trình trước đây chỉ thêm đầu ra mô hình vào yêu cầu tương ứng (Shi et al., 2023) hoặc sử dụng verbalizer để định dạng đầu ra, phương pháp của chúng tôi xây dựng các minh chứng với đầu ra mô hình được định dạng tốt hơn từ việc xử lý từng bước dựa trên XLT.

Như được minh họa trong Hình 4, đầu tiên chúng tôi lấy mẫu một số ví dụ từ tập phát triển và kết hợp các phần được yêu cầu vào XLT. Học zero-shot được thực hiện trên LLM để thu thập phản hồi được căn chỉnh thêm với những phản hồi của các mẫu. Chỉ những yêu cầu được căn chỉnh phản hồi mới được tập hợp với các phản hồi mô hình tương ứng để tạo thành các minh chứng cuối cùng cho học few-shot. Bằng cách này, các minh chứng được xây dựng với kiến thức logic phong phú qua XLT, điều này sẽ phục vụ cho việc tạo ra dựa trên XLT của các yêu cầu mới. Trong thực tế, chúng ta cũng có thể chỉnh sửa hoặc thiết kế các minh chứng để căn chỉnh tốt hơn với logic hướng dẫn.

Yêu cầu → XLT → LLM → Phản hồi
Dữ liệu phát triển → Minh chứng → Căn chỉnh phản hồi?

Hình 4: Quy trình xây dựng cho học few-shot.

3 Thí nghiệm
Để xác minh toàn diện hiệu quả của phương pháp chúng tôi về tính tổng quát độc lập với ngôn ngữ, chúng tôi đánh giá template XLT của chúng tôi trên các LLM khác nhau bao gồm các nhiệm vụ xử lý ngôn ngữ tự nhiên khác nhau bằng nhiều ngôn ngữ.

3.1 Thiết lập thí nghiệm

3.1.1 Nhiệm vụ và Benchmark
Chúng tôi tiến hành đánh giá trên bảy benchmark tiêu biểu liên quan đến các nhiệm vụ suy luận, hiểu biết và sinh tạo có thể đại diện cho các khả năng khác nhau của LLM, bao gồm cả ngôn ngữ có tài nguyên cao và thấp. Các benchmark này bao gồm 27 ngôn ngữ khác nhau, bao gồm tiếng Anh (en), tiếng Đức (de), tiếng Nga (ru), tiếng Pháp (fr), tiếng Trung giản thể (zh), tiếng Tây Ban Nha (es), tiếng Nhật (ja), tiếng Ý (it), tiếng Việt (vi), tiếng Thổ Nhĩ Kỳ (tr), tiếng Indonesia (id), tiếng Swahili (sw), tiếng Ả Rập (ar), tiếng Hàn (ko), tiếng Hy Lạp (el), tiếng Thái (th), tiếng Bulgaria (bg), tiếng Hindi (hi), tiếng Estonia (et), tiếng Bengal (bn), tiếng Tamil (ta), tiếng Galician (gl), tiếng Urdu (ur), tiếng Telugu (te), tiếng Java (jv), tiếng Haiti Creole (ht), và tiếng Quechua miền Nam (qu). Về thống kê phân bố ngôn ngữ trong Common Crawl Monthly Archives³ và hiệu suất ngôn ngữ của LLM (Shi et al., 2023; Ahuja et al., 2023), chúng tôi đã sắp xếp chúng theo thứ tự tần suất ngôn ngữ từ tài nguyên cao đến thấp. Đặc biệt, tần suất của một số ngôn ngữ thiểu số thậm chí còn ít hơn 0.1% (ví dụ, bn, ta, gl, ur, te, jv, ht, qu).

• Nhiệm vụ suy luận
– Suy luận số học. Benchmark MGSM (Shi et al., 2023) chứa các bài toán toán học cấp tiểu học và yêu cầu mô hình tính toán câu trả lời đúng. Nó bao gồm 11 ngôn ngữ, và chúng tôi sử dụng điểm độ chính xác để đánh giá.
– Suy luận thường thức. Benchmark XCOPA (Ponti et al., 2020) chứa một tiền đề và hai lựa chọn. Nó yêu cầu mô hình chọn lựa chọn nào là kết quả hoặc nguyên nhân của tiền đề. Nó bao gồm 11 ngôn ngữ từ 11 họ ngôn ngữ đa dạng, và chúng tôi sử dụng điểm độ chính xác để đánh giá.

• Nhiệm vụ hiểu biết
– Suy luận ngôn ngữ tự nhiên. Benchmark XNLI (Conneau et al., 2018) chứa một tiền đề và một giả thuyết và yêu cầu mô hình xác định liệu giả thuyết có được entail, contradiction, hay neutral dựa trên tiền đề. Nó bao gồm 15 ngôn ngữ, và chúng tôi sử dụng điểm độ chính xác để đánh giá.
– Nhận dạng paraphrase. Benchmark PAWS-X (Yang et al., 2019) chứa hai câu và yêu cầu mô hình phán đoán liệu chúng có paraphrase lẫn nhau hay không. Nó bao gồm 7 ngôn ngữ, và chúng tôi sử dụng điểm độ chính xác để đánh giá.

• Nhiệm vụ sinh tạo
– Trả lời câu hỏi. Benchmark MKQA (Longpre et al., 2021) chứa một câu hỏi miền mở và yêu cầu mô hình dự đoán một câu trả lời ngắn. Vì nó có các câu hỏi không thể trả lời hoặc câu hỏi dài không có câu trả lời chính xác, chúng tôi loại bỏ những câu hỏi này trong quá trình đánh giá. Nó bao gồm 25 ngôn ngữ, và chúng tôi chọn một tập con 10 ngôn ngữ, bao gồm de, en, es, fr, ja, ru, th, tr, vi, và zh. Chúng tôi sử dụng điểm F1 chồng chéo token để đánh giá.
– Tóm tắt. Benchmark XL-Sum* (Hasan et al., 2021) (250 mẫu test được lấy ngẫu nhiên từ XL-Sum mỗi ngôn ngữ) chứa một bài báo tin tức dài và muốn mô hình tóm tắt thành một văn bản ngắn. Nó bao gồm 44 ngôn ngữ, và chúng tôi chọn một tập con 6 ngôn ngữ, bao gồm en, es, fr, tr, vi, và zh. Chúng tôi sử dụng điểm ROUGE-1 (Lin, 2004) để đánh giá.
– Dịch máy. Benchmark FLORES* (Costa-jussà et al., 2022) (200 mẫu test được lấy ngẫu nhiên từ FLORES-200 mỗi ngôn ngữ) chứa văn bản song song từ các dự án Wikimedia cho 204 ngôn ngữ, tạo ra hơn 40,000 hướng dịch. Chúng tôi chọn một tập con 12 hướng, bao gồm dịch từ tài nguyên cao đến tài nguyên cao (tức là zh↔ru và de↔vi), dịch từ tài nguyên cao đến tài nguyên thấp (tức là zh↔th và zh↔jv), và dịch từ tài nguyên thấp đến tài nguyên thấp (tức là th↔gl và jv↔th). Chúng tôi sử dụng điểm SacreBLEU (Papineni et al., 2002; Post, 2018) để đánh giá.

Trong số các benchmark này, MGSM, XCOPA, XNLI, PAWS-X, và MKQA là song song, tức là các instance có nghĩa tương đương trên mỗi ngôn ngữ. Đối với tất cả các benchmark, chúng tôi báo cáo kết quả trên các tập test sử dụng tất cả các instance (Bảng 5), ngoại trừ XL-Sum và FLORES-200, nơi chúng tôi chỉ lấy mẫu 250 và 200 ví dụ tương ứng để hiển thị xu hướng hiệu suất sinh tạo. Trong cài đặt few-shot, chúng tôi chọn ngẫu nhiên các ví dụ từ tập phát triển nếu chúng có, nếu không, chúng tôi dịch tập huấn luyện tiếng Anh sang các ngôn ngữ tương ứng để xây dựng một số ví dụ.

3.1.2 Baseline
Prompt cơ bản là vanilla trong các thí nghiệm của chúng tôi được đề xuất và gợi ý trong công trình trước đây. Sau khi xác định prompt, chúng tôi định dạng mỗi instance đơn ngôn ngữ sử dụng prompt cơ bản tiếng Anh. Cài đặt này tương tự như prompting đơn ngôn ngữ trong MEGA (Ahuja et al., 2023). Các prompt cơ bản được sử dụng để đánh giá mỗi benchmark được liệt kê trong Bảng 5. Lưu ý, chúng tôi bỏ qua baseline sử dụng ngôn ngữ bản địa, vì MEGA (Ahuja et al., 2023) tiết lộ prompting đơn ngôn ngữ vượt trội hơn cross-lingual prompting.

Chain-of-Thought (CoT) prompting kích hoạt LLM tạo ra một chuỗi kết quả trung gian để giải quyết các nhiệm vụ suy luận (Wei et al., 2022c), vẫn hiệu quả trong các tình huống đa ngôn ngữ (Shi et al., 2023). Trong các thí nghiệm, chúng tôi thêm hướng dẫn "Hãy suy nghĩ từng bước và cho tôi biết câu trả lời cuối cùng" sau đầu vào để prompt LLM.

Translate-English tận dụng khả năng mạnh mẽ của LLM trong tiếng Anh để giải quyết các nhiệm vụ đa ngôn ngữ, như được gợi ý bởi cả Shi et al. (2023) và Ahuja et al. (2023). Phương pháp này dịch các instance từ các ngôn ngữ khác sang tiếng Anh trước. Trong thực tế, chúng tôi sử dụng API Google Translate để dịch các ví dụ sang tiếng Anh và áp dụng prompt cơ bản để định dạng chúng. Lưu ý, chúng tôi không áp dụng phương pháp này cho các nhiệm vụ sinh tạo vì chúng yêu cầu đầu ra bằng ngôn ngữ tương ứng chứ không phải tiếng Anh.

XLT sử dụng template được đề xuất bao gồm nhiều hướng dẫn được giới thiệu trong Phần 2. Các template XLT được khởi tạo cho mỗi benchmark được liệt kê trong Bảng 6.

Trong các tình huống học few-shot, đối với prompt cơ bản, chúng tôi sử dụng cùng một template làm đầu vào bổ sung cho mô hình. Đối với XLT, chúng tôi cung cấp các mẫu với đầu vào template XLT và mong đợi đầu ra từng bước mong muốn như được nêu trong Hình 4. Trong đánh giá tiếp theo, chúng tôi áp dụng cài đặt 5-shot, ngoại trừ các thí nghiệm XL-Sum*, sử dụng cài đặt 3-shot do hạn chế độ dài đầu vào.

3.1.3 LLM
Chúng tôi chủ yếu đánh giá hai LLM từ dòng mô hình GPT-3.5:
• text-davinci-003⁴ được huấn luyện sử dụng instruction tuning và reinforcement learning from human feedback (Ouyang et al., 2022). Nó có thể thực hiện một loạt các nhiệm vụ ngôn ngữ tự nhiên với kết quả đáng hài lòng.
• gpt-3.5-turbo⁴ được tối ưu hóa cho chat dựa trên text-davinci-003 và phù hợp cho các nhiệm vụ NLP truyền thống. Nó là mô hình GPT-3.5 có khả năng nhất.

Để xác minh khả năng tương thích của template XLT của chúng tôi, chúng tôi tiếp tục kết hợp LLaMA-2-Chat (Touvron et al., 2023) (Llama-2-70b-chat-hf) làm mô hình cơ sở của chúng tôi. Nó là một mô hình mã nguồn mở đã được huấn luyện thông qua supervised fine-tuning và reinforcement learning from human feedback trên mô hình LLaMA 2 cơ sở. Ngoài ra, chúng tôi cũng tham khảo các kết quả hiện có từ các LLM khác, chẳng hạn như code-davinci-002⁴, khi đánh giá có thể so sánh được. Trong quá trình suy luận, chúng tôi sử dụng greedy search (tức là temperature=0) để tạo ra phản hồi LLM. Chúng tôi thấy LLM có khả năng tuân theo hướng dẫn xuất sắc để phản hồi theo hướng dẫn của chúng tôi theo định dạng đã cho. Do đó, chúng tôi chỉ trích xuất phần sau "Answer format:" làm nhãn.

3.2 Kết quả thí nghiệm
Khả năng đa ngôn ngữ. Chúng tôi đánh giá toàn diện hiệu suất của XLT trên bảy nhiệm vụ. Điểm số trung bình của text-davinci-003 được tóm tắt trong Hình 1(a) và Bảng 1, và chi tiết hơn được liệt kê trong Phụ lục A. Đối với CoT prompting, nó có thể nâng cao các nhiệm vụ suy luận trong khi trở nên kém hiệu quả hơn trên các nhiệm vụ hiểu biết và sinh tạo. Về Translate-En prompting, nó có thể tăng hiệu suất trong cài đặt zero-shot trong khi có thể không hoạt động tốt trong cài đặt few-shot. Tổng thể, so với ba phương pháp baseline, XLT đạt được cải thiện đáng kể trên hai LLM cho tất cả các nhiệm vụ trên cả cài đặt zero-shot và few-shot bất kể sự khác biệt ngôn ngữ, ngoại trừ một sự giảm nhẹ trên benchmark PAWS-X trong cài đặt zero-shot. Đáng chú ý là XLT đạt được mức tăng đáng kể gần 20 điểm trung bình trong benchmark MGSM cho nhiệm vụ suy luận số học và khoảng 10 điểm trung bình trong benchmark MKQA cho nhiệm vụ trả lời câu hỏi miền mở. Các thí nghiệm chứng minh hiệu quả của XLT trong việc trao quyền cho LLM với khả năng đa ngôn ngữ.

Đối với thử nghiệm tương thích, chúng tôi liệt kê kết quả của LLaMA-2-Chat trên benchmark MGSM trong Bảng 7. Đáng chú ý là LLaMA 2 cũng có thể hưởng lợi từ cross-lingual-thought của chúng tôi, điều này tiếp tục chứng minh tính tổng quát của template XLT của chúng tôi. Tuy nhiên, mức tăng của LLaMA-2-Chat không tốt bằng các mô hình dựa trên GPT. Phân tích của chúng tôi tiết lộ khoảng cách này chủ yếu có thể được quy cho khả năng tuân theo hướng dẫn đa bước kém hơn của LLaMA 2.

Dân chủ hóa ngôn ngữ. Hơn nữa, chúng tôi cố gắng đánh giá mức độ dân chủ hóa của các nhiệm vụ giữa các ngôn ngữ bằng cách định nghĩa "điểm dân chủ hóa", tính toán tỷ lệ phần trăm trung bình hiệu suất đạt được bởi các ngôn ngữ khác nhau so với hiệu suất tốt nhất trong tất cả các ngôn ngữ. Cho điểm đánh giá s₁, s₂, ..., sₗ tương ứng với l ngôn ngữ trên một nhiệm vụ, điểm dân chủ hóa được công thức hóa như sau:
∑ᵢ₌₁ˡ sᵢ / l / max{sᵢ}ᵢ₌₁ˡ. (1)

Bảng 2 trình bày mức độ dân chủ hóa cho các nhiệm vụ trên các ngôn ngữ trong cả học zero-shot và few-shot, và chúng tôi tiếp tục tóm tắt nó trong Hình 1(b) bằng cách tính trung bình tất cả điểm số mỗi nhiệm vụ bất kể sự khác biệt về cài đặt và mô hình. Chúng ta có thể quan sát thấy XLT dẫn đến điểm dân chủ hóa cao hơn nói chung, đặc biệt là cho XCOPA và MKQA. Đối với MGSM, XNLI, và PAWS-X, XLT của chúng tôi có thể cải thiện hiệu suất trong nhiều ngôn ngữ, nơi hiệu suất tổng thể của baseline luôn thấp hơn nhưng khoảng cách giữa các ngôn ngữ nhỏ hơn như được thể hiện trong Bảng 7, 9, và 10. Kết luận, phương pháp của chúng tôi có thể giảm khoảng cách hiệu suất giữa các ngôn ngữ và cải thiện sự dân chủ hóa ngôn ngữ của LLM.

3.3 Phân tích sâu hơn
Trong phần này, chúng tôi tiếp tục điều tra các yếu tố ảnh hưởng đến hiệu suất của XLT và cách chúng ảnh hưởng đến các benchmark đa ngôn ngữ khác nhau.

3.3.1 Ablation của XLT
Đối với các biến thể XLT, chúng tôi chủ yếu tiến hành thí nghiệm để so sánh các chiến lược sau:
• Loại bỏ hướng dẫn. Vì XLT của chúng tôi bao gồm sáu hướng dẫn logic, chúng tôi vô hiệu hóa hướng dẫn Gán vai trò, Suy nghĩ Cross-lingual, và Giải quyết nhiệm vụ CoT riêng biệt để phân tích đóng góp mỗi hướng dẫn.
• Sắp xếp lại hướng dẫn. Xét đến tính logic của hướng dẫn của chúng tôi, chúng tôi tiếp tục thay đổi thứ tự của hướng dẫn trong XLT để khám phá liệu LLM sẽ xử lý nhiệm vụ khác nhau và dẫn đến kết quả khác nhau.
• Thay đổi từ nội dung. Vì prompt thường nhạy cảm với việc lựa chọn từ, chúng tôi xác minh tính mạnh mẽ của XLT khi thay thế từ khóa diễn đạt lại với "retell", "repeat", và "translate" trong hướng dẫn suy nghĩ cross-lingual.

Kết quả được trình bày trong Bảng 3, cho thấy XLT vượt trội hơn hầu hết các biến thể, do đó xác nhận hiệu quả và tính hợp lý của phương pháp XLT đề xuất của chúng tôi.

Hiệu quả của mỗi hướng dẫn. Kết quả từ hàng "Instruction Ablation" cho thấy: (1) Cross-lingual Thinking mang lại mức tăng đáng kể hơn so với các hướng dẫn khác. Điều này cho thấy khả năng cross-lingual thinking của LLM được kích hoạt, cho phép nó sử dụng kiến thức tiếng Anh để giải quyết nhiệm vụ hiệu quả; (2) Loại bỏ Role Assigning khỏi XLT cản trở sự hiểu biết của mô hình về mục tiêu cuối cùng cho các nhiệm vụ đa ngôn ngữ đa dạng, làm nổi bật khả năng chuyển giao nhiệm vụ của XLT; và (3) hiệu suất tốt hơn của XLT cũng có thể được quy cho CoT Task Solving, yêu cầu mô hình phản hồi các hướng dẫn phức tạp một cách từng bước.

Thứ tự của hướng dẫn logic. Sự sụt giảm hiệu suất là rõ ràng khi thứ tự của hướng dẫn logic được thiết kế của chúng tôi bị đổi chỗ. Khi thiết kế XLT, chúng tôi đã tính đến quy trình mà con người giải quyết các vấn đề đa ngôn ngữ, và thí nghiệm này tiếp tục xác nhận thứ tự tối ưu của template XLT của chúng tôi. Đặt hướng dẫn Role Assigning muộn hơn có thể gây nhầm lẫn cho mô hình ban đầu. Ngoài ra, việc tiến hành Cross-lingual Thinking trước Task Analyzing là rất quan trọng vì chúng tôi dựa vào khả năng giải quyết nhiệm vụ tiếng Anh của LLM để xử lý các nhiệm vụ đa ngôn ngữ.

Tính mạnh mẽ của việc lựa chọn từ cho từ khóa diễn đạt lại. Chúng ta có thể thấy rằng các từ khác nhau thực sự ảnh hưởng đến hiệu suất của XLT, nhưng nó ít nhạy cảm hơn với các biến thể khác. Thông qua thí nghiệm, chúng tôi đã xác định rằng "repeat" mang lại kết quả tốt hơn cho tóm tắt văn bản và dịch máy, trong khi "retell" phù hợp hơn cho năm nhiệm vụ còn lại. Mục tiêu của chúng tôi là cung cấp XLT với một template thống nhất hơn, trong khi vẫn cho phép người dùng tinh chỉnh các từ khóa cụ thể để có hiệu suất tối ưu trong các nhiệm vụ của họ.

3.3.2 Hiệu quả của XLT Few-shot Learning
Như đã đề cập trong Phần 2.2, việc xây dựng minh chứng cho XLT few-shot learning khác với phương pháp trước đây. Chúng tôi đã so sánh XLT và prompt cơ bản. Ở đây, chúng tôi tập trung vào việc xây dựng các cặp đầu vào-đầu ra minh chứng và so sánh các minh chứng khác nhau có thể được sử dụng để thực hiện XLT few-shot learning. Các minh họa có thể được tìm thấy trong Hình 5.

• Đầu vào prompt cơ bản + Đầu ra prompt cơ bản: Đây là định dạng minh chứng bình thường được sử dụng trong hầu hết công trình trước đây.
• Đầu vào prompt cơ bản + Đầu ra XLT: Ablation này nhằm tách biệt tác động của định dạng đầu vào và đầu ra trong minh chứng.
• Đầu vào XLT + Đầu ra XLT: Đây là phương pháp chúng tôi đã sử dụng trong công trình này.

Quan sát kết quả thí nghiệm được trình bày trong Bảng 4, chúng ta có thể kết luận rằng: (1) XLT few-shot learning của chúng tôi vượt trội hơn tất cả các biến thể khác, do đó xác nhận hiệu quả của nó. (2) Việc sử dụng minh chứng bình thường cho XLT few-shot learning dẫn đến giảm hiệu suất. (3) Việc chỉ kết hợp XLT làm đầu vào minh chứng mà không có đầu ra của nó không dẫn đến bất kỳ cải thiện nào. (4) Tính nhất quán trong minh chứng cho học few-shot là quan trọng, ngụ ý rằng định dạng đầu vào-đầu ra minh chứng nên được căn chỉnh tốt hơn với định dạng đầu vào-đầu ra học zero-shot của nó.

4 Công trình liên quan

4.1 Hiểu biết về khả năng LLM
Mặc dù có khả năng ấn tượng của LLM, việc xác định tác động của chúng đối với các nhiệm vụ xử lý ngôn ngữ tự nhiên là rất quan trọng. Liang et al. (2022) tiến hành đánh giá toàn diện về LLM từ nhiều khía cạnh khác nhau, chẳng hạn như độ chính xác, hiệu chuẩn, mạnh mẽ, công bằng, thiên vị, độc tính và hiệu quả. Bang et al. (2023) đánh giá rộng rãi mô hình ChatGPT trên nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên và thấy rằng mô hình hoạt động tốt trong các ngôn ngữ có tài nguyên cao nhưng thể hiện một số hạn chế trong các ngôn ngữ có tài nguyên thấp và chữ viết không Latin. Ngoài ra, các nghiên cứu của Jiao et al. (2023) và Hendy et al. (2023) so sánh các mô hình GPT khác nhau với các mô hình có giám sát cho nhiệm vụ dịch máy và thấy rằng các mô hình GPT có khả năng dịch thuật cạnh tranh trong các ngôn ngữ có tài nguyên cao nhưng hoạt động kém hiệu quả hơn trong các ngôn ngữ có tài nguyên thấp. Đáng chú ý là việc đạt được khả năng AI sinh tạo đa ngôn ngữ đòi hỏi kiến thức cross-lingual để tiếp tục cải thiện hiệu suất của mô hình. Trong bối cảnh này, Ahuja et al. (2023) đánh giá khả năng hiểu nhiệm vụ đa ngôn ngữ của các mô hình GPT và cố gắng nâng cao khả năng xử lý nhiệm vụ của chúng trong các ngôn ngữ khác bằng cách sử dụng kiến thức tiếng Anh. Công trình của chúng tôi cũng tập trung vào việc đánh giá khả năng đa ngôn ngữ của LLM, bao gồm khả năng suy luận, hiểu biết và sinh tạo. Đánh giá của chúng tôi cho thấy LLM thể hiện sự khác biệt trong khả năng tài nguyên cao và thấp, điều này đòi hỏi những nỗ lực bổ sung để nâng cao khả năng đa ngôn ngữ của chúng.

4.2 Xử lý nhiệm vụ đa ngôn ngữ
Kiến thức đa ngôn ngữ đã được chứng minh là có thể khai thác và chuyển giao giữa các ngôn ngữ để cải thiện hiệu suất mô hình (Devlin et al., 2019; Conneau et al., 2020; Raffel et al., 2020; Ouyang et al., 2021; Chi et al., 2021). Mặc dù nhiều nghiên cứu đã được dành cho các nhiệm vụ hiểu đa ngôn ngữ, các nhiệm vụ sinh tạo đa ngôn ngữ thách thức hơn, đặc biệt khi ngôn ngữ mục tiêu là tài nguyên thấp hoặc không phải tiếng Anh (Ma et al., 2021; Liu et al., 2020). Hai phương pháp có thể cho phép mô hình hỗ trợ xử lý nhiệm vụ đa ngôn ngữ: một là huấn luyện mô hình có giám sát bao gồm nhiều ngôn ngữ cho xử lý đa ngôn ngữ (Costa-jussà et al., 2022), và hai là huấn luyện mô hình pre-trained và sử dụng fine-tuning để chuyển giao kiến thức giữa các ngôn ngữ để đạt được khả năng đa ngôn ngữ (Chen et al., 2021, 2022). Tuy nhiên, sự xuất hiện của LLM đã làm cho việc xử lý trực tiếp các nhiệm vụ đa ngôn ngữ qua học trong ngữ cảnh trở nên khả thi (Brown et al., 2020; Ahuja et al., 2023). Những LLM này, với hàng trăm tỷ hoặc thậm chí hàng nghìn tỷ tham số, đòi hỏi một lượng đáng kể tài nguyên tính toán để huấn luyện, làm cho các phương pháp fine-tuning truyền thống kém khả thi hơn. Để cải thiện khả năng sinh tạo của LLM, các nhà nghiên cứu khám phá các phương pháp học trong ngữ cảnh không yêu cầu cập nhật tham số mô hình, chẳng hạn như few-shot prompting (Vilar et al., 2022), học prompt tự động (Shin et al., 2020), task-instruction prompting (Ye et al., 2023), chain-of-thought prompting (Wei et al., 2022c), v.v. Công trình của chúng tôi xây dựng dựa trên các phương pháp này và đề xuất một prompt tối ưu, tổng quát và độc lập với ngôn ngữ để nâng cao khả năng đa ngôn ngữ của LLM.

5 Kết luận
Công trình này điều tra khả năng xử lý ngôn ngữ của các mô hình ngôn ngữ lớn trong cài đặt đa ngôn ngữ và mong muốn phát triển một khung phổ quát để xử lý các nhiệm vụ đa ngôn ngữ đa dạng. Để đạt được mục tiêu này, chúng tôi đề xuất một prompt tổng quát, được gọi là XLT, để nâng cao khả năng đa ngôn ngữ và giảm khoảng cách hiệu suất giữa các ngôn ngữ trong các nhiệm vụ liên quan đến hiểu ngôn ngữ, suy luận và sinh tạo trong các ngôn ngữ không phải tiếng Anh và có tài nguyên thấp. Mặc dù phương pháp của chúng tôi có thể áp dụng chung cho các nhiệm vụ và ngôn ngữ, chúng tôi phát hiện ra rằng các yếu tố thiết kế prompting như logic hướng dẫn và lựa chọn từ có tác động rõ ràng đến hiệu quả của nó. Suy nghĩ cross-language trong XLT đặc biệt hiệu quả. Cuối cùng, chúng tôi hy vọng công trình này có thể truyền cảm hứng cho nghiên cứu tiếp theo để ưu tiên phát triển prompting tổng quát. Bằng cách đó, các mô hình ngôn ngữ lớn có thể bao gồm một phạm vi rộng hơn các phương thức và ngôn ngữ.

Lời cảm ơn
Tianyi Tang và Xin Zhao được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc theo Nghị định thư số 62222215, Quỹ Khoa học Tự nhiên Bắc Kinh theo Nghị định thư số 4222027 và L233008.

--- TRANG 10 ---
Hạn chế
Do những hạn chế do các benchmark đánh giá và chi phí API OpenAI áp đặt, chúng tôi đã tiến hành thử nghiệm trên 27 ngôn ngữ, chỉ chạm vào bề mặt của mảng ngôn ngữ rộng lớn trên thế giới. Bên cạnh đó, template XLT của chúng tôi dựa trên tiếng Anh. Việc khám phá liệu template được viết bằng ngôn ngữ nhiệm vụ có thể dẫn đến hiệu suất tốt hơn và cách xây dựng hướng dẫn tốt hơn trong mỗi ngôn ngữ là xứng đáng. Hơn nữa, chúng tôi chỉ xác minh hiệu quả của phương pháp chúng tôi trên hai mô hình dựa trên GPT (tức là text-davinci-003 và gpt-3.5-turbo) và LLaMA-2-Chat. Việc điều tra tính tổng quát của template chúng tôi trên nhiều mô hình hơn, chẳng hạn như BLOOM và PaLM, là đáng giá.

Tài liệu tham khảo
Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, et al. 2023. Mega: Multilingual evaluation of generative ai. arXiv preprint arXiv:2303.12528.

Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc.

Guanhua Chen, Shuming Ma, Yun Chen, Li Dong, Dongdong Zhang, Jia Pan, Wenping Wang, and Furu Wei. 2021. Zero-shot cross-lingual transfer of neural machine translation with multilingual pretrained encoders. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 15–26, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Guanhua Chen, Shuming Ma, Yun Chen, Dongdong Zhang, Jia Pan, Wenping Wang, and Furu Wei. 2022. Towards making the most of cross-lingual transfer for zero-shot neural machine translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 142–157, Dublin, Ireland. Association for Computational Linguistics.

Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. 2021. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3576–3588, Online. Association for Computational Linguistics.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440–8451, Online. Association for Computational Linguistics.

Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475–2485, Brussels, Belgium. Association for Computational Linguistics.

Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Robert Forkel et al. 2022. Glottocodes: Identifiers linking families, languages and dialects to comprehensive reference information. Semantic Web, 13(6):917–924.

Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021. XL-sum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693–4703, Online. Association for Computational Linguistics.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations.

Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210.

Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? yes with gpt-4 as the engine. arXiv preprint arXiv:2301.08745.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.

Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning. arXiv preprint arXiv:2304.05613.

Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.

Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.

Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019–9052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726–742.

Shayne Longpre, Yi Lu, and Joachim Daiber. 2021. MKQA: A linguistically diverse benchmark for multilingual open domain question answering. Transactions of the Association for Computational Linguistics, 9:1389–1406.

Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Alexandre Muzio, Saksham Singhal, Hany Hassan Awadalla, Xia Song, and Furu Wei. 2021. Deltalm: Encoder-decoder pre-training for language generation and translation by augmenting pretrained multilingual encoders. arXiv preprint arXiv:2106.13736.

OpenAI. 2023. Gpt-4 technical report. arXiv.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730–27744. Curran Associates, Inc.

Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2021. ERNIE-M: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 27–38, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulić, and Anna Korhonen. 2020. XCOPA: A multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2362–2376, Online. Association for Computational Linguistics.

Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186–191, Belgium, Brussels. Association for Computational Linguistics.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67.

Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.

Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations.

Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222–4235, Online. Association for Computational Linguistics.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2022. Prompting palm for translation: Assessing strategies and performance. arXiv preprint arXiv:2211.09102.

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022a. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022b. Emergent abilities of large language models. Transactions on Machine Learning Research. Survey Certification.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022c. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 24824–24837. Curran Associates, Inc.

Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3687–3692, Hong Kong, China. Association for Computational Linguistics.

Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, and Minjoon Seo. 2023. In-context instruction learning. arXiv preprint arXiv:2302.14691.

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.

Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12697–12706. PMLR.

Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023. Multilingual machine translation with large language models: Empirical results and analysis. arXiv preprint arXiv:2304.04675.

[Bảng và hình ảnh tiếp theo sẽ được dịch theo cùng định dạng...]
