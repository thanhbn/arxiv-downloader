# 2308.01825.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2308.01825.pdf
# Kích thước file: 3155302 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Preprint
MỐI QUAN HỆ TỶ LỆ TRONG VIỆC HỌC LẬP LUẬN TOÁN HỌC
VỚI CÁC MÔ HÌNH NGÔN NGỮ LỚN
Zheng Yuan∗, Hongyi Yuan∗†, Chengpeng Li†, Guanting Dong†, Keming Lu
Chuanqi Tan, Chang Zhou, Jingren Zhou
Alibaba DAMO Academy
{yuanzheng.yuanzhen,yuanhongyi.yhy }@alibaba-inc.com
{lichengpeng.lcp,dongguanting.dgt,lukeming.lkm }@alibaba-inc.com
{chuanqi.tcq,ericzhou.zc,jingren.zhou }@alibaba-inc.com
TÓM TẮT
Lập luận toán học là một nhiệm vụ thách thức đối với các mô hình ngôn ngữ lớn (LLM),
trong khi mối quan hệ tỷ lệ của nó so với khả năng của LLM vẫn chưa được khám phá
đầy đủ. Trong bài báo này, chúng tôi điều tra cách mà tổn thất tiền huấn luyện, lượng
dữ liệu có giám sát và lượng dữ liệu tăng cường ảnh hưởng đến hiệu suất lập luận của
một LLM có giám sát. Chúng tôi thấy rằng tổn thất tiền huấn luyện là một chỉ báo tốt
hơn về hiệu suất của mô hình so với số lượng tham số của mô hình. Chúng tôi áp dụng
tinh chỉnh có giám sát (SFT) với các lượng dữ liệu có giám sát khác nhau và tìm ra
một mối quan hệ log-tuyến tính giữa lượng dữ liệu và hiệu suất mô hình một cách thực
nghiệm, và chúng tôi thấy rằng các mô hình tốt hơn cải thiện ít hơn với các tập dữ liệu
có giám sát được mở rộng. Để tăng cường thêm các mẫu dữ liệu nhằm cải thiện hiệu
suất mô hình mà không cần bất kỳ nỗ lực nào của con người, chúng tôi đề xuất áp dụng
Tinh chỉnh Lấy mẫu Từ chối (RFT). RFT sử dụng các mô hình có giám sát để tạo ra
và thu thập các đường lối lập luận đúng làm tập dữ liệu tinh chỉnh tăng cường. Chúng
tôi thấy rằng với các mẫu tăng cường chứa nhiều đường lối lập luận riêng biệt hơn, RFT
cải thiện hiệu suất lập luận toán học nhiều hơn cho các LLM. Chúng tôi cũng thấy rằng
RFT mang lại nhiều cải thiện hơn cho các LLM kém hiệu suất hơn. Hơn nữa, chúng tôi
kết hợp các mẫu từ chối từ nhiều mô hình khác nhau, điều này đẩy LLaMA-7B đến độ
chính xác 49.3% trên GSM8K, vượt trội hơn đáng kể so với độ chính xác tinh chỉnh có
giám sát (SFT) là 35.9%. Chúng tôi công bố mã nguồn và dữ liệu tăng cường lấy mẫu
từ chối của chúng tôi tại https://github.com/OFA-Sys/gsm8k-ScRel .

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) (Anil et al., 2023; Touvron et al., 2023b; OpenAI, 2023) đã thể hiện
khả năng đáng kể trong các nhiệm vụ lập luận toán học khác nhau (Saxton et al., 2019; Cobbe et al., 2021; Light-
man et al., 2023). Việc hiểu, dự đoán và cải thiện khả năng lập luận toán học của LLM dựa trên các LLM
được tiền huấn luyện khác nhau và các tập dữ liệu có giám sát khác nhau là điều đáng quan tâm. Với kiến thức
này, chúng ta có thể quyết định tốt hơn về nỗ lực chúng ta đầu tư vào việc cải thiện LLM hoặc tăng cường
tập dữ liệu. Nhiều công trình gần đây đang tập trung vào việc sử dụng các lời nhắc khác nhau (Wei et al., 2022b;
Yao et al., 2023) hoặc tổng hợp / xếp hạng lại nhiều lần suy luận (Cobbe et al., 2021; Uesato et al., 2022;
Wang et al., 2023; Lightman et al., 2023) để cải thiện hiệu suất lập luận của mô hình. Trong khi học trong
ngữ cảnh (ICL) và thực hiện nhiều lần suy luận có thể cải thiện hiệu suất, nó tốn kém về mặt tính toán và
không phù hợp cho các tình huống triển khai trực tuyến. Do đó, chúng tôi tập trung vào hiệu suất của các LLM
có giám sát với chỉ một lần suy luận, đây là thiết lập gần hơn với việc triển khai trực tuyến.

Để đạt được mục tiêu này, chúng tôi điều tra thực nghiệm mối quan hệ tỷ lệ của các yếu tố ảnh hưởng đến
khả năng lập luận toán học của một LLM có giám sát, bao gồm tổn thất tiền huấn luyện, lượng dữ liệu có
giám sát và lượng dữ liệu tăng cường. Đầu tiên, chúng tôi phân tích hiệu suất tinh chỉnh có giám sát (SFT)
và ICL của các LLM. Chúng tôi quan sát thấy rằng tổn thất tiền huấn luyện có tương quan tuyến tính âm
gần đúng với độ chính xác SFT và ICL trong một khoảng nhất định, đây là chỉ báo hiệu suất tốt hơn so với
kích thước mô hình được tiền huấn luyện hoặc số lượng token được tiền huấn luyện. Thứ hai, chúng tôi phân
tích mối quan hệ
∗Đóng góp ngang nhau.
†Công việc được thực hiện trong thời gian thực tập tại Alibaba DAMO Academy.
1arXiv:2308.01825v2  [cs.CL]  13 Sep 2023

--- TRANG 2 ---
Preprint
Hình 1: Những phát hiện chính về mối quan hệ tỷ lệ trong việc học khả năng lập luận toán học với các LLM.
giữa SFT và các lượng dữ liệu có giám sát khác nhau. Chúng tôi quan sát thấy rằng hiệu suất mô hình có mối
quan hệ log-tuyến tính với lượng dữ liệu có giám sát trong khi mức tăng giảm dần với mô hình được tiền huấn
luyện tốt hơn. Thứ ba, chúng tôi muốn tận dụng chính mô hình để tạo ra nhiều dữ liệu có giám sát hơn nhằm
củng cố khả năng lập luận của nó và phân tích mối quan hệ tỷ lệ của lượng dữ liệu tăng cường. Chúng tôi áp
dụng lấy mẫu từ chối trên các mô hình SFT để lấy mẫu và chọn các đường lối lập luận đúng làm tập dữ liệu
tăng cường (Uesato et al., 2022; Zhu et al., 2023). Chúng tôi sử dụng các tập dữ liệu tăng cường này để tinh
chỉnh các LLM cơ sở, điều này sẽ đạt được hiệu suất tốt hơn so với SFT và chúng tôi ký hiệu nó là tinh chỉnh
lấy mẫu từ chối (RFT). Chúng tôi thấy rằng yếu tố chính ảnh hưởng đến hiệu suất RFT là lượng đường lối
lập luận riêng biệt có thể được tăng lên bằng cách lấy mẫu nhiều lần hơn hoặc kết hợp các mẫu từ nhiều mô
hình. Chúng tôi áp dụng RFT trên một số LLM được tiền huấn luyện và cho thấy cải thiện lớn hơn trên các
mô hình kém hiệu suất hơn. Chúng tôi thảo luận lý do RFT hoạt động là nó cung cấp nhiều đường lối lập luận
khiến các LLM có khả năng tổng quát hóa lập luận tốt hơn. Chúng tôi cũng thảo luận rằng RFT rẻ hơn nhiều
so với tiền huấn luyện về tài nguyên tính toán trong khi huấn luyện một LLM với tổn thất tiền huấn luyện
thấp hơn là giải pháp cơ bản.

Những phát hiện chính của bài báo này được trình bày trong Hình 1 và được tóm tắt ở đây:
• Khi tổn thất tiền huấn luyện nhỏ hơn (tức là mô hình được tiền huấn luyện tốt hơn), hiệu suất lập
luận mô hình của SFT và ICL tăng tuyến tính trong một phạm vi. Hiệu suất SFT cải thiện chậm hơn
so với ICL.
• SFT cải thiện theo cách log-tuyến tính với sự gia tăng lượng dữ liệu có giám sát. Lợi ích của việc
tăng lượng dữ liệu giảm dần khi mô hình được tiền huấn luyện tốt hơn.
• Hiệu suất mô hình cho RFT cải thiện khi lượng đường lối lập luận riêng biệt tăng lên. Hiệu suất
RFT cải thiện chậm hơn so với SFT.
• Sự kết hợp các mẫu lấy mẫu từ chối từ nhiều mô hình làm tăng thêm hiệu suất RFT, dẫn đến độ
chính xác 49.3 cho LLaMA-7B (+13.4 so với SFT), 50.3 cho LLaMA2-7B (+8.7 so với SFT),
52.1 cho LLaMA-13B (+9.1 so với SFT) và 55.4 cho LLaMA2-13B (+5.4 so với SFT).

2 CÔNG TRÌNH LIÊN QUAN
Học Lập luận Toán học với LLM Nghiên cứu gần đây về LLM đã phát hiện ra khả năng nổi bật trong
việc giải quyết các nhiệm vụ lập luận vượt qua một quy mô mô hình nhất định (Wei et al., 2022a). Những
khả năng lập luận như vậy trong LLM có thể được kích hoạt bằng tinh chỉnh, lời nhắc few-shot hoặc lời
nhắc zero-shot (Cobbe et al., 2021; Wei et al., 2021; Nye et al., 2021; Wei et al., 2022b; Kojima et al.,
2022). Một lượng lớn nghiên cứu tập trung vào các nhiệm vụ lập luận của các bài toán từ ngữ toán học
(MWP), và các phương pháp được đánh giá trên các benchmark bao gồm các cấp độ khác nhau của MWP
(Koncel-Kedziorski et al. (2016); Patel et al. (2021); Lan et al. (2021); Cobbe et al. (2021); Jie et al.
(2022); Yuan et al. (2023a); Fu et al. (2023a), và các nghiên cứu khác). Ý tưởng cốt lõi của việc cải thiện
khả năng lập luận toán học của LLM là tổng hợp các đường lối lập luận được lấy mẫu khác nhau trong quá
trình tinh chỉnh hoặc suy luận. Cobbe et al. (2021) đã huấn luyện và thiết kế một công cụ xác minh đường
lối lập luận để chọn kết quả đúng trong quá trình suy luận. Wang et al. (2023) đề xuất lấy mẫu các đường
lối lập luận khác nhau trong quá trình suy luận và sau đó rút ra kết quả cuối cùng bằng cách bỏ phiếu đa
số trên các câu trả lời hoặc thông qua các công cụ xác minh (Li et al., 2023). Một số công trình áp dụng
ý tưởng lấy mẫu từ chối cùng với các kỹ thuật khác để lọc các đường lối lập luận đa dạng được lấy mẫu
cho việc tăng cường dữ liệu tinh chỉnh (Huang et al., 2022; Zelikman et al., 2022; Ni et al., 2023; Zhu
et al., 2023). Lấy mẫu từ chối là một kỹ thuật tăng cường tinh chỉnh đơn giản nhưng hiệu quả và cũng được
sử dụng để điều chỉnh LLM theo sở thích của con người (Bai et al., 2022; Yuan et al., 2023b; Dong et al.,
2023; Touvron et al., 2023b; Song et al., 2023). Uesato et al. (2022) khám phá việc sử dụng các phương
pháp học tăng cường để cải thiện khả năng lập luận toán học của LLM và họ thảo luận thêm về sự khác biệt
giữa mô hình hóa phần thưởng dựa trên kết quả và dựa trên quá trình. Tiếp theo bởi Lightman et al. (2023),
họ thu thập các tín hiệu giám sát dựa trên quá trình quy mô lớn thông qua chú thích của con người và xác
minh rằng LLM có thể hưởng lợi nhiều hơn từ mô hình hóa phần thưởng dựa trên quá trình với giám sát
được chú thích bởi con người so với mô hình hóa phần thưởng dựa trên kết quả. Cũng có nghiên cứu trước
đây chưng cất khả năng lập luận nổi bật của LLM thành các mô hình ngôn ngữ nhỏ (Fu et al., 2023b; Shridhar
et al., 2023). So với các công trình trước đây (Zelikman et al., 2022; Uesato et al., 2022; Zhu et al., 2023;
Ni et al., 2023), chúng tôi sử dụng một cách đơn giản hơn để tạo ra các mẫu tăng cường mà không cần bất
kỳ mô hình phần thưởng cấp quá trình được huấn luyện nào và chúng tôi tập trung vào việc nghiên cứu mối
quan hệ tỷ lệ giữa LLM và khả năng lập luận toán học.

Luật Tỷ lệ của Các Mô hình Ngôn ngữ Lớn Việc hiểu và dự đoán mức tăng hiệu suất khi mô hình ngôn
ngữ mở rộng quy mô là điều quan trọng. Kaplan et al. (2020) đầu tiên điều tra và rút ra một mối quan hệ có
thể dự đoán được về cách số lượng tham số mô hình và kích thước dữ liệu đóng góp vào tổn thất qua nhiều
bậc độ lớn. Hoffmann et al. (2022) tinh chỉnh luật tỷ lệ trong (Kaplan et al., 2020) và tìm ra luật tỷ lệ cho
huấn luyện tối ưu tính toán. Muennighoff et al. (2023) khám phá và mở rộng luật tỷ lệ trong tình huống hạn
chế dữ liệu. Ngoài việc điều tra hiệu suất tỷ lệ cho tiền huấn luyện, Gao et al. (2022) thảo luận về luật tỷ lệ
cho các mô hình phần thưởng quá tham số hóa để điều chỉnh theo sở thích của con người, và Hernandez et al.
(2021) phát triển luật tỷ lệ để chuyển hiệu suất từ các mô hình được tiền huấn luyện sang các nhiệm vụ hạ
lưu. Henighan et al. (2020); Caballero et al. (2022) điều tra luật tỷ lệ của các bài toán toán học. Trong bài
báo này, chúng tôi đang điều tra mối quan hệ tỷ lệ của các mô hình ngôn ngữ lớn trong việc học các bài toán
từ ngữ toán học với tổn thất tiền huấn luyện, lượng dữ liệu có giám sát và lượng dữ liệu tăng cường.

3 CÁC YẾU TỐ CỦA KHẢ NĂNG LẬP LUẬN TOÁN HỌC TRONG LLM CÓ GIÁM SÁT
Mục tiêu của bài báo này là cố gắng hiểu hiệu suất của các LLM có giám sát trong lập luận toán học. Chúng
tôi kỳ vọng một LLM được tiền huấn luyện ρ học khả năng lập luận từ một tập dữ liệu lập luận có giám sát
D. Tập dữ liệu được định nghĩa bởi D={qi, ri, ai}i, trong đó q là một câu hỏi, r là một đường lối lập luận
chuỗi tư duy và a là một câu trả lời số. Chúng tôi thực hiện tinh chỉnh có giám sát trên tập dữ liệu D để có
được một mô hình SFT π. Chúng tôi sử dụng π để tạo ra các đường lối lập luận và câu trả lời trong tập kiểm
tra bằng cách giải mã tham lam và báo cáo độ chính xác (tức là acc hoặc maj1@1) làm chỉ số của chúng tôi
ở đây.

3.1 ĐỘ CHÍNH XÁC MÔ HÌNH SO VỚI TỔN THẤT TIỀN HUẤN LUYỆN
Các công trình trước đây nêu rằng LLM lớn hơn thể hiện khả năng lập luận tốt hơn trong cùng một loạt mô
hình (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a;b), và chúng tôi thấy LLaMA vượt
trội hơn GPT-3, điều này cho thấy số lượng tham số mô hình không nên là chỉ báo duy nhất của khả năng lập
luận. Trong khi các LLM có kiến trúc, tham số mô hình và số lượng token tiền huấn luyện khác nhau, chúng
tôi thấy rằng tổn thất tiền huấn luyện là một chỉ báo hiệu suất ổn định của khả năng lập luận toán học và
chúng tôi sử dụng nó để đại diện cho mô hình thay vì sử dụng tham số mô hình và số lượng token tiền huấn
luyện của chúng.

Chúng tôi phân tích hiệu suất SFT và ICL (8-shot) của GPT-3 (Brown et al., 2020), LLaMA (Touvron et al.,
2023a), LLaMA2 (Touvron et al., 2023b) và GPT-4 (OpenAI, 2023). Tổn thất tiền huấn luyện

--- TRANG 3 ---
Preprint
Hình 2: Hiệu suất của các cài đặt SFT (đường màu xanh) và ICL (đường màu đỏ) trên GSM8K. GPT-4 nêu
rằng họ sử dụng một phần dữ liệu GSM8K trong tiền huấn luyện, và đề xuất người khác coi hiệu suất của nó
nằm giữa SFT và ICL.

của các mô hình này được quan sát trong bài báo của họ, chúng ta nên lưu ý rằng tổn thất tiền huấn luyện
tương ứng với các tập dữ liệu tiền huấn luyện khác nhau và các tokenizer khác nhau, có nghĩa là chúng không
thể được so sánh một cách chặt chẽ (và chúng ta không thể sử dụng nó để thực hiện bất kỳ loại hồi quy nào
một cách trực tiếp) trong khi xu hướng giữa các tổn thất này vẫn mang tính giác ngộ. Chúng tôi sử dụng kết
quả của việc tinh chỉnh GPT-3 từ (Cobbe et al., 2021) và chúng tôi tinh chỉnh LLaMA và LLaMA2 trên tập
huấn luyện GSM8K (chi tiết trong Phụ lục A.1). Đối với học trong ngữ cảnh, chúng tôi sử dụng kết quả từ
bài báo LLaMA (Touvron et al., 2023a) và LLaMA2 (Touvron et al., 2023b).

Trong Hình 2, chúng ta có thể thấy rằng:
• Tổn thất tiền huấn luyện có tương quan tuyến tính âm gần đúng với độ chính xác SFT và ICL trong
khoảng tổn thất tiền huấn luyện đã cho.
• SFT vượt trội hơn ICL một cách nhất quán, trong khi những cải thiện giảm dần khi tổn thất tiền huấn
luyện thấp hơn.

Mối quan hệ tuyến tính của độ chính xác SFT và ICL có thể chỉ hoạt động trong khoảng đã cho. Lý do là
(1) độ dốc của ICL dốc hơn SFT, trong khi hiệu suất SFT nên lớn hơn hiệu suất ICL; (2) độ chính xác không
thể lớn hơn 1 hoặc nhỏ hơn 0. Về mặt lý thuyết, nên sử dụng −log(acc) thay vì acc làm biến phụ thuộc
trong khi chúng tôi tìm thấy mối quan hệ tuyến tính rõ ràng giữa tổn thất tiền huấn luyện và acc và sử dụng
acc làm biến phụ thuộc. LLaMA-2 7B(13B) có thể được xem như một phép xấp xỉ của việc tiếp tục huấn
luyện LLaMA 7B(13B). Khi nó huấn luyện lâu hơn, hiệu suất ICL và SFT của nó đều cải thiện mà không
thay đổi số lượng tham số. Từ các quan sát, một cách hiệu quả để cải thiện khả năng lập luận là huấn luyện
một mô hình cơ sở tốt hơn với tổn thất tiền huấn luyện thấp hơn (Tiền huấn luyện là tất cả những gì bạn
cần!). Các mô hình với tổn thất tiền huấn luyện thấp hơn cải thiện ít hơn từ tinh chỉnh có thể do các mô hình
đã thu được nhiều khả năng lập luận hơn trong quá trình tiền huấn luyện và dữ liệu có giám sát có thể cung
cấp ít tín hiệu hơn để giám sát chúng.

--- TRANG 4 ---
Preprint
Hình 3: Hiệu suất của SFT với các lượng dữ liệu có giám sát khác nhau trên GSM8K.

3.2 ĐỘ CHÍNH XÁC MÔ HÌNH SO VỚI SỐ LƯỢNG DỮ LIỆU CÓ GIÁM SÁT
Tinh chỉnh có giám sát thực sự cải thiện khả năng lập luận của LLM, chúng tôi muốn biết lượng dữ liệu có
giám sát ảnh hưởng như thế nào đến sự cải thiện của mô hình. Chúng tôi tinh chỉnh LLaMA và LLaMA2
với {1,1/2,1/4,1/8,1/16,1/32} lượng tập huấn luyện từ GSM8K (chi tiết trong Phụ lục A.2). Chúng tôi
muốn sử dụng thí nghiệm này để ngoại suy hiệu suất mô hình nếu chúng ta có nhiều dữ liệu có giám sát hơn.
Trong Hình 3, chúng tôi vẽ biểu đồ kết quả huấn luyện với các lượng dữ liệu có giám sát khác nhau. Từ hình
này, chúng ta có thể quan sát thấy rằng:
• Hiệu suất mô hình có mối quan hệ log-tuyến tính với lượng dữ liệu. Khi lượng dữ liệu tăng gấp đôi,
hiệu suất tăng lên một đơn vị.
• Mô hình tốt hơn cần nhiều lượng dữ liệu hơn để vượt trội hơn hiệu suất ICL của nó.
• Mô hình tốt hơn hưởng lợi ít hơn khi lượng dữ liệu có giám sát tăng gấp đôi.

Mối quan hệ log-tuyến tính ổn định trong {1,1/2,1/4,1/8} lượng dữ liệu huấn luyện. Từ quan sát, việc mở
rộng tập dữ liệu huấn luyện để cải thiện hiệu suất là điều đơn giản, đặc biệt là đối với các mô hình kém hơn.
Đối với các mô hình tốt hơn, nó hưởng lợi ít hơn, điều này lặp lại rằng các mô hình tốt hơn đã học được
nhiều khả năng lập luận hơn trong quá trình tiền huấn luyện.

3.3 ĐỘ CHÍNH XÁC MÔ HÌNH SO VỚI SỐ LƯỢNG DỮ LIỆU TĂNG CƯỜNG
Việc tăng lượng dữ liệu có nhãn lập luận toán học là khó khăn, đặc biệt là đề xuất một câu hỏi mới. Đối với
một học sinh có trình độ tốt, việc giải quyết hàng trăm bài toán từ ngữ toán học mỗi ngày là dễ dàng, nhưng
việc đưa ra các bài toán toán học đa dạng và mang tính giáo dục là rất khó. Vì vậy, hướng của chúng tôi
chuyển sang tăng cường dữ liệu mới bằng cách sử dụng các tài nguyên hiện có. Chúng tôi đã thử tăng cường
các truy vấn mới (chi tiết trong Phụ lục D.1) và tăng cường các bản sửa đổi (chi tiết trong Phụ lục D.2).
Những cách tiếp cận này không có hoặc có cải thiện tối thiểu so với SFT. Chúng tôi thấy rằng phiên bản
đơn giản hóa của lấy mẫu từ chối (Zhu et al., 2023) là một cách ngây thơ và hiệu quả để tăng cường các
đường lối lập luận mới và có thể cải thiện hiệu suất mô hình. Và chúng tôi thấy rằng yếu tố chính ảnh hưởng
đến tinh chỉnh trên dữ liệu tăng cường lấy mẫu từ chối (RFT) là lượng đường lối lập luận riêng biệt. Kết hợp
các mẫu lấy mẫu từ chối từ nhiều

--- TRANG 5 ---
Preprint
Cài đặt 7B 7B-2 13B 13B-2 33B
Tổn thất tiền huấn luyện 1.8 1.75 1.73 1.68 1.62
ICL 11.0/18.1 14.6/- 17.8/29.3 28.7/- 35.6/53.1
SFT 35.9/48.7 41.6/55.4 43.0/55.2 50.0/61.7 54.6/-
RFTk= 100 41.7/52.7 47.5/58.7 49.1/59.9 54.8/65.4 54.5/-
Đường lối đúng mỗi câu hỏi 53.3 60.8 62.5 71.6 88.7
Đường lối riêng biệt mỗi câu hỏi 5.25 5.19 5.26 5.29 2.78

Bảng 1: Hiệu suất của RFT với k= 100 trên GSM8K so với SFT và ICL. Lượng đường lối riêng biệt có
nghĩa là lượng danh sách phương trình riêng biệt ở đây.

mô hình, chúng ta có thể tinh chỉnh thêm một mô hình LLaMA-7B đến độ chính xác 49.3 (so với SFT 35.9)
và một mô hình LLaMA-13B đến độ chính xác 52.1 (so với SFT 43.0).

Tinh chỉnh Lấy mẫu Từ chối Mô hình SFT π có được khả năng thực hiện lập luận chuỗi tư duy zero-shot,
và chúng tôi sử dụng π để tạo ra nhiều đường lối lập luận đúng rij hơn để cung cấp cho tập dữ liệu huấn
luyện. Đối với mỗi qi, chúng tôi tạo ra k đường lối lập luận và câu trả lời ứng viên r, a với nhiệt độ 0.7
theo (Cobbe et al., 2021). Đầu tiên, chúng tôi lọc ra các đường lối lập luận có câu trả lời sai a̸=ai hoặc
tính toán sai dựa trên đánh giá Python. Mỗi đường lối lập luận chứa một danh sách phương trình ej, và
chúng tôi chọn một đường lối lập luận rij cho mỗi danh sách phương trình riêng biệt làm dữ liệu tăng cường
và loại bỏ các đường lối lập luận khác có cùng danh sách phương trình để loại bỏ các đường lối lập luận
tương tự. Thứ tự khác nhau của các phần tử (ví dụ: 3 + 4 = 7 và 4 + 3 = 7) hoặc thứ tự khác nhau của
các phương trình (ví dụ: 1 + 2 = 3, 3 + 4 = 7 và 1 + 4 = 5, 2 + 5 = 7) được coi là khác nhau. Việc
này hữu ích để các mô hình biết rằng các thứ tự này có thể được trao đổi và khó để các mô hình học điều
này chỉ với một đường lối lập luận mỗi bài toán. Chúng tôi định nghĩa D′π=D ∪ {qi, rij, ai}i,j là tập dữ
liệu tăng cường. Chúng tôi tinh chỉnh D′ trên LLM được tiền huấn luyện ρ thành πRFT như RFT, và chúng
tôi chi tiết cách chúng tôi áp dụng RFT trong Phụ lục A.3. Chúng tôi liệt kê kết quả của RFT với lấy mẫu
k= 100 đường lối lập luận ứng viên trên LLaMA và LLaMA-2 trong Bảng 1. Đối với ICL, SFT và RFT,
chúng tôi liệt kê maj1@1 (độ chính xác) và maj1@100 (lấy mẫu 100 lần và tính độ chính xác dựa trên bỏ
phiếu đa số) làm chỉ số.

Trong trường hợp các mô hình 7B và 13B, RFT mang lại mức tăng khoảng 5 đến 6 điểm trong maj1@1
và khoảng 4 điểm tăng trong maj1@100. Đối với các mô hình 33B, RFT không cải thiện hiệu suất so với
SFT. Lý do chính đến từ các mẫu tăng cường từ lấy mẫu từ chối. Chúng ta có thể thấy rằng các mô hình
tốt hơn tạo ra nhiều đường lối lập luận đúng hơn mỗi câu hỏi. Đối với LLaMA-33B-SFT, nó có thể tạo ra
trung bình 88.7 đường lối đúng mỗi câu hỏi. Tuy nhiên, nó quá khớp với tập huấn luyện và gặp khó khăn
trong việc tạo ra nhiều đường lối đa dạng hơn trên các câu hỏi tập huấn luyện. Lấy mẫu từ chối với 33B
rất tốn thời gian và chúng tôi không thực hiện tìm kiếm lưới nhiệt độ, chúng tôi đã thử sử dụng nhiệt độ
lớn hơn 1.0 để giải mã các mô hình LLaMA-33B-SFT, nó tạo ra 82.4 đường lối đúng và 4.77 đường lối
riêng biệt mỗi câu hỏi, đa dạng hơn so với sử dụng nhiệt độ 0.7 nhưng vẫn ít đa dạng hơn so với các mô
hình 7B và 13B. Chúng tôi thừa nhận rằng nên có một nhiệt độ (hoặc cấu hình tạo) có thể tạo ra nhiều
đường lối riêng biệt hơn và tạo ra kết quả tốt cho RFT trong 33B và thậm chí các mô hình lớn hơn trong
khi nó cần nhiều tài nguyên tính toán hơn cho suy luận so với lấy mẫu sử dụng các mô hình 7B và 13B.
Chúng tôi sẽ chỉ ra rằng chúng ta có thể chỉ sử dụng các mô hình 7B và 13B để lấy mẫu từ chối nhằm
cải thiện mô hình 33B.

Độ Chính xác Mô hình so với Số lượng Dữ liệu Lấy mẫu Từ chối Để hiểu hiệu suất của RFT, chúng
tôi thay đổi k trong 1,3,6,12,25,50,100 và áp dụng RFT. Chúng tôi cũng có một cài đặt khác của k= 100
trong khi không loại bỏ bất kỳ đường lối lập luận nào được ký hiệu là không loại trùng. Chúng tôi liệt kê
kết quả RFT với k khác nhau trong Hình 4. So sánh việc sử dụng RFT với k= 100 và không loại trùng,
hiệu suất tương tự và cho thấy rằng tốt hơn là ước tính hiệu suất RFT dựa trên lượng đường lối lập luận
riêng biệt thay vì số lượng mẫu tăng cường RFT. Hơn nữa, việc sử dụng loại trùng có hiệu suất tốt hơn
cho 3 trong 4 mô hình và cần ít thời gian huấn luyện hơn nhiều.

Khi sử dụng k= 3, RFT vượt trội hơn SFT 2 điểm một cách ổn định. Đối với hầu hết các điểm dữ liệu,
sử dụng k lớn hơn dẫn đến hiệu suất tốt hơn. Tuy nhiên, lợi ích của RFT đang giảm dần khi k tăng gấp
đôi. Chúng tôi tính toán các đường lối khác nhau mỗi câu hỏi cho k khác nhau trong Bảng 2. Chúng ta có
thể thấy rằng lượng đường lối lập luận khác nhau không tăng nhanh cùng với k tăng. Trong Hình 3, chúng
ta biết rằng việc tăng gấp đôi mẫu huấn luyện có thể có cải thiện hiệu suất tuyến tính. Việc tăng gấp đôi
đường lối lập luận nên

--- TRANG 6 ---
Preprint
Hình 4: Hiệu suất của RFT với các lượng số lần lấy mẫu k khác nhau trên GSM8K.

k 7B 7B-2 13B 13B-2 33B
1 1.17 1.19 1.15 1.18 1.06
3 1.44 1.47 1.41 1.45 1.16
6 1.74 1.78 1.69 1.76 1.28
12 2.20 2.23 2.11 2.21 1.46
25 2.93 2.93 2.88 2.94 1.77
50 3.94 3.91 3.90 3.94 2.19
100 5.25 5.19 5.26 5.29 2.78
400 (U13B) 12.84
500 (U33B) 13.65

Bảng 2: Các đường lối lập luận khác nhau mỗi câu hỏi được tạo ra bởi các mô hình SFT khác nhau với k khác nhau.

cải thiện ít hơn so với việc tăng gấp đôi mẫu huấn luyện vì việc có được các đường lối lập luận khác nhau
không thu được bất kỳ câu hỏi mới nào. Do đó, việc tăng gấp đôi k dẫn đến cải thiện hiệu suất giảm dần.

Kết hợp các mẫu lấy mẫu từ chối từ nhiều mô hình Kết quả thí nghiệm trên chứng minh sự tăng cường
hiệu suất trong lập luận toán học, hưởng lợi từ lấy mẫu từ chối. Thông qua các nghiên cứu tình huống
trong 4.1, chúng tôi chỉ ra rằng lấy mẫu từ chối có thể tăng cường dữ liệu huấn luyện với các đường lối
lập luận của các quá trình tính toán đa dạng. Tuy nhiên, các đường lối lập luận được lấy mẫu từ một mô
hình SFT duy nhất có thể không đa dạng về mặt logic. Do đó, chúng tôi kỳ vọng cải thiện thêm hiệu suất
lập luận toán học bằng cách tận dụng các đường lối lập luận được lấy mẫu từ chối tổng hợp từ các mô hình
khác nhau. Chúng tôi ký hiệu hai tập dữ liệu cuối cùng là D′U13B và D′U33B, được tổng hợp từ lấy mẫu
từ chối các mô hình khác nhau D′U13B=D′7B⊕ D′7B2⊕ D′13B⊕ D′13B2 và D′U33B=D′U13B⊕ D′33B,
trong đó U có nghĩa là các mô hình dưới một kích thước nhất định, 7B/13B/33B có nghĩa là LLaMA-
7B/13B/33B và 7B2/13B2 có nghĩa là LLaMA2-7B/13B. ⊕ có nghĩa là một quá trình tổng hợp trong
đó tất cả các đường lối lập luận từ các tập khác nhau được kết hợp trước tiên và sau đó Thuật toán 1 được
áp dụng để loại trùng các đường lối lập luận có cùng quá trình tính toán liên quan đến hình thức và thứ tự
phương trình.

Chúng ta có thể thấy, thông qua kết quả được trực quan hóa trong Hình 5, rằng việc sử dụng tập dữ liệu
tổng hợp D′U13B và D′U33B có thể dẫn đến hiệu suất tốt hơn một cách đồng nhất so với tinh chỉnh với
các tập dữ liệu từ một mô hình duy nhất trên các kích thước mô hình khác nhau. RFT trên hai tập dữ liệu
tăng cường này D′U13B và D′U33B giảm khoảng cách hiệu suất giữa các mô hình cùng kích thước trong
SFT và RFT k= 100, có nghĩa là các tập dữ liệu tăng cường kết hợp cung cấp đủ giám sát lập luận để lấp
đầy khoảng cách tiền huấn luyện. Chúng ta có thể giả định rằng với lượng dữ liệu có giám sát đủ, chỉ báo
hiệu suất nên là kích thước mô hình chứ không phải tổn thất tiền huấn luyện.

--- TRANG 7 ---
Preprint
Hình 5: Hiệu suất của RFT với các mẫu lấy mẫu từ chối từ nhiều mô hình.

Chúng tôi đã nêu rằng việc áp dụng RFT k= 100 trên các mô hình 33B rất tốn kém và cần tìm kiếm lưới
nhiệt độ để đạt được cải thiện so với SFT. Tuy nhiên, tinh chỉnh trên D′U13B có chi phí tính toán lấy mẫu
từ chối tương tự so với lấy mẫu 100 lần trên 33B và đạt được hiệu suất tốt hơn.

Một hiện tượng khác là việc bao gồm D′33B trong tổng hợp hầu như không ảnh hưởng đến hiệu suất. Để
đưa ra phân tích toàn diện hơn về kết quả, chúng tôi tính toán số lượng đường lối lập luận trung bình mỗi
câu hỏi trong Bảng 2 và vẽ biểu đồ Venn để trực quan hóa nguồn gốc của các đường lối lập luận khác nhau
được hiển thị trong Hình 6. Trong Bảng 2, số lượng đường lối lập luận trung bình của D′U13B và D′U33B
vượt qua những số của một mô hình duy nhất với lượng lớn, trong khi D′U33B chỉ có nhiều đường lối lập
luận hơn D′U13B một chút là 0.81. Trong khi đó, như được hiển thị trong Hình 6, các mô hình dưới và
bao gồm kích thước 13B có thể đóng góp các đường lối lập luận độc đáo với tỷ lệ tương tự trong D′U33B
khoảng 15%. Tuy nhiên, chỉ có 6.5% các đường lối lập luận có thể được thu được độc quyền từ mô hình
LLaMA-33B-SFT. Điều này cho thấy rằng mô hình SFT của 33B có thể cung cấp sự đa dạng lập luận hạn
chế khi lấy mẫu các câu hỏi huấn luyện. Phát hiện này phù hợp với kết quả trên trong Bảng 1, chỉ ra rằng
mô hình 33B (và có thể các mô hình 65B và 70B) có thể ghi nhớ tốt các đường lối lập luận được chú thích
bởi con người.

Đối với các mô hình 65B, chúng tôi thấy rằng việc sử dụng D′U13B không cải thiện hiệu suất so với SFT.
Lý do có thể là các mô hình tốt hơn hưởng lợi ít hơn từ lượng mẫu có giám sát trong khi nó đã học được
nhiều khả năng lập luận hơn trong quá trình tiền huấn luyện.

Nhìn chung, chúng ta có thể đi đến kết luận rằng (1) RFT cải thiện hiệu suất lập luận toán học của các
LLM (kém hơn) thông qua các đường lối lập luận đa dạng từ lấy mẫu từ chối của các mô hình SFT, và
việc tổng hợp nhiều đường lối lập luận đa dạng hơn có thể cải thiện hiệu suất thêm. (2) Các mô hình SFT
khác nhau có thể đóng góp các đường lối lập luận với các quá trình tính toán khác nhau từ lấy mẫu từ chối,
dẫn đến dữ liệu huấn luyện đa dạng hơn cho RFT, và các LLM có kích thước tham số lớn hơn có thể suy
giảm trong việc tạo ra các đường lối lập luận đa dạng như là kết quả của việc quá khớp với các câu hỏi huấn
luyện. Có thể có một cấu hình tạo hoặc cấu hình huấn luyện cho các LM đủ lớn để không quá khớp với
tập dữ liệu huấn luyện trong khi việc tìm ra chúng không phải là điều tầm thường.

So sánh với các baseline khác Chúng tôi so sánh kết quả RFT của chúng tôi khi huấn luyện trên D′U13B
với một số baseline và kết quả được chi tiết trong Bảng 3. Mặc dù LLaMA và LLaMA2 là những LLM
mã nguồn mở hàng đầu1, hiệu suất lập luận toán học của chúng vẫn thua kém so với các LLM độc quyền
hiện tại có quy mô tham số lớn hơn, chẳng hạn như GPT-4 và PaLM2. So với kết quả trên
1https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard

--- TRANG 8 ---
Preprint
Hình 6: Biểu đồ Venn về tỷ lệ các đường lối tính toán lập luận mà mỗi mô hình cung cấp cho D′U33B.
Ví dụ, 15.5% (trong phần màu vàng) các đường lối tính toán lập luận trong D′U33B chỉ có thể được tìm
thấy độc quyền trong kết quả lấy mẫu từ chối từ LLaMA2-13B-SFT.

các mô hình mã nguồn mở, kết quả của chúng tôi trên LLaMA thể hiện hiệu suất tốt hơn so với hai phương
pháp tăng cường lập luận hiện đại nhất gần đây. Phương pháp RFT của chúng tôi đơn giản hơn so với CoRE,
vì RFT không yêu cầu huấn luyện các mô hình xác minh và giải mã với Tìm kiếm Cây Monte Carlo (MCTS).
So với các mô hình ngôn ngữ được điều chỉnh mã nguồn mở khác, chúng ta có thể thấy rằng các mô hình
7B gặp khó khăn ở mức 35 điểm, rất tương tự với hiệu suất SFT của LLaMA-7B. Chúng tôi đoán rằng họ
sử dụng GSM8K trong giai đoạn tiền huấn luyện theo (OpenAI, 2023) hoặc giai đoạn tinh chỉnh điều chỉnh
con người theo (Qingyi et al., 2023). Việc sử dụng tập dữ liệu tăng cường D′U13B của chúng tôi để thay
thế GSM8K gốc có thể tăng cường đáng kể hiệu suất của các mô hình 7B của họ.

4 THẢO LUẬN

4.1 PHÂN PHỐI KHÁC NHAU CỦA CÁC ĐƯỜNG LỐI LẬP LUẬN
Trong phân tích trên về dữ liệu huấn luyện RFT, chúng tôi quan sát thấy rằng lấy mẫu từ chối có thể tăng
cường câu hỏi huấn luyện với các đường lối tính toán lập luận đa dạng. Trong phần này, chúng tôi điều tra
liệu các mô hình RFT có thể học tạo ra các đường lối lập luận khác nhau để đạt được câu trả lời đúng hay
không. Chúng tôi tinh chỉnh LLaMA và LLaMA2 7B và 13B trên D′U13B. Trong quá trình suy luận, chúng
tôi lấy mẫu 100 đường lối lập luận khác nhau từ mỗi mô hình được huấn luyện cho mỗi câu hỏi tập kiểm
tra với nhiệt độ 0.7. Đối với mỗi câu hỏi, chúng tôi tính toán số lượng quá trình tính toán khác nhau được
trình bày trong 100 đường lối lập luận được lấy mẫu dẫn đến câu trả lời đúng và vẽ biểu đồ tần suất liên
quan đến các câu hỏi tập kiểm tra. Các mô hình SFT và RFT trên các tập dữ liệu tự lấy mẫu (RFT k=100)
được bao gồm để so sánh.

Như được hiển thị trong Hình 7, các mô hình được huấn luyện bởi RFT trên D′U13B thể hiện nhiều số
lượng câu hỏi hơn so với các mô hình được huấn luyện bởi RFT k=100 và SFT trên số lượng lớn hơn các
quá trình tính toán độc đáo. Có nhiều số lượng câu hỏi hơn cho các mô hình SFT trong đó tất cả các đường
lối lập luận được lấy mẫu chỉ tương ứng với một quá trình tính toán duy nhất và các mô hình SFT hầu như
không thể tạo ra hơn 8 quá trình tính toán khác nhau

--- TRANG 9 ---
Preprint
Mô hình Cơ sở Huấn luyện maj1@1 maj1@K*
LLM Độc quyền
GPT-4 (OpenAI, 2023) 5-shot ICL 92.0 -
GPT-3-175B (Brown et al., 2020) SFT 34.0 -
PaLM2 (Anil et al., 2023) 8-shot ICL 80.7 91.0@K=40
PaLM-540B (Chowdhery et al., 2022) 8-shot ICL 56.5 74.4@K=40
Chinchilla-70B (Uesato et al., 2022) 5-shot ICL 43.7 58.6@K=96
Chinchilla-70B SFT 58.9 77.7@K=96
LLM Mã nguồn mở
GPT-Neo-2.7B (Black et al., 2021) FCS + PCS (Ni et al., 2023) 19.5 41.4
GPT-J-6B (Wang & Komatsuzaki, 2021) CoRE (Zhu et al., 2023) 34.9 63.2@K=40
ChatGLM2-6B (Zeng et al., 2022) 8-shot ICL 32.4 -
ChatGLM2-6B Điều chỉnh Con người 28.1 -
ChatGLM2-12B 8-shot ICL 40.9 -
ChatGLM2-12B Điều chỉnh Con người 38.1 -
InternLM-7B (Team, 2023) 4-shot ICL 31.2 -
InternLM-7B Điều chỉnh Con người 34.5
LLaMA-7B SFT 35.9 48.7
RFT của chúng tôi trên LLM mã nguồn mở
LLaMA-7B RFT-U13B 49.3 61.8
LLaMA2-7B RFT-U13B 50.3 65.6
LLaMA-13B RFT-U13B 52.1 66.2
LLaMA2-13B RFT-U13B 55.4 69.1

Bảng 3: So sánh kết quả GSM8K với các baseline khác. RFT-U13B có nghĩa là các mô hình được tinh chỉnh
trên D′U13B. FCS và PCS đại diện cho các giải pháp hoàn toàn đúng và giải pháp một phần đúng tương ứng.
*K=100 nếu không được chỉ định.

Hình 7: Biểu đồ tần suất của số lượng câu hỏi được giải quyết với số lượng khác nhau của các đường lối
tính toán lập luận độc đáo. Chúng tôi hiển thị sự khác biệt trong số lượng câu hỏi giữa SFT và RFT U13B
trong hai trường hợp khi số lượng đường lối tính toán lập luận độc đáo là 1 hoặc hơn 10.

cho một câu hỏi. Phân tích này chứng minh rằng các đường lối tính toán lập luận đa dạng trong dữ liệu huấn
luyện có thể trang bị cho các LLM khả năng tìm ra logic lập luận đa dạng để giải quyết các bài toán toán học.

--- TRANG 10 ---
Preprint
Kích thước mô hình 7B 7B-2 13B 13B-2 33B 65B 70B
FLOPs tiền huấn luyện 4.2×10228.4×10227.8×10221.6×10232.7×10235.5×10238.4×1023
FLOPs SFT 1.7×10173.3×10177.7×10171.3×10181.7×1018
FLOPs suy luận RFT 1.4×10182.6×10186.9×10181.4×10191.8×1019
FLOPs RFT-U33B 3.0×10185.7×10181.3×10192.2×10193.0×1019
Giờ GPU tiền huấn luyện 82k 184k 135k 368k 530k 1022k 1720k
Giờ GPU SFT 0.6 4 40 74 80
Giờ GPU suy luận RFT 10 0.1k 0.1k 4.3k 4.5k
Giờ GPU RFT-U33B 9 62 0.6k 1k 1.2k
Độ chính xác ICL 11.0 14.6 17.8 28.7 35.6 50.9 56.8
Độ chính xác SFT 35.9 41.6 43.0 50.0 54.6 59.3 63.2
Độ chính xác RFT-U33B 49.1 51.2 51.4 55.3 57.9 - -

Bảng 4: Thống kê FLOPs và giờ GPU cần thiết cho tiền huấn luyện, SFT, suy luận RFT và RFT. Chúng tôi
lấy giờ GPU tiền huấn luyện từ Touvron et al. (2023a;b). Giờ GPU cho suy luận RFT được tính toán cho
7,473 câu hỏi tập huấn luyện và 100 mẫu mỗi câu hỏi. Để tận dụng tối đa GPU và phù hợp với các mô hình
vào bộ nhớ GPU một cách hợp lý, chúng tôi điều chỉnh kích thước lô suy luận. Đối với các mô hình 33B,
65B và 70B, chúng tôi sử dụng DeepSpeed ZeRO3 (Rasley et al., 2020) để huấn luyện phân tán. Tất cả
giờ GPU dựa trên NVIDIA A100 80GB GPU. Lưu ý rằng chúng tôi sử dụng các tham số không nhúng để
tính toán FLOPs trong các thí nghiệm của chúng tôi.

4.2 HƯỚNG TỚI LẬP LUẬN TOÁN HỌC XUẤT SẮC
Từ các phát hiện của chúng tôi, có hai yếu tố chính có thể cải thiện khả năng lập luận toán học với một
lượng mẫu được chú thích bởi con người được đặt trước, bao gồm: (1) Tiền huấn luyện các LLM đến tổn
thất thấp hơn; (2) Tăng cường tinh chỉnh với lấy mẫu từ chối. Thông qua các thí nghiệm rộng rãi, chúng
tôi xác minh thực nghiệm các mối quan hệ tỷ lệ giữa hiệu suất lập luận toán học của LLM với cả hai yếu tố
tương ứng. Xuất phát từ sự quan tâm đến NLP bền vững, trong phần này, chúng tôi điều tra các tài nguyên
tính toán có thể cần thiết để ngoại suy hiệu suất toán học của LLM bằng cả hai yếu tố và thảo luận về cách
cải thiện hiệu suất hiệu quả hơn.

Chúng tôi ước tính các FLOPs tiền huấn luyện, SFT, suy luận RFT và RFT theo Kaplan et al. (2020)
và thời gian GPU trong Bảng 4, được chi tiết trong Phụ lục E. Chúng ta có thể thấy rằng thời gian chi phí
của SFT (∼1×10−5) và RFT (∼1×10−4) là không đáng kể so với tiền huấn luyện. Người ta luôn có thể
sử dụng SFT và RFT để cải thiện hiệu suất của mô hình. Tuy nhiên, có thể khó sử dụng RFT để tăng cường
hiệu suất thêm. Vì chúng ta cần nhiều số lần lấy mẫu hơn (ở mức độ mũ) để tăng các đường lối lập luận
riêng biệt và tồn tại một giới hạn trên của lượng đường lối lập luận riêng biệt cho một câu hỏi lập luận toán
học nhất định.

Chúng tôi giả định rằng hiệu suất tuân theo RFT >SFT>ICL, từ các phát hiện trong bài báo này, chúng ta
biết rằng tốc độ cải thiện tuân theo RFT <SFT<ICL. Và nếu chúng ta có một mô hình ngôn ngữ toàn năng
có tổn thất tiền huấn luyện giống như tính ngẫu nhiên của corpus, nó có thể có RFT = SFT = ICL = 100.
Vì vậy, khi bạn tiền huấn luyện một mô hình ngôn ngữ tốt hơn (tức là tổn thất tiền huấn luyện nhỏ hơn),
hiệu suất của mô hình bạn vẫn tuân theo RFT >SFT>ICL nhưng khoảng cách hiệu suất của chúng đang
giảm dần. Vì bạn có thể có được một mô hình RFT mà không cần quá nhiều nỗ lực (so với tiền huấn luyện),
thì điều quan trọng nhất chúng ta nên làm là giảm tổn thất tiền huấn luyện của mô hình. Từ LLaMA-7B đến
LLaMA2-7B, nó cần thêm 4.2×1022 FLOPs để có được cải thiện 2.1 trong cài đặt RFT-U33B với giảm
tổn thất tiền huấn luyện 0.05. Từ LLaMA-7B đến LLaMA-13B, nó thêm 3.6×1022 FLOPs để có được
cải thiện 2.3 trong cài đặt RFT-U33B với giảm tổn thất tiền huấn luyện 0.07. Trong khi việc giảm thiểu
tổn thất tiền huấn luyện tốn kém so với SFT và RFT, chúng tôi tin rằng các khả năng khác có thể tuân theo
một mô hình tương tự và tiền huấn luyện tốt hơn có thể có lợi cho tất cả các nhiệm vụ khác.

5 KẾT LUẬN
Trong bài báo này, chúng tôi đang điều tra mối quan hệ tỷ lệ trong việc giám sát khả năng lập luận toán học
với các mô hình ngôn ngữ lớn. Chúng tôi tìm ra mối quan hệ giữa hiệu suất toán học và tổn thất tiền huấn
luyện, lượng dữ liệu có giám sát và các đường lối lập luận riêng biệt. Chúng tôi thấy rằng các mô hình ngôn
ngữ tốt hơn hưởng lợi ít hơn với SFT và RFT, và điều quan trọng nhất là tiền huấn luyện một mô hình ngôn
ngữ tốt hơn hướng tới khả năng lập luận toán học xuất sắc.

6 LỜI CẢM ƠN
Chúng tôi muốn bày tỏ lòng biết ơn chân thành đến Tianhang Zhu, Runji Lin, Kai Dang, Keming Lu, Wei
Wang và Junyang Lin vì những hiểu biết có giá trị và đóng góp của họ cho bài báo này.

7 HẠN CHẾ
Trong bài báo này, chúng tôi bỏ lỡ các phần sau đây rất quan trọng cho việc xây dựng khả năng lập luận
toán học cho LLM và nên được thảo luận trong phiên bản sửa đổi của bài báo này hoặc các công trình tương lai.
• RFT cho các mô hình LLaMA 65B và 70B.
• Tiền huấn luyện trên corpus liên quan đến toán học. Điều này rõ ràng là hữu ích được hiển thị trong
Lewkowycz et al. (2022). Trong khi tổn thất tiền huấn luyện thu được ở đây không thể căn chỉnh
với tổn thất của các mô hình được tiền huấn luyện miền tổng quát.
• Chúng tôi không hồi quy bất kỳ luật tỷ lệ nào trong bài báo này vì nhiều con số được ước tính và
tổn thất tiền huấn luyện, lời nhắc ICL và cài đặt SFT của các mô hình khác nhau có thể không được
căn chỉnh.

TÀI LIỆU THAM KHẢO
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,
Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experi-
ence replay. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 30. Cur-
ran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/
paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf .

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.
arXiv preprint arXiv:2305.10403 , 2023.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Ols-
son, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-
Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse,
Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mer-
cado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna
Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Con-
erly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai:
Harmlessness from ai feedback, 2022.

Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Au-
toregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.
org/10.5281/zenodo.5297715 .

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-V oss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot
learners. ArXiv , abs/2005.14165, 2020. URL https://api.semanticscholar.org/
CorpusID:218971783 .

--- TRANG 11 ---
Preprint
Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. arXiv
preprint arXiv:2210.14891 , 2022.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-
skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-
nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas
Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways,
2022.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.

Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum,
and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment,
2023.

Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A
continuous effort to measure large language models' reasoning performance, 2023a.

Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language
models towards multi-step reasoning. arXiv preprint arXiv:2301.12726 , 2023b.

Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022.

Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo
Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative
modeling. arXiv preprint arXiv:2010.14701 , 2020.

Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer,
2021.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hen-
nigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,
Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.
Training compute-optimal large language models, 2022.

Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei
Han. Large language models can self-improve, 2022.

Zhanming Jie, Jierui Li, and Wei Lu. Learning to reason deductively: Math word problem solving
as complex relation extraction. In Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers) , pp. 5944–5955, Dublin, Ireland, May
2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.410. URL
https://aclanthology.org/2022.acl-long.410 .

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. CoRR , abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361 .

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL
https://openreview.net/forum?id=e2TBb5y0yFf .

--- TRANG 12 ---
Preprint
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.
MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, pp. 1152–1157, San Diego, California, June 2016. Association for Computational Linguis-
tics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136 .

Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang,
and Ee-Peng Lim. Mwptoolkit: An open-source framework for deep learning-based math word
problem solvers, 2021.

Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-
masesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam
Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with lan-
guage models, 2022.

Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making
language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 5315–
5333, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https:
//aclanthology.org/2023.acl-long.291 .

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint
arXiv:2305.20050 , 2023.

Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Noua-
mane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language
models, 2023.

Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir
Radev, and Jianfeng Gao. Learning math reasoning from self-sampled correct and partially-
correct solutions. In The Eleventh International Conference on Learning Representations , 2023.
URL https://openreview.net/forum?id=4D4TSJE6-K .

Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Au-
gustus Odena. Show your work: Scratchpads for intermediate computation with language models,
2021.

OpenAI. Gpt-4 technical report, 2023.

Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies , pp. 2080–
2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168 .

Si Qingyi, Wang Tong, Gu Naibin, Liu Rui, and Lin Zheng. Alpaca-cot: An instruction-tuning
platform with unified interface of instruction collection, parameter-efficient methods, and large
language models. https://github.com/PhoebusSi/alpaca-CoT , 2023.

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti-
mizations enable training deep learning models with over 100 billion parameters. In Proceedings
of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining ,
KDD '20, pp. 3505–3506, New York, NY , USA, 2020. Association for Computing Machin-
ery. ISBN 9781450379984. doi: 10.1145/3394486.3406703. URL https://doi.org/10.
1145/3394486.3406703 .

David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical rea-
soning abilities of neural models, 2019.

--- TRANG 13 ---
Preprint
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. Distilling reasoning capabilities into
smaller language models. In Findings of the Association for Computational Linguistics: ACL
2023 , pp. 7059–7073, Toronto, Canada, July 2023. Association for Computational Linguistics.
URLhttps://aclanthology.org/2023.findings-acl.441 .

Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.
Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492 , 2023.

InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities.
https://github.com/InternLM/InternLM , 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-
mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
language models, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
2023b.

Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia
Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and
outcome-based feedback, 2022.

Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language
Model. https://github.com/kingoflolz/mesh-transformer-jax , May 2021.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language
models. In The Eleventh International Conference on Learning Representations , 2023. URL
https://openreview.net/forum?id=1PL1NIMMrw .

Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
Du, Andrew M. Dai, and Quoc V . Le. Finetuned language models are zero-shot learn-
ers. ArXiv , abs/2109.01652, 2021. URL https://api.semanticscholar.org/
CorpusID:237416585 .

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto,
Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language
models. Trans. Mach. Learn. Res. , 2022, 2022a. URL https://api.semanticscholar.
org/CorpusID:249674500 .

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc
Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language mod-
els. ArXiv , abs/2201.11903, 2022b. URL https://api.semanticscholar.org/
CorpusID:246411621 .

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.

Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large
language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015 , 2023a.

--- TRANG 14 ---
Preprint
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank
responses to align language models with human feedback without tears, 2023b.

Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning with
reasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Ad-
vances in Neural Information Processing Systems , 2022. URL https://openreview.net/
forum?id=_3ELRdg2sgI .

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 , 2022.

Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, and Joseph E. Gonzalez. The wisdom of
hindsight makes language models better instruction followers, 2023.

Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang,
and Yujiu Yang. Solving math word problems via cooperative reasoning induced language mod-
els. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , pp. 4471–4485, Toronto, Canada, July 2023. Association for Compu-
tational Linguistics. URL https://aclanthology.org/2023.acl-long.245 .

A CÀI ĐẶT THÍ NGHIỆM CHI TIẾT

A.1 SFT TRÊN GSM8K
Chúng tôi tinh chỉnh GSM8K với 3 epoch và kích thước lô 128 trên GPU NVIDIA A100. Chúng tôi sử dụng
8 GPU cho các mô hình 7B và 13B, 16 GPU cho các mô hình 33B, và 32 GPU cho các mô hình 65B và
70B trong quá trình tinh chỉnh. Chúng tôi sử dụng tốc độ học đỉnh 2e-5 với 3% khởi động tốc độ học.
Chúng tôi đánh giá kết quả trên epoch cuối cùng. Chúng tôi sử dụng giải mã tham lam để tính maj1@1 và
giải mã với nhiệt độ 0.7 để tính maj1@100.

A.2 SFT TRÊN GSM8K ĐƯỢC LẤY MẪU XUỐNG
Chúng tôi lấy mẫu xuống ngẫu nhiên tập dữ liệu GSM8K để tinh chỉnh. Chúng tôi thấy rằng việc sử dụng
3 epoch cho dữ liệu ít sẽ dẫn đến kết quả rất kém được liệt kê trong Bảng 5. Chúng tôi tìm kiếm epoch
huấn luyện trong {3, 3/tỷ lệ dữ liệu} và đánh giá epoch mới nhất. Chúng tôi báo cáo kết quả kiểm tra tốt
hơn giữa hai cài đặt epoch khác nhau này.

A.3 TINH CHỈNH LẤY MẪU TỪ CHỐI TRÊN GSM8K
Chúng tôi sử dụng mô hình SFT π để lấy mẫu trên tập dữ liệu huấn luyện k= 100 lần với nhiệt độ 0.7.
Chúng tôi trích xuất danh sách phương trình trong các đường lối lập luận được tạo ra bằng cách tìm
<<equation>> trước tiên, loại bỏ tất cả khoảng trắng và nối danh sách chuỗi phương trình bằng một ký
hiệu đặc biệt thành một chuỗi (gọi là

--- TRANG 15 ---
Preprint
nó get equation trong thuật toán của chúng tôi) để loại trùng. Chúng tôi chọn các đường lối lập luận bằng
thuật toán này:

Thuật toán 1: Chọn lọc Đường lối Lập luận
Dữ liệu: Đường lối lập luận cho câu hỏi q, Rq
Kết quả: Đường lối lập luận được chọn cho câu hỏi q, Rsq
1 Khởi tạo đường lối lập luận được chọn, Rsq=list()
2 Khởi tạo tập hợp phương trình xuất hiện, Esq=set()
3 for r in Rq do
4    if getequation(r) ∉ Esq then
5       Rsq.append(r);
6       Esq.update([getequation(r)])
7    end
8    else
9       find rs ∈ Rsq s.t. getequation(rs) = getequation(r);
10      if Σi:rsi∈Esq,rsi≠rs Levenstein dist(r,rsi) > Σi:rsi∈Esq,rsi≠rs Levenstein dist(rs,rsi) then
11         rs = r;
12      end
13   end
14 end

Chúng tôi đang cố gắng tìm các đường lối lập luận khác biệt nhất dựa trên khoảng cách Levenstein. Ý tưởng
đến từ việc chúng tôi muốn các đường lối lập luận đa dạng để tổng quát hóa tốt hơn.

B KẾT QUẢ CHI TIẾT CỦA SFT VÀ RFT
Chúng tôi liệt kê kết quả chi tiết của SFT và RFT trong Bảng 5 và 6.

Mô hình Dữ liệu Epoch 7B 7B-2 13B 13B-2 33B 65B 70B-2
ICL-8shot 0 0 11.0 14.6 17.8 28.7 35.6 50.9 56.8
SFT 1/32 96 9.5 10.1 8.6 17.1 18.6 25.2 27.4
SFT 1/16 48 14.3 15.5 14.2 23.9 25.9 28.9 33.6
SFT 1/8 24 17.9 20.8 18.4 28.5 31.6 35.8 38.9
SFT 1/4 12 21.6 27.7 26.7 36.3 38.4 45.6 46.9
SFT 1/2 6 29.0 33.1 35.2 43.7 48.6 50.5 57.5
SFT 1/32 3 7.8 14.2 0.0 5.9 25.3 28.9 15.8
SFT 1/16 3 12.7 16.2 7.4 27.7 29.2 39.5 52.8
SFT 1/8 3 16.5 21.8 19.5 33.4 39.3 46.0 57.8
SFT 1/4 3 22.7 28.1 27.4 37.5 44.6 50.4 57.8
SFT 1/2 3 30.9 34.6 36.1 45.3 50.8 55.6 61.0
SFT 7.4K 3 35.9 41.6 43.0 50.0 54.6 59.3 63.2
RFT không loại trùng 1/32 3 37.5 - - - - - -
RFT không loại trùng 1/16 3 38.3 - - - - - -
RFT không loại trùng 1/8 3 41.1 - - - - - -
RFT không loại trùng 1/4 3 41.2 - - - - - -
RFT không loại trùng 1/2 3 43.9 - - - - - -
RFT không loại trùng 400K 3 43.6 46.7 46.9 53.7 - - -
RFT k=1 ∼12K 3 37.6 43.4 42.7 52.1 - - -
RFT k=3 ∼15K 3 39.0 45.3 45.2 51.9 - - -
RFT k=6 ∼18K 3 39.5 45.6 46.8 52.2 - - -
RFT k=12 ∼22K 3 41.6 45.3 48.0 53.1 - - -
RFT k=25 ∼28K 3 40.9 46.5 46.0 52.6 - - -
RFT k=50 ∼35K 3 40.7 47.0 49.4 54.5 - - -
RFT k=100 ∼47K 3 41.7 47.5 49.1 54.8 54.5 - -
RFT-U13B 104K 3 49.3 50.3 52.1 55.4 56.5 59.0 62.3
RFT-U33B 110K 3 49.1 51.2 51.4 55.3 57.9 59.7 64.8

Bảng 5: Kết quả số chi tiết trong bài báo này, một số thí nghiệm vẫn đang được chạy. Chúng tôi báo cáo
maj1@1 (độ chính xác) trong bảng này.

--- TRANG 16 ---
Preprint
Cài đặt 7B 7B-2 13B 13B-2 33B 65B 70B-2
ICL-8shot 11.0/18.1 14.6/- 17.8/29.3 28.7/- 35.6/53.1 50.9/69.7 56.8/-
SFT 35.9/48.7 41.6/55.4 43.0/55.2 50.0/61.7 54.6/72.6 59.3/69.7 63.2/73.5
RFT k=100 41.7/52.7 47.5/58.7 49.1/59.9 54.8/65.4 54.5/- - -
RFT-U13B 49.3/61.8 50.3/65.6 52.1/66.2 55.4/69.1 56.5/- 59.0/- 62.3/-
RFT-U33B 49.1/61.6 51.2/64.1 51.4/66.3 55.3/69.1 57.9/71.3 59.7/- 64.8/-

Bảng 6: Kết quả số chi tiết trong bài báo này, một số thí nghiệm vẫn đang được chạy. Chúng tôi báo cáo
maj1@100 trong bảng này.

Truy vấn A Weng kiếm được 12 đô la một giờ cho việc trông trẻ. Hôm qua, cô ấy chỉ trông trẻ 50 phút.
Cô ấy kiếm được bao nhiêu tiền?

Đường lối 1 Weng kiếm được 12/60 = $〈〈12/60=0.2〉〉0.2 một giờ cho 50 phút mà cô ấy làm việc hôm qua.
Do đó, cô ấy kiếm được $0.2 x 50 = $〈〈0.2*50=10〉〉10 cho 50 phút mà cô ấy làm việc hôm qua.#### 10

Đường lối 2 Weng kiếm được 12 / 60 = $〈〈12/60=0.2〉〉0.2 mỗi phút. Vậy, Weng kiếm được 50 * 0.2 =
$〈〈50*0.2=10〉〉10 cho 50 phút trông trẻ. #### 10

Đường lối 3 Vì có 60 phút trong một giờ, thì Weng đã làm 50/60 = 〈〈50/60=0.8333〉〉0.8333 giờ trông trẻ.
Vậy, cô ấy kiếm được $12 x 0.8333 = $〈〈12*0.8333=10〉〉10. #### 10

Đường lối 4 Cô ấy kiếm được 12 * 50 / 60 = $〈〈12*50/60=10〉〉10. #### 10

Đường lối 5 Weng kiếm được 12 * (50 / 60) = $〈〈12*(50/60)=10〉〉10. #### 10

Truy vấn B Một trường đại học địa phương đang cung cấp các bài học tiếng Đức và hiện tại có 8 sinh viên
đăng ký. Thông qua quảng cáo, 8 người nữa quan tâm nhưng một phần tư trong số họ đã bỏ học trong vòng
một ngày. 2 người nữa bực bội và rời đi. Lớp học sau đó tập hợp để chỉ ra khóa học thực sự đơn giản như
thế nào và tăng số lượng đăng ký lên 5 lần số lượng sinh viên đã đăng ký trong lớp, nhưng 2 người phải
bỏ học vì xung đột lịch trình. Sau một cuộc tập hợp cuối cùng, 6 người nữa đăng ký. Khi thời gian trôi qua,
một nửa lớp học cuối cùng đã bỏ học, và một nửa số sinh viên còn lại đã tốt nghiệp. Còn bao nhiêu người
vẫn đang đăng ký?

Đường lối 1 8+8= 〈〈8+8=16〉〉16 sinh viên đăng ký vào lớp tiếng Đức. 8*.25= 〈〈8*.25=2〉〉2 bỏ học
từ những người mới tuyển dụng. 2 người nữa rời đi, vậy lớp còn lại 16-2-2= 〈〈16-2-2=12〉〉12 sinh viên.
Có 12*5+12= 〈〈12*5+12=72〉〉72 tổng cộng sinh viên trong lớp sau cuộc tập hợp. Hai người phải bỏ lớp,
vậy còn lại 72-2= 〈〈72-2=70〉〉70. 70+6= 〈〈70+6=76〉〉76 đăng ký sau cuộc tập hợp cuối cùng. 76/2=
〈〈76/2=38〉〉38 còn lại sau khi một nửa lớp bỏ học. 38/2= 〈〈38/2=19〉〉19 còn lại sau khi một nửa lớp tốt
nghiệp. #### 19

Đường lối 2 8+8= 〈〈8+8=16〉〉16 sinh viên đăng ký sau cuộc tập hợp đầu tiên. 16/4= 〈〈16/4=4〉〉4 bỏ
học. 16-4= 〈〈16-4=12〉〉12 còn lại. 12*5+12= 〈〈12*5+12=72〉〉72 sau cuộc tập hợp thứ hai. 72-2=
〈〈72-2=70〉〉70 sau khi 2 người nữa bỏ học. 70+6= 〈〈70+6=76〉〉76 sau khi 6 người nữa đăng ký. 76/2=
〈〈76/2=38〉〉38 còn lại sau khi một nửa lớp bỏ học. 38/2= 〈〈38/2=19〉〉19 còn lại sau khi một nửa lớp tốt
nghiệp. #### 19

Đường lối 3 Quảng cáo mang lại 8+8= 〈〈8+8=16〉〉16 sinh viên. Trong số này, 16/4= 〈〈16/4=4〉〉4 bỏ
học. 16-4-2= 〈〈16-4-2=10〉〉10 còn lại. Sau đó 10*5= 〈〈10*5=50〉〉50 người nữa đăng ký. Điều này đưa
lớp đến 50+10= 〈〈50+10=60〉〉60 sinh viên. Sau đó 60+6= 〈〈60+6=66〉〉66 đăng ký. Vậy có 66+10=
〈〈66+10=76〉〉76 sinh viên. Sau đó 76/2= 〈〈76/2=38〉〉38 bỏ học. Vậy 76-38= 〈〈76-38=38〉〉38 còn lại.
Sau đó 38/2= 〈〈38/2=19〉〉19 tốt nghiệp. Vậy 38-19= 〈〈38-19=19〉〉19 còn lại. #### 19

Bảng 7: Các trường hợp của đường lối lập luận được tạo ra với độ phức tạp lập luận khác nhau từ lấy mẫu
từ chối cho RFT. Các tính toán được tô sáng màu đỏ.

C NGHIÊN CỨU TÌNH HUỐNG CỦA RFT
Trong phần này, chúng tôi trình bày các trường hợp của các mẫu huấn luyện từ lấy mẫu từ chối. Các nghiên
cứu tình huống sẽ làm sáng tỏ cách RFT có thể cải thiện hiệu suất lập luận toán học của LLM. Các trường
hợp được hiển thị trong Bảng 7. Như đã đề cập trước đó, RFT xem xét các đường lối lập luận với các quá
trình tính toán khác nhau liên quan đến hình thức hoặc thứ tự phương trình, dẫn đến câu trả lời đúng. Trong
các trường hợp từ Bảng 7, tất cả các đường lối lập luận từ RFT đều dẫn đến câu trả lời đúng là 10, trong
khi các quá trình tính toán của lập luận thì đa dạng. Đường lối 1 và 2, cũng như Đường lối 4 và 5, khác
nhau về hình thức phương trình như được tô sáng màu đỏ. Đường lối 1 và 2 trình bày một quá trình lập
luận tính toán hai bước trong khi Đường lối 4 và 5 chuyển sang một quá trình lập luận tính toán một bước.
Trường hợp này chứng minh rằng lấy mẫu từ chối có thể cung cấp nhiều tín hiệu giám sát hơn nhằm cải
thiện hiệu suất lập luận toán học. Các đường lối lập luận được lọc được lấy mẫu từ chính các LLM có chất
lượng tương tự như các minh chứng lập luận từ chú thích của con người.

--- TRANG 17 ---
Preprint
D THÍ NGHIỆM SƠ BỘ

D.1 TĂNG CƯỜNG TRUY VẤN TỰ ĐỘNG
Thông qua các thí nghiệm sơ bộ và nghiên cứu tình huống của chúng tôi, các lỗi được thực hiện bởi các
LLM được tinh chỉnh một phần được gây ra bởi các chuỗi lập luận không chính xác trong đó LLM hiểu
sai thông tin ngữ cảnh hoặc không xem xét tất cả thông tin trong các truy vấn. Mặc dù những chuỗi lập
luận không chính xác như vậy dẫn đến câu trả lời sai cho các truy vấn gốc, bản thân các chuỗi lập luận
đại diện cho logic hợp lý. Ví dụ, đối với truy vấn Josh decides to try flipping a house. He buys a house
for $80,000 and then puts in $50,000 in repairs. This increased the value of the house by 150%. How
much profit did he make?, một mô hình LLaMA được tinh chỉnh dự đoán The value of the house increased
by 80,000*.15=$12,000. So the house was worth 80,000+12,000=$92,000. So he made a profit of
92,000-80,000-50,000=$42,000 trong đó mô hình hiểu sai 150% thành 15%, nhưng chuỗi lập luận hợp
lý nếu chúng ta bỏ qua lỗi này.

Do đó, những dự đoán sai như vậy được thực hiện bởi các LLM có thể đúng dưới các truy vấn khác (nếu
chúng ta thay đổi 150% thành 15% trong ví dụ trên). Chúng tôi tiến hành thí nghiệm để tạo ra các truy vấn
cho các chuỗi lập luận được dự đoán. Đây là ý tưởng tương tự như hindsight experience replay (Andrychowicz
et al., 2017) trong học tăng cường trong đó phương pháp được thiết kế để xử lý các vấn đề phần thưởng
thưa thớt bằng cách thay đổi các mục tiêu gốc cho các mẫu thất bại để tạo thành các mẫu với phần thưởng
tích cực. Ý tưởng như vậy gần đây đã được HIR áp dụng (Zhang et al., 2023) để điều chỉnh tốt hơn các
LLM với các hướng dẫn.

Cụ thể, chúng tôi định dạng lại GSM8K ngược lại bằng cách dự đoán truy vấn từ kết quả lập luận thực tế
tương ứng và sau đó chúng tôi tinh chỉnh một mô hình LLaMA trên nhiệm vụ ngược. Chúng tôi sử dụng
mô hình này để tạo ra các truy vấn trên các chuỗi lập luận được dự đoán bởi một mô hình LLaMA được
tinh chỉnh bình thường trên tập huấn luyện của GSM8K, chính thức hóa một mẫu huấn luyện để tăng cường.
Chúng tôi thí nghiệm trên mô hình LLaMA 7B và tinh chỉnh các mô hình trên dữ liệu trộn mẫu gốc và tạo
ra hoặc chỉ trên các mẫu tạo ra.

Kết quả được hiển thị trong hình con bên trái trong Hình 8. Chúng ta có thể thấy rằng tinh chỉnh với dữ
liệu tăng cường truy vấn tự động dẫn đến kết quả tồi tệ nhất, và hiệu suất của việc trộn dữ liệu gốc với
dữ liệu tăng cường truy vấn tự động vẫn thua kém so với dữ liệu gốc. Hiệu suất tinh chỉnh cho lập luận
toán học không hưởng lợi từ ý tưởng ngây thơ của tăng cường truy vấn tự động. Thông qua một số nghiên
cứu tình huống của dữ liệu tạo ra, chúng tôi thấy rằng có hai khuyết điểm chính trong dữ liệu tạo ra. Thứ
nhất là một số chuỗi lập luận bản thân chúng không hợp lý về mặt logic, ví dụ, có thể có một số lỗi tính
toán trong các chuỗi lập luận. Thứ hai là truy vấn tạo ra có thể không phù hợp với một chuỗi lập luận.
Mô hình tạo truy vấn vẫn có thể hiểu sai thông tin trong các chuỗi lập luận. Cả hai khuyết điểm đều gây
ra chất lượng dữ liệu tăng cường tầm thường, do đó có thể là những lý do cho sự thất bại của quy trình
tăng cường dữ liệu này.

D.2 TĂNG CƯỜNG SỬA ĐỔI TỰ ĐỘNG
Chúng tôi cũng khám phá việc cải thiện khả năng lập luận toán học của LLM thông qua tăng cường sửa
đổi. Để trang bị cho LLaMA khả năng sửa đổi, chúng tôi tạo ra một tập dữ liệu sửa đổi bằng cách đầu
tiên lấy mẫu K đường lối lập luận từ một mô hình LLaMA được tinh chỉnh, sau đó nối truy vấn với một
trong các đường lối lập luận được lấy mẫu bằng cách sử dụng một mẫu, và cuối cùng ghép đôi với đường
lối lập luận thực tế để tạo thành một mẫu huấn luyện. Chúng tôi sử dụng nhiệt độ lấy mẫu 0.7 để tạo ra
các đường lối lập luận. Trong quá trình suy luận, chúng tôi sử dụng mô hình sửa đổi được tinh chỉnh để
sửa đổi dự đoán từ mô hình được tinh chỉnh bình thường.

Kết quả được hiển thị trong hình con giữa trong Hình 8. Chúng ta có thể thấy rằng với K= 1, mô hình
sửa đổi cải thiện độ chính xác cuối cùng một cách tối thiểu so với 36.09% đến 35.90%. Đáng ngạc nhiên,
khi chúng tôi tăng K, hiệu suất suy giảm. Khuyết điểm có thể của mô hình sửa đổi là các mẫu được tạo
ra trên tập huấn luyện cho huấn luyện sửa đổi gặp phải sự không khớp phân phối với các mẫu được tạo
ra trên tập kiểm tra cho suy luận sửa đổi. Các đường lối lập luận được lấy mẫu trên tập huấn luyện có thể

--- TRANG 18 ---
Preprint
Hình 8: Kết quả cho các phương pháp tăng cường dữ liệu tự động khác nhau. GSM. và H. đại diện cho
GSM8K và Hindsight tương ứng. Các đường chấm màu đỏ trong hình giữa và phải đại diện cho kết quả
của tinh chỉnh vanilla trên GSM8K.

có tương tự từ vựng lớn hơn với các đường lối lập luận thực tế so với những trên tập kiểm tra. Do đó,
chúng tôi thử hai quy trình khác nhau để giảm thiểu vấn đề như vậy.

1. Chúng tôi sử dụng đường lối lập luận được lấy mẫu có khoảng cách Levenstein lớn nhất trong số K
đường lối được lấy mẫu so với đường lối thực tế để tạo thành một mẫu huấn luyện.
2. Chúng tôi chia tập huấn luyện thành N fold, và tinh chỉnh một mô hình trên mỗi N−1 fold và lấy
mẫu đường lối lập luận trên fold còn lại.

Kết quả được hiển thị trong hình con giữa và phải trong Hình 8, chúng ta có thể thấy rằng khi tận dụng
khoảng cách Levenstein để chọn lọc đường lối lập luận, mô hình sửa đổi được tinh chỉnh có được sự tăng
cường hiệu suất, thu hoạch hiệu suất tốt hơn một cách đồng nhất so với baseline tinh chỉnh trên các K
khác nhau. Kết quả chứng minh rằng đối với hiệu suất sửa đổi, sự đa dạng từ vựng của các đường lối lập
luận quan trọng khi xây dựng các mẫu huấn luyện. Tuy nhiên, hiệu suất sửa đổi không hưởng lợi từ quy
trình N-fold.

E ƯỚC TÍNH FLOPS CỦA SFT VÀ RFT
Chúng tôi chủ yếu tuân theo các ký hiệu của (Kaplan et al., 2020) ở đây.

FLOPs Huấn luyện Đối với mỗi mẫu đầu vào có độ dài nctx trong tập dữ liệu GSM8K, chúng ta có thể
chia nó thành hai phần:
nctx = nQ + nR (1)
trong đó nQ, nR biểu thị độ dài của câu hỏi và đường lối lập luận tạo ra và câu trả lời tương ứng.
Ctrain ≈ 6NnctxNs (2)
trong đó Ns biểu thị số lượng mẫu.

FLOPs Suy luận Chúng tôi tính toán gần đúng FLOPs của mỗi token trong quá trình truyền xuôi:
Cforward(nctx) = 2N + 2nlayernctxdmodel (3)
Để đảm bảo kết quả chính xác và đáng tin cậy hơn, chúng tôi cũng tính đến bộ nhớ cache Key-Value (KV)
trong quá trình giải mã.
KV cache ≈ 4nlayerd²model (4)

--- TRANG 19 ---
Preprint
Do đó, chúng tôi thu được FLOPs mỗi token trong quá trình truyền xuôi có xem xét bộ nhớ cache KV.
C′forward(nctx) = 2N + 2nlayernctxdmodel − KV cache (5)
= 24nlayerd²model + 2nlayernctxdmodel − 4nlayerd²model (6)
= 20nlayerd²model + 2nlayernctxdmodel (7)
≈ 1.66N + 2nlayernctxdmodel (8)

Tổng FLOPs suy luận được tính như sau:
Ctotal = Ns·[nqCforward(nq) + Σ(i=nq to nq+nr) i·C′forward(i)] (9)
trong đó Ns biểu thị số lượng mẫu. nq, nr biểu thị độ dài trung bình (token) của truy vấn người dùng và
phản hồi tạo ra tương ứng. Trong tập dữ liệu GSM8K, nq ≈ 66 và nr ≈ 130.
