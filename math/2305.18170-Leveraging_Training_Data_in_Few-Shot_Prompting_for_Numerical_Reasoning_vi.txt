# Tận dụng Dữ liệu Huấn luyện trong Gợi ý Vài mẫu cho Lý luận Số học

Zhanming Jie
ByteDance Research
allan@bytedance.com

Wei Lu
Nhóm Nghiên cứu StatNLP
Đại học Công nghệ và Thiết kế Singapore
luwei@sutd.edu.sg

## Tóm tắt

Gợi ý chuỗi tư duy (CoT) với các mô hình ngôn ngữ lớn đã được chứng minh hiệu quả trong nhiều tác vụ xử lý ngôn ngữ tự nhiên, nhưng việc thiết kế các gợi ý khái quát hóa tốt cho các loại vấn đề đa dạng có thể đầy thử thách (Zhou et al., 2022), đặc biệt trong bối cảnh giải quyết bài toán từ ngữ toán học (MWP). Ngoài ra, thường có một lượng lớn dữ liệu huấn luyện có độ phủ đa dạng tốt hơn nhưng không có chú thích CoT, điều này hạn chế việc sử dụng các kỹ thuật học có giám sát. Để giải quyết các vấn đề này, chúng tôi nghiên cứu hai phương pháp tận dụng dữ liệu huấn luyện trong tình huống gợi ý vài mẫu: gợi ý chương trình động và chưng cất chương trình. Phương pháp của chúng tôi phần lớn được truyền cảm hứng từ Gao et al. (2022), nơi họ đề xuất thay thế CoT bằng các chương trình làm bước lý luận trung gian. Chiến lược gợi ý như vậy cho phép chúng tôi xác minh chính xác tính đúng đắn của câu trả lời thông qua việc thực thi chương trình trong giải quyết MWP. Gợi ý chương trình động của chúng tôi liên quan đến việc chú thích dữ liệu huấn luyện bằng cách lấy mẫu các chương trình đúng từ một mô hình ngôn ngữ lớn, trong khi chưng cất chương trình liên quan đến việc thích ứng một mô hình nhỏ hơn với dữ liệu huấn luyện được chú thích chương trình. Các thí nghiệm của chúng tôi trên ba tập dữ liệu MWP tiêu chuẩn chứng minh hiệu quả của những phương pháp này, mang lại các cải thiện đáng kể so với các baseline trước đây cho gợi ý và tinh chỉnh. Kết quả của chúng tôi cho thấy rằng việc tận dụng một lượng lớn dữ liệu huấn luyện có thể cải thiện khả năng khái quát hóa của các gợi ý và tăng cường hiệu suất của các mô hình nhỏ được tinh chỉnh trong giải quyết MWP¹.

## 1 Giới thiệu

Việc thiết kế các gợi ý hiệu quả là rất quan trọng cho thành công của gợi ý vài mẫu với các mô hình ngôn ngữ lớn (LLM) trong các tác vụ yêu cầu kỹ năng lý luận phức tạp (Wei et al., 2022; Zhou et al., 2022; Shrivastava et al., 2022; Fu et al., 2022). Đặc biệt đối với tác vụ bài toán từ ngữ số học, nó đặt ra một thách thức đáng kể để thiết kế một số lượng nhỏ các gợi ý chuỗi tư duy (CoT) (Wei et al., 2022) để giải quyết một phạm vi rộng các vấn đề.

¹Mã nguồn và dữ liệu của chúng tôi có sẵn tại https://github.com/allanj/dynamic-pal.

**Vấn đề:** Olivia có $23. Cô ấy mua năm chiếc bánh bagel với giá $3 mỗi chiếc. Cô ấy còn lại bao nhiều tiền?

**Gợi ý CoT**
LLM → CoT: Sau khi mua năm chiếc bánh bagel với giá $3 mỗi chiếc, Olivia đã tiêu 5 x $3 = $16. Vậy cô ấy còn lại 23 - 16 = 8 đô la.

**Gợi ý Chương trình**
LLM → Chương trình:
```python
def solution():
    money_initial = 23
    bagels = 5
    bagel_cost = 3
    money_spent = bagels * bagel_cost
    money_left = money_initial - money_spent
    ...
```

**Thực thi Chương trình**
Không đúng và Lấy mẫu lại ← đúng?
Đúng và Lưu

**Dữ liệu Huấn luyện**
Đáp án: 23 - 5 * 3 = 8

**Hình 1:** Chú thích chương trình với LLM.

May mắn thay, một lượng khiêm tốn dữ liệu huấn luyện thường có sẵn mặc dù không có chú thích chuỗi tư duy (CoT). Rubin et al. (2021) áp dụng một phương pháp dựa trên truy xuất để chọn các mẫu tương tự làm gợi ý. Trong khi phương pháp như vậy không hoạt động tốt cho lý luận số học so với gợi ý CoT (Wei et al., 2022), công trình gần đây (Magister et al., 2022) cũng đã thử chưng cất kiến thức CoT từ các mô hình ngôn ngữ lớn sang các mô hình ngôn ngữ nhỏ hơn. Các chú thích CoT được chưng cất cho phép chúng tôi tinh chỉnh thêm các mô hình ngôn ngữ nhỏ. Tuy nhiên, không có đảm bảo rằng các gợi ý CoT được tạo ra cho dữ liệu huấn luyện là đúng, và việc thực hiện đánh giá tự động để xác minh tính đúng đắn của CoT là thách thức. Như một giải pháp thay thế, công trình gần đây (Drori et al., 2022; Gao et al., 2022; Mishra et al., 2022) đã áp dụng các chương trình làm chuỗi lý luận trung gian trong các tác vụ như bài toán từ ngữ toán học (MWP), cho phép xác minh tự động các câu trả lời thông qua thực thi chương trình. Được truyền cảm hứng từ những phương pháp này, chúng tôi có thể thực hiện gợi ý với các mô hình tạo mã, như Codex (Chen et al., 2021), để chú thích dữ liệu huấn luyện bằng các chương trình có thể thực thi được.

```python
def solution():
    """Natalia đã bán kẹp cho 48 người bạn của cô ấy trong
    tháng Tư, và sau đó cô ấy bán một nửa số kẹp đó
    trong tháng Năm. Natalia đã bán tổng cộng bao nhiêu kẹp
    trong tháng Tư và tháng Năm?"""
    clips_april = 48
    clips_may = clips_april / 2
    clips_total = clips_april + clips_may
    result = clips_total
    return result
```

**Hình 2:** Ví dụ chương trình từ tập huấn luyện GSM8K theo định dạng trong PAL.

Hình 1 cho thấy quy trình chú thích chương trình tự động bằng các mô hình ngôn ngữ lớn. Như chúng ta có thể thấy trong ví dụ này, mặc dù câu trả lời cuối cùng "8 đô la" bằng CoT được tạo ra đúng, nhưng đường đi lý luận trung gian là sai vì tính toán sai cho "5×3". Thay vào đó, việc lấy mẫu chương trình tương đối nghiêm ngặt hơn ở chỗ chúng ta có thể thực thi để có được câu trả lời số thay vì CoT bằng ngôn ngữ tự nhiên. Rõ ràng, chúng ta có thể tiếp tục lấy mẫu chương trình với các nhiệt độ khác nhau cho đến khi câu trả lời được thực thi từ chương trình khớp với câu trả lời đúng. Khi chúng ta có được chương trình được chú thích, chúng ta có thể sử dụng dữ liệu huấn luyện "được chú thích" với chương trình pseudo-gold để cải thiện thêm hiệu suất trên tập kiểm tra.

Trong công trình này, chúng tôi chủ yếu nghiên cứu hai phương pháp để sử dụng các chương trình "được chú thích": gợi ý chương trình động và chưng cất chương trình (Magister et al., 2022). Gợi ý chương trình động sử dụng k mẫu huấn luyện tương tự nhất (với các chương trình pseudo-gold được chú thích) làm gợi ý vài mẫu. Chúng tôi sử dụng các bộ mã hóa câu có sẵn công khai và tiên tiến như OpenAI embeddings (Neelakantan et al., 2022)² và Sentence-T5 (Ni et al., 2022) để tính toán độ tương tự cosine. Mặt khác, chúng tôi theo Magister et al. (2022) để tinh chỉnh các mô hình ngôn ngữ nhỏ hơn trên dữ liệu huấn luyện pseudo-gold của chúng tôi. Nhìn chung, các thí nghiệm của chúng tôi trên ba tập dữ liệu bài toán từ ngữ toán học tiêu chuẩn chứng minh hiệu quả của việc tận dụng chương trình huấn luyện trong gợi ý vài mẫu của chúng tôi. Chúng tôi quan sát thấy các cải thiện đáng kể cho tất cả các tập dữ liệu, đặc biệt là cho tập dữ liệu MathQA (Amini et al., 2019), nơi các chủ đề đa dạng (ví dụ: vật lý, xác suất, v.v.) được liên quan đến các vấn đề mà gợi ý cố định kèm theo các ví dụ hạn chế không đủ để bao gồm toàn bộ phạm vi kiến thức cần thiết.

²https://openai.com/blog/new-and-improved-embedding-model/

| Tập dữ liệu | #Huấn luyện | #Chương trình | #Hợp lệ | #Kiểm tra |
|-------------|-------------|---------------|---------|-----------|
| GSM8K       | 07,473      | 6,363 (85.1%) | -       | 1,319     |
| SVAMP       | 03,138      | 3,071 (97.9%) | -       | 1,000     |
| MathQA †    | 16,191      | 7,676 (47.4%) | 2,411   | 1,605     |

**Bảng 1:** Thống kê tập dữ liệu và tỷ lệ phần trăm các chương trình được chú thích. †: Chúng tôi theo Jie et al. (2022) để có được phân chia được tiền xử lý.

## 2 Phương pháp

**Chú thích Dữ liệu Huấn luyện** Theo phương pháp trong mô hình ngôn ngữ hỗ trợ chương trình (PAL) (Gao et al., 2022), chúng tôi có thể lấy mẫu chương trình cho mỗi bài toán từ ngữ toán học làm chú thích. Cụ thể, chúng tôi sử dụng các gợi ý toán học từ PAL làm gợi ý khởi tạo để thực hiện gợi ý vài mẫu với các mô hình ngôn ngữ lớn (tức là Codex (Chen et al., 2021)). Chúng tôi tuân theo chính xác định dạng tương tự từ PAL (Gao et al., 2022) mà không có bất kỳ thay đổi nào, Hình 2 cho thấy một ví dụ chương trình từ tập huấn luyện GSM8K. Chúng tôi có thể xác minh tính đúng đắn của câu trả lời bằng cách so sánh kết quả từ việc thực thi chương trình với giá trị thực tế.

Đối với mỗi bài toán từ ngữ toán học x trong tập huấn luyện D, trước tiên chúng tôi thực hiện giải mã tham lam với nhiệt độ T = 0 để có được chương trình Python tốt nhất. Nếu câu trả lời dự đoán ŷ từ chương trình được thực thi P khớp với câu trả lời thực tế y, chúng tôi thêm tuple này (x, P, y) vào một tập dữ liệu huấn luyện mới D_prog. Nếu câu trả lời dự đoán không đúng, chúng tôi tăng nhiệt độ và tiếp tục lấy mẫu các chương trình cho đến khi tìm thấy một chương trình có câu trả lời đúng. Trong thực tế, chúng tôi có thể không luôn có được câu trả lời đúng và có ngân sách hạn chế cho việc sử dụng API Codex. Do đó, chúng tôi lấy mẫu nhiều nhất K lần cho mỗi trường hợp. Nếu chúng tôi không thể tìm thấy một chương trình có câu trả lời đúng trong K mẫu, chúng tôi loại bỏ trường hợp x. Kết quả là, kích thước của tập huấn luyện kết quả D_prog dự kiến sẽ nhỏ hơn tập huấn luyện gốc (tham khảo Bảng 1).

### 2.1 Gợi ý Chương trình Động

**Truy xuất Gợi ý** Với tất cả các trường hợp (x, P, y) trong D_prog, chúng tôi truy xuất M trường hợp liên quan nhất làm gợi ý. Chúng tôi sử dụng các embedding câu tiên tiến như sentence-T5 (Ni et al., 2022) và SimCSE (Gao et al., 2021) để có được biểu diễn cho mỗi bài toán từ ngữ toán học x. Sau đó chúng tôi tính toán độ tương tự cosine giữa mỗi mẫu kiểm tra và tất cả các mẫu huấn luyện. Dựa trên độ tương tự, chúng tôi chọn M mẫu tương tự nhất từ các trường hợp huấn luyện trong D_prog.

**Độ tương tự** Để xác minh thêm hiệu quả của việc sử dụng độ tương tự để chọn các mẫu gợi ý, chúng tôi cũng thí nghiệm với các chiến lược thay thế như lựa chọn ngẫu nhiên từ D_prog và chọn mẫu có độ tương tự thấp nhất.

### 2.2 Chưng cất Chương trình

Mục đích của chúng tôi là huấn luyện một mô hình nhỏ hơn bằng cách sử dụng dữ liệu được chú thích so với các LLM như Codex (Chen et al., 2021). Để làm điều này, chúng tôi theo phương pháp tinh chỉnh một mô hình được huấn luyện trước trên D_prog, tương tự như Magister et al. (2022). Với một bài toán từ ngữ toán học x, mục tiêu của chúng tôi là tạo ra chương trình Python tương ứng P. Chúng tôi sử dụng CodeGen có sẵn công khai (Nijkamp et al., 2022) cho tác vụ này vì nó được huấn luyện cụ thể cho việc tạo mã và các mô hình được huấn luyện trước có sẵn³. CodeGen là một mô hình tự hồi quy dựa trên Transformer tiêu chuẩn (Vaswani et al., 2017).

³https://huggingface.co/Salesforce/codegen-16B-mono

## 3 Thí nghiệm

**Tập dữ liệu và Thiết lập Thí nghiệm** Tương tự như Fu et al. (2022), chúng tôi chủ yếu tiến hành thí nghiệm trên các tập dữ liệu GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), và MathQA (Amini et al., 2019). Bảng 1 cho thấy thống kê và số lượng chương trình được chú thích. Các chương trình được chú thích thông qua gợi ý vài mẫu với PAL (Gao et al., 2022) (§2). Chúng tôi thực hiện gợi ý với Codex (code-davinci-002) nơi việc sử dụng API là miễn phí. Theo Gao et al. (2022), chúng tôi đặt số token tối đa cho việc tạo là 600. Tập huấn luyện trong tập dữ liệu SVAMP là dễ nhất vì chúng tôi có thể có được các chương trình pseudo-gold khoảng 98%. Chúng tôi chỉ quản lý để có được chú thích chương trình cho 47.4% các trường hợp cho MathQA, vì nó là thách thức nhất và có nhãn nhiễu (Fu et al., 2022) với các loại vấn đề đa dạng (ví dụ: vật lý, xác suất, hình học, v.v.).

Số lượng lấy mẫu tối đa K cho mỗi trường hợp huấn luyện được đặt là 5⁴, và nhiệt độ T là 0.5 theo thực hành trước đây (Zelikman et al., 2022). Chúng tôi loại bỏ trường hợp huấn luyện nếu không thể tìm thấy chương trình phù hợp. Số lượng gợi ý M được đặt là 8 theo công trình trước đây trong giải quyết bài toán từ ngữ toán học (Gao et al., 2022; Fu et al., 2022; Wei et al., 2022). Trong các thí nghiệm tinh chỉnh, chúng tôi sử dụng mô hình ngôn ngữ CodeGen 6B. Tốc độ học cho các thí nghiệm tinh chỉnh là 2e-5. Chúng tôi tinh chỉnh mô hình CodeGen với kích thước batch 48 và thí nghiệm với 40 epochs trên tất cả các tập dữ liệu. Các thí nghiệm tinh chỉnh được tiến hành với 8 GPU A100. Chúng tôi không thực hiện tìm kiếm siêu tham số cho tinh chỉnh. Tất cả các tham số được đặt theo các giá trị mặc định ở trên.

⁴Chúng tôi chọn K=5 để cân bằng giữa chi phí và hiệu quả. Tăng K có thể không dẫn đến cải thiện đáng kể.

| Mô hình | #Tham số | GSM8K | SVAMP | MathQA |
|---------|----------|-------|-------|--------|
| **Gợi ý** | | | | |
| LaMDA (Thoppilan et al., 2022) | 137B | 17.1 | - | - |
| PaLM (Chowdhery et al., 2022) | 540B | 58.1 | 79.0 | - |
| GPT-3 CoT (text-davinci-002) | 175B | 48.1 | - | - |
| Codex CoT (code-davinci-002) | 175B | 65.6 | 74.8 | 29.9 |
| Complex CoT (Fu et al., 2022) | 175B | 55.4 | - | 36.0 † |
| PAL (Gao et al., 2022) | 175B | 72.0 | 79.4 | - |
| PAL (tái tạo) | 175B | 71.6 | 77.4 | 30.0 |
| Gợi ý Chương trình Động của chúng tôi | 175B | 76.6 | 80.3 | 61.7 |
| **Tinh chỉnh** | | | | |
| GPT-3 | 175B | 33.1 | - | - |
| CoT Fine-tune (Magister et al., 2022) | 11B | 38.2 | - | - |
| CoT Fine-tune (CodeGen) | 6B | 35.3 | 40.2 | 25.3 |
| Chưng cất Chương trình của chúng tôi | 6B | 39.0 | 48.0 | 50.6 |

**Bảng 2:** So sánh hiệu suất với các phương pháp trước đây sử dụng gợi ý và tinh chỉnh. †: không thể so sánh trực tiếp vì họ sử dụng ít dữ liệu kiểm tra hơn.

**Kết quả Chính** Chúng tôi tiến hành cả thí nghiệm gợi ý và tinh chỉnh trên tất cả các tập dữ liệu. Bảng 2 cho thấy so sánh hiệu suất với các phương pháp gợi ý trước đây sử dụng các mô hình ngôn ngữ lớn. Tương tự như PAL (Gao et al., 2022), kết quả của chúng tôi chứng minh rằng các phương pháp dựa trên chương trình đạt được hiệu suất tốt nhất trên tất cả các tập dữ liệu. Phương pháp của chúng tôi, dựa trên cơ chế cơ bản giống như PAL, đạt được hiệu suất tiên tiến mới (tại thời điểm nộp bài) với mô hình 175B và có được nhiều nhất 5 điểm cải thiện tuyệt đối so với PAL. Trên tập dữ liệu dễ nhất, SVAMP, chúng tôi vẫn đạt được cải thiện 0.9 điểm so với baseline hiệu suất tốt nhất và tốt hơn 2.9 điểm so với PAL được tái tạo. Trên tập dữ liệu MathQA, được biết đến với nhiễu, chúng tôi thấy cải thiện đáng kể hơn 20 điểm so với các baseline gợi ý khác. Sự cải thiện đáng kể được quan sát cho thấy rằng việc sử dụng các ví dụ trong bối cảnh tạo điều kiện cho mô hình hiểu các câu hỏi và cho phép nó tạo ra giải pháp dựa trên các gợi ý tương tự. Những kết quả này cho thấy rằng việc truy xuất các ví dụ tương tự là quan trọng, đặc biệt là cho các tập dữ liệu phức tạp.

Ngoài phương pháp gợi ý của chúng tôi, chúng tôi cũng đánh giá hiệu quả của việc tinh chỉnh một mô hình ngôn ngữ nhỏ hơn trên dữ liệu huấn luyện được chú thích, như được hiển thị trong Bảng 2 (phần dưới). Chúng tôi tinh chỉnh mô hình CodeGen 6B trên dữ liệu huấn luyện với các chương trình được chú thích, và phương pháp của chúng tôi đạt được hiệu suất tốt hơn với cải thiện 0.8 điểm so với mô hình T5 11B trên tập dữ liệu GSM8K. Chúng tôi sử dụng phương pháp tương tự như Magister et al. (2022) để thực hiện gợi ý trên tập huấn luyện và có được CoT được chú thích. Đáng chú ý, đối với tập dữ liệu SVAMP, phương pháp tinh chỉnh của chúng tôi với các chương trình vượt trội đáng kể so với tinh chỉnh với CoT ngôn ngữ tự nhiên 7.8 điểm. Trên tập dữ liệu MathQA, được biết đến với nhãn nhiễu, hiệu suất tinh chỉnh của chúng tôi tốt hơn đáng kể so với hiệu suất gợi ý vanilla.

| | GSM8K | SVAMP | MathQA |
|---|-------|-------|--------|
| **Mẫu M Tương tự Nhất** | | | |
| OpenAI | 76.6 | 80.3 | 61.7 |
| SimCSE (Gao et al., 2021) | 76.4 | 80.1 | 61.0 |
| ST5 (Ni et al., 2022) | 76.6 | 79.9 | 61.6 |
| Ngẫu nhiên | 74.4 | 78.1 | 34.0 |
| **Mẫu M Ít tương tự Nhất** | | | |
| OpenAI | 73.5 | 78.2 | 34.1 |
| SimCSE (Gao et al., 2021) | 76.0 | 78.4 | 34.7 |
| ST5 (Ni et al., 2022) | 74.2 | 77.9 | 34.3 |

**Bảng 3:** So sánh hiệu suất giữa các biểu diễn câu khác nhau.

Gợi ý chương trình động đạt được cải thiện hơn 30 điểm so với PAL. So với phương pháp Tinh chỉnh CoT sử dụng CodeGen, phương pháp chưng cất chương trình của chúng tôi cũng tốt hơn 25.3 điểm về độ chính xác. Quan sát này càng làm nổi bật tầm quan trọng của việc tận dụng dữ liệu huấn luyện trong các tập dữ liệu phức tạp. Nói chung, hiệu suất tinh chỉnh với các mô hình nhỏ hơn kém hơn gợi ý vài mẫu với các mô hình ngôn ngữ lớn trên GSM8K và SVAMP, cho thấy rằng chưng cất chương trình có thể không đủ để bù đắp cho các hạn chế khái quát hóa của các mô hình ngôn ngữ nhỏ hơn.

**Chiến lược Truy xuất Gợi ý** Để biện minh thêm cho hiệu quả của việc sử dụng các mẫu tương tự nhất làm gợi ý, chúng tôi tiến hành thí nghiệm với các chiến lược truy xuất gợi ý khác nhau và kết quả được trình bày trong Bảng 3. Chiến lược "Ngẫu nhiên" là chọn ngẫu nhiên M mẫu làm gợi ý. Bảng cho thấy rằng việc sử dụng các embedding câu khác nhau dẫn đến hiệu suất nhất quán khi sử dụng chiến lược "mẫu M tương tự nhất". Tuy nhiên, việc sử dụng "mẫu ít tương tự nhất" liên tục dẫn đến giảm hiệu suất, đặc biệt là trên tập dữ liệu MathQA nơi dữ liệu đánh giá tương tự hơn với dữ liệu huấn luyện (Fu et al., 2022). Hơn nữa, các mẫu ít tương tự nhất không có khả năng bao gồm toàn bộ phổ thông tin cần thiết trong tập dữ liệu MathQA nơi tồn tại một phạm vi kiến thức rộng hơn. Chiến lược "Ngẫu nhiên" cũng cho thấy hiệu suất tương tự như việc sử dụng "mẫu ít tương tự nhất", cho thấy rằng cả hai đều không mang lại lợi ích bổ sung so với việc sử dụng chiến lược "mẫu tương tự nhất".

**Vấn đề:** Trong một lớp khiêu vũ gồm 20 học sinh, 20% ghi danh vào khiêu vũ đương đại, 25% số còn lại ghi danh vào khiêu vũ jazz, và phần còn lại ghi danh vào khiêu vũ hip-hop. Bao nhiêu phần trăm tổng số học sinh ghi danh vào khiêu vũ hip-hop?

```python
def solution():
    students_total = 20
    contemporary_students = students_total * 0.2
    jazz_students = (students_total - contemporary_students) * 0.25
    hip_hop_students = students_total - contemporary_students - jazz_students
    hip_hop_percentage = hip_hop_students / students_total * 100
    result = hip_hop_percentage
    return result
```

**Một phần Các Vấn đề Được Truy xuất:**
1. Có 400 học sinh. 120 học sinh chọn khiêu vũ làm môn tự chọn. 200 học sinh chọn nghệ thuật làm môn tự chọn. Phần còn lại chọn âm nhạc. Bao nhiêu phần trăm học sinh chọn âm nhạc?
2. Vào đêm khiêu vũ, 400 học sinh xuất hiện tại bữa tiệc. 70% học sinh xuất hiện đã được mời. Nếu 40% những người được mời đến bữa tiệc đã bị thu hồi lời mời và không được phép vào bữa tiệc, có bao nhiêu học sinh được mời đã tham dự bữa tiệc?
3. Tỷ lệ nam và nữ tại buổi khiêu vũ là 3:4. Có 60 nữ sinh tại buổi khiêu vũ. Các giáo viên chiếm 20% số nam sinh. Có bao nhiêu người tại buổi khiêu vũ?

**Chương trình Dự đoán**

**Hình 3:** Ví dụ dự đoán bằng phương pháp gợi ý của chúng tôi và các vấn đề được truy xuất tương ứng.

**Phân tích Gợi ý Định tính** Để hiểu rõ hơn về cách các gợi ý ảnh hưởng đến hiệu suất, chúng tôi so sánh kết quả giữa PAL và phương pháp của chúng tôi trên tập dữ liệu GSM8K. Các gợi ý được truy xuất bởi phương pháp của chúng tôi có mức độ chồng lấp từ ngữ cao hơn với câu hỏi. Hình 3 cho thấy một ví dụ về cách phương pháp của chúng tôi giúp đưa ra dự đoán chính xác hơn. Mã "* 100" được đánh dấu màu đỏ là thông tin mà PAL không thể tạo ra. Điều này cho thấy PAL có thể đã không tự tin về "phần trăm" cho câu hỏi này. Các gợi ý của chúng tôi, mặt khác, chứa nhiều câu hỏi liên quan đến "phần trăm" có khả năng giúp mô hình đưa ra dự đoán đúng hơn. Tuy nhiên, chúng tôi cũng lưu ý rằng phương pháp dựa trên độ tương tự không phải lúc nào cũng tốt hơn các gợi ý cố định của PAL. Trên GSM8K, PAL vẫn hoạt động tốt hơn trên 5.5% câu hỏi trong khi phương pháp dựa trên độ tương tự của chúng tôi hoạt động tốt hơn trên 10.3% tất cả câu hỏi. Do đó, các gợi ý dựa trên độ tương tự có thể tạo ra cải thiện tích cực nói chung.

## 4 Công trình Liên quan

Công trình của chúng tôi chủ yếu liên quan đến tài liệu gần đây kết hợp dữ liệu huấn luyện để cải thiện hiệu suất mô hình ngôn ngữ trên các tác vụ downstream. Chung et al. (2022) cho thấy rằng chúng ta có thể được hưởng lợi từ dữ liệu CoT bổ sung cho cả mô hình ngôn ngữ lớn và nhỏ. Li et al. (2022) lấy mẫu đường đi lý luận CoT cho dữ liệu huấn luyện và sử dụng chúng để đa dạng hóa các gợi ý trên tập dữ liệu GSM8K. Thay vào đó, chúng ta có thể sử dụng CoT được lấy mẫu để tinh chỉnh thêm các mô hình ngôn ngữ (Huang et al., 2022; Magister et al., 2022; Meng et al., 2022). Trong thực tế, chúng ta không thể đảm bảo tính đúng đắn của CoT được lấy mẫu, đặc biệt là cho tác vụ giải quyết bài toán từ ngữ toán học, yêu cầu các đường đi lý luận nghiêm ngặt. Các phương pháp gần đây (Magister et al., 2022; Wang et al., 2022b) cố gắng giảm tác động tiêu cực bằng cách khớp câu trả lời với CoT được tạo ra hoặc gán trọng số khác nhau cho các mẫu. Đồng thời với nghiên cứu này, Uesato et al. (2022) đề xuất sử dụng phần thưởng dựa trên bước để cải thiện hiệu suất cụ thể trên GSM8K. Để làm như vậy, các tác giả cần chú thích một phần dữ liệu để huấn luyện mô hình phần thưởng cơ bản. Tuy nhiên, những phương pháp này không thể hoàn toàn tránh khỏi hạn chế cơ bản vì việc đánh giá CoT ngôn ngữ tự nhiên từng bước là thách thức (Golovneva et al., 2022; Prasad et al., 2023). Phương pháp của chúng tôi được truyền cảm hứng từ việc tạo chương trình thông qua gợi ý vài mẫu (Gao et al., 2022), chúng tôi thực hiện gợi ý trên dữ liệu huấn luyện và dễ dàng xác minh tính đúng đắn của câu trả lời bằng cách thực thi chương trình, điều này cho phép chúng tôi có được các chương trình pseudo-gold đáng tin cậy hơn.

## 5 Kết luận và Công việc Tương lai

Được thúc đẩy bởi gợi ý dựa trên chương trình (Gao et al., 2022; Drori et al., 2022), chúng tôi có thể có được chương trình pseudo-gold làm bước lý luận trung gian cho dữ liệu huấn luyện. Sau đó chúng tôi trình bày hai phương pháp để sử dụng dữ liệu như vậy với chú thích chương trình trong cả tình huống gợi ý vài mẫu và tinh chỉnh. Trong gợi ý vài mẫu với LLM, chúng tôi lấy mẫu các mẫu tương tự làm gợi ý cho các thí nghiệm. Trong phương pháp tinh chỉnh, chúng tôi tinh chỉnh trực tiếp một mô hình ngôn ngữ được huấn luyện trước trên dữ liệu được chú thích chương trình. Các thí nghiệm của chúng tôi chứng minh rằng cả gợi ý vài mẫu và tinh chỉnh đều có thể được hưởng lợi đáng kể từ dữ liệu huấn luyện được chú thích với các chương trình, đặc biệt là cho các vấn đề phức tạp trong tập dữ liệu MathQA.

Đối với nghiên cứu tương lai, mục tiêu của chúng tôi là thiết kế một mô hình có cấu trúc tận dụng tiềm năng của dữ liệu với chú thích chương trình, đặc biệt là trong bối cảnh hiệu suất kém đáng kể của các mô hình ngôn ngữ nhỏ hơn. Thú vị là, ngay cả với những hạn chế của chúng, các mô hình có cấu trúc (Jie et al., 2022; Shao et al., 2022) đã thể hiện khả năng vượt trội so với gợi ý mô hình ngôn ngữ lớn trên MathQA. Ngoài ra, sự xuất hiện gần đây của các mô hình tuân theo hướng dẫn (Ouyang et al., 2022; Wang et al., 2022a), được minh họa bởi Alpaca (Taori et al., 2023), đã thúc đẩy sự quan tâm của chúng tôi trong việc trang bị cho các mô hình ngôn ngữ lớn khả năng lý luận toán học (Wang và Lu, 2023) trong khi duy trì tính toàn vẹn của khả năng hiểu ngôn ngữ cơ bản của chúng.

## Hạn chế

Các phương pháp chúng tôi đã sử dụng cho gợi ý và tinh chỉnh đã mang lại những cải thiện đáng chú ý, nhưng một số hạn chế vẫn tồn tại trong các ứng dụng thực tế. Để đạt được hiệu suất tối ưu, chúng tôi tiếp tục phụ thuộc vào gợi ý sử dụng các mô hình ngôn ngữ lớn, điều này tỏ ra tốn kém cho cộng đồng nghiên cứu. Hơn nữa, hiệu quả truy xuất có thể đặt ra thách thức khi xử lý các tập huấn luyện rộng lớn, vì việc xác định M mẫu hàng đầu cho mỗi ví dụ trở nên tốn thời gian hơn. Do đó, việc thiết kế một thuật toán hiệu quả hơn để tăng tốc quá trình truy xuất đại diện cho một lĩnh vực tiềm năng cho khám phá tương lai.

Mặc dù có tiềm năng cải thiện hiệu suất bằng cách lấy mẫu 40 đường đi lý luận cho mỗi câu hỏi như được trình bày bởi Wang et al. (2022a); Fu et al. (2022), chúng tôi không thể kết hợp phương pháp này do hạn chế ngân sách. Ngoài ra, mặc dù dữ liệu huấn luyện đã được chứng minh có lợi, những cải thiện cho các mô hình nhỏ hơn không đủ để vượt qua hiệu suất của các mô hình ngôn ngữ lớn. Quan sát này có thể cho thấy sự cần thiết cho một thiết kế mô hình khác biệt cơ bản hoặc một mô hình được huấn luyện trước vượt trội (ví dụ: Galactica (Taylor et al., 2022) hoặc Code-T5 (Wang et al., 2023)) như một cơ sở hiệu quả hơn cho tinh chỉnh.

## Lời cảm ơn

Chúng tôi muốn cảm ơn các nhà đánh giá ẩn danh, meta-reviewer của chúng tôi, và các chủ tọa khu vực cấp cao vì những nhận xét mang tính xây dựng và sự hỗ trợ cho công trình của chúng tôi. Nghiên cứu/dự án này được hỗ trợ bởi Quỹ Nghiên cứu Quốc gia Singapore và Phòng thí nghiệm Quốc gia DSO trong khuôn khổ Chương trình AI Singapore (Giải thưởng AISG số: AISG2-RP-2020-016), và Bộ Giáo dục, Singapore, trong khuôn khổ Chương trình Tier 3 (Giải thưởng số: MOET32020-0004).

## Tài liệu tham khảo

Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, và Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. Trong Proceedings of NAACL.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, và cộng sự. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, và cộng sự. 2022. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, và cộng sự. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, và cộng sự. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.

Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, và cộng sự. 2022. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 119(32):e2123433119.

Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, và Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, và Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.

Tianyu Gao, Xingcheng Yao, và Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence embeddings. Trong Proceedings of EMNLP.

Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, và Asli Celikyilmaz. 2022. Roscoe: A suite of metrics for scoring step-by-step reasoning. arXiv preprint arXiv:2212.07919.

Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, và Jiawei Han. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610.

Zhanming Jie, Jierui Li, và Wei Lu. 2022. Learning to reason deductively: Math word problem solving as complex relation extraction. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 5944–5955.

Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, và Weizhu Chen. 2022. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336.

Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, và Aliaksei Severyn. 2022. Teaching small language models to reason. arXiv preprint arXiv:2212.08410.

Yu Meng, Jiaxin Huang, Yu Zhang, và Jiawei Han. 2022. Generating training data with language models: Towards zero-shot language understanding. Trong Proceedings of NeuIPS.

Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, và Ashwin Kalyan. 2022. Lila: A unified benchmark for mathematical reasoning. Trong Proceedings of EMNLP.

Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, và cộng sự. 2022. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005.

Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, và Yinfei Yang. 2022. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. Trong Findings of the Association for Computational Linguistics: ACL 2022.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. ArXiv preprint, abs/2203.13474.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, và cộng sự. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744.

Arkil Patel, Satwik Bhattamishra, và Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? Trong Proceedings of NAACL.

Archiki Prasad, Swarnadeep Saha, Xiang Zhou, và Mohit Bansal. 2023. Receval: Evaluating reasoning chains via correctness and informativeness. arXiv preprint arXiv:2304.10703.

Ohad Rubin, Jonathan Herzig, và Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633.

Zhihong Shao, Fei Huang, và Minlie Huang. 2022. Chaining simultaneous thoughts for numerical reasoning. Trong Proceedings of EMNLP.

Disha Shrivastava, Hugo Larochelle, và Daniel Tarlow. 2022. Repository-level prompt generation for large language models of code. arXiv preprint arXiv:2206.12839.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca.

Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, và Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, và cộng sự. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239.

Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, và Irina Higgins. 2022. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong Proceedings of NeurIPS.

Tianduo Wang và Wei Lu. 2023. Learning multi-step reasoning by solving arithmetic tasks. Trong Proceedings of ACL.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, và Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.

Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Awadallah, và Jianfeng Gao. 2022b. List: Lite prompted self-training makes parameter-efficient few-shot learners. Trong Findings of the Association for Computational Linguistics: NAACL 2022, trang 2262–2281.

Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, và Steven CH Hoi. 2023. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, và Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.

Eric Zelikman, Yuhuai Wu, Jesse Mu, và Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Trong Proceedings of NeurIPS.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, và Ed Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625.
