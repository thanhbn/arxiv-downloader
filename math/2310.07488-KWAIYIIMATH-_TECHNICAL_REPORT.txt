# 2310.07488.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/math/2310.07488.pdf
# File size: 996809 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
KWAIYIIMATH: TECHNICAL REPORT
Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong Chen, Zhirui Yang, Shengnan Zhang,
Xue Zheng, Yan Li, Yuliang Liu, Xucheng Ye, Yiqiao Liao, Chao Liao, Bin Chen, Chengru Song,
Junchen Wan†, Zijia Lin, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, Kun Gai
Kuaishou Technology
ABSTRACT
Recent advancements in large language models (LLMs) have demonstrated re-
markable abilities in handling a variety of natural language processing (NLP)
downstream tasks, even on mathematical tasks requiring multi-step reasoning. In
this report, we introduce the KwaiYiiMath which enhances the mathematical rea-
soning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and
Reinforced Learning from Human Feedback (RLHF), including on both English
and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale
Chinese primary school mathematics test set (named KMath), consisting of 188
examples to evaluate the correctness of the problem-solving process generated by
the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-
of-the-art ( SOTA ) performance on GSM8k, CMath, and KMath compared with
the similar size models, respectively.
1 I NTRODUCTION
Recent advances in large language models (LLMs) have revolutionized the natural language pro-
cessing (NLP) landscape Kenton & Toutanova (2019); Brown et al. (2020), where scaling up model
size and the amount of data is one of the key ingredients Rae et al. (2021); Chowdhery et al. (2022);
Anil et al. (2023); Touvron et al. (2023a;b). State-of-the-art models trained on vast amounts of
data with extremely large model sizes, such as ChatGPT OpenAI (2022), GPT-4 OpenAI (2023)
and PaLM2 Anil et al. (2023), have shown unprecedented performance on a wide range of NLP
tasks Brown et al. (2020); Rae et al. (2021); Du et al. (2022); Lewkowycz et al. (2022); Chowdh-
ery et al. (2022); Ouyang et al. (2022); Tay et al. (2022); OpenAI (2022; 2023); Anil et al. (2023);
Touvron et al. (2023b).
Surprisingly, recent progress suggests that LLMs also have the potential to solve reasoning prob-
lems Clark et al. (2020); Talmor et al. (2020); Suzgun et al. (2022); Wei et al. (2022b). Specifically,
LLMs can perform soft deductive reasoning over natural language descriptions with implicit knowl-
edge stored in their parameters Wei et al. (2022b); Kojima et al. (2022); Fu et al. (2022); Shi et al.
(2022); Zhang et al. (2022b); Zhou et al. (2022b); Diao et al. (2023); Shum et al. (2023) or explicit
knowledge in external resources Wang et al. (2022a); Creswell et al. (2022); Zhou et al. (2022a);
Press et al. (2022); Dua et al. (2022); Reppert et al. (2023), and perform step-by-step reasoning just
with a few demonstrations or instructions via chain-of-thought prompting (CoT) Wei et al. (2022b).
In this report, we focus on how to enhance the mathematical reasoning capabilities of LLM through
an alignment process that includes supervised fine-tuning (SFT) and reinforcement learning from
human feedback (RLHF). Specifically, we introduce the KwaiYiiMath which is finetuned with hu-
man alignment techniques from KwaiYiiBase to tackle mathematical problems.
Experimental results show that KwaiYiiMath outperforms many open-source models in similar sizes
by a large margin and is approaching GPT-4 on three mathematical benchmarks including both
English and Chinese, i.e., GSM8k Cobbe et al. (2021), CMath Wei et al. (2023), and a small-scale
in-house dataset KMath.
†Corresponding author: wanjunchen@kuaishou.com.
1KwaiYiiBase is a large language model developed by Kuaishou https://github.com/kwai/KwaiYii/ .
1arXiv:2310.07488v2  [cs.CL]  19 Oct 2023

--- PAGE 2 ---
The structure of this report is as follows: Section 2 provides an overview of related work including
LLM and LLMs’ reasoning ability. Section 3 introduces the methodology of KwaiYiiMath includ-
ing the process of supervised fine-tuning and human preference alignment. Additionally, it also
describes details about the efforts in collecting large amounts of mathematical high-quality training
data. In Section 4, we report the experimental results on two public benchmarks and an in-house
dataset. Section 5 concludes this report and points out the future work of KwaiYiiMath.
2 R ELATED WORK
Large Language Models Nowadays, the advent of LLMs encourages the rethinking of the pos-
sibilities of artificial general intelligence (AGI). A recent report has even argued that GPT-4 might
bean early version of AGI system Bubeck et al. (2023). The success of LLMs consists of three
major aspects, including pre-training (how to pre-train a LLM based on large amounts of unlabelled
data), adaption (how to adapt the LLMs for better interaction with humans) and utilization (how to
use LLMs for solving various downstream tasks) Zhao et al. (2023). By pre-training on large-scale
corpora with language modeling objective, LLMs can acquire essential language understanding and
generation abilities Brown et al. (2020); Chowdhery et al. (2022).2
However, obviously, one of the major issues is the gap between the training objective and users’
objective: users want the model to “understand and follow their instructions” while the LLMs are
designed to predict the next token. To bridge the gap, instruction tuning (IT) and alignment tuning
(AT) are proposed to enhance the capabilities and controllability of LLMs, where IT mainly aims
to unlock the abilities of LLMs while the purpose of AT is to align the behaviors of LLMs with
human preferences. It refers to the process of further fine-tuning pre-trained LLMs on a collection
of formatted instance pairs (i.e., ( instruction ,output )), where instruction andoutput denote the hu-
man instruction and the desired output generated by the LLM that follows the human instruction,
respectively. Specifically, we first need to collect or construct ( instruction ,output ) pairs, including
manually constructed formatted instances Mishra et al. (2021); Victor et al. (2022); Muennighoff
et al. (2022); Wang et al. (2022d); Longpre et al. (2023); Zhou et al. (2023); Conover et al. (2023);
K¨opf et al. (2023), and automatically constructed formatted instances Wei et al. (2021); Bach et al.
(2022); Honovich et al. (2022); Wang et al. (2022c); Xu et al. (2023a;b); Ji et al. (2023). Note
that it has been widely shown that the number of tasks, the quality and diversity of instruction
instances poses an important impact on the performance of LLMs Ouyang et al. (2022); Victor
et al. (2022); Wei et al. (2021); Wang et al. (2022d); Chung et al. (2022); Taori et al. (2023); Zhou
et al. (2023). Then, we use these formatted instances or carefully selected formatted instances to
further fine-tune LLMs in a supervised learning way (also known as SFT) according to different
requirements. For example, we can use formatted instances of different modality, domains and ap-
plications to obtain different specific LLMs, including multimodal LLMs (InstructPix2Pix Brooks
et al. (2023), LLaV A Liu et al. (2023a), Video-LLaMA Zhang et al. (2023a), InstructBLIP Dai
et al. (2023), MultiModal-GPT Gong et al. (2023)), and domain and application specific LLMs (In-
structDial Gupta et al. (2022), LINGUIST Rosenbaum et al. (2022), InstructUIE Wang et al. (2023),
Writing-Alpaca Zhang et al. (2023b), Radiology-GPT Liu et al. (2023b), ChatDoctor Yunxiang et al.
(2023), Goat Liu & Low (2023), WizardCoder Luo et al. (2023b), etc). However, the SFT process
seems not to be stable enough due to the small amount of data and large model size. So, some studies
focus on how to combine instruction tuning and pre-training, mainly consisting of two directions: a
one-stage process (pre-trained from scratch with a mixture of pre-training data and instruction tuning
data) Raffel et al. (2020); Zeng et al. (2022) and two-stage process (fine-tuned with a mixture of pre-
training data and instruction tuning data) Iyer et al. (2022). AT refers to the process of reinforcement
learning from human feedback (RLHF) for better aligning LLMs with human preferences Ouyang
et al. (2022). RLHF mainly comprises three key components: an LLM after SFT, a reward model
learning to reflect human feedback for the text generated by the LLM, and a reinforcement learning
(RL) algorithm (e.g., Proximal Policy Optimization (PPO) Schulman et al. (2017)) to align the LLM
based on the guidance signals generated by the reward model. We train a reward model to predict
the human-preferred output based on the collected human feedback data and align the LLM based
on the reward signals generated by the reward model using RL algorithm.3
2We omit most details in pre-training since KwaiYii Math only concentrates on how to enhance the mathe-
matical reasoning abilities of KwaiYiiBase.
3We omit the details of alignment criteria and human feedback collection for brevity.
2

--- PAGE 3 ---
After pre-training and adaption, LLMs can serve as a general-purpose language task solver (to some
extent) by simply conditioning the models on a few examples (few-shot) or instructions describing
the task (zero-shot). The success of LLMs is often attributed to few-shot (in-context) or zero-shot
learning (i.e., emergent abilities Wei et al. (2022a) that may not be observed in previous smaller
language models).4This leads to the rapid development of the “prompting” technique, revolution-
izing the way that humans develop and use AI algorithms. Thus, designing prompts has become a
hot topic in NLP, including demonstration selection Liu et al. (2021); Rubin et al. (2021); Xie et al.
(2021); Zhang et al. (2022a); Kim et al. (2022); Lee et al. (2022); Levy et al. (2022); Su et al. (2022);
Ye et al. (2022), and demonstration order Liu et al. (2021); Lu et al. (2021).
Reasoning with Large Language Models Reasoning, the process of making inferences based on
existing knowledge, is the core of human intelligence. However, existing LLMs have struggled to
achieve high performance on system-2 tasks requiring slow and multi-step reasoning such as math-
ematical and commonsense reasoning Rae et al. (2021). To enhance the reasoning ability of LLMs
prompting, there are two major directions: strategy-enhanced reasoning and knowledge-enhanced
reasoning Qiao et al. (2022). Strategy-enhanced reasoning refers to design a better reasoning strat-
egy, such as the prompt design in single reasoning stage Wei et al. (2022b); Kojima et al. (2022);
Fu et al. (2022); Shi et al. (2022); Zhang et al. (2022b); Zhou et al. (2022b); Diao et al. (2023);
Shum et al. (2023), reasoning stage by stage Wang et al. (2022a); Creswell et al. (2022); Zhou et al.
(2022a); Press et al. (2022); Dua et al. (2022); Reppert et al. (2023), natural language rationales
optimization Ye & Durrett (2022); Wang et al. (2022c); Huang et al. (2022); Li et al. (2023b); Weng
et al. (2023); Yoran et al. (2023); Shinn et al. (2023); Madaan et al. (2023); Paul et al. (2023), and
external engine augmentation Liu et al. (2022); Chen et al. (2022); Gao et al. (2023); Lyu et al.
(2023); Imani et al. (2023). Knowledge enhanced reasoning refers to prompt LLMs with implicit
knowledge stored in LLM Li et al. (2022); Wang et al. (2022b); Magister et al. (2022); Ho et al.
(2022); Fu et al. (2023) or explicit knowledge in external resources Yang et al. (2022); Su et al.
(2022); Lu et al. (2022); He et al. (2022), such as wiki, the reasoning steps, etc.
3 M ETHOD
In this section, we introduce the details of KwaiYiiMath. Figure 1 shows the overview of our
model. Specifically, the left part of Figure 1 shows the training process overview of KwaiYiiMath,
and it mainly consists of two steps, supervised fine-tuning and human preference alignment. The
right part of Figure 1 shows the main three components, including SFT data collection, human
preference data collection, and human preference alignment training. We first collect high-quality
mathematical instruction data, in the form of <question, answer >pairs, to do the supervised fine-
tuning. Next, for each question, we first generate Kdifferent answers from SFT models and actor
models from reinforcement learning respectively, then classify them into good answers and bad
answers with the help of human annotation. In this way, we can collect a large amount of high-
quality data representing human preference and then use this data to train the reward model and do
human preference alignment training.
3.1 S UPERVISED FINE -TUNING
Previous work has shown that the diversity and quality of instruction data poses an important impact
on the SFT performance Zhou et al. (2023); Peng et al. (2023); Chen et al. (2023), a conclusion that
also holds true in LLMs’ mathematical reasoning ability Yuan et al. (2023); Luo et al. (2023a); Chern
et al. (2023); Yu et al. (2023). Therefore, we focus on how to collect or construct high-diversity and
high-quality instruction data.
To obtain as much diversity as possible in math data, we first collect math data from a wide range
of sources, including different difficulties (e.g., primary school, middle school, and university, etc.),
and different fields of math (e.g., algebra, geometry, and probability, etc.). Then, we generate in-
termediate rationales for math questions only with the final answer or without the answer using
open-source LLMs and ensure the correctness of intermediate rationales and answers through man-
ual annotation. We try to construct intermediate rationales for all mathematical instruction data
since Chain-of-Thought (CoT) Wei et al. (2022b) has been proven effective either in prompting or
4Note that emergent abilities may not occur in some LLMs.
3

--- PAGE 4 ---
Pre-train LM
KwaiYiiBase 13B
Supervised Fine-tuing
human preference alignment
RM + PPO DPO
KwaiYiiMathSupervised Fine-tuing   Data Collection
Pre-Algebra  Algebra Probability
NumTheory Calculus Geometry
...CoT Construction
Human Anonation
Diversity  AugmentationSFT Data
SFT Model
PPOquestion
+
good
answerquestion
+
bad
answerHuman Preference   Data Collection
Human
Preference
Data
Human
Preference
Data
SFT ModelReward
ModelHuman Preference   Alignment
Human
Preference
DataDPOHuman
Preference
DataPPO
RMinstruction alignmentFigure 1: The overview of KwaiYiiMath. The left part is the framework of the model training pro-
cess. The right part shows the details of three main components, including the SFT data collection,
human preference data collection, and human preference alignment training. The data used by RM,
PPO, and DPO are only in the same form and collection method, but their training datasets are dif-
ferent.
instruction data for fine-tuning Ho et al. (2022); Zhu et al. (2022); Li et al. (2023a). In addition to
mathematical data, we also sample 300k open-domain conversations from KwaiYiiChat5training
data to maintain the model’s ability to handle open-domain questions. More details of SFT data
collection are as follows.
3.1.1 D ATA DIVERSITY
We consider the diversity of instruction data mainly from two aspects: the diversity of instructions
and responses, respectively. Figure 2 illustrates the construction process of SFT data with an exam-
ple.
Instruction Diversity Inspired by Evol-Instruct Xu et al. (2023a); Luo et al. (2023b) which uses
LLM instead of humans to generate diverse instructions through a manual designed set of evolution-
ary actions, we designed Dual-evolving actions in depth and Constrained-mutation evolving actions
in breadth for math instruction. Nearing completion of our work, the authors of Evol-Instruct Xu
et al. (2023a); Luo et al. (2023b) had also adapted their idea to the math LLM and further pro-
posed Reinforcement Learning from Evol-Instruct Feedback Luo et al. (2023a) which combines
Evol-Instruct with reinforcement learning. However, the Evol-Instruct actions we use are somewhat
different. Specifically, we consider a logically complex mathematical problem Qis constructed by a
series of simple sub-problems [q1, q2, . . . , q n]. The original problem Qnaturally has a CoT relation
within sub-problems {qi}i=1...n. The math-solving process can be considered as the CoT process
of these sub-problems. Therefore, how to retain the CoT process in Evol-Instruct is a matter. Evol-
Instruct mentioned in Luo et al. (2023a) evolves original instruction QtoQ′, which is a one-to-one
paradigm no matter Downward evolution or Upward evolution action and will hurt the CoT process
ofQwhen making Downward evolution especially. We propose Dual-evolving actions in depth to
retain the CoT process and expand instruction diversity in the meantime. Specifically, Dual-evolving
actions include two steps:
• First we design prompts to decompose a mathematical problem Qinto multiple sub-
problems [q1, q2, . . . , q n]. Each sub-problem is individual from each other, and the CoT
5KwaiYiiChat is the finetuned model from KwaiYiiBase for open-domain conversations.
4

--- PAGE 5 ---
(a) Instruction Diversity Augmentation
(b) Response Diversity AugmentationLet's think step by step\nStep 1: Joe {...}, so Harry has 4 * 56 =
224 fish.\n\nThe answer is 224.In-Breadth EvolvingIn-Depth Evolving
Few-shot CoT
Zero-shot CoT
Zero-shot
......Sam has 7 fish, and Joe has 8 times as many fish as Sam.
How many fish does Joe have?
Of the 4 friends, Emily has 5 times as many birds as
Jane, and Jane has 6 times as many birds as Karen does.
If Karen has 3 birds, how many birds does Emily have?
......Question: Of the 3 friends, Harry has 4 times as many
fish as Joe, and Joe has 8 times as many fish as Sam
does. If Sam has 7 fish, how many fish does Harry have?
Answer:  Joe has 8*7 = <<8*7=56>>56 fish.\nHarry has
4*56 = <<4*56=224>>224 fish.\n#### 224
Joe has 8 * 7 = 50  fish.\nHarry has 4 * 50 = 220  fish.
\nThe answer is 224.
Joe has 8 * 7 = 56 fish.\nHarry has 4 * 56 = 224 fish.
\nThe answer is 224.To find out how many fish Harry has, {...} Harry has 4
times as many fish as Joe, Harry has 4 * 56 = 224 fish.
Wrong answer or
calculation
Figure 2: An example of SFT data diversity augmentation. We chose a sample from the train set of
GSM8k. The upper part shows the instruction diversity augmentation process. There are two new
instructions that are generated through the in-depth evolving and in-breadth evolving, respectively.
The bottom part shows the response diversity augmentation process. There are four new responses
that are generated through diverse reasoning path construction and one of them is filtered out due to
wrong calculation, even the final answer is correct.
process consists of the order of sub-problems, which is a one-to-many paradigm of Down-
ward evolution.
• Second we design prompts to increasingly enhance the difficulty of sub-problems and fur-
ther improve the diversity of instructions. Since our method consists of two steps, we call
it a Dual-evolving action.
In the in-breadth evolving, we use a constrained-evolving action that is inspired by the mutation
evolution Xu et al. (2023a). Specifically, We designed a prompt to evolve new problems based on
existing problems within a constrained scope. The purpose of adding scope constraints is to avoid
evolving actions that lead to unsolvable mathematical problems.
Response Diversity Previous work Ho et al. (2022); Yuan et al. (2023) has shown that diverse
reasoning paths can improve the reasoning performance of LLM, including: for a given instruction
sample in a training set, using multiple different models or sampling strategies to generate multiple
reasoning paths.
Inspired by that, we collect various available LLMs including open-source LLMs such as
Llama Touvron et al. (2023a;b) of different sizes, etc and different versions of KwaiYiiMath. Then,
we use these models to generate multiple reasoning paths given each question. Specifically, we fine-
tune open-source LLMs on mathematical datasets in a supervised fashion to obtain the ability to
generate more correct reasoning paths.6To further augment such abilities of LLMs and improve the
diversity of reasoning paths, we collect diverse CoT prompts,7and use different prompting strategies
such as zero-shot, zero-shot CoT and few-shot CoT. For each question qi, we generate kcandidate
reasoning paths and answers r,awith a temperature of 0.7 following Cobbe et al. (2021) and one
of prompting strategies based on different prompts. We first filter out reasoning paths with wrong
answers a̸=aior wrong calculations where equations are extracted from reasoning paths.8We re-
tain all reasoning paths with the same equation list as the augmented data unlike Yuan et al. (2023)
6Different versions of KwaiYiiMath are already fine-tuned on mathematical datasets in a supervised fashion.
7https://github.com/FranxYao/chain-of-thought-hub/tree/main/gsm8k/lib prompt
8Note that we only evaluate the correctness of extracted complete equation (e.g., 3 +4=7) instead of
incomplete equation (e.g., +4=7).
5

--- PAGE 6 ---
since we argue that diverse contexts also pose an important impact on the reasoning performance of
LLMs.
3.1.2 D ATA QUALITY
The most important thing for mathematical data is the correctness of the calculation process and the
final answers of the response. We denote D={qi, pi, ai}iis the mixed datasets, where qi, pi=
{e1, e2, . . . , e n,ˆai}are a question and a reasoning path respectively, {ei}idenote equation set,
ˆaiis the final answer, and aidenote the ground truth reasoning answer of qi. LLMs often make
calculation or conclusion mistakes in the reasoning path Gao et al. (2023); Chen et al. (2022) such
aseval(ei) =False orˆai̸=ai.
In order to attain high-quality data, we make an effort to ensure the correctness of both the final
answers and calculation processes. Specifically, we first use LLM-generated responses to extract
the final answers ˆaiand then filter out reasoning paths pjwith wrong answers pj:{ˆaj̸=aj}.
Then, we use the regular expression to extract the equation set {ei}iin response and utilize a Python
interpreter to evaluate the correctness of the response. As for the single true answer queries, we
control the both correctness of the final answer and the calculation process. As for the multiple true
answer queries, we only control the correctness of the calculation process.
3.2 H UMAN PREFERENCES ALIGNMENT
Despite the significant performance of fine-tuned LLMs on mathematical reasoning abilities, they
are still prone to generate content that contains reasoning errors, incorrect answers, or redundant
inference processes. We argue that the performance enhancement of LLMs not only derives from
supervised fine-tuning but also from human preferences alignment methods. Therefore, we use
two scalable alignment frameworks, reinforcement learning from human feedback (RLHF) and
DPO Rafailov et al. (2023), both are learning from human preferences for training aligned language
models and improving mathematical reasoning abilities and answer correctness.
3.2.1 R EINFORCEMENT LEARNING FROM HUMAN FEEDBACK
RLHF is to apply reinforcement learning directly on LLMs with human preferences as feedback, and
developing rapidly in the language models alignment field. Inspired by InstructGPT, we propose a
classic RLHF training pipeline that consists of two phases: 1) human preferences comparison data
collection and reward model training; 2) reinforcement learning with PPO.
Reward Model : The reward model is used to evaluate the quality of the SFT generation from the
aspect of mathematical result and procedure, as well as human preference.
Given an input, we sample a pair of responses from our SFT and PPO of different versions so that
RM can capture diverse data distribution. Some existing open-source preference datasets are also
combined to improve the generalization of RM, such as Anthropic Helpful and Harmless Bai et al.
(2022), OpenAI WebGPT Nakano et al. (2021).
The binary ranking loss function we use is consistent with Ouyang et al. (2022). We held out 5,000
examples as a test set to evaluate our model. The results are reported in Table 1. As a reference
point, we also evaluate other publicly available alternative solutions as baselines: the Open Assistant
reward model based on DeBERTa V3 Large He et al. (2020), and SteamSHP-XL Ethayarajh et al.
(2022) based on FLAN-T5-xl.
PPO : PPO is the most famous reinforcement learning method used in RLHF. It utilizes scores from
reward models as human feedback signals to fine-tune LLMs. By randomly sampling prompts from
SFT datasets and using policy-generated responses, we aim to enable fine-tuned models aligned with
human values. We also find it is crucial to whiten reward scores because of the reward hacking issue
and increase the training stability.
3.2.2 D IRECT PREFERENCE OPTIMIZATION (DPO)
RLHF methods need a reward model to fit with human preferences, then optimize language models
with reinforcement learning to generate high reward score answers. However, they require a large
6

--- PAGE 7 ---
consumption of computational resources and complex large-scale distributed settings. The number
of hypermeters in RLHF methods also increases the difficulty of the stability of optimizing results.
Direct Preference Optimization (DPO) is an RL-free method and easy to implement. By eliminating
the reward model, DPO only considers the policy model and reference model probability distri-
butions and uses a simple binary cross-entropy objective to optimize language models from human
preferences. DPO training data has a consistent pattern with the reward model data used for training.
To effectively improve the mathematical reasoning abilities of KwaiYiiMath, we sample a subset of
reward model data and conduct DPO training on the fine-tuned model.
4 E XPERIMENTS
We mainly evaluate KwaiYiiMath on three comprehensive and realistic benchmarks for measuring
mathematical reasoning ability, including two public benchmarks (GSM8k Cobbe et al. (2021) and
CMath Wei et al. (2023)), and an in-house dataset (KMath).
4.1 E VALUATION DATASETS
Table 1: Results on our Chinese and English test set of hu-
man preference benchmarks.
RM Model KwaiYiiMath-en KwaiYiiMath-zh
Open Assistant 63.79 65.80
SteamSHP-XL 55.90 54.43
KwaiYiiMath-RM 77.30 (+13.51) 78.48 (+12.68)GSM8k Cobbe et al. (2021) con-
tains 7,473 training examples and
1,319 test examples, mainly on grade
school-level English math problems.
Each question consists of basic arith-
metic operations (addition, subtrac-
tion, multiplication, and division),
and generally requires 2 to 8 reason-
ing steps to solve.
CMath Wei et al. (2023) is a Chinese elementary school math word problems dataset that comprises
1.7k9elementary school-level math word problems with detailed annotations, sourced from actual
Chinese workbooks and exams. CMath also has fine-grained annotations, including grade, number
of reasoning steps, digits, and distractors. These annotations can be used to evaluate the LLM’s
fine-grained mathematical reasoning ability and robustuess mathematical reasoning ability.
KMath is a small-scale in-house mathematical dataset that contains 188 Chinese math questions.
The questions of KMath are mainly on the grade school level, which consists of algebra, calculus,
geometry, and probability.
4.2 B ASELINES
The baseline models compared in our experiments can be divided into two categories: closed-source
models and open-source models.
Closed-source models Many technology companies have trained LLMs with strong abilities in
many downstream tasks, but for some reason do not release their model weights and these models
are referred to as closed-source models. In our experiments, we consider five closed-source LLMs
including GPT-4 OpenAI (2023), ChatGPT OpenAI (2022), Ernie Bot10, Minerva Zhu et al. (2022)
and MATH-QWEN-CHAT Bai et al. (2023).
Open-source models There are also many teams that open-source the LLMs they trained, and we
can directly download the model weights through open-source communities. These open-source
models also demonstrate excellent capabilities in many downstream tasks. In our experiments,
we select LLaMa1&2 Touvron et al. (2023a;b), BaiChuan1&2 Baichuan (2023), ChatGLM2-6B11,
QWen12, WizardMathLuo et al. (2023a), GAIRMath-Abel Chern et al. (2023) ,and MetaMath Yu
et al. (2023) as the open-source baselines.
9The CMath data set contains a total of 1.7k data, of which 960 are currently available for download.
10https://yiyan.baidu.com/
11https://github.com/THUDM/ChatGLM2-6B
12https://github.com/QwenLM/Qwen-7B
7

--- PAGE 8 ---
Table 2: Results of pass@1 (%) on GSM8k, CMath and KMath. The character ∗denotes that
results are attained from the related works, and the remaining results are attained from our tests.
The character †shows our best result from different human preferences alignment experiments.
Model #Params GSM8K CMath KMath
Closed-source models
GPT-4 OpenAI (2023) - 92.00∗86.00 75.00
ChatGPT OpenAI (2022) - 74.90∗73.83 59.57
Minerva Lewkowycz et al. (2022)8B 16.20∗- -
62B 52.40∗- -
540B 58.80∗- -
Ernie Bot Baidu (2023) - 56.23 84.33 72.87
MATH-QWEN-CHAT Bai et al. (2023)7B 62.50∗- -
14B 69.80∗- -
Open-source models
LLaMA-1 Touvron et al. (2023a)13B 17.80∗- -
33B 35.60∗- -
LLaMA-2 Touvron et al. (2023b)13B 28.70∗- -
34B 44.20∗- -
BaiChuan1 Baichuan (2023)7B 9.17∗- -
13B 26.76∗51.33 28.19
BaiChuan2 Baichuan (2023)7B 24.49∗- -
13B 52.77∗- -
WizardMath Luo et al. (2023a)13B 63.90∗50.83 23.40
70B 81.60∗- -
ChatGLM2 Zeng et al. (2022) 6B 29.20∗68.36 50.00
QWen Bai et al. (2023) 7B 51.60∗63.16 44.15
GAIRMath-Abel Chern et al. (2023)7B 59.74∗- -
13B 66.41∗- -
70B 83.62∗- -
MetaMath Yu et al. (2023)7B 66.50∗- -
13B 72.30∗- -
70B 82.30∗- -
KwaiYiiMath 13B 72.33 85.33 73.40
KwaiYiiMath-HPA†13B 73.31 85.83 74.47
4.3 T RAINING AND EVALUATION SETTINGS
Training The meta prompt used in training of KwaiYiiMath is the version from Vicuna Chiang
et al. (2023): A chat between a curious user and an artificial intelligence assistant. The assistant
gives helpful, detailed, and polite answers to the user’s questions. USER: {instruction }. ASSIS-
TANT: {response }.
We follow standard fine-tuning hyperparameters: 3 epochs using AdamW Loshchilov & Hutter
(2017) with β1= 0.9,β2= 0.95. Without warmup steps, we set the initial learning rate to 4e−5
and use the cosine learning rate decay strategy. The global batch size is set to 1024 examples and
texts longer than 2048 tokens are trimmed.
Evaluation For evaluation on GSM8k Cobbe et al. (2021), we generate responses using the greedy
decoding strategy. For each sample in CMath Wei et al. (2023) and KMath, we generate a single
8

--- PAGE 9 ---
1 2 3 4 5 6
Grade0.00.10.20.30.40.50.60.70.80.91.0Accuracy
KwaiYiiMath-13B
ChatGLM2-6B
Qwen 7B
Baichuan-13BWizardMath-13B
GPT-4∗
ChatGPT∗
ChatGLM-6B∗Ziya-LLaMA-13B∗
RWKV-7B∗
Chinese-Alpaca-33B∗
MOSS-16B∗(a)
1 2 3 4 5
Number of Reasoning Steps0.00.10.20.30.40.50.60.70.80.91.0Accuracy
KwaiYiiMath-13B
ChatGLM2-6B
Qwen 7B
Baichuan-13BWizardMath-13B
GPT-4∗
ChatGPT∗
ChatGLM-6B∗Ziya-LLaMA-13B∗
RWKV-7B∗
Chinese-Alpaca-33B∗
MOSS-16B∗ (b)
1 2 3 4 5
Number of Digits0.00.10.20.30.40.50.60.70.80.91.0Accuracy
KwaiYiiMath-13B
ChatGLM2-6B
Qwen 7B
Baichuan-13BWizardMath-13B
GPT-4∗
ChatGPT∗
ChatGLM-6B∗Ziya-LLaMA-13B∗
RWKV-7B∗
Chinese-Alpaca-33B∗
MOSS-16B∗
(c)
0 1 2 3 4 5
Num. Distractors0.00.10.20.30.40.50.60.70.80.91.0Accuracy
KwaiYiiMath-13B
ChatGLM2-6B
Qwen 7BBaichuan-13B
WizardMath-13B
GPT-4∗ChatGPT∗
Ziya-LLaMA-13B∗ (d)
Figure 3: (a) (b) (c): Average test accuracy against one of the problem complexity measures, in-
cluding grade, number of reasoning steps, and number of digits for each LLM. (d): Average test
accuracy against the number of distractors on the distractor dataset. The character ∗denotes that test
accuracy is attained from paper Wei et al. (2023)
response from each baseline model using nucleus sampling Holtzman et al. (2019) with topp= 0.9
and a temperature of τ= 0.7. We apply a repetition penalty of previously generated tokens with a
hyperparameter of 1.01Keskar et al. (2019). We limit the maximum token length of output to 2048.
Although all the compared models can generate the intermediate CoT process and final answer, we
evaluate all LLMs on GSM8k Cobbe et al. (2021) using few-shot CoT from Wei et al. (2022b) for
a fair comparison. We evaluate a solution as correct if the final answer matches the ground truth
solution, independent of the quality of the CoT preceding it. To evaluate correctness, we parse the
final answers and compare them using the SymPy library Meurer et al. (2017). For CMath Wei
et al. (2023), we use the code that is released with the data to evaluate the accuracy which only con-
siders the correctness of the final answer. For KMath, in order to evaluate the model results more
comprehensively, we not only evaluate the correctness of the answers but also the correctness of the
problem-solving process. Specifically, we evaluate a solution as correct through human annotation
if the answer is correct and the CoT problem-solving process is basically correct. In order to elim-
inate the bias of human annotation, the correctness of each sample is first labeled by three different
annotators, and then another quality assessment expert checks the labels. The baseline models are
evaluated in a zero-shot way since they have been aligned through SFT both for the CMath Wei et al.
(2023) and KMath.
4.4 M AINRESULTS
9

--- PAGE 10 ---
KwaiYiiMath_13B Chatglm2_6B RFT_7B Abel_13B Baichuan_13B WizardMath_13B0102030405060708090T est Accuracy(\%)GSM8k GSM8k_RobustFigure 4: Comparing the accuracy of the origin
GSM8k and GSM8k Robust.Results on three datasets are shown in Table 2.
We observe that KwaiYiiMath outperforms the
same size baseline LLMs on all benchmarks
and also surpasses closed-source LLMs in-
cluding ChatGPT and GPT4 on the KMath
dataset, showing that finetuning on diverse and
high-quality data is effective. On the CMath
dataset, KwaiYiiMath is close to GPT4 and
also achieves a large improvement over other
baseline models. It shows that the KwaiYi-
iMath is not only effective on English math-
ematics problems but also on Chinese mathe-
matics problems. Meanwhile, KwaiYiiMath-
HPA achieves improvement over KwaiYiiMath
on three benchmarks, showing the effectiveness
of the human preference alignment process.
4.5 F INE-GRAINED AND ROBUSTNESS RESULTS
In this subsection, we investigate the performance of the LLMs on mathematical problems of varying
complexity.
Fine-grained Results The CMath Wei et al. (2023) dataset provides the primary school grade cor-
responding to the question, which can indicate the comprehensive complexity of the question. In
addition, two dimensions are provided to more intuitively represent the complexity of the question,
namely the number of digits that an LLM needs to manipulate, and the number of reasoning steps
that an LLM needs to carry out in order to solve a problem. Intuitively, problems with higher arith-
metic complexity or reasoning complexity should be harder to solve, resulting in lower accuracy.
In Figure 3a, a distinct downward trend in accuracy is evident, signifying that the performance of all
models declines as the complexity increases. GPT-4 and KwaiYiiMath are the only two models that
achieve success (accuracy exceeding 60%) in math tests across all six elementary school grades and
achieve high accuracy (exceeding 80%) in tests for grades 1 to 4. The performance of KwaiYiiMath
is very close to GPT-4, and even slightly outperforms GPT-4 in some grades. Following GPT-4 and
KwaiYiiMath, ChatGPT, ChatGLM2-6b, and Qwen-7B demonstrate success in tests for grades 1 to
4, but encounter difficulties in grades 5 and 6 (accuracy under 60%).
From Figure 3b and 3c, it can be found that all models’ performance declines as either of the problem
complexity measures augments. Judged from the downward slopes of the plots, it is pertinent to
say that the reasoning complexity of the problem has generally a larger impact than the arithmetic
complexity.
Robustness Results The robustness experiment consists of two parts: adversarial evaluation on
the GSM8k robust dataset and the CMath distractor dataset. The GSM8k robust dataset is a dataset
released by Chern et al. (2023) that was established based on the GSM8k dataset. Chern et al. (2023)
randomly modified the numbers within the questions of the GSM8k test set, without altering any
other information in the questions, using GPT-4. The GSM8k robust dataset can be used to evaluate
whether the models overfit the training data, making the models susceptible to out-of-distribution
testing samples. Fiture 4 shows the performance of KwaiYiiMath on GSM8k robust dataset13. It
can be found that there is a slight decrease in the performance of KwaiYiiMath, which demonstrates
that the KwaiYiiMath also has strong robustness out-of-distribution testing samples.
The CMath distractor dataset is a small set released with CMath Wei et al. (2023) for testing the
robustness of the model to irrelevant information in the question. The CMath distractor dataset con-
tains 360 questions: 60 seed questions and 5 noisy versions of each seed question with varying num-
bers of distractors, from 1 to 5. The performance of models on the Cmath distractor dataset is plotted
in Figure 3d. From the figure, it can be observed that we observe that the performance of all LLMs,
with the exception of GPT-4, drops drastically as the number of distractors increases. KwaiYiiMath
13The results of RFT, Abel, and WizardMath are attained from Chern et al. (2023)
10

--- PAGE 11 ---
shows a slight ability of robustness, with the overall effect ranking second among all models, and the
accuracy on the 5 distractors subset is still greater than 40%. However, KwaiYiiMath also suffers
an accuracy drop of nearly 50% for problems augmented with merely three distractors. We guess
the performance decrease is because the training data used by the KwaiYiiMath are relatively clean
questions, that is, there is no distractor information in the questions. Therefore, when distractor
information appears in the question, especially information that is very similar to the origin question
to be solved, the model will generate many useless steps to solve the problem, resulting in the final
answer being wrong. This is also an important ability that needs to be improved in the future.
5 C ONCLUSIONS
In this report, we introduce the KwaiYiiMath which is fine-tuned from KwaiYiiBase to tackle math-
ematical problems. By utilizing a large amount of high-quality mathematical data to perform the
human alignment process, including SFT and RLHF, we have greatly enhanced the mathematical
reasoning capabilities of KwaiYiiMath. Experimental results also show that KwaiYiiMath outper-
forms many open-source models in similar sizes by a large margin and is approaching GPT-4 on
three mathematical benchmarks including English and Chinese. Experimental results on relevant
robust data sets also show that the KwaiYiiMath is robust to the disturbed information in question.
Although our work has achieved results close to GPT-4 on relevant benchmarks, in fact, there is
still a big gap in a wider range of tasks. In the future, we hope to further explore the methods to
improve the mathematical reasoning capabilities of LLMs and the intrinsic mechanism behind data
augmentation for LLMs.
REFERENCES
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.
arXiv preprint arXiv:2305.10403 , 2023.
Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak,
Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. Promptsource: An in-
tegrated development environment and repository for natural language prompts. arXiv preprint
arXiv:2202.01279 , 2022.
Jinze Bai et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023. URL https:
//arxiv.org/abs/2309.16609 .
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless
assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 ,
2022.
Baichuan. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305 , 2023.
URL https://arxiv.org/abs/2309.10305 .
Baidu. Ernie bot, March 2023. URL https://yiyan.baidu.com/ .
Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image
editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 18392–18402, 2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-
mar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general
intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023.
11

--- PAGE 12 ---
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay
Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data.
arXiv preprint arXiv:2307.08701 , 2023.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt-
ing: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588 , 2022.
Ethan Chern, Haoyang Zou, Xuefeng Li, Jiewen Hu, Kehua Feng, Junlong Li, and Pengfei Liu.
Generative ai for math: Abel, 2023. URL https://github.com/GAIR-NLP/abel .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:
An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/ .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod-
els.arXiv preprint arXiv:2210.11416 , 2022.
Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language.
arXiv preprint arXiv:2002.05867 , 2020.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.
Mike Conover, Matt Hayes, Ankit Mathur, Xiangrui Meng, Jianwei Xie, Jun Wan, Sam Shah, Ali
Ghodsi, Patrick Wendell, Matei Zaharia, et al. Free dolly: Introducing the world’s first truly open
instruction-tuned llm, 2023.
Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large lan-
guage models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712 , 2022.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language
models with instruction tuning. arXiv , 2023.
Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. Active prompting with chain-of-thought
for large language models. arXiv preprint arXiv:2302.12246 , 2023.
Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim
Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language
models with mixture-of-experts. In International Conference on Machine Learning , pp. 5547–
5569. PMLR, 2022.
Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. Successive prompting for decom-
posing complex questions. arXiv preprint arXiv:2212.04092 , 2022.
Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with \V-
usable information. In International Conference on Machine Learning , pp. 5988–6008. PMLR,
2022.
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting
for multi-step reasoning. arXiv preprint arXiv:2210.00720 , 2022.
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language
models towards multi-step reasoning. arXiv preprint arXiv:2301.12726 , 2023.
12

--- PAGE 13 ---
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and
Graham Neubig. Pal: Program-aided language models. In International Conference on Machine
Learning , pp. 10764–10799. PMLR, 2023.
Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu,
Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for
dialogue with humans. arXiv preprint arXiv:2305.04790 , 2023.
Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, and Jeffrey P Bigham.
Instructdial: improving zero and few-shot generalization in dialogue through instruction tuning.
InProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing ,
pp. 505–525, 2022.
Hangfeng He, Hongming Zhang, and Dan Roth. Rethinking with retrieval: Faithful large language
model inference. arXiv preprint arXiv:2301.00303 , 2022.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert
with disentangled attention. arXiv preprint arXiv:2006.03654 , 2020.
Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers.
arXiv preprint arXiv:2212.10071 , 2022.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. arXiv preprint arXiv:1904.09751 , 2019.
Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning
language models with (almost) no human labor. arXiv preprint arXiv:2212.09689 , 2022.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei
Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610 , 2022.
Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large
language models. arXiv preprint arXiv:2303.05398 , 2023.
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu,
Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model
instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 ,
2022.
Yunjie Ji, Yan Gong, Yong Deng, Yiping Peng, Qiang Niu, Baochang Ma, and Xiangang Li. Towards
better instruction following language models for chinese: Investigating the impact of training data
and evaluation. arXiv preprint arXiv:2304.07854 , 2023.
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of naacL-HLT , volume 1,
pp. 2, 2019.
Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.
Ctrl: A conditional transformer language model for controllable generation. arXiv preprint
arXiv:1909.05858 , 2019.
Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee.
Self-generated in-context learning: Leveraging auto-regressive language models as a demonstra-
tion generator. arXiv preprint arXiv:2206.08082 , 2022.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. Advances in neural information processing systems ,
35:22199–22213, 2022.
Andreas K ¨opf, Yannic Kilcher, Dimitri von R ¨utte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich ´ard Nagyfi, et al. Openassistant
conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327 ,
2023.
13

--- PAGE 14 ---
Young-Jun Lee, Chae-Gyun Lim, and Ho-Jin Choi. Does gpt-3 generate empathetic dialogues? a
novel in-context example selection method and automatic evaluation metric for empathetic dia-
logue generation. In Proceedings of the 29th International Conference on Computational Lin-
guistics , pp. 669–683, 2022.
Itay Levy, Ben Bogin, and Jonathan Berant. Diverse demonstrations improve in-context composi-
tional generalization. arXiv preprint arXiv:2212.06800 , 2022.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-
masesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative
reasoning problems with language models. Advances in Neural Information Processing Systems ,
35:3843–3857, 2022.
Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. Sym-
bolic chain-of-thought distillation: Small models can also” think” step-by-step. arXiv preprint
arXiv:2306.14050 , 2023a.
Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing
Qian, Baolin Peng, Yi Mao, et al. Explanations from large language models make small reasoners
better. arXiv preprint arXiv:2210.06726 , 2022.
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making
language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 5315–
5333, 2023b.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprint arXiv:2304.08485 , 2023a.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What
makes good in-context examples for gpt- 3?arXiv preprint arXiv:2101.06804 , 2021.
Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush V osoughi, Claire Cui, Denny Zhou,
and Andrew M Dai. Mind’s eye: Grounded language model reasoning through simulation. arXiv
preprint arXiv:2210.05359 , 2022.
Tiedong Liu and Bryan Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic
tasks. arXiv preprint arXiv:2305.14201 , 2023.
Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu,
Cheng Chen, Sekeun Kim, et al. Radiology-gpt: A large language model for radiology. arXiv
preprint arXiv:2306.08666 , 2023b.
Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective
instruction tuning. arXiv preprint arXiv:2301.13688 , 2023.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 , 2017.
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter
Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured
mathematical reasoning. arXiv preprint arXiv:2209.14610 , 2022.
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered
prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint
arXiv:2104.08786 , 2021.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qing-
wei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning
for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583 , 2023a.
14

--- PAGE 15 ---
Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing
Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with
evol-instruct. arXiv preprint arXiv:2306.08568 , 2023b.
Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki,
and Chris Callison-Burch. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379 ,
2023.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement
with self-feedback. arXiv preprint arXiv:2303.17651 , 2023.
Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn.
Teaching small language models to reason. arXiv preprint arXiv:2212.08410 , 2022.
Aaron Meurer, Christopher P Smith, Mateusz Paprocki, Ond ˇrejˇCert´ık, Sergey B Kirpichev, Matthew
Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K Moore, Sartaj Singh, et al. Sympy: symbolic
computing in python. PeerJ Computer Science , 3:e103, 2017.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization
via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773 , 2021.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le
Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual gen-
eralization through multitask finetuning. arXiv preprint arXiv:2211.01786 , 2022.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-
pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
question-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021.
OpenAI. Openai: Introducing chatgpt. 2022.
OpenAI. Gpt-4 technical report. 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in Neural Information Processing Systems , 35:
27730–27744, 2022.
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West,
and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint
arXiv:2304.01904 , 2023.
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning
with gpt-4. arXiv preprint arXiv:2304.03277 , 2023.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring
and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350 ,
2022.
Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei
Huang, and Huajun Chen. Reasoning with language model prompting: A survey. arXiv preprint
arXiv:2212.09597 , 2022.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 , 2021.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv
preprint arXiv:2305.18290 , 2023.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.
15

--- PAGE 16 ---
Justin Reppert, Ben Rachbach, Charlie George, Luke Stebbing Jungwon Byun, Maggie Appleton,
and Andreas Stuhlm ¨uller. Iterated decomposition: Improving science q&a by supervising reason-
ing processes. arXiv preprint arXiv:2301.01751 , 2023.
Andy Rosenbaum, Saleh Soltan, Wael Hamza, Yannick Versley, and Markus Boese. Linguist: Lan-
guage model instruction tuning to generate annotated utterances for intent classification and slot
tagging. arXiv preprint arXiv:2209.09900 , 2022.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context
learning. arXiv preprint arXiv:2112.08633 , 2021.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush V osoughi,
Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multi-
lingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057 , 2022.
Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic
memory and self-reflection. arXiv preprint arXiv:2303.11366 , 2023.
KaShun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with
chain-of-thought from labeled data. arXiv preprint arXiv:2302.12822 , 2023.
Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari
Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models
better few-shot learners. arXiv preprint arXiv:2209.01975 , 2022.
Mirac Suzgun, Nathan Scales, Nathanael Sch ¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks
and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261 , 2022.
Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. Leap-of-thought:
Teaching pre-trained models to systematically reason over implicit knowledge. Advances in Neu-
ral Information Processing Systems , 33:20227–20237, 2020.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.
Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won
Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning
paradigms. In The Eleventh International Conference on Learning Representations , 2022.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b.
Sanh Victor, Webson Albert, Raffel Colin, Bach Stephen, Sutawika Lintang, Alyafeai Zaid, Chaffin
Antoine, Stiegler Arnaud, Raja Arun, Dey Manan, et al. Multitask prompted training enables
zero-shot task generalization. In International Conference on Learning Representations , 2022.
Boshi Wang, Xiang Deng, and Huan Sun. Iteratively prompt pre-trained language models for chain
of thought. arXiv preprint arXiv:2203.08383 , 2022a.
Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren. Pinto: Faithful language
reasoning using prompt-generated rationales. arXiv preprint arXiv:2211.01562 , 2022b.
Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang, Rui Zheng, Junjie
Ye, Qi Zhang, Tao Gui, et al. Instructuie: Multi-task instruction tuning for unified information
extraction. arXiv preprint arXiv:2304.08085 , 2023.
16

--- PAGE 17 ---
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.
arXiv preprint arXiv:2212.10560 , 2022c.
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, An-
jana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al.
Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv
preprint arXiv:2204.07705 , 2022d.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint
arXiv:2109.01652 , 2021.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language
models. arXiv preprint arXiv:2206.07682 , 2022a.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
Neural Information Processing Systems , 35:24824–24837, 2022b.
Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath: Can your language model
pass chinese elementary school math test?, 2023.
Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. Large language
models are better reasoners with self-verification. CoRR, abs/2212.09561 , 2023.
Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context
learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080 , 2021.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and
Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.
arXiv preprint arXiv:2304.12244 , 2023a.
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with
parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196 , 2023b.
Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin, and Xiaodan Liang. Logicsolver: Towards
interpretable math word problem solving with logical prompt-enhanced learning. arXiv preprint
arXiv:2205.08232 , 2022.
Xi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reason-
ing. Advances in neural information processing systems , 35:30378–30392, 2022.
Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoyanov, Greg Durrett, and Ramakanth Pasunuru.
Complementary explanations for effective in-context learning. arXiv preprint arXiv:2211.13892 ,
2022.
Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. Answering
questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007 ,
2023.
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhen-
guo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions
for large language models. arXiv preprint arXiv:2309.12284 , 2023.
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scal-
ing relationship on learning mathematical reasoning with large language models. arXiv preprint
arXiv:2308.01825 , 2023.
Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. Chatdoctor: A medical chat model
fine-tuned on llama model using medical domain knowledge. arXiv preprint arXiv:2303.14070 ,
2023.
17

--- PAGE 18 ---
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 , 2022.
Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language
model for video understanding. arXiv preprint arXiv:2306.02858 , 2023a.
Yiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. arXiv
preprint arXiv:2211.04486 , 2022a.
Yue Zhang, Leyang Cui, Deng Cai, Xinting Huang, Tao Fang, and Wei Bi. Multi-task instruction
tuning of llama for specific scenarios: A preliminary study on writing assistance. arXiv preprint
arXiv:2305.13225 , 2023b.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in
large language models. arXiv preprint arXiv:2210.03493 , 2022b.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,
Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv
preprint arXiv:2303.18223 , 2023.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,
Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 , 2023.
Denny Zhou, Nathanael Sch ¨arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-
mans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex
reasoning in large language models. arXiv preprint arXiv:2205.10625 , 2022a.
Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and
Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint
arXiv:2211.09066 , 2022b.
Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang.
Solving math word problem via cooperative reasoning induced language models. arXiv preprint
arXiv:2210.16257 , 2022.
A A PPENDIX
A.1 C ASE STUDY
Table 3: A comparison case on KwaiYiiMath, ChatGPT, and GPT4 of the GSM8k dataset. All three
models answer correctly, with GPT4 giving the most concise answer that only included the core
calculation steps.
Instruction: While on vacation in Bali, Thea bought a hat from a craftsman worth $70.
If she gave the craftsman four $20 bills, how much change did she get?
Ground Truth Answer: 10
KwaiYiiMath First, we need to find out how much money Thea gave to the craftsman in total.
Since she gave him four $20 bills, that means she gave him 4 * $20 = $80.
Now, we know that the hat cost $70, so to find out how much change she got,
we simply subtract the cost of the hat from the total amount of money she gave:
$80 - $70 = $10. So, Thea got $10 in change. The answer is 10.
ChatGPT The total amount Thea paid the craftsman with four $20 bills is: 4 x $20 = $80
Since the hat only costs $70, Thea got a change of: $80 - $70 = $10
Therefore, Thea got a change of $10.
GPT4 Thea gave the craftsman $20 x 4 = $ <<20∗4 = 80 >>80.
So, she got $80 - $70 = $ <<80−70 = 10 >>10 change.
Answer: 10.
18

--- PAGE 19 ---
Table 4: A comparison case on KwaiYiiMath, ChatGPT, and GPT4 of the GSM8k dataset. KwaiYi-
iMath and GPT4 answer correctly, while ChatGPT answers incorrectly.
Instruction: Robert wants to buy a bookshelf and a sofa and has received two offers.
Furniture United’s offer includes a $1,350 advance payment and 6 monthly installments of $350 each.
Cozy Homes’ offer includes a $1,100 advance payment and 9 monthly installments of $250 each.
What is the cost difference between the two offers?
Ground Truth Answer: 100
KwaiYiiMath First, we need to calculate the total cost of each offer.
For Furniture United, the total cost is the sum of the advance payment and the
total amount paid in monthly installments. For Cozy Homes, the total cost is
the sum of the advance payment and the total amount paid in monthly installments.
Then, we subtract the lower total cost from the higher total cost to find the cost difference.
Furniture United: $1,350 + (6 * $350) = $1,350 + $2,100 = $3,450
Cozy Homes: $1,100 + (9 * $250) = $1,100 + $2,250 = $3,350
Cost difference: $3,450 - $3,350 = $100. The answer is 100.
ChatGPT The total cost of Furniture United’s offer can be calculated as:
$1,350 advance payment + 6 x $350 monthly installments = $3,150
The total cost of Cozy Homes’ offer can be calculated as:
$1,100 advance payment + 9 x $250 monthly installments = $3,350
To find the cost difference, we subtract the cost of Furniture United’s offer from the
cost of Cozy Homes’ offer:
$3,350 (Cozy Homes’ offer) - $3,150 (Furniture United’s offer) = $200
Therefore, the cost difference between the two offers is $200.
Furniture United’s offer is $200 cheaper than Cozy Homes’ offer.
GPT4 For Furniture United, the total cost of the monthly installments is
$350 * 6 = $ <<350∗6 = 2100 >>2,100. So, their total offer’s cost is
$1,350 (advance payment) + $2,100 (installments) = $ <<1350 + 2100 = 3450 >>3,450.
For Cozy Homes, the total cost of the monthly installments
is $250 * 9 = $ <<250∗9 = 2250 >>2,250. So, their total offer’s cost is
$1,100 (advance payment) + $2,250 (installments) = $ <<1100 + 2250 = 3350 >>3,350.
The cost difference between the two offers is $3,450 (Furniture United)
- $3,350 (Cozy Homes) = $ <<3450−3350 = 100 >>100. Answer: 100.
Table 5: A comparison case on KwaiYiiMath, ChatGPT, and GPT4 of CMath dataset. KwaiYiiMath
and GPT4 answer correctly, while ChatGPT answers incorrectly.
Instruction: 给直径为0.75米的水缸做一个木盖，木盖的直径比缸口直径大5厘米，
这个木盖的面积是多少平方米？
Ground Truth Answer: 0.5024
KwaiYiiMath 首先，将木盖直径与水缸直径相差的5厘米转换为米，即0.05米。
那么，木盖的直径为0.75米+0.05米=0.8米。
接下来，计算木盖的半径，即0.8米÷2=0.4米。
最后，利用圆的面积公式计算木盖的面积：
3.14×(0.4 米)²=3.14×0.16=0.5024 平方米。
因此，这个木盖的面积是0.5024平方米。
ChatGPT 水缸的直径为0.75米，则半径为0.75m / 2 = 0.375 米。
木盖的直径比缸口直径大5厘米，即比缸口半径大0.05米，因此木盖的半径为：
0.375米+ 0.05米= 0.425米。
那么，木盖的面积为：π× (0.425米)² -π× (0.375米)²≈0.155平方米
因此，这个木盖的面积约为0.155平方米。
GPT4 木盖的直径比缸口直径大5厘米，所以木盖的直径是：0.75米+ 0.05米= 0.8米。
木盖的半径为：0.8米÷ 2 = 0.4米。
木盖的面积为圆的面积公式：π× r²，其中r为半径，π≈3.14。
所以木盖的面积为：3.14 × (0.4 米)² = 3.14 × 0.16 平方米≈0.5024平方米。
19

--- PAGE 20 ---
Table 6: A comparison case on KwaiYiiMath, ChatGPT, and GPT4 of CMath dataset. All three
models use a series of equations to solve this question. KwaiYiiMath and GPT4 answer correctly,
while ChatGPT answers incorrectly.
Instruction: 妈妈和李阿姨一起到超市购物，妈妈买了 5千克大米和4千克面粉共付29.2元，
李阿姨也买了同样的4千克大米和5千克面粉则共付28.4元，每千克大米多少钱？
Ground Truth Answer: 3.6
KwaiYiiMath 设每千克大米的价格为x元，每千克面粉的价格为y元。
根据题目信息，我们可以得到以下两个 方程：
5x + 4y = 29.2
4x + 5y = 28.4
我们可以用消元法来解这个方程组。
首先，我们将第一个方程乘以 4，第二个方程乘以 5，得到：
20x + 16y = 116.8
20x + 25y = 142
接下来，我们将第二个方程从第一个方程中减去，得到：
9y = 25.2
然后，我们求解y:
y = 25.2 / 9
y = 2.8
现在我们知道每千克面粉的价格是2.8元，我们可以将y的值代入第一个方程，求解x
5x + 4(2.8) = 29.2
5x + 11.2 = 29.2
5x = 18
x = 18 / 5
x = 3.6
所以每千克大米的价格是3.6元。
ChatGPT 设每千克大米的价格为x元，则每千克面粉的价格为：
29.2元- 5千克× x元/千克÷ 4千克= 28.4元- 4千克× x元/千克÷ 5千克
简化可得：7.3元/千克- 5/4 x元/千克= 5.68元/千克- x元/千克
移项合并同类项可得：2.62元/千克= 1/4 x元/千克
即：x元/千克= 2.62 × 4 = 10.48 元/千克
因此，每千克大米的价格为10.48元。
GPT4 设每千克大米价格为x元，每千克面粉价格为y元，可以得到以下方程组：
5x + 4y = 29.2 （1）
4x + 5y = 28.4 （2）
为了求解这个方程组，我们可以将两个方程相加：
(5x + 4y) + (4x + 5y) = 29.2 + 28.4
9x + 9y = 57.6
x + y = 6.4 （3）
然后将方程（1）减去方程（2）： (5x + 4y) - (4x + 5y) = 29.2 - 28.4 x - y = 0.8 （4）
现在我们可以解这个方程组来找到每千克大米的价格：
将方程（3）和（4）相加得到：(x + y) + (x - y) = 6.4 + 0.8 2x = 7.2
所以每千克大米价格为: x = 7.2 ÷ 2 = 3.6 元
20

--- PAGE 21 ---
Table 7: A comparison case on KwaiYiiMath, ChatGPT, and GPT4 of KMath dataset. ChatGPT and
GPT4 answer incorrectly, while KwaiYiiMath answers correctly.
Instruction: 一根电线，第一次用去480米，比第二次用去的1.5倍少60米，第二次用去电线多少米？
Ground Truth Answer: 360
KwaiYiiMath 设第二次用去的电线长度为x米，根据题意可得方程:
480 = 1.5x - 60 。解这个方程，我们可以得到：
1.5x - 60 = 480
1.5x = 540
x = 360
所以，第二次用去的电线长度为360米。
ChatGPT 设第二次用去的电线长度为x，则第一次用去的电线长度为2x-60
（因为比第二次用去的1.5倍少60米）
根据题意得：第一次用去的电线长度+第二次用去的电线长度=总电线长度
即：2x-60 + x = 480
化简得：3x = 540
解方程得：x = 180
所以第二次用去的电线长度为180米。
GPT4 设第二次用去的电线长度为x米，
则第一次用去的电线长度为1.5x-60米。
根据题意，两次用去的电线总长度为480米，可以得到方程：
1.5x - 60 + x = 480
解这个方程，我们可以得到：
2.5x = 540
x = 216
所以，第二次用去的电线长度为216米。
Table 8: A additional comparison case on KwaiYiiMath, ChatGPT, and GPT4 of calculus field.
Instruction: 求g(x)=exp(xˆ2+1) 对x的微分
KwaiYiiMath 要计算g(x) = exp(xˆ2+1) 关于x的微分，我们需要使用链式法则。
首先，我们求导exp(xˆ2+1) 关于x的导数。
exp(xˆ2+1) 关于x的导数可以表示为：
d/dx(exp(xˆ2+1)) = d/dx(exp(xˆ2+1)) * d/dx(xˆ2+1)
这里，d/dx(xˆ2+1) = 2x ，因为xˆ2的导数是2x。
所以，d/dx(exp(xˆ2+1)) = exp(xˆ2+1) * 2x
现在我们求导g(x) = exp(xˆ2+1) 关于x的导数，
就是求导d/dx(exp(xˆ2+1)) * d/dx(xˆ2+1) ：
dg(x)/gx = exp(xˆ2+1) * 2x
这就是g(x)关于x的微分。
ChatGPT g’(x) = d/dx(exp(xˆ2+1))
使用链式法则，将指数函数部分的导数分离出来并乘以内部函数的导数：
g’(x) = exp(xˆ2+1) * d/dx(xˆ2+1)
对于xˆ2+1部分，也使用链式法则：
d/dx(xˆ2+1) = 2x * 1
将其代入原式得：g’(x) = exp(xˆ2+1) * 2x
因此，g(x)对x的微分为：g’(x) = 2x * exp(xˆ2+1)
GPT4 为了求g(x)=exp(xˆ2+1) 关于x的微分，我们采用链式法则求导。
链式法则表示为：假设y=f(u)和u=g(x)是可微函数，则复合函数y=f(g(x))的导数为：
(dy/dx) = (dy/du) * (du/dx)
这里，令u = xˆ2 + 1 ，那么g(x) = exp(u) 。
首先计算du/dx：du/dx = d(xˆ2 + 1)/dx = 2x
然后计算dy/du：dy/du = d(exp(u))/du = exp(u)
现在我们可以计算dy/dx：dy/dx = (dy/du) * (du/dx) = exp(u) * 2x = exp(xˆ2 + 1) * 2x
所以，g(x)=exp(x ˆ2+1)关于x的微分为：2x * exp(xˆ2 + 1) 。
21
