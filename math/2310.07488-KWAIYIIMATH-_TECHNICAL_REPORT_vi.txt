# 2310.07488.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2310.07488.pdf
# Kích thước tệp: 996809 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
KWAIYIIMATH: BÁO CÁO KỸ THUẬT
Jiayi Fu, Lei Lin, Xiaoyang Gao, Pengli Liu, Zhengzong Chen, Zhirui Yang, Shengnan Zhang,
Xue Zheng, Yan Li, Yuliang Liu, Xucheng Ye, Yiqiao Liao, Chao Liao, Bin Chen, Chengru Song,
Junchen Wan†, Zijia Lin, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, Kun Gai
Kuaishou Technology

TÓM TẮT
Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn (LLM) đã chứng minh khả năng đáng chú ý trong việc xử lý nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) xuôi dòng, thậm chí cả các nhiệm vụ toán học đòi hỏi suy luận nhiều bước. Trong báo cáo này, chúng tôi giới thiệu KwaiYiiMath, mô hình tăng cường khả năng suy luận toán học của KwaiYiiBase1 bằng cách áp dụng Điều chỉnh Tinh vi Có giám sát (SFT) và Học tăng cường từ Phản hồi Con người (RLHF), bao gồm cả nhiệm vụ toán học tiếng Anh và tiếng Trung. Đồng thời, chúng tôi cũng xây dựng một bộ dữ liệu kiểm tra toán học tiểu học Trung Quốc quy mô nhỏ (có tên KMath), bao gồm 188 ví dụ để đánh giá tính đúng đắn của quá trình giải quyết vấn đề được tạo ra bởi các mô hình. Các nghiên cứu thực nghiệm chứng minh rằng KwaiYiiMath có thể đạt được hiệu suất tốt nhất (SOTA) trên GSM8k, CMath và KMath so với các mô hình có kích thước tương tự.

1 GIỚI THIỆU
Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn (LLM) đã cách mạng hóa bối cảnh xử lý ngôn ngữ tự nhiên (NLP) Kenton & Toutanova (2019); Brown et al. (2020), trong đó việc mở rộng quy mô mô hình và lượng dữ liệu là một trong những yếu tố quan trọng Rae et al. (2021); Chowdhery et al. (2022); Anil et al. (2023); Touvron et al. (2023a;b). Các mô hình tiên tiến được đào tạo trên lượng dữ liệu khổng lồ với kích thước mô hình cực lớn, như ChatGPT OpenAI (2022), GPT-4 OpenAI (2023) và PaLM2 Anil et al. (2023), đã cho thấy hiệu suất chưa từng có trên một loạt các nhiệm vụ NLP Brown et al. (2020); Rae et al. (2021); Du et al. (2022); Lewkowycz et al. (2022); Chowdhery et al. (2022); Ouyang et al. (2022); Tay et al. (2022); OpenAI (2022; 2023); Anil et al. (2023); Touvron et al. (2023b).

Đáng ngạc nhiên, tiến bộ gần đây cho thấy LLM cũng có tiềm năng giải quyết các vấn đề suy luận Clark et al. (2020); Talmor et al. (2020); Suzgun et al. (2022); Wei et al. (2022b). Cụ thể, LLM có thể thực hiện suy luận suy diễn mềm trên các mô tả ngôn ngữ tự nhiên với kiến thức ẩn được lưu trữ trong các tham số của chúng Wei et al. (2022b); Kojima et al. (2022); Fu et al. (2022); Shi et al. (2022); Zhang et al. (2022b); Zhou et al. (2022b); Diao et al. (2023); Shum et al. (2023) hoặc kiến thức tường minh trong các tài nguyên bên ngoài Wang et al. (2022a); Creswell et al. (2022); Zhou et al. (2022a); Press et al. (2022); Dua et al. (2022); Reppert et al. (2023), và thực hiện suy luận từng bước chỉ với một vài minh chứng hoặc hướng dẫn thông qua việc nhắc chuỗi suy nghĩ (CoT) Wei et al. (2022b).

Trong báo cáo này, chúng tôi tập trung vào cách tăng cường khả năng suy luận toán học của LLM thông qua một quá trình căn chỉnh bao gồm điều chỉnh tinh vi có giám sát (SFT) và học tăng cường từ phản hồi con người (RLHF). Cụ thể, chúng tôi giới thiệu KwaiYiiMath được điều chỉnh tinh vi với các kỹ thuật căn chỉnh con người từ KwaiYiiBase để giải quyết các vấn đề toán học.

Kết quả thực nghiệm cho thấy KwaiYiiMath vượt trội hơn nhiều mô hình mã nguồn mở có kích thước tương tự với biên độ lớn và đang tiến gần đến GPT-4 trên ba bộ dữ liệu chuẩn toán học bao gồm cả tiếng Anh và tiếng Trung, tức là GSM8k Cobbe et al. (2021), CMath Wei et al. (2023), và một bộ dữ liệu nội bộ quy mô nhỏ KMath.

†Tác giả liên hệ: wanjunchen@kuaishou.com.
1KwaiYiiBase là một mô hình ngôn ngữ lớn được phát triển bởi Kuaishou https://github.com/kwai/KwaiYii/ .
1arXiv:2310.07488v2 [cs.CL] 19 Oct 2023

--- TRANG 2 ---
Cấu trúc của báo cáo này như sau: Phần 2 cung cấp tổng quan về các công trình liên quan bao gồm LLM và khả năng suy luận của LLM. Phần 3 giới thiệu phương pháp của KwaiYiiMath bao gồm quá trình điều chỉnh tinh vi có giám sát và căn chỉnh sở thích con người. Ngoài ra, nó cũng mô tả chi tiết về các nỗ lực trong việc thu thập lượng lớn dữ liệu đào tạo toán học chất lượng cao. Trong Phần 4, chúng tôi báo cáo kết quả thực nghiệm trên hai bộ dữ liệu chuẩn công khai và một bộ dữ liệu nội bộ. Phần 5 kết luận báo cáo này và chỉ ra công việc tương lai của KwaiYiiMath.

2 CÔNG TRÌNH LIÊN QUAN
Các Mô hình Ngôn ngữ Lớn Ngày nay, sự xuất hiện của LLM khuyến khích việc suy nghĩ lại về các khả năng của trí tuệ nhân tạo tổng quát (AGI). Một báo cáo gần đây thậm chí còn lập luận rằng GPT-4 có thể là một phiên bản sớm của hệ thống AGI Bubeck et al. (2023). Thành công của LLM bao gồm ba khía cạnh chính, bao gồm tiền đào tạo (cách tiền đào tạo một LLM dựa trên lượng lớn dữ liệu không nhãn), thích ứng (cách thích ứng LLM để tương tác tốt hơn với con người) và sử dụng (cách sử dụng LLM để giải quyết các nhiệm vụ xuôi dòng khác nhau) Zhao et al. (2023). Bằng cách tiền đào tạo trên kho dữ liệu quy mô lớn với mục tiêu mô hình hóa ngôn ngữ, LLM có thể thu được khả năng hiểu và tạo sinh ngôn ngữ cần thiết Brown et al. (2020); Chowdhery et al. (2022).2

Tuy nhiên, rõ ràng, một trong những vấn đề chính là khoảng cách giữa mục tiêu đào tạo và mục tiêu của người dùng: người dùng muốn mô hình "hiểu và tuân theo hướng dẫn của họ" trong khi LLM được thiết kế để dự đoán token tiếp theo. Để thu hẹp khoảng cách này, điều chỉnh hướng dẫn (IT) và điều chỉnh căn chỉnh (AT) được đề xuất để tăng cường khả năng và tính kiểm soát của LLM, trong đó IT chủ yếu nhằm khai thác khả năng của LLM trong khi mục đích của AT là căn chỉnh hành vi của LLM với sở thích con người. Nó đề cập đến quá trình điều chỉnh tinh vi thêm các LLM được tiền đào tạo trên một tập hợp các cặp thể hiện được định dạng (tức là (hướng dẫn, đầu ra)), trong đó hướng dẫn và đầu ra biểu thị hướng dẫn con người và đầu ra mong muốn được tạo ra bởi LLM tuân theo hướng dẫn con người, tương ứng. Cụ thể, trước tiên chúng ta cần thu thập hoặc xây dựng các cặp (hướng dẫn, đầu ra), bao gồm các thể hiện được định dạng thủ công Mishra et al. (2021); Victor et al. (2022); Muennighoff et al. (2022); Wang et al. (2022d); Longpre et al. (2023); Zhou et al. (2023); Conover et al. (2023); Köpf et al. (2023), và các thể hiện được định dạng tự động Wei et al. (2021); Bach et al. (2022); Honovich et al. (2022); Wang et al. (2022c); Xu et al. (2023a;b); Ji et al. (2023). Lưu ý rằng đã được chứng minh rộng rãi rằng số lượng nhiệm vụ, chất lượng và tính đa dạng của các thể hiện hướng dẫn có tác động quan trọng đến hiệu suất của LLM Ouyang et al. (2022); Victor et al. (2022); Wei et al. (2021); Wang et al. (2022d); Chung et al. (2022); Taori et al. (2023); Zhou et al. (2023). Sau đó, chúng tôi sử dụng những thể hiện được định dạng này hoặc những thể hiện được định dạng được lựa chọn cẩn thận để điều chỉnh tinh vi thêm LLM theo cách học có giám sát (còn được gọi là SFT) theo các yêu cầu khác nhau. Ví dụ, chúng ta có thể sử dụng các thể hiện được định dạng của các phương thức, miền và ứng dụng khác nhau để có được các LLM cụ thể khác nhau, bao gồm LLM đa phương thức (InstructPix2Pix Brooks et al. (2023), LLaVA Liu et al. (2023a), Video-LLaMA Zhang et al. (2023a), InstructBLIP Dai et al. (2023), MultiModal-GPT Gong et al. (2023)), và LLM cụ thể theo miền và ứng dụng (InstructDial Gupta et al. (2022), LINGUIST Rosenbaum et al. (2022), InstructUIE Wang et al. (2023), Writing-Alpaca Zhang et al. (2023b), Radiology-GPT Liu et al. (2023b), ChatDoctor Yunxiang et al. (2023), Goat Liu & Low (2023), WizardCoder Luo et al. (2023b), v.v.). Tuy nhiên, quá trình SFT có vẻ không đủ ổn định do lượng dữ liệu nhỏ và kích thước mô hình lớn. Vì vậy, một số nghiên cứu tập trung vào cách kết hợp điều chỉnh hướng dẫn và tiền đào tạo, chủ yếu bao gồm hai hướng: một quá trình một giai đoạn (tiền đào tạo từ đầu với hỗn hợp dữ liệu tiền đào tạo và dữ liệu điều chỉnh hướng dẫn) Raffel et al. (2020); Zeng et al. (2022) và quá trình hai giai đoạn (điều chỉnh tinh vi với hỗn hợp dữ liệu tiền đào tạo và dữ liệu điều chỉnh hướng dẫn) Iyer et al. (2022). AT đề cập đến quá trình học tăng cường từ phản hồi con người (RLHF) để căn chỉnh LLM tốt hơn với sở thích con người Ouyang et al. (2022). RLHF chủ yếu bao gồm ba thành phần chính: một LLM sau SFT, một mô hình phần thưởng học để phản ánh phản hồi con người cho văn bản được tạo ra bởi LLM, và một thuật toán học tăng cường (RL) (ví dụ, Tối ưu hóa Chính sách Gần đúng (PPO) Schulman et al. (2017)) để căn chỉnh LLM dựa trên các tín hiệu hướng dẫn được tạo ra bởi mô hình phần thưởng. Chúng tôi đào tạo một mô hình phần thưởng để dự đoán đầu ra được con người ưa thích dựa trên dữ liệu phản hồi con người được thu thập và căn chỉnh LLM dựa trên các tín hiệu phần thưởng được tạo ra bởi mô hình phần thưởng sử dụng thuật toán RL.3

2Chúng tôi bỏ qua hầu hết các chi tiết trong tiền đào tạo vì KwaiYiiMath chỉ tập trung vào cách tăng cường khả năng suy luận toán học của KwaiYiiBase.
3Chúng tôi bỏ qua các chi tiết về tiêu chí căn chỉnh và thu thập phản hồi con người để ngắn gọn.

--- TRANG 3 ---
Sau tiền đào tạo và thích ứng, LLM có thể phục vụ như một công cụ giải quyết nhiệm vụ ngôn ngữ đa năng (ở một mức độ nào đó) bằng cách đơn giản điều kiện hóa các mô hình trên một vài ví dụ (few-shot) hoặc hướng dẫn mô tả nhiệm vụ (zero-shot). Thành công của LLM thường được quy cho học few-shot (trong ngữ cảnh) hoặc zero-shot (tức là các khả năng xuất hiện Wei et al. (2022a) có thể không được quan sát thấy trong các mô hình ngôn ngữ nhỏ hơn trước đó).4 Điều này dẫn đến sự phát triển nhanh chóng của kỹ thuật "prompting", cách mạng hóa cách con người phát triển và sử dụng các thuật toán AI. Do đó, thiết kế prompt đã trở thành một chủ đề nóng trong NLP, bao gồm lựa chọn minh chứng Liu et al. (2021); Rubin et al. (2021); Xie et al. (2021); Zhang et al. (2022a); Kim et al. (2022); Lee et al. (2022); Levy et al. (2022); Su et al. (2022); Ye et al. (2022), và thứ tự minh chứng Liu et al. (2021); Lu et al. (2021).

Suy luận với Các Mô hình Ngôn ngữ Lớn Suy luận, quá trình đưa ra suy luận dựa trên kiến thức hiện có, là cốt lõi của trí tuệ con người. Tuy nhiên, các LLM hiện tại đã gặp khó khăn để đạt được hiệu suất cao trên các nhiệm vụ hệ thống-2 đòi hỏi suy luận chậm và nhiều bước như suy luận toán học và lý thuyết thông thường Rae et al. (2021). Để tăng cường khả năng suy luận của LLM prompting, có hai hướng chính: suy luận tăng cường chiến lược và suy luận tăng cường kiến thức Qiao et al. (2022). Suy luận tăng cường chiến lược đề cập đến thiết kế một chiến lược suy luận tốt hơn, như thiết kế prompt trong giai đoạn suy luận đơn lẻ Wei et al. (2022b); Kojima et al. (2022); Fu et al. (2022); Shi et al. (2022); Zhang et al. (2022b); Zhou et al. (2022b); Diao et al. (2023); Shum et al. (2023), suy luận từng giai đoạn Wang et al. (2022a); Creswell et al. (2022); Zhou et al. (2022a); Press et al. (2022); Dua et al. (2022); Reppert et al. (2023), tối ưu hóa lý do ngôn ngữ tự nhiên Ye & Durrett (2022); Wang et al. (2022c); Huang et al. (2022); Li et al. (2023b); Weng et al. (2023); Yoran et al. (2023); Shinn et al. (2023); Madaan et al. (2023); Paul et al. (2023), và tăng cường động cơ bên ngoài Liu et al. (2022); Chen et al. (2022); Gao et al. (2023); Lyu et al. (2023); Imani et al. (2023). Suy luận tăng cường kiến thức đề cập đến prompting LLM với kiến thức ẩn được lưu trữ trong LLM Li et al. (2022); Wang et al. (2022b); Magister et al. (2022); Ho et al. (2022); Fu et al. (2023) hoặc kiến thức tường minh trong các tài nguyên bên ngoài Yang et al. (2022); Su et al. (2022); Lu et al. (2022); He et al. (2022), như wiki, các bước suy luận, v.v.

3 PHƯƠNG PHÁP
Trong phần này, chúng tôi giới thiệu chi tiết về KwaiYiiMath. Hình 1 cho thấy tổng quan về mô hình của chúng tôi. Cụ thể, phần bên trái của Hình 1 cho thấy tổng quan quá trình đào tạo của KwaiYiiMath, và nó chủ yếu bao gồm hai bước, điều chỉnh tinh vi có giám sát và căn chỉnh sở thích con người. Phần bên phải của Hình 1 cho thấy ba thành phần chính, bao gồm thu thập dữ liệu SFT, thu thập dữ liệu sở thích con người, và đào tạo căn chỉnh sở thích con người. Trước tiên chúng tôi thu thập dữ liệu hướng dẫn toán học chất lượng cao, dưới dạng các cặp <câu hỏi, câu trả lời>, để thực hiện điều chỉnh tinh vi có giám sát. Tiếp theo, đối với mỗi câu hỏi, trước tiên chúng tôi tạo ra K câu trả lời khác nhau từ các mô hình SFT và các mô hình diễn viên từ học tăng cường tương ứng, sau đó phân loại chúng thành câu trả lời tốt và câu trả lời xấu với sự trợ giúp của chú thích con người. Theo cách này, chúng tôi có thể thu thập một lượng lớn dữ liệu chất lượng cao đại diện cho sở thích con người và sau đó sử dụng dữ liệu này để đào tạo mô hình phần thưởng và thực hiện đào tạo căn chỉnh sở thích con người.

3.1 ĐIỀU CHỈNH TINH VI CÓ GIÁM SÁT
Công trình trước đây đã chỉ ra rằng sự đa dạng và chất lượng của dữ liệu hướng dẫn có tác động quan trọng đến hiệu suất SFT Zhou et al. (2023); Peng et al. (2023); Chen et al. (2023), một kết luận cũng đúng trong khả năng suy luận toán học của LLM Yuan et al. (2023); Luo et al. (2023a); Chern et al. (2023); Yu et al. (2023). Do đó, chúng tôi tập trung vào cách thu thập hoặc xây dựng dữ liệu hướng dẫn có tính đa dạng cao và chất lượng cao.

Để có được càng nhiều sự đa dạng càng tốt trong dữ liệu toán học, trước tiên chúng tôi thu thập dữ liệu toán học từ một loạt các nguồn, bao gồm các mức độ khó khác nhau (ví dụ, tiểu học, trung học và đại học, v.v.), và các lĩnh vực toán học khác nhau (ví dụ, đại số, hình học và xác suất, v.v.). Sau đó, chúng tôi tạo ra lý do trung gian cho các câu hỏi toán học chỉ với câu trả lời cuối cùng hoặc không có câu trả lời bằng cách sử dụng LLM mã nguồn mở và đảm bảo tính đúng đắn của lý do trung gian và câu trả lời thông qua chú thích thủ công. Chúng tôi cố gắng xây dựng lý do trung gian cho tất cả dữ liệu hướng dẫn toán học vì Chuỗi Suy nghĩ (CoT) Wei et al. (2022b) đã được chứng minh hiệu quả trong cả prompting hoặc

4Lưu ý rằng các khả năng xuất hiện có thể không xảy ra trong một số LLM.

--- TRANG 4 ---
Mô hình LM Tiền đào tạo
KwaiYiiBase 13B
Điều chỉnh tinh vi có giám sát
căn chỉnh sở thích con người
RM + PPO DPO
KwaiYiiMath

Thu thập Dữ liệu Điều chỉnh tinh vi có giám sát
Tiền Đại số  Đại số Xác suất
Lý thuyết số Giải tích Hình học
...

Xây dựng CoT
Chú thích Con người
Tăng cường Đa dạng

Dữ liệu SFT
Mô hình SFT
PPO

câu hỏi
+
câu trả lời
tốt

câu hỏi
+
câu trả lời
xấu

Thu thập Dữ liệu Sở thích Con người
Dữ liệu
Sở thích
Con người

Dữ liệu
Sở thích
Con người

Mô hình SFT
Mô hình
Phần thưởng

Căn chỉnh Sở thích Con người
Dữ liệu
Sở thích
Con người

DPO

Dữ liệu
Sở thích
Con người

PPO
RM

căn chỉnh hướng dẫn

Hình 1: Tổng quan về KwaiYiiMath. Phần bên trái là khung của quá trình đào tạo mô hình. Phần bên phải cho thấy chi tiết của ba thành phần chính, bao gồm thu thập dữ liệu SFT, thu thập dữ liệu sở thích con người, và đào tạo căn chỉnh sở thích con người. Dữ liệu được sử dụng bởi RM, PPO, và DPO chỉ có cùng hình thức và phương pháp thu thập, nhưng bộ dữ liệu đào tạo của chúng là khác nhau.

dữ liệu hướng dẫn để điều chỉnh tinh vi Ho et al. (2022); Zhu et al. (2022); Li et al. (2023a). Ngoài dữ liệu toán học, chúng tôi cũng lấy mẫu 300k cuộc trò chuyện miền mở từ dữ liệu đào tạo KwaiYiiChat5 để duy trì khả năng xử lý các câu hỏi miền mở của mô hình. Chi tiết thêm về thu thập dữ liệu SFT như sau.

3.1.1 ĐA DẠNG DỮ LIỆU
Chúng tôi xem xét sự đa dạng của dữ liệu hướng dẫn chủ yếu từ hai khía cạnh: sự đa dạng của hướng dẫn và phản hồi, tương ứng. Hình 2 minh họa quá trình xây dựng dữ liệu SFT với một ví dụ.

Đa dạng Hướng dẫn Lấy cảm hứng từ Evol-Instruct Xu et al. (2023a); Luo et al. (2023b) sử dụng LLM thay vì con người để tạo ra các hướng dẫn đa dạng thông qua một tập hợp các hành động tiến hóa được thiết kế thủ công, chúng tôi thiết kế các hành động tiến hóa Dual-evolving theo chiều sâu và các hành động tiến hóa Constrained-mutation theo chiều rộng cho hướng dẫn toán học. Gần hoàn thành công việc của chúng tôi, các tác giả của Evol-Instruct Xu et al. (2023a); Luo et al. (2023b) cũng đã thích ứng ý tưởng của họ với LLM toán học và tiếp tục đề xuất Học tăng cường từ Phản hồi Evol-Instruct Luo et al. (2023a) kết hợp Evol-Instruct với học tăng cường. Tuy nhiên, các hành động Evol-Instruct chúng tôi sử dụng hơi khác. Cụ thể, chúng tôi xem xét một vấn đề toán học phức tạp về mặt logic Q được xây dựng bởi một loạt các vấn đề phụ đơn giản [q1, q2, ..., qn]. Vấn đề ban đầu Q tự nhiên có mối quan hệ CoT trong các vấn đề phụ {qi}i=1...n. Quá trình giải toán có thể được coi là quá trình CoT của các vấn đề phụ này. Do đó, cách giữ lại quá trình CoT trong Evol-Instruct là một vấn đề. Evol-Instruct được đề cập trong Luo et al. (2023a) phát triển hướng dẫn ban đầu Q thành Q′, đây là một mô hình một-đến-một bất kể hành động tiến hóa Xuống hoặc Lên và sẽ làm hại quá trình CoT của Q khi thực hiện tiến hóa Xuống đặc biệt. Chúng tôi đề xuất các hành động tiến hóa Dual-evolving theo chiều sâu để giữ lại quá trình CoT và mở rộng sự đa dạng hướng dẫn đồng thời. Cụ thể, các hành động tiến hóa Dual-evolving bao gồm hai bước:

• Đầu tiên chúng tôi thiết kế prompt để phân tách một vấn đề toán học Q thành nhiều vấn đề phụ [q1, q2, ..., qn]. Mỗi vấn đề phụ độc lập với nhau, và quá trình CoT

5KwaiYiiChat là mô hình được điều chỉnh tinh vi từ KwaiYiiBase cho các cuộc trò chuyện miền mở.

--- TRANG 5 ---
(a) Tăng cường Đa dạng Hướng dẫn
(b) Tăng cường Đa dạng Phản hồi

Hãy suy nghĩ từng bước\nBước 1: Joe {...}, vì vậy Harry có 4 * 56 = 224 cá.\n\nCâu trả lời là 224.

Tiến hóa Theo chiều rộng
Tiến hóa Theo chiều sâu
Few-shot CoT
Zero-shot CoT
Zero-shot
......

Sam có 7 con cá, và Joe có số cá nhiều gấp 8 lần Sam. Joe có bao nhiêu con cá?

Trong 4 người bạn, Emily có số chim nhiều gấp 5 lần Jane, và Jane có số chim nhiều gấp 6 lần Karen. Nếu Karen có 3 con chim, Emily có bao nhiêu con chim?

......

Câu hỏi: Trong 3 người bạn, Harry có số cá nhiều gấp 4 lần Joe, và Joe có số cá nhiều gấp 8 lần Sam. Nếu Sam có 7 con cá, Harry có bao nhiêu con cá?

Trả lời: Joe có 8*7 = <<8*7=56>>56 con cá.\nHarry có 4*56 = <<4*56=224>>224 con cá.\n#### 224

Joe có 8 * 7 = 50 con cá.\nHarry có 4 * 50 = 220 con cá.\nCâu trả lời là 224.

Joe có 8 * 7 = 56 con cá.\nHarry có 4 * 56 = 224 con cá.\nCâu trả lời là 224.

Để tìm ra Harry có bao nhiêu con cá, {...} Harry có số cá nhiều gấp 4 lần Joe, Harry có 4 * 56 = 224 con cá.

Câu trả lời sai hoặc tính toán sai

Hình 2: Một ví dụ về tăng cường đa dạng dữ liệu SFT. Chúng tôi chọn một mẫu từ tập huấn luyện của GSM8k. Phần trên cho thấy quá trình tăng cường đa dạng hướng dẫn. Có hai hướng dẫn mới được tạo ra thông qua tiến hóa theo chiều sâu và tiến hóa theo chiều rộng, tương ứng. Phần dưới cho thấy quá trình tăng cường đa dạng phản hồi. Có bốn phản hồi mới được tạo ra thông qua xây dựng đường dẫn suy luận đa dạng và một trong số chúng bị lọc ra do tính toán sai, mặc dù câu trả lời cuối cùng là đúng.

bao gồm thứ tự của các vấn đề phụ, đây là một mô hình một-đến-nhiều của tiến hóa Xuống.

• Thứ hai, chúng tôi thiết kế prompt để tăng cường khó khăn của các vấn đề phụ ngày càng nhiều và tiếp tục cải thiện sự đa dạng của các hướng dẫn. Vì phương pháp của chúng tôi bao gồm hai bước, chúng tôi gọi nó là hành động tiến hóa Dual-evolving.

Trong tiến hóa theo chiều rộng, chúng tôi sử dụng một hành động tiến hóa có ràng buộc được lấy cảm hứng từ tiến hóa đột biến Xu et al. (2023a). Cụ thể, Chúng tôi thiết kế một prompt để phát triển các vấn đề mới dựa trên các vấn đề hiện có trong một phạm vi có ràng buộc. Mục đích của việc thêm ràng buộc phạm vi là để tránh các hành động tiến hóa dẫn đến các vấn đề toán học không thể giải được.

Đa dạng Phản hồi Công trình trước đây Ho et al. (2022); Yuan et al. (2023) đã chỉ ra rằng các đường dẫn suy luận đa dạng có thể cải thiện hiệu suất suy luận của LLM, bao gồm: đối với một mẫu hướng dẫn cho trước trong một tập huấn luyện, sử dụng nhiều mô hình khác nhau hoặc chiến lược lấy mẫu để tạo ra nhiều đường dẫn suy luận.

Lấy cảm hứng từ điều đó, chúng tôi thu thập các LLM có sẵn khác nhau bao gồm LLM mã nguồn mở như Llama Touvron et al. (2023a;b) với các kích thước khác nhau, v.v. và các phiên bản khác nhau của KwaiYiiMath. Sau đó, chúng tôi sử dụng các mô hình này để tạo ra nhiều đường dẫn suy luận cho mỗi câu hỏi. Cụ thể, chúng tôi điều chỉnh tinh vi LLM mã nguồn mở trên các bộ dữ liệu toán học theo cách có giám sát để có được khả năng tạo ra nhiều đường dẫn suy luận đúng hơn.6 Để tiếp tục tăng cường những khả năng đó của LLM và cải thiện sự đa dạng của các đường dẫn suy luận, chúng tôi thu thập các prompt CoT đa dạng,7 và sử dụng các chiến lược prompting khác nhau như zero-shot, zero-shot CoT và few-shot CoT. Đối với mỗi câu hỏi qi, chúng tôi tạo ra k đường dẫn suy luận và câu trả lời ứng viên r,a với nhiệt độ 0.7 theo Cobbe et al. (2021) và một trong các chiến lược prompting dựa trên các prompt khác nhau. Trước tiên chúng tôi lọc ra các đường dẫn suy luận với câu trả lời sai a≠ai hoặc tính toán sai nơi các phương trình được trích xuất từ các đường dẫn suy luận.8 Chúng tôi giữ lại tất cả các đường dẫn suy luận với cùng danh sách phương trình như dữ liệu được tăng cường không giống Yuan et al. (2023) vì chúng tôi lập luận rằng các ngữ cảnh đa dạng cũng có tác động quan trọng đến hiệu suất suy luận của LLM.

6Các phiên bản khác nhau của KwaiYiiMath đã được điều chỉnh tinh vi trên các bộ dữ liệu toán học theo cách có giám sát.
7https://github.com/FranxYao/chain-of-thought-hub/tree/main/gsm8k/lib prompt
8Lưu ý rằng chúng tôi chỉ đánh giá tính đúng đắn của phương trình hoàn chỉnh được trích xuất (ví dụ, 3+4=7) thay vì phương trình không hoàn chỉnh (ví dụ, +4=7).

--- TRANG 6 ---
vì chúng tôi lập luận rằng các ngữ cảnh đa dạng cũng có tác động quan trọng đến hiệu suất suy luận của LLM.

3.1.2 CHẤT LƯỢNG DỮ LIỆU
Điều quan trọng nhất đối với dữ liệu toán học là tính đúng đắn của quá trình tính toán và câu trả lời cuối cùng của phản hồi. Chúng tôi ký hiệu D={qi, pi, ai}i là các bộ dữ liệu hỗn hợp, trong đó qi, pi={e1, e2, ..., en, âi} là một câu hỏi và một đường dẫn suy luận tương ứng, {ei}i biểu thị tập hợp phương trình, âi là câu trả lời cuối cùng, và ai biểu thị câu trả lời suy luận thực tế của qi. LLM thường mắc lỗi tính toán hoặc kết luận trong đường dẫn suy luận Gao et al. (2023); Chen et al. (2022) như eval(ei) = False hoặc âi≠ai.

Để đạt được dữ liệu chất lượng cao, chúng tôi nỗ lực đảm bảo tính đúng đắn của cả câu trả lời cuối cùng và quá trình tính toán. Cụ thể, trước tiên chúng tôi sử dụng các phản hồi được tạo ra bởi LLM để trích xuất câu trả lời cuối cùng âi và sau đó lọc ra các đường dẫn suy luận pj với câu trả lời sai pj:{âj≠aj}. Sau đó, chúng tôi sử dụng biểu thức chính quy để trích xuất tập hợp phương trình {ei}i trong phản hồi và sử dụng trình thông dịch Python để đánh giá tính đúng đắn của phản hồi. Đối với các truy vấn câu trả lời đúng duy nhất, chúng tôi kiểm soát cả tính đúng đắn của câu trả lời cuối cùng và quá trình tính toán. Đối với các truy vấn câu trả lời đúng nhiều lần, chúng tôi chỉ kiểm soát tính đúng đắn của quá trình tính toán.

3.2 CĂN CHỈNH SỞ THÍCH CON NGƯỜI
Mặc dù hiệu suất đáng kể của LLM được điều chỉnh tinh vi về khả năng suy luận toán học, chúng vẫn có xu hướng tạo ra nội dung chứa lỗi suy luận, câu trả lời không chính xác hoặc quy trình suy luận thừa. Chúng tôi lập luận rằng sự nâng cao hiệu suất của LLM không chỉ xuất phát từ điều chỉnh tinh vi có giám sát mà còn từ các phương pháp căn chỉnh sở thích con người. Do đó, chúng tôi sử dụng hai khung căn chỉnh có thể mở rộng, học tăng cường từ phản hồi con người (RLHF) và DPO Rafailov et al. (2023), cả hai đều học từ sở thích con người để đào tạo các mô hình ngôn ngữ được căn chỉnh và cải thiện khả năng suy luận toán học và tính chính xác câu trả lời.

3.2.1 HỌC TĂNG CƯỜNG TỪ PHẢN HỒI CON NGƯỜI
RLHF là áp dụng học tăng cường trực tiếp trên LLM với sở thích con người làm phản hồi, và phát triển nhanh chóng trong lĩnh vực căn chỉnh các mô hình ngôn ngữ. Lấy cảm hứng từ InstructGPT, chúng tôi đề xuất một pipeline đào tạo RLHF cổ điển bao gồm hai giai đoạn: 1) thu thập dữ liệu so sánh sở thích con người và đào tạo mô hình phần thưởng; 2) học tăng cường với PPO.

Mô hình Phần thưởng: Mô hình phần thưởng được sử dụng để đánh giá chất lượng của việc tạo sinh SFT từ khía cạnh kết quả và quy trình toán học, cũng như sở thích con người.

Cho một đầu vào, chúng tôi lấy mẫu một cặp phản hồi từ SFT và PPO của chúng tôi với các phiên bản khác nhau để RM có thể nắm bắt phân phối dữ liệu đa dạng. Một số bộ dữ liệu sở thích mã nguồn mở hiện có cũng được kết hợp để cải thiện tổng quát hóa của RM, như Anthropic Helpful and Harmless Bai et al. (2022), OpenAI WebGPT Nakano et al. (2021).

Hàm mất mát xếp hạng nhị phân mà chúng tôi sử dụng phù hợp với Ouyang et al. (2022). Chúng tôi giữ lại 5.000 ví dụ làm tập kiểm tra để đánh giá mô hình của chúng tôi. Kết quả được báo cáo trong Bảng 1. Như một điểm tham chiếu, chúng tôi cũng đánh giá các giải pháp thay thế có sẵn công khai khác làm đường cơ sở: mô hình phần thưởng Open Assistant dựa trên DeBERTa V3 Large He et al. (2020), và SteamSHP-XL Ethayarajh et al. (2022) dựa trên FLAN-T5-xl.

PPO: PPO là phương pháp học tăng cường nổi tiếng nhất được sử dụng trong RLHF. Nó sử dụng điểm số từ các mô hình phần thưởng làm tín hiệu phản hồi con người để điều chỉnh tinh vi LLM. Bằng cách lấy mẫu ngẫu nhiên prompt từ các bộ dữ liệu SFT và sử dụng các phản hồi được tạo ra bởi chính sách, chúng tôi nhằm mục đích cho phép các mô hình được điều chỉnh tinh vi được căn chỉnh với các giá trị con người. Chúng tôi cũng thấy rằng việc làm trắng điểm số phần thưởng rất quan trọng vì vấn đề hack phần thưởng và tăng tính ổn định đào tạo.

3.2.2 TỐI ƯU HÓA SỞ THÍCH TRỰC TIẾP (DPO)
Các phương pháp RLHF cần một mô hình phần thưởng để phù hợp với sở thích con người, sau đó tối ưu hóa các mô hình ngôn ngữ bằng học tăng cường để tạo ra câu trả lời điểm số phần thưởng cao. Tuy nhiên, chúng đòi hỏi tiêu thụ lớn tài nguyên tính toán và cài đặt phân tán quy mô lớn phức tạp. Số lượng siêu tham số trong các phương pháp RLHF cũng tăng độ khó của tính ổn định của kết quả tối ưu hóa. Tối ưu hóa Sở thích Trực tiếp (DPO) là một phương pháp không cần RL và dễ thực hiện. Bằng cách loại bỏ mô hình phần thưởng, DPO chỉ xem xét phân phối xác suất của mô hình chính sách và mô hình tham chiếu và sử dụng một mục tiêu entropy chéo nhị phân đơn giản để tối ưu hóa các mô hình ngôn ngữ từ sở thích con người. Dữ liệu đào tạo DPO có mô hình nhất quán với dữ liệu mô hình phần thưởng được sử dụng để đào tạo. Để cải thiện hiệu quả khả năng suy luận toán học của KwaiYiiMath, chúng tôi lấy mẫu một tập con của dữ liệu mô hình phần thưởng và tiến hành đào tạo DPO trên mô hình được điều chỉnh tinh vi.

4 THỰC NGHIỆM
Chúng tôi chủ yếu đánh giá KwaiYiiMath trên ba bộ dữ liệu chuẩn toàn diện và thực tế để đo lường khả năng suy luận toán học, bao gồm hai bộ dữ liệu chuẩn công khai (GSM8k Cobbe et al. (2021) và CMath Wei et al. (2023)), và một bộ dữ liệu nội bộ (KMath).

4.1 CÁC BỘ DỮ LIỆU ĐÁNH GIÁ
Bảng 1: Kết quả trên tập kiểm tra tiếng Trung và tiếng Anh của chúng tôi về các bộ dữ liệu chuẩn sở thích con người.

Mô hình RM KwaiYiiMath-en KwaiYiiMath-zh
Open Assistant 63.79 65.80
SteamSHP-XL 55.90 54.43
KwaiYiiMath-RM 77.30 (+13.51) 78.48 (+12.68)

GSM8k Cobbe et al. (2021) chứa 7.473 ví dụ huấn luyện và 1.319 ví dụ kiểm tra, chủ yếu về các vấn đề toán học tiếng Anh cấp tiểu học. Mỗi câu hỏi bao gồm các phép toán số học cơ bản (cộng, trừ, nhân và chia), và thường đòi hỏi từ 2 đến 8 bước suy luận để giải quyết.

CMath Wei et al. (2023) là một bộ dữ liệu các bài toán văn toán tiểu học Trung Quốc bao gồm 1.7k9 bài toán văn toán cấp tiểu học với các chú thích chi tiết, có nguồn gốc từ các sách bài tập và kỳ thi Trung Quốc thực tế. CMath cũng có các chú thích chi tiết, bao gồm cấp độ, số bước suy luận, chữ số và yếu tố gây nhiễu. Những chú thích này có thể được sử dụng để đánh giá khả năng suy luận toán học chi tiết và khả năng suy luận toán học mạnh mẽ của LLM.

KMath là một bộ dữ liệu toán học nội bộ quy mô nhỏ chứa 188 câu hỏi toán học tiếng Trung. Các câu hỏi của KMath chủ yếu ở cấp độ tiểu học, bao gồm đại số, giải tích, hình học và xác suất.

4.2 CÁC ĐƯỜNG CƠ SỞ
Các mô hình đường cơ sở được so sánh trong thí nghiệm của chúng tôi có thể được chia thành hai loại: mô hình nguồn đóng và mô hình nguồn mở.

Mô hình nguồn đóng Nhiều công ty công nghệ đã đào tạo LLM với khả năng mạnh mẽ trong nhiều nhiệm vụ xuôi dòng, nhưng vì một số lý do không phát hành trọng số mô hình của họ và các mô hình này được gọi là mô hình nguồn đóng. Trong thí nghiệm của chúng tôi, chúng tôi xem xét năm LLM nguồn đóng bao gồm GPT-4 OpenAI (2023), ChatGPT OpenAI (2022), Ernie Bot10, Minerva Zhu et al. (2022) và MATH-QWEN-CHAT Bai et al. (2023).

Mô hình nguồn mở Cũng có nhiều đội ngũ mã nguồn mở các LLM mà họ đào tạo, và chúng ta có thể trực tiếp tải xuống trọng số mô hình thông qua các cộng đồng mã nguồn mở. Những mô hình mã nguồn mở này cũng chứng minh khả năng xuất sắc trong nhiều nhiệm vụ xuôi dòng. Trong thí nghiệm của chúng tôi, chúng tôi chọn LLaMa1&2 Touvron et al. (2023a;b), BaiChuan1&2 Baichuan (2023), ChatGLM2-6B11, QWen12, WizardMath Luo et al. (2023a), GAIRMath-Abel Chern et al. (2023), và MetaMath Yu et al. (2023) làm các đường cơ sở mã nguồn mở.

9Bộ dữ liệu CMath chứa tổng cộng 1.7k dữ liệu, trong đó 960 hiện có sẵn để tải xuống.
10https://yiyan.baidu.com/
11https://github.com/THUDM/ChatGLM2-6B
12https://github.com/QwenLM/Qwen-7B

--- TRANG 8 ---
Bảng 2: Kết quả của pass@1 (%) trên GSM8k, CMath và KMath. Ký tự ∗ biểu thị rằng kết quả được đạt được từ các công trình liên quan, và các kết quả còn lại được đạt được từ các bài kiểm tra của chúng tôi. Ký tự † cho thấy kết quả tốt nhất của chúng tôi từ các thí nghiệm căn chỉnh sở thích con người khác nhau.

Mô hình #Tham số GSM8K CMath KMath
Mô hình nguồn đóng
GPT-4 OpenAI (2023) - 92.00∗86.00 75.00
ChatGPT OpenAI (2022) - 74.90∗73.83 59.57
Minerva Lewkowycz et al. (2022) 8B 16.20∗- -
62B 52.40∗- -
540B 58.80∗- -
Ernie Bot Baidu (2023) - 56.23 84.33 72.87
MATH-QWEN-CHAT Bai et al. (2023) 7B 62.50∗- -
14B 69.80∗- -
Mô hình nguồn mở
LLaMA-1 Touvron et al. (2023a) 13B 17.80∗- -
33B 35.60∗- -
LLaMA-2 Touvron et al. (2023b) 13B 28.70∗- -
34B 44.20∗- -
BaiChuan1 Baichuan (2023) 7B 9.17∗- -
13B 26.76∗51.33 28.19
BaiChuan2 Baichuan (2023) 7B 24.49∗- -
13B 52.77∗- -
WizardMath Luo et al. (2023a) 13B 63.90∗50.83 23.40
70B 81.60∗- -
ChatGLM2 Zeng et al. (2022) 6B 29.20∗68.36 50.00
QWen Bai et al. (2023) 7B 51.60∗63.16 44.15
GAIRMath-Abel Chern et al. (2023) 7B 59.74∗- -
13B 66.41∗- -
70B 83.62∗- -
MetaMath Yu et al. (2023) 7B 66.50∗- -
13B 72.30∗- -
70B 82.30∗- -
KwaiYiiMath 13B 72.33 85.33 73.40
KwaiYiiMath-HPA† 13B 73.31 85.83 74.47

4.3 CÀI ĐẶT ĐÀO TẠO VÀ ĐÁNH GIÁ
Đào tạo Meta prompt được sử dụng trong đào tạo KwaiYiiMath là phiên bản từ Vicuna Chiang et al. (2023): Một cuộc trò chuyện giữa một người dùng tò mò và một trợ lý trí tuệ nhân tạo. Trợ lý đưa ra câu trả lời hữu ích, chi tiết và lịch sự cho các câu hỏi của người dùng. NGƯỜI DÙNG: {instruction}. TRỢ LÝ: {response}.

Chúng tôi tuân theo các siêu tham số điều chỉnh tinh vi tiêu chuẩn: 3 epoch sử dụng AdamW Loshchilov & Hutter (2017) với β1 = 0.9, β2 = 0.95. Không có các bước khởi động, chúng tôi đặt tốc độ học ban đầu là 4e−5 và sử dụng chiến lược suy giảm tốc độ học cosine. Kích thước lô toàn cục được đặt là 1024 ví dụ và các văn bản dài hơn 2048 token sẽ bị cắt bớt.

Đánh giá Đối với đánh giá trên GSM8k Cobbe et al. (2021), chúng tôi tạo ra các phản hồi sử dụng chiến lược giải mã tham lam. Đối với mỗi mẫu trong CMath Wei et al. (2023) và KMath, chúng tôi tạo ra một

--- TRANG 9 ---
phản hồi duy nhất từ mỗi mô hình đường cơ sở sử dụng lấy mẫu hạt nhân Holtzman et al. (2019) với topp = 0.9 và nhiệt độ τ = 0.7. Chúng tôi áp dụng một hình phạt lặp lại của các token được tạo ra trước đó với một siêu tham số là 1.01 Keskar et al. (2019). Chúng tôi giới hạn độ dài token tối đa của đầu ra là 2048.

Mặc dù tất cả các mô hình được so sánh có thể tạo ra quá trình CoT trung gian và câu trả lời cuối cùng, chúng tôi đánh giá tất cả LLM trên GSM8k Cobbe et al. (2021) sử dụng few-shot CoT từ Wei et al. (2022b) để so sánh công bằng. Chúng tôi đánh giá một giải pháp là đúng nếu câu trả lời cuối cùng khớp với giải pháp thực tế, độc lập với chất lượng của CoT đi trước nó. Để đánh giá tính đúng đắn, chúng tôi phân tích các câu trả lời cuối cùng và so sánh chúng sử dụng thư viện SymPy Meurer et al. (2017). Đối với CMath Wei et al. (2023), chúng tôi sử dụng mã được phát hành cùng với dữ liệu để đánh giá độ chính xác chỉ xem xét tính đúng đắn của câu trả lời cuối cùng. Đối với KMath, để đánh giá kết quả mô hình một cách toàn diện hơn, chúng tôi không chỉ đánh giá tính đúng đắn của câu trả lời mà còn tính đúng đắn của quá trình giải quyết vấn đề. Cụ thể, chúng tôi đánh giá một giải pháp là đúng thông qua chú thích con người nếu câu trả lời đúng và quá trình giải quyết vấn đề CoT về cơ bản là đúng. Để loại bỏ sự thiên vị của chú thích con người, tính đúng đắn của mỗi mẫu trước tiên được gán nhãn bởi ba người chú thích khác nhau, và sau đó một chuyên gia đánh giá chất lượng khác kiểm tra các nhãn. Các mô hình đường cơ sở được đánh giá theo cách zero-shot vì chúng đã được căn chỉnh thông qua SFT cho cả CMath Wei et al. (2023) và KMath.

4.4 KẾT QUẢ CHÍNH
Kết quả trên ba bộ dữ liệu được hiển thị trong Bảng 2. Chúng tôi quan sát thấy rằng KwaiYiiMath vượt trội hơn các LLM đường cơ sở cùng kích thước trên tất cả các bộ dữ liệu chuẩn và cũng vượt qua các LLM nguồn đóng bao gồm ChatGPT và GPT4 trên bộ dữ liệu KMath, cho thấy rằng việc điều chỉnh tinh vi trên dữ liệu đa dạng và chất lượng cao là hiệu quả. Trên bộ dữ liệu CMath, KwaiYiiMath gần với GPT4 và cũng đạt được sự cải thiện lớn so với các mô hình đường cơ sở khác. Nó cho thấy rằng KwaiYiiMath không chỉ hiệu quả trên các vấn đề toán học tiếng Anh mà còn trên các vấn đề toán học tiếng Trung. Đồng thời, KwaiYiiMath-HPA đạt được sự cải thiện so với KwaiYiiMath trên ba bộ dữ liệu chuẩn, cho thấy hiệu quả của quá trình căn chỉnh sở thích con người.

4.5 KẾT QUẢ CHI TIẾT VÀ TÍNH MẠNH MẼ
Trong tiểu mục này, chúng tôi điều tra hiệu suất của các LLM trên các vấn đề toán học có độ phức tạp khác nhau.

Kết quả Chi tiết Bộ dữ liệu CMath Wei et al. (2023) cung cấp cấp tiểu học tương ứng với câu hỏi, có thể chỉ ra độ phức tạp toàn diện của câu hỏi. Ngoài ra, hai chiều được cung cấp để đại diện trực quan hơn cho độ phức tạp của câu hỏi, tức là số chữ số mà LLM cần thao tác, và số bước suy luận mà LLM cần thực hiện để giải quyết một vấn đề. Trực quan, các vấn đề có độ phức tạp số học cao hơn hoặc độ phức tạp suy luận cao hơn sẽ khó giải quyết hơn, dẫn đến độ chính xác thấp hơn.

Trong Hình 3a, một xu hướng giảm rõ rệt về độ chính xác là hiển nhiên, biểu thị rằng hiệu suất của tất cả các mô hình giảm khi độ phức tạp tăng. GPT-4 và KwaiYiiMath là hai mô hình duy nhất đạt được thành công (độ chính xác vượt quá 60%) trong các bài kiểm tra toán học trên tất cả sáu cấp tiểu học và đạt được độ chính xác cao (vượt quá 80%) trong các bài kiểm tra cho cấp 1 đến 4. Hiệu suất của KwaiYiiMath rất gần với GPT-4, và thậm chí vượt trội hơn GPT-4 một chút trong một số cấp. Theo sau GPT-4 và KwaiYiiMath, ChatGPT, ChatGLM2-6b, và Qwen-7B chứng minh thành công trong các bài kiểm tra cho cấp 1 đến 4, nhưng gặp khó khăn trong cấp 5 và 6 (độ chính xác dưới 60%).

Từ Hình 3b và 3c, có thể thấy rằng hiệu suất của tất cả các mô hình giảm khi bất kỳ thước đo độ phức tạp vấn đề nào tăng. Từ độ dốc giảm của các biểu đồ, có thể nói rằng độ phức tạp suy luận của vấn đề nhìn chung có tác động lớn hơn độ phức tạp số học.

Kết quả Tính mạnh mẽ Thí nghiệm tính mạnh mẽ bao gồm hai phần: đánh giá đối kháng trên bộ dữ liệu GSM8k mạnh mẽ và bộ dữ liệu CMath distractor. Bộ dữ liệu GSM8k mạnh mẽ là một bộ dữ liệu được phát hành bởi Chern et al. (2023) được thiết lập dựa trên bộ dữ liệu GSM8k. Chern et al. (2023) ngẫu nhiên thay đổi các số trong các câu hỏi của tập kiểm tra GSM8k, mà không thay đổi bất kỳ thông tin nào khác trong các câu hỏi, sử dụng GPT-4. Bộ dữ liệu GSM8k mạnh mẽ có thể được sử dụng để đánh giá liệu các mô hình có quá khớp dữ liệu huấn luyện hay không, làm cho các mô hình dễ bị tổn thương trước các mẫu kiểm tra ngoài phân phối. Hình 4 cho thấy hiệu suất của KwaiYiiMath trên bộ dữ liệu GSM8k mạnh mẽ13. Có thể thấy rằng có sự giảm nhẹ trong hiệu suất của KwaiYiiMath, điều này chứng minh rằng KwaiYiiMath cũng có tính mạnh mẽ cao đối với các mẫu kiểm tra ngoài phân phối.

Bộ dữ liệu CMath distractor là một tập hợp nhỏ được phát hành cùng với CMath Wei et al. (2023) để kiểm tra tính mạnh mẽ của mô hình đối với thông tin không liên quan trong câu hỏi. Bộ dữ liệu CMath distractor chứa 360 câu hỏi: 60 câu hỏi gốc và 5 phiên bản nhiễu của mỗi câu hỏi gốc với số lượng distractor khác nhau, từ 1 đến 5. Hiệu suất của các mô hình trên bộ dữ liệu CMath distractor được vẽ trong Hình 3d. Từ hình, có thể quan sát thấy rằng chúng tôi quan sát thấy rằng hiệu suất của tất cả LLM, ngoại trừ GPT-4, giảm mạnh khi số lượng distractor tăng. KwaiYiiMath cho thấy một khả năng tính mạnh mẽ nhỏ, với hiệu ứng tổng thể xếp hạng thứ hai trong tất cả các mô hình, và độ chính xác trên tập con 5 distractor vẫn lớn hơn 40%. Tuy nhiên, KwaiYiiMath cũng bị giảm độ chính xác gần 50% cho các vấn đề được tăng cường với chỉ ba distractor. Chúng tôi đoán sự giảm hiệu suất là do dữ liệu huấn luyện được sử dụng bởi KwaiYiiMath là các câu hỏi tương đối sạch, tức là không có thông tin distractor trong các câu hỏi. Do đó, khi thông tin distractor xuất hiện trong câu hỏi, đặc biệt là thông tin rất giống với câu hỏi gốc cần được giải quyết, mô hình sẽ tạo ra nhiều bước vô dụng để giải quyết vấn đề, dẫn đến câu trả lời cuối cùng bị sai. Đây cũng là một khả năng quan trọng cần được cải thiện trong tương lai.

13Kết quả của RFT, Abel, và WizardMath được lấy từ Chern et al. (2023)

--- TRANG 10 ---

[Hình 3: Các biểu đồ hiệu suất trên nhiều thang đo]

(a) Độ chính xác trung bình so với một trong các thước đo độ phức tạp vấn đề
(b) Độ chính xác trung bình so với số bước suy luận
(c) Độ chính xác trung bình so với số chữ số
(d) Độ chính xác trung bình so với số distractor

Hình 3: (a) (b) (c): Độ chính xác kiểm tra trung bình so với một trong các thước đo độ phức tạp vấn đề, bao gồm cấp độ, số bước suy luận, và số chữ số cho mỗi LLM. (d): Độ chính xác kiểm tra trung bình so với số distractor trên bộ dữ liệu distractor. Ký tự ∗ biểu thị rằng độ chính xác kiểm tra được lấy từ bài báo Wei et al. (2023)

[Hình 4: Biểu đồ so sánh]

KwaiYiiMath_13B Chatglm2_6B RFT_7B Abel_13B Baichuan_13B WizardMath_13B

Hình 4: So sánh độ chính xác của GSM8k gốc và GSM8k Robust.

--- TRANG 11 ---
5 KẾT LUẬN
Trong báo cáo này, chúng tôi giới thiệu KwaiYiiMath được điều chỉnh tinh vi từ KwaiYiiBase để giải quyết các vấn đề toán học. Bằng cách sử dụng một lượng lớn dữ liệu toán học chất lượng cao để thực hiện quá trình căn chỉnh con người, bao gồm SFT và RLHF, chúng tôi đã tăng cường đáng kể khả năng suy luận toán học của KwaiYiiMath. Kết quả thực nghiệm cũng cho thấy rằng KwaiYiiMath vượt trội hơn nhiều mô hình mã nguồn mở có kích thước tương tự với biên độ lớn và đang tiến gần đến GPT-4 trên ba bộ dữ liệu chuẩn toán học bao gồm tiếng Anh và tiếng Trung. Kết quả thực nghiệm trên các bộ dữ liệu mạnh mẽ liên quan cũng cho thấy rằng KwaiYiiMath có tính mạnh mẽ đối với thông tin nhiễu trong câu hỏi.

Mặc dù công việc của chúng tôi đã đạt được kết quả gần với GPT-4 trên các bộ dữ liệu chuẩn liên quan, trên thực tế, vẫn còn khoảng cách lớn trong một phạm vi nhiệm vụ rộng hơn. Trong tương lai, chúng tôi hy vọng tiếp tục khám phá các phương pháp để cải thiện khả năng suy luận toán học của LLM và cơ chế nội tại đằng sau việc tăng cường dữ liệu cho LLM.

TÀI LIỆU THAM KHẢO
[Danh sách tài liệu tham khảo dài từ trang 11-18]

PHỤ LỤC A
A.1 NGHIÊN CỨU TRƯỜNG HỢP
[Các bảng so sánh từ Bảng 3-8 với các ví dụ cụ thể về hiệu suất của các mô hình khác nhau trên các bài toán toán học]
