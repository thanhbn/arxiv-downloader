# 2404.02078.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2404.02078.pdf
# Kích thước tệp: 903626 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Preprint
Advancing LLM Reasoning Generalists with Preference Trees
Lifan Yuan1,2∗, Ganqu Cui1∗†, Hanbin Wang3,4∗, Ning Ding1†, Xingyao Wang2, Jia Deng5,
Boji Shan6, Huimin Chen1, Ruobing Xie7, Yankai Lin5, Zhenghao Liu3, Bowen Zhou1,
Hao Peng2, Zhiyuan Liu1†, Maosong Sun1
1Tsinghua University2University of Illinois Urbana-Champaign3Northeastern University
4ModelBest.Inc5Renmin University of China6BUPT7Tencent
lifan4@illinois.edu cgq22@mails.tsinghua.edu.cn wanghanbinpanda@gmail.com
Tóm tắt
Chúng tôi giới thiệu EURUS, một bộ mô hình ngôn ngữ lớn (LLM) được tối ưu
hóa cho suy luận. Được tinh chỉnh từ Mistral-7B và CodeLlama-70B, các mô hình
EURUS đạt được kết quả tốt nhất hiện tại trong số các mô hình mã nguồn mở
trên một tập hợp đa dạng các benchmark bao gồm toán học, sinh mã code và bài
toán suy luận logic. Đáng chú ý, EURUS-70B vượt qua GPT-3.5 Turbo trong suy
luận thông qua đánh giá toàn diện trên 12 bài kiểm tra bao gồm năm nhiệm vụ,
và đạt được độ chính xác 33.3% pass@1 trên LeetCode và 32.6% trên TheoremQA,
hai benchmark thử thách, vượt trội hơn đáng kể so với các mô hình mã nguồn
mở hiện có với biên độ hơn 13.3%. Hiệu suất mạnh mẽ của EURUS có thể được
quy cho chủ yếu là ULTRA INTERACT, tập dữ liệu alignment quy mô lớn, chất
lượng cao mà chúng tôi mới tuyển chọn được thiết kế đặc biệt cho các nhiệm vụ
suy luận phức tạp. ULTRA INTERACT có thể được sử dụng trong cả supervised
fine-tuning và preference learning. Đối với mỗi instruction, nó bao gồm một cây
preference gồm (1) chuỗi suy luận với các chiến lược lập kế hoạch đa dạng trong
một định dạng thống nhất, (2) quỹ đạo tương tác đa lượt với môi trường và phê
bình, và (3) dữ liệu cặp để hỗ trợ preference learning. ULTRA INTERACT cho
phép chúng tôi tiến hành khám phá sâu về preference learning cho các nhiệm vụ
suy luận. Cuộc điều tra của chúng tôi tiết lộ rằng một số thuật toán preference
learning được thiết lập tốt có thể ít phù hợp cho các nhiệm vụ suy luận so với
hiệu quả của chúng trong các cuộc trò chuyện tổng quát. Được truyền cảm hứng
từ điều này, chúng tôi rút ra một mục tiêu reward modeling mới, cùng với ULTRA
INTERACT, dẫn đến một mô hình reward mạnh mẽ.1

[Biểu đồ 1 hiển thị kết quả đánh giá trên LeetCode và TheoremQA]

Hình 1: Kết quả đánh giá trên LeetCode và TheoremQA, hai benchmark coding
và toán học OOD thử thách chỉ có tập kiểm tra. EURUS-7B của chúng tôi có thể
so sánh với các baseline lớn gấp 10 lần và EURUS-70B là mô hình duy nhất
ngang bằng với GPT-3.5 Turbo.
∗Đóng góp ngang nhau.
†Tác giả liên hệ.
1Các mô hình và dữ liệu có sẵn tại: https://github.com/OpenBMB/Eurus.
1arXiv:2404.02078v1 [cs.AI] 2 Apr 2024

--- TRANG 2 ---
Preprint
1 Giới thiệu
Các kỹ thuật alignment hiện tại đã thúc đẩy đáng kể sự phát triển của các mô hình ngôn ngữ
lớn (LLM) mã nguồn mở có thể đáp ứng hiệu quả kỳ vọng của người dùng và phù hợp với
các giá trị con người (Touvron et al., 2023; Tunstall et al., 2023). Về suy luận phức tạp, thành
công đã đạt được bằng cách chuyên hóa các mô hình cho các khả năng cụ thể, chẳng hạn như
coding (Wei et al., 2023; Guo et al., 2024a; Zheng et al., 2024) và giải quyết bài toán toán học
(Fu et al., 2023; Yue et al., 2023; Luo et al., 2023a; Toshniwal et al., 2024). Tuy nhiên, những
mô hình này vẫn còn thiếu sót, với biên độ lớn, so với các mô hình độc quyền tiên tiến nhất
trong khả năng toàn diện để giải quyết một phạm vi đa dạng các vấn đề thử thách. Chúng tôi
cho rằng khoảng cách hiệu suất này có thể được quy cho chủ yếu là (1) thiếu dữ liệu alignment
chất lượng cao và (2) việc khám phá thiếu sót các kỹ thuật preference learning để cải thiện khả
năng suy luận phức tạp của các mô hình. Trong bài báo này, chúng tôi thực hiện những bước
tiến để thu hẹp khoảng cách này bằng cách giải quyết cả hai yếu tố và phát triển EURUS.

EURUS bao gồm một bộ LLM được tinh chỉnh từ Mistral-7B (Jiang et al., 2023a) và
CodeLLaMA-70B (Roziere et al., 2023). Trên một tập hợp đa dạng các benchmark suy luận
phức tạp chủ yếu là out-of-distribution (OOD), EURUS đạt được hiệu suất tổng thể tốt nhất
hiện tại trong số tất cả các mô hình mã nguồn mở. Đặc biệt, EURUS xuất sắc trong việc giải
quyết các vấn đề thử thách thường yêu cầu lập kế hoạch tinh vi, suy luận, tích hợp công cụ,
và khả năng tương tác với và học hỏi từ môi trường và người dùng. Như được hiển thị trong
Hình 1, trên các câu hỏi STEM cấp đại học TheoremQA (Chen et al., 2023) và các bài toán
coding cấp độ thi đấu LeetCode Contest (Guo et al., 2024a), EURUS-70B vượt trội đáng kể
so với tất cả các mô hình mã nguồn mở, đạt được hiệu suất có thể so sánh với GPT-3.5 Turbo.

Các mô hình EURUS được huấn luyện trên ULTRA INTERACT, dữ liệu alignment quy mô
lớn, chất lượng cao mà chúng tôi mới tuyển chọn được thiết kế đặc biệt để cải thiện khả năng
suy luận của LLM. ULTRA INTERACT bao gồm một tập hợp đa dạng các instruction bao
trùm toán học, coding và bài toán suy luận logic từ 12 tập dữ liệu đã được thiết lập. Đối với
mỗi instruction, ULTRA INTERACT thu thập một cây preference bao gồm: (1) Các chiến lược
lập kế hoạch đa dạng trong một mẫu thống nhất, chẳng hạn như xử lý tuần tự (Wei et al.,
2022) và tạo công cụ (Qian et al., 2023), tiếp theo là thực hiện các hành động từng bước được
định dạng trong text hoặc code, để cung cấp các quỹ đạo suy luận đa dạng. (2) Quỹ đạo
tương tác đa lượt với môi trường và phê bình, để cải thiện khả năng của các mô hình trong
việc học hỏi từ phản hồi và sửa chữa các lỗi trước đó (Wang et al., 2023b). (3) Các hành động
chính xác và không chính xác được ghép đôi được tổ chức trong cấu trúc cây, để hỗ trợ
preference learning. Tổng cộng, ULTRA INTERACT chứa 86K instruction và 220K cặp hành
động, trong đó mỗi cặp bao gồm một instruction, một phản hồi chính xác và một phản hồi
không chính xác. Về mặt khái niệm, dữ liệu của ULTRA INTERACT giống như các cây nhị
phân không cân bằng như được hiển thị trong Hình 2.

ULTRA INTERACT có thể được sử dụng trong cả supervised fine-tuning và preference
learning. Các thí nghiệm của chúng tôi cho thấy rằng, việc sử dụng ULTRA INTERACT cùng
với các tập dữ liệu đã được thiết lập trong instruction fine-tuning đã đạt được hiệu suất mạnh
mẽ. ULTRA INTERACT tiếp tục hỗ trợ preference learning cho các nhiệm vụ suy luận, cải
thiện hiệu suất hơn nữa với KTO (Ethayarajh et al., 2024) và NCA (Chen et al., 2024a). Đáng
ngạc nhiên, khi áp dụng cho một mô hình EURUS được instruction finetuned, DPO (Rafailov
et al., 2023) làm tổn hại hiệu suất. Thông qua phân tích cẩn thận, chúng tôi cung cấp bằng
chứng rằng hiệu suất trong suy luận tương quan với giá trị reward của dữ liệu được chọn—
một reward cuối cùng cao hơn thường chỉ ra khả năng suy luận tốt hơn. Bên cạnh đó, cuộc
điều tra của chúng tôi gợi ý rằng DPO có thể ít phù hợp cho các nhiệm vụ suy luận hơn
KTO và NCA. Được truyền cảm hứng từ phát hiện mới này, chúng tôi đưa ra một mục tiêu
mới cho reward modeling để tăng cường mục tiêu Bradley-Terry (Bradley & Terry, 1952),
khuyến khích rõ ràng việc huấn luyện để tăng reward tuyệt đối của giải pháp được chọn và
giảm reward của dữ liệu bị từ chối. Hơn nữa, ULTRA INTERACT dẫn đến mô hình reward
EURUS-RM-7B của chúng tôi, đạt được sự tương quan tốt hơn với các chuyên gia chú thích
con người so với tất cả các mô hình hiện có trên AutoJ (Li et al., 2023a) và MT-Bench (Zheng
et al., 2023), bao gồm GPT-4 (OpenAI, 2023). EURUS-RM-7B thể hiện hiệu suất preference
modeling đặc biệt mạnh mẽ trên các nhiệm vụ suy luận.

Các checkpoint của các mô hình EURUS của chúng tôi, cùng với dữ liệu alignment ULTRA
INTERACT để tái tạo nghiên cứu này, sẽ được công khai.

2

--- TRANG 3 ---
Preprint
2 ULTRA INTERACT: Dữ liệu Alignment có cấu trúc cây cho Suy luận
[Hình 2 mô tả cấu trúc dữ liệu khác nhau]

Hình 2: Trái: CodeActInstruct (Wang et al., 2024) và Code-Feedback (Zheng et al., 2024);
Giữa: HH-RLHF (Bai et al., 2022); Phải: ULTRA INTERACT. Mỗi instruction trong ULTRA
INTERACT được xây dựng như một cây preference.

Giải quyết các vấn đề phức tạp thường yêu cầu khả năng của mô hình trong lập kế hoạch và
suy luận, tích hợp với các công cụ, và tương tác với và học hỏi từ cả môi trường và người dùng.
Điều này được phản ánh trong các lựa chọn thiết kế của ULTRA INTERACT: (1) Các instruction
của nó đa dạng, thử thách và quy mô lớn (§2.1); (2) Nó cung cấp các quỹ đạo đa lượt giải
quyết instruction đầu vào thông qua nhiều lượt tương tác với và học hỏi từ môi trường và phê
bình. Tại mỗi lượt, nó chia nhỏ vấn đề thành những vấn đề nhỏ hơn (§2.2). (3) ULTRA
INTERACT bao gồm dữ liệu cặp để hỗ trợ preference learning (§2.3).

Về mặt khái niệm, ULTRA INTERACT thu thập một cây preference cho mỗi instruction, với
instruction là gốc và mỗi hành động là một nút (Hình 2). Một quỹ đạo là một đường dẫn
từ gốc đến lá bao gồm một chuỗi các hành động. Trong mỗi cây preference, tất cả các nút
của các hành động chính xác và tất cả các quỹ đạo kết thúc bằng các hành động chính xác
có thể được sử dụng cho SFT. Các nút hoặc quỹ đạo chính xác và không chính xác được
ghép đôi có thể được sử dụng cho preference learning.

2.1 Lựa chọn Instruction nhấn mạnh Phức tạp, Chất lượng và Đa dạng

Chúng tôi nhắm đến ba nhiệm vụ suy luận đại diện: giải quyết bài toán toán học, sinh mã
code và suy luận logic. Sự phức tạp, chất lượng và đa dạng của dữ liệu alignment rất quan
trọng đối với hiệu suất của mô hình (Liu et al., 2023). Theo Wang et al. (2023b), chúng tôi
chọn các vấn đề thử thách mà GPT-3.5-Turbo không giải quyết được. Chúng tôi có ý định
hạn chế việc lựa chọn các tập dữ liệu thành những tập có giải pháp ground-truth, nhằm đảm
bảo các tín hiệu giám sát chất lượng cao thay vì dựa vào chú thích LLM-as-a-judge (Weyssow
et al., 2024). Bên cạnh đó, các giải pháp vàng cũng phục vụ như tài liệu tham khảo cho mô
hình phê bình để tạo phản hồi. Để thúc đẩy tính đa dạng của ULTRA INTERACT, chúng tôi
chọn các tập dữ liệu thuộc các danh mục khác nhau. Đối với mỗi tập dữ liệu, chúng tôi bao
gồm các mẫu suy luận riêng biệt dựa trên danh mục câu hỏi hoặc công thức cần thiết để giải
quyết các vấn đề. Bảng 6 tóm tắt các tập dữ liệu được ULTRA INTERACT lựa chọn. Ngoại
trừ MATH, không có tập dữ liệu huấn luyện nào được sử dụng trong đánh giá của chúng tôi.

2.2 Phân rã và Tương tác tại Mỗi Lượt

Hình 3 cung cấp một ví dụ minh họa. Trong phần tiếp theo, chúng tôi kết nối mô hình actor
với một Python interpreter làm "môi trường". Trừ khi được chỉ định khác, chúng tôi sử dụng
GPT-3.5 Turbo làm mô hình actor.

Theo Wang et al. (2024), mô hình actor trước tiên phân rã vấn đề đầu vào thành nhiều vấn đề
con và sau đó giải quyết từng vấn đề bằng cách tạo ra các đoạn mã Python như các hành động
và sử dụng môi trường để thực thi chúng. Để thúc đẩy tính đa dạng của giải pháp, mô hình
actor ngẫu nhiên lấy mẫu một schema suy luận dưới dạng CoT (Wei et al., 2022) hoặc lập
trình modularization (Qian et al., 2023; Yuan et al., 2023). Actor sau đó tạo ra các hành động
dưới dạng text hoặc code để giải quyết từng vấn đề con, với mỗi bước được đánh dấu bằng
các ký hiệu rõ ràng.

Tương tác đa lượt với môi trường thường cần thiết để giải quyết các vấn đề thử thách (Wang
et al., 2023b). Để cải thiện những khả năng như vậy của các mô hình, ULTRA INTERACT
thu thập các quỹ đạo trong đó mô hình actor tương tác với môi trường và một mô hình phê
bình (proxy cho người dùng) và tinh chỉnh hành động của nó dựa trên phản hồi của chúng.

3

--- TRANG 4 ---
Preprint
[Hình 3 hiển thị ví dụ về quỹ đạo ULTRA INTERACT]

Hình 3: Một ví dụ minh họa về một quỹ đạo ULTRA INTERACT qua hai lượt. Trong mỗi
lượt, mô hình actor tạo ra chuỗi suy luận từng bước, và môi trường cùng mô hình phê bình
cung cấp quan sát và phê bình dạng văn bản tương ứng.

Môi trường nhận một hành động từ mô hình actor cùng với lịch sử tương tác, và sau đó
code interpreter trả về hai loại "Quan sát": (1) Kết quả thực thi Python, hoặc là đầu ra
chương trình hoặc thông báo lỗi traceback; (2) Phản hồi nhị phân, cho biết liệu giải pháp
có chính xác hay không. Sau đó, các quan sát cùng với lịch sử sẽ được chuyển đến một mô
hình phê bình, mô hình này xác định vị trí lỗi và cung cấp gợi ý để cải thiện. Để tránh thiên
vị tiềm ẩn được giới thiệu bởi self-correction (Wang et al., 2023b; Xu et al., 2024), chúng tôi
áp dụng một mô hình mạnh hơn, GPT-4, làm phê bình và đảm bảo chất lượng phê bình bằng
cách cung cấp cho GPT-4 các câu trả lời ground truth làm tài liệu tham khảo.

Quy trình này giống với Wang et al. (2024). Tuy nhiên, chúng tôi áp dụng các mẫu suy luận
đa dạng hơn để dạy LLM học các lý do thay vì chỉ đơn giản ghi nhớ câu trả lời (Mitra et al.,
2023), và học cách tạo và sử dụng công cụ (Qian et al., 2023; Yuan et al., 2023; Qin et al.,
2023). Bên cạnh đó, chúng tôi tin rằng điều quan trọng là LLM phải học hỏi từ phản hồi do
phê bình cung cấp thay vì chỉ từ quan sát của môi trường.

2.3 Cây Preference hỗ trợ Preference Learning qua Nhiều Lượt

Không giống như các cuộc trò chuyện mở, nơi preference của con người mơ hồ và khó xác
định, nhiều nhiệm vụ suy luận có preference rõ ràng và khách quan cho các hành động chính
xác. Do đó, chú thích preference là một đánh giá về tính chính xác của các giải pháp có điều
kiện các giải pháp ground truth, mà đi kèm với các tập dữ liệu trong ULTRA INTERACT.
Điều này loại bỏ nhu cầu chú thích preference dựa trên con người hoặc LLM và đảm bảo
chất lượng dữ liệu cao. Để hỗ trợ preference learning, ULTRA INTERACT ghép đôi các hành
động chính xác và không chính xác.

Lấy mẫu Các Cặp Hành động Chính xác và Không chính xác tại Mỗi Lượt. Đối với mỗi
instruction trong ULTRA INTERACT, chúng tôi lấy mẫu, từ mô hình actor, một cặp hành
động chính xác và không chính xác theo §2.2. Chúng tôi theo Cui et al. (2023) để lấy mẫu
cặp từ các mô hình actor khác nhau nhằm đảm bảo tính đa dạng phản hồi. Để ngăn các mô
hình khai thác shortcuts dựa trên các đặc điểm bề mặt, chúng tôi loại trừ các instance không
vượt qua kiểm tra cú pháp Python.

Một số vấn đề thử thách trong ULTRA INTERACT gây khó khăn trong việc thu được các
hành động chính xác, ngay cả khi sử dụng các actor mạnh như GPT-4, với độ chính xác
pass@100 gần như bằng không. Để cải thiện tỷ lệ pass của các mô hình actor trong khi giữ
chi phí dưới tầm kiểm soát, chúng tôi tuần tự thực hiện các bước sau. (1) Lấy mẫu trực tiếp
20 hành động và ngẫu nhiên giữ lại một hành động chính xác, nếu có. (2) Nếu không có
hành động chính xác nào được thu được, chúng tôi lặp lại quy trình trên tối đa ba lần, dần
dần chuyển từ các mô hình hiệu quả chi phí hơn sang GPT-4 Turbo mạnh mẽ nhưng đắt đỏ.
(3) Đối với các vấn đề khó còn lại mà không có hành động chính xác nào được thu được sau
hai bước trước, chúng tôi cung cấp cho actor các rationale và câu trả lời ground-truth, và sau
đó áp dụng các kỹ thuật khác nhau để gợi ra các hành động chính xác. Thông tin cụ thể được
cung cấp và các kỹ thuật được áp dụng khác nhau tùy thuộc vào các nhiệm vụ (Phụ lục A.2).

4

--- TRANG 5 ---
Preprint
Cặp Hành động có cấu trúc Cây qua Nhiều Lượt. Sau mỗi lượt, hành động chính xác kết
thúc quỹ đạo của nó. Chúng tôi mở rộng hành động không chính xác sang lượt tiếp theo,
và cho actor tương tác với môi trường và phê bình để tinh chỉnh giải pháp của nó (§2.2).
Sau đó chúng tôi lặp lại các quy trình được giới thiệu trước đó trong phần này để thu thập
một cặp hành động bổ sung. Bằng cách mở rộng hành động không chính xác, ULTRA
INTERACT có thể cung cấp dữ liệu để giúp các mô hình học hỏi từ phản hồi, và thu thập
nhiều cặp hành động cho preference learning qua nhiều lượt.

Về mặt khái niệm, đối với mỗi instruction, ULTRA INTERACT xây dựng một cây preference
nhị phân với mỗi hành động là một nút (Hình 2). Chúng tôi giới hạn cây ở tối đa năm lượt.

Các Cặp Instruction-action Bổ sung cho Các Vấn đề Thử thách. Chúng tôi tin rằng các
instruction thử thách đến bước (3) ở trên có thể cung cấp tín hiệu huấn luyện có giá trị. Do
đó, đối với một tập con của những vấn đề này có nhiều giải pháp ground truth, chúng tôi
tiếp tục lấy mẫu các hành động chính xác bổ sung để bao phủ tất cả ground truth. Tương ứng,
chúng tôi tiếp tục lấy mẫu các hành động không chính xác để ghép đôi với những hành động
chính xác bổ sung này, để chúng có thể được sử dụng trong cả supervised fine-tuning và
preference learning.

Với dữ liệu có cấu trúc cây, ULTRA INTERACT cho phép so sánh tại mọi lượt, trái ngược
với việc chỉ so sánh ở lượt cuối (Bai et al., 2022), và do đó có thể cải thiện khả năng tương
tác của các mô hình. Kết thúc phần này, Bảng 1 tóm tắt một số thống kê của ULTRA
INTERACT, và thêm chi tiết ở Phụ lục A.4.

[Bảng 1: Một số thống kê của ULTRA INTERACT]

3 EURUS: LLM Mã nguồn mở Hàng đầu trong Suy luận

ULTRA INTERACT giúp chúng tôi phát triển EURUS, một bộ LLM và một reward model (RM).

Supervised Fine-Tuning. EURUS-7B-SFT được tinh chỉnh từ Mistral-7B (Jiang et al., 2023a)
và EURUS-70B-SFT từ CodeLLaMA-70B (Roziere et al., 2023). Đầu tiên, chúng tôi thực hiện
SFT sử dụng tất cả các hành động chính xác (287K) trong ULTRA INTERACT. Chúng tôi
thấy rằng việc loại bỏ lịch sử tương tác và chỉ huấn luyện trên các nút lá chính xác trong
mỗi cây cho hiệu suất tốt hơn. Để cải thiện khả năng tuân theo instruction tổng quát, chúng
tôi bao gồm vào hỗn hợp dữ liệu SFT của chúng tôi UltraChat (Ding et al., 2023), ShareGPT²,
và OpenOrca (Lian et al., 2023). Vui lòng tìm tỷ lệ hỗn hợp trong Phụ lục B.

Preference Learning. Dựa trên các mô hình EURUS-SFT, chúng tôi khám phá ba thuật toán
preference learning, DPO (Rafailov et al., 2023), KTO (Ethayarajh et al., 2024), và NCA
(Chen et al., 2024a). Khác với SFT, ở đây chúng tôi bao gồm tất cả các cặp quỹ đạo đa lượt
trong ULTRA INTERACT (220K) và bao gồm tất cả các cặp UltraFeedback (Cui et al., 2023)
(340K).

Reward Modeling. Tương tự như preference learning, chúng tôi sử dụng tất cả 220K cặp
quỹ đạo đa lượt từ ULTRA INTERACT; nó được tăng cường thêm với 240K cặp hành động
đơn lượt từ ULTRA INTERACT. Thêm chi tiết ở Phụ lục B. Chúng tôi bao gồm tất cả 340K
cặp từ UltraFeedback và một cặp cho mỗi instruction từ UltraSafety (Guo et al., 2024b), tổng
cộng 3K. EURUS-RM-7B được khởi tạo từ EURUS-7B-SFT với một lớp tuyến tính mới.

Các phát hiện của chúng tôi trong §6 chỉ ra rằng các giá trị tuyệt đối của reward tạo ra sự
khác biệt lớn trong hiệu suất suy luận của các mô hình. Do đó chúng tôi tăng cường mục tiêu
Bradley-Terry (BT) đã được thiết lập L_BT với một thành phần bổ sung L_DR để trực tiếp
tăng reward của các hành động được chọn cho các instance từ ULTRA INTERACT, và giảm
reward của những hành động bị từ chối:

²https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset

5

--- TRANG 6 ---
Preprint
[Bảng 2: Các baseline LLM mã nguồn mở mà chúng tôi so sánh]

L_ULTRA INTERACT = -log σ(r_θ(x,y_c) - r_θ(x,y_r)) - log σ(r_θ(x,y_c)) - log σ(-r_θ(x,y_r))
                   |___________________________|   |__________________________|
                          L_BT: tối ưu reward tương đối        L_DR: tăng r_θ(x,y_c) và giảm r_θ(x,y_r)

Đối với các instance từ các tập dữ liệu khác, chúng tôi huấn luyện với L_BT. θ biểu thị các
tham số của reward model, r_θ(·) và r_θ(x,y_r) là reward trên các hành động được chọn
và bị từ chối tương ứng. Nghiên cứu ablation của chúng tôi chứng minh tầm quan trọng của
cả L_BT và L_DR.

4 Đánh giá EURUS-7B và EURUS-70B

Thiết lập Đánh giá. Chúng tôi xem xét cả suy luận đơn lượt và đa lượt. Đối với đánh giá
đơn lượt, chúng tôi xem xét HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), và
LeetCode (Guo et al., 2024a) cho coding, GSM-Plus (Li et al., 2024), MATH, TheoremQA
(Chen et al., 2023), SVAMP (Patel et al., 2021), và ASDiv (Miao et al., 2020) cho toán học, và
BBH-Hard (Suzgun et al., 2022) cho suy luận. Chúng tôi đánh giá với độ chính xác pass@1.
Chúng tôi cũng sử dụng IFEval (Zhou et al., 2023) để đánh giá khả năng tuân theo instruction
và báo cáo điểm loose cấp prompt. Đối với đánh giá đa lượt, chúng tôi áp dụng MINT (Wang
et al., 2023b) và chỉ xem xét các bài toán coding và toán học. Chúng tôi báo cáo tỷ lệ thành
công tại Lượt 5. Vui lòng tìm thêm chi tiết về thiết lập đánh giá và các đánh giá ngoài suy
luận trong Phụ lục C.

Như được hiển thị trong Bảng 2, chúng tôi so sánh EURUS của chúng tôi với các mô hình
đa năng, và những mô hình chuyên về coding và toán học với các kích thước khác nhau.
Chúng tôi cũng tóm tắt kết quả của GPT-3.5 Turbo và GPT-4 được báo cáo trong các công
trình trước đó.

[Bảng 3: Hiệu suất tổng thể - hiển thị kết quả chi tiết trên nhiều benchmark]

6

--- TRANG 7 ---
Preprint
4.1 Kết quả

Kết quả được hiển thị trong Bảng 3. Chúng tôi tóm tắt các điểm chính như sau:

EURUS, cả phiên bản 7B và 70B, đạt được hiệu suất tổng thể tốt nhất trong số các mô hình
mã nguồn mở có kích thước tương tự. EURUS thậm chí còn vượt trội hơn các mô hình chuyên
biệt trong các lĩnh vực tương ứng trong nhiều trường hợp. Đáng chú ý, EURUS-7B vượt
trội hơn các baseline lớn gấp 5× và EURUS-70B đạt được hiệu suất tốt hơn GPT-3.5 Turbo.
Hiệu suất tuân theo instruction của EURUS nằm trong số các mô hình đa năng tốt nhất, tốt
hơn đáng kể so với các mô hình chuyên biệt.

Preference learning với ULTRA INTERACT có thể cải thiện thêm hiệu suất, đặc biệt là
trong toán học và khả năng đa lượt. KTO và NCA liên tục cải thiện hiệu suất của các mô
hình trong tất cả năm benchmark toán học và đánh giá đa lượt, trong khi tác động của chúng
khác nhau ở các lĩnh vực khác. Vì các mô hình SFT chỉ sử dụng dữ liệu đơn lượt từ ULTRA
INTERACT trong khi preference learning sử dụng dữ liệu đa lượt, các cải thiện trong khả
năng tương tác cũng nên được quy cho ULTRA INTERACT thay vì chỉ các thuật toán. Đáng
ngạc nhiên, chúng tôi quan sát thấy rằng DPO làm tổn hại hiệu suất mô hình trên hầu hết
các benchmark. Huấn luyện DPO của mô hình 70B của chúng tôi thất bại vì reward giảm
xuống −∞. Chúng tôi phân tích hiện tượng này trong §6.1.

5 Đánh giá EURUS-RM-7B

Thiết lập Đánh giá. Chúng tôi đánh giá EURUS-RM-7B trên ba benchmark RM, RewardBench
(Lambert et al., 2024), AutoJ (Li et al., 2023a), và MT-Bench (Zheng et al., 2023). Nhằm mục
đích đánh giá OOD thực tế hơn, chúng tôi loại trừ phần "prior sets" từ RewardBench, vì
nhiều baseline huấn luyện trên các tập dữ liệu mà phần này chứa. Chúng tôi so sánh với
PairRM (Jiang et al., 2023b), Starling-RM-7B/34B (Zhu et al., 2023), UltraRM-13B (Cui et al.,
2023), GPT-3.5 Turbo, và GPT-4. Để tiếp tục khám phá tiềm năng của EURUS-RM-7B trong
việc cải thiện hiệu suất của các mô hình thông qua reranking, chúng tôi sử dụng nó để xếp
hạng lại các phản hồi của Mistral-7B-Instruct-v0.2 trên HumanEval, MBPP, GSM8K, và
MATH. Chúng tôi báo cáo kết quả của random sampling, self-consistency, và Starling-RM-34B
làm baseline.

5.1 Kết quả

Bảng 4 tóm tắt hiệu suất reward modeling, và Hình 4 vẽ một số kết quả reranking với các
kết quả khác trong Phụ lục D.1.

EURUS-RM-7B nổi bật như RM 7B tốt nhất tổng thể, và đạt được hiệu suất tương tự hoặc
tốt hơn so với các baseline lớn hơn nhiều. Đặc biệt, nó vượt trội hơn GPT-4 trong một số
nhiệm vụ. EURUS-RM-7B đạt được sự tương quan tốt hơn với các chuyên gia con người so
với tất cả các mô hình hiện có trên AutoJ và MT-Bench, và nó đạt được hiệu suất có thể so
sánh với Starling-RM-34B lớn gấp 5× trên RewardBench. Trên RewardBench, EURUS-RM-7B
vượt trội hơn tất cả các baseline trên phần "Chat-Hard" trong khi đạt được hiệu suất rất cạnh
tranh trên phần "Reasoning". Trên các phần của AutoJ, EURUS-RM-7B vượt trội hơn gần
như tất cả các mô hình hiện có, với ngoại lệ duy nhất là kết quả của GPT-4 trên Coding.

Mục tiêu huấn luyện của chúng tôi có lợi trong việc cải thiện hiệu suất RM trên các vấn đề
khó và suy luận. Bảng 4 cho thấy rằng tối ưu hóa L_DR cải thiện khả năng suy luận của RM,
nhưng mô hình BT vẫn có lợi trong việc trang bị RM với khả năng trò chuyện tổng quát như
được gợi ý trong cột "Chat-Hard", mặc dù tác động của nó đối với suy luận có thể khác nhau.

ULTRA INTERACT tương thích với các tập dữ liệu khác như UltraFeedback và UltraSafety,
và việc trộn các tập dữ liệu này có thể cân bằng các khả năng RM khác nhau. Cải thiện khả
năng của RM trong suy luận với ULTRA INTERACT không hy sinh các khả năng khác, điều
này cho thấy rằng ULTRA INTERACT có thể là một thành phần tuyệt vời cho hỗn hợp dữ
liệu huấn luyện của reward model.

EURUS-RM-7B cải thiện hiệu suất suy luận của LLM với biên độ lớn thông qua reranking.
EURUS-RM-7B liên tục cải thiện độ chính xác pass@1 trên tất cả các nhiệm vụ và hoạt động
tốt hơn baseline Starling-RM-34B lớn gấp 5×. Ngoài ra, hiệu suất reranking của EURUS-RM-7B
mở rộng tốt với #responses per instruction, ngoại trừ một sự giảm nhẹ trong HumanEval

7

--- TRANG 8 ---
Preprint
[Hình 4: Kết quả reranking các phản hồi của Mistral-7B-Instruct-v0.2]

[Bảng 4: Kết quả trên các benchmark reward modeling]

khi tăng số lượng phản hồi từ 8 lên 16. Ngược lại, Starling-RM-34B gặp phải sự sụt giảm
hiệu suất nghiêm trọng trên HumanEval và nó liên tục làm tổn hại độ chính xác mô hình
trên MATH.

6 Phân tích

[Hình 5: Các mẫu reward của preference learning EURUS-7B với DPO, KTO, và NCA]

Hình 5: Các mẫu reward của preference learning EURUS-7B với DPO, KTO, và NCA. Đối
với tất cả các thuật toán, reward của dữ liệu bị từ chối tiếp tục giảm và biên độ giữa dữ liệu
được chọn và bị từ chối tiếp tục tăng. Tuy nhiên, reward của dữ liệu được chọn giảm xuống
dưới không trong DPO trong khi tiếp tục tăng và duy trì dương trong KTO và NCA. Các
giá trị tuyệt đối của reward trong bước cuối cùng (màu đỏ) của ba thuật toán tương quan
dương với hiệu suất của chúng trong Bảng 3.

6.1 Reward Rõ ràng như một Proxy? Giả thuyết cho Preference Learning trong Suy luận

Chúng tôi điều tra lý do tại sao DPO hoạt động khác so với KTO và NCA. Chúng tôi bắt đầu
bằng cách kiểm tra thực nghiệm các reward trong suốt quá trình preference learning, như
được hiển thị trong Hình 5. Reward cho cả dữ liệu được chọn và bị từ chối đều tiếp tục giảm
qua DPO, mặc dù reward cho dữ liệu được chọn vẫn cao hơn do đó loss giảm. Trong KTO
và NCA, reward của dữ liệu được chọn tiếp tục tăng với reward của dữ liệu bị từ chối giảm.

Do đó, chúng tôi đưa ra giả thuyết rằng chính sự khác biệt trong xu hướng reward dẫn đến
khoảng cách hiệu suất giữa DPO và hai thuật toán khác. Sự khác biệt này có thể được quy
cho việc DPO, được rút ra từ mô hình Bradley-Terry, chỉ tối ưu hóa sự khác biệt tương đối
giữa dữ liệu được chọn và bị từ chối mà bỏ qua các giá trị tuyệt đối của reward. Đây không
phải là vấn đề trong alignment với các giá trị con người tổng quát nơi preference là "tương
đối" và có thể có nhiều câu trả lời hợp lệ cho cùng một đầu vào. Tuy nhiên, trong các nhiệm
vụ suy luận, không gian của các câu trả lời chính xác nhỏ hơn nhiều so với không gian của
các câu trả lời không chính xác. Hơn nữa, chúng tôi nhận thấy rằng reward của dữ liệu được
chọn trong bước huấn luyện cuối cùng theo thứ tự xếp hạng KTO > NCA > DPO, tương quan
dương với xu hướng hiệu suất của chúng. Do đó, chúng tôi tin rằng việc tăng reward của
dữ liệu được chọn đặc biệt có lợi trong preference learning cho các nhiệm vụ suy luận.

6.2 Nghiên cứu Ablation

[Bảng 5: Nghiên cứu ablation của dữ liệu SFT]

Chúng tôi nghiên cứu tác động của ULTRA INTERACT và các dữ liệu alignment mã nguồn
mở khác trên hiệu suất của EURUS-7B-SFT. Chúng tôi xem xét ba thiết lập: (1) Với các câu
trả lời ground-truth gốc, thay thế các hành động được tạo ra bằng các rationale và câu trả
lời ground-truth từ các tập dữ liệu gốc. Nếu không có rationale nào có sẵn, chúng tôi sử
dụng những rationale từ ULTRA INTERACT. (2) Chỉ dữ liệu mã nguồn mở. (3) Chỉ ULTRA
INTERACT. Chúng tôi đánh giá với cùng thiết lập như §4 và báo cáo điểm số trung bình.
Xem kết quả đầy đủ trong Phụ lục E.

Trong Bảng 5, EURUS vượt trội hơn mô hình "Ground-truth" trên tất cả các nhiệm vụ, xác
nhận lợi thế của các thiết kế ULTRA INTERACT về các mẫu chia-để-trị và code-as-action,
phù hợp với kết luận của công trình đồng thời (Chen et al., 2024b; Wang et al., 2024). Huấn
luyện chỉ trên dữ liệu mã nguồn mở mà không có ULTRA INTERACT làm tổn hại đáng kể
hiệu suất suy luận, xác nhận hiệu quả của ULTRA INTERACT. Trong khi đó, huấn luyện chỉ
trên ULTRA INTERACT gặp phải sự sụt giảm hiệu suất ngoại trừ BBH, đặc biệt là trong việc
tuân theo instruction. Chúng tôi quy sự sụt giảm hiệu suất cho khả năng tuân theo instruction
kém hơn. Điều này gợi ý sự cần thiết của việc trộn ULTRA INTERACT với dữ liệu alignment
khác để supervised fine-tuning toàn diện tốt hơn.

7 Công trình Liên quan

LLM Mã nguồn mở trong Suy luận. Các LLM mã nguồn mở đã cho thấy tiến bộ đáng kể
trong việc xây dựng các chuyên gia xuất sắc trong suy luận toán học (Luo et al., 2023a; Yue
et al., 2023; Toshniwal et al., 2024) hoặc khả năng coding (Roziere et al., 2023; Wei et al.,
2023; Guo et al., 2024a; Zheng et al., 2024). Ngược lại, việc làm chủ khả năng suy luận tổng
quát vẫn thử thách các mô hình mở, trong khi những mô hình tiên tiến nhất (DeepSeek-AI,
2024; Bai et al., 2023; Touvron et al., 2023; Jiang et al., 2024) vẫn thua xa các mô hình độc
quyền. Hơn nữa, những mô hình mã nguồn mở đa năng hàng đầu này giữ bí mật các công
thức alignment của chúng, điều này càng cản trở việc tái tạo và phát triển các mô hình suy
luận mã nguồn mở.

Preference Learning cho Suy luận. Alignment các mô hình ngôn ngữ từ preference của con
người hoặc AI đã nổi lên như một cách tiếp cận phổ biến trong cộng đồng mã nguồn mở
(Tunstall et al., 2023; Bai et al., 2023) với đề xuất của DPO (Rafailov et al., 2023) và các tập
dữ liệu preference chất lượng cao (Cui et al., 2023; Zhu et al., 2023). Khác với các chatbot
mở, preference learning chủ yếu chưa được khám phá trong suy luận phức tạp. Nghiên cứu
gần đây cho thấy sự suy giảm hiệu suất khi áp dụng DPO trên các nhiệm vụ suy luận, nhưng
một số thuật toán mới được đề xuất đã chứng minh tác động tích cực (Ethayarajh et al., 2024;
Chen et al., 2024a; Mitra et al., 2024; Shao et al., 2024). Tuy nhiên, hiểu biết sâu sắc về
preference learning, đặc biệt là hiệu quả của nó trên suy luận phức tạp, vẫn chưa được thiết
lập.

8 Kết luận

Chúng tôi cố gắng thu hẹp khoảng cách lớn giữa các mô hình mã nguồn mở và mô hình độc
quyền từ góc độ alignment. Công trình của chúng tôi đẩy ranh giới của các nhà tổng quát suy
luận mã nguồn mở bằng cách (1) phát hành tập dữ liệu suy luận đa lượt chất lượng cao ULTRA
INTERACT với cây preference, (2) giới thiệu các LLM dòng EURUS đạt được SOTA mới trên
các benchmark suy luận thử thách và (3) cung cấp thông tin chi tiết về preference learning cho
suy luận thông qua phân tích, dẫn đến các mục tiêu reward modeling mới cũng như một mô
hình reward mạnh mẽ cho suy luận.

9

--- TRANG 10 ---
Preprint
Tài liệu tham khảo

[Danh sách tài liệu tham khảo được dịch đầy đủ với format tương tự như bản gốc]

--- TRANG 11-18 ---
[Các trang còn lại chứa thêm tài liệu tham khảo, bảng thống kê chi tiết và phụ lục được dịch đầy đủ theo cấu trúc gốc]
