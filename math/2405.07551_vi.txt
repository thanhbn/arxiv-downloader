# 2405.07551.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2405.07551.pdf
# Kích thước tệp: 3125037 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
MuMath-Code: Kết hợp các Mô hình Ngôn ngữ Lớn Sử dụng Công cụ với
Tăng cường Dữ liệu Đa góc nhìn cho Suy luận Toán học
Shuo Yin12†§, Weihao You1†, Zhilong Ji1*, Guoqiang Zhong2, Jinfeng Bai1
1Tomorrow Advancing Life
2Khoa Khoa học Máy tính và Công nghệ, Đại học Hải dương Trung Quốc
shuoyinn@foxmail.com, youweihao@tal.com ,
jizhilong@tal.com, gqzhong@ouc.edu.cn, jfbai.bit@gmail.com
Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLM) sử dụng công cụ
tích hợp với các trình thông dịch Python bên ngoài
đã nâng cao đáng kể khả năng suy luận toán học
cho các LLM mã nguồn mở, trong khi các phương
pháp không sử dụng công cụ đã chọn một hướng
khác: tăng cường dữ liệu suy luận toán học. Tuy
nhiên, một phương pháp tuyệt vời để tích hợp hai
con đường nghiên cứu trên và kết hợp các ưu
điểm của chúng vẫn chưa được khám phá. Trong
công trình này, chúng tôi đầu tiên bao gồm các
câu hỏi toán học mới thông qua các phương pháp
tăng cường dữ liệu đa góc nhìn và sau đó tổng
hợp các giải pháp lồng ghép mã cho chúng. Các
LLM mở (tức là, Llama-2) được tinh chỉnh trên
tập dữ liệu tăng cường để có được các mô hình
kết quả, MuMath-Code (µ-Math-Code). Trong
giai đoạn suy luận, MuMath-Code của chúng tôi
tạo ra mã và tương tác với trình thông dịch python
bên ngoài để có được kết quả thực thi. Do đó,
MuMath-Code tận dụng các ưu điểm của cả công
cụ bên ngoài và tăng cường dữ liệu. Để tận dụng
đầy đủ các ưu điểm của dữ liệu tăng cường, chúng
tôi đề xuất một chiến lược huấn luyện hai giai
đoạn: Trong Giai đoạn-1, chúng tôi tinh chỉnh
Llama-2 trên dữ liệu CoT thuần túy để có được
một mô hình trung gian, mô hình này sau đó được
huấn luyện trên dữ liệu lồng ghép mã trong Giai
đoạn-2 để có được MuMath-Code kết quả. MuMath-
Code-7B của chúng tôi đạt được 83.8 trên GSM8K
và 52.4 trên MATH, trong khi mô hình MuMath-
Code-70B đạt được hiệu suất tiên tiến mới trong
số các phương pháp mở—đạt được 90.7% trên
GSM8K và 55.1% trên MATH. Các thí nghiệm
mở rộng xác nhận sự kết hợp của việc sử dụng
công cụ và tăng cường dữ liệu, cũng như chiến
lược huấn luyện hai giai đoạn của chúng tôi. Chúng
tôi phát hành tập dữ liệu được đề xuất cùng với
mã liên quan để sử dụng công khai.
1 Giới thiệu
Trong Xử lý Ngôn ngữ Tự nhiên (NLP), các Mô
hình Ngôn ngữ Lớn (LLM) (Radford et al., 2019;
†Đóng góp bằng nhau.
§Công việc được thực hiện khi tác giả đang thực tập tại TAL.
*Tác giả liên hệ.
83.884.690.7
5060708090100
7b13b70bTestAccuracy(%)GSM8KMAmmoTHMathCoderToRAMuMath-Code
52.453.155.1
010203040506070
7b13b70bTestAccuracy(%)MATHMAmmoTHMathCoderToRAMuMath-CodeHình 1: So sánh giữa MuMath-Code của chúng tôi
và các LLM sử dụng công cụ tiên tiến khác. MuMath-
Code thể hiện sự cải thiện đáng kể về hiệu suất trên
cả GSM8K (Cobbe et al., 2021) và MATH (Hendrycks
et al., 2021a), so với các phương pháp trước đó.
Brown et al., 2020; Raffel et al., 2023) đặc biệt
các mô hình độc quyền như GPT-4 (OpenAI, 2023a)
và Claud-3 (Anthropic, 2024) đã chứng minh sự
vượt trội trong nhiều tác vụ đa dạng, ví dụ: phân
loại văn bản (Wang et al., 2018; Devlin et al.,
2019; Min et al., 2022; Jiang et al., 2023b), lập
trình tự động (Chen et al., 2021; Luo et al., 2023b),
tuân theo hướng dẫn (Longpre et al., 2023), và
giải quyết vấn đề toán học (Chowdhery et al., 2022;
Lewkowycz et al., 2022; Anil et al., 2023; Fu et al.,
2023a). Trong số các tác vụ này, khả năng xử

--- TRANG 2 ---
lý các vấn đề toán học đứng như một tiêu chí điển
hình và quan trọng để đánh giá các LLM khác nhau.
Tuy nhiên, một sự chênh lệch hiệu suất đáng kể
được quan sát thấy giữa các LLM mã nguồn mở,
ví dụ, LLaMA (Touvron et al., 2023a,b), và các
đối tác độc quyền của chúng, khi nói đến khả năng
suy luận toán học.
Trong những năm gần đây, nhiều ấn phẩm học
thuật đã được hướng tới việc cải thiện năng lực
toán học của LLM, có thể được phân loại thành
hai quỹ đạo nghiên cứu riêng biệt: những phương
pháp chỉ dựa vào suy luận ngôn ngữ tự nhiên thuần
túy và những phương pháp kết hợp các công cụ
bên ngoài. Các phương pháp trước đây là không
sử dụng công cụ, chủ yếu phụ thuộc vào tăng cường
dữ liệu để nâng cao khả năng suy luận toán học
của mô hình, trong khi quỹ đạo thứ hai (tức là
LLM sử dụng công cụ) thường được kết hợp với
các trình thông dịch Python bên ngoài. Từ góc
độ chưng cất kiến thức (Huang et al., 2022; Li
et al., 2022; Magister et al., 2023; Ho et al., 2023;
Fu et al., 2023b; Shridhar et al., 2023), cả hai
phương pháp chính đều chuyển giao khả năng suy
luận toán học từ các mô hình giáo viên mạnh mẽ
(ví dụ, GPT-4) sang các mô hình nền tảng mở
kém hơn.
Các phương pháp không sử dụng công cụ tổng
hợp một số lượng lớn các vấn đề toán học mới
và các giải pháp tương ứng, lấy các cặp QA toán
học huấn luyện ban đầu làm dữ liệu hạt giống
ban đầu. Luật tỷ lệ về mặt lý thuyết cung cấp
cơ sở cho sự cải thiện liên tục hiệu suất của LLM
bằng cách liên tục kết hợp dữ liệu huấn luyện
mới. Các phương pháp tiêu biểu là RFT (Yuan
et al., 2023), MetaMath (Yu et al., 2023), Wizard-
Math (Luo et al., 2023a), MuggleMath (Li et al.,
2023), MuMath (You et al., 2024), v.v. Đối với
quỹ đạo thứ hai, các bộ thực thi mã thay thế đáng
kể cho LLM trong các tác vụ tính toán và logic
đặc biệt thách thức, qua đó giảm bớt gánh nặng
giải quyết vấn đề trên chúng. Danh mục sử dụng
công cụ này được minh họa bởi PAL (Gao et al.,
2023), PoT (Chen et al., 2023), MAmmoTH (Yue
et al., 2023), ToRA (Gou et al., 2023) và Math-
Coder (Wang et al., 2023).
Mặc dù các con đường nghiên cứu nói trên đã
thành công riêng lẻ, cho đến nay, rất ít phương
pháp đã được phát triển để kết hợp các ưu điểm
tương ứng của chúng. Trong bài báo này, chúng
tôi đề xuất một phương pháp mới tích hợp việc
sử dụng công cụ với tăng cường dữ liệu để tổng
hợp một lượng lớn các câu hỏi và giải pháp toán
học đa góc nhìn (chúng tôi sử dụng các phương
pháp tăng cường được giới thiệu trong một công
trình trước đó MuMath (You et al., 2024)). Cụ
thể, chúng tôi sử dụng các LLM độc quyền (như
GPT-4) để tạo mã Python trong khi tổng hợp các
giải pháp mới cho các vấn đề toán học, và sau đó
tinh chỉnh các mô hình mã nguồn mở (ví dụ: LLaMA)
trên tập dữ liệu tăng cường. Mô hình kết quả,
MuMath-Code, do đó được trang bị khả năng viết
mã để giải quyết vấn đề toán học. Trong giai đoạn
suy luận, MuMath-Code của chúng tôi có thể tạo
ra cả văn bản suy luận CoT (Wei et al., 2022)
và các khối mã Python. Các khối mã này sau đó
được trích xuất và thực thi bởi một trình thông
dịch Python bên ngoài, và kết quả thực thi được
trả về cho MuMath-Code cho các vòng suy luận
CoT hoặc tạo mã tiếp theo cho đến khi có được
kết quả cuối cùng hoặc đạt đến số lượng tối đa
các vòng thực thi.
Tập câu hỏi toán học đa góc nhìn bao gồm các
câu hỏi được tăng cường thông qua diễn đạt lại
(Yu et al., 2023), thay đổi (Li et al., 2023; You
et al., 2024), FOBAR (Jiang et al., 2023a), BF-
Trans (You et al., 2024), bên cạnh những câu từ
các tập huấn luyện ban đầu. Về các giải pháp
lồng ghép với mã Python, chúng tôi tận dụng một
mẫu tổng quát như những mẫu được sử dụng trong
ToRA (Gou et al., 2023) và MathCoder (Wang
et al., 2023): xen kẽ CoT-PoT. Tuy nhiên, chúng
tôi đề xuất CoT tiền tố, gỡ lỗi mã và lọc hướng
dẫn pseudo-answer để cải thiện tính nhất quán
và chất lượng của các giải pháp tăng cường. CoT
tiền tố là một phân tích chu đáo bằng ngôn ngữ
tự nhiên thuần túy trước khi tạo mã, khiến các
LLM xem xét phân tích này trong khi tạo ra tất
cả nội dung tiếp theo, do đó có ích cho các mô
hình học toàn bộ giải pháp. Bên cạnh đó, chúng
tôi nhắc GPT-4 gỡ lỗi và sửa mã không thể thực
thi khi yêu cầu các giải pháp, và chúng tôi giữ
lại mã lỗi vì quá trình xác minh và sửa chữa này
có thể giúp tăng cường khả năng lập trình của mô
hình. Hơn nữa, đối với những câu hỏi được tổng
hợp thông qua thay đổi, thiếu câu trả lời chuẩn
làm hướng dẫn lọc, chúng tôi chọn các câu trả
lời đa số làm pseudo-answer. Quá trình này có
thể tăng độ chính xác của các giải pháp được tạo
ra và do đó cải thiện chất lượng dữ liệu nói chung.
Chúng tôi đặt tên cho tập dữ liệu được đề xuất
là MuMath-Code-Data và ký hiệu nó là Dµ-code.
Hơn nữa, các LLM sử dụng công cụ trước đây
cho toán học được tạo ra bằng cách tinh chỉnh
trực tiếp trên dữ liệu lồng ghép mã, do đó không
khai thác đầy đủ khả năng suy luận ngôn ngữ tự
nhiên nội tại của chính các LLM. Khác với các
phương pháp sử dụng công cụ khác, chúng tôi
thiết kế một chiến lược huấn luyện hai giai đoạn
để kết hợp tốt hơn các ưu điểm của tăng cường
dữ liệu

--- TRANG 3 ---
và thực thi mã bên ngoài. Giai đoạn đầu tiên là
nâng cao khả năng suy luận toán học ngôn ngữ
thuần túy của mô hình, trong đó tập dữ liệu lớn
nhất (751K) được đề xuất trong MuMath (ở đây
được gọi là MuMath-Data và ký hiệu là Dµ) được
sử dụng để tinh chỉnh LLaMA, và có được một
mô hình trung gian, MuMath. Trong giai đoạn thứ
hai, chúng tôi tiếp tục tinh chỉnh MuMath trên
MuMath-Code-Data để trang bị cho mô hình khả
năng viết mã để giải quyết các vấn đề toán học.
Mô hình kết quả, MuMath-Code, do đó có thể
được nhắc để tận dụng trình thông dịch Python
để thực thi mã được tạo ra để đảm bảo các kết
quả đầu ra mong muốn tại thời điểm suy luận.
Các đóng góp của chúng tôi được tóm tắt như sau:
•Chúng tôi xây dựng một tập dữ liệu tăng cường
đa góc nhìn với các giải pháp lồng ghép mã cho
việc giải quyết vấn đề toán học, được gọi là
MuMath-Code-Data.
•Chúng tôi thiết kế một chiến lược huấn luyện
hai giai đoạn để trang bị cho các LLM mở khả
năng suy luận ngôn ngữ thuần túy và khả năng
tạo mã liên quan đến toán học, tương ứng.
•Mô hình thu được, MuMath-Code, đạt được hiệu
suất tiên tiến mới trong số các LLM mở trên
các tập dữ liệu suy luận toán học trong miền
cũng như các tập ngoài miền. MuMath-Code-7B
có 83.8 trên GSM8K và 52.4 trên MATH, trong
khi MuMath-Code-70B đã đạt được 90.7% trên
GSM8K và 55.1% trên MATH.
2 Công trình liên quan
2.1 LLM không sử dụng công cụ cho Toán học
Tinh chỉnh dựa trên Lấy mẫu Loại bỏ (RFT, Yuan
et al., 2023) chỉ tăng cường các giải pháp thông
qua lấy mẫu loại bỏ để thu thập một loạt các con
đường suy luận khác nhau. Vì RFT không giới
thiệu các câu hỏi toán học mới, tính đa dạng của
tập dữ liệu tăng cường khá thấp, điều này hạn
chế việc cải thiện hiệu suất của các mô hình được
tinh chỉnh. Với mục đích kết hợp một phạm vi
rộng hơn các câu hỏi, MetaMath (Yu et al., 2023)
sử dụng diễn đạt lại, Tự xác minh (SV, Weng et
al., 2023) và FOBAR (Jiang et al., 2023a) để tạo
ra các câu hỏi mới. Về mặt lý tưởng, giống như
các câu hỏi ban đầu, cũng có câu trả lời chuẩn
để lọc các giải pháp cho những câu hỏi tăng cường
này. Để mang lại dữ liệu đa dạng hơn, WizardMath
(Xu et al., 2023; Luo et al., 2023a) và MuggleMath
(Li et al., 2023) chọn tạo ra những câu hỏi hoàn
toàn mới thông qua tiến hóa hoặc sửa đổi có định
hướng (thay đổi số, thêm điều kiện, tăng độ phức
tạp, v.v.) dựa trên các câu hỏi hạt giống. Những
câu hỏi được thay đổi này không có câu trả lời
chuẩn, do đó thiếu tiêu chí để lọc các giải pháp
được tổng hợp tương ứng.
Hơn nữa, MuMath (You et al., 2024) tận dụng
một số phương pháp nói trên, và bổ sung đề xuất
BF-Trans và thay thế biểu thức (v.v.) để thực hiện
tăng cường toàn diện, do đó xây dựng một tập
câu hỏi toán học đa góc nhìn với tính đa dạng
lớn hơn nhiều. Để cải thiện chất lượng dữ liệu,
lấy mẫu đa số phục vụ như quy tắc lọc cho các
giải pháp được tổng hợp cho những câu hỏi mới
không có câu trả lời được biết một cách xác định.
Thay vì lọc giải pháp, một công trình đương thời,
Xwin-Math (Li et al., 2024), sử dụng xác minh
với yêu cầu giải pháp trong quá trình tổng hợp
câu hỏi, qua đó cải thiện khả năng giải quyết của
các câu hỏi và độ chính xác của các câu trả lời.
Vì không có hạn chế về hướng sửa đổi câu hỏi,
Xwin-Math về mặt lý thuyết cung cấp một loạt
dữ liệu tổng hợp đa dạng rộng hơn. Cân bằng
hiệu quả và dễ dàng sao chép, trong bài báo này
MuMath-Code được đề xuất chọn sử dụng tăng
cường câu hỏi từ MuMath, mặc dù nó trực giao
với bất kỳ phương pháp tăng cường nào khác.
Tuy nhiên, như các mô hình xác suất, LLM về
bản chất có những hạn chế trong suy luận logic
và tính toán số học. Do đó, để cải thiện độ chính
xác của việc giải quyết vấn đề toán học trong khi
chỉ dựa vào khả năng của LLM đòi hỏi việc sử
dụng một tập dữ liệu lớn hơn đáng kể so với các
phương pháp sử dụng công cụ.
2.2 LLM sử dụng công cụ cho Toán học
Một quỹ đạo nghiên cứu khác nhấn mạnh sự phối
hợp giữa LLM và các công cụ bên ngoài. Những
nỗ lực tiên phong theo hướng này bao gồm Mô
hình Ngôn ngữ hỗ trợ Chương trình (PAL, Gao
et al., 2023) và Chương trình của Tư duy (PoT,
Chen et al., 2023). Hơn nữa, MAmmoTH (Yue et
al., 2023) tích hợp cả CoT và PoT theo cách thô
(mỗi mẫu chỉ tương ứng với một trong hai loại
giải pháp có thể này), cho phép suy luận linh hoạt
nơi các mô hình được tinh chỉnh có thể áp dụng
các phương pháp khác nhau cho các câu hỏi khác
nhau. Khác với MAmmoTH, ToRA (Gou et al.,
2023) xen kẽ các khối mã python và các phần
suy luận ngôn ngữ tự nhiên qua nhiều lượt cho
cùng một giải pháp, điều này cung cấp sự kết hợp
linh hoạt hơn của CoT và PoT. Tuy nhiên, cả
MAmmoTH và ToRA đều không sử dụng tăng cường
truy vấn, qua đó thu hẹp

--- TRANG 4 ---
phạm vi của các câu hỏi toán học, điều này thực
tế, hạn chế khả năng giải quyết vấn đề có thể
được thu thập. Wang et al. đề xuất một công trình
đương thời với ToRA, MathCoder (Wang et al.,
2023), trong đó mỗi giải pháp cũng được tổ chức
theo cách xen kẽ. Bên cạnh đó, họ giới thiệu các
vấn đề nội suy để giảm thiểu sự chênh lệch về
mức độ khó khăn giữa GSM8K (Cobbe et al., 2021)
và MATH (Hendrycks et al., 2021b). Do đó, giống
như MuMath-Code của chúng tôi, MathCoder cũng
là một sự kết hợp của việc sử dụng công cụ và
tăng cường câu hỏi toán học, mặc dù các câu hỏi
mới mà nó giới thiệu tương đối hẹp về phạm vi
và hạn chế về tính đa dạng.
Tương tự như ToRA và MathCoder, chúng tôi
cũng xây dựng các giải pháp như vậy để kết hợp
linh hoạt LLM với các công cụ thực thi mã bên
ngoài. Tuy nhiên, chúng tôi đề xuất CoT tiền tố,
gỡ lỗi mã, và lọc hướng dẫn pseudo-answer để
làm phong phú thêm các giải pháp và cải thiện
độ chính xác của chúng. Ngoài ra, khác với MathCoder,
tăng cường câu hỏi mà chúng tôi sử dụng là đa
góc nhìn, do đó cung cấp tính đa dạng lớn hơn
và phơi bày mô hình với phạm vi rộng hơn các
câu hỏi mới, qua đó nâng cao đáng kể khả năng
tổng quát hóa của mô hình.
3 Cơ sở lý thuyết
Chúng tôi sử dụng các câu hỏi tăng cường từ Mu-
Math (You et al., 2024) và tổng hợp các giải pháp
lồng ghép mã cho chúng. Để giúp các mô hình
học tốt hơn các giải pháp như vậy với việc tạo
mã đa lượt, thực thi mã và suy luận ngôn ngữ
tự nhiên thuần túy, chúng tôi đề xuất CoT tiền
tố, gỡ lỗi mã, và lọc hướng dẫn pseudo-answer
để tăng cường chất lượng của dữ liệu tổng hợp,
cũng như một chiến lược huấn luyện hai giai đoạn.
Hình 2 mô tả pipeline tổng thể.
3.1 Câu hỏi tăng cường MuMath
Các câu hỏi ban đầu từ các tập huấn luyện
của GSM8K (Cobbe et al., 2021) và
MATH (Hendrycks et al., 2021a) được lấy làm
tập câu hỏi hạt giống Qoriginal. Các phương pháp
tăng cường câu hỏi được sử dụng trong MuMath
được thực hiện trên tập hạt giống này, được tóm
tắt ngắn gọn như sau:
(1) Diễn đạt lại Viết lại một văn bản trong khi
giữ nguyên ý nghĩa ban đầu. Dựa trên thực tế
rằng một câu hỏi được diễn đạt lại giữ cùng ý
nghĩa với câu hỏi ban đầu, câu trả lời cuối cùng
của nó cũng phải giống nhau. Chúng tôi ký hiệu
tập câu hỏi được diễn đạt lại là Qrephrase.
(2) Thay đổi câu hỏi Có năm cách để thay đổi
các câu hỏi ban đầu, như thay đổi số và thêm
nhiều điều kiện hơn, được kết luận trong MuggleMath
(Li et al., 2023). Tập câu hỏi kết quả được tạo
ra thông qua thay đổi được gọi là Qalter = Qalter1
∪ Qalter2 ∪ Qalter3 ∪ Qalter4 ∪ Qalter5. Bên cạnh
đó, Thay thế Biểu thức, được đề xuất trong MuMath,
đầu tiên lấy các biểu thức của giải pháp cho một
câu hỏi ban đầu, sau đó thay đổi các toán tử tính
toán trong chúng. Dựa trên các biểu thức đã thay
đổi, một câu hỏi mới được yêu cầu tạo ra. Qreplace
đại diện cho tập câu hỏi được tạo ra bởi kỹ thuật
tăng cường này. Lưu ý rằng Qalter và Qreplace
không tương ứng với câu trả lời chính xác chắc
chắn do các sửa đổi trong ý nghĩa nội tại của
câu hỏi.
(3) FOBAR Theo Jiang et al. (2023a), chúng tôi
che một điều kiện nhất định trong một câu hỏi
ban đầu bằng cách thay thế nó bằng "X", và đồng
thời đưa ra câu trả lời cho câu hỏi ban đầu như
một điều kiện mới, qua đó tạo ra một câu hỏi đảo
ngược tìm cách xác định giá trị của ẩn số X.
Qfobar được sử dụng để đánh dấu tập câu hỏi
FOBAR.
(4) BF-Trans Biến đổi Lùi-Tiến (BF-Trans), được
đề xuất trong MuMath, nhằm xây dựng các câu
hỏi lùi như vậy có thể được trả lời thông qua suy
luận trực tiếp, bỏ qua sự cần thiết của việc giải
phương trình để tìm các biến ẩn (do đó giống với
dữ liệu được lấy mẫu từ phân phối ban đầu). Đối
với một cặp câu hỏi-câu trả lời nhất định, BF-
Trans đầu tiên sử dụng FOBAR để biến đổi câu
hỏi ban đầu thành câu hỏi lùi; thứ hai, chúng tôi
diễn đạt lại câu hỏi FOBAR thành một dạng mới
trong đó giá trị bị che được yêu cầu trực tiếp thay
vì sử dụng một biến ẩn X, dẫn đến một câu hỏi
"tiến thứ cấp" mà chúng tôi gọi là câu hỏi BF-
Trans. Tập hợp những câu hỏi BF-Trans này được
đánh dấu là Qbf.
Tóm lại, tất cả 10 tập con nói trên (5 trong Qalter)
tạo nên tập câu hỏi kết quả Q = Qoriginal ∪ Qrephrase
∪ Qalter ∪ Qreplace ∪ Qfobar ∪ Qbf. Dựa trên Q,
chúng tôi tạo ra 2 tập dữ liệu được gọi là MuMath-
Data và MuMath-Code-Data, nhấn mạnh suy luận
toán học ngôn ngữ tự nhiên thuần túy và tương
tác công cụ thông qua tạo mã, tương ứng.

--- TRANG 5 ---
Giai đoạn 1 Giai đoạn 2
Dữ liệu Suy luận 
Ngôn ngữ Tự nhiênDữ liệu Lồng ghép Mã 
và Tương tác Công cụ
MuMathMô hình 
Được Huấn luyện TrướcMuMath MuMath -CodePsuedo -
AnswerSố nguyên nhỏ nhất là gì …
[Prefix CoT]
Đầu tiên chúng ta cần tìm …
[Python Code]
import sympy
defsolve( ):
x=1
…
print(…)
Câu trả lời cuối cùng
Câu hỏi
Giải pháp
Nhắc để gỡ lỗi
Lỗi
Thành công[Đầu ra]
Hình 2: Minh họa phương pháp được đề xuất của chúng tôi. Mô hình nền tảng đầu tiên được huấn luyện thông qua một giai đoạn ban đầu, dẫn đến một mô hình trung gian có khả năng suy luận toán học mạnh mẽ hơn. Mô hình trung gian này sau đó được huấn luyện thêm trên tập dữ liệu được đề xuất để học tạo mã và tương tác công cụ, dẫn đến mô hình cuối cùng, MuMath-Code.
3.2 MuMath-Data
MuMath-Data (ký hiệu là Dµ) chỉ là tập dữ liệu
lớn nhất từ MuMath, chứa khoảng 750K mẫu với
các giải pháp suy luận CoT thuần túy cho các câu
hỏi trong Q.
Lấy mẫu Đa số Như được giới thiệu trong bài
báo của MuMath, đối với Qalter và Qreplace mà
mỗi câu hỏi không có câu trả lời tham chiếu, lấy
mẫu đa số được sử dụng để lọc tất cả các giải
pháp được tạo ngẫu nhiên và chỉ giữ lại những
giải pháp có câu trả lời đa số. Nói cách khác,
mỗi câu trả lời đa số phục vụ như một pseudo-
answer cho câu hỏi tương ứng.
4 Phương pháp
4.1 MuMath-Code-Data
Để tạo điều kiện cho việc tương tác với trình thông
dịch python, chúng tôi tổng hợp các giải pháp
lồng ghép mã cho các mô hình học, mỗi giải pháp
bao gồm tạo mã đa lượt, thực thi mã và suy luận
ngôn ngữ tự nhiên thuần túy.
Cụ thể, đối với mỗi câu hỏi từ Q, chúng tôi
nhắc các LLM độc quyền yêu cầu các giải pháp
mỗi giải pháp có ít nhất một khối mã, sau đó
được trích xuất và chuyển đến trình thông dịch
bên ngoài để thực thi. Mỗi kết quả thực thi được
thêm vào nội dung trước đó, ngay sau khối mã
tương ứng. Nếu việc thực thi mã thất bại, chúng
tôi thêm một lời nhắc để gỡ lỗi tích cực, sử dụng
tất cả nội dung trước đó như một lời nhắc hoàn
toàn mới để yêu cầu mã được sửa chữa, sau đó
chúng tôi trích xuất và thực thi lại. Bằng cách lặp
lại quá trình này nhiều lần, chúng tôi có được một
con đường suy luận bao gồm mã, kết quả thực
thi và phân tích ngôn ngữ tự nhiên. Con đường
suy luận này tương tự như của MathCoder (Wang
et al., 2023) và ToRA (Gou et al., 2023), nhưng
sự khác biệt nằm ở việc sử dụng CoT tiền tố được
đề xuất, gỡ lỗi mã, và lọc hướng dẫn pseudo-
answer, sẽ được phát triển chi tiết trong phần này.
Chúng tôi đánh dấu MuMath-Data-Code là Dµ-code.
CoT Tiền tố Chúng tôi đã quan sát thấy rằng trước
khi tạo mã, một phân tích ngôn ngữ tự nhiên thuần
túy chu đáo rất hữu ích cho hiệu suất của mô hình.
Do đó, chúng tôi cố ý thêm một suy luận CoT chu
đáo trước khi viết mã. Lời nhắc yêu cầu được sử
dụng là "Phân tích câu hỏi; liệt kê một số điểm
kiến thức liên quan đến câu hỏi và có lợi cho việc
giải quyết vấn đề.".
Gỡ lỗi Mã Một số nghiên cứu đã chỉ ra rằng việc
sử dụng dữ liệu sửa lỗi và xác minh có thể cải
thiện khả năng suy luận toán học của LLM. Do
đó, chúng tôi giới thiệu một quá trình sửa lỗi cho
tập dữ liệu tăng cường. Cụ thể, trong khi xây
dựng một giải pháp, nếu mã được tạo ra không
thể thực thi, chúng tôi thêm một lời nhắc "Mã
ở trên đã gặp vấn đề. Bây giờ hãy chỉ ra những
sai lầm của nó và sau đó sửa chúng." cho GPT-4
để gỡ lỗi mã và viết mã mới cho đến khi có được
mã có thể thực thi, hoặc đạt đến số lượng tối đa
các yêu cầu. Mã thất bại và thông tin lỗi được
giữ lại để trang bị cho các mô hình được tinh chỉnh
khả năng gỡ lỗi, và do đó nâng cao khả năng lập
trình cho việc giải quyết các vấn đề toán học.
Lọc Hướng dẫn Pseudo-Answer Trong
MuMath-Data, chúng tôi sử dụng lấy mẫu đa số
để lọc các giải pháp. Điều này cung cấp cho
chúng tôi pseudo-answer cho các câu hỏi tăng
cường tương ứng không có câu trả lời tham chiếu,
cũng có thể được sử dụng cho MuMath-Code-Data
để chọn các giải pháp. Phương pháp này cải thiện
độ chính xác của các giải pháp được tổng hợp,
qua đó dẫn đến sự nâng cao chất lượng tổng thể
của dữ liệu tăng cường.
Tóm lại, chúng tôi đánh dấu phần CoT (suy luận
ngôn ngữ tự nhiên thuần túy) thứ i là ci; phần
mã python thứ i được đánh dấu là pi, luôn bắt
đầu bằng ```python và kết thúc bằng ```; đầu
ra thực thi mã thứ i được ký hiệu là oi, bắt đầu
bằng ```output và kết thúc bằng ```. Để chính
thức hóa, một giải pháp kết quả s được định nghĩa
như sau:
s = n−1M
i=1 ci pi oi
cn
= c1 p1 o1 c2 p2 o2 ... cn−1 pn−1 on−1 cn, (1)
trong đó L là nối tất cả các lượt, và n là số lượng
phần CoT. Xem Phụ lục A cho một ví dụ.
4.2 Huấn luyện Hai Giai đoạn
Giai đoạn-1 Huấn luyện giai đoạn đầu tiên là trên
MuMath-Data, trong đó các mô hình tập trung vào
việc học khả năng suy luận toán học CoT thuần
túy. Mục tiêu học tập như sau:
L1 = −Eq,s∼Dµ lX
t=1 log P(xt|q, x<t; θ), (2)
trong đó giải pháp s = (x1, x2, ..., xl) chứa l token,
và θ là tham số của MuMath-Code.
Giai đoạn huấn luyện này ban cho các mô hình
khả năng suy luận toán học khá mạnh, có thể được
xem như một tác vụ sơ bộ cho việc học giai đoạn
thứ hai.
Giai đoạn-2 Huấn luyện giai đoạn thứ hai là trên
MuMath-Code-Data, trong đó các mô hình tập trung
vào dữ liệu xen kẽ PoT-CoT để học cách tương
tác với một công cụ bên ngoài (tức là, trình thông
dịch Python). Chúng tôi che loss của các đầu ra
từ việc thực thi mã, không nên được học bởi các
mô hình. Mục tiêu học tập là:

--- TRANG 6 ---
L2 =
−Eq,s∼Dµ-code nX
i=1 log P(ci pi|q, i−1M
j=1 cj pj oj; θ),
(3)
trong đó pn = ∅. Quá trình huấn luyện ở Giai
đoạn-2 nhất quán với suy luận, vì vậy chúng tôi
không cần xem xét vấn đề quên thảm khốc (về
suy luận ngôn ngữ tự nhiên ở Giai đoạn-1). Tại
thời điểm suy luận, sau khi được đưa ra một vấn
đề toán học, mô hình được tinh chỉnh cần tạo ra
mã để giải quyết vấn đề, và sau đó một trình thông
dịch bên ngoài thực thi mã và trả về kết quả để
mô hình tiếp tục tạo ra. Do đó, huấn luyện Giai
đoạn-2 mô phỏng quá trình suy luận trên bằng
cách che các loss của đầu ra thực thi.
5 Thí nghiệm
5.1 Thiết lập Thí nghiệm
Tập dữ liệu Các tập dữ liệu hạt giống cho tổng
hợp là các tập huấn luyện của hai benchmark suy
luận toán học phổ biến: GSM8K (Cobbe et al.,
2021) và MATH (Hendrycks et al., 2021a). GSM8K
chứa các bài toán toán học tiểu học, bao gồm 7,473
trường hợp huấn luyện và 1,319 trường hợp kiểm
tra; trong khi MATH bao gồm các bài toán thi toán
ở cấp trung học với 7,500 mẫu huấn luyện và
5,000 cho kiểm tra.
Chúng tôi lấy tập dữ liệu MuMath (You et al.,
2024) (750K) làm Dµ cho huấn luyện Giai đoạn-1,
và tập câu hỏi tăng cường MuMath Q được sử
dụng để xây dựng Dµ-code cho Giai đoạn-2; trong
Q, chúng tôi yêu cầu 15 giải pháp cho mỗi câu
hỏi có nguồn gốc từ GSM8K và 30 cho những câu
liên quan đến MATH, và sau đó thực hiện lọc để
có được 30K mẫu cho mỗi tập con câu hỏi, tạo
ra 600K tổng cộng.
Chi tiết Triển khai Nghiên cứu của chúng tôi sử
dụng LLaMA-2 (7B, 13B và 70B) (Touvron et al.,
2023b) và CodeLlama (7B, 13B, 34B, và 70B)
(Rozière et al., 2023) làm các mô hình nền tảng
cho tinh chỉnh toàn bộ tham số, tương ứng với
MuMath-Code-L và MuMath-Code-CL làm các mô
hình kết quả. Chúng tôi sử dụng AdamW làm bộ
tối ưu hóa và một bộ lập lịch tốc độ học cosine
với tỷ lệ khởi động 0.03. Trên tất cả các mô hình
và cả hai giai đoạn, chúng tôi huấn luyện 3 epoch
với kích thước batch toàn cục 128. Tất cả các mô
hình trừ LLaMA-70B

--- TRANG 7 ---
và CodeLlama-70B được huấn luyện bằng framework
Deepspeed, trong khi hai mô hình 70B đó được
huấn luyện bằng Megatron vì tốc độ. Phần cứng
chúng tôi sử dụng là GPU NVIDIA H800.
5.2 Kết quả So sánh
Như thể hiện trong Bảng 1, thí nghiệm so sánh
của các mô hình chúng tôi với hiện tại tiên tiến
nhất chứng minh rằng phương pháp của chúng
tôi liên tục đạt được hiệu suất vượt trội trên tất
cả các quy mô mô hình mã nguồn mở trên tất cả
các tập dữ liệu. Đáng chú ý, mô hình MuMath-
Code-L 7B của chúng tôi đã đạt được độ chính
xác kiểm tra 83.8 trên GSM8K, và MuMath-Code-
CL 7B đã đạt được điểm 52.4 trên MATH. Những
kết quả này vượt qua nhiều baseline mã nguồn
mở 70B và thậm chí một số LLM độc quyền. Ngoài
ra, MuMath-Code-CL 34B và 70B của chúng tôi
đạt được 55.0+ trên MATH, hai kết quả ấn tượng
xem xét rằng chúng được thực hiện bằng cách tận
dụng các kỹ thuật tăng cường dữ liệu dựa trên
tập huấn luyện ban đầu mà không kết hợp các
kho tàng toán học bổ sung rộng lớn cho tiền huấn
luyện.
Có một số phát hiện đáng chú ý từ thống kê thí
nghiệm được trình bày trong bảng, chẳng hạn như
hiệu suất của MuMath-Code-CL 13B trên MATH,
đăng ký ở 53.1, chỉ cao hơn một chút so với của
MuMath-Code-CL 7B, đứng ở 52.4. Hơn nữa, hiệu
suất của MuMath-Code-CL 34B trên MATH, ghi
điểm ở 55.0, rất gần với của MuMath-Code-CL
70B, ghi được điểm 55.1. Chúng tôi suy đoán rằng
điều này có thể được quy cho hiện tượng trong đó,
vượt quá một ngưỡng nhất định của khối lượng
dữ liệu, các ưu điểm được trao bởi việc tăng kích
thước mô hình có thể bị giảm thiểu hoặc thậm chí
bù trừ bởi các lợi ích thu được từ tập dữ liệu mở
rộng. Ngoài ra, các biến thể trong các framework
huấn luyện cũng có thể góp phần vào sự chênh
lệch được quan sát giữa hiệu suất của MuMath-
Code-CL 34B và 70B.
5.3 Hiệu quả của Chiến lược Huấn luyện Hai
Giai đoạn
MuMath-Code được phát sinh từ một quá trình
huấn luyện hai giai đoạn nâng cao khả năng suy
luận ngôn ngữ tự nhiên thuần túy và khả năng
tạo mã và tương tác với các công cụ bên ngoài.
Trong phần này, chúng tôi xác nhận hiệu quả của
chiến lược huấn luyện phân đôi này. Trừ khi được
chỉ định khác, tất cả các thí nghiệm ablation được
trình bày trong bài báo này đều được thực hiện
trên các mô hình 7B, vì hiệu quả thời gian. Chúng
tôi đã thiết kế một đánh giá so sánh hiệu suất mô
hình cho các chiến lược huấn luyện hai giai đoạn
và một giai đoạn. Huấn luyện hai giai đoạn được
đề cập ở đây như mô tả trong Phần 4.2, bao gồm
tiếp tục huấn luyện từ các checkpoint của giai
đoạn đầu tiên (các mô hình MuMath). Huấn luyện
một giai đoạn, áp dụng trực tiếp giai đoạn thứ
hai của huấn luyện trên các mô hình cơ sở. Trên
cả hai thiết lập, chúng tôi thay đổi khối lượng dữ
liệu của Dµ-code. Bảng 2 minh họa so sánh hiệu
suất của các mô hình được phát sinh từ cả hai
chiến lược trên các khối lượng dữ liệu khác nhau,
tiết lộ rằng huấn luyện chỉ trên Dµ-code tệ hơn
so với huấn luyện hai giai đoạn. Hơn nữa, bằng
cách hợp nhất dữ liệu huấn luyện từ cả hai giai
đoạn thành một tập dữ liệu duy nhất cho huấn
luyện một giai đoạn, chúng tôi quan sát thấy rằng
kết quả vẫn không thuận lợi bằng những kết quả
thu được từ hai giai đoạn huấn luyện riêng biệt.
Để xác nhận thêm hiệu quả của chiến lược huấn
luyện hai giai đoạn, chúng tôi chọn các mô hình
MetaMath (Yu et al., 2023) và Xwin-Math (Team,
2023) 7B làm các checkpoint ban đầu cho huấn
luyện Giai đoạn-2, mô phỏng kịch bản trong đó
các tập dữ liệu liên quan được sử dụng trong giai
đoạn đầu tiên (Do sự không khả dụng của các
mô hình và tập dữ liệu mới nhất được đề xuất
trong (Li et al., 2024), chúng tôi chọn sử dụng
Xwin-Math-7B-V1.0 được chi tiết trong kho GitHub
tương ứng). Bảng 2 minh họa rằng các mô hình
được tinh chỉnh từ các checkpoint MetaMath và
Xwin-Math trên Dµ-code (hai giai đoạn) vượt trội
hơn mô hình được huấn luyện trực tiếp từ Llama
(giai đoạn đơn), xác minh hiệu quả của chiến lược
huấn luyện hai giai đoạn cũng như tính tương thích
của Dµ-code với các tập dữ liệu CoT giai đoạn
đầu tiên khác nhau.
5.4 Nghiên cứu Ablation
Để xác minh CoT tiền tố và gỡ lỗi mã được đề
xuất, chúng tôi tương ứng sửa đổi các giải pháp
trong Dµ-code thông qua hai phương pháp riêng
biệt: phương pháp đầu tiên bao gồm việc loại bỏ
CoT tiền tố, qua đó loại bỏ phân tích sơ bộ chi
tiết và trực tiếp bắt đầu với việc viết mã; phương
pháp thứ hai bao gồm việc chỉ giữ lại mã cuối
cùng và được thực thi thành công và bỏ qua tất
cả mã không thể thực thi khác trước đó cũng như
quá trình gỡ lỗi tương ứng. Kết quả của nghiên
cứu ablation này được trình bày trong Bảng 3,
chứng minh rằng việc loại trừ CoT tiền tố hoặc
gỡ lỗi mã dẫn đến sự suy giảm độ chính xác kiểm
tra của mô hình. Điều này nhấn mạnh mạnh mẽ
tầm quan trọng của phân tích chu đáo trước khi
viết mã và quá trình sửa lỗi mã cho việc học của
mô hình.
Hơn nữa, chúng tôi thực hiện một thí nghiệm
ablation khác về lọc hướng dẫn pseudo-answer.
Trong Phần

--- TRANG 8 ---
Mô hình GSM8K MATH GSM-Hard SV AMP TabMWP ASDiv MAWPS
LLM mã nguồn đóng
Claud-3 Opus (Anthropic, 2024) 95.0 60.1 - - - -
GPT-4 (OpenAI, 2023b) 92.0 42.5 64.7 93.1 67.1 91.3 97.6
GPT-4 (PAL) 94.2 51.8 77.6 94.8 95.9 92.6 97.7
GPT-3.5 (OpenAI, 2023a) 80.8 35.5 55.9 83.0 69.1 87.3 94.6
GPT-3.5 (PAL) 78.6 38.7 67.6 77.8 79.9 81.0 89.4
LLM mở không sử dụng công cụ
7B
LLaMA-2 (Touvron et al., 2023b) 13.3 4.1 7.8 38.0 31.1 50.7 60.9
LLaMA-2 SFT (Touvron et al., 2023b) 41.3 7.2 16.1 31.9 27.8 47.4 60.0
WizardMath (Luo et al., 2023a) 54.9 10.7 20.6 57.3 38.1 59.1 73.7
MetaMath (Yu et al., 2023) 66.5 19.8 - - - - -
MuggleMath (Li et al., 2023) 68.4 - - - - - -
MuMath (You et al., 2024) 70.9 22.0 - 76.8 - 93.6 87.3
13B
LLaMA-2 (Touvron et al., 2023b) 24.3 6.3 13.6 43.1 39.5 56.3 70.4
LLaMA-2 SFT (Touvron et al., 2023b) 51.1 9.2 22.3 46.3 35.8 58.6 75.0
WizardMath (Luo et al., 2023a) 63.9 14.0 28.4 64.3 46.7 65.8 79.7
MetaMath (Yu et al., 2023) 72.3 22.4 - - - - -
MuggleMath (Li et al., 2023) 74 - - - - - -
MuMath (You et al., 2024) 76.4 25.3 - - - - -
70B
LLaMA-2 (Touvron et al., 2023b) 57.8 14.4 36.0 73.6 57.5 76.0 92.4
LLaMA-2 SFT (Touvron et al., 2023b) 69.3 14.9 39.0 64.0 53.0 71.3 84.8
WizardMath (Luo et al., 2023a) 81.6 22.7 50.3 80.0 49.8 76.2 86.2
MetaMath(Yu et al., 2023) 82.3 26.6 - - - - -
MuggleMath (Li et al., 2023) 82.3 - - - - - -
MuMath (You et al., 2024) 84.5 32.2 - 87.6 - 96.6 92.0
LLM mở sử dụng công cụ
7B
MAmmoTH (Yue et al., 2023) 53.6 31.5 - 67.7 - - -
MAmmoTH-Coder 59.4 33.4 - 71.4 - - -
CodeLLama (PAL) (Rozière et al., 2023) 34.0 16.6 33.6 59.0 47.3 61.4 79.6
MathCoder-L (Wang et al., 2023) 64.2 23.3 - 71.5 - - -
MathCoder-CL (Wang et al., 2023) 67.8 30.2 - 70.7 - - -
ToRA (Gou et al., 2023) 68.8 40.1 54.6 68.2 42.4 73.9 88.8
ToRA-Code (Gou et al., 2023) 72.6 44.6 56.0 70.4 51.6 78.7 91.3
MuMath-Code-L 83.8 48.8 70.5 87.6 65.6 86.2 94.7
MuMath-Code-CL 82.6 52.4 70.6 88.1 66.9 87.4 95.3
13B
MAmmoTH (Yue et al., 2023) 62.0 34.2 - 72.4 - - -
MAmmoTH-Coder(Yue et al., 2023) 64.7 36.3 - 73.7 - - -
CodeLlama (PAL) (Rozière et al., 2023) 39.9 19.9 39.0 62.4 59.5 65.3 86.0
MathCoder-L (Wang et al., 2023) 72.6 29.9 - 76.9 - - -
MathCoder-CL (Wang et al., 2023) 74.1 35.9 - 78.0 - - -
ToRA (Gou et al., 2023) 72.7 43.0 57.3 72.9 47.2 77.2 91.3
ToRA-Code (Gou et al., 2023) 75.8 48.1 60.5 75.7 65.4 81.4 92.5
MuMath-Code-L 84.3 49.9 70.6 87.9 64.9 86.4 94.9
MuMath-Code-CL 84.6 53.1 70.8 86.8 67.2 85.2 95
34B
CodeLLaMa (PAL) (Rozière et al., 2023) 53.3 23.9 49.4 71.0 63.1 72.4 91.5
MAmmoTH-Coder (Yue et al., 2023) 72.7 43.6 - 84.3 - - -
MathCoder-CL (Wang et al., 2023) 81.7 45.2 - 82.5 - - -
ToRA (Gou et al., 2023) 80.7 50.8 63.7 80.5 70.5 84.2 93.3
MuMath-Code-CL 87.6 55.0 68.8 91.4 74.9 87.9 92.9
70B
LLaMA-2 (PAL) 55.2 18.3 50.0 74.6 59.5 71.9 92.8
MAmmoTH (Yue et al., 2023) 76.9 41.8 - 82.4 - - -
MathCoder-L (Wang et al., 2023) 83.9 45.1 - 84.9 - - -
ToRA (Gou et al., 2023) 84.3 49.7 67.2 82.7 74.0 86.8 93.8
MuMath-Code-L 90.7 52.8 68.6 93 74 88.4 95.4
MuMath-Code-CL 89.5 55.1 70.1 92.9 77.4 87.9 94.7
Bảng 1: So sánh các phương pháp tiên tiến trên các tập dữ liệu khác nhau. Đối với LLM mở sử dụng công cụ, kết quả tốt nhất được in đậm và tốt thứ hai được gạch chân trong số các mô hình cùng quy mô được thử nghiệm trên cùng tập dữ liệu.

--- TRANG 9 ---
(a) Thử nghiệm trên GSM8K
 (b) Thử nghiệm trên MATH
Hình 3: Mở rộng tất cả các tập con của MuMath-Code-Data. Các mô hình trải qua một giai đoạn duy nhất (chỉ Giai đoạn-2) huấn luyện.
Suy luận Chiến lược Huấn luyệnLLaMA CodeLlama
GSM8K MATH GSM8K MATH
Không sử dụng công cụDmeta 66.5 19.8 - -
Dxwin 66.6 17.4 - -
Sử dụng công cụDµ-code 81.2 46.2 81 49.8
Dµ+Dµ-code 82.7 47.1 81.3 49.1
Dmeta→ D µ-code 82.3 47.4 - -
Dxwin→ D µ-code 82.0 47.2 - -
Dµ→ D µ-code 83.8 48.8 82.6 52.4
Bảng 2: Chiến lược huấn luyện hai giai đoạn cải thiện
hiệu suất của mô hình, trái ngược với huấn luyện giai
đoạn đơn.
Giải pháp Tổng hợpLLaMA CodeLlama
GSM8K MATH GSM8K MATH
với tất cả 83.8 48.8 82.6 52.4
không có CoT tiền tố 81.3 47.5 81.8 49.4
không có gỡ lỗi mã 82 47.1 82.1 52.1
không có cả hai 81.0 46.8 81.3 49.0
Bảng 3: Nghiên cứu ablation cho CoT tiền tố và gỡ lỗi
mã.
tion 4.1, chúng tôi lưu ý rằng pseudo-answer phù
hợp cho các câu hỏi tổng hợp thiếu câu trả lời
chính xác chắc chắn, tức là những câu trong Qalter
và Qreplace. Trong MuMath, bỏ phiếu đa số được
sử dụng để gán pseudo-answer cho những câu
hỏi này. Những pseudo-answer này sau đó cũng
được sử dụng để lọc dữ liệu cho Dµ-code trong
giai đoạn huấn luyện thứ hai. Như minh họa trong
Bảng 4, tinh chỉnh mô hình với dữ liệu được lọc
thông qua kỹ thuật pseudo-answer này tỏ ra có
lợi hơn so với các giải pháp thu được thông qua
lấy mẫu ngẫu nhiên trực tiếp. Xu hướng này duy
trì qua
Kích thước Dữ liệu Pseudo-AnswerLLaMA CodeLlama
GSM8K MATH GSM8K MATH
30Kcó 65.4 33.4 67.3 38.5
không 65.9 32.4 67.6 36.9
60Kcó 71.4 37.6 73.4 42.7
không 71.2 36.6 72.3 40.4
90Kcó 75.1 39.3 75.2 44.5
không 74.8 37.9 74.2 41.9
120Kcó 76.1 40.7 76.9 45.7
không 74.6 40.3 75.9 43.6
150Kcó 77.8 42.7 76.8 46
không 75.8 41.5 76.4 45
180Kcó 77.3 43.5 78.5 47.3
không 76.7 42.7 77.7 46.5
Bảng 4: Nghiên cứu ablation cho lọc hướng dẫn pseudo-
answer.
các khối lượng dữ liệu từ 30K đến 180K.
5.5 Nghiên cứu Mở rộng
Các thí nghiệm mở rộng cho các tập con khác
nhau của MuMath-Code-Data được mô tả trong
Hình 3. Những đường cong này đại diện cho các
thay đổi hiệu suất của các mô hình được huấn
luyện trên các tập con dữ liệu khác nhau theo số
lượng mẫu. Mô hình cơ sở là LLaMA 7B và nó
được huấn luyện trực tiếp trên các tập con của
Dµ-code (huấn luyện giai đoạn đơn). Rõ ràng rằng
với sự tăng lên của khối lượng dữ liệu, tất cả các
tập con liên tục góp phần vào sự nâng cao hiệu
suất của mô hình, và các đường cong vẫn chưa
cho thấy sự bão hòa. Điều này chỉ ra rằng việc
sử dụng phương pháp của chúng tôi cho phép tiếp
tục thêm dữ liệu để cải thiện thêm khả năng suy
luận toán học của LLM. Đối với kịch bản hai giai
đoạn trong đó mô hình ban đầu là một checkpoint
trung gian từ Giai đoạn-1, vui lòng tham khảo
Phụ lục B.

--- TRANG 10 ---
6 Kết luận
Trong bài báo này, chúng tôi đề xuất một tập dữ
liệu suy luận toán học đa góc nhìn và tích hợp
mã được gọi là MuMath-Code-Data, trong đó mỗi
giải pháp chứa tạo mã đa lượt, thực thi mã và
phân tích ngôn ngữ tự nhiên thuần túy (CoT).
Thông qua chiến lược huấn luyện hai giai đoạn,
các mô hình MuMath-Code của chúng tôi vượt trội
hơn các phương pháp mở tiên tiến và thậm chí
một số phương pháp độc quyền mạnh mẽ trên
các quy mô khác nhau trên các tập dữ liệu suy
luận trong miền (tức là, GSM8K và MATH) cũng
như những tập ngoài miền. Ngoài ra, các nghiên
cứu ablation chứng minh hiệu quả của ba phương
pháp mới cho tổng hợp dữ liệu: CoT tiền tố, gỡ
lỗi mã và lọc hướng dẫn pseudo-answer. Công
trình của chúng tôi đại diện cho một nỗ lực mới
trong việc tích hợp tăng cường câu hỏi toán học
(không sử dụng công cụ) với tạo và thực thi mã
(sử dụng công cụ) để nâng cao khả năng suy luận
toán học của LLM, và chúng tôi hy vọng nó có
thể truyền cảm hứng cho các nỗ lực nghiên cứu
tiếp theo.
7 Lời cảm ơn
Công trình này được hỗ trợ bởi Chương trình
R&D Trọng điểm Quốc gia Trung Quốc dưới Mã
số Grant 2020AAA0104500, HY Project dưới Mã
số Grant LZY2022033004, Quỹ Khoa học Tự nhiên
tỉnh Sơn Đông dưới các Mã số Grant ZR2020MF131
và ZR2021ZD19, Chương trình Khoa học và Công
nghệ Thanh Đảo dưới Mã số Grant 21-1-4-ny-
19-nsh, và Dự án Huấn luyện Kết hợp của Đại
học Hải dương Trung Quốc dưới Mã số Grant
202265007.

--- TRANG 11 ---
Tài liệu tham khảo
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, et al. 2023. Palm 2 technical report. arXiv
preprint arXiv:2305.10403 .
Anthropic. 2024. The claude 3 model family: Opus, son-
net, haiku. https://www.anthropic.com/news/
claude-3-family .
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, T. J. Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. ArXiv ,
abs/2005.14165.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluating
large language models trained on code.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W. Cohen. 2023. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,

--- TRANG 12 ---
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng,
and Tushar Khot. 2023a. Chain-of-thought hub: A
continuous effort to measure large language models'
reasoning performance.
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and
Tushar Khot. 2023b. Specializing smaller language
models towards multi-step reasoning. arXiv preprint
arXiv:2301.12726 .
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2023. Pal: Program-aided language
models.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,
Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu
Chen. 2023. Tora: A tool-integrated reasoning agent
for mathematical problem solving.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021a. Measuring mathematical
problem solving with the MATH dataset.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021b. Measuring mathemati-
cal problem solving with the math dataset. arXiv
preprint arXiv:2103.03874 .
Namgyu Ho, Laura Schmid, and Se-Young Yun. 2023.
Large language models are reasoning teachers.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,
Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.
Large language models can self-improve.Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu,
Yu Zhang, Zhenguo Li, and James T. Kwok. 2023a.
Forward-backward reasoning in large language mod-
els for mathematical verification.
Weisen Jiang, Yu Zhang, and James Kwok. 2023b. Ef-
fective structured prompting by meta-learning and
representative verbalizer. In Proceedings of the 40th
International Conference on Machine Learning , vol-
ume 202 of Proceedings of Machine Learning Re-
search , pages 15186–15199. PMLR.
Aitor Lewkowycz, Anders Andreassen, David Dohan,
Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,
Ambrose Slone, Cem Anil, Imanol Schlag, Theo
Gutman-Solo, et al. 2022. Solving quantitative rea-
soning problems with language models. Advances
in Neural Information Processing Systems , 35:3843–
3857.
Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nan-
ning Zheng, Han Hu, Zheng Zhang, and Houwen
Peng. 2024. Common 7b language models already
possess strong math capabilities. arXiv preprint
arXiv:2403.04706 .
Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting
Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang
Wang, and Chang Zhou. 2023. Query and response
augmentation cannot help out-of-domain math rea-
soning generalization.
Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen,
Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian,
Baolin Peng, Yi Mao, Wenhu Chen, and Xifeng
Yan. 2022. Explanations from large language models
make small reasoners better.
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. 2023. The flan
collection: Designing data and methods for effective
instruction tuning. arXiv preprint arXiv:2301.13688 .
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-
guang Lou, Chongyang Tao, Xiubo Geng, Qingwei
Lin, Shifeng Chen, and Dongmei Zhang. 2023a. Wiz-
ardmath: Empowering mathematical reasoning for
large language models via reinforced evol-instruct.
Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo
Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qing-
wei Lin, and Daxin Jiang. 2023b. Wizardcoder:
Empowering code large language models with evol-
instruct. arXiv preprint arXiv:2306.08568 .
Lucie Charlotte Magister, Jonathan Mallinson, Jakub
Adamek, Eric Malmi, and Aliaksei Severyn. 2023.
Teaching small language models to reason.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2022. MetaICL: Learning to learn
in context. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 2791–2809, Seattle, United States.
Association for Computational Linguistics.

--- TRANG 13 ---
OpenAI. 2023a. Chatgpt: Optimizing language
models for dialogue. https://openai.com/blog/
chatgpt .
OpenAI. 2023b. Gpt-4 technical report.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, , and
I. Sutskever. 2019. Language models are unsuper-
vised multitask learners. Technical Report .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2023. Exploring the limits
of transfer learning with a unified text-to-text trans-
former.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle,
Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom
Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wen-
han Xiong, Alexandre Défossez, Jade Copet, Faisal
Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier,
Thomas Scialom, and Gabriel Synnaeve. 2023. Code
llama: Open foundation models for code.
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya
Sachan. 2023. Distilling reasoning capabilities into
smaller language models. In Findings of the Asso-
ciation for Computational Linguistics: ACL 2023 ,
pages 7059–7073, Toronto, Canada. Association for
Computational Linguistics.
Xwin-Math Team. 2023. Xwin-math.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models.Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461 .
Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun
Luo, Weikang Shi, Renrui Zhang, Linqi Song,
Mingjie Zhan, and Hongsheng Li. 2023. Mathcoder:
Seamless code integration in llms for enhanced math-
ematical reasoning.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits rea-
soning in large language models. Advances in Neural
Information Processing Systems , 35:24824–24837.
Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He,
Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.
2023. Large language models are better reasoners
with self-verification. In Findings of the Associa-
tion for Computational Linguistics: EMNLP 2023 ,
pages 2550–2575, Singapore. Association for Com-
putational Linguistics.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin
Jiang. 2023. Wizardlm: Empowering large language
models to follow complex instructions.
Weihao You, Shuo Yin, Zhilong Ji, Xudong Zhao, Guo-
qiang Zhong, and Jinfeng Bai. 2024. Mumath: Multi-
perspective data augmentation for mathematical rea-
soning in large language models. In 2024 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics .
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,
Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo
Li, Adrian Weller, and Weiyang Liu. 2023. Meta-
math: Bootstrap your own mathematical questions
for large language models.
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting
Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and
Jingren Zhou. 2023. Scaling relationship on learning
mathematical reasoning with large language models.
Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao
Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023.
Mammoth: Building math generalist models through
hybrid instruction tuning.

--- TRANG 14 ---
A Ví dụ
Ví dụ A.1: Cặp QA MuMath-Code-Data
Câu hỏi:
Số nguyên nhỏ nhất có dư 1 khi chia cho 4, dư 1 khi chia cho 3, và dư 2 khi chia cho 5 là gì?
Giải pháp
Chúng ta cần tìm một nghiệm số nguyên dương cho hệ đồng dư thức tuyến tính. Chúng ta có thể sử dụng
Định lý Dư Trung Quốc để giải hệ này. Định lý Dư Trung Quốc nói rằng đối với bất kỳ hệ phương trình
nào như thế này, đều tồn tại một nghiệm. Hãy tìm nghiệm sử dụng hàm solve_congruence của sympy.
```python
from sympy.ntheory.modular import solve_congruence
def smallest_number():
congruences = [('1', '4'), ('1', '3'), ('2', '5')]
result = solve_congruence(*congruences)
return result[0]
result = smallest_number()
print(result)
```
```output
ValueError: 1 is not an integer
```
Mã ở trên đã gặp vấn đề. Bây giờ hãy chỉ ra những sai lầm của nó và sau đó sửa chúng.
Sai lầm:
Lỗi nằm ở định dạng đầu vào cho hàm 'solve_congruence'. Hàm này mong đợi các tuple có định dạng
'(dư, modulo)', nhưng đầu vào cần là số nguyên, không phải chuỗi.
Sửa chữa:
Hãy sửa đổi mã để chuyển đổi các chuỗi thành số nguyên.
```python
def smallest_number():
congruences = [(1, 4), (1, 3), (2, 5)]
result = solve_congruence(*congruences)
return result[0]
result = smallest_number()
print(result)
```
```output
NameError: name 'solve_congruence' is not defined
```
Mã ở trên đã gặp vấn đề. Bây giờ hãy chỉ ra những sai lầm của nó và sau đó sửa chúng.
Sai lầm:
Lỗi là hàm 'solve_congruence' không được định nghĩa. Nó nên được import từ module 'sympy' trước khi
có thể được sử dụng.

--- TRANG 15 ---
Sửa chữa:
Hãy sửa đổi mã để import hàm 'solve_congruence' từ module 'sympy'.
```python
from sympy.ntheory.modular import solve_congruence
def smallest_number():
congruences = [(1, 4), (1, 3), (2, 5)]
result = solve_congruence(*congruences)
return result[0]
result = smallest_number()
print(result)
```
```output
37
```
Số nguyên nhỏ nhất có dư 1 khi chia cho 4, dư 1 khi chia cho 3, và dư 2 khi chia cho 5 là $\\boxed{37}$.
B Kết quả Thí nghiệm Bổ sung
Dµ-code Chiến lược Huấn luyệnLLaMA CodeLlama
GSM8K MATH GSM8K MATH
100KDµ-code 69.7 36.1 72.4 41
Dµ→ D µ-code 77.1 41.5 80.3 46.1
200KDµ-code 76.2 41.4 78.3 44.2
Dµ→ D µ-code 80.4 46 80.7 49.1
300KDµ-code 77.1 43.7 78.2 46.8
Dµ→ D µ-code 79.5 46.4 83.2 50.2
400KDµ-code 78 44.3 79 47.8
Dµ→ D µ-code 81.6 48.5 81.9 50.9
500KDµ-code 79.8 45.7 80.2 48.9
Dµ→ D µ-code 82.8 48.7 82.6 52.2
600KDµ-code 81.2 46.2 81 49.8
Dµ→ D µ-code 83.8 48.8 82.6 52.4
Bảng 5: Chúng tôi thay đổi khối lượng dữ liệu của Dµ-code. Quan sát thấy rằng huấn luyện chỉ trên Dµ-code liên tục
kém hơn so với huấn luyện hai giai đoạn trên tất cả khối lượng dữ liệu.

--- TRANG 16 ---
(a) Thử nghiệm trên GSM8K
 (b) Thử nghiệm trên MATH
Hình 4: Mở rộng tất cả các tập con của MuMath-Code-Data. Mô hình đã được tinh chỉnh trên MuMath-Data. Có thể
quan sát thấy rằng các đường cong cho thấy xu hướng rất tương tự với những xu hướng trong Hình 3.
