# 2403.04706.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2403.04706.pdf
# Kích thước tệp: 3100882 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Các Mô hình Ngôn ngữ 7B Thông thường Đã Sở hữu Khả năng Toán học Mạnh mẽ
Chen Li1,4, Weiqi Wang2,4, Jingcheng Hu3,4, Yixuan Wei3,4,
Nanning Zheng1, Han Hu4, Zheng Zhang4*, Houwen Peng4*
1IAIR, Đại học Giao thông Tây An2Đại học Khoa học và Công nghệ Trung Quốc
3Đại học Thanh Hoa4Microsoft Research Asia
edward82@stu.xjtu.edu.cn {v-weiqiwang, t-jingchu, t-yixuanwei, zhez, houwen.peng}@microsoft.com
nnzheng@xjtu.edu.cn ancientmooner@gmail.com
Tóm tắt
Khả năng toán học trước đây được tin rằng chỉ xuất hiện trong các mô hình ngôn ngữ thông thường ở quy mô rất lớn hoặc cần đào tạo trước rộng rãi liên quan đến toán học. Bài báo này chỉ ra rằng mô hình LLaMA-2 7B với đào tạo trước thông thường đã thể hiện khả năng toán học mạnh mẽ, được chứng minh bằng độ chính xác ấn tượng 97.7% và 72.0% trên các benchmark GSM8K và MATH tương ứng, khi chọn phản hồi tốt nhất từ 256 lần sinh ngẫu nhiên. Vấn đề chính với mô hình cơ sở hiện tại là khó khăn trong việc khai thác nhất quán các khả năng toán học vốn có. Đáng chú ý, độ chính xác cho câu trả lời đầu tiên giảm xuống 49.5% và 7.9% trên các benchmark GSM8K và MATH tương ứng. Chúng tôi phát hiện rằng việc đơn giản mở rộng dữ liệu SFT có thể tăng cường đáng kể độ tin cậy của việc tạo ra câu trả lời đúng. Tuy nhiên, tiềm năng mở rộng rộng rãi bị hạn chế bởi sự khan hiếm của các câu hỏi toán học có sẵn công khai. Để vượt qua hạn chế này, chúng tôi sử dụng dữ liệu tổng hợp, điều này tỏ ra gần như hiệu quả như dữ liệu thực và không cho thấy sự bão hòa rõ ràng khi mở rộng lên khoảng một triệu mẫu. Phương pháp đơn giản này đạt được độ chính xác 82.6% trên GSM8K và 40.6% trên MATH sử dụng mô hình LLaMA-2 7B, vượt qua các mô hình trước đó lần lượt 14.2% và 20.8%. Chúng tôi cũng cung cấp những hiểu biết về hành vi mở rộng qua các độ phức tạp lý luận và loại lỗi khác nhau.

1 Giới thiệu
Khả năng toán học từ lâu được coi là thách thức đến mức được cho rằng chỉ xuất hiện trong các mô hình ngôn ngữ thông thường ở quy mô rất lớn. Ví dụ, các nghiên cứu của (Wei et al., 2022a,b) cho rằng chỉ có các mô hình với kích thước vượt quá 50
*Trưởng dự án. Chen, Weiqi, Jingcheng và Yixuan là thực tập sinh tại MSRA. GitHub: Xwin-Math Kho lưu trữ này sẽ được cập nhật liên tục.

Hình 1: Các dấu sao màu cam thể hiện độ chính xác đạt được bằng cách chọn phản hồi tốt nhất từ 256 lần sinh ngẫu nhiên của mô hình LLaMA-2 7B. Độ chính xác cao trên các benchmark MATH (trái) và GSM8K (phải) (72.0% và 97.7% tương ứng) cho thấy rằng LLaMA-2 7B đã sở hữu khả năng toán học mạnh mẽ, mặc dù tính ổn định trong việc tạo ra câu trả lời đúng có thể được tăng cường. Bài báo này chứng minh rằng bằng cách mở rộng dữ liệu SFT tổng hợp, tính ổn định có thể được cải thiện đáng kể như được chứng minh bởi các đường cong. Thông qua việc mở rộng đơn giản dữ liệu SFT này, mô hình hoạt động tốt nhất đã vượt qua mô hình GPT-4 đầu tiên 10.3% trên benchmark MATH.

tỷ tham số mới có thể đạt được độ chính xác có ý nghĩa hoặc hưởng lợi từ xử lý chuỗi suy nghĩ trên các bài toán toán học. Một chiến lược để trang bị cho các mô hình ngôn ngữ nhỏ hơn khả năng toán học liên quan đến việc tạo ra các mô hình cơ sở chuyên về toán học được đào tạo trên hàng trăm tỷ dữ liệu đào tạo trước liên quan đến toán học (Lewkowycz et al., 2022; Azerbayev et al., 2023). Tuy nhiên, độ chính xác của những mô hình như vậy vẫn khiêm tốn; ví dụ, Llemma-7B (Azerbayev et al., 2023) chỉ đạt được 36.4% trên tập dữ liệu GSM8K (Cobbe et al., 2021) và 18.0% trên tập dữ liệu MATH (Hendrycks et al., 2021).

Trong bài báo này, chúng tôi chứng minh rằng các mô hình ngôn ngữ thông thường có kích thước nhỏ, chẳng hạn như mô hình LLaMA-2 7B (Touvron et al., 2023b), đã sở hữu khả năng toán học mạnh mẽ mà không cần đào tạo trước cụ thể trên dữ liệu liên quan đến toán học. Đáng ngạc nhiên, chúng tôi phát hiện rằng với việc tinh chỉnh có giám sát chỉ trên hàng nghìn câu hỏi toán học (lưu ý rằng giai đoạn SFT không tăng cường khả năng như đã nêu trong arXiv:2403.04706v1  [cs.CL]  7 Mar 2024

--- TRANG 2 ---
Bảng 1: So sánh mở rộng dữ liệu SFT với câu hỏi toán học thực và tổng hợp. Kết quả cho thấy rằng các câu hỏi toán học tổng hợp gần như hiệu quả bằng câu hỏi thực.

Kích thước dữ liệu GSM8K-thực GSM8K-tổng hợp MATH-thực MATH-tổng hợp
0.94K 26.7 25.9 4.2 3.9
1.88K 32.8 31.9 5.6 4.9
3.75K 43.3 42.2 6.6 6.0
7.50K 50.2 49.5 8.4 7.9

(Bai et al., 2022; Ouyang et al., 2022)), mô hình có thể giải đúng 97.7% câu hỏi GSM8K và 72.0% câu hỏi MATH, khi chọn câu trả lời tốt nhất từ 256 lần sinh ngẫu nhiên, như được chỉ ra bởi các dấu sao màu cam trong Hình 1. Điều đáng chú ý là độ chính xác thậm chí đã vượt qua những gì được báo cáo cho mô hình GPT-4, đạt được 92.0% trên GSM8K và 42.5% trên MATH 1. Do đó, chúng tôi kết luận rằng mô hình LLaMA-2 7B thực sự đã phát triển khả năng toán học mạnh mẽ. Vấn đề chính là thiếu bảo đảm rằng câu trả lời đúng sẽ được khai thác, vì hầu hết các lần sinh đều sai. Thực tế, độ chính xác giảm xuống 49.5% trên GSM8K và 7.9% trên MATH nếu chúng ta chỉ xét một lần sinh ngẫu nhiên cho mỗi câu hỏi. Chúng tôi gọi đây là vấn đề bất ổn định.

Để giải quyết vấn đề bất ổn định, trước tiên chúng tôi quan sát thấy rằng độ chính xác cải thiện gần như tuyến tính hoặc thậm chí siêu tuyến tính với dữ liệu tinh chỉnh có giám sát (SFT) tăng theo cấp số nhân. Hơn nữa, chúng tôi lưu ý rằng độ chính xác còn xa mới đạt đến mức bão hòa khi sử dụng tất cả dữ liệu đào tạo GSM8K và MATH có sẵn (như được hiển thị trong Bảng 1). Quan sát này khuyến khích chúng tôi tiếp tục mở rộng dữ liệu SFT. Tuy nhiên, chúng tôi đối mặt với thách thức vì thiếu dữ liệu thực có thể truy cập công khai để hỗ trợ việc mở rộng liên tục này.

Để vượt qua hạn chế này, chúng tôi chuyển sang dữ liệu tổng hợp, sử dụng một mô hình ngôn ngữ uy tín, cụ thể là GPT-4 Turbo, để tạo ra các câu hỏi toán học tổng hợp. Chúng tôi phát hiện rằng một chiến lược sinh "hoàn toàn mới" đơn giản, nhắc GPT-4 Turbo tạo ra một câu hỏi hoàn toàn mới dựa trên những câu hỏi ưu tiên và sau đó áp dụng một bộ xác minh đơn giản (cũng dựa trên GPT-4 Turbo), đã rất hiệu quả. Cụ thể, như được chỉ ra trong Bảng 1, việc sử dụng các câu hỏi toán học được tạo tổng hợp có thể đạt độ chính xác gần như ngang bằng với các câu hỏi thực, làm nổi bật tiềm năng của các câu hỏi toán học SFT tổng hợp cho mục đích mở rộng.

1Các con số độ chính xác được báo cáo trong báo cáo kỹ thuật GPT-4 (OpenAI, 2023b). Các mô hình GPT-4 đang được cải thiện liên tục. GPT-4 Turbo (1106) API mới nhất đã tăng độ chính xác lên 94.8% trên GSM8K và 64.5% trên MATH. Tuy nhiên, mô hình LLaMA-2 7B sử dụng tốt nhất trong 256 lần sinh vẫn vượt trội hơn các mô hình GPT-4 mới nhất.

Việc tận dụng dữ liệu tổng hợp đã cho phép chúng tôi mở rộng dữ liệu SFT của mình đáng kể, ví dụ, từ 7.5K lên 960K trên GSM8K và từ 7.5K lên 480K trên MATH. Việc mở rộng dữ liệu này cho thấy hành vi mở rộng gần như hoàn hảo, như được vẽ trong Hình 1. Cụ thể, bằng cách đơn giản mở rộng dữ liệu SFT, mô hình của chúng tôi đã trở thành mô hình đầu tiên vượt quá 80% và 40% độ chính xác trên GSM8K và MATH tương ứng, sử dụng mô hình cơ sở LLaMA-2 7B tiêu chuẩn (đạt được 82.6% và 40.6% tương ứng)2.

Dữ liệu SFT tổng hợp đơn giản cũng tỏ ra hiệu quả từ các mô hình cơ sở mạnh hơn, chẳng hạn như LLaMA-2 70B, đạt được 90.6% trên GSM8K và 52.8% trên MATH. Theo hiểu biết tốt nhất của chúng tôi, đây là mô hình mã nguồn mở đầu tiên vượt quá 90% độ chính xác trên GSM8K. Đây cũng là mô hình mã nguồn mở đầu tiên vượt trội hơn GPT-4 (tức là GPT-4-0314) trên benchmark MATH, chứng minh hiệu quả của phương pháp mở rộng tổng hợp đơn giản của chúng tôi.

Ngoài những kết quả mạnh mẽ, chúng tôi cũng đã thu thập được những hiểu biết về tính hiệu quả của phương pháp của chúng tôi: 1) Khi quy mô dữ liệu SFT tăng lên, độ chính xác của mô hình có xu hướng đạt đến mức bão hòa khi sử dụng 256 lần thử; tuy nhiên, có sự gia tăng đáng kể khi sử dụng 1 phản hồi. Điều này cho thấy rằng trong khi giới hạn khả năng trên của mô hình vẫn khá ổn định, những cải thiện về hiệu suất chủ yếu do tăng cường tính ổn định trong việc tạo ra câu trả lời đúng. 2) Độ chính xác giải quyết các bài toán toán học tuân theo quy luật lũy thừa liên quan đến số bước chuỗi suy nghĩ (CoT) với các lượng dữ liệu SFT khác nhau. Một tập dữ liệu SFT mở rộng cải thiện độ tin cậy của mỗi bước lý luận. Tiếp tục tăng tỷ lệ mẫu đào tạo với các bước CoT dài hơn thông qua tái lấy mẫu có thể cải thiện đáng kể độ chính xác của mô hình cho các câu hỏi khó. 3) Phân tích các loại lỗi trong quá trình mở rộng cho thấy rằng lỗi tính toán được giảm thiểu dễ dàng hơn so với lỗi lý luận.

2 Kiểm tra Khả năng Toán học của Mô hình Ngôn ngữ

Các Chỉ số Chúng tôi sử dụng hai chỉ số để kiểm tra khả năng toán học của các mô hình ngôn ngữ.

2Đồng thời, DeepSeek-MATH-7B (Shao et al., 2024) cũng vượt quá 80% độ chính xác. Tuy nhiên, phương pháp của họ dựa vào một mô hình cơ sở mạnh hơn nhiều được đào tạo trước rộng rãi trên các bộ dữ liệu liên quan đến toán học và một thuật toán RL phức tạp. Kết quả của chúng tôi bổ sung cho kết quả của họ.

--- TRANG 3 ---
Đầu tiên là chỉ số Pass@N
Pass@N =E
Bài toán[min( c,1)], (1)
trong đó c biểu thị số câu trả lời đúng trong N phản hồi. Chỉ số này coi một câu hỏi được giải quyết nếu ít nhất một câu trả lời đúng được tạo ra từ N lần sinh ngẫu nhiên. Chúng tôi sử dụng chỉ số này để phản ánh tiềm năng hoặc khả năng của một mô hình trong việc giải quyết một câu hỏi toán học. Để tăng cường tính đa dạng của N lần sinh, chúng tôi đặt nhiệt độ của quá trình sinh là 0.73.

Thứ hai là chỉ số PassRatio@N
PassRatio@N =E
Bài toán⟨c/N⟩, (2)
đo lường tỷ lệ phần trăm câu trả lời đúng trong N câu trả lời được tạo ra. Chỉ số này tương đương một cách nào đó với Pass@1, nhưng với phương sai giảm.

Quan sát Dựa trên hai chỉ số này, chúng tôi kiểm tra hiệu suất của các mô hình LLaMA-2 trên các benchmark GSM8K và MATH4 như được hiển thị trong Hình 1. Để điều chỉnh các mô hình cho hai benchmark này trong các cài đặt tuân theo chỉ dẫn, chúng tôi sử dụng các phiên bản SFT của chúng, được đào tạo với lượng dữ liệu SFT hạn chế (tức là, 7.5K). Như đã được chứng minh trong (Bai et al., 2022; Ouyang et al., 2022), giai đoạn SFT không tăng cường khả năng (và thậm chí có thể dẫn đến giảm, như được đề cập trong bối cảnh "thuế căn chỉnh"). Do đó, việc sử dụng phiên bản SFT cung cấp một đánh giá công bằng về khả năng toán học của các mô hình.

Đầu tiên chúng tôi quan sát rằng các chỉ số Pass@256 cho mô hình LLaMA-2 7B trên cả hai benchmark đều cao một cách đáng kể: 97.7% trên GSM8K và 72.0% trên MATH. Điều này cho thấy rằng mô hình LLaMA-2 7B sở hữu khả năng mạnh mẽ để giải quyết các bài toán toán học.

Sau đó chúng tôi nhận thấy rằng PassRatio@256 thấp hơn đáng kể so với Pass@256, là 48.2% trên GSM8K và 7.9% trên MATH. Điều này cho thấy rằng trong khi các câu trả lời đúng cho hầu hết các câu hỏi toán học có mặt trong 256 lần sinh ngẫu nhiên, không có đảm bảo rằng các câu trả lời đúng sẽ được trích xuất một cách nhất quán, một hiện tượng mà chúng tôi gọi là "vấn đề bất ổn định".

3Điều đáng chú ý là hầu hết các mô hình toán học sử dụng chiến lược sinh tham lam với nhiệt độ được đặt là 0. Tuy nhiên, tác động của sự khác biệt này là tối thiểu.
4Theo (Lightman et al., 2023), chúng tôi sử dụng một tập con 500 mẫu kiểm tra từ benchmark MATH để có hiệu quả thí nghiệm.

Trong phần tiếp theo, chúng tôi sẽ trình bày một phương pháp đơn giản để giảm đáng kể vấn đề bất ổn định.

3 Mở rộng Dữ liệu SFT sử dụng Câu hỏi Toán học Tổng hợp

Trong phần này, trước tiên chúng tôi chứng minh rằng việc mở rộng dữ liệu SFT thực hạn chế có thể giảm thiểu đáng kể vấn đề bất ổn định. Chúng tôi cũng quan sát rằng độ chính xác chưa đạt đến mức bão hòa khi sử dụng đầy đủ dữ liệu đào tạo GSM8K và MATH có sẵn. Chúng tôi xem xét việc tiếp tục mở rộng dữ liệu SFT sử dụng các câu hỏi toán học tổng hợp. Để đạt được mục đích này, chúng tôi giới thiệu một phương pháp đơn giản để tạo dữ liệu tổng hợp sử dụng GPT-4 Turbo API. Dữ liệu tổng hợp tỏ ra hiệu quả như các câu hỏi toán học thực. Do đó, chúng tôi táo bạo mở rộng dữ liệu SFT tổng hợp lên 960K trên GSM8K và 480K trên MATH tương ứng, dẫn đến hành vi mở rộng gần như hoàn hảo, và đạt được độ chính xác tiên tiến.

Mở rộng sử dụng Câu hỏi Toán học Thực Chúng tôi bắt đầu bằng cách kiểm tra hành vi mở rộng của các câu hỏi toán học thực trên toàn bộ tập đào tạo GSM8K và MATH. Như được chỉ ra trong Bảng 1, chúng tôi quan sát thấy sự cải thiện độ chính xác nhất quán, tăng từ 26.7% lên 50.2% trên GSM8K, và từ 4.2% lên 8.4% trên MATH, không có dấu hiệu bão hòa.

Tạo Dữ liệu SFT Tổng hợp Vì dữ liệu thực đã cạn kiệt, chúng tôi suy nghĩ đến việc tiếp tục mở rộng dữ liệu SFT sử dụng các câu hỏi toán học được tạo tổng hợp.

Chúng tôi giới thiệu một phương pháp ba bước đơn giản với sự hỗ trợ của GPT-4 Turbo API:

•Bước 1. Tạo ra một câu hỏi toán học mới. Chúng tôi yêu cầu GPT-4 Turbo API tạo ra một câu hỏi hoàn toàn mới sử dụng một câu hỏi toán học tham khảo làm điểm khởi đầu. Để cải thiện tính hợp lệ của các câu hỏi mới, chúng tôi kết hợp ba quy tắc vào lời nhắc: Thứ nhất, câu hỏi mới phải tuân theo kiến thức thông thường; thứ hai, nó phải có thể giải quyết được độc lập với câu hỏi gốc; và thứ ba, nó không được bao gồm bất kỳ phản hồi trả lời nào. Bên cạnh đó, chúng tôi đã đặt các yêu cầu định dạng cụ thể cho câu hỏi và câu trả lời phù hợp với các tập dữ liệu mục tiêu khác nhau.

•Bước 2. Xác minh câu hỏi. Chúng tôi tiếp tục tăng cường chất lượng của các câu hỏi được tạo ra bằng cách xác nhận và tinh chỉnh chúng thông qua các lời giải thử nghiệm. Bằng cách tích hợp các bước giải quyết và

--- TRANG 4 ---
xác minh vào một lời nhắc duy nhất, chúng tôi đã phát hiện rằng phương pháp này liên tục nâng cao tính hợp lệ của các câu hỏi trên các benchmark khác nhau.

•Bước 3. Tạo ra câu trả lời chuỗi suy nghĩ (CoT). Chúng tôi yêu cầu GPT-4 Turbo tạo ra một phản hồi trả lời chuỗi suy nghĩ (CoT) cho mỗi câu hỏi mới được tạo ra.

Các thiết kế lời nhắc chi tiết được hiển thị trong Phụ lục A.

So sánh Dữ liệu SFT Tổng hợp với Dữ liệu Thực Để đánh giá chất lượng của các câu hỏi toán học được tạo tổng hợp, chúng tôi đánh giá tính hiệu quả của chúng so với các câu hỏi thực từ các tập đào tạo GSM8K và MATH, sử dụng mô hình LLaMA-2 7B, như được chi tiết trong Bảng 1. Kết quả cho thấy rằng các câu hỏi toán học tổng hợp gần như hiệu quả bằng các câu hỏi thực.

Chúng tôi cũng khám phá nhiều phương pháp tổng hợp khác như được đề xuất trong các công trình trước đây (Xu et al., 2023; Yu et al., 2023; An et al., 2023). Những phương pháp này cũng tỏ ra hiệu quả, mặc dù ít hiệu quả hơn một chút so với phương pháp của chúng tôi, như được minh họa trong Hình 6.

Mở rộng lên khoảng một Triệu Dữ liệu SFT Toán học Xét đến tính hiệu quả của phương pháp tổng hợp, chúng tôi tăng đáng kể quy mô của dữ liệu SFT cho cả bài toán GSM8K và MATH, lên 960K và 480K tương ứng. Hình 1 trình bày các kết quả chính sử dụng các kích thước khác nhau của dòng LLaMA-2. Chiến lược mở rộng đơn giản mang lại độ chính xác tiên tiến.

Cũng đáng chú ý là độ chính xác chưa đạt đến đỉnh. Khám phá các tác động của việc mở rộng bổ sung sẽ được để lại cho nghiên cứu tương lai của chúng tôi.

4 Thí nghiệm

4.1 Tập dữ liệu và Đánh giá

Chúng tôi tiến hành thí nghiệm trên 5 benchmark để đánh giá hiệu quả của phương pháp được đề xuất.

GSM8K (Cobbe et al., 2021). Đây là một tập dữ liệu toán học chất lượng cao, đa dạng về ngôn ngữ học, có kiến thức toán học chủ yếu bao gồm mức tiểu học. Nó bao gồm 7,473 ví dụ đào tạo và 1,319 trường hợp kiểm tra. Trong công trình này, chúng tôi sử dụng tập đào tạo của nó làm các câu hỏi cho trước để tạo ra dữ liệu tổng hợp mới.

MATH (Hendrycks et al., 2021). Tập dữ liệu này tập trung vào các bài toán toán học mức độ cạnh tranh đòi hỏi mức độ cao về khả năng lý luận và kiến thức toán học. Nó bao gồm 7,500 ví dụ đào tạo và 5,000 trường hợp kiểm tra. Chúng tôi sử dụng các ví dụ đào tạo để tạo dữ liệu tổng hợp.

SV AMP (Patel et al., 2021). Tập dữ liệu này bao gồm các bài toán toán học mức tiểu học. Chúng tôi sử dụng tất cả 1,000 trường hợp kiểm tra của nó để đánh giá hiệu suất cross-dataset của các mô hình của chúng tôi.

ASDiv (Miao et al., 2021). Tập dữ liệu này chứa một tập hợp các bài toán toán học với các mẫu ngôn ngữ và loại câu hỏi đa dạng. Chúng tôi áp dụng tập kiểm tra gồm 2,305 bài toán làm benchmark đánh giá.

Kỳ thi Trung học Quốc gia Hungary Benchmark đánh giá này được giới thiệu lần đầu bởi Grok-1 (xAI, 2023), được thiết kế để đánh giá khả năng ngoài miền của các mô hình toán học. Nó bao gồm 33 bài toán thách thức.

Điều đáng chú ý là câu trả lời cuối cùng của tập dữ liệu Kỳ thi Trung học Quốc gia Hungary được chú thích bởi con người, trong khi các benchmark khác được gắn nhãn bằng các script tự động, tương tự như các công trình trước đây (Luo et al., 2023; Gou et al., 2023).

4.2 Chi tiết Triển khai

Trong tổng hợp dữ liệu, chúng tôi sử dụng GPT-4 Turbo API, đặt nhiệt độ là 1.0 cho cả việc tạo câu hỏi và câu trả lời.

Đối với tinh chỉnh có giám sát, chúng tôi sử dụng bộ tối ưu hóa Adam với lịch trình tốc độ học cosine trong tổng cộng 3 epoch đào tạo. Tốc độ học tối đa được đặt 2e-5 (ngoại trừ 2e-6 cho mô hình Mistral-7b) và có 4% warm-up tuyến tính. Độ dài token tối đa được đặt 2048, và lời nhắc hệ thống Vicuna-v1.1 (Zheng et al., 2023) được sử dụng. Tất cả các thí nghiệm được tiến hành trên 8 ×Nvidia H100 GPU. Thí nghiệm tiêu tốn tài nguyên nhất của chúng tôi, liên quan đến mô hình 70B và 960K điểm dữ liệu, mất 1900 giờ H100 GPU.

Đối với đánh giá, chúng tôi sử dụng cùng lời nhắc như được sử dụng trong SFT và đặt độ dài chuỗi tối đa là 2048. vLLM (Kwon et al., 2023) được sử dụng trong việc tạo câu trả lời.

4.3 Kết quả Chính và So sánh với Mô hình Tiên tiến

Trong so sánh này, chúng tôi kiểm tra cả benchmark trong miền, GSM8K/MATH, và benchmark ngoài miền, chẳng hạn như Kỳ thi Trung học Quốc gia Hungary. Đối với đánh giá trong miền của mỗi benchmark, chúng tôi sử dụng dữ liệu được tổng hợp từ các mẫu đào tạo tương ứng của nó. Đối với GSM8K, 960K dữ liệu tổng hợp được sử dụng, trong khi đối với MATH, 480K dữ liệu tổng hợp được sử dụng. Đối với đánh giá ngoài miền,

--- TRANG 5 ---
Bảng 2: Hiệu suất lý luận toán học của các LLM khác nhau.

Mô hình GSM8K MATH
Mô hình nguồn đóng
GPT-4 Turbo (1106) 94.8 64.5
GPT-4-0314 94.7 52.6
GPT-4 (Achiam et al., 2023) 92.0 42.5
Claude-2 (Anthropic, 2023) 88.0 -
GPT-3.5-Turbo (OpenAI, 2023a) 80.8 34.1
Mô hình nguồn mở LLaMA-2-7B
WizardMath-7B (Luo et al., 2023) 54.9 10.7
MuggleMath-7B (Li et al., 2023) 68.4 -
MetaMath-7B (Yu et al., 2023) 66.5 19.8
LEMA-LLaMA-2-7B (An et al., 2023) 54.1 9.4
Xwin-Math-7B (của chúng tôi) 82.6 40.6
Mô hình nguồn mở Mistral-7B
WizardMath-7B-v1.1 (Luo et al., 2023) 83.2 33.0
MetaMath-Mistral-7B (Yu et al., 2023) 77.4 28.2
Xwin-Math-Mistral-7B (của chúng tôi) 89.2 43.7
Mô hình nguồn mở Llemma-7B
MetaMath-Llemma-7B (Yu et al., 2023) 69.2 30.0
Xwin-Math-Llemma-7B (của chúng tôi) 84.2 47.2
Mô hình nguồn mở LLaMA-2-13B
WizardMath-13B (Luo et al., 2023) 63.9 14.0
MuggleMath-13B (Li et al., 2023) 74.0 -
MetaMath-13B (Yu et al., 2023) 72.3 22.4
LEMA-LLaMA-2-13B (An et al., 2023) 65.7 12.6
Xwin-Math-13B (của chúng tôi) 88.1 44.9
Mô hình nguồn mở LLaMA-2-70B
WizardMath-70B (Luo et al., 2023) 81.6 22.7
MuggleMath-70B (Li et al., 2023) 82.3 -
MetaMath-70B (Yu et al., 2023) 82.3 26.6
LEMA-LLaMA-2-70B (An et al., 2023) 83.5 25.0
Xwin-Math-70B (của chúng tôi) 90.6 52.8

chúng tôi kiểm tra các mô hình được đào tạo sử dụng GSM8K, MATH, hoặc hỗn hợp của hai tập tổng hợp.

Đối với các mô hình cơ sở, chúng tôi xem xét cả các mô hình ngôn ngữ thông thường, tức là LLaMA-2 7B/13B/70B/Mistral-7B, và các mô hình chuyên về toán học, chẳng hạn như Llemma-7B, để đánh giá tính tổng quát của phương pháp được đề xuất.

Kết quả Trong miền Bảng 2 trình bày so sánh phương pháp được đề xuất với các mô hình tiên tiến nguồn mở và nguồn đóng. Trên tất cả các mô hình cơ sở, phương pháp của chúng tôi vượt trội đáng kể so với các phương pháp tốt nhất trước đây sử dụng cùng mô hình cơ sở được đào tạo trước.

Trên LLaMA-2-7B, phương pháp của chúng tôi vượt quá phương pháp tốt nhất trước đây tuyệt đối +14.2 trên GSM8K (so với MuggleMath-7B (Li et al., 2023)), và +20.8 trên MATH (so với MetaMath-7B (Yu et al., 2023)) tương ứng. Nó thậm chí vượt qua một số mô hình 70B mới nhất dành riêng cho khả năng toán học, chẳng hạn như WizardMath-70B (Luo et al., 2023) (82.6 so với 81.6 trên GSM8K). Trên LLaMA-2-13B, các

Bảng 3: Kết quả kiểm tra kỳ thi trung học quốc gia Hungary của các LLM khác nhau.

Mô hình Điểm Kiểm tra (%)
GPT-4 (Achiam et al., 2023) 68
Grok-1 (xAI, 2023) 59
Claude-2 (Anthropic, 2023) 55
GPT-3.5 Turbo (OpenAI, 2023a) 41
DeepSeek-LLM-67B-Chat (Bi et al., 2024) 58
Xwin-Math-70B (480K GSM8K) 22
Xwin-Math-70B (120K MATH) 51
Xwin-Math-70B (480K MATH) 59
Xwin-Math-70B (480K Hỗn hợp) 65

cải thiện là +14.1 trên GSM8K (so với MuggleMath-13B (Li et al., 2023)) và +22.5 trên MATH (so với MetaMath-13B (Yu et al., 2023)) tương ứng. Trên LLaMA-2-70B, các cải thiện là +7.1 trên GSM8K (so với LEMA-LLaMA-2-70B (An et al., 2023)) và +26.2 trên MATH (so với MetaMath-70B (Yu et al., 2023)) tương ứng.

Trên một mô hình ngôn ngữ thông thường mạnh hơn, tức là Mistral-7B, các cải thiện là +6.0 trên GSM8K và +10.7 trên MATH (so với WizardMath-7B-v1.1 (Luo et al., 2023)) tương ứng.

Trên một mô hình cơ sở chuyên về toán học, chẳng hạn như Llemma-7B, các cải thiện là +15.0 trên GSM8K và +17.2 trên MATH (so với MetaMath-Llemma-7B (Luo et al., 2023)) tương ứng.

Cũng đáng chú ý là mô hình LLaMA-2-70B của chúng tôi đạt được độ chính xác cạnh tranh với các phiên bản đầu của GPT-4 trên GSM8K và MATH. Theo hiểu biết của chúng tôi, đây là mô hình dựa trên LLaMA đầu tiên vượt trội hơn GPT-4-0314 trên MATH.

Những kết quả này chứng minh tính hiệu quả đáng kể và khả năng áp dụng rộng rãi của việc mở rộng dữ liệu SFT toán học tổng hợp.

Kết quả Ngoài miền Chúng tôi kiểm tra các mô hình được đào tạo sử dụng GSM8K, MATH, hoặc hỗn hợp của hai tập tổng hợp trên một benchmark ngoài miền, Kỳ thi Trung học Quốc gia Hungary, theo thực tiễn trong (xAI, 2023).

Bảng 3 hiển thị kết quả. Mô hình của chúng tôi được đào tạo trên dữ liệu hỗn hợp (240K dữ liệu tổng hợp MATH + 240K dữ liệu tổng hợp GSM8K) xếp thứ hai, chỉ sau GPT-4 và tốt hơn nhiều so với các mô hình khác. Ngoài ra, chúng tôi vẽ mối tương quan giữa điểm GSM8K và kỳ thi trung học quốc gia Hungary trong Phụ lục B. Kết quả cho thấy rằng không có sự overfitting benchmark đáng kể trong mô hình của chúng tôi.

Hình 2 (Trái) trình bày kết quả của mô hình

--- TRANG 6 ---
Hình 2: So sánh sự gia tăng quy mô dữ liệu SFT sử dụng một tập dữ liệu đơn lẻ hoặc tập dữ liệu hỗn hợp.

Hình 3: Đường cong Pass@256 và PassRatio@256 với kích thước dữ liệu tăng lên trên benchmark GSM8K và MATH.

được đào tạo trên dữ liệu tổng hợp GSM8K, trong khi Hình 2 (Giữa) trình bày kết quả của mô hình được đào tạo trên MATH. Chúng tôi phát hiện rằng độ chính xác của các benchmark khác cũng cải thiện khi lượng dữ liệu tăng lên đối với các mô hình được đào tạo với dữ liệu tổng hợp GSM8K hoặc MATH. Chúng tôi cũng lưu ý rằng các hành vi tổng quát hóa khác nhau đối với các mô hình GSM8K và MATH: 1) SV AMP và ASDiv hưởng lợi nhiều hơn từ các mô hình GSM8K so với các mô hình MATH. 2) Trong khi các mô hình MATH hoạt động tương đối tốt trên benchmark GSM8K, các mô hình GSM8K hoạt động kém hơn đáng kể trên các benchmark MATH.

Hình 2 (Phải) hiển thị kết quả của các mô hình sử dụng hỗn hợp GSM8K và MATH theo tỷ lệ 1:1. Những mô hình này thể hiện hành vi mở rộng cân bằng trong cả benchmark trong miền và ngoài miền.

4.4 Điều gì Xảy ra đằng sau Cải thiện Hiệu suất?

Pass@256 so với PassRatio@256 Để hiểu sâu hơn về những cải thiện hiệu suất, chúng tôi theo dõi chỉ số Pass@N và chỉ số PassRatio@N dưới kích thước dữ liệu khác nhau. Kết quả được hiển thị trong Hình 3. Với dữ liệu tổng hợp rất hạn chế (ví dụ 7.5K mẫu), mô hình Xwin-Math-70B đã có Pass@256 rất cao, cho thấy khả năng mạnh mẽ để tạo ra câu trả lời đúng thông qua nhiều lần thử. Trong khi đó, chỉ số Pass@256 chỉ thay đổi nhẹ với việc tăng lượng dữ liệu được sử dụng. Ngược lại, PassRatio@256, phản ánh tính ổn định để tạo ra câu trả lời đúng, tăng đáng kể với lượng dữ liệu tổng hợp, và xu hướng tăng trưởng của nó tương tự như Pass@1. Kết quả này xác nhận giả thuyết của chúng tôi rằng các cải thiện hiệu suất chủ yếu được gây ra bởi tính ổn định tốt hơn trong việc tạo ra câu trả lời thay vì khả năng mạnh hơn để trả lời câu hỏi.

Độ Chính xác Lý luận Một bước Ước tính Vì Chuỗi Suy nghĩ (CoT) được áp dụng trong suy luận, quá trình trả lời các bài toán toán học được hoàn thành bằng một quá trình lý luận nhiều bước. Do đó, chúng tôi giả thuyết rằng sự gia tăng trong độ chính xác câu trả lời cuối cùng có thể được diễn giải bằng sự cải thiện trong độ chính xác lý luận một bước. Dựa trên giả định này, nếu một câu hỏi có thể được trả lời về mặt lý thuyết bằng s bước lý luận trong CoT, độ chính xác câu trả lời cuối cùng có thể được ước lượng bằng hàm lũy thừa của độ chính xác lý luận một bước:

Acc final=Accs step (3)

Với phương trình này, độ chính xác bước có thể được ước tính từ độ chính xác câu trả lời cuối cùng. Chúng tôi thí nghiệm trên GSM8K. Đối với mỗi câu hỏi trong tập

--- TRANG 7 ---
Hình 4: Trái: Mối quan hệ giữa độ chính xác trung bình trên GSM8K và số bước CoT được chú thích với dữ liệu tăng lên. Đường liền nét được khớp sử dụng tất cả bảy điểm, trong khi đường đứt nét được khớp sử dụng bốn điểm đầu tiên. Phải: Thay đổi trong độ chính xác trung bình khi tái lấy mẫu được sử dụng để tăng độ dài CoT của dữ liệu đào tạo.

kiểm tra, chúng tôi tạo ra 256 phản hồi và sử dụng số bước trong các chú thích CoT của tập kiểm tra GSM8k làm các bước CoT lý thuyết. Chúng tôi vẽ đường cong để hiển thị mối quan hệ giữa số bước lý luận CoT và độ chính xác câu trả lời cuối cùng trung bình và hiển thị đường cong khớp dựa trên Phương trình 3. Chúng tôi kiểm tra các mô hình Xwin-Math-7B với dữ liệu tổng hợp khác nhau, và kết quả được hiển thị trong Hình 4. Đường liền nét được khớp sử dụng tất cả bảy điểm và Bảng 4 hiển thị độ chính xác một bước ước tính khi sử dụng lượng dữ liệu khác nhau sử dụng tất cả các điểm dữ liệu, và có thể thấy rằng độ chính xác một bước cải thiện đáng kể với nhiều dữ liệu hơn.

Tuy nhiên, khi chúng tôi khớp dựa trên Phương trình 3 với bốn điểm đầu tiên, như được hiển thị trong các đường đứt nét, chúng tôi phát hiện rằng ba điểm sau đó thấp hơn đáng kể so với đường cong. Chúng tôi tin rằng hiện tượng này có thể liên quan đến tỷ lệ nhỏ hơn của các bài toán phức tạp hơn trong dữ liệu đào tạo. Do đó, chúng tôi tái lấy mẫu 960K dữ liệu tổng hợp theo số câu trong lời giải CoT. Như có thể thấy từ Hình 4 (phải), khi tỷ lệ bài toán phức tạp được tăng lên, độ chính xác cho các bài toán đơn giản hơn gần như không thay đổi, nhưng độ chính xác cho các bài toán phức tạp hơn có thể được cải thiện đáng kể. Hơn nữa, việc sử dụng tái lấy mẫu dữ liệu có thể tăng PassRatio@256 của mô hình từ 71.1 lên 72.8. Kết quả thí nghiệm này cung cấp những hiểu biết mới về lựa chọn dữ liệu cho các nhiệm vụ lý luận toán học.

Ngoài ra, chúng tôi tiếp tục sử dụng GPT-4 Turbo để tìm vị trí mà bước đầu tiên trong câu trả lời của chúng tôi sai và chuẩn hóa vị trí đó theo tổng số bước trong mỗi câu trả lời. Khi độ chính xác một bước ước tính cao hơn, vị trí

Bảng 4: Độ chính xác lý luận một bước ước tính và vị trí lỗi đầu tiên chuẩn hóa trung bình bởi GPT-4 Turbo trong Xwin-Math-7B trên benchmark GSM8K.

Kích thước dữ liệu Acc step ước tính Vị trí lỗi đầu tiên chuẩn hóa
7.5K 78.9 67.1
120K 89.7 83.9
960K 94.2 90.9

Hình 5: Thay đổi trong tỷ lệ lỗi tính toán và lý luận trong quá trình tăng dữ liệu.

lỗi đầu tiên của việc chuẩn hóa được hoãn lại.

Sự Cải thiện trong Độ Chính xác của Tính toán Số học Đáng kể hơn so với Lý luận Logic Hiệu suất của mô hình dần dần cải thiện khi dữ liệu tổng hợp tăng lên. Để hiểu sâu hơn, chúng tôi phân tích tỷ lệ lỗi cho các loại lỗi khác nhau trên GSM8K. Chúng tôi phân loại lỗi thành hai loại: lỗi lý luận và lỗi tính toán. Lỗi lý luận chủ yếu bao gồm các vấn đề như mất điều kiện và nhầm lẫn khái niệm, trong khi lỗi tính toán bao gồm phân tích sai mối quan hệ định lượng và lỗi tính toán số học. Dựa trên kết quả thí nghiệm được minh họa trong Hình 5, chúng tôi quan sát thấy sự giảm dần trong tỷ lệ phần trăm của lỗi tính toán, cho thấy rằng GSM8K đang sửa lỗi tính toán với tốc độ nhanh hơn so với lỗi lý luận.

4.5 Ablation về Lược đồ Tổng hợp Dữ liệu

So sánh với Các Phương pháp Tổng hợp Dữ liệu Khác Chúng tôi so sánh phương pháp của chúng tôi với các phương pháp tổng hợp dữ liệu được sử dụng phổ biến sau:

Thêm Ràng buộc. Thêm một ràng buộc nữa vào câu hỏi gốc trong khi giữ nguyên những thứ khác, được sử dụng trong WizardMath và MuggleMath.

Thay đổi Số. Thay đổi các số xuất hiện trong bài toán trong khi giữ nguyên ngữ cảnh. được sử dụng trong MuggleMath.

Thay đổi Bối cảnh. Thay đổi bối cảnh trong

--- TRANG 8 ---
Hình 6: Hiệu suất GSM8K và MATH của các phương pháp tổng hợp khác nhau.

Bảng 5: Ablation của xác minh câu hỏi trên MATH.

Mô hình Pass@1 (%)
Xwin-Math-70B (7.5K dữ liệu) 28.9
Xwin-Math-70B (7.5K dữ liệu) w/o xác minh 28.1 (-0.8)
Xwin-Math-70B (30K dữ liệu) 37.6
Xwin-Math-70B (30K dữ liệu) w/o xác minh 36.6 (-1.0)

câu hỏi trong khi giữ nguyên những thứ khác.

Kết hợp Thay đổi Số và Bối cảnh. Một phương pháp lai kết hợp thay đổi cả số và bối cảnh.

Phương pháp MetaMath. Các phương pháp tổng hợp được đề xuất trong MetaMath, bao gồm tăng cường câu trả lời, diễn đạt lại câu hỏi, câu hỏi tự xác minh và câu hỏi FOBAR. Trong các thí nghiệm, chúng tôi tuân theo việc triển khai MetaMath nhưng sử dụng GPT-4 Turbo thay vì GPT-3.5 Turbo để tạo ra dữ liệu phản hồi sử dụng các câu hỏi đã phát hành của họ.

Kết quả thí nghiệm trong Hình 6 cho thấy rằng khi kích thước dữ liệu tương đối nhỏ, ví dụ, 7.5k và 30k mẫu, khoảng cách hiệu suất giữa các phương pháp khác nhau là không đáng kể. Tuy nhiên, khi kích thước dữ liệu tăng lên, phương pháp của chúng tôi và phương pháp với các ràng buộc được thêm vào cho thấy hiệu suất mạnh hơn. Điều này cho thấy rằng sự lựa chọn chiến lược tổng hợp dữ liệu trở nên quan trọng hơn khi kích thước dữ liệu tăng lên, và một số phương pháp có thể mở rộng dữ liệu hiệu quả hơn, do đó cải thiện hiệu suất.

Tác động của Xác minh Câu hỏi. Xác minh câu hỏi được sử dụng để tiếp tục cải thiện chất lượng sinh. Trong các thí nghiệm của chúng tôi, chúng tôi phát hiện rằng nó có thể cải thiện hiệu suất trên benchmark MATH, kết quả được hiển thị trong Bảng 5, trong khi chúng tôi không thấy tác động đáng kể trên tập dữ liệu GSM8K.

5 Công trình Liên quan

Mô hình Ngôn ngữ Lớn Các mô hình ngôn ngữ lớn (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023a,b) đã đạt được những thành tựu đáng kể, với hiệu suất ấn tượng trên một loạt các nhiệm vụ. Hiện tại, các mô hình ngôn ngữ lớn nguồn đóng, được đại diện bởi GPT (Brown et al., 2020; Achiam et al., 2023), Gemini (Team et al., 2023), Grok (xAI, 2023), và Claude-2 (Anthropic, 2023), là những mô hình tiên tiến nhất về hiệu suất. Tuy nhiên, các mô hình nguồn mở, được đại diện bởi LLaMA (Touvron et al., 2023a), LLaMA-2 (Touvron et al., 2023b) và Mixtral (Jiang et al., 2024), cũng đã tiến bộ nhanh chóng, và thậm chí đã cho thấy hiệu suất cạnh tranh với các mô hình nguồn đóng trên một số nhiệm vụ. Công trình của chúng tôi, nhằm mục đích cải thiện hiệu suất của các LLM nguồn mở trên các nhiệm vụ toán học bằng cách tinh chỉnh chúng trên dữ liệu tổng hợp.

Khung Lý luận để Cải thiện Khả năng Toán học Chuỗi suy nghĩ (Wei et al., 2022b) khuyến khích các LLM thực hiện lý luận nhiều bước bằng các lời nhắc được thiết kế cụ thể và có thể cải thiện hiệu suất lý luận. Dựa trên công trình này, nhiều công trình tiếp theo đề xuất những cải thiện thêm (Fu et al., 2022; Zhang et al., 2022; Kojima et al., 2022). Các công trình trên tập trung chủ yếu vào cách cải thiện hiệu suất thông qua thiết kế lời nhắc tốt hơn hoặc chiến lược suy luận mà không tinh chỉnh mô hình, trong khi công trình của chúng tôi tập trung vào cách cải thiện bản thân mô hình, và do đó những phương pháp này bổ sung cho chúng tôi.

LLM được Tinh chỉnh cho Toán học Một loại công trình khác (Lightman et al., 2023; Luo et al., 2023; Azerbayev et al., 2023; Yue et al., 2023; Yu et al., 2023; An et al., 2023; Li et al., 2023; Gou et al., 2023) cố gắng cải thiện hiệu suất trực tiếp bằng cách đào tạo mô hình trên dữ liệu toán học. Một cách trực tiếp là sử dụng tinh chỉnh để cải thiện mô hình. Một phương pháp được sử dụng rộng rãi là sử dụng dữ liệu tổng hợp, rất gần với phương pháp của chúng tôi: MetaMath (Yu et al., 2023) trình bày để bootstrap câu hỏi để tăng cường dữ liệu. LeMA (An et al., 2023) thu thập các cặp dữ liệu sửa lỗi bằng cách sử dụng GPT-4 làm bộ sửa lỗi. Và MuggleMath (Li et al., 2023) tăng cường tập dữ liệu GSM8K bằng cách kết hợp GPT-4 với một loạt các hoạt động được định nghĩa trước. So với những nỗ lực dựa trên dữ liệu tổng hợp này, phương pháp tổng hợp dữ liệu của chúng tôi đơn giản hơn nhiều và có thể mở rộng hơn do giới thiệu ít tiên nghiệm và ràng buộc hơn.

Mở rộng Dữ liệu SFT Gần đây, một số nỗ lực nghiên cứu đã tập trung vào quy mô dữ liệu cho tinh chỉnh có giám sát. Ví dụ, LIMA (Zhou et al., 2023) đề cập rằng tinh chỉnh với 1,000 chỉ dẫn chất lượng cao

--- TRANG 9 ---
có thể mang lại kết quả ấn tượng trong nhiều nhiệm vụ tổng quát khác nhau. Các nghiên cứu khác đã chỉ ra rằng hiệu suất mở rộng theo kích thước dữ liệu trong các nhiệm vụ toán học và mã hóa (Dong et al., 2023). Công trình gần đây (Bi et al., 2024) thậm chí sử dụng 1.5 triệu dữ liệu cho tinh chỉnh chỉ dẫn để có được hiệu suất hàng đầu. Tuy nhiên, những lý do nội tại đằng sau hiệu ứng mở rộng này chưa được điều tra kỹ lưỡng.

6 Kết luận

Nghiên cứu này tiết lộ rằng các mô hình ngôn ngữ 7B thông thường, chẳng hạn như LLaMA-2 7B, đã thể hiện khả năng toán học mạnh mẽ, thách thức niềm tin trước đây rằng lý luận toán học tiên tiến chỉ dành riêng cho các mô hình lớn hơn, được đào tạo trước rộng rãi hơn. Bằng cách mở rộng đáng kể dữ liệu SFT, chúng tôi đã cải thiện đáng kể tính ổn định của kỹ năng giải quyết bài toán toán học của mô hình. Phương pháp của chúng tôi đã cho phép các mô hình Xwin-Math đạt đến mức hiệu suất có thể so sánh và trong một số trường hợp vượt qua những mô hình lớn hơn của chúng. Phân tích của chúng tôi cũng chỉ ra rằng những cải thiện chủ yếu được quy cho độ chính xác cao hơn trong lý luận một bước và một việc tái lấy mẫu bổ sung của dữ liệu đào tạo có thể cải thiện độ chính xác của các câu hỏi khó hơn. Ngoài ra, chúng tôi thấy việc giảm đáng kể hơn các lỗi tính toán so với các lỗi lý luận logic. Nghiên cứu của chúng tôi đóng góp những hiểu biết có giá trị về khả năng toán học của các mô hình ngôn ngữ lớn.

Lời cảm ơn

Chen Li và Nanning Zheng được hỗ trợ một phần bởi NSFC dưới số tài trợ 62088102. Cảm ơn Shengnan An tại IAIR, Đại học Giao thông Tây An vì lời khuyên có giá trị của anh ấy về công trình này.

Tài liệu tham khảo

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.

Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. 2023. Learning from mistakes makes llm better reasoner. arXiv preprint arXiv:2310.20689.

Anthropic. 2023. Model card and evaluations for claude models.

Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback.

Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.

Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023. How abilities in large language models are affected by supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492.

Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720.

--- TRANG 10 ---
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. 2023. Tora: A tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.

Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213.

Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles.

Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models.

Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. 2023. Query and response augmentation cannot help out-of-domain math reasoning generalization. arXiv preprint arXiv:2310.05506.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. arXiv preprint arXiv:2305.20050.

Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583.

Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2021. A diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv:2106.15772.

OpenAI. 2023a. Gpt-3.5 turbo fine-tuning and api updates.

OpenAI. 2023b. GPT-4 technical report. CoRR, abs/2303.08774.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.

Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191.

Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.

Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

--- TRANG 11 ---
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837.

xAI. 2023. Grok-1.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244.

Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284.

Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653.

Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.

Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206.

--- TRANG 12 ---
A Lời nhắc Tổng hợp trên GSM8K

Lời nhắc 1: Tạo Câu hỏi
Vui lòng đóng vai một giáo viên toán học chuyên nghiệp.
Mục tiêu của bạn là tạo ra các bài toán toán học chất lượng cao để giúp học sinh học toán.
Bạn sẽ được đưa ra một câu hỏi toán học. Vui lòng tạo một câu hỏi mới dựa trên Câu hỏi Cho trước và các hướng dẫn sau.
Để đạt được mục tiêu, bạn có ba công việc.
# Vui lòng tạo ra một câu hỏi tương tự nhưng mới theo Câu hỏi Cho trước.
# Kiểm tra câu hỏi bằng cách giải từng bước để tìm ra xem nó có tuân thủ tất cả các nguyên tắc hay không.
# Sửa đổi câu hỏi đã tạo theo nhận xét kiểm tra của bạn để đảm bảo nó có chất lượng cao.
Bạn có năm nguyên tắc để làm điều này.
# Đảm bảo câu hỏi mới chỉ hỏi về một thứ, hợp lý, dựa trên Câu hỏi Cho trước, và có thể được trả lời chỉ với một số (số thực hoặc số nguyên). Ví dụ, KHÔNG hỏi, 'số lượng của A, B và C là bao nhiêu?'.
# Đảm bảo câu hỏi mới phù hợp với thói quen của cuộc sống. Ví dụ, số tiền ai đó có hoặc trả phải là số dương, và số người phải là số nguyên.
# Đảm bảo học sinh của bạn có thể trả lời câu hỏi mới mà không cần câu hỏi cho trước. Nếu bạn muốn sử dụng một số con số, điều kiện hoặc bối cảnh trong câu hỏi cho trước, vui lòng nêu lại chúng để đảm bảo không có thông tin nào bị bỏ sót trong câu hỏi mới của bạn.
# Vui lòng KHÔNG bao gồm lời giải trong câu hỏi của bạn.
# Nếu câu hỏi đã tạo đã tuân theo những nguyên tắc này khi bạn xác minh. Chỉ cần giữ nguyên mà không cần sửa đổi gì.
Câu hỏi Cho trước: câu hỏi cho trước
Đầu ra của bạn nên có định dạng sau:
CÂU HỎI ĐÃ TẠO: <câu hỏi đã tạo của bạn>
XÁC MINH VÀ SỬA ĐỔI: <giải câu hỏi từng bước và sửa đổi nó để tuân theo tất cả các nguyên tắc>
CÂU HỎI ĐÃ TẠO CUỐI CÙNG: <câu hỏi đã tạo cuối cùng của bạn>

Lời nhắc 2: Tạo Câu trả lời
Vui lòng đóng vai một giáo viên toán học chuyên nghiệp.
Mục tiêu của bạn là giải chính xác một bài toán toán học.
Để đạt được mục tiêu, bạn có hai công việc.
# Viết lời giải chi tiết cho Câu hỏi Cho trước.
# Viết câu trả lời cuối cùng cho câu hỏi này.
Bạn có hai nguyên tắc để làm điều này.
# Đảm bảo lời giải là từng bước.
# Đảm bảo câu trả lời cuối cùng chỉ là một số (số thực hoặc số nguyên).
Câu hỏi Cho trước: câu hỏi cho trước
Đầu ra của bạn nên có định dạng sau:
LỜI GIẢI: <lời giải chi tiết của bạn cho câu hỏi cho trước>
CÂU TRẢ LỜI CUỐI CÙNG: <câu trả lời cuối cùng của bạn cho câu hỏi chỉ với một số nguyên hoặc số thực>

Lời nhắc 3: Tạo Câu hỏi w/o xác minh
Vui lòng đóng vai một giáo viên toán học chuyên nghiệp.
Mục tiêu của bạn là tạo ra các bài toán toán học chất lượng cao để giúp học sinh học toán.
Bạn sẽ được đưa ra một câu hỏi toán học. Vui lòng tạo một câu hỏi mới dựa trên Câu hỏi Cho trước và các hướng dẫn sau.
Để đạt được mục tiêu, bạn có một công việc.
# Vui lòng tạo ra một câu hỏi tương tự nhưng mới theo Câu hỏi Cho trước.
Bạn có bốn nguyên tắc để làm điều này.
# Đảm bảo câu hỏi mới chỉ hỏi về một thứ, hợp lý, dựa trên Câu hỏi Cho trước, và có thể được trả lời chỉ với một số (số thực hoặc số nguyên). Ví dụ, KHÔNG hỏi, 'số lượng của A, B và C là bao nhiêu?'.
# Đảm bảo câu hỏi mới phù hợp với thói quen của cuộc sống. Ví dụ, số tiền ai đó có hoặc trả phải là số dương, và số người phải là số nguyên.
# Đảm bảo học sinh của bạn có thể trả lời câu hỏi mới mà không cần câu hỏi cho trước. Nếu bạn muốn sử dụng một số con số, điều kiện hoặc bối cảnh trong câu hỏi cho trước, vui lòng nêu lại chúng để đảm bảo không có thông tin nào bị bỏ sót trong câu hỏi mới của bạn.
# Bạn chỉ cần tạo câu hỏi mới. Vui lòng KHÔNG giải nó.
Câu hỏi Cho trước: câu hỏi cho trước
Đầu ra của bạn nên có định dạng sau:
CÂU HỎI ĐÃ TẠO: <câu hỏi đã tạo của bạn>

--- TRANG 13 ---
B Kết quả Bổ sung

Hình 7: Hiệu suất tổng hợp của Xwin-Math trên hai benchmark này chỉ sau GPT-4, chứng minh khả năng tổng quát hóa mạnh mẽ của mô hình chúng tôi.

[THIS IS CHART: A scatter plot showing the relationship between Hungarian Exam Score (%) on x-axis (20-70) and GSM8K Score (%) on y-axis (60-90). Various models are plotted including GPT-4, Grok-1, Claude-2, GPT-3.5 Turbo, DeepSeek-LLM-67B-Chat, and different versions of Xwin-Math-70B]

Bảng 6: Để xác thực rò rỉ dữ liệu benchmark trong quá trình tạo dữ liệu, chúng tôi so sánh loss LM trên: 1) một tập con đào tạo, là một tập con nhỏ với 256 mẫu từ tất cả dữ liệu tổng hợp; 2) một tập con đào tạo được tạo lại, nơi chúng tôi duy trì các câu hỏi gốc từ tập con đào tạo và sử dụng GPT-4 Turbo để viết lại câu trả lời; 3) một tập kiểm tra được tạo lại, nơi chúng tôi giữ nguyên các câu hỏi từ các chỉ số đánh giá và sử dụng GPT-4 Turbo để viết lại câu trả lời; 4) một tập kiểm tra tham khảo, nơi chúng tôi sử dụng tập kiểm tra làm seed để tạo ra các câu hỏi và câu trả lời mới thông qua GPT-4 Turbo. Tham khảo Skywork, chúng tôi cũng báo cáo hai chỉ số chính: ∆1=Ltest-regen −Ltest-ref, ∆2=Ltest-regen −Ltrain-regen, Vì ∆1 gần với 0 và ∆2 lớn hơn 0 đáng kể trong hai benchmark, chúng tôi tin rằng không có rò rỉ trong quá trình tổng hợp dữ liệu.

[THIS IS TABLE: Shows dataset comparison with columns for Ltest-regen, Ltest-ref, Ltrain, Ltrain-regen, ∆1, ∆2 for GSM8K and MATH datasets]

--- TRANG 14 ---
C Nghiên cứu Trường hợp Dữ liệu Tổng hợp

C.1 GSM8K

Câu hỏi Gốc
Randy có 60 cây xoài trong trang trại của mình. Anh ấy cũng có ít hơn 5 cây so với một nửa số cây dừa bằng số cây xoài. Randy có tổng cộng bao nhiêu cây trong trang trại của mình?

Câu hỏi Tổng hợp w/o Xác minh
Maria có 80 cây táo trong vườn của cô ấy. Cô ấy cũng có ít hơn 4 cây so với một phần ba số cây anh đào bằng số cây táo. Maria có tổng cộng bao nhiêu cây trong vườn của cô ấy?

Nhận xét Xác minh
Hãy kiểm tra câu hỏi đã tạo:
- Maria có 80 cây táo.
- Cô ấy có ít hơn 4 cây so với một phần ba số cây anh đào bằng số cây táo. Một phần ba số cây táo là 80/3, khoảng 26.67.
- Ít hơn bốn cây sẽ là khoảng 22.67, nhưng vì chúng ta không thể có phần lẻ của một cây, chúng ta làm tròn đến số nguyên gần nhất, là 27.
- Câu hỏi đã sửa đổi nên hỏi về số cây anh đào mà không có số thập phân. Vậy chúng ta điều chỉnh câu hỏi để có kết quả là số nguyên.
- Vì 80 không chia hết cho 3, để có số nguyên cây anh đào, chúng ta có thể thay đổi số để nó chia hết cho 3. Hãy sử dụng 81 cây táo thay vì 80.

Câu hỏi Tổng hợp với Xác minh
Maria có 81 cây táo trong vườn của cô ấy. Cô ấy cũng có ít hơn 4 cây so với một phần ba số cây anh đào bằng số cây táo. Maria có tổng cộng bao nhiêu cây trong vườn của cô ấy?

Lời giải Tổng hợp và Câu trả lời Cuối cùng
Đầu tiên, hãy xác định số cây anh đào mà Maria có. Chúng ta được nói rằng cô ấy có ít hơn 4 cây so với một phần ba số cây anh đào bằng số cây táo.

Vì Maria có 81 cây táo, hãy tìm một phần ba của số đó:
1/3 của 81 cây táo = 81 cây táo / 3 = 27 cây táo.

Bây giờ, vì cô ấy có ít hơn số này 4 cây anh đào, chúng ta trừ 4 khỏi 27 để tìm số cây anh đào:
27 cây anh đào - 4 = 23 cây anh đào.

Để tìm tổng số cây Maria có trong vườn của cô ấy, chúng ta cộng số cây táo với số cây anh đào:
81 cây táo + 23 cây anh đào = 104 cây.

Câu trả lời là 104.

--- TRANG 15 ---
C.2 MATH

Câu hỏi Gốc
Giá trị của log1028471 nằm giữa các số nguyên liên tiếp a và b. Tìm a+b.

Câu hỏi Tổng hợp w/o Xác minh
Giá trị của log216384 nằm giữa các số nguyên liên tiếp x và y. Tìm x+y.

Nhận xét Xác minh
Để đảm bảo câu hỏi có thể được giải, chúng ta cần kiểm tra xem logarit cho trước có thể được đánh giá và nằm giữa hai số nguyên liên tiếp hay không.
1. Xác định giá trị của 2n bằng 16384.
2. Nếu 16384 là lũy thừa của 2, thì log216384 sẽ là một số nguyên.
3. Kiểm tra để đảm bảo kết quả là số nguyên và tìm các số nguyên liên tiếp xung quanh nó.

Hãy thực hiện kiểm tra:
20= 1, 21= 2, 22= 4, 23= 8, 24= 16
...
214= 16384

Vậy, log216384 = 14.

Các số nguyên liên tiếp xung quanh 14 là (13 và 14) hoặc (14 và 15).

Vì logarit cho kết quả là số nguyên chính xác, để làm cho câu hỏi hợp lệ, chúng ta sẽ sửa đổi nó để sử dụng một số không phải là lũy thừa chính xác của 2.

CÂU HỎI ĐÃ TẠO ĐÃ SỬA ĐỔI:
Giá trị của log210000 nằm giữa các số nguyên liên tiếp m và n. Tìm m+n.

Bây giờ, hãy kiểm tra câu hỏi đã sửa đổi:
1. Vì 10000 không phải là lũy thừa của 2, log210000 sẽ không phải là số nguyên.
2. Mục tiêu là tìm hai số nguyên liên tiếp mà giá trị logarit này nằm giữa.
3. Biết rằng 213= 8192 và 214= 16384, log210000 nằm giữa 13 và 14.

Các số nguyên liên tiếp xung quanh log210000 là 13 và 14.

Câu hỏi Tổng hợp với Xác minh
Giá trị của log210000 nằm giữa các số nguyên liên tiếp m và n. Tìm m+n.

Lời giải Tổng hợp và Câu trả lời Cuối cùng
Để tìm các số nguyên liên tiếp mà log210000 nằm giữa, chúng ta cần tìm các lũy thừa của 2 nhỏ hơn và lớn hơn ngay lập tức so với 10000.

Đầu tiên, hãy tìm lũy thừa lớn nhất của 2 nhỏ hơn 10000. Bắt đầu với 21, 22, ... và tiếp tục, chúng ta tìm thấy:
210= 1024, 211= 2048, 212= 4096, 213= 8192, 214= 16384

Ở đây, chúng ta thấy rằng 213<10000 <214, có nghĩa là log210000 lớn hơn 13 nhưng nhỏ hơn 14.

Do đó, các số nguyên liên tiếp m và n mà log210000 nằm giữa là m= 13 và n= 14.

Bây giờ chúng ta sẽ cộng hai số nguyên này để tìm tổng cần thiết:
m+n= 13 + 14 = 27.

Câu trả lời là 27.

--- TRANG 16 ---
D Nghiên cứu Trường hợp Phản hồi Mô hình

D.1 GSM8K

Câu hỏi và Lời giải Chuẩn
Câu hỏi: Một thợ săn kho báu tìm thấy một rương kho báu chôn đầy đá quý. Có 175 viên kim cương, ít hơn 35 viên ruby so với kim cương, và số viên ngọc lục bảo gấp đôi số viên ruby. Có bao nhiêu viên đá quý trong rương?

Lời giải: Có 175 - 35 = 140 viên ruby trong rương kho báu. Có 140 x 2 = 280 viên ngọc lục bảo trong rương kho báu. Vậy, tổng số viên đá quý trong rương kho báu là 175 + 140 + 280 = 595.

Phản hồi – Xwin-Math-7B (7.5k dữ liệu) Giải mã Tham lam
Đầu tiên, hãy tìm ra số viên ruby. Chúng ta được nói có ít hơn 35 viên ruby so với kim cương. Vì có 175 viên kim cương, chúng ta có thể tính số viên ruby như sau:
Số viên ruby = Số viên kim cương - 35 = 175 - 35 = 140

Tiếp theo, chúng ta cần tìm ra số viên ngọc lục bảo. Chúng ta được nói có gấp đôi số viên ngọc lục bảo so với ruby. Vậy chúng ta có thể tính số viên ngọc lục bảo như sau:
Số viên ngọc lục bảo = 2 × Số viên ruby = 2 × 140 = 280

Bây giờ, chúng ta có thể tìm tổng số viên đá quý bằng cách cộng số viên kim cương, ruby và ngọc lục bảo lại với nhau:
Tổng số viên đá quý = Số viên kim cương + Số viên ruby + Số viên ngọc lục bảo = 175 + 140 + 280 = 605

Câu trả lời là: 605. Sai Tính toán Sai

Phản hồi – Xwin-Math-7B (7.5k dữ liệu) Một Mẫu từ Temperature=0.7
Hãy tìm số lượng mỗi loại đá quý trong rương từng bước.

Đầu tiên, chúng ta được cho số viên kim cương:
Số viên kim cương = 175

Tiếp theo, chúng ta được nói có ít hơn 35 viên ruby so với kim cương. Để tìm số viên ruby, chúng ta trừ 35 khỏi số viên kim cương:
Số viên ruby = Số viên kim cương - 35 = 175 - 35 = 140

Sau đó, chúng ta được nói có gấp đôi số viên ngọc lục bảo so với ruby. Để tìm số viên ngọc lục bảo, chúng ta nhân số viên ruby với 2:
Số viên ngọc lục bảo = 2 * Số viên ruby = 2 * 140 = 280

Bây giờ, chúng ta cộng số viên kim cương, ruby và ngọc lục bảo lại với nhau để tìm tổng số viên đá quý trong rương:
Tổng số viên đá quý = Số viên kim cương + Số viên ruby + Số viên ngọc lục bảo = 175 + 140 + 280 = 595

Câu trả lời là: 595. Đúng

Phản hồi – Xwin-Math-7B (960k dữ liệu) Giải mã Tham lam
Hãy phân tích thông tin được đưa ra trong bài toán từng bước.

1. Chúng ta biết có 175 viên kim cương.
2. Có ít hơn 35 viên ruby so với kim cương. Vì có 175 viên kim cương, chúng ta có thể tính số viên ruby bằng cách trừ 35 khỏi 175.

Số viên ruby = Số viên kim cương - 35 = 175 - 35 = 140

3. Có gấp đôi số viên ngọc lục bảo so với ruby. Chúng ta có thể tìm số viên ngọc lục bảo bằng cách nhân số viên ruby với 2.

Số viên ngọc lục bảo = 2 × Số viên ruby = 2 × 140 = 280

Bây giờ, để tìm tổng số viên đá quý trong rương, chúng ta cộng số viên kim cương, ruby và ngọc lục bảo lại với nhau.

Tổng số viên đá quý = Số viên kim cương + Số viên ruby + Số viên ngọc lục bảo = 175 + 140 + 280 = 595

Câu trả lời là: 595. Đúng

--- TRANG 17 ---
D.2 MATH

Câu hỏi và Lời giải Chuẩn
Câu hỏi: Nếu f(x) = (3x−2)/(x−2), giá trị của f(−2) +f(−1) +f(0) là bao nhiêu? Biểu diễn câu trả lời của bạn dưới dạng phân số thông thường.

Lời giải: f(−2)+f(−1)+f(0) = (3(−2)−2)/(−2−2) + (3(−1)−2)/(−1−2) + (3(0)−2)/(0−2) = (−8)/(−4) + (−5)/(−3) + (−2)/(−2) = 2 + 5/3 + 1 = 14/3

Phản hồi – Xwin-Math-7B (7.5k dữ liệu) Giải mã Tham lam
Để tìm giá trị của f(−2) +f(−1) +f(0), chúng ta cần đánh giá hàm số f(x) tại ba giá trị x này.

Đầu tiên, hãy tìm f(−2): f(x) = (3x−2)/(x−2) Thay x=−2: f(−2) = (3(−2)−2)/(−2−2) = (−6−2)/(−4) = (−8)/(−4)

Bây giờ, hãy tìm f(−1): Thay x=−1: f(−1) = (3(−1)−2)/(−1−2) = (−3−2)/(−3) = (−5)/(−3)

Cuối cùng, hãy tìm f(0): Thay x= 0: f(0) = (3(0)−2)/(0−2) = (−2)/(−2) = 1

Bây giờ, chúng ta có thể tìm tổng: f(−2) +f(−1) +f(0) = (−8)/(−4) + (−5)/(−3) + 1

Để cộng các phân số này, chúng ta cần tìm mẫu số chung, là bội số chung nhỏ nhất (BCNN) của ba mẫu số (-4, -2, và -3). BCNN của các mẫu số này là -12.

Bây giờ, chúng ta có thể viết lại mỗi phân số với mẫu số chung:
(−8)/(−4) = 4/3, (−5)/(−3) = 5/3, 1 = 12/12

Bây giờ, chúng ta có thể cộng các phân số: 4/3 + 5/3 + 12/12 = (4+5+12)/3 = 19/3

Câu trả lời là: 19/3. Sai Tính toán Sai Không hợp lý

Phản hồi – Xwin-Math-7B (7.5k dữ liệu) Một Mẫu từ Temperature=0.7
Để tìm giá trị của f(−2) +f(−1) +f(0), trước tiên chúng ta cần đánh giá mỗi hàm số tại các giá trị cụ thể của x này.

Cho f(−2): f(x) = (3x−2)/(x−2) Thay x=−2 vào phương trình: f(−2) = (3(−2)−2)/(−2−2) = (−6−2)/(−4) = (−8)/(−4) = 2

Cho f(−1): f(x) = (3x−2)/(x−2) Thay x=−1 vào phương trình: f(−1) = (3(−1)−2)/(−1−2) = (−3−2)/(−3) = (−5)/(−3) = 5/3

Cho f(0): f(x) = (3x−2)/(x−2) Thay x= 0 vào phương trình: f(0) = (3(0)−2)/(0−2) = (−2)/(−2) = 1

Bây giờ chúng ta có thể cộng các giá trị của f(−2), f(−1), và f(0): f(−2) +f(−1) +f(0) = 2 + 5/3 + 1 = 14/3

Câu trả lời là: 14/3. Đúng

Phản hồi – Xwin-Math-7B (480k dữ liệu) Giải mã Tham lam
Để tìm giá trị của f(−2) +f(−1) +f(0), chúng ta cần đánh giá hàm số f(x) tại mỗi điểm cho trước và sau đó tính tổng các kết quả.

Đầu tiên, hãy tìm f(−2): f(−2) = (3(−2)−2)/((−2)−2) = (−6−2)/(−2−2) = (−8)/(−4) = 2

Tiếp theo, hãy tìm f(−1): f(−1) = (3(−1)−2)/((−1)−2) = (−3−2)/(−1−2) = (−5)/(−3) = 5/3

Cuối cùng, hãy tìm f(0): f(0) = (3(0)−2)/(0−2) = (0−2)/(−2) = (−2)/(−2) = 1

Bây giờ, chúng ta có thể tính tổng các giá trị này: f(−2) +f(−1) +f(0) = 2 + 5/3 + 1 Để cộng những số này, chúng ta cần một mẫu số chung, là 3: 2 + 5/3 + 1 = (2·3)/3 + 5/3 + (1·3)/3 = 6/3 + 5/3 + 3/3

Bây giờ, chúng ta cộng các tử số: 6/3 + 5/3 + 3/3 = (6+5+3)/3 = 14/3

Câu trả lời là: 14/3. Đúng
