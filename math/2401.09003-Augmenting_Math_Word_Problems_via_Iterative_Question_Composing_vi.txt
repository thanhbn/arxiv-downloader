# 2401.09003.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2401.09003.pdf
# Kích thước tệp: 255392 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tăng cường Bài toán Từ Toán học thông qua Sáng tác Câu hỏi Lặp lại
Haoxiong Liu1*†, Yifan Zhang1*, Yifan Luo1 2, Andrew Chi-Chih Yao1 2
1Viện Khoa học Thông tin Liên ngành, Đại học Thanh Hoa, 2Viện Qizhi Thượng Hải
{liuhx20,zhangyif21,luoyf24 }@mails.tsinghua.edu.cn, andrewcyao@tsinghua.edu.cn
Tóm tắt
Mặc dù có những tiến bộ trong các mô hình ngôn ngữ lớn (LLM) cho suy luận toán học, việc giải quyết các bài toán toán học cấp độ thi đấu vẫn là một thách thức đáng kể, đặc biệt đối với các LLM mã nguồn mở không có công cụ bên ngoài. Chúng tôi giới thiệu bộ dữ liệu MMIQC, bao gồm hỗn hợp dữ liệu web đã xử lý và các cặp câu hỏi-phản hồi tổng hợp, nhằm nâng cao khả năng suy luận toán học của các mô hình ngôn ngữ cơ sở. Các mô hình được tinh chỉnh trên MMIQC liên tục vượt trội so với các mô hình tương ứng về hiệu suất trên điểm chuẩn MATH qua các kích thước mô hình khác nhau. Đáng chú ý, Qwen-72B-MMIQC đạt được độ chính xác 45.0%, vượt qua kỷ lục mã nguồn mở trước đó 8.2% và vượt trội hơn phiên bản GPT-4 ban đầu được phát hành năm 2023. Kết quả đánh giá mở rộng trên kỳ thi cuối cấp trung học Hungary cho thấy rằng sự cải thiện như vậy có thể khái quát hóa cho dữ liệu chưa từng thấy. Nghiên cứu loại bỏ của chúng tôi về MMIQC tiết lộ rằng một phần lớn của sự cải thiện có thể được quy cho phương pháp tăng cường mới của chúng tôi, Sáng tác Câu hỏi Lặp lại (IQC), bao gồm việc lặp lại sáng tác các câu hỏi mới từ các bài toán hạt giống bằng LLM và áp dụng lấy mẫu từ chối thông qua một LLM khác.
Mã nguồn —
https://github.com/iiis-ai/IterativeQuestionComposing
Bộ dữ liệu —
https://huggingface.co/datasets/Vivacem/MMIQC
Giới thiệu
Mặc dù các mô hình ngôn ngữ lớn đã được chứng minh là mạnh mẽ trong nhiều ứng dụng khác nhau (Chen et al. 2021; Brown et al. 2020; Ouyang et al. 2022; Park et al. 2023; Huang et al. 2022b), việc giải quyết các bài toán toán học đòi hỏi kỹ năng suy luận phức tạp vẫn là một nhiệm vụ thách thức. Trên MATH (Hendrycks et al. 2021b), một điểm chuẩn bài toán toán học cấp độ thi đấu chứa các bài toán đại số, giải tích, hình học, tổ hợp và lý thuyết số, các LLM cơ sở mã nguồn mở như họ LLaMA (Touvron et al. 2023a,b) không thể trả lời chính xác hầu hết các bài toán.
Công việc trước đây cố gắng nâng cao khả năng suy luận toán học của các mô hình cơ sở bằng cách tinh chỉnh chúng trên dữ liệu chuyên ngành cụ thể. Cụ thể, một hướng công việc (Azerbayev
*Những tác giả này đóng góp bằng nhau.
†Tác giả tương ứng.
Bản quyền © 2025, Hiệp hội Thúc đẩy Trí tuệ Nhân tạo (www.aaai.org). Tất cả quyền được bảo lưu.

[Tiếp tục với phần còn lại...]

--- TRANG 2 ---
Mistral-7B Mistral-7B-
MetaMathQAMistral-7B-
MMIQC*01020304050MATH PASS@1 CoT Độ chính xác (%)13.128.236.0Mô hình cơ sở: Mistral-7B
Llemma-34B Llemma-34B-
MetaMathQALlemma-34B-
MMIQC*01020304050
25.034.838.6Mô hình cơ sở: Llemma-34B
DeepSeek-67B DeepSeek-67B-
MetaMathQADeepSeek-67B-
MMIQC*01020304050
18.736.841.0Mô hình cơ sở: DeepSeek-67B
Qwen-72B Qwen-72B-
MetaMathQA*Qwen-72B-
MMIQC*01020304050
35.241.745.0Mô hình cơ sở: Qwen-72BHình 2: Hiệu suất của các mô hình cơ sở và các phiên bản tinh chỉnh của chúng trên điểm chuẩn MATH. Các mô hình được đánh dấu bằng ∗ được huấn luyện và đánh giá bởi chúng tôi. Chúng ta có thể thấy rằng các mô hình được tinh chỉnh trên MMIQC liên tục vượt trội so với các mô hình tương ứng với biên độ rõ ràng.

Cụ thể, MMIQC chứa khoảng 1200k cặp câu hỏi-phản hồi mà chúng tôi đã lọc và xử lý trước từ các trang web tại math.stackexchange.com, được bao gồm trong bộ dữ liệu RedPajama (Computer 2023). Mặt khác, đối với phần dữ liệu tổng hợp của MMIQC, chúng tôi tăng sự đa dạng bằng cách sử dụng nhiều loại phương pháp tăng cường được liệt kê dưới đây: 1) Nhắc GPT-4 với phiên bản tích hợp của các lời nhắc khởi động câu hỏi được sử dụng trong (Yu et al. 2023), và thực hiện lấy mẫu từ chối với GPT-3.5-Turbo trên cả bài toán hạt giống và bài toán được tăng cường. 2) Sử dụng lời nhắc được chỉnh sửa được trình bày trong (Liu et al. 2023) để yêu cầu GPT-4 tạo ra các bài toán tương tự với câu trả lời đã cho bài toán hạt giống của tập huấn luyện MATH. Mặc dù các câu trả lời được tạo ra có thể sai, chúng tôi cũng thực hiện lấy mẫu từ chối trên những bài toán này. 3) Thực hiện IQC (Sáng tác Câu hỏi Lặp lại) với tổng cộng 4 lần lặp. Chúng tôi lặp lại yêu cầu GPT-4 sáng tác các câu hỏi mới từ các bài toán hạt giống đã cho và thực hiện lấy mẫu từ chối để lọc những bài toán có câu trả lời phù hợp với câu trả lời của GPT-3.5-turbo. 4) Lọc một tập con 204k của MetaMathQA (Yu et al. 2023) và thêm nó vào bộ dữ liệu MMIQC (Thêm chi tiết về MMIQC sẽ được giới thiệu trong Phần ).

Chúng tôi tinh chỉnh một số mô hình cơ sở trên MMIQC, dẫn đến các mô hình liên tục đạt được biên độ lớn so với các mô hình tương ứng khi được đánh giá trên MATH, như được hiển thị trong Hình 2. Cụ thể, các mô hình Mistral-7B-MMIQC, Llemma-34B-MMIQC, DeepSeek-67B-MMIQC và Qwen-72B-MMIQC, được thu được bằng cách tinh chỉnh Mistral-7B (Jiang et al. 2023), Llemma-34B (Azerbayev et al. 2023) và DeepSeek-67B (Bi et al. 2024) trên MMIQC, đạt được độ chính xác 36.0%, 38.6%, 41.0% và 45.0% trên MATH, cao hơn 5.8%, 3.8%, 4.2% và 3.3% so với các mô hình tương ứng được tinh chỉnh trên MetaMathQA, tương ứng.

Chúng tôi cũng đánh giá các mô hình trên kỳ thi cuối cấp trung học quốc gia Hungary năm 2023 về toán học (Paster 2023b). Kết quả trong Hình 1 cho thấy rằng khả năng suy luận toán học mà các mô hình thu được thông qua việc được tinh chỉnh trên MMIQC có thể khái quát hóa cho các bài toán dự trữ chưa từng thấy.

Chúng tôi nhấn mạnh đóng góp của mình như sau:
• Chúng tôi đề xuất IQC (Sáng tác Câu hỏi Lặp lại), một phương pháp tăng cường dữ liệu có thể lặp lại tạo ra dữ liệu đa dạng bắt đầu từ một bộ dữ liệu hạt giống của các bài toán từ toán học.
• Chúng tôi phát hành MMIQC, một hỗn hợp dữ liệu web đã xử lý và các cặp câu hỏi-phản hồi tổng hợp. Ở các kích thước mô hình khác nhau, các mô hình được tinh chỉnh trên MMIQC liên tục vượt trội so với các mô hình tương ứng với biên độ rõ ràng trên tập kiểm tra MATH. Đáng chú ý, Qwen-72B-MMIQC đạt được độ chính xác 45.0%, vượt qua kỷ lục mã nguồn mở trước đó¹ 8.2% và vượt trội hơn phiên bản GPT-4 ban đầu được phát hành năm 2023. Sự cải thiện như vậy có thể khái quát hóa cho dữ liệu dự trữ chưa từng thấy, ví dụ như kỳ thi cuối cấp trung học Hungary.
• Kết quả của chúng tôi cho thấy rằng việc tái sử dụng dữ liệu chất lượng cao trong kho dữ liệu tiền huấn luyện trong giai đoạn tinh chỉnh có thể cải thiện hiệu suất mô hình, thành công kết hợp hai hướng công việc của tiền huấn luyện liên tục và tinh chỉnh có giám sát.
• Kết quả của chúng tôi cũng cho thấy rằng việc sử dụng nhiều phương pháp tăng cường để xây dựng bộ dữ liệu cho tinh chỉnh là một cách hiệu quả để thúc đẩy hiệu suất của LLM.

Công việc liên quan
Mô hình Ngôn ngữ Lớn Cơ sở. Các mô hình ngôn ngữ lớn cơ sở (LLM) được huấn luyện trên kho dữ liệu lớn (ví dụ 1.4T token văn bản cho Llama (Touvron et al. 2023a)) từ nhiều nguồn khác nhau với một hàm mất mát dự đoán token tiếp theo tự hồi quy đơn giản đã
¹Tại thời điểm viết vào tháng 1 năm 2024, theo hiểu biết của chúng tôi, SOTA mã nguồn mở trên MATH là mô hình DeepSeek-67B-MetaMathQA được báo cáo trong (Wang et al. 2023a), đạt được độ chính xác 36.8% mà không sử dụng công cụ bên ngoài.

--- TRANG 3 ---
đạt được thành công lớn trong nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên khác nhau (Radford et al. 2019; Brown et al. 2020; Touvron et al. 2023a,b; Jiang et al. 2023). Mặc dù những mô hình được tiền huấn luyện này không được dự định để phục vụ cho việc giải quyết các bài toán toán học phức tạp, (Wei et al. 2023) cho thấy rằng nhắc nhở ít mẫu có thể giúp các mô hình trả lời đúng một phần nhất định của các bài toán. Tuy nhiên, để đạt được hiệu suất tốt hơn, việc tinh chỉnh các LLM cơ sở trên dữ liệu chuyên ngành cụ thể là bắt buộc.

Tinh chỉnh LLM Cơ sở trên Bộ dữ liệu Toán học.
Thực hành hiện tại của việc tinh chỉnh LLM cơ sở trên các bộ dữ liệu toán học có thể được phân loại thành hai loại: 1) tiền huấn luyện liên tục (Lewkowycz et al. 2022; Azerbayev et al. 2023). Hướng công việc này thường thu thập dữ liệu văn bản toán học cấp độ tỷ token từ web, chẳng hạn như các trang web con toán học của Stack Exchange và ArXiv, và tinh chỉnh mô hình theo cùng cách như trong giai đoạn tiền huấn luyện. 2) SFT (Tinh chỉnh Có giám sát) (Yuan et al. 2023; Yu et al. 2023; Yue et al. 2023; Gou et al. 2023). Các công việc trong hướng này thu thập các cặp câu hỏi-phản hồi thông qua nhiều phương pháp khác nhau và huấn luyện các mô hình trên bộ dữ liệu của họ theo phong cách Alpaca. Do sự khan hiếm của các bộ dữ liệu cặp câu hỏi-phản hồi chất lượng cao có sẵn công khai và tính chất tốn kém của việc soạn thảo thủ công các bài toán từ toán học, cách tăng cường dữ liệu mới từ các bộ dữ liệu hiện có trở thành trọng tâm của những công việc này.

Công việc của chúng tôi nằm ở giữa hai hướng này: MMIQC là một hỗn hợp của kho dữ liệu tiền huấn luyện đã lọc và các cặp câu hỏi-phản hồi được tạo ra bằng các phương pháp tăng cường khác nhau.

Khung Suy luận để Giải quyết Bài toán Toán học. Nhiều nỗ lực đã được dành cho việc đạt được độ chính xác cao hơn trên các điểm chuẩn bài toán từ toán học bằng cách thiết kế các quy trình khác nhau của việc sử dụng các LLM đã cho để thu được câu trả lời, mà chúng tôi gọi là khung suy luận. Trong số đó, các phương pháp dựa trên nhắc nhở (Radford et al. 2019; Wei et al. 2023; Fu et al. 2022) đóng vai trò quan trọng trong việc kích hoạt khả năng suy luận tiềm năng cho các LLM cơ sở thông qua việc thiết kế cẩn thận các lời nhắc được hiển thị cho các mô hình. Tự nhất quán (Wang et al. 2023b) lấy mẫu nhiều đường dẫn lý luận cho một mô hình và sau đó quyết định câu trả lời bằng bỏ phiếu đa số. Trái ngược với tự nhất quán, (Cobbe et al. 2021; Uesato et al. 2022; Lightman et al. 2023) sử dụng Mô hình Phần thưởng Kết quả (ORM) và Mô hình Phần thưởng Quy trình (PRM) được huấn luyện trên các chú thích của con người làm trình xác minh để giúp chọn câu trả lời có điểm số cao nhất từ các đường dẫn suy luận được lấy mẫu của LLM. Thoát khỏi nhu cầu chú thích thủ công, (Wang et al. 2023a) chấm điểm một bước suy luận đã cho bằng cách ước tính tiềm năng của bước đó để dẫn đến một câu trả lời đúng một cách tự động.

Một số khung cũng bao gồm việc sử dụng các công cụ bổ sung và API bên ngoài. Nhắc nhở hỗ trợ chương trình (Gao et al. 2022; Yue et al. 2023) cung cấp các mẫu trong ngữ cảnh chứa mã Python cho LLM và sử dụng trình thông dịch mã để thực thi đầu ra nhằm tạo điều kiện cho suy luận. Hơn nữa, (Gou et al. 2023) xen kẽ lý luận ngôn ngữ tự nhiên với mã Sympy² và tinh chỉnh mô hình trên các quỹ đạo được lấy mẫu từ GPT-4 để theo khung của họ trong hai bước, đó là học tập bắt chước và định hình không gian đầu ra.

Chúng tôi lưu ý rằng kết quả của chúng tôi trong Hình 2 không bao gồm việc lấy mẫu nhiều lần, sử dụng trình xác minh hoặc trình thông dịch mã, do đó không thể được so sánh trực tiếp với kết quả được báo cáo trong những công việc này.

Sáng tác Câu hỏi Lặp lại
Các phương pháp tăng cường dữ liệu truyền thống chủ yếu tập trung vào việc chỉnh sửa câu hỏi hoặc câu trả lời trong khi vẫn giữ nguyên ý nghĩa ban đầu của chúng, hoặc tạo ra các bài toán tương tự, như được thảo luận trong (Yu et al. 2023) và (Liu et al. 2023). Tuy nhiên, những phương pháp này bị hạn chế về sự đa dạng vì chúng nhằm tạo ra các bài toán gần như giống hệt nhau. Cách tiếp cận của chúng tôi, được gọi là IQC (Sáng tác Câu hỏi Lặp lại), khác biệt với điều này bằng cách lặp lại xây dựng các bài toán phức tạp hơn. Nó tăng cường các bài toán ban đầu, thêm các bước suy luận bổ sung mà không thay đổi cấu trúc logic nội tại của chúng. Điều này đảm bảo rằng các bài toán mới được hình thành có liên kết hữu cơ với bài toán gốc và cẩn thận cố gắng không bao gồm các yếu tố ngoại lai được tạo ra bởi một sự chuyển tiếp lớn của quy trình suy luận.

Ký hiệu. Trong mô tả của chúng tôi, chúng tôi gọi sự kết hợp của một LLM, tokenizer của nó, các phương pháp mã hóa/giải mã, và một cấu hình tạo ra cố định (bao gồm chiến lược tạo ra, nhiệt độ lấy mẫu, và tiêu chí dừng) đơn giản là 'một LLM'. Đối với một LLM π, chúng tôi biểu thị phân phối đầu ra đã cho lời nhắc p∈ A* là π(·|p). Việc nối hai đoạn văn bản p1 và p2 được biểu thị là p1⊕p2.

Quy trình IQC bắt đầu bằng việc chỉ định một LLM πq cho sáng tác câu hỏi và một mô hình khác πr cho lấy mẫu từ chối. Một trình trích xuất câu trả lời là cần thiết để rút ra câu trả lời từ phản hồi. Hai phản hồi r1 và r2 được coi là tương đương, ký hiệu r1≃r2, nếu cùng một câu trả lời có thể được trích xuất từ cả hai. Quy trình bắt đầu với một bộ dữ liệu hạt giống S0={(qi, ai)}n i=1.

Trong lần lặp #1, chúng tôi nhắc πq với p1⊕x(q, a) cho mỗi (q, a)∈S0, trong đó x(·,·) là một mẫu văn bản chuyển đổi một cặp câu hỏi-phản hồi thành văn bản, và p1 yêu cầu một sáng tác câu hỏi-câu trả lời mới. Điều này tạo ra một bộ dữ liệu mới S1={(q′ i, a′ i)}n i=1, trong đó (q′ i, a′ i) =x−1(x′ i) và x′ i∼πq(·|p1⊕xi) là đầu ra cho mẫu thứ i. Chúng tôi tiếp tục nâng cao S1 bằng lấy mẫu từ chối từ πr, dẫn đến R1:={(q′ i, a(j) i)|a(j) i≃a′ i, i∈[n], j∈[m]}, trong đó a(j) i là các phản hồi được lấy mẫu từ πr(·|pr⊕q′ i). Bộ dữ liệu D1 sau đó được hình thành bằng cách kết hợp S1 và R1: D1:=S1∪R1.

Đối với mỗi lần lặp tiếp theo #k, quy trình nói trên được lặp lại bằng cách sử dụng Sk−1 làm bộ dữ liệu hạt giống, với các lời nhắc sáng tác câu hỏi pk khác nhau. Quy trình IQC hoàn chỉnh được mô tả chi tiết trong Thuật toán 1.

Câu hỏi Hạt giống:
Đánh giá
(5a2−13a+ 4)(2 a−3)
với a= 11 2.
Câu hỏi Lần lặp # 1:
Nếu b= 2a−3 và a= 11 2, giá trị của (5a2−13a+ 4)b là gì?
Câu hỏi Lần lặp # 2:
Cho b= 2a−3, a= 11 2, và c= 3b+ 5, tìm giá trị của c(5a2−13a+ 4).
Câu hỏi Lần lặp # 3:
Cho b= 2a−3, a= 11 2, c= 3b+ 5, và d= c2−4c, tìm giá trị của d+c(5a2−13a+ 4).
Câu hỏi Lần lặp # 4:
Cho b= 2a−3, a= 11 2, c= 3b+ 5, d=c2−4c, và e=d3+ 2cd−7, tìm giá trị của e+c(5a2− 13a+ 4) + d.

Hình 3: Một ví dụ về các câu hỏi được sáng tác thông qua IQC bởi GPT-4 được đưa ra 1 bài toán hạt giống trong tập huấn luyện MATH.

Bộ dữ liệu MMIQC
Trong phần này, chúng tôi giới thiệu cách mỗi phần của MMIQC được xây dựng chi tiết.

Tập con của MetaMathQA. Bộ dữ liệu MetaMathQA gốc được xây dựng bằng cách lấy mẫu GPT-3.5 k= 20 lần dưới nhiệt độ T= 0.7 cho mỗi bài toán trong tập huấn luyện của MATH (Hendrycks et al. 2021a) và GSM8K (Cobbe et al. 2021), hoặc các phiên bản được khởi động của nó. Chúng tôi hạn chế số lượng mẫu cho mỗi câu hỏi hoàn toàn giống nhau là 3 và 1 cho MATH và GSM8K, tương ứng, để thu được một tập con của MetaMathQA. Tập con này chứa 112.2K cặp câu hỏi-phản hồi GSM8K và 91.5K cặp MATH.

Tăng cường Câu trả lời và Khởi động Câu hỏi.
Chúng tôi tích hợp các phương pháp khởi động câu hỏi được sử dụng trong (Yu et al. 2023) vào một lời nhắc duy nhất được hiển thị trong Hình 5. Động lực của chúng tôi là với GPT-4 có khả năng hiểu ngôn ngữ tự nhiên cao, phong cách nhắc nhở ít mẫu được sử dụng trong (Yu et al. 2023) có thể ức chế sự đa dạng của các câu hỏi được tăng cường. Bộ dữ liệu hạt giống được xây dựng bởi các mẫu trong tập huấn luyện của MATH không chứa ngôn ngữ Asymptote trong các tuyên bố câu hỏi của chúng. Chúng tôi thực hiện lấy mẫu từ chối từ GPT-3.5 trên cả bộ dữ liệu hạt giống và các câu hỏi được tạo ra bằng cách sử dụng lời nhắc được hiển thị trong Hình 6, thu được 66.5K cặp câu hỏi-phản hồi. Chúng tôi sử dụng nhiệt độ T= 1.0 cho cả khởi động câu hỏi và lấy mẫu từ chối.

Bài toán Tương tự được Tăng cường. Với cùng bộ dữ liệu hạt giống, chúng tôi yêu cầu GPT-4 tạo ra 3 bài toán (với một lời giải, cho lấy mẫu từ chối) cho 1 bài toán hạt giống mỗi lần, sử dụng lời nhắc trong Hình 7. Điều này khác với thực hành trong (Liu et al. 2023), nơi họ yêu cầu GPT-3.5 tạo ra 10 câu hỏi tương tự được đưa ra 1 bài toán hạt giống vì chúng tôi thấy rằng GPT có xu hướng tạo ra một số bài toán gần như giống nhau bất kể bài toán hạt giống được đưa ra khi được yêu cầu tạo ra tới 10 bài toán mới. Chúng tôi sử dụng GPT-4 mạnh hơn thay vì GPT-3.5 xem xét lấy mẫu từ chối cần câu trả lời cho bài toán tốt hơn để đúng. Để kiểm soát chi phí, lời nhắc của chúng tôi nhấn mạnh rằng lời giải nên càng ngắn gọn càng tốt. Tổng số các bài toán tương tự được tăng cường và các cặp câu hỏi-phản hồi được lấy mẫu từ chối từ chúng là 38.2K. Lời nhắc lấy mẫu từ chối cũng giống như lời nhắc trong Hình 6. Chúng tôi sử dụng nhiệt độ T= 1.0 cho cả hai quy trình.

Sáng tác Câu hỏi Lặp lại. Chúng tôi thực hiện Sáng tác Câu hỏi Lặp lại cho 4 lần lặp như được mô tả trong Phần. Cụ thể, chúng tôi sử dụng GPT-4 cho mô hình sáng tác câu hỏi πq với nhiệt độ T= 0.7 và GPT-3.5 cho mô hình lấy mẫu từ chối πr với nhiệt độ T= 1.0. Các lời nhắc sáng tác câu hỏi và lời nhắc lấy mẫu từ chối được hiển thị trong Hình 4 và Hình 6, tương ứng. Mẫu văn bản x(·,·) mà chúng tôi sử dụng là một chương trình chuyển đổi mỗi cặp câu hỏi-phản hồi thành định dạng văn bản JSON, với các trường 'problem' và 'solution'. Bộ dữ liệu hạt giống cũng là các mẫu trong tập huấn luyện của MATH không chứa mã Asymptote trong các tuyên bố câu hỏi của chúng. Bộ dữ liệu kết quả có tổng cộng 55.1K mẫu.³ Chúng tôi cung cấp một ví dụ về các câu hỏi được tạo ra trong các lần lặp khác nhau tương ứng với cùng một bài toán hạt giống trong Hình 3. Chúng tôi lưu ý rằng mặc dù một số câu hỏi không nghiêm ngặt là một bài toán con hoặc bước con của bài toán tương ứng trong lần lặp trước như yêu cầu trong lời nhắc của chúng tôi, chúng vẫn là những câu hỏi hợp lệ có thể tăng sự đa dạng của bộ dữ liệu. Chúng tôi đã kiểm tra tính chính xác của 100 cặp QA được chọn ngẫu nhiên được tạo ra bởi IQC và thấy rằng 85% trong số chúng là đúng.

Mathematics Stack Exchange. Chúng tôi quan sát thấy rằng trong bộ dữ liệu OpenWebMath (Paster et al. 2023), dữ liệu từ Mathematics Stack Exchange cho thấy chất lượng cao và liên quan nhất đến toán học cấp độ thi đấu. Được thúc đẩy bởi điều này, chúng tôi trích xuất dữ liệu được thu thập từ Mathematics Stack Exchange trong RedPajama (Computer 2023) và xử lý trước nó thành các cặp câu hỏi-phản hồi. Đối với mỗi trang Mathematics Stack Exchange, chúng tôi chỉ giữ lại câu trả lời được xếp hạng đầu tiên bởi RedPajama. Sau đó chúng tôi lọc ra câu trả lời không chứa ký hiệu môi trường công thức '$'. Điều này dẫn đến một bộ dữ liệu với 1203.6K cặp câu hỏi-phản hồi.

Bảng 1 hiển thị thành phần của MMIQC. Khi tinh chỉnh các mô hình MMIQC chứa 3 lần lặp lại của các tập con được đề cập ở trên, ngoại trừ phần Mathematics Stack Exchange. Chúng tôi xáo trộn thứ tự của các mẫu sau khi kết hợp các tập con.

³Một phần của các mẫu được tạo ra bằng cách thực hiện IQC cho 2 lần lặp bằng cách sử dụng một phiên bản di sản của các lời nhắc.

Bảng 2: Nghiên cứu loại bỏ về tỷ lệ học tập tối ưu. Chúng tôi tinh chỉnh Mistral-7B trên MMIQC với các giá trị tỷ lệ học tập tối đa khác nhau và đánh giá các mô hình tinh chỉnh trên MATH để quyết định ứng cử viên tốt nhất.
LR 1 E-6 5 E-6 1E-5 2E-5 5 E-5 1 E-4
MATH(%) 32.3 35.1 36.0 35.4 31.5 27.1

Thí nghiệm
Thiết lập Tinh chỉnh
Chiến lược tinh chỉnh của chúng tôi chủ yếu tuân theo thực hành của (Taori et al. 2023), ngoại trừ chúng tôi sử dụng một mẫu lời nhắc khác để chuyển đổi các cặp câu hỏi-phản hồi. Đối với một mẫu từ Mathematics Stack Exchange, lời nhắc tương ứng được đưa vào mô hình trong quá trình huấn luyện là một nối đơn giản của câu hỏi và phản hồi với hai ký hiệu dòng mới. Đối với một mẫu từ các tập con khác, chúng tôi bổ sung thêm tiền tố 'Vui lòng giải quyết bài toán sau đây và đặt câu trả lời của bạn ở cuối với "Câu trả lời là: ".' vào việc nối câu hỏi-phản hồi.

Chúng tôi sử dụng thư viện HuggingFace transformers (Wolf et al. 2019) cho các thí nghiệm tinh chỉnh của chúng tôi.

Chúng tôi tinh chỉnh tất cả các mô hình trên MMIQC cho 1 epoch, sử dụng lịch trình tỷ lệ học tập tuyến tính với tỷ lệ khởi động 3%. Đối với việc chọn tỷ lệ học tập tối đa, chúng tôi thực hiện một thí nghiệm lựa chọn siêu tham số đơn giản được hiển thị trong Bảng 2 và xác định nó là 1e-5. Chúng tôi sử dụng định dạng số BFloat16 trong quá trình huấn luyện. Sử dụng DeepSpeed Zero-3 Stage (Rajbhandari et al. 2020), chúng tôi tinh chỉnh các mô hình 7B trên một nút 8xA800 GPU với kích thước micro batch là 8, và tích lũy gradient là 4, các mô hình 34B trên 2 nút với kích thước micro batch là 4 và tích lũy gradient là 4 và các mô hình ∼70B trên 4 nút với kích thước micro batch là 4 và tích lũy gradient là 2, duy trì kích thước batch hiệu quả là 256. Phải mất khoảng 14 giờ, 61 giờ và 90 giờ để tinh chỉnh các mô hình 7B, 34B và ∼70B dưới các thiết lập được nêu ở trên, tương ứng.

Đánh giá Mô hình
Để so sánh công bằng, trước tiên chúng tôi đánh giá các mô hình tinh chỉnh trên MATH (Hendrycks et al. 2021a), một điểm chuẩn bài toán từ toán học cấp độ thi đấu với 5000 bài toán kiểm tra trong thiết lập không mẫu. Chúng tôi nhắc tất cả các mô hình tinh chỉnh của chúng tôi với câu hỏi kiểm tra với tiền tố 'Vui lòng giải quyết bài toán sau đây và đặt câu trả lời của bạn ở cuối với "Câu trả lời là: ".', và trích xuất câu trả lời từ đầu ra bằng cách sử dụng một phiên bản đã chỉnh sửa của trình trích xuất câu trả lời được cung cấp trong (Lewkowycz et al. 2022). Chúng tôi sử dụng một loạt các quy tắc để suy ra liệu câu trả lời được trích xuất có giống với câu trả lời chính xác không, bao gồm so sánh bằng cách sử dụng SymPy (Meurer et al. 2017). Kết quả hoàn chỉnh của đánh giá của chúng tôi trên MATH và so sánh với các mô hình hiện có được hiển thị trong Bảng 3.

Đối với đánh giá trên kỳ thi cuối cấp trung học quốc gia Hungary năm 2023 về toán học, chúng tôi sử dụng lời nhắc ít mẫu được sử dụng trong (Paster 2023b). Chúng tôi đánh giá thủ công điểm số cho mỗi mô hình theo hướng dẫn của giám khảo. Kết quả được hiển thị trong Hình 1 là điểm số dưới điểm tối đa là 117.

--- TRANG 6 ---
Bạn sẽ được cung cấp 1 bài toán toán học ở định dạng json phân tách bằng dòng mới. Vui lòng tăng cường 5 bài toán đa dạng từ bài toán đã cho.
Cách bạn tăng cường một bài toán có thể là:
- Diễn đạt lại bài toán.
- Thay đổi kịch bản mà không chỉnh sửa các số lượng cụ thể.
- Đặt 1 số trong bài toán thành một biến ẩn, đặt câu trả lời trong bài toán và hỏi giá trị của biến là gì. Đảm bảo bài toán được tạo ra là hợp lý. Nếu không, bỏ qua phương pháp này.
- Các cách tiếp cận khác có thể đảm bảo tính chính xác của câu trả lời bạn cung cấp cho bài toán được tăng cường.
Phản hồi của bạn chỉ nên chứa văn bản ở định dạng json phân tách bằng dòng mới, giữ nguyên như bài toán đã cho.
Vui lòng sử dụng hai dấu gạch chéo ngược để đại diện cho một trong các chuỗi.

Hình 5: Lời nhắc chúng tôi sử dụng để thực hiện khởi động câu hỏi cho việc yêu cầu GPT-4.

Bạn sẽ được trình bày một bài toán toán học. Bạn nên giải quyết bài toán từng bước một cách cẩn thận. Trình bày câu trả lời cuối cùng ở định dạng latex boxed, ví dụ, 63π.

Hình 6: Lời nhắc chúng tôi sử dụng để thực hiện lấy mẫu từ chối từ GPT.

Bạn sẽ được cung cấp 1 bài toán toán học ở định dạng json phân tách bằng dòng mới. Vui lòng tạo ra 3 bài toán mới đa dạng tương tự như bài toán đã cho.
Phản hồi của bạn chỉ nên chứa văn bản ở định dạng json phân tách bằng dòng mới, giữ nguyên như bài toán đã cho. Lời giải cho các bài toán được tạo ra nên càng ngắn gọn càng tốt. Đảm bảo chỉ có một hộp trong lời giải và câu trả lời hoàn toàn giống với nội dung trong hộp. Vui lòng sử dụng hai dấu gạch chéo ngược để đại diện cho một trong các chuỗi.

Hình 7: Lời nhắc chúng tôi sử dụng để tạo ra câu hỏi tương tự với các bài toán hạt giống cho việc yêu cầu GPT-4.

Nghiên cứu Loại bỏ trên Các tập con của MMIQC
Để hiểu tỷ lệ đóng góp cho sự cải thiện được tiết lộ trong Bảng 3 của các tập con khác nhau của MMIQC, chúng tôi tinh chỉnh Mistral-7B với một loạt các tập huấn luyện được xây dựng bằng cách dần dần thêm các tập con. Khi MathStackExchange không được thêm vào, chúng tôi tinh chỉnh cho 3 epoch. Khi MathStackExchange được thêm vào bộ dữ liệu huấn luyện, chúng tôi trộn 3 lần lặp lại của dữ liệu khác với 1 lần lặp lại của MathStackExchange, và tinh chỉnh chỉ cho 1 epoch. Có thể thấy từ Bảng 4 rằng

• Mặc dù tập con đã lọc của chúng tôi từ MetaMathQA chỉ bằng một nửa kích thước của bộ dữ liệu gốc (có 395K mẫu, nhiều hơn tổng số mẫu của dữ liệu tổng hợp của chúng tôi), sự sụt giảm hiệu suất chỉ là 1.8%. Điều này cho thấy rằng chiến lược k= 20 trong (Yu et al. 2023) dẫn đến một số dư thừa.

• Dữ liệu Tăng cường Câu trả lời & Thúc đẩy Câu hỏi của chúng tôi giúp mô hình tinh chỉnh đánh bại Mistral-7B-MetaMathQA, xác minh giả thuyết của chúng tôi rằng việc trực tiếp yêu cầu GPT thực hiện khởi động câu hỏi hiệu quả hơn việc cung cấp ví dụ ít mẫu cho chúng.

• Phương pháp IQC của chúng tôi dẫn đến sự cải thiện đáng kể 3.1% từ độ chính xác cao 31.5% chỉ với 55.1K mẫu, cho thấy hiệu quả của nó. Hơn nữa, các lần lặp sau của IQC cũng chiếm một tỷ lệ nhất định của sự cải thiện, chứng minh rằng IQC là một phương pháp có thể liên tục tạo ra dữ liệu mới có thể giúp tăng sự đa dạng khi được thêm vào dữ liệu được tạo ra trong các lần lặp trước.

Kiểm tra Nhiễm bẩn
Chúng tôi kiểm tra các kết quả khớp n-gram cho MMIQC để đảm bảo rằng sự cải thiện không phải là kết quả của việc ghi nhớ trực tiếp. Chúng tôi sử dụng script được cung cấp bởi (Azerbayev et al. 2023) để kiểm tra các kết quả khớp n-gram giữa phần tổng hợp của MMIQC và tập kiểm tra MATH. Hóa ra đối với kiểm tra khớp 30-gram, có 44 lần trúng khớp giữa trường 'solution' của tập kiểm tra MATH và trường 'output' của MMIQC, ít hơn nhiều so với 168 lần trúng giữa đó của tập kiểm tra MATH và tập huấn luyện MATH. Hơn nữa, chúng tôi kiểm tra thủ công 44 lần trúng này và thấy rằng 43 trong số chúng thuộc trường hợp các bước trung gian của lời giải cho các câu hỏi tương tự nhưng khác nhau va chạm, với ngoại lệ duy nhất là câu hỏi 'Một đa giác đều có các góc trong là 144 độ. Đa giác có bao nhiêu cạnh?'. Điều này gần như loại trừ khả năng rằng các mô hình tinh chỉnh có ghi nhớ lời giải cho các bài toán trong tập kiểm tra, chỉ ra rủi ro rất thấp của nhiễm bẩn dữ liệu cho MMIQC.

Kết luận
Trong công việc này, chúng tôi giới thiệu một phương pháp tăng cường dữ liệu mới cho các bộ dữ liệu bài toán từ toán học được gọi là IQC (Sáng tác Câu hỏi Lặp lại) và sử dụng nó trong việc xây dựng

--- TRANG 7 ---
Bảng 3: Một phân tích so sánh về độ chính xác đạt được bởi các mô hình khác nhau trên điểm chuẩn MATH. Các mô hình được đánh dấu bằng dấu sao (*) được tinh chỉnh và đánh giá bởi chúng tôi. Các kết quả khác, trừ khi có trích dẫn khác, được rút ra từ (Wang et al. 2023a). So sánh này làm nổi bật những cải thiện đáng kể mà các mô hình tinh chỉnh của chúng tôi thể hiện so với các giải pháp hiện có trong độ chính xác giải quyết bài toán toán học.

[Bảng với các kết quả chi tiết của các mô hình khác nhau]

Bảng 4: Cách các tập con khác nhau của MMIQC ảnh hưởng đến độ chính xác của mô hình tinh chỉnh trên MATH.

[Bảng với kết quả từng bước thêm các tập con]

bộ dữ liệu MMIQC của chúng tôi. Kết quả đánh giá của chúng tôi cho thấy rằng các mô hình được tinh chỉnh trên MMIQC đạt được SOTA mới trên điểm chuẩn MATH. Những cải thiện của các mô hình của chúng tôi có lợi từ các nguồn dữ liệu đa dạng của MMIQC và hiệu quả của IQC.

Đối với các hướng tương lai, chúng tôi quan tâm đến việc làm thế nào để trang bị cho các mô hình mã nguồn mở khả năng sáng tác câu hỏi, để thực hiện IQC theo phong cách tự tiến hóa, tương tự như trong (Huang et al. 2022a). Bên cạnh đó, việc tích hợp các hệ thống xác minh (Wang et al. 2023a; Liu et al. 2023) được sử dụng ban đầu để cải thiện độ chính xác trong thời gian suy luận vào quy trình IQC, cũng là một chủ đề hấp dẫn.

Lời cảm ơn
Chúng tôi cảm ơn Yang Yuan, Kaiyue Wen, Xingyu Dang, và Jingqin Yang về những thảo luận hữu ích của họ.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ]
