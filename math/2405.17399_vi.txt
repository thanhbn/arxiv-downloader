# 2405.17399.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2405.17399.pdf
# Kích thước tệp: 2136068 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Transformers Có Thể Thực Hiện Phép Tính Với
Embeddings Phù Hợp
Sean McLeish1*, Arpit Bansal1∗, Alex Stein1, Neel Jain1, John Kirchenbauer1,
Brian R. Bartoldson2, Bhavya Kailkhura2, Abhinav Bhatele1, Jonas Geiping3,
Avi Schwarzschild4, Tom Goldstein1
1University of Maryland,2Lawrence Livermore National Laboratory,3ELLIS Institute Tübingen,
Max Planck Institute for Intelligent Systems, Tübingen AI Center,4Carnegie Mellon University
Tóm tắt
Hiệu suất kém của transformers trong các tác vụ phép tính dường như chủ yếu xuất phát từ việc chúng không thể theo dõi chính xác vị trí của từng chữ số bên trong một dãy lớn các chữ số. Chúng tôi khắc phục vấn đề này bằng cách thêm một embedding vào mỗi chữ số để mã hóa vị trí của nó so với đầu của số. Ngoài sự cải thiện mà những embeddings này cung cấp một cách riêng lẻ, chúng tôi chỉ ra rằng cách khắc phục này cho phép các sửa đổi kiến trúc như input injection và các lớp hồi quy để cải thiện hiệu suất hơn nữa.
Với vị trí được giải quyết, chúng tôi có thể nghiên cứu khả năng ngoại suy logic của transformers. Liệu chúng có thể giải quyết các bài toán phép tính lớn hơn và phức tạp hơn so với những gì có trong dữ liệu huấn luyện của chúng? Chúng tôi nhận thấy rằng việc huấn luyện chỉ trên các số 20 chữ số với một GPU duy nhất trong một ngày, chúng ta có thể đạt được hiệu suất tiên tiến, đạt tới 99% độ chính xác trên các bài toán cộng 100 chữ số. Cuối cùng, chúng tôi cho thấy những cải thiện về khả năng tính toán này cũng mở ra những cải thiện trong các tác vụ lý luận nhiều bước khác bao gồm sắp xếp và nhân.2
0 50 1000
50
100Positional Embedding:
FIRE
0 50 100Positional Embedding:
Abacus
0255075100
Accuracy
Length of Operand TwoLength of Operand One
Hình 1: Độ chính xác khớp chính xác zero shot trên phép cộng sử dụng mô hình transformer (chỉ decoder) độ sâu mười sáu được huấn luyện trên các toán hạng lên đến 20 chữ số. So với embeddings tiên tiến (trái), Abacus Embeddings mới của chúng tôi (phải) cải thiện đáng kể khả năng tổng quát hóa cho các độ dài chữ số chưa thấy. Phần bên trong của hình vuông đỏ biểu thị phân phối huấn luyện. Độ chính xác được lấy trung bình qua ba thử nghiệm.
∗Đóng góp Bằng nhau, liên hệ tới: smcleish@umd.edu ,bansal01@umd.edu .
2Mã nguồn có sẵn trên GitHub: github.com/mcleish7/arithmetic.
Hội nghị lần thứ 38 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2024).arXiv:2405.17399v2  [cs.LG]  23 Dec 2024

--- TRANG 2 ---
1 Giới thiệu
Phần lớn công trình gần đây về Mô hình Ngôn ngữ Lớn (LLMs) tập trung vào khả năng giải quyết vấn đề trong ngôn ngữ tự nhiên và tạo mã. Mặc dù có tiến bộ trong các lĩnh vực này, transformers vẫn gặp khó khăn trong việc thực hiện các tác vụ lý luận thuật toán và nhiều bước phức tạp trong setting zero shot mà không cần dùng đến công cụ. Để nghiên cứu lý luận thuật toán trong một môi trường phòng thí nghiệm sạch, cộng đồng học thuật tập trung vào các bài toán kiểm tra phép tính đơn giản như phép cộng. Phép cộng đủ đơn giản để các LLMs có kích thước khiêm tốn có thể (về nguyên tắc) được huấn luyện từ đầu để thực hiện nó mà không gặp phải các hạn chế về dung lượng và ngân sách huấn luyện, nhưng cũng đủ phức tạp để ngay cả các mô hình công nghiệp lớn cũng thất bại trên các số lớn mà không có trình thông dịch mã [Loeber, 2024].

Việc huấn luyện transformers cho phép tính cho phép chúng ta nghiên cứu một số câu hỏi quan trọng. Đầu tiên, chúng ta hỏi những lựa chọn thiết kế kiến trúc, đặc điểm tập dữ liệu và biến thể pipeline huấn luyện nào được yêu cầu để học một quá trình lý luận nhiều bước như phép cộng nhiều chữ số? Đi sâu hơn, sau đó chúng ta điều tra liệu những mô hình này có khả năng ngoại suy logic —liệu chúng có thể giải quyết các vấn đề có kích thước và độ khó lớn hơn so với những gì xuất hiện trong tập huấn luyện của chúng?

Các nghiên cứu trước đó cho thấy phép cộng khó đối với transformers [Lee et al., 2023, Shen et al., 2023, Zhou et al., 2023, 2024]. Các thí nghiệm của chúng tôi cho thấy rằng khó khăn này xuất phát từ việc chúng không thể biểu diễn rõ ràng vị trí chính xác của một chữ số trong một chuỗi dài các chữ số. Để giải quyết vấn đề này, chúng tôi đề xuất một sửa đổi đơn giản cho biểu diễn dữ liệu để trực tiếp giải quyết khuyết điểm này.

Abacus Embeddings của chúng tôi là các embeddings vị trí được học đơn giản được sử dụng để mã hóa vị trí trong mỗi span của các token số. Kết hợp Abacus Embeddings và embeddings vị trí tiêu chuẩn, chúng tôi quan sát những cải thiện đáng kể về độ chính xác sao cho các mô hình được huấn luyện với tối đa 20 chữ số toán hạng có thể tổng quát hóa cho các vấn đề với 120 chữ số toán hạng. Điều này đại diện cho yếu tố tổng quát hóa tiên tiến 6×, với tiên tiến trước đó chỉ là 2.5×. Theo hiểu biết của chúng tôi, đây là những chuỗi dài nhất mà việc học phép cộng đã từng được chứng minh.

Chúng tôi cũng nghiên cứu một số phương pháp khác để cải thiện phép tính và tổng quát hóa trong transformers. Chúng tôi nhận thấy rằng việc kết hợp input injection —skip connections được chèn giữa lớp input và mỗi lớp decoder—có thể giảm lỗi tổng quát hóa 50% so với baseline Abacus Embedding. Chúng tôi cũng nhận thấy rằng cùng với embeddings của chúng tôi, các kiến trúc transformer lặp, chứa các lớp hồi quy trong đó các tham số giống nhau được sử dụng lại nhiều lần, có thể đạt được khả năng tổng quát hóa gần hoàn hảo trên các bài toán cộng mà chúng tôi xem xét.

Vì các phương pháp đề xuất của chúng tôi giải quyết thành công các bài toán cộng lớn, chúng tôi đánh giá liệu các cách tiếp cận tương tự có thể được sử dụng để cải thiện các loại học thuật toán khác không. Chúng tôi khám phá các bài toán nhân lên đến 15 chữ số và sắp xếp trên các mảng lên đến 10 số, làm cho đây là nghiên cứu đầu tiên về các kỹ thuật tổng quát hóa độ dài cực đoan cho phép cộng chuyển sang các tác vụ thuật toán khác.

Đóng góp của chúng tôi có thể được tóm tắt như sau.
•Chúng tôi đề xuất một embedding vị trí mới gọi là Abacus Embeddings để nắm bắt tốt hơn tầm quan trọng của từng chữ số, dẫn đến khả năng tổng quát hóa trong phân phối gần hoàn hảo.
•Chúng tôi cho thấy rằng khi kết hợp Abacus Embeddings với input injection và looped transformers, hiệu suất được cải thiện thêm, tăng từ 92.9%lên99.1%trong độ chính xác ngoài phân phối, giảm 87% lỗi so với việc chỉ sử dụng embeddings với kiến trúc tiêu chuẩn.
•Chúng tôi đẩy khả năng tổng quát hóa độ dài vượt ra ngoài công trình hiện có và cho thấy rằng các mô hình của chúng tôi có thể giải quyết các vấn đề với gấp sáu lần số chữ số so với các mẫu lớn nhất trong tập huấn luyện, trong khi tiên tiến trước đó chỉ là hai lần rưỡi.
•Chúng tôi mở rộng các phát hiện của mình sang các vấn đề phức tạp hơn bao gồm nhân và sắp xếp, nơi chúng tôi cho thấy khả năng tổng quát hóa độ dài trong các lĩnh vực này.

2 Công trình Liên quan
Phép tính và Lý luận Thuật toán. Việc giải quyết phép tính với dự đoán token tiếp theo là một vấn đề khó thu hút nhiều sự chú ý [e.g. Saxton et al., 2019]. Tuy nhiên, trong các setting zero-shot, ngay cả các mô hình API thương mại cực kỳ mạnh cũng gặp khó khăn với các bài toán cộng rất lớn (e.g. lên đến 100 chữ số) mà không có quyền truy cập vào các công cụ. Trong số các nỗ lực cải thiện hiệu suất phép tính của các mô hình dựa trên transformer, việc đảo ngược các chữ số để các đối số được viết với chữ số ít quan trọng nhất trước tiên là phổ biến [Lee et al., 2023, Shen et al., 2023, Zhou et al., 2023, 2024]. Hơn nữa, việc thay đổi định dạng dữ liệu bằng cách thêm các ký tự chỉ mục rõ ràng cải thiện khả năng của mô hình cho phép cộng [Zhou et al., 2023, 2024, Olsson et al., 2022]. Công trình khác tiếp cận phép tính bằng cách nhúng số thực bằng cách chia tỷ lệ một token-embedding cố định duy nhất cho số [Golkar et al., 2023]. Hơn nữa, Dziri et al. [2023] cho thấy nhân là một vấn đề khó đối với GPT-3 [Brown et al., 2020] ngay cả khi được tinh chỉnh trên tác vụ này. Dziri et al. [2023] tiếp tục cho thấy rằng GPT-4 [OpenAI, 2023] gặp khó khăn trong việc đạt được độ chính xác trong phân phối cao trên phép nhân, ngay cả với một scratchpad. Tuy nhiên, Lee et al. [2023] nhận thấy rằng với một scratchpad chi tiết, các transformer nhỏ có thể thực hiện phép nhân trong phân phối.

Phép tính là một tập con của lớp lớn hơn các vấn đề lý luận thuật toán tập trung vào khả năng học và thực thi các thuật toán và tổng quát hóa cho các vấn đề dài hơn [Anil et al., 2022b, Jelassi et al., 2023, Yang et al., 2023b, Veli ˇckovi ´c et al., 2022, Rodionov and Prokhorenkova, 2024, Testolin, 2024]. Lĩnh vực lý luận thuật toán tổng quát hơn bao gồm công trình về các kiến trúc và modalites dữ liệu khác nhau nhằm học các thuật toán từ dữ liệu. Veli ˇckovi ´c et al. [2022] và Rodionov and Prokhorenkova [2024], ví dụ, huấn luyện mạng neural để thực thi các tác vụ thuật toán cụ thể bằng cách huấn luyện trên các cặp input-output cũng như các bước trung gian và gợi ý. Theo hướng tương tự và mặc dù ban đầu được đánh giá cao về hiệu quả, weight sharing và recurrence có thể được sử dụng để làm cho các mô hình thích ứng và giúp tổng quát hóa cho các vấn đề khó hơn [Dehghani et al., 2018, Sukhbaatar et al., 2019, Lan et al., 2020, Ibarz et al., 2022]. Schwarzschild et al. [2021] và Bansal et al. [2022] khám phá một cách tiếp cận học end-to-end sử dụng mạng neural tích chập hồi quy để học các thuật toán từ các cặp input-output, giải quyết các tác vụ thuật toán như prefix sums, mazes và chess. Weight sharing cho lý luận thuật toán cũng hữu ích với transformers và chúng tôi sử dụng looped transformer trong một số thí nghiệm của chúng tôi dưới đây. Một looped transformer có một transformer block được gọi hồi quy trên output của chính nó, phù hợp để thực thi các thuật toán lặp [Giannou et al., 2023, Yang et al., 2023a, de Luca and Fountoulakis, 2024]. Ngoài ra, công trình gần đây nhằm cải thiện lý luận trong LLMs [Zhou et al., 2023], nhưng McLeish et al. [2024] chứng minh rằng LLMs, ngay cả với trình thông dịch mã, cũng không hoàn hảo trong các tác vụ lý luận thuật toán, cho thấy nhu cầu thiết yếu về những tiến bộ trong phương pháp luận của chúng ta. Bài báo này thực hiện một bước hướng tới việc cải thiện khả năng phép tính và thuật toán của LLM mà không sử dụng công cụ.

Positional Embeddings. Việc chỉ ra vị trí của các token trong một chuỗi cho các mô hình transformer là rất quan trọng đối với modeling ngôn ngữ [Vaswani et al., 2017]. Absolute positional embeddings (APE) là các embeddings được học được thêm vào token embeddings trước lớp đầu tiên của transformer [Vaswani et al., 2017]. Tuy nhiên, những embeddings tuyệt đối này cản trở khả năng tổng quát hóa độ dài [Press et al., 2022]. Để giải quyết vấn đề này, Shaw et al. [2018] đề xuất relative embeddings (RPE) được nhúng trong quá trình tính toán attention, một cơ chế được đơn giản hóa thêm bởi Raffel et al. [2020]. Những người khác xây dựng dựa trên những công trình này để cải thiện khả năng tổng quát hóa độ dài bao gồm Sandwich [Chi et al., 2023], Kerple [Chi et al., 2022], và Alibi [Press et al., 2022] positional embeddings. Ngoài ra, Kazemnejad et al. [2023] cho thấy rằng các lớp decoder vẫn có thể học thông tin vị trí mà không có embeddings vị trí rõ ràng. No positional embeddings (NoPE) có thể đạt được hiệu suất tổng quát hóa độ dài tốt cho các tác vụ thuật toán nhỏ và thậm chí vượt trội hơn một số embeddings chuyên biệt. Rotary Positional Embeddings (RoPE) [Su et al., 2024] thường được sử dụng trong các transformer open source tiên tiến [e.g. Touvron et al., 2023]. Tuy nhiên, RoPE hạn chế khả năng tổng quát hóa độ dài vì các mô hình chỉ được huấn luyện sử dụng các rotations dựa trên độ dài dữ liệu huấn luyện [Kazemnejad et al., 2023, Press et al., 2022]. Để cải thiện khả năng tổng quát hóa độ dài, có thể thêm các tiện ích mở rộng hậu huấn luyện [Peng et al., 2024]. Mới nhất và hữu ích nhất cho phép tính là Functional Interpolation for Relative Position Embeddings (FIRE) [Li et al., 2023]. FIRE cho thấy khả năng tổng quát hóa độ dài mạnh nhất cho đến nay, dẫn đến khả năng tổng quát hóa độ dài 2.5× trên phép cộng [Zhou et al., 2024] khi kết hợp với randomized embeddings [Ruoss et al., 2023]. Chúng tôi đi vào chi tiết hơn về một số positional embeddings này trong Phụ lục A.1.1. Trong công trình này, chúng tôi tập trung vào NoPE và FIRE embeddings vì đây là những người biểu diễn tốt nhất cho phép cộng trong định dạng đảo ngược trong số các embeddings hiện có [Zhou et al., 2024].

3 Đạt được Tổng quát hóa Độ dài cho Phép cộng
Chúng tôi nghiên cứu một loạt các phương pháp để cải thiện khả năng phép tính của các mô hình ngôn ngữ được huấn luyện từ đầu, tập trung vào hai giả thuyết chính: (1) thông tin vị trí cho các chữ số riêng lẻ trong số đang bị mất và (2) recurrence có thể cải thiện khả năng lý luận của kiến trúc transformer trên các vấn đề lý luận phép tính nhiều bước. Chúng tôi thảo luận ngắn gọn về thiết lập huấn luyện và đánh giá trước khi mô tả chi tiết từng cải tiến của chúng tôi.

--- TRANG 3 ---
Hình 2: Hình ảnh hóa các định dạng dữ liệu và positional embeddings. Abacus Embeddings đưa ra các positional embeddings giống nhau cho tất cả các chữ số có cùng tầm quan trọng.

Thiết lập Thí nghiệm. Chúng tôi huấn luyện các mô hình ngôn ngữ nhân quả chỉ decoder để giải quyết các bài toán cộng. Theo công trình trước đó [Zhou et al., 2023, 2024, Shen et al., 2023, Kazemnejad et al., 2023, Lee et al., 2023], inputs được định dạng chữ số ít quan trọng nhất trước tiên, e.g. 98282 + 3859172 = 2787472. Không giống như công trình trước đó, chúng tôi không thêm bất kỳ padding nào giữa các chữ số [Shen et al., 2023] và không pad bất kỳ số nào với zeros, không trong trường hợp carry digits [Zhou et al., 2024], cũng không để làm cho tất cả toán hạng có cùng độ dài [Shen et al., 2023]. Chúng tôi huấn luyện trên tất cả các kết hợp độ dài toán hạng nhỏ hơn hoặc bằng i và j, trong đó i và j là độ dài tối đa của toán hạng thứ nhất và thứ hai, tương ứng. Cho nghiên cứu này, tất cả các tập huấn luyện có 20 triệu mẫu và i=j, do đó chúng ta có thể sử dụng một số để định nghĩa tập dữ liệu i, trong đó i là độ dài tối đa của cả hai toán hạng. Chúng tôi lấy mẫu dữ liệu có thay thế và chúng tôi phân tầng dữ liệu, để tất cả các cặp độ dài (i, j) được lấy mẫu bằng nhau trong quá trình huấn luyện. Để tạo điều kiện huấn luyện nhiều mô hình từ đầu, chúng tôi sử dụng thiết lập language model cramming [Geiping and Goldstein, 2023] và giới hạn mỗi lần chạy huấn luyện ở 8 exaFLOP tính toán (một GPU Nvidia RTX A4000 duy nhất trong 24 giờ); cho kết quả nhân, chúng tôi cho phép 64 exaFLOP (tám GPU Nvidia RTX A4000 trong 24 giờ). Trong quá trình huấn luyện, chúng tôi mask câu hỏi input và chỉ tính toán loss trên các chữ số đáp án. Để biết thêm chi tiết về xây dựng dữ liệu và huấn luyện, chúng tôi tham khảo Phụ lục A.2.

Chúng tôi báo cáo độ chính xác của mô hình cho mỗi cặp độ dài (i, j) và không giống như hầu hết công trình hiện có, chúng tôi cũng bao gồm độ chính xác cho các cặp trong đó i≠j để làm nổi bật tất cả các trường hợp ngoại suy. Việc lập bảng rộng rãi này tốn kém và làm cho inference trở thành gánh nặng tính toán chính của nghiên cứu này. Vì pipeline huấn luyện của chúng tôi tạo ra kết quả khá nhất quán, chúng tôi báo cáo trung bình qua ba lần chạy (thay vì sử dụng sơ đồ báo cáo best-of-ten [Zhou et al., 2024]). Chúng tôi đo độ chính xác theo nghĩa nghiêm ngặt, nơi chỉ các khớp chính xác của tất cả các chữ số output được tính là đúng, i.e. nếu một chữ số duy nhất không chính xác thì ví dụ sẽ được đánh dấu là sai và chúng tôi gọi đây là exact match accuracy. Chúng tôi có ba loại đánh giá sau: (i) in distribution (ID) nơi các mô hình được kiểm tra trên các vấn đề lên đến kích thước tối đa được thấy trong quá trình huấn luyện; (ii) out of distribution (OOD) nơi các mô hình được kiểm tra trên các vấn đề lớn hơn kích thước tối đa được thấy trong quá trình huấn luyện nhưng cả hai toán hạng đều tối đa 100 chữ số; (iii) và extreme out of distribution (100+ digit OOD) nơi các mô hình được kiểm tra trên các vấn đề trong đó cả hai toán hạng có cùng độ dài và cả hai đều hơn 100 chữ số và ít hơn 160 chữ số. Trong setting 100+ OOD, chúng tôi chỉ phân tích các vấn đề trong đó các toán hạng có cùng độ dài (i=j) do chi phí inference ở quy mô này.

Chúng tôi xem xét hai kiến trúc transformer tiêu chuẩn. Đầu tiên, chúng tôi sử dụng một mô hình transformer autoregressive tiêu chuẩn, nơi nhiều lớp decoder được xếp chồng theo cách feedforward. Thứ hai, chúng tôi tăng cường mô hình transformer tiêu chuẩn này bằng cách kết hợp input injection, nơi các inputs được nhúng được thêm vào input của mỗi lớp decoder [Ma et al., 2022, Bansal et al., 2022, Anil et al., 2022a]. Chúng tôi mô tả trực quan các kiến trúc trong Hình 22 của Phụ lục.

3.1 Abacus Embeddings Giúp Căn chỉnh Chữ số
Từ công trình trước đó và các thí nghiệm ban đầu của chúng tôi, chúng tôi quan sát thấy rằng ngay cả khi các số input được trình bày chữ số ít quan trọng nhất trước tiên và dữ liệu huấn luyện được phân tầng và dồi dào (vài triệu ví dụ), các transformer tiêu chuẩn vẫn gặp khó khăn trong việc học phép cộng nhiều chữ số. Chúng tôi cũng quan sát thấy rằng con người thực hiện phép cộng dài bằng cách đầu tiên căn chỉnh các chữ số có cùng tầm quan trọng thành các cột. Do đó, giả thuyết đầu tiên của chúng tôi là tầm quan trọng của mỗi chữ số (i.e. vị trí của mỗi chữ số so với đầu của số) không dễ dàng cho transformers biểu diễn, và rằng bài toán phụ này tạo ra nhiều rào cản hơn so với phép cộng thực tế.

Công trình trước đó giải quyết điều này bằng cách đề xuất các gợi ý chỉ mục rõ ràng trong inputs và outputs của phép cộng, ví dụ a6b7c5 +a1b6c3 =a7b3c9, nhận thấy rằng transformers hoạt động tốt hơn nhiều trên phép cộng với thông tin được cung cấp bởi những gợi ý như vậy [Zhou et al., 2023, 2024]. Tuy nhiên, gợi ý chỉ mục dạng này tăng độ dài context input cần thiết và tăng gấp đôi độ dài output và chi phí inference để giải quyết một bài toán cộng cho trước. Hơn nữa, Zhou et al. [2024] nhận thấy rằng khả năng tổng quát hóa của các mô hình được huấn luyện với gợi ý chỉ mục nhạy cảm với khởi tạo ngẫu nhiên cụ thể. Zhou et al. [2024] làm nổi bật điều này bằng cách huấn luyện các mô hình với các random seeds khác nhau, thay đổi khởi tạo trọng số và seeds thứ tự input dữ liệu, cho thấy sự biến đổi trong hiệu suất của những mô hình này có thể dao động từ gần hoàn hảo trên phép cộng 100 chữ số đến 0% độ chính xác ở phép cộng 90 chữ số.

Để giải quyết những hạn chế của transformers trong việc biểu diễn thông tin vị trí, chúng tôi thiết kế một positional embedding được xây dựng đặc biệt để mã hóa vị trí của mỗi chữ số so với đầu của số hiện tại. Chúng tôi gọi đây là Abacus Embeddings. Chúng tôi áp dụng cùng một positional embedding cho tất cả các chữ số có cùng tầm quan trọng, cung cấp một tín hiệu rõ ràng mà mô hình có thể sử dụng để căn chỉnh các chữ số. Chúng tôi mô tả trực quan những embeddings này trong Hình 2.

Chúng tôi lấy cảm hứng từ Randomized Embeddings [Ruoss et al., 2023] nhưng thay vì sử dụng các chỉ mục tăng dần ngẫu nhiên để biểu diễn vị trí trong một mẫu, chúng tôi sử dụng các chỉ mục tăng dần liên tiếp với vị trí bắt đầu ngẫu nhiên để cho phép tổng quát hóa độ dài. Cụ thể, trong quá trình huấn luyện, chúng tôi đưa ra các positional embeddings liên tiếp cho mỗi chữ số trong một số, bắt đầu từ một giá trị offset được chọn ngẫu nhiên từ U[1, k], trong đó k là một siêu tham số. Trừ khi được nêu khác, giá trị mặc định cho k trong nghiên cứu này là 100 và cho thấy điều này có thể được thay đổi trong Phụ lục A.5. Ví dụ, nếu input là 123, các encodings vị trí là β, β+ 1, β+ 2, trong đó β∼U[1,100], sau đó được truyền qua một ma trận embedding được học. Giá trị được lấy mẫu từ U[1, k] giống nhau cho tất cả các số trong một batch, có nghĩa là tất cả các chữ số có cùng tầm quan trọng đều có cùng positional embedding. Sơ đồ huấn luyện này cho phép mô hình thấy một loạt rộng các positional embeddings, ngay cả khi các chuỗi huấn luyện ngắn. Tại thời điểm kiểm tra, mỗi positional embedding bắt đầu từ một, i.e. β= 1.

Abacus Embeddings Giải quyết Phép cộng. Abacus Embeddings cải thiện hiệu suất tổng quát hóa lên đến 100 chữ số và hơn nữa cho các kiến trúc transformer tiêu chuẩn. Trong Hình 3 (trái), chúng tôi làm nổi bật sự cải thiện so sánh mà Abacus Embeddings có đối với các kiến trúc và embeddings transformer tiêu chuẩn để thực hiện phép cộng, lấy độ chính xác trung bình của ba mô hình trong tất cả các trường hợp. Kết quả độ chính xác cho các mô hình transformer tiêu chuẩn được huấn luyện với FIRE và Abacus, được kiểm tra cả trong miền (ID) và ngoài miền (OOD), cũng được hiển thị trong Hình 1. Ngoài ra, trong Phụ lục A.6, chúng tôi trình bày các biểu đồ lưới 2D tương tự cho một số thí nghiệm khác được mô tả dưới dạng biểu đồ thanh trong văn bản chính. Zhou et al. [2024] nhận thấy rằng độ dài toán hạng lên đến bốn mười chữ số được yêu cầu trong quá trình huấn luyện để có khả năng tổng quát hóa tốt cho phép cộng 100 chữ số trong quá trình kiểm tra (mặc dù không mạnh mẽ). Chúng tôi nhận thấy rằng với Abacus Embeddings của chúng tôi, chúng ta có thể đạt được độ chính xác tương tự và khả năng ngoại suy lớn hơn bằng cách sử dụng một mô hình tiêu chuẩn với input injection được huấn luyện trên kích thước toán hạng tối đa 20 chữ số.

Vì Abacus Embeddings là một biến thể của absolute positional embeddings, về mặt kỹ thuật chúng không thể tổng quát hóa vượt ra ngoài các vị trí tương đối được thấy trong quá trình huấn luyện. Tuy nhiên, siêu tham số k ngẫu nhiên hóa offset bắt đầu được sử dụng cho mỗi ví dụ cộng riêng lẻ có thể được tăng lên để cho phép tổng quát hóa bằng cách huấn luyện một loạt lớn hơn các embeddings cho một ngân sách tính toán cho trước. Liên quan đến điều này, Hình 9 trong Phụ lục cho thấy rằng việc huấn luyện trên các tập dữ liệu lớn hơn cải thiện hiệu suất, ngay cả đối với các toán hạng có ít hơn 100 chữ số.

3.2 Recurrence Trong Transformers Tăng cường Hiệu suất
Với positional embeddings được giải quyết, tiếp theo chúng tôi khám phá liệu các kiến trúc hồi quy có thể cải thiện thêm khả năng của transformers để thực hiện phép cộng nhiều chữ số. Chúng tôi sử dụng thuật ngữ recurrent block để chỉ một tập hợp các lớp decoder với trọng số riêng biệt và recurrences để chỉ số lần khối hồi quy được lặp lại. Chúng tôi sử dụng thuật ngữ effective depth để có nghĩa là số lượng lớp được sử dụng trong một transformer, cho dù trọng số của chúng có độc nhất hay không. Trừ khi được nêu khác, chúng tôi sử dụng kiến trúc hồi quy tối đa, i.e. chỉ một lớp độc nhất được lặp lại để đạt được effective depth.

Chúng tôi cũng sử dụng input injection, skip-connections truyền một bản sao của input tới mỗi lớp trong mạng.

Lợi ích của Recurrence. Trong Hình 3 (phải), chúng tôi so sánh tất cả các biến thể kiến trúc sử dụng cả FIRE và NoPE embeddings được huấn luyện trên phép cộng trên các toán hạng với tối đa 40 chữ số. Mặc dù có khoảng 10× ít tham số hơn so với các mô hình khác, chúng ta thấy rằng looped transformer (hồi quy, với input injection và progressive loss), đạt được hiệu suất out of distribution tốt nhất sử dụng một trong hai position embedding. Trong Hình 9 trong Phụ lục, chúng tôi cho thấy kết quả này mạnh mẽ trên nhiều kích thước dữ liệu huấn luyện.

Với các mô hình hồi quy, chúng ta có thể chọn thay đổi số lượng recurrences cho mỗi forward pass trong quá trình huấn luyện. Điều này có xu hướng cải thiện khả năng tổng quát hóa cho các tác vụ khó hơn tại thời điểm kiểm tra và cũng được gọi là progressive loss computation [Bansal et al., 2022]. Hàm loss này là một kết hợp lồi của các giá trị loss từ hai forward passes, một với số lượng recurrences danh nghĩa (vậy 16 cho một mô hình 1×16) và một với số lượng recurrences ngẫu nhiên nhỏ hơn.

Tiếp theo, chúng tôi khám phá hiệu ứng của việc thay đổi kích thước của khối hồi quy trong khi giữ effective depth cố định. Chúng tôi thực hiện ablation này bằng cách giảm một nửa số lượng lớp trong khối hồi quy và tăng gấp đôi số lượng recurrences, quét từ một mô hình với mười sáu lớp trong khối và một recurrence duy nhất (16×1, i.e. một transformer tiêu chuẩn), đến một lớp trong khối nhưng với mười sáu recurrences (1×16). Phân tích những kết quả này trong Hình 4, chúng tôi cho thấy những cải thiện hiệu suất tiếp theo là có thể trong một số trường hợp với sự kết hợp của cả recurrence và Abacus Embeddings. Đặc biệt, một mô hình với hai recurrences (8×2) gây ra một nửa lỗi của mô hình hoàn toàn không hồi quy (16×1) cho các vấn đề OOD và có độ chính xác tăng lên trên các vấn đề 100+ OOD.

Cuối cùng, trong Phụ lục A.7.3, chúng tôi thay đổi effective depth của các mô hình để phân tích tác động của số lượng tham số đối với tác vụ này, qua Abacus, FIRE và NoPE embeddings. Mặc dù các thí nghiệm được trình bày trong Hình 4 là một so sánh công bằng qua độ sâu, các mô hình transformer hoàn toàn tiêu chuẩn có nhiều tham số hơn so với các đối tác hồi quy của chúng. Trong Bảng 3 trong phụ lục, chúng tôi ghi lại số lượng tham số đến triệu gần nhất.

4 Đẩy Giới hạn của Lý luận Thuật toán cho Transformers
Trong khi có sự nhấn mạnh vào phép cộng như một vấn đề khó trong công trình hiện có, hiệu suất mạnh của phương pháp chúng tôi cho phép chúng tôi mở rộng sang những vấn đề thậm chí khó hơn, bao gồm nhân và sắp xếp và thậm chí nhiều phép toán cùng một lúc.

4.1 Phép cộng và Phép trừ
Chúng tôi huấn luyện các mô hình trên một tập dữ liệu được tạo thành từ hỗn hợp đều các mẫu cộng và trừ. Trong Hình 5, chúng tôi cho thấy kết quả từ các mô hình với 8 lớp trong khối hồi quy và 2 recurrences được huấn luyện với chính xác các siêu tham số giống nhau được sử dụng để huấn luyện các mô hình cộng ở trên. Chúng ta thấy rằng những mô hình transformer nhỏ này có thể đồng thời học cách ngoại suy cho cả phép toán đối xứng của phép cộng và phép toán phản đối xứng của phép trừ bằng cách sử dụng Abacus Embeddings.

--- TRANG 4 ---
ST ST w/ II
Architecture Type020406080100Exact Match Accuracy
92.9
3.6
4.3
97.9
2.9
3.226.7
0
0
30.6
0
0
Abacus, OOD
Abacus, 100+ OODFIRE, OOD
FIRE, 100+ OODNoPE, OOD
NoPE, 100+ OOD
LT ST ST w/ II
Architecture Type051015202530Exact Match Accuracy
24.3
23.9
15.3
11.4
8.7
11.0
FIRE, OOD NoPE, OOD

Hình 3: Trái: Độ chính xác khớp chính xác trung bình của ba mô hình độ sâu mười sáu trên dữ liệu kích thước 20, thay đổi kiến trúc và embeddings. Abacus Embeddings cải thiện độ chính xác cho phép cộng so với FIRE và NoPE Embeddings. Phải: Độ chính xác khớp chính xác trung bình của ba mô hình với effective depth mười sáu trên dữ liệu kích thước 40, thay đổi qua NoPE hoặc FIRE embeddings và kiến trúc. Các mô hình looped transformer hồi quy cải thiện độ chính xác cho phép cộng cho cả FIRE và NoPE embeddings.
Looped transformer (LT): Các lớp decoder chia sẻ trọng số, với input injection và progressive loss.
Standard Transformer (ST): Các lớp chỉ decoder được xếp chồng. Standard Transformer với Input Injection (ST w/ II): Standard Transformer với các tính năng input được thêm vào biểu diễn ẩn giữa mỗi lớp decoder.

16x1 8x2 4x4 2x8 1x16
Layers in Recurrent Block X Number of Recurrences020406080100Exact Match Accuracy
97.9
99.1
98.8
97.9
79.830.6
31.3
30.1
29.1
13.7
Abacus, OOD Abacus, 100+ OOD

Hình 4: Thay đổi kích thước của khối hồi quy, trong khi duy trì effective depth là 16 và huấn luyện trên dữ liệu kích thước 20. Chúng ta thấy rằng một mô hình hồi quy với tám lớp trong khối hồi quy và hai recurrences là chính xác nhất trong tất cả các mô hình effective depth 16, giảm một nửa tỷ lệ lỗi của một mô hình tiêu chuẩn với input injection trong đánh giá OOD. (Xem Hình 17 cho kết quả với FIRE và NoPE.)

4.2 Phép nhân Số nguyên
Bây giờ chúng tôi nghiên cứu một tác vụ khó hơn, phép nhân các số tự nhiên, nơi độ dài của output có thể là tổng độ dài của các toán hạng. So với phép cộng, nơi output nhiều nhất là một chữ số hơn toán hạng dài nhất, phép nhân có dependency khoảng cách xa hơn và độ dài output tăng nhanh hơn nhiều khi kích thước bài toán tăng.

Để thích nghi từ phép cộng sang phép nhân, chúng tôi thực hiện một số thay đổi nhỏ cho thiết lập của chúng tôi. Đầu tiên, chúng tôi loại bỏ input injection từ bên trong khối hồi quy và thứ hai, chúng tôi chia gradients trong khối hồi quy cho số lượng recurrences, giảm trọng số cập nhật gradient từ các batches với nhiều recurrences [Bansal et al., 2022]. (Chúng tôi phân tích tác động của những quyết định thiết kế này đối với các mô hình cộng trong Hình 19 của Phụ lục.) Chúng tôi chỉ kiểm tra looped transformers vì việc tính toán cần thiết cho huấn luyện và tìm kiếm siêu tham số cho phép nhân lớn hơn nhiều so với phép cộng, giới hạn chúng tôi trong phân tích quy mô nhỏ hơn nhiều.

Abacus Embeddings giúp looped transformers đạt được độ chính xác gần hoàn hảo trong phân phối cho phép nhân. Trong Hình 6, chúng tôi cho thấy cách phân phối huấn luyện, được bao quanh bởi hình vuông đỏ, bão hòa hoàn toàn với Abacus Embeddings. Trên thực tế, các mô hình với Abacus Embeddings của chúng tôi đạt được độ chính xác trong phân phối cao hơn trên phép nhân 15 chữ số so với công trình trước đó [Shen et al., 2023] và không yêu cầu padding mỗi toán hạng đến cùng độ dài với zeros. Đặc biệt, chúng tôi làm nổi bật rằng những vấn đề cụ thể mà các mô hình được huấn luyện với FIRE embeddings gặp khó khăn để giải quyết là những vấn đề khó nhất trong tập huấn luyện và Abacus Embeddings vượt trội hơn chúng trong khu vực quan trọng này (xem góc dưới bên phải của các hộp đỏ trong Hình 6).

4.3 Sắp xếp Mảng

Bảng 1: Độ chính xác khớp chính xác cho sắp xếp với các positional embeddings khác nhau. Tất cả kết quả đều là phần trăm của tập kiểm tra và tất cả các mô hình ở đây đều là standard transformers với tám lớp.

FIRE Abacus Abacus + FIRE
OOD (độ dài số - 30)55.32 68.63 67.28
OOD (độ dài mảng - 30) 21.35 9.67 21.11
Tất cả OOD (30×30) 3.73 2.65 4.48
Tất cả OOD (20×20) 14.65 9.78 16.91

Bảng 2: Độ chính xác cho sắp xếp với các kiến trúc khác nhau cho sắp xếp. ST biểu thị standard transformer, ST w/ II biểu thị standard transformer với input injection, và LT biểu thị các mô hình looped transformer. Standard transformer có độ chính xác khớp chính xác tốt nhất. Khi đo độ chính xác trong việc xác định chỉ phần tử tối thiểu của mảng, looped transformers vượt trội hơn tất cả những cái khác. Tất cả kết quả đều là phần trăm của tập kiểm tra.

ST ST w/ II LT
Tất cả OOD (khớp chuỗi chính xác) 4.48 3.84 2.60
Tất cả OOD (chỉ phần tử tối thiểu) 49.73 60.09 68.51

Trong khi cả phép cộng và phép nhân chỉ chấp nhận hai toán hạng, bây giờ chúng tôi phân tích tác vụ sắp xếp các mảng của nhiều số có độ dài biến đổi, một testbed thách thức hơn để đánh giá khả năng tổng quát hóa của Abacus Embeddings của chúng tôi. Chúng tôi trình bày mỗi bài toán sắp xếp sử dụng các chỉ mục chữ cái cho mỗi số (đảo ngược) trong một mảng input, nơi output mong đợi là các chỉ mục chữ cái theo thứ tự tăng dần. Ví dụ, a: 64957, b: 99963, c: 10218, d: 7141, e: 05781 = d, e, b, a, c. Chúng tôi huấn luyện với các mảng lên đến 10 số, mỗi số có lên đến 10 chữ số và sau đó đánh giá với các mảng lên đến 30 số, mỗi số có lên đến 30 chữ số. Chúng tôi đưa ra thêm chi tiết về quá trình xây dựng dữ liệu sắp xếp trong Phụ lục A.2.

Trong setting này, chúng tôi khám phá hai trục tổng quát hóa. Đầu tiên, chúng tôi tăng độ dài tối đa có thể của các số input lên 30 chữ số trong khi duy trì độ dài mảng tối đa là 10 và gọi kịch bản này là "OOD (độ dài số - 30)." Thứ hai, chúng tôi tăng số lượng inputs trong mảng cần được sắp xếp lên 30 trong khi giữ độ dài chữ số tối đa của mỗi số ở 10 và gọi kịch bản này là "OOD (độ dài mảng - 30)." Cuối cùng, chúng tôi xem xét một kịch bản trong đó cả hai trục được tăng đồng thời, được gọi là "tất cả OOD."

Trong Bảng 1, chúng tôi minh họa hiệu suất của một standard transformer (tám lớp) được huấn luyện với các embeddings khác nhau—FIRE, Abacus và sự kết hợp của chúng. Một lần nữa, kết quả của chúng tôi chứng minh rằng cách tiếp cận embedding kết hợp tăng cường khả năng tổng quát hóa của mô hình, vượt qua hiệu suất của một trong hai embedding riêng lẻ trong setting "tất cả OOD". Tuy nhiên, trong Bảng 2, chúng tôi quan sát kết quả hỗn hợp khi ghép nối sự kết hợp Abacus+FIRE Embeddings với các kiến trúc mô hình khác nhau có effective depth tám. Đối với sắp xếp, các kiến trúc khác nhau dường như phù hợp hơn với các loại ngoại suy khác nhau, ví dụ looped transformer tốt nhất trong việc ngoại suy để tìm phần tử tối thiểu nhưng không phải để sắp xếp toàn bộ mảng.

Nhìn chung, hiệu suất sắp xếp vượt trội của Abacus Embeddings nhấn mạnh tiềm năng hữu ích của chúng trên một phổ rộng hơn các tác vụ thuật toán ngoài phép tính cơ bản. Abacus Embeddings có thể đóng vai trò quan trọng trong các trường hợp sử dụng yêu cầu các mô hình transformer thực hiện nhiều tác vụ lý luận vị trí, số học và/hoặc quan hệ phức tạp.

4.4 Abacus và Relative Embeddings
Vì Abacus Embeddings chỉ được áp dụng cho số, để kết hợp Abacus Embeddings vào một mô hình mục đích chung, chúng phải tương thích với các relative embeddings khác để duy trì hiệu suất downstream tốt trên các tác vụ không phải phép tính. Chúng tôi kiểm tra những loại kết hợp này ở đây và kết luận rằng Abacus Embeddings bổ sung các kỹ thuật tốt cho ngôn ngữ tự nhiên, gợi ý rằng những kết hợp này có thể mạnh mẽ cho các mô hình chung quy mô lớn.

Mặc dù Abacus Embeddings được kết hợp ngầm với NoPE (no positional embeddings) embeddings cho tất cả các thí nghiệm được thấy cho đến nay, hầu hết các mô hình open source tiên tiến sử dụng Rotary Embeddings. Rotary Embeddings yếu cho khả năng tổng quát hóa độ dài. Chúng tôi cho thấy rằng việc kết hợp Abacus Embeddings với RoPE thực sự mang lại cải thiện trong khả năng tổng quát hóa độ dài toán hạng. Tuy nhiên, trong Hình 7, chúng tôi chứng minh tiềm năng thực sự để tích hợp Abacus Embeddings vào một hệ thống tổng quát hơn, cho thấy rằng sự kết hợp của Abacus Embeddings với FIRE mở ra khả năng tổng quát hóa vượt xa những vấn đề mà FIRE embeddings có thể giải quyết một mình.

0 50 1000
50
100ST w/ II
Abacus + FIRE
0 50 100ST w/ II
FIRE
0 50 100ST w/ II
Abacus + RoPE
0 50 100ST w/ II
RoPE
050100
Accuracy
Length of Operand TwoLength of Operand One

Hình 7: Độ chính xác khớp chính xác của standard transformer độ sâu 16 với input injection, được huấn luyện trên dữ liệu kích thước tối đa 20. Hình vuông đỏ biểu thị kiểm tra trong phân phối. Việc kết hợp Abacus Embeddings với FIRE hoặc RoPE embeddings cải thiện độ chính xác ngoài phân phối cho phép cộng, so với các mô hình baseline không có Abacus Embeddings.

5 Thảo luận & Hạn chế
Trong khi khả năng của LLMs đã tiến bộ đủ xa để bao gồm các tác vụ phức tạp bao gồm tạo mã và lý luận toán học, việc kiểm tra căng thẳng giới hạn của những mô hình này vẫn là một thách thức. Trong bài báo này, chúng tôi nghiên cứu các tác vụ lý luận toán học bao gồm cộng, nhân và sắp xếp để đánh giá những khả năng này trong một setting được kiểm soát. Chúng tôi phân tích khả năng của các mô hình ngôn ngữ chuyên biệt để học các tác vụ thuật toán trong setting zero shot, mà không có quyền truy cập vào các công cụ bên ngoài như trình thông dịch mã, v.v., khám phá lợi ích của các cải tiến kiến trúc khác nhau như embeddings được cải thiện và các lớp hồi quy.

Qua các thí nghiệm của chúng tôi, chúng tôi nhận thấy rằng Abacus Embeddings mới của chúng tôi cải thiện hiệu suất đáng kể cả khi được áp dụng cho transformers tiêu chuẩn cũng như các biến thể hồi quy. Chúng tôi liên tục đạt được khả năng tổng quát hóa độ dài ít nhất 6× (bị giới hạn bởi độ dài context) gấp hơn đôi các chứng minh ngoại suy trong công trình trước đó, đạt được kết quả gần hoàn hảo trên phép cộng lên đến 100 chữ số, với kết quả có thể lặp lại qua nhiều lần chạy huấn luyện. Chúng tôi chứng minh các tính chất bổ sung của Abacus Embeddings với các relative embeddings khác như FIRE, đạt được những cải thiện đáng kể trong hiệu suất nhân trong phân phối, và tạo ra tiến bộ trong vấn đề thách thức của sắp xếp mảng độ dài biến đổi.

Trái ngược với công trình trước đó, các thí nghiệm của chúng tôi khám phá các loại ngoại suy vượt ra ngoài chỉ khả năng tổng quát hóa độ dài cho phép cộng, trình bày một sửa đổi kiến trúc cải thiện hiệu suất trên nhiều tác vụ lý luận thuật toán đồng thời. Chúng tôi hy vọng rằng công trình của chúng tôi làm sâu sắc thêm hiểu biết của cộng đồng về những vấn đề này và mở đường cho những tiến bộ tiếp theo trong khả năng lý luận thuật toán của các mô hình ngôn ngữ lớn.

Hạn chế Có một số hạn chế nội tại đi kèm với bất kỳ nghiên cứu nào liên quan đến việc huấn luyện mô hình ngôn ngữ từ đầu dưới các ràng buộc tính toán. Tuy nhiên, điểm liên quan chính cho nghiên cứu này là mặc dù chúng tôi cho thấy tính tương thích của Abacus Embeddings với FIRE và RoPE embeddings, chúng tôi thực sự không khám phá bất kỳ tác vụ ngôn ngữ tự nhiên nào. Trong tương lai, một nghiên cứu quy mô lớn hơn bao gồm ngôn ngữ tự nhiên sẽ cần thiết để hiểu thêm về cách Abacus Embeddings sẽ hoạt động trên các tác vụ không đồng nhất bao gồm cả inputs số và ngôn ngữ tự nhiên.

Lời cảm ơn và Tiết lộ Tài trợ
Công trình này được thực hiện nhờ chương trình ONR MURI và chương trình AFOSR MURI. Hỗ trợ thương mại được cung cấp bởi Capital One Bank, chương trình Amazon Research Award và Open Philanthropy. Hỗ trợ tiếp theo được cung cấp bởi National Science Foundation (IIS-2212182), và bởi NSF TRAILS Institute (2229885). Tài nguyên tính toán được cung cấp bởi chương trình Department of Energy INCITE Allocation Program, và Lawrence Livermore National Labs.

Hơn nữa, công trình này được thực hiện dưới sự bảo trợ của Bộ Năng lượng Hoa Kỳ bởi Lawrence Livermore National Laboratory theo Hợp đồng DE-AC52-07NA27344 và được hỗ trợ bởi Chương trình LLNL-LDRD theo Dự án Số 24-ERD-010 (LLNL-CONF-2000175).

Tài liệu tham khảo
Cem Anil, Ashwini Pokle, Kaiqu Liang, Johannes Treutlein, Yuhuai Wu, Shaojie Bai, J Zico Kolter, and Roger B Grosse. Path independent equilibrium models can better exploit test-time computation. Advances in Neural Information Processing Systems, 35:7796–7809, 2022a.

Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. Advances in Neural Information Processing Systems, 35:38546–38556, 2022b.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein. End-to-end algorithm synthesis with recurrent networks: Logical extrapolation without overthinking. Advances in Neural Information Processing Systems, 35, 2022.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, and Alexander Rudnicky. Kerple: Kernelized relative positional embedding for length extrapolation. In Advances in Neural Information Processing Systems, 2022.

Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13522–13537, 2023.

Artur Back de Luca and Kimon Fountoulakis. Simulation of graph algorithms with looped transformers.arXiv preprint arXiv:2402.01107, 2024.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. In International Conference on Learning Representations, 2018.

Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D Hwang, et al. Faith and fate: Limits of transformers on compositionality. arXiv preprint arXiv:2305.18654, 2023.

Jonas Geiping and Tom Goldstein. Cramming: Training a language model on a single gpu in one day. In International Conference on Machine Learning, pages 11117–11143. PMLR, 2023.

Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In International Conference on Machine Learning, pages 11398–11442. PMLR, 2023.

Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, et al. xval: A continuous number encoding for large language models. arXiv preprint arXiv:2310.02989, 2023.

Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyriacos Nikiforou, Mehdi Bennani, Róbert Csordás, Andrew Joseph Dudzik, Matko Bošnjak, Alex Vitvitskyi, Yulia Rubanova, et al. A generalist neural algorithmic learner. In Learning on graphs conference, pages 2–1. PMLR, 2022.

Samy Jelassi, Stéphane d'Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and François Charton. Length generalization in arithmetic transformers. arXiv preprint arXiv:2306.15400, 2023.

Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. arXiv preprint arXiv:2305.19466, 2023.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=H1eA7AEtvS.

Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023.

Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions improves long context transformers. arXiv preprint arXiv:2310.04418, 2023.

John Loeber. #16: Notes on Arithmetic in GPT-4, February 2024. URL https://loeber.substack.com/p/16-notes-on-arithmetic-in-gpt-4.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.

Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: moving average equipped gated attention. arXiv preprint arXiv:2209.10655, 2022.

Sean McLeish, Avi Schwarzschild, and Tom Goldstein. Benchmarking chatgpt on algorithmic reasoning. arXiv preprint arXiv:2404.03441, 2024.

Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.

OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api.semanticscholar.org/CorpusID:257532815.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. International Conference on Learning Representations, 2024.

Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0.

Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. Limitations of language models in arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051, 2022.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020.

Gleb Rodionov and Liudmila Prokhorenkova. Discrete neural algorithmic reasoning. arXiv preprint arXiv:2402.11628, 2024.

Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. arXiv preprint arXiv:2305.16843, 2023.

David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.

Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Advances in Neural Information Processing Systems, 34, 2021.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018.

Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.

Ruoqi Shen, Sébastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, and Yi Zhang. Positional description matters for transformers arithmetic. arXiv preprint arXiv:2311.14737, 2023.

Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.

Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331–335, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1032. URL https://aclanthology.org/P19-1032.

Alberto Testolin. Can neural networks do arithmetic? a survey on the elementary numerical skills of state-of-the-art deep learning models. Applied Sciences, 14(2), 2024. ISSN 2076-3417. doi: 10.3390/app14020744. URL https://www.mdpi.com/2076-3417/14/2/744.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Petar Veli ˇckovi ´c, Adrià Puigdomènech Badia, David Budden, Razvan Pascanu, Andrea Banino, Misha Dashevskiy, Raia Hadsell, and Charles Blundell. The clrs algorithmic reasoning benchmark. In International Conference on Machine Learning, pages 22084–22102. PMLR, 2022.

Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. DeepNet: Scaling Transformers to 1,000 Layers. arXiv:2203.00555 [cs], March 2022. URL http://arxiv.org/abs/2203.00555.

Liu Yang, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos. Looped transformers are better at learning learning algorithms. arXiv preprint arXiv:2311.12424, 2023a.

Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, and Jie Tang. Gpt can solve mathematical problems without a calculator. arXiv preprint arXiv:2309.03241, 2023b.

Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12104–12113, 2022.

Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? a study in length generalization. arXiv preprint arXiv:2310.16028, 2023.

Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou. Transformers can achieve length generalization but not robustly. arXiv preprint arXiv:2402.09371, 2024.

A Phụ lục
Đóng góp của Tác giả
Sean McLeish* – Dẫn dắt dự án, phát triển ý tưởng, đóng góp vào mã, tổ chức phần lớn thí nghiệm và đóng góp vào việc viết.
Arpit Bansal* – Đóng góp lượng lớn vào ý tưởng, đóng góp vào mã, tổ chức thí nghiệm cho sắp xếp mảng, và đóng góp vào việc viết.
Alex Stein – Đóng góp vào mã, và giúp lên kế hoạch thí nghiệm.
Neel Jain – Đóng góp lượng lớn vào việc viết, và giúp lên kế hoạch thí nghiệm.
John Kirchenbauer – Đóng góp lượng lớn vào việc viết.
Brian R. Bartoldson, Bhavya Kailkhura – Giúp thiết lập đánh giá phép cộng song song quy mô lớn, đóng góp vào việc viết.
Abhinav Bhatele – Đóng góp vào việc viết.
Jonas Geiping, Avi Schwarzschild, Tom Goldstein – Phát triển ý tưởng, giúp lên kế hoạch và tổ chức thí nghiệm, và đóng góp lượng lớn vào việc viết.

A.1 Công trình Liên quan Mở rộng
A.1.1 Positional Embeddings.
FIRE embeddings là additive embeddings trong cơ chế attention: ARPE(X) = XWQ(XWK)T+B, trong đó Bi,j=fθ(log(c(i−j)+1)/log(cmax(i,L)+1)) và c, L là các tham số có thể học. Li et al. [2023] cho thấy thực nghiệm rằng những embeddings này cho phép khả năng tổng quát hóa độ dài và lý thuyết cho thấy chúng có khả năng biểu diễn nhiều loại embedding khác. Ruoss et al. [2023] đề xuất sử dụng một tập con ngẫu nhiên của một tập lớn hơn các vị trí có thể trong quá trình huấn luyện để các positional embeddings lớn hơn được huấn luyện. Zhou et al. [2024] sử dụng randomized FIRE [Ruoss et al., 2023, Li et al., 2023] embeddings để đạt được khả năng tổng quát hóa độ dài trên các tác vụ phép tính, sử dụng các vị trí ngẫu nhiên làm input cho multilayer perceptron nhỏ được sử dụng trong FIRE embeddings.

A.2 Tập dữ liệu
Phép cộng: Chúng tôi lấy mẫu đều, có thay thế, từ tất cả i×i độ dài toán hạng có thể lên đến kích thước tập dữ liệu tối đa 20 triệu, chúng tôi gọi đây là tập dữ liệu kích thước i trong văn bản chính. Để đánh giá, chúng tôi lấy mẫu 100 mẫu cho mỗi cặp độ dài toán hạng được đánh giá.

Bitwise OR: Input cho vấn đề này là hai vectơ nhị phân, vectơ input dài hơn là tất cả zeros và vectơ ngắn hơn chứa một số một. Output phải là độ dài của vectơ dài hơn với số một ở cùng vị trí như trong vectơ ngắn hơn. Nếu inputs có cùng độ dài, số một có thể ở trong một trong hai vectơ. Ví dụ 001⊕00000 = 00100. Để huấn luyện, chúng tôi lấy mẫu đầy đủ không gian của tất cả các vectơ có kích thước nhỏ hơn hoặc bằng kích thước vectơ input tối đa được định trước.

Sắp xếp: Cho một danh sách các số nguyên đảo ngược được chỉ mục bằng ký tự, output các ký tự theo thứ tự tăng dần. Ví dụ a: 64957, b: 99963, c: 10218, d: 7141, e: 05781 = d, e, b, a, c. Chúng tôi triển khai quá trình lấy mẫu cho sắp xếp theo cách giống như lưới. Chúng tôi truy vấn mỗi "ô vuông" của một lưới [1, n]×[1, n] cho đến khi kích thước tối đa đã được đạt tới cho tập dữ liệu. Khi truy vấn "ô vuông" (i, j), chúng tôi lấy mẫu ngẫu nhiên i số nguyên có kích thước nhỏ hơn hoặc bằng j chữ số. Chúng tôi lấy mẫu ngẫu nhiên các chỉ mục liên tiếp cho các số tự nhiên trong danh sách của chúng tôi cả trong thời gian huấn luyện và kiểm tra.

Phép nhân: Chúng tôi triển khai các tập dữ liệu nhân cho cả huấn luyện và kiểm tra theo cách chính xác giống như phép cộng, chỉ thay đổi phép toán được sử dụng để tính toán đáp án.

A.3 Bitwise OR trên Vectơ Nhị phân
Một điều kiện cần thiết để thực hiện phép cộng là căn chỉnh các chữ số có cùng tầm quan trọng. Chúng tôi bắt đầu bằng cách kiểm tra positional embeddings cho chính xác tác vụ này. Để làm điều này, chúng tôi phân tích tác vụ bitwise OR, nơi mô hình phải output OR theo vị trí căn chỉnh trái của hai vectơ nhị phân. Chúng tôi trình bày các mẫu từ tập dữ liệu trong Phần A.3.1, những cái này được căn chỉnh trái để đại diện cho tác vụ căn chỉnh các chữ số cho phép cộng đảo ngược.

LT ST ST w/ II
Architecture Type020406080100Exact Match Accuracy
71.8
7.1
10.5
83.1
3.7
6.8
55.6
6.4
4.7
Abacus, OOD FIRE, OOD NoPE, OOD

Hình 8: Độ chính xác của các mô hình trên tác vụ bitwise OR khi được huấn luyện trên dữ liệu có kích thước lên đến 20, thay đổi qua các positional embeddings và kiến trúc khác nhau. Abacus Embeddings cải thiện mạnh hiệu suất trên tác vụ này.

Chúng tôi huấn luyện standard transformer, standard transformer với input injection và các mô hình looped transformer trên tác vụ position wise or, trên một tập dữ liệu nơi độ dài tối đa của một trong hai vectơ input là hai mươi. Kết quả này được hiển thị trong Hình 8. Ở đây chúng ta thấy rằng Abacus Embeddings cho phép tất cả các mô hình tổng quát hóa xa hơn trên tác vụ này so với các embeddings khác mà công trình trước đó tập trung vào phép cộng. Như với phép cộng, chúng ta thấy rằng looped transformers hoạt động tốt hơn so với các kiến trúc tiêu chuẩn với FIRE hoặc NoPE embeddings. Chúng tôi lưu ý rằng những độ chính xác này không cao như chúng tôi báo cáo cho phép cộng. Chúng tôi giả thuyết rằng điều này là do mô hình phải dự đoán cùng một token nhiều lần liên tiếp, điều này đã được cho là nguyên nhân gây ra lỗi trong công trình cộng trước đó[Qian et al., 2022]. Khi chúng tôi phân tích các lỗi trong tác vụ này, chúng tôi nhận thấy chúng chủ yếu được gây ra bởi mô hình xuất ra quá ít hoặc quá nhiều zeros.

A.3.1 Dữ liệu Ví dụ
000010 ⊕00000000000000 = 00001000000000
000100 ⊕0000000 = 0001000
001⊕00000 = 00100

A.4 Các Mô hình Cộng được Huấn luyện trên Kích thước Dữ liệu Khác nhau
Qua Hình 9, chúng ta thấy rằng việc tăng kích thước của các toán hạng trong tập huấn luyện cho phép khả năng tổng quát hóa tốt hơn trên một trăm chữ số cho tất cả các mô hình. Điều này một phần do phương pháp lấy mẫu cho việc huấn luyện Abacus Embeddings. Vì siêu tham số ngẫu nhiên hóa offset k= 100 được cố định qua các thí nghiệm, có nhiều embeddings được huấn luyện hơn nếu các toán hạng được thấy trong quá trình huấn luyện dài hơn. Kích thước của tập OOD dưới 100 được giảm khi kích thước của các toán hạng được thấy trong quá trình huấn luyện tăng, vì danh mục ID giờ đây bao gồm dữ liệu này. Tuy nhiên, điều này vẫn cho thấy rằng kích thước của các toán hạng được thấy trong quá trình huấn luyện tác động trực tiếp đến khả năng tổng quát hóa, với kích thước huấn luyện lớn hơn cho phép khả năng tổng quát hóa tốt hơn.

A.5 Tổng quát hóa Độ dài Cực đoan cho Phép cộng
Absolute positional embeddings phải được học trong quá trình huấn luyện nếu không chúng không thể sử dụng được tại thời điểm kiểm tra. Điều này giới hạn Abacus Embeddings của chúng tôi được huấn luyện với siêu tham số ngẫu nhiên hóa offset k= 100. Một cách có thể để giải quyết vấn đề tổng quát hóa này là tăng giá trị của k trong quá trình kiểm tra. Trong Hình 10 (trái), chúng tôi cho thấy độ chính xác khớp chính xác của năm mô hình looped transformer, với tám lớp trong khối hồi quy và hai recurrences được huấn luyện trên dữ liệu kích thước 20 với Abacus Embeddings và k= 101, tổng quát hóa cho phép cộng 120 chữ số. Chúng tôi chỉ hiển thị độ chính xác cho các toán hạng có cùng độ dài trong Hình 10 (trái), thấy những mô hình này liên tục đạt được độ chính xác 95% và cao hơn. Chúng ta thấy điều này qua bài báo, phương pháp này mạnh mẽ hơn nhiều so với cái được trình bày bởi Zhou et al. [2024].

0102030405060708090100110120
Operand Length0102030405060708090100Exact Match AccuracyRun 1
Run 2
Run 3
Run 4
Run 5
In Distribution
0102030405060708090100110120
Operand Length0102030405060708090100Exact Match Accuracyk=100
k=75
k=50
k=25
In Distribution

Hình 10: Trái: Độ chính xác khớp chính xác của năm mô hình được huấn luyện trên dữ liệu kích thước 20, tổng quát hóa tốt cho phép cộng 120 chữ số, một ngoại suy 6×. Phải: Độ chính xác khớp chính xác của năm mô hình được huấn luyện trên dữ liệu kích thước 20, siêu tham số ngẫu nhiên hóa offset k= 25,50,75 và 100.
Chỉ hiển thị độ chính xác cho các toán hạng có cùng độ dài.

Trong Hình 10 (phải) và 11, chúng tôi tiếp tục thay đổi siêu tham số ngẫu nhiên hóa offset tối đa và kích thước của các số trong dữ liệu huấn luyện. Trong Hình 10 (phải), chúng tôi cho thấy rằng việc thay đổi siêu tham số ngẫu nhiên hóa offset tối đa (k) thay đổi lượng ngoại suy khi chúng ta tăng k lên 100, như mong đợi, Điều này cho phép chúng ta tổng quát hóa cho các toán hạng trên một googol. Trong Hình 11, chúng tôi cho thấy các mô hình được huấn luyện trên dữ liệu kích thước 30 và 40 với các giá trị lớn hơn cho k, khả năng tổng quát hóa độ dài tối đa 6.8× từ huấn luyện. Chúng ta thấy các mô hình gặp khó khăn trong việc sử dụng các embeddings lớn nhất, ví dụ embedding 214 trong Hình 11 (phải), điều này là do việc huấn luyện ngẫu nhiên của embeddings, có nghĩa là các embeddings lớn nhất được cập nhật không thường xuyên. Điều này có thể được khắc phục bằng việc huấn luyện lâu hơn nhưng để giữ nhất quán với các kết quả khác, chúng tôi chỉ huấn luyện trong 24 giờ trên một A4000 duy nhất. Do đó, chúng ta có thể dễ dàng tăng k lên các giá trị lớn hơn và thực hiện phép tính với nhiều chữ số hơn, với dữ liệu huấn luyện phù hợp.

020406080100120140160180200
Operand Length0102030405060708090100Exact Match Accuracyk=175
k=150
k=125
In Distribution
020406080100120140160180200220
Operand Length0102030405060708090100Exact Match Accuracyk=175
k=150
k=125
In Distribution

Hình 11: Trái: Độ chính xác khớp chính xác của năm mô hình được huấn luyện trên dữ liệu kích thước 30, siêu tham số ngẫu nhiên hóa offset k= 125,150 và 175. Phải: Độ chính xác khớp chính xác của năm mô hình được huấn luyện trên dữ liệu kích thước 40, siêu tham số ngẫu nhiên hóa offset k= 125,150 và 175.
Chỉ hiển thị độ chính xác cho các toán hạng có cùng độ dài. Những kết quả này từ các mô hình có 8 lớp trong khối hồi quy và 2 recurrences và được huấn luyện trên dữ liệu kích thước 30 với k khác nhau, mỗi dòng là trung bình của 3 mô hình.

A.6 Biểu đồ Đầy đủ 100 x 100 cho Phép cộng
Ở đây chúng tôi trình bày độ chính xác trung bình dưới dạng heatmaps cho các thí nghiệm cộng chính được hiển thị trong toàn bộ bài báo. Hình 12 (trái) tương ứng với Trên Trái của Hình 9. Hình 12 (phải) tương ứng với Trên Phải Hình 9 và Trái của Hình 3. Hình 13 (trái) tương ứng với Dưới Trái Hình 9. Hình 13 (phải) tương ứng với Dưới Phải Hình 9 và Phải của Hình 3. Hình 14 tương ứng với Hình 4 và 17. Tất cả những hình này cho thấy khả năng tổng quát hóa của Abacus Embeddings trong cả hai chiều của bài toán cộng.

0
50
100LT, Abacus
 LT, FIRE
 LT, NoPE
0
50
100ST, Abacus
 ST, FIRE
 ST, NoPE
0 50 1000
50
100ST w/ II, Abacus
0 50 100ST w/ II, FIRE
0 50 100ST w/ II, NoPE
020406080100
Length of Operand TwoLength of Operand One
0
50
100LT, Abacus
 LT, FIRE
 LT, NoPE
0
50
100ST, Abacus
 ST, FIRE
 ST, NoPE
0 50 1000
50
100ST w/ II, Abacus
0 50 100ST w/ II, FIRE
0 50 100ST w/ II, NoPE
020406080100
Length of Operand TwoLength of Operand One

Hình 12: Biểu đồ độ chính xác khớp chính xác đầy đủ 100×100, lấy trung bình qua ba mô hình. Trái: Dữ liệu huấn luyện kích thước 10, tương ứng với Trên Trái của Hình 9; Phải: Dữ liệu huấn luyện kích thước 20, tương ứng với Trên Phải của Hình 9 và Trái của Hình 3.

A.7 Ablations Phép cộng
A.7.1 Phân tích các Tính chất Trung gian của Recurrence
Nhờ kiến trúc looped transformer, chúng tôi có thể trích xuất các giải pháp trung gian từ các mô hình, cho phép chúng tôi vẽ outputs của mô hình qua các lần lặp của khối hồi quy. Chúng tôi trình bày một ví dụ trong Hình 15 và gợi ý rằng mức độ diễn giải này có thể được tận dụng trong công trình tương lai. Mô hình được trình bày là một mô hình 1×16, một lớp decoder và mười sáu recurrences. Chúng tôi không hiển thị đầy đủ 16 lần lặp trong biểu đồ này để dễ đọc nhưng những mô hình này duy trì một điểm cố định đến 16 lần lặp và hơn nữa.

0
50
100LT, Abacus
 LT, FIRE
 LT, NoPE
0
50
100ST, Abacus
 ST, FIRE
 ST, NoPE
0 50 1000
50
100ST w/ II, Abacus
0 50 100ST w/ II, FIRE
0 50 100ST w/ II, NoPE
020406080100
Length of Operand TwoLength of Operand One
0
50
100LT, Abacus
 LT, FIRE
 LT, NoPE
0
50
100ST, Abacus
 ST, FIRE
 ST, NoPE
0 50 1000
50
100ST w/ II, Abacus
0 50 100ST w/ II, FIRE
0 50 100ST w/ II, NoPE
020406080100
Length of Operand TwoLength of Operand One

Hình 13: Biểu đồ độ chính xác khớp chính xác đầy đủ 100×100, lấy trung bình qua ba mô hình. Trái: Dữ liệu huấn luyện kích thước 30, tương ứng với Dưới Trái Hình 9; Phải: Dữ liệu huấn luyện kích thước 40, tương ứng với Dưới Phải Hình 9 và Phải của Hình 3.

A.7.2 Loại bỏ Masking Trước Dấu Bằng
Chúng tôi mask tất cả các token trước dấu bằng trong tất cả các thí nghiệm của chúng tôi, chúng tôi giả thuyết rằng với thời gian huấn luyện nhiều hơn, ràng buộc này có thể được loại bỏ. Trong Hình 16, chúng tôi cho thấy hiệu ứng của việc huấn luyện với cùng lượng flops như các thí nghiệm cộng khác mà không masking trước dấu bằng.

A.7.3 Thay đổi Effective Depth
Chúng tôi bắt đầu trong Hình 17 bằng cách hiển thị một bản sao của Hình 4, lần này bao gồm so sánh với FIRE và NoPE embeddings. Thấy, một lần nữa, những cải thiện mà Abacus Embeddings mang lại cho phép cộng.

Trong Hình 18, chúng tôi trình bày các mô hình với effective depths 8 và hơn 16, tương ứng. Trong Hình 18 (trái), chúng ta thấy rằng các mô hình effective depth 8 hoạt động kém hơn các mô hình với 8 lớp trong khối hồi quy và hai recurrences được hiển thị trong Hình 4, chứng minh lợi ích của recurrence trong trường hợp này. Chúng ta thấy độ chính xác rất cao từ tất cả các mô hình trong Hình 18 (phải). Một lần nữa, các mô hình hồi quy độ sâu 32 vượt trội hơn các mô hình tiêu chuẩn với input injection, mặc dù nó chỉ có khoảng một phần tư tham số và đạt được độ chính xác OOD trung bình cao nhất trong tất cả các mô hình được trình bày. Những ablations này cho thấy rằng với Abacus Embeddings, tác vụ cộng có thể được học qua nhiều effective depths với các mức độ chính xác khác nhau.

Trong Hình 19 (trái), chúng tôi loại bỏ input injection đến các lớp trung gian trong khối hồi quy, chỉ giữ input injection đến lớp đầu tiên của khối hồi quy. Trong Hình 19 (phải), chúng tôi chia gradients trong khối hồi quy cho số lượng recurrences cho các mô hình looped transformer trong quá trình huấn luyện. Chúng ta thấy những thay đổi hiệu suất rất nhỏ cho tất cả các mô hình được hiển thị trong Hình 19, với mô hình 2×8 cải thiện hiệu suất của nó một chút trong biểu đồ trái và mô hình 4×4 cải thiện một chút trong biểu đồ phải. Chúng tôi ablate những lựa chọn thiết kế này vì chúng tôi phải loại bỏ input injection bên trong khối hồi quy và chia gradients trong khối hồi quy cho số lượng recurrences cho các mô hình nhân được hiển thị trong Hình 6. Do đó, chúng ta có thể kết luận sẽ chỉ có những thay đổi hiệu suất rất nhỏ trong trường hợp này cho phép cộng.

A.7.4 Thêm randomized Padding
Abacus Embeddings đưa ra priors mạnh cho các tác vụ số nhưng không có chúng, looped transformers hoạt động tốt hơn so với các kiến trúc transformer tiêu chuẩn mà chúng tôi trình bày. Kết quả được hiển thị trong Hình 20 phù hợp với giả thuyết rằng với ít priors hơn, các mô hình looped transformer có thể tổng quát hóa tốt hơn. Trong trường hợp này, các priors được giảm vì dữ liệu huấn luyện được nhiễu với các ký tự pad ngẫu nhiên, một phương pháp được cho thấy cải thiện khả năng tổng quát hóa độ dài trong công trình trước đó [Shen et al., 2023].

A.7.5 Gợi ý Chỉ mục
Zhou et al. [2023] "lấy mẫu ngẫu nhiên các gợi ý chỉ mục liên tiếp từ một tập hợp gợi ý được sắp xếp trước với 102 ký hiệu," ví dụ a6b7c5 +a1b6c3 =a7b3c9. Chúng tôi triển khai phương pháp này theo hai cách. Thứ nhất, tuần hoàn, ở đây chúng tôi coi danh sách là tuần hoàn khi lấy mẫu. Thứ hai, không tuần hoàn, điều này giảm số lượng mẫu nhận được embeddings sau này trong thứ tự vì chúng tôi chỉ lấy mẫu từ danh sách theo thứ tự. Chúng ta thấy kết quả tương tự cho các mô hình được huấn luyện trên tối đa hai mười chữ số như Zhou et al. [2023]. Chúng tôi lưu ý rằng định dạng của chúng tôi lấy độ chính xác khớp chính xác trung bình làm nổi bật tính mạnh mẽ vì nếu một trong ba mô hình được kiểm tra không tổng quát hóa tốt, điều này sẽ tác động mạnh đến độ chính xác được báo cáo. Chúng tôi chỉ hiển thị so sánh với dữ liệu huấn luyện kích thước 20 do chi phí tăng của việc đánh giá những mô hình gợi ý chỉ mục này, vì inputs và outputs dài khoảng gấp đôi so với các câu hỏi thông thường, thời gian inference tăng mạnh. Do những vấn đề mạnh mẽ được làm nổi bật bởi Zhou et al. [2024] với các phương pháp của họ, chúng tôi cố gắng hết sức để tái tạo trung thực công trình của họ trong thiết lập thí nghiệm của chúng tôi, lưu ý rằng có lẽ một random seed hoặc khởi tạo tốt hơn có thể tạo ra kết quả tốt hơn cho những mô hình này.

A.8 Thông tin Thí nghiệm Bổ sung
Trong công trình này, chúng tôi xem xét ba loại mô hình khác nhau, standard transformer cổ điển, standard transformer với input injection, và looped transformers. Chúng tôi mô tả trực quan những cái này trong Hình 22.

0
50
10016x1, Abacus
 16x1, FIRE
 16x1, NoPE
0
50
1008x2, Abacus
 8x2, FIRE
 8x2, NoPE
0
50
1004x4, Abacus
 4x4, FIRE
 4x4, NoPE
0
50
1002x8, Abacus
 2x8, FIRE
 2x8, NoPE
0 50 1000
50
1001x16, Abacus
0 50 1001x16, FIRE
0 50 1001x16, NoPE
020406080100
Length of Operand TwoLength of Operand One

Hình 14: Biểu đồ độ chính xác khớp chính xác đầy đủ 100x100, lấy trung bình qua ba mô hình, liên quan đến Hình 4 và 17.

Hình 15: Biểu đồ cho thấy sự cải thiện của dự đoán qua các lần lặp "suy nghĩ" trên một bài toán cộng 100 chữ số.

Input Prompt:
587928785434679080355608971949871667189221012941443697496891519051264419888571617
0096255295233702836+4358110391552830769683978480187501721764900525218097903808750
786159803668915002036143168815597779644=
Answer:
919576073626374550845911684630020084191658772891994105418527595750262943203928417
58606474262584957001[EOS]
(Lưu ý rằng biểu đồ bị cắt ngắn.)

LT ST ST w/ II
Architecture Type020406080100Exact Match Accuracy
9.1
0.5
0.8
53.0
0.7
1.3
84.9
3.5
2.1
Abacus, OOD FIRE, OOD NoPE, OOD

Hình 16: Hiệu ứng của việc loại bỏ masking của loss trước dấu "=" trong tác vụ cộng. Tất cả các mô hình hoạt động tệ hơn khi được huấn luyện trong 24 giờ trên một Nvidia RTX A4000 duy nhất nếu chúng ta không mask câu hỏi input trong hàm loss.

16x1 8x2 4x4 2x8 1x16
Layers in Recurrent Block X Number of Recurrences020406080100Exact Match Accuracy
97.9
2.9
3.2
99.1
3.6
3.7
98.8
5.5
4.8
97.9
3.7
2.8
79.8
5.3
7.930.6
0
0
31.3
0
0
30.1
0
0
29.1
0
0
13.7
0
0
Abacus, OOD
Abacus, 100+ OODFIRE, OOD
FIRE, 100+ OODNoPE, OOD
NoPE, 100+ OOD

Hình 17: Tiếp tục của Hình 4, bao gồm FIRE và NoPE embeddings. Chúng ta thấy Abacus Embeddings hoạt động tốt nhất cho tất cả các mô hình.

LT ST ST w/ II
Architecture Type020406080100Exact Match Accuracy
49.2
4.1
2.2
96.4
4.5
4.7
97.5
4.1
3.60.1
0
0
28.1
0
0
29.7
0
0
Abacus, OOD
Abacus, 100+ OODFIRE, OOD
FIRE, 100+ OODNoPE, OOD
NoPE, 100+ OOD
1x32 4x8 8x8
Recurrences X Size of Block020406080100Exact Match Accuracy
98.6
3.4
3.9
99.6
4.9
5.5
98.2
4.1
4.228.6
0
0
30.5
0
0
29.1
0
0
Abacus, OOD
Abacus, 100+ OODFIRE, OOD
FIRE, 100+ OODNoPE, OOD
NoPE, 100+ OOD

Hình 18: Trái: Các mô hình effective depth 8, được huấn luyện trên dữ liệu kích thước 20. Những mô hình này hoạt động kém hơn các mô hình với tám lớp trong khối hồi quy và hai recurrences được hiển thị trong Hình 4, cho thấy lợi ích của recurrence cho phép cộng. Phải: Các mô hình effective depth >16, được huấn luyện trên dữ liệu kích thước 20. Các mô hình chứa nhiều tham số hơn so với tất cả các mô hình khác mà chúng tôi trình bày, cho thấy nhiều hơn rằng effective depth hơn 16 không nhất thiết cải thiện độ chính xác trong setting này.

8x2 4x4 2x8
Layers in Recurrent Block X Number of Recurrences020406080100Exact Match Accuracy
97.1
98.3
98.129.1
30.7
26.3
Abacus, OOD Abacus, 100+ OOD
8x2 4x4 2x8
Layers in Recurrent Block X Number of Recurrences020406080100Exact Match Accuracy
98.3
99.6
95.229.9
30.7
31.3
Abacus, OOD Abacus, 100+ OOD

Hình 19: Bản sao của các mô hình looped transformer được hiển thị trong Hình 4, để kiểm tra các sửa đổi mà chúng tôi sử dụng để huấn luyện các mô hình cộng không tác động tiêu cực đến việc huấn luyện cộng, lấy trung bình của ba mô hình trong mỗi trường hợp. Trái: không có input injection đến các lớp bên trong khối hồi quy, chỉ đến lớp đầu tiên của khối hồi quy. Phải: chia gradients trong khối hồi quy cho số lượng recurrences.

Do kiến trúc looped transformer, số lượng recurrences tại thời điểm huấn luyện có thể khác với số lượng recurrences tại thời điểm kiểm tra, mặc dù chúng tôi không sử dụng điều này trong công trình này.

LT ST ST w/ II
Architecture Type020406080100Exact Match Accuracy
72.8
4.9
6.3
1.5
3.6
3.3
1.4
2.9
3.4
Abacus, OOD FIRE, OOD NoPE, OOD

Hình 20: Hiệu ứng của việc thêm randomized padding vào dữ liệu huấn luyện chỉ cho tác vụ cộng. Các mô hình Looped transformer có thể duy trì độ chính xác cao khi random padding được thêm vào dữ liệu.

LT ST ST w/ II
Architecture Type020406080100Exact Match Accuracy
13.8
6.0
14.6
12.3
9.9
5.8
FIRE Rand, OOD FIRE Rand Circular Hints, OOD

Hình 21: Sử dụng gợi ý chỉ mục và randomized FIRE embeddings, được trình bày bởi Zhou et al. [2024], huấn luyện trên dữ liệu kích thước 20 với phương pháp luận của chúng tôi, chẳng hạn như masking trước dấu bằng. Điều này sẽ có thể so sánh với "1 to 20" trong Hình 13 được trình bày bởi Zhou et al. [2024] và Hình 3 của công trình chúng tôi.

Hình 22: Hình ảnh hóa ba kiến trúc mà chúng tôi nghiên cứu.

Vì Abacus Embeddings là một biến thể của absolute embeddings, chỉ được sử dụng lại cho số, chúng có thể được kết hợp với relative embeddings được triển khai trong các mô hình hiện tại. Nếu tất cả các chữ số input vào mô hình được tokenized riêng lẻ, chúng ta có thể thực hiện một phép toán thời gian tuyến tính để tìm và gán relative embeddings cho tất cả các số trong một input, thấp hơn chi phí bậc hai phát sinh bởi attention. Việc huấn luyện một số lượng nhỏ Abacus Embeddings có thể đủ để xử lý tất cả các inputs số cho phép cộng vì chúng được sử dụng lại. Để triển khai đầy đủ phương pháp luận của chúng tôi, tất cả các số cũng phải được đảo ngược, điều này có thể được triển khai với các biểu thức chính quy đơn giản trên tất cả inputs và outputs.

Chúng tôi sử dụng character level tokenizer cho tất cả các thí nghiệm và greedy decoding trong tất cả việc kiểm tra. Chúng tôi huấn luyện tất cả các mô hình với local batch size là batch size tối đa là lũy thừa của hai sẽ vừa với mười sáu gigabytes bộ nhớ GPU. Đối với các mô hình nhân, chúng tôi đầu tiên lấy loss trung bình qua các mẫu trước khi lấy trung bình qua tất cả các mẫu trong một batch, thay vì lấy loss trung bình qua tất cả token trong một batch; chúng tôi nhận thấy điều này dẫn đến việc huấn luyện ổn định hơn một chút. Chúng tôi lưu ý rằng việc huấn luyện các mô hình để giải quyết phép nhân yêu cầu nhiều điều chỉnh siêu tham số hơn so với phép cộng, có lẽ ngụ ý rằng đó là một tác vụ khó hơn để học. Ngoài ra, các mô hình FIRE yêu cầu ngân sách tính toán lớn hơn nhiều cho tìm kiếm siêu tham số so với các mô hình Abacus cho phép nhân. Trong Bảng 3, chúng tôi trình bày số lượng tham số xấp xỉ cho các mô hình được huấn luyện với input injection và Abacus Embeddings.

Sử dụng Tính toán. Chúng tôi nêu chi tiết việc sử dụng GPU mặc định cho mỗi thí nghiệm trong Bảng 4. Đối với một số thí nghiệm, chẳng hạn như khả năng tổng quát hóa độ dài cực đoan (Hình 10) và gợi ý chỉ mục (Hình 21), cần nhiều giờ GPU hơn để kiểm tra, những cái này được bao gồm trong tổng số giờ GPU được sử dụng. Pipeline kiểm tra của chúng tôi cho phép cộng và Bitwise OR sử dụng GPU Nvidia V100. Do vấn đề kỹ thuật, 'torch.compile' không thể được sử dụng trên GPU V100 mà chúng tôi sử dụng, do đó những người khác có thể giảm thời gian tính toán này trong các nghiên cứu tương lai. Tất cả tính toán được cung cấp bởi tài nguyên nội bộ. Trong giai đoạn khám phá của dự án này, chúng tôi đã sử dụng nhiều giờ GPU hơn để kiểm tra và thiết kế các thí nghiệm được hiển thị, sử dụng khoảng 1.5 terabytes lưu trữ của toàn bộ dự án. Một ước tính về tổng tính toán cần thiết cho tất cả các kết quả được trình bày trong bài báo chính là 10,039 giờ GPU. Kết quả phụ lục yêu cầu thêm 18,278 giờ GPU.

A.8.1 Siêu tham số
Chúng tôi nêu chi tiết những gì chúng tôi tin là một tập con quan trọng của các giá trị siêu tham số mặc định trong Bảng 5. Một danh sách đầy đủ tất cả các siêu tham số và cấu hình mô hình được chứa trong phát hành mã. Đối với các mô hình nhân với FIRE embeddings, tỷ lệ học là 0.00006, do sự bất ổn lớn ở tỷ lệ học cao hơn mà không được trải nghiệm đối với Abacus Embeddings.

A.8.2 Phát hành Mã
Chúng tôi sẽ phát hành tất cả mã và tập dữ liệu trên GitHub với Giấy phép MIT.

Bảng 3: Số lượng tham số, đến triệu gần nhất, trong một mô hình với Abacus Embeddings và input injection.

Layers in Recurrent Block Recurrences Parameters (Millions)
16 1 122
8 2 64
4 4 34
2 8 19
1 16 12

Bảng 4: Số giờ GPU Nvidia mặc định được sử dụng để huấn luyện một mô hình.

Dataset Number of GPU Hours (training) Number of GPU Hours (testing)
Addition 24 - RTX A4000 65.8 - V100
Bitwise OR 1 - RTX A4000 45 - V100
Sorting 24 - RTX A4000 64 - RTX A4000
Multiplication 192 - RTX A4000 0.83 - RTX A4000

Bảng 5: Giá trị siêu tham số mặc định.

Hyperparameter Default Value
Hidden Size 1024
Intermediate Size 2048
Embedding Size 1024
Number of Attention Heads 16
Progressive Loss Alpha [Bansal et al., 2022] 1.0
Data Type float16/float32
Optimizer AdamW [Loshchilov and Hutter, 2017]
Global Batch Size 8192
Batch Size Ramp 0.6
Learning Rate 0.0001
Learning Rate Scheduler Trapezoid [Zhai et al., 2022]
Activation Function GELUglu [Shazeer, 2020]
Normalization Layer LayerNorm [Ba et al., 2016]
Normalization Type Post
Offset Randomization Hyperparameter (k) 100
Initialization Deepnorm [Wang et al., 2022]

Checklist Bài báo NeurIPS
1.Claims
Câu hỏi: Các tuyên bố chính được đưa ra trong tóm tắt và giới thiệu có phản ánh chính xác đóng góp và phạm vi của bài báo không?
Câu trả lời: [Có]
Lý do: Vui lòng xem Phần 3.
Hướng dẫn:
•Câu trả lời NA có nghĩa là tóm tắt và giới thiệu không bao gồm các tuyên bố được đưa ra trong bài báo.
•Tóm tắt và/hoặc giới thiệu nên nêu rõ các tuyên bố được đưa ra, bao gồm các đóng góp được thực hiện trong bài báo và các giả định và hạn chế quan trọng. Câu trả lời Không hoặc NA cho câu hỏi này sẽ không được các nhà đánh giá đánh giá tốt.
•Các tuyên bố được đưa ra nên phù hợp với kết quả lý thuyết và thực nghiệm, và phản ánh mức độ kết quả có thể được mong đợi để tổng quát hóa cho các setting khác.
•Việc bao gồm các mục tiêu khát vọng làm động lực là tốt miễn là rõ ràng rằng những mục tiêu này không được đạt được bởi bài báo.

2.Limitations
Câu hỏi: Bài báo có thảo luận về các hạn chế của công trình được thực hiện bởi các tác giả không?
Câu trả lời: [Có]
Lý do: Vui lòng xem Phần 5.
Hướng dẫn:
•Câu trả lời NA có nghĩa là bài báo không có hạn chế trong khi câu trả lời Không có nghĩa là bài báo có hạn chế, nhưng những hạn chế đó không được thảo luận trong bài báo.
• Các tác giả được khuyến khích tạo một phần "Limitations" riêng biệt trong bài báo của họ.
•Bài báo nên chỉ ra bất kỳ giả định mạnh nào và mức độ mạnh mẽ của kết quả đối với các vi phạm những giả định này (e.g., giả định độc lập, setting không nhiễu, mô hình đặc trưng tốt, xấp xỉ tiệm cận chỉ giữ cục bộ). Các tác giả nên suy ngẫm về cách những giả định này có thể bị vi phạm trong thực tế và hậu quả sẽ là gì.
•Các tác giả nên suy ngẫm về phạm vi của các tuyên bố được đưa ra, e.g., nếu phương pháp chỉ được kiểm tra trên một số tập dữ liệu hoặc với một số lần chạy. Nói chung, kết quả thực nghiệm thường phụ thuộc vào các giả định ngầm, cần được diễn giải.
•Các tác giả nên suy ngẫm về các yếu tố ảnh hưởng đến hiệu suất của phương pháp. Ví dụ, một thuật toán nhận dạng khuôn mặt có thể hoạt động kém khi độ phân giải hình ảnh thấp hoặc hình ảnh được chụp trong ánh sáng yếu. Hoặc một hệ thống speech-to-text có thể không được sử dụng một cách đáng tin cậy để cung cấp phụ đề đóng cho các bài giảng trực tuyến vì nó không xử lý được thuật ngữ kỹ thuật.
•Các tác giả nên thảo luận về hiệu quả tính toán của các thuật toán được đề xuất và cách chúng mở rộng theo kích thước tập dữ liệu.
•Nếu có thể áp dụng, các tác giả nên thảo luận về các hạn chế có thể của phương pháp của họ để giải quyết các vấn đề về quyền riêng tư và công bằng.
•Mặc dù các tác giả có thể lo sợ rằng sự trung thực hoàn toàn về các hạn chế có thể được các nhà đánh giá sử dụng làm cơ sở để từ chối, kết quả tệ hơn có thể là các nhà đánh giá khám phá ra các hạn chế không được thừa nhận trong bài báo. Các tác giả nên sử dụng phán đoán tốt nhất của họ và nhận ra rằng các hành động cá nhân có lợi cho tính minh bạch đóng vai trò quan trọng trong việc phát triển các chuẩn mực bảo tồn tính toàn vẹn của cộng đồng. Các nhà đánh giá sẽ được hướng dẫn cụ thể để không phạt sự trung thực về các hạn chế.

3.Theory Assumptions and Proofs
Câu hỏi: Đối với mỗi kết quả lý thuyết, bài báo có cung cấp tập hợp đầy đủ các giả định và một chứng minh hoàn chỉnh (và chính xác) không?
Câu trả lời: [NA]
Lý do: Không có định lý/chứng minh.
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không bao gồm kết quả lý thuyết.
•Tất cả các định lý, công thức và chứng minh trong bài báo nên được đánh số và tham chiếu chéo.
•Tất cả các giả định nên được nêu rõ ràng hoặc được tham chiếu trong phát biểu của bất kỳ định lý nào.
•Các chứng minh có thể xuất hiện trong bài báo chính hoặc tài liệu bổ sung, nhưng nếu chúng xuất hiện trong tài liệu bổ sung, các tác giả được khuyến khích cung cấp một phác thảo chứng minh ngắn để cung cấp trực giác.
•Ngược lại, bất kỳ chứng minh không chính thức nào được cung cấp trong lõi của bài báo nên được bổ sung bởi các chứng minh chính thức được cung cấp trong phụ lục hoặc tài liệu bổ sung.
• Các định lý và Bổ đề mà chứng minh dựa vào nên được tham chiếu đúng cách.

4.Experimental Result Reproducibility
Câu hỏi: Bài báo có tiết lộ đầy đủ tất cả thông tin cần thiết để tái tạo các kết quả thực nghiệm chính của bài báo đến mức ảnh hưởng đến các tuyên bố và/hoặc kết luận chính của bài báo (bất kể mã và dữ liệu có được cung cấp hay không)?
Câu trả lời: [Có]
Lý do: Vui lòng xem Phần 3, Phần A.8.
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không bao gồm thí nghiệm.
•Nếu bài báo bao gồm thí nghiệm, câu trả lời Không cho câu hỏi này sẽ không được các nhà đánh giá đánh giá tốt: Việc làm cho bài báo có thể tái tạo là quan trọng, bất kể mã và dữ liệu có được cung cấp hay không.
•Nếu đóng góp là một tập dữ liệu và/hoặc mô hình, các tác giả nên mô tả các bước được thực hiện để làm cho kết quả của họ có thể tái tạo hoặc có thể xác minh.
•Tùy thuộc vào đóng góp, khả năng tái tạo có thể được thực hiện theo nhiều cách khác nhau. Ví dụ, nếu đóng góp là một kiến trúc mới, việc mô tả kiến trúc đầy đủ có thể đủ, hoặc nếu đóng góp là một mô hình cụ thể và đánh giá thực nghiệm, có thể cần thiết để làm cho những người khác có thể sao chép mô hình với cùng tập dữ liệu, hoặc cung cấp quyền truy cập vào mô hình. Nói chung. việc phát hành mã và dữ liệu thường là một cách tốt để thực hiện điều này, nhưng khả năng tái tạo cũng có thể được cung cấp thông qua các hướng dẫn chi tiết về cách sao chép kết quả, quyền truy cập vào một mô hình được lưu trữ (e.g., trong trường hợp của một mô hình ngôn ngữ lớn), phát hành checkpoint mô hình, hoặc các phương tiện khác phù hợp với nghiên cứu được thực hiện.
•Mặc dù NeurIPS không yêu cầu phát hành mã, hội nghị yêu cầu tất cả các submission cung cấp một số cách hợp lý cho khả năng tái tạo, có thể phụ thuộc vào bản chất của đóng góp. Ví dụ
(a)Nếu đóng góp chủ yếu là một thuật toán mới, bài báo nên làm rõ cách tái tạo thuật toán đó.
(b)Nếu đóng góp chủ yếu là một kiến trúc mô hình mới, bài báo nên mô tả kiến trúc một cách rõ ràng và đầy đủ.
(c)Nếu đóng góp là một mô hình mới (e.g., một mô hình ngôn ngữ lớn), thì nên có cách để truy cập mô hình này để tái tạo kết quả hoặc một cách để tái tạo mô hình (e.g., với một tập dữ liệu mã nguồn mở hoặc hướng dẫn về cách xây dựng tập dữ liệu).
(d)Chúng tôi nhận ra rằng khả năng tái tạo có thể khó khăn trong một số trường hợp, trong trường hợp đó các tác giả được chào đón để mô tả cách cụ thể họ cung cấp cho khả năng tái tạo. Trong trường hợp các mô hình closed-source, có thể quyền truy cập vào mô hình bị hạn chế theo một cách nào đó (e.g., đối với người dùng đã đăng ký), nhưng những nhà nghiên cứu khác phải có một số đường dẫn để tái tạo hoặc xác minh kết quả.

5.Open access to data and code
Câu hỏi: Bài báo có cung cấp quyền truy cập mở vào dữ liệu và mã, với hướng dẫn đầy đủ để tái tạo trung thực các kết quả thực nghiệm chính, như được mô tả trong tài liệu bổ sung?
Câu trả lời: [Có]
Lý do: Chúng tôi sẽ tải lên mã triển khai và tập dữ liệu của chúng tôi lên Github. Trong chu kỳ submission, chúng tôi cung cấp một triển khai ẩn danh có thể được tìm thấy trong tài liệu bổ sung của submission này. Triển khai của chúng tôi được cấp phép theo giấy phép MIT.
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không bao gồm thí nghiệm yêu cầu mã.
•Vui lòng xem hướng dẫn submission mã và dữ liệu NeurIPS (https://nips.cc/public/guides/CodeSubmissionPolicy) để biết thêm chi tiết.
•Mặc dù chúng tôi khuyến khích việc phát hành mã và dữ liệu, chúng tôi hiểu rằng điều này có thể không khả thi, vì vậy "Không" là một câu trả lời chấp nhận được. Các bài báo không thể bị từ chối đơn giản vì không bao gồm mã, trừ khi điều này là trung tâm của đóng góp (e.g., cho một benchmark mã nguồn mở mới).
•Các hướng dẫn nên chứa lệnh chính xác và môi trường cần thiết để chạy để tái tạo kết quả. Xem hướng dẫn submission mã và dữ liệu NeurIPS (https://nips.cc/public/guides/CodeSubmissionPolicy) để biết thêm chi tiết.
•Các tác giả nên cung cấp hướng dẫn về truy cập và chuẩn bị dữ liệu, bao gồm cách truy cập dữ liệu thô, dữ liệu đã xử lý, dữ liệu trung gian và dữ liệu được tạo ra, v.v.
•Các tác giả nên cung cấp scripts để tái tạo tất cả kết quả thực nghiệm cho phương pháp mới được đề xuất và baselines. Nếu chỉ một tập con thí nghiệm có thể tái tạo, họ nên nêu rõ những cái nào được bỏ qua khỏi script và tại sao.
•Tại thời điểm submission, để bảo tồn tính ẩn danh, các tác giả nên phát hành các phiên bản ẩn danh (nếu có thể áp dụng).
•Cung cấp càng nhiều thông tin càng tốt trong tài liệu bổ sung (được đính kèm với bài báo) được khuyến nghị, nhưng bao gồm URL đến dữ liệu và mã được cho phép.

6.Experimental Setting/Details
Câu hỏi: Bài báo có chỉ định tất cả các chi tiết huấn luyện và kiểm tra (e.g., chia dữ liệu, siêu tham số, cách chúng được chọn, loại optimizer, v.v.) cần thiết để hiểu kết quả không?
Câu trả lời: [Có]
Lý do: Vui lòng xem Phần 3 và A.8.
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không bao gồm thí nghiệm.
•Setting thí nghiệm nên được trình bày trong lõi của bài báo đến mức chi tiết cần thiết để đánh giá cao kết quả và hiểu chúng.
•Chi tiết đầy đủ có thể được cung cấp với mã, trong phụ lục, hoặc dưới dạng tài liệu bổ sung.

7.Experiment Statistical Significance
Câu hỏi: Bài báo có báo cáo thanh lỗi được định nghĩa phù hợp và chính xác hoặc thông tin thích hợp khác về ý nghĩa thống kê của các thí nghiệm không?
Câu trả lời: [Không]
Lý do: Mặc dù chúng tôi lấy trung bình qua nhiều thử nghiệm, chúng tôi không báo cáo thanh lỗi chính xác.
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không bao gồm thí nghiệm.
•Các tác giả nên trả lời "Có" nếu kết quả được đi kèm với thanh lỗi, khoảng tin cậy, hoặc kiểm định ý nghĩa thống kê, ít nhất cho các thí nghiệm hỗ trợ các tuyên bố chính của bài báo.
•Các yếu tố biến đổi mà thanh lỗi đang nắm bắt nên được nêu rõ ràng (ví dụ, chia train/test, khởi tạo, vẽ ngẫu nhiên của một tham số nào đó, hoặc chạy tổng thể với các điều kiện thí nghiệm cho trước).
•Phương pháp tính toán thanh lỗi nên được giải thích (công thức dạng đóng, gọi đến một hàm thư viện, bootstrap, v.v.)
• Các giả định được thực hiện nên được đưa ra (e.g., lỗi phân phối Normally).
•Nên rõ ràng liệu thanh lỗi là độ lệch chuẩn hay lỗi chuẩn của trung bình.
•Việc báo cáo thanh lỗi 1-sigma là OK, nhưng người ta nên nêu rõ. Các tác giả nên ưu tiên báo cáo thanh lỗi 2-sigma hơn là nêu rằng họ có 96% CI, nếu giả thuyết về tính Normally của lỗi không được xác minh.
•Đối với các phân phối bất đối xứng, các tác giả nên cẩn thận không hiển thị trong bảng hoặc hình các thanh lỗi đối xứng sẽ tạo ra kết quả ngoài phạm vi (e.g. tỷ lệ lỗi âm).
•Nếu thanh lỗi được báo cáo trong bảng hoặc biểu đồ, Các tác giả nên giải thích trong văn bản cách chúng được tính toán và tham chiếu các hình hoặc bảng tương ứng trong văn bản.

8.Experiments Compute Resources
Câu hỏi: Đối với mỗi thí nghiệm, bài báo có cung cấp thông tin đầy đủ về tài nguyên máy tính (loại compute workers, bộ nhớ, thời gian thực thi) cần thiết để tái tạo các thí nghiệm không?
Câu trả lời: [Có]
Lý do: Vui lòng xem Phần A.8.
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không bao gồm thí nghiệm.
•Bài báo nên chỉ ra loại compute workers CPU hoặc GPU, cụm nội bộ, hoặc nhà cung cấp cloud, bao gồm bộ nhớ và lưu trữ liên quan.
•Bài báo nên cung cấp lượng tính toán cần thiết cho mỗi lần chạy thí nghiệm riêng lẻ cũng như ước tính tổng tính toán.
•Bài báo nên tiết lộ liệu toàn bộ dự án nghiên cứu có yêu cầu nhiều tính toán hơn so với các thí nghiệm được báo cáo trong bài báo hay không (e.g., thí nghiệm sơ bộ hoặc thất bại không thành công trong bài báo).

9.Code Of Ethics
Câu hỏi: Nghiên cứu được tiến hành trong bài báo có tuân thủ, về mọi khía cạnh, với Quy tắc Đạo đức NeurIPS https://neurips.cc/public/EthicsGuidelines?
Câu trả lời: [Có]
Lý do: Chúng tôi đã đọc và tuân theo quy tắc đạo đức.
Hướng dẫn:
•Câu trả lời NA có nghĩa là các tác giả chưa xem xét Quy tắc Đạo đức NeurIPS.
•Nếu các tác giả trả lời Không, họ nên giải thích các hoàn cảnh đặc biệt yêu cầu sự sai lệch khỏi Quy tắc Đạo đức.
•Các tác giả nên đảm bảo bảo tồn tính ẩn danh (e.g., nếu có sự cân nhắc đặc biệt do pháp luật hoặc quy định trong quyền tài phán của họ).

10.Broader Impacts
Câu hỏi: Bài báo có thảo luận cả tác động tích cực tiềm tàng đến xã hội và tác động tiêu cực tiềm tàng đến xã hội của công trình được thực hiện không?
Câu trả lời: [NA]
Lý do: Tác động xã hội của việc cải thiện khả năng transformer trên phép tính đơn giản được giới hạn trong những hiệu ứng có thể của việc cải thiện nhẹ hiểu biết của cộng đồng. Những tác vụ cụ thể này cách xa biên giới của tác hại/lợi ích tiềm tàng đối với xã hội.
Hướng dẫn:
• Câu trả lời NA có nghĩa là không có tác động xã hội của công trình được thực hiện.
•Nếu các tác giả trả lời NA hoặc Không, họ nên giải thích tại sao công trình của họ không có tác động xã hội hoặc tại sao bài báo không giải quyết tác động xã hội.
•Ví dụ về tác động xã hội tiêu cực bao gồm các cách sử dụng độc hại hoặc không có ý định tiềm tàng (e.g., thông tin sai lệch, tạo hồ sơ giả, giám sát), cân nhắc về công bằng (e.g., triển khai các công nghệ có thể đưa ra quyết định tác động không công bằng đến các nhóm cụ thể), cân nhắc về quyền riêng tư, và cân nhắc về bảo mật.
•Hội nghị mong đợi rằng nhiều bài báo sẽ là nghiên cứu nền tảng và không gắn liền với các ứng dụng cụ thể, chứ đừng nói đến triển khai. Tuy nhiên, nếu có đường dẫn trực tiếp đến bất kỳ ứng dụng tiêu cực nào, các tác giả nên chỉ ra điều đó. Ví dụ, việc chỉ ra rằng một cải thiện trong chất lượng của các mô hình tạo sinh có thể được sử dụng để tạo deepfakes cho thông tin sai lệch là hợp lý. Mặt khác, không cần thiết phải chỉ ra rằng một thuật toán chung để tối ưu hóa mạng neural có thể cho phép mọi người huấn luyện các mô hình tạo Deepfakes nhanh hơn.
•Các tác giả nên xem xét những tác hại có thể xảy ra khi công nghệ được sử dụng như dự định và hoạt động chính xác, những tác hại có thể xảy ra khi công nghệ được sử dụng như dự định nhưng đưa ra kết quả không chính xác, và những tác hại theo sau từ việc sử dụng sai (có ý định hoặc không có ý định) công nghệ.
•Nếu có tác động xã hội tiêu cực, các tác giả cũng có thể thảo luận về các chiến lược giảm thiểu có thể (e.g., phát hành mô hình có cổng, cung cấp biện pháp phòng thủ ngoài tấn công, cơ chế giám sát việc sử dụng sai, cơ chế giám sát cách một hệ thống học từ phản hồi theo thời gian, cải thiện hiệu quả và khả năng tiếp cận của ML).

11.Safeguards
Câu hỏi: Bài báo có mô tả các biện pháp bảo vệ đã được đặt ra cho việc phát hành có trách nhiệm dữ liệu hoặc mô hình có nguy cơ cao để sử dụng sai (e.g., mô hình ngôn ngữ được huấn luyện trước, trình tạo hình ảnh, hoặc tập dữ liệu được lấy) không?
Câu trả lời: [NA]
Lý do: Chúng tôi không phát hành các mô hình có tiềm năng sử dụng sai.
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không đặt ra những rủi ro như vậy.
•Các mô hình được phát hành có nguy cơ cao để sử dụng sai hoặc dual-use nên được phát hành với các biện pháp bảo vệ cần thiết để cho phép sử dụng được kiểm soát của mô hình, ví dụ bằng cách yêu cầu người dùng tuân thủ hướng dẫn sử dụng hoặc hạn chế để truy cập mô hình hoặc triển khai bộ lọc an toàn.
•Các tập dữ liệu đã được lấy từ Internet có thể đặt ra rủi ro an toàn. Các tác giả nên mô tả cách họ tránh phát hành hình ảnh không an toàn.
•Chúng tôi nhận ra rằng việc cung cấp các biện pháp bảo vệ hiệu quả là thách thức, và nhiều bài báo không yêu cầu điều này, nhưng chúng tôi khuyến khích các tác giả tính đến điều này và nỗ lực tốt nhất.

12.Licenses for existing assets
Câu hỏi: Những người sáng tạo hoặc chủ sở hữu ban đầu của tài sản (e.g., mã, dữ liệu, mô hình), được sử dụng trong bài báo, có được ghi nhận đúng cách và giấy phép và điều khoản sử dụng có được đề cập rõ ràng và được tôn trọng đúng cách không?
Câu trả lời: [Có]
Lý do: Vui lòng xem Phần A.8.
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không sử dụng tài sản hiện có.
• Các tác giả nên trích dẫn bài báo gốc đã tạo ra gói mã hoặc tập dữ liệu.
•Các tác giả nên nêu phiên bản nào của tài sản được sử dụng và, nếu có thể, bao gồm một URL.
• Tên của giấy phép (e.g., CC-BY 4.0) nên được bao gồm cho mỗi tài sản.
•Đối với dữ liệu được lấy từ một nguồn cụ thể (e.g., trang web), bản quyền và điều khoản dịch vụ của nguồn đó nên được cung cấp.
•Nếu tài sản được phát hành, thông tin giấy phép, bản quyền và điều khoản sử dụng trong gói nên được cung cấp. Đối với các tập dữ liệu phổ biến, paperswithcode.com/datasets đã sắp xếp giấy phép cho một số tập dữ liệu. Hướng dẫn cấp phép của họ có thể giúp xác định giấy phép của một tập dữ liệu.
•Đối với các tập dữ liệu hiện có được đóng gói lại, cả giấy phép gốc và giấy phép của tài sản phái sinh (nếu nó đã thay đổi) nên được cung cấp.
•Nếu thông tin này không có sẵn trực tuyến, các tác giả được khuyến khích liên hệ với những người tạo ra tài sản.

13.New Assets
Câu hỏi: Các tài sản mới được giới thiệu trong bài báo có được tài liệu hóa tốt và tài liệu có được cung cấp cùng với tài sản không?
Câu trả lời: [NA]
Lý do: Chúng tôi chuyển tiếp các chi tiết xây dựng mẫu nhưng chúng tôi không phát hành bất kỳ tập dữ liệu thực tế nào.
Hướng dẫn:
• Câu trả lời NA có nghĩa là bài báo không phát hành tài sản mới.
•Các nhà nghiên cứu nên truyền đạt các chi tiết của tập dữ liệu/mã/mô hình như một phần của submissions của họ thông qua các mẫu có cấu trúc. Điều này bao gồm các chi tiết về huấn luyện, giấy phép, hạn chế, v.v.
•Bài báo nên thảo luận liệu và cách sự đồng ý được thu được từ những người có tài sản được sử dụng.
•Tại thời điểm submission, hãy nhớ ẩn danh tài sản của bạn (nếu có thể áp dụng). Bạn có thể tạo một URL ẩn danh hoặc bao gồm một tệp zip ẩn danh.

14.Crowdsourcing and Research with Human Subjects
Câu hỏi: Đối với thí nghiệm crowdsourcing và nghiên cứu với đối tượng con người, bài báo có bao gồm văn bản đầy đủ của hướng dẫn được đưa cho người tham gia và ảnh chụp màn hình, nếu có thể áp dụng, cũng như chi tiết về bồi thường (nếu có) không?
Câu trả lời: [NA]
Lý do: Bài báo không liên quan đến crowdsourcing cũng như nghiên cứu với đối tượng con người.
Hướng dẫn:
•Câu trả lời NA có nghĩa là bài báo không liên quan đến crowdsourcing cũng như nghiên cứu với đối tượng con người.
•Bao gồm thông tin này trong tài liệu bổ sung là tốt, nhưng nếu đóng góp chính của bài báo liên quan đến đối tượng con người, thì càng nhiều chi tiết càng tốt nên được bao gồm trong bài báo chính.
•Theo Quy tắc Đạo đức NeurIPS, những công nhân tham gia vào thu thập dữ liệu, sắp xếp, hoặc lao động khác nên được trả ít nhất mức lương tối thiểu ở quốc gia của người thu thập dữ liệu.

15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects
Câu hỏi: Bài báo có mô tả các rủi ro tiềm tàng phát sinh bởi người tham gia nghiên cứu, liệu những rủi ro như vậy có được tiết lộ cho đối tượng hay không, và liệu sự chấp thuận của Institutional Review Board (IRB) (hoặc sự chấp thuận/đánh giá tương đương dựa trên yêu cầu của quốc gia hoặc tổ chức của bạn) có được thu được hay không?
Câu trả lời: [NA]
Lý do: Bài báo không liên quan đến crowdsourcing cũng như nghiên cứu với đối tượng con người.
Hướng dẫn:
•Câu trả lời NA có nghĩa là bài báo không liên quan đến crowdsourcing cũng như nghiên cứu với đối tượng con người.
•Tùy thuộc vào quốc gia nơi nghiên cứu được tiến hành, sự chấp thuận IRB (hoặc tương đương) có thể được yêu cầu cho bất kỳ nghiên cứu đối tượng con người nào. Nếu bạn có được sự chấp thuận IRB, bạn nên nêu rõ điều này trong bài báo.
•Chúng tôi nhận ra rằng các quy trình cho điều này có thể khác nhau đáng kể giữa các tổ chức và địa điểm, và chúng tôi mong đợi các tác giả tuân thủ Quy tắc Đạo đức NeurIPS và hướng dẫn cho tổ chức của họ.
•Đối với submissions ban đầu, đừng bao gồm bất kỳ thông tin nào sẽ phá vỡ tính ẩn danh (nếu có thể áp dụng), chẳng hạn như tổ chức tiến hành đánh giá.