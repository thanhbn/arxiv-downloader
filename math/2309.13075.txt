# 2309.13075.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/math/2309.13075.pdf
# File size: 1189753 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SCREWS
 : A M ODULAR FRAMEWORK FOR
REASONING WITH REVISIONS
Kumar Shridhar∗†Harsh Jhamtani‡Hao Fang‡
Benjamin Van Durme‡Jason Eisner‡Patrick Xia‡
†ETH Zurich‡Microsoft Semantic Machines
shkumar@ethz.ch patrickxia@microsoft.com
ABSTRACT
Large language models (LLMs) can improve their accuracy on various tasks
through iteratively refining and revising their output based on feedback. We ob-
serve that these revisions can introduce errors, in which case it is better to roll
back to a previous result. Further, revisions are typically homogeneous: they use
the same reasoning method that produced the initial answer, which may not cor-
rect errors. To enable exploration in this space, we present SCREWS , a modular
framework for reasoning with revisions. It is comprised of three main modules:
Sampling ,Conditional Resampling , and Selection , each consisting of sub-modules
that can be hand-selected per task. We show that SCREWS not only unifies several
previous approaches under a common framework, but also reveals several novel
strategies for identifying improved reasoning chains. We evaluate our framework
with state-of-the-art LLMs (ChatGPT and GPT-4) on a diverse set of reasoning
tasks and uncover useful new reasoning strategies for each: arithmetic word prob-
lems, multi-hop question answering, and code debugging. Heterogeneous revision
strategies prove to be important, as does selection between original and revised
candidates.
1 I NTRODUCTION
Large Language Models (LLMs) have proven effective on a variety of reasoning tasks (OpenAI,
2023). However, the LLM output is not always correct on its first attempt, and it is often necessary to
iteratively refine the outputs to ensure that the desired goal is achieved (Madaan et al., 2023; Welleck
et al., 2022; Zheng et al., 2023). These refinement methods assume that subsequent outputs (either
by the same model, or by an external model or some tool) lead to better performance. However,
there is no guarantee that subsequent versions must be better; as Figure 1 illustrates, refinement can
lead to a wrong answer. This motivates a Selection strategy whereby the model can select an earlier
output.
In addition, past work on iterative refinement typically assumes a single, fixed reasoning strategy
(Welleck et al., 2022; Huang et al., 2022; Madaan et al., 2023; Zheng et al., 2023). Humans, however,
are more flexible. A student preparing for an exam may use deductive reasoning to solve problems
and inductive reasoning to verify the results; or a product manager may use a brainstorming strategy
to list several ideas and then switch to a prioritization strategy to rank them based on their feasibility
or impact. Thus, we propose a modular approach to answer refinements, allowing us to test different
strategies.
In this work, we introduce SCREWS , a modular framework for reasoning with revisions.1Figure
2 introduces the three main modules of the framework in detail, namely Sampling ,Conditional
Resampling , and Selection . For a given task and input sequence, we instantiate SCREWS by fixing
the submodules for each module (for example, we might select “Chain of Thought” for Sampling ).
∗Work done while interning at Microsoft.
1SCREWS
 stands for “ Sampling, Conditional REsampling WithSelection.” Our code and results are
available at https://github.com/kumar-shridhar/Screws/ .
1arXiv:2309.13075v1  [cs.AI]  20 Sep 2023

--- PAGE 2 ---
SAMPLING
QUESTION
Raymond and Samantha are cousins. Raymond was born 6 years before
Samantha. Raymond had a son at the age of 23. If Samantha is now 31,
how many years ago was Raymond's son born?
If Samantha is 31 now, and
Raymond was born 6 years
before her, then Raymond is
31+6 = <<31+6=37>>37
years old. If Raymond was 23
years old when his son was
born, then the number of
years that have passed since
his son was born is 37-23 =
<<37-23=14>>14 years.CONDITIONAL RESAMPLING
If Samantha is 31 now, and
Raymond was born 6 years
before her, then Raymond is
31-6 = <<31-6=25>>25
years old. Raymond's son was
born when he was 23 years
old, so that was 23-25 =
<<23-25=-2>>-2 years ago. -2 years
S E L E C T I O N
NFigure 1: An example demonstrating that Conditional Resampling (also known as “ refinement ”) can
lead to incorrect modification of the original answer. A Selection module can decide to retract the
modification and instead choose the original answer, which in this case is the correct one.
The initial outputs generated by Sampling are passed to Conditional Resampling , which decides
whether to generate a revision conditioned on the initial sample, and does so if needed. Finally,
all samples and revisions are given to the Selection module, which selects the best one. Given the
modular nature of our framework, several recently proposed self-refining methods can be improved
by using other components of the framework. An example is the combination of the self-refinement
method (Madaan et al., 2023) with our model-based selection strategy, which can improve overall
performance; more such strategies are described in section 5.
We evaluate SCREWS on a variety of reasoning tasks: arithmetic reasoning, multi-hop question
answering, and code debugging, using ChatGPT (Brown et al., 2020) or GPT-4 (OpenAI, 2023). Our
proposed strategies achieve substantial improvements (10–15%) over vanilla strategies of sampling
and resampling. We demonstrate the usefulness of heterogeneous resampling, which can help the
model modify its reasoning, leading to a substantial improvement over the baselines at a very low
overall cost. We also discuss the importance of a model-based selection strategy that allows the
model to roll back to its previous more confident outputs, an important component for modern
LLMs.
2 B ACKGROUND
Sampling Prompting LLMs to generate a series of intermediate steps has proven to be effective
for improving their reasoning capabilities (Wei et al., 2022; Lewkowycz et al., 2022; Kojima et al.,
2022; Wang et al., 2022). Some approaches in this direction include Chain of Thought (Wei et al.,
2022; Zhang et al., 2022; Wang et al., 2022) and adding “Let’s think step by step” to the prompt
(Kojima et al., 2022). Another approach is “question decomposition”, which decomposes the main
problem into simpler problems and solves them iteratively (Min et al., 2019; Shridhar et al., 2022;
Zhou et al., 2022; Jhamtani et al., 2023; Radhakrishnan et al., 2023). Each of these approaches has
2

--- PAGE 3 ---
TOOLPROB
LLM
...MOST
RECENTCHAIN
OF 
THOUGHTSELF-
ASK
SELECTION
SELF-SELECTMAJORITY 
VOTINGOTHER
LLMSANS
ONLY
SUB QUES
DECOMPLLM
SAMPLING CONDITIONAL 
RESAMPLING
Self-Refine
Self-Consistency
Self-ImproveLLMs Know (Mostly) Least to Most
PHP CoT
Self-CorrectSocratic CoT
Program of ThoughtsINPUT
OUTPUT
...(x)
(y)
...Figure 2: Overview of our modular framework for reasoning with revisions, SCREWS . Each of the
three large boxes (“modules”) contains several alternatives (“submodules”). A lot of past works can
be viewed as instances of our framework, namely Self-Refine (Madaan et al., 2023), Least to Most
(Zhou et al., 2022), LLMs Know (Mostly) (Kadavath et al., 2022), Self-Consistency (Wang et al.,
2022), Self-Improve (Huang et al., 2022), PHP CoT (Zheng et al., 2023), Self-Correct (Welleck
et al., 2022), Socratic CoT (Shridhar et al., 2022), Program of Thoughts (Chen et al., 2022), among
many others. (...) represents other sub-components that can be added to each module, like cached
memory or web search for Sampling , fine-tuned model or external verifier for Conditional Resam-
pling , and human- or oracle-based selection for the Selection module, among others.
its own advantages depending on the underlying task (Shridhar et al., 2023). However, we are not
aware of work combining these methods.
Conditional Resampling The use of feedback to improve generated samples has been well stud-
ied, where the feedback can come either from humans (Tandon et al., 2021; Bai et al., 2022; Elgo-
hary et al., 2021), from reward models (Ziegler et al., 2019; Lu et al., 2022; Shridhar et al., 2022;
Christiano et al., 2017; Lightman et al., 2023), from external tools such as code interpreters (Schick
et al., 2023; Chen et al., 2022), or from other LLMs (Madaan et al., 2023; Welleck et al., 2022; Fu
et al., 2023; Peng et al., 2023; Yang et al., 2022; Zheng et al., 2023; Cohen et al., 2023; Ling et al.,
2023; Khalifa et al., 2023). However, even if these feedback mechanisms are infallible, the resulting
revisions may introduce new errors.2
2Prior work uses the term “refinement,” which we do not use because refinement implies finer (improved)
responses, which does not always occur.
3

--- PAGE 4 ---
Selection When using LLMs to evaluate and revise the output, the most common selection tech-
nique is to always select the final output (Madaan et al., 2023; Shinn et al., 2023; Zheng et al.,
2023; Yao et al., 2022; Chen et al., 2023b; Weng et al., 2022). However, this can lead to accepting
incorrect changes made to previously correct outputs. Other selection methods involve ranking mul-
tiple sampled outputs (Cobbe et al., 2021) or majority voting (Wang et al., 2022; Lewkowycz et al.,
2022; Zheng et al., 2023). These methods often use a homogeneous sampling strategy with changes
in temperature or other similar hyper-parameters. Our work extends the strategy to heterogeneous
sampling and selection.
3SCREWS : M ETHODOLOGY
In this section, we describe SCREWS , our proposed modular framework for reasoning with revisions
to tackle different reasoning tasks. Given a problem x, the goal is to generate an answer a, which in
our experiments may be a string or a number. SCREWS consists of three main modules: Sampling ,
Conditional Resampling , and Selection . Different variants of SCREWS are obtained by instantiating
these modules in different ways. The options for each module are described below and illustrated
schematically in Figure 2.
All of our methods will invoke one or more stochastic functions, where each function ψmaps a tuple
of input strings to a result stringythat contains useful information. In practice, ψdeterministically
constructs a prompt from the input strings and then samples yfrom a large pretrained language
model as a stochastic continuation of this prompt. For a given tuple of input strings, the prompt
constructed for ψwill typically be a formatted encoding of this tuple, preceded by a task specific
instruction and several demonstrations (few-shot examples) that illustrate how ψshould map other
encoded input tuples to their corresponding continuations (Brown et al., 2020). For concreteness,
the prompts we use in our experiments are illustrated in Appendix B.
3.1 S AMPLING
We consider three instantiations of the sampling module. Different instantiations may be appropriate
for different tasks.
Answer Only In this method, for a given problem x, the model ψdirectly generates the answer
y=ψ(x)without any intermediate steps. This is the simplest and most naive sampling method.
The value of yis returned as the answer a(if there is no further revision of y).
Chain of Thought (CoT) For many reasoning tasks today, generating explanations improves the
quality of the final answer (Wei et al., 2022; Kojima et al., 2022). Chain of Thought sampling
encourages the model to explain the intermediate step-by-step reasoning en route to a decision. This
approach is now commonly used in several reasoning tasks. Again, we define y=ψ(x), but now
we expect the prompt continuation to consist of step-by-step reasoning culminating in the step by
step answer y, as demonstrated by the few-shot examples included in the prompt. The answer ais
extracted from yusing a simple deterministic pattern-matching heuristic.
Sub-question decomposition This method decomposes the problem xinto simpler sub-questions
[x1, x2, . . . , x n]. For each sub-question xiin turn ( i= 1,2, . . . , n ), the model is called to generate
the corresponding sub-answer yi=ψ(x, x1, y1, . . . , x i−1, yi−1, xi). Note that we generate all ques-
tions before seeing any answers; that choice follows Shridhar et al. (2023), who found this approach
to work better than interleaved generation of questions and answers. The sequence of questions
may be generated in a single step, either by a call to a stochastic function ψquestion , or by a custom
question generation module that has been fine-tuned on human-written questions as in Cobbe et al.
(2021). The answer ais extracted from ynwith a simple heuristic as in CoT.
3.2 C ONDITIONAL RESAMPLING
The resultyfrom the Sampling module can be viewed as a provisional result ,ycurr. This is passed
to the Conditional Resampling module where a decision is made whether or not to revise it. This
is done in two steps: first deciding whether or not to revise, and then if so, resampling a new result
4

--- PAGE 5 ---
ynextusing one of the sampling methods mentioned above. The resampling is conditional because
ynextmay depend on ycurr. While there are many methods for Conditional Resampling , our work
focuses on the following instantiations:
Self-Ask Kadavath et al. (2022) uses a function ψask(x,ycurr). The first token of the result indicates
whetherycurris correct, for example by starting with “Yes” or “No”. If “Yes”, we do not resample; if
“No”, we must resample a revised answer ynext. In principle, the revision could be iterated, although
Kadavath et al. (2022) did not do this, nor do our experiments in this paper.
In our version of self-ask, ψaskis formulated so that ynextappears in the result string ψask(x,ycurr)
following the word “No”. Thus, both steps are efficiently performed by a single call to ψask(x,ycurr).
For this method, we always use greedy decoding (temperature 0), which deterministically selects
whichever of “Yes” or “No” is more probable.3Demonstrations for the prompt are shown in Ap-
pendix B.2.
When the sampling module (Section 3.1) used sub-question decomposition to produce a chain of
sub-answers ycurr= [y1, . . . , y n], rather than checking and revising only the final result step ynby
calling ψask(x, yn), we can instead check and revise each step, at the cost of more calls to ψask.
For each provisional sub-answer yiin turn (starting with i= 1), we predict whether it is correct
by calling ψask(x, x1, y1, . . . , x i−1, yi−1, xi, yi). The first time the output is “No”, we resample
y′
ithrough y′
n, yielding the revised result ynext= [y1, . . . , y i−1, y′
i, . . . , y′
n]. In principle, self-ask
could then be applied again at later steps > iof both the original and revised chains; then choosing
among the many resulting chains, using the selection procedures of the next section, would resemble
branching in a reasoning tree (Yao et al., 2023).
Tool-Based LLM For some tasks, we construct ψaskso that it is allowed to use tools (Schick et al.,
2023). The reason is that in tasks like fact-checking, it is futile to ask the LLM to check ycurrbecause
it might not have the requisite knowledge for evaluation. The tools can be used to collect additional
information or facts to help the model detect and fix problems in its own generated answer. Tools
like search engines or fact retrievers can be used to evaluate correctness and generate a new revision.
Some other tools like code interpreters are not capable of generating text, but can still be used to
evaluate correctness.
3.3 S ELECTION
The last module in SCREWS is the Selection module. In this step, we use either a model ψselect or
simple heuristics to select the final resultyfrom which we then extract the final answer a. In effect,
this allows us to construct a simple ensemble of multiple systems.
LLM-Based Selection Just as an LLM was used above to evaluate whether ycurris good, an LLM
can be used to evaluate whether ynextis better. We call ψselect(x,ycurr,ynext)to choose between two
result strings.4Note that it could be naturally extended to choose among more than two answers.
When selection and sampling are implemented using the same LLM, we refer to the method as
self-select (e.g., in Figure 2). The prompts for ψselectin our experiments are shown in Appendix B.3.
Rule-Based Selection We consider the other methods we study to be rule-based. Past work on
iterative refinement (Madaan et al., 2023; Huang et al., 2022; Zheng et al., 2023) always selects the
most recent revision. Majority voting is a simple traditional ensembling method that has been used
for selection (Wang et al., 2022; Lewkowycz et al., 2022), but it is costly because it requires several
samples.
3A threshold other than 50% could be tuned to optimize the downstream reward of the whole system. This
compensates for bias toward the “Yes” or “No” token, and also considers how much resampling followed by
selection will actually improve the final accuracy and harm the speed of the system. Orthogonally, the correct-
ness probability of ycurrcould be assessed by a dedicated ψcheck(x,ycurr), but we were unsuccessful with this as
ψcheckwas poorly calibrated, mirroring findings on model calibration (Kadavath et al., 2022; Xiong et al., 2023).
4We found that the order of ycurrandynextin the prompt was unimportant; in our reported results, we
randomized this order.
5

--- PAGE 6 ---
3.4 O THER POSSIBILITIES
There are other possible ways to instantiate each module. Tools like web-based search or cache-
based retrieval could be used to generate the initial attempt in the Sampling module. A fine-tuned
classification model could be used to verify outputs in the Conditional Resampling module. Simi-
larly, a fine-tuned model could be used for the Selection module. In this paper, however, we study
only the instantiations described above.
4 E XPERIMENTS
4.1 T ASKS
We test the effectiveness and flexibility of SCREWS on three categories of reasoning tasks: GSM8K
(Cobbe et al., 2021) for arithmetic reasoning, StrategyQA (Geva et al., 2021) for multi-hop question
answering, and Big-Bench (BIG-bench authors, 2023) AutoDebugging5for code debugging. The
GSM8K dataset is a grade-school-level math word problem dataset with a test set of 1319 samples,
each requiring two to eight steps to solve. GSM8K includes sub-questions that were generated by
a fine-tuned GPT-3 model and correspond to the steps in a particular correct CoT solution. Since
these sub-questions were generated with oracle knowledge of a correct CoT solution, we refer to
experiments using them as“Subq (Or)”. We use “Subq (QG)” for the fairer experimental condition
where we instead generated the subquestions from ChatGPT using two-shot prompts (which are
provided in Appendix B.4).6
Following Magister et al. (2023) and Shridhar et al. (2023), we test on the first 490 samples from the
training set of StrategyQA (since their test set is unlabeled). The demonstration examples for our
various stochastic functions ψwere drawn randomly from the rest of the training set. StrategyQA
also includes human-annotated oracle subquestions (which we again use for “Subq (Or)” results) and
related facts that can assist in answering the main question (which we use for tool-based conditional
resampling as in Section 3.2). Finally, the Auto Debugging dataset tests whether a model can answer
questions about the intermediate state of a program without executing the code. The dataset consists
of 34 coding examples, of which 33 were used as test examples and 1 as a demonstration example
in the prompt.
4.2 E XPERIMENTAL SETUP
We always report exact-match accuracy: the percentage of examples on which our final answer
amatches the gold answer. For all of our experiments, we use the ChatGPT API (Brown et al.,
2020) from July 2023 ( gpt-3.5-turbo-0301 ). This model is a decoder-only Transformer
LLM (Vaswani et al., 2017) that was fine-tuned using reinforcement learning with human feed-
back (Ziegler et al., 2019; Christiano et al., 2017). Some experiments were also performed using
GPT-4 (OpenAI, 2023) to show the scaling capabilities of our framework.
Sampling With all choices of the Sampling module, we use 5-shot sampling for GSM8K and
StrategyQA and 1-shot sampling for Auto Debugging. Greedy decoding (temp = 0) is used for the
main experiments while higher temperature (0.7) is used for the majority voting experiments (one
sample was generated with temp = 0 and the other four at temp = 0.7). All prompts are provided in
Appendix B.1.
Conditional Resampling Greedy decoding is used to first make a binary resampling decision
and then to sample. 4-shot prompts (with two correct and two incorrect samples) are used for
the GSM8K and StrategyQA datasets, while a 2-shot prompt (with one correct and one incorrect
5https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_
tasks/auto_debugging/
6Unsurprisingly, the Subq (Or) sub-questions proved to be consistently better, as we will see in Section 5. In
addition to their oracle knowledge of a human-written answer, some of the sub-questions themselves may also
have been human-written: the sub-question generation model was fine-tuned on around 800 human-written
examples, and some of those examples may also be included in the released dataset ( https://github.
com/openai/grade-school-math#socratic-dataset ).
6

--- PAGE 7 ---
Sampling Conditional Resampling Accuracy
CoT- 71.64
CoT 73.00
Subq (QG) 73.69
Subq (Or) 73.99
Subq (QG)- 71.87
CoT 73.99
Subq (QG) 71.26
Subq (Or)- 78.62
CoT 78.99
Subq (Or) 78.24
Table 1: The improvements achieved by using Conditional Resampling for the GSM8K dataset,
whereynextis always selected. CoT refers to the Chain of Thought method, while Subq refers
to the Subquestion Decomposition method. Subq (QG) refers to the case where subquestions are
generated by the ChatGPT model, while Subq (Or) refers to the Oracle questions present in the
Socratic version of the dataset.
sample) is used for Auto Debugging. For StrategyQA, we use tool-based resampling by including
the provided facts from the dataset into the prompt (Appendix B.2) to simulate a (perfect) fact
retrieval tool.
Selection For the self-select strategy, the prompts include two examples and selection was pro-
duced with greedy decoding (prompts in Appendix B.3). For majority voting, a majority vote on the
final answers was taken over k∈ {1,3,4,5}samples. Ties were broken randomly.
5 R ESULTS
5.1 GSM8K
Conditional Resampling Works Better with Method Change Previous work (Madaan et al.,
2023) has shown that when a chain-of-thought method is used for initial Sampling , reasoning ability
is improved by Conditional Resampling with the same method. The benefit comes from taking the
previous sample into account.
We reproduced this previous finding: the CoT scores for GSM8K improved by 1.4 points after
resampling with CoT (71.6 to 73.0), as shown in Table 1. However, when the initial Sampling
used subquestion decomposition, we found that resampling with subquestion decomposition ac-
tually harmed accuracy. It decreased the score by about 0.5 points (71.9 to 71.3 with generated
subquestions, 78.6 to 78.2 with oracle subquestions).
What gave the best results—for all three Sampling methods—was Conditional Resampling with
adifferent method from the originally chosen one. It gave a large gain over Sampling when the
original Sampling used CoT and Resampling used subquestion decomposition (71.6 to 73.7, with
generated subquestions) and vice versa (71.9 to 74.0). Even with oracle subquestions, moderate
gains are still seen when resampling with CoT (78.6 to 79.0). This demonstrates that it is useful to
change methods using Conditional Resampling , a novel finding with our framework.7
Importance of Selection Module Conditional Resampling does not invariably improve every out-
put. In fact, we saw in Table 1 that for some settings, it may harm the output quality even on average.
This is why the Selection module is useful—to detect and reject cases of harmful revisions.
First, as a starting point, the left half of Table 2 considers using Selection only as an ensembling
technique to combine the outputs of two independent Sampling strategies. (Note that this matrix
7In principle, we could also use resampling and selection to combine Subq (QG) with Subq (Or); we may
try this in a future version of this paper.
7

--- PAGE 8 ---
Method Independent Sampling Conditional Resampling
CoT Subq (QG) Subq (Or) CoT Subq (QG) Subq (Or)
CoT 71.64 74.90 [85.36] 81.34 [89.08] 72.93 [73.08] 73.76 [73.76] 73.99 [73.99]
Subq (QG) 74.90 [85.36] 71.87 - 73.99 [75.43] 72.40 [72.40] -
Subq (Or) 81.34 [89.08] - 78.62 78.99 [81.50] - 79.22 [79.22]
Table 2: Impact of Selection on the GSM8K data set on Independent Sampling andConditional
Resampling . The upper bound from using a Selection oracle is given in square brackets.
is symmetric.) Although CoT and subquestion decomposition are about equally good Sampling
strategies (71.6 and 71.9), using a Selection module to select the better of the two achieves a 3-point
gain (to 74.9). Much larger gains (up to 85.4) are potentially available from improving Selection—
the upper bound on performance (if Selection always chose the better option) is shown in square
brackets. This shows that the two Sampling strategies have largely complementary errors. A similar
pattern applies when the subquestion decomposition method is permitted to use oracle subquestions,
which improves performance across the board to 81.34.
The right half of Table 2 shows Selection between the Sampled andConditionally Resampled pre-
dictions from Table 1. (This matrix is asymmetric.) For CoT, the results remain the same at 73.99,
which is due to the fact that the upper bound is at 73.99, showing no room for further improvement.
For other cases with subquestioning, we see an improvement of up to 1 point. Finally, we observe
that the Selection module is far from perfect and has room for further improvement, as seen from
the upper bounds. A Selection method ought to look at features of the two answers that turn out to
be correlated with correctness, and we hypothesize that models fine-tuned specifically for Selection
may prove more effective than few-shot learning at identifying these features.
The right half of Table 2 is the cheaper method, because we observe ψaskresamples on only 5-15%
of the examples rather than all of them. A tradeoff between accuracy and cost is shown in Figure 4.
1 2 3 4 5
Number of Samples7072747678808284Accuracy (%)
CoT
Subq(Or)
Subq(QG)
CoT + Subq(Or) + Subq(QG)
(a) Impact of the number of samples on the accu-
racy when majority voting is used as a selection
method.
3 4 5 3 4 5 3
Number of Samples708090100Accuracy (%)CoT + maj
CoT + oracle
Subq(Or) + maj
Subq(Or) + oracle
CoT + Subq(Or) + Subq(QG) + maj
CoT + Subq(Or) + Subq(QG) + oracle(b) Comparison of perfect selection (“+ perfect”) vs. ma-
jority voting (“+ maj”) for different methods of CoT and
Subq.
Figure 3: The + in graph (a) shows that majority voting with 3 diverse samples (CoT + Subq(Or) +
Subq(QG)) outperforms both CoT and Subq(Or) even with 5 samples. Graph (b) shows the potential
of the selection method when a perfect selector is used. It can be thought of as the upper bound of
the selection mechanism. Both figures are for the GSM8K dataset.
Selection and Voting Unweighted majority vote has been one of the most popular Selection meth-
ods in past work (Wang et al., 2022; Lewkowycz et al., 2022; Zheng et al., 2023), since it requires
no training. The two lines in Figure 3(a) generally show improvement from Sampling more times
from the same model (at temperature 0.7) and Selecting by majority vote.
8

--- PAGE 9 ---
Recalling that the left half of Table 2 showed benefit from ensembling independent samples from
2 different Sampling methods (up to 81.34 accuracy when oracle subquestions are allowed), we
observe that majority vote is a convenient way to do so for 3 different methods (where all methods
can now use temperature 0). This achieves 83.62 accuracy, as shown by the ⋆in Figure 3(a). Of
course, model-based Selection could potentially do even better than majority voting. The 7 points for
k≥3in (a) are repeated as the dark bars in Figure 3(b), with the light bars showing the upper bounds
that could be achieved by replacing majority voting with a perfect Selection method. The best upper
bound corresponds again to the use of 3 different methods. In principle, one could ensemble over a
larger set by allowing each of the 3 methods to contribute multiple samples.
5.2 S TRATEGY QA
Vanilla resampling does not improve what model does not know →A need for tools For the
StrategyQA dataset, we observe in Table 3 that accuracy is harmed by Conditional Resampling with
the same Sampling method, without Selection , as was sometimes the case for GSM8K. On Strate-
gyQA, however, even Selection usually does not repair the problem, perhaps because StrategyQA
requires multi-hop question answering. When the model lacks the necessary factual knowledge,
Self-Ask will be insufficient. A real example at the bottom of Figure 5 shows how resampling can
preserve an incorrect claim generated by the model.
To help the model decide whether and how to revise the answer, we try including relevant facts
(provided by StrategyQA) into the resampling prompt, as shown in Appendix B.2.1, to simulate the
result one may get by using an external tool like a fact retriever. As Table 3 shows, this yields a
2-point improvement (“Facts re” vs. “Internal s”) over Sampling , for both CoT and Subq (QG).
We assume that tool invocations are expensive, which is why we include facts only during Condi-
tional Resampling . In practice, the initial result is revised only 10–35% of the time, and therefore
“Facts” does not need to invoke a tool call for every input example.8To achieve this speedup, we do
not include facts in the prompt when initially calling to ψaskto decide whether to resample, but only
when we actually generate ynext.
Method Sampling Conditional Resampling Selection
Knowledge Source: Internal sInternal re Facts re Intsvs. Int re Intsvs. Facts re
StrategyQA
CoT 77.18 74.54 79.02 75.76 78.41
Subq (Or) 85.91 78.97 84.69 85.30 86.30
Subq (QG) 78.16 74.69 80.40 78.78 80.00
Code Debugging
Answer Only 73.52 82.35 88.23 [91.20]
CoT 70.58 73.52 73.52 [73.52]
Answer Only + CoT - - 85.29 [88.23]
Table 3: Comparing different strategies for the StrategyQA (top) and Big Bench Code Debugging
(bottom) datasets. For StrategyQA, external facts are provided to the model (“Facts”) versus relying
on the model’s internal capabilities (“Internal”). The numbers in square brackets indicate upper
bound performance, assuming perfect selection. Subscripts “s” and “re” refer to Sampling and
Resampling respectively.
5.3 C ODE DEBUGGING
The effectiveness of SCREWS For the code debugging task, we observed that the Answer Only
method achieves similar scores to CoT,9as reported in the bottom half of Table 3, suggesting that
no particular Sampling method is superior on all datasets. However, we see the benefits of using
8However, if the facts were included during Sampling , the performance can increase beyond 90%.
9We do not experiment with subquestion decomposition as subquestions are not part of this dataset.
9

--- PAGE 10 ---
SCREWS , as we find that with Answer Only, adding Conditional Resampling followed by Selection
leads to a performance boost of 15 points (from 73.52 to 88.23). While the dataset size limits
our ability to make concrete conclusions, the findings here support the conclusions drawn on other
datasets: Resampling andSelection lead to benefits and heterogenous sampling can prove effective.
6 A DDITIONAL ANALYSIS
6.1 T OTAL COST
1000 2000 3000 4000 5000 6000
Input + Output Tokens7072747678808284Accuracy (%)
CoT
CoT + CoT Resamp
CoT + Subq Resamp
Subq
Subq + CoT Resamp
Subq + Subq Resamp
CoT + Subq -> Sel
Figure 4: On GSM8K, sampling cost vs. accuracy.
The blue line (copied from Figure 3(a)) shows a base-
line of majority voting over k∈ {1,3,4,5}CoT
samples. The shaped points are the other strategies
from Section 5.1 that use CoT and Subq (QG).SCREWS supports many methods with dif-
ferent cost/accuracy tradeoffs. Figure 4 dis-
plays the strategies that use CoT and Subq
(QG) on GSM8K. The cost is represented
as the total count of input tokens (prompt +
query) and output tokens for all LLM calls
needed by that strategy, averaged over test
examples. Generally, Subq (QG) is expen-
sive as it is costly to call ψquestion . How-
ever, it is affordable to use it in Conditional
Resampling only (■), since resampling only
occurs 10–15% of the time. This method is
both cheaper and more accurate than Sam-
pling either with Subq (QG) ( +) or 3 times
with CoT ( •). Appendix A discusses a de-
tailed breakdown of each module’s input and
output token costs.
6.2 M ORE REVISION STEPS
We saw in Section 5.1 on GSM8K that Sampling with Subq (Or) (78.62 accuracy) is improved
slightly by Conditional Resampling with CoT (78.99) and then Selection (79.22). Like Madaan
et al. (2023), we did not find much benefit from additional iterations of Conditional Resam-
pling +Selection : a second iteration gives 79.45, and a third gives 79.52. These small improvements
probably do not justify the added cost.
6.3 L ARGER LLM S
Replacing ChatGPT with GPT-4 greatly increased the Sampling accuracy on GSM8K, to 91.45 for
CoT and 90.80 for Subq (Or). Choosing between those two samples with GPT-4-based Selection
further increased the accuracy to 93.10, which falls between the accuracy of majority voting over
k= 3 andk= 4 CoT samples from GPT-4 (92.94 and 93.93 respectively). Even using ChatGPT-
based Selection achieved 92.58, which is still an improvement over CoT alone.
6.4 S ELECTED EXAMPLES
The top two examples of Figure 5, on the GSM8K dataset, demonstrate the effectiveness of the
Selection module. The first example shows how an error introduced by Conditional Resampling
can be reverted by Selection . The second example shows how a correction found by Conditional
Resampling can be kept by Selection .
The last example in Figure 5, on the StrategyQA dataset, illustrates that ordinary Resampling is
unlikely to correct an incorrect fact generated by the LLM. However, providing the correct facts
during Resampling gives the model access to new information, leading to the correct answer.
10

--- PAGE 11 ---
Question Gunter is trying to count the jelly beans in a jar. He asks his friends how many they think are in the jar. One says 80. Another
says 20 more than half the first one. A third says 25% more than the first one. What is their average guess?
Sampling (A) The second person guesses 20 + 1 /2∗80 =<<20 + 1 /2∗80 = 60>>60jelly beans. The third person guesses
80∗1.25 =<<80∗1.25 = 100>>100 jelly beans. The total number of jelly beans is 80 + 60 + 100 = <<80 + 60 + 100 =
240>>240. The average guess is 240/3 =<<240/3 = 80>>80. The answer is 80. ✓
Conditional Resampling (B) The average guess is (80+60+1 .25∗80)/3 =<<(80+60+1 .25∗80)/3 = 85>>85jelly beans.
The second person guesses 80/2 + 20 = <<80/2 + 20 = 60 >>60jelly beans. The average guess is (80 + 60 + 1 .25∗80)/3 =
<<(80 + 60 + 1 .25∗80)/3 = 85>>85jelly beans. The answer is 85. ✗
Selection (A)✓
Question Adam has $100 and wants to spend it to open a rock stand. He can buy rocks for $5 each and sell them for $7 each. If he
invests all his money in the rock stand but only sells 60% of his inventory, how much money does he lose?
Sampling (A) Adam can buy $100/$5 per rock = <<100/5 = 20>>20rocks. Adam sells 60/100 x 20 rocks = <<60/100∗20 =
12>>12rocks. Adam makes $7 x 12 rocks = $<<7∗12 = 84>>84. Adam loses $100 - $5 x 20 rocks = $<<100−5∗20 = 0>>0.
He invested all his money and sold all the rocks he bought, so he didn’t lose any money. The answer is 0. ✗
Conditional Resampling (B) Adam can buy $100/$5 per rock = <<100/5 = 20>>20rocks. Adam sells 60/100 x 20 rocks =
<<60/100∗20 = 12>>12rocks. Adam makes $7 x 12 rocks = $<<7∗12 = 84>>84. However, he spent $5 x 20 rocks =
$<<5∗20 = 100>>100, so he loses $100 - $84 = $<<100−84 = 16>>16. The answer is 16. ✓
Selection (B)✓
Question : Would Alexander Hamilton have known about koalas?
Sample : Alexander Hamilton died in 1804. The first koala specimen was collected in 1792. It is possible that Hamilton knew about
koalas, but it is unlikely. The answer is Unknown ✗
Conditional Resampling : Alexander Hamilton died in 1804. The first koala specimen was collected in 1792. It is possible that
Hamilton knew about koalas. The answer is True ✗
Facts :Alexander Hamilton died in 1804. The first published depiction of a koala was in 1810
Tool-based Resample :No,Alexan derHamil tonwould nothave known about koalas. Theanswer isFalse✓
Figure 5: The top two examples demonstrate the importance of the Selection module for the GSM8K
dataset. The last example shows how tool use (“Facts”) can be helpful for the StrategyQA dataset.
7 D ISCUSSION
7.1 K EYFINDINGS
Based on our experiments with three reasoning datasets using our framework, we conclude the
following:
•Selection plays an important role : Although Conditional Resampling often improves the
result of Sampling ,Selection can help avoid errors from the case where it does not. It was
beneficial on all three datasets.
•Heterogeneous vs. homogeneous resampling : Using different reasoning methods for
Sampling andConditional Resampling can lead to higher accuracy, with or without Se-
lection .
•Missing external knowledge hurts Conditional Resampling : Resampling cannot fix in-
correct facts generated by the model. Tool-based resampling can therefore get better results
(as simulated using StrategyQA).
•No uniformly best strategy : There was no clear winning method for each of the modules.
Simple baseline methods sometimes beat more complex ones: CoT uses only one call to
ψand beats Subq (QG) in GSM8K, always selecting ynextbeats self-select for StrategyQA
with “Facts,” and Answer Only works surprisingly well for Code Debugging.
7.2 F UTURE WORK
SCREWS combines the three important modules Sampling ,Conditional Resampling andSelection in
a modular framework. The best configuration of modules will vary by task and could be identified
through a method such as exhaustive search, Monte Carlo Tree Search, or reinforcement learning.
The modules themselves could be fine-tuned to improve end-to-end performance.
11

--- PAGE 12 ---
If we want to optimize cost along with accuracy, (Chen et al., 2023a) proposed several methods for
speeding up the stochastic functions ψ. Their “LLM Cascade” strategy in particular is a heteroge-
neous (but unconditional) resampling method that starts with smaller, cheaper models.
It is possible that for some reasoning tasks, additional modules could be useful. For instance, Re-
sampling orSelection might be preceded by Critiquing , orSelection might be generalized to Com-
bination .
7.3 C ONCLUSION
We have proposed SCREWS , a modular reasoning-with-revisions framework to answer reason-
ing questions with LLMs. We demonstrated the usefulness of the three main components of the
framework— Sampling ,Conditional Resampling , and Selection —on three reasoning datasets. The
flexible nature of our framework allows it to be configured for each task and extended to other tasks
in the future.
12

--- PAGE 13 ---
REFERENCES
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, T. J. Henighan, Nicholas Joseph, Saurav Kadavath, John
Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez,
Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario
Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and
Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human
feedback. ArXiv , abs/2204.05862, 2022.
BIG-bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of
language models. Transactions on Machine Learning Research , 2023. ISSN 2835-8856.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,
R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems ,
volume 33, pp. 1877–1901. Curran Associates, Inc., 2020.
Lingjiao Chen, Matei A. Zaharia, and James Y . Zou. FrugalGPT: How to use large language models
while reducing cost and improving performance. ArXiv , abs/2305.05176, 2023a.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:
Disentangling computation from reasoning for numerical reasoning tasks. ArXiv , abs/2211.12588,
2022.
Xinyun Chen, Maxwell Lin, Nathanael Sch ¨arli, and Denny Zhou. Teaching large language models
to self-debug. ArXiv , abs/2304.05128, 2023b.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wal-
lach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Process-
ing Systems , volume 30. Curran Associates, Inc., 2017.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. ArXiv , abs/2110.14168, 2021.
Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. LM vs LM: Detecting factual errors via
cross examination. ArXiv , abs/2305.13281, 2023.
Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and
Ahmed Hassan Awadallah. NL-EDIT: Correcting semantic parse errors through natural language
interaction. In Proceedings of the 2021 Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language Technologies , pp. 5599–5610, Online,
June 2021. Association for Computational Linguistics.
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. GPTScore: Evaluate as you desire.
ArXiv , abs/2302.04166, 2023.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle
use a laptop? A question answering benchmark with implicit reasoning strategies. Transactions
of the Association for Computational Linguistics , 9:346–361, 2021.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei
Han. Large language models can self-improve. ArXiv , abs/2210.11610, 2022.
Harsh Jhamtani, Hao Fang, Patrick Xia, Eran Levy, Jacob Andreas, and Benjamin Van Durme. Nat-
ural language decomposition and interpretation of complex utterances. ArXiv , abs/2305.08677,
2023.
13

--- PAGE 14 ---
Saurav Kadavath, Tom Conerly, Amanda Askell, T. J. Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zachary Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk,
Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav
Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, John Kernion, Shauna Kravec, Liane
Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom B. Brown, Jack
Clark, Nicholas Joseph, Benjamin Mann, Sam McCandlish, Christopher Olah, and Jared Kaplan.
Language models (mostly) know what they know. ArXiv , abs/2207.05221, 2022.
Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Ho Hin Lee, and Lu Wang.
Discriminator-guided multi-step reasoning with language models. ArXiv , abs/2305.14934, 2023.
Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,
K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems , volume 35, pp.
22199–22213. Curran Associates, Inc., 2022.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,
Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo,
Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning
problems with language models. ArXiv , abs/2206.14858, 2022.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee,
Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. ArXiv ,
abs/2305.20050, 2023.
Z. Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su.
Deductive verification of Chain-of-Thought reasoning. ArXiv , abs/2306.03872, 2023.
Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Am-
manabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning.
ArXiv , abs/2205.13636, 2022.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Ma-
jumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-Refine: Iterative refinement
with self-feedback. ArXiv , abs/2303.17651, 2023.
Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn.
Teaching small language models to reason. In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers) , pp. 1773–1781, Toronto,
Canada, July 2023. Association for Computational Linguistics.
Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. Multi-hop reading com-
prehension through question decomposition and rescoring. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics , pp. 6097–6109, Florence, Italy, July
2019. Association for Computational Linguistics.
OpenAI. GPT-4 technical report. ArXiv , abs/2303.08774, 2023.
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars
Lid´en, Zhou Yu, Weizhu Chen, and Jianfeng Gao. Check your facts and try again: Improving
large language models with external knowledge and automated feedback. ArXiv , abs/2302.12813,
2023.
Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson E. Denison, Danny Hernan-
dez, Esin Durmus, Evan Hubinger, John Kernion, Kamil.e Lukovsiut.e, Newton Cheng, Nicholas
Joseph, Nicholas Schiefer, Oliver Rausch, Sam McCandlish, Sheer El Showk, Tamera Lanham,
Tim Maxwell, Venkat Chandrasekaran, Zac Hatfield-Dodds, Jared Kaplan, Janina Brauner, Sam
Bowman, and Ethan Perez. Question decomposition improves the faithfulness of model-generated
reasoning. ArXiv , abs/2307.11768, 2023.
Timo Schick, Jane Dwivedi-Yu, Roberto Dess `ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,
Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to
use tools. ArXiv , abs/2302.04761, 2023.
14

--- PAGE 15 ---
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu
Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.
Kumar Shridhar, Jakub Macina, Mennatallah El-Assady, Tanmay Sinha, Manu Kapur, and Mrin-
maya Sachan. Automatic generation of socratic subquestions for teaching math word problems.
InProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing ,
pp. 4136–4149, Abu Dhabi, United Arab Emirates, December 2022. Association for Computa-
tional Linguistics.
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. Distilling reasoning capabilities into
smaller language models. In Findings of the Association for Computational Linguistics: ACL
2023 , pp. 7059–7073, Toronto, Canada, July 2023. Association for Computational Linguistics.
Niket Tandon, Aman Madaan, Peter Clark, Keisuke Sakaguchi, and Yiming Yang. Interscript: A
dataset for interactive learning of scripts through error feedback. ArXiv , abs/2112.07867, 2021.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
Information Processing Systems , volume 30. Curran Associates, Inc., 2017.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. Self-
Consistency improves chain of thought reasoning in language models. ArXiv , abs/2203.11171,
2022.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V .
Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural
Information Processing Systems , volume 35, pp. 24824–24837. Curran Associates, Inc., 2022.
Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin
Choi. Generating sequences by learning to self-correct. ArXiv , abs/2211.00053, 2022.
Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. Large language
models are better reasoners with self-verification. volume abs/2212.09561, 2022.
Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs
express their uncertainty? an empirical evaluation of confidence elicitation in llms. ArXiv ,
abs/2306.13063, 2023.
Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with
recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pp. 4393–4479, Abu Dhabi, United Arab Emirates, December
2022. Association for Computational Linguistics.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
ReAct: Synergizing reasoning and acting in language models. ArXiv , abs/2210.03629, 2022.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv ,
abs/2305.10601, 2023.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alexander J. Smola. Automatic chain of thought
prompting in large language models. ArXiv , abs/2210.03493, 2022.
Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-Hint Prompting
improves reasoning in large language models. ArXiv , abs/2304.09797, 2023.
Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schu-
urmans, Olivier Bousquet, Quoc Le, and Ed Huai hsin Chi. Least-to-most prompting enables
complex reasoning in large language models. ArXiv , abs/2205.10625, 2022.
Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul
Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv ,
abs/1909.08593, 2019.
15

--- PAGE 16 ---
A T OKEN COST
Table A shows the token cost of input and output for each module in SCREWS . Due to its iterative
nature, subquestion decomposition requires on average four times more input tokens than the other
modules. For Conditional Resampling , the model first predicts whether it wants to modify its output
or not, using one token (“Yes” or “No”) for each sample and then only for the answers starting with
“No”, it resamples. For the Selection module, the model chooses one of the two samples presented
to it, using one token ( AorB) for the output.
Method Input Tokens Output Tokens Total Tokens
Subquestion generation step ψquestion
Subq (QG) 360 180 540
Sampling step ψ
CoT 774 307 1081
CoT ( k= 5) 3870 1530 5400
Subq (Or) 3187 413 3600
Subq (QG) 3121 434 3555
Conditional Resampling step ψask
CoT 869 105 1184
Subq (Or) 3525 131 3656
Subq (QG) 3780 136 3916
Selection step ψselect
Selection 1296 1 1297
Table 4: Average counts of input and output tokens for each choice of each module (step) in
SCREWS . Many of the methods in Table 1 need to call multiple modules. We remark that the input
tokens at each step include output tokens from previous steps. The counts shown for later steps
average not only over examples, but also over choices of method for the previous steps.
B P ROMPTS
Below are abbreviated versions of the prompts used in the experiments, including instructions and
demonstrations. For readability, we show only 1–2 demonstrations in each prompt. In each demon-
stration, the demonstrated result string is highlighted for the reader’s convenience, but this highlight-
ing is not included in the prompt. Each prompt shown would be followed by the test question and
then the cue (e.g., “Answer:”) that indicates that a result string should follow.
B.1 S AMPLING
For Chain of Thought (CoT) and Subquestion Decomposition for GSM8K and StrategyQA, 5-shot
prompts were used. For Auto Debugging, a 1-shot prompt was used.
B.1.1 C HAIN OF THOUGHT
GSM8K
I am a highly intelligent question answering bot. I will answer the last question ‘Question’ providing equation
in<<>> format in step by step manner.
Question: James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a
year?
Answer: He writes each friend 3∗2 =<<3∗2 = 6>>6pages aweek. So he writes
6∗2 =<<6∗2 = 12>>12pages everyweek. That means hewrites 12∗52 =<<12∗52 = 624>>624
pages ayear. Theanswer is624
16

--- PAGE 17 ---
StrategyQA
You are a highly intelligent question answering bot. You will answer the question ‘Question’ in as details as
possible.
Question: Is coal needed to practice parachuting?
Answer: Parachut ingrequires aparachute. Parachutes aremade from nylon.Nylonismade from coal. The
answer isTrue
Auto Debugging
Answer the ’Question’ based on the provided code and provide explanation.
Question:
def f1():
return str(x) + 'hello'
def f2():
return f1(2 *x)
x = f2(524)
What is the value of x at the end of this program?
Output: First, x=2*524=1048 andthen ‘hello’ isappended toit.Soxbecomes1048hello
B.1.2 S UBQUESTION DECOMPOSITION
While subquestion decomposition uses a single prompt, each example requires multiple API calls
because the next subquestion needs to be appended to the prompt.
GSM8K
I am a highly intelligent question answering bot. I will answer the last question ‘Q’ providing equation in <<
>>format keeping the Problem and previous Q and A into account.
Problem: There are 5 houses on a street, and each of the first four houses has 3 gnomes in the garden. If there
are a total of 20 gnomes on the street, how many gnomes does the fifth house have?
Q: How many gnomes are in the first four houses?
A:Inthefirstfour houses, there areatotalof4houses *3gnomes =<<4∗3 = 12>>12gnomes. Theanswer
is12
Q: How many gnomes does the fifth house have?
A:There fore, thefifth house had20totalgnomes -12gnomes =<<20−12 = 8>>8gnomes. Theanswer is8
StrategyQA
You are a highly intelligent question answering bot. You will answer the last question ‘Q’ keeping the Problem
and previous Q and A into account and then answer the Final Question based on all the previous answer ‘A’.
Problem: Is coal needed to practice parachuting?
Q: What is one of the most important item that you need to go parachuting?
A:Parachut ingrequires aparachute.
Q: What is #1 made out of?
A:Parachutes aremade from nylon.
Q: Is #2 originally made from coal?
A:Nylonismade from coal.
Final Question: Is coal needed to practice parachuting?
Final Answer: True
B.1.3 A NSWER ONLY
Answer Only was only used for Auto Debugging in a 1-shot manner and that one example is pro-
vided below:
Input:
def f1():
return str(x) + 'hello'
17

--- PAGE 18 ---
def f2():
return f1(2 *x)
x = f2(524)
What is the value of x at the end of this program?
Output:1048hello
B.2 C ONDITIONAL RESAMPLING
We used 4-shot prompting for conditional resampling (two positive and two negative) for GSM8K
and StrategyQA and 2-shot (one positive and one negative) for Auto Debugging. One pair of each
is presented below:
B.2.1 C HAIN OF THOUGHT
GSM8K
Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How
many clips did Natalia sell altogether in April and May?
Answer: In April, Natalia sold 48 clips. In May, Natalia sold half of the toal clips meaning 72/2<<72/2 =
36>>36clips. So, Natalia sold a total of 48 + 36<<48 + 36 = 84 >>84clips in April and May. The answer is
84
You are a math teacher. Do you think the reasoning process for the given problem is correct? Let’s check the
‘Answer’ in details, and then decide ‘Yes’ or ‘No’ and then write the correct ‘Final Answer’.
Answer: No,thereasoningisincorrect. FinalAnswer: Natalia sold 48/2 =<<48/2 = 24>>24clips inMay.
Soshesold atotalof48 + 24<<48 + 24 = 72 >>72clips. Theanswer is72
Question: There are 5 houses on a street, and each of the first four houses has 3 gnomes in the garden. If there
are a total of 20 gnomes on the street, how many gnomes does the fifth house have?
Answer:In the first four houses, there are a total of 4 houses * 3 gnomes =<<4∗3 = 12>>12gnomes.
Therefore, the fifth house had 20 total gnomes - 12 gnomes =<<20−12 = 8>>8gnomes. The answer is 8
You are a math teacher. Do you think the reasoning process for the given problem is correct? Let’s check the
‘Answer’ in details, and then decide ‘Yes’ or ‘No’ and then write the correct ‘Final Answer’.
Answer: Yes, thereasoningiscorrect. FinalAnswer: Inthefirst four houses, there areatotalof4houses
*3gnomes =<<4∗3 = 12>>12gnomes. There fore, thefifth house had 20totalgnomes -12gnomes
=<<20−12 = 8>>8gnomes. Theanswer is8
StrategyQA
Question: Could Durian cause someone’s stomach to feel unwell?
Answer: Durian has a pungent odor that many people describe as being similar to feet and onions. Unpleasant
smells can make people feel excited and they like it. The answer is False
You are an expert teacher. Do you think the reasoning process for the given problem is correct? Let’s check
the ‘Answer’ in details, and then decide ‘Yes’ or ‘No’ and then write the correct ‘Final Answer’.
Answer: No,thereasoningisincorrect. FinalAnswer: Durian hasapungent odor thatmany peopledescribe
asbeingsimilartofeetandonions. Unpleas antsmells canmake peoplefeelnauseous. Theanswer isTrue
Question: Was Daniel thrown into the lion’s den in the New Testament?
Answer:The Book of Daniel is a book in the Old Testament of the Bible. The Bible is divided into the Old
Testament and the New Testament. The New Testament focuses on four Gospels regarding the life of Jesus.
The answer is False
You are an expert teacher. Do you think the reasoning process for the given problem is correct? Let’s check
the ‘Answer’ in details, and then decide ‘Yes’ or ‘No’ and then write the correct ‘Final Answer’.
Answer: Yes, thereasoningiscorrect. FinalAnswer: TheBook ofDaniel isabook intheOldTestament of
theBible. TheBible isdivided intotheOldTestament andtheNew Testament. TheNew Testament focuses on
four Gospels regardingthelifeofJesus.Theanswer isFalse
StrategyQA (Resampling with facts)
You are a highly intelligent question answering bot. You will answer the question ’Question’ in as details as
possible. ’Facts’ are provided to assist you in answering the questions.
Question: Are vinegar pickled cucumbers rich in lactobacillus?
Facts: Pickles made with vinegar are not probiotic and are simply preserved. Pickles made through a soak in a
18

--- PAGE 19 ---
salt brine solution begin to ferment because of lactobacillus.
Answer: No,vinegardoes notcontainlactobacil lus.Theanswer isFalse
Question: Does Masaharu Morimoto rely on glutamic acid?
Facts: Masaharu Morimoto is a Japanese chef. Japanese cuisine relies on several forms of seaweed as
ingredients and flavorings for broth like kombu dashi. Glutamic acid has been identified as the flavoring
component in kombu seaweed.
Answer: Yes, Japanese chef uses alotofglutamic acid. Theanswer isTrue
Auto Debugging
Input:
def f1():
return str(x) + 'hello'
def f2():
return f1(2 *x)
x = f2(524)
What is the value of x at the end of this program?
Output:1048hello
Verdict: Yes, theanswer iscorrect.
FinalAnswer:1048hello
Input:
def f1():
return str(x) + 'hello'
def f2():
return f1(2 *x)
x = f2(524)
What is the value of x at the end of this program?
Output:524
Verdict: No,theanswer isincorrect.
FinalAnswer:1048hello
B.2.2 S UBQUESTION DECOMPOSITION
GSM8K
For each subquestion, the main problem and all previous subquestions along with the model-generated
solutions are provided in order to solve the current subquestion.
Here is a math question and its solution.
Problem: Noah is a painter. He paints pictures and sells them at the park. He charges $60 for a large painting
and $30 for a small painting. Last month he sold eight large paintings and four small paintings. If he sold twice
as much this month, how much is his sales for this month?
How much did Noah earn from the large paintings? Noah earned $60/large painting x 8 large paintings =
$<<60∗8 = 480>>480for the large paintings. The answer is 480
Question: How much did Noah earn from the small paintings?
Answer: He also earned $60/small painting x 4 small paintings = $<<60∗4 = 240>>240 for the small
paintings. The answer is 240
You are a math teacher. Do you think the reasoning process for the given problem is correct? Let’s check the
‘Answer’ in details, and then decide ‘Yes’ or ‘No’ and then write the correct ‘Final Answer’.
Answer: No,thereasoningisincorrect. FinalAnswer: Healsoearned $30/small paint ingx4small paint ings =
$<<30∗4 = 120>>120forthesmall paint ings. Theanswer is120
Here is a math question and its solution.
Problem: To make pizza, together with other ingredients, Kimber needs 10 cups of water, 16 cups of flour, and
1/2 times as many teaspoons of salt as the number of cups of flour. Calculate the combined total number of
cups of water, flour, and teaspoons of salt that she needs to make the pizza.
How many teaspoons of salt does Kimber need? To make the pizza, Kimber half as many teaspoons of salt as
19

--- PAGE 20 ---
the number of cups of flour, meaning she needs 1/2*16 = <<16∗1/2 = 8>>8teaspoons of salt. The answer is
8
How many cups of flour and teaspoons of salt does Kimber need? The total number of cups of flour and
teaspoons of salt she needs is 8+16 = <<8 + 16 = 24 >>24. The answer is 24
Question: How many cups of water, flour, and salt does Kimber need?
Answer: She also needs 10 cups of water, which means the total number of cups of water and flour and
teaspoons of salt she needs is 24 + 10 = <<24 + 10 = 34 >>34. The answer is 34
You are a math teacher. Do you think the reasoning process for the given problem is correct? Let’s check the
‘Answer’ in details, and then decide ‘Yes’ or ‘No’ and then write the correct ‘Final Answer’.
Answer: Yes, thereasoningiscorrect. FinalAnswer: Shealso needs 10cups ofwater,which means thetotal
numberofcups ofwaterandflour andteaspoons ofsaltsheneeds is24 + 10 = <<24 + 10 = 34 >>>34.
Theanswer is34
StrategyQA
Here is a question and its answer.
Context: Would a diet of ice eventually kill a person?
Ice is the solid state of what? Ice can be melted into water, which consists of hydrogen and oxygen.
What nutrients are needed to sustain human life? Humans need carbohydrates, proteins, and fats that are
contained in foods.
Question: Are most of #2 absent from #1?
Answer: Water does not contain fat, carbohydrates or protein.
You are an expert teacher. Based on the provided context, do you think the reasoning process for the given
problem is correct? Let’s check the ‘Answer’ in details, and then decide ‘Yes’ or ‘No’ and then write the
correct ‘Final Answer’.
Answer: Yes, thereasoningiscorrect. FinalAnswer: Waterdoes notcontainfat,carbohydrates orprotein.
Here is a question and its answer.
Context: Can binary numbers and standard alphabet satisfy criteria for a strong password?
Which characters make up binary numbers? Binary numbers only contain 0 and 1.
Which characters make up the standard English alphabet? The standard alphabet contains twenty six letters
but no special characters.
Question: Does #1 or #2 include special characters or symbols?
Answer: Yes, it contains all the special characters.
You are an expert teacher. Based on the provided context, do you think the reasoning process for the given
problem is correct? Let’s check the ‘Answer’ in details, and then decide ‘Yes’ or ‘No’ and then write the
correct ‘Final Answer’.
Answer: No,thereasoningisincorrect. FinalAnswer: Neitherbinary digitsnorEnglish alphabets consists of
anyspecialcharacterswhich isneeded forastrong password.
B.3 S ELECTION
The LLM-based selection module ψselect uses a 2-shot prompt. The 2 demonstrations in the prompt
are shown below, for each dataset.
GSM8K
You are an expert math teacher. You are provided with a question and two answers. Lets check the ‘Answer
choices’ step by step, and then decide which answer is correct ‘(A)’ or ‘(B)’
Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How
many clips did Natalia sell altogether in April and May?
Answer choices:
(A) In April, Natalia sold 48 clips. In May, Natalia sold 24 clips. So, Natalia sold a total of 72 clips in
April and May. The answer is 72. So in May she sold 48 clips. Total clips sold in April and May =
72 + 48 = <<72 + 48 = 120 >>120. The answer is 120
(B) Natalia sold 48/2 =<<48/2 = 24>>24clips in May. The answer is 24. Natalia sold
48 + 24 = <<48 + 24 = 72 >>clips altogether. The answer is 72
Answer: ( B)
You are an expert math teacher. You are provided with a question and two answers. Lets check the ‘Answer
choices’ step by step, and then decide which answer is correct ‘(A)’ or ‘(B)’
20

--- PAGE 21 ---
Question: Dolly has two books. Pandora has one. If both Dolly and Pandora read each others’ books as well
as their own, how many books will they collectively read by the end?
Answer choices:
(A) There are a total of 2 + 1 =<<2 + 1 = 3>>3books. The answer is 3. Dolly and Pandora both read all 3
books, so 3 books/person x 2 people = <<3∗2 = 6>>6books total. The answer is 6
(B) The total number of books are 2∗1 =<<2∗1 = 2>>2books. The answer is 2. Dolly and Pandora read
each other’s books as well as their own, so the total number of books they read is 3 books. The answer is 3
Answer: ( A)
StrategyQA
You are the expert in the field. You are provided with a question and two answers. Lets check the reasoning
process of each of the answer step by step, and then decide which answer is correct ‘(A)’ or ‘(B)’
Question: Could Durian cause someone’s stomach to feel unwell?
Answer choices:
(A) Durian has a pungent odor that many people describe as being similar to feet and onions. Unpleasant
smells can make people feel nauseous. The answer is True
(B) Durian has a pungent odor that many people describe as being similar to feet and onions. Unpleasant
smells can make people feel excited and they like it. The answer is False
Answer: ( A)
You are the expert in the field. You are provided with a question and two answers. Lets check the reasoning
process of each of the answer step by step, and then decide which answer is correct ‘(A)’ or ‘(B)’
Question: Was Daniel thrown into the lion’s den in the New Testament?
Answer choices:
(A) The Book of Daniel is a book in the New Testament of the Bible. The Bible is divided into the Old
Testament and the New Testament. The New Testament focuses on the life of Daniel. The answer is True
(B) The Book of Daniel is a book in the Old Testament of the Bible. The Bible is divided into the Old
Testament and the New Testament. The New Testament focuses on four Gospels regarding the life of Jesus.
The answer is False
Answer: ( B)
Auto Debugging
You are an expert Python debugger. You are provided with a question and two answers. Your job is to decide
which answer is correct ‘(A)’ or ‘(B)’
Question:
def f1():
return str(x) + 'hello'
def f2():
return f1(2 *x)
x = f2(524)
What is the value of x at the end of this program?
Answer choices:
(A)524hello
(B)1048hello
Answer: ( B)
B.4 Q UESTION GENERATION
5-shot prompts were used for generating subquestions for GSM8K dataset. An example is provided
below:
GSM8K
I am a highly intelligent question generation bot. I will take the given question ‘Q’ and will decompose the
main question into all ‘subquestions’ required to solve the question step by step.
21

--- PAGE 22 ---
Q: James writes a 3-page letter to 2 different friends twice a week. How many pages does he write a year?
Subquestions: How many pages does hewrite each week? How many pages does hewrite everyweek? How
many pages does hewrite ayear?
StrategyQA
I am a highly intelligent question generation bot. I will take the given question ‘Q’ and will decompose the
main question into all ‘subquestions’ required to solve the question step by step.
Q: Can you buy Casio products at Petco?
Subquestions: What kind ofproducts does Casiomanufacture? What kind ofproducts does Petco sell? Does
#1overlapwith #2?
22
