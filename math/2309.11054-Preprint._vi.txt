# 2309.11054.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2309.11054.pdf
# Kích thước tệp: 1007372 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Preprint.
THIẾT KẾ CHAIN-OF-THOUGHT TRONG GIẢI QUYẾT BÀI TOÁN
Zhanming Jie∗, Trung Quoc Luong∗, Xinbo Zhang∗, Xiaoran Jin, Hang Li
ByteDance Research
{allan, trung.luong, zhangxinbo.freya }@bytedance.com
{xiaoran.jin, lihang.lh }@bytedance.com
TÓM TẮT
Chain-of-Thought (CoT) đóng vai trò quan trọng trong lý luận để giải quyết bài toán. Chúng tôi tiến hành khảo sát toàn diện các phương pháp thiết kế CoT, so sánh CoT ngôn ngữ tự nhiên thông thường với các program CoT khác nhau, bao gồm self-describing program, comment-describing program, và non-describing program. Hơn nữa, chúng tôi nghiên cứu tác động của ngôn ngữ lập trình đến program CoT, so sánh Python và Wolfram Language. Thông qua các thí nghiệm rộng rãi trên GSM8K, MATHQA, và SVAMP, chúng tôi phát hiện rằng program CoT thường có hiệu quả vượt trội trong giải quyết bài toán. Đặc biệt, kết hợp hoạt động tốt nhất với 30B tham số vượt trội GPT-3.5-turbo một cách đáng kể. Kết quả cho thấy self-describing program mang lại sự đa dạng lớn hơn và do đó có thể đạt được hiệu suất cao hơn nói chung. Chúng tôi cũng phát hiện rằng Python là lựa chọn ngôn ngữ tốt hơn Wolfram cho program CoT. Kết quả thí nghiệm cung cấp hướng dẫn có giá trị cho thiết kế CoT trong tương lai có tính đến cả ngôn ngữ lập trình và phong cách mã hóa để tiến bộ thêm. Dữ liệu và mã nguồn của chúng tôi được công bố công khai¹.

1 GIỚI THIỆU
Giải quyết bài toán là nhiệm vụ lý tưởng để đánh giá khả năng lý luận đa bước của các mô hình ngôn ngữ lớn (LLM). LLM thể hiện khả năng lý luận đáng chú ý với việc sử dụng chains-of-thought, vượt trội hơn các phương pháp trước đó trong nhiều tác vụ lý luận khác nhau (Lightman et al., 2023; Wei et al., 2022b; Touvron et al., 2023a). Tuy nhiên, thử thách tạo ra chains-of-thought (CoT) đáng tin cậy (Wei et al., 2022b) vẫn còn tồn tại, đặc biệt là trong các trường hợp phức tạp và tinh tế của giải quyết bài toán (Golovneva et al., 2022). Nghiên cứu gần đây đã tập trung vào cải tiến các chiến lược kỹ thuật prompt hoặc phát triển các biểu diễn CoT mới, chẳng hạn như program CoT (Gao et al., 2023; He-Yueya et al., 2023).

Mặc dù các cách tiếp cận hiện có có thể tăng cường hiệu suất tổng thể (Lu et al., 2023), một so sánh toàn diện các CoT khác nhau vẫn chưa có trong tài liệu. Trong bài báo này, chúng tôi tiến hành khảo sát toàn diện nhiều thiết kế CoT, bao gồm ngôn ngữ tự nhiên (NL) và program CoT, chẳng hạn như self-describing program, comment-describing program, và non-describing program. Hình 1 minh họa các biểu diễn CoT khác nhau để giải quyết bài toán trắc nghiệm.

Đối với program CoT, ngoài ngôn ngữ lập trình Python phổ biến, chúng tôi cũng sử dụng Wolfram Language (Wolfram, 2015), một ngôn ngữ lập trình khoa học được biết đến với khả năng biểu đạt tự nhiên các biểu thức toán học phức tạp.

Một lợi thế của program CoT là tính hợp lệ của chúng có thể dễ dàng được xác minh bằng cách thực thi các chương trình. Ví dụ, chúng ta có thể dễ dàng biểu diễn một phương trình trong một dòng (ví dụ: Hình 1 Wolfram dòng 1) và giải nó với các hàm tích hợp sẵn (tức là Solve[]). NL CoT không có khả năng này, nhưng chúng có thể dịch tốt hơn các câu hỏi trong ngôn ngữ thành mô tả lý luận bằng cách tận dụng sức mạnh của LLM. Chúng tôi xem xét ba loại program CoT. Self-describing program (SDP) tương tự như PAL (Gao et al., 2023) trong đó tên biến được trích xuất từ các câu hỏi.

∗Đóng góp ngang nhau, thứ tự quyết định bởi chữ cái đầu của họ.
¹https://github.com/lqtrung1998/mwp_cot_design

--- TRANG 2 ---
Preprint.
Hãy bắt đầu bằng cách gán biến cho tuổi của A, B, và C. Đặt A = tuổi của A. Đặt B = tuổi của B. Đặt C = tuổi của C. Từ bài toán, chúng ta biết rằng: A = B + 2 (A lớn hơn B hai tuổi) B = 2C (B gấp đôi tuổi C) A + B + C = 27 (Tổng tuổi của họ là 27) Bây giờ chúng ta có thể thay thế phương trình thứ hai vào phương trình thứ nhất để có: A = 2C + 2 Sau đó chúng ta có thể thay thế cả hai phương trình vào phương trình thứ ba để có: (2C + 2) + 2C + (C) = 27 Đơn giản hóa phương trình này, chúng ta có: 5C + 2 = 27; 5C = 25; C = 5 Bây giờ chúng ta biết C là 5, chúng ta có thể sử dụng phương trình thứ hai để tìm B: B = 2C = 2(5) = 10 Do đó, đáp án là <E>. B 10 tuổi. Do đó, đáp án là: E

Câu hỏi: a lớn hơn b hai tuổi, b gấp đôi tuổi c. nếu tổng tuổi của a, b và c là 27, thì b bao nhiêu tuổi? a) 7, b) 9, c) 8, d) 11, e) 10
Đáp án lý luận:
NL CoT

Input
Wolfram CDP CoT
Wolfram SDP CoT
Python CDP CoT
Python SDP CoT

1.def solution():
2.import math
3.import sympy
4.a=sympy.Symbol('a')
5.b=sympy.Symbol('b')
6.c=sympy.Symbol('c')
7.options=[7,9,8,11,10]
8.a_two_years_older_than_b=sympy.Eq(a,b+2)
9.b_twice_as_c=sympy.Eq(b,c*2)
10.sum_of_a_b_c=sympy.Eq(a+b+c,27)
11.b_value=sympy.solve([a_two_years_older_than_b,b_twice_as_c,sum_of_a_b_c])[b]
12.correct_option=None
13.for i,option in enumerate(options):
14.if math.fabs(option-b_value)<1e-4:
15.correct_option=chr(ord('A')+i)
16.break
17.result=correct_option
18.return result

1.v1=a==b+2(*a lớn hơn b hai tuổi*)
2.v2=b==2*c(*gấp đôi tuổi c*)
3.v3=a+b+c==27(*tổng tuổi của a, b và c là 27*)
4.option=<|A->7,B->9,C->8,D->11,E->10|>
5.v4=Solve[{v1,v2,v3},{a,b,c}](*Bước 1: Giải hệ phương trình*)
6.v5=b/.v4[[1]](*Bước 2: Lấy giá trị của b*)
7.Keys[Select[option,#==v5&]](*Vậy lựa chọn đúng là*)

1.a_is_two_years_older_than_b=a-2==b
2.b_is_twice_as_old_as_c=b==2*c
3.total_age=a+b+c==27
4.option=<|A->7,B->9,C->8,D->11,E->10|>
5.solution=Solve[{a_is_two_years_older_than_b,b_is_twice_as_old_as_c,total_age},{a,b,c}]
6.b_age_value=b/.solution[[1]]
7.Keys[Select[option,#==b_age_value&]]

1.def solution():
2.import math
3.import sympy
4.v1=sympy.Symbol('a')
5.v2=sympy.Symbol('b')
6.v3=sympy.Symbol('c')
7.options=[7,9,8,11,10]
8.v4=sympy.Eq(v1,v2+2)#a lớn hơn b hai tuổi
9.v5=sympy.Eq(v2,v3*2)#b gấp đôi tuổi c
10.v6=sympy.Eq(v1+v2+v3,27)#tổng tuổi của a, b và c là 27
11.v7=sympy.solve([v4,v5,v6])[v2]#b bao nhiêu tuổi
12.correct_option=None
13.for i,option in enumerate(options):
14.if math.fabs(option-v7)<1e-4:
15.correct_option=chr(ord('A')+i)
16.break
17.result=correct_option
18.return result

Hình 1: Ví dụ về các biểu diễn CoT: Natural Language (NL) CoT, Comment-Describing Program (CDP) và Self-Describing Program (SDP) trong cả Wolfram và Python.

Ngược lại, non-describing program (NDP) chỉ sử dụng tên biến trừu tượng (ví dụ: v1 và v2). Trong SDP, các chương trình có thể được tạo ra dễ dàng hơn từ các câu hỏi, trong khi ở NDP, các chương trình có thể được sử dụng hiệu quả hơn trong lý luận. Để kết hợp điểm mạnh của cả hai loại, chúng tôi giới thiệu comment-describing program (CDP), một thiết kế CoT mới kết hợp tên biến trừu tượng với các bình luận ngôn ngữ tự nhiên.

Theo thông lệ chung (Uesato et al., 2022; Lightman et al., 2023), chúng tôi tiến hành các thí nghiệm fine-tuning, reranking, và majority-voting để so sánh các CoT trên bộ dữ liệu GSM8K (Cobbe et al., 2021), MATHQA (Amini et al., 2019), và SVAMP (Patel et al., 2021). Trong cài đặt tốt nhất, phương pháp sử dụng mô hình 30B với reward model reranking có thể vượt trội hơn hiệu suất few-shot của GPT-3.5-turbo khoảng 2.9% trên GSM8K, 18% trên MATHQA và 8% trên SVAMP.

Chúng tôi đưa ra những kết luận chính sau đây từ các thí nghiệm:

1. Program CoT thường hoạt động tốt hơn natural language CoT, cho thấy việc sử dụng CoT cứng nhắc hơn là tốt hơn.

2. Sự hiện diện của ngôn ngữ tự nhiên trong SDP và CDP là quan trọng để đạt hiệu suất cao so với NDP. SDP thường vượt trội hơn CDP, vì nó có thể tạo ra CoT đa dạng hơn và do đó đạt hiệu suất cao hơn trong majority voting và reranking.

3. Program CoT trong Python hoạt động tốt hơn những cái trong Wolfram, khi CoT cùng loại.

4. Bằng cách kết hợp việc sử dụng các loại CoT khác nhau, chúng ta có thể tăng cường hiệu suất tổng thể, cho thấy tiềm năng cho thiết kế CoT tiếp theo tận dụng điểm mạnh của tất cả các loại CoT.

Những phát hiện của chúng tôi cung cấp những hiểu biết có giá trị cho việc thiết kế CoT trong giải quyết bài toán và lý luận rộng hơn với LLM.

--- TRANG 3 ---
Preprint.
2 THIẾT KẾ CHAIN-OF-THOUGHT
2.1 NATURAL LANGUAGE COTS (NL)
Wei et al. (2022b) đề xuất kỹ thuật chain-of-thought prompting để tăng cường khả năng lý luận phức tạp của LLM. Phương pháp này cố gắng mô phỏng quá trình tư duy giải quyết các vấn đề lý luận đa bước. Như được mô tả trong khối thứ hai của Hình 1, cách tiếp cận chain-of-thought này cho giải quyết bài toán tạo ra các mô tả lý luận từng bước bằng ngôn ngữ tự nhiên và cung cấp câu trả lời cuối cùng ở cuối quá trình lý luận.

2.2 PROGRAM COTS
Chúng tôi tập trung vào hai ngôn ngữ lập trình riêng biệt: Wolfram Language (Wolfram, 2015) và Python (Van Rossum & Drake Jr, 1995). Nghiên cứu gần đây (Wang et al., 2023a) cũng sử dụng hai ngôn ngữ này trong tool-based Transformers (Schick et al., 2023). Wolfram Language, với Wolfram Mathematica làm động cơ thực thi², là một ngôn ngữ biểu cảm và linh hoạt có thể biểu diễn hiệu quả các khái niệm toán học phức tạp. Nó có các hàm toán học tích hợp phong phú cho đại số, giải phương trình, v.v., và một cú pháp được thiết kế trực quan và thoải mái. Mặt khác, Python là một ngôn ngữ đa năng đã được áp dụng rộng rãi trong tài liệu gần đây cho giải quyết bài toán (Gao et al., 2023; He-Yueya et al., 2023). Với bản chất tương phản của Wolfram và Python, chúng tôi tiến hành so sánh toàn diện qua tất cả các loại program CoT trong hai ngôn ngữ. Tiếp theo, chúng tôi mô tả thiết kế của các loại CoT, với Hình 1 trình bày các trường hợp của chúng trong hai ngôn ngữ.

Self-Describing Program (SDP) Thiết kế đầu tiên chúng tôi xem xét là self-describing program (SDP) như được hiển thị ở phía dưới bên phải của Hình 1. Nó trình bày giải pháp theo cách từng bước và định nghĩa tên biến sử dụng ngôn ngữ tự nhiên, tương tự như của Gao et al. (2023). Một lợi thế của SDP là người ta có thể giải quyết vấn đề bằng cách thực thi trực tiếp chương trình. Lợi thế khác là tên biến từ câu hỏi, làm cho việc tạo ra các bước lý luận dễ dàng hơn cho LLM. Khi ghi nhãn chương trình, chúng tôi tuân theo một số hướng dẫn tổng quát: (1) sử dụng các phép toán cấp cao để làm cho chương trình ngắn gọn và dễ hiểu trực quan, (2) liệt kê tên biến theo thứ tự của chúng trong câu hỏi, và (3) đảm bảo rằng tên biến có ý nghĩa, mô tả và được viết trong quy ước đặt tên snake case (ví dụ: viết thường và phân tách bằng dấu gạch dưới).

Comment-Describing Program (CDP) Mặc dù thiết kế ngắn gọn, SDP có một số vấn đề. Các tên self-describing có thể không đủ tổng quát qua các bài toán và không đủ thông tin để cung cấp ngữ cảnh phong phú trong CoT. Do đó, chúng tôi xem xét comment-describing program (CDP) sử dụng tên biến chuẩn hóa, ví dụ: v1, v2, và các bình luận ngắn gọn mô tả bước lý luận và giải quyết vấn đề. Hình 1 (phía dưới bên trái) hiển thị một ví dụ trong Python và Wolfram. Bình luận trong dòng khai báo là một tuyên bố ngắn gọn về vấn đề cung cấp chi tiết. Bình luận trong dòng lý luận giải thích mục đích của bước, hiển thị như một lệnh hoặc hướng dẫn. Vì ngôn ngữ Python thường yêu cầu cú pháp nghiêm ngặt hơn, các dòng khai báo bổ sung, chẳng hạn như dòng khai báo symbol Sympy, phải được bao gồm trong chương trình để làm cho nó có thể thực thi. Trong những dòng như vậy, bình luận được bỏ qua.

Non-Describing Program (NDP) Chúng tôi cũng xem xét một biến thể trong đó các bình luận của CDP được loại bỏ. NDP cũng có thể được coi là một cách tiếp cận trái ngược với SDP trong đó ở cái trước tên biến được định nghĩa bằng ngôn ngữ tự nhiên và ở cái sau tên biến được định nghĩa như các ký hiệu trừu tượng.

3 THU THẬP DỮ LIỆU
Chúng tôi xem xét ba bộ dữ liệu trong công việc này, GSM8K (Cobbe et al., 2021), MATHQA (Amini et al., 2019), và SVAMP (Patel et al., 2021). Với các câu hỏi đã cho, chúng tôi phát triển một phương pháp để chú thích bán tự động các CoT trong tập huấn luyện. Nói chung, chúng tôi sử dụng kỹ thuật few-shot prompting để có được CDP và SDP trong cả Python và Wolfram, cũng như NL CoT.

²https://www.wolfram.com/engine/

--- TRANG 4 ---
Preprint.
Hình 2: Tổng quan về thu thập dữ liệu, với CDP làm ví dụ.

Cách tiếp cận chú thích được hỗ trợ bởi LLM của chúng tôi hoạt động như sau. Đầu tiên chúng tôi tạo thủ công một số lượng nhỏ các chú thích CoT, sau đó để LLM truy xuất các chú thích CoT tương tự làm ví dụ và tạo CoT dựa trên các ví dụ theo cách few-shot. Sau đó chúng tôi tự động thực thi các chương trình và lấy các chú thích được xác minh chính xác làm kết quả chú thích. Quá trình được lặp lại ba đến năm lần và cuối cùng, chúng tôi chú thích thủ công những cái vẫn không thể vượt qua xác minh. Chúng tôi sử dụng Wolfram CoT làm ví dụ để minh họa các chi tiết chú thích.

(1) Chú thích seed thủ công ban đầu Chúng tôi chọn ngẫu nhiên 20 mẫu từ bộ dữ liệu cho chú thích self-describing program và chú thích comment-describing program, tương ứng. Các chương trình được chú thích phải tuân theo định nghĩa CoT và ngữ pháp Wolfram. Chúng tôi tiến hành xác minh chéo giữa các tác giả, thực thi các chương trình bằng Wolfram Mathematica, và có được kết quả chú thích của các mẫu được thực thi thành công. 20 mẫu và các chú thích chính xác của chúng được coi là tập hoàn thành ban đầu, và các mẫu khác trong bộ dữ liệu được coi là tập làm việc ban đầu.

(2) Thu thập embedding câu hỏi Chúng tôi có được tất cả embedding của câu hỏi trong bộ dữ liệu bằng cách gọi trực tiếp API của "text-embedding-ada-002" từ OpenAI³.

(3) Chú thích LLM dựa trên truy xuất Đối với mỗi mẫu được chú thích trong tập làm việc, chúng tôi truy xuất top-k ví dụ tương tự nhất (Liu et al., 2022; Gao et al., 2021) từ tập hoàn thành dựa trên độ tương tự cosine của các embedding câu hỏi. Đối với chú thích CDP, chúng tôi sử dụng các câu hỏi của top-k ví dụ và các chương trình CDP của chúng làm prompt, và để LLM trả về chương trình CDP cho mẫu đã cho. Định dạng của một ví dụ được trình bày trong Hình 2. Ở đây chúng tôi chọn "gpt-3.5-turbo" làm LLM và k được đặt là 5. Đối với chú thích SDP, chúng tôi sử dụng các câu hỏi của top-k ví dụ và các chương trình SDP của chúng làm prompt, và để LLM trả về chương trình SDP.

(4) Xác minh tự động, cập nhật tập hoàn thành và tập làm việc Sau khi có được tất cả chú thích của tập làm việc được trả về bởi LLM, các CDP và SDP được chú thích được thực thi

³https://platform.openai.com/docs/guides/embeddings

--- TRANG 5 ---
Preprint.
sử dụng Wolfram Mathematica, và sau đó kết quả được so sánh với ground truth để xác định tính chính xác. Đối với GSM8K và SVAMP, vì các câu trả lời nên là số, chúng tôi coi các câu trả lời không bằng nhau trừ khi chúng có thể được chuyển đổi thành float và giá trị của chúng khác nhau nhiều nhất 1e−3. Đối với MATHQA, do định dạng câu hỏi trắc nghiệm, chúng tôi áp dụng exact match để so sánh câu trả lời. Các mẫu có kết quả chính xác sau khi thực thi được đưa vào tập hoàn thành, và được loại bỏ khỏi tập làm việc.

(5) Lặp lại bước 3 và bước 4 từ ba đến năm lần cho đến khi tập làm việc trống hoặc không có mẫu mới nào có thể được thêm vào tập hoàn thành.

(6) Chỉnh sửa thủ công tập làm việc còn lại Nếu vẫn còn bất kỳ mẫu nào trong tập làm việc, chúng tôi tiến hành chú thích thủ công trên các mẫu, cho đến khi các chương trình có thể có được kết quả chính xác sử dụng Wolfram Mathematica.

Cách tạo Python CoT và NL CoT cũng giống như trên. Lưu ý rằng đối với NL CoT, vì chúng không thể được xác minh trực tiếp bởi một engine, chúng tôi chỉ áp dụng một quy tắc đơn giản theo NL CoT, "Therefore the answer is:", để có được các câu trả lời. NDP trong Wolfram và Python có thể được thu được bằng cách loại bỏ các bình luận trong CDP tương ứng của chúng.

4 PHƯƠNG PHÁP LUẬN
Phù hợp với các nghiên cứu trước đây (Uesato et al., 2022; Lightman et al., 2023), chúng tôi sử dụng supervised fine-tuning (SFT), self-consistency decoding (còn được gọi là majority voting) (Wang et al., 2023b), và các phương pháp luận reward model reranking trên bộ dữ liệu được chú thích của chúng tôi.

4.1 SUPERVISED FINE-TUNING
Chúng tôi tiến hành SFT trên một mô hình ngôn ngữ được huấn luyện trước sử dụng các câu hỏi và chú thích chain-of-thought trong mỗi bộ dữ liệu. Việc huấn luyện nhằm tối đa hóa khả năng xảy ra của câu trả lời cho trước câu hỏi. Trong đánh giá, chúng tôi trích xuất câu trả lời cuối cùng được tạo ra bởi mô hình SFT. Như được hiển thị trong Hình 1, NL CoT đặt câu trả lời cuối cùng trong câu cuối cùng, "Therefore, the answer is E.". Trong các trường hợp của SDP, CDP, và NDP, chúng tôi thực thi chương trình để có được câu trả lời.

4.2 MAJORITY VOTING
Trong self-consistency decoding (Wang et al., 2023b)⁴, trước tiên chúng tôi lấy mẫu một số lượng nhất định các CoT từ mô hình ngôn ngữ. Sau đó chúng tôi thực hiện majority voting qua các câu trả lời được trích xuất từ tất cả các CoT được lấy mẫu và chọn câu trả lời cuối cùng được ủng hộ nhiều nhất trong tất cả các câu trả lời. Chúng tôi đơn giản áp dụng chiến lược temperature sampling (Ackley et al., 1985; Ficler & Goldberg, 2017) với T = 1.0, vì được báo cáo (Wang et al., 2023b) rằng self-consistency decoding thường mạnh mẽ với các chiến lược sampling và siêu tham số.

4.3 RERANKING VỚI REWARD MODEL
Theo Cobbe et al. (2021), một reward model (RM) được huấn luyện để xác định xem một câu trả lời cho câu hỏi có đúng hay không. Với mô hình SFT, chúng tôi thực hiện sampling để có được một số lượng nhất định các giải pháp CoT cho câu hỏi. Như một thực hành phổ biến, reward model là một mô hình ngôn ngữ được khởi tạo từ mô hình SFT. Tương tự như outcome-based reward model (ORM) (Uesato et al., 2022), reward model được huấn luyện để dự đoán một nhãn nhị phân cho biết giải pháp "correct" hoặc "incorrect"⁵. Một khi đầu vào đi qua reward model, phân loại được tiến hành với một bộ phân loại tuyến tính trên hidden state của token cuối cùng. Cuối cùng, giải pháp với điểm "correct" cao nhất trong số các ứng viên được chọn làm câu trả lời cuối cùng.

Vì chúng tôi không có các cặp "correct" và "incorrect" được chú thích rõ ràng, chúng tôi áp dụng mô hình tại epoch thứ 2 trong quá trình supervised fine-tuning để lấy mẫu các cặp giải pháp. Theo Cobbe et al. (2021),

⁴Chúng tôi sử dụng thuật ngữ "majority voting" trong phần còn lại của bài báo này trừ khi được chỉ định.
⁵Các thí nghiệm sơ bộ của chúng tôi với process-based reward (Lightman et al., 2023) cho thấy hiệu suất tương tự với outcome-based reward. Chúng tôi quy lý do cho chất lượng của các nhãn process tự động (Uesato et al., 2022).

--- TRANG 6 ---
Preprint.
[Bảng 1: Hiệu suất supervised fine-tuning của tất cả các loại CoT. Các số được hiển thị in đậm là cao nhất trong cùng cài đặt.]

việc sử dụng các checkpoint tại các epoch ban đầu có thể cung cấp các giải pháp đa dạng hơn để huấn luyện một reward model. Đối với mỗi câu hỏi trong dữ liệu huấn luyện D, chúng tôi lấy mẫu K giải pháp. Sau đó chúng tôi sử dụng tất cả các mẫu có chứa cả giải pháp đúng và sai để huấn luyện reward model trong ba epoch.

5 CÁC THÍ NGHIỆM
5.1 CÀI ĐẶT THÍ NGHIỆM
Chúng tôi tiến hành các thí nghiệm trên ba bộ dữ liệu: GSM8K (Cobbe et al., 2021), MATHQA (Amini et al., 2019)⁶, và SVAMP (Patel et al., 2021). Phụ lục §B minh họa quy trình tiền xử lý cho bộ dữ liệu MathQA và SVAMP. Dữ liệu huấn luyện cho tất cả các loại CoT được thu thập bằng phương pháp được mô tả trong §3. Chúng tôi báo cáo kết quả của few-shot prompting sử dụng GPT-3.5-turbo, majority voting, và RM reranking⁷.

Chúng tôi áp dụng mô hình ngôn ngữ được huấn luyện trước Galactica (Taylor et al., 2022) được huấn luyện trên kho ngữ liệu khoa học quy mô lớn và mã lập trình. Mô hình Galactica cho thấy hiệu suất vượt trội trong giải quyết bài toán so với các mô hình nền tảng khác như LLaMA (Touvron et al., 2023a) trong các thí nghiệm sơ bộ của chúng tôi. Trong suốt các thí nghiệm, chúng tôi sử dụng kích thước mô hình 6.7B⁸ và 30B⁹ có sẵn trong HuggingFace. Chúng tôi sử dụng framework Megatron-Deepspeed¹⁰ để supervised fine-tuning hiệu quả, theo BLOOM (Scao et al., 2022). Mô hình được fine-tuned trong 40 epoch với độ dài sequence tối đa là 1024. Vui lòng tham khảo Phụ lục (Bảng 8) cho cài đặt siêu tham số.

Chúng tôi chọn mô hình SFT có độ chính xác tốt nhất để sampling để có được kết quả majority voting (Wang et al., 2023b). Để huấn luyện reward model, chúng tôi tạo ra 100 mẫu cho mỗi câu hỏi trong tập huấn luyện sử dụng SFT checkpoint tại epoch thứ hai và so sánh chúng với ground-truth để xác định các nhãn đúng. Bằng cách sử dụng một checkpoint sớm hơn, chúng tôi có thể có sự đa dạng sampling nhiều hơn, điều này hữu ích cho việc huấn luyện reward model. Như được mô tả trong §4.3, chúng tôi khởi tạo reward model với SFT model checkpoint tốt nhất và fine-tune nó trong ba epoch.

⁶Do hạn chế về ngân sách tính toán, chúng tôi chỉ thí nghiệm với 15k mẫu ngẫu nhiên của tập huấn luyện MATHQA.
⁷Chúng tôi cũng thí nghiệm với RM-weighted voting và hiệu suất tương tự như reranking (Phụ lục §C).
⁸https://huggingface.co/facebook/galactica-6.7b
⁹https://huggingface.co/facebook/galactica-30b
¹⁰https://github.com/bigscience-workshop/Megatron-DeepSpeed

--- TRANG 7 ---
Preprint.
5.2 KẾT QUẢ SUPERVISED FINE-TUNING
Bảng 1 trình bày kết quả supervised fine-tuning qua tất cả các bộ dữ liệu, ngôn ngữ, và loại CoT. Nói chung, các CoT dựa trên chương trình hoạt động tốt hơn natural language CoT. Kích thước mô hình mở rộng tương quan với sự gia tăng đáng chú ý trong hiệu suất của natural language CoT, có thể do cải thiện khả năng hiểu ngôn ngữ tự nhiên. Tuy nhiên, các CoT dựa trên chương trình vẫn vượt trội hơn natural language CoT một cách nhất quán và đáng kể. Các mô hình SFT được mô tả ở đây sau đó được sử dụng cho majority voting và reranking.

[Bảng 2: So sánh hiệu suất giữa few-shot prompting, majority voting và reranking. Các số in đậm là tốt nhất và tốt hơn đáng kể so với tốt thứ hai với p < 0.001.]

5.3 KẾT QUẢ CHÍNH
Bảng 2 cho thấy so sánh giữa các phương pháp khác nhau: GPT-3.5-turbo¹¹ prompting, majority voting, và reranking. Chúng ta có thể quan sát thấy rằng các mô hình lớn hơn (tức là 30B) có thể cải thiện đáng kể hiệu suất so với các mô hình nhỏ hơn (tức là 6.7B) trên tất cả các bộ dữ liệu. Các biến thể tốt nhất của chúng tôi có thể vượt trội hơn few-shot prompting GPT-3.5-turbo một cách rõ rệt.

Hiệu suất Prompting Các ví dụ few-shot được chọn ngẫu nhiên từ các chú thích huấn luyện cho tất cả các loại CoT. Trong số các phương pháp sử dụng GPT-3.5-turbo, natural language thường tốt hơn comment-describing program, self-describing program, và non-describing program, trong khi self-describing program trong Python tốt hơn trên GSM8K và SVAMP. Chúng tôi quy lý do cho hiệu suất thấp là do tính khả dụng hạn chế của các chương trình trong dữ liệu huấn luyện trước của GPT-3 (Brown et al., 2020), điều này dường như đúng đối với hầu hết các LLM hiện có (Touvron et al., 2023a; Taylor et al., 2022; Touvron et al., 2023b). Hơn nữa, việc tổng quát hóa cho các vấn đề mới chỉ với một vài ví dụ học in-context là thách thức. Trong khi nghiên cứu đang tiến hành giải quyết những thách thức này (Min et al., 2022; Hao et al., 2022; Coda-Forno et al., 2023), công việc của chúng tôi không tập trung vào khía cạnh này. Các CoT của SDP trong Python tương tự hơn với các mã lập trình trong kho ngữ liệu huấn luyện trước, và do đó việc sử dụng chúng dẫn đến hiệu suất tốt hơn trên GSM8K và SVAMP. Đối với bộ dữ liệu ồn ào hơn, MATHQA, natural language CoT có xu hướng đoán trên các câu hỏi trắc nghiệm, ngay cả khi chúng không chính xác, trong khi program CoT có xu hướng chọn không quyết định nếu không có câu trả lời hợp lệ nào có sẵn.

So sánh Program và Natural Language Nói chung, hiệu suất với tất cả các loại program CoT đều nhất quán tốt hơn so với natural language CoT. Sự vượt trội này đặc biệt rõ ràng trong trường hợp MATHQA nơi program CoT, kết hợp với reranking, dẫn đến cải thiện hiệu suất vượt quá 10 điểm cho mô hình 6.7B so với natural language CoT. Điều này là do các câu trả lời ở định dạng trắc nghiệm trong MATHQA, và do đó các dự đoán không chính xác mà kết quả thực thi chương trình là "null-result" có thể dễ dàng được lọc ra trước khi thực hiện majority voting hoặc reranking¹². Bảng 3 trình bày tỷ lệ phần trăm câu trả lời null-result trong dự đoán MATHQA. Mặc dù natural language CoT tạo ra ít câu trả lời null-result hơn, hiệu suất của chúng với câu trả lời null-result tệ hơn, như được hiển thị trong Bảng 4. Do đó, việc loại bỏ những mẫu đó là cần thiết.

[Bảng 3: Tỷ lệ phần trăm câu trả lời null-result trong dự đoán MathQA Wolfram.]

[Bảng 4: Độ chính xác majority voting NL so với tỷ lệ phần trăm câu trả lời null-result trong CDP.]

vì voting/re-ranking có thể bị sai lệch bởi chúng. Ngược lại, natural language CoT có xu hướng chọn một câu trả lời (ví dụ: A, B), bất kể độ chính xác của CoT, vì CoT không thể được thực thi như một chương trình. Tuy nhiên, có những ngoại lệ khi chúng tôi sử dụng non-describing program. Ví dụ, hiệu suất của "majority voting + NDP" với Python sử dụng mô hình 6.7B tệ hơn so với natural language trên SVAMP. Quan sát tương tự cũng áp dụng cho bộ dữ liệu GSM8K với mô hình 30B cho cả ngôn ngữ Wolfram và Python. Không có ngôn ngữ tự nhiên, NDP có khả năng hiểu ngôn ngữ yếu hơn so với SDP và CDP.

So sánh Program CoT Dưới cả chiến lược majority voting và reranking, self-describing program nhất quán đạt hiệu suất tốt nhất, tiếp theo là comment-describing program, và sau đó là non-describing program. Không giống như trong SFT, self-describing program cung cấp sự đa dạng hơn và do đó có xu hướng hoạt động tốt hơn trong voting và re-ranking. Đáng chú ý, mô hình 30B với Python, "reranking + SDP", đạt hiệu suất tốt nhất trên GSM8K và SVAMP. Hiệu suất cũng cao hơn 2.9 điểm và 8.6 điểm so với cách tiếp cận prompting tốt nhất với GPT-3.5-turbo trên GSM8K và SVAMP, tương ứng. "reranking + SDP" với Wolfram cũng đạt hiệu suất tốt nhất trên bộ dữ liệu MATHQA ồn ào, với cải thiện +28 điểm so với GPT-3.5-turbo prompting. Mặc dù hiệu suất với CDP tệ hơn SDP, chúng ta có thể thấy rằng các phương pháp CDP tốt nhất vẫn có thể vượt trội hơn cách tiếp cận GPT-3.5-turbo prompting tốt nhất trên tất cả các bộ dữ liệu.

So sánh Ngôn ngữ Lập trình Các mô hình 6.7B và 30B hoạt động tốt nhất thường là các phương pháp trong Python, như được hiển thị trong Bảng 2. Ngoại lệ duy nhất là mô hình 30B tốt nhất với Python kém hơn 0.5 điểm so với mô hình 30B tốt nhất với Wolfram. Đối với non-describing và self-describing program, việc sử dụng Python thường vượt trội hơn việc sử dụng Wolfram. Đối với comment-describing program, các phương pháp sử dụng Python và Wolfram có hiệu suất tương đương, với mô hình 6.7B sử dụng Wolfram có hiệu suất tốt hơn trên SVAMP.

¹¹https://platform.openai.com/docs/models/gpt-3-5
¹²Chúng tôi sẽ thấy nhiều dự đoán cho "null" làm câu trả lời voting/reranking nếu chúng tôi không loại bỏ chúng.

--- TRANG 8 ---
Preprint.
6 PHÂN TÍCH
6.1 SỐ LƯỢNG INSTANCES CHO SAMPLING
Chúng tôi đo lường ảnh hưởng của số lượng instances được lấy mẫu K trong quá trình majority voting. Chúng tôi thay đổi số K từ 1 đến 100 và đánh giá độ chính xác cho các mô hình 6.7B và 30B. Hình 3 cung cấp kết quả trên GSM8K. Hiệu suất của tất cả các phương pháp cải thiện nhanh chóng với sự gia tăng của K và trở nên ổn định khi K lớn hơn 30. Cụ thể, cả CDP và NDP đều yêu cầu số K nhỏ hơn so với SDP và NL. Kết quả cho thấy CDP và NDP mang tính xác định hơn trong khi SDP và NL đa dạng hơn. Với CoT đa dạng hơn, SDP và NL có thể đạt được nhiều cải thiện hơn với nhiều mẫu hơn trong majority voting.

[Hình 3: Majority voting liên quan đến số lượng instances được lấy mẫu khác nhau (Trái: 6.7B; Phải: 30B). Chúng tôi chỉ mô tả hiệu suất trong Python cho mục đích minh họa.]

6.2 THỐNG KÊ SAMPLING BIỂU DIỄN
Bảng 5 báo cáo kết quả dự đoán mô hình trên GSM8K, bao gồm tỷ lệ phần trăm dự đoán chính xác về mặt cú pháp (tức là tỷ lệ thực thi), tỷ lệ phần trăm câu trả lời chính xác (tức là precision) và cơ hội có được ít nhất một câu trả lời chính xác trong 100 mẫu (tức là correct@100). Ở đây, chính xác về mặt cú pháp có nghĩa là, chúng ta có thể trích xuất hoặc thực thi CoT để có được một câu trả lời hợp lệ (ví dụ: chữ cái A, B, C, hoặc D cho MATHQA, và giá trị số cho GSM8K và SVAMP).

Có thể thấy rằng NL CoT có correct@100 cao và tỷ lệ thực thi nhưng với precision thấp nhất so với tất cả các loại CoT khác. Điều này có thể là do cú pháp ngôn ngữ tự nhiên đơn giản và thách thức đối với các mô hình ở quy mô hiện tại để thực hiện các phép tính chính xác mà không có sự trợ giúp của một động cơ tính toán. Đáng chú ý là CDP thường có precision cao nhất và tỷ lệ thực thi, và điểm correct@100 tương đối cao, và SDP có tỷ lệ thực thi thấp nhất nhưng điểm correct@100 cao nhất và precision tương đối cao. Kết quả tiếp tục hỗ trợ giả thuyết của chúng tôi rằng CDP mang tính xác định và chính xác hơn, trong khi SDP có mức độ đa dạng cao hơn, và do đó có cơ hội cao hơn để có được câu trả lời chính xác với rủi ro gây ra nhiều lỗi hơn. Do đó, chúng tôi kết luận rằng có sự cân bằng giữa đa dạng và chính xác là quan trọng cho hiệu suất cao hơn trong voting và reranking. Tỷ lệ thực thi của CDP và NDP tương tự, nhưng CDP ghi điểm cao hơn trong correct@100 và đạt precision tốt hơn đáng kể. Quan sát như vậy cho thấy lợi ích của việc bao gồm các bình luận ngôn ngữ tự nhiên.

[Bảng 5: Thống kê sampling trên bộ dữ liệu GSM8K.]

6.3 GIỚI HẠN TRÊN
Chúng tôi phân tích kết quả trong reranking để khám phá tiềm năng của các thiết kế CoT (NL, CDP/SDP trong Wolfram/Python). Chúng tôi tính toán độ chính xác khi bất kỳ CoT nào đúng, được coi là giới hạn trên của tất cả các loại CoT. Chúng tôi xem xét hiệu suất tốt nhất của các loại CoT riêng lẻ và giới hạn trên của tất cả các loại CoT trong Bảng 6. Chúng tôi thấy rằng giới hạn trên của các CoT trên mô hình 30B là 98.8%, 93.0%, 95.0% trên GSM8K, MATHQA, SVAMP, tương ứng. Điều này cho thấy tiềm năng kết hợp các CoT để tạo ra biểu diễn chính xác hơn. Chúng tôi để lại điều này như công việc trong tương lai.

[Bảng 6: Hiệu suất tốt nhất của các loại CoT riêng lẻ, và giới hạn trên của tất cả các loại CoT (nếu bất kỳ CoT nào đúng).]

[Hình 4: Tỷ lệ phần trăm trường hợp thất bại được dự đoán chính xác trong các loại CoT khác nhau.]

Chúng tôi cũng phân tích kết quả của reward model reranking 30B bằng cách so sánh các loại CoT khác nhau trên cùng một ví dụ (Hình 4). Mặc dù SDP có hiệu suất tổng thể tốt hơn, một lượng không nhỏ thất bại được giải quyết chính xác bởi CDP hoặc NL. Quan sát tương tự cũng áp dụng cho các trường hợp thất bại của CDP hoặc NL. Kết quả trên cho thấy CDP, SDP, và NL có lợi thế riêng biệt cho giải quyết bài toán. Chúng tôi tiến hành một thí nghiệm khác sử dụng phương pháp xem xét ba loại CoT một cách bình đẳng trong quá trình majority voting và reranking. Đối với reranking, chúng tôi huấn luyện một reward model có khả năng phân biệt và xếp hạng ba loại CoT. Số lượng giải pháp CoT được lấy mẫu được đặt là 100 để so sánh công bằng. Cụ thể, chúng tôi thực hiện majority voting và reranking trên 100 giải pháp chứa ba loại CoT, trong đó CDP, SDP ở Wolfram. Bảng 7 cho thấy so sánh giữa kết quả tổng hợp và hiệu suất tốt nhất trước đó trong Bảng 2. Những cải thiện đáng kể cho thấy vẫn còn tiềm năng lớn cho thiết kế CoT tốt hơn tích hợp điểm mạnh của cả ba loại CoT.

[Bảng 7: Hiệu suất tổng hợp các loại CoT CDP, SDP, và NL trong Wolfram.]

--- TRANG 9 ---
Preprint.
7 CÔNG TRÌNH LIÊN QUAN
Lý luận toán học thông qua CoT prompting (Wei et al., 2022b), trên các mô hình ngôn ngữ lớn (Wei et al., 2022a), đã trải qua sự phát triển đáng kể trong những năm gần đây, được chứng minh bởi một số lượng lớn các phương pháp CoT được đề xuất. Trong số đó, Uesato et al. (2022) áp dụng process-based và outcome-based reward để ghi điểm các CoT ngôn ngữ tự nhiên trên GSM8K (Cobbe et al., 2021), cải thiện đáng kể hiệu quả giải quyết vấn đề. Lightman et al. (2023) tăng cường khả năng của process-based reward model và đạt được những cải thiện đáng kể trên bộ dữ liệu MATH thách thức (Hendrycks et al., 2021). Hơn nữa, các nỗ lực nghiên cứu gần đây đã mở rộng CoT ngôn ngữ tự nhiên đơn giản, bao gồm nhiều cách tiếp cận khác nhau được thiết kế để tăng cường và tối ưu hóa hiệu suất prompting. Cụ thể, Fu et al. (2023) giới thiệu khái niệm complexity-based prompts, cho thấy rằng LLM ưa thích chuỗi lý luận dài, thường dẫn đến hiệu suất vượt trội. Hơn nữa, các phương pháp được đề xuất bởi Zhou et al. (2023b) và Khot et al. (2023) thực hiện phân tách các vấn đề thành một loạt các câu hỏi đơn giản hơn và có thể quản lý được. Tương tự, Nye et al. (2021) trình bày khái niệm "Scratchpad", được thiết kế để trình bày rõ ràng các phép tính trung gian cho mô hình ngôn ngữ lớn. Mặc dù những tiến bộ này đáng kể, việc đảm bảo tính chính xác của CoT vẫn là một thách thức.

Bản chất xác định của các chương trình ngày càng thu hút sự chú ý của các nhà nghiên cứu sử dụng các phương pháp hỗ trợ chương trình cho giải quyết bài toán. Imani et al. (2023) phát triển một chiến lược đảm bảo tính nhất quán câu trả lời giữa lý luận chương trình và ngôn ngữ tự nhiên, do đó tăng cường độ tin cậy. Trong các theo đuổi tương tự, cả Chen et al. (2022) và Gao et al. (2023) đều đề xuất việc sử dụng các chương trình Python làm prompts. Bằng cách chuyển giao các nhiệm vụ thực thi cho Python interpreter, họ có thể giảm thiểu các vấn đề liên quan đến lý luận hoặc tính toán không chính xác. Các chương trình được sử dụng trong những cách tiếp cận này tương tự như self-describing programs của chúng tôi, nơi các biến được biểu diễn bằng ngôn ngữ tự nhiên. Zhou et al. (2023a) tiếp tục kết hợp ngôn ngữ tự nhiên và chương trình bằng cách sử dụng code interpreter trong GPT-4 (OpenAI, 2023). Đồng thời, nghiên cứu của Drori et al. (2022) và Li et al. (2022) đã chứng minh hiệu quả của việc tạo ra các chương trình Python hoàn toàn ký hiệu để giải quyết các câu hỏi MATH (Hendrycks et al., 2021) trong các cuộc thi lập trình. He-Yueya et al. (2023) cho phép lý luận khai báo trong một chương trình bằng cách nhúng các biểu thức ký hiệu vào prompts ngôn ngữ tự nhiên. Để đáp ứng sự đa dạng của các loại program CoT, công việc của chúng tôi nhằm cung cấp một phân tích và so sánh toàn diện các biểu diễn. Mục tiêu của chúng tôi là khám phá các đặc điểm riêng biệt và lợi thế tiềm năng của chúng.

8 KẾT LUẬN
Chúng tôi đã tiến hành một nghiên cứu toàn diện về thiết kế chain-of-thought cho giải quyết bài toán, bao gồm ngôn ngữ tự nhiên và program CoT. Chúng tôi phân loại program CoT thành non-describing program, self-describing program, và comment-describing program. Thông qua các thí nghiệm rộng rãi trên GSM8K, MATHQA và SVAMP, chúng tôi thấy rằng self-describing program thường đạt hiệu suất tốt nhất và vượt trội hơn few-shot prompting của GPT-3.5-turbo. Tốt hơn là sử dụng program CoT hơn là natural language CoT để giải quyết bài toán. Self-describing program và comment-describing program hoạt động tốt hơn non-describing program. Trong số hai cái đầu, self-describing program hoạt động tốt hơn comment-describing program. Program CoT trong Python hoạt động tốt hơn program CoT trong Wolfram. Chúng tôi hy vọng những phát hiện thí nghiệm của chúng tôi sẽ cung cấp những hiểu biết có giá trị cho việc thiết kế chain-of-thought trong tương lai trong giải quyết bài toán.

TÀI LIỆU THAM KHẢO
David H Ackley, Geoffrey E Hinton, và Terrence J Sejnowski. A learning algorithm for boltzmann machines. Cognitive science, 9(1):147–169, 1985. URL https://www.cs.toronto.edu/~hinton/absps/cogscibm.pdf.

Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, và Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of NAACL, 2019. URL https://arxiv.org/abs/1905.13319.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Proceedings of NeurIPS, 2020. URL https://arxiv.org/abs/2005.14165.

Wenhu Chen, Xueguang Ma, Xinyi Wang, và William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022. URL https://arxiv.org/abs/2211.12588.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.

Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane X Wang, và Eric Schulz. Meta-in-context learning in large language models. arXiv preprint arXiv:2305.12907, 2023. URL https://arxiv.org/abs/2305.12907.

Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, et al. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 119(32):e2123433119, 2022. URL https://arxiv.org/abs/2112.15594.

Jessica Ficler và Yoav Goldberg. Controlling linguistic style aspects in neural language generation. In Proceedings of EMNLP, 2017. URL https://aclanthology.org/W17-4912.

Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, và Tushar Khot. Complexity-based prompting for multi-step reasoning. In Proceedings of ICLR, 2023. URL https://arxiv.org/abs/2210.00720.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, và Graham Neubig. PAL: Program-aided language models. In Proceedings of ICML, 2023. URL https://arxiv.org/abs/2211.10435.

Tianyu Gao, Adam Fisch, và Danqi Chen. Making pre-trained language models better few-shot learners. In Proceedings of ACL, 2021. URL https://aclanthology.org/2021.acl-long.295/.

Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, và Asli Celikyilmaz. Roscoe: A suite of metrics for scoring step-by-step reasoning. In Proceedings of ICLR, 2022. URL https://arxiv.org/abs/2212.07919.

Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, và Furu Wei. Structured prompting: Scaling in-context learning to 1,000 examples. arXiv preprint arXiv:2212.06713, 2022. URL https://arxiv.org/abs/2212.06713.

--- TRANG 10 ---
Preprint.
Joy He-Yueya, Gabriel Poesia, Rose E Wang, và Noah D Goodman. Solving math word problems by combining language models with symbolic solvers. arXiv preprint arXiv:2304.09102, 2023. URL https://arxiv.org/abs/2304.09102.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, và Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Proceedings of Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://arxiv.org/abs/2103.03874.

Shima Imani, Liang Du, và Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023. URL https://arxiv.org/abs/2303.05398.

Zhanming Jie, Jierui Li, và Wei Lu. Learning to reason deductively: Math word problem solving as complex relation extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5944–5955, 2022.

Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, và Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In Proceedings of ICLR, 2023. URL https://arxiv.org/abs/2210.02406.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092–1097, 2022. URL https://arxiv.org/abs/2203.07814.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, và Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023. URL https://arxiv.org/abs/2305.20050.

Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, và Weizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, 2022. URL https://aclanthology.org/2022.deelio-1.10/.

Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, và Kai-Wei Chang. A survey of deep learning for mathematical reasoning. In Proceedings of ACL, 2023. URL https://arxiv.org/abs/2212.10535.

Sewon Min, Mike Lewis, Luke Zettlemoyer, và Hannaneh Hajishirzi. Metaicl: Learning to learn in context. In Proceedings of NAACL, 2022. URL https://arxiv.org/abs/2110.15943.

Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. URL https://arxiv.org/abs/2112.00114.

OpenAI. GPT-4 technical report, 2023. URL https://arxiv.org/abs/2303.08774.

Arkil Patel, Satwik Bhattamishra, và Navin Goyal. Are nlp models really able to solve simple math word problems? In Proceedings of NAACL, 2021. URL https://arxiv.org/abs/2103.07191.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022. URL https://arxiv.org/abs/2211.05100.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, và Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. URL https://arxiv.org/abs/2302.04761.

--- TRANG 11 ---
Preprint.
Minghuan Tan, Lei Wang, Lingxiao Jiang, và Jing Jiang. Investigating math word problems using pretrained multilingual language models. In Proceedings of the 1st Workshop on Mathematical Natural Language Processing (MathNLP), pp. 7–16, 2022.

Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, và Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022. URL https://arxiv.org/abs/2211.09085.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. URL https://arxiv.org/abs/2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. URL https://arxiv.org/abs/2307.09288.

Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, và Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. URL https://arxiv.org/abs/2211.14275.

Guido Van Rossum và Fred L Drake Jr. Python reference manual. Centrum voor Wiskunde en Informatica Amsterdam, 1995.

Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, và Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635, 2023a. URL https://arxiv.org/abs/2307.10635.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, và Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In Proceedings of ICLR, 2023b. URL https://arxiv.org/abs/2203.11171.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research, 8 2022a. URL https://arxiv.org/abs/2206.07682.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of NeurIPS, 2022b. URL https://arxiv.org/abs/2201.11903.

Stephen Wolfram. An Elementary Introduction to the Wolfram Language. Wolfram Media, Inc.; 3rd edition, 2015. URL https://www.wolfram.com/language/elementary-introduction/3rd-ed/.

Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023a. URL https://arxiv.org/pdf/2308.07921.pdf.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. In Proceedings of ICLR, 2023b. URL https://arxiv.org/abs/2205.10625.

--- TRANG 12 ---
Preprint.
A SIÊU THAM SỐ
[Bảng 8: Siêu tham số cho Supervised Fine-Tuning và Reward Modeling cho quy mô mô hình 6B và 30B tham số.]

B XỬ LÝ BỘ DỮ LIỆU
Chúng tôi tiền xử lý bộ dữ liệu MathQA gốc (Amini et al., 2019)¹³ để lọc ra một số instances không hợp lệ có chứa câu trả lời không chính xác (Tan et al., 2022; Jie et al., 2022). Ví dụ, một số phương trình được chú thích không dẫn đến câu trả lời chính xác trong MathQA. Đối với SVAMP, tập huấn luyện đến từ implementation gốc (Patel et al., 2021)¹⁴.

C RM-WEIGHTED VOTING
[Bảng 9: Hiệu suất majority voting được trọng số bởi reward model.]

¹³https://huggingface.co/datasets/math_qa
¹⁴https://github.com/arkilpatel/SVAMP
