# 2306.01707.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/math/2306.01707.pdf
# File size: 434257 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Learning Multi-Step Reasoning by Solving Arithmetic Tasks
Tianduo Wang and Wei Lu
StatNLP Research Group
Singapore University of Technology and Design
{tianduo_wang,luwei}@sutd.edu.sg
Abstract
Mathematical reasoning is regarded as a neces-
sary ability for Language Models (LMs). Re-
cent works demonstrate large LMs’ impressive
performance in solving math problems. The
success is attributed to their Chain-of-Thought
(CoT) reasoning abilities, i.e., the ability to de-
compose complex questions into step-by-step
reasoning chains, but such ability seems only
to emerge from models with abundant parame-
ters. This work investigates how to incorporate
relatively small LMs with the capabilities of
multi-step reasoning. We propose to inject such
abilities by continually pre-training LMs on a
synthetic dataset MSATwhich is composed of
Multi-stepArithmetic Tasks. Our experiments
on four math word problem datasets show the
effectiveness of the proposed method in enhanc-
ing LMs’ math reasoning abilities.1
1 Introduction
Making Language Models (LMs) perform math-
ematical reasoning is a valuable, yet challenging
research objective (Hendrycks et al., 2021; Cobbe
et al., 2021). Recently, we have witnessed large-
scale LMs’ impressive performance on a series
of reasoning tasks via chain-of-thought prompt-
ing (Wei et al., 2022). This method elicits large
LM’s ability to decompose a complex problem into
several intermediate steps. However, it is believed
that such ability only emerges from sufficiently
large models (empirically more than 100B param-
eters) (Wei et al., 2022). In this paper, we exam-
ine how to incorporate moderate-sized LMs, e.g.,
RoBERTa (Liu et al., 2019), with such multi-step
reasoning ability via continual pre-training to im-
prove the performance on math problems.
Correctly understanding numbers is a pre-
requisite of mathematical reasoning abilities. But
Wallace et al. (2019) shows that medium-sized
1Our code and data are released at https://github.com/
TianduoWang/MsAT .
Question:Rogerhas5tennisballs.Hebuys2morecansoftennisballs.Eachcanhave3tennisballs.Howmanytennisballsdoeshehavenow?Chain-of-thought (Wei et al., 2022):Code-style multi-step expression (ours):(step 1)(step 2)Math expression:5 + 2 ×3<Num0><Num1><Num2>Symbolic expression:<Num0> + <Num1> ×<Num2>Ans:11Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.N0= 2. N1= 3. N2= N1* N0.N3= 5. Ans = N2+ N3.Figure 1: A math word problem example with different
kinds of answers. In Question ,<Num0> ,<Num1> , and
<Num2> are special tokens used for masking numbers.
LMs have a deficiency in numerical comprehen-
sion. To overcome this issue, previous works inject
numerical reasoning skills into LMs following two
approaches. The first is masking numbers with
special tokens, and generating symbolic expres-
sions with a structured neural decoder (Xie and
Sun, 2019; Jie et al., 2022). An example of such
expression is provided in Figure 1. The second
strategy continually pre-trains LMs on synthetic
numerical tasks, which requires models to learn
how to perform computation involving numbers
(Geva et al., 2020; Pi et al., 2022).
However, both approaches suffer from critical
limitations. For symbolic methods, they neglect
the information carried by the numbers, which
could provide crucial hints for solving math prob-
lems (Wu et al., 2021; Liang et al., 2022). As for
continual pre-training methods, LMs’ arithmetic
skills are not reliable. Previous works indicate that
such skills are highly influenced by the training
data (Razeghi et al., 2022) and hard for extrapola-
tion (Wallace et al., 2019).
Motivated by these shortcomings, we propose to
first pre-train moderate-sized LMs on a synthetic
dataset called MSAT(Multi-stepArithmetic Tasks)arXiv:2306.01707v3  [cs.CL]  7 Jun 2023

--- PAGE 2 ---
Y=8.Z=2. X-Y=Z. X=?N0=2.N1=8. Ans=N0+N1Autoregressive DecoderAdapterPre-trained LM EncoderFigure 2: An illustration of the continual pre-training
process on our Seq2Seq model. We attach adapter mod-
ules to each layer of LM encoder and fix LM’s param-
eters (shaded area) during pre-training. Tokens N0,N1,
and Ansin the output are the variable names only used
by the decoder. Our DAG structured model is similarly
pre-trained with the only difference on the decoder part.
before downstream task fine-tuning. To make sure
LMs capture the information carried by the num-
bers, we keep the numbers in the questions instead
of masking them during both pre-training and fine-
tuning. Instead of making LMs conduct computa-
tion internally, MSATencourages LMs to generate
a series of intermediate steps leading to the answer.
Experiments on four math word problem datasets
with two backbone models demonstrate the effec-
tiveness of our method in enhancing LMs’ math
reasoning performance.
2 Method
Our method essentially appends a continual pre-
training stage before fine-tuning LMs on down-
stream tasks. The continual pre-training serves
two purposes: first, we tokenize numbers digit-by-
digit to improve LMs’ numerical comprehension;
second, we make LMs learn multi-step reasoning
skills from the proposed synthetic task.
2.1 Digit tokenization for numbers
Sub-word tokenization methods, e.g., byte pair en-
coding (BPE) (Sennrich et al., 2016), is one of the
reasons why moderated-sized LMs poorly under-
stand numbers (Wallace et al., 2019). BPE-based
tokenizers split text based on the token frequency in
the training corpus, which can be counter-intuitive
when dealing with numbers. For example, numbers
"520" and " 521" will be tokenized into [" 520"] and
["5", "21"] respectively by the RoBERTaTokenizer2
of the Transformers library (Wolf et al., 2020).
Such inconsistent tokenization strategy for num-
bers undermines LM’s numerical understanding
ability. Hence, we tokenize numbers digit-by-digit
for both pre-training and fine-tuning.
2https://huggingface.co/docs/transformers/
model_doc/roberta2.2 Multi-step Arithmetic Tasks (M SAT)
The core of our method is the synthetic task MSAT
where LMs can learn multi-step reasoning skills.
Like MWP tasks, MSATcan be formulated as a
Seq2Seq task: the input of a MSATexample de-
scribes an arithmetic question, while the output is
a reasoning chain leading to the answer. Specif-
ically, each input sequence is composed of three
components: question context ,equation , and ques-
tion variable . Equation is a sequence of symbols
and operators ( +,−,×,÷,=) that builds equality
relationship between symbols. Given an equation,
only one of the symbols is set as the question vari-
able, while other symbols will be listed in question
context with their numerical values.
The output sequence of MSATis constructed
in a code-style multi-step reasoning format. Each
step consists of two sub-steps: variable assignment
andcalculation . In variable assignment, numbers
appear in the input sequence are assigned to the
variable names that are exclusive for decoder. In
calculation, a new variable is generated from the
calculation of the existing variables. This makes
our outputs become executable Python code so that
the numerical answer can be calculated by an ex-
ternal Python interpreter. Both inputs and outputs
ofMSATare generated purely automatically. De-
tails about the construction of MSATare provided
in Appendix A.1.
2.3 Pre-training via adapter-tuning
Directly training on synthetic data that are largely
different from the natural language corpus harms
LMs’ language prowess (Geva et al., 2020). There-
fore, we adopt a two-stage tuning strategy (Wang
and Lu, 2022) to inject reasoning skills into LMs.
Specifically, we perform adapter-tuning (Houlsby
et al., 2019) on MSATand then jointly fine-tune
adapter and LM backbone on downstream tasks.
It mitigates catastrophic forgetting because LM’s
original parameters are largely preserved during
adapter-tuning (Houlsby et al., 2019).
We consider two backbone models to verify the
effectiveness of our method. In particular, we se-
lect a sequence-to-sequence (Seq2Seq) model (Lan
et al., 2021) and a directed acyclic graph (DAG)
structured model (Jie et al., 2022) that both adopt
RoBERTa baseto encode the input questions. More
details of these models are provided in §3.1. Fig-
ure 2 shows an overview of the proposed pre-
training method.

--- PAGE 3 ---
ModelMA WPS ASDiv-A SV AMP SV AMP (hard)
Acc.∆ Acc.∆ Acc.∆ Acc. ∆
Large language models (PaLM 540B) (code-davici-002) (PaLM 540B)
w/ Chain-of-Thought prompting 93.3 80.4 79.0 -
Seq2Seq models
ROBERT AGEN(Lan et al., 2021)
w/ symbolic masks 88.4 72.1 30.3 30.3♡
w/ digit tokenization 84.1 ( -4.3) 71.9 ( -0.2) 27.6 ( -2.7) 19.6 ( -10.7)
MSAT-R OBERT AGEN(OURS) 91.6 (+3.2) 81.8 (+9.7) 39.8 (+9.5) 36.2 (+5.9)
DAG structured models
DEDUCT REASONER (Jie et al., 2022)
w/ symbolic masks 92.0 85.0 45.0 45.0♡
w/ digit tokenization 91.6 ( -0.4) 84.1 ( -0.9) 44.4 ( -0.6) 42.8 ( -2.2)
MSAT-D EDUCT REASONER (OURS)94.3 (+2.3) 87.5 (+2.5) 48.9 (+3.9) 48.2 (+3.2)
Table 1: Accuracy (%) comparison between large language models (LLMs), backbone model baselines, and our
method. ∆: performance gap compared with the symbolic mask baselines. ♡: For baselines with symbolic masks,
performance on SV AMP (hard) is the same as SV AMP because the actual numbers are replaced by symbolic tokens.
The results of LLMs with chain-of-thought prompting are from Wei et al. (2022).
3 Experiments
Now we investigate whether our pre-training
method facilitates models on Math Word Problem
(MWP) solving tasks. All results are averaged over
three different runs.
3.1 Experimental setup
Existing datasets We consider three commonly-
used MWP datasets: MAWPS (Koncel-Kedziorski
et al., 2016), ASDiv-A (Miao et al., 2020), and
SV AMP (Patel et al., 2021). The statistics of these
datasets is provided in Table 2. More details can be
found in Appendix A.2. We report five-fold cross-
validation results for both MAWPS and ASDiv-A
and test set accuracy for SV AMP following previ-
ous practice (Lan et al., 2021; Jie et al., 2022).
SV AMP (hard) We find more than 85% of the
numbers in the above datasets are smaller than 102.
To investigate the extrapolation performance of the
models trained with MSAT, we create SV AMP
(hard) from the original SV AMP dataset by re-
placing the numbers with much larger ones in-
spired by Gao et al. (2022). More details about
SV AMP (hard) and number distribution of the ex-
isting datasets are provided in Appendix A.3.
Dataset # DataAvg. input Avg. output
length reasoning steps
MAWPS 1,987 30.3 1.4
ASDiv-A 1,217 32.3 1.2
SV AMP 1,000 34.7 1.2
Table 2: Existing dataset statistics.Models We consider both sequence-to-sequence
(Seq2Seq) models and directed acyclic graph
(DAG) structured models as our backbone mod-
els. For Seq2Seq model, we choose ROBERT A-
GEN(Lan et al., 2021), an encoder-decoder model
with RoBERTa baseas the encoder combined with a
Transformer decoder. For DAG structured model,
we choose DEDUCT REASONER (Jie et al., 2022)
that combines RoBERTa basewith a DAG decoder.
In their original implementation, both models re-
place numbers with symbolic mask tokens. Hence,
we additionally consider a baseline for each back-
bone model that uses actual numbers with digit
tokenization. We name the models that are based
on these two backbone models and pre-trained with
our method as MSAT-R OBERT AGENandMSAT-
DEDUCT REASONER respectively. We also com-
pare our models to large LMs, e.g., PaLM (Chowd-
hery et al., 2022) and Codex (Chen et al., 2021),
with chain-of-thought prompting (Wei et al., 2022).
All models are evaluated via greedy decoding.
More implementation details, e.g., training hyper-
parameters, are provided in Appendix B.
3.2 Main results
Table 1 compares our models with backbone model
baselines and large LMs. On all datasets, digit
tokenization baselines consistently perform worse
than their symbolic mask counterparts, indicating
the deficiency of the numeracy comprehension of
the original RoBERTa model. However, the models
trained with MSATsurpass both baselines by a
large margin, which demonstrates the effectiveness
of our pre-training method.

--- PAGE 4 ---
2 4 6 8 10 12
Pre-training steps (thousand)020406080100MsAT accuracy
MsAT acc.
SVAMP acc.
36404448
SVAMP accuracy
Figure 3: Performance on MSATand SV AMP with
respect to the pre-training steps. Results are obtained
from 3 different runs.
SV AMP (hard) We can observe that, on SV AMP
(hard), the accuracies of digital tokenization base-
lines decrease dramatically (10.7 points drop for
ROBERT AGENand 2.2 points drop for DEDUC -
TREASONER ) compared with baselines with sym-
bolic masks, while the models trained with MSAT
still outperforms symbolic mask baselines by 5.9
and 3.2 points respectively. This shows that not
only does our models obtain better results than the
baselines on the existing tasks, but it is also more
robust in handling out-of-distribution numbers.
Compare with large language models We also
observe that, on relatively simple tasks, i.e.,
MAWPS and ASDiv-A, RoBERTa-based models
can outperform large LMs. But for the more chal-
lenging task SV AMP, there is still a large perfor-
mance gap. We believe this is because SV AMP
requires models to have a better understanding of
natural languages. Jie et al. (2022) also reports
that varying LM encoders results in significant per-
formance disparities on SV AMP, indicating that
SV AMP performance is closely tied to model’s nat-
ural language capabilities.
4 Pre-training analysis
In this section, we provide a careful analysis of our
pre-training method from various perspectives to
understand why it works.
4.1 Pre-training task performance
We visualize how the performance of pre-training
task MSATand one of the MWP tasks SV AMP
changes with pre-training steps in Figure 3. It can
be observed that the performance on both synthetic
and natural language tasks tends to improve gradu-
ally as the number of pre-training steps increases.
Figure 3 demonstrates that LMs are capable of
learning multi-step reasoning gradually from the
synthetic task MSAT. The acquired multi-step rea-
020406080100Accuracy (%)
9.163.396.5MsAT
10152025303540
10.128.639.8SVAMP
Answer only Math expression Code-style expressionFigure 4: Comparison between different output expres-
sion formats. Results are obtained from our Seq2Seq
model (with code-style expressions) and its variants.
soning ability can subsequently be transferred to
the downstream MWP solving tasks, enhancing
performance during the fine-tuning phase.
4.2 Reasoning format of M SAT
The reasoning format of MSATdictates the spe-
cific reasoning skills that LMs will acquire during
pre-training. We demonstrate the superiority of our
code-style multi-step reasoning format by compar-
ing it with two different reasoning expressions.
Effect of producing intermediate steps While it
is a common practice to train LMs towards directly
producing the numerical answers of the arithmetic
questions (Geva et al., 2020; Pi et al., 2022), a re-
cent work shows that LMs’ arithmetic skills are not
reliable (Razeghi et al., 2022). To explore whether
LMs can learn reasoning skills from MSATwithout
intermediate steps, we pre-train LMs on a variant of
MSATby replacing step-by-step output sequences
with only numerical answers. Figure 4 compares
this model (answer only) with our model (code-
style). Its poor performance on both MSATand
SV AMP confirms the necessity of producing inter-
mediate reasoning steps during pre-training.
Structured code-style expression We next in-
vestigate the importance of applying the structured
code-style reasoning expressions by comparing it
with the less formatted math expressions. We argue
that, compared with math expressions that only con-
tain numbers and operators, our code-style expres-
sions are more suitable for multi-step reasoning due
to the structure information in the output sequences.
Our experiments in Figure 4 demonstrate the su-
periority of the code-style output expressions. We
can see that models with math expressions perform
consistently worse than models with code-style
multi-step reasoning format on both pre-training
task M SAT and MWP solving task SV AMP.

--- PAGE 5 ---
1.5 2.0 2.5
Pre-training difficulty90.090.490.891.291.6MAWPS acc.
MAWPS acc.
MAWPS difficulty
1.5 2.0 2.5
Pre-training difficulty80.080.480.881.281.682.0ASDiv-A acc.
ASDiv-A acc.
ASDiv-A difficultyFigure 5: Performance on MAWPS and ASDiv-A with
respect to pre-training difficulty. The difficulty levels of
two MWP tasks are also added for reference.
4.3 Difficulty level of M SAT
Leveraging synthetic data for pre-training provides
the advantage of enabling highly customizable dif-
ficulty levels for the training data. Here we define
the difficulty level of a reasoning task as the aver-
aged reasoning steps that are required to solve the
problems. From Figure 5, we see that pre-training
LMs on MSATs that are harder than downstream
tasks generally leads to better results. It’s important
to note that, broadly speaking, the difficulty level
of a reasoning task, particularly those involving
natural language, is not solely determined by the
number of reasoning steps. One example is that,
though both ASDiv-A and SV AMP have an aver-
aged reasoning steps of 1.2 (see Table 2), SV AMP
is considered more difficult as it requires high-level
natural language understanding (Patel et al., 2021).
4.4 Perform adapter-tuning on M SAT
Tuning all parameters of LM encoders on synthetic
data that are largely different from the pre-training
corpus may lead to catastrophic forgetting (Geva
et al., 2020). To explore the importance of per-
forming adapter-tuning on MSAT, we create a vari-
ant of our method in which we perform full fine-
tuning on MSAT. We compare this variant with
our models in Figure 6. It can be observed that
both full fine-tuning and adapter-tuning can achieve
good performance on MSAT, but adapter-tuning
outperforms fine-tuning on all downstream MWP
datasets, which demonstrates the benefits of per-
forming adapter-tuning on M SAT.
5 Related Work
In this work, we focus on improving moderate-
sized LM’s MWP performance by injecting multi-
step reasoning ability. Hence, our work closely
relates to both reasoning ability injection (Geva
et al., 2020; Pi et al., 2022) and MWP solving (Xie
and Sun, 2019; Patel et al., 2021; Jie et al., 2022).
92949698Accuracy (%)9696.5MsAT
86889092
86.791.6MAWPS
7476788082
75.581.8ASDiv-A
3234363840
34.539.8SVAMP
Full fine-tuning Adapter-tuningFigure 6: MSATand downstream task performance
comparison between full fine-tuning and adapter-tuning
during pre-training.
Reasoning skills injection This technique refers
to continually pre-training LMs on certain
intentionally-crafted tasks to enhance their reason-
ing abilities. GenBERT (Geva et al., 2020) pre-
trains LMs on templated-based synthetic data to
inject numerical skills into the LMs. PoET (Pi
et al., 2022) improves LMs’ reasoning ability by
pre-training them on tabular data towards imitating
program executors. Both methods involve training
LMs to produce numerical answers directly, which
can be unreliable (Razeghi et al., 2022). Our work
focuses on injecting into LMs the capability for
solving complex arithmetic problems step-by-step.
Solving MWP with specialized architectures
One of the research lines of MWP solving focuses
on designing specialized achiectures for math rea-
soning (Xie and Sun, 2019; Lan et al., 2021; Jie
et al., 2022). For example, Lan et al. (2021) com-
bines RoBERTa (Liu et al., 2019) with a Trans-
former (Vaswani et al., 2017) decoder, and Jie et al.
(2022) augments encoder-only LMs with a directed
acyclic graph decoder. One of the shortages of such
models is the information loss caused by masking
actual numbers in the questions with symbolic to-
kens (Wu et al., 2021). In this work, we propose
to represent actual numbers with digit tokenization,
and improve models’ multi-step reasoning ability
by pre-training them on a synthetic task M SAT.
6 Conclusion
We propose a novel synthetic pre-training task,
MSAT, to incorporate LMs with multi-step reason-
ing skills that improve performance on MWP tasks.
This pre-training task encourages LMs to generate
intermediate reasoning steps instead of predicting
final numerical answers directly. Our experiments
show that the proposed method is effective in im-
proving the moderate-sized LM’s performance on
MWP solving tasks.

--- PAGE 6 ---
Limitations
Limited number of operators considered Fol-
lowing previous methods (Lan et al., 2021), we
only consider binary operators ( +,−,×, and÷).
As we adopt a code-style output format, it is pos-
sible to introduce other non-binary operators sup-
ported by the Python interpreter, e.g., sum() and
max() . However, obtaining labeled data with such
operators may require laborious efforts. We believe
it is an interesting research question on exploring
how to teach models to solve practical questions
e.g., math word problems, by writing code in a
low-resource setting (Jie and Lu, 2023).
Limited performance due to greedy decoding
All the results we report in this work are produced
via greedy decoding. A recent work (Wang et al.,
2023) reports that making large LMs generate mul-
tiple answers and selecting the answer with the
most votes can boost performance by a large mar-
gin. However, performing beam search for sym-
bolic neural reasoners, e.g., DeductReasoner, can
be challenging in that searching space increases ex-
ponentially with the number of variables in the
question (Jie et al., 2022). Designing effective
beam search strategies for symbolic neural reason-
ers is a promising direction.
Acknowledgements
We would like to thank the anonymous review-
ers, our meta-reviewer, and senior area chairs for
their insightful comments and support with this
work. We would also like to thank members
of our StatNLP research group for helpful dis-
cussions. This research/project is supported by
the National Research Foundation Singapore and
DSO National Laboratories under the AI Singa-
pore Program (AISG Award No: AISG2-RP-2020-
016), and Ministry of Education, Singapore, un-
der its Academic Research Fund (AcRF) Tier 2
Programme (MOE AcRF Tier 2 Award No: MOE-
T2EP20122-0011)
References
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374 .Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2022. Pal: Program-aided language
models. arXiv preprint arXiv:2211.10435 .
Mor Geva, Ankit Gupta, and Jonathan Berant. 2020.
Injecting numerical reasoning skills into language
models. In Proceedings of ACL .
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021. Measuring mathematical
problem solving with the math dataset. In Proceed-
ings of NeurIPS .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for nlp. In Pro-
ceedings of ICML .
Zhanming Jie, Jierui Li, and Wei Lu. 2022. Learning to
reason deductively: Math word problem solving as
complex relation extraction. In Proceedings of ACL .
Zhanming Jie and Wei Lu. 2023. Leveraging training
data in few-shot prompting for numerical reasoning.
InFindings of ACL .
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate
Kushman, and Hannaneh Hajishirzi. 2016. Mawps:
A math word problem repository. In Proceedings of
NAACL .
Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan,
Bing Tian Dai, Yan Wang, Dongxiang Zhang, and
Ee-Peng Lim. 2021. Mwptoolkit: An open-source
framework for deep learning-based math word prob-
lem solvers. arXiv preprint arXiv:2109.00799 .
Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin,
Yunshi Lan, Jie Shao, and Xiangliang Zhang. 2022.
MWP-BERT: Numeracy-augmented pre-training for
math word problem solving. In Findings of NAACL .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In Proceedings of ICLR .

--- PAGE 7 ---
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
2020. A diverse corpus for evaluating and developing
english math word problem solvers. In Proceedings
of ACL .
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, et al. 2019. Pytorch: An imperative style,
high-performance deep learning library. In Proceed-
ings of NeurIPS .
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are NLP models really able to solve simple
math word problems? In Proceedings of NAACL .
Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin,
Yan Gao, Qiang Fu, Jian-Guang Lou, and Weizhu
Chen. 2022. Reasoning like program executors. In
Proceedings of EMNLP .
Yasaman Razeghi, Robert L Logan IV , Matt Gardner,
and Sameer Singh. 2022. Impact of pretraining term
frequencies on few-shot reasoning. In Proceedings
of ICML .
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of ACL .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of NeurIPS .
Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,
and Matt Gardner. 2019. Do NLP models know num-
bers? probing numeracy in embeddings. In Proceed-
ings of EMNLP-IJCNLP .
Tianduo Wang and Wei Lu. 2022. Differentiable data
augmentation for contrastive sentence representation
learning. In Proceedings of EMNLP .
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, and Denny Zhou. 2023. Self-consistency im-
proves chain of thought reasoning in language mod-
els. In Proceedings of ICLR .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. In Proceedings of NeurIPS .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, et al. 2020. Transformers:
State-of-the-art natural language processing. In Pro-
ceedings of EMNLP .
Qinzhuo Wu, Qi Zhang, Zhongyu Wei, and Xuanjing
Huang. 2021. Math word problem solving with
explicit numerical values. In Proceedings of ACL-
IJCNLP .
Zhipeng Xie and Shichao Sun. 2019. A goal-driven
tree-structured neural model for math word problems.
InProceedings of IJCAI .A Additional information about datasets
In this section, we provide additional details about
the datasets that we used in the experiments.
A.1 Construction of M SAT
The proposed MSATis a synthetic Seq2Seq task
where the inputs describe arithmetic questions and
outputs are the solutions represented by a code-
style multi-step reasoning format. Both inputs and
outputs of MSATcan be generated automatically.
To construct an example of MSAT, we first gener-
ate the input sequence and then produce the output
solution accordingly. In all, we generate 85,000
examples and split them into 80,000 and 5,000 for
training and evaluation respectively.
Input sequence construction We start by prepar-
ing a set of equation templates and each equation
template contains no more than 3 binary operators
(+,−,×, and÷). By enumerating the possible
combinations of operators, we obtain 4+42+43=
84equation templates in total. The first step to con-
struct an input arithmetic question is to instantiate
an equation from an equation template. For exam-
ple, given an equation template " <Num0> + <Num1> =
<Num2> ", we assign each variable a value that makes
the equality hold and a variable name selected from
the capitalized letters. The numbers in the ques-
tions are sampled from 0to10,000 . The last step is
to randomly pick a variable as the question variable.
Therefore, the resulting input arithmetic question
may look like: " A=1. C=3. A+B=C. B? "
Output sequence construction Given an equa-
tion and a question variable, the output is first con-
structed as a math expression leading to the value
of the question variable. Notice that an equation
can be represented as a binary tree where the vari-
ables are the terminal nodes and operators are the
non-terminal nodes. Hence, the output can be pro-
duced by a "tree inversion" algorithm (see Figure 7)
from an equation and a question variable.
=+Num0Num1Num2=Num1-Num2Num0
Figure 7: An illustration of the "tree inversion" algo-
rithm that produces an output expression from an arith-
metic question. The question variable is highlighted.

--- PAGE 8 ---
020406080
65
27
8MAWPS
020406053
35
12ASDiv-A
020406053
32
15SVAMP
0204060
162658SVAMP (hard)Portion (%)
<20 20~100 >100Figure 8: Number distribution for different datasets.
A.2 Existing datasets
MA WPS (Koncel-Kedziorski et al., 2016) It is
a popular benchmark dataset for math word prob-
lems. We use the five-fold split provided by Lan
et al. (2021) for evaluation.
ASDiv-A (Miao et al., 2020) This is an English
math word problem task containing various linguis-
tic patterns and problem categories. We obtain the
data and five-fold split from Patel et al. (2021).
SV AMP (Patel et al., 2021) It is a challenge set
created for MWP model robustness evaluation. The
examples in SV AMP are from ASDiv-A with de-
liberately designed variations. Such variations in-
clude: changing questions, adding irrelevant infor-
mation, etc. Following the evaluation protocol sug-
gested by Patel et al. (2021), we train our models
over 3,138 training examples from a combination
of MAWPS and ASDiv-A.
A.3 SV AMP (hard)
SV AMP (hard) is used to evaluate models’ extrap-
olation ability on the out-of-distribution numbers.
We sample numbers from from 10to10,000 , a sig-
nificantly different range from the original one,
to replace the original numbers in SV AMP. Every
question in SV AMP (hard) corresponds to a ques-
tion in SV AMP. Although it is straightforward to
sample a large number and use it to replace the
numbers, we expect the created questions to make
sense. We achieve this by making sure the new
numerical results have the same type as the orig-
inal ones. For example, if the original numerical
answer is a positive integer, then we make sure the
new numerical answer is also a positive integer. We
compare the number distribution of existing MWP
datasets and SV AMP (hard) in Figure 8.B Implementation details
Our method is implemented in Python 3.8 with
HuggingFace’s Transformers (Wolf et al., 2020)
and PyTorch (Paszke et al., 2019) libraries. All
experiments can be conducted on one NVIDIA
RTX 6000 GPU with 22 GB memory.
B.1 Backbone Model implementation
For our MSAT-R OBERT AGEN and MSAT-
DEDUCT REASONER , we build the backbone mod-
els following the implementation provided by Lan
et al. (2021) and Jie et al. (2022) respectively. The
encoders for both models are initialized with the
pre-trained weights of RoBERTa base. The adapter
modules (Houlsby et al., 2019) are added to each
layer of the encoders with a bottleneck dimension
of 64. More details about the mdoel architectures
are provided in Table 3.
ROBERT AGEN DEDUCT REASONER
# Params. 139.71 M 142.40 M
# Attention heads 8 -
Hidden dim. 768 768
Feedforward dim. 1024 768
# Layers 2 -
Activation ReLU ReLU
Dropout 0.1 0.1
Label smoothing 0.05 -
# Constants 17 17
Table 3: Hyperparameters of model architectures.
B.2 Training configurations
PRE-TRAINING FINE-TUNING
Batch size 32 16
Max steps 10,000 50,000
Optimizer AdamW (Loshchilov and Hutter, 2019)
Weight decay 0.01 0.01
Max grad norm 0.1 1.0
Learning rate 3e-5 1e-5
LR scheduler Linear Linear
Table 4: Pre-training and fine-tuning hyperparameters.
