# 2402.06332.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/math/2402.06332.pdf
# File size: 1614922 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Technical Report
InternLM-Math: Open Math Large Language Models Toward
Verifiable Reasoning
Huaiyuan Ying1,2*, Shuo Zhang1,3∗, Linyang Li3, Zhejian Zhou1,4, Yunfan Shao1,3
Zhaoye Fei1,3, Yichuan Ma1, Jiawei Hong1,3, Kuikun Liu1, Ziyi Wang1, Yudong Wang1
Zijian Wu1,5, Shuaibin Li1, Fengzhe Zhou1, Hongwei Liu1, Songyang Zhang1
Wenwei Zhang1, Hang Yan1, Xipeng Qiu3, Jiayu Wang1, Kai Chen1, Dahua Lin1
1Shanghai AI Laboratory
2Tsinghua University
3Fudan University, School of Computer Science
4University of Southern California
5Shanghai Jiaotong University
internlm@pjlab.org.cn
Abstract
The math abilities of large language models can represent their abstract
reasoning ability. In this paper, we introduce and open-source our math
reasoning LLMs InternLM-Math which is continue pre-trained from In-
ternLM2. We unify chain-of-thought reasoning, reward modeling, formal
reasoning, data augmentation, and code interpreter in a unified seq2seq
format and supervise our model to be a versatile math reasoner, verifier,
prover, and augmenter. These abilities can be used to develop the next
math LLMs or self-iteration. InternLM-Math obtains open-sourced state-
of-the-art performance under the setting of in-context learning, supervised
fine-tuning, and code-assisted reasoning in various informal and formal
benchmarks including GSM8K, MATH, Hungary math exam, MathBench-
ZH, and MiniF2F. Our pre-trained model achieves 30.3 on the MiniF2F
test set without fine-tuning. We further explore how to use LEAN to solve
math problems and study its performance under the setting of multi-task
learning which shows the possibility of using LEAN as a unified platform
for solving and proving in math. Our models, codes, and data are released
athttps://github.com/InternLM/InternLM-Math .
Demo: https://huggingface.co/spaces/internlm/internlm2-math-7b
1 Introduction
Large language models (Brown et al., 2020; Lewkowycz et al., 2022b; Taylor et al., 2023;
OpenAI, 2023; Anil et al., 2023; InternLM, 2023; Azerbayev et al., 2023b; Google, 2023; Shao
et al., 2024) have shown significant abilities in mathematical reasoning tasks from grade
school (Cobbe et al., 2021) to high school levels (Hendrycks et al., 2021a) by using chain-of-
thought reasoning (Wei et al., 2022) or program-of-thought reasoning (Chen et al., 2023b;
Gao et al., 2023).
Building such models requires pre-training on math corpora and supervised fine-tuning
on math problems. We introduce InternLM-Math based on InternLM2-Base models1. In-
ternLM2 shows strong performance in various aspects including math, code, chat experience,
instruction following, and creative writing. We retrieve and collect math-related data to con-
tinue pre-training on InternLM2-Base and obtain state-of-the-art performance on informal
∗Equal contribution.
1https://github.com/InternLM/InternLM
1arXiv:2402.06332v2  [cs.CL]  24 May 2024

--- PAGE 2 ---
Technical Report
and formal math reasoning benchmarks outperforming Minerva (Lewkowycz et al., 2022b)
and Llemma (Azerbayev et al., 2023b).
During supervised fine-tuning, we supervise InternLM-Math not only on solving math
problems using chain-of-thought and code interpreters but also many tasks for developing
math LLMs which include reward modeling and augment helper. We also introduce using
LEAN for translating between natural languages and LEAN statements (Han et al., 2021),
solving easy math problems, and proving math statements. InternLM-Math series models
achieve open-sourced state-of-the-art performance on multiple benchmarks2and score
more than 90% relative to GPT-4 (OpenAI, 2023).
Our contributions including:
•We open-source our base and SFT LLMs in math reasoning. It achieves open-
sourced SOTA under the setting of ICL, SFT, RM reranking, and Python-assisted in
various benchmarks.
•We unify chain-of-thought reasoning, reward modeling, data augmentation, and
formal reasoning under a unified seq2seq format. We supervise our model with
both problem-solving and verification abilities.
•We propose reasoning interleaved with coding (RICO) and achieve state-of-the-art
math reasoning with Python’s assistance.
•We explore using LEAN to solve math word problems and we investigate its
performance concerning data size during multi-task learning.
Figure 1: 4-shot MATH performances with 256 times majority voting. Comparison is based
on our pre-trained base model, Llemma (Azerbayev et al., 2023b), and Minerva (Lewkowycz
et al., 2022b). The figure is modified from Azerbayev et al. (2023b).
2 Related Work
Math Pre-training Pre-training helps LLMs acquire computational and mathematical
knowledge from various sources, such as math corpora (Han et al., 2021; Lewkowycz et al.,
2022b; Paster et al., 2023; Wang et al., 2023d), problem sets (Lightman et al., 2023), and
synthetic data (Hendrycks et al., 2021b; Liu & Low, 2023; Yang et al., 2023b). ArXiv with
2This work is concurrent with Deepseek-Math.
2

--- PAGE 3 ---
Technical Report
abundant math contents is usually used in math pre-training (Lewkowycz et al., 2022a;
Taylor et al., 2023; Azerbayev et al., 2023a). Paster et al. (2023) extracts math web pages
from common crawl which can be complementary to arXiv. Math problems including
GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021a) are used during pre-
training (OpenAI, 2023). Synthetic problems generated via rules or math program scripts
(Hendrycks et al., 2021b) can also be used for pre-training. InternLM-Math collects pre-train
data from math corpora and synthetic data which establish its math ability.
Math Fine-tuning Building a stronger augmented chain-of-thought dataset (Yu et al.,
2023b; Yue et al., 2023; Li et al., 2023; Liu & Yao, 2024) for SFT to improve math reasoning
performance has received lots of interest. Problem augmentation (Luo et al., 2023; Yu et al.,
2023b; Li et al., 2023; Liu & Yao, 2024) and reasoning path augmentation (Zelikman et al.,
2022; Huang et al., 2022; Zhu et al., 2023; Yuan et al., 2023) are two common ways which
show significant effectiveness. Reinforcement learning (Uesato et al., 2022; Luo et al., 2023;
Wang et al., 2023b; Singh et al., 2023) has also been explored to fine-tune language models to
lift reasoning performance which requires a reward model (i.e. verifier) that can distinguish
correct and incorrect reasoning processes and answers. Compared to previous work, we
not only build a stronger chain-of-thought SFT dataset but also integrate multiple abilities
all-in-one including verification, proof, code interpreter, and data augment helper.
Math Verifier Reward models (verifiers) are usually used for reranking multiple candidate
reasoning paths. Outcome reward models (ORM) (Cobbe et al., 2021) and process reward
models (PRM) (Uesato et al., 2022; Lightman et al., 2023) leverage LLMs to verify the
correctness of answers and processes. ORM is less efficient than PRM (Lightman et al., 2023;
Wang et al., 2023b) while PRM requires many human experts for labeling. To reduce human
labor in labeling PRM, Yu et al. (2023a); Wang et al. (2023b) determine the correctness of
processes based on multiple completions from the process. Compared to previous work
which trained two separate networks for the policy network and the reward model. We
combine these two functions into one unified seq2seq format. After SFT, our model can be
used for solving problems or verifying reasoning paths. Furthermore, we explore leveraging
formal math language LEAN to verify reasoning paths by translating to LEAN codes.
Math Code Interpreter Code interpreters can complement LLMs with complex calculation
capabilities via various Python libraries. Early explorations use program-of-thought (Chen
et al., 2023b; Gao et al., 2023) but lack interaction between LLMs and tools, which may
not be able to handle scenarios that require multi-step reasoning and calculation. Recent
works (Gou et al., 2023; Wang et al., 2023a) try to more seamlessly integrate code interpreter
with reasoning by allowing the model to summarize the results based on the one-time code
execution outcomes. InternLM-Math explores reasoning interleaved with coding (RICO), i.e.,
the reasoning and coding processes are interleaved for multiple rounds until the problem is
solved, which is more natural and close to the problem-solving process and fully exploits
the reasoning capabilities of LLM.
Math Prover Solving a math problem with a correct answer by LLMs still cannot ensure
its process’s correctness. However, proving a statement using formal languages like Isabelle
(Paulson, 2000), LEAN (Moura & Ullrich, 2021), and Coq (The Coq Development Team,
2023) can promise it. Training an LLM to automated theorem proving in formal languages
(Han et al., 2021; Polu et al., 2022; Azerbayev et al., 2023a; Yang et al., 2023a; Welleck & Saha,
2023; Zheng et al., 2023) is hard due to sparse data. InternLM-Math achieves state-of-the-art
few-shot performances on MiniF2F (Zheng et al., 2021) which shows potential in building a
strong math prover.
3 Pre-training
In this section, we first describe our pre-training data composition. Then, we outline our
data post-processing method we perform on the training data. Finally, we dive into the
details of our training strategy.
3

--- PAGE 4 ---
Technical Report
Domain Dataset Unique Tokens(B) Epochs Tokens(B)
CC Retrieved Data knowledge pile 20 4 80
Special Domain Dataopen-web-math 6 4 24
algebraic-stack 4 4 16
others 1 4 4
Synthetic Data num 0.2 5 1
Total - 31.2 - 125
Table 1: Pre-train data usage for InternLM-Math. Unique Tokens refers to the number of
tokens in the original dataset. Tokens refers to the total number of tokens in the dataset
consumed during pre-training.
3.1 Pretrain data composition
To achieve a competitive performance in mathematical domains, we collected a diverse
collection of high-quality data. We do not leverage any LLM-generated data during pre-
training. This data is systematically categorized into the following categories:
CC Retrieved Data We employed Query of CC (Fei et al., 2024) to retrive the training
corpus for InternLM2-Math-Base. We select math-related corpus from Query of CC corpus
as the first part of continue pretraining data. This part includes 20B tokens.
Domain-Specific Data We selected from open-source dataset (Azerbayev et al., 2023b)
and in-house high-quality datasets in the field of mathematics including web pages, codes,
arXiv, forums, and books. This part includes 11B tokens.
Synthetic Data We synthesized numerical operation data to improve the model’s nu-
merical operation capabilities. We included five common operations including arithmetic,
exponentiation, logarithmic, trigonometric, and polynomial calculations. For numerical
operations, we traversed a set of commonly used values and randomly sampled a wider
range of values within 10-digit numbers. To prevent the model from overfitting to specific
templates, we diversely constructed templates to ensure the model’s numerical computing
capabilities generalize to some extent. This part includes 0.2B tokens.
3.2 Data post-processing
To enhance the quality of the training data, we follow the approach of Query of CC(Fei et al.,
2024) and implement a series of data post-processing strategies. Specifically, we trained a
scoring model to identify high-quality datasets. Subsequently, We used the Minhash-LSH
method for deduplication of the training data. In our practice, we filtered out duplicate data
with a similarity exceeding 0.7.
We further conducted exact formulation decontamination for the special domain data on the
MATH test set. We extract all formulations within a given paragraph. If the concatenation
of formulations hits any in the MATH test set, we simply remove them.
3.3 Training strategy
After collecting and post-processing high-quality retrieved and domain-specific data, we set
different training epochs. For details of the data ratios, see Table 1. In total, we collected
31.2 billion high-quality tokens across the datasets, we follow Muennighoff et al. (2023) to
use 4 epochs for most datasets. To continue pre-training on InternLM2-Base, we adopt the
same training strategy across different model sizes, as described in InternLM (InternLM,
2023), and we use InternEvo3as the training framework. During training, we used a
3https://github.com/InternLM/InternEvo
4

--- PAGE 5 ---
Technical Report
ProblemCOTORMProblemCOTPRMProblemCOTLEANProblem
CILEANProblemProblemLEANLEAN StateLEANNLStatementCOTCode InterpreterLEAN Calculation CodeLEAN StateLEAN TacticLEAN Calculation CodeCorrect / IncorrectStep K: Correct / IncorrectInternLM2-MathAUGProblemAUG ProblemProblemAUGAUG ProblemCOTCorrect/IncorrectProblemCOTNL StatementProblemRMCOTNL StatementLEAN StateLEANLEAN TacticLEAN
Figure 2: The left part shows the query and response formats in SFT. The right parts show
two possible usages of our SFT model. The upper right is a pipeline of synthesizing new
problems using our augment helper, COT, and RM abilities. The lower right is a pipeline of
solving informal problems using formal languages with COT and formal reasoning abilities.
context length of 4096. For documents that are too long or too short, we either truncate
or concatenate them to achieve the desired context length. We adopt mixed-precision
training with bfloat16 and FlashAttention2 (Dao, 2023) to attain optimal memory utilization
and training efficiency. The standard AdamW (Loshchilov & Hutter, 2017) optimizer was
employed with hyperparameters β1=0.9,β2=0.95, and weight decay =0.1. We use a
standard cosine learning rate scheduler. Specifically, the model’s learning rate reaches a
maximum of lrmax=3e−5 after 2000 warm-up steps, and then gradually decreases to
a minimum of lrmin=3e−6 over the course of training. In the continued pre-training
process, a total of 125 billion tokens were trained. For the 20B model, we early stop at 80
billion tokens based on in-context learning performance.
4 Supervised Fine-tuning
Dislike other math-specialized LLMs that focus on solving math problems, our models are
targeted to be math solvers and also be ready for self-improving which requires abilities
including problem augmentation, reward modeling, self-verifying, formal reasoning, and
code interpreters. Our SFT data contains high-quality human-written, rule-generated, and
LLM-generated data for the abovementioned abilities, the detailed data composition can be
seen in Table 16 and our models generated responses can be seen in Appendix D. We show
the query and response format of SFT in Figure 2.
Chain-of-thought We utilize MetaMath (Yu et al., 2023b) as our fundamental English
chain-of-thought data resource which brings consistent reasoning improvement to various
LLMs. We leverage in-house Chinese datasets for Chinese chain-of-thought abilities. To
improve the math reasoning abilities of our models’ weaknesses, we apply reasoning path
augmentation (Zelikman et al., 2022) on specific datasets. Inspired by Uesato et al. (2022),
we use final answers and our trained PRM to filter augmented reasoning paths.
Although we add synthesis calculation data in pre-training, the models still perform badly
on complex calculations. In the spirit of scratchpad (Nye et al., 2021; Liu & Low, 2023),
we decompose complex calculations including multiple brackets computation, decimal
computation, fraction computation and reduction, remainder problems, and power com-
putation into calculatable steps. We find simply adding scratchpad-like calculation data
will not influence the model calculation behavior. The reason comes from the format of
SFT data which requires the model immediately to generate the equation answer like
(12+17)3=24389 inside the chain-of-thought process, and we denote it as calculation
5

--- PAGE 6 ---
Technical Report
hallucination . To alleviate calculation hallucination , we match and rewrite such equations
((12+17)3=293=841∗29= (800+40+1)∗29=23200 +1160+29=24389) inside the
chain-of-thought processes as much as possible. This improves calculation ability during
chain-of-thought reasoning.
We also introduce several fundamental math abilities including checking prime, the game
of 24, factoring an integer, and calculating triangle areas. Our principle of synthesizing
these data is The model generates chain-of-thought based on an algorithm flow chart. The next
token prediction must be determined by generated tokens and not dependent on future tokens . Use
triangle area calculation as an example, we require the model to first check if three sides can
formulate a triangle and if it is a right triangle. The model does not need to assume it forms
a right triangle without chain-of-thought reasoning. Table 4 gives an example. We also
give an example response that does not follow our principle since predicting right triangle
requires the model to know future tokens 32+42=52.
Triangle Area
Query: Calculate the area of a triangle with length: 3,4,5.
Response: As 3 +4>5, they can form a triangle.
32=9, 42=16, 52=25, 9+16=25.
So 32+42=52, it is a right triangle.
The area is 3 ∗4/2=12/2 =6.
Bad Response: Since this triangle is a right triangle (32+42=52).
The area is 3 ∗4/2=12/2 =6.
Reward Modeling Reward modeling helps the model to better rerank its generation and
filter synthesis data for self-improving. Inspired by Math Shepherd (Wang et al., 2023b), we
unify ORM and PRM into the same format of seq2seq learning. We also add Chinese ORM
data using the model self-sampling chain-of-thought with corresponding final answers.
Formal Math Reasoning Instead of informal natural language problem solving, our data
also includes formal reasoning samples based on LEAN 3. Our target is to use LEAN as a
solver, verifier, and prover. We distill gpt-4-1106 to solve the GSM8K train set problems
using LEAN 3. We select all codes that can generate correct answers. This part includes
6705 samples. We split half into the format of generating LEAN based on the problem used
for training a solver and another half into the format of translating between LEAN and COT
used for training a verifier. The verifier translates COT to LEAN and can verify by checking
every calculation of COT based on the LEAN calculation. As a prover, the model needs to
translate between informal and formal statements and generate tactics based on the LEAN
state. We utilize the MathLib-train dataset (mathlib Community, 2020) extracted from Han
et al. (2021) as the prover training data.
Augmentation helper Another important aspect of our data is to help construct synthe-
sized data for self-improving. By rephrasing or augmenting a question, one can easily obtain
an enlarged question diversity (Luo et al., 2023; Yu et al., 2023b; Li et al., 2023). Translating
question-answer pairs into a natural language statement is a requirement of using formal
math language for proving.
5 Code Interpreter
Recent attempts (Chen et al., 2023b; Wang et al., 2023a; Gou et al., 2023) have explored
enhancing the complex calculation ability of LLMs by tools, where code interpreters have
become popular due to their flexibility and functionality backed by various Python libraries.
Early explorations use programs as a kind of thought strategy (Chen et al., 2023b; Gao et al.,
2023) but are unsuitable for multi-step reasoning and calculation since LLMs cannot see
6

--- PAGE 7 ---
Technical Report
Figure 3: The example of reasoning interleaved with coding (RICO) and conventional chain-
of-thought.
the code execution results. Recent works (Wang et al., 2023a; Gou et al., 2023) try to more
seamlessly integrate code interpreters with reasoning but are incompatible with and require
extra modification for general chat services.
We solve the above-mentioned issue by letting LLMs do reasoning interleaved with coding
(RICO), where LLMs conduct reasoning in the same format as chat response and adopt a
general tool-calling protocol to use the code interpreter. Such a design not only allows the
full exploitation of the existing reasoning capability of LLMs when using code interpreters
but also allows a direct full integration of tool-calling capability into the chat model. Thus,
different capabilities (such as tool calling and chat) of a model can provide service in a
universal tool-augmented system, which we believe to be more similar to that of GPT-4
(OpenAI, 2023).
Specifically, as shown in Figure 3, when answering a math problem, we prompt the model to
conduct symbolic reasoning and program generation and then observe the code execution
results in each round. The model will continue such rounds until it fully answers the
problem after summarizing the results, unlike previous methods(Wang et al., 2023a; Gou
et al., 2023) that essentially write code once. The reasoning process uses the same format as
a general chat response instead of using different marks for both text and code (Wang et al.,
2023a). Such a design allows the model to fully exploit its reasoning ability learned in the
conventional SFT corpus when using the code interpreter. The program generation can be
regarded as a general tool calling, unlike ToRA(Gou et al., 2023), which embeds the code
in the text response by markdown syntax. This resolves the ambiguity in the markdown
symbol of the code block when deploying the LLMs in a tool-augmented chat system.
The construction of the training data for the math code interpreter adopts an iterative
data update and hard example mining strategy to reduce the reliance on GPT-4. At each
7

--- PAGE 8 ---
Technical Report
iteration, we first use the model trained in the previous iteration to generate responses on
the train set of GSM8K and MATH. Since the model cannot fully fit the train set, we use
GPT-4-turbo to generate responses on the remaining train set once. The correct responses
generated by the most recent model and GPT-4-turbo will be used to train a new model for
the next iteration. The initial data is generated by ToRA-70B (Gou et al., 2023), which is not
ideal due to format differences but can be converted to correct responses to train the initial
model. InternLM2-Chat and InternLM2-Math models adopt the same training data for code
interpreter capability.
6 Experiments
6.1 Pre-train Performance
To validate the performances of our pretrained base models, we use the standard benchmark
for math informal reasoning: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al.,
2021a) and evaluate them using in-context learning. We adopt the few-shot templates from
OpenCompass (Contributors, 2023). We use majority voting (Wang et al., 2023c) accuracy as
the metric. The results are listed in Table 2. InternLM2-Math-Base models outperform their
initial checkpoints InternLM2-Base on both benchmarks which shows the effectiveness of
continue pre-training. InternLM2-Math-Base-7B obtains 21.5 on MATH which outperforms
Llemma-7B with 18.0. InternLM2-Math-Base-20B obtains 27.3 on MATH which outperforms
Llemma-34B and performs similarly with Minerva-62B with a smaller size.
Table 2: Compare pre-trained models using ICL. The metric is majority voting accuracy.
K=100 for the GSM8K benchmark and K=256 for the MATH benchmark. We use greedy
decoding when K=1. We sample our models using a temperature of 0.7 when K>1.
Benchmark GSM8K MATH
Model MAJ@1 MAJ@K MAJ@1 MAJ@K
Llama2-7B (Touvron et al., 2023) 14.6 - 2.5 -
Llemma-7B (Azerbayev et al., 2023b) 36.4 54.0 18.0 33.5
InternLM2-Base-7B 36.5 - 8.6 -
InternLM2-Math-Base-7B 49.2 75.7 21.5 35.6
Minerva-8B (Lewkowycz et al., 2022b) 16.2 28.4 14.1 25.4
InternLM2-Base-20B 54.6 - 13.7 -
InternLM2-Math-Base-20B 63.7 84.8 27.3 46.2
Llemma-34B 51.5 69.3 25.0 43.1
Minerva-62B 52.4 68.5 27.6 43.4
Minerva-540B 58.8 78.5 33.6 50.3
One pre-trained base model may have good ICL performance while performing mediocre
after SFT due to data overlap among pre-train and SFT. We perform SFT on different models
with the same SFT dataset MetaMath (Yu et al., 2023b) to check our models do not suffer
such phenomenon. We show results in Table 3. Using MetaMath for SFT, InternLM2-Math-
Base-7B still has superiority over Mistral-7B and Llemma-7B on the MATH benchmark.
InternLM2-Math-Base-20B outperforms Llemma-34B on both benchmarks.
To show the formal math reasoning ability of our model, we conduct ICL4on MiniF2F
benchmark (Zheng et al., 2021) which includes different level math problems in language
LEAN. LEAN can check whether the generated formal proof completes the statement’s
goals. We compare InternLM2-Math-Base with other pre-trained language models in Table 4.
InternLM2-Math-7B-Base solves 74 of 244 problems and achieves 30.3 which yields a new
state-of-the-art performance. InternLM2-Math-7B-Base and InternLM2-Math-20B-Base find
25 and 24 new proofs respectively which do not appear in the official MiniF2F repository5.
Like Gloeckle et al. (2023); Azerbayev et al. (2023b), we do not find the formal reasoning
4The setting is following https://github.com/wellecks/llemma formal2formal
5https://github.com/openai/miniF2F/blob/main/LEAN/src/test.LEAN
8

--- PAGE 9 ---
Technical Report
Table 3: Compare pre-trained models by fine-tuning on MetaMath dataset (Yu et al., 2023b).
The metric is greedy accuracy. †results come from Yu et al. (2023b). * results come from
Wang et al. (2023b).
Model GSM8K MATH
MetaMath-Llama2-7B (Touvron et al., 2023) † 66.5 19.8
MetaMath-Mistral-7B (Jiang et al., 2023) † 77.7 28.2
MetaMath-Llemma-7B (Azerbayev et al., 2023b) † 69.2 30.0
MetaMath- InternLM2-Math-Base-7B 76.4 33.8
MetaMath- InternLM2-Math-Base-20B 80.7 36.1
MetaMath-Llemma-34B * 75.8 34.8
performances scale with model parameter sizes. We leave it to the future work of how data
and parameter sizes influence formal reasoning performances.
Table 4: MiniF2F test set performance. The search budget is the same as Azerbayev et al.
(2023b) which is 1 ×32. We use 3-shot and LEAN 4 for base models.
Model Type Search MiniF2F-test
ReProver (Yang et al., 2023a) SFT - 26.5
LLMStep (Welleck & Saha, 2023) SFT 1 ×32 27.9
Code-Llama-7B (Rozi `ere et al., 2023) ICL 1 ×32 20.5
Code-Llama-34B ICL 1 ×32 22.1
Mistral-7B-v0.1 (Jiang et al., 2023) ICL 1 ×32 22.1
Mixtral-8x7B-v0.1 (Jiang et al., 2024) ICL 1 ×32 23.4
Llemma-7B (Azerbayev et al., 2023b) ICL 1 ×32 26.2
Llemma-34B ICL 1 ×32 25.8
Deepseek-coder-7B-v1.5-Base (Guo et al., 2024) ICL 1 ×32 28.7
Deepseek-math-7B-Base (Shao et al., 2024) ICL 1 ×32 28.3
InternLM2-7B-Base ICL 1 ×32 22.1
InternLM2-20B-Base ICL 1 ×32 25.4
InternLM2-Math-7B-Base ICL 1 ×32 30.3
InternLM2-Math-20B-Base ICL 1 ×32 29.5
6.2 SFT Performance
To show the performance of our SFT models, we conduct math reasoning using chain-of-
thought, reward modeling, formal reasoning, and code interpreter. We will also test several
abilities introduced in our SFT process including game-of-24 and prime checker.
6.2.1 COT Reasoning
We evaluate SFT models on GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021a),
Hungary math exam, and MathBench-ZH6using zero-shot chain-of-thought reasoning. We
use the Hungary math exam to test the model generalization ability and use MathBench-ZH
to examine Chinese math ability. MathBench-ZH contains 600 Chinese math problems from
primary school, middle school, high school, or university level. For each choice problem in
MathBench-ZH, we will shuffle the choice order 4 times. A model gives a correct answer 4
times can be considered as correct. We show results in Table 5. InternLM2-Math-7B achieves
78.1, 34.6, 55, and 40 on GSM8K, MATH, the Hungary math exam, and MathBench-ZH
respectively which show much stronger in-domain and out-of-domain performance at the
same model size. InternLM2-Math-7B also shows better performance compared to using
MetaMath for SFT which proves our SFT data can better activate the model’s reasoning
ability. InternLM2-Math-20B obtains 37.7 and 66 on MATH and Hungary math exam which
6https://github.com/open-compass/MathBench
9

--- PAGE 10 ---
Technical Report
is only behind GPT-4. It achieves state-of-the-art performance with a much smaller size
compared to Qwen-72B and DeepSeek-67B.
Table 5: Compare SFT models using zero-shot COT reasoning. The metric is greedy accuracy.
Model GSM8K MATH Hungary MathBench-ZH
Qwen-7B-Chat (Alibaba, 2023) 51.7 11.6 19 25.0
DeepSeek-7B-Chat (Bi et al., 2024) 63.0 15.8 28.5 12.7
InternLM2-Chat-7B 70.7 23.0 - 29.2
ChatGLM3-6B (Du et al., 2022) 53.8 20.4 32 15.2
MetaMath-Mistral-7B (Jiang et al., 2023) 77.7 28.2 29 -
MetaMath-Llemma-7B (Azerbayev et al., 2023b) 69.2 30.0 - -
InternLM2-Math-7B 78.1 34.6 55 40.0
InternLM2-Chat-20B 79.6 31.9 - 37.8
MetaMath-Llemma-34B (Azerbayev et al., 2023b) 75.8 34.8 - -
InternLM2-Math-20B 82.6 37.7 66 45.3
Qwen-72B-Chat (Alibaba, 2023) 78.9 35.2 52 47.8
DeepSeek-67B-Chat (Bi et al., 2024) 84.1 32.6 58 33.2
ChatGPT 80.8 34.1 41 21.5
GPT-4 (original version) 92.0 42.5 68 47.2
6.2.2 Reward Modeling
Reward models can be used for answer reranking to improve model performances (Cobbe
et al., 2021; Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023b). We will test
the performance of our reward modeling by ORM and PRM reranking. We use the same
SFT model for inference and reward model reranking. We further test using LEAN as a
reward model (LRM) by requiring the model to convert COT to LEAN codes and execute
LEAN codes. We will mark this COT as correct with LEAN codes and COT obtain the
same results. Notice that using LEAN as a reward model for tasks like GSM8K can only
check the calculation processes but not the logic processes. We test on GSM8K and MATH
(500 test problems), the results are plotted in Figure 4 and shown in Table 19. We find that
generally using PRM outperforms ORM, and ORM outperforms majority voting which is
consistent with (Lightman et al., 2023; Wang et al., 2023b). We find that using LEAN as
RM has a significant advantage in reranking GSM8K with 7B models, but the advantage
diminishes with 20B models. There are lots of improvement rooms between RM reranking
performances and oracle performances.
We also compare our performance with other RM reranking models in Table 6. Compared to
MetaMath-DeepSeek-67B reranking (Wang et al., 2023b), our 20B model uses fewer inference
times, a smaller model size, and achieves better performances with an accuracy of 50.0
which shows that our RM is effective. However, compared to GPT-4-MathMix (Lightman
et al., 2023), the performance gap is still large.
Table 6: Reranking performance on MATH(500) compared to other baselines.
Model Greedy Method Performance
InternLM-MATH-7B 34.6 PRM K=100 47.0
InternLM-MATH-20B 37.7 PRM K=100 50.0
MetaMath-Llemma-34B 34.8 PRM K=256 47.3
MetaMath-DeepSeek-67B 36.8 PRM K=256 48.1
GPT-4-MathMix - ORM K=100 71.0
GPT-4-MathMix - PRM K=100 74.5
GPT-4-MathMix - ORM K=1860 72.4
GPT-4-MathMix - PRM K=1860 78.2
10

--- PAGE 11 ---
Technical Report
Figure 4: Reranking performances using our reward models on GSM8K and MATH. Oracle
shows the upper bound performance which is calculated by Pass@K.
6.2.3 Formal Ability
Targeting a large language model to conduct verifiable math reasoning, multiple abilities are
required including translating informal problems or proof statements into formal statements
and solving or proofing formal statements using formal languages.
Formal Translation We first evaluate the ability to translate between natural language
and formal statements. We follow Azerbayev et al. (2023a) to translate statements extracted
from undergraduate mathematics textbooks. The results are listed in Table 7. While our
models outperform ProofGPT and their SFT version (Azerbayev et al., 2023a) in bidirectional
translation, we still lag behind CodeX (Chen et al., 2021) and GPT-4 (OpenAI, 2023). Similar
to formal reasoning, we do not find significant benefit from scaling parameters.
Table 7: Evaluate translating between LEAN and informal language statement. Results
except our models are copied from https://github.com/zhangir-azerbayev/ProofNet . The
metric is the BLEU-4 score.
Model Formalization Informalization
ProofGPT-1.3B (Azerbayev et al., 2023a) 8.1 5.1
ProofGPT-6.7B (Azerbayev et al., 2023a) 4.7 6.0
ProofGPT-1.3B back-translated (Azerbayev et al., 2023a) 10.7 -
CodeX (Chen et al., 2021) 25.1 13.2
GPT-4 (OpenAI, 2023) 27.1 -
InternLM2-Math-7B 15.0 9.4
InternLM2-Math-20B 15.7 10.2
Using LEAN to solve GSM8K LEAN is mainly used as a proof assistant to prove a
statement instead of solving a problem. To unify the solving and proving math problems,
we explore using LEAN to solve math problems. We require the model to generate comments
before LEAN codes and list an example in Table 6.2.3. We use LEAN 3 to compile and
output results as answers. Performances are listed in Table 8. Our 7B model can obtain
11

--- PAGE 12 ---
Technical Report
70.4 using LEAN to solve GSM8K, while our 20B model can achieve 73.9. We find that our
models perform worse than COT. The reasons that using LEAN underperforms COT are: (1)
a−b=0 (when a:N<b:N) (2) division: a/b= [a/b](when a:N,b:N). These cause
inconvenience in some basic calculations. However, we show the possibility of using LEAN
as a unified interface for solving and proving math problems.
Solve GSM8K use LEAN 3
Query: A robe takes 2 bolts of blue fiber and half that much white fiber. How many
bolts in total does it take?
Response:
-- Define the amount of blue fiber needed for one robe
def blue_fiber_per_robe : N:= 2
-- Define the amount of white fiber needed for one robe as half of the blue fiber
def white_fiber_per_robe : N:= blue_fiber_per_robe / 2
-- Calculate the total amount of fiber needed for one robe
def total_fiber_per_robe : N:= blue_fiber_per_robe + white_fiber_per_robe
-- Output the result
#eval total_fiber_per_robe -- This will evaluate to 3 bolts in total
Output: 3
Table 8: The performance on GSM8K using COT and LEAN.
Model COT LEAN
InternLM2-Math-7B 78.1 70.4
InternLM2-Math-20B 82.6 73.9
6.2.4 Code Interpreter
As shown in Table 9. InternLM2-Math-7B can already surpass all the previous open-source
models on MATH benchmark except InternLM2-Chat-20B. InternLM2-Math-20B outper-
forms InternLM2-Chat-20B and obtains the best results among open-source models. Since
InternLM2-Math and InternLM2-Chat adopt the same training data for code interpreters,
we believe the improvements of InternLM2-Math over InternLM2-Chat models result in the
improvement of math reasoning ability.
6.2.5 Other abilities
Game of 24 To test the ability on Game of 24, we use the test set from Yao et al. (2023).
When we sample once for each question, our models achieve an accuracy of 26 and 35
respectively for 7B and 20B model sizes, which greatly outperforms fine-tuned Llamma2-
7B and even GPT-4. The performance can match some multi-sampled methods, which
demonstrates the capabilities of our generated step-by-step searching process SFT data.
Prime checker For prime number verification, we sampled a test dataset containing 20
numbers for each digit number from 2 to 10 containing 10 prime and 10 composite numbers.
Our model can judge correctly for almost all the numbers, whatever their digits, this is
better than GPT-4 which performs worse when digits become bigger.
12

--- PAGE 13 ---
Technical Report
Table 9: Compare SFT models using Python Code Interpreter. The metric is greedy accuracy.
* This is tested on GPT-4 web version in 2023 Aug.
Model GSM8K MATH
DeepSeek-Coder-Instruct-7B (Guo et al., 2024) 62.8 28.6
MathCODER-CL-7B (Wang et al., 2023a) 67.8 30.2
DeepSeek-Coder-Instruct-1.5-7B (Guo et al., 2024) 72.6 34.1
ToRA-7B (Gou et al., 2023) 72.6 44.6
InternLM2-Chat-7B 77.9 45.1
InternLM2-Math-7B 79.4 50.9
MathCODER-CL-13B (Wang et al., 2023a) 74.1 35.9
MathCODER-CL-34B (Wang et al., 2023a) 81.7 45.2
ToRA-13B (Gou et al., 2023) 75.8 48.1
ToRA-Code-34B (Wang et al., 2023a) 80.7 50.8
InternLM2-Chat-20B 84.5 51.2
InternLM2-Math-20B 80.7 54.3
ToRA-70B (Gou et al., 2023) 84.3 49.7
GPT-4 Code Interpreter * (Zhou et al., 2023) 97.0 69.7
Table 10: The performance of Game-of-24.
Model Accuracy Sample times
Fine-tuned Llama2-7B (Yu et al., 2023a) 11 1
GPT-4 COT (Yao et al., 2023) 4 1
InternLM2-Math-7B 26 1
InternLM2-Math-20B 35 1
TOT (Yao et al., 2023) 45 12( b=1)
OVM (Yu et al., 2023a) 79 20
7 Discussion
7.1 Compare Pretrain Performance with Pretraining Token Amount
We pre-train on InternLM-Base-7B for 200B tokens to evaluate how many epochs should
we pretrain. We evaluate our models’ performance every 40B tokens using ICL and SFT on
MetaMath in Table 12. We find that after 80B tokens, the performance does not improve
significantly. When training longer to 200B (approximate 7 epochs) tokens, the performances
start to degenerate.
7.2 LEAN Performance on GSM8K
We introduce using LEAN to solve GSM8K problems previously. LEAN is sparse during
pre-training and fine-tuning compared to natural language, LATEX, and Python. We want
to understand how LEAN’s ability is regarded to the data size. We ablate the performance
of LEAN solving GSM8K using different amounts of SFT data in Table 13. We find if we
only use GSM8K-LEAN during SFT, the LEAN performance is worse than using GSM8K-
LEAN and MetaMath together. We also find that if we SFT without MetaMath, the LEAN
performance is sensitive to data amount. While if we train SFT with MetaMath, the LEAN
performance is less sensitive which shows that the model mainly needs to learn grammar
from GSM8K-LEAN data and learn reasoning ability from MetaMath. These findings
show the reasoning ability of MetaMath helps performance on LEAN with sparse data and
suggests a multi-task training strategy of these abilities.
13

--- PAGE 14 ---
Technical Report
Table 11: Prime number verification test by examining 10 prime numbers and 10 composite
numbers for digits from 2 to 10, all ending with 1, 3, 7, or 9 (Ending with other numbers are
composite numbers.). The results ‘a/b’ indicate the number of correct identifications, with
’a’ representing prime numbers and ‘b’ representing composite numbers.
Model 2 3 4 5 6 7 8 9 10
InternLM2-Math-7B 10/10 10/10 10/8 10/8 10/6 10/7 10/10 10/10 10/10
InternLM2-Math-20B 10/10 10/10 10/6 10/9 10/10 10/6 10/10 10/9 10/9
gpt-4-0125-preview 10/10 10/10 3/8 2/9 0/10 0/10 0/10 1/10 0/10
Table 12: The pretrain performance on different pretrain token amounts.
Benchmark GSM8K MATH
Tokens ICL SFT ICL SFT
40B 30.9 75.3 19.5 30.2
80B 38.7 77.4 21.4 30.7
120B 39.0 76.4 21.5 33.8
160B 38.4 75.8 21.4 31.9
200B 35.1 76.7 21.1 31.3
7.3 Data Ablation Study in SFT
To study the impact of the composition of our mixture data, we carried out ablation studies
for each kind of data in the final mixture. We include the MetaMath dataset for every
ablation study to keep basic abilities to solve math questions step by step. The evaluation
results are demonstrated in Table 14. Compared with using MetaMath for SFT, our InternLM-
Math-7B model and the model with COT data achieve better results on MATH and GSM8K.
The other compositions, though containing even much fewer tokens than MetaMath, lead
to undermined results on almost all test datasets. This may be attributed to format and
style transfer in code, reward model format, and other instructions. However, the SFT
data performs better than MetaMath + COT after integration, indicating the latent mutual
helpfulness of comprehensive data beyond their original special usage.
7.4 False positive in MATH BenchMark
For a math problem that requires an answer, one can always try some special cases to guess
the correct answers with incorrect processes. If the question is about counting, generated
answers may be sampled wrongly, and obtain a correct answer by chance. This could lead
us to overestimate the math reasoning ability of our models and other LLMs. We randomly
sample 5 samples for each difficulty level where the InternLM2-Math-20B gives a correct
answer in the MATH test set and human-labeled the predicted process of these 25 problems
to report this issue. Table 15 shows the results. One of the false positive cases writes:
False Positive Case Study
Query: How many ordered pairs of positive integers (m,n)satisfy gcd(m,n) = 2
and lcm [m,n] =108?
Response: I know that for any positive integers mand n,gcd(m,n)·lcm[m,n] =mn.
So, if I apply this formula to the given problem, I get 2 ·108=mn. This means that
mand nare factors of 216. I also know that gcd(m,n) = 2, which means that m
and nare both even, and neither of them is divisible by 4. So, I need to find the
number of ways to choose two even factors of 216 that are not divisible by 4. I
can list the even factors of 216 as follows: 2, 4, 6, 8, 9, 12, 18, 24, 27, 36, 54, 72, 108, 216.
Out of these, the ones that are not divisible by 4 are 2, 6, 9, 18, 27, 54, 108. I can pair
14

--- PAGE 15 ---
Technical Report
Table 13: Performance on GSM8K using COT and LEAN with different SFT datasets.
Setting COT LEAN
InternLM-Math-Base-7B 49.2 -
+ MetaMath SFT 76.4 -
+ Our SFT 78.1 70.4
GSM8K-LEAN1⁄1 - 58.0
GSM8K-LEAN1⁄2 - 53.1
GSM8K-LEAN1⁄4 - 35.1
GSM8K-LEAN1⁄8 - 38.4
GSM8K-LEAN1⁄1+ MetaMath 75.7 66.0
GSM8K-LEAN1⁄2+ MetaMath 75.6 65.0
GSM8K-LEAN1⁄4+ MetaMath 77.7 58.8
GSM8K-LEAN1⁄8+ MetaMath 77.4 53.4
Table 14: The ablation results of our SFT data composition.
Benchmarks MATH GSM8K
InternLM-Math-7B 34.60 78.09
Only MetaMath 33.78 76.35
MetaMath + COT 34.02 77.63
MetaMath + CI 33.56 77.63
MetaMath + Formal 32.12 76.12
MetaMath + RM 29.84 75.13
MetaMath + Augmenter 32.08 75.89
up these factors as follows: (2, 108 ),(6, 54),(9, 27),(18, 18 ). There are four pairs,
and each pair corresponds to a different ordered pair (m,n). For example, (2, 108 )
corresponds to (m,n) = ( 108, 2 ). So, there are four ordered pairs (m,n)that satisfy
the given conditions. Answer 4.
In this case, the model lists 9 and 27 as an even number, which leads to a false (m,n)pair
(9, 27). Moreover, the pairs (6, 54),(18, 18 ), though satisfying the condition given by model
prediction, do not meet the title requirements. The prediction just accidentally lists four
pairs as answers. However, if the evaluation takes only the outcome into consideration,
this sample will be ”correct”. Similar cases appear more frequently as the difficulty level
increases. Among these false positive cases, most problems have an integer as the answer,
especially a single-digit number, and the solution can be carried out without detailed
calculations or proof of cases. These make it easy for the model to speculate a correct answer
excluding step-by-step reasoning. Simply judging model abilities only by outcome accuracy
may be unilateral. Therefore, one of our future works is to use reward models or LEAN to
provide a better process checking.
Table 15: The Process Correctness of in MATH BenchMark using our 20B model.
Difficulty level Results correct Process correct
Level 1 5 5
Level 2 5 4
Level 3 5 4
Level 4 5 4
Level 5 5 3
15

--- PAGE 16 ---
Technical Report
8 Conclusions
We propose InternLM-Math as our first step toward a verifiable mathematical reasoning
ability. The excellent InternLM-Math-Base has the potential for versatile math reasoning
tasks. Despite the strong performance in informal and formal reasoning of InternLM-Math,
our model can be viewed as a starting point for self-improving. InternLM-Math integrates
COT and augment helper abilities can be used for synthesizing new problems and new
responses. InternLM-Math obtains the abilities of ORM, PRM, and LEAN can be used for
verifying the answers and processes of generated responses. We believe such verifiable data
augmentation will improve the model’s ability at high throughput.
Limitations
Chain-of-thought reasoning We match and rewrite the formulation in the spirit of scratch-
pad during COT while it introduces the following problems. The first problem is we
cannot easily match all equations and calculations that need to be rewritten (e.g. We solve
x3−3x2+3x−1=0, and obtain x=1). The second problem is multiple ‘=‘ induces
models to repeat more. The third problem is this sometimes generates naive step-by-step cal-
culations which may annoy end users and could be alleviated via implicit chain-of-thought
reasoning (Deng et al., 2023). We will solve such problems in the future work.
No self-critique ability Although our model can conduct ORM or PRM. We do not contain
any SFT data to let the model apply self-critique since such data can be hard to generate
and verify by any means. We will research how to generate self-critique SFT data with
verification.
Process reward modeling We find that our model does not have a significant PRM perfor-
mance which may be due to the confusing format among PRM and SFT and the unbalanced
distribution between positive processes and negative processes.
Code-switch Due to our SFT data distribution, English data is larger than Chinese data
which will cause code-switch during some instructions or problem formats.
SFT data is using LEAN 3 We use LEAN 3 as our SFT data since GPT-4 can only generate
LEAN 3 codes for GSM8K (even if you require it to apply LEAN 4). Furthermore, we find
the data of translating between formal and informal from MathLib is preprocessed in LEAN
3. We will move our SFT data to LEAN 4 in the future version.
Data contamination on LEAN-repo We use LEAN codes introduced from Algebraic-
Stack (Azerbayev et al., 2023b), and we do not check the contamination of AlgebraicStack
on MiniF2F. AlgebraicStack may or may not contain MiniF2F solutions. However, the
comparison between Llemma and our model is fair.
Sensitive to Prompt The models are sensitive to format and given instruction prompts. If
we give different prompts before the question (e.g. Question:, Q:, Please solve this question
step by step), it may obtain different performances.
References
Alibaba. Qwen technical report, 2023.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre
Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2
technical report. arXiv preprint arXiv:2305.10403 , 2023.
Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir
Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving
undergraduate-level mathematics, 2023a.
16

--- PAGE 17 ---
Technical Report
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,
Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language
model for mathematics, 2023b.
Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui
Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language
models with longtermism. arXiv preprint arXiv:2401.02954 , 2024.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini
Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya
Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,
Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners. ArXiv , abs/2005.14165, 2020. URL https://api.semanticscholar.
org/CorpusID:218971783 .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,
Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,
Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,
Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-
hammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,
Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,
Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,
Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code.
2021.
Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei
Zhang, and Jia Li. Breaking language barriers in multilingual mathematical reasoning:
Insights and observations, 2023a.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts
prompting: Disentangling computation from reasoning for numerical reasoning tasks.
Transactions on Machine Learning Research , 2023b.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers
to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.
OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation
models. https://github.com/open-compass/opencompass , 2023.
Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning.
2023.
Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and
Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. ArXiv ,
abs/2311.01460, 2023. URL https://api.semanticscholar.org/CorpusID:264935229 .
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.
Glm: General language model pretraining with autoregressive blank infilling. In Proceed-
ings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , pp. 320–335, 2022.
Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Hang Yan, Xipeng Qiu, and Dahua Lin.
Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora.
arXiv e-prints , art. arXiv:2401.14624, January 2024. doi: 10.48550/arXiv.2401.14624.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,
and Graham Neubig. Pal: Program-aided language models, 2023.
17

--- PAGE 18 ---
Technical Report
Fabian Gloeckle, Baptiste Roziere, Amaury Hayat, and Gabriel Synnaeve. Temperature-
scaled large language models for lean proofstep prediction. In The 3rd Workshop on
Mathematical Reasoning and AI at NeurIPS’23 , 2023.
Google. Gemini: A family of highly capable multimodal models, 2023.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan,
and Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem
solving, 2023.
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen,
Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder:
When the large language model meets programming – the rise of code intelligence, 2024.
Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers, and Stanislas Polu.
Proof artifact co-training for theorem proving with language models. arXiv preprint
arXiv:2102.06203 , 2021.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,
Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the
MATH dataset. 2021a.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,
Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the
math dataset. arXiv preprint arXiv:2103.03874 , 2021b.
Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei
Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small
language models with scalable training strategies. arXiv preprint arXiv:2404.06395 , 2024.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and
Jiawei Han. Large language models can self-improve, 2022.
InternLM. Internlm: A multilingual language model with progressively enhanced capabili-
ties. https://github.com/InternLM/InternLM , 2023.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,
Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian
Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088 , 2024.
Khan. Khan-exercises. https://github.com/Khan/khan-exercises , 2021.
Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury
Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. Hypertree proof search for
neural theorem proving. Advances in neural information processing systems , 35:26337–26349,
2022.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,
Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai
Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning
problems with language models, 2022a.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,
Vinay V . Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai
Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning
problems with language models. In NeurIPS , 2022b.
Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang
Wang, and Chang Zhou. Query and response augmentation cannot help out-of-domain
math reasoning generalization. arXiv preprint arXiv:2310.05506 , 2023.
18

--- PAGE 19 ---
Technical Report
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee,
Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv
preprint arXiv:2305.20050 , 2023.
Haoxiong Liu and Andrew Chi-Chih Yao. Augmenting math word problems via iterative
question composing, 2024.
Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou,
Wenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. Mathbench: Evaluating the
theory and application proficiency of llms with a hierarchical mathematics benchmark,
2024.
Tiedong Liu and Bryan Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on
arithmetic tasks. arXiv preprint arXiv:2305.14201 , 2023.
Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. arXiv e-prints ,
art. arXiv:1711.05101, November 2017. doi: 10.48550/arXiv.1711.05101.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo
Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering
mathematical reasoning for large language models via reinforced evol-instruct, 2023.
Math-eval. TAL-SCQ5K. https://github.com/math-eval/TAL-SCQ5K , 2023.
The mathlib Community. The lean mathematical library. In Proceedings of the 9th ACM
SIGPLAN International Conference on Certified Programs and Proofs , POPL ’20. ACM, January
2020. doi: 10.1145/3372885.3373824. URL http://dx.doi.org/10.1145/3372885.3373824 .
Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming
language. In Andr ´e Platzer and Geoff Sutcliffe (eds.), Automated Deduction – CADE 28 , pp.
625–635, Cham, 2021. Springer International Publishing. ISBN 978-3-030-79876-5.
Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus,
Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained
language models, 2023.
Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton,
and Augustus Odena. Show your work: Scratchpads for intermediate computation with
language models. ArXiv , abs/2112.00114, 2021. URL https://api.semanticscholar.org/
CorpusID:244773644 .
OpenAI. Gpt-4 technical report, 2023.
Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An
open dataset of high-quality mathematical web text. arXiv preprint arXiv:2310.06786 , 2023.
Lawrence C. Paulson. Isabelle: The next 700 theorem provers, 2000.
Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and
Ilya Sutskever. Formal mathematics statement curriculum learning, 2022.
Baptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,
Yossi Adi, Jingyu Liu, Tal Remez, J ´er´emy Rapin, Artyom Kozhevnikov, Ivan Evtimov,
Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong,
Alexandre D ´efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas
Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models
for code, 2023.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K.
Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning
in open language models, 2024.
19

--- PAGE 20 ---
Technical Report
Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J. Liu,
James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi,
Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Hanie Sedghi, Igor Mordatch,
Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen
Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi,
Noah Constant, Roman Novak, Rosanne Liu, Tris Brian Warkentin, Yundi Qian, Ethan
Dyer, Behnam Neyshabur, Jascha Narain Sohl-Dickstein, and Noah Fiedel. Beyond
human data: Scaling self-training for problem-solving with language models. ArXiv ,
abs/2312.06585, 2023. URL https://api.semanticscholar.org/CorpusID:266163375 .
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis
Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language
model for science. 2023.
The Coq Development Team. The Coq reference manual – release 8.18.0. https://coq.
inria.fr/doc/V8.18.0/refman , 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude
Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman
Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,
Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning
Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,
Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,
Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat
models, 2023.
Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang,
Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with
process- and outcome-based feedback, 2022.
Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang,
Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in
llms for enhanced mathematical reasoning, 2023a.
Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang
Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations.
CoRR, abs/2312.08935 , 2023b.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang,
Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought
reasoning in language models. In The Eleventh International Conference on Learning Repre-
sentations , 2023c. URL https://openreview.net/forum?id=1PL1NIMMrw .
Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In
Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Processing , pp. 845–854, Copenhagen, Denmark,
September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1088.
URL https://aclanthology.org/D17-1088 .
Zengzhi Wang, Rui Xia, and Liu Pengfei. Generative ai for math: Part i – mathpile: A
billion-token-scale pretraining corpus for math. arXiv preprint arXiv:2312.17120 , 2023d.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V
Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language
models. Advances in Neural Information Processing Systems , 35:24824–24837, 2022.
20

--- PAGE 21 ---
Technical Report
Sean Welleck and Rahul Saha. Llmstep: Llm proofstep suggestions in lean. arXiv preprint
arXiv:2310.18457 , 2023.
Kaiyu Yang, Aidan M Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad
Godil, Ryan Prenger, and Anima Anandkumar. Leandojo: Theorem proving with retrieval-
augmented language models. arXiv preprint arXiv:2306.15626 , 2023a.
Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai,
and Jie Tang. Gpt can solve mathematical problems without a calculator. arXiv preprint
arXiv:2309.03241 , 2023b.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and
Karthik Narasimhan. Tree of Thoughts: Deliberate problem solving with large language
models, 2023.
Fei Yu, Anningzhe Gao, and Benyou Wang. Outcome-supervised verifiers for planning in
mathematical reasoning, 2023a.
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T.
Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own
mathematical questions for large language models, 2023b.
Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan,
Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan
Liu, and Maosong Sun. Advancing llm reasoning generalists with preference trees, 2024.
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan,
Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning
with large language models, 2023.
Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu
Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning.
arXiv preprint arXiv:2309.05653 , 2023.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning
with reasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
(eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.
net/forum?id= 3ELRdg2sgI .
Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu, Jiankai Sun, Huajian Xin,
Jianhao Shen, Zhenguo Li, and Yu Li. Lyra: Orchestrating dual correction in automated
theorem proving, 2023.
Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: a cross-system benchmark
for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110 , 2021.
Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu,
Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word
problems using gpt-4 code interpreter with code-based self-verification, 2023.
Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiax-
ing Zhang, and Yujiu Yang. Solving math word problems via cooperative reasoning
induced language models. In Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pp. 4471–4485, Toronto, Canada, July
2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.
acl-long.245 .
21

--- PAGE 22 ---
Technical Report
A SFT Data Composition
We list our SFT Data Composition in Table 16.
B SFT Experiment Configs
Our SFT model is initialized from the pre-trained InternLM-Math-Base models, respectively.
The peak learning rate is set to lrmax=4e−5, and the minimal learning rate during warming
up is lrmax=6e−6. These settings are the same for both 7B and 20B models. The SFT
training data is tokenized into 622.7 M tokens. The batch size is set to 1 per GPU with a
packed dataset whose max sequence length is 32768. We train the model on these data using
Adam Optimizer for three epochs with 32 GPUs, and the training procedure takes around
7 hours to finish for 7B models. For the 20B model, we will use 64 GPUs and the training
procedure takes about 10 hours.
C Detailed performance on MATH benchmark
We list detailed performance on the MATH benchmark in Table 17 and Table 18. We list
reranking performance on GSM8K and MATH in Table 19.
D Case studies
We list cases generated by InternLM-Math-7B.
Formal Translation
Query: Translate Schur inequality into LEAN 3: x,y,z≥0,x3+y3+z3+3xyz≥
xy(x+y) +yz(y+z) +xz(x+z)
Response: (a := α)
theorem schur_inequality {a : Type*} [linear_ordered_field a] {x y z : a}
(hnonneg : 0 ≤x) (hnonneg1 : 0 ≤y) (hnonneg2 : 0 ≤z)
x ˆ 3 + y ˆ 3 + z ˆ 3 + 3 * x * y * z ≥x * y * (x + y) + y * z * (y + z) +
x * z * (x + z)
Prime number Check
Query: Check if 5287 is a prime or not.
Response: [√
5287] =72
We need to check prime numbers between 3 to 72.
Prime numbers: 3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71
5287 / 3 = 1762 ... 1
5287 / 7 = 756 ... 5
5287 / 11 = 480 ... 7
5287 / 13 = 407 ... 6
5287 / 17 = 311 ... 0
5287 can be divided by 17.
5287 is a composite number.
22

--- PAGE 23 ---
Technical Report
Formal Proof found by InternLM-Math using LEAN 4: amc12a 2013 p4
theorem amc12a_2013_p4 : (2 ˆ 2014 + 2 ˆ 2012) / (2 ˆ 2014 - 2 ˆ 2012) = (5 : R
) / 3 := by
--⊢(2 ˆ 2014 + 2 ˆ 2012) / (2 ˆ 2014 - 2 ˆ 2012) = 5 / 3
field_simp
--⊢(2 ˆ 2014 + 2 ˆ 2012) * 3 / (2 ˆ 2014 - 2 ˆ 2012) = 5
simp only [div_eq_inv_mul]
--⊢(2 ˆ 2014 - 2 ˆ 2012)−1* ((2 ˆ 2014 + 2 ˆ 2012) * 3) = 5
norm_cast
--⊢(↑(Int.subNatNat (2 ˆ 2014) (2 ˆ 2012)))−1*↑((2 ˆ 2014 + 2 ˆ 2012) *
3) = 5
norm_num
-- no goals
Outcome Reward Model
Query: Given a question and an answer, check is it correct?
Question: Determine the number of ways to arrange the letters of the word THAT.
Answer: The word THAT has 4 letters. We can arrange these 4 letters in
4!=4∗3∗2∗1=24 ways.
Therefore, there are 24 ways to arrange the letters of the word THAT.
The answer is: 24.
Response: This answer is wrong.
Process Reward Model
Query: Given a question and an answer, check is it correct?
Question: Camy made a list of every possible distinct five-digit positive integer
that can be formed using each of the digits 1, 3, 4, 5 and 9 exactly once in each integer.
What is the sum of the integers on Camy’s list?
Answer: There are 5 !=120 possible five-digit positive integers that can be formed
using each of the digits 1, 3, 4, 5, and 9 exactly once.
The sum of these integers can be found by considering the sum of each place value.
For the ones place, the sum is 1 ·120=120.
For the tens place, the sum is 3 ·120=360.
For the hundreds place, the sum is 4 ·120=480.
For the thousands place, the sum is 5 ·120=600.
For the ten thousands place, the sum is 9 ·120=1080.
Adding up all these sums, we get 120 +360+480+600+1080 =2880 .
The answer is: 2880.
Response: Step 1: There are 5 !=120 possible five-digit positive integers
that can be formed using each of the digits 1, 3, 4, 5, and 9 exactly once. <PRM + >
Step 2: The sum of these integers can be found by considering the sum of each place
value. <PRM + >
Step 3: For the ones place, the sum is 1 ·120=120.<PRM - >
Step 4: For the tens place, the sum is 3 ·120=360.<PRM - >
Step 5: For the hundreds place, the sum is 4 ·120=480.<PRM - >
Step 6: For the thousands place, the sum is 5 ·120=600.<PRM - >
Step 7: For the ten thousands place, the sum is 9 ·120=1080.<PRM - >
Step 8: Adding up all these sums, we get 120 +360+480+600+1080 =2880 .
<PRM - >
Step 9: The answer is: 2880. <PRM - >.
23

--- PAGE 24 ---
Technical Report
Reward model
Query: Given a question and two answers, which one is better?
Question: My average score on 8 math tests is 90. If my average score on the first 5
tests was 87, what was my average score on the last 3 tests?
Answer 1: I scored a total of 8 ∗90=720 on all 8 tests. The total of 5 ∗87=435 on
the first 5 tests leaves a total of 720 −435=285 for the last 3 tests, so the average is
285÷3=95.
Answer 2: I scored a total of 720 on all 8 tests. The total of 435 on the first 5 tests
leaves a total of 285 for the last 3 tests, so the average is 285 ÷3=95
Response: Answer 2 is better.
Change question and answer into a proof statement
Query: Convert this question and answer into a proof format.
Question: Grant scored 10 points higher on his math test than John. John received
twice as many points as Hunter who scored a x on his math test. What was Grant’s
test score? If we know the answer to the above question is 100, what is the value of
unknown variable x?
Answer: We know that John received twice as many points as Hunter, who scored
x on his math test. So, John scored 2x points. Grant scored 10 points higher
than John, so Grant scored 2x + 10 points. We are given that Grant’s test score
is 100, so we can write: 2x + 10 = 100. Solving for x, we get: x = 45. The value of x is 45.
Response: Prove that if Grant scored 10 points higher on his math test than
John, and John received twice as many points as Hunter who scored x on his math
test, then Grant’s test score is 100 if and only if x = 45.
Augment a harder Problem
Query: Please augment a new harder problem based on:
Question: Find the remainder of x2+10x+4 divided by x−2
Response: Find the remainder of (x2+10x+4)2divided by (x−2)2.
E Results on InternLM-Math-Plus
We improve InternLM-Math to InternLM-Math-Plus with new pre-training corpora and
fine-tuning datasets. We have four sizes of InternLM-Math-Plus including 1.8B, 7B, 20B,
and 8x22B. We initialize our models (1.8B, 7B, and 20B) from InternLM-2, and 8x22B from
Mixtral-8x22B (Jiang et al., 2024). We evaluate the performance of InternLM2-Math-Plus on
formal math reasoning benchmark MiniF2F-test via LEAN 4 (in Table 20) and informal math
reasoning benchmarks MATH, GSM8K, and MathBench-A (Liu et al., 2024) (in Table 22).
We further test MATH using Python (in Table 21).
24

--- PAGE 25 ---
Technical Report
Datasets Type Lang Description Size
MetaMath (Yu et al., 2023b) COT, GPT EN Bootstrapp questions on GSM8K & MATH. 395K
Goat (Liu & Low, 2023) COT, Rule EN Synthetic data for arithmetic tasks. 100K
Arithmetic (original) COT, Rule EN 40k multiple brackets computation, 40k dec-
imal computation, 35k fraction computa-
tion, 5k fraction reduction, 15k remainder
problems, and 4k power computation.140K
Prime (original) COT, Rule EN & ZH Judging a number is prime step by step. 15K
Game-of-24 (original) COT, Rule ZH Solving game-of- k(not limited to 24) step
by step by searching.10K
Factoring (original) COT, Rule EN & ZH Factorizing a number step by step. 11K
Triangle (original) COT, Rule EN & ZH Calculating triangle area step by step. 1K
Commonsense (original) COT, Rule EN & ZH Math commonsense dataset. 14K
Mammoth (Yue et al., 2023) COT, CI EN A mix of multiple COT and POT datasets. 216K
PRM800K (Lightman et al.,
2023)COT, GPT EN Selected correct reasoning paths. 7K
Khan (Khan, 2021) COT, Mixed EN A math pertaining corpus. 83K
TAL-SCQ5K-ZH (Math-
eval, 2023)COT, Human ZH Train set of a Chinese multiple-choice
dataset.3K
TAL-SCQ5K-EN (Math-
eval, 2023)COT, Human EN Train set of an English multiple-choice
dataset.3K
Inhouse-ZH (original) COT, Human ZH In-house Chinese math datasets. 293K
MathOctopus (Chen et al.,
2023a)COT, GPT ZH Only select Chinese problems from it. 7K
Math23K-AUG (Wang et al.,
2017)COT, OSLLM ZH Augment Math23K filtered with final an-
swers.195K
MATH-AUG (Hendrycks
et al., 2021b)COT, OSLLM EN Augment MATH filtered with our PRM. 28K
COT-few-shot (original) COT, Mixed EN & ZH Add 1% few-shot format data for most
datasets.15K
Code-Interpreter (original) CI, GPT,
OSLLMEN & ZH A merged dataset of MATH & GSM8K train,
and Chinese K12 level problems. Solutions
are generated by GPT4 or InternLM-70B.
Each interleaves natural language, code,
and execution results.76K
Math-Shepherd (Wang
et al., 2023b)RM, OSLLM EN Merge of Math Shepherd and self-sampled
Math Shepherd-like data. Convert them
into the format of ORM and PRM.445K
ORM-ZH (original) RM, OSLLM ZH Self-sampled ORM data on Chinese prob-
lems.104K
GSM8K-LEAN (original) Formal, GPT EN LEAN 3 codes on GSM8K train set. 4K
COT-LEAN (original) Formal, GPT EN Translate between COT and a LEAN code. 3K
NL-LEAN (Han et al., 2021) Formal, Hu-
manEN Translate between natural language and
LEAN statement.91K
MathLib-Train (Han et al.,
2021)Formal, Hu-
manEN Extracted LEAN state and tactic based on
MathLib.169K
QA-theorem (original) Augmenter,
GPTEN Rephrasing a question and an answer into a
natural language proof statement.1K
Augment-helper (original) Augmenter,
GPTEN ChatGPT-generated question augmentation
(Luo et al., 2023; Li et al., 2023).7K
GSM8K-rephrased (origi-
nal)Augmenter,
GPTEN Reformat rephrased questions from Meta-
Math (Yu et al., 2023b) for rephrasing.2K
All - EN & ZH Combine them with further deduplication
and test set decontamination.2.26M
Table 16: The detailed SFT data composition information. We list the data types and source
types here. OSLLM denotes open-sourced LLMs.
25

--- PAGE 26 ---
Technical Report
Table 17: Performances of MATH clustered by category. * These results come from our
reproduction.
Model Prob. Precal. Inter. Pre-Alg. Alg. Geo. Num. Overall
MetaMath
Llemma-7B * 24.5 14.7 13.7 45.6 43.0 22.8 18.3 28.7
InternLM-Math-Base-7B 28.5 16.9 14.5 53.5 50.0 25.5 24.1 33.4
InternLM-Math-Base-20B 32.3 19.0 15.7 56.3 54.5 27.6 25.4 36.1
Our SFT Data
InternLM-Math-7B 28.9 17.0 17.2 53.5 51.4 27.3 25.6 34.6
InternLM-Math-20B 29.1 21.8 19.2 59.1 55.9 30.0 30.2 37.7
Table 18: Performances of MATH clustered by difficulty. * These results come from our
reproduction.
Model Level 1 Level 2 Level 3 Level 4 Level 5 Overall
MetaMath
Llemma-7B * 66.1 44.5 34.5 20.5 8.2 28.7
InternLM-Math-Base-7B 74.1 50.4 37.9 25.9 11.3 33.4
InternLM-Math-Base-20B 75.6 55.3 41.8 28.3 12.2 36.1
Our SFT Data
InternLM-Math-7B 75.1 53.0 39.3 26.1 12.5 34.6
InternLM-Math-20B 75.5 55.6 45.4 31.7 13.7 37.7
Table 19: Using reward model reranking on multiple math benchmarks. Since we use
temperate=0.7 when sampling, it is natural that sampling 1 times perform worse than
greedy decoding.
Model Metric 1 2 4 8 16 32 64 100
7B/GSM8KMAJ 79.0 79.0 81.8 82.9 83.3 83.2 83 83.5
ORM 79.0 80.0 82.3 83.5 83.6 83.8 83.9 84.2
PRM 79.0 79.8 82.5 83.3 83.9 83.6 83.7 84.0
LRM 79.0 80.4 82.1 83.6 85.2 85.3 85.5 85.8
Oracle 79.0 84.4 88.6 91.3 93.7 95.7 96.7 97.0
20B/GSM8KMAJ 82.1 82.1 85.5 86.4 87.6 88.0 88.5 88.2
ORM 82.1 84.3 86.7 87.4 88.6 88.7 89.2 89.0
PRM 82.1 85.0 87.0 87.5 88.9 88.9 89.5 89.3
LRM 82.1 84.7 87.5 88.3 88.9 89.1 89.4 89.0
Oracle 82.1 88.1 91.3 94.4 96.0 97.1 98.0 98.3
7B/MATHMAJ 34.2 34.8 36.8 40.6 43.6 45.4 44.4 44.8
ORM 34.2 35.8 37.6 40.6 43.6 45.8 45.2 46.0
PRM 34.2 37.0 39.4 42.0 44.0 46.2 46.0 47.0
Oracle 34.2 42.2 49.2 57.6 65.6 71.4 77.6 80.8
20B/MATHMAJ 32.6 34.2 40.6 46.0 47.2 48.4 47.4 47.4
ORM 32.6 37.8 43.6 46.8 48.6 49.6 49.4 48.8
PRM 32.6 37.0 41.8 45.8 48.2 49.2 49.6 50.0
Oracle 32.6 44.8 54.0 61.2 66.8 73.0 79.6 81.8
26

--- PAGE 27 ---
Technical Report
Table 20: Performance of various models on MiniF2F-test.
Models MiniF2F-test
ReProver (Yang et al., 2023a) 26.5
LLMStep (Welleck & Saha, 2023) 27.9
GPT-F Expert Iteration (Polu et al., 2022) 36.6
HTPS (Lample et al., 2022) 41.0
Llemma-7B 26.2
Llemma-34B 25.8
InternLM2-Math-7B-Base 30.3
InternLM2-Math-20B-Base 29.5
InternLM2-Math-Plus-1.8B 38.9
InternLM2-Math-Plus-7B 43.4
InternLM2-Math-Plus-20B 42.6
InternLM2-Math-Plus-Mixtral8x22B 37.3
Table 21: Performance of various models on MATH and GSM8K.
Models MATH MATH-Python GSM8K
MiniCPM-2B (Hu et al., 2024) 10.2 - 53.8
InternLM2-Math-Plus-1.8B 37.0 41.5 58.8
InternLM2-Math-7B 34.6 50.9 78.1
Deepseek-Math-7B-RL (Shao et al., 2024) 51.7 58.8 88.2
InternLM2-Math-Plus-7B 53.0 59.7 85.8
InternLM2-Math-20B 37.7 54.3 82.6
InternLM2-Math-Plus-20B 53.8 61.8 87.7
Mixtral8x22B-Instruct-v0.1 (Jiang et al., 2024) 41.8 - 78.6
Eurux-8x22B-NCA (Yuan et al., 2024) 49.0 - -
InternLM2-Math-Plus-Mixtral8x22B 58.1 68.5 91.8
Table 22: Performance of various models on MathBench-A.
InternLM2-Math-Plus Arithmetic Primary Middle High College Average
-1.8B 43.0 43.3 25.4 18.9 4.7 27.1
-7B 61.4 78.3 52.5 40.5 21.7 50.9
-20B 65.8 79.7 59.5 47.6 24.8 55.5
-Mixtral8x22B 77.5 82.0 63.6 50.3 36.8 62.0
27
