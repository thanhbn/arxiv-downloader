# 2309.05653.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2309.05653.pdf
# Kích thước tệp: 1062653 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bản thảo. Công trình đang triển khai
MA MMO TH: XÂY DỰNG CÁC MÔ HÌNH TOÁN HỌC TỔNG QUÁT
THÔNG QUA ĐIỀU CHỈNH HƯỚNG DẪN KẾT HỢP
♣Xiang Yue∗,‡Xingwei Qu,†Ge Zhang,○Yao Fu,§Wenhao Huang,
♣Huan Sun,♣Yu Su,†Wenhu Chen∗
†Đại học Waterloo,♣Đại học Bang Ohio,‡HKUST,○Đại học Edinburgh,§01.AI
yue.149@osu.edu, wenhuchen@uwaterloo.ca
https://tiger-ai-lab.github.io/MAmmoTH/
TÓM TẮT
Chúng tôi giới thiệu MAmmoTH, một loạt các mô hình ngôn ngữ lớn (LLM) mã nguồn mở
được thiết kế đặc biệt cho việc giải quyết các bài toán toán học tổng quát. Các mô hình MAmmoTH được
huấn luyện trên MathInstruct, bộ dữ liệu điều chỉnh hướng dẫn được chúng tôi tuyển chọn tỉ mỉ.
MathInstruct được tổng hợp từ 13 bộ dữ liệu toán học với lý luận trung gian,
trong đó sáu bộ có lý luận mới được chúng tôi tuyển chọn. Nó trình bày một sự kết hợp độc đáo
giữa lý luận chuỗi tư duy (CoT) và lý luận chương trình tư duy (PoT), đồng thời đảm
bảo phủ sóng rộng rãi các lĩnh vực đa dạng trong toán học. Sự kết hợp giữa CoT và PoT không
chỉ khai phá tiềm năng sử dụng công cụ mà còn cho phép các quá trình tư duy khác nhau
cho các bài toán toán học khác nhau. Kết quả là, loạt MAmmoTH vượt trội đáng kể so với
các mô hình mã nguồn mở hiện có trên chín bộ dữ liệu lý luận toán học ở tất cả các quy mô với mức tăng độ chính xác trung bình từ 16% đến 32%. Đáng chú ý,
mô hình MAmmoTH-7B của chúng tôi đạt 33% trên MATH (một bộ dữ liệu cấp độ cuộc thi),
vượt qua mô hình mã nguồn mở 7B tốt nhất (WizardMath) 23%, và
mô hình MAmmoTH-34B đạt độ chính xác 44% trên MATH, thậm chí vượt qua kết quả CoT của GPT-4. Công trình của chúng tôi nhấn mạnh tầm quan trọng của việc phủ sóng bài toán đa dạng và sử dụng lý luận kết hợp trong việc phát triển các mô hình toán học tổng quát vượt trội.
+24+26+32+21
020406080
7B13B30B70BĐộ chính xác (%)CơSởSoTAMAmmoTH (Của chúng tôi)+20+19+28+16
020406080
7B13B30B70BĐộ chính xác (%)CơSởSoTAMAmmoTH (Của chúng tôi)Bộ dữ liệu trong miềnBộ dữ liệu ngoài miềnWeng kiếm được $12 một giờ cho việc trông trẻ. Hôm qua, cô ấy chỉ trông trẻ 50 phút. Cô ấy kiếm được bao nhiều?Weng kiếm được 12/60 = 0.2 mỗi phút.Làm 50 phút, cô ấy kiếm được 0.2 x 50 = 10hourly_rate= 12;time_worked= 50/60;earnings = hourly_rate* time_worked;print(round(earnings, 2))Chuỗi-tư-duy(CoT)Chương-trình-tư-duy(PoT)MathInstructĐiều chỉnh Hướng dẫn Kết hợp
MAmmoTHCác Bài toán Toán học Đa dạng
Hình 1: Hiệu suất vượt trội của MAmmoTH, một loạt các mô hình được điều chỉnh hướng dẫn để giải quyết
một tập hợp đa dạng các bài toán toán học sử dụng lý luận CoT và PoT kết hợp. MAmmoTH vượt trội đáng kể
so với các mô hình cơ sở và SoTA trên cả bộ kiểm tra trong miền và ngoài miền, ở tất cả các quy mô.
∗Xiang Yue và Wenhu Chen là các tác giả chính của bài báo. Họ đóng góp ngang nhau cho dự án này.
1arXiv:2309.05653v3 [cs.CL] 3 Tháng 10 2023

--- TRANG 2 ---
Bản thảo. Công trình đang triển khai
1 GIỚI THIỆU
Công trình này tập trung vào lý luận toán học, một khả năng quan trọng của các mô hình ngôn ngữ lớn
(LLM) hiện đại (OpenAI, 2023; Anil et al., 2023). Bất chấp những tiến bộ gần đây trong lĩnh vực này, một khoảng cách đáng chú ý
tồn tại giữa các LLM mã nguồn đóng và mã nguồn mở—các mô hình mã nguồn đóng như GPT-4 (OpenAI,
2023), PaLM-2 (Anil et al., 2023), và Claude 2 (Bai et al., 2022) thống trị các tiêu chuẩn lý luận toán học
phổ biến như GSM8K (Cobbe et al., 2021) và MATH (Hendrycks et al.,
2021b), trong khi các mô hình mã nguồn mở như Llama (Touvron et al., 2023a;b), Falcon (Penedo et al.,
2023), OPT (Zhang et al., 2022) tụt hậu trên tất cả các tiêu chuẩn với khoảng cách lớn.
Các nỗ lực hiện tại để thu hẹp khoảng cách này có hai hướng: (1) Tiếp tục tiền huấn luyện như Galactica (Taylor et al.,
2022) và MINERVA (Lewkowycz et al., 2022), tiếp tục huấn luyện LLM trên dữ liệu web liên quan đến toán học
hơn 100B token. Cách tiếp cận này cải thiện khả năng lý luận khoa học tổng quát của mô hình nhưng phát sinh chi phí tính toán cao. (2) Tinh chỉnh cụ thể cho bộ dữ liệu như tinh chỉnh lấy mẫu từ chối
(RFT) (Yuan et al., 2023) và WizardMath (Luo et al., 2023), tinh chỉnh LLM
sử dụng dữ liệu có giám sát cụ thể cho các bộ dữ liệu nhất định. Mặc dù những cách tiếp cận này cải thiện hiệu suất trong miền, chúng không thể tổng quát hóa cho phạm vi rộng hơn các nhiệm vụ lý luận toán học ngoài dữ liệu tinh chỉnh của chúng. Ví dụ, cả RFT và WizardMath đều có thể tăng độ chính xác trên GSM8K (Cobbe
et al., 2021) 30%+, một trong những bộ dữ liệu tinh chỉnh của chúng, nhưng làm giảm độ chính xác trên các bộ dữ liệu ngoài miền như MMLU-Math (Hendrycks et al., 2021a) hoặc AQuA (Ling et al., 2017) lên đến 10%.
Trong bài báo này, chúng tôi nhằm đề xuất một cách tiếp cận điều chỉnh hướng dẫn toán học nhẹ nhưng có thể tổng quát hóa để
nâng cao khả năng lý luận toán học tổng quát (tức là, không giới hạn ở các nhiệm vụ tinh chỉnh) của
LLM. Các phương pháp hiện có (Luo et al., 2023; Yuan et al., 2023; Taylor et al., 2022) chủ yếu tập trung vào
các cách tiếp cận Chuỗi-tư-duy (CoT) (Wei et al., 2022b; Nye et al., 2022) để giải quyết các bài toán toán học
thông qua mô tả ngôn ngữ tự nhiên từng bước. Cách tiếp cận này xuất sắc trong tính tổng quát để bao phủ
hầu hết các chủ đề toán học nhưng gặp khó khăn với độ chính xác tính toán, và các quy trình lý luận toán học hoặc thuật toán phức tạp (ví dụ, giải nghiệm phương trình bậc hai và tính giá trị riêng của ma trận).
Ngược lại, các gợi ý ở định dạng mã như các cách tiếp cận Chương-trình-tư-duy (PoT) (Chen et al.,
2022) và PAL (Madaan et al., 2022; Gao et al., 2023) sử dụng các công cụ bên ngoài (tức là, trình thông dịch Python)
để đơn giản hóa đáng kể quá trình giải toán. Cách tiếp cận này ủng hộ việc chuyển giao quá trình tính toán
cho trình thông dịch Python bên ngoài để giải quyết các quy trình lý luận toán học và thuật toán phức tạp (ví dụ, giải phương trình bậc hai với sympy hoặc tính giá trị riêng của ma trận với
numpy). Tuy nhiên, PoT có hạn chế trong việc xử lý các tình huống lý luận trừu tượng hơn, như lý luận
thường thức, logic hình thức, và đại số trừu tượng, đặc biệt khi không có API tích hợp.
Để tận dụng điểm mạnh của cả hai cách tiếp cận CoT và PoT, chúng tôi giới thiệu một bộ dữ liệu điều chỉnh hướng dẫn toán học kết hợp mới MathInstruct, có hai đặc điểm chính: (1) phủ sóng rộng các lĩnh vực toán học và mức độ phức tạp khác nhau, và (2) lý luận CoT & PoT kết hợp.
MathInstruct dựa trên bảy bộ dữ liệu lý luận toán học hiện có và sáu bộ dữ liệu mới được tuyển chọn
(xem chi tiết trong Bảng 1). Chúng tôi sử dụng MathInstruct để tinh chỉnh các mô hình Llama (Touvron et al., 2023a;b;
Rozière et al., 2023) với các quy mô khác nhau từ 7B đến 70B. Các mô hình MAmmoTH
kết quả (Hình 1) thể hiện tiềm năng chưa từng có trong việc phục vụ như các nhà toán học tổng quát.
Chúng tôi đánh giá MAmmoTH trên một phổ các bộ dữ liệu, bao gồm các bộ kiểm tra trong miền (IND)—
GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQuA-RAT (Ling et al., 2017),
NumGLUE (Mishra et al., 2022b)—và các bộ kiểm tra ngoài miền (OOD)—SVAMP (Patel et al.,
2021), SAT (Zhong et al., 2023), MMLU-Math (Hendrycks et al., 2021a), Mathematics (Davies
et al., 2021), và SimulEq (Koncel-Kedziorski et al., 2016). So với các phương pháp hiện có, các mô hình của chúng tôi tổng quát hóa tốt hơn cho các bộ dữ liệu OOD và cải thiện đáng kể hiệu suất
của các LLM mã nguồn mở trong lý luận toán học. Đáng chú ý, trên bộ dữ liệu MATH cấp độ cuộc thi phổ biến (Hendrycks et al., 2021b), mô hình 7B của chúng tôi có thể đánh bại WizardMath (mã nguồn mở MATH
SoTA) (Luo et al., 2023) gấp 3,5 lần (35,2% so với 10,7%), và MAmmoTH-Coder 34B của chúng tôi (tinh chỉnh trên
Code Llama (Rozière et al., 2023)) thậm chí có thể đánh bại kết quả của GPT-4 (sử dụng CoT).
Chúng tôi nêu bật các đóng góp từ hai góc độ: (1) Từ góc độ kỹ thuật dữ liệu, chúng tôi trình bày MathInstruct, một bộ dữ liệu điều chỉnh hướng dẫn toán học chất lượng cao, kết hợp nhiều
bài toán toán học và lý luận kết hợp. (2) Từ góc độ mô hình hóa, chúng tôi điều tra
tác động của các nguồn dữ liệu khác nhau và định dạng đầu vào-đầu ra thông qua huấn luyện và đánh giá trên
50 mô hình và đường chuẩn khác nhau từ 7B đến 70B. Các mô hình của chúng tôi, bao gồm MAmmoTH và
MAmmoTH-Coder, đạt được mức tăng độ chính xác đáng kể so với các mô hình mã nguồn mở hiện có.
2

--- TRANG 3 ---
Bản thảo. Công trình đang triển khai
Bộ dữ liệu Huấn luyện Loại Chú thích # Mẫu Đặc điểm Lĩnh vực
GSM8K (Cobbe et al., 2021) CoT Con người 7K Thi cấp trường ∎
GSM8K-RFT (Yuan et al., 2023) CoT Llama 28K Llama + Đã xác thực ∎
AQuA-RAT (Ling et al., 2017) CoT Con người 90K Thi GRE/GMAT ∎
MATH (Hendrycks et al., 2021b) CoT Con người 7K Thi Toán học ∎ ∎ ∎ ∎ ∎ ∎ ∎
TheoremQA (Chen et al., 2023) ? CoT GPT-4 600 GPT4 + Đã xác thực ∎ ∎ ∎ ∎ ∎
Camel-Math (Li et al., 2023a) CoT GPT-4 50K GPT4 (Chưa xác thực) ∎ ∎ ∎ ∎ ∎
College-Math ? CoT GPT-4 1.8K GPT4 (Chưa xác thực) ∎
GSM8K? PoT GPT4 14K GPT4 + Đã xác thực ∎
AQuA-RAT ? PoT GPT4 9.7K GPT4 + Đã xác thực ∎
MATH? PoT GPT4 7K GPT4 + Đã xác thực ∎ ∎ ∎ ∎
TheoremQA ? PoT GPT4 700 GPT4 + Đã xác thực ∎ ∎ ∎ ∎ ∎
MathQA (Amini et al., 2019) PoT Con người 25K Tập con AQuA-RAT ∎
NumGLUE (Mishra et al., 2022a) PoT Con người 13K Chú thích Lila ∎
MathInstruct 260K ∎ ∎ ∎ ∎ ∎ ∎ ∎
Bảng 1: Tổng quan về MathInstruct của chúng tôi. ? có nghĩa là với lý luận MỚI được chúng tôi tuyển chọn bằng cách
gợi ý GPT-4. Chúng tôi đã lọc ra các mẫu tăng cường có câu trả lời không nhất quán với
chú thích của bộ dữ liệu gốc. Các ô vuông màu khác nhau đại diện cho các lĩnh vực khác nhau trong toán học: ∎
Tiền-Đại số; ∎Trung-Đại số; ∎Đại số; ∎Xác suất; ∎Lý thuyết số; ∎Giải tích; ∎Hình học.
2 CÁCH TIẾP CẬN CỦA CHÚNG TÔI
2.1 BỐI CẢNH
Lý luận toán học đóng vai trò như một thước đo quan trọng để đánh giá khả năng thực hiện
lý luận nhiều bước phức tạp và định lượng của LLM. Trước đây, đây là một nhiệm vụ thách thức đối với các mạng nơ-ron, vốn gặp khó khăn để giải quyết ngay cả các bài toán cộng và trừ cơ bản (Yang et al., 2023).
Tuy nhiên, các LLM gần đây đã có những tiến bộ đáng kể trong lý luận toán học. Những đột phá quan trọng
đã được thực hiện thông qua gợi ý CoT (Wei et al., 2022b; Nye et al., 2022) và gợi ý PoT (Chen et al., 2022; Gao et al., 2023). Gợi ý CoT khuyến khích LLM giải quyết các bài toán
từng bước trên một bảng nháp, nâng cao cả độ chính xác và khả năng giải thích trong lý luận toán học. Cách tiếp cận này trái ngược với các phương pháp truyền thống tạo ra câu trả lời trực tiếp. Gợi ý PoT, mặt khác, công thức hóa quá trình lý luận trung gian như một chương trình, được thực thi
với một công cụ bên ngoài như Python, để tính toán câu trả lời. Phương pháp này cải thiện độ bền trong việc giải quyết các bài toán toán học phức tạp bằng cách chuyển giao các phép tính cho các công cụ bên ngoài. Tuy nhiên, hầu hết
công trình hiện có (Zhou et al., 2023a) trong PoT giới hạn ở các mô hình độc quyền như GPT-4 (OpenAI,
2023) và Codex (Chen et al., 2021). Tiềm năng PoT của các mô hình mã nguồn mở vẫn chưa được thấy rõ.
Công trình của chúng tôi nhằm tối ưu hóa khả năng lý luận CoT và PoT của LLM thông qua điều chỉnh hướng dẫn.
2.2 TUYỂN CHỌN MỘT BỘ DỮ LIỆU ĐIỀU CHỈNH HƯỚNG DẪN ĐA DẠNG VÀ KẾT HỢP
Nghiên cứu của chúng tôi nhằm tổng hợp một danh sách các bộ dữ liệu điều chỉnh hướng dẫn toán học chất lượng cao và đa dạng, nổi bật với ba đặc điểm chính: (1) phủ sóng rộng các lĩnh vực toán học và
mức độ phức tạp khác nhau, và (2) lý luận CoT & PoT kết hợp.
Phủ sóng Rộng các Lĩnh vực Toán học và Mức độ Phức tạp Khác nhau: Chúng tôi nhằm đại diện
rộng rãi các lĩnh vực toán học và mức độ phức tạp trong bộ dữ liệu của chúng tôi. Điều này đảm bảo tiếp xúc với một tập hợp đa dạng kiến thức toán học, thúc đẩy tính linh hoạt trong các mô hình của chúng tôi. Dựa trên những tiêu chí này, chúng tôi thu hẹp lựa chọn xuống một số bộ dữ liệu chất lượng cao được áp dụng rộng rãi và bao gồm các lĩnh vực toán học và mức độ phức tạp khác nhau, như GSM8K, MATH, AQuA, Camel, và TheoremQA. Hơn nữa, chúng tôi nhận thấy thiếu sự bao phủ cho kiến thức toán học cấp đại học, như đại số trừu tượng
và logic hình thức, trong các bộ dữ liệu hiện có. Để khắc phục điều này, chúng tôi sử dụng GPT-4 để tổng hợp lý luận CoT cho
các câu hỏi trong TheoremQA và tạo các cặp câu hỏi-CoT thông qua Self-Instruct (Wang et al., 2023h),
sử dụng một vài mẫu hạt giống tìm thấy trực tuyến.
3

--- TRANG 4 ---
Bản thảo. Công trình đang triển khai
Lý luận CoT và PoT Kết hợp: Trái ngược với công trình trước đây (Yuan et al., 2023; Luo et al., 2023;
Lee et al., 2023; Wang et al., 2023g) tập trung vào CoT, bộ dữ liệu của chúng tôi kết hợp chiến lược cả hai.
Sự tích hợp này nâng cao tính linh hoạt của bộ dữ liệu, phục vụ các cách tiếp cận giải quyết bài toán toán học khác nhau. Tuy nhiên, hầu hết các bộ dữ liệu hiện có cung cấp lý luận chương trình hạn chế, dẫn đến mất cân bằng giữa lý luận CoT và PoT. Để lấp đầy khoảng trống, chúng tôi sử dụng GPT-4 để bổ sung lý luận PoT cho các bộ dữ liệu được chọn, bao gồm MATH, AQuA, GSM8K, và TheoremQA. Chúng tôi sau đó lọc các chương trình được tổng hợp bởi GPT-4 bằng cách so sánh kết quả thực thi của chúng với sự thật chuẩn được chú thích bởi con người, đảm bảo chất lượng cao của các lý luận được thêm vào.
Theo các hướng dẫn này, bộ dữ liệu hướng dẫn của chúng tôi, được chi tiết trong Bảng 1, bao gồm 260K cặp (hướng dẫn, phản hồi), bao phủ một phạm vi rộng các lĩnh vực toán học cốt lõi (số học, đại số, xác suất, giải tích, và hình học, v.v.), bao gồm lý luận CoT và PoT kết hợp, và cung cấp sự đa dạng
trong cả ngôn ngữ và mức độ khó. Điều này chứng thực chất lượng cao và đặc điểm độc đáo của nó.
2.3 THIẾT LẬP HUẤN LUYỆN
Chúng tôi thống nhất tất cả các tập con trong MathInstruct của chúng tôi để tuân thủ cấu trúc của một bộ dữ liệu hướng dẫn giống Alpaca (Taori et al., 2023). Chuẩn hóa này đảm bảo rằng các mô hình được tinh chỉnh có thể xử lý dữ liệu một cách nhất quán, bất kể định dạng bộ dữ liệu gốc. Chúng tôi chọn các mô hình mã nguồn mở
Llama-2 (Touvron et al., 2023b) và Code Llama (Rozière et al., 2023) làm mô hình cơ sở của chúng tôi.
Chúng tôi tinh chỉnh những mô hình này bao gồm 7B, 13B, 34B, và 70B trên MathInstruct, cho phép chúng tôi
xác thực MathInstruct của chúng tôi ở nhiều quy mô. Chúng tôi tinh chỉnh tất cả các mô hình với thư viện
transformers của Huggingface (Wolf et al., 2019). Chúng tôi sử dụng tốc độ học 2e-5 cho các mô hình 7B và 13B,
và 1e-5 cho các mô hình 34B và 70B. Chúng tôi đặt kích thước lô ở 128 và sử dụng bộ lập lịch cosine với
giai đoạn khởi động 3% trong ba epoch. Để huấn luyện hiệu quả các mô hình 34B và 70B đòi hỏi tính toán cao, chúng tôi sử dụng huấn luyện DeepSpeed với giai đoạn ZeRO-3 (Rajbhandari et al., 2020).
2.4 THIẾT LẬP ĐÁNH GIÁ
Huấn luyện kết hợp của chúng tôi cho phép các mô hình giải quyết bài toán sử dụng cả cách tiếp cận CoT hoặc PoT. Theo mặc định, mô hình cung cấp giải pháp CoT. Để chuyển sang cách tiếp cận PoT, người ta có thể thêm cụm từ kích hoạt "Hãy viết một chương trình để giải quyết bài toán" sau câu hỏi.
Đánh giá sơ bộ của chúng tôi tiết lộ rằng PoT thường vượt trội hơn CoT, đáng chú ý trong các câu hỏi mở như GSM8K và MATH, vì các giải pháp có thể lập trình tốt hơn trong việc giải quyết các quy trình lý luận toán học và thuật toán phức tạp. Tuy nhiên, PoT gặp khó khăn với các tình huống lý luận trừu tượng
như lý luận thường thức, logic hình thức, và đại số trừu tượng, đặc biệt khi không có API tích hợp. Để kết hợp thêm sức mạnh của cả hai cách tiếp cận, chúng tôi giới thiệu một chiến lược giải mã kết hợp đơn giản: Mô hình trước tiên thử gợi ý PoT. Nếu chương trình không thể thực thi, chúng tôi quay lại gợi ý CoT. Heuristic này nâng cao đáng kể hiệu suất tổng thể của mô hình chúng tôi (xem thêm thảo luận trong phần 3.4).
3 THỰC NGHIỆM
3.1 BỘ DỮ LIỆU ĐÁNH GIÁ
Chúng tôi đã chọn các bộ dữ liệu đánh giá đa dạng (Bảng 2), bao gồm nhiều mẫu trong miền và ngoài miền khác nhau trên các lĩnh vực toán học đa dạng, để đánh giá khả năng lý luận toán học tổng quát của các mô hình.
Đối với các bộ dữ liệu trong miền, chúng tôi xem xét GSM8K (Cobbe et al., 2021), MATH (Hendrycks
et al., 2021b), AQuA-RAT (Ling et al., 2017), và NumGLUE (Mishra et al., 2022b). Đối với
các bộ dữ liệu ngoài miền, chúng tôi chọn SVAMP (Patel et al., 2021), Mathematics (Davies et al.,
2021), SimulEq (Koncel-Kedziorski et al., 2016), SAT-Math (Zhong et al., 2023), và MMLU-
Math (Hendrycks et al., 2021a). Việc lựa chọn rộng rãi các bộ dữ liệu đánh giá bao gồm các bài toán toán học
từ tiểu học, trung học, và đại học. Một số bộ dữ liệu thậm chí bao gồm logic hình thức và
lý luận thường thức. Việc lựa chọn những bộ dữ liệu này nhằm đảm bảo đánh giá toàn diện khả năng của
các mô hình tổng quát hóa cho các tình huống không quen thuộc và các lĩnh vực toán học khác nhau. Các bộ dữ liệu đánh giá được chọn bao gồm cả câu hỏi mở và câu hỏi trắc nghiệm.
4

--- TRANG 5 ---
Bản thảo. Công trình đang triển khai
Bộ dữ liệu Đánh giá # Mẫu Trong Miền? Hình thức Trả lời Lĩnh vực
GSM8K (Cobbe et al., 2021) 1319 CÓ Dạng mở ∎
MATH (Hendrycks et al., 2021b) 5000 CÓ Dạng mở ∎ ∎ ∎ ∎ ∎ ∎ ∎
AQuA-RAT (Ling et al., 2017) 254 CÓ Trắc nghiệm ∎
NumGLUE (Mishra et al., 2022b) 1042 CÓ Dạng mở ∎
SVAMP (Patel et al., 2021) 1000 KHÔNG Dạng mở ∎
Mathematics (Davies et al., 2021) 1000 KHÔNG Dạng mở ∎ ∎ ∎ ∎
SimulEq (Koncel-Kedziorski et al., 2016) 514 KHÔNG Dạng mở ∎
SAT-Math (Zhong et al., 2023) 220 KHÔNG Trắc nghiệm ∎ ∎ ∎
MMLU-Math (Hendrycks et al., 2021a) 974 KHÔNG Trắc nghiệm ∎ ∎ ∎ ∎
Bảng 2: Tổng quan toàn diện về các bộ dữ liệu đánh giá của chúng tôi, gồm nhiều bài toán trong miền và
ngoài miền trên các lĩnh vực toán học đa dạng. Các ô vuông màu khác nhau đại diện cho các lĩnh vực khác nhau trong toán học: ∎Tiền-Đại số; ∎Trung-Đại số; ∎Đại số; ∎Xác suất;
∎Lý thuyết số; ∎Giải tích; ∎Hình học.
3.2 CÁC ĐƯỜNG CHUẨN
Chúng tôi phân chia các đường chuẩn thành bốn loại sau:
•LLM mã nguồn đóng: Chúng tôi xem xét 4 LLM mã nguồn đóng bao gồm GPT-4 (OpenAI, 2023),
GPT-4 (Code Interpreter), PaLM-2 Unicorn (Anil et al., 2023), Claude-2 (Bai et al., 2022) và
Codex (Chen et al., 2021). GPT-4, PaLM-2, và Claude-2 sử dụng gợi ý CoT trong khi GPT-4 (Code
Interpreter) và Codex sử dụng gợi ý PoT.
•Llama Cơ sở: Đối với các mô hình cơ sở, chúng tôi xem xét Llama-1/2 (Touvron et al., 2023a;b), Llama-2-
Chat (Touvron et al., 2023b).
•Mô hình Lập trình: Để so sánh với các mô hình lập trình khác nhau, chúng tôi chọn Code-Llama (Rozière et al.,
2023), CodeT5+ (Wang et al., 2023i) và CodeGen (Nijkamp et al., 2023).
•Tiền huấn luyện STEM: Chúng tôi bao gồm Galactica (Taylor et al., 2022) chủ yếu để hiểu hiệu suất
của các mô hình chuyên biệt trong kiến thức STEM.
•Điều chỉnh Hướng dẫn: Chúng tôi bao gồm Orca-Platypus (Mukherjee et al., 2023), Vicuna-1.5 (Zheng
et al., 2023b), Tulu (Wang et al., 2023g), Platypus-2 (Lee et al., 2023) và Guanaco (Dettmers
et al., 2023). Chúng tôi bao phủ một phổ rộng các mô hình được huấn luyện với các loại bộ dữ liệu khác nhau.
•Tinh chỉnh Cụ thể cho Bộ dữ liệu: Chúng tôi bao gồm cả RFT (Yuan et al., 2023) và WizardMath (Luo et al.,
2023), tập trung tinh chỉnh các mô hình để thích ứng với bộ dữ liệu GSM8K và MATH. Chúng tôi bao gồm
chúng để hiểu khả năng tổng quát hóa của chúng.
Đối với hầu hết các đường chuẩn, chúng tôi chọn gợi ý CoT để tối đa hóa hiệu suất của chúng do khả năng
tạo chương trình kém của chúng. Tất cả 'Mô hình Mã' sử dụng gợi ý PoT. Đối với GSM8K, MATH,
AQuA, và NumGLUE, chúng tôi sẽ đánh giá cả thiết lập học theo ngữ cảnh 8-shot và zero-shot để
báo cáo điểm số cao nhất. Đối với SVAMP, Mathematics, SimulEq, SAT, và MMLU, chúng tôi sử dụng học theo ngữ cảnh 5-shot để duy trì tính nhất quán với công trình trước đây (Wei et al., 2022b; Chen et al., 2023).
Các mẫu few-shot của chúng tôi chủ yếu được lấy từ PHP1(Zheng et al., 2023a). Đối với MAmmoTH và
MAmmoTH-Coder, chúng tôi luôn đánh giá trong thiết lập 0-shot. Đối với tất cả các mô hình, chúng tôi cho phép độ dài chuỗi tối đa 2048 token để giải mã. Đối với các câu hỏi trắc nghiệm, nếu câu trả lời được tạo ra thiếu một lựa chọn, chúng tôi ánh xạ bằng cách gợi ý lại mô hình: "Vui lòng tìm lựa chọn gần nhất với [câu trả lời được tạo ra]. Các lựa chọn là [lựa chọn]".
3.3 KẾT QUẢ CHÍNH
Chúng tôi báo cáo kết quả trong miền và ngoài miền trong Bảng 3 và Bảng 4 tương ứng. Nhìn chung, chúng ta có thể thấy rằng MAmmoTH và MAmmoTH-Coder có thể vượt trội hơn mô hình SoTA ở các quy mô khác nhau. Nói chung, mức tăng hiệu suất cho các bộ dữ liệu OOD đáng kể hơn so với các bộ dữ liệu IND.
Những kết quả này cho chúng ta thấy tiềm năng của các mô hình chúng tôi như một nhà toán học tổng quát. Trên một số bộ dữ liệu,
MAmmoTH-Coder-34B và MAmmoTH-70B thậm chí vượt qua các LLM mã nguồn đóng.
1https://github.com/chuanyang-Zheng/Progressive-Hint
5

--- TRANG 6 ---
Bản thảo. Công trình đang triển khai
Mô hình Cơ sở Math-SFT? GSM8K MATH AQuA NumGLUE Trung bình
Mô hình Mã nguồn đóng
GPT-4 - Không rõ 92.0†42.5†72.6†- -
GPT-4 (Code-Interpreter) - Không rõ 97.0†69.7†- - -
PaLM-2 - Không rõ 80.7†34.3†64.1 - -
Claude-2 - Không rõ 85.2†32.5†60.9 - -
Codex (PoT) - Không 71.6†36.8†54.1†- -
ART (InstructGPT) - Không rõ 71.0 - 54.2 - -
Mô hình Tham số 7B
Llama-1 - Không 10.7†2.9†22.6 24.7 15.5
Llama-2 - Không 14.6†2.5†30.3 29.9 19.3
Galactica-6.7B GAL GAL-Instruct 10.2 2.2 25.6 25.8 15.9
Code-Llama (PoT) - Không 25.2 13.0 24.0 26.8 22.2
AQuA-SFT Llama-2 AQuA 11.2 3.6 35.6 12.2 15.6
Llama-1 RFT Llama-1 GSM8K 46.5†5.2 18.8 21.1 22.9
WizardMath Llama-2 GSM8K+MATH 54.9†10.7†26.3 36.1 32.0
MAmmoTH Llama-2 MathInstruct 53.6 31.5 44.5 61.2 47.7
MAmmoTH-Coder Code-Llama MathInstruct 59.4 33.4 47.2 66.4 51.6
∆ +5 +21 +12 +30 +20
Mô hình Tham số 13-15B
Llama-1 - Không 17.8†3.9†26.0 24.8 18.1
Llama-2 - Không 28.7†3.9†25.1 8.8 16.6
Code-Llama (PoT) - Không 36.1 16.4 28.7 29.2 27.6
CodeT5+ (PoT) - Không 12.5 2.4 20.5 19.4 13.7
CodeGen+ (PoT) - Không 12.7 3.4 24.5 22.5 15.7
Vicuna-1.5 Llama-2 Không 28.4†5.8 24.8 36.9 23.9
Llama-1 RFT Llama-1 GSM8K 52.1†5.1 16.1 24.5 24.4
Orca-Platypus Llama-2 Platypus 38.4 3.0 18.9 35.3 23.9
Platypus Llama-2 Platypus 25.7 2.5 33.4 42.3 25.9
WizardMath Llama-2 GSM8K+MATH 63.9†14.0†21.2 40.8 34.9
MAmmoTH Llama-2 MathInstruct 62.0 34.2 51.6 68.7 54.1
MAmmoTH-Coder Code-Llama MathInstruct 64.7 36.3 46.9 66.8 53.7
∆ +1 +20 +18 +26 +19
Mô hình Tham số 30-34B
Llama-1 - Không 35.6†7.1†33.4 28.4 26.1
Code-Llama (PoT) - Không 44.0 23.1 25.2 29.3 30.4
Llama-1 RFT Llama-1 GSM8K 56.5†7.4†18.5 24.3 26.6
Galactica-30B GAL GAL-Instruct 41.7 12.7 28.7 34.7 29.4
Platypus Llama-1 Platypus 37.8 9.3 27.9 40.5 28.8
Tulu Llama-2 Tulu 51.0 10.8 25.5 43.4 32.6
MAmmoTH-Coder Code-Llama MathInstruct 72.7 43.6 54.7 71.6 60.7
∆ +16 +21 +21 +28 +28
Mô hình Tham số 65-70B
Llama-1 - Không 50.9†10.6†35.0 50.2 36.6
Llama-2 - Không 56.8†13.5†40.9 50.4 40.4
Llama-2-Chat Llama-2 Không 54.9 18.6 37.0 51.6 40.5
Guanaco Llama-2 Không 59.2 4.1 45.2 53.5 40.5
WizardMath Llama-2 GSM8K+MATH 81.6†22.7†20.0 48.9 43.3
Platypus Llama-2 Platypus 70.6 15.6 51.2 55.4 48.1
MAmmoTH Llama-2 MathInstruct 76.9 41.8 65.0 74.4 64.5
∆ -5 +19 +14 +19 +16
Bảng 3: Bảng tổng hợp tất cả kết quả đánh giá trong miền. Kết quả được đánh dấu † được sao chép
từ các bài báo khác, có thể tìm thấy trên bảng xếp hạng paperswithcode. Math-SFT? có nghĩa là
mô hình có được điều chỉnh hướng dẫn trên bất kỳ bộ dữ liệu lý luận toán học nào không. Các số màu hồng làm nổi bật số cao nhất trong quy mô và bộ dữ liệu tương ứng. Lưu ý rằng không tồn tại phiên bản 30B+ cho Llama-2 hoặc phiên bản 70B cho Code-Llama.
Từ Bảng 3, chúng ta có thể quan sát rằng các đối thủ chính của chúng tôi cho các bộ dữ liệu IND là WizardMath (Luo
et al., 2023) và Platypus (Lee et al., 2023). Huấn luyện của WizardMath dựa chủ yếu vào GSM8K
6

--- TRANG 7 ---
Bản thảo. Công trình đang triển khai
Mô hình SVAMP Mathematics SimulEq SAT-Math MMLU-Math Trung bình
Mô hình Mã nguồn đóng
GPT-4 97.0†- - 95†- -
Codex (PoT) 85.2†- - 68†- -
Mô hình Tham số 7B
Llama-1 24.5 6.2 4.6 22.7 30.6 17.7
Llama-2 34.5 6.0 5.0 26.8 29.8 20.4
Code-Llama (PoT) 49.4 21.7 3.5 28.6 26.9 26.0
Llama-1 RFT 21.1 5.1 11.0 12.5 21.7 14.3
Galactica-6.7B 25.6 4.6 4.2 17.5 28.0 16.0
WizardMath 36.1 9.3 12.8 25.4 31.1 28.6
Toolformer 29.4†- - - - -
MAmmoTH 67.7 46.3 41.2 42.7 42.6 48.1
MAmmoTH-Coder 71.4 55.4 45.9 40.5 48.3 52.3
∆ +22 +34 +33 +14 +17 +24
Mô hình Tham số 13B
Llama-1 34.7 6.9 5.4 27.7 30.7 21.0
Llama-2 35.1 11.5 5.8 32.7 34.4 23.9
Code-Llama (PoT) 60.0 21.3 3.8 25.9 27.7 27.7
Vicuna-1.5 55.7 10 6.6 34.0 34.1 28.1
Llama-1 RFT 46.5 6.7 10.1 13.2 21.6 19.6
WizardMath 51.9 14.1 14.9 24.5 32.1 27.5
Platypus 55.4 11.4 7.4 36.8 35.5 29.3
Orca-Platypus 56.8 12.6 7.9 29.5 41.6 29.7
MAmmoTH 72.4 49.2 43.2 46.8 47.6 51.8
MAmmoTH-Coder 73.7 61.5 47.1 48.6 48.3 55.8
∆ +14 +40 +33 +12 +7 +26
Mô hình Tham số 30-34B
Llama-1 48.8 12.8 11.2 33.4 39.0 29.0
Code-Llama (PoT) 69.1 34.5 6.8 26.8 21.6 31.7
Llama-1 RFT 55.4 7.6 12.8 20.4 37.9 26.8
Galactica-30B 41.6 11.8 13.2 37.7 37.9 28.4
Tulu 59.0 10.7 10.3 31.3 39.8 30.2
Platypus 51.7 13.8 13.6 38.6 41.0 31.7
MAmmoTH-Coder 84.3 65.4 51.8 60.9 53.8 63.2
∆ +15 +31 +38 +22 +13 +32
Mô hình Tham số 65-70B
Llama-1 55.3 14.2 15.2 37.4 44.1 33.2
Llama-2 63.8 20.5 14.0 51.3 47.1 39.3
Llama-2-Chat 71.5 19.2 21.7 44.1 46.9 40.6
WizardMath 71.8 17.1 37.9 13.2 27.4 33.4
Guanaco 66.8 17.8 20.2 50.0 47.3 40.4
Platypus 51.8 26.3 21.7 55.9 52.5 41.6
MAmmoTH 82.4 55.6 51.4 66.4 56.7 62.5
∆ +11 +29 +14 +11 +4 +21
Bảng 4: Bảng tổng hợp tất cả kết quả đánh giá ngoài miền. Kết quả được đánh dấu † được sao chép
từ các bài báo khác, có thể tìm thấy trên bảng xếp hạng paperswithcode.
và bộ dữ liệu MATH. Do đó, kết quả của WizardMath có tính cạnh tranh cao trên hai bộ dữ liệu này.
Tuy nhiên, huấn luyện cụ thể cho bộ dữ liệu có thể có hại cho các bộ dữ liệu OOD như AQuA. Ngược lại,
Platypus tinh chỉnh LLM trên một phạm vi rộng các bộ dữ liệu lý luận văn bản và toán học. Nó cải thiện
SoTA mã nguồn mở trên một số bộ dữ liệu. Tương tự, MAmmoTH có thể đạt được cải thiện tổng thể trên toàn bộ bảng. Một quan sát chính là MAmmoTH đặc biệt mạnh trong việc giải quyết các bài toán toán học phức tạp hơn trong MATH, nơi mức tăng của mô hình chúng tôi so với WizardMath (mã nguồn mở SoTA trên
MATH) có thể vượt quá 25% ở các quy mô khác nhau.
7

--- TRANG 8 ---
Bản thảo. Công trình đang triển khai
8.620.419.932.822.927.029.629.832.039.941.641.042.648.147.90.020.040.060.0
GSM + MATHNgoài miềnTổng thểĐộ chính xácLlama-2 Cơ sởWizardMath (GSM + MATH CoT)MAmmoTH (MathInstruct - CoT)MAmmoTH (MathInstruct - PoT)MAmmoTH (MathInstruct - Kết hợp)
Hình 2: Điều tra về ảnh hưởng của huấn luyện kết hợp CoT & PoT trên mô hình Llama-2 7B.
"Ngoài miền" đề cập đến năm bộ dữ liệu được chi tiết trong Bảng 2. Các thông tin quan trọng bao gồm: 1) Mô hình
SoTA, sử dụng tinh chỉnh CoT cụ thể cho bộ dữ liệu trên GSM và MATH, thể hiện hiệu suất mạnh trong các miền của nó nhưng gặp khó khăn trong các tình huống OOD; 2) Các nguồn dữ liệu đa dạng trong MathInstruct
cho phép mô hình toán học tổng quát tốt hơn; 3) Tinh chỉnh trên các tập con PoT thường vượt trội hơn tinh chỉnh trên các tập con CoT; 4) Huấn luyện kết hợp mang lại mô hình hiệu suất tốt nhất. Kết quả phân tích trên từng bộ dữ liệu có thể tìm thấy trong Bảng 6 Phụ lục.
Từ Bảng 4, chúng ta có thể quan sát rằng đối thủ chính của chúng tôi cho các bộ dữ liệu OOD là Platypus (Lee et al.,
2023). Tương tự như kết quả trong miền, Platypus có thể mang lại mức tăng so với các mô hình cơ sở
trên toàn bộ bảng, đặc biệt trên bộ dữ liệu MMLU-Math, ngang bằng với MAmmoTH-70B. Đáng chú ý là mức tăng hiệu suất của mô hình chúng tôi trên các bộ dữ liệu OOD thậm chí còn đáng kể hơn
so với các bộ dữ liệu trong miền. Điều này chứng tỏ khả năng tổng quát hóa đáng kể của các mô hình chúng tôi
cho các bài toán toán học chưa từng thấy. Đáng chú ý, MAmmoTH-7B cũng thúc đẩy hiệu suất CoT của WizardMath-7B đáng kể
trên MMLU-Math 9%, bao gồm số lượng đáng kể các câu hỏi ngoài các chủ đề chúng tôi
bao phủ trong bộ dữ liệu huấn luyện của chúng tôi.
So sánh giữa Các Mô hình Cơ sở Khác nhau. Trong các thí nghiệm của chúng tôi, chúng tôi thử nghiệm với cả
Llama-2 và Code-Llama làm mô hình cơ sở. Từ hai bảng, chúng ta có thể quan sát rằng Code-
Llama liên tục tốt hơn Llama-2, đặc biệt trên các bộ dữ liệu OOD. Khoảng cách giữa MAmmoTH
và MAmmoTH-Coder thậm chí có thể đạt đến 5%. Đáng ngạc nhiên, hiệu suất trung bình trên
các bộ dữ liệu OOD của MAmmoTH-Coder (34B) thực sự cao hơn MAmmoTH (70B). Chúng tôi tin
MAmmoTH-Coder hưởng lợi rất nhiều từ huấn luyện mã liên tục của Code-Llama, không chỉ nâng cao khả năng PoT mà còn cải thiện kỹ năng lý luận tổng quát của Llama.
3.4 NGHIÊN CỨU LOẠI TRỪ VỀ NGUỒN DỮ LIỆU
Loại trừ Nguồn Dữ liệu. Để hiểu rõ hơn những yếu tố nào đóng góp cho mức tăng lớn
của MAmmoTH so với các đường chuẩn hiện có, chúng tôi thiết lập một nhóm thí nghiệm kiểm soát trong Hình 2. Chúng tôi
nghiên cứu các thiết lập sau:
(1)MAmmoTH (MathInstruct - CoT): Thí nghiệm này nhằm hiểu mức độ dữ liệu CoT được tuyển chọn của chúng tôi có thể cải thiện tổng quát hóa so với mô hình SoTA WizardMath (Luo et al., 2023)
được huấn luyện cụ thể trên GSM + MATH. Như có thể thấy, trong khi hy sinh độ chính xác trên GSM + MATH
3%, tinh chỉnh tập con CoT của chúng tôi cải thiện độ chính xác tổng thể chín bộ dữ liệu từ 27% lên 32%.
(2)MAmmoTH (MathInstruct - PoT): Thí nghiệm này nhằm hiểu ưu thế của tập con PoT
của chúng tôi. Như có thể quan sát, tinh chỉnh tập con PoT của chúng tôi có thể cải thiện đáng kể độ chính xác tổng thể từ 27% lên 41%. Sự loại trừ này phản ánh tầm quan trọng của việc mở khóa khả năng tạo chương trình
của mô hình chúng tôi.
(3)MAmmoTH (MathInstruct - Kết hợp): Chúng tôi tiếp tục kết hợp CoT và PoT làm dữ liệu huấn luyện kết hợp để đạt hiệu suất tổng thể tốt nhất là 47,9%. Mức tăng kết hợp này đến từ hai khía cạnh:
• Tập con CoT giúp duy trì kỹ năng lý luận dựa trên ngôn ngữ chung để xử lý các tình huống mà
PoT không thể xử lý tốt, ví dụ, các câu hỏi trắc nghiệm lý luận trừu tượng trong AQuA và MMLU.
• Tập con PoT có thể dạy mô hình cách sử dụng API Python để giải quyết các bài toán toán học phức tạp
với độ chính xác cao, ví dụ, các bài toán MATH yêu cầu tính toán phức tạp.
8

--- TRANG 9 ---
Bản thảo. Công trình đang triển khai
Dữ liệu Huấn luyện GSM MATH AQuA NumG SVA Mat Sim SAT MMLU TBình
- 14.6 2.5 30.3 29.9 34.5 6.0 5.0 26.8 29.8 -25.3
G 56.6 9.2 24.4 32.1 65.4 20.5 12.3 27.2 25.2 -22.7
G + M 58.1 28.2 26.0 34.7 64.8 50.1 17.1 28.6 28.4 -19.5
G + M + C 57.4 28.5 26.2 37.5 65.3 50.4 17.7 29.3 28.7 -19.2
G + M + C + A 57.5 29.1 46.9 42.2 65.8 49.6 32.7 42.3 43.1 -4.8
G + M + C + A + N 56.5 28.9 38.2 63.7 64.1 47.9 40.8 38.6 44.5 -3.4
Dữ liệu Hiện có 31.4 18.4 40.3 53.3 61.8 27.9 45.6 32.7 38.4 -9.0
MathInstruct 53.6 31.5 44.5 61.2 67.7 46.3 41.2 42.7 42.6 47.9
Bảng 5: Ảnh hưởng của các tập con chính khác nhau trong MathInstruct dựa trên Llama-2 7B. G: GSM8K,
M: MATH, C: Camel, A: AQuA, N: NumGLUE. "Dữ liệu hiện có": tập con của MathInstruct
trong Bảng 1 bằng cách loại trừ tất cả lý luận MỚI được chúng tôi tuyển chọn. Chúng tôi rút gọn Mathematics thành Mat,
SimulEq thành Sim, NumGLUE thành NumG, và SVAMP thành SVA để tiết kiệm không gian.
Chúng tôi đặt một số nghiên cứu trường hợp trong Phụ lục B để chứng minh ưu thế tương ứng của PoT và CoT
trong việc giải quyết các loại bài toán toán học khác nhau. Để tóm tắt, chúng tôi quy mức tăng đáng kể cho: 1)
nguồn dữ liệu đa dạng bao phủ các lĩnh vực toán học và mức độ phức tạp khác nhau và 2) sự kết hợp của CoT &
PoT điều chỉnh hướng dẫn và chiến lược giải mã.
Ảnh hưởng của Các Tập con Chính. Với các nguồn đa dạng của MathInstruct được sử dụng trong huấn luyện
MAmmoTH, điều quan trọng là hiểu mỗi bộ dữ liệu đóng góp như thế nào vào hiệu suất tổng thể của
mô hình. Chúng tôi tập trung vào năm tập con quan trọng: GSM8K, MATH, Camel, AQuA và NumGLUE.
Chúng tôi tiến hành thí nghiệm dần dần thêm từng bộ dữ liệu vào huấn luyện và so sánh hiệu suất
với mô hình được tinh chỉnh trên toàn bộ MathInstruct. Như chúng ta có thể thấy từ Bảng 5, khi dữ liệu
không đa dạng trong huấn luyện ở đầu (ví dụ, chỉ GSM8K), hiệu suất tổng quát hóa tổng thể rất tệ: mô hình chỉ phù hợp với dữ liệu trong phân phối và gặp khó khăn trả lời các câu hỏi ngoài câu hỏi GSM. Và khi dần dần thêm các tập con chính khác, ngoài việc thấy cải thiện
trên bộ kiểm tra riêng của nó, chúng ta có thể quan sát MAmmoTH trở thành một nhà toán học tổng quát tốt hơn.
Những kết quả này nhấn mạnh tác động đáng kể của các nguồn dữ liệu đa dạng đến hiệu suất MAmmoTH,
một khía cạnh cốt lõi của việc làm cho MAmmoTH trở thành một nhà toán học tổng quát. Kết quả cũng cung cấp những hiểu biết có giá trị
cho các nỗ lực tuyển chọn và thu thập dữ liệu trong tương lai (ví dụ, chúng ta nên luôn thu thập dữ liệu đa dạng và tránh
thu thập chỉ các loại dữ liệu cụ thể).
Để giúp hiểu đóng góp của 6 bộ dữ liệu mới được tuyển chọn như thể hiện trong Bảng 1, chúng tôi loại bỏ
chúng khỏi MathInstruct, và huấn luyện một mô hình trên dữ liệu hiện có. Như thể hiện trong hai hàng cuối
của Bảng 5, dữ liệu mới được tuyển chọn của chúng tôi cải thiện đáng kể hiệu suất trên nhiều bộ dữ liệu và dẫn đến mức tăng 9% tổng thể, phản ánh tầm quan trọng của bộ dữ liệu được tuyển chọn MỚI.
Ảnh hưởng của Giải mã Kết hợp. Để chứng minh hiệu quả của phương pháp giải mã kết hợp, chúng tôi tiến hành thí nghiệm như được nêu trong tiểu mục 2.4. Theo mặc định, chúng tôi ban đầu thử phương pháp giải mã PoT cho một câu hỏi nhất định. Nếu nó không tạo ra một truy vấn có thể thực thi, chúng tôi sau đó chuyển sang
phương pháp giải mã CoT. Hiệu suất của các phương pháp giải mã khác nhau (CoT, PoT, và Kết hợp)
được thể hiện trong Bảng 7. Giải mã kết hợp này cải thiện hiệu suất trên mọi bộ kiểm tra, cho thấy rằng
mô hình của chúng tôi có thể tận dụng hiệu quả điểm mạnh của cả chiến lược giải mã CoT và PoT.
4 KẾT LUẬN
Trong bài báo này, chúng tôi đề xuất một cách tiếp cận điều chỉnh hướng dẫn toán học mới để kích hoạt khả năng lý luận toán học của các LLM mã nguồn mở. Thông qua nghiên cứu toàn diện, chúng tôi cho thấy rằng các mô hình của chúng tôi
có thể vượt trội so với hiệu suất SoTA ở các quy mô khác nhau với biên độ lớn. Các mô hình của chúng tôi hưởng lợi
rất nhiều từ: 1) phủ sóng rộng các lĩnh vực toán học và mức độ phức tạp khác nhau, và 2) sự kết hợp của
huấn luyện CoT và PoT. Bộ dữ liệu điều chỉnh hướng dẫn của chúng tôi chứa 260K mẫu, làm cho tinh chỉnh
có thể chi trả cao ngay cả đối với các phòng thí nghiệm học thuật. Công trình của chúng tôi mở đường cho các nghiên cứu tương lai
để kích hoạt khả năng cốt lõi của LLM trong các lĩnh vực chuyên biệt.
9

--- TRANG 10 ---
Bản thảo. Công trình đang triển khai
TÀI LIỆU THAM KHẢO
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, và Hannaneh
Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based
formalisms. Trong Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long
and Short Papers), trang 2357–2367, 2019. doi: 10.18653/v1/N19-1245. URL https://
aclanthology.org/N19-1245.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.
ArXiv preprint, abs/2305.10403, 2023. URL https://arxiv.org/abs/2305.10403.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harm-
lessness from ai feedback. ArXiv preprint, abs/2212.08073, 2022. URL https://arxiv.
org/abs/2212.08073.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. ArXiv preprint, abs/2107.03374, 2021. URL https://
arxiv.org/abs/2107.03374.
Wenhu Chen, Xueguang Ma, Xinyi Wang, và William W Cohen. Program of thoughts prompt-
ing: Disentangling computation from reasoning for numerical reasoning tasks. ArXiv preprint,
abs/2211.12588, 2022. URL https://arxiv.org/abs/2211.12588.
Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi
Wang, và Pan Lu. Theoremqa: A theorem-driven question answering dataset. ArXiv preprint,
abs/2305.12524, 2023. URL https://arxiv.org/abs/2305.12524.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod-
els. ArXiv preprint, abs/2210.11416, 2022. URL https://arxiv.org/abs/2210.11416.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. ArXiv preprint, abs/2110.14168, 2021. URL https://arxiv.
org/abs/2110.14168.
Alex Davies, Petar Veličković, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomašev,
Richard Tanburn, Peter Battaglia, Charles Blundell, András Juhász, et al. Advancing mathe-
matics by guiding human intuition with ai. Nature, 600(7887):70–74, 2021. URL https:
//www.nature.com/articles/s41586-021-04086-x.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. Qlora: Efficient finetuning
of quantized llms. ArXiv preprint, abs/2305.14314, 2023. URL https://arxiv.org/abs/
2305.14314.
Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen,
Olivier Bousquet, và Denny Zhou. Compositional semantic parsing with large language mod-
els. International Conference on Learning Representations (ICLR), 2023. URL https:
//openreview.net/forum?id=gJW8hSGBys8.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, và
Graham Neubig. Pal: Program-aided language models. Trong International Conference on Machine
Learning, trang 10764–10799. PMLR, 2023. URL https://proceedings.mlr.press/
v202/gao23f/gao23f.pdf.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, và Weizhu Chen.
Critic: Large language models can self-correct with tool-interactive critiquing. ArXiv preprint,
abs/2305.11738, 2023. URL https://arxiv.org/abs/2305.11738.
10

--- TRANG 11 ---
Bản thảo. Công trình đang triển khai
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob
Steinhardt. Measuring massive multitask language understanding. Trong 9th International Confer-
ence on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021a.
URL https://openreview.net/forum?id=d7KBjmI3GmQ.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric
Tang, Dawn Song, và Jacob Steinhardt. Measuring mathematical problem solv-
ing with the math dataset. Trong Thirty-fifth Conference on Neural Information Pro-
cessing Systems Datasets and Benchmarks Track (Round 2), 2021b. URL https:
//datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/
be83ab3ecd0db773eb2dc1b0a17836a1-Paper-round2.pdf.
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, và Nate Kushman. Learning to
solve arithmetic word problems with verb categorization. Trong Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing (EMNLP), trang 523–533, 2014. doi: 10.
3115/v1/D14-1058. URL https://aclanthology.org/D14-1058.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, và Yusuke Iwasawa. Large
language models are zero-shot reasoners. NeurIPS, 2022.
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, và Siena Du-
mas Ang. Parsing algebraic word problems into equations. Transactions of the Association
for Computational Linguistics, 3:585–597, 2015. doi: 10.1162/tacl_a_00160. URL https:
//aclanthology.org/Q15-1042.
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, và Hannaneh Hajishirzi.
MAWPS: A math word problem repository. Trong Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, trang 1152–1157, 2016. doi: 10.18653/v1/N16-1136. URL https://aclanthology.
org/N16-1136.
Ariel N Lee, Cole J Hunter, và Nataniel Ruiz. Platypus: Quick, cheap, and powerful refinement
of llms. ArXiv preprint, abs/2308.07317, 2023. URL https://arxiv.org/abs/2308.
07317.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-
masesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative
reasoning problems with language models. Advances in Neural Information Processing Systems,
35:3843–3857, 2022. URL https://openreview.net/pdf?id=IFXTZERXdM7.
Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, và Bernard Ghanem.
Camel: Communicative agents for" mind" exploration of large scale language model society.
ArXiv preprint, abs/2303.17760, 2023a. URL https://arxiv.org/abs/2303.17760.
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, và Weizhu Chen. Making
language models better reasoners with step-aware verifier. Trong Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 5315–
5333, 2023b. URL https://aclanthology.org/2023.acl-long.291.pdf.
Wang Ling, Dani Yogatama, Chris Dyer, và Phil Blunsom. Program induction by rationale gener-
ation: Learning to solve and explain algebraic word problems. Trong Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 158–167,
2017. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.
Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective in-
struction tuning. ICML, 2023. URL https://openreview.net/pdf?id=ZX4uS605XV.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qing-
wei Lin, Shifeng Chen, và Dongmei Zhang. Wizardmath: Empowering mathematical reasoning
for large language models via reinforced evol-instruct. ArXiv preprint, abs/2308.09583, 2023.
URL https://arxiv.org/abs/2308.09583.
11

--- TRANG 12 ---
Bản thảo. Công trình đang triển khai
Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, và Graham Neubig. Language mod-
els of code are few-shot commonsense learners. Trong Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing, trang 1384–1403, 2022. URL https:
//aclanthology.org/2022.emnlp-main.90.pdf.
Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tan-
may Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, và Ashwin Kalyan. LILA:
A unified benchmark for mathematical reasoning. Trong Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing, trang 5807–5832, 2022a. URL https:
//aclanthology.org/2022.emnlp-main.392.
Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral,
và Ashwin Kalyan. NumGLUE: A suite of fundamental yet challenging mathematical reasoning
tasks. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), trang 3505–3523, 2022b. doi: 10.18653/v1/2022.acl-long.246. URL
https://aclanthology.org/2022.acl-long.246.
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, và
Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. ArXiv
preprint, abs/2306.02707, 2023. URL https://arxiv.org/abs/2306.02707.
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,
và Caiming Xiong. Codegen: An open large language model for code with multi-turn program
synthesis. Trong International Conference on Learning Representations (ICLR), 2023. URL https:
//openreview.net/pdf?id=iaYcJKpY2B_.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:
Scratchpads for intermediate computation with language models. Trong Deep Learning for Code
Workshop, 2022. URL https://arxiv.org/abs/2112.00114.
OpenAI. Gpt-4 technical report. ArXiv preprint, abs/2303.08774, 2023. URL https://arxiv.
org/abs/2303.08774.
Arkil Patel, Satwik Bhattamishra, và Navin Goyal. Are NLP models really able to solve simple
math word problems? Trong Proceedings of the 2021 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, trang 2080–
2094, 2021. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology.org/
2021.naacl-main.168.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,
Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, và Julien Launay. The refinedweb
dataset for falcon llm: outperforming curated corpora with web data, and web data only. ArXiv
preprint, abs/2306.01116, 2023. URL https://arxiv.org/abs/2306.01116.
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng Gao. Instruction tuning
with gpt-4. ArXiv preprint, abs/2304.03277, 2023. URL https://arxiv.org/abs/2304.
03277.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, và Yuxiong He. Zero: Memory opti-
mizations toward training trillion parameter models. Trong SC20: International Conference for
High Performance Computing, Networking, Storage and Analysis, trang 1–16. IEEE, 2020. URL
https://dl.acm.org/doi/10.5555/3433701.3433727.
Subhro Roy và Dan Roth. Solving general arithmetic word problems. Trong Proceedings of the 2015
Conference on Empirical Methods in Natural Language Processing, trang 1743–1752, 2015. doi:
10.18653/v1/D15-1202. URL https://aclanthology.org/D15-1202.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code.
ArXiv preprint, abs/2308.12950, 2023. URL https://arxiv.org/abs/2308.12950.
12

--- TRANG 13 ---
Bản thảo. Công trình đang triển khai
Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,
Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish
Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V.
Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica,
Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,
Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan,
Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, và Alexander M. Rush. Multitask
prompted training enables zero-shot task generalization. Trong The Tenth International Confer-
ence on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. URL
https://openreview.net/forum?id=9Vrb9D0WI4.
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks
and whether chain-of-thought can solve them. ArXiv preprint, abs/2210.09261, 2022. URL
https://arxiv.org/abs/2210.09261.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, và Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.
https://github.com/tatsu-lab/stanford_alpaca, 2023.
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,
Andrew Poulton, Viktor Kerkez, và Robert Stojnic. Galactica: A large language model for
science. ArXiv preprint, abs/2211.09085, 2022. URL https://arxiv.org/abs/2211.
09085.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. ArXiv preprint, abs/2302.13971, 2023a. URL https:
//arxiv.org/abs/2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foun-
dation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023b. URL https:
//arxiv.org/abs/2307.09288.
Boshi Wang, Xiang Deng, và Huan Sun. Iteratively prompt pre-trained language models for chain
of thought. Trong Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, trang 2714–2730. Association for Computational Linguistics, 2022a. URL https:
//aclanthology.org/2022.emnlp-main.174.
Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, và Huan Sun.
Towards understanding chain-of-thought prompting: An empirical study of what matters. Trong Pro-
ceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), trang 2717–2739. Association for Computational Linguistics, 2023a. doi: 10.18653/
v1/2023.acl-long.153. URL https://aclanthology.org/2023.acl-long.153.
Boshi Wang, Xiang Yue, và Huan Sun. Can chatgpt defend the truth? automatic dialectical eval-
uation elicits llms' deficiencies in reasoning. ArXiv preprint, abs/2305.13160, 2023b. URL
https://arxiv.org/abs/2305.13160.
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, và Ee-Peng Lim.
Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language
models. ArXiv preprint, abs/2305.04091, 2023c. URL https://arxiv.org/abs/2305.
04091.
Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, và Zhi-
fang Sui. Making large language models better reasoners with alignment. ArXiv preprint,
abs/2309.02144, 2023d. URL https://arxiv.org/abs/2309.02144.
Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R
Loomba, Shichang Zhang, Yizhou Sun, và Wei Wang. Scibench: Evaluating college-level scien-
tific problem-solving abilities of large language models. ArXiv preprint, abs/2307.10635, 2023e.
URL https://arxiv.org/abs/2307.10635.
13

--- TRANG 14 ---
Bản thảo. Công trình đang triển khai
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, và Denny Zhou. Self-
consistency improves chain of thought reasoning in language models. International Conference
on Learning Representations (ICLR), 2023f. URL https://openreview.net/pdf?id=
1PL1NIMMrw.
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,
Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Es-
haan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob An-
derson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi,
Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravse-
haj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan
Reddy A, Sumanta Patro, Tanay Dixit, và Xudong Shen. Super-NaturalInstructions: Gener-
alization via declarative instructions on 1600+ NLP tasks. Trong Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language Processing, trang 5085–5109, 2022b. URL
https://aclanthology.org/2022.emnlp-main.340.
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi
Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels
go? exploring the state of instruction tuning on open resources. ArXiv preprint, abs/2306.04751,
2023g. URL https://arxiv.org/abs/2306.04751.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, và
Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.
The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023), 2023h.
URL https://aclanthology.org/2023.acl-long.754.pdf.
Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, và Steven CH Hoi.
Codet5+: Open code large language models for code understanding and generation. ArXiv
preprint, abs/2305.07922, 2023i. URL https://arxiv.org/abs/2305.07922.
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, và Quoc V. Le. Finetuned language models are zero-shot learners. Trong The Tenth
International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,
2022, 2022a. URL https://openreview.net/forum?id=gEZrGCozdqR.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,
Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.
Advances in Neural Information Processing Systems, 35:24824–24837, 2022b. URL https:
//openreview.net/pdf?id=_VjQlMeSB_J.
Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, và Quoc V Le. Simple synthetic data reduces
sycophancy in large language models. ArXiv preprint, abs/2308.03958, 2023. URL https:
//arxiv.org/abs/2308.03958.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface's transform-
ers: State-of-the-art natural language processing. ArXiv preprint, abs/1910.03771, 2019. URL
https://arxiv.org/abs/1910.03771.
Sang Michael Xie, Aditi Raghunathan, Percy Liang, và Tengyu Ma. An explanation of in-
context learning as implicit bayesian inference. Trong The Tenth International Conference on
Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. URL https:
//openreview.net/forum?id=RdJVFCHjUMI.
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, và Qizhe
Xie. Decomposition enhances reasoning via self-evaluation guided decoding. ArXiv preprint,
abs/2305.00633, 2023. URL https://arxiv.org/abs/2305.00633.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, và
Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.
ArXiv preprint, abs/2304.12244, 2023. URL https://arxiv.org/abs/2304.12244.
14

--- TRANG 15 ---
Bản thảo. Công trình đang triển khai
Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, và Jie Tang.
Gpt can solve mathematical problems without a calculator. ArXiv preprint, abs/2309.03241, 2023.
URL https://arxiv.org/abs/2309.03241.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran, Karthik R Narasimhan, và Yuan Cao. Re-
act: Synergizing reasoning and acting in language models. Trong International Conference on
Learning Representations (ICLR), 2023. URL https://openreview.net/pdf?id=WE_
vluYUL-X.
Qinyuan Ye, Bill Yuchen Lin, và Xiang Ren. CrossFit: A few-shot learning challenge for cross-
task generalization in NLP. Trong Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing, trang 7163–7189, 2021. doi: 10.18653/v1/2021.emnlp-main.572.
URL https://aclanthology.org/2021.emnlp-main.572.
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok,
Zhenguo Li, Adrian Weller, và Weiyang Liu. Metamath: Bootstrap your own mathematical
questions for large language models. ArXiv preprint, abs/2309.12284, 2023. URL https:
//arxiv.org/abs/2309.12284.
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, và Chang Zhou. Scaling
relationship on learning mathematical reasoning with large language models. ArXiv preprint,
abs/2308.01825, 2023. URL https://arxiv.org/abs/2308.01825.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-
pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer
language models. ArXiv preprint, abs/2205.01068, 2022. URL https://arxiv.org/abs/
2205.01068.
Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, và Yu Li. Progressive-hint prompting
improves reasoning in large language models. ArXiv preprint, abs/2304.09797, 2023a. URL
https://arxiv.org/abs/2304.09797.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. ArXiv preprint, abs/2306.05685, 2023b. URL https://arxiv.org/abs/
2306.05685.
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,
Weizhu Chen, và Nan Duan. Agieval: A human-centric benchmark for evaluating foundation
models. ArXiv preprint, abs/2304.06364, 2023. URL https://arxiv.org/abs/2304.
06364.
Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia,
Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code
interpreter with code-based self-verification. ArXiv preprint, abs/2308.07921, 2023a. URL
https://arxiv.org/abs/2308.07921.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,
Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. ArXiv preprint, abs/2305.11206, 2023b.
URL https://arxiv.org/abs/2305.11206.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-
mans, Olivier Bousquet, Quoc Le, và Ed Chi. Least-to-most prompting enables complex rea-
soning in large language models. International Conference on Learning Representations (ICLR),
2023c. URL https://openreview.net/pdf?id=WZH7099tgfM.
15

--- TRANG 16 ---
Bản thảo. Công trình đang triển khai
A CÔNG TRÌNH LIÊN QUAN
A.1 BỘ DỮ LIỆU LÝ LUẬN TOÁN HỌC
Công trình của chúng tôi dựa trên văn học lý luận toán học hiện có. Ban đầu, lý luận toán học
chủ yếu tập trung vào giải quyết các bài toán toán học cơ bản tổng hợp như AddSub (Hosseini et al.,
2014) và các bộ dữ liệu lý luận số học khác (Koncel-Kedziorski et al., 2015; Roy & Roth, 2015;
Patel et al., 2021). Sau đó, các bộ dữ liệu bài toán từ toán học khó hơn (Cobbe et al., 2021; Amini
et al., 2019; Ling et al., 2017; Hendrycks et al., 2021b) đã được đề xuất để tập trung vào việc giải quyết
các bài toán từ toán học thực tế. NumGLUE (Mishra et al., 2022b) và LiLA (Mishra et al., 2022a)
tổng hợp văn học hiện có để xây dựng một bộ sưu tập bộ dữ liệu đa dạng hơn. Tuy nhiên, các bộ dữ liệu này
chủ yếu tập trung vào các bài toán toán học cấp trường. Để tiếp tục kiểm tra giới hạn của LLM trong việc giải quyết các
bài toán toán học phức tạp hơn, MMLU (Hendrycks et al., 2021a) bao gồm các bài toán toán học đại học trong
bộ đánh giá của nó. Gần đây hơn, (Chen et al., 2023; Wang et al., 2023e) đã đề xuất giải quyết
các bài toán khoa học và toán học cấp đại học thách thức hơn. Bộ dữ liệu điều chỉnh hướng dẫn của chúng tôi được xây dựng
dựa trên công trình hiện có để bao gồm một bộ sưu tập đa dạng các bài toán toán học từ các lĩnh vực con khác nhau.
A.2 LÝ LUẬN VỚI CÁC MÔ HÌNH NGÔN NGỮ LỚN
LLM đã chứng minh khả năng tuyệt vời để lý luận với sự giúp đỡ của gợi ý Chuỗi-tư-duy
(Wei et al., 2022b; Kojima et al., 2022; Wang et al., 2023f). Suzgun et al. (2022) đã chỉ ra
rằng CoT đã có thể vượt qua hiệu suất con người trên các nhiệm vụ BIG-Bench thách thức. Sau đó, một số
công trình khác (Drozdov et al., 2023; Zhou et al., 2023c; Nye et al., 2022; Wang et al., 2022a;
2023a; Li et al., 2023b; Wang et al., 2023d; Yu et al., 2023) cũng đề xuất các cách tiếp cận khác nhau để
sử dụng LLM giải quyết các nhiệm vụ lý luận bằng cách cho phép các bước trung gian. ReAct Yao et al. (2023) đề xuất tận dụng các công cụ bên ngoài như công cụ tìm kiếm để nâng cao kỹ năng lý luận LLM. Một xu hướng khác
là cho phép khả năng của LLM sử dụng chương trình làm quy trình tư duy như PoT (Chen et al., 2022).
Một số công trình tiếp theo bao gồm tự phê bình (Gou et al., 2023), tự đánh giá (Xie et al., 2023), lập kế hoạch-và-
giải quyết (Wang et al., 2023c). Những phương pháp này đề xuất nâng cao khả năng của LLM giải quyết các bài toán toán học
với PoT. Tự phê bình (Gou et al., 2023) và tự đánh giá (Xie et al., 2022) đều áp dụng tự
đánh giá để nâng cao độ bền của chương trình được tạo ra. Lập kế hoạch-và-giải quyết (Wang et al., 2023c)
thay vào đó áp dụng hướng dẫn lập kế hoạch chi tiết hơn để giúp LLM tạo ra một kế hoạch lý luận cấp cao.
Những phương pháp này đều chứng minh mang lại cải thiện đáng kể so với PoT.
A.3 ĐIỀU CHỈNH HƯỚNG DẪN TRONG CÁC MÔ HÌNH NGÔN NGỮ
Điều chỉnh hướng dẫn là một phần của dòng công trình được thiết kế để "căn chỉnh" các mô hình ngôn ngữ với các
mục tiêu hữu ích hơn và sở thích của con người. Bước điều chỉnh hướng dẫn được xem như một bước chính để kích hoạt khả năng nhất định của LLM để phản hồi các hướng dẫn của con người. Trước đây, điều chỉnh hướng dẫn chủ yếu tập trung vào việc nâng cao khả năng tuân theo hướng dẫn mục đích tổng quát của LLM. Kể từ 2021,
CrossFit (Ye et al., 2021) và NaturalInstruction (Wang et al., 2022b), FLAN (Wei et al., 2022a)
và T0 (Sanh et al., 2022) nằm trong số làn sóng đầu tiên của nỗ lực điều chỉnh hướng dẫn để hiểu
khả năng tổng quát hóa của LLM. Sau đó, FLAN-v2 (Chung et al., 2022; Longpre et al., 2023)
đã được đề xuất để hiểu tác động của việc mở rộng quy mô các bộ dữ liệu hướng dẫn để hiểu
tác động của nó đến hiệu suất mô hình. Những cách tiếp cận này chủ yếu áp dụng các bộ dữ liệu được chú thích bởi con người để xây dựng bộ dữ liệu tuân theo hướng dẫn. Gần đây hơn, nhiều công trình (Wang et al., 2023h; Xu et al.,
2023; Peng et al., 2023; Zhou et al., 2023b; Wang et al., 2023g) đề xuất sử dụng dữ liệu tuân theo hướng dẫn
tổng hợp chưng cất từ GPT-3/4 để căn chỉnh các LLM mã nguồn mở. Nỗ lực tương tự nhất với
chúng tôi là Platypus (Lee et al., 2023) nhằm sử dụng một bộ dữ liệu chuyên biệt theo lĩnh vực để xây dựng một
bộ dữ liệu tuân theo hướng dẫn quy mô nhỏ để nâng cao khả năng lý luận của LLM.
16

--- TRANG 17 ---
Bản thảo. Công trình đang triển khai
B NGHIÊN CỨU TRƯỜNG HỢP
Chúng tôi tiến hành so sánh giữa kết quả PoT và CoT trong Hình 3, Hình 4 và
Hình 5. Trong ví dụ đầu tiên, mặc dù cả PoT và CoT đều có thể giải quyết bài toán, CoT đưa ra
một giải pháp rất dài dòng để đưa ra câu trả lời. Giải pháp như vậy không chỉ chậm mà còn không ổn định. Trong
trường hợp thứ hai và thứ ba, chúng ta có thể tiếp tục thấy ưu thế của PoT so với CoT bằng cách sử dụng các công cụ bên ngoài và gói Python để đơn giản hóa đáng kể giải pháp. Hình 6 cho thấy một số loại câu hỏi
(đặc biệt là câu hỏi logic hình thức) không dễ được xử lý bằng chương trình. Để giải quyết
những loại câu hỏi này, CoT là một lựa chọn tốt hơn.
PoT
from sympy import factorial value = factorial(factorial(3)) / factorial(3) print(value) >>> 120 Trước tiên, chúng ta cần hiểu biểu thức đã cho: !!!!! Bước 1: Đơn giản hóa biểu thức bên trong dấu ngoặc đơn. 3! = 3 × 2 × 1 = 6 Vậy, biểu thức trở thành: #!# Bước 2: Nhớ lại thuộc tính của giai thừa nói rằng 𝑛!$ = 𝑛! cho bất kỳ số nguyên dương 𝑘 và bất kỳ số nguyên dương 𝑛 ≥ 𝑘. Trong trường hợp của chúng ta, chúng ta có 𝑛 = 6 và 𝑘 = 6. Sử dụng thuộc tính này, chúng ta có thể viết lại biểu thức thành: #!# Bước 3: Đơn giản hóa biểu thức hơn nữa bằng cách áp dụng thuộc tính được đề cập trong Bước 2. 6! = 6 × 5 × 4 × 3 × 2 × 1 = 720 Vậy, biểu thức trở thành: %&'# Bước 4: Chia 720 cho 6. 720 ÷ 6 = 120 Vậy, biểu thức đơn giản hóa cuối cùng là: 120 Do đó, giá trị của !!!!! là 120. Câu trả lời là: 𝟏𝟐𝟎
CoT Giá trị của !!!!! là gì?
Hình 3: Ví dụ 1: PoT và CoT đều có thể giải quyết bài toán, tuy nhiên, CoT đưa ra một giải pháp rất dài dòng để đưa ra câu trả lời.
PoT
from sympy import factorial word = "TEEPE" n = len(word) d = word.count('E') ways = factorial(n)/factorial(d) print(ways) >>> 20 Trước tiên chúng ta đếm các cách sắp xếp nếu tất cả các chữ cái là duy nhất, là 4!. Sau đó vì các E không phải là duy nhất, chúng ta chia cho 2! cho các cách sắp xếp của hai E, với câu trả lời 4! 2! = 60. Câu trả lời là: 𝟔𝟎
CoT Xác định số cách sắp xếp các chữ cái của từ TEPEE.
Hình 4: Ví dụ 2: PoT tạo ra giải pháp đúng trong khi CoT thất bại.
17

--- TRANG 18 ---
Bản thảo. Công trình đang triển khai
Câu hỏi: 𝑎 được chọn từ tập hợp {1,2,3,4}, và 𝑏 được chọn từ tập hợp {5,6,7,8,9}. Xác suất tích 𝑎𝑏 là số nguyên tố là bao nhiêu?PoT
from sympy import isprime # define the sets a_set = {1,2,3,4} b_set = {5,6,7,8,9} n_a = len(a_set) n_b = len(b_set) prime_count = 0 prime_ab_count = 0 prime_a_count = prime_ab_count prime_b_count = prime_ab_count # iterate over all possible pairs for a in a_set: for b in b_set: if isprime(a) or isprime(b): if isprime(a*b): # increment the counters prime_count += 1 prime_ab_count += 1 # check if a is prime if isprime(a): prime_a_count += 1 if isprime(b): prime_b_count += 1 probability = prime_ab_count / (n_a * n_b) print(probability) >>> 0.1 Trước tiên, chúng ta cần tìm tổng số cách kết hợp có thể của việc chọn một số từ mỗi tập hợp. Có 4 số trong tập hợp đầu tiên và 5 số trong tập hợp thứ hai. Vậy, có 4 ⋅ 5 = 20 cách kết hợp có thể. Bây giờ, hãy xem xét các số nguyên tố. Một số nguyên tố là một số lớn hơn 1 không có ước số nào khác ngoài 1 và chính nó. Chúng ta cần tìm các cặp (a, b) mà a hoặc b là số nguyên tố, hoặc cả hai. 1. Nếu a là số nguyên tố, có 4 số nguyên tố trong tập hợp đầu tiên: 2, 3, 5, và 7. Vậy, có 4 cách kết hợp có thể mà a là số nguyên tố. 2. Nếu b là số nguyên tố, có 5 số nguyên tố trong tập hợp thứ hai: 3, 5, 7, 11, và 13. Vậy, có 5 cách kết hợp có thể mà b là số nguyên tố. 3. Nếu cả a và b đều là số nguyên tố, chúng ta cần tìm các cặp mà các số nguyên tố là 2, 3, 5, hoặc 7. Có 4 cặp trong trường hợp này: (2, 3), (2, 5), (2, 7), và (3, 5). Bây giờ, hãy cộng số cách kết hợp mà a hoặc b là số nguyên tố: 4 (a là nguyên tố) + 5 (b là nguyên tố) + 4 (cả hai đều là nguyên tố) = 13. Vậy, có 13 cách kết hợp mà a hoặc b là số nguyên tố. Vì có 20 cách kết hợp có thể tổng cộng, xác suất tích ab là số nguyên tố là !"#$ Do đó, xác suất là !"#$ Câu trả lời là: 𝟏𝟑𝟐𝟎
CoT
Hình 5: Ví dụ 3: PoT tạo ra giải pháp đúng trong khi CoT thất bại.
Nếu 𝑛 sao cho 36 ≤ 𝑛 ≤ 72, thì 𝑥 = 𝑛²+2𝑛𝑛+4 + 16/𝑛+4 𝑛+4 thỏa mãn.
Lựa chọn Đáp án: (A) 20 < 𝑥 < 54 (B) 23 < 𝑥 < 58 (C) 25 < 𝑥 < 64 (D) 28 < 𝑥 < 60 (E) Không có cái nào trong số này Cái nào sau đây KHÔNG thể là các cạnh của một tam giác cân? Lựa chọn Đáp án: (A) 2,2,3 (B) 2,2,4 (C) 4,12,12 (D) 5,5,5 Biểu thức ở tử số có thể được viết lại thành 𝑛+2𝑛+4!, và do đó biểu thức bằng 𝑛+2𝑛+4. Chúng ta thay thế giá trị nhỏ nhất của 𝑛, là 36, và được 42. Chúng ta thay thế giá trị lớn nhất của 𝑛, là 72, và được 88. Vậy câu trả lời là (E) Không có cái nào trong số này Điều này là do trong tam giác cân, độ dài hai cạnh bằng nhau, và độ dài cạnh thứ ba phải nhỏ hơn tổng độ dài của hai cạnh kia. Trong trường hợp này, 4 bằng tổng của hai cạnh kia (2+2), không phải nhỏ hơn. Do đó, câu trả lời là (B) 2,2,4.
Chọn bản dịch tốt nhất thành logic vị từ. Một số CEO giàu có hơn mọi người. (𝐶𝑥: 𝑥 là CEO; 𝑃𝑥: 𝑥 là người; 𝑊𝑥𝑦: 𝑥 giàu có hơn 𝑦)
Lựa chọn Đáp án: (A) ∃𝑥𝐶𝑥•∀𝑦𝑃𝑦•𝑊𝑥𝑦 (B) ∃𝑥𝐶𝑥⊃∀𝑦𝑃𝑦⊃𝑊𝑥𝑦 (C) ∃𝑥𝐶𝑥•∀𝑦𝑃𝑦⊃𝑊𝑥𝑦 (D) ∀𝑥𝑃𝑥⊃∃𝑦𝐶𝑦⊃𝑊𝑥𝑦 ∃𝑥: Tồn tại một 𝑥 𝐶𝑥: 𝑥 là CEO, và ∀𝑦: Với mọi 𝑦 𝑃𝑦: 𝑦 là người, và 𝑊𝑥𝑦: 𝑥 giàu có hơn 𝑦 Vậy, bằng tiếng Anh đơn giản, biểu thức này nói: "Tồn tại một 𝑥 sao cho 𝑥 là CEO và với mọi 𝑦, nếu 𝑦 là người thì 𝑥 giàu có hơn 𝑦." Điều này phù hợp với phát biểu gốc. Do đó, câu trả lời là (A) ∃𝒙𝑪𝒙•∀𝒚𝑷𝒚•𝑾𝒙𝒚 CoT CoT
CoT
Hình 6: Ví dụ 4: Một số loại câu hỏi (ví dụ, logic hình thức) khó giải quyết bằng PoT nhưng có thể được xử lý bằng CoT.
18

--- TRANG 19 ---
Bản thảo. Công trình đang triển khai
Mô hình GSM MATH AQuA NumG SVA Mat Sim SAT MMLU Tổng thể
Cơ sở 14.6 2.5 30.3 29.9 34.5 6.0 5.0 26.8 29.8 19.9
WizzardMath 54.9 10.7 26.3 36.1 36.1 9.3 12.8 25.4 31.1 27.0
MAmmoTH (MathInstruct - CoT) 49.2 9.9 42.2 37.1 48.5 9.5 17.3 34.1 39.8 32.0
MAmmoTH (MathInstruct - PoT) 50.8 28.9 28.6 52.7 65.0 46.7 42.0 25.9 28.3 41.0
MAmmoTH (MathInstruct) 53.6 31.5 44.5 61.2 67.7 46.3 41.2 42.7 42.6 47.9
Bảng 6: Kết quả phân tích của Hình 2. Điều tra về ảnh hưởng của huấn luyện kết hợp CoT & PoT
trên mô hình Llama-2 7B.
Mô hình Giải mã GSM MATH AQuA NumG SVA Mat Sim SAT MMLU TBình
MAmmoTH-7B CoT 50.5 10.4 43.7 44.0 47.3 9.2 18.9 32.7 39.9 33.0
PoT 51.6 28.7 43.3 52.3 65.1 41.9 48.2 39.1 44.6 46.1
Kết hợp 53.6 31.5 44.5 61.2 67.7 46.3 41.2 42.7 42.6 47.9
MAmmoTH-Coder-7B CoT 22.4 7.9 36.2 36.0 37.0 8.2 7.2 32.7 34.6 24.7
PoT 58.8 32.1 47.2 57.1 71.1 53.9 44.6 40.0 47.8 50.3
Kết hợp 59.4 33.4 47.2 66.4 71.4 55.4 45.9 40.5 48.3 52.0
MAmmoTH-13B CoT 56.3 12.9 45.3 45.6 53.8 11.7 22.4 43.6 42.3 37.1
PoT 61.3 32.6 48.8 59.6 72.2 48.5 40.3 46.8 45.4 50.6
Kết hợp 62.0 34.2 51.6 68.7 72.4 49.2 43.2 46.8 47.6 52.9
MAmmoTH-Coder-13B CoT 32.1 10.2 40.6 36.2 43.0 9.6 10.1 40.9 36.6 28.8
PoT 64.3 35.2 46.8 54.2 73.2 60.0 44.2 48.2 48.2 52.7
Kết hợp 64.7 36.3 46.9 66.8 73.7 61.5 47.1 48.6 48.3 54.9
MAmmoTH-Coder-33B CoT 34.3 11.6 39.0 36.2 44.6 10.8 10.9 46.4 42.9 30.7
PoT 72.3 42.8 53.8 59.6 84.0 64.7 50.6 58.6 52.7 59.9
Kết hợp 72.7 43.6 54.7 71.6 84.3 65.4 51.8 60.9 53.8 62.1
MAmmoTH-70B CoT 72.4 21.1 57.9 58.9 71.6 20.0 31.9 57.3 52.1 49.2
PoT 76.7 40.1 60.2 64.3 81.7 55.3 45.3 64.1 53.5 60.1
Kết hợp 76.9 41.8 65.0 74.4 82.4 55.6 51.4 66.4 56.7 63.4
Bảng 7: Ảnh hưởng của các phương pháp giải mã khác nhau trên từng bộ dữ liệu.
C HẠN CHẾ
Dù được huấn luyện trên một tập hợp đa dạng các bộ dữ liệu lý luận toán học, các mô hình MAmmoTH có thể thể hiện hạn chế khi đối mặt với các bài toán ngoài lĩnh vực chuyên môn chính của chúng như
phân tích toán học, phân tích phức tạp, lý thuyết đồ thị, phân tích số, v.v. Do đó, các mô hình của chúng tôi
không phù hợp để giải quyết các bài toán phức tạp hơn trong những lĩnh vực này. Ngoài ra, chúng chưa được huấn luyện
với các bài toán dạng chứng minh, do đó khả năng chứng minh định lý của chúng cũng hạn chế. Trong tương lai, chúng tôi
muốn mở rộng bộ kỹ năng của các mô hình để bao phủ thêm các lĩnh vực và bài toán chứng minh định lý.
Cũng có nguy cơ các mô hình MAmmoTH tạo ra nội dung có thể có hại, gây khó chịu, hoặc thiên vị,
đặc biệt nếu chúng được yêu cầu trả lời các câu hỏi ngoài toán học. Loạt MAmmoTH có thể
bị sử dụng sai mục đích cho các mục đích độc hại, như lan truyền thông tin sai lệch hoặc thăm dò các chủ đề nhạy cảm.
Các nhà phát triển nên tiến hành kiểm tra an toàn và điều chỉnh phù hợp với các ứng dụng cụ thể của họ trước khi
triển khai bất kỳ mô hình MAmmoTH nào. Mặc dù chúng tôi đã cố gắng hết sức để đảm bảo tính sạch sẽ và
tinh khiết của dữ liệu huấn luyện, chúng tôi không thể đảm bảo tính hoàn hảo tuyệt đối. Không chắc nhưng không
không thể một số câu hỏi không phù hợp đã lọt qua quá trình tuyển chọn.
Công trình trong tương lai có thể tiếp tục khám phá cách tiếp tục cải thiện độ bền và khả năng tổng quát hóa
của MAmmoTH trong lý luận toán học. Ví dụ, công trình gần đây xác định "sycophancy" và
"hiệu ứng Clever Hans" trong lý luận: LLM không thể duy trì giải pháp trung thực cho các nhiệm vụ lý luận khi
được thách thức bởi các lập luận và phê bình vô lý không hợp lệ của người dùng (Wang et al., 2023b). Các phương pháp tiềm năng để cải thiện độ bền lý luận của các mô hình có thể bao gồm việc khám phá các phương pháp can thiệp dữ liệu tổng hợp như được khám phá trong (Wei et al., 2023).
19
