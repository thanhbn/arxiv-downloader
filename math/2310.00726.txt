# 2310.00726.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/math/2310.00726.pdf
# File size: 624516 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Improving Length-Generalization in
Transformers via Task Hinting
Pranjal Awasthi
Google Research
pranjalawasthi@google.comAnupam Gupta
Carnegie Mellon University and Google Research
anupamg@cs.cmu.edu
October 3, 2023
Abstract
It has been observed in recent years that transformers have problems with length generalization for
certain types of reasoning and arithmetic tasks. In particular, the performance of a transformer model
trained on tasks (say addition) up to a certain length (e.g., 5 digit numbers) drops sharply when applied
to longer instances of the same problem. This work proposes an approach based on task hinting towards
addressing length generalization. Our key idea is that while training the model on task-specific data, it is
helpful to simultaneously train the model to solve a simpler but related auxiliary task as well.
We study the classical sorting problem as a canonical example to evaluate our approach. We design a
multitask training framework and show that models trained via task hinting significantly improve length
generalization. In particular, for sorting we show that it is possible to train models on data consisting of
sequences having length at most 20, and improve the test accuracy on sequences of length 100from less
than1%(for standard training) to more than 92% (via task hinting).
Our study uncovers several interesting aspects of length generalization. We observe that while sev-
eral auxiliary tasks may seem natural a priori , their effectiveness in improving length generalization
differs dramatically. We further use probing and visualization-based techniques to understand the inter-
nal mechanisms via which the model performs the task, and propose a theoretical construction consistent
with the observed learning behaviors of the model. Based on our construction, we show that introducing
a small number of length dependent parameters into the training procedure can further boost the perfor-
mance on unseen lengths. Finally, we also show the efficacy of our task hinting based approach beyond
sorting, giving hope that these techniques will be applicable in broader contexts.
1 Introduction
Large transformer models trained on massive datasets continue to demonstrate impressive capabilities across
a range of tasks in language understanding, image modeling and other domains [Radford et al., 2019, Brown
et al., 2020, Chowdhery et al., 2022, Chen et al., 2022, Tu et al., 2023]. At the same time there is a growing
body of work on the limitations and vulnerabilities of such models. This work concerns the length general-
ization problem. For many natural tasks—especially ones involving multi-step reasoning such as addition,
multiplication, program execution etc.—there is a natural notion of the length of an input, e.g., the number
of digits when performing the addition task [Anil et al., 2022]. It has been observed that the performance
of transformers on such tasks drops sharply when tested on instances with lengths not seen during training
[Nye et al., 2021, Zhang et al., 2022, Jelassi et al., 2023, Abbe et al., 2023]. As formalized in Abbe et al.
[2023] this phenomenon can also be studied as an extreme form of out-of-distribution (OOD) robustness
where the support of the test-set distribution is disjoint from that of the training distribution.
1arXiv:2310.00726v1  [cs.LG]  1 Oct 2023

--- PAGE 2 ---
Current approaches to tackle length generalization can be broadly divided into two categories. One set of
recent works start with a pre-trained large language model (LLM) and investigate fine-tuning/in-context
learning for extrapolating to larger lengths. Most notably, the works of Wei et al. [2022], Anil et al. [2022]
observe that in-context learning strategies such as chain-of-thought prompting andscratchpad prompting
can help improve the out-of-distribution performance of LLMs. Another set of works consider case studies
on simpler tasks, and perform task-specific training to improve length generalization [Zhang et al., 2022,
Jelassi et al., 2023, Abbe et al., 2023].
Our work falls into the second category: our goal is to develop general-purpose training techniques to im-
prove length generalization. To put the challenges underlying length-generalization in context, let us list
several natural approaches that do not seem to help. For instance, it was observed in Anil et al. [2022] that
simply scaling the model and data-set sizes alone does not suffice for length generalization. The authors
also observed that while scratchpad prompting helps during in-context learning, fine-tuning a pre-trained
LLM on scratchpads does not seem to work, which is surprising. Similarly, the authors in Zhang et al.
[2022] observed that while using a pre-trained BERT [Devlin et al., 2018] model helps improve the per-
formance on the LEGO task that the authors introduced in the work, the improvements are limited and
not enough to address the problem beyond a certain point. Hence training-time techniques to address the
length generalization problem either introduce task-specific architectures [Zhang et al., 2022] or perform
data priming /curriculum learning , where data from higher-length instances is introduced into the training
procedure [Jelassi et al., 2023, Abbe et al., 2023]. Interestingly, the authors in Jelassi et al. [2023] observed
that while introducing a small amount of training data from instances of length nmay help in generalizing
to test instances also of length n, the model could fail completely on test instances of length n+ 1!
Motivated by the above works, we study whether there are general-purpose training techniques for improv-
ing length generalization. As in prior works, we focus on some simple tasks as our use-cases, and consider
training transformer models from scratch. For most of the paper we consider the classical problem of sorting
as a canonical example. Given an unsorted sequence of natural numbers of length n, we consider training
decoder-only transformer models to learn to output the sorted sequence. We work with standard transformer
architectures and explicitly refrain from using even a small amount of data from higher length sequences,
either during training or for model selection. Our main contribution is the framework of task hinting for tack-
ling length generalization. Our approach rests on the core idea that as humans, learning to solve a particular
task also involves learning to solve simpler useful sub-tasks. For instance, a human student who claims to
sort numbers well is also expected to know how to compare two numbers, identify successor/predecessor of
a number in a sequence, count the number of occurrences of a number in a sequence and so on.
Hence we propose a multi-task learning framework where the transformer network is trained simultaneously
to solve a main task (such as sorting) and an auxiliary task (such as identifying the successor element). We
show that this approach leads to a powerful framework for improving length generalization. In particular,
we demonstrate that by training the model only on data of up to length n= 20 sequences, one can improve
the test accuracy on length n= 100 sequence from less than 1%(for standard training) to more than 92%
(via task hinting). In the second part of the paper we perform a deeper investigation of when and how task
hinting helps. We observe that while many auxiliary tasks may seem natural a priori , their effect on length
generalization varies greatly. For the task of sorting sequences, we observe that the task of identifying the
successor element helps the most, while the task of counting helps the least.
We further use visualization techniques to conclude that for each task the transformer network tends to be
biased towards a particular mechanism for solving the task. Perhaps naturally, auxiliary tasks that align well
with this bias tend to help the most. We identify certain computational primitives that the network tends
2

--- PAGE 3 ---
to implicitly capture at various layers and propose a theoretical construction of a sorting transformer that is
consistent with the empirical findings. Based on our theory we identify a small number of length-dependent
parameters whose introduction into the model boosts the length generalization of transformers significantly
(even for models that are trained without task hinting). Finally, we demonstrate the effectiveness of our
proposed framework for another simple task namely that of incrementing a number.
2 Related Work
Length generalization in transformers is a challenging problem, with several confounding factors, such as the
role of positional embeddings, architectural choices, and dataset formatting and/or prompting strategies. The
works of Dubois et al. [2019], Press et al. [2021] propose modifications to the standard attention mechanism
to enable length extrapolation. The work of Newman et al. [2020] observes a surprising role played by
the presence/absence of the EOS token. In particular, they observe the models without the EOS token
extrapolate significantly better to higher lengths.
The work of Anil et al. [2022] explores in-context learning strategies for improving length generalization.
The authors show that length generalization can be significantly improved for tasks such as parity and vari-
able assignment by prompting via scratchpads. They also observe certain counter-intuitive behaviors, such
as the lack of improvements in length generalization when fine-tuning a model via scratchpad prompts.
While it is conceivable that length generalization can be improved via more complex scratchpads/chain-of-
thoughts, augmenting a training dataset with such prompts may not always be feasible, and may lead to a
significant blow-up of the input context length [Malach, 2023]. As another example, the recent work of Liu
and Low [2023] fine-tunes an open source LLaMA model [Touvron et al., 2023] for multi-digit multipli-
cation via scratchpad/chain-of-thought training. It observes that while in-distribution accuracy significantly
improves, the resulting models continue to suffer from length generalization.
The work of Zhang et al. [2022] proposes a LEGO task that has a similar flavor to the task of sorting. The
authors observe that when training a BERT model from scratch for length n= 6, the in-distribution accuracy
is100% , but the accuracy for n= 8. . .12is no better than random. Moreover, they show that training a
specific architecture, namely the ALBERT model [Lan et al., 2019], improves the length generalization to
some extent. In Jelassi et al. [2023] the authors propose the idea of data priming for length generalization.
This involves introducing a small amount (less than 1%) of the data from higher lengths (i.e., the test
distribution) into the training process to improve the out of distribution performance. However, the authors
observe that priming a dataset for an unseen length nmay not have any benefits for performance at length
n+ 1. In a similar vein, the authors in Abbe et al. [2023] propose a curriculum learning procedure, where
data from higher and higher lengths are gradually improved into the training procedure.
Our work also involves understanding the internal learning mechanisms of the trained models via simple
projection based techniques. In a similar vein, the recent work of Nanda et al. [2023a] studies a depth-one
network trained for addition modulo 113, using d= 128 -dimensional representations. Using the structured
nature (and limited size) of the task, they show how zooming in on neurons and analyzing network weights
can help understand the underlying mechanisms. Another set of recent works (e.g., by Li et al. [2022],
Nanda et al. [2023b]) use probing to find mappings from the internal representations of networks to the
actual external state of the problem (Othello) being solved. The focus of our work is on showing that broad-
spectrum techniques—based on simple projections onto the embedding and unembedding bases—can result
in surprisingly valuable insights.
Finally, our work uses the framework of multitask learning that has a rich literature [Crawshaw, 2020].
3

--- PAGE 4 ---
Figure 3.1: An example input sequence for decoder only model training. The mask ensures that we only penalize the
model for predictions at the output positions.
Traditionally, multitask learning is used for obtaining good representations that can adapt to new auxiliary
tasks using small amounts of additional data. In contrast, in this work we use multitask learning primarily
to improve the out-of-distribution robustness of the main task itself.
3 Sorting
For the majority of the paper we focus on sorting as our canonical example. We consider solving this task
via decoder-only transformer models [Brown et al., 2020] trained from scratch. We work with a vocabulary
Σof integers from 1to100and introduce two additional tokens, ⊥as the end-of-input delimiter, and PAD
as a padding token to ensure that all input sequences during training have the same length. Given an input
sequence, we train a decoder-only causal transformer model to predict the sorted sequence one token at a
time. The training is done via the standard next-token prediction framework with the cross-entropy loss. See
Figure 3.1 for an example input sequence, and the mask we use to penalize the model only for the output
positions.
Our training dataset consists of sequences of lengths up to 20, where each sequence is formed by drawing
numbers from Σ ={1,2, . . . , 100}uniformly at random with replacement. Furthermore, to simulate the
realistic setting where data freely available on the internet is biased towards shorter sequence lengths, we
ensure that 80% of the training set consists of sequence lengths from {2,3,4,5}, and the remaining 20%
consists of sequence lengths {6,7, . . . , 20}. We first investigate whether (and by how much) scaling the data
and model can help with length-generalization. To do this, we train a depth-2 model (see Appendix B for
the hyperparameter settings) with dataset sizes of {1M,10M,40M,160M}, and we also train models with
depths {2,4,8,12}on a1Mtraining set. All the models are trained using the Adam optimizer [Kingma
and Ba, 2014] for 100kgradient steps1with a batch size of 1024 , and a one-cycle cosine learning rate
[Loshchilov and Hutter, 2016] starting with the base learning rate of 1e−5. (We use the first 10epochs for
a linear warmup to the base learning rate.) When evaluating the models, we use greedy decoding.
The result of data scaling is shown in Figure 3.2. While more data helps to some extent—the test accuracy on
length 50sequences improves from 64% to close to 90%—further scaling does not help and all the models
achieves less than 1%accuracy on length 100sequences. Here test accuracy refers to the fraction of the
sequences in the test set ( 100kexamples per sequence length) where the model outputs the correct sorted
sequence. Similarly, scaling the depth from 2to4helps improve the accuracy on length 50sequences, but
we do not observe any further benefits (Figure 3.3) thereafter. Again, the accuracy for length 100sequences
is less than 1%for all model and data sizes. This is consistent with the behavior observed in Nye et al.
[2021]: model and data scaling alone does not seem enough to tackle length-generalization.
3.1 Task Hinting
We now introduce the framework of task hinting . We consider a multi-task setup where we train the model
to simultaneously perform well on the main sorting task (Figure 3.1), and also an auxiliary task. This
1The in-distribution test accuracy always reaches 100% well within the first 100ksteps.
4

--- PAGE 5 ---
0 20 40 60 80 100
Sequence Length020406080100T est Accuracy
1M
10M
40M
160MFigure 3.2: Effect of data scaling on length gener-
alization. While performance improves on length 50
sequences, there is no benefit at higher lengths.
0 20 40 60 80 100
Sequence Length020406080100T est Accuracy
depth 2
depth 4
depth 8
depth 12Figure 3.3: Effect of model scaling on length gener-
alization. All the models have less than 1%test ac-
curacy for length 100sequences.
auxiliary task corresponds to a simpler sub-task associated with “truly learning” a solution to the main task.
In this section, let us focus on the successor task : given an input sequence and a particular element afrom
the sequence, the model must learn to predict its successor, i.e., the element that follows ain the sorted
sequence (see Figure 3.4 for an example). .
Figure 3.4: An example input sequence for the successor task.
In order to jointly learn the two tasks, we use the hard-parameter-sharing model for multi-task learning
[Crawshaw, 2020] where the entire model backbone is shared across the two tasks, and a task-specific
classification head is used at the final layer to make predictions for the respective tasks. We train the models
as before for 100ksteps, each time alternating between performing gradient updates on the main task and
auxiliary task. The training dataset size is split equally among the two tasks.
0 20 40 60 80 100
Sequence Length020406080100T est Accuracy
1M
10M
40M
160M
Figure 3.5: Effect of data scaling for task hinting. We observe consistent improvements in test accuracy on higher
length sequences.
Figure 3.5 shows the effect of scaling the training-set size on a depth-2 model with task hinting. In contrast
to the single-task setup, we see consistent gains as the training set size increases. In particular, for dataset
size of 160Mthe test accuracy for length 100reaches to 52.4%. Furthermore, by modifying the training
set slightly so that 10% of the sequences involve non-trivial repetitions (see Appendix B for details), the
depth-2 model trained via task hinting achieves 92.6%test accuracy on length 100sequences! In contrast,
5

--- PAGE 6 ---
the model obtained without task hinting continues to have test accuracy close to 0on length 100sequences,
even on this modified training set.
0 20 40 60 80 100
Sequence length0.00.20.40.60.81.0T est Accuracyno hint
hint
Figure 3.6: Comparing test accuracy for hinting vs.
no hinting for increasing sequence lengths; higher is
better.
0 20 40 60 80 100
Sequence length051015202530Edit Distanceno hint
hintFigure 3.7: Comparing the edit distance for hinting
vs. no hinting for increasing sequence lengths; lower
is better.
In Figures 3.6 and 3.7 we compare the test performance of the depth-2 model trained on a 160M dataset via
task hinting and the model obtained via standard training, as we increase the test sequence length. Both the
models are trained on the modified dataset that contains 10% of sequences with non-trivial repetitions. We
look at two metrics: (a) the full-sequence accuracy, i.e., whether the model outputs the entire sorted sequence
correctly, and (b) the edit distance between the true sorted sequence and the predicted sequence. We see the
performance of the no-hinting model drops sharply with sequence length; in contrast, the performance of
the hinting model remains much more stable.
To further investigate the robustness of the trained models, we test them on distributions beyond uniform
random sampling. We construct test distributions of the form rep (i, r), where a sequence of length iis
created by sampling ⌊i/r⌋elements uniformly at random without replacement, and repeating each rtimes.
(The remaining i− ⌊i/r⌋relements are drawn uniformly at random with replacement.) Figures 3.8 and 3.9
compare the performance of the hinting-based and no-hinting models for repetition values ( r) in{2,3,4,5}.
Again, we observe that the hinting-based models are stable in their performance, both in terms of their
full-sequence accuracy and their edit distance.
Alternative Hints. Many other natural auxiliary tasks can serve as hints for the principal task of sorting.
In Figure 3.10 we present two such tasks. The first is a “ count ” task where, given a sequence of only two
numbers repeated a certain number of times the model has to identify the least occurring one. The underlying
idea is that sorting requires producing an output with the correct number of occurrences of any particular
0.00.51.0T est Acc.Reps 2 Reps 3
50 100
Seq. Length0.00.51.0T est Acc.Reps 4
50 100
Seq. LengthReps 5
no hint
hint
Figure 3.8: Test accuracy comparison of hinting vs.
no hinting models on repetitions.
01020Edit DistanceReps 2 Reps 3
50 100
Seq. Length020Edit DistanceReps 4
50 100
Seq. LengthReps 5
no hint
hintFigure 3.9: Edit distance comparison of hinting vs.
no hinting on repetitions.
6

--- PAGE 7 ---
number, and hence understanding whether the output contains fewer or equal occurrences of a number. A
very similar intuition underlies the second task, which is a “ fill” task: given a sequence containing a single
number repeated some number of times, followed by a prefix of that sequence, the model has to fill in the
remaining entries.
Figure 3.10: An example input sequence for count hints and fill hints.
We now compare the performance of the models trained via the three different types of hints—the successor
hint from the previous secton, and these count and fill hints—in Figure 3.11. Observe that the length
generalization varies greatly depending on the type of hint used. In particular, while the fill hints result
in a marginal improvement over the standard model without hinting, the use of count hints results in a worse
performance than having no hints at all!
4 Interpreting Hints
Given the large differences in the performance of models trained with different kinds of hints, we now turn
to visualization and probing techniques to try and understand the mechanism by which the network learns
the sorting task. To begin with, some notation:
1. For a given trained model, let E∈Rq×dbe the learned input embedding (usually called the embedding
table ); here qis the vocabulary size, and dis the embedding dimensionality. (In our experiments,
q= 103 andd= 1024 .) We call the rows of Etheencoder basis ; the use of the term “basis” is
not unreasonable here, since experimentally we find that the rows are nearly orthogonal and of very
similar lengths.
2. Let (W, b)denote the classifier used at the last layer to make the next-token prediction. Here Wis a
d×qmatrix (usually called the softmax layer ), and bis the bias vector of size d. We also observe that
the columns of Ware nearly-orthogonal, and we call these vectors the decoder basis . These two bases
are nearly orthogonal to each other as well, and hence span 2q= 206 of the d= 1024 dimensions.
0 20 40 60 80 100
Sequence Length020406080100T est Accuracy
successor hint
count hint
fill hint
Figure 3.11: Test accuracy comparison of various hinting tasks. Not all auxiliary tasks lead to improved length
generalization, and some (such as counting) leads to performance degradation.
7

--- PAGE 8 ---
As the network performs inference on an input σσσ=⟨σ0, . . . , σ T−1⟩, we can compute the intermediate em-
beddings for each token σiin the sequence and visualize them in the encoder and decoder bases. Formally, a
standard decoder-only transformer model consists of layers of attention blocks , where each attention block
consists of a layer of self-attention followed by a layer of MLP . Hence, for a given input σσσand position
index i, letXpre
i,jdenote the embedding of token σiobtained at depth jthbefore applying the MLP at that
depth, and let Xpost
i,jthe embedding after applying the MLP. We then visualize several positions ifor various
inputs by projecting the pre-MLP and the post-MLP embeddings onto the encoder and the decoder bases.
These projections are often insightful, since the basis vectors naturally correspond to vocabulary symbols.
0 20 40 60 80 1000510depth 0 position 2 pre-MLP, Encoder
0 20 40 60 80 1000510depth 0 position 2 post-MLP, Encoder
0 20 40 60 80 1002
02depth 0 position 2 pre-MLP, Decoder
0 20 40 60 80 1002
02depth 0 position 2 post-MLP, Decoder
0 20 40 60 80 1002
02depth 1 position 2 pre-MLP, Encoder
0 20 40 60 80 10005depth 1 position 2 post-MLP, Encoder
0 20 40 60 80 1005
05depth 1 position 2 pre-MLP, Decoder
0 20 40 60 80 100010depth 1 position 2 post-MLP, Decoder
Figure 4.12: The projection of token 43at position 2onto the encoder and the decoder bases. We observe
a noisy copy operation being implemented in the encoder basis (see row 1, plot 1 in red).
0 20 40 60 80 10005depth 0 position 8 pre-MLP, Encoder
0 20 40 60 80 1000.02.55.0depth 0 position 8 post-MLP, Encoder
0 20 40 60 80 1002.5
0.02.5depth 0 position 8 pre-MLP, Decoder
0 20 40 60 80 1002.5
0.02.5depth 0 position 8 post-MLP, Decoder
0 20 40 60 80 10005depth 1 position 8 pre-MLP, Encoder
0 20 40 60 80 1000.02.5depth 1 position 8 post-MLP, Encoder
0 20 40 60 80 1000510depth 1 position 8 pre-MLP, Decoder
0 20 40 60 80 100020depth 1 position 8 post-MLP, Decoder
Figure 4.13: The projection of token 43at position 8onto the encoder and the decoder bases. We observe
anIdentity+Successor operation being implemented in the decoder basis after the second attention layer
(see row 2, plot 3 in red).
0 20 40 60 80 1000510depth 0 position 5 pre-MLP, Encoder
0 20 40 60 80 10005depth 0 position 5 post-MLP, Encoder
0 20 40 60 80 1000.02.5depth 0 position 5 pre-MLP, Decoder
0 20 40 60 80 1002.5
0.02.5depth 0 position 5 post-MLP, Decoder
0 20 40 60 80 1002.5
0.02.5depth 1 position 5 pre-MLP, Encoder
0 20 40 60 80 1002
0depth 1 position 5 post-MLP, Encoder
0 20 40 60 80 100010depth 1 position 5 pre-MLP, Decoder
0 20 40 60 80 100020depth 1 position 5 post-MLP, Decoder
Figure 4.14: The projection of token ⊥at position 5onto the encoder and the decoder bases. We observe
a noisy min operation being implemented in the decoder basis (see row 1, plot 3 in red).
As an example consider the input sequence: σσσ=⟨5,17,43,78,92,⊥⟩of five numbers that have to be
sorted. We consider a depth-two trained model (via standard training) and plot the projected embeddings
for token σ2= 43 in Figure 4.12. The embeddings after first attention layer (depth-0 pre-MLP) are highly
concentrated on the token 43in the encoder basis, suggesting a (noisy) copy operation being implemented
by the layer. This tendency of tokens to simply copy themselves in the encoder basis is observed for tokens
appearing before the ⊥token at all points in the inference.
Next, in Figure 4.13 we plot the token 43again, but now when it appears at position 8, i.e., when it is part of
the output sequence. We again observe the noisy copy operation in the encoder basis, but the behavior in the
decoder basis is quite different. Specifically, the embedding after the second attention layer (depth- 1pre-
MLP) is highly concentrated on both token 43and on its successor in the sorted sequence, i.e., on token 78.
8

--- PAGE 9 ---
In fact, we consistently observe this two-peak phenomenon in the depth- 1pre-MLP embedding for tokens in
the output sequence—they appear to implement an Identity+Successor operation. The final MLP layer then
acts as a denoiser , reducing/removing the spike on the identity part to ensure that the final embeddings are
concentrated correctly on the successor element—hence the classification based on (W, b)correctly outputs
the successor element.
Finally, let us examine the embeddings for the end-of-input ⊥token in Figure 4.14. Here we consistently
observe that a noisy minimum operation is being implemented right after the first attention layer (depth-
0): the embedding has largest inner product with the vector in the decoding basis that corresponds to the
minimum element in the input!
To summarize, consider input σ0, . . . , σ n−1, with the ⊥token at location n. We consistently observe that
the embeddings suggest the following learning mechanism:
(i) Any token σiin position i < n has a sharp spike on the encoding basis vector corresponding to symbol
σithroughout the inference.
(ii) The embedding for the end-of-input delimiter ⊥typically implements a noisy minimum operation in
the decoding basis after the depth- 0self-attention later.
(iii) Any token in position i > n (i.e., part of the output) often implements the Identity+Successor opera-
tion after the depth- 1self-attention layer. The depth- 1MLP acts as a denoiser , removing the spike on
the symbol itself, which then correctly highlights only the successor.
The empirical evidence suggests that the network aims to solve the sorting task using a natural algorithm:
(a) first finding the minimum element to follow the ⊥symbol, and thereafter (b) computing the successor
element for each element. Moreover, this suggests why the successor hints are highly beneficial: these
hints align well with the solution concepts that the network is trying to learn. In order to further validate
this hypothesis we compare how effective the internal representations of these depth-2 models (trained
with/without hints) are at implementing the above-mentioned mechanisms. In particular, we measure how
often:
(i) the embedding for the ⊥token after the depth- 0self-attention layer computes the minimum input
element (this is measured by computing the dot-product of the embedding with the decoding basis),
and
(ii) the embedding for tokens in the output sequence (those after ⊥) correctly implement the Identity+Successor
mechanism after the layer- 2attention operation.
Figures 4.15 and 4.16 show that using successor hints significantly improves the accuracy of these two
mechanisms in the internal representations, especially at lengths not seen during training. We conjecture
that in general, auxiliary tasks that align well with the implicit bias of the network tend to help the most to
obtain out-of-distribution robustness.
Our analysis above shows that direct projection-based techniques can help demystify some algorithmic
mechanisms underlying transformer networks, and provide interesting insights. Moreover, the generality of
the techniques gives hope that they can be used for other large-scale problems.
5 Theoretical Analysis
The previous sections relied on the toolkit of visualization and probing using the encoder/decoder bases
to gather empirical evidence about the learned mechanism, and the effectiveness of the successor finding
task. In this section we ask the questions: can we give a theoretical construction that matches the empirical
9

--- PAGE 10 ---
0 100 200 300 400 500
Sequence Length020406080Acc. of min findingno hint
hintFigure 4.15: The accuracy of implementing the min
finding operation after layer- 1attention.
0 100 200 300 400 500
Sequence Length020406080100Acc. of Iden.+Succ.no hint
hintFigure 4.16: The accuracy of implementing the Iden-
tity+Successor operation after layer- 2attention.
findings, and that can be implemented via a shallow transformer model? What does this construction tell
us about length generalization? Recent theoretical works have alluded to the possibility that log-precision
transformers may capture the complexity class of TC0circuits [Merrill et al., 2022]. Since [Chandra et al.,
1984] show that sorting is indeed in TC0, it is conceivable that one can design constant-depth transformer
models for sorting.
While there may be many such constructions of shallow transformer models, we impose some additional
constraints: (a) we ask for a depth-two model, and (b) the size of the network should be independent of
the input length n, even though the parameters could depend logarithmically on n. Finally, we want a
construction that displays the empirical properties we observe in Section 4. We hope that by getting a
theoretical construction that is close to the empirically observed behavior, we may be able to generate more
practically useful insights from the theory.
Formally, we fix an alphabet Σof size q. We have one special symbol ⊥, which is the end-of-sequence
delimiter. Let Σ′denote the extended alphabet Σ∪ {⊥} . We associate Σwith the naturals {1,2, . . . , q },
with the usual total order on them. Since we seek to sort sequences using next-token prediction, the input is
a sequence of length Tconsisting of three conceptual parts:
1. pre-delimiter: a sequence σ0, σ1, . . . , σ n−1where each σi∈Σ. These represent the unsorted input.
2. the end-of-sequence delimiter: σn=⊥.
3. post-delimiter: a sequence of i=T−n−1symbols σn+1, σT+2, . . . , σ T−1from Σ, which ideally
represent the smallest isymbols in the input σσσ[0:n−1](in non-decreasing order).
Given this sequence σσσwe want to predict the next symbol in the sorted order of the input σσσ[0:n−1]. Fi-
nally, we consider transformers with the tempered softmax operation, i.e., given x∈Rd,softmax τ(x)i=
eτxi/P
jeτxj. In our construction, we consider transformer models where τ=βlnnandβis a tun-
able/learnable parameter, and nis the sequence length. This is a departure from the standard practice of
always setting τ= 1,independent of the input length . We prove the following theorem:
Theorem 5.1. For any alphabet of size qand bit precision complexity b, there exists a depth-2 decoder only
transformer model with two attention heads, embedding dimensionality and hidden layer dimensionality
ofO(q), and network weights encoded using bbits of precision that correctly solves the sorting task on
sequence of length up to 2Ω(b). Furthermore, the network displays the following characteristics:
1. For any position i < n , the embedding obtained after the first attention layer is highly concentrated
onσiin the encoding basis, hence implementing a copy operation.
2. For token ⊥, the embedding after the first attention layer has the highest dot product (in the decoding
basis) with the smallest element in the sequence, hence implementing the minoperation.
10

--- PAGE 11 ---
3. For any position i > n , the embedding obtained after the second attention layer is concentrated (in
the decoding basis) on the token at position iand the next largest element in the sorted sequence,
thereby implementing the Identity&Successor operation.
Algorithmic Implications. Note that our theoretical construction relies on the ability to apply length-
dependent tempered-softmax operations. This is important for us to ensure that the performance of the
network does not degrade with increasing sequence lengths. Given this theoretical construction, we ask
whether incorporating length-dependent tempered-softmax operations suggested by the theory could help
with length generalization in practice. In order to implement this, we modify the Flaxformer codebase
[Heek et al., 2023] to introduce the tempered softmax at each attention layer (with each its own learnable
βparameter). We train depth-two transformer models on the same training set of size 160M, both with and
without hints, and compare the performance with and without tempered softmax operations.
Standard Tempered
softmax softmax
Test Acc. 50 89 99.4
Test Acc. 100 0.0 45.2
Table 1: Test accuracy of depth-2 model trained
without hints and with/without tempered softmax.Standard Tempered
softmax softmax
Test Acc. 50 99.2 99.7
Test Acc. 100 52.4 64.8
Table 2: Test accuracy of depth-2 model trained
with successor hints and with/without tempered
softmax.
As we observe in Tables 1 and 2, the introduction of the tempered softmax significantly improves length
generalization of models trained via standard training, as well as those trained via task hinting. Furthermore,
the tempered softmax helps across all data scale ranges. In particular, even for the model trained without
hints on a training set of size 1M, the test accuracy on sequences of length 100 increases from 0%to42%
due to the introduction of the tempered softmax!
6 Task Hinting for Other Problems
In this section we discuss the effectiveness of our proposed approach for another problem: that of incre-
menting a positive integer, i.e., adding 1to it. As we will see, it is quite challenging for transformers to be
able to generalize on unseen lengths even for this simple setting. We again train decoder-only models that
produce one token at a time. Similar to the case of sorting, we use the ⊥token to denote the end of the
input sequence. Each example in the training set is a sequence of the form: [1,2,3,⊥,4,2,1], where the
output is being produced in reverse order , given that is the way in which humans tend to solve this task.
The training set contains 1Minstances of lengths up to 10. Similar to the case of sorting, we skew the
distribution towards shorter sequences by sampling 80% of the instances from lengths up to 4. Finally, we
ensure that 10% of the samples end with a random sequence of 9s, since these instances are important for
the model to learn the notion of a carry .
Solving the increment task via a causal decoder-only network presents a different set of challenges than
sorting—the instance is no longer permutation-invariant, and as the number of output tokens increases,
the model has to attend to a specific position farther to the left in the input sequence. We compare the
length-generalization properties of models obtained via standard training versus those obtained via either
task hinting or via introducing the tempered softmax operation. For task hinting, we consider the natural
hint of making the model output the carry sequence along with the output sequence. Hence an instance from
11

--- PAGE 12 ---
the auxiliary task will be structured as
[1,2,3,⊥,4,↑,0,2,↑,0,1,↑,0],
where the ↑token represents the fact that the model should output the correct carry value at the next step.
We train depth-four transformer models for this task and evaluate their test accuracy on the task of solving
the increment problem correctly.
↓Model, →n 11 12 13 14 15 16 17 18 19 20
Standard 98.293.881.560.141 23.210 4.11.40.3
Hinting 99.496.488.469.247.727.113.762.20.5
Temp. softmax 99.897.591.478.362.146.429 16 8 4
Table 3: Test accuracy comparison of various models on the increment task.
Table 3 compares the performance of the model trained via standard training to (a) the model trained via
task hinting, and (b) the model trained using the tempered softmax. We observe that while task hinting helps
improve length generalization, the improvements are smaller compared to the improvements for sorting.
However, we observe that the model based on tempered softmax helps improve the length generalization to
a much greater extent.
7 Discussion and Limitations
In this work we proposed task hinting as an effective approach for the problem of length generalization.
We observe that using hints that have a strong alignment with the internal biases of the learning mechanism
can result in significant gains in out-of-distribution robustness for the problem of sorting integers. For this
setting, we use probing and visualization-based techniques to investigate the internal learning mechanisms;
these allow us to explain the success of the successor-based hints that we use in our experiments. In general,
even these probing/visualization approaches may not always be feasible for large-scale settings, so designing
the appropriate hinting tasks may be a problem in itself: it would be good to develop a principled approach
for deciding on hinting tasks.
While we observed that other natural hinting tasks, such as the count task and the fill task did not help
(and sometimes even hurt the performance), we feel that these are useful auxiliary capabilities for a sorting
network, and it would be good to understand their lack of success at a deeper level. Moreover, it would
also be interesting to combine multiple hints, and make the network benefit from learn more than two tasks
simultaneously. We tried this approach for the sorting problem, where we trained the model to do well on
all the three types of hinting tasks simultaneously, but observed mixed or even negative results.
Our work also proposes the introduction of length-dependent parameters into the attention mechanism, and
observe that they significantly boost the robustness of the models for both the sorting problem and the
increment problem. It would be interesting to apply this to larger-scale settings of training language models,
and to evaluate whether any gains in robustness can be obtained on more general reasoning tasks. Finally,
when using the framework of multitask learning to make the network learn both tasks simultaneously, we did
not make efforts to optimize the various parameters of the setup, and followed a simple recipe of alternating
gradient updates on each task. Further optimizations in this stage could lead to better performance.
12

--- PAGE 13 ---
8 Acknowledgements
We thank Nishanth Dikkala and Guru Guruganesh for several useful discussions and insightful comments
that helped improve the results and the readability of the paper.
References
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877–1901, 2020.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language
modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Good-
man, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-
image model. arXiv preprint arXiv:2209.06794 , 2022.
Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew
Carroll, Chuck Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical AI. arXiv preprint
arXiv:2307.14334 , 2023.
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose
Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large lan-
guage models. Advances in Neural Information Processing Systems , 35:38546–38556, 2022.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for
intermediate computation with language models. arXiv preprint arXiv:2112.00114 , 2021.
Yi Zhang, Arturs Backurs, S ´ebastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling
transformers with LEGO: a synthetic reasoning task. arXiv preprint arXiv:2206.04301 , 2022.
Samy Jelassi, St ´ephane d’Ascoli, Carles Domingo-Enrich, Yuhuai Wu, Yuanzhi Li, and Franc ¸ois Charton.
Length generalization in arithmetic transformers. arXiv preprint arXiv:2306.15400 , 2023.
Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. Generalization on the unseen, logic reasoning
and degree curriculum. arXiv preprint arXiv:2301.13105 , 2023.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Infor-
mation Processing Systems , 35:24824–24837, 2022.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
13

--- PAGE 14 ---
Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapolation to
longer sequences. arXiv preprint arXiv:1911.03872 , 2019.
Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input
length extrapolation. arXiv preprint arXiv:2108.12409 , 2021.
Benjamin Newman, John Hewitt, Percy Liang, and Christopher D Manning. The EOS decision and length
extrapolation. arXiv preprint arXiv:2010.07174 , 2020.
Eran Malach. Auto-regressive next-token predictors are universal learners. arXiv preprint
arXiv:2309.06979 , 2023.
Tiedong Liu and Bryan Kian Hsiang Low. Goat: Fine-tuned LLaMA outperforms GPT-4 on arithmetic
tasks. arXiv preprint arXiv:2305.14201 , 2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,
Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971 , 2023.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite BERT for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942 , 2019.
Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt. Progress measures for
grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217 , 2023a.
Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Vi ´egas, Hanspeter Pfister, and Martin Wattenberg.
Emergent world representations: Exploring a sequence model trained on a synthetic task. arXiv preprint
arXiv:2210.13382 , 2022.
Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models of
self-supervised sequence models, 2023b.
Michael Crawshaw. Multi-task learning with deep neural networks: A survey. arXiv preprint
arXiv:2009.09796 , 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint
arXiv:1608.03983 , 2016.
William Merrill, Ashish Sabharwal, and Noah A Smith. Saturated transformers are constant-depth threshold
circuits. Transactions of the Association for Computational Linguistics , 10:843–856, 2022.
Ashok K. Chandra, Larry J. Stockmeyer, and Uzi Vishkin. Constant depth reducibility. SIAM J. Comput. ,
13(2):423–439, 1984. doi: 10.1137/0213028. URL https://doi.org/10.1137/0213028 .
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and
Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL http://github.
com/google/flax .
14

--- PAGE 15 ---
A Formal Constructions [Proof of Theorem 5.1]
We now show how to implement a min/successor operation via two-layer transformers, which allows us to
sort using next-token predictions.
A.1 Notation
Fix an alphabet Σof size q. We have one special symbol ⊥, which is the end-of-sequence delimiter. Let Σ′
denote the extended alphabet Σ∪ {⊥} . We associate Σwith the naturals {1,2, . . . , q }, with the usual total
order on them.
Since we seek to sort sequences using next-token prediction, the input is a sequence of length Tconsists of
three conceptual parts:
1. the pre-delimiter part: a sequence of nsymbols σ0, σ1, . . . , σ n−1where each σi∈Σ. These represent
the unsorted input.
2. the end-of-sequence delimiter: σn=⊥.
3. the post-delimiter part: a sequence of some i=T−n−1symbols σn+1, σT+2, . . . , σ T−1again from
Σ, which ideally represent the smallest isymbols in the input σσσ[0:n−1](in non-decreasing order).
Given this sequence σσσwe want to predict the next symbol in the sorted order of the input σσσ[0:n−1].
A.2 The Transformer Architecture
The process works as follows:
1. The initial embedding function Enc: Σ′→Rdmaps each symbol to a vector in Rd. LetX0∈RT×d
be the embedding of the input sequence σσσ, where the ithrow of X0equals Enc(σi). We use Xtito
denote the ithrow of Xt.
2. There are battention blocks which transform this input: we denote the operation of attention block t
byBt:RT×d→RT×d, and hence
Xt:=Bt(Xt−1).
Each block contains a self-attention layer anda multi-layer perceptron (MLP) layer , followed a layer
normalization operation, such that
Bt:=
normalize ◦(I+fmlp
t)◦(I+fattn
t)
.
The identity maps are the residual stream to which the results of the various operations get repeatedly
added in.
3. Each self-attention layer consists of hattention heads : the columns of the matrix Xtare split into
hmatrices Xt1, . . . ,Xth, each with d/h columns and Trows. Each head is specified by matrices
Q, K, V . It takes a matrix X∈RT×d/hand produces a matrix of the same dimensions as follows:
fattn(X;K, Q, V ) := smax τ(XKQ⊺X⊺)XV
15

--- PAGE 16 ---
Here the smax operator takes a matrix Aand a parameter τand defines
smax τ(A)ij=eτAij1(i≥j)P
j′≤jeτAij′.
(Note that the above operator combines the softmax operation and the auto-regressive behavior.)
Finally, the resulting hsub-matrices are concatenated together to give the result of the entire self-
attention layer; let fattn
tdenote the composite function. Define Yt:= (I+fattn
t)Xt−1as the result
of adding back the original signal to the result.
4. Next, the multi-layer perceptron (MLP) layer (which in our case is a two-layer perceptron) is a trans-
formation fmlp(Y;W1, W2, b1, b2)that is specified by two matrices W1, W2and bias vectors b1, b2.
It is the result of applying the following map to each row y⊺ofYseparately:
y7→W2σ(W1y+b1) +b2.
Here the map σ(·)is usually the component-wise ReLU operation (or in more complicated settings,
other non-linear operators like GeLU or GLU).
5. The final piece in each block is the layer normalization operation, which again is applied to each row
of the current embedding independently. Given a vector x∈Rd, it subtracts µ:=∥x∥1/dfrom each
coordinate to make it zero-mean, and then divides each entry by σ:=qP
ix2
i/d; this makes the
Euclidean length√
d. We denote this operation by normalize.
6. Unrolling, the entire transformer map is
Xb= 
Bb◦Bb−1◦ ··· ◦ B1
X0.
7. The final transformation is the decoding/unembedding operation, which takes Xb∈RT×dand applies
some decoding map Dec:Rd→Σ′independently on each row of Xb. This produces characters in
Σ′—these are the predictions for the next symbols. For our decoder-only constructions, the only
relevant prediction is that of the last symbol: we output this prediction Dec(Xb,T−1)as the next
token, thereby increasing the length by 1—this longer string is then the input for the next iteration.
We now show how to implement each of these attention blocks for the sorting network.
A.3 The Encoding Function
Fix a set of unit vectors {es,e′
s}s∈Σ′which are all orthogonal to each other in Rd. The initial embedding is
simple: each symbol a∈Σis encoded by the vector ea+e′
a, and the end-of-input delimiter is encoded as
e⊥+e′
⊥. This gives us the input embedding X0.
A.4 Block #1
The first block has two goals: (i) it gets each token to implement a “min/copy” operation (in which the
end-of-input delimiter predicts the minimum element from the input, whereas each other token just predicts
itself), and (ii) the tokens corresponding to the same symbol before and after the end-of-input delimiter
distinguish themselves, so that the second block can act on them accordingly.
16

--- PAGE 17 ---
A.4.1 Block #1: Self-Attention Layer
There are two attention heads in the first self-attention layer, each getting some d/2columns of the matrix
X0. We denote the resulting two sub-matrices by X01,X02∈RT×d/2, and ensure that for each s, the span
of{es}s∈Σ′lies in the subspace corresponding to the first d/2coordinates, and the span of {e′
s}lies in the
one for the other d/2coordinates.
In the entire construction, we set τ= 3 ln n, where nis the length of the input. Define the Q, K, V matrices
for the attention heads as follows:
• Attention Head #1, which operates on a subspace containing the vectors {e0,e1,e2, . . . ,eq}: for
some positive scalar C≥1to be specified below, define
Qea=ea+Ce⊥ Kea=ea Vea=eea (A.1)
Qe⊥=e⊥ Ke⊥=e⊥ Ve⊥=ee⊥. (A.2)
(Here, and subsequently, the matrices Q, K, V map all vectors orthogonal to the specified vectors to
zero.) The vectors {ees}s∈Σ′are fresh orthonormal vectors.
• Attention Head #2, which operates on a subspace containing the vectors {e′
0,e′
1,e′
2, . . . ,e′
q}: define
Qe′
a=e′
a Ke′
a=e′
a Ve′
a=be′
a (A.3)
Qe′
⊥=X
b∈Σγbe′
b Ke′
⊥=e′
⊥ Ve⊥= 0. (A.4)
Here{γb}b∈Σare also values to be specified soon. Again, the vectors {be′
b}b∈Σare fresh orthonormal
vectors.
This means that for any symbol a∈Σat some position ibefore the ⊥delimiter, the first attention head
outputsP
j≤i:σj=aeτeeσj+P
j≤i:σj̸=aeeσjP
j≤i:σj=aeτ+P
j≤i:σj̸=a1=na,[0,i]eτeea+P
j≤i:σj̸=aeeσj
na,[0,i]eτ+ (i+ 1−na,[0,i])
Here na,[x,y]is the number of occurrences of ain the multiset {σx, . . . , σ y}. Now since τ≥3 lnn, most of
the attention is on all occurrences of the same symbol aseen thus far, and hence this expression is
(1−O(1/n4))·eea+O(1/n2)·u1i,
where u1is some “error” vector of unit norm. Similarly, the second attention head gives
(1−O(1/n4))·bea+O(1/n2)·u2i,
for some other error vector u2. Hence, adding back in the residual, we get that the ithentry (for i < n ,
where σi=afor some a∈Σ) gives us
(I+fattn
1)(X0i)≈X0i+eea+bea=ea+e′
a+eea+bea. (A.5)
Here and henceforth, we will use the “ ≈” to hide error vectors of length O(1/n2).
17

--- PAGE 18 ---
Now a similar analysis shows that for position i > n (such that X0i=a), setting C= 3 then most of a’s
attention (in the first head) is on the ⊥delimiter, and hence
(I+fattn
1)(X0i)≈(ea+e′
a) +ee⊥+be′
a. (A.6)
Finally, the ⊥delimiter pays most of its attention to itself in first head, whereas in the second head it pays
attention to all the tokens (weighted by the eτγbmultipliers). Defining αb:=eτγb, we get
(I+fattn
1)(X0i)≈(e⊥+e′
⊥) +ee⊥+P
bαbnb,[0,n)be′
bP
bαbnb,[0,n)+ 1. (A.7)
Since we have identified the symbols of Σwith{1,2, . . . , q }, we can define γb= (q−b+ 1) , and hence
lnαb:= 3( q−b+ 1) ln n. Since the αbvalues decrease rapidly as bincreases, the fraction on the right
assigns most of its weight to vector be′
bcorresponding to the minimum element in the input σ[0,n). In other
words, we get
(I+fattn
1)(X0i)≈(e⊥+e′
⊥) +ee⊥+ min
b∈σσσ[0,n−1]be′
b. (A.8)
Let us denote the output of the first self-attention layer by Y1; i.e.,
Y1:= (I+fattn
1)(X0).
A.4.2 Block #1: MLP Layer
Recall that the MLP layer is applied to each embedding separately, and there is no interaction between the
embeddings of different tokens. The first MLP layer has two goals:
1. The first goal is to convert the ee⊥vector in embedding of some post-delimiter ato the corresponding
−eea. To this end, the
fmlp
1,1(x) :=X
b∈Σσ(⟨x,eb⟩+⟨x,ee⊥⟩ −1)(−ee′
b−ee⊥). (A.9)
Recall that σ(z) := max(0 , z)is the ReLU function.
2. The second goal is to shift the coordinates of the eeavectors, so that they appear in the second half of
the coordinates instead of the first. For this we use
fmlp
1,2(x) :=X
b∈Σ
(σ(⟨x,eeb⟩ −σ(⟨x,−eeb⟩)·(ee′
b−eeb)
(A.10)
Finally, fmlp
1(x) :=fmlp
1,1(x) +fmlp
1,2(x). This gives us the output of the first attention block:
X1:= (I+fmlp
1)(Y1).
As mentioned above, we do not use the layer normalization in this construction, so this X1is now fed to the
second attention block.
To summarize,
X1i= (I+fmlp
1)(Y1i) =

(ea+e′
a) +ee′
a+be′
a+u3i fori < n
(ea+e′
a)−ee′
a+be′
a+u3i fori > n , and
(e⊥+e′
⊥) +ee⊥+ min b∈σσσ[0,n−1]be′
b+u3ifori=n.
The error vectors u3iabove have magnitude O(1/n2).
18

--- PAGE 19 ---
A.5 Block #2
The second block now ensures that the ⊥token predicts the minimum element, whereas each other token
predicts its successor. The non-trivial part of this construction arises from duplicates in the input, so that
each symbol a∈Σhas to infer whether the number of copies of aalready output equals the number in the
input part of σσσ, and accordingly predict whether to output another aor the successor to a. (Observe that this
is an ordinal concept, and not a cardinal one: the network does not need the actual count of the a’s that have
been output, but to just know whether the number of a’s output is strictly less than the number in the input.)
A.5.1 Block #2: Self-Attention Layer
The self-attention layer of the second block again has two attention heads:
• Attention Head #1, which again operates on a subspace containing the “unprimed” vectors:
Qea=ea Kea=ea Veea=bea (A.11)
Qe⊥=e⊥ Ke⊥=e⊥ Ve⊥= 0. (A.12)
Again, the matrices Q, K, V map all vectors orthogonal to the specified vectors to zero. Recall that
theτparameter is the softmax operator is set to 3 lnn.
• Attention Head #2, which operates on the primed vectors: define
Qe′
a=X
b>aγbe′
b Ke′
a=e′
a Ve′
a=εbe′
a (A.13)
Qe′
⊥=e′
⊥ Ke′
⊥=e′
⊥ Ve′
⊥= 0. (A.14)
(We will fix the value of ε >0below.)
Since we are at the final block, we are no longer concerned with the part of the input in σ[0:n−1], and hence
focus on positions nand beyond. The ⊥delimiter at position nprimarily pays attention to itself in both
attention heads, since τisΩ(log n). This means it remains unchanged, and
(I+fattn
2)(X1n) = (e⊥+e′
⊥) +ee⊥+ min
b∈σσσ[0,n−1]be′
b+u4n, (A.15)
where the new error vector u4nis still of the order O(1/n2).
Next, consider any position i > n , such that σi=afor some a∈Σ. The first attention head gives
P
j≤i:σj=aeCV(X1j) +P
j≤i:σj̸=aV(X1j)
eCna,[0.i]+ (n−na,[0,i])=eC(na,[0,n]−na,[n+1,i])bea+P
j≤i:σj̸=abeσj
eCna,[0.i]+ (n−na,[0,i])(A.16)
Again, since C= Ω(ln n), this is approximately
(na,[0,n]−na,[n+1,i])bea+u4i, (A.17)
where the error vector u4ihas tiny norm O(1/n2). The second attention head for the same symbol σigives
P
b>aαbnb,[0,i]εbe′
b+P
b≤anb,[0,i]εbe′
bP
b>aαbnb,[0,i]+P
b≤anb,[0,i]. (A.18)
19

--- PAGE 20 ---
Recall that αb=eτγb=n3(q−b+1). If the symbol ais not the largest symbol of the input (so that other
symbols b > a follow it in the input), this expression is ε(min b>abe′
b+u′
4i), with the error vector u′
4ihaving
a tiny norm compared to minb>abe′
b. As before, we define
Y2:= (I+fattn
2)(X1)
to be the outcome of this self-attention layer.
LetbPbe the projection of these embeddings on the subspace spanned by the “hatted” vectors {bea,be′
a}a∈Σ.
Then
bPY2i=bP(I+fattn
2)(Y1i)
=

minb∈σσσ[0,n−1]be′
b+u5i fori=n, and
(na,[0,n]−na,[n+1,i])bea+be′
a+εP
b>aαbnb,[0,i]be′
b+P
b≤anb,[0,i]be′
b P
b>aαbnb,[0,i]+P
b≤anb,[0,i]+u5ifori > n .
(A.19)
A.5.2 Block #2: MLP Layer
The final MLP layer of the second and final block has a simple task:
fmlp
2(x) :=X
b∈Σσ(⟨x,−eeb⟩)·(−be′
b). (A.20)
This has the effect of adding in (−be′
a)to any post-delimiter a, and hence“nullifying” the be′
a. The net effect
(again seen after projection onto the hatted subspace) is
bPX2i=bP(I+fmlp
2)(Y2i)
=

minb∈σσσ[0,n−1]be′
b+u5i fori=n, and
(na,[0,n]−na,[n+1,i])bea+εP
b>aαbnb,[0,i]be′
b+P
b≤anb,[0,i]be′
b P
b>aαbnb,[0,i]+P
b≤anb,[0,i]+u5ifori > n .
A.6 The Decoding Layer
Proof of Theorem 5.1. The decoding (or unembedding) layer outputs the element afor which the vector
bea+be′
ahas the largest inner product with the current embedding. In other words, σipredicts
arg max
a∈Σ⟨X2i,bea+be′
a⟩. (A.21)
From the above construction we have the following properties that establish the correctness of the network:
1. For the delimiter at position i=n, this is simply the minimum element from σσσ[0,n−1].
2. For any other location i > n withσi=a, there are two cases:
(a) Suppose there are multiple copies of ain the input σσσ[0,n−1], and not all of them have been output
yet. This means na,[0,n]> n a,[n+1,i], and hence the maximum in (A.21) is achieved by aitself,
as long as ε≤1/2, say. This results in predicting and outputting another copy of a.
20

--- PAGE 21 ---
(b) Else suppose the number of copies of ain the output already equals that in the input. In this
case, the argmax in (A.21) is achieved at the smallest element b∈σσσ[0,n−1]that is larger than a;
this is indeed the correct “successor” element for ato predict. One exception is when i= 2n,
but then we do not need any further predictions.
Here we have crucially used that the maximizing vector has norm at least a constant, which means that the
error vectors of length O(1/n2)do not alter the result.
A.7 The Layer Normalization
The construction above (using d=O(|Σ|)coordinates) did not use the layer normalization operation;
however, we can convert it to incorporate this operation as well. Recall that layer normalization operates on
the embedding of each token independently: (i) given a vector x∈Rd, it subtracts the mean µx:=1
d∥x∥1
from each coordinate, and then (ii) renormalizes it to have squared norm d.
We take the above construction using vectors x∈Rdand extend it by adding dnew coordinates and
appending an analogous construction using the negative of these vectors. The new embedding ¯xhas mean
µ¯x= 0, and hence the step (i) of layer normalization does not change anything.
This means that at the end of the first block, each of the embeddings in our construction have squared length
≈4. This means the renormalization only changes the magnitude of the embeddings, but their relative sizes
remain the same. Consequently, the computations in the second block remain unchanged. The final layer
normalization again shifts and renormalizes the embedding, but this does not change the outcome.
A.8 Agreement with Experimental Results
1. Consider the embedding after the first self-attention layer: decoding this embedding of ⊥(given in
(A.7) gives us the minimum element, whereas decoding the embedding of any σifori > n (as given
in (A.6)) gives us the element σiitself. This “min/copy” behavior can be observed in the experimental
results.
2. After the second self-attention layer, consider the last occurrence of any symbol ain the output (say at
some position i > n , as given in (A.19)): since na,[0,n]=na,[n+1,i]by our assumption, decoding this
embedding puts most of its mass along a(due tobe′
a) and its successor (due to εP
b>aαbnb,[0,i]be′
b+P
b≤anb,[0,i]be′
b P
b>aαbnb,[0,i]+P
b≤anb,[0,i]).
Again, this “Identity+Successor” behavior shows up in the experiments.
3. Finally, the last MLP layer nullifies the mass on the atoken itself, thereby leaving most of the mass
on the successor. This aspect also shows up in the experiments.
B Experimental Settings and Hyperparameters
Sorting Dataset construction. For the sorting problem, we construct the data set for the main task by
sampling a length in {2,3, . . . , 20}from a skewed distribution. For a given length, we sample an input
sequence by independently drawing random numbers in {1,100}uniformly at random with replacement,
for each input position. The skewed length distribution places 80% of the probability mass equally on
lengths in {2,3,4,5}and the remaining 20% uniformly on lengths in {6,7, . . . , 20}.
We also train models on variants of the above dataset that contain non-trivial repetitions. These datasets
are created by first picking a length ℓfrom the same skewed distribution. With probability 0.9, we follow
21

--- PAGE 22 ---
the same procedure as above for creating the input sequence. With the remaining probability 0.1, we pick
ℓ
2numbers uniformly at random from {1,2, . . . , 100}(without replacement) and create the input sequence
by independently drawing numbers from this set (uniformly, with repetitions) for each position in the input
sequence.
The training dataset for the successor hint is created in the same manner as above: having picked the
input sequence, we pick a position uniformly at random and use the corresponding number to create the
successor hint. For creating the count hint dataset we again pick the length of the sequence from the skewed
distribution. Then we pick two numbers a, bat random (without replacement) from {1,2, . . . , 100}. Finally,
with probability 0.5we repeat them the same number of times, and with the remaining probability either a
orbis chosen (equally) to be the under-represented number. The amount of under-representation is chosen
uniformly among the valid choices, but restricted to be at most 5. Similarly, for the fill task , we choose
the length ℓfrom the skewed distribution and choose to repeat the element some number of times uniformly
chosen from {1,2, . . . ,⌊ℓ/2⌋}. To construct the test set for each sequence length, we sample 100kexamples
uniformly at random with replacement.
All our models for the sorting network use the architecture and parameters detailed in Table 4. We do not
use position embeddings in our architectures, as causal decoder-only models are not permutation invariant.
In all our experiments with using fixed and learned positional embeddings, we observed comparable or even
worse performance compared to models using no positional embedding.
Parameter Value
Embedding size d 1024
V ocabulary size q 103
Position embedding type None
# Attention heads h 16
MLP inner dimensionality d′2048
Sequence length 512
Base learning rate 1e-5
Optimizer Adam
LR warmup Linear for 10epochs
LR decay schedule Cosine, one cycle with default parameters
Dropout None
Activation GELU
Table 4: Hyperparameters for the sorting task.
Increment Dataset Construction. For the problem of incrementing a positive integer, we construct the data
set for the main task by sampling a length in {2,3, . . . , 10}from a skewed distribution. The distribution
places 80% mass equally on lengths {2,3,4}and the remaining 20% equally on lengths {5, . . . , 10}. For
each length ℓ, with probability 0.9we sample input position 0(the most significant digit) uniformly at ran-
dom from {1, . . . , 9}and the remaining positions uniformly at random (with replacement) from {0, . . . , 9}.
With the remaining probability of 0.1. we randomly replace the last kpositions with the digit 9, where k
is chosen uniformly from {1,2, . . . , ℓ}. Our test dataset for each length consists of 100knumbers chosen
uniformly at random. For the increment task we use the parameters detailed in Table 5 and always train
depth- 4models.
22

--- PAGE 23 ---
Parameter Value
Embedding size d 1024
V ocabulary size q 14
Position embedding type None
# Attention heads h 16
MLP inner dimensionality d′2048
Sequence length 512
Base learning rate 1e-5
Optimizer Adam
LR warmup Linear for 10epochs
LR decay schedule Cosine, one cycle with default parameters
Dropout None
Activation GELU
Table 5: Hyperparameters for the increment task.
23
