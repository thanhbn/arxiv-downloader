# 2402.03300.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2402.03300.pdf
# Kích thước tệp: 1844642 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
DeepSeekMath: Đẩy giới hạn của suy luận toán học 
trong các mô hình ngôn ngữ mở
Zhihong Shao1,2∗†, Peiyi Wang1,3∗†, Qihao Zhu1,3∗†, Runxin Xu1, Junxiao Song1
Xiao Bi1, Haowei Zhang1, Mingchuan Zhang1, Y.K. Li1, Y. Wu1, Daya Guo1∗
1DeepSeek-AI,2Đại học Thanh Hoa,3Đại học Bắc Kinh
{zhihongshao,wangpeiyi,zhuqh,guoday}@deepseek.com
https://github.com/deepseek-ai/DeepSeek-Math
Tóm tắt
Suy luận toán học đặt ra một thách thức đáng kể cho các mô hình ngôn ngữ do bản chất phức tạp và có cấu trúc của nó. Trong bài báo này, chúng tôi giới thiệu DeepSeekMath 7B, tiếp tục huấn luyện trước DeepSeek-Coder-Base-v1.5 7B với 120B token liên quan đến toán học được lấy từ Common Crawl, cùng với dữ liệu ngôn ngữ tự nhiên và mã. DeepSeekMath 7B đã đạt được điểm ấn tượng 51.7% trên benchmark MATH cấp độ thi đấu mà không cần dựa vào các bộ công cụ bên ngoài và kỹ thuật bỏ phiếu, tiệm cận mức hiệu suất của Gemini-Ultra và GPT-4. Tự nhất quán trên 64 mẫu từ DeepSeekMath 7B đạt 60.9% trên MATH. Khả năng suy luận toán học của DeepSeekMath được quy cho hai yếu tố chính: Thứ nhất, chúng tôi khai thác tiềm năng đáng kể của dữ liệu web công khai thông qua một pipeline lựa chọn dữ liệu được thiết kế tỉ mỉ. Thứ hai, chúng tôi giới thiệu Group Relative Policy Optimization (GRPO), một biến thể của Proximal Policy Optimization (PPO), nhằm tăng cường khả năng suy luận toán học đồng thời tối ưu hóa việc sử dụng bộ nhớ của PPO.

Hình 1|Độ chính xác Top1 của các mô hình mã nguồn mở trên benchmark MATH cấp độ thi đấu (Hendrycks et al., 2021) mà không sử dụng các bộ công cụ bên ngoài và kỹ thuật bỏ phiếu.
∗Những người đóng góp cốt lõi.
†Công việc được thực hiện trong thời gian thực tập tại DeepSeek-AI.arXiv:2402.03300v3 [cs.CL] 27 Apr 2024

--- TRANG 2 ---
1. Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) đã cách mạng hóa cách tiếp cận suy luận toán học trong trí tuệ nhân tạo, thúc đẩy những tiến bộ đáng kể trong cả benchmark suy luận định lượng (Hendrycks et al., 2021) và benchmark suy luận hình học (Trinh et al., 2024). Hơn nữa, những mô hình này đã chứng minh là hữu ích trong việc hỗ trợ con người giải quyết các vấn đề toán học phức tạp (Tao, 2023). Tuy nhiên, các mô hình tiên tiến như GPT-4 (OpenAI, 2023) và Gemini-Ultra (Anil et al., 2023) không được công khai, và các mô hình mã nguồn mở hiện có thể truy cập hiện tại tụt hậu đáng kể về hiệu suất.

Trong nghiên cứu này, chúng tôi giới thiệu DeepSeekMath, một mô hình ngôn ngữ chuyên biệt về lĩnh vực cụ thể vượt trội đáng kể so với khả năng toán học của các mô hình mã nguồn mở và tiệm cận mức hiệu suất của GPT-4 trên các benchmark học thuật. Để đạt được điều này, chúng tôi tạo ra DeepSeekMath Corpus, một corpus huấn luyện trước quy mô lớn chất lượng cao bao gồm 120B token toán học. Bộ dữ liệu này được trích xuất từ Common Crawl (CC) bằng một bộ phân loại dựa trên fastText (Joulin et al., 2016). Trong lần lặp đầu tiên, bộ phân loại được huấn luyện bằng cách sử dụng các thể hiện từ OpenWebMath (Paster et al., 2023) làm ví dụ tích cực, đồng thời kết hợp một lựa chọn đa dạng của các trang web khác để phục vụ như các ví dụ tiêu cực. Sau đó, chúng tôi sử dụng bộ phân loại để khai thác thêm các thể hiện tích cực từ CC, được tinh chỉnh thêm thông qua chú thích của con người. Bộ phân loại sau đó được cập nhật với bộ dữ liệu được tăng cường này để cải thiện hiệu suất của nó. Kết quả đánh giá chỉ ra rằng corpus quy mô lớn có chất lượng cao, khi mô hình cơ sở DeepSeekMath-Base 7B của chúng tôi đạt 64.2% trên GSM8K (Cobbe et al., 2021) và 36.2% trên bộ dữ liệu MATH cấp độ thi đấu (Hendrycks et al., 2021), vượt trội hơn Minerva 540B (Lewkowycz et al., 2022a). Ngoài ra, DeepSeekMath Corpus là đa ngôn ngữ, vì vậy chúng tôi nhận thấy sự cải thiện trong các benchmark toán học tiếng Trung (Wei et al., 2023; Zhong et al., 2023). Chúng tôi tin rằng kinh nghiệm của chúng tôi trong xử lý dữ liệu toán học là một điểm khởi đầu cho cộng đồng nghiên cứu, và có không gian cải thiện đáng kể trong tương lai.

DeepSeekMath-Base được khởi tạo với DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), vì chúng tôi nhận thấy rằng bắt đầu từ một mô hình huấn luyện mã là một lựa chọn tốt hơn so với một LLM tổng quát. Hơn nữa, chúng tôi quan sát thấy huấn luyện toán học cũng cải thiện khả năng của mô hình trên các benchmark MMLU (Hendrycks et al., 2020) và BBH (Suzgun et al., 2022), cho thấy nó không chỉ tăng cường khả năng toán học của mô hình mà còn khuếch đại khả năng suy luận tổng quát.

Sau huấn luyện trước, chúng tôi áp dụng tinh chỉnh chỉ dẫn toán học cho DeepSeekMath-Base với dữ liệu chain-of-thought (Wei et al., 2022), program-of-thought (Chen et al., 2022; Gao et al., 2023), và tool-integrated reasoning (Gou et al., 2023). Mô hình kết quả DeepSeekMath-Instruct 7B đánh bại tất cả các đối thủ 7B và có thể so sánh với các mô hình mã nguồn mở được tinh chỉnh chỉ dẫn 70B.

Hơn nữa, chúng tôi giới thiệu Group Relative Policy Optimization (GRPO), một thuật toán học tăng cường (RL) biến thể của Proximal Policy Optimization (PPO) (Schulman et al., 2017). GRPO không sử dụng mô hình critic, thay vào đó ước tính baseline từ điểm số nhóm, giảm đáng kể tài nguyên huấn luyện. Bằng cách chỉ sử dụng một tập con của dữ liệu tinh chỉnh chỉ dẫn tiếng Anh, GRPO có được một cải thiện đáng kể so với DeepSeekMath-Instruct mạnh mẽ, bao gồm cả các tác vụ toán học trong lĩnh vực (GSM8K: 82.9% →88.2%, MATH: 46.8% →51.7%) và ngoài lĩnh vực (ví dụ: CMATH: 84.6% →88.8%) trong giai đoạn học tăng cường. Chúng tôi cũng cung cấp một mô hình thống nhất để hiểu các phương pháp khác nhau, như Rejection Sampling Fine-Tuning (RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO và GRPO. Dựa trên mô hình thống nhất như vậy, chúng tôi thấy rằng tất cả các phương pháp này được khái niệm hóa như các kỹ thuật RL trực tiếp hoặc đơn giản hóa. Chúng tôi cũng tiến hành các thí nghiệm mở rộng, ví dụ: huấn luyện trực tuyến so với ngoại tuyến, giám sát kết quả so với giám sát quá trình, RL một lượt so với lặp lại, v.v., để điều tra sâu sắc các yếu tố cốt lõi của mô hình này. Cuối cùng, chúng tôi giải thích tại sao RL của chúng tôi tăng cường hiệu suất của các mô hình được tinh chỉnh chỉ dẫn, và tóm tắt thêm các hướng tiềm năng để đạt được RL hiệu quả hơn dựa trên mô hình thống nhất này.

1.1. Đóng góp
Đóng góp của chúng tôi bao gồm huấn luyện trước toán học có thể mở rộng, cùng với việc khám phá và phân tích học tăng cường.

Huấn luyện trước Toán học quy mô lớn
• Nghiên cứu của chúng tôi cung cấp bằng chứng thuyết phục rằng dữ liệu Common Crawl có thể truy cập công khai chứa thông tin có giá trị cho mục đích toán học. Bằng cách thực hiện một pipeline lựa chọn dữ liệu được thiết kế tỉ mỉ, chúng tôi thành công xây dựng DeepSeekMath Corpus, một bộ dữ liệu chất lượng cao 120B token từ các trang web được lọc cho nội dung toán học, gần như lớn gấp 7 lần kích thước của các trang web toán học được sử dụng bởi Minerva (Lewkowycz et al., 2022a) và gấp 9 lần kích thước của OpenWebMath mới được phát hành (Paster et al., 2023).

• Mô hình cơ sở được huấn luyện trước DeepSeekMath-Base 7B của chúng tôi đạt hiệu suất có thể so sánh với Minerva 540B (Lewkowycz et al., 2022a), cho thấy số lượng tham số không phải là yếu tố chủ chốt duy nhất trong khả năng suy luận toán học. Một mô hình nhỏ hơn được huấn luyện trước trên dữ liệu chất lượng cao cũng có thể đạt hiệu suất mạnh mẽ.

• Chúng tôi chia sẻ những phát hiện từ các thí nghiệm huấn luyện toán học. Huấn luyện mã trước huấn luyện toán học cải thiện khả năng của mô hình giải quyết các vấn đề toán học cả với và không có sử dụng công cụ. Điều này cung cấp một câu trả lời một phần cho câu hỏi lâu nay: liệu huấn luyện mã có cải thiện khả năng suy luận không? Chúng tôi tin rằng có, ít nhất là đối với suy luận toán học.

• Mặc dù huấn luyện trên các bài báo arXiv là phổ biến, đặc biệt trong nhiều bài báo liên quan đến toán học, nó không mang lại cải thiện đáng chú ý nào trên tất cả các benchmark toán học được áp dụng trong bài báo này.

Khám phá và Phân tích Học tăng cường
• Chúng tôi giới thiệu Group Relative Policy Optimization (GRPO), một thuật toán học tăng cường hiệu quả và hiệu suất. GRPO không sử dụng mô hình critic, thay vào đó ước tính baseline từ điểm số nhóm, giảm đáng kể tài nguyên huấn luyện so với Proximal Policy Optimization (PPO).

• Chúng tôi chứng minh rằng GRPO tăng cường đáng kể hiệu suất của mô hình được tinh chỉnh chỉ dẫn DeepSeekMath-Instruct, bằng cách chỉ sử dụng dữ liệu tinh chỉnh chỉ dẫn. Hơn nữa, chúng tôi quan sát thấy sự tăng cường trong hiệu suất ngoài lĩnh vực trong quá trình học tăng cường.

• Chúng tôi cung cấp một mô hình thống nhất để hiểu các phương pháp khác nhau, như RFT, DPO, PPO, và GRPO. Chúng tôi cũng tiến hành các thí nghiệm mở rộng, ví dụ: huấn luyện trực tuyến so với ngoại tuyến, giám sát kết quả so với giám sát quá trình, học tăng cường một lượt so với lặp lại, v.v., để điều tra sâu sắc các yếu tố cốt lõi của mô hình này.

• Dựa trên mô hình thống nhất của chúng tôi, chúng tôi khám phá các lý do đằng sau hiệu quả của học tăng cường, và tóm tắt một số hướng tiềm năng để đạt được học tăng cường hiệu quả hơn của các LLM.

1.2. Tóm tắt Đánh giá và Số liệu
• Suy luận Toán học tiếng Anh và tiếng Trung: Chúng tôi tiến hành đánh giá toàn diện các mô hình của chúng tôi trên các benchmark tiếng Anh và tiếng Trung, bao gồm các vấn đề toán học từ cấp tiểu học đến cấp đại học. Các benchmark tiếng Anh bao gồm GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), SAT (Azerbayev et al., 2023), OCW Courses (Lewkowycz et al., 2022a), MMLU-STEM (Hendrycks et al., 2020). Các benchmark tiếng Trung bao gồm MGSM-zh (Shi et al., 2023), CMATH (Wei et al., 2023), Gaokao-MathCloze (Zhong et al., 2023), và Gaokao-MathQA (Zhong et al., 2023). Chúng tôi đánh giá khả năng của mô hình tạo ra các lời giải toán học tự chứa mà không sử dụng công cụ, và cũng khả năng giải quyết vấn đề bằng Python.

Trên các benchmark tiếng Anh, DeepSeekMath-Base cạnh tranh với Minerva 540B mã nguồn đóng (Lewkowycz et al., 2022a), và vượt trội hơn tất cả các mô hình cơ sở mã nguồn mở (ví dụ: Mistral 7B (Jiang et al., 2023) và Llemma-34B (Azerbayev et al., 2023)), bất kể chúng có trải qua huấn luyện trước toán học hay không, thường với một khoảng cách đáng kể. Đáng chú ý, DeepSeekMath-Base vượt trội trên các benchmark tiếng Trung, có thể do chúng tôi không theo các công trình trước đây (Azerbayev et al., 2023; Lewkowycz et al., 2022a) để thu thập dữ liệu huấn luyện trước toán học chỉ tiếng Anh, và cũng bao gồm những dữ liệu chất lượng cao không phải tiếng Anh. Với tinh chỉnh chỉ dẫn toán học và học tăng cường, DeepSeekMath-Instruct và DeepSeekMath-RL kết quả cho thấy hiệu suất mạnh mẽ, đạt độ chính xác hơn 50% trên bộ dữ liệu MATH cấp độ thi đấu lần đầu tiên trong cộng đồng mã nguồn mở.

• Toán học Hình thức: Chúng tôi đánh giá DeepSeekMath-Base bằng cách sử dụng tác vụ chứng minh định lý từ không chính thức đến chính thức từ (Jiang et al., 2022) trên miniF2F (Zheng et al., 2021) với Isabelle (Wenzel et al., 2008) được chọn làm trợ lý chứng minh. DeepSeekMath-Base chứng minh hiệu suất autoformalization few-shot mạnh mẽ.

• Hiểu biết Ngôn ngữ Tự nhiên, Suy luận, và Mã: Để xây dựng một hồ sơ toàn diện về khả năng hiểu biết tổng quát, suy luận, và lập trình của mô hình, chúng tôi đánh giá DeepSeekMath-Base trên benchmark Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020) bao gồm 57 tác vụ trắc nghiệm bao phủ các chủ đề đa dạng, BIG-Bench Hard (BBH) (Suzgun et al., 2022) bao gồm 23 tác vụ thách thức mà hầu hết yêu cầu suy luận nhiều bước để giải quyết, cũng như HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021) được sử dụng rộng rãi để đánh giá các mô hình ngôn ngữ mã. Huấn luyện trước toán học có lợi cho cả hiệu suất hiểu biết ngôn ngữ và suy luận.

2. Huấn luyện trước Toán học

2.1. Thu thập Dữ liệu và Khử nhiễm
Trong phần này, chúng tôi sẽ nêu ra quy trình xây dựng DeepSeekMath Corpus từ Common Crawl. Như được mô tả trong Hình 2, chúng tôi trình bày một pipeline lặp lại chứng minh cách thu thập có hệ thống một corpus toán học quy mô lớn từ Common Crawl, bắt đầu với một corpus hạt giống (ví dụ: một bộ sưu tập nhỏ nhưng chất lượng cao của dữ liệu liên quan đến toán học). Đáng chú ý là cách tiếp cận này cũng có thể áp dụng cho các lĩnh vực khác, chẳng hạn như lập trình.

Đầu tiên, chúng tôi chọn OpenWebMath (Paster et al., 2023), một bộ sưu tập văn bản web toán học chất lượng cao, làm corpus hạt giống ban đầu của chúng tôi. Sử dụng corpus này, chúng tôi huấn luyện một mô hình fastText (Joulin et al., 2016) để thu hồi thêm các trang web toán học giống OpenWebMath. Cụ thể, chúng tôi chọn ngẫu nhiên 500.000 điểm dữ liệu từ corpus hạt giống làm ví dụ huấn luyện tích cực và 500.000 trang web khác từ Common Crawl làm ví dụ tiêu cực. Chúng tôi sử dụng một thư viện mã nguồn mở¹ cho huấn luyện, cấu hình chiều vector thành 256, tốc độ học thành 0.1, độ dài tối đa của n-gram từ thành 3, số lần xuất hiện tối thiểu của từ thành 3, và số epoch huấn luyện thành 3. Để giảm kích thước của Common Crawl gốc, chúng tôi sử dụng các kỹ thuật khử trùng lặp dựa trên URL và khử trùng lặp gần, dẫn đến 40B trang web HTML. Sau đó chúng tôi thu hồi các trang web toán học từ Common Crawl đã khử trùng lặp với mô hình fastText.

Để lọc ra nội dung toán học chất lượng thấp, chúng tôi xếp hạng các trang được thu thập theo điểm số của chúng được dự đoán bởi mô hình fastText, và chỉ giữ lại những trang xếp hạng cao nhất. Khối lượng dữ liệu được giữ lại được đánh giá thông qua các thí nghiệm huấn luyện trước trên top 40B, 80B, 120B, và 160B token. Trong lần lặp đầu tiên, chúng tôi chọn giữ lại top 40B token.

Sau lần lặp đầu tiên của việc thu thập dữ liệu, nhiều trang web toán học vẫn chưa được thu thập, chủ yếu vì mô hình fastText được huấn luyện trên một tập hợp các ví dụ tích cực thiếu đa dạng đủ. Do đó chúng tôi xác định thêm các nguồn web toán học để làm phong phú corpus hạt giống, để chúng tôi có thể tối ưu hóa mô hình fastText. Cụ thể, chúng tôi đầu tiên tổ chức toàn bộ Common Crawl thành các domain rời rạc; một domain được định nghĩa là các trang web chia sẻ cùng một base URL. Đối với mỗi domain, chúng tôi tính toán tỷ lệ phần trăm các trang web được thu thập trong lần lặp đầu tiên. Các domain có hơn 10% trang web được thu thập được phân loại là liên quan đến toán học (ví dụ: mathoverflow.net). Sau đó, chúng tôi chú thích thủ công các URL liên quan đến nội dung toán học trong các domain đã xác định này (ví dụ: mathoverflow.net/questions). Các trang web liên kết đến các URL này, nhưng chưa được thu thập, sẽ được thêm vào corpus hạt giống. Cách tiếp cận này cho phép chúng tôi thu thập thêm các ví dụ tích cực, từ đó huấn luyện một mô hình fastText được cải thiện có khả năng thu hồi thêm dữ liệu toán học trong lần lặp tiếp theo. Sau bốn lần lặp thu thập dữ liệu, chúng tôi kết thúc với 35.5M trang web toán học, tổng cộng 120B token. Trong lần lặp thứ tư, chúng tôi nhận thấy rằng gần 98% dữ liệu đã được thu thập trong lần lặp thứ ba, vì vậy chúng tôi quyết định ngừng thu thập dữ liệu.

Để tránh nhiễm benchmark, chúng tôi theo Guo et al. (2024) để lọc ra các trang web chứa câu hỏi hoặc câu trả lời từ các benchmark toán học tiếng Anh như GSM8K (Cobbe et al., 2021) và MATH (Hendrycks et al., 2021) và các benchmark tiếng Trung như CMATH (Wei et al., 2023) và AGIEval (Zhong et al., 2023). Tiêu chí lọc như sau: bất kỳ đoạn văn bản nào chứa chuỗi 10-gram khớp chính xác với bất kỳ chuỗi con nào từ các benchmark đánh giá sẽ bị loại bỏ khỏi corpus huấn luyện toán học của chúng tôi. Đối với các văn bản benchmark ngắn hơn 10 gram nhưng có ít nhất 3 gram, chúng tôi sử dụng khớp chính xác để lọc ra các trang web bị nhiễm.

--- TRANG 6 ---
2.2. Xác thực Chất lượng của DeepSeekMath Corpus
Chúng tôi tiến hành các thí nghiệm huấn luyện trước để điều tra cách DeepSeekMath Corpus được so sánh với các corpus huấn luyện toán học được phát hành gần đây:

• MathPile (Wang et al., 2023c): một corpus đa nguồn (8.9B token) được tổng hợp từ sách giáo khoa, Wikipedia, ProofWiki, CommonCrawl, StackExchange, và arXiv, với phần lớn (hơn 85%) có nguồn từ arXiv;

• OpenWebMath (Paster et al., 2023): dữ liệu CommonCrawl được lọc cho nội dung toán học, tổng cộng 13.6B token;

• Proof-Pile-2 (Azerbayev et al., 2023): một corpus toán học bao gồm OpenWebMath, AlgebraicStack (10.3B token mã toán học), và các bài báo arXiv (28.0B token). Khi thí nghiệm trên Proof-Pile-2, chúng tôi theo Azerbayev et al. (2023) để sử dụng tỷ lệ arXiv:Web:Code là 2:4:1.

2.2.1. Thiết lập Huấn luyện
Chúng tôi áp dụng huấn luyện toán học cho một mô hình ngôn ngữ được huấn luyện trước tổng quát với 1.3B tham số, chia sẻ cùng một framework với các DeepSeek LLM (DeepSeek-AI, 2024), được ký hiệu là DeepSeek-LLM 1.3B. Chúng tôi huấn luyện riêng biệt một mô hình trên mỗi corpus toán học trong 150B token. Tất cả các thí nghiệm được tiến hành bằng framework huấn luyện HAI-LLM (High-flyer, 2023) hiệu quả và nhẹ. Theo thực tiễn huấn luyện của DeepSeek LLM, chúng tôi sử dụng optimizer AdamW (Loshchilov và Hutter, 2017) với β1=0.9, β2=0.95, và weight_decay=0.1, cùng với một lịch trình tốc độ học đa bước trong đó tốc độ học đạt đỉnh sau 2.000 bước khởi động, giảm xuống 31.6% sau 80% quá trình huấn luyện, và giảm thêm xuống 10.0% của đỉnh sau 90% quá trình huấn luyện. Chúng tôi đặt giá trị tối đa của tốc độ học là 5.3e-4, và sử dụng kích thước batch là 4M token với độ dài ngữ cảnh 4K.

[Bảng 1 - Hiệu suất của DeepSeek-LLM 1.3B được huấn luyện trên các corpus toán học khác nhau]

2.2.2. Kết quả Đánh giá
DeepSeekMath Corpus có chất lượng cao, bao phủ nội dung toán học đa ngôn ngữ, và có kích thước lớn nhất.

• Chất lượng cao: Chúng tôi đánh giá hiệu suất downstream trên 8 benchmark toán học bằng cách sử dụng few-shot chain-of-thought prompting Wei et al. (2022). Như được hiển thị trong Bảng 1, có một sự dẫn đầu rõ ràng về hiệu suất của mô hình được huấn luyện trên DeepSeekMath Corpus. Hình 3 cho thấy mô hình được huấn luyện trên DeepSeekMath Corpus chứng minh hiệu suất tốt hơn so với Proof-Pile-2 tại 50B token (1 epoch đầy đủ của Proof-Pile-2), cho thấy chất lượng trung bình của DeepSeekMath Corpus cao hơn.

• Đa ngôn ngữ: DeepSeekMath Corpus bao gồm dữ liệu bằng nhiều ngôn ngữ, chủ yếu có tiếng Anh và tiếng Trung là hai ngôn ngữ được đại diện nhiều nhất. Như được hiển thị trong Bảng 1, huấn luyện trên DeepSeekMath Corpus tăng cường hiệu suất suy luận toán học cả tiếng Anh và tiếng Trung. Ngược lại, các corpus toán học hiện tại, chủ yếu tập trung vào tiếng Anh, cho thấy cải thiện hạn chế và thậm chí có thể cản trở hiệu suất trong suy luận toán học tiếng Trung.

• Quy mô lớn: DeepSeekMath Corpus lớn hơn nhiều lần so với các corpus toán học hiện tại. Như được mô tả trong Hình 3, DeepSeek-LLM 1.3B, khi được huấn luyện trên DeepSeekMath Corpus, cho thấy một đường cong học tập dốc hơn cùng với những cải thiện lâu dài hơn. Ngược lại, các corpus baseline nhỏ hơn nhiều, và đã được lặp lại nhiều vòng trong quá trình huấn luyện, với hiệu suất mô hình kết quả nhanh chóng đạt đến một cao nguyên.

2.3. Huấn luyện và Đánh giá DeepSeekMath-Base 7B
Trong phần này, chúng tôi giới thiệu DeepSeekMath-Base 7B, một mô hình cơ sở với khả năng suy luận mạnh mẽ, đặc biệt trong toán học. Mô hình của chúng tôi được khởi tạo với DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024) và được huấn luyện trong 500B token. Phân phối dữ liệu như sau: 56% từ DeepSeekMath Corpus, 4% từ AlgebraicStack, 10% từ arXiv, 20% là mã Github, và 10% còn lại là dữ liệu ngôn ngữ tự nhiên từ Common Crawl cả tiếng Anh và tiếng Trung. Chúng tôi chủ yếu áp dụng thiết lập huấn luyện được chỉ định trong Phần 2.2.1, ngoại trừ việc chúng tôi đặt giá trị tối đa của tốc độ học là 4.2e-4 và sử dụng kích thước batch là 10M token.

Chúng tôi tiến hành một đánh giá toàn diện về khả năng toán học của DeepSeekMath-Base 7B, tập trung vào khả năng tạo ra các lời giải toán học tự chứa mà không dựa vào các công cụ bên ngoài, giải quyết vấn đề toán học bằng công cụ, và tiến hành chứng minh định lý hình thức. Ngoài toán học, chúng tôi cũng cung cấp một hồ sơ tổng quát hơn của mô hình cơ sở, bao gồm hiệu suất của nó về hiểu biết ngôn ngữ tự nhiên, suy luận, và kỹ năng lập trình.

Giải quyết Vấn đề Toán học với Suy luận Từng bước Chúng tôi đánh giá hiệu suất của DeepSeekMath-Base trong việc giải quyết vấn đề toán học bằng cách sử dụng few-shot chain-of-thought prompting (Wei et al., 2022), trên tám benchmark bằng tiếng Anh và tiếng Trung. Các benchmark này bao gồm suy luận định lượng (ví dụ: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), và CMATH (Wei et al., 2023)) và các vấn đề trắc nghiệm (ví dụ: MMLU-STEM (Hendrycks et al., 2020) và Gaokao-MathQA (Zhong et al., 2023)), bao phủ các lĩnh vực toán học đa dạng từ độ phức tạp cơ bản đến cấp đại học.

Như được hiển thị trong Bảng 2, DeepSeekMath-Base 7B dẫn đầu về hiệu suất trên tất cả tám benchmark trong số các mô hình cơ sở mã nguồn mở (bao gồm mô hình tổng quát được sử dụng rộng rãi Mistral 7B (Jiang et al., 2023) và Llemma 34B mới được phát hành (Azerbayev et al., 2023) đã trải qua huấn luyện toán học trên Proof-Pile-2 (Azerbayev et al., 2023)). Đáng chú ý, trên bộ dữ liệu MATH cấp độ thi đấu, DeepSeekMath-Base vượt trội hơn các mô hình cơ sở mã nguồn mở hiện tại hơn 10% tuyệt đối, và vượt trội hơn Minerva 540B (Lewkowycz et al., 2022a), một mô hình cơ sở mã nguồn đóng lớn gấp 77 lần được xây dựng trên PaLM (Lewkowycz et al., 2022b) và được huấn luyện thêm trên các văn bản toán học.

[Bảng 2 - So sánh giữa DeepSeekMath-Base 7B và các mô hình cơ sở mạnh mẽ]

Giải quyết Vấn đề Toán học với Sử dụng Công cụ Chúng tôi đánh giá suy luận toán học hỗ trợ bằng chương trình trên GSM8K và MATH bằng cách sử dụng few-shot program-of-thought prompting (Chen et al., 2022; Gao et al., 2023). Các mô hình được nhắc để giải quyết mỗi vấn đề bằng cách viết một chương trình Python trong đó các thư viện như math và sympy có thể được sử dụng cho các tính toán phức tạp. Kết quả thực thi của chương trình được đánh giá là câu trả lời. Như được hiển thị trong Bảng 3, DeepSeekMath-Base 7B vượt trội hơn Llemma 34B tiên tiến nhất trước đây.

[Bảng 3 - Đánh giá few-shot khả năng của các mô hình cơ sở]

Toán học Hình thức Tự động hóa chứng minh hình thức có lợi để đảm bảo độ chính xác và độ tin cậy của các chứng minh toán học và tăng cường hiệu quả, với sự chú ý ngày càng tăng trong những năm gần đây. Chúng tôi đánh giá DeepSeekMath-Base 7B trên tác vụ chứng minh từ không chính thức đến chính thức từ (Jiang et al., 2022) là tạo ra một chứng minh hình thức dựa trên một tuyên bố không chính thức, một đối tác hình thức của tuyên bố, và một chứng minh không chính thức. Chúng tôi đánh giá trên miniF2F (Zheng et al., 2021), một benchmark cho toán học hình thức cấp Olympic, và tạo ra một chứng minh hình thức trong Isabelle cho mỗi vấn đề với few-shot prompting. Theo Jiang et al. (2022), chúng tôi tận dụng các mô hình để tạo ra các phác thảo chứng minh, và thực thi bộ chứng minh tự động sẵn có Sledgehammer (Paulson, 2010) để điền vào các chi tiết bị thiếu. Như được hiển thị trong Bảng 3, DeepSeekMath-Base 7B chứng minh hiệu suất mạnh mẽ trong autoformalization chứng minh.

[Bảng 4 - Đánh giá về hiểu biết ngôn ngữ tự nhiên, suy luận, và benchmark mã]

Hiểu biết Ngôn ngữ Tự nhiên, Suy luận, và Mã Chúng tôi đánh giá hiệu suất mô hình về hiểu biết ngôn ngữ tự nhiên trên MMLU (Hendrycks et al., 2020), suy luận trên BBH (Suzgun et al., 2022), và khả năng lập trình trên HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021). Như được hiển thị trong Bảng 4, DeepSeekMath-Base 7B thể hiện những cải thiện đáng kể về hiệu suất trên MMLU và BBH so với người tiền nhiệm, DeepSeek-Coder-Base-v1.5 (Guo et al., 2024), minh họa tác động tích cực của huấn luyện toán học đối với hiểu biết ngôn ngữ và suy luận. Ngoài ra, bằng cách bao gồm các token mã cho huấn luyện liên tục, DeepSeekMath-Base 7B hiệu quả duy trì hiệu suất của DeepSeek-Coder-Base-v1.5 trên hai benchmark lập trình. Tổng thể, DeepSeekMath-Base 7B vượt trội đáng kể so với mô hình tổng quát Mistral 7B (Jiang et al., 2023) trên ba benchmark suy luận và lập trình.

3. Tinh chỉnh Có giám sát

3.1. Tuyển chọn Dữ liệu SFT
Chúng tôi xây dựng một bộ dữ liệu tinh chỉnh chỉ dẫn toán học bao gồm các vấn đề tiếng Anh và tiếng Trung từ các lĩnh vực toán học khác nhau và các mức độ phức tạp khác nhau: các vấn đề được ghép đôi với các lời giải theo định dạng chain-of-thought (CoT) (Wei et al., 2022), program-of-thought (PoT) (Chen et al., 2022; Gao et al., 2023), và tool-integrated reasoning (Gou et al., 2023). Tổng số ví dụ huấn luyện là 776K.

• Bộ dữ liệu toán học tiếng Anh: Chúng tôi chú thích các vấn đề GSM8K và MATH với các lời giải tích hợp công cụ, và áp dụng một tập con của MathInstruct (Yue et al., 2023) cùng với tập huấn luyện của Lila-OOD (Mishra et al., 2022) trong đó các vấn đề được giải quyết bằng CoT hoặc PoT. Bộ sưu tập tiếng Anh của chúng tôi bao gồm các lĩnh vực toán học đa dạng, ví dụ: đại số, xác suất, lý thuyết số, giải tích, và hình học.

• Bộ dữ liệu toán học tiếng Trung: Chúng tôi thu thập các vấn đề toán học K-12 tiếng Trung bao gồm 76 chủ đề phụ như phương trình tuyến tính, với các lời giải được chú thích theo cả định dạng CoT và tool-integrated reasoning.

3.2. Huấn luyện và Đánh giá DeepSeekMath-Instruct 7B
Trong phần này, chúng tôi giới thiệu DeepSeekMath-Instruct 7B trải qua tinh chỉnh chỉ dẫn toán học dựa trên DeepSeekMath-Base. Các ví dụ huấn luyện được nối ngẫu nhiên cho đến khi đạt độ dài ngữ cảnh tối đa là 4K token. Chúng tôi huấn luyện mô hình trong 500 bước với kích thước batch là 256 và tốc độ học không đổi là 5e-5.

Chúng tôi đánh giá hiệu suất toán học của mô hình cả không có và có sử dụng công cụ, trên 4 benchmark suy luận định lượng bằng tiếng Anh và tiếng Trung. Chúng tôi đánh giá mô hình của chúng tôi so với các mô hình hàng đầu của thời điểm đó:

• Các mô hình mã nguồn đóng bao gồm: (1) gia đình GPT trong đó GPT-4 (OpenAI, 2023) và GPT-4 Code Interpreter² là những mô hình có khả năng nhất, (2) Gemini Ultra và Pro (Anil et al., 2023), (3) Inflection-2 (Inflection AI, 2023), (4) Grok-1³, cũng như các mô hình mới được phát hành bởi các công ty Trung Quốc bao gồm (5) Baichuan-3⁴, (6) GLM-4 mới nhất⁵ từ gia đình GLM (Du et al., 2022). Các mô hình này có mục đích chung, hầu hết đã trải qua một loạt các quy trình alignment.

• Các mô hình mã nguồn mở bao gồm: các mô hình tổng quát như (1) DeepSeek-LLM-Chat 67B (DeepSeek-AI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) SeaLLM-v2 7B (Nguyen et al., 2023), và (4) ChatGLM3 6B (ChatGLM3 Team, 2023), cũng như các mô hình được cải tiến trong toán học bao gồm (5) InternLM2-Math 20B⁶ được xây dựng trên InternLM2 và trải qua huấn luyện toán học theo sau bởi tinh chỉnh chỉ dẫn, (6) Math-Shepherd-Mistral 7B áp dụng huấn luyện PPO (Schulman et al., 2017) cho Mistral 7B (Jiang et al., 2023) với một mô hình reward được giám sát quá trình, (7) chuỗi WizardMath (Luo et al., 2023) cải thiện suy luận toán học trong Mistral 7B và Llama-2 70B (Touvron et al., 2023) bằng cách sử dụng evolve-instruct (tức là, một phiên bản của tinh chỉnh chỉ dẫn sử dụng các chỉ dẫn được phát triển bởi AI) và huấn luyện PPO với các vấn đề huấn luyện chủ yếu có nguồn từ GSM8K và MATH, (8) MetaMath 70B (Yu et al., 2023) là Llama-2 70B được tinh chỉnh trên một phiên bản được tăng cường của GSM8K và MATH, (9) ToRA 34B Gou et al. (2023) là CodeLlama 34B được tinh chỉnh để thực hiện suy luận toán học tích hợp công cụ, (10) MAmmoTH 70B (Yue et al., 2023) là Llama-2 70B được tinh chỉnh chỉ dẫn trên MathInstruct.

Như được hiển thị trong Bảng 5, trong thiết lập đánh giá nơi việc sử dụng công cụ không được phép, DeepSeekMath-Instruct 7B chứng minh hiệu suất mạnh mẽ của suy luận từng bước. Đáng chú ý, trên bộ dữ liệu MATH cấp độ thi đấu, mô hình của chúng tôi vượt trội hơn tất cả các mô hình mã nguồn mở và phần lớn các mô hình độc quyền (ví dụ: Inflection-2 và Gemini Pro) ít nhất 9% tuyệt đối. Điều này đúng ngay cả đối với các mô hình lớn hơn đáng kể (ví dụ: Qwen 72B) hoặc đã được tăng cường cụ thể thông qua học tăng cường tập trung vào toán học (ví dụ: WizardMath-v1.1 7B). Trong khi DeepSeekMath-Instruct cạnh tranh với các mô hình độc quyền Trung Quốc GLM-4 và Baichuan-3 trên MATH, nó vẫn kém hiệu suất so với GPT-4 và Gemini Ultra.

Trong thiết lập đánh giá nơi các mô hình được phép tích hợp suy luận ngôn ngữ tự nhiên và sử dụng công cụ dựa trên chương trình để giải quyết vấn đề, DeepSeekMath-Instruct 7B tiếp cận độ chính xác 60% trên MATH, vượt trội hơn tất cả các mô hình mã nguồn mở hiện tại. Trên các benchmark khác, mô hình của chúng tôi cạnh tranh với DeepSeek-LLM-Chat 67B, tiên tiến nhất trước đây lớn gấp 10 lần.

4. Học tăng cường

4.1. Group Relative Policy Optimization
Học tăng cường (RL) đã được chứng minh là hiệu quả trong việc cải thiện thêm khả năng suy luận toán học của các LLM sau giai đoạn Supervised Fine-Tuning (SFT) (Luo et al., 2023; Wang et al., 2023b). Trong phần này, chúng tôi giới thiệu thuật toán RL hiệu quả và hiệu suất của chúng tôi, Group Relative Policy Optimization (GRPO).

4.1.1. Từ PPO đến GRPO
Proximal Policy Optimization (PPO) (Schulman et al., 2017) là một thuật toán RL actor-critic được sử dụng rộng rãi trong giai đoạn tinh chỉnh RL của các LLM (Ouyang et al., 2022). Cụ thể, nó tối ưu hóa các LLM bằng cách tối đa hóa mục tiêu surrogate sau:

J_PPO(θ) = E[q∼P(Q),o∼π_θ_old(O|q)] [1/|o| ∑_{t=1}^{|o|} min(π_θ(o_t|q,o_{<t})/π_θ_old(o_t|q,o_{<t}) A_t, clip(π_θ(o_t|q,o_{<t})/π_θ_old(o_t|q,o_{<t}), 1-ε, 1+ε) A_t)]  (1)

trong đó π_θ và π_θ_old là các mô hình policy hiện tại và cũ, và q, o là các câu hỏi và đầu ra được lấy mẫu từ bộ dữ liệu câu hỏi và policy cũ π_θ_old, tương ứng. ε là một siêu tham số liên quan đến clipping được giới thiệu trong PPO để ổn định huấn luyện. A_t là advantage, được tính bằng cách áp dụng Generalized Advantage Estimation (GAE) (Schulman et al., 2015), dựa trên các reward {r_{≥t}} và một hàm value đã học V_ψ.

Vì hàm value được sử dụng trong PPO thường là một mô hình khác có kích thước tương đương với mô hình policy, nó mang lại một gánh nặng bộ nhớ và tính toán đáng kể. Ngoài ra, trong quá trình huấn luyện RL, hàm value được coi là một baseline trong việc tính toán advantage để giảm phương sai. Trong khi trong ngữ cảnh LLM, thường chỉ token cuối cùng được gán điểm reward bởi mô hình reward, điều này có thể làm phức tạp việc huấn luyện một hàm value chính xác tại mỗi token. Để giải quyết điều này, như được hiển thị trong Hình 4, chúng tôi đề xuất Group Relative Policy Optimization (GRPO), loại bỏ nhu cầu xấp xỉ hàm value bổ sung như trong PPO, và thay vào đó sử dụng reward trung bình của nhiều đầu ra được lấy mẫu, được tạo ra để phản hồi cùng một câu hỏi, làm baseline. Cụ thể hơn, đối với mỗi câu hỏi q, GRPO lấy mẫu một nhóm đầu ra {o_1, o_2, ..., o_G} từ policy cũ π_θ_old và sau đó tối ưu hóa mô hình policy bằng cách tối đa hóa mục tiêu sau:

J_GRPO(θ) = E[q∼P(Q),{o_i}_{i=1}^G∼π_θ_old(O|q)] [1/G ∑_{i=1}^G 1/|o_i| ∑_{t=1}^{|o_i|} min(π_θ(o_{i,t}|q,o_{i,<t})/π_θ_old(o_{i,t}|q,o_{i,<t}) Â_{i,t}, clip(π_θ(o_{i,t}|q,o_{i,<t})/π_θ_old(o_{i,t}|q,o_{i,<t}), 1-ε, 1+ε) Â_{i,t}) - β D_KL(π_θ||π_ref)]  (3)

trong đó ε và β là các siêu tham số, và Â_{i,t} là advantage được tính dựa trên các reward tương đối của các đầu ra bên trong mỗi nhóm chỉ, sẽ được chi tiết trong các phần tiếp theo. Cách tương đối nhóm mà GRPO tận dụng để tính toán các advantage, phù hợp với bản chất so sánh của các mô hình reward, vì các mô hình reward thường được huấn luyện trên các bộ dữ liệu so sánh giữa các đầu ra trên cùng một câu hỏi. Cũng lưu ý rằng, thay vì thêm penalty KL trong reward, GRPO điều chỉnh bằng cách trực tiếp thêm divergence KL giữa policy được huấn luyện và policy tham chiếu vào loss, tránh làm phức tạp việc tính toán Â_{i,t}.

Và khác với hạng phạt KL được sử dụng trong (2), chúng tôi ước tính divergence KL với estimator không thiên vị sau đây (Schulman, 2020):

D_KL(π_θ||π_ref) = π_ref(o_{i,t}|q,o_{i,<t})/π_θ(o_{i,t}|q,o_{i,<t}) - log(π_ref(o_{i,t}|q,o_{i,<t})/π_θ(o_{i,t}|q,o_{i,<t})) - 1  (4)

được đảm bảo là dương.

4.1.2. RL Giám sát Kết quả với GRPO
Hình thức, đối với mỗi câu hỏi q, một nhóm đầu ra {o_1, o_2, ..., o_G} được lấy mẫu từ mô hình policy cũ π_θ_old. Một mô hình reward sau đó được sử dụng để ghi điểm các đầu ra, tạo ra G reward r={r_1, r_2, ..., r_G} tương ứng. Tiếp theo, các reward này được chuẩn hóa bằng cách trừ đi trung bình nhóm và chia cho độ lệch chuẩn nhóm. Giám sát kết quả cung cấp reward được chuẩn hóa ở cuối mỗi đầu ra o_i và đặt các advantage Â_{i,t} của tất cả các token trong đầu ra bằng reward được chuẩn hóa, tức là Â_{i,t} = r̃_i = (r_i - mean(r))/std(r), và sau đó tối ưu hóa policy bằng cách tối đa hóa mục tiêu được định nghĩa trong phương trình (3).

4.1.3. RL Giám sát Quá trình với GRPO
Giám sát kết quả chỉ cung cấp reward ở cuối mỗi đầu ra, có thể không đủ và hiệu quả để giám sát policy trong các tác vụ toán học phức tạp. Theo Wang et al. (2023b), chúng tôi cũng khám phá giám sát quá trình, cung cấp reward ở cuối mỗi bước suy luận. Hình thức, được cho câu hỏi q và G đầu ra được lấy mẫu {o_1, o_2, ..., o_G}, một mô hình reward quá trình được sử dụng để ghi điểm mỗi bước của các đầu ra, tạo ra các reward tương ứng: R = {{r^{index(1)}_1, ..., r^{index(K_1)}_1}, ..., {r^{index(1)}_G, ..., r^{index(K_G)}_G}}, trong đó index(j) là chỉ số token cuối của bước thứ j, và K_i là tổng số bước trong đầu ra thứ i. Chúng tôi cũng chuẩn hóa các reward này với trung bình và độ lệch chuẩn, tức là r̃^{index(j)}_i = (r^{index(j)}_i - mean(R))/std(R). Tiếp theo, giám sát quá trình tính toán advantage của mỗi token như tổng của các reward được chuẩn hóa từ các bước tiếp theo, tức là Â_{i,t} = ∑_{index(j)≥t} r̃^{index(j)}_i, và sau đó tối ưu hóa policy bằng cách tối đa hóa mục tiêu được định nghĩa trong phương trình (3).

4.1.4. RL Lặp lại với GRPO
Khi quá trình huấn luyện học tăng cường tiến triển, mô hình reward cũ có thể không đủ để giám sát mô hình policy hiện tại. Do đó, chúng tôi cũng khám phá RL lặp lại với GRPO. Như được hiển thị trong Thuật toán 1, trong GRPO lặp lại, chúng tôi tạo ra các tập huấn luyện mới cho mô hình reward dựa trên kết quả lấy mẫu từ mô hình policy và liên tục huấn luyện mô hình reward cũ bằng cách sử dụng cơ chế replay kết hợp 10% dữ liệu lịch sử. Sau đó, chúng tôi đặt mô hình tham chiếu là mô hình policy, và liên tục huấn luyện mô hình policy với mô hình reward mới.

4.2. Huấn luyện và Đánh giá DeepSeekMath-RL
Chúng tôi tiến hành RL dựa trên DeepSeekMath-Instruct 7B. Dữ liệu huấn luyện của RL là các câu hỏi định dạng chain-of-thought liên quan đến GSM8K và MATH từ dữ liệu SFT, bao gồm khoảng 144K câu hỏi. Chúng tôi loại trừ các câu hỏi SFT khác để điều tra tác động của RL trên các benchmark thiếu dữ liệu trong suốt giai đoạn RL. Chúng tôi xây dựng tập huấn luyện của các mô hình reward theo (Wang et al., 2023b). Chúng tôi huấn luyện mô hình reward ban đầu của chúng tôi dựa trên DeepSeekMath-Base 7B với tốc độ học 2e-5. Đối với GRPO, chúng tôi đặt tốc độ học của mô hình policy là 1e-6. Hệ số KL là 0.04. Đối với mỗi câu hỏi, chúng tôi lấy mẫu 64 đầu ra. Độ dài tối đa được đặt là 1024, và kích thước batch huấn luyện là 1024. Mô hình policy chỉ có một lần cập nhật duy nhất sau mỗi giai đoạn khám phá. Chúng tôi đánh giá DeepSeekMath-RL 7B trên các benchmark theo DeepSeekMath-Instruct 7B. Đối với DeepSeekMath-RL 7B, GSM8K và MATH với suy luận chain-of-thought có thể được coi là các tác vụ trong lĩnh vực và tất cả các benchmark khác có thể được coi là các tác vụ ngoài lĩnh vực.

Bảng 5 chứng minh hiệu suất của các mô hình mã nguồn mở và đóng với cả suy luận chain-of-thought và tool-integrated trên các benchmark tiếng Anh và tiếng Trung. Chúng tôi thấy rằng:
1) DeepSeekMath-RL 7B đạt độ chính xác 88.2% và 51.7% trên GSM8K và MATH, tương ứng, sử dụng suy luận chain-of-thought. Hiệu suất này vượt trội so với tất cả các mô hình mã nguồn mở trong phạm vi 7B đến 70B, cũng như phần lớn các mô hình mã nguồn đóng. 2) Quan trọng, DeepSeekMath-RL 7B chỉ được huấn luyện trên dữ liệu tinh chỉnh chỉ dẫn định dạng chain-of-thought của GSM8K và MATH, bắt đầu từ DeepSeekMath-Instruct 7B. Mặc dù phạm vi hạn chế của dữ liệu huấn luyện, nó vượt trội hơn DeepSeekMath-Instruct 7B trên tất cả các số liệu đánh giá, thể hiện hiệu quả của học tăng cường.

5. Thảo luận

Trong phần này, chúng tôi sẽ chia sẻ những phát hiện của chúng tôi trong các thí nghiệm huấn luyện trước và RL.

5.1. Bài học Kinh nghiệm trong Huấn luyện trước
Chúng tôi đầu tiên chia sẻ kinh nghiệm của chúng tôi trong huấn luyện trước. Trừ khi được chỉ định khác, chúng tôi sẽ tuân thủ các thiết lập huấn luyện được nêu trong Phần 2.2.1. Đáng chú ý là, khi đề cập đến DeepSeekMath Corpus trong phần này, chúng tôi sử dụng một bộ dữ liệu 89B-token từ lần lặp thứ hai của quá trình thu thập dữ liệu.

5.1.1. Huấn luyện Mã có lợi cho Suy luận Toán học
Một giả thuyết phổ biến nhưng chưa được xác minh cho rằng huấn luyện mã cải thiện suy luận. Chúng tôi cố gắng đưa ra một câu trả lời một phần cho điều này, đặc biệt trong lĩnh vực toán học: huấn luyện mã cải thiện khả năng của mô hình thực hiện suy luận toán học cả có và không có sử dụng công cụ.

Để nghiên cứu cách huấn luyện mã ảnh hưởng đến suy luận toán học, chúng tôi đã thí nghiệm với các thiết lập huấn luyện hai giai đoạn và một giai đoạn sau đây:

Huấn luyện Hai giai đoạn
• Huấn luyện Mã trong 400B Token → Huấn luyện Toán học trong 150B Token: Chúng tôi huấn luyện DeepSeek-LLM 1.3B trong 400B token mã theo sau bởi 150B token toán học;
• Huấn luyện Tổng quát trong 400B Token → Huấn luyện Toán học trong 150B Token: Như một thí nghiệm đối chứng, chúng tôi cũng thí nghiệm với các token tổng quát (được lấy mẫu từ một corpus tổng quát quy mô lớn được tạo bởi DeepSeek-AI) thay vì các token mã trong giai đoạn đầu tiên của huấn luyện, trong nỗ lực điều tra lợi thế của các token mã so với các token tổng quát trong việc cải thiện suy luận toán học.

Huấn luyện Một giai đoạn
• Huấn luyện Toán học trong 150B Token: Chúng tôi huấn luyện DeepSeek-LLM 1.3B trong 150B token toán học;
• Huấn luyện trên hỗn hợp của 400B Token Mã và 150B Token Toán học: Huấn luyện toán học sau huấn luyện mã làm giảm hiệu suất lập trình. Chúng tôi điều tra liệu các token mã, khi được trộn với các token toán học cho huấn luyện một giai đoạn, có còn cải thiện suy luận toán học và cũng giảm thiểu vấn đề quên thảm khốc.

Kết quả Bảng 6 và Bảng 7 chứng minh hiệu suất downstream dưới các thiết lập huấn luyện khác nhau.

Huấn luyện mã có lợi cho suy luận toán học hỗ trợ chương trình, cả dưới thiết lập huấn luyện hai giai đoạn và một giai đoạn. Như được hiển thị trong Bảng 6, dưới thiết lập huấn luyện hai giai đoạn, chỉ riêng huấn luyện mã đã tăng cường đáng kể khả năng giải quyết các vấn đề GSM8K và MATH bằng Python. Huấn luyện toán học trong giai đoạn thứ hai tạo ra những cải thiện thêm. Thú vị, dưới thiết lập huấn luyện một giai đoạn, trộn các token mã và token toán học hiệu quả giảm thiểu vấn đề quên thảm khốc phát sinh từ huấn luyện hai giai đoạn, và cũng tạo ra sự cộng hưởng giữa lập trình (Bảng 7) và suy luận toán học hỗ trợ chương trình (Bảng 6).

Huấn luyện mã cũng cải thiện suy luận toán học mà không sử dụng công cụ. Dưới thiết lập huấn luyện hai giai đoạn, giai đoạn ban đầu của huấn luyện mã đã tạo ra những cải thiện vừa phải. Nó cũng tăng cường hiệu quả của huấn luyện toán học tiếp theo, cuối cùng dẫn đến hiệu suất tốt nhất. Tuy nhiên, kết hợp các token mã và token toán học cho huấn luyện một giai đoạn làm tổn hại đến suy luận toán học mà không sử dụng công cụ. Một suy đoán là DeepSeek-LLM 1.3B, do quy mô hạn chế của nó, thiếu khả năng để hoàn toàn tiếp thu cả dữ liệu mã và toán học đồng thời.

5.1.2. Các bài báo arXiv có vẻ không hiệu quả trong việc Cải thiện Suy luận Toán học
Các bài báo arXiv thường được bao gồm như một thành phần của dữ liệu huấn luyện trước toán học (Azerbayev et al., 2023; Lewkowycz et al., 2022a; Polu và Sutskever, 2020; Wang et al., 2023c). Tuy nhiên, phân tích chi tiết về tác động của chúng đối với suy luận toán học chưa được tiến hành rộng rãi. Có lẽ ngược với trực giác, theo các thí nghiệm của chúng tôi, các bài báo arXiv có vẻ không hiệu quả trong việc cải thiện suy luận toán học. Chúng tôi thí nghiệm với các mô hình có kích thước khác nhau, bao gồm DeepSeek-LLM 1.3B và DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), sử dụng các corpus arXiv đã trải qua các pipeline xử lý khác nhau:

• MathPile (Wang et al., 2023c): một corpus 8.9B-token được phát triển với các quy tắc heuristic làm sạch và lọc, hơn 85% trong số đó là các bài báo arXiv khoa học;
• ArXiv-RedPajama (Computer, 2023): toàn bộ các tệp LaTeX arXiv với preamble, bình luận, macro, và thư mục tham khảo được loại bỏ, tổng cộng 28.0B token.

Trong các thí nghiệm của chúng tôi, chúng tôi huấn luyện riêng biệt DeepSeek-LLM 1.3B trong 150B token và DeepSeek-Coder-Base-v1.5 7B trong 40B token trên mỗi corpus arXiv. Có vẻ như các bài báo arXiv không hiệu quả trong việc cải thiện suy luận toán học. Khi được huấn luyện trên một corpus chỉ có arXiv, cả hai mô hình đều không hiển thị cải thiện đáng chú ý hoặc thậm chí suy thoái trên các benchmark toán học khác nhau về độ phức tạp khác nhau được sử dụng trong nghiên cứu này. Các benchmark này bao gồm các bộ dữ liệu suy luận định lượng như GSM8K và MATH (Bảng 8), thách thức trắc nghiệm như MMLU-STEM (Bảng 8), và toán học hình thức như miniF2F (Bảng 9).

Tuy nhiên, kết luận này có những hạn chế và nên được xem xét một cách thận trọng. Chúng tôi chưa nghiên cứu:
• Tác động của các token arXiv đối với các tác vụ liên quan đến toán học cụ thể không được bao gồm trong nghiên cứu này, chẳng hạn như informalization của các định lý là chuyển đổi các tuyên bố hoặc chứng minh hình thức thành các phiên bản không chính thức của chúng;
• Hiệu quả của các token arXiv khi được kết hợp với các loại dữ liệu khác;
• Liệu lợi ích của các bài báo arXiv sẽ tự thể hiện ở quy mô mô hình lớn hơn.

Do đó, cần khám phá thêm, mà chúng tôi để lại cho các nghiên cứu tương lai.

5.2. Thông tin về Học tăng cường

5.2.1. Hướng tới một Mô hình Thống nhất
Trong phần này, chúng tôi cung cấp một mô hình thống nhất để phân tích các phương pháp huấn luyện khác nhau, như SFT, RFT, DPO, PPO, GRPO, và tiến hành thêm các thí nghiệm để khám phá các yếu tố của mô hình thống nhất. Tổng quát, gradient đối với tham số θ của một phương pháp huấn luyện có thể được viết như:

∇_θ J_A(θ) = E[(q,o)∼D] [1/|o| ∑_{t=1}^{|o|} GC_A(q,o,t,π_rf) ∇_θ log π_θ(o_t|q,o_{<t})]  (5)

Có tồn tại ba thành phần chính: 1) Nguồn Dữ liệu D, xác định dữ liệu huấn luyện; 2) Hàm Reward π_rf, là nguồn của tín hiệu reward huấn luyện; 3) Thuật toán A: xử lý dữ liệu huấn luyện và tín hiệu reward thành hệ số gradient GC xác định độ lớn của hình phạt hoặc tăng cường cho dữ liệu. Chúng tôi phân tích một số phương pháp đại diện dựa trên mô hình thống nhất như vậy:

• Supervised Fine-tuning (SFT): SFT tinh chỉnh mô hình được huấn luyện trước trên dữ liệu SFT được con người chọn lọc.
• Rejection Sampling Fine-tuning (RFT): RFT tinh chỉnh thêm mô hình SFT trên các đầu ra được lọc được lấy mẫu từ mô hình SFT dựa trên các câu hỏi SFT. RFT lọc các đầu ra dựa trên tính chính xác của câu trả lời của chúng.
• Direct Preference Optimization (DPO): DPO tinh chỉnh thêm mô hình SFT bằng cách tinh chỉnh nó trên các đầu ra được tăng cường được lấy mẫu từ mô hình SFT, sử dụng loss DPO theo cặp.
• Online Rejection Sampling Fine-tuning (Online RFT): Khác với RFT, Online RFT khởi tạo mô hình policy bằng mô hình SFT và tinh chỉnh nó bằng cách tinh chỉnh với các đầu ra được tăng cường được lấy mẫu từ mô hình policy thời gian thực.
• PPO/GRPO: PPO/GRPO khởi tạo mô hình policy bằng mô hình SFT và tăng cường nó với các đầu ra được lấy mẫu từ mô hình policy thời gian thực.

Chúng tôi tóm tắt các thành phần của các phương pháp này trong Bảng 10. Vui lòng tham khảo Phụ lục A.1 để biết quy trình dẫn xuất chi tiết hơn.

Quan sát về Nguồn Dữ liệu Chúng tôi chia nguồn dữ liệu thành hai loại, lấy mẫu trực tuyến, và lấy mẫu ngoại tuyến. Lấy mẫu trực tuyến biểu thị rằng dữ liệu huấn luyện đến từ kết quả khám phá của mô hình policy huấn luyện thời gian thực, trong khi lấy mẫu ngoại tuyến biểu thị rằng dữ liệu huấn luyện đến từ kết quả lấy mẫu của mô hình SFT ban đầu. RFT và DPO theo phong cách ngoại tuyến, trong khi Online RFT và GRPO theo phong cách trực tuyến.

Như được hiển thị trong Hình 5, chúng tôi thấy rằng Online RFT vượt trội đáng kể so với RFT trên hai benchmark. Cụ thể, Online RFT có thể so sánh với RFT ở giai đoạn đầu của huấn luyện nhưng đạt được lợi thế tuyệt đối ở giai đoạn sau, chứng minh sự ưu việt của huấn luyện trực tuyến. Điều này trực quan, vì ở giai đoạn ban đầu, actor và mô hình SFT thể hiện sự tương đồng gần gũi, với dữ liệu được lấy mẫu chỉ hiển thị những khác biệt nhỏ. Tuy nhiên, ở giai đoạn sau, dữ liệu được lấy mẫu từ actor sẽ thể hiện những khác biệt đáng kể hơn, và lấy mẫu dữ liệu thời gian thực sẽ mang lại lợi thế lớn hơn.

Quan sát về Hệ số Gradient Thuật toán xử lý tín hiệu reward thành hệ số gradient để cập nhật tham số mô hình. Chúng tôi chia hàm reward thành 'Rule' và 'Model' trong các thí nghiệm của chúng tôi. Rule đề cập đến việc đánh giá chất lượng của một phản hồi dựa trên tính chính xác của câu trả lời, và Model biểu thị rằng chúng tôi huấn luyện một mô hình reward để ghi điểm mỗi phản hồi. Dữ liệu huấn luyện của mô hình reward dựa trên đánh giá quy tắc. Các phương trình 10 và 21 làm nổi bật một khác biệt chính giữa GRPO và Online RFT: GRPO độc đáo điều chỉnh hệ số gradient của nó dựa trên giá trị reward được cung cấp bởi mô hình reward. Điều này cho phép tăng cường và phạt phân biệt các phản hồi theo độ lớn khác nhau của chúng. Ngược lại, Online RFT thiếu tính năng này; nó không phạt các phản hồi không chính xác và tăng cường đồng nhất tất cả các phản hồi với câu trả lời chính xác ở cùng mức độ cường độ.

Như được chứng minh trong Hình 5, GRPO vượt trội so với online RFT, từ đó làm nổi bật hiệu quả của việc thay đổi hệ số gradient tích cực và tiêu cực. Ngoài ra, GRPO+PS cho thấy hiệu suất vượt trội so với GRPO+OS, cho thấy lợi ích của việc sử dụng hệ số gradient tinh vi, nhận biết từng bước. Hơn nữa, chúng tôi khám phá RL lặp lại, trong các thí nghiệm của chúng tôi, chúng tôi tiến hành hai vòng lặp. Như được hiển thị trong Hình 6, chúng tôi nhận thấy rằng RL lặp lại cải thiện đáng kể hiệu suất, đặc biệt là ở lần lặp đầu tiên.

5.2.2. Tại sao RL hoạt động?
Trong bài báo này, chúng tôi tiến hành học tăng cường dựa trên một tập con của dữ liệu tinh chỉnh chỉ dẫn, và nó đạt được cải thiện hiệu suất đáng kể so với mô hình tinh chỉnh chỉ dẫn. Để giải thích thêm tại sao học tăng cường hoạt động. Chúng tôi đánh giá độ chính xác Pass@K và Maj@K của các mô hình Instruct và RL trên hai benchmark. Như được hiển thị trong Hình 7, RL tăng cường hiệu suất của Maj@K nhưng không phải Pass@K. Những phát hiện này chỉ ra rằng RL tăng cường hiệu suất tổng thể của mô hình bằng cách làm cho phân phối đầu ra mạnh mẽ hơn, nói cách khác, có vẻ như cải thiện được quy cho việc tăng cường phản hồi chính xác từ TopK thay vì tăng cường khả năng cơ bản. Tương tự, (Wang et al., 2023a) đã xác định một vấn đề misalignment trong các tác vụ suy luận trong mô hình SFT, cho thấy rằng hiệu suất suy luận của các mô hình SFT có thể được cải thiện thông qua một loạt các chiến lược alignment preference (Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b).

5.2.3. Làm thế nào để Đạt được RL hiệu quả hơn?
Chúng tôi chứng minh RL hoạt động khá tốt trong các tác vụ suy luận toán học. Chúng tôi cũng cung cấp một mô hình thống nhất để hiểu các phương pháp huấn luyện đại diện khác nhau. Trong mô hình này, tất cả các phương pháp được khái niệm hóa như các kỹ thuật RL trực tiếp hoặc đơn giản hóa. Như được tóm tắt trong Phương trình 5, có tồn tại ba thành phần chính: Nguồn Dữ liệu, Thuật toán, và Hàm Reward. Chúng tôi cung cấp một số hướng tương lai tiềm năng về ba thành phần.

Nguồn Dữ liệu Nguồn dữ liệu là nguyên liệu thô của tất cả các phương pháp huấn luyện. Trong ngữ cảnh của RL, chúng tôi cụ thể đề cập đến nguồn dữ liệu như các câu hỏi không được gán nhãn với các đầu ra được lấy mẫu từ mô hình policy. Trong bài báo này, chúng tôi chỉ sử dụng các câu hỏi từ giai đoạn tinh chỉnh chỉ dẫn và một lấy mẫu nucleus ngây thơ để lấy mẫu các đầu ra. Chúng tôi nghĩ đây là một lý do tiềm năng mà pipeline RL của chúng tôi chỉ cải thiện hiệu suất Maj@K. Trong tương lai, chúng tôi sẽ khám phá pipeline RL của chúng tôi trên các prompt câu hỏi ngoài phân phối, kết hợp với các chiến lược lấy mẫu (giải mã) tiên tiến, như những chiến lược dựa trên các phương pháp tìm kiếm cây (Yao et al., 2023). Ngoài ra, các kỹ thuật suy luận hiệu quả (Kwon et al., 2023; Leviathan et al., 2023; Xia et al., 2023, 2024), xác định hiệu quả khám phá của các mô hình policy, cũng đóng một vai trò cực kỳ quan trọng.

Thuật toán Thuật toán xử lý dữ liệu và tín hiệu reward thành hệ số gradient để cập nhật tham số mô hình. Dựa trên Phương trình 5, ở một mức độ nào đó, tất cả các phương pháp hiện tại hoàn toàn TIN TƯỞNG tín hiệu của hàm reward để tăng hoặc giảm xác suất có điều kiện của một token nhất định. Tuy nhiên, không thể đảm bảo rằng tín hiệu reward luôn đáng tin cậy, đặc biệt trong các tác vụ cực kỳ phức tạp. Ví dụ, ngay cả bộ dữ liệu PRM800K (Lightman et al., 2023), đã được chú thích cẩn thận bởi các người chú thích được đào tạo tốt, vẫn chứa khoảng 20% chú thích không chính xác⁷. Để kết thúc này, chúng tôi sẽ khám phá thuật toán học tăng cường mạnh mẽ chống lại các tín hiệu reward nhiễu. Chúng tôi tin rằng các phương pháp alignment WEAK-TO-STRONG (Burns et al., 2023) như vậy sẽ mang lại một thay đổi cơ bản cho các thuật toán học tập.

Hàm Reward Hàm reward là nguồn của tín hiệu huấn luyện. Trong RL, hàm reward thường là mô hình reward neural. Chúng tôi nghĩ có tồn tại ba hướng quan trọng cho các mô hình reward: 1) Làm thế nào để tăng cường khả năng tổng quát hóa của mô hình reward. Mô hình reward phải được tổng quát hóa hiệu quả để xử lý các câu hỏi ngoài phân phối và các đầu ra giải mã tiên tiến; nếu không, học tăng cường có thể chỉ ổn định phân phối của các LLM thay vì cải thiện khả năng cơ bản của chúng; 2) Làm thế nào để phản ánh sự không chắc chắn của mô hình reward. Sự không chắc chắn có thể tiềm năng hoạt động như một cầu nối liên kết giữa mô hình reward yếu và các thuật toán học tập từ yếu đến mạnh; 3) Làm thế nào để xây dựng hiệu quả các mô hình reward quá trình chất lượng cao có thể cung cấp các tín hiệu huấn luyện tinh vi cho quá trình suy luận (Lightman et al., 2023; Wang et al., 2023b).

6. Kết luận, Hạn chế, và Công việc Tương lai
Chúng tôi trình bày DeepSeekMath, vượt trội hơn tất cả các mô hình mã nguồn mở trên benchmark MATH cấp độ thi đấu và tiệm cận hiệu suất của các mô hình đóng. DeepSeekMath được khởi tạo với DeepSeek-Coder-v1.5 7B và trải qua huấn luyện liên tục trong 500B token, với một thành phần đáng kể của dữ liệu huấn luyện là 120B token toán học có nguồn từ Common Crawl. Nghiên cứu ablation mở rộng của chúng tôi cho thấy các trang web mang lại tiềm năng đáng kể cho dữ liệu toán học chất lượng cao, trong khi arXiv có thể không có lợi như chúng tôi mong đợi. Chúng tôi giới thiệu Group Relative Policy Optimization (GRPO), một biến thể của Proximal Policy Optimization (PPO), có thể cải thiện đáng kể khả năng suy luận toán học với ít tiêu thụ bộ nhớ hơn. Kết quả thí nghiệm cho thấy rằng GRPO hiệu quả ngay cả khi DeepSeekMath-Instruct 7B đã đạt điểm cao trên các benchmark. Chúng tôi cũng cung cấp một mô hình thống nhất để hiểu một loạt các phương pháp và tóm tắt một số hướng tiềm năng cho học tăng cường hiệu quả hơn.

Mặc dù DeepSeekMath đạt được điểm ấn tượng trên các benchmark suy luận định lượng, khả năng của nó về hình học và chứng minh định lý tương đối yếu hơn so với các mô hình đóng. Ví dụ, trong thử nghiệm khô của chúng tôi, mô hình không thể xử lý các vấn đề liên quan đến tam giác và ellipse, có thể chỉ ra thiên vị chọn lọc dữ liệu trong huấn luyện trước và tinh chỉnh. Ngoài ra, bị hạn chế bởi quy mô mô hình, DeepSeekMath kém hơn GPT-4 về khả năng few-shot. GPT-4 có thể cải thiện hiệu suất với các đầu vào few-shot, trong khi DeepSeekMath cho thấy hiệu suất tương tự trong đánh giá zero-shot và few-shot. Trong tương lai, chúng tôi sẽ cải thiện thêm pipeline chọn lọc dữ liệu được thiết kế của chúng tôi để xây dựng corpus được huấn luyện trước chất lượng cao hơn. Ngoài ra, chúng tôi sẽ khám phá các hướng tiềm năng (Phần 5.2.3) cho học tăng cường hiệu quả hơn của các LLM.

Tài liệu tham khảo
[Tiếp theo là danh sách tài liệu tham khảo dài...]
