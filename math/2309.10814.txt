# 2309.10814.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/math/2309.10814.pdf
# File size: 988630 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Natural Language Embedded Programs for
Hybrid Language Symbolic Reasoning
Tianhua Zhang†∗, Jiaxin Ge♢∗, Hongyin Luo‡∗, Yung-Sung Chuang‡, Mingye Gao‡,
Yuan Gong‡, Xixin Wu†, Yoon Kim‡, Helen Meng†, James Glass‡
†The Chinese University of Hong Kong,‡Massachusetts Institute of Technology,♢Peking University
tzhang@se.cuhk.edu.hk, aomaru@stu.pku.edu.cn, hyluo@mit.edu
Abstract
How can we perform computations over natu-
ral language representations to solve tasks that
require symbolic and numeric reasoning? We
propose natural language embedded programs
(NLEP) as a unifying framework for address-
ing math/symbolic reasoning, natural language
understanding, and instruction following tasks.
Our approach prompts a language model to gen-
erate full Python programs that define functions
over data structures which contain natural lan-
guage representations of structured knowledge.
A Python interpreter then executes the gener-
ated code and prints the output. Despite using a
task-general prompt, we find that this approach
can improve upon strong baselines across a
range of different tasks including math and
symbolic reasoning, text classification, ques-
tion answering, and instruction following. We
found that the generated programs are inter-
pretable since they outline the exact reasoning
process followed by the program interpreter.
1 Introduction
Solving complex language tasks often requires per-
forming computations over natural language repre-
sentations. For language-based reasoning, chain-
of-thought prompting (CoT; Wei et al., 2022) has
emerged as a promising approach for surfacing
the symbolic reasoning capabilities of large lan-
guage models (LLMs). However, certain types
of computations (e.g., arithmetic) are unnatural
to perform in pure language space, and hence
present difficulties for LLMs. General-purpose
programming languages, on the other hand, pro-
vide convenient abstractions as well as predefined
libraries and functions for natively implementing
many types of symbolic computations, and there
∗Equal contribution. Correspondence to Hongyin
Luo at hyluo@mit.edu . Work done during Jiaxin Ge vis-
iting MIT. Source code is available at https://github.com/
luohongyin/LangCode .has been much recent work on interleaving pro-
gram calls within CoT-style reasoning to extend the
capabilities of LLMs. While promising, existing
methods are generally limited to narrow types of
tasks such as math and symbolic reasoning (Chen
et al., 2022; Cai et al., 2023; Gao et al., 2023), sim-
ple API calling (Schick et al., 2023; Paranjape et al.,
2023; Liang et al., 2023a), and database accessing
(Cheng et al., 2022). These works moreover rely on
task-specific prompts which are hard to generalize
across datasets.
This work describes a task-general approach for
combining the language-based reasoning capabili-
ties of LLMs with symbolic computations enabled
by the use of programs. Specifically, we prompt
LLMs to generate natural language embedded pro-
grams (NLEPs), which are fully executable Python
programs containing appropriate package import-
ing, structured natural language representations of
knowledge, function definitions for problem solv-
ing, and response printing. The generated NLEP
is then executed using a Python interpreter that
captures the standard output of the program as the
response. An example of an NLEP generated by
GPT-4 is shown in Figure 1.
NLEPs use code as a scaffold to reason over
natural language representations of data and knowl-
edge. This makes our approach different from
ToolFormer (Schick et al., 2023) and language
model as tool maker (LATM; Cai et al., 2023),
which instead use language as the scaffold and inter-
leave API calls within natural language sentences
during LLM generation. Compared to program-
of-thought (PoT; Chen et al., 2022) and program
aided language models (PAL; Gao et al., 2023),
which mainly focus on math and symbolic prob-
lems, NLEPs handles natural language, math, and
symbolic tasks with significantly better generaliza-
tion ability based on task-general prompts. Com-
pared to existing neuro-symbolic augmentations
approaches needing new logical language (Li et al.,arXiv:2309.10814v2  [cs.CL]  29 Mar 2024

--- PAGE 2 ---
  # Step 1: Import necessary libraries from datetime import datetime # Step 2: Define a dictionary storing the knowledge about US presidents us_presidents = {     "Dwight D. Eisenhower": {"birth_date": "1890-10-14", "term_start": "1953-01-20"},     "John F. Kennedy":      {"birth_date": "1917-05-29", "term_start": "1961-01-20"},     "Lyndon B. Johnson":    {"birth_date": "1908-08-27", "term_start": "1963-11-22"},     "Richard Nixon":        {"birth_date": "1913-01-09", "term_start": "1969-01-20"},     "Gerald Ford":          {"birth_date": "1913-07-14", "term_start": "1974-08-09"},     "Jimmy Carter":         {"birth_date": "1924-10-01", "term_start": "1977-01-20"},     "Ronald Reagan":        {"birth_date": "1911-02-06", "term_start": "1981-01-20"},     "George H. W. Bush":    {"birth_date": "1924-06-12", "term_start": "1989-01-20"},     "Bill Clinton":         {"birth_date": "1946-08-19", "term_start": "1993-01-20"},     "George W. Bush":       {"birth_date": "1946-07-06", "term_start": "2001-01-20"},     "Barack Obama":         {"birth_date": "1961-08-04", "term_start": "2009-01-20"},     "Donald Trump":         {"birth_date": "1946-06-14", "term_start": "2017-01-20"},     "Joe Biden":            {"birth_date": "1942-11-20", "term_start": "2021-01-20"}, } # Step 3: Define a function that finds the presidents who are elected after a certain year and born on a certain day of the week def find_presidents(presidents, year, day_of_week):     result = []     for president, info in presidents.items():         birth_date = datetime.strptime(info["birth_date"], "%Y-%m-%d")         term_start = datetime.strptime(info["term_start"], "%Y-%m-%d")         if birth_date.strftime("%A") == day_of_week and term_start.year > year:             result.append(president)     return result # Step 4: Print an answer in natural language using the knowledge and function defined above year = 1950 day_of_week = "Monday" presidents = find_presidents(us_presidents, year, day_of_week) if presidents:     print("The US presidents who were elected after 1950 and born on Mondays are: " + ", ".join(presidents) + ".") else:     print("There are no US presidents who were elected after 1950 and born on Mondays.")ToolUsing
Structuredknowledge
Functiondefinition
PrintanswerThe US presidents who were elected after 1950 and born on Mondays are: Gerald Ford, Ronald Reagan, Bill Clinton.Instruction:Who are the US presidents elected after 1950 and born on Monday?
Stdout:NLEP generated by GPT-4Figure 1: A generated NLEP correctly answers the given question while ChatGPT-4 obtains an incorrect answer (link). This
NLEP uses the date-weekday conversion tool in the datetime package, constructs structured knowledge about US presidents,
implements a selection function, and outputs natural language responses depending on the function output. A more detailed
comparison between NLEP and ChatGPT-4 code interpreter is shown in Figure 5.
2023) and predefined solvers (Pan et al., 2023),
NLEP uses a general programming interface thus
can handle a significantly wider range of tasks
with more flexible programming elements includ-
ing packages, databases, and APIs.
Experiments across math and symbolic reason-
ing, question answering and instruction following,
and text classification tasks demonstrate that (1)
NLEPs conducts accurate reasoning on both struc-
tured and unstructured tasks and inputs; (2) NLEP’s
step-by-step, meta prompting strategy can signif-
icantly improve the prompt efficiency across dif-
ferent tasks. As a result, we conclude that pro-
gramming language prompting with NLEP is more
capable and generalizable than existing natural lan-
guage and neuro-symbolic prompting strategies.
2 Approach: NLEP Prompting
An NLEP is a program containing both program-
ming code and natural language. NLEPs use natu-
ral language in several different ways. First, it uses
natural language comments to guide step-by-step
program generation. Second, language is used to
represent structured knowledge through Python’s
native data structures (e.g., dictionaries and lists).Finally, an NLEP uses language to print fluent re-
sponses to the user input by constructing a standard
output string containing references to program vari-
ables.
The hybrid language-symbolic design of NLEP
enables generalized problem solving for natural
language, math, symbolic reasoning, and API call-
ing tasks, which have traditionally been tackled
by separate mechanisms. This approach combines
the benefits of language-based reasoning with pro-
gram synthesis: comments and knowledge in nat-
ural language improve program generation, while
the structured/symbolic reasoning powered by pro-
gram interpreters provides more accurate compu-
tations than would have been obtained via direct
decoding from LLMs.
An example of an NLEP for answering a ques-
tion is shown in Figure 5. In the generated program,
each section is preceded by comments in natural
language, and the defined counting function uses
knowledge stored in a key-value dictionary (which
itself is generated from GPT-4’s internal knowl-
edge) to find the correct answer. Finally, the answer
is printed through a natural language response. In

--- PAGE 3 ---
this example, we generated 5 independent NLEPs
and found that they achieve 100% accuracy, com-
pared to 60% for ChatGPT-4 and 40% GPT-4 API.
NLEP structure. More generally, each NLEP
contains four sections: (1) importing necessary li-
braries, (2) defining variables containing structured
knowledge, (3) implementing problem-solving
functions, and (4) printing the response in natural
language. Instead of providing direct solutions for
each task, we guide the model to arrive at a solution
following this four-step process. The structured
and modularized programming design disentangles
the knowledge extraction and reasoning steps, in
contrast to PoT (Chen et al., 2022), where different
segments may intertwine. As show in the exam-
ple in Figure 1, an NLEP answers the question by
constructing a structured knowledge dictionary con-
taining the birthday and start date of the US presi-
dents. To recognize the weekdays, the program uti-
lizes pre-defined functions in the datetime pack-
age. The selected answers are stored in a list and
then embedded into an output template. The NLEP
also handles the situation when no answer is found.
The correct answer is then printed by the NLEP.
Task-general demonstration prompts. As is
standard in chain-of-thought prompting (Nye et al.,
2021; Wei et al., 2022), our approach uses demon-
stration prompts for NLEP generation. However,
unlike previous approaches our demonstrations are
not task-specific. The unified NLEP structure mo-
tivates models to solve different problems in the
same disentangled four steps, bypassing the ne-
cessity for task-specific, in-domain examples to
explicitly teach the models for a particular task.
For example, for all classification tasks we con-
sider we use the same demonstration prompt (de-
rived from SST2). Similarly, we use mostly the
same prompt for our math and symbolic reasoning
tasks. This task-general prompt is similar in spirit
to zero-shot chain-of-thought prompting (Kojima
et al., 2023) which adds a task-agnostic prompt
(“Let’s think step-by-step ”) to elicit the rea-
soning capabilities of LLMs in a task-agnostic way.
The prompts used for the various tasks are given
in Table 1, and the exact prompts are given in Ap-
pendix D. In summary, we use 4 different demon-
stration prompts across 16 tasks, each of which
works well within a task category. Thus, while the
proposed method is not fully task-agnostic in the
strictest sense of the term, it is still more flexible
than previous approaches that combine program
synthesis with chain-of-thought prompting (Chenet al., 2022; Gao et al., 2023), which use examples
from the dataset to craft prompts.
Programmatic reasoning for natural language
understanding. Prior works on combining pro-
gram synthesis with LLM-based reasoning have
generally focused on math and symbolic reasoning
tasks (Chen et al., 2022; Gao et al., 2023), and it
has not been clear how such methods could be ex-
tended to address natural language understanding
(NLU) tasks. We show that NLEPs can be straight-
forwardly extended to text classification tasks.
For question answering, we apply NLEP prompt-
ing and the target output is constructed by the gen-
erated programs. Classification tasks, on the other
hand, are handled by a different type of NLEP con-
sisting of a decision tree since standard program-
based reasoning may not seamlessly translate to
text-based classification and yield optimal perfor-
mance. Each node of the decision tree is anno-
tated by a simple natural language sentence, and
theYes/Nodecisions at each node are handled in
a zero-shot way by an entailment classifier, which
has in general been shown to be an effective ap-
proach to zero-shot text classification (Obamuyide
and Vlachos, 2018; Condoravdi et al., 2003; Ge
et al., 2023). Concretely, given the tree we com-
pute the entailment score between the input and
the language description associated with each node
and traverse the decision tree until a leaf node is
reached. We emphasize that the topology of the
tree and the textual description of each node is gen-
erated by the prompted LLM only once per bench-
mark. The decision maker per question is indeed
the smaller entailment models for tree traversal.
Consequently, NLEP leverages LLMs to efficiently
construct decision trees, assisting smaller models in
making informed decisions and augmenting overall
performance. The demonstration prompt for clas-
sification tasks is given by a manually constructed
example from SST2 (Wang et al., 2018) for dif-
ferent benchmarks. We find that this prompt can
generate NLEPs containing sensible decision trees
for various classification tasks without requiring
task-specific examples. An example of the gener-
ated program and the corresponding decision tree
is shown in Figure 2.
3 Experiments
We evaluate natural language embedded programs
(NLEPs) on 16 tasks across three broad task cate-
gories. The tasks and corresponding prompts are
summarized in Table 1.

--- PAGE 4 ---
    # Step 1: Define the criteria of the decision tree.    criterions = {        'is_positive': ‘My feeling is positive',        'is_sad': 'I feel sad',        'is_angry': 'I feel angry',        'is_afraid': 'I feel afraid',        'is_happy': 'I feel happy',        'is_love': 'I feel love',        'is_surprised': 'I feel surprised'    }    # Step 2: Define the balanced decision tree for this classification task    tree = {        'root': 'is_positive',        'is_positive': {'yes': 'is_happy', 'no': 'is_sad'},        'is_happy': {'yes': 'I feel happy', 'no': 'is_love'},        'is_love': {'yes': 'I feel love', 'no': 'is_surprised'},        'is_surprised': {'yes': 'I feel surprised', 'no': 'I feel happy'},        'is_sad': {'yes': 'I feel sad', 'no': 'is_angry'},        'is_angry': {'yes': 'I feel angry', 'no': 'is_afraid'},        'is_afraid': {'yes': 'I feel afraid', 'no': 'I feel sad'}    }Input Prompt# Task: Emotion classification# Description: I wrote a sentence about my feeling, determine which emotion do I feel.# Possible emotions: ['sad', 'happy', 'love', 'angry', 'afraid', 'surprised']# Generated decision Tree:is_positiveis_happyis_sad“happy”is_love“love”is_surprised“happy”“surprised”“sad”is_angry“angry”is_afraid“angry”“sad”YESYES
YESYESYESYESYESNONONONOClassification accuracy of zero-shot methods (%) GPT-3: 42.7   6-class RoBERTa-Ent: 49.2   Decision Tree RoBERTa-Ent: 54.5
NONONO  RoBERTa_Ent(      f“{is_angry} is entailed by {input}”  )  Ent.  Con.YES  Ent.  Con.NO
EntailmentClassificationFigure 2: A decision tree structure generated within an NLEP for emotion classification based on task description using an
example program for SST2 as the prompt. The branching of each node is decided by a RoBERTa (Liu et al., 2019) text entailment
model. This language-based decision tree generated by an NLEP outperforms GPT-3 and entailment-based multi-class prediction
(Ge et al., 2023) without needing any task-specific examples (i.e., exemplars specific to the emotion classification dataset).
Math and symbolic reasoning tasks include
Tracking Shuffled Objects, Dyck Language, Word
Sorting and Chinese Remainder Theorem from
BigBench (Srivastava et al., 2023), Scheduling
Meeting task from Cai et al. (2023), GSM-Hard1
benchmark of math word problems from Gao et al.
(2023), and Game of 24 (Yao et al., 2023a). We
use two examples for all tasks except for Game of
24, for which we applied a word sorting example
to elicit stronger game-playing reasoning ability.
The exact NLEP prompts we used are given in
Appendix D.1 and D.2.
Question answering and instruction follow-
ingtasks include the StrategyQA (Geva et al.,
2021a), TruthfulQA (Lin et al., 2022), and Vicu-
naQA (Chiang et al., 2023) benchmarks. Strate-
gyQA requires models to answer multi-hop ques-
tions with “ Yes” or “ No”. TruthfulQA and Vicu-
naQA contain questions and instructions requiring
free-form responses. VicunaQA also allows us to
test how NLEPs perform in the popular instruction-
following setting. The evaluation metrics on ques-
tion answering focus on accuracy, relevance, and
factuality of the generated answers. The prompts in
Appendix D.1 are used for StrategyQA. For Truth-
fulQA and VicunaQA, we added an example with
a longer response to encourage more detailed re-
sponse generation.
Text classification tasks includes tasks that re-
1We opted for GSM-Hard to mitigate potential data con-
tamination from GSM8K. The numbers in GSM-Hard are
large, less likely to have been encountered by LLMs during
training, and hence augmenting the task complexity. However,
since GSM-Hard is automatically constructed, there are cases
where the answers are not reasonable or the questions appear
peculiar. We detail the analysis in Appendix B.Domain Task Prompt
Math and Symbolic
ReasoningTracking Shuffled Objects (7) D.1
Dyck Language D.1
Word Sorting D.1
Chinese Remainder Theorem D.1
Scheduling Meeting D.1
GSM-Hard D.1
Game of 24 D.2
Question AnsweringStrategyQA D.1
TruthfulQA D.3
VicunaQA D.3
Text ClassificationSST2 D.4
Cola D.4
Emotion-Classification D.4
Amazon Review D.4
Hate-Speech D.4
Social Bias Frame D.4
Table 1: Summary descriptions of the various tasks considered
in this work.
quire understanding of both natural language inputs
and labels. We evaluate NLEP on movie-review
classification (SST2; Socher et al., 2013), linguistic
acceptance (COLA; Warstadt et al., 2019), emotion
classification (Saravia et al., 2018), amazon review
(Ni et al., 2019), hate speech detection (de Gibert
et al., 2018), and stereotypes recognition (Sap et al.,
2019). We use the prompts in Appendix D.1 for
model-free classification. For decision tree genera-
tion, the prompts in Appendix D.4 are applied.
3.1 Math and Symbolic Reasoning
We compare NLEP prompting with chain-of-
thought (CoT; Wei et al., 2022), program-of-
thought (PoT; Chen et al., 2022), and LLMs as
tool makers (LATM; Cai et al., 2023). We also
compare against tree-of-thought (ToT; Yao et al.,
2023a) on the Game of 24 benchmark, where ToT
outperforms CoT by a significant margin (but re-

--- PAGE 5 ---
quires many more calls to the LLM). We evalu-
ate CoT and PoT with both task-general and task-
specific demonstrations. Since LATM needs in-
domain input-output pairs to create tools, we only
report the results with task-specific LATM.
Task-general prompting. For task-general
prompts we use two examples as the in-context
demonstration for the math and symbolic reason-
ing benchmarks (see Table 1 and Appendix D). For
CoT, we present two examples with intermediate
reasoning represented in natural language rather
than as programs. Our task-general PoT implemen-
tation takes the math and symbolic reasoning lines
similar as (Chen et al., 2022) and (Gao et al., 2023),
but without the step-by-step programming scheme
in NLEP as an ablation.
Task-specific prompting baselines. We report
the task-specific prompting performance as an “up-
per bound” for each task. For CoT, we use the
same prompting settings (from 3 to 8-shot) adopted
in previous studies (Cobbe et al., 2021; Cai et al.,
2023; Fu et al., 2023). For PoT, we use the same
in-context examples as in the task-specific CoT ex-
amples, but provide intermediate reasoning steps
in Python code. On the GSM-Hard benchmark,
we adopt the demonstrations (9-shot) for GSM8K
used in (Chen et al., 2022). For the Chinese Re-
mainder Theorem and Scheduling Meeting bench-
marks, we construct the in-context examples with
the first three successful instances of task-general
PoT. For LATM, we evaluate its performance on
Tracking Shuffled Objects (7) using the provided
tool and cite the results for other tasks from (Cai
et al., 2023). Details are shown in Appendix E.
Program synthesis approaches (PoT and NLEP)
may sometimes generate non-executable programs
if lack task-specific programming demonstration.
For both approaches, we select certain benchmarks
to resample up to three additional programs if the
returned program failed at execution. Since this
condition is triggered only if program execution
fails, there is no label leakage. We discuss this
further in Section 4 and provide results details in
Appendix B.
3.1.1 Results
We show the main results of NLEP prompting on
six math and symbolic reasoning tasks in Table 2.
An example of NLEP generated for solving a Dyck
language problem is shown in Figure 3(a).
GPT-4 Results. Among the three approaches
employing task-general prompts, NLEP demon-strates superior performance over CoT across 5
of 6 tasks and outperforms PoT on 4 of 6 tasks.
Additionally, NLEP achieves equivalent perfor-
mance to PoT on Word Sorting benchmark. The
large performance gap between NLEP and CoT
suggests that programmatic reasoning can enable
more accurate answers. Compared to PoT, NLEP
achieves significantly higher average accuracy, es-
pecially on the Dyck Language (66.4% →91.6%)
and the complex Chinese Remainder Theorem
(84.4% →97.2%) tasks. On GSM-Hard, we con-
firm the same phenomenon discovered by (Gao
et al., 2023) where language does not further bene-
fit the calculation accuracy with GPT-4.
NLEP also achieves comparable performance to
task-specific, few-shot prompting methods. No-
tably, our method achieves the best performance on
Tracking Shuffled Objects (7) and Dyck Language,
and outperforms task-specific CoT on many bench-
marks. On the Word Sorting benchmark, NLEP
only fails on one instance where the input word
sequence contains “steelmake" and GPT-4 auto-
matically corrected it to “steelmaker". We find that
the high scores of task-specific PoT on Word Sort-
ing and Chinese Remainder Theorem come from
the generally applicable programming code from
the in-context demonstrations.
GPT-3.5 Results. We observe a significant de-
cline in performance with GPT-3.5 for all reasoning
approaches across nearly all benchmarks. However
NLEP still achieves the best average performance,
exhibiting significant improvement on 5 of 6 tasks
over both task-specific and task-general CoT base-
lines. NLEP exceeds task-general PoT notably
across 4 tasks and demonstrates comparable per-
formance on the Word Sorting benchmark. On the
Dyck Language benchmark, program-based strate-
gies (PoT and NLEP with task-general prompts)
failed to accomplish the problem without task-
specific examples, highlighting the need for strong
backbone LLMs.
Game of 24 results. Table 3 shows the results
on the challenging Game of 24 task from (Yao
et al., 2023a). Our approach also surpasses the ora-
cle setup of IO/CoT, which calculates the success
rate of IO/CoT by considering the best of 100 sam-
ples for each instance. However, unlike ToT which
requires in-context demonstrations for each decom-
posed sub-task, NLEP prompting achieves a sig-
nificant performance gain over ToT (b=1) without
requiring a computationally expensive multi-chain
inference procedure.

--- PAGE 6 ---
GPT-4 GPT-3.5-Turbo
(a) Task-Specific (b) Task-General (c) Task-Specific (d) Task-General Tasks / Method
CoT PoT LATM CoT PoT NLEP CoT PoT CoT PoT NLEP
Tracking Shuffled Objects 100.0 100.0 100.0 81.2 98.4 100.0 68.0 6.8 51.2 88.4 74.4
Dyck Language 63.6†60.8 87.5†39.6 66.4 91.6 20.4†28.4 38.0 4.0 7.2
Word Sorting 90.9†100.0 99.1†84.4 99.6 99.6 59.2†100.0 75.2 100.0 99.6
Chinese Remainder Theorem 0.0†100.0 100.0†0.0 84.4 97.2 0.0†100.0 0.0 72.4 96.4
Scheduling Meeting 55.6†75.2 100.0†82.8 85.2 93.2 18.9†33.6 39.6 49.2 85.6
GSM-Hard 57.4 74.1 – 54.9 69.3 67.7 45.0 63.4 42.8 52.2 54.1
Average 61.3 85.0 97.3 57.2 83.9 91.6 35.3 55.4 41.1 61.0 69.6
Table 2: Performance on math and symbolic reasoning tasks with both task-specific and task-general demonstration prompts.
†stands for results from (Cai et al., 2023). LATM results are not available for GSM-Hard benchmark as it is hard to derive a
generally applicable tool function for all test cases.
Prompt Method Accuracy (%)
Task-specificCoT 4
ToT (b = 1) 45
ToT (b = 5) 74
Task-generalPoT 52
NLEP 66
Table 3: Performance on the Game of 24 benchmark. CoT
and ToT stand for chain-of-thought (Wei et al., 2022) and
tree-of-thought (Yao et al., 2023a) prompting respectively.†
shows the results from (Yao et al., 2023a).
3.2 QA and Instruction Following
StrategyQA. Experiment results are presented in
Table 4. With GPT-4, NLEP achieves the best per-
formance under the task-general prompt setting
and is competitive with the task-specific CoT. With
GPT-3.5, although the scores of code-based strate-
gies decrease more than CoT (PoT: 18.4%, NLEP:
20.1%, task-general CoT: 10.5%, task-specific CoT:
10.1%), NLEP still exceeds PoT by a significant
margin. An example of output is shown in 3(b).
TruthfulQA. We also evaluate how NLEP
prompting influences the factuality of question an-
swering with the TruthfulQA benchmark (Lin et al.,
2022). A fine-tuned GPT-3 model is applied for
automatic scoring. In this experiment, we compare
the vanilla auto-regressive text generation method
against NLEP. Traditionally, such question answer-
ing tasks have been solved only with black-box
language model without explicit symbolic compu-
tations due to the complexity of test questions.
The results are shown in Table 5. With GPT-
4, the truth score of NLEP prompting strategy is
close to standard LLM-based generation, while the
informativeness score is higher. However, perfor-
mance degrades significant with GPT-3.5-Turbo,
indicating a strong dependence on the program-
ming ability of the underlying language model.VicunaQA. Results on the VicunaQA bench-
mark are shown in Figure 4, where we follow the
standard approach and evaluate the answers using
GPT-4. We find that GPT-4 prefers its own gen-
erations, which are generally more detailed than
GPT-3.5-Turbo and NLEP responses. To control
for the bias due to response lengths, we also as-
sess all responses without the requirement about
details using another evaluation prompt. The evalu-
ation prompts with and without the requirement on
details is shown in Appendix F.1 and F.2.
As we demonstrate in Figure 4, this assessment
leads to different results on GPT-4. After remov-
ing the detail requirement in the automatic scor-
ing pipeline, NLEP achieves better performance.
This suggests that NLEP can help GPT-4 gener-
ate accurate, factual, and relevant responses. How-
ever, human-generated programs for pretraining the
GPT-4 models usually do not embed long pieces
of natural language. As a result, the responses
generated by NLEP have a limited level of detail.
3.3 Text Classification
Finally, we evaluate whether NLEPs can be applied
to solve text classification tasks that have tradition-
ally been difficult for pure program synthesis-based
approaches. As discussed in section 2, we manu-
ally construct a decision tree NLEP for SST2 and
use it as a prompt to guide GPT models to generate
decision trees for other tasks only with task and la-
bel descriptions. We highlight that the decision tree
is generated once per benchmark. Subsequently,
all decision-making for each specific question is
carried out with RoBERTa/DeBERTa to perform
the tree traversal re-using the generate tree topol-
ogy. An example input and output NLEP generated
by GPT-4 for emotion classification is shown in
Figure 2.

--- PAGE 7 ---
# Step 1: Import necessary built-in libraries
# No need to import
# Step 2: Define dictionaries storing the rappers' information
rappers = {
    "Lil Wayne" : {
        "real_name" : "Dwayne Michael Carter Jr." ,
        "grammy_awards" : 5
    },
    "Jay-Z": {
        "real_name" : "Shawn Corey Carter" ,
        "grammy_awards" : 23
    }
}
# Step 3: Define a function that checks if a rapper has over quadruple the Grammy awards of another
def check_quadruple_awards (rappers, rapper1, rapper2):
    if rappers[rapper2][ "grammy_awards" ] > 4 * rappers[rapper1][ "grammy_awards" ]:
        return "yes"
    else:
        return "no"
# Step 4: Print an answer using the defined function and varible
answer = check_quadruple_awards(rappers, "Lil Wayne" , "Jay-Z")
print(answer)
yes(b) StrategyQA: Lil Wayne similar real name rapper has over quadruple Wayne’s Grammy awards?
Stdout:NLEP generated by GPT-4
… As of now, Lil Wayne has won 5 Grammy awards, while Lil Twist has not won any. Thus, 
Lil Twist does not have over quadruple Wayne’s Grammy awards. So the answer is no.Few-shot CoT:(a) Dyck Langauge: {{}((<>)
# Step 1: Import necessary built-in libraries
# No need to import
# Step 2: Define necessary functions that generally solve this type of problem
def complete_sequence (sequence ):
    open_brackets = [ '{', '(', '<']
    close_brackets = [ '}', ')', '>']
    stack = []
    for char in sequence:
        if char in open_brackets:
            stack.append(char)
        elif char in close_brackets:
            if len(stack) == 0:
                return "Invalid sequence"
            else:
                stack.pop()
    while len(stack) != 0:
        open_bracket = stack.pop()
        sequence += close_brackets[open_brackets.index(open_bracket)]
    return sequence
# Step 3: Define constant variables for the task
sequence = '{{}((<>)'
# Step 4: Print an answer in natural language.
completed_sequence = complete_sequence(sequence)
print(f"To complete the sequence, we need to close all the open brackets in the correct order. The completed 
sequence is {completed_sequence} .")NLEP generated by GPT-4
Stdout: To complete the sequence, we need to close all the open brackets in the correct order. The 
completed sequence is {{}((<>))} .
Task-General PoT: {{}((<>)({Figure 3: NLEP generated for solving Dyck language problem. The instruction is “Complete the rest of the sequence, making
sure that the parentheses are closed properly. ” An example for StrategyQA is outlined in Figure 6.
settingGPT-4 GPT-3.5-Turbo
Task-specific Task-general Task-specific Task-general
CoT CoT PoT NLEP (ours) CoT CoT PoT NLEP (ours)
StrategyQA 81.7 78.6 68.6 81.2 71.6 68.1 50.2 61.1
Table 4: Performance on the StrategyQA benchmark. The experimental setup is the same as in Table 2. Note that LLMs do not
always generate “ Yes” or “ No”. and we only predict the “ Yes” label if the “ Yes” string is generated explicitly. See Appendices
D.1 and E for implementation details.
Foundation Model Mode True Info True * Info
GPT-4Text 76.01 97.55 73.56
NLEP 75.76 99.63 75.40
GPT-3.5-TurboText 68.91 98.90 67.93
NLEP 61.69 97.18 59.00
Table 5: Performance of GPT-4 and GPT-3.5-Turbo on the
TruthfulQA benchmark.
We compare NLEP against two baseline meth-
ods. Our first baseline uses the zero-shot classifica-
tion method proposed in (Ge et al., 2023) (“multi-
class prompting”). This method uses the same
entailment models but makes the prediction with-
out the tree structure. Our second baseline asks
a human expert to design a decision tree for each
task also based on the SST-2 example. The re-
sults shown in Table 6 show that NLEP generatedby GPT-4 outperforms multi-class prompting and
human-generated tree baselines on most datasets.
Model-free NLEP . We also tried using the task-
general prompt shown in D.1 to generate NLEPs
that directly use programs to solve these tasks.
These programs do not need any neural models and
are hence very efficient (e.g., finishing the entire
validation set in about 2 seconds on CPUs). The re-
sults can be found in Table 6 (“Model-free NLEP”).
While not achieving the performance of entailment-
based methods, the generated NLEP significantly
outperforms random baselines, suggesting that this
may be a viable approach for quickly extracting
simple and interpretable classifiers from LLMs.

--- PAGE 8 ---
Model MethodPerformance (Num. Classes)
cola (2) emotion (6) amazon (5) hsd (2) sbic (3) Average
RoBERTaMulti-class Prompting 65.87 49.2 33.31 67.78 52.99 53.83
Human-Generated Tree 69.03 22.20 26.88 64.85 58.37 48.27
NLEP w/ GPT-3.5 56.66 35.1 33.46 67.36 38.25 46.17
NLEP w/ GPT-4 68.94 54.5 38.88 70.92 55.95 57.65
DeBERTaMulti-class Prompting 53.50 51.93 37.01 67.78 59.08 53.86
Human-Generated Tree 69.22 32.15 33.00 72.18 55.02 52.31
NLEP w/ GPT-3.5 49.66 39.00 36.18 70.29 52.49 49.52
NLEP w/ GPT-4 68.36 55.4 40.2 70.08 59.68 58.74
None Model-free NLEP w/o Tree 69.13 40.55 25.76 59.62 37.63 46.54
Table 6: Zero-shot performance of different human and LLM-generated classification schemes. The GPT-4 generated decision
trees consistently exhibit significant improvement. For model-free NLEP, generated code can be executed on the entire validation
set in 2 seconds and notably surpasses the random baseline, with cola notably matching the state-of-the-art performance. Results
on SST2 benchmark is outlined in Appendix C.
Tasks / MethodsCodeLlama7b NLEP-CodeLlama7b CodeLlama13b Claude2
Task-Specific Task-General Zero-Shot Task-General Task-General
PoT PoT NLEP NLEP PoT NLEP PoT NLEP
Tracking Shuffled Objects 95.6 21.2 30.0 84.4 23.2 23.2 93.6 96.0
Dyck Language 15.2 0.8 0.8 1.2 2.0 2.8 33.6 60.4
Word Sorting 78.0 98.0 93.2 98.4 95.6 97.6 99.6 99.6
Chinese Remainder Theorem 100.0 0.0 18.8 0.0 1.2 27.2 40.4 60.4
Scheduling Meeting 32.0 4.0 24.8 34.4 5.6 8.8 10.4†24.0†
Table 7: Performance on five reasoning tasks adopted by Cai et al. (2023): (a) Prompting CodeLlama-7b-Instruct (Rozière et al.,
2023) with task-specific and task-general demonstrations. (b) We train CodeLlama-7b (Rozière et al., 2023) with out-of-domain
NLEP-format examples and report the zero-shot performance. (c) Prompting CodeLlama-13b-Instruct (Rozière et al., 2023) with
task-general demonstrations. (d) Prompting Claude-2 with task-general demonstrations by API. The demonstration examples
remain consistent with these outlined in Table 2.†indicates human evaluation.
4 Discussion
Execution failures and retries. While the task-
general PoT and NLEP lack programming demon-
strations for the target task, GPT-4 in general is
able to generate bug-free programs as presented
in Appendix B Table 11. Notably, both PoT and
NLEP obtain execution error rate of 0 on Tracking
Shuffled Objects (7) and Word Sort tasks. One ad-
vantage of the program synthesis approaches such
as PoT and NLEP is that non-executable programs
can be identified and filtered. This gives LLMs
the chance to “self-correct" and generate new an-
swers, and we take advantage of this in our math
and symbolic reasoning tasks by generating up to
three programs if there is an execution failure on
certain benchmarks. (For fair comparison we ap-
ply this reattempting scheme to PoT as well). We
ablate on this mechanism in Appendix B, Tables
8, 9 and 11. Besides effectively reducing the exe-
cution error as presented in Table 11, these retries
greatly enhance the reasoning accuracy. In partic-
ular, 12% and 15.6% improvement is observed onthe Chinese Remainder Theorem and the Schedul-
ing Meeting tasks in Table 8(b). In this work we
only experiment extra retries with larger tempera-
tures for diverse sampling and leave more advanced
“self-correction" algorithms (e.g., those that make
use of error messages (Cai et al., 2023; Hu et al.,
2023)) for future work.
Different foundation LLMs for NLEP. The
performance of task-general PoT and NLEP with
more foundation models is reported in Table 7.
We provide detailed analysis in Appendix B. Over-
all, NLEP demonstates superior performance com-
pared to task-general PoT, particularly excelling in
complex tasks such as the Chinese Remainder The-
orem. We note the trend of performance decline
on the reasoning tasks as the abilities of underling
LLMs decreased from GPT-4, Claude-2, GPT-3.5-
Turbo to CodeLlama (Rozière et al., 2023). How-
ever, this observation is not limited to NLEP, it also
applies to other prevalent reasoning approaches
like PoT. For example, on the Dyck Language task,
GPT-3.5-Turbo only achieves 7.2% (4.0%) accu-
racy with NLEP (PoT) prompting, while GPT-4

--- PAGE 9 ---
Model # NLEP >Text Detail % Score% Length
Bias
GPT-4 23.75yes 93.08 72.72
no 105.06 26.67
GPT-3.538.75yes 101.22 3.13
-Turbo no 102.50 10.34
0 20 40 60 80 100
Number of GamesGPT-4
(detail)
GPT-4
(no detail)
GPT-3.5-Turbo
(detail)
GPT-3.5-Turbo
(no detail)12.5 17.5 70.0
57.5 28.75 13.75
56.25 20.0 23.75
50.0 27.5 22.5Comparison of Model Performances
Win
Tie
Lose
Figure 4: Automatic evaluations of NLEP against standard
LLM-based generation with different models. # NLEP >Text
means that the % of NLEP responses containing more tokens
than the baseline. Detail means if the evaluation metric con-
siders details and response lengths. Score stands for the scores
received by NLEP divided by the baseline scores (>100 means
NLEP is better). Win,tie, and losestand for the % of evalua-
tion cases resulting in each category. Length Bias shows how
much the evaluation pipeline prefers longer or shorter answers
(lower means fairer, introduced in Appendix F.3).
improves these figures to 91.6% (66.4%). It indi-
cates that strong programming ability of underlying
LLMs is vital to generate accurate responses and
attain satisfactory performance. Surprisingly, zero-
shot CodeLlama-7b (Rozière et al., 2023) trained
using NLEP-style data (without in-domain exam-
ples) demonstrates superiority on Tracking Shuf-
fled Objects (7) benchmark over NLEP prompted
GPT-3.5 and Word Sorting benchmark over task-
general CoT prompted GPT-3.5, even with signifi-
cantly fewer parameters. It shows the potential for
effective training of compact large language mod-
els, enabling them to achieve performance levels
comparable to those of extremely large models.
5 Related Work
Large language models for reasoning. State-of-
the-art LLMs (OpenAI, 2022, 2023; Touvron et al.,
2023; Zeng et al., 2022) have shown very strong
performance on complicated reasoning tasks, in-
cluding commonsense (Geva et al., 2021b), math
(Cobbe et al., 2021), symbolic reasoning (Suzgun
et al., 2022), and programming (Austin et al., 2021;
Chen et al., 2021). Tackling such tasks with LLMs
often requires prompting them with demonstrations
that elicit their reasoning capabilities. (Wei et al.,
2022) proposed chain-of-thought prompting tech-
nique that encourages language to generate answersstep-by-step. (Wang et al., 2022) found that self-
consistency can further improve the performance
of chain of thoughts reasoning ability. (Kojima
et al., 2023) discovered that LLMs can perform rea-
soning without any demonstrations through adding
the incantation “ Let’s think step-by-step ".
Tree of thoughts (Yao et al., 2023a) and graph of
thoughts (Yao et al., 2023b; Besta et al., 2023)
were proposed to tackle tasks that require more
complicated reasoning processes. These improved
reasoning methods apply chain of thoughts as
the atomic reasoning step but organize reasoning
“chains” through more advanced mechanisms.
Programs and tools. Previous studies have found
that some limitations of LLMs can be overcome
by combining program synthesis techniques with
prompt-based learning. Program of thoughts (Chen
et al., 2022) and program aided language model
(Gao et al., 2023) both translate mathematical ques-
tions to equations and use the python interpreter
to ensure the correctness of the calculations. An-
other line of related work for enabling LLMs to use
tools is through interleaving API calls during LLM
generation (Qin et al., 2023; Liang et al., 2023b;
Mialon et al., 2023; Tang et al., 2023). APIs can aid
many tasks that are challenging for LLMs by pro-
viding tailored tools (e.g., calculators, search) that
can solve specific tasks. Toolformer (Schick et al.,
2023) addresses reasoning tasks by using prede-
fined tools, and LLMs as tool makers (LATM) can
implement functions solving a class of tasks based
on few-shot examples (Cai et al., 2023). With these
solutions, the correctness of a prediction can be en-
sured if correct API is called and correct inputs are
selected. Existing works on combining program
synthesis and tool usage with LLMs generally rely
on task-specific prompts, in contrast to the more
task-general prompt explored in the present work.
6 Conclusion
This work describes natural language embedded
programs (NLEP), which flexibly combine natural
language reasoning with program synthesis within
prompt-based learning to tackle a variety of tasks.
Our experiments demonstrate that NLEPs expand
the scope of applications that can be addressed by
program synthesis by more closely incorporating
natural language during code generation.

--- PAGE 10 ---
Acknowledgement
This research was supported by the Center for Per-
ceptual and Interactive Intelligence (CPII) Ltd un-
der the Innovation and Technology Commission’s
InnoHK Scheme.
Limitation
We found that the NLEP prompts are not suit-
able for generating long-form natural language re-
sponses. Experimental results on VicunaQA show
that most responses generated by NLEP prompting
have fewer tokens than responses obtained from
usual LLM generation. This feature is expected, be-
cause most naturally-occurring programs (on which
the LLMs were pretrained) do not contain large
chunks of natural language. Future work could
consider incorporating (potentially synthetically
generated) programs with longer-form natural lan-
guage within the pretraining set to enable the ap-
plication of NLEP to more involved NLG tasks. In
the context of prompt-based approaches without
parameter adjustments, the design of prompts can
affect performance. While we aimed to mitigate
this issue by expanding the scope of evaluation
benchmarks and minimizing random factors, we
did not engage in extensive prompt variation analy-
sis. Further exploration into the impact of prompt
variations remains an avenue for enhancing the ro-
bustness of reasoning approaches.
Ethical Statement
This work intends to design a accurate and inter-
pretable reasoning framework for language that en-
tails more transparent and responsible LLM appli-
cations. However, the program generation method
is more capable to handle different tasks in both
areas of natural and program languages, infecting
both humans and computing systems. As a re-
sult, we believe program generation models need
stronger alignment and careful management.
References
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen
Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.
Program synthesis with large language models. arXiv
preprint arXiv:2108.07732 .
Maciej Besta, Nils Blach, Ales Kubicek, Robert Ger-
stenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz
Lehmann, Michal Podstawski, Hubert Niewiadomski,Piotr Nyczyk, et al. 2023. Graph of thoughts: Solv-
ing elaborate problems with large language models.
arXiv preprint arXiv:2308.09687 .
Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen,
and Denny Zhou. 2023. Large language models as
tool makers. arXiv preprint arXiv:2305.17126 .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluating
large language models trained on code.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W Cohen. 2022. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588 .
Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu
Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong,
Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer,
et al. 2022. Binding language models in symbolic
languages. In The Eleventh International Conference
on Learning Representations .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
Cleo Condoravdi, Dick Crouch, Valeria de Paiva, Rein-
hard Stolle, and Daniel G. Bobrow. 2003. Entailment,
intensionality and text understanding. In Proceedings
of the HLT-NAACL 2003 Workshop on Text Meaning ,
pages 38–45.
Ona de Gibert, Naiara Perez, Aitor García-Pablos, and
Montse Cuadros. 2018. Hate speech dataset from
a white supremacy forum. In Proceedings of the
2nd Workshop on Abusive Language Online (ALW2) ,

--- PAGE 11 ---
pages 11–20, Brussels, Belgium. Association for
Computational Linguistics.
Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng,
and Tushar Khot. 2023. Chain-of-thought hub: A
continuous effort to measure large language models’
reasoning performance.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2023. Pal: Program-aided language
models. In International Conference on Machine
Learning , pages 10764–10799. PMLR.
Jiaxin Ge, Hongyin Luo, Yoon Kim, and James Glass.
2023. Entailment as robust self-learner. arXiv
preprint arXiv:2305.17197 .
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021a. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021b. Did Aris-
totle Use a Laptop? A Question Answering Bench-
mark with Implicit Reasoning Strategies. Transac-
tions of the Association for Computational Linguis-
tics (TACL) .
Yi Hu, Haotong Yang, Zhouchen Lin, and Muhan Zhang.
2023. Code prompting: a neural symbolic method
for complex reasoning in large language models.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2023. Large lan-
guage models are zero-shot reasoners.
Ziyang Li, Jiani Huang, and Mayur Naik. 2023. Scallop:
A language for neurosymbolic programming. Pro-
ceedings of the ACM on Programming Languages ,
7(PLDI):1463–1487.
Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,
Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,
Shaoguang Mao, Yun Wang, Linjun Shou, Ming
Gong, and Nan Duan. 2023a. TaskMatrix.AI: Com-
pleting Tasks by Connecting Foundation Models with
Millions of APIs. arXiv:2303.16434 .
Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,
Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,
Shaoguang Mao, Yun Wang, Linjun Shou, Ming
Gong, and Nan Duan. 2023b. Taskmatrix.ai: Com-
pleting tasks by connecting foundation models with
millions of apis.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
Truthfulqa: Measuring how models mimic human
falsehoods. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 3214–3252.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,
Delip Rao, Eric Wong, Marianna Apidianaki, and
Chris Callison-Burch. 2023. Faithful chain-of-
thought reasoning.
Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-
foros Nalmpantis, Ram Pasunuru, Roberta Raileanu,
Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,
Asli Celikyilmaz, Edouard Grave, Yann LeCun, and
Thomas Scialom. 2023. Augmented language mod-
els: a survey.
Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Jus-
tifying recommendations using distantly-labeled re-
views and fine-grained aspects. In Proceedings of
the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 188–197.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,
Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma,
David Luan, Charles Sutton, and Augustus Odena.
2021. Show your work: Scratchpads for intermediate
computation with language models.
Abiola Obamuyide and Andreas Vlachos. 2018. Zero-
shot relation classification as textual entailment. In
Proceedings of the First Workshop on Fact Extraction
and VERification (FEVER) , pages 72–78, Brussels,
Belgium. Association for Computational Linguistics.
OpenAI. 2022. Introducing chatgpt.
OpenAI. 2023. Gpt-4 technical report.
Liangming Pan, Alon Albalak, Xinyi Wang, and
William Yang Wang. 2023. Logic-lm: Empower-
ing large language models with symbolic solvers
for faithful logical reasoning. arXiv preprint
arXiv:2305.12295 .
Bhargavi Paranjape, Scott Lundberg anbd Sameer Singh,
Hannaneh Hajishirzi, Luke Zettlemoyer, and
Marco Tulio Ribeiro. 2023. ART: Automatic multi-
step reasoning and tool-use for large language mod-
els.arXiv:2303.09014 .
Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen,
Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,
Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,
Huadong Wang, Cheng Qian, Runchu Tian, Kunlun
Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen
Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,
Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,
Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,
Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng
Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and
Maosong Sun. 2023. Tool learning with foundation
models.

--- PAGE 12 ---
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle,
Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom
Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wen-
han Xiong, Alexandre Défossez, Jade Copet, Faisal
Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier,
Thomas Scialom, and Gabriel Synnaeve. 2023. Code
llama: Open foundation models for code.
Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-
sky, Noah A Smith, and Yejin Choi. 2019. Social bias
frames: Reasoning about social and power implica-
tions of language. arXiv preprint arXiv:1911.03891 .
Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,
Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-
textualized affect representations for emotion recog-
nition. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing ,
pages 3687–3697, Brussels, Belgium. Association
for Computational Linguistics.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
Language models can teach themselves to use tools.
arXiv preprint arXiv:2302.04761 .
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
InProceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing , pages
1631–1642, Seattle, Washington, USA. Association
for Computational Linguistics.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
et al. 2023. Beyond the imitation game: Quanti-
fying and extrapolating the capabilities of language
models.
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny
Zhou, , and Jason Wei. 2022. Challenging big-bench
tasks and whether chain-of-thought can solve them.
arXiv preprint arXiv:2210.09261 .
Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,
Qiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca:
Generalized tool learning for language models with
3000 simulated cases.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. Glue:
A multi-task benchmark and analysis platform for
natural language understanding. In Proceedings ofthe 2018 EMNLP Workshop BlackboxNLP: Analyz-
ing and Interpreting Neural Networks for NLP , pages
353–355.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, and Denny Zhou. 2022. Self-consistency im-
proves chain of thought reasoning in language mod-
els.arXiv preprint arXiv:2203.11171 .
Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
Transactions of the Association for Computational
Linguistics , 7:625–641.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903 .
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L Griffiths, Yuan Cao, and Karthik
Narasimhan. 2023a. Tree of thoughts: Deliberate
problem solving with large language models. arXiv
preprint arXiv:2305.10601 .
Yao Yao, Zuchao Li, and Hai Zhao. 2023b. Be-
yond chain-of-thought, effective graph-of-thought
reasoning in large language models. arXiv preprint
arXiv:2305.16582 .
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 .

--- PAGE 13 ---
A Additional Examples
A detailed comparison between NLEP and
ChatGPT-4 is shown in Figure 5. An example of
generated NLEP for StrategyQA problem is out-
lined in Figure 6.
B Additional Results and Analysis on
Math and Symbolic Reasoning
We present the detailed experimental results of
math and symbolic reasoning tasks in Tables 8 to
9, with execution failure analysis in Table 11. The
significance test analysis is outlined in Table 10.
GPT Results . We report the results of Table 2
with more details in Table 8. The effect of extra re-
tries described in Section 4 is highlighted with ( →).
The detailed experimental results on the Game of
24 benchmark is listed in Table 9.
Significance Test . We report the significance
test in Table 10 for task-general PoT and NLEP
prompted GPT-4 results. Due to API cost hamper,
we run significance test using bootstrap test:
https://github.com/neubig/util-scripts/
blob/master/paired-bootstrap.py . The
hyperparameters are configured to the default set-
ting: num_samples=10000, sample_ratio=0.5
Besides, to make the results more reliable and
reproducible, we performed the major experiments
using a temperature of 0 as reported in Table 8,
and assessed NLEP across a diverse range of tasks
to show its general capabilities.
Results with Different Fundation Models. To
investigate the effect of NLEP on different large
language models, we report the results with Claude-
22and CodeLlama (Rozière et al., 2023) in Ta-
ble 7. Following the guidance of the instruction-
following models3, we employ a chat session to
include task-specific and task-general prompts as
previous turns by interleaving the “user" and “assis-
tant" messages with a system message “Provide an-
swers in Python" at the beginning for CodeLlama-
7b-Instruct and CodeLlama-13b-Instruct. Hence,
we only treat bug-free Python programs that have
the desired results after execution as correct an-
swers, regardless of natural language outputs since
we explicitly prompt CodeLlama to generate the an-
swer in Python. Unlike the prominent performance
of GPT-4, the positive impact of NLEP with CodeL-
lama is diminished due to the much smaller model
size and greatly reduced programming capability.
2https://www.anthropic.com/api
3https://github.com/facebookresearch/codellamaAlthough NLEP prompting outperforms the task-
general PoT by a large margin on Chinese Remain-
der Theorem and Scheduling Meeting benchmarks,
a non-negligible performance gap is observed be-
tween NLEP and task-specific PoT on most tasks.
We notice a decline or performance on Scheduling
Meeting bechmark of 13b model over 7b. Interest-
ingly, we found that CodeLlama-13b prefers to use
tools (built-in library) more for solving Scheduling
Meeting problems while CodeLlama-7b tends to
do it directly “by hand”. However, the tool use of a
less-proficient model may result in more possible
execution errors. For instance, 113 over 125 exe-
cution errors (total 250 test cases) of CodeLlama-
13b is Error: ‘datetime.time’ object has
no attribute ‘overlaps’ while CodeLlama-7b
only has 3 execution errors. The performance with
Claude-2 is more prominent, with NLEP demon-
strates superior performance on Dyck Language,
Scheduling Meeting and Chinese Remainder Theo-
rem benchmarks.
To further investigate the benefits of NLEP, we
fine-tune a CodeLlama-7b (Rozière et al., 2023)
model using NLEP-style instances, resulting in
a variant that we term NLEP-CodeLlama. Note
that our training corpus does not include specific
evaluation tasks. During the evaluation phase,
we adopt zero-shot prompting strategy, where the
model is provided with only test instances without
in-context demonstrations. As presented in Table
7(b), zero-shot NLEP-CodeLlama exhibits consis-
tent performance improvement on 5 of 6 tasks. The
only exception is the Chinese Remainder Theo-
rem benchmark, which is notably more complex in
nature. Remarkably, zero-shot NLEP-CodeLlama
demonstrates superior performance on Word Sort-
ing benchmark when compared to task-general CoT
prompted GPT-3.5-Turbo, and outperforms NLEP
prompted GPT-3.5-Turbo on Tracking Shuffled Ob-
jects (7) benchmark, despite a considerably lower
parameter size.
Limitation of GSM-Hard Benchmark. We
opted for GSM-Hard (Gao et al., 2023) to mitigate
potential data contamination from GSM8K. The
numbers in GSM-Hard are large, less likely to have
been encountered by LLMs during training, and
hence augmenting the task complexity. However,
since GSM-Hard is automatically constructed by
replacing small numbers in GSM-8K with large
ones, there are peculiar cases where sometimes
models may refuse to reply or automatically solve
it. An example is as follows: the original ques-

--- PAGE 14 ---
Tracking Shuffled
Objects (7)Dyck
LanguageWord
SortingChinese
Remainder TheoremScheduling
MeetingGSM-Hard
(a) Task-Specific Prompting: GPT-4
CoT 100.0 63.6†90.9†0.0†55.6†57.4
PoT 100.0 60.8 100.0 100.0 75.2 74.1
LATM 100.0 87.5†99.1†100.0†100.0†-
(b) Task-General Prompting: GPT-4
CoT 81.2 39.6 84.4 0.0 82.8 54.9
PoT 98.4 66.4 99.6 76.4 (→84.4) 84.4 (→85.2) 69.3
NLEP (Ours) 100.0 91.6 99.6 85.2 (→97.2) 77.6 (→93.2) 67.7
(c) Task-Specific Prompting: GPT-3.5-Turbo
CoT 68.0 20.4†59.2†0.0†18.9†45.0
PoT 6.8 28.4 100.0 100.0 33.6 63.4
(d) Task-General Prompting: GPT-3.5-Turbo
CoT 51.2 38.0 75.2 0.0 39.6 42.8
PoT 88.4 4.0 100.0 58.4 (→72.4) 46.8 ( →49.2) 39.0 ( →52.2)
NLEP (Ours) 74.4 7.2 99.6 94.8 ( →96.4) 75.2 (→85.6) 50.9 (→54.1)
Table 8: Performance on six reasoning tasks.†stands for results from LATM (Cai et al., 2023). Results with†and of LATM are
reported on the last 240 instances with the first 10 instances as training and validation sets for tool making according to LATM’s
design. LATM is not appropriate for GSM-Hard benchmark as it is hard to derive a generally applicable tool function for all
test cases. We mainly experiment LATM with GPT-4 as the tool maker since (Cai et al., 2023) found that GPT-3.5 fails in all 5
trials on hard tasks like Tracking Shuffled Objects (5). If the generated tool is not general enough or only suitable for training
samples, the tool using phase will fail. We perform experiments using GPT-4 and GPT-3.5-Turbo with a temperature of 0 for all
settings except PoT and NLEP on GSM-Hard in (b) which use a temperature of 0.5 to increase the sampling diversity. Since
task-general PoT and NLEP lack task-specific programming instruction, they may generate non-executable Python programs.
We select some settings and give each instance failed at execution up to three additional trials with temperature=0.4 to diversify
the possible outputs. No label leakage is involved in this process as only the success of execution is used as a judgement. We
report the results with these extra retries on execution failures in ( →). The highest score among each sub-table (a), (b), (c) and
(d) is underlined and the best result for each task is in bold .
settingTask-Specific Task-General
IO CoTIO
(best of 100)CoT
(best of 100)ToT
(b=1)ToT
(b=5)PoT NLEP (ours)
Game of 24 7.3†4.0†33.0†49.0†45.0†74.0†52 (→52) 63 ( →66)
Table 9: Performance on Game of 24 benchmark.†stands for results from (Yao et al., 2023a).
tion in GSM8K is “ Claire makes a 3 egg omelet
every morning for breakfast. How many dozens
of eggs will she eat in 4 weeks? ” The number 3
is replaced with 6022727 and the corresponding
question in GSM-Hard is “ Claire makes a 6022727
egg omelet every morning for breakfast. How many
dozens of eggs will she eat in 4 weeks? ” The output
of task-general PoT aligns with the ground-truth,
which is 6022727 ×7×4/12 = 14053029 .667,
a decimal number. However, GPT-4 with NLEP
prompting regards the number of dozens should
be an integer, and consequently returns the result
as6022727 ×7×4//12 = 14053029 . This gives
additional challenges in evaluation, and we thus
mainly evaluate the former five reasoning bench-
marks using more models in Table 7, which might
serve as more reliable performance indicators.Execution Failure Analysis. We present the
execution failure statistics of code-based reasoning
strategies in Table 11. The effect of extra retries de-
scribed in Section 4 is highlighted with ( →). Note
that different from task-specific PoT with demon-
strations showing how to return the desired outputs
in Python program, e.g.,
# Python code , r e t u r n ans
A l i c e = " s t r i k e r "
Bob = " r i g h t winger "
C l a i r e = " l e f t winger "
Dave = " benchwarmer "
Eve = " g o a l k e e p e r "
Fred = " c e n t e r m i d f i e l d e r "
G e r t r u d e = " c h e e r l e a d e r "
Eve , C l a i r e = C l a i r e , Eve
G er tr u de , A l i c e = Alice , G e r t r u d e
Fred , Bob = Bob , Fred
Dave , Fred = Fred , Dave
Fred , Bob = Bob , Fred
Bob , Eve = Eve , Bob
C l a i r e , A l i c e = Alice , C l a i r e
ans = G e r t r u d e

--- PAGE 15 ---
Tasks/EvaluationAccuracy Significance Test
PoT NLEP NLEP win/lose/tie 95% Confidence Interval P-value
Tracking Shuffled Objects (7) 98.4 100.0 86.8/0.0/13.2[1.000,1.000]
[0.960,1.000]0.132
Dyck Languages 66.4 91.6 100.0/0.0/0.0[0.864,0.960]
[0.576,0.744]0.000
Word Sorting 99.6 99.6 0.0/0.0/100.0[0.984,1.000]
[0.984,1.000]/
Chinese Remainder Theorem 84.4 97.2 100.0/0.0/0.0[0.944,1.000]
[0.776,0.904]0.000
Scheduling Meeting 85.2 93.2 98.5/0.7/0.8[0.888,1.976]
[0.784,0.912]0.015
GSM-Hard 69.3 67.7 0.92/88.8/20.0[0.642,0.712]
[0.657,0.728]PoT better 0.112
Table 10: We report the significance test results for task-general PoT and NLEP prompted GPT-4 in Table 2. Due to
API cost hamper, we run significance test using bootstrap test: https://github.com/neubig/util-scripts/blob/master/
paired-bootstrap.py . The hyperparameters are configured to the default setting: num_samples=10000, sample_ratio=0.5 .
GPT-4 GPT-3.5-Turbo
Task-Specific Task-General Task-Specific Task-General
PoT PoT NLEP PoT PoT NLEP
Track Shuffled Objects (7) 0 0 0 233 26 24
Dyck Language 16 24 10 32 81 26
Word Sort 0 0 0 0 0 0
Chinese Remainder Theorem 0 32 (→0) 37 ( →6) 0 46 (→7) 4 ( →0)
Scheduling Meeting 0 3 (→0) 43 ( →0) 2 15 (→2) 36 ( →0)
GSM-Hard 17 6 8 31 464 (→145) 95 ( →13)
Table 11: Execution failure statistics on six math and symbolic reasoning tasks. Results with extra reties are reported in ( →). For
task-specific PoT, we report the execution error statistics with None as the return value of safe_execute() function following
the source code of (Chen et al., 2022): https://github.com/wenhuchen/Program-of-Thoughts/blob/main/tool.py . It
includes instances where the generated programs do not contain the required line of code: ans={desired_result} , which are
explicitly required in the instruction and given in the prompt demonstration. Under this scenario, we cannot capture the execution
results of task-specific PoT.
we need to design rules to extract the target an-
swers from the execution results of task-general
PoT and NLEP since they are allowed to generate
free-from outputs. For example, given the gener-
ated programs,
# S te p 1 : I m p o r t n e c e s s a r y b u i l t − i n l i b r a r i e s
# No need t o i m p o r t
# S te p 2 : D e f i n e n e c e s s a r y f u n c t i o n s t h a t g e n e r a l l y
s o l v e t h i s t y p e of problem
d e f s w a p _ p o s i t i o n s ( p o s i t i o n s , swaps ) :
f o r swap i n swaps :
p o s i t i o n s [ swap [ 0 ] ] , p o s i t i o n s [ swap [ 1 ] ] =
p o s i t i o n s [ swap [ 1 ] ] , p o s i t i o n s [ swap [ 0 ] ]
r e t u r n p o s i t i o n s
# S te p 3 : D e f i n e c o n s t a n t v a r i a b l e s f o r t h e t a s k
p o s i t i o n s = {
" A l i c e " : " s t r i k e r " ,
" Bob " : " r i g h t winger " ,
" C l a i r e " : " l e f t winger " ,
" Dave " : " benchwarmer " ,
" Eve " : " g o a l k e e p e r " ,
" Fred " : " c e n t e r m i d f i e l d e r " ,
" G e r t r u d e " : " c h e e r l e a d e r "
}
swaps = [
( " Eve " , " C l a i r e " ) ,
( " G e r t r u d e " , " A l i c e " ) ,( " Fred " , " Bob " ) ,
( " Dave " , " Fred " ) ,
( " Fred " , " Bob " ) ,
( " Bob " , " Eve " ) ,
( " C l a i r e " , " A l i c e " )
]
# S te p 4 : P r i n t an answer i n n a t u r a l l a n g u a g e .
f i n a l _ p o s i t i o n s = s w a p _ p o s i t i o n s ( p o s i t i o n s , swaps )
p r i n t ( f " At t h e end of t h e match , G e r t r u d e i s p l a y i n g
{ f i n a l _ p o s i t i o n s [ ’ Ge r tr ud e ’ ] } . " )
we need to extract “striker", the target answer,
from the execution results “At the end of the match,
Gertrude is playing striker.".
Although task-specific PoT explicitly instructs
the model to generate a required line of code
ans={desired_result} to capture the final result
by adding
# Python code , r e t u r n ans
in the instruction and explicitly provides task-
specific examples in the prompt, e.g.,
# Python code , r e t u r n ans
. . . . . .
ans = G e r t r u d e

--- PAGE 16 ---
we found that GPT models sometimes fail to
accomplish the requirement. On Tracking Shuf-
fled Objects (7) benchmark, only 17 programs
out of 250 test cases generated by GPT-3.5 suc-
cessfully contain the required line of code, i.e.,
ans={desired_result} , which explains the huge
number of failures (233). In addition to already
having task-specific programming examples, the
inability to distinguish between the erroneous pro-
grams and lack of required line of code is another
reason why we do not apply the error retries on
task-specific PoT.
NLEP v.s. Task-general PoT Compared to PoT,
NLEP offers structured and modularized Python
blocks. This disentanglement of knowledge ex-
traction from problem solving facilitates enhanced
interpretability and controllability. In contrast, PoT
may interwine different components, particularly
blending knowledge and reasoning steps. An il-
lustrative example is provided below: the question
from GSM-Hard is Toula went to the bakery and
bought various types of pastries. She bought 3
dozen donuts which cost 5804528 per dozen, 2
dozen mini cupcakes which cost 80 per dozen, and
6 dozen mini cheesecakes for 55 per dozen. How
much was the total cost? The generated Python
block of NLEP is
# S te p 1 : I m p o r t n e c e s s a r y b u i l t − i n l i b r a r i e s
# No need t o i m p o r t
# S te p 2 : D e f i n e n e c e s s a r y f u n c t i o n s t h a t g e n e r a l l y
s o l v e t h i s t y p e of problem
d e f c a l c u l a t e _ t o t a l _ c o s t ( i t e m s ) :
t o t a l _ c o s t = 0
f o r it em i n i t e m s :
t o t a l _ c o s t += ite m [ ’ q u a n t i t y ’ ] *it em [ ’ p r i c e
’ ]
r e t u r n t o t a l _ c o s t
# S te p 3 : D e f i n e c o n s t a n t v a r i a b l e s f o r t h e t a s k
i t e m s = [
{" name " : " d o n u t s " , " q u a n t i t y " : 3 , " p r i c e " :
5804528} ,
{" name " : " mini c u p c a k e s " , " q u a n t i t y " : 2 , " p r i c e " :
80} ,
{" name " : " mini c h e e s e c a k e s " , " q u a n t i t y " : 6 , "
p r i c e " : 55} ,
]
# S te p 4 : P r i n t an answer i n n a t u r a l l a n g u a g e .
t o t a l _ c o s t = c a l c u l a t e _ t o t a l _ c o s t ( i t e m s )
p r i n t ( f " The t o t a l c o s t of t h e p a s t r i e s t h a t Toula
bought from t h e b a k e r y i s ${ t o t a l _ c o s t } . T hi s
i n c l u d e s t h e c o s t of { i t e m s [ 0 ] [ ’ q u a n t i t y ’ ] }
dozen { i t e m s [ 0 ] [ ’ name ’ ] } a t ${ i t e m s [ 0 ] [ ’ p r i c e
’ ] } p e r dozen , { i t e m s [ 1 ] [ ’ q u a n t i t y ’ ] } dozen {
i t e m s [ 1 ] [ ’ name ’ ] } a t ${ i t e m s [ 1 ] [ ’ p r i c e ’ ] } p e r
dozen , and { i t e m s [ 2 ] [ ’ q u a n t i t y ’ ] } dozen { i t e m s
[ 2 ] [ ’ name ’ ] } a t ${ i t e m s [ 2 ] [ ’ p r i c e ’ ] } p e r dozen
. \ nThe c o r r e c t answer i s ${ t o t a l _ c o s t } . " )
The code returned by PoT is
d o n u t s = 3 *5804528
c u p c a k e s = 2 *80
c h e e s e c a k e s = 6 *55
t o t a l _ c o s t = d o n u t s + c u p c a k e s + c h e e s e c a k e sNum cola emotion amazon hsd sbic
Internal Node 4 7 8 4 3
Leaf Node 4 8 10 5 4
Label 2 6 5 2 3
Table 12: Statistics of NLEP generated decision trees for each
benchmark in Table 6.
p r i n t ( t o t a l _ c o s t )
ans = G e r t r u d e
C Additional Results on Text
Classification
Statistics of Decision Trees. We list the statics
of the generated decision tree by NLEP prompting
in Table 12. Each internal decision node in the
generated tree has two branches, “yes” and “no” ,
indicating whether it satisfies the requirement or
not.
Classification Results on SST2 Benchmark.
Since we use the trees derived from SST2 to prompt
the LLM for the classification tasks, it would be
inappropriate to use these examples for SST2 itself.
For SST2, we thus use an automatically generated
decision tree for the CoLA task to prompt GPT-4
to generate a new tree for SST2. As shown in Table
13, the automatically generated tree matches the
performance of the SST2 decision tree created by
the authors.
SST2 Performance
Model-Free 66.17
RoBERTa-Manual 83.03
RoBERTa-Automatic 87.36
DeBERTa-Manual 84.06
DeBERTa-Automatic 93.49
Table 13: Performance of manually crafted vs. generated
decision trees on SST2.

--- PAGE 17 ---
  # Step 1: Import necessary built-in libraries # No need to import  # Step 2: Define knowledge about the secretary-general of United Nations secretaries_general = {     "Trygve Lie":              "Europe",     "Dag Hammarskjöld":        "Europe",     "U Thant":                 "Asia",     "Kurt Waldheim":           "Europe",     "Javier Pérez de Cuéllar": "South America",     "Boutros Boutros-Ghali":   "Africa",     "Kofi Annan":              "Africa",     "Ban Ki-moon":             "Asia",     "António Guterres":        "Europe" } # Step 3: Define a function that counts the number of secretary-general not from Europe def count_non_european(secretaries_general):     return len([region for region in secretaries_general.values() if region != "Europe"]) # Step 4: Print an answer in natural language non_european_count = count_non_european(secretaries_general) print(f"Out of all the Secretaries-General of the United Nations, {non_european_count} of them are not from Europe.")ToolUsingStructuredknowledge
FunctiondefinitionPrintanswerOut of all the Secretaries-General of the United Nations, 5 of them are not from Europe.Stdout:Instruction:How many secretaries-general of United Nations are not from Europe?
102030405060708090100Accuracy (%)100
6040GPT-4: NLEPGPT-4: ChatGPTGPT-4: OpenAI API (T = 0.5)
Answering the question withfive independent generationsNLEP generated by GPT-4:
User
Wrong answerFigure 5: NLEP answering a question which requires numeric reasoning of structured knowledge. ChatGPT-4 code interpreter
(currently the advanced data analysis option) constantly prefers to answer this question with plain natural language.

--- PAGE 18 ---
# Step 1: Import necessary built-in libraries
# No need to import
# Step 2: Define dictionaries storing the rappers' information
rappers = {
    "Lil Wayne" : {
        "real_name" : "Dwayne Michael Carter Jr." ,
        "grammy_awards" : 5
    },
    "Jay-Z": {
        "real_name" : "Shawn Corey Carter" ,
        "grammy_awards" : 23
    }
}
# Step 3: Define a function that checks if a rapper has over quadruple the Grammy awards of another
def check_quadruple_awards (rappers, rapper1, rapper2):
    if rappers[rapper2][ "grammy_awards" ] > 4 * rappers[rapper1][ "grammy_awards" ]:
        return "yes"
    else:
        return "no"
# Step 4: Print an answer using the defined function and varible
answer = check_quadruple_awards(rappers, "Lil Wayne" , "Jay-Z")
print(answer)
yes(b) StrategyQA: Lil Wayne similar real name rapper has over quadruple Wayne’s Grammy awards?
Stdout:NLEP generated by GPT-4
… As of now, Lil Wayne has won 5 Grammy awards, while Lil Twist has not won any. Thus, 
Lil Twist does not have over quadruple Wayne’s Grammy awards. So the answer is no.Few-shot CoT:(a) Dyck Langauge: {{}((<>)
# Step 1: Import necessary built-in libraries
# No need to import
# Step 2: Define necessary functions that generally solve this type of problem
def complete_sequence (sequence ):
    open_brackets = [ '{', '(', '<']
    close_brackets = [ '}', ')', '>']
    stack = []
    for char in sequence:
        if char in open_brackets:
            stack.append(char)
        elif char in close_brackets:
            if len(stack) == 0:
                return "Invalid sequence"
            else:
                stack.pop()
    while len(stack) != 0:
        open_bracket = stack.pop()
        sequence += close_brackets[open_brackets.index(open_bracket)]
    return sequence
# Step 3: Define constant variables for the task
sequence = '{{}((<>)'
# Step 4: Print an answer in natural language.
completed_sequence = complete_sequence(sequence)
print(f"To complete the sequence, we need to close all the open brackets in the correct order. The completed 
sequence is {completed_sequence} .")NLEP generated by GPT-4
Stdout: To complete the sequence, we need to close all the open brackets in the correct order. The 
completed sequence is {{}((<>))} .
Task-General PoT: {{}((<>)({Figure 6: NLEP generated for solving StrategyQA problem. The instruction is “Answer the question with yes or no. ”

--- PAGE 19 ---
D Prompts for Task-General Strategies
D.1 Prompts for Table 2 and 4
We list the prompts for the task-general chain-of-
thought (CoT), our implementation of program-of-
thoughts (PoT) and the proposed natural language
embedded programs (NLEP) strategies in the fol-
lowing code segments. They share the same two
NLEP examples (one is for natural language rea-
soning and the other is for mathematical reasoning)
but with different forms of intermediate reasoning
steps (e.g., code, text etc.) to evaluate the average
performance of different strategies.
Prompt for task-general chain-of-thought
(CoT) in Table 2 and 4. The detailed intermediate
natural language reasoning chains are generated by
prompting GPT-4 given the input and target output.
’ ’ ’
Answer t h e problem based on t h e g i v e n i n s t r u c t i o n
and i n p u t .
### I n s t r u c t i o n : I d e n t i f y t h e odd one o u t .
### I n p u t : T w i t t e r , I n s t a g r a m , Telegram
### Answer :
Let ’ s t h i n k s t e p by s t e p .
1 . S t a r t by u n d e r s t a n d i n g t h e t a s k i n s t r u c t i o n . The
t a s k i s t o i d e n t i f y
t h e odd one o u t from a g i v e n l i s t .
2 . Look a t t h e i n p u t . The i n p u t c o n s i s t s of t h r e e
i t e m s : T w i t t e r ,
I n s t a g r a m , and Telegram .
3 . I d e n t i f y what t h e s e i t e m s a r e . T w i t t e r and
I n s t a g r a m a r e s o c i a l media
p l a t f o r m s , w h i l e Telegram i s a messaging app .
4 . Compare t h e i t e m s t o f i n d t h e odd one o u t .
T w i t t e r and I n s t a g r a m a r e
p r i m a r i l y used f o r s h a r i n g i n f o r m a t i o n , images , and
v i d e o s . On t h e o t h e r
hand , Telegram i s mainly used f o r i n s t a n t messaging
and voice −over −IP
s e r v i c e .
5 . Determine t h e odd one o u t based on t h e comparison
. In t h i s case ,
Telegram i s t h e odd one o u t b e c a u s e i t s e r v e s a
d i f f e r e n t p r i m a r y
f u n c t i o n compared t o T w i t t e r and I n s t a g r a m .
6 . F o r m u l a t e t h e t a r g e t o u t p u t . The t a r g e t o u t p u t
s h o u l d c l e a r l y s t a t e
t h a t Telegram i s t h e odd one o u t and p r o v i d e t h e
r e a s o n why i t i s so . The
r e a s o n b e i n g t h a t T w i t t e r and I n s t a g r a m a r e s o c i a l
media p l a t f o r m s mainly
f o r s h a r i n g i n f o r m a t i o n , images , and v i d e o s w h i l e
Telegram i s a cloud −
based i n s t a n t messaging and voice −over −IP s e r v i c e .
The c o r r e c t answer i s Telegram .
### I n s t r u c t i o n : Use t h e g i v e n d a t a t o c a l c u l a t e t h e
median .
### I n p u t : [ 2 , 3 , 7 , 8 , 10]
### Answer :
Let ’ s t h i n k s t e p by s t e p .
1 . S t a r t by u n d e r s t a n d i n g t h e t a s k , which i s t o
c a l c u l a t e t h e median of a
g i v e n d a t a s e t . The median i s t h e middle v a l u e i n a
s o r t e d , a s c e n d i n g or
d e s c e n d i n g , l i s t of numbers .
2 . Look a t t h e g i v e n i n p u t , which i s a l i s t of
numbers : [ 2 , 3 , 7 , 8 , 1 0 ] .
3 . N o t i c e t h a t t h e l i s t i s a l r e a d y s o r t e d i n
a s c e n d i n g o r d e r . I f i t wasn ’
t , t h e f i r s t s t e p would be t o s o r t i t .
4 . U n d e r s t a n d t h a t t o f i n d t h e median , we need t o
f i n d t h e middle v a l u e .I f t h e l i s t has an odd number of o b s e r v a t i o n s , t h e
median i s t h e middle
number . I f t h e l i s t has an even number of
o b s e r v a t i o n s , t h e median i s t h e
a v e r a g e of t h e two middle numbers .
5 . Count t h e number of v a l u e s i n t h e l i s t . There a r e
5 v a l u e s , which i s
an odd number , so t h e median w i l l be t h e middle
v a l u e .
6 . I d e n t i f y t h e middle v a l u e . S i n c e t h e r e a r e 5
v a l u e s , t h e middle v a l u e
i s t h e 3 rd v a l u e when c o u n t i n g from e i t h e r end .
7 . Find t h e 3 rd v a l u e i n t h e l i s t , which i s 7 .
8 . Conclude t h a t t h e median of t h e g i v e n d a t a s e t i s
7 .
The c o r r e c t answer i s 7 .
’ ’ ’
Prompt for task-general NLEP in Table 2 and
4.The intermediate program reasoning chains are
generated by prompting GPT-4 given the input and
target output.
Write a bug − f r e e Python program t h a t can g e n e r a t e
t h e answer t o t h e g i v e n i n s t r u c t i o n when
c o r r e c t l y e x e c u t e d .
### I n s t r u c t i o n : I d e n t i f y t h e odd one o u t .
### I n p u t : T w i t t e r , I n s t a g r a m , Telegram
### Python program :
‘ ‘ ‘
# S te p 1 : I m p o r t n e c e s s a r y b u i l t − i n l i b r a r i e s
from c o l l e c t i o n s i m p o r t O r d e r e d D i c t
# S te p 2 : D e f i n e n e c e s s a r y f u n c t i o n s t h a t g e n e r a l l y
s o l v e t h i s t y p e of problem
d e f f i n d _ o d d _ o n e _ o u t ( s e r v i c e s , i n p u t _ s e r v i c e s ) :
d e s c r i p t i o n s = [ s e r v i c e s [ s e r v i c e ] f o r s e r v i c e i n
i n p u t _ s e r v i c e s ]
f o r d e s c r i p t i o n i n d e s c r i p t i o n s :
i f d e s c r i p t i o n s . c o u n t ( d e s c r i p t i o n ) == 1 :
r e t u r n i n p u t _ s e r v i c e s [ d e s c r i p t i o n s . i n d e x
( d e s c r i p t i o n ) ]
r e t u r n None
# S te p 3 : D e f i n e c o n s t a n t v a r i a b l e s f o r t h e t a s k
s e r v i c e s = O r d e r e d D i c t ( [
( " T w i t t e r " , " a s o c i a l media p l a t f o r m mainly f o r
s h a r i n g i n f o r m a t i o n , images and v i d e o s " ) ,
( " I n s t a g r a m " , " a s o c i a l media p l a t f o r m mainly
f o r s h a r i n g i n f o r m a t i o n , images and v i d e o s
" ) ,
( " Telegram " , " a cloud − based i n s t a n t messaging
and voice −over −IP s e r v i c e " ) ,
] )
i n p u t _ s e r v i c e s = [ " T w i t t e r " , " I n s t a g r a m " , " Telegram
" ]
# S te p 4 : P r i n t an answer i n n a t u r a l l a n g u a g e .
odd_one_out = f i n d _ o d d _ o n e _ o u t ( s e r v i c e s ,
i n p u t _ s e r v i c e s )
i f odd_one_out :
o t h e r _ s e r v i c e s = [ s e r v i c e f o r s e r v i c e i n
i n p u t _ s e r v i c e s i f s e r v i c e != odd_one_out ]
p r i n t ( f " The odd one o u t i s { odd_one_out } . {
o t h e r _ s e r v i c e s [ 0 ] } and { o t h e r _ s e r v i c e s [ 1 ] }
a r e { s e r v i c e s [ o t h e r _ s e r v i c e s [ 0 ] ] } w h i l e {
odd_one_out } i s { s e r v i c e s [ odd_one_out ] } . \
nThe c o r r e c t answer i s { odd_one_out } . " )
‘ ‘ ‘
### I n s t r u c t i o n : Use t h e g i v e n d a t a t o c a l c u l a t e t h e
median .
### I n p u t : [ 2 , 3 , 7 , 8 , 10]
### Python program :
‘ ‘ ‘
# S te p 1 : I m p o r t n e c e s s a r y b u i l t − i n l i b r a r i e s
# No need t o i m p o r t
# S te p 2 : D e f i n e n e c e s s a r y f u n c t i o n s t h a t g e n e r a l l y
s o l v e t h i s t y p e of problem
d e f c a l c u l a t e _ m e d i a n ( d a t a ) :
d a t a . s o r t ( )
l e n g t h = l e n ( d a t a )

--- PAGE 20 ---
i f l e n g t h % 2 == 0 :
r e t u r n ( d a t a [ l e n g t h / / 2 ] + d a t a [ l e n g t h / / 2 −
1 ] ) / 2
e l s e :
r e t u r n d a t a [ l e n g t h / / 2 ]
# S te p 3 : D e f i n e c o n s t a n t v a r i a b l e s f o r t h e t a s k
d a t a = [ 2 , 3 , 7 , 8 , 10]
# S te p 4 : P r i n t an answer i n n a t u r a l l a n g u a g e .
median = c a l c u l a t e _ m e d i a n ( d a t a )
p r i n t ( f " To f i n d t h e median of a d a t a s e t , we need t o
a r r a n g e t h e d a t a i n a s c e n d i n g o r d e r and t h e n
f i n d t h e middle v a l u e . In t h i s case , t h e g i v e n
d a t a i s a l r e a d y a r r a n g e d i n a s c e n d i n g o r d e r .
S i n c e t h e r e a r e { l e n ( d a t a ) } v a l u e s i n t h e d a t a
s e t , t h e median w i l l be t h e middle value , which
i s t h e { l e n ( d a t a ) / / 2 + 1} rd v a l u e . Hence , t h e
median of t h e g i v e n d a t a s e t i s { median } . \ nThe
c o r r e c t answer i s { median } . " )
‘ ‘ ‘
Prompt for our task-general program-of-
thoughts (PoT) implementation in Table 2 and
4.The intermediate program reasoning chains are
obtained from NLEP. We keep the key implemen-
tation steps of Python programs but eliminate the
step-by-step programming instructions and detailed
natural language elaboration as an ablation.
Write a bug − f r e e Python program t h a t can g e n e r a t e
t h e answer t o t h e g i v e n i n s t r u c t i o n when
c o r r e c t l y e x e c u t e d .
### I n s t r u c t i o n : I d e n t i f y t h e odd one o u t .
### I n p u t : T w i t t e r , I n s t a g r a m , Telegram
### Python program :
‘ ‘ ‘
s o f t w a r e s = {
’ s o c i a l media ’ : [ ’ t w i t t e r ’ , ’ i n s t a g r a m ’ ] ,
’ communication ’ : [ ’ t e l e g r a m ’ ]
}
f o r genre , apps i n s o f t w a r e s . i t e m s ( ) :
i f l e n ( apps ) == 1 :
p r i n t ( apps [ 0 ] )
‘ ‘ ‘
### I n s t r u c t i o n : Use t h e g i v e n d a t a t o c a l c u l a t e t h e
median .
### I n p u t : [ 2 , 3 , 7 , 8 , 10]
### Python program :
‘ ‘ ‘
d a t a = [ 2 , 3 , 7 , 8 , 10]
d a t a . s o r t ( )
l e n g t h = l e n ( d a t a )
i f l e n g t h % 2 == 0 :
p r i n t ( ( d a t a [ l e n g t h / / 2 ] + d a t a [ l e n g t h / / 2 − 1 ] ) /
2)
e l s e :
p r i n t ( d a t a [ l e n g t h / / 2 ] )
‘ ‘ ‘
D.2 Prompts for Table 3
The Game of 24 task is much more challenging and
we replace the first example in Appendix D.1 with
a word sorting example to elicit stronger reasoning
ability.
Prompt for task-general NLEP in Table 3.
The intermediate program reasoning chains are gen-
erated by prompting GPT-4 given the input and
target output.
Write a bug − f r e e Python program t h a t can g e n e r a t e
t h e answer t o t h e g i v e n i n s t r u c t i o n when
c o r r e c t l y e x e c u t e d .### I n s t r u c t i o n : Arrange t h e f o l l o w i n g words t o make
t h e l o n g e s t p o s s i b l e word .
### I n p u t : the , had , not , been
### Python program :
‘ ‘ ‘
# S e c t i o n 1 : D e f i n e n e c e s s a r y f u n c t i o n s and
c a l c u l a t e i n t e r m e d i a t e v a r i a b l e s
d e f l o n g e s t _ w o r d ( words ) :
from i t e r t o o l s i m p o r t p e r m u t a t i o n s
a l l _ w o r d s = [ ’ ’ . j o i n ( p ) f o r p i n p e r m u t a t i o n s
( ’ ’ . j o i n ( words ) ) ]
a l l _ w o r d s . s o r t ( key= len , r e v e r s e =True )
wi th open ( ’ e n g l i s h _ w o r d s . t x t ’ ) as w o r d _ f i l e : #
Assuming you have a l i s t of e n g l i s h words
e n g l i s h _ w o r d s = s e t ( word . s t r i p ( ) . lower ( ) f o r
word i n w o r d _ f i l e )
f o r word i n a l l _ w o r d s :
i f word . lower ( ) i n e n g l i s h _ w o r d s :
r e t u r n word
r e t u r n None
# S e c t i o n 2 : D e f i n e c o n s t a n t v a r i a b l e s
words = [ " t h e " , " had " , " n o t " , " been " ]
# S e c t i o n 3 : I n s e r t v a r i a b l e s i n t e x t o u t p u t s u s i n g
f − s t r i n g s .
l o n g e s t = l o n g e s t _ w o r d ( words )
i f l o n g e s t :
p r i n t ( f " The l o n g e s t word t h a t can be made from
t h e l e t t e r s i n t h e words \ " { ’ , ’ . j o i n ( words
) } \ " i s \ " { l o n g e s t } \ " . " )
‘ ‘ ‘
### I n s t r u c t i o n : Use t h e g i v e n d a t a t o c a l c u l a t e t h e
median .
### I n p u t : [ 2 , 3 , 7 , 8 , 10]
### Python program :
‘ ‘ ‘
# S te p 1 : I m p o r t n e c e s s a r y b u i l t − i n l i b r a r i e s
# No need t o i m p o r t
# S te p 2 : D e f i n e n e c e s s a r y f u n c t i o n s t h a t g e n e r a l l y
s o l v e t h i s t y p e of problem
d e f c a l c u l a t e _ m e d i a n ( d a t a ) :
d a t a . s o r t ( )
l e n g t h = l e n ( d a t a )
i f l e n g t h % 2 == 0 :
r e t u r n ( d a t a [ l e n g t h / / 2 ] + d a t a [ l e n g t h / / 2 −
1 ] ) / 2
e l s e :
r e t u r n d a t a [ l e n g t h / / 2 ]
# S te p 3 : D e f i n e c o n s t a n t v a r i a b l e s f o r t h e t a s k
d a t a = [ 2 , 3 , 7 , 8 , 10]
# S te p 4 : P r i n t an answer i n n a t u r a l l a n g u a g e .
median = c a l c u l a t e _ m e d i a n ( d a t a )
p r i n t ( f " To f i n d t h e median of a d a t a s e t , we need t o
a r r a n g e t h e d a t a i n a s c e n d i n g o r d e r and t h e n
f i n d t h e middle v a l u e . In t h i s case , t h e g i v e n
d a t a i s a l r e a d y a r r a n g e d i n a s c e n d i n g o r d e r .
S i n c e t h e r e a r e { l e n ( d a t a ) } v a l u e s i n t h e d a t a
s e t , t h e median w i l l be t h e middle value , which
i s t h e { l e n ( d a t a ) / / 2 + 1} rd v a l u e . Hence , t h e
median of t h e g i v e n d a t a s e t i s { median } . " )
‘ ‘ ‘
Prompt for our task-general program-of-
thoughts (PoT) implementation in Table 3. The
intermediate program reasoning chains are ob-
tained from NLEP. We keep the key implemen-
tation steps of Python programs but eliminate the
step-by-step programming instructions and detailed
natural language elaboration as an ablation.
Write a bug − f r e e Python program t h a t can g e n e r a t e
t h e answer t o t h e g i v e n i n s t r u c t i o n when
c o r r e c t l y e x e c u t e d .
### I n s t r u c t i o n : Arrange t h e f o l l o w i n g words t o make
t h e l o n g e s t p o s s i b l e word .
### I n p u t : the , had , not , been
### Python program :

--- PAGE 21 ---
‘ ‘ ‘
d e f l o n g e s t _ w o r d ( words ) :
from i t e r t o o l s i m p o r t p e r m u t a t i o n s
a l l _ w o r d s = [ ’ ’ . j o i n ( p ) f o r p i n p e r m u t a t i o n s
( ’ ’ . j o i n ( words ) ) ]
a l l _ w o r d s . s o r t ( key= len , r e v e r s e =True )
wi th open ( ’ e n g l i s h _ w o r d s . t x t ’ ) as w o r d _ f i l e : #
Assuming you have a l i s t of e n g l i s h words
e n g l i s h _ w o r d s = s e t ( word . s t r i p ( ) . lower ( ) f o r
word i n w o r d _ f i l e )
f o r word i n a l l _ w o r d s :
i f word . lower ( ) i n e n g l i s h _ w o r d s :
r e t u r n word
r e t u r n None
words = [ " t h e " , " had " , " n o t " , " been " ]
l o n g e s t = l o n g e s t _ w o r d ( words )
i f l o n g e s t :
p r i n t ( l o n g e s t )
‘ ‘ ‘
### I n s t r u c t i o n : Use t h e g i v e n d a t a t o c a l c u l a t e t h e
median .
### I n p u t : [ 2 , 3 , 7 , 8 , 10]
### Python program :
‘ ‘ ‘
d a t a = [ 2 , 3 , 7 , 8 , 10]
d a t a . s o r t ( )
l e n g t h = l e n ( d a t a )
i f l e n g t h % 2 == 0 :
p r i n t ( ( d a t a [ l e n g t h / / 2 ] + d a t a [ l e n g t h / / 2 − 1 ] ) /
2)
e l s e :
p r i n t ( d a t a [ l e n g t h / / 2 ] )
‘ ‘ ‘
D.3 Prompts for NLEP in Table 5 and Figure
4
For experiments in TruthfulQA and VicunaQA,
we added the following example into the NLEP
prompts shown in Appendix D.1 to encourage gen-
erating more detailed responses:
# Write a bug − f r e e Python program t h a t can g e n e r a t e
t h e answer t o t h e g i v e n i n s t r u c t i o n when
c o r r e c t l y e x e c u t e d . Do n o t ask f o r u s e r i n p u t .
For r e a s o n i n g t a s k s , d e f i n e f u n c t i o n s f i r s t and
t h e n d e f i n e v a r i a b l e s . For knowledge i n t e n s i v e
t a s k s , d e f i n e v a r i a b l e s b e f o r e d e f i n i n g
f u n c t i o n s . Do n o t d e f i n e any v a r i a b l e t h a t
d i r e c t l y s t o r e s t h e f i n a l answer . I f t h e r e i s a
knowledge d e f i n i t i o n s t e p , use d i c t i o n a r i e s t o
s t o r e b ot h t h e knowledge and d e t a i l e d
e x p l a n a t i o n .
### I n s t r u c t i o n : D i s c u s s t h e c a u s e s of t h e G r e a t
D e p r e s s i o n
### I n p u t : None
### Python program :
‘ ‘ ‘
# S te p 1 : I m p o r t n e c e s s a r y b u i l t − i n l i b r a r i e s
# No need t o i m p o r t
# S te p 2 : D e f i n e d i c t i o n a r i e s s t o r i n g d e t a i l e d
knowledge a b o u t t h e g r a t d e p r e s s i o n
d e p r e s s i o n _ n a m e = " The G r e a t D e p r e s s i o n "
d e p r e s s i o n _ p e r i o d = "1929 −1939"
d e p r e s s i o n _ c o u n t r i e s = " t h e U n i t e d S t a t e s and
c o u n t r i e s around t h e world "
d e p r e s s i o n _ c a u s e s = {
" Stock Market Crash of 1 9 2 9 " : " In October of
1929 , t h e s t o c k market e x p e r i e n c e d a
s i g n i f i c a n t f a l l t h a t wiped o u t m i l l i o n s of
i n v e s t o r s . Thi s e v e n t i s c o n s i d e r e d by
many t o be t h e i n i t i a l t r i g g e r of t h e G r e a t
D e p r e s s i o n . " ,
" O v e r p r o d u c t i o n " : " During t h e 1920 s , many
i n d u s t r i e s produced more goods t h a n
consumers wanted or c o u l d a f f o r d . T hi s
u l t i m a t e l y l e d t o a d e c l i n e i n demand f o r
goods , c a u s i n g j o b l o s s , lower wages , and
b u s i n e s s f a i l u r e . " ," High T a r i f f s and War Debts " : " P r o t e c t i o n i s t
t r a d e p o l i c i e s i n t h e form of hi gh t a r i f f s
l e d t o a d e c l i n e i n g l o b a l t r a d e , as o t h e r
c o u n t r i e s r e t a l i a t e d w ith t a r i f f s of t h e i r
own . A d d i t i o n a l l y , many c o u n t r i e s were
s t r u g g l i n g t o r e p a y war d e b t s , which l e d t o
economic i n s t a b i l i t y . " ,
" Bank F a i l u r e s " : " As demand f o r goods d e c l i n e d ,
many banks began t o f a i l , c a u s i n g a l o s s of
c o n f i d e n c e i n t h e banking system . T his l e d
t o a massive w i t h d r a w a l of money from
banks , c a u s i n g even more banks t o f a i l . " ,
" Drought C o n d i t i o n s " : " The Dust Bowl was a
s e v e r e d r o u g h t and d u s t storm t h a t h i t t h e
G r e a t P l a i n s r e g i o n of t h e U n i t e d S t a t e s i n
t h e 1930 s . Th is had a s i g n i f i c a n t i m p a c t
on a g r i c u l t u r e , c a u s i n g many f a r m e r s t o
l o s e t h e i r l a n d and l i v e l i h o o d s which
worsened t h e e f f e c t s of t h e d e p r e s s i o n . "
}
# S te p 3 : D e f i n e n e c e s s a r y f u n c t i o n s t h a t g e n e r a l l y
s o l v e t h i s t y p e of problem
# Do n o t need t o d e f i n e f u n c t i o n s
# S te p 4 : P r i n t t h e answer and e x p l a i n i n n a t u r a l
l a n g u a g e by c a l l i n g t h e i n f o r m a t i o n i n t h e
d e f i n e d knowledge d i c t i o n a r y ‘ d e p r e s s i o n _ c a u s e s
‘
p r i n t ( f "{ d e p r e s s i o n _ n a m e } was a p e r i o d of economic
d e c l i n e t h a t l a s t e d from { d e p r e s s i o n _ p e r i o d } ,
making i t t h e l o n g e s t − l a s t i n g d e p r e s s i o n i n
modern h i s t o r y . I t a f f e c t e d n o t on ly {
d e p r e s s i o n _ c o u n t r i e s } , c a u s i n g s u b s t a n t i a l
s o c i a l and economic u p h e a v a l . \ n " )
p r i n t ( f " There were s e v e r a l major c a u s e s of {
d e p r e s s i o n _ n a m e } , which i n c l u d e : \ n " )
# L i s t c a u s e s and e x p l a n a t i o n s i n ‘ d e p r e s s i o n _ c a u s e s
‘ wi th a f o r − lo op .
f o r i , ( cause , d e s c r i p t i o n ) i n enumerate (
d e p r e s s i o n _ c a u s e s . i t e m s ( ) , 1) :
p r i n t ( f "{ i } . { c a u s e } − { d e s c r i p t i o n } \ n " )
p r i n t ( f " O v e r a l l , { d e p r e s s i o n _ n a m e } was c a u s e d by a
c o m b i n a t i o n of f a c t o r s , i n c l u d i n g economic ,
e n v i r o n m e n t a l , and p o l i t i c a l f a c t o r s . I t s
i m p a c t was widespread , a f f e c t i n g m i l l i o n s of
p e o p l e around t h e world . " )
‘ ‘ ‘
### I n s t r u c t i o n : I d e n t i f y t h e odd one o u t .
### I n p u t : T w i t t e r , I n s t a g r a m , Telegram
### Python program :
‘ ‘ ‘
# S te p 1 : I m p o r t n e c e s s a r y b u i l t − i n l i b r a r i e s
from c o l l e c t i o n s i m p o r t O r d e r e d D i c t
# S te p 2 : D e f i n e d i c t i o n a r i e s s t o r i n g d e t a i l e d
knowledge a b o u t t h e main f u n c t i o n of each
a p p l i c a t i o n
s e r v i c e s = {
" T w i t t e r " : " a s o c i a l media p l a t f o r m mainly f o r
s h a r i n g i n f o r m a t i o n , images and v i d e o s " ,
" I n s t a g r a m " : " a s o c i a l media p l a t f o r m mainly f o r
s h a r i n g i n f o r m a t i o n , images and v i d e o s " ,
" Telegram " : " a cloud − based i n s t a n t messaging and
voice −over −IP s e r v i c e " ,
}
# S te p 3 : D e f i n e a f u n c t i o n t h a t f i n d s t h e d i f f e r e n t
a p p l i c a t i o n
d e f f i n d _ o d d _ o n e _ o u t ( s e r v i c e s , i n p u t _ s e r v i c e s ) :
d e s c r i p t i o n s = [ s e r v i c e s [ s e r v i c e ] f o r s e r v i c e i n
i n p u t _ s e r v i c e s ]
f o r d e s c r i p t i o n i n d e s c r i p t i o n s :
i f d e s c r i p t i o n s . c o u n t ( d e s c r i p t i o n ) == 1 :
r e t u r n i n p u t _ s e r v i c e s [ d e s c r i p t i o n s . i n d e x
( d e s c r i p t i o n ) ]
r e t u r n None
# S te p 4 : P r i n t t h e answer i n n a t u r a l l a n g u a g e by
c a l l i n g t h e i n f o r m a t i o n s t o r e d i n ‘ s e r v i c e s ‘
and t h e d e f i n e d f u n c t i o n ‘ f i n d _ o d d _ o n e _ o u t ‘
i n p u t _ s e r v i c e s = [ " T w i t t e r " , " I n s t a g r a m " , " Telegram
" ]
odd_one_out = f i n d _ o d d _ o n e _ o u t ( s e r v i c e s ,
i n p u t _ s e r v i c e s )

--- PAGE 22 ---
i f odd_one_out :
o t h e r _ s e r v i c e s = [ s e r v i c e f o r s e r v i c e i n
i n p u t _ s e r v i c e s i f s e r v i c e != odd_one_out ]
p r i n t ( f " The odd one o u t i s { odd_one_out } . {
o t h e r _ s e r v i c e s [ 0 ] } and { o t h e r _ s e r v i c e s [ 1 ] }
a r e { s e r v i c e s [ o t h e r _ s e r v i c e s [ 0 ] ] } w h i l e {
odd_one_out } i s { s e r v i c e s [ odd_one_out ] } . " )
‘ ‘ ‘
### I n s t r u c t i o n : C a l c u l a t e t h e t o t a l s u r f a c e a r e a of
a cube w it h a s i d e l e n g t h of 5 cm .
### I n p u t : None
### Python program :
‘ ‘ ‘
# S te p 1 : I m p o r t n e c e s s a r y b u i l t − i n l i b r a r i e s
# No need t o i m p o r t
# S te p 2 : D e f i n e a f u n c t i o n t h a t c a l c u l a t e t h e
s u r f a c e a r e a of cubes
d e f c a l c u l a t e _ s u r f a c e _ a r e a ( s i d e _ l e n g t h ) :
r e t u r n 6 *( s i d e _ l e n g t h **2)
# S te p 3 : D e f i n e d i c t i o n a r i e s s t o r i n g t h e cube
i n f o r m a t i o n
cube = {
" s i d e _ l e n g t h " : 5 # Sid e l e n g t h of t h e cube
}
# S te p 4 : P r i n t a s t e p −by− s t e p c a l c u l a t i o n answer i n
n a t u r a l l a n g u a g e u s i n g t h e d e f i n e d f u n c t i o n
and v a r i b l e
s i d e _ l e n g t h = cube [ " s i d e _ l e n g t h " ]
s u r f a c e _ a r e a = c a l c u l a t e _ s u r f a c e _ a r e a ( s i d e _ l e n g t h )
p r i n t ( f " The s u r f a c e a r e a of a cube i s found by
c a l c u l a t i n g t h e a r e a of one of i t s f a c e s and
m u l t i p l y i n g i t by s i x ( s i n c e a cube has s i x
f a c e s ) . The a r e a of a cube f a c e i s s i m p l y i t s
s i d e l e n g t h s q u a r e d . \ n " )
p r i n t ( f " Thus f o r t h i s p a r t i c u l a r cube : " )
p r i n t ( f " S u r f a c e Area = 6 x ( S id e Length ) \ ^ 2 " )
p r i n t ( f " = 6 x ( { s i d e _ l e n g t h } cm ) \ ^ 2 " )
p r i n t ( f " = 6 x { s i d e _ l e n g t h **2} cm \ ^ 2 " )
p r i n t ( f " = { s u r f a c e _ a r e a } cm \ n " )
p r i n t ( f " The t o t a l s u r f a c e a r e a of t h i s cube i s {
s u r f a c e _ a r e a } s q u a r e c e n t i m e t e r s . " )
‘ ‘ ‘
‘ ‘ ‘
D.4 Prompts for Table 6 and 13
We use the following prompt for the entailment-
based NLEP results in Table 6. The model-free
result uses the NLEP prompt shown in D.1.
" " " Write a Python f u n c t i o n t h a t c o n s t r u c t s a
d e c i s i o n t r e e a c c o r d i n g t o t h e g i v e n examples
t h a t can g e n e r a t e t h e c o r r e c t l a b e l of t h e
g i v e n c l a s s i f i c a t i o n t a s k . " " "
### A v a i l a b l e f u n c t i o n s ( s h a r e d f o r a l l t a s k s ) :
# R e t u r n s whether t h e h y p o t h e s i s i n e n t a i l e d by t h e
p r e m i s e .
d e f e n t a i l m e n t ( h y p o t h e s i s , premise , model , t o k e n i z e r
) :
p r o p o s i t i o n = f ’ { h y p o t h e s i s } i s e n t a i l e d by {
p r e m i s e } . ’
i n p u t s = t o k e n i z e r ( p r o p o s i t i o n , r e t u r n _ t e n s o r s
=" p t " , t r u n c a t i o n =True , padding =True ,
max_length =128)
o u t p u t s = model ( **i n p u t s ) [ ’ l o g i t s ’ ] [ 0 ]
e n t _ l a b e l = i n t ( o u t p u t s [ 0 ] > o u t p u t s [ 2 ] )
i f e n t _ l a b e l == 1 :
r e t u r n ’ yes ’
e l s e :
r e t u r n ’ no ’
# Use t h e c o n s t r u c t e d d e c i s i o n t r e e t o p r e d i c t t h e
l a b e l of t h e s e n t e n c e .
d e f t r e e _ p r e d i c t ( s e n t e n c e , c r i t e r i o n s , t r e e , model ,
t o k e n i z e r ) :
node = t r e e [ ’ r o o t ’ ]
w h i l e node n o t i n POSSIBLE_CLASSES :e n t _ l a b e l = e n t a i l m e n t ( c r i t e r i o n s [ node ] ,
s e n t e n c e , model , t o k e n i z e r )
node = t r e e [ node ] [ e n t _ l a b e l ]
r e t u r n node
### Task : Movie r e v i e w c l a s s i f i c a t i o n
### P o s s i b l e c l a s s e s : [ p o s i t i v e , n e g a t i v e ]
### Examples :
" " "
− c o n t a i n s no wit , onl y l a b o r e d gags
− [ The movie i s wise | The movie i s n o t wise | 1 ] , [
t h e s t o r y i s fun | t h e s t o r y i s n o t b o r i n g
| 1 ] , [ t h e r e v i e w i s p o s i t i v e | t h e r e v i e w i s
n e g a t i v e | 1 ]
− t h a t l o v e s i t s c h a r a c t e r s and communicates
something r a t h e r b e a u t i f u l a b o u t human n a t u r e
− [ The c h a r a c t e r s a r e l o v e l y | The c h a r a c t e r s a r e
awful | 0 ] , [ t h e s c r i p t i s t o u c h i n g | t h e
s c r i p t i s dry | 0 ] , [ t h e r e v i e w i s p o s i t i v e |
t h e r e v i e w i s n e g a t i v e | 0 ]
− on t h e w o r s t revenge −of − the − n e r d s c l i c h e s t h e
f i l m m a k e r s c o u l d d r e d g e up
− [ The movie i s n o v e l | The movie i s m o s t l y
p l a t i t u d e s | 1 ] , [ t h e r e v i e w i s n e g a t i v e | 1 ]
− a r e more d e e p l y t h o u g h t t h r o u g h t h a n i n most r i g h t
− t h i n k i n g f i l m s
− [ The takeaway of t h e movie i s p r o f o u n d | The
i d e a of t h e movie i s s h a l l o w | 0 ] , [ t h e
r e v i e w i s p o s i t i v e | t h e r e v i e w i s n e g a t i v e
| 0 ]
" " "
### D e f i n e p o s s i b l e c l a s s e s
POSSIBLE_CLASSES = [ ’ p o s i t i v e ’ , ’ n e g a t i v e ’ ]
### Python program :
‘ ‘ ‘
d e f g e t _ d e c i s i o n _ t r e e ( s e n t e n c e , model , t o k e n i z e r ) :
# S te p 1 : d e f i n e c r i t e r i o n s of t h e d e c i s i o n t r e e
.
c r i t e r i o n s = [
’ T hi s movie i s i n t e r e s t i n g ’ ,
’ The movie has a good s c r i p t ’ ,
’ The c h a r a c t e r s a r e awsome ’ ,
’ T hi s movie i s wise ’
]
# S te p 2 : d e f i n e t h e D e c i s i o n Tree f o r
c l a s s i f i c a t i o n
t r e e = {
’ r o o t ’ : 0 ,
0 : { ’ yes ’ : 1 , ’ no ’ : 3} ,
1 : { ’ yes ’ : ’ p o s i t i v e ’ , ’ no ’ : 2} ,
2 : { ’ yes ’ : ’ p o s i t i v e ’ , ’ no ’ : ’ n e g a t i v e ’ } ,
3 : { ’ yes ’ : ’ p o s i t i v e ’ , ’ no ’ : ’ n e g a t i v e ’ }
}
r e t u r n c r i t e r i o n s , t r e e
‘ ‘ ‘
When we test the SST-2 performance based on
a generated Cola decision tree in Table 13, we use
the following prompt:
Write a Python f u n c t i o n t h a t c o n s t r u c t s a d e c i s i o n
t r e e a c c o r d i n g t o t h e g i v e n examples t h a t can
g e n e r a t e t h e c o r r e c t l a b e l of t h e g i v e n
c l a s s i f i c a t i o n t a s k .
### A v a i l a b l e APIs ( s h a r e d f o r a l l t a s k s ) :
# R e t u r n s whether t h e h y p o t h e s i s i n e n t a i l e d by t h e
p r e m i s e .
d e f e n t a i l m e n t ( h y p o t h e s i s , premise , model , t o k e n i z e r
) :
p r o p o s i t i o n = f ’ { h y p o t h e s i s } i s e n t a i l e d by {
p r e m i s e } . ’
i n p u t s = t o k e n i z e r ( p r o p o s i t i o n , r e t u r n _ t e n s o r s
=" p t " , t r u n c a t i o n =True , padding =True ,
max_length =128)
o u t p u t s = model ( **i n p u t s ) [ ’ l o g i t s ’ ] [ 0 ]
e n t _ l a b e l = i n t ( o u t p u t s [ 0 ] > o u t p u t s [ 2 ] )
i f e n t _ l a b e l == 1 :
r e t u r n ’ yes ’
e l s e :
r e t u r n ’ no ’

--- PAGE 23 ---
# Use t h e c o n s t r u c t e d d e c i s i o n t r e e t o p r e d i c t t h e
l a b e l of t h e s e n t e n c e .
d e f t r e e _ p r e d i c t ( s e n t e n c e , c r i t e r i o n s , t r e e , model ,
t o k e n i z e r ) :
node = t r e e [ ’ r o o t ’ ]
w h i l e node n o t i n POSSIBLE_CLASSES :
e n t _ l a b e l = e n t a i l m e n t ( c r i t e r i o n s [ node ] ,
s e n t e n c e , model , t o k e n i z e r )
node = t r e e [ node ] [ e n t _ l a b e l ]
r e t u r n node
### Task : Grammar c o r r e c t n e s s c l a s s i f i c a t i o n
### P o s s i b l e c l a s s e s : [ ’ a c c p e t a b l e ’ , ’ u n a c c e p t a b l e ’ ]
### D e f i n e p o s s i b l e c l a s s e s
POSSIBLE_CLASSES = [ ’ a c c p e t a b l e ’ , ’ u n a c c e p t a b l e ’ ]
### D e c i s i o n Tree Logic :
− I f v e r b s a r e n o t c o r r e c t l y c o n s t r u c t e d , t h e
s e n t e n c e i s i m m e d i a t e l y l a b e l e d as u n a c c e p t a b l e
.
− I f v e r b s a r e c o r r e c t :
The t r e e t h e n c h e c k s i f t h e s e n t e n c e has c o r r e c t
p u n c t u a t i o n
− I f i n c o r r e c t , l a b e l t h e s e n t e n c e as
u n a c c e p t a b l e
− I f c o r r e c t :
The n e x t c r i t e r i o n t o be a s s e s s e d i s t h e
s u b j e c t − ve r b agreement .
− I f s u b j e c t and ve r b d i s a g r e e , l a b e l t h e
s e n t e n c e as u n a c c e p t a b l e .
− I f t h e y agree , check f o r s e n t e n c e
f r a g m e n t s .
− I f t h e s e n t e n c e i s a fragment , l a b e l
i t as u n a c c e p t a b l e .
− I f i t i s n o t a s e n t e n c e fragment ,
l a b e l t h e s e n t e n c e as a c c e p t a b l e .
### Python code f o r t h e d e c i s i o n t r e e :
‘ ‘ ‘ python
d e f g e t _ d e c i s i o n _ t r e e ( s e n t e n c e , model , t o k e n i z e r ) :
# S te p 1 : d e f i n e c r i t e r i o n s of t h e d e c i s i o n t r e e
c r i t e r i o n s = {
’ c o r r e c t _ v e r b s ’ : ’ The v e r b s a r e c o r r e c t l y
c o n s t r u c t e d i n t h e s e n t e n c e ’ ,
’ c o r r e c t _ p u n c t u a t i o n ’ : ’ The s e n t e n c e i s
p u n c t u a t e d c o r r e c t l y ’ ,
’ s u b j e c t _ v e r b _ a g r e e m e n t ’ : ’ The s u b j e c t and
v er b a g r e e i n t h e s e n t e n c e ’ ,
’ n o _ s e n t e n c e _ f r a g m e n t s ’ : ’ The s e n t e n c e i s
n o t a fragment ’ ,
}
# S te p 2 : d e f i n e t h e b a l a n c e d d e c i s i o n t r e e f o r
t h i s c l a s s i f i c a t i o n t a s k
t r e e = {
’ r o o t ’ : ’ c o r r e c t _ v e r b s ’ ,
’ c o r r e c t _ v e r b s ’ : { ’ yes ’ : ’
c o r r e c t _ p u n c t u a t i o n ’ , ’ no ’ : ’
u n a c c e p t a b l e ’ } ,
’ c o r r e c t _ p u n c t u a t i o n ’ : { ’ yes ’ : ’
s u b j e c t _ v e r b _ a g r e e m e n t ’ , ’ no ’ : ’
u n a c c e p t a b l e ’ } ,
’ s u b j e c t _ v e r b _ a g r e e m e n t ’ : { ’ yes ’ : ’
n o _ s e n t e n c e _ f r a g m e n t s ’ , ’ no ’ : ’
u n a c c e p t a b l e ’ } ,
’ n o _ s e n t e n c e _ f r a g m e n t s ’ : { ’ yes ’ : ’ a c c e p t a b l e
’ , ’ no ’ : ’ u n a c c e p t a b l e ’ }
}
r e t u r n c r i t e r i o n s , t r e e
‘ ‘ ‘
The input format of target tasks is
### Task : Grammar c o r r e c t n e s s c l a s s i f i c a t i o n
### P o s s i b l e c l a s s e s : [ a c c e p t a b l e , u n a c c e p t a b l e ]

--- PAGE 24 ---
E Implementation Details for
Task-Specific Strategies
We detail the few-shot chain-of-thought (CoT) and
program-of-thought (PoT) prompting under the
task-specific setting in Tables 2 and 4:
•Tracking Shuffled Objects (7). We use the
same 3-shot examples as used by previous
work for both task-specific CoT and PoT. The
three examples are related to Tracking Shuf-
fled Objects (3) and the models need to learn
from demonstrations and generalize to seven
objects test cases. The difference between
CoT and PoT lies on the format of intermedi-
ate reasoning: CoT adopts natural language as
the reasoning chains while we transform the
thought process into concise Python code for
PoT.
•Dyck Language. We cite the results of CoT
from LATM (Cai et al., 2023) and transform
the reasoning steps of the 3-shot examples
used by previous chain-of-thought work into
Python code for PoT. In order to evaluate the
generalization ability of program-of-thought
prompting, we try to avoid directly giving gen-
erally applicable Python program that can be
used for all test instances.
•Word Sorting. We cite the results of CoT
from LATM (Cai et al., 2023) and transform
the reasoning steps of the 3-shot examples
used by previous chain-of-thought work into
Python code for PoT. Since the task can be
effectively resolved by just few lines of code,
i.e., read in the given input and use sorted()
function, e.g.,
# Python code , r e t u r n ans
words = [ ’ oven ’ , ’ costume ’ , ’ c o u n t e r p a r t ’ ]
ans = " " . j o i n ( s o r t e d ( words ) )
it can be regarded that the generally applicable
tool is already given in the input prompt.
•Chinese Remainder Theorem. We cite the
results of CoT from LATM (Cai et al., 2023).
We build the in-context examples (3-shot)
with the first three successful instances of task-
general PoT as we construct the Python rea-
soning chains from the generated programs of
task-general PoT with GPT-4. Indeed, for this
complicated task, the provided program in the
demonstration can also be regarded as a gener-
ally applicable tool. That’s a main reason whytask-specific PoT can obtain 100% accuracy
on this benchmark.
•Scheduling Meeting. We cite the results of
CoT from LATM (Cai et al., 2023). We build
the in-context examples (3-shot) with the first
three successful instances of task-general PoT
as we construct the Python reasoning chains
from the generated programs of task-general
PoT with GPT-4. However, unlike giving
the “ground-truth" problem solving tool for
Chinese Remainder Theorem, the provided
Python reasoning chains can only derive the
correct answer for each specific demonstra-
tion question but can not be directly applied
to all scenarios due to the complexity of the
problem. We hope to compare this setup with
Chinese Remainder Theorem and evaluate the
performance of task-specific PoT on compli-
cated tasks through different in-context learn-
ing demonstrations.
•GSM-Hard. We use the same 8-shot exam-
ples as used by previous work on GSM8K
dataset for CoT GSM-Hard. For PoT, we
adopt the 9-shot examples on GSM8K dataset
from program-of-thought (Chen et al., 2022)
containing concise Python code as reasoning
chains.
•StrategyQA. We remove 1 example that ap-
pears in the development set from the 6-shot
demonstration of previous work (Lyu et al.,
2023) for CoT. As PoT is not designed and ap-
plied for natural language question answering
task, we did not reproduce task-specific PoT
results on StrategyQA benchmark.

--- PAGE 25 ---
F Evaluation Prompts for VicunaQA
We have two metrics for VicunaQA. The first met-
ric assesses the level of details and biases to long
responses generated by GPT-4, while the other met-
ric does not ask for details.
F.1 Evaluation prompt asking for details.
prompt = f ’ ’ ’ [ Q u e s t i o n ] \ n{ q u e s _ s t r }
[ The S t a r t of A s s i s t a n t 1 ’ s Answer ] \ n{ g p t 4 _ r e s }
\ n [ The End of A s s i s t a n t 1 ’ s Answer ]
[ The S t a r t of A s s i s t a n t 2 ’ s Answer ] \ n{ t a r g e t _ r e s }
\ n [ The End of A s s i s t a n t 2 ’ s Answer ]
[ System ]
We would l i k e t o r e q u e s t your f e e d b a c k on t h e
p e r f o r m a n c e of two AI a s s i s t a n t s i n r e s p o n s e t o
t h e u s e r q u e s t i o n d i s p l a y e d above . \ n P l e a s e
r a t e t h e h e l p f u l n e s s , r e l e v a n c e , a cc u ra c y ,
l e v e l of d e t a i l s of t h e i r r e s p o n s e s . Each
a s s i s t a n t r e c e i v e s an o v e r a l l s c o r e on a s c a l e
of 1 t o 10 , where a h i g h e r s c o r e i n d i c a t e s
b e t t e r o v e r a l l p e r f o r m a n c e . \ n P l e a s e f i r s t
o u t p u t a s i n g l e l i n e c o n t a i n i n g onl y two v a l u e s
i n d i c a t i n g t h e s c o r e s f o r A s s i s t a n t 1 and 2 ,
r e s p e c t i v e l y . The two s c o r e s a r e s e p a r a t e d by a
s p a c e . In t h e s u b s e q u e n t l i n e , p l e a s e p r o v i d e
a c om p re h e ns i ve e x p l a n a t i o n of your e v a l u a t i o n ,
a v o i d i n g any p o t e n t i a l b i a s and e n s u r i n g t h a t
t h e o r d e r i n which t h e r e s p o n s e s were p r e s e n t e d
does n o t a f f e c t your judgment . ’ ’ ’
F.2 Evaluation prompt not asking for details.
prompt = f ’ ’ ’ [ Q u e s t i o n ] \ n{ q u e s _ s t r }
[ The S t a r t of A s s i s t a n t 1 ’ s Answer ] \ n{ g p t 4 _ r e s }
\ n [ The End of A s s i s t a n t 1 ’ s Answer ]
[ The S t a r t of A s s i s t a n t 2 ’ s Answer ] \ n{ t a r g e t _ r e s }
\ n [ The End of A s s i s t a n t 2 ’ s Answer ]
[ System ]
We would l i k e t o r e q u e s t your f e e d b a c k on t h e
p e r f o r m a n c e of two AI a s s i s t a n t s i n r e s p o n s e t o
t h e u s e r q u e s t i o n d i s p l a y e d above . \ n P l e a s e
r a t e t h e r e l e v a n c e and a c c u r a c y of t h e i r
r e s p o n s e s . Each a s s i s t a n t r e c e i v e s an o v e r a l l
s c o r e on a s c a l e of 1 t o 10 , where a h i g h e r
s c o r e i n d i c a t e s b e t t e r o v e r a l l p e r f o r m a n c e . \
n P l e a s e f i r s t o u t p u t a s i n g l e l i n e c o n t a i n i n g
on ly two v a l u e s i n d i c a t i n g t h e s c o r e s f o r
A s s i s t a n t 1 and 2 , r e s p e c t i v e l y . The two s c o r e s
a r e s e p a r a t e d by a s p a c e . In t h e s u b s e q u e n t
l i n e , p l e a s e p r o v i d e a c om p r eh e ns i v e
e x p l a n a t i o n of your e v a l u a t i o n , a v o i d i n g any
p o t e n t i a l b i a s and e n s u r i n g t h a t t h e o r d e r i n
which t h e r e s p o n s e s were p r e s e n t e d does n o t
a f f e c t your judgment . Do n o t b i a s on e i t h e r
l o n g e r or s h o r t e r answers . ’ ’ ’
F.3 Calculation of Length Bias
Suppose we have Nevaluation cases, each receiv-
ing 2 candidate responses. A GPT-4 scorer decides
the winner between the candidates. astands for the
number of cases where a candidate response with
more tokens wins. The length bias is calculated by
lb=|a
N−0.5| ∗2 (1)
