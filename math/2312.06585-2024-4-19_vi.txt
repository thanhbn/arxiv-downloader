# 2312.06585.pdf
# Chuyển đổi từ PDF sang TXT  
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2312.06585.pdf
# Kích thước tệp: 761238 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
2024-4-19
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để 
Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
Avi Singh1,*, John D Co-Reyes1,*, Rishabh Agarwal1,2,*,
Ankesh Anand1, Piyush Patil1, Xavier Garcia1, Peter J. Liu1, James Harrison1, Jaehoon Lee1, Kelvin Xu1,
Aaron Parisi1, Abhishek Kumar1, Alex Alemi1, Alex Rizkowsky1, Azade Nova1, Ben Adlam1, Bernd Bohnet1,
Gamaleldin Elsayed1, Hanie Sedghi1, Igor Mordatch1, Isabelle Simpson1, Izzeddin Gur1, Jasper Snoek1,
Jeffrey Pennington1, Jiri Hron1, Kathleen Kenealy1, Kevin Swersky1, Kshiteej Mahajan1, Laura Culp1, Lechao
Xiao1, Maxwell L Bileschi1, Noah Constant1, Roman Novak1, Rosanne Liu1, Tris Warkentin1, Yundi Qian1,
Yamini Bansal1, Ethan Dyer1, Behnam Neyshabur1, Jascha Sohl-Dickstein1, Noah Fiedel1
*Đóng góp như nhau,1Google DeepMind,2Mila
Tinh chỉnh các mô hình ngôn ngữ (LM) trên dữ liệu do con người tạo ra vẫn là một thực hành phổ biến. Tuy nhiên, 
hiệu suất của các mô hình như vậy thường bị giới hạn bởi số lượng và tính đa dạng của dữ liệu chất lượng cao do con người tạo ra.
Trong bài báo này, chúng tôi khám phá liệu chúng ta có thể vượt qua dữ liệu con người trong các nhiệm vụ mà chúng ta có quyền truy cập vào phản hồi vô hướng,
ví dụ, trên các bài toán có thể xác minh tính đúng đắn. Để làm điều này, chúng tôi nghiên cứu một 
phương pháp tự huấn luyện đơn giản dựa trên expectation-maximization, mà chúng tôi gọi là ReST𝐸𝑀, trong đó chúng tôi (1)
tạo ra các mẫu từ mô hình và lọc chúng bằng phản hồi nhị phân, (2) tinh chỉnh mô hình trên 
các mẫu này, và (3) lặp lại quá trình này một vài lần. Thử nghiệm trên các benchmark lý luận MATH nâng cao và mã hóa APPS
sử dụng các mô hình PaLM-2, chúng tôi thấy rằng ReST𝐸𝑀 có thể mở rộng quy mô thuận lợi với kích thước mô hình và
vượt trội đáng kể so với tinh chỉnh chỉ trên dữ liệu con người. Nhìn chung, những phát hiện của chúng tôi cho thấy tự huấn luyện với
phản hồi có thể giảm sự phụ thuộc vào dữ liệu do con người tạo ra.
Từ khóa: RL từ phản hồi bên ngoài, EM cho RL, Ngôn ngữ, LLM, Lý luận, Mã hóa, Tự cải thiện
1. Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) đang cách mạng hóa bối cảnh deep learning, thể hiện
khả năng đáng chú ý trong việc tạo ra văn bản có chất lượng con người và giải quyết các nhiệm vụ ngôn ngữ đa dạng (Google
et al., 2023; OpenAI, 2023). Trong khi tinh chỉnh có giám sát (SFT) trên dữ liệu do con người thu thập tiếp tục
tăng cường hiệu suất của chúng trên các nhiệm vụ quan tâm, việc thu thập dữ liệu con người chất lượng cao tạo ra một
nút thắt cổ chai đáng kể. Điều này đặc biệt đòi hỏi cao đối với các nhiệm vụ giải quyết vấn đề phức tạp, đòi hỏi đáng kể
tài nguyên và kiến thức chuyên môn. Để giải quyết trở ngại này, dữ liệu tổng hợp do mô hình tạo ra nổi lên như
một giải pháp thay thế đầy hứa hẹn, cung cấp khả năng mở rộng quy mô và hiệu quả chi phí, miễn là chất lượng của nó có thể được đảm bảo.
Trong khi LLM có tiềm năng tự đánh giá dữ liệu được tạo ra, bài báo này khám phá một cài đặt đơn giản hơn
trong đó một tín hiệu phản hồi vô hướng bên ngoài đóng vai trò như một chỉ số chất lượng cho từng mẫu được tạo ra.
Để nghiên cứu việc huấn luyện trên dữ liệu do mô hình tạo ra, chúng tôi xem xét một phương pháp tự huấn luyện
đơn giản nhưng mạnh mẽ cho các mô hình ngôn ngữ chỉ đòi hỏi hai khả năng: 1) tạo ra các mẫu từ 
mô hình và 2) đánh giá các mẫu này bằng một cơ chế chấm điểm. Phương pháp này có những điểm tương đồng
với Reinforced Self-Training (ReST) được đề xuất bởi Gulcehre et al. (2023). Chúng tôi thực hiện một số sửa đổi
đối với ReST (chi tiết trong Phần 3), và gọi phương pháp của chúng tôi là ReST𝐸𝑀. Chúng tôi chỉ ra rằng ReST𝐸𝑀 có thể được xem
như việc áp dụng expectation-maximization cho reinforcement learning (Dayan và Hinton, 1997; Peters
và Schaal, 2007), mà chúng tôi trình bày một cách chính thức trong Phần 3. Cụ thể, ReST𝐸𝑀 xen kẽ giữa
các bước expectation và maximization:
Tác giả liên hệ: singhavi@google.com, rishabhagarwal@google.com, jcoreyes@google.com
Xuất bản trong Transactions on Machine Learning Research (04/2024)
©2024 Google DeepMind. Tất cả quyền được bảo lưu arXiv:2312.06585v4  [cs.LG]  18 Apr 2024

--- TRANG 2 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
1520253035404-shot Độ Chính Xác Thử Nghiệm (%)GPT-4
LLaMA-2 70BWizardMath 70BMetaMath 70B
Inflection-1Llemma 34B
Llemma 7B 
Mistral 7B (maj@4)Minerva 62BMinerva 540B
PaLM 2-SPaLM 2-L
Grok-0 (33B)PaLM 2-L (ReSTEM)
PaLM 2-S (ReSTEM)Lý luận: MATH
304050600-shot Độ Chính Xác (%)PaLM 2-S*PaLM 2-LGPT-4
GPT-3.5 (ChatGPT)WizardCoder 15B
LLaMA-2 70BCode LLaMA 34BCode Llama Python 34B
Inflection-1
Mistral 7BGrok-0 (33B)PaLM 2-L (ReSTEM)
PaLM 2-S* (ReSTEM)Tạo Mã: HumanEval
Hình 1|Tự huấn luyện với ReST𝐸𝑀 cải thiện đáng kể hiệu suất thử nghiệm của các mô hình PaLM 2 trên
hai benchmark thử thách: MATH và HumanEval. Kết quả cho các mô hình khác được hiển thị để cho thấy tiến bộ chung
trên các nhiệm vụ này và thường không thể so sánh được do sự khác biệt về quy mô mô hình. Kết quả GPT-4
được lấy từ Bubeck et al. (2023). Trục x gần đúng biểu thị thời gian phát hành (không theo tỷ lệ).
1.Tạo ra (E-step): Mô hình ngôn ngữ tạo ra nhiều mẫu đầu ra cho mỗi bối cảnh đầu vào
. Sau đó, chúng tôi lọc các mẫu này bằng phần thưởng nhị phân để thu thập bộ dữ liệu huấn luyện.
2.Cải thiện (M-step): Mô hình ngôn ngữ gốc được tinh chỉnh có giám sát trên bộ dữ liệu huấn luyện
từ bước Tạo ra trước đó. Mô hình được tinh chỉnh sau đó được sử dụng trong bước Tạo ra tiếp theo
.
ReST𝐸𝑀, với các thích ứng khác nhau của nó (Phần 4), đã chứng minh thành công trong việc tăng cường 
các mô hình ngôn ngữ trên các lĩnh vực đa dạng, bao gồm dịch máy (Gulcehre et al., 2023; Norouzi et al.,
2016), phân tích ngữ nghĩa (Agarwal et al., 2019), căn chỉnh sở thích (Dong et al., 2023), và lý luận
cơ bản (Yuan et al., 2023; Zelikman et al., 2022). Tuy nhiên, các công trình trước đây chủ yếu áp dụng
huấn luyện với dữ liệu tự tạo cho các mô hình ngôn ngữ tương đối nhỏ (lên đến 7B tham số), với
khả năng mở rộng quy mô hạn chế được quan sát đối với các mô hình lớn hơn (Yuan et al., 2023). Bổ sung cho những nỗ lực này, công trình của chúng tôi nhằm nghiên cứu tính hiệu quả và khả năng mở rộng quy mô của dữ liệu tổng hợp do mô hình tạo ra so với dữ liệu do con người tạo ra trong hai lĩnh vực thử thách, ít được khám phá: giải quyết vấn đề toán học cấp độ thi đấu (MATH) (Hendrycks et al., 2021b) và tạo mã (APPS) (Hendrycks et al., 2021a).

Các phát hiện thực nghiệm của chúng tôi tiết lộ những tiến bộ đáng kể trong cả khả năng lý luận toán học và tạo mã
khi áp dụng ReST𝐸𝑀 cho các mô hình PaLM 2 với quy mô khác nhau (Hình 1). Đáng chú ý,
các mô hình được tinh chỉnh trên dữ liệu tổng hợp do mô hình tạo ra thể hiện mức tăng hiệu suất lớn hơn đáng kể
so với những mô hình được huấn luyện trên dữ liệu do con người viết (Hình 2, 3). Thú vị, việc vượt quá một vài
lần lặp của ReST𝐸𝑀 dẫn đến cải thiện giảm dần, cho thấy khả năng overfitting trên số lượng nhỏ
các vấn đề huấn luyện (Hình 4). Ngoài ra, các mô hình được tinh chỉnh bằng ReST𝐸𝑀 cải thiện
hiệu suất pass@k cũng như hiệu suất bỏ phiếu đa số. Hơn nữa, các mô hình được tinh chỉnh này chứng minh
hiệu suất nâng cao trên các benchmark liên quan nhưng được giữ lại, bao gồm các vấn đề toán (GSM8K và
Hungarian HS finals), mã hóa (HumanEval), và các nhiệm vụ Big-Bench Hard. Chúng tôi cũng thực hiện các nghiên cứu ablation để điều tra ảnh hưởng của số lượng các giải pháp do mô hình tạo ra, các vấn đề huấn luyện, và
các lần lặp cho tinh chỉnh ReST𝐸𝑀. Nhìn chung, các phát hiện của chúng tôi cho thấy tự huấn luyện với phản hồi là một
phương pháp đầy hứa hẹn để giảm sự phụ thuộc vào dữ liệu do con người tạo ra.
Các đóng góp chính của công trình này là:
•Chúng tôi giới thiệu ReST𝐸𝑀 cho phép học từ dữ liệu tự tạo cho LLM, sử dụng một
Xuất bản trong Transactions on Machine Learning Research (04/2024) 2

--- TRANG 3 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
phương pháp expectation-maximization có nguyên tắc trong khung reinforcement learning.
•Chúng tôi chứng minh rằng huấn luyện trên các giải pháp tự tạo vượt trội hơn huấn luyện trên các giải pháp do con người tạo ra
trong các lĩnh vực giải quyết vấn đề, chẳng hạn như toán học và tạo mã.
•Thông qua các nghiên cứu ablation toàn diện, chúng tôi xác định chính xác các yếu tố quan trọng cần thiết để đạt được
hiệu suất tối ưu.
•Các LLM được tinh chỉnh với ReST𝐸𝑀 thể hiện khả năng chuyển giao mạnh mẽ trên các nhiệm vụ giữ lại khác nhau.
2. Kiến thức nền tảng
Một mô hình ngôn ngữ tự hồi quy tạo ra một chuỗi đầu ra 𝒚=(𝑦1,𝑦2,....𝑦𝑇) cho trước một bối cảnh (hoặc
đầu vào nguồn) 𝒙=(𝑥1,𝑥2,...𝑥𝐿), trong đó các token 𝑥𝑙,𝑦𝑡 thuộc một từ vựng cố định. Tạo ra tự hồi quy
bao gồm việc dự đoán các token từng cái một, dựa trên các token được tạo ra trước đó.
Giả sử rằng mô hình được tham số hóa bởi 𝜃, phân phối xác suất có điều kiện để tạo ra
một chuỗi 𝒚 cho trước 𝒙 là
𝑝𝜃(𝒚|𝒙)=𝑇Ö
𝑡=1𝑝𝜃(𝑦𝑡|𝒚<𝑡,𝒙),
với quy ước 𝒚1:0=∅ và 𝒚1:𝑡−1=(𝑦1,𝑦2,....𝑦𝑡−1). Để dễ ký hiệu, chúng tôi định nghĩa 𝑝(𝑦𝑡|𝑥):=
𝑝(𝑦𝑡|𝑦<𝑡,𝑥). Xác suất dự đoán token thứ 𝑡, 𝑦𝑡, 𝑝(𝑦𝑡|𝑥), được xác định bằng softmax với
nhiệt độ 𝛾: 𝑝(𝑦𝑡|𝑥)=exp(𝑧𝑡/𝛾)Í𝑀
𝑖=1exp(𝑧𝑖/𝛾), trong đó 𝑧𝑡 là điểm số logit cho token 𝑦𝑡. Giá trị nhiệt độ 𝛾 cao hơn
đưa vào nhiều tính ngẫu nhiên hơn, trong khi giá trị thấp hơn làm cho đầu ra xác định hơn
bằng cách ưu tiên các từ có khả năng xảy ra nhất.
Cho trước một bộ dữ liệu D của các đầu vào 𝒙 và các đầu ra do con người tạo ra 𝒚, tinh chỉnh có giám sát (SFT)
huấn luyện policy bằng cách tối thiểu hóa hàm mất mát negative log likelihood:
LSFT(𝜃)=−𝔼(𝒙,𝒚)∼D"𝑇∑︁
𝑡=1log𝑝𝜃(𝑦𝑡|𝒚1:𝑡−1,𝒙)#
. (1)
Chúng tôi cũng giả sử có quyền truy cập vào phần thưởng cấp độ chuỗi xác định (hoặc terminal) 𝑟(𝒙,𝒚). Khi đó, mục tiêu
reinforcement learning (RL) tương ứng với:
LRL(𝜃)=𝔼𝒙∼D
𝔼𝒚∼𝑝𝜃(𝒚|𝒙)[𝑟(𝒙,𝒚)]
.
Tối ưu hóa trực tiếp hàm mất mát LRL bằng các phương pháp RL trực tuyến, chẳng hạn như policy gradients, đòi hỏi cập nhật
và lấy mẫu từ policy nhiều lần trong quá trình huấn luyện. Tuy nhiên, chi phí tính toán của
tinh chỉnh trên dòng liên tục các mẫu mới trở thành một hạn chế của các phương pháp trực tuyến, đặc biệt
khi kích thước của mạng policy tăng lên hàng chục hoặc hàng trăm tỷ tham số. Chúng tôi thảo luận về một
giải pháp thay thế cho các phương pháp RL trực tuyến như vậy trong phần tiếp theo.
3. Expectation-Maximization cho Reinforced Self-Training
Expectation-Maximization (EM) cho RL Chúng tôi trước tiên mô tả khung EM dựa trên RL với
các mô hình ngôn ngữ, dựa trên công trình trước đây của Dayan và Hinton (1997). Hãy định nghĩa một biến tối ưu nhị phân O, sao cho 𝑝(𝑂=1|𝒙,𝒚)∝𝑓(𝑟(𝒙,𝒚)), cho một hàm không giảm không âm
𝑓:ℝ→ℝ+. Chúng tôi muốn tối đa hóa log-likelihood của việc quan sát 𝑂=1 (đạt được phần thưởng cao):
log𝑝(𝑂=1|𝒙):=log∑︁
𝒚𝑝𝜃(𝒚|𝒙)𝑝(𝑂=1|𝒙,𝒚).
Xuất bản trong Transactions on Machine Learning Research (04/2024) 3

--- TRANG 4 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
Tuy nhiên, tổng trên tất cả các chuỗi có thể 𝒚 thường không thể tính toán được. Thay vì tối đa hóa
log𝑝(𝑂=1;𝒙), người ta có thể xem xét tối đa hóa ELBO 𝐿(𝑝𝜃,𝑞) của nó với respect tới các tham số 𝜃 và
phân phối biến phân 𝑞(𝑦|𝑥). Cụ thể,
log𝑝(𝑂=1|𝒙)=log𝔼𝑞(𝒚|𝒙)𝑝(𝑂=1|𝒙,𝒚)𝑝𝜃(𝒚|𝒙)
𝑞(𝒚|𝒙)
≥𝔼𝑞(𝒚|𝒙)
log𝑝(𝑂=1|𝒙,𝒚)𝑝𝜃(𝒚|𝒙)
𝑞(𝒚|𝒙)
(Bất đẳng thức Jensen)
=𝔼𝑞(𝒚|𝒙)[log𝑝(𝑂=1|𝒙,𝒚)]−KL[𝑞(𝒚|𝒙)||𝑝𝜃(𝒚|𝒙)]
=:𝐿(𝑝𝜃,𝑞) (2)
Thuật toán EM (Dempster et al., 1977) cho Phương trình 2 xen kẽ giữa E-step và M-step:
tại lần lặp 𝑡, ký hiệu tham số mô hình ngôn ngữ là 𝜃𝑡 và phân phối biến phân là 𝑞𝑡.
•E-step: 𝑞𝑡+1=arg max 𝑞𝐿(𝑝𝜃𝑡,𝑞). Vì 𝐿(𝑝𝜃𝑡,𝑞) có thể được viết dưới dạng −𝐾𝐿[𝑞(𝒚|𝒙)||𝑞∗(𝒚|𝒙)],𝑞𝑡+1(𝒚|
𝒙)∝𝑞∗(𝒚|𝒙):=𝑝(𝑂=1|𝒙,𝒚)𝑝𝜃𝑡(𝒚|𝒙). Do đó, bước này tương đương với việc cân bằng các mẫu đầu ra
từ phân phối mô hình ngôn ngữ có điều kiện dựa trên khả năng của chúng để đạt được
phần thưởng cao.
•M-step: 𝜃𝑡+1:=arg max 𝜃𝐿(𝑝𝜃,𝑞𝑡+1)=arg min 𝜃KL
𝑞𝑡+1(𝒚|𝒙)||𝑝𝜃(𝒚|𝒙)
=arg min 𝜃Í
𝒚−𝑞𝑡+1(𝒚|
𝒙)log𝑝𝜃(𝒚|𝒙). Như vậy, bước này tương ứng với việc tối đa hóa hàm mất mát negative log-likelihood có trọng số.
Xen kẽ giữa các bước trên đảm bảo cải thiện đơn điệu trong ELBO: 𝐿(𝑝𝜃𝑡+1,𝑞𝑡+1)≥
𝐿(𝑝𝜃𝑡,𝑞𝑡+1)≥𝐿(𝑝𝜃𝑡,𝑞𝑡).
EM với phần thưởng không âm. Nếu các phần thưởng là không âm và 𝑓 được đặt thành hàm đồng nhất
, thì 𝑝(𝑂=1|𝒙,𝒚)∝𝑟(𝒙,𝒚) có nghĩa là 𝑞𝑡+1(𝒚|𝒙)∝𝑟(𝒙,𝒚)𝑝𝜃𝑡(𝒚|𝒙). Trong trường hợp này,
các tham số policy cập nhật 𝜃𝑡+1 thu được từ M-step tại lần lặp 𝑡 được cho bởi:
𝜃𝑡+1:=arg max
𝜃𝔼𝑥∼Dh
𝔼𝒚∼𝑝𝑡
𝜃(𝒚|𝒙)[𝑟(𝒙,𝒚)log𝑝𝜃(𝒚|𝒙)]i
. (3)
So sánh phương trình trên với mục tiêu RL thông thường (LRL) tiết lộ sự khác biệt chính
giữa RL tiêu chuẩn và RL dựa trên EM: cách dữ liệu đầu ra được lấy mẫu. RL tiêu chuẩn liên tục
cập nhật policy và sử dụng policy mới nhất này để thu thập dữ liệu. Ngược lại, RL dựa trên EM sử dụng một
policy lấy mẫu cố định từ lần lặp trước, tách rời thu thập dữ liệu khỏi tối ưu hóa policy. Sự tách rời này
trong các phương pháp dựa trên EM cho phép mở rộng quy mô dễ dàng hơn đối với các mạng policy lớn, chẳng hạn như LLM.
ReST𝐸𝑀 Được thúc đẩy bởi khung EM, giờ đây chúng tôi thảo luận về một phiên bản đơn giản hóa của phương pháp Reinforced Self-
Training (ReST) của Gulcehre et al. (2023). Phương pháp này, mà chúng tôi gọi là ReST𝐸𝑀, tách rời
thu thập dữ liệu (E-step) và tối ưu hóa policy (M-step) trong một pipeline RL thông thường. Thuật toán 1 phác thảo
thuật toán ReST𝐸𝑀 với nhiều lần lặp, trong đó mỗi lần lặp tương ứng với một bước Generate
và Improve. Chúng tôi mô tả các bước này chi tiết bên dưới.
•Generate (E-step): Trong bước này, chúng tôi tạo ra một bộ dữ liệu D𝑖 bằng cách lấy mẫu nhiều chuỗi đầu ra
từ policy hiện tại 𝑝𝜃: D𝑖={(𝒙𝑗,𝒚𝑗)|𝑁
𝑗=1 s.t. 𝒙𝑗∼D,𝒚𝑗∼𝑝𝜃(𝒚|𝒙𝑗)}. Ở đây, các đầu vào
được lấy mẫu lại từ bộ dữ liệu gốc 𝒙𝑗∼D. Các chuỗi đầu ra trong D𝑖 sau đó được chấm điểm
bằng hàm phần thưởng nhị phân 𝑟(𝒙,𝒚). Trong các thí nghiệm của chúng tôi, chúng tôi điều kiện mô hình ngôn ngữ
bằng cách sử dụng prompt few-shot với các chương trình cho tạo mã và các giải pháp từng bước cho các bài toán
toán học.
Xuất bản trong Transactions on Machine Learning Research (04/2024) 4

--- TRANG 5 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
Thuật toán 1: ReST (Expectation-Maximization). Cho trước một policy ban đầu (ví dụ, pre-trained
LM), ReST𝐸𝑀 lặp lại áp dụng các bước Generate và Improve để cập nhật policy.
Input: D: Bộ dữ liệu huấn luyện, D𝑣𝑎𝑙: Bộ dữ liệu xác thực, L(𝒙,𝒚;𝜃): hàm mất mát, 𝑟(𝒙,𝒚): Hàm phần thưởng không âm, 𝐼: số lần lặp, 𝑁: số mẫu trên mỗi bối cảnh
for 𝑖=1 to 𝐼 do
// Generate (E-step)
Tạo ra bộ dữ liệu D𝑖 bằng cách lấy mẫu: D𝑖={(𝒙𝑗,𝒚𝑗)|𝑁
𝑗=1 s.t. 𝒙𝑗∼D,𝒚𝑗∼𝑝𝜃(𝒚|𝒙𝑗)}
Chú thích D𝑖 với phần thưởng 𝑟(𝒙,𝒚).
// Improve (M-step)
while phần thưởng cải thiện trên D𝑣𝑎𝑙 do
Tối ưu hóa 𝜃 để tối đa hóa mục tiêu: 𝐽(𝜃)=𝔼(𝒙,𝒚)∼D𝑖[𝑟(𝒙,𝒚)log𝑝𝜃(𝒚|𝒙)]
end
end
Output: Policy 𝑝𝜃
•Improve (M-step): Trong lần lặp thứ 𝑖, chúng tôi sử dụng bộ dữ liệu mới D𝑖 từ bước Generate để
tinh chỉnh policy 𝑝𝜃. Để giảm thiểu overfitting cụ thể cho nhiệm vụ, chúng tôi tối thiểu hóa sự lệch khỏi mô hình cơ sở
bằng cách luôn tinh chỉnh mô hình ngôn ngữ pretrained cơ sở. Đối với tinh chỉnh, chúng tôi tối thiểu hóa
hàm mất mát negative log-likelihood có trọng số phần thưởng 𝐽(𝜃)=𝔼(𝒙,𝒚)∼D𝑖[𝑟(𝒙,𝒚)log𝑝𝜃(𝒚|𝒙)]. Một khi
policy được cải thiện, một bộ dữ liệu mới với các mẫu chất lượng tốt hơn có thể được tạo ra một lần nữa.
Sự khác biệt với ReST (Gulcehre et al., 2023). Không giống như ReST, chúng tôi kiềm chế việc bổ sung các đầu ra do con người tạo ra vào D𝑖 trong
bước Generate vì dữ liệu như vậy có thể không phải lúc nào cũng tối ưu cho việc học
hoặc có thể không dễ dàng có sẵn. Hơn nữa, mỗi bước Improve tinh chỉnh mô hình cơ sở thay vì
mô hình thu được từ lần lặp ReST trước đó. Điều này dẫn đến hiệu suất cụ thể cho nhiệm vụ tương đương
nhưng hiệu suất chuyển giao tốt hơn nhiều trên các nhiệm vụ giữ lại (xem Hình 7).
Nhận xét. Các thí nghiệm của chúng tôi tập trung vào các cài đặt giải quyết vấn đề với phần thưởng nhị phân (hoặc 0 hoặc 1),
không giống như các phần thưởng giá trị thực có giới hạn được giả sử bởi Gulcehre et al. (2023). Cụ thể, đối với mỗi
bước Generate, Gulcehre et al. (2023) thực hiện nhiều bước Improve, trong đó mỗi bước Improve
có thể được xem như một M-step với hàm 𝑓(𝑟(𝒙,𝒚))=𝑟(𝒙,𝒚)> 𝜏, trong đó 𝜏∈ℝ+ tăng trong
các M-step liên tiếp. Tuy nhiên, với phần thưởng nhị phân, bất kỳ giá trị nào của 𝜏∈(0,1) tương ứng với các
bước Improve giống hệt nhau.
4. Công trình liên quan
Một số phương pháp trước đây có thể được thể hiện bằng khung expectation-maximization được trình bày
trong Phần 3. Chúng tôi thảo luận về các phương pháp và mối quan hệ của chúng với ReST𝐸𝑀 trong phần này.
•Expert Iteration (ExiT) (Anthony et al., 2017) xen kẽ giữa hai bước: cải thiện expert
và chưng cất policy. Trong bước cải thiện expert (E-step), chúng tôi kết hợp một policy cơ sở
với một thủ tục tìm kiếm để tạo ra các mẫu từ một policy tốt hơn, được gọi là expert policy.
Sau đó, trong bước chưng cất policy (M-step), chúng tôi sử dụng các mẫu expert này để huấn luyện policy cơ sở
một cách có giám sát, cải thiện nó một cách hiệu quả để khớp với expert policy. Trong khi ExiT sử dụng
monte-carlo tree-search, chúng tôi đơn giản sử dụng temperature sampling để thu thập các mẫu từ
expert policy trong ReST. Tuy nhiên, việc cải thiện E-step trong ReST bằng khung ExIT thông qua
các thủ tục tìm kiếm và lập kế hoạch với các mô hình ngôn ngữ sẽ thú vị cho nghiên cứu tương lai. Ví dụ, Huang et al. (2022) thực hiện một lần lặp duy nhất của ReST𝐸𝑀 trên các bài toán lý luận toán học đơn giản
Xuất bản trong Transactions on Machine Learning Research (04/2024) 5

--- TRANG 6 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
. Tuy nhiên, không giống như cài đặt của chúng tôi, họ không giả sử có quyền truy cập vào phần thưởng tính đúng đắn và
thay vào đó sử dụng bỏ phiếu đa số (Wang et al., 2023) như một thủ tục tìm kiếm trong E-step.
•Self-Taught Reasoner (STaR) (Zelikman et al., 2022) sử dụng greedy decoding thay vì
temperature sampling cho E-step trong ReST𝐸𝑀, giới hạn trong một giải pháp do mô hình tạo ra
cho mỗi bài toán trong quá trình thu thập dữ liệu. Ngoài ra, STaR đề xuất rationalization như một
giải pháp thay thế cho temperature sampling, trong đó mô hình ngôn ngữ được cung cấp với câu trả lời đúng
như một phần của đầu vào để tạo ra các giải pháp đúng cho các bài toán khó. Tuy nhiên, trong các thí nghiệm
sơ bộ của chúng tôi, rationalization dẫn đến sự gia tăng đáng kể trong các giải pháp dương tính giả
dẫn đến câu trả lời đúng nhưng với lý luận sai.
•Rejection Sampling Fine-tuning (RFT) (Yuan et al., 2023) cải thiện hiệu suất lý luận
trên GSM8K và tương ứng với việc chạy một generate (E-step) và improve (M-step) duy nhất của
ReST𝐸𝑀. Trong khi RFT chứng minh cải thiện hiệu suất hạn chế trên GSM8K với việc tăng
khả năng của mô hình ngôn ngữ, ReST𝐸𝑀 đạt được lợi ích lớn hơn trên các benchmark APPS và MATH thử thách hơn
khi mở rộng quy mô khả năng của mô hình PaLM 2. Hơn nữa, chúng tôi quan sát thấy rằng việc sử dụng nhiều
lần lặp của ReST𝐸𝑀 dẫn đến lợi ích hiệu suất lớn hơn.
•Iterative Maximum Likelihood (IML) tối ưu hóa một policy bằng cách sử dụng mục tiêu
log-likelihood có trọng số phần thưởng trên dữ liệu tự thu thập. IML đã được chứng minh là hoạt động tốt với
các mô hình ngôn ngữ quy mô tương đối nhỏ cho phân tích ngữ nghĩa (Agarwal et al., 2019; Liang et al., 2016), dịch máy
(Wu et al., 2016) và lý luận toán học đơn giản (Ni et al., 2022). Mỗi E-step và
M-step trong IML được thực hiện trên một mini-batch các ví dụ huấn luyện thay vì toàn bộ bộ dữ liệu huấn luyện
, như được thực hiện trong ReST𝐸𝑀. Trong IML, policy đã học có thể phân kỳ đáng kể khỏi mô hình
pretrained ban đầu, điều này có thể biểu hiện như overfitting cụ thể cho nhiệm vụ, trong đó mô hình hoạt động
tốt trên nhiệm vụ mục tiêu nhưng mất khả năng tổng quát hóa cho các nhiệm vụ hoặc lĩnh vực khác. Ngoài ra,
tính chất kết hợp chặt chẽ của thu thập dữ liệu và tối ưu hóa policy trong IML dẫn đến chi phí
tính toán cao với các LM lớn, làm cho nó đắt đỏ hơn đáng kể so với ReST𝐸𝑀.
•Reward weighted regression (RWR) (Peters và Schaal, 2007) tương ứng với EM trong đó
chúng tôi đặt 𝑝(𝑂=1|𝒙,𝒚)∝exp(𝑟(𝒙,𝒚)) trong Phần 3. RWR trước đây đã được áp dụng cho điều khiển robot
, vì nó có thể dễ dàng áp dụng cho các hàm phần thưởng không nhị phân. Norouzi et al. (2016) dựa trên
RWR để đề xuất một biến thể tổng quát của IML cho dịch máy.
•Reward ranked fine-tuning (RAFT) (Dong et al., 2023) có thể được giải thích như việc xen kẽ
giữa E-step và M-step trên các mini-batch, trong đó E-step sử dụng mẫu đầu ra với
phần thưởng tối đa cho mỗi bối cảnh đầu vào. Đối với các hàm phần thưởng nhị phân, RAFT tương tự như
IML và như vậy, có thể được xem như một thể hiện của ReST𝐸𝑀.
Các công trình liên quan khác: TRICE (Phan et al., 2023) đề xuất một phương pháp dựa trên EM để tối đa hóa
marginal log-likelihood (MML) của việc tạo ra một câu trả lời đúng cho một bài toán lý luận, trong đó
chuỗi lý luận chain-of-thought được coi như một biến tiềm ẩn. Trong khi E-step trong ReST𝐸𝑀 đơn giản tương ứng
với lấy mẫu từ mô hình và lọc bằng phần thưởng nhị phân, TRICE sử dụng Markov-chain Monte
Carlo với một control variate để xấp xỉ gradient MML. Sordoni et al. (2023) đề xuất một
phương pháp dựa trên EM không có gradient, tương tự như RAFT, cho tối ưu hóa prompt cho các LLM đông lạnh.
Được truyền cảm hứng bởi phiên bản trước của bản thảo này, Agarwal et al. (2024) nghiên cứu liệu dữ liệu do mô hình tạo ra
có thể vượt trội hơn dữ liệu con người cho prompting few-shot và many-shot hay không. Họ thấy rằng
đây thực sự là trường hợp, đặc biệt đối với prompting few-shot.
Xuất bản trong Transactions on Machine Learning Research (04/2024) 6

--- TRANG 7 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
ReST𝐸𝑀 ReST STaR RFT
Bắt đầu từ mô hình được tinh chỉnh ✗ ✓ ✗ ✗
Tinh chỉnh từ mô hình cơ sở trong mỗi lần lặp ✓ ✗ ✓ N/A
Sử dụng rationalization cho câu hỏi chưa giải quyết ✗ ✗ ✓ ✗
Temperature sampling cho khám phá ✓ ✓ ✗ ✓
Thí nghiệm với LM lớn ✓ ✗ ✗ ✓
Nhiều lần lặp ✓ ✓ ✓ ✗
Lợi ích lớn hơn trên các mô hình lớn hơn ✓ N/A N/A ✗
Đánh giá trên các nhiệm vụ giữ lại ✓ ✗ ✗ ✗
Bảng 1|Sự khác biệt giữa ReST𝐸𝑀 và các phương pháp liên quan khác sử dụng dữ liệu tổng hợp
để nâng cao khả năng của mô hình ngôn ngữ.
5. Thí nghiệm và phân tích
Mục tiêu của các thí nghiệm của chúng tôi là trả lời các câu hỏi sau:
1. ReST𝐸𝑀 hiệu quả như thế nào so với tinh chỉnh trên dữ liệu do con người tạo ra?
2. Cần bao nhiêu lần lặp để có hiệu suất tối ưu? ReST𝐸𝑀 dẫn đến overfitting trên tập huấn luyện nhanh như thế nào?
3. ReST𝐸𝑀 ảnh hưởng như thế nào đến hiệu suất pass@k và bỏ phiếu đa số?
4. Nếu chúng tôi tinh chỉnh bằng dữ liệu do mô hình tạo ra trên một nhiệm vụ cụ thể, chúng tôi có thấy chuyển giao tích cực
sang các nhiệm vụ liên quan không? Có bất kỳ sự suy giảm hiệu suất nào so với mô hình cơ sở khi
đánh giá các mô hình được tinh chỉnh của chúng tôi trên một bộ nhiệm vụ rộng không?
5. Chúng tôi cần bao nhiêu dữ liệu đầu vào để có được hầu hết các lợi ích hiệu suất từ ReST𝐸𝑀? Một lần lặp của ReST𝐸𝑀 có đủ không?
Bộ dữ liệu huấn luyện. Chúng tôi đánh giá ReST𝐸𝑀 chủ yếu trên giải quyết vấn đề toán học bằng cách sử dụng bộ dữ liệu
MATH của Hendrycks (Hendrycks et al., 2021b) và tạo mã bằng cách sử dụng bộ dữ liệu APPS (Introductory)
(Hendrycks et al., 2021a). MATH và APPS (Introductory) chứa 7500 và 2342 vấn đề huấn luyện
tương ứng. Chúng tôi chọn các nhiệm vụ này vì các đầu ra của mô hình có thể được đánh giá tự động
là đúng hay sai, hoàn toàn phù hợp với ReST𝐸𝑀. Cả hai bộ dữ liệu này đều cung cấp phần thưởng nhị phân: trên
MATH, các câu trả lời do mô hình tạo ra có thể dễ dàng được xác minh tính đúng đắn bằng cách sử dụng câu trả lời ground-truth,
trong khi trên APPS, các test case xác định liệu mã được tạo ra có đúng hay không.
Các mô hình. Chúng tôi sử dụng các mô hình PaLM 2 (Google et al., 2023) với các API công khai trên Google Cloud cho
các thí nghiệm, bao gồm PaLM 2-S (Bison), PaLM 2-S* (Codey), và PaLM 2-L (Unicorn).
Đánh giá. Chúng tôi báo cáo hiệu suất tổng quát hóa bằng cách sử dụng các phân chia thử nghiệm của các bộ dữ liệu MATH và APPS
(Introductory). Để đo lường hiệu suất chuyển giao, chúng tôi xem xét các bộ dữ liệu GSM8K (Cobbe et al., 2021),
Hungarian HS finals (Paster, 2023), và HumanEval (Chen et al., 2021). Chúng tôi cũng đánh giá
các mô hình của chúng tôi bằng cách sử dụng benchmark Big-Bench Hard (Suzgun et al., 2022) để đánh giá khả năng chung.
Tất cả các đánh giá tuân theo các cài đặt từ Google et al. (2023), trừ khi được chỉ định khác.
Chi tiết triển khai. Trong mỗi lần lặp của ReST𝐸𝑀, chúng tôi tạo ra một số lượng giải pháp cố định
cho mỗi bài toán cho E-step: 32 cho bộ dữ liệu MATH và 64 cho bộ dữ liệu APPS. Để
tạo ra các giải pháp, chúng tôi lấy mẫu từ mô hình ngôn ngữ bằng cách sử dụng top-K sampling với K=40 và
nhiệt độ 0.7. Tuy nhiên, việc sử dụng trực tiếp tất cả các giải pháp do mô hình tạo ra này có thể dẫn đến một
bộ dữ liệu không cân bằng, vì chúng tôi sẽ có nhiều giải pháp đúng hơn cho các bài toán dễ hơn. Để giảm thiểu
điều này, chúng tôi đã đưa ra một ngưỡng cắt cho số lượng giải pháp tối đa cho mỗi bài toán, một lựa chọn thiết kế
Xuất bản trong Transactions on Machine Learning Research (04/2024) 7

--- TRANG 8 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
0 1 2 3
Số lần lặp2025303540Pass@1 Độ Chính Xác Thử Nghiệm (%)Hendrycks MATH
Palm-2-S Palm-2-L Palm-2-L-SFT Palm-2-S-SFT
0 1 2 3
Số lần lặp 607080Pass@1 Độ Chính Xác Thử Nghiệm (%)Chuyển giao sang GSM8K
Hình 2|ReST𝐸𝑀 cho giải quyết vấn đề toán học. Hiệu suất thử nghiệm trên MATH và GSM8K (chuyển giao) cho
PaLM 2-S* và PaLM 2-L như một hàm của các lần lặp ReST𝐸𝑀. Chúng tôi cũng báo cáo hiệu suất của các mô hình
được tinh chỉnh thông qua SFT trên dữ liệu do con người tạo ra như một baseline. Lần lặp 0 tương ứng với hiệu suất mô hình pretrained
. Theo Google et al. (2023), chúng tôi sử dụng greedy decoding cho đánh giá.
0 1 2
Số lần lặp1820222426Pass@1 Độ Chính Xác Thử Nghiệm (%)APPS (Introductory)
Palm-2-S* Palm-2-L Palm-2-L-SFT Palm-2-S*-SFT
0 1 2
Số lần lặp 40455055Pass@1 Độ Chính Xác Thử Nghiệm (%)Chuyển giao sang HumanEval
Hình 3|ReST𝐸𝑀 cho tạo mã. Hiệu suất thử nghiệm trên APPS (introductory) và Hu-
manEval (chuyển giao) cho PaLM 2-S* và PaLM 2-L như một hàm của các lần lặp ReST𝐸𝑀.
cũng được sử dụng bởi Zelikman et al. (2022), bao gồm trong bộ dữ liệu tinh chỉnh: 10 cho cả MATH
và APPS. Phương pháp này đảm bảo tính đa dạng trong dữ liệu huấn luyện và bảo vệ khỏi overfitting
trên các bài toán dễ hơn. Để tinh chỉnh, chúng tôi sử dụng prompt few-shot (và câu hỏi) như đầu vào cho
mô hình, và sử dụng các giải pháp do mô hình tạo ra như mục tiêu. Chúng tôi chỉ áp dụng hàm mất mát dự đoán token tiếp theo
(Phương trình 1) trên các mục tiêu.
5.1. ReST𝐸𝑀 trên MATH và APPS
Hình 2 và 3 cho thấy hiệu suất của ReST𝐸𝑀 khi được huấn luyện trên các bộ dữ liệu MATH và APPS,
tương ứng. Chúng tôi thấy rằng MATH hưởng lợi từ việc thực hiện nhiều lần lặp của ReST𝐸𝑀, cả về mặt
hiệu suất trên tập thử nghiệm MATH, cũng như chuyển giao sang GSM8K. Mặt khác, chúng tôi thấy rằng
hầu hết các lợi ích cho APPS đến từ lần lặp đầu tiên, và nhiều lần lặp hơn dẫn đến sự suy giảm trên
cả APPS và HumanEval.
Thú vị, Hình 2 và 3 chứng minh rằng tinh chỉnh trên các giải pháp do mô hình tạo ra vượt trội đáng kể
Xuất bản trong Transactions on Machine Learning Research (04/2024) 8

--- TRANG 9 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
0 1 2 3
Số lần lặp 354045505560Pass@1 Hiệu Suất (%)Hendrycks MATH
Palm-2-L (Train) Palm-2-L (Test)
0 1 2
Số lần lặp 2030405060Pass@1 Hiệu Suất (%)APPS (Introductory)
Palm-2-L (Train) Palm-2-L (Test)
Hình 4|Khoảng cách hiệu suất train-test trên (trái) MATH với PaLM-2-L, và (phải) APPS với PaLM-
2-S*, như một hàm của các lần lặp ReST𝐸𝑀.
so với việc sử dụng các giải pháp do con người viết, đặc biệt đối với mô hình PaLM 2-L. Điều này phù hợp với
các phát hiện của Yuan et al. (2023) và công trình gần đây về chưng cất LLM bằng dữ liệu do mô hình tạo ra (Agarwal
et al., 2023; Gu et al., 2023). Tuy nhiên, không giống như Yuan et al. (2023), người quan sát thấy lợi ích giảm dần
từ dữ liệu do mô hình tạo ra trên GSM8K khi mở rộng quy mô khả năng mô hình, kết quả của chúng tôi cho thấy xu hướng ngược lại
: ReST𝐸𝑀 dẫn đến lợi ích hiệu suất lớn hơn khi khả năng mô hình tăng lên. Trên bộ dữ liệu MATH,
cải thiện độ chính xác thử nghiệm với ReST𝐸𝑀 là 5.94% cho PaLM 2-S so với 6.34% cho mô hình
PaLM 2-L lớn hơn. Tương tự, trên bộ dữ liệu APPS, các cải thiện là 5.6% cho PaLM 2-S* so với
6.4% cho PaLM 2-L. Điều này ngoài thực tế là các mô hình lớn hơn bắt đầu với hiệu suất ban đầu mạnh mẽ hơn nhiều
, và các cải thiện trên các benchmark này thường trở nên khó khăn hơn khi hiệu suất baseline tăng lên.
Khoảng cách hiệu suất train-test. Hình 4 cho thấy rằng trong khi hiệu suất huấn luyện tăng tuyến tính
với số lần lặp ReST𝐸𝑀, hiệu suất tập thử nghiệm thì không. Đối với MATH, các cải thiện hiệu suất thử nghiệm
nhỏ sau lần lặp đầu tiên, và đối với APPS, chúng tôi quan sát thấy sự suy giảm hiệu suất
trong lần lặp thứ 2. Chúng tôi nghi ngờ rằng sự suy giảm hiệu suất có thể do overfitting trên
tập nhỏ các vấn đề huấn luyện. Vì bộ dữ liệu APPS nhỏ hơn khoảng một phần ba so với bộ dữ liệu MATH,
nó bị ảnh hưởng nhiều hơn bởi vấn đề này.
5.2. Tác động lên Pass@K và Hiệu Suất Bỏ Phiếu Đa Số
Để nghiên cứu tác động của tinh chỉnh với ReST𝐸𝑀 lên sự đa dạng của các đầu ra được tạo ra của mô hình cuối cùng
, chúng tôi đánh giá hiệu suất pass@k (Chen et al., 2021) và bỏ phiếu đa số (Wang et al., 2023)
của mô hình PaLM 2-L được tinh chỉnh so với mô hình cơ sở.
Pass@K đo lường xác suất rằng ít nhất một trong K giải pháp được tạo ra cho một bài toán là
đúng, tức là, đưa ra câu trả lời đúng cho các bài toán toán học hoặc vượt qua tất cả các unit test cho tạo mã
. Hình 5 cho thấy hiệu suất của Palm-2-L trên chỉ số pass@K. Chúng tôi thấy rằng mô hình
thu được sau tinh chỉnh ReST𝐸𝑀 mạnh hơn đối với tất cả các giá trị K, với khoảng cách hiệu suất thường
cao nhất đối với K=1.
Bỏ phiếu đa số đầu tiên lấy mẫu một tập hợp đa dạng các đường lý luận thay vì chỉ lấy cái greedy
, và sau đó chọn câu trả lời nhất quán nhất bằng cách marginalize các đường lý luận được lấy mẫu.
Đối với Hendrycks MATH, có thể sử dụng bỏ phiếu đa số để tối đa hóa hiệu suất Pass@1, và chúng tôi
thấy rằng khi sử dụng 64 mẫu cho mỗi câu hỏi, PaLM 2-L được tinh chỉnh với ReST𝐸𝑀 đạt được độ chính xác thử nghiệm
48.82, trong khi mô hình cơ sở đạt 44.02.
Xuất bản trong Transactions on Machine Learning Research (04/2024) 9

--- TRANG 10 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
0 20 40 60
Số mẫu (K)40%60%80%Pass @ K Độ Chính Xác Thử Nghiệm (%)HumanEval
PaLM-2-L
PaLM-2-L (ReST)
2 4 6 8 10
Số mẫu (K)10%20%30%40%Pass @ K Độ Chính Xác Thử Nghiệm (%)APPS (Introductory)
PaLM-2-L
PaLM-2-L (ReST)
0 20 40 60
Số mẫu (K)20%30%40%50%60%70%80%Pass @ K Độ Chính Xác Thử Nghiệm (%)Hendrycks MATH
Palm-2-L
Palm-2-L (ReST)
Hình 5|Kết quả Pass@K cho mô hình PaLM-2-L pretrained cũng như mô hình được tinh chỉnh với ReST𝐸𝑀.
Với một số mẫu K cố định, tinh chỉnh với ReST𝐸𝑀 cải thiện đáng kể hiệu suất Pass@K.
Chúng tôi đặt nhiệt độ thành 1.0 và sử dụng nucleus sampling với 𝑝=0.95.
5.3. Nghiên Cứu Ablation
Tác động của nhiều lần lặp Kết quả của chúng tôi cho thấy rằng nhiều lần lặp đôi khi có thể dẫn đến
over-fitting trên tập train (Hình 4). Điều này đặt ra câu hỏi liệu nhiều lần lặp có thực sự cần thiết hay không
. Có tốt hơn không nếu thu thập một bộ dữ liệu lớn hơn và chỉ thực hiện một lần lặp duy nhất của ReST𝐸𝑀?
Để nghiên cứu điều này, chúng tôi thu thập một bộ dữ liệu với mô hình PaLM-2-L cơ sở trên Hendrycks MATH có
số lượng giải pháp gấp 3× cho mỗi bài toán so với được sử dụng trong một lần lặp duy nhất của ReST𝐸𝑀 cho E-step
. Tinh chỉnh với bộ dữ liệu này dẫn đến hiệu suất pass@1 là 40.3%, thấp hơn so với 41% trong lần lặp thứ hai
và 41.9% trong lần lặp thứ ba, như được hiển thị trong Hình 2. Các kết quả này cho thấy rằng thực hiện nhiều
lần lặp của ReST𝐸𝑀 dẫn đến hiệu suất cao hơn so với một lần lặp duy nhất với dữ liệu gấp 3 lần.
So sánh dữ liệu do mô hình tạo ra với dữ liệu con người Một điểm mạnh chính của ReST𝐸𝑀 là khả năng
tạo ra nhiều giải pháp đúng cho mỗi bài toán. Điều này cung cấp dữ liệu huấn luyện bổ sung có giá trị
so với dữ liệu do con người tạo ra, thường chỉ cung cấp một giải pháp duy nhất cho mỗi bài toán. Trong khi
điều này làm cho việc so sánh trong Hình 2 và 3 không hoàn toàn công bằng, nó cũng làm nổi bật tiềm năng của ReST𝐸𝑀
để tăng cường hiệu suất với các giải pháp đa dạng và đúng.
Để cho phép so sánh táo với táo, chúng tôi tiến hành nghiên cứu sau: chúng tôi chọn tất cả
các câu hỏi Hendrycks MATH mà chúng tôi có ít nhất một giải pháp đúng do mô hình tạo ra, dẫn đến
khoảng 5K câu hỏi. Đối với 5K câu hỏi này, chúng tôi chạy hai thí nghiệm tinh chỉnh: SFT(5K) trong đó
chúng tôi tinh chỉnh trên các giải pháp do con người viết (một câu hỏi cho mỗi câu hỏi), và ReST∗(5K) trong đó chúng tôi tinh chỉnh trên
các giải pháp do mô hình tạo ra (cũng một câu hỏi cho mỗi câu hỏi, được chọn ngẫu nhiên).
Các kết quả trong Hình 6 (phải), cho thấy rằng ReST𝐸𝑀 vượt trội hơn tinh chỉnh trên dữ liệu con người ngay cả trong
cài đặt hạn chế hơn nhiều này. Hơn nữa, hiệu quả của ReST(5K) so với ReST∗(5K) làm nổi bật
lợi ích bổ sung về hiệu suất mà chúng tôi có thể đạt được bằng cách dành nhiều compute hơn cho việc lấy mẫu một
số lượng lớn các giải pháp và thực hiện nhiều lần lặp của ReST𝐸𝑀.
Chưng cất với dữ liệu do ReST𝐸𝑀 tạo ra Các kết quả trên cho thấy rằng dữ liệu tự tạo có thể
tốt hơn dữ liệu con người để tinh chỉnh các mô hình ngôn ngữ. Chúng tôi giả thuyết rằng điều này có thể là do
các giải pháp do mô hình tạo ra có tính in-distribution hơn so với các giải pháp do con người viết. Điều này đặt ra
câu hỏi liệu dữ liệu do ReST𝐸𝑀 tạo ra có thể có lợi cho các mô hình khác với mô hình tạo ra
dữ liệu hay không.
Để trả lời câu hỏi này, chúng tôi xem xét một cài đặt chưng cất trên MATH trong đó chúng tôi tinh chỉnh PaLM 2-S
bằng dữ liệu được tạo ra bởi PaLM 2-L, dẫn đến các giải pháp cho khoảng 5K câu hỏi. Cụ thể, chúng tôi chạy
Xuất bản trong Transactions on Machine Learning Research (04/2024) 10

--- TRANG 11 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
SFT (7K) SFT (5K)ReST* (5K)ReSTEM (5K)
Phương pháp (Số câu hỏi)3436384042Pass@1 Hiệu Suất (%)Hendrycks MATH (Test)
SFT (Human) Distill* (2-L) ReSTEM (2-S)Distill (2-L)
Phương pháp (Nguồn Dữ Liệu)15.017.520.022.525.027.530.0Pass@1 Hiệu Suất (%)PaLM 2-S trên Hendrycks MATH (Test)
Hình 6|Trái. So sánh ReST𝐸𝑀 với SFT trên MATH. SFT đề cập đến tinh chỉnh trên dữ liệu con người,
trong khi ReST* đề cập đến một phiên bản của ReST𝐸𝑀 với một lần lặp sử dụng chỉ một mẫu đúng cho mỗi
bài toán. Ở đây, ReST biểu thị ReST𝐸𝑀 với 3 lần lặp. Đối với mỗi phương pháp, chúng tôi ký hiệu số lượng
câu hỏi trong ngoặc đơn. Phải. Tác động của Dữ Liệu Do Mô Hình Tạo Ra cho Chưng Cất.
hai thí nghiệm chưng cất: Distill∗(2-L) trong đó chúng tôi tinh chỉnh trên các giải pháp do teacher tạo ra (một
cho mỗi câu hỏi), tương tự như ReST (5K), và Distill (2-L), bao gồm nhiều giải pháp cho mỗi bài toán,
được tạo ra trong lần lặp cuối cùng của ReST𝐸𝑀 với PaLM 2-L.
Kết quả của chúng tôi, được hiển thị trong Hình 6 (phải), tiết lộ rằng Distill∗ vượt trội hơn hiệu suất đạt được
bằng cách tinh chỉnh trên các giải pháp do con người viết, mặc dù có số lượng câu hỏi huấn luyện nhỏ hơn.
Ngoài ra, tinh chỉnh PaLM 2-S với nhiều giải pháp từ PaLM 2-L, cụ thể là Distill (2-L), là
vượt trội hơn so với việc sử dụng các giải pháp tự tạo thông qua ReST𝐸𝑀. Cải thiện này có thể do số lượng
câu hỏi huấn luyện với giải pháp trong dữ liệu do PaLM 2-L tạo ra lớn hơn so với 2-S. Nhìn chung,
các kết quả này cho thấy rằng dữ liệu do mô hình tạo ra có thể hiệu quả hơn để tinh chỉnh các mô hình nhỏ hơn
so với việc dựa vào dữ liệu do con người tạo ra.
ReSTEMReST
Phương pháp21.021.221.421.621.822.0Pass@1 Độ Chính Xác Thử Nghiệm (%)APPS (Introductory)
Palm-2-S*-SFTReSTEMReST
Phương pháp384042444648Pass@1 Độ Chính Xác Thử Nghiệm (%)Chuyển giao sang HumanEval
Hình 7|ReST𝐸𝑀 vs ReST sử dụng PaLM 2-S*. ReST vs ReST𝐸𝑀 Một sự khác biệt lớn giữa
ReST𝐸𝑀 và ReST là trong khi ReST𝐸𝑀 luôn tinh chỉnh
mô hình cơ sở cho mỗi lần lặp, ReST tiếp tục
tinh chỉnh mô hình từ lần lặp cuối cùng.
Chúng tôi chạy một ablation so sánh các tùy chọn này
bằng cách sử dụng PaLM 2-S* trong Hình 7 và quan sát rằng trong khi
ReST và ReST𝐸𝑀 có hiệu suất tương tự trên
APPS, hiệu suất chuyển giao sang HumanEval là
tốt hơn đáng kể với ReST𝐸𝑀.
Tác động của kích thước bộ dữ liệu Vì một trong những thành phần chính cần thiết cho ReST𝐸𝑀 là một bộ dữ liệu các bối cảnh đầu vào
(ví dụ, câu hỏi cho MATH), chúng tôi quan tâm đến việc đánh giá ảnh hưởng của số lượng bài toán đầu vào
. Các kết quả từ các ablation bộ dữ liệu của chúng tôi sử dụng mô hình PaLM-2-L trên Hendrycks MATH,
Hình 8 (trái), cho thấy rằng việc sử dụng chỉ 1000 câu hỏi MATH dẫn đến lợi ích đáng kể, có nghĩa là
phương pháp này rất hiệu quả về số lượng prompt cần thiết. Tuy nhiên, chúng tôi lưu ý sự giảm nhẹ
hiệu suất khi sử dụng 4,000 câu hỏi so với 2,000, cho thấy variance tiềm ẩn trong
quá trình tinh chỉnh. Lý tưởng nhất, việc tiến hành thí nghiệm này nhiều lần sẽ giúp định lượng variance này
, nhưng điều này tốn kém về mặt tài nguyên một cách cấm đoán. Nhìn chung, chúng tôi thấy rằng ReST𝐸𝑀 khá hiệu quả về mẫu
và lợi ích hiệu suất từ ReST𝐸𝑀 cải thiện khi chúng tôi tăng kích thước bộ dữ liệu.
Xuất bản trong Transactions on Machine Learning Research (04/2024) 11

--- TRANG 12 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
0 1000 2000 4000 7000
Số Câu Hỏi3436384042Pass@1 Hiệu Suất (%)Hendrycks MATH (Test)
Rất Khó Khó Trung Bình Dễ
Mức Độ Khó Của Bài Toán020406080100Tỷ Lệ Thành Công Trung Bình (%)Tỷ Lệ Thành Công Trung Bình Theo Mức Độ Khó
Mô Hình Cơ Sở
ReSTEM
Hình 8|Trái. Hiệu suất cho một lần lặp duy nhất của ReST𝐸𝑀 như một hàm của kích thước bộ dữ liệu (số lượng
câu hỏi) trên MATH. Phải. Cải thiện từ ReST𝐸𝑀 dựa trên mức độ khó của câu hỏi.
Những Câu Hỏi Nào Hưởng Lợi Nhất từ ReST𝐸𝑀 Chúng tôi đánh giá sự nâng cao hiệu suất của ReST𝐸𝑀
trên các mức độ khó khác nhau của câu hỏi trong bộ dữ liệu Hendrycks MATH. Các câu hỏi được phân loại dựa trên
tỷ lệ thành công từ mô hình cơ sở ở cài đặt nhiệt độ T=1.0 thành bốn loại: "dễ"
(trả lời đúng 75%-100% thời gian), "trung bình" (50%-75%), "khó" (25%-50%), và "rất
khó" (dưới 25%). Hình 8 (phải) trình bày tỷ lệ thành công trung bình cho các loại này, so sánh
mô hình cơ sở với mô hình được tinh chỉnh ReST𝐸𝑀. Kết quả cho thấy rằng ReST𝐸𝑀 liên tục
cải thiện hiệu suất trên tất cả các mức độ khó, với lợi ích cao nhất đến từ các câu hỏi được phân loại
là trung bình và khó.
5.4. Tác động lên Khả năng Lý luận
Boolean ExpressionsCausal JudgementDate UnderstandingDisambiguation QADyck LanguagesFormal FallaciesGeometric ShapesHyperbaton
Movie RecommendationMulti-step Arithmetic [Two]Navigate
Object CountingPenguins in a Table
Reasoning about Colored ObjectsRuin Names
Salient Translation Error DetectionSnarks
Sports UnderstandingTemporal SequencesWeb of LiesWord Sorting
Logical Deduction (avg)
Tracking Shuffled Objects (avg)
Nhiệm Vụ Big-Bench Hard (BBH)30405060708090100Hiệu Suất Few-shot với CoTPaLM 2-L PaLM 2-L (MATH) PaLM 2-L (APPS)
CoT Direct
Loại Prompt6065707580Hiệu Suất BBH Trung BìnhPaLM 2-L
PaLM 2-L (APPS)
PaLM 2-L (MATH)
Hình 9|So sánh các mô hình ReST𝐸𝑀 với mô hình cơ sở trên bộ nhiệm vụ Big-Bench Hard.
Các đánh giá được tiến hành trên nhiều checkpoint, và các đường thẳng đen dọc biểu thị độ lệch
chuẩn.
Khả năng chung. BIG-Bench cung cấp một bộ hơn 200 nhiệm vụ có thể được sử dụng để thăm dò
hiệu suất của LLM trên một loạt các lĩnh vực và khả năng. BIG-Bench Hard (BBH) (Suzgun et al.,
Xuất bản trong Transactions on Machine Learning Research (04/2024) 12

--- TRANG 13 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
2022) là một tập con của 23 nhiệm vụ BIG-Bench trong đó thế hệ LLM trước đó, chẳng hạn như Codex
và PaLM 540B, có hiệu suất dưới mức người đánh giá trung bình. Chúng tôi tuân theo giao thức của Google et al.
(2023) và đánh giá trên BBH bằng cách sử dụng cả prompting few-shot và chain-of-thought. Hình 9 cho thấy
hiệu suất của các mô hình được tinh chỉnh ReST𝐸𝑀, và so sánh chúng với mô hình PaLM-2 cơ sở. Chúng tôi
thấy không có sự suy giảm lớn nào trên bất kỳ nhiệm vụ BBH nào. Hơn nữa, mô hình được tinh chỉnh trên Hendrycks
MATH vượt trội hơn mô hình cơ sở trên bộ này khi sử dụng prompting chain-of-thought, và
mô hình được tinh chỉnh trên APPS cũng cho thấy lợi ích hiệu suất nhẹ. Khi sử dụng prompting trực tiếp, cả ba
mô hình đều hoạt động tương tự.
Giải quyết vấn đề. Để kiểm tra khả năng giải quyết vấn đề toán học trên một tập đánh giá "thế giới thực" được giữ lại
, chúng tôi đánh giá mô hình của chúng tôi trên kỳ thi chung kết trung học phổ thông Hungary năm 2023 môn toán học,
theo giao thức đánh giá từ Paster (2023). Cụ thể, chúng tôi đánh giá mô hình PaLM 2-L,
được tinh chỉnh với ReST𝐸𝑀 trên Hendrycks MATH, sử dụng prompt 1-shot từ Grok, lấy mẫu các giải pháp
bằng nhiệt độ 0.1, và chấm điểm thủ công các đầu ra bằng rubric được cung cấp bởi các giám khảo.
Kết quả từ đánh giá được hiển thị trong Hình 10. Chúng tôi thấy rằng PaLM-2-L được tinh chỉnh với ReST𝐸𝑀
hoạt động tốt trên kỳ thi này, vượt trội hơn hiệu suất của tất cả các mô hình hiện tại trừ GPT-4.
20 30 40 50 60 70
Điểm Kỳ Thi Chung Kết HS Hungary (%)30405060708090Điểm GSM8K (%)MetaMath 7BMetaMath Mistral 7B OpenChat 3.5
Code Llama 34BLlemma 34BGPT-3.5 TurboGPT-4
Grok-0 (33B)Grok-1
Qwen 7BClaude 2
Mistral 7BMAmmoTH 7BPaLM 2-L (ReSTEM)Điểm Kỳ Thi vs Hiệu Suất GSM8K của Các Mô Hình Khác Nhau
Hình 10|Kết quả chuyển giao trên Kỳ Thi Chung Kết HS Hungary. Kết quả cho các mô hình khác ngoài PaLM-2-L
được tinh chỉnh với ReST𝐸𝑀 được lấy từ Paster (2023). Một số mô hình chuyên về toán học
hoạt động tốt trên benchmark GSM8K được sử dụng rộng rãi nhưng hoạt động kém trên kỳ thi Hungary. Ngược lại,
mô hình PaLM 2-L được tinh chỉnh với ReST𝐸𝑀 hoạt động tốt trên cả hai benchmark này.
6. Thảo luận
Trong bài báo này, chúng tôi đề xuất huấn luyện trên dữ liệu do mô hình tạo ra kết hợp với hàm phần thưởng,
thông qua ReST𝐸𝑀, để cải thiện hiệu suất của LLM trên các nhiệm vụ giải quyết vấn đề. Hơn nữa, chúng tôi
chứng minh rằng ReST𝐸𝑀 có nền tảng lý thuyết trong việc áp dụng expectation-maximization
cho RL. Chúng tôi đánh giá ReST𝐸𝑀 trên giải quyết vấn đề toán học và tạo mã, và cho thấy rằng
ReST𝐸𝑀 mang lại lợi ích hiệu suất đáng kể với chi phí tính toán tương đối thấp, đặc biệt khi
so sánh với chi phí pre-training. Các thí nghiệm của chúng tôi cũng cho thấy rằng ReST𝐸𝑀 không dẫn đến
sự suy giảm trên các nhiệm vụ khác. Chúng tôi tiến hành một số ablation để hiểu rõ hơn về điểm mạnh và
điểm yếu của phương pháp này, và thấy rằng nó hiệu quả về dữ liệu, nhưng cũng đòi hỏi một số cảnh giác để tránh
over-fitting.
Có một số hạn chế liên quan đến ReST𝐸𝑀. Thứ nhất, phương pháp này đòi hỏi một
tập huấn luyện các vấn đề hoặc prompt có kích thước vừa phải, cần được thu thập (từ con người) cho bất kỳ
nhiệm vụ mới nào quan tâm. Thứ hai, ReST𝐸𝑀 cũng đòi hỏi quyền truy cập vào hàm phần thưởng được thiết kế thủ công hoặc đã học
, lý tưởng là một hàm có thể được tính toán tự động. Cuối cùng, trong khi ReST𝐸𝑀 cho phép
cải thiện hiệu suất đáng kể trong hiệu suất pass@1, nó có thể không hoàn toàn thu hẹp khoảng cách đến hiệu suất pass@K
cho cùng một nhiệm vụ (với K đủ lớn). Nghiên cứu tương lai về tự cải thiện trong
các mô hình ngôn ngữ nên tập trung vào việc tự động hóa các phần thủ công của pipeline (có thể thông qua
các mô hình ngôn ngữ), và khám phá các cải tiến thuật toán giảm khoảng cách đến hiệu suất pass@K.
Lời cảm ơn
Chúng tôi muốn cảm ơn Tom Le Paine đã cung cấp phản hồi cho bản thảo đầu. Chúng tôi cũng thừa nhận
Benjamin Anderson, Sridhar Thiagarajan, Feryal Behbahani, Aleksandra Faust, Doina Precup, Olivier
Bachem, và Slav Petrov cho các thảo luận hữu ích.
Đóng góp của tác giả
Avi, Rishabh, và JD cùng lãnh đạo dự án. Avi chịu trách nhiệm về cơ sở hạ tầng huấn luyện và đánh giá
, ablation và thí nghiệm trên MATH, JD lãnh đạo các thí nghiệm trên APPS, Rishabh chịu trách nhiệm về
việc viết bài báo, đánh giá, và ablation chưng cất.
Ankesh, Piyush, Ethan, và Behnam quan sát những phát hiện sơ bộ về hiệu quả của dữ liệu do mô hình tạo ra
trên MATH cho các mô hình Minerva và thúc đẩy nghiên cứu này. Piyush cũng giúp Avi
trong việc thiết lập cơ sở hạ tầng. Xavier, Peter, James, Jaeheoon, Kelvin và Yamini tham gia vào các thảo luận dự án
. Jascha và Noah tài trợ và tư vấn cho dự án. Tất cả các tác giả khác đã cung cấp phản hồi
về công trình này.
Tài liệu tham khảo
R. Agarwal, C. Liang, D. Schuurmans, và M. Norouzi. Learning to generalize from sparse and
underspecified rewards. In International conference on machine learning, trang 130–140. PMLR,
2019.
R. Agarwal, N. Vieillard, P. Stanczyk, S. Ramos, M. Geist, và O. Bachem. Gkd: Generalized knowledge
distillation for auto-regressive sequence models. arXiv preprint arXiv:2306.13649, 2023.
R. Agarwal, A. Singh, L. M. Zhang, B. Bohnet, S. Chan, A. Anand, Z. Abbas, A. Nova, J. D. Co-Reyes,
E. Chu, F. Behbahani, A. Faust, và H. Larochelle. Many-shot in-context learning, 2024.
T. Anthony, Z. Tian, và D. Barber. Thinking fast and slow with deep learning and tree search.
Advances in neural information processing systems, 30, 2017.
S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. M.
Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, và Y. Zhang. Sparks of artificial general intelligence:
Early experiments with GPT-4. CoRR, abs/2303.12712, 2023. doi: 10.48550/ARXIV.2303.12712.
URL https://doi.org/10.48550/arXiv.2303.12712.
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,
N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,
B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P.
Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol,
A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer,
Xuất bản trong Transactions on Machine Learning Research (04/2024) 14

--- TRANG 15 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, và W. Zaremba. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,
R. Nakano, C. Hesse, và J. Schulman. Training verifiers to solve math word problems. arXiv
preprint arXiv:2110.14168, 2021.
P. Dayan và G. E. Hinton. Using expectation-maximization for reinforcement learning. Neural
Computation, 9(2):271–278, 1997.
A. P. Dempster, N. M. Laird, và D. B. Rubin. Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society: series B (methodological), 39(1):1–22, 1977.
H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, và T. Zhang. Raft: Reward ranked
finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.
Google, R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,
Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.
Y. Gu, L. Dong, F. Wei, và M. Huang. Knowledge distillation of large language models. arXiv preprint
arXiv:2306.08543, 2023.
C. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant, A. Ahern,
M. Wang, C. Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint
arXiv:2308.08998, 2023.
D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He,
D. Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938,
2021a.
D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, và J. Steinhardt. Measuring
mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b.
J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, và J. Han. Large language models can self-improve.
CoRR, abs/2210.11610, 2022. doi: 10.48550/ARXIV.2210.11610. URL https://doi.org/10.
48550/arXiv.2210.11610.
C. Liang, J. Berant, Q. Le, K. D. Forbus, và N. Lao. Neural symbolic machines: Learning semantic
parsers on freebase with weak supervision. arXiv preprint arXiv:1611.00020, 2016.
A. Ni, J. P. Inala, C. Wang, A. Polozov, C. Meek, D. Radev, và J. Gao. Learning math reasoning from
self-sampled correct and partially-correct solutions. In The Eleventh International Conference on
Learning Representations, 2022.
M. Norouzi, S. Bengio, N. Jaitly, M. Schuster, Y. Wu, D. Schuurmans, et al. Reward augmented
maximum likelihood for neural structured prediction. Advances In Neural Information Processing
Systems, 29, 2016.
OpenAI. Gpt-4 technical report, 2023.
K. Paster. Testing language models on a held-out high school national finals exam. https://
huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam, 2023.
J. Peters và S. Schaal. Reinforcement learning by reward-weighted regression for operational space
control. In Proceedings of the 24th international conference on Machine learning, trang 745–750,
2007.
Xuất bản trong Transactions on Machine Learning Research (04/2024) 15

--- TRANG 16 ---
Vượt Qua Dữ Liệu Con Người: Mở Rộng Quy Mô Tự Huấn Luyện để Giải Quyết Vấn Đề với Các Mô Hình Ngôn Ngữ
D. Phan, M. D. Hoffman, D. Dohan, S. Douglas, T. A. Le, A. Parisi, P. Sountsov, C. Sutton, S. Vikram,
và R. A. Saurous. Training chain-of-thought via latent-variable inference. arXiv preprint
arXiv:2312.02179, 2023.
A. Sordoni, X. Yuan, M.-A. Côté, M. Pereira, A. Trischler, Z. Xiao, A. Hosseini, F. Niedtner, và
N. Le Roux. Joint prompt optimization of stacked llms using variational inference. In Thirty-seventh
Conference on Neural Information Processing Systems, 2023.
M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi,
D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv
preprint arXiv:2210.09261, 2022.
X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, và D. Zhou. Self-
consistency improves chain of thought reasoning in language models. In The Eleventh International
Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,
2023. URL https://openreview.net/pdf?id=1PL1NIMMrw.
Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,
et al. Google's neural machine translation system: Bridging the gap between human and machine
translation. arXiv preprint arXiv:1609.08144, 2016.
Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, và C. Zhou. Scaling relationship on learning mathematical
reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.
E. Zelikman, Y. Wu, J. Mu, và N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances
in Neural Information Processing Systems, 35:15476–15488, 2022.
Xuất bản trong Transactions on Machine Learning Research (04/2024) 16
