# 2312.06585.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT  
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/math/2312.06585.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 761238 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================

--- TRANG 1 ---
2024-4-19
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ 
Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
Avi Singh1,*, John D Co-Reyes1,*, Rishabh Agarwal1,2,*,
Ankesh Anand1, Piyush Patil1, Xavier Garcia1, Peter J. Liu1, James Harrison1, Jaehoon Lee1, Kelvin Xu1,
Aaron Parisi1, Abhishek Kumar1, Alex Alemi1, Alex Rizkowsky1, Azade Nova1, Ben Adlam1, Bernd Bohnet1,
Gamaleldin Elsayed1, Hanie Sedghi1, Igor Mordatch1, Isabelle Simpson1, Izzeddin Gur1, Jasper Snoek1,
Jeffrey Pennington1, Jiri Hron1, Kathleen Kenealy1, Kevin Swersky1, Kshiteej Mahajan1, Laura Culp1, Lechao
Xiao1, Maxwell L Bileschi1, Noah Constant1, Roman Novak1, Rosanne Liu1, Tris Warkentin1, Yundi Qian1,
Yamini Bansal1, Ethan Dyer1, Behnam Neyshabur1, Jascha Sohl-Dickstein1, Noah Fiedel1
*ÄÃ³ng gÃ³p nhÆ° nhau,1Google DeepMind,2Mila
Tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ (LM) trÃªn dá»¯ liá»‡u do con ngÆ°á»i táº¡o ra váº«n lÃ  má»™t thá»±c hÃ nh phá»• biáº¿n. Tuy nhiÃªn, 
hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh nhÆ° váº­y thÆ°á»ng bá»‹ giá»›i háº¡n bá»Ÿi sá»‘ lÆ°á»£ng vÃ  tÃ­nh Ä‘a dáº¡ng cá»§a dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao do con ngÆ°á»i táº¡o ra.
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i khÃ¡m phÃ¡ liá»‡u chÃºng ta cÃ³ thá»ƒ vÆ°á»£t qua dá»¯ liá»‡u con ngÆ°á»i trong cÃ¡c nhiá»‡m vá»¥ mÃ  chÃºng ta cÃ³ quyá»n truy cáº­p vÃ o pháº£n há»“i vÃ´ hÆ°á»›ng,
vÃ­ dá»¥, trÃªn cÃ¡c bÃ i toÃ¡n cÃ³ thá»ƒ xÃ¡c minh tÃ­nh Ä‘Ãºng Ä‘áº¯n. Äá»ƒ lÃ m Ä‘iá»u nÃ y, chÃºng tÃ´i nghiÃªn cá»©u má»™t 
phÆ°Æ¡ng phÃ¡p tá»± huáº¥n luyá»‡n Ä‘Æ¡n giáº£n dá»±a trÃªn expectation-maximization, mÃ  chÃºng tÃ´i gá»i lÃ  ReSTğ¸ğ‘€, trong Ä‘Ã³ chÃºng tÃ´i (1)
táº¡o ra cÃ¡c máº«u tá»« mÃ´ hÃ¬nh vÃ  lá»c chÃºng báº±ng pháº£n há»“i nhá»‹ phÃ¢n, (2) tinh chá»‰nh mÃ´ hÃ¬nh trÃªn 
cÃ¡c máº«u nÃ y, vÃ  (3) láº·p láº¡i quÃ¡ trÃ¬nh nÃ y má»™t vÃ i láº§n. Thá»­ nghiá»‡m trÃªn cÃ¡c benchmark lÃ½ luáº­n MATH nÃ¢ng cao vÃ  mÃ£ hÃ³a APPS
sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh PaLM-2, chÃºng tÃ´i tháº¥y ráº±ng ReSTğ¸ğ‘€ cÃ³ thá»ƒ má»Ÿ rá»™ng quy mÃ´ thuáº­n lá»£i vá»›i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ 
vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i tinh chá»‰nh chá»‰ trÃªn dá»¯ liá»‡u con ngÆ°á»i. NhÃ¬n chung, nhá»¯ng phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i cho tháº¥y tá»± huáº¥n luyá»‡n vá»›i
pháº£n há»“i cÃ³ thá»ƒ giáº£m sá»± phá»¥ thuá»™c vÃ o dá»¯ liá»‡u do con ngÆ°á»i táº¡o ra.
Tá»« khÃ³a: RL tá»« pháº£n há»“i bÃªn ngoÃ i, EM cho RL, NgÃ´n ngá»¯, LLM, LÃ½ luáº­n, MÃ£ hÃ³a, Tá»± cáº£i thiá»‡n
1. Giá»›i thiá»‡u
CÃ¡c MÃ´ hÃ¬nh NgÃ´n ngá»¯ Lá»›n (LLM) Ä‘ang cÃ¡ch máº¡ng hÃ³a bá»‘i cáº£nh deep learning, thá»ƒ hiá»‡n
kháº£ nÄƒng Ä‘Ã¡ng chÃº Ã½ trong viá»‡c táº¡o ra vÄƒn báº£n cÃ³ cháº¥t lÆ°á»£ng con ngÆ°á»i vÃ  giáº£i quyáº¿t cÃ¡c nhiá»‡m vá»¥ ngÃ´n ngá»¯ Ä‘a dáº¡ng (Google
et al., 2023; OpenAI, 2023). Trong khi tinh chá»‰nh cÃ³ giÃ¡m sÃ¡t (SFT) trÃªn dá»¯ liá»‡u do con ngÆ°á»i thu tháº­p tiáº¿p tá»¥c
tÄƒng cÆ°á»ng hiá»‡u suáº¥t cá»§a chÃºng trÃªn cÃ¡c nhiá»‡m vá»¥ quan tÃ¢m, viá»‡c thu tháº­p dá»¯ liá»‡u con ngÆ°á»i cháº¥t lÆ°á»£ng cao táº¡o ra má»™t
nÃºt tháº¯t cá»• chai Ä‘Ã¡ng ká»ƒ. Äiá»u nÃ y Ä‘áº·c biá»‡t Ä‘Ã²i há»i cao Ä‘á»‘i vá»›i cÃ¡c nhiá»‡m vá»¥ giáº£i quyáº¿t váº¥n Ä‘á» phá»©c táº¡p, Ä‘Ã²i há»i Ä‘Ã¡ng ká»ƒ
tÃ i nguyÃªn vÃ  kiáº¿n thá»©c chuyÃªn mÃ´n. Äá»ƒ giáº£i quyáº¿t trá»Ÿ ngáº¡i nÃ y, dá»¯ liá»‡u tá»•ng há»£p do mÃ´ hÃ¬nh táº¡o ra ná»•i lÃªn nhÆ°
má»™t giáº£i phÃ¡p thay tháº¿ Ä‘áº§y há»©a háº¹n, cung cáº¥p kháº£ nÄƒng má»Ÿ rá»™ng quy mÃ´ vÃ  hiá»‡u quáº£ chi phÃ­, miá»…n lÃ  cháº¥t lÆ°á»£ng cá»§a nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘áº£m báº£o.
Trong khi LLM cÃ³ tiá»m nÄƒng tá»± Ä‘Ã¡nh giÃ¡ dá»¯ liá»‡u Ä‘Æ°á»£c táº¡o ra, bÃ i bÃ¡o nÃ y khÃ¡m phÃ¡ má»™t cÃ i Ä‘áº·t Ä‘Æ¡n giáº£n hÆ¡n
trong Ä‘Ã³ má»™t tÃ­n hiá»‡u pháº£n há»“i vÃ´ hÆ°á»›ng bÃªn ngoÃ i Ä‘Ã³ng vai trÃ² nhÆ° má»™t chá»‰ sá»‘ cháº¥t lÆ°á»£ng cho tá»«ng máº«u Ä‘Æ°á»£c táº¡o ra.
Äá»ƒ nghiÃªn cá»©u viá»‡c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u do mÃ´ hÃ¬nh táº¡o ra, chÃºng tÃ´i xem xÃ©t má»™t phÆ°Æ¡ng phÃ¡p tá»± huáº¥n luyá»‡n
Ä‘Æ¡n giáº£n nhÆ°ng máº¡nh máº½ cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ chá»‰ Ä‘Ã²i há»i hai kháº£ nÄƒng: 1) táº¡o ra cÃ¡c máº«u tá»« 
mÃ´ hÃ¬nh vÃ  2) Ä‘Ã¡nh giÃ¡ cÃ¡c máº«u nÃ y báº±ng má»™t cÆ¡ cháº¿ cháº¥m Ä‘iá»ƒm. PhÆ°Æ¡ng phÃ¡p nÃ y cÃ³ nhá»¯ng Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng
vá»›i Reinforced Self-Training (ReST) Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Gulcehre et al. (2023). ChÃºng tÃ´i thá»±c hiá»‡n má»™t sá»‘ sá»­a Ä‘á»•i
Ä‘á»‘i vá»›i ReST (chi tiáº¿t trong Pháº§n 3), vÃ  gá»i phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i lÃ  ReSTğ¸ğ‘€. ChÃºng tÃ´i chá»‰ ra ráº±ng ReSTğ¸ğ‘€ cÃ³ thá»ƒ Ä‘Æ°á»£c xem
nhÆ° viá»‡c Ã¡p dá»¥ng expectation-maximization cho reinforcement learning (Dayan vÃ  Hinton, 1997; Peters
vÃ  Schaal, 2007), mÃ  chÃºng tÃ´i trÃ¬nh bÃ y má»™t cÃ¡ch chÃ­nh thá»©c trong Pháº§n 3. Cá»¥ thá»ƒ, ReSTğ¸ğ‘€ xen káº½ giá»¯a
cÃ¡c bÆ°á»›c expectation vÃ  maximization:
TÃ¡c giáº£ liÃªn há»‡: singhavi@google.com, rishabhagarwal@google.com, jcoreyes@google.com
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024)
Â©2024 Google DeepMind. Táº¥t cáº£ quyá»n Ä‘Æ°á»£c báº£o lÆ°u arXiv:2312.06585v4  [cs.LG]  18 Apr 2024

--- TRANG 2 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
1520253035404-shot Äá»™ ChÃ­nh XÃ¡c Thá»­ Nghiá»‡m (%)GPT-4
LLaMA-2 70BWizardMath 70BMetaMath 70B
Inflection-1Llemma 34B
Llemma 7B 
Mistral 7B (maj@4)Minerva 62BMinerva 540B
PaLM 2-SPaLM 2-L
Grok-0 (33B)PaLM 2-L (ReSTEM)
PaLM 2-S (ReSTEM)LÃ½ luáº­n: MATH
304050600-shot Äá»™ ChÃ­nh XÃ¡c (%)PaLM 2-S*PaLM 2-LGPT-4
GPT-3.5 (ChatGPT)WizardCoder 15B
LLaMA-2 70BCode LLaMA 34BCode Llama Python 34B
Inflection-1
Mistral 7BGrok-0 (33B)PaLM 2-L (ReSTEM)
PaLM 2-S* (ReSTEM)Táº¡o MÃ£: HumanEval
HÃ¬nh 1|Tá»± huáº¥n luyá»‡n vá»›i ReSTğ¸ğ‘€ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t thá»­ nghiá»‡m cá»§a cÃ¡c mÃ´ hÃ¬nh PaLM 2 trÃªn
hai benchmark thá»­ thÃ¡ch: MATH vÃ  HumanEval. Káº¿t quáº£ cho cÃ¡c mÃ´ hÃ¬nh khÃ¡c Ä‘Æ°á»£c hiá»ƒn thá»‹ Ä‘á»ƒ cho tháº¥y tiáº¿n bá»™ chung
trÃªn cÃ¡c nhiá»‡m vá»¥ nÃ y vÃ  thÆ°á»ng khÃ´ng thá»ƒ so sÃ¡nh Ä‘Æ°á»£c do sá»± khÃ¡c biá»‡t vá» quy mÃ´ mÃ´ hÃ¬nh. Káº¿t quáº£ GPT-4
Ä‘Æ°á»£c láº¥y tá»« Bubeck et al. (2023). Trá»¥c x gáº§n Ä‘Ãºng biá»ƒu thá»‹ thá»i gian phÃ¡t hÃ nh (khÃ´ng theo tá»· lá»‡).
1.Táº¡o ra (E-step): MÃ´ hÃ¬nh ngÃ´n ngá»¯ táº¡o ra nhiá»u máº«u Ä‘áº§u ra cho má»—i bá»‘i cáº£nh Ä‘áº§u vÃ o
. Sau Ä‘Ã³, chÃºng tÃ´i lá»c cÃ¡c máº«u nÃ y báº±ng pháº§n thÆ°á»Ÿng nhá»‹ phÃ¢n Ä‘á»ƒ thu tháº­p bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n.
2.Cáº£i thiá»‡n (M-step): MÃ´ hÃ¬nh ngÃ´n ngá»¯ gá»‘c Ä‘Æ°á»£c tinh chá»‰nh cÃ³ giÃ¡m sÃ¡t trÃªn bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n
tá»« bÆ°á»›c Táº¡o ra trÆ°á»›c Ä‘Ã³. MÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh sau Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng trong bÆ°á»›c Táº¡o ra tiáº¿p theo
.
ReSTğ¸ğ‘€, vá»›i cÃ¡c thÃ­ch á»©ng khÃ¡c nhau cá»§a nÃ³ (Pháº§n 4), Ä‘Ã£ chá»©ng minh thÃ nh cÃ´ng trong viá»‡c tÄƒng cÆ°á»ng 
cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ trÃªn cÃ¡c lÄ©nh vá»±c Ä‘a dáº¡ng, bao gá»“m dá»‹ch mÃ¡y (Gulcehre et al., 2023; Norouzi et al.,
2016), phÃ¢n tÃ­ch ngá»¯ nghÄ©a (Agarwal et al., 2019), cÄƒn chá»‰nh sá»Ÿ thÃ­ch (Dong et al., 2023), vÃ  lÃ½ luáº­n
cÆ¡ báº£n (Yuan et al., 2023; Zelikman et al., 2022). Tuy nhiÃªn, cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y chá»§ yáº¿u Ã¡p dá»¥ng
huáº¥n luyá»‡n vá»›i dá»¯ liá»‡u tá»± táº¡o cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ tÆ°Æ¡ng Ä‘á»‘i nhá» (lÃªn Ä‘áº¿n 7B tham sá»‘), vá»›i
kháº£ nÄƒng má»Ÿ rá»™ng quy mÃ´ háº¡n cháº¿ Ä‘Æ°á»£c quan sÃ¡t Ä‘á»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n (Yuan et al., 2023). Bá»• sung cho nhá»¯ng ná»— lá»±c nÃ y, cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i nháº±m nghiÃªn cá»©u tÃ­nh hiá»‡u quáº£ vÃ  kháº£ nÄƒng má»Ÿ rá»™ng quy mÃ´ cá»§a dá»¯ liá»‡u tá»•ng há»£p do mÃ´ hÃ¬nh táº¡o ra so vá»›i dá»¯ liá»‡u do con ngÆ°á»i táº¡o ra trong hai lÄ©nh vá»±c thá»­ thÃ¡ch, Ã­t Ä‘Æ°á»£c khÃ¡m phÃ¡: giáº£i quyáº¿t váº¥n Ä‘á» toÃ¡n há»c cáº¥p Ä‘á»™ thi Ä‘áº¥u (MATH) (Hendrycks et al., 2021b) vÃ  táº¡o mÃ£ (APPS) (Hendrycks et al., 2021a).

CÃ¡c phÃ¡t hiá»‡n thá»±c nghiá»‡m cá»§a chÃºng tÃ´i tiáº¿t lá»™ nhá»¯ng tiáº¿n bá»™ Ä‘Ã¡ng ká»ƒ trong cáº£ kháº£ nÄƒng lÃ½ luáº­n toÃ¡n há»c vÃ  táº¡o mÃ£
khi Ã¡p dá»¥ng ReSTğ¸ğ‘€ cho cÃ¡c mÃ´ hÃ¬nh PaLM 2 vá»›i quy mÃ´ khÃ¡c nhau (HÃ¬nh 1). ÄÃ¡ng chÃº Ã½,
cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh trÃªn dá»¯ liá»‡u tá»•ng há»£p do mÃ´ hÃ¬nh táº¡o ra thá»ƒ hiá»‡n má»©c tÄƒng hiá»‡u suáº¥t lá»›n hÆ¡n Ä‘Ã¡ng ká»ƒ
so vá»›i nhá»¯ng mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u do con ngÆ°á»i viáº¿t (HÃ¬nh 2, 3). ThÃº vá»‹, viá»‡c vÆ°á»£t quÃ¡ má»™t vÃ i
láº§n láº·p cá»§a ReSTğ¸ğ‘€ dáº«n Ä‘áº¿n cáº£i thiá»‡n giáº£m dáº§n, cho tháº¥y kháº£ nÄƒng overfitting trÃªn sá»‘ lÆ°á»£ng nhá»
cÃ¡c váº¥n Ä‘á» huáº¥n luyá»‡n (HÃ¬nh 4). NgoÃ i ra, cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh báº±ng ReSTğ¸ğ‘€ cáº£i thiá»‡n
hiá»‡u suáº¥t pass@k cÅ©ng nhÆ° hiá»‡u suáº¥t bá» phiáº¿u Ä‘a sá»‘. HÆ¡n ná»¯a, cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh nÃ y chá»©ng minh
hiá»‡u suáº¥t nÃ¢ng cao trÃªn cÃ¡c benchmark liÃªn quan nhÆ°ng Ä‘Æ°á»£c giá»¯ láº¡i, bao gá»“m cÃ¡c váº¥n Ä‘á» toÃ¡n (GSM8K vÃ 
Hungarian HS finals), mÃ£ hÃ³a (HumanEval), vÃ  cÃ¡c nhiá»‡m vá»¥ Big-Bench Hard. ChÃºng tÃ´i cÅ©ng thá»±c hiá»‡n cÃ¡c nghiÃªn cá»©u ablation Ä‘á»ƒ Ä‘iá»u tra áº£nh hÆ°á»Ÿng cá»§a sá»‘ lÆ°á»£ng cÃ¡c giáº£i phÃ¡p do mÃ´ hÃ¬nh táº¡o ra, cÃ¡c váº¥n Ä‘á» huáº¥n luyá»‡n, vÃ 
cÃ¡c láº§n láº·p cho tinh chá»‰nh ReSTğ¸ğ‘€. NhÃ¬n chung, cÃ¡c phÃ¡t hiá»‡n cá»§a chÃºng tÃ´i cho tháº¥y tá»± huáº¥n luyá»‡n vá»›i pháº£n há»“i lÃ  má»™t
phÆ°Æ¡ng phÃ¡p Ä‘áº§y há»©a háº¹n Ä‘á»ƒ giáº£m sá»± phá»¥ thuá»™c vÃ o dá»¯ liá»‡u do con ngÆ°á»i táº¡o ra.
CÃ¡c Ä‘Ã³ng gÃ³p chÃ­nh cá»§a cÃ´ng trÃ¬nh nÃ y lÃ :
â€¢ChÃºng tÃ´i giá»›i thiá»‡u ReSTğ¸ğ‘€ cho phÃ©p há»c tá»« dá»¯ liá»‡u tá»± táº¡o cho LLM, sá»­ dá»¥ng má»™t
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 2

--- TRANG 3 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
phÆ°Æ¡ng phÃ¡p expectation-maximization cÃ³ nguyÃªn táº¯c trong khung reinforcement learning.
â€¢ChÃºng tÃ´i chá»©ng minh ráº±ng huáº¥n luyá»‡n trÃªn cÃ¡c giáº£i phÃ¡p tá»± táº¡o vÆ°á»£t trá»™i hÆ¡n huáº¥n luyá»‡n trÃªn cÃ¡c giáº£i phÃ¡p do con ngÆ°á»i táº¡o ra
trong cÃ¡c lÄ©nh vá»±c giáº£i quyáº¿t váº¥n Ä‘á», cháº³ng háº¡n nhÆ° toÃ¡n há»c vÃ  táº¡o mÃ£.
â€¢ThÃ´ng qua cÃ¡c nghiÃªn cá»©u ablation toÃ n diá»‡n, chÃºng tÃ´i xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c cÃ¡c yáº¿u tá»‘ quan trá»ng cáº§n thiáº¿t Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c
hiá»‡u suáº¥t tá»‘i Æ°u.
â€¢CÃ¡c LLM Ä‘Æ°á»£c tinh chá»‰nh vá»›i ReSTğ¸ğ‘€ thá»ƒ hiá»‡n kháº£ nÄƒng chuyá»ƒn giao máº¡nh máº½ trÃªn cÃ¡c nhiá»‡m vá»¥ giá»¯ láº¡i khÃ¡c nhau.
2. Kiáº¿n thá»©c ná»n táº£ng
Má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»± há»“i quy táº¡o ra má»™t chuá»—i Ä‘áº§u ra ğ’š=(ğ‘¦1,ğ‘¦2,....ğ‘¦ğ‘‡) cho trÆ°á»›c má»™t bá»‘i cáº£nh (hoáº·c
Ä‘áº§u vÃ o nguá»“n) ğ’™=(ğ‘¥1,ğ‘¥2,...ğ‘¥ğ¿), trong Ä‘Ã³ cÃ¡c token ğ‘¥ğ‘™,ğ‘¦ğ‘¡ thuá»™c má»™t tá»« vá»±ng cá»‘ Ä‘á»‹nh. Táº¡o ra tá»± há»“i quy
bao gá»“m viá»‡c dá»± Ä‘oÃ¡n cÃ¡c token tá»«ng cÃ¡i má»™t, dá»±a trÃªn cÃ¡c token Ä‘Æ°á»£c táº¡o ra trÆ°á»›c Ä‘Ã³.
Giáº£ sá»­ ráº±ng mÃ´ hÃ¬nh Ä‘Æ°á»£c tham sá»‘ hÃ³a bá»Ÿi ğœƒ, phÃ¢n phá»‘i xÃ¡c suáº¥t cÃ³ Ä‘iá»u kiá»‡n Ä‘á»ƒ táº¡o ra
má»™t chuá»—i ğ’š cho trÆ°á»›c ğ’™ lÃ 
ğ‘ğœƒ(ğ’š|ğ’™)=ğ‘‡Ã–
ğ‘¡=1ğ‘ğœƒ(ğ‘¦ğ‘¡|ğ’š<ğ‘¡,ğ’™),
vá»›i quy Æ°á»›c ğ’š1:0=âˆ… vÃ  ğ’š1:ğ‘¡âˆ’1=(ğ‘¦1,ğ‘¦2,....ğ‘¦ğ‘¡âˆ’1). Äá»ƒ dá»… kÃ½ hiá»‡u, chÃºng tÃ´i Ä‘á»‹nh nghÄ©a ğ‘(ğ‘¦ğ‘¡|ğ‘¥):=
ğ‘(ğ‘¦ğ‘¡|ğ‘¦<ğ‘¡,ğ‘¥). XÃ¡c suáº¥t dá»± Ä‘oÃ¡n token thá»© ğ‘¡, ğ‘¦ğ‘¡, ğ‘(ğ‘¦ğ‘¡|ğ‘¥), Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh báº±ng softmax vá»›i
nhiá»‡t Ä‘á»™ ğ›¾: ğ‘(ğ‘¦ğ‘¡|ğ‘¥)=exp(ğ‘§ğ‘¡/ğ›¾)Ãğ‘€
ğ‘–=1exp(ğ‘§ğ‘–/ğ›¾), trong Ä‘Ã³ ğ‘§ğ‘¡ lÃ  Ä‘iá»ƒm sá»‘ logit cho token ğ‘¦ğ‘¡. GiÃ¡ trá»‹ nhiá»‡t Ä‘á»™ ğ›¾ cao hÆ¡n
Ä‘Æ°a vÃ o nhiá»u tÃ­nh ngáº«u nhiÃªn hÆ¡n, trong khi giÃ¡ trá»‹ tháº¥p hÆ¡n lÃ m cho Ä‘áº§u ra xÃ¡c Ä‘á»‹nh hÆ¡n
báº±ng cÃ¡ch Æ°u tiÃªn cÃ¡c tá»« cÃ³ kháº£ nÄƒng xáº£y ra nháº¥t.
Cho trÆ°á»›c má»™t bá»™ dá»¯ liá»‡u D cá»§a cÃ¡c Ä‘áº§u vÃ o ğ’™ vÃ  cÃ¡c Ä‘áº§u ra do con ngÆ°á»i táº¡o ra ğ’š, tinh chá»‰nh cÃ³ giÃ¡m sÃ¡t (SFT)
huáº¥n luyá»‡n policy báº±ng cÃ¡ch tá»‘i thiá»ƒu hÃ³a hÃ m máº¥t mÃ¡t negative log likelihood:
LSFT(ğœƒ)=âˆ’ğ”¼(ğ’™,ğ’š)âˆ¼D"ğ‘‡âˆ‘ï¸
ğ‘¡=1logğ‘ğœƒ(ğ‘¦ğ‘¡|ğ’š1:ğ‘¡âˆ’1,ğ’™)#
. (1)
ChÃºng tÃ´i cÅ©ng giáº£ sá»­ cÃ³ quyá»n truy cáº­p vÃ o pháº§n thÆ°á»Ÿng cáº¥p Ä‘á»™ chuá»—i xÃ¡c Ä‘á»‹nh (hoáº·c terminal) ğ‘Ÿ(ğ’™,ğ’š). Khi Ä‘Ã³, má»¥c tiÃªu
reinforcement learning (RL) tÆ°Æ¡ng á»©ng vá»›i:
LRL(ğœƒ)=ğ”¼ğ’™âˆ¼D
ğ”¼ğ’šâˆ¼ğ‘ğœƒ(ğ’š|ğ’™)[ğ‘Ÿ(ğ’™,ğ’š)]
.
Tá»‘i Æ°u hÃ³a trá»±c tiáº¿p hÃ m máº¥t mÃ¡t LRL báº±ng cÃ¡c phÆ°Æ¡ng phÃ¡p RL trá»±c tuyáº¿n, cháº³ng háº¡n nhÆ° policy gradients, Ä‘Ã²i há»i cáº­p nháº­t
vÃ  láº¥y máº«u tá»« policy nhiá»u láº§n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Tuy nhiÃªn, chi phÃ­ tÃ­nh toÃ¡n cá»§a
tinh chá»‰nh trÃªn dÃ²ng liÃªn tá»¥c cÃ¡c máº«u má»›i trá»Ÿ thÃ nh má»™t háº¡n cháº¿ cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p trá»±c tuyáº¿n, Ä‘áº·c biá»‡t
khi kÃ­ch thÆ°á»›c cá»§a máº¡ng policy tÄƒng lÃªn hÃ ng chá»¥c hoáº·c hÃ ng trÄƒm tá»· tham sá»‘. ChÃºng tÃ´i tháº£o luáº­n vá» má»™t
giáº£i phÃ¡p thay tháº¿ cho cÃ¡c phÆ°Æ¡ng phÃ¡p RL trá»±c tuyáº¿n nhÆ° váº­y trong pháº§n tiáº¿p theo.
3. Expectation-Maximization cho Reinforced Self-Training
Expectation-Maximization (EM) cho RL ChÃºng tÃ´i trÆ°á»›c tiÃªn mÃ´ táº£ khung EM dá»±a trÃªn RL vá»›i
cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯, dá»±a trÃªn cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y cá»§a Dayan vÃ  Hinton (1997). HÃ£y Ä‘á»‹nh nghÄ©a má»™t biáº¿n tá»‘i Æ°u nhá»‹ phÃ¢n O, sao cho ğ‘(ğ‘‚=1|ğ’™,ğ’š)âˆğ‘“(ğ‘Ÿ(ğ’™,ğ’š)), cho má»™t hÃ m khÃ´ng giáº£m khÃ´ng Ã¢m
ğ‘“:â„â†’â„+. ChÃºng tÃ´i muá»‘n tá»‘i Ä‘a hÃ³a log-likelihood cá»§a viá»‡c quan sÃ¡t ğ‘‚=1 (Ä‘áº¡t Ä‘Æ°á»£c pháº§n thÆ°á»Ÿng cao):
logğ‘(ğ‘‚=1|ğ’™):=logâˆ‘ï¸
ğ’šğ‘ğœƒ(ğ’š|ğ’™)ğ‘(ğ‘‚=1|ğ’™,ğ’š).
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 3

--- TRANG 4 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
Tuy nhiÃªn, tá»•ng trÃªn táº¥t cáº£ cÃ¡c chuá»—i cÃ³ thá»ƒ ğ’š thÆ°á»ng khÃ´ng thá»ƒ tÃ­nh toÃ¡n Ä‘Æ°á»£c. Thay vÃ¬ tá»‘i Ä‘a hÃ³a
logğ‘(ğ‘‚=1;ğ’™), ngÆ°á»i ta cÃ³ thá»ƒ xem xÃ©t tá»‘i Ä‘a hÃ³a ELBO ğ¿(ğ‘ğœƒ,ğ‘) cá»§a nÃ³ vá»›i respect tá»›i cÃ¡c tham sá»‘ ğœƒ vÃ 
phÃ¢n phá»‘i biáº¿n phÃ¢n ğ‘(ğ‘¦|ğ‘¥). Cá»¥ thá»ƒ,
logğ‘(ğ‘‚=1|ğ’™)=logğ”¼ğ‘(ğ’š|ğ’™)ğ‘(ğ‘‚=1|ğ’™,ğ’š)ğ‘ğœƒ(ğ’š|ğ’™)
ğ‘(ğ’š|ğ’™)
â‰¥ğ”¼ğ‘(ğ’š|ğ’™)
logğ‘(ğ‘‚=1|ğ’™,ğ’š)ğ‘ğœƒ(ğ’š|ğ’™)
ğ‘(ğ’š|ğ’™)
(Báº¥t Ä‘áº³ng thá»©c Jensen)
=ğ”¼ğ‘(ğ’š|ğ’™)[logğ‘(ğ‘‚=1|ğ’™,ğ’š)]âˆ’KL[ğ‘(ğ’š|ğ’™)||ğ‘ğœƒ(ğ’š|ğ’™)]
=:ğ¿(ğ‘ğœƒ,ğ‘) (2)
Thuáº­t toÃ¡n EM (Dempster et al., 1977) cho PhÆ°Æ¡ng trÃ¬nh 2 xen káº½ giá»¯a E-step vÃ  M-step:
táº¡i láº§n láº·p ğ‘¡, kÃ½ hiá»‡u tham sá»‘ mÃ´ hÃ¬nh ngÃ´n ngá»¯ lÃ  ğœƒğ‘¡ vÃ  phÃ¢n phá»‘i biáº¿n phÃ¢n lÃ  ğ‘ğ‘¡.
â€¢E-step: ğ‘ğ‘¡+1=arg max ğ‘ğ¿(ğ‘ğœƒğ‘¡,ğ‘). VÃ¬ ğ¿(ğ‘ğœƒğ‘¡,ğ‘) cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t dÆ°á»›i dáº¡ng âˆ’ğ¾ğ¿[ğ‘(ğ’š|ğ’™)||ğ‘âˆ—(ğ’š|ğ’™)],ğ‘ğ‘¡+1(ğ’š|
ğ’™)âˆğ‘âˆ—(ğ’š|ğ’™):=ğ‘(ğ‘‚=1|ğ’™,ğ’š)ğ‘ğœƒğ‘¡(ğ’š|ğ’™). Do Ä‘Ã³, bÆ°á»›c nÃ y tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c cÃ¢n báº±ng cÃ¡c máº«u Ä‘áº§u ra
tá»« phÃ¢n phá»‘i mÃ´ hÃ¬nh ngÃ´n ngá»¯ cÃ³ Ä‘iá»u kiá»‡n dá»±a trÃªn kháº£ nÄƒng cá»§a chÃºng Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c
pháº§n thÆ°á»Ÿng cao.
â€¢M-step: ğœƒğ‘¡+1:=arg max ğœƒğ¿(ğ‘ğœƒ,ğ‘ğ‘¡+1)=arg min ğœƒKL
ğ‘ğ‘¡+1(ğ’š|ğ’™)||ğ‘ğœƒ(ğ’š|ğ’™)
=arg min ğœƒÃ
ğ’šâˆ’ğ‘ğ‘¡+1(ğ’š|
ğ’™)logğ‘ğœƒ(ğ’š|ğ’™). NhÆ° váº­y, bÆ°á»›c nÃ y tÆ°Æ¡ng á»©ng vá»›i viá»‡c tá»‘i Ä‘a hÃ³a hÃ m máº¥t mÃ¡t negative log-likelihood cÃ³ trá»ng sá»‘.
Xen káº½ giá»¯a cÃ¡c bÆ°á»›c trÃªn Ä‘áº£m báº£o cáº£i thiá»‡n Ä‘Æ¡n Ä‘iá»‡u trong ELBO: ğ¿(ğ‘ğœƒğ‘¡+1,ğ‘ğ‘¡+1)â‰¥
ğ¿(ğ‘ğœƒğ‘¡,ğ‘ğ‘¡+1)â‰¥ğ¿(ğ‘ğœƒğ‘¡,ğ‘ğ‘¡).
EM vá»›i pháº§n thÆ°á»Ÿng khÃ´ng Ã¢m. Náº¿u cÃ¡c pháº§n thÆ°á»Ÿng lÃ  khÃ´ng Ã¢m vÃ  ğ‘“ Ä‘Æ°á»£c Ä‘áº·t thÃ nh hÃ m Ä‘á»“ng nháº¥t
, thÃ¬ ğ‘(ğ‘‚=1|ğ’™,ğ’š)âˆğ‘Ÿ(ğ’™,ğ’š) cÃ³ nghÄ©a lÃ  ğ‘ğ‘¡+1(ğ’š|ğ’™)âˆğ‘Ÿ(ğ’™,ğ’š)ğ‘ğœƒğ‘¡(ğ’š|ğ’™). Trong trÆ°á»ng há»£p nÃ y,
cÃ¡c tham sá»‘ policy cáº­p nháº­t ğœƒğ‘¡+1 thu Ä‘Æ°á»£c tá»« M-step táº¡i láº§n láº·p ğ‘¡ Ä‘Æ°á»£c cho bá»Ÿi:
ğœƒğ‘¡+1:=arg max
ğœƒğ”¼ğ‘¥âˆ¼Dh
ğ”¼ğ’šâˆ¼ğ‘ğ‘¡
ğœƒ(ğ’š|ğ’™)[ğ‘Ÿ(ğ’™,ğ’š)logğ‘ğœƒ(ğ’š|ğ’™)]i
. (3)
So sÃ¡nh phÆ°Æ¡ng trÃ¬nh trÃªn vá»›i má»¥c tiÃªu RL thÃ´ng thÆ°á»ng (LRL) tiáº¿t lá»™ sá»± khÃ¡c biá»‡t chÃ­nh
giá»¯a RL tiÃªu chuáº©n vÃ  RL dá»±a trÃªn EM: cÃ¡ch dá»¯ liá»‡u Ä‘áº§u ra Ä‘Æ°á»£c láº¥y máº«u. RL tiÃªu chuáº©n liÃªn tá»¥c
cáº­p nháº­t policy vÃ  sá»­ dá»¥ng policy má»›i nháº¥t nÃ y Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u. NgÆ°á»£c láº¡i, RL dá»±a trÃªn EM sá»­ dá»¥ng má»™t
policy láº¥y máº«u cá»‘ Ä‘á»‹nh tá»« láº§n láº·p trÆ°á»›c, tÃ¡ch rá»i thu tháº­p dá»¯ liá»‡u khá»i tá»‘i Æ°u hÃ³a policy. Sá»± tÃ¡ch rá»i nÃ y
trong cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn EM cho phÃ©p má»Ÿ rá»™ng quy mÃ´ dá»… dÃ ng hÆ¡n Ä‘á»‘i vá»›i cÃ¡c máº¡ng policy lá»›n, cháº³ng háº¡n nhÆ° LLM.
ReSTğ¸ğ‘€ ÄÆ°á»£c thÃºc Ä‘áº©y bá»Ÿi khung EM, giá» Ä‘Ã¢y chÃºng tÃ´i tháº£o luáº­n vá» má»™t phiÃªn báº£n Ä‘Æ¡n giáº£n hÃ³a cá»§a phÆ°Æ¡ng phÃ¡p Reinforced Self-
Training (ReST) cá»§a Gulcehre et al. (2023). PhÆ°Æ¡ng phÃ¡p nÃ y, mÃ  chÃºng tÃ´i gá»i lÃ  ReSTğ¸ğ‘€, tÃ¡ch rá»i
thu tháº­p dá»¯ liá»‡u (E-step) vÃ  tá»‘i Æ°u hÃ³a policy (M-step) trong má»™t pipeline RL thÃ´ng thÆ°á»ng. Thuáº­t toÃ¡n 1 phÃ¡c tháº£o
thuáº­t toÃ¡n ReSTğ¸ğ‘€ vá»›i nhiá»u láº§n láº·p, trong Ä‘Ã³ má»—i láº§n láº·p tÆ°Æ¡ng á»©ng vá»›i má»™t bÆ°á»›c Generate
vÃ  Improve. ChÃºng tÃ´i mÃ´ táº£ cÃ¡c bÆ°á»›c nÃ y chi tiáº¿t bÃªn dÆ°á»›i.
â€¢Generate (E-step): Trong bÆ°á»›c nÃ y, chÃºng tÃ´i táº¡o ra má»™t bá»™ dá»¯ liá»‡u Dğ‘– báº±ng cÃ¡ch láº¥y máº«u nhiá»u chuá»—i Ä‘áº§u ra
tá»« policy hiá»‡n táº¡i ğ‘ğœƒ: Dğ‘–={(ğ’™ğ‘—,ğ’šğ‘—)|ğ‘
ğ‘—=1 s.t. ğ’™ğ‘—âˆ¼D,ğ’šğ‘—âˆ¼ğ‘ğœƒ(ğ’š|ğ’™ğ‘—)}. á» Ä‘Ã¢y, cÃ¡c Ä‘áº§u vÃ o
Ä‘Æ°á»£c láº¥y máº«u láº¡i tá»« bá»™ dá»¯ liá»‡u gá»‘c ğ’™ğ‘—âˆ¼D. CÃ¡c chuá»—i Ä‘áº§u ra trong Dğ‘– sau Ä‘Ã³ Ä‘Æ°á»£c cháº¥m Ä‘iá»ƒm
báº±ng hÃ m pháº§n thÆ°á»Ÿng nhá»‹ phÃ¢n ğ‘Ÿ(ğ’™,ğ’š). Trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, chÃºng tÃ´i Ä‘iá»u kiá»‡n mÃ´ hÃ¬nh ngÃ´n ngá»¯
báº±ng cÃ¡ch sá»­ dá»¥ng prompt few-shot vá»›i cÃ¡c chÆ°Æ¡ng trÃ¬nh cho táº¡o mÃ£ vÃ  cÃ¡c giáº£i phÃ¡p tá»«ng bÆ°á»›c cho cÃ¡c bÃ i toÃ¡n
toÃ¡n há»c.
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 4

--- TRANG 5 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
Thuáº­t toÃ¡n 1: ReST (Expectation-Maximization). Cho trÆ°á»›c má»™t policy ban Ä‘áº§u (vÃ­ dá»¥, pre-trained
LM), ReSTğ¸ğ‘€ láº·p láº¡i Ã¡p dá»¥ng cÃ¡c bÆ°á»›c Generate vÃ  Improve Ä‘á»ƒ cáº­p nháº­t policy.
Input: D: Bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n, Dğ‘£ğ‘ğ‘™: Bá»™ dá»¯ liá»‡u xÃ¡c thá»±c, L(ğ’™,ğ’š;ğœƒ): hÃ m máº¥t mÃ¡t, ğ‘Ÿ(ğ’™,ğ’š): HÃ m pháº§n thÆ°á»Ÿng khÃ´ng Ã¢m, ğ¼: sá»‘ láº§n láº·p, ğ‘: sá»‘ máº«u trÃªn má»—i bá»‘i cáº£nh
for ğ‘–=1 to ğ¼ do
// Generate (E-step)
Táº¡o ra bá»™ dá»¯ liá»‡u Dğ‘– báº±ng cÃ¡ch láº¥y máº«u: Dğ‘–={(ğ’™ğ‘—,ğ’šğ‘—)|ğ‘
ğ‘—=1 s.t. ğ’™ğ‘—âˆ¼D,ğ’šğ‘—âˆ¼ğ‘ğœƒ(ğ’š|ğ’™ğ‘—)}
ChÃº thÃ­ch Dğ‘– vá»›i pháº§n thÆ°á»Ÿng ğ‘Ÿ(ğ’™,ğ’š).
// Improve (M-step)
while pháº§n thÆ°á»Ÿng cáº£i thiá»‡n trÃªn Dğ‘£ğ‘ğ‘™ do
Tá»‘i Æ°u hÃ³a ğœƒ Ä‘á»ƒ tá»‘i Ä‘a hÃ³a má»¥c tiÃªu: ğ½(ğœƒ)=ğ”¼(ğ’™,ğ’š)âˆ¼Dğ‘–[ğ‘Ÿ(ğ’™,ğ’š)logğ‘ğœƒ(ğ’š|ğ’™)]
end
end
Output: Policy ğ‘ğœƒ
â€¢Improve (M-step): Trong láº§n láº·p thá»© ğ‘–, chÃºng tÃ´i sá»­ dá»¥ng bá»™ dá»¯ liá»‡u má»›i Dğ‘– tá»« bÆ°á»›c Generate Ä‘á»ƒ
tinh chá»‰nh policy ğ‘ğœƒ. Äá»ƒ giáº£m thiá»ƒu overfitting cá»¥ thá»ƒ cho nhiá»‡m vá»¥, chÃºng tÃ´i tá»‘i thiá»ƒu hÃ³a sá»± lá»‡ch khá»i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ
báº±ng cÃ¡ch luÃ´n tinh chá»‰nh mÃ´ hÃ¬nh ngÃ´n ngá»¯ pretrained cÆ¡ sá»Ÿ. Äá»‘i vá»›i tinh chá»‰nh, chÃºng tÃ´i tá»‘i thiá»ƒu hÃ³a
hÃ m máº¥t mÃ¡t negative log-likelihood cÃ³ trá»ng sá»‘ pháº§n thÆ°á»Ÿng ğ½(ğœƒ)=ğ”¼(ğ’™,ğ’š)âˆ¼Dğ‘–[ğ‘Ÿ(ğ’™,ğ’š)logğ‘ğœƒ(ğ’š|ğ’™)]. Má»™t khi
policy Ä‘Æ°á»£c cáº£i thiá»‡n, má»™t bá»™ dá»¯ liá»‡u má»›i vá»›i cÃ¡c máº«u cháº¥t lÆ°á»£ng tá»‘t hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c táº¡o ra má»™t láº§n ná»¯a.
Sá»± khÃ¡c biá»‡t vá»›i ReST (Gulcehre et al., 2023). KhÃ´ng giá»‘ng nhÆ° ReST, chÃºng tÃ´i kiá»m cháº¿ viá»‡c bá»• sung cÃ¡c Ä‘áº§u ra do con ngÆ°á»i táº¡o ra vÃ o Dğ‘– trong
bÆ°á»›c Generate vÃ¬ dá»¯ liá»‡u nhÆ° váº­y cÃ³ thá»ƒ khÃ´ng pháº£i lÃºc nÃ o cÅ©ng tá»‘i Æ°u cho viá»‡c há»c
hoáº·c cÃ³ thá»ƒ khÃ´ng dá»… dÃ ng cÃ³ sáºµn. HÆ¡n ná»¯a, má»—i bÆ°á»›c Improve tinh chá»‰nh mÃ´ hÃ¬nh cÆ¡ sá»Ÿ thay vÃ¬
mÃ´ hÃ¬nh thu Ä‘Æ°á»£c tá»« láº§n láº·p ReST trÆ°á»›c Ä‘Ã³. Äiá»u nÃ y dáº«n Ä‘áº¿n hiá»‡u suáº¥t cá»¥ thá»ƒ cho nhiá»‡m vá»¥ tÆ°Æ¡ng Ä‘Æ°Æ¡ng
nhÆ°ng hiá»‡u suáº¥t chuyá»ƒn giao tá»‘t hÆ¡n nhiá»u trÃªn cÃ¡c nhiá»‡m vá»¥ giá»¯ láº¡i (xem HÃ¬nh 7).
Nháº­n xÃ©t. CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i táº­p trung vÃ o cÃ¡c cÃ i Ä‘áº·t giáº£i quyáº¿t váº¥n Ä‘á» vá»›i pháº§n thÆ°á»Ÿng nhá»‹ phÃ¢n (hoáº·c 0 hoáº·c 1),
khÃ´ng giá»‘ng nhÆ° cÃ¡c pháº§n thÆ°á»Ÿng giÃ¡ trá»‹ thá»±c cÃ³ giá»›i háº¡n Ä‘Æ°á»£c giáº£ sá»­ bá»Ÿi Gulcehre et al. (2023). Cá»¥ thá»ƒ, Ä‘á»‘i vá»›i má»—i
bÆ°á»›c Generate, Gulcehre et al. (2023) thá»±c hiá»‡n nhiá»u bÆ°á»›c Improve, trong Ä‘Ã³ má»—i bÆ°á»›c Improve
cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° má»™t M-step vá»›i hÃ m ğ‘“(ğ‘Ÿ(ğ’™,ğ’š))=ğ‘Ÿ(ğ’™,ğ’š)> ğœ, trong Ä‘Ã³ ğœâˆˆâ„+ tÄƒng trong
cÃ¡c M-step liÃªn tiáº¿p. Tuy nhiÃªn, vá»›i pháº§n thÆ°á»Ÿng nhá»‹ phÃ¢n, báº¥t ká»³ giÃ¡ trá»‹ nÃ o cá»§a ğœâˆˆ(0,1) tÆ°Æ¡ng á»©ng vá»›i cÃ¡c
bÆ°á»›c Improve giá»‘ng há»‡t nhau.
4. CÃ´ng trÃ¬nh liÃªn quan
Má»™t sá»‘ phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»ƒ hiá»‡n báº±ng khung expectation-maximization Ä‘Æ°á»£c trÃ¬nh bÃ y
trong Pháº§n 3. ChÃºng tÃ´i tháº£o luáº­n vá» cÃ¡c phÆ°Æ¡ng phÃ¡p vÃ  má»‘i quan há»‡ cá»§a chÃºng vá»›i ReSTğ¸ğ‘€ trong pháº§n nÃ y.
â€¢Expert Iteration (ExiT) (Anthony et al., 2017) xen káº½ giá»¯a hai bÆ°á»›c: cáº£i thiá»‡n expert
vÃ  chÆ°ng cáº¥t policy. Trong bÆ°á»›c cáº£i thiá»‡n expert (E-step), chÃºng tÃ´i káº¿t há»£p má»™t policy cÆ¡ sá»Ÿ
vá»›i má»™t thá»§ tá»¥c tÃ¬m kiáº¿m Ä‘á»ƒ táº¡o ra cÃ¡c máº«u tá»« má»™t policy tá»‘t hÆ¡n, Ä‘Æ°á»£c gá»i lÃ  expert policy.
Sau Ä‘Ã³, trong bÆ°á»›c chÆ°ng cáº¥t policy (M-step), chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c máº«u expert nÃ y Ä‘á»ƒ huáº¥n luyá»‡n policy cÆ¡ sá»Ÿ
má»™t cÃ¡ch cÃ³ giÃ¡m sÃ¡t, cáº£i thiá»‡n nÃ³ má»™t cÃ¡ch hiá»‡u quáº£ Ä‘á»ƒ khá»›p vá»›i expert policy. Trong khi ExiT sá»­ dá»¥ng
monte-carlo tree-search, chÃºng tÃ´i Ä‘Æ¡n giáº£n sá»­ dá»¥ng temperature sampling Ä‘á»ƒ thu tháº­p cÃ¡c máº«u tá»«
expert policy trong ReST. Tuy nhiÃªn, viá»‡c cáº£i thiá»‡n E-step trong ReST báº±ng khung ExIT thÃ´ng qua
cÃ¡c thá»§ tá»¥c tÃ¬m kiáº¿m vÃ  láº­p káº¿ hoáº¡ch vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ sáº½ thÃº vá»‹ cho nghiÃªn cá»©u tÆ°Æ¡ng lai. VÃ­ dá»¥, Huang et al. (2022) thá»±c hiá»‡n má»™t láº§n láº·p duy nháº¥t cá»§a ReSTğ¸ğ‘€ trÃªn cÃ¡c bÃ i toÃ¡n lÃ½ luáº­n toÃ¡n há»c Ä‘Æ¡n giáº£n
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 5

--- TRANG 6 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
. Tuy nhiÃªn, khÃ´ng giá»‘ng nhÆ° cÃ i Ä‘áº·t cá»§a chÃºng tÃ´i, há» khÃ´ng giáº£ sá»­ cÃ³ quyá»n truy cáº­p vÃ o pháº§n thÆ°á»Ÿng tÃ­nh Ä‘Ãºng Ä‘áº¯n vÃ 
thay vÃ o Ä‘Ã³ sá»­ dá»¥ng bá» phiáº¿u Ä‘a sá»‘ (Wang et al., 2023) nhÆ° má»™t thá»§ tá»¥c tÃ¬m kiáº¿m trong E-step.
â€¢Self-Taught Reasoner (STaR) (Zelikman et al., 2022) sá»­ dá»¥ng greedy decoding thay vÃ¬
temperature sampling cho E-step trong ReSTğ¸ğ‘€, giá»›i háº¡n trong má»™t giáº£i phÃ¡p do mÃ´ hÃ¬nh táº¡o ra
cho má»—i bÃ i toÃ¡n trong quÃ¡ trÃ¬nh thu tháº­p dá»¯ liá»‡u. NgoÃ i ra, STaR Ä‘á» xuáº¥t rationalization nhÆ° má»™t
giáº£i phÃ¡p thay tháº¿ cho temperature sampling, trong Ä‘Ã³ mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c cung cáº¥p vá»›i cÃ¢u tráº£ lá»i Ä‘Ãºng
nhÆ° má»™t pháº§n cá»§a Ä‘áº§u vÃ o Ä‘á»ƒ táº¡o ra cÃ¡c giáº£i phÃ¡p Ä‘Ãºng cho cÃ¡c bÃ i toÃ¡n khÃ³. Tuy nhiÃªn, trong cÃ¡c thÃ­ nghiá»‡m
sÆ¡ bá»™ cá»§a chÃºng tÃ´i, rationalization dáº«n Ä‘áº¿n sá»± gia tÄƒng Ä‘Ã¡ng ká»ƒ trong cÃ¡c giáº£i phÃ¡p dÆ°Æ¡ng tÃ­nh giáº£
dáº«n Ä‘áº¿n cÃ¢u tráº£ lá»i Ä‘Ãºng nhÆ°ng vá»›i lÃ½ luáº­n sai.
â€¢Rejection Sampling Fine-tuning (RFT) (Yuan et al., 2023) cáº£i thiá»‡n hiá»‡u suáº¥t lÃ½ luáº­n
trÃªn GSM8K vÃ  tÆ°Æ¡ng á»©ng vá»›i viá»‡c cháº¡y má»™t generate (E-step) vÃ  improve (M-step) duy nháº¥t cá»§a
ReSTğ¸ğ‘€. Trong khi RFT chá»©ng minh cáº£i thiá»‡n hiá»‡u suáº¥t háº¡n cháº¿ trÃªn GSM8K vá»›i viá»‡c tÄƒng
kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯, ReSTğ¸ğ‘€ Ä‘áº¡t Ä‘Æ°á»£c lá»£i Ã­ch lá»›n hÆ¡n trÃªn cÃ¡c benchmark APPS vÃ  MATH thá»­ thÃ¡ch hÆ¡n
khi má»Ÿ rá»™ng quy mÃ´ kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh PaLM 2. HÆ¡n ná»¯a, chÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng viá»‡c sá»­ dá»¥ng nhiá»u
láº§n láº·p cá»§a ReSTğ¸ğ‘€ dáº«n Ä‘áº¿n lá»£i Ã­ch hiá»‡u suáº¥t lá»›n hÆ¡n.
â€¢Iterative Maximum Likelihood (IML) tá»‘i Æ°u hÃ³a má»™t policy báº±ng cÃ¡ch sá»­ dá»¥ng má»¥c tiÃªu
log-likelihood cÃ³ trá»ng sá»‘ pháº§n thÆ°á»Ÿng trÃªn dá»¯ liá»‡u tá»± thu tháº­p. IML Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh lÃ  hoáº¡t Ä‘á»™ng tá»‘t vá»›i
cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ quy mÃ´ tÆ°Æ¡ng Ä‘á»‘i nhá» cho phÃ¢n tÃ­ch ngá»¯ nghÄ©a (Agarwal et al., 2019; Liang et al., 2016), dá»‹ch mÃ¡y
(Wu et al., 2016) vÃ  lÃ½ luáº­n toÃ¡n há»c Ä‘Æ¡n giáº£n (Ni et al., 2022). Má»—i E-step vÃ 
M-step trong IML Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn má»™t mini-batch cÃ¡c vÃ­ dá»¥ huáº¥n luyá»‡n thay vÃ¬ toÃ n bá»™ bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n
, nhÆ° Ä‘Æ°á»£c thá»±c hiá»‡n trong ReSTğ¸ğ‘€. Trong IML, policy Ä‘Ã£ há»c cÃ³ thá»ƒ phÃ¢n ká»³ Ä‘Ã¡ng ká»ƒ khá»i mÃ´ hÃ¬nh
pretrained ban Ä‘áº§u, Ä‘iá»u nÃ y cÃ³ thá»ƒ biá»ƒu hiá»‡n nhÆ° overfitting cá»¥ thá»ƒ cho nhiá»‡m vá»¥, trong Ä‘Ã³ mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng
tá»‘t trÃªn nhiá»‡m vá»¥ má»¥c tiÃªu nhÆ°ng máº¥t kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cho cÃ¡c nhiá»‡m vá»¥ hoáº·c lÄ©nh vá»±c khÃ¡c. NgoÃ i ra,
tÃ­nh cháº¥t káº¿t há»£p cháº·t cháº½ cá»§a thu tháº­p dá»¯ liá»‡u vÃ  tá»‘i Æ°u hÃ³a policy trong IML dáº«n Ä‘áº¿n chi phÃ­
tÃ­nh toÃ¡n cao vá»›i cÃ¡c LM lá»›n, lÃ m cho nÃ³ Ä‘áº¯t Ä‘á» hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i ReSTğ¸ğ‘€.
â€¢Reward weighted regression (RWR) (Peters vÃ  Schaal, 2007) tÆ°Æ¡ng á»©ng vá»›i EM trong Ä‘Ã³
chÃºng tÃ´i Ä‘áº·t ğ‘(ğ‘‚=1|ğ’™,ğ’š)âˆexp(ğ‘Ÿ(ğ’™,ğ’š)) trong Pháº§n 3. RWR trÆ°á»›c Ä‘Ã¢y Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng cho Ä‘iá»u khiá»ƒn robot
, vÃ¬ nÃ³ cÃ³ thá»ƒ dá»… dÃ ng Ã¡p dá»¥ng cho cÃ¡c hÃ m pháº§n thÆ°á»Ÿng khÃ´ng nhá»‹ phÃ¢n. Norouzi et al. (2016) dá»±a trÃªn
RWR Ä‘á»ƒ Ä‘á» xuáº¥t má»™t biáº¿n thá»ƒ tá»•ng quÃ¡t cá»§a IML cho dá»‹ch mÃ¡y.
â€¢Reward ranked fine-tuning (RAFT) (Dong et al., 2023) cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch nhÆ° viá»‡c xen káº½
giá»¯a E-step vÃ  M-step trÃªn cÃ¡c mini-batch, trong Ä‘Ã³ E-step sá»­ dá»¥ng máº«u Ä‘áº§u ra vá»›i
pháº§n thÆ°á»Ÿng tá»‘i Ä‘a cho má»—i bá»‘i cáº£nh Ä‘áº§u vÃ o. Äá»‘i vá»›i cÃ¡c hÃ m pháº§n thÆ°á»Ÿng nhá»‹ phÃ¢n, RAFT tÆ°Æ¡ng tá»± nhÆ°
IML vÃ  nhÆ° váº­y, cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° má»™t thá»ƒ hiá»‡n cá»§a ReSTğ¸ğ‘€.
CÃ¡c cÃ´ng trÃ¬nh liÃªn quan khÃ¡c: TRICE (Phan et al., 2023) Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p dá»±a trÃªn EM Ä‘á»ƒ tá»‘i Ä‘a hÃ³a
marginal log-likelihood (MML) cá»§a viá»‡c táº¡o ra má»™t cÃ¢u tráº£ lá»i Ä‘Ãºng cho má»™t bÃ i toÃ¡n lÃ½ luáº­n, trong Ä‘Ã³
chuá»—i lÃ½ luáº­n chain-of-thought Ä‘Æ°á»£c coi nhÆ° má»™t biáº¿n tiá»m áº©n. Trong khi E-step trong ReSTğ¸ğ‘€ Ä‘Æ¡n giáº£n tÆ°Æ¡ng á»©ng
vá»›i láº¥y máº«u tá»« mÃ´ hÃ¬nh vÃ  lá»c báº±ng pháº§n thÆ°á»Ÿng nhá»‹ phÃ¢n, TRICE sá»­ dá»¥ng Markov-chain Monte
Carlo vá»›i má»™t control variate Ä‘á»ƒ xáº¥p xá»‰ gradient MML. Sordoni et al. (2023) Ä‘á» xuáº¥t má»™t
phÆ°Æ¡ng phÃ¡p dá»±a trÃªn EM khÃ´ng cÃ³ gradient, tÆ°Æ¡ng tá»± nhÆ° RAFT, cho tá»‘i Æ°u hÃ³a prompt cho cÃ¡c LLM Ä‘Ã´ng láº¡nh.
ÄÆ°á»£c truyá»n cáº£m há»©ng bá»Ÿi phiÃªn báº£n trÆ°á»›c cá»§a báº£n tháº£o nÃ y, Agarwal et al. (2024) nghiÃªn cá»©u liá»‡u dá»¯ liá»‡u do mÃ´ hÃ¬nh táº¡o ra
cÃ³ thá»ƒ vÆ°á»£t trá»™i hÆ¡n dá»¯ liá»‡u con ngÆ°á»i cho prompting few-shot vÃ  many-shot hay khÃ´ng. Há» tháº¥y ráº±ng
Ä‘Ã¢y thá»±c sá»± lÃ  trÆ°á»ng há»£p, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i prompting few-shot.
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 6

--- TRANG 7 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
ReSTğ¸ğ‘€ ReST STaR RFT
Báº¯t Ä‘áº§u tá»« mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh âœ— âœ“ âœ— âœ—
Tinh chá»‰nh tá»« mÃ´ hÃ¬nh cÆ¡ sá»Ÿ trong má»—i láº§n láº·p âœ“ âœ— âœ“ N/A
Sá»­ dá»¥ng rationalization cho cÃ¢u há»i chÆ°a giáº£i quyáº¿t âœ— âœ— âœ“ âœ—
Temperature sampling cho khÃ¡m phÃ¡ âœ“ âœ“ âœ— âœ“
ThÃ­ nghiá»‡m vá»›i LM lá»›n âœ“ âœ— âœ— âœ“
Nhiá»u láº§n láº·p âœ“ âœ“ âœ“ âœ—
Lá»£i Ã­ch lá»›n hÆ¡n trÃªn cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n âœ“ N/A N/A âœ—
ÄÃ¡nh giÃ¡ trÃªn cÃ¡c nhiá»‡m vá»¥ giá»¯ láº¡i âœ“ âœ— âœ— âœ—
Báº£ng 1|Sá»± khÃ¡c biá»‡t giá»¯a ReSTğ¸ğ‘€ vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p liÃªn quan khÃ¡c sá»­ dá»¥ng dá»¯ liá»‡u tá»•ng há»£p
Ä‘á»ƒ nÃ¢ng cao kháº£ nÄƒng cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯.
5. ThÃ­ nghiá»‡m vÃ  phÃ¢n tÃ­ch
Má»¥c tiÃªu cá»§a cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i lÃ  tráº£ lá»i cÃ¡c cÃ¢u há»i sau:
1. ReSTğ¸ğ‘€ hiá»‡u quáº£ nhÆ° tháº¿ nÃ o so vá»›i tinh chá»‰nh trÃªn dá»¯ liá»‡u do con ngÆ°á»i táº¡o ra?
2. Cáº§n bao nhiÃªu láº§n láº·p Ä‘á»ƒ cÃ³ hiá»‡u suáº¥t tá»‘i Æ°u? ReSTğ¸ğ‘€ dáº«n Ä‘áº¿n overfitting trÃªn táº­p huáº¥n luyá»‡n nhanh nhÆ° tháº¿ nÃ o?
3. ReSTğ¸ğ‘€ áº£nh hÆ°á»Ÿng nhÆ° tháº¿ nÃ o Ä‘áº¿n hiá»‡u suáº¥t pass@k vÃ  bá» phiáº¿u Ä‘a sá»‘?
4. Náº¿u chÃºng tÃ´i tinh chá»‰nh báº±ng dá»¯ liá»‡u do mÃ´ hÃ¬nh táº¡o ra trÃªn má»™t nhiá»‡m vá»¥ cá»¥ thá»ƒ, chÃºng tÃ´i cÃ³ tháº¥y chuyá»ƒn giao tÃ­ch cá»±c
sang cÃ¡c nhiá»‡m vá»¥ liÃªn quan khÃ´ng? CÃ³ báº¥t ká»³ sá»± suy giáº£m hiá»‡u suáº¥t nÃ o so vá»›i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ khi
Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh cá»§a chÃºng tÃ´i trÃªn má»™t bá»™ nhiá»‡m vá»¥ rá»™ng khÃ´ng?
5. ChÃºng tÃ´i cáº§n bao nhiÃªu dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c háº§u háº¿t cÃ¡c lá»£i Ã­ch hiá»‡u suáº¥t tá»« ReSTğ¸ğ‘€? Má»™t láº§n láº·p cá»§a ReSTğ¸ğ‘€ cÃ³ Ä‘á»§ khÃ´ng?
Bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ ReSTğ¸ğ‘€ chá»§ yáº¿u trÃªn giáº£i quyáº¿t váº¥n Ä‘á» toÃ¡n há»c báº±ng cÃ¡ch sá»­ dá»¥ng bá»™ dá»¯ liá»‡u
MATH cá»§a Hendrycks (Hendrycks et al., 2021b) vÃ  táº¡o mÃ£ báº±ng cÃ¡ch sá»­ dá»¥ng bá»™ dá»¯ liá»‡u APPS (Introductory)
(Hendrycks et al., 2021a). MATH vÃ  APPS (Introductory) chá»©a 7500 vÃ  2342 váº¥n Ä‘á» huáº¥n luyá»‡n
tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i chá»n cÃ¡c nhiá»‡m vá»¥ nÃ y vÃ¬ cÃ¡c Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ tá»± Ä‘á»™ng
lÃ  Ä‘Ãºng hay sai, hoÃ n toÃ n phÃ¹ há»£p vá»›i ReSTğ¸ğ‘€. Cáº£ hai bá»™ dá»¯ liá»‡u nÃ y Ä‘á»u cung cáº¥p pháº§n thÆ°á»Ÿng nhá»‹ phÃ¢n: trÃªn
MATH, cÃ¡c cÃ¢u tráº£ lá»i do mÃ´ hÃ¬nh táº¡o ra cÃ³ thá»ƒ dá»… dÃ ng Ä‘Æ°á»£c xÃ¡c minh tÃ­nh Ä‘Ãºng Ä‘áº¯n báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¢u tráº£ lá»i ground-truth,
trong khi trÃªn APPS, cÃ¡c test case xÃ¡c Ä‘á»‹nh liá»‡u mÃ£ Ä‘Æ°á»£c táº¡o ra cÃ³ Ä‘Ãºng hay khÃ´ng.
CÃ¡c mÃ´ hÃ¬nh. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh PaLM 2 (Google et al., 2023) vá»›i cÃ¡c API cÃ´ng khai trÃªn Google Cloud cho
cÃ¡c thÃ­ nghiá»‡m, bao gá»“m PaLM 2-S (Bison), PaLM 2-S* (Codey), vÃ  PaLM 2-L (Unicorn).
ÄÃ¡nh giÃ¡. ChÃºng tÃ´i bÃ¡o cÃ¡o hiá»‡u suáº¥t tá»•ng quÃ¡t hÃ³a báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c phÃ¢n chia thá»­ nghiá»‡m cá»§a cÃ¡c bá»™ dá»¯ liá»‡u MATH vÃ  APPS
(Introductory). Äá»ƒ Ä‘o lÆ°á»ng hiá»‡u suáº¥t chuyá»ƒn giao, chÃºng tÃ´i xem xÃ©t cÃ¡c bá»™ dá»¯ liá»‡u GSM8K (Cobbe et al., 2021),
Hungarian HS finals (Paster, 2023), vÃ  HumanEval (Chen et al., 2021). ChÃºng tÃ´i cÅ©ng Ä‘Ã¡nh giÃ¡
cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i báº±ng cÃ¡ch sá»­ dá»¥ng benchmark Big-Bench Hard (Suzgun et al., 2022) Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng chung.
Táº¥t cáº£ cÃ¡c Ä‘Ã¡nh giÃ¡ tuÃ¢n theo cÃ¡c cÃ i Ä‘áº·t tá»« Google et al. (2023), trá»« khi Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh khÃ¡c.
Chi tiáº¿t triá»ƒn khai. Trong má»—i láº§n láº·p cá»§a ReSTğ¸ğ‘€, chÃºng tÃ´i táº¡o ra má»™t sá»‘ lÆ°á»£ng giáº£i phÃ¡p cá»‘ Ä‘á»‹nh
cho má»—i bÃ i toÃ¡n cho E-step: 32 cho bá»™ dá»¯ liá»‡u MATH vÃ  64 cho bá»™ dá»¯ liá»‡u APPS. Äá»ƒ
táº¡o ra cÃ¡c giáº£i phÃ¡p, chÃºng tÃ´i láº¥y máº«u tá»« mÃ´ hÃ¬nh ngÃ´n ngá»¯ báº±ng cÃ¡ch sá»­ dá»¥ng top-K sampling vá»›i K=40 vÃ 
nhiá»‡t Ä‘á»™ 0.7. Tuy nhiÃªn, viá»‡c sá»­ dá»¥ng trá»±c tiáº¿p táº¥t cáº£ cÃ¡c giáº£i phÃ¡p do mÃ´ hÃ¬nh táº¡o ra nÃ y cÃ³ thá»ƒ dáº«n Ä‘áº¿n má»™t
bá»™ dá»¯ liá»‡u khÃ´ng cÃ¢n báº±ng, vÃ¬ chÃºng tÃ´i sáº½ cÃ³ nhiá»u giáº£i phÃ¡p Ä‘Ãºng hÆ¡n cho cÃ¡c bÃ i toÃ¡n dá»… hÆ¡n. Äá»ƒ giáº£m thiá»ƒu
Ä‘iá»u nÃ y, chÃºng tÃ´i Ä‘Ã£ Ä‘Æ°a ra má»™t ngÆ°á»¡ng cáº¯t cho sá»‘ lÆ°á»£ng giáº£i phÃ¡p tá»‘i Ä‘a cho má»—i bÃ i toÃ¡n, má»™t lá»±a chá»n thiáº¿t káº¿
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 7

--- TRANG 8 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
0 1 2 3
Sá»‘ láº§n láº·p2025303540Pass@1 Äá»™ ChÃ­nh XÃ¡c Thá»­ Nghiá»‡m (%)Hendrycks MATH
Palm-2-S Palm-2-L Palm-2-L-SFT Palm-2-S-SFT
0 1 2 3
Sá»‘ láº§n láº·p 607080Pass@1 Äá»™ ChÃ­nh XÃ¡c Thá»­ Nghiá»‡m (%)Chuyá»ƒn giao sang GSM8K
HÃ¬nh 2|ReSTğ¸ğ‘€ cho giáº£i quyáº¿t váº¥n Ä‘á» toÃ¡n há»c. Hiá»‡u suáº¥t thá»­ nghiá»‡m trÃªn MATH vÃ  GSM8K (chuyá»ƒn giao) cho
PaLM 2-S* vÃ  PaLM 2-L nhÆ° má»™t hÃ m cá»§a cÃ¡c láº§n láº·p ReSTğ¸ğ‘€. ChÃºng tÃ´i cÅ©ng bÃ¡o cÃ¡o hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh
Ä‘Æ°á»£c tinh chá»‰nh thÃ´ng qua SFT trÃªn dá»¯ liá»‡u do con ngÆ°á»i táº¡o ra nhÆ° má»™t baseline. Láº§n láº·p 0 tÆ°Æ¡ng á»©ng vá»›i hiá»‡u suáº¥t mÃ´ hÃ¬nh pretrained
. Theo Google et al. (2023), chÃºng tÃ´i sá»­ dá»¥ng greedy decoding cho Ä‘Ã¡nh giÃ¡.
0 1 2
Sá»‘ láº§n láº·p1820222426Pass@1 Äá»™ ChÃ­nh XÃ¡c Thá»­ Nghiá»‡m (%)APPS (Introductory)
Palm-2-S* Palm-2-L Palm-2-L-SFT Palm-2-S*-SFT
0 1 2
Sá»‘ láº§n láº·p 40455055Pass@1 Äá»™ ChÃ­nh XÃ¡c Thá»­ Nghiá»‡m (%)Chuyá»ƒn giao sang HumanEval
HÃ¬nh 3|ReSTğ¸ğ‘€ cho táº¡o mÃ£. Hiá»‡u suáº¥t thá»­ nghiá»‡m trÃªn APPS (introductory) vÃ  Hu-
manEval (chuyá»ƒn giao) cho PaLM 2-S* vÃ  PaLM 2-L nhÆ° má»™t hÃ m cá»§a cÃ¡c láº§n láº·p ReSTğ¸ğ‘€.
cÅ©ng Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi Zelikman et al. (2022), bao gá»“m trong bá»™ dá»¯ liá»‡u tinh chá»‰nh: 10 cho cáº£ MATH
vÃ  APPS. PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘áº£m báº£o tÃ­nh Ä‘a dáº¡ng trong dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  báº£o vá»‡ khá»i overfitting
trÃªn cÃ¡c bÃ i toÃ¡n dá»… hÆ¡n. Äá»ƒ tinh chá»‰nh, chÃºng tÃ´i sá»­ dá»¥ng prompt few-shot (vÃ  cÃ¢u há»i) nhÆ° Ä‘áº§u vÃ o cho
mÃ´ hÃ¬nh, vÃ  sá»­ dá»¥ng cÃ¡c giáº£i phÃ¡p do mÃ´ hÃ¬nh táº¡o ra nhÆ° má»¥c tiÃªu. ChÃºng tÃ´i chá»‰ Ã¡p dá»¥ng hÃ m máº¥t mÃ¡t dá»± Ä‘oÃ¡n token tiáº¿p theo
(PhÆ°Æ¡ng trÃ¬nh 1) trÃªn cÃ¡c má»¥c tiÃªu.
5.1. ReSTğ¸ğ‘€ trÃªn MATH vÃ  APPS
HÃ¬nh 2 vÃ  3 cho tháº¥y hiá»‡u suáº¥t cá»§a ReSTğ¸ğ‘€ khi Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c bá»™ dá»¯ liá»‡u MATH vÃ  APPS,
tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i tháº¥y ráº±ng MATH hÆ°á»Ÿng lá»£i tá»« viá»‡c thá»±c hiá»‡n nhiá»u láº§n láº·p cá»§a ReSTğ¸ğ‘€, cáº£ vá» máº·t
hiá»‡u suáº¥t trÃªn táº­p thá»­ nghiá»‡m MATH, cÅ©ng nhÆ° chuyá»ƒn giao sang GSM8K. Máº·t khÃ¡c, chÃºng tÃ´i tháº¥y ráº±ng
háº§u háº¿t cÃ¡c lá»£i Ã­ch cho APPS Ä‘áº¿n tá»« láº§n láº·p Ä‘áº§u tiÃªn, vÃ  nhiá»u láº§n láº·p hÆ¡n dáº«n Ä‘áº¿n sá»± suy giáº£m trÃªn
cáº£ APPS vÃ  HumanEval.
ThÃº vá»‹, HÃ¬nh 2 vÃ  3 chá»©ng minh ráº±ng tinh chá»‰nh trÃªn cÃ¡c giáº£i phÃ¡p do mÃ´ hÃ¬nh táº¡o ra vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 8

--- TRANG 9 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
0 1 2 3
Sá»‘ láº§n láº·p 354045505560Pass@1 Hiá»‡u Suáº¥t (%)Hendrycks MATH
Palm-2-L (Train) Palm-2-L (Test)
0 1 2
Sá»‘ láº§n láº·p 2030405060Pass@1 Hiá»‡u Suáº¥t (%)APPS (Introductory)
Palm-2-L (Train) Palm-2-L (Test)
HÃ¬nh 4|Khoáº£ng cÃ¡ch hiá»‡u suáº¥t train-test trÃªn (trÃ¡i) MATH vá»›i PaLM-2-L, vÃ  (pháº£i) APPS vá»›i PaLM-
2-S*, nhÆ° má»™t hÃ m cá»§a cÃ¡c láº§n láº·p ReSTğ¸ğ‘€.
so vá»›i viá»‡c sá»­ dá»¥ng cÃ¡c giáº£i phÃ¡p do con ngÆ°á»i viáº¿t, Ä‘áº·c biá»‡t Ä‘á»‘i vá»›i mÃ´ hÃ¬nh PaLM 2-L. Äiá»u nÃ y phÃ¹ há»£p vá»›i
cÃ¡c phÃ¡t hiá»‡n cá»§a Yuan et al. (2023) vÃ  cÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y vá» chÆ°ng cáº¥t LLM báº±ng dá»¯ liá»‡u do mÃ´ hÃ¬nh táº¡o ra (Agarwal
et al., 2023; Gu et al., 2023). Tuy nhiÃªn, khÃ´ng giá»‘ng nhÆ° Yuan et al. (2023), ngÆ°á»i quan sÃ¡t tháº¥y lá»£i Ã­ch giáº£m dáº§n
tá»« dá»¯ liá»‡u do mÃ´ hÃ¬nh táº¡o ra trÃªn GSM8K khi má»Ÿ rá»™ng quy mÃ´ kháº£ nÄƒng mÃ´ hÃ¬nh, káº¿t quáº£ cá»§a chÃºng tÃ´i cho tháº¥y xu hÆ°á»›ng ngÆ°á»£c láº¡i
: ReSTğ¸ğ‘€ dáº«n Ä‘áº¿n lá»£i Ã­ch hiá»‡u suáº¥t lá»›n hÆ¡n khi kháº£ nÄƒng mÃ´ hÃ¬nh tÄƒng lÃªn. TrÃªn bá»™ dá»¯ liá»‡u MATH,
cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c thá»­ nghiá»‡m vá»›i ReSTğ¸ğ‘€ lÃ  5.94% cho PaLM 2-S so vá»›i 6.34% cho mÃ´ hÃ¬nh
PaLM 2-L lá»›n hÆ¡n. TÆ°Æ¡ng tá»±, trÃªn bá»™ dá»¯ liá»‡u APPS, cÃ¡c cáº£i thiá»‡n lÃ  5.6% cho PaLM 2-S* so vá»›i
6.4% cho PaLM 2-L. Äiá»u nÃ y ngoÃ i thá»±c táº¿ lÃ  cÃ¡c mÃ´ hÃ¬nh lá»›n hÆ¡n báº¯t Ä‘áº§u vá»›i hiá»‡u suáº¥t ban Ä‘áº§u máº¡nh máº½ hÆ¡n nhiá»u
, vÃ  cÃ¡c cáº£i thiá»‡n trÃªn cÃ¡c benchmark nÃ y thÆ°á»ng trá»Ÿ nÃªn khÃ³ khÄƒn hÆ¡n khi hiá»‡u suáº¥t baseline tÄƒng lÃªn.
Khoáº£ng cÃ¡ch hiá»‡u suáº¥t train-test. HÃ¬nh 4 cho tháº¥y ráº±ng trong khi hiá»‡u suáº¥t huáº¥n luyá»‡n tÄƒng tuyáº¿n tÃ­nh
vá»›i sá»‘ láº§n láº·p ReSTğ¸ğ‘€, hiá»‡u suáº¥t táº­p thá»­ nghiá»‡m thÃ¬ khÃ´ng. Äá»‘i vá»›i MATH, cÃ¡c cáº£i thiá»‡n hiá»‡u suáº¥t thá»­ nghiá»‡m
nhá» sau láº§n láº·p Ä‘áº§u tiÃªn, vÃ  Ä‘á»‘i vá»›i APPS, chÃºng tÃ´i quan sÃ¡t tháº¥y sá»± suy giáº£m hiá»‡u suáº¥t
trong láº§n láº·p thá»© 2. ChÃºng tÃ´i nghi ngá» ráº±ng sá»± suy giáº£m hiá»‡u suáº¥t cÃ³ thá»ƒ do overfitting trÃªn
táº­p nhá» cÃ¡c váº¥n Ä‘á» huáº¥n luyá»‡n. VÃ¬ bá»™ dá»¯ liá»‡u APPS nhá» hÆ¡n khoáº£ng má»™t pháº§n ba so vá»›i bá»™ dá»¯ liá»‡u MATH,
nÃ³ bá»‹ áº£nh hÆ°á»Ÿng nhiá»u hÆ¡n bá»Ÿi váº¥n Ä‘á» nÃ y.
5.2. TÃ¡c Ä‘á»™ng lÃªn Pass@K vÃ  Hiá»‡u Suáº¥t Bá» Phiáº¿u Äa Sá»‘
Äá»ƒ nghiÃªn cá»©u tÃ¡c Ä‘á»™ng cá»§a tinh chá»‰nh vá»›i ReSTğ¸ğ‘€ lÃªn sá»± Ä‘a dáº¡ng cá»§a cÃ¡c Ä‘áº§u ra Ä‘Æ°á»£c táº¡o ra cá»§a mÃ´ hÃ¬nh cuá»‘i cÃ¹ng
, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t pass@k (Chen et al., 2021) vÃ  bá» phiáº¿u Ä‘a sá»‘ (Wang et al., 2023)
cá»§a mÃ´ hÃ¬nh PaLM 2-L Ä‘Æ°á»£c tinh chá»‰nh so vá»›i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ.
Pass@K Ä‘o lÆ°á»ng xÃ¡c suáº¥t ráº±ng Ã­t nháº¥t má»™t trong K giáº£i phÃ¡p Ä‘Æ°á»£c táº¡o ra cho má»™t bÃ i toÃ¡n lÃ 
Ä‘Ãºng, tá»©c lÃ , Ä‘Æ°a ra cÃ¢u tráº£ lá»i Ä‘Ãºng cho cÃ¡c bÃ i toÃ¡n toÃ¡n há»c hoáº·c vÆ°á»£t qua táº¥t cáº£ cÃ¡c unit test cho táº¡o mÃ£
. HÃ¬nh 5 cho tháº¥y hiá»‡u suáº¥t cá»§a Palm-2-L trÃªn chá»‰ sá»‘ pass@K. ChÃºng tÃ´i tháº¥y ráº±ng mÃ´ hÃ¬nh
thu Ä‘Æ°á»£c sau tinh chá»‰nh ReSTğ¸ğ‘€ máº¡nh hÆ¡n Ä‘á»‘i vá»›i táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ K, vá»›i khoáº£ng cÃ¡ch hiá»‡u suáº¥t thÆ°á»ng
cao nháº¥t Ä‘á»‘i vá»›i K=1.
Bá» phiáº¿u Ä‘a sá»‘ Ä‘áº§u tiÃªn láº¥y máº«u má»™t táº­p há»£p Ä‘a dáº¡ng cÃ¡c Ä‘Æ°á»ng lÃ½ luáº­n thay vÃ¬ chá»‰ láº¥y cÃ¡i greedy
, vÃ  sau Ä‘Ã³ chá»n cÃ¢u tráº£ lá»i nháº¥t quÃ¡n nháº¥t báº±ng cÃ¡ch marginalize cÃ¡c Ä‘Æ°á»ng lÃ½ luáº­n Ä‘Æ°á»£c láº¥y máº«u.
Äá»‘i vá»›i Hendrycks MATH, cÃ³ thá»ƒ sá»­ dá»¥ng bá» phiáº¿u Ä‘a sá»‘ Ä‘á»ƒ tá»‘i Ä‘a hÃ³a hiá»‡u suáº¥t Pass@1, vÃ  chÃºng tÃ´i
tháº¥y ráº±ng khi sá»­ dá»¥ng 64 máº«u cho má»—i cÃ¢u há»i, PaLM 2-L Ä‘Æ°á»£c tinh chá»‰nh vá»›i ReSTğ¸ğ‘€ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c thá»­ nghiá»‡m
48.82, trong khi mÃ´ hÃ¬nh cÆ¡ sá»Ÿ Ä‘áº¡t 44.02.
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 9

--- TRANG 10 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
0 20 40 60
Sá»‘ máº«u (K)40%60%80%Pass @ K Äá»™ ChÃ­nh XÃ¡c Thá»­ Nghiá»‡m (%)HumanEval
PaLM-2-L
PaLM-2-L (ReST)
2 4 6 8 10
Sá»‘ máº«u (K)10%20%30%40%Pass @ K Äá»™ ChÃ­nh XÃ¡c Thá»­ Nghiá»‡m (%)APPS (Introductory)
PaLM-2-L
PaLM-2-L (ReST)
0 20 40 60
Sá»‘ máº«u (K)20%30%40%50%60%70%80%Pass @ K Äá»™ ChÃ­nh XÃ¡c Thá»­ Nghiá»‡m (%)Hendrycks MATH
Palm-2-L
Palm-2-L (ReST)
HÃ¬nh 5|Káº¿t quáº£ Pass@K cho mÃ´ hÃ¬nh PaLM-2-L pretrained cÅ©ng nhÆ° mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh vá»›i ReSTğ¸ğ‘€.
Vá»›i má»™t sá»‘ máº«u K cá»‘ Ä‘á»‹nh, tinh chá»‰nh vá»›i ReSTğ¸ğ‘€ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t Pass@K.
ChÃºng tÃ´i Ä‘áº·t nhiá»‡t Ä‘á»™ thÃ nh 1.0 vÃ  sá»­ dá»¥ng nucleus sampling vá»›i ğ‘=0.95.
5.3. NghiÃªn Cá»©u Ablation
TÃ¡c Ä‘á»™ng cá»§a nhiá»u láº§n láº·p Káº¿t quáº£ cá»§a chÃºng tÃ´i cho tháº¥y ráº±ng nhiá»u láº§n láº·p Ä‘Ã´i khi cÃ³ thá»ƒ dáº«n Ä‘áº¿n
over-fitting trÃªn táº­p train (HÃ¬nh 4). Äiá»u nÃ y Ä‘áº·t ra cÃ¢u há»i liá»‡u nhiá»u láº§n láº·p cÃ³ thá»±c sá»± cáº§n thiáº¿t hay khÃ´ng
. CÃ³ tá»‘t hÆ¡n khÃ´ng náº¿u thu tháº­p má»™t bá»™ dá»¯ liá»‡u lá»›n hÆ¡n vÃ  chá»‰ thá»±c hiá»‡n má»™t láº§n láº·p duy nháº¥t cá»§a ReSTğ¸ğ‘€?
Äá»ƒ nghiÃªn cá»©u Ä‘iá»u nÃ y, chÃºng tÃ´i thu tháº­p má»™t bá»™ dá»¯ liá»‡u vá»›i mÃ´ hÃ¬nh PaLM-2-L cÆ¡ sá»Ÿ trÃªn Hendrycks MATH cÃ³
sá»‘ lÆ°á»£ng giáº£i phÃ¡p gáº¥p 3Ã— cho má»—i bÃ i toÃ¡n so vá»›i Ä‘Æ°á»£c sá»­ dá»¥ng trong má»™t láº§n láº·p duy nháº¥t cá»§a ReSTğ¸ğ‘€ cho E-step
. Tinh chá»‰nh vá»›i bá»™ dá»¯ liá»‡u nÃ y dáº«n Ä‘áº¿n hiá»‡u suáº¥t pass@1 lÃ  40.3%, tháº¥p hÆ¡n so vá»›i 41% trong láº§n láº·p thá»© hai
vÃ  41.9% trong láº§n láº·p thá»© ba, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 2. CÃ¡c káº¿t quáº£ nÃ y cho tháº¥y ráº±ng thá»±c hiá»‡n nhiá»u
láº§n láº·p cá»§a ReSTğ¸ğ‘€ dáº«n Ä‘áº¿n hiá»‡u suáº¥t cao hÆ¡n so vá»›i má»™t láº§n láº·p duy nháº¥t vá»›i dá»¯ liá»‡u gáº¥p 3 láº§n.
So sÃ¡nh dá»¯ liá»‡u do mÃ´ hÃ¬nh táº¡o ra vá»›i dá»¯ liá»‡u con ngÆ°á»i Má»™t Ä‘iá»ƒm máº¡nh chÃ­nh cá»§a ReSTğ¸ğ‘€ lÃ  kháº£ nÄƒng
táº¡o ra nhiá»u giáº£i phÃ¡p Ä‘Ãºng cho má»—i bÃ i toÃ¡n. Äiá»u nÃ y cung cáº¥p dá»¯ liá»‡u huáº¥n luyá»‡n bá»• sung cÃ³ giÃ¡ trá»‹
so vá»›i dá»¯ liá»‡u do con ngÆ°á»i táº¡o ra, thÆ°á»ng chá»‰ cung cáº¥p má»™t giáº£i phÃ¡p duy nháº¥t cho má»—i bÃ i toÃ¡n. Trong khi
Ä‘iá»u nÃ y lÃ m cho viá»‡c so sÃ¡nh trong HÃ¬nh 2 vÃ  3 khÃ´ng hoÃ n toÃ n cÃ´ng báº±ng, nÃ³ cÅ©ng lÃ m ná»•i báº­t tiá»m nÄƒng cá»§a ReSTğ¸ğ‘€
Ä‘á»ƒ tÄƒng cÆ°á»ng hiá»‡u suáº¥t vá»›i cÃ¡c giáº£i phÃ¡p Ä‘a dáº¡ng vÃ  Ä‘Ãºng.
Äá»ƒ cho phÃ©p so sÃ¡nh tÃ¡o vá»›i tÃ¡o, chÃºng tÃ´i tiáº¿n hÃ nh nghiÃªn cá»©u sau: chÃºng tÃ´i chá»n táº¥t cáº£
cÃ¡c cÃ¢u há»i Hendrycks MATH mÃ  chÃºng tÃ´i cÃ³ Ã­t nháº¥t má»™t giáº£i phÃ¡p Ä‘Ãºng do mÃ´ hÃ¬nh táº¡o ra, dáº«n Ä‘áº¿n
khoáº£ng 5K cÃ¢u há»i. Äá»‘i vá»›i 5K cÃ¢u há»i nÃ y, chÃºng tÃ´i cháº¡y hai thÃ­ nghiá»‡m tinh chá»‰nh: SFT(5K) trong Ä‘Ã³
chÃºng tÃ´i tinh chá»‰nh trÃªn cÃ¡c giáº£i phÃ¡p do con ngÆ°á»i viáº¿t (má»™t cÃ¢u há»i cho má»—i cÃ¢u há»i), vÃ  ReSTâˆ—(5K) trong Ä‘Ã³ chÃºng tÃ´i tinh chá»‰nh trÃªn
cÃ¡c giáº£i phÃ¡p do mÃ´ hÃ¬nh táº¡o ra (cÅ©ng má»™t cÃ¢u há»i cho má»—i cÃ¢u há»i, Ä‘Æ°á»£c chá»n ngáº«u nhiÃªn).
CÃ¡c káº¿t quáº£ trong HÃ¬nh 6 (pháº£i), cho tháº¥y ráº±ng ReSTğ¸ğ‘€ vÆ°á»£t trá»™i hÆ¡n tinh chá»‰nh trÃªn dá»¯ liá»‡u con ngÆ°á»i ngay cáº£ trong
cÃ i Ä‘áº·t háº¡n cháº¿ hÆ¡n nhiá»u nÃ y. HÆ¡n ná»¯a, hiá»‡u quáº£ cá»§a ReST(5K) so vá»›i ReSTâˆ—(5K) lÃ m ná»•i báº­t
lá»£i Ã­ch bá»• sung vá» hiá»‡u suáº¥t mÃ  chÃºng tÃ´i cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c báº±ng cÃ¡ch dÃ nh nhiá»u compute hÆ¡n cho viá»‡c láº¥y máº«u má»™t
sá»‘ lÆ°á»£ng lá»›n cÃ¡c giáº£i phÃ¡p vÃ  thá»±c hiá»‡n nhiá»u láº§n láº·p cá»§a ReSTğ¸ğ‘€.
ChÆ°ng cáº¥t vá»›i dá»¯ liá»‡u do ReSTğ¸ğ‘€ táº¡o ra CÃ¡c káº¿t quáº£ trÃªn cho tháº¥y ráº±ng dá»¯ liá»‡u tá»± táº¡o cÃ³ thá»ƒ
tá»‘t hÆ¡n dá»¯ liá»‡u con ngÆ°á»i Ä‘á»ƒ tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯. ChÃºng tÃ´i giáº£ thuyáº¿t ráº±ng Ä‘iá»u nÃ y cÃ³ thá»ƒ lÃ  do
cÃ¡c giáº£i phÃ¡p do mÃ´ hÃ¬nh táº¡o ra cÃ³ tÃ­nh in-distribution hÆ¡n so vá»›i cÃ¡c giáº£i phÃ¡p do con ngÆ°á»i viáº¿t. Äiá»u nÃ y Ä‘áº·t ra
cÃ¢u há»i liá»‡u dá»¯ liá»‡u do ReSTğ¸ğ‘€ táº¡o ra cÃ³ thá»ƒ cÃ³ lá»£i cho cÃ¡c mÃ´ hÃ¬nh khÃ¡c vá»›i mÃ´ hÃ¬nh táº¡o ra
dá»¯ liá»‡u hay khÃ´ng.
Äá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y, chÃºng tÃ´i xem xÃ©t má»™t cÃ i Ä‘áº·t chÆ°ng cáº¥t trÃªn MATH trong Ä‘Ã³ chÃºng tÃ´i tinh chá»‰nh PaLM 2-S
báº±ng dá»¯ liá»‡u Ä‘Æ°á»£c táº¡o ra bá»Ÿi PaLM 2-L, dáº«n Ä‘áº¿n cÃ¡c giáº£i phÃ¡p cho khoáº£ng 5K cÃ¢u há»i. Cá»¥ thá»ƒ, chÃºng tÃ´i cháº¡y
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 10

--- TRANG 11 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
SFT (7K) SFT (5K)ReST* (5K)ReSTEM (5K)
PhÆ°Æ¡ng phÃ¡p (Sá»‘ cÃ¢u há»i)3436384042Pass@1 Hiá»‡u Suáº¥t (%)Hendrycks MATH (Test)
SFT (Human) Distill* (2-L) ReSTEM (2-S)Distill (2-L)
PhÆ°Æ¡ng phÃ¡p (Nguá»“n Dá»¯ Liá»‡u)15.017.520.022.525.027.530.0Pass@1 Hiá»‡u Suáº¥t (%)PaLM 2-S trÃªn Hendrycks MATH (Test)
HÃ¬nh 6|TrÃ¡i. So sÃ¡nh ReSTğ¸ğ‘€ vá»›i SFT trÃªn MATH. SFT Ä‘á» cáº­p Ä‘áº¿n tinh chá»‰nh trÃªn dá»¯ liá»‡u con ngÆ°á»i,
trong khi ReST* Ä‘á» cáº­p Ä‘áº¿n má»™t phiÃªn báº£n cá»§a ReSTğ¸ğ‘€ vá»›i má»™t láº§n láº·p sá»­ dá»¥ng chá»‰ má»™t máº«u Ä‘Ãºng cho má»—i
bÃ i toÃ¡n. á» Ä‘Ã¢y, ReST biá»ƒu thá»‹ ReSTğ¸ğ‘€ vá»›i 3 láº§n láº·p. Äá»‘i vá»›i má»—i phÆ°Æ¡ng phÃ¡p, chÃºng tÃ´i kÃ½ hiá»‡u sá»‘ lÆ°á»£ng
cÃ¢u há»i trong ngoáº·c Ä‘Æ¡n. Pháº£i. TÃ¡c Ä‘á»™ng cá»§a Dá»¯ Liá»‡u Do MÃ´ HÃ¬nh Táº¡o Ra cho ChÆ°ng Cáº¥t.
hai thÃ­ nghiá»‡m chÆ°ng cáº¥t: Distillâˆ—(2-L) trong Ä‘Ã³ chÃºng tÃ´i tinh chá»‰nh trÃªn cÃ¡c giáº£i phÃ¡p do teacher táº¡o ra (má»™t
cho má»—i cÃ¢u há»i), tÆ°Æ¡ng tá»± nhÆ° ReST (5K), vÃ  Distill (2-L), bao gá»“m nhiá»u giáº£i phÃ¡p cho má»—i bÃ i toÃ¡n,
Ä‘Æ°á»£c táº¡o ra trong láº§n láº·p cuá»‘i cÃ¹ng cá»§a ReSTğ¸ğ‘€ vá»›i PaLM 2-L.
Káº¿t quáº£ cá»§a chÃºng tÃ´i, Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 6 (pháº£i), tiáº¿t lá»™ ráº±ng Distillâˆ— vÆ°á»£t trá»™i hÆ¡n hiá»‡u suáº¥t Ä‘áº¡t Ä‘Æ°á»£c
báº±ng cÃ¡ch tinh chá»‰nh trÃªn cÃ¡c giáº£i phÃ¡p do con ngÆ°á»i viáº¿t, máº·c dÃ¹ cÃ³ sá»‘ lÆ°á»£ng cÃ¢u há»i huáº¥n luyá»‡n nhá» hÆ¡n.
NgoÃ i ra, tinh chá»‰nh PaLM 2-S vá»›i nhiá»u giáº£i phÃ¡p tá»« PaLM 2-L, cá»¥ thá»ƒ lÃ  Distill (2-L), lÃ 
vÆ°á»£t trá»™i hÆ¡n so vá»›i viá»‡c sá»­ dá»¥ng cÃ¡c giáº£i phÃ¡p tá»± táº¡o thÃ´ng qua ReSTğ¸ğ‘€. Cáº£i thiá»‡n nÃ y cÃ³ thá»ƒ do sá»‘ lÆ°á»£ng
cÃ¢u há»i huáº¥n luyá»‡n vá»›i giáº£i phÃ¡p trong dá»¯ liá»‡u do PaLM 2-L táº¡o ra lá»›n hÆ¡n so vá»›i 2-S. NhÃ¬n chung,
cÃ¡c káº¿t quáº£ nÃ y cho tháº¥y ráº±ng dá»¯ liá»‡u do mÃ´ hÃ¬nh táº¡o ra cÃ³ thá»ƒ hiá»‡u quáº£ hÆ¡n Ä‘á»ƒ tinh chá»‰nh cÃ¡c mÃ´ hÃ¬nh nhá» hÆ¡n
so vá»›i viá»‡c dá»±a vÃ o dá»¯ liá»‡u do con ngÆ°á»i táº¡o ra.
ReSTEMReST
PhÆ°Æ¡ng phÃ¡p21.021.221.421.621.822.0Pass@1 Äá»™ ChÃ­nh XÃ¡c Thá»­ Nghiá»‡m (%)APPS (Introductory)
Palm-2-S*-SFTReSTEMReST
PhÆ°Æ¡ng phÃ¡p384042444648Pass@1 Äá»™ ChÃ­nh XÃ¡c Thá»­ Nghiá»‡m (%)Chuyá»ƒn giao sang HumanEval
HÃ¬nh 7|ReSTğ¸ğ‘€ vs ReST sá»­ dá»¥ng PaLM 2-S*. ReST vs ReSTğ¸ğ‘€ Má»™t sá»± khÃ¡c biá»‡t lá»›n giá»¯a
ReSTğ¸ğ‘€ vÃ  ReST lÃ  trong khi ReSTğ¸ğ‘€ luÃ´n tinh chá»‰nh
mÃ´ hÃ¬nh cÆ¡ sá»Ÿ cho má»—i láº§n láº·p, ReST tiáº¿p tá»¥c
tinh chá»‰nh mÃ´ hÃ¬nh tá»« láº§n láº·p cuá»‘i cÃ¹ng.
ChÃºng tÃ´i cháº¡y má»™t ablation so sÃ¡nh cÃ¡c tÃ¹y chá»n nÃ y
báº±ng cÃ¡ch sá»­ dá»¥ng PaLM 2-S* trong HÃ¬nh 7 vÃ  quan sÃ¡t ráº±ng trong khi
ReST vÃ  ReSTğ¸ğ‘€ cÃ³ hiá»‡u suáº¥t tÆ°Æ¡ng tá»± trÃªn
APPS, hiá»‡u suáº¥t chuyá»ƒn giao sang HumanEval lÃ 
tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ vá»›i ReSTğ¸ğ‘€.
TÃ¡c Ä‘á»™ng cá»§a kÃ­ch thÆ°á»›c bá»™ dá»¯ liá»‡u VÃ¬ má»™t trong nhá»¯ng thÃ nh pháº§n chÃ­nh cáº§n thiáº¿t cho ReSTğ¸ğ‘€ lÃ  má»™t bá»™ dá»¯ liá»‡u cÃ¡c bá»‘i cáº£nh Ä‘áº§u vÃ o
(vÃ­ dá»¥, cÃ¢u há»i cho MATH), chÃºng tÃ´i quan tÃ¢m Ä‘áº¿n viá»‡c Ä‘Ã¡nh giÃ¡ áº£nh hÆ°á»Ÿng cá»§a sá»‘ lÆ°á»£ng bÃ i toÃ¡n Ä‘áº§u vÃ o
. CÃ¡c káº¿t quáº£ tá»« cÃ¡c ablation bá»™ dá»¯ liá»‡u cá»§a chÃºng tÃ´i sá»­ dá»¥ng mÃ´ hÃ¬nh PaLM-2-L trÃªn Hendrycks MATH,
HÃ¬nh 8 (trÃ¡i), cho tháº¥y ráº±ng viá»‡c sá»­ dá»¥ng chá»‰ 1000 cÃ¢u há»i MATH dáº«n Ä‘áº¿n lá»£i Ã­ch Ä‘Ã¡ng ká»ƒ, cÃ³ nghÄ©a lÃ 
phÆ°Æ¡ng phÃ¡p nÃ y ráº¥t hiá»‡u quáº£ vá» sá»‘ lÆ°á»£ng prompt cáº§n thiáº¿t. Tuy nhiÃªn, chÃºng tÃ´i lÆ°u Ã½ sá»± giáº£m nháº¹
hiá»‡u suáº¥t khi sá»­ dá»¥ng 4,000 cÃ¢u há»i so vá»›i 2,000, cho tháº¥y variance tiá»m áº©n trong
quÃ¡ trÃ¬nh tinh chá»‰nh. LÃ½ tÆ°á»Ÿng nháº¥t, viá»‡c tiáº¿n hÃ nh thÃ­ nghiá»‡m nÃ y nhiá»u láº§n sáº½ giÃºp Ä‘á»‹nh lÆ°á»£ng variance nÃ y
, nhÆ°ng Ä‘iá»u nÃ y tá»‘n kÃ©m vá» máº·t tÃ i nguyÃªn má»™t cÃ¡ch cáº¥m Ä‘oÃ¡n. NhÃ¬n chung, chÃºng tÃ´i tháº¥y ráº±ng ReSTğ¸ğ‘€ khÃ¡ hiá»‡u quáº£ vá» máº«u
vÃ  lá»£i Ã­ch hiá»‡u suáº¥t tá»« ReSTğ¸ğ‘€ cáº£i thiá»‡n khi chÃºng tÃ´i tÄƒng kÃ­ch thÆ°á»›c bá»™ dá»¯ liá»‡u.
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 11

--- TRANG 12 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
0 1000 2000 4000 7000
Sá»‘ CÃ¢u Há»i3436384042Pass@1 Hiá»‡u Suáº¥t (%)Hendrycks MATH (Test)
Ráº¥t KhÃ³ KhÃ³ Trung BÃ¬nh Dá»…
Má»©c Äá»™ KhÃ³ Cá»§a BÃ i ToÃ¡n020406080100Tá»· Lá»‡ ThÃ nh CÃ´ng Trung BÃ¬nh (%)Tá»· Lá»‡ ThÃ nh CÃ´ng Trung BÃ¬nh Theo Má»©c Äá»™ KhÃ³
MÃ´ HÃ¬nh CÆ¡ Sá»Ÿ
ReSTEM
HÃ¬nh 8|TrÃ¡i. Hiá»‡u suáº¥t cho má»™t láº§n láº·p duy nháº¥t cá»§a ReSTğ¸ğ‘€ nhÆ° má»™t hÃ m cá»§a kÃ­ch thÆ°á»›c bá»™ dá»¯ liá»‡u (sá»‘ lÆ°á»£ng
cÃ¢u há»i) trÃªn MATH. Pháº£i. Cáº£i thiá»‡n tá»« ReSTğ¸ğ‘€ dá»±a trÃªn má»©c Ä‘á»™ khÃ³ cá»§a cÃ¢u há»i.
Nhá»¯ng CÃ¢u Há»i NÃ o HÆ°á»Ÿng Lá»£i Nháº¥t tá»« ReSTğ¸ğ‘€ ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ sá»± nÃ¢ng cao hiá»‡u suáº¥t cá»§a ReSTğ¸ğ‘€
trÃªn cÃ¡c má»©c Ä‘á»™ khÃ³ khÃ¡c nhau cá»§a cÃ¢u há»i trong bá»™ dá»¯ liá»‡u Hendrycks MATH. CÃ¡c cÃ¢u há»i Ä‘Æ°á»£c phÃ¢n loáº¡i dá»±a trÃªn
tá»· lá»‡ thÃ nh cÃ´ng tá»« mÃ´ hÃ¬nh cÆ¡ sá»Ÿ á»Ÿ cÃ i Ä‘áº·t nhiá»‡t Ä‘á»™ T=1.0 thÃ nh bá»‘n loáº¡i: "dá»…"
(tráº£ lá»i Ä‘Ãºng 75%-100% thá»i gian), "trung bÃ¬nh" (50%-75%), "khÃ³" (25%-50%), vÃ  "ráº¥t
khÃ³" (dÆ°á»›i 25%). HÃ¬nh 8 (pháº£i) trÃ¬nh bÃ y tá»· lá»‡ thÃ nh cÃ´ng trung bÃ¬nh cho cÃ¡c loáº¡i nÃ y, so sÃ¡nh
mÃ´ hÃ¬nh cÆ¡ sá»Ÿ vá»›i mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh ReSTğ¸ğ‘€. Káº¿t quáº£ cho tháº¥y ráº±ng ReSTğ¸ğ‘€ liÃªn tá»¥c
cáº£i thiá»‡n hiá»‡u suáº¥t trÃªn táº¥t cáº£ cÃ¡c má»©c Ä‘á»™ khÃ³, vá»›i lá»£i Ã­ch cao nháº¥t Ä‘áº¿n tá»« cÃ¡c cÃ¢u há»i Ä‘Æ°á»£c phÃ¢n loáº¡i
lÃ  trung bÃ¬nh vÃ  khÃ³.
5.4. TÃ¡c Ä‘á»™ng lÃªn Kháº£ nÄƒng LÃ½ luáº­n
Boolean ExpressionsCausal JudgementDate UnderstandingDisambiguation QADyck LanguagesFormal FallaciesGeometric ShapesHyperbaton
Movie RecommendationMulti-step Arithmetic [Two]Navigate
Object CountingPenguins in a Table
Reasoning about Colored ObjectsRuin Names
Salient Translation Error DetectionSnarks
Sports UnderstandingTemporal SequencesWeb of LiesWord Sorting
Logical Deduction (avg)
Tracking Shuffled Objects (avg)
Nhiá»‡m Vá»¥ Big-Bench Hard (BBH)30405060708090100Hiá»‡u Suáº¥t Few-shot vá»›i CoTPaLM 2-L PaLM 2-L (MATH) PaLM 2-L (APPS)
CoT Direct
Loáº¡i Prompt6065707580Hiá»‡u Suáº¥t BBH Trung BÃ¬nhPaLM 2-L
PaLM 2-L (APPS)
PaLM 2-L (MATH)
HÃ¬nh 9|So sÃ¡nh cÃ¡c mÃ´ hÃ¬nh ReSTğ¸ğ‘€ vá»›i mÃ´ hÃ¬nh cÆ¡ sá»Ÿ trÃªn bá»™ nhiá»‡m vá»¥ Big-Bench Hard.
CÃ¡c Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c tiáº¿n hÃ nh trÃªn nhiá»u checkpoint, vÃ  cÃ¡c Ä‘Æ°á»ng tháº³ng Ä‘en dá»c biá»ƒu thá»‹ Ä‘á»™ lá»‡ch
chuáº©n.
Kháº£ nÄƒng chung. BIG-Bench cung cáº¥p má»™t bá»™ hÆ¡n 200 nhiá»‡m vá»¥ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ thÄƒm dÃ²
hiá»‡u suáº¥t cá»§a LLM trÃªn má»™t loáº¡t cÃ¡c lÄ©nh vá»±c vÃ  kháº£ nÄƒng. BIG-Bench Hard (BBH) (Suzgun et al.,
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 12

--- TRANG 13 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
2022) lÃ  má»™t táº­p con cá»§a 23 nhiá»‡m vá»¥ BIG-Bench trong Ä‘Ã³ tháº¿ há»‡ LLM trÆ°á»›c Ä‘Ã³, cháº³ng háº¡n nhÆ° Codex
vÃ  PaLM 540B, cÃ³ hiá»‡u suáº¥t dÆ°á»›i má»©c ngÆ°á»i Ä‘Ã¡nh giÃ¡ trung bÃ¬nh. ChÃºng tÃ´i tuÃ¢n theo giao thá»©c cá»§a Google et al.
(2023) vÃ  Ä‘Ã¡nh giÃ¡ trÃªn BBH báº±ng cÃ¡ch sá»­ dá»¥ng cáº£ prompting few-shot vÃ  chain-of-thought. HÃ¬nh 9 cho tháº¥y
hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh ReSTğ¸ğ‘€, vÃ  so sÃ¡nh chÃºng vá»›i mÃ´ hÃ¬nh PaLM-2 cÆ¡ sá»Ÿ. ChÃºng tÃ´i
tháº¥y khÃ´ng cÃ³ sá»± suy giáº£m lá»›n nÃ o trÃªn báº¥t ká»³ nhiá»‡m vá»¥ BBH nÃ o. HÆ¡n ná»¯a, mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh trÃªn Hendrycks
MATH vÆ°á»£t trá»™i hÆ¡n mÃ´ hÃ¬nh cÆ¡ sá»Ÿ trÃªn bá»™ nÃ y khi sá»­ dá»¥ng prompting chain-of-thought, vÃ 
mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh trÃªn APPS cÅ©ng cho tháº¥y lá»£i Ã­ch hiá»‡u suáº¥t nháº¹. Khi sá»­ dá»¥ng prompting trá»±c tiáº¿p, cáº£ ba
mÃ´ hÃ¬nh Ä‘á»u hoáº¡t Ä‘á»™ng tÆ°Æ¡ng tá»±.
Giáº£i quyáº¿t váº¥n Ä‘á». Äá»ƒ kiá»ƒm tra kháº£ nÄƒng giáº£i quyáº¿t váº¥n Ä‘á» toÃ¡n há»c trÃªn má»™t táº­p Ä‘Ã¡nh giÃ¡ "tháº¿ giá»›i thá»±c" Ä‘Æ°á»£c giá»¯ láº¡i
, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i trÃªn ká»³ thi chung káº¿t trung há»c phá»• thÃ´ng Hungary nÄƒm 2023 mÃ´n toÃ¡n há»c,
theo giao thá»©c Ä‘Ã¡nh giÃ¡ tá»« Paster (2023). Cá»¥ thá»ƒ, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh PaLM 2-L,
Ä‘Æ°á»£c tinh chá»‰nh vá»›i ReSTğ¸ğ‘€ trÃªn Hendrycks MATH, sá»­ dá»¥ng prompt 1-shot tá»« Grok, láº¥y máº«u cÃ¡c giáº£i phÃ¡p
báº±ng nhiá»‡t Ä‘á»™ 0.1, vÃ  cháº¥m Ä‘iá»ƒm thá»§ cÃ´ng cÃ¡c Ä‘áº§u ra báº±ng rubric Ä‘Æ°á»£c cung cáº¥p bá»Ÿi cÃ¡c giÃ¡m kháº£o.
Káº¿t quáº£ tá»« Ä‘Ã¡nh giÃ¡ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 10. ChÃºng tÃ´i tháº¥y ráº±ng PaLM-2-L Ä‘Æ°á»£c tinh chá»‰nh vá»›i ReSTğ¸ğ‘€
hoáº¡t Ä‘á»™ng tá»‘t trÃªn ká»³ thi nÃ y, vÆ°á»£t trá»™i hÆ¡n hiá»‡u suáº¥t cá»§a táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh hiá»‡n táº¡i trá»« GPT-4.
20 30 40 50 60 70
Äiá»ƒm Ká»³ Thi Chung Káº¿t HS Hungary (%)30405060708090Äiá»ƒm GSM8K (%)MetaMath 7BMetaMath Mistral 7B OpenChat 3.5
Code Llama 34BLlemma 34BGPT-3.5 TurboGPT-4
Grok-0 (33B)Grok-1
Qwen 7BClaude 2
Mistral 7BMAmmoTH 7BPaLM 2-L (ReSTEM)Äiá»ƒm Ká»³ Thi vs Hiá»‡u Suáº¥t GSM8K cá»§a CÃ¡c MÃ´ HÃ¬nh KhÃ¡c Nhau
HÃ¬nh 10|Káº¿t quáº£ chuyá»ƒn giao trÃªn Ká»³ Thi Chung Káº¿t HS Hungary. Káº¿t quáº£ cho cÃ¡c mÃ´ hÃ¬nh khÃ¡c ngoÃ i PaLM-2-L
Ä‘Æ°á»£c tinh chá»‰nh vá»›i ReSTğ¸ğ‘€ Ä‘Æ°á»£c láº¥y tá»« Paster (2023). Má»™t sá»‘ mÃ´ hÃ¬nh chuyÃªn vá» toÃ¡n há»c
hoáº¡t Ä‘á»™ng tá»‘t trÃªn benchmark GSM8K Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i nhÆ°ng hoáº¡t Ä‘á»™ng kÃ©m trÃªn ká»³ thi Hungary. NgÆ°á»£c láº¡i,
mÃ´ hÃ¬nh PaLM 2-L Ä‘Æ°á»£c tinh chá»‰nh vá»›i ReSTğ¸ğ‘€ hoáº¡t Ä‘á»™ng tá»‘t trÃªn cáº£ hai benchmark nÃ y.
6. Tháº£o luáº­n
Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u do mÃ´ hÃ¬nh táº¡o ra káº¿t há»£p vá»›i hÃ m pháº§n thÆ°á»Ÿng,
thÃ´ng qua ReSTğ¸ğ‘€, Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a LLM trÃªn cÃ¡c nhiá»‡m vá»¥ giáº£i quyáº¿t váº¥n Ä‘á». HÆ¡n ná»¯a, chÃºng tÃ´i
chá»©ng minh ráº±ng ReSTğ¸ğ‘€ cÃ³ ná»n táº£ng lÃ½ thuyáº¿t trong viá»‡c Ã¡p dá»¥ng expectation-maximization
cho RL. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ ReSTğ¸ğ‘€ trÃªn giáº£i quyáº¿t váº¥n Ä‘á» toÃ¡n há»c vÃ  táº¡o mÃ£, vÃ  cho tháº¥y ráº±ng
ReSTğ¸ğ‘€ mang láº¡i lá»£i Ã­ch hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ vá»›i chi phÃ­ tÃ­nh toÃ¡n tÆ°Æ¡ng Ä‘á»‘i tháº¥p, Ä‘áº·c biá»‡t khi
so sÃ¡nh vá»›i chi phÃ­ pre-training. CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i cÅ©ng cho tháº¥y ráº±ng ReSTğ¸ğ‘€ khÃ´ng dáº«n Ä‘áº¿n
sá»± suy giáº£m trÃªn cÃ¡c nhiá»‡m vá»¥ khÃ¡c. ChÃºng tÃ´i tiáº¿n hÃ nh má»™t sá»‘ ablation Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n vá» Ä‘iá»ƒm máº¡nh vÃ 
Ä‘iá»ƒm yáº¿u cá»§a phÆ°Æ¡ng phÃ¡p nÃ y, vÃ  tháº¥y ráº±ng nÃ³ hiá»‡u quáº£ vá» dá»¯ liá»‡u, nhÆ°ng cÅ©ng Ä‘Ã²i há»i má»™t sá»‘ cáº£nh giÃ¡c Ä‘á»ƒ trÃ¡nh
over-fitting.
CÃ³ má»™t sá»‘ háº¡n cháº¿ liÃªn quan Ä‘áº¿n ReSTğ¸ğ‘€. Thá»© nháº¥t, phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Ã²i há»i má»™t
táº­p huáº¥n luyá»‡n cÃ¡c váº¥n Ä‘á» hoáº·c prompt cÃ³ kÃ­ch thÆ°á»›c vá»«a pháº£i, cáº§n Ä‘Æ°á»£c thu tháº­p (tá»« con ngÆ°á»i) cho báº¥t ká»³
nhiá»‡m vá»¥ má»›i nÃ o quan tÃ¢m. Thá»© hai, ReSTğ¸ğ‘€ cÅ©ng Ä‘Ã²i há»i quyá»n truy cáº­p vÃ o hÃ m pháº§n thÆ°á»Ÿng Ä‘Æ°á»£c thiáº¿t káº¿ thá»§ cÃ´ng hoáº·c Ä‘Ã£ há»c
, lÃ½ tÆ°á»Ÿng lÃ  má»™t hÃ m cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»± Ä‘á»™ng. Cuá»‘i cÃ¹ng, trong khi ReSTğ¸ğ‘€ cho phÃ©p
cáº£i thiá»‡n hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ trong hiá»‡u suáº¥t pass@1, nÃ³ cÃ³ thá»ƒ khÃ´ng hoÃ n toÃ n thu háº¹p khoáº£ng cÃ¡ch Ä‘áº¿n hiá»‡u suáº¥t pass@K
cho cÃ¹ng má»™t nhiá»‡m vá»¥ (vá»›i K Ä‘á»§ lá»›n). NghiÃªn cá»©u tÆ°Æ¡ng lai vá» tá»± cáº£i thiá»‡n trong
cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ nÃªn táº­p trung vÃ o viá»‡c tá»± Ä‘á»™ng hÃ³a cÃ¡c pháº§n thá»§ cÃ´ng cá»§a pipeline (cÃ³ thá»ƒ thÃ´ng qua
cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯), vÃ  khÃ¡m phÃ¡ cÃ¡c cáº£i tiáº¿n thuáº­t toÃ¡n giáº£m khoáº£ng cÃ¡ch Ä‘áº¿n hiá»‡u suáº¥t pass@K.
Lá»i cáº£m Æ¡n
ChÃºng tÃ´i muá»‘n cáº£m Æ¡n Tom Le Paine Ä‘Ã£ cung cáº¥p pháº£n há»“i cho báº£n tháº£o Ä‘áº§u. ChÃºng tÃ´i cÅ©ng thá»«a nháº­n
Benjamin Anderson, Sridhar Thiagarajan, Feryal Behbahani, Aleksandra Faust, Doina Precup, Olivier
Bachem, vÃ  Slav Petrov cho cÃ¡c tháº£o luáº­n há»¯u Ã­ch.
ÄÃ³ng gÃ³p cá»§a tÃ¡c giáº£
Avi, Rishabh, vÃ  JD cÃ¹ng lÃ£nh Ä‘áº¡o dá»± Ã¡n. Avi chá»‹u trÃ¡ch nhiá»‡m vá» cÆ¡ sá»Ÿ háº¡ táº§ng huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡
, ablation vÃ  thÃ­ nghiá»‡m trÃªn MATH, JD lÃ£nh Ä‘áº¡o cÃ¡c thÃ­ nghiá»‡m trÃªn APPS, Rishabh chá»‹u trÃ¡ch nhiá»‡m vá»
viá»‡c viáº¿t bÃ i bÃ¡o, Ä‘Ã¡nh giÃ¡, vÃ  ablation chÆ°ng cáº¥t.
Ankesh, Piyush, Ethan, vÃ  Behnam quan sÃ¡t nhá»¯ng phÃ¡t hiá»‡n sÆ¡ bá»™ vá» hiá»‡u quáº£ cá»§a dá»¯ liá»‡u do mÃ´ hÃ¬nh táº¡o ra
trÃªn MATH cho cÃ¡c mÃ´ hÃ¬nh Minerva vÃ  thÃºc Ä‘áº©y nghiÃªn cá»©u nÃ y. Piyush cÅ©ng giÃºp Avi
trong viá»‡c thiáº¿t láº­p cÆ¡ sá»Ÿ háº¡ táº§ng. Xavier, Peter, James, Jaeheoon, Kelvin vÃ  Yamini tham gia vÃ o cÃ¡c tháº£o luáº­n dá»± Ã¡n
. Jascha vÃ  Noah tÃ i trá»£ vÃ  tÆ° váº¥n cho dá»± Ã¡n. Táº¥t cáº£ cÃ¡c tÃ¡c giáº£ khÃ¡c Ä‘Ã£ cung cáº¥p pháº£n há»“i
vá» cÃ´ng trÃ¬nh nÃ y.
TÃ i liá»‡u tham kháº£o
R. Agarwal, C. Liang, D. Schuurmans, vÃ  M. Norouzi. Learning to generalize from sparse and
underspecified rewards. In International conference on machine learning, trang 130â€“140. PMLR,
2019.
R. Agarwal, N. Vieillard, P. Stanczyk, S. Ramos, M. Geist, vÃ  O. Bachem. Gkd: Generalized knowledge
distillation for auto-regressive sequence models. arXiv preprint arXiv:2306.13649, 2023.
R. Agarwal, A. Singh, L. M. Zhang, B. Bohnet, S. Chan, A. Anand, Z. Abbas, A. Nova, J. D. Co-Reyes,
E. Chu, F. Behbahani, A. Faust, vÃ  H. Larochelle. Many-shot in-context learning, 2024.
T. Anthony, Z. Tian, vÃ  D. Barber. Thinking fast and slow with deep learning and tree search.
Advances in neural information processing systems, 30, 2017.
S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. M.
Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, vÃ  Y. Zhang. Sparks of artificial general intelligence:
Early experiments with GPT-4. CoRR, abs/2303.12712, 2023. doi: 10.48550/ARXIV.2303.12712.
URL https://doi.org/10.48550/arXiv.2303.12712.
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,
N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,
B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P.
Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol,
A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer,
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 14

--- TRANG 15 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, vÃ  W. Zaremba. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,
R. Nakano, C. Hesse, vÃ  J. Schulman. Training verifiers to solve math word problems. arXiv
preprint arXiv:2110.14168, 2021.
P. Dayan vÃ  G. E. Hinton. Using expectation-maximization for reinforcement learning. Neural
Computation, 9(2):271â€“278, 1997.
A. P. Dempster, N. M. Laird, vÃ  D. B. Rubin. Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society: series B (methodological), 39(1):1â€“22, 1977.
H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, vÃ  T. Zhang. Raft: Reward ranked
finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.
Google, R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,
Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.
Y. Gu, L. Dong, F. Wei, vÃ  M. Huang. Knowledge distillation of large language models. arXiv preprint
arXiv:2306.08543, 2023.
C. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant, A. Ahern,
M. Wang, C. Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint
arXiv:2308.08998, 2023.
D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He,
D. Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938,
2021a.
D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, vÃ  J. Steinhardt. Measuring
mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b.
J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, vÃ  J. Han. Large language models can self-improve.
CoRR, abs/2210.11610, 2022. doi: 10.48550/ARXIV.2210.11610. URL https://doi.org/10.
48550/arXiv.2210.11610.
C. Liang, J. Berant, Q. Le, K. D. Forbus, vÃ  N. Lao. Neural symbolic machines: Learning semantic
parsers on freebase with weak supervision. arXiv preprint arXiv:1611.00020, 2016.
A. Ni, J. P. Inala, C. Wang, A. Polozov, C. Meek, D. Radev, vÃ  J. Gao. Learning math reasoning from
self-sampled correct and partially-correct solutions. In The Eleventh International Conference on
Learning Representations, 2022.
M. Norouzi, S. Bengio, N. Jaitly, M. Schuster, Y. Wu, D. Schuurmans, et al. Reward augmented
maximum likelihood for neural structured prediction. Advances In Neural Information Processing
Systems, 29, 2016.
OpenAI. Gpt-4 technical report, 2023.
K. Paster. Testing language models on a held-out high school national finals exam. https://
huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam, 2023.
J. Peters vÃ  S. Schaal. Reinforcement learning by reward-weighted regression for operational space
control. In Proceedings of the 24th international conference on Machine learning, trang 745â€“750,
2007.
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 15

--- TRANG 16 ---
VÆ°á»£t Qua Dá»¯ Liá»‡u Con NgÆ°á»i: Má»Ÿ Rá»™ng Quy MÃ´ Tá»± Huáº¥n Luyá»‡n Ä‘á»ƒ Giáº£i Quyáº¿t Váº¥n Äá» vá»›i CÃ¡c MÃ´ HÃ¬nh NgÃ´n Ngá»¯
D. Phan, M. D. Hoffman, D. Dohan, S. Douglas, T. A. Le, A. Parisi, P. Sountsov, C. Sutton, S. Vikram,
vÃ  R. A. Saurous. Training chain-of-thought via latent-variable inference. arXiv preprint
arXiv:2312.02179, 2023.
A. Sordoni, X. Yuan, M.-A. CÃ´tÃ©, M. Pereira, A. Trischler, Z. Xiao, A. Hosseini, F. Niedtner, vÃ 
N. Le Roux. Joint prompt optimization of stacked llms using variational inference. In Thirty-seventh
Conference on Neural Information Processing Systems, 2023.
M. Suzgun, N. Scales, N. SchÃ¤rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi,
D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv
preprint arXiv:2210.09261, 2022.
X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, vÃ  D. Zhou. Self-
consistency improves chain of thought reasoning in language models. In The Eleventh International
Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,
2023. URL https://openreview.net/pdf?id=1PL1NIMMrw.
Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,
et al. Google's neural machine translation system: Bridging the gap between human and machine
translation. arXiv preprint arXiv:1609.08144, 2016.
Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, vÃ  C. Zhou. Scaling relationship on learning mathematical
reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.
E. Zelikman, Y. Wu, J. Mu, vÃ  N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances
in Neural Information Processing Systems, 35:15476â€“15488, 2022.
Xuáº¥t báº£n trong Transactions on Machine Learning Research (04/2024) 16
