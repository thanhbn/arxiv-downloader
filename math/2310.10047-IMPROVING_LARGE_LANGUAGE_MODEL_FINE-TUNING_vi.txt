# Cải thiện việc fine-tuning mô hình ngôn ngữ lớn để giải quyết các bài toán

Yixin Liu∗1, Avi Singh2, C. Daniel Freeman2, John D. Co-Reyes2, Peter J. Liu2
1Đại học Yale, 2Google DeepMind
yixin.liu@yale.edu, peterjliu@google.com

TÓM TẮT

Mặc dù thành công trong nhiều tác vụ ngôn ngữ tự nhiên, việc giải quyết các bài toán vẫn là một thách thức đáng kể đối với các mô hình ngôn ngữ lớn (LLM). Một khoảng cách lớn tồn tại giữa hiệu suất pass-at-one và pass-at-N của LLM trong việc giải quyết các bài toán, gợi ý rằng LLM có thể gần với việc tìm ra các giải pháp đúng, điều này thúc đẩy nghiên cứu của chúng tôi về các phương pháp fine-tuning để khai thác hiệu suất của LLM. Sử dụng tập dữ liệu MATH đầy thách thức, chúng tôi khảo sát ba chiến lược fine-tuning: (1) fine-tuning giải pháp, nơi chúng tôi fine-tune để tạo ra một giải pháp chi tiết cho một bài toán nhất định; (2) xếp hạng lại cụm giải pháp, nơi LLM được fine-tune như một công cụ xác minh/đánh giá giải pháp để lựa chọn giữa các cụm giải pháp ứng cử được tạo ra; (3) fine-tuning tuần tự đa tác vụ, tích hợp cả tác vụ tạo giải pháp và đánh giá một cách hiệu quả để nâng cao hiệu suất LLM. Với các phương pháp này, chúng tôi trình bày một nghiên cứu thực nghiệm kỹ lưỡng trên một loạt các mô hình PaLM 2 và tìm ra: (1) Chất lượng và phong cách của các giải pháp từng bước được sử dụng để fine-tuning có thể tạo ra tác động đáng kể đến hiệu suất mô hình; (2) Trong khi xếp hạng lại giải pháp và bỏ phiếu đa số đều hiệu quả trong việc cải thiện hiệu suất mô hình khi được sử dụng riêng biệt, chúng cũng có thể được sử dụng cùng nhau để có được sự cải thiện hiệu suất còn lớn hơn; (3) Fine-tuning đa tác vụ tách biệt tuần tự các tác vụ tạo giải pháp và đánh giá có thể cung cấp hiệu suất cải thiện so với baseline fine-tuning giải pháp. Được hướng dẫn bởi những hiểu biết này, chúng tôi thiết kế một công thức fine-tuning đạt được khoảng 58,8% độ chính xác trên tập dữ liệu MATH với các mô hình PaLM 2-L được fine-tune, cải thiện độ chính xác 11,2% so với hiệu suất few-shot của mô hình PaLM 2-L được huấn luyện trước với bỏ phiếu đa số.

1 GIỚI THIỆU

Giải quyết các bài toán là một tác vụ đầy thách thức ngay cả đối với các mô hình ngôn ngữ lớn tiên tiến nhất (LLM), ví dụ như GPT-4 (OpenAI, 2023) và PaLM 2 (Anil et al., 2023), vì nó đòi hỏi khả năng tư duy sáng tạo, lý luận toán học và tính toán số. Tuy nhiên, LLM đã cho thấy tiềm năng đạt được hiệu suất tốt hơn trong tác vụ giải quyết bài toán này, vì khả năng LLM có thể tìm ra câu trả lời đúng cao hơn đáng kể khi chúng được phép thử bài toán nhiều lần. Ví dụ, với giải mã tham lam, PaLM 2-L được huấn luyện trước có thể đạt được khoảng 33,4% độ chính xác. Tuy nhiên, khi lấy mẫu 64 giải pháp bằng cách sử dụng lấy mẫu nhiệt độ, có ít nhất một giải pháp đúng (pass@64) 79,4% thời gian (Bảng 2). Khoảng cách hiệu suất lớn này gợi ý rằng LLM có thể có khả năng tạo ra các giải pháp đúng trong khi vật lộn để phân biệt giải pháp đúng và sai.

Do đó, chúng tôi nghiên cứu các phương pháp fine-tuning cụ thể cho tác vụ có thể cải thiện khả năng tạo giải pháp và đánh giá của LLM sao cho khoảng cách hiệu suất nói trên có thể được giảm thiểu. Cụ thể, chúng tôi khám phá ba phương pháp fine-tuning:

(1) Fine-tuning giải pháp từng bước có giám sát (SSFT). Như một phương pháp baseline, chúng tôi khảo sát xem liệu các LLM được huấn luyện trước có thể hưởng lợi từ một giai đoạn fine-tuning có giám sát hay không. Để làm điều này, chúng tôi fine-tune các LLM để tạo ra giải pháp từng bước và câu trả lời cuối cùng như trong Lightman et al. (2023).

(2) Xếp hạng lại cụm giải pháp (SCR). Để tăng cường khả năng đánh giá giải pháp của LLM, chúng tôi tiếp tục fine-tune bộ tạo như một công cụ đánh giá giải pháp để xếp hạng lại giải pháp ứng cử. Trong khi việc lấy mẫu-xếp hạng giải pháp, hoặc xếp hạng lại, đã được khảo sát trong công trình trước đây (Cobbe et al., 2021), chúng tôi đề xuất một kỹ thuật mới có thể mang lại lợi ích của cả bỏ phiếu đa số (Wang et al., 2023) và xếp hạng lại cùng nhau, trong khi giảm chi phí xếp hạng. Cụ thể, chúng tôi đầu tiên nhóm các câu trả lời ứng cử thành các cụm khác nhau theo sự tương đương toán học của chúng, đây là một bước trung gian trong bỏ phiếu đa số. Sau đó, chúng tôi áp dụng công cụ đánh giá giải pháp cho các giải pháp trong các cụm thường xuyên nhất để đạt được cải thiện thêm so với kết quả bỏ phiếu đa số.

(3) Fine-tuning tuần tự đa tác vụ. Ngoài tác vụ đánh giá giải pháp, chúng tôi cũng quan tâm đến việc cải thiện hiệu suất của LLM trên tác vụ tạo giải pháp và khám phá xem liệu mục tiêu huấn luyện của đánh giá giải pháp có thể có lợi cho mô hình tạo giải pháp hay không. Để làm điều này, chúng tôi đề xuất một thiết lập học tập đa tác vụ tuần tự cho mô hình tạo nơi tác vụ đánh giá giải pháp được định dạng dưới dạng tác vụ tạo ngôn ngữ tự nhiên, sao cho mục tiêu huấn luyện của nó có thể cung cấp tín hiệu giám sát có ý nghĩa cho mô hình tạo giải pháp. Cụ thể, chúng tôi fine-tune mô hình theo cách tuần tự: fine-tuning (1) như một bộ tạo giải pháp (SSFT), (2) như một công cụ đánh giá giải pháp (SCR), (3) như một bộ tạo (SSFT) một lần nữa.

Chúng tôi tiến hành các thí nghiệm toàn diện trên tập dữ liệu MATH đầy thách thức với PaLM 2-S* và PaLM 2-L– các biến thể nhỏ và lớn của PaLM 2 tương ứng (Anil et al., 2023) – dẫn đến những phát hiện này:

• Đối với SSFT, chất lượng và phong cách của các giải pháp từng bước có thể tạo ra tác động lớn đến mô hình được fine-tune, vì chúng hưởng lợi nhiều hơn từ các giải pháp chi tiết, được định dạng tốt.

• Xếp hạng lại các giải pháp trong các cụm giải pháp thường xuyên nhất có thể mang lại hiệu suất tốt hơn so với xếp hạng lại tất cả các giải pháp trong khi đồng thời đạt được hiệu quả tính toán tốt hơn, điều mà chúng tôi tin rằng có thể là một thực hành tiêu chuẩn tốt hơn cho công việc trong tương lai.

• Fine-tuning tuần tự đa tác vụ được đề xuất của chúng tôi có thể cải thiện hiệu suất mô hình tạo giải pháp một cách hiệu quả hơn so với chỉ fine-tuning giải pháp có giám sát, cho thấy lợi ích của việc huấn luyện mô hình cho cả tác vụ tạo giải pháp và đánh giá, trình bày một nỗ lực thành công trong việc tận dụng tín hiệu học tập của một tác vụ đánh giá nhị phân cho một mô hình tạo.

2 KIẾN THỨC NỀN TẢNG

Giải quyết bài toán là một tác vụ quan trọng (Hendrycks et al., 2021a;b; Cobbe et al., 2021) để đo lường khả năng lý luận và tính toán số của LLM. Trong công việc này, chúng tôi tập trung vào tập dữ liệu MATH (Hendrycks et al., 2021b), bao gồm các bài toán được thu thập từ các cuộc thi toán học trung học, cùng với các giải pháp do con người viết chứa cả giải thích bằng ngôn ngữ tự nhiên và các giải pháp chân lý cơ bản cuối cùng. Tập dữ liệu MATH là thách thức ngay cả đối với các mô hình ngôn ngữ lớn tiên tiến gần đây (LLM), chẳng hạn như GPT-4 (OpenAI, 2023) và PaLM 2 (Anil et al., 2023), vì chúng chỉ có thể đạt được 42,5% và 33,2% độ chính xác pass-at-1 (Anil et al., 2023).

Độ chính xác thường được tính toán thông qua một hàm chấm điểm tự động g kiểm tra sự tương đương toán học giữa giải pháp chân lý cơ bản A và giải pháp mô hình Ã:

g(A,Ã) = 1 nếu Ã tương đương với A,
0 ngược lại. (1)

Công việc gần đây đã đề xuất nhiều phương pháp khác nhau để cải thiện hiệu suất LLM trên tác vụ giải quyết bài toán. Cụ thể, bỏ phiếu đa số, hoặc tự nhất quán, có thể mang lại cải thiện đáng kể so với hiệu suất baseline của LLM (Lewkowycz et al., 2022; Wang et al., 2023). Trong suốt công việc này, chúng tôi sẽ sử dụng hiệu suất pass-at-1, pass-at-N, và bỏ phiếu đa số của mô hình để đánh giá và so sánh mô hình. Các định nghĩa cụ thể là:

(1) Pass@1 (pass-at-1): độ chính xác của giải pháp được giải mã tham lam của mô hình AG, tức là g(A, AG).

(2) Pass@N (pass-at-N): hiệu suất oracle luôn chọn giải pháp đúng khi nó được trình bày trong N giải pháp được lấy mẫu nhiệt độ, {Ã1,Ã2, ...,ÃN}, tức là max i∈{1,2,...,N}g(A,Ãi).

(3) Maj1@N (majority-voting-at-N): N giải pháp được lấy mẫu đầu tiên được nhóm theo sự tương đương toán của chúng, tức là g(Ãi,Ãj). Sau đó, một giải pháp Ã* từ một cụm thường xuyên nhất được chọn để tính toán độ chính xác, g(A,Ã*).

(4) MajK@N: Tương tự như Pass@N, chúng tôi định nghĩa một oracle luôn chọn giải pháp đúng khi nó được trình bày trong các cụm bỏ phiếu đa số top-K, {Ã*1,Ã*2, ...,Ã*K}, vì vậy độ chính xác của nó là max i∈{1,2,...,K}g(A,Ã*i).

Một dòng công việc khác tận dụng các công cụ bên ngoài như chương trình Python để tăng cường khả năng của LLM (Chen et al., 2022; Wu et al., 2023; Yue et al., 2023; Zhou et al., 2023). Trong công việc này, chúng tôi tập trung vào việc cải thiện khả năng vốn có của LLM để giải quyết bài toán mà không cần sự giúp đỡ từ các công cụ bên ngoài.

3 PHƯƠNG PHÁP

3.1 FINE-TUNING GIẢI PHÁP CÓ GIÁM SÁT

Trong Hendrycks et al. (2021b); Cobbe et al. (2021) các mô hình được fine-tune để tạo ra không chỉ câu trả lời cuối cùng mà còn cả quá trình từng bước để giải quyết bài toán.

S, A←M(P), (2)

trong đó P là bài toán, S,A là giải pháp từng bước chân lý cơ bản và câu trả lời cuối cùng tương ứng, và M là một LLM. Trong huấn luyện, giải pháp S và câu trả lời cuối cùng A được nối thành một chuỗi văn bản duy nhất X, và mô hình được fine-tune với mất mát entropy chéo theo mô hình ước lượng khả năng cực đại (MLE):

Lmle=−logpM(X|P), (3)

trong đó pM là phân bố xác suất được đưa ra bởi mô hình ngôn ngữ tự hồi quy M:

pM(X|P) = ∏ipM(xi|X0,....,i−1, P). (4)

Ở đây, xi là token thứ i trong X, X0,....,i−1 là tiền tố trước xi.

Để thu thập các giải pháp từng bước chân lý cơ bản, chúng tôi sử dụng hai nguồn: (1) các giải pháp do con người viết gốc trong tập dữ liệu MATH, (2) các giải pháp do GPT-4 tạo ra được cung cấp trong Lightman et al. (2023) với việc gợi ý chuỗi suy nghĩ để tạo ra các giải pháp từng bước. Phân tích sơ bộ của chúng tôi cho thấy các giải pháp gốc trong tập dữ liệu MATH trừu tượng hơn trong khi các giải pháp do GPT-4 tạo ra chi tiết và cụ thể hơn.

3.2 XẾP HẠNG LẠI CỤM GIẢI PHÁP

Chúng tôi lưu ý rằng có hai khoảng cách đáng kể đối với hiệu suất giải quyết bài toán của LLM trong Bảng 2: (1) khoảng cách giữa kết quả giải mã tham lam của mô hình (Pass@1) và kết quả bỏ phiếu đa số (Maj1@N); (2) khoảng cách giữa hiệu suất best-at-1 bỏ phiếu đa số của mô hình (Maj1@N) và hiệu suất best-at-K (MajK@N). Để thu hẹp những khoảng cách này, chúng tôi fine-tune LLM được huấn luyện trước như một công cụ xác minh/đánh giá giải pháp, theo Cobbe et al. (2021). Tuy nhiên, không giống như trong công việc trước đây nơi một số lượng lớn (ví dụ, 1000) giải pháp ứng cử đều được xếp hạng lại bởi công cụ đánh giá, chúng tôi kết hợp sức mạnh của bỏ phiếu đa số và xếp hạng lại bằng cách chỉ xếp hạng lại các cụm giải pháp top-K. Chúng tôi tin rằng chiến lược xếp hạng lại này vừa mạnh mẽ vừa tiết kiệm chi phí, như sẽ được trình bày chi tiết trong phần sau.

Để sử dụng công cụ đánh giá để chấm điểm từng giải pháp ứng cử, chúng tôi hình thành tác vụ chấm điểm như một bài toán phân loại trong định dạng hoàn thành văn bản, được lấy cảm hứng từ công việc liên quan về việc sử dụng LLM để đánh giá văn bản (Liu et al., 2023; Fu et al., 2023). Cụ thể, chúng tôi định nghĩa một hàm ánh xạ T chuyển đổi bài toán P và một giải pháp ứng cử X̃ thành một gợi ý T(P,X̃): "Đây là một bài toán: P. Đây là một giải pháp ứng cử: X̃. Giải pháp ứng cử trên là ". Sau đó chúng tôi diễn giải xác suất dự đoán mô hình của từ "đúng" (hoặc "sai") là token tiếp theo như xác suất của giải pháp đúng (hoặc sai):

pcls("đúng" |X̃, P ) =pM("đúng" |T(P,X̃)), (5)

pcls("sai" |X̃, P ) =pM("sai" |T(P,X̃)). (6)

Sau đó chúng tôi có thể định nghĩa xác suất được chuẩn hóa sau đây như điểm số giải pháp ứng cử:

Scls(X̃|P) = pcls("đúng" |X̃, P) / (pcls("đúng" |X̃, P) + pcls("sai" |X̃, P)). (7)

Với định dạng chấm điểm này, chúng tôi khảo sát hai mục tiêu huấn luyện:

(1) Mất mát biên cho so sánh từng cặp:

Lcls-margin = max(0, logScls(X̃sai|P)−logScls(X̃đúng|P) +λ), (8)

trong đó X̃đúng và X̃sai đại diện cho một giải pháp đúng và sai tương ứng, và λ là một siêu tham số cho biên.

(2) Mất mát entropy chéo cho phân loại. Định dạng chấm điểm mà chúng tôi thiết kế tương đương với một bài toán phân loại đa lớp nơi "đúng" và "sai" là những lựa chọn hợp lệ duy nhất. Do đó, chúng tôi có thể fine-tune mô hình bằng cách sử dụng mất mát entropy chéo cho tác vụ phân loại này:

Lcls-xent =− 1{X̃ là đúng}(X̃) logpcls("đúng" |X̃, P) + 1{X̃ là sai}(X̃) logpcls("sai" |X̃, P) (9)

3.3 FINE-TUNING TUẦN TỰ ĐA TÁC VỤ

Mục tiêu huấn luyện dựa trên MLE được định nghĩa trong Eq. 3 hơi mâu thuẫn với mục tiêu đánh giá nhị phân cuối cùng – liệu câu trả lời cuối cùng có đúng hay không. Công việc liên quan đã khám phá việc điều chỉnh huấn luyện tốt hơn với việc đánh giá tác vụ bằng cách sử dụng mục tiêu học tập đối lập (Edunov et al., 2018; Liu et al., 2022; Zhao et al., 2023), diễn giải xác suất dự đoán mô hình của một giải pháp ứng cử X̃ như điểm chất lượng của nó và sử dụng mất mát biên để khuyến khích mô hình gán xác suất cao hơn cho các ứng cử tốt hơn:

Lseq= max(0, logpM(X̃sai|P)−logpM(X̃đúng|P) +λ). (10)

Sau đó, mô hình được fine-tune với các mục tiêu huấn luyện MLE và đối lập cùng nhau:

Lctr=Lseq+α1Lmle, (11)

trong đó α1 là một siêu tham số.

Tuy nhiên, mục tiêu học tập đối lập có thể không phù hợp với tác vụ giải quyết bài toán vì bản chất nhị phân của tác vụ – mục tiêu đòi hỏi mô hình sử dụng khả năng của token cho hai mục đích: (1) dự đoán token tiếp theo, và (2) đánh giá chất lượng của toàn bộ chuỗi văn bản. Nó có thể là một mục tiêu hợp lý cho các tác vụ tạo ngôn ngữ tự nhiên như tóm tắt văn bản nơi tính đúng đắn của dự đoán token tiếp theo có liên quan chặt chẽ đến chất lượng văn bản tổng thể. Tuy nhiên, đối với giải quyết bài toán, tính đúng đắn của một giải pháp có thể được quyết định bởi chỉ một vài token, làm cho tác vụ dự đoán token tiếp theo xa hơn và không tương thích với tác vụ đánh giá giải pháp. Do đó, chúng tôi kết hợp các mục tiêu huấn luyện trong §3.1 (Eq. 3) và §3.2 (Eq. 8 và Eq. 9), giới thiệu một thiết lập học tập mới hình thành cả tác vụ tạo giải pháp bài toán và đánh giá như các tác vụ tạo ngôn ngữ tự nhiên:

Lmul-margin =Lcls-margin +α2Lmle, (12)

Lmul-xent =Lcls-xent +α3Lmle, (13)

trong đó α2 và α3 là các siêu tham số. Chúng tôi tin rằng chúng tôi có thể tận dụng tốt hơn khả năng của LLM với thiết lập huấn luyện này vì nó gần hơn với tác vụ pre-training (tức là dự đoán token tiếp theo). Trong các thí nghiệm sơ bộ của chúng tôi, chúng tôi thấy rằng khó cân bằng hai thuật ngữ mất mát trong Eq. 12 và Eq. 13 và các mô hình bắt đầu overfit mục tiêu huấn luyện MLE rất sớm, có thể do kích thước hạn chế của tập dữ liệu. Do đó, chúng tôi tối ưu hóa mục tiêu đa tác vụ theo cách tuần tự – thay vì fine-tune mô hình trên cả hai mục tiêu huấn luyện, chúng tôi đầu tiên fine-tune mô hình như một bộ tạo (Eq. 8 hoặc Eq. 9), sau đó như một công cụ đánh giá (Eq. 3), sau đó cuối cùng như một bộ tạo một lần nữa.

4 THÍ NGHIỆM

4.1 THIẾT LẬP THÍ NGHIỆM

Tập dữ liệu. Các thí nghiệm của chúng tôi được tiến hành trên tập dữ liệu MATH. Để tránh overfitting, chúng tôi theo Lightman et al. (2023) bằng cách sử dụng các phần chia dữ liệu họ cung cấp, nơi 4,5K ví dụ thử nghiệm gốc được sử dụng để huấn luyện và xác thực, và 500 ví dụ thử nghiệm còn lại được sử dụng để đánh giá mô hình. Chúng tôi tận dụng hai nguồn giải pháp từng bước đúng cho việc huấn luyện mô hình: (1) các giải thích do con người viết gốc được cung cấp trong tập dữ liệu MATH; (2) các giải pháp đúng do mô hình tạo ra được cung cấp trong PRM800K (Lightman et al., 2023), chỉ bao gồm một tập con các bài toán trong tập dữ liệu MATH gốc. Thống kê tập dữ liệu được cung cấp trong Bảng 1.

Đánh giá. Chúng tôi báo cáo độ chính xác giải pháp trung bình (hoặc tính đúng đắn) cho tất cả các thí nghiệm. Tính đúng đắn của giải pháp được tạo ra được so sánh với giải pháp chân lý cơ bản bằng cách sử dụng script chấm điểm tự động được cung cấp bởi Lightman et al. (2023). Script này kiểm tra sự tương đương toán học thay vì sự tương đương văn bản đơn giản. Hai phương pháp tạo giải pháp chủ yếu được sử dụng để đánh giá hiệu suất mô hình: (1) giải mã tham lam cho hiệu suất Pass@1, (2) lấy mẫu nucleus (Holtzman et al., 2020) cho hiệu suất bỏ phiếu đa số (Maj1@N), nơi chúng tôi sử dụng cùng các siêu tham số lấy mẫu như trong Lewkowycz et al. (2022). Cụ thể, nhiệt độ lấy mẫu được đặt thành 0,6, và giá trị top-p được đặt thành 0,95.

4.2 THÍ NGHIỆM I: FINE-TUNING GIẢI PHÁP CÓ GIÁM SÁT

Chúng tôi fine-tune PaLM 2-S* và PaLM 2-L trên các giải pháp từng bước với mục tiêu huấn luyện MLE (Eq. 3). Ba chiến lược fine-tuning cụ thể được khám phá: (1) fine-tuning chỉ sử dụng các giải pháp MATH gốc; (2) fine-tuning chỉ sử dụng các giải pháp GPT-4 PRM800K; (3) fine-tuning trên cả giải pháp MATH và PRM800K. Chúng tôi sử dụng hiệu suất mô hình trên tập xác thực để lựa chọn checkpoint, và tất cả các mô hình được fine-tune đạt được hiệu suất tốt nhất trong vòng hai epoch. Kết quả được hiển thị trong Bảng 2, nơi hiệu suất few-shot với các mô hình PaLM 2 được huấn luyện trước được cung cấp để so sánh. Kết quả few-shot được thu được bằng cách sử dụng một gợi ý 4-shot tùy chỉnh được thiết kế trong Lewkowycz et al. (2022).

Chúng tôi quan sát thấy rằng fine-tuning nói chung hữu ích cho mô hình để đạt được hiệu suất tốt hơn so với hiệu suất few-shot của các checkpoint được huấn luyện trước. Hơn nữa, chất lượng và phong cách của các giải pháp có thể có tác động lớn đến hiệu suất mô hình, vì các mô hình được fine-tune trên các giải pháp PRM800K đạt được hiệu suất tốt hơn đáng kể so với những mô hình được fine-tune trên các giải pháp MATH gốc.
