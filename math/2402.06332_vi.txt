# 2402.06332.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/math/2402.06332.pdf
# Kích thước tệp: 1614922 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Báo cáo Kỹ thuật
InternLM-Math: Mô hình Ngôn ngữ Lớn Toán học Mở hướng tới
Lý luận Có thể Xác minh
Huaiyuan Ying1,2*, Shuo Zhang1,3∗, Linyang Li3, Zhejian Zhou1,4, Yunfan Shao1,3
Zhaoye Fei1,3, Yichuan Ma1, Jiawei Hong1,3, Kuikun Liu1, Ziyi Wang1, Yudong Wang1
Zijian Wu1,5, Shuaibin Li1, Fengzhe Zhou1, Hongwei Liu1, Songyang Zhang1
Wenwei Zhang1, Hang Yan1, Xipeng Qiu3, Jiayu Wang1, Kai Chen1, Dahua Lin1
1Shanghai AI Laboratory
2Tsinghua University
3Fudan University, School of Computer Science
4University of Southern California
5Shanghai Jiaotong University
internlm@pjlab.org.cn
Tóm tắt
Khả năng toán học của các mô hình ngôn ngữ lớn có thể thể hiện khả năng lý luận trừu tượng của chúng. Trong bài báo này, chúng tôi giới thiệu và mở mã nguồn các LLM lý luận toán học InternLM-Math được tiếp tục tiền huấn luyện từ InternLM2. Chúng tôi thống nhất lý luận chuỗi tư duy, mô hình hoá phần thưởng, lý luận hình thức, tăng cường dữ liệu, và thông dịch mã trong một định dạng seq2seq thống nhất và giám sát mô hình của chúng tôi để trở thành một người lý luận, xác minh, chứng minh và tăng cường toán học đa năng. Những khả năng này có thể được sử dụng để phát triển các LLM toán học tiếp theo hoặc tự lặp. InternLM-Math đạt được hiệu suất tiên tiến mã nguồn mở dưới thiết lập học trong ngữ cảnh, tinh chỉnh có giám sát, và lý luận hỗ trợ mã trong các benchmark không chính thức và chính thức khác nhau bao gồm GSM8K, MATH, kỳ thi toán Hungary, MathBench-ZH, và MiniF2F. Mô hình tiền huấn luyện của chúng tôi đạt 30.3 trên bộ test MiniF2F mà không cần tinh chỉnh. Chúng tôi tiếp tục khám phá cách sử dụng LEAN để giải các bài toán và nghiên cứu hiệu suất của nó dưới thiết lập học đa nhiệm vụ cho thấy khả năng sử dụng LEAN như một nền tảng thống nhất để giải và chứng minh trong toán học. Các mô hình, mã và dữ liệu của chúng tôi được phát hành tại https://github.com/InternLM/InternLM-Math .
Demo: https://huggingface.co/spaces/internlm/internlm2-math-7b
1 Giới thiệu
Các mô hình ngôn ngữ lớn (Brown et al., 2020; Lewkowycz et al., 2022b; Taylor et al., 2023;
OpenAI, 2023; Anil et al., 2023; InternLM, 2023; Azerbayev et al., 2023b; Google, 2023; Shao
et al., 2024) đã cho thấy khả năng đáng kể trong các nhiệm vụ lý luận toán học từ cấp tiểu học
(Cobbe et al., 2021) đến cấp trung học (Hendrycks et al., 2021a) bằng cách sử dụng lý luận
chuỗi tư duy (Wei et al., 2022) hoặc lý luận chương trình tư duy (Chen et al., 2023b;
Gao et al., 2023).
Xây dựng các mô hình như vậy đòi hỏi tiền huấn luyện trên kho dữ liệu toán học và tinh chỉnh
có giám sát trên các bài toán. Chúng tôi giới thiệu InternLM-Math dựa trên các mô hình InternLM2-Base1. InternLM2 cho thấy hiệu suất mạnh trong nhiều khía cạnh khác nhau bao gồm toán học, mã, trải nghiệm trò chuyện,
tuân theo hướng dẫn, và viết sáng tạo. Chúng tôi thu thập và tập hợp dữ liệu liên quan đến toán học để
tiếp tục tiền huấn luyện trên InternLM2-Base và đạt được hiệu suất tiên tiến trên
∗Đóng góp ngang nhau.
1https://github.com/InternLM/InternLM
1arXiv:2402.06332v2  [cs.CL]  24 May 2024

--- TRANG 2 ---
Báo cáo Kỹ thuật
các benchmark lý luận toán học không chính thức và chính thức vượt trội hơn Minerva (Lewkowycz et al., 2022b)
và Llemma (Azerbayev et al., 2023b).
Trong quá trình tinh chỉnh có giám sát, chúng tôi giám sát InternLM-Math không chỉ giải
các bài toán bằng chuỗi tư duy và thông dịch mã mà còn nhiều nhiệm vụ để phát triển
các LLM toán học bao gồm mô hình hoá phần thưởng và trợ lý tăng cường. Chúng tôi cũng giới thiệu việc sử dụng
LEAN để dịch giữa ngôn ngữ tự nhiên và các câu lệnh LEAN (Han et al., 2021),
giải các bài toán dễ, và chứng minh các câu lệnh toán học. Chuỗi mô hình InternLM-Math
đạt được hiệu suất tiên tiến mã nguồn mở trên nhiều benchmark2và ghi điểm
hơn 90% so với GPT-4 (OpenAI, 2023).
Đóng góp của chúng tôi bao gồm:
•Chúng tôi mở mã nguồn các LLM cơ sở và SFT trong lý luận toán học. Nó đạt được
SOTA mã nguồn mở dưới thiết lập ICL, SFT, RM reranking, và hỗ trợ Python trong
các benchmark khác nhau.
•Chúng tôi thống nhất lý luận chuỗi tư duy, mô hình hoá phần thưởng, tăng cường dữ liệu,
và lý luận hình thức dưới một định dạng seq2seq thống nhất. Chúng tôi giám sát mô hình của chúng tôi với
cả khả năng giải quyết vấn đề và xác minh.
•Chúng tôi đề xuất lý luận xen kẽ với lập trình (RICO) và đạt được lý luận toán học
tiên tiến với sự hỗ trợ của Python.
•Chúng tôi khám phá việc sử dụng LEAN để giải các bài toán từ toán học và
nghiên cứu hiệu suất của nó liên quan đến kích thước dữ liệu trong quá trình học đa nhiệm vụ.

Hình 1: Hiệu suất MATH 4-shot với bỏ phiếu đa số 256 lần. So sánh dựa trên
mô hình cơ sở tiền huấn luyện của chúng tôi, Llemma (Azerbayev et al., 2023b), và Minerva (Lewkowycz
et al., 2022b). Hình được chỉnh sửa từ Azerbayev et al. (2023b).
2 Nghiên cứu Liên quan
Tiền huấn luyện Toán học Tiền huấn luyện giúp các LLM thu được kiến thức tính toán và toán học
từ các nguồn khác nhau, chẳng hạn như kho dữ liệu toán học (Han et al., 2021; Lewkowycz et al.,
2022b; Paster et al., 2023; Wang et al., 2023d), bộ bài toán (Lightman et al., 2023), và
dữ liệu tổng hợp (Hendrycks et al., 2021b; Liu & Low, 2023; Yang et al., 2023b). ArXiv với
2Công việc này đồng thời với Deepseek-Math.
2

--- TRANG 3 ---
Báo cáo Kỹ thuật
nội dung toán học phong phú thường được sử dụng trong tiền huấn luyện toán học (Lewkowycz et al., 2022a;
Taylor et al., 2023; Azerbayev et al., 2023a). Paster et al. (2023) trích xuất các trang web toán học
từ common crawl có thể bổ sung cho arXiv. Các bài toán toán học bao gồm
GSM8K (Cobbe et al., 2021) và MATH (Hendrycks et al., 2021a) được sử dụng trong
tiền huấn luyện (OpenAI, 2023). Các bài toán tổng hợp được tạo ra thông qua quy tắc hoặc script chương trình toán học
(Hendrycks et al., 2021b) cũng có thể được sử dụng để tiền huấn luyện. InternLM-Math thu thập dữ liệu
tiền huấn luyện từ kho dữ liệu toán học và dữ liệu tổng hợp thiết lập khả năng toán học của nó.
Tinh chỉnh Toán học Xây dựng một bộ dữ liệu chuỗi tư duy tăng cường mạnh hơn (Yu et al.,
2023b; Yue et al., 2023; Li et al., 2023; Liu & Yao, 2024) cho SFT để cải thiện hiệu suất lý luận toán học
đã nhận được rất nhiều quan tâm. Tăng cường bài toán (Luo et al., 2023; Yu et al.,
2023b; Li et al., 2023; Liu & Yao, 2024) và tăng cường đường dẫn lý luận (Zelikman et al.,
2022; Huang et al., 2022; Zhu et al., 2023; Yuan et al., 2023) là hai cách phổ biến
cho thấy hiệu quả đáng kể. Học tăng cường (Uesato et al., 2022; Luo et al., 2023;
Wang et al., 2023b; Singh et al., 2023) cũng đã được khám phá để tinh chỉnh các mô hình ngôn ngữ
nâng cao hiệu suất lý luận đòi hỏi một mô hình phần thưởng (tức là trình xác minh) có thể phân biệt
các quá trình lý luận và câu trả lời đúng và sai. So với các công trình trước đây, chúng tôi
không chỉ xây dựng một bộ dữ liệu SFT chuỗi tư duy mạnh hơn mà còn tích hợp nhiều khả năng
tất cả trong một bao gồm xác minh, chứng minh, thông dịch mã, và trợ lý tăng cường dữ liệu.
Trình xác minh Toán học Các mô hình phần thưởng (trình xác minh) thường được sử dụng để sắp xếp lại
nhiều đường dẫn lý luận ứng viên. Các mô hình phần thưởng kết quả (ORM) (Cobbe et al., 2021) và các mô hình phần thưởng quá trình (PRM) (Uesato et al., 2022; Lightman et al., 2023) tận dụng các LLM để xác minh
tính đúng đắn của các câu trả lời và quá trình. ORM kém hiệu quả hơn PRM (Lightman et al., 2023;
Wang et al., 2023b) trong khi PRM đòi hỏi nhiều chuyên gia con người để gán nhãn. Để giảm
lao động con người trong việc gán nhãn PRM, Yu et al. (2023a); Wang et al. (2023b) xác định
tính đúng đắn của các quá trình dựa trên nhiều hoàn thành từ quá trình. So với các công trình trước đây
đã huấn luyện hai mạng riêng biệt cho mạng chính sách và mô hình phần thưởng. Chúng tôi
kết hợp hai chức năng này thành một định dạng seq2seq thống nhất. Sau SFT, mô hình của chúng tôi có thể được
sử dụng để giải quyết vấn đề hoặc xác minh các đường dẫn lý luận. Hơn nữa, chúng tôi khám phá việc tận dụng
ngôn ngữ toán học hình thức LEAN để xác minh các đường dẫn lý luận bằng cách dịch sang mã LEAN.
Thông dịch Mã Toán học Các thông dịch mã có thể bổ sung cho các LLM với khả năng tính toán phức tạp
thông qua các thư viện Python khác nhau. Các khám phá ban đầu sử dụng chương trình tư duy (Chen
et al., 2023b; Gao et al., 2023) nhưng thiếu tương tác giữa các LLM và công cụ, có thể
không thể xử lý các tình huống đòi hỏi lý luận và tính toán nhiều bước. Các nghiên cứu gần đây
(Gou et al., 2023; Wang et al., 2023a) cố gắng tích hợp thông dịch mã một cách liền mạch hơn
với lý luận bằng cách cho phép mô hình tóm tắt kết quả dựa trên kết quả thực thi mã một lần.
InternLM-Math khám phá lý luận xen kẽ với lập trình (RICO), tức là,
các quá trình lý luận và lập trình được xen kẽ trong nhiều vòng cho đến khi bài toán được
giải quyết, điều này tự nhiên hơn và gần gũi với quá trình giải quyết vấn đề và khai thác đầy đủ
khả năng lý luận của LLM.
Người chứng minh Toán học Giải một bài toán với một câu trả lời đúng bằng các LLM vẫn không thể đảm bảo
tính đúng đắn của quá trình. Tuy nhiên, chứng minh một câu lệnh sử dụng ngôn ngữ hình thức như Isabelle
(Paulson, 2000), LEAN (Moura & Ullrich, 2021), và Coq (The Coq Development Team,
2023) có thể hứa hẹn điều đó. Huấn luyện một LLM để chứng minh định lý tự động trong ngôn ngữ hình thức
(Han et al., 2021; Polu et al., 2022; Azerbayev et al., 2023a; Yang et al., 2023a; Welleck & Saha,
2023; Zheng et al., 2023) khó khăn do dữ liệu thưa thớt. InternLM-Math đạt được hiệu suất few-shot
tiên tiến trên MiniF2F (Zheng et al., 2021) cho thấy tiềm năng trong việc xây dựng một
người chứng minh toán học mạnh.
3 Tiền huấn luyện
Trong phần này, trước tiên chúng tôi mô tả thành phần dữ liệu tiền huấn luyện của chúng tôi. Sau đó, chúng tôi nêu
phương pháp hậu xử lý dữ liệu mà chúng tôi thực hiện trên dữ liệu huấn luyện. Cuối cùng, chúng tôi đi sâu vào
chi tiết chiến lược huấn luyện của chúng tôi.
3

--- TRANG 4 ---
Báo cáo Kỹ thuật
Lĩnh vực Bộ dữ liệu Token Duy nhất(B) Epochs Token(B)
Dữ liệu Thu thập CC knowledge pile 20 4 80
Dữ liệu Lĩnh vực Đặc biệt open-web-math 6 4 24
algebraic-stack 4 4 16
khác 1 4 4
Dữ liệu Tổng hợp num 0.2 5 1
Tổng cộng - 31.2 - 125
Bảng 1: Sử dụng dữ liệu tiền huấn luyện cho InternLM-Math. Token Duy nhất đề cập đến số lượng
token trong bộ dữ liệu gốc. Token đề cập đến tổng số token trong bộ dữ liệu
được tiêu thụ trong quá trình tiền huấn luyện.
3.1 Thành phần dữ liệu tiền huấn luyện
Để đạt được hiệu suất cạnh tranh trong lĩnh vực toán học, chúng tôi đã thu thập một
bộ sưu tập đa dạng của dữ liệu chất lượng cao. Chúng tôi không tận dụng bất kỳ dữ liệu được tạo ra bởi LLM nào trong
tiền huấn luyện. Dữ liệu này được phân loại một cách có hệ thống thành các danh mục sau:
Dữ liệu Thu thập CC Chúng tôi sử dụng Query of CC (Fei et al., 2024) để thu thập kho dữ liệu huấn luyện
cho InternLM2-Math-Base. Chúng tôi chọn kho dữ liệu liên quan đến toán học từ kho dữ liệu Query of CC
làm phần đầu tiên của dữ liệu tiền huấn luyện tiếp tục. Phần này bao gồm 20B token.
Dữ liệu Lĩnh vực Cụ thể Chúng tôi đã chọn từ bộ dữ liệu mã nguồn mở (Azerbayev et al., 2023b)
và các bộ dữ liệu chất lượng cao nội bộ trong lĩnh vực toán học bao gồm trang web, mã,
arXiv, diễn đàn, và sách. Phần này bao gồm 11B token.
Dữ liệu Tổng hợp Chúng tôi tổng hợp dữ liệu phép toán số để cải thiện khả năng
phép toán số của mô hình. Chúng tôi bao gồm năm phép toán phổ biến bao gồm số học,
lũy thừa, logarit, lượng giác, và tính toán đa thức. Đối với các phép toán số, chúng tôi
duyệt qua một tập hợp các giá trị thường được sử dụng và lấy mẫu ngẫu nhiên một phạm vi rộng hơn
của các giá trị trong phạm vi số 10 chữ số. Để ngăn mô hình overfitting với các
template cụ thể, chúng tôi xây dựng đa dạng các template để đảm bảo khả năng tính toán số
của mô hình tổng quát hóa ở một mức độ nào đó. Phần này bao gồm 0.2B token.
3.2 Hậu xử lý dữ liệu
Để nâng cao chất lượng của dữ liệu huấn luyện, chúng tôi tuân theo cách tiếp cận của Query of CC(Fei et al.,
2024) và thực hiện một loạt các chiến lược hậu xử lý dữ liệu. Cụ thể, chúng tôi đã huấn luyện một
mô hình chấm điểm để xác định các bộ dữ liệu chất lượng cao. Sau đó, chúng tôi sử dụng phương pháp Minhash-LSH
để khử trùng lặp dữ liệu huấn luyện. Trong thực tế của chúng tôi, chúng tôi lọc ra dữ liệu trùng lặp
với độ tương tự vượt quá 0.7.
Chúng tôi tiếp tục thực hiện khử nhiễm công thức chính xác cho dữ liệu lĩnh vực đặc biệt trên
bộ test MATH. Chúng tôi trích xuất tất cả các công thức trong một đoạn văn cho trước. Nếu việc nối
các công thức trùng với bất kỳ công thức nào trong bộ test MATH, chúng tôi đơn giản loại bỏ chúng.
3.3 Chiến lược huấn luyện
Sau khi thu thập và hậu xử lý dữ liệu thu thập và lĩnh vực cụ thể chất lượng cao, chúng tôi đặt
các epoch huấn luyện khác nhau. Để biết chi tiết về tỷ lệ dữ liệu, xem Bảng 1. Tổng cộng, chúng tôi đã thu thập
31.2 tỷ token chất lượng cao trên các bộ dữ liệu, chúng tôi tuân theo Muennighoff et al. (2023) để
sử dụng 4 epoch cho hầu hết các bộ dữ liệu. Để tiếp tục tiền huấn luyện trên InternLM2-Base, chúng tôi áp dụng
chiến lược huấn luyện tương tự trên các kích cỡ mô hình khác nhau, như được mô tả trong InternLM (InternLM,
2023), và chúng tôi sử dụng InternEvo3làm framework huấn luyện. Trong quá trình huấn luyện, chúng tôi đã sử dụng
3https://github.com/InternLM/InternEvo
4

--- TRANG 5 ---
Báo cáo Kỹ thuật
ProblemCOTORMProblemCOTPRMProblemCOTLEANProblem
CILEANProblemProblemLEANLEAN StateLEANNLStatementCOTCode InterpreterLEAN Calculation CodeLEAN StateLEAN TacticLEAN Calculation CodeCorrect / IncorrectStep K: Correct / IncorrectInternLM2-MathAUGProblemAUG ProblemProblemAUGAUG ProblemCOTCorrect/IncorrectProblemCOTNL StatementProblemRMCOTNL StatementLEAN StateLEANLEAN TacticLEAN
Hình 2: Phần bên trái hiển thị các định dạng truy vấn và phản hồi trong SFT. Các phần bên phải hiển thị
hai cách sử dụng có thể có của mô hình SFT của chúng tôi. Phần trên bên phải là một pipeline tổng hợp
bài toán mới sử dụng trợ lý tăng cường, COT, và khả năng RM của chúng tôi. Phần dưới bên phải là một pipeline
giải quyết bài toán không chính thức sử dụng ngôn ngữ hình thức với khả năng COT và lý luận hình thức.
độ dài ngữ cảnh là 4096. Đối với các tài liệu quá dài hoặc quá ngắn, chúng tôi cắt bớt hoặc
nối chúng lại để đạt được độ dài ngữ cảnh mong muốn. Chúng tôi áp dụng huấn luyện độ chính xác hỗn hợp
với bfloat16 và FlashAttention2 (Dao, 2023) để đạt được việc sử dụng bộ nhớ tối ưu
và hiệu quả huấn luyện. Bộ tối ưu hóa AdamW tiêu chuẩn (Loshchilov & Hutter, 2017) được
sử dụng với các siêu tham số β1=0.9,β2=0.95, và weight decay =0.1. Chúng tôi sử dụng một
bộ lập lịch tốc độ học cosine tiêu chuẩn. Cụ thể, tốc độ học của mô hình đạt
tối đa lrmax=3e−5 sau 2000 bước warm-up, và sau đó giảm dần
xuống mức tối thiểu lrmin=3e−6 trong suốt quá trình huấn luyện. Trong quá trình tiền huấn luyện
tiếp tục, tổng cộng 125 tỷ token đã được huấn luyện. Đối với mô hình 20B, chúng tôi dừng sớm tại
80 tỷ token dựa trên hiệu suất học trong ngữ cảnh.
4 Tinh chỉnh Có giám sát
Không giống như các LLM chuyên về toán học khác tập trung vào giải quyết bài toán, mô hình của chúng tôi
nhắm đến trở thành người giải toán và cũng sẵn sàng để tự cải thiện đòi hỏi các khả năng
bao gồm tăng cường bài toán, mô hình hoá phần thưởng, tự xác minh, lý luận hình thức, và
thông dịch mã. Dữ liệu SFT của chúng tôi chứa dữ liệu chất lượng cao do con người viết, tạo ra theo quy tắc, và
do LLM tạo ra cho các khả năng nêu trên, thành phần dữ liệu chi tiết có thể
được thấy trong Bảng 16 và các phản hồi được tạo ra bởi mô hình của chúng tôi có thể được thấy trong Phụ lục D. Chúng tôi hiển thị
định dạng truy vấn và phản hồi của SFT trong Hình 2.
Chuỗi tư duy Chúng tôi sử dụng MetaMath (Yu et al., 2023b) làm nguồn dữ liệu
chuỗi tư duy tiếng Anh cơ bản mang lại sự cải thiện lý luận nhất quán cho
các LLM khác nhau. Chúng tôi tận dụng các bộ dữ liệu tiếng Trung nội bộ cho khả năng chuỗi tư duy tiếng Trung.
Để cải thiện khả năng lý luận toán học của các điểm yếu của mô hình, chúng tôi áp dụng tăng cường đường dẫn lý luận
(Zelikman et al., 2022) trên các bộ dữ liệu cụ thể. Lấy cảm hứng từ Uesato et al. (2022),
chúng tôi sử dụng câu trả lời cuối cùng và PRM được huấn luyện của chúng tôi để lọc các đường dẫn lý luận được tăng cường.
Mặc dù chúng tôi thêm dữ liệu tính toán tổng hợp trong tiền huấn luyện, các mô hình vẫn hoạt động kém
trong các phép tính phức tạp. Theo tinh thần của scratchpad (Nye et al., 2021; Liu & Low, 2023),
chúng tôi phân tách các phép tính phức tạp bao gồm tính toán nhiều dấu ngoặc, tính toán
thập phân, tính toán và rút gọn phân số, bài toán dư, và tính toán lũy thừa
thành các bước có thể tính toán. Chúng tôi thấy rằng việc đơn giản thêm dữ liệu tính toán giống scratchpad
sẽ không ảnh hưởng đến hành vi tính toán của mô hình. Lý do đến từ định dạng của
dữ liệu SFT đòi hỏi mô hình ngay lập tức tạo ra câu trả lời phương trình như
(12+17)3=24389 bên trong quá trình chuỗi tư duy, và chúng tôi gọi nó là
5

--- TRANG 6 ---
Báo cáo Kỹ thuật
ảo giác tính toán . Để giảm thiểu ảo giác tính toán , chúng tôi so khớp và viết lại các phương trình như vậy
((12+17)3=293=841∗29= (800+40+1)∗29=23200 +1160+29=24389) bên trong
các quá trình chuỗi tư duy càng nhiều càng tốt. Điều này cải thiện khả năng tính toán trong
lý luận chuỗi tư duy.
Chúng tôi cũng giới thiệu một số khả năng toán học cơ bản bao gồm kiểm tra số nguyên tố, trò chơi
24, phân tích một số nguyên, và tính diện tích tam giác. Nguyên tắc tổng hợp
dữ liệu này của chúng tôi là Mô hình tạo ra chuỗi tư duy dựa trên một biểu đồ luồng thuật toán. Việc dự đoán
token tiếp theo phải được xác định bởi các token đã tạo ra và không phụ thuộc vào các token tương lai . Sử dụng
tính diện tích tam giác làm ví dụ, chúng tôi yêu cầu mô hình trước tiên kiểm tra xem ba cạnh có thể
tạo thành một tam giác và nếu nó là một tam giác vuông. Mô hình không cần phải giả định nó tạo thành
một tam giác vuông mà không có lý luận chuỗi tư duy. Bảng 4 đưa ra một ví dụ. Chúng tôi cũng
đưa ra một ví dụ phản hồi không tuân theo nguyên tắc của chúng tôi vì việc dự đoán tam giác vuông
đòi hỏi mô hình phải biết các token tương lai 32+42=52.
Diện tích Tam giác
Truy vấn: Tính diện tích của một tam giác có độ dài: 3,4,5.
Phản hồi: Vì 3 +4>5, chúng có thể tạo thành một tam giác.
32=9, 42=16, 52=25, 9+16=25.
Vậy 32+42=52, đó là một tam giác vuông.
Diện tích là 3 ∗4/2=12/2 =6.
Phản hồi Tệ: Vì tam giác này là một tam giác vuông (32+42=52).
Diện tích là 3 ∗4/2=12/2 =6.
Mô hình hoá Phần thưởng Mô hình hoá phần thưởng giúp mô hình sắp xếp lại việc tạo ra của nó tốt hơn và
lọc dữ liệu tổng hợp để tự cải thiện. Lấy cảm hứng từ Math Shepherd (Wang et al., 2023b), chúng tôi
thống nhất ORM và PRM thành cùng một định dạng học seq2seq. Chúng tôi cũng thêm dữ liệu ORM tiếng Trung
sử dụng mô hình tự lấy mẫu chuỗi tư duy với các câu trả lời cuối cùng tương ứng.
Lý luận Toán học Hình thức Thay vì giải quyết vấn đề bằng ngôn ngữ tự nhiên không chính thức, dữ liệu của chúng tôi
cũng bao gồm các mẫu lý luận hình thức dựa trên LEAN 3. Mục tiêu của chúng tôi là sử dụng LEAN như một
người giải, xác minh, và chứng minh. Chúng tôi chưng cất gpt-4-1106 để giải quyết các bài toán trong bộ huấn luyện GSM8K
sử dụng LEAN 3. Chúng tôi chọn tất cả các mã có thể tạo ra câu trả lời đúng. Phần này bao gồm
6705 mẫu. Chúng tôi chia đôi thành định dạng tạo ra LEAN dựa trên bài toán được sử dụng
để huấn luyện một người giải và nửa kia thành định dạng dịch giữa LEAN và COT
được sử dụng để huấn luyện một trình xác minh. Trình xác minh dịch COT sang LEAN và có thể xác minh bằng cách kiểm tra
mọi phép tính của COT dựa trên phép tính LEAN. Như một người chứng minh, mô hình cần
dịch giữa các câu lệnh không chính thức và chính thức và tạo ra các chiến thuật dựa trên
trạng thái LEAN. Chúng tôi sử dụng bộ dữ liệu MathLib-train (mathlib Community, 2020) được trích xuất từ Han
et al. (2021) làm dữ liệu huấn luyện người chứng minh.
Trợ lý Tăng cường Một khía cạnh quan trọng khác của dữ liệu của chúng tôi là giúp xây dựng
dữ liệu tổng hợp để tự cải thiện. Bằng cách diễn đạt lại hoặc tăng cường một câu hỏi, người ta có thể dễ dàng có được
một sự đa dạng câu hỏi mở rộng (Luo et al., 2023; Yu et al., 2023b; Li et al., 2023). Dịch
các cặp câu hỏi-câu trả lời thành một câu lệnh ngôn ngữ tự nhiên là một yêu cầu của việc sử dụng ngôn ngữ toán học hình thức
để chứng minh.
5 Thông dịch Mã
Các nỗ lực gần đây (Chen et al., 2023b; Wang et al., 2023a; Gou et al., 2023) đã khám phá
việc nâng cao khả năng tính toán phức tạp của các LLM bằng công cụ, trong đó các thông dịch mã đã
trở nên phổ biến do tính linh hoạt và chức năng được hỗ trợ bởi các thư viện Python khác nhau.
Các khám phá ban đầu sử dụng chương trình như một loại chiến lược tư duy (Chen et al., 2023b; Gao et al.,
2023) nhưng không phù hợp với lý luận và tính toán nhiều bước vì các LLM không thể thấy
6

--- TRANG 7 ---
Báo cáo Kỹ thuật
Hình 3: Ví dụ về lý luận xen kẽ với lập trình (RICO) và chuỗi tư duy
thông thường.
kết quả thực thi mã. Các nghiên cứu gần đây (Wang et al., 2023a; Gou et al., 2023) cố gắng tích hợp
thông dịch mã một cách liền mạch hơn với lý luận nhưng không tương thích và đòi hỏi
sửa đổi bổ sung cho các dịch vụ trò chuyện chung.
Chúng tôi giải quyết vấn đề đã đề cập ở trên bằng cách để các LLM thực hiện lý luận xen kẽ với lập trình
(RICO), trong đó các LLM thực hiện lý luận với định dạng giống như phản hồi trò chuyện và áp dụng một
giao thức gọi công cụ chung để sử dụng thông dịch mã. Thiết kế như vậy không chỉ cho phép
khai thác đầy đủ khả năng lý luận hiện có của các LLM khi sử dụng thông dịch mã
mà còn cho phép tích hợp đầy đủ trực tiếp khả năng gọi công cụ vào mô hình trò chuyện. Do đó,
các khả năng khác nhau (như gọi công cụ và trò chuyện) của một mô hình có thể cung cấp dịch vụ trong một
hệ thống tăng cường công cụ phổ quát, mà chúng tôi tin là tương tự hơn với GPT-4
(OpenAI, 2023).
Cụ thể, như được hiển thị trong Hình 3, khi trả lời một bài toán, chúng tôi nhắc mô hình
thực hiện lý luận tượng trưng và tạo chương trình và sau đó quan sát kết quả thực thi mã
trong mỗi vòng. Mô hình sẽ tiếp tục các vòng như vậy cho đến khi nó trả lời đầy đủ
bài toán sau khi tóm tắt kết quả, không giống như các phương pháp trước đây(Wang et al., 2023a; Gou
et al., 2023) về cơ bản viết mã một lần. Quá trình lý luận sử dụng định dạng giống như
một phản hồi trò chuyện chung thay vì sử dụng các dấu hiệu khác nhau cho cả văn bản và mã (Wang et al.,
2023a). Thiết kế như vậy cho phép mô hình khai thác đầy đủ khả năng lý luận đã học trong
kho dữ liệu SFT thông thường khi sử dụng thông dịch mã. Việc tạo chương trình có thể được
coi như một lời gọi công cụ chung, không giống như ToRA(Gou et al., 2023), nhúng mã
trong phản hồi văn bản bằng cú pháp markdown. Điều này giải quyết sự mơ hồ trong ký hiệu markdown
của khối mã khi triển khai các LLM trong một hệ thống trò chuyện tăng cường công cụ.
Việc xây dựng dữ liệu huấn luyện cho thông dịch mã toán học áp dụng một chiến lược
cập nhật dữ liệu lặp và khai thác ví dụ khó để giảm sự phụ thuộc vào GPT-4. Tại mỗi
7

--- TRANG 8 ---
Báo cáo Kỹ thuật
lặp, trước tiên chúng tôi sử dụng mô hình được huấn luyện trong lặp trước để tạo ra phản hồi trên
bộ huấn luyện của GSM8K và MATH. Vì mô hình không thể phù hợp hoàn toàn với bộ huấn luyện, chúng tôi sử dụng
GPT-4-turbo để tạo ra phản hồi trên bộ huấn luyện còn lại một lần. Các phản hồi đúng
được tạo ra bởi mô hình gần đây nhất và GPT-4-turbo sẽ được sử dụng để huấn luyện một mô hình mới cho
lặp tiếp theo. Dữ liệu ban đầu được tạo ra bởi ToRA-70B (Gou et al., 2023), không
lý tưởng do sự khác biệt về định dạng nhưng có thể được chuyển đổi thành các phản hồi đúng để huấn luyện mô hình
ban đầu. Các mô hình InternLM2-Chat và InternLM2-Math áp dụng cùng dữ liệu huấn luyện cho
khả năng thông dịch mã.
6 Thí nghiệm
6.1 Hiệu suất Tiền huấn luyện
Để xác nhận hiệu suất của các mô hình cơ sở tiền huấn luyện của chúng tôi, chúng tôi sử dụng benchmark tiêu chuẩn
cho lý luận không chính thức toán học: GSM8K (Cobbe et al., 2021) và MATH (Hendrycks et al.,
2021a) và đánh giá chúng bằng cách sử dụng học trong ngữ cảnh. Chúng tôi áp dụng các template few-shot từ
OpenCompass (Contributors, 2023). Chúng tôi sử dụng độ chính xác bỏ phiếu đa số (Wang et al., 2023c) như
metric. Kết quả được liệt kê trong Bảng 2. Các mô hình InternLM2-Math-Base vượt trội hơn
các checkpoint ban đầu InternLM2-Base trên cả hai benchmark cho thấy hiệu quả của
tiến tục tiền huấn luyện. InternLM2-Math-Base-7B đạt 21.5 trên MATH vượt trội hơn
Llemma-7B với 18.0. InternLM2-Math-Base-20B đạt 27.3 trên MATH vượt trội hơn
Llemma-34B và hoạt động tương tự với Minerva-62B với kích cỡ nhỏ hơn.
Bảng 2: So sánh các mô hình tiền huấn luyện sử dụng ICL. Metric là độ chính xác bỏ phiếu đa số.
K=100 cho benchmark GSM8K và K=256 cho benchmark MATH. Chúng tôi sử dụng greedy
decoding khi K=1. Chúng tôi lấy mẫu các mô hình của chúng tôi sử dụng nhiệt độ 0.7 khi K>1.
Benchmark GSM8K MATH
Mô hình MAJ@1 MAJ@K MAJ@1 MAJ@K
Llama2-7B (Touvron et al., 2023) 14.6 - 2.5 -
Llemma-7B (Azerbayev et al., 2023b) 36.4 54.0 18.0 33.5
InternLM2-Base-7B 36.5 - 8.6 -
InternLM2-Math-Base-7B 49.2 75.7 21.5 35.6
Minerva-8B (Lewkowycz et al., 2022b) 16.2 28.4 14.1 25.4
InternLM2-Base-20B 54.6 - 13.7 -
InternLM2-Math-Base-20B 63.7 84.8 27.3 46.2
Llemma-34B 51.5 69.3 25.0 43.1
Minerva-62B 52.4 68.5 27.6 43.4
Minerva-540B 58.8 78.5 33.6 50.3
Một mô hình cơ sở tiền huấn luyện có thể có hiệu suất ICL tốt trong khi hoạt động trung bình
sau SFT do sự chồng chéo dữ liệu giữa tiền huấn luyện và SFT. Chúng tôi thực hiện SFT trên các mô hình khác nhau
với cùng bộ dữ liệu SFT MetaMath (Yu et al., 2023b) để kiểm tra các mô hình của chúng tôi không gặp phải
hiện tượng như vậy. Chúng tôi hiển thị kết quả trong Bảng 3. Sử dụng MetaMath cho SFT, InternLM2-Math-
Base-7B vẫn có ưu thế so với Mistral-7B và Llemma-7B trên benchmark MATH.
InternLM2-Math-Base-20B vượt trội hơn Llemma-34B trên cả hai benchmark.
Để hiển thị khả năng lý luận toán học hình thức của mô hình của chúng tôi, chúng tôi thực hiện ICL4trên benchmark MiniF2F
(Zheng et al., 2021) bao gồm các bài toán toán học ở mức độ khác nhau trong ngôn ngữ
LEAN. LEAN có thể kiểm tra xem bằng chứng hình thức được tạo ra có hoàn thành mục tiêu của câu lệnh không.
Chúng tôi so sánh InternLM2-Math-Base với các mô hình ngôn ngữ tiền huấn luyện khác trong Bảng 4.
InternLM2-Math-7B-Base giải quyết 74 trong số 244 bài toán và đạt 30.3 tạo ra một
hiệu suất tiên tiến mới. InternLM2-Math-7B-Base và InternLM2-Math-20B-Base tìm thấy
25 và 24 bằng chứng mới tương ứng không xuất hiện trong kho lưu trữ MiniF2F chính thức5.
Giống như Gloeckle et al. (2023); Azerbayev et al. (2023b), chúng tôi không thấy hiệu suất lý luận hình thức
4Thiết lập tuân theo https://github.com/wellecks/llemma formal2formal
5https://github.com/openai/miniF2F/blob/main/LEAN/src/test.LEAN
8

--- TRANG 9 ---
Báo cáo Kỹ thuật
Bảng 3: So sánh các mô hình tiền huấn luyện bằng tinh chỉnh trên bộ dữ liệu MetaMath (Yu et al., 2023b).
Metric là độ chính xác greedy. †kết quả đến từ Yu et al. (2023b). * kết quả đến từ
Wang et al. (2023b).
Mô hình GSM8K MATH
MetaMath-Llama2-7B (Touvron et al., 2023) † 66.5 19.8
MetaMath-Mistral-7B (Jiang et al., 2023) † 77.7 28.2
MetaMath-Llemma-7B (Azerbayev et al., 2023b) † 69.2 30.0
MetaMath- InternLM2-Math-Base-7B 76.4 33.8
MetaMath- InternLM2-Math-Base-20B 80.7 36.1
MetaMath-Llemma-34B * 75.8 34.8
mở rộng theo kích cỡ tham số mô hình. Chúng tôi để lại cho nghiên cứu tương lai về việc dữ liệu
và kích cỡ tham số ảnh hưởng đến hiệu suất lý luận hình thức như thế nào.
Bảng 4: Hiệu suất bộ test MiniF2F. Ngân sách tìm kiếm giống như Azerbayev et al.
(2023b) là 1 ×32. Chúng tôi sử dụng 3-shot và LEAN 4 cho các mô hình cơ sở.
Mô hình Loại Tìm kiếm MiniF2F-test
ReProver (Yang et al., 2023a) SFT - 26.5
LLMStep (Welleck & Saha, 2023) SFT 1 ×32 27.9
Code-Llama-7B (Rozi `ere et al., 2023) ICL 1 ×32 20.5
Code-Llama-34B ICL 1 ×32 22.1
Mistral-7B-v0.1 (Jiang et al., 2023) ICL 1 ×32 22.1
Mixtral-8x7B-v0.1 (Jiang et al., 2024) ICL 1 ×32 23.4
Llemma-7B (Azerbayev et al., 2023b) ICL 1 ×32 26.2
Llemma-34B ICL 1 ×32 25.8
Deepseek-coder-7B-v1.5-Base (Guo et al., 2024) ICL 1 ×32 28.7
Deepseek-math-7B-Base (Shao et al., 2024) ICL 1 ×32 28.3
InternLM2-7B-Base ICL 1 ×32 22.1
InternLM2-20B-Base ICL 1 ×32 25.4
InternLM2-Math-7B-Base ICL 1 ×32 30.3
InternLM2-Math-20B-Base ICL 1 ×32 29.5
6.2 Hiệu suất SFT
Để hiển thị hiệu suất của các mô hình SFT của chúng tôi, chúng tôi thực hiện lý luận toán học sử dụng chuỗi tư duy,
mô hình hoá phần thưởng, lý luận hình thức, và thông dịch mã. Chúng tôi cũng sẽ kiểm tra một số
khả năng được giới thiệu trong quá trình SFT của chúng tôi bao gồm game-of-24 và trình kiểm tra số nguyên tố.
6.2.1 Lý luận COT
Chúng tôi đánh giá các mô hình SFT trên GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021a),
kỳ thi toán Hungary, và MathBench-ZH6sử dụng lý luận chuỗi tư duy zero-shot. Chúng tôi
sử dụng kỳ thi toán Hungary để kiểm tra khả năng tổng quát hóa của mô hình và sử dụng MathBench-ZH
để kiểm tra khả năng toán học tiếng Trung. MathBench-ZH chứa 600 bài toán toán học tiếng Trung từ
tiểu học, trung học cơ sở, trung học phổ thông, hoặc cấp đại học. Đối với mỗi bài toán trắc nghiệm trong
MathBench-ZH, chúng tôi sẽ xáo trộn thứ tự lựa chọn 4 lần. Một mô hình đưa ra câu trả lời đúng 4
lần có thể được coi là đúng. Chúng tôi hiển thị kết quả trong Bảng 5. InternLM2-Math-7B đạt
78.1, 34.6, 55, và 40 trên GSM8K, MATH, kỳ thi toán Hungary, và MathBench-ZH
tương ứng cho thấy hiệu suất trong lĩnh vực và ngoài lĩnh vực mạnh hơn nhiều ở cùng
kích cỡ mô hình. InternLM2-Math-7B cũng cho thấy hiệu suất tốt hơn so với việc sử dụng
MetaMath cho SFT chứng minh dữ liệu SFT của chúng tôi có thể kích hoạt khả năng lý luận của mô hình tốt hơn.
InternLM2-Math-20B đạt 37.7 và 66 trên MATH và kỳ thi toán Hungary
chỉ sau GPT-4. Nó đạt hiệu suất tiên tiến với kích cỡ nhỏ hơn nhiều
so với Qwen-72B và DeepSeek-67B.
6https://github.com/open-compass/MathBench
9

--- TRANG 10 ---
Báo cáo Kỹ thuật
Bảng 5: So sánh các mô hình SFT sử dụng lý luận COT zero-shot. Metric là độ chính xác greedy.
Mô hình GSM8K MATH Hungary MathBench-ZH
Qwen-7B-Chat (Alibaba, 2023) 51.7 11.6 19 25.0
DeepSeek-7B-Chat (Bi et al., 2024) 63.0 15.8 28.5 12.7
InternLM2-Chat-7B 70.7 23.0 - 29.2
ChatGLM3-6B (Du et al., 2022) 53.8 20.4 32 15.2
MetaMath-Mistral-7B (Jiang et al., 2023) 77.7 28.2 29 -
MetaMath-Llemma-7B (Azerbayev et al., 2023b) 69.2 30.0 - -
InternLM2-Math-7B 78.1 34.6 55 40.0
InternLM2-Chat-20B 79.6 31.9 - 37.8
MetaMath-Llemma-34B (Azerbayev et al., 2023b) 75.8 34.8 - -
InternLM2-Math-20B 82.6 37.7 66 45.3
Qwen-72B-Chat (Alibaba, 2023) 78.9 35.2 52 47.8
DeepSeek-67B-Chat (Bi et al., 2024) 84.1 32.6 58 33.2
ChatGPT 80.8 34.1 41 21.5
GPT-4 (phiên bản gốc) 92.0 42.5 68 47.2
6.2.2 Mô hình hoá Phần thưởng
Các mô hình phần thưởng có thể được sử dụng để sắp xếp lại câu trả lời để cải thiện hiệu suất mô hình (Cobbe
et al., 2021; Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023b). Chúng tôi sẽ kiểm tra
hiệu suất của mô hình hoá phần thưởng của chúng tôi bằng sắp xếp lại ORM và PRM. Chúng tôi sử dụng cùng
mô hình SFT để suy luận và sắp xếp lại mô hình phần thưởng. Chúng tôi tiếp tục kiểm tra việc sử dụng LEAN như một
mô hình phần thưởng (LRM) bằng cách yêu cầu mô hình chuyển đổi COT sang mã LEAN và thực thi
mã LEAN. Chúng tôi sẽ đánh dấu COT này là đúng với mã LEAN và COT có được
cùng kết quả. Lưu ý rằng việc sử dụng LEAN như một mô hình phần thưởng cho các nhiệm vụ như GSM8K chỉ có thể
kiểm tra các quá trình tính toán nhưng không kiểm tra các quá trình logic. Chúng tôi kiểm tra trên GSM8K và MATH
(500 bài toán test), kết quả được vẽ trong Hình 4 và hiển thị trong Bảng 19. Chúng tôi thấy rằng
nói chung việc sử dụng PRM vượt trội hơn ORM, và ORM vượt trội hơn bỏ phiếu đa số phù hợp
với (Lightman et al., 2023; Wang et al., 2023b). Chúng tôi thấy rằng việc sử dụng LEAN như
RM có lợi thế đáng kể trong việc sắp xếp lại GSM8K với các mô hình 7B, nhưng lợi thế
giảm dần với các mô hình 20B. Có rất nhiều chỗ cải thiện giữa hiệu suất sắp xếp lại RM
và hiệu suất oracle.
Chúng tôi cũng so sánh hiệu suất của chúng tôi với các mô hình sắp xếp lại RM khác trong Bảng 6. So với
MetaMath-DeepSeek-67B reranking (Wang et al., 2023b), mô hình 20B của chúng tôi sử dụng ít thời gian suy luận hơn,
kích cỡ mô hình nhỏ hơn, và đạt hiệu suất tốt hơn với độ chính xác 50.0
cho thấy rằng RM của chúng tôi hiệu quả. Tuy nhiên, so với GPT-4-MathMix (Lightman
et al., 2023), khoảng cách hiệu suất vẫn còn lớn.
Bảng 6: Hiệu suất sắp xếp lại trên MATH(500) so với các baseline khác.
Mô hình Greedy Phương pháp Hiệu suất
InternLM-MATH-7B 34.6 PRM K=100 47.0
InternLM-MATH-20B 37.7 PRM K=100 50.0
MetaMath-Llemma-34B 34.8 PRM K=256 47.3
MetaMath-DeepSeek-67B 36.8 PRM K=256 48.1
GPT-4-MathMix - ORM K=100 71.0
GPT-4-MathMix - PRM K=100 74.5
GPT-4-MathMix - ORM K=1860 72.4
GPT-4-MathMix - PRM K=1860 78.2
10

--- TRANG 11 ---
Báo cáo Kỹ thuật
Hình 4: Hiệu suất sắp xếp lại sử dụng các mô hình phần thưởng của chúng tôi trên GSM8K và MATH. Oracle
hiển thị hiệu suất giới hạn trên được tính bằng Pass@K.
6.2.3 Khả năng Hình thức
Nhắm đến một mô hình ngôn ngữ lớn để thực hiện lý luận toán học có thể xác minh, nhiều khả năng được
yêu cầu bao gồm dịch các bài toán không chính thức hoặc câu lệnh chứng minh thành câu lệnh hình thức
và giải quyết hoặc chứng minh các câu lệnh hình thức sử dụng ngôn ngữ hình thức.
Dịch Hình thức Trước tiên chúng tôi đánh giá khả năng dịch giữa ngôn ngữ tự nhiên
và các câu lệnh hình thức. Chúng tôi tuân theo Azerbayev et al. (2023a) để dịch các câu lệnh được trích xuất
từ sách giáo khoa toán học đại học. Kết quả được liệt kê trong Bảng 7. Trong khi các mô hình của chúng tôi
vượt trội hơn ProofGPT và phiên bản SFT của chúng (Azerbayev et al., 2023a) trong dịch hai chiều,
chúng tôi vẫn thua kém CodeX (Chen et al., 2021) và GPT-4 (OpenAI, 2023). Tương tự
như lý luận hình thức, chúng tôi không thấy lợi ích đáng kể từ việc mở rộng tham số.
Bảng 7: Đánh giá dịch giữa LEAN và câu lệnh ngôn ngữ không chính thức. Kết quả
ngoại trừ các mô hình của chúng tôi được sao chép từ https://github.com/zhangir-azerbayev/ProofNet . 
Metric là điểm BLEU-4.
Mô hình Hình thức hóa Không chính thức hóa
ProofGPT-1.3B (Azerbayev et al., 2023a) 8.1 5.1
ProofGPT-6.7B (Azerbayev et al., 2023a) 4.7 6.0
ProofGPT-1.3B back-translated (Azerbayev et al., 2023a) 10.7 -
CodeX (Chen et al., 2021) 25.1 13.2
GPT-4 (OpenAI, 2023) 27.1 -
InternLM2-Math-7B 15.0 9.4
InternLM2-Math-20B 15.7 10.2
Sử dụng LEAN để giải GSM8K LEAN chủ yếu được sử dụng như một trợ lý chứng minh để chứng minh một
câu lệnh thay vì giải quyết một bài toán. Để thống nhất việc giải quyết và chứng minh bài toán toán học,
chúng tôi khám phá việc sử dụng LEAN để giải quyết bài toán. Chúng tôi yêu cầu mô hình tạo ra bình luận
trước mã LEAN và liệt kê một ví dụ trong Bảng 6.2.3. Chúng tôi sử dụng LEAN 3 để biên dịch và
xuất kết quả làm câu trả lời. Hiệu suất được liệt kê trong Bảng 8. Mô hình 7B của chúng tôi có thể đạt
11

--- TRANG 12 ---
Báo cáo Kỹ thuật
70.4 sử dụng LEAN để giải GSM8K, trong khi mô hình 20B của chúng tôi có thể đạt 73.9. Chúng tôi thấy rằng các mô hình của chúng tôi
hoạt động kém hơn COT. Lý do việc sử dụng LEAN hoạt động kém hơn COT là: (1)
a−b=0 (khi a:N<b:N) (2) phép chia: a/b= [a/b](khi a:N,b:N). Điều này gây
bất tiện trong một số phép tính cơ bản. Tuy nhiên, chúng tôi cho thấy khả năng sử dụng LEAN
như một giao diện thống nhất để giải quyết và chứng minh bài toán toán học.
Giải GSM8K sử dụng LEAN 3
Truy vấn: Một chiếc áo choàng cần 2 cuộn sợi xanh và một nửa lượng sợi trắng. Tổng cộng cần
bao nhiêu cuộn?
Phản hồi:
-- Định nghĩa lượng sợi xanh cần cho một chiếc áo choàng
def blue_fiber_per_robe : N:= 2
-- Định nghĩa lượng sợi trắng cần cho một chiếc áo choàng là một nửa lượng sợi xanh
def white_fiber_per_robe : N:= blue_fiber_per_robe / 2
-- Tính tổng lượng sợi cần cho một chiếc áo choàng
def total_fiber_per_robe : N:= blue_fiber_per_robe + white_fiber_per_robe
-- Xuất kết quả
#eval total_fiber_per_robe -- Điều này sẽ tính ra 3 cuộn tổng cộng
Đầu ra: 3
Bảng 8: Hiệu suất trên GSM8K sử dụng COT và LEAN.
Mô hình COT LEAN
InternLM2-Math-7B 78.1 70.4
InternLM2-Math-20B 82.6 73.9
6.2.4 Thông dịch Mã
Như được hiển thị trong Bảng 9. InternLM2-Math-7B đã có thể vượt qua tất cả các mô hình mã nguồn mở
trước đây trên benchmark MATH ngoại trừ InternLM2-Chat-20B. InternLM2-Math-20B vượt trội hơn
InternLM2-Chat-20B và đạt kết quả tốt nhất trong số các mô hình mã nguồn mở. Vì
InternLM2-Math và InternLM2-Chat áp dụng cùng dữ liệu huấn luyện cho thông dịch mã,
chúng tôi tin rằng sự cải thiện của InternLM2-Math so với các mô hình InternLM2-Chat là kết quả
của việc cải thiện khả năng lý luận toán học.
6.2.5 Các khả năng khác
Game of 24 Để kiểm tra khả năng trên Game of 24, chúng tôi sử dụng bộ test từ Yao et al. (2023).
Khi chúng tôi lấy mẫu một lần cho mỗi câu hỏi, các mô hình của chúng tôi đạt độ chính xác 26 và 35
tương ứng cho kích cỡ mô hình 7B và 20B, vượt trội hơn rất nhiều so với Llamma2-
7B được tinh chỉnh và thậm chí cả GPT-4. Hiệu suất có thể sánh với một số phương pháp lấy mẫu nhiều lần,
điều này chứng minh khả năng của dữ liệu SFT quá trình tìm kiếm từng bước được tạo ra của chúng tôi.
Trình kiểm tra số nguyên tố Đối với việc xác minh số nguyên tố, chúng tôi lấy mẫu một bộ dữ liệu test chứa 20
số cho mỗi chữ số từ 2 đến 10 bao gồm 10 số nguyên tố và 10 số hợp số.
Mô hình của chúng tôi có thể đánh giá đúng hầu hết các số, dù có bao nhiêu chữ số, điều này
tốt hơn GPT-4 hoạt động kém hơn khi số chữ số trở nên lớn hơn.
12

--- TRANG 13 ---
Báo cáo Kỹ thuật
Bảng 9: So sánh các mô hình SFT sử dụng Thông dịch Mã Python. Metric là độ chính xác greedy.
* Điều này được kiểm tra trên phiên bản web GPT-4 vào tháng 8 năm 2023.
Mô hình GSM8K MATH
DeepSeek-Coder-Instruct-7B (Guo et al., 2024) 62.8 28.6
MathCODER-CL-7B (Wang et al., 2023a) 67.8 30.2
DeepSeek-Coder-Instruct-1.5-7B (Guo et al., 2024) 72.6 34.1
ToRA-7B (Gou et al., 2023) 72.6 44.6
InternLM2-Chat-7B 77.9 45.1
InternLM2-Math-7B 79.4 50.9
MathCODER-CL-13B (Wang et al., 2023a) 74.1 35.9
MathCODER-CL-34B (Wang et al., 2023a) 81.7 45.2
ToRA-13B (Gou et al., 2023) 75.8 48.1
ToRA-Code-34B (Wang et al., 2023a) 80.7 50.8
InternLM2-Chat-20B 84.5 51.2
InternLM2-Math-20B 80.7 54.3
ToRA-70B (Gou et al., 2023) 84.3 49.7
GPT-4 Code Interpreter * (Zhou et al., 2023) 97.0 69.7
Bảng 10: Hiệu suất của Game-of-24.
Mô hình Độ chính xác Số lần lấy mẫu
Fine-tuned Llama2-7B (Yu et al., 2023a) 11 1
GPT-4 COT (Yao et al., 2023) 4 1
InternLM2-Math-7B 26 1
InternLM2-Math-20B 35 1
TOT (Yao et al., 2023) 45 12( b=1)
OVM (Yu et al., 2023a) 79 20
7 Thảo luận
7.1 So sánh Hiệu suất Tiền huấn luyện với Lượng Token Tiền huấn luyện
Chúng tôi tiền huấn luyện trên InternLM-Base-7B trong 200B token để đánh giá bao nhiêu epoch nên
chúng tôi tiền huấn luyện. Chúng tôi đánh giá hiệu suất mô hình của chúng tôi mỗi 40B token sử dụng ICL và SFT trên
MetaMath trong Bảng 12. Chúng tôi thấy rằng sau 80B token, hiệu suất không cải thiện
đáng kể. Khi huấn luyện lâu hơn đến 200B (khoảng 7 epoch) token, hiệu suất
bắt đầu thoái hóa.
7.2 Hiệu suất LEAN trên GSM8K
Chúng tôi giới thiệu việc sử dụng LEAN để giải quyết bài toán GSM8K trước đây. LEAN thưa thớt trong
tiền huấn luyện và tinh chỉnh so với ngôn ngữ tự nhiên, LATEX, và Python. Chúng tôi muốn
hiểu khả năng của LEAN được coi như thế nào đối với kích thước dữ liệu. Chúng tôi ablate hiệu suất
của LEAN giải quyết GSM8K sử dụng các lượng dữ liệu SFT khác nhau trong Bảng 13. Chúng tôi thấy nếu chúng tôi
chỉ sử dụng GSM8K-LEAN trong SFT, hiệu suất LEAN kém hơn việc sử dụng GSM8K-
LEAN và MetaMath cùng nhau. Chúng tôi cũng thấy rằng nếu chúng tôi SFT mà không có MetaMath, hiệu suất LEAN
nhạy cảm với lượng dữ liệu. Trong khi nếu chúng tôi huấn luyện SFT với MetaMath, hiệu suất LEAN
ít nhạy cảm hơn cho thấy rằng mô hình chủ yếu cần học ngữ pháp
từ dữ liệu GSM8K-LEAN và học khả năng lý luận từ MetaMath. Những phát hiện này
cho thấy khả năng lý luận của MetaMath giúp hiệu suất trên LEAN với dữ liệu thưa thớt và
gợi ý một chiến lược huấn luyện đa nhiệm vụ của những khả năng này.
13

--- TRANG 14 ---
Báo cáo Kỹ thuật
Bảng 11: Kiểm tra xác minh số nguyên tố bằng cách kiểm tra 10 số nguyên tố và 10 số hợp số
cho các chữ số từ 2 đến 10, tất cả kết thúc bằng 1, 3, 7, hoặc 9 (Kết thúc bằng các số khác là
số hợp số.). Kết quả 'a/b' chỉ ra số lượng nhận dạng đúng, với
'a' đại diện cho số nguyên tố và 'b' đại diện cho số hợp số.
Mô hình 2 3 4 5 6 7 8 9 10
InternLM2-Math-7B 10/10 10/10 10/8 10/8 10/6 10/7 10/10 10/10 10/10
InternLM2-Math-20B 10/10 10/10 10/6 10/9 10/10 10/6 10/10 10/9 10/9
gpt-4-0125-preview 10/10 10/10 3/8 2/9 0/10 0/10 0/10 1/10 0/10
Bảng 12: Hiệu suất tiền huấn luyện trên các lượng token tiền huấn luyện khác nhau.
Benchmark GSM8K MATH
Token ICL SFT ICL SFT
40B 30.9 75.3 19.5 30.2
80B 38.7 77.4 21.4 30.7
120B 39.0 76.4 21.5 33.8
160B 38.4 75.8 21.4 31.9
200B 35.1 76.7 21.1 31.3
7.3 Nghiên cứu Ablation Dữ liệu trong SFT
Để nghiên cứu tác động của thành phần dữ liệu hỗn hợp của chúng tôi, chúng tôi đã thực hiện các nghiên cứu ablation
cho từng loại dữ liệu trong hỗn hợp cuối cùng. Chúng tôi bao gồm bộ dữ liệu MetaMath cho mọi
nghiên cứu ablation để giữ khả năng cơ bản giải quyết bài toán từng bước. Kết quả đánh giá
được trình bày trong Bảng 14. So với việc sử dụng MetaMath cho SFT, mô hình InternLM-
Math-7B của chúng tôi và mô hình với dữ liệu COT đạt kết quả tốt hơn trên MATH và GSM8K.
Các thành phần khác, mặc dù chứa ít token hơn nhiều so với MetaMath, dẫn đến
kết quả kém hơn trên hầu hết các bộ dữ liệu test. Điều này có thể được quy cho việc chuyển định dạng và
phong cách trong mã, định dạng mô hình phần thưởng, và các hướng dẫn khác. Tuy nhiên, dữ liệu SFT
hoạt động tốt hơn MetaMath + COT sau khi tích hợp, cho thấy sự hữu ích lẫn nhau
tiềm ẩn của dữ liệu toàn diện vượt ra ngoài mục đích sử dụng đặc biệt ban đầu của chúng.
7.4 False positive trong MATH BenchMark
Đối với một bài toán yêu cầu một câu trả lời, người ta luôn có thể thử một số trường hợp đặc biệt để đoán
câu trả lời đúng với các quá trình sai. Nếu câu hỏi về đếm, câu trả lời được tạo ra
có thể được lấy mẫu sai, và có được câu trả lời đúng bằng tình cờ. Điều này có thể dẫn
chúng ta đánh giá quá cao khả năng lý luận toán học của các mô hình của chúng tôi và các LLM khác. Chúng tôi ngẫu nhiên
lấy mẫu 5 mẫu cho mỗi mức độ khó trong đó InternLM2-Math-20B đưa ra câu trả lời đúng
trong bộ test MATH và gán nhãn thủ công quá trình dự đoán của 25 bài toán này
để báo cáo vấn đề này. Bảng 15 hiển thị kết quả. Một trong những trường hợp false positive viết:
Nghiên cứu Trường hợp False Positive
Truy vấn: Có bao nhiêu cặp số nguyên dương có thứ tự (m,n)thỏa mãn gcd(m,n) = 2
và lcm [m,n] =108?
Phản hồi: Tôi biết rằng đối với bất kỳ số nguyên dương m và n nào,gcd(m,n)·lcm[m,n] =mn.
Vậy, nếu tôi áp dụng công thức này cho bài toán đã cho, tôi có 2 ·108=mn. Điều này có nghĩa là
m và n là các thừa số của 216. Tôi cũng biết rằng gcd(m,n) = 2, có nghĩa là m
và n đều chẵn, và không có số nào chia hết cho 4. Vậy, tôi cần tìm
số cách chọn hai thừa số chẵn của 216 mà không chia hết cho 4. Tôi
có thể liệt kê các thừa số chẵn của 216 như sau: 2, 4, 6, 8, 9, 12, 18, 24, 27, 36, 54, 72, 108, 216.
Trong số này, những số không chia hết cho 4 là 2, 6, 9, 18, 27, 54, 108. Tôi có thể ghép
14

--- TRANG 15 ---
Báo cáo Kỹ thuật
Bảng 13: Hiệu suất trên GSM8K sử dụng COT và LEAN với các bộ dữ liệu SFT khác nhau.
Thiết lập COT LEAN
InternLM-Math-Base-7B 49.2 -
+ MetaMath SFT 76.4 -
+ SFT của chúng tôi 78.1 70.4
GSM8K-LEAN1⁄1 - 58.0
GSM8K-LEAN1⁄2 - 53.1
GSM8K-LEAN1⁄4 - 35.1
GSM8K-LEAN1⁄8 - 38.4
GSM8K-LEAN1⁄1+ MetaMath 75.7 66.0
GSM8K-LEAN1⁄2+ MetaMath 75.6 65.0
GSM8K-LEAN1⁄4+ MetaMath 77.7 58.8
GSM8K-LEAN1⁄8+ MetaMath 77.4 53.4
Bảng 14: Kết quả ablation của thành phần dữ liệu SFT của chúng tôi.
Benchmark MATH GSM8K
InternLM-Math-7B 34.60 78.09
Chỉ MetaMath 33.78 76.35
MetaMath + COT 34.02 77.63
MetaMath + CI 33.56 77.63
MetaMath + Formal 32.12 76.12
MetaMath + RM 29.84 75.13
MetaMath + Augmenter 32.08 75.89
các thừa số này như sau: (2, 108 ),(6, 54),(9, 27),(18, 18 ). Có bốn cặp,
và mỗi cặp tương ứng với một cặp có thứ tự khác nhau (m,n). Ví dụ, (2, 108 )
tương ứng với (m,n) = ( 108, 2 ). Vậy, có bốn cặp có thứ tự (m,n)thỏa mãn
các điều kiện đã cho. Câu trả lời 4.
Trong trường hợp này, mô hình liệt kê 9 và 27 là số chẵn, dẫn đến cặp (m,n) sai
(9, 27). Hơn nữa, các cặp (6, 54),(18, 18 ), mặc dù thỏa mãn điều kiện được đưa ra bởi dự đoán
mô hình, không đáp ứng yêu cầu của đề bài. Dự đoán chỉ tình cờ liệt kê bốn
cặp làm câu trả lời. Tuy nhiên, nếu đánh giá chỉ xem xét kết quả,
mẫu này sẽ là "đúng". Các trường hợp tương tự xuất hiện thường xuyên hơn khi mức độ khó
tăng. Trong số những trường hợp false positive này, hầu hết các bài toán có một số nguyên làm câu trả lời,
đặc biệt là một số có một chữ số, và giải pháp có thể được thực hiện mà không cần
tính toán chi tiết hoặc chứng minh trường hợp. Những điều này làm cho mô hình dễ dàng suy đoán một câu trả lời đúng
loại trừ lý luận từng bước. Việc đơn giản đánh giá khả năng mô hình chỉ bằng độ chính xác kết quả
có thể là một phía. Do đó, một trong những công việc tương lai của chúng tôi là sử dụng các mô hình phần thưởng hoặc LEAN để
cung cấp kiểm tra quá trình tốt hơn.
Bảng 15: Tính Đúng đắn của Quá trình trong MATH BenchMark sử dụng mô hình 20B của chúng tôi.
Mức độ khó Kết quả đúng Quá trình đúng
Mức 1 5 5
Mức 2 5 4
Mức 3 5 4
Mức 4 5 4
Mức 5 5 3
15

--- TRANG 16 ---
Báo cáo Kỹ thuật
8 Kết luận
Chúng tôi đề xuất InternLM-Math như bước đầu tiên của chúng tôi hướng tới khả năng lý luận toán học có thể xác minh.
InternLM-Math-Base xuất sắc có tiềm năng cho các nhiệm vụ lý luận toán học đa năng. Mặc dù
hiệu suất mạnh trong lý luận không chính thức và chính thức của InternLM-Math,
mô hình của chúng tôi có thể được xem như một điểm khởi đầu để tự cải thiện. InternLM-Math tích hợp
COT và khả năng trợ lý tăng cường có thể được sử dụng để tổng hợp bài toán mới và phản hồi
mới. InternLM-Math có được khả năng ORM, PRM, và LEAN có thể được sử dụng để
xác minh câu trả lời và quá trình của các phản hồi được tạo ra. Chúng tôi tin rằng việc tăng cường dữ liệu có thể xác minh như vậy
sẽ cải thiện khả năng của mô hình với thông lượng cao.
Hạn chế
Lý luận chuỗi tư duy Chúng tôi so khớp và viết lại công thức theo tinh thần scratch-
pad trong COT trong khi nó giới thiệu các vấn đề sau. Vấn đề đầu tiên là chúng tôi
không thể dễ dàng so khớp tất cả các phương trình và tính toán cần được viết lại (ví dụ Chúng tôi giải
x3−3x2+3x−1=0, và có được x=1). Vấn đề thứ hai là nhiều '=' khuyến khích
mô hình lặp lại nhiều hơn. Vấn đề thứ ba là điều này đôi khi tạo ra các tính toán từng bước ngây thơ
có thể làm phiền người dùng cuối và có thể được giảm thiểu thông qua lý luận chuỗi tư duy ngầm
(Deng et al., 2023). Chúng tôi sẽ giải quyết những vấn đề như vậy trong công việc tương lai.
Không có khả năng tự phê bình Mặc dù mô hình của chúng tôi có thể thực hiện ORM hoặc PRM. Chúng tôi không chứa
bất kỳ dữ liệu SFT nào để cho phép mô hình áp dụng tự phê bình vì dữ liệu như vậy có thể khó tạo ra
và xác minh bằng bất kỳ phương tiện nào. Chúng tôi sẽ nghiên cứu cách tạo ra dữ liệu SFT tự phê bình với
xác minh.
Mô hình hoá phần thưởng quá trình Chúng tôi thấy rằng mô hình của chúng tôi không có hiệu suất PRM đáng kể
có thể do định dạng gây nhầm lẫn giữa PRM và SFT và phân phối không cân bằng
giữa các quá trình tích cực và tiêu cực.
Chuyển đổi mã Do phân phối dữ liệu SFT của chúng tôi, dữ liệu tiếng Anh lớn hơn dữ liệu tiếng Trung
sẽ gây ra chuyển đổi mã trong một số hướng dẫn hoặc định dạng bài toán.
Dữ liệu SFT đang sử dụng LEAN 3 Chúng tôi sử dụng LEAN 3 làm dữ liệu SFT của chúng tôi vì GPT-4 chỉ có thể tạo ra
mã LEAN 3 cho GSM8K (ngay cả khi bạn yêu cầu nó áp dụng LEAN 4). Hơn nữa, chúng tôi thấy
dữ liệu dịch giữa chính thức và không chính thức từ MathLib được tiền xử lý trong LEAN
3. Chúng tôi sẽ chuyển dữ liệu SFT của chúng tôi sang LEAN 4 trong phiên bản tương lai.
Nhiễm dữ liệu trên LEAN-repo Chúng tôi sử dụng mã LEAN được giới thiệu từ Algebraic-
Stack (Azerbayev et al., 2023b), và chúng tôi không kiểm tra sự nhiễm của AlgebraicStack
trên MiniF2F. AlgebraicStack có thể chứa hoặc không chứa các giải pháp MiniF2F. Tuy nhiên,
so sánh giữa Llemma và mô hình của chúng tôi là công bằng.
Nhạy cảm với Prompt Các mô hình nhạy cảm với định dạng và prompt hướng dẫn đã cho. Nếu
chúng tôi đưa ra các prompt khác nhau trước câu hỏi (ví dụ Question:, Q:, Please solve this question
step by step), nó có thể có được hiệu suất khác nhau.
Tài liệu tham khảo
Alibaba. Qwen technical report, 2023.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre
Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2
technical report. arXiv preprint arXiv:2305.10403 , 2023.
Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir
Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving
undergraduate-level mathematics, 2023a.
16

--- TRANG 17 ---
Báo cáo Kỹ thuật
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,
Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language
model for mathematics, 2023b.
Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui
Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language
models with longtermism. arXiv preprint arXiv:2401.02954 , 2024.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini
Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya
Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,
Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners. ArXiv , abs/2005.14165, 2020. URL https://api.semanticscholar.
org/CorpusID:218971783 .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,
Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,
Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,
Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-
hammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,
Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,
Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,
Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code.
2021.
Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei
Zhang, and Jia Li. Breaking language barriers in multilingual mathematical reasoning:
Insights and observations, 2023a.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts
prompting: Disentangling computation from reasoning for numerical reasoning tasks.
Transactions on Machine Learning Research , 2023b.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers
to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.
OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation
models. https://github.com/open-compass/opencompass , 2023.
Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning.
2023.
Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul Smolensky, Vishrav Chaudhary, and
Stuart Shieber. Implicit chain of thought reasoning via knowledge distillation. ArXiv ,
abs/2311.01460, 2023. URL https://api.semanticscholar.org/CorpusID:264935229 .
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.
Glm: General language model pretraining with autoregressive blank infilling. In Proceed-
ings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , pp. 320–335, 2022.
Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Hang Yan, Xipeng Qiu, and Dahua Lin.
Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora.
arXiv e-prints , art. arXiv:2401.14624, January 2024. doi: 10.48550/arXiv.2401.14624.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,
and Graham Neubig. Pal: Program-aided language models, 2023.
17

--- TRANG 18 ---
Báo cáo Kỹ thuật
Fabian Gloeckle, Baptiste Roziere, Amaury Hayat, and Gabriel Synnaeve. Temperature-
scaled large language models for lean proofstep prediction. In The 3rd Workshop on
Mathematical Reasoning and AI at NeurIPS'23 , 2023.
Google. Gemini: A family of highly capable multimodal models, 2023.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan,
and Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem
solving, 2023.
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen,
Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder:
When the large language model meets programming – the rise of code intelligence, 2024.
Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers, and Stanislas Polu.
Proof artifact co-training for theorem proving with language models. arXiv preprint
arXiv:2102.06203 , 2021.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,
Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the
MATH dataset. 2021a.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,
Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the
math dataset. arXiv preprint arXiv:2103.03874 , 2021b.
Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei
Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small
language models with scalable training strategies. arXiv preprint arXiv:2404.06395 , 2024.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and
Jiawei Han. Large language models can self-improve, 2022.
InternLM. Internlm: A multilingual language model with progressively enhanced capabili-
ties. https://github.com/InternLM/InternLM , 2023.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,
Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian
Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088 , 2024.
Khan. Khan-exercises. https://github.com/Khan/khan-exercises , 2021.
Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury
Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. Hypertree proof search for
neural theorem proving. Advances in neural information processing systems , 35:26337–26349,
2022.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,
Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai
Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning
problems with language models, 2022a.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,
Vinay V . Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai
Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning
problems with language models. In NeurIPS , 2022b.
Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang
Wang, and Chang Zhou. Query and response augmentation cannot help out-of-domain
math reasoning generalization. arXiv preprint arXiv:2310.05506 , 2023.
18

--- TRANG 19 ---
Báo cáo Kỹ thuật
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee,
Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv
preprint arXiv:2305.20050 , 2023.
Haoxiong Liu and Andrew Chi-Chih Yao. Augmenting math word problems via iterative
question composing, 2024.
Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou,
Wenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. Mathbench: Evaluating the
theory and application proficiency of llms with a hierarchical mathematics benchmark,
2024.
Tiedong Liu and Bryan Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on
arithmetic tasks. arXiv preprint arXiv:2305.14201 , 2023.
Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. arXiv e-prints ,
art. arXiv:1711.05101, November 2017. doi: 10.48550/arXiv.1711.05101.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo
Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering
mathematical reasoning for large language models via reinforced evol-instruct, 2023.
Math-eval. TAL-SCQ5K. https://github.com/math-eval/TAL-SCQ5K , 2023.
The mathlib Community. The lean mathematical library. In Proceedings of the 9th ACM
SIGPLAN International Conference on Certified Programs and Proofs , POPL '20. ACM, January
2020. doi: 10.1145/3372885.3373824. URL http://dx.doi.org/10.1145/3372885.3373824 .
Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming
language. In Andr ´e Platzer and Geoff Sutcliffe (eds.), Automated Deduction – CADE 28 , pp.
625–635, Cham, 2021. Springer International Publishing. ISBN 978-3-030-79876-5.
Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus,
Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained
language models, 2023.
Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton,
and Augustus Odena. Show your work: Scratchpads for intermediate computation with
language models. ArXiv , abs/2112.00114, 2021. URL https://api.semanticscholar.org/
CorpusID:244773644 .
OpenAI. Gpt-4 technical report, 2023.
Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An
open dataset of high-quality mathematical web text. arXiv preprint arXiv:2310.06786 , 2023.
Lawrence C. Paulson. Isabelle: The next 700 theorem provers, 2000.
Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and
Ilya Sutskever. Formal mathematics statement curriculum learning, 2022.
Baptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan,
Yossi Adi, Jingyu Liu, Tal Remez, J ´er´emy Rapin, Artyom Kozhevnikov, Ivan Evtimov,
Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong,
Alexandre D ´efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas
Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models
for code, 2023.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K.
Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning
in open language models, 2024.
19

--- TRANG 20 ---
Báo cáo Kỹ thuật
Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J. Liu,
James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi,
Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Hanie Sedghi, Igor Mordatch,
Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen
Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi,
Noah Constant, Roman Novak, Rosanne Liu, Tris Brian Warkentin, Yundi Qian, Ethan
Dyer, Behnam Neyshabur, Jascha Narain Sohl-Dickstein, and Noah Fiedel. Beyond
human data: Scaling self-training for problem-solving with language models. ArXiv ,
abs/2312.06585, 2023. URL https://api.semanticscholar.org/CorpusID:266163375 .
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis
Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language
model for science. 2023.
The Coq Development Team. The Coq reference manual – release 8.18.0. https://coq.
inria.fr/doc/V8.18.0/refman , 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude
Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman
Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,
Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning
Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,
Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,
Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat
models, 2023.
Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang,
Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with
process- and outcome-based feedback, 2022.
Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang,
Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in
llms for enhanced mathematical reasoning, 2023a.
Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang
Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations.
CoRR, abs/2312.08935 , 2023b.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang,
Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought
reasoning in language models. In The Eleventh International Conference on Learning Repre-
sentations , 2023c. URL https://openreview.net/forum?id=1PL1NIMMrw .
Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In
Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Processing , pp. 845–854, Copenhagen, Denmark,
September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1088.
URL https://aclanthology.org/D17-1088 .
Zengzhi Wang, Rui Xia, and Liu Pengfei. Generative ai for math: Part i – mathpile: A
billion-token-scale pretraining corpus for math. arXiv preprint arXiv:2312.17120 , 2023d.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V
Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language
models. Advances in Neural Information Processing Systems , 35:24824–24837, 2022.
20

--- TRANG 21 ---
Báo cáo Kỹ thuật
Sean Welleck and Rahul Saha. Llmstep: Llm proofstep suggestions in lean. arXiv preprint
arXiv:2310.18457 , 2023.
Kaiyu Yang, Aidan M Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad
Godil, Ryan Prenger, and Anima Anandkumar. Leandojo: Theorem proving with retrieval-
augmented language models. arXiv preprint arXiv:2306.15626 , 2023a.
Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai,
and Jie Tang. Gpt can solve mathematical problems without a calculator. arXiv preprint
arXiv:2309.03241 , 2023b.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and
Karthik Narasimhan. Tree of Thoughts: Deliberate problem solving with large language
models, 2023.
Fei Yu, Anningzhe Gao, and Benyou Wang. Outcome-supervised verifiers for planning in
mathematical reasoning, 2023a.
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T.
Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own
mathematical questions for large language models, 2023b.
Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan,
Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan
Liu, and Maosong Sun. Advancing llm reasoning generalists with preference trees, 2024.
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan,
Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning
with large language models, 2023.
Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu
Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning.
arXiv preprint arXiv:2309.05653 , 2023.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning
with reasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho
(eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.
net/forum?id= 3ELRdg2sgI .
Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu, Jiankai Sun, Huajian Xin,
Jianhao Shen, Zhenguo Li, and Yu Li. Lyra: Orchestrating dual correction in automated
theorem proving, 2023.
Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: a cross-system benchmark
for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110 , 2021.
Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu,
Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word
problems using gpt-4 code interpreter with code-based self-verification, 2023.
Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiax-
ing Zhang, and Yujiu Yang. Solving math word problems via cooperative reasoning
induced language models. In Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pp. 4471–4485, Toronto, Canada, July
2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.
acl-long.245 .
21

--- TRANG 22 ---
Báo cáo Kỹ thuật
A Thành phần Dữ liệu SFT
Chúng tôi liệt kê Thành phần Dữ liệu SFT của chúng tôi trong Bảng 16.
B Cấu hình Thí nghiệm SFT
Mô hình SFT của chúng tôi được khởi tạo từ các mô hình InternLM-Math-Base tiền huấn luyện, tương ứng.
Tốc độ học đỉnh được đặt là lrmax=4e−5, và tốc độ học tối thiểu trong quá trình warming
up là lrmax=6e−6. Những thiết lập này giống nhau cho cả mô hình 7B và 20B. Dữ liệu huấn luyện SFT
được token hóa thành 622.7 M token. Batch size được đặt là 1 mỗi GPU với một
bộ dữ liệu đóng gói có độ dài chuỗi tối đa là 32768. Chúng tôi huấn luyện mô hình trên những dữ liệu này sử dụng
Adam Optimizer trong ba epoch với 32 GPU, và quá trình huấn luyện mất khoảng
7 giờ để hoàn thành cho các mô hình 7B. Đối với mô hình 20B, chúng tôi sẽ sử dụng 64 GPU và quá trình huấn luyện
mất khoảng 10 giờ.
C Hiệu suất chi tiết trên benchmark MATH
Chúng tôi liệt kê hiệu suất chi tiết trên benchmark MATH trong Bảng 17 và Bảng 18. Chúng tôi liệt kê
hiệu suất sắp xếp lại trên GSM8K và MATH trong Bảng 19.
D Nghiên cứu trường hợp
Chúng tôi liệt kê các trường hợp được tạo ra bởi InternLM-Math-7B.
Dịch Hình thức
Truy vấn: Dịch bất đẳng thức Schur sang LEAN 3: x,y,z≥0,x3+y3+z3+3xyz≥
xy(x+y) +yz(y+z) +xz(x+z)
Phản hồi: (a := α)
theorem schur_inequality {a : Type*} [linear_ordered_field a] {x y z : a}
(hnonneg : 0 ≤x) (hnonneg1 : 0 ≤y) (hnonneg2 : 0 ≤z)
x ˆ 3 + y ˆ 3 + z ˆ 3 + 3 * x * y * z ≥x * y * (x + y) + y * z * (y + z) +
x * z * (x + z)
Kiểm tra số nguyên tố
Truy vấn: Kiểm tra xem 5287 có phải là số nguyên tố hay không.
Phản hồi: [√
5287] =72
Chúng ta cần kiểm tra các số nguyên tố từ 3 đến 72.
Các số nguyên tố: 3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71
5287 / 3 = 1762 ... 1
5287 / 7 = 756 ... 5
5287 / 11 = 480 ... 7
5287 / 13 = 407 ... 6
5287 / 17 = 311 ... 0
5287 có thể chia hết cho 17.
5287 là một số hợp số.
22

--- TRANG 23 ---
Báo cáo Kỹ thuật
Chứng minh Hình thức được tìm thấy bởi InternLM-Math sử dụng LEAN 4: amc12a 2013 p4
theorem amc12a_2013_p4 : (2 ˆ 2014 + 2 ˆ 2012) / (2 ˆ 2014 - 2 ˆ 2012) = (5 : R
) / 3 := by
--⊢(2 ˆ 2014 + 2 ˆ 2012) / (2 ˆ 2014 - 2 ˆ 2012) = 5 / 3
field_simp
--⊢(2 ˆ 2014 + 2 ˆ 2012) * 3 / (2 ˆ 2014 - 2 ˆ 2012) = 5
simp only [div_eq_inv_mul]
--⊢(2 ˆ 2014 - 2 ˆ 2012)−1* ((2 ˆ 2014 + 2 ˆ 2012) * 3) = 5
norm_cast
--⊢(↑(Int.subNatNat (2 ˆ 2014) (2 ˆ 2012)))−1*↑((2 ˆ 2014 + 2 ˆ 2012) *
3) = 5
norm_num
-- no goals
Mô hình Phần thưởng Kết quả
Truy vấn: Cho một câu hỏi và một câu trả lời, kiểm tra xem nó có đúng không?
Câu hỏi: Xác định số cách sắp xếp các chữ cái của từ THAT.
Câu trả lời: Từ THAT có 4 chữ cái. Chúng ta có thể sắp xếp 4 chữ cái này theo
4!=4∗3∗2∗1=24 cách.
Do đó, có 24 cách sắp xếp các chữ cái của từ THAT.
Câu trả lời là: 24.
Phản hồi: Câu trả lời này sai.
Mô hình Phần thưởng Quá trình
Truy vấn: Cho một câu hỏi và một câu trả lời, kiểm tra xem nó có đúng không?
Câu hỏi: Camy đã lập một danh sách mọi số nguyên dương năm chữ số riêng biệt có thể
được hình thành sử dụng mỗi chữ số 1, 3, 4, 5 và 9 chính xác một lần trong mỗi số nguyên.
Tổng của các số nguyên trong danh sách của Camy là bao nhiêu?
Câu trả lời: Có 5 !=120 số nguyên dương năm chữ số có thể được hình thành
sử dụng mỗi chữ số 1, 3, 4, 5, và 9 chính xác một lần.
Tổng của những số nguyên này có thể được tìm thấy bằng cách xem xét tổng của mỗi giá trị hàng.
Đối với hàng đơn vị, tổng là 1 ·120=120.
Đối với hàng chục, tổng là 3 ·120=360.
Đối với hàng trăm, tổng là 4 ·120=480.
Đối với hàng nghìn, tổng là 5 ·120=600.
Đối với hàng chục nghìn, tổng là 9 ·120=1080.
Cộng tất cả những tổng này, chúng ta có 120 +360+480+600+1080 =2880 .
Câu trả lời là: 2880.
Phản hồi: Bước 1: Có 5 !=120 số nguyên dương năm chữ số có thể
được hình thành sử dụng mỗi chữ số 1, 3, 4, 5, và 9 chính xác một lần. <PRM + >
Bước 2: Tổng của những số nguyên này có thể được tìm thấy bằng cách xem xét tổng của mỗi giá trị hàng.
<PRM + >
Bước 3: Đối với hàng đơn vị, tổng là 1 ·120=120.<PRM - >
Bước 4: Đối với hàng chục, tổng là 3 ·120=360.<PRM - >
Bước 5: Đối với hàng trăm, tổng là 4 ·120=480.<PRM - >
Bước 6: Đối với hàng nghìn, tổng là 5 ·120=600.<PRM - >
Bước 7: Đối với hàng chục nghìn, tổng là 9 ·120=1080.<PRM - >
Bước 8: Cộng tất cả những tổng này, chúng ta có 120 +360+480+600+1080 =2880 .
<PRM - >
Bước 9: Câu trả lời là: 2880. <PRM - >.
23

--- TRANG 24 ---
Báo cáo Kỹ thuật
Mô hình phần thưởng
Truy vấn: Cho một câu hỏi và hai câu trả lời, câu nào tốt hơn?
Câu hỏi: Điểm trung bình của tôi trong 8 bài kiểm tra toán là 90. Nếu điểm trung bình của tôi trong 5
bài kiểm tra đầu tiên là 87, điểm trung bình của tôi trong 3 bài kiểm tra cuối cùng là bao nhiêu?
Câu trả lời 1: Tôi đã ghi được tổng cộng 8 ∗90=720 trong tất cả 8 bài kiểm tra. Tổng 5 ∗87=435 trong
5 bài kiểm tra đầu tiên để lại tổng 720 −435=285 cho 3 bài kiểm tra cuối cùng, vậy điểm trung bình là
285÷3=95.
Câu trả lời 2: Tôi đã ghi được tổng cộng 720 trong tất cả 8 bài kiểm tra. Tổng 435 trong 5 bài kiểm tra đầu tiên
để lại tổng 285 cho 3 bài kiểm tra cuối cùng, vậy điểm trung bình là 285 ÷3=95
Phản hồi: Câu trả lời 2 tốt hơn.
Chuyển câu hỏi và câu trả lời thành một câu lệnh chứng minh
Truy vấn: Chuyển đổi câu hỏi và câu trả lời này thành định dạng chứng minh.
Câu hỏi: Grant ghi được 10 điểm cao hơn trong bài kiểm tra toán của mình so với John. John nhận được
gấp đôi số điểm so với Hunter người ghi được x điểm trong bài kiểm tra toán của mình. Điểm số bài kiểm tra của Grant
là bao nhiêu? Nếu chúng ta biết câu trả lời cho câu hỏi trên là 100, giá trị của
biến chưa biết x là bao nhiêu?
Câu trả lời: Chúng ta biết rằng John nhận được gấp đôi số điểm so với Hunter, người ghi được
x điểm trong bài kiểm tra toán của mình. Vậy, John ghi được 2x điểm. Grant ghi được 10 điểm cao hơn
so với John, vậy Grant ghi được 2x + 10 điểm. Chúng ta được cho rằng điểm số bài kiểm tra của Grant
là 100, vậy chúng ta có thể viết: 2x + 10 = 100. Giải cho x, chúng ta có: x = 45. Giá trị của x là 45.
Phản hồi: Chứng minh rằng nếu Grant ghi được 10 điểm cao hơn trong bài kiểm tra toán của mình so với
John, và John nhận được gấp đôi số điểm so với Hunter người ghi được x điểm trong bài kiểm tra toán
của mình, thì điểm số bài kiểm tra của Grant là 100 khi và chỉ khi x = 45.
Tăng cường một Bài toán Khó hơn
Truy vấn: Vui lòng tăng cường một bài toán mới khó hơn dựa trên:
Câu hỏi: Tìm phần dư của x2+10x+4 chia cho x−2
Phản hồi: Tìm phần dư của (x2+10x+4)2chia cho (x−2)2.
E Kết quả trên InternLM-Math-Plus
Chúng tôi cải thiện InternLM-Math thành InternLM-Math-Plus với kho dữ liệu tiền huấn luyện mới và
bộ dữ liệu tinh chỉnh. Chúng tôi có bốn kích cỡ của InternLM-Math-Plus bao gồm 1.8B, 7B, 20B,
và 8x22B. Chúng tôi khởi tạo các mô hình của chúng tôi (1.8B, 7B, và 20B) từ InternLM-2, và 8x22B từ
Mixtral-8x22B (Jiang et al., 2024). Chúng tôi đánh giá hiệu suất của InternLM2-Math-Plus trên
benchmark lý luận toán học hình thức MiniF2F-test thông qua LEAN 4 (trong Bảng 20) và các benchmark lý luận toán học
không chính thức MATH, GSM8K, và MathBench-A (Liu et al., 2024) (trong Bảng 22).
Chúng tôi tiếp tục kiểm tra MATH sử dụng Python (trong Bảng 21).
24

--- TRANG 25 ---
Báo cáo Kỹ thuật
Bộ dữ liệu Loại Ngôn ngữ Mô tả Kích cỡ
MetaMath (Yu et al., 2023b) COT, GPT EN Câu hỏi bootstrap trên GSM8K & MATH. 395K
Goat (Liu & Low, 2023) COT, Rule EN Dữ liệu tổng hợp cho các nhiệm vụ số học. 100K
Arithmetic (gốc) COT, Rule EN 40k tính toán nhiều dấu ngoặc, 40k tính toán
thập phân, 35k tính toán phân số, 5k rút gọn
phân số, 15k bài toán dư, và 4k tính toán lũy thừa.140K
Prime (gốc) COT, Rule EN & ZH Đánh giá một số là nguyên tố từng bước. 15K
Game-of-24 (gốc) COT, Rule ZH Giải game-of- k(không giới hạn ở 24) từng
bước bằng cách tìm kiếm.10K
Factoring (gốc) COT, Rule EN & ZH Phân tích một số từng bước. 11K
Triangle (gốc) COT, Rule EN & ZH Tính diện tích tam giác từng bước. 1K
Commonsense (gốc) COT, Rule EN & ZH Bộ dữ liệu thường thức toán học. 14K
Mammoth (Yue et al., 2023) COT, CI EN Hỗn hợp của nhiều bộ dữ liệu COT và POT. 216K
PRM800K (Lightman et al.,
2023)COT, GPT EN Đường dẫn lý luận đúng được chọn. 7K
Khan (Khan, 2021) COT, Mixed EN Một kho dữ liệu toán học liên quan. 83K
TAL-SCQ5K-ZH (Math-
eval, 2023)COT, Human ZH Bộ huấn luyện của một bộ dữ liệu trắc nghiệm
tiếng Trung.3K
TAL-SCQ5K-EN (Math-
eval, 2023)COT, Human EN Bộ huấn luyện của một bộ dữ liệu trắc nghiệm
tiếng Anh.3K
Inhouse-ZH (gốc) COT, Human ZH Bộ dữ liệu toán học tiếng Trung nội bộ. 293K
MathOctopus (Chen et al.,
2023a)COT, GPT ZH Chỉ chọn bài toán tiếng Trung từ đó. 7K
Math23K-AUG (Wang et al.,
2017)COT, OSLLM ZH Tăng cường Math23K được lọc với câu trả lời
cuối cùng.195K
MATH-AUG (Hendrycks
et al., 2021b)COT, OSLLM EN Tăng cường MATH được lọc với PRM của chúng tôi. 28K
COT-few-shot (gốc) COT, Mixed EN & ZH Thêm 1% dữ liệu định dạng few-shot cho hầu hết
bộ dữ liệu.15K
Code-Interpreter (gốc) CI, GPT,
OSLLMEN & ZH Bộ dữ liệu hợp nhất của MATH & GSM8K train,
và bài toán cấp K12 tiếng Trung. Giải pháp
được tạo ra bởi GPT4 hoặc InternLM-70B.
Mỗi cái xen kẽ ngôn ngữ tự nhiên, mã,
và kết quả thực thi.76K
Math-Shepherd (Wang
et al., 2023b)RM, OSLLM EN Hợp nhất Math Shepherd và dữ liệu tự lấy mẫu
giống Math Shepherd. Chuyển đổi chúng
thành định dạng ORM và PRM.445K
ORM-ZH (gốc) RM, OSLLM ZH Dữ liệu ORM tự lấy mẫu trên bài toán
tiếng Trung.104K
GSM8K-LEAN (gốc) Formal, GPT EN Mã LEAN 3 trên bộ huấn luyện GSM8K. 4K
COT-LEAN (gốc) Formal, GPT EN Dịch giữa COT và mã LEAN. 3K
NL-LEAN (Han et al., 2021) Formal, Hu-
manEN Dịch giữa ngôn ngữ tự nhiên và
câu lệnh LEAN.91K
MathLib-Train (Han et al.,
2021)Formal, Hu-
manEN Trích xuất trạng thái và chiến thuật LEAN dựa trên
MathLib.169K
QA-theorem (gốc) Augmenter,
GPTEN Diễn đạt lại một câu hỏi và một câu trả lời thành một
câu lệnh chứng minh ngôn ngữ tự nhiên.1K
Augment-helper (gốc) Augmenter,
GPTEN Tăng cường câu hỏi được tạo ra bởi ChatGPT
(Luo et al., 2023; Li et al., 2023).7K
GSM8K-rephrased (gốc)Augmenter,
GPTEN Định dạng lại câu hỏi được diễn đạt lại từ Meta-
Math (Yu et al., 2023b) để diễn đạt lại.2K
Tất cả - EN & ZH Kết hợp chúng với khử trùng lặp thêm
và khử nhiễm bộ test.2.26M
Bảng 16: Thông tin thành phần dữ liệu SFT chi tiết. Chúng tôi liệt kê các loại dữ liệu và loại nguồn
ở đây. OSLLM biểu thị các LLM mã nguồn mở.
25

--- TRANG 26 ---
Báo cáo Kỹ thuật
Bảng 17: Hiệu suất của MATH được nhóm theo danh mục. * Những kết quả này đến từ
việc tái tạo của chúng tôi.
Mô hình Prob. Precal. Inter. Pre-Alg. Alg. Geo. Num. Tổng thể
MetaMath
Llemma-7B * 24.5 14.7 13.7 45.6 43.0 22.8 18.3 28.7
InternLM-Math-Base-7B 28.5 16.9 14.5 53.5 50.0 25.5 24.1 33.4
InternLM-Math-Base-20B 32.3 19.0 15.7 56.3 54.5 27.6 25.4 36.1
Dữ liệu SFT của chúng tôi
InternLM-Math-7B 28.9 17.0 17.2 53.5 51.4 27.3 25.6 34.6
InternLM-Math-20B 29.1 21.8 19.2 59.1 55.9 30.0 30.2 37.7
Bảng 18: Hiệu suất của MATH được nhóm theo độ khó. * Những kết quả này đến từ
việc tái tạo của chúng tôi.
Mô hình Mức 1 Mức 2 Mức 3 Mức 4 Mức 5 Tổng thể
MetaMath
Llemma-7B * 66.1 44.5 34.5 20.5 8.2 28.7
InternLM-Math-Base-7B 74.1 50.4 37.9 25.9 11.3 33.4
InternLM-Math-Base-20B 75.6 55.3 41.8 28.3 12.2 36.1
Dữ liệu SFT của chúng tôi
InternLM-Math-7B 75.1 53.0 39.3 26.1 12.5 34.6
InternLM-Math-20B 75.5 55.6 45.4 31.7 13.7 37.7
Bảng 19: Sử dụng sắp xếp lại mô hình phần thưởng trên nhiều benchmark toán học. Vì chúng tôi sử dụng
temperate=0.7 khi lấy mẫu, việc lấy mẫu 1 lần hoạt động kém hơn
giải mã greedy là tự nhiên.
Mô hình Metric 1 2 4 8 16 32 64 100
7B/GSM8KMAJ 79.0 79.0 81.8 82.9 83.3 83.2 83 83.5
ORM 79.0 80.0 82.3 83.5 83.6 83.8 83.9 84.2
PRM 79.0 79.8 82.5 83.3 83.9 83.6 83.7 84.0
LRM 79.0 80.4 82.1 83.6 85.2 85.3 85.5 85.8
Oracle 79.0 84.4 88.6 91.3 93.7 95.7 96.7 97.0
20B/GSM8KMAJ 82.1 82.1 85.5 86.4 87.6 88.0 88.5 88.2
ORM 82.1 84.3 86.7 87.4 88.6 88.7 89.2 89.0
PRM 82.1 85.0 87.0 87.5 88.9 88.9 89.5 89.3
LRM 82.1 84.7 87.5 88.3 88.9 89.1 89.4 89.0
Oracle 82.1 88.1 91.3 94.4 96.0 97.1 98.0 98.3
7B/MATHMAJ 34.2 34.8 36.8 40.6 43.6 45.4 44.4 44.8
ORM 34.2 35.8 37.6 40.6 43.6 45.8 45.2 46.0
PRM 34.2 37.0 39.4 42.0 44.0 46.2 46.0 47.0
Oracle 34.2 42.2 49.2 57.6 65.6 71.4 77.6 80.8
20B/MATHMAJ 32.6 34.2 40.6 46.0 47.2 48.4 47.4 47.4
ORM 32.6 37.8 43.6 46.8 48.6 49.6 49.4 48.8
PRM 32.6 37.0 41.8 45.8 48.2 49.2 49.6 50.0
Oracle 32.6 44.8 54.0 61.2 66.8 73.0 79.6 81.8
26

--- TRANG 27 ---
Báo cáo Kỹ thuật
Bảng 20: Hiệu suất của các mô hình khác nhau trên MiniF2F-test.
Mô hình MiniF2F-test
ReProver (Yang et al., 2023a) 26.5
LLMStep (Welleck & Saha, 2023) 27.9
GPT-F Expert Iteration (Polu et al., 2022) 36.6
HTPS (Lample et al., 2022) 41.0
Llemma-7B 26.2
Llemma-34B 25.8
InternLM2-Math-7B-Base 30.3
InternLM2-Math-20B-Base 29.5
InternLM2-Math-Plus-1.8B 38.9
InternLM2-Math-Plus-7B 43.4
InternLM2-Math-Plus-20B 42.6
InternLM2-Math-Plus-Mixtral8x22B 37.3
Bảng 21: Hiệu suất của các mô hình khác nhau trên MATH và GSM8K.
Mô hình MATH MATH-Python GSM8K
MiniCPM-2B (Hu et al., 2024) 10.2 - 53.8
InternLM2-Math-Plus-1.8B 37.0 41.5 58.8
InternLM2-Math-7B 34.6 50.9 78.1
Deepseek-Math-7B-RL (Shao et al., 2024) 51.7 58.8 88.2
InternLM2-Math-Plus-7B 53.0 59.7 85.8
InternLM2-Math-20B 37.7 54.3 82.6
InternLM2-Math-Plus-20B 53.8 61.8 87.7
Mixtral8x22B-Instruct-v0.1 (Jiang et al., 2024) 41.8 - 78.6
Eurux-8x22B-NCA (Yuan et al., 2024) 49.0 - -
InternLM2-Math-Plus-Mixtral8x22B 58.1 68.5 91.8
Bảng 22: Hiệu suất của các mô hình khác nhau trên MathBench-A.
InternLM2-Math-Plus Arithmetic Primary Middle High College Average
-1.8B 43.0 43.3 25.4 18.9 4.7 27.1
-7B 61.4 78.3 52.5 40.5 21.7 50.9
-20B 65.8 79.7 59.5 47.6 24.8 55.5
-Mixtral8x22B 77.5 82.0 63.6 50.3 36.8 62.0
27
