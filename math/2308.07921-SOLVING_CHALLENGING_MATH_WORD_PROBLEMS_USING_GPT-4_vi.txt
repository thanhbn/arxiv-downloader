Lý luận Mã Lý luận & Mã
Xác minh Câu trả lời
Bỏ phiếu đa số có trọng số hướng dẫn xác minh
Câu hỏi: Cho rằng số lặp lại trong biểu diễn thập phân của 1/19 chứa 18 chữ số, tìm chữ số thứ 39 trong biểu diễn thập phân.

wT=1 wU=0.5 wF=0.2
Cho rằng chuỗi lặp lại mỗi 18 chữ số, chữ số thứ 39 sẽ giống như chữ số thứ 3. Vậy, chúng ta cần tìm chữ số thứ 3 trong biểu diễn thập phân của 1/19

Thực thi mã......
Chữ số thứ 3 trong biểu diễn thập phân của 1/19 là 2.
Bây giờ hãy xác minh xem giải pháp này có bằng chữ số thứ 39 không.
Kết quả xác minh là True.
Do đó, giải pháp cuối cùng của chúng tôi là
Đã xác minh: True
2

Nếu chúng ta tính toán một vài chữ số thập phân đầu tiên của 1/19, chúng ta sẽ thấy một mẫu xuất hiện. Hãy tính toán điều này bằng Python.
Thực thi mã......
Chữ số thứ 39 sẽ giống như chữ số thứ 21, thực tế là chữ số thứ 3 trong chuỗi tính toán của chúng ta. Hãy tìm chữ số này.
Cho tính chất lặp lại của biểu diễn thập phân của 1/19, chúng ta có thể tự tin nói rằng chữ số thứ 39 thực sự sẽ là 2

Vì biểu diễn thập phân lặp lại mỗi 18 chữ số, chữ số thứ 39 sẽ giống như chữ số thứ 21.
Kết quả là 0. Bây giờ hãy xác minh câu trả lời.
Thực thi mã......
Thực thi mã......
Mẫu 18 chữ số lặp lại là "052631578947368421". Chữ số thứ 21, giống như chữ số thứ 39, là '5', không phải '0'.
Kết quả xác minh là False.
Kết quả cuối cùng của chúng tôi là
Đã xác minh: False
5

5 ×
Các câu trả lời ứng viên:
2 √
Bỏ phiếu đa số: Bỏ phiếu đa số có trọng số hướng dẫn xác minh:
True Uncertain False
2: 4 ( 3, 1, 0 )
5: 6 ( 1, 1, 4 )
score2 = 1×3 + 0.5×1 + 0.2×0 = 3.5
score5 = 1×1 + 0.5×1 + 0.2×4 = 2.3
Thực thi mã......
Thực thi mã......
Đã xác minh: Uncertain
ví dụ
(a) (b)
(ii) CSV Bỏ phiếu Đa số
(i) SC CoT
wT wU wF
Đầu ra Đầu ra
Đầu vào Đầu vào
....... .......
4 < 6 3.5 > 2.3

Hình 4: (a) Minh họa về Bỏ phiếu đa số ngây thơ (Wang et al., 2023) và Bỏ phiếu đa số có trọng số hướng dẫn xác minh của chúng tôi. Quy trình đầy đủ của khung Bỏ phiếu Đa số Có trọng số Hướng dẫn Xác minh được đề xuất. Chúng tôi sử dụng mô hình để tạo ra nhiều giải pháp khác nhau. Sau đó chúng tôi phát hiện trạng thái tự xác minh của mỗi giải pháp, và phân loại chúng thành ba trạng thái: True, Uncertain, và False. Theo trạng thái xác minh, chúng tôi gán cho mỗi giải pháp một trọng số khác nhau, và sử dụng kết quả phân loại để bỏ phiếu điểm của mỗi câu trả lời có thể.

Bảng 1: Độ chính xác (%) trên bộ dữ liệu MATH. VW-voting là viết tắt của bỏ phiếu đa số có trọng số hướng dẫn xác minh. (Tổng thể: Kết quả trên các chủ đề con MATH khác nhau (Hendrycks et al., 2021))

[THIS IS TABLE: Shows accuracy percentages across different subjects and methods, with columns for Code-based Verification, VW-Voting, and various math subjects like Intermediate Algebra, Precalculus, Geometry, etc.]

Lưu ý rằng kết quả cao đáng kinh ngạc này dựa trên khả năng mạnh mẽ của mô hình cơ sở GPT4-Code, và phương pháp của chúng tôi khuếch đại các phẩm chất tốt của GPT4-Code, với khả năng xác minh các giải pháp.

Lưu ý rằng mặc dù việc thêm Tự kiểm tra dựa trên Mã có thể cải thiện hiệu suất của từng môn học riêng lẻ, mức độ cải thiện khác nhau giữa các môn học, từ 7.6% chỉ đến 0.6%. Đặc biệt, bài toán Hình học chỉ có độ chính xác tăng 0.6%, mặc dù độ chính xác GPT4-Code ban đầu chỉ là 54.0%, thấp so với các môn học khác. Sự khác biệt này có thể được quy cho thực tế rằng việc giải quyết bài toán hình học thường yêu cầu đa phương thức (Chen et al., 2023), một khái niệm vượt ra ngoài phạm vi của bài báo này.

4.2 HIỆU SUẤT TRÊN CÁC BỘ DỮ LIỆU KHÁC
Ngoài bộ dữ liệu MATH thử thách, chúng tôi cũng đã thực hiện phương pháp của mình trên các bộ dữ liệu lý luận khác như GSM8K (Cobbe et al., 2021), MMLU-Math, và MMLU-STEM (Hendrycks et al., 2020). Các kết quả tương ứng có thể được xem trong Bảng 2 và Bảng 3. Khi được tích hợp trên GPT-
8

--- TRANG 9 ---
4-code, phương pháp của chúng tôi vượt trội hơn các phương pháp khác trong cuộc thi, đạt được kết quả tiên tiến trên tất cả các bộ dữ liệu. Các môn học khác trong benchmark MMLU được cung cấp trong Hình 8. Một phân tích so sánh kết quả của chúng tôi với các kỹ thuật tiên tiến trước đây và các mô hình mã nguồn mở cũng được cung cấp.

Bảng 2: Hiệu suất trên bộ dữ liệu GSM8K.
Phương pháp | Đường dẫn lấy mẫu | Độ chính xác(%)
GPT-3.5 (5-shot) | – | 57.1
GPT-4 (5-shot CoT) | – | 92.0
GPT-4 (PHP) | 40 | 96.5
GPT-4 (Model selection) | 15 | 96.8
GPT4-Code | - | 92.9
GPT4-Code + CSV +Voting | 5 | 97.0

Bảng 3: Hiệu suất trên bộ dữ liệu MMLU.
Phương pháp | Bộ dữ liệu | Độ chính xác(%) | Few-shot
Chinchilla (Hoffmann et al., 2022) | Math | 35.7 | 5-shot
Galactica (Taylor et al., 2022) | Math | 41.3 | 5-shot
GPT4-Code | Math | 87.5 | zero-shot
GPT4-Code + CSV +Voting | Math | 89.2 | zero-shot
LLaMA 2 | STEM | 58.0 | 5-shot
OpenLLM | STEM | 70.6 | 5-shot
GPT-4 | STEM | 82.7 | zero-shot
GPT4-Code | STEM | 86.8 | zero-shot
GPT4-Code + CSV +Voting | STEM | 87.0 | zero-shot

[THIS IS FIGURE: Two graphs showing code usage frequency vs accuracy across different levels and subjects]

Hình 5: Bốn điểm trên mỗi đường cong tương ứng với kết quả sử dụng Lời nhắc 1, Lời nhắc 2, Lời nhắc Cơ bản và Lời nhắc Tự kiểm tra dựa trên Mã, tương ứng. (a) Độ chính xác của các cấp độ khác nhau ở các tần suất sử dụng mã khác nhau. (b) Độ chính xác của các môn học khác nhau ở các tần suất sử dụng mã khác nhau.

Bảng 2 minh họa rằng bỏ phiếu đa số hướng dẫn xác minh là một khung hiệu quả để giảm số lượng đường dẫn lấy mẫu, so với GPT-4 với lựa chọn mô hình (Zhao et al., 2023) và PHP (Zheng et al., 2023).

Bảng 3 trình bày so sánh hiệu suất của mô hình chúng tôi với các mô hình hiện có (Hoffmann et al., 2022; Taylor et al., 2022) trên bộ dữ liệu MMLU-Math và với các mô hình mã nguồn mở tiên tiến4 trên MMLU-STEM. Các mô hình mã nguồn mở vẫn bị vượt qua đáng kể bởi các đối tác mã nguồn đóng. Để giải quyết khoảng cách này, chúng tôi đã tạo ra bộ dữ liệu và sẽ công khai nó trong tương lai gần. Ý định của chúng tôi là tạo điều kiện cho việc tinh chỉnh các LLM mã nguồn mở. Ví dụ, mô hình mã nguồn mở LLaMA 2 (Touvron et al., 2023) có thể sử dụng dữ liệu này để tăng cường thêm khả năng lý luận toán học của nó.

4.3 TẦN SUẤT SỬ DỤNG MÃ CỦA CÁC LỜI NHẮC ĐỀ XUẤT
Tương tự như cách tiếp cận được thực hiện trong Mục 3.1, chúng tôi thu thập dữ liệu để làm sáng tỏ mối tương quan giữa độ chính xác và Tần suất Sử dụng Mã trên các chiều khác nhau - lời nhắc (lời nhắc CSV được đề xuất cũng như các lời nhắc được sử dụng trong thí nghiệm thử nghiệm), môn học và mức độ khó. Như được hiển thị trong Hình 5, hành vi của mô hình phù hợp với kỳ vọng của chúng tôi khi thêm các lời nhắc dựa trên mã. Mỗi đường trong Hình 5 có xu hướng đi lên rõ ràng, chứng minh rằng việc tăng Tần suất Sử dụng Mã gây ra cải thiện chung về độ chính xác. Lợi ích hiệu suất khi sử dụng nhiều mã rõ ràng hơn ở các cấp độ khó cao hơn, trong khi ở các cấp độ thấp hơn, lợi ích hiệu suất không quá nổi bật, như được hiển thị trong Hình 5 (a). Ngoài ra, Tần suất Sử dụng Mã tăng đều đặn với

4https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
9

--- TRANG 10 ---
sự gia tăng của mức độ khó. Điều này cho thấy các bài toán toán học khó hơn yêu cầu sử dụng mã thường xuyên hơn, ngụ ý rằng việc gọi mã nhiều lần có thể là một lý do quan trọng tại sao GPT4-Code có lợi thế như vậy trong việc giải quyết các bài toán toán học khó. Có một xu hướng tương tự trong Hình 5 (b).

4.4 NGHIÊN CỨU LOẠI BỎ VÀ THẢO LUẬN
So sánh giữa Tự xác minh Ngôn ngữ Tự nhiên và dựa trên Mã: để nhấn mạnh tầm quan trọng của mã trong giai đoạn tự xác minh, chúng tôi đã sử dụng một lời nhắc tự xác minh ngôn ngữ tự nhiên riêng biệt. Trong phương pháp này, GPT4-Code được hướng dẫn xác minh giải pháp thông qua ngôn ngữ tự nhiên thay vì dựa vào xác minh dựa trên mã, như được trình bày trong Bảng 4. Độ chính xác đạt được với phương pháp này thấp hơn một chút so với Lời nhắc Cơ bản. Hơn nữa, chúng tôi quan sát thấy sự suy giảm độ chính xác cho 4 trong số 7 chủ đề con, cho thấy rằng việc dựa vào đơn thuần xác minh ngôn ngữ tự nhiên không chỉ có thể làm tổn hại độ chính xác mà còn ảnh hưởng tiêu cực đến hiệu suất. Ngược lại, xác minh dựa trên mã nâng cao độ chính xác trên tất cả 7 chủ đề con khi so sánh với Lời nhắc Cơ bản.

Phân tích Bỏ phiếu Đa số Có trọng số Hướng dẫn Xác minh: ban đầu chúng tôi biên soạn ma trận nhầm lẫn (TP/TN/FP/FN), ghi lại các giải pháp với tự xác minh phù hợp với trạng thái True và False đã đề cập trong Eq. 1 từ năm đường dẫn lấy mẫu riêng biệt. Chi tiết của ma trận nhầm lẫn được trình bày trong Phụ lục A.1.1. Từ dữ liệu này, chúng tôi tính toán Precision, Recall, và Accuracy. (Các giải pháp trong trạng thái True được xem là tích cực.) Kết quả được trình bày trong Hình 6. So với Accuracy, chúng tôi quan sát thấy sự nâng cao số lượng 22.3% và 5.6% trong Precision và Recall trung bình, tương ứng. Cụ thể, Precision trung bình đăng ký ở 95.88%. Điều này ngụ ý rằng Accuracy có tiềm năng trở nên cao hơn nhiều, nếu nhiều giải pháp đạt được trạng thái True đã xác minh trước khi đưa ra câu trả lời cuối cùng.

Loại bỏ siêu tham số trong Bỏ phiếu Đa số Có trọng số Hướng dẫn Xác minh: chúng tôi cũng thực hiện các nghiên cứu loại bỏ về siêu tham số wv ∈ {wT, wU, wF} trong Eq. 1. Khi cài đặt siêu tham số thỏa mãn wT > wU ≥ wF, hiệu suất của bỏ phiếu đa số có trọng số hướng dẫn xác minh nhất quán vượt qua các phương pháp bỏ phiếu đa số ngây thơ trên tất cả các đường dẫn lấy mẫu. Ngược lại, khi chúng tôi đặt siêu tham số (wT = 0.5, wU = 0.5, wF = 1), hiệu suất dưới cấu hình này tệ hơn so với bỏ phiếu đa số ngây thơ. Do đó, phương pháp được đề xuất của chúng tôi, bỏ phiếu đa số có trọng số hướng dẫn xác minh, dễ điều chỉnh và mạnh mẽ.

5 KẾT LUẬN VÀ HẠN CHẾ
Trong bài báo này, chúng tôi bắt đầu với các thí nghiệm thử nghiệm trên GPT4-Code để khám phá cách việc sử dụng mã của nó ảnh hưởng đến hiệu suất trong lý luận toán học. Bằng cách phân tích Tần suất Sử dụng Mã và độ chính xác, chúng tôi xác định rằng kỹ năng giải quyết bài toán toán học của GPT4-Code có thể được quy cho khả năng tạo và thực thi mã của nó, cũng như hiệu quả trong việc điều chỉnh và sửa chữa giải pháp khi đối mặt với đầu ra thực thi không hợp lý. Mở rộng trên sự hiểu biết này, chúng tôi giới thiệu ý tưởng về tự xác minh rõ ràng dựa trên mã và bỏ phiếu đa số có trọng số hướng dẫn xác minh, với mục tiêu nâng cao khả năng toán học của GPT4-Code.

Tuy nhiên, có những hạn chế trong công trình của chúng tôi mà chúng tôi dự định khám phá thêm trong tương lai. Thứ nhất, phân tích và cải tiến của chúng tôi hiện tại tập trung vào GPT4-Code, điều này có phần hạn chế. Chúng tôi nhằm áp dụng các phương pháp cho các LLM khác. Thứ hai, kỹ thuật tự xác minh rõ ràng dựa trên mã và bỏ phiếu đa số có trọng số hướng dẫn xác minh của chúng tôi có thể tạo ra các bộ dữ liệu chính xác hơn. Các bộ dữ liệu này sẽ bao gồm việc tạo giải pháp dựa trên mã từng bước chi tiết và
10

--- TRANG 11 ---
xác minh dựa trên mã, có thể giúp cải thiện các LLM mã nguồn mở như LLaMA 2 (Touvron et al., 2023) và nâng cao khả năng toán học của chúng. Mặc dù chúng tôi chưa điều tra phương pháp này, chúng tôi để dành nó cho công việc tương lai.

[THIS IS FIGURE: Two graphs showing (a) precision, recall, and accuracy across different reasoning paths, and (b) accuracy vs number of sampled reasoning paths with different weight settings]

Hình 6: (a) cho thấy độ chính xác, khả năng thu hồi, và độ chính xác trên các đường dẫn lý luận khác nhau. (b) cho thấy độ chính xác đáp ứng với số lượng đường dẫn lý luận được lấy mẫu khi trọng số được đặt thành các giá trị khác nhau.

11

--- TRANG 12 ---
TÀI LIỆU THAM KHẢO
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2022.

Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: A theorem-driven question answering dataset, 2023.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764–10799. PMLR, 2023.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ArXiv, abs/2009.03300, 2020. URL https://api.semanticscholar.org/CorpusID:221516475.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pp. 22199–22213, 2022.

Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843–3857, 2022.

Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315–5333, 2023.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.

Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022.

12

--- TRANG 13 ---
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.

A. Newell and H.A. Simon. Human Problem Solving. ACS symposium series. Prentice-Hall, 1972. ISBN 9780134454030. URL https://books.google.com.hk/books?id=h03uAAAAMAAJ.

OpenAI. Gpt-4 technical report, 2023.

Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw.

Yingxu Wang and Vincent Chiew. On the cognitive process of human problem solving. Cognitive Systems Research, 11(1):81–92, 2010. ISSN 1389-0417.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_VjQlMeSB_J.

Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification, 2023.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv, abs/2305.10601, 2023.

Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with large language models for reasoning. arXiv preprint arXiv:2305.14333, 2023.

Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2023.

13

--- TRANG 14 ---
PHỤ LỤC
Phụ lục này chứa hai phần. Phần đầu tiên cung cấp chi tiết thí nghiệm, bao gồm kết quả thí nghiệm chi tiết trên bộ dữ liệu MATH và MMLU. Phần thứ hai trình bày một số ví dụ về GPT4-Code.

A CHI TIẾT THÍ NGHIỆM
A.1 KẾT QUẢ THÍ NGHIỆM CHI TIẾT TRÊN BỘ DỮ LIỆU MATH
A.1.1 MA TRẬN NHẦM LẪN
Ma trận nhầm lẫn là một bố cục bảng cụ thể cho phép trực quan hóa hiệu suất của một thuật toán. Nó đặc biệt hữu ích cho các vấn đề phân loại, và chúng tôi sử dụng nó để phân tích hiệu suất của quá trình xác minh của chúng tôi.

Bản thân ma trận là một lưới hai chiều, 2x2, cho việc phân loại nhị phân của kết quả xác minh. Mỗi hàng của ma trận đại diện cho các trường hợp trong một lớp được dự đoán, được xác định bởi kết quả xác minh do mô hình ngôn ngữ đưa ra, trong khi mỗi cột đại diện cho các trường hợp trong một lớp thực tế, được xác định bởi tính đúng đắn thực tế của câu trả lời do mô hình đưa ra. Bảng 5 cho thấy ma trận trông như thế nào cho quá trình xác minh của chúng tôi:

Bảng 5: Ma trận Nhầm lẫn của Xác minh
| | Câu trả lời Đúng | Câu trả lời Sai |
|---|---|---|
| Xác minh True | TP | FP |
| Xác minh False | FN | TN |

Đây là ý nghĩa của bốn thuật ngữ:
• True Positive (TP): Các trường hợp trong đó kết quả xác minh của mô hình là 'True', và câu trả lời thực sự đúng.
• True Negative (TN): Các trường hợp trong đó kết quả xác minh của mô hình là 'False', và câu trả lời thực sự sai.
• False Positive (FP): Các trường hợp trong đó kết quả xác minh của mô hình là 'True', nhưng câu trả lời thực sự sai.
• False Negative (FN): Các trường hợp trong đó kết quả xác minh của mô hình là 'False', nhưng câu trả lời thực sự đúng.

Ma trận này rất hữu ích để đo lường nhiều hơn chỉ độ chính xác đơn thuần, dựa trên đó Precision và Recall là hai chỉ số quan trọng. Chúng được định nghĩa trong Eq. 3 và ý nghĩa của chúng như sau:
• Precision là tỷ lệ các trường hợp liên quan trong số các trường hợp được truy xuất. Đây là thước đo độ chính xác của bộ phân loại khi nó dự đoán lớp tích cực.
• Recall là tỷ lệ của tổng số trường hợp liên quan thực sự được truy xuất. Đây là thước đo khả năng của bộ phân loại để tìm tất cả các trường hợp tích cực.

Precision = TP/(TP + FP), Recall = TP/(TP + FN)    (3)

Nói cách khác, precision trả lời câu hỏi "Tỷ lệ câu trả lời được xác minh TRUE thực sự đúng là bao nhiêu?", trong khi recall trả lời "Tỷ lệ câu trả lời đúng thực tế được xác minh TRUE là bao nhiêu?" Cho ý nghĩa của nó, bỏ phiếu hướng dẫn xác minh có giới hạn hiệu quả khi độ chính xác của xác minh cao.

14

--- TRANG 15 ---
A.1.2 PHÂN TÍCH SỬ DỤNG GÓI PYTHON
Bảng 6 phác thảo việc sử dụng các gói Python khác nhau trong thí nghiệm của chúng tôi. Trong số đó, chúng tôi thấy rằng gói sympy được sử dụng thường xuyên nhất, làm nổi bật vai trò trung tâm của nó trong các nhiệm vụ tính toán được thực hiện.

Bảng 6: Tần suất sử dụng gói Python trên bộ dữ liệu MATH.
| Gói | sympy | numpy | math | fractions | itertools | cmath | scipy | matplotlib | functools | collections | statistics |
|---|---|---|---|---|---|---|---|---|---|---|---|
| Tất cả | 0.4168 | 0.0284 | 0.1590 | 0.0094 | 0.0034 | 0.0034 | 0.0016 | 0.0010 | 0.0004 | 0.0004 | 0.0002 |
| Đúng | 0.3907 | 0.0241 | 0.1638 | 0.0110 | 0.0029 | 0.0026 | 0.0009 | 0.0003 | 0.0003 | 0.0006 | 0.0003 |
| đúng trên mỗi mã | 0.3323 | 0.0205 | 0.1393 | 0.0094 | 0.0025 | 0.0022 | 0.0007 | 0.0003 | 0.0003 | 0.0005 | 0.0003 |
| Sai | 0.4724 | 0.0383 | 0.1493 | 0.0058 | 0.0045 | 0.0052 | 0.0032 | 0.0026 | 0.0007 | 0 | 0 |
| sai trên mỗi mã | 0.3194 | 0.0259 | 0.1009 | 0.004 | 0.0031 | 0.0035 | 0.0022 | 0.0018 | 0.0004 | 0 | 0 |
| c/w trên mỗi mã | 104% | 79% | 138% | 238% | 80% | 63% | 34% | 14% | 57% | NaN | NaN |

A.2 KẾT QUẢ THÍ NGHIỆM CHI TIẾT TRÊN BỘ DỮ LIỆU MMLU
Hình 8 minh họa rằng GPT4-Code thực hiện tương đối kém trong một số lĩnh vực nhất định, như kỹ thuật và nhân văn, với một thiếu sót được đánh dấu đặc biệt trong vi sinh học, nơi nó đạt được điểm số dưới 60%. Những quan sát này vạch ra các lĩnh vực cụ thể cần điều tra và cải tiến thêm, do đó phác thảo hướng cho các cải tiến tương lai trong mô hình.

B VÍ DỤ
Trong phần này, chúng tôi cung cấp thêm ví dụ.

15

--- TRANG 16-23 ---
[Phần này chứa nhiều hình ảnh biểu đồ và bảng ví dụ về hiệu suất của GPT4-Code trên các bộ dữ liệu MMLU khác nhau, cũng như các ví dụ chi tiết về cách mô hình giải quyết các bài toán toán học cụ thể. Do độ phức tạp và số lượng lớn của nội dung, tôi sẽ không dịch toàn bộ phần này mà chỉ cung cấp bản dịch của các phần chính.]

[Các bảng và ví dụ cuối cùng cho thấy cách GPT4-Code xử lý các bài toán toán học khác nhau, bao gồm việc tự gỡ lỗi khi gặp lỗi thực thi mã và cách nó điều chỉnh giải pháp dựa trên kết quả thực thi mã.]
