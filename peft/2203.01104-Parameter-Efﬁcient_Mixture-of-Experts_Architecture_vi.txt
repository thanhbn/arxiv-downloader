# Kiến trúc Hỗn hợp Chuyên gia Tiết kiệm Tham số
cho Mô hình Ngôn ngữ Tiền huấn luyện

Ze-Feng Gao1;4;5, Peiyu Liu1;4, Wayne Xin Zhao1;4y, Zhong-Yi Lu2 và Ji-Rong Wen1;3;4
1Trường Trí tuệ Nhân tạo Gaoling, Đại học Nhân dân Trung Quốc
2Khoa Vật lý, Đại học Nhân dân Trung Quốc
3Trường Thông tin, Đại học Nhân dân Trung Quốc
4Phòng thí nghiệm Trọng điểm Bắc Kinh về Phương pháp Quản lý và Phân tích Dữ liệu Lớn
5Viện Hàn lâm Trí tuệ Nhân tạo Bắc Kinh, Bắc Kinh, 100084, Trung Quốc
{zfgao,liupeiyustu,zlu,jrwen}@ruc.edu.cn, batmanﬂy@gmail.com

Tóm tắt
Gần đây, kiến trúc Hỗn hợp Chuyên gia (viết tắt là MoE) đã đạt được thành công đáng kể trong việc tăng dung lượng mô hình của các mô hình ngôn ngữ quy mô lớn. Tuy nhiên, MoE yêu cầu tích hợp nhiều tham số hơn đáng kể so với mô hình cơ sở được mở rộng. Trong bài báo này, chúng tôi đề xuất xây dựng kiến trúc MoE tiết kiệm tham số bằng cách chia sẻ thông tin giữa các chuyên gia. Chúng tôi áp dụng toán tử tích ma trận (MPO, một phân tích tensor từ vật lý nhiều thể lượng tử) để tái cấu trúc ma trận tham số trong lớp chuyên gia và tăng dung lượng mô hình cho các mô hình ngôn ngữ tiền huấn luyện bằng cách chia sẻ các tham số của tensor trung tâm (chứa thông tin cốt lõi) giữa các chuyên gia khác nhau trong khi cho phép tính đặc thù thông qua các tensor phụ trợ (bổ sung cho tensor trung tâm) của các chuyên gia khác nhau. Để giải quyết vấn đề tối ưu hóa không cân bằng, chúng tôi tiếp tục thiết kế chiến lược mặt nạ gradient cho kiến trúc MoE dựa trên MPO. Các thí nghiệm mở rộng dựa trên T5 và GPT-2 cho thấy hiệu suất và hiệu quả được cải thiện của mô hình ngôn ngữ tiền huấn luyện (giảm 27.2 lần tổng số tham số cho hiệu suất mô hình vượt trội, so với Switch Transformers). Mã nguồn của chúng tôi được công bố tại https://github.com/RUCAIBox/MPOE.

1 Giới thiệu
Các mô hình ngôn ngữ tiền huấn luyện quy mô lớn (PLM), như BERT (Devlin et al., 2018) và T5 (Raffel et al., 2020), đã trở thành tiêu chuẩn thực tế trong xử lý ngôn ngữ tự nhiên (NLP). Bằng cách tích hợp một số lượng lớn tham số được tiền huấn luyện trên tập dữ liệu đa mục đích, PLM có thể đạt được hiệu suất xuất sắc trong nhiều tác vụ NLP. Để tăng dung lượng mô hình, một hướng đầy hứa hẹn là khám phá các thuộc tính mở rộng với mô hình hỗn hợp chuyên gia (MoE) (Jacobs et al., 1991; Shazeer et al., 2017) để phát triển các PLM mạnh mẽ hơn. Bằng cách tích hợp nhiều mạng chuyên gia, MoE lập lịch việc học các mẫu dữ liệu thông qua một thành phần định tuyến thường được thực hiện bằng một hàm cổng nào đó, điều này tăng dung lượng mô hình mà không tăng chi phí tính toán theo tỷ lệ. Mặc dù có hiệu quả, nhưng đã được chứng minh rằng kiến trúc MoE không hiệu quả về tham số (Zuo et al., 2021), xét đến cải thiện đạt được so với chi phí liên quan. Hầu hết các nghiên cứu hiện tại (Yang et al., 2021; Roller et al., 2021; Lewis et al., 2021) cho rằng vấn đề này là do tải không cân bằng của các chuyên gia, tập trung vào việc cải thiện các chiến lược định tuyến.

Tuy nhiên, một câu hỏi quan trọng về kiến trúc MoE đã bị bỏ qua trong các nghiên cứu trước đây: liệu tất cả các tham số tăng thêm từ các chuyên gia có thực sự cần thiết để tăng dung lượng mô hình hay không. Vì các chuyên gia khác nhau từ mạng MoE thường được huấn luyện với các mẫu dữ liệu có tương quan (ví dụ, tương quan mẫu từ dữ liệu huấn luyện), có thể dẫn đến sự dư thừa tham số giữa các chuyên gia. Thật vậy, sự dư thừa chuyên gia đã được xác định trong các nghiên cứu hiện tại, nơi Fedus et al. (2021) chưng cất các mô hình MoE thưa thành các mô hình dày đặc và Kim et al. (2021) cắt tỉa các chuyên gia để nén các mô hình MoE. Phát hiện này thúc đẩy chúng tôi phát triển kiến trúc MoE tiết kiệm tham số bằng cách giảm sự dư thừa tham số của nó. Theo trực giác, một cách tiếp cận đơn giản là chia sẻ một tỷ lệ nhất định các tham số giữa các chuyên gia. Tuy nhiên, rất khó xác định và tối ưu hóa các tham số quan trọng mã hóa thông tin được chia sẻ giữa các chuyên gia, vì các mạng chuyên gia thường bao gồm các ma trận dày đặc.

Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp chia sẻ tham số mới được lấy cảm hứng từ phân tích toán tử tích ma trận (MPO) từ vật lý nhiều thể lượng tử (Gao et al., 2020), phân tích một ma trận thành một tích tuần tự của các tensor địa phương (tensor trung tâm hoặc tensor phụ trợ). Không giống như các phương pháp phân tích ma trận khác, MPO có thể tổ chức lại và tổng hợp thông tin quan trọng của ma trận gốc thành tensor trung tâm một cách hiệu quả. Mặt khác, các tensor phụ trợ có vai trò bổ sung cho tensor trung tâm để phục hồi ma trận gốc (Liu et al., 2021). Trong thiết lập của MoE, xét đến các biến thể tham số nhỏ giữa các chuyên gia, chúng tôi suy đoán rằng các tensor trung tâm của các chuyên gia khác nhau (với phân tích MPO cho mỗi chuyên gia) có thể rất giống nhau. Nếu các tensor trung tâm có thể được chia sẻ cho tất cả các mạng chuyên gia, chúng tôi sẽ giảm đáng kể các tham số của kiến trúc MoE.

Để đạt được điều này, chúng tôi đề xuất một kiến trúc MoE tiết kiệm tham số dựa trên MPO mới, được gọi là MPOE. Dựa trên kiến trúc MoE cổ điển (Shazeer et al., 2017), phương pháp của chúng tôi giới thiệu một phần mở rộng chính cho phép các chuyên gia chia sẻ một tensor trung tâm toàn cục trong khi giữ các tensor phụ trợ đặc thù cho chuyên gia. Trong thiết lập của chúng tôi, ma trận tham số trong một chuyên gia duy nhất được hình thành bởi tích của tensor trung tâm được chia sẻ toàn cục và các tensor phụ trợ tương ứng. Vì tensor trung tâm chứa hầu hết các tham số từ phân tích MPO, phương pháp MPOE của chúng tôi có thể giảm đáng kể các tham số của kiến trúc MoE gốc. Một ưu điểm chính khác của MPO là các tensor phụ trợ được liên kết chặt chẽ với tensor trung tâm (Pirvu et al., 2010), và được đảm bảo về mặt lý thuyết rằng bất kỳ thay đổi nào từ các tensor phụ trợ có thể được truyền đến tensor trung tâm. Có nghĩa là, mặc dù một tỷ lệ lớn tham số được chia sẻ, các tensor phụ trợ địa phương vẫn cho phép các chuyên gia nắm bắt các biến thể hoặc khác biệt cụ thể theo các mẫu dữ liệu định tuyến. Tuy nhiên, việc tối ưu hóa trực tiếp kiến trúc MPOE có thể dẫn đến vấn đề tối ưu hóa không cân bằng, nơi các tensor trung tâm được cập nhật thường xuyên hơn các tensor phụ trợ trong quá trình tinh chỉnh. Do đó, chúng tôi tiếp tục đề xuất một chiến lược mặt nạ gradient che giấu gradient tensor trung tâm để giảm thiểu hiệu quả vấn đề tối ưu hóa không cân bằng.

Theo hiểu biết tốt nhất của chúng tôi, đây là nỗ lực đầu tiên giảm sự dư thừa tham số của kiến trúc MoE với phân tích ma trận cấu trúc. Chúng tôi thực hiện các thí nghiệm mở rộng để đánh giá hiệu quả của kiến trúc MPOE trên hai PLM đại diện, T5 và GPT. Các thí nghiệm đã chứng minh hiệu quả của phương pháp chúng tôi trong việc tăng dung lượng mô hình (ít hơn 27.2 lần tham số cho hiệu suất mô hình vượt trội, so với một số PLM tăng cường MoE cạnh tranh.

2 Sơ bộ

2.1 Hỗn hợp Chuyên gia (MoE)
Chúng tôi trước tiên mô tả kiến trúc hỗn hợp chuyên gia (MoE) (Shazeer et al., 2017), đã được sử dụng để tăng cường dung lượng mô hình của các mô hình dựa trên Transformer. Cho G(x) và Ei(x) biểu thị các vector đầu ra của mạng cổng và đầu ra của mạng chuyên gia thứ i cho một đầu vào x nhất định, tương ứng. Đầu ra của kiến trúc MoE y có thể được tính chính thức là:
y = Σ(i=1 đến n) G(x)Ei(x). (1)

Hàm softmax được áp dụng rộng rãi như hàm cổng G(x). Kiến trúc MoE cổng thưa, sử dụng cơ chế cổng top-k nhiễu để giảm chi phí tính toán, đã được đề xuất trong Shazeer et al. (2017). Nó thêm nhiễu Gaussian có thể điều chỉnh với H(), và sau đó chỉ giữ lại các giá trị top-k với KeepTopK() và đặt phần còn lại bằng 0. Điều này chỉ giữ lại các chuyên gia top k để được đánh giá với:
G(x) = softmax(KeepTopK(H(x);k)). (2)

Hơn nữa, Switch Transformer thiết kế một chiến lược định tuyến chuyển đổi để đơn giản hóa hàm cổng này bằng cách định tuyến đến một chuyên gia duy nhất (Fedus et al., 2021).

2.2 Tensor và Toán tử Tích Ma trận
Chúng tôi gọi các mảng một chiều là vector (ký hiệu bằng chữ thường in đậm, ví dụ, v), các mảng hai chiều là ma trận (ký hiệu bằng chữ hoa in đậm, ví dụ, W), và các mảng có chiều cao hơn là tensor (ký hiệu bằng chữ hoa in đậm nghiêng, ví dụ, T).

Phân tích MPO (Oseledets, 2011) (còn gọi là phân tích tensor-train) đã là một kỹ thuật phân tích ma trận được sử dụng rộng rãi từ vật lý nhiều thể lượng tử, phân tích một ma trận (tensor bậc 2) thành m tensor địa phương (Pirvu et al., 2010). Cho một ma trận WIJ ∈ R^(I×J), phân tích MPO được cho trong định dạng sau:
MPO(W) = Π(k=1 đến m) T^(k)[dk-1;ik;jk;dk], (3)

trong đó I = Π(k=1 đến n) ik và J = Π(k=1 đến n) jk, T^(k) là một tensor bậc 4 có kích thước dk-1×ik×jk×dk. dk là chiều của liên kết nối T^(k) và T^(k+1). Theo Gao et al. (2020), ma trận gốc W có thể được tái cấu trúc chính xác bằng co tensor của MPO(W) mà không cắt ngắn liên kết {dk}_(k=1 đến m). Hình 1 trình bày minh họa quy trình phân tích MPO cho một ma trận (m=5). Phân tích chi tiết hơn về các cách phân tích khác nhau (tức là, m=3;5;7;9) sẽ được đưa ra trong Mục 4.5. Sau phân tích MPO, tensor trung tâm (tensor ở giữa) với hầu hết các tham số có thể mã hóa thông tin cốt lõi của ma trận gốc, trong khi các tensor phụ trợ (phần còn lại của những tensor này) chỉ với một tỷ lệ nhỏ tham số đóng vai trò bổ sung cho tensor trung tâm.

3 Phương pháp
Để giảm sự dư thừa thông tin giữa các chuyên gia khác nhau, chúng tôi thiết kế kiến trúc MoE dựa trên MPO để tăng dung lượng mô hình theo cách tiết kiệm tham số. Chúng tôi đầu tiên mô tả kiến trúc MoE dựa trên MPO và sau đó giới thiệu thuật toán tối ưu hóa được cải thiện để học các tham số trong kiến trúc này.

3.1 Hỗn hợp Chuyên gia dựa trên MPO
Kiến trúc MoE trước đây (Jacobs et al., 1991; Shazeer et al., 2017) thường coi các chuyên gia khác nhau như các thành phần riêng lẻ, yêu cầu một bản sao hoàn chỉnh của các tham số mạng cho mỗi chuyên gia. Mặc dù đã được phát hiện (Fedus et al., 2021; Kim et al., 2021) rằng tồn tại thông tin dư thừa giữa các chuyên gia khác nhau trong kiến trúc MoE, nhưng không dễ dàng xác định các tham số có thể chia sẻ từ mạng kết nối cao.

Xét vấn đề này, giải pháp của chúng tôi được lấy cảm hứng từ một ưu điểm quan trọng của phân tích MPO: nó có thể tổ chức lại và tổng hợp thông tin cốt lõi trong các tensor trung tâm (Gao et al., 2020) như đã đề cập. Dựa trên thuộc tính này, ý tưởng cốt lõi của phương pháp chúng tôi là chia sẻ các tensor trung tâm cho tất cả các lớp chuyên gia và cho phép tính đặc thù thông qua các tensor phụ trợ đặc thù cho chuyên gia.

Kiến trúc MoE Tiết kiệm Tham số. Mạng Transformer bao gồm hai thành phần thần kinh chính, đó là FFN và multi-head attention. Theo công trình trước đây về PLM dựa trên MoE (Shazeer et al., 2017; Fedus et al., 2021), chúng tôi coi các lớp FFN là chuyên gia cần được mở rộng, trong khi phương pháp của chúng tôi có thể áp dụng tổng quát cho các thành phần mô hình dựa trên ma trận khác nhau. Một phương pháp đơn giản để giảm sự dư thừa thông tin là chia sẻ một tỷ lệ tham số giữa các chuyên gia. Tuy nhiên, trong các mạng dựa trên Transformer, các chuyên gia (tức là, FFN ở đây) chủ yếu bao gồm các ma trận dày đặc lớn, khó chia sẻ tham số từng phần từ những ma trận này. Là giải pháp của chúng tôi, chúng tôi xem xét chia sẻ tham số thông qua phân tích MPO, để các tensor trung tâm được rút ra có thể được chia sẻ linh hoạt giữa các ma trận.

Thiết kế MoE Nhẹ. Cụ thể, chúng tôi đơn giản hóa cuộc thảo luận bằng cách giả định rằng một chuyên gia tương ứng với một ma trận tham số tại mỗi lớp, và tương tự đối với các trường hợp đa ma trận. Chúng tôi xem xét kiến trúc MoE của n chuyên gia, mỗi chuyên gia có L lớp, để có tổng cộng L×n ma trận, ký hiệu bởi {W^(l,i)}_(l=1,i=1 đến L,n). Như đã thảo luận trong Mục 2.2, một ma trận có thể được phân tích thành m tensor, bao gồm một tensor trung tâm và m-1 tensor phụ trợ. Trong công trình này, chúng tôi xem xét năm tensor được phân tích, tức là, m=5. Tại lớp thứ l, kết quả phân tích có thể được ký hiệu bởi {C^(l,i), A_1^(l,i), A_2^(l,i), A_3^(l,i), A_4^(l,i)}_(i=1 đến n), trong đó C^(l,i) và A_j^(l,i) là các tensor trung tâm và phụ trợ của ma trận tham số thứ i, tương ứng, tại lớp thứ l. Để phát triển kiến trúc MoE dựa trên MPO, ý tưởng cốt lõi là chia sẻ các tensor trung tâm như tham số toàn cục và giữ các tensor phụ trợ đặc thù cho chuyên gia như tham số địa phương, tức là, C^(l,1) = C^(l,2) = ... = C^(l,n) (∀l=1...L), và chúng tôi ký hiệu tensor trung tâm toàn cục tại lớp thứ l bằng C^(l). Theo cách này, chúng tôi chỉ có thể giữ L tensor trung tâm cho kiến trúc MoE L lớp. Đối với MPO, quá trình phân tích trong suốt với các mô-đun bên ngoài, để chúng tôi có thể tái sử dụng cơ chế định tuyến trước đây (Mục 2.1) bằng cách phân phối các mẫu dữ liệu đến các chuyên gia khác nhau. Một khác biệt nhỏ là chúng tôi chỉ cần xem xét định tuyến đến các tensor địa phương cho mỗi ma trận vì tensor toàn cục được chia sẻ giữa các chuyên gia. Chúng tôi gọi kiến trúc MoE dựa trên MPO như vậy là MPOE.

Thảo luận. Vì tensor trung tâm chứa hầu hết thông tin từ các ma trận tham số gốc (Gao et al., 2020), một câu hỏi quan trọng là liệu kiến trúc hiện tại có cho phép đủ tính linh hoạt và đặc thù cho mỗi chuyên gia hay không. Để trả lời câu hỏi này, chúng tôi tham khảo một thuộc tính quan trọng của phân tích MPO từ vật lý nhiều thể lượng tử (Pirvu et al., 2010): được đảm bảo về nguyên tắc, rằng bất kỳ thay đổi nào trên một tensor sẽ được truyền đến toàn bộ tập tensor địa phương. Nói cách khác, chỉ điều chỉnh các tensor phụ trợ (giữ tensor trung tâm cố định) có thể dẫn đến hiệu quả tương tự như điều chỉnh toàn bộ ma trận. Vì các tham số của tensor trung tâm được chia sẻ, phương pháp của chúng tôi có thể giảm đáng kể số lượng tham số thực tế cho kiến trúc MoE với cùng số lượng chuyên gia. Giả sử mô hình gốc bao gồm n chuyên gia với T tham số mỗi chuyên gia, chúng ta có tổng số n×T tham số. Cụ thể, cho β biểu thị tỷ lệ tham số của tensor phụ trợ so với tensor trung tâm cho các mạng chuyên gia. Cho tổng số T cho một mạng chuyên gia, các tensor trung tâm và phụ trợ tương ứng với số lượng tham số T/(β+1) và βT/(β+1), tương ứng. Vì phương pháp MPOE của chúng tôi chia sẻ tensor trung tâm, số lượng tham số cuối cùng sẽ là T/(β+1) + n×βT/(β+1). Do đó, phương pháp MPOE của chúng tôi tương ứng với tỷ lệ (n×β+1)/(n×(β+1)) của quy mô tham số gốc. Trong các thí nghiệm của chúng tôi, tỷ lệ β khoảng 12, và (n×β+1)/(n×(β+1)) xấp xỉ bằng 0.19 khi n=8. Tỷ lệ như vậy sẽ giảm thêm khi chúng ta có nhiều chuyên gia hơn. Có thể thấy rằng phương pháp MPOE của chúng tôi có thể giảm quy mô tham số một cách hiệu quả.

3.2 Giảm thiểu Tối ưu hóa Không cân bằng
Vì các chuyên gia chia sẻ tensor trung tâm trong phương pháp MPOE, các tham số tương ứng của tensor trung tâm sẽ được cập nhật thường xuyên hơn so với những tham số trong các tensor phụ trợ trong quá trình tinh chỉnh. Điều này có xu hướng dẫn đến vấn đề tối ưu hóa không cân bằng như được báo cáo bởi Xu et al. (2021), do sự lệch khỏi trọng số tiền huấn luyện. Kết quả là, rất quan trọng phải phát triển một kỹ thuật tối ưu hóa ổn định hơn phù hợp với kiến trúc MPOE.

Lấy cảm hứng từ giải pháp chiến lược gradient dropout (Tseng et al., 2020; Xu et al., 2021), chúng tôi đề xuất che giấu gradient cho tensor trung tâm để cải thiện tối ưu hóa mô hình cho kiến trúc MoE dựa trên MPO. Tại mỗi lần lặp, chúng tôi lấy một xác suất nhất định pb để loại bỏ bản cập nhật trong tensor trung tâm. Điều này có thể giảm thiểu hiệu quả tối ưu hóa không cân bằng được gây ra bởi các bản cập nhật thường xuyên của tensor trung tâm. Cụ thể, chúng tôi tạo ra một mặt nạ nhị phân b được rút ra từ phân phối Bernoulli với xác suất mặt nạ pb, có thể được tính bởi b ~ Bernoulli(pb). Chúng tôi ký hiệu ΔC là bản cập nhật của tensor trung tâm tại mỗi lần lặp:
ΔC = ∂L(C)/∂C × (1-b). (4)

Pb càng lớn, tensor trung tâm càng ít được cập nhật. Đặc biệt, khi pb bằng 1, có nghĩa là các tham số của tensor trung tâm bị đóng băng cho mỗi đầu vào của dữ liệu. Chi phí tính toán của việc cập nhật tensor trung tâm cũng có thể được giảm với thủ thuật này.

Lưu ý rằng thủ thuật mặt nạ gradient chỉ được áp dụng cho các tensor trung tâm. Đối với các tensor phụ trợ, chúng tôi thực hiện cập nhật gradient tiêu chuẩn để học các tham số. So với hai cách thay thế để thực hiện kỹ thuật mặt nạ gradient, tức là, che giấu tiền kích hoạt hoặc hậu kích hoạt trong các lớp FFN, chúng tôi thấy rằng chiến lược che giấu dựa trên lấy mẫu như vậy có thể cải thiện hiệu quả hiệu suất mô hình trong các thí nghiệm của chúng tôi.

3.3 Quy trình Thuật toán Tổng thể
Phương pháp của chúng tôi có thể được áp dụng tổng quát cho các mô hình dựa trên MoE khác nhau để tăng dung lượng mô hình. Trong công trình này, chúng tôi áp dụng các PLM mở rộng MoE (Radford et al., 2019) để nghiên cứu.

Thuật toán 1 trình bày một quy trình hoàn chỉnh cho quy trình cập nhật được đề xuất, có thể được tóm tắt ngắn gọn như sau. Đầu tiên, chúng tôi có được PLM và thực hiện phân tích MPO cho mỗi ma trận trọng số của các lớp FFN trong Transformer. Đối với mỗi ma trận trọng số, chúng tôi phân tích nó thành một tensor trung tâm C và một danh sách các tensor phụ trợ A. Trong kiến trúc MoE gốc, chúng tôi sẽ có n tập các tham số được phân tích như vậy. Tiếp theo, điểm quan trọng nằm ở chỗ chúng tôi chia sẻ tensor trung tâm C trong quá trình phân tích nhưng giữ các tensor phụ trợ đặc thù cho chuyên gia. Theo cách này, mỗi chuyên gia bao gồm một tập các tensor phụ trợ và một tensor trung tâm được chia sẻ. Để phục hồi ma trận FFN gốc trong một chuyên gia cụ thể, chúng tôi có thể đơn giản nhân tensor trung tâm được chia sẻ với các tensor phụ trợ đặc thù cho chuyên gia. Sau đó, chúng tôi áp dụng chiến lược mặt nạ gradient để cập nhật các tham số trong những chuyên gia này, tức là, che giấu gradient của tensor trung tâm.

Vì các tham số của tensor trung tâm lớn hơn hai bậc độ lớn so với các tham số của các tensor phụ trợ (Liu et al., 2021), chi phí của các mạng dựa trên MoE sẽ được giảm đáng kể bằng cách chia sẻ tensor trung tâm.

3.4 Thảo luận
Đối với vấn đề không hiệu quả tham số của các mạng dựa trên MoE, các nghiên cứu hiện tại chủ yếu tập trung vào việc giảm thiểu tải không cân bằng của các chuyên gia, đã đề xuất các phương pháp định tuyến khác nhau để cân bằng xác suất định tuyến của các chuyên gia khác nhau, như BASE-Layer (Lewis et al., 2021), HASHLayer (Roller et al., 2021), GShard (Lepikhin et al., 2021) và Switch Transformers (Fedus et al., 2021). Để so sánh, chúng tôi nhằm mục đích giảm sự dư thừa thông tin bằng cách chia sẻ các tham số chung giữa các chuyên gia. Thực tế, phương pháp MPOE có thể được tăng cường thêm với các phương pháp định tuyến được cải thiện hiện có.

Cụ thể, Deepspeed-MoE đề xuất sử dụng kiến trúc MoE dư thừa kim tự tháp để giảm các tham số của kiến trúc MoE (Rajbhandari et al., 2022), trong khi công trình của chúng tôi có góc nhìn khác để cải thiện kiến trúc MoE gốc bằng cách chia sẻ tham số giữa các chuyên gia khác nhau.

4 Thí nghiệm
Trong mục này, chúng tôi đầu tiên thiết lập các thí nghiệm và sau đó báo cáo kết quả và phân tích. Sau đó, chúng tôi tiến hành phân tích chi tiết dưới các thiết lập thí nghiệm khác nhau. Ở đây, chúng tôi sử dụng các mô hình T5 (Raffel et al., 2020) và GPT-2 (Radford et al., 2019) làm mô hình cơ sở trong các thí nghiệm của chúng tôi.

4.1 Thiết lập Thí nghiệm
Bộ dữ liệu. Để đánh giá hiệu quả của MPOE được đề xuất như một chiến lược hiệu quả để cải thiện dung lượng mô hình của PLM, chúng tôi theo thiết lập của T5 và GPT-2 để thực hiện các thí nghiệm trên các tác vụ Hiểu biết Ngôn ngữ Tự nhiên (NLU) và Sinh ra Ngôn ngữ Tự nhiên (NLG). Cụ thể, chúng tôi đánh giá các tác vụ NLG trong benchmark GLUE (Wang et al., 2018), tác vụ mô hình hóa ngôn ngữ với WikiText-2 (Merity et al., 2017), tác vụ sinh văn bản với IMDB (Maas et al., 2011) và EMNLP2017 WMT News (Guo et al., 2018). Hơn nữa, chúng tôi theo thiết lập của Raffel et al. (2020) trên benchmark GLUE để so sánh trực tiếp với mô hình T5.

Benchmark GLUE bao gồm nhiều bộ dữ liệu (MRPC, QQP, SST-2, MNLI, RTE, QNLI, CoLA). Các tập kiểm tra gốc không có sẵn công khai, và theo Zhang et al. (2021a), đối với các bộ dữ liệu có ít hơn 10K mẫu (RTE, MRPC, STS-B, CoLA), chúng tôi chia tập xác thực gốc làm đôi, sử dụng một nửa để xác thực và nửa còn lại để kiểm tra.

Chỉ số Đánh giá. Chúng tôi sử dụng độ phức tạp (PPL) (Brown et al., 1992) để đo lường mức độ mô hình xác suất dự đoán tốt một mẫu so với sự thật cơ bản. Để đánh giá tỷ lệ của các n-gram chồng chéo giữa các mẫu được sinh ra và thực, chúng tôi sử dụng điểm BLEU-n (Papineni et al., 2002). Chúng tôi cũng xem xét điểm Self-BLEU-n (Zhu et al., 2018) để đánh giá đặc biệt sự đa dạng của các mẫu được sinh ra. Đối với các chỉ số được sử dụng trong benchmark GLUE, chúng tôi theo Mahabadi et al. (2021) và sử dụng tương quan Matthew cho CoLA, Pearson cho STS-B, và độ chính xác cho các tác vụ khác.

Phương pháp So sánh. Chúng tôi áp dụng T5 và GPT-2 làm kiến trúc cơ sở cho cả MoE và MPOE. Theo Shazeer et al. (2017), chúng tôi mở rộng các thành phần FFN với kiến trúc MoE chứa n chuyên gia trong mỗi khối Transformer của mô hình T5 và GPT-2. Chúng tôi gọi phương pháp này là "+MoE". Switch Transformers (Fedus et al., 2021) sử dụng chiến lược đơn giản hóa định tuyến chỉ đến một chuyên gia duy nhất thay vì định tuyến top-2 trong MoE. Chúng tôi gọi phương pháp này là "+Switch". Để đảm bảo so sánh công bằng, chúng tôi duy trì cùng số lượng (n=8) chuyên gia cho các baseline và MPOE. Chúng tôi cũng triển khai một phiên bản nâng cao của MPOE với n=16 chuyên gia, được gọi là "+MPOE++". Dựa trên mô hình gpt2 được phát hành, mô hình t5-base và mô hình t5-large được cung cấp bởi Huggingface, chúng tôi đầu tiên khởi tạo các chuyên gia, sau đó tinh chỉnh các mô hình trên các tác vụ downstream. Đối với mô hình T5, chúng tôi theo thiết lập trong Mahabadi et al. (2021) và tinh chỉnh tất cả các tham số của mô hình trên tất cả các tác vụ. Đối với các tác vụ downstream khác nhau, chúng tôi chạy quét siêu tham số và chọn cấu hình tốt nhất theo kết quả độ chính xác trên tập xác thực. Các siêu tham số mà chúng tôi điều chỉnh bao gồm các epoch, kích thước batch và tỷ lệ học.

4.2 Kết quả Chính
Trong các thí nghiệm chính của chúng tôi, chúng tôi áp dụng T5 (Raffel et al., 2020), GPT-2 (Radford et al., 2019), Switch Transformers (Fedus et al., 2021) và MoEfication (Zhang et al., 2021b) làm baseline, và báo cáo kết quả so sánh của cả tác vụ NLU và NLG trong Bảng 1.

Nhìn chung, so với các biến thể MoE này, phương pháp MPOE được đề xuất của chúng tôi đạt được cải thiện hiệu suất trong khi tiết kiệm tham số hơn. Đối với tác vụ NLU, phương pháp được đề xuất của chúng tôi ("+MPOE") vượt trội so với phương pháp baseline tốt nhất, tức là, "+Switch" (88.82 so với 88.50 cho T5-Large) với việc giảm lên đến 27.2 lần tổng số tham số trong benchmark GLUE. Bằng cách phóng to các bộ dữ liệu tài nguyên thấp như CoLA và MRPC, phương pháp của chúng tôi mang lại những cải thiện đáng kể hơn. Điều này cho thấy rằng việc chia sẻ tham số giữa các chuyên gia tăng cường hiệu ứng chuyển giao tích cực của thông tin từ các bộ dữ liệu khác hướng tới việc học các bộ dữ liệu tài nguyên thấp. Đối với tác vụ NLG, GPT-2+MPOE đạt được lợi ích trong điểm BLEU-2 (1.72 cho GPT-2+MoE và 2.37 cho GPT-2+Switch) với việc giảm 3.7 lần tổng số tham số trên bộ dữ liệu EMNLP News. Điều này cho thấy rằng GPT-2 cũng hưởng lợi từ việc chia sẻ các tensor trung tâm.

Hơn nữa, T5+MPOE++ và GPT-2+MPOE++ hoạt động tốt hơn khi chúng tôi thêm nhiều tensor phụ trợ như các chuyên gia bổ sung. Điều này chứng minh sự cần thiết của việc cải thiện dung lượng mô hình (Shazeer et al., 2017), vì nhiều tham số của chuyên gia có xu hướng dẫn đến dung lượng mô hình được cải thiện.

4.3 Đánh giá về Học Đa tác vụ
Để chứng minh hiệu quả của MPOE trong học đa tác vụ, chúng tôi áp dụng mô hình T5-Base để phân tích có thể so sánh với Hyperformer (Mahabadi et al., 2021). Chúng tôi tiến hành các thí nghiệm trên benchmark GLUE đa tác vụ. Các chỉ số chi tiết có thể được tìm thấy trong Mục 4.1. Lưu ý rằng so với Hyperformer, phương pháp MPOE không tích hợp các thành phần mạng thần kinh bổ sung, do đó nó linh hoạt hơn để được sử dụng với các PLM.

Bảng 2 hiển thị kết quả trên benchmark GLUE cho T5-base (Raffel et al., 2020), Hyperformer (Mahabadi et al., 2021) và MPOE. Như chúng ta có thể thấy, hiệu suất của phương pháp MPOE nhất quán tốt hơn Hypernetwork trong tất cả các trường hợp, trong khi MPOE tiết kiệm tham số hơn (258M so với 343M tổng số tham số). Điều này tiếp tục chứng minh những lợi ích tiềm năng của phương pháp MPOE trong thiết lập học đa tác vụ, nơi tensor trung tâm học thông tin chung giữa các tác vụ và tensor phụ trợ học thông tin đặc thù cho tác vụ.

4.4 Kết quả Phân tích Loại bỏ
Phương pháp của chúng tôi đã tích hợp hai cải tiến mới: (1) kiến trúc MoE với chia sẻ tham số (PS) giữa các chuyên gia dựa trên phân tích MPO và (2) mặt nạ gradient (GM) để giảm thiểu tối ưu hóa không cân bằng.

Để xác minh hiệu quả của mỗi thành phần, chúng tôi tiến hành nghiên cứu phân tích loại bỏ trên bộ dữ liệu WikiText-2 để phân tích đóng góp của mỗi phần. Chúng tôi áp dụng PPL, BLEU-2 và BLEU-4 làm chỉ số đánh giá, và xem xét loại bỏ chiến lược chia sẻ tham số và mặt nạ gradient tương ứng.

Kết quả phân tích loại bỏ của phương pháp MPOE của chúng tôi được hiển thị trong Bảng 3. Chúng ta có thể thấy rằng loại bỏ bất kỳ thành phần nào sẽ dẫn đến giảm hiệu suất mô hình. Điều này cho thấy hiệu quả của tất cả các thành phần này trong phương pháp của chúng tôi. Bên cạnh đó, chia sẻ tham số dường như quan trọng hơn chiến lược mặt nạ gradient, mang lại sự giảm hiệu suất lớn hơn sau khi bị loại bỏ.

4.5 Phân tích Chi tiết
Phân tích MPO có các cách phân tích khác nhau. Tuy nhiên, phương pháp MPOE yêu cầu một dạng phân tích MPO được định nghĩa trước khi có thể được sử dụng. Do đó, các cách phân tích khác nhau có thể ảnh hưởng đến hiệu quả của phương pháp MPOE. Để xác minh điều này, chúng tôi thực hiện phân tích chi tiết về các cách phân tích khác nhau của phân tích MPO. Chúng tôi trình bày ba biến thể của MPOE với độ dài khác nhau của các tensor địa phương được tạo ra bởi phân tích MPO theo kinh nghiệm. Bảng 4 hiển thị kết quả đánh giá trên bộ dữ liệu WikiText-2 về các tác vụ NLG. Như chúng ta có thể thấy, các biến thể của m > 3 đều vượt trội so với mô hình GPT-2. Ngoài ra, chúng ta có thể quan sát thấy rằng nhiều tensor địa phương hoạt động tương tự nhưng dẫn đến chi phí bộ nhớ cao hơn. Do đó, chúng tôi cuối cùng chọn đặt m=5 cho kiến trúc MPOE xem xét sự đánh đổi giữa chi phí và chất lượng.

5 Công trình Liên quan
Chúng tôi sẽ xem xét các công trình liên quan ở bốn khía cạnh.

PLM với MoE. Đã được báo cáo rằng các mô hình có nhiều tham số hơn thường được coi là có dung lượng mô hình lớn hơn (Fedus et al., 2021; Zuo et al., 2021). Để tăng dung lượng mô hình, một hướng đầy hứa hẹn là khám phá các thuộc tính mở rộng với kiến trúc MoE được giới thiệu bởi Jacobs et al. (1991). Do đó, Shazeer et al. (2017) đầu tiên áp dụng kiến trúc MoE cho các mô hình ngôn ngữ quy mô lớn. Sau đó, Switch Transformers (Fedus et al., 2021), GShard (Lepikhin et al., 2021), BASELayer (Lewis et al., 2021) và HashLayer (Roller et al., 2021) nghiên cứu cách xây dựng mô hình dựa trên Transformer quy mô lớn với MoE cũng như cải thiện chiến lược định tuyến, có thể sử dụng dung lượng mô hình tốt hơn. Ngoài ra, Zhang et al. (2021b) đề xuất một chiến lược cho việc kích hoạt thưa của kiến trúc MoE. He et al. (2021) đề xuất một hệ thống huấn luyện phân tán cho việc huấn luyện nhanh MoE. Zoph et al. (2022) đề xuất một mô hình chuyên gia thưa với huấn luyện ổn định hơn. Yu et al. (2022) đề xuất một mô hình chuyên gia thưa dựa trên kiến trúc all-MLP. Ngược lại, phương pháp của chúng tôi nhằm mục đích giảm sự dư thừa thông tin bằng cách chia sẻ tham số giữa các chuyên gia.

Phân tích Toán tử Tích Ma trận. Phân tích toán tử tích ma trận (MPO) (Pirvu et al., 2010) được đề xuất trong vật lý nhiều thể lượng tử, còn gọi là phân tích tensor-train (TT) (Oseledets, 2011). Một danh mục chính của các nghiên cứu MPO dựa vào nén mô hình (Gao et al., 2020). Họ tập trung vào nén ma trận trọng số và các lớp tích chập (Novikov et al., 2015; Garipov et al., 2016; Sun et al., 2020). Hơn nữa, phân tích MPO được sử dụng để nén các PLM cũng như cho phép tinh chỉnh nhẹ trong các tác vụ downstream (Liu et al., 2021). Trong công trình này, chúng tôi sử dụng cơ chế phân tích như vậy để chia sẻ tham số nhằm xây dựng kiến trúc MoE tiết kiệm tham số.

Các Biến thể Cải tiến của MoE. Mặc dù đạt được hiệu suất, kiến trúc MoE đã bị cản trở bởi độ phức tạp mô hình và chi phí bộ nhớ cao (Shazeer et al., 2017; Fedus et al., 2021). Vấn đề này có thể được giảm thiểu bằng cách sử dụng chưng cất (Fedus et al., 2021) và cắt tỉa chuyên gia (Kim et al., 2021). Sau đó, Kudugunta et al. (2021) và Zuo et al. (2021) chỉ ra rằng các mạng con có thể được sử dụng khi sử dụng mô hình. Thật vậy, phương pháp của chúng tôi có thể được tăng cường thêm bằng các phương pháp hiện có này để cải thiện thời gian suy luận.

Học Đa tác vụ. Việc khai thác các kiến trúc MoE cho học đa tác vụ là một hướng rất hứa hẹn trong những năm gần đây (Ma et al., 2018). Houlsby et al. (2019) đề xuất huấn luyện các adapter cho mỗi tác vụ riêng biệt trong khi giữ mô hình cố định. Nghiên cứu tiếp theo đề xuất rằng các tham số mô hình có thể được chia sẻ giữa các tác vụ, và các tham số adapter đặc thù cho tác vụ được giới thiệu (Stickland và Murray, 2019). Dựa trên ý tưởng này, Mahabadi et al. (2021) và Pilault et al. (2020) đề xuất rằng tinh chỉnh đa tác vụ tiết kiệm tham số cho các mô hình dựa trên transformer thông qua các siêu mạng được chia sẻ. Phương pháp của chúng tôi khác với những công trình này ở chỗ phương pháp MPOE cho phép chúng tôi giảm kích thước mô hình trong khi giữ cùng số lượng chuyên gia, và đồng thời đạt được cải thiện hiệu suất cho học đa tác vụ.

6 Kết luận
Trong bài báo này, chúng tôi đã đề xuất một kiến trúc MoE tiết kiệm tham số để tăng dung lượng mô hình dựa trên phân tích MPO. Đầu tiên, chúng tôi chia sẻ các tensor trung tâm giữa các chuyên gia khác nhau dựa trên phân tích MPO, giảm đáng kể các tham số mô hình của kiến trúc MoE. Sau đó, chúng tôi thiết kế chiến lược mặt nạ gradient để giảm thiểu các vấn đề tối ưu hóa không cân bằng và đảm bảo rằng các tensor khác nhau nắm bắt các loại thông tin khác nhau một cách hiệu quả. Các thí nghiệm mở rộng đã chứng minh rằng phương pháp của chúng tôi vượt trội so với một số chiến lược mở rộng PLM cạnh tranh, đặc biệt là về việc cải thiện hiệu quả tham số của kiến trúc MoE.

Trong tương lai, chúng tôi sẽ tăng cường phương pháp MPOE được đề xuất với các phương pháp định tuyến được đề xuất gần đây, như BASELayer (Lewis et al., 2021), HASHLayer (Roller et al., 2021) và GShard (Lepikhin et al., 2021). Chúng tôi cũng sẽ xem xét khám phá các phương pháp phân tích bổ sung để phát triển kiến trúc MoE tiết kiệm tham số.

Lời cảm ơn
Công trình này được hỗ trợ một phần bởi Quỹ Khoa học Tự nhiên Bắc Kinh dưới Grant số 4222027, Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc dưới Grant số 62206299 và 11934020, Chương trình Nhà khoa học Trẻ Xuất sắc Bắc Kinh dưới Grant số BJJWZYJH012019100020098 và Viện Hàn lâm Trí tuệ Nhân tạo Bắc Kinh (BAAI). Xin Zhao là tác giả liên lạc.

Tài liệu tham khảo
[Các tài liệu tham khảo được duy trì nguyên bản tiếng Anh như trong văn bản gốc]
