# 2311.17601.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2311.17601.pdf
# File size: 185574 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2311.17601v1  [cs.LG]  29 Nov 2023Continual Learning with Low Rank Adaptation
Martin Wistuba
Amazon Web ServicesPrabhu Teja S
Amazon Web Services
Lukas Balles
Amazon Web ServicesGiovanni Zappella
Amazon Web Services
Abstract
Recent work using pretrained transformers has shown impres sive perfor-
mance when ﬁne-tuned with data from the downstream problem o f interest.
However, they struggle to retain that performance when the d ata character-
istics changes. In this paper, we focus on continual learnin g, where a pre-
trained transformer is updated to perform well on new data, w hile retaining
its performance on data it was previously trained on. Earlie r works have tack-
led this primarily through methods inspired from prompt tun ing. We ques-
tion this choice, and investigate the applicability of Low R ank Adaptation
(LoRA) to continual learning. On a range of domain-incremen tal learning
benchmarks, our LoRA-based solution, CoLoR, yields state- of-the-art per-
formance, while still being as parameter efﬁcient as the pro mpt tuning based
methods.
1 Introduction
A primal feature of human cognitive abilities is to incremen tally and continually update knowledge
of a problem; a child can seamlessly learn to recognize newer breeds of dogs without forgetting pre-
viously learned ones. Modern machine learning systems, how ever, fail at this. When naïve methods
for ﬁne-tuning are used to update the weights, they perform w ell on the speciﬁc dataset it has been
ﬁne-tuned on, while losing performance on previous ones, a p henomenon called catastrophic forget-
ting[9,20]. This issue, while not as drastic for modern pre-trained tr ansformers [ 25], is still a major
hindrance to the deployment of reliable systems. Continual learning [ 4,21] deals with this problem
of periodically updating a model with new data, while avoidi ng forgetting previous information.
In practice, data arrives as a sequence of datasets and we aim at performing well on the latest dataset
while retaining performance on the previous ones. Several p aradigms of continual learning are
deﬁned based on the differences between each dataset. In dom ain-incremental learning (DIL), the set
of labels is ﬁxed, whereas the data distribution can change a rbitrarily. In class-incremental learning
(CIL), the set of labels is growing with new datasets which po ses the challenge of recognizing newly
introduced classes. In task-incremental learning (TIL), w e learn to solve different tasks and the
number of tasks grows incrementally. At training and predic tion time, we are aware of the task
identity which is not the case in the other settings.
With transformer-based models becoming commonplace, seve ral continual learning methods have
been proposed that use speciﬁc architectural components of those models. These methods are
heavily inspired by the parameter-efﬁcient ﬁne-tuning met hods in NLP [ 28], primarily, prompt tun-
ing [ 15]. Prompt tuning prepends a set of learnable parameters to th e outputs of the input embed-
ding layer and trains only those, while keeping the rest of th e model frozen. Learning to Prompt
Workshop on Distribution Shifts, 37th Conference on Neural Information Processing Systems (NeurIPS 2023).

--- PAGE 2 ---
(L2P) [ 38] trains a set of input-dependent prompts that are shared acr oss datasets, which encourages
transfer. S-Prompts [ 31] instead learns a single prompt per dataset, and proposes a m ethod to deter-
mine which prompt to use at inference. We discuss several oth er works in Appendix A. However,
the choice of using prompt tuning is not justiﬁed sufﬁcientl y in these methods beyond parameter-
efﬁciency, despite prior work [ 12,30] demonstrating prompt tuning is slower to train and achieve s
lower test time performance than the full ﬁne-tuning counte rpart.
In this work we revisit this choice, in light of evidence from the NLP community that shows low
rank update methods [ 12] perform better than prompt-based ones. We propose an adapt ation of S-
Prompts, the state-of-the-art for domain-incremental lea rning, called CoLoR for efﬁcient continual
training of vision transformers showing a signiﬁcant impro vement in predictive performance. With
an empirical evaluation on three domain-incremental bench marks, we show that CoLoR outperforms
prompt-based methods such as L2P and S-Prompts in terms of av erage accuracy and forgetting.
Furthermore, we show that these gains are achieved with appr oximately the same number of model
parameters. We propose a simple extension to our method call ed CoLoR++ that yields state-of-the-
art results on Split CIFAR-100.
2 Continual Low Rank Adaptation
We, discuss Low Rank Adaptation (LoRA), and then present our method Continual Low Rank Adap-
tation (CoLoR).
2.1 Low Rank Adaptation
We focus on vision transformers in this work, but this approa ch is sufﬁciently general to be used
with other pre-trained transformers. A detailed descripti on of vision transformers is provided in
Appendix B. Traditional ﬁne-tuning updates all the weights of a pre-tr ained transformer with the
data of a downstream task. Low Rank Adaptation [ 12] constraints the update to a low rank one.
An update to a parameter matrix W∈Rd×kof the form W←W+ ∆Wis constrained by
parameterizing ∆W=BA whereA∈Rr×kandB∈Rd×r. This restricts ∆Wto a rank r, and
is also parameter-efﬁcient; when r≪k, the total number of parameters that are updated is r(d+k)
instead of kdas is in the case of full ﬁne-tuning. In addition, LoRA is appl ied only to query and
value embedding matrices ( WQandWV) in all the layers of the network, thereby further reducing
the number of trainable parameters compared to full ﬁne-tun ing. At inference, the added parameters
can be merged with the old parameters, keeping the inference time unaffected.
2.2 CoLoR – Training and Inference
Training CoLoR leverages a pretrained model hand extends it using LoRA to train an expert
model for each dataset D. Let us denote the expert model for dataset DwithfD(x) =gD◦h(x;ΘD)
where the parameters of hare frozen but it is extended by dataset-speciﬁc LoRA module s parameter-
ized byΘD.gDrefers to the dataset-speciﬁc classiﬁcation layer gD(x) =softmax(w⊺
Dh(x;ΘD)+
bD)which uses the [CLS] token of the vision transformer. The tra inable parameters of the network
areΘD,l={At,l
Q,BD,l
Q,AD,l
V,BD,l
V}corresponding to all LoRA components added to each layer
l, and the parameters of the classiﬁer wD,bD. The overall network is trained with a loss appropriate
for the downstream problem.
Inference As the dataset identiﬁer Dis not available at inference time, we use a simple unsuper-
vised method [ 31] to infer it. We estimate kdataset prototype vectors for each dataset Dat training
time as follows. First, we embed each training instance usin gh(without LoRA modules), and run
k-means on those feature embeddings. We store the kcluster centers which serve as representatives
for dataset D. At inference time for an instance x, we estimate the cluster center which is nearest
toh(x). Then, we use fˆDto make the prediction for x, whereˆDis the dataset corresponding to the
nearest cluster center.
2

--- PAGE 3 ---
3 Experiments
Experimental setup Our experiments closely mirror those of [ 31]. For domain incremental learn-
ing experiments, we show results on CORe50 [ 18] and DomainNet [ 23]. CORe50 is a benchmark
for continual object recognition with 50 classes from 11 dat asets with 8 of them acting as the training
set, and the rest as the test set. DomainNet is a benchmark for image classiﬁcation with 345 classes
and 6 datasets. For class incremental experiments, we use Sp lit CIFAR-100 [ 37] which splits the
CIFAR-100 into 10 datasets of 10 contiguous classes each.
To facilitate a fair comparison of baselines, we use a ViT-B- 16 model [ 6] pretrained on ImageNet21k
from the timm library [ 34], and report average accuracy, i.e., the fraction of correctly classiﬁed test
instances up to the current dataset. Our code base is built on top of S-Prompts [ 31].
We provide a summary of our results here, and present detaile d tables in Appendix D(Tables 2
to4). We, primarily, focus on memory-free methods here and rele gate a broader comparison with
replay-based methods to the Appendix.
406080Average Accuracy74.8 75.578.383.185.5CORe50
40506070
47.649.2
40.150.669.7DomainNet
EWC [ 14] LwF [ 17] L2P [ 33] S-Prompts [ 31] CoLoR
Figure 1: Results on two different datasets for domain-incr emental learning. CoLoR improves by
2%-19% over the next best memory-free method.
CoLoR demonstrates new state-of-the-art results in domain -incremental learning. In Fig-
ure1, CoLoR demonstrates superior performance compared to all o ther methods. It outperforms
its closest competitor by 2% on CORe50, and 19% on DomainNet. Furthermore, CoLoR performs
on par or better than replay-based methods (Appendix, Table 3).
6080100Average Accuracy 47.060.783.8
67.372.786.5Split CIFAR-100
EWC [ 14]
LwF [ 17]L2P [ 33]
S-Prompts [ 31]CoLoR
CoLoR++
Figure 2: CoLoR improves by more than 5%
on CIFAR-100 in the class-incremental scenario
over S-Prompts.LoRA is beneﬁcial in class-incremental learn-
ing. Results on Split CIFAR-100 support our ar-
gument that LoRA is a better choice than prompt
tuning, as CoLoR yields better results than S-
Prompts (Figure 2). However, CoLoR lags be-
hind L2P due to the quality of representations
extracted by ViT ( h(·)) for the dataset identiﬁ-
cation method. To address this shortcoming, we
propose the CoLoR++, which uses the representa-
tion extracted by the network after the ﬁrst dataset
update, i.e.,h(x,Θ1). We believe that this fea-
ture extractor effectively represents the data as
it has been trained on a portion of it, leading to
improved results. A comparable enhancement is
also noticed in domain-incremental learning, al-
beit to a lesser extent (Appendix, Table 3).
CoLoR retains the parameter-efﬁciency of S-
Prompts Table 2summarizes the additional pa-
rameters required for CoLoR and its prompt-
tuning competitors on a hypothetical two class problem. Sin ce this efﬁciency holds only true for low
ranksr, we report the additional accuracy results in Figure 3and Tables 3and4in the Appendix.
3

--- PAGE 4 ---
Table 1: Number of trainable parameters for each method. We r eport the parameters trained for
DyTox, L2P, S-Prompts, and CoLoR ( r= 1) for a hypothetical two class problem. †numbers are
reproduced from [ 31].
DyTox† L2P† S-Prompts†CoLoR
Additional Parameters per Dataset (on average)1.42M 18.43K 52.22K 38.40K
1.65%↑0.02%↑ 0.06%↑ 0.04%↑
100101102
LoRA Rank r83858789Avg. AccuracyCORe50
100101102
LoRA Rank r4954596469DomainNet
100101102
LoRA Rank r677073Split CIFAR-100
Figure 3: Increasing the rank by keeping all other settings ﬁ xed. Increasing the rank beyond 2-digit
numbers yields only minor improvements in most cases. CoLoR outperforms its best competitor
even with the smallest rank.
It is apparent, that for the same number of parameters, CoLoR still provides better results than its
competitors. Furthermore, increasing the rank allows to tr ade parameter-efﬁciency for prediction
performance.
CoLoR closes the gap between DIL and TIL. In previous experiments, we assume no access
to the dataset identiﬁer at inference, and use our dataset id entiﬁcation method to determine which
LoRA module to use. In Table 2, we show the results for using an oracle dataset identiﬁcati on
method. A substantial increase in accuracy is expected as th e dataset identiﬁcation is non-trivial; in
particular, in CIL a wrongful dataset prediction leads to a m is-classiﬁcation. However, for DIL this
happens to a lesser degree and CoLoR closes the gap between TI L and DIL. Finally, TIL perfor-
mance can be construed to be the upper bound of using LoRA-bas ed modules for continual learning.
Importantly, this upper bound is signiﬁcantly higher than t he one oftentimes attained by training a
single model using all data (see Appendix, Table 4).
Table 2: Inferred dataset id vs known dataset id with CoLoR. W e report the performance in the case
where the dataset id is inferred as explained above and in the case there the correct dataset id is
provided by an oracle. While the oracle-based setting is not realistic, this comparison is still useful
to investigate the performance of the algorithm. This exper iment is not applicable for CORe50.
DomainNet Split CIFAR-100
CoLoR (inferred dataset id) 69.67 71.42
CoLoR (correct dataset id) 73.68 98.67
4 Conclusions
In this work, we scrutinized the omnipresence of prompt tuni ng in recent continual learning meth-
ods in favor of other parameter-efﬁcient ﬁne-tuning (PEFT) methods. We did this by introducing
CoLoR, a LoRA-based continual learning method. We empirica lly demonstrated that it outperforms
its prompt tuning counterpart in domain- and class-increme ntal learning by a large margin and re-
mains as parameter-efﬁcient. Furthermore, we improved the unsupervised dataset identiﬁcation
strategy by using the representation of the ﬁne-tuned model . This change resulted in new state-of-
the-art results on Split CIFAR-100.
4

--- PAGE 5 ---
References
[1] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davi de Abati, and Simone Calderara. Dark
experience for general continual learning: a strong, simpl e baseline. In NeurIPS , 2020. 8,9,
10
[2] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Contrasti ve continual learning. In ICCV ,
2021. 9,10
[3] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, T halaiyasingam Ajanthan,
Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories
in continual learning. arXiv preprint arXiv:1902.10486 , 2019. 8,9,10
[4] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Par isot, Xu Jia, Aleš Leonardis, Gre-
gory Slabaugh, and Tinne Tuytelaars. A continual learning s urvey: Defying forgetting in clas-
siﬁcation tasks. IEEE transactions on pattern analysis and machine intellig ence, 44(7):3366–
3385, 2021. 1
[5] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Z iyan Wu, and Rama Chellappa.
Learning without memorizing. In CVPR , pages 5138–5146. Computer Vision Foundation /
IEEE, 2019. 8
[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov , Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 wo rds: Transformers for image
recognition at scale. In ICLR , 2021. 3,8
[7] Arthur Douillard, Alexandre Ramé, Guillaume Couairon, and Matthieu Cord. Dytox: Trans-
formers for continual learning with dynamic token expansio n. In CVPR , 2022. 8,9
[8] Beyza Ermis, Giovanni Zappella, Martin Wistuba, Aditya Rawal, and Cédric Archambeau.
Memory efﬁcient continual learning with transformers. In NeurIPS , 2022. 8
[9] Robert M French. Catastrophic forgetting in connection ist networks. Trends in cognitive
sciences , 3(4):128–135, 1999. 1
[10] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and D ahua Lin. Learning a uniﬁed
classiﬁer incrementally via rebalancing. In CVPR , 2019. 8
[11] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,
Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parame ter-efﬁcient transfer learning
for NLP. In ICML , volume 97 of Proceedings of Machine Learning Research , pages 2790–
2799. PMLR, 2019. 8
[12] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In In-
ternational Conference on Learning Representations , 2022. 2
[13] Ronald Kemker and Christopher Kanan. Fearnet: Brain-i nspired model for incremental learn-
ing. In ICLR , 2018. 8
[14] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, J oel Veness, Guillaume Desjardins,
Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnie szka Grabska-Barwinska,
et al. Overcoming catastrophic forgetting in neural networ ks. In Proceedings of the national
academy of sciences , 2017. 3,8,9,10
[15] Brian Lester, Rami Al-Rfou, and Noah Constant. The powe r of scale for parameter-efﬁcient
prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in N atural Lan-
guage Processing , pages 3045–3059, Online and Punta Cana, Dominican Republi c, November
2021. Association for Computational Linguistics. 1,8
5

--- PAGE 6 ---
[16] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizin g continuous prompts for generation.
InProceedings of the 59th Annual Meeting of the Association fo r Computational Linguistics
and the 11th International Joint Conference on Natural Lang uage Processing (Volume 1: Long
Papers) , pages 4582–4597, Online, August 2021. Association for Com putational Linguistics.
8
[17] Zhizhong Li and Derek Hoiem. Learning without forgetti ng.IEEE transactions on pattern
analysis and machine intelligence , 40(12):2935–2947, 2017. 3,8,9,10
[18] Vincenzo Lomonaco and Davide Maltoni. CORe50: a new dat aset and benchmark for con-
tinuous object recognition. In Sergey Levine, Vincent Vanh oucke, and Ken Goldberg, editors,
Proceedings of the 1st Annual Conference on Robot Learning , volume 78 of Proceedings of
Machine Learning Research , pages 17–26. PMLR, 13–15 Nov 2017. 3
[19] Francesco Marra, Cristiano Saltori, Giulia Boato, and Luisa Verdoliva. Incremental learning
for the detection and classiﬁcation of gan-generated image s. In WIFS , 2019. 8
[20] Michael McCloskey and Neal J Cohen. Catastrophic inter ference in connectionist networks:
The sequential learning problem. In Psychology of learning and motivation , volume 24, pages
109–165. Elsevier, 1989. 1
[21] German I Parisi, Ronald Kemker, Jose L Part, Christophe r Kanan, and Stefan Wermter. Con-
tinual lifelong learning with neural networks: A review. Neural networks , 113:54–71, 2019.
1
[22] Lorenzo Pellegrini, Gabriele Grafﬁeti, Vincenzo Lomo naco, and Davide Maltoni. Latent replay
for real-time continual learning. In IROS , 2020.
[23] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate S aenko, and Bo Wang. Moment
matching for multi-source domain adaptation. In ICCV , 2019. 3
[24] Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. GDum b: A simple approach that
questions our progress in continual learning. In ECCV , 2020. 8,9,10
[25] Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan D yer. Effect of scale on catastrophic
forgetting in neural networks. In International Conference on Learning Representations , 2022.
1
[26] Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian K habsa, Mike Lewis, and Amjad
Almahairi. Progressive prompts: Continual learning for la nguage models. In ICLR . OpenRe-
view.net, 2023. 8
[27] Hippolyt Ritter, Aleksandar Botev, and David Barber. O nline structured laplace approxima-
tions for overcoming catastrophic forgetting. In NeurIPS , pages 3742–3752, 2018. 8
[28] Sebastian Ruder, Jonas Pfeiffer, and Ivan Vuli ´c. Modular and parameter-efﬁcient ﬁne-tuning
for NLP models. In Proceedings of the 2022 Conference on Empirical Methods in N atural
Language Processing: Tutorial Abstracts , pages 23–29, Abu Dubai, UAE, December 2022.
Association for Computational Linguistics. 1
[29] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, P aola Cascante-Bonilla, Donghyun
Kim, Assaf Arbelle, Rameswar Panda, Rogério Feris, and Zsol t Kira. Coda-prompt: Continual
decomposed attention-based prompting for rehearsal-free continual learning. In CVPR , pages
11909–11919. IEEE, 2023. 8
[30] Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yank ai Lin, Huadong Wang, Kaiyue
Wen, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou, Maosong Sun, an d Jie Zhou. On transferability
of prompt tuning for natural language processing. In Proceedings of the 2022 Conference
of the North American Chapter of the Association for Computa tional Linguistics: Human
Language Technologies , pages 3949–3969, Seattle, United States, July 2022. Assoc iation for
Computational Linguistics. 2
6

--- PAGE 7 ---
[31] Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. S-prompts l earning with pre-trained trans-
formers: An occam’s razor for domain incremental learning. InNeurIPS , 2022. 2,3,4,8,9,
10
[32] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, H an Zhang, Chen-Yu Lee, Xiaoqi
Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprom pt: Complementary prompting
for rehearsal-free continual learning. pages 631–648, 202 2.8
[33] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruox i Sun, Xiaoqi Ren, Guolong Su,
Vincent Perot, Jennifer Dy, and Tomas Pﬁster. Learning to pr ompt for continual learning. In
CVPR , 2022. 3,9,10
[34] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models ,
2019. 3
[35] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zichen g Liu, Yandong Guo, and Yun
Fu. Large scale incremental learning. In CVPR , 2019. 8,9,10
[36] Fei Ye and Adrian G. Bors. Learning latent representati ons across multiple data domains using
lifelong V AEGAN. In ECCV (20) , volume 12365 of Lecture Notes in Computer Science , pages
777–795. Springer, 2020. 8
[37] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin ual learning through synaptic intel-
ligence. In ICML , 2017. 3
[38] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for
vision-language models. arXiv preprint arXiv:2109.01134 , 2021. 2,8
7

--- PAGE 8 ---
Appendix A Related Work
Continual learning methods can be broadly classiﬁed based o n how they retain the information
learned in previous datasets. Replay-based methods tackle catastrophic forgetting by using some
additional data which is used when training on the new data [ 1–3,10,19,24,35]. These methods
store a few data points from previous datasets in a memory of l imited size and replay those data
points during training. Memory-free approaches replace tr ue data points with generated or auxiliary
data, which is replayed [ 13,36].
Regularization-based methods oftentimes require no memory and avoid forgetting b y adding regu-
larization terms to the loss function. These terms can eithe r regularize the weights directly to avoid
changing important weights [ 14,27] or regularizing activation outputs [ 5,17].
With the advent of large scale pre-trained transformers, me mory-free continual learning based on
prompt-tuning [ 15] for domain-incremental or class-incremental learning, o r adapters [ 11] for task-
incremental learning [ 8] have been proposed recently. Learning To Prompt (L2P) [ 38], based
on prompt-tuning, learns a set of input-dependent prompts t hat are shared across datasets. Dual-
Prompts [ 32] extends this by learning adding dataset-dependent and dat aset-independent prompts at
various points in the network. In addition to this idea, foll ow-up work proposes to learn components
which are combined to prompts at inference time [ 29]. Works that simplify the problem by learning
a per-dataset prompt that are combined for efﬁcient forward transfer exist. However, this requires
to assume a task-incremental setting where old prompts are n ot further updated [ 26] or access to
old data [ 7]. S-Prompts overcomes this problem by training assuming a t ask-incremental setting
and then solving the task identiﬁcation problem at inferenc e time using clustering [ 31]. The work
discussed here for continual learning for transformers rel ies on variations of prompt-tuning or preﬁx-
tuning [ 16]. Additionally, S-Prompts is primarily shown to work for do main-incremental scenarios.
Our method, CoLoR, extends this line of work by using LoRA mod ules, retains the simplicity of
S-Prompts, and is effective at both domain incremental and t ask incremental learning scenarios.
Appendix B Vision Transformer
In this section, we describe the Vision Transformer [ 6] (ViT) that we use in this paper. ViT ingests
an image I∈RW×H×3, and ﬁrst extracts patches of size P×P, totallingW×H
P2patches per image.
Each of these patches is ﬂattened and embedded into a Ddimensional space. To this a learned
position encoding ( Epos) is added, and a special token called the classiﬁcation ([CL S]) token is
concatenated. We refer to this as X0∈RN×DwhereN=W×H
P2+ 1. This operation can be
represented as
X0= [[CLS];I1
pE;···I(N−1)
pE]+Epos. (1)
This feature representation is processed through Llayers of multi-head self attention layers.
Xa
l=MHSA(Xl−1)+Xl−1
Xl=FFN(Xa
l)+Xa
l/bracerightbigg
∀l= 1...L
The function MHSA consists of mutiple SA modules that functi on in parallel. Each SA module can
be written as
SA(Xl) = softmax/parenleftBigg
XlWl
QWl
KTXT
l
2√
d/parenrightBigg
XlWV (2)
and the FFN as
FFN(Xl) =GeLU(Wl
2GeLU(Wl
1Xl+bl
1)+bl
2). (3)
The [CLS] token at XLis fed into a linear layer RD→RCthat outputs the logits for classiﬁcation.
The set of trainable parameters for ﬁne-tuning is {Wl
∗,bl
∗}L
l=1.
8

--- PAGE 9 ---
Appendix C Training hyperparameters
We closely follow the protocol by earlier work to allow for fa ir comparison [ 31]. We adopt their
data augmentation which consists of simple horizontal ﬂips and random crops. We use a batch size
of128and a weight decay of 0.0002 . We set learning rates and epochs to minimize training budge t.
In most cases, we use 50epochs with the exception of CORe50 where we use 20. As a default, we
use a learning rate of 10−3. For CIFAR-100, we use 0.01, for CORe50, 0.02. Cosine annealing is
used to decay the learning rate over time. Unless otherwise s tated, we use a LoRA rank of 64. We
set the number of clusters to k= 5as recommended for S-Prompts [ 31] in DIL. For CIL, we set the
number of clusters to two times the number of new classes, i.e., 20 for Split CIFAR-100. The choice
of number of clusters and the rank is ablated in § 3and Appendix E.
Appendix D Results
In this section, we extend the results in Figures 1and2by comparing CoLoR to replay-based
methods in Tables 3and4.
For the domain incremental scenario presented in Table 3, we observe that CoLoR outperforms
replay method with limited buffer sizes on most datasets. On DomainNet, performance of CoLoR
is only matched by that of DyTox which uses a replay buffer.
In Table 4, we present detailed results for Split CIFAR-100. For ﬁne-t uning, we ﬁne-tune the entire
ViT model and mask the outputs for classes not present in an up date by setting those logits to −∞.
We ﬁnd that this is important for L2P, without which its perfo rmance suffers drastically. Using
“class-masking”, ﬁne-tuning results in Table 4are substantially higher than the ones reported in
literature as FT-seq and FT-seq-frozen. Furthermore, we re port the results obtained when training
the ViT on all data using LoRA, and ﬁne-tuning the entire mode l as the upper bound.
Table 3: Average accuracy results on three domain-incremen tal benchmarks. CoLoR consistently
outperforms alternative approaches even if these have acce ss to previous data. This includes the
self-reported upper bound for S-Prompts which has access to all data. Results marked with †from
[33], and with‡from [ 31].
Method Buffer Size CORe50 DomainNet
S-Prompts (upper bound)∞84.01‡63.22‡
LoRA (r= 64 ) 96.15 ±0.07 73.62±0.02
DyTox [ 7]
50/class79.21‡±0.10 62.94‡
ER [3] 80.10†±0.56 -
GDumb [ 24] 74.92†±0.25 -
BiC [ 35] 79.28†±0.30 -
DER++ [ 1] 79.70†±0.44 -
Co2L [2] 79.75†±0.84 -
L2P [ 33] 81.07†±0.13 -
EWC [ 14]
074.82†±0.60 47.62‡
LwF [ 17] 75.45†±0.40 49.19‡
L2P [ 33] 78.33†±0.06 40.15‡
S-Prompts [ 31] (k= 5) 83.13‡±0.51 50.62‡
CoLoR (r= 1, k= 5) 84.88 ±0.10 67.71±0.08
CoLoR (r= 8, k= 5) 85.72 ±0.48 68.87±0.04
CoLoR (r= 64, k= 5) 85.52 ±0.42 69.67±0.04
CoLoR++ ( r= 64, k= 5) 86.75±0.40 70.06±0.05
9

--- PAGE 10 ---
Table 4: Class-incremental learning on CIFAR-100. CoLoR ou tperforms S-Prompts and CoLoR++
all other continual learning methods. This includes the sel f-reported upper bound for L2P. Results
marked with * are taken from [ 33]. We report new results for training on all data using LoRA an d
full ﬁne-tuning.
Method Buffer sizeSplit CIFAR-100
Average Acc ( ↑) Forgetting ( ↓)
L2P (upper bound)
∞90.85*±0.12 N/A
LoRA (r= 64 ) 92.49 ±0.07 N/A
Fine-Tuning 92.11 ±0.10 N/A
ER [3]
50/class82.53*±0.17 16.46*±0.25
GDumb [ 24] 81.67* ±0.02 N/A
BiC [ 35] 81.4*2 ±0.85 17.31*±1.02
DER++ [ 1] 83.94* ±0.34 14.55*±0.73
Co2L [2] 82.49* ±0.89 17.48*±1.80
L2P-R [ 33] 86.31* ±0.59 5.83*±0.61
ER [3]
10/class67.87*±0.57 33.33*±1.28
GDumb [ 24] 67.14* ±0.37 N/A
BiC [ 35] 66.11* ±1.76 35.24*±1.64
DER++ [ 1] 61.06* ±0.87 39.87*±0.99
Co2L [2] 72.15* ±1.32 28.55*±1.56
L2P-R [ 33] 84.21* ±0.53 7.72*±0.77
FT-seq-frozen
017.72*±0.34 59.09*±0.25
FT-seq 33.61* ±0.85 86.87*±0.20
FT+class masking 67.02 ±4.20 24.37±3.76
EWC [ 14] 47.01* ±0.29 33.27*±1.17
LwF [ 17] 60.69* ±0.63 27.77*±2.17
L2P [ 33] 83.83* ±0.04 7.63*±0.30
S-Prompts [ 31] (k= 5) 57.17 ±1.57 19.56±0.86
S-Prompts [ 31] (k= 10 ) 65.71 ±1.50 14.76±0.75
S-Prompts [ 31] (k= 20 ) 67.31 ±1.34 12.47±1.49
CoLoR (r= 64, k= 5) 59.98 ±0.04 18.69±0.41
CoLoR (r= 64, k= 10 ) 68.51 ±0.23 10.65±0.04
CoLoR (r= 1, k= 20 ) 70.87 ±0.23 10.16±0.19
CoLoR (r= 8, k= 20 ) 71.22 ±0.11 10.22±0.18
CoLoR (r= 64, k= 20 ) 71.42 ±0.24 10.27±0.39
CoLoR++ ( r= 1, k= 20 ) 85.27 ±0.24 6.55±0.46
CoLoR++ ( r= 64, k= 20 ) 86.47±0.07 6.25±0.34
Appendix E Ablations
In this section, we study the effect of the number of clusters kon the average accuracy. We vary k
by ﬁxing all other hyperparameters to the defaults describe d in Appendix C.
In Figure 4, we observe a similar behavior as that of increasing rank in F igure 3for the number
of clusters: more yields better results for CIL, where choos ing a large enough number of clusters
results in a substantial increase in performance. The advan tages of increasing kfurther diminish very
quickly. This is not surprising given that in this scenario, the clusters represent individual classes.
Therefore, if kis smaller than the number of classes in an update (in this cas e10), the centroids
are not able to represent the dataset sufﬁciently causing da taset detection failures. This is clearly
demonstrated by the saturation that we achieve once kreaches the number of new classes. We ﬁnd
that the choice of kis not too sensitive; above a certain small threshold, its ch oice has relatively little
inﬂuence on the results. Optimizing it is relatively cheap a s it does not require retraining the model.
10

--- PAGE 11 ---
100101102
Number of Clusters k506070Average AccuracyS-Prompts ( k= 20 )Split CIFAR-100
Figure 4: Increasing the number of clusters without changin g any other setting. This signiﬁcantly
improves the performance in CIL (Split CIFAR-100) until kequals the number of new classes.
11
