# 2106.09685.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2106.09685.pdf
# Kích thước tệp: 1609513 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
LORA: THÍCH ỨNG HẠNG THẤP CỦA CÁC MÔ HÌNH NGÔN NGỮ LỚN

Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen
Microsoft Corporation
{edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com
yuanzhil@andrew.cmu.edu
(Phiên bản 2)

TÓM TẮT
Một mô hình quan trọng trong xử lý ngôn ngữ tự nhiên bao gồm việc tiền huấn luyện quy mô lớn trên dữ liệu miền chung và thích ứng với các tác vụ hoặc miền cụ thể. Khi chúng ta tiền huấn luyện các mô hình lớn hơn, việc tinh chỉnh đầy đủ, tái huấn luyện tất cả tham số mô hình, trở nên ít khả thi hơn. Lấy GPT-3 175B làm ví dụ - triển khai các phiên bản độc lập của mô hình được tinh chỉnh, mỗi mô hình có 175B tham số, là cực kỳ tốn kém. Chúng tôi đề xuất Thích ứng Hạng Thấp, hay LoRA, đóng băng các trọng số mô hình tiền huấn luyện và tiêm các ma trận phân rã hạng có thể huấn luyện vào mỗi lớp của kiến trúc Transformer, giảm đáng kể số lượng tham số có thể huấn luyện cho các tác vụ downstream. So với GPT-3 175B được tinh chỉnh bằng Adam, LoRA có thể giảm số lượng tham số có thể huấn luyện 10.000 lần và yêu cầu bộ nhớ GPU giảm 3 lần. LoRA hoạt động ngang bằng hoặc tốt hơn tinh chỉnh về chất lượng mô hình trên RoBERTa, DeBERTa, GPT-2 và GPT-3, mặc dù có ít tham số có thể huấn luyện hơn, thông lượng huấn luyện cao hơn, và không giống adapter, không có độ trễ suy luận bổ sung. Chúng tôi cũng cung cấp một nghiên cứu thực nghiệm về sự thiếu hụt hạng trong thích ứng mô hình ngôn ngữ, làm sáng tỏ hiệu quả của LoRA. Chúng tôi phát hành một gói tạo điều kiện tích hợp LoRA với các mô hình PyTorch và cung cấp triển khai cũng như checkpoint mô hình cho RoBERTa, DeBERTa và GPT-2 tại https://github.com/microsoft/LoRA.

1 GIỚI THIỆU
Nhiều ứng dụng trong xử lý ngôn ngữ tự nhiên dựa vào việc thích ứng một mô hình ngôn ngữ tiền huấn luyện quy mô lớn với nhiều ứng dụng downstream. Việc thích ứng như vậy thường được thực hiện thông qua tinh chỉnh, cập nhật tất cả tham số của mô hình tiền huấn luyện. Nhược điểm chính của tinh chỉnh là mô hình mới chứa nhiều tham số như trong mô hình ban đầu. Khi các mô hình lớn hơn được huấn luyện vài tháng một lần, điều này chuyển từ một "bất tiện" đơn thuần đối với GPT-2 (Radford et al., b) hoặc RoBERTa large (Liu et al., 2019) thành một thách thức triển khai quan trọng đối với GPT-3 (Brown et al., 2020) với 175 tỷ tham số có thể huấn luyện.

Nhiều người đã tìm cách giảm thiểu điều này bằng cách chỉ thích ứng một số tham số hoặc học các mô-đun bên ngoài cho các tác vụ mới. Bằng cách này, chúng ta chỉ cần lưu trữ và tải một số lượng nhỏ tham số cụ thể cho tác vụ ngoài mô hình tiền huấn luyện cho mỗi tác vụ, tăng đáng kể hiệu quả hoạt động khi triển khai. Tuy nhiên, các kỹ thuật hiện có

*Đóng góp bằng nhau.
⁰So với V1, bản thảo này bao gồm baseline tốt hơn, thí nghiệm trên GLUE, và nhiều hơn về độ trễ adapter.
¹Trong khi GPT-3 175B đạt được hiệu suất không tầm thường với few-shot learning, tinh chỉnh tăng hiệu suất của nó đáng kể như được thể hiện trong Phụ lục A.

--- TRANG 2 ---
thường gây ra độ trễ suy luận (Houlsby et al., 2019; Rebuffi et al., 2017) bằng cách mở rộng độ sâu mô hình hoặc giảm độ dài chuỗi có thể sử dụng của mô hình (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021) (Phần 3). Quan trọng hơn, các phương pháp này thường không thể sánh được với baseline tinh chỉnh, tạo ra sự đánh đổi giữa hiệu quả và chất lượng mô hình.

Chúng tôi lấy cảm hứng từ Li et al. (2018a); Aghajanyan et al. (2020) cho thấy rằng các mô hình được học quá tham số hóa thực tế nằm trong một chiều nội tại thấp. Chúng tôi giả thuyết rằng sự thay đổi trọng số trong quá trình thích ứng mô hình cũng có "hạng nội tại" thấp, dẫn đến cách tiếp cận Thích ứng Hạng Thấp (LoRA) được đề xuất của chúng tôi. LoRA cho phép chúng ta huấn luyện một số lớp dày đặc trong mạng neural một cách gián tiếp bằng cách tối ưu hóa các ma trận phân rã hạng của sự thay đổi các lớp dày đặc trong quá trình thích ứng thay vì, trong khi giữ các trọng số tiền huấn luyện bị đóng băng, như được thể hiện trong Hình 1. Sử dụng GPT-3 175B làm ví dụ, chúng tôi cho thấy rằng hạng rất thấp (tức là r trong Hình 1 có thể là một hoặc hai) là đủ ngay cả khi hạng đầy đủ (tức là d) cao tới 12,288, làm cho LoRA vừa hiệu quả về lưu trữ vừa hiệu quả về tính toán.

LoRA sở hữu một số lợi thế chính:
• Một mô hình tiền huấn luyện có thể được chia sẻ và sử dụng để xây dựng nhiều mô-đun LoRA nhỏ cho các tác vụ khác nhau. Chúng ta có thể đóng băng mô hình được chia sẻ và chuyển đổi tác vụ một cách hiệu quả bằng cách thay thế các ma trận A và B trong Hình 1, giảm đáng kể yêu cầu lưu trữ và chi phí chuyển đổi tác vụ.
• LoRA làm cho việc huấn luyện hiệu quả hơn và giảm rào cản phần cứng để tham gia lên đến 3 lần khi sử dụng các bộ tối ưu thích ứng vì chúng ta không cần tính toán gradient hoặc duy trì trạng thái bộ tối ưu cho hầu hết các tham số. Thay vào đó, chúng ta chỉ tối ưu hóa các ma trận hạng thấp được tiêm, nhỏ hơn nhiều.
• Thiết kế tuyến tính đơn giản của chúng tôi cho phép chúng ta hợp nhất các ma trận có thể huấn luyện với các trọng số bị đóng băng khi triển khai, không gây ra độ trễ suy luận so với mô hình được tinh chỉnh đầy đủ, theo thiết kế.
• LoRA trực giao với nhiều phương pháp trước đây và có thể được kết hợp với nhiều trong số chúng, chẳng hạn như prefix-tuning. Chúng tôi cung cấp một ví dụ trong Phụ lục E.

Thuật ngữ và Quy ước. Chúng tôi thường xuyên tham khảo đến kiến trúc Transformer và sử dụng các thuật ngữ thông thường cho các chiều của nó. Chúng tôi gọi kích thước chiều đầu vào và đầu ra của một lớp Transformer là d_model. Chúng tôi sử dụng W_q, W_k, W_v và W_o để chỉ các ma trận chiếu query/key/value/output trong mô-đun self-attention. W_0 hoặc W^0 tham chiếu đến ma trận trọng số tiền huấn luyện và ΔW là cập nhật gradient tích lũy của nó trong quá trình thích ứng. Chúng tôi sử dụng r để biểu thị hạng của một mô-đun LoRA. Chúng tôi tuân theo các quy ước được đặt ra bởi (Vaswani et al., 2017; Brown et al., 2020) và sử dụng Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) để tối ưu hóa mô hình và sử dụng chiều feedforward MLP Transformer d_ffn = 4d_model.

2 PHÁT BIỂU BÀI TOÁN
Trong khi đề xuất của chúng tôi không phụ thuộc vào mục tiêu huấn luyện, chúng tôi tập trung vào mô hình hóa ngôn ngữ như trường hợp sử dụng động lực của chúng tôi. Dưới đây là một mô tả ngắn gọn về bài toán mô hình hóa ngôn ngữ và, đặc biệt, việc tối đa hóa xác suất có điều kiện với một prompt cụ thể cho tác vụ.

Giả sử chúng ta được cung cấp một mô hình ngôn ngữ autoregressive tiền huấn luyện P_Φ(y|x) được tham số hóa bởi Φ. Ví dụ, P_Φ(y|x) có thể là một người học đa tác vụ chung như GPT (Radford et al., b; Brown et al., 2020) dựa trên kiến trúc Transformer (Vaswani et al., 2017). Xem xét việc thích ứng mô hình tiền huấn luyện này với các tác vụ sinh văn bản có điều kiện downstream, chẳng hạn như tóm tắt, đọc hiểu máy (MRC), và ngôn ngữ tự nhiên sang SQL (NL2SQL). Mỗi tác vụ downstream được đại diện bởi một tập dữ liệu huấn luyện của các cặp ngữ cảnh-mục tiêu: Z = {(x_i, y_i)}_{i=1,...,N}, trong đó cả x_i và y_i đều là chuỗi token. Ví dụ, trong NL2SQL, x_i là một truy vấn ngôn ngữ tự nhiên và y_i là lệnh SQL tương ứng của nó; đối với tóm tắt, x_i là nội dung của một bài báo và y_i là tóm tắt của nó.

--- TRANG 3 ---
Trong quá trình tinh chỉnh đầy đủ, mô hình được khởi tạo bằng trọng số tiền huấn luyện Φ_0 và được cập nhật thành Φ_0 + ΔΦ bằng cách liên tục theo gradient để tối đa hóa mục tiêu mô hình hóa ngôn ngữ có điều kiện:

max_ΔΦ ∑_{(x,y)∈Z} ∑_{t=1}^{|y|} log(P_Φ_0+ΔΦ(y_t|x, y_{<t}))     (1)

Một trong những nhược điểm chính của tinh chỉnh đầy đủ là đối với mỗi tác vụ downstream, chúng ta học một tập tham số khác nhau có kích thước |ΔΦ| bằng |Φ_0|. Do đó, nếu mô hình tiền huấn luyện lớn (như GPT-3 với |Φ_0| ≈ 175 tỷ), việc lưu trữ và triển khai nhiều phiên bản độc lập của mô hình được tinh chỉnh có thể là thách thức, nếu có thể thực hiện được.

Trong bài báo này, chúng tôi áp dụng một cách tiếp cận hiệu quả tham số hơn, trong đó số gia tham số cụ thể cho tác vụ ΔΦ = ΔΦ(Θ) được mã hóa thêm bởi một tập tham số có kích thước nhỏ hơn nhiều Θ với |Θ| ≪ |Φ_0|. Nhiệm vụ tìm ΔΦ do đó trở thành tối ưu hóa trên Θ:

max_Θ ∑_{(x,y)∈Z} ∑_{t=1}^{|y|} log(P_Φ_0+ΔΦ(Θ)(y_t|x, y_{<t}))     (2)

Trong các phần tiếp theo, chúng tôi đề xuất sử dụng biểu diễn hạng thấp để mã hóa ΔΦ vừa hiệu quả về tính toán vừa hiệu quả về bộ nhớ. Khi mô hình tiền huấn luyện là GPT-3 175B, số lượng tham số có thể huấn luyện |Θ| có thể nhỏ tới 0,01% của |Φ_0|.

3 CÁC GIẢI PHÁP HIỆN CÓ CÓ ĐỦ TỐT KHÔNG?
Vấn đề mà chúng tôi đặt ra để giải quyết không phải là mới. Kể từ khi xuất hiện học chuyển giao, hàng chục công trình đã tìm cách làm cho việc thích ứng mô hình hiệu quả hơn về tham số và tính toán. Xem Phần 6 cho một khảo sát về một số công trình nổi tiếng. Sử dụng mô hình hóa ngôn ngữ làm ví dụ, có hai chiến lược nổi bật khi nói đến thích ứng hiệu quả: thêm các lớp adapter (Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2021; Rückle et al., 2020) hoặc tối ưu hóa một số dạng của các kích hoạt lớp đầu vào (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020; Liu et al., 2021). Tuy nhiên, cả hai chiến lược đều có hạn chế của chúng, đặc biệt trong một tình huống sản xuất quy mô lớn và nhạy cảm với độ trễ.

Các Lớp Adapter Gây Ra Độ Trễ Suy Luận. Có nhiều biến thể của adapter. Chúng tôi tập trung vào thiết kế ban đầu của Houlsby et al. (2019) có hai lớp adapter cho mỗi khối Transformer và một thiết kế gần đây hơn của Lin et al. (2020) chỉ có một lớp trên mỗi khối nhưng có thêm LayerNorm (Ba et al., 2016). Trong khi có thể giảm độ trễ tổng thể bằng cách cắt tỉa các lớp hoặc khai thác cài đặt đa tác vụ (Rückle et al., 2020; Pfeiffer et al., 2021), không có cách trực tiếp nào để bỏ qua việc tính toán bổ sung trong các lớp adapter. Điều này có vẻ như không phải là vấn đề vì các lớp adapter được thiết kế để có ít tham số (đôi khi <1% của mô hình gốc) bằng cách có chiều nút cổ chai nhỏ, điều này hạn chế số FLOPs chúng có thể thêm vào. Tuy nhiên, các mạng neural lớn dựa vào tính song song phần cứng để giữ độ trễ thấp, và các lớp adapter phải được xử lý tuần tự. Điều này tạo ra sự khác biệt trong cài đặt suy luận trực tuyến nơi kích thước batch thường nhỏ bằng một. Trong một tình huống chung không có tính song song mô hình, chẳng hạn như chạy suy luận trên GPT-2 (Radford et al., b) medium trên một GPU duy nhất, chúng tôi thấy sự gia tăng đáng chú ý về độ trễ khi sử dụng adapter, ngay cả với chiều nút cổ chai rất nhỏ (Bảng 1).

Vấn đề này trở nên tồi tệ hơn khi chúng ta cần phân mảnh mô hình như được thực hiện trong Shoeybi et al. (2020); Lepikhin et al. (2020), bởi vì độ sâu bổ sung yêu cầu nhiều hoạt động GPU đồng bộ hơn như AllReduce và Broadcast, trừ khi chúng ta lưu trữ các tham số adapter một cách dư thừa nhiều lần.

Tối Ưu Hóa Trực Tiếp Prompt Khó Khăn. Hướng khác, như được minh họa bởi prefix tuning (Li & Liang, 2021), gặp phải một thách thức khác. Chúng tôi quan sát thấy rằng prefix tuning khó tối ưu hóa và hiệu suất của nó thay đổi không đơn điệu theo các tham số có thể huấn luyện, xác nhận các quan sát tương tự trong bài báo gốc. Cơ bản hơn, việc dành một phần độ dài chuỗi cho thích ứng nhất thiết giảm độ dài chuỗi có sẵn để xử lý một tác vụ downstream, điều mà chúng tôi nghi ngờ làm cho việc điều chỉnh prompt kém hiệu quả hơn so với các phương pháp khác. Chúng tôi hoãn nghiên cứu về hiệu suất tác vụ đến Phần 5.

--- TRANG 4 ---
[Bảng 1 và phần tiếp theo của tài liệu được dịch tiếp tục...]
