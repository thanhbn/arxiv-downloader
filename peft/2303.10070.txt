# 2303.10070.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2303.10070.pdf
# File size: 737786 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
A Unified Continual Learning Framework with General
Parameter-Efficient Tuning
Qiankun Gao1Chen Zhao†2Yifan Sun3Teng Xi3Gang Zhang3Bernard Ghanem2Jian Zhang†1
1Peking University Shenzhen Graduate School2King Abdullah University of Science and Technology (KAUST)3Baidu Inc.
gqk@stu.pku.edu.cn chen.zhao@kaust.edu.sa zhangjian.sz@pku.edu.cn
Abstract
The “pre-training →downstream adaptation” presents
both new opportunities and challenges for Continual Learn-
ing (CL). Although the recent state-of-the-art in CL is
achieved through Parameter-Efficient-Tuning (PET) adap-
tation paradigm, only prompt has been explored, limiting its
application to Transformers only. In this paper, we position
prompting as one instantiation of PET, and propose a uni-
fied CL framework with general PET, dubbed as Learning-
Accumulation-Ensemble (LAE). PET, e.g., using Adapter,
LoRA, or Prefix, can adapt a pre-trained model to down-
stream tasks with fewer parameters and resources. Given
a PET method, our LAE framework incorporates it for CL
with three novel designs. 1) Learning: the pre-trained
model adapts to the new task by tuning an online PET mod-
ule, along with our adaptation speed calibration to align
different PET modules, 2) Accumulation: the task-specific
knowledge learned by the online PET module is accumu-
lated into an offline PET module through momentum up-
date, 3) Ensemble: During inference, we respectively con-
struct two experts with online/offline PET modules (which
are favored by the novel/historical tasks) for prediction en-
semble. We show that LAE is compatible with a battery of
PET methods and gains strong CL capability. For exam-
ple, LAE with Adaptor PET surpasses the prior state-of-
the-art by 1.3% and 3.6% in last-incremental accuracy on
CIFAR100 and ImageNet-R datasets, respectively. Code is
available at https://github.com/gqk/LAE .
1. Introduction
Continual Learning (CL) of new knowledge is an essen-
tial ability for AI models in the constantly changing world.
However, neural networks often suffer from catastrophic
forgetting [9, 39], in which previously learned knowledge is
forgotten when the model incorporates novel information.
Although many works have been devoted to reducing for-
getting, such as dynamic networks [41, 50, 26, 18], regular-
†Corresponding authors.
Task N K PredictionQuery Selection1
234
5
Task N
Offline PET Module
(historical knowledge)3. Ensemble Prediction
Our LAE: A unified framework 
that explores general PET methods LearnableApproaches based on Prompt Pool:  Prompt Learning + Prompt Sele ction
1. Learning
2. AccumuationOnline PET Module
(Prompt/Adapter/LoRA)Figure 1: The pipeline of our LAE framework vs.
prompt-pool approaches. Above: Prompt-pool ap-
proaches, which query prompts from a pool of learn-
able prompts. Below: Proposed LAE, where an online
Parameter-Efficient Tuning (PET) module attached to the
pre-trained model to adapt a new task quickly, and an offline
PET module accumulates the learned knowledge slowly.
During inference, we use the ensemble of the predictions of
the online and offline PET modules as the final prediction.
ization [28, 20, 52, 1], and memory replay [38, 15, 6, 30, 2],
their performance still falls short of practical requirements.
Recently, pre-training and downstream adaptation tech-
niques have opened up new opportunities and challenges for
CL. Basically, these techniques [4, 36, 12, 11, 48] pre-train
a deep model on large-scale data and then adapt the pre-
trained model to novel tasks. We observe that downstream
adaptation and CL are important for each other. On the one
hand, in realistic AI systems, pre-trained models sometimes
needs to be adapted to multiple downstream tasks sequen-
tially, yielding the need of CL. On the other hand, recent ef-
forts [47, 46, 45] show that the “pre-training →downstream
adaptation” techniques can boost CL performance.
Specifically, L2P [47], DualPrompt [46], and ESN [45]
all use a popular adaptation technique named Parameter-
Efficient-Tuning (PET). Generally, PET adapts pre-trained
models to downstream tasks with much fewer learnable pa-
rameters, as well as fewer resources. Though these ap-
proaches have advanced the state-of-the-art in CL, they still
have some limitations. 1) They are all constrained to a spe-
cific PET method, i.e., prompt tuning, limiting their flexibil-
1arXiv:2303.10070v2  [cs.CV]  19 Aug 2023

--- PAGE 2 ---
ity, considering that prompt can only cooperate with trans-
formers and does not accommodate other network architec-
tures. 2) Most of them rely on selecting task-specific pa-
rameters (the prompt tokens, in particular) for each indi-
vidual task. The selection tends to be noisy with increasing
task numbers and the task-specific prompts appear homoge-
neous, according to our investigation in the supplementary.
To circumvent these issues, this paper proposes
Learning-Accumulation-Ensemble (LAE), a unified CL
framework resort to the general Parameter-Efficient Tuning
(PET). LAE is not restricted to Prompt, but can also utilize
various other PET modules as shown in Fig. 2 (b). Given a
PET method, our LAE directly reshapes it for CL with three
steps, i.e., learning, accumulation, and ensemble.
•1) Learning with calibrated speed. The pre-trained
model adapts to the new task by tuning an online PET mod-
ule. To accommodate various PET methods, a key chal-
lenge is that different PET modules have different adap-
tation speeds (for novel tasks), as well as different forget-
ting speeds (for historical tasks). In response, we design an
adaptation calibration strategy, based on the gradient anal-
ysis for different PET modules. We empirically show that
this calibration strategy aligns different PET against each
other and is critical for LAE to be a unified framework.
•2) Accumulation of multi-task knowledge. After
adapting the pre-trained model to a new task, the parameters
in the online PET module are prone to the current novel task
and may not fit historical tasks. Instead of memorizing mul-
tiple sets of PET modules and selecting some subsets (as in
L2P and DualPrompt) for individual tasks, LAE accumu-
lates all the knowledge of already-seen tasks into a single
offline PET module through momentum update. This sim-
ple accumulation avoids noisy selection and is competent
foralleviating catastrophic forgetting, especially when the
amount of learned tasks is large (Figs. 3 and 4 in Sec. 5).
•3) Ensemble of two expert models. The online and
offline PET modules respectively contain more novel and
historical knowledge, therefore, two expert models con-
structed with them are correspondingly better at handling
newer and older tasks. Instead of inference only using
the online or offline expert model, we integrate the outputs
of two expert models by an energy indicator (detailed in
Sec. 4.2) to obtain the prediction for an inference sample
from any learned task. This expert ensemble strategy helps
our framework to achieve a more robust performance com-
pared to inference using one of the expert models alone.
The contributions of this paper are summarized as follows:
• We thoroughly investigate the novel Continual Learn-
ing paradigm that constantly adapts a pre-trained
model to novel tasks using general Parameter-
Efficient Tuning (PET) methods, and propose a unified
Learning-Accumulation-Ensemble (LAE) framework.• Our LAE framework reshapes a given PET method
into a competitive Memory-Free Continual Learning
approach with three novel designs: Learning with cal-
ibrated speed, Accumulation of multi-task knowledge,
and Ensemble of two expert models constructed with
online and offline PET modules.
• We conduct extensive experiments on CIFAR100
and ImageNet-R benchmarks, on all of which, our
LAE consistently achieves superior incremental per-
formance than previous state-of-the-art approaches.
2. Related Works
Parameter-Efficient Tuning. As an efficient alternative to
full fine-tuning, Adapter-Tuning [16] was first proposed to
transfer large pre-trained Language models to downstream
tasks. Inspired by textual prompting, Prompt-Tuning [25]
and Prefix-Tuning [27] insert learnable tokens to adapt to
the new task. More advanced methods [51, 7, 17, 33]
achieved comparable or superior performance to full fine-
tuning, and keep the same inference cost by merging the
additional learnable parameters to the original pre-trained
model. Following the step of successful Vision Transform-
ers [5, 31], VPT [19] and AdapterFormer [3] have been pro-
posed to solve visual transfer learning problems. Prompt-
Tuning and Prefix-Tuning depend on the transformer ar-
chitecture because they modify input or hidden tokens.
Adapter and its variants are network architecture generaliz-
able since they are new modules that can be implemented
in forms compatible with pre-trained models. All types
of Parameter-Efficient Tuning modules can be integrated
into our LAE framework as long as they are suitable for
the pre-trained model, but we focus on the representative
Adapter [16], LoRA [17], and Prefix [27] in this paper.
Continual Learning. The central problem of Contin-
ual Learning (CL) is fighting catastrophic forgetting [9].
Memory-based approaches [38, 15, 6, 30, 2] save a subset
of learned samples into a memory buffer and replay them
when learning a new task. Memory-Free approaches do not
rely on old samples that may raise privacy concerns, they
dynamically expand the network or isolate parameters for
different tasks [41, 50, 26, 18], regularize the network pa-
rameters that are important to learned tasks [28, 20, 52, 1],
and replay generative or synthetic data [42, 49, 43, 8]. Con-
ventional CL approaches learn tasks from scratch using a
randomly initialized model, while pre-trained models have
received little attention from CL researchers until recently.
Two pioneering works [47, 46] introduce Prompt-Tuning to
CL and achieve much higher incremental performance than
previous approaches, demonstrating the advantage of using
pre-trained models in CL. Side-Tuning [53] adopts a tech-
nique similar to Adapter-Tuning but requires the task iden-
tity of the inference sample. In this paper, we propose a

--- PAGE 3 ---
unified framework for Memory-Free CL that can incorpo-
rate various types of PET modules. Particularly, we focus
on practical Class-Incremental Learning with the potential
to extend our LAE to other CL scenarios in future work.
3. Preliminaries
3.1. Continual Learning Formulation
We focus on Continual Learning with incremental
classes, i.e., Class-Incremental Learning (CIL), where a
model sequentially learns tasks T:={T1,T2,···,Tn}, the
ithtaskTihas|Ti|categories, the train set of Tiis denoted
asDi, and the categories are non-overlapping between
tasks. The model f(·;θ,ϕ)predicts the category label y∈
Yfor a given sample x∈ X of the learned tasks, where Yis
all seen categories, θandϕare parameters of feature extrac-
tor and classification head, respectively. In this paper, the
feature extractor is a pre-trained model parameterized by
θpreattached with the Parameter-Efficient Tuning module
parameterized by θpet, and ϕ=concatenate( ϕold,ϕnew),
where ϕoldandϕneware classifiers of all learned tasks T1:i
and the current learning task Ti. Since θpreandϕoldare
kept fixed during learning a new task, we may omit them
for concise in the rest of this paper.
3.2. Parameter-Efficient Tuning Revisit
Parameter-Efficient Tuning (PET) keeps the pre-trained
model frozen and tunes a small number of additional learn-
able parameters, called PET module in the paper. Below we
revisit several representative PET modules, in which gis the
module that PET attached to, eandhare input and output
of the orginal gandh′is output of gattached with PET.
Adapter [16] is a small module that can be inserted to any
layer ( i.e.,g) of the pre-trained model. As shown in Fig. 2
(b), the adapter is generally a residual block composed of a
down-projection with parameters Wdown , a nonlinear acti-
vation function σ(·), and an up-projection with parameters
Wup. The two projections can be convolution [37] for CNN
or linear [16] layers for Transformer architectures, respec-
tively. We formulate the adapter as follows:
h′=h+σ(h∗Wdown)∗Wup, (1)
where the ∗is matrix multiplication or convolution opera-
tion,σis the activation function. Alternatively, the adapter
can also be parallel with glike a residual branch [53, 10]:
h′=h+σ(e∗Wdown)∗Wup. (2)
LoRA [17] assumes the change of parameters is in a low-
rank space when tuning the pre-trained model on a down-
stream task. For a linear layer with weight W∈Rd×d′, the
weight updates ∆Wcan be decomposed into the multipli-
cation of two small matrices:
∆W=WdownWup, (3)where Wdown∈Rd×randWup∈Rr×d′. For the con-
volution layer, the updates can be reshaped into the ker-
nel shape. Finally, LoRA modifies the forward pass of the
adapted layer into the following form:
h′=h+e∗(WdownWup), (4)
where ∗is matrix multiplication or convolution operation,
the bias and reshape operation are omitted for conciseness.
Since LoRA adapts the weight of g, the weight updates can
be merged into gto reduce the inference latency.
Prefix [27] and Prompt [25] are learnable tokens
prepended to the input of a transformer block or keys and
values of the attention module. Given two sets of prefix to-
kensPk,Pv∈Rl×dthe attention module is modified as:
h′= Attn ( xW q,[Pk,eWk],[Pv,eWv]), (5)
where [·,·]is concatenate, and Attn is defined as:
Attn ( Q,K,V) := softmaxQKT
√
d
V,
and the multi-head mechanism is omitted for conciseness.
In this paper, we follow [10] to add a learnable scale pa-
rameter sto the parallel Adapter (Eq. 2) and LoRA (Eq. 4)
respectively to obtain:
h′=h+s·σ(e∗Wdown)∗Wup, (6)
h′=h+s·e∗(WdownWup). (7)
The Eqs. (6) and (7) are general forms of Eqs. (2) and (4),
and they degenerate to Eq. (2) and (4) when sis the constant
1. We use these PETs in Eqs. (6) and (7) rather than Eqs. (1)
and (4) in our experiments (Sec. 5).
In addition to the three PET modules described above,
there are many others, such as AdaptBias [7], Com-
pacter [33], and AdapterFormer [3], and there will be new
and superior PET methods in future as well. All of them
can be applied to our CL framework (see Sec. 4.2), as long
as they are compatible with the pre-trained model.
4. Methodology
4.1. Naive Baseline
We construct a baseline by leveraging pre-trained mod-
els and PET techniques, following the naive sequential fine-
tuning (Seq-FT) that was usually considered as the lower
bound of CIL. Intuitively, our baseline creates a PET mod-
ule and attaches it to the pre-trained model, then sequen-
tially learns tasks in the same way as Seq-FT but keeping
the pre-trained model frozen. We chose the local cross-
entropy loss (CE) rather than the global CE loss as the learn-
ing objective for Seq-FT and our baseline because local CE

--- PAGE 4 ---
AccumulationClassifier Offline
Pre-trained 
Model 
(frozen)Onlineĺ
Learning
Hidden StatesLoRA LoRA
ƚƚĞŶƚŝŽŶNon linea rAdapter

Adapter
......
LoRAu
Parallel Adapter
OnlineOnline OfflineOfflineEnsemble
Two
ExpertsAnchor
(b) PET Modules (c) Inference (a) TrainingAttachĺ
Attachĺ
  
  kv
qkvkvkvup
down
up
downFigure 2: Illustration of our LAE framework. The left (a) is the training process, the right (c) is the inference flow, and
the middle (b) lists some representative Parameter-Efficient Tuning (PET) modules attached to a transformer attention block,
modules connected with dashed lines are optional and we use one of the PET modules in the experiments. There is no
residual connection in Parallel Adapter. “Online” is a PET module to learn knowledge from the new task and “Offline” is a
PET module to accumulate knowledge. The pre-trained model is omitted in the inference flow for concise.
empirically performs better than global CE when using a
large pre-trained model [47, 46, 45]. The local CE is the
standard CE computed on categories of the current task:
L=1
|Di|X
(x,y)∈DiLce(mask( f(x;θ,ϕ)), y),(8)
where yis the ground truth label of the input xin the cur-
rent training set Di,mask(·)is a function that filters out the
logits of old categories. The Eq. (8) falls back to global CE
when mask(·)is removed. Although our baseline is very
naive, the performance is comparable to the state-of-the-art
DualPrompt when using the same Prefix module.
4.2. Proposed Framework
LAE builds upon our network architecture generalizable
baseline and additionally introduces three novel designs,
yielding a robust framework that can readily reshape any
PET methods into a competitive Continual Learning ap-
proach. In the following, we will delve into these three key
aspects of LAE: learning, accumulation, and ensemble.
Learning with calibrated speed . We observed that PET
modules vary in their speed for acquiring new knowledge,
leading to disparities in performance . In theory, adapting
the PET module to a new task too quickly can lead to over-
fitting and result in worse catastrophic forgetting, whereas
slower adaptation can maintain the model’s stability but
limit its plasticity. We argue that aligning the adaptation
speeds of different PET modules is crucial for transforming
them into an efficient and robust CL approach in a unified
way. To address this, we propose calibrating PET modules
to align their adaptation speeds.
Moreover, as the same PET module θpetis shared by
new and old tasks, the changes made to θpetfor the newtask can cause forgetting for the old tasks. Therefore, slow-
ing down the change in θpet,e.g., by reducing the learning
rate (see supplementary), can help mitigate catastrophic for-
getting. Kumar et al. [22] showcase how the linear probing
followed by fine-tuning strategy effectively balances perfor-
mance across out-of-distribution and in-distribution tasks.
We employ a similar technique to calibrate the θpet’s adap-
tation speed relative to ϕnew. Specifically, at the beginning
of the training, we only learn ϕnewwithθpetfrozen; then
afterϕnewhas sufficiently learned and the loss has signifi-
cantly decreased, we jointly learn both ϕnewandθpet.
According to the study by He et al. [10], the Prefix can be
equivalently transformed into a similar form to the Adapter:
h′←(1−λ(e))h+λ(e)σ(eW 1)W2, (9)
in which W1=WqP⊤
k,W2=Pv,σ=softmax and
λ(e) =P
iexp 
eWqP⊤
k
iP
iexp 
eWqP⊤
k
i+P
jexp 
eWqW⊤
kC⊤
j.
(10)
AsPkusually contains much fewer tokens than input C,
λ(e)is often a small positive number close to 0, which im-
pacts the gradient of Prefix tokens Pv:
∂L
∂Pv= (∂h′
∂Pv)⊤∂L
∂h′=λ(e)(σ(eW 1))⊤∂L
∂h′.(11)
Therefore, the gradient of Pvis significantly smaller
thanWupof the corresponding Adapter parameterized by
Wdown=W1andWup=W2, and we can arrive at a simi-
lar conclusion regarding PkandWdown . Then, we can eas-
ily observe that Prefix adapts to the new task much slower
than Adapter. This is partly why the prompts for different

--- PAGE 5 ---
tasks in prior approaches [47, 46] are prone to be homo-
geneous (see supplementary). Here we align Prefix with
Adapter by compensating its gradient by1
λ(e)and adding
two learnable scaling parameters skandsv, calibrating the
Prefix described by Eq. (9) into the following form:
h′←(1−λ(e))h+σ 
sk·eW 1
(sv·W2). (12)
The adaptation speed of the calibrated Prefix is nearly
equivalent to the Adapter depicted in Eq. (6) and elevates
its performance to be on par with the Adapter. For other
PET modules, we can also analyze them specifically and
then calibrate their adaptation speeds to align with Adapter.
By aligning the adaptation speed of PET modules and
calibrating their adaptation speed relative to the classi-
fiers, our framework achieves a better and more consistent
stability-plasticity balance with various PET modules.
Accumulation of multi-task knowledge . The PET mod-
uleθpetis designed to continuously adapt to new tasks,
making the model more proficient in dealing with novel
tasks. However, this adaptation process can result in the
model gradually forgetting how to handle older tasks. To
address this issue, we propose to create an additional expert
for older tasks to complement the expert for newer tasks,
drawing inspiration from the Complementary Learning Sys-
tem [34, 23] of the human brain, which involves the hip-
pocampus rapidly learning new knowledge and the neocor-
tex integrating learned knowledge in an offline manner over
time. We achieve this by duplicating the online PET module
θon
pet(i.e., theθpetin the baseline) attached to the model as
the offline PET module θoff
petafter the model has learned the
first task. The θoff
petslowly accumulates the learned knowl-
edge when the model learns a new task by an accumulation
function, and we empirically find the simple Exponential
Moving Average (EMA) algorithm works well for our LAE:
θoff
pet←α·θoff
pet+ (1−α)·θon
pet, (13)
where α∈(0,1)is a large ( i.e., close to 1) weight decay.
This way, the neocortex-like offline PET module gradu-
ally integrates the learned knowledge in a slow offline man-
ner, while the hippocampus-like online PET module con-
tinues to rapidly learn new knowledge. Then, we can obtain
two experts for newer tasks and older tasks with θon
petand
θoff
pet, respectively. However, the task a sample belongs to
is unknown during inference, we need to devise a method
to effectively utilize both experts for inference.
Ensemble of two expert models . Two expert models con-
structed with θon
petandθoff
petare respectively proficient at
handling newer and older tasks. Instead of inference only
using the online or offline expert model, we integrate their
outputs to obtain the prediction for an inference sample.A classifier can be viewed as an energy model when we
define the unnormalized negative log probability as the en-
ergy function [24]. The optimization goal of the energy
model is to minimize the energy of the model on the data
distribution of its learning task. Previous research [29] has
shown that the energy of an energy model trained on one
data domain is generally very high on other data domains.
Therefore, the Eq. (8) actually continuously minimizes the
energy of the ϕnewon the new task. Even if old data is not
used during training, the energy of the old data on ϕnewwill
be very high, as demonstrated by the recent work ESN [45].
Likewise, as the θon
petandθoff
petrespectively contain rel-
atively more novel and historical knowledge, theoretically,
the energy produced by θon
petfor the sample of the newer
tasks should be smaller than that produced by θoff
pet, and the
vice versa for the sample of older tasks. Therefore, choos-
ing the prediction result with the lowest energy as the final
prediction of an inference sample seems like a simple but
effective solution. However, in practice, we find that nor-
malizing the energy produced by θon
petandθoff
petbefore en-
semble yields more robust results. Therefore, we adopt the
following ensemble algorithm instead:
fens(oon,ooff) := max 
σ(oon), σ 
ooff
, (14)
where σis the softmax function, oonandooffare outputs
of the online and offline expert models ( i.e.,f(·;θon
pet,ϕ)
andf(·;θoff
pet,ϕ)) for an inference sample, respectively.
As illustrated in Fig. 2, in our LAE framework, the
model learns a new task with θon
petand accumulates the
learned knowledge to θoff
pet, the two experts favored by
newer and older tasks are ensembled to get the final pre-
diction for an inference sample. Our LAE can be applied to
the pre-trained model in any network architecture as long as
the PET modules are compatible with the model.
5. Experiment
5.1. Datasets and Evaluation Protocols
Our experiments use models pre-trained on the Im-
ageNet21k [40] dataset without specified, and we fol-
low prior works to train and evaluate the model on CI-
FAR100 [21] and ImageNet-R [14] benchmarks.
CIFAR100 is an extensively used dataset in prior contin-
ual learning (CL) works, containing 100 classes, each class
with 500 training and 100 test images of size 32 ×32×3.
ImageNet-R is first introduced to CL by Wang et al. [46],
including 200 subcategories of ImageNet [40], but its sam-
ples are in different styles, such as cartoon, graffiti, and
origami. There are also some hard examples from ImageNet
that standard models, e.g., ResNet [13], fail to classify. The
original dataset is split into the train set with 24000 sam-
ples and the test set with 6000 samples, and the number of
training and testing samples varies between classes.

--- PAGE 6 ---
Table 1: Benchmark Results on CIFAR100. The PET mod-
ules are inserted into the first 5 transformer blocks of the
standard ViT-B/16 pre-trained on the ImageNet21k dataset.
The “5, 10, 20” indicate the size of PET modules.
Approach PET Module A10(↑) ¯A10(↑)
Joint-FT - 92.00 ±0.18 -
Seq-FT - 77.61 ±0.37 85.82 ±0.86
BaselineAdapter5 82.30 ±1.20 88.18 ±0.31
Adapter10 81.76 ±1.21 87.84 ±0.49
LoRA5 83.24 ±1.54 88.48 ±0.31
LoRA10 82.55 ±1.61 88.35 ±0.48
Prefix10 84.49 ±0.30 89.34 ±0.59
Prefix20 84.44 ±0.75 89.46 ±0.42
L2P [47] Prompt 82.57 ±0.42 86.95 ±0.68
DualPrompt [46] Prefix20 84.27 ±0.41 88.92 ±0.78
ESN [45] Prompt 84.18 ±0.08 88.49 ±0.64
LAE (Ours)Adapter5 85.59±0.46 89.96±0.44
Adapter10 85.33 ±0.20 89.77 ±0.50
LoRA5 85.56 ±0.16 89.63 ±0.41
LoRA10 85.37 ±0.39 89.87 ±0.50
Prefix10 85.17 ±0.14 89.73 ±0.43
Prefix20 85.25 ±0.66 89.71 ±0.42
We follow prior works to split the dataset into 10 tasks,
and all tasks have the same number of classes, i.e., 10
for CIFAR100 and 20 for ImageNet-R. We evaluate the
model by the widely used incremental metrics: last in-
cremental accuracy ANand average incremental accuracy
¯AN=1
NPN
i=1Ai, where Nis the total number of tasks
(i.e., 10), and Aiis formally defined as:
Ai=1
|Dtest
1:i|X
(x,y)∈Dtest
1:i1(ˆy=y), (15)
where 1(·)is the indicator function that maps the boolean
value to {0,1},Dtest
1:iis the test set of all seen tasks so far,
ˆyandyare predicted and ground truth labels of input x.We
ran all experiments 3 times with different class orders and
report the mean and standard deviation of these 3 runs .
5.2. Implementation and Training Details
To make fair comparisons, we consider state-of-the-art
approaches [47, 46] based on pre-trained models like our
LAE and using the PyTorch code released by Jaeho Lee
to conduct experiments. The joint fine-tuning (Joint-FT)
and the naive sequential fine-tuning (Seq-FT) usually rec-
ognized as the upper and lower bounds of CIL are imple-
mented in our codebase, referring to the code of Jaeho Lee.
We also compare with recent work ESN [45], using its of-
ficial PyTorch code. We chose three types of represen-
tative PET modules and two sizes per type for our base-
line and LAE framework, where the size denotes the down-
projection dimension of the Adapter, the rank of LoRA, or
the length of the Prefix described in Sec. 3. We assume
https://github.com/JH-LEE-KRTable 2: Benchmark Results on ImageNet-R. The PET
modules are inserted into the first 5 transformer blocks
of the standard ViT-B/16 pre-trained on the ImageNet21k
dataset. The “5, 10, 20” indicate the size of PET modules.
Approach PET Module A10(↑) ¯A10(↑)
Joint-FT - 79.69 ±0.16 -
Seq-FT - 40.42 ±3.28 59.39 ±2.36
BaselineAdapter5 61.63 ±2.51 71.58 ±2.33
Adapter10 57.08 ±3.67 68.58 ±2.89
LoRA5 60.79 ±2.63 70.50 ±2.23
LoRA10 57.62 ±3.74 68.21 ±2.85
Prefix10 68.94 ±1.25 75.31 ±1.51
Prefix20 68.99 ±0.98 75.38 ±1.43
L2P [47] Prompt 63.91 ±1.60 69.27 ±2.25
DualPrompt [46] Prefix20 68.99 ±0.08 74.21 ±1.13
ESN [45] Prompt 62.61 ±0.96 68.58 ±1.64
LAE (Ours)Adapter5 72.66±0.63 78.91 ±0.89
Adapter10 72.45 ±0.81 79.07±0.88
LoRA5 72.00 ±0.75 78.33 ±0.96
LoRA10 71.83 ±0.57 78.24 ±0.90
Prefix10 71.85 ±0.66 77.44 ±1.12
Prefix20 72.05 ±0.66 77.55 ±1.00
that only a single PET module is attached to the pre-trained
model in the previous discussion for convenience, in prac-
tice, multiple PET modules are inserted into the Attention
blocks of Transformers or the convolution blocks of Con-
vNets in the shallow layers, following DualPrompt [46].
The training strategy of our baseline and LAE frame-
work is the same as DualPrompt, i.e., training the model
with Adam optimizer for 5 and 50 epochs, and constant
learning rate 0.03 and 0.005 based on batch size 256, for
CIFAR100 and ImageNet-R, respectively. The EMA algo-
rithm’s weight decay αdefined in Eq. (13) is empirically set
to 0.9999 in all experiments. The freezing epochs of PET
modules are set to 3 and 30 for CIFAR100 and ImageNet-R,
respectively. The data augmentation is consistent with that
used in model pre-training. We train Joint-FT and Seq-FT
with the recommended fine-tuning strategy of ViT [5], but
the number of training epochs is the same as ours. More
details can be found in the supplementary materials.
5.3. Benchmark Results
CIFAR100 benchmark results are present in Tab. 1. All ap-
proaches use the same ViT-B/16 [5] model pre-trained on
the ImageNet21k [40] dataset. The numerical suffix of the
PET module denotes its size ( i.e., down-projection dimen-
sion or length). L2P and DualPrompt are state-of-the-art ap-
proaches that adopt a pool to store Prompt or Prefix. How-
ever, the accuracy of their prompt selection gradually de-
clines with the increase in the number of learning tasks and
the prompts for different tasks appear homogeneous (see
supplementary). Therefore, our baseline is very naive but
achieves comparable performance to L2P and DualPrompt,
and our LAE framework with all 6 PET modules consis-
tently surpasses DualPrompt and ESN by about 1.5% in last

--- PAGE 7 ---
2 4 6 8 10
Number of Learned Tasks828486889092949698Incremental Accuracy
L2P
DualPrompt
ENS
LAE-Adapter5
LAE-LoRA5
LAE-Preﬁx20(a) CIFAR100
2 4 6 8 10
Number of Learned Tasks6468727680848892Incremental Accuracy
L2P
DualPrompt
ENS
LAE-Adapter5
LAE-LoRA5
LAE-Preﬁx20 (b) ImageNet-R
Figure 3: Task-by-Task Incremental Accuracy on two
benchmarks. The lines show the task-by-task evaluation re-
sults of L2P [47], DualPrompt [46], ENS [45], and our LAE
framework with different PET modules.
incremental accuracy A10. Although PET modules have
different performances in the baseline, they achieve better
and same level performance in our LAE, mainly due to the
calibration of adaptation speed. In particular, DualPrompt
has 3-10x more learnable parameters than our LAE.
ImageNet-R benchmark is more difficult than CIFAR100,
but it can better demonstrate the advantages of our LAE
framework. From the results shown in Tab. 2, our baseline
can only achieve comparable performance to DualPrompt
when using Prefix. This is because the Adapter and LoRA
adapt to a new task faster than Prefix, which is enlarged in
the ImageNet-R dataset but successfully addressed by the
adaption speed calibration of our LAE framework. Thus,
we can see that our LAE achieves more than 3.5% perfor-
mance improvement over DualPrompt in terms of the last
incremental accuracy A10, which is also scaled up com-
pared to the easier CIFAR100 dataset. We can also observe
that the size of the PET modules has little impact on perfor-
mance in our LAE framework, and our LAE is more robust
to the class order, while our baseline has a relatively large
variance between different class orders.
The real-world CL is an endless procedure, and the per-
formance of each learning phase is equally important to the
AI system. So, we also plot the task-by-task incremental
accuracy in Fig. 3a and 3b. We can observe that our LAE
with all three types of PET modules performs better than
L2P and DualPrompt at almost all learning phases. Our
LAE outperforms others by a wider margin in the 20-task
experiments presented in the supplementary material, high-
lighting its ability to handle long-term CL scenarios . In
the supplementary, we compare our LAE with the contem-
porary CODA-Prompt [44] approach on ImageNet-R and
DomainNet [35] datasets. The parameters and computation
comparison can also be found in the supplementary.
5.4. Ablation Study
Our LAE consists of three main novel designs, i.e., learn-
ing, accumulation, and ensemble, so we ablate on them and
report the results in Tab. 3. The first and last rows are
our baseline and LAE framework, respectively. The per-Table 3: Ablation study on three key designs of our LAE
framework: Learning (Lea.), Accumulation (Acc.), and
Ensemble (Ens.). The experiments are conducted on the
ImageNet-R dataset with Adapter10.
Lea. Ens. Acc. A10(↑) ¯A10(↑)
✗ ✗ ✗ 57.08±3.67 68.58 ±2.89
✗ ✓ ✗ 63.36±1.46 73.78 ±1.54
✗ ✓ ✓ 65.22±2.56 75.71 ±1.18
✓ ✗ ✗ 70.30±1.48 77.25 ±1.39
✓ ✓ ✗ 71.37±1.24 78.27 ±1.30
✓ ✗ ✓ 72.80±0.81 78.63 ±0.95
✓ ✓ ✓ 72.45±0.81 79.07±0.88
2 4 6 8 10
Number of Learned Tasks7075808590Incremental Accuracy
(a) Adapter5
2 4 6 8 10
Number of Learned Tasks7075808590Incremental Accuracy
Online
Oﬄine
Ensemble (b) Adapter10
Figure 4: Ablation on inference with Online Adapter, Of-
fline Adapter, and our Experts Ensemble. The three strate-
gies correspond to rows 4, 6, and 7 in Table 3.
formance drops the most when removing our learning with
calibrated speed, demonstrating that it contributes the most
to our LAE. Accumulation and Ensemble are also impor-
tant to our LAE, without them the last incremental accuracy
decreases by 2.15%. The sixth row indicates our LAE in-
ference with the Offline PET module only, whose A10is
even better than the inference by our expert ensemble. As
illustrated in Fig. 4, inference by expert ensemble performs
better than inference with the Online or Offline PET mod-
ule alone in the earlier learning phases. However, as the
number of learned tasks increases, the advantage of expert
ensemble over the Offline PET module gradually decreases,
partly due to the performance of the old tasks dominating
the overall performance. Nonetheless, inference with expert
ensemble yields more robust performance in most cases.
We also conducted ablation experiments on the calibra-
tion made to Prefix. We can see from Tab. 4 that both gra-
dient compensation and learnable scaling parameters indi-
vidually lead to significant improvements in performance.
Moreover, when used together, the performance gain is ap-
proximately equal to the sum of the gains achieved by using
each of them separately, indicating that their contributions
to the performance are independent of each other.
5.5. Attach Position of PET Modules
Our LAE inserts PET modules directly into the first 5
Transformer blocks, following the DualPrompt. As shown

--- PAGE 8 ---
Table 4: Ablation study on the calibration of Prefix: gra-
dient compensation and learnable scaling parameters. The
experiments are conducted on ImageNet-R with Prefix20.
Compensation Scale A10(↑) ¯A10(↑)
✗ ✗ 69.23±0.47 75.11 ±1.30
✓ ✗ 70.53±0.55 76.16 ±1.16
✗ ✓ 70.38±0.52 76.46 ±1.25
✓ ✓ 72.05±0.66 77.55 ±1.00
1-5 4-8 6-10 8-12
Attached Position (start-end)60.062.565.067.570.072.575.077.580.0Incremental AccuracyLast Avg
1 2 3 4 5 6 7 8 9 10
Index of the End Block68707274767880Incremental Accuracy
Last
Avg
Figure 5: Ablation on the attached position of PET mod-
ule (left) and the number of inserted blocks (right). There
are 12 transformer blocks in the pre-trained model, “1-5”
indicates attaching PET modules to the first 5 transformer
blocks. The PET modules are inserted into the transformer
starting from the first block in the right figure.
in Fig. 5 (left), inserting the PET modules in the shallow-
est position produces better results than inserting them in
deeper positions, which is consistent with the observation in
DualPrompt. Additionally, Fig. 5 (right) shows that insert-
ing PET modules in the first 6 Transformer blocks achieves
the best performance while inserting them in the first 5
Transformer blocks ( i.e., the default setting of our LAE)
also results in nearly the same performance.
5.6. Results on Transformer variant and ConvNet
Prefix and Prompt are not flexible enough to be applied
to ConvNets and Transformer variants, while our LAE is
model architecture generalizable due to the ability to lever-
age various PET modules. We choose the Swin Trans-
former [31] and ConvNeXt [32] to validate our LAE.
Swin Transformer is a representative window-based Vi-
sion Transformer, but L2P and DualPrompt cannot be di-
rectly applied to it because the inserted tokens may dis-
rupt the proper division of the windows. Therefore, we
only compare our LAE with our baseline when using Swin
Transformer, and report the results in Tab. 5. We can see
that our LAE achieves better performance with Swin-B than
with ViT-B/16, which is mainly due to the superior per-
formance of Swin-B. Moreover, our LAE significantly im-
proves the performance of our baseline on all two datasets.
ConvNeXt is a modern ConvNet for the 2020s that outper-
forms Swin Transformer by incorporating several novel de-
signs into the standard ResNet [13]. Similarly, we compareTable 5: The comparison between our LAE framework with
our baseline on two datasets using the Swin-B model pre-
trained on the ImageNet22k dataset. Adapters are inserted
in the shallower 10 of 24 transformer blocks.
Approach Dataset A10(↑) ¯A10(↑)
BaselineCIFAR10085.85±0.69 90.36 ±0.67
LAE (Ours) 86.52±0.38 90.58±0.61
BaselineImageNetR71.81±1.09 78.91 ±1.41
LAE (Ours) 73.38±0.70 80.01±1.51
Table 6: The comparison between our LAE framework with
our baseline on two datasets using the ConvNeXt-B model
pre-trained on the ImageNet22k dataset. Adapters are in-
serted in the shallower 15 of 36 convolution blocks.
Approach Dataset A10(↑) ¯A10(↑)
BaselineCIFAR10086.40±0.07 91.00 ±0.31
LAE (Ours) 87.01±0.28 91.18±0.34
BaselineImageNetR76.35±1.36 82.60 ±1.79
LAE (Ours) 78.38±0.80 83.95±1.16
our LAE with the baseline in Tab. 6. The Adapter’s down
and up projections are implemented using 1×1convolution
layers. Both our baseline and LAE achieve significantly
better performance using ConvNeXt-B compared to using
ViT-B/16 and Swin-B, highlighting LAE’s strengths beyond
being limited to Transformers. Our LAE consistently im-
proves the performance of our baseline on both datasets.
6. Conclusion
This paper thoroughly studied the novel Continual
Learning (CL) paradigm that starts with a pre-trained model
and continuously adapts the model to arriving tasks uti-
lizing general Parameter-Efficient Tuning (PET) methods.
We constructed a naive baseline that achieved performance
comparable to the prior state-of-the-art approaches. We pro-
posed the Learning-Accumulation-Ensemble (LAE) frame-
work by introducing three novel designs to the baseline.
Our LAE can convert any PET method into an efficient CL
approach without accessing any old data. We conducted
extensive experiments to validate the effectiveness of our
LAE, and the results demonstrated that our LAE signifi-
cantly outperforms the previous state-of-the-art approaches.
Limitations . There are still some limitations that need to be
improved in the future, such as how to accumulate knowl-
edge more efficiently and better ensemble expert models.
Moreover, due to the lack of large-scale datasets that do not
overlap with the pre-training dataset, our LAE has not been
verified in CL scenarios with a larger number of tasks.
Overall, this paper provides a new solution for Memory-
Free CL and offers some theoretical and experimental ref-
erences for future research on this novel CL paradigm.

--- PAGE 9 ---
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
2018.
[2] Lama Alssum, Juan Leon Alcazar, Merey Ramazanova,
Chen Zhao, and Bernard Ghanem. Just a glimpse: Rethink-
ing temporal information for video continual learning. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) Workshop , 2023.
[3] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
Yibing Song, Jue Wang, and Ping Luo. Adaptformer:
Adapting vision transformers for scalable visual recognition.
CoRR , 2022.
[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: pre-training of deep bidirectional trans-
formers for language understanding. In Proceedings of the
Annual Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL) , 2019.
[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image
is worth 16x16 words: Transformers for image recognition
at scale. In Proceedings of the International Conference on
Learning Representations (ICLR) , 2021.
[6] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas
Robert, and Eduardo Valle. Podnet: Pooled outputs distil-
lation for small-tasks incremental learning. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
2020.
[7] Chin-Lun Fu, Zih-Ching Chen, Yun-Ru Lee, and Hung-yi
Lee. Adapterbias: Parameter-efficient token-dependent rep-
resentation shift for adapters in NLP tasks. In Proceedings of
the Annual Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL) , 2022.
[8] Qiankun Gao, Chen Zhao, Bernard Ghanem, and Jian Zhang.
R-DFCIL: relation-guided representation learning for data-
free class incremental learning. In Proceedings of the Euro-
pean Conference on Computer Vision (ECCV) , 2022.
[9] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville,
and Yoshua Bengio. An empirical investigation of catas-
trophic forgetting in gradient-based neural networks. arXiv
preprint arXiv:1312.6211 , 2013.
[10] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. Towards a unified view
of parameter-efficient transfer learning. In Proceedings of
the International Conference on Learning Representations
(ICLR) , 2022.
[11] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross B. Girshick. Masked autoencoders are
scalable vision learners. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2022.
[12] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and
Ross B. Girshick. Momentum contrast for unsupervisedvisual representation learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2020.
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , 2016.
[14] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-
vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,
Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt,
and Justin Gilmer. The many faces of robustness: A critical
analysis of out-of-distribution generalization. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , 2021.
[15] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and
Dahua Lin. Learning a unified classifier incrementally via
rebalancing. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , 2019.
[16] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for NLP. In Proceedings of the International Con-
ference on Machine Learning (ICML) , 2019.
[17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. In Pro-
ceedings of the International Conference on Learning Rep-
resentations (ICLR) , 2022.
[18] Steven C. Y . Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-
Hung Chen, Yi-Ming Chan, and Chu-Song Chen. Compact-
ing, picking and growing for unforgetting continual learning.
InProceedings of the Advances in Neural Information Pro-
cessing Systems (NeurIPS) , 2019.
[19] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge J. Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In Proceedings of the European Confer-
ence on Computer Vision (ECCV) , 2022.
[20] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku-
maran, and Raia Hadsell. Overcoming catastrophic for-
getting in neural networks. Proceedings of the National
Academy of Sciences (PNAS) , 2017.
[21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. Technical Report , 2009.
[22] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones,
Tengyu Ma, and Percy Liang. Fine-tuning can distort pre-
trained features and underperform out-of-distribution. In
Proceedings of the International Conference on Learning
Representations (ICLR) . OpenReview.net, 2022.
[23] Dharshan Kumaran, Demis Hassabis, and James L McClel-
land. What learning systems do intelligent agents need?
complementary learning systems theory updated. Trends in
cognitive sciences , 2016.
[24] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and
Fujie Huang. A tutorial on energy-based learning. Predicting
structured data , 2006.

--- PAGE 10 ---
[25] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
of scale for parameter-efficient prompt tuning. In Proceed-
ings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP) , 2021.
[26] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and
Caiming Xiong. Learn to grow: A continual structure learn-
ing framework for overcoming catastrophic forgetting. In
Proceedings of the International Conference on Machine
Learning (ICML) , 2019.
[27] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz-
ing continuous prompts for generation. In Proceedings of
the Joint Conference of the 59th Annual Meeting of the As-
sociation for Computational Linguistics and the 11th Inter-
national Joint Conference on Natural Language Processing
(ACL-IJCNLP 2021) , 2021.
[28] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence (TPAMI) , 2017.
[29] Weitang Liu, Xiaoyun Wang, John D. Owens, and Yixuan Li.
Energy-based out-of-distribution detection. In Proceedings
of the Advances in Neural Information Processing Systems
(NeurIPS) , 2020.
[30] Yaoyao Liu, Bernt Schiele, and Qianru Sun. Adaptive aggre-
gation networks for class-incremental learning. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , 2021.
[31] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , 2021.
[32] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2022.
[33] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian
Ruder. Compacter: Efficient low-rank hypercomplex adapter
layers. In Proceedings of the Advances in Neural Informa-
tion Processing Systems (NeurIPS) , 2021.
[34] James L McClelland, Bruce L McNaughton, and Randall C
O’Reilly. Why there are complementary learning systems in
the hippocampus and neocortex: insights from the successes
and failures of connectionist models of learning and memory.
Psychological review , 1995.
[35] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate
Saenko, and Bo Wang. Moment matching for multi-source
domain adaptation. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV) , pages
1406–1415, 2019.
[36] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. Improving language understanding by gen-
erative pre-training. 2018.
[37] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
Learning multiple visual domains with residual adapters. In
Proceedings of the Advances in Neural Information Process-
ing Systems (NeurIPS) , 2017.
[38] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert. icarl: Incremental classifierand representation learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2017.
[39] Anthony V . Robins. Catastrophic forgetting, rehearsal and
pseudorehearsal. Connect. Sci. , 1995.
[40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International Journal of
Computer Vision (IJCV) , 2015.
[41] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-
van Pascanu, and Raia Hadsell. Progressive neural networks.
CoRR , 2016.
[42] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.
Continual learning with deep generative replay. In Proceed-
ings of the Advances in Neural Information Processing Sys-
tems (NeurIPS) , 2017.
[43] James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen,
Hongxia Jin, and Zsolt Kira. Always be dreaming: A new ap-
proach for data-free class-incremental learning. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV) , 2021.
[44] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola
Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar
Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Contin-
ual decomposed attention-based prompting for rehearsal-free
continual learning. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2023.
[45] Yabin Wang, Zhiheng Ma, Zhiwu Huang, Yaowei Wang,
Zhou Su, and Xiaopeng Hong. Isolation and impartial ag-
gregation: A paradigm of incremental learning without in-
terference. 2023.
[46] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,
Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent
Perot, Jennifer G. Dy, and Tomas Pfister. Dualprompt: Com-
plementary prompting for rehearsal-free continual learning.
InProceedings of the European Conference on Computer Vi-
sion (ECCV) , 2022.
[47] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,
Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jen-
nifer G. Dy, and Tomas Pfister. Learning to prompt for con-
tinual learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , 2022.
[48] Teng Xi, Yifan Sun, Deli Yu, Bi Li, Nan Peng, Gang Zhang,
Xinyu Zhang, Zhigang Wang, Jinwen Chen, Jian Wang,
Lufei Liu, Haocheng Feng, Junyu Han, Jingtuo Liu, Errui
Ding, and Jingdong Wang. UFO: unified feature optimiza-
tion. In Proceedings of the European Conference on Com-
puter Vision (ECCV) , 2022.
[49] Hongxu Yin, Pavlo Molchanov, Jose M. Alvarez, Zhizhong
Li, Arun Mallya, Derek Hoiem, Niraj K. Jha, and Jan Kautz.
Dreaming to distill: Data-free knowledge transfer via deep-
inversion. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2020.
[50] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju
Hwang. Lifelong learning with dynamically expandable net-

--- PAGE 11 ---
works. In Proceedings of the International Conference on
Learning Representations (ICLR) , 2018.
[51] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bit-
fit: Simple parameter-efficient fine-tuning for transformer-
based masked language-models. In Proceedings of the An-
nual Meeting of the Association for Computational Linguis-
tics (ACL) , 2022.
[52] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In Proceedings of
the International Conference on Machine Learning (ICML) ,
2017.
[53] Jeffrey O. Zhang, Alexander Sax, Amir Zamir, Leonidas J.
Guibas, and Jitendra Malik. Side-tuning: A baseline for net-
work adaptation via additive side networks. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
2020.

--- PAGE 12 ---
Supplementary Materials
In the supplementary materials, we further validate the
proposed LAE framework by providing the following:
• Section A: Additional Experimental Details.
• Section B: Additional Experimental Results.
• Section C: Investigation on Prompt Learning and Se-
lection from Pool in Prompt-Pool-based Approaches.
A. Additional Experimental Details
Data Augmentation . We adopt a very simple data augmen-
tation strategy for training, following L2P [47] and Dual-
Prompt [46]. 1)Images are randomly resized to 224×224
using the bilinear interpolation algorithm. 2)Images are
normalized by min-max (for ViT [5]) or standard deviation
(for Swin Transformer [31] and ConvNeXt [32]) normaliza-
tion. 3)Images are randomly flipped from horizontal. Dur-
ing inference, images are resized to 256×256and cropped
to224×224from central. All other approaches take the
same data augmentation strategy as ours for fair compar-
isons. The PyTorch-like code is present in Algorithm 1.
Hyper-Parameter . Our LAE introduced two additional
hyper-parameters, i.e., the weight decay αof the Exponen-
tial Moving Average (EMA) algorithm and freezing epochs
of the online Parameter-Efficient Tuning (PET) module. We
did not intentionally search for these parameters and set α
to a value very close to 1, such as the default value 0.9999
we used. The number of freezing epochs can be determined
by the change in loss after freezing the online PET module
and is typically set to the value where the loss no longer
decreases. We set this value to 3 for CIFAR100 and scaled
it proportionally for ImageNet-R, on all of which our LAE
achieved superior performance than other competitors.
Training, Inference and Evaluation . The training and in-
ference of our LAE framework are very easy to implement,
the PyTorch-like pseudocode is provided in Algorithms 2.
It is important to note that our evaluation metric A10(Equa-
tion 15 in the paper) is slightly different from the following
metric used by original L2P and DualPrompt:
A10=1
1010X
j=11
|Dtest
j|X
(x,y)∈Dtest
j1(ˆy=y), (XVI)
where Dtest
jis the test set of the jthtask. We train and eval-
uate on three different class orders, while L2P, DualPrompt,
and ESN [45] only evaluate on one class order in their orig-
inal papers. Additionally, ESN uses a different pre-trained
checkpoint from L2P and DualPrompt, but we correct thisTable VII: 20-Task Benchmark Results on CIFAR100. The
PET modules are inserted into the first 5 transformer blocks
of the standard ViT-B/16 pre-trained on the ImageNet21k
dataset. The “5, 10, 20” indicate the size of PET modules.
Approach PET Module A20(↑) ¯A20(↑)
L2P [47] Prompt 80.10 ±0.72 85.29 ±0.50
DualPrompt [46] Prefix20 82.02 ±0.32 89.50 ±0.11
ESN [45] Prompt 80.56 ±0.94 90.47 ±1.19
LAE (Ours)Adapter5 83.89 ±0.60 92.35±0.55
Adapter10 83.81 ±0.35 92.32 ±0.57
LoRA5 83.92 ±0.36 92.15 ±0.47
LoRA10 83.35 ±0.20 91.71 ±0.88
Prefix10 83.82 ±0.18 92.07 ±0.72
Prefix20 83.93±0.28 92.21 ±0.53
Table VIII: 20-Task Benchmark Results on ImageNet-R.
The PET modules are inserted into the first 5 transformer
blocks of the standard ViT-B/16 pre-trained on the Ima-
geNet21k dataset. The “5, 10, 20” indicate the size of PET
modules.
Approach PET Module A20(↑) ¯A20(↑)
L2P [47] Prompt 59.85 ±1.38 66.33 ±2.46
DualPrompt [46] Prefix20 66.61 ±0.24 76.94 ±1.39
ESN [45] Prompt 58.65 ±0.83 70.94 ±1.88
LAE (Ours)Adapter5 69.66 ±1.16 81.69 ±1.00
Adapter10 69.19 ±1.25 81.78±0.77
LoRA5 68.91 ±1.40 80.99 ±1.17
LoRA10 69.07 ±1.49 81.12 ±1.09
Prefix10 69.67±0.86 79.97 ±0.97
Prefix20 69.34 ±0.84 79.90 ±1.08
issue when using its code. The above differences lead to
slightly different experimental results reported in their orig-
inal papers from the data reported by us.
B. Additional Experimental Results
20-Task Benchmark Results . To further validate the ef-
ficacy of our LAE in longer-term Continual Learning sce-
narios, we split the CIFAR100 [21] and ImageNet-R [40]
datasets into 20 tasks, each containing 5 (for CIFAR100)
or 10 (for ImageNet-R) classes. We then conducted experi-
ments and reported the mean and standard deviation of three
runs in different class orders in Tables VII and VIII. Similar
to the 10-task experiments in the paper, we plot the task-by-
task evaluation results in Figure VIa and VIb for the 20-task
experiments on CIFAR100 and ImageNet-R. From these ta-
bles and figures, we can observe a wider performance gap
between our LAE and other competitors compared to the
10-task experiments, suggesting that our LAE is more effec-

--- PAGE 13 ---
Algorithm 1 Data Augmentation Code (PyTorch-like)
def build_train_transform(model):
transforms = [T.RandomResizedCrop(224), T.RandomHorizontalFlip(), T.ToTensor()]
if not isinstance(model, VisionTransformer):
transforms.append(T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD))
return T.Compose(transforms)
def build_inference_transform(model):
transforms = [T.Resize(256), T.CenterCrop(224), T.ToTensor()]
if not isinstance(model, VisionTransformer):
transforms.append(T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD))
return T.Compose(transforms)
Algorithm 2 Training and Inference Code (PyTorch-like)
# model: the pre-trained model; pet_on: online PET module; pet_off: offline PET module;
def train(model, pet_on, pet_off, dataloader, optimizer, task_id, alpha):
model = attach(model, pet_on)
for e in range(MAX_EPOCHS):
if e == 0 and not_the_first_task(task_id):
freeze(pet_on)
elif e == NUM_FREEZING_EPOCHS:
unfreeze(pet_on)
for input, target in dataloader:
pred = mask(model(input), task_id) # Eq. (8) in the paper
loss = cross_entropy(pred, target) # Eq. (8) in the paper
optimizer.zero_grad()
loss.backward()
optimizer.step()
ema_update(pet_off, pet_on, alpha) # Eq. (13) in the paper
def inference(model, pet_on, pet_off, input):
pred_on, pred_off = attach(model, pet_on)(input), attach(model, pet_off)(input)
pred = max(softmax(pred_on, dim=-1), softmax(pref_off, dim=-1)) # Eq. (14) in the paper
return argmax(pred)
Table IX: Comparison with CODA-Prompt on 5-task Do-
mainNet Benchmark, 5- and 10-task ImageNet-R bench-
marks. “ AN” and “ FN” are last incremental accuracy and
last average forgetting for N-task benchmarks respectively.
Approach Joint-FT CODA-Prompt LAE (Prefix10)
DomainNet 5-taskA5(↑) 74.91 67.11 68.37
F5(↓) - 13.79 8.33
ImageNet-R5-taskA5(↑) 81.08 75.32 76.69
F5(↓) - 6.09 6.17
10-taskA10(↑) 81.08 74.31 74.43
F10(↓) - 5.63 5.22
tive at mitigating forgetting and achieving a better stability-
plasticity balance in longer-term Continual Learning.
Comparison with CODA-Prompt . The contemporary
CODA-Prompt [44] approach demonstrates remarkable
performance. Nevertheless, upon reviewing the authors’
released code, we identified three potential sources of un-
fair comparison: 1) A distinct ImageNet-R train-test split
in contrast to DualPrompt. 2) The model is pretrained on
ImageNet-21k and subsequently fine-tuned on ImageNet-
1K. 3) Varied training strategies, such as the number of
epochs and learning rates. Our initial experiments reveal
that when utilizing DualPrompt’s train-test split, CODA-
Prompt consistently underperforms our LAE. To ensure aTable X: The sensitiveness w.r.t. EMA’s weight decay α.
α 0.999 0.9999 0.99999
A10(↑) 71.40 ±1.02 72.66 ±0.63 72.58 ±0.40
¯A10(↑) 78.04 ±1.03 78.91 ±0.89 78.67 ±0.94
fair evaluation, we adopt CODA-Prompt’s settings for our
experiments and extend our assessment to the Domain-
Net [35] dataset. All results are showcased in Table IX,
where we present average forgetting rates instead of aver-
age incremental accuracy.
Sensitive Analysis on EMA’s Weight Decay . Weight de-
cayαplays an important role in the knowledge accumula-
tion of the offline PET module. A small value can lead to
the integration of too much unstable new knowledge during
the learning process, while a large value can result in the
offline PET module being unable to effectively absorb new
knowledge. In all of our experiments in the paper, we set the
weight decay of EMA to 0.9999, which is the default value
in the timm library. Our experimental results in Table X
demonstrate that this value yields the best performance.
Memory and computation complexity . Our LAE requires
two forward passes (one with θpetoffand the other with
θpeton) per inference sample, yielding computational costs
https://github.com/huggingface/pytorch-image-models

--- PAGE 14 ---
3 6 9 12 15 18
Number of Learned Tasks81848790939699Incremental Accuracy
L2P
DualPrompt
ENS
LAE-Adapter5
LAE-LoRA5
LAE-Preﬁx20(a) CIFAR100
3 6 9 12 15 18
Number of Learned Tasks6065707580859095Incremental Accuracy
L2P
DualPrompt
ENS
LAE-Adapter5
LAE-LoRA5
LAE-Preﬁx20 (b) ImageNet-R
Figure VI: Task-by-Task Incremental Accuracy on two 20-task benchmarks. The lines illustrate the task-by-task evaluation
results of L2P [47], DualPrompt [46], ENS [45], and our LAE framework with different PET modules.
Table XI: The statistics of introduced parameters by ap-
proaches on 10-task benchmarks. “A10”, “L10” and “P20”
indicate Adapter10, LoRA10 and Prefix20 respectively.
Approach DualPrompt LAE (A10) LAE (L10) LAE (P20)
#Param. (M) 1.03 0.15 0.29 0.29
on par with L2P, DualPrompt, and the contemporary ap-
proach CODA-Prompt. Additionally, due to the constant
number of parameters maintained across all tasks, LAE in-
troduces fewer new parameters, as illustrated in Table XI.
C. Prompt Learning and Selection from Pool
L2P [47] and DualPrompt [46] are two representative ap-
proaches that leverage prompt tuning [25] to address the
problem of Continual Learning. L2P first proposes to use
a pool to store prompts shared across tasks, where a set of
prompts that match the sample are selected from the pool
to predict the sample’s label. In contrast, DualPrompt di-
rectly learns a set of task-specific E-Prompts for each task
and stores them in the pool. During inference, the best-
matched prompts ( i.e., the prompts learned for the task that
the sample belongs to) are selected for the given sample.
The performance of these approaches is influenced by
two key factors. 1)The ability to learn optimal prompts for
each task is crucial for achieving better plasticity, i.e., the
ability to learn new knowledge. Better performance can be
achieved only by sufficiently learning new knowledge while
retaining as much previous knowledge as possible. 2)the
ability to accurately select the best-matched prompts for the
inference sample is more critical. Because even if optimal
prompts are learned for each task, inference using the wrong
prompts can still result in poor prediction results. Follow-
2 4 6 8 10
Number of Learned Tasks5060708090100E-Prompt AccuracyFigure VII: The E-Prompts selection accuracy of Dual-
Prompt on the test set of ImageNet-R.
ing, we take DualPrompt as an example to investigate these
two abilities of prompt-pool-based approaches.
To begin with, we assume that DualPrompt can learn
the optimal E-prompts for each task. We then evaluate
whether it can accurately select the right E-prompts during
inference. As shown in Figure VII, we observe that the E-
prompts selected by DualPrompt are completely accurate
after learning the first task. However, as the number of
learned tasks increases, the accuracy of the prompt selection
gradually decreases. By the time the 10th task is learned,
the selection accuracy drops to below 50%. Therefore, we
conclude that if DualPrompt cannot address this issue, it is
difficult to apply it to longer-term Continual Learning sce-
narios.
In addition, according to Figure 3 in our paper, we ob-
serve that DualPrompt performs worse than our LAE when
learning the first task, regardless of whether our LAE uses
Adapter [16] with fewer parameters, the LoRA [17] with the
equivalent number of parameters, or Prefix [27] ( i.e., Du-
alPrompt’s E-Prompt) with slightly more parameters than
DualPrompt. This indicates that Prompt/Prefix may not be
as effective as Adapter and LoRA in learning new knowl-
edge on these two datasets, as well as DualPrompt could

--- PAGE 15 ---
Table XII: Evaluation results on all tasks using 10 sets of task-specific E-Prompts. “#E-Prompts” denotes the index of the
E-Prompts, e.g., “1” indicates evaluation using the first task’s E-Prompts.
#E-Prompts 1 2 3 4 5 6 7 8 9 10
A10(↑) 63.78 66.93 67.57 67.88 68.48 68.38 68.80 68.88 68.87 68.47
¯A10(↑) 69.11 72.54 72.60 72.63 72.69 72.68 72.72 72.73 72.73 72.69
Table XIII: Evaluation results on each task using 10 sets of task-specific E-Prompts. “#E-Prompts” denotes the index of the
E-Prompts, e.g., “1” indicates evaluation using the first task’s E-Prompts.
#E-Prompts 1 2 3 4 5 6 7 8 9 10
Task 1 69.64 73.27 72.77 72.94 73.76 73.10 72.61 72.94 72.44 72.28
Task 2 67.23 71.73 71.03 70.89 71.87 71.03 71.31 71.31 71.59 70.61
Task 3 63.93 67.16 70.90 71.89 70.40 69.90 70.65 70.40 69.40 69.90
Task 4 54.61 60.17 64.52 68.35 67.48 67.30 64.52 64.35 63.65 62.96
Task 5 61.01 64.52 65.64 65.36 68.16 68.16 67.04 65.50 65.22 65.78
Task 6 59.33 63.73 63.73 65.14 66.37 67.43 67.43 66.55 66.20 66.20
Task 7 57.02 58.93 58.41 57.71 57.89 57.71 61.18 62.22 62.39 60.49
Task 8 61.47 61.92 62.14 61.03 61.47 62.81 65.26 68.37 67.26 65.03
Task 9 74.85 76.50 74.70 75.15 75.30 74.70 77.25 77.10 78.89 77.40
Task 10 65.53 67.72 69.36 68.40 68.95 68.95 68.81 69.08 69.63 71.41
not learn the optimal E-Prompts for the first task because
they did not calibrate the Prefix like our LAE. This suggests
that it is necessary to explore different Parameter-Efficient
Tuning (PET) methods and calibrate PET modules.
Moreover, our naive baseline only uses one set of Pre-
fixes, while DualPrompt learns a set of Prefixes for each
task, totaling 10 sets, yet they achieve similar performance.
We evaluate all 10 tasks using the 10 sets of E-Prompts
learned by DualPrompt separately, and the results in Ta-
ble XII show that the differences in the last and average
incremental accuracy using the 2nd-10th sets of E-Prompts
are very small. Table XIII presents the prediction results
of each task using each set of E-Prompts, for most tasks,
the prediction results using the 2nd-10th sets of E-Prompts
are very close. These analyses reveal that from the learning
of the second task, task-specific E-Prompts tend to become
homogeneous. According to our analysis in the paper, an
important reason for this is that the adaptation speed of the
Prefix is much slower than classifiers and other PET mod-
ules ( i.e., Adapter [16] and LoRA [17]).
