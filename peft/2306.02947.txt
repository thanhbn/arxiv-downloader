# 2306.02947.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2306.02947.pdf
# File size: 1392744 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Continual Learning with Pretrained Backbones
by Tuning in the Input Space
Simone Marullo∗†
∗DINFO
University of Florence
Florence, Italy
simone.marullo@unifi.itMatteo Tiezzi†
†DIISM
University of Siena
Siena, Italy
matteo.tiezzi@unisi.itMarco Gori†‡
‡MAASAI
Universit `e Cˆote d’Azur
Nice, France
marco.gori@unisi.it
Stefano Melacci†
†DIISM
University of Siena
Siena, Italy
mela@diism.unisi.itTinne Tuytelaars§
§ESAT
KU Leuven
Leuven, Belgium
tinne.tuytelaars@esat.kuleuven.be
Abstract —The intrinsic difficulty in adapting deep learning
models to non-stationary environments limits the applicability
of neural networks to real-world tasks. This issue is critical
in practical supervised learning settings, such as the ones in
which a pre-trained model computes projections toward a latent
space where different task predictors are sequentially learned
over time. As a matter of fact, incrementally fine-tuning the
whole model to better adapt to new tasks usually results in
catastrophic forgetting, with decreasing performance over the
past experiences and losing valuable knowledge from the pre-
training stage. In this paper, we propose a novel strategy to
make the fine-tuning procedure more effective, by avoiding to
update the pre-trained part of the network and learning not only
the usual classification head, but also a set of newly-introduced
learnable parameters that are responsible for transforming the
input data. This process allows the network to effectively leverage
the pre-training knowledge and find a good trade-off between
plasticity and stability with modest computational efforts, thus
especially suitable for on-the-edge settings. Our experiments on
four image classification problems in a continual learning setting
confirm the quality of the proposed approach when compared to
several fine-tuning procedures and to popular continual learning
methods.
Index Terms —Continual learning, neural networks, prompt
models, fine-tuning, input tuning, friendly training.
I. I NTRODUCTION
The outstanding performance achieved by Machine Learn-
ing solutions in a vast variety of fields [1] and well-defined
tasks [2] is usually restricted to a very specific setting, where
This work was partly supported by the PRIN 2017 project RexLearn,
funded by the Italian Ministry of Education, University and Research (grant
no. 2017TWNMH2). This work was also partially supported by TAILOR
and by HumanE-AI-Net, projects funded by EU Horizon 2020 research and
innovation programme under GA No 952215 and No 952026, respectively.
Accepted for publication at the IEEE International Joint Conference on
Neural Networks (IJCNN) 2023 (DOI: TBA). ©2023 IEEE. Personal use of
this material is permitted. Permission from IEEE must be obtained for all
other uses, in any current or future media, including reprinting/republishing
this material for advertising or promotional purposes, creating new collective
works, for resale or redistribution to servers or lists, or reuse of any
copyrighted component of this work in other works.it’s assumed that all training data is available from the start and
sampled from a static distribution in an independent manner
(i.i.d.). This scenario does not contemplate the case in which
neural models are progressively adapted to novel data that is
sampled over time from non-stationary distributions. Recently,
the limitations implied by the i.i.d. assumption have gained
wider attention and novel models that are designed to learn
over time started to emerge [3]. Such models are loosely
inspired by human cognition, which typically works in an
incremental fashion with new notions being learnt in a fruitful
relation with previously acquired knowledge, so that both old
and new skills are often refined through their interaction [4].
This is in stark contrast with the default behavior of neural
networks, where a huge performance drop on old data is
typically noticed when adapting weights on novel and never-
seen-before datasets presented over time. This issue, generally
referred to as catastrophic forgetting [5], has been known since
the early connectionist movement [4] and is still far from
being solved in a general and satisfying way. To address these
issues, Continual Learning (CL) develops methods suitable for
problems in which training data are presented over time and
potentially in a lifelong manner.
Intelligent edge devices, including sensors, actuators,
robotic platforms, etc., with modest computational resources,
are becoming ubiquitous and there is a growing need of Ma-
chine Learning-driven processing capabilities for perception,
understanding and personalization in the Internet of Things.
Indeed, such devices are capable of acquiring continuous data
streams [6], that could be processed with CL-based solutions
running on the edge devices themselves. Several challenges
must be faced when deploying CL methods to edge devices.
First of all, typical state-of-the-art neural architectures (e.g.,
Transformers [7]) with many encoding layers might be of lim-
ited applicability due to scarcity of memory and computational
capabilities. While offloading to the cloud might be a simple
workaround, it raises privacy concerns and complexity issues.arXiv:2306.02947v2  [cs.LG]  8 Jun 2023

--- PAGE 2 ---
Moreover, most of the state-of-the-art CL methods involve
rehearsal procedures [3], i.e., re-visiting some exemplars of
past concepts in order to “refresh” the knowledge of the
model. This opens to new issues concerning storage capacity
and, again, privacy [8], given that it introduces the need of
long-term storage of training data. Methods based on latent
replay (rather than input replay) achieve better compression
and obfuscate potentially private data, but storage issues are
not solved and privacy preservation may be totally unreliable
[9]. Another issue with the current CL research concerns the
limited scale of the tackled tasks, with only limited efforts
spent in evaluating the performance of CL methods in realistic
applications.
In the context of edge devices with limited computational
capabilities, it seems reasonable to start from networks pre-
trained on large data collections, since they are powerful tools
to compute informed latent representations and they allow to
reduce training efforts. However, their embedded knowledge
might be significantly lost when progressively fine-tuning
the network to novel domains in a CL fashion (especially
when rehearsal is not performed [10]). An alternative to fine-
tuning has recently become popular in the Machine Learning
community, i.e., learning parameters that affect the model
input with the goal of conditioning the network rather than
changing its internal weights. These Prompt Tuning models
[11]–[13] were conceived to foster the exploitation of very-
large-scale networks, in order to effectively leverage their
capabilities in specific downstream tasks. However, most of
the work in this line focuses on large Transformer models
and the connection with CL is not deeply investigated [14].
Motivated by these considerations, in this paper we propose
and investigate the appropriateness of different tuning options
when facing a CL problem based on a pre-trained network,
frequently referred to as backbone , focusing on methods that
require lower computational efforts and no-rehearsal, well
suited for edge devices. In particular, we propose Input Tuning
(IT), an alternative form of Prompt Tuning, in order to
efficiently adapt the model to new data and achieve a good
trade-off between plasticity and stability. We provide an ex-
perimental analysis conducted on different datasets available in
the related literature, comparing several fine-tuning procedures
and well-known continual learning methods. The contributions
of this paper are the following ones: (1) we propose the
adoption of Input Tuning procedures to better leverage pre-
trained backbones in CL; (2) we experimentally evaluate the
impact of fine-tuning on common CL benchmarks, providing
reference results; (3) we investigate and show the benefits of
Input Tuning in a continual setting, with edge-friendly neural
architectures, both for small and large domain shifts.
This paper is organized as follows. Related work is pre-
sented in Section II, while Input Tuning is described in
Section III. Experiments are in Section IV and Section V,
while conclusions and suggestions for future work are drawn
in Section VI.II. R ELATED WORKS
The wide availability of pre-trained models offers several
opportunities to transfer their knowledge to specific down-
stream tasks. The simplest approach consists in fine-tuning the
models on the novel task data. This is typically demanding
in terms of resources, especially in the case of large-scale
models, due to the memory occupation (gradients as well as
activations) and the operations in the weight update routine.
The generic notion of pre-training has been shown to im-
plicitly make some CL problems easier [15], following the
intuition that it is more likely to end up in more informed
solutions compared to randomly-initialized models, due to
the skills learned in the pre-training stage. Of course, an
inappropriate fine-tuning procedure might yield a model too
strongly focused on the novel task, losing the advantage from
the previously learned knowledge. However, while the limits
of transferring pre-trained models to downstream tasks were
recently studied [16], when it comes to specifically studying or
evaluating the concrete impact of adapting pre-trained models
to a CL context, the scientific literature is relatively scarce
[10]. Ramasesh et al. [17] have pointed out that resistance
to forgetting consistently scales with the network size when
employing pre-trained models. Still, several questions remain
unanswered, especially concerning smaller-scale models in
computationally-restricted environments, as the ones we study
in this paper. Another line of work [18], [19] replaces the pre-
training step with a continual procedure, especially in the self-
supervised setting. While this approach paves the way for the
exploitation of decentralized and streaming data in a variety of
contexts, it is not focused on the practical scenario in which
downstream tasks are learnt in an incremental fashion.
Another research topic related to this paper consists of
Prompt Tuning models [11], [12], [20], that were originally
conceived in Natural Language Processing (NLP), and that
open a new perspective on how to adapt a pre-trained model
to a novel environment, related to the one of the pre-training
stage. Prompt Tuning models learn parameters that are actually
part of the input stage, in order to discover the proper way to
condition the model, rather than changing the values of its
internal weights or learning additional internal parameters (as
in the case of Adapters [21]). Such technique has been mostly
applied to Transformer models, both for language tasks and
vision tasks. In this paper we focus on a specific type of tuning
that is not restricted to Transformer models, and that we study
in the the context of CL.
Tweaking the input space to alter the behavior of neural
networks has already been investigated for a variety of pur-
poses. With this strategy, researchers successfully managed to
re-program a trained model to perform on a very different task
[22], while others [23], [24] developed a curriculum-learning
inspired methodology to effectively learn from a continuous
set of learning problems with gradually increasing complexity.
Recently, this idea has been studied in the context of transfer
learning for image classification [13], and it has been shown
that learning parameters to transform the input data is a

--- PAGE 3 ---
competitive approach to deal with large pre-trained models,
especially Transformer-based ones. Researchers are starting to
apply this intuition to the CL perspective [14], but they are
mostly focused on rather large ViT [25] models. Moreover,
they heavily rely on the capability of the considered model
to extract global semantically-rich representations to guide the
input transformation, while they lack explicit comparison with
the non-Transformer case. In this paper we specifically study
how altering the input data by means of newly introduced
learnable parameters can yield efficient and computationally
affordable CL, starting from a pre-trained backbone.
Input Image
IT -P A D IT -A DD
Transformed Inputs Transformation
Classifier Classifier
Fig. 1: Sketch of the proposed Input Tuning procedure. Two
variants (IT-P AD, IT-A DD) of the transformation function
g(·,·), showing examples of the newly introduced learnable
parameters θgand of the resulting transformed inputs ˜x.
III. C ONTINUALLY TUNING A PRE-TRAINED MODEL
We consider a specific setting in which learning is per-
formed by an edge device with limited computational re-
sources, shipped with a neural network that was pre-trained
on an initial task (usually large-scale data). Without any loss
of generality, we focus on image classification problems, so
that the input of the network is a w×hRGB image. As
usual, the network can be considered to be composed by a
feature extractor mthat extracts higher-level features, and a
classification head cthat returns the predicted confidence yon
a set of classes,
y=c(m(x, θm), θc),
where θmandθcare the parameters (weights and biases) of the
feature extractor and the classification head, respectively, and
yis a vector with a number of components equal to the number
of classes. For simplicity, we consider the classification head
to be composed only of the last linear-projection and non-
linearity. The network knowledge can be transferred to other
somewhat related tasks by removing cand replacing it with
a task-specific head ˆc, with its own new θˆc. The original m
acts as a backbone, and θmcan be fine-tuned on the new task,
together with learning from scratch the new θˆc. We indicate
with FT such a fine-tuning approach, that can be possibly
restricted to θˆcand a subset of θm. We refer to that settingas FT-Partial. We further distinguish these cases from a more
lightweight option called Bias Tuning (BT), which is known
to target a particularly small and expressive subset of model
parameters [26], i.e., the biases of the neurons of the whole
network, together with the usual θˆc. We assume that the net is
simple enough to perform fast inference with the considered
hardware, i.e., in an amount of time that is acceptable for the
target scenario.
We focus on the popular CL setting in which learning
consists of a certain number ( T≥1) of separated training
sessions, Sj,j= 1, . . . , T , where Tcould be potentially
infinite (lifelong learning). Each session comes with data Dj,
and there is no overlap between data batches of different
sessions, Dj∩Dh=∅,∀(j, h). In each Sjthe selected learner
is presented with a task and after the end of the session the task
data are no more available for further training. Before starting
the CL process, we plug a novel classification head ˆcon top of
the pre-trained m, using a large number of output neurons (at
least equal to the expected total number of classes at the end
of the whole learning process), and randomly initializing its
parameters. Of course, we assume that the pre-training dataset
is sufficiently generic to be helpful for tackling a variety of
downstream learning problems. The final goal is to effectively
tune the model in a progressive manner, using data coming
from the stream of tasks, without storing past information. We
indicate with θ(j)
∗the values of the parameters after session
Sj, where ∗is a placeholder for all the different parameters
mentioned in this paper. We expect the overall model at the
end of the sequence of tasks,
y= ˆc
m
x, θ(T)
m
, θ(T)
ˆc
,
to have high average accuracy on all the tasks, keeping the
average forgetting of knowledge at minimum.
We will mostly focus on the class-incremental setting with
some insights on the domain-incremental one, in both cases in
a fully supervised scenario. In the class-incremental setting we
are given data partitioned into a large number of classes and
we simulate CL by limiting each session Sjto a specific subset
of them (disjoint subsets). For each Sjthe classifier outputs
confidence scores ylimited to the session-related classes,
thus exploiting a portion of the head ˆc, referred to as ˆcj.1
Gradients are only computed for the parameters that involve
the output neurons in ˆcj, whose values are indicated with θˆcj.
This is an instance of the so-called “label trick” [27], a rather
simple technique that has a very effective role in preventing
interference and, as such, it is considered as a stable part of
all the models. In the domain-incremental case the learner is
presented with new data labelled over the same set of classes
through the sessions, but originating from a different data
population, thus the label trick does not apply. In fact, for each
Sjthe classifier outputs the full set of confidence scores using
the whole ˆc, and gradients are always computed with respect
to all the θˆc. In all the CL experiments of this paper we always
1In case of softmax activation on the output units, our notation yrefers to
the non-normalized logits (and not to the softmax output).

--- PAGE 4 ---
assume that the task identity is not known at test time, since
it is more challenging and more realistic. It is important to
remark the difference of what we have described so far from
the more common transfer learning process in which all the
data (∪T
j=1Dj) is simultaneously available to tune the model,
that we refer to as Joint Learning (JL) setting, since all the new
tasks are jointly as processed. As a matter of fact, learning in a
CL setting is significantly more challenging than in JL, given
that we expect the model to keep a reasonable performance on
past tasks while learning the new ones in a sequential manner,
without storing past data.
A. Proposed Approach
In this paper, we propose to consider a generic Input Tuning
(IT) in which the original input image xis transformed into ˜x
by some function gbefore feeding it to the network (Fig. 1),
˜x=g(x, θg)
y= ˆc(m(˜x, θm), θˆc),
where θgarenewly introduced learnable parameters involved
in the transformation function only. The purpose of the opti-
mization carried out during the CL process is to jointly train
the parameters θˆcof the classifier ˆctogether with the novel
input tuning parameters θg. The first Input Tuning approach we
consider, which we refer to as IT-P AD, consists in framing the
input image with a border (referred to as Frame ) of learnable
“pixels”, described by θg. Another simple alternative, hence-
forth denoted as IT-A DD, consists in transforming the input by
adding to xa learnable tensor θg(termed as Perturbation ) of
the same shape as the input image, shared by all the possible
inputs. Fig. 1 shows a visual sketch of these transformations,
including an example taken from the experiments that will be
thoroughly discussed in Sec. IV.
In order to provide a glimpse on the outcome of IT, we
report in Tab. I the change in accuracy we get when comparing
a non-tuned baseline model (i.e., a model in which only θˆc
are trained while the whole backbone is kept fixed) with the
previously introduced fine-tuning or Input Tuning procedures,
both when performing transfer learning using all the data (JL,
third column), and when performing sequential CL (fourth
column), which is the focus of this work and will be detailed
in the following. It is evident that what works very well in
the JL setting is not necessarily well-suited for the CL case,
confirming the importance of investigating alternative ways
of exploiting pre-trained backbones in CL. Interestingly, the
IT instances (IT-P ADand IT-A DD) are the ones that better
perform in CL, even if they learn a relatively small set of
parameters.
Whenever a continual learning problem spans clearly het-
erogeneous distributions (for example in the case of data
collected from multiple sources), sharing the exact same
transformation g(·, θg)for all the tasks (referred to as stan-
dard approach , depicted in Fig. 2a) may not be optimal.
For this reason, we propose to learn a different input trans-
formation in different training sessions, by learning inde-
pendent θg’s, one for each task of the session. We willTuning Learnt Parameters Joint Continual
None θˆc 66.12 44.49
BT θˆcand Biases of m(∼1k) +4.44 -2.77
FT-Partial1 θˆcandθ′
m(4.7M) +1.69 -30.47
FT-Partial2 θˆcandθ′′
m(8.4M) -2.16 -32.20
IT-P AD θˆcandFrame (0.1M) +1.86 +7.09
IT-A DD θˆcandPerturbation (0.15M) -0.56 +0.32
TABLE I: Preview of the impact of different tuning approaches
on CIFAR100 in the Joint (accuracy) and Continual (average
task accuracy measured at the end of the learning sequence)
Learning settings. The first row reports the absolute results
of the baseline. The differences with respect to them are
reported in the other rows. While Bias Tuning (BT) is very
effective when all the examples are simultaneously available
for training, Input Tuning is a competitive approach in the CL
setting - where partial fine-tuning (FT-Partial) badly fails. θ′
m
andθ′′
mare two different subsets of the backbone parameters
– see Sec. IV for more details.
indicate with θgjthe transformation parameters learnt in
session Sj, that are about the j-th task. In the challenging
setting we consider, the task identity is not known at test
time, so that, after the training session St, we transform
a test sample xinton
˜xj=g
x, θ(j)
gj
, j= 1, . . . , to
. In
theclass-incremental case, after the training session St, we
then computen
yj= ˆcj
m
˜xj, θ(t)
m
, θ(j)
ˆcj
, j= 1, . . . , to
,
and we concatenate the yj’s to get a vector of confidence
scores involving all the classes explored so far, that is used
to make the final prediction on x(recall that each yjis
only about a subset of classes). Differently, in the domain-
incremental case, after the training session Stwe compute n
yj= ˆc
m
˜xj, θ(t)
m
, θ(t)
ˆc
, j= 1, . . . , to
, and for each
class we take the maximum confidence score to get the final
decision vector (recall that here each yjis about all the
classes, since the set of classes is shared by all the tasks).
Since all these operations can be run in parallel over the
different transformations (up to the final yj’s), we refer to
such classification procedure with the term parallel classifier
(see Fig. 2b).
IV. E XPERIMENTS
We describe our experimental investigation by introduc-
ing the considered datasets in Section IV-A, competitors in
Section IV-B, neural architectures and experimental setup
in Section IV-C, and by reporting and discussing results in
Section IV-D, followed by an in-depth analysis of proposed
approach.
A. Datasets
In order to assess the performance of the proposed learning
algorithm, we exploit multiple datasets available in the litera-
ture. CIFAR100 [28] is a popular Image Classification dataset,
consisting of 60k32×32color images from 100 different
classes (500 training images per class). RESISC45 dataset [29]

--- PAGE 5 ---
Class Incr emental  
(Test at time t ) Domain Incr emental  
(Test at time t ) CA T(a)
Class Incr emental  
(Test at time t ) Domain Incr emental  
(Test at time t ) CA T MAX MAX (b)
Fig. 2: Sketch of Input Tuning computational pipeline for the class-incremental anddomain-incremental cases, going from
input xto class-confidence scores y(logits in the case of softmax) at inference time: (a) standard approach, (b) the parallel
classifier variant.
is a benchmark for Remote Sensing Image Scene Classification
(RESISC). This dataset contains 32k color images, covering
45 scene classes (560 training images for class). FIVEDS [30]
is the concatenation of five well-known image classification
datasets: CIFAR-10 [28], MNIST [31], Fashion-MNIST [32],
SVHN [33], and notMNIST [34]. Even though each single
benchmark is individually fairly easy when using pretrained
models, FIVEDS is challenging because of the forgetting
arising from very different distributions in the input space.
DomainNet [35] is a collection of images labelled over 345
classes with multiple drastic domain shifts. In this paper, we
exploit 4 subsets (sketch, real, painting, clipart), totalling 250k
training samples and 110k test samples.
We define four different CL problems using the just de-
scribed four datasets: ( i.) CIFAR100/T10, where CIFAR100
is randomly divided into 10 tasks (each task contains 10
classes) ( ii.) RESISC45/T9 with 9 tasks (each task contains
5 classes); ( iii.) FIVEDS/T5, where each task consists of the
10 classes available in each of the subdatasets populating
FIVEDS, and ( iv.) DOMAINNET/T4, where each task con-
tains new examples of the same set of classes but from a
different domain. While ( i.,ii.,iii.) are class-incremental , (iv.)
isdomain-incremental . Moreover, ( i.,ii.) are Single Source
Data, while ( iii.,iv.) are Multiple Source Data.
B. Competitors
We compare the proposed IT-P ADand IT-A DDwith a
baseline model that trains only the classification head (param-
eters θˆc), BT, and FT-Partial (two cases, described below).
In IT-P AD, IT-A DDand the baseline model, the learning
process is driven by a loss that involves the supervised data
available in each session, i.e., Dj. Differently, in the case
of FT-Partial, the loss is augmented with CL regularizers
from state-of-the art approaches.2Our goal is to investigate
whether IT is a competitive strategy without changing the loss
function and without introducing extra computations. As a
2As we will show in Section V, fine-tuning without any CL regularizers is
impractical due to strong forgetting.matter of fact, a variety of different specific CL techniques
have been developed in the last decade [3]. In particular,
data regularization methods are mostly inspired by knowledge
distillation [36]. Learning without Forgetting (LwF) [37] is
the simplest instance and it basically performs distillation on
output logits computed on the currently available data, from
the model obtained at the end of session St−1to the current
model in St. Clearly, distillation is restricted to the units of the
classes learnt up to time St. Alternatively, Learning without
Memorizing (LwM) [38] is geared towards attention. Specifi-
cally, it consists of an additional loss used to preserve attention
maps over different sessions (see [39] for details). Finally,
we also consider regularization approaches that directly target
the weights, such as Elastic Weight Consolidation (EWC)
[40]. The importance of the weights is estimated through an
approximation of the Fisher Information Matrix, and EWC
enforces regularity on the learnable parameters between θ(t−1)
∗
andθ(t)
∗according to such estimated importance. Authors of
[41] proposed an even simpler instance of EWC, since they
show that the importance of each parameter to the fulfilment of
a learning task can be estimated by accumulating individual
weight changes; such method is called Path Integral in the
following (also known as Synaptic Intelligence).
C. Experimental Setup
We focus on a specific class of convolutional networks
for image classification, the widely popular ResNets [42],
following our edge-oriented scenario in which the computa-
tional budget is indeed limited. In particular, we will consider
the rather small ResNet-18 (11M parameters), pretrained on
ImageNet [43] (input at the resolution of 224×224). Dif-
ferently from a Transformer-based architecture, it requires
smaller memory and it is less computationally demanding
(compared to those instances of Transformer models with a
relatively smaller number of parameters - such as ViT-B/16,
84M parameters). In the following we consider two differ-
ent partial finetuning settings, FT-Partial1 and FT-Partial2,
focusing on the last module of the considered architecture,

--- PAGE 6 ---
which is made of the two last BasicBlocks3and represents a
remarkable fraction (70% of the total parameters) of the whole
architecture. In FT-Partial1 we restrict the tuning operations
to the parameters contained in the last BasicBlock (4.7M),
and we refer to these parameters with θ′
m. In FT-Partial2 we
also include the ones in the penultimate BasicBlock (+3.7M),
indicating with θ′′
mthe union of these parameters.
In general, we use cross-entropy as classification loss func-
tion, and the Adam optimizer is exploited for all the learnable
parameters unless otherwise specified; a smaller batch size B
is employed for the smaller datasets ( B= 16 for CIFAR100,
RESISC45; B= 64 for FIVEDS, DOMAINNET). In all
the experiments, networks are randomly initialized, using the
same seed for the different approaches, and we report results
averaged over 3 runs with different initializations. Following
[44], training for multiple epochs decouples the effects of
forgetting and underfitting. As such, unless otherwise stated,
we select a sufficient number of epochs to obtain a stable
configuration of the parameters at the end of each task. Since
we assume to have a limited computational budget, hyper-
parameters are shared across the different learning problems
(i.e., not tuned specifically for the dataset at hand). In all
the IT-P ADexperiments we learn a 32-pixel thick border.
Concerning CL strategies4, we adopt parameters suggested
by the respective authors and further suggested by [39]: for
LwF we set the temperature to 2; for EWC the fusion of the old
and new importance weights is done with α= 0.5; for LwM
we set β=γ= 1.0; for Path Integral we fix the damping
parameter to 0.1 as proposed in the original work. Concerning
batch normalization layers, we employ running averages at test
time for Single Source Data and fixed pretraining statistics for
Multiple Source Data. While the impact on the final metrics
is modest, it can be easily grasped that, in the case of very
heterogeneous data across different tasks, learning the tuning
of the model with fixed statistics results in less task-specific
adaptation, hence less forgetting. On the other hand, if all the
data are relatively homogeneous in terms of global statistics
(e.g. CIFAR100), adapting the model in running mode could
benefit from slightly more representative statistics with respect
to the pretraining domain.
We measure the average accuracy and average forgetting at
the end of the learning sequence (i.e. t=T). The average
accuracy ¯aTat the end of the considered task sequence is
selected as the main metric, as in most of the continual
learning literature. Let be τthe task/session index, at,τis the
accuracy computed on the test set of task τ, with the model
obtained after training on task t(i.e., with parameters θ(t)
∗).
Average forgetting ¯fTis also helpful to evaluate the average
magnitude of accuracy drops over the sequence. Formally,
¯at=1
ttX
τ=1at,τ,¯ft=1
t−1t−1X
τ=1max
τ′∈{1,···,t−1}(aτ′,τ−at,τ).
3Please refer to the PyTorch implementation of the ResNet ar-
chitecture, https://pytorch.org/vision/0.8/ modules/torchvision/models/resnet.
html for further details.
4Refer to the original papers for details on the role of these parameters.In all the following results, the terms accuracy andforgetting
refer to the aforementioned average values.
D. Results and Discussion
The results of our experimental activities are reported in
Tab. II and discussed in the following.
In the case of CIFAR100/T10 (Single Source Data) the
domain shift from one task to another one in the sequence
is relatively small and qualitatively all the data share similar
visual features [45]. The first noteworthy remark is that the
baseline, which only learns the classifier ˆcwith the label trick,
works surprisingly well with respect to fine-tuning paired with
well-known CL regularizers. Interestingly enough, bias tuning
(BT) shows no practical improvement over the baseline, given
the forgetting due to the incremental nature of the learning
problem. The same trend can be observed for the partial fine-
tuning options (FT-Partial1, FT-Partial2). On the contrary, both
the IT approaches have at least the same accuracy as the
baseline, showing that they are an appropriate way to strive for
better performance, with slight additional complexity on top
of the label trick baseline. While the improvement provided
by IT-A DDis marginal, IT-P ADsignificantly improves the
test accuracy without being particularly exposed to forgetting
compared to the other options. As expected (Sec. III), the
parallel classifier approach does not help due to relative data
homogeneity of different tasks (Single Source Data).
In RESISC45/T9 (Single Source Data), one may wonder
whether a relatively high semantic affinity between the pre-
training domain (ImageNet) and the target tasks is crucial in
order to get improvements with respect to the baseline. In fact,
RESISC45 is a publicly available dataset of satellite imagery,
which features a quite large semantic and perceptual shift with
respect to the pre-training domain. Similarly to the previous
case, the regularization-based CL strategies are struggling in
achieving and maintaining a good performance throughout
the task sequence, dropping to accuracy levels that are lower
than the baseline. On the other hand, IT-P ADis again the
best performing option and provides an appreciable accuracy
increase, while being less affected by forgetting than the other
tuning options (excluding FT-Partial1-LwF, characterized by a
consistently lower accuracy though).
In FIVEDS/T5 (Multiple Source Data) we experiment with
data coming from multiple sources, possibly distant in the
semantic and perceptual spaces. Interestingly enough, in this
case LwF provides the highest performance. Given that it has
been originally proposed as a task-incremental strategy it is not
surprising that it performs well on a sequence of very distinct
tasks (both in the semantic and in the perceptual spaces). At
the same time, we can see that IT-P ADand IT-A DDpro-
vide valuable improvement when implemented in the parallel
classifier . Moreover, compared to LwF and Path Integral, the
amount of learnt parameters (and the computational burden)
is smaller, there is no need to store tensors of the same size as
the weights (importance weights for PathInt, model snapshot
for LwF) and forgetting is lower.

--- PAGE 7 ---
Tuning Learnt Parameters CIFAR100/T10 RESISC45/T9 FIVEDS/T5 DOMAINNET/T4
Accuracy ↑Forgetting ↓ Accuracy ↑Forgetting ↓ Accuracy ↑Forgetting ↓ Accuracy ↑Forgetting ↓
None θˆc 44.49±0.2213.92±1.49 57.14±0.5022.51±1.15 44.20±2.7917.78±1.30 35.93±0.1318.98±0.32
BT θˆcand Biases of m(∼1k) 41.72±0.3029.91±0.81 46.79±2.0635.24±1.19 20.36±2.8656.92±2.11 38.85±1.9323.28±2.31
FT-Partial1-LwF θˆcandθ′
m(4.7M) 39.18±0.8324.62±0.65 54.46±1.1512.07±1.41 52.72±1.1528.85±1.70 – –
FT-Partial1-LwM θˆcandθ′
m(4.7M) 38.26±0.7222.19±0.81 52.51±1.0625.92±1.49 29.34±0.8567.02±4.05 – –
FT-Partial1-EWC θˆcandθ′
m(4.7M) 39.62±0.4822.85±0.73 54.17±0.4525.75±1.46 38.45±2.0255.18±0.76 19.05±1.779.72±0.71
FT-Partial1-PathInt θˆcandθ′
m(4.7M) 37.46±1.2223.49±0.97 52.95±1.6424.47±1.35 51.38±1.3425.12±0.52 38.55±1.485.7±1.86
FT-Partial2-LwF θˆcandθ′′
m(8.4M) 43.03±0.4928.61±0.22 53.78±0.4529.27±2.05 61.45±3.0228.68±0.23 – –
FT-Partial2-LwM θˆcandθ′′
m(8.4M) 42.06±0.6328.57±1.04 53.94±0.8828.94±2.60 40.98±0.8735.71±2.45 – –
FT-Partial2-EWC θˆcandθ′′
m(8.4M) 41.91±0.8126.76±0.64 56.03±1.3927.17±0.41 46.25±0.8853.11±1.05 29.59±1.3111.21±1.42
FT-Partial2-PathInt θˆcandθ′′
m(8.4M) 42.11±1.0827.27±1.01 54.98±1.8626.02±0.80 60.24±2.0529.09±2.62 39.41±1.6516.17±3.68
IT-P AD(Standard) θˆcandFrame (0.1M) 51.58±1.7819.31±1.46 61.06±0.9916.99±0.94 44.65±1.2331.86±0.89 38.74±0.4418.85±1.39
IT-A DD(Standard) θˆcandPerturbation (0.15M) 44.81±0.8321.01±0.85 57.18±1.6319.43±0.64 36.09±1.8544.29±0.97 33.67±0.3022.45±0.34
IT-P AD(Parallel) θˆcandFrame (0.1M/task) 46.54±0.8212.88±2.27 55.84±1.5919.68±1.81 53.36±0.5519.97±1.40 43.93±1.2216.12±0.91
IT-A DD(Parallel) θˆcandPerturb. (0.15M/task) 41.82±1.2313.05±1.14 52.47±2.8318.29±2.51 56.32±0.4317.75±0.37 41.18±0.8317.34±1.61
TABLE II: Average accuracy ( ↑, higher is better) and forgetting ( ↓, lower is better) measured at the end of the learning
sequence. Notice that some CL strategies are not well-suited for the domain-incremental setting (see the paper text; invalid
configurations are then marked with “–”). In the second column we report the set of parameters subject to optimization: we
always learn the classification head; we tune network weights in the BT, FT-Partial approaches and we learn transformation
parameters ( Perturbation orFrame ) in the IT approaches. We report the number of such learnt parameters in brackets.
The case of DOMAINNET/T4 (Multiple Source Data) de-
parts from the previous ones, being it a domain-incremental
setting. The semantic space is shared by all the tasks, which
feature remarkably different visual styles and perceptual fea-
tures (color, texture, etc.). Although the obtained accuracy may
seem a bit low, it is important to remark that it is a very chal-
lenging learning problem, with many examples that are hard
even for humans and especially for convolutional networks,
that are known to heavily rely on texture [46]. We did not apply
LwF and LwM, that do not fit well the domain-incremental
setting. In fact, the knowledge distillation term would call for
the network output with new data to (a) fit the classification
loss and (b) be similar to the output of the model learnt at
the previous task, which is not expected to help in effectively
learning with low forgetting. Also, it should be noted that
in this case the baseline itself cannot exploit the label trick,
given that each task contains data for the entire class set. As
such, several methods offer improvements over the baseline,
including the quite simple bias tuning (BT). Moreover, Path
Integral gives also a similar improvement, although implying
to compute and store parameter-specific importance weights.
On the other hand, it can be clearly appreciated that the
proposed IT-P ADimplemented with parallel classifier features
the highest accuracy, confirming the importance of learning
independent transformations in Multiple Source Data.
V. I NDEPTH ANALYSIS
We perform additional experiments aimed at gaining more
insights on the previously described results, focusing on the
CIFAR100/T10 learning problem.
In Fig. 3 we report the accuracy obtained in a comparative
experiment with 5 different variants of the IT-P ADscheme. ( i.)
IT-P AD-Online is the setting in which training is performed
with a single pass on the training data. This speeds up learningbut it greatly reduces the extent of the improvement at the end
of training (compared to Tab. II). ( ii.) IT-P AD-Fix refers to
the setting in which we kept fixed the Frame (the learnable
padding) after the first task. Results show that learning the
additional pixels is really beneficial only if they are allowed
to adapt to the slight variations of the different tasks. ( iii.) IT-
PAD-Small is obtained reducing by a factor of 4 the thickness
of the frame border, decreasing the total amount of additional
parameters by a factor of 5; it is interesting to notice that
the accuracy is 4% higher than the Baseline (first row of
Tab. II) also in that setting. ( iv.) IT-P AD-Latent is about
applying the padding operation in a latent space (right after
the first two convolutional layers) and benefits of a comparable
improvement with a similar amount of additional learnable
parameters ( <0.1M). On the other hand, ( v.) combining
the most promising IT approach with BT (IT-P AD+Bias),
completely vanishes any improvement, coherently with what
was suggested in [13]. This analysis confirms the value of the
simple-but-effective vanilla IT-P ADscheme.
40.0 42.5 45.0 47.5 50.0 52.5
Accuracy (in %)Baseline
IT-Pad-Online
IT-Pad-Fix
IT-Pad-Small
IT-Pad-Latent
IT-Pad
IT-Pad+Bias
Fig. 3: Average test accuracy measured at the end of the
learning sequence, for variants of IT-P ADin CIFAR100/T10.
Performing the transformation in some latent space (instead of
the input) is under-performing and adding bias tuning seems
to be counterproductive.

--- PAGE 8 ---
In Tab. III, we further investigate the online setting, showing
the test accuracy for the different methods. In general, the gap
between the IT-P ADapproach and all the competitors is even
wider with respect to the multi-epoch setting, given that fine-
tuning a larger portion of the network would require a larger
amount of update steps in order to obtain a stable configuration
of the weights, conflicting with the single-epoch constraint.
Tuning Learnt Parameters Accuracy ↑ Forgetting ↓
None θˆc 41.38±0.56 14.11±1.88
BT θˆcand Biases ( ∼1k) 43.94±2.31 24.87±3.18
FT-Partial1 (LwF) θˆcandθ′
m(4.7M) 26.07±0.41 20.19±0.92
FT-Partial1 (LwM) θˆcandθ′
m(4.7M) 26.09±0.64 20.08±0.68
FT-Partial1 (EWC) θˆcandθ′
m(4.7M) 25.88±0.83 23.11±0.77
FT-Partial1 (PathInt) θˆcandθ′
m(4.7M) 26.06±1.98 20.84±1.41
FT-Partial2 (LwF) θˆcandθ′′
m(8.4M) 29.13±0.71 22.48±0.86
FT-Partial2 (LwM) θˆcandθ′′
m(8.4M) 22.84±0.55 31.83±1.29
FT-Partial2 (EWC) θˆcandθ′′
m(8.4M) 23.14±0.32 21.32±0.58
FT-Partial2 (PathInt) θˆcandθ′′
m(8.4M) 26.69±1.72 24.85±1.95
IT-P AD(Standard) θˆcandFrame (0.1M) 45.65±1.20 17.89±1.92
IT-A DD(Standard) θˆcandPerturb. (0.15M) 42.48±1.34 14.87±0.82
TABLE III: Results on CIFAR100/T10 in the online setting:
average accuracy and forgetting are measured at the end of
the learning sequence.
In Fig. 4 we provide some insights on the test accuracy,
measured during the learning sequence and at the end of it,
respectively. In Fig. 4 (left), we show that the IT-P ADapproach
typically has the highest average accuracy throughout the
learning sequence, while using FT-Partial without any further
CL regularizers is not a viable option. In Fig. 4 (right) we can
see that the task accuracy of IT-P ADis pretty uniform over
the entire set (with no drastic forgetting on the old tasks) and
generally the highest among the considered approaches.
VI. C ONCLUSIONS
We presented Input Tuning, a novel tuning procedure for
the exploitation of pre-trained models in the context of con-
tinual learning by tweaking the input data (especially when
inserting a frame of learnable pixels, referred to as IT-P AD).
We empirically showed that the proposed method is simple
but effective in consistently improving the quality of the
outcome on multiple learning problems, and can be promptly
extended to the Multiple Source Data case. While the visible
improvements can be intuitively connected with findings about
the prevalence of forgetting in the last layers of the network
[47], [48], we plan to get deeper insights into the learning
dynamics of the proposed method.
REFERENCES
[1] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Language models
are few-shot learners,” in Advances in Neural Information Processing
Systems , 2020.
[2] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-
Fei, “ImageNet Large Scale Visual Recognition Challenge,” Intl. Journal
of Computer Vision (IJCV) , vol. 115, no. 3, pp. 211–252, 2015.
0123456789
Training session102030405060708090Avg.accuracy
IT-P AD
FT -Partial2
Baseline
LwF (FT -Partial2)
LwM (FT -Partial2)
EWC (FT -Partial2)
PathInt (FT -Partial2)
0123456789
Task ID01020304050607080Task ﬁnal accuracy
IT-P AD
FT -Partial2
Baseline
FT -Partial2 (LwF)
BTFig. 4: CIFAR100/T10, models from Tab. II. Left: Average
accuracy during the entire learning sequence. The blue curve
(FT-Partial2, with no CL regularizers) highlights the fact that
naively fine-tuning is not a practical option. IT-P AD(green)
shows the best behavior throughout all the learning sequence.
Right: Task-specific accuracy, at the end of the learning
sequence. Fine-tuning without CL regularizers (FT-Partial2,
blue) has extremely low accuracy, excluding the very last
task. BT (grey) and FT-Partial2 (LwF, orange) best-performing
tasks are concentrated in the last part of the sequence (task id
≥6). IT-P ADconsistently beats the baseline and is the best
for most of the tasks.
[3] M. Delange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis,
G. Slabaugh, and T. Tuytelaars, “A continual learning survey: Defying
forgetting in classification tasks,” IEEE Transactions on Pattern Analysis
and Machine Intelligence , pp. 1–1, 2021.
[4] R. M. French, “Catastrophic forgetting in connectionist networks,”
Trends in Cognitive Sciences , vol. 3, no. 4, pp. 128–135, 1999.
[5] I. J. Goodfellow, M. Mirza, X. Da, A. C. Courville, and Y . Bengio, “An
empirical investigation of catastrophic forgeting in gradient-based neural
networks,” in Proc. of the Intl. Conf. on Learning Representations, ICLR ,
2014.
[6] M. G. S. Murshed, C. Murphy, D. Hou, N. Khan, G. Ananthanarayanan,
and F. Hussain, “Machine learning at the network edge: A survey,” ACM
Comput. Surv. , 2021.
[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances
in Neural Information Processing Systems , 2017.
[8] L. Pellegrini, G. Graffieti, V . Lomonaco, and D. Maltoni, “Latent replay
for real-time continual learning,” in Proc. of the IEEE Intl. Conf. on
Intelligent Robots and Systems (IROS) , 2020.
[9] A. Mahendran and A. Vedaldi, “Understanding deep image representa-
tions by inverting them,” in Proc. of the IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR) , 2015.
[10] O. Ostapenko, T. Lesort, P. Rodriguez, M. R. Arefin, A. Douillard,
I. Rish, and L. Charlin, “Continual Learning with Foundation Models:
An Empirical Study of Latent Replay,” in Proc. of the 1st Conf. on
Lifelong Learning Agents (CoLLAs) . PMLR, 2022.
[11] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts
for generation,” Proc. of the 59th Annual Meeting of the Assoc. for
Computational Linguistics and the 11th Intl. Joint Conf. on Natural
Language Processing , vol. abs/2101.00190, 2021.
[12] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for
parameter-efficient prompt tuning,” in Proc. of the 2021 Conf. on
Empirical Methods in Natural Language Processing . Assoc. for
Computational Linguistics, 2021.
[13] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and
S.-N. Lim, “Visual prompt tuning,” in Proc. of the European Conf. on
Computer Vision (ECCV) , 2022.

--- PAGE 9 ---
[14] Z. Wang, Z. Zhang, C.-Y . Lee, H. Zhang, R. Sun, X. Ren, G. Su,
V . Perot, J. Dy, and T. Pfister, “Learning to prompt for continual
learning,” in Proc. of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) , 2022.
[15] S. V . Mehta, D. Patil, S. Chandar, and E. Strubell, “An empirical
investigation of the role of pre-training in lifelong learning,” arXiv
preprint arXiv:2112.09153 , 2021.
[16] S. Abnar, M. Dehghani, B. Neyshabur, and H. Sedghi, “Exploring the
limits of large scale pre-training,” in Proc. of the Intl. Conf. on Learning
Representations (ICLR) , 2022.
[17] V . V . Ramasesh, A. Lewkowycz, and E. Dyer, “Effect of scale on
catastrophic forgetting in neural networks,” in Proc. of the Intl. Conf.
on Learning Representations (ICLR) , 2022.
[18] D. Hu, S. Yan, Q. Lu, L. Hong, H. Hu, Y . Zhang, Z. Li, X. Wang,
and J. Feng, “How well does self-supervised pre-training perform with
streaming data?” in Intl. Conf. on Learning Representations (ICLR) ,
2022.
[19] A. Cossu, T. Tuytelaars, A. Carta, L. Passaro, V . Lomonaco, and
D. Bacciu, “Continual Pre-Training Mitigates Forgetting in Language
and Vision,” arXiv preprint arXiv:2205.09357 , 2022.
[20] S. An, Y . Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-
G. Lou, “Input-tuning: Adapting unfamiliar inputs to frozen pretrained
models,” arXiv preprint arXiv:2203.03131 , 2022.
[21] J. Pfeiffer, A. R ¨uckl´e, C. Poth, A. Kamath, I. Vuli ´c, S. Ruder, K. Cho,
and I. Gurevych, “Adapterhub: A framework for adapting transformers,”
inProceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations , 2020, pp. 46–54.
[22] G. F. Elsayed, I. Goodfellow, and J. Sohl-Dickstein, “Adversarial repro-
gramming of neural networks,” in Proc. of the Intl. Conf. on Learning
Representations (ICLR) , 2019.
[23] S. Marullo, M. Tiezzi, M. Gori, and S. Melacci, “Friendly training:
Neural networks can adapt data to make learning easier,” in Proc. of the
Intl. Joint Conf. on Neural Networks (IJCNN) , 2021.
[24] ——, “Being friends instead of adversaries: Deep networks learn from
data simplified by other networks,” in Proc. of the AAAI Conf. on
Artificial Intelligence , 2022.
[25] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:
Transformers for image recognition at scale,” in Proc. of the Intl. Conf.
on Learning Representations (ICLR) , 2021.
[26] H. Cai, C. Gan, L. Zhu, and S. Han, “Tinytl: Reduce memory, not
parameters for efficient on-device learning,” in Advances in Neural
Information Processing Systems , 2020.
[27] C. Zeno, I. Golan, E. Hoffer, and D. Soudry, “Task-Agnostic Continual
Learning Using Online Variational Bayes With Fixed-Point Updates,”
Neural Computation , 2021.
[28] A. Krizhevsky, “Learning Multiple Layers of Features from
Tiny Images,” University of Toronto, Tech. Rep., 2009,
https://www.cs.toronto.edu/˜kriz/learning-features-2009-TR.pdf.
[29] G. Cheng, J. Han, and X. Lu, “Remote sensing image scene classifica-
tion: Benchmark and state of the art,” Proceedings of the IEEE , 2017.
[30] S. Ebrahimi, F. Meier, R. Calandra, T. Darrell, and M. Rohrbach,
“Adversarial continual learning,” Proc. of European Conference on
Computer Vision (ECCV) , 2020.
[31] Y . LeCun, “The mnist database of handwritten digits,” http://yann. lecun.
com/exdb/mnist/ , 1998.
[32] H. Xiao, K. Rasul, and R. V ollgraf, “Fashion-mnist: a novel image
dataset for benchmarking machine learning algorithms,” arXiv preprint
arXiv:1708.07747 , 2017.
[33] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y . Ng,
“Reading digits in natural images with unsupervised feature learning,”
inAdvances in Neural Information Processing Systems , 2011.
[34] Y . Bulatov. (2011) notmnist dataset. [Online]. Available: http:
//yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html
[35] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang, “Moment
matching for multi-source domain adaptation,” in Proc. of the IEEE Intl.
Conf. on Computer Vision (ICCV) , 2019.
[36] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a
neural network,” in Advances in Neural Information Processing Systems
- Workshop on Deep Learning , 2014.
[37] Z. Li and D. Hoiem, “Learning without forgetting,” IEEE Transactions
on Pattern Analysis and Machine Intelligence , 2017.[38] P. Dhar, R. V . Singh, K.-C. Peng, Z. Wu, and R. Chellappa, “Learning
without memorizing,” in Proc. IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR) , 2019.
[39] M. Masana, X. Liu, B. Twardowski, M. Menta, A. D. Bagdanov, and
J. van de Weijer, “Class-incremental learning: survey and performance
evaluation,” IEEE Transactions on Pattern Analysis and Machine Intel-
ligence , 2022.
[40] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,
A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska
et al. , “Overcoming catastrophic forgetting in neural networks,” Proc.
of the National Academy of Sciences , 2017.
[41] F. Zenke, B. Poole, and S. Ganguli, “Continual learning through synaptic
intelligence,” in Proc. of the Intl. Conf. on Machine Learning (ICML) ,
2017.
[42] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proc. of the IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR) , 2016.
[43] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-
Fei, “ImageNet Large Scale Visual Recognition Challenge,” Intl. Journal
of Computer Vision (IJCV) , 2015.
[44] P. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara, “Dark
experience for general continual learning: a strong, simple baseline,” in
Advances in Neural Information Processing Systems , 2020.
[45] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, “Continual
lifelong learning with neural networks: A review,” Neural Networks ,
2019.
[46] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann,
and W. Brendel, “Imagenet-trained CNNs are biased towards texture;
increasing shape bias improves accuracy and robustness.” in Proc. of
the Intl. Conf. on Learning Representations (ICLR) , 2019.
[47] T. Lesort, T. George, and I. Rish, “Continual learning in deep networks:
an analysis of the last layer,” arXiv preprint arXiv:2106.01834 , 2021.
[48] V . V . Ramasesh, E. Dyer, and M. Raghu, “Anatomy of catastrophic
forgetting: Hidden representations and task semantics,” in Proc. of the
Intl. Conf. on Learning Representations (ICLR) , 2021.
