LMTuner: Một Framework Huấn luyện thân thiện với người dùng và tích hợp cao
để tinh chỉnh các Mô hình Ngôn ngữ Lớn

Yixuan Weng1, Zhiqi Wang1,2, Huanxuan Liao1,2, Shizhu He1,2, Shengping Liu3, Kang Liu1,2, Jun Zhao1,2
1Phòng thí nghiệm Nhận thức và Trí tuệ Quyết định cho Hệ thống Phức tạp,
Viện Tự động hóa, Viện Hàn lâm Khoa học Trung Quốc
2Trường Trí tuệ Nhân tạo, Đại học Viện Hàn lâm Khoa học Trung Quốc
3Unisound, Bắc Kinh, Trung Quốc

Tóm tắt

Với sự phát triển mạnh mẽ trong lĩnh vực các mô hình ngôn ngữ lớn (LLM), nhu cầu về việc huấn luyện tăng cường hiệu quả được thiết kế riêng cho các ngành và lĩnh vực cụ thể tiếp tục gia tăng. Hiện tại, các framework được sử dụng chủ yếu thiếu thiết kế mô-đun, thường cần rất nhiều công việc lập trình để bắt đầu huấn luyện LLM. Để giải quyết vấn đề này, chúng tôi giới thiệu "LMTuner", một hệ thống có tính khả dụng cao, tích hợp được và có thể mở rộng để huấn luyện LLM một cách nhanh chóng và với đầu vào tối thiểu từ người dùng. LMTuner bao gồm ba mô-đun chính - Mô-đun Tương tác, Huấn luyện và Suy luận. Chúng tôi khẳng định rằng tính khả dụng và tính tích hợp của LMTuner làm giảm bớt sự phức tạp trong việc huấn luyện các mô hình ngôn ngữ lớn. Đáng chú ý, ngay cả một người dùng mới bắt đầu cũng có thể bắt đầu huấn luyện các mô hình ngôn ngữ lớn trong vòng năm phút. Hơn nữa, nó tích hợp các framework DeepSpeed và hỗ trợ các phương pháp Tinh chỉnh Hiệu quả như Low Rank Adaptation (LoRA), Quantized LoRA (QLoRA), v.v., cho phép huấn luyện các mô hình ngôn ngữ có quy mô từ 300M đến 130B tham số sử dụng một máy chủ duy nhất. Trang chủ của LMTuner và video screencast hiện đã có sẵn công khai.

1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLM) đang thể hiện hiệu suất chưa từng có trong nhiều tác vụ hiểu và sinh ngôn ngữ tự nhiên nhờ vào khả năng học từ dữ liệu văn bản rộng lớn với cách thức sinh tạo. Điều này đã dẫn đến số lượng ngày càng tăng của các nhà nghiên cứu và kỹ sư bắt tay vào việc huấn luyện các mô hình ngôn ngữ riêng của họ cho các ngành và lĩnh vực cụ thể. Tuy nhiên, việc huấn luyện LLM đặt ra yêu cầu cao về kỹ năng kỹ thuật, và các kỹ thuật đa dạng có thể áp dụng cho việc huấn luyện như vậy hầu hết đều riêng biệt. Điều này không chỉ làm tăng độ phức tạp của các dự án liên quan mà còn làm tăng chi phí học tập cần thiết để huấn luyện các mô hình ngôn ngữ.

Với sự phát triển tiến bộ của công nghệ mô hình ngôn ngữ lớn sinh tạo, nhiều kỹ thuật khác nhau đã xuất hiện, được tích hợp vào các bộ công cụ khác nhau. Như được thể hiện trong Bảng 1, khi nhắm đến tính song song mô hình, MegatronLM đứng như một lựa chọn ưu tiên, trong khi bitsandbytes phục vụ mục đích lượng tử hóa mô hình, và Opendelta tạo điều kiện thuận lợi cho việc triển khai công nghệ Tinh chỉnh Hiệu quả. Tuy nhiên, tính linh hoạt được cung cấp bởi các công cụ riêng lẻ này đi kèm với chi phí là các nhà phát triển phải dành thời gian đáng kể để điều phối việc sử dụng các mô-đun công cụ đa dạng, khiến việc ứng dụng trực tiếp trở nên khó khăn. Để giải quyết những thách thức này, một số framework như h2oGPT và Lamini đã cố gắng hợp nhất một số chức năng này. Tuy nhiên, các framework này thường chỉ tích hợp một tập con của các kỹ thuật có sẵn, thường thiếu bao phủ toàn diện các công nghệ mô hình thường được sử dụng.

Do đó, trong bài báo này, chúng tôi giới thiệu LMTuner, một framework mới giảm thiểu đáng kể các rào cản bằng cách cung cấp một hệ thống mô-đun dễ sử dụng, có thể mở rộng và tích hợp. Như được mô tả trong Hình 1, LMTuner bao gồm ba mô-đun. 1) Mô-đun Tương tác cho phép giao tiếp thân thiện với người dùng, tự động điều chỉnh các tham số dựa trên nhu cầu và bối cảnh của người dùng, lý tưởng cho người dùng không có kỹ thuật. 2) Mô-đun Huấn luyện tự động xử lý việc huấn luyện bằng cách sử dụng các tham số này, giúp người dùng tiết kiệm sự phức tạp của việc thiết lập. 3) Mô-đun Suy luận sử dụng các mô hình đã được huấn luyện cho các tác vụ khác nhau khi hoàn thành huấn luyện. Với các mô-đun này, các nhà phát triển có thể dễ dàng tạo ra các mô hình mong muốn và tự động chuyển các tham số sang mô-đun tiếp theo để huấn luyện liền mạch. Điều này làm đơn giản hóa toàn bộ quá trình, cho phép hệ thống được triển khai và sử dụng ngay lập tức khi hoàn thành huấn luyện.

Tóm lại, hệ thống LMTuner được đề xuất của chúng tôi cung cấp tính khả dụng và linh hoạt lớn hơn, cho phép người dùng nhanh chóng cấu hình các tham số để huấn luyện các mô hình ngôn ngữ theo nhu cầu cụ thể và hiệu quả khởi tạo việc huấn luyện. Các đóng góp của chúng tôi như sau:

• Chúng tôi đã đề xuất LMTuner, đây là hệ thống huấn luyện có tính khả dụng cao và tích hợp cho LLM. Nó miễn phí sử dụng và thân thiện với giấy phép (Apache 2.0). Và chúng tôi mở mã nguồn tại https://github.com/WENGSYX/LMTuner.

• LMTuner tự hào về tính khả dụng cao, chỉ cần một dòng code duy nhất để khởi chạy. Nó tạo điều kiện thuận lợi cho việc khởi động nhanh chóng để huấn luyện mô hình ngôn ngữ lớn bằng cách cho phép người dùng tương tác bằng ngôn ngữ tự nhiên với LMTuner.

• Chúng tôi đã tích hợp một loạt các kỹ thuật phù hợp để huấn luyện các mô hình ngôn ngữ lớn, bao gồm các mô hình, bộ dữ liệu QA chuyên ngành, các phương pháp Tinh chỉnh Hiệu quả và các siêu tham số cụ thể. Sự tích hợp này thúc đẩy nghiên cứu và phát triển các mô hình ngôn ngữ lớn.

2 Công trình liên quan

Sự phát triển của các mô hình ngôn ngữ tiền huấn luyện đã mang lại nhiều công cụ mô hình ngôn ngữ và một cộng đồng NLP thịnh vượng, trong đó có "Transformers". Nó thiết lập một loạt các lớp mô hình và cung cấp API để triển khai các mô hình transformer có thể mở rộng dễ dàng. Các công cụ khác liên quan chặt chẽ đến việc xây dựng mô hình ngôn ngữ bao gồm Fairseq, MegatronLM, MedConQA, OpenDelta và h2oGPT. Khác với những công cụ này, LMTuner được thiết kế đặc biệt để huấn luyện các LLM tự hồi quy. Với thiết kế mô-đun của mình, nó cho phép kết hợp tự do các mô hình tiền huấn luyện khác nhau, bộ dữ liệu, framework mô hình, cài đặt ngoại suy độ dài tự động và các phương pháp PEFT trong một framework. Với sự hỗ trợ của các LLM kiểu đối thoại như GPT-4, người dùng có thể hoàn thành toàn bộ quá trình huấn luyện chỉ với một dòng code duy nhất.

Các nghiên cứu gần đây đã đề xuất nhiều hướng để huấn luyện LLM, bao gồm dữ liệu chất lượng cao, các phương pháp tinh chỉnh hiệu quả, cấu trúc mô hình và ngoại suy độ dài. Hệ thống LMTuner tích hợp các kỹ thuật tiên tiến nhất trong các lĩnh vực này, cho phép người dùng đưa ra lựa chọn theo nhu cầu của họ. LMTuner tích hợp các kỹ thuật này thành các mô-đun riêng biệt, tạo điều kiện thuận lợi cho việc lựa chọn và sử dụng của người dùng.

Các LLM trải qua tiền huấn luyện hướng dẫn có thể phù hợp với các hướng dẫn của con người và đã được phát hiện sở hữu nhiều khả năng không có trong các mô hình ngôn ngữ nhỏ hơn, chẳng hạn như tạo công cụ, khám phá môi trường, tự xác minh và lý luận phức tạp. Tận dụng những khả năng này của LLM, Mô-đun Tương tác của LMTuner có thể giúp người dùng phân tích nhu cầu của họ và đề xuất các cài đặt cần thiết thông qua phương pháp tương tác dựa trên ngôn ngữ tự nhiên. Thực tiễn sử dụng LLM như các tác nhân tương tác này cũng đã được áp dụng trong việc ra quyết định tương tác và hoạt động như trợ lý nghiên cứu.

3 LMTuner

LMTuner là một hệ thống mã nguồn mở cung cấp giao diện dòng lệnh (CLI) để huấn luyện LLM:

```python
# Khởi chạy chính thức cho LMTuner.
from LMTuner import Let_Tune
Let_Tune()

>>> [AI] Chào mừng! Tôi là trợ lý AI của bạn. Hiện tại hỗ trợ huấn luyện mô hình của bạn. Cần có kế hoạch rõ ràng, mục tiêu trước tiên.
>>> [TRẢ LỜI]:
```

Như được thể hiện trong Hình 2, LMTuner cho phép phát triển huấn luyện LLM thông qua các cuộc trò chuyện đơn giản. Điều này có thể cải thiện hiệu quả kỹ thuật khi huấn luyện LLM và giảm gánh nặng code.

3.1 Mô-đun Tương tác

Mô-đun Tương tác của LMTuner, sử dụng các tính năng System Message và Function của GPT-4 (hoặc ChatGPT), đóng vai trò là trợ lý huấn luyện LLM. Mô-đun này, trong quá trình khởi tạo, tích hợp các vấn đề huấn luyện thông thường, cấu hình tham số và các phương pháp có thể lựa chọn vào System Messages, từ đó làm đơn giản hóa quá trình huấn luyện. Chức năng thiết yếu của GPT-4 trong bối cảnh này là xác định các cấu hình huấn luyện cần thiết. Một chức năng, Set_ARGS, có sẵn để sửa đổi các tham số trong cấu hình huấn luyện. Để cho phép GPT-4 đánh giá các cài đặt tham số khác nhau, chúng tôi sử dụng pynvml để giám sát GPU máy chủ, tích hợp thông tin này trong nội dung System Message.

Mô-đun Tương tác giúp ngăn ngừa các lỗi cấu hình của người dùng thông qua giao diện thân thiện với người dùng và khả năng thích ứng cao. Như được thể hiện trong Hình 3, người dùng có thể trình bày nhu cầu của họ theo cách đối thoại. LMTuner sau đó phân tích lời nói của họ và đề xuất các cài đặt huấn luyện phù hợp. Điều này hoạt động bởi vì System Message chứa kiến thức về huấn luyện LLM, bù đắp cho việc thiếu hụt chuyên môn như vậy thường gặp ở các nhà nghiên cứu và kỹ sư không có nền tảng trong huấn luyện mô hình ngôn ngữ lớn.

Mô-đun Tương tác, trong khi cung cấp tính linh hoạt và dễ sử dụng to lớn, ngăn ngừa các vấn đề tiềm ẩn như lỗi cấu hình người dùng. Như được thể hiện trong Hình 3, người dùng với các yêu cầu khác nhau chỉ cần thể hiện nhu cầu của họ bằng ngôn ngữ tự nhiên. Hệ thống LMTuner có khả năng phân tích và đề xuất các tham số huấn luyện phù hợp. Nếu người dùng không muốn huấn luyện mô hình trên thiết bị hiện tại, LMTuner sẽ tự động tạo một tệp Readme.md bao gồm cấu hình môi trường, xử lý mô hình và hướng dẫn code huấn luyện như Hình 4, tạo điều kiện thuận lợi cho việc thiết lập nhanh chóng cho người dùng trên một thiết bị mới.

Khi tất cả các tham số huấn luyện được hoàn thiện, LMTuner lưu một bản sao của chúng trong một tệp ARGS.json. Nếu muốn khởi tạo huấn luyện nhanh chóng với các tham số tương tự, họ chỉ cần truyền tên đường dẫn của ARGS.json vào hàm, từ đó tránh các cuộc đối thoại lặp lại dư thừa.

```python
# Khởi chạy nhanh cho LMTuner.
from LMTuner import Let_Tune
Let_Tune(ARGS='./ARGS.config')

>>> [LMTuner] Chúng tôi sẽ huấn luyện mô hình ~ Bắt đầu!
>>> [2023-07-19 05:18:34,778] [INFO] [runner.py:555: main] cmd = python -u -m deepspeed.launcher.launch --world_info = xxxxx main.py --seed 1234 ......
```

3.2 Mô-đun Huấn luyện

Mô-đun huấn luyện trong LMTuner có các tính năng tích hợp cao, dễ gọi và có thể mở rộng. Hiện tại, các LLM chủ đạo hầu hết tương tự nhau về kiến trúc, và quá trình huấn luyện cũng như tính toán loss nói chung đều nhất quán. Do đó, chúng tôi xây dựng các kỹ thuật cần thiết theo cách mô-đun hóa ở cấp độ code theo các yêu cầu khác nhau. Đồng thời, thiết kế như vậy cũng tạo điều kiện thuận lợi cho các kỹ sư có kỹ năng lập trình để thay thế trực tiếp các mô-đun tương ứng thông qua hooks. Trong phần còn lại của phần này, chúng tôi sẽ giới thiệu từng mô-đun kỹ thuật riêng biệt.

Bộ dữ liệu. Tính khả dụng của dữ liệu huấn luyện chất lượng cao là rất quan trọng để phát triển các LLM có khả năng. Để tạo điều kiện truy cập giá cả phải chăng đến các bộ dữ liệu phù hợp cho việc trả lời câu hỏi, chúng tôi đã tuyển chọn và chuẩn bị một bộ sưu tập các bộ dữ liệu QA bao phủ các lĩnh vực đa dạng, bao gồm tiếng Anh, tiếng Trung, y tế và pháp lý. Hơn nữa, để cho phép phát triển các mô hình với tên và nhân cách có thể tùy chỉnh, các bộ dữ liệu đã được tăng cường với các cặp câu hỏi-trả lời tổng hợp hỏi về danh tính của mô hình (ví dụ: "Xin chào, tôi là [TÊN MÔ HÌNH]"). Trong quá trình huấn luyện, các token [TÊN MÔ HÌNH] có thể được thay thế động với tên ưa thích cho mỗi phiên bản mô hình.

```python
from LMTuner.dataset import LMTunerDataset

dataset = LMTunerDataset()

# Đặt tên cho mô hình của bạn
dataset.set_model_name('LMTuner')

# Thêm mẫu bộ dữ liệu QA
dataset.add_sample(['Bạn là ai?', "Tôi là LMTuner, sidekick cá nhân của bạn!"])
```

Tuy nhiên, bất chấp sự phong phú của các bộ dữ liệu được tuyển chọn của chúng tôi, chúng vốn dĩ có những hạn chế nhất định về độ bao phủ và tính đa dạng. Để tăng cường các bộ dữ liệu tích hợp sẵn và tính đến nhu cầu cụ thể của người dùng, hệ thống của chúng tôi cũng cung cấp hỗ trợ liền mạch cho dữ liệu huấn luyện tùy chỉnh. Người dùng có thể đơn giản cung cấp đường dẫn cục bộ đến các tệp dữ liệu trả lời câu hỏi định dạng JSONL của riêng họ. Lựa chọn thiết kế này cung cấp tính linh hoạt lớn hơn cho người dùng, trao quyền cho họ để điều chỉnh phân phối huấn luyện theo yêu cầu ứng dụng độc đáo của họ. Ví dụ, người dùng có thể cung cấp các bộ dữ liệu độc quyền chứa thông tin nhạy cảm hoặc bí mật không phù hợp để phát hành công khai. Khả năng sử dụng trực tiếp các tệp JSONL cục bộ tránh được nhu cầu chuyển bộ dữ liệu sang các nền tảng bên ngoài.

```python
# Định dạng Bộ dữ liệu Tùy chỉnh Tiền huấn luyện
{
  "input": "",
  "output": "Với sự phát triển mạnh mẽ trong lĩnh vực...",
}

# Định dạng Bộ dữ liệu Tùy chỉnh Hướng dẫn
{
  "input": "Con người: Bạn là ai?",
  "output": "Trợ lý: Tôi là LMTuner, sidekick cá nhân của bạn!",
}
```

Mô hình. Những tiến bộ gần đây trong xử lý ngôn ngữ tự nhiên đã được kích hoạt bởi kiến trúc transformer. Framework SwissArmyTransformer tạo điều kiện thuận lợi cho việc phát triển hiệu quả các mô hình transformer đa dạng bằng cách tách rời các thành phần cốt lõi có thể tái sử dụng khỏi các mô-đun có thể hoán đổi cụ thể cho mô hình. Các mô-đun nhẹ này gắn vào backbone được chia sẻ thông qua hooks, cho phép lặp lại và tùy chỉnh nhanh chóng. Ngược lại, Transformers cung cấp các triển khai được tối ưu hóa của các kiến trúc chính tắc và các mô hình tiền huấn luyện để sử dụng trong sản xuất.

Hệ thống LMTuner kết hợp những điểm mạnh bổ sung này để phát triển và triển khai mô hình linh hoạt. Nó sử dụng SwissArmyTransformer để xây dựng các kiến trúc được thiết kế riêng và tích hợp liền mạch các mô hình tiền huấn luyện của Transformers. Sự tổng hợp giữa mô-đun hóa code và các mô hình tiền huấn luyện rộng lớn này hứa hẹn sẽ nâng cao năng suất, tăng tốc đổi mới và cải thiện hiểu biết ngôn ngữ trong thế giới thực. LMTuner thúc đẩy mô hình hóa khám phá bằng cách đổi mới các thiết kế mới được xây dựng trên cơ sở hạ tầng transformer, đồng thời hưởng lợi từ những tiến bộ tiên tiến trong tiền huấn luyện mô hình ngôn ngữ. Việc sử dụng có chọn lọc cả hai thư viện đứng ra để thúc đẩy có ý nghĩa các hệ thống xử lý ngôn ngữ tự nhiên thông qua việc tạo mẫu nhanh các mô hình chuyên biệt và triển khai có thể truy cập các khả năng tiên tiến.

Tinh chỉnh Hiệu quả. LMTuner sử dụng công nghệ ZeRO của Deepspeed theo mặc định để giảm tải tham số và cải thiện thông lượng huấn luyện. Ngoài ra, LMTuner cung cấp các phương pháp tinh chỉnh hiệu quả tham số (PEFT) bao gồm LoRA và QLoRA, và các phương pháp tinh chỉnh hiệu quả bộ nhớ (MEFT) bao gồm LOMO và Quantization. Chúng hỗ trợ huấn luyện LLM với việc sử dụng bộ nhớ thấp. Các phương pháp này được triển khai theo cách mô-đun hóa ở cấp độ code của LMTuner, vì vậy chúng có thể được kết hợp và sử dụng dễ dàng một cách tự do.

Nội suy Vị trí. Để hỗ trợ tốt hơn việc mô hình hóa ngữ cảnh dài, LMTuner đã tích hợp một số quy mô của RoPE. Chúng tôi đã triển khai Xpos và một số phương pháp nội suy vị trí gần đây như nội suy tuyến tính, nội suy động, NTK-Aware Scaled RoPE (NTKv1) và NTK-By-Parts (NTKv2). Các phương pháp động chọn tham số quy mô chính xác dựa trên độ dài chuỗi, thay vì phải chấp nhận một sự đánh đổi cố định giữa độ dài chuỗi tối đa và hiệu suất trên các chuỗi ngắn hơn, tức là sử dụng các giá trị vị trí chính xác cho 2048 ngữ cảnh đầu tiên và sau đó tính toán lại các vector vị trí cho mỗi độ dài chuỗi mới khi mô hình tạo ra các marker từng cái một. Một mức độ nhất định của mô hình hóa ngữ cảnh dài có thể đạt được bằng cách chọn các phương pháp quy mô khác nhau.

Các Chi tiết Khác. Chúng tôi sử dụng phân phối xác suất trên các chuỗi token làm mục tiêu tối ưu hóa với loss cross-entropy và sử dụng optimizer Lion theo mặc định để tối ưu hóa LLM, bởi vì nó đã được chứng minh là hiệu quả bộ nhớ hơn Adam. Trong quá trình huấn luyện, chúng tôi ghi lại loss, tốc độ học và số lượng token tương ứng tại mỗi bước sử dụng wandb, và hiển thị chúng trong trình duyệt thông qua biểu đồ đường, giúp người dùng quan sát trạng thái huấn luyện trong quá trình huấn luyện.

3.3 Mô-đun Suy luận

LMTuner tải các trọng số mô hình cuối cùng sau khi huấn luyện và tạo ra các phần tiếp theo có điều kiện trên các ngữ cảnh đã cho cho đến khi đạt đến độ dài tối đa. Để tăng tốc suy luận, LMTuner cung cấp các phương pháp lượng tử hóa mô hình bao gồm lượng tử hóa INT8 và INT4. Bằng cách lượng tử hóa các trọng số điểm nổi 16-bit thành các số nguyên độ rộng bit thấp hơn như 8-bit hoặc 4-bit, thời gian tính toán và việc sử dụng bộ nhớ trong quá trình suy luận có thể được giảm. LMTuner lượng tử hóa trọng số của các lớp được chọn trong một mô hình đã được huấn luyện, trong khi giữ các activation ở định dạng điểm nổi 16-bit. Sau khi lượng tử hóa, độ trễ suy luận trên CPU và thông lượng trên GPU có thể được cải thiện đáng kể với sự suy giảm ít trong chất lượng mô hình.

4 Một Trường hợp Chạy

Đối tượng mục tiêu của LMTuner là các kỹ sư và nhà nghiên cứu machine learning trong học thuật và công nghiệp. Người dùng mới có thể tận dụng tương tác hướng dẫn của LMTuner trong khi các chuyên gia vẫn giữ toàn quyền kiểm soát các chi tiết triển khai. Nó so sánh thuận lợi với các hệ thống hiện tại bằng cách kết hợp tính thân thiện với người dùng, khả năng mở rộng và tính tích hợp trong một giao diện thống nhất. LMTuner là mã nguồn mở dưới giấy phép Apache 2.0, cho phép sử dụng miễn phí trong các sản phẩm thương mại. Bằng cách mở mã nguồn LMTuner, chúng tôi hy vọng sẽ thúc đẩy tiến bộ trong huấn luyện mô hình ngôn ngữ lớn và giảm các rào cản để tận dụng những công nghệ biến đổi này. Tính khả dụng của một hệ thống dễ sử dụng, có thể tùy chỉnh cao sẽ có lợi cho cộng đồng rộng lớn hơn.

Giả sử chúng ta cần huấn luyện một LLM y tế có thể hỗ trợ trong chẩn đoán bệnh nhân, và chúng ta có hai GPU A6000 với 48GB VRAM mỗi cái, cũng như một bộ dữ liệu QA y tế (MedDialog). Sử dụng LMTuner, chúng ta có thể tự động xác định quá trình huấn luyện, bao gồm việc lựa chọn mô hình LLama-7B và một tập hợp các siêu tham số tương ứng.

trong khi Bảng 2 thể hiện hiệu suất của tập test MedDialog (không có lựa chọn mô hình thủ công). Thay vào đó, chúng tôi đã chọn mô hình cuối cùng thu được sau khi huấn luyện LMTuner, đã hoàn thành 10 epochs. Chúng tôi quan sát thấy rằng mô hình được huấn luyện bởi LMTuner vượt trội hơn các mô hình hiện có trong các chỉ số như Bleu, Meteor và NIST. Điều này cho thấy rằng mô hình được huấn luyện đã sẵn sàng để sử dụng trực tiếp.

5 Kết luận

LMTuner đại diện cho một nỗ lực tiên phong để tạo điều kiện thuận lợi cho việc huấn luyện mô hình ngôn ngữ lớn thông qua tính khả dụng và tính mô-đun được nâng cao. Chúng tôi tin rằng LMTuner đại diện cho một bước quan trọng hướng tới việc nhận ra toàn bộ tiềm năng của các mô hình ngôn ngữ lớn. Bằng cách tiếp tục tích hợp các kỹ thuật mới nổi và phản hồi cộng đồng, khả năng của nó sẽ chỉ phát triển theo thời gian.

Tính khả dụng của LMTuner như một dự án mã nguồn mở mang đến những cơ hội thú vị để nâng cao LLM. Chúng tôi hy vọng việc nhấn mạnh vào tính khả dụng và tính mở rộng sẽ thúc đẩy có ý nghĩa công việc trong tương lai trong lĩnh vực xác định mô hình này.

Hạn chế

Trong khi LMTuner được thiết kế để thân thiện với người dùng và trực quan, nó có thể không nắm bắt được tất cả các yêu cầu cụ thể của người dùng trong phiên bản hiện tại. Quá trình huấn luyện có thể cần được điều chỉnh hoặc các kỹ thuật bổ sung có thể cần được tích hợp để đạt được hiệu suất tối ưu cho một số tác vụ chuyên biệt nhất định. Một số yêu cầu phức tạp có thể cần thiết phải sửa đổi code thủ công, điều này có thể làm tăng đường cong học tập cho người dùng không chuyên. Tuy nhiên, hạn chế này cũng đang được giải quyết tích cực thông qua việc phát triển và cập nhật liên tục hệ thống để nâng cao khả năng hiểu yêu cầu của người dùng.
