# 2306.15706.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2306.15706.pdf
# Kích thước tệp: 4012948 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tinh chỉnh prompt xấp xỉ cho các mô hình tiền huấn luyện thị giác-ngôn ngữ
Qiong Wu12, Shubin Huang1, Yiyi Zhou12, Pingyang Dai1, Annan Shu3, Guannan Jiang3,
Rongrong Ji12
1Phòng thí nghiệm trọng điểm về tính toán hiệu quả và nhận thức đáng tin cậy đa phương tiện,
Bộ Giáo dục Trung Quốc, Đại học Hạ Môn, 361005, Cộng hòa nhân dân Trung Hoa.
2Viện Trí tuệ nhân tạo, Đại học Hạ Môn, 361005, Cộng hòa nhân dân Trung Hoa.
3Phòng Sản xuất thông minh, Công ty TNHH Công nghệ Contemporary Amperex
{qiong, shubinhuang }stu.xmu.edu.cn, {zhouyiyi, pydai }xmu.edu.cn,
{shuan01, jianggn }catl.com, rrjixmu.edu.cn

Tóm tắt
Tinh chỉnh prompt là một cách hiệu quả về tham số để triển khai các mô hình tiền huấn luyện quy mô lớn cho các tác vụ downstream bằng cách thêm các token cụ thể cho tác vụ. Đối với các mô hình tiền huấn luyện thị giác-ngôn ngữ (VLP), tinh chỉnh prompt thường yêu cầu một số lượng lớn token có thể học để thu hẹp khoảng cách giữa tiền huấn luyện và các tác vụ downstream, điều này làm tăng đáng kể chi phí tính toán đã cao. Trong bài báo này, chúng tôi xem xét lại nguyên lý của tinh chỉnh prompt cho các mô hình VLP dựa trên Transformer, và tiết lộ rằng tác động của các token prompt mềm thực tế có thể được xấp xỉ thông qua các bước khuếch tán thông tin độc lập, từ đó tránh được việc mô hình hóa attention toàn cục tốn kém và giảm độ phức tạp tính toán ở mức độ lớn. Dựa trên phát hiện này, chúng tôi đề xuất một phương pháp Tinh chỉnh Prompt Xấp xỉ (APT) mới hướng tới học chuyển giao VL hiệu quả. Để xác thực APT, chúng tôi áp dụng nó cho hai mô hình VLP đại diện, đó là ViLT và METER, và tiến hành các thí nghiệm rộng rãi trên một loạt các tác vụ downstream. Đồng thời, khả năng tổng quát hóa của APT cũng được xác thực trên CLIP cho phân loại hình ảnh và StableDiffusion cho tạo ảnh từ văn bản. Kết quả thí nghiệm không chỉ cho thấy lợi ích về hiệu suất vượt trội và hiệu quả tính toán của APT so với các phương pháp tinh chỉnh prompt thông thường, ví dụ +7,01% độ chính xác và -82,30% chi phí tính toán bổ sung trên METER, mà còn xác nhận ưu điểm của nó so với các phương pháp học chuyển giao hiệu quả tham số khác1.

Giới thiệu
Tinh chỉnh prompt (Li và Liang 2021; Cui và cộng sự 2021; Radford và cộng sự 2021; Liu và cộng sự 2021; Jia và cộng sự 2022; Zhou và cộng sự 2022a,b) là một cách hiệu quả về tham số để thích ứng các mô hình tiền huấn luyện quy mô lớn với các tác vụ downstream. Nó chèn nhiều token prompt vào chuỗi đầu vào để thống nhất phân phối dữ liệu tiền huấn luyện và downstream (Petroni và cộng sự 2019; Radford và cộng sự 2021), từ đó tránh việc tinh chỉnh toàn bộ tốn kém của các mô hình tiền huấn luyện. Các tiến bộ gần đây (Li và Liang 2021; Jia và cộng sự 2022; Zhou và cộng sự 2022a,b) sử dụng các token có thể huấn luyện để thay thế các token thủ công cho việc thích ứng trên các tác vụ downstream, được gọi là tinh chỉnh prompt mềm.

--- TRANG 2 ---
...
... ......... ...
...
...
... ........
... ...... ...
...
........Prompt InputPrompt Input... ...
(a) Ma trận Attention Toàn cục......
......
...
... ......
......
...
...InputPrompt Input...
(b) Ma trận attention của APT của chúng tôi
Input-Prompt
Input-onlyInput-only
Prompt-only Prompt-InputInput-Prompt
Xấp xỉHình 2: Minh họa các ma trận self-attention toàn cục với và không có APT. (a) là các ma trận attention của tinh chỉnh prompt thông thường. Trong (b), các phần prompt-input và prompt-only được loại bỏ, và attention input-prompt được xấp xỉ bởi APT.

các tác vụ downstream, như thể hiện trong Hình 1-a. Ngoài ra, chúng tôi cũng nhận thấy rằng ngay cả với một loạt token, tinh chỉnh prompt mềm vẫn có tác động hạn chế đến chuỗi đầu vào, tức là trọng số attention, dẫn đến thích ứng không tối ưu, như thể hiện trong Hình 1-b. Xét rằng các token này thường được tham gia vào self-attention, có độ phức tạp tính toán bậc hai theo độ dài đầu vào (Vaswani và cộng sự 2017), thích ứng không hiệu quả này sẽ tăng đáng kể chi phí tính toán đã cao của các mô hình VLP.

Bằng cách xem xét lại nguyên lý của tinh chỉnh prompt, chúng tôi thấy rằng tồn tại một giải pháp tiềm năng cho thích ứng VL hiệu quả. Cụ thể, tinh chỉnh prompt nhằm sử dụng các token bổ sung để ảnh hưởng đến chuỗi đầu vào, nhằm tối thiểu hóa khoảng cách giữa tiền huấn luyện và các tác vụ downstream (Petroni và cộng sự 2019; Cui và cộng sự 2021). Đối với tinh chỉnh prompt mềm, các token thường được chèn vào các lớp self-attention của các mô hình VLP (Li và Liang 2021; Jia và cộng sự 2022; Zhou và cộng sự 2022a). Thông qua phân tích self-attention, chúng tôi quan sát thấy rằng ma trận trọng số attention thu được thực tế có thể được chia thành bốn phần phụ như thể hiện trong Hình 2-a. Ở đây, chúng tôi gọi chúng là các ma trận attention input-only, input2prompt, prompt2input và prompt-only, tương ứng. Dưới thiết lập tinh chỉnh prompt sâu (Jia và cộng sự 2022), tức là các prompt được áp dụng theo lớp, việc tính toán prompt2input và prompt-only thực sự có thể được bỏ qua và sẽ không ảnh hưởng đến tinh chỉnh prompt của lớp tiếp theo. Và input-only là hoạt động mặc định của các mô hình tiền huấn luyện không thể thay đổi. Trong trường hợp này, chìa khóa để cải thiện tinh chỉnh prompt nằm ở input2prompt, về bản chất là một bước khuếch tán thông tin từ các token prompt đến chuỗi đầu vào dưới góc độ lý thuyết đồ thị (Zhou và cộng sự 2020). Tuy nhiên, chúng tôi thấy rằng chức năng của nó thực sự có thể được xấp xỉ thông qua một quá trình hiệu quả hơn độc lập với attention toàn cục, từ đó cải thiện hiệu quả của thích ứng VL.

Được thúc đẩy bởi quan sát này, chúng tôi đề xuất một phương pháp tinh chỉnh prompt xấp xỉ (APT) mới cho các mô hình VLP trong bài báo này. Tương tự như tinh chỉnh prompt sâu (Jia và cộng sự 2022; Li và Liang 2021), APT chèn một tập hợp các token có thể học vào mỗi lớp self-attention của mô hình VLP. Như thể hiện trong Hình 2-b, một điểm khác biệt quan trọng là chúng tôi tách riêng các token này khỏi self-attention toàn cục tốn kém và xấp xỉ hiệu ứng của chúng một cách độc lập bằng cách tổng hợp các token prompt với các phép biến đổi thứ hạng thấp. Theo cách này, APT được đề xuất có thể khuếch tán thông tin hiệu quả hơn từ các token prompt đến chuỗi đầu vào trong khi tránh self-attention toàn cục tốn kém, như thể hiện trong Hình 1-b.

Để xác thực APT, chúng tôi áp dụng nó cho hai mô hình VLP dựa trên deep-fusion, đó là ViLT (Kim, Son, và Kim 2021) và METER (Dou và cộng sự 2022), trên ba benchmark VL bao gồm VQA (Antol và cộng sự 2015), NLVR2 (Suhr và cộng sự 2019) và Flickr30K (Plummer và cộng sự 2017). Ngoài ra, chúng tôi cũng kiểm tra khả năng tổng quát hóa của nó trên CLIP (Radford và cộng sự 2021) cho tác vụ phân loại base-to-new và trên StableDiffusion (Rombach và cộng sự 2022; Ruiz và cộng sự 2022) cho tạo ảnh từ văn bản. Kết quả thí nghiệm không chỉ cho thấy ưu điểm rõ ràng của APT so với các phương pháp tinh chỉnh prompt thông thường (Li và Liang 2021; Jia và cộng sự 2022; Zhou và cộng sự 2022a,b), ví dụ +8,30% độ chính xác trên VQA2.0 cho METER trong khi giảm đến 17,70% chi phí tính toán bổ sung. APT của chúng tôi cũng mang lại hiệu suất tốt hơn so với hầu hết các phương pháp học chuyển giao hiệu quả tham số (PETL) (Jia và cộng sự 2022; Hu và cộng sự 2022; Sung, Cho, và Bansal 2022; He và cộng sự 2022), ví dụ 70,94% trên VQA2.0 cho ViLT và 80,97% trên NLVR2 cho METER.

Nhìn chung, đóng góp của chúng tôi gồm ba khía cạnh:
• Chúng tôi xác định các thách thức chính của tinh chỉnh prompt trên các mô hình VLP thông thường, ví dụ ViLT (Kim, Son, và Kim 2021) và METER (Dou và cộng sự 2022), đó là chi phí tính toán quá mức và hiệu quả tinh chỉnh prompt thấp.
• Chúng tôi đề xuất một phương pháp tinh chỉnh prompt xấp xỉ (APT) mới cho cả tinh chỉnh prompt hiệu quả về tham số và tính toán, xấp xỉ ảnh hưởng của các token prompt thông qua các bước tổng hợp độc lập.
• APT được đề xuất không chỉ vượt trội hơn các phương pháp tinh chỉnh prompt hiện có mà còn đạt được hiệu suất tốt hơn so với các phương pháp PETL khác trên 2 mô hình VLP và 4 tác vụ VL. Khả năng tổng quát hóa của nó cũng được xác thực trên CLIP và StableDiffusion.

--- TRANG 3 ---
mô hình đại diện khác gọi là ViLT (Kim, Son, và Kim 2021), xử lý thông tin hình ảnh và văn bản chỉ với một mạng Transformer đầu-cuối.

Tinh chỉnh Prompt
Tinh chỉnh prompt (Brown và cộng sự 2020; Petroni và cộng sự 2019; Li và Liang 2021; Cui và cộng sự 2021; Radford và cộng sự 2021; Jia và cộng sự 2022; Zhou và cộng sự 2022a,b; Liu và cộng sự 2021) là một cách hiệu quả về tham số để thích ứng các mô hình tiền huấn luyện với các tác vụ downstream. Cụ thể, đối với các prompt thủ công (Petroni và cộng sự 2019; Radford và cộng sự 2021), nó thường chèn một cụm từ prompt được định nghĩa trước vào các chuỗi đầu vào, do đó nhắc nhở mô hình về kiến thức tiền huấn luyện, ví dụ, "Đây là một bức tranh của [X]". Tuy nhiên, tinh chỉnh prompt cứng phụ thuộc nhiều vào thiết kế thủ công. Để khắc phục vấn đề này, tinh chỉnh prompt mềm (Li và Liang 2021; Jia và cộng sự 2022; Zhou và cộng sự 2022b) được đề xuất để tự động học các prompt có thể huấn luyện thông qua thích ứng tác vụ downstream. Về vị trí đặt prompt, tinh chỉnh prompt mềm có thể được chia thành hai mẫu, tức là mẫu nông (Li và Liang 2021) và mẫu sâu (Jia và cộng sự 2022). Các phương pháp tinh chỉnh prompt nông (Lester, Al-Rfou, và Constant 2021; Li và Liang 2021) chỉ mở rộng chuỗi đầu vào bằng các vector có thể huấn luyện ở lớp đầu tiên, trong khi các phương pháp tinh chỉnh prompt sâu (Jia và cộng sự 2022) mở rộng chuỗi đầu vào giữa hai lớp bất kỳ bằng các token có thể huấn luyện.

Học Chuyển giao Hiệu quả Tham số
Học Chuyển giao Hiệu quả Tham số (PETL) (Houlsby và cộng sự 2019; Mahabadi và cộng sự 2021; Zhang và cộng sự 2020; Guo, Rush, và Kim 2021; Sung, Nair, và Raffel 2021; Mahabadi, Henderson, và Ruder 2021; Sung, Cho, và Bansal 2022; Hu và cộng sự 2022; He và cộng sự 2022; Mao và cộng sự 2022) nhằm cập nhật một số lượng nhỏ tham số để tiếp cận hiệu suất được tinh chỉnh đầy đủ trên các tác vụ downstream. Ngoài tinh chỉnh prompt, một mô hình phổ biến là các phương pháp dựa trên adapter (Houlsby và cộng sự 2019; Mahabadi và cộng sự 2021; Mahabadi, Henderson, và Ruder 2021; Gao và cộng sự 2021; Zhang và cộng sự 2021; Sung, Cho, và Bansal 2022), được gọi tắt là Adapter, chèn các mạng nhẹ vào mô hình tiền huấn luyện để chiếu các đặc trưng ẩn vào không gian dữ liệu downstream. Để tránh chi phí tính toán bổ sung trong quá trình suy luận, Hu và cộng sự đề xuất một phương pháp thích ứng thứ hạng thấp (LoRA) (Hu và cộng sự 2022), dựa trên việc tái tham số hóa trọng số. Trong lĩnh vực học thị giác-ngôn ngữ, VL-adapter (Sung, Cho, và Bansal 2022) chèn các mạng chiều thấp vào một mô hình ngôn ngữ tiền huấn luyện để thích ứng với các tác vụ VL thông thường (Chen và cộng sự 2015; Goyal và cộng sự 2017; Suhr và cộng sự 2019). Điểm khác biệt chính với bài báo này là mô hình ngôn ngữ không được tiền huấn luyện VL, thiếu khả năng tổng quát hóa đủ cho các mô hình VLP thông thường.

Kiến thức Cơ bản
Trước khi giới thiệu phương pháp của chúng tôi, trước tiên chúng tôi tóm tắt nguyên lý của tinh chỉnh prompt cho các mô hình VLP. Cụ thể, cho một mô hình thị giác-ngôn ngữ (VLP) tiền huấn luyện, ký hiệu là G(·), và ví dụ hình ảnh-văn bản của tác vụ downstream, ký hiệu là (I, T), mục tiêu của tinh chỉnh prompt là tối thiểu hóa mất mát thích ứng với một tập hợp các token prompt P∈Rp×d:

argmin
PL
G(I, T, P |θ+)
, (1)

trong đó θ+ là các trọng số tiền huấn luyện của G và sẽ được cố định trong quá trình tinh chỉnh prompt3. L là hàm mục tiêu của tác vụ downstream.

Xét rằng các tham số được cố định trong quá trình thích ứng, các đặc trưng của chuỗi đầu vào khó được cập nhật cho tác vụ downstream. Trong trường hợp này, các token prompt P thường được sử dụng trong self-attention của các mô hình VLP để khuếch tán thông tin liên quan đến tác vụ đến chuỗi đầu vào X∈Rn×d:

[X′||P′] =SA(X||P), (2)

trong đó SA(·) đại diện cho mô-đun self-attention. X′ và P′ là các đầu ra tương ứng của X và P, tương ứng. Cụ thể, X' và P' được thu được bởi

X′=AIXW v+AIPPW v,
P′=APIXW v+APPW v,(3)

trong đó AI, AIP, API và AP là các ma trận attention phụ, tương ứng với các phần input-only, input2prompt, prompt2input và prompt-only được mô tả trong phần giới thiệu và thể hiện trong Hình 2. Wq, Wk và Wv là các ma trận trọng số của các phép chiếu Q, K, V trong SA.

Dưới thiết lập theo lớp (Jia và cộng sự 2022), các token prompt được khởi tạo cho mỗi lớp và sẽ không được sử dụng trong SA tiếp theo. Trong trường hợp này, việc tính toán P′ thực sự có thể được loại bỏ, có thể giảm độ phức tạp bởi O(2pd2+ 4npd+ 2p2d), trong đó p thường là một giá trị lớn trên các tác vụ VL.

Cuối cùng, việc cập nhật đặc trưng của các mô hình VLP với các token prompt có thể được nới lỏng thành

X′=AIXW v+AIPPW v
=γI
γI+γIPσ(XW q(XW k)T
√
d)XW v
+γIP
γI+γIPσ(XW q(PW k)T
√
d)PW v,

trong đó γI=X
eQKT
i, γIP=X
eQPkT
j(4)

Ở đây, σ(·) là hàm Softmax, và γI và γIP là tỷ lệ attention cho chuỗi đầu vào và các token prompt, tương ứng. Trong Phương trình 4, số hạng đầu tiên là cập nhật self-attention của các đặc trưng đầu vào, đây là hoạt động bắt buộc của các mô hình VLP. Cuối cùng, hiệu quả của tinh chỉnh prompt nằm ở số hạng thứ hai, về bản chất là một bước khuếch tán thông tin có trọng số từ P đến X. Vì tích số thang điểm vẫn được yêu cầu, bước khuếch tán này cũng tốn kém.

Tinh chỉnh Prompt Xấp xỉ
Dựa trên quan sát trên, chúng tôi đề xuất tinh chỉnh prompt xấp xỉ (APT) để mô hình hóa các tác động attention của các token prompt. Cụ thể, chúng tôi có thể có được quá trình tinh chỉnh prompt của APT với công thức sau:

X′=SA(X) +APT (X,P). (5)

--- TRANG 4 ---
Để đơn giản, chúng tôi coi việc khuếch tán thông tin từ P đến X là ∆X:

∆X=APT (X,P)
=γIP
γI+γIPσ(XW q(PW k)T)PW v.(6)

Để xấp xỉ ∆X, trước tiên chúng tôi tập trung vào việc tổng hợp thông tin của các token prompt, ký hiệu là ∆X′:

∆X′=σ(XW qWT
kPT)PW v. (7)

Lưu ý rằng, Wv được cố định trong SA, và P là một ma trận có thể huấn luyện. Cuối cùng, chúng tôi có thể trực tiếp cập nhật phép chiếu của các token prompt vào không gian con V, tức là đặt PW v là P′. Tương tự, chúng tôi có thể đơn giản hóa PW kWT
q là K. Sau đó, X có thể được lấy trực tiếp làm Q mà không cần phép chiếu, và việc tính toán chuyển đổi X và P thành Q, K và V của SA có thể được tiết kiệm.

Tiếp theo, chúng tôi chỉ ra rằng V có thể được biến đổi tuyến tính thành K:

∆P=PW kWT
q−PW v,
=P(WkWT
q−Wv).(8)

Ở đây, ∆P biểu thị sự khác biệt giữa V và K. Vì V có thể được biến đổi thành K bằng một phép biến đổi tuyến tính, chúng tôi xấp xỉ Phương trình 7 là

∆X′=σ
X(P′Wp+P′)T
P′, (9)

trong đó Wp∈Rd×d nhằm biến đổi các token prompt từ V sang K. Tuy nhiên, việc tính toán PW p vẫn không rẻ do chiều đặc trưng cao.

Vì thành phần chiều nội tại thấp (Li và cộng sự 2018; Aghajanyan, Gupta, và Zettlemoyer 2021) đóng vai trò chủ đạo trong tối ưu hóa mô hình, thứ hạng cho Wp là hữu hạn theo định lý về thứ hạng của ma trận:

rank(Wp)≤rank(WkWT
q) +rank(Wv), (10)

trong đó rank(·) là thứ hạng của ma trận. Chúng tôi có thể xấp xỉ việc tổng hợp các token prompt theo cách thứ hạng thấp:

∆X′=σ
X(P′W1W2+P′)T
P′. (11)

Ở đây, W1∈Rd×r và W2∈Rr×d là hai ma trận chiều thấp, trong đó r≪d. Thứ hạng của ma trận chiếu W1W2 bị giới hạn bởi r. Cách chúng tôi thu được các ma trận Q, K và V cho mô hình hóa attention rẻ hơn so với attention toàn cục ban đầu.

Sau đó, chúng tôi xem xét cách hợp nhất đầu ra ban đầu của mô-đun self-attention SA(X) và thông tin của các token prompt ∆X. Vì việc tính toán attention vẫn liên quan đến chuỗi đầu vào, khó giảm độ phức tạp của xấp xỉ thông qua tính toán độc lập. Trong trường hợp này, để thích ứng cách thích ứng tác động của mỗi token prompt, một giải pháp đơn giản là kích hoạt ma trận attention bằng ReLU thay vì Softmax và bỏ qua mục trọng số. Sau đó, Phương trình 6 có thể được biểu diễn là

∆X=ψ
X(P′W1W2+P′)T
P′, (12)

trong đó ψ(·) đại diện cho kích hoạt ReLU. Theo cách này, trọng số cho các prompt phụ thuộc vào chuẩn của các token prompt và mối quan hệ của chúng với chuỗi đầu vào.

Hơn nữa, từ việc tính toán trọng số trong Phương trình 6, chúng tôi quan sát thấy rằng hiệu ứng của các token prompt không chỉ bị ảnh hưởng bởi sự phụ thuộc của chúng vào chuỗi đầu vào, mà còn bởi tổng các attention đến chuỗi đầu vào. Với tính chất nội tại của hàm Softmax rằng giá trị tối đa có tác động nhất, chúng tôi định nghĩa lại Phương trình 6 bởi

∆X=α·ψ(X(P′W1W2+P′)T)P′,
trong đó α=max{P′W1W2+P′},(13)

trong đó max{·} là hàm tối đa cho trọng số của mỗi token. Do đó, APT có thể điều chỉnh toàn cục việc khuếch tán thông tin từ các token prompt. Vì hàm kích hoạt của Phương trình 13 không còn dựa vào ma trận attention ban đầu, APT dễ triển khai hơn cho các mô hình VLP.

Đến nay, chúng tôi đã xem xét đầy đủ hiệu ứng của các token prompt trong việc khuếch tán thông tin liên quan đến tác vụ đến chuỗi đầu vào. Sau đó, chúng tôi cũng tính đến hiệu ứng của các token prompt trên ma trận attention ban đầu. Như thể hiện trong Phương trình 4, việc khuếch tán thông tin cũng ảnh hưởng đến ma trận attention ban đầu bằng cách tăng mẫu số của trọng số cho mục từ mô-đun VLP. Cuối cùng, chúng tôi thêm một thang điểm có thể học s cho toàn bộ đầu ra, và APT được đề xuất có thể được tóm tắt như sau:

X′=AIXW v+AIPPW v
≈es·
SA(X) +α·ψ(X(P′W1W2+P′)T)P′
,
trong đó α=max{P′W1W2+P′}.
(14)

Ở đây, giá trị có thể học s kiểm soát tổng lượng thông tin được khuếch tán bởi APT và cũng làm cho đầu ra của các mô-đun attention phù hợp với các lớp sau.

Cuối cùng, phương pháp APT được đề xuất tách biệt hiệu ứng của các token prompt khỏi mô-đun attention ban đầu. Tính độc lập của APT mang lại hai lợi ích chính: (1) Khuếch tán thông tin có thể phá vỡ giới hạn của các mẫu từ mô hình VLP, tức là không bị ràng buộc bởi chuẩn hóa dựa trên Softmax. (2) Chi phí tính toán được giảm đáng kể khoảng O(2pd2). Trong thực tế, nó có thể tiết kiệm khoảng 82,30% và 62,62% tính toán cho ViLT (Kim, Son, và Kim 2021) và METER (Dou và cộng sự 2022) so với các phương pháp tinh chỉnh prompt thông thường.

Thí nghiệm
Bộ dữ liệu và Thiết lập Thí nghiệm
Bộ dữ liệu và Metric. VQA2.0 (Goyal và cộng sự 2017) là một trong những bộ dữ liệu phổ biến nhất cho tác vụ trả lời câu hỏi thị giác (VQA). Nó sử dụng hình ảnh từ MS-COCO (Ren, Kiros, và Zemel 2015) và có khoảng 443.757, 214.254 và 447.793 ví dụ VQA cho huấn luyện, xác thực và kiểm tra, tương ứng. NLVR2 (Suhr và cộng sự 2019) được xây dựng cho lý luận thị giác. Nó chứa 107.292 ví dụ về các câu tiếng Anh được viết bởi con người cho các cặp ảnh. Flickr30k (Plummer và cộng sự 2017) là một bộ dữ liệu benchmark được sử dụng rộng rãi trong tác vụ kết hợp hình ảnh-văn bản này. Bộ dữ liệu bao gồm 31.783 hình ảnh, và mỗi hình có năm chú thích tương ứng. Đối với CLIP, chúng tôi xác thực APT trên 11 bộ dữ liệu phân loại hình ảnh phổ biến, bao gồm ImageNet (Deng và cộng sự 2009), Caltech101 (Fei-Fei,

--- TRANG 5 ---
Bảng 1: So sánh APT và các phương pháp tinh chỉnh prompt thông thường cho ViLT và METER trên VQA, NLVR2 và Flickr30K. Hiệu suất tốt nhất được in đậm trong khi hiệu suất thứ hai được gạch chân.

[Bảng dữ liệu so sánh hiệu suất các phương pháp]

Hình 3: Trực quan hóa kết quả attention của shallow prompt, deep prompt và APT của chúng tôi với ViLT trên bộ dữ liệu VQA2.0. Màu sắc biểu thị mức độ attention, màu đỏ càng đậm thì mức độ càng cao và ngược lại. So với shallow prompt và deep prompt, APT có thể khuếch tán thông tin prompt hiệu quả hơn đến chuỗi đầu vào từ các lớp thấp của ViLT, xem các mũi tên đỏ.

Fergus, và Perona 2007), OxfordPets (Parkhi và cộng sự 2012), StandfordCars (Krause và cộng sự 2013), Flowers102 (Nilsback và Zisserman 2008), Food101 (Bossard, Guillaumin, và Gool 2014), FGVCAircraft (Maji và cộng sự 2013), SUN397 (Xiao và cộng sự 2010), DTD (Cimpoi và cộng sự 2014), EuroSAT (Helber và cộng sự 2019), UCF101 (Soomro, Zamir, và Shah 2012)4. Benchmark toàn diện này bao gồm các bộ dữ liệu che phủ một tập hợp đa dạng các tác vụ thị giác, bao gồm phân loại về đối tượng chung, cảnh, hành động và các danh mục tinh vi. Nó cũng bao gồm các tác vụ chuyên biệt như nhận dạng kết cấu và hình ảnh vệ tinh.

Chi tiết thực hiện. Chúng tôi xác thực APT trên hai mô hình VLP dựa trên deep-fusion, đó là ViLT (Kim, Son, và Kim 2021) và METER (Dou và cộng sự 2022), và một mạng VLP dựa trên shallow-fusion gọi là CLIP (Radford và cộng sự 2021). Đối với ViLT, chúng tôi thêm APT vào mỗi lớp SA của nó. Chúng tôi đặt giá trị thứ hạng r trong Phương trình 11 là 4 và số lượng token prompt p = 200 làm thiết lập mặc định. Các token prompt được khởi tạo bằng phân phối chuẩn với trung bình 0,0 và phương sai 0,02. Và chúng tôi chỉ áp dụng một attention đơn thay vì multi-head (Devlin và cộng sự 2019) cho phương pháp APT được đề xuất. Trong quá trình huấn luyện, chúng tôi cập nhật classifier, class token và modal-type embedding, trong khi các tham số còn lại của ViLT được giữ cố định. Đối với mỗi tác vụ, chúng tôi tuân theo các thiết lập mặc định của nó và tăng learning rate lên năm lần. Đối với METER, APT được chèn vào các lớp self-attention và cross-attention của nó. Các thiết lập còn lại giống như ViLT. Đối với CLIP (Radford và cộng sự 2021), chúng tôi chèn APT vào các lớp self-attention của text encoder của nó, và chúng tôi đặt thứ hạng r = 2 và số lượng prompt p = 4. APT được tối ưu hóa bằng SGD với learning rate 2×10−4 và weight decay 0,3 trong 10 epoch. Theo (Radford và cộng sự 2021), chúng tôi cũng sử dụng một cụm từ prompt cứng "a photo of [X]", được đưa vào text encoder của CLIP.

Kết quả thí nghiệm
So sánh với các phương pháp tinh chỉnh prompt. Trước tiên chúng tôi so sánh APT với hai phương pháp tinh chỉnh prompt mềm phổ biến, tức là deep prompt (Jia và cộng sự 2022) và shallow prompt (Li và Liang 2021), trong Bảng 1. Đối với tất cả các phương pháp, số lượng prompt được đặt là 200 để so sánh công bằng. Từ Bảng 1, hiệu suất của các phương pháp tinh chỉnh prompt hiện có rất tụt hậu so với phương pháp tinh chỉnh đầy đủ, tức là −7,90% đến −3,62% trên ViLT và −11,26% đến −10,00% trên METER. Những kết quả này cũng tệ hơn so với hiệu suất của chúng trên NLP (Li và Liang 2021) và các tác vụ thị giác (Jia và cộng sự 2022), cho thấy thách thức của tinh chỉnh prompt trên các mô hình VLP. Trong số các phương pháp so sánh này, Deep Prompt cho thấy kết quả tốt hơn so với shallow prompt trong hầu hết các trường hợp, trong khi kích thước tham số của nó lớn hơn và tương tự như APT. Đáng chú ý, các cải thiện trung bình của APT so với các phương pháp prompt này là +2,73% đến +7,01% trên ViLT và +8,30% đến +9,56% trên METER, tương ứng, trong khi tính toán bổ sung được tiết kiệm có thể lên đến 82,30% trên ViLT và 91,95% trên METER. Đồng thời, hiệu suất của APT gần như tiếp cận tinh chỉnh đầy đủ, ví dụ −0,89% và −1,70% trung bình cho ViLT và METER, tương ứng. Xem xét số lượng nhỏ tham số được cập nhật, những kết quả này thực sự có ý nghĩa.

Để có được các so sánh trực quan hơn, chúng tôi cũng trực quan hóa các attention của các phương pháp tinh chỉnh prompt này trong Hình 3. Trong

--- TRANG 6 ---
Bảng 2: So sánh APT và các phương pháp PETL hiện đại cho ViLT và METER trên VQA, NLVR2 và Flickr30K. Hiệu suất tốt nhất được in đậm và tốt thứ hai được gạch chân.

[Bảng dữ liệu so sánh hiệu suất]

Hình 4: So sánh giữa APT và các phương pháp PETL khác về hiệu suất và kích thước tham số. APT có sự cân bằng tốt hơn giữa hiệu suất và chi phí tham số.

hình này, chúng tôi chọn 15 token hoạt động nhất của đầu vào thị giác và văn bản, và 30 token prompt hàng đầu để trực quan hóa, tổng cộng 60 token. Các ma trận attention toàn cục có thể được chia thành sáu phần phụ, tức là Text-Text, Text-Image, Text-Prompt, Image-Text, Image-Image, Image-Prompt. Từ những ví dụ này, trước tiên chúng ta có thể quan sát thấy rằng trong các lớp thấp hơn của mô hình VLP, việc trao đổi thông tin chủ yếu xảy ra giữa các token của cùng một phương thức, và các prompt hầu như không ảnh hưởng đến chuỗi đầu vào. Khi quá trình suy luận tiến triển, tác động của các prompt thông thường trở nên rõ ràng hơn một chút. Đối với shallow prompt, tác động của các token của nó vẫn còn ít, trong khi deep prompt sẽ tốt hơn ở một vài lớp cuối của mô hình. Các kết quả trên cũng phù hợp với hiệu suất của chúng trên các tác vụ VL. Ngược lại, APT có thể khuếch tán thông tin prompt hiệu quả đến chuỗi đầu vào của các mô hình VLP, xem các mũi tên. Và trọng số attention của nó trở nên tập trung hơn trong các lớp cao hơn, cho thấy hiệu quả của nó đối với thích ứng tác vụ.

So sánh với các phương pháp PETL hiện có. Tiếp theo, chúng tôi so sánh APT với một loạt các phương pháp PETL, bao gồm LoRA (Hu và cộng sự 2022), VL-Adapter (Adapter) (Sung, Cho, và Bansal 2022) và Scaled Parallel Adapter (Scaled PA) (He và cộng sự 2022), kết quả được đưa ra trong Bảng 25. Từ bảng này, trước tiên chúng ta có thể thấy rằng LoRA hiệu quả nhất về cả tham số và tính toán do sơ đồ tái tham số hóa thứ hạng thấp của nó. So với các mô hình ngôn ngữ tiền huấn luyện (Liu và cộng sự 2019; Brown và cộng sự 2020), hiệu suất của nó trên các mô hình VLP kém hơn nhiều, đặc biệt là trên các tác vụ rất khác biệt so với tiền huấn luyện, ví dụ VQA và NLVR2, cho thấy thách thức của thích ứng VL. Chúng ta cũng có thể thấy rằng mặc dù các phương pháp dựa trên adapter cho thấy khả năng thích ứng tốt hơn so với LoRA, chúng vẫn hoạt động kém hơn APT của chúng tôi. So với VL-Adapter, APT có thể đạt được những cải thiện rõ ràng trên ViLT và METER, trong khi tiết kiệm khoảng 46,07% và 28,28% tham số, tương ứng. Đối với Scaled PA tiên tiến nhất, APT kém hơn một chút về chi phí tham số và tính toán, nhưng hiệu suất thích ứng của nó luôn tốt hơn so với Scaled PA trên hai mô hình VLP. Nhìn chung, những kết quả này cho thấy rằng APT của chúng tôi là một phương pháp cạnh tranh trong PETL với tiềm năng lớn.

Trong Hình 4, chúng tôi cũng trình bày so sánh hiệu suất của APT với các phương pháp PETL khác với chi phí tham số khác nhau. Có thể thấy rằng Deep Prompt kém hơn nhiều so với các phương pháp khác về hiệu quả tham số và hiệu suất, cho thấy khó khăn của nó trong thích ứng VL. Adapter (Sung, Cho, và Bansal 2022) và Scaled PA (He và cộng sự 2022), là các phương pháp PETL tiên tiến, đều hiệu quả về tham số, và khả năng thích ứng của chúng cũng hợp lý trên VQA. Tuy nhiên, hiệu suất tổng thể của hai phương pháp này gần nhau, điều này nằm ngoài mong đợi. So với các phương pháp dựa trên adapter này, hiệu suất của APT có thể đạt được những cải thiện rõ ràng ở quy mô khoảng 2M tham số, trở nên ổn định khi kích thước tham số tăng.

Nghiên cứu Ablation. Trước tiên chúng tôi kiểm tra tác động của prompt

--- TRANG 7 ---
Bảng 3: Nghiên cứu ablation về các cấu trúc khác nhau và số lượng token prompt. *thiết lập mặc định.

[Bảng dữ liệu ablation study]

Bảng 4: So sánh zero-shot CLIP (CLIP), CoOp, CoCoOp và APT trên tác vụ phân loại base to new.

[Bảng dữ liệu so sánh trên các datasets]

số và giá trị thứ hạng trong Phương trình 11, kết quả được đưa ra trong Bảng 3. Ở đây, "identity" biểu thị trực tiếp sử dụng các token prompt làm K và V trong Phương trình 9, trong khi "dense" có nghĩa là phép biến đổi thứ hạng thấp không được sử dụng trong Phương trình 11. Quan sát đầu tiên từ Bảng 3 là việc tăng các token prompt có lợi cho các mô hình VLP, có thể thu được cải thiện trên cả hai tác vụ, ví dụ từ 100 đến 200. Tuy nhiên, khi vượt quá 200, lợi ích của nó là ít ỏi so với các phương pháp prompt khác như thể hiện trong Hình 4, điều này cũng cho thấy hiệu quả của APT đối với các mô hình VLP. Về giá trị thứ hạng, hiệu suất của "identity" cho thấy rằng việc trực tiếp sử dụng các token prompt cho attention không tối ưu. Và xấp xỉ thứ hạng thấp có thể cân bằng tốt hơn giữa hiệu suất và chi phí tham số, ví dụ giá trị thứ hạng r = 4, thậm chí vượt trội hơn so với phép biến đổi dense trên NLVR2. Nhìn chung, những kết quả này xác nhận tốt hiệu quả của APT đối với thích ứng VL hiệu quả.

Khả năng tổng quát hóa trên CLIP. Chúng tôi tiếp tục kiểm tra khả năng tổng quát hóa của APT trên mô hình VLP dựa trên shallow-fusion, tức là CLIP (Radford và cộng sự 2021), dưới tác vụ phân loại base-to-new (Zhou và cộng sự 2022a), kết quả được đưa ra trong Bảng 4. Trong tác vụ này, mô hình cần thích ứng với bộ dữ liệu base, và sẽ được đánh giá thêm trên dữ liệu chưa thấy (bộ dữ liệu new). Các phương pháp so sánh bao gồm zero-shot CLIP, CoOp (Zhou và cộng sự 2022b) và CoCoOp (Zhou và cộng sự 2022a). Từ bảng này, trước tiên chúng ta quan sát thấy rằng zero-shot CLIP có khả năng học chuyển giao mạnh. Do tiền huấn luyện quy mô lớn, nó có thể thu được hiệu suất vượt trội dưới các đánh giá tác vụ mới. Tuy nhiên, mà không tinh chỉnh trên các bộ dữ liệu base, hiệu suất của nó kém hơn nhiều so với các phương pháp PETL. Đối với CoOp, nó có thể đạt được hiệu suất thỏa đáng cho thích ứng tác vụ base. Tuy nhiên, khả năng tổng quát hóa của nó bị hạn chế đối với các tác vụ mới, chỉ 63,22% trung bình, cho thấy vấn đề over-fitting. Ngược lại, APT có thể thu được hiệu suất tốt trong việc thích ứng các tác vụ base trong khi tổng quát hóa tốt cho các tác vụ mới. So với các phương pháp PETL mới nhất cho CLIP, tức là CoCoOp, hiệu suất của nó cũng luôn tốt hơn dưới hai thiết lập. Những kết quả này xác nhận khả năng tổng quát hóa của APT.

Khả năng tổng quát hóa trên StableDiffusion. Chúng tôi cũng kiểm tra khả năng tổng quát hóa của APT trên StableDiffusion (Rombach và cộng sự 2022) theo thiết lập của DreamBooth (Ruiz và cộng sự 2022), và phương pháp so sánh là LoRA, kết quả được đưa ra trong Hình 5. Từ hình ảnh, chúng ta có thể thấy rằng những con chó được tạo ra với các prompt khác nhau có thể giữ cùng một thuộc tính. Tương tự như LoRA, APT liên kết các thuộc tính của con chó trong tập huấn luyện với từ vựng cụ thể "sks". Những kết quả này cho thấy rằng APT cũng có khả năng tạo ảnh từ văn bản.

Kết luận
Trong bài báo này, chúng tôi tập trung vào các vấn đề về chi phí tính toán cao và thích ứng không hiệu quả của tinh chỉnh prompt trên các mô hình tiền huấn luyện thị giác-ngôn ngữ (VLP). Bằng cách xem xét lại nguyên lý của tinh chỉnh prompt, chúng tôi có thể hiểu rằng chìa khóa để cải thiện tinh chỉnh prompt nằm ở việc khuếch tán thông tin của nó đến chuỗi đầu vào, thực sự có thể độc lập với self-attention toàn cục tốn kém thông qua các xấp xỉ hiệu quả. Được thúc đẩy bởi quan sát này, chúng tôi đề xuất một phương pháp tinh chỉnh prompt xấp xỉ (APT) mới hướng tới thích ứng VL hiệu quả. APT xấp xỉ các tác động của các token prompt đến chuỗi đầu vào thông qua thiết kế tổng hợp token thứ hạng thấp, giảm chi phí tính toán ở mức độ lớn. Chúng tôi xác thực APT trên 2 mô hình VLP và 3 benchmark VL, và cũng tổng quát VPT cho CLIP cho phân loại hình ảnh và StableDiffusion cho tạo ảnh theo chủ đề. Kết quả định lượng và định tính không chỉ cho thấy ưu điểm rõ ràng của APT so với các phương pháp tinh chỉnh prompt hiện có về cả hiệu quả tính toán và hiệu suất, mà còn vượt trội hơn các phương pháp PETL so sánh trên các tác vụ VL này.

--- TRANG 8 ---
Tài liệu tham khảo
[Danh sách các tài liệu tham khảo được giữ nguyên bằng tiếng Anh theo yêu cầu]
