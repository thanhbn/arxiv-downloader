# Báo cáo Kỹ thuật
LCM-L ORA: MỘT MÔ-ĐUN TĂNG TỐC STABLE-DIFFUSION
PHỔ QUÁT

Simian Luo∗,1Yiqin Tan∗,1Suraj Patil†,2Daniel Gu†Patrick von Platen2
Apolin ´ario Passos2Longbo Huang1Jian Li1Hang Zhao1
1IIIS, Đại học Thanh Hoa2Hugging Face
{luosm22, tyq22 }@mails.tsinghua.edu.cn
{suraj, patrick, apolinario }@huggingface.co
{dgu8957 }@gmail.com
{longbohuang, lijian83, hangzhao }@tsinghua.edu.cn

TÓM TẮT

Các Mô hình Nhất quán Ẩn (LCMs) (Luo et al., 2023) đã đạt được hiệu suất ấn tượng trong việc tăng tốc các tác vụ tạo sinh văn bản thành hình ảnh, tạo ra những hình ảnh chất lượng cao với số bước suy luận tối thiểu. LCMs được chưng cất từ các mô hình khuếch tán ẩn được huấn luyện trước (LDMs), chỉ yêu cầu khoảng 32 giờ huấn luyện GPU A100. Báo cáo này mở rộng thêm tiềm năng của LCMs theo hai khía cạnh: Thứ nhất, bằng cách áp dụng chưng cất LoRA cho các mô hình Stable-Diffusion bao gồm SD-V1.5 (Rombach et al., 2022), SSD-1B (Segmind., 2023), và SDXL (Podell et al., 2023), chúng tôi đã mở rộng phạm vi của LCM tới các mô hình lớn hơn với lượng tiêu thụ bộ nhớ ít hơn đáng kể, đạt được chất lượng tạo hình ảnh vượt trội. Thứ hai, chúng tôi xác định các tham số LoRA thu được thông qua chưng cất LCM như một mô-đun tăng tốc Stable-Diffusion phổ quát, được đặt tên là LCM-LoRA. LCM-LoRA có thể được cắm trực tiếp vào các mô hình Stable-Diffusion được tinh chỉnh khác nhau hoặc LoRAs mà không cần huấn luyện, do đó đại diện cho một bộ tăng tốc có thể áp dụng phổ quát cho các tác vụ tạo hình ảnh đa dạng. So với các bộ giải PF-ODE số trước đây như DDIM (Song et al., 2020), DPM-Solver (Lu et al., 2022a;b), LCM-LoRA có thể được xem như một bộ giải PF-ODE thần kinh cắm-và-sử-dụng có khả năng tổng quát hóa mạnh mẽ. Trang dự án: https://github.com/luosiallen/latent-consistency-model.

1 GIỚI THIỆU

Các Mô hình Khuếch tán Ẩn (LDMs) (Rombach et al., 2022) đã đóng vai trò quan trọng trong việc tạo ra những hình ảnh chi tiết cao và sáng tạo từ các đầu vào đa dạng như văn bản và phác thảo. Mặc dù thành công, quá trình lấy mẫu ngược chậm chạp vốn có của LDMs cản trở ứng dụng thời gian thực, làm giảm trải nghiệm người dùng. Các mô hình mã nguồn mở hiện tại và các kỹ thuật tăng tốc vẫn chưa thể lấp đầy khoảng cách tới việc tạo sinh thời gian thực trên GPU tiêu dùng tiêu chuẩn. Các nỗ lực tăng tốc LDMs thường rơi vào hai loại: loại thứ nhất liên quan đến các ODE-Solver tiên tiến, như DDIM (Song et al., 2020), DPM-Solver (Lu et al., 2022a) và DPM-Solver++ (Lu et al., 2022b), để đẩy nhanh quá trình tạo sinh. Chiến lược thứ hai liên quan đến chưng cất LDMs để hợp lý hóa hoạt động của chúng. Các phương pháp ODE-Solver, mặc dù giảm số bước suy luận cần thiết, vẫn đòi hỏi chi phí tính toán đáng kể, đặc biệt khi kết hợp hướng dẫn không phân loại (Ho & Salimans, 2022). Trong khi đó, các phương pháp chưng cất như Guided-Distill (Meng et al., 2023), mặc dù đầy hứa hẹn, đối mặt với những hạn chế thực tế do yêu cầu tính toán chuyên sâu của chúng. Việc tìm kiếm sự cân bằng giữa tốc độ và chất lượng trong hình ảnh được tạo bởi LDM vẫn tiếp tục là một thách thức trong lĩnh vực này.

Gần đây, Các Mô hình Nhất quán Ẩn (LCMs) (Luo et al., 2023) đã xuất hiện, được lấy cảm hứng từ Các Mô hình Nhất quán (CMs) (Song et al., 2023), như một giải pháp cho vấn đề lấy mẫu chậm trong tạo hình ảnh. LCMs tiếp cận quá trình khuếch tán ngược bằng cách coi nó như một vấn đề ODE dòng xác suất mở rộng (PF-ODE). Chúng dự đoán một cách đổi mới giải pháp trong không gian ẩn, bỏ qua nhu cầu cho các giải pháp lặp lại thông qua các ODE-Solver số. Điều này dẫn đến một sự tổng hợp cực kỳ hiệu quả của hình ảnh độ phân giải cao, chỉ cần 1 đến 4 bước suy luận. Ngoài ra, LCMs nổi bật về mặt hiệu quả chưng cất, chỉ yêu cầu 32 giờ huấn luyện A100 cho suy luận bước tối thiểu.

Dựa trên điều này, Tinh chỉnh Nhất quán Ẩn (LCF) (Luo et al., 2023) đã được phát triển như một phương pháp để tinh chỉnh LCMs được huấn luyện trước mà không cần bắt đầu từ mô hình khuếch tán giáo viên. Đối với các tập dữ liệu chuyên biệt—như những tập dữ liệu cho anime, hình ảnh thực tế hoặc fantasy—các bước bổ sung là cần thiết, chẳng hạn như sử dụng Chưng cất Nhất quán Ẩn (LCD) (Luo et al., 2023) để chưng cất một LDM được huấn luyện trước thành một LCM hoặc trực tiếp tinh chỉnh một LCM sử dụng LCF. Tuy nhiên, việc huấn luyện bổ sung này có thể là rào cản cho việc triển khai nhanh chóng LCMs trên các tập dữ liệu đa dạng, đặt ra câu hỏi quan trọng liệu suy luận nhanh, không cần huấn luyện trên các tập dữ liệu tùy chỉnh có thể đạt được không.

Để trả lời câu hỏi trên, chúng tôi giới thiệu LCM-LoRA, một mô-đun tăng tốc phổ quát không cần huấn luyện có thể được cắm trực tiếp vào các mô hình Stable-Diffusion (SD) (Rombach et al., 2022) được tinh chỉnh khác nhau hoặc SD LoRAs (Hu et al., 2021) để hỗ trợ suy luận nhanh với các bước tối thiểu. So với các bộ giải ODE dòng xác suất (PF-ODE) số trước đây như DDIM (Song et al., 2020), DPM-Solver (Lu et al., 2022a), và DPM-Solver++ (Lu et al., 2022b), LCM-LoRA đại diện cho một lớp mới của mô-đun bộ giải PF-ODE dựa trên mạng thần kinh. Nó thể hiện khả năng tổng quát hóa mạnh mẽ trên các mô hình SD được tinh chỉnh khác nhau và LoRAs.

2 CÔNG TRÌNH LIÊN QUAN

Các Mô hình Nhất quán Song et al. (2023) đã thể hiện tiềm năng đáng chú ý của các mô hình nhất quán (CMs), một lớp mô hình tạo sinh mới nâng cao hiệu quả lấy mẫu mà không hy sinh chất lượng đầu ra. Những mô hình này sử dụng kỹ thuật ánh xạ nhất quán một cách khéo léo ánh xạ các điểm dọc theo quỹ đạo Phương trình Vi phân Thường (ODE) tới nguồn gốc của chúng, do đó cho phép tạo sinh một bước nhanh chóng. Nghiên cứu của họ đặc biệt nhắm vào các tác vụ tạo hình ảnh trên ImageNet 64x64 (Deng et al., 2009) và LSUN 256x256 (Yu et al., 2015), thể hiện hiệu quả của CMs trong những lĩnh vực này. Tiến xa hơn trong lĩnh vực này, Luo et al. (2023) đã tiên phong các mô hình nhất quán ẩn (LCMs) trong bối cảnh tổng hợp văn bản thành hình ảnh. Bằng cách xem quá trình khuếch tán ngược có hướng dẫn như việc giải quyết một ODE Dòng Xác suất mở rộng (PF-ODE), LCMs một cách khéo léo dự đoán giải pháp của những ODE như vậy trong không gian ẩn. Cách tiếp cận đổi mới này giảm đáng kể nhu cầu cho các bước lặp lại, do đó cho phép tạo sinh nhanh chóng hình ảnh độ trung thực cao từ đầu vào văn bản và thiết lập một tiêu chuẩn mới cho hiệu suất tốt nhất trên tập dữ liệu LAION-5B-Aesthetics (Schuhmann et al., 2022).

Tinh chỉnh Hiệu quả Tham số Tinh chỉnh Hiệu quả Tham số (PEFT) (Houlsby et al., 2019) cho phép tùy chỉnh các mô hình có sẵn cho các tác vụ cụ thể trong khi giới hạn số lượng tham số cần huấn luyện lại. Điều này giảm cả tải tính toán và nhu cầu lưu trữ. Trong số các kỹ thuật đa dạng dưới ô PEFT, Thích ứng Thứ hạng Thấp (LoRA) (Hu et al., 2021) nổi bật. Chiến lược của LoRA liên quan đến việc huấn luyện một tập hợp tham số tối thiểu thông qua tích hợp các ma trận thứ hạng thấp, một cách ngắn gọn biểu diễn các điều chỉnh cần thiết trong trọng số của mô hình để tinh chỉnh. Trong thực tế, điều này có nghĩa là trong quá trình tối ưu hóa cụ thể cho tác vụ, chỉ những ma trận này được học và phần lớn trọng số được huấn luyện trước được giữ nguyên. Do đó, LoRA giảm đáng kể khối lượng tham số cần được sửa đổi, do đó nâng cao hiệu quả tính toán và cho phép tinh chỉnh mô hình với ít dữ liệu hơn đáng kể.

Số học Tác vụ trong Các Mô hình Được Huấn luyện Trước Số học tác vụ (Ilharco et al., 2022; Ortiz-Jimenez et al., 2023; Zhang et al., 2023) đã trở thành một phương pháp đáng chú ý để nâng cao khả năng của các mô hình được huấn luyện trước, cung cấp một chiến lược hiệu quả về chi phí và có thể mở rộng cho chỉnh sửa trực tiếp trong không gian trọng số. Bằng cách áp dụng trọng số được tinh chỉnh của các tác vụ khác nhau vào một mô hình, các nhà nghiên cứu có thể cải thiện hiệu suất của nó trên những tác vụ này hoặc gây ra quên lãng bằng cách phủ định chúng. Mặc dù có triển vọng, việc hiểu tiềm năng đầy đủ của số học tác vụ và các nguyên tắc cơ bản của nó vẫn là những lĩnh vực khám phá tích cực.

3 LCM-LORA

3.1 CHƯNG CẤT LORA CHO LCM

Mô hình Nhất quán Ẩn (LCM) (Luo et al., 2023) được huấn luyện sử dụng phương pháp chưng cất có hướng dẫn một giai đoạn, tận dụng không gian ẩn của một bộ mã hóa-giải mã được huấn luyện trước để chưng cất một mô hình khuếch tán có hướng dẫn thành một LCM. Quá trình này liên quan đến việc giải quyết một ODE Dòng Xác suất mở rộng (PF-ODE), một công thức toán học đảm bảo các mẫu được tạo ra theo một quỹ đạo dẫn đến hình ảnh chất lượng cao. Việc chưng cất tập trung vào duy trì độ trung thực của những quỹ đạo này trong khi giảm đáng kể số lượng bước lấy mẫu cần thiết. Phương pháp bao gồm các đổi mới như kỹ thuật Bỏ qua Bước để đẩy nhanh hội tụ. Mã giả của LCD được cung cấp trong Thuật toán 1.

Vì quá trình chưng cất của Các Mô hình Nhất quán Ẩn (LCM) được thực hiện dựa trên các tham số từ một mô hình khuếch tán được huấn luyện trước, chúng ta có thể xem chưng cất nhất quán ẩn như một quá trình tinh chỉnh cho mô hình khuếch tán. Điều này cho phép chúng ta sử dụng các phương pháp tinh chỉnh hiệu quả tham số, như LoRA (Thích ứng Thứ hạng Thấp) (Hu et al., 2021). LoRA cập nhật một ma trận trọng số được huấn luyện trước bằng cách áp dụng một phân tích thứ hạng thấp. Cho một ma trận trọng số W0∈Rd×k, việc cập nhật được biểu diễn như W0+ ∆W=W0+BA, trong đó B∈Rd×r,A∈Rr×k, và thứ hạng r≤min(d, k). Trong quá trình huấn luyện, W0 được giữ không đổi, và các cập nhật gradient chỉ được áp dụng cho A và B. Lượt truyền tiến được sửa đổi cho một đầu vào x là:

h=W0x+ ∆Wx=W0x+BAx. (1)

Trong phương trình này, h biểu diễn vector đầu ra, và các đầu ra của W0 và ∆W=BA được cộng lại với nhau sau khi được nhân với đầu vào x. Bằng cách phân tích ma trận tham số đầy đủ thành tích của hai ma trận thứ hạng thấp, LoRA giảm đáng kể số lượng tham số có thể huấn luyện, do đó giảm sử dụng bộ nhớ. Bảng 3.1 so sánh tổng số tham số trong mô hình đầy đủ với các tham số có thể huấn luyện khi sử dụng kỹ thuật LoRA. Rõ ràng là bằng cách kết hợp kỹ thuật LoRA trong quá trình chưng cất LCM, số lượng tham số có thể huấn luyện được giảm đáng kể, hiệu quả giảm yêu cầu bộ nhớ cho huấn luyện.

Luo et al. (2023) chủ yếu chưng cất mô hình stable diffusion cơ bản, như SD-V1.5 và SD-V2.1. Chúng tôi mở rộng quá trình chưng cất này tới các mô hình mạnh mẽ hơn với khả năng văn bản thành hình ảnh nâng cao và số lượng tham số lớn hơn, bao gồm SDXL (Podell et al., 2023) và SSD-1B (Segmind., 2023). Các thí nghiệm của chúng tôi chứng minh rằng mô hình LCD thích ứng tốt với các mô hình lớn hơn. Kết quả được tạo ra của các mô hình khác nhau được hiển thị trong Hình 2.

3.2 LCM-LORA NHƯ MÔ-ĐUN TĂNG TỐC PHỔ QUÁT

Dựa trên các kỹ thuật tinh chỉnh hiệu quả tham số, như LoRA, người ta có thể tinh chỉnh các mô hình được huấn luyện trước với yêu cầu bộ nhớ giảm đáng kể. Trong khuôn khổ của LoRA, các tham số LoRA kết quả có thể được tích hợp liền mạch vào các tham số mô hình gốc. Trong Phần 3.1, chúng tôi chứng minh tính khả thi của việc sử dụng LoRA cho quá trình chưng cất của Các Mô hình Nhất quán Ẩn (LCMs). Mặt khác, người ta có thể tinh chỉnh trên các tập dữ liệu tùy chỉnh cho các ứng dụng hướng tác vụ cụ thể. Hiện tại có một loạt rộng lớn các tham số tinh chỉnh có sẵn để lựa chọn và sử dụng. Chúng tôi khám phá rằng các tham số LCM-LoRA có thể được kết hợp trực tiếp với các tham số LoRA khác được tinh chỉnh trên các tập dữ liệu của các phong cách cụ thể. Sự kết hợp như vậy tạo ra một mô hình có khả năng tạo hình ảnh theo các phong cách cụ thể với các bước lấy mẫu tối thiểu, mà không cần bất kỳ huấn luyện nào thêm. Như được hiển thị trong Hình 1, ký hiệu các tham số được tinh chỉnh LCM-LoRA là τLCM, được xác định là "vector tăng tốc", và các tham số LoRA được tinh chỉnh trên tập dữ liệu tùy chỉnh là τ′, đó là "vector phong cách", chúng tôi thấy rằng một LCM tạo ra hình ảnh tùy chỉnh có thể được thu được như

θ′LCM=θpre+τ′LCM, (2)

trong đó

τ′LCM=λ1τ′+λ2τLCM (3)

là sự kết hợp tuyến tính của vector tăng tốc τLCM và vector phong cách τ′. Ở đây λ1 và λ2 là các siêu tham số. Kết quả tạo sinh của các tham số LoRA phong cách cụ thể và sự kết hợp của chúng với các tham số LCM-LoRA được hiển thị trong Hình 3. Lưu ý rằng chúng tôi không thực hiện huấn luyện thêm trên các tham số kết hợp.

4 KẾT LUẬN

Chúng tôi trình bày LCM-LoRA, một mô-đun tăng tốc phổ quát không cần huấn luyện cho Stable-Diffusion (SD). LCM-LoRA có thể phục vụ như một mô-đun bộ giải dựa trên mạng thần kinh độc lập và hiệu quả để dự đoán giải pháp của PF-ODE, cho phép suy luận nhanh với các bước tối thiểu trên các mô hình SD được tinh chỉnh khác nhau và SD LoRAs. Các thí nghiệm mở rộng về tạo sinh văn bản thành hình ảnh đã chứng minh khả năng tổng quát hóa mạnh mẽ và sự vượt trội của LCM-LoRA.

5 ĐÓNG GÓP & LỜI CẢM ƠN

Công trình này dựa trên Các Mô hình Nhất quán Ẩn (LCMs) của Simian Luo và Yiqin Tan (Luo et al., 2023). Dựa trên LCMs, Simian Luo đã viết mã chưng cất LCM-SDXL ban đầu, và cùng với Yiqin Tan, chủ yếu hoàn thành báo cáo kỹ thuật này. Yiqin Tan khám phá ra tính chất số học của các tham số LCM. Suraj Patil đầu tiên hoàn thành việc huấn luyện LCM-LoRA, khám phá ra khả năng tổng quát hóa mạnh mẽ của nó, và thực hiện phần lớn việc huấn luyện. Suraj Patil và Daniel Gu thực hiện việc tái cấu trúc xuất sắc của codebase LCM-SDXL ban đầu và cải thiện hiệu quả huấn luyện, tích hợp liền mạch nó vào thư viện Diffusers. Patrick von Platen đã sửa đổi và đánh bóng báo cáo kỹ thuật này, cũng như tích hợp LCM vào thư viện Diffusers. Longbo Huang, Jian Li, Hang Zhao đồng cố vấn bài báo LCMs gốc, và đánh bóng báo cáo kỹ thuật này. Chúng tôi tiếp tục cảm ơn Apolin ´ario Passos và Patrick von Platen vì đã tạo ra demo và triển khai LCMs xuất sắc. Chúng tôi cũng muốn cảm ơn Sayak Paul và Pedro Cuenca vì đã giúp viết tài liệu cũng như Radam ´es Ajna vì đã tạo ra các demo. Chúng tôi đánh giá cao các tài nguyên tính toán được cung cấp bởi các nhóm Hugging Face Diffusers để hỗ trợ các thí nghiệm của chúng tôi. Cuối cùng, chúng tôi trân trọng các cuộc thảo luận sâu sắc từ các thành viên cộng đồng LCM.
