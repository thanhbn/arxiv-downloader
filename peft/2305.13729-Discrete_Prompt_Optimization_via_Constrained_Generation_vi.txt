# Tối ưu hóa Prompt Rời rạc thông qua Sinh có Ràng buộc
cho Re-ranker Zero-shot

Sukmin Cho Soyeong Jeong Jeongyeon Seo Jong C. Park∗
Khoa Tin học
Viện Khoa học và Công nghệ Tiên tiến Hàn Quốc
{nelllpic,starsuzi,yena.seo,jongpark}@kaist.ac.kr

## Tóm tắt

Re-ranker, được sử dụng để sắp xếp lại các tài liệu đã truy xuất theo điểm relevance với truy vấn đã cho, đã thu hút sự chú ý trong tác vụ truy xuất thông tin (IR). Thay vì fine-tuning mô hình ngôn ngữ pre-trained (PLM), mô hình ngôn ngữ quy mô lớn (LLM) được sử dụng như một re-ranker zero-shot với kết quả xuất sắc. Trong khi LLM rất phụ thuộc vào prompt, tác động và việc tối ưu hóa prompt cho re-ranker zero-shot vẫn chưa được khám phá. Cùng với việc nhấn mạnh tác động của tối ưu hóa lên re-ranker zero-shot, chúng tôi đề xuất một phương pháp tối ưu hóa prompt rời rạc mới, Constrained Prompt generation (Co-Prompt), với metric ước lượng mức tối ưu cho re-ranking. Co-Prompt hướng dẫn các văn bản được sinh từ PLM hướng tới prompt tối ưu dựa trên metric mà không cần cập nhật tham số. Kết quả thực nghiệm cho thấy Co-Prompt dẫn đến hiệu suất re-ranking vượt trội so với các baseline. Ngoài ra, Co-Prompt sinh ra các prompt dễ hiểu hơn cho con người so với các phương pháp tối ưu hóa prompt khác.

## 1 Giới thiệu

Truy xuất thông tin (IR) là tác vụ tìm kiếm các tài liệu liên quan đến truy vấn đã cho từ một corpus lớn. Khi việc re-ranking các tài liệu được lấy từ retriever có thể nâng cao hiệu suất và độ trễ một cách hiệu quả, các nghiên cứu gần đây đã đề xuất nhiều loại re-ranker bằng cách fine-tuning các mô hình ngôn ngữ pre-trained (PLM) (Nogueira and Cho, 2019; Nogueira et al., 2020). Hơn nữa, Sachan et al. (2022) cho thấy rằng các mô hình ngôn ngữ quy mô lớn (LLM) như GPT-3 (Brown et al., 2020) có thể được khai thác như một re-ranker zero-shot với prompt mô tả tác vụ. Họ cũng nhấn mạnh tầm quan trọng của một prompt phù hợp để khai thác toàn bộ hiệu suất của LLM, thay vì cập nhật các tham số. Họ chọn một prompt tối ưu trong số các ứng cử viên được tạo thủ công bằng cross-validation. Tuy nhiên, việc tìm kiếm thủ công như vậy cho các prompt rời rạc rất tốn kém và không tối ưu về khả năng chuyển giao.

Để giải quyết vấn đề này, một số phương pháp đã được đề xuất để tự động tối ưu hóa prompt rời rạc. Chúng tập trung vào phân loại văn bản hoặc tác vụ mask-filling trong khi đánh giá thấp sinh mở (Shin et al., 2020; Gao et al., 2021; Prasad et al., 2022). Gần đây, Deng et al. (2022) giải quyết tối ưu hóa prompt rời rạc có thể áp dụng cho các tác vụ sinh với reinforcement learning bằng cách thiết kế hàm reward, đo lường văn bản được sinh thuộc về một nhãn rời rạc. Vì vẫn có những tác vụ không được aligned, đòi hỏi điểm số liên tục của output, chúng tôi hướng tới tối ưu hóa prompt cho một trong những tác vụ như vậy: re-ranking.

Trong bài báo này, chúng tôi đề xuất Constrained Prompt generation, Co-Prompt, như tối ưu hóa prompt rời rạc từ trái sang phải mà không cần huấn luyện mô hình bổ sung. Bằng cách định nghĩa metric của prompt tối ưu cho re-ranking, chúng tôi diễn giải quá trình tìm kiếm prompt tối ưu như sinh có ràng buộc với hai module: một re-ranker zero-shot như discriminator và bất kỳ PLM decoder-only nào như generator. Discriminator tính toán likelihood (tức là metric) rằng chuỗi prompt là tối ưu để hướng dẫn LLM phân biệt các tài liệu liên quan trong tập lớn cho một truy vấn đã cho. Generator lấy mẫu các token prompt có prior cao từ các chuỗi prompt trước đó để hạn chế hiệu quả các ứng cử viên prompt cho discriminator đánh giá. Tổng quan về Co-Prompt được hiển thị trong Hình 1.

Chúng tôi xác thực phương pháp của mình, Co-Prompt, so với các baseline tối ưu hóa khác trên hai LLM, T0 (Sanh et al., 2022) và OPT (Zhang et al., 2022), với hai dataset benchmark, MS-MARCO (Nguyen et al., 2016) và Natural Question (Kwiatkowski et al., 2019). Kết quả thực nghiệm cho thấy Co-Prompt liên tục sinh ra các prompt hoạt động tốt bất kể LLM và dataset so với các baseline. Các phân tích định tính cũng hỗ trợ khả năng diễn giải của các prompt được sinh bởi Co-Prompt, tương tự như các mẫu ngôn ngữ con người.

Đóng góp của chúng tôi trong công trình này có ba khía cạnh:
• Chúng tôi nhấn mạnh tác động của prompt tối ưu cho re-ranker zero-shot bằng cách khai thác các phương pháp tối ưu hóa.
• Chúng tôi đề xuất Co-Prompt, một phương pháp tối ưu hóa prompt rời rạc mới thông qua sinh có ràng buộc cho re-ranker zero-shot.
• Chúng tôi thực nghiệm cho thấy Co-Prompt liên tục hướng dẫn re-ranker tốt so với các baseline và output của nó tương tự như các mẫu ngôn ngữ con người.

## 2 Công trình liên quan

**Document Ranking với Generative Model**
Sử dụng mô hình sinh là một trong những phương pháp thống trị để xếp hạng các tài liệu đã truy xuất bằng cách định nghĩa điểm relevance như điểm query likelihood (Nogueira dos Santos et al., 2020; Ju et al., 2021). Gần đây hơn, Sachan et al. (2022, 2023) cho thấy rằng LLM phục vụ như một re-ranker zero-shot hoặc một module huấn luyện của một dense retriever không giám sát. Tuy nhiên, không giống như của chúng tôi, họ yêu cầu các prompt thủ công được thiết kế cẩn thận, có thể có hạn chế về khả năng chuyển giao.

**Prompt Optimization** Khi prompting được xem là một biến quan trọng khi khai thác LLM cho các tác vụ NLP khác nhau, việc tìm prompt tối ưu đã trở nên quan trọng để có được hiệu suất tốt nhất từ LLM (Kojima et al., 2022; Xie et al., 2022). Gần đây, công việc tối ưu hóa prompt đã tập trung vào tìm kiếm prompt rời rạc (Shin et al., 2020; Gao et al., 2021; Deng et al., 2022) hoặc học soft prompt trên không gian liên tục (Liu et al., 2021; Qin and Eisner, 2021; Lester et al., 2021). Trong khi các phương pháp tối ưu hóa hiện có chủ yếu xem xét phân loại văn bản hoặc tác vụ mask-filling, khả năng áp dụng của chúng cho re-ranking vẫn chưa được khám phá đầy đủ. Trong bài báo này, chúng tôi nhắm mục tiêu tối ưu hóa prompt rời rạc cho re-ranker zero-shot để có được điểm relevance cao hơn cho các cặp liên quan hơn thông qua sinh có ràng buộc.

**Constrained Generation** Sinh có ràng buộc nhằm mục đích tạo ra các chuỗi văn bản tuân theo một ràng buộc nhất định (Keskar et al., 2019). Sử dụng discriminator để hướng dẫn sinh hướng tới ràng buộc thông qua quy tắc Bayes là một trong những phương pháp sinh ràng buộc được sử dụng rộng rãi (Dathathri et al., 2020; Krause et al., 2021; Chaffin et al., 2022). Được truyền cảm hứng bởi hiệu quả của phương pháp dựa trên discriminator, chúng tôi áp dụng re-ranker zero-shot như một discriminator khi sinh các chuỗi prompt rời rạc tối ưu.

## 3 Phương pháp

### 3.1 Kiến thức cơ bản

Một LLM re-rank tài liệu đã truy xuất d liên quan đến điểm relevance với truy vấn đã cho q như điểm sinh truy vấn:

logP(d|q) ∝ logP(q|d, ρ)
= 1/|q| Σ_t logP(q_t|q_{<t}, d, ρ), (1)

trong đó |q| biểu thị độ dài token của truy vấn q và ρ là một prompt ngôn ngữ tự nhiên hướng dẫn LLM sinh truy vấn q. Vì prompt ρ là biến duy nhất có thể kiểm soát trong Phương trình 1, tìm kiếm prompt tối ưu là một cách đơn giản nhưng hiệu quả để nâng cao hiệu suất của LLM. Do đó, trong công trình này, chúng tôi tập trung vào chiến lược tối ưu hóa prompt.

### 3.2 Constrained Prompt Generation

Chúng tôi định nghĩa prompt tối ưu ρ* cho re-ranker tối đa hóa điểm sinh truy vấn:

ρ* = arg max_ρ E_{(d_i,q_i)∈D}[P(q_i|d_i, ρ)], (2)

trong đó D là dataset cho retriever, bao gồm các cặp truy vấn và tài liệu liên quan của nó.

Chúng tôi giải quyết tác vụ tìm kiếm prompt tối ưu ρ* cho dataset cặp tài liệu-truy vấn D với sinh có ràng buộc dựa trên discriminator. Việc sinh được hướng dẫn bởi quy tắc Bayes:

P(ρ_t|D, ρ_{1:t-1}) ∝ P_{MD}(D_s|ρ_{1:t})P_{MG}(ρ_t|ρ_{1:t-1}), (3)

trong đó MD là re-ranker zero-shot phục vụ như discriminator, MG là PLM decoder-only như generator, và D_s là dataset được lấy mẫu từ D.

**Discriminator** Discriminator MD đo lường mức hiệu quả của chuỗi prompt ρ_{1:t} hướng dẫn re-ranker zero-shot sinh truy vấn từ tài liệu đã cho bằng cách tính toán likelihood P_{MD}(D_s|ρ), được định nghĩa như expectation của điểm relevance giữa các cặp tài liệu-truy vấn (q_i, d_i) của dataset được lấy mẫu D_s với prompt ρ:

P_{MD}(D_s|ρ) = E_{(d_i,q_i)∈D_s}[P_{MD}(q_i|d_i, ρ)]. (4)

Chúng tôi sử dụng likelihood này như metric cho prompt tối ưu. Tùy chọn khác của P_{MD} được hiển thị trong Phụ lục B.1.

**Generator** Generator MG lấy mẫu pool của các prompt để được đánh giá bởi discriminator vì tính toán Phương trình 3 của tất cả các token có thể trong từ vựng đòi hỏi chi phí tính toán cực kỳ cao. PLM decoder-only được khai thác để lấy mẫu các token prompt ρ_t có prior cao P_{MG}(ρ_t|ρ_{1:t-1}) theo cách zero-shot.

Chúng tôi kết hợp các module này để tối ưu hóa prompt bằng cách thực hiện lặp lại hai bước: sinh ứng cử viên và đánh giá. Chúng tôi chọn sử dụng beam search như chiến lược decoding cho sinh prompt từ trái sang phải. Các bước chi tiết của chiến lược decoding được hiển thị trong Algorithm 1.

## 4 Thiết lập thực nghiệm

Chúng tôi mô tả các thiết lập thực nghiệm để xác thực hiệu suất của các prompt. Code của chúng tôi được công bố tại github.com/zomss/Co-Prompt.

**Datasets** Chúng tôi sử dụng hai dataset truy xuất thông tin: 1) MS-MARCO (Nguyen et al., 2016), được thu thập từ log tìm kiếm Bing, và 2) Natural Question (NQ, Kwiatkowski et al. (2019)), được lấy từ công cụ tìm kiếm Google. Chúng tôi chỉ sử dụng dữ liệu tài liệu của dataset để đánh giá. Thông tin chi tiết hơn được hiển thị trong Phụ lục A.1.

**Evaluation Metrics** Chúng tôi đánh giá kết quả bằng hai metric, ACC và nDCG. 1) ACC là tỷ lệ phần trăm của các tài liệu liên quan trong tổng số được truy xuất. 2) nDCG, normalized discounted cumulative gain, phản ánh rằng các tài liệu liên quan hơn nên ghi điểm xếp hạng cao hơn.

**Retriever & Re-ranker** Chúng tôi chọn hai retriever sparse và dense được sử dụng rộng rãi như retriever của chúng tôi, đó là 1) BM25 (Robertson and Zaragoza, 2009) và 2) DPR (Karpukhin et al., 2020), tương ứng. Cho re-ranker zero-shot, chúng tôi sử dụng 1) T0 (Sanh et al., 2022) và 2) OPT (Zhang et al., 2022). Chúng tôi mô tả thông tin chi tiết hơn trong Phụ lục A.3 và A.4.

**Prompt Baselines** Chúng tôi so sánh Co-Prompt với bốn baseline: 1) Null Prompt là prompt trống không có token nào. 2) P-Tuning là phương pháp tối ưu hóa soft prompt tạo ra prompt embedding từ prompt encoder (Liu et al., 2021). 3) RL-Prompt là phương pháp tối ưu hóa prompt rời rạc bằng cách huấn luyện policy network (Deng et al., 2022). Lưu ý rằng chúng tôi sửa đổi RL-Prompt và P-Tuning áp dụng được cho tác vụ re-ranking. 4) Manual Prompt, được đề xuất tại Sachan et al. (2022), được đưa ra như "Please write a question based on this passage", theo giả định rằng đó là một trong những prompt tốt nhất mà con người có thể tìm thấy. Cuối cùng, 5) Co-Prompt, phương pháp được đề xuất của chúng tôi, là phương pháp tối ưu hóa prompt rời rạc trong sinh zero-shot từ trái sang phải. Lưu ý rằng chi tiết triển khai của baseline được hiển thị trong Phụ lục A.5.

**Implementation Details** Discriminator MD là cùng mô hình với re-ranker zero-shot. Vì generator MG nên là mô hình decoder-only, trong trường hợp T0, GPT2-Large (Radford et al., 2019) được sử dụng như generator. OPT, một mô hình decoder-only, được sử dụng như cả discriminator và generator. Chúng tôi sử dụng start token như "Please" để so sánh trực tiếp với manual prompt và cố định beam width B là 10 và độ dài prompt tối đa L là 10 trong thực nghiệm của chúng tôi.

**Environment** Chúng tôi tiến hành tất cả thực nghiệm bao gồm tìm kiếm prompt và re-ranking tài liệu trên GPU V100 32GB. Chúng tôi sử dụng framework BEIR (Thakur et al., 2021) cho đánh giá kết quả re-ranked và dataset truy xuất passage. Ngoài ra, các retriever, BM25 và DPR, đều từ cùng framework. Chúng tôi sử dụng T0 và OPT với 3B và 2.7B tham số mỗi cái cho discriminator và re-ranker được mở công khai trên Huggingface model hub (Wolf et al., 2020).

## 5 Kết quả

Trong phần này, chúng tôi hiển thị kết quả tổng thể của phương pháp của chúng tôi, Co-Prompt, với phân tích chi tiết.

**Overall Results** Như được hiển thị trong Bảng 1, Co-prompt liên tục cho thấy tăng hiệu suất mạnh mẽ trong tất cả các tình huống, bất kể LLM, dataset, và retriever. Cụ thể, Co-Prompt, áp dụng cho OPT, đạt được kết quả tốt hơn so với các phương pháp khác. Điều này cho thấy rằng các prompt được sinh bởi phương pháp của chúng tôi phù hợp hơn để đóng vai trò như một hướng dẫn để dẫn dắt LLM so với các phương pháp tối ưu hóa prompt khác. Kết quả chi tiết hơn về hiệu suất re-ranked với các metric khác nhau được hiển thị trong Phụ lục B.3.

**Impact of Start Tokens** Chúng tôi khai thác các tùy chọn khác của start token như "Score" và "This" như được hiển thị trong Bảng 2. Bất kể start token, Co-Prompt liên tục sinh ra các prompt kích thích hiệu suất của LLM một cách hiệu quả. Tuy nhiên, chúng tôi quan sát thấy rằng việc tìm start token tối ưu cho dataset là quan trọng để đạt được kết quả tốt hơn.

**Impact of Generator** Như được hiển thị trong Bảng 3, ngay cả khi các generator khác nhau được sử dụng, các prompt được sinh bởi các generator khác nhau hướng dẫn re-ranker zero-shot một cách hiệu quả. Tuy nhiên, sự khác biệt về hiệu suất được gây ra bởi sự không khớp từ vựng giữa hai module. Chúng tôi thấy rằng, mặc dù phương pháp của chúng tôi không thay đổi đáng kể về hiệu suất đối với generator, một generator phù hợp hơn có thể cần thiết để có kết quả tốt hơn.

**Relevance Score** Chúng tôi phân tích phân phối điểm relevance giữa các cặp tài liệu-truy vấn tích cực hoặc tiêu cực. Vì các tài liệu tiêu cực cho một truy vấn đã cho được truy xuất từ BM25, những cái tiêu cực có liên quan đến truy vấn nhưng không thể tìm thấy câu trả lời trực tiếp. Như được hiển thị trong Hình 2, chúng tôi chỉ ra rằng sự khác biệt phân phối tồn tại giữa các cặp mặc dù có một số chồng chéo. Ngoài ra, LLM có thể phân biệt cặp nào là tích cực, ngay cả không có prompt. Tuy nhiên, chúng tôi quan sát thấy rằng hiệu ứng của tối ưu hóa prompt rời rạc trên re-ranker zero-shot là theo hướng tăng trung bình và phương sai của điểm relevance.

**Case Study of Prompts** Bảng 2 hiển thị các prompt rời rạc được sinh bởi phương pháp của chúng tôi và baseline prompt rời rạc khi khai thác OPT như re-ranker. Trong khi các prompt từ RL-prompt là những từ ngữ vô nghĩa không đúng ngữ pháp gần với chuỗi từ ngẫu nhiên, phương pháp của chúng tôi, Co-Prompt, sinh ra các prompt có thể diễn giải cho con người, tuân theo các mẫu ngôn ngữ con người, và vượt qua hiệu suất của các prompt rời rạc khác. Ngoài ra, từ 'question', một trong những từ khóa mô tả tác vụ, được bao gồm trong các prompt từ Co-Prompt bất kể dataset. Điều này ngụ ý rằng các prompt từ phương pháp của chúng tôi có thể cung cấp giao diện người dùng tự nhiên để cải thiện hiểu biết của con người về cách LLM hoạt động. Xem Phụ lục B.3 để có thêm ví dụ về Co-Prompt.

## 6 Kết luận

Trong bài báo này, chúng tôi đề xuất Co-Prompt, tối ưu hóa prompt từ trái sang phải cho re-ranker zero-shot thông qua sinh có ràng buộc. Co-Prompt hiệu quả hạn chế các ứng cử viên prompt và đánh giá mức tối ưu của các prompt này mà không cần cập nhật tham số nào. Chúng tôi thực nghiệm cho thấy phương pháp của chúng tôi đạt được hiệu suất vượt trội liên tục trong tất cả các thực nghiệm. Ngoài ra, tác động của tối ưu hóa prompt bao gồm baseline trên re-ranker zero-shot nhấn mạnh tầm quan trọng của nó. Chúng tôi cũng trình bày một kết quả thú vị là prompt tối ưu có thể diễn giải được cho con người. Cho công việc tương lai, chúng tôi dự định mở rộng phương pháp của chúng tôi sang các tác vụ sinh mở khác sử dụng LLM.

## Hạn chế

Như được hiển thị trong Bảng 1, phương pháp của chúng tôi được chứng minh thực nghiệm là hiệu quả trong hai LLM. Tuy nhiên, OPT, một mô hình decoder-only, phù hợp hơn cho các prompt được sinh bởi Co-Prompt. Điều này có vẻ là vì T0, mô hình encoder-decoder, yêu cầu một generator riêng biệt như GPT-2. Hiệu suất của prompt có thể thay đổi tùy thuộc vào generator tham gia trong từ vựng và quá trình huấn luyện. Ngoài ra, có sự cân bằng giữa thời gian tìm kiếm và hiệu suất. Trong khi việc tăng kích thước beam và số cặp tài liệu-truy vấn nâng cao xác suất tìm thấy prompt tối ưu hơn, nó làm cho thời gian tìm kiếm dài hơn tỷ lệ thuận.

## Tuyên bố đạo đức

Công trình của chúng tôi góp phần nâng cao hiệu suất truy xuất của re-ranker zero-shot bằng cách tối ưu hóa prompt rời rạc thông qua sinh có ràng buộc. Chúng tôi hoàn toàn nhận thức về khả năng của các prompt xúc phạm hoặc khó chịu gây ra bởi bias của chính generator mặc dù không có prompt như vậy trong thực nghiệm của chúng tôi. Vì không có huấn luyện bổ sung cho tối ưu hóa prompt, phương pháp của chúng tôi gặp khó khăn trong việc loại bỏ bias của chính mô hình ngôn ngữ. Khi các nghiên cứu về giảm bias của mô hình ngôn ngữ hoặc lọc các biểu thức không phù hợp trong văn bản đang được tiến hành tích cực, những vấn đề này được kỳ vọng sẽ được giải quyết đầy đủ trong tương lai.

## Lời cảm ơn

Công trình này được hỗ trợ bởi grant từ Viện Xúc tiến Công nghệ Thông tin và Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (Số 2018-0-00582, Dự đoán và tăng cường phân phối độ tin cậy thông qua phân tích ngôn ngữ và thu thập tài liệu bằng chứng tự động).
