# 2303.07910.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2303.07910.pdf
# File size: 1961057 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Revisit Parameter-Efï¬cient Transfer Learning: A Two-Stage Paradigm
Hengyuan Zhao1, Hao Luo2, Yuyang Zhao3, Pichao Wang2, Fan Wang2, Mike Zheng Shou1
1Show Lab, National University of Singapore,2Alibaba Group,3National University of Singapore
(hengyuan.z, yuyang.zhao)@u.nus.edu, (michuan.lh, fan.w)@alibaba-inc.com
(pichaowang, mike.zheng.shou)@gmail.com
Abstract
Parameter-Efï¬cient Transfer Learning (PETL) aims at
efï¬ciently adapting large models pre-trained on massive
data to downstream tasks with limited task-speciï¬c data.
In view of the practicality of PETL, previous works focus
on tuning a small set of parameters for each downstream
task in an end-to-end manner while rarely considering the
task distribution shift issue between the pre-training task
and the downstream task. In this paper, we propose a novel
two-stage paradigm, where the pre-trained model is ï¬rst
aligned to the target distribution, and then the task-relevant
information is leveraged for effective adaptation. Speciï¬-
cally, the ï¬rst stage is to narrow the task distribution shift
by tuning the scale and shift in the LayerNorm layers. In
the second stage, to efï¬ciently learn the task-relevant infor-
mation, we propose a Taylor expansion-based importance
score to identify task-relevant channels for the downstream
task and then only tune such a small portion of channels,
making the adaptation to be parameter-efï¬cient. Overall,
we present a promising new direction for PETL, and the
proposed paradigm achieves state-of-the-art performance
on the average accuracy of 19 downstream tasks. Codes
will be available here.
1. Introduction
Large vision transformer models [14, 39, 58] have
demonstrated exceptional performance on large-scale im-
age classiï¬cation tasks [11]. Inspired by the successful us-
age of large language models [12, 4, 48, 34], there is a
growing interest in leveraging the pre-trained knowledge
from large vision transformer models for downstream tasks.
The most common and direct way is to ï¬ne-tune the whole
model on the small downstream dataset. Nevertheless,
ï¬ne-tuning all the parameters ( akafull ï¬ne-tuning) on a
small dataset can lead to two severe challenges: (1) full
ï¬ne-tuning is prone to overï¬tting when the tuned massive
weights of pre-trained models are not comparable with the
limited downstream training data; (2) the high computa-
DownstreamÂ data
Pre-trainedÂ 
Model
NarrowÂ 
distributionÂ shift
Stage1
AddÂ task-relevantÂ 
Module
Stage2AddÂ 
prompts
CurrentÂ PETLÂ methods
UpstreamÂ 
data
ProposedÂ newÂ paradigmAdapter
OneÂ Stage
Learn scalingÂ 
andÂ shiftNAS
LearnÂ Low-
rankÂ martixFigure 1: An illustration of our new paradigm.
tion costs and storage requirements of a large number of
model parameters (since each task requires storing a sepa-
rate model) make it harder to be applied to extreme storage-
constrained devices.
To address the above two problems, recent research
works investigate parameter-efï¬cient transfer learning
(PETL) [27, 25, 37], aiming at efï¬ciently adapting large
models to downstream tasks with limited data. Main-
stream PETL methods can be categorized into two types:
(1) designing additional modules (adapter [25] or visual
prompt [27]) to learn task-relevant information; (2) narrow-
ing the task distribution shift between the pre-trained and
downstream tasks via feature scaling and shifting [37]. In-
spired by the effectiveness of the two approaches, we re-
visit PETL from the perspective of both task distribution
shift and add the task-relevant module and present a novel
two-stage paradigm called TTC-Tuning in this paper.
For narrowing task distribution shift, SSF [37] inserts ad-
ditional scale and shift parameters into MLP, MHSA and
LayerNorm components to modulate the features. Instead
of using additional parameters, tuning normalization layers
is a common way to align distributions in transfer learning
tasks [54]. Thus, we follow the concept of modulating fea-
tures but propose a more effective and efï¬cient technique to
align the task distribution, i.e., tuning the layer normaliza-
tion (LayerNorm) parameters. As shown in Fig. 2, Layer-
1arXiv:2303.07910v1  [cs.CV]  14 Mar 2023

--- PAGE 2 ---
Original
Acc. = 90.5%SVHN (VTAB-Natural)
Acc. = 41.8%w\ tuning LayerNormEuroSAT (VTAB-Specialized)
Acc. = 88.6%Clevr (VTAB-Structured)
Acc. = 35.4%
Acc. = 95.9% Acc. = 80.6%Tuning Layernorm LayerFigure 2: The t-SNE visualization of ï¬nal [CLS] token of
the test set from SVHN, EuroSAT, and Clevr tasks. â€œOrigi-
nalâ€ represents the feature extracted from the original back-
bone while â€œw/ LayerNormâ€ tuning means extracted from
the backbone tuned with LayerNorm only.
Norm tuning can greatly improve the discrimination ability
of the pre-trained model on downstream tasks. Compared
with SSF [37], LayerNorm tuning uses less than 15% pa-
rameters (0.03M v.s.0.21M) but outperforms SSF by 0.5%
in the absolute top-1 accuracy. Therefore, we adopt Layer-
Norm tuning as our ï¬rst step for task distribution alignment.
Besides aligning the task distribution, TTC-Tuning also
considers adding task-relevant module which has been
shown to be crucial by previous studies. For some challenge
datasets, such as medical images (Camelyon dataset [53])
and 3D scene images (Clevr-Count [29]), only aligning the
task distribution lead to inferior improvement due to the
large knowledge gap between the downstream task and pre-
trained model. Previous PETL methods [27, 25, 26, 28,
66, 7, 37] mainly propose parameter-efï¬cient tuning mod-
ules to implicitly leverage the task-relevant information by
adding tokens or adapting the whole features. However,
these methods treat each parameter as equivalent and just
insert some ï¬xed modules to automatically adapt the whole
network to the downstream tasks. Here, we raise an es-
sential question: can we identify the important parameters
for a speciï¬c downstream task and then ï¬ne-tune only these
task-relevant parameters?
Inspired by the channel bias in few-shot learning [41]
and model pruning [35], we hypothesize and experimentally
validate that channel inequality exists in different tasks. We
can explicitly leverage such task-relevant information to
tune only a small portion of task-relevant channels, lead-
ing to comparable or even better performance. To verify
this hypothesis, we investigate the contributions of individ-
ual channels in downstream adaptation. The contributions
are measured by a proposed Taylor expansion-based impor-
tance score. As shown in Fig. 3, different tasks have differ-
ent task-relevant channels in the same layer. Thus, we can
select the task-relevant channels based on the contributions
and utilize a simple adapter to transform such channels for
efï¬cient adaptation.
In summary, our contributions are three-fold:â€¢ We propose a new two-stage paradigm to solve the
PETL from the perspective of both task distribution
shift and add task-relevant tunable module.
â€¢ We experimentally verify the effectiveness of only tun-
ing the LayerNorm layer to align distributions and de-
velop a novel tuning module that ï¬rst selects task-
relevant channels via the proposed Taylor expansion-
based importance score. Such designs lead to a few
extra parameters.
â€¢ Our novel paradigm outperforms the previous state-of-
the-art method SSF [37] with a 1.7% increase in ac-
curacy across 19 downstream tasks. This result high-
lights the effectiveness of our approach and its poten-
tial to make a signiï¬cant impact in various applica-
tions.
2. Related Work
2.1. Vision Transformers
Transformers [52] have shown remarkable performance
on natural language processing and computer vision tasks.
Numerous vision transformers [5, 15, 13, 1, 17, 20, 49, 63,
51, 39, 56, 68] have been proposed following the pioneer-
ing work of ViT [14]. Most of these models gradually in-
crease in size to achieve state-of-the-art results and learn
rich representations through various architectural designs.
Adopting these models for downstream tasks signiï¬cantly
reduces the training complexity and delivers promising re-
sults rapidly. Given a plain Vision Transformer (ViT) [14]
withLlayers and an input image I2R3HWthat ï¬rst
divided into Nnon-overlapped patches and then passed into
an embedding layer projected into Ddimensions. Each
transformer layer includes a multi-head self-attention block
(MHSA) and a multi-perceptron block (MLP).
2.2. Parameter-Efï¬cient Transfer Learning
PETL focuses on adapting the pre-trained model on a
downstream task with a few parameters. Two lines of
PETL approaches have been proposed recently. On the
one hand, learning task-relevant information by applying
prompts [27, 38, 61, 67, 45, 57] to the input tokens or adding
a trainable module [25, 7, 28, 6, 65] to adapt pre-trained
information have acquired promising results for the perfor-
mance and efï¬ciency. On the other hand, aligning the distri-
bution between pre-trained and downstream tasks has been
shown to be a strong baseline, as demonstrated in [37].
Task-relevant modules. VPT [27] injects the prompts into
the transformer layerâ€™s input tokens with a small number
of extra parameters. However, one main limitation of VPT
is that it relies on hand-crafted selection to determine the
optimal prompt length for each task. This can be inï¬‚exi-
ble when applying the method to new tasks. VPT includes
2

--- PAGE 3 ---
Caltech101:Â AÂ DatasetÂ ofÂ 101Â ObjectÂ CategoriesÂ 
forÂ classification.Flowers102:Â AÂ DatasetÂ forÂ FineÂ­GrainedÂ 
ClassificationÂ ofÂ VariousÂ TypesÂ ofÂ Flowers.DMLab:Â PredictingÂ NavigationÂ TimeÂ ofÂ PreÂ­TrainedÂ RLÂ 
AgentsÂ toÂ ReachÂ Objects.Figure 3: We have identiï¬ed task-relevant channels from the last layer of ViT-B on three tasks. This suggests that different
tasks may prioritize different channels within the same layer.
two variants VPT-Shallow and VPT-Deep associated with
the number of inserted layers. VPT-Shallow only inserts
prompts into the ï¬rst transformer layer L1and VPT-Deep
inserts all the transformer layers. Given the input tokens
x2R(N+1)Dand the prompts P2RnDthat contains n
prompts with dimension D, we can formulate the combined
tokensx0is
x0= [x;P]; (1)
wherex02R(N+n+1)Dwill be passed into the following
MHSA and MLP blocks.
Adapter [25] proposes an MLP-like module, a success-
ful design that adopts a residual pathway to keep the orig-
inal information and transform task-relevant information
by learning a down-projection Wdown2RD0D(where
D0D) and an up-projection Wup2RDD0with a
nonlinearity activation operation . Given an input tokens
xl2R(N+1)Dinl-th layer, the output of a adapter block
is
xl
out=xl+ [Wl
up(Wl
down[xl]T)]T; (2)
where []Trepresents transpose operation. However, the
number of trainable parameters in Adapter-like methods
is not small and produces inferior performance. Besides,
LoRA [26] optimizes a low-rank decomposition matrix with
a low intrinsic dimension to project the matrices of query,
key, and value used in the MHSA block in ViT. Further-
more, a neural architecture search algorithm called NOAH
[66] has been proposed, which incorporates Adapter [25],
LoRA [26], and VPT [27] into its network search space.
Narrow task distribution shift. SSF [37] In addition to the
above prompt-based and adapter-based methods, a recently
introduced technique called SSF that has shown promising
results involves scaling and shifting the features of the pre-
trained model. SSF [37] leverages two learnable vectors
2RDand2RDto scale and shift the feature map in
each transformer operation ( i.e., Linear operation or Lay-
erNorm operation). Assuming the input of SSF module is
x2R(N+1)D, the output y2R(N+1)Dcan be written
as following
y=x+; (3)
0.000 0.002 0.004 0.006 0.008 0.010 0.012 0.014 0.016
Jensen Shannon Divergence Distance
020040060080010001200CountSVHN (VTAB-Natural)
SSF
LayerNorm Tuning
0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175
Symmetric KL distance050100150200250300CountEuroSAT (VTAB-Specialized)
SSF
LayerNorm TuningFigure 4: We analyzed the distribution of JSD for the [CLS]
token on the SVHN and EuroSAT tasks. The JSD between
the original feature and the feature generated by SSF is rep-
resented by â€œSSF,â€ while the JSD between the original fea-
ture and the feature produced after tuning the LayerNorm
layer is represented by â€œLayerNorm Tuningâ€.
whereis the Hadamard product. Motivated by this work,
we extend this method to tuning the LayerNorm layer to
reduce the distribution shifts and demonstrate the effective-
ness on multi-downstream tasks.
3. Approach
We propose a two-stage paradigm for achieving
parameter-efï¬cient transfer learning, as shown in Fig. 5. In
the ï¬rst stage, we align the task distribution by tuning the
LayerNorm layer while keeping the other components of
the original backbone frozen. In the second stage, we use
a Taylor expansion-based Importance Score (TIS) to iden-
tify the most relevant channels for the downstream task, by
computing gradients on the training set with stage1â€™s model.
Then, we introduce the TTC-Module, a tunable module that
transforms the task-relevant channels while freezes other
channels.
3.1. Narrow the Task Distribution Shift
In this section, we ï¬rst brieï¬‚y review the Layer Normal-
ization (LN) [2]. LN is a widely used normalization tech-
nique in transformers [52, 14] to solve the problem of the
inconsistent amount of input tokens in natural language pro-
cessing tasks and provide valid normalization in the MLP
block. We empirically ï¬nd that for PETL, tuning Layer-
Norm layer could efï¬ciently change the mean and variance
3

--- PAGE 4 ---
DownstreamÂ data
LayerNorm
MHSA
LayerNorm
MLP
ğ¿Ã—Eembedded Patches
HeadInput xâˆˆ â„ğ‘+1 Ã—ğ·
ğ‘+1
Output: yâˆˆ â„ğ‘+1 Ã—ğ·xâ€²â€² âˆˆ â„ğ‘+1 Ã— ğ¾
xâ€² âˆˆ â„ğ‘+1 Ã— ğ¾LayerNorm
MHSA
TTC-Module
Layernorm
MLP
Task-relevant feature Frozen feature Transformed featureLinear
Frozen
TunedLayerNorm
LayerNorm
MLPğ¿Ã—Eembedded
Patches
HeadQKV
QKV
QKVğ‘¤1
ğ‘¤2
ğ‘¤3
ğ‘¤ğ‘–
ğ‘¤ğ·
ğ‘¾_ğ’‘ğ’“ğ’ğ’‹
Predicted 
LabelMHSA
Ground TruthTransformer Layer Calculate the Importance Score ğœ•â„’(ğ‘¥)
ğœ•ğ‘Šğ‘ğ‘Ÿğ‘œğ‘—â‹…ğ‘Šğ‘ğ‘Ÿğ‘œğ‘—
Attention
Stage1
Stage2
Determine
 the
task
-
relevant channels
Aligning distribution
T
ask
-
relevant modul
 e
Backwardğ‘¥ âˆˆ â„ğµÃ—ğ‘+1 Ã—ğ·
â„’Cross-Entropy Loss
Important weightsFigure 5: An overview of our novel paradigm to parameter-efï¬cient transfer learning.
of feature distribution as mentioned in Fig. 2. Assuming the
inputx2RB(N+1)D, the outputy2RB(N+1)Dcan
be formulated as:
y=x E[x]p
Var[x] ++; (4)
whereandare scaling and bias factors, respectively.
E[]and Var []are expectations and variances that will lead
to zero mean and unit variance.
Second, we analyze the statistics of the last [CLS] token
to compare the effectiveness of tuning LN and SSF mod-
ule. In particular, we assume the baseline as the original
model distribution and compute the distances between this
distribution with the distribution of LN tuning and SSF, re-
spectively. Consider two probability distributions are pand
q, we use Jensenâ€“Shannon Divergence (JSD) [16] as the
metric to compute the distance Las:
L=1
2(KL(log(p);m) +KL(log(q);m));
m= (p+q)=2;(5)
whereKLis Kullbackâ€“Leibler divergence [32]. Figure 4
displays the distance distributions, where the blue his-
togram represents the distance between the original model
and the model trained with SSF, while the pink histogram
compares the distance of the original model with the Lay-
erNorm model. Examining the JSD distribution, the range
and the covariance of SSF are larger than LayerNorm. No-
tably, a signiï¬cant number of samples are located at zero,
indicating that the distribution is the same as the original
model. On the other hand, a considerable number of sam-
ples are located far away from the original model, suggest-
ing that SSF may ï¬t some samples while ignoring others. Incontrast, our LayerNorm tuning has a more compact distri-
bution and appears unbiased towards any particular sample.
3.2. Task-Relevant Channel Selection using Taylor
Expansion-Based Importance Scores
While aligning the distribution between pre-trained and
downstream tasks can be effective for small distribution
shifts, to handle various task distribution shifts, we need
to introduce an extra learnable module that has been proved
crucial by other PETL methods proposed [27, 25, 26, 66].
However, unlike these methods treat each channel equally
during ï¬ne-tuning, we hypothesize that only tuning a small
portion of full channels is enough for adaptation. We note
that the network weights are closely related to the task
labels as mentioned in [36], and thus we aim to select
task-relevant weights by feeding the downstream training
set. Various methods [40, 35, 21, 59] for selecting network
weights have been studied in the ï¬elds of network prun-
ing and compression. To this end, we propose a Taylor
expansion-based Importance Score (TIS) to evaluate the im-
portance of each weight.
We conjecture that the task-relevant weights highly inï¬‚u-
ence the network output, and removing these weights will
drastically inï¬‚uence the loss value. Thus, the importance of
weight can be quantiï¬ed by the difference in loss induced
by removing this weight. Given a subset fx;ygrandomly
sampled from training set, the importance score Iwj
iof a
weight parameter wj
i2R11can be formulated by
Iwj
i= (L(F(x;W);yjwj
i= 0) L(F(x;W);y))2;(6)
whereLis the task-speciï¬c loss (the cross-entropy loss in
this paper),Fis the transformer network, Wis the total
4

--- PAGE 5 ---
model weights and yare the labels of data x. As previous
studies [43, 59, 62] point out that this score can be approx-
imated with the ï¬rst-order Taylor expansion. Thus, the ï¬-
nal importance score ^Iwj
iof a weight parameter wj
ican be
rewritten as:
^Iwj
i=@L(x)
@wj
iwj
i: (7)
Thus the importance score ^Iwj
ican be represented with a
gradient term and the weight parameter wj
i.
Up to this point, we can use the above score to evalu-
ate the task-relevant weights. However, our method aims to
ï¬nd task-relevant channels of a given feature map. Thus, we
need to translate the task-relevant weights to task-relevant
channels. As shown in Fig. 6, we ï¬rst decompose the
process of linear operation. Assuming a weight matrix
W2RDDand a feature map X2R(N+1)D, we can
get the output Y2R(N+1)Das:
Y= [WXT]T: (8)
We deï¬ne each weight wi2R1DinWand each token
xT
i2RD1inXT2RD(N+1). Summing the items of
output Yin channel-wise we can get:
Sum(Y;dim = 1) = [w1(xT
1+xT
2+:::+xT
N+1);
w2(xT
1+xT
2+:::+xT
N+1);
:::
wD(xT
1+xT
2+:::+xT
N+1); ]:(9)
Thus, we can ï¬nd that task-relevant weights wicould rep-
resent the task-relevant channels of a given feature map Y.
Calculating the importance score of a weight wicould be
approximated by summing over Eq. 7 of all the parameters
inwi,i.e., the ï¬nal importance score Sican be calculate as:
Si=X
j2J^Iwj
i; (10)
whereJrepresents the index set of a weight wiandwj
i2
R11is a parameter in wi.
3.3. Task-Relevant Module
Having obtained Taylor expansion-based Importance
score, we will select top- Ktask-relevant channels of each
feature map in transformer layers. Assuming a feature map
isx2R(N+1)D, we will select Klargest value of impor-
tance score vectorS= [S1;S2;:::;Si2R11;1iD]
in this feature map. The selected feature x02R(N+1)K
then be fed into a trainable linear layer and outputs the trans-
formed feature
x00=x0+ Linear(x0); (11)
ğ·ğ·ğ‘+ 1
ğ‘¤1
ğ‘¤2
ğ‘¤3 â‹…â‹…â‹…
ğ‘¤ğ·
ğ‘¥1ğ‘‡ğ‘¥2ğ‘‡ğ‘¥ğ‘+1ğ‘‡ğ·
ğ‘¤1ğ‘¥1ğ‘‡ğ‘¤1ğ‘¥2ğ‘‡ğ‘¤1ğ‘¥3ğ‘‡ğ‘¤1ğ‘¥ğ‘+1ğ‘‡â‹…â‹…â‹…
ğ‘¤2ğ‘¥1ğ‘‡ğ‘¤2ğ‘¥2ğ‘‡ğ‘¤2ğ‘¥3ğ‘‡ğ‘¤2ğ‘¥ğ‘+1ğ‘‡â‹…â‹…â‹…
ğ‘¤ğ·ğ‘¥1ğ‘‡ğ‘¤ğ·ğ‘¥2ğ‘‡ğ‘¤ğ·ğ‘¥3ğ‘‡ğ‘¤ğ·ğ‘¥ğ‘+1ğ‘‡â‹…â‹…â‹…ğ·
ğ‘+ 1ğ‘¾ğ‘¿ğ‘»
ğ‘¾ğ‘¿ğ‘»=Figure 6: Illustration of the Decomposition of a Linear Op-
eration.
wherex002R(N+1)Kwill be passed into the next opera-
tion in the transformer layer and Linear()is linear layer
operation that the only new involved layer with parame-
tersKK. Here, we adopt a shortcut connection to pre-
serve the original information and prevent error accumula-
tion across the transformer layers. This strategy helps alle-
viate the training difï¬culty.
Tuning channels vs. Tuning weights. Note that if we con-
siderKtask-relevant weights, the extra parameters will be
KD, and the FLOPs will be NKD. On the other
hand, focusing on Ktask-relevant channels only requires
aKKlinear layer for tuning, with the FLOPs count
ofNKK. In this paper, we set our default values
asK= 96 andD= 768 , and the number of extra pa-
rameters for tuning task-relevant weights is 8 times larger
than that for tuning task-relevant channels. Therefore, to
maintain fewer extra parameters for storage, it is better to
tune task-relevant channels instead. We also attempted to
tune the task-relevant weights directly, but the results for
19 downstream tasks were inferior compared to tuning the
task-relevant channels as illustrated in Tab. 7. We hypoth-
esize that the linear combination of task-relevant channels
will contribute more to task performance.
4. Experiments
4.1. Experiments on VTAB-1K Benchmark
Dataset. VTAB-1K [64] contains 19 visual classiï¬cation
tasks which cover a broad spectrum of domains and seman-
tics in three groups, i.e.,Natural ,Specialized , and Struc-
tured . The Natural group contains 7 classic classiï¬cation
datasets [31, 18, 9, 46, 47, 44, 60] of natural images. The
Specialized group involves 4 datasets [53, 22, 8, 30] of two
special scenarios: medical and remote-sensing. The Struc-
tured group has 8 datasets [29, 3, 19, 42, 33], mainly fo-
cusing on understanding the structure of a scene, such as
object counting, and depth prediction. Each task of VTAB-
1K contains 1000 training images. Following [27, 37], we
use the 800-200 TRAIN -VAL split to determine the hyperpa-
rameters and the entire 1000 training data to train the ï¬nal
5

--- PAGE 6 ---
Natural Specialized Structured# Params (M)
Cifar100
Caltech101
DTD
Flower102
Pets
SVHN
Sun397
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSpr-Loc
dSpr-Ori
sNORB-Azim
sNORB-Ele
Average
Full [27] 85.8 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 65.6
Linear?0.04 61.5 88.4 73.9 97.9 86.8 41.8 51.0 80.7 88.6 76.1 74.1 35.4 30.3 35.7 59.8 16.4 24.3 18.0 22.6 56.0
Bias [27] 0.14 72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 62.1
VPT-Shallow [27] 0.11 77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 64.9
VPT-Deep [27] 0.60 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 69.4
Adapter [25] 0.27 69.2 90.0 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 71.4
SSF [37] 0.24 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.5 73.1
TTC-Tuning (ours) 0.19 78.4 92.4 74.0 99.4 91.6 91.6 56.0 88.3 94.6 87.4 76.5 82.0 65.5 54.3 82.3 82.2 55.4 30.9 39.1 74.8
Table 1: Comparisons with state-of-the-art PETL methods on the VTAB-1K benchmark with ViT-B/16 . â€œ?â€ means the
model has been retrained to produce better results. The entries noted by grey represents the baseline algorithms. The best
and second-best results of PETL methods are noted by green and underline , respectively.
Natural Specialized Structured# Params (M)
Cifar100
Caltech101
DTD
Flower102
Pets
SVHN
Sun397
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSpr-Loc
dSpr-Ori
sNORB-Azim
sNORB-Ele
Average
Full [27] 86.7 72.2 88.0 71.4 98.3 89.5 90.1 45.0 86.6 96.9 87.7 79.4 75.7 59.8 54.6 78.6 79.4 53.6 34.6 40.9 74.2
Linear [27] 0.04 61.4 90.2 74.8 99.5 90.2 42.7 55.8 81.5 90.1 82.1 69.4 39.1 35.9 40.1 65.0 20.3 26.0 14.3 27.8 56.4
Bias [27] 0.29 73.0 86.8 65.6 97.7 87.5 56.4 52.3 80.4 91.6 76.1 72.5 47.3 48.5 34.7 66.2 57.6 36.2 34.7 66.2 62.1
VPT-Deep [27] 0.24 79.6 90.8 78.0 99.5 91.4 42.3 51.7 84.9 96.2 85.0 72.0 67.6 59.4 50.1 61.3 74.4 50.6 25.7 25.7 68.6
TTC-Tuning (ours) 0.19 76.1 92.4 76.6 99.7 92.8 88.5 55.1 88.0 95.8 87.5 75.4 82.3 62.5 52.4 83.4 82.6 54.3 30.6 39.8 74.5
Table 2: Comparisons with state-of-the-art methods on the VTAB-1K benchmark with Swin-B .
model. We report the average top-1 accuracy on the TEST
set.
Baselines and state-of-the-art approaches. We compare
our method with three baselines, Full ï¬ne-tuning, Linear,
and Bias, and three state-of-the-art methods Adapter [25],
VPT [27], and SSF [37]. Bias method only updates all the
bias terms in the pre-trained backbone.
Performance with ViT backbone. We compare our TTC-
tuning with the above 7 baselines in Tab. 1. We use ViT-
B/16 as the backbone and insert TTC-Module in each trans-
former layer. The default Kis set to 96, 1/8 of the to-
tal channels, leading to the trainable parameter number be-
ing only 0.11M. First , our TTC-Tuning achieves the aver-
age accuracy of 74.8% on the 19 downstream tasks, out-
performing the full ï¬ne-tuning on 18 out of 19 tasks and
gains the improvement of 6.2%, 3.3%, and 13.9% in the
three groups, respectively, with only additional 0.13% of
the backbone parameters. Such results reï¬‚ect that TTC-
Tuning can greatly reduce the storage space and alleviate
the overï¬tting problem commonly occurring in full ï¬ne-
tuning large models. Second , compared with Adapter [25]
that treats all the channels equally, selecting a part of task-
relevant channels for each downstream task is more effec-
tive and efï¬cient, outperforming it by 3.4% in average ac-
curacy. Moreover, our TTC-Tuning outperforms VPT [27]
by 5.0%, 4.3%, and 6.5% in the three groups, respectively.
Third , compared with the distribution alignment method
SSF [37], our TTC-Tuning surpasses it by 1.7%. These re-
sults demonstrate that instead of aligning distribution only
(i.e., SSF) or learning task-relevant information ( i.e., VPT,
Adapter), leveraging the two-stage paradigm can maintain
lower-level parameter costs and improve the performance.Performance with Swin Transformer Backbone. To ver-
ify the effectiveness of TTC-Tuning with different back-
bones, we apply TTC-Tuning on hierarchical transformers,
i.e., Swin-B [39]. We use the same setting of inserting
TTC-Module as in the ViT backbone. Considering deep
layers contain more semantic information in the hierarchi-
cal structure, instead of applying TTC-Module on all the
transformer layers, we insert it to the last half of the layers
in the stage3 and all layers of the stage4 of the Swin-B to
keep a similar level of trainable parameters. The results of
Tab. 2 show that TTC-Tuning outperforms Full ï¬ne-tuning
in all three groups with only 0.2% parameters while other
methods cannot. In addition, compared with PETL method,
TTC-Tuning outperforms VPT [27] by 6.2%, 2.2%, and
7.6% in the three groups, respectively. All the results above
suggest that our TTC-tuning is also applicable for the hi-
erarchical transformers and can yield much more improve-
ment than other PETL methods.
Complexity Analysis. In our analysis, we consider a ViT-
B backbone with Llayers andDdimensions, along with N
tokens for a single image. We also assume that the inter-
mediate dimension of Adapter [25] is D0, that the prompt
length of VPT [27] is n, and that the total insert times of
SSF [37] is min the whole ViT-B backbone. Finally, we
compare our proposed TTC-Module approach to Adapter,
VPT, and SSF in terms of parameters and FLOPs, as sum-
marized in Tab. 4. Notably, our selection of Kas1
8Dis
fairly small compared to D. When we compare our ap-
proach to SSF, we ï¬nd that the number of parameters for
TTC-Module is1
64LDD , while the number of parameters
for SSF ismLD . Examining the ViT-B backbone, we ï¬nd
thatm= 74 and1
64D= 12 , our parameters and FLOPs are
6

--- PAGE 7 ---
Natural Specialized Structured
No.
LN Tuning Stage1
LN Tuning Stage2
TTC
# Params (M)
Cifar100
Caltech101
DTD
Flower102
Pets
SVHN
Sun397
Camelyon
EuroSAT
Resisc45
Retinopathy
Clevr-Count
Clevr-Dist
DMLab
KITTI-Dist
dSpr-Loc
dSpr-Ori
sNORB-Azim
sNORB-Ele
Average
17 7 7 0.04 61.5 88.4 73.9 97.9 86.8 41.8 51.0 80.7 88.6 76.1 74.1 35.4 30.3 35.7 59.8 16.4 24.3 18.0 22.6 56.0
23 7 7 0.08 74.9 91.6 75.2 99.2 91.4 90.5 55.5 86.6 95.9 87.1 76.1 80.6 65.0 53.1 80.9 75.5 55.4 25.5 37.5 73.6
37 7 3 0.15 74.7 91.6 73.6 99.1 90.8 90.2 52.9 87.4 95.4 86.4 75.1 80.5 63.8 51.5 80.6 78.9 55.5 28.6 40.2 73.1
43 7 3 0.15 77.3 91.7 72.9 99.4 91.1 90.6 54.4 84.2 94.3 87.3 75.4 82.0 65.1 53.0 80.9 82.1 55.9 28.8 38.2 73.9
57 3 3 0.19 75.0 91.9 73.9 99.4 90.8 90.8 54.6 87.6 95.8 87.6 75.3 80.8 64.0 52.4 80.5 82.1 55.3 29.0 40.1 74.1
63 3 3 0.19 78.4 92.4 74.0 99.4 91.6 91.6 56.0 88.3 94.6 87.4 76.5 82.0 65.5 54.3 82.3 82.2 55.4 30.9 39.1 74.8
Table 3: Evaluation of our proposed Paradigm .
Adapter VPT-Deep SSF TTC-Module
# Extra Parameters 2LDD0nLD mLD LKK
# Extra FLOPs 2NLDD02n(2N+n)LD mNLD NLKK
Table 4: A complexity analysis of Adapter [25], VPT [27],
SSF [37], and our proposed TTC-Module.
smaller than SSF. Overall, our analysis suggests that TTC-
Module may offer a more efï¬cient and effective approach
to transfer learning.
4.2. Evaluation
Ablation Studies. To evaluate the effectiveness of tuning
the LayerNorm layer and our proposed TTC-Module, we
conduct ablation studies on the two components in our two-
stage paradigm in Tab. 3. First , in the ï¬rst stage, we ï¬ne-
tune LayerNorm to align task distribution, which has al-
ready outperformed the previous best model (73.6% (2nd
row of Tab. 3) v.s.73.1% (SSF). Second , when combining
the second stage on top of the LayerNorm tuning, the TTC-
Module can yield an improvement of 1.2%. Third , to fur-
ther verify the effectiveness of TTC-Module, we insert TTC
directly into the baseline Linear model, gaining an improve-
ment of 17.1% (4th row). Fourth , we tune the LayerNorm
and TTC-Module together in one stage (5th row), achieving
an accuracy of 74.1%, worse than the two-stage paradigm
by 0.7%. All the results above demonstrate the effective-
ness of the proposed LayerNorm tuning, TTC-Module, and
the necessity of a two-stage paradigm.
Tuning Channels vs. Tuning Weights As illustrated in
Sec. 3.3, tuning selected Kchannels via a linear adapter
only useK
D=1
8parameters of directly tuning the weights
of ViT layer. In addition, with fewer learnable parameters,
the model is less prone to overï¬tting to the small dataset.
We compare the performance of tuning weights and tuning
channels in Tab. 7. The number of parameters of tuning
weights is relatively high (0.88M) while tuning channels
with only 0.11M parameters can gain 7.8% improvements
in total 19 downstream tasks.
Effectiveness of Task-Relevant Channel Selection. To
verify the effectiveness and necessity of the proposed Taylor
expansion-based Importance Score (TIS) channel selection,
we compare three channel selection strategies in Tab. 5a.
These strategies included Random Channel Selection (RC),
L2 Norm, and Taylor expansion-based Importance Score(TIS). RC selects Kchannels randomly, and to reduce the
impact of outliers, we randomly selected three sets of chan-
nels (RC-1/2/3). L2 Norm determines task-relevant chan-
nels based on the L2 Norm of features in each channel. The
task-relevant strategies achieve better and more robust per-
formance than RC. In addition, our TIS can select more im-
portant and representative channels than L2 Norm, outper-
forming it by 3.0%.
Insert Depth. Insert depth is one important factor that in-
ï¬‚uences performance. We report the results when insert-
ing TTC-Module to the last llayers of ViT-B in Tab. 5b.
Without the TTC-Module (stage one only), the accuracy
is 74.9%, while the accuracy gradually improved to 78.4%
with the increase of insert depth. Upon analyzing the results
in Tab. 5b, we observe that inserting the TTC-Module only
in the last two layers achieved an accuracy of 77.8%, indi-
cating that deeper layers contribute more to the ï¬nal results.
Notably, when we remove the TTC-Module in the ï¬rst four
layers, the accuracy was 77.9%, with only a 0.5% gap to the
best result of 78.4%.
Insert Position. We evaluated the insertion position of our
TTC-Module, as shown in Tab. 5c. Speciï¬cally, we in-
sert the module after the MHSA and MLP blocks, respec-
tively. Our ï¬ndings indicate that inserting the module after
the MLP block yields better results, consistent with similar
ï¬ndings for SSF. Additionally, we insert our module after
both blocks, which led to lower performance at 77.1% with
an increase in parameters. We conjecture that only one po-
sition is enough for adaptation, and repeat adaptation will
increase the difï¬culty of optimization.
Number of selected channels K.The number of selected
channels (K) is the most important hyperparameter related
to the design of TTC-Module, inï¬‚uencing the model ar-
chitecture and the number of trainable parameters. Unlike
VPT, which selects the best prompt length for each task,
we use the same Kfor all tasks for a fair comparison. In
Tab. 5d, as we increase the value of K, the performance
improves and peaks at K= 96 . When further increasing
the learnable channels, the performance degrades. We hy-
pothesize that a larger value of Kmay involve too much
task-irrelevant information and can make tuning hyperpa-
rameters more difï¬cult.
Analysis. In Figure 7, we analyzed the parameter shift after
tuning the LayerNorm layer (stage1) and jointly tuning Lay-
7

--- PAGE 8 ---
Acc. Params. (M)
Linear?61.5 0.08
RC-1 72.1 0.23
RC-2 74.1 0.23
RC-3 71.3 0.23
L2 Norm 75.4 0.23
TIS 78.4 0.23
(a)Channel Selection .Layers Acc. Params. (M)
Linear?61.5 0.08
0 74.9 0.12
2 77.8 0.13
4 77.7 0.15
8 77.9 0.19
12 78.4 0.23
(b)Insert depth .Insert Position Acc. # Params. (M)
Full 68.9 86.7
Linear?61.5 0.08
LayerNorm?73.6 0.11
MHSA 78.4 0.23
MLP 78.6 0.23
MHSA+MLP 77.1 0.34
(c)Insert position .Top-K Acc. Params. (M)
Linear?61.5 0.08
32 78.2 0.13
64 77.7 0.17
96 78.4 0.23
128 77.4 0.31
192 76.5 0.56
(d)Different K.
Table 5: Evaluation of different designs . Acc.: Top-1 accuracy (%); Params.: parameters (M). Linear?represents the
baseline results for better comparison.
1 2 3 4 5 6 7 8 9 10 11 12
Layers0.00.51.01.52.0JSD density1e4
 Norm1-Weight
Original
LayerNorm
TTC-Tuning
1 2 3 4 5 6 7 8 9 10 11 12
Layers0123456JSD density1e5
 Norm1-Bias
Original
LayerNorm
TTC-Tuning
1 2 3 4 5 6 7 8 9 10 11 12
Layers0.00.51.01.52.02.5JSD density1e4
 Norm2-Weight
Original
LayerNorm
TTC-Tuning
1 2 3 4 5 6 7 8 9 10 11 12
Layers0.00.51.01.52.0JSD density1e4
 Norm2-Bias
Original
LayerNorm
TTC-Tuning
Figure 7: Comparison of parameter shift after tuning the LayerNorm layer (stage1) and jointly tuning LayerNorm and TTC-
Module (stage2).
erNorm and TTC-Module (stage2). Our ï¬ndings indicate
that deeper layers led to larger shifts in the weight and bias
of both â€œNorm1â€ and â€œNorm2â€ (the two LayerNorm lay-
ers in ViT-B). Speciï¬cally, we observed obvious deviations
in the weight parameter of the shallow layers of â€Norm2â€
which differed from â€Norm1â€. We also evaluate the repre-
sentation ability to conduct stage1 tuning in Tab. 8 by uti-
lizing KNN [10] algorithm to cluster the feature of [CLS]
token. The results suggest that stage1 is indeed effective in
improving representation ability.
4.3. Experiments on Domain Generalization
In addition to evaluating the model on test data of the
same distribution, modern deep neural networks commonly
suffer from performance degradation when the testing dis-
tribution is different from that of the training set, i.e., do-
main shift, which is inevitable in a real-world application.
Dataset. We use the ImageNet-1K [11] as the source do-
main with 16-shot per category and evaluate our model
on ImageNetV2 [50], ImageNet-Sketch [55], ImageNet-
A [24], and ImageNet-R [23].
Results. In Tab. 6, we compare our TTC-tuning with
Adapter [25], VPT [27], LoRA [26], and NOAH [66] on
the above datasets. We can make two observations. First ,
TTC-tuning outperforms the previous best method (NOAH)
on three of the four target datasets and achieves compa-
rable performance on ImageNetV2. Speciï¬cally, TTC-
tuning yields an improvement of 0.9% on ImageNet-R
over NOAH. Second , our TTC-tuning achieves an accu-
racy of 75.5% on the source domain, greatly outperform-
ing previous methods by 4%. Since the backbone model is
pre-trained on ImageNet-21K, the results on ImageNet-1K
show that TTC-tuning can better align the supersetâ€™s com-
plex distribution with the subsetâ€™s relatively simple distribu-
tion. The two observations demonstrate the superiority of
our TTC-tuning over previous PETL techniques on strongSource Target
ImageNet -V2 -Sketch -A -R
Adapter [25] 70.5 59.1 16.4 5.5 22.1
VPT [27] 70.5 58.0 18.3 4.6 23.2
LoRA [26] 70.8 59.3 20.0 6.9 23.3
NOAH [66] 71.5 66.1 24.8 11.9 28.5
TTC-Tuning (ours) 75.5 65.9 25.6 11.9 29.4
Table 6: Comparison with previous methods on domain
generalization.
# Params (M)VTAB-1K
Natural Specialized Structured Average
Tuning weights 0.88+0.08 78.1 84.2 57.1 72.9
Tuning Channels 0.11+0.08 83.4 +5:3 86.7 +2:5 61.5 +4:4 74.8 +1:9
Table 7: We evaluated the effects of tuning task-relevant
weights and channels on 19 downstream tasks. The a+b no-
tation represents the combination of parameters introduced
by both external and internal modules.
Method SVHN EuroSAT Clevr-Count
w/o stage1 34.50 84.44 29.00
w/ stage1 91.23 92.96 80.33
Table 8: We evaluate the stage1 using the KNN algorithm
to test the representation ability of [CLS] token.
generalization ability.
5. Conclusion
Since previous PETL methods can be divided into two
streams: learning task-relevant information and aligning the
distributions between pre-trained and downstream tasks, we
ï¬rst propose a two-stage paradigm by combining the two
lines of approaches. We ï¬rst narrow the distribution shifts
and propose a Taylor expansion-based importance score to
select task-relevant channels for efï¬cient adaption. In sum-
mary, our novel paradigm represents a new direction em-
phasizing the importance of considering distribution shifts
when ï¬ne-tuning downstream tasks.
8

--- PAGE 9 ---
References
[1] Alaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bo-
janowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Na-
talia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit:
Cross-covariance image transformers. In NeurIPS , 2021.
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016.
[3] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward,
Marcus Wainwright, Heinrich K Â¨uttler, Andrew Lefrancq, Si-
mon Green, V Â´Ä±ctor Vald Â´es, Amir Sadik, et al. Deepmind lab.
arXiv preprint arXiv:1612.03801 , 2016.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. In NeurIPS , 2020.
[5] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda.
Crossvit: Cross-attention multi-scale vision transformer for
image classiï¬cation. In Proceedings of the IEEE/CVF in-
ternational conference on computer vision , pages 357â€“366,
2021.
[6] Hao Chen, Ran Tao, Han Zhang, Yidong Wang, Wei Ye,
Jindong Wang, Guosheng Hu, and Marios Savvides. Conv-
adapter: Exploring parameter efï¬cient transfer learning for
convnets. arXiv preprint arXiv:2208.07463 , 2022.
[7] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,
Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapt-
ing vision transformers for scalable visual recognition. arXiv
preprint arXiv:2205.13535 , 2022.
[8] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens-
ing image scene classiï¬cation: Benchmark and state of the
art.Proceedings of the IEEE , 105(10):1865â€“1883, 2017.
[9] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A.
Vedaldi. Describing textures in the wild. In Proceedings of
the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) , 2014.
[10] Thomas Cover and Peter Hart. Nearest neighbor pattern
classiï¬cation. IEEE transactions on information theory ,
13(1):21â€“27, 1967.
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical im-
age database. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
248â€“255. Ieee, 2009.
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018.
[13] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming
Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining
Guo. Cswin transformer: A general vision transformer
backbone with cross-shaped windows. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12124â€“12134, 2022.
[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020.
[15] St Â´ephane dâ€™Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S
Morcos, Giulio Biroli, and Levent Sagun. Convit: Improv-
ing vision transformers with soft convolutional inductive bi-
ases. In International Conference on Machine Learning ,
pages 2286â€“2296. PMLR, 2021.
[16] Dominik Maria Endres and Johannes E Schindelin. A new
metric for probability distributions. IEEE Transactions on
Information theory , 49(7):1858â€“1860, 2003.
[17] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,
Zhicheng Yan, Jitendra Malik, and Christoph Feichten-
hofer. Multiscale vision transformers. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 6824â€“6835, 2021.
[18] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-
ative visual models from few training examples: An incre-
mental bayesian approach tested on 101 object categories.
Inconference on computer vision and pattern recognition
workshop , pages 178â€“178. IEEE, 2004.
[19] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The kitti dataset. The Inter-
national Journal of Robotics Research , 32(11):1231â€“1237,
2013.
[20] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,
and Yunhe Wang. Transformer in transformer. In NeurIPS ,
2021.
[21] Y He, G Kang, X Dong, Y Fu, and Y Yang. Soft ï¬lter
pruning for accelerating deep convolutional neural networks.
InIJCAI International Joint Conference on Artiï¬cial Intelli-
gence , 2018.
[22] Patrick Helber, Benjamin Bischke, Andreas Dengel, and
Damian Borth. Eurosat: A novel dataset and deep learning
benchmark for land use and land cover classiï¬cation. IEEE
Journal of Selected Topics in Applied Earth Observations
and Remote Sensing , 2019.
[23] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-
vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,
Samyak Parajuli, Mike Guo, et al. The many faces of robust-
ness: A critical analysis of out-of-distribution generalization.
InProceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pages 8340â€“8349, 2021.
[24] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-
hardt, and Dawn Song. Natural adversarial examples. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 15262â€“15271,
2021.
[25] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efï¬cient transfer
learning for nlp. In International Conference on Machine
Learning (ICML) , pages 2790â€“2799. PMLR, 2019.
[26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021.
9

--- PAGE 10 ---
[27] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,
Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-
sual prompt tuning. In ECCV , 2022.
[28] Shibo Jie and Zhi-Hong Deng. Convolutional bypasses
are better vision transformer adapters. arXiv preprint
arXiv:2207.07039 , 2022.
[29] Justin Johnson, Bharath Hariharan, Laurens Van
Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. Clevr: A diagnostic dataset for compositional
language and elementary visual reasoning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 2901â€“2910, 2017.
[30] Kaggle and EyePacs. Kaggle diabetic retinopathy detection.
2015.
[31] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009.
[32] Solomon Kullback and Richard A Leibler. On informa-
tion and sufï¬ciency. The annals of mathematical statistics ,
22(1):79â€“86, 1951.
[33] Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning
methods for generic object recognition with invariance to
pose and lighting. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
volume 2, pages IIâ€“104. IEEE, 2004.
[34] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam
Shazeer, and Zhifeng Chen. Gshard: Scaling giant models
with conditional computation and automatic sharding. arXiv
preprint arXiv:2006.16668 , 2020.
[35] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning ï¬lters for efï¬cient convnets. arXiv
preprint arXiv:1608.08710 , 2016.
[36] Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and
Xiaodi Hou. Revisiting batch normalization for practical do-
main adaptation. arXiv preprint arXiv:1603.04779 , 2016.
[37] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao
Wang. Scaling & shifting your features: A new baseline for
efï¬cient model tuning. In Advances in Neural Information
Processing Systems (NeurIPS) , 2022.
[38] Lingbo Liu, Bruce XB Yu, Jianlong Chang, Qi Tian, and
Chang-Wen Chen. Prompt-matched semantic segmentation.
arXiv preprint arXiv:2208.10159 , 2022.
[39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 10012â€“10022, 2021.
[40] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A ï¬lter
level pruning method for deep neural network compression.
InProceedings of the IEEE international conference on com-
puter vision , pages 5058â€“5066, 2017.
[41] Xu Luo, Jing Xu, and Zenglin Xu. Channel importance mat-
ters in few-shot image classiï¬cation. In International con-
ference on machine learning , pages 14542â€“14559. PMLR,
2022.
[42] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander
Lerchner. dsprites: Disentanglement testing sprites dataset,
2017.[43] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Fro-
sio, and Jan Kautz. Importance estimation for neural net-
work pruning. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 11264â€“
11272, 2019.
[44] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-
sacco, Bo Wu, and Andrew Y Ng. Reading digits in natural
images with unsupervised feature learning. 2011.
[45] Xing Nie, Bolin Ni, Jianlong Chang, Gaomeng Meng, Chun-
lei Huo, Zhaoxiang Zhang, Shiming Xiang, Qi Tian, and
Chunhong Pan. Pro-tuning: Uniï¬ed prompt tuning for vi-
sion tasks. arXiv preprint arXiv:2207.14381 , 2022.
[46] M-E Nilsback and Andrew Zisserman. A visual vocabulary
for ï¬‚ower classiï¬cation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , volume 2, pages 1447â€“1454. IEEE, 2006.
[47] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and
CV Jawahar. Cats and dogs. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 3498â€“3505. IEEE, 2012.
[48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a uniï¬ed text-to-text transformer. The Journal of Machine
Learning Research , 21(1):5485â€“5551, 2020.
[49] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie
Zhou, and Cho-Jui Hsieh. Dynamicvit: Efï¬cient vision
transformers with dynamic token sparsiï¬cation. In NeurIPS ,
2021.
[50] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
Vaishaal Shankar. Do imagenet classiï¬ers generalize to im-
agenet? In International Conference on Machine Learning
(ICML) , pages 5389â€“5400. PMLR, 2019.
[51] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,
Gabriel Synnaeve, and Herv Â´e JÂ´egou. Going deeper with im-
age transformers. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 32â€“42, 2021.
[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , 2017.
[53] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Co-
hen, and Max Welling. Rotation equivariant cnns for digital
pathology. In International Conference on Medical image
computing and computer-assisted intervention , pages 210â€“
218. Springer, 2018.
[54] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol-
shausen, and Trevor Darrell. Tent: Fully test-time adaptation
by entropy minimization. In ICLR , 2021.
[55] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P
Xing. Learning robust global representations by penalizing
local predictive power. In NeurIPS , 2019.
[56] Pichao Wang, Xue Wang, Fan Wang, Ming Lin, Shuning
Chang, Wen Xie, Hao Li, and Rong Jin. Kvt: k-nn at-
tention for boosting vision transformers. arXiv preprint
arXiv:2106.00515 , 2021.
[57] Shijie Wang, Jianlong Chang, Zhihui Wang, Haojie Li, Wanli
Ouyang, and Qi Tian. Fine-grained retrieval prompt tuning.
arXiv preprint arXiv:2207.14465 , 2022.
10

--- PAGE 11 ---
[58] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions. In Proceedings of the
IEEE/CVF international conference on computer vision ,
pages 568â€“578, 2021.
[59] Zhenyu Wang, Hao Luo, WANG Pichao, Feng Ding, Fan
Wang, and Hao Li. Vtc-lfc: Vision transformer compression
with low-frequency components. In Advances in Neural In-
formation Processing Systems .
[60] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,
and Antonio Torralba. Sun database: Large-scale scene
recognition from abbey to zoo. In 2010 IEEE computer so-
ciety conference on computer vision and pattern recognition ,
pages 3485â€“3492. IEEE, 2010.
[61] Yinghui Xing, Qirui Wu, De Cheng, Shizhou Zhang, Guo-
qiang Liang, and Yanning Zhang. Class-aware visual prompt
tuning for vision-language pre-trained model. arXiv preprint
arXiv:2208.08340 , 2022.
[62] Huanrui Yang, Hongxu Yin, Pavlo Molchanov, Hai Li, and
Jan Kautz. Nvit: Vision transformer compression and param-
eter redistribution. arXiv preprint arXiv:2110.04869 , 2021.
[63] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,
Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng
Yan. Tokens-to-token vit: Training vision transformers from
scratch on imagenet. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 558â€“567,
2021.
[64] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov,
Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo-
longa, Andre Susano Pinto, Maxim Neumann, Alexey Doso-
vitskiy, et al. A large-scale study of representation learning
with the visual task adaptation benchmark. arXiv preprint
arXiv:1910.04867 , 2019.
[65] Bowen Zhang, Xiaojie Jin, Weibo Gong, Kai Xu, Zhao
Zhang, Peng Wang, Xiaohui Shen, and Jiashi Feng. Mul-
timodal video adapter for parameter efï¬cient video text re-
trieval. arXiv preprint arXiv:2301.07868 , 2023.
[66] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural
prompt search. arXiv preprint arXiv:2206.04673 , 2022.
[67] Zangwei Zheng, Xiangyu Yue, Kai Wang, and Yang You.
Prompt vision transformer for domain generalization. arXiv
preprint arXiv:2208.08914 , 2022.
[68] Jingkai Zhou, Pichao Wang, Fan Wang, Qiong Liu, Hao Li,
and Rong Jin. Elsa: Enhanced local self-attention for vision
transformer. arXiv preprint arXiv:2112.12786 , 2021.
11
