# Cải thiện tham số hiệu quả cho việc tinh chỉnh các mô hình ngôn ngữ lớn

Yuchao Li, Fuli Luo, Chuanqi Tan, Mengdi Wang,
Songfang Huang, Shen Li, Junjie Bai
Tập đoàn Alibaba
{laiyin.lyc, lfl259702, chuanqi.tcq, didou.wmd, songfang.hsf, litan.ls, j.bai}@alibaba-inc.com

## Tóm tắt

Với số lượng tham số tăng lên đáng kể trong các mô hình ngôn ngữ, các phương pháp thưa thớt đã nhận được sự chú ý nghiên cứu ngày càng tăng để nén và tăng tốc các mô hình. Trong khi hầu hết các nghiên cứu tập trung vào cách giữ lại chính xác các trọng số phù hợp trong khi duy trì hiệu suất của mô hình nén, có những thách thức trong chi phí tính toán và dung lượng bộ nhớ của việc huấn luyện thưa thớt khi nén các mô hình ngôn ngữ quy mô lớn. Để giải quyết vấn đề này, chúng tôi đề xuất phương pháp Huấn luyện Thưa thớt Hiệu quả Tham số (PST) để giảm số lượng tham số có thể huấn luyện trong quá trình huấn luyện nhận biết độ thưa thớt trong các tác vụ downstream. Cụ thể, chúng tôi đầu tiên kết hợp các tiêu chí không dữ liệu và dựa trên dữ liệu để đo lường tầm quan trọng của trọng số một cách hiệu quả và chính xác. Sau đó, chúng tôi khảo sát tính dư thừa nội tại của tầm quan trọng trọng số dựa trên dữ liệu và rút ra hai đặc tính rõ ràng tức là tính hạng thấp và tính có cấu trúc. Dựa trên đó, hai nhóm ma trận nhỏ được giới thiệu để tính toán tầm quan trọng dựa trên dữ liệu của trọng số, thay vì sử dụng ma trận điểm tầm quan trọng lớn ban đầu, do đó làm cho việc huấn luyện thưa thớt tiết kiệm tài nguyên và hiệu quả tham số. Các thí nghiệm với các mạng đa dạng (tức là BERT, RoBERTa và GPT-2) trên hàng chục bộ dữ liệu chứng minh PST hoạt động ngang bằng hoặc tốt hơn so với các phương pháp thưa thớt trước đây, mặc dù chỉ huấn luyện một số lượng nhỏ tham số. Chẳng hạn, so với các phương pháp thưa thớt trước đây, PST của chúng tôi chỉ yêu cầu 1,5% tham số có thể huấn luyện để đạt được hiệu suất tương đương trên BERT.

## 1 Giới thiệu

Nhiều ứng dụng trong xử lý ngôn ngữ tự nhiên đã tuân theo một mô hình, trong đó đầu tiên pre-train một mô hình ngôn ngữ lớn và sau đó tinh chỉnh nó hướng tới nhiều tác vụ downstream. Mặc dù có thành công lớn, các mô hình ngôn ngữ quy mô lớn với hàng triệu đến hàng tỷ tham số này cần dung lượng bộ nhớ rất lớn và chi phí tính toán trong việc tinh chỉnh các bộ dữ liệu downstream và cả giai đoạn suy luận, điều này ngăn cản chúng được áp dụng trực tiếp vào các tác vụ khác nhau.

Để giảm thiểu gánh nặng tính toán và bộ nhớ trong suy luận mô hình ngôn ngữ, một hướng đầy hứa hẹn là cắt tỉa [McCarley et al., 2019; Zhang and He, 2020], loại bỏ các trọng số/kênh/lớp không quan trọng một cách độc lập để giảm chi phí tính toán và bộ nhớ. Trong số này, việc cắt tỉa không có cấu trúc, tức là độ thưa thớt, được nghiên cứu rộng rãi vì nó có thể đạt được tỷ lệ nén cao hơn với hiệu suất cạnh tranh.

Các phương pháp thưa thớt trước đây đề xuất các tiêu chí khác nhau để tính toán tầm quan trọng của mỗi trọng số, có thể được phân loại thô thành hai loại: không dữ liệu [Han et al., 2015; Tanaka et al., 2020] và dựa trên dữ liệu [Sanh et al., 2020; Wang et al., 2020a]. So sánh được thể hiện trong Bảng 1. Các phương pháp tiêu chí không dữ liệu tính toán tầm quan trọng của trọng số dựa trên chính trọng số đó mà không có dữ liệu nào liên quan, chẳng hạn như cắt tỉa độ lớn (MaP) [Han et al., 2015]. Mặc dù các tiêu chí không dữ liệu có hiệu quả tính toán và bộ nhớ cao, chúng bỏ qua rằng vai trò của mỗi trọng số thay đổi rộng rãi qua các tác vụ downstream khác nhau, dẫn đến suy giảm hiệu suất mô hình. Các phương pháp tiêu chí dựa trên dữ liệu điển hình tập trung vào thiết kế các tiêu chí quan trọng chính xác để tính toán điểm tầm quan trọng dựa trên bộ dữ liệu cụ thể, được chứng minh là thành công trong việc giảm chi phí suy luận tính toán của mô hình ngôn ngữ mà không có sự giảm hiệu suất. Tuy nhiên, các tiêu chí dựa trên dữ liệu này giới thiệu thêm tính toán và tham số có thể huấn luyện để có được đo lường tầm quan trọng, điều này làm tăng đáng kể dung lượng bộ nhớ và chi phí tính toán trong quá trình huấn luyện nhận biết độ thưa thớt. Ví dụ, cắt tỉa chuyển động (MvP) [Sanh et al., 2020] tính toán tầm quan trọng bằng cách nhân trọng số và gradient của chúng và do đó cần bộ nhớ thêm để lưu ma trận điểm tầm quan trọng, có cùng kích thước với trọng số. GraSP [Wang et al., 2020a] giới thiệu chi phí tính toán thêm để tính tích hessian-gradient.

Trong bài báo này, chúng tôi đề xuất phương pháp Huấn luyện Thưa thớt Hiệu quả Tham số (PST) để giảm số lượng tham số liên quan đến tính toán tầm quan trọng trọng số, có thể giải quyết vấn đề yêu cầu tài nguyên trong huấn luyện thưa thớt trong khi tính toán điểm tầm quan trọng chính xác. Xem xét hiệu quả của tiêu chí không dữ liệu và độ chính xác của tiêu chí dựa trên dữ liệu, sự kết hợp của chúng được áp dụng để tận dụng lợi thế của cả hai. Sau đó, để giảm số lượng tham số có thể huấn luyện thêm, tức là điểm tầm quan trọng được giới thiệu bởi tiêu chí dựa trên dữ liệu, việc huấn luyện ma trận tầm quan trọng lớn được chuyển đổi thành việc điều chỉnh nhiều ma trận nhỏ, dựa trên hai quan sát cơ bản sau đây:

• **Tính hạng thấp**: chúng tôi phân tích hạng của trọng số và gradient dựa trên các công trình trước đây và quan sát rằng tất cả chúng đều có hạng cực thấp, có nghĩa là hạng của ma trận điểm tầm quan trọng (kết hợp của ma trận trọng số và gradient) cũng nhỏ. Do đó nó có thể được biểu diễn bằng một tập hợp các ma trận phân tách hạng (tức là A và B trong Bảng 1 và Hình 1).

• **Tính có cấu trúc**: chúng tôi khảo sát phân phối của trọng số thưa thớt và quan sát hiện tượng rằng có một số hàng/cột ít quan trọng hơn những cái khác nói chung, điều này truyền cảm hứng cho chúng tôi giới thiệu một tập hợp các ma trận nhỏ để đo lường tầm quan trọng của mỗi hàng/cột trong trọng số (tức là R và C trong Bảng 1 và Hình 1).

Hai tập hợp ma trận nhỏ được giới thiệu để biểu diễn tính hạng thấp và tính có cấu trúc trong điểm tầm quan trọng dựa trên dữ liệu, tương ứng. Việc tính toán điểm tầm quan trọng trong tác vụ downstream cụ thể được tái cấu trúc bởi các ma trận nhỏ này. Với việc thay thế, yêu cầu tài nguyên cho tính toán tiêu chí dựa trên dữ liệu được giảm đáng kể. Hơn nữa, chúng tôi càng giảm số lượng tham số có thể huấn luyện bằng cách biểu diễn cập nhật trọng số với phân tách hạng thấp, tối ưu hóa một tập hợp ma trận hạng thấp thay vì trọng số để nắm bắt sự thay đổi của nó.

Đóng góp của chúng tôi có thể được tóm tắt như sau:

• Chúng tôi đề xuất phương pháp Huấn luyện Thưa thớt Hiệu quả Tham số (PST), giảm số lượng tham số có thể huấn luyện cho việc huấn luyện thưa thớt mô hình ngôn ngữ lớn và do đó tối ưu hóa quy trình tinh chỉnh và suy luận theo cách hiệu quả tham số.

• Chúng tôi khai thác cả tính hạng thấp và tính có cấu trúc trong điểm tầm quan trọng dựa trên dữ liệu và do đó thay thế nó bằng nhiều ma trận nhỏ. Điều này dẫn đến một lĩnh vực nghiên cứu mới, cách nén tính dư thừa của điểm tầm quan trọng để có được tầm quan trọng của trọng số một cách hiệu quả.

• Các thí nghiệm mở rộng chứng minh tính hiệu quả của phương pháp của chúng tôi trên các mô hình ngôn ngữ lớn pre-trained điển hình khác nhau (ví dụ: BERT, RoBERTa và GPT-2) trên các bộ dữ liệu đa dạng. Đặc biệt, so với các công trình trước đây, PST đạt được 98,5% tiết kiệm tham số có thể huấn luyện với cải thiện điểm trung bình 0,12 trong GLUE.

## 2 Các công trình liên quan

**Tinh chỉnh hiệu quả tham số.** Tinh chỉnh hiệu quả tham số giảm số lượng tham số có thể huấn luyện bằng cách tối ưu hóa các mô-đun nhẹ khác nhau thay vì trọng số pre-trained ban đầu. Chẳng hạn, [Houlsby et al., 2019] giới thiệu một adapter có thể huấn luyện với số lượng tham số nhỏ để đạt được tinh chỉnh hiệu quả tham số. [Lester et al., 2021] đề xuất tinh chỉnh prompt hiệu quả chỉ tối ưu hóa một vector cụ thể cho tác vụ nhỏ. [He et al., 2021] trình bày một khung thống nhất sử dụng nhiều mô-đun từ các công trình trước đây. Ngoài ra, [Guo et al., 2020] đề xuất chỉ cập nhật một số lượng nhỏ các phần tử trong các vector có thể huấn luyện cho tinh chỉnh hiệu quả tham số. [Hu et al., 2021] giới thiệu hai ma trận hạng thấp để xấp xỉ các cập nhật tham số. Tuy nhiên, các mô hình được tinh chỉnh bởi các phương pháp này có cùng số lượng trọng số như mô hình pre-trained, vẫn dẫn đến chi phí tính toán và bộ nhớ lớn khi suy luận. Khác với chúng, chúng tôi đề xuất phương pháp huấn luyện thưa thớt hiệu quả tham số để cắt tỉa các trọng số không quan trọng trong mô hình ngôn ngữ trong quá trình huấn luyện, giảm yêu cầu tài nguyên của suy luận mạng.

**Suy luận hiệu quả tham số.** Có một số kỹ thuật nén mô hình ngôn ngữ phổ biến, ví dụ: cắt tỉa, lượng tử hóa và phân tách hạng thấp. Trong số này, cắt tỉa được sử dụng rộng rãi, giảm số lượng tham số trong suy luận mạng. Cắt tỉa có cấu trúc trực tiếp loại bỏ trọng số có cấu trúc (ví dụ: đầu attention [McCarley et al., 2019], kênh [Wang et al., 2020b] hoặc lớp [Zhang and He, 2020]) để nén và tăng tốc các mô hình ngôn ngữ lớn. Ngược lại, cắt tỉa không có cấu trúc, tức là độ thưa thớt, loại bỏ các trọng số không quan trọng riêng lẻ một cách độc lập. Các công trình trước đây đề xuất các tiêu chí khác nhau để chọn trọng số không đáng kể để cắt tỉa, chẳng hạn như trọng số tuyệt đối [Gordon et al., 2020], xấp xỉ taylor [Molchanov et al., 2019], tích hessian-gradient [Wang et al., 2020a] và điểm nổi bật không dữ liệu [Tanaka et al., 2020]. Tuy nhiên, các phương pháp này hoặc đề xuất tiêu chí tầm quan trọng hiệu quả tính toán nhưng dẫn đến hiệu suất mạng tệ hơn (tức là cắt tỉa độ lớn), hoặc thiết kế tiêu chí tầm quan trọng chính xác có thể cần chi phí tính toán lớn (tức là cắt tỉa chuyển động và GraSP). Khác với các phương pháp này, cách tiếp cận của chúng tôi khai thác tính dư thừa nội tại của ma trận tầm quan trọng trọng số và đề xuất huấn luyện thưa thớt hiệu quả tham số để có được mạng thưa thớt tốt hơn với yêu cầu tài nguyên thấp hơn.

## 3 Phương pháp đề xuất

### 3.1 Kiến thức cơ bản

Chúng tôi đầu tiên thiết lập ký hiệu chung để phân tích các phương pháp thưa thớt. Nói chung, đối với ma trận trọng số W ∈ ℝⁿˣᵏ, một chiến lược thưa thớt mạng giới thiệu điểm tầm quan trọng S ∈ ℝⁿˣᵏ để xác định trọng số nào nên được loại bỏ. Dựa trên S, một mặt nạ nhị phân M ∈ {0,1}ⁿˣᵏ có thể được tạo ra cho tính toán Y = (W ⊙ M)X, trong đó Y ∈ ℝⁿˣᵐ và X ∈ ℝᵏˣᵐ lần lượt là đầu ra và đầu vào của lớp. ⊙ biểu thị tích Hadamard. Một chiến lược phổ biến là giữ top-v của trọng số W dựa trên điểm tầm quan trọng S. Do đó, chúng tôi định nghĩa hàm f(S,v) chọn v giá trị lớn nhất trong S để tạo ra mặt nạ nhị phân M:

Mᵢ,ⱼ = f(S,v)ᵢ,ⱼ = {1, nếu Sᵢ,ⱼ trong top-v;
                      0, ngược lại.                    (1)

Trong công trình này, chúng tôi tập trung vào huấn luyện thưa thớt lặp lại, loại bỏ các trọng số không quan trọng và cập nhật điểm tầm quan trọng từng bước. Các phương pháp trước đây chứng minh rằng chiến lược này cho phép mạng phục hồi từ mất mát thông tin do độ thưa thớt. Do đó, quá trình tối ưu hóa của tinh chỉnh mô hình ngôn ngữ là:

min_{W,S} L(W ⊙ f(S,v); D); s.t. v/(nk) ≤ 1-p                  (2)

trong đó D là bộ dữ liệu quan sát, L biểu thị hàm mất mát, và p biểu thị tỷ lệ nén mục tiêu. Việc cập nhật S phụ thuộc vào các chiến lược thưa thớt khác nhau. Ví dụ, cắt tỉa chuyển động [Sanh et al., 2020] sử dụng S⁽ᵗ⁾ = ∑ᵢ₌₁ᵗ (∂L/∂W)⁽ᵢ⁾ ⊙ W⁽ᵢ⁾ để tính toán điểm tầm quan trọng.

### 3.2 Huấn luyện Thưa thớt Hiệu quả Tham số

Như được trình bày trong [Zhao et al., 2020] và [Zhang et al., 2021], mặt nạ nhị phân cuối cùng được tạo ra bởi điểm tầm quan trọng có thể huấn luyện tương tự như mặt nạ được tạo ra trực tiếp bởi cắt tỉa độ lớn, và sự khác biệt giữa chúng phụ thuộc vào bộ dữ liệu cụ thể. Điều này có nghĩa là tầm quan trọng của mỗi trọng số phụ thuộc vào giá trị tuyệt đối của nó và vai trò của nó trong các tác vụ downstream. Do đó, chúng tôi đề xuất điểm tầm quan trọng mới S⁽ᵗ⁾ = |W⁽ᵗ⁾| + Ŝ⁽ᵗ⁾, trong đó |W⁽ᵗ⁾| và Ŝ⁽ᵗ⁾ lần lượt biểu thị tầm quan trọng không dữ liệu và dựa trên dữ liệu của trọng số tại bước thứ t. Được truyền cảm hứng từ các công trình trong [Sanh et al., 2020; Zhang et al., 2021], chúng tôi có thể trực tiếp tối ưu hóa điểm tầm quan trọng bằng SGD để có được điểm tầm quan trọng dựa trên dữ liệu Ŝ, và do đó điểm tầm quan trọng tại bước thứ t được viết lại như:

S⁽ᵗ⁾ = |W⁽ᵗ⁾| + λ∑ᵢ₌₁ᵗ (∂L/∂W)⁽ᵢ⁾ ⊙ W⁽ᵢ⁾;                      (3)

trong đó λ là siêu tham số để cân bằng điểm tầm quan trọng không dữ liệu và dựa trên dữ liệu. Đối với điểm tầm quan trọng không dữ liệu |W⁽ᵗ⁾|, nó không cần bất kỳ tham số thêm nào, điều này hiệu quả về tài nguyên. Do đó, chúng tôi chỉ xem xét việc nén điểm tầm quan trọng dựa trên dữ liệu ∑ᵢ₌₁ᵗ (∂L/∂W)⁽ᵢ⁾ ⊙ W⁽ᵢ⁾ để đạt được huấn luyện thưa thớt hiệu quả tham số.

**Tính hạng thấp.** Như chúng ta biết, rank(W ⊙ ∂L/∂W) ≤ rank(W) · rank(∂L/∂W), có nghĩa là hạng của điểm tầm quan trọng dựa trên dữ liệu phụ thuộc vào hạng của W và ∂L/∂W. Công trình trước đây [Hu et al., 2021] chứng minh rằng gradient của trọng số ∂L/∂W có hạng nội tại thấp, thậm chí có thể là một hoặc hai trong các mô hình ngôn ngữ. Do đó hạng của ma trận điểm tầm quan trọng dựa trên dữ liệu gần với hạng của ma trận trọng số. Các tài liệu hiện có [Oymak et al., 2019; Li et al., 2021] cho thấy rằng trong mạng neural, trọng số lớn W được huấn luyện thường tự nhiên mang cấu trúc trọng số hạng thấp xấp xỉ. Theo đó, chúng tôi có thể suy ra điểm tầm quan trọng dựa trên dữ liệu cũng có hạng nội tại thấp. Do đó, chúng tôi giới thiệu hai ma trận hạng thấp nhỏ A ∈ ℝⁿˣʳ¹ và B ∈ ℝʳ¹ˣᵏ để biểu diễn phần hạng nội tại thấp của điểm tầm quan trọng dựa trên dữ liệu Ŝ, trong đó r₁ là siêu tham số, kiểm soát số lượng tham số có thể huấn luyện cho điểm tầm quan trọng. Để làm cho điểm tầm quan trọng dựa trên dữ liệu của mỗi trọng số giống nhau khi bắt đầu, A và B được khởi tạo với khởi tạo Gaussian và khởi tạo zero tương ứng, và được tối ưu hóa trực tiếp bằng SGD.

**Tính có cấu trúc.** Nói chung, các phương pháp thưa thớt loại bỏ trọng số mà không có bất kỳ ràng buộc nào, có nghĩa là phân phối của kết quả thưa thớt (mặt nạ nhị phân M) không thể kiểm soát. Tuy nhiên, như được thể hiện trong Hình 2, mặt nạ nhị phân M được tạo ra bởi điểm tầm quan trọng S cho thấy mẫu cấu trúc rõ ràng. Chẳng hạn, hình con bên phải trong Hình 2(a) cho thấy có nhiều hàng với cực ít trọng số được giữ lại. Để định lượng hiện tượng như vậy, chúng tôi tính toán tỷ lệ thưa thớt của mỗi cột/hàng trong M nhị phân, sau đó có được biểu đồ của chúng bằng cách chia tỷ lệ thưa thớt thành nhiều khoảng và tính toán phần trăm cột và hàng có tỷ lệ thưa thớt thuộc về các khoảng tương ứng. Hình con bên trái trong Hình 2(a) chứng minh rằng có khoảng 30% hàng trong đó tất cả trọng số được loại bỏ, trong khi hầu hết các cột có tỷ lệ thưa thớt tương tự. Ngược lại, Hình 2(b) cho thấy hầu hết các cột có tỷ lệ thưa thớt rất cao. Do đó, chúng tôi kết luận rằng trọng số của các cột/hàng khác nhau đáng kể về tầm quan trọng. Dựa trên quan sát, chúng tôi đề xuất hai ma trận điểm tầm quan trọng cấu trúc R ∈ ℝⁿˣ¹ và C ∈ ℝ¹ˣᵏ để đo lường tầm quan trọng của mỗi cột/hàng trong trọng số. Việc cập nhật chúng là:

R⁽ᵗ⁾ = ∑ᵢ₌₀ᵗ ∑ⱼ₌₀ᵏ [(∂L/∂W)⁽ᵢ⁾ ⊙ W⁽ᵢ⁾][:,j];
C⁽ᵗ⁾ = ∑ᵢ₌₀ᵗ ∑ⱼ₌₀ⁿ [(∂L/∂W)⁽ᵢ⁾ ⊙ W⁽ᵢ⁾]j,[:];                    (4)

Tóm lại, điểm tầm quan trọng dựa trên dữ liệu trở thành:

Ŝ⁽ᵗ⁾ = α₁A⁽ᵗ⁾B⁽ᵗ⁾ + α₂(R⁽ᵗ⁾ + C⁽ᵗ⁾);                          (5)

trong đó α₁ và α₂ là các siêu tham số để cân bằng điểm tầm quan trọng hạng thấp và cấu trúc, tương ứng.

Để giảm thêm yêu cầu tài nguyên của huấn luyện thưa thớt, chúng tôi theo [Hu et al., 2021] để ràng buộc việc cập nhật trọng số bằng cách biểu diễn nó với phân tách hạng thấp W⁽ᵗ⁾ = W⁽⁰⁾ + U⁽ᵗ⁾V⁽ᵗ⁾, trong đó U ∈ ℝⁿˣʳ², V ∈ ℝʳ²ˣᵏ và r₂ kiểm soát tham số có thể huấn luyện của trọng số. Do đó, điểm tầm quan trọng trong phương pháp của chúng tôi là:

S⁽ᵗ⁾ = |W⁽⁰⁾ + U⁽ᵗ⁾V⁽ᵗ⁾| + α₁A⁽ᵗ⁾B⁽ᵗ⁾ + α₂(R⁽ᵗ⁾ + C⁽ᵗ⁾).          (6)

Dựa trên đó, tính toán của mỗi lớp trở thành:

Y = [(W⁽⁰⁾ + U⁽ᵗ⁾V⁽ᵗ⁾) ⊙ f(|W⁽⁰⁾ + U⁽ᵗ⁾V⁽ᵗ⁾|
    + α₁A⁽ᵗ⁾B⁽ᵗ⁾ + α₂(R⁽ᵗ⁾ + C⁽ᵗ⁾), v)]X.                         (7)

Cần lưu ý rằng, sau khi tinh chỉnh, tất cả trọng số được hoàn thiện và quy trình suy luận sẽ là Y = W̃X, trong đó W̃ là thưa thớt, W̃ = [(W⁽⁰⁾ + U⁽ᵗ⁾V⁽ᵗ⁾) ⊙ f(|W⁽⁰⁾ + U⁽ᵗ⁾V⁽ᵗ⁾| + α₁A⁽ᵗ⁾B⁽ᵗ⁾ + α₂(R⁽ᵗ⁾ + C⁽ᵗ⁾), v)]. Do đó, quy trình suy luận hiệu quả về tham số và tài nguyên.

Quá trình tối ưu hóa của huấn luyện thưa thớt của chúng tôi là:

min_{U,V,A,B,R,C} L((W⁽⁰⁾ + UV) ⊙ f(|W⁽⁰⁾ + UV|
+ α₁AB + α₂(R + C), v); D);
s.t. v/(nk) ≤ 1-p                                                  (8)

Ngoài ra, số lượng tham số có thể huấn luyện trong phương pháp của chúng tôi là (n+k)(r₁+r₂+1), cực kỳ nhỏ hơn so với số lượng ban đầu 2nk khi r₁ và r₂ nhỏ.

## 4 Thí nghiệm

### 4.1 Thiết lập đánh giá

**Bộ dữ liệu và mô hình chính.** Chúng tôi tiến hành thí nghiệm với BERT [Devlin et al., 2019], RoBERTa [Liu et al., 2019] và GPT-2 [Radford et al., 2019] trong các tác vụ downstream khác nhau. Đối với BERT và RoBERTa, chúng tôi sử dụng các benchmark GLUE [Wang et al., 2018] để đánh giá. Đối với GPT-2, chúng tôi đánh giá nó trên E2E, DART và WebNLG.

**Chi tiết triển khai.** Đối với BERT base, chúng tôi đặt batch size = 32 và thực hiện tìm kiếm siêu tham số trên learning rate ∈ {3e-5, 5e-5, 1e-4, 5e-4} và epoch ∈ {20, 40} trên QNLI, SST-2, CoLA, STS-B, MRPC, RTE và epoch ∈ {10, 20} trên MNLI, QQP. Hơn nữa, chúng tôi sử dụng batch size là 16 cho RoBERTa, cũng như tìm kiếm siêu tham số trên learning rate ∈ {1e-5, 2e-5, 3e-5, 5e-5}. Không gian tìm kiếm epoch giống như BERT base. Đối với GPT-2, chúng tôi huấn luyện mô hình trong 5 epoch sử dụng batch size là 8 và learning rate ban đầu là 1e-4. Khi huấn luyện, chúng tôi sử dụng tối ưu hóa AdamW và bộ lập lịch learning rate tuyến tính. Tất cả mô hình được khởi tạo với trọng số pre-trained. Chúng tôi theo [Zhu and Gupta, 2018] để sử dụng lịch trình độ thưa thớt khối. Chúng tôi cũng thêm một vài bước khởi động ở đầu huấn luyện (10% bước huấn luyện) và làm mát ở cuối huấn luyện (30% bước huấn luyện), điều này thực nghiệm cải thiện hiệu suất đặc biệt trong các chế độ độ thưa thớt cao. Đối với PST, chúng tôi đặt λ = α₁ = α₂ = 1 và r₁ = r₂ = 8.

### 4.2 Kết quả

**BERT và RoBERTa.** Bảng 2 cho thấy phương pháp của chúng tôi đạt được sự giảm lớn nhất về tham số có thể huấn luyện với hiệu suất ngang bằng hoặc tốt hơn so với các phương pháp trước đây. Chúng tôi khởi tạo điểm tầm quan trọng bằng giá trị tuyệt đối của trọng số pre-trained cho cắt tỉa chuyển động để tránh có được hiệu suất tệ. Chẳng hạn, chúng tôi đạt được cải thiện điểm trung bình 0,73 với tiết kiệm 98,9% tham số có thể huấn luyện trên RoBERTa large khi tỷ lệ độ thưa thớt là 90%. Hơn nữa, chúng tôi quan sát rằng MaP vượt trội hơn các phương pháp khác với ít hoặc không mất mát so với mô hình dày đặc tinh chỉnh ở tỷ lệ độ thưa thớt thấp (50%). Tuy nhiên, khi tăng tỷ lệ độ thưa thớt lên 90%, nó có được sự giảm hiệu suất rõ ràng dù trong BERT hay RoBERTa. Ngược lại, phương pháp của chúng tôi PST hoạt động kém với tỷ lệ độ thưa thớt thấp nhưng có được hiệu suất tốt hơn các phương pháp khác ở tỷ lệ độ thưa thớt cao hơn, điều này cũng được thể hiện trong Hình 3. Trong khi đó, mặc dù RoBERTa đạt được hiệu suất tốt hơn BERT sau khi tinh chỉnh, mô hình sau huấn luyện thưa thớt hoạt động tệ hơn BERT. Đối với trường hợp này, chúng tôi thấy rằng RoBERTa có learning rate mặc định nhỏ hơn BERT trên các tác vụ downstream, điều này cho thấy RoBERTa phụ thuộc vào trọng số pre-trained nhiều hơn BERT. Các phương pháp thưa thớt làm cho một số trọng số trở thành zero. Những thay đổi trọng số này trong RoBERTa có thể có tác động lớn hơn đến các tác vụ downstream. Chúng tôi phải lưu ý rằng đây không phải là hiện tượng phổ biến, các mô hình lớn hơn thường ổn định hơn các mô hình nhỏ hơn trong lĩnh vực nén mô hình [Li et al., 2020].

**GPT-2.** Chúng tôi tiếp tục xác minh rằng phương pháp của chúng tôi cũng có thể thắng trên mô hình NLG. Như được thể hiện trong Bảng 3, PST của chúng tôi đạt được hiệu suất tốt nhất trong khi huấn luyện số lượng tham số cực kỳ nhỏ hơn trong ba tác vụ downstream. Đặc biệt, so với MvP, chúng tôi có được cải thiện 6,25 BLEU trong khi tiết kiệm 98,8% tham số có thể huấn luyện trên WebNLG.

### 4.3 Nghiên cứu phân tích

**Điểm tầm quan trọng.** Thiết kế điểm tầm quan trọng đóng vai trò quan trọng trong PST được đề xuất của chúng tôi. Chúng tôi kết hợp điểm tầm quan trọng không dữ liệu và dựa trên dữ liệu, và phân tách điểm tầm quan trọng dựa trên dữ liệu thành hai tập hợp ma trận nhỏ dựa trên tính hạng thấp và tính có cấu trúc của nó. Chính xác, chúng tôi so sánh bảy điểm tầm quan trọng khác nhau trên BERT base trong Bảng 5. Chúng tôi điều chỉnh r₁ và r₂ để làm cho tất cả các phương pháp có cùng số lượng tham số có thể huấn luyện. Kết quả cho thấy điểm tầm quan trọng được đề xuất đạt được hiệu suất tốt nhất trong các tác vụ downstream khác nhau. Hơn nữa, tính có cấu trúc quan trọng hơn tính hạng thấp cho điểm tầm quan trọng so với dòng 2 và 3.

**Hạng r₁ và r₂.** Bảng 4 cho thấy ảnh hưởng của hạng r₁ và r₂. Chúng tôi quan sát rằng mặc dù hiệu suất mô hình tăng khi hạng tăng, cao hơn không nhất thiết tốt hơn. Khi một hạng thấp hơn (tức là r₁ = 4 hoặc r₂ = 4), việc tăng hạng khác sẽ cải thiện độ chính xác của mô hình. Nhưng khi một hạng đủ lớn (tức là r₁ = 16 hoặc r₂ = 16), việc tăng hạng khác không nhất thiết cải thiện hiệu suất mô hình. Điều này gợi ý rằng hạng r₁ và r₂ cũng có thể được tìm kiếm để khám phá cấu hình phù hợp nhất cho các tác vụ downstream khác nhau.

### 4.4 Phân tích

**Phân phối trọng số thưa thớt.** Hình 4(a) cho thấy tổng quan về phân phối của các trọng số còn lại của MaP, MvP và PST tương ứng tại cùng một lớp với tỷ lệ độ thưa thớt 90%. So với MaP có xu hướng loại bỏ trọng số gần zero và MvP loại bỏ trọng số với giá trị lớn hơn, PST có phân phối mượt mà hơn, giữ trọng số cả với giá trị lớn hơn và nhỏ hơn. Hình 4(b)(c)(d) hiển thị trọng số so với điểm tầm quan trọng của MaP, MvP và PST, tương ứng. Các trọng số được cắt tỉa và còn lại lần lượt là chấm xám và xanh. Chúng tôi quan sát rằng PST phản ánh đặc tính của cả phương pháp không dữ liệu (MaP) và dựa trên dữ liệu (MvP). MaP tính toán điểm tầm quan trọng của trọng số dựa trên giá trị tuyệt đối của chúng và do đó cho thấy đường cong hình chữ V. MvP loại bỏ bất kỳ trọng số nào bất kể giá trị tuyệt đối của chúng (trừ zero). Tuy nhiên, PST không chỉ xem xét giá trị tuyệt đối của trọng số mà còn giữ lại trọng số với giá trị tuyệt đối thấp, và do đó cho thấy sự kết hợp của hai phân phối của chúng.

**Tính tương tự của mặt nạ nhị phân.** Chúng tôi sử dụng khoảng cách Hamming để tính toán tính tương tự của mặt nạ nhị phân M giữa các phương pháp khác nhau. Hình 5 cho thấy mặt nạ nhị phân thưa thớt M của PST gần với MaP hơn MvP, có nghĩa là điểm tầm quan trọng không dữ liệu chiếm tỷ lệ lớn hơn trong PST. Hơn nữa, như được thể hiện trong Hình 5(c) và Hình 5(d), tính tương tự giữa MaP và PST giảm khi độ sâu của các lớp trong mô-đun FFN tăng. Điều này chứng minh rằng PST dần dần giảm tác động của điểm tầm quan trọng không dữ liệu với việc sâu của lớp. Tuy nhiên, với sự gia tăng độ sâu của các lớp, tính tương tự giữa MvP và PST tăng trong lớp đầu vào của mô-đun FFN và giảm trong lớp đầu ra của mô-đun FFN. Điều này cho thấy rằng điểm tầm quan trọng của PST khám phá thông tin mới khác với MaP và MvP trong lớp đầu ra.

## 5 Kết luận

Trong bài báo này, chúng tôi đề xuất phương pháp huấn luyện thưa thớt hiệu quả tham số (PST) để giảm số lượng tham số có thể huấn luyện và yêu cầu tài nguyên trong quá trình tinh chỉnh nhận biết độ thưa thớt của các mô hình ngôn ngữ lớn. Chúng tôi đầu tiên kết hợp các tiêu chí không dữ liệu và dựa trên dữ liệu để tính toán tầm quan trọng của trọng số. Sau đó chúng tôi khám phá hai đặc tính (tức là tính hạng thấp và tính có cấu trúc) của điểm tầm quan trọng dựa trên dữ liệu, và do đó giới thiệu hai tập hợp ma trận hiệu quả tham số để thay thế ma trận điểm tầm quan trọng lớn ban đầu. Các thí nghiệm mở rộng trên các mô hình ngôn ngữ khác nhau chứng minh tính hiệu quả của PST trong việc giảm độ phức tạp tính toán và yêu cầu tài nguyên trong tinh chỉnh thưa thớt.
