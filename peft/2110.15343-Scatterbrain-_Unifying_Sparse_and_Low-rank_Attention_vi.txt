# 2110.15343.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2110.15343.pdf
# Kích thước tệp: 3277983 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Scatterbrain: Thống nhất Xấp xỉ Attention Thưa và Hạng thấp

Beidi Chen∗y, Tri Dao∗y, Eric Winsory, Zhao Songx, Atri Rudraz, và Christopher Réy
yKhoa Khoa học Máy tính, Đại học Stanford
xAdobe Research
zKhoa Khoa học Máy tính và Kỹ thuật, Đại học Buffalo, SUNY
{beidic,trid,winsor}@stanford.edu ,zsong@adobe.com ,atri@buffalo.edu ,
chrismre@cs.stanford.edu
29 tháng 10, 2021

Tóm tắt
Những tiến bộ gần đây trong các Transformer hiệu quả đã khai thác tính thưa hoặc tính chất hạng thấp của ma trận attention để giảm thiểu các nút thắt tính toán và bộ nhớ khi mô hình hóa các chuỗi dài. Tuy nhiên, việc cân bằng sự đánh đổi giữa chất lượng mô hình và hiệu quả để thực hiện xấp xỉ một-kích-thước-phù-hợp-tất-cả cho các tác vụ khác nhau vẫn là một thách thức. Để hiểu rõ hơn sự đánh đổi này, chúng tôi quan sát thấy rằng các xấp xỉ thưa và hạng thấp vượt trội ở các chế độ khác nhau, được xác định bởi nhiệt độ softmax trong attention, và thưa + hạng thấp có thể vượt trội hơn từng loại riêng lẻ. Được truyền cảm hứng từ thuật toán robust-PCA cổ điển cho phân tách thưa và hạng thấp, chúng tôi đề xuất Scatterbrain, một cách mới để thống nhất attention thưa (thông qua locality sensitive hashing) và hạng thấp (thông qua kernel feature map) để xấp xỉ chính xác và hiệu quả. Ước lượng này không thiên lệch với sai số thấp có thể chứng minh được. Chúng tôi chứng minh thực nghiệm rằng Scatterbrain có thể đạt được sai số thấp hơn 2:1 so với các baseline khi dùng như một thay thế trực tiếp trong sinh ảnh BigGAN và T2T-ViT đã được đào tạo trước. Trên một vision transformer T2T đã được đào tạo trước, ngay cả khi không tinh chỉnh, Scatterbrain có thể giảm 98% bộ nhớ attention với chi phí chỉ 1% giảm độ chính xác. Chúng tôi chứng minh Scatterbrain cho đào tạo đầu-cuối với perplexity tốt hơn tới 4 điểm và độ chính xác trung bình tốt hơn 5 điểm so với các transformer hiệu quả thưa hoặc hạng thấp trên các tác vụ mô hình hóa ngôn ngữ và long-range-arena.

1 Giới thiệu
Các mô hình Transformer [63] đã được áp dụng trong nhiều ứng dụng đa dạng, bao gồm xử lý ngôn ngữ tự nhiên [7,26,50], xử lý hình ảnh [10,47], và nhận dạng giọng nói [42]. Đào tạo các Transformer lớn đòi hỏi tài nguyên tính toán và bộ nhớ lớn, đặc biệt khi mô hình hóa các chuỗi dài, chủ yếu do độ phức tạp bậc hai (theo chiều dài chuỗi) trong các lớp attention. Những tiến bộ gần đây trong các transformer hiệu quả [17,22,35,36,65] tận dụng xấp xỉ attention để vượt qua nút thắt bằng cách xấp xỉ các ma trận attention. Tuy nhiên, việc tìm một phương pháp xấp xỉ mạnh mẽ cân bằng sự đánh đổi hiệu quả-độ chính xác trên nhiều tác vụ khác nhau là một thách thức [57, 58].

Chúng tôi phân loại hầu hết các phương pháp hiện có để tính toán ma trận attention hiệu quả thành hai nhóm chính: khai thác tính thưa, ví dụ Reformer [36], SMYRF [22], hoặc tính chất hạng thấp của các ma trận attention, ví dụ Linformer [65], Linear Transformer [35], và Performer [17]. Tuy nhiên, các kỹ thuật này thường có điểm mạnh khác nhau và tập trung vào hiệu suất của các tác vụ cụ thể, vì vậy các xấp xỉ của chúng vẫn gây ra suy giảm độ chính xác trên nhiều tác vụ khác. Ví dụ, theo một bài báo benchmark gần đây [57] và các thí nghiệm của chúng tôi, attention dựa trên hạng thấp có thể kém hiệu quả hơn trên dữ liệu có cấu trúc phân cấp hoặc các tác vụ mô hình hóa ngôn ngữ, trong khi các biến thể dựa trên thưa không hoạt động tốt trên các tác vụ phân loại.

∗Đóng góp bằng nhau. Thứ tự được xác định bằng tung đồng xu.
1arXiv:2110.15343v1 [cs.LG] 28 Oct 2021

--- TRANG 2 ---
85%95%
Xây dựng Kernel & Hash
  , ,PHÂN LOẠI Đầu vào THƯA
HẠNG THẤPSCATTERBRAIN

Hình 1: Trái: các chế độ mà xấp xỉ thưa+hạng thấp chính xác hơn, dựa trên entropy của các ma trận attention. Phải: Quy trình làm việc của Scatterbrain. Đối với lớp attention trong Transformers, sau khi tính toán các ma trận Query Q, Key K, và Value V, chúng tôi xấp xỉ softmax (QK>)V với hai thành phần: (i) thưa SV (ii) hạng thấp (Q)((K)>V).

Chúng tôi quan sát thấy rằng các xấp xỉ thưa và hạng thấp bổ sung cho nhau đối với nhiều ma trận attention trong thực tế, và thưa+hạng thấp có thể vượt trội hơn từng loại riêng lẻ (Hình 1 trái). Chúng tôi phân loại thực nghiệm các chế độ mà xấp xỉ thưa hoặc hạng thấp đạt được sai số tốt hơn dựa trên nhiệt độ softmax của attention (trong đó entropy của phân phối softmax có thể được sử dụng như một đại diện). Chúng tôi mong đợi rằng các phương pháp thưa hoạt động tốt nếu attention phụ thuộc vào một vài entries (entropy softmax thấp). Ngược lại, các phương pháp hạng thấp hoạt động tốt hơn nếu attention phụ thuộc vào hỗn hợp của nhiều thành phần (entropy softmax cao). Điều này giải thích hiện tượng rằng các phương pháp dựa trên thưa và hạng thấp hiện tại vượt trội trên các loại tác vụ khác nhau. Một câu hỏi tự nhiên là liệu có thể hiểu và thống nhất điểm mạnh của cả hai phương pháp không. Mặc dù việc tìm tổ hợp tối ưu của các xấp xỉ thưa và hạng thấp là NP-hard, Robust PCA [9] là một giải pháp thời gian đa thức với sai số xấp xỉ chặt chẽ. Chúng tôi quan sát thấy rằng Robust PCA đạt được sai số xấp xỉ thấp hơn so với chỉ thưa hoặc hạng thấp trên các ma trận attention. Sự khác biệt rõ ràng nhất ở entropy "tầm trung", nơi chúng tôi quan sát thấy rằng có thể giảm sai số lên tới 95%.

Mối liên hệ giữa Robust PCA và ước lượng ma trận attention cung cấp cơ hội để thực hiện xấp xỉ mạnh mẽ hơn. Cụ thể, với một ma trận attention, có thể thực hiện xấp xỉ thưa+hạng thấp một cách thích ứng để đạt được sai số thấp. Tuy nhiên, nó đi kèm với ba thách thức: (i) Làm thế nào để phân tách các ma trận attention thành các thành phần thưa và hạng thấp và ước lượng chúng một cách hiệu quả và chính xác; Robust PCA chính xác nhưng chậm và yêu cầu tạo ra toàn bộ attention, trong khi việc cộng đơn giản attention thưa và hạng thấp sẽ không chính xác do tính toán kép. (ii) Không rõ liệu có bảo đảm lý thuyết rằng xấp xỉ thưa + hạng thấp nghiêm ngặt tốt hơn thưa hoặc hạng thấp trong một số chế độ, mặc dù chúng tôi quan sát thấy sự tách biệt thực nghiệm. (iii) Sai số xấp xỉ thấp hơn chuyển đổi như thế nào thành hiệu suất đầu-cuối trong các tác vụ thực tế.

Trong bài báo này, chúng tôi đề xuất Scatterbrain, một ước lượng mạnh mẽ chính xác và hiệu quả của các ma trận attention với các bảo đảm lý thuyết để giải quyết các thách thức trên. Cụ thể:

•Trong Phần 3, chúng tôi quan sát thấy rằng xấp xỉ thưa và hạng thấp bổ sung cho nhau và chứng minh rằng cấu trúc thưa + hạng thấp xuất hiện tự nhiên khi các phần tử trong chuỗi đầu vào tạo thành cụm. Chúng tôi đặc trưng hóa và phân tích lý thuyết các chế độ mà thưa, hạng thấp, và thưa+hạng thấp vượt trội, được quyết định bởi nhiệt độ softmax của attention.

•Trong Phần 4, được truyền cảm hứng từ thuật toán Robust PCA cổ điển, chúng tôi đề xuất Scatterbrain, kết hợp hiệu quả các ma trận thưa và hạng thấp để xấp xỉ attention. Đặc biệt, chúng tôi sử dụng Locality Sensitive Hashing (LSH) để xác định các entries lớn của ma trận attention (sau softmax) mà không cần tạo ra toàn bộ ma trận và sau đó tận dụng xấp xỉ kernel để tham số hóa phần hạng thấp. Chúng tôi chứng minh rằng phương pháp của chúng tôi có sai số xấp xỉ nghiêm ngặt thấp hơn so với baseline hạng thấp.

•Trong Phần 5, chúng tôi xác thực thực nghiệm lý thuyết và phương pháp đề xuất của chúng tôi, cho thấy rằng Scatterbrain xấp xỉ ma trận attention một cách chính xác, hiệu quả bộ nhớ cho các chuỗi dài, và hoạt động tốt trên các tác vụ khác nhau. Đầu tiên, chúng tôi cho thấy rằng độ chính xác xấp xỉ của nó gần với oracle Robust PCA của chúng tôi và đạt được sai số thấp hơn 2,1 lần so với các baseline hiệu quả khác trên các benchmark thực tế. Điều này dẫn đến ứng dụng trực tiếp của Scatterbrain như một thay thế drop-in cho full attention đã được đào tạo trước, do đó giảm tới 98% bộ nhớ cần thiết cho tính toán attention trong T2T-ViT và BigGAN đã được đào tạo trước trong khi duy trì chất lượng tương tự. Cuối cùng, chúng tôi cho thấy rằng độ chính xác và hiệu quả vượt trội của nó có thể cải thiện sự đánh đổi hiệu quả-độ chính xác của đào tạo Transformer đầu-cuối. Trên tác vụ mô hình hóa ngôn ngữ WikiText-103, Scatterbrain đạt được perplexity tốt hơn tới 1 điểm so với Reformer và Performer. Trên 5 tác vụ long-range benchmark, Scatterbrain cải thiện độ chính xác trung bình tới 5 điểm.¹

2 Thiết lập Bài toán và Công trình Liên quan
Đầu tiên chúng tôi định nghĩa bài toán xấp xỉ mà chúng tôi nhằm giải quyết trong bài báo này. Sau đó chúng tôi thảo luận về các ứng dụng của kỹ thuật thưa và hạng thấp trong các Transformer hiệu quả và giới thiệu thuật toán robust PCA.

Công thức Bài toán: Trong bài toán xấp xỉ ma trận attention, chúng ta được cho ba ma trận, query, key, và value, Q;K;V∈Rn×d để tính toán softmax (QK>)V. Chúng ta tìm cách giảm độ phức tạp bậc hai của softmax (QK>) (được áp dụng theo hàng) với sai số xấp xỉ thấp. Chính xác hơn, đối với một quy trình xấp xỉ f, chúng ta tối thiểu hóa hai mục tiêu, sai số xấp xỉ E ≜ ∥f(Q;K) - softmax(QK>)∥²F, và chi phí tính toán/bộ nhớ C(f()).

Xấp xỉ Thưa, Hạng thấp cho Ma trận Attention: Công trình gần đây khai thác các mẫu thưa hoặc tìm ánh xạ hạng thấp của các ma trận attention ban đầu để vượt qua các nút thắt tính toán và bộ nhớ trong Transformers [17,22,35,36,53,65]. Nói chung, chúng ta có thể chia hầu hết các kỹ thuật thành hai loại – xấp xỉ thưa và hạng thấp. Reformer [36] là một biến thể thưa tiêu biểu sử dụng LSH [3] để truy xuất hoặc phát hiện các vị trí của ma trận attention có giá trị lớn và giảm tính toán từ O(n²) xuống O(n log n). Performer [17] là một ví dụ về biến thể hạng thấp, sử dụng kernelization để tránh tính toán O(n²d) rõ ràng. Một vấn đề của xấp xỉ thưa hoặc hạng thấp là cấu trúc của các ma trận attention thay đổi trong thực tế, và việc thực hiện xấp xỉ mạnh mẽ trên một phạm vi rộng các ma trận attention là thách thức. Ví dụ, Wang et al. [65] quan sát thấy rằng attention có xu hướng có nhiều cấu trúc hạng thấp hơn ở các lớp thấp hơn và Ramsauer et al. [51] cho thấy rằng chúng thưa hơn ở giai đoạn cuối của đào tạo. Lý tưởng nhất, chúng ta muốn thống nhất điểm mạnh của cả hai kỹ thuật, nhưng việc tìm tổ hợp tốt nhất của xấp xỉ thưa và hạng thấp là NP-hard.

Thưa + Hạng thấp và Robust PCA: May mắn thay, Robust PCA cổ điển [9] trình bày một thuật toán đa thức để tìm các tổ hợp gần tối ưu hoặc tốt của xấp xỉ thưa và hạng thấp của các ma trận. Cấu trúc ma trận thưa + hạng thấp đã được nghiên cứu kỹ lưỡng trong thống kê và xử lý tín hiệu từ cuối những năm 2000 [9]. Cấu trúc này tự nhiên tổng quát hóa các ma trận hạng thấp [33,62] và thưa [60]. Scatterbrain được xây dựng trên một dòng công trình, ví dụ Bigbird [70], Longformer [5] với chủ đề kết hợp nhiều loại attention. Tuy nhiên, mặc dù có nhiều bài báo, xấp xỉ ma trận thưa + hạng thấp này chưa được nghiên cứu nghiêm ngặt trong bối cảnh của các ma trận attention. Chúng tôi thực hiện nghiên cứu này và cho thấy làm thế nào chúng ta có thể nới lỏng xấp xỉ thưa + hạng thấp từ robust PCA, làm cho nó hiệu quả trong khi vẫn giữ lại độ chính xác của PCA. Trên thực tế, kết quả của chúng tôi làm sáng tỏ thêm lý do tại sao Bigbird hoặc Longformer hoạt động, vì chúng là các trường hợp đặc biệt của một cấu trúc có nguyên tắc duy nhất. Một cuộc thảo luận mở rộng về công trình liên quan có trong Phụ lục A.

3 Đặc trưng hóa Xấp xỉ Thưa + Hạng thấp cho Ma trận Attention
Chúng tôi thúc đẩy việc sử dụng xấp xỉ thưa + hạng thấp của các ma trận attention với quan sát chính rằng đối với nhiều ma trận attention, xấp xỉ thưa và hạng thấp bổ sung cho nhau, và tổ hợp lý tưởng của chúng (thông qua Robust PCA) có thể vượt trội hơn cả hai (Phần 3.1). Hơn nữa, chúng tôi lập luận rằng cấu trúc thưa + hạng thấp có thể xuất hiện tự nhiên khi các phần tử trong chuỗi đầu vào tạo thành cụm, như được quyết định bởi nhiệt độ softmax (Phần 3.2).

--- TRANG 3 ---
3.1 Quan sát Thúc đẩy: Cấu trúc Hạng thấp và Thưa của Ma trận Attention
Chúng tôi đặc trưng hóa thực nghiệm các chế độ mà xấp xỉ thưa và hạng thấp phù hợp, dựa trên nhiệt độ softmax (mà chúng tôi sử dụng entropy phân phối softmax là một đại diện). Cụ thể, trong Hình 1 (trái), chúng tôi trình bày sai số xấp xỉ của các ma trận attention gốc và xấp xỉ (thưa hoặc hạng thấp) của các ma trận được lấy mẫu từ một Transformer 4 lớp được đào tạo trên phân loại đánh giá IMDb [57]. Chúng tôi có hai quan sát:

1. Xấp xỉ thưa và hạng thấp bổ sung cho nhau: thưa vượt trội khi tỷ lệ nhiệt độ softmax thấp (tức là, entropy thấp), và hạng thấp vượt trội khi nhiệt độ softmax cao (tức là, entropy cao).
2. Một tổ hợp lý tưởng của thưa và hạng thấp (đường màu cam trong Hình 1 trái), thu được bằng robust PCA, có thể đạt được sai số thấp hơn cả hai.

Các quan sát tương tự trên các benchmark khác và chi tiết được trình bày trong Phụ lục B.

3.2 Một Mô hình Sinh về Cách Cấu trúc Thưa + Hạng thấp Có thể Xuất hiện

Hình 2: Trực quan hóa quá trình sinh, cho ba giá trị khác nhau của khoảng cách nội cụm σ (nhỏ, trung bình, và lớn). Các vector từ chuỗi đầu vào (hàng của Q) tạo thành cụm nằm xấp xỉ trên hình cầu đơn vị. Các màu khác nhau đại diện cho các cụm khác nhau.

Tham số hóa thưa + hạng thấp có tính biểu đạt hơn so với chỉ thưa hoặc hạng thấp riêng lẻ. Thật vậy, trong Phụ lục, chúng tôi xây dựng một họ ma trận attention để cho thấy sự tách biệt giữa khả năng xấp xỉ của thưa + hạng thấp so với chỉ thưa hoặc hạng thấp: đối với một ma trận attention n×n, chỉ thưa hoặc hạng thấp cần O(n²) tham số để có sai số xấp xỉ ε trong chuẩn Frobenius, trong khi thưa + hạng thấp chỉ cần O(n) tham số.

Hơn nữa, chúng tôi lập luận ở đây rằng thưa + hạng thấp là một ứng cử viên tự nhiên để xấp xỉ các ma trận attention chung. Chúng tôi mô tả một mô hình sinh về cách cấu trúc thưa + hạng thấp trong ma trận attention có thể xuất hiện khi các phần tử của chuỗi đầu vào tạo thành cụm. Theo quá trình này, chúng tôi đặc trưng hóa cách nhiệt độ softmax quyết định khi nào chúng ta cần ma trận thưa, hạng thấp, hoặc thưa + hạng thấp để xấp xỉ ma trận attention. Kết quả này củng cố quan sát trong Phần 3.1.

Quá trình sinh của các phần tử phân cụm trong chuỗi đầu vào Chúng tôi mô tả ở đây một mô hình sinh của chuỗi đầu vào cho attention, được tham số hóa bởi nhiệt độ nghịch đảo β ∈ R⁺ và khoảng cách nội cụm σ ∈ R⁺.

Quá trình 1. Cho Q ∈ R^(n×d), với d ≫ (log^(3/2)(n)), mỗi hàng của Q được sinh ngẫu nhiên như sau:
1. Với C = Θ(√n), lấy mẫu C số tâm cụm c₁,...,cᶜ ∈ R^d độc lập từ N(0,I_d/√d).
2. Với mỗi cụm xung quanh c_i, lấy mẫu n_i = O(1) số phần tử xung quanh c_i, có dạng z_ij = c_i + r_ij với j = 1,...,n_i trong đó r_ij ∼ N(0,σ²I_d/√d). Giả sử tổng số phần tử là n = n₁ + ... + nᶜ và σ = O(1/log^(1/4)n).

Cho Q là ma trận có các hàng là vector z_ij với i = 1,...,C và j = 1,...,n_i. Cho A = QQ^T và để ma trận attention là M = exp(βA).

Chúng tôi trực quan hóa quá trình sinh này trong Hình 2.

Nhiệt độ softmax và sai số xấp xỉ Chúng tôi đặc trưng hóa khi nào sử dụng thưa, hạng thấp, hoặc thưa + hạng thấp để xấp xỉ các ma trận attention trong Quá trình 1, tùy thuộc vào nhiệt độ nghịch đảo β. Trực giác ở đây là nhiệt độ nghịch đảo tương ứng với cường độ tương tác giữa các cụm. Nếu β lớn, tương tác nội cụm chiếm ưu thế ma trận attention, phân phối softmax nhọn, và vậy chúng ta chỉ cần ma trận thưa để xấp xỉ attention. Nếu β nhỏ, thì attention liên cụm tương tự như attention nội cụm, phân phối softmax khuếch tán, và chúng ta có thể xấp xỉ nó bằng ma trận hạng thấp. Trong chế độ trung gian của β, chúng ta cần phần thưa để bao phủ attention nội cụm và phần hạng thấp để xấp xỉ attention liên cụm.

Chúng tôi chính thức hóa trực giác này trong Định lý 1 (trong các bounds dưới đây chúng ta nghĩ σ là một hằng số). Tất cả các chứng minh có trong Phụ lục D.

Định lý 1. Cho M_β là ma trận attention trong Quá trình 1. Cố định ε ∈ (0,1). Cho R ∈ R^(n×n) là một ma trận. Xem xét xấp xỉ hạng thấp, thưa, và thưa + hạng thấp cho M.

1. Nhiệt độ cao: Giả sử β = o(log n/log d).
   (a) Hạng thấp: Tồn tại R với rank n^(o(1)) (và do đó n^(1+o(1)) tham số) sao cho ||M - R||_F ≤ εn.
   (b) Thưa: Nếu R có độ thưa o(n²), thì ||M - R||_F = Ω(n).

2. Nhiệt độ trung: Giả sử Θ((1-ε)/2) log n ≤ β ≤ O(ε log n).
   (a) Thưa + hạng thấp: Tồn tại R thưa + hạng thấp với n^(1+o(1)) tham số với ||M - R||_F ≤ εn.
   (b) Hạng thấp: Nếu R sao cho n · rank(R) = Ω(n), thì ||M - R||_F = Ω(n).
   (c) Thưa: Nếu R có độ thưa o(n²), thì ||M - R||_F = Ω(n).

3. Nhiệt độ thấp: Giả sử β = Ω(log n).
   (a) Hạng thấp: Nếu R sao cho n · rank(R) = Ω(n), thì ||M - R||_F = Ω(e^(-β(1-ε)/2)).
   (b) Thưa: Tồn tại R với độ thưa O(n) sao cho ||M - R||_F ≤ εe^(-β(1-ε)/2).

4 Scatterbrain: Thống nhất Attention Thưa và Hạng thấp

Hình 3: So sánh định tính về độ chính xác và hiệu quả xấp xỉ, giữa Robust PCA, attention thưa (Reformer) và hạng thấp (Performer), và Scatterbrain. Scatterbrain chính xác hơn trong khi vẫn hiệu quả.

Chúng tôi trình bày Scatterbrain, và cho thấy rằng nó xấp xỉ attention một cách chính xác và hiệu quả. Phần 4.1 mô tả các thách thức của việc thiết kế một xấp xỉ chính xác và hiệu quả, và cách các baseline rõ ràng như Robust PCA hoặc tổ hợp đơn giản của attention thưa và hạng thấp thất bại trong việc đáp ứng cả hai tiêu chí. Phần 4.2 chứng minh cách Scatterbrain giải quyết các thách thức (Hình 1 chứa một sơ đồ của Scatterbrain). Trong Phần 4.3, chúng tôi cho thấy rằng Scatterbrain không thiên lệch với phương sai thấp hơn có thể chứng minh được so với các baseline hạng thấp như Performer.

Hình 3 cho thấy so sánh định tính giữa các phương pháp khác nhau để xấp xỉ ma trận attention: Robust PCA chính xác nhưng chậm, attention thưa (ví dụ, Reformer) và hạng thấp (ví dụ, Performer) nhanh và hiệu quả bộ nhớ nhưng có thể không chính xác lắm, trong khi Scatterbrain chính xác hơn so với các đối tác thưa và hạng thấp trong khi vẫn hiệu quả như nhau.

Thông tin chi tiết hơn về việc triển khai hiệu quả của Scatterbrain có trong Phụ lục C.

4.1 Thách thức của Việc Thiết kế Xấp xỉ Thưa + Hạng thấp Chính xác và Hiệu quả
Chúng tôi tìm kiếm một xấp xỉ thưa + hạng thấp của ma trận attention² A vừa chính xác vừa hiệu quả. Baseline lý thuyết tự nhiên của Robust PCA quá chậm và yêu cầu quá nhiều bộ nhớ, trong khi cách đơn giản nhất để kết hợp attention thưa và hạng thấp thất bại do tính toán kép trên support của attention thưa.

²Để đơn giản hóa thảo luận, chúng tôi xem xét ma trận attention chưa chuẩn hóa A = exp(QK^T), bỏ qua tỷ lệ thông thường của 1/√d và hằng số chuẩn hóa softmax.

--- TRANG 4 ---
1. Nếu mục tiêu là độ chính xác, Robust PCA là thuật toán được nghiên cứu nhiều nhất để tìm xấp xỉ thưa + hạng thấp cho một ma trận cho trước. Nó nới lỏng bài toán NP-hard của việc tìm xấp xỉ thưa + hạng thấp tốt nhất thành một bài toán tối ưu hóa lồi, với các ràng buộc nuclear norm và ℓ₁. Mặc dù có thể được giải quyết trong thời gian đa thức, nó chậm hơn nhiều bậc để được sử dụng trong mỗi vòng lặp của một vòng lặp đào tạo. Hơn nữa, nó yêu cầu tạo ra ma trận attention, điều này đi ngược lại mục đích chính của việc giảm yêu cầu tính toán và bộ nhớ.

2. Mặt khác, một cách hiệu quả để có được xấp xỉ thưa + hạng thấp của ma trận attention là đơn giản cộng các entries của xấp xỉ thưa S (ví dụ, từ Reformer) và xấp xỉ hạng thấp Q̃K̃ᵀ với Q̃, K̃ ∈ Rⁿˣᵐ (ví dụ, từ Performer). Ma trận thưa S thường có support được xác định ngẫu nhiên [16], bởi LSH [22,36], hoặc bởi clustering [53]. Trên support của S, có khả năng bao gồm các vị trí của các entries lớn của ma trận attention A, các entries của S khớp với những của A. Có thể nhân (S + Q̃K̃ᵀ)V = SV + Q̃(K̃ᵀV) hiệu quả vì S thưa, và nhóm Q̃(K̃ᵀV) giảm độ phức tạp nhân ma trận khi m ≪ n, từ O(n²m) xuống O(nmd). Xấp xỉ S + Q̃K̃ᵀ khớp Q̃K̃ᵀ bên ngoài supp(S), do đó nó có thể chính xác ở đó nếu Q̃K̃ᵀ chính xác. Tuy nhiên, S + Q̃K̃ᵀ sẽ không chính xác trên support của S do các đóng góp từ cả S và từ Q̃K̃ᵀ. Việc điều chỉnh Q̃K̃ᵀ để bù trừ đóng góp từ S là khó khăn, đặc biệt nếu chúng ta muốn tránh tạo ra Q̃K̃ᵀ để có hiệu quả.

4.2 Scatterbrain: Trực giác và Mô tả Thuật toán
Insight đơn giản đằng sau phương pháp của chúng tôi là trên support của ma trận thưa S, thay vì cố gắng khớp các entries của ma trận attention A, chúng ta có thể đặt các entries của S để bù trừ đóng góp từ phần hạng thấp Q̃K̃ᵀ. Bằng cách này, xấp xỉ S + Q̃K̃ᵀ sẽ khớp A chính xác trên support của S, và sẽ khớp Q̃K̃ᵀ bên ngoài supp(S), có nghĩa là nó vẫn sẽ chính xác ở đó nếu Q̃K̃ᵀ chính xác. Chúng ta không cần tạo ra toàn bộ ma trận Q̃K̃ᵀ vì chỉ cần một tập con các entries của nó, do đó xấp xỉ của chúng ta sẽ hiệu quả về tính toán và bộ nhớ.

Scatterbrain do đó tiến hành theo ba bước: chúng tôi xây dựng xấp xỉ hạng thấp Q̃K̃ᵀ ≈ A, và xây dựng ma trận thưa S sao cho S + Q̃K̃ᵀ khớp A trên support của S, sau đó cuối cùng nhân SV và Q̃(K̃ᵀV) và kết hợp kết quả. Cụ thể hơn:

1. Xấp xỉ Hạng thấp. Chúng tôi định nghĩa quy trình LowRank trả về hai ma trận Q̃, K̃ ∈ Rⁿˣᵐ sao cho Q̃K̃ᵀ xấp xỉ A. Cụ thể, chúng tôi sử dụng ánh xạ đặc trưng kernel ngẫu nhiên φ: Rᵈ → Rᵐ trong đó φ(x) = (1/√m)exp(Wx - ||x||²/2) với W ∈ Rᵐˣᵈ được lấy mẫu ngẫu nhiên, từng entry, từ phân phối chuẩn tắc N(0,1). Chúng tôi áp dụng φ cho mỗi vector hàng của ma trận Q, K, và ký hiệu Q̃ = φ(Q) và K̃ = φ(K) (theo hàng). Lưu ý rằng chúng tôi không tạo ra Q̃K̃ᵀ.

2. Xấp xỉ Thưa. Chúng tôi định nghĩa quy trình Sparse trả về ma trận thưa S khớp A - Q̃K̃ᵀ trên supp(S). Cụ thể, sử dụng một họ các hàm hash nhạy cảm vị trí, tính toán các mã hash của mỗi vector query và key trong ma trận Q, K (theo hàng). Cho Ω là tập hợp các vị trí (i,j) mà qᵢ và kⱼ có cùng mã hash (tức là, rơi vào cùng hash bucket). Cho S là ma trận thưa có support là Ω, và với mỗi (i,j) ∈ Ω, định nghĩa

    Sᵢ,ⱼ = exp(qᵢᵀkⱼ) - φ(qᵢ)ᵀφ(kⱼ) = exp(qᵢᵀkⱼ) - q̃ᵢᵀk̃ⱼ,    (1)

    trong đó qᵢ, kⱼ, q̃ᵢ, k̃ⱼ là hàng thứ i và j của Q, K, Q̃, K̃ tương ứng. Lưu ý rằng chúng tôi không tạo ra Q̃K̃ᵀ.

3. Xấp xỉ Scatterbrain. Với Q̃, K̃ trả về từ LowRank và S từ Sparse, chúng tôi tính toán đầu ra attention (chưa chuẩn hóa) với

    Õ = (Q̃K̃ᵀ + S)V = Q̃(K̃ᵀV) + SV.    (2)

Thuật toán chính xác, bao gồm bước chuẩn hóa, cũng như biến thể causal/unidirectional, được mô tả trong Phụ lục C. Chúng tôi cũng lưu ý tính linh hoạt của Scatterbrain: nó có thể sử dụng các loại xấp xỉ hạng thấp và thưa khác nhau như các thành phần phụ của nó. Sự kết hợp của Reformer và Performer chỉ là một instance của Scatterbrain. Thay vì sử dụng Reformer như một thành phần thưa, chúng ta có thể sử dụng local attention [5] hoặc random block-sparse attention [16]. Thay vì sử dụng Performer [17] như một thành phần hạng thấp, chúng ta cũng có thể sử dụng Linear attention [35] hoặc global tokens như trong BigBird [70].

--- TRANG 5 ---
Hình 4: MSE per-entry cho các xấp xỉ khác nhau, qua một phạm vi độ lớn của qᵀk. Scatterbrain có MSE thấp cho cả entries nhỏ và lớn, do đó vượt trội hơn các đối tác thưa (Reformer) và hạng thấp (Performer).

Phương pháp Scatterbrain sẽ hoạt động chính xác theo cách tương tự. Miễn là thành phần hạng thấp không thiên lệch (ví dụ, Performer), sự kết hợp của nó với bất kỳ thành phần thưa nào trong Scatterbrain sẽ cho ra một ước lượng không thiên lệch của ma trận attention như được chỉ ra dưới đây.

4.3 Scatterbrain: Phân tích
Phương pháp của chúng tôi kết hợp xấp xỉ hạng thấp Q̃K̃ᵀ (có rank m ≪ n) với xấp xỉ thưa S. Chúng tôi lập luận rằng nó chính xác (sai số xấp xỉ thấp hơn so với baseline) và hiệu quả (scaling tương tự như chỉ thưa hoặc hạng thấp). Insight chính của phân tích là xấp xỉ của chúng tôi chính xác cho các entries trên support của S (được chọn bởi LSH), có khả năng lớn. Đối với các entries không trong support của S (có khả năng nhỏ), xấp xỉ của chúng tôi khớp phần hạng thấp (Performer) Q̃K̃ᵀ, không thiên lệch và có phương sai thấp cho các entries này. Kết quả là, Scatterbrain giữ lại tính không thiên lệch của Performer [17] nhưng với phương sai nghiêm ngặt thấp hơn.

Chúng tôi so sánh Scatterbrain với baseline hạng thấp (Performer) và baseline thưa (Reformer). Performer cũng dựa trên xấp xỉ kernel φ, và đơn giản sử dụng Q̃K̃ᵀ để xấp xỉ ma trận attention A. Reformer sử dụng LSH để xác định các entries lớn của A, sau đó tính toán ma trận thưa S sao cho Sᵢⱼ = exp(qᵢᵀkⱼ) với ij ∈ supp(S).

Độ chính xác: Do cách S được định nghĩa trong Phương trình (1), Q̃K̃ᵀ + S khớp A = exp(QKᵀ) chính xác tại các vị trí (i,j) ∈ Ω, là các vị trí có khả năng có giá trị lớn. Điều này giải quyết một điểm yếu của các phương pháp hạng thấp (ví dụ, Performer) nơi ước lượng hạng thấp không chính xác cho các vị trí có giá trị lớn. Chúng tôi phân tích kỳ vọng và phương sai per entry của ước lượng của chúng tôi dưới đây (chứng minh trong Phụ lục D).

Định lý 2. Định nghĩa ψ(q,k) = exp(qᵀk), ψ̂ₚfₑ như ước lượng của Performer và ψ̂ₛb như ước lượng của Scatterbrain. Ký hiệu Sᵈ⁻¹ ⊂ Rᵈ là hình cầu đơn vị. Giả sử q,k ∈ Sᵈ⁻¹ sao cho ||q - k|| < σ. Khi đó:

E[ψ̂ₛb(q,k)] = ψ(q,k); Var[ψ̂ₛb(q,k)] = (1-p)Var[ψ̂ₚfₑ(q,k)] < Var[ψ̂ₚfₑ(q,k)]    (3)

trong đó p = exp(-σ²/(4σ²) - 2ln d - O(ln ln d)).

Do đó Scatterbrain không thiên lệch, tương tự như Performer [17], nhưng với phương sai nghiêm ngặt thấp hơn. Phương sai nhỏ nếu exp(qᵀk) nhỏ (vì Var(ψ̂ₚfₑ(q,k)) sẽ nhỏ), hoặc nếu exp(qᵀk) lớn (vì xác suất không được chọn bởi LSH, 1-p, sẽ nhỏ). Trong Hình 4, chúng tôi vẽ MSE per-entry của các phương pháp khác nhau từ Định lý 2 khi xấp xỉ unnormalized softmax attention exp(QKᵀ). Scatterbrain có thể xấp xỉ tốt cả entries nhỏ (tương tự như baseline hạng thấp, Performer), cũng như entries lớn (tương tự như baseline thưa, Reformer). Do đó Scatterbrain có MSE thấp hơn nhiều so với Performer cho entries lớn, và MSE thấp hơn so với Reformer cho entries nhỏ.

Hiệu quả: Trong Phương trình (2), tính toán SV hiệu quả vì S thưa, và Q̃(K̃ᵀV) hiệu quả vì cách chúng ta kết hợp phép nhân ma trận (scaling như O(nmd) thay vì O(n²d), lớn hơn nhiều nếu m ≪ n).

Chúng tôi xác thực hai tính chất này của phương pháp chúng tôi trong Phần 5.

5 Thí nghiệm
Chúng tôi xác thực ba tuyên bố cho thấy Scatterbrain cung cấp xấp xỉ chính xác và hiệu quả cho các ma trận attention, cho phép nó vượt trội hơn các baseline thưa và hạng thấp trên các tập dữ liệu benchmark.

•Trong Phần 5.1, chúng tôi đánh giá sai số xấp xỉ và độ chính xác kiểm tra của các phương pháp xấp xỉ khác nhau trên các mô hình đã được đào tạo trước như BigGAN và Vision Transformer. Chúng tôi cho thấy rằng xấp xỉ bởi Scatterbrain gần với oracle Robust PCA và có sai số xấp xỉ thấp hơn tới 2:1 so với các baseline hiệu quả khác.

--- TRANG 6 ---
Hình 5: Đầu tiên: so sánh xấp xỉ giữa Scatterbrain và "cận dưới" Robust PCA của nó. Thứ hai: so sánh sai số so với entropy giữa SMYRF, Performer và Scatterbrain, ba đại diện của xấp xỉ thưa, hạng thấp và thưa+hạng thấp. Thứ ba và thứ tư: điểm Inception (cao hơn là tốt hơn) và điểm FID (thấp hơn là tốt hơn) của các biến thể attention khác nhau cho BigGAN đã được đào tạo trước.

•Trong Phần 5.2, chúng tôi xác thực rằng khi được đào tạo đầu-cuối, Scatterbrain vượt trội hơn baselines (attention thưa hoặc hạng thấp) trên nhiều tác vụ benchmark đa dạng, bao gồm mô hình hóa ngôn ngữ, phân loại, và các benchmark Long-range Arena (LRA). Scatterbrain đạt được độ chính xác trung bình cao hơn tới 5 điểm trên benchmark LRA so với Performer và Reformer.

•Trong Phần 5.3, chúng tôi chứng minh khả năng mở rộng của Scatterbrain, cho thấy rằng nó có mức sử dụng bộ nhớ và thời gian tương đương với các baseline đơn giản hơn (chỉ thưa hoặc hạng thấp) qua một phạm vi độ dài chuỗi đầu vào (Phần 5.3), trong khi yêu cầu bộ nhớ nhỏ hơn tới 12 lần so với full attention.

Tất cả chi tiết (siêu tham số, phân chia dữ liệu, v.v.), cùng với các thí nghiệm bổ sung, có trong Phụ lục E.

5.1 Độ chính xác Xấp xỉ của Scatterbrain

Bảng 1: Độ chính xác Top-1 của T2T Vision Transformer đã được đào tạo trước trên ImageNet với các thay thế attention khác nhau. Sai số đại diện cho sai số xấp xỉ chuẩn hóa trung bình so với full attention.

Attention | Top-1 Acc | Sai số (trung bình)
Full Attention | 81.7% | -
SMYRF | 79.8% | 11.4%
Performer | 80.1% | 7.5%
Baseline SMYRF + Performer | 79.7% | 12.6%
Scatterbrain | 80.7% | 5.3%

Chúng tôi đánh giá độ chính xác xấp xỉ của Scatterbrain theo ba bước: (1) so sánh nó với Robust PCA (thưa+hạng thấp), nền tảng lý thuyết và oracle của chúng tôi (2) so sánh nó với SMYRF³ [22], Performer [17], là các biến thể phổ biến của xấp xỉ thưa và hạng thấp cho attention tương ứng và một baseline ngây thơ cộng trực tiếp SMYRF và Performer, (3) đánh giá độ chính xác suy luận khi thay thế full attention bằng xấp xỉ Scatterbrain.

Scatterbrain đạt sai số trong vòng 20% của oracle robust PCA, và sai số thấp hơn tới 2:1 so với SMYRF và Performer. Khi dùng như thay thế trực tiếp cho full attention, ngay cả khi không đào tạo, Scatterbrain có thể giảm bộ nhớ attention của Vision Transformer 98% với chi phí chỉ 0.8% giảm độ chính xác.

Thiết lập: Chúng tôi sử dụng các ma trận attention từ BigGAN và T2T-ViT đã được đào tạo trước. BigGAN là mô hình tiên tiến trong Sinh ảnh cho ImageNet. BigGAN có một lớp attention duy nhất ở độ phân giải 64 × 64 (4096 queries). T2T-ViT có 14 lớp attention. Scatterbrain đặt tỷ lệ giữa SMYRF và Performer dựa trên entropy của một tập con quan sát được của các ma trận attention trong các lớp khác nhau. Chúng tôi phân bổ nhiều bộ nhớ hơn cho thành phần hạng thấp so với phần thưa nếu entropy cao.

Scatterbrain và Robust PCA: Đầu tiên chúng tôi cho thấy rằng Scatterbrain xấp xỉ các ma trận attention đã được đào tạo trước nhanh hơn 10⁵ lần trong khi sai số xấp xỉ của nó nằm trong vòng 20% trung bình. Chúng tôi cũng cung cấp một ví dụ trực quan hóa trên 100 ma trận attention từ quá trình sinh BigGAN trong Hình 5 (trái).

Scatterbrain so với SMYRF và Performer: Chúng tôi cho thấy rằng Scatterbrain xấp xỉ các ma trận attention dày đặc đã được đào tạo trước với sai số rất thấp so với thưa (Reformer) hoặc hạng thấp (Performer). Đo sai số xấp xỉ Frobenius trên tác vụ sinh ảnh BigGAN, Scatterbrain đạt sai số thấp hơn 2× so với Performer.

³SMYRF là một biến thể của Reformer không yêu cầu key và query phải giống nhau, điều này cần thiết cho các thí nghiệm trong phần này.

--- TRANG 7 ---
Bảng 2: Hiệu suất của Scatterbrain, Reformer, Performer và Full-Attention trên các benchmark Long-Range-Arena và 2 tác vụ mô hình hóa ngôn ngữ phổ biến. Chúng tôi cố định cùng số lượng tham số (1/8 của full) được sử dụng để xấp xỉ ma trận attention cho mỗi phương pháp.

Attention | Copy (ppl) | WikiText-103 (ppl)
Full Attention | 1 | 25.258
Reformer | 6.8 | 27.68
Performer | 49 | 66
Scatterbrain | 2.58 | 26.72

Attention | ListOps | Text | Retrieval | Image | Pathfinder | Avg
Full Attention | 38.2 | 63.29 | 80.85 | 41.78 | 73.98 | 59.62
Reformer | 36.85 | 58.12 | 78.36 | 28.3 | 67.95 | 53.9
Performer | 35.75 | 62.36 | 78.83 | 39.71 | 68.6 | 57.05
Scatterbrain | 38.6 | 64.55 | 80.22 | 43.65 | 69.91 | 59.38

Thay thế trực tiếp cho full attention: Chúng tôi cho thấy rằng xấp xỉ chính xác dẫn trực tiếp đến Suy luận hiệu quả. Chúng tôi thay thế attention dày đặc của BigGAN bằng lớp Scatterbrain mà không có sửa đổi khác. Trong Hình 5 (hai bên phải), chúng tôi cho thấy điểm Inception và FID cho Scatterbrain và các baseline khác dưới các ngân sách bộ nhớ khác nhau. Tương tự, chúng tôi sử dụng T2T-ViT [69], là một token-to-token vision Transformer đã được đào tạo trước trên ImageNet [25]. Trong Bảng 1, chúng tôi cho thấy sai số xấp xỉ trung bình của Scatterbrain cho mỗi lớp và độ chính xác kiểm tra đầu-cuối sau khi thay thế full attention bằng Scatterbrain và các baseline khác. Đáng chú ý, Scatterbrain đạt 80.7% độ chính xác Top-1, chỉ giảm 1% so với 81.7% ban đầu bằng full attention trong khi giảm tới 98% mức sử dụng bộ nhớ.

5.2 Hiệu suất Đào tạo Đầu-cuối
Xấp xỉ chính xác các ma trận attention của Scatterbrain cho phép nó vượt trội hơn các phương pháp Transformer hiệu quả khác trên các tác vụ benchmark. Qua một phạm vi các tác vụ đa dạng, cả các tác vụ tự hồi quy thường được sử dụng (mô hình hóa chuỗi) và các tác vụ phân loại long-range benchmark (Long-Range Arena), Scatterbrain vượt trội hơn Performer (baseline hạng thấp) và Reformer (baseline thưa) tới 4 điểm.

5.2.1 Tác vụ Tự hồi quy
Trên tác vụ mô hình hóa ngôn ngữ tiêu chuẩn WikiText-103, Scatterbrain có được perplexity tốt hơn 1 điểm so với Reformer (baseline thưa), đạt trong vòng 1.5 điểm so với full attention.

Thiết lập: Chúng tôi so sánh hiệu suất của Scatterbrain với Reformer và Performer trên một tác vụ tổng hợp phổ biến, Copy, và một tác vụ mô hình hóa ngôn ngữ lớn: WikiText103 [45]. Reformer là một biến thể đại diện dựa trên xấp xỉ thưa và Performer là một biến thể dựa trên xấp xỉ hạng thấp. Mô hình cơ sở là vanilla Transformer [63]. Chúng tôi quan sát thấy rằng nói chung phân bổ nhiều ngân sách bộ nhớ hơn cho thưa có xu hướng hoạt động tốt hơn, vì vậy Scatterbrain đặt tỷ lệ thành 3:1 (thưa: thành phần hạng thấp) để đơn giản. Thống kê của mỗi tập dữ liệu và siêu tham số mô hình có trong Phụ lục E. Chúng tôi báo cáo kết quả tốt nhất của mỗi phương pháp trong perplexity.

Kết quả: Bảng 2 cho thấy perplexity kiểm tra cho Scatterbrain và các baseline khác dưới cùng ngân sách tham số (mỗi xấp xỉ chỉ được phép tính toán 1/8 của tính toán đầy đủ). Scatterbrain đạt perplexity tương đương so với mô hình Transformer full attention trên Copy và WikiText-103. Đáng chú ý, Scatterbrain đạt perplexity thấp hơn 4 điểm trên Copy và thấp hơn 1 điểm trên WikiText-103 so với Reformer, trong khi Performer không đào tạo ổn định trên các tác vụ tự hồi quy (loss không giảm).

Phân tích: Chúng tôi cũng phân tích kết quả bằng cách trực quan hóa sai số của Reformer (thưa), Performer (hạng thấp), và Scatterbrain (thưa + hạng thấp) với cùng số lượng tham số khi xấp xỉ các ma trận attention đầy đủ cho mỗi lớp attention trong quá trình đào tạo (Phụ lục E). Kết luận là đối với các tác vụ mô hình hóa ngôn ngữ, thưa+hạng thấp có sai số xấp xỉ nhỏ nhất trong hầu hết các trường hợp, và thưa có sai số lớn nhất, điều này khớp với kết quả đầu-cuối. Nó cũng xác nhận quan sát trong bài báo benchmark phổ biến [57] rằng các xấp xỉ dựa trên kernel hoặc hạng thấp kém hiệu quả hơn đối với dữ liệu có cấu trúc phân cấp.

--- TRANG 8 ---
5.2.2 Tác vụ Phân loại
Trên một bộ tác vụ long-range benchmark (Long Range Arena), Scatterbrain vượt trội hơn Reformer (baseline thưa) và Performer (baseline hạng thấp) tới 5 điểm trung bình.

Thiết lập: Chúng tôi so sánh hiệu suất của Scatterbrain với Reformer và Performer trên ListOps, hai phân loại: phân loại văn bản đánh giá IMDb ở mức byte, phân loại hình ảnh trên chuỗi pixel, một tác vụ truy xuất văn bản, và pathfinder. Các tập dữ liệu được lấy từ Long Range Arena (LRA) Benchmark [57], là một benchmark phổ biến gần đây được thiết kế để kiểm tra các Transformer hiệu quả. Tương tự như các tác vụ tự hồi quy ở trên, chúng tôi sử dụng Reformer và Performer làm baseline. Mô hình cơ sở cũng là vanilla Transformer. Chúng tôi tuân theo giao thức đánh giá từ [57]. Chúng tôi báo cáo độ chính xác tốt nhất của mỗi phương pháp.

Kết quả: Bảng 2 cho thấy độ chính xác cá nhân và trung bình của mỗi tác vụ cho Scatterbrain và các baseline khác dưới cùng ngân sách tham số. Đặc biệt, mỗi xấp xỉ chỉ được phép sử dụng 12.5% tính toán đầy đủ. Chúng ta có thể thấy Scatterbrain rất gần với full attention ngay cả với việc giảm lớn tính toán và bộ nhớ. Hơn nữa, nó vượt trội hơn tất cả các baseline khác một cách nhất quán trên mọi tác vụ và đạt được cải thiện độ chính xác trung bình hơn 5 điểm so với xấp xỉ dựa trên thưa Reformer và cải thiện độ chính xác trung bình hơn 2 điểm so với biến thể dựa trên hạng thấp Performer.

Hình 6: Tốc độ và bộ nhớ yêu cầu bởi các phương pháp attention hiệu quả khác nhau. Scatterbrain có khả năng cạnh tranh với SMYRF (baseline thưa) và Performer (baseline hạng thấp), trong khi nhanh hơn tới 3× và hiệu quả bộ nhớ hơn 12× so với full attention đối với độ dài chuỗi 4096.

Phân tích: Tương tự, để phân tích hiệu suất của Reformer, Performer và Scatterbrain, chúng tôi trực quan hóa sai số xấp xỉ của chúng với cùng số lượng tham số khi xấp xỉ các ma trận attention đầy đủ cho mỗi lớp attention trong quá trình đào tạo (Phụ lục E). Chúng tôi lại thấy rằng Scatterbrain có sai số xấp xỉ nhỏ nhất, trong khi Performer tồi tệ nhất trên ListOps và Reformer có sai số lớn nhất trên các tác vụ phân loại, điều này khớp với kết quả đầu-cuối và xác nhận các quan sát trước đó của chúng tôi (xấp xỉ thưa và hạng thấp vượt trội trong các chế độ khác nhau).

5.3 Hiệu quả của Scatterbrain, Mở rộng với Độ dài Chuỗi Đầu vào
Chúng tôi bao gồm các nghiên cứu ablation về khả năng mở rộng của Scatterbrain trong Hình 6, cho thấy rằng nó hiệu quả tính toán và bộ nhớ như các baseline đơn giản hơn như SMYRF và Performer, trong khi nhanh hơn tới 3× và hiệu quả bộ nhớ hơn 12× so với full attention đối với độ dài chuỗi 4096. Điều này chứng minh rằng sự kết hợp của chúng tôi giữa thưa và hạng thấp kế thừa hiệu quả của chúng.

Chúng tôi báo cáo thời gian chạy và mức tiêu thụ bộ nhớ của các độ dài chuỗi từ 512 đến 32768. Chúng tôi sử dụng batch size 16 cho tất cả các lần chạy và tiến hành thí nghiệm trên GPU V100. Vì hiệu quả sẽ phụ thuộc lớn vào phần cứng và chi tiết triển khai, chúng tôi thực hiện so sánh công bằng với nỗ lực tốt nhất. Chúng tôi điều chỉnh việc triển khai Pytorch từ thư viện pytorch-fast-transformers cho các baseline của chúng tôi và triển khai Scatterbrain tương tự mà không có bất kỳ kernel cuda tùy chỉnh nào.

6 Thảo luận
Hạn chế. Vì Scatterbrain có attention thưa như một thành phần, nó chưa thân thiện với phần cứng (trên GPU và TPU) như thành phần hạng thấp, sử dụng phép nhân ma trận dày đặc được tối ưu hóa rất tốt. Đây là hạn chế tương tự mà các phương pháp attention thưa khác gặp phải, nhưng chúng tôi hào hứng rằng các kernel GPU thưa hiệu quả hơn đang được phát triển [29, 31].

--- TRANG 9 ---
Tác động tiêu cực tiềm ẩn lên xã hội. Công trình của chúng tôi tìm hiểu vai trò của xấp xỉ ma trận (và có thể tiết kiệm năng lượng) trong lớp attention, có thể cải thiện nhiều ứng dụng, mỗi ứng dụng có lợi ích và tác hại tiềm ẩn riêng. Ví dụ, làm cho mô hình hóa ngôn ngữ hiệu quả hơn về tính toán và bộ nhớ có thể tạo điều kiện cho việc lan truyền thông tin sai lệch, và xử lý hình ảnh và video tốt hơn có thể làm cho giám sát tự động dễ dàng hơn. Để giảm thiểu những rủi ro này, cần phải giải quyết các vấn đề cụ thể của ứng dụng như quyền riêng tư và công bằng, vượt ra ngoài các chỉ số lỗi mà chúng tôi đã xem xét. Đặc biệt, đối với các mô hình ngôn ngữ (LM), trong khi công trình của chúng tôi giải quyết một phần vấn đề chi phí môi trường của LM được nêu ra trong [6], nó không giải quyết các vấn đề khác như dữ liệu đào tạo khó hiểu [6].

Thảo luận và công trình tương lai. Trong công trình này, chúng tôi đưa ra quan sát về cấu trúc thưa + hạng thấp của attention trong các mô hình Transformer và đặc trưng hóa lý thuyết các chế độ mà thưa, hạng thấp và thưa + hạng thấp vượt trội, dựa trên nhiệt độ softmax của các ma trận attention. Được thúc đẩy bởi quan sát này, chúng tôi trình bày Scatterbrain, một cách mới để thống nhất điểm mạnh của cả phương pháp thưa và hạng thấp để xấp xỉ attention chính xác và hiệu quả với các bảo đảm có thể chứng minh được. Chúng tôi xác thực thực nghiệm tính hiệu quả của Scatterbrain trên BigGAN đã được đào tạo trước, vision transformer, cũng như đào tạo đầu-cuối của vanilla transformer. Chúng tôi dự đoán rằng nghiên cứu về bài toán xấp xỉ cốt lõi này có thể hữu ích trong các bối cảnh khác, như các lớp attention tổng quát với tính phi tuyến khác ngoài softmax, và lớp đầu ra rộng trong mô hình hóa ngôn ngữ hoặc phân loại cực đại.

Lời cảm ơn
Chúng tôi cảm ơn Xun Huang, Sarah Hooper, Albert Gu, Ananya Kumar, Sen Wu, Trenton Chang, Megan Leszczynski, và Karan Goel vì những thảo luận hữu ích và phản hồi về các bản thảo đầu của bài báo.

Chúng tôi biết ơn sự hỗ trợ của NIH dưới số U54EB020405 (Mobilize), NSF dưới số CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), và 1937301 (RTML); ONR dưới số N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); Quỹ Moore, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Quỹ Okawa, American Family Insurance, Google Cloud, Salesforce, Total, chương trình HAI-AWS Cloud Credits for Research, Stanford Data Science Initiative (SDSI), và các thành viên của dự án Stanford DAWN: Facebook, Google, và VMWare. Trung tâm Mobilize là Trung tâm Tài nguyên Công nghệ Y sinh, được tài trợ bởi NIH National Institute of Biomedical Imaging and Bioengineering thông qua Grant P41EB027060. Chính phủ Hoa Kỳ được ủy quyền sao chép và phân phối bản in lại cho các mục đích Chính phủ bất kể bất kỳ ghi chú bản quyền nào trên đó. Bất kỳ ý kiến, phát hiện, và kết luận hoặc khuyến nghị nào được thể hiện trong tài liệu này là của các tác giả và không nhất thiết phản ánh quan điểm, chính sách, hoặc sự tán thành, dù rõ ràng hay ngụ ý, của NIH, ONR, hoặc Chính phủ Hoa Kỳ. Nghiên cứu của Atri Rudra được hỗ trợ bởi NSF grant CCF-1763481.

Tài liệu tham khảo
[1] Keivan Alizadeh, Ali Farhadi, và Mohammad Rastegari. Butterfly transform: An efficient FFT based neural architecture design. Trong The Conference on Computer Vision and Pattern Recognition (CVPR), 2020.

[2] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, và Ludwig Schmidt. Practical and optimal lsh for angular distance. Trong C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, và R. Garnett, biên tập viên, Advances in Neural Information Processing Systems (NeurIPS), trang 1225–1233. 2015.

[3] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, và Ludwig Schmidt. Practical and optimal LSH for angular distance. Trong Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 1, trang 1225–1233, 2015.

[4] R Artusi, P Verderio, và E Marubini. Bravais-pearson and spearman correlation coefficients: meaning, test of hypothesis and confidence interval. The International journal of biological markers, 17(2):148–151, 2002.

--- TRANG 10 ---
[5] Iz Beltagy, Matthew E Peters, và Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

[6] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, và Margaret Mitchell. On the dangers of stochastic parrots: Can language models be too big? Trong Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, New York, NY, USA, 2021. Association for Computing Machinery.

[7] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và cộng sự. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

[8] Emmanuel J Candès và Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717–772, 2009.

[9] Emmanuel J Candès, Xiaodong Li, Yi Ma, và John Wright. Robust principal component analysis? Journal of the ACM (JACM), 58(3):1–37, 2011.

[10] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và Sergey Zagoruyko. End-to-end object detection with transformers. Trong European Conference on Computer Vision, trang 213–229. Springer, 2020.

[11] Beidi Chen và Anshumali Shrivastava. Densified winner take all (wta) hashing for sparse datasets. Trong Uncertainty in artificial intelligence, 2018.

[12] Beidi Chen, Anshumali Shrivastava, và Rebecca C Steorts. Unique entity estimation with application to the syrian conflict. The Annals of Applied Statistics, 12(2):1039–1067, 2018.

[13] Beidi Chen, Yingchen Xu, và Anshumali Shrivastava. Fast and accurate stochastic gradient estimation. 2019.

[14] Beidi Chen, Tharun Medini, James Farwell, Charlie Tai, Anshumali Shrivastava, và cộng sự. SLIDE: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems. Proceedings of Machine Learning and Systems, 2:291–306, 2020.

[15] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava, và Christopher Ré. Mongoose: A learnable lsh framework for efficient neural network training. Trong The International Conference on Learning Representations (ICLR), 2021.

[16] Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

[17] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, và cộng sự. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.

[18] Shabnam Daghaghi, Tharun Medini, Nicholas Meisburger, Beidi Chen, Mengnan Zhao, và Anshumali Shrivastava. A tale of two efficient and informative negative sampling distributions. Trong International Conference on Machine Learning, trang 2319–2329. PMLR, 2021.

[19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, và Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.

[20] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, và Christopher Ré. Learning fast algorithms for linear transforms using butterfly factorizations. Trong The International Conference on Machine Learning (ICML), 2019.

--- TRANG 11 ---
[21] Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, và Christopher Ré. Kaleidoscope: An efficient, learnable representation for all structured linear maps. Trong The International Conference on Learning Representations (ICLR), 2020.

[22] Giannis Daras, Nikita Kitaev, Augustus Odena, và Alexandros G Dimakis. Smyrf: Efficient attention using asymmetric clustering. arXiv preprint arXiv:2010.05315, 2020.

[23] Christopher De Sa, Christopher Re, và Kunle Olukotun. Global convergence of stochastic gradient descent for some non-convex matrix problems. Trong International Conference on Machine Learning, trang 2332–2341. PMLR, 2015.

[24] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher Ré, và Atri Rudra. A two-pronged progress in structured dense matrix vector multiplication. Trong Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, trang 1060–1079. SIAM, 2018.

[25] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong 2009 IEEE Conference on Computer Vision and Pattern Recognition, trang 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.

[26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[27] Yihe Dong, Piotr Indyk, Ilya Razenshteyn, và Tal Wagner. Learning space partitions for nearest neighbor search. Trong International Conference on Learning Representations (ICLR), 2019.

[28] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, và cộng sự. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

[29] Trevor Gale, Matei Zaharia, Cliff Young, và Erich Elsen. Sparse GPU kernels for deep learning. Trong Supercomputing, 2020.

[30] Aristides Gionis, Piotr Indyk, Rajeev Motwani, và cộng sự. Similarity search in high dimensions via hashing. Trong Vldb, tập 99, trang 518–529, 1999.

[31] Scott Gray, Alec Radford, và Diederik P Kingma. GPU kernels for block-sparse weights. arXiv preprint arXiv:1711.09224, 3, 2017.

[32] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, và Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. Trong Advances in neural information processing systems (NeurIPS), 2020.

[33] Harold Hotelling. Analysis of a complex of statistical variables into principal components. Journal of educational psychology, 24(6):417, 1933.

[34] Piotr Indyk và Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. Trong Proceedings of the thirtieth annual ACM symposium on Theory of computing, trang 604–613, 1998.

[35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. Trong International Conference on Machine Learning, trang 5156–5165. PMLR, 2020.

[36] Nikita Kitaev, Łukasz Kaiser, và Anselm Levskaya. Reformer: The efficient transformer. Trong The International Conference on Machine Learning (ICML), 2020.

[37] Alex Krizhevsky, Geoffrey Hinton, và cộng sự. Learning multiple layers of features from tiny images. 2009.

[38] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. Trong The International Conference on Learning Representations (ICLR), 2020.

--- TRANG 12 ---
[39] Valerii Likhosherstov, Krzysztof Choromanski, Jared Davis, Xingyou Song, và Adrian Weller. Sub-linear memory: How to make performers slim. arXiv preprint arXiv:2012.11346, 2020.

[40] Drew Linsley, Junkyung Kim, Vijay Veerabadran, và Thomas Serre. Learning long-range spatial dependencies with horizontal gated-recurrent units. arXiv preprint arXiv:1805.08315, 2018.

[41] Zichang Liu, Zhaozhuo Xu, Alan Ji, Jonathan Li, Beidi Chen, và Anshumali Shrivastava. Climbing the wol: Training for cheaper inference. arXiv preprint arXiv:2007.01230, 2020.

[42] Haoneng Luo, Shiliang Zhang, Ming Lei, và Lei Xie. Simplified self-attention for transformer-based end-to-end speech recognition. Trong 2021 IEEE Spoken Language Technology Workshop (SLT), trang 75–81. IEEE, 2021.

[43] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, và Luke Zettlemoyer. Luna: Linear unified nested attention. arXiv preprint arXiv:2106.01540, 2021.

[44] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, và Christopher Potts. Learning word vectors for sentiment analysis. Trong Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, trang 142–150, 2011.

[45] Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.

[46] Nikita Nangia và Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning. arXiv preprint arXiv:1804.06028, 2018.

[47] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, và Dustin Tran. Image transformer. Trong International Conference on Machine Learning, trang 4055–4064. PMLR, 2018.

[48] Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, và Amjad Abu-Jbara. The acl anthology network corpus. Language Resources and Evaluation, 47(4):919–944, 2013.

[49] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, và Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. Trong The International Conference on Learning Representations (ICLR), 2020.

[50] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.

[51] Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, và cộng sự. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020.

[52] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 12(12), 2011.

[53] Aurko Roy, Mohammad Saffar, Ashish Vaswani, và David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68, 2021.

[54] Anshumali Shrivastava và Ping Li. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips). Trong Advances in Neural Information Processing Systems (NeurIPS), trang 2321–2329, 2014.

[55] Vikas Sindhwani, Tara N. Sainath, và Sanjiv Kumar. Structured transforms for small-footprint deep learning. Trong Advances in Neural Information Processing Systems, trang 3088–3096, 2015.

--- TRANG 13 ---
[56] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, và Armand Joulin. Adaptive attention span in transformers. Trong Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2019.

[57] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, và Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020.

[58] Yi Tay, Mostafa Dehghani, Dara Bahri, và Donald Metzler. Efficient transformers: A survey. arXiv preprint arXiv:2009.06732, 2020.

[59] Richard Taylor. Interpretation of the correlation coefficient: a basic review. Journal of diagnostic medical sonography, 6(1):35–39, 1990.

[60] Reginald P Tewarson và Reginald P Tewarson. Sparse matrices, tập 69. Academic Press New York, 1973.

[61] Anna Thomas, Albert Gu, Tri Dao, Atri Rudra, và Christopher Ré. Learning compressed transforms with low displacement rank. Trong Advances in neural information processing systems (NeurIPS), trang 9052–9060, 2018.

[62] Madeleine Udell và Alex Townsend. Why are big data matrices approximately low rank? SIAM Journal on Mathematics of Data Science, 1(1):144–160, 2019.

[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.

[64] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.

[65] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, và Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.

[66] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, và Michael Auli. Pay less attention with lightweight and dynamic convolutions. Trong The International Conference on Learning Representations (ICLR), 2019.

[67] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, và Vikas Singh. Nystromformer: A Nystrom-based algorithm for approximating self-attention. arXiv preprint arXiv:2102.03902, 2021.

[68] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, và Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.

[69] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, và Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.

[70] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và cộng sự. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33, 2020.

[71] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, và Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. arXiv preprint arXiv:2107.02192, 2021.

--- TRANG 14 ---
Phụ lục

Mục lục
A Công trình Liên quan Mở rộng 17
A.1 Robust PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.2 Transformers Hiệu quả . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.3 Locality Sensitive Hashing cho Đào tạo Mạng Neural Hiệu quả . . . . . . . . . . . . . . . 18
A.4 Ma trận Có cấu trúc cho Mô hình Học máy Hiệu quả . . . . . . . . . . . . . . . . . . . . 18

B Quan sát Thúc đẩy: Cấu trúc Hạng thấp và Thưa của Ma trận Attention 19
B.1 Thiết lập . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.2 Quan sát 1: Sai số xấp xỉ thưa và hạng thấp tương quan âm . . . . . . . . . . . . . . . 19
B.3 Quan sát 2: Sai số xấp xỉ thưa thấp hơn khi entropy softmax thấp và sai số xấp xỉ hạng thấp thấp hơn khi entropy cao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.4 Quan sát 3: Thưa + Hạng thấp đạt sai số xấp xỉ tốt hơn so với chỉ thưa hoặc hạng thấp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

C Chi tiết Thuật toán và Triển khai Scatterbrain 22

D Chứng minh 23
D.1 Tính Biểu đạt của Ma trận Thưa + Hạng thấp . . . . . . . . . . . . . . . . . . . . . . . . 23
D.2 Mô hình Sinh, Nhiệt độ Softmax, và Xấp xỉ Ma trận . . . . . . . . . . . . . . . . . . . . 26
D.3 Scatterbrain: Phân tích . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

E Thí nghiệm và Chi tiết Bổ sung 31
E.1 Tập dữ liệu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
E.2 Thiết lập . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
E.3 Nghiên cứu Ablation Khác . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
E.4 Phân tích . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
E.5 Thí nghiệm Bổ sung về Tinh chỉnh Bert trên GLUE . . . . . . . . . . . . . . . . . . . . . 34

F Thảo luận và Công trình Tương lai Khác 34

--- TRANG 15 ---
A Công trình Liên quan Mở rộng

A.1 Robust PCA
Robust Principle Component Analysis (robust PCA) là bài toán tìm cách phân tích ma trận M thành tổng của các thành phần thưa và hạng thấp: M = S + L. Đây là một sửa đổi của PCA để phù hợp với các quan sát bị hỏng (hay nhiễu). Phần thưa bao phủ nhiễu, trong khi phần hạng thấp phục hồi các thành phần chính. Phương pháp phổ biến nhất để giải quyết vấn đề này là relaxation lồi [8], trong đó người ta tối thiểu hóa lỗi ∥M - S - L∥²F chịu ràng buộc ℓ₁ trên ∥S∥₁ và ràng buộc nuclear norm trên ∥L∥*, để thúc đẩy tính thưa của S và tính hạng thấp của L. Bài toán lồi này có thể được giải quyết với nhiều phương pháp khác nhau, như phương pháp interior point hoặc phương pháp Augmented Lagrange Multipliers.

Trong bối cảnh của chúng tôi, để tìm phân tách thưa + hạng thấp của ma trận attention, cũng có thể heuristically "bóc tách" phần thưa bằng cách tìm các entries lớn của ma trận attention, sau đó tìm phân tách hạng thấp của phần còn lại. Để tránh tạo ra toàn bộ ma trận attention, có thể sử dụng LSH để tìm vị trí tiềm năng của các entries lớn, và sử dụng matrix completion [52] để tìm phân tách hạng thấp. Gradient descent có thể tìm global optimum cho bài toán matrix completion này [23]. Tuy nhiên, nó vẫn yêu cầu quá nhiều iteration để được sử dụng trong mỗi bước đào tạo.

A.2 Transformers Hiệu quả
Xấp xỉ Thưa, Hạng thấp: Mô hình dựa trên Transformer như BERT [38] đã đạt được hiệu suất chưa từng có trong xử lý ngôn ngữ tự nhiên. Gần đây, Vision Transformers [28,69] cũng đã đạt được hiệu suất tương đương với mạng neural tích chập truyền thống trong các tác vụ thị giác máy tính [66]. Tuy nhiên, tính toán bậc hai của các lớp attention hạn chế khả năng mở rộng của Transformers. Có nhiều hướng hiện tại để vượt qua nút thắt này, bao gồm xấp xỉ ma trận attention như Reformer [36], Performer [17], tận dụng mô-đun bộ nhớ phụ có thể truy cập nhiều token cùng lúc [38,39,56] như Longformer [5] và BigBird [70], recurrence dựa trên segment như Transformer-XL [19] và Compressive Transformer [49]. Vui lòng tham khảo một survey gần đây [58] để biết thêm chi tiết. Trong bài báo này, chúng tôi chủ yếu khám phá trong phạm vi xấp xỉ các ma trận attention dày đặc hoặc đầy đủ.

Sự kết hợp hiện tại của Attention Thưa và Hạng thấp: Trọng tâm của chúng tôi vào bài toán xấp xỉ ma trận cổ điển và được định nghĩa rõ ràng, trái ngược với việc chỉ thiết kế một mô hình hiệu quả hoạt động tốt trên các tác vụ downstream (ví dụ, Longformer, Luna, Long-short transformer, v.v.) mang lại cho chúng tôi một số lợi thế: (i) Dễ hiểu và phân tích lý thuyết hơn (Phần 3, 4). Chúng ta thấy rằng Scatterbrain cho ra một ước lượng không thiên lệch của ma trận attention, và chúng ta cũng có thể hiểu cách phương sai của nó thay đổi. (ii) Đánh giá rõ ràng dựa trên sai số xấp xỉ, cũng như khả năng thay thế trực tiếp lớp attention đầy đủ bằng attention Scatterbrain mà không cần đào tạo lại (Phần 5). Thiết lập này ngày càng quan trọng vì các mô hình transformer ngày càng lớn và việc đào tạo chúng từ đầu trở nên cấm đoán về chi phí. Các phương pháp khác như Luna và Long-short transformer không tương thích ngược với các mô hình đã được đào tạo trước.

Ở đây chúng tôi so sánh Scatterbrain với các công trình khác được người đánh giá đề cập, cho thấy hầu hết chúng là các trường hợp đặc biệt của Scatterbrain. Chúng tôi cũng sẽ thêm cuộc thảo luận này vào phiên bản cập nhật của bản thảo.

•Longformer [5]: một trường hợp đặc biệt của Scatterbrain trong đó thành phần thưa là local attention, và thành phần hạng thấp là global tokens. Global tokens có thể được coi là một dạng hạn chế của xấp xỉ hạng thấp.

•BigBird [70]: một trường hợp đặc biệt của Scatterbrain trong đó thành phần thưa là local + random sparse attention, và thành phần hạng thấp là global tokens. Việc sử dụng global tokens làm cho mô hình không phù hợp với mô hình hóa autoregressive. Mặt khác, tính tổng quát của Scatterbrain cho phép nó sử dụng các loại attention hạng thấp khác (ví dụ, Performer), và do đó Scatterbrain hoạt động trên cả thiết lập attention causal/autoregressive và bidirectional/non-causal. Động lực của BigBird cũng khá khác với của chúng tôi: họ nhằm thiết kế attention hiệu quả sao cho toàn bộ mô hình Transformer vẫn là universal approximator và Turing complete. Mục tiêu của chúng tôi cụ thể hơn và dễ đánh giá hơn: chúng tôi xấp xỉ các ma trận attention, để có sai số Frobenius nhỏ giữa attention Scatterbrain và các ma trận attention đầy đủ.

•Luna [43] (công trình đồng thời): họ sử dụng một chuỗi phụ có độ dài cố định và hai bước attention liên tiếp: chuỗi ngữ cảnh attend đến chuỗi phụ, và sau đó chuỗi query attend đến chuỗi phụ. Điều này tương tự về tinh thần với attention hạng thấp (Linformer) và global tokens, nhưng nó không phải là xấp xỉ hạng thấp do tính phi tuyến giữa hai bước attention. Không rõ với chúng tôi rằng nó kết hợp các loại attention khác nhau.

•Long-short transformer [71] (công trình đồng thời): một trường hợp đặc biệt của Scatterbrain trong đó thành phần thưa là local attention và thành phần hạng thấp là Linformer.

A.3 Locality Sensitive Hashing cho Đào tạo Mạng Neural Hiệu quả
Locality Sensitive Hashing (LSH) đã được nghiên cứu kỹ lưỡng trong approximate nearest-neighbor search [2,11,27,30,34,54]. Vì phương pháp brute-force cho similarity search tốn kém về mặt tính toán, các nhà nghiên cứu đã nghĩ ra nhiều cấu trúc indexing khác nhau để tăng tốc quá trình tìm kiếm. Thường thì điều này đi kèm với sự đánh đổi về chất lượng tìm kiếm. Dựa trên các cấu trúc indexing này, có thể đạt được thời gian tìm kiếm sub-linear. LSH cũng đã được sử dụng trong các bài toán ước lượng [12, 13].

Gần đây, đã có một số công trình tận dụng các cấu trúc dữ liệu LSH cho đào tạo mạng neural hiệu quả. Trong quá trình đào tạo, các ma trận trọng số được sửa đổi từ từ thông qua gradients dẫn xuất từ hàm mục tiêu. Nếu chúng ta coi các trọng số như dữ liệu tìm kiếm và đầu vào như queries, chúng ta có thể xem đào tạo mạng neural như một bài toán similarity search. Ví dụ, [14,18,41] đề xuất một thuật toán thực hiện tính toán forward và backward thưa thông qua maximum inner product search trong quá trình đào tạo. Nó dựa trên quan sát rằng mô hình thường over-parameterized nên activation cho một đầu vào cho trước có thể thưa và LSH được sử dụng để tìm hoặc áp đặt cấu trúc thưa. Tương tự, các thuật toán dựa trên LSH cũng đã được sử dụng trong Transformers [14,15], trong đó LSH được sử dụng để nắm bắt cấu trúc thưa của các ma trận attention. Chúng có thể giảm đáng kể nút thắt bộ nhớ của các mô-đun self-attention đặc biệt trên các chuỗi dài trong Transformer. Mặc dù [15] đã thực hiện một số khám phá để cải thiện sự đánh đổi accuracy-efficiency của LSH thông qua learnable LSH, hầu hết các công trình trên có hiểu biết hạn chế về khi nào và ở đâu LSH có thể hoạt động tốt.

A.4 Ma trận Có cấu trúc cho Mô hình Học máy Hiệu quả
Thưa + hạng thấp là một ví dụ về một lớp ma trận có cấu trúc: những ma trận có thuật toán nhân ma trận-vector nhanh tiệm cận (độ phức tạp thời gian o(n²)) và ít tham số (độ phức tạp không gian o(n²)). Các ví dụ phổ biến bao gồm ma trận thưa, hạng thấp, và ma trận dựa trên fast transforms (ví dụ, Fourier transform, circulant, Toeplitz, Legendre transform, Chebyshev transform, và tổng quát hơn là orthogonal polynomial transforms). Các lớp ma trận này, và tổng quát hóa của chúng, đã được sử dụng trong học máy để thay thế các ma trận dày đặc trong các lớp fully connected, convolutional, và recurrent [32,55,61]. De Sa et al. [24] cho thấy rằng bất kỳ ma trận có cấu trúc nào cũng có thể được viết dưới dạng tích của các ma trận thưa, và tích của các ma trận thưa ngay cả với fixed sparsity pattern đã được chỉ ra là hiệu quả trong việc tham số hóa các mô hình nén [1,20,21].

Trong thiết lập của chúng tôi, việc xấp xỉ ma trận attention với các lớp ma trận có cấu trúc tổng quát hơn này vẫn còn khó khăn. Điều này là do nhiều trong số chúng là cố định (ví dụ, Fourier transform, orthogonal polynomial transforms), và thiếu thuật toán hiệu quả để tìm ma trận có cấu trúc gần nhất với một ma trận attention cho trước.

--- TRANG 16 ---
B Quan sát Thúc đẩy: Cấu trúc Hạng thấp và Thưa của Ma trận Attention

Chúng tôi nhằm xây dựng hiểu biết sâu sắc hơn về cấu trúc thưa và hạng thấp trong các ma trận attention thực tế: nơi mà mỗi loại vượt trội, và tiềm năng cho sự kết hợp của chúng. Cụ thể, chúng tôi

•cho thấy rằng sai số xấp xỉ thưa và hạng thấp tương quan âm (thông qua kiểm định thống kê),
•đặc trưng hóa các chế độ mà mỗi loại xấp xỉ thưa và hạng thấp phù hợp, như được quyết định bởi entropy của phân phối attention softmax, và
•chứng minh rằng thưa + hạng thấp có tiềm năng đạt xấp xỉ tốt hơn so với từng loại riêng lẻ.

B.1 Thiết lập
Ký hiệu M là ma trận attention (sau softmax) và H là entropy. Chúng tôi đo sai số xấp xỉ bằng chuẩn Frobenius của ma trận gốc và xấp xỉ (thưa hoặc hạng thấp). Tất cả các ma trận attention quan sát được trong phần này đều từ (1) một Transformer vanilla 4 lớp được đào tạo từ đầu trên phân loại đánh giá IMDb ở mức ký tự [57] (2) một Transformer vanilla 16 lớp được đào tạo từ đầu trên WikiText103 [45] (3) một BigGAN 1 lớp (attention) đã được đào tạo trước trên ImageNet [25]. Để thu thập các ma trận attention cho IMDb và WikiText103, trước tiên chúng tôi lưu checkpoint của các mô hình trong mỗi epoch; sau đó đánh giá 100 mẫu từ dữ liệu validation cho mỗi checkpoint và thu thập ma trận attention từ mỗi lớp mỗi head. Lưu ý chúng tôi lấy median của các thống kê (sai số) cho những 100 mẫu đó nếu khó trực quan hóa. Để thu thập ma trận attention cho BigGAN, chúng tôi sinh 100 mẫu và thu thập attention trong lúc thực hiện.

B.2 Quan sát 1: Sai số xấp xỉ thưa và hạng thấp tương quan âm

Bảng 3: Hệ số tương quan Spearman's rank, Pearson và Kendall's Tau giữa sai số xấp xỉ Thưa và Hạng thấp trên IMDb, WikiText-103, và BigGAN-ImageNet. Giá trị P < 0.05 cho thấy ý nghĩa thống kê. Hai sai số tương quan âm.

[TABLE showing correlation coefficients and p-values for IMDb, WikiText103, and BigGAN-ImageNet]

Chúng tôi cố định số lượng tham số, K, được phép cho mỗi xấp xỉ ma trận attention và thu thập các sai số từ xấp xỉ thưa và hạng thấp lý tưởng: top K entries cho mỗi hàng của ma trận đối với thưa và top K eigenvalues cho hạng thấp. Sau đó chúng tôi chạy ba kiểm định tương quan thống kê tiêu chuẩn [4,59], Spearman, Pearson và Kendall's Tau trên sai số xấp xỉ thưa và hạng thấp cho tất cả các ma trận. Chúng ta có thể thấy từ Bảng 3 rằng các sai số tương quan âm đáng kể (p-value < 0.05). Hơn nữa, ba biểu đồ bên trái trên Hình 7 trực quan hóa tương quan giữa hai sai số trên ba tập dữ liệu.

Tương quan âm này cho thấy rằng có một số tính chất của phân phối attention softmax quyết định khi nào thưa hoặc hạng thấp vượt trội. Chúng tôi xác thực tuyên bố này trong quan sát tiếp theo.

B.3 Quan sát 2: Sai số xấp xỉ thưa thấp hơn khi entropy softmax thấp và sai số xấp xỉ hạng thấp thấp hơn khi entropy cao

Chúng tôi trực quan hóa sai số xấp xỉ thưa và hạng thấp so với entropy của ma trận attention H(M) (áp dụng cho mỗi hàng, sau đó lấy trung bình) trên biểu đồ bên phải trong Hình 7. Các ma trận attention là 1024×1024 (được padding) nên trục x có phạm vi từ [0, ln(1024)]. Đối với phân phối entropy cao (khuếch tán hơn) các ma trận hạng thấp xấp xỉ ma trận attention tốt. Đối với phân phối entropy thấp (nhọn hơn), các ma trận thưa phù hợp hơn.

Điều này ngụ ý rằng các xấp xỉ thưa và hạng thấp có thể bổ sung cho nhau: nếu chúng ta có thể kết hợp điểm mạnh của cả hai, có thể đưa ra một xấp xỉ tốt hơn qua các tình huống tổng quát hơn. Do đó, trong quan sát tiếp theo, chúng tôi cố gắng kết hợp các xấp xỉ thưa và hạng thấp.

--- TRANG 17 ---
Hình 7: Đặc trưng hóa mối quan hệ giữa phân phối softmax của mỗi hàng ma trận attention và sai số xấp xỉ của thưa, hạng thấp và thưa+hạng thấp. Các biểu đồ trên, giữa và dưới lần lượt cho IMDb, WikiText103 và BigGAN-ImageNet. Trái: Sai số xấp xỉ của thưa và hạng thấp tương quan âm. Thưa hoạt động tốt khi hạng thấp không, và ngược lại. Phải: Entropy của phân phối attention softmax (tức là, tỷ lệ của logits) quyết định các chế độ mà thưa, hạng thấp, hoặc thưa + hạng thấp hoạt động tốt. Thưa + hạng thấp mang lại xấp xỉ tốt hơn so với chỉ thưa hoặc hạng thấp, trên toàn bộ phạm vi.

--- TRANG 18 ---
B.4 Quan sát 3: Thưa + Hạng thấp đạt sai số xấp xỉ tốt hơn so với chỉ thưa hoặc hạng thấp

Chúng tôi tìm một xấp xỉ của ma trận attention có dạng S + L, trong đó S thưa và L hạng thấp. Bài toán này có lịch sử phong phú và thường được giải quyết bằng Robust PCA. Như được thể hiện trong Hình 7, qua phạm vi entropy, xấp xỉ thưa + hạng thấp có thể đạt sai số thấp hơn so với chỉ thưa hoặc hạng thấp khi chọn tỷ lệ mix đúng của xấp xỉ thưa và hạng thấp một cách lý tưởng (với robust-PCA).

Được thúc đẩy bởi thực tế rằng các xấp xỉ thưa và hạng thấp của ma trận attention có điểm mạnh bổ sung (Quan sát 1 và 2), có thể muốn kết hợp chúng (Quan sát 3) với hy vọng tạo ra một xấp xỉ mạnh mẽ hơn hoạt động tốt trên các loại ma trận attention khác nhau. Điều trên đưa ra ba thách thức chính mà chúng tôi đã giải quyết trong bài báo chính:

•làm thế nào để tìm phân tách thưa + hạng thấp của ma trận attention hiệu quả về tính toán (thuật toán được nghiên cứu nhiều nhất, robust PCA, chậm hơn nhiều bậc để thực hiện ở mỗi iteration đào tạo) và hiệu quả về bộ nhớ (tức là, không cần tạo ra toàn bộ ma trận) (Phần 4),

•nếu chúng ta có thể tìm được phân tách thưa + hạng thấp như vậy, xấp xỉ chính xác đến mức nào (Phần 4.3),

•tham số hóa thưa + hạng thấp có tính biểu đạt như thế nào, tức là, có những lớp ma trận tự nhiên nào mà thưa + hạng thấp cho xấp xỉ tốt hơn tiệm cận so với chỉ thưa hoặc hạng thấp không) (Phần 3)?

--- TRANG 19 ---
C Chi tiết Thuật toán và Triển khai Scatterbrain

Cho Q, K ∈ R^(n×d) là ma trận query và key tương ứng, và V ∈ R^(n×d) là ma trận value. Cho các hàng của Q là q₁, ..., qₙ, và các hàng của K là k₁, ..., kₙ. Attention tính toán:

softmax(QK^T)V;

với softmax được áp dụng theo hàng, trong đó với mỗi vector v ∈ R^n, softmax(v) = (1/∑ⱼ₌₁ⁿ e^(vⱼ)) ⟨e^(v₁), ..., e^(vₙ)⟩. Ở đây chúng tôi bỏ qua tỷ lệ thông thường của QK^T/√d để đơn giản vì điều đó có thể được nhúng vào Q hoặc K. Lưu ý rằng softmax(QK^T) = D^(-1)exp(QK^T), trong đó hàm exponential được áp dụng theo element và D là ma trận đường chéo chứa các hằng số chuẩn hóa softmax (D_{i,i} = ∑ⱼ₌₁ⁿ exp(qᵢ^T kⱼ)). Khi đó attention có dạng D^(-1)exp(QK^T)V.

Chúng tôi mô tả thuật toán xấp xỉ Scatterbrain trong Thuật toán 1. Điều này bao gồm bước chuẩn hóa.

Thuật toán 1 Xấp xỉ Scatterbrain của Attention
1: Đầu vào: Q, K, V ∈ R^(n×d), siêu tham số m, k, l
2: procedure Init(m, k, l)
3:    Lấy mẫu W ∈ R^(m×d) trong đó Wᵢ ~ N(0,1) i.i.d.
4:    Kernels: R^d → R^m, φ(x) = exp(Wx - ||x||²/2)/√m
5:    Hash ∀l ∈ [L], Hₗ = {hₗ,ₖ}ₖ∈[K], H = ∪ₗ∈[L] Hₗ
6: end procedure
7: procedure LowRankApprox(Q, K, V, φ)
8:    Q̃ = φ(Q), K̃ = φ(K)    . áp dụng cho mỗi hàng
9:    return Q̃(K̃^T V), Q̃(K̃^T)1ₙ.
10: end procedure
11: procedure SparseApprox(Q, K, V, φ, H)
12:    Ω = {(i,j) | H(Qᵢ) = H(Kⱼ)}
13:    S ma trận thưa có support là Ω
14:    for (i,j) ∈ Ω do
15:        Sᵢⱼ = exp(qᵢ^T kⱼ) - φ(qᵢ)^T φ(kⱼ).
16:    end for
17:    return SV, S1ₙ.
18: end procedure
19: procedure ScatterbrainApprox(Q, K, V)
20:    φ, h ← Init(m, k, l).
21:    Oₗᵣ, Dₗᵣ ← LowRankApprox(Q, K, V, φ).
22:    Oₛ, Dₛ ← SparseApprox(Q, K, V, φ, h).
23:    return diag(Dₗᵣ + Dₛ)^(-1)(Oₗᵣ + Oₛ).
24: end procedure

Autoregressive / Causal / Unidirectional Attention Để xấp xỉ autoregressive attention, chúng tôi đơn giản sử dụng biến thể autoregressive của low-rank attention, và áp dụng autoregressive mask cho sparse attention. Cụ thể, cho M ∈ R^(n×n) là autoregressive mask, có tam giác dưới là tất cả số một và phần còn lại của các entries là không. Ma trận attention chưa chuẩn hóa là exp((QK^T) ⊙ M), và đầu ra chưa chuẩn hóa là exp((QK^T) ⊙ M)V, trong đó ⊙ là phép nhân theo element.

Biến thể autoregressive hạng thấp tính toán ((Q̃K̃^T) ⊙ M)V, mặc dù với kernel GPU tùy chỉnh / triển khai để không cần tạo ra ma trận n×n. Đối với thành phần thưa, chúng tôi đơn giản mask out các vị trí Sᵢⱼ với i > j. Tức là, chúng tôi có thể thực hiện S ⊙ M hiệu quả. Kết quả là, chúng tôi có thể tính toán đầu ra Scatterbrain ((Q̃K̃^T) ⊙ M)V + (S ⊙ M)V hiệu quả.

--- TRANG 20 ---
D Chứng minh

D.1 Tính Biểu đạt của Ma trận Thưa + Hạng thấp

Để thúc đẩy việc sử dụng ma trận thưa + hạng thấp, chúng tôi mô tả một họ ma trận attention trong đó ma trận thưa + hạng thấp cần ít tham số hơn tiệm cận để xấp xỉ ma trận attention, so với chỉ ma trận thưa hoặc hạng thấp. Đối với những trường hợp đó, chỉ thưa hoặc hạng thấp cần số lượng tham số bậc hai (O(n²), trong đó n×n là kích thước của ma trận attention) để có sai số xấp xỉ ε trong chuẩn Frobenius, trong khi thưa + hạng thấp chỉ cần O(n) tham số.

Chúng tôi xây dựng một họ ma trận cho thấy sự tách biệt giữa khả năng xấp xỉ của thưa + hạng thấp so với chỉ thưa hoặc hạng thấp. Cụ thể hơn, chúng tôi sẽ sử dụng đường chéo + hạng thấp (một trường hợp đặc biệt của thưa + hạng thấp).

Ví dụ 1. Cho σ ký hiệu một tham số thỏa mãn σ ∈ (0, 1/2]. Xem xét construction ngẫu nhiên sau của ma trận Q ∈ R^(n×d) với d ≥ 6⌈2log n⌉ và d = Θ(2log n), trong đó mỗi entry của Q được chọn độc lập và đều từ {±1/√d}. Cho M = exp(βQQ^T) trong đó exp là hàm exponential theo element (chúng tôi bỏ qua term chuẩn hóa của softmax ở đây).

Có thể chỉ ra (ví dụ bằng bất đẳng thức Hoeffding) rằng với xác suất cao

(QQ^T)ᵢ,ⱼ = {
    1,                    nếu i = j;
    ∈ [−σ, σ],           ngược lại.

Vì M = exp(βQQ^T) trong đó exp là hàm exponential theo element,

Mᵢ,ⱼ = {
    e^β,                             nếu i = j;
    ∈ [1 − O(σ), 1 + O(σ)],         ngược lại.

Trực giác, vì ma trận attention M có các entries đường chéo lớn, ma trận hạng thấp sẽ không thể xấp xỉ nó tốt. Tuy nhiên, các entries ngoài đường chéo cũng có kích thước hợp lý, do đó làm cho xấp xỉ thưa khó khăn. Với thưa + hạng thấp, chúng ta có thể sử dụng phần thưa để biểu diễn đường chéo, và phần hạng thấp để biểu diễn các elements còn lại, cho phép nó xấp xỉ ma trận này tốt. Chúng tôi chính thức hóa sự tách biệt này trong định lý dưới đây.

Định lý 3. Cho M là ma trận attention từ Ví dụ 1. Với bất kỳ ε ∈ [0,1], với xác suất ít nhất 1 − n^(-1), tồn tại ước lượng thưa + hạng thấp với O(ε^(-1)n^(3/2)log n) tham số đạt sai số Frobenius ε√n. Với bất kỳ ma trận R ∈ R^(n×n) có rank r sao cho nr = Ω(n) (ví dụ, R có o(n²) tham số), với xác suất ít nhất 1 − n^(-1), chúng ta có ||M − R||_F = Ω(√n). Hơn nữa, bất kỳ ma trận E_S nào có row sparsity k (mỗi hàng có ít hơn k non-zeros) sao cho nk = o(1) (ví dụ, E_S có o(n²) tham số) sẽ có lỗi ||M − E_S||_F = Ω(√n) với xác suất ít nhất 1 − n^(-1).

Chúng ta thấy rằng với bất kỳ ε ∈ [0,1], bất kỳ ước lượng hạng thấp hoặc thưa nào cho M với o(n²) tham số có Ω(ε^(-1)) lần lỗi của ước lượng thưa + hạng thấp với O(ε^(-1)n^(1.5)log n) tham số.

Chứng minh của Định lý 3. Với mỗi i ∈ [n], cho qᵢ ký hiệu hàng thứ i của Q ∈ R^(n×d). Định nghĩa J ∈ R^(n×n) là ma trận toàn số 1. Định nghĩa T = M − βJ − QQ^T. Do đó,

Tᵢ,ⱼ = {
    e^β − 1,                            nếu i = j
    e^(βqᵢ^T qⱼ) − 1 − βqᵢ^T qⱼ,         ngược lại.

Bằng bất đẳng thức Hoeffding, với một cặp i ≠ j, chúng ta có

P(|qᵢ^T qⱼ − E[qᵢ^T qⱼ]| ≥ σ) ≤ 2exp(−σ²/(2 · 1/√d · 1/√d · 2)) = 2exp(−dσ²/2).

Lưu ý rằng E[qᵢ^T qⱼ] = 0.

Bằng union bound trên tất cả các cặp i ≠ j (có n(n−1)/2 cặp như vậy), với xác suất ít nhất 1 − n²exp(−dσ²/2), chúng ta có

qᵢ^T qⱼ ∈ [−σ, σ] với mọi i ≠ j.

Vì chúng ta giả sử rằng d ≥ 6⌈2log n⌉, chúng ta có

n²exp(−dσ²/2) ≤ n²exp(−3log n) = n^(-1).

Do đó qᵢ^T qⱼ ∈ [−σ, σ] với mọi i ≠ j với xác suất ít nhất 1 − n^(-1). Trong phần còn lại của chứng minh, chúng tôi chỉ xem xét trường hợp này (trong đó qᵢ^T qⱼ ∈ [−σ, σ] với mọi i ≠ j).

Vì 1 + x ≤ e^x ≤ 1 + x + x² với |x| < 1, chúng ta có thể bound các elements ngoài đường chéo |Tᵢ,ⱼ| ≤ σ². Cụ thể, với mọi i ≠ j,

|Tᵢⱼ| = |e^(βqᵢ^T qⱼ) − 1 − βqᵢ^T qⱼ| ≤ β²(qᵢ^T qⱼ)²/2 ≤ σ².    (4)

Ước lượng thưa + hạng thấp: Chúng tôi sử dụng ước lượng thưa + hạng thấp sau:

E_SL = (e^β − 1)I + βJ + QQ^T,
       |________|   |_________|
         thưa      hạng thấp

trong đó (e^β − 1)I có row sparsity 1 và rank(βJ + QQ^T) ≤ d + 1 = O(σ^(-2)log n).

Lưu ý rằng ước lượng E_SL khớp M chính xác trên đường chéo, và trên ngoài đường chéo nó khác M bởi Tᵢⱼ. Do đó, sai số Frobenius của ước lượng thưa + hạng thấp là

||M − E_SL||_F² ≤ √n(n−1)σ² ≤ σ²n.

Đặt σ = ε√n^(-1/4) với 0 < ε ≤ 1, khi đó
(i) Số lượng tham số thưa + hạng thấp là n + n · rank ≤ n + O(σ^(-2)log n) = O(ε^(-1)n^(1.5)log n).
(ii) Sai số Frobenius là ε√n.

Ước lượng hạng thấp: Chúng tôi muốn lập luận rằng xấp xỉ hạng thấp sẽ yêu cầu nhiều tham số hơn. Nếu chúng ta xấp xỉ ma trận (e^β − 1)I bằng ma trận R với rank r, thì ma trận hiệu sẽ có ít nhất n − r singular values có độ lớn (e^β − 1)/2. Kết quả là, bởi định lý Eckart–Young–Mirsky,

||(e^β − 1)I − R||_F ≥ (1/2)√n − r.

Định nghĩa T' = T − (e^β − 1)I, khi đó T' là tất cả 0 trên đường chéo và có giá trị tuyệt đối σ² trên các entries ngoài đường chéo. Do đó ||T'||_F ≤ σ²n = ε²√n.

Chúng tôi muốn chỉ ra rằng nếu R' là ma trận rank r' thì ||M − R'||_F ≥ (1/2)√n − r' − d^(-1)||T'||_F. Chúng tôi lập luận bằng phản chứng. Giả sử tồn tại ma trận R' có rank r' sao cho

||M − R'||_F ≤ (1/2)√n − r' − d^(-1)||T'||_F.

Định nghĩa R = R' − βJ − QQ^T, nên M − R' = (e^β − 1)I − R + T'. Chúng ta thấy rằng:

||(e^β − 1)I − R||_F = ||M − R' − T'||_F ≤ ||M − R'||_F + ||T'||_F ≤ (1/2)√n − r' − d^(-1)||T'||_F + ||T'||_F = (1/2)√n − rank(R).

Điều này mâu thuẫn với kết quả trên, nói rằng ||(e^β − 1)I − R||_F ≥ (1/2)√n − rank(R).

Do đó bất kỳ ước lượng hạng thấp nào với rank r sao cho nr = Ω(n), có Ω(n²) tham số, sẽ có lỗi ít nhất Ω(√n − rd^(-1))||T'||_F = Ω(√n), là Ω(ε^(-1)) lần lỗi của ước lượng thưa + hạng thấp trên.

Ước lượng thưa: Đối với ước lượng thưa của chúng tôi, dễ thấy rằng với bất kỳ E_S ∈ R^(n×n) nào có row sparsity k (mỗi hàng có ít hơn k non-zeros),

||M − E_S||_F = Ω(√n(n−k)).

Điều này ngụ ý rằng để đạt lỗi O(ε√n), chúng ta cần nk = O(1), yêu cầu Ω(n²) tham số.

Bây giờ chúng tôi xây dựng ma trận cho thấy sự tách biệt tốt hơn giữa khả năng xấp xỉ của thưa + hạng thấp so với chỉ thưa hoặc hạng thấp.

Ví dụ 2. Xem xét construction ngẫu nhiên sau của ma trận Q ∈ R^(n×d) với d ≥ 6⌈2r log n⌉ và d = Θ(2r log n) (σ ∈ (0,1] và gần 0 và r là o(log n)): mỗi entry của Q được chọn độc lập và đều từ {±√r/d}. Cho M = exp(βQQ^T) trong đó exp là hàm exponential theo element.

Tương tự như Ví dụ 1, với xác suất cao, chúng ta có:

(QQ^T)ᵢ,ⱼ = {
    r,                    nếu i = j;
    ∈ [−σ, σ],           ngược lại.

Chúng ta cũng có:

Mᵢ,ⱼ = {
    e^(βr),                         nếu i = j;
    ∈ [1 − O(σ), 1 + O(σ)],         ngược lại.

Bằng cách đặt r appropriately, chúng ta có thể chính thức hóa sự tách biệt giữa khả năng xấp xỉ của ma trận thưa, hạng thấp, và thưa + hạng thấp:

Định lý 4. Cho M là ma trận attention từ Ví dụ 2. Bất kỳ ước lượng thưa hoặc hạng thấp nào của M cần Ω(n²) tham số cho lỗi O(εn) với xác suất ít nhất 1 − n^(-1) trong khi ước lượng thưa + hạng thấp cần O(n) tham số cho lỗi O(εn) với xác suất ít nhất 1 − n^(-1).

Chứng minh của Định lý 4. Tương tự như chứng minh của Định lý 3, bằng bất đẳng thức Hoeffding, với một cặp i ≠ j, chúng ta có

P(|qᵢ^T qⱼ − E[qᵢ^T qⱼ]| ≥ σ) ≤ 2exp(−σ²/(2 · r/√d · r/√d · 2)) = 2exp(−dσ²/2r).

Lưu ý rằng E[qᵢ^T qⱼ] = 0. Bằng union bound trên tất cả các cặp i ≠ j (có n(n−1)/2 cặp như vậy), với xác suất ít nhất 1 − n^(-1) (vì d ≥ 6⌈2r log n⌉), chúng ta có

qᵢ^T qⱼ ∈ [−σ, σ] với mọi i ≠ j.

Vì chúng ta giả sử rằng d ≥ 6⌈2log n⌉, chúng ta có Trong phần còn lại của chứng minh, chúng tôi chỉ xem xét trường hợp này (trong đó qᵢ^T qⱼ ∈ [−σ, σ] với mọi i ≠ j).

Cho T = M − (e^(βr) − 1)I + βJ, trong đó J là ma trận toàn số một. Chúng ta thấy rằng T là không trên đường chéo. Hơn nữa, sử dụng thực tế rằng e^x ≤ 1 + 2|x| với mọi x ∈ [−1, 1], các entries ngoài đường chéo của T có độ lớn tối đa 2σ.

Chúng tôi xem xét 3 ước lượng khác nhau.

Ước lượng thưa + hạng thấp: Ước lượng của chúng tôi là

E_SL = (e^(βr) − 1)I + βJ,
       |_____________|   |___|
         thưa         hạng thấp

trong đó (e^(βr) − 1)I có row sparsity 1 và rank(βJ) = 1.

Sai số Frobenius của xấp xỉ thưa + hạng thấp là

||M − E_SL||_F ≤ O(σ√n(n−1)) = O(εn).

Chúng ta có:
(i) Số lượng tham số thưa + hạng thấp là n(1 + 1) = O(n).
(ii) Sai số Frobenius của nó là O(εn).

Ước lượng hạng thấp: Chúng tôi muốn lập luận rằng xấp xỉ hạng thấp sẽ yêu cầu nhiều tham số hơn. Từ quan sát tương tự rằng bất kỳ ma trận R nào có rank r sao cho nr = Ω(1),

||(e^(βr) − 1)I − R||_F = Ω(e^(βr));

(bởi định lý Eckart–Young–Mirsky), chúng tôi thu được kết quả tương tự như chứng minh của Định lý 3.

Nếu R' là ma trận có rank r sao cho nr = Ω(1), thì ||M − R'||_F = Ω(εn) − ||T||_F = Ω(εn) − O(εn) = Ω(εn). Do đó bất kỳ ma trận hạng thấp nào với O(n²) tham số sẽ có lỗi Ω(εn).

Ước lượng thưa: Tương tự như chứng minh của Định lý 3, đối với ước lượng thưa của chúng tôi, dễ thấy rằng với bất kỳ E_S ∈ R^(n×n) nào có row sparsity k (mỗi hàng có ít hơn k non-zeros),

||M − E_S||_F = Ω(√n(n−k)).

Điều này ngụ ý rằng để có lỗi O(εn), chúng ta cần Ω(n²) tham số.

--- TRANG 21 ---
D.2 Mô hình Sinh, Nhiệt độ Softmax, và Xấp xỉ Ma trận

Ở đây chúng tôi chỉ ra 3 trường hợp mà tùy thuộc vào nhiệt độ softmax, chúng ta sẽ cần hạng thấp, hạng thấp + thưa, hoặc thưa để xấp xỉ ma trận attention.

Chúng tôi bắt đầu với một số ký hiệu trước. Cho ma trận B, cho B[i,j] là entry ở hàng thứ i và cột thứ j. Với một phạm vi [l,r], chúng tôi định nghĩa ma trận B[l,r] trong đó B[l,r][i,j] = B[i,j] nếu B[i,j] ∈ [l,r] và B[l,r] = 0 ngược lại (tức là, B[l,r] chỉ giữ các entries cho B nằm trong phạm vi [l,r], với các entries khác được zero out). Chúng tôi viết supp(C) cho tập hợp các vị trí của non-zeros trong C. Chúng tôi cho λᵢ(D) là eigenvalue lớn thứ i (theo giá trị tuyệt đối) của D.

Để chứng minh Định lý 1, trước tiên chúng tôi định nghĩa một lớp ma trận tổng quát hơn, chứng minh rằng ma trận attention trong Process 1 là một tập con của lớp này (với xác suất cao), và sau đó chỉ ra rằng Định lý 1 đúng cho lớp tổng quát hơn này. Chúng tôi giới thiệu một tham số bổ sung l ∈ R⁺, ngoài nhiệt độ nghịch đảo β và khoảng cách intro-cluster σ.

Lớp Ma trận 1. Cho Q ∈ R^(n×d) với mỗi hàng của Q có ℓ₂-norm trong [1 − O(σ), 1 + O(σ)], và cho A = QQ^T. Hơn nữa:

1. Cho H = A[1/l, 2^(1/l)] với l ≫ Ω(1). Giả sử rằng H là block diagonal với Θ(√n) blocks, và supp(H) là o(n²). Tức là, các entries lớn của QQ^T tạo thành ma trận block diagonal.

2. Cho L = A − H khi đó L = A[−σ, σ] trong đó σ = o(1/log d). Giả sử rằng có một phần không đổi của các elements trong supp(L) nằm trong [0, σ]. Giả sử rằng supp(A[0, σ]) là Θ(n²).

Cho M = exp(βA).

Bây giờ chúng tôi chỉ ra rằng Process 1 là một tập con của Matrix Class 1, với xác suất cao.

Lemma 5. Ma trận M trong Process 1 là tập con của Matrix Class 1, trong đó l = 1/(12σ).

Chứng minh. Đầu tiên chúng tôi bound norm của mỗi hàng trong Q trong Process 1. Với bất kỳ i, j, chúng ta có

||zᵢⱼ||₂² = ||cᵢ + rᵢⱼ||₂² = ||cᵢ||₂² + 2cᵢᵀrᵢⱼ + ||rᵢⱼ||₂².

Vì cᵢ ~ N(0, I_d/√d), ||cᵢ||₂² ∈ [1 − 2σ, 1 + 2σ] với xác suất ít nhất 1 − 2e^(-d σ²/8) (bằng lập luận tiêu chuẩn sử dụng thực tế rằng biến ngẫu nhiên χ² là sub-exponential). Tương tự, ||rᵢⱼ||₂² ∈ [σ² − 4σ, σ² + 4σ] với xác suất ít nhất 1 − 2e^(-d σ²/8). Bằng concentration of measure, chúng ta cũng có thể bound 2cᵢᵀrᵢⱼ ∈ [−2σ² − 3σ, 2σ² + 3σ] tương tự. Do đó, chúng ta có ||zᵢⱼ||₂² ∈ [1 − O(σ), 1 + O(σ)].

Bây giờ chúng tôi chỉ ra rằng các entries lớn của QQ^T tạo thành ma trận block diagonal. Với xác suất cao, các entries lớn đến từ inner product nội cụm, và các entries nhỏ đến từ inner product liên cụm.

Chúng tôi bound inner product nội cụm:

z^T_{ij} z_{ik} = (cᵢ + rᵢⱼ)^T (cᵢ + rᵢₖ)
                = ||cᵢ||₂² + cᵢᵀrᵢⱼ + cᵢᵀrᵢₖ + r^T_{ij} rᵢₖ.

Tương tự như lập luận trên, bằng concentration of measure, ||cᵢ||₂² ∈ [1 − σ, 1 + σ] với xác suất cao (chúng tôi sẽ chọn σ = Θ(σ)). Các cross terms cᵢᵀrᵢⱼ và cᵢᵀrᵢₖ có thể được bound sử dụng bất đẳng thức Cauchy-Schwarz nằm trong [−σ, σ] với xác suất cao. Và term thứ tư r^T_{ij} rᵢₖ nằm trong [−σ², σ²] với xác suất cao.

Do đó, inner product nằm trong 1 − O(σ) với xác suất cao. Điều này thỏa mãn điều kiện đầu tiên trong Matrix Class 1, với l = 1/(12σ), giả sử σ ≪ σ.

Chúng tôi sử dụng lập luận tương tự để bound inner product liên cụm. Với i ≠ i'

z^T_{ij} z_{i'k} = (cᵢ + rᵢⱼ)^T (cᵢ' + rᵢ'ₖ)
                 = cᵢᵀcᵢ' + cᵢᵀrᵢ'ₖ + cᵢ'ᵀrᵢⱼ + r^T_{ij} rᵢ'ₖ.

Bằng concentration of measure, cᵢᵀcᵢ' ∈ [−σ, σ]. Tương tự như trường hợp nội cụm, chúng ta có thể bound ba term khác, nên inner product này nằm trong [−O(σ), O(σ)]. Điều này thỏa mãn điều kiện thứ hai trong Matrix Class 1.

Để chứng minh Định lý 1 cho Matrix Class 1, chúng tôi bắt đầu với một số lemma kỹ thuật.

Lemma 6. Cho F ∈ R^(N×N) ≥ 0 là ma trận đối xứng. Cho λ_max là eigenvalue lớn nhất của F. Giả sử N ≥ 2, chúng ta có

λ_max ≥ min_{i≠j} F[i,j].

Chứng minh. Vì F đối xứng, λ_max thực và

λ_max = max_{u≠0} (u^T F u)/(u^T u).

Cho u là vector toàn số 1, khi đó

λ_max ≥ (1/N) ∑_{i=j} F[i,j] ≥ (1/N) ∑_{i≠j} F[i,j] ≥ (1/N) N(N-1) min_{i≠j} F[i,j] = min_{i≠j} F[i,j],

trong đó bước thứ hai theo từ tất cả các entries đường chéo không âm, bước cuối theo từ N ≥ 2

Điều trên ngụ ý kết quả sau:

Corollary 7. Cho F ∈ R^(N×N) ≥ 0 là ma trận block diagonal. Cho r là số lượng các blocks m×m trong F với m ≥ 2. Khi đó λ_r(F) ít nhất là element non-diagonal nhỏ nhất trong bất kỳ block m×m nào (m ≥ 2) trong F.

Chứng minh. Bằng Lemma 6, mỗi block m×m B (m ≥ 2) tự nó có max eigenvalue ít nhất min_{i≠j∈[m]} B[i,j]. Claim sau đó theo từ thực tế rằng bất kỳ eigenvalue nào của B cũng là eigenvalue của F.

Chúng tôi sẽ cần hàm sau cho lập luận hạng thấp của chúng tôi:

f_k(x) = ∑_{i=0}^k (x^i)/(i!).

Lưu ý rằng f_{∞}(x) = e^x.

Definition 1. Cho σ ∈ (0, 1/10) và L > 0. Chúng tôi nói một hàm f: R → R là (σ, L)-close với e^y nếu |e^y - f(y)| ≤ σ với bất kỳ y ∈ [-L, L].

Lemma 8. Với bất kỳ σ ∈ (0, 1/10) và L > 0. Nếu D ≥ 10(L + log(1/σ)) thì hàm f_D(y) là (σ, L)-close với e^y.

Chứng minh. Nhớ lại định nghĩa của hàm f_D,

e^x = f_D(x) + ∑_{i=D+1}^∞ (x^i)/(i!);

Đủ để chỉ ra rằng |e^y - f_D(y)| < σ nếu chúng ta có

(x^{D+1})/((D+1)!) ≤ σ/2;

Chúng ta có thể chỉ ra rằng

(y^D)/(D!) ≤ (L^D)/(D!) ≤ (L^D)/((D/4)^D) = (4L/D)^D ≤ (1/2)^D = σ^{10}

trong đó bước đầu tiên theo từ |y| ≤ L, bước thứ hai theo từ n! ≥ (n/4)^n, bước thứ tư theo từ D ≥ 10L, bước cuối theo từ D ≥ 10log(1/σ) và σ ∈ (0, 1/10).

Chúng tôi cũng sẽ sử dụng thực tế sau:

Lemma 9. Với bất kỳ D = o(log n/log d), chúng ta có

rank(f_D) ≤ n^{o(1)}.

Chứng minh. Chúng ta có thể upper bound rank(f_D(A)) theo nghĩa sau:

rank(f_D(A)) ≤ (rank(A))^D ≤ d^D = 2^{D log d} = 2^{o(log n)} = n^{o(1)}.

trong đó bước thứ hai theo từ rank(A) ≤ d, bước thứ tư theo từ D = o(log n/log d).

Cuối cùng chúng tôi sẵn sàng chứng minh định lý:

Chứng minh. Ý tưởng cơ bản là: (i) Sử dụng f_k(βA) để có xấp xỉ hạng thấp (ii) Sử dụng exp(βH) để có phần thưa.

Phạm vi nhỏ, tức là β = o(log n/log d).

Xấp xỉ hạng thấp: R = f_k(βA).

Vì mỗi entry của A nằm trong [−1, 1], mỗi entry của βA nằm trong [−β, β]. Nhưng lưu ý rằng trong trường hợp này β = o(log n/log d) = O(log n). Bằng định nghĩa của k, mỗi entry của exp(βA) - f_k(βA) có giá trị tuyệt đối ≤ σ.

Do đó lỗi tổng thể là ≤ σ√n.

Chỉ thưa: Bằng giả thiết, m = Θ(||L||_0) entries trong A là ≈ 0, chính xác là các entries trong exp(βA) là ≈ 1. Do đó bất kỳ xấp xỉ thưa nào (nói) m/2 có lỗi ≥ √(m/2) = Ω(√||L||_0). Bằng giả thiết của chúng tôi, ||L||_0 = Θ(n²).

Phạm vi trung, tức là Θ((1-ε)/2) log n ≤ β ≤ O(ε log n) và β = O(log n).

Chỉ thưa: lập luận tương tự như trong low range.

Thưa + hạng thấp: Phần hạng thấp R = f_st(βA). Bằng Lemma 9, điều này có rank n^{o(1)}, nên nó có n^{1+o(1)} tham số.

Phần thưa là S = e^{βH} - R|_{supp(H)}. Rõ ràng điều này cần |supp(H)| tham số.

Cho E = M - (S + R). Khi đó (i) trong supp(H), E là tất cả 0. (ii) ngoài supp(H), bằng định nghĩa, các entries của A nằm trong [−σ, σ], trong phạm vi hiện tại của β là [−O(σ log n), O(σ log n)]. Do đó tất cả các entries của E có giá trị tuyệt đối ≤ σ. Bằng định nghĩa của k, chúng ta có ||E||_F ≤ σ√n.

Chỉ hạng thấp: Cho R̃ là rank r ≤ n^{o(1)-1} xấp xỉ M. Khi đó sử dụng lập luận tương tự như lower bound hiện tại của chúng tôi, chúng ta có ||E||_F + ||M - R̃||_F ≥ ||RES||_F (điều này có nghĩa là lỗi ||E||_F + ||M - R̃||_F).

Bây giờ lưu ý rằng S = e^{βH} - (f_k(βA))|_{supp(H)} là ma trận đối xứng, block diagonal với r = Θ(√n) blocks. Corollary 7 ngụ ý rằng λ_r(S) ít nhất là giá trị non-diagonal nhỏ nhất trong S. Bây giờ giá trị non-diagonal nhỏ nhất trong e^{βH} là ≈ e^{β/(12σ) log n} = n^{β/(12σ)}. Mặt khác, giá trị lớn nhất trong (f_k(βA))|_{supp(H)} là

(k^k)/(k!) e^{βk} ≤ (k^k)/(k!) e^{βk-1} ≤ (k^k)/(k!) e^{β log n} ≤ (k^k)/(k!) e^{O(β log n log^{-1}σ)} ≤ (k^k)/(k!) e^{O(β log n)} ≤ (k^k)/(k!) n^{O(β)} = n^{o(1)}.

Do đó λ_r(S) là Θ(n^{β/(12σ)}). Claim sau đó theo vì ||E||_F ≤ σ√n và rank R̃ ≤ r - 1 (Eckart-Young-Mirsky theorem).

Phạm vi lớn, tức là β = Ω(log n).

Chỉ thưa: S = e^{βH}. Lưu ý rằng mỗi entry trong E = M - S được upper bound bởi ≤ e^{βσ} e^{-o(β log d)}. Khi đó ||E||_F ≤ √n e^{βσ} e^{-o(β log d)} ≤ √n e^{βσ} e^{-o(β) + o(β log d)} ≤ √n e^{βσ} e^{-o(β)} = √n e^{βσ} e^{-β/l}.

Chỉ hạng thấp: vì ||E||_F là e^{-β/l}, đủ để lập luận rằng bất kỳ xấp xỉ rank r nào cho S có lỗi ≥ e^{-β/l}. Nhưng điều sau theo vì λ_r(S) ≥ e^{-β/l}. Điều này là vì e^{βH} đối xứng và mỗi entry trong H là ≥ 1/l. Khi đó chúng ta có thể sử dụng Corollary 7. Eckart-Young-Mirsky sau đó hoàn thành chứng minh.

--- TRANG 22 ---
D.3 Scatterbrain: Phân tích

Ở đây chúng tôi chứng minh Định lý 2, chỉ ra rằng xấp xỉ Scatterbrain không thiên lệch và phân tích phương sai của nó. Chúng tôi phát biểu lại định lý ở đây để thuận tiện cho người đọc.

Định lý. Định nghĩa ψ(q,k) = exp(q^T k), ψ̂_{pfe} là ước lượng của Performer và ψ̂_{sb} là ước lượng của Scatterbrain. Ký hiệu S^{d-1} ⊂ R^d là hình cầu đơn vị. Giả sử q,k ∈ S^{d-1} sao cho ||q - k|| < σ. Khi đó:

E[ψ̂_{sb}(q,k)] = ψ(q,k); Var[ψ̂_{sb}(q,k)] = (1-p)Var[ψ̂_{pfe}(q,k)] < Var[ψ̂_{pfe}(q,k)]

trong đó p = exp(-σ²/(4σ²) - 2ln d - O(ln ln d)).

Chứng minh. Cho A_{ij} = exp(q_i^T k_j) là entry ij của ma trận attention chưa chuẩn hóa, A^{lr}_{ij} = φ(q_i)^T φ(k_j) entry của xấp xỉ hạng thấp (Performer), và cho A^{sb}_{ij} là entry của xấp xỉ Scatterbrain (thưa + hạng thấp). Bằng construction của ma trận attention Scatterbrain (Phương trình (1)), nếu ij ∈ Ω, trong đó Ω là tập hợp các indices được chọn bởi LSH, thì:

A^{sb}_{ij} = (Q̃K̃^T + S)_{ij} = φ(q_i)^T φ(k_j) + exp(q_i^T k_j) - φ(q_i)^T φ(k_j) = exp(q_i^T k_j).

Nếu ij ∉ Ω, thì

A^{sb}_{ij} = (Q̃K̃^T + S)_{ij} = φ(q_i)^T φ(k_j) + 0 = φ(q_i)^T φ(k_j).

Nói cách khác, A^{sb} khớp A trên các indices trong Ω, và khớp A^{lr} trên các indices không trong Ω.

Để chỉ ra rằng A^{sb} là ước lượng không thiên lệch của A, chúng tôi đơn giản sử dụng thực tế rằng A^{lr} cũng là ước lượng không thiên lệch của A [17, Lemma 1]:

E[A^{sb}_{ij}] = P(ij ∈ Ω)E[A_{ij}|ij ∈ Ω] + P(ij ∉ Ω)E[A^{lr}_{ij}|ij ∉ Ω]
                = P(ij ∈ Ω)A_{ij} + P(ij ∉ Ω)A_{ij}
                = A_{ij}.

Nói cách khác, E[ψ̂_{sb}(q,k)] = ψ(q,k).

Bây giờ chúng tôi phân tích phương sai per-entry của A^{sb}. Vì A^{sb} là ước lượng không thiên lệch của A, bằng law of total variance,

Var(A^{sb}_{ij}) = P(ij ∈ Ω)Var(A_{ij}|ij ∈ Ω) + P(ij ∉ Ω)Var(A^{lr}_{ij}|ij ∉ Ω)
                  = P(ij ∈ Ω) · 0 + P(ij ∉ Ω)Var(A^{lr}_{ij})
                  = P(ij ∉ Ω)Var(A^{lr}_{ij}).

Để tính xác suất rằng index ij không trong Ω (tức là, không được chọn bởi LSH), chúng tôi sử dụng bound tiêu chuẩn trên cross-polytope LSH [3, Theorem 1]:

p := P(ij ∈ Ω) = exp(-σ²/(4σ²) - 2ln d - O(ln ln d)).

Do đó,

Var(A^{sb}_{ij}) = (1-p)Var(A^{lr}_{ij}) < Var(A^{lr}_{ij}).

Nói cách khác, Var[ψ̂_{sb}(q,k)] = (1-p)Var[ψ̂_{pfe}(q,k)] < Var[ψ̂_{pfe}(q,k)].

Cụ thể hơn, bằng cách plug in phương sai của A^{lr} [17, Lemma 2], chúng ta có

Var(A^{sb}_{ij}) = (1-p) · (1/m)exp(||q_i + k_j||²) (exp(2q_i^T k_j) - 1 - exp(||q_i + k_j||²)),

trong đó p = exp(-σ²/(4σ²) - 2ln d - O(ln ln d))

--- TRANG 23 ---
E Thí nghiệm và Chi tiết Bổ sung

E.1 Tập dữ liệu

ImageNet [25]: ImageNet là một trong những benchmark phân loại hình ảnh được sử dụng rộng rãi nhất. Trong các thí nghiệm của chúng tôi trong Phần 5.1 về đánh giá độ chính xác xấp xỉ của Scatterbrain, cả BigGAN và Vision Transformer đều được đào tạo trước trên tập dữ liệu này. Nó có khoảng 1.2 triệu hình ảnh đào tạo và 50,000 hình ảnh validation.

WikiText103 [45] và Copy [36]: WikiText103 là một tập dữ liệu phổ biến cho các mô hình tự hồi quy. Nó từ một tập hợp hơn 100 triệu tokens được trích xuất từ tập hợp các bài viết được xác minh tốt và nổi bật trên Wikipedia. Nó có 28,475 bài viết đào tạo, 60 cho validation và 60 cho testing.

Copy là một tác vụ tổng hợp nhân bản chuỗi trong đó đầu vào có dạng 0w0w và w ∈ {0,...,N}. Nó trước đây được sử dụng trong [15,36]. Tác vụ này hữu ích để chứng minh tính hiệu quả của attention long range: nó yêu cầu lookups attention không cục bộ. Nó không thể được giải quyết bởi bất kỳ mô hình nào dựa trên sparse attention với phạm vi hạn chế như local attention.

Long Range Arena (LRA) [57]: Đây là một benchmark gần đây để đánh giá các transformer hiệu quả với chuỗi đầu vào dài. Chúng tôi đã sử dụng ListOps [46], phân loại văn bản đánh giá IMDb ở mức byte [44], truy xuất tài liệu ở mức byte [48], phân loại hình ảnh trên chuỗi pixel [37] và Pathfinder [40]. Chúng tôi tuân theo cùng cơ chế đánh giá từ [57] nhưng triển khai phiên bản riêng của chúng tôi trong Pytorch (như data loader).

GLUE [64]: GLUE là một benchmark đa tác vụ tiêu chuẩn trong NLP. Nó có các tác vụ single-sentence, CoLA và SST-2; tác vụ similarity và paraphrase, MRPC, STS-B, QQP; và tác vụ inference, MNLI, QNLI, RTE và WNLI. Đối với các thí nghiệm bổ sung của chúng tôi dưới đây (không đủ chỗ để bao gồm trong bài báo chính), chúng tôi tuân theo truyền thống từ [22, 26, 68] và truncate tất cả các chuỗi đầu vào thành 128 tokens.

E.2 Thiết lập

BigGAN: Chúng tôi điều chỉnh cùng mô hình BigGAN đã được đào tạo trước từ [22] mà không có đào tạo bổ sung. Mô hình có một lớp attention duy nhất ở độ phân giải 64×64 (4096). Tương tự như công trình trước đó, chúng tôi cũng thay thế lớp attention đầy đủ của nó bằng Scatterbrain ở cùng độ phân giải. Hình 5 trong bài báo chính cho thấy so sánh best-effort với [1/32, 1/16, 1/8, 1/4, 1/2] ngân sách tham số. Ví dụ, nếu được cho ngân sách tham số 1/2, chúng tôi báo cáo hiệu suất tốt nhất của Smyrf từ lựa chọn 32/64/128 hash round 64/32/16 cluster size.

T2-ViT: Chúng tôi sử dụng mô hình vision transformer T2T-ViT-14 đã được đào tạo trước từ [69] với kích thước hình ảnh 224×224. Mà không có bất kỳ đào tạo bổ sung nào, chúng tôi chỉ thay thế lớp attention bằng Scatterbrain và các baseline khác và đánh giá sai số xấp xỉ và độ chính xác phân loại trên các testings ImageNet. Một lần nữa, chúng tôi báo cáo hiệu suất tốt nhất best-effort của mỗi xấp xỉ với ngân sách tham số nhất định.

Mô hình Tự hồi quy: Chúng tôi tuân theo thiết lập từ repo phổ biến https://github.com/NVIDIA/DeepLearningExamples để đào tạo vanilla Transformer từ đầu trên WikiText103, ngoại trừ việc chunking WikiText103 thành độ dài chuỗi 1024 để mô phỏng các chuỗi đầu vào dài. Mô hình là 16 lớp với 8 head và 512 model dimension. Chúng tôi đào tạo tất cả các mô hình trong 30 epochs và báo cáo Testing Perplexity tốt nhất. Mô hình chúng tôi sử dụng cho tác vụ Copy đơn giản là một transformer 2-layer-4-head và độ dài chuỗi cũng là 1024. Chúng tôi thực hiện 5 lần chạy và báo cáo trung bình. Bảng 4 trình bày kết quả với độ lệch chuẩn.

Mô hình Phân loại: Chúng tôi tuân theo thiết lập mô hình từ [57,67]. Chúng tôi chia sẻ cùng phát hiện với [67] rằng accuracy cho tác vụ Retrieval thực sự cao hơn so với báo cáo trong [57].

Tỷ lệ giữa thành phần Thưa và Hạng thấp: Có một số quy tắc mà chúng tôi đã sử dụng trong các thí nghiệm của chúng tôi để đặt tỷ lệ này. Đối với inference, chúng tôi đặt tỷ lệ này dựa trên entropy của một tập con quan sát được của các ma trận attention trong các lớp khác nhau: chúng tôi phân bổ nhiều bộ nhớ hơn cho thành phần hạng thấp so với thành phần thưa nếu entropy cao. Đối với training, nói chung phân bổ nhiều ngân sách bộ nhớ hơn cho thưa có xu hướng hoạt động tốt hơn, vì vậy trong thí nghiệm, chúng tôi đặt tỷ lệ thành 3:1 (thưa: thành phần hạng thấp) để đơn giản. Hơn nữa, trong công trình tương lai, có thể hữu ích khi làm cho tỷ lệ này thích ứng trong quá trình đào tạo. Ví dụ, trong giai đoạn đầu của đào tạo và các lớp đầu, các ma trận attention thường đều hơn (entropy cao hơn). Do đó, sai số xấp xỉ có thể thậm chí thấp hơn nếu tỷ lệ ưu tiên các thành phần dựa trên hạng thấp. Một phương pháp có thể là giám sát sai số xấp xỉ của các thành phần thưa và hạng thấp so với full attention thường xuyên và điều chỉnh ngân sách bộ nhớ tương ứng. Chúng tôi sẽ thêm cuộc thảo luận trên vào bản thảo cập nhật.

--- TRANG 24 ---
Bảng 4: Hiệu suất của Scatterbrain, reformer, performer và Full-Attention trên các benchmark Long-Range-Arena và 2 tác vụ mô hình hóa ngôn ngữ phổ biến. Chúng tôi cố định cùng số lượng tham số (1/8 của full) được sử dụng để xấp xỉ ma trận attention cho mỗi phương pháp.

[THIS IS TABLE:
Shows performance metrics for different attention mechanisms on various tasks including Copy, WikiText-103, ListOps, Text, Retrieval, Image, and Pathfinder. Contains data for Full Attention, Reformer, Performer, and Scatterbrain with standard deviations.]

Hình 8: Hai biểu đồ trên trình bày Sai số Xấp xỉ so với Entropy của ma trận attention cho reformer, performer và Scatterbrain trên Copy (trái) và WikiText103 (phải). Hai biểu đồ dưới trình bày Sai số Xấp xỉ so với Entropy của ma trận attention cho reformer, performer và Scatterbrain trên Text-IMDb (trái) và Image-Cifar10 (phải). Nhớ lại chúng tôi quan sát thấy rằng entropy của phân phối attention softmax (tức là, tỷ lệ của logits) quyết định các chế độ mà thưa, hạng thấp, hoặc thưa + hạng thấp hoạt động tốt. Scatterbrain mang lại xấp xỉ tốt hơn so với reformer hoặc performer trong hầu hết các trường hợp; performer hoạt động tồi tệ nhất trên các tác vụ mô hình hóa ngôn ngữ trong khi reformer hoạt động tồi tệ nhất trên các tác vụ phân loại. Những biểu đồ này để phân tích sai số xấp xỉ khớp với hiệu suất của chúng trên các tác vụ downstream.

--- TRANG 25 ---
E.3 Nghiên cứu Ablation Khác

E.3.1 Ngân sách Bộ nhớ
Chúng tôi trình bày một nghiên cứu ablation về ngân sách tham số cho tác vụ mô hình hóa ngôn ngữ WikiText-103. Chúng tôi cho thấy rằng Scatterbrain vượt trội hơn các baseline thưa và hạng thấp của nó qua một phạm vi ngân sách tham số. Kết quả được trình bày trong Bảng 5.

Phân tích: Chúng tôi đã quan sát thấy rằng Scatterbrain vượt trội hơn các baseline thưa và hạng thấp của nó dưới các ngân sách bộ nhớ khác nhau. Tương tự như những gì chúng tôi tìm thấy trong Phần 5.2, Performer không đào tạo ổn định ngay cả với 1/4 bộ nhớ attention đầy đủ. Tuy nhiên, dưới framework Scatterbrain, Performer có thể được kết hợp với Reformer một cách thanh lịch để đạt cùng độ chính xác trong khi chỉ sử dụng một nửa bộ nhớ và nhanh hơn Reformer bằng cách khai thác cấu trúc thưa+hạng thấp trong ma trận attention.

Bảng 5: Chúng tôi chạy WikiText-103 LM với sweep của 1/4, 1/8, 1/16 ngân sách bộ nhớ. Chúng tôi cho thấy validation perplexity và speed-up so với full attention với các lớp Attention hiệu quả khác nhau.

[THIS IS TABLE:
Shows memory budgets (1/4, 1/8, 1/16) with Perplexity and Speed-up metrics for Smyrf, Performer, and Scatterbrain]

E.3.2 Các baseline Thưa và Hạng thấp Khác nhau
Scatterbrain đủ tổng quát để chứa các loại xấp xỉ thưa và hạng thấp khác nhau như các thành phần phụ của nó. Cụ thể, chúng ta có thể kết hợp Local attention hoặc block sparse (từ Sparse Transformer và BigBird) + Performer (thay vì Reformer + Performer) theo cách tương tự. Support của ma trận thưa S do đó sẽ được cố định và không thích ứng với đầu vào, nhưng tất cả các bước khác hoàn toàn giống nhau.

Chúng tôi đã chạy thí nghiệm bổ sung trên tổ hợp Local attention + Performer và BigBird. Nhớ lại rằng trong Phụ lục E, chúng tôi đã cho thấy Scatterbrain có thể giảm bộ nhớ attention của Vision Transformer 98% với chi phí chỉ 0.8% giảm độ chính xác khi dùng như thay thế drop-in cho full attention mà không cần đào tạo trên ImageNet. Chúng tôi cho thấy kết quả cho biến thể local+performer với cùng ngân sách bộ nhớ trong Bảng 6.

Chúng tôi cũng đã chạy thí nghiệm bổ sung trên Local attention trên tác vụ mô hình hóa ngôn ngữ Copy và Wikitext-103 (Bảng 7). Chúng ta thấy rằng Local attention có khả năng cạnh tranh hợp lý trên Wikitext-103 nhưng không hoạt động tốt trên Copy. Kết quả không ngạc nhiên như được ghi nhận trong bài báo Reformer rằng Copy yêu cầu lookups attention không cục bộ.

E.3.3 Các baseline Thưa và Hạng thấp Khác nhau

E.4 Phân tích
Nhớ lại trong Phần 5, chúng tôi đã báo cáo phân tích sau khi trực quan hóa sai số của reformer (thưa), performer (hạng thấp), và Scatterbrain (thưa + hạng thấp) với cùng số lượng tham số khi xấp xỉ các ma trận attention đầy đủ cho mỗi lớp attention trong quá trình đào tạo. Trong Hình 8, chúng tôi trình bày trực quan hóa.

Bảng 6: Độ chính xác Top-1 của T2T Vision Transformer đã được đào tạo trước trên ImageNet với các thay thế attention khác nhau. Sai số đại diện cho sai số xấp xỉ chuẩn hóa trung bình so với full attention.

[THIS IS TABLE:
Shows Top-1 accuracy for different attention mechanisms including Full Attention, SMYRF, Local, Performer, BigBird, and Scatterbrain variants]

--- TRANG 26 ---
Bảng 7: Thí nghiệm bổ sung cho Local attention trên tác vụ mô hình hóa ngôn ngữ Copy và Wikitext-103.

[THIS IS TABLE:
Shows performance metrics for different attention mechanisms (Full Attention, Reformer, Performer, Local, Scatterbrain) on Copy and WikiText-103 tasks, with perplexity values]

Kết luận cho các tác vụ mô hình hóa ngôn ngữ là thưa+hạng thấp có sai số xấp xỉ nhỏ nhất trong hầu hết các trường hợp, và thưa có sai số lớn nhất, điều này khớp với kết quả đầu-cuối. Nó cũng xác nhận quan sát trong bài báo benchmark phổ biến [57] rằng các xấp xỉ dựa trên kernel hoặc hạng thấp kém hiệu quả hơn đối với dữ liệu có cấu trúc phân cấp. Đối với các tác vụ phân loại, chúng tôi lại thấy rằng Scatterbrain có sai số xấp xỉ nhỏ nhất, trong khi performer tồi tệ nhất trên ListOps và reformer có sai số lớn nhất trên các tác vụ phân loại, điều này khớp với kết quả đầu-cuối và xác nhận các quan sát trước đó của chúng tôi (xấp xỉ thưa và hạng thấp vượt trội trong các chế độ khác nhau).

E.5 Thí nghiệm Bổ sung về Tinh chỉnh Bert trên GLUE
Chúng tôi cung cấp thí nghiệm bổ sung về tinh chỉnh Bert trên GLUE trong Bảng 8. Chúng tôi tuân theo thiết lập tương tự như [22]. Chúng tôi thay thế tất cả các lớp attention trong mô hình Bert base bằng Scatterbrain và các baseline khác. Sau đó chúng tôi tinh chỉnh Bert trên 9 tác vụ downstream trong 3 epochs với batch size 32 và learning rate 3e-5. Ngân sách tham số là 1/2 của full attention vì độ dài chuỗi 128 không quá dài. Chúng ta có thể thấy Scatterbrain vượt trội hơn tất cả các baseline khác trong hầu hết các tác vụ downstream.

Bảng 8: Kết quả của GLUE khi thay thế ma trận attention dày đặc bằng smyrf, performer và Scatterbrain trong mô hình BERT base. Chúng tôi cố định cùng số lượng tham số (1/2 của full) được sử dụng để xấp xỉ ma trận attention cho mỗi phương pháp.

[THIS IS TABLE:
Shows GLUE benchmark results across multiple tasks (CoLA, SST-2, MRPC, etc.) comparing Full, Smyrf, Performer, and Scatterbrain methods with various metrics]

F Thảo luận và Công trình Tương lai Khác
Trong bài báo này, chúng tôi trình bày Scatterbrain, thống nhất điểm mạnh của xấp xỉ thưa và hạng thấp. Nó được truyền cảm hứng từ các quan sát về cấu trúc ma trận attention được tạo ra bởi dữ liệu và hàm softmax cũng như thuật toán robust-PCA cổ điển. Trong triển khai và phân tích của chúng tôi, chúng tôi có reformer/Smyrf và performer như backbone cho các xấp xỉ thưa và hạng thấp vì các tính chất của chúng, ví dụ Performer không thiên lệch. Scatterbrain về cơ bản là một framework để kết hợp điểm mạnh của các biến thể thưa và hạng thấp, vì vậy nó có thể dễ dàng được mở rộng cho các biến thể khác, như Routing Transformer [53] hoặc Nystromformer [67]. Hơn nữa, các quan sát của chúng tôi về mối liên hệ giữa entropy và sai số xấp xỉ hạng thấp/thưa cũng cung cấp cơ hội để phát hiện hiệu quả phương pháp xấp xỉ hoặc nén để chọn cho các kiến trúc hoặc benchmark khác nhau.
