# 2406.10785.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2406.10785.pdf
# File size: 7870253 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ShareLoRA: Parameter Efficient and Robust Large Language Model
Fine-tuning via Shared Low-Rank Adaptation
Yurun Song
UC Irvine
yuruns@uci.eduJunchen Zhao
UC Irvine
junchez3@uci.edu
Ian G. Harris
UC Irvine
harris@ics.uci.eduSangeetha Abdu Jyothi
UC Irvine, VMware Research
sangeetha.aj@uci.edu
Abstract
In this paper, we introduce Share dLowRank
Adaptation (ShareLoRA), a Large Language
Model (LLM) fine-tuning technique that bal-
ances parameter efficiency, adaptability, and
robustness without compromising performance.
By strategically sharing the low-rank weight
matrices across different layers, ShareLoRA
achieves 44% to 96% reduction in trainable
parameters compared to standard LoRA, along-
side a substantial decrease in memory overhead.
This efficiency gain scales with model size,
making ShareLoRA particularly advantageous
for resource-constrained environments. Impor-
tantly, ShareLoRA not only maintains model
performance but also exhibits robustness in
both classification and generation tasks across
diverse models, including RoBERTa, GPT-2,
and LLaMA series (1, 2, and 3). It consistently
outperforms LoRA in zero-shot, few-shot, and
continual fine-tuning scenarios, achieving up to
1.2% average accuracy improvement, and en-
hanced generalization across domains. In con-
tinual learning settings, ShareLoRA achieves
1.2% higher accuracy on GSM8K, 0.6% on
HumanEval, and 0.5% on both MMLU and
MMLU-Pro. Our results demonstrate that
ShareLoRA supports high-quality fine-tuning
while offering strong generalization and contin-
ual adaptation across various model scales and
diverse tasks.1
1 Introduction
As Pretrained Language Models (PLMs) have
gained prominence (Devlin et al., 2019; Liu et al.,
2019; Radford et al., 2019), researchers are in-
creasingly focused on optimizing the utilization
of these models’ pre-trained weights. Traditional
fine-tuning, which involves adjusting all parame-
ters of a PLM for a specific dataset or task, is often
resource-intensive and time-consuming, especially
1https://github.com/Rain9876/ShareLoRAgiven the massive scale of large language mod-
els (LLMs) (Brown and et.al, 2020; Kaplan et al.,
2020; Hoffmann and et.al, 2022; et.al, 2022; Zhang
et al., 2022; et.al, 2023b).
Parameter-Efficient Fine-Tuning (PEFT) has
proven to be an effective strategy for mitigating
the challenges associated with extensive parame-
ter adjustments. By modifying only a select sub-
set of a model’s parameters, PEFT enables cost-
effective adaptation to domain-specific tasks while
preserving performance levels comparable to those
achieved with full fine-tuning (Houlsby et al., 2019;
Li and Liang, 2021a; Lin et al., 2020; Lei et al.,
2023; He et al., 2022, 2023; Mahabadi et al., 2021).
Techniques like Low-Rank Adaptation (LoRA) (Hu
et al., 2021) stand out within PEFT by demonstrat-
ing that models fine-tuned with a reduced param-
eter set can match the performance of those fine-
tuned with full parameters, effectively bridging the
gap in efficiency and efficacy.
Given the impressive performance of LoRA, sub-
sequent studies have aimed to enhance its effi-
ciency, mainly by reducing the number of trainable
parameters to minimize the memory footprint dur-
ing the fine-tuning process. However, significantly
lowering the trainable parameters can lead to slow
convergence, while insufficient reductions may en-
courage the model to easily overfit. Moreover, ex-
isting PEFT methods often struggle to maintain ro-
bustness across different domains after fine-tuning.
To address these challenges, we introduce
ShareLoRA, an efficient and straightforward PEFT
method that effectively balances trainable param-
eter selection while optimizing the model’s adapt-
ability, minimizing memory requirements, and en-
suring robustness across domains. Our approach
leverages the observation that low-rank weight ma-
trices A and B do not need to be uniquely config-
ured across layers to achieve optimal PEFT per-
formance in PLMs. Instead, we propose sharing
either matrix A or B across all layers while main-arXiv:2406.10785v2  [cs.CL]  18 May 2025

--- PAGE 2 ---
taining its counterpart as distinct in each layer. This
strategy meets several key objectives:
•Parameter Efficiency: Sharing a low-rank ma-
trix across layers reduces trainable parameters by
44% to96% compared to standard LoRA, for
models such as LLaMA-7B. This memory reduc-
tion scales with model size which is critical for
efficient fine-tuning LLMs on consumer GPUs
and edge devices.
•Model Adaptability: Keeping the shared matrix
trainable preserves the model’s adaptability, al-
lowing it to effectively learn and adapt to new
tasks and domains. Also, the updated weights
for each component that LoRA applies remain
unique yet share a common base, promoting con-
sistency across layers while allowing for task-
specific adaptations.
•Continual Adaption: ShareLoRA exhibits ro-
bustness when continual fin-tuning to domains
different from the one it was fine-tuned on. This
generalization capability sets it apart from tradi-
tional LoRA and other PEFT methods, which of-
ten struggle to maintain performance when faced
with out-of-domain tasks.
Our extensive experiments across multiple mod-
els, including RoBERTa, GPT-2, and LLaMA se-
ries, demonstrate that ShareLoRA not only pre-
serves model performance but also shows remark-
able robustness across a variety of tasks in both
classification and generation.
2 Related Works
PLMs are trained on large datasets to develop broad
linguistic representations (Devlin et al., 2019; Liu
et al., 2019; Raffel et al., 2020), but often fall
short in specialized tasks due to a lack of domain
knowledge. Traditional approaches involve fully
fine-tuning PLMs to enhance domain-specific per-
formance (Xu and Wang, 2023; Xie et al., 2020;
Dabre et al., 2019). However, with the increas-
ing size of PLMs (Workshop et al., 2023; et.al,
2023b,a; Zhang et al., 2022), this method becomes
too resource-heavy. As an alternative, Parameter
Efficient Fine-tuning (PEFT) provides an efficient
way to maintain performance with less computa-
tional expense.
PEFT methods have become crucial for adapt-
ing large-scale pre-trained models to specific tasks
without extensively overhauling their parameters.
This approach conserves computational resources
and boosts efficiency. For example, Prefix tun-ing (Li and Liang, 2021a) adds parameters to the
hidden states across layers, subtly influencing the
model’s behavior without changing its underlying
architecture. Prompt tuning (Lester et al., 2021)
alters prompts and updates only the associated pa-
rameters, focusing on specific areas of model per-
formance. BitFit (Zaken et al., 2022) updates only
the biases within the model, resulting in minimal
yet effective modifications.
One notable PEFT technique is Low-Rank Adap-
tation (LoRA) (Hu et al., 2021), which achieves
efficient fine-tuning by incorporating a low-rank
matrix adaptation mechanism alongside the exist-
ing weights of linear layers. This approach reduces
memory overhead while preserving the effective-
ness of the fine-tuning process.
Recent enhancements to LoRA have signif-
icantly broadened its capabilities. QLoRA
(Dettmers et al., 2023) optimizes LoRA for the
fine-tuning of quantized models, thereby increas-
ing efficiency. ReLoRA (Lialin et al., 2023) incor-
porates a warm-up strategy during pre-training to
boost adaptability. LoraHub (Huang et al., 2024)
streamlines the process by automating the creation
of custom LoRA modules for specific tasks. Addi-
tionally, GLoRA (Chavan et al., 2023) introduces
a prompt module that fine-tunes weights and bi-
ases, enhancing performance across a variety of
applications.
Despite these advancements, LoRA still faces
significant memory overhead due to high activa-
tion memory usage in LoRA layers during the
fine-tuning phase. To address this issue, LoRA-
FA (Zhang et al., 2023) strategically freezes the
low-rank Amatrix and updates only the Bmatrix.
This approach significantly reduces the number of
trainable parameters and activation memory, thus
enhancing the efficiency of fine-tuning large lan-
guage models without substantially impacting per-
formance.
However, LoRA-FA does not adequately de-
crease the total number of parameters that need
to be stored, presenting a considerable challenge in
contexts where computational resources and stor-
age are constrained. Additionally, by freezing the
Amatrix, LoRA-FA limits the model’s capacity to
adapt and learn from new data during fine-tuning.
This rigidity can hinder the model’s performance,
particularly in complex or domain-specific tasks.
In contrast, our proposed approach ShareLoRA
offers a more dynamic and flexible strategy by al-
lowing either matrix AorB, or both, to be shared

--- PAGE 3 ---
ABnWn
AB1W1
AB0W0
ShareAAnBWn
A1BW1
A0BW0
ShareBABWn
ABW1
ABW0
ShareABShared AB
W
Shared AB
W
Shared AB
W
Query Key ValueFrozen
Trainable
Trainable & SharingFigure 1: Overview of ShareLoRA: The implementation of ShareA, ShareB, and ShareAB across all layers (left),
including ShareA applied across self-attention layers (right).
across different layers. This method not only pre-
serves the model’s adaptability but also further re-
duces the memory requirements.
3 Method
In this section, we provide a detailed description of
our proposed PEFT approach ShareLoRA, as illus-
trated in Figure 1. ShareLoRA facilitates flexible
configurations through two primary dimensions: 1.
the choice of sharing between the matrices A, B,
or both A and B (ShareA, ShareB, and ShareAB),
and2.the scope of sharing, which can be across
different layers such as self-attention layers. This
framework allows for a variety of combinations,
enabling tailored adaptation of low-rank models to
specific tasks.
ShareA Configuration In the ShareA configu-
ration, the low-rank matrix Ais uniformly shared
across all layers, with each layer employing its own
unique matrix Bi. The formula for weight adapta-
tion in each layer ican be expanded to detail the
influence on model transformation:
∆Wi=αAB i=αrX
k=1A:,kBk,:,i (1)
where A:,krepresents the k-th column of A, and
Bk,:,iis the k-th row of matrix Bi. This equation
shows that each layer’s weight change, ∆Wi, is a
linear combination of the columns of Aweighted
by the corresponding elements of Bi. This shared
projection-down matrix Areduces the dimensional-
ity uniformly across all layers, thereby minimizing
redundancy in learning and memory usage while
enabling tailored output transformations through
layer-specific matrices Bi.
ShareB Configuration In the ShareB configura-
tion, matrix Bis uniformly shared across all layers,while each layer employs its own unique matrix Ai.
The weight adjustment for each layer is expressed
as:
∆Wi=αAiB=αrX
k=1Ai,:,kBk,: (2)
where Ai,:,kdenotes the k-th column of matrix Ai
for layer i, and Bk,:represents the k-th row of the
shared matrix B. Here, the uniform projection-up
matrix Bensures consistent expansion of the trans-
formed data back to the output dimension across
all layers, while the distinct Aimatrices allow for
adaptation to the specific input characteristics of
each layer.
ShareAB Configuration When both matrices A
andBare shared across all layers, the change in
weights is simplified, leading to substantial param-
eter reduction:
∆W=αAB =αrX
k=1A:,kBk,: (3)
where both A:,kandBk,:are shared across all lay-
ers. This configuration significantly reduces the
model complexity by eliminating the need for dis-
tinct matrices in each layer, thus reducing memory
requirements and computational overhead. The en-
tire model operates under a uniform transformation
schema, which simplifies training and storage but
requires careful calibration of the initial values and
ongoing adjustments during fine-tuning to preserve
model effectiveness across diverse tasks.
Sharing Across Self-Attention Layers In the
ShareA configuration of ShareLoRA applied to
PLMs across all self-attention layers, the matrices
AQ,AK, and AVare shared. These matrices are
responsible for reducing the dimensionality of the
inputs for Queries (Q), Keys (K), and Values (V)

--- PAGE 4 ---
Method # Params MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg.
Rb(FT)* 125.0M 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4
Rb(BitFit)* 0.1M 84.7 93.7 92.7 62.0 91.8 84.0 81.5 90.8 85.2
Rb(AdptD)* 0.3M 87.1±.094.2±.188.5±1.160.8±.493.1±.190.2±0071.5±2.789.7±.384.4
Rb(AdptD)* 0.9M 87.3±.194.7±.388.4±.1 62.6±.993.0±.290.6±.075.9±2.290.3±.185.4
Rb(Prefix)* 0.36M 85.21 93 .81 87 .25 59 .31 90 .77 87 .75 54 .51 88 .48 80.9
Rb(IA3)* 0.06M 83.95 93 .92 87 .00 59 .58 90 .88 87 .99 71 .12 90 .30 83.1
Rb(LoRA)* 0.3M 87.5±.395.1±.289.7±.763.4±1.293.3±.390.8±.186.6±.791.5±.287.2
Rb(L-FA)* 0.15M 86.8 94 .8 90 63 .6 92 .5 90 .1 67 .9 89 .6 84.4
Rb(VERA)* 0.04M — 94.6±.189.5±.5 65.6±.891.8±.2 — 78.7±.790.7±.285.2
Rb(Tied-LoRA)* 0.04M — 94.4±.588.5±1.061.9±1.692.2±.2 — 76.2±1.089.8±.383.8
Rb(VB-LoRA)* 0.03M — 94.4±.289.5±.5 63.3±.792.2±.2 — 82.3±1.390.8±.185.4
Rb(ShareA) 0.16M 87.3±.295.0±.389.9±.863.8±1.192.8±.1890.3±.0587.1±.591.4±.187.2
Rl(FT)* 335.0M 90.2 96.4 90.9 68 .0 94 .7 92.2 86.6 92 .4 88.9
Rl(LoRA)* 0.8M 90.6±.296.2±.590.9±1.268.2±1.994.9±.391.6±.187.4±1.192.6±.289.0
Rl(L-FA)* 0.4M 90.1 96 90 68 94 .4 91 .1 86 .1 92 88.5
Rl(VeRA)* 0.06M — 96.1±0.190.9±0.768.0±0.894.4±0.2 — 85.9±0.791.7±0.887.8
Rl(Tied-LoRA)* 0.07M — 94.8±0.689.7±1.064.7±1.294.1±0.1 — 81.2±0.190.8±0.385.9
Rl(VB-LoRA)* 0.03M — 96.1±0.291.4±0.668.3±0.794.7±0.5 — 86.6±1.391.8±0.188.2
Rl(ShareA) 0.4M 90.7±.196.1±.191.1±.867.7±1.595.1±.191.3±.190.3±.392.5±.189.3
Rl(Prefix)* 0.9M 89.30 95 .76 88 .24 59 .01 93 .32 88 .88 74 .01 90 .92 84.9
Rl(IA3)* 0.18M 88.63 94 .61 86 .52 61 .15 94 .25 89 .45 81 .23 92 .22 86.0
Rl(LoRA) † 0.8M 90.6±.296.2±.590.2±1.068.2±1.994.8±.391.6±.285.2±1.192.3±.588.6
Rl(ShareAB) † 0.03M 90.2±.195.9±.389.7±1.062.3±.994.6±.189.7±.183.0±0.890.3±.287.0
Rl(ShareB) † 0.4M 90.4±.196.0±.390.4±.4 65.8±.894.6±.191.0±.184.1±1.291.4±.288.0
Rl(ShareA) † 0.4M 90.7±.196.1±.190.0±.567.7±1.595.0±.191.3±.185.9±.891.8±.288.6
Table 1: RoBERTa baseand RoBERTa large with different adaptation methods on the GLUE benchmark. ∗indicates
numbers published in prior works. †indicates runs configured in a setup similar to (Houlsby et al., 2019) and (Hu
et al., 2021) for a fair comparison.
respectively, we term it as ShareA qkvin the follow-
ing paragraphs. The process for each component
in the i-th self-attention layer is formalized as fol-
lows:
Qi=XiAQBQi (4)
Ki=XiAKBKi (5)
Vi=XiAVBVi (6)
Attention (Qi, Ki, Vi) =softmax 
QiKT
ip
dKi!
Vi,
(7)
where Xidenotes the input to the i-th self-attention
layer. Each matrix AQ,AK, and AVfacilitates a
consistent reduction in input dimensions across all
layers, which simplifies the model architecture by
maintaining a uniform approach to processing the
foundational aspects of self-attention. The unique
matrices BQi,BKi, and BVifor each component
allow for tailored transformations that meet the
specific needs of each self-attention layer.
4 Experiments
In our study, we conduct a comprehensive evalua-
tion of the downstream performance of ShareLoRA
across several series models, including RoBERTa
(Liu et al., 2019) and GPT-2 (Radford et al., 2019).We benchmark these results against other estab-
lished approaches such as LoRA (Hu et al., 2021),
LoRA-FA (Zhang et al., 2023). Additionally, we
extend the application of ShareLoRA to large-scale
model in LLaMA series (et.al, 2023b, et.al, 2023a,
Dubey et al., 2024) architectures, particularly in
few-shot, zero-shot scenarios. Furthermore, our
experiments cover a range of model sizes, from 7
billion to 13 billion parameters, and included both
quantized and unquantized model variants. All
tests were performed on the Nvidia A6000 and
RTX 3090 GPUs. For experiment hyper-parameter
settings, see Appendix Table 8-Table 11.
4.1 Datasets
The experiment datasets are primarily divided into
three categories: Natural Language Understanding
(NLU), Natural Language Generation (NLG) and
few-shot tasks, using the same configuration and
datasets as LoRA (Hu et al., 2021) and (Dettmers
et al., 2023).
For NLU, we employ the GLUE benchmark
(Wang et al., 2019), which includes MNLI, SST-2,
MRPC, CoLA, QNLI, QQP, RTE, and STS-B tasks.
Notably, for MRPC, RTE, and STS-B tasks, we ini-
tialize the LoRA modules with the trained MNLI
checkpoint as (Hu et al., 2021) demonstrated. For

--- PAGE 5 ---
Method # Params BLUE NIST MET ROUGE-L CIDEr
GPT-2 M (FT)* 354.92M 68.2 8.62 46.2 71.0 2.47
GPT-2 M (AdapterL)* 0.37M 66.3 8.41 45.0 69.8 2.40
GPT-2 M (AdapterL)* 11.09M 68.9 8.71 46.1 71.3 2.47
GPT-2 M (PreLayer)* 0.35M 69.7 8.81 46.1 71.4 2.49
GPT-2 M (VeRA)* 0.10M 70.1 8 .81 46.6 71 .5 2 .50
GPT-2 M (LoRA) 0.35M 69.5±.78.74±.0846.56±.2 71.51±.3 2.50±.01
GPT-2 M (ShareB) 0.20M 67.1±.78.55±.0945.12±.4 69.45±.6 2.37±.01
GPT-2 M (ShareA) 0.20M 69.7±.48.75±.0546.60±.171.63±.12.51±.01
GPT-2 L (FT)* 774.03M 68.5 8.78 46.0 69.9 2.45
GPT-2 L (AdapterL)* 0.88M 69.1 8.68 46.3 71.4 2.49
GPT-2 L (AdapterL)* 23.00M 68.9 8.70 46.1 71.3 2.45
GPT-2 L (PreLayer)* 0.77M 70.3 8.85 46.3 71.7 2.47
GPT-2 L (VERA)* 0.17M 70.3 8.85 46.9 71.6 2 .52
GPT-2 L (LoRA) 0.77M 69.8±.48.80±.0446.69±.1 71.71±.32.52±.01
GPT-2 L (ShareB) 0.39M 69.7±.28.80±.0146.17±.3 70.94±.5 2.49±.02
GPT-2 L (ShareA) 0.39M 70.0±.18.83±.0346.60±.171.74±.12.52±.02
Table 2: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG Challenge. For all
metrics, higher is better. LoRA ShareA outperforms several baselines with comparable or fewer trainable parameters.
* indicates numbers published in prior works.
Method # Params MMLU Method # Params MMLU
LLaMA 7B * 6738.4M 35.1 LLaMA 13B * 13015M 46.9
LLaMA 7B (LoRA)* 159.9M 40.67 LLaMA 13B (LoRA)* 250.3M 47.49
LLaMA 7B (LoRA) 159.9M 41.65±1.0LLaMA 13B (LoRA) 250.3M 47.60±1.4
LLaMA 7B (ShareA qkv) 135.5M 41.01±0.8 LLaMA 13B (ShareA qkv) 212.0M 48.76±0.7
LLaMA 7B (ShareA) 89.3M 40.93±0.5 LLaMA 13B (ShareA) 139.1M 48.15±0.5
LLaMA2 7B * 6898.3M 45.7 LLaMA2 13B * 13266M 53.8
LLaMA2 7B (LoRA) 159.9M 47.47±1.1 LLaMA2 13B (LoRA) 250.3M 55.31±0.2
LLaMA2 7B (ShareA qkv) 135.5M 47.88±0.1 LLaMA2 13B (ShareA qkv) 212.0M 55.66±0.1
LLaMA2 7B (ShareA) 89.3M 48.19±0.4LLaMA2 13B (ShareA) 139.1M 55.53±0.3
Table 3: LLaMA and LLaMA2, ranging from 7B to 13B, are fine-tuned using different sharing approaches on
the Alpaca datasets and evaluated on the MMLU 5 shot benchmark. The configuration runs is based on the setup
described in (Dettmers et al., 2023).* indicates numbers published in prior works, reported by (Xu et al., 2023).
NLG, we replicate experiments similar to those of
LoRA using the E2E challenge dataset (Novikova
et al., 2017), following the same experimental
setup.
Additionally, we expand our experiments to few-
shot and zero-shot tasks on larger models, demon-
strating our approach’s adaptability. Following the
configuration outlined in (Dettmers et al., 2023),
we employ Alpaca (Taori et al., 2023), CodeAl-
paca(Chaudhary, 2023) and MATH (Hendrycks
et al., 2021b) for LoRA and ShareLoRA, us-
ing the MMLU benchmark (Hendrycks et al.,
2021a) for evaluation. Some other benchmarks
like ARC (Chollet, 2019), Hellaswrag (Zellers
et al., 2019), MMLU-Pro (Wang et al., 2024), Hu-
manEval(Chen et al., 2021) and GSM8K (Cobbe
et al., 2021) are used for comparison of model
adaptability. All experimental setups are consistent
with those described studies and demonstration of
their repositories, based on the best of our knowl-edge.
4.2 Baselines
Full Fine-Tuning (FT) is a commonly used
approach for model adaptation involving with
updating all model’s parameters.
LoRA (Hu et al., 2021) is a technique that
introduces a pair of rank decomposition trainable
matrices alongside existing weight matrices in
neural networks.
Bitfit (Zaken et al., 2022) is a technique for
updating only a select small subset of biases
parameters, to improve performance on new tasks
while freezing all other pre-trained weights.
PreLayer/Prefix (Li and Liang, 2021b) is a
parameter-efficient technique for customizing large
language models by learning specific activations
after each Transformer layer for designated prefix
tokens, while the main model parameters remain
unchanged.

--- PAGE 6 ---
Adapter (Houlsby et al., 2019) involves inserting
adapter layers between neural modules such as
the self-attention and MLP modules, enhancing
model flexibility without extensive modifications.
AdapterL (Lin et al., 2020) introduces adapters
after the MLP module followed by a LayerNorm,
while AdapterD (Rücklé et al., 2021) increases
efficiency by omitting some adapter layers.
IA3(Liu et al., 2022) is a PEFT approach that
enhances model performance by scaling activations
with learned vectors.
LoRA-FA (Zhang et al., 2023) is a memory-
efficient approach to fine-tuning large language
models by reducing the activation memory
required.
VERA (Kopiczko et al., 2023) reduces trainable
parameters by using frozen random matrices
and learned scaling vectors, matching LoRA’s
performance more efficiently.
Tied-LoRA (Renduchintala et al., 2023) improves
parameter efficiency by tying weights and training
fewer low-rank matrices, matching LoRA perfor-
mance with significantly fewer parameters.
VB-LoRA (Li et al., 2024) achieves extreme
parameter efficiency by generating low-rank
adaptation weights from a shared vector bank
using a differentiable top-k selection.
Method MMLU ARC (c) Hellaswarg GSM8K
LLaMA 7B (LoRA) 41.28 48.49 76.74 2.43
LLaMA 7B (ShareA) 40.67 48.82 76.67 3.16
LLaMA 13B (LoRA) 45.02 51.34 79.46 5.79
LLaMA 13B (ShareA) 46.04 51.19 79.53 6.17
LLaMA2 7B (LoRA) 45.68 49.60 77.14 3.21
LLaMA2 7B (ShareA) 47.09 50.14 76.77 6.06
LLaMA2 13B (LoRA) 53.21 51.28 76.59 12.33
LLaMA2 13B (ShareA) 53.70 52.48 79.43 14.99
Table 4: Performance of LLaMA models trained on
the Alpaca General dataset and tested in a zero-shot of
MMLU, ARC Challenge, and Hellaswarg, and in a five-
shot of GSM8K, using the lm-eval-harness leaderboard
(Gao et al., 2023). This table demonstrates the model’s
cross-domain adaptability in common sense, reasoning,
and mathematics after finetuning on the general dataset.
5 Results
Parameter Efficiency and Performance
ShareLoRA demonstrates significant parameter
efficiency while maintaining or improving perfor-
mance across various model sizes and tasks. For
large-scale LLaMA models, as shown in Table 3,
ShareA reduces trainable parameters by 44% com-pared to LoRA. Despite this substantial reduction,
ShareA achieves comparable or improved MMLU
scores, with LLaMA 13B showing an increase
from 47.60 to 48.15.
On the E2E NLG Challenge in Table 2, ShareA
demonstrates markedly greater efficiency on GPT-
2 models: it reduces LoRA’s parameter count by
43% on the Medium model, yet still achieves per-
formance gains. Specifically, GPT-2 Medium’s
BLEU improves from 69.5 to 69.7 and its ROUGE-
L from 71.51 to 71.63, while GPT-2 Large’s BLEU
increases from 69.8 to 70.0.
Notably, while ShareA consistently outperforms
LoRA, our experiments show that ShareB and
ShareAB generally underperform compared to
ShareA. For instance, in the GPT-2 Large model,
Table 2 shows ShareB achieves a BLEU score of
69.7 and a ROUGE-L score of 70.94, which are
lower than both LoRA and ShareA.
Comparing ShareLoRA to other state-of-the-art
PEFT methods, we observe competitive or superior
performance. For instance, on the GLUE bench-
mark using RoBERTa-large, Table 1 shows ShareA
achieves an average score of 88.6 on GLUE, com-
pared to 84.9 for Prefix-tuning while using sig-
nificantly fewer parameters. Even in its most ag-
gressive configuration, ShareAB, with only 0.03M
trainable parameters which reduces 96% trainable
parameters compared to LoRA, outperforms IA3
which uses 0.18M parameters, achieving an aver-
age score of 87.0 on GLUE compared to IA3’s 86.0.
Furthermore, under a similar trainable parameter
size, ShareA demonstrates better performance than
LoRA-FA. For example, ShareA achieves an av-
erage GLUE score of 89.3 with 0.4M parameters
on RoBERTa-large, surpassing LoRA-FA’s score
of 88.5 with the same parameter count.
Model Adaptability ShareLoRA demonstrates
superior adaptability across a diverse range of tasks
and model sizes. In experiments with RoBERTa-
base model on the GLUE benchmark shown in
Table 1, ShareA exhibits particular strength on
smaller datasets that are typically prone to overfit-
ting. Specifically, on tasks such as MRPC, CoLA,
and RTE, ShareA achieves performance gains of
0.2% to 0.5%. These improvements are especially
noteworthy given that these datasets have gener-
ally reached full convergence under standard train-
ing configurations (Hu et al., 2021), suggesting
ShareLoRA’s ability to extract additional perfor-
mance even in challenging scenarios.

--- PAGE 7 ---
Method MATH MMLU MMLU Pro
LLaMA3 8B (LoRA) 12.46 65.93 31.80
LLaMA3 8B (ShareA) 13.24 66.35 32.55
LLaMA3.1 8B (LoRA) 15.36 65.59 32.86
LLaMA3.1 8B (ShareA) 15.10 65.78 33.69
Table 5: Performance of LLaMA3 trained on the MATH
dataset (Hendrycks et al., 2021b) and evaluated in a zero-
shot of MATH and in five-shot of MMLU and MMLU-
Pro. It highlights the model’s ability to maintain cross-
domain adaptability in common sense and reasoning
domains after finetuning on mathematics.
Figure 2: Memory Consumption of LLaMA3 70B with
QLoRA and QLoRA-shareA (QShareA).
ShareA further showcases enhanced transfer
learning capabilities. When fine-tuning on adap-
tive tasks like MRPC, RTE, and STS-B using the
best MNLI checkpoint, ShareA consistently per-
forms on par with or outperforms LoRA. Notably,
ShareA outperforms other PEFT methods in this
transfer learning scenario as well. For instance, on
the RTE task, ShareA, with 0.16M parameters for
RoBERTa-base, achieves a score of 87.1, signifi-
cantly surpassing Prefix-tuning’s 54.51 as shown
in Table 1. ShareA also demonstrates superior per-
formance when compared to methods with similar
trainable parameter sizes, such as BitFit with 0.1M
parameters and LoRA-FA with 0.15M parameters.
This highlights ShareA’s efficiency in parameter
utilization and its ability to extract better perfor-
mance from a given parameter budget, particularly
in transfer learning scenarios.
Robustness Across Domains ShareLoRA shows
strong robustness and adaptability across both di-
verse task domains and varying model sizes. As
presented in Tables 3 and 4, ShareLoRA con-
sistently surpasses LoRA in zero-shot and few-shot learning scenarios across multiple evaluation
benchmarks.
On the LLaMA2-7B model, ShareLoRA im-
proves MMLU accuracy by 0.7%, while on the
LLaMA2-13B model, it achieves a 0.5% gain. Be-
yond MMLU, ShareLoRA delivers average perfor-
mance gains of 1.8% and 1.3% on LLaMA2-7B
and LLaMA2-14B models, respectively, with ac-
curacy improvements ranging from 0.5% to 2.5%
across various tasks. These results collectively
underscore ShareLoRA’s effectiveness in enhanc-
ing model generalization and transferability across
both small and large-scale language models.
Continual Adaptation To assess the robustness
and knowledge retention during continual fine-
tuning, we deploy the LLaMA3 and LLaMA3.1
models on the MATH dataset. We then evaluate
their performance in mathematics and across other
domains, such as MMLU and MMLU-Pro, to com-
pare how well these models preserve knowledge,
as shown in Table 5. Our findings indicate that
both ShareLoRA and LoRA deliver matched per-
formances for directly fine-tuned domains. How-
ever, when adapting these fine-tuned models to
other evaluation benchmarks, ShareLoRA demon-
strates greater robustness, outperforming LoRA.
Specifically, on MMLU-Pro, ShareLoRA outper-
forms LoRA by 0.86% on LLaMA3.1 and 0.75%
on LLaMA3.
We also investigate continual fine-tuning across
multiple tasks—starting from Alpaca, followed by
GSM8K, then CodeAlpaca, and finally returning
to Alpaca in Table 6. ShareLoRA consistently
outperforms LoRA in this setting, with observed
gains of 0.5% on MMLU and MMLU-Pro, 1.2%
on GSM8K, and 0.6% on HumanEval, highlighting
its robustness in multi-task continual learning.
6 Analysis and Discussion
Relative Importance of LoRA Components
Our experimental findings demonstrate that both
LoRA and ShareA consistently outperform ShareB
in a variety of classification and generative tasks,
across most metrics. Within the LoRA framework,
the up-projection matrix B plays a pivotal role by
significantly enhancing the dimensionality of the
low-rank representation. Consequently, it is both
practical and justifiable to share the less critical
module, LoRA A, while retaining the integrity of
B. However, sharing both matrices A and B simul-
taneously tends to compromise too much critical

--- PAGE 8 ---
LLaMA3 8B Phase 1 ALPACA Phase 2 GSM8K Phase 3 CodeALPACA Phase 4 ALPACA
Tasks MMLU MMLU Pro GSM8K HumanEval MMLU MMLU Pro
LoRA qv 65.44 33.15 55.64 38.41 65.32 33.14
ShareLoRA qv 65.94 33.85 56.78 39.02 65.30 33.36
Table 6: Continual Adaption across multiple tasks, starting with Alpaca, followed by GSM8K, then CodeAlpaca,
and finally revisiting Alpaca. At each stage, we evaluate the effectiveness of continual adaptation by leveraging the
best checkpoints from the preceding task, comparing both LoRA and ShareLoRA for LLaMA3 8B.
information. Particularly in generative tasks, opt-
ing to share component A instead of B within the
ShareLoRA framework is strategically beneficial,
as seen in Table 2. This is because expanding the
intermediate dimension proves more crucial and
challenging than compressing high-dimensional
features in complex generative scenarios.
Sharing Attention QKV vs. Sharing All The
distinction between sharing the self-attention mech-
anism and all linear modules exists on MLP com-
ponents like gates and up/down projections. This
leads to a discrepancy in trainable parameters be-
tween LoRA’s A and B. The strategic choice in-
volves deciding whether to uniformly share weights
across all layers (ShareA) or selectively share them,
such as only for the down projection (ShareAB)
while maintaining unique weights for other compo-
nents like the up projection and gates. Preliminary
results in Appendix Figure 5 suggest that selec-
tive sharing, particularly of the QKV matrices in
Share qkv, provides an effective balance by aligning
closely with both ShareA and LoRA , potentially
mitigating overfitting risks.
Memory Footprint In the context of smaller
models like RoBERTa and GPT-2, ShareA yields
minimal parameter savings, which is negligible
given modern GPU capacities. However, with
larger models like LLaMA, ShareA demonstrates
more substantial reductions. Specifically, the
LLaMA 7B and 13B models cut down approxi-
mately 60 million and 110 million trainable param-
eters, when compared to the LoRA. This leads to
substantial efficiency gains, reducing both compu-
tational footprint and disk storage needs.
As depicted in Figure 2 and Figure 7, in
the Llama3 70B model, the ShareA adaptation
achieves a 6.3GB reduction in memory footprint
under the quantization configuration. Meanwhile,
in the Llama2 13B model with the LoRA config-
uration, ShareA manages to reduce the memory
footprint by 3.8GB and enhances training speed
by approximately 3%. The confidence intervals inTable 3 illustrate that ShareA not only improves per-
formance but also increases robustness over stan-
dard LoRA, underscoring the practical advantages
of ShareLoRA in LLMs.
SVD Analysis of LoRA and ShareA Weights
We conducted a Singular Value Decomposition
(SVD) analysis on the weights of LLaMA 13B for
both LoRA and ShareA, as shown in Figure 3 in
the Appendix. The results reveal distinct patterns
in their singular value distributions across layers.
LoRA weights exhibit a sharp decrease in singular
values, indicating a concentration of information
in a few dominant components. This could lead
to specialization but might also increase the risk
of overfitting. In contrast, ShareA weights show a
smoother, more gradual decrease in singular values,
suggesting a more balanced distribution of informa-
tion among components. This balanced distribution
contributes to ShareA’s enhanced adaptability and
generalization capability across different tasks.
These findings provide insight into why ShareA
may offer improved robustness and continue train-
ing performance compared to LoRA. The more
uniform singular values distribution in ShareA sug-
gests that it captures richer features, leading to
better generalization across various domains.
7 Conclusion
In this paper, we introduce ShareLoRA, an opti-
mization of the LoRA architecture that shares either
the up or down projection across different layers.
ShareLoRA significantly reduces the number of
trainable parameters by at least half relative to the
original LoRA and shows improved performance
on fully converged datasets. Through extensive ex-
perimentation with NLU, NLG, and zero-shot tasks
on models of varying scales, ShareLoRA demon-
strates a strong balance between computational ef-
ficiency and robust performance. It consistently
maintains high adaptability, strong robustness, and
effective continual learning capabilities across di-
verse tasks and architectures.

--- PAGE 9 ---
8 Limitation
The limitations of ShareLoRA are primarily in
its convergence speed and practical applications.
ShareAB and ShareB tend to converge more slowly
compared to LoRA, though ShareA shows a con-
vergence rate that is largely competitive with LoRA
on smaller datasets, with only a slight lag on larger
datasets. This indicates that ShareA is quite adept
at easily converged datasets and effectively mitigat-
ing near-overfitting scenarios.
Regarding the practical application of GPUs,
ShareLoRA introduces some complexities in the
parallel training process on multiple GPUs. This
is primarily due to the need for consistent synchro-
nization of the Shared Module, once it is replicated
across various GPUs at every computational step.
References
Tom B. Brown and Benjamin Mann et.al. 2020. Lan-
guage models are few-shot learners. Preprint ,
arXiv:2005.14165.
Sahil Chaudhary. 2023. Code alpaca: An instruction-
following llama model for code generation. https:
//github.com/sahil280114/codealpaca .
Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing,
and Zhiqiang Shen. 2023. One-for-all: Generalized
lora for parameter-efficient fine-tuning. Preprint ,
arXiv:2306.07967.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, Alex Ray, Raul Puri, Gretchen Krueger,
Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela
Mishkin, Brooke Chan, Scott Gray, and 39 others.
2021. Evaluating large language models trained on
code.
François Chollet. 2019. On the measure of intelligence.
Preprint , arXiv:1911.01547.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. arXiv preprint arXiv:2110.14168 .
Raj Dabre, Atsushi Fujita, and Chenhui Chu. 2019.
Exploiting multilingualism through multistage fine-
tuning for low-resource neural machine translation.
InProceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 1410–
1416, Hong Kong, China. Association for Computa-
tional Linguistics.Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. 2023. Qlora: Efficient finetuning
of quantized llms. Preprint , arXiv:2305.14314.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing.Preprint , arXiv:1810.04805.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela
Fan, and 1 others. 2024. The llama 3 herd of models.
arXiv preprint arXiv:2407.21783 .
Chowdhery et.al. 2022. Palm: Scaling language model-
ing with pathways. Preprint , arXiv:2204.02311.
Touvron et.al. 2023a. Llama 2: Open foundation and
fine-tuned chat models. Preprint , arXiv:2307.09288.
Touvron et.al. 2023b. Llama: Open and efficient founda-
tion language models. Preprint , arXiv:2302.13971.
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Bider-
man, Sid Black, Anthony DiPofi, Charles Foster,
Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
Haonan Li, Kyle McDonell, Niklas Muennighoff,
Chris Ociepa, Jason Phang, Laria Reynolds, Hailey
Schoelkopf, Aviya Skowron, Lintang Sutawika, and
5 others. 2023. A framework for few-shot language
model evaluation.
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. 2022. Towards a
unified view of parameter-efficient transfer learning.
Preprint , arXiv:2110.04366.
Shwai He, Run-Ze Fan, Liang Ding, Li Shen, Tianyi
Zhou, and Dacheng Tao. 2023. Mera: Merging
pretrained adapters for few-shot learning. Preprint ,
arXiv:2308.15982.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2021a. Measuring massive multitask language under-
standing. Preprint , arXiv:2009.03300.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021b. Measuring mathematical
problem solving with the math dataset. NeurIPS .
Jordan Hoffmann and Sebastian Borgeaud et.al. 2022.
Training compute-optimal large language models.
Preprint , arXiv:2203.15556.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for NLP. In
Proceedings of the 36th International Conference
on Machine Learning , volume 97 of Proceedings
of Machine Learning Research , pages 2790–2799.
PMLR.

--- PAGE 10 ---
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2021. Lora: Low-rank adaptation of
large language models. Preprint , arXiv:2106.09685.
Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu
Pang, Chao Du, and Min Lin. 2024. Lorahub: Effi-
cient cross-task generalization via dynamic lora com-
position. Preprint , arXiv:2307.13269.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. Preprint ,
arXiv:2001.08361.
Dawid J Kopiczko, Tijmen Blankevoort, and Yuki M
Asano. 2023. Vera: Vector-based random matrix
adaptation. arXiv preprint arXiv:2310.11454 .
Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua
Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vin-
cent Y Zhao, Yuexin Wu, Bo Li, Yu Zhang, and Ming-
Wei Chang. 2023. Conditional adapters: Parameter-
efficient transfer learning with fast inference. In
Thirty-seventh Conference on Neural Information
Processing Systems .
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing ,
pages 3045–3059, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Xiang Lisa Li and Percy Liang. 2021a. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 4582–
4597, Online. Association for Computational Lin-
guistics.
Xiang Lisa Li and Percy Liang. 2021b. Prefix-
tuning: Optimizing continuous prompts for gener-
ation. Preprint , arXiv:2101.00190.
Yang Li, Shaobo Han, and Shihao Ji. 2024. Vb-lora:
extreme parameter efficient fine-tuning with vector
banks. arXiv preprint arXiv:2405.15179 .
Vladislav Lialin, Namrata Shivagunde, Sherin Muck-
atira, and Anna Rumshisky. 2023. Relora: High-
rank training through low-rank updates. Preprint ,
arXiv:2307.05695.
Zhaojiang Lin, Andrea Madotto, and Pascale Fung.
2020. Exploring versatile generative language model
via parameter-efficient transfer learning. Preprint ,
arXiv:2004.03829.
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-
hta, Tenghao Huang, Mohit Bansal, and Colin A Raf-
fel. 2022. Few-shot parameter-efficient fine-tuningis better and cheaper than in-context learning. Ad-
vances in Neural Information Processing Systems ,
35:1950–1965.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. Preprint , arXiv:1907.11692.
Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa
Dehghani, and James Henderson. 2021. Parameter-
efficient multi-task fine-tuning for transformers via
shared hypernetworks. Preprint , arXiv:2106.04489.
Jekaterina Novikova, Ond ˇrej Dušek, and Verena Rieser.
2017. The e2e dataset: New challenges for end-to-
end generation. Preprint , arXiv:1706.09254.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research ,
21(140):1–67.
Adithya Renduchintala, Tugrul Konuk, and Oleksii
Kuchaiev. 2023. Tied-lora: Enhancing parameter
efficiency of lora with weight tying. arXiv preprint
arXiv:2311.09578 .
Andreas Rücklé, Gregor Geigle, Max Glockner,
Tilman Beck, Jonas Pfeiffer, Nils Reimers, and
Iryna Gurevych. 2021. Adapterdrop: On the ef-
ficiency of adapters in transformers. Preprint ,
arXiv:2010.11918.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
Glue: A multi-task benchmark and analysis plat-
form for natural language understanding. Preprint ,
arXiv:1804.07461.
Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni,
Abhranil Chandra, Shiguang Guo, Weiming Ren,
Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max
Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue,
and Wenhu Chen. 2024. Mmlu-pro: A more robust
and challenging multi-task language understanding
benchmark. Preprint , arXiv:2406.01574.
BigScience Workshop, :, Teven Le Scao, and An-
gela Fan et.al. 2023. Bloom: A 176b-parameter
open-access multilingual language model. Preprint ,
arXiv:2211.05100.

--- PAGE 11 ---
Yuqing Xie, Wei Yang, Luchen Tan, Kun Xiong,
Nicholas Jing Yuan, Baoxing Huai, Ming Li, and
Jimmy Lin. 2020. Distant supervision for multi-stage
fine-tuning in retrieval-based question answering. In
Proceedings of The Web Conference 2020 , WWW
’20, page 2934–2940, New York, NY , USA. Associa-
tion for Computing Machinery.
Lingling Xu and Weiming Wang. 2023. Improving
aspect-based sentiment analysis with contrastive
learning. Natural Language Processing Journal ,
3:100009.
Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui
Tao, and Fu Lee Wang. 2023. Parameter-efficient
fine-tuning methods for pretrained language mod-
els: A critical review and assessment. Preprint ,
arXiv:2312.12148.
Elad Ben Zaken, Shauli Ravfogel, and Yoav Gold-
berg. 2022. Bitfit: Simple parameter-efficient
fine-tuning for transformer-based masked language-
models. Preprint , arXiv:2106.10199.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machine really finish your sentence? In Proceedings
of the 57th Annual Meeting of the Association for
Computational Linguistics .
Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen
Chu, and Bo Li. 2023. Lora-fa: Memory-efficient
low-rank adaptation for large language models fine-
tuning. Preprint , arXiv:2308.03303.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open
pre-trained transformer language models. Preprint ,
arXiv:2205.01068.

--- PAGE 12 ---
A Hyperparameters
In our study, we limits the extent of hyperparam-
eter optimization in order to maintain consistency
with prior research (Hu et al., 2021; Dettmers et al.,
2023; Mahabadi et al., 2021; Gao et al., 2023), fa-
cilitating a direct comparison. Furthermore, we
aims to investigate the behaviors of underfitting
and overfitting across different scenarios using the
LoRA and ShareLoRA approaches applied to vari-
ous model size.
Specifically, under the current training setup,
both LoRA and ShareLoRA exhibit signs of non-
convergence when applied to the LLaMA 7B
model. On the other hand, LoRA demonstrates
clear overfitting when used with the LLaMA2 13B
model, suggesting that the model training has gone
beyond the point of optimal generalization.
For the models LLaMA 13B and LLaMA 2 7B,
their performances are comparable. Both models
reach a point of convergence and display fluctua-
tions around this state, indicating that they are fully
trained. It helps us understand the differing impacts
of LoRA and ShareLoRA on these models under a
set of reasonable training configurations.
The hyperparameter setting for RoBERTa is in
Table 8 and for LLaMA are in Table 10 and 11. The
number of trainable parameters in Table 7, should
remain consistent between QLoRA and LoRA for
LLaMA 7B and 13B in Table 3, as both models
utilize BFloat16. However, the reduced number of
trainable parameters is influenced by the implemen-
tation described in (Dettmers et al., 2023), which
reduces the trainable parameters by half when quan-
tizing to 4 bits. This is also reported the same by
(Xu et al., 2023), and we maintain this parameter
count to ensure consistency.
We conducted five experiments with Roberta and
GPT-2, and three experiments for all tasks related
to LLaMA using different seeds. The results pre-
sented are all averages.
B LLaMA Performance Analysis
In Figures 4 and 5, we present the Dev Set per-
formance changes for both LLaMA and LLaMA2
models, ranging from 7B to 13B, to observe the
differences in performance over steps. The results
demonstrate that ShareA and ShareA qkvconfigu-
rations offer several advantages over their counter-
parts, as discussed in Section 6.
For both the 7B and 13B models, ShareA and
ShareA qkvconfigurations maintain higher averageaccuracy compared to the traditional LoRA setup.
Specifically, ShareA demonstrates consistent per-
formance improvements, particularly in the stabil-
ity of accuracy over different steps. This indicates
that ShareA is more robust and less prone to fluctu-
ations compared to LoRA.
The robustness of ShareLoRA extends to quan-
tized models. Table 7 shows that QShareA
(QLoRA with ShareA) maintains strong perfor-
mance even with substantial parameter reduction.
In the case of LLaMA 7B, QShareA achieves an
MMLU score of 41.11, surpassing QLoRA’s score
of 40.63. This trend continues with larger models:
for LLaMA 13B, QShareA slightly outperforms
QLoRA with scores of 47.17 and 47.13 respec-
tively, while using significantly fewer parameters.
These performance gains are consistently observed
across different model sizes, including LLaMA2
7B and LLaMA 13B, highlighting ShareLoRA’s
broad applicability and scalability.
The analysis in Figure 4 further enriches our re-
sults by incorporating confidence intervals which
map the performance stability of LoRA, QLoRA,
ShareA, and QShareA. From these plots, it is ev-
ident that while LoRA occasionally outperforms
QLoRA, the overall performance trends of LoRA
and QLoRA are closely aligned in LLaMA 7B. In
particular, for the LLaMA 13B, the performance
of ShareA and QShareA after 5000 steps is com-
pletely superior than LoRA and QLoRA. It is cru-
cial to highlight that both LoRA and QLoRA dis-
play larger fluctuations in performance compared
to ShareA and QShareA, underscoring a potentially
greater variability in model outcomes across differ-
ent experimental seeds.
C Convergence Analysis
In Figure 6, we analyze the convergence trends
across both the MNLI and CoLA datasets for the
RoBERTa-large model, demonstrating differing be-
haviors among the sharing strategies and others.
Notably, while ShareA begins with slightly lower
performance compared to LoRA, it progressively
matches LoRA’s accuracy on the MNLI dataset.
ShareB and ShareAB, in contrast, consistently un-
derperform relative to both LoRA and ShareA. This
pattern is similarly observed with the CoLA dataset,
where ShareA’s performance is robust, closely com-
peting with LoRA. Both ShareB and ShareAB are
worse than LoRA alone.
At the same time, LoRA-FA only reaches per-

--- PAGE 13 ---
Method # Params MMLU Method # Params MMLU
LLaMA 7B (QLoRA)* 79.9M 38.8 LLaMA 13B (QLoRA)* 125.2M 47.8
LLaMA 7B (QLoRA)* 79.9M 39.96 LLaMA 13B (QLoRA)* 125.2M 47.29
LLaMA 7B (QLoRA) 79.9M 40.63±0.9 LLaMA 13B (QLoRA) 125.2M 47.13±0.9
LLaMA 7B (QShareA qkv) 67.7M 40.63±0.5 LLaMA 13B (QShareA qkv) 106.0M 47.36±0.7
LLaMA 7B (QShareA) 44.6M 41.11±0.2 LLaMA 13B (QShareA) 69.5M 47.17±0.8
Table 7: Performance comparison of LLaMA 7B and 13B with QLoRA and QShareA under the same configuration
of (Dettmers et al., 2023), ∗is similar experiment results collected from prior work (Xu et al., 2023)
Figure 3: Distribution of Singular Values for LLaMA 13B: SVD Decomposition Analysis of LoRA (left) and
ShareA (right) across All Layers.
formance levels comparable to ShareB, lagging
behind both ShareA and LoRA. This suggests
that ShareA not only sustains competitive conver-
gence capabilities but also outperforms LoRA-FA
in terms of robustness and eventual alignment with
LoRA’s top performance.
In term of training loss, all models exhibit a sim-
ilar declining trend over the training epochs. How-
ever, ShareA distinguishes itself by slightly lagging
behind LoRA initially in terms of speed of con-
vergence but substantial surpassing both ShareB
and LoRA-FA overall. This differential suggests
that ShareA offers a balanced approach, effectively
managing a slower initial convergence for consis-
tent long-term gains.
D Memory Footprint
We utilizes float32 for QLoRA modules to en-
hance the performance of quantized models, while
bfloat16 is employed for LoRA fine-tuning. We em-
ploy the standard AdamW optimizer with a batch
size of 1, a sequence length of 512, and do not use
gradient checkpointing.
The chart in Figure 7 depicts memory usage
across four configurations of the Llama2 13B
model: LoRA, LoRA-Shared A, QLoRA, and
QLoRA-Shared A, highlighting the impact ofmodel scaling and adaptations on resource needs.
It shows a memory reduction of 3.8 GB when using
LoRA-Shared A compared to the LoRA configura-
tion, and a further savings of 2.1 GB with QLoRA-
Shared A compared to QLoRA. LoRA-Shared oper-
ates independently from QLoRA strategies, thereby
reducing memory usage further without interfering
with LoRA or QLoRA configurations.

--- PAGE 14 ---
LLaMA  7B LLaMA  13B
LLaMA  7B LLaMA  13BSteps Steps
Steps StepsFigure 4: LLaMA 7B & 13B on LoRA / ShareA (upper) and on QLoRA / QShareA (down) MMLU Dev Performance
with the standard deviation error distribution of different seeds
Figure 5: Average Performance Plot for Various LLaMA Models on the Alpaca-MMLU Dev Dataset

--- PAGE 15 ---
MNLI Testing Loss CoLA  Testing Loss
EpochsEpochs
MNLI Testing Loss CoLA  Testing Loss
EpochsEpochsFigure 6: Convergence Performance for MNLI and CoLA datasets

--- PAGE 16 ---
Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B
Optimizer AdamW
Warmup Ratio 0.06
LR Schedule Linear
Batch Size (per device) 16 16 16 32 32 16 32 16
# Epochs 30 60 30 80 25 25 80 40
RoBERTa base Learning Rate 5E-04 5E-04 4E-04 4E-04 4E-04 5E-04 5E-04 4E-04
ShareLoRA LoRA Config. rq=rv= 8
LoRA α 8
Max Seq. Len. 512
seed 0,1,2,3,4
Batch Size (per device) 4
# Epochs 10 10 20 20 10 20 20 10
RoBERTa large Learning Rate 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04
ShareLoRA † LoRA Config. rq=rv= 8
LoRA α 8
Max Seq. Len. 512
seed 0,1,2,3,4
Table 8: Configuration and training details for RoBERTa base LoRA on different datasets.
Dataset E2E Challege
Optimizer AdamW
Weight Decay 0.01
Dropout Prob 0.1
Batch Size (per device) 8
# Epochs 5
Warmup Steps 500
Learning Rate Schedule Linear
Label Smooth 0.1
Learning Rate 0.002
Adaptation rq=rv= 4
LoRA α 32
Beam Size. 10
Length Penalty 0.9
no repeat ngram size 4
Table 9: Configuration and training details for GPT-2 LoRA on E2E Challenge
Model Parameters Batch size LR Steps Source Length Target Length LoRA r LoRA α
LLaMA1 & 2 7B 16 2e-4 10000 384 128 64 16
LLaMA1 & 2 13B 16 2e-4 10000 384 128 64 16
LLaMA3 & 3.1 8B 16 2e-5 5000 384 128 64 16
Table 10: Training hyperparameters for LLaMA and QLLaMA.
Parameters MMLU Source Length Temperature Top P Beam size
7B 2048 0.7 0.9 1
13B 2048 0.7 0.9 1
Table 11: Evaluation hyperparameters for LLaMA and QLLaMA.

--- PAGE 17 ---
Figure 7: Memory Consumption required for LLaMA2 13B.
