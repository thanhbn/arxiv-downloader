# 2309.06759.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2309.06759.pdf
# Kích thước file: 885385 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Scaled Prompt-Tuning cho Sinh Ngôn Ngữ Tự Nhiên Few-Shot
Ting Hu
Hasso Plattner Institute
University of Potsdam
Potsdam, Germany
ting.hu@hpi.deChristoph Meinel
Hasso Plattner Institute
University of Potsdam
Potsdam, Germany
meinel@hpi.deHaojin Yang
Hasso Plattner Institute
University of Potsdam
Potsdam, Germany
haojin.yang@hpi.de

Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLMs) ngày càng tăng cho thấy khả năng hiểu và sinh ngôn ngữ mạnh mẽ hơn, trong khi nhu cầu bộ nhớ và chi phí tính toán của việc tinh chỉnh LLMs trên các tác vụ hạ dòng là không thể bỏ qua. Bên cạnh đó, tinh chỉnh thường đòi hỏi một lượng dữ liệu nhất định từ các tác vụ cá nhân trong khi chi phí thu thập dữ liệu là một vấn đề khác cần xem xét trong các ứng dụng thực tế. Trong công trình này, chúng tôi tập trung vào các phương pháp Tinh chỉnh Hiệu quả Tham số (PEFT) cho Sinh Ngôn ngữ Tự nhiên (NLG) few-shot, nhằm đông cứng hầu hết các tham số trong LLMs và chỉ tinh chỉnh một tập con nhỏ các tham số trong các trường hợp few-shot để giảm dung lượng bộ nhớ, chi phí huấn luyện và chi phí gán nhãn trong khi duy trì hoặc thậm chí cải thiện hiệu suất. Chúng tôi đề xuất phương pháp Scaled Prompt-Tuning (SPT) vượt trội hơn PT thông thường với hiệu suất và khả năng tổng quát hóa tốt hơn nhưng không làm tăng đáng kể chi phí huấn luyện. Nghiên cứu sâu hơn về SPT trung gian cho thấy khả năng chuyển giao vượt trội của SPT trong các tình huống few-shot, cung cấp một công thức cho các hoàn cảnh thiếu dữ liệu và hạn chế tính toán. Hơn nữa, so sánh toàn diện các phương pháp PEFT hiện có cho thấy rằng một số cách tiếp cận thể hiện hiệu suất tốt với chi phí huấn luyện khiêm tốn như Prefix-Tuning trong nghiên cứu trước đây có thể gặp khó khăn trong các tác vụ NLG few-shot, đặc biệt trên các tập dữ liệu thách thức.

1 Giới thiệu
Với sự xuất hiện và phát triển của các Mô hình Ngôn ngữ Lớn (LLMs), tinh chỉnh đã trở thành mô hình chủ đạo trong Xử lý Ngôn ngữ Tự nhiên bất kể các mô hình nền tảng mạnh mẽ mới xuất hiện gần đây như GPT-4 và kỹ thuật prompt engineering đi kèm. Mặc dù tinh chỉnh cung cấp phương tiện hiệu quả để chuyển giao kiến thức đã được tiền huấn luyện sang các tác vụ hạ dòng và cần một lượng nhỏ dữ liệu văn bản so với các kho dữ liệu tiền huấn luyện, nó vẫn đòi hỏi một lượng đáng kể bộ nhớ trên thiết bị và kéo theo chi phí huấn luyện đáng kể. Những điều này thúc đẩy việc phát triển một chủ đề nghiên cứu khác, Tinh chỉnh Hiệu quả Tham số (PEFT), đông cứng hầu hết các tham số trong LLMs và chỉ tinh chỉnh một phần nhỏ trong số chúng, dẫn đến giảm dung lượng bộ nhớ và chi phí tính toán với hiệu suất tương đương với tinh chỉnh thông thường. Hơn nữa, sự xuất hiện của học trong ngữ cảnh (Brown et al., 2020) thúc đẩy sự lạc quan cho các tình huống few-shot và thu hút sự quan tâm nghiên cứu đáng kể trong các phương pháp PEFT few-shot, dành riêng cho chi phí thấp nhất của việc áp dụng LLMs vào các tình huống khan hiếm dữ liệu và hạn chế tài nguyên.

Câu hỏi cốt lõi của PEFT là tham số nào trong LLMs sẽ được tinh chỉnh để chúng ta có thể tinh chỉnh ít tham số nhất có thể với độ giảm hiệu suất ít nhất trên các tác vụ hạ dòng. Nhiều cách tiếp cận, như Adapter (Houlsby et al., 2019) và Prompt-Tuning (Lester et al., 2021), khác nhau về tham số có thể huấn luyện và hiệu suất đã được đề xuất và áp dụng trong nhiều lĩnh vực khác nhau. Mặc dù có một số nghiên cứu gần đây về PEFT few-shot cho các tác vụ Hiểu Ngôn ngữ Tự nhiên (NLU), sự hiểu biết của chúng ta về các phương pháp PEFT trên các tác vụ Sinh Ngôn ngữ Tự nhiên (NLG), bao gồm sinh văn bản từ Biểu diễn Ý nghĩa (MR) và sinh văn bản từ Đồ thị Tri thức (KG), trong các trường hợp few-shot là không đầy đủ, điều này thúc đẩy chúng tôi đi sâu vào các chi tiết. Chúng tôi tổng kết các đóng góp của mình dưới đây.

• Chúng tôi đưa ra Scaled Prompt-Tuning (SPT) vượt trội hơn hẳn Prompt-Tuning thông thường với các tham số có thể huấn luyện thêm không đáng kể.

• SPT thể hiện khả năng chuyển giao tốt hơn so với tinh chỉnh trong các trường hợp few-shot, cung cấp một công thức trong môi trường hạn chế tài nguyên mà không cần chi phí gán nhãn thêm thông qua SPT trung gian.

• So sánh toàn diện các phương pháp PEFT hiện có cho thấy rằng các cách tiếp cận hoạt động tốt khi có đủ số lượng thể hiện dữ liệu như Prefix-Tuning có thể gặp khó khăn trong các trường hợp few-shot, đặc biệt trên các tập dữ liệu thách thức.

2 Công trình liên quan
Các cách tiếp cận PEFT cố gắng tinh chỉnh một phần nhỏ tham số và đông cứng hầu hết các tham số trong LLMs, đòi hỏi ít dung lượng bộ nhớ và tiêu thụ năng lượng hơn. Các phương pháp này khác nhau ở việc tham số nào được tinh chỉnh trên các tác vụ hạ dòng. Công trình PEFT đầu tiên là Adapter (Houlsby et al., 2019), nơi các mô-đun bottleneck nhỏ có thể huấn luyện được chèn vào các lớp BERT (Devlin et al., 2018) cho mỗi tác vụ. Cụ thể hơn, các adapter được chèn vào hai vị trí của mỗi lớp: sau phép chiếu theo sau mô-đun Multi-Head Attention và sau hai Mạng Feed-Forward (FFNs). Bằng cách tinh chỉnh các Adapter specific tác vụ trên benchmark GLUE (Wang et al., 2018), độ giảm hiệu suất nằm trong khoảng 0,4% so với tinh chỉnh, trong khi chỉ 3,6% tham số được thêm vào. Một cách trực quan, các vị trí chèn của các Adapter và khả năng của các adapter đa tác vụ là những câu hỏi thú vị. He et al. (2021) tiếp tục chứng minh rằng việc thêm adapter sau các FFN là đủ để đóng gói và tinh chỉnh thông tin cụ thể của tác vụ từ các tham số đông cứng, vì FFNs có thể sử dụng tốt hơn việc sửa đổi ở khả năng lớn hơn. Điều này hiệu quả làm giảm 50% adapter được chèn trong công trình trước đây. Mặt khác, AdapterFusion (Pfeiffer et al., 2020) thực hiện thuật toán học hai giai đoạn: huấn luyện adapter cụ thể tác vụ và kết hợp adapter trong mô-đun fusion. Các tác giả cho thấy rằng việc kết hợp kiến thức từ các tác vụ khác nhau thu được bởi các adapter tương ứng của chúng có thể có lợi cho từng tác vụ cá nhân. Mặc dù có nhu cầu bộ nhớ ít hơn và chi phí tinh chỉnh, adapter thực sự giới thiệu khoảng 4-6% thời gian suy luận thêm, vì tất cả các tham số của BERT và các adapter được chèn đều tham gia vào suy luận. AdapterDrop (Rücklé et al., 2020) tiếp tục đề xuất loại bỏ một số lượng biến đổi các adapter từ các lớp BERT thấp hơn. Nó động giảm chi phí tính toán tại thời gian chạy khi thực hiện suy luận trên nhiều tác vụ và duy trì hiệu suất tác vụ ở mức độ lớn. Xem xét các lớp Fully Connected (FC) trong các adapter bottleneck vẫn có số lượng tham số tương đối lớn, Compacter (Karimi Mahabadi et al., 2021) giới thiệu các Lớp Nhân Hypercomplex Tham số hóa (PHM) hiệu quả hơn để thay thế các lớp FC, nơi trọng số của mỗi lớp FC được tính như tổng của một số tích Kronecker. Karimi Mahabadi et al. (2021) tiếp tục giảm các tham số có thể huấn luyện bằng cách để các adapter trên các lớp chia sẻ trọng số chậm và thay đổi trong các ma trận rank-one nhanh, dẫn đến Compacter++. Compacter hoạt động ngang bằng với tinh chỉnh khi áp dụng cho T5-Base (Raffel et al., 2020) trên benchmark GLUE bằng cách chỉ huấn luyện 0,047% tham số.

Một hướng nghiên cứu khác chèn các tham số có thể huấn luyện thêm ở các định dạng khác thay vì các mô-đun bottleneck, bao gồm Prompt-Tuning và Prefix-Tuning. Prompt-Tuning khác với prompting, tức là học trong ngữ cảnh. Học trong ngữ cảnh nở rộ khi GPT-3 (Brown et al., 2020) xuất hiện, có thể trực tiếp thích ứng với một số tác vụ hạ dòng bằng cách nhập prompt thay vì tinh chỉnh tham số. Các prompt thường có một số mô tả về các tác vụ theo sau là một số exemplar và nội dung cụ thể mà chúng ta muốn mô hình giúp đỡ. Prompt-Tuning (Lester et al., 2021) thêm vào trước một soft prompt có thể huấn luyện vào embedding đầu vào của mô hình cho các tác vụ hạ dòng cụ thể. Chỉ các prompt liên tục được cập nhật trong quá trình huấn luyện. Phương pháp này hoạt động tốt khi áp dụng trên các mô hình được tiền huấn luyện trong thiết lập đa tác vụ như T5. Tuy nhiên, tác động của soft prompt có thể trở nên yếu hơn và yếu hơn khi mô hình đi sâu hơn, sau đó một hướng nghiên cứu khác sửa đổi các biểu diễn trong các mô hình hiệu quả hơn xuất hiện. Prefix-Tuning (Li and Liang, 2021) thêm vào trước một tiền tố liên tục có thể huấn luyện vào đầu vào của mỗi lớp cho mô hình kiểu decoder và hai tiền tố cho mô hình encoder-decoder, riêng biệt. Bằng cách chỉ học 0,1% tham số của BART (Lewis et al., 2019), Prefix-Tuning đạt được hiệu suất tương đương với tinh chỉnh trên các tác vụ sinh văn bản từ bảng. P-Tuning v2 (Liu et al., 2021) chia sẻ ý tưởng tương tự với Prefix-Tuning, chèn prompt đa lớp trong mô hình trong khi nghiên cứu hiệu suất trên các tác vụ NLU.

Các phương pháp khác khám phá tinh chỉnh các tham số khác trong LLMs. Intrinsic SAID (Aghajanyan et al., 2020) chứng minh thực nghiệm rằng LLMs có chiều nội tại rất thấp và đề xuất tinh chỉnh các tham số trong một không gian con chiều thấp hơn, được đạt được bằng phép chiếu tuyến tính ngẫu nhiên thông qua biến đổi Fastfood. FISH Mask (Sung et al., 2021) chọn một tập con tham số để cập nhật dựa trên thông tin Fisher ước tính của chúng. BitFit (Zaken et al., 2021) chứng minh rằng chỉ tinh chỉnh các số hạng bias trong BERT có thể cạnh tranh với tinh chỉnh với quy mô dữ liệu huấn luyện nhỏ đến trung bình. Nó nêu ra giả thuyết rằng tinh chỉnh chủ yếu là về việc tiết lộ kiến thức được tạo ra bởi huấn luyện mô hình ngôn ngữ, hơn là học kiến thức ngôn ngữ cụ thể tác vụ mới. LoRA (Hu et al., 2021) tiêm các ma trận phân tách rank có thể huấn luyện vào mỗi lớp Transformer dựa trên giả thuyết rằng sự thay đổi trong trọng số trong quá trình thích ứng mô hình có rank nội tại thấp. Nó không giới thiệu độ trễ suy luận và giảm độ dài chuỗi đầu vào trong khi duy trì hiệu suất cao trên RoBERTa, GPT-2 và thậm chí GPT-3. (IA)³(Liu et al., 2022) giới thiệu ba vector học được để tái tỷ lệ Keys và Values trong các mô-đun attention và các kích hoạt bên trong của các FFN point-wise, tương ứng, thông qua phép nhân từng phần tử. Cách tiếp cận này được thực hiện trên T0 (Sanh et al., 2021) được tiền huấn luyện theo cách đa prompt và đa tác vụ. UniPELT (Mao et al., 2021) tiếp tục kết hợp các phương pháp PEFT khác nhau như các mô-đun con, bao gồm Adapters, LoRA, và Prefix-Tuning, và học kích hoạt những cái phù hợp với thiết lập dữ liệu hoặc tác vụ hiện tại tốt nhất thông qua cơ chế gating trên các tác vụ GLUE. Tương tự, He et al. (2021) phân tích thiết kế của các phương pháp PEFT khác nhau và tái khung chúng như các sửa đổi đối với các trạng thái ẩn cụ thể trong LLMs. Nó thiết lập một khung thống nhất với Adapters, Prefix-Tuning, và LoRA trong BART, và thực hiện các thí nghiệm trên các tác vụ NLU, tóm tắt và dịch máy.

Chúng tôi so sánh các phương pháp trên từ các khía cạnh khác nhau trong Bảng 1. Như chúng ta có thể thấy, có ít phương pháp PEFT hiện có tập trung vào các tác vụ NLG, chưa kể đến các tác vụ NLG few-shot. Do đó, sự hiểu biết và nhận thức về các đặc điểm của các phương pháp PEFT trên các tác vụ NLG trong các trường hợp few-shot là thiếu hụt, đây là điều chúng tôi tập trung trong công trình này. Hơn nữa, có một số công trình liên quan đề xuất các ý tưởng liên quan đến tỷ lệ mà chúng tôi so sánh dưới đây. (IA)³(Liu et al., 2022) giới thiệu ba vector học được để tái tỷ lệ Keys, Values, và các kích hoạt bên trong. He et al. (2021) đề xuất các adapter song song được tỷ lệ và chứng minh rằng yếu tố tỷ lệ là quan trọng đối với các adapter song song. Scaled Prompt-Tuning được đề xuất của chúng tôi có thể được coi là một biến thể đơn giản hóa của chúng, không đặt bất kỳ sửa đổi thêm nào cho mỗi lớp Transformer và tinh chỉnh ít tham số hơn nhiều so với của họ.

3 Phương pháp
Prompt-Tuning thông thường đông cứng các tham số của LLMs và chỉ tinh chỉnh embedding của k token bổ sung, tức là soft prompt, cho các tác vụ hạ dòng cá nhân. Giả sử chuỗi đầu vào với l token là X={x₁, x₂, ..., xₗ}. Sau khi đi qua lớp embedding, chuỗi được biểu diễn bởi ma trận X_e∈R^(l×n_e), trong đó n_e là chiều ẩn của các embedding. Soft prompt có thể huấn luyện, được ký hiệu là X_p∈R^(k×n_e), được thêm vào trước embedding chuỗi đầu vào X_e, dẫn đến ma trận embedding cập nhật X_h = [X_p; X_e]∈R^((k+l)×n_e), được đưa vào các khối sau của LLMs để tính toán tiếp theo, trong khi chỉ X_p được tối ưu hóa trong quá trình huấn luyện.

Chúng tôi đề xuất Scaled Prompt-Tuning trong đó soft prompt có thể huấn luyện X_p với một vector tỷ lệ có thể huấn luyện bổ sung s∈R^(k×1) được sử dụng. Do đó, ma trận embedding cập nhật là X_h = [s⊙X_p; X_e], trong đó ⊙ biểu thị tích Hadamard. Nghĩa là, mỗi yếu tố tỷ lệ trong vector tỷ lệ s được áp dụng để tỷ lệ embedding của một soft token cá nhân, tạo ra khoảng cách biểu diễn thu hẹp giữa soft prompt và embedding chuỗi đầu vào. Các lựa chọn thay thế khác có thể là sử dụng một yếu tố tỷ lệ giá trị đơn hoặc ma trận tỷ lệ s∈R^(k×n_e), trong khi cả hai đều có hiệu suất kém hơn vector tỷ lệ một chiều được đề xuất theo kinh nghiệm. Hình 1 mô tả phương pháp SPT được đề xuất, và LLM encoder-decoder mà chúng tôi làm việc là T5-large (Raffel et al., 2020) với tổng cộng 770M tham số.

Hình 1: Scaled Prompt-Tuning được đề xuất. Các tham số của encoder và decoder được đông cứng, trong khi soft prompt và vector tỷ lệ được huấn luyện cho các tác vụ hạ dòng.

4 Thí nghiệm
4.1 Dữ liệu và metric đánh giá
Thí nghiệm được thực hiện trên ba tập dữ liệu NLG: WebNLG 2020¹, E2E², và DART³. Vì chúng chứa số lượng lớn các thể hiện, chúng tôi lấy mẫu các tập con thể hiện từ mỗi tập dữ liệu cho tinh chỉnh few-shot. Quá trình lấy mẫu được thực hiện ba lần cho mỗi tình huống few-shot. WebNLG 2020 có 16 danh mục KGs, và chúng tôi lấy mẫu từ mỗi danh mục. Tập dữ liệu E2E không có định nghĩa danh mục, sau đó chúng tôi phân biệt các thể hiện dựa trên số lượng cặp slot-value trong MRs, do đó phân loại chúng thành 6 nhóm riêng biệt và lấy mẫu từ mỗi nhóm. Các mẫu trong DART từ 6 nguồn và chúng tôi lấy mẫu từ mỗi nguồn.

Việc chuyển đổi dữ liệu có cấu trúc thành đầu vào của LLMs là quan trọng. Điều này liên quan đến paradigm tiền huấn luyện của LLM mà các phương pháp PEFT được áp dụng. Trong công trình này, mô hình chúng tôi áp dụng là T5, được tiền huấn luyện theo cách text-to-text. Do đó, chúng tôi sử dụng phương pháp chuyển đổi dữ liệu có cấu trúc trong Bảng 2. Đối với WebNLG 2020 và DART, chúng tôi tuyến tính hóa các triple trong KGs và thêm vào trước token <S>, <P>, và <O> cho chủ thể, vị từ, và đối tượng của mỗi triple, tương ứng. Về E2E, chúng tôi tuyến tính hóa các cặp slot-value trong MRs, và chèn token <S> và <V> trước slot và value, riêng biệt. Quá trình chuyển đổi này được chứng minh là hiệu quả theo công trình trước đây (?) và các thí nghiệm của chúng tôi. Token <S>, <P>, <O>, và <V> là các dấu phân cách duy trì thông tin cấu trúc ở một mức độ nào đó trong các chuỗi văn bản được chuyển đổi.

Đối với mỗi tập dữ liệu, chúng tôi sử dụng các metric được cung cấp trong benchmark để đánh giá. WebNLG 2020 áp dụng BLEU, METEOR, chrF++, TER, BERTScore, và BLEURT. E2E sử dụng BLEU, NIST, METEOR, ROUGE-L, và CIDEr. DART sử dụng BLEU, METEOR, TER, BERTScore, MoverScore, và BLEURT. Hầu hết các metric đo lường sự tương tự giữa các câu được sinh ra và các tham chiếu từ các khía cạnh khác nhau, càng cao càng tốt. TER đo lường có bao nhiêu thực thể trong dữ liệu có cấu trúc cho trước được truyền đạt chính xác trong các câu được sinh ra, càng thấp càng tốt. Xem xét sự bất ổn định huấn luyện của học few-shot, chúng tôi thực hiện các thí nghiệm ba lần với các seed ngẫu nhiên khác nhau trên mỗi tập con được lấy mẫu. Cuối cùng, chúng tôi trình bày kết quả đánh giá trung bình của chín thí nghiệm trong mỗi trường hợp few-shot dưới đây nếu không được tuyên bố đặc biệt.

4.2 Các phương pháp và chi tiết triển khai
Chúng tôi so sánh SPT được đề xuất với các công trình liên quan, bao gồm Adapter, LoRA, Compacter, Prefix-Tuning, Prompt-Tuning, và các phương pháp gần đây hơn, IA³ và UniPELT. Các phương pháp PEFT này được áp dụng cho T5-large với 770M tham số. Đối với Prompt-Tuning và SPT, độ dài của prompt là 50 trên tất cả các tập dữ liệu. Về Prefix-Tuning, chúng tôi trực tiếp huấn luyện các tham số tiền tố thay vì thay thế chúng bằng các mô-đun bottleneck để so sánh chặt chẽ hơn với phương pháp được đề xuất của chúng tôi. Hơn nữa, các mô-đun bottleneck không vượt trội hơn các tham số tiền tố đơn giản trong các thí nghiệm của chúng tôi mặc dù tính hiệu quả của chúng trong công trình liên quan (Li and Liang, 2021). Đối với các phương pháp khác, chúng tôi bắt đầu từ các thiết lập mặc định được cung cấp trong thư viện adapter-transformers⁴ và tinh chỉnh thêm chúng trên các tập dữ liệu khác nhau nếu các mặc định không áp dụng được. DART là một tập dữ liệu thách thức và đòi hỏi nhiều tinh chỉnh hơn về tỷ lệ học và cấu hình. Các thiết lập chi tiết của các phương pháp PEFT được tóm tắt trong Bảng 3. Đối với mỗi thí nghiệm, chúng tôi lưu checkpoint dẫn đến điểm BLEU cao nhất trên tập dev và đánh giá thêm nó trên tập test.

4.3 So sánh các phương pháp PEFT few-shot
4.3.1 WebNLG 2020
Chúng tôi đánh giá các phương pháp PEFT trong các trường hợp 8-, 16-, 50- và 100-shot trên tập dữ liệu WebNLG 2020 trong Hình 2 (a). Trong số các phương pháp có mặt, Compacter đạt được hiệu suất tốt nhất và thậm chí vượt trội hơn Fine-Tuning trong khi chỉ tinh chỉnh 0,053% tham số. Prefix-Tuning rõ ràng tụt hậu so với những phương pháp khác trong các tình huống 8-shot và 16-shot. Với điểm BLEU thấp như vậy, các văn bản được sinh ra hầu như chỉ sao chép một số dữ liệu có cấu trúc cho trước và thất bại trong việc tạo thành các câu trôi chảy. Ngoài ra, việc thay thế các vector tiền tố bằng các mô-đun bottleneck phức tạp hơn và chèn tiền tố trong decoder không thấy sự cải thiện hiệu suất rõ ràng. Những hiện tượng này không được thể hiện trong công trình liên quan như Li and Liang (2021) vì hầu hết chúng thực hiện tinh chỉnh toàn bộ tập dữ liệu, điều này chứng minh hiệu suất có thể chấp nhận được của Prefix-Tuning. Trên thực tế, Prefix-Tuning dần trở nên mạnh mẽ khi số lượng thể hiện có sẵn tăng lên và thậm chí vượt trội hơn Prompt-Tuning trong trường hợp 100-shot, thể hiện tính hiệu quả của Prefix-Tuning. Do đó, kết luận của chúng tôi là Prefix-Tuning rất nhạy cảm với số lượng thể hiện huấn luyện trên WebNLG 2020 và hoạt động kém trong các trường hợp few-shot cực kỳ.

Một phương pháp khác thu hút sự chú ý của chúng tôi trong Hình 2 (a) là UniPELT, một sự kết hợp của LoRA, Prefix-Tuning, và Adapter, tuyên bố tận dụng tất cả chúng. Tuy nhiên, UniPELT tụt hậu đáng kể so với Adapter và LoRA trong khi vượt trội hơn Prefix-Tuning. Có vẻ như Prefix-Tuning đóng vai trò chính trong UniPELT và do đó dẫn đến hiệu suất kém hơn so với các phương pháp khác. Chúng tôi phân tích rằng ba phương pháp liên quan trong UniPELT có thuộc tính hội tụ khác nhau và nhạy cảm với các siêu tham số như tỷ lệ học, như Bảng 3 cho thấy. Sự khác biệt này được khuếch đại trên các tác vụ sinh và một tỷ lệ học cá nhân không thể tạo điều kiện cho sự hội tụ của tất cả các tham số có thể huấn luyện trong các tình huống few-shot, mang lại hiệu suất dưới mức.

Mặt khác, SPT được đề xuất rõ ràng vượt trội hơn Prompt-Tuning và ngang bằng với Adapter. Bảng 4 tiếp tục hiển thị các kết quả đánh giá chi tiết của Fine-Tuning, Prompt-Tuning, SPT, và phương pháp hoạt động tốt nhất trong mỗi trường hợp few-shot để làm sáng tỏ sự cải thiện của phương pháp được đề xuất và khoảng cách hiệu suất so với tốt nhất. Các kết quả của các metric khác như METEOR và BLEURT gần như phù hợp với BLEU trong khi khoảng cách của các điểm số có thể hẹp hơn. Do đó, chúng tôi chủ yếu tham khảo điểm BLEU khi thảo luận về hiệu suất của các phương pháp khác nhau dưới đây trừ khi được nêu rõ ràng.

Như chúng ta có thể thấy, SPT vượt trội hơn Prompt-Tuning thông thường khoảng 5,3 điểm trong điểm BLEU trong các trường hợp 16-, 50- và 100-shot. Compacter là phương pháp hoạt động tốt nhất trong tất cả các trường hợp few-shot và vượt trội hơn SPT tối đa 3,2 điểm BLEU. Khi toàn bộ tập dữ liệu được liên quan đến tinh chỉnh, UniPELT đạt được hiệu suất vượt trội trên hầu hết các metric đánh giá và thậm chí vượt trội hơn Fine-Tuning. Điều này phù hợp với Mao et al. (2021) trong đó UniPELT kết hợp một số phương pháp PEFT và đạt được sự cải thiện hiệu suất trên các tập dữ liệu hạ dòng.

--- TRANG 6 ---
(a) WebNLG 2020

(b) E2E

(c) DART

Hình 2: Hiệu suất của các phương pháp PEFT trong các trường hợp few-shot trên ba tập dữ liệu.

có xu hướng hội tụ khác nhau và nhạy cảm với các siêu tham số như tỷ lệ học, như Bảng 3 cho thấy. Sự khác biệt này được khuếch đại trên các tác vụ sinh và một tỷ lệ học cá nhân không thể tạo điều kiện cho sự hội tụ của tất cả các tham số có thể huấn luyện trong các tình huống few-shot, mang lại hiệu suất dưới mức.

[Bảng 4: Kết quả đánh giá toàn diện của một số phương pháp PEFT trên WebNLG 2020 trong các trường hợp few-shot...]

4.3.2 E2E
Xu hướng hiệu suất của các phương pháp PEFT trên tập dữ liệu E2E trong các trường hợp few-shot khác với trên WebNLG 2020, như Hình 2 (b) mô tả. Không có phương pháp đơn lẻ nào có thể thống trị trong tất cả các tình huống few-shot. Compacter nổi bật trong các trường hợp 8- và 16-shot. IA³ sau đó thắng thế so với những phương pháp khác khi có nhiều mẫu hơn có sẵn để tinh chỉnh. Cả Prefix-Tuning và UniPELT đều hoạt động tốt hơn nhiều trên tập dữ liệu E2E so với trên WebNLG 2020. Chúng tôi đoán lý do là E2E là một tập dữ liệu ít thách thức hơn với dữ liệu có cấu trúc đơn giản hơn, và việc tinh chỉnh các vector tiền tố là đủ để tinh chỉnh các biểu diễn ẩn cho sinh. Hơn nữa, LoRA và Prompt-Tuning trở thành những phương pháp hoạt động kém trong các trường hợp few-shot theo Hình 2 (b).

Hơn nữa, SPT vẫn vượt trội hơn Prompt-Tuning với một khoảng cách lớn, các chi tiết được liệt kê trong Bảng 5. Khoảng cách hiệu suất lớn nhất giữa chúng là 4,8 điểm trong điểm BLEU trong trường hợp 50-shot. Sự khác biệt lớn nhất về hiệu suất giữa SPT và phương pháp hoạt động tốt nhất là 3,5 điểm BLEU trong tình huống 8-shot. Tương tự, UniPELT thắng thế so với các phương pháp khác và ngang bằng với Fine-Tuning khi tất cả các mẫu trong tập dữ liệu được sử dụng để tinh chỉnh. Quan trọng là, SPT được đề xuất chỉ tụt hậu so với UniPELT 1,4 điểm trong điểm BLEU trong khi UniPELT tinh chỉnh gấp 170 lần số tham số so với SPT.

[Bảng 5: Kết quả đánh giá toàn diện của một số phương pháp PEFT trên E2E trong các trường hợp few-shot...]

4.3.3 DART
Tập dữ liệu này là thách thức nhất trong số các tập dữ liệu mà chúng tôi làm việc, vì các thể hiện đến từ nhiều miền khác nhau và ở các định dạng gốc khác biệt. Như Hình 2 (c) hiển thị, LoRA là phương pháp hoạt động tốt nhất trong trường hợp 8-shot, và Compacter vượt trội hơn những phương pháp khác trong các tình huống 16-, 50-, và 100-shot. Prefix-tuning và UniPELT một lần nữa hoạt động kém đáng kể so với những phương pháp khác thậm chí trong trường hợp 100-shot, ngầm phản ánh rằng các thể hiện trong DART thách thức hơn so với trong WebNLG 2020.

Hơn nữa, SPT ổn định vượt trội hơn Prompt-Tuning, và sự cải thiện hiệu suất lớn nhất là 1,8 điểm trong điểm BLEU trong trường hợp 100-shot, như Bảng 6 trình bày chi tiết. Trong khi đó, SPT hoạt động kém hơn Compacter 3 điểm BLEU. Khi toàn bộ tập dữ liệu được sử dụng trong tinh chỉnh, khoảng cách hiệu suất giữa SPT và Adapter, phương pháp hàng đầu hiện tại, là 1,8 điểm BLEU. SPT được đề xuất một lần nữa hứa hẹn xem xét Adapter huấn luyện hơn 100 lần số tham số của SPT.

[Bảng 6: Kết quả đánh giá toàn diện của một số phương pháp PEFT trên DART trong các trường hợp few-shot...]

4.3.4 Tóm tắt
Theo so sánh ở trên, chúng tôi đưa ra một số gợi ý về việc áp dụng các phương pháp PEFT dưới đây. Thứ nhất, Compacter luôn là ứng viên khi có một số lượng nhỏ thể hiện có sẵn xem xét hiệu suất tốt và chi phí tinh chỉnh khiêm tốn của nó trên tất cả các tập dữ liệu. Thứ hai, SPT được đề xuất là lựa chọn tối ưu khi bộ nhớ trên thiết bị bị hạn chế trong khi có một số lượng lớn thể hiện huấn luyện có sẵn. Thứ ba, Prefix-tuning và UniPELT không phải là lựa chọn tốt trong các trường hợp few-shot đặc biệt trên các tập dữ liệu thách thức.

4.4 Độ dài prompt
Độ dài prompt có tác động không thể tránh khỏi đến hiệu suất của SPT. Chúng tôi thực hiện SPT sử dụng tất cả các thể hiện trong mỗi tập dữ liệu cá nhân để tìm các thiết lập tối ưu, sau đó được sử dụng trong các tình huống few-shot. Các cấu hình của các phương pháp PEFT khác nhau trong Bảng 3 đều được xác định từ trường hợp all-shot. Do đó, một lập luận là hiệu suất của các phương pháp được đề cập ở trên có thể được cải thiện thêm nếu các thiết lập siêu tham số được thiết kế đặc biệt cho mỗi trường hợp few-shot. Điều này có thể là trường hợp trong khi các so sánh trình bày ở trên vẫn công bằng và phản ánh tính mạnh mẽ và khả năng tổng quát hóa của các phương pháp.

Hình 3 mô tả độ dài prompt so với BLEU và TER trên ba tập dữ liệu. Điểm BLEU tăng lên khi prompt trở nên dài hơn trong khi tỷ lệ tăng trưởng giảm dần trên ba tập dữ liệu. TER giảm theo sự tăng lên của độ dài prompt trên tập dữ liệu WebNLG 2020 và DART. Do đó, chúng tôi đặt độ dài prompt là 50, xem xét không có khoảng cách hiệu suất đáng kể khi độ dài prompt thay đổi từ 50 đến 60.

4.5 Sự ổn định tinh chỉnh
Theo nghiên cứu trước đây, tinh chỉnh trên các tác vụ hạ dòng nhỏ là không ổn định. Khi nói đến PEFT few-shot, vấn đề ổn định có thể nghiêm trọng hơn. Hình 4 minh họa hiệu suất trung bình và dải lỗi của ba phương pháp tinh chỉnh trong các trường hợp few-shot: Fine-Tuning, Prompt-Tuning, và SPT. Về các tập dữ liệu, tinh chỉnh trên WebNLG 2020 ổn định hơn so với những tập khác chủ yếu vì nó có nhiều thể hiện được liên quan đến tinh chỉnh trong cùng một hoàn cảnh few-shot như những tập khác. Fine-Tuning ổn định hơn so với những phương pháp khác, điều này hợp lý cho số lượng lớn nhất các tham số có thể tinh chỉnh. Trong khi đó, SPT ngang bằng với Fine-Tuning trên E2E và rõ ràng vượt trội hơn Prompt-Tuning trên DART về mặt ổn định. Ngoài ra, SPT thể hiện một lợi thế hiệu suất gần như tuyệt đối so với Prompt-Tuning trên tập dữ liệu WebNLG 2020 và E2E.

[Hình 3: Tác động của độ dài prompt đến BLEU và TER cho Scaled Prompt-Tuning...]

4.6 Tinh chỉnh đa tác vụ so với tinh chỉnh trung gian
Chúng tôi tiếp tục nghiên cứu cách SPT được đề xuất hoạt động trong paradigm huấn luyện đa tác vụ few-shot và huấn luyện trung gian, nơi WebNLG 2020 và E2E được liên quan. Đối với tinh chỉnh đa tác vụ few-shot, cùng các mẫu n-shot từ hai tập dữ liệu, tương ứng, được trộn và sử dụng trong tinh chỉnh. Theo Hình 5, tinh chỉnh đa tác vụ luôn dẫn đến sự sụt giảm hiệu suất so với tinh chỉnh đơn tác vụ, bất kể phương pháp tinh chỉnh nào có mặt. Khi số lượng shot tăng lên, hiệu suất của Fine-Tuning đa tác vụ trên hai tập dữ liệu dần cải thiện, trong khi SPT đa tác vụ trình bày sự cải thiện biên. Lý do có thể là hai tập dữ liệu khác biệt về nguồn dữ liệu và định dạng, do đó một soft prompt đơn khó có thể dung hòa chúng.

Hình 6 áp dụng paradigm huấn luyện trung gian cho Fine-Tuning và Scaled Prompt-Tuning để tiết lộ khả năng chuyển giao giữa các tác vụ của chúng. Hai hướng chuyển giao là từ WebNLG 2020 đến E2E và từ E2E đến WebNLG 2020. Theo Hình 6 (a), mô hình được fine-tuned trước trên WebNLG 2020 và sau đó fine-tuned trên E2E vượt trội hơn mô hình chỉ được fine-tuned trên E2E trong các trường hợp few-shot. Tuy nhiên, mô hình được fine-tuned trước trên E2E và sau đó fine-tuned trên WebNLG 2020 không rõ ràng vượt trội hơn mô hình chỉ được fine-tuned trên WebNLG 2020. Những điều này thể hiện rằng Fine-Tuning dẫn đến khả năng chuyển giao hạn chế của các tham số trong khi hướng chuyển giao có ý nghĩa quan trọng. Một cách trực quan, WebNLG 2020 phức tạp và thách thức hơn so với tập dữ liệu E2E, và kiến thức mà các tham số thu được sau khi được fine-tuned trên WebNLG 2020 có thể có lợi cho tập dữ liệu E2E, nhưng không phải ngược lại. Đây là lý do tại sao chúng ta thấy lợi thế hiệu suất khi thứ tự fine-tuning trung gian là WebNLG 2020 trước và E2E sau. Ngoài ra, chúng tôi chứng kiến rằng mô hình được fine-tuned trên một tập dữ liệu cho thấy một số khả năng zero-shot trên tập dữ liệu khác.

Đối với SPT trung gian, Hình 6 (b) trình bày một câu chuyện hơi khác. Có sự cải thiện hiệu suất đáng kể trong cả hai hướng chuyển giao, từ WebNLG 2020 đến E2E và từ E2E đến WebNLG 2020, trong các trường hợp few-shot. Hơn nữa, SPT dẫn đến khả năng zero-shot mạnh mẽ hơn của các tham số trên E2E so với Fine-Tuning. Những điều này cung cấp gợi ý sau đây trong các ứng dụng thực tế. Với một số lượng nhỏ thể hiện có sẵn và tài nguyên tính toán hạn chế trong thiết bị, chúng ta có thể trực tiếp lấy các thể hiện từ các tập dữ liệu hiện có trong tác vụ tương tự và tiến hành SPT trung gian. Điều này không cần thiết chi phí gán nhãn bổ sung và dung lượng bộ nhớ lớn trong khi mang lại hiệu suất tốt.

5 Kết luận
Trong công trình này, chúng tôi nghiên cứu các phương pháp PEFT few-shot trên các tác vụ sinh văn bản từ dữ liệu có cấu trúc. Scaled Prompt-Tuning được đề xuất gần như không giới thiệu các tham số có thể huấn luyện và tính toán thêm so với Prompt-Tuning thông thường trong khi cải thiện đáng kể hiệu suất. Hơn nữa, chúng tôi đánh giá toàn diện một số phương pháp PEFT trong các trường hợp few-shot trên ba tập dữ liệu NLG. Các thí nghiệm chứng minh rằng không có phương pháp đơn lẻ nào có thể luôn thắng thế so với những phương pháp khác trong tất cả các hoàn cảnh với một số lượng tương đối nhỏ các tham số có thể huấn luyện. Trong khi đó, hiệu suất của một số phương pháp như Prefix-Tuning và UniPELT có thể xấu đi đáng kể trong các trường hợp few-shot trên các tập dữ liệu thách thức như DART. Chúng tôi tiếp tục nghiên cứu tinh chỉnh đa tác vụ và tinh chỉnh trung gian kết hợp với các phương pháp PEFT trong các hoàn cảnh few-shot. Phương pháp SPT được đề xuất thể hiện khả năng chuyển giao tốt trong các trường hợp few-shot. Điều này cung cấp một lược đồ hứa hẹn trong các tình huống mà một số lượng hạn chế thể hiện từ các tác vụ hạ dòng có sẵn, không giới thiệu bất kỳ chi phí gán nhãn thêm nào hoặc đòi hỏi dung lượng bộ nhớ lớn.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được dịch sang tiếng Việt...]
