# 2205.12148.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2205.12148.pdf
# File size: 950919 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Hyper-X: A Uniﬁed Hypernetwork for Multi-Task Multilingual Transfer
Ahmet Üstün1, Arianna Bisazza1, Gosse Bouma1,
Gertjan van Noord1, Sebastian Ruder2
1University of Groningen
2Google Research
a.ustun@rug.nl
Abstract
Massively multilingual models are promising
for transfer learning across tasks and lan-
guages. However, existing methods are un-
able to fully leverage training data when it is
available in different task-language combina-
tions. To exploit such heterogeneous supervi-
sion, we propose Hyper-X , a single hypernet-
work that uniﬁes multi-task and multilingual
learning with efﬁcient adaptation. This model
generates weights for adapter modules condi-
tioned on both tasks and language embeddings.
By learning to combine task and language-
speciﬁc knowledge, our model enables zero-
shot transfer for unseen languages and task-
language combinations. Our experiments on
a diverse set of languages demonstrate that
Hyper-X achieves the best or competitive gain
when a mixture of multiple resources is avail-
able, while being on par with strong baselines
in the standard scenario. Hyper-X is also con-
siderably more efﬁcient in terms of parame-
ters and resources compared to methods that
train separate adapters. Finally, Hyper-X con-
sistently produces strong results in few-shot
scenarios for new languages, showing the ver-
satility of our approach beyond zero-shot trans-
fer.1
1 Introduction
Transfer learning across languages and tasks has
long been an important focus in NLP (Ruder et al.,
2019). Recent advances in massively multilingual
transformers (MMTs; Devlin et al., 2019; Conneau
et al., 2020) show great success in this area. A
beneﬁt of such models is their ability to transfer
task-speciﬁc information in a high-resource source
language to a low-resource target language (Fig-
ure 1, 1). Alternatively, such models can lever-
age knowledge from multiple tasks for potentially
stronger generalization (Figure 1, 2).
1Our code for Hyper-X will be released at
https://github.com/ahmetustun/hyperx
NER enPre-trained  
Model
Pre-trained  
ModelFine-tuned
Modelar tr
Fine-tuned  
Modelar trPOS en
Single-T ask
Pre-trained  
ModelFine-tuned
Modelar tr
ar trNER
POS
en
en
Multi-T ask
Pre-trained  
ModelFine-tuned  
ModeltrNER
POSar
trNER
ar POSen
en
Mixed-Language Multi-T ask
Figure 1: Experimental settings of different (zero-shot)
cross-lingual transfer scenarios. Single-task (1) is the
standard setting; multi-task (2) enables cross-task trans-
fer. Mixed-language multi-task (3) additionally allows
leveraging task data from multiple source languages for
different tasks.
Over time, many research communities have
been developing resources for speciﬁc languages
of focus (Strassel and Tracey, 2016; Nivre et al.,
2018; Wilie et al., 2020). In practice, it is thus
common for data to be available for different tasks
in a mixture of different languages. For instance,
in addition to English data for both POS tagging
and Named Entity Recognition (NER), a treebank
with POS annotation may be available for Turkish,
while NER data may be available for Arabic. This
example is illustrated in Figure 1, 3.
In contrast to existing cross-lingual transfer
paradigms such as single-task zero-shot transfer
(Hu et al., 2020) or few-shot learning (Lauscher
et al., 2020a), multi-task learning on such a mix-arXiv:2205.12148v3  [cs.CL]  25 Oct 2022

--- PAGE 2 ---
MODEL DESCRIPTION X-Lang. New Lang. M- Task X-Pair ( LT)
MAD-XCross-lingual transfer via language/task adapters 4 4 7 4(Pfeiffer et al., 2020b)
HyperFormerMulti-task learning via shared hypernet adapters 7 7 4 7(Mahabadi et al., 2021b)
Parameter Space Fact.Transfer to unseen task-language pairs via PSF 7 7 4 4(PSF; Ponti et al., 2021)
Hyper-X (this work) Multi-language/task transfer via a uniﬁed hypernet 4 4 4 4
Table 1: A comparison of existing approaches and Hyper-X based on their transfer capabilities. We characterize
approaches based on whether they can perform cross-lingual transfer (X-Lang.) and cross-task transfer via multi-
task learning (M-Task) in the zero-shot setting or to unseen language-task pairs (X-Pair). As a particular case
of cross-lingual transfer, ‘New Lang’ represents the case when transfer is generalizable to unseen languages not
covered by the multilingual pre-trained model.
ture of datasets (mixed-language multi-task) poses
an opportunity to leverage all available data and
to transfer information across both tasks and lan-
guages to unseen task–language combinations
(Ponti et al., 2021).
Standard ﬁne-tuning strategies, however, are lim-
ited in their ability to leverage such heterogeneous
task and language data. Speciﬁcally, MMTs are
prone to suffer from catastrophic forgetting and
interference (Wang et al., 2020) when they are ﬁne-
tuned on multiple sources. Adapters (Houlsby et al.,
2019), a parameter-efﬁcient ﬁne-tuning alternative
are commonly used for transfer either across tasks
(Mahabadi et al., 2021b) or languages (Üstün et al.,
2020) but require training a new adapter for each
new language (Pfeiffer et al., 2020b).
In this paper, we propose a uniﬁed hypernet-
work, HYPER -Xthat is particularly suited to this
setting by leveraging multiple sources of informa-
tion including different languages and tasks within
a single model. The core idea consists of taking
language and task embeddings as input, and gen-
erating adapter parameters via a hypernetwork for
the corresponding task-language combination. By
parameterizing each task and language separately,
Hyper-X enables adaptation to unseen combina-
tions at test time while exploiting all available data
resources.
Additionally, Hyper-X can make seamless use of
masked language modelling (MLM) on unlabelled
data, which enables it to perform zero-shot adapta-
tion to languages not covered by the MMT during
pre-training. MLM also enables Hyper-X to learn
a language representation even without available
task-speciﬁc data.
In sum, our work brings together a number of
successful transfer ‘ingredients’ that have beenexplored in very recent literature (see Table 1),
namely multi-task learning, multilingual learn-
ing, further pre-training, along a high degree of
compute- and time-efﬁciency.
We evaluate Hyper-X for cross-lingual transfer
on two sequence labelling tasks, namely part-of-
speech (POS) tagging and named-entity recogni-
tion (NER) in 16 languages—7 of which are not
covered in pre-training—across the three experi-
mental setups depicted in Figure 1. Our experi-
ments demonstrate that Hyper-X is on par with
strong baselines for cross-lingual transfer from En-
glish. In the multi-task and mixed-language set-
tings, Hyper-X shows a large improvement com-
pared to the standard baselines and matches the per-
formance of the less efﬁcient adapter-based model
due to its ability to leverage heterogeneous sources
of supervision. Analysis highlights that Hyper-X is
superior in terms of efﬁciency–performance trade-
offs. Finally, we evaluate our model in a few-shot
setting, where Hyper-X consistently achieves com-
petitive performance across different languages and
tasks, which suggests the usability of our approach
in continuous learning scenarios.
2 Background
2.1 Adapters
Adapters (Rebufﬁ et al., 2018) are light-weight bot-
tleneck layers inserted into a MMT to ﬁne-tune
the model for a new task (Houlsby et al., 2019),
language (Pfeiffer et al., 2020b) or domain (Bapna
and Firat, 2019). The pre-trained weights of the
transformer remain ﬁxed and only adapter parame-
ters are updated. This setup prevents catastrophic
forgetting (McCloskey and Cohen, 1989) by encap-
sulating specialized knowledge.

--- PAGE 3 ---
Multi-Head  
(Self) AttentionFeed-forwardAdd & Norm2x Feed-forward
layerAdd & Norm
FF DownFF Up
ReLU
FF Up (W h)
Source Projector
NetworkReshape
Parameter V ector
staskslangslayer
Figure 2: Overview of Hyper-X. The hypernetwork (1)
takes the concatenation of task, language and layer em-
beddings as input and generates a ﬂat parameter vec-
tor. Before the ﬁnal transformation, the source projec-
tor network projects the combination of these embed-
dings to a smaller dimension. The parameter vector is
then reshaped and cast to weights of the adapter (2),
which are inserted into a transformer layer (3).
Formally, an adapter module Aiat layer icon-
sists of a down-projection Di2Rhbof the in-
putzi2Rhwith the bottleneck dimension b, a
non-linear function (ReLU) and an up-projection
Ui2Rbh:
Ai(zi) =Ui:ReLU (Di:zi) +zi (1)
where this feed-forward network is followed by a
residual link connecting to the input zi.
2.2 Hypernetworks
A hypernetwork is a network that generates the
weights for a larger main network (Ha et al., 2016).
When using a hypernetwork, the main model learns
the desired objective (e.g. classiﬁcation) whereas
the hypernetwork takes an auxiliary input (usu-
ally an embedding) that represents the structure of
the weights and generates parameters of the main
model. A hypernetwork thus enables learning a sin-
gle parameter space shared across multiple transfer
dimensions such as tasks (Mahabadi et al., 2021b)
or languages (Platanios et al., 2018) while also al-
lowing input-speciﬁc reparametrization.
More concretely, a hypernetwork is a generator
functionHthat takes an embedding s(h)2Rds
representing the input sources, and generates themodel parameters :
,H(s(h)) (2)
WhileHcan be any differentiable function, it is
commonly parameterized as a simple linear trans-
form ( Wh) that generates a ﬂat vector with the
dimension of da, which corresponds to the total
number of model parameters. Whis shared across
all input sources, enabling maximum sharing.
3 Hyper-X
We propose, Hyper-X, an efﬁcient adaptation of
a MMT by exploiting multiple sources of infor-
mation for transfer to an unseen language or task-
language pairs. Speciﬁcally, Hyper-X learns to
combine task and language-speciﬁc knowledge in
the form of embeddings using a hypernetwork.
Conditioned on the task and language embeddings,
the hypernetwork generates composite adapter lay-
ers for the corresponding task-language combina-
tion (e.g. NER in Turkish), thereby enabling trans-
fer to arbitrary task-language pairs at test time. Fig-
ure 2 provides an overview of our model.
By jointly learning from task and language in-
formation, Hyper-X overcomes some of the lim-
itations of prior work: Unlike adapter-based ap-
proaches (Pfeiffer et al., 2020b; Üstün et al., 2020)
that transfer cross-lingual information only to the
task of the task adapter, our model is capable
of leveraging supervision—and positive transfer—
from both multiple tasks and languages. Moreover,
unlike Ponti et al. (2021) who require annotated
data in one of the target tasks for each language,
Hyper-X is able to perform zero-shot transfer even
when there is no annotated data from any of the
target tasks, by using MLM as an auxiliary task for
each language.
3.1 A Hypernetwork for Task-Language
Adapters
We use a standard hypernetwork as the parameter
generator function. However, instead of generat-
ing the full model parameters, our hypernetwork
generates the parameters for each adapter layer.
Concretely, the hypernetwork Hgenerates adapter
parameters where each adapter layer Aiconsists of
down and up-projection matrices ( Di,Ui):
Di;Ui,H(s(h)) (3)

--- PAGE 4 ---
Decoupling Tasks and Languages In Hyper-X,
we condition the parameter generation on the in-
put task and language. Therefore, given a com-
bination of task t2 ft1; :::; t mgand language
l2 fl1; :::; l ng, the source embedding contains
knowledge from both sources: s(h)(t; l). We
parameterize each task and language via separate
embeddings, which enables adaptation to any task-
language combination. Task and language embed-
dings ( s(t);s(l)) are low-dimensional vectors that
are learned together with the parameters of the hy-
pernetwork. During training, for each mini-batch
we update these embeddings according to the task
and language that the mini-batch is sampled from.
MLM as Auxiliary Task Hyper-X learns sepa-
rate tasks and languages embeddings—as long as
the task and language have been seen during train-
ing. As annotated data in many under-represented
languages is limited, we employ MLM as an auxil-
iary task during training to enable computing em-
beddings for every language. Moreover, MLM en-
ables a better zero-shot performance for languages
that are not included in MMT pre-training (see
§ 6.2 for a detailed analysis of the impact of MLM).
Sharing Across Layers In addition to the task
and language embedding, we learn a layer embed-
ding s(i)(Mahabadi et al., 2021b; Ansell et al.,
2021) corresponding to the transformer layer index
iwhere the respective adapter module is plugged in.
Since Hyper-X generates an adapter for each Trans-
former layer, learning independent layer embed-
dings allows for information sharing across those
layers. Moreover, as layer embeddings allow the
use of a single hypernetwork for all Transformer
layers, they reduce the trainable parameters, i.e.,
size of the hypernetwork, by a factor corresponding
to the number of layers of the main model.
Combining Multiple Sources To combine lan-
guage, task and layer embeddings, we use a simple
source projector network Psas part of our hypernet-
work. This module consisting of two feed-forward
layers with a ReLU activation takes the concatena-
tion of the three embeddings and learns a combined
embedding s(p)2Rdpwith a potentially smaller
dimension:
s(h)=s(l)s(t)s(i)(4)
s(p)=Ps(s(h)) (5)
where s(h)2Rdsrefers to the concatenated em-
bedding before the Ps, with ds=dl+dt+di.This component enables learning how to combine
source embeddings while also reducing the total
number of trainable parameters.
4 Experiments
Dataset and Languages We conduct experi-
ments on two downstream tasks: part-of-speech
(POS) tagging and named entity recognition (NER).
For POS tagging, we use the Universal Dependen-
cies (UD) 2.7 dataset (Zeman et al., 2020) and for
NER, we use WikiANN (Pan et al., 2017) with the
train, dev and test splits from Rahimi et al. (2019).
In addition to these two tasks, we also use masked
language modelling (MLM) on Wikipedia articles
as an auxiliary task. We limit the number of sen-
tences from Wikipedia to 100K for each language,
in order to control the impact of dataset size and to
reduce the training time.
For the language selection, we consider: (i) typo-
logical diversity based on language family, script
and morphosyntactic attributes; (ii) a combination
of high-resource and low-resource languages based
on available data in downstream task; (iii) pres-
ence in the pre-training data of mBERT; and (iv)
presence of a language in the two task-speciﬁc
datasets.2We provide the details of the language
and dataset selection in Appendix A.
Experimental Setup We evaluate Hyper-X for
zero-shot transfer in three different settings: (1)
English single-task, where we train the models
only on English data for each downstream task sep-
arately. (2) English multi-task, where the mod-
els are trained on English POS and NER data at
the same time. (3) Mixed-language multi-task,
where we train the models in a multi-task setup,
but instead of using only English data for both
POS and NER, we use a mixture of task-language
combinations. In order to measure zero-shot per-
formance in this setup, following Ponti et al. (2021)
we create two different partitions from all possible
language-task combinations in such a way that a
task-language pair is always unseen for one of the
partitions (e.g. NER-Turkish and POS-Arabic in
Figure 1). Details of partitions and our partitioning
strategy are given in Appendix A.
2(i) and (ii) are necessary for a realistic setting and to
evaluate full-scale cross-lingual capabilities; (iii) allows us to
measure if models are able to extend the limits of the MMT;
(iv) enables us to assess supervision from a mixture of task
and language combinations.

--- PAGE 5 ---
4.1 Baselines and Model Variants
mBERT (Devlin et al., 2019) is a MMT that is
pre-trained for 104 languages. We use mBERT
by ﬁne-tuning all the model parameters on the
available sources. As this standard approach
enables cross-lingual transfer from both a single
source or a set of language-task combinations,
we compare it to Hyper-X in all three settings.
Moreover, we use mBERT as the base model for
both Hyper-X and the other baselines.
MAD-X (Pfeiffer et al., 2020b) is an adapter-based
modular framework for cross-lingual transfer learn-
ing based on MMTs. It combines a task-speciﬁc
adapter with language-speciﬁc adapters that are
independently trained for each language using
MLM. We train MAD-X language adapters on the
same Wikipedia data that is used for Hyper-X, for
all languages with a default architecture.3Finally,
for the mixed-language setup, as the original
MAD-X does not allow standard multi-task
training, we train the task adapters by using
multiple source languages but for NER and POS
separately. We call this model MAD-X MS .
Parameter Space Factorization (Ponti et al.,
2021) is a Bayesian framework that learns a
parameter generator from multiple tasks and
languages for the softmax layer on top of a
MMT. However, if a language lacks annotated
training data, this model cannot learn the required
latent variable for the corresponding language.
Therefore, we evaluate this baseline only for
the mixed-language multi-task setting using the
same partitions as Hyper-X. We use the original
implementation with default hyper-parameters and
low-rank factorization.
Model Variants We evaluated two variants of
Hyper-X in order to see the impact of Hypernet-
work size: Hyper-X Base model ﬁne-tunes 76m
parameters ( ds= 192 ), compatible with MAD-X
in terms of total number of trainable parameters,
and Hyper-X Small updates only 13m parameters
(ds= 32 ). Table 3 shows the parameter counts
together with the corresponding runtime.
3MAD-X also introduce ‘invertible adapters’ that adapt
token embeddings. We did not use them for simpler experi-
mental setup. Note that, as our hypernetwork is able to gener-
ate parameters for any component, it is possible to generate
invertible adapters as in MAD-X.4.2 Training Details
For all the experiments, we used a batch size of
32 and a maximum sequence length of 256. We
trained Hyper-X for 100,000 updates steps by us-
ing a linearly decreasing learning rate of 1e-4 with
4000 warm-up steps. We evaluated checkpoints ev-
ery 5,000 steps, and used the best checkpoint w.r.t.
the average validation score for testing. As for
baselines, we trained mBERT and MAD-X tasks
adapters for 20 epochs by using learning rate of
1e-5 and 1e-4 respectively with the same scheduler
and warm-up steps. Since MAD-X requires pre-
requisite language adapters, we trained language
adapters for 100,000 steps for each language sepa-
rately.
In terms of model size, we use a bottleneck
dimension of 256 to learn adapters for Hyper-X.
Similarly, we train language and adapters with di-
mension of 256 and 48 for MAD-X to create a
comparable baseline. In Hyper-X, as input to the
hypernetwork, dimensions for task, language and
layer embeddings are all set to 64 (total 192). Dur-
ing training, we create homogeneous mini-batches
for each task-language combination to learn the
corresponding embeddings together with the hy-
pernetwork. Moreover, following Mahabadi et al.
(2021b), we also update the original layer-norm
parameters. During multi-task training, we use
temperature-based sampling with T= 5 to bal-
ance each task-language pair during training (See
Appendix § B.1 for details).
5 Zero-shot Transfer Results
Table 2 shows the aggregate zero-shot results in
NER and POS tagging respectively. In addition
to the average scores across all 15 zero-shot lan-
guages, we show the average of the 8 ‘seen’ and 7
‘unseen’ languages separately with respect to lan-
guage coverage of mBERT. We present results for
English single-task, English multi-task and Mixed-
language multi-task settings.
Overall, Hyper-X Base performs on par with the
strongest baseline when transferring from English.
In the presence of additional sources, such as a mix-
ture of task-language pairs, Hyper-X outperforms
both mBERT and parameter space factorization
(PSF). In comparison to MAD-X, Hyper-X gener-
ally performs better on seen languages. We relate
this to the uniﬁed hypernetwork enabling maxi-
mum sharing between languages and higher utiliza-
tion of the pre-trained capacity in contrast to the

--- PAGE 6 ---
Named-Entity Part-of-Speech
#Params / Recognition Tagging
Source Method Time SEEN UNSEEN ALL SEEN UNSEEN ALL
mBERT 177m / 2h 53.4 40.3 47.3 66.3 48.9 58.1
MAD-X 76m / 116h 54.3 51.1 52.8 67.7 62.6 65.4
English Hyper-X Small 13m / 16h 54.2 47.7 51.2 66.5 57.9 62.5
(Single-Task) Hyper-X Base 76m / 18h 54.4 50.7 52.7 67.8 58.7 63.5
mBERT 177m / 2h 53.8 40.4 47.6 65.8 47.7 57.3
English Hyper-X Small 13m / 16h 52.2 49.3 50.8 65.1 57.9 61.7
(Multi-Task) Hyper-X Base 76m / 18h 54.4 51.1 52.9 67.0 59.7 63.6
mBERT 177m / 2h 56.4 48.7 52.8 67.2 54.7 61.4
PSF 185m / 4h 58.1 54.1 56.2 70.4 53.8 62.7
MAD-X MS 76m / 116h 62.4 62.2 62.3 70.7 67.0 69.0
Mixed-Language Hyper-X Small 13m / 16h 62.0 58.3 60.3 70.7 63.2 67.2
(Multi-Task) Hyper-X Base 76m / 18h 63.3 61.0 62.3 71.5 63.8 67.9
Table 2: Zero-shot cross-lingual transfer results averaged over 3 runs on Named-Entity Recognition (NER; F1)
and Part-of-Speech Tagging (POS; Accuracy) for mBERT, MAD-X (Pfeiffer et al., 2020b), parameter space factor-
ization (PSF; Ponti et al., 2021) and Hyper-X. We highlight the best results per-setting in bold. We also report the
total number of parameters and ﬁne-tuning time for all models. Note that Hyper-X corresponds to a single model
trained for each partition while MAD-X consists of Nindependently trained adapters for each task and language.
MAD-X MS refers to an adapted version of the original model trained on multiple source languages but each task
separately.
isolated adapters. On unseen languages, Hyper-X
is outperformed by MAD-X in most cases. How-
ever, we emphasize that MAD-X requires training
separate language adapters for each new language,
which makes it considerably less resource-efﬁcient
than Hyper-X (see § 6.1).
English Single-Task When English is used as
the only source language for each task separately,
Hyper-X (Base) performs on par with MAD-X for
NER (52.7 vs 52.8 F1) but falls behind for POS
tagging (63.5 vs 65.4 Acc.) on average. Both mod-
els signiﬁcantly outperform mBERT. Looking at
the individual language results, Hyper-X performs
slightly better on ‘seen’ languages compared to
MAD-X in NER and POS tagging respectively.
For ‘unseen’ languages, both MAD-X and Hyper-X
beneﬁt from MLM, which results in large improve-
ments with respect to mBERT. Between the two
models, MAD-X achieves a higher average score
in both NER and POS tagging.
English Multi-Task In a multi-task setting
where only English data is available, ﬁne-tuning
mBERT for both target tasks at the same time gives
mixed results compared to single-task training—
in line with previous ﬁndings noting catastrophic
forgetting and interference in MMTs (Wang et al.,
2020). Hyper-X Base, on the other hand, shows a
small but consistent improvement on the majorityof languages, with 0.2 (F1) and 0.1 (Acc.) aver-
age increase in NER and POS tagging respectively.
This conﬁrms that Hyper-X is able to mitigate inter-
ference while allowing for sharing between tasks
when enough capacity is provided.4
Mixed-Language Multi-Task In this setting, a
mixture of language data is provided for NER
and POS via two separate training partitions while
keeping each task-language pair unseen in one of
these partitions. All the models including mBERT
achieve better zero-shot scores compared to the
previous settings. Among the baselines, parameter
space factorization (PSF) gives a larger improve-
ment compared to mBERT on both tasks, indicat-
ing the importance of task- and language-speciﬁc
parametrization for adapting a MMT. Hyper-X
Base produces the largest performance gain among
the models that trains only a single model: it
achieves 9.0 (F1) and 4.3 (Acc.) average increase
for NER and POS. Although both PSF and Hyper-
X enable adaptation conditioned on a mixture of
task and language combinations, we relate the dif-
ference between PSF and Hyper-X to the contrast
in parameter generation. PSF only generates pa-
rameters of the softmax layer and is thus unable
to adapt deeper layers of the model. Hyper-X, on
4MAD-X learns independent adapters for each target task,
which does not allow for positive cross-task transfer.

--- PAGE 7 ---
Model #Params. Training Time
mBERT 177m 2h
PSF 185m 4h
MAD-X 76m 116h
,!Language Adapters 4.7m x l 7h xl
,!Task Adapters 0.9m x t 2h xt
Hyper-X Small 13m 16h
Hyper-X Base 76m 18h
Table 3: Compute efﬁciency with respect to number
of ﬁne-tuned parameters and training time for mBERT,
PSF, MAD-X and Hyper-X. Training time includes
both NER and POS-tagging. For MAD-X, the total
number of parameters and training time is calculated
for 16 ( l) languages and 2 ( t) tasks.
the other hand, generates adapter layer parameters
inserted throughout the model, which provide a
higher degree of adaptation ﬂexibility. Hyper-X
outperforms PSF particularly on unseen languages
as it beneﬁts from MLM as an auxiliary task.
Finally, Hyper-X tends to perform slightly better
on seen languages compared to the adapted multi-
source version of MAD-X. However, MAD-X out-
performs Hyper-X on unseen languages by 1.2 (F1)
and 2.8 (Acc.) for NER and POS respectively. Be-
sides the expected beneﬁts of independently trained
language adapters in MAD-X, we relate this to the
limited cross-task supervision for unseen languages
in Hyper-X for this setting. Especially, when the
target task is POS, most of the unseen languages
have only 100 sentences available in NER dataset,
which leaves only a little margin for improvements.
6 Analysis
6.1 Parameter and Time Efﬁciency
Table 3 shows the ﬁne-tuned parameter counts
and the training time required for the baselines
and Hyper-X models. Unlike mBERT, PSF and
Hyper-X, MAD-X consists of 16 and 2 indepen-
dently trained language and task adapters respec-
tively. In terms of parameter efﬁciency, MAD-X
and Hyper-X Base models correspond to 43% of
mBERT’s parameters. However, in terms of train-
ing time, Hyper-X Base is trained only once for
about 18 hours, as opposed to MAD-X’s consid-
erably high total training time (116 hours in to-
tal). Thus, considering the competitive zero-shot
performances across different languages and set-
tings, Hyper-X Base provides a better efﬁciency-
performance trade-off. Furthermore, in the case of
NER POSFigure 3: Impact of auxiliary MLM traning on zero-
shot results for SEEN and UNSEEN language groups on
NER and POS tagging, when MLM data removed from
the corresponding groups incrementally.
adding more languages, MAD-X’s parameter count
and training time increase linearly with the number
of new languages, while Hyper-X’s computational
cost remains the same.
As Hyper-X model variants, we evaluated
two different sizes of the source embedding
(ds; 32!192). Although Hyper-X Small is much
more parameter-efﬁcient (7.2% of mBERT’s pa-
rameters) and takes slightly less time to train (16h),
its zero-shot performance is signiﬁcantly lower
than the base model, especially for unseen lan-
guages. Nevertheless, Hyper-X Small remains a
valid alternative for particularly ‘seen’ languages.
6.2 Impact of Auxiliary MLM Training
Figure 3 demonstrates the impact of auxiliary
MLM training in Hyper-X Base for the mixed-
language multi-task setting. As this setting pro-
vides training instances for each task and language,
we evaluated the impact of MLM by removing
the corresponding Wikipedia data ﬁrst for ‘seen’
languages, then for ‘all’ languages. As shown in
the ﬁgure, although the availability of MLM data
slightly increases seen language performance, it
mainly boosts the scores in unseen languages: +6.2
F1 and +10.5 Acc. for NER and POS respectively.
Furthermore, when MLM data is removed for only
seen languages, Hyper-X can mostly recover perfor-
mance on seen languages, conﬁrming the dominant
effect of MLM on unseen languages.
6.3 Impact of Source Languages
In the mixed-language multi-task setting, we delib-
erately avoid grouping languages from same fam-
ilies to different partitions, in order to restrict the
transfer from the same-language family instances,
and to observe the effect of cross-task supervision.

--- PAGE 8 ---
NER POSFigure 4: Impact of source language for Hyper-X Base
performance on SEEN ,UNSEEN language groups in
mixed-language multi-task setup.
However, we also evaluate the impact of source
languages in this setup, to measure the degree of
potential positive transfer. To this end, we switched
the partitions of kk,mt,yue , so that all of them
will likely beneﬁt from a high-resource language
from the same family for the same target task. Fig-
ure 4 and 5 shows the aggregated results in both
Hyper-X Base and mBERT. Firstly, both models
beneﬁt from positive transfer. Secondly, although
the relative increase in mBERT is slightly higher
Hyper-X still outperforms mBERT with a large
margin, showing the robustness of our model with
regard to different partitions.
6.4 Few-shot Transfer
Fine-tuning an MMT with a few target instances
has been shown to increase zero-shot performances
(Lauscher et al., 2020b). Therefore, we evaluate
Hyper-X for few-shot transfer on 5 languages—3
of which are high-resource and covered by mBERT
and 2 are low-resource and unseen. To this end, we
further ﬁne-tune Hyper-X and the corresponding
baselines that are trained initially in the English
multi-task by using 5, 10, 20, and 50 training in-
stances for each language separately on NER and
POS-tagging (see details in Appendix §D).
Figure 6 presents the average results compar-
ing mBERT to MAD-X. Similar to the zero-shot
results, on seen languages, Hyper-X constantly pro-
vides better adaptation than both baselines for NER
and POS. On unseen languages, MAD-X gives the
best result on average. This is because MAD-X
starts with better initial representations for Maltese
and Uyghur. When more samples are provided
Hyper-X reduces the initial gap. Overall, Hyper-X
consistently achieves the best or competitive perfor-
mance on the majority of the experiments, except
‘unseen’ languages for POS tagging, showing the
NER POSFigure 5: Impact of source language for mBERT per-
formance on SEEN ,UNSEEN language groups in mixed-
language multi-task setup.
effectiveness of our approach beyond the standard
zero-shot transfer. Taken together with the parame-
ter and training efﬁciency, these results show that
Hyper-X can be easily extended to new languages
without incurring large computing costs.
7 Related Work
Adapters As a parameter-efﬁcient alternative to
standard ﬁne-tuning, adapters have been used for
quick training (Rücklé et al., 2021), multi-task
learning (Stickland and Murray, 2019) and knowl-
edge composition (Pfeiffer et al., 2021a; Wang
et al., 2021; Poth et al., 2021). Moreover, Ma-
habadi et al. (2021a) and He et al. (2022a) ex-
tended adapters for better performance with fewer
parameters. In the context of multilingual transfer,
adapters enable allocation of additional language-
speciﬁc capacity, thereby mitigating the ‘curse of
multilinguality’ (Üstün et al., 2020). Such lan-
guage adapters (Pfeiffer et al., 2020b; Ansell et al.,
2021) achieve high zero-shot results when com-
bined with task adapters and enable generalization
to languages unseen during pre-training via MLM-
based adaptation (Pfeiffer et al., 2021b). Philip et al.
(2020) and Üstün et al. (2021) also used monolin-
gual adapters for zero-shot and unsupervised NMT.
Hypernetworks in NLP Tay et al. (2021) pro-
pose a multi-task model that uses a hypernet-
work to condition on input to learn task-speciﬁc
reparametrizations. Similarly, Mahabadi et al.
(2021b) generate task-speciﬁc adapters via a hy-
pernetwork. Recently, He et al. (2022b) use a
hypernetwork to generate prompts. For multilin-
gual learning, where the input sources correspond
to language embeddings, Üstün et al. (2020) and
Ansell et al. (2021) learn these embeddings from
the typological feature vectors of languages, en-
abling generalization to unseen languages based

--- PAGE 9 ---
10 20 30 40 50
Number of Samples5055606570F1
NER ('seen')
mbert
madx
hyperx-small
hyperx-base
10 20 30 40 50
Number of Samples3545556575F1
NER ('unseen')
mbert
madx
hyperx-small
hyperx-base
10 20 30 40 50
Number of Samples7075808590Accuracy
POS Tagging ('seen')
mbert
madx
hyperx-small
hyperx-base
10 20 30 40 50
Number of Samples5060708090Accuracy
POS Tagging ('unseen')
mbert
madx
hyperx-small
hyperx-baseFigure 6: Few-shot transfer for 5 new languages on NER, POS-tagging. Results are averaged over SEEN
(ar,tr,zh ) and UNSEEN (mt,ug ) languages. In ﬁrst three settings, both Hyper-X models competitive or better
than other models. Results for all few-shot experiments are given in Appendix D
on a hypernetwork. In a similar spirit to our work,
parameter space factorization (PSF; Ponti et al.,
2021), learns task and language-speciﬁc embed-
dings from seen task-language combinations. How-
ever, unlike our model, these embeddings are used
for task/language-speciﬁc parametrization in the
softmax layer.
8 Conclusion
We have proposed Hyper-X, a novel approach for
multi-task multilingual transfer learning, based
on a uniﬁed hypernetwork that leverages hetero-
geneous sources of information, such as multi-
ple tasks and languages. By learning to generate
composite adapters for each task-language com-
binations that modify the parameters of a pre-
trained multilingual transformer, Hyper-X allows
for maximum information sharing and enables zero-
shot prediction for arbitrary task-language pairs at
test time. Through a number of experiments, we
demonstrate that Hyper-X is competitive with the
state-of-the-art when transferring from a source
language. When a mixture of tasks and languages
is available, Hyper-X outperforms several strong
baselines on many languages, while being more
parameter and time efﬁcient. Finally, we show that
for few-shot transfer, Hyper-X is a strong option
with a less computing cost than baselines for the
initial task adaptation.
9 Limitations
Firstly, although our experiments show the poten-
tial of Hyper-X to beneﬁt from multiple tasks for
zero-shot transfer, so far we evaluated our model
on a limited set of tasks: NER and POS-tagging,
which may limit the generalizability of our model
to other tasks.
Secondly, for the few-shot transfer, we limit our
experiments to languages that we learn via MLMand to existing tasks. Our work does not include
languages without MLM data as well as completely
new tasks. Learning the task and language embed-
dings separately, however, creates a possibility to
interpolate existing embeddings for new languages
or new tasks, which especially may work for the
few-shot learning. We leave exploration of these
two limitations to future work.
Acknowledgements
We would like to thank Noah Constant, Asa Cooper
Stickland and the anonymous reviewers for their
helpful feedback on a previous version of this paper.
We also would like to thank the Center for Infor-
mation Technology of the University of Groningen
for providing access to the Peregrine HPC cluster.
References
Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Se-
bastian Ruder, Goran Glavaš, Ivan Vuli ´c, and Anna
Korhonen. 2021. MAD-G: Multilingual adapter
generation for efﬁcient cross-lingual transfer. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021 , pages 4762–4781, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Ankur Bapna and Orhan Firat. 2019. Simple, scal-
able adaptation for neural machine translation. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 1538–
1548, Hong Kong, China. Association for Computa-
tional Linguistics.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale. In
Proceedings of the 58th Annual Meeting of the Asso-

--- PAGE 10 ---
ciation for Computational Linguistics , pages 8440–
8451.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186.
David Ha, Andrew Dai, and Quoc V Le. 2016. Hyper-
networks. In International Conference on Learning
Representations .
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Kirkpatrick, and Graham Neubig. 2022a. Towards a
uniﬁed view of parameter-efﬁcient transfer learning.
InProceedings of ICLR 2022 .
Yun He, Huaixiu Steven Zheng, Yi Tay, Jai Gupta,
Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuang Li,
Zhao Chen, Donald Metzler, Heng-Tze Cheng, and
Ed H. Chi. 2022b. HyperPrompt: Prompt-based
Task-Conditioning of Transformers. arXiv preprint
arXiv:2203.00759 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efﬁcient transfer learning for nlp.
InInternational Conference on Machine Learning ,
pages 2790–2799.
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-
ham Neubig, Orhan Firat, and Melvin Johnson.
2020. XTREME: A Massively Multilingual Multi-
task Benchmark for Evaluating Cross-lingual Gener-
alization. In Proceedings of ICML 2020 .
Anne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and
Goran Glavaš. 2020a. From Zero to Hero: On
the Limitations of Zero-Shot Cross-Lingual Trans-
fer with Multilingual Transformers. In Proceedings
of EMNLP 2020 .
Anne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and
Goran Glavaš. 2020b. From zero to hero: On the
limitations of zero-shot language transfer with mul-
tilingual Transformers. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 4483–4499, On-
line. Association for Computational Linguistics.
Rabeeh Karimi Mahabadi, James Henderson, and Se-
bastian Ruder. 2021a. Compacter: Efﬁcient low-
rank hypercomplex adapter layers. In Advances in
neural information processing systems .
Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa
Dehghani, and James Henderson. 2021b. Parameter-
efﬁcient multi-task ﬁne-tuning for transformers via
shared hypernetworks. In Proceedings of the 59th
Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International JointConference on Natural Language Processing (Vol-
ume 1: Long Papers) , pages 565–576, Online. As-
sociation for Computational Linguistics.
Michael McCloskey and Neal J Cohen. 1989. Catas-
trophic interference in connectionist networks: The
sequential learning problem. In Psychology of learn-
ing and motivation , volume 24, pages 109–165. El-
sevier.
Joakim Nivre, Mitchell Abrams, Željko Agi ´c, Lars
Ahrenberg, Lene Antonsen, Katya Aplonova,
Maria Jesus Aranzabe, Gashaw Arutie, Masayuki
Asahara, Luma Ateyah, Mohammed Attia, Aitz-
iber Atutxa, Liesbeth Augustinus, Elena Badmaeva,
Miguel Ballesteros, Esha Banerjee, Sebastian Bank,
Verginica Barbu Mititelu, Victoria Basmov, John
Bauer, Sandra Bellato, Kepa Bengoetxea, Yevgeni
Berzak, Irshad Ahmad Bhat, Riyaz Ahmad Bhat, Er-
ica Biagetti, Eckhard Bick, Rogier Blokland, Vic-
toria Bobicev, Carl Börstell, Cristina Bosco, Gosse
Bouma, Sam Bowman, Adriane Boyd, Aljoscha Bur-
chardt, Marie Candito, Bernard Caron, Gauthier
Caron, Gül¸ sen Cebiro ˘glu Eryi ˘git, Flavio Massim-
iliano Cecchini, Giuseppe G. A. Celano, Slavomír
ˇCéplö, Savas Cetin, Fabricio Chalub, Jinho Choi,
Yongseok Cho, Jayeol Chun, Silvie Cinková, Au-
rélie Collomb, Ça ˘grı Çöltekin, Miriam Connor, Ma-
rine Courtin, Elizabeth Davidson, Marie-Catherine
de Marneffe, Valeria de Paiva, Arantza Diaz de Ilar-
raza, Carly Dickerson, Peter Dirix, Kaja Dobrovoljc,
Timothy Dozat, Kira Droganova, Puneet Dwivedi,
Marhaba Eli, Ali Elkahky, Binyam Ephrem, Tomaž
Erjavec, Aline Etienne, Richárd Farkas, Hector
Fernandez Alcalde, Jennifer Foster, Cláudia Fre-
itas, Katarína Gajdošová, Daniel Galbraith, Mar-
cos Garcia, Moa Gärdenfors, Sebastian Garza, Kim
Gerdes, Filip Ginter, Iakes Goenaga, Koldo Go-
jenola, Memduh Gökırmak, Yoav Goldberg, Xavier
Gómez Guinovart, Berta Gonzáles Saavedra, Matias
Grioni, Normunds Gr ¯uz¯ıtis, Bruno Guillaume, Cé-
line Guillot-Barbance, Nizar Habash, Jan Haji ˇc, Jan
Hajiˇc jr., Linh Hà M ˜y, Na-Rae Han, Kim Harris,
Dag Haug, Barbora Hladká, Jaroslava Hlavá ˇcová,
Florinel Hociung, Petter Hohle, Jena Hwang, Radu
Ion, Elena Irimia, O .lájídé Ishola, Tomáš Jelínek, An-
ders Johannsen, Fredrik Jørgensen, Hüner Ka¸ sıkara,
Sylvain Kahane, Hiroshi Kanayama, Jenna Kan-
erva, Boris Katz, Tolga Kayadelen, Jessica Ken-
ney, Václava Kettnerová, Jesse Kirchner, Kamil
Kopacewicz, Natalia Kotsyba, Simon Krek, Sooky-
oung Kwak, Veronika Laippala, Lorenzo Lam-
bertino, Lucia Lam, Tatiana Lando, Septina Dian
Larasati, Alexei Lavrentiev, John Lee, Phuong
Lê H `ông, Alessandro Lenci, Saran Lertpradit, Her-
man Leung, Cheuk Ying Li, Josie Li, Keying
Li, KyungTae Lim, Nikola Ljubeši ´c, Olga Logi-
nova, Olga Lyashevskaya, Teresa Lynn, Vivien
Macketanz, Aibek Makazhanov, Michael Mandl,
Christopher Manning, Ruli Manurung, C ˘at˘alina
M˘ar˘anduc, David Mare ˇcek, Katrin Marheinecke,
Héctor Martínez Alonso, André Martins, Jan
Mašek, Yuji Matsumoto, Ryan McDonald, Gus-
tavo Mendonça, Niko Miekka, Margarita Misir-

--- PAGE 11 ---
pashayeva, Anna Missilä, C ˘at˘alin Mititelu, Yusuke
Miyao, Simonetta Montemagni, Amir More, Laura
Moreno Romero, Keiko Sophie Mori, Shinsuke
Mori, Bjartur Mortensen, Bohdan Moskalevskyi,
Kadri Muischnek, Yugo Murawaki, Kaili Müürisep,
Pinkey Nainwani, Juan Ignacio Navarro Horñiacek,
Anna Nedoluzhko, Gunta Nešpore-B ¯erzkalne, Lu-
ong Nguy ˜ên Thi ., Huy `ên Nguy ˜ên Thi .Minh, Vitaly
Nikolaev, Rattima Nitisaroj, Hanna Nurmi, Stina
Ojala, Adédayo .Olúòkun, Mai Omura, Petya Osen-
ova, Robert Östling, Lilja Øvrelid, Niko Partanen,
Elena Pascual, Marco Passarotti, Agnieszka Pate-
juk, Guilherme Paulino-Passos, Siyao Peng, Cenel-
Augusto Perez, Guy Perrier, Slav Petrov, Jussi Piitu-
lainen, Emily Pitler, Barbara Plank, Thierry Poibeau,
Martin Popel, Lauma Pretkalnin ,a, Sophie Prévost,
Prokopis Prokopidis, Adam Przepiórkowski, Ti-
ina Puolakainen, Sampo Pyysalo, Andriela Rääbis,
Alexandre Rademaker, Loganathan Ramasamy,
Taraka Rama, Carlos Ramisch, Vinit Ravishankar,
Livy Real, Siva Reddy, Georg Rehm, Michael
Rießler, Larissa Rinaldi, Laura Rituma, Luisa
Rocha, Mykhailo Romanenko, Rudolf Rosa, Davide
Rovati, Valentin Ros ,ca, Olga Rudina, Jack Rueter,
Shoval Sadde, Benoît Sagot, Shadi Saleh, Tanja
Samardži ´c, Stephanie Samson, Manuela Sanguinetti,
Baiba Saul ¯ıte, Yanin Sawanakunanon, Nathan
Schneider, Sebastian Schuster, Djamé Seddah, Wolf-
gang Seeker, Mojgan Seraji, Mo Shen, Atsuko Shi-
mada, Muh Shohibussirri, Dmitry Sichinava, Na-
talia Silveira, Maria Simi, Radu Simionescu, Katalin
Simkó, Mária Šimková, Kiril Simov, Aaron Smith,
Isabela Soares-Bastos, Carolyn Spadine, Antonio
Stella, Milan Straka, Jana Strnadová, Alane Suhr,
Umut Sulubacak, Zsolt Szántó, Dima Taji, Yuta
Takahashi, Takaaki Tanaka, Isabelle Tellier, Trond
Trosterud, Anna Trukhina, Reut Tsarfaty, Fran-
cis Tyers, Sumire Uematsu, Zde ˇnka Urešová, Lar-
raitz Uria, Hans Uszkoreit, Sowmya Vajjala, Daniel
van Niekerk, Gertjan van Noord, Viktor Varga,
Eric Villemonte de la Clergerie, Veronika Vincze,
Lars Wallin, Jing Xian Wang, Jonathan North
Washington, Seyi Williams, Mats Wirén, Tsegay
Woldemariam, Tak-sum Wong, Chunxiao Yan,
Marat M. Yavrumyan, Zhuoran Yu, Zden ˇek Žabokrt-
ský, Amir Zeldes, Daniel Zeman, Manying Zhang,
and Hanzhi Zhu. 2018. Universal dependencies 2.3.
LINDAT/CLARIAH-CZ digital library at the Insti-
tute of Formal and Applied Linguistics (ÚFAL), Fac-
ulty of Mathematics and Physics, Charles Univer-
sity.
Xiaoman Pan, Boliang Zhang, Jonathan May, Joel
Nothman, Kevin Knight, and Heng Ji. 2017. Cross-
lingual name tagging and linking for 282 languages.
InProceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 1946–1958, Vancouver,
Canada. Association for Computational Linguistics.
Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,
Kyunghyun Cho, and Iryna Gurevych. 2021a.
AdapterFusion: Non-destructive task composition
for transfer learning. In Proceedings of the 16thConference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Volume ,
pages 487–503, Online. Association for Computa-
tional Linguistics.
Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aish-
warya Kamath, Ivan Vuli ´c, Sebastian Ruder,
Kyunghyun Cho, and Iryna Gurevych. 2020a.
Adapterhub: A framework for adapting transform-
ers. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2020): Systems Demonstrations , pages 46–
54, Online. Association for Computational Linguis-
tics.
Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Sebas-
tian Ruder. 2020b. Mad-x: An adapter-based frame-
work for multi-task cross-lingual transfer. In Pro-
ceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing .
Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Sebas-
tian Ruder. 2021b. UNKs everywhere: Adapting
multilingual language models to new scripts. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 10186–
10203, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Jerin Philip, Alexandre Berard, Matthias Gallé, and
Laurent Besacier. 2020. Monolingual adapters for
zero-shot neural machine translation. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pages
4465–4470, Online. Association for Computational
Linguistics.
Emmanouil Antonios Platanios, Mrinmaya Sachan,
Graham Neubig, and Tom Mitchell. 2018. Contex-
tual parameter generation for universal neural ma-
chine translation. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing , pages 425–435.
Edoardo M. Ponti, Ivan Vuli ´c, Ryan Cotterell, Marinela
Parovic, Roi Reichart, and Anna Korhonen. 2021.
Parameter space factorization for zero-shot learning
across tasks and languages. Transactions of the As-
sociation for Computational Linguistics , 9:410–428.
Clifton Poth, Jonas Pfeiffer, Andreas Rücklé, and Iryna
Gurevych. 2021. What to pre-train on? Efﬁcient
intermediate task selection. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 10585–10605, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Afshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-
sively multilingual transfer for NER. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics , pages 151–164, Flo-
rence, Italy. Association for Computational Linguis-
tics.

--- PAGE 12 ---
Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea
Vedaldi. 2018. Efﬁcient parametrization of multi-
domain deep neural networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition , pages 8119–8127.
Andreas Rücklé, Gregor Geigle, Max Glockner,
Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna
Gurevych. 2021. AdapterDrop: On the efﬁciency
of adapters in transformers. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 7930–7946, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Sebastian Ruder, Matthew E Peters, Swabha
Swayamdipta, and Thomas Wolf. 2019. Trans-
fer learning in natural language processing. In
Proceedings of the 2019 conference of the North
American chapter of the association for computa-
tional linguistics: Tutorials , pages 15–18.
Asa Cooper Stickland and Iain Murray. 2019. Bert and
pals: Projected attention layers for efﬁcient adapta-
tion in multi-task learning. In International Confer-
ence on Machine Learning , pages 5986–5995.
Stephanie Strassel and Jennifer Tracey. 2016. Lorelei
language packs: Data, tools, and resources for tech-
nology development in low resource languages. In
Proceedings of the Tenth International Conference
on Language Resources and Evaluation (LREC’16) ,
pages 3273–3280.
Yi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, and Da-
Cheng Juan. 2021. HyperGrid Transformers: To-
wards A Single Model for Multiple Tasks. In Pro-
ceedings of ICLR 2021 .
Ahmet Üstün, Alexandre Berard, Laurent Besacier, and
Matthias Gallé. 2021. Multilingual unsupervised
neural machine translation with denoising adapters.
InProceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing , pages
6650–6662, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.
Ahmet Üstün, Arianna Bisazza, Gosse Bouma, and
Gertjan van Noord. 2020. UDapter: Language adap-
tation for truly Universal Dependency parsing. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 2302–2315, Online. Association for Computa-
tional Linguistics.
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-
anjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang,
and Ming Zhou. 2021. K-Adapter: Infusing Knowl-
edge into Pre-Trained Models with Adapters. In
Findings of the Association for Computational Lin-
guistics: ACL-IJCNLP 2021 , pages 1405–1418, On-
line. Association for Computational Linguistics.
Zirui Wang, Zachary C. Lipton, and Yulia Tsvetkov.
2020. On negative interference in multilingual mod-
els: Findings and a meta-learning treatment. InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 4438–4450, Online. Association for Computa-
tional Linguistics.
Bryan Wilie, Karissa Vincentio, Genta Indra Winata,
Samuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim,
Sidik Soleman, Rahmad Mahendra, Pascale Fung,
Syafri Bahar, and Ayu Purwarianti. 2020. IndoNLU:
Benchmark and resources for evaluating Indonesian
natural language understanding. In Proceedings of
the 1st Conference of the Asia-Paciﬁc Chapter of the
Association for Computational Linguistics and the
10th International Joint Conference on Natural Lan-
guage Processing , pages 843–857, Suzhou, China.
Association for Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language process-
ing. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing:
System Demonstrations , pages 38–45, Online. Asso-
ciation for Computational Linguistics.
Daniel Zeman, Joakim Nivre, Mitchell Abrams,
Elia Ackermann, Noëmi Aepli, Hamid Aghaei,
Željko Agi ´c, Amir Ahmadi, Lars Ahrenberg,
Chika Kennedy Ajede, Gabriel ˙e Aleksandravi ˇci¯ut˙e,
Ika Alﬁna, Lene Antonsen, Katya Aplonova, An-
gelina Aquino, Carolina Aragon, Maria Jesus Aran-
zabe, Hórunn Arnardóttir, Gashaw Arutie, Jes-
sica Naraiswari Arwidarasti, Masayuki Asahara,
Luma Ateyah, Furkan Atmaca, Mohammed Attia,
Aitziber Atutxa, Liesbeth Augustinus, Elena Bad-
maeva, Keerthana Balasubramani, Miguel Balles-
teros, Esha Banerjee, Sebastian Bank, Verginica
Barbu Mititelu, Victoria Basmov, Colin Batche-
lor, John Bauer, Seyyit Talha Bedir, Kepa Ben-
goetxea, Gözde Berk, Yevgeni Berzak, Irshad Ah-
mad Bhat, Riyaz Ahmad Bhat, Erica Biagetti, Eck-
hard Bick, Agn ˙e Bielinskien ˙e, Kristín Bjarnadóttir,
Rogier Blokland, Victoria Bobicev, Loïc Boizou,
Emanuel Borges Völker, Carl Börstell, Cristina
Bosco, Gosse Bouma, Sam Bowman, Adriane Boyd,
Kristina Brokait ˙e, Aljoscha Burchardt, Marie Can-
dito, Bernard Caron, Gauthier Caron, Tatiana Cav-
alcanti, Gül¸ sen Cebiro ˘glu Eryi ˘git, Flavio Massimil-
iano Cecchini, Giuseppe G. A. Celano, Slavomír ˇCé-
plö, Savas Cetin, Özlem Çetino ˘glu, Fabricio Chalub,
Ethan Chi, Yongseok Cho, Jinho Choi, Jayeol
Chun, Alessandra T. Cignarella, Silvie Cinková, Au-
rélie Collomb, Ça ˘grı Çöltekin, Miriam Connor, Ma-
rine Courtin, Elizabeth Davidson, Marie-Catherine
de Marneffe, Valeria de Paiva, Mehmet Oguz
Derin, Elvis de Souza, Arantza Diaz de Ilar-
raza, Carly Dickerson, Arawinda Dinakaramani,
Bamba Dione, Peter Dirix, Kaja Dobrovoljc, Tim-
othy Dozat, Kira Droganova, Puneet Dwivedi,

--- PAGE 13 ---
Hanne Eckhoff, Marhaba Eli, Ali Elkahky, Binyam
Ephrem, Olga Erina, Tomaž Erjavec, Aline Eti-
enne, Wograine Evelyn, Sidney Facundes, Richárd
Farkas, Marília Fernanda, Hector Fernandez Al-
calde, Jennifer Foster, Cláudia Freitas, Kazunori
Fujita, Katarína Gajdošová, Daniel Galbraith, Mar-
cos Garcia, Moa Gärdenfors, Sebastian Garza,
Fabrício Ferraz Gerardi, Kim Gerdes, Filip Gin-
ter, Iakes Goenaga, Koldo Gojenola, Memduh
Gökırmak, Yoav Goldberg, Xavier Gómez Guino-
vart, Berta González Saavedra, Bernadeta Grici ¯ut˙e,
Matias Grioni, Loïc Grobol, Normunds Gr ¯uz¯ıtis,
Bruno Guillaume, Céline Guillot-Barbance, Tunga
Güngör, Nizar Habash, Hinrik Hafsteinsson, Jan
Hajiˇc, Jan Haji ˇc jr., Mika Hämäläinen, Linh
Hà M ˜y, Na-Rae Han, Muhammad Yudistira Han-
ifmuti, Sam Hardwick, Kim Harris, Dag Haug,
Johannes Heinecke, Oliver Hellwig, Felix Hen-
nig, Barbora Hladká, Jaroslava Hlavá ˇcová, Florinel
Hociung, Petter Hohle, Eva Huber, Jena Hwang,
Takumi Ikeda, Anton Karl Ingason, Radu Ion,
Elena Irimia, O .lájídé Ishola, Tomáš Jelínek, Anders
Johannsen, Hildur Jónsdóttir, Fredrik Jørgensen,
Markus Juutinen, Sarveswaran K, Hüner Ka¸ sıkara,
Andre Kaasen, Nadezhda Kabaeva, Sylvain Ka-
hane, Hiroshi Kanayama, Jenna Kanerva, Boris
Katz, Tolga Kayadelen, Jessica Kenney, Václava
Kettnerová, Jesse Kirchner, Elena Klementieva,
Arne Köhn, Abdullatif Köksal, Kamil Kopacewicz,
Timo Korkiakangas, Natalia Kotsyba, Jolanta Ko-
valevskait ˙e, Simon Krek, Parameswari Krishna-
murthy, Sookyoung Kwak, Veronika Laippala, Lu-
cia Lam, Lorenzo Lambertino, Tatiana Lando,
Septina Dian Larasati, Alexei Lavrentiev, John Lee,
Phuong Lê H `ông, Alessandro Lenci, Saran Lert-
pradit, Herman Leung, Maria Levina, Cheuk Ying
Li, Josie Li, Keying Li, Yuan Li, KyungTae Lim,
Krister Lindén, Nikola Ljubeši ´c, Olga Loginova,
Andry Luthﬁ, Mikko Luukko, Olga Lyashevskaya,
Teresa Lynn, Vivien Macketanz, Aibek Makazhanov,
Michael Mandl, Christopher Manning, Ruli Manu-
rung, C ˘at˘alina M ˘ar˘anduc, David Mare ˇcek, Katrin
Marheinecke, Héctor Martínez Alonso, André Mar-
tins, Jan Mašek, Hiroshi Matsuda, Yuji Matsumoto,
Ryan McDonald, Sarah McGuinness, Gustavo Men-
donça, Niko Miekka, Karina Mischenkova, Mar-
garita Misirpashayeva, Anna Missilä, C ˘at˘alin Mi-
titelu, Maria Mitrofan, Yusuke Miyao, AmirHossein
Mojiri Foroushani, Amirsaeid Moloodi, Simonetta
Montemagni, Amir More, Laura Moreno Romero,
Keiko Sophie Mori, Shinsuke Mori, Tomohiko
Morioka, Shigeki Moro, Bjartur Mortensen, Bohdan
Moskalevskyi, Kadri Muischnek, Robert Munro,
Yugo Murawaki, Kaili Müürisep, Pinkey Nainwani,
Mariam Nakhlé, Juan Ignacio Navarro Horñiacek,
Anna Nedoluzhko, Gunta Nešpore-B ¯erzkalne, Lu-
ong Nguy ˜ên Thi ., Huy `ên Nguy ˜ên Thi .Minh, Yoshi-
hiro Nikaido, Vitaly Nikolaev, Rattima Nitisaroj,
Alireza Nourian, Hanna Nurmi, Stina Ojala, Atul Kr.
Ojha, Adédayo .Olúòkun, Mai Omura, Emeka On-
wuegbuzia, Petya Osenova, Robert Östling, Lilja
Øvrelid, ¸ Saziye Betül Özate¸ s, Arzucan Özgür,
Balkız Öztürk Ba¸ saran, Niko Partanen, Elena Pas-cual, Marco Passarotti, Agnieszka Patejuk, Guil-
herme Paulino-Passos, Angelika Peljak-Łapi ´nska,
Siyao Peng, Cenel-Augusto Perez, Natalia Perkova,
Guy Perrier, Slav Petrov, Daria Petrova, Jason Phe-
lan, Jussi Piitulainen, Tommi A Pirinen, Emily Pitler,
Barbara Plank, Thierry Poibeau, Larisa Ponomareva,
Martin Popel, Lauma Pretkalnin ,a, Sophie Prévost,
Prokopis Prokopidis, Adam Przepiórkowski, Tiina
Puolakainen, Sampo Pyysalo, Peng Qi, Andriela
Rääbis, Alexandre Rademaker, Taraka Rama, Lo-
ganathan Ramasamy, Carlos Ramisch, Fam Rashel,
Mohammad Sadegh Rasooli, Vinit Ravishankar,
Livy Real, Petru Rebeja, Siva Reddy, Georg Rehm,
Ivan Riabov, Michael Rießler, Erika Rimkut ˙e,
Larissa Rinaldi, Laura Rituma, Luisa Rocha, Eiríkur
Rögnvaldsson, Mykhailo Romanenko, Rudolf Rosa,
Valentin Ros ,ca, Davide Rovati, Olga Rudina, Jack
Rueter, Kristján Rúnarsson, Shoval Sadde, Pegah
Safari, Benoît Sagot, Aleksi Sahala, Shadi Saleh,
Alessio Salomoni, Tanja Samardži ´c, Stephanie Sam-
son, Manuela Sanguinetti, Dage Särg, Baiba Saul ¯ıte,
Yanin Sawanakunanon, Kevin Scannell, Salvatore
Scarlata, Nathan Schneider, Sebastian Schuster,
Djamé Seddah, Wolfgang Seeker, Mojgan Seraji,
Mo Shen, Atsuko Shimada, Hiroyuki Shirasu, Muh
Shohibussirri, Dmitry Sichinava, Einar Freyr Sig-
urðsson, Aline Silveira, Natalia Silveira, Maria Simi,
Radu Simionescu, Katalin Simkó, Mária Šimková,
Kiril Simov, Maria Skachedubova, Aaron Smith, Is-
abela Soares-Bastos, Carolyn Spadine, Stein hór Ste-
ingrímsson, Antonio Stella, Milan Straka, Emmett
Strickland, Jana Strnadová, Alane Suhr, Yogi Les-
mana Sulestio, Umut Sulubacak, Shingo Suzuki,
Zsolt Szántó, Dima Taji, Yuta Takahashi, Fabio Tam-
burini, Mary Ann C. Tan, Takaaki Tanaka, Sam-
son Tella, Isabelle Tellier, Guillaume Thomas, Li-
isi Torga, Marsida Toska, Trond Trosterud, Anna
Trukhina, Reut Tsarfaty, Utku Türk, Francis Ty-
ers, Sumire Uematsu, Roman Untilov, Zde ˇnka Ure-
šová, Larraitz Uria, Hans Uszkoreit, Andrius Utka,
Sowmya Vajjala, Daniel van Niekerk, Gertjan van
Noord, Viktor Varga, Eric Villemonte de la Clerg-
erie, Veronika Vincze, Aya Wakasa, Joel C. Wallen-
berg, Lars Wallin, Abigail Walsh, Jing Xian Wang,
Jonathan North Washington, Maximilan Wendt,
Paul Widmer, Seyi Williams, Mats Wirén, Chris-
tian Wittern, Tsegay Woldemariam, Tak-sum Wong,
Alina Wróblewska, Mary Yako, Kayo Yamashita,
Naoki Yamazaki, Chunxiao Yan, Koichi Yasuoka,
Marat M. Yavrumyan, Zhuoran Yu, Zden ˇek Žabokrt-
ský, Shorouq Zahra, Amir Zeldes, Hanzhi Zhu, and
Anna Zhuravleva. 2020. Universal dependencies 2.7.
LINDAT/CLARIAH-CZ digital library at the Insti-
tute of Formal and Applied Linguistics (ÚFAL), Fac-
ulty of Mathematics and Physics, Charles Univer-
sity.

--- PAGE 14 ---
A Language Selection
Table 4 shows that the details for languages such
as language code, UD treebank id and language
family. For POS tagging, we use the Universal De-
pendencies (UD) 2.7 dataset (Zeman et al., 2020)
and for NER, we use WikiANN (Pan et al., 2017)
with the train, dev and test splits from Rahimi
et al. (2019). To partition languages for the mixed-
language multi-task setting, we group languages
from the same families into the same partitions to
avoid a strong supervision from the same language
family when evaluating zero-shot predictions for
unseen task-language combinations. When there is
no available training data in the target treebank, we
use the test split for the mixed-language multi-task
setting.
B Experimental Details
B.1 Impact of Sampling
Hyper-X is a single model that is trained at once
for multiple languages and task simultaneously.
However, as the amount of total MLM training
data is considearbly larger than NER and POS-
tagging data, we experimented with two differ-
ent sampling methods: size propotional sampling
and temperature-based sampling ( t= 5). For
the temperature-based sampling, we independently
sample a batch for each task-language combination.
Figure 7 shows the impact of different sampling
methods on the zero-shot performance for ‘seen’,
‘unseen’ language groups together with average
over all languages. As seen, temperature-based
sampling, greatly increase performance for all lan-
guage groups on both NER and POS-tagging. This
suggest that when MLM data does not restricted by
sampling, it highly inﬂuences the learning objec-
tive which results a catastrophic forgetting on the
target tasks.
B.2 Implementation and Computing
Infrastructure
All the experiments are conducted using Tesla
V100 GPUs. We did not use parallel training on
multiple GPUs, so each experiment was conducted
on a single GPU. Parameters that are ﬁne-tuned for
each model and total runtime are reported in the
section (§ 6.1). We implemented Hyper-X by using
Transformers library (Wolf et al., 2020) and the
code will be released upon publication. We used
adapterhub (Pfeiffer et al., 2020a) for MAD-X, and
NER POSFigure 7: Impact of sampling for SEEN ,UNSEEN lan-
guage groups on NER and POS tagging.
the original repository for parameter space factor-
ization (Ponti et al., 2021). Hyper-parameters that
are used in experiments are given in the section 4.
We did not conduct a hyper-parameter search due
to the computational limitations, and used the refer-
ence values in most cases: only the dimension for
language adapters in MAD-X is changed to match
with the same parameter count of Hyper-X. Finally
for mBERT, we did a preliminary experiments with
learning rate of 1e-4 and 1e-5, and pick the latter
one as it produced better performance.
C Detailed Results
The results that are averaged over 3 runs for each
language are given in Table 6
D Few Shot Experiments
For the few-shot transfer experiments, we ﬁne-tune
each model for 50 epochs with the same hyper-
parameters. We disable the learning rate decay as
only a few training instances are provided to the
models. Note that, in these experiments, we always
start with the models that are already trained in the
zero-shot setting and perform ﬁne-tuning for each
language and task separately. For the selection of
training samples, we randomly sample instances
regardless of the labels, as the initial models are
already trained for these tasks on English data.
Table 5 show that few-shot results for NER and
POS-tagging respectively.

--- PAGE 15 ---
NER POS
Language Code UD Treebank Family Train Dev Test Train Dev Test
English en EWT IE, Germanic 20000 10000 10000 12543 2002 2077
Arabic ar PADT Afro-Asiatic, Semitic 20000 10000 10000 6075 909 680
Breton br KEB IE, Celtic 1000 1000 1000 - - 888
Chinese zh GSD Sino-Tibetan 20000 10000 10000 3997 500 500
Icelandic is PUD IE, Germanic 1000 1000 1000 - - 1000
Kazakh kk KTB Turkic, Northwestern 1000 1000 1000 32 - 1047
Tamil ta TTB Dravidian, Southern 15000 1000 1000 400 80 120
Turkish tr IMST Turkic, Southwestern 20000 10000 10000 3664 988 983
Yoruba yo YTB Niger-Congo, Defoid 100 100 100 - - 318
Faroese fo OFT IE, Germanic 100 100 100 - - 1208
Guarani gn Thomas Tupian, Tupi-Guarani 100 100 100 - - 98
Upper Sorbian yo UFAL IE, Slavic 100 100 100 - - 23
Maltese mt MUDT Afro-Asiatic, Semitic 100 100 100 1123 433 518
Sanskrit sa UFAL Indic 100 100 100 - - 230
Uyghur ug UDT Turkic, Southeastern 100 100 100 1656 900 900
Cantonese yue HK Sino-Tibetan 20000 10000 10000 - - 1004
Table 4: Languages that are used in the experiments, together with corresponding language code, UD treebank and
language families. We used WikiANN (Pan et al., 2017; Rahimi et al., 2019) and UD version 2.7 (Zeman et al.,
2020) for NER and POS-tagging respectively.
mBERT MAD-X Hyper-X Small Hyper-X Base
ar tr zh mt ug ar tr zh mt ug ar tr zh mt ug ar tr zh mt ug
0 42.6 72.5 36.4 43.4 12.5 40.3 71.5 34.9 64.4 30.4 37.2 71.6 34.2 61.3 22.4 39.9 73.2 34.6 63.6 22.5
5 54.7 72.8 42.0 53.9 21.8 52.4 73.5 39.3 67.3 37.5 56.5 74.6 39.5 66.4 28.0 56.9 72.9 39.3 65.6 30.4
10 69.2 76.0 42.1 53.4 30.4 64.1 75.2 43.8 76.1 44.3 65.1 75.0 44.9 78.0 39.6 67.3 74.2 44.4 78.3 34.2
20 69.5 78.5 49.4 53.2 30.2 66.1 77.4 48.6 82.1 45.1 66.8 76.7 51.9 80.3 39.8 68.8 77.8 52.1 80.9 43.8
50 74.5 82.1 52.3 69.1 42.5 70.2 81.0 50.7 84.9 60.6 71.7 80.9 54.6 82.1 53.2 73.7 80.9 54.8 83.6 52.5
0 53.4 72.0 67.5 24.6 28.9 54.0 73.2 67.3 70.8 57.3 53.4 69.2 65.6 58.8 40.4 54.4 71.0 66.5 59.7 50.6
5 76.2 75.1 73.1 51.7 55.8 76.4 76.3 73.3 80.1 72.4 75.4 75.7 76.3 73.2 62.1 78.4 74.2 77.9 75.6 63.9
10 81.8 76.6 79.5 60.8 58.9 83.4 76.9 78.6 83.8 73.9 84.3 76.8 81.6 75.3 63.9 84.8 75.9 81.9 79.3 66.0
20 86.9 78.6 84.3 68.7 60.3 86.7 79.3 84.2 85.8 74.7 87.2 78.4 87.1 78.9 65.9 87.3 76.7 86.8 82.3 67.5
50 90.2 81.3 89.1 77.9 67.3 90.5 81.9 88.4 90.1 77.2 90.8 82.3 90.4 83.4 66.3 91.2 81.6 90.8 86.0 69.0
Table 5: Per language results for few-shot experiments, where models are further ﬁne-tuned with a few training
instances (0, 5, 10, 20, 50) from NER and POS datasets. For the language selection, ar,tr,zh are covered by
mBERT and mt,ug are unseen.

--- PAGE 16 ---
English English Mixed-Language
Single-Task Multi-Task Multi-Task
mB MX HX.32 HX.192 mB HX.32 HX.192 mB PSF MX HX.32 HX.192Named entity recognitionena;b84.2 81.6 83.6 83.8 83.6 82.1 82.6 81.8 79.2 82.2 83.8 83.7
arb40.6 40.3 42.9 39.7 42.6 37.2 39.9 45.5 43.4 53.5 47.8 49.2
bra62.9 67.2 67.1 70.2 66.5 66.5 69.5 70.5 70.9 72.3 74.7 76.1
isb65.0 70.0 71.0 72.9 69.2 70.7 73.5 70.6 73.5 77.5 77.3 80.2
kka47.2 46.7 49.6 46.3 45.9 42.6 47.3 55.4 57.1 58.9 64.5 59.0
tab53.8 51.0 47.3 50.6 50.6 49.7 51.0 53.7 52.2 60.4 61.1 62.2
tra70.2 71.5 73.8 71.4 72.5 71.6 72.5 77.2 78.2 78.9 80.3 82.7
yoa47.6 53.0 42.1 50.2 46.8 44.9 47.1 43.4 45.6 54.4 44.8 50.2
zha39.5 34.9 39.5 34.7 36.4 34.2 34.6 35.0 43.5 43.3 45.7 46.5
gna43.6 50.3 49.0 57.5 41.7 55.4 54.1 52.2 56.8 65.0 63.5 66.1
hsbb65.4 75.6 62.2 68.6 61.4 64.3 74.6 73.8 75.3 84.1 78.5 80.0
fob62.1 69.1 69.0 70.7 60.7 74.7 74.9 63.3 68.4 83.1 76.8 82.2
mtb34.1 64.4 63.0 62.8 43.4 61.3 63.6 61.1 73.9 73.4 67.7 77.8
saa29.6 33.1 33.2 34.6 29.0 30.3 30.8 30.4 43.7 48.2 43.2 43.6
ugb12.8 30.4 20.1 21.1 12.5 22.4 22.5 23.8 16.4 38.8 33.7 27.5
yuea34.6 34.8 37.7 39.5 34.3 36.6 37.3 36.6 44.0 42.5 44.4 49.6
SEEN 53.4 54.3 54.2 54.5 53.8 52.2 54.4 56.4 58.1 62.4 62.0 63.3
UNSEEN 40.3 51.1 47.7 50.7 40.4 49.3 51.1 48.7 54.1 62.2 58.3 61.0
ALL 47.3 52.8 51.2 52.7 47.6 50.8 52.9 52.8 56.2 62.3 60.3 62.3Part-of-speech taggingena;b97.0 96.8 96.6 96.1 96.9 96.5 96.8 96.5 95.3 96.7 96.7 96.7
ara53.4 54.0 53.1 54.4 52.6 53.4 54.4 62.0 67.6 55.9 61.6 65.0
brb66.8 70.5 65.2 70.8 68.6 69.9 70.4 64.7 69.7 73.8 74.9 72.5
isa82.1 82.8 83.1 83.9 84.1 82.4 83.0 83.2 81.6 84.7 85.4 85.8
kkb74.6 75.2 73.1 75.7 75.2 72.2 75.1 70.4 79.7 80.6 80.4 80.5
taa58.0 59.1 58.5 59.5 58.5 52.6 58.6 63.1 67.2 62.2 61.7 62.7
trb72.0 73.2 70.6 70.4 70.1 69.2 71.0 70.6 73.5 75.1 74.8 75.6
yob55.6 60.3 58.3 60.0 58.4 55.2 56.6 58.8 57.4 64.2 61.0 63.2
zhb67.5 67.3 70.2 67.4 63.1 65.6 66.5 64.9 66.6 69.2 65.8 66.8
gnb27.2 34.9 31.2 37.0 28.3 35.1 36.7 38.6 36.3 44.5 40.8 41.1
hsba71.3 76.2 75.7 73.9 69.9 75.3 73.2 70.3 69.0 80.4 77.5 78.5
foa87.2 88.3 86.4 87.9 80.5 85.8 86.4 82.1 81.1 88.9 88.6 88.6
mta24.6 70.8 61.4 52.7 28.2 58.8 59.7 40.7 38.1 74.3 63.9 64.0
sab39.4 46.3 43.1 39.5 40.5 46.3 45.9 48.1 50.4 54.5 56.6 54.6
uga28.9 57.3 44.3 56.4 26.7 40.4 50.6 40.2 37.2 59.7 53.0 56.0
yueb63.6 64.2 62.9 63.6 63.1 62.4 64.0 63.2 64.6 66.4 62.2 64.0
SEEN 66.3 67.7 66.5 67.8 66.3 65.1 67.0 67.2 70.4 70.7 70.7 71.5
UNSEEN 48.9 62.6 57.9 58.7 48.2 57.7 59.5 54.7 53.8 67.0 63.2 63.8
ALL 58.1 65.4 62.5 63.5 57.9 61.7 63.6 61.4 62.7 69.0 67.2 67.9
Table 6: Zero-shot cross-lingual transfer results averaged over 3 runs for Named-Entity Recognition (NER; F1)
and Part-of-Speech Tagging (POS; Accuracy) for mBERT (mB), MAD-X (MX) and parameter space factorization
(PSF) models, together with Hyper-X Small (HX.32) and Base (HX.192). Superscripts denote the partitioning that
is used for mixed-language multi-task setting
