# 2405.03003.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2405.03003.pdf
# Kích thước tệp: 12824077 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tinh chỉnh hiệu quả tham số với Biến đổi Fourier rời rạc
Ziqi Gao1 2 *Qichao Wang3 *Aochuan Chen1 *Zijing Liu4Bingzhe Wu5Liang Chen3Jia Li1 2
Tóm tắt
Thích ứng thấp hạng (LoRA) gần đây đã thu hút được nhiều sự quan tâm trong việc tinh chỉnh các mô hình nền tảng. Nó hiệu quả giảm số lượng tham số có thể huấn luyện bằng cách kết hợp các ma trận thấp hạng A và B để biểu diễn sự thay đổi trọng số, tức là ∆W=BA. Bất chấp sự tiến bộ của LoRA, nó vẫn đối mặt với thách thức lưu trữ khi xử lý các tùy chỉnh mở rộng hoặc các mô hình cơ sở lớn hơn. Trong nghiên cứu này, chúng tôi nhằm mục đích nén thêm các tham số có thể huấn luyện bằng cách tận dụng sức mạnh biểu đạt của biến đổi Fourier. Cụ thể, chúng tôi giới thiệu FourierFT, phương pháp coi ∆W như một ma trận trong miền không gian và chỉ học một phần nhỏ các hệ số phổ của nó. Với các hệ số phổ được huấn luyện, chúng tôi thực hiện biến đổi Fourier rời rạc nghịch đảo để khôi phục ∆W. Thực nghiệm cho thấy, phương pháp FourierFT của chúng tôi có hiệu suất tương đương hoặc tốt hơn với ít tham số hơn so với LoRA trên các tác vụ khác nhau, bao gồm hiểu ngôn ngữ tự nhiên, sinh ngôn ngữ tự nhiên, tinh chỉnh hướng dẫn và phân loại hình ảnh. Ví dụ, khi thực hiện tinh chỉnh hướng dẫn trên mô hình LLaMA2-7B, FourierFT vượt trội hơn LoRA chỉ với 0,064M tham số có thể huấn luyện, so với 33,5M của LoRA. Mã nguồn của chúng tôi được phát hành tại https://github.com/Chaos96/fourierft .

1. Giới thiệu
Các mô hình nền tảng lớn (LFMs) đã chứng minh hiệu suất xuất sắc trên các tác vụ thuộc nhiều lĩnh vực, bao gồm xử lý ngôn ngữ tự nhiên (NLP) (Liu et al., 2019; He et al., 2020; Radford et al., 2019; Brown et al., 2020; Li et al., 2022) và thị giác máy tính (CV) (Liu et al., 2023a;b; Singh et al., 2022; Rombach et al., 2022). Nhờ vào khả năng ấn tượng của chúng, việc tinh chỉnh LFMs cho một loạt các tác vụ hạ nguồn đã trở nên phổ biến (Wang et al., 2022; Taori et al., 2023; Qiu et al., 2020). Dưới mô hình tinh chỉnh đầy đủ, mô hình mới thích ứng với từng tác vụ tùy chỉnh thường chứa số lượng tham số bằng với mô hình gốc (Qiu et al., 2020; Raffel et al., 2020; Chen et al., 2024; Gao et al., 2024). Khi các mô hình trở nên lớn hơn và nhu cầu tùy chỉnh mở rộng, nhu cầu lưu trữ các checkpoint tinh chỉnh tăng lên, dẫn đến việc tiêu thụ bộ nhớ và lưu trữ tốn kém.

*Đóng góp bằng nhau1Đại học Khoa học và Công nghệ Hồng Kông (Quảng Châu)2Đại học Khoa học và Công nghệ Hồng Kông3Đại học Trung Sơn4Học viện Kinh tế Số Quốc tế5Phòng thí nghiệm AI, Tencent. Liên hệ: Jia Li <jialee@ust.hk >.

Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học máy, Vienna, Áo. PMLR 235, 2024. Bản quyền 2024 thuộc về (các) tác giả.

Hình 1. Tóm tắt hiệu suất (trục y) của các phương pháp tinh chỉnh với số lượng tham số có thể huấn luyện khác nhau (trục x) trên các tác vụ NLP (trái) và CV (phải). Phía bên trái hiển thị tác vụ tinh chỉnh hướng dẫn, trong đó mô hình LLaMA2-7B được tinh chỉnh với Alpaca và đánh giá bởi GPT-4. Phía bên phải hiển thị tác vụ phân loại hình ảnh, trong đó Vision Transformer (ViT) được tinh chỉnh và kiểm tra trên tập dữ liệu DTD. Các vòng tròn đen (●) biểu diễn phương pháp Tinh chỉnh đầy đủ (FF). Các vòng tròn cam (●) biểu diễn phương pháp LoRA với r={32,64,128} (trái) và r={8,16,32} (phải). Các vòng tròn xanh (●) biểu diễn phương pháp được đề xuất của chúng tôi với n={1000,2000} (trái) và n={3000,10000} (phải).

Như một cách phổ biến để giải quyết vấn đề này, LoRA (Hu et al., 2021) biểu diễn sự thay đổi trọng số bằng hai ma trận thấp hạng A và B, tức là W0+∆W=W0+BA. Bất chấp hiệu suất tuyệt vời của LoRA, số lượng lớn tham số có thể huấn luyện của nó vẫn mang lại tiêu thụ cơ sở hạ tầng CNTT cao, ảnh hưởng đến cả cộng đồng công cộng và người dùng cá nhân. Đối với trường hợp đầu tiên, một ví dụ trực quan là một bộ điều hợp LoRA (trọng số tinh chỉnh) cho một phong cách cụ thể của mô hình stable diffusion (Rombach et al., 2022) yêu cầu khoảng 40MB bộ nhớ. Điều này đòi hỏi các cộng đồng LFM (ví dụ, Civitai (Civitai, 2024)) phải gánh chịu chi phí lưu trữ và băng thông cao để phục vụ cơ sở người dùng lớn. Đối với trường hợp sau, ít tham số hơn có nghĩa là tiết kiệm RAM trực tiếp khi tải trọng số tinh chỉnh trong các ứng dụng di động, cho phép tùy chỉnh đầy đủ cho người dùng cá nhân (Zhou et al., 2022). Vì vậy, chúng tôi tự nhiên đặt ra câu hỏi: Làm thế nào chúng ta có thể nén tích cực các tham số có thể huấn luyện hơn nữa để tinh chỉnh LFMs?

Các nghiên cứu trước đây đã chứng minh sức mạnh biểu đạt của cơ sở Fourier trong nén dữ liệu, trong đó thông tin phổ cực kỳ thưa thớt có thể được sử dụng để khôi phục dữ liệu có độ tin cậy cao (ví dụ, các vectơ tín hiệu 1D (Zwartjes & Gisolf, 2007; Duarte & Baraniuk, 2013; Rudelson & Vershynin, 2008) và ma trận hình ảnh 2D (Vlaardingerbroek & Boer, 2013; Song et al., 2021; Shi et al., 2014)). Quan trọng hơn, khi xử lý các ma trận tổng quát hơn (không phải hình ảnh) thiếu ngữ nghĩa không gian mạnh và không thưa thớt về tần số, biến đổi Fourier vẫn có thể xử lý khôi phục hiệu quả (Chen & Chi, 2013; Yang & Xie, 2016). Được thúc đẩy bởi điều này, chúng tôi khám phá tiềm năng cập nhật sự thay đổi trọng số ∆W bằng các hệ số phổ thưa thớt của nó để tinh chỉnh LFMs.

Trong bài báo này, chúng tôi nhằm mục đích giảm mạnh số lượng tham số có thể huấn luyện để tinh chỉnh LFMs. Vì vậy, chúng tôi đề xuất FourierFT (Biến đổi Fourier cho Tinh chỉnh), phương pháp coi sự thay đổi trọng số ∆W như một ma trận trong miền không gian, và học các hệ số phổ thưa thớt của nó. Cụ thể, chúng tôi đầu tiên chọn ngẫu nhiên n phần tử phổ được chia sẻ trên tất cả các lớp. Đối với mỗi lớp, FourierFT học n hệ số phổ nằm tại n phần tử đã chọn này và sau đó trực tiếp áp dụng biến đổi Fourier rời rạc nghịch đảo để tính toán ∆W được cập nhật. Do đó, tinh chỉnh một mô hình tiền huấn luyện với Lt lớp chỉ yêu cầu lưu trữ 2n tham số phần tử và nLt tham số hệ số cho FourierFT.

Thực nghiệm cho thấy, chúng tôi so sánh phương pháp của mình với các biến thể LoRA tiên tiến và các phương pháp tinh chỉnh hiệu quả tham số khác trên các tác vụ khác nhau bao gồm (1) hiểu ngôn ngữ tự nhiên (trên benchmark GLUE), (2) sinh ngôn ngữ tự nhiên (trên benchmark E2E), (3) tinh chỉnh hướng dẫn (với các mô hình họ LLaMA), và (4) phân loại hình ảnh (với các bộ biến đổi thị giác). FourierFT luôn có thể đạt được hiệu suất tương đương hoặc thậm chí tốt hơn so với LoRA, với khoảng 6.0%, 9.4%, 0.2% và 9.2% tham số có thể huấn luyện của LoRA cho 4 tác vụ này, tương ứng. Ví dụ trong Hình 1, trên tác vụ tinh chỉnh hướng dẫn, phương pháp FourierFT của chúng tôi vượt trội hơn LoRA chỉ với 64K tham số có thể huấn luyện. Hơn nữa, nó đạt được điểm số tương đương với Tinh chỉnh đầy đủ chỉ với 128K tham số.

2. Các công trình liên quan
Tinh chỉnh hiệu quả tham số. Với sự mở rộng nhanh chóng của các mô hình nền tảng lớn (LFM), việc thích ứng chúng một cách hiệu quả cho các tác vụ cụ thể đã trở nên thách thức và quan trọng. Vì vậy, nhiều phương pháp tinh chỉnh hiệu quả tham số (PEFT) được đề xuất, chứng minh khả năng ấn tượng về cả hiệu quả và độ chính xác. Các phương pháp PEFT hiện có được chia thành hai loại: phương pháp không dựa trên trọng số và phương pháp dựa trên trọng số.

Các phương pháp không dựa trên trọng số không tối ưu hóa LFMs tiền huấn luyện ở mức trọng số. Thay vào đó, chúng đạt được tinh chỉnh bằng cách giới thiệu các mô-đun bổ sung hoặc tối ưu hóa gợi ý và tiền tố. Tinh chỉnh bộ điều hợp (He et al., 2021; Rebuffi et al., 2017; Pfeiffer et al., 2020; Houlsby et al., 2019; Rückle et al., 2020; Lin et al., 2020) nhằm mục đích giới thiệu các mô-đun thần kinh nhẹ, gọi là bộ điều hợp, giữa các lớp tiền huấn luyện của mô hình cơ sở. Các phương pháp này giữ nguyên trọng số tiền huấn luyện và tinh chỉnh hiệu quả các bộ điều hợp cho các tác vụ tùy chỉnh. Tinh chỉnh gợi ý (Brown et al., 2020; Lester et al., 2021; Gao et al., 2020; Diao et al., 2022) và tinh chỉnh tiền tố (Li & Liang, 2021) chèn các gợi ý hoặc token tiền tố bổ sung vào các lớp của mô hình cơ sở. Các phương pháp dựa trên trọng số, được đại diện bởi LoRA (Hu et al., 2021), giới thiệu và sau đó cập nhật các thay đổi trọng số có thể được hợp nhất với trọng số gốc để tránh độ trễ suy luận. Sự đổi mới của LoRA nằm ở việc nhân các ma trận thấp hạng để xấp xỉ các thay đổi trọng số. Dựa trên điều này, AdaLoRA (Zhang et al., 2023) mở rộng phương pháp LoRA bằng cách phân phối ngân sách tham số trên các ma trận trọng số với điểm số quan trọng. Ngoài ra, Q-LoRA (Dettmers et al., 2023) đề xuất lan truyền ngược gradient qua LoRA thông qua mô hình tiền huấn luyện được lượng tử hóa với 4-bit NormalFloat.

Ở đây, chúng tôi tập trung vào các phương pháp dựa trên trọng số và đạt được giảm tham số rất lớn với sức mạnh biểu đạt của cơ sở Fourier, thay vì theo cấu trúc thấp hạng.

Biến đổi Fourier thưa thớt trong Học sâu. Biến đổi Fourier thưa thớt (SFT) đã phát triển mạnh trong nhiều lĩnh vực của học sâu (DL). Kỹ thuật SFT chủ yếu liên quan đến việc sử dụng các hệ số phổ thưa thớt của các phần tử phổ quan trọng (Xu et al., 2020; Ehrlich & Davis, 2019; Gueguen et al., 2018; Tang et al., 2022) hoặc thậm chí ngẫu nhiên (Lin et al., 2014; Rawat et al., 2019; Herrmann, 2010), để học biểu diễn. Một ứng dụng quan trọng của kỹ thuật này là khôi phục ma trận. Patel et al. (2011) thiết kế một phương pháp cảm biến nén dựa trên gradient để khôi phục hình ảnh bằng thông tin Fourier thưa thớt của chúng. Shechtman et al. (2014) đề xuất một phương pháp khôi phục pha hiệu quả cải thiện khôi phục dữ liệu bằng các hệ số Fourier thưa thớt. Quan trọng là, các nghiên cứu trước đây (Chen & Chi, 2013; Yang & Xie, 2016; Gao et al., 2022) cho thấy rằng ngay cả khi dữ liệu gốc không thưa thớt về tần số, SFT vẫn có thể khôi phục dữ liệu hiệu quả với rất ít tham số. Mặc dù các nghiên cứu trước đây thiếu nghiên cứu về khôi phục cho các ma trận trọng số của mô hình DL bằng SFT, các phương pháp đã đề cập ở trên cung cấp hỗ trợ tiềm năng cho công trình này.

2

--- TRANG 3 ---
Tinh chỉnh hiệu quả tham số với Biến đổi Fourier rời rạc

Trọng số tiền huấn luyện 𝑊∈ℝ!!×!"𝐵=0𝐴=𝒩(0,𝜎!)ℎ𝑥𝑑#
𝑑$𝑟Trọng số tiền huấn luyện 𝑊∈ℝ!!×!"ℎ𝑥𝑑#
𝑑$Các phần tử ngẫu nhiên(chia sẻ trên các lớp)ℝ!×#𝑛Hệ số: Cố định: Có thể huấn luyệnLoRAFourierFTIDFTMa trận phổ dày đặc F

Hình 2. Tổng quan về phương pháp LoRA (trái) và FourierFT (phải) của chúng tôi. Trong LoRA, chỉ có các ma trận thấp hạng (r) A và B được huấn luyện. Sự thay đổi trọng số được biểu diễn bởi phép nhân của chúng, tức là ∆W=BA. Đối với mỗi trọng số tiền huấn luyện W, số lượng tham số có thể huấn luyện về lý thuyết trong LoRA là r×(d1+d2). Trong FourierFT, chúng tôi đầu tiên tạo ngẫu nhiên ma trận phần tử phổ R2×n, được chia sẻ trên tất cả các lớp để giảm yêu cầu lưu trữ tham số. Ma trận phổ hoàn chỉnh được tạo thành bởi một vectơ hệ số có thể huấn luyện Rn nằm tại các phần tử đã chọn và 0 tại các phần tử còn lại. Chúng tôi thu được sự thay đổi trọng số ∆W bằng cách trực tiếp thực hiện biến đổi Fourier rời rạc nghịch đảo (IDFT) trên ma trận phổ được cập nhật. Đối với tất cả L lớp đã thích ứng, FourierFT cần lưu trữ n×(2+L) tham số.

3. Phương pháp
Chúng tôi trình bày FourierFT (được mô tả trong Hình 2), một phương pháp tinh chỉnh hiệu quả tham số dựa trên biến đổi Fourier rời rạc. FourierFT theo nguyên tắc chỉ học sự thay đổi trong trọng số tiền huấn luyện, như được đề xuất bởi LoRA (Hu et al., 2021). Tuy nhiên, không giống như LoRA, FourierFT không áp dụng cấu trúc thấp hạng mà học một tập hợp các hệ số phổ của cơ sở Fourier. Cụ thể, chúng tôi khởi tạo ngẫu nhiên ma trận phần tử phổ, được cố định và chia sẻ trên tất cả các lớp. Chúng tôi làm cho các hệ số phổ nằm tại các phần tử đã chọn có thể huấn luyện, cùng nhau tạo thành ma trận phổ. Cuối cùng, chúng tôi áp dụng biến đổi Fourier rời rạc nghịch đảo cho ma trận phổ, tạo ra đối tác miền không gian của nó như sự thay đổi trọng số được cập nhật.

3.1. Lần chuyển tiếp
Chúng tôi tuân theo mô hình chỉ học các thay đổi trọng số, được các phương pháp dựa trên LoRA áp dụng (Hu et al., 2021; Dettmers et al., 2023; Zhang et al., 2023). Điều này có thể tránh độ trễ suy luận bằng cách hợp nhất trọng số tiền huấn luyện và sự thay đổi của nó. Chính thức, chúng tôi định nghĩa mỗi ma trận trọng số tiền huấn luyện là W0∈Rd1×d2, và sự thay đổi trọng số để tinh chỉnh là ∆W∈Rd1×d2. LoRA nhằm mục đích tham số hóa ∆W dưới dạng phân rã thấp hạng trong lần chuyển tiếp:

h=W0x+∆Wx=W0x+BAx, (1)

trong đó B∈Rd1×r và A∈Rr×d2 với hạng r≪min(d1, d2) là các ma trận có thể huấn luyện.

Lợi thế của FourierFT là cơ sở Fourier trực giao và biểu đạt cho phép khôi phục các thay đổi trọng số có thông tin. Điều này đầy hứa hẹn cho việc đạt được hiệu suất tương đương với LoRA với ít tham số hơn đáng kể. Chúng tôi đầu tiên khởi tạo ngẫu nhiên ma trận phần tử E∈R2×n chứa các phần tử phổ 2D rời rạc. Sau đó chúng tôi khởi tạo ngẫu nhiên các hệ số c∈Rn với phân phối Gaussian thông thường. Lần chuyển tiếp được đề xuất là:

F=TODENSE (E, c) (2)
Sp,q=∑j=0d1−1∑k=0d2−1Fj,kei2π(pd1j+qd2k)(3)
h=W0x+∆Wx
=W0x+αR(S)x.(4)

Cụ thể, TODENSE trong Phương trình 2 đại diện cho việc xây dựng ma trận phổ F∈Rd1×d2, tức là Fj,k=cl(tương ứng 0), nếu j=E0,l&k=E1,l(tương ứng khác). Phương trình 3 tính toán ma trận không gian S thông qua biến đổi Fourier rời rạc nghịch đảo, trong đó i biểu diễn đơn vị ảo. Cuối cùng, trong Phương trình 4, chúng tôi lấy phần thực của ma trận phức S (được ký hiệu là R(S)) và chia tỷ lệ bằng α. Xin lưu ý rằng tất cả các lớp đều liên quan đến việc huấn luyện các vectơ c khác nhau, trong khi chia sẻ ma trận E và giá trị α.

Mã giả cho FourierFT được hiển thị như Thuật toán 1, tuân theo phong cách PyTorch.

Khởi tạo cho Ma trận phần tử E. Các nghiên cứu trước đây thiếu nghiên cứu về tầm quan trọng của các phần tử phổ trong sự thay đổi trọng số. Do đó, chúng tôi lấp đầy khoảng trống này bằng cách giới thiệu độ lệch tần số có thể điều chỉnh, khiến các phần tử có nhiều khả năng được lấy mẫu trong khu vực này. Ngoài việc lấy mẫu ngẫu nhiên các phần tử trong ma trận phổ có kích thước đầy đủ d1×d2 (tức là không có độ lệch), chúng tôi cũng thực hiện lấy mẫu phần tử với độ lệch hướng tới tần số trung tâm được ưa thích, ví dụ như tần số thấp, trung bình hoặc cao. Chính thức, chúng tôi áp dụng bộ lọc thông dải Gaussian (Gonzales & Wintz, 1987) để mô hình hóa xác suất lấy mẫu cho phần tử (u, v),0≤u≤d1−1,0≤v≤d2−1:

p(u, v)=exp⎛⎝−(D2−fc2DW)2⎞⎠, (5)

trong đó D biểu diễn khoảng cách từ điểm (u, v) đến gốc tọa độ (trung tâm của ma trận), fc là tần số trung tâm được ưa thích, và W biểu diễn băng thông. Trong Hình 3, chúng tôi trực quan hóa bản đồ xác suất lấy mẫu của ma trận phổ có kích thước 768×768 với fc và W=200 khác nhau.

fc=0
fc=100
fc=200
fc=350
fc=480
0 0.5 1

Hình 3. Trực quan hóa xác suất lấy mẫu phần tử ở các tần số trung tâm được ưa thích fc khác nhau.

Xin lưu ý rằng trừ khi được nêu đặc biệt, FourierFT được thiết lập mặc định để khởi tạo phần tử không có độ lệch tần số.

3.2. Tóm tắt tham số
Chúng tôi tóm tắt số lượng tham số có thể huấn luyện cho LoRA và FourierFT trong Bảng 1. LoRA dựa vào một cặp ma trận có thể huấn luyện A và B cho mỗi lớp. Gọi số lượng lớp để tinh chỉnh là Lt. Tổng số tham số trong

Bảng 1. Số lượng tham số có thể huấn luyện về lý thuyết và yêu cầu lưu trữ để tinh chỉnh. Đối với cả phương pháp LoRA và FourierFT, chỉ có các lớp truy vấn và giá trị được tinh chỉnh trong các kiến trúc transformer. Các cấu hình được chọn chính xác trong 'Phần thực nghiệm' được đánh dấu.

Mô hình cơ sở LoRA FourierFT
r # Tham số có thể huấn luyện Byte cần thiết n # Tham số có thể huấn luyện Byte cần thiết
RoBERTa Base 4 147K 574KB 200 4.8K 18.8KB
8 295K 1.13MB 200 24K 94KB
RoBERTa Large 4 393K 1.5MB 200 9.6K 36.5KB
8 786K 3MB 1000 48K 183KB
GPT-2 Medium 4 350K 1.34MB 500 24K 94KB
8 786K 3MB 1000 48K 188KB
GPT-2 Large 4 737K 2.81MB 500 36K 141KB
8 1.47M 5.74MB 1000 72K 282KB
LLaMA-2 7B 16 8.39M 32.8MB 1000 64K 250KB
64 33.5M 131.1MB 2000 128K 500KB
LLaMA-2 13B 16 13.1M 51.2MB 1000 80K 312KB
64 52.4M 204.8MB 2000 160K 625KB
ViT Base 8 295K 1.13MB 3000 72K 281KB
16 590K 2.25MB 10000 239K 934KB
ViT Large 8 786K 2.93MB 3000 144K 563KB
16 1.57M 6MB 10000 480K 1.83MB

LoRA được xác định bởi hạng r và kích thước của trọng số d=d1=d2: |Θ|LoRA=2×d×Lt×r. Đối với Fourier, tổng số có dạng: |Θ|FourierFT=n×Lt. Như một ví dụ trực quan, mô hình RoBERTa Base chứa 12 khối transformer với d=768, dẫn đến Lt=24 lớp khi chúng tôi chỉ tinh chỉnh các lớp truy vấn và giá trị. Do đó, chúng tôi có |Θ|LoRA=294,912 cho r=8, và |Θ|FourierFT=24,000 cho n=1000. Trong Bảng 1, chúng tôi đánh dấu các cấu hình mà LoRA và phương pháp của chúng tôi đạt được hiệu suất tương ứng trong các thí nghiệm sau.

Chúng tôi lưu ý rằng lợi thế của hiệu quả tham số trong FourierFT trở nên nổi bật hơn khi quy mô của mô hình (chiều sâu và chiều rộng) tăng (ví dụ, RoBERTa Base → RoBERTa Large). Điều này có thể do |Θ|LoRA có mối quan hệ tuyến tính rõ ràng với chiều rộng d, không giống như |Θ|FourierFT.

4. Thực nghiệm
Trong phần này, chúng tôi đánh giá FourierFT trong các lĩnh vực xử lý ngôn ngữ tự nhiên (NLP) và thị giác máy tính (CV). Đối với NLP, chúng tôi triển khai FourierFT để tinh chỉnh (1) RoBERTa (Base & Large) trên hiểu ngôn ngữ tự nhiên (GLUE, (Wang et al., 2018)), (2) GPT-2 (Medium & Large) trên sinh ngôn ngữ tự nhiên (E2E, (Novikova et al., 2017)) và (3) các mô hình họ LLaMA (7B & 13B) trên tinh chỉnh hướng dẫn. Đối với CV, chúng tôi áp dụng FourierFT để tinh chỉnh (4) các bộ biến đổi thị giác (Base & Large) trên phân loại hình ảnh. Cuối cùng, chúng tôi thực hiện các nghiên cứu ablation để phân tích hiệu ứng của độ lệch tần số, khả năng mở rộng tham số, và

4

--- TRANG 5 ---
Tinh chỉnh hiệu quả tham số với Biến đổi Fourier rời rạc

Bảng 2. Hiệu suất của các phương pháp tinh chỉnh khác nhau với các mô hình RoBERTa Base (RoB base) và RoBERTa Large (RoB large) trên 6 tập dữ liệu của benchmark GLUE. Chúng tôi báo cáo hệ số tương quan Matthew (MCC) cho CoLA, hệ số tương quan Pearson (PCC) cho STS-B và độ chính xác (Acc.) cho tất cả các tác vụ còn lại. Chúng tôi báo cáo kết quả trung vị của 5 lần chạy, mỗi lần sử dụng hạt giống ngẫu nhiên khác nhau. Kết quả tốt nhất cho mỗi tập dữ liệu được hiển thị in đậm. Giá trị càng cao càng tốt cho tất cả các chỉ số trong 6 tập dữ liệu.

Mô hình & Phương pháp # Tham số có thể huấn luyện SST-2 (Acc.) MRPC (Acc.) CoLA (MCC) QNLI (Acc.) RTE (Acc.) STS-B (PCC) Trung bình
RoB base(FF) 125M 94.8 90.2 63.6 92.8 78.7 91.2 85.2
RoB base(BitFit) 0.1M 93.7 92.7 62 91.8 81.5 90.8 85.4
RoB base(AdptD) 0.3M 94.2±0.1 88.5±1.1 60.8±0.4 93.1±0.1 71.5±2.7 89.7±0.3 83.0
RoB base(AdptD) 0.9M 94.7±0.3 88.4±0.1 62.6±0.9 93.0±0.2 75.9±2.2 90.3±0.1 84.2
RoB base(LoRA) 0.3M 95.1±0.2 89.7±0.7 63.4±1.2 93.3±0.3 78.4±0.8 91.5±0.2 85.2
RoB base(AdaLoRA) 0.3M 94.5±0.2 88.7±0.5 62.0±0.6 93.1±0.2 81.0±0.6 90.5±0.2 85.0
RoB base(DyLoRA) 0.3M 94.3±0.5 89.5±0.5 61.1±0.3 92.2±0.5 78.7±0.7 91.1±0.6 84.5
RoB base(FourierFT) 0.024M 94.2±0.3 90.0±0.8 63.8±1.6 92.2±0.1 79.1±0.5 90.8±0.2 85.0
RoB large(FF) 356M 96.4 90.9 68 94.7 86.6 92.4 88.2
RoB large(AdptP) 3M 96.1±0.3 90.2±0.7 68.3±1.0 94.8±0.2 83.8±2.9 92.1±0.7 87.6
RoB large(AdptP) 0.8M 96.6±0.2 89.7±1.2 67.8±2.5 94.8±0.3 80.1±2.9 91.9±0.4 86.8
RoB large(AdptH) 6M 96.2±0.3 88.7±2.9 66.5±4.4 94.7±0.2 83.4±1.1 91.0±1.7 86.8
RoB large(AdptH) 0.8M 96.3±0.5 87.7±1.7 66.3±2.0 94.7±0.2 72.9±2.9 91.5±0.5 84.9
RoB large(LoRA) 0.8M 96.2±0.5 90.2±1.0 68.2±1.9 94.8±0.3 85.2±1.1 92.3±0.5 87.8
RoB large(FourierFT) 0.048M 96.0±0.2 90.9±0.3 67.1±1.4 94.4±0.4 87.4±1.6 91.9±0.4 88.0

sức mạnh biểu đạt của cơ sở Fourier.

Đường cơ sở. Chúng tôi so sánh phương pháp FourierFT của mình với các phương pháp tinh chỉnh hiệu quả tham số (PEFT) phổ biến. Để đảm bảo so sánh toàn diện và công bằng, chúng tôi ưu tiên sao chép các thiết lập được sử dụng trong các công trình trước đây và tái sử dụng kết quả được báo cáo của họ. Các đường cơ sở được bao gồm là:

● Tinh chỉnh đầy đủ (FF) - Trong quá trình tinh chỉnh, mô hình cơ sở được khởi tạo với trọng số và độ lệch tiền huấn luyện, và tất cả các tham số sẽ trải qua cập nhật gradient.
● Bitfit (Zaken et al., 2021) - Chỉ có các vectơ độ lệch được tinh chỉnh trong khi tất cả các tham số khác được cố định.
● Tinh chỉnh bộ điều hợp - Dòng nghiên cứu này lần đầu tiên được khám phá bởi Houlsby et al. (2019), đề xuất phương pháp AdapterH. AdapterH chèn các bộ điều hợp hai lớp giữa các mô-đun tự chú ý và FNN, theo sau bởi kết nối dư sau đó. Chúng tôi so sánh với ba biến thể bổ sung của nó. AdapterL (Lin et al., 2020) hiệu quả hơn về tham số, với các lớp bộ điều hợp chỉ được áp dụng sau các mô-đun MLP và tiếp theo sau LayerNorm. AdapterP (Pfeiffer et al., 2020) triển khai các lớp bộ điều hợp sau lớp feed-forward. Thiết kế này được chọn thông qua tìm kiếm lưới bao gồm tất cả các thiết lập liên quan đến vị trí, số lượng bộ điều hợp, v.v. AdapterD (Rückle et al., 2020) nâng cao hơn nữa hiệu quả tham số bằng cách bỏ các lớp bộ điều hợp không được kích hoạt.
● LoRA (Hu et al., 2021) - LoRA là phương pháp tiên tiến nhất cho PEFT. Nó tham số hóa các cập nhật trọng số tăng dần bằng cách sử dụng các ma trận thấp hạng có thể huấn luyện.
● DyLoRA (Valipour et al., 2022) - Phương pháp này huấn luyện các mô hình LoRA động không cần tìm kiếm cho việc chọn hạng tốt nhất.
● AdaLoRA (Zhang et al., 2023) - Phương pháp này đề xuất tinh chỉnh dựa trên SVD và cắt bớt các giá trị đơn lẻ dư thừa với phân bổ hạng nhận biết tầm quan trọng.

4.1. Hiểu ngôn ngữ tự nhiên
Mô hình và Tập dữ liệu. Chúng tôi đánh giá phương pháp của mình trên benchmark GLUE (Đánh giá hiểu ngôn ngữ chung (Wang et al., 2018)), bao gồm một loạt rộng các tác vụ hiểu ngôn ngữ tự nhiên (NLU), bao gồm các tác vụ phân loại câu đơn, các tác vụ tương tự và paraphrase và các tác vụ suy luận ngôn ngữ tự nhiên. Chúng tôi tinh chỉnh các mô hình nền tảng RoBERTa Base và Large tiền huấn luyện (Liu et al., 2019) để đánh giá.

Chi tiết triển khai. Đối với cả hai mô hình, FourierFT được phép có 1000 trong số 768² (RoBERTa Base) và 1024² (RoBERTa Large) hệ số phổ có thể huấn luyện trong mỗi lớp, tức là n=1000. Chúng tôi lấy mẫu ngẫu nhiên các phần tử phổ không có độ lệch tần số, được chia sẻ¹ trên tất cả 24 (Base) và 48 (Large) lớp. Đối với tất cả 6 tập dữ liệu trong GLUE, chúng tôi điều chỉnh các siêu tham số của tốc độ học và các giá trị chia tỷ lệ. Chúng tôi tuân theo thiết lập thí nghiệm được áp dụng trong Hu et al. (2021), bao gồm việc tinh chỉnh chỉ các trọng số truy vấn và giá trị trong mỗi khối transformer và

¹Chúng tôi sử dụng giá trị 2024 làm hạt giống cho tất cả các lớp.

5

--- TRANG 6 ---
Tinh chỉnh hiệu quả tham số với Biến đổi Fourier rời rạc

Bảng 3. Kết quả từ các mô hình GPT-2 Medium và Large trên benchmark E2E. Chúng tôi trình bày kết quả từ epoch cuối cùng. Đối với tất cả các chỉ số, giá trị cao hơn cho thấy hiệu suất tốt hơn. * cho biết kết quả được lấy từ các công trình trước đây. Kết quả tốt nhất được hiển thị in đậm.

Mô hình Phương pháp # Tham số có thể huấn luyện BLEU NIST METEOR ROUGE-L CIDEr
GPT-2 Medium FT* 354.92M 68.2 8.62 46.2 71.0 2.47
AdptL* 0.37M 66.3 8.41 45.0 69.8 2.40
AdptL* 11.09M 68.9 8.71 46.1 71.3 2.47
AdptH* 11.09M 67.3±.68.5±.07 46.0±.2 70.7±.22.44±.01
LoRA 0.35M 68.9±.38.76±.06 46.6±.1 71.5±.12.53±.03
FourierFT 0.048M 69.1±.18.82±.0547.0±.3 71.8±.12.51±.02
GPT-2 Large FT* 774.03M 68.5 8.78 46.0 69.9 2.45
AdptL* 0.88M 69.1±.18.68±.03 46.3±.0 71.4±.2 2.49±.0
AdptL* 23.00M 68.9±.38.70±.04 46.1±.1 71.3±.22.45±.02
LoRA 0.77M 70.1±.38.83±.02 46.8±.2 72.0±.32.47±.02
FourierFT 0.072M 70.2±.28.90±.02 47.0±.2 71.8±.12.50±.02

Bảng 4. Điểm số trung bình trên MT-Bench và Vicuna được đánh giá bởi GPT-4. † cho biết cập nhật các lớp khác ngoài lmhead. Điểm số cao hơn thì tốt hơn.

Mô hình Phương pháp # Tham số có thể huấn luyện MT-Bench Vicuna
LLaMA1-7B LoRA† 159.9M 5.05±.36.85±.4
LoRA 33.5M 4.99±.36.81±.3
FourierFT 0.064M 5.09±.66.85±.8
LLaMA1-13B LoRA† 250.3M 5.28±.67.02±.3
LoRA 52.4M 5.21±.46.97±.4
FourierFT 0.08M 5.23±.37.14±.5
LLaMA2-7B LoRA† 159.9M 5.19±.17.38±.3
LoRA 33.5M 5.20±.37.35±.6
FourierFT 0.064M 5.18±.37.49±.4
LLaMA2-13B LoRA† 250.3M 5.78±.27.89±.5
LoRA 52.4M 5.80±.27.89±.6
FourierFT 0.08M 5.82±.37.92±.5

tinh chỉnh đầy đủ đầu phân loại. Chúng tôi cung cấp các siêu tham số trong Bảng 9 ở Phụ lục.

Kết quả. Kết quả được tóm tắt trong Bảng 2. Theo Hu et al. (2021), Zhang et al. (2023) và Valipour et al. (2022), chúng tôi chỉ định số lượng tham số có thể huấn luyện cho các lớp tinh chỉnh loại trừ đầu phân loại. Chúng tôi báo cáo trung vị của 5 kết quả hạt giống ngẫu nhiên, trong đó epoch tốt nhất được chọn cho mỗi lần chạy. Nhìn chung, FourierFT đạt được hiệu suất tốt hơn hoặc tương đương so với các phương pháp đường cơ sở với ít tham số có thể huấn luyện hơn đáng kể. Đáng chú ý, FourierFT vượt trội hơn tất cả các đường cơ sở bao gồm cả tinh chỉnh đầy đủ RoBERTa Base trên CoLA và RoBERTa Large trên RTE. Như đã đề cập trong Phần 3.2, số lượng tham số của LoRA phụ thuộc vào cả chiều rộng và chiều sâu của các mô hình, dẫn đến sự tăng trưởng đếm lớn hơn (LoRA: 0.8M/0.3M≈2.7; của chúng tôi: 0.048M/0.024M=2) so với FourierFT. Tuy nhiên, FourierFT vẫn hoạt động tương đương với LoRA, chứng minh tiềm năng khả năng mở rộng của phương pháp chúng tôi khi đối mặt với các mô hình thậm chí còn lớn hơn.

4.2. Sinh ngôn ngữ tự nhiên
Mô hình và Tập dữ liệu. Chúng tôi đánh giá hiệu suất của FourierFT trên tác vụ sinh ngôn ngữ tự nhiên (NLG) E2E (Novikova et al., 2017). Chúng tôi tinh chỉnh các mô hình GPT-2 (Radford et al., 2019) Medium (354M) và Large (774M), cả hai đều chỉ có bộ giải mã và có 24 và 36 khối transformer tương ứng. Benchmark E2E chứa khoảng 42.000 mẫu huấn luyện, 4.600 mẫu xác thực và 4.600 mẫu kiểm tra từ lĩnh vực nhà hàng.

Chi tiết triển khai. Chúng tôi báo cáo kết quả trước đây cho các đường cơ sở khác ngoài LoRA. Đối với cả LoRA và phương pháp của chúng tôi, chúng tôi tinh chỉnh các mô hình GPT-2 Medium và Large với bộ lập lịch tốc độ học tuyến tính trong 5 epochs, nơi chúng tôi điều chỉnh kích thước batch và tốc độ học. Chúng tôi báo cáo kết quả trung bình trên 3 lần chạy, nơi epoch cuối cùng được chọn cho mỗi lần chạy. Chúng tôi cung cấp các siêu tham số trong Bảng 10 ở Phụ lục.

Kết quả. Chúng tôi hiển thị kết quả trong Bảng 3. Chúng tôi lưu ý rằng FourierFT có thể đạt được hiệu suất tốt nhất trên hầu hết các chỉ số. Quan trọng hơn, FourierFT chỉ yêu cầu 13.7% và 9.4% số lượng tham số của LoRA, đối với các mô hình GPT-2 Medium và Large tương ứng.

4.3. Tinh chỉnh hướng dẫn
Mô hình và Tập dữ liệu. Tinh chỉnh hướng dẫn, như được mô tả trong (Ouyang et al., 2022; Wei et al., 2021; Mishra et al., 2021), đề cập đến quá trình tinh chỉnh một mô hình ngôn ngữ trên một tập hợp các cặp gợi ý và phản hồi. Chúng tôi áp dụng LoRA và FourierFT để tinh chỉnh các họ LLaMA (Touvron et al., 2023a) và LLaMA2 (Touvron et al., 2023b). Cụ thể, chúng tôi xem xét LLaMA-7B, LLaMA-13B, LLaMA2-7B và LLaMA2-13B làm các mô hình cơ sở, được tinh chỉnh trên tập dữ liệu Alpaca (Taori et al., 2023). Alpaca chứa 51K bản trình diễn tuân theo hướng dẫn được tạo ra từ text-davinci-003 (GPT-3.5) (Wang et al., 2022). Để đánh giá, chúng tôi sử dụng các mô hình tinh chỉnh để tạo ra phản hồi cho các câu hỏi được định trước, đến từ MT-Bench (Zheng et al., 2023) và Vicuna Eval (Chiang et al., 2023). GPT-4 lấy những câu trả lời này làm đầu vào và đánh giá chúng với điểm số trong phạm vi 10.

Chi tiết triển khai. Đối với LoRA, chúng tôi sử dụng r=64 và áp dụng hai cấu hình: (1) cập nhật tất cả các lớp tuyến tính ngoại trừ đầu mô hình hóa ngôn ngữ (lmhead); (2) chỉ cập nhật các ma trận WQ và WV. Đối với FourierFT, chúng tôi chỉ áp dụng cấu hình sau với n=1000. Để đảm bảo

6

--- TRANG 7 ---
Tinh chỉnh hiệu quả tham số với Biến đổi Fourier rời rạc

Bảng 5. Kết quả tinh chỉnh với các mô hình ViT Base và Large trên các tập dữ liệu phân loại hình ảnh khác nhau. Chúng tôi báo cáo độ chính xác (%) sau 10 epochs. Trung bình biểu thị độ chính xác trung bình của mỗi phương pháp trên tất cả các tập dữ liệu. Hiệu suất tốt nhất được hiển thị in đậm.

Mô hình Phương pháp # Tham số có thể huấn luyện OxfordPets StanfordCars CIFAR10 DTD EuroSAT FGVC RESISC45 CIFAR100 Trung bình
ViT-Base LP - 90.28±0.43 25.76±0.28 96.41±0.0269.77±0.6788.72±0.1317.44±0.43 74.22±0.10 84.28±0.11 68.36
FF 85.8M 93.14±0.40 79.78±1.15 98.92±0.0577.68±1.2199.05±0.0954.84±1.23 96.13±0.13 92.38±0.13 86.49
LoRA 581K 93.19±0.36 45.38±0.41 98.78±0.0574.95±0.4098.44±0.1525.16±0.16 92.70±0.18 92.02±0.12 77.58
FourierFT 72K 93.21±0.26 46.11±0.24 98.58±0.0775.09±0.3798.29±0.0427.51±0.64 91.97±0.31 91.20±0.14 77.75
FourierFT 239K 93.05±0.34 56.36±0.66 98.69±0.0877.30±0.6198.78±0.1132.44±0.99 94.26±0.20 91.45±0.18 80.29
ViT-Large LP - 91.11±0.30 37.91±0.27 97.78±0.0473.33±0.2692.64±0.0824.62±0.24 82.02±0.11 84.28±0.11 72.96
FF 303.3M 94.43±0.56 88.90±0.26 99.15±0.0581.79±1.0199.04±0.0868.25±1.63 96.43±0.07 93.58±0.19 90.20
LoRA 1.57M 94.82±0.09 73.25±0.36 99.13±0.0381.79±0.4598.63±0.0742.32±0.98 94.71±0.25 94.87±0.10 84.94
FourierFT 144K 94.46±0.28 69.56±0.30 99.10±0.0480.83±0.4398.65±0.0939.92±0.68 93.86±0.14 93.31±0.09 83.71
FourierFT 480K 94.84±0.05 79.14±0.67 99.08±0.0181.88±0.5098.66±0.0351.28±0.68 95.20±0.07 93.37±0.11 86.68

khả năng huấn luyện trên một GPU đơn, chúng tôi triển khai phương pháp lượng tử hóa trong Dettmers et al. (2023) để tinh chỉnh. Chúng tôi huấn luyện với cả hai phương pháp chỉ trong một epoch, và báo cáo điểm số trung bình của tất cả các câu trả lời. Chúng tôi cung cấp thiết lập siêu tham số trong Bảng 11 ở Phụ lục.

Kết quả. Kết quả được hiển thị trong Bảng 4. Chúng tôi thấy rằng sức mạnh biểu đạt của mô hình 13B mạnh hơn nhiều so với mô hình 7B, bất kể phương pháp tinh chỉnh nào được sử dụng. Hơn nữa, FourierFT khớp chặt chẽ hoặc hơi vượt trội hiệu suất của LoRA với ít hơn 0.2% tham số của nó. Chúng tôi cung cấp các ví dụ thực tế chứa câu hỏi, câu trả lời và đánh giá trong Phụ lục D.

4.4. Phân loại hình ảnh
Mô hình và Tập dữ liệu. Chúng tôi đánh giá phương pháp của mình trên tác vụ phân loại hình ảnh. Chúng tôi áp dụng các phiên bản Base và Large của mô hình nền tảng CV phổ biến, Vision Transformer (ViT) (Dosovitskiy et al., 2020). Các ViT được tiền huấn luyện trên tập dữ liệu ImageNet-21K (Ridnik et al., 2021). Các tập dữ liệu để tinh chỉnh bao gồm OxfordPets (372), CIFAR10 (10), DTD (47), EuroSAT (10) và RESISC45 (45) với không gian nhãn nhỏ, cũng như StanfordCars (196), FGVC (100) và CIFAR100 (100) với không gian nhãn lớn. Thông tin chi tiết được cung cấp trong Bảng 8 ở Phụ lục.

Chi tiết triển khai. Chúng tôi bao gồm ba đường cơ sở để đánh giá: Tinh chỉnh đầy đủ (FF), Thăm dò tuyến tính (LP, tinh chỉnh chỉ đầu phân loại), và LoRA. Đối với cả LoRA và phương pháp của chúng tôi, chỉ có các ma trận truy vấn và giá trị của ViT được cập nhật. Chúng tôi sử dụng r=16 cho LoRA và n={3000,10000} cho FourierFT. Chúng tôi điều chỉnh tốc độ học và weight decay cho tất cả các phương pháp, và đặt epoch huấn luyện tối đa là 10. Chúng tôi cung cấp các siêu tham số trong Bảng 12 ở Phụ lục.

²Số trong ngoặc cho biết số lượng lớp cho mỗi tập dữ liệu.

Kết quả. Bảng 5 tóm tắt kết quả cho 8 tập dữ liệu phân loại hình ảnh với các mô hình ViT Base và Large. Cả phương pháp LoRA và FourierFT đều vượt trội hơn đáng kể so với Thăm dò tuyến tính, chứng minh hiệu quả của chúng trong lĩnh vực CV. Phương pháp của chúng tôi thu được hiệu suất tương ứng bằng cách sử dụng 12.4% và 9.2% số lượng tham số của LoRA, với các mô hình ViT Base và Large tương ứng. Đáng chú ý, khi chúng tôi tăng số lượng tham số của FourierFT lên 41.1% (ViT Base) và 30.6% (ViT Large) của LoRA, nó có thể vượt trội hơn LoRA lần lượt 3.5% và 2.0%. Hơn nữa, phương pháp của chúng tôi thậm chí có thể (nhẹ nhàng) vượt trội hơn phương pháp Tinh chỉnh đầy đủ trên OxfordPets và DTD với mô hình ViT Large.

4.5. Nghiên cứu
Hiệu ứng của Độ lệch tần số. Chúng tôi khám phá cách hiệu suất bị ảnh hưởng bởi độ lệch tần số, tức là tần số trung tâm fc trong Phương trình 5. Chúng tôi trực tiếp áp dụng thiết lập siêu tham số tối ưu được tìm kiếm trong Bảng 2 và tinh chỉnh RoBERTa Base trên các tập dữ liệu MRPC, STS-B, CoLA và RTE. Từ Hình 5, chúng tôi lưu ý rằng hiệu suất tinh chỉnh của FourierFT không có độ lệch tần số nào có thể vượt qua hầu hết các trường hợp bị hạn chế bởi độ lệch tần số trung tâm. Điều này cho thấy tính phổ quát của phương pháp chúng tôi. Đáng ngạc nhiên, chúng tôi thấy rằng luôn có thể thu được kết quả tốt hơn "Không có độ lệch" bằng cách duyệt qua các giá trị fc. Vì việc duyệt này không hiệu quả, chúng tôi không tiến hành khám phá thêm trong bài báo này. Tuy nhiên, chúng tôi tin rằng việc làm cho fc có thể huấn luyện sẽ là một hướng mới đầy hứa hẹn để cải thiện FourierFT.

Khả năng mở rộng tham số. Chúng tôi khám phá mối quan hệ giữa số lượng tham số có thể huấn luyện và hiệu suất của LoRA và phương pháp của chúng tôi. Chúng tôi sử dụng tập hợp các hạng r={1,2,4,6,8,15} cho LoRA và n={50,100,200,1000,6144,12288} cho FourierFT trên 6 tác vụ của benchmark GLUE. Đối với cả LoRA và của chúng tôi, tốc độ học và các siêu tham số chia tỷ lệ được điều chỉnh. Để công bằng, chúng tôi đảm bảo rằng số lượng thử nghiệm cho việc tìm kiếm siêu tham số là 30 cho cả hai phương pháp. Như được hiển thị trong Hình 4, phương pháp của chúng tôi vượt trội hơn LoRA trên tất cả 6 tập dữ liệu. Chi tiết, phương pháp của chúng tôi tốt hơn đáng kể so với LoRA với cùng số lượng tham số, tức là {r=4, n=6144} & {r=8, n=12288}. Hơn nữa, chúng tôi quan sát thấy rằng số lượng tham số lớn hơn không phải lúc nào cũng mang lại lợi ích hiệu suất cho LoRA. Ngược lại, việc tăng n có thể cải thiện độ chính xác của FourierFT một cách nhất quán. Trên hầu hết các tác vụ, FourierFT với n=50 có thể đạt được hiệu suất tương đương hoặc thậm chí tốt hơn (MRPC, CoLA, RTE) so với LoRA với r=1. Trong trường hợp này, số lượng tham số trong LoRA khoảng 31 × so với của chúng tôi.

Tính biểu đạt của cơ sở. Biến đổi Fourier rời rạc nghịch đảo (IDFT) trong Phương trình 3 tương đương với phép nhân ma trận (Lu et al., 2021): S=BfFB⊺f, trong đó B là ma trận biến đổi của IDFT chứa cơ sở Fourier. Để đánh giá tính biểu đạt của nó, chúng tôi thay thế cơ sở Fourier bằng cơ sở ngẫu nhiên và trực giao tương ứng. Cụ thể, đối với F∈Rd1×d2, chúng tôi khởi tạo cơ sở ngẫu nhiên B1r∈Rd1×d1 và B2r∈Rd2×d2 với phân phối Gaussian thông thường. Sau đó Phương trình 3 trở thành S=B1rFB2r. Một cách tương tự được sử dụng cho cơ sở trực giao. Chúng tôi so sánh FourierFT với cơ sở ngẫu nhiên (R-B) và cơ sở trực giao (O-B) trên benchmark GLUE. Bảng 6 hiển thị kết quả. Chúng tôi lưu ý rằng cơ sở Fourier được sử dụng trong phương pháp của chúng tôi vượt trội hơn cơ sở ngẫu nhiên và trực giao. Ngoài ra, sức mạnh biểu đạt của cơ sở trực giao mạnh hơn nhiều so với cơ sở ngẫu nhiên. Sức mạnh biểu đạt mạnh hơn của cơ sở Fourier so với cơ sở trực giao tổng quát có thể được quy cho việc nắm bắt hiệu quả thông tin phổ của ∆W.

Bảng 6. Kết quả với ba loại cơ sở.
Mô hình RTE CoLA
Của chúng tôi R-B O-B Của chúng tôi R-B O-B
Base 79.1 72.7(↓8.1%) 75.6(↓4.4%) 63.8 58.7(↓8.0%) 60.0(↓6.0%)
Large 87.4 81.8(↓6.4%) 83.6(↓4.3%) 67.1 64.8(↓3.4%) 66.1(↓1.5%)

5. Kết luận
Trong bài báo này, chúng tôi nhằm mục đích đạt được bộ nhớ lưu trữ cực kỳ thấp cho một lần tinh chỉnh đơn của các mô hình nền tảng lớn. Điều này sẽ cho phép tùy chỉnh nhiều lần tinh chỉnh cho các lĩnh vực, tác vụ hoặc sở thích người dùng khác nhau. Để đạt được điều này, chúng tôi đề xuất một phương pháp tinh chỉnh đơn giản nhưng mạnh mẽ coi các thay đổi trọng số như các ma trận miền không gian và chỉ học các hệ số thưa thớt trong miền phổ. So với các đường cơ sở kiểu LoRA, cách tiếp cận của chúng tôi giảm số lượng tham số có thể huấn luyện khoảng 8∼500× trên một loạt rộng các tác vụ trong lĩnh vực NLP và CV.

6. Tuyên bố tác động
Bài báo này trình bày một công trình có mục tiêu thúc đẩy lĩnh vực Học máy. Có nhiều hậu quả xã hội tiềm tàng của công trình của chúng tôi, không có gì mà chúng tôi cảm thấy phải được nêu bật cụ thể ở đây.

Lời cảm ơn
Công trình này được hỗ trợ bởi Tài trợ NSFC số 62206067, Kế hoạch Nghiên cứu Hợp tác Xuyên trường C019 của HKUST–HKUST(GZ) 20 for 20 và Kế hoạch Tài trợ Liên kết Guangzhou-HKUST(GZ) 2023A03J0673.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo dài, giữ nguyên như trong bản gốc]

--- TRANG 9 ---
[Nội dung tiếp theo của tài liệu tham khảo]

--- TRANG 10 ---
[Nội dung tiếp theo của tài liệu tham khảo]

--- TRANG 11 ---
[Nội dung tiếp theo của tài liệu tham khảo]

--- TRANG 12 ---
Phụ lục của "Tinh chỉnh hiệu quả tham số với Biến đổi Fourier rời rạc"

A. Chi tiết về các tập dữ liệu
A.1. Benchmark GLUE
Benchmark GLUE (Wang et al., 2018) (Đánh giá hiểu ngôn ngữ chung) được sử dụng rộng rãi trong lĩnh vực NLP. GLUE bao gồm một tập hợp 8 tập dữ liệu NLP: MNLI (suy luận), SST-2 (phân tích cảm xúc), MRPC (phát hiện diễn giải), CoLA (khả năng chấp nhận ngôn ngữ), QNLI (suy luận), QQP (hỏi đáp), RTE (suy luận), và STS-B (tương tự văn bản). Chúng tôi tóm tắt thống kê của chúng trong bảng sau.

[Bảng 7 và các bảng tiếp theo với nội dung chi tiết về tập dữ liệu]

--- TRANG 13 ---
[Tiếp tục nội dung phụ lục]

--- TRANG 14 ---
[Tiếp tục nội dung phụ lục]

--- TRANG 15 ---
[Tiếp tục nội dung phụ lục]

--- TRANG 16 ---
[Tiếp tục nội dung phụ lục]

--- TRANG 17 ---
[Tiếp tục nội dung phụ lục]

--- TRANG 18 ---
[Tiếp tục nội dung phụ lục]

--- TRANG 19 ---
[Tiếp tục nội dung phụ lục]
