# Adapters: Một Thư Viện Thống Nhất cho Học Chuyển Giao Hiệu Quả Tham Số và Modular

Chúng tôi giới thiệu Adapters, một thư viện mã nguồn mở thống nhất học chuyển giao hiệu quả tham số và modular trong các mô hình ngôn ngữ lớn. Bằng cách tích hợp 10 phương pháp adapter đa dạng vào một giao diện thống nhất, Adapters mang lại sự dễ sử dụng và cấu hình linh hoạt. Thư viện của chúng tôi cho phép các nhà nghiên cứu và thực hành tận dụng tính modular của adapter thông qua các khối composition, cho phép thiết kế các thiết lập adapter phức tạp. Chúng tôi chứng minh hiệu quả của thư viện bằng cách đánh giá hiệu suất của nó so với fine-tuning đầy đủ trên các nhiệm vụ NLP khác nhau. Adapters cung cấp một công cụ mạnh mẽ để giải quyết các thách thức của các mô hình fine-tuning thông thường và thúc đẩy học chuyển giao hiệu quả và modular hơn.

## Giới Thiệu

Kích thước ngày càng tăng của các mô hình ngôn ngữ lớn được đào tạo sẵn (LLMs) đã khiến mô hình học chuyển giao đã được thiết lập của fine-tuning tất cả các tham số mô hình trên một nhiệm vụ downstream trở nên cực kỳ tốn kém. Hơn nữa, yêu cầu về hiệu quả tham số khi fine-tuning, mặc dù chắc chắn là tối quan trọng, không phải là nhược điểm duy nhất của mô hình fine-tuning LLM chủ đạo. Nó cũng gặp phải các vấn đề quan trọng khác như can thiệp tiêu cực, thiếu chuyển giao tích cực giữa các nhiệm vụ trong học đa nhiệm vụ, quên thảm khốc, và khái quát hóa kém.

Hai hướng nghiên cứu liên quan chặt chẽ nhằm giải quyết tập hợp những thách thức này đã thu hút sự chú ý đáng kể gần đây. Thứ nhất, fine-tuning hiệu quả tham số tập trung vào khía cạnh hiệu quả tính toán và tính khả thi bằng cách chỉ fine-tuning một số lượng nhỏ tham số cho các nhiệm vụ downstream. Thứ hai, học chuyển giao modular tập trung vào khía cạnh chuyển giao kiến thức bằng cách thiết kế các module mạng tự chứa có thể được tổng hợp để có hiệu suất đa nhiệm vụ và khái quát hóa tốt hơn. Trong thực tế, những điều này thường đại diện cho hai mặt của cùng một đồng xu. Các phương pháp thiết kế các thành phần nhỏ trong một mô hình ngôn ngữ để fine-tuning trên dữ liệu nhiệm vụ được gán nhãn, từ đây được gọi chung là adapters, vừa hiệu quả về tham số vừa có tính modular.

Việc phát hành ban đầu của AdapterHub đánh dấu nỗ lực đầu tiên để một cách có hệ thống làm cho adapters trở nên dễ tiếp cận với các nhà nghiên cứu và thực hành trong một framework dễ sử dụng. AdapterHub đã đề xuất một framework để dễ dàng tích hợp, đào tạo và sử dụng adapters cho các mô hình Transformer tiên tiến với những thay đổi tối thiểu. Nó cũng bổ sung thiết lập một nền tảng mở để chia sẻ, khám phá và tiêu thụ các module adapter được đào tạo sẵn. Trong khi AdapterHub ban đầu tập trung vào các adapter kiểu bottleneck, lĩnh vực các phương pháp adapter đã mở rộng đáng kể kể từ đó.

Với sự quan tâm ngày càng tăng đối với các phương pháp adapter, các công cụ và thư viện mới đã được phát triển. OpenDelta, PEFT của HuggingFace và LLM-Adapters là những ví dụ gần đây về các thư viện cố gắng thống nhất các phương pháp adapter trong một cơ sở mã duy nhất và mở rộng khả năng áp dụng của chúng cho các kiến trúc mô hình mới. Tuy nhiên, những công trình này chỉ tập trung vào khía cạnh hiệu quả tham số của adapters, bỏ qua khía cạnh modular của những phương pháp này.

Đóng góp. Dựa trên phiên bản ban đầu của AdapterHub, do đó chúng tôi đề xuất Adapters, một thư viện mới nhằm thống nhất học chuyển giao hiệu quả tham số và modular. So với lần lặp đầu tiên của AdapterHub và các thư viện đồng thời, các đóng góp chính của chúng tôi có thể được tóm tắt như sau: 1) Chúng tôi đề xuất một thư viện tự chứa tích hợp 10 phương pháp adapter đa dạng vào một giao diện thống nhất để sử dụng dễ dàng và cấu hình linh hoạt; 2) chúng tôi phát triển một cách đơn giản để tận dụng tính modular của adapters bằng cách thiết kế các khối composition cho phép định nghĩa linh hoạt các thiết lập adapter phức tạp; 3) chúng tôi tích hợp tất cả các phương pháp vào 20 mô hình dựa trên Transformer trải dài các ứng dụng NLP, thị giác và đa phương thức; 4) chúng tôi đánh giá hiệu suất của các triển khai adapter của chúng tôi so với fine-tuning đầy đủ trên một tập hợp đa dạng các nhiệm vụ.

## Bối Cảnh

Chúng tôi sử dụng thuật ngữ adapter theo nghĩa tổng quát hơn để chỉ một họ rộng lớn các phương pháp học chuyển giao chia sẻ hai thuộc tính định nghĩa: hiệu quả tham số và tính modular.

### Hiệu Quả Tham Số

Để các tham số của một mô hình ngôn ngữ được cấu thành từ một tập hợp các tham số được đào tạo sẵn Θ(đóng băng) và một tập hợp các tham số Φ (trong đó Φ có thể được giới thiệu mới hoặc Φ⊂Θ). Trong quá trình fine-tuning, các phương pháp adapter chỉ tối ưu hóa Φ theo hàm mất mát L trên tập dữ liệu D:

Φ*←arg min Φ L(D;{Θ,Φ})

Các phương pháp adapter khác nhau chèn tham số Φ tại các vị trí khác nhau của một mô hình lớn được đào tạo sẵn. Các adapter bottleneck, như một trong những phương pháp đầu tiên, giới thiệu các lớp feed-forward bottleneck trong mỗi lớp của một mô hình Transformer. Các thiết kế tiếp theo đã điều chỉnh self-attention của mô hình Transformer, các thuật ngữ bias, prompt đầu vào hoặc embeddings. Các dòng công việc bổ sung đã tập trung vào việc tối ưu hóa hiệu quả tham số và hiệu quả runtime của adapters hoặc đã cố gắng thống nhất nhiều thành phần trong một framework duy nhất.

### Tính Modular

Một mô hình học sâu modular được cấu thành từ các module mà mỗi module nắm bắt một chức năng cụ thể của mô hình đầy đủ, chẳng hạn như nhiệm vụ hoặc khả năng ngôn ngữ. Pfeiffer et al. đề xuất một phân loại các phương pháp học sâu modular bao gồm các chiều hàm tính toán, định tuyến, tổng hợp và đào tạo.

Định tuyến và tổng hợp có ý nghĩa đặc biệt ở đây vì chúng điều phối việc composition của nhiều module adapter, một chức năng quan trọng được tính modular tạo ra. Công việc hiện tại mẫu mực bao gồm sử dụng định tuyến ngẫu nhiên thông qua adapters, trung bình tham số adapter, tổng hợp chức năng tuần tự của các module adapter cũng như tổng hợp đầu ra có trọng số và dựa trên attention.

Cuối cùng, dọc theo chiều đào tạo, tính modular của adapters cho phép sử dụng các module adapter được đào tạo sẵn làm khởi tạo cho fine-tuning thêm.

## Thư Viện Adapters

Adapters được xây dựng trên nhiều quyết định thiết kế được thiết lập trong bản phát hành ban đầu của AdapterHub, nhưng cung cấp các mở rộng đáng kể cả 'theo chiều ngang' (ví dụ: mở rộng hỗ trợ cho nhiều kiến trúc neural được đào tạo sẵn hơn, mở rộng phạm vi bao quát của các kiến trúc adapter) và 'theo chiều dọc' (ví dụ: thêm các khả năng composition và xử lý mới). Bảng 1 đưa ra tổng quan về sự khác biệt giữa AdapterHub ban đầu và Adapters.

### Tích Hợp Transformers

Không giống như AdapterHub ban đầu, Adapters được thiết kế như một gói độc lập hoạt động như một add-on cho thư viện Transformers. Adapters cung cấp các triển khai adapter và các phương pháp quản lý có thể được chèn vào các checkpoint Transformer được đào tạo sẵn mà không cần sửa đổi mã mô hình gốc trực tiếp. Chúng tôi cung cấp hai cách tiếp cận cho mục đích này: (i) bằng cách gắn vào các mô hình hiện có và (ii) bằng cách cung cấp các lớp mô hình chuyên biệt của riêng chúng tôi.

### Giao Diện Adapter Thống Nhất

Adapters định nghĩa một giao diện chung của các phương pháp bao gồm toàn bộ vòng đời làm việc với adapters. Điều này bao gồm các phương pháp để thêm, kích hoạt, lưu, phát hành, tải, tổng hợp và xóa các module adapter. Khi thêm một adapter mới vào một mô hình, nó được đặt một chuỗi định danh duy nhất. Tất cả các phương pháp liên quan đến adapter sau đó chỉ sử dụng chuỗi này để xác định module adapter mà một thao tác nên được thực hiện.

### Các Phương Pháp Adapter

Mỗi phương pháp adapter được định nghĩa bởi một đối tượng cấu hình hoặc chuỗi cho phép tùy chỉnh linh hoạt các thuộc tính khác nhau của một module adapter, bao gồm vị trí, dung lượng, kết nối dư, khởi tạo, v.v. Chúng tôi phân biệt giữa các phương pháp đơn bao gồm một loại module adapter và các phương pháp phức tạp bao gồm nhiều loại module adapter khác nhau.

#### Các Phương Pháp Đơn

Adapters hỗ trợ các phương pháp adapter đơn giới thiệu tham số trong các module feed-forward mới như adapters bottleneck, giới thiệu prompt tại các vị trí khác nhau như prefix tuning, tái tham số hóa các module hiện có như LoRA hoặc tái tỷ lệ biểu diễn đầu ra của chúng như (IA)³.

#### Các Phương Pháp Phức Tạp

Trong khi các phương pháp fine-tuning hiệu quả khác nhau và cấu hình thường được đề xuất như độc lập, việc kết hợp chúng để đào tạo chung đã được chứng minh là có lợi. Để làm cho quá trình này dễ dàng hơn, Adapters cung cấp khả năng nhóm nhiều instance cấu hình sử dụng lớp ConfigUnion.

### Composition Adapter

Trong khi các khía cạnh modular và composability của adapters đã thấy sự quan tâm ngày càng tăng trong nghiên cứu, các thư viện mã nguồn mở hiện có đã phần lớn bỏ qua những khía cạnh này. Adapters làm cho các composition adapter trở thành một phần trung tâm và dễ tiếp cận của việc làm việc với adapters bằng cách cho phép định nghĩa các thiết lập adapter phức tạp, được tổng hợp. Chúng tôi định nghĩa một tập hợp các khối composition đơn giản mà mỗi khối nắm bắt một phương pháp cụ thể để tổng hợp chức năng của nhiều adapters.

### Các Mô Hình Được Hỗ Trợ

Tại thời điểm phát hành, Adapters có hỗ trợ tích hợp sẵn cho 20 kiến trúc mô hình được áp dụng rộng rãi được bao gồm trong thư viện Transformers. Điều này bao gồm các mô hình encoder văn bản như BERT và DeBERTa, các mô hình decoder văn bản như GPT-2, các mô hình sequence-to-sequence như BART và T5, các mô hình encoder thị giác như ViT, cũng như các mô hình đa phương thức như CLIP.

### Hệ Sinh Thái AdapterHub

Adapters được tích hợp vào hệ sinh thái mã nguồn mở rộng lớn hiện tại được giới thiệu bởi AdapterHub. Nổi bật nhất, điều này bao gồm AdapterHub.ml như một nền tảng để chia sẻ và khám phá các module adapter được đào tạo sẵn. Adapters tiếp tục mở rộng khả năng chia sẻ adapters bằng cách tích hợp với Model Hub của HuggingFace, đã nổi lên như một trong những nền tảng chính để mở mã nguồn các checkpoint mô hình.

## Đánh Giá Adapter

Ngoài sự dễ sử dụng đã đề cập, chúng tôi chỉ ra rằng các phương pháp adapter được cung cấp bởi thư viện của chúng tôi có hiệu suất tốt trên một loạt các cài đặt. Để làm điều này, chúng tôi tiến hành đánh giá trên các triển khai adapter đơn được cung cấp bởi Adapters. Chúng tôi chứng minh hiệu quả của những phương pháp này so với fine-tuning đầy đủ trên nhiều loại nhiệm vụ khác nhau.

### Thiết Lập

Chúng tôi tiến hành tìm kiếm lưới trên một loạt các siêu tham số đào tạo phổ biến, thay đổi tỷ lệ học giữa {10⁻⁵, 10⁻⁴, 5·10⁻⁴, 10⁻⁴, 10⁻³} và số epoch giữa {5, 10, 20, 30}. Chúng tôi cũng bổ sung lưới với một số siêu tham số cụ thể của adapter.

### Kết Quả

Điều rút ra rõ ràng từ các đánh giá của chúng tôi là tất cả các triển khai adapter được cung cấp bởi framework của chúng tôi đều cạnh tranh với fine-tuning mô hình đầy đủ, trên tất cả các lớp nhiệm vụ. Các cách tiếp cận cung cấp nhiều siêu tham số có thể điều chỉnh hơn như Adapters Bottleneck, LoRA và Prefix Tuning có thể dự đoán có hiệu suất topline cao nhất, thường vượt qua fine-tuning đầy đủ. Tuy nhiên, các phương pháp cực kỳ tiết kiệm tham số như (IA)³, chỉ thêm <0.005% số tham số của mô hình cơ sở, cũng hoạt động đáng khen ngợi và chỉ kém một phần nhỏ.

## Kết Luận

Chúng tôi đã trình bày Adapters, một thư viện mới để nghiên cứu và ứng dụng adapters. Không giống như các giải pháp tương tự, Adapters tập trung đồng đều vào khía cạnh hiệu quả tham số và modular của adapters. Thư viện của chúng tôi triển khai một tập hợp đa dạng các phương pháp adapter dưới một giao diện thống nhất cho phép cấu hình linh hoạt và trộn lẫn các cách tiếp cận khác nhau. Chúng tôi đã đề xuất một hệ thống khối xây dựng đơn giản để tận dụng tính modular của adapters để xây dựng các thiết lập adapter phức tạp. Adapters tích hợp chặt chẽ vào các hệ sinh thái HuggingFace và AdapterHub và các triển khai adapter của nó cho thấy hiệu suất cạnh tranh với fine-tuning đầy đủ.
