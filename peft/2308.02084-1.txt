# 2308.02084.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2308.02084.pdf
# File size: 1217876 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
Efficient Model Adaptation for Continual Learning
at the Edge
Zachary A. Daniels, Jun Hu, Michael Lomnitz, Phil Miller, Aswin Raghavan, Joe Zhang, Michael Piacentino, and
David Zhang
Abstract —Most machine learning (ML) systems assume sta-
tionary and matching data distributions during training and de-
ployment. This is often a false assumption. When ML models are
deployed on real devices, data distributions often shift over time
due to changes in environmental factors, sensor characteristics,
and task-of-interest. While it is possible to have a human-in-
the-loop to monitor for distribution shifts and engineer new
architectures in response to these shifts, such a setup is not
cost-effective. Instead, non-stationary automated ML (AutoML)
models are needed. This paper presents the Encoder-Adaptor-
Reconfigurator (EAR) framework for efficient continual learning
under domain shifts. The EAR framework uses a fixed deep neu-
ral network (DNN) feature encoder and trains shallow networks
on top of the encoder to handle novel data. The EAR framework
is capable of 1) detecting when new data is out-of-distribution
(OOD) by combining DNNs with hyperdimensional computing
(HDC), 2) identifying low-parameter neural adaptors to adapt the
model to the OOD data using zero-shot neural architecture search
(ZS-NAS), and 3) minimizing catastrophic forgetting on previous
tasks by progressively growing the neural architecture as needed
and dynamically routing data through the appropriate adaptors
and reconfigurators for handling domain-incremental and class-
incremental continual learning. We systematically evaluate our
approach on several benchmark datasets for domain adaptation
and demonstrate strong performance compared to state-of-the-
art algorithms for OOD detection and few-/zero-shot NAS.
Impact Statement —Generally, ML systems assume stationary
and matching data distributions during training and deploy-
ment. In practice, deployed ML systems encounter shifts in
the input distribution over time, e.g., due to environmental
factors. Human-in-the-loop monitoring for distribution shifts and
hand-engineering new ML architectures is cost-prohibitive. Our
framework automatically identifies when domain shifts occur and
adapts the neural architecture to account for these shifts. In
edge computing and Internet-of-Things (IoT) applications, the
deployed hardware has limited compute resources. Compared
to many existing approaches for AutoML, we focus on fast,
computationally-inexpensive methods for OOD detection and
NAS to learn low-parameter adaptor models.
Index Terms —AutoML, Continual Learning, Edge Comput-
ing, Out-of-Distribution Sample Detection, Progressive Neural
Networks, Zero-Shot Neural Architecture Search
Submitted: May 31, 2023
This research is based upon work supported in part by the Office of the
Director of National Intelligence (ODNI), Intelligence Advanced Research
Projects Activity (IARPA), via Contract No: 2022-21100600001. The views
and conclusions contained herein are those of the authors and should not be
interpreted as necessarily representing the official policies, either expressed
or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government
is authorized to reproduce and distribute reprints for governmental purposes
notwithstanding any copyright annotation therein.
All authors are researchers with the Center for Vision Technologies, SRI
International, Princeton, New Jersey 08540 USA (corresponding author email:
zachary.daniels@sri.com)I. I NTRODUCTION
IN traditional ML, it is assumed that the distribution of input
features and output labels do not change once the model
is trained; i.e., the model is fit to a stationary distribution of
features on a specific task, and during inference/deployment,
the model is applied to the same task with a matching
distribution of input features. In contrast, ML models deployed
on real-world devices for applications involving IoT, edge
computing, and sensor network analysis often contend with
distribution shift over time due to changes in 1) the sensor
(e.g., a model trained on high-resolution imagery is applied
to low-resolution imagery), 2) the task space (e.g., a model
trained to detect one set of vehicles is reused for detecting a
different set of vehicles), or 3) the environment (e.g., a model
trained on sunny days is run during a storm).
While it is possible for a human-in-the-loop to monitor for
distribution shifts and engineer new architectures to account
for these shifts, such a setup is generally not cost-effective. A
more practical alternative is to develop a ML system that can
automatically determine 1) when the distribution of inputs and
outputs has changed, and 2) determine how to adapt its archi-
tecture to handle the new data distribution while maintaining
performance on previous distributions. Furthermore, for many
of the applications involving ML under distribution shifts,
adaptation must be done quickly on resource-constrained
hardware. This work proposes an approach to efficient non-
stationary AutoML.
Specifically, we focus on the problem of domain- and class-
incremental continual learning [1], [2] where distribution shifts
in the input and output spaces occur over time. Domain-
incremental continual learning involves learning the same kind
of problem under different contexts (e.g., modalities). Class-
incremental continual learning involves incrementally learning
to assign labels to a growing set of classes, potentially under
different contexts.
In our experiments, we consider four cases of domain shift:
1) in the most extreme case, both the input modality and output
label sets change, 2) the input modality changes, but the class
labels remain the same, 3) the input modality remains the
same, but the label sets change, and 4) in the most subtle
case, the input modality and class labels remain the same, but
environmental factors change.
The goal of this work is to learn to identify when the
data distribution has changed and rapidly adapt ML models to
new domains (non-stationary AutoML) with limited compute
resources. We introduce the Encoder-Adaptor-ReconfiguratorarXiv:2308.02084v2  [cs.LG]  13 Oct 2023

--- PAGE 2 ---
2
(EAR) framework, which consists of three components:
•Encoder: Fixed pre-trained feature extraction backbone
•Adaptors: Shallow NNs that facilitate feature transfer
to new data distributions
•Reconfigurator: Light-weight model that enables rapid
adaptation to new task spaces with little re-training
We focus on the research problems of 1) how to identify when
data distributions shift, 2) how to grow the ML model as
needed, balancing performance on the new domain and model
efficiency, and 3) how to perform continual learning via intel-
ligent dynamic data routing through adaptors/reconfigurators
to minimize catastrophic forgetting of previous domains. This
paper presents the following contributions:
•Introduction of the Encoder-Adaptor-Reconfigurator
framework for efficient model adaptation to distribution
shifts on resource-constrained hardware
•Formulation of and training procedure for learning deep
hyperdimensional (HD) [3] adaptor-reconfigurators for
joint OOD detection and robust classification
•Formulation of a spectral analysis-based approach to
zero-shot neural architecture search [4]
•Demonstration of the EAR framework in a continual
learning setting via progressive neural networks [5], [6]
with dynamic data routing
•High performance compared to state-of-the-art algorithms
for OOD detection and few/zero-shot NAS on benchmark
domain adaptation datasets
II. B ACKGROUND
A. Progressive Neural Networks for Continual Learning
We explore the problem of continual learning [1], [2] where
an agent is trained on a sequence of tasks, and the agent must
balance plasticity vs stability: it must learn to solve the new
task while minimizing (catastrophic) forgetting of previous
tasks. Van de Ven et al. [2] categorized continual learning
into three types: task-, domain-, and class-incremental. Task-
incremental learning involves solving sequences of tasks where
the agent is explicitly told the current task-of-interest. In the
domain-incremental setting, the agent does not know what the
the current task-of-interest is, but the structure of the problem
does not change between tasks (only the input distribution
exhibits shifts), and the agent can solve the current task
without explicitly identifying the task. In the class-incremental
setting, the structure of the problem changes over time (i.e.,
new class sets are added). In this case, the agent must both
identify and subsequently solve the current task-of-interest.
The EAR framework is designed for the most-challenging
class-incremental setting, but can be trivially applied to the
task-incremental and domain-incremental settings.
There are three broad approaches to training continual learn-
ing agents [7]. Replay-based methods save examples of previ-
ously encountered tasks (explicitly, via exemplars/prototypes,
via generative memory, or in compressed representations), and
periodically replay these examples to minimize catastrophic
forgetting. Regularization-based methods impose constraints
on the learning behavior of the network (e.g., through mod-
ification of the loss function) to prevent the model fromoverfitting to the new task and overwriting knowledge about
the old tasks. Architecture-based methods intelligently grow
and prune the model architecture. Our EAR framework is
an architecture-based approach to continual learning. Other
architecture-based approaches include Progressive Neural Net-
works [5], [6], Dynamically Expanding Networks [8], “Com-
pacting, Picking, and Growing” [9], “Learn to Grow” [10],
and Comprehensively Progressive Bayesian NNs [11].
The EAR architecture can be thought of as a special case of
a progressive NN. Progressive NNs grow lateral connections,
thus, avoiding forgetting at the cost of increased resource use.
The EAR framework progressively grows adaptors and recon-
figurators off of a frozen feature encoder backbone. To extend
progressive NNs, we propose a novel method for dynamically
routing data through the appropriate adaptors/reconfigurators,
and we utilize ZS-NAS to identify where the adaptors should
be added and the structure of the adaptors.
B. Out-of-Distribution Detection with Deep Neural Networks
Our approach aims to automatically identify when the input
distribution has changed (OOD detection) [12]. In particular,
we focus on the setting of novelty detection [13] where the
OOD detector only sees in-distribution (ID) samples during
training. Our model learns adaptors that project data into
learned representations that can be used for joint OOD detec-
tion and classification. There are multiple approaches to OOD
detection with deep networks. In the most basic approach,
a deep network is trained with standard cross-entropy and
feature vector immediately preceeding the classifier layer is
used to train an OOD detection model (e.g., using off-the-
shelf classic OOD and novelty detection algorithms such as
the one-class SVM [14], empirical and robust covariance
estimates [15], local outlier factors [16], isolation forests [17],
[18]). Alternatively, statistics about the softmax probability
distribution output by a pre-trained neural network model
can be analyzed to separate OOD from ID samples (e.g.,
[19]). Extending beyond single-layer approaches, state-of-the-
art methods fuse information from multiple layers of pre-
trained networks to detect OOD samples (e.g., gram matrices
[20] and hyperdimensional feature fusion [21]). Finally, the
structure and training of the NN can be modified to learn repre-
sentations designed for joint OOD detection and classification,
e.g., using supervised contrastive learning in combination with
a deep nearest neighbor classifier [22]. Our approach combines
multi-layer feature analysis with learned representations, ex-
tending the HD feature fusion method by using adaptors to
automatically learn projections from features to HD vectors
instead of using random fixed projection matrices.
C. Hyperdimensional Computing
Our approach to OOD detection and classification is built
around hyperdimensional computing (HDC) [3], [23], [24].
HDC is a neuro-inspired neurosymbolic compute paradigm
that represents discrete pieces of information as high-
dimensional, low-precision, distributed vectors. In contrast to
DNNs, HDC uses low-power, requires low-precision, and has
been shown to be robust to corruptions in the input data.

--- PAGE 3 ---
3
HDC is built off of the mathematics of manipulating random
pseudo-orthogonal vectors in high-dimensional spaces. HDC
is built around two key operations: 1) binding which takes two
input vectors and generates a new vector that is dissimilar to
each of the inputs and 2) bundling (a.k.a. superposition) which
takes two or more input vectors and generates a new vector
that is similar to the inputs. We learn adaptors that represent
layer-wise features as pseduo-orthogonal binary vectors, and
then, reconfigurators use majority voting to bundle all of the
samples from a class into a binary class prototype vector.
New samples can be classified into existing classes if its HD
vector is close to one of the class prototype vectors based on
hamming distance, or it can be classified as OOD if it is far
from all of the class prototypes.
D. Zero-Shot Neural Architecture Search
DNNs have been shown to naturally facilitate transfer
learning [25], even by simply finetuning the final layer(s) of
the model. However, when there are dramatic shifts in the
input distribution or task-space, finetuning just the final layers
may be insufficient, and finetuning the entire network could be
cost-prohibitive and lead to catastrophic forgetting of previous
domains. Furthermore, a network architecture which is opti-
mized to one domain may not perform optimally on another.
The EAR framework freezes a feature encoder network trained
on one domain and learns sets of adaptors/reconfigurators that
tap the encoder at different locations. To optimize performance
on the new task while constraining the growth of the model, it
is necessary to carefully design the adaptor layers and identify
which tap points will best facilitate transfer learning to the new
domain. To identify the location and structure of the adaptors
and reconfigurators, we employ NAS [26].
NAS consists of three components:
•The search space is the space of what architectures can
be represented in the search process
•The search strategy is the algorithm used to find and
test architectures, balancing exploration and exploitation.
Examples include reinforcement learning, Bayesian opti-
mization, and evolutionary algorithms.
•The performance estimation strategy: quantifies how
good an architecture is, what the performance measure
is, and what the constraints are.
In this work, the search space defines what the adaptors look
like and where to place them, the search strategy compatible
with our EAR framework is any global optimization strategy
(we use Bayesian optimization), and our main novel contribu-
tion is defining the performance estimation strategy.
The EAR framework is intended to be deployed on low-
resource hardware and ideally adapts in a quick and ef-
ficient manner. For these reasons, we focus on zero-shot
neural architecture search [4], [27], [28]. ZS-NAS evaluates
candidate architectures without training the architecture via
proxy heuristics, which predict properties correlated with how
well a candidate model is expected to perform after it is
trained. We propose an approach based on spectral analysis
of the feature spaces of the adaptors, but a number of other
ZS-NAS approaches exist, including snip [29], grasp [30],
Fig. 1: High-level diagram of the EAR architecture
fisher score [31], Jacobian-covariance score [4], Synflow [32],
grad-norm, Φ-Score [33], and Zen-NAS [33]. Most work
formulates the proxy heuristic in terms of the gradients of
the candidate architecture over a random batch of data. In
contrast, our approach uses a gradient-free proxy, making it
more computationally-efficient to compute.
III. M ETHODOLOGY
A. The Encoder-Adaptor-Reconfigurator Framework for Con-
tinual Learning
We propose the EAR framework for handling domain
shift over time in compute-constrained settings (Fig. 1). The
framework is composed of three components: a fixed pre-
trained feature extraction backbone encoder , a set of shallow
NNadaptors that facilitate feature transfer to new data distri-
butions, and a light-weight reconfigurator that enables rapid
adaptation to new task spaces with little re-training.
In our experiments, the encoder is a DNN pre-trained
on a large-scale dataset and finetuned on the first domain
encountered; after which, it is frozen. As the model encounters
new domains, shallow adaptors, which are laterally connected
to tap points of the encoder, are learned. These adaptors effi-
ciently transform the features tuned for the first domain to be
useful for subsequent domains. Our adaptors consist of a few
convolutional and dense layers that transform unconstrained
feature vectors into binary HD feature vectors (Section III-B).
The output per-adaptor HD vectors feed into a reconfigurator
that is the predictive model for the new domain (a joint OOD
detector and classifier). The reconfigurator bundles all of per-
adaptor HD vectors into a single aggregated HD vector per
input instance. During training, the aggregated HD vectors of
all data in the training set from a single class are bundled into
a single prototype per class. During inference, classification
is performed by comparing the aggregated HD vector of an
instance with all of the class prototype vectors stored by the
reconfigurator. The instance is assigned the class of the nearest
prototype, or if it isn’t close to any prototype, it is assigned
to be OOD for the domain associated with the reconfigurator
(details in Section III-B).
In Fig. 2, we show the high-level control flow of the EAR
architecture in a continual learning scenario. We assume that
the model only sees data streaming from a single domain/task

--- PAGE 4 ---
4
Fig. 2: Control flow for continual learning using the EAR framework
at a given time, but the domain/task can shift at any time.
Thus, the model needs to operate over sequences of tasks
where task boundaries are not known. As new data arrives,
it is passed through the encoder once and then passed through
each set of adaptors/reconfigurators. If the data is ID according
to any reconfigurator, then it is classified according to the
reconfigurator of closest match (smallest OOD score). When a
new task is encountered, new samples will appear to be OOD
to all reconfigurators. Once the model is sufficiently confident
that there has been a domain shift, it will verify with an oracle
(e.g., human-in-the-loop) that a shift has occurred, and it will
start collecting data for the new domain. Note that we do not
discuss how to automatically annotate novel samples in this
paper as it is not our focus, and thus assume an oracle exists for
this purpose. Once the buffer hits its capacity limit, the oracle
labels the data, and the system uses ZS-NAS (Section III-C) to
identify the structure and placements of a new set of adaptors
and reconfigurator, trained on the collected data. To determine
when a new domain/task appears, the model monitors whether
the proportion of the last Ndata samples assigned as OOD
is greater than a specified threshold; after which, the update
process is triggered.
B. Deep Hyperdimensional Computing for Joint Out-of-
Distribution Sample Detection and Classification
Wilson et al. showed that HDC [21] can be combined with
pre-trained DNNs for novelty detection (i.e., when the OOD
detector only sees ID data during training). Their approach
projects the outputs of every layer of a pre-trained NN to a
high-dimensional random vector via a binding operation. The
layer-wise HD vectors are aggregated into a single vector, and
HD prototype vectors are learned for each class. If the HD
vector of a new instance has a distance larger than a fixed
threshold to all of the class prototypes, it is flagged as OOD.
This work extends the method of Wilson et al. The novelty
of our approach is: 1) instead of using random fixed linear
projections to map layer-wise outputs to HD vectors, we use
adaptors to learn non-linear projections to a pre-determined
fixed set of HD vectors; 2) we perform fusion from a small
subset of layers determined via ZS-NAS instead of every layer;
3) we project to binary HD vectors instead of real-valued HD
vectors, saving memory and enabling lighter computation; and
4) our method enables the model to learn to perform joint OODdetection-classification in an end-to-end manner, resulting in
more discriminative representation learning.
1) Basic Model for Classification: To begin, we explain
how the adaptors can learn a classification model by mapping
input samples to HD vectors using a combination of NNs and
HDC operations. For a specific domain, given an input sample
x, an adaptor ftap
ada(.)map features from a tap point tapof the
encoder model fenc(.)to a binary HD vector htap
ada(.):
πtap
ada(x) =ftap
ada(fenc(x)),
htap
ada(x) =sample element i
as 1 with probability πtap,(i)
ada(x),
as 0 with probability 1−πtap,(i)
ada(x)
∀i∈[0, len(htap
ada(x))](1)
The adaptors predict a score between 0 and 1 for each element
of the HD vector (e.g., via a sigmoid activation). To cast this
pseudo-binary output vector to a binary HD vector, we sample
each element according to the element-wise score.
The reconfigurator serves two purposes:
1) It aggregates the HD vectors over the set of all adaptors
Afor the domain into an HD vector hagg(.)by bundling:
hagg(x) =round
1
|A|X
(tap,ada )∈Ahtap
ada(x)
 (2)
2) It learns prototypes for each class hclass
proto for the domain
by bundling all training instances from a class XC:
hclass
proto =round 
1
|XC|X
xc∈XChagg(xc)!
(3)
To assign a class label to an input instance x, we compute
hagg(x)and select the class with smallest hamming distance:
ˆy(x) =argmin classdhamming (hagg(x), hclass
proto) (4)
2) Training the Adaptors: To train the adaptors, we first
generate a unique HD vector per class per adaptor (i.e., for
ten classes and five adaptors, fifty pseudo-orthogonal vectors
are generated) via Algorithm 1 ( ⊗is the Kronecker product).
Algorithm 1 first generates a symmetric orthogonal matrix
satisfying {−1,1}n×n. The first row and column of the matrix
are removed to improve stability of training the adaptors.

--- PAGE 5 ---
5
Algorithm 1 Algorithm for generating binary HD vectors
Require: n > k
Require: nis power-of-two
k←number of classes ×number of adaptors
n←dimensionality of HD vector + 1
C←1 1
1−1
C0←clone (C)
i= 0
while i <log2(n/2)do
C←C0⊗C
i←i+ 1
end while
C←C[1 :n,1 :n]
Shuffle the rows of C
C←C[0 :k,0 :n−1]
C[C=−1]←0
While this breaks orthogonality, the matrix remains pseudo-
orthogonal. The rows of the matrix are shuffled (helps stabilize
the training of the adaptors), and HD vectors for each com-
bination of adaptor and class label are selected. Finally, all
elements of the HD vectors that are −1are set to 0.
We select the dimensionality of the HD vectors to be
2⌈log2(#adaptors ∗#classes +1)⌉−1. This ensures that for each
adaptor-class pair, there will be a unique mutually (pseudo-
)orthogonal HD vector. This vector serves as the target output
for any data sample of the specific class that passes through the
corresponding adaptor. By forcing orthogonality between the
HD vectors across adaptors by construction of the target HD
vectors, binding operations are not needed during aggregation.
To map from inputs xto the target HD vectors for an
adaptor, we treat the mapping as a high-dimensional binary
multi-label classification problem. We use the weighted binary
focal cross-entropy loss [34] averaged over the every element
iof the predicted HD vector, which forces the adaptors to
focus on harder-to-classify samples during training:
ℓfoc=1
#dims#dimsX
i=0−αi∗(1−p(i)
target )γ∗log(p(i)
target )(5)
p(i)
target is the probability that the element ioutput by the
adaptor is assigned to its correct target value, αis a weight
term that corrects for imbalance (computed from the training
data) within the binary element-wise “labels”, and γis a
constant controlling how much focus is put on harder-to-
classify samples ( gamma = 2 in our experiments).
Beyond enabling easy feature-fusion between adaptors,
there are other practical reasons for projecting to target binary
class HD vectors at each adaptor. Every class for every adaptor
is assigned a unique HD vector. The goal is to map all input
instances to the HD vector corresponding to their class label
for the given adaptor. To learn this mapping, if each HD
vector has a dimensionality of D, then a binary classifier is
learned for each of the Ddimensions. Because each element
is assigned either 0 or 1 randomly and all instances from the
same class share a target class vector, this effectively meansthat the model is learning to predict a random partitioning of
the classes into two meta-labels for each element of the HD
vector. Because the target HD vectors are mutually orthogonal
per class, these Dclassifiers have low-redundancy. The map-
ping to an HD vector can be thought of as an ensembling
method whereby each adaptor forms a strong multi-class
classifier by ensembling many low-redundancy weak binary
classifiers. Furthermore, bundling between adaptors is another
mathematically-principled form of ensembling. Combining all
of these characteristics, the mapping from inputs to a final
HD vector embedding ultimately leads to highly discriminative
models with high noise tolerance.
3) Out-of-Distribution Detection: To determine if a sample
is OOD for a given reconfigurator, the aggregate HD vector is
computed and compared against the class prototype HD vec-
tors in the reconfigurator. If the minimum hamming distance
to any prototype is larger than a fixed threshold τ, then the
sample is predicted to be OOD:
ood(x) = min
classdhamming (hagg(x), hclass
proto)> τ (6)
This works well if we are only interested in determining if a
sample is OOD for a single domain. It is often the case that we
need to determine if a sample is OOD over all domains, or we
need to identify which set of adaptors/reconfigurators the data
should be routed through when knowledge of the true domain
is unknown. Such cases may require different thresholds for
the reconfigurator, and the OOD scores of each reconfigurator
may not be one-to-one comparable (e.g., if it is naturally easier
to (over)fit a set of adaptors to one domain vs another, noise
characteristics may make distances to the nearest prototype not
directly comparable). Thus, we need a calibrated OOD score
that is comparable between adaptor sets.
To obtain this calibrated OOD score, we fit a probability
distribution over the distances to the nearest prototype for
the training set. Building upon work in extreme value theory
and open set classification [35], we fit a 3-Parameter Weibull
distribution [36] to the ID samples:
PDF weib(x) =(
b
a x−c
ab−1exp
− x−c
ab
,ifx > c
0, ifx≤c
(7)
In Eq. 7, ais the scale parameter, bis the shape parameter,
andcis the location parameter. These parameters are fit via
maximum-likelihood estimation, and generally, the location
parameter becomes zero, simplifying to a 2-Parameter Weibull
distribution. In Fig. 3, we show a Weibull distribution fit to
ID data. Data tends to have a strong right-tail, and thus, we
need a more expressive distribution than the Gaussian.
Once the distribution is fit, we can score samples based
on how likely the data is to be ID. We use the CDF of
the fit Weibull distribution to compute the probability that
the distance between a random in-distribution sample and its
nearest class prototype is less than the distance between the
observed sample and its nearest prototype. We select a hard
threshold on the probability τπto make the final prediction of

--- PAGE 6 ---
6
Fig. 3: Estimating the distribution of ID samples using the
Weibull distribution; comparing the densities of ID and OOD
data determined by distance to the nearest class prototype
whether the sample is OOD for each reconfigurator:
ood(x) =CDF weib(min
classdhamming (hagg(x), hclass
proto))> τπ
(8)
C. A Spectral Analysis-Based Approach to Zero-Shot Neural
Architecture Search for Adaptation
Finally, we need to determine how to grow the set of
adaptors/reconfigurators as new domains are encountered by
employing NAS. Traditional NAS can be incredibly expensive,
taking multiple GPU-days to perform. Our goal is for the NAS
to be performed on compute-light hardware; thus, we don’t
have the computational budget to run full NAS. We improve
the efficiency of the NAS in two ways: 1) we freeze the feature
extracting backbone and only search for the architecture of
shallow adaptor layers, significantly reducing the candidate
architecture search space and improving model training speed,
and 2) we use ZS-NAS where we avoid training the candidate
architectures, instead evaluating the candidate model quality
via proxy heuristics. We define our NAS as follows:
•Search Space: Definition of adaptors/reconfigurators
within reasonable parameter bounds specified by a human
•Search Strategy: Candidate selection via a global opti-
mizer; we use Bayesian optimization using Gaussian Pro-
cess Upper Confidence Bounds [37], [38] with sequential
domain reduction [39] as the acquisition function
•Performance Estimation Strategy: Zero-shot proxy
heuristic via spectral analysis of the nearest neighbor
graph of a random batch of input samples
Proxies for ZS-NAS generally aim to maximize one of
the following properties: expressivity, trainability, or gener-
alization [40]. These methods generally require computing or
approximating gradients over one or more random batches of
data. Computing and storing gradients can be computationally-
and memory-expensive. We propose a gradient-free proxy
heuristic. Furthermore, existing ZS-NAS methods do not as-
sume the use of a frozen backbone network and may not work
as expected when used with the EAR framework. Our ap-proach considers the unique properties of the EAR architecture
for the resource-constrained use case:
•Maximize the expressivity of the representations of each
adaptor assuming a fixed pre-trained encoder
•Minimize the redundancy of the representations learned
across all adaptors
•Minimize the number of trainable parameters
The first objective of our ZS-NAS is to maximize the
expressivity of each adaptor. A random batch of data is passed
through each randomly-initialized, untrained adaptor and fea-
tures are extracted from the layer preceding the final projection
to HD space. The Laplacian of the 2-nearest neighbor graph
Lis computed for this batch of samples in “random” feature
space. The encoder already extracts useful representations,
so we don’t want to learn adaptors that degenerate features
to a singular point or scramble the features into a uniform
space. We hypothesize that an expressive adaptor will result
in a rocky feature landscape characterised by small clusters
of data points. This can be achieved by maximizing the
number of connected components in the nearest neighbor
graph. The Laplacian is decomposed into Eigenvalues λand
Eigenvectors ⃗ v. The number of connected components of the
nearest neighbor graph is computed by counting the number
of 0-valued Eigenvalues [41]. We relax the constraint that the
score is directly proportional to the number of connected com-
ponents, and instead, construct a score that uses the number
ofloosely connected components ( γcontrols the strictness of
the connected component count; we set γ= 3):
sada
exp=X
imax(1 −λada
i,0)γ(9)
The second objective of our approach is to minimize the
redundancy of the representations learned across all adaptors.
We reuse the Eigenvalues and Eigenvectors from the preceding
computation and perform spectral clustering [42] for each
adaptor using the approach of Damle et al. [43]. The number
of clusters is set based on the number of Eigenvalues smaller
than 0.1. The redundancy score sredmeasures cluster overlap
using the adjusted mutual information metric [44] between all
pairs of adaptors.
The final objective is to minimize the number of train-
able adaptor/reconfigurator parameters. sparsimply counts the
number of trainable parameters across all of the adaptor layers.
The final proxy heuristic for our proposed ZS-NAS score is
a weighted sum of the three component scores:
s=X
alladaptors
sada
exp+β0∗X
alladaptorssada
par
+
β1∗X
alladaptor pairssadai,adaj
red(10)
In our experiments, β0is set to 3∗10−6andβ1is set to 5.
IV. R ESULTS
We validate our approach on a four of benchmark datasets
compared to state-of-the-art benchmark algorithms for OOD
detection with DNNs and ZS-NAS.

--- PAGE 7 ---
7
A. Experimental Setting
Our experiments are conducted on a machine with two Intel
Xeon Gold 6240R CPUs with 24 cores/48 threads each, an
NVidia A5000 with 24 GB VRAM, and 500 GB RAM. We im-
plement our models in Python using TensorFlow 2 in compiled
mode. Our encoder network is an EfficientNetV2B3 model
[45]. We use the Adam optimizer [46] with a learning rate of
0.001 with no weight decay for learning HD representations,
a learning rate of 0.005 with a weight decay parameter of
10−5for training the baseline cross-entropy-based NN, and a
learning rate of 0.001 and weight decay of 10−7for the deep
nearest neighbor model trained with the supervised contrastive
loss [47]. For training the NNs, we use a batch size of 128 and
train for 40 epochs. For the OOD and ZS-NAS experiments,
we run 15 trials with different random seeds per experimental
setting and different train/test dataset splits.
B. Datasets
We evaluate on variants of four benchmark datasets: PACS
[48], Office 31 [49], Office Home [50], and DomainNet [51].
We focus on the use case where data is limited (generally
<100training samples per class). We evaluate OOD detection
from four perspectives, which requires constructing variants
of the aforementioned datasets: 1) both the input modality
and output class label sets change (Office Home - Disjoint
Domains/Classes), 2) the input modality changes, but the
class labels remain the same (PACS), 3) the input modality
remains the same, but the class labels change (Office 31 -
Split Classes), and 4) the input modality and class labels
remain the same, but environmental factors change (Office
31). For our continual learning experiment, we modify the
DomainNet dataset by selecting twenty classes from three
different modalities, each with 4000 samples, split into six
disjoint tasks. Statistics of the datasets appear in Table I.
Dataset Domain #Classes #Training Samples #Test Samples
PACS
PACS Art/Painting 7 1539 509
PACS Cartoon 7 1760 584
PACS Photo 7 1255 415
PACS Sketch 7 2947 982
Office 31
Office 31 Product Images 31 2123 693
Office 31 Webcam 31 608 187
Office 31 - Split Classes
Office 31 Product Images - Class Set 1 16 1082 352
Office 31 Product Images - Class Set 2 15 1041 341
Office Home - Disjoint Domains/Classes
Office Home Art - Class Set 1 32 908 289
Office Home Clipart - Class Set 1 32 1688 544
Office Home Real - Class Set 1 32 1742 565
Office Home Art - Class Set 2 33 1612 521
Office Home Clipart - Class Set 2 33 933 297
Office Home Real - Class Set 2 33 1550 500
DomainNet
DomainNet Quickdraw - Class Set 1 10 1000 3000
DomainNet Real - Class Set 1 10 1000 3000
DomainNet Sketch - Class Set 1 10 1000 3000
DomainNet Quickdraw - Class Set 2 10 1000 3000
DomainNet Real - Class Set 2 10 1000 3000
DomainNet Sketch - Class Set 2 10 1000 3000
TABLE I: Statistics on the first random split of the datasets
used in our experimentsC. Novelty Detection and Classification using Deep HDC
In the first set of experiments, we take a network with an
EfficientNetV2B3 encoder pretrained on Imagenet, and we
finetune the entire network on the first domain of interest
for image classification and OOD detection. We then apply
the network to mixed samples from the test sets of the ID
domain and unseen OOD domain. To minimize the effect of
the Imagenet pretraining, the first domain always consists of
a modality closely aligned to natural images. We consider
five training styles: 1) training the encoder plus a 2-layer per-
ceptron using standard cross-entropy, 2) training the encoder
plus an embedding layer for deep nearest neighbors using
the supervised contrastive loss [22], 3) training the encoder
plus a single final layer HD adaptor/reconfigurator, 4) training
the encoder plus 2-layer perceptron HD adaptors for seven
tap points (blocks 1, 4, 7, 12, 19, 31, and “head” from the
EfficientNetV2B3 model) and a final reconfigurator, and 5)
training the encoder, HD adaptors whose tap points (subset of
the previous tap points) and architecture are determined using
ZS-NAS, and reconfigurator.
To compare with networks not designed explicitly for OOD
detection, we train post-hoc OOD detection algorithms on
one of i) the features immediately preceding the classification
layer, ii) the output softmax probabilities, or iii) using the
feature outputs of all 32 layers in the encoder network. For
setting (i), we consider off-the-shelf classic OOD and novelty
detection algorithms: one-class SVM [14], empirical and ro-
bust covariance estimates [15], local outlier factors [16], and
isolation forests [17], [18] as implemented in scikit-learn. For
setting (ii), we employ the deep baseline for OOD detection
proposed by Hendrycks and Gimpel [19]. For setting (iii), we
consider two state-of-the-art methods: using layer-wise gram
matrices [20] and hyperdimensional feature fusion [21]. We
also compare with deep nearest neighbors, which is designed
for joint OOD detection and classification.
We consider the following metrics for evaluation:
•Classification accuracy of the ID test samples to measure
the discriminability
•When the ID and OOD samples share class labels,
we also examinw the classification accuracy of the
OOD test samples to measure inherent knowledge trans-
fer/robustness to noise from distribution shifts
•To measure performance on OOD detection, we compute:
–Area under the receiver-operator curve (AUROC)
when ID samples are consider the positive class
–Macro-F1-measure of predicting ID vs OOD
–True Negative Rate at True Positive Rate 95%
(TNR@TPR95) and 90% (TNR@TPR90) where the
ID data is treated as the positive class
•Number of parameters, adaptors, and embedding dimen-
sions to get a sense of the model efficiency and capacity
In the cases where a binary decision is necessary, we evaluate
against optimal thresholds on the scoring functions. Note
that these data sets are relatively challenging for the OOD
detection problem; thus, some metrics, even using state-of-
the-art algorithms for OOD are low compared to running these
algorithms on other OOD detection benchmark datasets. We

--- PAGE 8 ---
8
also apply the Kruskal-Wallis test (non-parametric ANOV A)
comparing the performance of the different OOD algorithms
within a specific experimental setting followed by the posthoc
Dunn test with Bonferroni correction to identify which OOD
algorithms differ from the ZS-NAS deep HD-based model
in a statistically significant way (p-value of 0.05). In the
experimental results tables ⊙represents that no statistically
significant findings can be determined, ⊕represents the ZS-
NAS HD model is significantly better than the comparator,
and⊖represents the ZS-NAS HD model is significantly worse
than the comparator.
In Tables II, IV, VI, and VIII, we look at how the classi-
fication performance of the proposed approach is effected by
various types of domain shift. We notice the following trends:
•The learned HD-based approaches outperform the stan-
dard and deep nearest neighbor-based approaches w.r.t.
ID accuracy by ˜2-7% across all datasets
•Na¨ıvely adding multi-layer perceptron adaptors to all of
the candidate tap points can result in slightly improved
classification performance, but comes at the cost of a
significant increase in the number of model parameters.
•The ZS-NAS-determined adaptors generally use signifi-
cantly smaller architectures while only exhibiting a minor
decrease in performance compared to the “all layers”
setting and outperform the non-HD-based models.
•Just learning a single final layer adaptor is a strong
baseline, matching performance with the ZS-NAS model
in terms of classification accuracy in most cases.
•Interestingly, when applying the model trained on one
domain to a different domain with the same class set,
the learned HD-based models significantly outperform
the non-HD-based models in terms of accuracy, suggest-
ing that the HD representation is more robust to noise
caused by distribution shift. This is especially apparent
for perceptually-similar domains, e.g., transferring from
natural images to photorealistic paintings. The “all layers”
model generalizes worse than the ZS-NAS model.
In Tables III, V, VII, and IX, we look at how OOD detection
performance is effected by various types of domain shift. We
notice the following trends:
•Regardless of setting, the learned HD-based models
match or exceed the performance of the other models.
•The “all layers” HD model occasionally underperforms
on the OOD sample detection task compared to the “final
layer” and ZS-NAS HD models (e.g., in the PACS photo-
to-sketch transfer task). This is especially apparent when
looking at the TNR@TPR metrics.
D. Comparison of Different NAS Methods
In the second set of experiments, we compare the proposed
ZS-NAS method (“spectral”) to existing NAS methods. We
finetune the encoder and a learn a set of adaptors on the
first domain, and then freeze the encoder and learn a set of
adaptors on the second domain. We compare against adding
a final layer adaptor and finetuning the entire network on the
second task, adding a final layer adaptor without finetuning
the encoder, randomly selecting the location and architecturesMethod Accuracy ID #Parameters #Adaptors Embed Dim
office home disjoint - real disjoint : art disjoint
standard training 0.68±0.09⊙ 14559270 .0 1.0 128.0
scl 0.64±0.07⊕ 14559270 .0 1.0 128.0
learned hdfinal layer 0.73±0.05⊙ 14459269 .0 1.0 63.0
learned hdalllayers 0.74±0.05⊙ 124864951 .0 7.0 255.0
learned hdzsnas 0.73±0.06⊙ 13414837 .8 6.07 255.0
office home disjoint - real disjoint : clipart disjoint
standard training 0.68±0.09⊙ 14559270 .0 1.0 128.0
scl 0.64±0.07⊕ 14559270 .0 1.0 128.0
learned hdfinal layer 0.73±0.05⊙ 14459269 .0 1.0 63.0
learned hdalllayers 0.74±0.05⊙ 124864951 .0 7.0 255.0
learned hdzsnas 0.73±0.06⊙ 13414837 .8 6.07 255.0
TABLE II: Comparing classification performance on the
Office Home - Disjoint Domains/Classes datasets where input
modality and output class label sets change across domains
Method AUROC macro-F1 TNR@TPR95 TNR@TPR90
office home disjoint - real disjoint : art disjoint
oc-svm 0.78±0.04⊙ 0.45±0.07⊕ 0.19±0.06⊙ 0.35±0.09⊙
empirical cov 0.76±0.07⊙ 0.61±0.07⊕ 0.19±0.08⊙ 0.33±0.1⊙
robust cov 0.73±0.06⊙ 0.61±0.08⊕ 0.15±0.07⊙ 0.28±0.1⊙
lof 0.73±0.06⊙ 0.58±0.06⊕ 0.12±0.05⊕ 0.26±0.09⊙
isof 0.76±0.07⊙ 0.64±0.04⊕ 0.2±0.08⊙ 0.34±0.1⊙
deep baseline 0.73±0.06⊙ 0.67±0.04⊙ 0.18±0.05⊙ 0.3±0.08⊙
gram 0.7±0.06⊕ 0.65±0.05⊙ 0.12±0.04⊕ 0.25±0.08⊙
hdfusion 0.67±0.07⊕ 0.62±0.05⊕ 0.09±0.04⊕ 0.19±0.07⊕
scl 0.69±0.06⊕ 0.63±0.05⊕ 0.1±0.05⊕ 0.2±0.06⊕
learned hdfinal layer 0.79±0.04⊙ 0.72±0.03⊙ 0.18±0.06⊙ 0.35±0.08⊙
learned hdalllayers 0.81±0.03⊙ 0.73±0.03⊙ 0.21±0.05⊙ 0.37±0.06⊙
learned hdzsnas 0.8±0.03⊙ 0.72±0.03⊙ 0.22±0.07⊙ 0.38±0.09⊙
office home disjoint - real disjoint : clipart disjoint
oc-svm 0.75±0.04⊙ 0.51±0.06⊕ 0.17±0.04⊙ 0.31±0.06⊙
empirical cov 0.73±0.06⊙ 0.61±0.09⊕ 0.14±0.06⊙ 0.27±0.09⊙
robust cov 0.72±0.06⊙ 0.57±0.13⊕ 0.12±0.08⊙ 0.24±0.1⊙
lof 0.71±0.04⊙ 0.6±0.07⊕ 0.11±0.05⊕ 0.23±0.07⊙
isof 0.73±0.06⊙ 0.65±0.05⊙ 0.18±0.06⊙ 0.31±0.09⊙
deep baseline 0.72±0.05⊙ 0.68±0.04⊙ 0.16±0.04⊙ 0.29±0.06⊙
gram 0.77±0.04⊙ 0.71±0.03⊙ 0.19±0.06⊙ 0.34±0.08⊙
hdfusion 0.83±0.05⊙ 0.76±0.04⊙ 0.33±0.12⊙ 0.49±0.14⊙
scl 0.65±0.07⊕ 0.61±0.05⊕ 0.12±0.04⊕ 0.19±0.06⊕
learned hdfinal layer 0.77±0.03⊙ 0.72±0.02⊙ 0.17±0.04⊙ 0.32±0.05⊙
learned hdalllayers 0.77±0.04⊙ 0.71±0.04⊙ 0.17±0.05⊙ 0.31±0.06⊙
learned hdzsnas 0.77±0.04⊙ 0.72±0.04⊙ 0.2±0.06⊙ 0.34±0.09⊙
TABLE III: Comparing OOD detection performance on the
Office Home - Disjoint Domains/Classes datasets where input
modality and output class label sets change across domains
Method Accuracy ID Accuracy OOD #Parameters #Adaptors Embed Dim
pacs - photo : cartoon
standard training 0.89±0.02⊕ 0.19±0.05⊕ 14556045 .0 1.0 128.0
scl 0.89±0.02⊕ 0.18±0.04⊕ 14556045 .0 1.0 128.0
learned hdfinal layer 0.96±0.01⊙ 0.26±0.05⊙ 14459269 .0 1.0 63.0
learned hdalllayers 0.96±0.01⊙ 0.15±0.02⊕ 41575351 .0 7.0 63.0
learned hdzsnas 0.95±0.02⊙ 0.26±0.06⊙ 13400336 .73 6.07 63.0
pacs - photo : painting
standard training 0.89±0.02⊕ 0.41±0.04⊕ 14556045 .0 1.0 128.0
scl 0.89±0.02⊕ 0.4±0.03⊕ 14556045 .0 1.0 128.0
learned hdfinal layer 0.96±0.01⊙ 0.55±0.05⊙ 14459269 .0 1.0 63.0
learned hdalllayers 0.96±0.01⊙ 0.5±0.03⊙ 41575351 .0 7.0 63.0
learned hdzsnas 0.95±0.02⊙ 0.52±0.07⊙ 13400336 .73 6.07 63.0
pacs - photo : sketch
standard training 0.89±0.02⊕ 0.22±0.08⊙ 14556045 .0 1.0 128.0
scl 0.89±0.02⊕ 0.21±0.06⊙ 14556045 .0 1.0 128.0
learned hdfinal layer 0.96±0.01⊙ 0.27±0.08⊙ 14459269 .0 1.0 63.0
learned hdalllayers 0.96±0.01⊙ 0.17±0.01⊕ 41575351 .0 7.0 63.0
learned hdzsnas 0.95±0.02⊙ 0.29±0.09⊙ 13400336 .73 6.07 63.0
TABLE IV: Comparing classification performance on the
PACS datasets where input modalities change, but the output
class label remains the same across domains
of the adaptor set, using few-shot NAS (with a single training
epoch per candidate evaluation), and comparing to other zero-
shot proxy heuristics: computing the ℓ2-norm of the gradients
(grad norm) of the candidate architectures, synflow (designed
to maximize the sparsity of the candidate architectures) [32],
and the ϕ-score [33] and Jacobian-covariance (jacob cov)
scores [4], which try to maximize network expressivity. In
all cases, we add spar(scaled by trial-and-error) to the base

--- PAGE 9 ---
9
Method AUROC macro-F1 TNR@TPR95 TNR@TPR90
pacs - photo : cartoon
oc-svm 0.67±0.07⊙ 0.58±0.08⊙ 0.11±0.06⊙ 0.2±0.11⊙
empirical cov 0.64±0.09⊙ 0.43±0.09⊕ 0.04±0.03⊕ 0.12±0.08⊕
robust cov 0.66±0.11⊙ 0.54±0.16⊙ 0.1±0.11⊕ 0.17±0.17⊕
lof 0.69±0.07⊙ 0.55±0.08⊕ 0.15±0.06⊙ 0.25±0.08⊙
isof 0.68±0.09⊙ 0.58±0.15⊙ 0.07±0.04⊕ 0.16±0.09⊕
deep baseline 0.57±0.16⊕ 0.6±0.12⊙ 0.07±0.04⊕ 0.14±0.07⊕
gram 0.6±0.13⊙ 0.59±0.1⊙ 0.13±0.1⊙ 0.23±0.14⊙
hdfusion 0.82±0.04⊙ 0.78±0.04⊙ 0.23±0.15⊙ 0.39±0.18⊙
scl 0.69±0.08⊙ 0.7±0.06⊙ 0.06±0.03⊕ 0.15±0.06⊕
learned hdfinal layer 0.8±0.07⊙ 0.76±0.07⊙ 0.29±0.14⊙ 0.48±0.17⊙
learned hdalllayers 0.73±0.04⊙ 0.72±0.02⊙ 0.08±0.05⊕ 0.17±0.09⊕
learned hdzsnas 0.74±0.11⊙ 0.71±0.09⊙ 0.25±0.12⊙ 0.4±0.16⊙
pacs - photo : painting
oc-svm 0.75±0.04⊕ 0.72±0.03⊕ 0.22±0.06⊕ 0.39±0.07⊕
empirical cov 0.79±0.03⊙ 0.63±0.09⊕ 0.18±0.08⊕ 0.33±0.09⊕
robust cov 0.79±0.03⊙ 0.7±0.09⊕ 0.19±0.08⊕ 0.33±0.09⊕
lof 0.77±0.04⊕ 0.64±0.08⊕ 0.2±0.05⊕ 0.34±0.05⊕
isof 0.78±0.04⊙ 0.69±0.05⊕ 0.2±0.07⊕ 0.38±0.08⊕
deep baseline 0.7±0.06⊕ 0.69±0.04⊕ 0.18±0.06⊕ 0.33±0.07⊕
gram 0.66±0.07⊕ 0.63±0.07⊕ 0.16±0.05⊕ 0.28±0.08⊕
hdfusion 0.77±0.03⊕ 0.72±0.03⊕ 0.14±0.06⊕ 0.29±0.09⊕
scl 0.83±0.03⊙ 0.77±0.03⊙ 0.24±0.07⊙ 0.42±0.08⊙
learned hdfinal layer 0.83±0.02⊙ 0.78±0.02⊙ 0.38±0.06⊙ 0.57±0.06⊙
learned hdalllayers 0.87±0.01⊙ 0.8±0.02⊙ 0.34±0.05⊙ 0.55±0.06⊙
learned hdzsnas 0.84±0.02⊙ 0.79±0.02⊙ 0.38±0.08⊙ 0.58±0.07⊙
pacs - photo : sketch
oc-svm 0.63±0.17⊙ 0.51±0.19⊕ 0.14±0.18⊙ 0.23±0.24⊙
empirical cov 0.66±0.17⊙ 0.44±0.18⊕ 0.1±0.14⊕ 0.22±0.23⊙
robust cov 0.72±0.16⊙ 0.62±0.17⊙ 0.17±0.19⊙ 0.27±0.23⊙
lof 0.65±0.19⊙ 0.48±0.18⊕ 0.15±0.15⊙ 0.26±0.23⊙
isof 0.75±0.12⊙ 0.64±0.18⊙ 0.17±0.21⊙ 0.28±0.28⊙
deep baseline 0.54±0.21⊕ 0.61±0.14⊙ 0.06±0.06⊕ 0.11±0.11⊕
gram 0.82±0.12⊙ 0.8±0.09⊙ 0.28±0.25⊙ 0.46±0.3⊙
hdfusion 0.94±0.05⊙ 0.92±0.04⊙ 0.66±0.32⊙ 0.76±0.29⊙
scl 0.81±0.1⊙ 0.79±0.08⊙ 0.27±0.22⊙ 0.4±0.27⊙
learned hdfinal layer 0.84±0.11⊙ 0.79±0.1⊙ 0.42±0.26⊙ 0.59±0.27⊙
learned hdalllayers 0.69±0.08⊙ 0.74±0.03⊙ 0.05±0.07⊕ 0.11±0.13⊕
learned hdzsnas 0.81±0.21⊙ 0.78±0.12⊙ 0.4±0.26⊙ 0.55±0.28⊙
TABLE V: Comparing the OOD detection performance on
the PACS datasets where input modalities change, but the
output class label remains the same across domains
Method Accuracy ID #Parameters #Adaptors Embed Dim
office 31split classes - amazon split 1 : amazon split 2
standard training 0.79±0.05⊕ 14557206 .0 1.0 128.0
scl 0.8±0.04⊙ 14557206 .0 1.0 128.0
learned hdfinal layer 0.84±0.03⊙ 14459269 .0 1.0 63.0
learned hdalllayers 0.85±0.03⊙ 69281207 .0 7.0 127.0
learned hdzsnas 0.84±0.04⊙ 13578750 .27 6.13 127.0
TABLE VI: Comparing classification performance on the
Office 31 - Split Classes datasets where input modalities
remain the same, but output class labels change
Method AUROC macro-F1 TNR@TPR95 TNR@TPR90
office 31split classes - amazon split 1 : amazon split 2
oc-svm 0.81±0.03⊙ 0.69±0.03⊕ 0.21±0.06⊙ 0.38±0.07⊙
empirical cov 0.79±0.04⊙ 0.67±0.06⊕ 0.18±0.05⊙ 0.33±0.08⊙
robust cov 0.79±0.04⊙ 0.7±0.04⊕ 0.19±0.06⊙ 0.33±0.08⊙
lof 0.79±0.04⊙ 0.71±0.06⊙ 0.18±0.07⊙ 0.32±0.08⊙
isof 0.77±0.05⊙ 0.7±0.03⊕ 0.18±0.05⊙ 0.33±0.08⊙
deep baseline 0.75±0.05⊕ 0.72±0.04⊙ 0.2±0.05⊙ 0.35±0.09⊙
gram 0.73±0.04⊕ 0.69±0.03⊕ 0.16±0.04⊙ 0.29±0.06⊙
hdfusion 0.74±0.05⊕ 0.71±0.04⊕ 0.09±0.05⊕ 0.19±0.08⊕
scl 0.78±0.04⊙ 0.74±0.03⊙ 0.13±0.07⊙ 0.24±0.1⊕
learned hdfinal layer 0.83±0.03⊙ 0.78±0.03⊙ 0.23±0.06⊙ 0.43±0.09⊙
learned hdalllayers 0.85±0.02⊙ 0.8±0.02⊙ 0.25±0.06⊙ 0.43±0.07⊙
learned hdzsnas 0.83±0.02⊙ 0.77±0.02⊙ 0.2±0.05⊙ 0.4±0.06⊙
TABLE VII: Comparing the OOD detection performance on
the Office 31 - Split Classes datasets where input modalities
remain the same, but output class labels change
heuristic to constrain the memory footprint of the adaptor set.
We evaluate the approaches based on the accuracy of the
adaptor set on the new domain test data and in terms of
computational and memory efficiency as determined by the
number of trainable parameters, time to perform the NAS,
number of adaptors, and size of embedding. Again we useMethod Accuracy ID Accuracy OOD #Parameters #Adaptors Embed Dim
office 31 - amazon : webcam
standard training 0.79±0.03⊙ 0.35±0.05⊕ 14559141 .0 1.0 128.0
scl 0.79±0.02⊙ 0.12±0.04⊕ 14559141 .0 1.0 128.0
learned hdfinal layer 0.8±0.02⊙ 0.41±0.05⊙ 14459269 .0 1.0 63.0
learned hdalllayers 0.81±0.02⊙ 0.37±0.04⊙ 124864951 .0 7.0 255.0
learned hdzsnas 0.8±0.02⊙ 0.43±0.04⊙ 13612005 .33 6.13 255.0
TABLE VIII: Comparing classification performance on the
Office 31 datasets where input modalities and class labels
remain the same across domains, but the environmental
factors (resolution of the imagery) change between domains
Method AUROC macro-F1 TNR@TPR95 TNR@TPR90
office 31 - amazon : webcam
oc-svm 0.82±0.02⊙ 0.53±0.02⊕ 0.23±0.05⊙ 0.41±0.04⊙
empirical cov 0.81±0.03⊙ 0.66±0.02⊙ 0.22±0.06⊙ 0.39±0.1⊙
robust cov 0.78±0.04⊙ 0.58±0.08⊕ 0.16±0.1⊙ 0.32±0.14⊙
lof 0.81±0.03⊙ 0.62±0.03⊕ 0.17±0.04⊙ 0.34±0.06⊙
isof 0.8±0.04⊙ 0.61±0.03⊕ 0.26±0.07⊙ 0.44±0.08⊖
deep baseline 0.77±0.05⊙ 0.68±0.04⊙ 0.19±0.06⊙ 0.34±0.08⊙
gram 0.74±0.03⊙ 0.65±0.03⊙ 0.17±0.04⊙ 0.31±0.05⊙
hdfusion 0.73±0.04⊙ 0.62±0.03⊙ 0.1±0.03⊙ 0.2±0.04⊙
scl 0.85±0.01⊖ 0.73±0.02⊙ 0.25±0.05⊙ 0.43±0.05⊖
learned hdfinal layer 0.78±0.03⊙ 0.68±0.02⊙ 0.18±0.05⊙ 0.33±0.05⊙
learned hdalllayers 0.85±0.01⊖ 0.72±0.02⊙ 0.24±0.04⊙ 0.44±0.05⊖
learned hdzsnas 0.79±0.02⊙ 0.69±0.02⊙ 0.19±0.03⊙ 0.33±0.05⊙
TABLE IX: Comparing the OOD detection performance on
the Office 31 datasets where the input modalities and class
labels remain the same across domains, but the environmental
factors (resolution of the imagery) change between domains
the Kruskal-Wallis ANOV A test with posthoc Dunn test with
Bonferroni correction (p-value of 0.05). Results appear in
Table X. We observe:
•The spectral method achieves accuracy equal or higher
to the next best ZS-NAS approach in all settings.
•In most cases, the spectral method is as good or better
than finetuning the entire network in terms of accuracy.
•Once again, learning a final layer adaptor is a surprisingly
strong baseline.
•The spectral method is 2-7x faster for heuristic evaluation
compared to other ZS-NAS methods (note that these
methods were re-implemented by the authors and may
not necessarily be fully optimized).
•The size of the adaptor sets identified by the proposed
method are generally within the same order of magnitude
as the other approaches, but the spectral method seems
more flexible to growing and shrinking as needed. For ex-
ample, on the Office 31 dataset, the spectral method finds
a model roughly 2-3 times the size of the other methods
but achieves a 13% increase in accuracy compared to the
next-best ZS-NAS method.
E. OOD Detection and Dynamic Data Routing with Multiple
Sets of Adaptors/Reconfigurators
In this section, we look at how the model performs when
there are two sets of adaptors/reconfigurators for two distinct
domains, and the data must appropriately routed to the correct
adaptor set. We consider five metrics: 1) accuracy of applying
the adaptors from the second domain to test data from the
second domain to verify that the model learns strong models
on the new domain despite using a frozen encoder model, 2)
applying the adaptors from the second domain to OOD test
data from the first domain to see if there is some inherent

--- PAGE 10 ---
10
Metric Accuracy New #Trainable Parameters Search Time (s) #Adaptors Embed Dim
office 31 - amazon : webcam
finetune alllayers 0.9±0.08⊙ 14459269 .0±0.0⊙ 0.0±0.0⊙ 1.0 63.0
finetune final layer 0.94±0.02⊙ 100863 .0±0.0⊖ 0.0±0.0⊙ 1.0 63.0
random 0.81±0.36⊙ 3878524 .67±4317844 .29⊙ 0.0±0.0⊙ 5.87 246.47
few shot 0.84±0.02⊕ 389977 .6±26599 .17⊙ 3375.02±420.16⊕ 6.67 255.0
grad norm 0.8±0.03⊕ 370988 .87±27955 .23⊖ 1000.45±14.21⊙ 6.33 255.0
synflow 0.82±0.03⊕ 377331 .13±28226 .82⊙ 1920.09±11.64⊕ 6.47 255.0
ϕ-score 0.83±0.03⊕ 369056 .0±28895 .17⊖ 1898.06±23.3⊕ 6.4 255.0
jacob cov 0.79±0.02⊕ 348685 .33±21601 .64⊖ 893.43±39.43⊙ 5.87 255.0
spectral 0.97±0.01⊙ 889030 .87±158014 .84⊙ 359.72±15.61⊙ 5.93 255.0
office 31split classes - amazon split 1 : amazon split 2
finetune alllayers 0.83±0.05⊕ 14459269 .0±0.0⊙ 0.0±0.0⊙ 1.0 63.0
finetune final layer 0.89±0.02⊙ 100863 .0±0.0⊖ 0.0±0.0⊙ 1.0 63.0
random 0.79±0.3⊙ 3326234 .93±2734746 .09⊙ 0.0±0.0⊙ 6.0 127.0
few shot 0.89±0.02⊙ 358857 .13±26543 .18⊙ 3245.72±455.65⊕ 6.6 127.0
grad norm 0.89±0.02⊙ 340185 .73±24446 .33⊙ 995.79±47.31⊙ 6.27 127.0
synflow 0.89±0.02⊙ 352025 .47±29209 .71⊙ 1866.85±18.85⊕ 6.53 127.0
ϕ-score 0.88±0.02⊙ 345580 .73±27199 .91⊙ 1850.41±22.53⊕ 6.47 127.0
jacob cov 0.9±0.02⊙ 316406 .8±32323 .29⊖ 773.48±43.44⊙ 5.73 127.0
spectral 0.9±0.02⊙ 758325 .73±203864 .54⊙ 297.61±8.54⊙ 6.0 127.0
office home disjoint - real disjoint : clipart disjoint
finetune alllayers 0.77±0.04⊙ 14459269 .0±0.0⊕ 0.0±0.0⊙ 1.0 63.0
finetune final layer 0.78±0.03⊙ 100863 .0±0.0⊖ 0.0±0.0⊙ 1.0 63.0
random 0.49±0.4⊙ 5836242 .73±5743305 .71⊙ 0.0±0.0⊙ 5.67 255.0
few shot 0.73±0.05⊙ 386918 .93±25202 .76⊙ 4528.15±422.91⊕ 6.67 255.0
grad norm 0.67±0.04⊙ 351930 .0±0.0⊙ 998.3±10.84⊙ 6.0 255.0
synflow 0.7±0.05⊙ 365740 .93±25850 .9⊙ 1925.56±17.63⊕ 6.27 255.0
ϕ-score 0.7±0.19⊙ 485802 .47±382769 .61⊙ 1881.11±52.96⊕ 6.6 255.0
jacob cov 0.69±0.05⊙ 346080 .53±29502 .77⊙ 828.48±45.21⊙ 5.87 255.0
spectral 0.73±0.06⊙ 423169 .93±126290 .1⊙ 349.68±11.81⊙ 6.33 255.0
pacs - photo : sketch
finetune alllayers 0.89±0.04⊙ 14459269 .0±0.0⊕ 0.0±0.0⊙ 1.0 63.0
finetune final layer 0.85±0.01⊙ 100863 .0±0.0⊖ 0.0±0.0⊙ 1.0 63.0
random 0.63±0.33⊙ 3757066 .73±3440392 .9⊙ 0.0±0.0⊙ 5.67 60.87
few shot 0.82±0.02⊙ 324094 .33±40026 .81⊙ 4229.97±512.58⊕ 6.2 63.0
grad norm 0.83±0.01⊙ 326880 .13±22344 .09⊙ 951.56±35.8⊙ 6.27 63.0
synflow 0.83±0.02⊙ 336076 .73±27354 .67⊙ 1817.42±12.48⊕ 6.47 63.0
ϕ-score 0.81±0.01⊕ 338345 .33±37314 .0⊙ 1707.05±37.57⊙ 6.4 63.0
jacob cov 0.84±0.01⊙ 347068 .47±30656 .88⊙ 673.58±19.3⊙ 6.2 63.0
spectral 0.84±0.01⊙ 357978 .93±31536 .53⊙ 288.78±13.05⊙ 6.53 63.0
TABLE X: Comparisons of different neural architecture search approaches for identifying the location and the structure of
the adaptors for transfer to a new domain.
robustness to noise from distribution shift in the model, 3)
the macro-F1-measure, which measures how well the data is
routed to the appropriate adaptor set, 4) “combined accuracy
routing score”, which is the accuracy of jointly routing to the
correct set of adaptors and then making correct predictions,
and the 5) “combined accuracy shared classes score”, which
is the accuracy of predicting the correct class even when routed
to the wrong adaptor (if the domains share a class set). Results
are shown in Table XI. We observe:
•Despite using a frozen encoder and a set of shallow
adaptors, the model performs well on new domain data.
•Surprisingly, the adaptors of the new domain handle OOD
samples from the original domain extremely well ( >50%
accuracy) when classes are shared despite not being
trained on them. We expect this is because the adaptor
was originally trained on the first domain, HDC is known
to be robust to noise, and the learned HD classifiers can
be thought of as a heavily regularized learning scheme
(ensemble of many weak classifiers).
•The routing mechanism seems generally effective.
•The imperfections in routing do cause a noticeable drop
in accuracy, but accuracy is still at acceptable levels
considering the difficulty of the problem.
•Because of the strong robustness to noise due to distri-bution shift, even when the data samples are incorrectly
routed, when classes are shared, the model can often still
succeed in assigning the correct classes.
F . Class-Incremental Continual Learning
In the final set of experiments, we create a curriculum
involving six tasks, each of which consists of classifying data
from the DomainNet dataset. The model is fed one sample at
a time, determines which set of adaptors to send the data to,
and makes predictions regarding the class label and likelihood
of the sample being OOD. The selected adaptor and OOD
scores are stored over a window of 50 time steps. When at
least 60% of the last 50 samples have an OOD score of 0.7
or greater, the model triggers a learning phase wherein 1000
samples are collected and labeled by an oracle. The model
runs the ZS-NAS to identify the locations and structures of the
adaptors, and the adaptor set is trained on the collected data.
Every 2000 samples, the task changes (model has no prior
knowledge of when tasks change). Each task appears twice,
and the ordering of the curriculum is random. We measure the
moving average over the last 50 samples as the performance
metric. We compare three problem formulations: 1) an upper
bound on performance where the model is given knowledge of

--- PAGE 11 ---
11
Setting Accuracy New-to-New Accuracy New-to-Old Routing macro-F1 Combined Accuracy Routing Combined Accuracy Shared Classes
office 31: amazon : webcam 0.97±0.01 0.65±0.02 0.79±0.02 0.71±0.02 0.83±0.02
office 31split classes: amazon split 1 : amazon split 2 0.9±0.02 N/A 0.85±0.02 0.78±0.02 N/A
office home disjoint: real disjoint : clipart disjoint 0.73±0.06 N/A 0.72±0.04 0.59±0.03 N/A
pacs: photo : sketch 0.84±0.01 0.53±0.02 0.75±0.17 0.7±0.15 0.8±0.12
TABLE XI: Understanding the robustness of the learned adaptors and ability to dynamically routing the data through the
appropriate adaptor sets in the 2-domain problem
the current task, 2) a “slow” dynamic routing algorithm where
the model determines which set of adaptors to send the current
sample to based on a majority vote using ID scores from
the last 50 samples, and 3) an instantaneous dynamic routing
algorithm where each data sample is treated independently,
and the best adaptor is selected greedily per sample.
The evaluation curves and overall accuracy of the different
routing mechanisms appear in Fig. 4 where we observe:
•The model exhibits the successful continual learning.
•The slow routing algorithm performs very similarly to the
optimal routing algorithm, but always has a time-delay
penalty after the task change due to needing to gather
enough evidence to switch its current adaptor set.
•The instantaneous routing algorithm is able to imme-
diately correct for the task change, but it makes more
mistakes about the task identity, resulting in lower overall
performance compared to the “slow” routing algorithm.
•The model correctly learns adaptors for the six tasks (with
some early misfires). Updates are generally triggered soon
after task changes.
•Once the six adaptors are learned, OOD scores remain
low and the update phases no longer trigger.
V. C ONCLUSION AND FUTURE WORK
We presented a novel framework for efficient and rapid
model adaptation on resource-constrained devices using the
EAR architecture. We presented novel technical contributions
related to progressive NNs for continual learning, deep HDC-
based OOD detection, and spectral analysis-based ZS-NAS.
We found that the learned HD-based approaches outperformed
the standard cross-entropy-trained and deep nearest neighbor-
based approaches with respect to ID accuracy by ˜2-7% across
all datasets, demonstrating the discriminative power of the
deep HD classifier. Similarly, on the OOD detection task, the
learned HD model generally matched or exceeded the perfor-
mance of all baseline methods. Our spectral method for ZS-
NAS was shown to be 2-7x faster for heuristic evaluation than
other ZS-NAS methods while discovering in low-parameter
architectures that achieved equal or better performance on
the downstream classification and OOD tasks. Interestingly,
in the domain-incremental setting (different modalities, same
classes), the EAR architecture is shown to be extremely robust
to shifts in modality, achieving high classification accuracy
even when routed to the incorrect set of adaptors. Finally, in
the class-incremental continual learning setting, the proposed
approach effectively identifies task changes and the “slow”
dynamic routing mechanism achieves performance close to
the optimal routing mechanism (78.3% overall accuracy vs
80.6%).However, there are still some components of our approach
that prevent the system from being fully autonomous. Princi-
pally, the current approach requires an oracle i) to verify that
the domain has truly changed and ii) to subsequently label
data for the new domain. The addition of a pseudo-labeling
mechanism would reduce the need for an oracle. Our proposed
approach also assumes the device running the model has no
bound on memory; i.e., our model continually grows new
sets of adaptors. We need a mechanism for not only growing
adaptors, but updating and pruning existing adaptors when the
data distribution shifts to handle devices with limited memory.
REFERENCES
[1] M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis,
G. Slabaugh, and T. Tuytelaars, “A continual learning survey: Defying
forgetting in classification tasks,” IEEE TPAMI , 2021.
[2] G. M. van de Ven, T. Tuytelaars, and A. S. Tolias, “Three types of
incremental learning,” Nature Machine Intelligence , 2022.
[3] P. Kanerva, “Hyperdimensional computing: An introduction to comput-
ing in distributed representation with high-dimensional random vectors,”
Cognitive computation , 2009.
[4] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, “Neural architecture
search without training,” in ICML . PMLR, 2021.
[5] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
K. Kavukcuoglu, R. Pascanu, and R. Hadsell, “Progressive neural
networks,” arXiv:1606.04671 , 2016.
[6] H. M. Fayek, L. Cavedon, and H. R. Wu, “Progressive learning: A deep
learning framework for continual learning,” Neural Networks , vol. 128,
2020.
[7] Z. Chen and B. Liu, “Lifelong machine learning,” Synthesis Lectures on
AI and ML , 2018.
[8] J. Yoon, E. Yang, J. Lee, and S. J. Hwang, “Lifelong learning with
dynamically expandable networks,” arXiv:1708.01547 , 2017.
[9] C.-Y . Hung, C.-H. Tu, C.-E. Wu, C.-H. Chen, Y .-M. Chan, and C.-
S. Chen, “Compacting, picking and growing for unforgetting continual
learning,” NeurIPS , 2019.
[10] X. Li, Y . Zhou, T. Wu, R. Socher, and C. Xiong, “Learn to grow:
A continual structure learning framework for overcoming catastrophic
forgetting,” in ICML . PMLR, 2019.
[11] G. Yang, C. S. Y . Wong, and R. Savitha, “Robust continual learn-
ing through a comprehensively progressive bayesian neural network,”
arXiv:2202.13369 , 2022.
[12] J. Yang, K. Zhou, Y . Li, and Z. Liu, “Generalized out-of-distribution
detection: A survey,” arXiv:2110.11334 , 2021.
[13] M. Markou and S. Singh, “Novelty detection: a review—part 1: statis-
tical approaches,” Signal processing , 2003.
[14] L. M. Manevitz and M. Yousef, “One-class svms for document classi-
fication,” JMLR , 2001.
[15] D. Pe ˜na and F. J. Prieto, “Multivariate outlier detection and robust
covariance matrix estimation,” Technometrics , 2001.
[16] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander, “Lof: identifying
density-based local outliers,” in ACM SIGMOD International Confer-
ence on Management of Data , 2000.
[17] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation forest,” in ICDM .
IEEE, 2008.
[18] ——, “Isolation-based anomaly detection,” ACM TKDD , 2012.
[19] D. Hendrycks and K. Gimpel, “A baseline for detecting misclassified
and out-of-distribution examples in neural networks,” arXiv:1610.02136 ,
2016.
[20] C. S. Sastry and S. Oore, “Detecting out-of-distribution examples with
gram matrices,” in ICML . PMLR, 2020.
[21] S. Wilson, T. Fischer, N. S ¨underhauf, and F. Dayoub, “Hyperdimensional
feature fusion for out-of-distribution detection,” in WACV , 2023.

--- PAGE 12 ---
12
Fig. 4: Showing continual learning on a curriculum of six tasks repeated twice. Purple lines denote task changes. Yellow bars
denote data collection and training periods. Orange lines denote misfires of detecting shifts in the distribution. All results are
moving averages over the last 50 samples, reset after each training period.
[22] Y . Sun, Y . Ming, X. Zhu, and Y . Li, “Out-of-distribution detection with
deep nearest neighbors,” in ICML . PMLR, 2022.
[23] A. Thomas, S. Dasgupta, and T. Rosing, “A theoretical perspective on
hyperdimensional computing,” JAIR , 2021.
[24] D. Kleyko, D. A. Rachkovskij, E. Osipov, and A. Rahimi, “A survey on
hyperdimensional computing aka vector symbolic architectures, part i:
Models and data transformations,” ACM Computing Surveys .
[25] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y . Zhu, H. Zhu, H. Xiong, and
Q. He, “A comprehensive survey on transfer learning,” Proceedings of
the IEEE , 2020.
[26] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A
survey,” JMLR , 2019.
[27] M. S. Abdelfattah, A. Mehrotra, Ł. Dudziak, and N. D. Lane, “Zero-cost
proxies for lightweight nas,” arXiv:2101.08134 , 2021.
[28] H. Chen, M. Lin, X. Sun, and H. Li, “Nas-bench-zero: A large scale
dataset for understanding zero-shot neural architecture search,” 2021.
[29] N. Lee, T. Ajanthan, and P. H. Torr, “Snip: Single-shot network pruning
based on connection sensitivity,” arXiv:1810.02340 , 2018.
[30] C. Wang, G. Zhang, and R. Grosse, “Picking winning tickets before
training by preserving gradient flow,” arXiv:2002.07376 , 2020.
[31] J. Turner, E. J. Crowley, M. O’Boyle, A. Storkey, and G. Gray,
“Blockswap: Fisher-guided block substitution for network compression
on a budget,” arXiv:1906.04113 , 2019.
[32] H. Tanaka, D. Kunin, D. L. Yamins, and S. Ganguli, “Pruning neural
networks without any data by iteratively conserving synaptic flow,”
NeurIPS , 2020.
[33] M. Lin, P. Wang, Z. Sun, H. Chen, X. Sun, Q. Qian, H. Li, and R. Jin,
“Zen-nas: A zero-shot nas for high-performance image recognition,” in
ICCV , 2021.
[34] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar, “Focal loss for
dense object detection,” in ICCV , 2017.
[35] W. J. Scheirer, L. P. Jain, and T. E. Boult, “Probability models for open
set recognition,” IEEE TPAMI , 2014.
[36] H. Rinne, The Weibull distribution: a handbook . CRC Press, 2008.
[37] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian optimiza-
tion of machine learning algorithms,” NeurIPS , 2012.
[38] F. Nogueira, “Bayesian Optimization: Open source constrained
global optimization tool for Python,” 2014–. [Online]. Available:
https://github.com/fmfn/BayesianOptimization
[39] N. Stander and K. Craig, “On the robustness of a simple domain
reduction scheme for simulation-based optimization,” Engineering Com-
putations , 2002.
[40] W. Chen, X. Gong, J. Wu, Y . Wei, H. Shi, Z. Yan, Y . Yang, and
Z. Wang, “Understanding and accelerating neural architecture search
with training-free and theory-grounded metrics,” arXiv:2108.11939 ,
2021.
[41] A. Marsden, “Eigenvalues of the laplacian and their relationship to the
connectedness of a graph,” University of Chicago, REU , 2013.
[42] U. V on Luxburg, “A tutorial on spectral clustering,” Statistics and
computing , 2007.
[43] A. Damle, V . Minden, and L. Ying, “Simple, direct and efficient multi-
way spectral clustering,” Information and Inference: A Journal of the
IMA, 2019.
[44] N. X. Vinh, J. Epps, and J. Bailey, “Information theoretic measures for
clusterings comparison: is a correction for chance necessary?” in ICML ,
2009.
[45] M. Tan and Q. Le, “Efficientnetv2: Smaller models and faster training,”
inICML . PMLR, 2021.
[46] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv:1412.6980 , 2014.[47] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y . Tian, P. Isola,
A. Maschinot, C. Liu, and D. Krishnan, “Supervised contrastive learn-
ing,” NeurIPS , 2020.
[48] D. Li, Y . Yang, Y .-Z. Song, and T. M. Hospedales, “Deeper, broader
and artier domain generalization,” in ICCV , 2017.
[49] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, “Adapting visual category
models to new domains,” in ECCV . Springer, 2010.
[50] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan,
“Deep hashing network for unsupervised domain adaptation,” in CVPR ,
2017.
[51] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang, “Moment
matching for multi-source domain adaptation,” in ICCV , 2019.
PLACE
PHOTO
HEREDr. Zachary A. Daniels is a research computer
scientist with the Vision Systems Lab of SRI Interna-
tional’s Center for Vision Technologies in Princeton,
NJ. He received his bachelor’s in computer science
with a minor in cognitive science from Lehigh
University in 2014. He received his doctorate in
computer science with a focus on machine learning,
computer vision, and artificial intelligence in 2020
from Rutgers University.
PLACE
PHOTO
HEREDr. David Zhang is a Senior Technical Manager
in the Vision Systems Lab of the Center for Vision
Technologies at SRI International with experience in
algorithm and embedded software development. He
has expertise in computer vision and machine learn-
ing algorithms, with a focus on edge computing and
video surveillance, tracking, and enhancements in
degraded visual environments. He received his PhD
in Physics from The Pennsylvania State University
in 2001.
