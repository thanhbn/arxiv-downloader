# SKELETON: Một Framework Mới để Tăng Tốc Các Mô Hình Ngôn Ngữ thông qua Prompt Tuning Định Vị Neuron Tác Vụ

## Tóm tắt
Các phương pháp prompt tuning đã cho thấy hiệu suất tương đương với các phương pháp huấn luyện tổng quát như các phương pháp fine-tuning hiệu quả tham số (PEFT) trong nhiều tác vụ hiểu ngôn ngữ tự nhiên khác nhau. Tuy nhiên, các phương pháp prompt tuning hiện tại vẫn sử dụng toàn bộ kiến trúc mô hình ngay cả khi giải quyết một tác vụ cụ thể, điều này ngăn chúng tăng tốc độ suy luận trong quá trình ứng dụng. Trong bài báo này, chúng tôi đề xuất một framework prompt tuning mới gọi là SKELETON để sử dụng hiệu quả mô hình ngôn ngữ về mặt độ phức tạp bộ nhớ và thời gian cho việc giải quyết các tác vụ khác nhau, chỉ giữ lại các neuron liên quan đến tác vụ bằng cách sử dụng phương pháp khả năng giải thích. Từ framework của chúng tôi, chúng tôi có thể giải quyết hiệu quả các tác vụ khác nhau bằng cách chỉ sử dụng các neuron liên quan đến tác vụ và thêm vào đầu các token prompt cụ thể cho tác vụ phù hợp chỉ với một mô hình ngôn ngữ duy nhất. Các thí nghiệm cho thấy phương pháp của chúng tôi cải thiện đáng kể hiệu quả suy luận (tối đa tăng tốc ×1.73) cho các benchmark được sử dụng rộng rãi, cho thấy hiệu suất tương đương với phương pháp prompt tuning. Hơn nữa, phương pháp của chúng tôi có thể áp dụng trên nhiều kiến trúc dựa trên transformer khác nhau, xác nhận tính thực tiễn và khả năng mở rộng của nó.

## 1 Giới thiệu
Với sự thành công của các phương pháp fine-tuning hiệu quả tham số (PEFT), các mô hình ngôn ngữ được huấn luyện trước thực hiện các tác vụ khác nhau bằng cách chỉ điều chỉnh một số lượng nhỏ tham số. Trong số các phương pháp PEFT, các phương pháp prompt tuning được sử dụng rộng rãi để giải quyết nhiều tác vụ vì chúng chỉ yêu cầu một vài token có thể điều chỉnh được thêm vào đầu các token đầu vào trong khi đóng băng các tham số gốc của mô hình ngôn ngữ. Chiến lược này tiết kiệm bộ nhớ, vì nó cho phép mô hình giải quyết các tác vụ khác nhau chỉ với một vài token prompt cụ thể cho tác vụ trong khi giữ nguyên các tham số gốc. Tuy nhiên, chúng ta có thể đặt ra một câu hỏi nghiên cứu về đặc tính cụ thể cho tác vụ này của prompt tuning:

"Tận dụng prompt tuning, liệu chúng ta có thể cô đọng kiến thức tác vụ vào một vài neuron tác vụ trong mô hình đã đóng băng để sử dụng hiệu quả nó không?"

Các framework prompt tuning hiện tại bị hạn chế bởi việc cần sử dụng toàn bộ kiến trúc mô hình gốc trong quá trình suy luận. Thực tế, các phương pháp này có thể yêu cầu thời gian suy luận bổ sung, vì các token prompt được thêm vào đầu gây ra chi phí tính toán thêm. Do đó, các phương pháp hiện tại đã cố gắng tăng tốc độ suy luận của các mô hình prompt-tuned bằng cách cắt bỏ các token prompt không cần thiết. Tuy nhiên, chúng chỉ cải thiện hiệu quả suy luận một cách nhỏ lẻ, vì số lượng token bị cắt bỏ chỉ là một phần nhỏ (≤0.1%) của tất cả các tham số của mô hình ngôn ngữ.

Các nghiên cứu trước đây đã cố gắng điều tra và phân tích các neuron liên quan đến tác vụ trong các mô hình ngôn ngữ để giải thích hành vi của mô hình ngôn ngữ. Cụ thể, Panigrahi et al. (2023) đã đề xuất huấn luyện các tham số mới, che đi các tham số gốc của mô hình ngôn ngữ để xác định các neuron tác vụ. Yang et al. (2024) đã sử dụng phương pháp khả năng giải thích để xác định các neuron liên quan đến tác vụ và tiết lộ rằng một số neuron liên quan đến tác vụ ảnh hưởng đáng kể đến đầu ra của các tác vụ cụ thể (tức là hiểu ngôn ngữ tự nhiên, thiên vị xã hội). Tuy nhiên, trong khi họ đã giới thiệu phương pháp để xác định các neuron liên quan đến tác vụ, họ không đề xuất cách tiếp cận thực tiễn để sử dụng hiệu quả những neuron đó trong quá trình huấn luyện và suy luận.

Để giải quyết thử thách này, chúng tôi đề xuất một framework huấn luyện mới, SKELETON, được thiết kế để nâng cao hiệu quả của các mô hình ngôn ngữ bằng cách chỉ tận dụng các neuron liên quan đến tác vụ để giải quyết các tác vụ khác nhau. Cụ thể, chúng tôi sử dụng attribution (Yang et al., 2023, 2024), một phương pháp khả năng giải thích, để xác định và chỉ sử dụng các neuron liên quan đến tác vụ trong quy trình prompt tuning. Framework của chúng tôi chỉ yêu cầu một vài token prompt được cập nhật và đóng băng các tham số gốc của mô hình ngôn ngữ, vì nó áp dụng phương pháp prompt tuning. Hơn nữa, SKELETON tăng tốc độ suy luận của mô hình ngôn ngữ một cách đáng kể vì nó chỉ sử dụng các neuron liên quan đến tác vụ. Ví dụ, để giải quyết một tác vụ cụ thể (ví dụ: phân tích cảm xúc, suy luận ngôn ngữ tự nhiên), phương pháp của chúng tôi thêm vào đầu các token prompt liên quan đến tác vụ và chỉ kích hoạt các neuron liên quan đến tác vụ để giải quyết tác vụ.

Chúng tôi thể hiện phương pháp của mình trên nhiều benchmark hiểu ngôn ngữ tự nhiên được sử dụng rộng rãi và tiết lộ rằng phương pháp của chúng tôi vượt trội đáng kể so với các phương pháp PEFT hiện tại về hiệu quả suy luận mà không hy sinh hiệu suất gốc của mỗi tác vụ. Đáng ngạc nhiên, phương pháp của chúng tôi cải thiện tốc độ suy luận lên đến ×1.73 với tỷ lệ cắt bỏ tham số 65%. Chúng tôi cũng tiến hành nhiều phân tích khác nhau để đưa ra hướng dẫn sử dụng framework prompt tuning mới của chúng tôi, chẳng hạn như điều tra loại lớp nào quan trọng để xử lý kiến thức tác vụ trong mô hình ngôn ngữ. Hơn nữa, framework prompt tuning của chúng tôi thực tiễn và có thể mở rộng vì nó có thể áp dụng cho bất kỳ kiến trúc dựa trên transformer nào.

## 2 Kiến thức nền tảng

### 2.1 Fine-tuning hiệu quả tham số
Các phương pháp Fine-tuning hiệu quả tham số (PEFT) đã nổi lên như một tiêu chuẩn của các phương pháp huấn luyện hiệu quả cho mô hình ngôn ngữ. Trong số đó, các phương pháp prompt tuning đã được áp dụng cho nhiều tác vụ NLP khác nhau. Các phương pháp prompt tuning chỉ cập nhật các token prompt trong khi duy trì các tham số gốc của mô hình ngôn ngữ; do đó, đặc tính này làm cho quá trình huấn luyện tiết kiệm bộ nhớ và hiệu quả trong việc đưa kiến thức tác vụ vào mô hình ngôn ngữ. Tuy nhiên, chúng vẫn đối mặt với hạn chế vì chúng không hiệu quả về tham số trong quá trình suy luận. Do đó, các phương pháp hiện tại đề xuất phương pháp lựa chọn token prompt để nâng cao tốc độ suy luận của các mô hình ngôn ngữ prompt-tuned. Tuy nhiên, chúng chỉ cải thiện hiệu quả suy luận một cách nhỏ lẻ vì số lượng token bị cắt bỏ chỉ là một tỷ lệ nhỏ của toàn bộ số lượng token đầu vào.

### 2.2 Định vị kiến thức tác vụ
Mặc dù có hiệu suất đáng kể của các mô hình ngôn ngữ, việc điều tra chính xác vai trò của mỗi tham số trong mô hình ngôn ngữ trong việc xử lý một tác vụ cụ thể vẫn đầy thử thách. Do đó, các nghiên cứu hiện tại đã cố gắng xác định các neuron cụ thể cho tác vụ trong việc thực hiện một tác vụ cụ thể. Panigrahi et al. (2023); Yang et al. (2023, 2024) đã nghiên cứu để định vị các neuron cho việc thực hiện một tác vụ cụ thể bằng cách làm rõ vai trò của mỗi tham số trong mô hình ngôn ngữ. Panigrahi et al. (2023) đã đề xuất phương pháp định vị kiến thức tác vụ dựa trên huấn luyện, model grafting, định vị kiến thức tác vụ bằng cách huấn luyện các tham số mới để che các tham số gốc. Mặc dù nó đã xác định hiệu quả các neuron tác vụ, nó đã yêu cầu lưu trữ bổ sung quá mức cho các tham số che mới được huấn luyện, tương đương với số lượng tham số mô hình.

Yang et al. (2023, 2024) đã soi sáng các neuron tác vụ bằng cách sử dụng kỹ thuật attribution, một phương pháp khả năng giải thích rút ra tầm quan trọng của mỗi đặc trưng khi giải quyết một tác vụ cụ thể. Yang et al. (2023, 2024) đã xác minh rằng attribution hiệu quả trong việc phát hiện các neuron tác vụ để giải quyết một tác vụ cụ thể và đề xuất phương pháp phát hiện neuron tác vụ có thể áp dụng cho các tác vụ mô hình hóa ngôn ngữ. Tuy nhiên, Yang et al. (2023, 2024) có hạn chế chỉ trong việc cắt bỏ các neuron tác vụ; nếu chúng ta cắt bỏ một số lượng lớn neuron, hiệu suất của một tác vụ cụ thể có thể bị suy giảm đáng kể. Hơn nữa, nghiên cứu trước đây chỉ đề xuất phương pháp cắt bỏ các neuron tác vụ và còn xa mới đề xuất phương pháp thực tiễn để cải thiện tốc độ suy luận của các mô hình ngôn ngữ định vị tác vụ.

Kiến trúc của các biến thể transformer bao gồm các quá trình tính toán neural phức tạp; do đó, việc thỏa mãn tính toàn vẹn của sự phụ thuộc chiều cho các mạng liền kề trong khi cắt bỏ các trọng số là thử thách. Chúng tôi áp dụng phương pháp attribution và đề xuất một framework thực tiễn mới để tăng tốc mô hình ngôn ngữ, trong đó framework chỉ sử dụng các neuron tác vụ trong khi huấn luyện mô hình ngôn ngữ thông qua prompt tuning, giải quyết các thử thách trong việc cắt bỏ mạng trong các biến thể transformer.

## 3 Định nghĩa bài toán

### 3.1 Prompt Tuning
Chúng tôi áp dụng phương pháp prompt tuning để đề xuất framework mới cho việc tăng tốc mô hình ngôn ngữ. Chính thức, giả sử chúng ta có hàm Pθ(y|x) = ∏_{k=1}^K Pθ(yk|x, y₁, ..., yk₋₁) đại diện cho mô hình ngôn ngữ, trong đó θ là các tham số của mô hình ngôn ngữ. Prompting là phương pháp thêm thông tin bổ sung vào mô hình để điều kiện hóa trong quá trình sinh y của nó. Thông thường, prompting được thực hiện bằng cách thêm một chuỗi token Pτ ∈ ℝ^{l×d} vào đầu vào x như Pθ(y|[Pτ;x]), trong đó l và d là độ dài token và chiều đặc trưng, và Pτ là tham số có thể huấn luyện cho tác vụ cụ thể τ. Trong quá trình prompt tuning, chúng ta chỉ cập nhật Pτ và nhận P'τ để tối đa hóa likelihood của y bằng gradient descent trong khi đóng băng các tham số của mô hình ngôn ngữ θ. Sau prompt tuning, chúng ta nhận được hàm sinh có điều kiện mới Pθ(y|[P'τ;x]).

### 3.2 Framework SKELETON
Trong framework SKELETON, chúng tôi vẫn chỉ cập nhật các token prompt, đóng băng các tham số gốc. Tuy nhiên, chúng tôi giả định rằng có tổ hợp tham số tối thiểu θτ ⊂ θ để giải quyết tác vụ cụ thể τ; do đó, chúng tôi trước tiên định nghĩa neuron tác vụ như sau:

**Neuron Tác vụ.** Cho Pθ là mô hình ngôn ngữ, trong đó θ = {θ₁, ..., θN} là tập hợp các tham số của nó. Khi đó, θτ ⊂ θ được định nghĩa là các neuron tác vụ cho tác vụ τ nếu phương trình dưới đây được thỏa mãn:

∑_{(x,y)∈Dτ} L(y, Pθ([P'τ;x])) ≈ ∑_{(x,y)∈Dτ} L(y, Pθτ([P''τ;x]))

trong đó Dτ có nghĩa là tập dữ liệu cho tác vụ τ và L là hàm điểm số (ví dụ: hàm mất mát hoặc độ chính xác). P'τ và P''τ là các prompt cụ thể cho tác vụ. N có nghĩa là tổng số tham số trong mô hình ngôn ngữ.

Framework SKELETON áp dụng chiến lược prompt tuning chỉ sử dụng các neuron tác vụ θτ, và nhận được hàm sinh có điều kiện định vị tác vụ mới Pθτ(y|[P''τ;x]). Do đó, chúng ta có thể sử dụng mô hình ngôn ngữ hiệu quả hơn về thời gian bằng cách chỉ sử dụng những neuron đó trong framework SKELETON. Ngoài việc hiệu quả về thời gian, framework này cũng hiệu quả về bộ nhớ vì nó chỉ yêu cầu một phần nhỏ của các tham số gốc θτ và chỉ một vài token prompt cụ thể cho tác vụ P''τ.

## 4 Phương pháp

Trong phần này, chúng tôi mô tả quá trình của SKELETON. Chúng tôi trước tiên huấn luyện mô hình ngôn ngữ bằng phương pháp prompt tuning để đưa kiến thức tiên nghiệm của tác vụ cụ thể vào nó. Sau đó, chúng tôi định lượng mức độ liên quan tác vụ của mỗi neuron để xác định các neuron tác vụ trong mô hình ngôn ngữ. Cuối cùng, chúng tôi huấn luyện lại các token prompt bằng phương pháp prompt tuning để cô đọng kiến thức tác vụ chỉ vào các neuron tác vụ. Lưu ý rằng mô hình ngôn ngữ gốc chỉ duy trì các neuron tác vụ thông qua phương pháp cắt bỏ trong quá trình này. Sau khi áp dụng framework SKELETON, chúng ta có thể sử dụng mô hình ngôn ngữ hiệu quả bằng cách chỉ sử dụng prompt cụ thể cho tác vụ và các neuron tác vụ, tăng tốc độ suy luận cho tác vụ cụ thể.

### 4.1 Định lượng mức độ liên quan tác vụ của các Neuron
Phần này mô tả quá trình tính toán mức độ liên quan tác vụ của mỗi neuron. Chúng tôi áp dụng phương pháp attribution để định lượng mức độ liên quan kỹ năng của các neuron trong mô hình ngôn ngữ được huấn luyện trước. Chính thức, cho các token prompt cụ thể cho tác vụ Pτ và các token đầu vào văn bản x được cung cấp cho Pθ, đóng góp của neuron thứ i vào dự đoán đầu ra y được định nghĩa như sau:

A^{(Pτ,y,x)}_i(h) = |hi × ∂Pθ(y|[Pτ;x])/∂hi|

trong đó h tương ứng với biểu diễn ẩn, và ∂Pθ(y|[Pτ;x])/∂hi có nghĩa là gradient của logit Pθ(y|[Pτ;x]) đối với neuron thứ i.

Chúng tôi áp dụng các biến thể transformer; do đó, các activations và gradients được tính cho tất cả các biểu diễn token đầu vào. Do đó, nếu văn bản đầu vào xj bao gồm Kj token, mỗi neuron có Kj attribution. Ngoài ra, có nhiều instance dữ liệu cho mỗi tác vụ τ; do đó, chúng tôi tổng hợp các attribution cho token và instance như sau:

A^{(Pτ,Dτ)}_i(h) = ∑_{j}^N ∑_{k}^{Kj} (1/Kj)A^{(Pτ,yj,xj,tk)}_i(h)

trong đó A^{(Pτ,yj,xj,tk)}_i(h) có nghĩa là điểm attribution được tính cho token văn bản đầu vào tk ∈ xj. Dτ và N có nghĩa là tập dữ liệu cụ thể cho tác vụ và số lượng instance trong tập dữ liệu, tương ứng.

### 4.2 Xác định các Neuron liên quan đến tác vụ
Sau khi định lượng mức độ liên quan tác vụ của mỗi neuron, chúng tôi xác định các neuron liên quan đến tác vụ và loại bỏ các neuron không liên quan đến tác vụ bằng phương pháp cắt bỏ có cấu trúc. Chúng tôi trước tiên sắp xếp các neuron của toàn bộ các lớp mục tiêu theo điểm mức độ liên quan tác vụ. Sau đó, chúng tôi xác định tỷ lệ neuron tối ưu bằng thuật toán Binary search để giữ lại ít neuron tác vụ nhất có thể mà không làm tổn hại kiến thức tác vụ. Cụ thể, chúng tôi tính độ chính xác cho tập validation Dval để tìm kiếm tỷ lệ neuron tối ưu không làm giảm hiệu suất tác vụ. Chúng tôi định nghĩa margin ψ và coi θτ là các neuron tác vụ nếu sự suy giảm độ chính xác sau định vị neuron nhỏ hơn margin được chỉ định ψ.

### 4.3 Định vị Neuron tác vụ
Sau khi xác định các neuron tác vụ, chúng tôi loại bỏ các neuron không liên quan đến tác vụ khỏi mô hình ngôn ngữ bằng phương pháp cắt bỏ có cấu trúc. Giả sử ma trận trọng số W ∈ ℝ^{d×m} là tham số nhân ma trận tuyến tính, và sau đó ma trận sau cắt bỏ được ký hiệu là W̃ = (Wij)_{1≤i≤d, j∈M}, trong đó M là tập hợp các chỉ số neuron liên quan đến tác vụ về W. Nếu bias term b ∈ ℝ^m được thêm vào phép toán cho phép biến đổi affine, bias term cũng có thể được cắt bỏ bằng cách thực hiện phép toán b̃ = (bi)_{i∈M} tương tự. Các tham số định vị tác vụ được sử dụng để tính biểu diễn mới bằng cách thực hiện phép biến đổi h̃W hoặc h̃W + b̃.

### 4.4 Prompt Tuning định vị Neuron tác vụ
Framework SKELETON của chúng tôi theo bốn bước để huấn luyện mô hình: (1) bước huấn luyện đầu tiên để đưa kiến thức tác vụ tiên nghiệm; (2) bước định lượng tính mức độ liên quan tác vụ cho mỗi neuron; (3) bước định vị neuron tác vụ; (4) bước prompt tuning cuối cùng để cô đọng kiến thức tác vụ.

**Bước đưa kiến thức tiên nghiệm.** Phương pháp attribution định lượng đóng góp của mỗi neuron vào dự đoán đầu ra; do đó, mô hình ngôn ngữ phải có khả năng giải quyết tác vụ cụ thể để định lượng chính xác kiến thức cụ thể cho tác vụ của mỗi neuron. Do đó, chúng tôi trước tiên thực hiện quá trình đưa kiến thức tiên nghiệm. Cụ thể, chúng tôi huấn luyện mô hình ngôn ngữ Pθ thông qua prompt tuning và nhận prompt cụ thể cho tác vụ P'τ.

**Bước định lượng.** Sau khi đưa kiến thức tác vụ vào mô hình ngôn ngữ, chúng tôi định lượng mức độ liên quan tác vụ Ai của mỗi neuron cho Pθ và P'τ bằng Phương trình 2 và 3.

**Bước định vị Neuron tác vụ.** Sau khi định lượng mức độ liên quan tác vụ Ai, chúng tôi trước tiên sắp xếp các neuron theo mức độ liên quan tác vụ theo thứ tự giảm dần. Sau đó, chúng tôi chọn và duy trì chỉ top-p neuron θτ ⊂ θ từ mô hình ngôn ngữ bằng cách cắt bỏ các neuron không liên quan đến tác vụ. Lưu ý rằng chúng tôi sử dụng thuật toán Binary search để tìm tỷ lệ tối ưu của các neuron p ∈ [0.0, 0.05, ..., 0.95, 1.0].

**Bước cô đọng kiến thức.** Trong bước định vị neuron tác vụ, chúng tôi cô đọng kiến thức tác vụ chỉ vào các neuron tác vụ. Mặc dù chúng tôi không bao giờ cập nhật các tham số gốc của mô hình ngôn ngữ, chúng tôi có thể cô đọng kiến thức tác vụ vào các neuron tác vụ bằng cách chỉ cập nhật P'τ. Chúng tôi huấn luyện mô hình định vị neuron tác vụ Pθτ và nhận prompt tác vụ cô đọng kiến thức mới P''τ.

### 4.5 Chi tiết của Framework SKELETON

**Thỏa mãn tính toàn vẹn chiều.** Phần này mô tả các quá trình định vị tác vụ chi tiết. Chúng tôi cắt bỏ các neuron không liên quan đến tác vụ khỏi mô hình ngôn ngữ. Các mô hình ngôn ngữ gần đây đã được huấn luyện bằng kiến trúc transformer, nhưng chúng bao gồm các quá trình tính toán neural phức tạp. Ví dụ, các mạng được kết nối tuần tự phụ thuộc lẫn nhau về mặt chiều. Giả sử có hai feed-forward networks (FFNs) tuần tự, W₁ ∈ ℝ^{d×m} và W₂ ∈ ℝ^{m×d}. Nếu chúng ta cắt bỏ FFN đầu tiên và nhận trọng số bị cắt W̃₁ ∈ ℝ^{d×m'}, quá trình cắt bỏ này ảnh hưởng đến sự không nhất quán của chiều với trọng số trong FFN thứ hai. Cụ thể, W₂ bị buộc phải được cắt bỏ thành W̃₂ ∈ ℝ^{m'×d}.

**Tăng tốc độ suy luận.** Framework của chúng tôi không chỉ chuyển đổi giá trị bị cắt bỏ của trọng số thành giá trị zero mà loại bỏ hoàn toàn cột (tức là neuron) của chúng. Do đó, framework của chúng tôi tăng tốc độ suy luận mô hình ngôn ngữ. Chúng tôi không xem xét các loại mạng khác (ví dụ: module Attention, lớp Embedding, đầu mô hình ngôn ngữ) vì chúng đóng vai trò quan trọng trong việc hiểu ngôn ngữ của mô hình.

## 5 Thí nghiệm

### 5.1 Thiết lập thí nghiệm

**Tập dữ liệu.** Chúng tôi tiến hành thí nghiệm trên năm tập dữ liệu hiểu ngôn ngữ tự nhiên được sử dụng rộng rãi. Cụ thể, chúng tôi sử dụng SST-2, IMDB (phân tích cảm xúc); AGNews (phân loại chủ đề); MRPC (matching văn bản ngữ nghĩa); CB (suy luận ngôn ngữ tự nhiên) để xác minh khả năng áp dụng của phương pháp chúng tôi.

**Mô hình và phương pháp huấn luyện.** Chúng tôi sử dụng hai biến thể transformer, BERT và RoBERTa, cho các thí nghiệm của chúng tôi vì chúng là một số mô hình ngôn ngữ cơ bản phổ biến nhất. Chúng tôi chọn phương pháp prompt tuning để so sánh với phương pháp của chúng tôi, và cũng bao gồm kết quả của fine-tuning cơ bản và multi-task fine-tuning để cho thấy hiệu suất upper-bound của mỗi tác vụ.

**Chi tiết triển khai.** Chúng tôi đánh giá SKELETON và các baseline trên GPU NVIDIA A5000. Chúng tôi huấn luyện mô hình sử dụng l = 20 token prompt trong 100 epochs với điều kiện early-stop cho tất cả tập dữ liệu. Ngoài ra, chúng tôi đặt margin ψ = 1.0% cho điểm độ chính xác validation được sử dụng trong thuật toán Binary search. Chúng tôi định lượng mức độ liên quan tác vụ chỉ sử dụng n = 20 instance dữ liệu của mỗi tập dữ liệu huấn luyện. Chúng tôi chỉ định các module mục tiêu cho định vị neuron tác vụ là các mạng FFN vì kết quả thí nghiệm cho các mạng FFN vượt trội so với các module khác.

### 5.2 SKELETON hiệu quả và mạnh mẽ

**Thí nghiệm tập dữ liệu.** Chúng tôi đánh giá và báo cáo hiệu suất tác vụ và hiệu quả của mỗi phương pháp trong Bảng 1 cho năm benchmark hiểu ngôn ngữ tự nhiên. Chúng tôi áp dụng BERT-base và RoBERTa-base để tiến hành thí nghiệm. Kết quả cho thấy phương pháp của chúng tôi hoạt động tương đương với phương pháp prompt tuning và thậm chí vượt trội về độ chính xác cho một số tập dữ liệu chỉ sử dụng một số lượng nhỏ tham số định vị tác vụ (ví dụ: tối đa 65% tỷ lệ cắt bỏ). Đáng ngạc nhiên, các mạng bị cắt bỏ cho thấy tối đa ×1.72 tăng tốc; do đó, những kết quả này tiết lộ rằng phương pháp của chúng tôi thành công trong việc định vị các sub-networks liên quan đến tác vụ hiệu quả từ mô hình ngôn ngữ.

**Thí nghiệm siêu tham số.** Chúng tôi tiến hành thí nghiệm để điều tra hiệu quả thời gian của framework chúng tôi cho các siêu tham số khác nhau. Cụ thể, chúng tôi tìm kiếm các kích thước batch khác nhau và số lượng token đầu vào để thể hiện hiệu quả bằng cách so sánh framework của chúng tôi với phương pháp prompt tuning. Kết quả tiết lộ rằng framework của chúng tôi cho thấy cải thiện hiệu quả mạnh mẽ trong các siêu tham số khác nhau, cho thấy tốc độ tốt hơn ở kích thước batch cao hơn và số lượng token được sử dụng rộng rãi.

### 5.3 Thí nghiệm phương pháp định vị tác vụ

Chúng tôi so sánh phương pháp của chúng tôi với các phương pháp lựa chọn neuron tác vụ khác: Activation, Gradient, và Random, để biện minh cho sự xuất sắc của phương pháp attribution. Activation xác định các neuron tác vụ bằng cách sử dụng các giá trị kích hoạt của neuron. Gradient chọn các neuron tác vụ bằng cách sử dụng các giá trị gradient của mỗi neuron. Random chọn ngẫu nhiên các neuron tác vụ. Đáng ngạc nhiên, thậm chí Activation và Gradient không cho thấy hiệu suất cạnh tranh so với Random. Tuy nhiên, SKELETON (tức là Attribution) vượt trội so với các phương pháp khác trong hai tập dữ liệu, tiết lộ sự xuất sắc của phương pháp chúng tôi.

### 5.4 Định vị tác vụ cụ thể theo module

Các biến thể Transformer có nhiều loại module khác nhau (ví dụ: module attention và FFNs). Do đó, chúng tôi kiểm tra module nào phù hợp để duy trì kiến thức của mô hình ngôn ngữ (BERT-base). Cụ thể, chúng tôi phân loại các lớp thành bốn phân đoạn: (1) Tất cả lớp (All), (2) Lớp Attention (Attn), (3) Lớp Dense (Dense), và (4) Lớp Feed-forward (FFN). Từ những thí nghiệm này, chúng tôi tiết lộ rằng FFNs phù hợp để duy trì kiến thức của mô hình ngôn ngữ trong thiết lập prompt tuning của chúng tôi. Áp dụng module FFN mang lại hiệu suất nhanh hơn so với các module khác mà không làm tổn hại độ chính xác.

### 5.5 Đưa kiến thức tiên nghiệm là quan trọng

Phần này mô tả tầm quan trọng của việc đưa kiến thức tác vụ tiên nghiệm để định lượng chính xác mức độ liên quan tác vụ. Kết quả thí nghiệm tiết lộ rằng việc đưa kiến thức tác vụ tiên nghiệm có vai trò quan trọng trong việc định lượng mức độ liên quan tác vụ. Loại trừ bước Đưa kiến thức tiên nghiệm dẫn đến hội tụ chậm hơn và độ chính xác tác vụ thấp hơn so với phương pháp của chúng tôi.

### 5.6 SKELETON có thể mở rộng cho mô hình lớn hơn

Chúng tôi tiến hành thí nghiệm để xác minh rằng phương pháp của chúng tôi có thể mở rộng cho mô hình lớn hơn. Cụ thể, chúng tôi áp dụng BERT-large để cho thấy khả năng mở rộng của framework chúng tôi. Những kết quả này tiết lộ rằng phương pháp của chúng tôi có thể mở rộng cho mô hình lớn hơn.

## 6 Kết luận

Trong nghiên cứu này, chúng tôi đề xuất SKELETON, khám phá cực hạn của phương pháp prompt tuning bằng cách định vị các neuron liên quan đến tác vụ từ mô hình ngôn ngữ gốc. SKELETON điều tra sub-network định vị tác vụ tối ưu từ mô hình ngôn ngữ và cô đọng kiến thức tác vụ bằng cách chỉ sử dụng các tham số liên quan đến tác vụ trong quá trình prompt-tuning. Chúng tôi thể hiện phương pháp của chúng tôi cho nhiều benchmark khác nhau và tiết lộ rằng SKELETON cải thiện đáng kể hiệu quả của mô hình ngôn ngữ, cho thấy sự tăng tốc độ suy luận đáng kể.

## Hạn chế

Framework của chúng tôi tăng tốc độ suy luận của mô hình ngôn ngữ, xác định sub-network tối ưu để sử dụng mô hình ngôn ngữ. Tuy nhiên, trong khi cách tiếp cận của chúng tôi thể hiện hiệu suất tương đương và thỉnh thoảng kết quả ưu việt hơn một chút so với phương pháp prompt tuning hiện tại, những cải thiện này không nhất quán trên toàn bộ hiệu suất tác vụ. Ngoài ra, các thí nghiệm của chúng tôi bị giới hạn trong năm tập dữ liệu hiểu ngôn ngữ tự nhiên được sử dụng rộng rãi; do đó, chúng tôi nên mở rộng sang một phạm vi rộng các tập dữ liệu để xác thực khả năng áp dụng của framework chúng tôi. Hơn nữa, chúng tôi chọn các mạng feed-forward để định vị các neuron tác vụ trong mô hình ngôn ngữ vì chúng đầy đủ để tăng tốc độ suy luận. Tuy nhiên, toàn bộ các module trong Transformer nên được xem xét như mục tiêu định vị để đạt được tăng tốc suy luận cực hạn; chúng tôi để lại điểm này như nghiên cứu tương lai.
