# 2312.00968.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2312.00968.pdf
# Kích thước tệp: 1108448 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Omni-SMoLA: Tăng cường Mô hình Đa phương thức Tổng quát với Hỗn hợp Mềm của
Các Chuyên gia Hạng thấp
Jialin Wu Xia Hu Yaqing Wang Bo Pang Radu Soricut
Google Research
{jialinwu, xiahu, yaqingwang, bopang, rsoricut }@google.com
Tóm tắt
Các mô hình đa phương thức lớn (LMM) thể hiện hiệu suất
đáng chú ý trên nhiều tác vụ. Tuy nhiên, các LMM tổng quát
thường gặp phải sự suy giảm hiệu suất khi được tinh chỉnh
trên một tập hợp lớn các tác vụ. Nghiên cứu gần đây cho
thấy rằng kiến trúc Hỗn hợp Chuyên gia (MoE) hữu ích cho
việc tinh chỉnh hướng dẫn, nhưng đối với LMM có kích
thước tham số khoảng O(50-100B), chi phí cấm đoán của
việc nhân bản và lưu trữ các mô hình chuyên gia hạn chế
nghiêm trọng số lượng chuyên gia chúng ta có thể sử dụng.
Chúng tôi đề xuất Omni-SMoLA, một kiến trúc sử dụng
phương pháp Soft MoE để kết hợp (một cách mềm mại)
nhiều chuyên gia đa phương thức hạng thấp, và tránh việc
đưa vào một số lượng đáng kể tham số mới so với các
mô hình MoE thông thường. Trực giác cốt lõi ở đây là mô
hình lớn cung cấp một xương sống nền tảng, trong khi các
chuyên gia nhẹ khác nhau học tập chuyên môn hóa cư trú,
hoặc theo từng phương thức hoặc đa phương thức. Các thí
nghiệm rộng rãi chứng minh rằng phương pháp SMoLA
giúp cải thiện hiệu suất tổng quát trên một loạt rộng các tác
vụ sinh thị giác-ngôn ngữ, đạt được hiệu suất tổng quát
SoTA mới thường khớp hoặc vượt trội so với các đường
cơ sở LMM chuyên môn đơn lẻ, cũng như hiệu suất chuyên
môn SoTA mới.

1. Giới thiệu
Các mô hình đa phương thức lớn (LMM) [7–9, 14, 33, 53]
thể hiện hiệu suất đáng chú ý trên nhiều tác vụ khác nhau
bao gồm trả lời câu hỏi thị giác, tạo chú thích hình ảnh,
hiểu tài liệu thị giác, v.v. Cho đến nay, hiệu suất tốt nhất
trên hầu hết các tác vụ này được đạt bởi các LMM chuyên
môn, nhưng quy mô lớn của chúng khiến việc triển khai
nhiều chuyên gia như vậy cùng một lúc trở nên không thực
tế. Kết quả là, các LMM tổng quát xuất hiện như một lựa
chọn rõ ràng, trong đó mô hình như vậy được huấn luyện
và triển khai để xử lý một loạt rộng các tác vụ sử dụng
cùng một bộ tham số mô hình.

Xây dựng một mô hình tổng quát duy nhất để giải quyết
nhiều tác vụ vẫn là thách thức. Một phương pháp đơn giản
là tinh chỉnh các tham số mô hình với dữ liệu có giám sát
đại diện cho nhiều tác vụ. Tuy nhiên, nghiên cứu gần đây
cho thấy rằng điều này gây ra sự suy giảm hiệu suất không
đáng kể so với hiệu suất của chuyên gia một tác vụ [7]. Có
khả năng là, mặc dù các tác vụ này chia sẻ cùng cấu hình
phương thức (ví dụ, hình ảnh + văn bản làm đầu vào, văn
bản làm đầu ra), những gì mô hình cần giải quyết là đáng
kể đa dạng – ví dụ, một số tác vụ yêu cầu nhận dạng bản
sắc chi tiết của nội dung thị giác, những tác vụ khác có thể
dựa vào kiến thức thế giới bên ngoài cảnh thị giác, trong
khi những tác vụ khác yêu cầu đọc và hiểu văn bản từ hình
ảnh.

Công trình gần đây [48] cho thấy rằng các mô hình Hỗn
hợp Chuyên gia (MoE) được hưởng lợi nhiều hơn từ việc
tinh chỉnh hướng dẫn so với các mô hình dày đặc, và phục
vụ như các kiến trúc ứng cử viên tốt để xây dựng các mô
hình ngôn ngữ lớn tổng quát. Một cách trực quan, điều
này nên hoạt động tốt vì các mô-đun chuyên gia khác nhau
có thể chuyên môn hóa và xử lý các tác vụ khác nhau. Tuy
nhiên, có một vấn đề rõ ràng với việc áp dụng thiết kế MoE
trên các khối Transformer cho các mô hình quy mô lớn:
các khối transformer khác nhau dẫn đến việc nhân bản
các tham số mô hình sử dụng các chuyên gia hạng cao.
Điều này tạo ra tình huống trong đó quy mô của mỗi khối
mô hình chuyên gia so với các đối tác mô hình dày đặc của
chúng bị hạn chế nhiều hơn.

Trong công trình này, chúng tôi giải quyết các hạn chế
nêu trên bằng cách giới thiệu Omni-SMoLA, một kiến trúc
kết hợp hiệu quả nhiều chuyên gia đa phương thức hạng
thấp. Sử dụng kiến trúc này, chúng tôi chứng minh khả
năng mạnh mẽ để thích ứng các mô hình được huấn luyện
trước để giải quyết các tác vụ chuyên môn. Trực giác cốt
lõi là một mô hình lớn được huấn luyện trước (hoặc được
tinh chỉnh hướng dẫn) cung cấp một xương sống nền tảng
của các khả năng (chúng tôi ký hiệu mô hình này bằng θ∗),
trong khi các chuyên gia nhẹ khác nhau học thêm các
chuyên môn hóa (có thể là kiến thức, phong cách, hoặc
khả năng). Cụ thể, đối với các phương thức được xem xét
trong bài báo này (văn bản & thị giác), kiến trúc Omni-
SMoLA bao gồm ba bộ chuyên gia, tập trung vào các token
văn bản, token thị giác và token đa phương thức, tương
ứng, để đáp ứng các nhu cầu khác nhau từ các tác vụ khác
nhau.

Nói chung, thiết kế SMoLA có một số đặc tính quan trọng.
Đầu tiên, do việc áp dụng thiết kế chuyên gia hạng thấp
[43] và không giống như các mô hình transformer MoE
thông thường [15, 18, 30, 48], tổng số lượng tham số không
tỷ lệ với tích của số lượng chuyên gia và số lượng tham số
trong mỗi chuyên gia vì xương sống vẫn chứa phần lớn
tham số. Điều này cho phép nó vượt qua hạn chế về số
lượng chuyên gia được sử dụng, giúp đạt được hiệu suất
tổng quát tốt hơn. Thứ hai, thiết kế này có khả năng tương
thích với bất kỳ kiến trúc mô hình lớn nào, dày đặc hoặc
MoE. Và cuối cùng nhưng không kém phần quan trọng,
nó cho phép tự do có thể áp dụng các kiến trúc mô hình
khác nhau giữa giai đoạn huấn luyện trước và giai đoạn
học đa tác vụ (hoặc tinh chỉnh hướng dẫn).

Chúng tôi đánh giá phương pháp Omni-SMoLA trên nhiều
cài đặt khác nhau, bắt đầu từ PaLI-3 [8] (một LMM 5B) và
PaLI-X [7] (một LMM 55B), các mô hình có hiệu suất hiện
tại tốt nhất (SOTA) trên một loạt rộng các chuẩn mực thị
giác-ngôn ngữ. Các cài đặt bao gồm các tác vụ tạo chú
thích hình ảnh và tác vụ trả lời câu hỏi thị giác khác nhau,
và chúng tôi thí nghiệm với các kết hợp có thể về mặt
chuyên môn hóa mô hình. Chúng tôi phát hiện rằng: (1)
Omni-SMoLA đạt được hiệu suất trung bình tốt hơn so với
các đường cơ sở tinh chỉnh mô hình đầy đủ cho cả PaLI-3
và PaLI-X; các thí nghiệm của chúng tôi cho thấy rằng nó
đạt được kết quả SoTA mới trên nhiều chuẩn mực thị giác-
ngôn ngữ, cả trong cài đặt tổng quát và trong cài đặt
chuyên môn; (2) hiệu suất cải thiện với việc giới thiệu các
chuyên gia Omni, và cũng tăng với số lượng chuyên gia;
(3) mặc dù có thêm các mô-đun và một số lượng lớn
chuyên gia mỗi mô-đun, tốc độ suy luận chỉ chậm hơn
một chút so với các mô hình cơ sở, cho thấy hiệu quả của
thiết kế này.

2. Công trình Liên quan
2.1. Các Mô hình Đa phương thức Lớn
Được truyền cảm hứng từ thành công của Mô hình Ngôn
ngữ Lớn [5, 11, 13], có sự quan tâm ngày càng tăng trong
việc xây dựng các mô hình đa phương thức lớn (LMM)
[8, 9, 14, 33] được thiết kế để hiểu cả tín hiệu thị giác và
ngôn ngữ đồng thời [14, 32]. Phương pháp chính là tích
hợp một bộ mã hóa hình ảnh được huấn luyện trước, biểu
diễn hình ảnh như một chuỗi các nhúng liên tục, với mô
hình ngôn ngữ tự hồi quy [7–9, 14, 33, 53]. Loạt công trình
PaLI [7–9] tích hợp các mô hình ViT được huấn luyện trước
[ ?] vào khung ngôn ngữ mã hóa-giải mã. PaLM-E [14] kết
hợp các bộ mã hóa thị giác như các phương thức cảm biến
vào mô hình ngôn ngữ và cho phép mô hình xử lý nhiều
hình ảnh trong các câu văn bản theo cách linh hoạt. BLIP-2
[33] đề xuất một Transformer Truy vấn nhẹ để tận dụng bộ
mã hóa hình ảnh và mô hình ngôn ngữ được huấn luyện
trước đông lạnh cho các tác vụ đa phương thức.

2.2. Tinh chỉnh Hiệu quả Tham số
Gần đây thành công của việc mở rộng quy mô mô hình
khuyến khích sự phát triển của các mô hình ngôn ngữ lớn
hơn [7, 10, 44, 52]. Trong khi đó, tinh chỉnh hiệu quả tham
số [3, 22, 24, 42, 45, 51, 52] nhằm khám phá một giải pháp
hiệu quả hơn để thích ứng các mô hình lớn với các tác vụ
hạ nguồn cụ thể. Thay vì tinh chỉnh mô hình đầy đủ cập
nhật toàn bộ tập tham số mô hình, tinh chỉnh hiệu quả
tham số cập nhật hoặc thêm một số lượng tương đối nhỏ
tham số và để lại phần còn lại của tham số mô hình cố
định [52]. FISH Mask[51] áp dụng một mặt nạ thưa thớt cố
định trên các tham số mô hình và chỉ cập nhật các tham số
được chọn bởi mặt nạ. Adapter [3, 22, 42, 45] chèn các lớp
dày đặc mới có thể huấn luyện vào Transformer và để lại
các tham số mô hình gốc đông lạnh. Prefix-tuning [34] và
prompt-tuning [31] đông lạnh tham số của mô hình và học
các prompts liên tục. LoRA [24] tiêm các ma trận phân rã
hạng thấp có thể huấn luyện vào mọi lớp của Transformer
và đông lạnh các tham số mô hình ngôn ngữ được huấn
luyện trước. Cụ thể, LoRA cho thấy khả năng xuất sắc để
đạt được hiệu suất cạnh tranh hoặc thậm chí tốt hơn so với
tinh chỉnh chỉ với 0.1% tham số có thể huấn luyện [24, 57]

2.3. Hỗn hợp Chuyên gia cho Học Đa tác vụ
Các kiến trúc Hỗn hợp Chuyên gia (MoE) tập trung vào
việc tăng cường khả năng tính toán có điều kiện và mở
rộng tham số trong các kiến trúc mạng thần kinh như
Transformer. Các mô hình transformer MoE [17, 29, 46,
61] thường sử dụng N mạng feed-forward, được gọi là
"chuyên gia". Mỗi chuyên gia này có bộ trọng số có thể
huấn luyện riêng biệt, cho phép chúng tạo ra các biểu diễn
riêng biệt cho mỗi token đầu vào dựa trên thông tin ngữ
cảnh. Học đa tác vụ (MTL), một chủ đề ML phổ biến trong
nhiều năm, nhằm tìm giải pháp để đồng thời cải thiện hiệu
suất trên nhiều tác vụ quan tâm [6, 35]. Gần đây, các
phương pháp hỗn hợp chuyên gia (MoE) [25, 26, 47] đã
trở thành một phương pháp tiềm năng cho MTL [16], được
hưởng lợi từ chiến lược tách biệt không gian tham số và
áp dụng phần mô hình liên quan cho các tác vụ khác nhau.

Được truyền cảm hứng từ những tiến bộ này, có sự quan
tâm ngày càng tăng trong việc điều tra ứng dụng của MoE
trong các mô hình lớn dựa trên Transformer. Một số
phương pháp áp dụng MoE trong cấu trúc Transformer của
các mô hình ngôn ngữ lớn [15, 18, 30, 48]. Gshard [30] giới
thiệu ý tưởng mở rộng Transformer trong LMM với các
lớp MoE, trong đó lớp feed forward của mọi Transformer
khác được thay thế bằng một lớp MoE Sparsely-Gated.
Cấu trúc MoE Transformer này sau đó được sử dụng trong
[15] để phát triển một họ các mô hình ngôn ngữ Decoder-
only, và [48] phát hiện ra rằng các mô hình LLM được sửa
đổi MoE được hưởng lợi nhiều hơn từ việc tinh chỉnh
hướng dẫn so với LLM dày đặc.

Một số phương pháp khác khám phá việc kết hợp MoE
với tinh chỉnh hiệu quả tham số. AdaMix [56] đề xuất một
cơ chế hỗn hợp adapter để cải thiện hiệu suất tinh chỉnh
theo từng tác vụ. Công trình liên quan nhất là nghiên cứu
đồng thời [60] giới thiệu hỗn hợp LoRA bằng cách tổng
có trọng số của các đầu ra LoRA khác nhau. Mặc dù tương
tự về mặt khái niệm, phương pháp SMoLA của chúng tôi
khác biệt bằng cách có chi phí tính toán thấp hơn đáng kể,
và cũng cho phép hàng trăm chuyên gia xử lý một và nhiều
phương thức với chi phí tốc độ suy luận không đáng kể.
Chúng tôi phát hiện rằng việc mở rộng đến hàng trăm
chuyên gia là quan trọng để đạt được hiệu suất tổng quát
được cải thiện.

3. Phương pháp
3.1. Kiến thức Cơ bản
Thích ứng Hạng thấp (LoRA). Thích ứng Hạng thấp
(LoRA) [23] là một kỹ thuật được thiết kế để tăng cường
khả năng thích ứng của các mô hình transformer được
huấn luyện trước với các tác vụ mới với sự gia tăng nhỏ
trong số lượng tham số có thể huấn luyện. Nó có thể được
áp dụng trên bất kỳ lớp tuyến tính nào, cung cấp khả năng
tương thích lớn với các mô hình lớn gần đây.

Chúng tôi ký hiệu W∈Rd1×d2 như ma trận trọng số cho
một lớp tuyến tính từ mô hình lớn. LoRA giới thiệu hai ma
trận hạng thấp Win∈Rr×d1 và Wout∈Rd2×r cho mỗi lớp,
trong đó r≪min{d1,d2}. Win và Wout được áp dụng liên
tiếp vào đầu vào của lớp tuyến tính để chiếu đầu vào vào
một không gian hạng thấp và sau đó chiếu trở lại không
gian đầu ra. Các trọng số được thích ứng W′ có thể được
biểu diễn như W′=W+WoutWin. Vì hạng của Win và Wout
bị giới hạn bởi r và thường nhỏ hơn nhiều so với d1 và d2,
phương pháp LoRA phục vụ như một cơ chế thích ứng
nhỏ gọn và hiệu quả.

Hỗn hợp Mềm Chuyên gia (Soft MoE). Chúng tôi tóm tắt
ngắn gọn mô hình Soft MoE trong phần này (chi tiết có thể
tìm thấy trong [43]). Ý tưởng cốt lõi là học một mô-đun
điều phối có thể điều phối các token đầu vào đến các
chuyên gia khác nhau, và một mô-đun kết hợp có thể kết
hợp kết quả từ tất cả các chuyên gia và chiếu chúng trở lại
không gian token gốc.

Chúng tôi ký hiệu đầu vào của khối transformer là X∈
RN×d1, bao gồm N token. Soft MoE giới thiệu một ma trận
định tuyến Φ∈RE×d1 tương ứng với E chuyên gia. Bộ
điều phối và kết hợp được biểu diễn bởi Eq. 1 và 2: norm
ký hiệu chuẩn hóa l2 và α là một vô hướng có thể học.

D=softmax (α·norm (Φ)norm (X)T,axis=1 )(1)
C=softmax (α·norm (Φ)norm (X)T,axis=0 )(2)

Mỗi mô hình chuyên gia fi (thường là Khối MLP) hoạt
động trên lát cắt tương ứng của đầu vào được điều phối
˜xi= (DX)i,: để tạo ra ˜yi=fi(˜xi). Sau đó, bộ kết hợp C
chiếu đầu ra ˜Y= [˜y0,˜y1, ...˜yE-1] vào không gian token
Y=CT˜Y.

3.2. Khối SMoLA
Thiết kế MoE thông thường sử dụng các chuyên gia hạng
cao trong các khối MLP của chúng mà trực tiếp học xử lý
các đầu vào khác nhau. Do đó, các chuyên gia này nặng về
tham số và yêu cầu huấn luyện trước đắt đỏ. Phương pháp
SMoLA dựa vào việc thêm (vào một mô hình cơ sở ban
đầu được ký hiệu là θ∗) các chuyên gia sử dụng kiến trúc
Soft MoE, trong khi đồng thời tránh tăng đáng kể số lượng
tham số bằng cách kết hợp mềm nhiều chuyên gia hạng
thấp được khởi tạo bằng không. Một cách trực quan, mô
hình cơ sở ban đầu θ∗ phục vụ như một xương sống nền
tảng, và các chuyên gia hạng thấp bổ sung phục vụ như
"chuyên gia" thu thập kiến thức chuyên môn bổ sung và
xử lý các trường hợp sử dụng khác nhau.

Mô hình cơ sở θ∗ có thể được khởi tạo với các checkpoint
được huấn luyện trước (thô), được tinh chỉnh đa tác vụ,
hoặc được tinh chỉnh hướng dẫn. Sử dụng checkpoint thô
cung cấp một xương sống tổng quát hơn, trong khi check-
point được tinh chỉnh đa tác vụ cung cấp một xương sống
tập trung vào một bộ kỹ năng yêu cầu của các tác vụ liên
quan – chúng tôi coi quyết định có sử dụng cái này hay cái
khác làm xương sống là phụ thuộc vào ứng dụng. Lựa
chọn của chúng tôi cho Soft MoE [43] để khởi tạo khối
SMoLA xuất phát từ các đặc tính mong muốn mà kiến trúc
này thể hiện: có thể vi phân hoàn toàn, không có việc loại
bỏ token, và không có vấn đề cân bằng chuyên gia.

Phần bên phải của Hình 1 trình bày một khối SMoLA.
SMoLA hoạt động trên các lớp tuyến tính để có độ linh
hoạt và tương thích tối đa. Chúng tôi ký hiệu W∗
(W∗∈Rd1×d2) như ma trận trọng số của một lớp tuyến
tính trong mô hình cơ sở θ∗ và X∈RN×d1 như đầu vào với
N token. Theo [43], chúng tôi giới thiệu ma trận định tuyến
Φ∈RE×d1 và tính toán bộ điều phối D∈RE×N và bộ kết
hợp C∈RE×N sử dụng Eq. 1 và 2 cho E chuyên gia.

SMoLA áp dụng một phương pháp lấy cảm hứng từ LoRA
cho các khối chuyên gia. Chúng tôi giới thiệu các ma trận
hạng thấp có thể huấn luyện Wout
i,Win
i cho chuyên gia thứ
i, tạo ra đầu ra ˜yi như trong Eq. 3.

˜yi=Wout
iWin
i(DX)T
i,: (3)

Sau đó, đầu ra của SMoLA Y kết hợp các đầu ra của mỗi
chuyên gia và các đầu ra tuyến tính ban đầu, như trong
Eq. 4.

Y=XW∗+CT[˜y0,˜y1, ...˜yE-1] (4)

3.3. Omni-SMoLA
Theo mặc định, các khối SMoLA nhận làm đầu vào tất cả
các token, bất kể phương thức của chúng (được ký hiệu
bởi SMoLA MM trong phần tiếp theo). Tuy nhiên, chúng tôi
lưu ý rằng các tác vụ đa phương thức khác nhau có thể đặt
trọng tâm khác nhau vào cách các phương thức khác nhau
được sử dụng. Ví dụ, tạo chú thích hình ảnh dựa nhiều
hơn vào các token thị giác, các tác vụ VQA trên hình ảnh
nặng văn bản và sử dụng OCR ngược dòng tập trung nhiều
hơn vào văn bản, trong khi VQA hình ảnh tự nhiên phải
dựa vào cả token thị giác và văn bản.

Được truyền cảm hứng từ [55], SMoLA có thể được cấu
hình một cách liền mạch để chỉ thích ứng token cho các
phương thức được chọn. Chúng tôi ký hiệu các khối
SMoLA chỉ nhận token thị giác hoặc token văn bản là
SMoLA V hoặc SMoLA T, tương ứng. SMoLA MM ám chỉ
các khối SMoLA nhận cả token thị giác và văn bản. Như
được hiển thị trong Hình 1, Omni-SMoLA (được ký hiệu
bởi SMoLA O trong phần tiếp theo) kết hợp thông qua tổng
các đầu ra xương sống ban đầu với các đầu ra của SMoLA
MM và các đầu ra được nối của SMoLA V và SMoLA T.

3.4. Các Đặc tính của Omni-SMoLA
Hiệu quả Tham số và Độ phức tạp Thời gian. Việc tích
hợp LoRA và Soft MoE dẫn đến một sự kết hợp đạt được
một sự giảm đáng kể trong số lượng tham số cần thiết để
thích ứng, so với MoE truyền thống [30]. Các ma trận hạng
thấp được giới thiệu bởi LoRA có chiều thấp hơn đáng kể
so với các ma trận feedforward hạng đầy đủ, đảm bảo
rằng sự gia tăng tham số là tối thiểu (và có thể kiểm soát
thông qua siêu tham số hạng). Điều này không chỉ dẫn
đến một mô hình gọn hơn, mà còn giảm yêu cầu bộ nhớ,
làm cho việc tăng số lượng chuyên gia để tăng cường hiệu
suất trở nên khả thi.

Hơn nữa, chi phí suy luận của việc áp dụng Omni-SMoLA
là không đáng kể. Cho dmax ký hiệu max{d1,d2} và r ký
hiệu hạng mỗi chuyên gia, độ phức tạp thời gian của các
khối SMoLA mỗi lớp là O(ENdmax+E(d1+d2)r). Đối với
một lớp đơn, nó tăng chi phí từ O(Nd1d2) đến O(Nd1d2+
ENdmax+E(d1+d2)r). Số lượng chuyên gia E luôn nhỏ
hơn nhiều so với min{d1,d2}, trong khi hạng r (thường là
một số nguyên nhỏ như 4) nhỏ hơn nhiều so với độ dài
token đầu vào, đặc biệt là cho các cài đặt đa phương thức
nơi một hình ảnh độ phân giải cao duy nhất có thể dễ dàng
chịu trách nhiệm cho hàng nghìn token thị giác.

Chiều Mở rộng Thay thế. Các phương pháp mở rộng
truyền thống trong mạng thần kinh thường liên quan đến
việc tăng kích thước của mô hình, hoặc bằng cách thêm
nhiều lớp hơn hoặc tăng chiều của các lớp hiện có. Mặt
khác, phương pháp được đề xuất giới thiệu một chiều mở
rộng thay thế. Bằng cách tận dụng kích hoạt thưa thớt và
thích ứng hiệu quả tham số, phương pháp được đề xuất
đạt được mở rộng thông qua việc tăng số lượng chuyên
gia hạng thấp, mà lần lượt không dẫn đến sự gia tăng
nghiêm trọng của tổng kích thước tham số mô hình.

Khả năng Mở rộng cho Tăng trưởng Tương lai. Thiết kế
của phương pháp được đề xuất vốn dĩ hỗ trợ khả năng mở
rộng, đáp ứng tăng trưởng và thích ứng tương lai một cách
dễ dàng. Khi các yêu cầu của một tác vụ phát triển, các
mô-đun chuyên gia hạng thấp bổ sung có thể được tích
hợp một cách liền mạch vào kiến trúc, tăng cường khả
năng của mô hình mà không cần thiết phải đại tu hoàn
toàn. Điều này trái ngược hoàn toàn với các phương pháp
mở rộng truyền thống, thường yêu cầu các chiều và số
lượng lớp được xác định trước, hạn chế khả năng thích
ứng của mô hình với các tình huống thay đổi.

4. Thí nghiệm
4.1. Thiết lập Thí nghiệm
Hỗn hợp Huấn luyện. Chúng tôi xem xét ba hỗn hợp:
•Hỗn hợp Tạo chú thích Hình ảnh : COCO captions1[27] ,
Textcaps [49], VizWiz-Cap [21].
•Hỗn hợp VQA : VQAv22[19], OK-VQA [37], VizWiz-
VQA [20], ST-VQA [4], TextVQA [50], OCRVQA [41],
InfoVQA [40], DocVQA [39], ChartQA [38], AI2D [28].
•Hỗn hợp Đầy đủ : kết hợp hỗn hợp Tạo chú thích Hình
ảnh và hỗn hợp VQA.

Theo mặc định, chúng tôi sử dụng hỗn hợp đầy đủ trong
các thí nghiệm của chúng tôi để mô phỏng tình huống kết
hợp một loạt rộng các tác vụ khác nhau. Ngoại lệ duy nhất
là Phần 4.3.6, nơi chúng tôi đo lường tác động của việc sử
dụng các hỗn hợp tập trung hơn.

Prompt Tác vụ. Chúng tôi không sử dụng các prompt cụ
thể theo chuẩn mực để đạt được tính linh hoạt tốt hơn của
các mô hình tổng quát. Theo [7] và [8], chúng tôi sử dụng
Generate the alttext in {lang}at 0: như prompt tạo chú
thích và Answer in en: {question }như prompt VQA.

1Để phù hợp với bản chất đa ngôn ngữ của các mô hình PaLI, ở đây
chúng tôi đã sử dụng một biến thể của chú thích COCO chỉ tiếng Anh gốc
bao gồm các chú thích được dịch cho 35 ngôn ngữ bổ sung.
2Bao gồm các câu hỏi được dịch cho 13 ngôn ngữ bổ sung.

--- TRANG 5 ---
[Bảng 1 với các kết quả thí nghiệm]

--- TRANG 6 ---
[Bảng 2 với các kết quả thí nghiệm tiếp theo]

--- TRANG 7 ---
[Bảng 3 và 4 với các nghiên cứu ablation]

--- TRANG 8 ---
[Bảng 5 và 6 với các kết quả bổ sung]

--- TRANG 9 ---
Tài liệu Tham khảo
[Danh sách 61 tài liệu tham khảo với các chi tiết xuất bản]

--- TRANG 10 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 11 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 12 ---
[Phụ lục A và B với các nghiên cứu bổ sung]
