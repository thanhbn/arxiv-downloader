# Tinh chỉnh tham số hiệu quả nhạy cảm cho thị giác

## Tóm tắt

Tinh chỉnh tham số hiệu quả cho thị giác (PEFT) đã trở thành một giải pháp thay thế mạnh mẽ cho việc tinh chỉnh toàn bộ để thích ứng các mô hình thị giác được huấn luyện trước với các tác vụ hạ nguồn, chỉ điều chỉnh một số lượng nhỏ tham số trong khi đóng băng phần lớn còn lại để giảm gánh nặng lưu trữ và khó khăn tối ưu hóa. Tuy nhiên, các phương pháp PEFT hiện tại đưa các tham số có thể huấn luyện vào cùng các vị trí trên các tác vụ khác nhau chỉ dựa vào kinh nghiệm thực tế của con người và bỏ qua khoảng cách miền. Để giải quyết vấn đề này, chúng tôi nghiên cứu nơi nào để đưa vào và cách phân bổ các tham số có thể huấn luyện bằng cách đề xuất một sơ đồ Tinh chỉnh tham số hiệu quả nhạy cảm cho thị giác (SPT) mới, phân bổ thích ứng các tham số có thể huấn luyện vào các vị trí quan trọng cụ thể cho tác vụ với ngân sách tham số có thể điều chỉnh mong muốn. Cụ thể, SPT của chúng tôi đầu tiên nhanh chóng xác định các tham số nhạy cảm cần điều chỉnh cho một tác vụ nhất định theo cách phụ thuộc dữ liệu. Tiếp theo, SPT của chúng tôi tiếp tục tăng cường khả năng biểu diễn cho các ma trận trọng số có số lượng tham số nhạy cảm vượt quá ngưỡng được định trước bằng cách sử dụng các phương pháp điều chỉnh có cấu trúc hiện có, ví dụ LoRA hoặc Adapter, để thay thế việc điều chỉnh trực tiếp các tham số nhạy cảm đã chọn (điều chỉnh không có cấu trúc) trong ngân sách. Các thí nghiệm rộng rãi trên nhiều tác vụ nhận dạng hạ nguồn cho thấy SPT của chúng tôi bổ sung cho các phương pháp PEFT hiện có và cải thiện đáng kể hiệu suất của chúng, ví dụ SPT cải thiện Adapter với backbone ViT-B/16 được huấn luyện trước có giám sát lần lượt 4.2% và 1.4% độ chính xác Top-1 trung bình, đạt hiệu suất SOTA trên các benchmark FGVC và VTAB-1k.

## 1. Giới thiệu

Để thích ứng hiệu quả các biểu diễn được huấn luyện trước với các tác vụ hạ nguồn, lựa chọn thực tế là tinh chỉnh toàn bộ, khởi tạo mô hình với các trọng số được huấn luyện trước và điều chỉnh tất cả các tham số. Tuy nhiên, tinh chỉnh toàn bộ thông thường cần lưu trữ một phiên bản tham số riêng biệt cho mỗi tác vụ và mỗi kịch bản triển khai. Điều này có thể cực kỳ tốn kém về mặt lưu trữ vì chi phí lưu trữ tăng tuyến tính với số lượng trường hợp có thể, xem xét có nhiều loại tác vụ hạ nguồn và môi trường triển khai động, đặc biệt khi triển khai các mô hình thị giác lớn lên hệ thống di động. Ví dụ, chỉ việc lưu trữ một mô hình ViT-H được huấn luyện trước lớn duy nhất trên đĩa cục bộ đã yêu cầu ít nhất 2.3GB, trong khi Top-10 ứng dụng Mỹ chỉ yêu cầu tổng cộng 2.2GB vào tháng 5/2021.

Đáng chú ý, một giải pháp mới nổi là thay thế tinh chỉnh thông thường bằng Tinh chỉnh tham số hiệu quả cho thị giác (PEFT), chỉ điều chỉnh một số lượng nhỏ tham số có thể huấn luyện trong khi đóng băng phần lớn còn lại được chia sẻ bởi nhiều tác vụ. Vì các phương pháp PEFT thể hiện ít hơn 1% tham số có thể huấn luyện, gánh nặng lưu trữ được giảm đáng kể. Một thuộc tính hấp dẫn khác của PEFT là điều chỉnh ít tham số hơn giúp giảm khó khăn tối ưu hóa và giảm thiểu vấn đề quá khớp khi thích ứng các mô hình được huấn luyện trước lớn trên tập dữ liệu đích, từ đó đạt được hiệu suất tương đương hoặc thậm chí tốt hơn so với tinh chỉnh thông thường. Mặc dù đầy hứa hẹn, các phương pháp PEFT hiện tại đưa các tham số có thể huấn luyện vào cùng các vị trí cho tất cả các tác vụ hạ nguồn, dựa vào kinh nghiệm thực tế của con người và bỏ qua khoảng cách miền và đặc điểm cụ thể cho tác vụ, điều này hạn chế hiệu suất của chúng. Ví dụ, theo cách không phụ thuộc tác vụ, Prompt Tuning-deep và Adapter tương ứng thêm các tham số có thể huấn luyện vào các lớp tự chú ý nhiều đầu và mạng feed-forward cho tất cả các tác vụ khác biệt như được mô tả trong Hình 1 (a).

Để giải quyết thách thức cơ bản này, chúng tôi khám phá nơi nào để đưa vào và cách phân bổ các tham số có thể huấn luyện dưới ngân sách tham số mong muốn bằng cách trình bày một sơ đồ Tinh chỉnh tham số hiệu quả nhạy cảm cho thị giác (SPT) mới xác định các vị trí quan trọng cụ thể cho tác vụ để phân bổ thích ứng các tham số có thể huấn luyện. Vì các trọng số được huấn luyện trước tại các vị trí khác nhau có những đóng góp khác nhau cho các tác vụ hạ nguồn khác nhau, chúng tôi đầu tiên đề xuất một tiêu chí mới để nhanh chóng xác định các tham số nhạy cảm cụ thể cho tác vụ cần điều chỉnh theo cách phụ thuộc dữ liệu. Lấy cảm hứng từ các số liệu cắt tỉa mô hình, chúng tôi đề xuất đo lường độ nhạy cảm tham số với việc giảm mất mát khi được điều chỉnh, có thể được xấp xỉ bằng khai triển Taylor bậc một được rút ra trong một lượt truyền xuôi và truyền ngược duy nhất trước khi tinh chỉnh trong một lần. Tiêu chí độ nhạy cảm của chúng tôi đơn giản và hiệu quả, có thể xác định các vị trí quan trọng cụ thể cho tác vụ để đưa các tham số có thể huấn luyện cho bất kỳ backbone nào một cách nhanh chóng. Ví dụ, tính toán độ nhạy cảm cho backbone ViT-B/16 chỉ mất 5.5 giây với một GPU duy nhất trên bất kỳ tập dữ liệu VTAB-1k nào.

Với tiêu chí của chúng tôi, chúng tôi quan sát thực nghiệm rằng tỷ lệ của các tham số nhạy cảm cho mỗi khối thực sự thay đổi đáng kể giữa các tác vụ khác nhau trong Phần 4.4. Để phân bổ các tham số có thể huấn luyện dưới ngân sách tham số có thể huấn luyện mong muốn, một giải pháp trực quan là điều chỉnh trực tiếp các kết nối trọng số nhạy cảm nhất, mà chúng tôi gọi là điều chỉnh không có cấu trúc. Mặc dù đơn giản và linh hoạt, điều chỉnh không có cấu trúc chỉ điều chỉnh một vài tham số vẫn thiếu khả năng biểu diễn và khó khăn trong việc thu hẹp khoảng cách miền. Để giải quyết vấn đề này, chúng tôi đề xuất kết hợp thêm điều chỉnh có cấu trúc để thay thế điều chỉnh không có cấu trúc tại các ma trận trọng số nhạy cảm có số lượng tham số nhạy cảm vượt quá ngưỡng được định trước để cải thiện khả năng biểu diễn dưới ngân sách tham số tương tự. Điều chỉnh có cấu trúc có thể được thực hiện bằng bất kỳ phương pháp điều chỉnh có cấu trúc hiệu quả tham số nào hiện có, ví dụ chèn một mô-đun adapter tuần tự sau các ma trận trọng số nhạy cảm. Do đó, SPT của chúng tôi kết hợp thích ứng cả mức độ điều chỉnh không có cấu trúc và có cấu trúc và phân bổ các tham số có thể huấn luyện với tính linh hoạt cao và khả năng biểu diễn cho mỗi tác vụ hạ nguồn riêng biệt.

Bài báo này có những đóng góp chính sau. 1) Chúng tôi thực hiện khám phá tiên phong để xác định các vị trí quan trọng cụ thể cho tác vụ trong thiết lập PEFT, nhanh chóng, hiệu quả, linh hoạt để áp dụng cho các backbone khác nhau với các chiến lược huấn luyện trước khác nhau, và trực giao với các phương pháp PEFT hiện có. 2) Dựa trên tiêu chí độ nhạy cảm, chúng tôi đề xuất một chiến lược phân bổ tham số có thể huấn luyện kết hợp thích ứng cả điều chỉnh không có cấu trúc và có cấu trúc dưới ngân sách tham số mong muốn để đạt được tính linh hoạt cao, dung lượng lớn và sự cân bằng thuận lợi giữa hiệu quả tham số và độ chính xác. 3) Các thí nghiệm rộng rãi trên tổng cộng 24 tác vụ nhận dạng hạ nguồn với cả backbone vision Transformer đơn giản và phân cấp dưới các huấn luyện trước có giám sát và tự giám sát cho thấy SPT của chúng tôi bổ sung cho các phương pháp PEFT hiện có và cải thiện hiệu suất của chúng với biên độ lớn. Ví dụ, SPT cải thiện Adapter lên 4.2% độ chính xác Top-1 trung bình, vượt trội so với các phương pháp PEFT SOTA trên benchmark FGVC.
