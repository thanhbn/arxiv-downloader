# 2111.01998.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/peft/2111.01998.pdf
# File size: 303272 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
OpenPrompt: An Open-source Framework for Prompt-learning
Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen,
Zhiyuan Liuy,Hai-Tao Zhengy,Maosong Sun
Tsinghua University, Bejing, China
fdingn18, hsd20, zwl19, yl-chen21 g@mails.tsinghua.edu.cn
Abstract
Prompt-learning has become a new paradigm
in modern natural language processing, which
directly adapts pre-trained language models
(PLMs) to cloze -style prediction, autoregres-
sive modeling, or sequence to sequence gen-
eration, resulting in promising performances
on various tasks. However, no standard im-
plementation framework of prompt-learning
is proposed yet, and most existing prompt-
learning codebases, often unregulated, only
provide limited implementations for speciﬁc
scenarios. Since there are many details such as
templating strategy, initializing strategy, and
verbalizing strategy, etc. need to be considered
in prompt-learning, practitioners face impedi-
ments to quickly adapting the desired prompt
learning methods to their applications. In this
paper, we present OpenPrompt, a uniﬁed easy-
to-use toolkit to conduct prompt-learning over
PLMs. OpenPrompt is a research-friendly
framework that is equipped with efﬁciency,
modularity, and extendibility, and its combin-
ability allows the freedom to combine different
PLMs, task formats, and prompting modules
in a uniﬁed paradigm. Users could expediently
deploy prompt-learning frameworks and eval-
uate the generalization of them on different
NLP tasks without constraints. OpenPrompt is
publicly released at https://github.com/
thunlp/OpenPrompt .
1 Introduction
Pre-trained language models (PLMs) (Han et al.,
2021a; Qiu et al., 2020) have been widely proven
to be effective in natural language understanding
and generation, ushering in a new era of modern
natural language processing (NLP). In the early
stage of this revolution, a standard approach to
adapt PLMs to various speciﬁc NLP tasks is the
equal contribution
ycorresponding authorspretraining-ﬁnetuning paradigm, where additional
parameters and task-speciﬁc objectives are intro-
duced in the tuning procedure. However recently,
the paradigm of the adaptation of PLMs is shifting.
Originated in T5 (Raffel et al., 2019) and GPT-
3 (Brown et al., 2020), researchers ﬁnd that PLMs
can be effectively stimulated by textual prompts or
demonstrations, especially in low-data scenarios.
Take a simple prompt-based sentiment classiﬁ-
cation for example, the pipeline consists of a tem-
plate and a verbalizer, where a template is used to
process the original text with some extra tokens,
and a verbalizer projects original labels to words
in the vocabulary for ﬁnal prediction. Assume
the template is “ <text> It is<mask> ”, where
the token <text> stands for the original text,
and the verbalizer is f“positive”:“great”, “neg-
ative”:“terrible” g. The sentence “ Albert Einstein
was one of the greatest intellects of his time. ” will
ﬁrst be wrapped by the pre-deﬁned template as “ Al-
bert Einstein was one of the greatest intellects of
his time. It is <mask> ”. The wrapped sentence is
then tokenized and fed into a PLM to predict the
distribution over vocabulary on the <mask> token
position. It is expected that the word great should
have a larger probability than terrible .
As illustrated above, prompt-learning projects
the downstream tasks to pre-training objectives
for PLMs with the help of textual or soft-
encoding prompts. A series of studies of prompt-
learning (Liu et al., 2021a) have been proposed
to investigate the strategies of constructing tem-
plates (Schick and Sch ¨utze, 2021; Gao et al., 2021;
Liu et al., 2021b), verbalizers (Hu et al., 2021), op-
timization (Lester et al., 2021), and application (Li
and Liang, 2021; Han et al., 2021b; Ding et al.,
2021a) for this paradigm.
A prompt-learning problem could be regarded
as a synthesis of PLMs, human prior knowledge,
and speciﬁc NLP tasks that need to be handled.arXiv:2111.01998v1  [cs.CL]  3 Nov 2021

--- PAGE 2 ---
Example PLM Template Verbalizer Task Reference
Naive TC MLM & Seq2Seq M. text M. One-Many Text Classiﬁcation -
Naive KP LM & Seq2Seq M. text - Knowledge Probing -
Naive FET MLM M. text (meta info) M. One-Many Entity Typing (Ding et al., 2021a)
PTR MLM M. text (complex) M. One-One Relation Extratcion (Han et al., 2021b)
P-tuning LM Soft tokens M. One-One Text Classiﬁcation (Liu et al., 2021b)
Preﬁx-tuning LM, Seq2Seq Soft tokens - Text Generation (Li and Liang, 2021)
LM-BFF MLM A. text M. One-Many Text Classiﬁcation (Gao et al., 2021)
Table 1: Some examples implemented by OpenPrompt, where M. is the abbreviation of manually deﬁned
and A. is the abbreviation of automatically generated. Note that different approaches focus on different parts
in prompt-learning. Additional to the whole pipeline, our speciﬁc implementations of these methods are in-
tegrated into the speciﬁc classes of OpenPrompt. For example, the core implementation of KPT is in the
KnowledgeableVerbalizer class.
Hence, it is hard to support the particular implemen-
tations of prompt-learning elegantly with the cur-
rent deep learning or NLP libraries while there is
also a lack of a standard paradigm. Previous works
pursue the most efﬁcient way to implement prompt-
learning with the least modiﬁcation to the existing
framework for traditional ﬁne-tuning, resulting in
poor readability and even unstable reproducibility.
Moreover, the performance of a prompt-learning
pipeline varies greatly with the choice of templates
and verbalizers (Zhao et al., 2021), creating more
barriers for implementations. Lastly, there is no
comprehensive open-source framework particularly
designed for prompt-learning at present, which
makes it difﬁcult to try out new methods and make
rigorous comparisons for previous approaches.
To this end, we present OpenPrompt, an open-
source, easy-to-use, and extensible toolkit for
prompt-learning. OpenPrompt modularizes the
whole framework of prompt-learning and considers
the interactions between each module. We high-
light the feature of combinability of OpenPrompt,
which supports ﬂexible combinations of diverse
task formats, PLMs, and prompting modules. For
example, we can easily adapt preﬁx-tuning (Li and
Liang, 2021) to a text classiﬁcation task in Open-
Prompt. This feature enables users to assess the
generalization of their prompt-learning models on
various tasks, but not only the performance on spe-
ciﬁc tasks.
Speciﬁcally, in OpenPrompt, a Template class
is used to deﬁne or generate textual or soft-
encoding templates to wrap the original input. To
ﬂexibly support various templates under a uniﬁed
paradigm, we design a new template language that
could easily conduct token-level customization for
the corresponding attributes. For example, users
can specify which tokens are shared embedding,trainable, or in what way these tokens are to be
post-processed, without having to perform com-
plex implementations for speciﬁc templates. A
Verbalizer projects the classiﬁcation labels to
words in the vocabulary, and a PromptModel
is responsible for the training and inference pro-
cess. Each module in OpenPrompt is clearly de-
ﬁned while retaining its independence and coupling
so that researchers can easily deploy a model and
make targeted improvements. We also implement
baselines with OpenPrompt and evaluate them on
a broad scope of NLP tasks, demonstrating the ef-
fectiveness of OpenPrompt.
The area of prompt-learning is in the exploratory
stage with rapid development. Hopefully, Open-
Prompt could help beginners quickly understand
prompt-learning, enable researchers to efﬁciently
deploy prompt-learning research pipeline, and em-
power engineers to readily apply prompt-learning
to practical NLP systems to solve real-world prob-
lems. OpenPrompt will not only open source all
the code, but will also continue to update the docu-
mentation to provide detailed tutorials.
2 Background
Prompt-learning reveals what the next generation
of NLP may look like.
Although PLMs have achieved tremendous suc-
cess on almost all the subtasks in NLP, one problem
still hangs in the air, have we really fully exploited
the potential of PLMs, especially the big ones?
Conventional ﬁne-tuning uses extra task-speciﬁc
heads and objectives for adaptation, but this strat-
egy may face two issues. On the one hand, such
an approach creates a natural gap between model
tuning and pre-training. On the other hand, as the
number of model parameters increases, this ﬁne-
tuning approach becomes increasingly difﬁcult to

--- PAGE 3 ---
operate due to the massive computational volume
(e.g., GPT-3 (Brown et al., 2020)).
By mimicking the process of pre-training,
prompt-learning intuitively bridges the gap be-
tween pre-training and model tuning. Practically,
this paradigm is surprisingly effective in low-data
regime (Le Scao and Rush, 2021; Gao et al., 2021).
For example, with appropriate template, zero-shot
prompt-learning could even outperform 32-shot
ﬁne-tuning (Ding et al., 2021a). Another promising
empirical attribute of prompt-learning is the poten-
tial to stimulate large-scale PLMs. When it comes
to a 10B model, solely optimizing prompts (the
parameters of the model are ﬁxed) could achieve
comparable performance to full parameter ﬁne-
tuning (Lester et al., 2021). These practical studies
imply that we may use prompts to more effectively
and efﬁciently dig the knowledge kept in PLMs,
leading to a deeper understanding of the underlying
principles of their mechanisms (Wei et al., 2021;
Qin et al., 2021; Vu et al., 2021).
From a practical implementation point of view,
prompt-learning is actually complex and requires a
lot of detailed consideration. With general-purpose
NLP under the prompt-learning paradigm as our
target, we present OpenPrompt, a uniﬁed toolkit
to effectively and efﬁciently implement prompt-
learning approaches. OpenPrompt demonstrates
a comprehensive view of the programming de-
tails of prompt-learning, and enables practition-
ers to quickly understand the mechanisms and
practical attributes of this technique. And one
can quickly deploy existing representative prompt-
learning algorithms that are already implemented
in the package under a uniﬁed programming frame-
work. Moreover, OpenPrompt allows researchers
or developers to quickly try out new ideas of
prompt-learning, which not only includes newly
designed templates or verbalizers, but also the ex-
ploration of the attributes of prompt-learning, e.g.,
prompt-based adversarial attacking.
3 Design and Implementation
As stated in §1, prompt-learning is a comprehen-
sive process that combines PLMs, human knowl-
edge, and speciﬁc NLP tasks. Keeping that in mind,
the design philosophy is to simultaneously consider
the independence and mutual coupling of each mod-
ule. As illustrated in Figure 1, OpenPrompt pro-
vides the full life-cycle of prompt-learning based
on PyTorch (Paszke et al., 2019). In this section, weﬁrst introduce the combinability of OpenPrompt,
and then the detailed design and implementation of
each component in OpenPrompt.
3.1 Combinability
In the NLP world, we usually adopt different PLMs
with corresponding objective functions to differ-
ent underlying tasks (roughly, classiﬁcation and
generation). But in prompt learning, given that
the core idea of the framework is to mimic pre-
training tasks in the downstream task, which are
essentially ”predicting words based on context”,
we can further unify the execution of downstream
tasks. OpenPrompt supports a combination of tasks
(classiﬁcation and generation), PLMs (MLM, LM
and Seq2Seq), and prompt modules (different tem-
plates and verbalizers) in a ﬂexible way. For exam-
ple, from a model perspective, T5 (Raffel et al.,
2019) is not only used for span prediction and
GPT (Brown et al., 2020) is not only used for gen-
erative tasks. From the perspective of prompting,
preﬁx-tuning can also be used for classiﬁcation,
and soft prompt can be used for generation. All
these combinations can easily be implemented and
validated on NLP tasks in our framework so that
we can better understand the mechanisms involved.
3.2 Pre-trained Language Models
One core idea of prompt-learning is to use addi-
tional context with masked tokens to imitate the
pre-training objectives of PLMs and better stimu-
late these models. Hence, the choice of PLMs is
crucial to the whole pipeline of prompt-learning.
PLMs could be roughly divided into three groups
according to their pre-training objectives.
The ﬁrst group of PLMs use masked language
modeling (MLM) to reconstruct a sequence cor-
rupted by random masked tokens, where only the
losses of the masked tokens are computed. Typical
PLMs with MLM objective include BERT (Devlin
et al., 2019), RoBERTa (Liu et al., 2019), etc, and
such an objective is regarded suitable for natural
language understanding (NLU). The second group
exploits the autoregressive-style language model-
ing (LM) to predict the current token according to
its leading tokens. GPT-3 (Brown et al., 2020) is
one of the representative works adopting this objec-
tive. The third part is the sequence-to-sequence
(Seq2Seq) models, which aim to generate a se-
quence with a decoder conditioned on a separate en-
coder for an input sequence. Typical seq2seq PLMs

--- PAGE 4 ---
⼯具包设计图
5FNQMBUF%BUBTFU5PLFOJ[FS1-.T1SPNQU5PLFOJ[FS1SPNQU%BUBTFU5FNQMBUF&NCFEEJOHT7FSCBMJ[FS1SPNQU.PEFM1SPNQU5SBJOFS
examplewrapped exampleinput for PLMslogits for wordsWrapper Class: These classes aim to make prompt-learning align with PyTorch pipeline, and users do not need to modify them.PLM-related Class: These classes support the calling and management of various PLMs.Prompt-related Class: These classes are unique modules for prompt-learning, and they can be implemented by users.Dataset-related Class:  These classes support the uAliAes for datasets across diﬀerent NLP tasks.wrapped exampleFigure 1: The overall architecture of OpenPrompt. Note that according to the prompt-learning strategies, not
all the modules are necessarily used. For example, in generation tasks, there are no verbalizers in the learning
procedure. The PromptTrainer is a controller that controls the data ﬂow and the training process with some
unique attributes, users can also implement the training process in a conventional fashion.
include T5 (Raffel et al., 2020), MASS (Song et al.,
2019) and BART (Lewis et al., 2020), etc.
Different PLMs have different attributes, result-
ing in various adaptation capabilities for different
NLP tasks in prompt-learning. Practically in Open-
Prompt, we support directly loading PLMs from
huggingface transformers1(Wolf et al., 2020), and
PLMs implemented by other libraries will be sup-
ported in the future. Once the PLM is determined,
researchers could deploy a known valid prompt-
learning pipeline (e.g., RoBERTa for few-shot sen-
timent classiﬁcation) or explore other uses of PLM
that could exploit its potential. Users of Open-
Prompt do not need to implement objective heads
for different PLMs to calculate the corresponding
loss, a uniﬁed interface can perform these opera-
tions automatically (§ 3.6).
3.3 Tokenization
Tokenization is a crucial step in processing data
for NLP, and it faces new challenges in prompt-
learning. After designing the template, the spe-
ciﬁc implementation of the tokenization for orig-
inal input and the designed template could be
time-consuming and error-prone. First, in prompt-
learning, some speciﬁc information such as the
indices of entities and masked tokens should be
1https://huggingface.co/modelscarefully tackled in tokenization. Some small er-
rors, such as the mismatch of masked token indices,
may lead to serious consequences. Moreover, con-
catenation and truncation issues after tokenization
(templates are not supposed to be truncated) should
also be handled. Since different PLMs may have
different tokenization strategies, we should also
consider the inconsistency in the details of addi-
tional context processing.
In OpenPrompt, we speciﬁcally design the tok-
enization module for prompt-learning and signif-
icantly simplify the process. By using our encap-
sulated data processing APIs, users could use the
human-readable style to design templates and con-
veniently operate on the input and the template at
the same time. Our component integrates complex
information from input and template and then con-
ducts tokenization. Based on the choice of PLMs
(MLM, LM, and Seq2Seq), OpenPrompt automati-
cally chooses the appropriate tokenizer in prompt-
learning, which could save considerable time for
users to process prompt-related data.
3.4 Templates
As one of the central parts of prompt-learning, a
template module wraps the original text with the
textual or soft-encoding template. A template nor-
mally contains contextual tokens (textual or soft)

--- PAGE 5 ---
1# Example A. Hard prompt for topic classification
2a {"mask"} news: {"meta": "title"} {"meta": "description"}
3
4# Example B. Hard prompt for entity typing
5{"meta": "sentence"}. In this sentence, {"meta": "entity"} is a {"mask"},
6
7# Example C. Soft prompt (initialized by textual tokens)
8{"meta": "premise"} {"meta": "hypothesis"} {"soft": "Does the first sentence entails
the second ?"} {"mask"} {"soft"}.
9
10# Example D. The power of scale
11{"soft": None, "duplicate": 100} {"meta": "text"} {"mask"}
12
13# Example E. Post processing script support
14# e.g. write an lambda expression to strip the final punctuation in data
15{"meta": "context", "post_processing": lambda s: s.rstrip(string.punctuation)}. {"
soft": "It was"} {"mask"}
16
17# Example F. Mixed prompt with two shared soft tokens
18{"meta": "premise"} {"meta": "hypothesis"} {"soft": "Does"} {"soft": "the", "soft_id
": 1} first sentence entails {"soft_id": 1} second?
19
20# Example G. Specify the title should not be truncated
21a {"mask"} news: {"meta": "title", "shortenable": False} {"meta": "description"}
Figure 2: Some examples of our template language. In our template language, we can use the key “meta” to refer
the original input text (Example B), parts of the original input (Example A, C, G), or other key information. We
can also freely specify which tokens are hard and which are soft (and their initialization strategy). We could assign
an id for a soft token to specify which tokens are sharing embeddings (Example F). OpenPrompt also supports the
post processing (Example E) for each token, e.g., lambda expression or MLP.
and masked tokens. In OpenPrompt, all the tem-
plates are inherited from a common base class with
universal attributes and abstract methods.
Previous works design a wide variety of tem-
plates, including manually written template (Schick
and Sch ¨utze, 2021) and pure soft template (Lester
et al., 2021). Gu et al. (2021) report a mix of
manual template tokens and soft (trainable) to-
kens sometimes yields better results than separate
manual template and soft template. In Liu et al.
(2021b), a promising performance is achieved by
ﬁxing the majority of manual tokens while tuning a
small number of the others. In Han et al. (2021b),
the template is contextualized, which needs to be
ﬁlled with the head entity and the tail entity to form
a complete one, moreover, the output of multiple
positions is used in the loss calculation in their
template. Logan IV et al. (2021) design null tem-
plate with simple concatenation of the inputs and
an appended <mask> token.
It’s not reasonable to design a template format
for each prompt since it will require high learning
cost for practical use. To this end, in OpenPrompt,
we design a template language to ease the prob-
lem, with which we can construct various types of
templates under a uniﬁed paradigm. Our templatelanguage takes insight from the dict grammer of
Python. And such a design ensures ﬂexibility and
clarity at the same time, allowing users to build
different prompts with relative ease.
More speciﬁcally, a template node is a text (or
empty text) with an attributes’ description. In our
template language, one is free to edit the attributes
of each token in the template, such as which charac-
ters are shared embedding, how the characters are
post-processed (e.g. by MLP), etc. We show some
template examples in Figure 2, and the detailed tu-
torial for writing templates is in our documentation
https://thunlp.github.io/OpenPrompt .
1from openprompt import
ManualVerbalizer
2
3promptVerbalizer = ManualVerbalizer(
4 classes = classes,
5 label_words = {
6 "negative": ["bad"],
7 "positive": ["good", "
wonderful", "great"],
8 },
9 tokenizer = bertTokenizer,
10)
Figure 3: An example to deﬁne a Verbalizer, the num-
ber of the label words for each class is ﬂexible.

--- PAGE 6 ---
MLMLMPreﬁxNLUSeq2SeqManualSoftMixManualContextKnowNLGAutoFix PLMTrainingPLMTemplateVerbalizerTask
UnﬁxP-tuningPrompt-tuningPreﬁx-tuningPTRP-tuningSoftFigure 4: The illustration of the validation space of OpenPrompt. By driving different modules of the framework,
we could implement and evaluate different methods on a broad set of NLP tasks. We show four examples in this
illustration, the colored lines denote the implementation ﬂow of the corresponding method.
3.5 Verbalizers
When it comes to prompt-based classiﬁcation, a
verbalizer class should be constructed to map origi-
nal labels to label words in the vocabulary. When
a PLM predicts a probability distribution over the
vocabulary for one masked position, a verbalizer
will extract the logits of label words and integrate
the logits of label words to the corresponding class,
thereby responsible for the loss calculation. Fig-
ure 3 shows a simple way to deﬁne a binary senti-
ment classiﬁcation verbalizer.
Similar to templates, all the verbalizer classes are
also inherited from a common base class with nec-
essary attributes and abstract methods. Additional
to manually-deﬁned verbalizers, we implement au-
tomatic verbalizers like AutomaticVerbalizer and
KnowledgeableVerbalizer (Hu et al., 2021). More-
over, important operations like calibrations (Zhao
et al., 2021) are also realized in OpenPrompt.
3.6 PromptModel
In OpenPrompt, we use a PromptModel ob-
ject to be responsible for training and inference,
which contains a PLM, a Template object, and a
Verbalizer object (optional). Users could ﬂex-
ibly combine these modules and deﬁne advanced
interactions among them. A model-agnostic for-
ward method is implemented in the base class to
predict words for the masked positions. One goal
of this module is that users do not need to speciﬁ-
cally implement heads for different PLMs, but use
a uniﬁed API to “predict words for positions that
need to be predicted” regardless of the pre-training
objective. An example to deﬁne a PromptModel
is shown in Figure 5.1from openprompt import
PromptForClassification
2
3promptModel = PromptForClassification(
4 template = promptTemplate,
5 model = bertModel,
6 verbalizer = promptVerbalizer,
7)
8
9promptModel.eval()
10with torch.no_grad():
11 for batch in data_loader:
12 logits = promptModel(batch)
13 preds = torch.argmax(logits,
dim = -1)
14 print(classes[preds])
Figure 5: An example to deﬁne a PromptModel and
conduct evaluation.
3.7 Training
From the perspective of trainable parameters, the
training of prompt-learning could be divided into
two types of strategies. The ﬁrst strategy simulta-
neously tunes the prompts and the PLM, which
is veriﬁed to be effective in a low-data regime
(OpenPrompt also provides a FewshotSampler
to support the few-shot learning scenario). The
second strategy is to only train the parameters of
prompts and keep the PLM frozen, this is regarded
as a parameter-efﬁcient tuning method and is con-
sidered as a promising way to stimulate super-large
PLMs. Both of these strategies can be called with
one click in the trainer (or runner) module of Open-
Prompt. Trainer modules in OpenPrompt imple-
ment training process accompanied with prompt-
oriented training tricks, e.g. the ensemble of tem-
plates. Meanwhile, OpenPrompt supports exper-
imentation through conﬁguration to easily drive
large-scale empirical study.

--- PAGE 7 ---
4 Evaluation
OpenPrompt aims to support a broad set of NLP
tasks under the paradigm of prompt-learning. In
terms of evaluation, we use OpenPrompt to im-
plement various baselines and assess them on the
corresponding NLP tasks. We show the validation
space in Figure 4. And the evaluation tasks include
WebNLG (Gardent et al., 2017) for conditional
generation, GLUE (Wang et al., 2018) and Super-
GLUE (Wang et al., 2019) for natural language un-
derstanding; SemEval (Hendrickx et al., 2010) for
relation extraction; Few-NERD (Ding et al., 2021b)
for ﬁne-grained entity typing; MNLI (Williams
et al., 2017), AG’s News (Zhang et al., 2015), DB-
Pedia (Lehmann et al., 2015) and IMDB (Maas
et al., 2011) for text classiﬁcation; LAMA (Petroni
et al., 2019) for knowledge probing. The processors
of these datasets have already been implemented in
OpenPrompt, and they are all inherited from a com-
mon base DataProcessor class. To keep the
results up to date, we are constantly updating and
reporting the latest results on our GitHub repository
https://github.com/thunlp/OpenPrompt .
5 Conclusion and Future Work
We propose OpenPrompt, a uniﬁed, easy-to-use
and extensible toolkit for prompt-learning. Open-
Prompt establishes a uniﬁed framework with
clearly deﬁned blocks and ﬂexible interactions to
support solid research on prompt-learning. At the
application level, OpenPrompt could facilitate re-
searchers and developers to effectively and efﬁ-
ciently deploy prompt-learning pipelines. In the fu-
ture, we will continue to integrate new techniques
and features to OpenPrompt to facilitate the re-
search progress of prompt-learning.
References
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of ACL , pages 4171–4186,
Minneapolis, Minnesota.
Ning Ding, Yulin Chen, Xu Han, Guangwei Xu,
Pengjun Xie, Hai-Tao Zheng, Zhiyuan Liu, Juanzi
Li, and Hong-Gee Kim. 2021a. Prompt-learningfor ﬁne-grained entity typing. Arxiv preprint ,
2108.10604.
Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang,
Xu Han, Pengjun Xie, Hai-Tao Zheng, and Zhiyuan
Liu. 2021b. Few-nerd: A few-shot named entity
recognition dataset. In Proceedings of ACL .
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of ACL , pages 3816–3830,
Online.
Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017. The webnlg
challenge: Generating text from rdf data. In Pro-
ceedings of INLG , pages 124–133.
Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.
2021. Ppt: Pre-trained prompt tuning for few-shot
learning. arXiv preprint arXiv:2109.04332 .
Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu,
Xiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang,
Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan,
Yang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu,
Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan,
Wayne Xin Zhao, and Jun Zhu. 2021a. Pre-trained
models: Past, present and future. ArXiv preprint ,
abs/2106.07139.
Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and
Maosong Sun. 2021b. Ptr: Prompt tuning with rules
for text classiﬁcation. ArXiv preprint , 2105.11259.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid ´O S´eaghdha, Sebastian
Pad´o, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. SemEval-2010 task 8:
Multi-way classiﬁcation of semantic relations be-
tween pairs of nominals. In Proceedings of SemEval ,
pages 33–38.
Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan
Liu, Juanzi Li, and Maosong Sun. 2021. Knowl-
edgeable prompt-tuning: Incorporating knowledge
into prompt verbalizer for text classiﬁcation. ArXiv
preprint , 2108.02035.
Teven Le Scao and Alexander M Rush. 2021. How
many data points is a prompt worth? In Proceedings
of NAACL , pages 2627–2636.
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
Dimitris Kontokostas, Pablo N Mendes, Sebastian
Hellmann, Mohamed Morsey, Patrick Van Kleef,
S¨oren Auer, et al. 2015. Dbpedia–a large-scale, mul-
tilingual knowledge base extracted from wikipedia.
Semantic web , 6(2):167–195.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efﬁcient prompt
tuning. ArXiv preprint , abs/2104.08691.

--- PAGE 8 ---
Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of ACL , pages
7871–7880, Online.
Xiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:
Optimizing continuous prompts for generation. In
Proceedings ACL , pages 4582–4597, Online. Asso-
ciation for Computational Linguistics.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2021a. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ArXiv preprint , abs/2107.13586.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021b. Gpt
understands, too. arXiv preprint arXiv:2103.10385 .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
RoBERTa: A robustly optimized BERT pretraining
approach. ArXiv preprint , abs/1907.11692.
Robert L Logan IV , Ivana Bala ˇzevi´c, Eric Wallace,
Fabio Petroni, Sameer Singh, and Sebastian Riedel.
2021. Cutting down on prompts and parameters:
Simple few-shot learning with language models.
arXiv preprint arXiv:2106.13353 .
Andrew Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of ACL .
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, et al. 2019. Pytorch: An imperative style,
high-performance deep learning library. Proceed-
ings of NeurIPS , 32:8026–8037.
Fabio Petroni, Tim Rockt ¨aschel, Patrick Lewis, Anton
Bakhtin, Yuxiang Wu, Alexander H Miller, and Se-
bastian Riedel. 2019. Language models as knowl-
edge bases? arXiv preprint arXiv:1909.01066 .
Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin,
Ning Ding, Zhiyuan Liu, Juanzi Li, Lei Hou, Peng
Li, Maosong Sun, et al. 2021. Exploring low-
dimensional intrinsic task subspace via prompt tun-
ing. arXiv preprint arXiv:2110.07867 .
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,
Ning Dai, and Xuanjing Huang. 2020. Pre-trained
models for natural language processing: A survey.
Science China Technological Sciences , pages 1–26.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limitsof transfer learning with a uniﬁed text-to-text trans-
former. ArXiv preprint , abs/1910.10683.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search , 21(140):1–67.
Timo Schick and Hinrich Sch ¨utze. 2021. Exploiting
cloze-questions for few-shot text classiﬁcation and
natural language inference. In Proceedings of the
16th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Main Vol-
ume, pages 255–269, Online. Association for Com-
putational Linguistics.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2019. Mass: Masked sequence to sequence
pre-training for language generation. arXiv preprint
arXiv:1905.02450 .
Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou,
and Daniel Cer. 2021. Spot: Better frozen model
adaptation through soft prompt transfer. arXiv
preprint arXiv:2110.07904 .
Alex Wang, Yada Pruksachatkun, Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel R Bowman. 2019. Super-
glue: A stickier benchmark for general-purpose
language understanding systems. arXiv preprint
arXiv:1905.00537 .
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R Bowman. 2018.
Glue: A multi-task benchmark and analysis platform
for natural language understanding. arXiv preprint
arXiv:1804.07461 .
Colin Wei, Sang Michael Xie, and Tengyu Ma. 2021.
Why do pretrained language models help in down-
stream tasks? an analysis of head and prompt tun-
ing.
Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. arXiv
preprint arXiv:1704.05426 .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of EMNLP , pages 38–45,
Online.
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siﬁcation. Advances in neural information process-
ing systems .

--- PAGE 9 ---
Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Im-
proving few-shot performance of language models.
arXiv preprint arXiv:2102.09690 .
