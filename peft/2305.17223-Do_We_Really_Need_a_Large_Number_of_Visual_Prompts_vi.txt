# Có thực sự cần một số lượng lớn Visual Prompts không?

Youngeun Kim
Đại học Yale
youngeun.kim@yale.edu

Yuhang Li
Đại học Yale
yuhang.li@yale.edu

Abhishek Moitra
Đại học Yale
abhishek.moitra@yale.edu

Ruokai Yin
Đại học Yale
ruokai.yin@yale.edu

Priyadarshini Panda
Đại học Yale
priya.panda@yale.edu

## Tóm tắt

Do sự quan tâm ngày càng tăng đối với việc điều chỉnh các mô hình trên các thiết bị edge có tài nguyên hạn chế, việc học chuyển giao hiệu quả tham số đã được khám phá rộng rãi. Trong số các phương pháp khác nhau, Visual Prompt Tuning (VPT), việc thêm các prompts có thể học được vào không gian đầu vào, cho thấy hiệu suất fine-tuning cạnh tranh so với việc huấn luyện toàn bộ tham số mạng. Tuy nhiên, VPT tăng số lượng token đầu vào, dẫn đến chi phí tính toán bổ sung. Trong bài báo này, chúng tôi phân tích tác động của số lượng prompts đối với hiệu suất fine-tuning và hoạt động self-attention trong kiến trúc vision transformer. Thông qua phân tích lý thuyết và thực nghiệm, chúng tôi cho thấy rằng việc thêm nhiều prompts hơn không dẫn đến cải thiện hiệu suất tuyến tính. Hơn nữa, chúng tôi đề xuất kỹ thuật Prompt Condensation (PC) nhằm ngăn chặn sự suy giảm hiệu suất từ việc sử dụng một số lượng nhỏ prompts. Chúng tôi xác thực các phương pháp của mình trên các tác vụ FGVC và VTAB-1k và cho thấy rằng phương pháp của chúng tôi giảm số lượng prompts khoảng 70% trong khi vẫn duy trì độ chính xác.

## 1. Giới thiệu

Parameter-Efficient Transfer Learning (PETL) đã trở thành một phương pháp phổ biến trong nhiều lĩnh vực khác nhau vì nó cho phép fine-tuning các mô hình được pre-trained với việc sử dụng bộ nhớ tối thiểu trên các thiết bị edge có tài nguyên hạn chế [31,45,46,48,14,16]. Trong PETL, một mô hình lớn với hàng tỷ tham số, như transformer [8,38], đầu tiên được huấn luyện trên một tập dữ liệu khổng lồ trên máy chủ đám mây, sau đó được fine-tuning với tài nguyên tính toán/bộ nhớ hạn chế trên các thiết bị edge. Trong số các phương pháp PETL khác nhau, Visual Prompt Tuning (VPT) [17] đầy hứa hẹn do khả năng cập nhật một tập con nhỏ các tham số trong khi đạt được độ chính xác cao hơn các phương pháp khác. Về mặt kỹ thuật, VPT giới thiệu các token prompt có thể học được, được thêm vào trước các token patch hình ảnh đầu vào hoặc trung gian.

Trong khi VPT có thể tạo ra hiệu quả bộ nhớ, việc sử dụng các token prompt bổ sung dẫn đến chi phí tính toán tăng từ self-attention và các lớp tuyến tính [26,42,8]. Chúng tôi báo cáo FLOPs theo số lượng prompts trong Bảng 1, cho thấy rằng chi phí tính toán của VPT tăng đáng kể khi số lượng prompts tăng. Nếu 200 prompts được thêm vào không gian đầu vào của ViT-B, chi phí tính toán (tức là FLOPs) gần như tăng gấp đôi so với mô hình không có prompts nào. Điều này cho thấy có một sự đánh đổi không thể tránh khỏi giữa số lượng prompts và chi phí tính toán trong VPT.

Với sự đánh đổi như vậy, việc đặt câu hỏi là tự nhiên: Hiệu suất fine-tuning thay đổi như thế nào theo số lượng prompts? Để tìm câu trả lời, chúng tôi đo độ chính xác kiểm tra theo số lượng prompts. Thú vị là, như được hiển thị trong Hình 1, chúng tôi phát hiện rằng việc giảm số lượng prompts cho huấn luyện VPT khoảng 50% không dẫn đến sự sụt giảm đáng kể, và phần lớn sự sụt giảm hiệu suất xảy ra trong khoảng 10%∼40%. Kết quả ngụ ý rằng mối tương quan giữa số lượng prompts và độ chính xác fine-tuning không phải là tuyến tính.

Để cung cấp thêm hiểu biết tốt hơn về các prompts trong VPT, chúng tôi phân tích tác động của số lượng token prompt đối với độ chính xác fine-tuning bằng cách giải quyết một số câu hỏi: Tại sao số lượng prompts và hiệu suất fine-tuning có mối tương quan phi tuyến? Số lượng prompts ảnh hưởng như thế nào đến hoạt động self-attention? Nếu có sự sụt giảm hiệu suất với số lượng prompts ít hơn, làm thế nào chúng ta có thể phục hồi sự sụt giảm độ chính xác? Chúng tôi cung cấp cả phân tích thực nghiệm và toán học để trả lời những câu hỏi như vậy. Điều này có thể cung cấp thông tin chi tiết về hành vi của mô hình VPT và cơ chế self-attention của nó, có thể giúp các nhà nghiên cứu hiểu rõ hơn về VPT và có khả năng cải thiện thiết kế prompt.

Đồng thời, việc phân tích tác động này đối với chi phí tính toán là cần thiết để đảm bảo rằng phương pháp vẫn thực tế cho việc triển khai trên các thiết bị edge có tài nguyên cực kỳ hạn chế.

Một quan sát đáng chú ý từ Hình 1 là sự suy giảm hiệu suất trong chế độ <50% số lượng prompts là không tầm thường. Để giải quyết điều này, chúng tôi đề xuất Prompt Condensation (PC), một kỹ thuật giảm số lượng token prompt với sự sụt giảm độ chính xác tối thiểu. PC bao gồm ba bước: (1) Tính toán điểm số quan trọng cho mỗi prompt. Ở đây, chúng tôi đề xuất một metric toàn cục để đo điểm số quan trọng của mỗi prompt, cung cấp độ chính xác tốt hơn so với các metric dựa trên attention cục bộ [25,30,10]. (2) Chọn k% prompts hàng đầu dựa trên điểm số quan trọng, và loại bỏ các prompts còn lại. (3) Fine-tuning các prompts được chọn trong khi đóng băng các tham số khác.

Tóm lại, các đóng góp của chúng tôi có thể như sau:
• Trong một nghiên cứu đầu tiên thuộc loại này, chúng tôi phân tích tác động của số lượng token visual prompt đối với độ chính xác fine-tuning và hoạt động self-attention trong VPT.
• Chúng tôi phát hiện rằng số lượng prompts không tỷ lệ thuận tuyến tính với cải thiện hiệu suất. Để hỗ trợ điều này, chúng tôi cung cấp phân tích thực nghiệm và toán học.
• Để phục hồi sự sụt giảm hiệu suất với một số lượng nhỏ prompts, chúng tôi đề xuất Prompt Condensation (PC). Phương pháp của chúng tôi có thể giảm số lượng prompts khoảng 70% trong khi vẫn duy trì hiệu suất.

## 2. Công trình liên quan

### 2.1. Parameter Efficient Transfer Learning (PETL)

Fine-tuning hiệu quả các mô hình lớn được pre-trained trên các thiết bị edge đã trở thành một chủ đề nghiên cứu phổ biến do tính thực tế và hiệu suất cao của nó [31,45,46,48,14,16]. Thay vì huấn luyện toàn bộ tập hợp các tham số trong mạng neural, các nhà nghiên cứu tập trung vào cách sử dụng một phần trăm nhỏ trọng số để tối đa hóa hiệu suất chuyển giao. Để đạt được mục tiêu này, một số phương pháp [32,35,15,3] chèn một mô-đun bottleneck nhẹ vào mô hình transformer, cho phép gradient được tính toán chỉ cho một số lượng nhỏ tham số. TinyTL [3] và BitFit [43] đề xuất cập nhật bias term để fine-tuning mô hình. Các phương pháp khác [45,34] thêm các mạng phụ có thể được tối ưu hóa trong khi giữ mô hình lớn ban đầu đóng băng. Một phương pháp hiệu quả khác để giảm yêu cầu bộ nhớ là thưa thớt hóa [18] hoặc lượng tử hóa activation [4,5,11,9] trong quá trình tính toán gradient ngược. Gần đây, VPT [17] thêm vào trước các tham số có thể huấn luyện vào không gian đầu vào của mô hình được pre-trained, đạt được độ chính xác tương tự (và đôi khi thậm chí tốt hơn) so với full fine-tuning trong khi chỉ tối ưu hóa khoảng 1% tham số. Tuy nhiên, việc thêm một số lượng lớn prompts có thể tăng đáng kể chi phí tính toán của mô hình. Trong công trình này, chúng tôi phân tích cách số lượng prompts ảnh hưởng đến hiệu suất fine-tuning.

**Tầm quan trọng của công trình chúng tôi.** Prompt tuning là một trong những hướng nghiên cứu chính để fine-tuning mô hình được pre-trained quy mô lớn. Xem xét rằng prompt learning được áp dụng cho nhiều ứng dụng khác nhau, chúng tôi nhằm cải thiện hiệu quả của phương pháp prompt tuning. Mục tiêu của chúng tôi khác biệt với các công trình trước [7,1,6,47,19], như các phương pháp dựa trên adapter hoặc huấn luyện một phần, chủ yếu tìm cách nâng cao hiệu suất trên các tác vụ downstream với các phương pháp khác nhau. Hơn nữa, vì kỹ thuật của chúng tôi không yêu cầu bất kỳ sửa đổi nào đối với kiến trúc mô hình, nó cung cấp tiềm năng hứa hẹn cho việc mở rộng trong các phương pháp prompt learning tương lai.

### 2.2. Token Sparsification

Chi phí tính toán của ViT [8] tăng khi số lượng token được cung cấp cho mô hình tăng [36]. Để giảm thiểu vấn đề này, các công trình trước đây nhằm giảm số lượng token [10,30,27,23,13,42,21,22,33]. Liang et al. [25] định nghĩa điểm số quan trọng của mỗi token dựa trên sự tương tự của nó với token [CLS]. Rao et al. [30] đề xuất một mô-đun dự đoán với Gumbel-Softmax để thưa thớt hóa token, được huấn luyện chung với các tham số mô hình. Meng et al. [27] đề xuất một mạng quyết định có thể bật/tắt các head và block trong kiến trúc transformer. Các tác giả của [41] đề xuất một mô-đun dừng thích ứng tính toán xác suất cho mỗi token để xác định khi nào dừng xử lý. Tuy nhiên, các phương pháp này yêu cầu cập nhật các tham số trọng số bên trong transformer hoặc một mô-đun bổ sung, điều này khó áp dụng cho kịch bản PETL. Gần đây, [2] đề xuất một kỹ thuật token merging không cần huấn luyện, giảm dần số lượng token trong mỗi block của vision transformer để tăng tốc độ suy luận. Tuy nhiên, phương pháp của họ sẽ khó áp dụng cho các token prompt vì các token prompt được giới thiệu ở mọi layer.

## 3. Kiến thức cơ bản

**Vision Transformer.** Công trình của chúng tôi dựa trên ViT [8] xử lý các token hình ảnh với nhiều hoạt động attention. Hình ảnh đầu vào được cắt thành nhiều patch (tức là token). Sau đó, trong mỗi layer, hoạt động self-attention được áp dụng cho các token hình ảnh. Giả sử chúng ta có token embedding X∈R^(n×d), Query Q=XW^q, Key K=XW^k, Value V=XW^v với phép chiếu tuyến tính. Sau đó, hoạt động attention có thể được công thức hóa như sau:

Attention(Q, K, V) = Softmax(QK^T/√d) V = AV, (1)

trong đó A là ma trận self-attention sau hàm Softmax. Chúng tôi sử dụng Multi-Head Self-Attention (MHSA) trong phương pháp của mình, lấy đầu ra của nhiều block attention single-head, sau đó chiếu đầu ra kết hợp bằng ma trận tham số bổ sung.

head_i = Attention(XW_i^q, XW_i^k, XW_i^v). (2)
MHSA(X) = Concat[head_1, ..., head_H]W^o + X. (3)

Các token đầu ra được tạo bởi block MHSA được đưa vào Feed-Forward Network (FFN), bao gồm hai lớp fully-connected với một lớp kích hoạt GELU ở giữa. Trong layer encoder cuối cùng của Transformer, token [CLS] được trích xuất từ các token đầu ra và được sử dụng để dự đoán lớp.

**Visual Prompt Tuning.** Visual Prompt Tuning (VPT) [17] đề xuất một kỹ thuật fine-tuning hiệu quả bộ nhớ bằng cách thêm một tập hợp các prompts có thể học được ở các layer đầu vào/trung gian. Tùy thuộc vào vị trí thêm prompts, VPT có hai phiên bản: VPT-Shallow và VPT-Deep.

Gọi X_i∈R^(n×d) là token embedding ở layer i∈{1,2,...,L}, và F_i(·) là các hoạt động bên trong layer i. VPT-Shallow thêm vào trước m prompts P_1∈R^(m×d) vào token embedding đầu vào X_1.

[Z_2, X_2] = F_1([P_1; X_1]). (4)
[Z_(i+1), X_(i+1)] = F_i([Z_i; X_i]) for 1 < i ≤ L. (5)

Ở đây, Z_i là các token đầu ra từ layer i. Lưu ý rằng chỉ P_1 và classification head được huấn luyện.

Mặt khác, VPT-Deep giới thiệu prompts P_i∈R^(m×d) ở mọi layer.

[__, X_(i+1)] = F_i([P_i; X_i]) for 1 ≤ i ≤ L. (6)

VPT-Deep cho thấy hiệu suất cao hơn VPT-Shallow bằng cách sử dụng nhiều prompts hơn. Trong công trình của chúng tôi, chúng tôi tập trung vào VPT-Deep do hiệu suất vượt trội của nó. Mặc dù VPT yêu cầu sử dụng bộ nhớ ít hơn đáng kể cho huấn luyện, chi phí tính toán tăng khi tổng số token tăng.

## 4. Phân tích về Số lượng Visual Prompts

Trong phần này, chúng tôi phân tích tác động của prompts đối với hoạt động self-attention và độ chính xác fine-tuning. Chúng tôi đầu tiên chứng minh hai quan sát, sau đó chúng tôi cung cấp hỗ trợ toán học tại sao hiệu suất không cải thiện tuyến tính khi chúng ta sử dụng nhiều prompts hơn.

**Quan sát 1:** Giảm số lượng prompts không làm giảm độ chính xác một cách tuyến tính. Ngoài Hình 1, chúng tôi cung cấp thêm bằng chứng thực nghiệm về mối tương quan giữa số lượng prompts và độ chính xác fine-tuning. Chúng tôi đánh giá độ chính xác kiểm tra của phương pháp chúng tôi trên các tác vụ FGVC và VTAB-1k [44] với việc thay đổi số lượng prompts. Đáng chú ý rằng mỗi tập dữ liệu yêu cầu một số lượng prompts cụ thể để đạt được hiệu suất tối ưu, như được báo cáo trong [17]. Chúng tôi tập trung vào các tập dữ liệu yêu cầu hơn 10 prompts cho cả VPT-Shallow và VPT-Deep vì việc sử dụng ít hơn 10 prompts không dẫn đến chi phí tính toán đáng kể. Chúng tôi trình bày sự thay đổi hiệu suất theo số lượng prompts trong Bảng 2. Phân tích của chúng tôi cho thấy rằng đối với phần lớn các tập dữ liệu, việc giảm số lượng prompts khoảng 50% không dẫn đến sự suy giảm hiệu suất đáng kể. Ngoài ra, phần lớn sự giảm hiệu suất xảy ra trong khoảng 10% đến 40%, cho thấy mối quan hệ giữa độ chính xác và số lượng prompts không phải là tuyến tính.

**Quan sát 2:** Ma trận self-attention có rank thấp trước/sau khi thêm prompts. Công trình trước [40] cho thấy rằng ma trận self-attention trong ViT có rank thấp. Theo hướng suy nghĩ tương tự, chúng tôi điều tra rank của ma trận self-attention khi chúng ta thêm prompts. Trong Hình 2, chúng tôi so sánh eigenvalue tích lũy của ma trận self-attention A không có prompts và có prompts. Kết quả của chúng tôi cho thấy rằng ma trận self-attention vẫn có rank thấp ngay cả khi prompts được thêm vào ma trận self-attention. Đặc biệt, đối với tập dữ liệu Stanford Cars, chúng tôi thêm 200 prompts là một số lượng lớn token hơn các token hình ảnh ban đầu (tức là 197), nhưng xu hướng eigenvalue tích lũy không thay đổi. Nhìn chung, kết quả ngụ ý rằng chỉ một vài prompts ảnh hưởng đến hoạt động self-attention.

Để hiểu tại sao số lượng prompts không tương quan tuyến tính với hoạt động self-attention và độ chính xác, chúng tôi cung cấp phân tích toán học ở đây. Chúng tôi sử dụng rank của ma trận rank thấp xấp xỉ của ma trận attention như một metric thay thế để đánh giá tác động của prompt đối với hoạt động self-attention.

**Định lý 1** (Self-attention có rank thấp. Được chứng minh trong [40]).
Gọi A∈R^(n×n) là ma trận self-attention, và v∈R^n là vector cột của ma trận value V. Khi đó, tồn tại ma trận rank thấp Ã∈R^(n×n) thỏa mãn

Pr(‖Ãv^T - Av^T‖ < ε‖Av^T‖) > 1 - o(1), (7)

trong đó rank của Ã bị chặn, tức là rank(Ã) = Θ(log(n)).

**Mệnh đề 1.** Đối với bất kỳ ma trận rank thấp Ã_n∈R^(n×n) và Ã_(n+m)∈R^((n+m)×(n+m)) thỏa mãn Pr(‖Ãv^T - Av^T‖ < ε‖Av^T‖) > 1 - o(1), chúng ta có

rank(Ã_(n+m)) - rank(Ã_n) = O(log(m)), (8)

trong đó m là số lượng prompts.

**Chứng minh.** Dựa trên Định lý 1, với lỗi bị chặn Pr(‖Ãv^T - Av^T‖ < ε‖Av^T‖) > 1 - o(1), rank của Ã_n và Ã_(n+m) có thể là:

α log(n) ≤ rank(Ã_n) ≤ β log(n), (9)
α log(n+m) ≤ rank(Ã_(n+m)) ≤ β log(n+m), (10)

trong đó α và β là các hằng số cho cận dưới và cận trên tương ứng. Khi đó, chúng ta có

log(n+m)α/nβ ≤ rank(Ã_(n+m)) - rank(Ã_n) ≤ log(n+m)β/nα. (11)

Chúng ta thu được Phương trình 8 theo biến m. Các chi tiết bổ sung có thể được tìm thấy trong Phụ lục.

Mệnh đề 1 chứng minh rằng sự gia tăng rank của ma trận self-attention rank thấp tuân theo xu hướng logarit. Vì hàm logarit là lõm, tác dụng của việc thêm prompts mới đối với hoạt động attention giảm dần khi số lượng prompts tăng. Ví dụ, việc tăng số lượng prompts từ 0 đến 50 có tác động lớn hơn việc tăng số lượng prompts từ 150 đến 200. Phân tích này phù hợp với Quan sát 1 của chúng tôi, trong đó việc giảm số lượng prompts khoảng 50% không dẫn đến sự sụt giảm hiệu suất đáng kể, nhưng phần lớn sự sụt giảm hiệu suất tồn tại trong khoảng 10%∼40%.

## 5. Prompt Condensation

Mặc dù việc giảm số lượng prompts lên đến 50% cho thấy sự suy giảm hiệu suất nhẹ, sự sụt giảm hiệu suất không tầm thường trong chế độ số lượng prompts nhỏ. Trong Bảng 2, sự sụt giảm hiệu suất chính xảy ra dưới 40% prompts trên hầu hết các tập dữ liệu. Để giải quyết điều này, chúng tôi đề xuất một kỹ thuật gọi là Prompt Condensation (PC).

**Phát biểu bài toán.** Mục tiêu của chúng tôi là giảm thiểu số lượng prompts trong khi duy trì độ chính xác. Gọi P = {p_1, p_2, ..., p_N} là tập hợp các prompts, và P' là tập prompt đã được nén có số lượng phần tử ít hơn. Khi đó mục tiêu của chúng tôi có thể được viết như:

min_(P') |L(θ, P) - L(θ, P')|, (12)

trong đó L(·) là hàm mục tiêu của một tác vụ, θ là các tham số mô hình. Đồng thời, chúng tôi cũng nhằm giảm thiểu số lượng prompts bên trong P'.

Trong việc thiết kế mô hình của chúng tôi cho kịch bản Parameter Efficient Transfer Learning (PETL), chúng tôi xem xét các nguyên tắc sau: (1) Các tham số mô hình không thể được cập nhật do hạn chế bộ nhớ. Do đó, chỉ các prompts có thể được huấn luyện. (2) Các mô-đun bổ sung như những mô-đun được đề xuất trong [30,27,41] không thể được sử dụng. Với những hạn chế này, hầu hết các phương pháp thưa thớt hóa token đều khó áp dụng trong trường hợp của chúng tôi. Thay vào đó, phương pháp của chúng tôi tập trung vào việc xác định các prompts quan trọng và fine-tuning chúng mà không cần cập nhật/thêm bất kỳ tham số mô hình nào.

**Tất cả các prompts có quan trọng như nhau không?** Lựa chọn thiết kế quan trọng cho PC là có nên nén cùng một số lượng prompts cho mỗi layer hay không. Để tìm hiểu điều này, chúng tôi đo sự thay đổi độ chính xác theo prompts trong mỗi layer. Chúng tôi loại bỏ prompts ở layer l trong khi các layer khác giữ cùng số lượng prompts. Như được hiển thị trong Hình 3, chúng tôi quan sát thấy rằng prompts ở các layer khác nhau có những đóng góp khác nhau cho độ chính xác, và xu hướng thay đổi giữa các tập dữ liệu khác nhau. Quan sát này dẫn chúng tôi đến việc tận dụng điểm số toàn cục trên tất cả các layer, khác với điểm số theo layer (tức là sử dụng sự tương tự hàng trong self-attention) được sử dụng rộng rãi trong công trình trước [25, 30, 10].

**Prompt Scoring.** Chúng tôi định nghĩa tác động của prompt p_i bằng cách tính toán sự khác biệt của hàm mục tiêu từ mô hình VPT đã được fine-tuned.

‖ΔL(θ, p_i)‖_2 = ‖L(θ, P) - L(θ, P'_i)‖_2, (13)

trong đó P'_i là tập prompt đã được sửa đổi bằng cách zero hóa p_i ∈ P. Với xấp xỉ Taylor, chúng ta có thể xấp xỉ L(θ, P'_i) tại p_i = 0 như

L(θ, P'_i) ≈ L(θ, P) - (dL(θ)/dp_i)p_i. (14)

Chúng tôi chỉ sử dụng hạng tử bậc nhất vì hạng tử vượt quá bậc hai yêu cầu lưu trữ bộ nhớ khổng lồ. Nếu chúng ta thay thế Phương trình 14 vào Phương trình 13, chúng ta thu được

‖ΔL(θ, p_i)‖_2 ≈ ‖(dL(θ)/dp_i)p_i‖_2. (15)

Chúng tôi tính trung bình Phương trình 15 trên tất cả các mẫu dữ liệu để tính toán điểm số quan trọng.

s_{p_i} = (1/|D|) ∑_{d∈D} ‖(dL(θ,d)/dp_i)p_i‖_2, (16)

trong đó D là tập dữ liệu đầu vào. Lưu ý rằng, việc tính toán điểm số quan trọng không mang lại chi phí tính toán khổng lồ vì chúng tôi chỉ cần tính toán gradient ngược cho các prompts.

Khi chúng tôi tính toán điểm số quan trọng cho mỗi prompt, chúng tôi chọn các prompts có k% điểm số cao nhất trên tất cả các layer. Phương pháp chọn prompt toàn cục này vốn dĩ phân bổ số lượng prompts tối ưu cho mỗi layer. Mặt khác, với việc chọn prompt cục bộ theo layer, chúng ta sẽ ép buộc việc chọn k% prompt hàng đầu một cách đồng nhất trên tất cả các layer có thể cản trở khả năng biểu diễn trong mô hình. Trong các thí nghiệm của chúng tôi, chúng tôi cho thấy điểm số toàn cục cung cấp hiệu suất tốt hơn so với các metric cục bộ theo layer.

Phương pháp của chúng tôi tương tự như filter pruning trong CNNs [24,28] ở khía cạnh sử dụng khai triển Taylor. Tuy nhiên, chúng tôi đã sáng tạo điều chỉnh khái niệm này đến cấp độ token, trình bày một granularity cơ bản khác biệt trong chiến lược pruning. Theo hiểu biết của chúng tôi, công trình của chúng tôi là đầu tiên sử dụng thông tin gradient trực tiếp cho token pruning trong bối cảnh kiến trúc Vision Transformer (ViT). Do đó, chúng tôi tin rằng nghiên cứu của chúng tôi mở đường cho khả năng áp dụng các kỹ thuật channel pruning hiện có vào token pruning trong ViTs.

**Quy trình Huấn luyện Tổng thể.** Thuật toán 1 minh họa quy trình tổng thể của Prompt Condensation. Chúng tôi đầu tiên huấn luyện tập prompt ban đầu P (Dòng 1). Sau đó chúng tôi tính toán điểm số quan trọng của mỗi prompt bên trong P (Dòng 2). Sau đó, chúng tôi sắp xếp điểm số quan trọng và chọn các prompts có k% điểm số cao nhất (Dòng 3). Điều này cung cấp tập prompt đã được nén P'. Chúng tôi loại bỏ (100-k)% prompts còn lại. Cuối cùng, các prompts trong P' được fine-tuned (Dòng 4). Để fine-tuning, chúng tôi sử dụng ít epoch N_p hơn so với epoch huấn luyện VPT ban đầu N_v. Chúng tôi phân tích ảnh hưởng của N_p trong Phần 6.3. Lưu ý rằng toàn bộ quá trình huấn luyện đóng băng các tham số trọng số trên toàn mô hình ngoại trừ classifier cuối cùng.

## 6. Thí nghiệm

### 6.1. Thiết lập Thí nghiệm

**Kiến trúc.** Chúng tôi tiến hành thí nghiệm sử dụng hai kiến trúc transformer được pre-trained trên ImageNet-22k, tức là Vision Transformer (ViT-B/16) [8] và Swin Transformer (Swin-B) [26].

**Tập dữ liệu.** Chúng tôi sử dụng các tác vụ FGVC và VTAB-1k làm tập dữ liệu của chúng tôi. FGVC bao gồm 5 tập dữ liệu, bao gồm CUB-200-2011 [39], NABirds [37], Oxford Flowers [29], Stanford Dogs [20], và Stanford Cars [12]. VTAB-1k [44] chứa 19 tập dữ liệu với nhiều domain thị giác khác nhau. Theo công trình trước [44,17], chúng tôi sử dụng phân chia 800-200 được cung cấp của tập train để huấn luyện và báo cáo điểm độ chính xác trung bình trên kiểm tra trong ba lần chạy. Đối với cả tập dữ liệu FGVC và VTAB-1k, chúng tôi chọn các tập dữ liệu cho thấy sự sụt giảm độ chính xác không tầm thường (tức là ≥1%) với 10% prompts so với VPT ban đầu. Do đó, chúng tôi có 8 tập dữ liệu cho ViT: {Stanford Cars, Clevr-count, DMLab, dSprites-location, dSprites-orientation, smallNORB-azimuth, smallNORB-elevation, SVHN}, và 5 tập dữ liệu cho Swin: {Clevr-count, Clevr-distance, dSprites-location, smallNORB-azimuth, SVHN}. Chi tiết về việc chọn tập dữ liệu được cung cấp trong Phụ lục.

Chúng tôi quan sát thấy rằng sự sụt giảm hiệu suất không tầm thường có xu hướng xảy ra trong các tác vụ downstream khó khăn hơn. Để minh họa điều này, chúng tôi tính toán mean và standard deviation của độ chính xác kiểm tra trên các tác vụ downstream, tách chúng thành những tác vụ có sự sụt giảm hiệu suất không tầm thường (≥1%) và tầm thường (<1%). Các tác vụ này được bắt nguồn từ các tập dữ liệu FGVC và VTAB-1k. Kết quả của chúng tôi cho thấy rằng các tập dữ liệu có sự sụt giảm độ chính xác không tầm thường thể hiện độ chính xác trung bình là 58.91±20.23%, trong khi những tập có sự sụt giảm độ chính xác tầm thường thể hiện độ chính xác trung bình cao hơn là 81.96±11.54%.

**Siêu tham số.** Chúng tôi tuân theo các siêu tham số (ví dụ: weight decay, learning rate) được báo cáo trong [17]. Mỗi tập dữ liệu có một số lượng prompts khác nhau, được xác định bởi công trình trước [17] báo cáo số lượng prompts có hiệu suất tốt nhất cho mỗi tập dữ liệu. Trong giai đoạn fine-tuning prompt, chúng tôi tắt dropout và sử dụng ×0.1 learning rate VPT ban đầu. Để prompt condensation, chúng tôi huấn luyện lại các prompts được chọn trong 20 epoch, ngắn hơn so với quá trình huấn luyện VPT ban đầu. Trong Thuật toán 1, chúng tôi đặt số epoch N_v để huấn luyện VPT là 100, theo bài báo gốc [17].

### 6.2. So sánh Hiệu suất

Chúng tôi đầu tiên đánh giá hiệu quả của Prompt Condensation (PC) với số lượng prompts hạn chế. Cụ thể, chúng tôi thay đổi số lượng prompts từ 10% đến 50%, trong đó ký hiệu k% biểu thị việc sử dụng k% số lượng prompts được báo cáo trong [17]. Chúng tôi so sánh hiệu suất của PC với các mô hình sau:

• **VPT (baseline):** Chúng tôi huấn luyện mô hình được pre-trained ImageNet với k% prompts ban đầu.
• **PC w/o fine-tuning:** Từ VPT đã được huấn luyện với 100% prompts, chúng tôi tính toán điểm số quan trọng (Phương trình 16) của mỗi prompt và chọn k% prompts hàng đầu dựa trên điểm số và loại bỏ phần còn lại.

Hình 4 và 5 trình bày so sánh hiệu suất của VPT-Deep và VPT-Shallow tương ứng. Từ kết quả, chúng tôi đưa ra các quan sát sau: (1) Đối với VPT-Deep, PC duy trì hiệu suất chỉ với 20∼30% số lượng prompts, chứng minh hiệu quả của nó so với baseline VPT naïve. (2) Việc cải thiện hiệu suất đạt được bằng cách áp dụng PC cho VPT-Shallow tương đối thấp hơn so với VPT-Deep. Điều này có thể được quy cho việc VPT-Shallow có số lượng prompts ban đầu nhỏ hơn, dẫn đến ít không gian cải thiện hiệu suất hơn. Đồng thời, VPT-Deep cho hiệu suất cao hơn mô hình VPT-Shallow. Do đó, chúng tôi tập trung vào VPT-Deep trong bài báo này. (3) Thú vị là, PC w/o fine-tuning với VPT-Deep không cho thấy sự sụt giảm hiệu suất đáng kể với 40∼50% prompts ban đầu. Điều này cho thấy rằng điểm số quan trọng prompt của chúng tôi phản ánh chính xác tác động của mỗi prompt đối với độ chính xác tổng thể. (4) Đối với chế độ 10∼30%, có sự suy giảm hiệu suất đáng kể khi không có fine-tuning. Tuy nhiên, điều này có thể được phục hồi hoàn toàn bằng fine-tuning prompts, chứng minh fine-tuning là một giai đoạn cần thiết cho PC. (5) Kết quả của Swin cũng cung cấp xu hướng tương tự như ViT, như được hiển thị trong Hình 6.

### 6.3. Phân tích Thí nghiệm

**Lựa chọn Thiết kế cho Prompt Scoring.** Trong phương pháp của chúng tôi, chúng tôi tính toán gradient để đánh giá tầm quan trọng của mỗi prompt (Phương trình 16). Dựa trên điểm số này, chúng tôi chọn k% prompts có điểm số cao nhất trên tất cả các layer. Để điều tra hiệu quả của kỹ thuật prompt scoring của chúng tôi, chúng tôi so sánh nó với một số biến thể.

• **Global Prompt Condensation (ours-Global):** Phương pháp đề xuất của chúng tôi, trong đó k% prompts có điểm số cao nhất được chọn trên tất cả các layer.
• **Local Prompt Condensation (ours-Local):** Thay vì xem xét toàn bộ layer, chúng tôi chọn k% prompts có điểm số cao nhất trong một layer. Phương pháp này đảm bảo rằng số lượng prompts được chọn là như nhau trên tất cả các layer.
• **[CLS]-Sim:** Chúng tôi áp dụng sự tương tự self-attention giữa token prompt và token [CLS] như một kỹ thuật scoring được lấy cảm hứng từ một dòng công trình trước [25,30,10]. Ở đây, chúng tôi cũng chọn k% prompts có điểm số cao nhất trong một layer.

Trong Bảng 3, chúng tôi trình bày so sánh hiệu suất đạt được bằng cách sử dụng ba kỹ thuật prompt scoring khác nhau. Kết quả chứng minh rằng phương pháp scoring toàn cục đề xuất của chúng tôi, xem xét tầm quan trọng của prompts trên tất cả các layer, vượt trội hơn hai kỹ thuật scoring khác, đặc biệt đối với phần trăm PC thấp hơn (ví dụ 10%). Do đó, các phát hiện của chúng tôi cho thấy rằng một metric scoring toàn cục là cần thiết cho PC, vì tầm quan trọng của prompts thay đổi giữa các layer khác nhau.

**Phân bố prompt theo layer.** Chúng tôi trình bày visualization của phân bố prompt theo layer cho PC với các phần trăm prompts khác nhau (50%, 30%, và 10%) trong Hình 7. Số lượng prompts trung bình trong mỗi layer được tính toán trên tất cả các tập dữ liệu, và mean cùng standard deviation được báo cáo. Kết quả cho thấy rằng prompts ở các layer đầu có tác động tối thiểu đến độ chính xác đối với hầu hết các tập dữ liệu. Hơn nữa, việc giảm phần trăm prompts dẫn đến standard deviation cao hơn, ngụ ý rằng số lượng prompts tối ưu thay đổi giữa các tập dữ liệu. Do đó, một phương pháp PC toàn cục là cần thiết để xác định số lượng prompts tối ưu ở mỗi layer trên các tập dữ liệu khác nhau.

**Phân tích GPU Latency.** Chúng tôi phân tích thời gian latency thực tế của VPT với PC trên GPU. Về mặt lý thuyết, độ phức tạp của hoạt động self-attention tăng bậc hai khi độ dài token đầu vào tăng. Tuy nhiên, điều này có thể không đúng trong thực tế do các yếu tố như thông số kỹ thuật phần cứng [17,8]. Để điều tra lợi thế của PC đối với GPU latency, chúng tôi đo GPU latency cho 64 hình ảnh trên ba môi trường GPU khác nhau: Quadro RTX5000, V100, và A100. Chúng tôi thực hiện thí nghiệm trên ba tập dữ liệu với số lượng prompts ban đầu khác nhau (các tập dữ liệu StanfordCar, DMLab, và SVHN ban đầu có 200, 100, và 50 prompts tương ứng). Trong Bảng 4, chúng tôi quan sát thấy rằng PC đề xuất giảm GPU latency cho tất cả các cấu hình. Như chúng tôi mong đợi, hiệu quả của PC cao hơn trong trường hợp có số lượng prompts lớn hơn như Stanford Cars. Hơn nữa, PC toàn cục cho latency tương tự như PC cục bộ. Điều này củng cố thêm việc sử dụng điểm số quan trọng toàn cục đạt được độ chính xác cao hơn với chi phí tính toán không đáng kể. Chúng tôi đo FLOPs của VPT với PC trong Bảng 5 để hỗ trợ quan sát của chúng tôi về GPU latency, trong đó kết quả cho thấy xu hướng tương tự.

**Phân tích về Số lượng Epoch Fine-tuning.** Một siêu tham số quan trọng trong phương pháp của chúng tôi là số lượng epoch fine-tuning prompt (N_p trong Thuật toán 1). Tuy nhiên, thời gian fine-tuning dài hơn đi kèm với chi phí tính toán cao hơn, không tương thích với các kịch bản huấn luyện trên thiết bị. Để xác định số lượng epoch fine-tuning tối ưu, chúng tôi đo độ chính xác validation trung bình trên tất cả các tập dữ liệu downstream của VTAB-1K với 10% prompts. Như được hiển thị trong Hình 8(a), độ chính xác ổn định khoảng epoch 20. Dựa trên quan sát này, chúng tôi đặt số lượng epoch fine-tuning là 20 cho tất cả thí nghiệm. Ngoài ra, Hình 8(b) minh họa thời gian tính toán tương đối giữa huấn luyện VPT ban đầu, prompt scoring, và prompt fine-tuning (dòng 1, dòng 2, và dòng 4 trong Thuật toán 1 tương ứng). Kết quả chứng minh rằng phương pháp PC của chúng tôi (prompt scoring + fine-tuning) yêu cầu ít hơn 25% thời gian tính toán cần thiết cho huấn luyện VPT ban đầu. Những kết quả này cho thấy rằng phương pháp của chúng tôi rất phù hợp cho các kịch bản huấn luyện trên thiết bị.

**Triển khai Thực tế của Prompt Condensation.** Trong các ứng dụng thực tế, có thể không phải lúc nào cũng rõ ràng liệu có sự sụt giảm hiệu suất không tầm thường với một số lượng nhỏ prompts hay không. Trong những trường hợp như vậy, chúng ta có thể sử dụng một metric chi phí tính toán tương đối (tức là tỷ lệ [token hình ảnh ban đầu] so với [prompt + token hình ảnh ban đầu]) để quyết định có áp dụng Prompt Condensation (PC) hay không. Ví dụ, xem xét một kịch bản với 197 token ban đầu (196 + token [CLS]) và 100 token prompt. Trong trường hợp này, việc thêm prompts dẫn đến tăng chi phí tính toán 100/197 = 50.76%. Nếu việc bao gồm prompts dẫn đến chi phí tính toán bổ sung ≥K%, chúng ta có thể chọn triển khai PC. Nếu không, sẽ có lợi hơn khi bỏ qua PC.

## 7. Kết luận

Trong nghiên cứu này, mục tiêu của chúng tôi là điều tra ảnh hưởng của số lượng prompts đối với VPT và tác động của nó đối với cả chi phí tính toán và hiệu suất fine-tuning. Các phát hiện của chúng tôi cho thấy rằng việc giảm số lượng prompts khoảng 50% không ảnh hưởng đáng kể đến độ chính xác đã được fine-tuned, với phần lớn sự sụt giảm hiệu suất xảy ra trong khoảng 10% đến 40%. Ngoài ra, chúng tôi chứng minh rằng việc tăng số lượng prompts không tăng cường tuyến tính rank tối đa của các ma trận self-attention xấp xỉ. Đồng thời, chúng tôi đề xuất Prompt Condensation (PC), một kỹ thuật nén có thể phục hồi hiệu quả sự suy giảm hiệu suất gây ra bởi việc sử dụng một số lượng nhỏ prompts. Nhìn chung, chúng tôi hy vọng rằng phân tích và quan sát của chúng tôi có thể cung cấp thông tin chi tiết cho các nhà nghiên cứu trong việc thiết kế visual prompts.
