# 2208.08340.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2208.08340.pdf
# Kích thước file: 1891251 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
1
Điều chỉnh Prompt Kép Phương thức cho Mô hình Đã Huấn luyện trước Thị giác-Ngôn ngữ
Yinghui Xing*, Qirui Wu*, De ChengB, Shizhou Zhang, Guoqiang Liang, Peng Wang, Yanning Zhang.
Tóm tắt —Với sự xuất hiện của các mô hình thị giác-ngôn ngữ đã huấn luyện trước lớn như CLIP, các biểu diễn có thể chuyển đổi có thể được điều chỉnh cho một loạt các tác vụ hạ nguồn thông qua điều chỉnh prompt. Điều chỉnh prompt tìm kiếm thông tin có lợi cho các tác vụ hạ nguồn từ kiến thức tổng quát được lưu trữ trong mô hình đã huấn luyện trước. Một phương pháp được đề xuất gần đây có tên Context Optimization (CoOp) giới thiệu một tập hợp các vectơ có thể học được như là prompt văn bản từ phía ngôn ngữ. Tuy nhiên, việc điều chỉnh prompt văn bản một mình chỉ có thể điều chỉnh "bộ phân loại" tổng hợp, trong khi các đặc trưng hình ảnh được tính toán của bộ mã hóa hình ảnh không thể bị ảnh hưởng, dẫn đến các giải pháp không tối ưu. Trong bài báo này, chúng tôi đề xuất một mô hình điều chỉnh prompt kép phương thức (DPT) mới thông qua việc học prompt văn bản và thị giác đồng thời. Để làm cho đặc trưng hình ảnh cuối cùng tập trung nhiều hơn vào khái niệm thị giác mục tiêu, một lược đồ điều chỉnh prompt thị giác nhận thức lớp (CA VPT) được đề xuất thêm trong DPT của chúng tôi. Trong lược đồ này, prompt thị giác nhận thức lớp được tạo ra động bằng cách thực hiện sự chú ý chéo giữa các đặc trưng prompt văn bản và nhúng token vá hình ảnh để mã hóa cả thông tin liên quan đến tác vụ hạ nguồn và thông tin thể hiện thị giác. Các kết quả thực nghiệm rộng rãi trên 11 tập dữ liệu chứng minh tính hiệu quả và khả năng khái quát của phương pháp đề xuất. Mã của chúng tôi có sẵn tại https://github.com/fanrena/DPT.
Từ khóa chỉ mục —Học few-shot, Học chuyển giao, Phân loại hình ảnh, Điều chỉnh Prompt, Mô hình Thị giác-Ngôn ngữ

I. GIỚI THIỆU
Gần đây, các nghiên cứu về các mô hình thị giác-ngôn ngữ quy mô lớn (VLM), như CLIP [1] và ALIGN [2], đã đạt được tiến bộ đáng kể trong việc học biểu diễn [3]–[5]. Hưởng lợi từ lượng lớn dữ liệu hình ảnh-văn bản, mô hình thị giác-ngôn ngữ đã huấn luyện trước quy mô lớn có thể học các khái niệm thị giác mở được tạo ra từ ngôn ngữ tự nhiên, do đó cho phép chuyển giao zero-shot đến các tác vụ hạ nguồn. Cụ thể, mô hình thị giác-ngôn ngữ được cấu thành từ hai thành phần: bộ mã hóa hình ảnh và bộ mã hóa văn bản. Khi một tác vụ phân loại mới xuất hiện, người ta có thể tổng hợp bộ phân loại bằng cách đưa mô tả ngôn ngữ tự nhiên của các lớp vào bộ mã hóa văn bản.

--- TRANG 2 ---
2
mô-đun trích xuất đặc trưng và một bộ phân loại. Tương tự, quá trình đưa prompt văn bản vào bộ mã hóa văn bản có thể được xem như là sự tổng hợp của một bộ phán loại, và bộ mã hóa hình ảnh trích xuất
các đặc trưng hình ảnh. Giả sử rằng các mô hình thị giác-ngôn ngữ đã huấn luyện trước quy mô lớn đã nắm bắt được hầu hết kiến thức tổng quát (các khái niệm thị giác) cho các tác vụ hạ nguồn.
Cơ chế prompting thực hiện là truy vấn thông tin phù hợp, có lợi cho các tác vụ hạ nguồn, từ mô hình đã huấn luyện trước. Như thể hiện trong Hình 1, đối với một hình ảnh đầu vào có nhiều đối tượng thị giác (khái niệm), ví dụ, trường hợp đầu tiên chứa một người và một xe máy, bộ mã hóa hình ảnh trích xuất tất cả các đặc trưng hình ảnh của các đối tượng, tức là các bản đồ chú ý của Zero-Shot CLIP và CoOp đều làm nổi bật cả người và xe máy. Tuy nhiên, tác vụ hạ nguồn yêu cầu nhãn lớp đầu ra là "xe máy"—chú thích sự thật cơ bản.
CoOp cố gắng cho phép mô hình đưa ra "xe máy" bằng cách điều chỉnh chỉ "bộ phân loại" trong khi giữ nguyên các đặc trưng hình ảnh "người" và "xe máy" được làm nổi bật. Có sự đồng thuận trong cộng đồng thị giác rằng các đặc trưng có tầm quan trọng [10]! Do đó, chúng tôi tin rằng việc áp dụng điều chỉnh prompt chỉ cho bộ mã hóa văn bản trong khi sử dụng trực tiếp bộ mã hóa hình ảnh cố định cho tác vụ hạ nguồn là không tối ưu. Trong bài báo này, chúng tôi giới thiệu các prompt thị giác trong không gian đầu vào hình ảnh và đề xuất một mô hình điều chỉnh prompt kép phương thức (DPT) bằng cách học prompt văn bản và prompt thị giác cho cả bộ mã hóa văn bản và hình ảnh đồng thời nhằm điều chỉnh mô hình đã huấn luyện trước cho các tác vụ hạ nguồn bằng cách điều chỉnh cả "bộ phân loại" và "đặc trưng hình ảnh".

Cụ thể, đối với điều chỉnh prompt thị giác trong một bộ mã hóa hình ảnh dựa trên ViT, chúng tôi giới thiệu một số lượng nhỏ các tham số có thể huấn luyện trong đầu vào của các khối transformer trong khi giữ nguyên bộ mã hóa hình ảnh đã huấn luyện trước. Việc chèn prompt thị giác có thể điều chỉnh trực tiếp các nhúng token vá hình ảnh. đặc trưng hình ảnh, thông qua các trọng số tự chú ý và hấp thụ các vectơ giá trị được dẫn xuất từ prompt. Để làm cho mô hình đã huấn luyện trước chuyển giao tốt hơn cho tác vụ hạ nguồn, chúng tôi tiếp tục giới thiệu một cơ chế điều chỉnh prompt thị giác nhận thức lớp (CA VPT) vào khung DPT của chúng tôi để giúp đặc trưng hình ảnh cuối cùng thu được tập trung nhiều hơn vào khái niệm thị giác mục tiêu. Do đó, chúng tôi nhằm mục đích mã hóa cả thông tin liên quan đến tác vụ và thông tin thể hiện thị giác vào các prompt thị giác, Prompt thị giác nhận thức lớp được tạo ra động bằng cách thực hiện sự chú ý chéo giữa các đặc trưng prompt văn bản và nhúng vá hình ảnh thị giác và được kỳ vọng sẽ bao gồm các đặc trưng ngữ nghĩa phong phú hơn của các đối tượng thị giác mục tiêu. Do đó, đặc trưng hình ảnh cuối cùng được thu thập, được tính toán bằng cách hấp thụ thông tin từ các nhúng vá hình ảnh và các prompt thị giác nhận thức lớp của chúng tôi, có thể tập trung nhiều hơn vào các lớp tương ứng với các tác vụ hạ nguồn. Cuối cùng, mô hình DPT tổng thể được đề xuất được học với prompt văn bản, prompt thị giác và prompt thị giác nhận thức lớp đồng thời. Như thể hiện trong Hình 1, việc điều chỉnh các mô hình đã huấn luyện trước với DPT của chúng tôi cho thấy một vùng chú ý hình ảnh tập trung vào tác vụ hơn.

Những đóng góp chính của bài báo này có thể được tóm tắt theo ba khía cạnh sau:
• Phương pháp được đề xuất chứng minh một mô hình điều chỉnh prompt kép phương thức mới cho việc điều chỉnh mô hình thị giác-ngôn ngữ đã huấn luyện trước lớn bằng cách học đồng thời các prompt thị giác và văn bản từ cả hai đầu của bộ mã hóa văn bản và hình ảnh.
• Để khuyến khích các prompt thị giác chứa một cách rõ ràng thông tin liên quan đến tác vụ hạ nguồn, chúng tôi tiếp tục giới thiệu prompt thị giác nhận thức lớp vào DPT của chúng tôi. Nó được tạo ra động bằng cách thực hiện sự chú ý chéo giữa các đặc trưng prompt văn bản và nhúng token thị giác.
• Các kết quả thực nghiệm rộng rãi trên 11 tập dữ liệu chứng minh tính hiệu quả của phương pháp được đề xuất và cho thấy sự vượt trội so với các phương pháp điều chỉnh prompt khác với biên độ lớn, cũng như khả năng khái quát của nó.

Phần còn lại của bài báo này được tổ chức như sau. Phần II giới thiệu các công trình liên quan. Chi tiết về phương pháp đề xuất của chúng tôi được trình bày chi tiết trong Phần III. Trong Phần IV, chúng tôi báo cáo kết quả của các thí nghiệm toàn diện trên 11 tập dữ liệu được sử dụng trong điều chỉnh prompt, điều này chứng minh tính hiệu quả của phương pháp của chúng tôi. Cuối cùng, kết luận về công trình của chúng tôi được trình bày trong Phần V.

II. CÔNG TRÌNH LIÊN QUAN
A. Mô hình Đã Huấn luyện trước Thị giác-Ngôn ngữ
Việc học các biểu diễn thị giác dưới sự giám sát của ngôn ngữ tự nhiên đã được chứng minh là hiệu quả và đã thu hút nhiều sự chú ý [1], [2], [11], [12]. Đối với các mô hình thị giác-ngôn ngữ, khớp hình ảnh-văn bản và học đối lập đa phương thức là hai vấn đề quan trọng. Trong CLIP [1], hai bộ mã hóa liên quan đến các phương thức thị giác và ngôn ngữ được thiết kế, và những nhúng hình ảnh và văn bản này sau đó được căn chỉnh bằng cách sử dụng hàm mất mát entropy chéo đối xứng. Tương tự, ALIGN [2] cũng sử dụng kiến trúc bộ mã hóa kép, nhưng nó chiếu các nhúng hình ảnh và văn bản vào cùng một không gian ngữ nghĩa để tính toán điểm số tương tự giữa các phương thức thị giác và ngôn ngữ. Điều này làm cho tương tác thị giác-ngôn ngữ hiệu quả hơn. Cả hai mô hình này đều được huấn luyện trước trên các tập dữ liệu hình ảnh-văn bản quy mô lớn với hàm mất mát đối lập, và có thể được chuyển giao cho các tác vụ hạ nguồn. Nghiên cứu về việc chuyển giao CLIP cho các tác vụ hạ nguồn khác nhau, như phân loại hình ảnh [9], [13]–[15], truy xuất video-văn bản [16], theo dõi [17], và nhiều hơn nữa [18]–[21] đang phát triển mạnh mẽ. Để tăng cường hiệu suất của CLIP cho các tác vụ hạ nguồn, CLIP-Adapter [13] đã giới thiệu các bộ điều chỉnh đặc trưng trên nhánh thị giác hoặc ngôn ngữ và điều chỉnh chúng trên tác vụ phân loại few-shot. Zhang et al. [14] tiếp tục đề xuất một CLIP-Adapter không cần huấn luyện (tức là TIP-Adapter), tạo ra các trọng số bằng một mô hình cache key-value được xây dựng từ tập huấn luyện few-shot. Với ít huấn luyện hơn nhiều, TIP-Adapter hiệu quả hơn CLIP-Adapter. Như một khung thay thế để giảm khoảng cách giữa các hình thức mục tiêu của việc huấn luyện trước mô hình và tinh chỉnh, việc học dựa trên prompt đã trở thành một chủ đề tích cực trong cả cộng đồng NLP và thị giác máy tính. Tuy nhiên, sự khác biệt giữa hai phương thức khác nhau gây ra khó khăn trong việc điều chỉnh prompt. Gần đây, Zhou et al. [9] đã đề xuất một chiến lược tối ưu hóa ngữ cảnh (CoOp) để tự động học các prompt tối ưu, điều này làm tăng đáng kể độ chính xác nhận dạng. Công trình của chúng tôi cũng tập trung vào việc chuyển giao mô hình thị giác-ngôn ngữ đã huấn luyện trước cho các tác vụ hạ nguồn thông qua prompting.

B. Học Prompt
Học prompt bắt nguồn từ cộng đồng NLP [6], [7], [22] và ban đầu đề cập đến việc áp dụng một hàm cố định

--- TRANG 3 ---
3
Bộ chọn
Một bức ảnh của một con chó
Một bức ảnh của một con mèo ZSCLIP
Bộ mã hóa văn bản ZSCLIP
Bộ mã hóa hình ảnh Sắp xếp & Chọn
Độ tương tự
Cosine Đặc trưng
Prompt văn bản
... Top- K N Đặc trưng
Prompt văn bản
Prompt
Thị giác Nhúng
Vá hình ảnh ICLS Token
Lớp hình ảnh TCLS Token
Lớp văn bản Prompt
Văn bản Prompt
Nhận thức lớp × n
. . . Lớp Transformer Bộ mã hóa văn bản
Đặc trưng hình ảnh Đặc trưng
Prompt văn bản
Điểm tương tự. . . TCLS
...
...
. . . . . .. . . . . .
ICLS ICLS Chó Mèo . . . Bộ chọn. . .
Trình tạo
CA VPT Lớp Transformer . . .
Đặc trưng prompt văn bản
top- K N

Hình 2. Kiến trúc tổng thể của phương pháp DPT đề xuất của chúng tôi. Nó bao gồm ba thành phần có thể học: prompt văn bản, prompt thị giác và prompt thị giác nhận thức lớp được tạo ra từ một mô-đun trình tạo Điều chỉnh Prompt Thị giác Nhận thức Lớp (CA VPT) có kiến trúc chi tiết được minh họa trong Hình 3.

cho các token đầu vào, cung cấp hướng dẫn về tác vụ cho mô hình. Trong cộng đồng thị giác máy tính, học prompt đã được khám phá trong cả mô hình thị giác [23]–[25] và mô hình thị giác-ngôn ngữ [1], [9], [15], [18], [26]. Đặc biệt, điều chỉnh prompt thị giác (VPT) [23] đã đạt được những cải thiện hiệu suất đáng kể chỉ với một lượng nhỏ tham số bổ sung, tức là các prompt, trong không gian đầu vào. Các mô hình thị giác-ngôn ngữ đã được nghiên cứu trong phân loại hình ảnh [9], [15], [26]–[29], nhận dạng video [30], và học đa phương thức [31]–[33]. Trong số đó, CoOp [9] đạt được tối ưu hóa prompt liên tục từ dữ liệu hạ nguồn để điều chỉnh các mô hình thị giác-ngôn ngữ đã huấn luyện trước. Tuy nhiên, CoOp có thể giới thiệu các bước điều chỉnh prompt không phù hợp, điều này có thể cản trở việc thăm dò kiến thức tổng quát [26]. Để cải thiện khả năng khái quát của CLIP, Zhu et al. [26] đã đề xuất một phương pháp điều chỉnh prompt mới, cụ thể là ProGrad, để giải quyết xung đột giữa mỗi bước điều chỉnh và kiến thức tổng quát mà CLIP đã dự đoán. Conditional CoOp (CoCoOp) [15] mở rộng CoOp bằng cách học một token có điều kiện đầu vào cho mỗi hình ảnh để cải thiện khả năng khái quát đa lĩnh vực của CoOp. Được thúc đẩy bởi thực tế là hàm mất mát đối lập có thể cải thiện khả năng khái quát của mô hình, Sahoo et al. [34] đã giới thiệu một phương pháp điều chỉnh prompt đối lập. Nó tăng cường hàm mất mát entropy chéo tiêu chuẩn với hai hạng mục mất mát đối lập bổ sung để học các prompt có thể khái quát mà không giới thiệu bất kỳ tham số bổ sung nào. Lu et al. [27] đã học các nhúng đầu ra của prompt thay vì các nhúng đầu vào và sử dụng phân phối Gaussian để mô hình hóa chúng một cách hiệu quả. Bahng et al. [28] đã đề xuất một phương pháp prompting cho mạng CNN để điều chỉnh các mô hình thị giác-ngôn ngữ đã huấn luyện trước cho các tác vụ hạ nguồn. Ngược lại, Zhang et al. [29] đã sử dụng thuật toán tìm kiếm kiến trúc mạng thần kinh để xác định cấu hình tối ưu với các adapter và prompt như là các thành phần nhỏ.

Hầu hết các phương pháp hiện có điều chỉnh prompt chỉ trong các bộ mã hóa văn bản và bỏ qua các manh mối trong đặc trưng hình ảnh. Công trình của chúng tôi đề xuất một mô hình điều chỉnh prompt kép phương thức, giới thiệu cả prompt văn bản và prompt thị giác cho mô hình thị giác-ngôn ngữ. Hơn nữa, một prompt thị giác nhận thức lớp được đề xuất để cho phép đặc trưng hình ảnh chú ý nhiều hơn đến đối tượng tiền cảnh mục tiêu cho các tác vụ hạ nguồn.

C. Học Chuyển giao
Hưởng lợi từ quy mô lớn của dữ liệu được chú thích, hiệu suất của các mạng thần kinh sâu đã được tăng cường đáng kể. Tuy nhiên, do chi phí ghi nhãn, việc thu thập các tập dữ liệu huấn luyện quy mô lớn với các chú thích chính xác là khó khăn [14]. Học chuyển giao [35]–[39] nhằm mục đích chuyển giao kiến thức tổng quát từ một lĩnh vực sang các lĩnh vực liên quan với dữ liệu huấn luyện hạn chế, đã được chứng minh là một giải pháp khả thi cho việc học few-shot [40]–[47]. Một số công trình đã cố gắng điều chỉnh một số lượng nhỏ tham số trong khi giữ hầu hết các tham số của mô hình đã huấn luyện trước được đóng băng. Ví dụ, [37] điều chỉnh mạng đã huấn luyện trước bằng cách huấn luyện một mạng phụ nhẹ được hợp nhất với mạng đã huấn luyện trước đóng băng thông qua phép cộng. [38] đã đề xuất một mô-đun bias hiệu quả bộ nhớ mới, tức là mô-đun dư nhẹ, để tinh chỉnh bộ trích xuất đặc trưng bằng cách học các bản đồ đặc trưng dư nhỏ. Rebuffi et al. [39] đã giới thiệu một adapter dư vào mô hình và chỉ huấn luyện mạng adapter để cải thiện độ chính xác của các biểu diễn đặc thù cho lĩnh vực.

Mặt khác, một số phương pháp dựa trên học tự giám sát, như MoCo [48], BYOL [49], và MAE [50], cũng có thể giảm bớt yêu cầu về dữ liệu huấn luyện quy mô lớn. Gần đây, các mô hình thị giác-ngôn ngữ được huấn luyện trước trên các cặp hình ảnh-văn bản quy mô lớn đã chứng minh sự vượt trội của chúng. Do đó, việc khai thác tiềm năng của những mô hình này cho các tác vụ hạ nguồn là rất quan trọng. Bài báo này tập trung vào việc chuyển giao

--- TRANG 4 ---
4
kiến thức học được từ chúng đến các tác vụ hạ nguồn thông qua prompting.

III. PHƯƠNG PHÁP LUẬN
Trong phần này, trước tiên chúng tôi xem xét lại mô hình CLIP. Sau đó, chúng tôi trình bày chi tiết từng thành phần của mô hình điều chỉnh prompt kép phương thức (DPT) được đề xuất, bao gồm prompt văn bản, prompt thị giác và prompt thị giác nhận thức lớp. Khung của DPT được đề xuất của chúng tôi được minh họa trong Hình 2. Cuối cùng, chúng tôi cung cấp hàm mất mát của DPT và một chiến lược khởi động để tăng tốc quá trình huấn luyện.

A. Mô hình Học Đối lập Ngôn ngữ-Hình ảnh (CLIP)
Mô hình CLIP nhằm mục đích căn chỉnh không gian đặc trưng hình ảnh và không gian đặc trưng văn bản, điều này cho phép mô hình có khả năng chuyển giao zero-shot đến các tác vụ hạ nguồn. CLIP được cấu thành từ hai bộ mã hóa: một được thiết kế cho hình ảnh, và một khác được thiết kế cho văn bản. Bộ mã hóa văn bản áp dụng transformer [51] để mã hóa thông tin văn bản. Bộ mã hóa hình ảnh có thể là một mô hình CNN, như ResNet [5], hoặc một transformer thị giác, như ViT [52]. Trong phương pháp của chúng tôi, chúng tôi chọn ViT làm bộ mã hóa hình ảnh để tương thích với prompt thị giác trong [23].

Với số lượng khổng lồ 400 triệu cặp mẫu hình ảnh-văn bản, CLIP được huấn luyện dưới khung học đối lập, trong đó hình ảnh và văn bản liên kết được coi là các mẫu tích cực, trong khi các mẫu không liên kết được coi là các mẫu tiêu cực. Sau đó, tất cả các tham số của mô hình CLIP đã huấn luyện trước được giữ đóng băng cho các tác vụ hạ nguồn mà không cần bất kỳ tinh chỉnh nào. Trong các tác vụ hạ nguồn, một prompt được tạo thủ công được đưa vào đầu văn bản để tổng hợp một bộ phân loại tuyến tính zero-shot bằng cách nhúng các tên lớp của tập dữ liệu mục tiêu. Lấy tác vụ phân loại làm ví dụ, token "[CLASS]" có thể được mở rộng trước bằng một template, như "a photo of a [CLASS]". Sau đó, câu được coi là một prompt và được mã hóa bởi bộ mã hóa văn bản để tạo ra một vectơ trọng số wi, i={1, ..., K}, trong đó K là tổng số danh mục. Đồng thời, các đặc trưng hình ảnh x được thu thập bởi bộ mã hóa hình ảnh. Xác suất dự đoán có thể được tính toán bằng

p(y=i|x) = exp(sim(x,wi)/τ) / ∑_{j=1}^K exp(sim(x,wj)/τ), (1)

trong đó sim(·,·) biểu thị tính toán độ tương tự cosine, và τ là hệ số nhiệt độ được học bởi CLIP.

B. Prompt Văn bản và Prompt Thị giác
Prompt Văn bản. Được biết rằng các prompt được tạo thủ công cho mô hình CLIP có thể mất thời gian đáng kể và yêu cầu chuyên môn để điều chỉnh từ ngữ, vì một thay đổi nhỏ trong cách diễn đạt có thể dẫn đến suy giảm hiệu suất đáng kể. Được thúc đẩy bởi việc điều chỉnh prompt trong các mô hình NLP, CoOp [9] đã giới thiệu một tập hợp các vectơ nhúng từ có thể điều chỉnh để học các prompt ưa thích máy cho đầu văn bản, mà chúng tôi gọi là prompt văn bản. Gọi {u1,u2, ...,uM} biểu thị M vectơ ngữ cảnh có thể học, và nhúng từ của token lớp văn bản được biểu thị bởi ci, i={1, ..., K}; sau đó, prompt cho lớp thứ i có thể được biểu thị là ti={u1,u2, ...,uM,ci}. Bằng cách chuyển tiếp ti vào bộ mã hóa văn bản g(·), chúng ta có thể thu được một vectơ trọng số phân loại cho khái niệm thị giác thứ i. Xác suất dự đoán tương ứng có thể được tính toán bằng

p(y=i|x) = exp(sim(x, g(ti))/τ) / ∑_{j=1}^K exp(sim(x, g(tj))/τ), (2)

trong đó x biểu thị các đặc trưng hình ảnh được trích xuất, và g(·) biểu thị bộ mã hóa văn bản.

Prompt Thị giác. Đối với các mô hình thị giác-ngôn ngữ, có hai bộ mã hóa cho các phương thức thị giác và ngôn ngữ. Việc điều chỉnh prompt văn bản một mình không đủ để giảm khoảng cách giữa các tác vụ đã huấn luyện trước và hạ nguồn, dẫn đến kết quả không tối ưu. Được thúc đẩy bởi điều chỉnh prompt thị giác (VPT) [23] được đề xuất để điều chỉnh các transformer thị giác, chúng tôi giới thiệu một prompt thị giác vào bộ mã hóa hình ảnh của mô hình CLIP. Các vá hình ảnh

Ij ∈ R^{3×h×w} | j ∈ N, 1 ≤ j ≤ Np

được nhúng đầu tiên vào một không gian tiềm ẩn d-chiều như sau:

ej^0 = Embed(Ij) ej^0 ∈ R^d, j = 1,2, ..., Np. (3)

Gọi El = {ej^l ∈ R^d | j ∈ N, 1 ≤ j ≤ Np} và Pl = {pi^l ∈ R^d | i ∈ N, 1 ≤ i ≤ P} biểu thị một tập hợp các nhúng vá hình ảnh và prompt thị giác cho lớp transformer thứ l, tương ứng. Giả sử sl ∈ R^d là một token lớp có thể học trong bộ mã hóa hình ảnh, khác với token lớp văn bản được sử dụng trong prompt văn bản vì token sau là một nhúng từ liên quan đến danh mục. Có hai phiên bản của prompt thị giác, VPT-Shallow và VPT-Deep, trong [23]. Chúng tôi đã tìm thấy theo kinh nghiệm rằng VPT-Deep có thể đạt được hiệu suất vượt trội (xem Bảng I), và do đó chúng tôi sử dụng VPT-Deep trong việc thực hiện của chúng tôi trong Phần IV.

Prompt thị giác được giới thiệu cho mỗi lớp transformer, tức là:

[sl, El] = Φl([sl-1, Pl-1, El-1]), l = 1,2, ..., L. (4)

--- TRANG 5 ---
5
BẢNG I
KẾT QUẢ CHÍNH CỦA 11 TẬP DỮ LIỆU DƯỚI CÀI ĐẶT 16-SHOTS.

Các phương pháp | EuroSAT | Caltech101 | Oxford Flowers | Food101 | FGVC Aircraft | DTD | OxfordPets | Stanford Cars | Sun397 | UCF101 | ImageNet | Trung bình
---|---|---|---|---|---|---|---|---|---|---|---|---
ZSCLIP [1] | 45.49 | 91.28 | 66.63 | 80.62 | 19.08 | 44.03 | 87.38 | 60.19 | 62.06 | 63.52 | 59.61 | 61.81
CoOp [9] | 83.12 | 94.45 | 95.07 | 78.20 | 33.94 | 67.20 | 88.88 | 75.79 | 72.31 | 79.10 | 66.55 | 75.87
CoCoOp [15] | 74.99 | 94.01 | 79.97 | 82.36 | 23.64 | 59.34 | 90.98 | 64.25 | 69.75 | 73.13 | 65.07 | 70.68
ProGrad [26] | 82.49 | 95.18 | 94.60 | 81.15 | 32.50 | 65.98 | 90.43 | 74.85 | 73.22 | 78.52 | 66.60 | 75.96
ProDA [27] | 83.28 | 95.5 | 95.98 | 81.89 | 34.68 | 70.76 | 90.6 | 77.64 | 75.07 | 81.85 | 67.62 | 77.72
VPT | 92.17 | 94.85 | 93.80 | 81.29 | 39.98 | 67.16 | 90.32 | 72.03 | 69.84 | 80.17 | 64.17 | 76.89
VLP | 91.90 | 95.10 | 96.05 | 78.42 | 42.92 | 68.06 | 90.33 | 78.81 | 72.12 | 82.04 | 66.91 | 78.42
DPT | 91.16 | 95.61 | 96.60 | 79.25 | 48.37 | 70.16 | 91.22 | 82.55 | 70.97 | 81.43 | 66.85 | 79.47

Nhìn chung, hiệu suất có tương quan tích cực với độ sâu prompt. Do đó, chúng tôi sử dụng VPT-Deep trong mô hình của chúng tôi. sL sau đó được chiếu bởi một lớp chiếu tuyến tính LP để thu được đặc trưng hình ảnh cuối cùng. Để đơn giản, toàn bộ quá trình trích xuất đặc trưng hình ảnh có thể được biểu thị bằng

x' = f([s0, P0, ⋯, PL, E0]), (5)

trong đó f(·) biểu thị bộ mã hóa hình ảnh.

Lưu ý rằng quá trình tính toán của bộ mã hóa hình ảnh, tức là mô hình ViT, có thể được xem như một quá trình suy luận cảnh toàn cục, và sl gộp các khái niệm thị giác từ các nhúng vá hình ảnh từng lớp. Với sự trợ giúp của prompt thị giác, khái niệm thị giác mục tiêu tương ứng với tác vụ hạ nguồn có thể được làm nổi bật thêm trong sl thông qua hoạt động tự chú ý trong mỗi lớp transformer. Bằng cách chèn prompt thị giác vào mỗi lớp transformer, hoạt động tự chú ý cho sl có thể bị ảnh hưởng theo hai cách, vì cả key và value đều được thêm vào đầu thông qua prompt thị giác: 1) Trọng số chú ý có thể bị ảnh hưởng để cho phép sl tập trung nhiều hơn vào các nhúng vá hình ảnh, bao gồm khái niệm mục tiêu; 2) Các prompt thị giác cũng phục vụ như các vectơ giá trị cho hoạt động tự chú ý và do đó sl có thể hấp thụ thông tin bổ sung mà prompt thị giác đã học.

Tuy nhiên, các prompt thị giác ngây thơ được thiết kế như các vectơ có thể học không bị ràng buộc, và chúng chỉ có thể học một số thông tin liên quan đến tác vụ hạ nguồn một cách ngầm định bằng cách điều chỉnh prompt trên các tập dữ liệu tác vụ hạ nguồn. Trong công trình này, chúng tôi đề xuất điều chỉnh prompt thị giác nhận thức lớp (CA VPT) để tạo ra prompt thị giác bằng cách sử dụng cả thông tin liên quan đến tác vụ từ phía văn bản và thông tin theo từng thể hiện từ phía thị giác.

C. Điều chỉnh Prompt Thị giác Nhận thức Lớp
Prompt thị giác nhận thức lớp nhằm mục đích mã hóa thông tin liên quan đến tác vụ một cách rõ ràng. Trình tạo CA VPT của chúng tôi nhận hai phía đầu vào, thông tin theo từng thể hiện từ phía thị giác và thông tin liên quan đến tác vụ từ phía văn bản. Các đặc trưng prompt văn bản được tính toán bởi bộ mã hóa văn bản với tất cả các token lớp văn bản thể hiện tốt thông tin liên quan đến tác vụ. Tuy nhiên, khi chúng tôi nhập các đặc trưng prompt văn bản với tất cả các token lớp văn bản vào trình tạo CA VPT, độ phức tạp tính toán của trình tạo CA VPT tăng tuyến tính với số lượng lớp trên mỗi tác vụ hạ nguồn. Để giảm độ phức tạp tính toán của trình tạo CA VPT của chúng tôi thành hằng số, chúng tôi chọn top-KN các đặc trưng prompt văn bản với sự trợ giúp của mô-đun Suy luận CLIP Zero-Shot (phần bên phải của Hình 2). Lưu ý rằng hiệu suất cuối cùng không nhạy cảm với KN và hiệu suất cuối cùng dao động chỉ với 0.1%∼0.2% khi thiết lập KN khác nhau. Sau đó, việc đưa prompt văn bản với top-KN token lớp văn bản [CLASS] vào bộ mã hóa văn bản tạo ra KN vectơ đặc trưng, tức là gj ∈ RD, 1 ≤ j ≤ KN, trong đó thông tin liên quan đến tác vụ được mã hóa.

Một prompt thị giác nhận thức lớp được tạo ra động bằng cách thực hiện sự chú ý chéo giữa các đặc trưng prompt văn bản từ phía văn bản và các đầu vào của khối transformer từ phía thị giác, như minh họa trong Hình 3.

Sau khi ánh xạ của một lớp kết nối đầy đủ, chúng ta có thể thu được KN vectơ truy vấn qj ∈ Rd, 1 ≤ j ≤ KN. Các vectơ key và value k ∈ Rn×d đều được thu thập từ các đầu vào của lớp transformer thị giác tương ứng, bao gồm nhúng vá hình ảnh, nhúng token lớp hình ảnh, và prompt thị giác trong đó n biểu thị tổng số của chúng. Prompt thị giác nhận thức lớp được đề xuất của chúng tôi P̃j^l ∈ Rd cho lớp thứ l được tính toán như sau:

oj^l = Softmax(qjWq(kWk)T / √dk)kWv, 1 ≤ j ≤ KN, (6)

P̃j^l = LN(oj^l + qj), 1 ≤ j ≤ KN, (7)

trong đó LN(·) biểu thị chuẩn hóa lớp. Wq ∈ Rd×dk, Wk ∈ Rd×dk, và Wv ∈ Rd×d biểu thị các tham số của sự chú ý chéo.

Để đảm bảo hiệu quả của prompt thị giác nhận thức lớp, chúng tôi giới thiệu thêm một bộ phân loại K-chiều trên đầu của KN đầu ra của lớp LN, và hàm mất mát entropy chéo được áp dụng trên các logit K-chiều như sau:

L^ca_ce = -∑_i yi log pi, 1 ≤ i ≤ K, (8)

trong đó pi biểu thị logit thứ i từ việc phân loại LN(oj^l), K biểu thị số lượng lớp và y biểu thị mã hóa một nóng cho lớp mục tiêu sự thật cơ bản. Lưu ý rằng chỉ có oj^l được dẫn xuất từ qj, tương ứng với lớp mục tiêu sự thật cơ bản, sẽ được phân loại.

Vì nhúng token lớp hình ảnh trong các lớp sâu hơn thường chứa thông tin ngữ nghĩa liên quan đến tác vụ nhiều hơn, prompt thị giác nhận thức lớp chỉ được áp dụng cho một vài lớp cuối cùng của bộ mã hóa hình ảnh trong việc thực hiện của chúng tôi.

--- TRANG 6 ---
6
[Biểu đồ và hình ảnh thể hiện kết quả thí nghiệm trên các tập dữ liệu khác nhau với số lượng mẫu huấn luyện từ 1, 2, 4, 8, 16 shots]

Hình 4. Kết quả chính trên 11 tập dữ liệu với 1,2,4,8,16 shots với ViT-B/32. Lưu ý rằng chúng tôi cũng so sánh phương pháp của chúng tôi với CPT [33] về độ chính xác trung bình.

D. Huấn luyện DPT
Một hàm mất mát entropy chéo được áp dụng để tối thiểu hóa khoảng cách giữa chú thích sự thật cơ bản và xác suất dự đoán được tính toán bởi Phương trình (2).

L_ce = -∑_i yi log p(y=i|x''), 1 ≤ i ≤ K, (9)

trong đó y biểu thị chú thích sự thật cơ bản, p(y=i|x'') biểu thị xác suất dự đoán từ Phương trình (2), và x'' là đặc trưng hình ảnh cuối cùng thu được:

x'' = f([s0, P0, ⋯, Pl, P̃j^{l+1}, ⋯, P̃j^L, E0]), (10)

Hàm mất mát tổng kết hợp hai hàm mất mát entropy chéo với một siêu tham số cân bằng α như sau:

L_total = αL^ca_ce + L_ce. (11)

--- TRANG 7 ---
7
E. Chiến lược Khởi động
Để tăng tốc quá trình huấn luyện, chúng tôi áp dụng một chiến lược khởi động được hướng dẫn bởi kiến thức tổng quát trong vài epoch đầu tiên của huấn luyện. Xem xét rằng mô hình CLIP lưu trữ kiến thức tổng quát, chúng tôi huấn luyện mô hình của chúng tôi để học từ CLIP zero-shot. Hàm mất mát chúng tôi sử dụng cho vài epoch đầu tiên có thể được mô tả như sau:

L = L_coop + L_vpt + βL_ce + αL^ca_ce (12)

trong đó L_coop là hàm mất mát chúng tôi sử dụng cho huấn luyện CoOp, L_vpt là hàm mất mát chúng tôi sử dụng trong huấn luyện VPT, và L_ce là hàm mất mát chúng tôi sử dụng trong huấn luyện VLP. β là một siêu tham số cân bằng. Đối với L_coop, chúng tôi sử dụng hàm mất mát entropy chéo để tối thiểu hóa khoảng cách giữa chú thích sự thật cơ bản và xác suất dự đoán được tính toán bởi Phương trình (2).

L_coop = -∑_i yi log p(y=i|x), 1 ≤ i ≤ K, (13)

Đối với L_vpt, xác suất dự đoán được tính toán bởi Phương trình (1) thay vì Phương trình (2).

L_vpt = -∑_i yi log p(y=i|x''), 1 ≤ i ≤ K, (14)

Bằng cách thay đổi hàm mất mát L_ce trong vài epoch đầu tiên của huấn luyện thành Phương trình (12), chúng tôi sử dụng kiến thức tổng quát để hướng dẫn quá trình khởi động. Trong quá trình huấn luyện, DPT được đề xuất giữ toàn bộ các tham số của cả bộ mã hóa hình ảnh và văn bản cố định trong khi tối ưu hóa Prompt văn bản, Prompt thị giác và các tham số để tạo ra prompt thị giác nhận thức lớp.

F. Thảo luận về CA VPT
Hình 3 minh họa quá trình tính toán chi tiết của các prompt thị giác nhận thức lớp được đề xuất. Như thể hiện trong Hình 3, trình tạo CA VPT nhận hai loại đầu vào. Các đặc trưng prompt văn bản từ phía văn bản bao gồm thông tin liên quan đến tác vụ, trong khi các nhúng vá hình ảnh từ phía hình ảnh biểu thị thông tin thể hiện thị giác. Đầu tiên, trình tạo CA VPT thực hiện sự chú ý chéo giữa các đặc trưng prompt văn bản và nhúng vá hình ảnh, trong đó các vectơ truy vấn được ánh xạ từ các đặc trưng prompt văn bản trong khi các key và value được dẫn xuất từ các nhúng vá hình ảnh. Thông qua hoạt động chú ý chéo, những nhúng vá hình ảnh bao gồm nhiều thông tin ngữ nghĩa hơn về các đối tượng thuộc về các lớp của các tác vụ hạ nguồn sẽ được làm nổi bật nhiều hơn. Kết quả là, các đầu ra của sự chú ý chéo sẽ bao gồm nhiều đặc trưng hơn của các đối tượng sự thật cơ bản. Sau đó, các prompt thị giác nhận thức lớp của chúng tôi được tạo ra thêm với một hoạt động "Add and Norm" bổ sung tương tự như một lớp transformer điển hình. Vì các prompt thị giác nhận thức lớp của chúng tôi bao gồm các đặc trưng ngữ nghĩa phong phú hơn của các đối tượng mục tiêu sự thật cơ bản, đặc trưng hình ảnh cuối cùng thu được, được tính toán bằng cách hấp thụ thông tin từ các nhúng vá hình ảnh và các prompt thị giác nhận thức lớp của chúng tôi, có thể tập trung nhiều hơn vào các lớp tương ứng với các tác vụ hạ nguồn.

IV. THỰC NGHIỆM
A. Tập dữ liệu và Chi tiết Thực hiện.
Để đánh giá tính hiệu quả của phương pháp của chúng tôi, chúng tôi đã tiến hành thực nghiệm trên 11 tập dữ liệu phân loại, cụ thể là EuroSAT [53], Caltech101 [54], OxfordFlowers [55], Food101 [56], FGVCAircraft [57], DTD [58], OxfordPets [59], StanfordCars [60], ImageNet1K [61], Sun397 [62], và UCF101 [63], như trong [1], [9]. Những tập dữ liệu này bao gồm một loạt rộng các tác vụ thị giác máy tính, bao gồm phân loại hình ảnh về các đối tượng chung, danh mục tinh tế, vệ tinh, kết cấu, hiểu biết cảnh và hình ảnh nhận dạng hành động.

Theo các giao thức đánh giá few-shot được sử dụng phổ biến trong CLIP [1], chúng tôi cũng áp dụng 1, 2, 4, 8, và 16 shots cho huấn luyện mô hình và kiểm tra chúng trên toàn bộ tập dữ liệu kiểm tra. Các kết quả được báo cáo là trung bình của ba lần chạy để so sánh công bằng.

Chúng tôi áp dụng ViT-Base/32 làm mạng xương sống cho tất cả các thí nghiệm. Tất cả các thí nghiệm được tiến hành dựa trên mã được phát hành chính thức của CoOp [9] và mã được phát hành chính thức của CLIP [1]. Đối với VPT, độ dài prompt được đặt là 10 cho mỗi lớp của mạng, và chúng được khởi tạo giống như prompt văn bản trong CoOp [9]. Trong quá trình huấn luyện mô hình, chúng tôi áp dụng phương pháp tối ưu hóa SGD, và tốc độ học được giảm theo quy tắc cosine. Epoch tối đa cho VPT giống với CoOp [9]. Kỹ thuật khởi động được áp dụng trong 10 epoch đầu tiên với tốc độ học cố định 10^{-5} trên VPT. Tốc độ học cho VPT được tìm kiếm đầu tiên trong {0.01, 0.001, 0.0001, 0.00001} và giữ không đổi cho prompt thị giác trong tất cả các thí nghiệm. Đối với prompt văn bản, chúng tôi theo CoOp [9] với độ dài ngữ cảnh là 16.

Đối với các phương pháp VLP và DPT được đề xuất của chúng tôi, số epoch tối đa được đặt là 100 cho 16/8/4/2 shots, 60 epoch cho 1 shot (số epoch tối đa được đặt là 20 cho ImageNet.) ngoại trừ Caltech101 và OxfordPets trong DPT, được đặt là 60 cho kịch bản 16-shot. Kỹ thuật khởi động giống với CoOp và VPT (epoch khởi động được đặt là 1 cho ImageNet trên cả hai đầu). KN được đặt là 10. CA VPT được chèn vào lớp cuối cùng của bộ mã hóa hình ảnh.

Tham số cân bằng α được đặt là 0.3, và β được đặt là 0.1. Đối với huấn luyện sớm của VLP và DPT, kiến thức tổng quát được sử dụng làm hướng dẫn cho 30 epoch đầu tiên (10 cho ImageNet).

B. So sánh với các Phương pháp Hiện có
Các phương pháp điều chỉnh prompt đại diện hiện có bao gồm phương pháp CoOp đáng chú ý [9], và chính mô hình CLIP [1] được sử dụng cho phân loại zero-shot (tức là Zero-Shot Clip). Do đó, chúng tôi áp dụng hai mô hình này làm phương pháp so sánh chính.

Vì DPT của chúng tôi giới thiệu thêm prompt thị giác và prompt thị giác nhận thức lớp so với chỉ prompt văn bản trong CoOp, để tiết lộ cách mỗi thành phần đóng góp vào việc cải thiện hiệu suất, chúng tôi thực hiện thêm VPT và VLP ngoài DPT như sau:

• VPT có nghĩa là giới thiệu chỉ một prompt thị giác ngây thơ vào đầu thị giác của mô hình CLIP [1] và prompt văn bản được tạo thủ công

--- TRANG 8 ---
8
BẢNG II
KẾT QUẢ CỦA 11 TẬP DỮ LIỆU DƯỚI CÀI ĐẶT 16-SHOTS VỚI VIT-B/16.

Các phương pháp | EuroSAT | Caltech101 | Oxford Flowers | Food101 | FGVC Aircraft | DTD | OxfordPets | Stanford Cars | Sun397 | UCF101 | ImageNet | Trung bình
---|---|---|---|---|---|---|---|---|---|---|---|---
ZSCLIP [1] | 47.69 | 93.75 | 70.69 | 85.97 | 24.81 | 43.09 | 89.07 | 65.55 | 62.61 | 67.54 | 64.51 | 65.03
CoOp [9] | 83.74 | 95.17 | 96.73 | 84.17 | 44.06 | 69.60 | 92.07 | 82.73 | 74.54 | 82.59 | 71.62 | 79.73
CoCoOp [15] | 72.07 | 95.71 | 88.74 | 87.37 | 30.09 | 62.53 | 93.33 | 71.60 | 72.36 | 77.90 | 70.38 | 74.73
ProGrad [26] | 84.29 | 95.89 | 96.30 | 86.68 | 41.23 | 68.83 | 93.25 | 81.71 | 75.10 | 81.16 | 71.94 | 79.67
ProDA [27] | 85.17 | 96.23 | 97.54 | 87.29 | 44.40 | 72.46 | 93.42 | 83.89 | 77.19 | 85.12 | 72.73 | 81.40
VPT | 92.67 | 96.27 | 96.59 | 87.03 | 51.11 | 71.26 | 92.76 | 81.44 | 72.93 | 85.19 | 69.98 | 81.57
VLP | 91.87 | 96.08 | 97.37 | 84.57 | 52.99 | 72.20 | 93.11 | 85.62 | 74.48 | 86.36 | 72.46 | 82.46
DPT | 92.10 | 96.06 | 97.59 | 85.00 | 57.85 | 72.65 | 93.45 | 88.24 | 74.29 | 85.31 | 72.49 | 83.18

prompt, ví dụ, "a photo of a [CLASS]", được áp dụng cho đầu văn bản.

• VLP biểu thị mô hình điều chỉnh prompt kép phương thức để học đồng thời prompt thị giác (V) và văn bản (L), trong đó prompt văn bản được thiết kế giống như trong CoOp [9], và prompt thị giác hoàn toàn giống như VPT-Deep trong VPT [23].

• DPT chỉ ra rằng chúng tôi tiếp tục tích hợp CA VPT vào bộ mã hóa hình ảnh dựa trên VLP.

Kết quả đánh giá tổng thể được thể hiện trong Hình 4, báo cáo độ chính xác phân loại trên 11 tập dữ liệu dưới tất cả các cài đặt few-shot. So với các phương pháp cơ sở, DPT của chúng tôi đạt được hiệu suất vượt trội trung bình trên 11 tập dữ liệu. Hình 4 và Bảng I rõ ràng thể hiện như sau: 1) DPT vượt trội hơn CoOp và zero-shot CLIP với biên độ lớn. Sự gia tăng hiệu suất cơ bản tỷ lệ thuận với số lượng shots. Cụ thể, DPT vượt trội hơn zero-shot CLIP 17.6% và vượt trội hơn CoOp 3.53% trung bình trên 11 tập dữ liệu dưới cài đặt 16-shot. Kết quả đã xác minh sự vượt trội của mô hình DPT được đề xuất. 2) So sánh kết quả của VPT với CoOp, VPT thu được kết quả tốt hơn CoOp trung bình. Dưới cài đặt 16-shot, VPT có thể thu được 1% cải thiện hiệu suất trung bình trên tất cả 11 tập dữ liệu. Điều này cho thấy rằng việc điều chỉnh prompt thị giác từ đầu hình ảnh thay vì prompt văn bản có thể thu được kết quả hiệu quả hơn. Đáng chú ý rằng VPT và CoOp thu được kết quả không nhất quán trên các tập dữ liệu khác nhau, điều này chỉ ra rằng việc điều chỉnh prompt thị giác và prompt văn bản có thể có hiệu ứng bổ sung. 3) So sánh kết quả của VLP với những của VPT và CoOp, VLP đạt được kết quả tốt hơn VPT và CoOp, điều này cho thấy rằng việc điều chỉnh prompt kép phương thức từ cả đầu thị giác và văn bản rõ ràng tốt hơn việc điều chỉnh bất kỳ prompt đơn phương thức nào cho tác vụ hạ nguồn. 4) Với sự trợ giúp của prompt thị giác nhận thức lớp, kết quả của DPT của chúng tôi được cải thiện rõ ràng so với VLP. Cụ thể, DPT thu được 1% cải thiện hiệu suất so với VLP trung bình trên 11 tập dữ liệu dưới cài đặt 16-shot, điều này cho thấy tầm quan trọng lớn của CA VPT của chúng tôi.

C. Khái quát Lĩnh vực
Trong phần này, chúng tôi nhằm mục đích tiết lộ mức độ mạnh mẽ của phương pháp của chúng tôi đối với sự dịch chuyển phân phối so với các phương pháp cơ sở.

Tập dữ liệu. Theo cài đặt trong CoOp [9], chúng tôi sử dụng ImageNet làm tập dữ liệu nguồn. Các tập dữ liệu mục tiêu là ImageNetV2 [64], ImageNet-Sketch [65], ImageNet-A [66], và ImageNet-R [67].

Cài đặt. Chúng tôi chọn CoOp và VPT làm các phương pháp cơ sở. Cả ba phương pháp đều được huấn luyện trên tập dữ liệu nguồn với 1 ví dụ trên mỗi lớp, và suy luận zero-shot được tiến hành trên các tập dữ liệu mục tiêu.

Kết quả. Như thể hiện trong Bảng III, phương pháp của chúng tôi đạt được hiệu suất tốt nhất trên ImageNet, ImageNetV2, ImageNet-Sketch, và ImageNet-R, trong khi phương pháp của chúng tôi kém hiệu quả trên ImageNet-A, chứa các ví dụ đối nghịch tự nhiên. Điều này gợi ý rằng phương pháp của chúng tôi có tính mạnh mẽ mạnh hơn các phương pháp cơ sở nhưng có xu hướng dễ bị tổn thương hơn khi đối mặt với các ví dụ đối nghịch so với CoOp. Ngược lại, mô hình VPT thu được kết quả kém trên các tập dữ liệu mục tiêu, cho thấy rằng VPT kém mạnh mẽ hơn CoOp và phương pháp của chúng tôi.

D. Phân tích Thêm
Phân tích về độ sâu chèn CA VPT. CA VPT là một mô-đun cắm-và-chơi và có thể được sử dụng trong các lớp tùy ý của xương sống ViT. Để điều tra các lớp phù hợp nhất cho CA VPT, chúng tôi đã tiến hành thí nghiệm toàn diện theo cả cách thức từ dưới lên và từ trên xuống với các giá trị độ sâu khác nhau, tức là {1→12, 4→12, 8→12, 12} cho cách thức từ trên xuống và {1, 1→4, 1→8, 1→12} cho cách thức từ dưới lên, trên mô hình VLP, trên tất cả 11 tập dữ liệu. Một cài đặt thí nghiệm bổ sung về việc chia sẻ CA VPT qua các lớp khác nhau cũng được tiến hành theo cách thức từ trên xuống bằng cách chia sẻ các tham số của CA VPT. Như thể hiện trong Hình 6, kết quả của cách thức từ trên xuống tốt hơn nhiều so với cách thức từ dưới lên, và lớp cuối cùng của Transformer là lớp phù hợp nhất cho CA VPT, gợi ý rằng CA VPT đóng vai trò quan trọng hơn ở các lớp sâu hơn. Ngoài ra, so sánh CA VPT được chia sẻ và CA VPT nguyên bản, CA VPT được chia sẻ có thể đạt được kết quả tốt hơn một chút trong khi có ít tham số hơn.

Phân tích về độ dài của CA VPT. Để điều tra độ dài phù hợp của CA VPT, chúng tôi đã tiến hành thí nghiệm toàn diện trên các độ dài khác nhau của CA VPT, tức là {0, 1, 5, 10, 20, 50, 100}. Độ dài 0 chỉ ra rằng phương pháp bị suy giảm thành VLP nhưng không có prompt thị giác trong lớp cuối cùng của xương sống ViT. Đối với một số tập dữ liệu, lấy EuroSAT [53] làm ví dụ, nó chỉ chứa 10 lớp, điều này không đủ để thu được hơn 10 CA VPT. Do đó, khi số lượng CA VPT yêu cầu lớn hơn

--- TRANG 9 ---
9
[Biểu đồ tiếp tục thể hiện kết quả thí nghiệm với ViT-B/16]

Hình 5. Kết quả chính trên 11 tập dữ liệu với 1,2,4,8,16 shots với ViT-B/16. Lưu ý rằng chúng tôi cũng so sánh phương pháp của chúng tôi với CPT [33] về độ chính xác trung bình.

số lượng lớp, chúng tôi lấy số lượng lớp làm độ dài của CA VPT để thu được kết quả tương ứng. Như thể hiện trong Bảng V, việc đặt độ dài là 10 đạt được độ chính xác tốt nhất.

Phân tích về hàm mất mát trên mô-đun CA VPT.
Trong mô-đun CA VPT được đề xuất, chúng tôi áp dụng hàm mất mát entropy chéo để khuyến khích việc căn chỉnh token lớp thị giác và các đặc trưng prompt văn bản. Để chứng minh tính hiệu quả của hàm mất mát, chúng tôi đã tối ưu hóa mô hình với α khác nhau trên mô-đun CA VPT. Kết quả thí nghiệm được thể hiện trong Bảng IV. Chúng ta có thể thấy rõ ràng rằng việc đặt α = 0.3 giúp cải thiện độ chính xác trung bình 0.44%, điều này minh họa đáng kể tính hiệu quả của hàm mất mát như vậy.

Phân tích về các tham số của các mô hình khác nhau. Vì DPT giới thiệu nhiều tham số hơn VLP và VPT, câu hỏi phát sinh: liệu VLP hoặc VPT có thể đạt được hiệu suất tương tự như DPT với cùng số lượng tham số không? Chúng tôi tăng

--- TRANG 10 ---
10
[Biểu đồ về phân tích độ sâu chèn CA VPT]

Hình 6. Độ chính xác trung bình trên 11 tập dữ liệu với các lớp khác nhau có CA VPT được chèn. i→j chỉ ra lớp transformer mà CA VPT được chèn vào.

số lượng prompt thị giác lên 120 cho VLP và VPT để so sánh hiệu suất của chúng với DPT dưới số lượng tham số cạnh tranh. Lưu ý rằng CoOp giới hạn số lượng token đầu vào. Do đó, CoOp không được thảo luận trong phần này. Như thể hiện trong Bảng VI, với số lượng prompt thị giác lớn hơn, cả hiệu suất VPT và VLP đều giảm mạnh, gợi ý rằng việc đơn giản tăng kích thước tham số sẽ cản trở hiệu suất.

BẢNG III
SO SÁNH VỚI CÁC PHƯƠNG PHÁP PROMPT ĐƠN PHƯƠNG THỨC VỀ TÍNH MẠNH MẼ
ĐỐI VỚI SỰ DỊCH CHUYỂN PHÂN PHỐI DƯỚI KỊCH BẢN 1-SHOT.

Phương pháp | ImageNet | -V2 | -S | -A | -R | Trung bình OOD | Trung bình
---|---|---|---|---|---|---|---
CoOp [9] | 59.92 | 52.88 | 37.32 | 28.52 | 62.12 | 48.15 | 45.21
VPT | 59.64 | 52.18 | 35.74 | 21.31 | 59.93 | 45.76 | 42.29
DPT | 62.37 | 55.15 | 39.65 | 27.79 | 64.79 | 49.95 | 46.85

BẢNG IV
ĐỘ CHÍNH XÁC TRUNG BÌNH TRÊN 11 TẬP DỮ LIỆU VỚI α KHÁC NHAU.

α | 0 | 0.1 | 0.3 | 0.5 | 0.7 | 1
---|---|---|---|---|---|---
Trung bình | 78.96 | 79.27 | 79.47 | 79.25 | 79.28 | 79.27

BẢNG V
ĐỘ CHÍNH XÁC TRUNG BÌNH TRÊN 11 TẬP DỮ LIỆU VỚI CÁC ĐỘ DÀI KHÁC NHAU CỦA CA VPT.

Độ dài | 0 | 1 | 5 | 10 | 20 | 50 | 100
---|---|---|---|---|---|---|---
Trung bình | 78.41 | 79.21 | 79.29 | 79.47 | 79.35 | 79.28 | 79.33

Phân tích về các xương sống khác nhau. Để tiếp tục thể hiện tính hiệu quả của phương pháp của chúng tôi, chúng tôi đã tiến hành thí nghiệm trên xương sống ViT-B/16. Như thể hiện trong Hình 5 và Bảng II, DPT vượt trội hơn zero-shot CLIP và CoOp 18.15% và 3.45% trung bình trên 11 tập dữ liệu dưới cài đặt 16-shot, điều này chứng minh sự vượt trội của DPT với các xương sống khác.

BẢNG VI
DPT SO VỚI VLP LỚN HƠN SO VỚI VPT TRÊN 11 TẬP DỮ LIỆU DƯỚI CÀI ĐẶT 16 SHOTS.
VCTX BIỂU THỊ SỐ LƯỢNG PROMPT THỊ GIÁC.

Phương pháp | # tham số | Trung bình
---|---|---
VPT(VCTX=10) | 92,160 | 76.89
VPT(VCTX=120) | 1,105,920 | 73.64
VLP(VCTX=10) | 100,352 | 78.42
VLP(VCTX=120) | 1,114,112 | 71.71
DPT | 1,136,384 | 79.47

Những kết luận tương tự cũng có thể được rút ra rằng việc điều chỉnh prompt thị giác hiệu quả hơn prompt văn bản, và việc điều chỉnh chung prompt thị giác-văn bản cũng tăng cường độ chính xác phân loại.

Tóm lại, kết quả thí nghiệm dưới xương sống ViT-B/16 nhất quán với những kết quả dưới xương sống ViT-B/32, điều này chỉ ra tính hiệu quả và tính hợp lý của việc điều chỉnh prompt kép phương thức.

E. Trực quan hóa bản đồ chú ý.
Trong Hình 1, chúng tôi trực quan hóa và so sánh bản đồ chú ý cho lớp cuối cùng của CoOp và mô hình được điều chỉnh DPT của chúng tôi để hiểu sâu hơn về phương pháp được đề xuất. Hình 1 (a) thể hiện các hình ảnh gốc với các đối tượng mục tiêu trong hộp bao quanh màu đỏ. Hình 1 (b) thể hiện các bản đồ chú ý của phương pháp cơ sở Zero-shot CLIP/CoOp. Vì CoOp không điều chỉnh bộ mã hóa hình ảnh, các bản đồ chú ý giống với zero-shot CLIP. Hình 1 (c) mô tả các bản đồ chú ý của phương pháp DPT được đề xuất của chúng tôi. Có thể thấy rõ ràng rằng Zero-shot CLIP/CoOp thường tập trung vào hầu hết các đối tượng điển hình trong hình ảnh, trong khi DPT có xu hướng tập trung nhiều hơn vào đối tượng thị giác mục tiêu (khái niệm).

Chúng tôi thể hiện các ví dụ bổ sung về trực quan hóa trong Hình 7. Tất cả những ví dụ này được lấy mẫu từ Caltech101, StanfordCars, Food101, FGVCAircraft và OxfordPets. Trong Hình 7(a), chúng tôi đã chú thích đối tượng quan tâm trong hộp màu đỏ. Hình 7(b) chứng minh bản đồ chú ý của Zeroshot CLIP/CoOp. Vì CoOp không có sự thay đổi nào đối với đặc trưng hình ảnh, các bản đồ chú ý giống với ZS CLIP. Có thể thấy rõ ràng rằng nhiều đối tượng được làm nổi bật trong khi mô hình chỉ chú ý một chút đến các đối tượng quan tâm trong các tác vụ hạ nguồn. Trong Hình 7(c), thể hiện sự trực quan hóa của VPT, đối tượng quan tâm được làm nổi bật tốt. Điều này cho thấy rằng VPT đã học một số kiến thức liên quan đến tác vụ hạ nguồn. Sự trực quan hóa của VLP và DPT được thể hiện trong Hình 7(d) và (e). So sánh Hình 7(d) và Hình 7(e), đối tượng quan tâm sẽ được tập trung hơn, và nhiều đối tượng không liên quan ít được làm nổi bật hơn trong Hình 7(e). Điều này cho thấy rằng CA VPT có thể giúp mô hình chú ý nhiều hơn đến đối tượng đúng thay vì các đối tượng không liên quan đến tác vụ khác.

Hai hàng cuối cùng của Hình 7 thể hiện một số trường hợp thất bại điển hình. Có thể thấy rõ ràng rằng các vùng tương ứng với các lớp mục tiêu sự thật cơ bản trên các bản đồ chú ý được trực quan hóa cho ZSCLIP và VPT không được làm nổi bật, tức là, Các đặc trưng hình ảnh tương ứng với các lớp mục tiêu không đáng kể trong các đặc trưng hình ảnh oracle được trích xuất từ các mô hình nền tảng đã huấn luyện trước. Như đã phân tích ở trên, prompt văn bản có thể phục vụ như các bộ phân loại được tổng hợp, trong khi prompt thị giác được kỳ vọng

--- TRANG 11 ---
11
[Hình ảnh so sánh trực quan hóa bản đồ chú ý của các phương pháp biến thể]

Hình 7. So sánh trực quan hóa bản đồ chú ý của các phương pháp biến thể. (a) Hình ảnh gốc. (b) Zero-shot CLIP/CoOp. (c) VPT-Deep. (d) VLP. (e) DPT. Đối tượng GT được chú thích được đánh dấu bằng hộp màu đỏ. Hai hàng cuối cùng là các trường hợp thất bại khi mô hình không thể tập trung vào đối tượng GT được chú thích.

để truy vấn kiến thức phù hợp được lưu trữ trong bộ mã hóa hình ảnh đã huấn luyện trước. Nếu các đặc trưng hình ảnh không rất đáng kể trong bộ mã hóa hình ảnh nền tảng đã huấn luyện trước, việc điều chỉnh các đặc trưng hình ảnh thu được để tập trung vào lớp mục tiêu không dễ dàng, đặc biệt trong các trường hợp few-shot.

V. KẾT LUẬN
Trong bài báo này, chúng tôi đề xuất một mô hình điều chỉnh prompt kép phương thức mới để điều chỉnh mô hình thị giác-ngôn ngữ đã huấn luyện trước lớn cho các tác vụ hạ nguồn bằng cách học prompt thị giác và văn bản đồng thời. Để làm cho đặc trưng hình ảnh cuối cùng thu được tập trung nhiều hơn vào khái niệm thị giác mục tiêu, chúng tôi tiếp tục mã hóa cả thông tin liên quan đến tác vụ hạ nguồn và thông tin thể hiện hình ảnh vào prompt thị giác và đề xuất prompt thị giác nhận thức lớp, được tạo ra động bằng cách thực hiện sự chú ý chéo giữa các đặc trưng prompt văn bản và nhúng token hình ảnh. Kết quả thí nghiệm rộng rãi trên 11 tập dữ liệu chứng minh tính hiệu quả của phương pháp được đề xuất và cho thấy sự vượt trội so với các phương pháp điều chỉnh prompt khác với biên độ lớn.

TÀI LIỆU THAM KHẢO
[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., "Learning transferable visual models from natural language supervision," in International Conference on Machine Learning. PMLR, 2021, pp. 8748–8763.

--- TRANG 12 ---
12
[2] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig, "Scaling up visual and vision-language representation learning with noisy text supervision," in International Conference on Machine Learning. PMLR, 2021, pp. 4904–4916.

[3] K. Desai and J. Johnson, "Virtex: Learning visual representations from textual annotations," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 11 162–11 173.

[4] Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz, "Contrastive learning of medical visual representations from paired images and text," arXiv preprint arXiv:2010.00747, 2020.

[5] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.

[6] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh, "Autoprompt: Eliciting knowledge from language models with automatically generated prompts," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 4222–4235.

[7] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, "How can we know what language models know?" Transactions of the Association for Computational Linguistics, vol. 8, pp. 423–438, 2020.

[8] B. Lester, R. Al-Rfou, and N. Constant, "The power of scale for parameter-efficient prompt tuning," arXiv preprint arXiv:2104.08691, 2021.

[9] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, "Learning to prompt for vision-language models," International Journal of Computer Vision, pp. 1–12, 2022.

[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik, "Rich feature hierarchies for accurate object detection and semantic segmentation," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 580–587.

[11] Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, and J. Liu, "Uniter: Universal image-text representation learning," in European conference on computer vision. Springer, 2020, pp. 104–120.

[12] Y. Li, F. Liang, L. Zhao, Y. Cui, W. Ouyang, J. Shao, F. Yu, and J. Yan, "Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm," arXiv preprint arXiv:2110.05208, 2021.

[13] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao, "Clip-adapter: Better vision-language models with feature adapters," arXiv preprint arXiv:2110.04544, 2021.

[14] R. Zhang, R. Fang, P. Gao, W. Zhang, K. Li, J. Dai, Y. Qiao, and H. Li, "Tip-adapter: Training-free clip-adapter for better vision-language modeling," arXiv preprint arXiv:2111.03930, 2021.

[15] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, "Conditional prompt learning for vision-language models," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16 816–16 825.

[16] H. Fang, P. Xiong, L. Xu, and W. Luo, "Transferring image-clip to video-text retrieval via temporal relations," IEEE Transactions on Multimedia, 2022.

[17] J. Yang, Z. Li, F. Zheng, A. Leonardis, and J. Song, "Prompting for multi-modal tracking," in Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 3492–3500.

[18] Y. Rao, W. Zhao, G. Chen, Y. Tang, Z. Zhu, G. Huang, J. Zhou, and J. Lu, "Denseclip: Language-guided dense prediction with context-aware prompting," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 18 082–18 091.

[19] R. Zhang, Z. Zeng, Z. Guo, and Y. Li, "Can language understand depth?" in Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 6868–6874.

[20] R. Mokady, A. Hertz, and A. H. Bermano, "Clipcap: Clip prefix for image captioning," arXiv preprint arXiv:2111.09734, 2021.

[21] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao, and H. Li, "Pointclip: Point cloud understanding by clip," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 8552–8562.

[22] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing," arXiv preprint arXiv:2107.13586, 2021.

[23] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim, "Visual prompt tuning," arXiv preprint arXiv:2203.12119, 2022.

[24] Z. Wang, Z. Zhang, C.-Y. Lee, H. Zhang, R. Sun, X. Ren, G. Su, V. Perot, J. Dy, and T. Pfister, "Learning to prompt for continual learning," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 139–149.

[25] H. Bahng, A. Jahanian, S. Sankaranarayanan, and P. Isola, "Exploring visual prompts for adapting large-scale models," arXiv preprint arXiv:2203.17274, 2022.

[26] B. Zhu, Y. Niu, Y. Han, Y. Wu, and H. Zhang, "Prompt-aligned gradient for prompt tuning," arXiv preprint arXiv:2205.14865, 2022.

[27] Y. Lu, J. Liu, Y. Zhang, Y. Liu, and X. Tian, "Prompt distribution learning," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5206–5215.

[28] H. Bahng, A. Jahanian, S. Sankaranarayanan, and P. Isola, "Exploring visual prompts for adapting large-scale models," arXiv preprint arXiv:2203.17274, vol. 1, no. 3, p. 4, 2022.

[29] Y. Zhang, K. Zhou, and Z. Liu, "Neural prompt search," arXiv preprint arXiv:2206.04673, 2022.

[30] B. Ni, H. Peng, M. Chen, S. Zhang, G. Meng, J. Fu, S. Xiang, and H. Ling, "Expanding language-image pretrained models for general video recognition," in Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part IV. Springer, 2022, pp. 1–18.

[31] D. Li, J. Li, H. Li, J. C. Niebles, and S. C. Hoi, "Align and prompt: Video-and-language pre-training with entity prompts," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 4953–4963.

[32] H. Yang, J. Lin, A. Yang, P. Wang, C. Zhou, and H. Yang, "Prompt tuning for generative multimodal pretrained models," arXiv preprint arXiv:2208.02532, 2022.

[33] Y. Yao, A. Zhang, Z. Zhang, Z. Liu, T.-S. Chua, and M. Sun, "Cpt: Colorful prompt tuning for pre-trained vision-language models," arXiv preprint arXiv:2109.11797, 2021.

[34] A. Sahoo, A. Senapati, A. Das, Y. Kim, R. Feris, and R. Panda, "Frustratingly simple contrastive prompt tuning for vision-language models."

[35] Y. Lu, W. Wang, C. Yuan, X. Li, and Z. Lai, "Manifold transfer learning via discriminant regression analysis," IEEE Transactions on Multimedia, vol. 23, pp. 2056–2070, 2020.

[36] P. Jing, Y. Su, L. Nie, and H. Gu, "Predicting image memorability through adaptive transfer learning from external sources," IEEE Transactions on Multimedia, vol. 19, no. 5, pp. 1050–1062, 2016.

[37] J. O. Zhang, A. Sax, A. Zamir, L. Guibas, and J. Malik, "Side-tuning: a baseline for network adaptation via additive side networks," in Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16. Springer, 2020, pp. 698–714.

[38] H. Cai, C. Gan, L. Zhu, and S. Han, "Tinytl: Reduce memory, not parameters for efficient on-device learning," Advances in Neural Information Processing Systems, vol. 33, pp. 11 285–11 297, 2020.

[39] S.-A. Rebuffi, H. Bilen, and A. Vedaldi, "Learning multiple visual domains with residual adapters," Advances in neural information processing systems, vol. 30, 2017.

[40] Y. Zhu, W. Min, and S. Jiang, "Attribute-guided feature learning for few-shot image recognition," IEEE Transactions on Multimedia, vol. 23, pp. 1200–1209, 2020.

[41] H. Zhang, H. Li, and P. Koniusz, "Multi-level second-order few-shot learning," IEEE Transactions on Multimedia, 2022.

[42] H. Cheng, J. T. Zhou, W. P. Tay, and B. Wen, "Graph neural networks with triple attention for few-shot learning," IEEE Transactions on Multimedia, 2023.

[43] K. Guo, C. Shen, B. Hu, M. Hu, and X. Kui, "Rsnet: relation separation network for few-shot similar class recognition," IEEE Transactions on Multimedia, 2022.

[44] H. Huang, J. Zhang, J. Zhang, J. Xu, and Q. Wu, "Low-rank pairwise alignment bilinear network for few-shot fine-grained image classification," IEEE Transactions on Multimedia, vol. 23, pp. 1666–1680, 2020.

[45] P. Tian, H. Yu, and S. Xie, "An adversarial meta-training framework for cross-domain few-shot learning," IEEE Transactions on Multimedia, 2022.

[46] X. Zhong, C. Gu, M. Ye, W. Huang, and C.-W. Lin, "Graph complemented latent representation for few-shot image classification," IEEE Transactions on Multimedia, 2022.

[47] L. Zhang, Y. Du, J. Shen, and X. Zhen, "Learning to learn with variational inference for cross-domain image classification," IEEE Transactions on Multimedia, 2022.

[48] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, "Momentum contrast for unsupervised visual representation learning," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 9729–9738.

[49] J.-B. Grill, F. Strub, F. Altch´e, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar et al., "Bootstrap your own latent-a new approach to self-supervised learning,"

--- TRANG 13 ---
13
Advances in neural information processing systems, vol. 33, pp. 21 271–21 284, 2020.

[50] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, "Masked autoencoders are scalable vision learners," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16 000–16 009.

[51] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.

[52] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, "An image is worth 16x16 words: Transformers for image recognition at scale," ICLR, 2021.

[53] P. Helber, B. Bischke, A. Dengel, and D. Borth, "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification," IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 12, no. 7, pp. 2217–2226, 2019.

[54] L. Fei-Fei, R. Fergus, and P. Perona, "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories," in 2004 conference on computer vision and pattern recognition workshop. IEEE, 2004, pp. 178–178.

[55] M.-E. Nilsback and A. Zisserman, "Automated flower classification over a large number of classes," in 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing. IEEE, 2008, pp. 722–729.

[56] L. Bossard, M. Guillaumin, and L. V. Gool, "Food-101–mining discriminative components with random forests," in European conference on computer vision. Springer, 2014, pp. 446–461.

[57] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi, "Fine-grained visual classification of aircraft," arXiv preprint arXiv:1306.5151, 2013.

[58] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi, "Describing textures in the wild," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 3606–3613.

[59] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar, "Cats and dogs," in 2012 IEEE conference on computer vision and pattern recognition. IEEE, 2012, pp. 3498–3505.

[60] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, "3d object representations for fine-grained categorization," in Proceedings of the IEEE international conference on computer vision workshops, 2013, pp. 554–561.

[61] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, "Imagenet: A large-scale hierarchical image database," in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248–255.

[62] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, "Sun database: Large-scale scene recognition from abbey to zoo," in 2010 IEEE computer society conference on computer vision and pattern recognition. IEEE, 2010, pp. 3485–3492.

[63] K. Soomro, A. R. Zamir, and M. Shah, "Ucf101: A dataset of 101 human actions classes from videos in the wild," arXiv preprint arXiv:1212.0402, 2012.

[64] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar, "Do imagenet classifiers generalize to imagenet?" in International Conference on Machine Learning. PMLR, 2019, pp. 5389–5400.

[65] H. Wang, S. Ge, Z. Lipton, and E. P. Xing, "Learning robust global representations by penalizing local predictive power," Advances in Neural Information Processing Systems, vol. 32, 2019.

[66] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song, "Natural adversarial examples," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 262–15 271.

[67] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo et al., "The many faces of robustness: A critical analysis of out-of-distribution generalization," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 8340–8349.
